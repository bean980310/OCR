--- ABSTRACT ---
엔티티에 대한 세계 지식을 쉽게 기억함에도 불구하고, 사전 훈련된 언어 모델(LM)은 질문 답변 작업에서 멀티홉 추론을 수행하기 위해 두 개 이상의 사실을 구성하는 데 어려움을 겪습니다. 이 연구에서 우리는 구조화된 지식 그래프에 대한 랜덤 워크에 의존하여 이러한 한계를 개선하는 기술을 제안합니다. 구체적으로, 우리는 소프트 프롬프트를 사용하여 LM이 멀티홉 질문을 답변으로 이어지는 랜덤 워크 경로에 매핑하는 방법을 학습하여 인코딩된 지식을 연결하도록 안내합니다. 두 개의 T5 LM에 우리의 방법을 적용하면 2홉 추론이 필요한 질문에 답하는 데 있어 표준 튜닝 접근 방식보다 상당한 개선이 있음을 보여줍니다. 1
--- METHOD ---
영어: 두 개의 T5 LM에서 s는 2홉 추론이 필요한 질문에 답하는 데 있어 표준 튜닝 접근 방식보다 상당한 개선을 보여줍니다. 1 서론 데이비드 베컴의 딸은 어디에서 태어났습니까?와 같은 질문에 답하기 위해 멀티홉 추론을 수행하려면 두 가지 기본 역량이 필요합니다. C1: 선행 지식 보유(데이비드 베컴의 딸은 하퍼 베컴이고, 하퍼 베컴은 로스앤젤레스에서 태어났습니다.) 및 C2: 내재화된 지식을 구성하는 능력입니다. BERT(Devlin 등, 2019) 및 T5(Raffel 등, 2020)와 같은 현대의 사전 훈련된 언어 모델(LM)은 사실적 지식을 인코딩하는 데 능숙한 것으로 나타났습니다(Petroni 등, 2019; Zhong 등, 2021; Roberts 등, 2020). 이러한 능력은 엔티티 및 관계에 대한 지식과 명시적으로 통합하여 더욱 강화할 수 있습니다(Bosselut 등, 2019; Sun 등, 2020; Wang 등, 2021, ia). 동시에 이러한 LM은 종종 인코딩하는 지식을 구성하는 데 어려움을 겪습니다(Kassner 등, 2020; Talmor 등, 2020; Moiseev 등, 2022). 따라서 C2를 충족하지 못합니다. 이러한 한계를 극복하기 위해 이전 연구에서는 멀티홉 질문을 모델이 더 쉽게 답할 수 있는 단일 홉 하위 질문으로 분해하는 방법을 제안했습니다.★Google Research에서 인턴십하는 동안 수행한 작업.(Min et al., 2019; Perez et al., 2020, ia).그러나 이러한 방법은 완전히 별도의 모델을 교육하거나 인간 주석을 활용해야 합니다(Patel et al., 2022).또한 모델이 관련 사실을 포함하는 추가 텍스트를 명시적으로 받는 작업에 초점을 맞추므로 내재화한 지식을 실제로 구성할 수 있는지 여부가 불분명합니다.이 연구에서는 LM이 멀티홉 추론을 수행하는 독립적이고 자체 포함적인 기능을 개선하는 것을 목표로 합니다.구조화된 지식 그래프에서 샘플링한 엔티티 노드 간의 무작위 워크 경로가 LM이 엔티티 지식을 구성하는 데 유용한 교육 신호를 제공할 수 있다고 가정합니다.이를 테스트하기 위해 두 가지 T5 모델(LARGE 및 XXL, Raffel et al., 2020)에 대한 사례 연구를 수행합니다. 구체적으로, 먼저 멀티홉 질문에 답하는 데 필요한 단일 홉 지식을 LM에 통합합니다(실제로 C1이 충족됨을 보장합니다). 이것만으로는 2홉 추론이 필요한 질문에 대한 실질적인 개선을 보여주기에 충분하지 않음을 보여줍니다. 그런 다음, 인코딩된 구조화된 지식에 대한 랜덤 워크에서 소프트 프롬프트(Qin 및 Eisner, 2021; Lester et al., 2021)를 학습하여 지식 통합 Tmodel을 조정하고, 멀티홉 질문을 입력으로 제공했을 때 LM에서 이 기능을 트리거하는 두 가지 방법을 고안합니다. 첫 번째 방법인 Parse-then-Hop(PATH)은 두 개의 특수 소프트 프롬프트를 사용합니다. 하나는 질문에서 엔터티와 관계를 구문 분석하고, 다른 하나는 랜덤 워크의 출력과 유사한 답변으로 가는 경로를 생성합니다. 두 번째 방법인 MIXHOP은 QA 작업과 랜덤 워크 훈련을 결합한 혼합물에서 단일 프롬프트를 학습하여 모델이 PATH 작업을 암묵적으로 학습할 수 있도록 합니다. 이 두 소프트 프롬프트 방법은 모두 동일한 기본 LM(고정된 상태로 유지)을 사용하고 이를 안내하여 내재화된 엔터티 지식을 구성합니다. 우리의
--- EXPERIMENT ---
s는 제안하는 기술을 사용하여 T5 모델에서 랜덤 워크를 통합하면 해당 모델의 능력을 크게 향상시킬 수 있음을 시사한다.질문: Violet Tendencies의 감독은 어디에서 태어났는가?관련 지식 방법 1: Parse-then-Hop(PATH) Violet Tendencies ; 감독; HP 출생지 (Violet Tendencies, 감독, Casper Andreas), (Casper Andreas, 출생지, 스웨덴) PP 질문 KNIT지식 통합 KNITviolet Tendencies ; 감독; Casper Andreas; 출생지; 스웨덴 조정 David Beckham ; 딸 Random Walk Training THarper Beckham frozen 방법 2: MIXHOP HP David Beckham ; 딸 장소 David Beckham ; 딸 Harper ✈ 질문 KNIT 출생지 Beckham 출생지 Los Angeles MP KNITviolet Tendencies ; 감독; 출생지 그림 1: 접근 방식 개요. 색깔이 칠해진 직사각형 상자는 소프트 프롬프트를 나타냅니다: 호핑 프롬프트(HP), 파싱 프롬프트(PP), MIXHOP 접근법을 위한 프롬프트(MP). ④는 연결을 나타냅니다. 더 큰 모델 크기에서 엔터티 중심의 2홉 질문(Ho et al., 2020)에 답합니다. 간단히 말해, T5-XXL에서 저희 방법은 이전에 제안된 프롬프트 튜닝 접근법(Lester et al., 2021; Vu et al., 2022)과 전체 모델 미세 조정보다 개선되었으며, PATH와 MIXHOP은 각각 전체 모델을 미세 조정하는 것보다 정확한 일치 점수에서 ~16점과 ~9.6점의 이득을 보였습니다. T5-LARGE의 경우 저희 방법은 표준 프롬프트 튜닝 방법보다 개선되었지만 미세 조정을 사용하여 달성한 성능에는 미치지 못하여 최대 11B 매개변수가 있는 더 큰 모델이 소프트 프롬프트를 통한 랜덤 워크에서 제공되는 훈련 신호를 활용하는 데 더 적합하다는 것을 시사합니다. 2 방법 2.1 모델 우리는 100K 단계에 대한 접두사 LM 목적을 사용하여 조정된 체크포인트를 사용하여 두 개의 T5.1.1 모델(Raffel et al., 2020)-T5-Large(770M 매개변수) 및 T5-XXL(11B 매개변수)에 방법을 적용합니다(Lester et al., 2021). 2. 지식 통합 우리는 먼저 우리가 사용하는 LM에 멀티홉 질문에 답하는 데 필요한 전제 조건인 단일 홉 지식(C1)이 있는지 확인합니다. 이는 예비 실험에서 우리가 사용한 T5 모델이 멀티홉 추론에 대한 이 기본 기준을 충족하지 못하는 것으로 나타났기 때문에 필요합니다(표 1 참조). 구체적으로 우리는 Bosselut et al.(2019)을 따르고 질문에 답하기 위해 구성해야 하는 관련 지식을 포함하는 지식 그래프(KG) 트리플에 대한 LMS를 미세 조정합니다. 즉, e1과 e2가 엔티티이고 r이 관계인 트리플(e1, r, e2)이 주어지면 T5 모델을 미세 조정하여 문자열 &quot;el; r1&quot;을 입력으로 받고 Prefix LM 목적(Raffel et al., 2020)을 사용하여 &quot;e2&quot;를 출력으로 생성합니다. 치명적인 망각(McCloskey and Cohen, 1989)을 피하고 LM의 언어 이해 능력을 유지하기 위해 지식 통합 훈련 인스턴스를 모델의 사전 훈련 코퍼스(예: C4(Raffel et al., 2020))와 50:50으로 혼합합니다. 결과 모델을 Knowledge-Integrated T5(KNIT5)로 표시합니다. 2.3 소프트 프롬프트를 사용하여 지식 구성 랜덤 워크 훈련 이 방법은 KNIT5 LM이 관련 KG에 대한 랜덤 워크로 훈련하여 인코딩된 지식을 연결하도록 안내하는 데 중점을 둡니다. 랜덤 워크를 공식화합니다. 여기서는 공유된 엔터티를 통해 선형적으로 연결된 엔터티-관계 엔터티 트리플의 시퀀스로 간주됩니다.그림 1은 길이가 3인 랜덤 워크(Violet Tendencies; 감독; Casper Andreas; 출생지; 스웨덴)의 예를 보여줍니다.랜덤 워크 훈련을 수행하기 위해 소프트 프롬프트(Li 및 Liang, 2021; Lester et al., 2021; Qin 및 Eisner, 2021)에 의존합니다.이것은 LM의 입력에 추가된 학습 가능한 토큰 벡터의 시퀀스입니다.중요한 점은 훈련 중에만 이러한 벡터를 업데이트하여 주 LM의 유틸리티와 인코딩된 지식을 그대로 유지하면서 매개변수 효율성을 유지한다는 것입니다.훈련 절차는 다음과 같습니다.먼저 2.2절에서 사용한 KG에서 길이가 n인 균일한 랜덤 워크를 수행하여 이를 연결하는 관계에 의해 삽입된 엔터티의 시퀀스인 요소를 갖는 세트를 생성합니다: (e1, r1, e2, ..., rn-1, en). 학습하는 동안 KNIT5는 초기 엔터티와 중간 관계(e1, 11, 12, . . ., rn-1)만 있는 불완전한 경로를 입력으로 받고 전체 경로(e1, r1, e2, r2 . . ., rn−1, en)를 생성하는 작업을 맡습니다. KNIT5에서 이 기능을 트리거하는 학습된 프롬프트를 호핑 프롬프트라고 합니다. 2.4 호핑 프롬프트를 사용하여 QA 수행하기 자연어 질문을 지식 그래프의 적절한 경로에 매핑하기 위해 호핑 프롬프트를 활용하는 두 가지 새로운 기술을 제안합니다. 구문 분석 후 호핑(PATH) 소프트 프롬프트의 모듈성을 활용하고 질문과 랜덤 워크 쿼리에서 관계 구조를 구문 분석하는 책임을 별도의 특수 프롬프트를 사용하여 분산하여 기본 모델을 동일하게 유지합니다. 우리는 위에서 설명한 Hopping Prompts에 대한 입력과 유사한, 불완전한 랜덤 워크 쿼리에 대한 질문을 구문 분석하는 &quot;구문 분석&quot; 프롬프트를 훈련합니다. 예를 들어, 질문 &quot;데이비드 베컴의 딸은 어디에서 태어났습니까?&quot;는 &quot;데이비드 베컴 ; 딸 ; 출생지&quot;로 구문 분석됩니다. 그런 다음 구문 분석 단계의 출력을 입력으로 사용하여 구문 분석 프롬프트를 호핑 프롬프트와 바꾼 다음 추론을 실행하여 질문의 엔터티에서 답변까지의 경로를 얻습니다. 그림 1에서 볼 수 있듯이 &quot;데이비드 베컴 ; 딸; 하퍼 베컴 ; 출생지 ; 로스앤젤레스&quot;. 질문에서 적절한 관계 구조를 구문 분석하는 것은 쉽고 자체적으로 이루어져야 한다고 가정합니다. 왜냐하면 외부 지식을 호출하는 것과 달리 질문의 표면적 형태만 사용하기 때문입니다. 외부 지식은 Hopping Prompts에 위임됩니다. MIXHOP 우리는 QA 작업과 Hopping Prompts 작업(50:50)을 혼합하여 단일 프롬프트 세트를 공동으로 훈련하여 이전 방법에서 포워드 패스 수를 절반으로 줄이는 것을 제안합니다. 여기서 우리의 주요 동기는 모델이 질문의 엔터티를 답변 엔터티에 명시적으로 연결하는 구조화된 지식에 질문을 매핑하도록 하는 다양한 훈련 신호를 제공하는 것입니다. PATH와 마찬가지로 MIXHOP은 그림 1에서 볼 수 있듯이 무작위 보행 경로를 직접 출력으로 생성합니다. 3 실험 설정 3.1 데이터 멀티홉 QA 데이터 세트 기존의 멀티홉 QA 데이터 세트는 모델이 추론할 수 있는 추가 문단(Yang et al., 2018; Trivedi et al., 2022)을 제공하지만, 우리는 이러한 맥락이 생략된 더 어려운 폐쇄형 QA 설정(Roberts et al., 2020)에서 작업합니다. 구체적으로, 우리는 WikiData에서 얻은 98,284개의 엔터티와 29개의 관계에 초점을 맞춘 2홉 영어 질문이 포함된 2WikiMultiHopQA 데이터 세트(Ho et al., 2020)의 &quot;구성적&quot; 및 &quot;추론적&quot; 하위 세트를 사용합니다(Vrandečić and Krötzsch, 2014). 우리는 각 질문에 답하는 데 필요한 정확한 구조화된 지식을 엔터티-관계-엔터티 트리플 형태로 고유하게 제공하기 때문에 이 데이터 세트를 선택합니다. 이러한 특정 하위 세트에 대한 테스트 분할은 비공개이므로 검증 분할을 테스트 세트로 사용하고 학습 세트의 10%를 검증에 사용합니다. 총 72,759개의 학습, 8,085개의 검증, 6,768개의 테스트 질문이 있습니다. 1홉 QA 데이터 세트 테스트하는 모델에 필수 1홉 지식이 있는지 특성화하기 위해 2WikiMultiHopQA에서 1홉 질문을 추가로 구성합니다. 영어: 각 2홉 질문에 제공된 엔터티 트리플에 수동으로 정의된 템플릿을 적용합니다(부록 C 참조). 예를 들어, 트리플 Inception ; director ; Christopher Nolan은 Who is the director of Inception?으로 변환됩니다. 83,643개의 학습, 5,022개의 검증, 6,440개의 테스트 QA 인스턴스가 생깁니다. 이 구성된 데이터 세트를 1WikiHopQA라고 합니다. 지식 통합 데이터 2WikiMultiHopQA 데이터 세트(98,개의 엔터티와 29개의 관계, 총 95,000개의 트리플)에서 제공된 기준 진실 트리플 세트를 사용하여 방법에 대한 KG를 구축합니다. 랜덤 워크 학습 코퍼스 위 KG의 각 엔터티에 대해 길이가 3인 최대 20개의 랜덤 워크를 샘플링합니다. 각각은 엔터티 간 2홉 인스턴스에 해당합니다. 이 단계를 다른 시드로 5번 반복하고 중복된 경로를 삭제하며 결과적으로 총 165,324개의 고유 경로를 얻습니다. 중요한 점은 누출을 방지하기 위해 QA 작업의 검증 및 테스트 세트에서 트리플을 포함하는 경로를 보류하여 각각 훈련/검증/테스트 세트로 155,311/8,085/6,768 경로를 얻는다는 것입니다.이런 식으로 실험에서는 모델이 새로운 구조(KG의 완전한 경로)에 엔터티를 성공적으로 배치해야 하는 일반화 유형을 테스트합니다.이러한 일반화의 기본 지식(1홉 트리플)은 모델에 인코딩되지만 구성은 인코딩되지 않습니다.이는 더 엄격하고 눈에 띄는 구성적 일반화 벤치마크(Lake and Baroni, 2018; Kim and Linzen, 2020)에서 어휘 및 구조적 일반화 테스트의 부분 버전으로 볼 수 있습니다.3.2 기준선 및 비교 제안된 접근 방식을 표준 미세 조정 및 프롬프트 조정(Lester et al., 2021)과 비교합니다.¹Balachandran et al.과 같은 작업 (2021)은 NaturalQuestions(Kwiatkowski et al., 2019)와 같은 더 인기 있는 데이터 세트의 질문에 대한 비지도 매핑을 지식 그래프의 경로에 제안했지만, 이러한 경로에 대한 초기 조사에서 광범위하게 노이즈가 있는 것으로 나타났습니다. 설정 모델 LARGE XXL 모델 EM FT4.6.PT KNIT5-LARGE 22.83 84.KNIT6.31.KNIT5-XXL 58.36 92.T6.8.FT KNIT5 22.43.표 1: 1WikiHopQA에서 T5 및 KNIT5가 달성한 테스트 EM 점수. PT: 프롬프트 튜닝, FT: 미세 튜닝. 이를 사용하여 중간 엔터티나 관계 없이 직접 답변을 생성합니다. 또한, 우리는 관련 작업에 대해 사전 훈련된 프롬프트로 프롬프트를 초기화하는 프롬프트 튜닝 방법인 SPOT(Vu et al., 2022)도 적용합니다. 우리의 적용에서, 우리는 Hopping Prompts의 값을 사용하여 프롬프트를 초기화하고, 이를 SPOT-전송하여 KNIT5 모델이 PATH 및 MIXHOP과 유사하게 전체 출력을 생성하도록 안내합니다. 우리는 폐쇄형 도서 QA 설정(Roberts et al., 2020)에서 운영하기 때문에, 우리의 방법은 우리가 고려한 데이터 세트에 대한 이전 접근 방식과 직접 비교할 수 없습니다. 이 모든 접근 방식은 훈련 중에 문단 맥락을 받습니다. 폐쇄형 도서 형식의 현재 데이터 세트를 고려한 다른 방법은 두 가지뿐입니다(Press et al., 2023; Wang et al., 2022). 그러나 두 방법 모두 검증 세트의 더 작은 하위 세트를 테스트 세트로 사용하고 다른 사전 훈련된 모델에서 테스트하기 때문에 결과를 보고된 값과 직접 비교하는 것은 비현실적입니다. 4 실험 및 결과² 다음과 같이 결과를 보고하고 요약합니다. 1홉 지식 통합은 2홉 질문에 대한 미미한 개선으로만 이어집니다. 먼저 T5 모델이 2홉 질문에 답하는 데 필요한 1홉 지식을 인코딩하고 구성하는 정도와 추가 지식 통합(KNIT5를 통해)이 두 가지 능력을 모두 개선할 수 있는지 여부를 확인합니다. 표 1과 3에서 T5 모델은 1홉과 2홉 질문에 모두 답하는 데 어려움을 겪는다는 것을 알 수 있으며, 2홉 질문에 대한 성공을 입증하는 데 필요한 정확한 1홉 엔터티 지식이 심각하게 부족하다는 것을 시사합니다. KNIT5 LM은 T5 대응 제품보다 1WikiHopQA에서 상당한 이득을 보여 이러한 한계를 극복했습니다. ex2에서 ~16.5 및 ~34.8포인트의 향상을 보였습니다. 모든 실험에 대한 훈련 세부 정보는 부록 A에서 확인할 수 있습니다. 표 2: 무작위 워크를 생성하도록 KNIT 모델을 얻기 위해 Hopping Prompts를 훈련하여 얻은 가장 잘 보고된 검증 EM 및 F1 점수. N = 8085. 미세 조정 설정에서 각각 LARGE 및 XXL 크기에서 act match(EM) 점수(표 1). 그러나 이것은 2홉 질문에서 향상을 보여주기에 충분하지 않습니다. T5에 대한 최대 이득은 KNIT5-XXL을 프롬프트 조정하여 달성한 2.2포인트에 불과합니다(표 3 참조). 이는 필수 1홉 지식을 부여받은 후에도 두 LM 모두 더 복잡한 질문에 성공적으로 답할 수 없음을 시사하며, Moiseev et al.(2022)의 결과와 일치합니다. 두 KNIT5 모델 모두 지식 통합 실험에서 KG를 거의 완벽하게 암기한다는 점에 유의하세요(10,000개 미만의 학습 단계에서 ~96% EM 달성, 부록 B.1 참조). 따라서 2홉 질문에 대한 제한은 엔터티 지식이 부족해서가 아니라 암기된 사실을 구성하거나 연결할 수 없기 때문일 가능성이 큽니다. 새로운 랜덤 워크로 일반화하려면 더 큰 LM의 신속한 튜닝이 필요할 수 있습니다. 이제 모든 제안된 QA 방법에 대한 중요한 구성 요소인 랜덤 워크를 생성하는 모델의 성능 분석으로 넘어갑니다. 프롬프트 튜닝 LM은 학습 중에는 보이지 않지만 암기한 사실로 구성된 KG 경로로 얼마나 잘 일반화할까요? 이 단계에서는 소프트 프롬프트(홉핑 프롬프트라고 함)를 활용하여 LM이 암기된 엔터티 지식을 연결하고 랜덤 워크를 수행하는 것과 유사한 경로를 생성하도록 안내해야 했습니다. 즉, 성공적인 출력 생성을 용이하게 하기 위해 인코더에서 필요한 조건을 제공해야 하는 것은 LM 전체가 아니라 홉핑 프롬프트입니다. 또한 완전한 기억(훈련 세트로의 누출로 인해)을 방지하기 위해 주요 QA 작업의 검증 및 테스트 세트에서 트리플이 포함된 경로를 명시적으로 보류했다는 점을 기억하세요. 이런 식으로 모델이 일반화된 방식으로 KG 경로를 구성하는 방법을 얼마나 학습했는지 측정할 수 있습니다. 이를 위해 엔터티의 전체 생성된 범위에 걸쳐 EM 및 F1 점수를 연결 관계에 의해 끼워넣은 상태에서 계산합니다. EM이 F1보다 상당히 엄격한데, F1이 parPrompt-Tuning Fine-Tuning Size SPOT PATH MIXHOP T5 KNITT5 KNITLARGE XXL 4.6.5.8.10.03 11.19 7.22 8.12.92 13.47 20.03 29.6.23에 보상을 주기 때문입니다.표 3: 2WikiMultiHopQA에서 다양한 튜닝 방법을 통해 달성한 테스트 세트 EM 점수(Ho et al., 2020). SPOT(Vu et al., 2022), PATH 및 MIXHOP은 KNIT5를 기본 모델로 사용합니다. 대상과 생성된 출력 간의 토큰의 최종 중복. 표 2는 Hopping Prompts를 사용하여 조정된 랜덤 워크 작업의 검증 세트에서 KNIT5-LARGE 및 KNIT5-XXL에 대한 이러한 점수를 보여줍니다. 표 2에서 KNIT5-large(~EM)와 KNIT5-XXL(~58 EM) 사이에 상당한 격차가 있음을 알 수 있으며, 이는 LARGE 모델이 학습 세트 외부의 엔터티 및 관계를 포함하는 랜덤 워크 경로로 일반화하기 어렵다는 것을 시사합니다. 이 관찰을 통해 보류된 KG 경로로 일반화하는 데 있어 KNIT5-LARGE와 KNIT5-XXL 간의 격차는 2홉 QA를 위해 테스트할 때 반영될 가능성이 높다는 결론을 내립니다. 즉, KNIT5-LARGE를 기본 모델로 하는 우리의 프롬프팅 방법은 훈련 중에 실제 경로를 만나지 않았기 때문에 테스트 세트 문제에서 어려움을 겪을 것으로 예상하고, 동시에 KNIT5-XXL의 경우 그 반대일 것으로 예상합니다.또한 XXL 크기 모델이 달성한 EM 점수는 완벽한 값보다 훨씬 낮아 이러한 격차를 개선하기 위한 향후 작업에 중요한 방향을 강조합니다.무작위 워크에 대한 훈련은 2홉 기능을 상당히 개선하지만 대부분은 더 큰 LM에서 우리는 무작위 워크가 제공한 훈련 신호를 활용하여 KNIT5가 기억한 1홉 지식을 구성하는 세 가지 방법을 사용했습니다.PATH(저희), MIXHOP(저희), SPOT(Vu et al., 2022). 공간 부족으로 인해 이러한 각 방법의 출력 예와 중간 단계 분석(예: 구문 분석)이 부록 B에 나와 있습니다. 표에서 XXL 크기 모델의 경우 세 가지 방법 모두 T5 및 KNIT5에서 표준 튜닝 접근 방식에 비해 2홉 질문에 대한 성능이 상당히 향상되었음을 알 수 있습니다. 특히 KNIT5-XXL의 경우 랜덤 워크 통합 방법이 미세 조정보다 더 향상되는데, 이는 매개변수 효율적 방법에 비해 전이 학습에서 더 나을 것으로 종종 예상됩니다. 세 가지 중에서 PATH 방법이 2홉 질문에 답하는 데 가장 큰 향상(미세 조정 KNIT5-XXL보다 약 16포인트 향상)을 보였습니다. 이것은 동일한 기본 모델에서 작동하는 별도의 전문화된 프롬프트를 학습하여 자연어를 불완전한 구조화된 지식으로 먼저 구문 분석한 다음 질문에 답하기 위해 확장하는 동시에 중간 단계를 이끌어내는 것(Wang et al., 2022)의 가능성을 보여줍니다. 이는 최근의 맥락 내 프롬프트 방법(Wei et al., 2022b; Nye et al., 2022)과 유사합니다. MIXHOP 방법(미세 조정보다 약 9.6포인트 이득)은 PATH에는 미치지 못하지만 SPOT(미세 조정보다 약 6.포인트 이득)보다 여전히 개선되어 더 큰 모델 크기에서 다중 홉 추론을 수행하는 데 있어 관련 작업의 공동 훈련이 순차적 훈련(SPOT에서 사용하는 것과 같은)보다 개선될 수 있음을 시사합니다. T5-LARGE 및 Knit5-large의 경우 제안된 방법은 표준 프롬프트 튜닝보다 개선된 모습을 보였지만 PATH는 프롬프트 튜닝 KNIT5LARGE보다 3.33포인트의 이득을 보였지만 미세 조정으로 달성한 성능에는 미치지 못했습니다. 그러나 일반적인 프롬프트 튜닝에 비해 사소하지 않은 개선은 랜덤 워크가 제공하는 훈련 신호의 일반적인 이점을 시사하는데, 이는 결국 한 자릿수 더 큰 모델에서 가장 인상적입니다. 전반적으로 이러한 결과는 자연어 멀티홉 질문(MIXHOP) 또는 구문 분석(PATH)을 감안할 때 KNIT5-LARGE가 부분적으로 새로운 랜덤 워크를 생성할 수 없다는 랜덤 워크 테스트의 가설과 일치합니다. 5
--- CONCLUSION ---
우리는 프롬프트 튜닝(Lester et al., 2021)을 기반으로 하는 접근 방식을 사용하여 구조화된 지식에 대한 랜덤 워크에서 얻은 훈련 신호를 활용하여 최대 11B 매개변수(T5-XXL)가 있는 LM에서 기억된 세계 지식의 구성을 바람직한 범위까지 트리거할 수 있음을 보여줍니다. 이를 통해 표준 전체 모델 미세 조정을 넘어서도 LM이 2홉 질문에 답하는 능력이 상당히 향상됩니다. 제한 사항 T5 모델의 멀티홉 기능에서 사소하지 않은 개선을 보였음에도 불구하고, 우리의 작업에는 여러 가지 제한 사항이 있습니다. 2홉으로 제한 먼저, 우리는 각 질문을 질문에 답하는 데 필요한 정확하고 노이즈 없는 단일 홉 지식을 포함하는 트리플 체인에 고유하게 매핑하기 때문에 2WikiHopMultiQA(Ho et al., 2020)를 기본 데이터 세트로 선택했습니다. 그러나 이는 분석이 2홉으로만 제한된다는 단점이 있습니다(단, Press et al.(2023, sec 3.5)의 주장을 참조하세요.이들은 3홉과 4홉 질문은 모국어 화자조차 이해하기 어려울 정도로 복잡하다고 제안했습니다).그럼에도 불구하고, 랜덤 워크 학습 방법은 정의상 일반적이며 여러 홉으로 확장할 수 있지만, 2홉 이상의 추론이 필요한 QA 작업에서의 효과는 아직 측정되지 않았습니다.지식 그래프 크기 이 논문에서 우리의 초점은 모델이 복잡한 2홉 질문에 답하기 위해 내면화된 지식을 연결할 수 있도록 하는 것이었습니다.그러나 이를 위해서는 모델이 질문에 답하는 데 필요한 세계 지식을 보유해야 하며, 이를 위해 데이터 세트에 제공된 구조화된 트리플을 사용하여 구성된 KG를 암기해야 했습니다.지식 구성에 초점을 맞추는 것과 세계 지식을 완전히 인코딩하는 것 사이의 이러한 균형은 KG의 크기를 작게 제한했습니다(98,284개의 엔터티와 29개의 관계에 불과).대부분의 실제 응용 프로그램에서는 비실용적일 수 있습니다. 향후 작업에서는 기존 KG에 상당히 많은 양의 추가 트리플을 추가하여 더 큰 크기의 KG(Vrandečić 및 Krötzsch, 2014)를 실험하고 멀티홉 추론에 미치는 영향을 측정할 것입니다.다양한 QA 작업 부족 마지막으로, 질문에서 구조화된 지식으로의 링크가 부족하기 때문에 TriviaQA(Roberts 등, 2020), NaturalQuestions(Kwiatkowski 등, 2019) 등과 같은 CBQA 버전이 있는 인기 있는 데이터 세트를 고려하지 못했습니다.향후 작업에서는 엔티티 및 관계형 연결 기술(Balachandran 등, 2021; Agarwal 등, 2021)을 적용하여 이러한 QA 데이터 세트에 구조화된 지식으로의 (아마도) 노이즈가 있는 링크를 추가할 수 있으며, 이를 통해 방법에 대한 보다 전체적인 그림을 그릴 수 있습니다. 또한, 이는 모델 내에서 인코딩할 엔터티와 관계의 양을 상당히 늘리기 때문에 위의 제한(KG 크기)도 극복할 것입니다.더 큰 모델에 대한 의미 11B 매개변수를 사용하여 가장 큰 T5 LM(T5-XXL)에서 2홉 추론을 트리거하는 데 명확한 개선이 나타났지만, 현대 연구에 따르면 다단계 추론 용량은 2~3배 더 큰 LM에서 자연스럽게 나타납니다(Brown et al., 2020; Chowdhery et al., 2022; Wei et al., 2022b,a).그러나 이러한 LM은 맥락 내 예제에서 이점을 얻습니다(특히 이를 조정하는 것이 사소하지 않고 비용이 많이 들기 때문에).따라서 우리의 방법이 이러한 모델의 용량을 더욱 개선할 수 있는지 여부는 불분명합니다.리소스 제한으로 인해 작업에서 이러한 LM을 테스트하지 않았습니다. 감사의 말 Google Research의 Noah Constant, Chung-Ching Chang, Brian Lester, Ben Withbroe에게 도움이 되는 의견과 조언에 감사드립니다. 또한 세 명의 익명의 검토자에게도 유용한 피드백을 주셔서 감사드립니다. 참고문헌 Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou. 2021. 지식 강화 언어 모델 사전 학습을 위한 지식 그래프 기반 합성 코퍼스 생성. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 3554-3565쪽, 온라인. 컴퓨터 언어학 협회. Vidhisha Balachandran, Bhuwan Dhingra, Haitian Sun, Michael Collins, William Cohen. 2021. 자연어 질문에 대한 배경 지식의 효과 조사. Deep Learning Inside Out(DeeLIO): Deep Learning Architectures를 위한 지식 추출 및 통합에 관한 제2회 워크숍의 회의록, 25-30페이지, 온라인. Association for Computational Linguistics. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi. 2019. COMET: 자동 지식 그래프 구축을 위한 상식 변환기. Association for Computational Linguistics의 제57회 연례 회의록, 4762-4779페이지, 이탈리아 피렌체. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학회. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa. 2020. 추론 단계의 종합적 평가를 위한 멀티홉 QA 데이터 세트 구성. 제28회 국제 컴퓨터 언어학회 학술대회 논문집, 6609-6625쪽, 스페인 바르셀로나(온라인). 국제 컴퓨터 언어학 위원회. Nora Kassner, Benno Krojer, Hinrich Schütze. 2020. 사전 학습된 언어 모델이 지식에 대한 상징적 추론기인가? 24th Conference on Computational Natural Language Learning의 회의록, 552-564페이지, 온라인. Association for Computational Linguistics. Najoung Kim과 Tal Linzen. 2020. COGS: 의미적 해석에 기반한 구성적 일반화 과제. 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP)의 회의록, 9087-9105페이지, 온라인. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov. 2019. 자연스러운 질문: 질문 답변 연구의 벤치마크. Association for Computational Linguistics의 거래, 7:452–466. Brenden Lake와 Marco Baroni. 2018. 체계성 없는 일반화: 시퀀스-투-시퀀스 순환 네트워크의 구성 기술에 관하여. 기계 학습에 관한 국제 컨퍼런스에서, 2873-2882쪽. PMLR. Brian Lester, Rami Al-Rfou, Noah Constant. 2021. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. 2021 자연어 처리 경험적 방법에 관한 컨퍼런스의 회의록에서, 3045-3059쪽, 온라인 및 도미니카 공화국 푼타카나. Association for Computational Linguistics. Xiang Lisa Li와 Percy Liang. 2021. 접두사 튜닝: 생성을 위한 연속 프롬프트 최적화. 제59회 전산언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 45824597쪽, 온라인. 전산언어학 협회. Michael McCloskey와 Neal J Cohen. 1989. 연결주의 네트워크의 파괴적 간섭: 순차적 학습 문제. 학습과 동기의 심리학, 제24권, 109-165쪽. Elsevier. Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi. 2019. 질문 분해 및 재채점을 통한 멀티홉 독해 이해. 제57회 전산언어학 협회 연례 회의록, 6097-6109쪽, 이탈리아 피렌체. 전산언어학 협회. Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi. 2022. SKILL: 대규모 언어 모델을 위한 구조화된 지식 주입. 2022년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 1581-1588쪽, 미국 시애틀. 컴퓨터 언어학 협회. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2022. 작업 표시: 언어 모델을 사용한 중급 계산을 위한 스크래치패드. Deep Learning for Code Workshop에서. Pruthvi Patel, Swaroop Mishra, Mihir Parmar, Chitta Baral. 2022. 질문 분해 단위만 있으면 되는가? 2022년 자연어 처리 경험적 방법 컨퍼런스 회의록, 4553-4569페이지, 아랍에미리트 아부다비. 전산언어학 협회. 이선 페레즈, 패트릭 루이스, 웬타우 이, 경현 조, 다우웨 키엘라. 2020. 질문 답변을 위한 비지도 질문 분해. 2020년 자연어 처리 경험적 방법 컨퍼런스 회의록(EMNLP), 8864-8880페이지, 온라인. 전산언어학 협회. 파비오 페트로니, 팀 록테셸, 세바스찬 리델, 패트릭 루이스, 안톤 바흐틴, 위샹 우, 알렉산더 밀러. 2019. 지식 기반으로서의 언어 모델? 2019년 자연어 처리 경험적 방법에 대한 컨퍼런스 및 자연어 처리에 대한 제9회 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 2463-2473쪽, 중국 홍콩. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis. 2023. 언어 모델에서 구성성 갭 측정 및 좁히기. ICLR 2023 제출. Guanghui Qin과 Jason Eisner. 2021. 질문하는 법 배우기: 소프트 프롬프트를 혼합하여 LM 쿼리하기. Association for Computational Linguistics: Human Language Technologies의 2021년 북미 지부 회의록, 5203-5212쪽, 온라인. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. J. Mach. Learn. Res., 21(140):1–67. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. 언어 모델의 매개변수에 얼마나 많은 지식을 넣을 수 있을까? 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 5418-5426쪽, 온라인. Association for Computational Linguistics. Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. 2020. COLAKE: 문맥화된 언어와 지식 임베딩. 제28회 국제 계산 언어학 컨퍼런스 회의록, 3660~3670쪽, 바르셀로나, 스페인(온라인). 국제 계산 언어학 위원회. Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant. 2020. oLMpics-언어 모델 사전 학습이 포착하는 것에 대한. 계산 언어학 협회 논문집, 8:743~758. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal. 2022. MuSiQue: 단일 홉 질문 구성을 통한 멀티홉 질문. 계산 언어학 협회 논문집, 10:539~554. Denny Vrandečić, Markus Krötzsch. 2014. Wikidata: 무료 협업 지식 기반. ACM 커뮤니케이션, 57(10):78–85. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou&#39;, and Daniel Cer. 2022. SPOT: 소프트 프롬프트 전송을 통한 더 나은 동결 모델 적응. Association for Computational Linguistics(제1권: 장문 논문)의 제60회 연례 회의록, 5039-5059쪽, 더블린, 아일랜드. Association for Computational Linguistics. Boshi Wang, Xiang Deng, and Huan Sun. 2022. 사고의 사슬을 위한 반복적 프롬프트 사전 학습 언어 모델. 2022 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록, 2714-2730쪽, 아부다비, 아랍에미리트. Association for Computational Linguistics. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang. 2021. KEPLER: 지식 임베딩 및 사전 훈련된 언어 표현을 위한 통합 모델. Association for Computational Linguistics의 트랜잭션, 9:176–194. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus. 2022a. 대규모 언어 모델의 새로운 능력. 기계 학습 연구 트랜잭션. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. 신경 정보 처리 시스템의 발전. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. 2018. HotpotQA: 다양하고 설명 가능한 멀티홉 질문 답변을 위한 데이터 세트. 2018년 자연어 처리 경험적 방법 컨퍼런스 회의록, 2369-2380페이지, 벨기에 브뤼셀. Association for Computational Linguistics. Zexuan Zhong, Dan Friedman, Danqi Chen. 2021. 사실적 탐색은 [MASK]: 학습 대 회상 학습. 2021년 북미 계산언어학회 학술대회 논문집: 인간언어기술, 5017-5033쪽, 온라인.계산언어학회.학습 및 실험 세부 사항 하이퍼파라미터 T1.1 체크포인트(Raffel 등, 2020)를 학습하는 데 사용된 기본 하이퍼파라미터와 옵티마이저, 그리고 프롬프트 튜닝 및 SPOT 논문(Lester 등, 2021; Vu 등, 2022)에서 사용된 하이퍼파라미터와 옵티마이저를 사용합니다.모든 프롬프트 튜닝 실험에 대해 프롬프트 길이를 100으로 설정하고 Lester 등(2021)에 따라 T5 모델의 어휘에서 상위 100개 토큰으로 초기화했습니다.각각 최대 100K 및 200K 단계에 대해 모델을 미세 조정하고 프롬프트 조정합니다. 수렴에 대한 학습을 중단하고, 가장 우수한 검증 성능을 보이는 체크포인트를 사용하여 평가합니다. 표 4, 5, 6은 각 유형의 실험에 대한 하이퍼파라미터 값을 보여줍니다. 모든 결과는 단일 실행에서 얻은 것입니다. 하드웨어 및 컴퓨팅 LARGE 모델에 대한 프롬프트 튜닝 및 미세 조정 실험은 16개 TPUv3 칩에서 실행되었고, XXL 모델에 대한 실험은 64개 TPUv3 칩에서 실행되었습니다. 한 가지 예외는 지식 통합(C4에 대한 지속적인 사전 학습, 더 큰 배치 크기, 더 긴 시퀀스 포함)으로, XXL의 경우 256개 TPUv3 칩을 사용했고 LARGE의 경우 64개 TPUv3 칩을 사용했습니다. 코드 메트릭 계산 및 체크포인트의 경우 github에서 오픈 소스로 공개된 T5 및 T5x 코드 기반을 사용합니다.34 신속한 튜닝 실험의 경우 역시 오픈 소스인 원래 코드 기반(Lester et al., 2021)을 조정했습니다.데이터 2WikiMultiHopQA 데이터 세트(Ho et al., 2020)는 Apache 2.0 라이선스로 출시되었습니다.하이퍼파라미터 값 기억(EM) 모델 XXL LARGE4 6 8 10배치 크기 학습률 드롭아웃 32(XXL), 128(LARGE) 0.0.학습 단계 100K(조기 중단 포함) 표 4: T5LARGE 및 T5-XXL 미세 튜닝에 사용된 하이퍼파라미터. 배치 크기와 학습 단계를 제외한 값은 Raffel et al.(2020)과 동일하게 유지되었습니다. 하이퍼파라미터 배치 크기 학습률 드롭아웃 값 0.0. 학습 단계 100K(조기 중단 포함) 표 5: 지식 통합 실험에 사용된 하이퍼파라미터. 배치 크기와 학습 단계를 제외한 값은 Raffel 등(2020)과 동일하게 유지. 하이퍼파라미터 값 배치 크기 32(XXL), 128(LARGE) 학습률 0. 프롬프트 길이 드롭아웃 0. 학습 단계 200K(조기 중단 포함) 표 6: 모든 프롬프트 튜닝 실험에 사용된 하이퍼파라미터. 배치 크기를 제외한 값은 Lester 등(2021)과 동일하게 유지되었고, 학습 단계 수는 Vu 등(2022)과 동일하게 유지되었으며, Vu 등은 더 긴 학습이 유익하다고 밝혔습니다. https://github.com/google-research/ text-to-text-transfer-transformer/tree/main/thttps://github.com/google-research/t5x Shttps://github.com/google-research/ prompt-tuning https://github.com/Alab-NII/2wikimultihop 학습 단계(천 단위) 그림 2: 다양한 KNIT5 모델 크기에 대한 KG 기억의 시간 경과. 주어(e₁)와 관계(r)를 T5 모델에 입력으로 주어졌을 때 객체 엔터티(e2)를 생성하기 위해 계산된 EM 점수. B 추가 분석 B.1 지식 통합 단일 홉 엔터티 지식을 통합하는 것은 우리 방법의 중요한 부분입니다. 모델은 실제로 이 지식을 얼마나 잘 인코딩할 수 있을까요? 그림 2는 두 모델 간의 기억 역학을 보여줍니다. 이는 e₁과 r이 주어졌을 때 e2를 생성하는 정확한 일치 점수로 측정됩니다. 그림 2에서 XXL 및 LARGE 모델은 각각 5,000 및 10,000단계 내에서 KG의 96%를 기억할 수 있음을 알 수 있습니다.배치 크기가 512인 경우 이는 XXL 및 LARGE의 경우 각각 27회 및 54회 데이터 세트를 탐색하는 것으로 해석됩니다.여기서 중요한 주의 사항은 모델의 일반적인 언어 이해와 같은 기능을 유지하기 위해 모델이 C(Raffel et al., 2020)에서도 조정되고 있다는 것입니다.즉, C4 코퍼스에서 학습하지 않은 경우에도 비교적 더 빨리 KG를 기억할 것으로 예상할 수 있지만, 이는 다른 NLP 작업에서 원래 유용성을 크게 상실한 과적합된 모델로 이어져 상쇄 효과가 발생합니다.B.2 PATH의 구문 분석 단계 구문 분석 단계는 Parse-then-Hop 접근 방식이 성공하는 데 필수적입니다. 여기서 우리는 모델이 2WikiMultiHopQA의 2홉 질문에 답하는 데 필요한 관계 구조를 얼마나 잘 추출할 수 있는지에 대한 추가 분석을 수행합니다. 구문 분석 단계의 목적은 초기 엔터티(시드 노드)만 포함하고 그 뒤에 최종 엔터티로 이어지는 관계(에지)가 이어지는 불완전한 랜덤 워크를 나타내는 시퀀스를 출력으로 생성하는 것입니다. 모델 관계 EM 엔터티 EM 전체 EM KNIT5-LARGE 98.76.78.KNIT5-XXL 99.78.80.표 7: 테스트 세트 질문에 대한 PATH의 구문 분석 하위 작업에 대한 메트릭. 예를 들어, 질문이 &quot;인셉션(영화)의 감독은 어디에서 태어났습니까?&quot;인 경우 구문 분석 단계의 출력은 다음과 같아야 합니다. 인셉션(영화); 감독; 출생지 여기서 인셉션(영화)은 엔터티 e1이고, 감독과 출생지는 각각 관계 r1과 r2입니다. 우리는 모델이 6,768개의 테스트 세트 질문에 대해 이 세 가지 요소를 성공적으로 추출한 정도를 세 가지 양을 측정하여 분석합니다. (1) 관계 EM은 관계 쌍(여기서는 &quot;감독; 출생지&quot;)의 기준 진실 범위와 모델 출력에서 추출한 것 사이에서 계산된 정확한 일치 점수입니다. (2) 관계 EM과 유사하지만 초기 엔터티만 고려하는 엔터티 EM, (3) 전체 출력과 대상 사이의 정확한 일치 점수를 계산하는 전체 EM입니다. 표는 두 KNIT5 모델을 프롬프트 튜닝하여 얻은 이러한 값을 보여줍니다. 표 7에서 두 모델을 프롬프트 튜닝하면 질문에서 관계 쌍을 추출할 때 거의 완벽한 EM 값을 얻을 수 있음을 알 수 있습니다. 그러나 모델은 엔터티를 복사할 때 이러한 성능을 유지할 수 없으므로 이 작업에서 전체 EM 점수가 낮아집니다. 우리는 잘못된 엔터티 예측이 있는 무작위로 샘플링된 출력에 대한 수동 분석을 수행했으며 대부분의 오류가 토큰이 누락되어 발생하는 것으로 나타났습니다. 영어: 중간 이름 또는 위의 예에서 &quot;(영화)&quot;와 같은 엔터티에 대한 추가 정보(다른 예로는 &quot;동프리시아 백작&quot; 또는 &quot;(XXX년 출생)&quot;, &quot;(XXX년 사망)&quot; 등과 같은 엔터티 제목이 있습니다) B.3 예시 출력 표 8, 9, 10 및 11은 이 작업에서 사용된 다양한 접근 방식의 출력 예를 보여줍니다(XXL 크기 모델에 대한 예 표시). 아래에서 이러한 각 사례에 대해 자세히 설명합니다. • 표 8에서 무작위 워크의 교육 신호를 활용하는 모든 접근 방식이 성공하는 반면 실패하지 않는 방법을 튜닝합니다. 또한 세 가지 무작위 워크 통합 방법은 모두 구문 분석된 관계 구조와 중간 엔터티에 대해 동의합니다. • 표 9에서 제안된 두 가지 방법(PATH 및 MIXHOP)만 성공하는 반면 다른 모든 방법은 실패합니다. SPOT은 올바른 중간 엔터티(Sally Hemings)를 올바르게 예측하지만 최종 엔터티(John Wayles)를 예측할 수 없습니다. • 표 10은 모든 접근 방식이 실패하는 예를 보여줍니다. 그러나 이 질문은 모호합니다. 왜냐하면 이모는 아버지의 자매 또는 어머니의 자매를 의미할 수 있기 때문입니다. 우리의 무작위 워크 통합 방법은 이러한 관계 구조를 올바르게 예측하지만 중간 및 최종 엔터티를 해결할 수 없습니다. • 표 11은 모든 접근 방식이 잘못된 것으로 평가되었지만 실제로는 올바른 예를 보여줍니다. 여기서 우리는 기본 진실 답변인 &quot;영국&quot;이 잘못된 형태라고 주장합니다. 질문이 사람의 국적을 묻기 때문입니다. 무작위 워크 통합 방법은 관계 구조와 중간 엔터티를 성공적으로 예측합니다. 게다가 모든 접근 방식은 영국 또는 영어를 예측하는데, 이는 영국인에게 더 수용 가능한 국적 형태입니다. 이 문제는 TriviaQA(Roberts et al., 2020)와 유사하게 기본 진실 답변 공간에 엔터티에 대한 별칭을 추가하면 완화할 수 있습니다. C 1WikiHopQA를 구성하기 위한 템플릿 여기서 우리는 2WikiMultiHopQA(Ho et al., 2020) 데이터 세트를 사용하여 단일 홉 지식만 필요한 영어 질문-답변 쌍의 컬렉션인 1WikiHopQA를 구성하는 프로세스를 설명합니다. 2WikiMultiHopQA 데이터 세트는 각 2홉 질문에 집합적으로 답하는 고유한 단일 홉 트리플 시퀀스를 제공합니다. 이는 총 95,103개의 고유한 트리플에 해당합니다. 98,284개의 고유한 엔터티와 29개의 관계. 표 12에서 볼 수 있듯이 각 관계에 대해 다양한 템플릿 세트를 수동으로 정의합니다. 많은 관계의 경우 질문 템플릿에 대한 여러 가지 다른 의역이 있습니다. 예를 들어, 관계 director는 다음과 같이 번역됩니다. X의 감독은 누구입니까? 또는 영화 X를 누가 감독했습니까? 이러한 질문에서: 영화 New World(1995년 영화) 감독의 매장 장소는 어디였습니까? 답변: Père Lachaise Cemetery 모델 설정 응답 FT Forest Lawn Memorial Park T5-XXL PT Forest Lawn Memorial Park FT New York PT SPOT KNIT5-XXL PATH MIXHOP Forest Lawn Memorial Park New World; 감독; Alain Corneau; 매장 장소; Père Lachaise Cemetery PP: New World; 감독; 매장 장소 HP: New World; 감독; Alain Corneau; 매장 장소; Père Lachaise Cemetery New World; 감독; Alain Corneau; 매장 장소;페르 라셰즈 묘지 표 8: 무작위 워크를 활용한 방법이 성공하지만 기준선은 실패하는 사례의 예.질문: 해리엇 헤밍스의 외할아버지는 누구입니까?답변: 존 웨일즈 모델 설정 응답 FT 테드 휴즈 T5-XXL PT 존 헤밍스 FT 베티 헤밍스 PT SPOT KNIT5-XXL 존 헤밍스 해리엇 헤밍스 ;어머니 ;샐리 헤밍스 ;아버지 ;토머스 헤밍스 PP: 해리엇 헤밍스 ;어머니 ;아버지 PATH HP: 해리엇 헤밍스 ;어머니 ;샐리 헤밍스 ;아버지 ;존 웨일즈 해리엇 헤밍스 ;어머니 ;샐리 헤밍스 ;아버지 ;존 웨일즈 MIXHOP 표 9: 모든 기준선이 실패하고 추가적으로 SPOT(Vu et al., 2022)도 잘못된 최종 엔터티를 생성하지만 제안하는 두 가지 방법이 성공하는 사례의 예. 한국어: 질문: Christopher Blom Paus의 이모는 누구입니까?답: Hedevig Christine Paus 모델 설정 응답 FT Saxe-Lauenburg의 Clotilde T5-XXL PT FT PT SPOT KNIT5-XXL Annemarie Blom Paus Anna of Oldenburg Christina Paus Christopher Blom Paus ;아버지 ;Ole Paus ;형제 ;Kjersti Bua Paus PP: Christopher Blom Paus ;어머니 ;형제 PATH HP: Christopher Blom Paus ;어머니 ;Margrete Laarmann;형제 ;Kjartan Flóki MIXHOP Christopher Blom Paus ;어머니 ;Ulla Blom;형제 ;Gunnar Blom 표 10: 모든 접근 방식이 실패하는 모호한 질문의 예(이모는 아버지의 자매 또는 어머니의 자매일 수 있음). 중요한 점은, 랜덤 워크를 사용하는 방법은 질문에 답하는 데 필요한 관계를 정확하게 생성하지만 올바른 엔터티를 예측하는 데 실패한다는 것입니다. 사례에서 전체 세트에서 템플릿을 랜덤하게 샘플링하여 각각에 동일한 가중치를 둡니다. 총 83,643개의 훈련, 5,022개의 검증, 6개의 테스트 QA 쌍이 생깁니다. 질문: John Bede Dalley의 아버지는 어느 국적입니까? 답: 영국 모델 설정 응답 FT 영국 T5-XXL PT 영국 FT 영국 PT 영국 SPOT KNIT5-XXL John Bede Dalley; 아버지; William Dalley %; 시민권 국가; 영국 PP: John Bede Dalley; 아버지; 시민권 국가 PATH HP: John Bede Dalley; 아버지; William Bede Dalley; 시민권 국가; 영국 MIXHOP John Bede Dalley; 아버지; William Dalley, 1st Viscount Darnley; 시민권 국가; 영어: British Table 11: 모든 모델이 질문에 올바르게 답하지 못하는 시나리오의 예이지만, 이는 별칭을 포함하지 않으므로 데이터 세트 때문일 가능성이 높습니다.관계 템플릿 공간 관계 템플릿 공간 director X의 감독은 누구입니까?, 영화 X를 감독한 사람은 누구입니까?생년월일 X의 생년월일은 무엇입니까?, X의 생일은 언제입니까?, X는 언제 태어났습니까?founded by date of death country country of X die?, date of initiation death of X is one country?, X is one country from?, X의 제작자 국적은 무엇입니까?X는 어느 나라에서 왔습니까?, X의 연주자 국적은 무엇입니까?X의 어머니는 누구입니까?, X의 어머니는 누구입니까?X의 창립자는 누구입니까?, X를 창립한 사람은 누구입니까?X는 언제 창립되었습니까?X를 제조한 사람은 누구입니까?시민권 노래 X의 연주자는 누구입니까?, 노래 X를 연주한 사람은 누구입니까?수상 X가 받은 상은 무엇입니까?, 받은 곳 X는 어떤 상을 받았습니까?출생 X는 어디에서 태어났습니까?, X의 출생지는 어디입니까? cause of death Why did X?, What was the cause of X&#39;s death? place burial of Where was X died?, Where is the place of death of death of X? creator Who is the creator of X?, Who creat- place ated X? of Where did X go to prison?, Where child Who is the child of X? detention presenter was X detained? Who is the presenter of X?, Who sented X? predoctoral Who is the doctoral advisor editor Who published X?, Which company published X? educationed at Where did X graduate?, what ma mather of X buried?, Where did X study? spouse Who is the editor of X?, Who edited brothers X? Who is X of brothers?, Who is X&#39;s brothers?, Who is X&#39;s brothers?, Who is X&#39;s spouse? 고용주 X의 고용주는 누구입니까?, X는 어디에서 일합니까? 학생 X의 선생님은 누구였습니까?, X의 선생님은 누구였습니까? 아버지 X의 아버지는 누구입니까?, X의 아버지는 누구입니까? 표 12: 1WikiHopQA를 만드는 데 사용된 29개 관계 각각에 대한 질문 템플릿. X는 주어를 의미합니다.
