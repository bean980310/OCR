--- ABSTRACT ---
Matcha-TTS는 최적 수송 조건 흐름 매칭(OT-CFM)을 사용하여 학습한 빠른 TTS 음향 모델링을 위한 새로운 인코더-디코더 아키텍처입니다. 이를 통해 점수 매칭을 사용하여 학습한 모델보다 적은 합성 단계에서 높은 출력 품질을 낼 수 있는 ODE 기반 디코더가 생성됩니다. 또한 신중한 설계 선택을 통해 각 합성 단계가 빠르게 실행됩니다. 이 방법은 확률적이고 비자기 회귀적이며 외부 정렬 없이 처음부터 말하는 법을 학습합니다. 강력한 사전 학습된 기준 모델과 비교했을 때 Matcha-TTS 시스템은 가장 작은 메모리 공간을 차지하고 긴 발화에서 가장 빠른 모델의 속도와 맞먹으며 청취 테스트에서 가장 높은 평균 의견 점수를 얻습니다. 색인 용어-확산 모델, 흐름 매칭, 음성 합성, 텍스트-음성, 음향 모델링 1.
--- METHOD ---
확률적이고, 비자기회귀적이며, 외부 정렬 없이 처음부터 말하는 법을 학습합니다. 강력한 사전 훈련된 기준 모델과 비교할 때, Matcha-TTS 시스템은 메모리 사용량이 가장 적고, 긴 발화에서 가장 빠른 모델의 속도와 맞먹으며, 청취 테스트에서 가장 높은 평균 의견 점수를 얻습니다. 색인 용어-확산 모델, 흐름 매칭, 음성 합성, 텍스트-음성, 음향 모델링 1. 서론 확산 확률 모델(DPM)(참조 [1])은 현재 이미지 합성[2, 3], 동작 합성[4, 5], 음성 합성[6, 7, 8, 9, 10]과 같은 연속 값 데이터 생성 작업에 대한 심층 생성 모델링에서 새로운 표준을 설정하고 있습니다. 이는 이 논문의 주제입니다. DPM은 데이터(일명 대상) 분포를 사전(일명 소스) 분포, 예를 들어 가우시안으로 변환하는 확산 프로세스를 정의합니다. 그런 다음 확산 과정을 역전시키는 샘플링 과정을 학습합니다. 두 과정은 순방향 및 역방향 시간 확률적 미분 방정식(SDES)으로 공식화할 수 있습니다[11]. 역방향 시간 SDE 초기값 문제를 풀면 학습된 데이터 분포에서 샘플이 생성됩니다. 또한 각 역방향 시간 SDE에는 확률 흐름 ODE[11, 12]라고 하는 해당 상미분 방정식(ODE)이 있으며, 이는 SDE와 정확히 동일한 분포를 설명합니다(그리고 샘플링합니다). 확률 흐름 ODE는 연속 시간 정규화 흐름(CNF)[13]과 유사하지만 값비싼 ODE 솔버를 통해 역전파하거나 보조 변수를 사용하여 역 ODE를 근사할 필요가 없는 소스 샘플을 데이터 샘플로 바꾸는 결정적 프로세스입니다[13]. DPM의 SDE 공식은 데이터 분포의 점수 함수(로그 확률 밀도의 기울기)를 근사하여 학습합니다[11]. 학습 목표는 우도에 대한 증거 하한(ELBO)에서 파생할 수 있는 평균 제곱 오차(MSE)의 형태를 취합니다. 이는 빠르고 간단하며 일반적인 정규화 흐름 모델과 달리 모델 아키텍처에 어떠한 제한도 부과하지 않습니다. 그러나 수치적 SDE/ODE 솔버 없이도 효율적인 학습을 허용하는 반면, DPM은 각 샘플이 SDE를 정확하게 풀기 위해 순서대로 계산되는 수많은 반복(단계)이 필요하기 때문에 합성 속도가 느리다는 단점이 있습니다. 이러한 각 단계 이 작업은 Knut and Alice Wallenberg Foundation에서 자금을 지원한 Wallenberg AI, 자율 시스템 및 소프트웨어 프로그램(WASP)과 한국 산업통상자원부에서 자금을 지원한 산업 전략 기술 개발 프로그램(보조금 번호 20023495)에서 부분적으로 지원했습니다. 전체 신경망을 평가해야 합니다. 이 느린 합성 속도는 오랫동안 DPM의 주요 실질적인 문제였습니다. 이 논문은 연속 정규화 흐름을 기반으로 하는 확률적이고 비자기회귀적이며 빠른 샘플 기반 TTS 음향 모델인 Matcha-TTS¹를 소개합니다. 두 가지 주요 혁신이 있습니다. 1. 우선, 디코더에서 1D CNN과 Transformers의 조합을 사용하는 개선된 인코더-디코더 TTS 아키텍처를 제안합니다. 이는 메모리 소비를 줄이고 평가가 빠르며 합성 속도를 개선합니다. 2. 둘째, 데이터 분포에서 샘플링하는 ODE를 학습하는 새로운 방법인 최적 전송 조건부 흐름 매칭(OT-CFM)[14]을 사용하여 이러한 모델을 학습합니다. 기존 CNF 및 점수 매칭 확률 흐름 ODES와 비교할 때 OT-CFM은 소스에서 대상까지의 경로를 더 간단하게 정의하여 DPM보다 적은 단계로 정확한 합성을 가능하게 합니다.
--- EXPERIMENT ---
모든 결과는 두 혁신 모두 합성을 가속화하여 속도와 합성 품질 간의 균형을 줄인다는 것을 보여줍니다.Matcha-TTS는 빠르고 가볍지만 외부 정렬기가 필요 없이 말하고 정렬하는 법을 배웁니다.강력한 사전 학습된 기준 모델과 비교할 때 Matcha-TTS는 더 나은 자연스러움 등급으로 빠른 합성을 달성합니다.오디오 예제와 코드는 https://shivammehta25.github.io/Matcha-TTS/에서 제공됩니다.2. 배경 2.1. 최근 인코더-디코더 TTS 아키텍처 DPM은 파형 생성[6, 10] 및 엔드투엔드 TTS[7]를 포함하여 수많은 음성 합성 작업에 적용되어 인상적인 결과를 얻었습니다.Diff-TTS[9]는 음향 모델링에 DPM을 적용한 최초의 회사입니다.얼마 지나지 않아 Grad-TTS[8]는 확산 프로세스를 SDE로 개념화했습니다. 이러한 모델과 Fast Grad-TTS[15]와 같은 하위 모델은 비자기 회귀적이지만 TorToiSe[16]는 양자화된 잠재 변수를 사용하여 자기 회귀적 TTS 모델에서 DPM을 입증했습니다.위의 모델은 많은 최신 TTS 음향 모델과 마찬가지로 인코더에 Transformer 블록이 있는 인코더-디코더 아키텍처를 사용합니다.FastSpeech 1 및 2[17, 18]와 같은 많은 모델은 위치 종속성에 사인파 위치 임베딩을 사용합니다.이는 긴 시퀀스에 대한 일반화가 잘 되지 않는 것으로 밝혀졌습니다.참조[19].GlowTTS[20], VITS[21] 및 Grad-TTS는 대신 상대적 위치 임베딩을 사용합니다[22].안타깝게도 이러한 모델은 짧은 컨텍스트 창 외부의 입력을 &quot;단어 가방&quot;으로 처리하여 종종 부자연스러운 음조를 생성합니다. LinearSpeech [23]는 대신 ROPE(회전 위치 임베딩) [24]를 사용했는데, 이는 상대 임베딩보다 계산 및 메모리 측면에서 유리하며 더 긴 거리로 일반화됩니다 [25, 19]. 따라서 Matcha-TTS는 인코더에서 ROPE가 있는 Transformers를 사용하여 Grad-TTS에 비해 RAM 사용량을 줄입니다. 저희는 ROPE를 사용하는 최초의 SDE 또는 ODE 기반 TTS 방법이라고 믿습니다. 저희는 TTS에 흐름 매칭을 사용하고 이름이 일부 사람들이 Taco(tron)보다 선호하는 &quot;말차 차&quot;와 비슷하기 때문에 저희 접근 방식을 Matcha-TTS라고 부릅니다. 최신 TTS 아키텍처는 디코더 네트워크 설계 측면에서도 다릅니다. 정규화 흐름 기반 방법인 Glow-TTS [20] 및 OverFlow [26]는 확장된 1D 합성곱을 사용합니다. [9, 27]과 같은 DPM 기반 방법도 마찬가지로 1D 합성곱을 사용하여 멜 스펙트로그램을 합성합니다. 반면 Grad-TTS[8]는 2D 합성곱을 사용하는 U-Net을 사용합니다. 이는 멜 스펙트로그램을 이미지로 취급하고 시간과 주파수 모두에서 암묵적으로 변환 불변성을 가정합니다. 그러나 음성 멜 스펙트라는 주파수 축을 따라 완전히 변환 불변하지 않으며 2D 디코더는 일반적으로 텐서에 추가 차원을 도입하기 때문에 더 많은 메모리가 필요합니다. 한편, FastSpeech 1 및 2와 같은 비확률적 모델은 (1D) Transformers가 있는 디코더가 장거리 종속성과 빠른 병렬 합성을 학습할 수 있음을 보여주었습니다. Matcha-TTS도 디코더에서 Transformers를 사용하지만 Stable Diffusion 이미지 생성 모델[3]의 2D U-Net에서 영감을 받은 1D U-Net 디자인입니다. 일부 TTS 시스템(예: FastSpeech [17])은 외부에서 제공되는 정렬에 의존하지만, 대부분 시스템은 동시에 말하고 정렬하는 법을 배울 수 있습니다.하지만 빠르고 효과적인 학습을 위해 단조로운 정렬을 장려하거나 적용하는 것이 중요한 것으로 밝혀졌습니다.[28, 29] 이를 위한 한 가지 메커니즘은 Glow-TTS [20] 및 VITS [21]에서 사용되는 단조로운 정렬 검색(MAS)입니다.특히 Grad-TTS [8]는 사전 손실이라는 용어를 사용하여 입력 심볼을 출력 프레임에 정렬하는 법을 빠르게 학습합니다.이러한 정렬은 또한 로그 도메인에서 MSE를 최소화하는 결정적 지속 시간 예측기를 학습하는 데 사용됩니다.Matcha-TTS는 정렬 및 지속 시간 모델링에 동일한 방법을 사용합니다.마지막으로, Matcha-TTS는 모든 디코더 피드포워드 계층에서 BigVGAN [30]의 스네이크 베타 활성화를 사용한다는 점에서 다릅니다.2.2. 흐름 매칭 및 TTS 현재, 일부 최고 품질의 TTS 시스템은 DPMS[8, 16] 또는 이산 시간 정규화 흐름[21, 26]을 활용하며, 연속 시간 흐름은 덜 탐구되었습니다.Lipman 등[14]은 최근 확률 흐름 ODE와 CNF를 통합하고 확장하는 ODE를 사용한 합성 프레임워크를 도입했습니다.그런 다음 DPM에 대한 점수 함수를 학습하거나 클래식 CNF[13]와 같이 학습 시간에 수치 ODE 솔버를 사용하는 대신 조건부 흐름 매칭(CFM)이라는 간단한 벡터장 회귀 손실을 사용하여 합성을 위한 ODE를 학습하는 효율적인 접근 방식을 제시할 수 있었습니다.중요하게도 최적 전송의 아이디어를 활용하여 CFM을 설정하여 샘플을 소스 분포에서 데이터 분포로 매핑하는 동안 거의 변하지 않는 간단한 벡터장을 갖는 ODE를 생성할 수 있습니다.기본적으로 확률 질량을 직선을 따라 전송하기 때문입니다.이 기술을 OT-CFM이라고 합니다.정류 흐름[31]은 비슷한 아이디어로 동시 작업을 나타냅니다. 간단한 경로는 ODE를 몇 가지 이산화 단계를 사용하여 정확하게 풀 수 있음을 의미합니다.즉, DPM보다 적은 신경망 평가로 정확한 모델 샘플을 그릴 수 있어 동일한 품질에 대해 훨씬 빠른 합성이 가능합니다.CFM은 대부분 증류에 기반을 둔 SDE/ODE 기반 TTS를 가속화하기 위한 이전 접근 방식과 다른 새로운 기술입니다(예: [27, 15, 32]).Matcha-TTS 이전에 CFM 기반 음향 모델링에 대한 유일한 공개 사전 인쇄본은 Meta의 Voicebox 모델이었습니다[33].Voicebox(VB)는 대규모 학습 데이터를 기반으로 다양한 텍스트 가이드 음성 채우기 작업을 수행하는 시스템으로, 영어 변형(VB-En)은 60,000시간 분량의 독점 데이터로 학습되었습니다. VB는 Matcha-TTS와 상당히 다릅니다.VB는 마스킹과 CFM을 조합하여 학습한 TTS, 잡음 제거 및 텍스트 유도 음향 채우기를 수행하는 반면 Matcha-TTS는 OT-CFM만을 사용하여 학습한 순수한 TTS 모델입니다.VB는 AliBi[19] 자기 주의 편향을 사용하여 합성곱 위치 인코딩을 사용하는 반면, 텍스트 인코더는 RoPE를 사용합니다.VB와 달리, 우리는 표준 데이터로 학습하고 코드와 체크포인트를 공개적으로 제공합니다.VB-En은 3억 3천만 개의 매개변수를 사용하는데, 이는 우리 실험에서 MatchaTTS 모델보다 18배 더 큽니다.또한, VB는 학습에 외부 정렬을 사용하는 반면, Matcha-TTS는 외부 정렬 없이 말하는 법을 학습합니다.3. 방법 이제 흐름 매칭 학습(3.1절)을 개략적으로 설명한 다음(3.2절) 제안하는 TTS 아키텍처에 대한 세부 정보를 제공합니다.3.1. 최적 수송 조건부 흐름 매칭 여기서는 흐름 매칭에 대한 개략적인 개요를 제공하고, 먼저 벡터장에 의해 생성된 확률 밀도 경로를 소개한 다음 제안하는 방법에서 사용되는 OT-CFM 목표로 이어진다. 표기법과 정의는 주로 [14]을 따른다. x가 복잡하고 알려지지 않은 데이터 분포 q(x)에서 샘플링된 데이터 공간 Rd의 관측치를 나타낸다고 하자. 확률 밀도 경로는 시간에 따라 달라지는 확률 밀도 함수 pt : [0,1]× Rd R0이다. 데이터 분포 q에서 샘플을 생성하는 한 가지 방법은 확률 밀도 경로 pt를 구성하는 것이다. 여기서 t = [0,1]이고 po(x) : N(x; 0, I)는 사전 분포이고, 여기서 p₁(x)는 데이터 분포 q(x)를 근사한다. 예를 들어, CNF는 먼저 벡터장 vt: [0,1] × Rd → Rd를 정의하고, 이는 ODE t(x) = vt (Ot(x)); Φο(α) = x를 통한 흐름 &amp;t: [0, 1] × Rd → Rd를 생성합니다. (1) 이는 데이터 포인트의 주변 확률 분포인 경로 pt를 생성합니다. Eq. (1)의 초기값 문제를 풀어 근사된 데이터 분포 p₁에서 샘플링할 수 있습니다. po에서 p₁ ≈q로의 확률 경로 pt를 생성하는 알려진 벡터장 ut가 있다고 가정합니다. 흐름 매칭 손실은 LFM(0) = Et,pt(x)||Ut (x) — vt (x; 0) || ², 여기서 t~ · U[0, 1]이고 v₁(x; 0)은 매개변수 0을 갖는 신경망입니다. 그럼에도 불구하고, 벡터장 ut와 타겟 확률 pt에 접근하는 것이 자명하지 않기 때문에 실제로는 흐름 매칭이 어렵습니다. 따라서 조건부 흐름 매칭은 대신 LCFM(0) = Et,q(x1),Pt(x|x1)||Ut(x|x1) — vt(x; 0) ||²를 고려합니다. (3) Vt 이것은 다루기 힘든 주변 확률 밀도와 벡터장을 조건부 확률 밀도와 조건부 벡터장으로 대체합니다. 중요한 것은 이것들이 일반적으로 다루기 쉽고 폐쇄 형태의 솔루션을 가지고 있으며, LCFM(0)과 LFM(0)이 모두 0에 대해 동일한 기울기를 갖는다는 것을 더 보여줄 수 있다는 것입니다[14]. Matcha-TTS는 특히 간단한 기울기를 갖는 CFM 변형인 최적 수송 조건부 흐름 매칭(OT-CFM) [14]을 사용하여 학습됩니다. OT-CFM 손실 함수는 다음과 같이 작성할 수 있습니다. OT에서 L(0) = Et,q(x1),P0(x0)||u₁ (01 (x)|x1) — vt ($t (x)|μ; 0) ||², (4)로 정의하여 (x) = (1 − (1 − σmin) t) xo + tx₁를 xo에서 x1로의 흐름으로 정의합니다. 여기서 각 데이터 x₁는 [14]와 같이 랜덤 샘플 xo ~ N(0, I)에 매칭됩니다. 그 기울기 벡터 필드(기대값이 학습 대상)는 u(þt(x0)|x1) = x1σmin)xo이며, 이는 선형적이고 시불변하며 (OT OT xo 및 x1에만 의존합니다. 이러한 속성은 DPM에 비해 더 쉽고 빠른 학습, 더 빠른 생성 및 더 나은 성능을 가능하게 합니다.μ 업샘플 wwwww Ceil h1:L 지속 시간 예측기 투영 텍스트 인코더 tɛkst 음성화 V₁(x|µ; 0) 흐름 예측 네트워크 0 N(0,1) t=μ 시간 단계 임베딩 네트 Resnet1D Transformer 블록 Resnet1DTransformer 블록 Resnet1D Transformer 블록 Resnet1D Transformer 블록 Resnet1D Transformer 블록 Resnet1D Transformer 블록 다운샘플링 블록 Mid 블록 업샘플링 블록 텍스트 xt =그림 1: 합성 시간에 제안된 접근 방식의 개요. Matcha-TTS의 경우 x1은 음향 프레임이고 μ는 영어: 다음 섹션에 설명된 아키텍처를 사용하여 텍스트에서 예측된 해당 프레임의 조건부 평균값. σmin은 작은 값(실험에서 1e-4)을 갖는 하이퍼파라미터입니다. 3.2. 제안된 아키텍처 Matcha-TTS는 신경 TTS를 위한 비자기회귀 인코더-디코더 아키텍처입니다. 아키텍처 개요는 그림 1에 나와 있습니다. 텍스트 인코더 및 지속 시간 예측기 아키텍처는 [20, 8]을 따르지만 상대적 임베딩 대신 회전 위치 임베딩[24]을 사용합니다. 정렬 및 지속 시간 모델 학습은 [8]에 설명된 대로 MAS 및 사전 손실 Lenc를 사용합니다. 반올림된 예측 지속 시간은 인코더에서 출력한 벡터를 업샘플링(복제)하여 텍스트와 선택한 지속 시간을 기준으로 예측된 평균 음향 특징(예: mel-spectrogram)인 μ를 얻는 데 사용됩니다. 이 평균은 합성에 사용된 벡터 필드 vt(þt(x0)|µ; 0)를 예측하는 디코더를 조절하는 데 사용되지만, 초기 노이즈 샘플 xo의 평균으로는 사용되지 않습니다(Grad-TTS와 달리).그림 2는 Matcha-TTS 디코더 아키텍처를 보여줍니다.[3]에서 영감을 받아, 입력을 다운샘플링 및 업샘플링하기 위한 1D 합성 잔여 블록을 포함하는 U-Net이며, [8]에서와 같이 흐름 매칭 단계 t = [0,1]이 내장되어 있습니다.각 잔여 블록 뒤에는 Transformer 블록이 오는데, 그 피드포워드 네트가 스네이크 베타 활성화를 사용합니다[30].이러한 Transformer는 위치 임베딩을 사용하지 않습니다.이미 인코더에서 음소 간 위치 정보가 구워졌고, 합성 및 다운샘플링 연산은 동일한 음소 내의 프레임 간에 이를 보간하고 서로의 상대적 위치를 구별하는 역할을 하기 때문입니다. 이 디코더 네트워크는 Grad-TTS에서 사용하는 2D 합성곱 전용 U-Net보다 평가 속도가 훨씬 빠르고 메모리 소모도 적습니다[8]. 4. 실험 제안된 접근 방식의 유효성을 검증하기 위해 청취 테스트를 포함한 여러 실험에서 세 개의 사전 학습된 기준선과 비교했습니다. 모든 실험은 NVIDIA RTX 3090 GPU에서 수행되었습니다. 오디오와 코드는 shivammehta25.github.io/Matcha-TTS/를 참조하세요. 4.1. 데이터 및 시스템 LJ Speech 데이터 세트(공개 도메인 2 https://keithito.com/LJ-Speech-Dataset/ 400T(x) 그림 2: Matcha-TTS 디코더(그림 1의 흐름 예측 네트워크)를 읽는 여성 미국 영어 원어민 화자)의 표준 분할에 대한 실험을 수행하여 이 데이터에 대한 Matcha-TTS 아키텍처 버전을 학습했습니다. 우리는 [8]과 동일한 인코더와 지속 시간 예측기(즉, 동일한 하이퍼파라미터)를 사용했지만 인코더에서 위치 임베딩만 달랐습니다.우리의 훈련된 흐름 예측 네트워크(디코더)는 그림 2에서 볼 수 있듯이 두 개의 다운샘플링 블록을 사용한 다음 두 개의 중간 블록과 두 개의 업샘플링 블록을 사용했습니다.각 블록에는 은닉 차원 256, 두 개의 헤드, 어텐션 차원 64 및 &#39;snakebeta&#39; 활성화를 갖는 하나의 Transformer 계층이 있습니다[30].espeak-ng 백엔드가 있는 Phonemizer³[34]를 사용하여 입력 문자소를 IPA 폰으로 변환했습니다.우리는 배치 크기 32 및 학습 속도 1e-4의 2개 GPU에서 500k 업데이트를 훈련했으며, 훈련된 시스템을 MAT라고 레이블했습니다. MAT는 LJ Speech에 사용할 수 있는 사전 훈련된 체크포인트가 있는 널리 사용되는 세 가지 신경 TTS 기준 접근 방식, 즉 강력한 DPM 기반 음향 모델인 Grad-TTS* [8] (레이블 GRAD), 빠른 비확률적 음향 모델인 FastSpeech 2(FS2), 불연속 시간 정규화 흐름을 갖춘 강력한 확률적 엔드투엔드 TTS 시스템인 VITS³와 비교되었습니다. 기준선은 각각의 링크된 저장소의 공식 체크포인트를 사용했습니다. 공식 구현을 제공하지 않는 FS2의 경우 대신 Meta의 FairSeq의 체크포인트를 사용했습니다. 새로운 아키텍처로 인한 효과에서 CFM 훈련의 효과를 분리하기 위해 MAT와 동일한 최적화 하이퍼파라미터를 사용하여 대신 OT-CFM 목적을 사용하여 GRAD 아키텍처를 훈련했습니다. 이를 통해 GCFM이라는 레이블이 붙은 절제가 생성되었습니다. 모든 음향 모델(즉, VITS를 제외한 모든 시스템)에 대해 우리는 파형 생성을 위해 사전 훈련된 HiFi-GAN [35] LJ Speech 체크포인트 LJ_V17을 사용했고, [36]에서 도입한 것과 같은 노이즈 제거 필터는 2.5e-4의 강도로 사용했습니다. 가장 중요한 것으로, 우리의 실험에는 VOC로 표시된 보코딩된 홀드아웃 음성도 포함되었습니다. DPM과 같은 ODE 기반 모델은 속도와 품질을 절충할 수 있습니다. 따라서 우리는 ODE 솔버에 대해 다른 단계 수를 사용하여 훈련된 ODE 기반 시스템에서 합성을 평가했습니다. [8]과 마찬가지로, 우리는 1차 Euler 전방 ODE 솔버를 사용했는데, 여기서 단계 수는 일반적으로 NFE로 약칭되는 함수(즉, 신경망) 평가 수와 같습니다. 이로 인해 일부 시스템에 여러 조건이 발생했습니다. 우리는 이러한 조건을 MAT-n, GRAD-n 및 GCFM-n으로 표시했고, n은 NFE입니다. 영어: [8]에서 NFE 10과 100이 Grad-TTS에 대해 동일한 MOS를 제공한다고 보고했기 때문에 NFE 10 이하를 사용했습니다(NFE 50은 공식 코드 기본값). 모든 조건에서 [8]과 유사하게 합성에 0.667의 온도를 사용했습니다. 표 1은 평가 조건의 개요를 제공합니다. 3 https://github.com/bootphon/phonemizer 4 https://github.com/huawei-noah/ Speech-Backbones/tree/main/Grad-TTS Shttps://github.com/jaywalnut 310/vits 6https://github.com/facebookresearch/fairseq 7https://github.com/jik876/hifi-gan/Condition Params. RAM RTF (u±o) WER MOS VOC 13.9M 0.001±0.1.97 4.13±0.FSVITS 41.2M 36.3M GRAD-10 14.8M GRAD-6.0 0.010±0.004 4.18 3.29±0.12.4 0.074±0.083 2.7.8 0.049±0.013 3.&quot; &quot; 3.71±0.3.49±0.0.019±0.006 3.69 3.20±0.&quot; &quot; GCFM-0.019±0.004 2.3.57±0.MAT-MAT-MAT-18.2M 4.8 0.038±0.019 2.3.84±0.&quot; &quot; &quot; &quot; 합성 시간(초)0.TC 0.1000FSGRAD-GRAD-GCFM-MAT-MAT-MAT-0.019±0.008 2.15 3.77±0.0.015±0.006 2.34 3.65±0.표 1: 평가 조건(ODE 기반 방법의 경우 NFE 사용)과 매개변수 수, 학습에 필요한 최소 GPU RAM(GiB), 테스트 세트의 실시간 요소(보코딩 시간 포함), 백분율로 나타낸 ASR WER, 95% 신뢰 구간을 적용한 평균 의견 점수. 각 열의 최상의 TTS 조건은 굵은 글씨로 표시했습니다. VOC의 매개변수 수와 RTF는 보코더와 관련이 있습니다. 4.2. 평가, 결과 및 논의 우리는 객관적, 주관적으로 접근 방식을 평가했습니다. 먼저 모든 시스템의 훈련 중(배치 크기 32 및 fp16에서) 매개변수 수와 최대 메모리 사용량을 측정했고, 그 결과를 표 1에 나열했습니다. MAT는 GRAD/GCFM과 크기가 거의 같고 다른 모든 시스템보다 작다는 것을 알 수 있습니다. 특히 보코더(13.9M 매개변수)를 MAT 매개변수 수에 추가한 후에도 VITS보다 작습니다. 더 중요한 점은 모든 기준선보다 메모리를 적게 사용한다는 것입니다. 이는(매개변수 수보다) 훈련할 수 있는 모델의 크기와 강력함에 대한 주요 제한 요소입니다. 시스템을 훈련한 후 테스트 세트를 합성할 때 실시간 요인(RTF) 평균과 표준 편차를 계산하고, 결과에 Whisper 매체[37] ASR 시스템을 적용할 때 단어 오류율(WER)을 평가하여 다양한 조건의 합성 속도와 이해도를 평가했습니다. 강력한 ASR 시스템의 WERS는 이해도[38]와 잘 상관 관계가 있기 때문입니다. 표 1의 결과는 MAT가 두 가지 합성 단계만 사용하더라도 가장 알아들을 수 있는 시스템임을 시사합니다.또한 MAT는 VITS보다 훨씬 빠르고, 동일한 NFE에서 GRAD/GCFM보다 동등하거나 더 빠르며, 가장 빠른 설정에서는 FS2보다 약간 느릴 뿐입니다.합성된 오디오의 자연스러움을 평가하기 위해 평균 의견 점수(MOS) 청취 테스트를 실행했습니다.테스트 세트에서 길이가 다른 40개의 발화(10개 그룹 4개)를 선택하고 모든 조건을 사용하여 각 발화를 합성했으며, EBU R128을 사용하여 모든 자극의 크기를 정규화했습니다.80명의 피험자(헤드폰을 사용하는 영어 원어민이라고 자체 보고)가 Prolific을 통해 크라우드소싱되어 이러한 자극을 듣고 평가했습니다.각 자극에 대해 청취자에게 &quot;합성된 음성이 얼마나 자연스럽게 들리나요?&quot;라고 물었고, Blizzard Challenge [39]에서 채택한 1(&quot;완전히 부자연스러움&quot;)에서 5(&quot;완전히 자연스러움&quot;)까지의 정수 등급 척도로 응답을 제공했습니다. 각 10개 발화 그룹은 청취자가 평가했으며, 청취자는 분의 중간 완료 시간에 대해 3파운드를 받았습니다. 주의가 산만한 청취자는 걸러내어 [26]과 똑같은 방식으로 대체되었습니다. 결국 각 조건에 대해 800개의 평가를 얻었습니다. 정규 근사에 기반한 신뢰 구간과 함께 결과 MOS 값이 표 1에 나열되어 있습니다. MOS 값은 청취자 인구 통계 및 지시([40, 41] 참조)와 같이 자극 품질 외부의 여러 변수에 따라 달라지므로 절대적인 지표로 취급해서는 안 됩니다. 따라서 다른 논문과 MOS 값을 비교하는 것은 의미가 없을 가능성이 큽니다. 모든 쌍의 조건에 t-검정을 적용하면 모든 차이는 텍스트 길이(문자) 그림 3: 음향 모델의 프롬프트 길이 대 합성 시간의 산점도. 회귀선은 로그-로그 축으로 인해 곡선으로 표시됩니다. 영어: 쌍 (MAT-10, MAT-4), (MAT-4, VITS), (VITS, MAT-2), (MAT2, GCFM-4), 및 (GCFM-4, GRAD-10)을 제외하고 a = 0.05 수준에서 통계적으로 유의미한 것으로 밝혀졌습니다. 즉, MAT는 항상 동일한 NFE에 대해 GRAD보다 유의미하게 더 높은 자연스러움을 평가 받았으며 항상 FS2를 능가했습니다. MAT-4&gt;GCFM-4&gt;GRAD-4이기 때문에 새로운 아키텍처와 학습 방법 모두 자연스러움 향상에 기여했습니다. GRAD-10이 GRAD-4보다 훨씬 우수했지만 MAT-10과 MAT-4가 비슷한 성과를 보였다는 사실은 GRAD가 좋은 합성 품질을 위해 많은 단계가 필요한 반면 MAT는 더 적은 단계로 좋은 수준에 도달했음을 시사합니다. 마지막으로 VITS는 MOS 측면에서 MAT-2 및 MAT-4와 유사한 성과를 보였습니다. MAT10은 평가에서 MAT-4와 비슷했지만 VITS보다 유의미하게 더 우수했습니다. 주어진 n에 대해 MAT-n은 항상 동일하거나 더 빠른 RTF를 가진 모든 시스템보다 높은 점수를 받았습니다. 요약하자면, Matcha-TTS는 모든 비교 가능한 기준선보다 비슷하거나 더 나은 자연스러움을 달성했습니다. 마지막으로, GPT-28 모델을 사용하여 길이가 다른 180개의 문장을 생성하고 그림 3에 벽시계 합성 시간을 표시하고 최소 제곱 회귀선을 데이터에 맞춰 합성 속도가 다른 모델에 대해 어떻게 확장되는지 평가했습니다. 결과에 따르면 MAT-2 합성 속도는 긴 발화에서 FS2와 경쟁하게 되고 MAT-4도 그 뒤를 따릅니다. 이에 가장 큰 기여를 한 것은 새로운 아키텍처인 듯합니다(GRAD-4와 GCFM-4는 모두 훨씬 느리기 때문). MAT와 GRAD의 격차는 발화가 길어질수록 커질 뿐입니다. 5.
--- CONCLUSION ---
S 및 향후 작업 조건부 흐름 매칭을 사용하여 훈련된 빠르고 확률적이며 고품질의 ODE 기반 TTS 음향 모델인 Matcha-TTS를 소개했습니다. 이 접근 방식은 비자기회귀적이고 메모리 효율적이며 말하고 정렬하는 방법을 함께 학습합니다. 세 가지 강력한 사전 훈련된 기준선과 비교할 때 Matcha-TTS는 뛰어난 음성 자연성을 제공하며 긴 발화에서 가장 빠른 모델의 속도와 일치할 수 있습니다. 실험 결과 새로운 아키텍처와 새로운 훈련이 이러한 개선에 기여한다는 것을 보여줍니다. 매력적인 향후 작업으로는 모델을 여러 화자로 만들고 확률적 지속 시간 모델링을 추가하고 자발적인 말과 같은 어렵고 다양한 데이터에 적용하는 것이 있습니다[42]. 6. 참고문헌 [1] Y. Song 및 S. Ermon, &quot;데이터 분포의 기울기를 추정하여 생성 모델링,&quot; Proc. NeurIPS, 2019. [2] P. Dhariwal 및 A. Nichol, &quot;확산 모델이 이미지 합성에서 GANS를 이김,&quot; Proc. NeurIPS, 2021, pp. 8780-8794. 8 https://huggingface.co/gpt[3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, Proc. CVPR, 2022, pp. 10684–10695. [4] S. Alexanderson, R. Nagy, J. Beskow 및 GE Henter, &quot;듣기, 잡음 제거, 동작! 확산 모델을 사용한 오디오 기반 모션 합성,&quot; ACM ToG, vol. 42, 아니. 4, 2023, 기사 44. [5] S. Mehta, S. Wang, S. Alexanderson, J. Beskow, É. Székely 및 GE Henter, &quot;Diff-TTSG: 확률적 통합 음성 및 제스처 합성의 잡음 제거&quot;, Proc. SSW, 2023. [6] N. Chen, Y. Zhang, H. Zen, RJ Weiss, M. Norouzi 및 W. Chan, &quot;WaveGrad: 파형 생성을 위한 그래디언트 추정&quot;, Proc. ICLR, 2021. [7] N. Chen, Y. Zhang, H. Zen, RJ Weiss, M. Norouzi, N. Dehak, W. Chan, &quot;WaveGrad 2: 텍스트-음성 합성을 위한 반복적 개선&quot;, Proc. Interspeech, 2021, 3765-3769쪽. [8] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, M. Kudinov, &quot;Grad-TTS: 텍스트-음성을 위한 확산 확률적 모델&quot;, Proc. ICML, 2021, 8599-8608쪽. [9] M. Jeong, H. Kim, SJ Cheon, BJ Choi, NS Kim, &quot;Diff-TTS: 텍스트-음성을 위한 잡음 제거 확산 모델&quot;, Proc. Interspeech, 2021, 3605-3609쪽. [10] Z. Kong, W. Ping, J. Huang, K. Zhao, B. Catanzaro, &quot;DiffWave: 오디오 합성을 위한 다재다능한 확산 모델&quot;, Proc. ICLR, 2021년. [11] Y. Song, J. Sohl-Dickstein, DP Kingma, A. Kumar, S. Ermon, B. Poole, &quot;확률적 미분 방정식을 통한 점수 기반 생성 모델링&quot;, Proc. ICLR, 2021년. [12] MS Albergo 및 E. Vanden-Eijnden, &quot;확률적 보간을 사용한 정규화 흐름 구축&quot;, Proc. ICLR, 2022년. [13] RTQ Chen, Y. Rubanova, J. Bettencourt 등, &quot;신경 상미분 방정식&quot;, Proc. 영어: NeurIPS, 2018. [14] Y. Lipman, RTQ Chen, H. Ben-Hamu et al., &quot;생성 모델링을 위한 흐름 매칭&quot;, Proc. ICLR, 2023. [15] I. Vovk, T. Sadekova, V. Gogoryan, V. Popov, M. Kudinov, and J. Wei, &quot;Fast Grad-TTS: CPU에서 효율적인 확산 기반 음성 생성을 향해&quot;, Proc. Interspeech, 2022. [16] J. Betker, &quot;스케일링을 통한 더 나은 음성 합성&quot;, arXiv 사전 인쇄본 arXiv:2305.07243, 2023. [17] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, &quot;FastSpeech: 빠르고 견고하며 제어 가능한 텍스트 음성 변환,&quot; Proc. NeurIPS, 2019. [18] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, &quot;FastSpeech 2: 빠르고 고품질의 엔드투엔드 텍스트 음성 변환,&quot; Proc. ICLR, 2021. [19] O. Press, NA Smith, and M. Lewis, &quot;짧게 학습하고 길게 테스트: 선형 편향이 있는 주의가 입력 길이 외삽을 가능하게 함,&quot; Proc. ICLR, 2022. [20] J. Kim, S. Kim, J. Kong, and S. Yoon, &quot;Glow-TTS: 단조 정렬 검색을 통한 텍스트 음성 변환을 위한 생성 흐름,&quot; Proc. NeurIPS, 2020, 8067-8077쪽. [21] J. Kim, J. Kong 및 J. Son, &quot;VITS: 종단간 텍스트-음성을 위한 적대적 학습을 갖춘 조건부 변분 자동 인코더&quot;, Proc. ICML, 2021, 5530-5540쪽. [22] P. Shaw, J. Uszkoreit 및 A. Vaswani, &quot;상대적 위치 표현을 갖춘 자기 주의&quot;, Proc. NAACL, 2018. [23] H. Zhang, Z. Huang, Z. Shang, P. Zhang, 및 Y. Yan, &quot;LinearSpeech: 선형 복잡도를 갖춘 병렬 텍스트-음성 변환&quot;, Proc. Interspeech, 2021, pp. 4129-4133. [24] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, 및 Y. Liu, &quot;RoFormer: 회전 위치 임베딩을 갖춘 향상된 변압기&quot;, arXiv 사전 인쇄본 arXiv:2104.09864, 2021. [25] U. Wennberg 및 GE Henter, &quot;변환기 기반 언어 모델에서 번역 불변 자기 주의의 경우&quot;, Proc. ACL-IJCNLP Vol. 2, 2021, pp. 130-140. [26] S. Mehta, A. Kirkland, H. Lameris, J. Beskow, É. Székely, GE Henter, &quot;OverFlow: 더 나은 TTS를 위해 신경 변환기 위에 흐름을 놓기&quot;, Proc. Interspeech, 2023. [27] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, Y. Ren, &quot;ProDiff: 고품질 텍스트 음성 변환을 위한 점진적 고속 확산 모델&quot;, Proc. MM, 2022, 2595-2605쪽. [28] O. Watts, GE Henter, J. Fong, C. Valentini-Botinhao, &quot;시퀀스 대 시퀀스 신경 TTS의 개선은 어디에서 오는가?&quot; in Proc. SSW, 2019, pp. 217-222. [29] S. Mehta, É. Székely, J. Beskow, and GE Henter, &quot;Neural HMMs are all you need (for-quality attention-free TTS)&quot; in Proc. ICASSP, 2022, pp. 7457-7461. [30] S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, &quot;BigVGAN: 대규모 학습이 가능한 범용 신경 보코더&quot; in Proc. ICLR, 2023. [31] X. Liu et al., &quot;직선적이고 빠른 흐름: 정류된 흐름으로 데이터를 생성하고 전송하는 방법 학습&quot; in Proc. ICLR, 2023. [32] Z. Ye, W. Xue, X. Tan, J. Chen, Q. Liu 및 Y. Guo, &quot;CoMoSpeech: 일관성 모델을 통한 단계적 음성 및 노래 음성 합성&quot;, Proc. ACM MM, 2023, pp. 1831–1839. [33] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz 외, &quot;Voicebox: 대규모 텍스트 안내 다국어 범용 음성 생성,&quot; arXiv 사전 인쇄본 arXiv:2306.15687, 2023. [34] M. Bernard 및 H. Titeux, &quot;Phonemizer: Python에서 여러 언어를 위한 텍스트에서 음소로 변환,&quot; J. Open Source Softw., vol. 6, no. 68, p. 3958, 2021. [35] J. Kong, J. Kim 및 J. Bae, &quot;HiFi-GAN: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대 네트워크,&quot; Proc. 영어: NeurIPS, 2020, pp. 17022-17 033. [36] R. Prenger, R. Valle 및 B. Catanzaro, &quot;WaveGlow: 음성 합성을 위한 흐름 기반 생성 네트워크&quot;, Proc. ICASSP, 2019, pp. 3617-3621. [37] A. Radford, JW Kim, T. Xu, G. Brockman, C. Mcleavey 및 I. Sutskever, &quot;대규모 약한 감독을 통한 강력한 음성 인식&quot;, Proc. ICML, 2023, pp. 28 492–28 518. [38] J. Taylor 및 K. Richmond, &quot;ASR 기반 TTS 평가를 위한 신뢰 구간&quot;, Proc. Interspeech, 2021. [39] K. Prahallad, A. Vadapalli, N. Elluru, G. Mantena, B. Pulugundla 외, &quot;Blizzard Challenge 2013 – 인도어 과제&quot;, Proc. Blizzard Challenge Workshop, 2013. [40] C.-H. Chiang, W.-P. Huang 및 H. yi Lee, &quot;TTS에 대한 주관적 평가의 세부 사항을 보다 엄격하게 보고해야 하는 이유&quot;, Proc. Interspeech, 2023, pp. 5551-5555. [41] A. Kirkland, S. Mehta, H. Lameris, GE Henter, E. Szekely et al., &quot;MOS 구덩이에 갇힘: TTS 평가에서 MOS 테스트 방법론에 대한 중요한 분석&quot;, Proc. SSW, 2023. [42] É. Székely, GE Henter, J. Beskow 및 J. Gustafson, &quot;발견된 데이터에서 나온 자발적 대화 음성 합성&quot;, Proc. Interspeech, 2019, pp. 4435-4439.
