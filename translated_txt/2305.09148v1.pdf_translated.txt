--- ABSTRACT ---
최근 연구에 따르면 문장 수준 번역 순위 작업으로 학습된 듀얼 인코더 모델이 교차 언어 문장 임베딩에 효과적인 방법인 것으로 나타났습니다. 그러나 저희의 연구에 따르면 토큰 수준 정렬도 다국어 시나리오에서 매우 중요한데, 이는 이전에 완전히 탐구되지 않았습니다. 저희의 연구 결과를 바탕으로 문장 수준과 토큰 수준 정렬을 모두 통합하는 교차 언어 문장 임베딩을 위한 듀얼 정렬 사전 학습(DAP) 프레임워크를 제안합니다. 이를 달성하기 위해 저희는 모델이 일방적 맥락화된 토큰 표현을 사용하여 번역 대응물을 재구성하는 방법을 학습하는 새로운 표현 번역 학습(RTL) 작업을 도입합니다. 이 재구성 목표는 모델이 토큰 표현에 번역 정보를 임베딩하도록 장려합니다. 번역 언어 모델링과 같은 다른 토큰 수준 정렬 방법에 비해 RTL은 듀얼 인코더 아키텍처에 더 적합하고 계산적으로 효율적입니다. 세 가지 문장 수준 교차 언어 벤치마크에 대한 광범위한 실험은 저희의 접근 방식이 문장 임베딩을 크게 개선할 수 있음을 보여줍니다. 저희의 코드는 https://github.com/ChillingDream/DAP에서 사용할 수 있습니다. 1
--- INTRODUCTION ---
교차 언어 문장 임베딩은 교차 언어 문장 검색(Artetxe 및 Schwenk, 2019b) 및 교차 언어 자연어 추론(Conneau et al., 2018)을 포함한 다양한 자연어 처리(NLP) 작업을 위해 다국어 텍스트를 단일 통합 벡터 공간으로 인코딩합니다. 텍스트 시퀀스는 효율적으로 검색하여 밀집 표현 간의 내적을 사용하여 비교할 수 있습니다. 문장 임베딩 작업은 이제 사전 훈련된 언어 모델(Devlin * Microsoft에서 인턴십하는 동안 수행한 작업. * 해당 저자. 0.0.(a) 문장 정렬. 0.0.0.(b) 이중 정렬. 그림 1: 아랍어와 영어의 Tatoeba 문장 쌍의 토큰 표현 시각화. 고차원 벡터는 주성분 분석을 통해 2D 공간에 투영됩니다. 다국어 BERT에서 미세 조정된 두 모델의 결과를 보여줍니다. 그림 1(a)에 표시된 모델은 번역 순위 작업으로만 미세 조정되어 큰 정렬 오류가 발생합니다. 이러한 정렬 오류는 1(b)에 표시된 것처럼 제안된 RTL 방법으로 효과적으로 제거할 수 있습니다. et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020b,a). 사전 훈련된 모델의 CLS 토큰을 미세 조정하여 입력 텍스트 시퀀스를 단일 벡터 표현으로 인코딩합니다.최근 연구에 따르면 이중 사전 훈련된 인코더와 함께 번역 순위 작업을 사용하면 더 우수한 문장 임베딩이 가능함을 보여주었습니다(Yang et al., 2019; Chidambaram et al., 2019; Yang et al., 2021; Chi et al., 2021; Feng et al., 2022).CLS 토큰을 미세 조정하는 목적은 문장 수준 정렬을 학습하고 전체 문장의 정보를 CLS 토큰으로 압축하는 것입니다.이 방법을 사용하면 의미적으로 관련된 문장의 CLS 토큰이 더 큰 내적을 갖게 됩니다.그러나 다국어 시나리오에서 토큰 수준 정렬도 중요하며 교차 언어 문장 임베딩에서 미세 조정 정렬 작업은 아직 완전히 탐구되지 않았습니다.그림 1에서 볼 수 있듯이 두 개의 병렬 코퍼스 간의 토큰 표현 유사성을 시각화합니다. CLS 토큰과 관련된 목적에 대한 학습은 토큰 표현이 임베딩 공간에 분산되도록 합니다. 우리의 관찰을 바탕으로, 우리는 교차 언어 문장 임베딩을 위한 효율적인 이중 정렬 사전 학습(DAP) 프레임워크를 제안합니다. 임베딩 모델은 문장 수준 정렬과 토큰 수준 정렬 모두에 대해 학습됩니다. 이전의 교차 언어 사전 학습 연구(Chi et al., 2021; Feng et al., 2022)는 토큰 정렬을 달성하기 위해 번역 언어 모델링(TLM)을 사용합니다. 이 논문에서는 변환기 모델을 사용하여 병렬 비영어 문장의 토큰 표현을 기반으로 전체 영어 입력을 재구성하는 새로운 표현 번역 학습(RTL) 방법을 소개합니다. RTL 목적을 최적화함으로써, 모델은 영어 문장의 정보를 비영어 대응물의 표현에 임베딩하는 방법을 학습합니다. TLM과 달리, RTL을 계산하는 데는 단측 자체 맥락화된 표현만 필요하며 추가 피드포워드 전파가 필요하지 않습니다. 우리는 공개 코퍼스에서 모델을 훈련하고 세 가지 교차 언어 작업(바이텍스트 검색, 바이텍스트 마이닝, 교차 언어 자연어 추론)에서 평가합니다. 우리의 결과는 DAP가 교차 언어 문장 임베딩을 효과적으로 개선할 수 있음을 보여줍니다. 우리의 기여는 다음과 같이 요약됩니다. • 우리는 문장 수준 작업을 위한 새로운 교차 언어 사전 훈련 프레임워크 DAP를 제안하여 표현 변환 학습을 통해 문장 수준과 토큰 수준 정렬을 모두 달성하는데, 이는 이중 인코더에 더 적합하고 이전 정렬 방법에 비해 계산적으로 효율적입니다. • 세 가지 교차 언어 작업에 대한 광범위한 실험은 DAP가 문장 임베딩을 상당히 개선한다는 것을 보여줍니다. • 우리는 중간 크기의 데이터 세트에서 모델을 훈련하고 그 성능이 대규모 최첨단 사전 훈련 모델과 유사하다는 것을 발견했습니다. 2
--- RELATED WORK ---
2.1 교차 언어 사전 훈련 영어용 BERT(Devlin et al., 2019)의 성공에 따라, 다중 언어 BERT는 공유된 다중 언어 어휘를 구축하고 마스크 언어 모델링(MLM) 목표로 여러 단일 언어 코퍼스를 사용하여 훈련함으로써 나왔습니다.XLM(Conneau and Lample, 2019)은 MLM을 이중 텍스트 코퍼스로 확장한 번역 언어 모델링(TLM) 작업을 제안하여 모델이 번역 쌍에서 교차 언어 정렬을 학습할 수 있도록 합니다.Unicoder(Huang et al., 2019)는 모델이 더 많은 관점에서 교차 언어 정보를 포착할 수 있도록 세 가지 이중 텍스트 사전 훈련 작업을 도입합니다.XLM-R(Conneau et al., 2020a)은 단일 언어 데이터의 양과 훈련 시간을 늘립니다.이들은 병렬 코퍼스를 사용하지 않고도 이전 작업보다 더 나은 성능을 달성합니다.2. 문장 임베딩 이중 인코더 아키텍처는 Guo et al.에 의해 처음 제안되었습니다. (2018). 그들은 각각 소스 및 타겟 문장을 통합 임베딩 공간에 인코딩하고 내적을 사용하여 유사도 점수를 계산합니다. 모델은 번역 순위 작업에서 학습되어 부정 예제보다 번역 쌍에 대한 모델 점수가 높아집니다. Yang et al. (2019)은 부정 쌍 간의 거리를 더욱 확대하는 가산 마진 소프트맥스로 듀얼 인코더를 향상시킵니다. 가산 마진 소프트맥스를 기반으로 LaBSE(Feng et al., 2022)는 번역 순위 작업과 MLM 작업 및 TLM 작업을 결합하여 더 큰 코퍼스에서 학습합니다. InfoXLM(Chi et al., 2021)은 통합 정보 이론적 프레임워크에서 교차 언어 사전 학습에 사용된 MLM, TLM 및 번역 순위 작업을 해석하여 문장 수준의 상호 정보를 극대화하기 위한 교차 언어 대조 학습을 제안합니다. 3
--- METHOD ---
s는 교차 언어 문장 임베딩을 위한 것입니다. 그러나 저희의 연구에 따르면 토큰 수준 정렬은 다국어 시나리오에서도 매우 중요한데, 이는 이전에 완전히 탐구되지 않았습니다. 저희의 연구 결과를 바탕으로 저희는 문장 수준과 토큰 수준 정렬을 모두 통합하는 교차 언어 문장 임베딩을 위한 이중 정렬 사전 학습(DAP) 프레임워크를 제안합니다. 이를 달성하기 위해 저희는 모델이 일방적 맥락화된 토큰 표현을 사용하여 번역 대응물을 재구성하는 방법을 학습하는 새로운 표현 번역 학습(RTL) 과제를 도입합니다. 이 재구성 목표는 모델이 토큰 표현에 번역 정보를 임베딩하도록 장려합니다. 번역 언어 모델링과 같은 다른 토큰 수준 정렬 방법과 비교할 때 RTL은 이중 인코더 아키텍처에 더 적합하고 계산적으로 효율적입니다. 광범위한
--- EXPERIMENT ---
3개의 문장 수준 교차 언어 벤치마크에서 우리의 접근 방식이 문장 임베딩을 상당히 개선할 수 있음을 보여줍니다. 우리의 코드는 https://github.com/ChillingDream/DAP에서 제공됩니다. 1 서론 교차 언어 문장 임베딩은 교차 언어 문장 검색(Artetxe 및 Schwenk, 2019b) 및 교차 언어 자연어 추론(Conneau et al., 2018)을 포함한 다양한 자연어 처리(NLP) 작업을 위해 다국어 텍스트를 단일 통합 벡터 공간으로 인코딩합니다. 텍스트 시퀀스는 효율적으로 검색하여 밀집 표현 간의 내적을 사용하여 비교할 수 있습니다. 문장 임베딩 작업은 이제 사전 훈련된 언어 모델(Devlin * Microsoft에서 인턴십하는 동안 수행한 작업. * 해당 저자. 0.0.(a) 문장 정렬. 0.0.0.(b) 이중 정렬. 그림 1: 아랍어와 영어의 Tatoeba 문장 쌍의 토큰 표현 시각화. 고차원 벡터는 주성분 분석을 통해 2D 공간에 투영됩니다. 다국어 BERT에서 미세 조정된 두 모델의 결과를 보여줍니다. 그림 1(a)에 표시된 모델은 번역 순위 작업으로만 미세 조정되어 큰 정렬 오류가 발생합니다. 이러한 정렬 오류는 1(b)에 표시된 것처럼 제안된 RTL 방법으로 효과적으로 제거할 수 있습니다. et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020b,a). 사전 훈련된 모델의 CLS 토큰을 미세 조정하여 입력 텍스트 시퀀스를 단일 벡터 표현으로 인코딩합니다.최근 연구에 따르면 이중 사전 훈련된 인코더와 함께 번역 순위 작업을 사용하면 더 우수한 문장 임베딩이 가능함을 보여주었습니다(Yang et al., 2019; Chidambaram et al., 2019; Yang et al., 2021; Chi et al., 2021; Feng et al., 2022).CLS 토큰을 미세 조정하는 목적은 문장 수준 정렬을 학습하고 전체 문장의 정보를 CLS 토큰으로 압축하는 것입니다.이 방법을 사용하면 의미적으로 관련된 문장의 CLS 토큰이 더 큰 내적을 갖게 됩니다.그러나 다국어 시나리오에서 토큰 수준 정렬도 중요하며 교차 언어 문장 임베딩에서 미세 조정 정렬 작업은 아직 완전히 탐구되지 않았습니다.그림 1에서 볼 수 있듯이 두 개의 병렬 코퍼스 간의 토큰 표현 유사성을 시각화합니다. CLS 토큰과 관련된 목적에 대한 훈련은 토큰 표현이 임베딩 공간에 분산되도록 합니다. 우리의 관찰을 바탕으로, 우리는 교차 언어 문장 임베딩을 위한 효율적인 이중 정렬 사전 훈련(DAP) 프레임워크를 제안합니다. 임베딩 모델은 문장 수준 정렬과 토큰 수준 정렬 모두에 대해 훈련됩니다. 이전의 교차 언어 사전 훈련 연구(Chi et al., 2021; Feng et al., 2022)는 토큰 정렬을 달성하기 위해 번역 언어 모델링(TLM)을 사용합니다. 이 논문에서는 변환기 모델을 사용하여 병렬 비영어 문장의 토큰 표현을 기반으로 전체 영어 입력을 재구성하는 새로운 표현 번역 학습(RTL) 방법을 소개합니다. RTL 목적을 최적화함으로써, 모델은 영어 문장의 정보를 비영어 대응물의 표현에 임베딩하는 방법을 학습합니다. TLM과 달리, RTL을 계산하는 데는 단측 자체 맥락화된 표현만 필요하며 추가 피드포워드 전파가 필요하지 않습니다. 우리는 공개 코퍼스에서 모델을 훈련하고 세 가지 교차 언어 작업(바이텍스트 검색, 바이텍스트 마이닝, 교차 언어 자연어 추론)에서 평가합니다. 우리의 결과는 DAP가 교차 언어 문장 임베딩을 효과적으로 개선할 수 있음을 보여줍니다. 우리의 기여는 다음과 같이 요약됩니다. • 우리는 문장 수준 작업을 위한 새로운 교차 언어 사전 훈련 프레임워크 DAP를 제안하여 표현 변환 학습을 통해 문장 수준과 토큰 수준 정렬을 모두 달성하는데, 이는 이중 인코더에 더 적합하고 이전 정렬 방법에 비해 계산적으로 효율적입니다. • 세 가지 교차 언어 작업에 대한 광범위한 실험은 DAP가 문장 임베딩을 상당히 개선한다는 것을 보여줍니다. • 우리는 중간 크기의 데이터 세트에서 모델을 훈련하고 그 성능이 대규모 최첨단 사전 훈련된 모델의 성능과 유사하다는 것을 발견했습니다. 2 관련 연구 2.1 교차 언어 사전 훈련 영어용 BERT(Devlin et al., 2019)의 성공에 따라, 다중 언어 BERT는 공유된 다중 언어 어휘를 구축하고 마스크 언어 모델링(MLM) 목표로 여러 단일 언어 코퍼스를 사용하여 훈련함으로써 나왔습니다.XLM(Conneau and Lample, 2019)은 MLM을 이중 텍스트 코퍼스로 확장한 번역 언어 모델링(TLM) 작업을 제안하여 모델이 번역 쌍에서 교차 언어 정렬을 학습할 수 있도록 합니다.Unicoder(Huang et al., 2019)는 모델이 더 많은 관점에서 교차 언어 정보를 포착할 수 있도록 세 가지 이중 텍스트 사전 훈련 작업을 도입합니다.XLM-R(Conneau et al., 2020a)은 단일 언어 데이터의 양과 훈련 시간을 늘립니다.이들은 병렬 코퍼스를 사용하지 않고도 이전 연구보다 더 나은 성능을 달성합니다.2. 문장 임베딩 이중 인코더 아키텍처는 Guo et al.에 의해 처음 제안되었습니다. (2018). 그들은 소스 및 타겟 문장을 각각 통합 임베딩 공간에 인코딩하고 내적을 사용하여 유사도 점수를 계산합니다. 모델은 번역 순위 작업에서 학습되어 부정 예제보다 번역 쌍에 대한 모델 점수가 높아집니다. Yang et al. (2019)은 부정 쌍 간의 거리를 더욱 확대하는 가산 마진 소프트맥스로 듀얼 인코더를 향상시킵니다. 가산 마진 소프트맥스를 기반으로 LaBSE(Feng et al., 2022)는 번역 순위 작업과 MLM 작업 및 TLM 작업을 결합하여 더 큰 코퍼스에서 학습합니다. InfoXLM(Chi et al., 2021)은 통합 정보 이론적 프레임워크에서 교차 언어 사전 학습에 사용된 MLM, TLM 및 번역 순위 작업을 해석하여 문장 수준의 상호 정보를 최대화하기 위한 교차 언어 대조 학습을 제안합니다. 3 방법 3.1 예비 변압기 인코더 변압기 인코더는 현대 언어 모델에서 널리 채택되었습니다(Vaswani et al., 2017; Devlin et al., 2019; Conneau and Lample, 2019). 이는 임베딩 계층과 셀프 어텐션 모듈이 있는 L개의 스택된 변압기 블록으로 구성됩니다. 각 입력 토큰 x¿는 초기 은닉 벡터 ho로 벡터 공간에 인코딩됩니다. 그런 다음 각 변압기 블록에서 i번째 토큰 h½의 은닉 벡터는 이전 계층에서 출력된 모든 은닉 벡터의 셀프 어텐션 융합에서 계산됩니다. h= (h₁, h₁₂,,h&#39;s) = ƒ¹ (h²¯¹). (1) 마지막으로 맥락화된 토큰 표현 f(x) = ƒ¹(ƒL−¹(…… · ƒ¹ (h°º))). 교차 언어 사전 학습 마스크 언어 모델링(MLM)(Devlin et al., 2019) 및 TranslaRTL 헤드 g(사전 학습 전용) Dorina lebt LRTL LRTL LRTL LTR Dorina is alive [MASK] [MASK] [MASK] [CLS] Dorina lebt [CLS] Dorina is alive 공유 변환기 매개변수 인코더 f [CLS] Dorina lebt [CLS] Dorina is alive [CLS] Dorina lebt [CLS] Dorina is alive [CLS] Dorina lebt [CLS] Dorina is alive 그림 2: 듀얼 정렬 사전 학습 프레임워크의 워크플로. 공유 12계층 변환기 인코더로 듀얼 인코더 방식으로 바이텍스트 쌍을 인코딩하고 각각 문장 표현 및 토큰 표현을 사용하여 번역 순위 손실 및 표현 번역 손실을 계산합니다. 이중 정렬 언어 모델링(TLM)(Conneau 및 Lample, 2019)은 교차 언어 사전 학습을 위한 두 가지 일반적인 작업입니다. MLM은 단일 언어 코퍼스에서 수행됩니다. 임의로 선택된 입력 토큰의 하위 집합은 특수 [MASK] 토큰이나 다른 임의 토큰으로 대체되고, 모델은 컨텍스트에 따라 이러한 손상된 토큰을 복구하는 방법을 학습합니다. TLM은 다음과 같은 목적으로 MLM을 교차 언어 시나리오로 확장합니다. LTLM(x, y) = l (x + y, ƒ (m(x) Ð m(y))), (2) 여기서 는 시퀀스 연결 연산자를 나타내고 m은 요소별 임의 대체를 나타냅니다. 모델은 학습하는 동안 번역에서 마스크되지 않은 토큰을 사용하여 마스크된 토큰을 예측할 수 있습니다. 이런 식으로 모델은 병렬 코퍼스를 사용하여 교차 언어 토큰 수준 정렬을 학습합니다. 그러나 TLM은 소스 및 대상 문장의 토큰이 중간 계층에서 상호 액세스할 수 있는 교차 인코더 아키텍처를 위해 설계되었습니다. 결과적으로 TLM으로 학습된 모델은 문장이 독립적으로 인코딩되는 추론 단계에서는 사용할 수 없는 이 정보 교환에 의존할 수 있습니다. 또한 TLM을 계산하려면 연결된 문장 쌍을 입력하는 추가 피드포워드 전파가 필요하여 학습 비용이 증가합니다. 제안하는 표현 변환 학습 과제는 두 가지 약점을 모두 극복할 수 있습니다. 3.2 모델 구조 듀얼 정렬 사전 학습 프레임워크는 듀얼 인코더 모델 f와 표현 변환 학습(RTL) 헤드 g의 두 가지 변환기 모델을 포함합니다. 인코더 모델의 경우 12개 레이어의 변환기 인코더 블록, 12개 어텐션 헤드, 768차원 숨겨진 상태를 갖춘 가장 인기 있는 BERT 아키텍처를 채택합니다. Devlin et al. (2019)에 따라 특수 토큰 [CLS]를 입력에 추가합니다. f(x) = f([CLS],x1,...,xs). (3) CLS 토큰의 숨겨진 벡터를 전체 문장 fs(x)의 표현으로 사용합니다. 다른 다국어 언어 모델과 마찬가지로, 우리 모델은 언어에 구애받지 않습니다. 즉, 모든 언어가 동일한 단일 변환기를 공유합니다. RTL 헤드는 맨 위에 어휘 예측 헤드가 있는 K개의 변환기 인코더 블록의 스택입니다. RTL 헤드의 기능은 소스 문장 h²의 토큰 표현에서 번역 문장 y를 재구성하는 것입니다(본 논문에서 소스 문장은 영어가 아닌 문장을 나타냄): K Kg(h, y) = x (WTgk (gk−1 (….. gº (h,y)))), gº(h,y)=(h,,hs, [MASK],..., [MASK]), × Sy (4) 여기서 소프트맥스 함수이고 W는 어휘 예측 헤드의 가중치 행렬입니다. 실험에서 K =인 작은 RTL 헤드가 일반적으로 가장 좋은 성능을 발휘한다는 것을 발견했습니다. 3.3 사전 학습 작업 문장 수준과 토큰 수준 정렬을 모두 달성하기 위해 번역 순위 작업과 표현 번역 학습 작업으로 구성된 사전 학습 프레임워크를 설계합니다. 이 두 목표는 학습 중에 동시에 활용됩니다. 전체 절차는 그림 2에 나와 있습니다.3.3.번역 순위 번역 순위(TR) 작업으로 학습된 듀얼 인코더 모델은 교차 언어 임베딩을 학습하는 데 효과적인 것으로 입증되었습니다(Yang 등, 2019; Feng 등, 2022; Chi 등, 2021).이러한 모델은 병렬 문장의 임베딩 쌍의 유사성과 일치하지 않는 쌍의 비유사성을 최대화하는 방법을 학습합니다.따라서 내적을 순위 지표로 사용하는 검색 및 마이닝 작업을 해결하는 데 적합합니다.다음과 같이 학습 작업을 공식화합니다(Feng 등, 2022).NLTR N Σ10g i=ep(xi,Yi) &#39;Σ³ ³ e(xi,j)&#39; (5) 여기서 B는 배치 크기이고 (x, y)는 일반적으로 fs(x) f¸(y)인 각 텍스트의 표현의 유사성으로 정의됩니다. 이 논문에서 우리는 CLS 토큰의 은닉 벡터를 사용하여 문장을 표현한다.3.3.2 표현 번역 학습 LTR을 최소화하는 것은 본질적으로 상호 정보 I(x; y)의 하한을 최대화한다(Oord et al., 2018; Chi et al., 2021).그러나 모델이 문장의 모든 정보를 완벽하게 포함하는 임베딩을 찾는 것은 어렵다.결과적으로, 모델은 고수준 글로벌 정보에만 주의를 기울이고 일부 로컬 토큰 수준 정보를 무시할 수 있다.이를 위해 우리는 보조 손실을 추가하여 모델이 전체 모델에서 토큰 수준 정보를 보존하도록 한다.SLRTL = Σ CE(g(f*(x), Y)i, Yi), (6) i=여기서 f(x)는 CLS를 제외한 x의 모든 은닉 벡터를 나타내고 CE는 교차 엔트로피를 나타낸다.우리는 RTL 목적을 계산할 때 CLS 토큰을 포함하지 않는다는 점에 주목할 가치가 있는데, 이는 그것이 번역 순위 목적을 수렴하기 어렵게 만들기 때문이다. 안정적이고 일관된 목표로 RTL 헤드를 훈련하기 위해 재구성 방향은 항상 비영어 문장에서 영어 번역으로 향합니다.번역 순위 목표와 결합하면 최종 손실이 발생합니다.LDAP = LTR + CRTL (7) RTL은 추가 피드포워드 전파가 필요하지 않으므로 RTL은 약간의 계산만 도입하고 사전 훈련 속도를 크게 늦추지 않습니다.시간이 많이 걸리는 유일한 연산은 방대한 어휘에 대한 소프트맥스로, 부정 샘플링 및 계층적 소프트맥스(실험에서는 사용하지 않음)와 같은 기술을 통해 더욱 완화할 수 있습니다.4 실험 이 섹션에서는 먼저 훈련 설정을 설명합니다.그런 다음 세 가지 문장 수준 교차 언어 작업에 대한 이전 작업과 방법을 비교합니다. 4.1 사전 학습 데이터 Artetxe와 Schwenk(2019b)를 따라 OPUS 웹사이트(Tiedemann, 2012)에서 다운로드한 Europarl, United Nations Parallel Corpus, OpenSubtitles, Tanzil, CCMatrix 및 WikiMatrix 코퍼스를 결합하여 36개 언어(XTREME Tatoeba 벤치마크에서 사용됨)에 대한 병렬 학습 데이터를 수집합니다. 3.3절에서 언급했듯이 다른 모든 언어를 영어에 맞춰서 영어를 포함하는 병렬 코퍼스만 수집합니다. 영어가 아닌 각 언어에 대해 무작위로 최대 100만 개의 문장 쌍을 보관합니다. 전체 데이터 세트는 5.7GB 데이터로 일반적인 대규모 사전 학습(Feng et al., 2022; Chi et al., 2021)보다 훨씬 적지만, 저희의 방법은 여전히 최첨단 기술과 비슷한 성능을 달성합니다. 4.2 구현 세부 사항 Huggingface 모델 허브에 게시된 체크포인트를 사용하여 각각 다국어 BERT 기반 또는 XLM-R 기반에서 인코더 모델을 초기화하고, 해당 인코더 모델에 의해 마지막 K 트랜스포머 계층에서 K 계층 RTL 헤드를 초기화합니다. 최대 문장 길이는 32개 토큰으로 제한되며, 32개 토큰보다 긴 문장은 잘립니다. Tesla V100 GPU에서 1일 동안 5e-5의 학습 속도와 1024의 총 배치 크기를 사용하여 AdamW 옵티마이저를 사용하여 100,단계 동안 모델을 학습합니다. 보고된 결과는 세 가지 다른 시드의 평균입니다. 방향 xx-en en-xx 모델 14랭 28랭 36랭 14랭 28랭 36랭 InfoXLM 77.80.LaBSE 93.mBERT* 45.45.38.mBERT(재계산됨) 42.42.36.43.43.37.mBERT+TR 94.93.90.93.93.90.MBERT+TR+TLM 94.93.90.93.93.90.MBERT+DAP 94.94.90.94.94.91.XLM-R* 60.63.57.XLM-R(재계산됨) 59.60.55.57.58.53.XLM-R+TR 93.94.91.91.91.86.XLM-R+TR+TLM 93.92.89.94.94.92.XLM-R+DAP 95.94.91.95.95.92.표 1: Tatoeba 이중 텍스트 검색 작업의 평균 정확도. 방향 &quot;xx―en&quot;은 검색이 영어 코퍼스에서 수행되고 그 반대의 경우도 마찬가지임을 의미합니다. 14개 언어와 28개 언어는 모든 36개 언어의 다른 하위 집합을 의미합니다. mBERT 및 XLM-R 모델의 경우, 우리는 이전의 가장 좋은 구현(*가 있는 결과는 (Hu et al., 2020)에서 가져옴)과 재계산된 정확도를 모두 보고합니다. InfoXLM 및 LaBSE의 결과는 해당 논문에서 가져왔습니다. LaBSE의 경우 공정한 비교를 위해 mBERT 어휘를 사용하여 결과를 가져옵니다. 굵은 글씨는 해당 모델이 그룹 중에서 가장 좋은 성능을 보임을 의미합니다. 우리는 밑줄을 사용하여 모든 변형보다 성능이 뛰어난 최첨단 방법을 식별합니다.4.3 비교 모델 제안된 표현 번역 학습의 효과를 보여주기 위해 먼저 기본 모델(mBERT 또는 XLM-R)과 TR-finetuned 버전과 비교합니다.또한 TLM을 활용하는 방법의 변형도 소개합니다.더 나아가 우리의 접근 방식을 두 가지 최첨단 다국어 모델인 InfoXLM(Chi et al., 2021)과 LaBSE(Feng et al., 2022)와 비교합니다.InfoXLM과 LaBSE는 우리 방법보다 10배 더 많은 학습 데이터를 사용하고 더 큰 배치 크기로 더 오랫동안 학습한다는 점에 주목할 가치가 있습니다.4.4 이중 텍스트 검색 이중 텍스트 검색에서 소스 언어의 쿼리 문장이 주어지면 모델은 대상 언어의 문장 모음 중에서 가장 관련성이 높은 문장을 검색해야 합니다. 이전 연구(Feng et al., 2022; Chi et al., 2021; Artetxe and Schwenk, 2019b)에 따라 Tatoeba 데이터 세트를 사용하여 제로샷 방식으로 사전 학습 프레임워크를 평가합니다. Tatoeba에는 300개 이상의 언어로 된 병렬 문장이 포함되어 있으며, XTREME 벤치마크(Hu et al., 2020)의 36개 언어 버전을 사용합니다. 각 언어에는 영어와 페어링된 최대 1000개의 문장이 있습니다. 결과 36개 언어 모두에 대해 테스트를 실시하고 LASER(Artetxe and Schwenk, 2019b)에서 테스트한 14개 언어와 XTREME에서 테스트한 36개 언어에 대한 평균 정확도를 보고합니다. 그 밖에 리소스가 부족한 테스트 언어에 대한 관찰을 바탕으로 새로운 28개 언어 그룹을 설정했습니다. 원래 36개 언어 중에서 일부 희소 언어는 문장 쌍이 1000개 미만이고, 일부는 문장 쌍이 약 200개에 불과하며, 이러한 언어의 정확도는 두 검색 방향(&quot;en xx&quot;와 &quot;xx→en&quot;은 30% 이상의 차이가 있음) 사이에서 일관되지 않으며 리소스가 풍부한 다른 언어보다 상당히 낮다는 것을 관찰했습니다. 이는 작은 테스트 세트에서 얻은 결과가 더 큰 테스트 세트에서 얻은 결과만큼 신뢰할 수 없음을 나타냅니다. 따라서 모든 언어에 1000개의 테스트 쌍이 포함된 28개 언어 버전을 보고합니다. 각 언어의 검색 정확도는 부록 A에 보고되어 있습니다. 표 1에서 DAP 방법이 다른 모든 변형보다 상당히 우수한 것을 관찰했습니다. mBERT와 XLM-R은 문장 수준의 목적이 없기 때문에 가장 나쁜 성능을 보였습니다. TLM은 &quot;en→xx&quot; 방향에서 TR의 성능을 개선하지만 &quot;xx―en&quot; 방향에서는 저하되었습니다. 반면 DAP는 일관된 개선을 가져왔습니다. 두 가지 최신 방법과 비교했을 때, 우리의 방법은 InfoXLM보다 훨씬 더 나은 성과를 보였으며 LaBSE보다 약간 뒤처질 뿐이었습니다. 모델 fr-en de-en ru-en zh-en 평균 LaBSE Р RFPRFPRFPRFF 96.3 93.6 95.0 99.4 95.4 97.3 99.3 93.1 96.1 90.4 88.3 89.4 94.mBERT(재계산됨) 75.1 68.2 71.5 77.8 69.0 73.1 70.1 52.9 60.3 63.1 50.6 56.2 65.MBERT+TR mBERT+TR+TLM mBERT+DAP 96.1 90.9 93.4 98.8 94.0 96.3 98.4 89.8 93.9 96.0 93.8 94.9 94.95.6 90.9 93.2 98.3 94.0 96.1 97.0 89.7 93.2 93.9 95.7 94.8 94.95.1 94.1 94.698.1 94.7 96.4 98.6 91.4 94.9 95.7 94.2 94.9 95.XLM-R(재계산) 81.3 68.2 74.2 86.6 77.0 81.5 87.6 74.0 80.2 77.0 54.9 64.1 75.XLM-R+TR XLM-R+TR+TLM XLM-R+DAP 92.6 92.1 92.4 96.3 94.6 95.4 97.3 91.0 94.0 96.6 87.5 91.8 93.91.4 91.6 91.5 94.0 95.5 94.7 94.4 90.9 92.7 92.8 90.3 91.5 92.95.3 93.194.2 99.0 95.2 97.1 98.1 93.3 95.6 96.7 92.6 94.6 95.표 2: BUCC 훈련 세트에 대한 평가. 임계값은 최적의 F1 점수를 달성하도록 선택됩니다. 모델 LaBSE fr-en PR FPR 92.8 82.5 87.4 96.6 85.62.de-en ru-en zh-en 평균 FPRFPRFF 90.5 91.2 85.9 88.5 85.5 70.4 77.2 85.62.51.50.0 56.MBERT* MBERT+TR mBERT+TR+TLM mBERT+DAP XLM-R* mBERT(재계산) 80.1 42.1 55.2 83.7 38.2 52.5 69.1 28.9 40.8 65.8 20.2 30.9 44.93.6 75.2 83.4 97.3 77.1 86.0 91.3 77.2 83.693.0 69.7 79.7 83.92.4 75.0 82.8 96.2 78.2 86.3 90.1 77.2 83.1 90.9 75.8 82.6 83.92.1 83.4 87.6 96.2 83.6 89.5 90.1 82.4 86.1 92.5 75.7 83.3 86.67.66.73.56.7 66.XLM-R(재계산) 85.9 47.3 61.0 88.6 48.3 62.5 85.8 54.3 66.5 77.7 27.3 40.4 57.89.7 79.1 84.194.2 80.3 86.7 89.6 80.2 84.7 92.2 66.1 77.0 83.88.1 75.8 81.5 91.2 79.8 85.1 86.3 80.6 83.4 89.6 72.6 80.2 82.92.1 82.1 86.8 96.6 81.1 88.2 89.5 88.1 88.8 93.7 75.0 83.3 86.XLM-R+TR XLM-R+TR+TLM XLM-R+DAP 표 3: BUCC 테스트 세트에 대한 평가. 임계값은 훈련 세트에서 최적의 F1 점수를 달성하도록 선택됩니다. mBERT 및 XLM-R 모델의 경우, 우리는 이전의 최상의 구현(*가 있는 결과는 (Hu et al., 2020)에서 가져옴)과 재계산된 점수를 모두 보고합니다. 훈련 비용을 고려할 때, 이 결과는 DAP의 잠재력을 보여주었다고 생각합니다. 4.5 이중 텍스트 마이닝 이중 텍스트 마이닝에서 모델은 단일 언어 코퍼스 쌍에서 병렬 문장 쌍(예: 번역)을 감지해야 합니다. 우리는 BUCC 데이터 세트(Zweigenbaum et al., 2017)를 사용하여 평가를 수행하는데, 여기에는 fr-en, de-en, ru-en 및 zh-en의 네 가지 언어 쌍이 포함됩니다. 각 코퍼스에는 150k에서 1.2M개의 쌍이 없는 문장과 어떤 문장이 번역 쌍인지 알려주는 골드 라벨이 포함됩니다. Artetxe와 Schwenk(2019a)에 따라, 우리는 주어진 후보의 코사인과 양방향으로 이웃의 평균 코사인 간의 비율을 사용합니다. 학습 세트는 어떤 쌍을 선택해야 할지 결정하기 위한 최상의 임계값(Schwenk, 2018)을 학습하는 데 사용됩니다. 채점 함수와 임계값에 대한 자세한 내용은 부록 B에서 확인할 수 있습니다. 결과 표 2는 최적화 후 학습 세트에서 네 언어 쌍에 대한 정밀도, 재현율 및 F1 점수를 보여줍니다. LaBSE의 결과는 Huggingface 모델 허브에 공개된 체크포인트를 사용하여 생성됩니다. 원래 논문에서 이 작업을 평가하지 않았고 합리적인 결과를 얻지 못했기 때문에 InfoXLM의 결과는 보고하지 않습니다. 저희 방법은 모든 변형과 LaBSE보다 성능이 뛰어나며, 이는 저희 모델이 더 나은 분리성을 가진 임베딩 공간을 학습한다는 것을 의미합니다. 최적화된 모델을 테스트 세트에서 테스트할 때 저희 모델은 놀라운 일반화 능력을 보여주고 표 3에 표시된 것처럼 다른 방법에 대한 격차를 확대합니다. 저희는 최첨단 LaBSE보다 0.9%, 다른 변형보다 최소 3.0% 더 우수한 성능을 보입니다. 검색 작업과 유사하게 mBERT 및 XLM-R은 Model InfoXLM LaBSE MBERT MBERT+TR en fr es de el bg ru tr ar vi th zh hi SW ur Avg 86.4 80.3 80.9 79.3 77.8 79.3 77.6 75.6 74.2 77.1 74.6을 수행합니다. 77.0 72.2 67.5 67.3 76.85.4 80.2 80.5 78.8 78.6 80.1 77.5 75.1 75.0 76.5 69.0 75.8 71.9 71.5 68.1 76.82.1 74.4 74.9 71.2 67.9 69.5 69.6 62.8 66.2 70.6 54.6 69.7 60.4 50.9 58.0 66.8 2.0 74.3 75.1 72.9 69.9 73.1 70.6 68.6 67.4 73.6 61.3 70.8 65.0 62.6 61.0 69.mBERT+TR+TLM 82.8 75.2 74.4 72.0 69.3 70.6 69.4 66.1 66.1 70.6 58.9 67.3 63.7 60.6 59.5 68.8 1.8 75.6 76.2 74.4 72.6 74.9 72.0 71.3 69.7 74.4 63.6 72.3 67.3 67.3 63.2 71.83.8 77.6 78.2 75.4 75.0 77.0 74.8 72.7 72.0 74.5 72.1 72.9 69.6 64.2 66.0 73.83.5 76.4 76.8 75.7 74.2 76.2 74.6 71.8 71.1 74.2 69.1 72.9 68.8 66.8 65.2 73.XLM-R+TR+TLM 84.6 77.4 76.9 74.9 68.1 69.8 69.4 68.1 61.7 68.9 62.6 66.9 61.4 61.7 57.5 68.XLM-R+DAP 82.9 77.0 77.7 75.7 75.2 76.0 74.7 73.1 72.5 74.2 71.9 73.0 69.8 70.5 66.0 74.mBERT+DAP XLM-R XLM-R+TR 표 4: XNLI 언어 간 자연어 추론의 정확도. InfoXLM의 결과는 해당 논문에서 가져왔습니다. 최악입니다. TLM은 zh-en에 대한 개선을 가져오지만 fr-en에 대한 악화됩니다. DAP는 모든 메트릭에서 지속적으로 가장 우수한 성능을 보입니다. 더욱이 DAP의 성능에서 관찰된 개선은 검색 작업에 비해 더 큽니다. 이는 DAP가 복잡한 작업에서 성과를 향상시키는 데 더 효과적임을 나타내며, 어려운 문제를 해결하는 데 귀중한 도구로서의 잠재력을 시사합니다.4.6 언어 간 자연어 추론 자연어 추론(NLI)은 미세 조정 하에서 모델의 분류 성과를 평가하는 잘 알려진 작업입니다.목표는 입력 문장 쌍 간의 관계를 예측하는 것입니다.후보 관계는 함축, 모순 및 중립입니다.XNLI(Conneau et al., 2018)는 NLI를 15개 언어의 다국어 설정으로 확장합니다.Chi et al.(2021)에 따라 영어 학습 세트로 모델을 미세 조정하고 다른 언어의 테스트 세트에서 직접 평가합니다.미세 조정의 하이퍼 매개변수는 부록 C에 보고되어 있습니다.결과 표 4는 15개 언어에 대한 정확도를 보여줍니다.검색 및 마이닝 작업에 비해 변형 간의 차이가 비교적 작은 것을 관찰했습니다. 우리는 두 문장 사이의 관계를 판단하는 것이 코사인 유사도에 의존하지 않기 때문에 사전 학습을 다운스트림 작업으로 직접 전송할 수 없기 때문이라고 생각합니다.mBERT 변형은 모두 긍정적인 결과를 보였으며 DAP의 개선이 가장 컸습니다.하지만 XLM-R 변형의 경우 DAP만 기본 모델로서 성능을 유지했습니다.TR 및 TLM 변형은 성능 저하를 겪었습니다.우리는 XLM-R이 이미 잘 훈련되었기 때문이라고 생각합니다.표 5: 86.6 71.방향 Tatoeba BUCC XNLI xx-en 91.en-xx 90.5 84.1 69.둘 다 90.8 86.3 70.세 가지 작업에 걸쳐 다른 RTL 방향의 성능. &quot;xx→en&quot;은 RTL 헤드가 영어가 아닌 토큰 표현을 사용하여 영어 문장을 재구성하고 그 반대의 경우도 마찬가지임을 의미합니다. &quot;둘 다&quot;는 배치의 절반에서 각각 두 방향에서 RTL 손실을 계산하고 평균을 구함을 의미합니다. 다국어 모델과 지속적인 사전 학습은 분류 용량을 개선하기에 충분하지 않습니다. 그러나 DAP가 잘 훈련된 기본 모델의 분류 성능에 해를 끼치지 않음을 보여줍니다. 5 분석 이 섹션에서는 DAP에 대한 더 깊은 이해를 얻기 위한 실험을 수행합니다. 각 설정에서 Tatoeba에 대한 36개 언어와 두 가지 검색 방향에 대한 평균 정확도, BUCC 테스트 세트에 대한 평균 Fscore 및 XNLI에 대한 평균 정확도를 보고합니다. 모든 변형은 mBERT에서 훈련되었습니다. 5.1 번역 방향 우리 방법에서 RTL 헤드는 비영어에서 영어로만 번역하는 방법을 학습합니다. 여기서는 반대 방향이 사전 학습에 도움이 될 수 있는지 조사합니다. 재구성할 언어의 모델을 상기시키기 위해 TLM과 같이 RTL 헤드 앞의 표현에 언어 임베딩을 추가합니다. 표 5에서 볼 수 있듯이 영어에서 비영어로 번역하는 것은 반대 방향보다 성능이 훨씬 떨어집니다. 또한 혼합 훈련은 91.91.90.87.86.86.085.72.71.571.70.0.Tatoeba 0.P BUCC - XNLI 0.1.K Tatoeba BUCC -XNLI 그림 3: 세 가지 작업에 걸쳐 다양한 재구성 비율의 성능.그림 4: 세 가지 작업에 걸쳐 다양한 수의 RTL 헤드 레이어의 성능.모델 FLOPS 대기 시간 11.0G 0.중간 성능.두 방향 간의 차이는 목표의 분산에 기인합니다.RTL이 소스 언어의 표현을 대상 언어에 맞춰 조정한다고 가정합니다.따라서 재구성 대상이 여러 언어 간에 계속 전환하면 RTL이 수렴하기 어려워집니다.5.2 재구성 비율 RTL 작업의 목표를 더 잘 이해하기 위해 RTL 헤드가 다른 대상 토큰 표현이 액세스 가능한 일부 대상 문장만 재구성하면 되는 실험을 수행합니다.재구성할 토큰은 확률 p로 무작위로 선택됩니다. p가 클수록 RTL 작업이 더 어려워집니다.그림 3에서 p &lt; 1인 변형은 모든 작업에서 유사한 성능을 보이며 p = 1에서 급격한 증가가 있음을 알 수 있습니다.이는 마스크되지 않은 대상 토큰 표현이 정보 누출을 일으켜 RTL 헤드가 소스 문장에서 정렬을 학습할 필요가 없기 때문이라고 생각합니다.5.3 RTL 헤드의 복잡도 RTL 헤드의 복잡도와 사전 학습 성능 간의 관계를 조사합니다.K = 1, 2, 3, 4로 설정하여 RTL 헤드가 소스 문장의 표현에서 정렬된 정보를 추출할 수 있는 다양한 기능을 제공합니다.그림 4에서 세 가지 작업은 RTL 헤드의 복잡도와 관련하여 다른 경향을 보입니다.타토에바의 정확도만 K와 함께 계속 증가하지만 더 큰 K에서 얻는 이득은 특히 K 2 이후 감소합니다.다른 두 작업의 경우 더 큰 K는 부정적인 영향을 미칩니다. 우리는 가설 = MBERT+TR MBERT+TR+TLM 33.7G 1.MBERT+DAP 16.5G 0.표 6: 다양한 사전 학습 방법의 계산 효율성. 지연 시간 단위는 샘플당 밀리초입니다. RTL 작업을 더 어렵게 만드는 더 작은 K가 모델이 더 많은 정보 표현을 생성하도록 강제할 것이라는 것을 보여줍니다. K = 2로 설정하면 세 가지 작업에서 가장 우수한 일반적인 교차 언어 성능을 얻을 수 있습니다. 5.4 계산 효율성 계산 효율성은 사전 학습 작업을 설계할 때 중요한 요소입니다. 더 효율적인 방법을 사용하면 모델이 더 많은 단계에 대해 더 큰 데이터 세트에서 학습할 수 있습니다. 우리는 각각 방법과 TLM에 대한 피드포워드 부동 소수점 연산(FLOP)을 계산합니다. 또한 학습 환경에서의 학습 지연 시간을 보고합니다. PyTorch 분산 데이터 병렬을 사용하여 Tesla V100 GPU에서 총 배치 크기 512로 지연 시간을 측정합니다. 표 6에서 DAP는 TR 전용 기준선에 비해 훈련 비용을 약 50%만 증가시키는 것을 알 수 있는데, 이는 방대한 어휘에 대한 소프트맥스를 줄이기 위해 부정 샘플링을 사용하면 더욱 개선될 수 있습니다. 반면 TLM은 12계층 인코더를 통한 추가 피드포워드 전파로 인해 150% 이상의 훈련 비용을 도입합니다. 따라서 DAP는 교차 언어 사전 훈련에 더 효율적이고 확장 가능합니다. 6
--- CONCLUSION ---
이 논문에서 우리는 토큰 수준 정렬이 교차 언어 작업에 필수적이라는 것을 발견했습니다. 이러한 관찰을 바탕으로 우리는 문장 수준 및 토큰 수준 정렬을 모두 가능하게 하는 교차 언어 문장 임베딩을 위한 이중 정렬 사전 학습 프레임워크를 제시합니다. 이 프레임워크는 번역 순위 작업과 토큰 표현이 효율적인 방식으로 번역 대응물의 모든 정보를 포함하도록 장려하는 새로 제안된 표현 번역 학습 작업으로 구성됩니다. 우리는 중간 크기의 코퍼스에서 모델을 학습합니다. DAP로 학습한 모델은 토큰 수준 정렬이 없거나 정렬 작업으로 TLM을 사용한 세 가지 문장 수준 교차 언어 작업에서 변형보다 상당히 우수한 성과를 보였으며, 더 큰 배치 크기와 학습 단계로 10배 더 많은 데이터에서 학습한 최첨단 사전 학습 작업과 비슷한 성능을 달성했습니다. 이러한 결과는 우리의 접근 방식이 교차 언어 문장 임베딩에 필수적인 개선을 가져온다는 것을 보여줍니다. 제한 사항 우리의 방법은 효율적이고 확장 가능하지만 제한된 계산 리소스로 인해 대규모 코퍼스에서 사전 학습을 수행하지 않았습니다. 데이터의 질과 양은 사전 학습 모델에 중요한 요소입니다. 저희 모델은 36개 언어만 다루기 때문에 많은 희귀 언어에 대한 서비스를 제공할 수 없습니다. 이 논문은 단지 새로운 사전 학습 방향을 제안할 뿐이며 많은 학습 트릭을 사용하지 않습니다. DAP의 전체 기능을 탐색하는 것은 향후 작업으로 남겨둡니다. 게다가 RTL 작업이 저희 DAP 프레임워크에 대한 유일한 토큰 정렬 작업은 아닙니다. 토큰 표현을 기반으로 하는 다른 목표도 조사할 가치가 있습니다. 가장 좋은 목표 형태는 아직 연구 중입니다. 참고문헌 Mikel Artetxe 및 Holger Schwenk. 2019a. 다국어 문장 임베딩을 사용한 마진 기반 병렬 코퍼스 마이닝. 2019년 7월 28일-8월 2일 이탈리아 피렌체에서 열린 제57회 계산 언어학 협회 컨퍼런스의 회의록, ACL 2019, 제1권: 장문 논문, 3197-3203쪽. 계산 언어학 협회. MasMikel Artetxe와 Holger Schwenk. 2019b. ZeroShot Cross-Lingual Transfer and Beyond를 위한 sively Multilingual Sentence Embeddings. Association for Computational Linguistics의 거래, 7:597-610. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, Ming Zhou. 2021. InfoXLM: 교차 언어 모델 사전 학습을 위한 정보 이론적 프레임워크. Association for Computational Linguistics: Human Language Technologies의 북미 지부 2021년 회의록, 3576-3588쪽, 온라인. Association for Computational Linguistics. Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yunhsuan Sung, Brian Strope, Ray Kurzweil. 2019. 다중 작업 듀얼 인코더 모델을 통한 언어 간 문장 표현 학습. NLP를 위한 표현 학습에 대한 제4회 워크숍(RepLANLP-2019)의 회의록, 250-259쪽, 이탈리아 피렌체. 계산 언어학 협회. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. 2020a. 규모에 따른 비지도 언어 간 표현 학습. 제58회 연례 총회 의사록, 84408451페이지, 온라인. Association for Computational Linguistics. Alexis Conneau와 Guillaume Lample. 2019. Crosslingual Language Model Pretraining. Advances in Neural Information Processing Systems, 32권. Curran Associates, Inc. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov. 2018. XNLI: 교차 언어 문장 표현 평가. 2018년 자연어 처리 경험적 방법 컨퍼런스 의사록, 24752485페이지, 벨기에 브뤼셀. Association for Computational Linguistics. Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov. 2020b. 사전 학습된 언어 모델에서 나타나는 교차 언어 구조. Association for Computational Linguistics의 제58회 연례 회의록, 6022-6034쪽, 온라인. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 트랜스포머 사전 학습. Association for Computational Linguistics의 북미 지부 2019년 회의록: Human Language Technologies, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. Association for Computational Linguistics. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang. 2022. 언어에 구애받지 않는 BERT 문장 임베딩. Association for Computational Linguistics(제1권: 장문 논문)의 제60회 연례 회의록, ACL 2022, 아일랜드 더블린, 2022년 5월 22-27일, 878-891쪽. Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernández Ábrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. 2018. 이중 언어 문장 임베딩을 사용한 효과적인 병렬 코퍼스 마이닝. 제3회 기계 번역 컨퍼런스의 회의록: 연구 논문, WMT 2018, 벨기에, 브뤼셀, 2018년 10월 31일-11월 1일, 165-176쪽. Association for Computational Linguistics. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson. 2020. XTREME: 교차 언어 일반화를 평가하기 위한 대규모 다국어 멀티태스크 벤치마크. ArXiv:2003.11080 [cs]. Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Ming Zhou. 2019. Unicoder: 여러 교차 언어 작업으로 사전 학습한 범용 언어 인코더. 2019년 자연어 처리 경험적 방법에 대한 컨퍼런스 및 자연어 처리에 대한 제9회 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 2485-2494쪽, 중국 홍콩. 계산언어학 협회. Aäron van den Oord, Yazhe Li, Oriol Vinyals. 2018. 대조적 예측 코딩을 통한 표현 학습. CoRR, abs/1807.03748. ArXiv: 1807.03748. Holger Schwenk. 2018. 공동 다국어 공간에서 병렬 데이터 필터링 및 마이닝. 2018년 7월 15-20일 호주 멜버른에서 열린 ACL 2018의 제56회 연례 총회록, 2권: 단편 논문, 228-234쪽. Association for Computational Linguistics. Jorg Tiedemann. 2012. OPUS의 병렬 데이터, 도구 및 인터페이스. 제8회 언어 자원 및 평가 국제 컨퍼런스(LREC&#39;2012)의록. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc. Yinfei Yang, Gustavo Hernández Ábrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. 2019. Additive Margin Softmax를 사용한 양방향 듀얼 인코더를 사용한 다국어 문장 임베딩 개선. 제28회 국제 인공지능 공동 학술대회 논문집, IJCAI 2019, 마카오, 중국, 2019년 8월 10-16일, 5370-5378쪽. Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. 2021. 조건부 마스크 언어 모델을 사용한 범용 문장 표현 학습. 2021 자연어 처리 경험적 방법에 대한 컨퍼런스의 진행 사항, EMNLP 2021, 가상 이벤트/푼타카나, 도미니카 공화국, 2021년 11월 7-11일, 6216-6228쪽. Association for Computational Linguistics. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017. 두 번째 BUCC 공유 과제 개요: 비교 가능한 코퍼스에서 병렬 문장 발견. 10번째 비교 가능한 코퍼스 구축 및 사용 워크숍의 진행 사항, BUCC@ACL 2017, 밴쿠버, 캐나다, 2017년 8월 3일, 60-67쪽. Association for Computational Linguistics. 전체 Tatoeba 결과 표 7과 표 8에 모든 언어의 Tatoeba 검색 정확도를 보고합니다. 우리의 접근 방식은 대부분 언어에서 양방향으로 다른 기준선보다 지속적으로 우수한 성과를 보이며, 특히 &quot;en→xx&quot; 방향에서 이점이 큽니다. TR 전용 모델의 성능은 jv, kk, sw, tl과 같은 언어에서 입증된 것처럼 두 방향 사이에서 크게 달라질 수 있음을 관찰했습니다. 반면, 우리의 접근 방식은 훨씬 더 안정적인 성능을 보이며, 이는 양방향 애플리케이션에 유용합니다. B BUCC의 점수 함수 유사점 간 방향 비교와 달리 여백 기반 방법은 측정의 규모 불일치를 설명합니다. 우리는 Artetxe와 Schwenk(2019a)가 제안한 방법을 채택했습니다. f(x, y) ΣzЄNk(x)k(x, y)(x,y) + ΣzENk(3)(z,y) k(8) 여기서 Nk(x)는 다른 언어에서 x의 k개 최근접 이웃 집합을 나타냅니다. 실험에서 우리는 k = 4로 설정했습니다. 특정 임계값 Y를 사용하여 f(x, y) ≥y인 문장 쌍은 정렬된 것으로 식별됩니다. 여러 정렬된 쌍에 나타나는 x에 대해 가장 높은 점수를 가진 쌍을 선택합니다. 최상의 임계값을 결정하기 위해 먼저 모든 후보의 점수를 계산하고 이를 정렬된 시퀀스로 정렬합니다. 다음으로, 모델 mBERT+TR af ar bg bn de el es et eu fa fi fr he hi hu id it 95.5 90.3 94.5 88.8 99.1 96.4 98.1 97.4 95.1 94.0 96.5 95.3 90.6 95.5 96.8로 F1 점수를 계산합니다. 95.4 94.4 96.mBERT+TR+TLM 95.9 89.7 94.6 87.0 99.1 95.6 98.3 96.8 95.0 94.0 95.7 95.4 91.5 95.6 95.6 95.0 93.8 95.MBERT+DAP 96.9 91.8 95.4 89.3 99.1 96.8 98.4 98.0 96.2 95.9 97.1 95.5 93.0 96.8 97.0 95.9 95.5 96.XLM-R+TR 95.0 90.0 92.9 89.3 99.1 93.9 98.1 97.8 95.3 95.3 96.9 95.3 91.1 96.4 97.0 95.1 94.4 96.XLM-R+TR+TLM 92.7 90.2 94.3 88.8 99.1 95.5 97.3 96.8 93.8 94.4 95.9 94.2 91.2 96.4 95.9 96.0 94.4 94.XLM-R+DAP 96.1 93.1 95.7 91.4 99.2 96.7 98.4 98.1 96.0 94.9 97.3 95.5 93.6 97.3 97.0 96.4 96.3 96.jv ka kk ko ml mr nl pt ru SW ta te th tl tr ur vi zh MBERT+TR 29.3 81.0 62.6 91.2 97.7 91.6 96.2 95.4 95.6 75.1 84.0 90.2 96.2 67.7 98.2 89.6 96.9 95.mBERT+TR+TLM 31.2 79.2 64.7 91.8 97.5 92.0 95.9 95.4 94.8 77.2 85.3 89.7 96.0 71.0 97.7 91.3 96.9 95.mBERT+DAP 30.2 79.9 63.8 93.2 98.5 92.5 96.6 96.2 95.5 77.9 83.1 88.5 96.9 70.1 98.5 90.8 97.5 95.XLM-R+TR 46.3 90.5 75.7 92.7 98.5 93.2 96.7 95.4 94.7 73.3 84.4 93.6 96.7 74.2 97.2 91.6 97.5 95.XLM-R+TR+TLM 23.4 92.4 69.2 91.6 97.2 90.4 95.7 95.5 94.3 72.8 71.0 88.5 96.4 55.8 97.1 85.9 97.0 94.XLM-R+DAP 27.3 93.7 68.5 93.3 98.4 92.5 96.6 96.1 95.4 77.2 80.8 92.3 98.2 65.6 98.3 90.3 98.2 95.모델 MBERT+TR af 표 7: 방향 xx→en의 36개 언어에 대한 검색 정확도. ar bg bn de el es et eu fa fi fr he hi hu id it ja 94.8 88.7 93.3 86.2 98.8 95.4 97.4 96.3 94.7 94.3 95.6 95.8 89.7 95.0 95.6 94.3 95.1 95.MBERT+TR+TLM 95.7 88.0 93.8 85.8 98.9 96.1 97.6 96.3 94.8 93.7 94.8 95.3 89.6 95.3 94.4 94.1 94.1 95.MBERT+DAP 96.3 90.6 94.3 87.8 98.9 96.1 98.1 98.0 96.0 95.6 96.4 95.4 92.2 96.0 96.5 95.2 95.8 96.XLM-R+TR 87.6 90.3 92.0 85.5 98.3 95.9 96.2 95.9 92.8 93.1 95.4 92.4 91.6 94.3 95.6 94.0 94.4 90.XLM-R+TR+TLM 96.1 89.3 93.9 90.0 99.1 93.9 98.2 97.0 94.9 95.7 96.8 95.4 89.6 97.1 96.5 95.3 94.4 96.XLM-R+DAP 96.3 92.2 95.4 91.2 98.9 96.6 98.6 98.1 95.7 96.0 97.1 96.3 93.1 97.0 97.2 96.3 96.1 97.jv 카 kk ko ml mr nl pt ru SW ta te th tl tr ur vi zh 43.4 81.5 66.4 91.8 97.4 92.3 96.1 94.6 94.8 72.3 83.4 89.3 95.8 70.6 96.8 89.5 97.3 94.mBERT+TR+TLM 46.3 78.0 67.8 92.5 98.0 92.2 95.9 94.7 94.2 74.9 84.0 89.7 95.8 74.6 96.8 90.4 97.6 94.MBERT+DAP MBERT+TR XLM-R+TR 47.3 80.8 65.4 92.3 98.3 93.3 97.2 95.6 94.8 75.6 82.4 89.7 96.4 75.5 98.2 91.7 97.8 95.16.1 88.3 57.6 89.8 96.2 87.3 95.4 95.5 93.9 59.5 62.5 81.6 95.3 46.8 97.0 82.2 96.7 92.XLM-R+TR+TLM 49.8 90.6 82.6 92.4 98.5 94.2 97.0 95.0 94.2 81.5 86.0 96.6 96.9 80.2 96.6 92.6 97.7 95.XLM-R+DAP 47.3 91.6 75.3 93.4 99.0 93.6 96.8 95.6 95.1 78.5 86.3 94.9 97.8 77.1 97.9 92.7 98.0 96.표 8: 방향의 36개 언어에 대한 검색 정확도 en→xx. 두 연속 점수의 각 중간 지점으로 설정하고 최적의 y를 찾습니다. 이 절차는 학습 세트에서 수행됩니다. C XNLI 미세 조정 미세 조정 하이퍼 매개변수 설정은 표 9에 나와 있습니다. {le5, 3e-5, 5e-5, 7e-5} 중에서 학습률을 검색했습니다. 배치 크기 학습률 5e-에포크 최대 시퀀스 길이 가중치 감소 표 9: XNLI 미세 조정의 하이퍼 매개변수 설정.
