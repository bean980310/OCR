--- ABSTRACT ---
암묵적 신경 표현 또는 신경 복사장(NeRF)의 인기로 인해 재구성된 장면의 후처리 및 3D 콘텐츠 생성과 같은 작업을 위해 암묵적 3D 모델과 상호 작용하는 편집 방법에 대한 절실한 필요성이 있습니다. 이전 연구에서는 다양한 관점에서 NeRF 편집을 탐구했지만 편집 유연성, 품질 및 속도에 제한이 있어 직접 편집 응답과 즉각적인 미리보기를 제공하지 못했습니다. 핵심 과제는 편집 지침을 직접 반영하고 즉시 업데이트할 수 있는 로컬 편집 가능한 신경 표현을 구상하는 것입니다. 이러한 격차를 메우기 위해 Seal-3D1이라는 암묵적 표현을 위한 새로운 대화형 편집 방법과 시스템을 제안합니다. 이를 통해 사용자는 광범위한 NeRF*Equal 기여로 NeRF 모델을 픽셀 수준에서 자유롭게 편집할 수 있습니다. *연락처 저자. 프로젝트 페이지: https://windingwind.github.io/seal-3d/ 1&quot;Seal&quot;은 Adobe Photoshop의 고무 도장 이름에서 유래했습니다. 백본과 유사하며 편집 효과를 즉시 미리 볼 수 있습니다. 효과를 얻기 위해, 교사 모델에서 NeRF 모델의 원래 공간에 편집 지침을 매핑하는 제안된 프록시 함수와 로컬 사전 학습 및 글로벌 미세 조정을 사용한 학생 모델에 대한 2단계 학습 전략을 통해 과제를 해결합니다. NeRF 편집 시스템은 다양한 편집 유형을 보여주기 위해 구축되었습니다. 저희 시스템은 약 1초의 상호 작용 속도로 매력적인 편집 효과를 얻을 수 있습니다. 1.
--- INTRODUCTION ---
암묵적 신경 표현, 예를 들어 신경 광도장(NeRF) [24]은 3D 장면을 모델링하기 위한 신경망을 사용한 새로운 3D 표현으로 점점 더 주목을 받고 있습니다. 비교적 낮은 메모리 소비로 높은 재구성 정확도와 렌더링 품질의 이점을 누리는 NeRF와 그 변형 [50, 3, 33, 26, 4, 45, 41]은 3D 재구성, 새로운 뷰 합성, 가상/증강 현실과 같은 많은 3D 응용 프로그램에서 큰 잠재력을 보여주었습니다. 새로운 암묵적 표현의 인기와 증가하는 암묵적 3D 모델의 수로 인해 이러한 3D 모델과 상호 작용할 수 있는 인간 친화적인 편집 도구에 대한 수요가 절실합니다. 암묵적 신경 표현으로 편집하는 것은 표현을 완전히 강화하는 데 필요한 기본 기술입니다. 실제 세계에서 재구성된 객체는 캡처된 데이터의 노이즈와 재구성 알고리즘의 한계로 인해 아티팩트가 포함될 가능성이 높습니다. 일반적인 3D 스캐닝 파이프라인에서 아티팩트를 제거하기 위한 수동 수정 및 개선이 일반적인 단계입니다. 반면, 3D 게임, 애니메이션, 영화 촬영과 같은 3D 콘텐츠 제작 애플리케이션에서 아티스트는 일반적으로 기존 3D 모델을 기반으로 새로운 콘텐츠를 만들어야 합니다. 이전 작업에서는 객체 분할[20, 44], 객체 제거[19], 모양 편집[14, 27, 22], 객체 블렌딩[7] 등을 포함하여 NeRF로 표현된 3D 장면을 편집하려고 시도했습니다. 이러한 기존 NeRF 편집 방법은 주로 거친 객체 수준 편집에 초점을 맞추고 있으며 수렴 속도는 대화형 편집의 요구 사항을 충족할 수 없습니다. 일부 최근 방법[48, 5]은 메시를 편집 프록시로 도입하여 NeRF 편집을 메시 편집으로 변환합니다. 이를 위해 사용자는 추가 메싱 도구에서 작업해야 하므로 대화형성과 사용자 친화성이 제한됩니다. 저희가 아는 한, 빠른 수렴 속도로 신경 광도장의 대화형 픽셀 수준 편집을 지원할 수 있는 기존 방법은 없으며, 이는 주로 아래에서 설명하는 과제 때문입니다. 기존의 명시적 3D 표현(예: 포인트 클라우드, 텍스처 메시, 점유 볼륨)과 달리 객체와 장면의 명시적 지오메트리 구조를 저장하는 묵시적 표현은 신경망을 사용하여 지오메트리와 색상을 포함한 3D 장면의 기능을 쿼리합니다. 메시 기반 표현을 예로 들면 기존의 3D 편집 방법은 대상 객체 표면 영역과 객체 텍스처에 해당하는 정점을 변위시켜 객체 지오메트리를 변경할 수 있습니다. 시각적 효과와 기본 표현 간에 명시적으로 설명 가능한 대응 관계가 없으면 묵시적 3D 모델을 편집하는 것은 간접적이고 어렵습니다. 또한 장면의 로컬 영역에서 묵시적 네트워크 매개변수를 찾기 어렵기 때문에 네트워크 매개변수를 적용하면 원치 않는 전역적 변경이 발생할 수 있습니다. 이로 인해 세분화된 편집에 더 많은 어려움이 발생합니다. 이 논문에서는 이러한 격차를 메우기 위해 Seal-3D라고 하는 3D 장면의 묵시적 신경 표현을 위한 대화형 픽셀 수준 편집 방법과 시스템을 제안합니다. 이름은 인기 있는 2D 이미지 편집 소프트웨어인 Adobe PhotoShop[1]에서 따온 것인데, 그 봉인 도구가 비슷한 편집 작업을 제공하기 때문이다.그림 1에서 보듯이, 편집 시스템은 예시로 다섯 가지 유형의 편집으로 구성된다.1) 경계 상자 도구.복사-붙여넣기 작업처럼 경계 상자 안에 있는 것들을 변형하고 크기를 조정한다.2) 브러싱 도구.선택한 영역에 지정된 색상을 칠하고 유화 붓이나 조각도구처럼 표면 높이를 늘리거나 줄일 수 있다.3) 앵커 도구.사용자가 제어점을 자유롭게 이동하고 사용자 입력에 따라 주변 공간에 영향을 미칠 수 있도록 한다.4) 색상 도구.객체 표면의 색상을 편집한다.대화형 NeRF 편집 효과를 얻기 위해 위에서 논의한 암묵적 표현의 과제를 해결한다. 첫째, 명시적 편집 지침과 암묵적 네트워크 매개변수 업데이트 간의 대응 관계를 확립하기 위해 대화형 GUI에서 사용자 편집 지침에 의해 결정된 대상 3D 공간을 원래 3D 장면 공간에 매핑하는 프록시 함수와 프록시 함수가 원래 장면에서 획득한 해당 콘텐츠 감독으로 매개변수를 업데이트하는 교사-학생 증류 전략을 제안합니다. 둘째, 로컬 편집을 가능하게 하기 위해, 즉 비로컬 암묵적 표현에서 글로벌 3D 장면에 대한 로컬 편집 효과의 영향을 완화하기 위해 2단계 학습 프로세스를 제안합니다. 편집 영역에 대한 로컬 손실로 위치 임베딩 그리드만 업데이트하는 사전 학습 단계와 글로벌 광도 손실로 임베딩 그리드와 MLP 디코더를 모두 업데이트하는 미세 조정 단계입니다. 이 설계를 사용하면 사전 학습 단계에서 로컬 편집 기능을 업데이트하고 미세 조정 단계에서 로컬 편집 영역을 편집되지 않은 공간의 글로벌 구조와 색상과 혼합하여 뷰 일관성을 달성합니다. 이 디자인은 편집을 즉시 미리 볼 수 있다는 이점이 있습니다. 사전 학습은 매우 빠르게 수렴할 수 있으며 약 1초 이내에 로컬 편집 효과를 제공합니다. 요약하면, 우리의 기여는 다음과 같습니다. • 우리는 신경 광도장을 위한 최초의 대화형 픽셀 수준 편집 방법과 시스템을 제안하는데, 이는 지오메트리(바운딩 박스 도구, 브러시 도구, 앵커 도구) 및 색상 편집을 포함한 여러 유형의 세밀한 편집 도구를 예시합니다. • 명시적 편집 지침과 암묵적 네트워크 매개변수 업데이트 간의 대응 관계를 확립하기 위한 프록시 함수가 제안되고, 매개변수를 업데이트하기 위한 교사-학생 증류 전략이 제안됩니다. • 글로벌 3D 장면을 오염시키지 않고 로컬 세밀한 편집을 즉시 미리 볼 수 있도록 하는 2단계 학습 전략이 제안됩니다. 2.
--- RELATED WORK ---
새로운 관점 합성. 장면의 포즈 이미지 캡처 세트가 주어지면, 새로운 관점 합성의 과제는 임의의 새로운 관점에서 사진처럼 사실적인 이미지를 생성하는 것입니다.최근에 신경망이 편집 안내 생성에 도입되었습니다.즉각적인 미리보기를 위한 2단계 학생 교육 학생 교육 감독 fo xs, ds 로컬 손실 소스 공간 S FM ct, ot 편집(스케일) xt, dt 교사 대상 공간 T 로컬 업데이트 로컬 사전 학습 C, D 글로벌 손실 글로벌 미세 조정 f 학생 그림 2: 편집 프레임워크의 그림.왼쪽: 사용자 편집 후 대상 공간의 3D 포인트 및 뷰 방향이 원래 소스 공간에 매핑되어 학생 교육을 위한 교사 모델에서 안내 Ct, σt를 얻습니다.오른쪽: 학생 교육은 두 단계로 구성됩니다.네트워크의 부분 매개변수를 로컬 손실로 업데이트하여 즉각적인 미리보기를 제공하는 빠른 사전 학습과 글로벌 손실로 미세 조정합니다. 렌더링 파이프라인은 폭셀[21, 35], 포인트 클라우드[2, 6], 다중 평면 이미지(MPI)[17, 23, 52] 및 암시적 표현[36, 24]과 같은 다중 표현에 활용됩니다. 일반적으로 Neural radiance field(NeRF) [24]는 단일 MLP를 사용하여 장면을 밀도와 색상의 체적 필드로 암묵적으로 인코딩하고 볼륨 렌더링을 활용하여 뷰 종속 효과로 인상적인 렌더링 결과를 얻습니다. 이는 인간 [30, 42], 변형 가능한 객체 [28, 29], 포즈 추정 [18], 자율 시스템 [32, 49], 표면 재구성 [45, 41], 실내 장면 [47], 도시 [38, 43] 등에 대한 많은 후속 작업에 영감을 주었습니다. NeRF의 MLP 표현은 폭셀 [33, 37], 해시그리드 [26] 및 텐서 분해 [4, 40]를 포함한 하이브리드 표현을 통해 향상되고 가속화될 수 있습니다. 이 논문에서 우리의 대화형 편집 프레임워크는 NeRF 추론을 위한 실시간 렌더링 속도와 최첨단 품질의 새로운 뷰 합성을 달성하는 Instant-NGP [26]를 기반으로 개발되었습니다. 신경 장면 편집. 장면 편집은 컴퓨터 비전 및 그래픽 분야에서 널리 연구된 문제였습니다. 초기
--- METHOD ---
그리고 우리의 새로운 사전 학습 전략에 의해 즉시(≈1초) 미리 보기(왼쪽)를 달성하는 시스템 Seal-3D. 고품질 편집 결과는 짧은 기간(1~2분)의 미세 조정을 통해 추가로 얻을 수 있습니다. 구현된 편집 도구(오른쪽)의 편집 결과는 원래 표면(왼쪽)에 풍부한 음영 세부 정보(예: 그림자)가 있어 뷰 일관성이 있습니다. 초록 암묵적 신경 표현 또는 신경 광도장(NeRF)의 인기로 인해 재구성된 장면의 후처리 및 3D 콘텐츠 생성과 같은 작업을 위해 암묵적 3D 모델과 상호 작용하는 편집 방법에 대한 절실한 필요성이 있습니다. 이전 연구에서는 다양한 관점에서 NeRF 편집을 탐구했지만 편집 유연성, 품질 및 속도에 제한이 있어 직접적인 편집 응답과 즉각적인 미리 보기를 제공하지 못했습니다. 핵심 과제는 편집 지침을 직접 반영하고 즉시 업데이트할 수 있는 로컬 편집 가능한 신경 표현을 구상하는 것입니다. 격차를 메우기 위해 Seal-3D1이라는 암묵적 표현을 위한 새로운 대화형 편집 방법과 시스템을 제안합니다. 이를 통해 사용자는 광범위한 NeRF*Equal 기여도로 픽셀 수준에서 자유롭게 NeRF 모델을 편집할 수 있습니다. *연락처 저자. 프로젝트 페이지: https://windingwind.github.io/seal-3d/ 1&quot;Seal&quot;은 Adobe Photoshop의 고무 도장 이름에서 유래했습니다. 백본과 유사하며 편집 효과를 즉시 미리 볼 수 있습니다. 효과를 얻기 위해 교사 모델에서 NeRF 모델의 원래 공간에 편집 지침을 매핑하는 제안된 프록시 함수와 로컬 사전 학습 및 글로벌 미세 조정을 통한 학생 모델에 대한 2단계 학습 전략을 통해 과제를 해결합니다. NeRF 편집 시스템은 다양한 편집 유형을 선보이도록 구축되었습니다. 저희 시스템은 약 1초의 대화형 속도로 매력적인 편집 효과를 얻을 수 있습니다. 1. 서론 암묵적 신경 표현, 예를 들어 신경 광도장(NeRF) [24]은 3D 장면을 모델링하기 위한 신경망을 갖춘 새로운 3D 표현으로 점점 더 많은 주목을 받고 있습니다. 높은 재구성 정확도와 비교적 낮은 메모리 소비로 렌더링 품질을 활용하는 NeRF와 그 변형[50, 3, 33, 26, 4, 45, 41]은 3D 재구성, 새로운 뷰 합성, 가상/증강 현실과 같은 많은 3D 애플리케이션에서 큰 잠재력을 보여주었습니다. 새로운 암묵적 표현의 인기와 증가하는 수의 암묵적 3D 모델로 인해 이러한 3D 모델과 상호 작용할 수 있는 인간 친화적인 편집 도구에 대한 수요가 절실합니다. 암묵적 신경 표현으로 편집하는 것은 표현을 완전히 강화하는 데 필요한 기본 기술입니다. 실제 세계에서 재구성된 객체는 캡처된 데이터의 노이즈와 재구성 알고리즘의 한계로 인해 아티팩트가 포함될 가능성이 높습니다. 일반적인 3D 스캐닝 파이프라인에서 아티팩트를 제거하기 위한 수동 수정 및 정제가 일반적인 단계입니다. 반면 3D 게임, 애니메이션 및 필름 촬영과 같은 3D 콘텐츠 생성 애플리케이션에서 아티스트는 일반적으로 기존 3D 모델을 기반으로 새 콘텐츠를 만들어야 합니다. 이전 연구에서는 객체 분할[20, 44], 객체 제거[19], 모양 편집[14, 27, 22], 객체 블렌딩[7] 등을 포함하여 NeRF로 표현된 3D 장면을 편집하려는 시도가 있었습니다. 이러한 기존 NeRF 편집 방법은 주로 거친 객체 수준 편집에 초점을 맞추고 있으며 수렴 속도는 대화형 편집의 요구 사항을 충족할 수 없습니다. 일부 최근 방법[48, 5]은 편집 프록시로 메시를 도입하여 NeRF 편집을 메시 편집으로 변환합니다. 이를 위해 사용자는 추가 메싱 도구에서 작업해야 하므로 대화형성과 사용자 친화성이 제한됩니다. 저희가 아는 한, 빠른 수렴 속도로 신경 광도장의 대화형 픽셀 수준 편집을 지원할 수 있는 기존 방법은 없으며, 이는 주로 아래에서 설명하는 과제 때문입니다. 객체와 장면의 명시적 지오메트리 구조를 저장하는 포인트 클라우드, 텍스처 메시, 점유 볼륨과 같은 기존의 명시적 3D 표현과 달리 암시적 표현은 신경망을 사용하여 지오메트리와 색상을 포함한 3D 장면의 기능을 쿼리합니다. 기존의 3D 편집 방법은 메시 기반 표현을 예로 들면 대상 객체 표면 영역과 객체 텍스처에 해당하는 정점을 변위시켜 객체 지오메트리를 변경할 수 있습니다. 시각적 효과와 기본 표현 간에 명확하게 설명 가능한 대응 관계가 없으면 암묵적 3D 모델을 편집하는 것은 간접적이고 어렵습니다. 또한 장면의 로컬 영역에서 암묵적 네트워크 매개변수를 찾기 어렵기 때문에 네트워크 매개변수를 적용하면 원치 않는 전역적 변경이 발생할 수 있습니다. 이로 인해 세부적인 편집에 더 많은 어려움이 따릅니다. 이러한 격차를 메우기 위해 이 논문에서는 Seal-3D라고 하는 3D 장면의 암묵적 신경 표현을 위한 대화형 픽셀 수준 편집 방법과 시스템을 제안합니다. 이 이름은 인기 있는 2D 이미지 편집 소프트웨어인 Adobe PhotoShop[1]에서 따온 것으로, 해당 seal 도구가 유사한 편집 작업을 제공하기 때문입니다. 그림 1에서 볼 수 있듯이 편집 시스템은 다음과 같은 5가지 유형의 편집으로 구성됩니다. 1) 경계 상자 도구. 복사-붙여넣기 작업처럼 경계 상자 내부의 사물을 변형하고 크기를 조정합니다. 2) 브러싱 도구. 선택한 영역에 지정된 색상을 칠하고 오일 페인트 브러시나 조각도구처럼 표면 높이를 늘리거나 줄일 수 있습니다. 3) 앵커 도구. 사용자가 제어점을 자유롭게 이동하고 사용자 입력에 따라 주변 공간에 영향을 미칠 수 있습니다. 4) 색상 도구. 객체 표면의 색상을 편집합니다. 대화형 NeRF 편집 효과를 얻기 위해 위에서 논의한 암묵적 표현의 과제를 해결합니다. 먼저, 명시적 편집 지침과 암묵적 네트워크 매개변수 업데이트 간의 대응 관계를 확립하기 위해 대화형 GUI에서 사용자 편집 지침에 의해 결정된 대상 3D 공간을 원래 3D 장면 공간에 매핑하는 프록시 함수와 원래 장면에서 프록시 함수가 획득한 해당 콘텐츠 감독으로 매개변수를 업데이트하는 교사-학생 증류 전략을 제안합니다. 둘째, 로컬 편집을 가능하게 하기 위해, 즉 비로컬 암묵적 표현 하에서 글로벌 3D 장면에 대한 로컬 편집 효과의 영향을 완화하기 위해, 우리는 두 단계의 훈련 과정을 제안합니다. 편집 영역에 대한 로컬 손실로 위치 임베딩 그리드만 업데이트하는 사전 훈련 단계와 글로벌 퇴화를 방지하기 위해 후속 MLP 디코더를 동결하는 단계, 임베딩 그리드와 MLP 디코더를 모두 글로벌 광도 손실로 업데이트하는 미세 조정 단계입니다. 이 설계를 사용하면 사전 훈련 단계에서 로컬 편집 피처를 업데이트하고 미세 조정 단계에서 로컬 편집 영역을 편집되지 않은 공간의 글로벌 구조와 색상과 혼합하여 뷰 일관성을 달성합니다. 이 설계는 편집의 즉각적인 미리보기라는 이점이 있습니다. 사전 훈련은 매우 빠르게 수렴할 수 있으며 약 1초 이내에 로컬 편집 효과를 제공합니다. 요약하면, 우리의 기여는 다음과 같습니다. • 우리는 신경 광도장을 위한 최초의 대화형 픽셀 수준 편집 방법과 시스템을 제안하는데, 이는 기하학(경계 상자 도구, 브러시 도구, 앵커 도구)과 색상 편집을 포함한 세밀한 여러 유형의 편집 도구를 예시합니다. • 명시적 편집 지침과 암묵적 네트워크 매개변수 업데이트 간의 대응 관계를 확립하기 위한 프록시 함수가 제안되고, 매개변수를 업데이트하기 위한 교사-학생 증류 전략이 제안됩니다. • 글로벌 3D 장면을 오염시키지 않고 로컬 세밀한 편집의 즉각적인 미리 보기를 가능하게 하는 2단계 훈련 전략이 제안됩니다. 2. 관련 연구 새로운 뷰 합성. 장면의 포즈 이미지 캡처 세트가 주어지면, 새로운 뷰 합성의 과제는 임의의 새로운 뷰에서 사진처럼 사실적인 이미지를 생성하는 것입니다. 최근, 신경망이 편집 안내 생성에 도입되었습니다.즉각적 미리보기를 위한 2단계 학생 학습 학생 학습 감독 fo xs, ds 로컬 손실 소스 공간 S FM ct, ot 편집(스케일) xt, dt 교사 대상 공간 T 로컬 업데이트 로컬 사전 학습 C, D 전역 손실 전역 미세 조정 f 학생 그림 2: 편집 프레임워크의 그림.왼쪽: 사용자 편집 후 대상 공간의 3D 포인트 및 뷰 방향이 원래 소스 공간에 매핑되어 교사 모델에서 학생 학습을 위한 안내 Ct, σt를 얻습니다.오른쪽: 학생 학습은 두 단계로 구성됩니다.즉각적 미리보기를 제공하기 위한 빠른 사전 학습은 로컬 손실로 네트워크의 부분 매개변수를 업데이트하고 전역 손실로 미세 조정합니다.렌더링 파이프라인이며 폭셀[21, 35], 포인트 클라우드[2, 6], 다중 평면 이미지(MPI)[17, 23, 52] 및 암시적 표현[36, 24]과 같은 여러 표현에 활용됩니다. 일반적으로 Neural radiance field(NeRF) [24]는 단일 MLP를 사용하여 장면을 밀도와 색상의 체적 필드로 암묵적으로 인코딩하고 볼륨 렌더링을 활용하여 뷰 종속 효과로 인상적인 렌더링 결과를 얻습니다. 이는 인간 [30, 42], 변형 가능한 객체 [28, 29], 포즈 추정 [18], 자율 시스템 [32, 49], 표면 재구성 [45, 41], 실내 장면 [47], 도시 [38, 43] 등에 대한 많은 후속 작업에 영감을 주었습니다. NeRF의 MLP 표현은 폭셀 [33, 37], 해시그리드 [26] 및 텐서 분해 [4, 40]를 포함한 하이브리드 표현을 통해 향상되고 가속화될 수 있습니다. 이 논문에서는 NeRF 추론을 위한 실시간 렌더링 속도와 최첨단 품질의 새로운 뷰 합성을 달성하는 Instant-NGP [26]를 기반으로 대화형 편집 프레임워크를 개발했습니다.신경망 장면 편집.장면 편집은 컴퓨터 비전 및 그래픽 분야에서 폭넓게 연구된 문제였습니다.초기 방법은 삽입 [15, 54], 재조명 [16], 구성 [31], 객체 이동 [12, 34] 등을 통해 단일 정적 뷰를 편집하는 데 중점을 두었습니다.신경망 렌더링의 개발로 최근 연구에서는 장면 수준, 객체 수준 및 픽셀 수준 편집으로 분류할 수 있는 3D 장면의 여러 수준에서 편집을 수행하려고 시도합니다.장면 수준 편집 방법은 조명 [8] 및 전역 팔레트 [14]와 같은 장면의 전역 모양을 변경하는 데 중점을 둡니다.내재적 분해 [51, 27, 9, 46, 53, 10]는 재료와 조명 필드를 풀어 텍스처 또는 조명 편집을 가능하게 합니다. 그러나 장면 수준 방법은 전역 속성만 수정할 수 있으며 지정된 객체에 적용할 수 없습니다.객체 수준 편집 방법은 암묵적으로 표현된 객체를 조작하기 위해 다양한 전략을 사용합니다.ObjectNERF[44]는 객체별 잠재 코드를 활용하여 신경 광도장을 객체로 분해하여 객체를 이동, 제거 또는 복제할 수 있도록 합니다.Liu et al.[20]은 의미 수준 색상이나 기하학을 수정하기 위해 편집 지침에 따라 부분적으로 최적화된 조건부 광도장 모델을 설계합니다.NeRF 편집[48]과 NeuMesh[5]는 객체 편집을 안내하는 편집 프록시로 NeRF로 재구성된 변형 가능한 메시를 도입합니다.그러나 이러한 방법은 객체 수준의 강체 변환으로 제한되거나 임의의 분포 밖 편집 범주로 일반화할 수 없습니다.반대로 픽셀 수준 편집은 객체 엔터티에 의해 제한되는 대신 픽셀에 의해 정확하게 선택된 세분화된 편집 지침을 제공하는 것을 목표로 합니다.우리가 아는 한, NeuMesh[5]는 이 수준에서 편집을 달성하는 유일한 기존 방법입니다. 그러나 이는 편집 범주를 제한하는 메시 스캐폴드에 따라 달라지는데, 예를 들어 메시 외부 기하 구조를 만들 수 없습니다. 반면에 저희 편집 프레임워크는 프록시 기하 구조를 필요로 하지 않아 보다 직접적이고 광범위할 수 있습니다. 게다가 신경 편집 방법의 성능을 최적화하는 것은 여전히 미해결 문제로 남아 있습니다. 기존 방법은 몇 분 또는 몇 시간의 최적화와 추론이 필요합니다. 저희 방법은 즉각적인 상호 작용(즉, 2차 수준) 성능을 달성한 최초의 픽셀 수준 신경 편집 프레임워크입니다. 3. 방법 신경 광도장을 위한 상호 작용적인 픽셀 수준 편집 방법인 Seal-3D를 소개합니다. 전체 파이프라인은 그림 2에 나와 있으며, 픽셀 수준 프록시 매핑 기능, 교사-학생 훈련 프레임워크, 프레임워크 하의 학생 NeRF 네트워크를 위한 2단계 훈련 전략으로 구성되어 있습니다. 저희 편집 워크플로는 사용자가 지정한 편집 규칙에 따라 쿼리 포인트와 광선 방향을 매핑하는 프록시 기능부터 시작합니다. 그런 다음 NeRF-to-NeRF 교사-학생 증류 프레임워크가 이어지며, 여기서 기하학 및 색상의 편집 매핑 규칙이 있는 교사 모델이 학생 모델의 학습을 감독합니다(3.2절). 대화형 미세 조정 편집의 핵심은 학생 모델에 대한 2단계 학습입니다(3.3절). 추가 사전 학습 단계에서 교사 모델의 편집 공간 내부에 있는 점, 광선 방향 및 추론된 기준 진실이 이전에 샘플링, 계산 및 캐시됩니다. 지역성이 있는 매개변수만 업데이트되고 전역적 변경을 일으키는 매개변수는 동결됩니다. 사전 학습 단계 후에 학생 모델은 전역 학습 단계로 미세 조정됩니다. 3.1. NeRF 기반 편집 문제 개요 먼저 신경 광도장에 대해 간략하게 소개한 다음 NeRF 기반 편집 문제의 과제와 기존 솔루션의 한계를 분석합니다. 3.1.1 NeRF 예비 지식 신경 광도장(NeRF)은 3D 장면에 대한 암묵적 표현을 5D 함수로 제공합니다. f : (x, y, z, 0,6) → (c, σ), 여기서 x = (x, y, z)는 3D 위치이고 d = (0,0)은 뷰 방향이며 c와 σ는 각각 색상과 볼륨 밀도를 나타냅니다. 5D 함수는 일반적으로 MLP fo로 매개변수화됩니다. 이미지 픽셀을 렌더링하려면 방향 d를 가진 광선 r이 카메라의 내부 및 외부에 따라 카메라 위치 o에서 픽셀 중심을 통과합니다. K개의 점 Xi otid, i = : 1, 2, ..., K가 광선을 따라 샘플링되고 네트워크 fo가 해당 색상 및 밀도에 대해 쿼리됩니다. (Ci, σi) = fø(xi, d) (1) 그런 다음 예측된 픽셀 색상 Ĉ (r)과 깊이 값 D(r)이 볼륨 렌더링을 통해 계산됩니다. K (r) = ΣΤ;aici, i=K i=D(r) = Tiarti T₁ = [[(1 − αj), α = 1 exp (σidi) (3) j
--- EXPERIMENT ---
모든 그림은 4.3절과 그림 12. 전역 미세 조정에서 제시됩니다. 사전 학습 후, 우리는 거친 미리보기를 완전히 수렴된 결과로 정제하기 위해 미세 조정을 계속합니다. 이 단계는 표준 NeRF 학습과 유사하지만, 감독 레이블은 이미지 픽셀 대신 교사 추론 프로세스에 의해 생성됩니다. Lglobal = 3||CT - ĈS ||2 + À4||Âª – Â³||1 (8) rЄR 여기서 R은 미니배치에서 샘플링된 광선 세트를 나타내고, (ĈT, DT),(ĈS, DS)는 각각 (cT, σT), (cs, os)에 따라 Eq. (2)에 따라 광선 r을 따라 누적됩니다. 학생 네트워크는 학습하는 교사 네트워크보다 더 나은 품질의 결과를 생성할 수 있다는 점을 언급할 가치가 있습니다. 이는 교사 추론 프로세스의 매핑 작업이 가상 기준 진실에서 일부 뷰 불일치 아티팩트를 생성할 수 있기 때문입니다. 그러나 증류하는 동안 학생 네트워크는 뷰 일관성 견고성을 적용하는 다중 뷰 훈련으로 인해 이러한 아티팩트를 자동으로 제거할 수 있습니다.자세한 내용은 섹션 4.2 및 그림 6을 참조하십시오.4. 실험 및 분석 4.1. 구현 세부 정보 네트워크.해시 그리드 내의 모양 및 색상 잠재 정보를 풀어내기 위해 Instant-NGP [26]의 NeRF 네트워크 아키텍처에서 단일 해시 테이블을 두 개로 분할했습니다.오픈 소스 PyTorch 구현 torch-ngp [39]의 원래 밀도 그리드와 동일한 설정을 가진 밀도 그리드 Go와 색상 그리드 GC입니다.이렇게 하는 이유는 다른 것에 영향을 주지 않고 색상 또는 기하 속성 중 하나를 세밀하게 편집할 수 있도록 하기 위해서입니다.나머지 네트워크 아키텍처는 시그마 MLP fo와 색상 MLP fc를 포함하여 동일하게 유지됩니다. 공간 지점 x와 시야 방향 d에 대해 네트워크는 다음과 같이 볼륨 밀도 σ와 색상 c를 예측합니다.σ,z= · ƒº (G° (x)) C = = ƒc (Gc(x), z, SH(d)) (9) (10) 여기서 z는 중간 기하 특징이고, SH는 구면 고조파 방향 인코더입니다[26].사전 학습(1.1초) PSNR: 35와 동일합니다.원래 모델 Ft 출력 미세 조정(61.4초) PSNR: 45.그림 5: 앵커 편집의 예: 가짜 치아.원래 모델 Ft 출력 사전 학습(1.0초) 미세 조정(34.5초) PSNR: 35.PSNR: 40.~ 그림 6: 실제 장면에서 편집의 예: (DTU 스캔 83).그림 7: 개체 전송 편집의 예: 레고 장면(NeRF Blender)에서 가족 장면(Tanks and Temples)으로. Instant-NGP의 설정에서 fo는 숨겨진 채널 64가 있는 2개의 레이어를 가지고 있고, fc는 숨겨진 채널 64가 있는 3개의 레이어를 가지고 있으며, z는 15채널 피처입니다.우리는 NeRF Blender Synthetic 데이터 세트[24]의 Lego 장면에서 수정된 NeRF 네트워크를 vanilla 아키텍처와 비교합니다.우리는 30,000번의 반복 동안 장면에서 우리 네트워크와 vanilla 네트워크를 훈련합니다.결과는 다음과 같습니다.· 우리: 훈련 시간 441초, PSNR 35.08dB· vanilla: 훈련 시간 408초, PSNR 34.44dB 우리는 수정된 아키텍처에서 런타임이 약간 더 느리고 품질이 더 높은 것을 관찰했는데, 이는 이 수정이 무시할 수 있는 변화를 일으킨다는 것을 나타냅니다.훈련.우리는 편집 프레임워크의 NeRF 백본으로 Instant-NGP[26]를 선택합니다.우리의 구현은 오픈소스 PyTorch 구현 torchngp[39]를 기반으로 합니다.모든 실험은 단일 NVIDIA RTX 3090 GPU에서 실행됩니다.우리는 원래 네트워크 아키텍처를 약간 수정한다는 점에 유의하세요. 자세한 내용은 보충 자료를 참조하십시오. = 사전 학습 단계에서 ₁ = 2 = 1로 설정하고 학습률을 0.05로 고정합니다. 미세 조정 단계에서 λ3 X4 1을 초기 학습률 0.01로 설정합니다. 사전 학습된 NeRF 모델에서 시작하여 로컬 사전 학습의 50100에포크(약 0.5-1초)와 글로벌 미세 조정의 약 50에포크(약 40초)를 수행합니다. 에포크 수와 시간 소모는 편집 유형과 장면의 복잡성에 따라 조정할 수 있습니다. 백본보다 속도가 뛰어난 tiny-cuda-nn [25]이 없는 상태에서 성능을 테스트했으며, 이는 성능에 추가 최적화의 여지가 있음을 나타냅니다. 데이터 세트. 합성 NeRF Blender 데이터 세트 [24]와 실제로 캡처한 Tanks and Temples [13] 및 DTU [11] 데이터 세트에서 편집을 평가합니다. 우리는 훈련과 평가를 위해 프레임의 공식 데이터 세트 분할을 따른다.4.2. 실험 결과 정성적 NeRF 편집 결과.우리는 경계 모양(그림 4 및 6), 브러싱(그림 3), 앵커(그림 5) 및 색상(그림 1)을 포함하여 설계한 모든 종류의 편집 범주에서 광범위한 실험 결과를 제공한다.우리의 방법은 2차 레벨에서 즉시 미리 보기를 지원하여 성능을 크게 향상시킬 뿐만 아니라 그림 3의 들어올린 면의 음영 효과 및 그림 8의 범프 표면의 그림자와 같이 시각적으로 더 사실적인 편집 모양을 생성한다.게다가 학생 네트워크에서 생성된 결과는 교사 레이블보다 성능이 우수할 수도 있다.예를 들어, 그림 6에서 Ft 출력에는 뷰 불일치로 인해 떠다니는 아티팩트가 포함된다.3.3절에서 분석한 것처럼 증류 프로세스는 이를 제거하는 데 성공한다. 또한 객체 전송의 예를 제공합니다(그림 7).Blender 데이터 세트의 Lego 장면에 있는 전구가 Tanks and Temples 데이터 세트의 가족 장면에 있는 아이의 머리로 전송됩니다.대화형 편집 지침 편집 결과 Ours NeuMesh Ours NeuMesh 그림 8: NeuMesh [5]와 저희 방법 간의 텍스처/색상 페인팅 비교.NeuMesh는 미세 조정에 수 시간이 필요한 반면 저희는 몇 초만 필요합니다.원래 GT(Blender에서 렌더링) NeuMesh PSNR 27.렌더링 속도: 0.009 FPS 저희 PSNR 28.28FPS 그림 9: NeuMesh [5]와 저희 방법 간의 정성적 및 정량적 비교.PSNR은 편집 결과와 동일한 편집을 적용한 기준 진실 메시의 렌더링에서 계산됩니다.지침 Liu et al. [20] NeuMesh [5] 저희 그림 10: 기준선 [20, 5]과 저희의 픽셀 단위 편집 기능 비교. [20]이 다른 두 가지와 같은 픽셀 단위 편집 작업에 초점을 맞추지 않는다는 점에 유의하세요.우리는 그들의 방법과 경쟁하지 않아야 합니다.베이스라인과의 비교.기존 작업은 지오메트리 편집이나 모양 편집 중 하나에 초점을 맞춘 편집 유형에 대한 강력한 제한이 있는 반면, 우리는 두 가지를 동시에 수행할 수 있습니다.브러싱 및 앵커 도구는 기존 방법에서 지원하지 않는 사용자 안내 프록시 외부 지오메트리 구조를 만들 수 있습니다.NeuMesh [5]에서 지원하는 색상 및 텍스처 페인팅과 1초 30초 60초 PSNR: 26.28.사전 학습 없음 40.PSNR: 32.30.미세 조정 없음 29.PSNR: 31.37.42.전체 모델(1초 사전 학습 + 59초 미세 조정) 그림 11: 2단계 학습 전략에 대한 소거 연구.미세 조정 없음의 저하 세부 정보를 확대합니다. w/o MLP 고정 사전 학습 고정 MLP(저희 방법) 그림 12: MLP 고정에 대한 소거 연구.Liu et al. [20]. 그림 8은 낙서와 텍스처 페인팅 작업에서 저희 방법과 NeuMesh [5] 간의 두 가지 비교를 보여줍니다.저희 방법은 결과에 눈에 띄는 색상 편향과 아티팩트가 포함된 NeuMesh보다 상당히 우수한 성능을 보입니다.반면에 저희 방법은 기하학적 범프에 의해 발생한 그림자 효과도 렌더링하는 데 성공합니다.그림 9는 NeRF Blender[24]에서 Mic에 적용한 동일한 비강체 블렌딩의 결과를 보여줍니다.메시가 없으므로 NeuMesh[5]보다 더 많은 세부 정보를 얻을 수 있으며 메시 해상도에 제한이 없습니다.그림 10은 기존 NeRF 편집 방법과 저희 방법의 픽셀 단위 편집 기능에 대한 개요를 보여줍니다.Liu et al. [20]의 방법은 픽셀 단위 편집 작업에 초점을 맞추지 않고 논문에서 텍스처가 없는 간단한 객체만 지원합니다. 그들의 방법은 편집된 객체 내에서 전반적인 색상 저하를 유발하는데, 이는 매우 불리합니다.이는 그들의 잠재 코드가 세분화된 로컬 피처 대신 장면의 글로벌 색상 피처만 모델링하기 때문입니다.우리의 방법은 로컬 인식 임베딩 그리드로 인해 세분화된 로컬 편집을 지원합니다.4.3. Ablation Studies 2단계 학습 전략의 효과.우리의 사전 학습 및 미세 조정 전략의 효과를 검증하기 위해 그림 11에서 전체 전략(3번째 행), 미세 조정 전용(1번째 행) 및 사전 학습 전용(2번째 행)을 비교합니다.우리의 사전 학습은 단 1초 만에 거친 결과를 생성할 수 있는 반면 광도 미세 조정은 그렇게 짧은 기간 내에 모양을 거의 변경할 수 없습니다.사전 학습 단계는 또한 이후의 미세 조정을 강화하는데, 30초 만에 우리의 전체 전략은 더 완전한 결과를 생성합니다.그러나 사전 학습은 로컬 과적합 및 글로벌 저하라는 부작용이 있습니다.따라서 우리의 2단계 전략은 둘 사이의 좋은 균형을 이루고 최적의 결과를 생성합니다. 사전 학습 단계에서 MLP 고정. 그림 12에서 사전 학습 단계에서 모든 MLP 매개변수를 고정하는 설계를 검증합니다. 이 결과는 MLP가 주로 전역 정보를 포함하므로 MLP 디코더가 고정되지 않으면 전역 퇴화로 이어진다는 분석을 확인합니다. 5.
--- CONCLUSION ---
우리는 즉각적인 미리보기를 지원하는 신경 광도장에 대한 픽셀 수준 편집을 위한 대화형 프레임워크를 도입했습니다. 구체적으로, 우리는 2단계 교사-학생 훈련 방법을 활용하여 편집 지침을 제공하고 2단계 훈련 전략을 설계하여 즉각적인 네트워크 수렴을 달성하여 미리보기로 거친 결과를 얻습니다. 이전 연구와 달리, 우리의 방법은 명시적 프록시(예: 메시)가 필요하지 않아 대화형성과 사용자 친화성이 향상됩니다. 또한, 우리의 방법은 편집된 표면에서 음영 효과를 보존하는 것을 지원합니다. 한 가지 한계는 우리의 방법이 반사 반사와 같은 복잡한 뷰 종속 조명 효과를 지원하지 않고 장면 조명을 변경할 수 없다는 것입니다. 이는 내재적 분해를 도입하여 개선할 수 있습니다. 게다가, 우리의 방법은 원래 NeRF 네트워크의 재구성 실패(예: 떠다니는 아티팩트)를 처리하지 않습니다. 감사의 말 이 연구는 중앙 대학의 기초 연구 기금, NSFC의 지원금(62103372, 62088101, 62233013)으로 일부 지원되었습니다. 저장성 핵심연구개발계획(2021C03037); 저장연구실(121005-PI2101); 저장대학교 CAD&amp;CG 정보기술센터 및 국가중점연구실. 참고문헌 [1] Adobe Inc. Adobe photoshop.[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, Victor Lempitsky. 신경점 기반 그래픽. 2020.[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan. Mip-nerf: 앤티앨리어싱 신경 광도장을 위한 다중 스케일 표현. ICCV, 2021.[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su. Tensorf: 텐서 광도장. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. 1,[5] Chong Bao 및 Bangbang Yang, Zeng Junyi, Bao Hujun, Zhang Yinda, Cui Zhaopeng 및 Zhang Guofeng. Neumesh: 기하학 및 텍스처 편집을 위한 얽힘 해제된 신경망 기반 암묵적 필드 학습. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. 2, 3, 4, 8,[6] Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu 및 Bing Zeng. 다중 평면 투영을 통한 신경점 클라우드 렌더링. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 78307839페이지, 2020.[7] Jianfei Guo, Zhiyuan Yang, Xi Lin 및 Qingfu Zhang. 템플릿 약화: 범주별 개체 이미지에서 밀집된 모양 대응을 모델링하기 위해. arXiv 사전 인쇄본 arXiv:2111.04237, 2021.[8] Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser. 객체 중심 신경 장면 렌더링. arXiv 사전 인쇄본 arXiv: 2012.08503, 2020.[9] Jon Hasselgren, Nikolai Hofmann, Jacob Munkberg. 몬테카를로 렌더링 및 노이즈 제거를 사용한 이미지의 모양, 빛 및 재료 분해. arXiv:2206.03380, 2022.[10] Jinkai Hu, Chengzhong Yu, Hongli Liu, Lingqi Yan, Yiqian Wu, Xiaogang Jin. 다중 기능 융합을 사용한 심층 실시간 체적 렌더링. ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH &#39;23, New York, NY, USA, 2023. Association for Computing Machinery.[11] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aanæs. 대규모 다중 시점 입체시 평가. 2014 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 406-413페이지. IEEE, 2014.[12] Natasha Kholgade, Tomas Simon, Alexei Efros, and Yaser Sheikh. 스톡 3D 모델을 사용하여 단일 사진에서 3D 객체 조작. ACM Transactions on Computer Graphics, 33(4), 2014.[13] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017.[14] Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon Wetzstein, Kalyan Sunkavalli. Palettenerf: 신경 광도장의 팔레트 기반 모양 편집. arXiv 사전 인쇄본 arXiv:2212.10699, 2022. 2,[15] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker. 복잡한 실내 장면에 대한 역 렌더링: 단일 이미지의 모양, 공간적으로 변하는 조명 및 svbrdf. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2475-2484페이지, 2020.[16] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli, Miloš Hašan, Zexiang Xu, Ravi Ramamoorthi, Manmohan Chandraker. 단일 이미지에서 실내 장면 조명의 물리적 기반 편집. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, 2022년 10월 23-27일, Proceedings, Part VI, 555-572페이지. Springer, 2022.[17] Zhengqi Li, Wenqi Xian, Abe Davis, Noah Snavely. Crowdsampling the plenoptic function. European Conference on Computer Vision, 178-196페이지. Springer, 2020.[18] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey. Barf: Bundle-adjusting nervous radiance fields. IEEE 국제 컴퓨터 비전 컨퍼런스(ICCV), 2021.[19] Hao-Kang Liu, I Shen, Bing-Yu Chen, et al. Nerf-in: rgb-d 사전 확률을 사용한 자유형 nerf 인페인팅. arXiv 사전 인쇄본 arXiv:2206.04901, 2022.[20] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. 조건부 광도 필드 편집. 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 2021. 2, 3, 4, 8,[21] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. 신경 볼륨: 이미지에서 동적 렌더링 가능 볼륨 학습. ACM Trans. Graph., 38(4):65:1-65:14, 2019년 7월.[22] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri. Sked: 스케치 가이드 텍스트 기반 3D 편집, 2023.[23] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar. 로컬 라이트 필드 융합: 처방적 샘플링 지침을 사용한 실용적인 뷰 합성. ACM Transactions on Graphics(TOG), 2019.[24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 장면을 신경 광도 필드로 표현. ECCV, 2020. 1, 3, 8,[25] Thomas Müller. Tiny CUDA 신경망 프레임워크, 2021. https://github.com/nvlabs/tiny-cuda-nn.[26] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 기본형. ACM Trans. Graph., 41(4):102:1– 102:15, 2022년 7월. 1, 3, 4, 7,[27] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller, Sanja Fidler. 이미지에서 삼각형 3D 모델, 재료 및 조명 추출. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 82808290페이지, 2022. 2,[28] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 광도장. ICCV, 2021.[29] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: 위상적으로 변하는 신경 광도장에 대한 고차원 표현. ACM Trans. Graph., 40(6), 2021년 12월.[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou. 신경체: 동적 인간의 새로운 관점 합성을 위한 구조화된 잠재 코드를 가진 암묵적 신경 표현. CVPR, 2021.[31] Patrick Pérez, Michel Gangnet, Andrew Blake. 푸아송 이미지 편집. ACM SIGGRAPH 2003 논문, 313-318페이지. 2003.[32] Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, Lincheng Li, Yingfeng Chen, Gimhee Lee, Qi Ye. Neurar: 암묵적 신경 표현을 가진 자율적 3D 재구성을 위한 신경 불확실성. IEEE Robotics and Automation Letters, 8(2):1125-1132, 2023.[33] Sara Fridovich-Keil 및 Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht 및 Angjoo Kanazawa. Plenoxels: 신경망이 없는 광채 필드. CVPR, 2022. 1,[34] Rakshith Shetty, Mario Fritz 및 Bernt Schiele. 적대적 장면 편집: 약한 감독에서 자동 객체 제거. 신경 정보 처리 시스템의 발전 31, 7716-7726페이지, 캐나다 몬트리올, 2018. Curran Associates.[35] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein 및 Michael Zollhöfer. Deepvoxels: 지속적인 3D 기능 임베딩 학습. Proc. Computer Vision and Pattern Recognition(CVPR), IEEE, 2019.[36] Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein. 장면 표현 네트워크: 연속 3차원 구조 인식 신경 장면 표현. 신경 정보 처리 시스템의 발전, 2019.[37] Cheng Sun, Min Sun, Hwann-Tzong Chen. 직접 복셀 그리드 최적화: 광도장 재구성을 위한 초고속 수렴. CVPR, 2022.[38] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan T. Barron, Henrik Kretzschmar. Block-NeRF: 확장 가능한 대규모 장면 신경 뷰 합성. arXiv, 2022.[39] Jiaxiang Tang. Torch-ngp: instant-ngp의 pytorch 구현, 2022. https://github.com/ashawkey/torch-ngp. 7,[40] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang 및 Gang Zeng. 랭크-잔여 분해를 통한 압축 가능-구성 가능 nerf. arXiv 사전 인쇄본 arXiv:2205.14870, 2022.[41] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura 및 Wenping Wang. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암묵적 표면 학습. arXiv 사전 인쇄본 arXiv:2106.10689, 2021. 1,[42] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron 및 Ira Kemelmacher-Shlizerman. HumanNeRF: 단안 비디오에서 움직이는 사람의 자유 시점 렌더링. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 16210-16220페이지, 2022년 6월.[43] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin. Bungeenerf: 극단적인 다중 스케일 장면 렌더링을 위한 진행적 신경 광도장. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022년.[44] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui. 편집 가능한 장면 렌더링을 위한 학습 객체-구성적 신경 광도장. 국제 컴퓨터 비전 컨퍼런스(ICCV), 2021년 10월. 2,[45] Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman. 신경 암묵적 표면의 볼륨 렌더링. 2021년 제35회 신경 정보 처리 시스템 컨퍼런스. 1,[46] 웨이차이 예, 슈오 첸, 총 바오, 후준 바오, 마크 폴레페이스, 자오펭 추이, 궈펭 장. Intrinsicnerf: 편집 가능한 새로운 뷰 합성을 위한 내재적 신경 광도장 학습. 2022.[47] 제하오 유, 송유 펭, 마이클 니마이어, 토르스텐 사틀러, 안드레아스 가이거. Monosdf: 신경 암묵적 표면 재구성을 위한 단안적 기하학적 단서 탐색. 신경 정보 처리 시스템(NeurIPS)의 발전, 2022.[48] 유-지에 위안, 양-톈 순, 위-쿤 라이, 위웬 마, 롱페이 지아, 린 가오. Nerf-editing: 신경 광도장의 기하 편집. Computer Vision and Pattern Recognition(CVPR), 2022. 2, 3,[49] Jing Zeng, Yanxu Li, Yunlong Ran, Shuo Li, Fei Gao, Lincheng Li, Shibo He, Jiming Chen, Qi Ye. 자율적 암묵적 재구성을 위한 효율적인 뷰 경로 계획. 2023 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA), 4063-4069페이지, 2023.[50] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. Nerf++: 신경 광도장 분석 및 개선. arXiv:2010.07492, 2020.[51] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, Jonathan T Barron. Nerfactor: 알려지지 않은 조명 하에서 모양과 반사율의 신경 인수 분해. ACM Transactions on Graphics(TOG), 40(6):1-18, 2021.[52] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely. 스테레오 확대: 다중 평면 이미지를 사용한 학습 뷰 합성. SIGGRAPH, 2018.[53] Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dianbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao, et al. 12-sdf: 신경 sdf에서 광선 추적을 통한 내재적 실내 장면 재구성 및 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12489-12498페이지, 2023.[54] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng 및 Rui Tang. 미분 가능한 몬테카를로 레이트레이싱을 사용하여 복잡한 실내 장면의 학습 기반 역 렌더링. SIGGRAPH Asia 2022 컨퍼런스 논문. ACM, 2022.
