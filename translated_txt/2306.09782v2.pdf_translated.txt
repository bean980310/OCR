--- ABSTRACT ---
대규모 언어 모델(LLM)은 자연어 처리(NLP)에 혁명을 일으켰지만 학습을 위해 엄청난 GPU 리소스를 요구합니다. LLM 학습에 대한 임계값을 낮추면 연구자의 참여가 늘어나 학계와 사회 모두에 이롭습니다. 기존 접근 방식은 소수의 매개변수를 조정하거나 추가하는 매개변수 효율적 미세 조정에 초점을 맞추었지만, 제한된 리소스로 LLM의 전체 매개변수를 조정하는 과제를 해결한 사람은 거의 없습니다. 이 연구에서는 그래디언트 계산과 매개변수 업데이트를 한 단계로 결합하여 메모리 사용량을 줄이는 새로운 최적화 도구인 LowMemory Optimization(LOMO)을 제안합니다. LOMO를 기존 메모리 절약 기술과 통합하여 표준 방식(DeepSpeed 솔루션)에 비해 메모리 사용량을 10.8%까지 줄였습니다. 결과적으로, 우리의 접근 방식은 각각 24GB 메모리를 갖춘 8×RTX 3090이 있는 단일 머신에서 65B 모델의 전체 매개변수 미세 조정을 가능하게 합니다.1
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 자연어 처리(NLP)에 혁명을 일으켜 출현 및 그로킹(Wei et al., 2022)과 같은 놀라운 능력을 보여주며 모델 크기가 점점 더 커지도록 했습니다. 그러나 30B~175B 매개변수와 같은 수십억 개의 매개변수로 이러한 모델을 학습시키면 NLP 연구의 기준이 높아집니다. LLM을 튜닝하려면 종종 8×80GB 장치와 같은 값비싼 GPU 리소스가 필요하기 때문에 소규모 연구실과 회사가 이 연구 분야에 참여하기 어렵습니다. 최근 LORA(Hu et al., 2022) 및 접두사 튜닝(Li and Liang, 2021)과 같은 매개변수 효율적인 미세 튜닝 방법(Ding et al., 2022)은 제한된 리소스로 LLM을 튜닝하기 위한 솔루션을 제공합니다. *교신 저자. ¹코드와 데이터는 https://github.com/OpenLMLab/LOMO에서 제공됩니다. 그러나 이러한 방법은 매개변수 효율적 미세 조정보다 더 강력한 접근 방식으로 인정된 전체 매개변수 미세 조정에 대한 실용적인 솔루션을 제공하지 못합니다(Ding et al., 2022; Sun et al., 2023). 이 연구에서는 리소스가 제한된 시나리오에서 전체 매개변수 미세 조정을 달성하기 위한 기술을 탐색하는 것을 목표로 합니다. LLM의 메모리 사용의 네 가지 측면, 즉 활성화, 최적화기 상태, 그래디언트 텐서 및 매개변수를 분석하고 세 가지 측면에서 학습 프로세스를 최적화합니다. 1) 알고리즘 관점에서 최적화기의 기능을 재고하고 SGD가 LLM의 전체 매개변수 미세 조정 측면에서 좋은 대체물임을 발견했습니다. 이를 통해 SGD는 중간 상태를 저장하지 않으므로 최적화기 상태의 전체 부분을 제거할 수 있습니다(3.1절). 2) 그림 1에 나와 있는 제안하는 최적화기인 LOMO는 그래디언트 텐서의 메모리 사용량을 가장 큰 그래디언트 텐서의 메모리 사용량과 동일한 O(1)로 줄입니다(3.2절). 3) LOMO를 사용한 혼합 정밀도 학습을 안정화하기 위해, 우리는 학습 중에 그래디언트 정규화, 손실 스케일링, 특정 계산을 완전한 정밀도로 전환합니다(3.3절). 우리의 기술은 매개변수 사용과 활성화, 가장 큰 그래디언트 텐서에 해당하는 메모리 사용량을 초래합니다. 우리는 완전한 매개변수 미세 조정의 메모리 사용량을 극단적으로 끌어올려 추론 사용과 동등하게 만듭니다. 이는 전방 후방 프로세스의 메모리 사용량이 전방 프로세스만의 메모리 사용량보다 적어서는 안 되기 때문입니다. LOMO를 사용하여 메모리를 절약할 때, 매개변수 업데이트 프로세스가 여전히 SGD와 동일하기 때문에 미세 조정 프로세스가 손상되지 않도록 해야 한다는 점에 유의해야 합니다. 우리는 LOMO의 메모리 및 처리량 성능을 경험적으로 평가하고, LOMO를 사용하면 RTX 3090 GPU 8개만으로 65B 모델을 성공적으로 학습할 수 있음을 보여줍니다. 또한, After Backward After Update PPPPPMrecordients O(n) Before Backward SGD GGGGGGPPPGGGFused Backward StepFused Update StepPPPPPPLOMO GGGGGGFused Backward StepFused Update StepPPPPPPMemory of Gradients 0(1) P GGGGGGIn Memory Off Memory Updated Parameter Fused Backward StepFused Update StepPPPPPPGGGGGG그림 1: 역전파 및 매개변수 업데이트 단계에서 SGD와 LOMO의 비교. Pi는 모델의 매개변수를 나타내고 Gi는 Pi에 해당하는 그래디언트를 나타냅니다. LOMO 융합 그래디언트 계산 및 매개변수 업데이트를 한 단계로 수행하여 그래디언트 텐서의 크기를 최소화합니다. 제안하는 기술의 다운스트림 성능을 검증하기 위해 LOMO를 적용하여 SuperGLUE 데이터 세트 컬렉션(Wang et al., 2019)에서 LLM의 전체 매개변수를 조정합니다. 경험적 결과는 수십억 개의 매개변수를 가진 LLM을 최적화하기 위한 LOMO의 효율성과 효과성을 보여줍니다. 전반적으로 우리의 기여는 다음과 같습니다. • 우리는 SGD가 LLM의 전체 매개변수를 성공적으로 미세 조정할 수 있다는 것을 시사하는 이론적 분석을 제공합니다. 이전에 SGD의 광범위한 사용을 방해했던 문제는 더 이상 LLM 미세 조정에 심각한 문제가 아닐 수 있습니다. • 우리는 미세 조정 프로세스를 손상시키지 않고 GPU 메모리 사용량을 크게 절약하기 위해 LOMO라는 이름의 LOW-Memory Optimization을 제안합니다. • 메모리 사용량과 처리량 성능에 대한 철저한 평가를 통해 리소스가 제한된 시나리오에서 LLM을 최적화하는 데 있어 LOMO의 효과를 경험적으로 검증합니다. 이는 다운스트림 작업에 대한 성능 평가로 더욱 뒷받침됩니다. 2
--- RELATED WORK ---
이 섹션에서는 전체 매개변수 미세 조정 중 메모리 절약 기술에 대한 관련 작업을 제시합니다. 이러한 기술은 LOMO와 효과적으로 결합하여 메모리 소비를 더욱 줄일 수 있습니다. 활성화 검사점 바닐라 역전파 동안 순방향 패스의 모든 활성화는 그래디언트를 계산하기 위해 메모리에 저장됩니다. 이는 특히 대규모 언어 모델의 경우 상당한 메모리 오버헤드가 될 수 있습니다. 또는 모든 활성화를 삭제하고 그래디언트 계산을 위해 필요에 따라 다시 계산하여 메모리를 절약할 수 있습니다. 그러나 이로 인해 상당한 추가 계산 비용이 발생할 수 있습니다. 활성화 검사점(또는 그래디언트 검사점)은 메모리 사용량과 계산 비용을 모두 고려하여 절충안 솔루션을 제공합니다(Chen et al., 2016). 계산 그래프에서 전략적으로 선택된 검사점 노드의 활성화는 순방향 패스 후에 메모리에 보관되는 반면 나머지 노드의 활성화는 최대 한 번만 다시 계산됩니다. 활성화 메모리는 추가 순방향 패스를 한 번 희생하여 원래 양의 제곱근으로 줄일 수 있습니다. 혼합 정밀도 학습 혼합 정밀도 학습은 학습 속도를 높이고 메모리 사용량을 줄일 수 있기 때문에 대규모 언어 모델을 학습하는 데 널리 사용되는 접근 방식이 되었습니다(Narayanan et al., 2021; Rajbhandari et al., 2020). 매개변수, 활성화 및 기울기에 반 정밀도 저장소를 사용함으로써 혼합 정밀도 학습은 고처리량 계산을 가능하게 합니다. 안정성과 모델 정확도를 유지하기 위해 Micikevicius et al.(2018)은 가중치의 전체 정밀도 사본 사용, 손실 스케일링 및 전체 정밀도에서 특정 산술 연산 실행을 포함하는 세 가지 기술을 제안했습니다. 이기종 학습 시스템 여러 연구(Rhu et al., 2016; Wang et al., 2018; Ren et al., 2021a)에서 CPU 및 NVMe 메모리와 같은 이기종 메모리를 활용하여 GPU 메모리 소비를 줄이려고 시도했습니다. L2L(Pudipeddi et al., 2020)은 계층 간 전략을 사용하는데, 여기서 특정 계층의 계산에 필요한 텐서만 GPU 메모리로 전송되고 나머지 텐서는 CPU 메모리에 보관됩니다. ZeRO-Offload(Ren et al., 2021b)는 ZeRO-2(Rajbhandari et al., 2020)의 확장으로, CPU 메모리에 그래디언트와 옵티마이저 상태를 예약하고 CPU 계산을 통해 매개변수를 업데이트합니다. ZeRO-Infinity(Rajbhandari et al., 2021)는 ZERO-3(Rajbhandari et al., 2020)의 ZeROOffload의 후속 발전으로, 모델 크기를 더욱 확장할 수 있습니다.
--- METHOD ---
LORA(Hu et al., 2022) 및 접두사 튜닝(Li and Liang, 2021)과 같은 s(Ding et al., 2022)는 제한된 리소스로 LLM을 튜닝하기 위한 솔루션을 제공합니다. 그러나 이러한 방법은 매개변수 효율적 미세 조정(Ding et al., 2022; Sun et al., 2023)보다 더 강력한 접근 방식으로 인정된 전체 매개변수 미세 조정에 대한 실용적인 솔루션을 제공하지 않습니다. 이 작업에서 우리는 리소스가 제한된 시나리오에서 전체 매개변수 미세 조정을 달성하기 위한 기술을 탐구하는 것을 목표로 합니다. 우리는 LLM에서 메모리 사용의 네 가지 측면, 즉 활성화, 최적화 상태, 그래디언트 텐서 및 매개변수를 분석하고 세 가지 측면에서 학습 프로세스를 최적화합니다.1) 알고리즘 관점에서 최적화기의 기능을 재고하고 SGD가 LLM의 전체 매개변수를 미세 조정하는 측면에서 좋은 대체물이라는 것을 발견했습니다.SGD는 중간 상태를 저장하지 않으므로 최적화기 상태의 전체 부분을 제거할 수 있습니다(3.1절).2) 그림 1에 나와 있는 제안된 최적화기인 LOMO는 그래디언트 텐서의 메모리 사용량을 가장 큰 그래디언트 텐서의 메모리 사용량과 동일한 O(1)로 줄입니다(3.2절).3) LOMO를 사용하여 혼합 정밀도 학습을 안정화하기 위해 그래디언트 정규화, 손실 스케일링을 통합하고 학습 중에 특정 계산을 전체 정밀도로 전환합니다(3.3절).우리의 기술은 매개변수 사용량과 활성화 및 가장 큰 그래디언트 텐서 사용량과 동일한 메모리 사용량을 생성합니다. 우리는 전체 매개변수 미세 조정의 메모리 사용량을 극단적으로 끌어올려 추론 사용과 동등하게 만들었습니다. 이는 전방 후방 프로세스의 메모리 사용량이 전방 프로세스만보다 적어서는 안 되기 때문입니다. LOMO를 사용하여 메모리를 절약할 때 매개변수 업데이트 프로세스가 여전히 SGD와 동등하기 때문에 미세 조정 프로세스가 손상되지 않도록 보장하는 것이 중요합니다. 우리는 LOMO의 메모리 및 처리량 성능을 경험적으로 평가하고 LOMO를 사용하면 RTX 3090 GPU 8개만으로 65B 모델을 성공적으로 훈련할 수 있음을 보여줍니다. 또한, After Backward After Update PPPPPMrecordients O(n) Before Backward SGD GGGGGGPPPGGGFused Backward StepFused Update StepPPPPPPLOMO GGGGGGFused Backward StepFused Update StepPPPPPPMemory of Gradients 0(1) P GGGGGGIn Memory Off Memory Updated Parameter Fused Backward StepFused Update StepPPPPPPGGGGGG그림 1: 역전파 및 매개변수 업데이트 단계에서 SGD와 LOMO의 비교. Pi는 모델의 매개변수를 나타내고 Gi는 Pi에 해당하는 그래디언트를 나타냅니다. LOMO 융합 그래디언트 계산 및 매개변수 업데이트를 한 단계로 수행하여 그래디언트 텐서의 크기를 최소화합니다. 제안하는 기술의 다운스트림 성능을 검증하기 위해 LOMO를 적용하여 SuperGLUE 데이터 세트 컬렉션(Wang et al., 2019)에서 LLM의 전체 매개변수를 조정합니다. 경험적 결과는 수십억 개의 매개변수가 있는 LLM을 최적화하기 위한 LOMO의 효율성과 효과성을 보여줍니다. 전반적으로 우리의 기여는 다음과 같습니다. • SGD가 LLM의 전체 매개변수를 성공적으로 미세 조정할 수 있음을 시사하는 이론적 분석을 제공합니다. 이전에 SGD의 광범위한 사용을 방해했던 문제는 더 이상 LLM 미세 조정에 심각한 문제가 아닐 수 있습니다. • 미세 조정 프로세스에 해를 끼치지 않고 GPU 메모리 사용량을 크게 절약하기 위해 LOMO라는 LOW-Memory Optimization을 제안합니다. • 메모리 사용량과 처리량 성능에 대한 철저한 평가를 통해 리소스가 제한된 시나리오에서 LLM을 최적화하는 데 있어 LOMO의 효과를 경험적으로 검증합니다. 이는 다운스트림 작업에 대한 성능 평가로 더욱 뒷받침됩니다. 2 관련 작업 이 섹션에서는 전체 매개변수 미세 조정 중 메모리 절약 기술에 대한 관련 작업을 제시합니다. 이러한 기술은 LOMO와 효과적으로 결합하여 메모리 소비를 더욱 줄일 수 있습니다. 활성화 체크포인팅 바닐라 역전파 동안, 전방 패스의 모든 활성화는 그래디언트를 계산하기 위해 메모리에 저장됩니다. 이는 특히 대규모 언어 모델의 경우 상당한 메모리 오버헤드가 될 수 있습니다. 또는 모든 활성화를 삭제하고 그래디언트 계산을 위해 필요에 따라 다시 계산하여 메모리를 절약할 수 있습니다. 그러나 이는 상당한 추가 계산 비용을 초래할 수 있습니다. 활성화 체크포인팅(또는 그래디언트 체크포인팅)은 메모리 사용량과 계산 비용을 모두 고려하여 절충안을 제공합니다(Chen et al., 2016). 계산 그래프에서 전략적으로 선택된 체크포인트 노드의 활성화는 전방 패스 후에 메모리에 보관되는 반면, 나머지 노드의 활성화는 최대 한 번만 다시 계산됩니다. 활성화 메모리는 추가 전방 패스를 한 번 희생하여 원래 양의 제곱근으로 줄일 수 있습니다. 혼합 정밀도 학습 혼합 정밀도 학습은 학습 속도를 높이고 메모리 사용량을 줄일 수 있기 때문에 대규모 언어 모델을 학습하는 데 널리 사용되는 접근 방식이 되었습니다(Narayanan et al., 2021; Rajbhandari et al., 2020). 매개변수, 활성화 및 기울기에 반 정밀도 저장소를 사용함으로써 혼합 정밀도 학습은 고처리량 계산을 가능하게 합니다. 안정성과 모델 정확도를 유지하기 위해 Micikevicius et al.(2018)은 가중치의 전체 정밀도 사본 사용, 손실 스케일링 및 전체 정밀도에서 특정 산술 연산 실행을 포함하는 세 가지 기술을 제안했습니다. 이기종 학습 시스템 여러 연구(Rhu et al., 2016; Wang et al., 2018; Ren et al., 2021a)에서 CPU 및 NVMe 메모리와 같은 이기종 메모리를 활용하여 GPU 메모리 소비를 줄이려고 시도했습니다. L2L(Pudipeddi et al., 2020)은 계층 간 전략을 사용하는데, 여기서 특정 계층의 계산에 필요한 텐서만 GPU 메모리로 전송되고 나머지 텐서는 CPU 메모리에 보관됩니다. ZeRO-Offload(Ren et al., 2021b)는 ZeRO-2(Rajbhandari et al., 2020)의 확장으로, CPU 메모리에 그래디언트와 옵티마이저 상태를 예약하고 CPU 계산을 통해 매개변수를 업데이트합니다. ZeRO-Infinity(Rajbhandari et al., 2021)는 ZERO-3(Rajbhandari et al., 2020)의 ZeROOffload의 후속 발전으로, 모델 크기를 더욱 확장할 수 있습니다. 위에서 언급한 LOMO와 직교하는 방법 외에도 최근 개발로 인해 여러 가지 메모리 효율적인 최적화 기술이 도입되었습니다. MeZO(Malladi et al., 2023)는 영차 최적화 접근 방식을 사용하여 두 개의 순방향 패스를 사용하여 그래디언트를 추정하고 매개변수를 제자리에서 업데이트합니다.GaLore(Zhao et al., 2024)는 그래디언트에 대해 저순위 분해를 수행하고 이러한 근사화된 그래디언트를 매개변수 업데이트에 사용합니다.다른 접근 방식은 최적화기 상태를 양자화하여 메모리 사용량을 줄입니다(Dettmers et al., 2022; Sun et al., 2020b).이러한 방법과 비교할 때 LOMO는 그래디언트를 근사하지도 않고 저비트 양자화를 요구하지도 않습니다.3 방법 3.1 최적화기의 기능 재고 최적화기 상태는 LLM 학습에 사용되는 메모리의 상당 부분을 차지합니다.Adam(Kingma and Ba, 2015)과 같은 최신 최적화기는 매개변수 크기의 두 배인 중간 상태를 저장합니다.매개변수의 크기가 증가함에 따라 최적화기 상태가 메모리 사용의 지배적인 용어가 됩니다. 3.1.1 SGD 사용 Adam은 심층 모델 학습에서 큰 성공을 거두었지만, 우리는 &quot;LLM 미세 조정을 위해 더 저렴한 최적화 도구를 사용할 수 있을까?&quot;라는 질문을 던집니다. 우리의 답은 가장 기본적인 최적화 도구인 SGD입니다. 다행히도 범위를 제한할 때 LLM 미세 조정을 위한 허용 가능한 솔루션이라는 것을 알게 되었습니다. 이전 연구에서는 종종 SGD의 세 가지 과제, 즉 1) 큰 곡률 손실 표면, 2) 국소 최적해, 3) 안장점(Ruder, 2016; Sun et al., 2020a)에 대해 논의합니다. 최신 최적화 도구는 1) 문제를 처리하는 데 효과적임을 보여주었으며 어떤 경우에는 2)와 3)을 완화할 수 있습니다. 그러나 범위를 LLM 미세 조정으로 제한할 때 이 세 가지 과제는 다를 수 있습니다. 더 부드러운 손실 표면 한 가지 중요한 가정은 LLM의 매개변수 공간이 매우 매끄럽고 매개변수에 대한 작은 섭동이 손실을 너무 많이 변경하지 않는다는 것입니다. 경험적 결과와 이론적 분석이 이 가정을 뒷받침합니다(Hao et al., 2019). 더 큰 모델이 더 매끄러운 손실 표면을 갖는다고 믿는다면, LLM의 손실 표면은 큰 곡률을 가져서는 안 되므로 1) 문제는 문제가 되지 않는다는 결론을 내릴 수 있습니다. 이는 LLM에 자연어 기반 작업(또는 코드로 사전 학습된 경우 코드 기반)을 가르칠 때에만 성립합니다. 사전 학습 작업과 관련 없는 합성 손실 함수는 실제로 큰 곡률 문제에 직면하게 됩니다. 국소 최적값이 충분히 좋습니다. 미세 조정의 목표는 모델 자체를 크게 변경하지 않고도 LLM을 새로운 작업과 도메인에 적응시키는 것입니다. 따라서 국소 최적값이 종종 충분히 좋은 솔루션(Kawaguchi et al., 2019)이고, 제한된 학습 데이터(사전 학습 코퍼스와 비교)로 인해 모델을 멀리 떨어진 전역 최적값으로 푸시하기 어렵습니다. 먼 안장점 마찬가지로 일반적인 NLP 작업의 경우 LLM의 초기점은 계곡에 있어야 합니다. 모델이 명령어(작업)로 사전 학습된 경우, 새 작업과 유사한 사전 학습된 작업을 찾을 가능성이 더 높기 때문에 이 현상이 훨씬 더 분명해질 수 있습니다. 안장점은 일반적으로 능선에 나타나고 계곡과 거리가 있으므로 매개변수를 사전 학습된 값에서 너무 멀리 변경하지 않으면 안장점 문제가 발생하지 않을 수 있습니다. 그러나 SGD가 최신 최적화 도구에 비해 강력한 최적화 도구라는 보장은 없습니다. 우리의 직관은 LLM을 미세 조정하기 위한 간단하고 실용적인 솔루션을 만들고 결함을 식별하여 지속적으로 개선하는 것입니다. 3.1.2 암묵적 배치 크기 위의 정성적 논의 외에도 SGD로 LLM을 미세 조정하는 것의 안정성에 대한 심층 분석을 제공하고자 합니다. 사전 학습된 모델 f(.)가 매개변수 0, 학습 세트 D {d1, d2, .., dn}, 손실 함수 L을 갖는다고 가정합니다. 두 개의 데이터 포인트가 있는 배치에 대한 SGD의 한 단계 업데이트는 다음과 같습니다. 0&#39; = 0 – α[VL(di, f(di, 0)) + VL(dj, f(dj, 0))], 알고리즘 1 LOMO의 퓨전 업데이트 필요 사항: L개 레이어와 p개 매개변수가 있는 모델 f(·), 매개변수 ✪ Є R³, 학습률 a, 최대 단계 T, 학습 데이터 세트 D, 손실 함수 L 1: t = 2의 경우: 1,..., T do 샘플 배치 B = (x, y) CD = 01 + [0; 0의 경우; 레이어 1] 3: ŷ ← f(x, 0) ▷ 정방향 패스 4: l← L(y, ŷ) 5: L,..., 1에 대해 do ▷ 역방향 6: (1) де 7: 91 ← მ8: θι + θι - α * gi 9: 없음 ▷ 명확한 기울기 종료 여기서 a는 학습률이고 di, dj는 두 개의 다른 학습 샘플입니다. 다음으로, 이 두 학습 샘플 di, dj에 대한 SGD의 두 단계 순차적 업데이트는 010-aVL(di, f(di, = ,0)), (2) = 0201 aVL(dj, f (dj, 01))이 될 수 있습니다. (3) 10: 11: 끝 미분 평균값 정리에 의해 L(dj, f(dj, 01)) = L(dj, f(dj, 0)) + VL(dj, §)(f(dj, 0₁) — ƒ (d;, 0)), 02 = 0 aVL(di, f (di, 0)) - aVL(dj, f(dj, 0)) (4) - aVVL(dj, §)(ƒ (d;, 01) – ƒ (d;, 0))], (5) 02 = 0 – a[VL(di, f (di, 0)) + VL(dj, f(dj, 0))] — aV[VL(dj, §)(fƒ (dj, 01) — ƒ (dj, 0))], (6) 여기서 ε는 f(d;, 0)과 f(dj, 01), 그리고 (6)에서 (1)을 뺀 것이 aV[VL(dj,§)(f(dj,01) — ƒ(dj,¤))]와 같음을 알 수 있습니다. 손실 표면이 충분히 매끄럽다고 가정하면 이 항은 무시할 수 있게 됩니다. 이는 매끄러운 손실 표면 위에 SGD 최적화기를 활용하면 더 큰 배치 크기를 의미할 수 있음을 시사합니다. 위에서 언급했듯이 LLM의 손실 표면이 매끄럽다고 가정하는 것이 합리적이며, 더 큰 배치 크기는 더 강한 학습 안정성을 나타내므로 SGD 최적화기를 사용한 LLM의 미세 조정 프로세스는 안정적이라고 믿습니다. 이는 또한 SGD가 작은 모델에서는 실패했지만 큰 모델에서는 작동하는 이유를 설명합니다. 3.2 LOMO: LOW-메모리 최적화 그래디언트 텐서는 매개변수 텐서의 그래디언트를 나타내며 매개변수와 같은 크기를 가지므로 큰 메모리 오버헤드가 발생합니다. PyTorch(Paszke et al., 2017)와 같은 최신 딥 러닝 학습 프레임워크는 모든 매개변수에 대한 그래디언트 텐서를 저장합니다. 그래디언트 텐서를 저장하는 데는 두 가지 이유가 있습니다. 최적화기 상태 계산과 그래디언트 정규화입니다. SGD를 최적화기로 사용하기 때문에 그래디언트에 의존하는 최적화기 상태가 없으며 그래디언트 정규화에 대한 몇 가지 대안이 있습니다. 따라서 알고리즘 1에 설명된 대로 저메모리 최적화(LOMO)를 제안하여 그래디언트 계산과 매개변수 업데이트를 한 단계로 융합하여 그래디언트 텐서를 저장하지 않도록 했습니다. 자세히 말하면, 바닐라 그래디언트 하강을 grad ᎧᏝ = P = p-lr*grad로 표현할 수 있습니다. 이는 두 단계 프로세스로, 먼저 그래디언트를 계산하고 이를 매개변수로 업데이트합니다. 융합 버전은 p = p-lr ас др입니다. 핵심 아이디어는 그래디언트가 계산되면 매개변수를 즉시 업데이트하여 메모리에 그래디언트 텐서를 저장하지 않는 것입니다. 이는 역방향 전파에 후크 함수를 주입하여 달성할 수 있습니다.² PyTorch는 후크 함수를 주입하기 위한 관련 API를 제공하지만 현재 API로는 정확한 즉각적인 업데이트를 구현할 수 없습니다. 대신 최대 하나의 매개변수의 그래디언트를 메모리에 저장하고 역방향 전파와 함께 각 매개변수를 하나씩 업데이트합니다. 저희의 접근 방식은 모든 매개변수의 그래디언트를 저장하는 것에서 하나의 매개변수의 그래디언트만 저장하는 것으로 그래디언트의 메모리 사용량을 줄입니다. 대부분의 LOMO 메모리 사용량은 매개변수 효율적 미세 조정(PEFT) 방법과 일치하며, 이는 LOMO를 2와 결합한다는 것을 나타냅니다. 일부 가중치가 공유되는 경우 이에 따라 다른 후크 함수를 주입해야 합니다. 이러한 방법은 그래디언트가 차지하는 메모리를 약간만 증가시킵니다. 이를 통해 PEFT 방법에 대해 훨씬 더 많은 매개변수를 조정할 수 있습니다. 3.3 LOMO를 사용한 훈련 안정화 3.3. 그래디언트 정규화 및 클리핑의 대안 그래디언트 정규화 및 클리핑은 그래디언트 폭발 및 소멸 문제(Chen et al., 2018)를 처리하는 데 필수적인 도구이지만, 이를 계산하는 과정에서는 모든 매개변수의 그래디언트 텐서를 사용해야 합니다. 여기서 두 가지 대안을 제안합니다. • 노름이 아닌 값으로 그래디언트 텐서를 클리핑합니다. • 추가 역방향 패스에서 그래디언트 노름을 계산합니다. 그래디언트 텐서를 값으로 클리핑하는 것은 그래디언트 노름 접근 방식 이전의 그래디언트 폭발에 대한 간단하지만 효과적인 솔루션입니다. 값으로 클리핑하는 것의 주요 관심사는 일부 그래디언트 요소를 잘라내면 그래디언트 텐서의 방향이 변경될 수 있다는 것입니다. 예를 들어, 2차원 벡터 [1.3, 0.8]과 이를 클리핑한 버전 [1.0,0.8](1.0으로 클리핑)은 다른 방향을 나타냅니다. 저희의 경험에 따르면 학습률이 높을 때 값으로 클리핑하는 것이 성능이 떨어지는데, 그 이유는 이 경우 잘림이 더 자주 발생하기 때문입니다. 그러나 값에 따른 클리핑은 중간 및 작은 학습 속도에서 잘 수행됩니다. 학습 속도의 규모는 작업과 데이터에 따라 크게 달라지지만 일반적으로 학습 속도가 le - - 3 미만인 경우 값에 따른 클리핑을 사용하는 것이 좋습니다. 우리의 접근 방식은 역전파와 함께 매개변수를 업데이트하기 때문에 그래디언트 노름을 직접 계산할 수 없으므로 특정 매개변수를 업데이트할 때 나머지 매개변수의 노름을 알 수 없습니다. 그러나 각 매개변수의 그래디언트 노름을 계산하고 누적하기 위한 추가 패스를 도입하여 그래디언트 노름을 계산하는 하나와 매개변수를 업데이트하는 하나의 두 개의 역방향 패스를 생성할 수 있습니다. 메모리 사용량은 변경되지 않지만 속도는 희생됩니다. 논란의 여지가 있는 솔루션 현재의 학습 프레임워크는 모든 매개변수를 기반으로 그래디언트 노름을 계산하며 두 개의 역방향 패스가 필요합니다. 추가 역방향 패스를 저장하는 한 가지 솔루션은 매개변수 그룹(예: 인접 레이어)으로 그래디언트 텐서의 노름을 근사하는 것입니다. 이 방법은 실제로 편향되어 있습니다. 다른 매개변수에 대해 다른 업데이트 단계 크기가 발생하기 때문입니다. 업데이트할 때 매개변수는 그래디언트 노름에 따라 스케일 인수로 곱해집니다. 그래디언트 규범이 매개변수 그룹마다 다르기 때문에 이러한 근사값은 스케일 계수의 차이로 이어진다. 이 그룹화된 그래디언트 클리핑 방법은 그래디언트 규범에 따라 서로 다른 매개변수 그룹에 동적 학습률을 적용하는 것으로 볼 수 있다. Sun et al. (2020a)은 SGD에서 모든 매개변수에 동일한 학습률을 사용하는 것이 항상 적절한 것은 아니라고 제안하므로, 우리의 접근 방식이 SGD에 더 많은 이점을 제공할 잠재력이 있다고 믿는다. 우리는 이 탐구를 설득력 있는 미래 방향으로 남겨둔다. 3.3.2 정밀도 저하 완화 혼합 정밀도 학습은 일반적으로 학습 프로세스를 가속화하는 데 사용된다. 정밀도 저하를 완화하기 위해 동적 손실 스케일링을 활용하고 특정 계산을 전체 정밀도로 전환한다. 손실 스케일링 접근 방식은 FP16 학습 중 언더플로를 방지하고, 역방향 패스 전에 특정 요인으로 손실을 확대하고, 같은 요인으로 그래디언트를 줄이는 데 중요하다. 이러한 맥락에서 우리는 동적 손실 스케일러를 LOMO와 통합하여 학습 절차 전반에 걸쳐 스케일링 요인을 동적으로 조정한다. 지정된 수의 역방향 패스 동안 오버플로가 발생하지 않으면 스케일 인수가 두 배가 됩니다. 그렇지 않으면 이 단계는 삭제되고 스케일 인수가 반으로 줄어듭니다. 이 프로세스는 그래디언트 정규화 중에 발생한 시나리오를 반영합니다. 역방향이 완료될 때까지 오버플로가 있는지 여부는 알 수 없습니다. 따라서 두 개의 역방향 패스를 수행합니다. 첫 번째 패스는 오버플로를 식별하고 두 번째 패스는 오버플로가 감지되지 않으면 매개변수를 업데이트합니다. 동적 손실 스케일링을 위한 이 두 역방향 패스는 그래디언트 정규화와 동시에 실행할 수 있습니다. 정규화 및 스케일링과 같은 작업에 대한 매개변수를 효과적으로 업데이트하고 그래디언트를 처리하기 위해 그래디언트와 연관된 매개변수는 이러한 계산 내에서 전체 정밀도로 변환됩니다. 4
--- EXPERIMENT ---
이 섹션에서는 메모리 프로필, 처리량 및 다운스트림 성능의 세 가지 측면에서 제안하는 방법을 평가합니다.추가로 설명하지 않는 경우 모든 실험은 7B에서 65B까지의 LLAMA 모델(Touvron et al., 2023)로 수행됩니다.최적화자 상태 73.7% 12.3% 기울기 매개변수 12.3% 1.8% 활성화 기울기 매개변수 24.1% 매개변수 24.1% 86.1% 48.3% 3.4% 활성화 12.3% 1.6% 활성화 기울기 (a) AdamW 최적화자 상태로 학습 (b) SGD로 학습 (c) LOMO로 학습 그림 2: LLaMA-7B를 학습하기 위해 다른 최적화자를 사용할 때 각 부분의 메모리 사용 비율.시퀀스 길이와 배치 크기는 각각 512와 8로 설정됩니다. AC 매개변수 기울기 최적 상태 활성화 총 메모리 ☑ 45.147.Adam W 12.12.75.1.102.☑ 45.96.SGD 12.12.25.✓ 1.51.Х 45.59.LOMO 12.0.0.1.14.표 1: 다양한 설정에서 LLaMA-7B를 학습할 때의 메모리 사용량(GB).AC는 활성화 검사점을 의미합니다.시퀀스 길이와 배치 크기는 각각 512와 8로 설정됩니다.4.1 메모리 프로파일 먼저 다양한 설정에서 학습하는 동안 모델 상태와 활성화의 메모리 사용량을 프로파일링합니다. 표 1에서 보여준 것처럼, LOMO 옵티마이저를 사용하면 LLaMA-7B 모델을 학습하는 맥락에서 AdamW 옵티마이저(Loshchilov 및 Hutter, 2019)와 비교했을 때 메모리 사용량이 102.20GB에서 14.58GB로 상당히 감소하고, SGD와 비교했을 때는 51.99GB에서 14.58GB로 상당히 감소합니다. 메모리 사용량이 크게 감소한 것은 주로 그래디언트 및 옵티마이저 상태의 메모리 요구 사항이 감소했기 때문입니다. 그 결과 메모리는 대부분 추론 중 메모리 사용량과 비례하여 학습 프로세스의 매개변수에 의해 차지됩니다. 옵티마이저 상태 그림 2는 널리 채택된 구성인 LLaMA-7B 학습에 AdamW 옵티마이저를 사용하면 상당 비율의 메모리(73.7%)가 옵티마이저 상태에 할당되는 것을 보여줍니다. 이 결과는 가중치 업데이트를 위해 가중치, 모멘텀 및 분산의 전체 정밀도 사본이 최적화기 상태 내에 유지되는 혼합 정밀도 학습 접근 방식의 결과입니다.AdamW 최적화기를 SGD 최적화기로 대체하면 메모리에서 최적화기 상태의 비율을 효과적으로 줄일 수 있으므로 GPU 메모리 사용량(102.20GB에서 51.99GB로)이 완화됩니다.이 감소는 SGD 최적화기가 전체 정밀도 모멘텀과 분산을 저장할 필요가 없기 때문입니다.LOMO의 경우 매개변수 업데이트와 역방향이 한 단계로 융합되어 최적화기 상태 메모리가 더 이상 필요 없습니다.그래디언트 LOMO를 사용한 학습 프로세스 중에 매개변수는 그래디언트를 수신하자마자 즉시 업데이트되고 그 후 그래디언트는 메모리에서 삭제됩니다.결과적으로 그래디언트 메모리 소비의 상한은 가장 큰 크기의 매개변수 행렬과 관련된 그래디언트에 의해 결정됩니다.이 접근 방식은 매개변수 크기만큼 메모리 사용량을 상당히 줄입니다. 활성화 한 배치에서 512×8 토큰이 있는 7B 모델을 훈련하려면 활성화를 위해 상당한 양의 메모리가 필요합니다.LOMO는 활성화 체크포인팅과 같은 활성화 메모리 감소 기술과 호환됩니다.활성화 체크포인팅을 LOMO와 통합하면 활성화로 인한 메모리 사용량을 줄일 수 있습니다.매개 변수 최적화 하드웨어 메모리(GB) 처리량(TGS) 7B Adam W 8 × RTX15.67.7B SGD 8 × RTX9.69.7B LOMO 1 x RTX13.769.13B SGD 8 × RTX15.32.13B LOMO 2 x RTX15.66.30B LOMO 4 × RTX19.11.65B LOMO 8 x RTX19.4.표 2: RTX 3090 GPU 8개가 있는 서버에서 테스트한 처리량.시퀀스 길이와 배치 크기는 각각 1024와 1로 설정되었습니다. 메모리는 학습 중 GPU당 할당된 최대 메모리를 나타냅니다.처리량은 각 GPU에서 초당 처리하는 토큰 수(TGS)를 나타냅니다.45.61GB에서 1.79GB로 증가했습니다.4.처리량 LOMO의 처리량 성능을 AdamW 및 SGD와 비교하여 평가합니다.실험은 PCIe 마더보드를 통해 상호 연결된 8개의 RTXGPU가 장착된 서버에서 수행됩니다.시퀀스 길이와 배치 크기는 각각 1과 1로 설정됩니다.처리량은 초당 GPU당 처리되는 토큰 수(TGS)로 측정되며 매개변수 분할은 ZeRO-3(Rajbhandari et al., 2020)를 사용하여 달성되었습니다.7B 모델의 경우 LOMO는 AdamW 및 SGD보다 약 11배 높은 놀라운 처리량을 보여줍니다.이러한 상당한 개선은 LOMO가 단일 GPU에서 7B 모델을 학습하여 GPU 간 통신 오버헤드를 줄일 수 있는 능력에 기인할 수 있습니다. AdamW에 비해 SGD의 처리량이 약간 더 높은 것은 SGD에서 모멘텀과 분산 계산을 제외했기 때문일 수 있습니다. 13B 모델의 경우 메모리 제한으로 인해 사용 가능한 8개 RTX 3090 GPU에서 AdamW로 학습할 수 없었습니다. LOMO에 모델 병렬성이 필요한 이 시나리오에서 LOMO는 처리량 면에서 여전히 SGD보다 성능이 뛰어납니다. 이러한 장점은 LOMO의 메모리 효율적 속성과 동일한 설정으로 모델을 학습하는 데 두 개의 GPU만 필요하기 때문에 통신 비용이 감소하고 처리량이 증가하기 때문입니다. 또한 30B 모델을 학습할 때 SGD는 8개 RTX 3090 GPU에서 메모리 부족(OOM) 문제가 발생하는 반면 LOMO는 4개 GPU만으로도 좋은 성능을 보입니다. 마지막으로 8개 RTX 3090 GPU를 사용하여 65B 모델을 성공적으로 학습하여 4.93 TGS의 처리량을 달성했습니다. 이러한 서버 구성과 LOMO를 활용하면 각각 512개의 토큰을 포함하는 1000개 샘플에 대한 학습 프로세스에는 약 3.6시간이 필요합니다. 4.3 다운스트림 성능 대규모 언어 모델을 미세 조정하는 데 LOMO의 효과를 평가하기 위해 광범위한 실험을 수행합니다. 미세 조정이 필요 없는 Zero-shot과 현재 가장 인기 있는 매개변수 효율적 미세 조정 기술 중 하나인 LoRA의 두 가지 다른 방법과 LOMO를 비교합니다. (Hu et al., 2022)에서 설명한 대로 LORA는 밀집 계층을 다시 매개변수화하고 추론 중에 지연 시간을 도입하지 않으면서 낮은 순위 행렬만 업데이트합니다. 우리는 SuperGLUE 데이터 세트 컬렉션을 사용하여 모델 성능을 평가하며, 특히 RTE(Dagan et al., 2005), BoolQ(Clark et al., 2019), WSC(Levesque et al., 2012), WIC(Pilehvar and Camacho-Collados, 2019), MultiRC(Khashabi et al., 2018), COPA(Roemmele et al., 2011)에 초점을 맞춥니다. 대규모 언어 모델을 실행하는 데 따른 높은 계산 비용을 감안하여 MeZO(Malladi et al., 2023)에 따라 학습 세트에서 1000개의 학습 데이터와 검증 세트에서 1000개의 테스트 데이터를 무작위로 샘플링하고 동일한 난수 시드를 사용하여 얻은 최상의 결과를 보고합니다. 실험에 사용된 프롬프트는 MeZO와 동일하며 하이퍼파라미터는 부록 A에 자세히 나와 있습니다. 추론하는 동안, 우리는 프롬프트에 다른 레이블이나 후보를 삽입하고 각 레이블에 대한 평균 로그 우도를 계산합니다. 가장 높은 점수를 가진 레이블이 모델의 답으로 선택됩니다. 성능을 평가하기 위해, 우리는 정확도를 평가 지표로 사용합니다. 방법 매개변수 RTE BoolQ WSC WIC MultiRC COPA Avg. 제로샷 LORA 7B 57.7B 85.66.85.36.5 49.64.4 65.42.85.0 56.84.87.0 78.LOMO 7B 86.87.66.4 71.84.89.80.제로샷 13B 60.65.36.5 49.43.88.57.LORA LOMO 13B 89.87.63.69.86.92.81.13B 89.87.75.74.85.93.84.제로샷 30B 53.74.36.50.46.89.58.LORA 30B 91.89.83.7 74.87.93.86.LOMO 30B 92.89.85.74.87.93.87.Zero-shot 65B 59.73.44.51.48.91.61.LORA 65B 93.90.88.5 74.90.97.89.LOMO 65B 93.90.92.3 75.89.97.89.표 3: 모든 크기에서 LLaMA를 사용한 SuperGLUE의 주요 결과(1,000개의 학습 예제 포함).4.3.1 주요 결과 Zero-shot 및 LORA와 비교한 LOMO의 다운스트림 성능은 표 3에 나와 있습니다.결과를 바탕으로 다음과 같은 관찰 결과를 얻었습니다.LOMO는 Zero-shot보다 훨씬 더 나은 성능을 보입니다.모든 6개의 데이터 세트와 모델 크기에서 LOMO는 Zero-shot보다 지속적으로 우수한 결과를 달성했으며 LLaMA-13B를 사용하여 평균 이득이 포인트 이상이었습니다. 이전 연구에서는 제로샷 설정에서 대규모 언어 모델의 인상적인 기능을 보여주었지만, 미세 조정은 여전히 특정 다운스트림 작업에 대해 놀라운 성능 향상을 가져옵니다. 실험 결과는 다양한 크기의 대규모 언어 모델을 최적화하는 데 LOMO의 효과를 확인합니다. LOMO는 일반적으로 대부분의 실험에서 LORA보다 성능이 뛰어납니다. 예를 들어 LOMO가 LORA에 비해 강력한 성능을 제공하여 LLaMA13B를 사용하여 평균 2.8포인트의 이득을 얻는다는 것을 보여줍니다. 이는 모델 성능이 매개변수 효율적 미세 조정보다 전체 매개변수 미세 조정에서 더 많은 이점을 얻는다는 것을 시사합니다. 전자가 더 많은 매개변수를 조정하기 때문입니다. LOMO는 성능과 효율성 사이에서 좋은 균형을 이루므로 미세 조정에 경쟁력 있는 선택입니다. 어떤 경우에는 LOMO가 LORA보다 성능이 떨어집니다. 한 가지 가능한 이유는 사용하는 비교적 작은 학습 세트로, 대규모 모델의 전체 매개변수 미세 조정에 충분하지 않을 수 있습니다. 또한 LORA와 LOMO는 서로 다른 모델 아키텍처를 사용합니다. 구체적으로, LORA는 특정 시나리오에서 유리할 수 있는 모델 튜닝을 위한 단축키를 제공합니다. 실제로 이 두 가지 방법은 상충되거나 상호 배타적이지 않습니다. 다음 하위 섹션에서는 LORA와 LOMO를 결합해도 모델 성능에 해가 되지 않고 대부분의 경우 성능이 향상됨을 확인합니다. LOMO는 최대 650억 개의 매개변수 모델까지 효율적으로 확장합니다. 8× RTX 3090이 장착된 단일 머신에서 모든 실험을 수행했음에도 불구하고 LOMO는 65개 매개변수 규모에서도 일관되게 강력한 성능을 보여줍니다. 이는 리소스가 제한된 시나리오에서 LLM을 최적화하는 데 있어 LOMO의 효과를 더욱 뒷받침합니다. 4.3.2 LORA와 LOMO LOMO와 LORA는 근본적으로 서로 독립적입니다. 이 주장을 확인하기 위해 BoolQ 및 MultiRC 데이터 세트에서 LLAMA-13B를 사용하여 실험을 수행합니다. 결과는 그림 3에 나와 있습니다. LOMO는 LORA가 달성한 더 높은 결과에 관계없이 LoRA의 성능을 일관되게 향상시킨다는 것을 알 수 있습니다. 이는 LOMO와 LORA가 사용하는 다양한 미세 조정 방법이 상호 보완적임을 시사합니다. 구체적으로 LOMO는 사전 훈련된 모델 가중치를 미세 조정하는 데 중점을 두는 반면 LORA는 추가 모듈을 조정합니다. 결과적으로 LOMO는 LORA의 성능을 저하시키지 않습니다. 오히려 다운스트림 작업에 대한 더 나은 모델 조정을 용이하게 합니다. 5
--- CONCLUSION ---
이 논문에서는 대규모 lan에 대한 전체 매개변수 미세 조정을 용이하게 하도록 설계된 새로운 최적화 도구인 LOw-Memory Optimization(LOMO)을 소개합니다.정확도(%) 89.87.LORA LORA LORA+LOMO LORA+LOMO 86.88.88.88.87.87.87.86.87.87.87.87.87.87.정확도(%) 86.86.86.86.86.86.86.85.85.85.85.84.586.w/o LORALora 어텐션 차원 (a) BoolQ 84.w/o LORALora 어텐션 차원 (b) MultiRC 그림 3: BoolQ 및 MultiRC 데이터 세트에서 LLaMA-13B를 사용한 결과(1,000개의 학습 예제 사용). &quot;LORA+LOMO&quot;는 LOMO를 사용하여 사전 학습된 모델 가중치를 미세 조정하는 동안 LoRA 모듈을 주입하는 것을 의미합니다. 리소스가 제한된 게이지 모델입니다. RTX 3090과 같은 소비자용 GPU가 장착된 서버에서 65B 모델을 미세 조정할 수 있는 가능성을 보여주었습니다. LOMO의 메모리 사용량을 분석하고, 처리량 테스트를 수행하고, SuperGLUE에 대한 실험을 수행하여 효과와 잠재적 영향을 보여주었습니다. 앞으로의 작업은 대규모 언어 모델을 학습하는 데 필요한 리소스 임계값을 더욱 낮추어 이러한 모델에 대한 더 폭넓은 액세스와 채택을 가능하게 하는 것입니다. 현재 LOMO로 학습할 때 대부분의 메모리는 매개변수로 점유됩니다. 따라서 유망한 방향 중 하나는 메모리 사용량을 크게 줄일 수 있는 매개변수 양자화 기술을 탐구하는 것입니다. 또한 LOMO에 더 적용 가능한 시나리오를 조사하고 이 분야를 발전시키는 데 상당한 가치가 있는 대규모 언어 모델을 최적화하기 위한 이론적 분석을 탐구할 계획입니다. 제한 사항 그래디언트 정규화 및 클리핑과 관련된 과제에 대응하여 대체 최적화 방법을 개발했습니다. LOMO에 대한 그래디언트 정규화가 메모리 사용량을 증가시키지는 않지만, 현재 구현에서는 추가적인 역방향 패스가 필요하여 그래디언트 정규화가 필수적인 시나리오에서 학습 속도가 느려질 수 있습니다.시간 및 리소스 제약으로 인해 실험은 SuperGLUE 벤치마크의 하위 집합으로 제한되었으며 A100과 같은 고급 GPU에서 LOMO의 처리량을 평가하지 않았습니다.윤리적 성명 이 논문은 각각의 라이선스를 준수하여 오픈 소스 모델 LLAMA를 사용합니다.RTE, BoolQ, WSC, WIC, MultiRC 및 COPA를 포함한 사용된 데이터 세트는 공개 및 무료 사용을 허용합니다.감사의 말 이 연구는 중국 국가중점연구개발계획(No.2022ZD0160102)의 지원을 받았습니다.이 연구의 계산은 복단대학의 CFFF 플랫폼을 사용하여 수행되었습니다.참고문헌 Tianqi Chen, Bing Xu, Chiyuan Zhang 및 Carlos Guestrin.2016. 선형 이하 메모리 비용으로 딥 네트 학습. arXiv 사전 인쇄본 arXiv:1604.06174. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, Proceedings of Machine Learning Research의 80권, 793-802쪽. PMLR. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: 자연스러운 예/아니요 질문의 놀라운 어려움 탐구. 2019년 북미 컴퓨터 언어학 협회 지부 회의록: 인간 언어 기술, NAACL-HLT 2019, 미국 미네소타주 미니애폴리스, 2019년 6월 2-7일, 권(긴 논문과 짧은 논문), 2924-2936쪽. 컴퓨터 언어학 협회. Ido Dagan, Oren Glickman, Bernardo Magnini. 2005. PASCAL 텍스트적 함의 인식 과제. 기계 학습 과제, 예측 불확실성 평가, 시각적 객체 분류 및 텍스트적 함의 인식, 첫 번째 PASCAL 기계 학습 과제 워크숍, MLCW 2005, 영국 사우샘프턴, 2005년 4월 11-13일, 개정된 선택 논문, 컴퓨터 과학 강의 노트 3944권, 177-190쪽. Springer. 팀 데트머스, 마이크 루이스, 샘 슐라이퍼, 루크 제틀모이어. 2022. 블록 단위 양자화를 통한 8비트 최적화. 학습 표현에 관한 제10차 국제 회의, ICLR 2022, 가상 이벤트, 2022년 4월 25~29일. OpenReview.net. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li 및 Maosong Sun. 2022. 델타 튜닝: 사전 훈련된 언어 모델을 위한 매개변수 효율적인 방법에 대한 포괄적인 연구입니다. CoRR, ABS/2203.06904. 하오 야루(Yaru Hao), 이동(Li Dong), 푸루 웨이(Furu Wei), 케 쉬(Ke Xu). 2019. BERT의 효과성 시각화 및 이해. 2019년 자연어 처리 경험적 방법 컨퍼런스 및 제9회 자연어 처리 국제 공동 컨퍼런스, EMNLP-IJCNLP 2019, 홍콩, 중국, 2019년 11월 3-7일, 4141-4150쪽. 계산언어학 협회. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022. Lora: 대규모 언어 모델의 저순위 적응. 제10회 학습 표현 국제 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net. Kenji Kawaguchi, Jiaoyang Huang, Leslie Pack Kaelbling. 2019. 모든 지역적 최소값은 비볼록 머신 러닝에서 유도된 모델의 전역적 최소값입니다. Neural Comput., 31(12):22932323. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth. 2018. 표면 너머를 바라보기: 여러 문장에 대한 독해 이해에 대한 과제. 2018년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, NAACL-HLT 2018, 루이지애나주 뉴올리언스, 2018년 6월 1-6일, 1권(장편 논문), 252-262쪽. 컴퓨터 언어학 협회. Diederik P. Kingma와 Jimmy Ba. 2015. Adam: 확률적 최적화를 위한 방법. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, 2015년 5월 7-9일, Conference Track Proceedings. Hector J. Levesque, Ernest Davis, Leora Morgenstern. 2012. The winograd schema challenge. Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, 2012년 6월 10-14일. AAAI Press. Xiang Lisa Li와 Percy Liang. 2021. Prefix-tuning: Optimizing Continuous prompts for generation. 제59회 계산언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스, ACL/IJCNLP 2021, (제1권: 장문 논문), 가상 이벤트, 2021년 8월 1일-6일, 45824597쪽. 계산언어학 협회. Ilya Loshchilov 및 Frank Hutter. 2019. 분리된 가중치 감소 정규화. 제7회 학습 표현 국제 컨퍼런스, ICLR 2019, 미국 루이지애나주 뉴올리언스, 2019년 5월 6일-9일. OpenReview.net. Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen 및 Sanjeev Arora. 2023. 순방향 패스만으로 언어 모델 미세 조정. CORR, abs/2305.17333. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu. 2018. 혼합 정밀도 훈련. 국제 학습 표현 컨퍼런스에서. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. megatron-lm을 사용한 GPU 클러스터에서의 효율적인 대규모 언어 모델 훈련. 국제 고성능 컴퓨팅, 네트워킹, 스토리지 및 분석 컨퍼런스 회의록, 1-15페이지. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer. 2017. pytorch에서의 자동적 차별화. Mohammad Taher Pilehvar와 José Camacho-Collados. 2019. Wic: 문맥에 민감한 의미 표현을 평가하기 위한 문맥 내 단어 데이터 세트. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, NAACL-HLT 2019, 미국 미네소타주 미니애폴리스, 2019년 6월 2-7일, 1권(긴 논문과 짧은 논문), 1267~1273쪽. 컴퓨터 언어학 협회. Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, Sujeeth Bharadwaj. 2020. 새로운 실행 알고리즘을 사용하여 상수 메모리가 있는 대규모 신경망 학습. arXiv 사전 인쇄본 arXiv:2002.05645. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. 2020. Zero: 조 매개변수 모델 학습을 위한 메모리 최적화. SC20: 고성능 컴퓨팅, 네트워킹, 스토리지 및 분석을 위한 국제 컨퍼런스, 116페이지. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. 2021. Zero-infinity: 극한 규모의 딥 러닝을 위한 GPU 메모리 벽 깨기. 고성능 컴퓨팅, 네트워킹, 스토리지 및 분석을 위한 국제 컨퍼런스 회의록, 1-14페이지. Jie Ren, Jiaolin Luo, Kai Wu, Minjia Zhang, Hyeran Jeon, and Dong Li. 2021a. Sentinel: Efficient tensor migration and allocation on heterogeneous memory systems for deep learning. 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), 598-611쪽. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021b. Zerooffload: Democratizing billion-scale model training. USENIX Annual Technical Conference, 551564쪽. Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W. Keckler. 2016. vdnn: 확장 가능하고 메모리 효율적인 신경망 설계를 위한 가상화된 딥 신경망. 제49회 IEEE/ACM 국제 마이크로아키텍처 심포지엄(MICRO), 1-13페이지. Melissa Roemmele, Cosmin Adrian Bejan, Andrew S. Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. 상식적 추론의 논리적 형식화, 2011 AAAI 봄 심포지엄 논문, 기술 보고서 SS-11-06, 미국 캘리포니아주 스탠포드, 2011년 3월 21-23일. AAAI. Sebastian Ruder. 2016. 경사 하강 최적화 알고리즘 개요. CoRR, abs/1609.04747. Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao. 2020a. 머신 러닝 관점에서 본 최적화 방법 조사. IEEE Trans. Cybern., 50(8):3668-3681. Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. 2023. 대규모 언어 모델을 따르는 교육을 위한 중국어 교육 데이터에 대한 fullparameter와 lora 기반 fine-tuning 간의 비교 연구. CORR, abs/2304.08109. Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2020b. 딥 신경망의 초저정밀 4비트 학습. Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기반 언어 모델. CORR, abs/2302.13971. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. 2019. Superglue: 범용 언어 이해 시스템을 위한 더 끈적끈적한 벤치마크. Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 2019년 12월 8일-14일, 캐나다 브리티시 컬럼비아주 밴쿠버, 3261-3275쪽. Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Song, Zenglin Xu, Tim Kraska. 2018. Superneurons: 딥 신경망 훈련을 위한 동적 GPU 메모리 관리. ACM SIGPLAN 공지, 53:41-53. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus. 2022. 대규모 언어 모델의 새로운 능력.Trans.Mach.Learn.Res., 2022.Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.2024.Galore: Memory-efficient LLM training by gradient low-rank project.CORR, abs/2403.03507.A 하이퍼파라미터 실험에 사용하는 하이퍼파라미터는 표 4에 나와 있습니다.제한된 계산 리소스로 인해 동일한 난수 시드로 수행한 실험에서 가장 높은 결과를 보고합니다.B 학습 역학 LOMO의 학습 역학을 분석하기 위해 그림과 그림 5에서 각각 LOMO와 LORA를 사용하여 BoolQ(Clark et al., 2019)에서 학습한 LLaMA-7B에 대한 학습 손실 곡선과 검증 정확도를 제시합니다.LOMO를 사용한 학습 프로세스 동안 손실은 초기 단계에서 빠르게 수렴한 다음 안정화되고 점차 감소하는 경향이 있습니다. 개발 세트의 정확도는 일반적으로 학습 단계 수가 증가함에 따라 상승 추세를 보입니다.학습 손실 실험 하이퍼파라미터 값 LR 일정 선형 최대 그래드 규범 1.배치 크기 # 에포크 학습률 {5e-2, 3e-2} LOMO 워밍업 비율 {0.05, 0.1, 0.2} 옵티마이저 학습률 Adam W 5e-LORA 워밍업 비율 0.LORA 구성 rq = rv =LORA aLORA 옵티마이저 LORA 학습률 LORA 워밍업 비율 Adam W 5e-0.LORA+LOMO LORA 구성 LORA a гq = rv = {1,2,4,8}LOMO 학습률 {5e-3, 1e-3, 5e-4} LOMO 워밍업 비율 0.05, 0.표 4: 실험에 사용된 하이퍼파라미터 || |단계 LORA LOMO 0.0.검증 정확도 0.0.0.0.|| LORA LOMO단계 그림 4: BoolQ에서 LLaMA-7B에 대한 훈련 손실 곡선. 그림 5: BoolQ에서 LLaMA-7B의 검증 정확도.
