--- ABSTRACT ---
멀티트랙 음악 전사는 음악 오디오 입력을 여러 악기의 음표로 동시에 전사하는 것을 목표로 합니다. 이는 일반적으로 만족스러운 결과를 얻기 위해 더 복잡한 모델이 필요한 매우 어려운 작업입니다. 또한 이전 연구는 대부분 일반 악기의 전사에 초점을 맞추었지만, 음악 작품에 존재하는 경우 일반적으로 가장 중요한 신호원인 보컬은 무시했습니다. 이 논문에서는 멀티트랙 전사를 위한 오디오 입력의 시간-주파수 표현을 모델링하기 위해 새로운 딥 신경망 아키텍처인 Perceiver TF를 제안합니다. Perceiver TF는 시간적 일관성을 모델링하기 위해 추가 Transformer 계층이 있는 계층적 확장을 도입하여 Perceiver 아키텍처를 강화합니다. 따라서 당사 모델은 더 나은 확장성을 가진 Perceiver의 이점을 상속받아 단일 모델에서 여러 악기의 전사를 잘 처리할 수 있습니다. 실험에서 Perceiver TF를 훈련하여 멀티태스크 학습 방식으로 보컬뿐만 아니라 12개 악기 클래스를 모델링합니다. 우리의 결과는 제안된 시스템이 다양한 공개 데이터 세트에서 최첨단 대응 제품(예: MT3 및 SpecTNT)보다 성능이 우수함을 보여줍니다. 색인 용어 시간-주파수, 인지자, 자동 음악 전사, 멀티태스크 학습, 랜덤 믹싱 증강. 1.
--- INTRODUCTION ---
자동 음악 전사(AMT)는 음악 오디오 입력을 각 음표에 시작, 피치, 지속 시간 및 속도 속성이 포함된 일련의 음표로 전사하는 것을 목표로 하는 음악 정보 검색(MIR) 작업입니다. 출력은 일반적으로 MIDI 형식으로 제공됩니다. 멀티트랙 설정에서 AMT 시스템은 입력에 있는 모든 악기를 식별하고 그에 따라 연관된 음표를 MIDI 출력 채널로 추정해야 합니다. 이상적으로 말하면, 각 해당 채널에 대해 식별된 악기를 사용하여 출력 MIDI의 합성 오디오 믹스는 음악적으로 그럴듯한 방식으로 원래 입력 오디오와 유사해야 합니다. 최근 몇 년 동안 딥 러닝 기술[1, 2]을 사용하여 상당한 진전을 이루었지만, 분석과 검토 결과 두 가지 주요 과제가 아직 효과적으로 해결되지 않았음을 나타냅니다. 모델 확장성과 악기 구별입니다. 멀티트랙 AMT는 일반적으로 매우 어려운 작업으로 간주됩니다. 일반적으로 사용되는 악기의 수는 최대 100개에 달할 수 있습니다. 그 중에서도 기타, 바이올린, 신시사이저와 같은 일반 악기의 음표는 음색, 표현력, 연주 기법의 엄청난 변화로 인해 특성화하기 어렵습니다. 그 외에도 일반적으로 가장 우세한 악기인 보컬은 가사와 표현을 전달하기 위해 음색과 피치를 변화시킵니다. 모든 악기를 동시에 처리하려면 더 나은 모델 확장성이 필요합니다. 기존 멀티택 AMT 시스템에 대한 관찰 결과 피아노와 기타와 같은 인기 있는 피치 악기에 대해 종종 많은 거짓 양성 음표가 생성되는 것으로 나타났습니다. 예를 들어, 현악 합주단의 음표는 피아노로 대량 캡처됩니다. 이는 시스템이 명확한 음색 종속 기능을 제공하지 않거나 다양한 악기의 음색 변화에 강하지 않기 때문일 수 있습니다. 추론을 하는 동안 시스템이 각 악기 소스를 믹스에서 구별할 수 있다면 이 문제를 완화할 수 있다고 생각합니다. 모델 확장성을 해결하기 위해 Perceiver의 증강된 변형인 Perceiver TF를 제안합니다[3]. Perceiver는 고차원 데이터 입력을 처리하기 위해 Transformer 제품군에서 더 나은 확장성으로 잘 알려져 있습니다. 이 작업에서 우리는 오디오 입력을 위해 스펙트로그램을 채택했으며, T와 F는 각각 시간 및 주파수 축의 길이를 나타냅니다. 멀티트랙 AMT의 경우 여러 악기의 음색에 따라 피치를 모델링하는 기능이 매우 중요하므로 고해상도 주파수 축을 따라 유용한 기능을 포착하기 위해 보다 포괄적인 작업이 필요합니다. 최근 이 목적을 위해 SpecTNT 아키텍처[4]가 제안되었고 보컬 멜로디 추출(AMT의 하위 작업)에서 최첨단 성능을 달성했습니다. SpecTNT는 계층 구조의 두 개의 Transformer로 구성되며, 하위 레벨 Transformer는 프레임의 스펙트럼에서 직접 셀프 어텐션을 수행합니다. 그러나 이러한 설계는 어텐션 계산의 3차 복잡도, 즉 O(TF² + T²)로 이어져 더 복잡한 작업에 대한 확장성이 제한됩니다. 이를 위해 우리는 Perceiver와 SpecTNT의 비사소한 조합을 구상합니다. Perceiver를 계층적으로 확장합니다. 그 결과 Perceiver TF는 교차 주의를 활용하여 각 프레임의 잠재적 병목 현상으로 스펙트럼 특징을 추출하고 시간 축을 따라 자체 주의를 위한 추가 Transformer를 추가하여 전체적으로 O(TF+T²)의 2차 복잡도를 생성합니다. F는 일반적으로 크기 때문에 이러한 복잡도 감소가 상당하여 모델이 더 많은 악기를 동시에 처리할 수 있습니다. 악기 구별을 해결하기 위해 음악 소스 분리(MSS) [5, 6]에서 학습한 랜덤 믹싱 증강 기술을 채택하여 각 악기 줄기를 입력 오디오 믹스에서 분리하는 것을 목표로 합니다 [7]. 또한 각 하위 작업이 악기의 전사를 모델링하는 다중 작업 학습 방식으로 AMT 모델을 학습합니다. 랜덤 믹싱 기술과 함께 이 다중 작업 설계를 사용하면 엄청난 양의 증강된 학습 샘플로 학습할 수 있는 유연성이 더 커집니다. 우리의 전략은 악기 인식[8] 또는 MSS[9]와 함께 AMT 작업을 공동으로 훈련하여 악기 의존적 기능의 모델을 알리는 이전 연구와 다릅니다. 우리의 지식에 따르면, 다중 트랙 AMT를 개선하기 위해 랜덤 혼합 기술을 사용한 연구는 거의 이루어지지 않았습니다. 2.
--- RELATED WORK ---
다중 악기 AMT는 여러 이전 연구에서 탐구되었습니다.Wu et al.[10] 및 Hung et al.[8]은 다중 작업 학습 방식으로 관련 작업이 있는 전사 모델을 훈련했습니다.Tanaka et al.은 클러스터링 접근 방식을 사용하여 전사된 악기를 분리[11]하는 반면 Cheuk et al.은 비지도 학습 기술을 사용하여 위치 임베딩 스펙트럼 교차 주의 스펙트럼 교차 주의 Ex N Ex N 잠재 변환기 잠재 변환기 시간 변환기 ST-OT-스펙트럼 교차 주의 0-Ex N 잠재 변환기 x M x L 그림 1. Perceiver TF 모듈의 블록 다이어그램.위치 임베딩은 먼저 Oo로 표시된 잠재 배열에 추가됩니다.Spectral Cross-Attention 모듈은 스펙트럼 입력 St를 et에 투사한 다음 잠재 변환기 모듈이 투사합니다.시간 변환기는 모든 시간 단계의 0/4을 처리하여 시간적 일관성을 모델링합니다. 자세한 내용은 3.2절에서 설명합니다. 저자원 데이터 세트에서 필사 증명[1, 12]. 이러한 이전 사례는 피아노롤 표현을 기반으로 하는 모델이 악기에 따라 달라지는 시작점, 피치, 음의 지속 시간을 포착할 수 있음을 보여주었습니다. 피아노롤 접근 방식과 달리 Gardner et al.[2]은 멀티트랙 AMT를 처리하기 위해 MT3라는 시퀀스 간 모델을 제안하는 새로운 패러다임을 만들었습니다. 그들은 오디오에서 멀티트랙 MIDI 토큰을 모델링하기 위해 표준 인코더-디코더 Transformer를 훈련시키고 여러 공개 데이터 세트에서 최첨단 성능을 보여주었습니다. 이와 대조적으로, 보컬 필사는 일반적으로 AMT와 동일한 목표를 공유하지만 문헌에서 독립적인 작업으로 취급됩니다. 훈련 데이터가 부족하기 때문에 폴리포닉 음악 오디오에서 음표 수준 출력을 필사하는 데 초점을 맞춘 연구는 거의 없습니다. 최근 Wang et al.은 500곡의 중국 노래를 포함한 인간이 주석을 단 데이터 세트를 공개했습니다[13]. 그들은 작업의 기준선에 대한 CNN 기반 모델(EFN)을 제공합니다. [14]에서는 보컬의 F0 추정치에서 파생된 가상 라벨을 활용하기 위한 교사-학생 훈련 계획이 제안되었습니다. 최근 [15]에서는 프런트 엔드로 MSS가 필요한 보컬 전사 시스템을 제안했습니다. 이 연구에서는 보컬과 다중 악기 전사를 결합하는 통합 프레임워크를 제안하며 MSS 프런트 엔드와 같은 사전 훈련된 모듈에 의존하지 않습니다. 3.
--- METHOD ---
OLOGY 이 작업에서는 두 가지 주요 이유 때문에 시퀀스 대 시퀀스(seq-to-seq) 방식 대신 피아노롤 방식을 채택합니다. 첫째, 부분적으로 레이블이 지정된 데이터에서 학습하기 위해 손실 계산을 조작하는 것이 더 쉽습니다. 예를 들어, 반주의 MIDI 기준 진실을 사용할 수 없는 보컬 전사 데이터 세트를 결합하는 seq-to-seq 모델을 학습하는 것은 사소한 일이 아닙니다. 둘째, seq-to-seq의 추론 시간 복잡도는 자기 회귀적 특성으로 인해 음표(토큰) 수에 따라 달라집니다. 오디오 입력에 복잡하고 밀도가 높은 폴리포닉 음표가 있는 악기가 많이 포함된 경우 추론이 매우 느립니다. 제안하는 모델도 Transformer 지향 아키텍처이지만, 피아노롤을 직접 예측하기 위해 인코더 부분에 초점을 맞춥니다. 다음 섹션에서는 제안된 모델 아키텍처(섹션 3.1-3.3)와 랜덤 믹싱 증강 기술(섹션 3.4)을 설명합니다. 우리 모델은 세 개의 하위 모듈로 구성되어 있습니다: 합성곱 모듈, 지각 TF 모듈, 출력 모듈. 입력 스펙트로그램은 먼저 로컬 피처 집계를 위해 합성곱 모듈을 통과합니다. 그런 다음 여러 지각 TF 블록을 포함하는 지각 TF 모듈은 피처를 추출하고 각 시간 단계에서 시간 임베딩을 출력합니다. 마지막으로 출력 모듈은 피아노롤 출력을 위해 시간 임베딩을 원하는 차원으로 투영합니다. 3.1. 합성곱 모듈 Transformer 기반 모델의 프런트 엔드로 합성곱 신경망(CNN)을 사용하는 것은 음성 인식 파이프라인에서 일반적인 설계 선택이 되었습니다[16]. 이전 연구에서도 CNN 프런트 엔드가 많은 MIR 작업에서 SpecTNT 및 MIRTransformer에서 중요한 역할을 한다는 것을 발견했습니다[4, 17, 18, 19]. 이 관행에 따라 평균 풀링을 사용하여 여러 잔여 단위[20]를 쌓아 주파수 축의 차원을 줄입니다. 결과적인 시간-주파수 표현을 S = [So, S₁,..., ST-1] Є RTXFXC로 표시합니다. 여기서 T, F 및 C는 각각 시간, 주파수 및 채널의 차원을 나타냅니다. 3.2. Perceiver TF 모듈 기존의 Perceiver 아키텍처는 두 가지 주요 구성 요소[3]를 포함합니다. (i) 입력 데이터와 잠재 배열을 잠재 배열로 매핑하는 교차 주의 모듈, (ii) 잠재 배열을 잠재 배열로 매핑하는 Transformer 타워. 이 구조에서 Perceiver를 확장하기 위한 설계 원리는 두 가지입니다. (1) 시간 단계 St의 스펙트럼 표현이 피치 및 음색 정보를 전달하는 데 중요하므로 교차 주의 모듈의 입력 데이터로 사용되어 시간 단계 t에 대한 잠재 배열로 스펙트럼 정보를 투사합니다. 각 잠재 배열은 로컬 스펙트럼 특징을 추출하는 역할을 합니다. (2) 서로 다른 시간 단계의 잠재 배열 시퀀스가 있으므로 시간 축을 따라 로컬 스펙트럼 정보를 교환하여 시간적 코히어런스를 학습하는 Transformer가 필요합니다. Perceiver TF 아키텍처는 그림 1에 나와 있습니다. Perceiver TF 블록에는 스펙트럼 교차 어텐션, 잠재 변환기, 시간 변환기라는 세 가지 Transformer 스타일 모듈이 포함되어 있으며, 각각 스펙트럼, 채널별, 시간 정보를 모델링하는 역할을 합니다. 각각에는 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다. 스펙트럼 교차 어텐션(SCA) 모듈은 입력 스펙트럼 표현 St에서 직접 작동하여 키(K) 및 값(V) 행렬에 투영합니다. 기존 Transformer와 달리 Perceiver의 교차 어텐션 모듈은 잠재 배열을 쿼리(Q) 행렬에 매핑한 다음 그에 따라 QKV 셀프 어텐션을 수행합니다. 우리는 K개의 학습 가능한 잠재 배열 0º € RKXD를 초기화하기 위해 Perceiver 설계를 따릅니다. 여기서 K는 인덱스 차원이고 D는 채널 차원입니다. 그런 다음 T번 동안 0°를 반복하고 각각을 시간 단계 t에 연관시킵니다. 그러면 이를 0 = ... 1이 되도록 지정합니다. 즉, 모든 잠재 배열은 시간 축을 따라 동일한 초기화에서 나옵니다. 이 Ot는 첫 번째 Perceiver TF 블록의 스펙트럼 정보를 전체 블록 스택에 전달하는 중요한 역할을 합니다. h번째 반복에서 SCA의 쿼리-키-값(QKV) 어텐션은 다음과 같이 작성할 수 있습니다. fscA : {}, St} → 0 (h+1), 이 프로세스는 Perceiver TF 블록이 반복될 때 반복되어 ½)와 입력 St 사이의 연결을 유지합니다. 교차 어텐션 모듈의 설계는 Perceiver의 계산 확장성을 크게 개선하는 열쇠입니다. 예를 들어, 우리의 SCA는 O(FK)로 이어지고, 이는 SpecTNT [4]의 스펙트럼 변환기의 O(F²)보다 훨씬 저렴합니다. 왜냐하면 K(잠재 배열의 차원)가 일반적으로 작기 때문입니다(즉, K &lt; F). 잠재 변환기 모듈은 SCA 모듈 뒤에 위치합니다. 여기에는 O½의 잠재 배열에서 표준 셀프 어텐션을 수행하기 위한 N개의 변환기 스택이 포함되어 있습니다. 그 결과 복잡도 O(NK²)도 효율적입니다. AMT의 맥락에서 이 프로세스는 온셋, 피치 및 악기 간의 상호 작용이 명시적으로 모델링됨을 의미합니다. 다중 트랙 AMT를 수행하기 위해 Klatent 배열을 초기화하고 각 잠재 배열이 하나의 특정 작업을 처리하도록 학습합니다. [21]에 따라 악기의 경우 두 개의 잠재 배열을 배열하여 각각 온셋 및 프레임별(피치) 활성화를 모델링합니다. 이는 K = 2J로 이어지고, 여기서 J는 대상 악기의 수입니다. 시간 변환기 모듈은 서로 다른 시간 단계의 ½ 쌍 간의 통신을 가능하게 하도록 배치됩니다. 시간 변환기가 각 잠재 배열의 시간적 위치를 이해하도록 하기 위해 초기화 중에 각 O에 훈련 가능한 위치 임베딩을 추가합니다. ½³ (k), k = 0, ……., K−1이 O½의 각 잠재 배열을 나타낸다고 하면, 각각이 잠재 배열의 해당 입력 시퀀스인 [0 (k), 0 (k),..., 01(k)]를 제공하는 K개의 병렬 표준 변환기를 배열합니다. 모듈은 M번 반복되어 O(MT²)의 복잡도를 생성합니다. 마지막으로, 전체 모듈을 형성하기 위해 Perceiver TF 블록을 L번 반복합니다. 원래 Perceiver와 달리 스펙트럼 교차 주의와 잠재 변환기의 가중치는 반복되는 블록에서 공유되지 않는다는 점에 유의하세요. 3.3. 출력 모듈 우리는 각각 시작 및 프레임별 잠재 배열 출력에 대해 시그모이드 활성화 함수를 사용하는 두 개의 GRU 모듈[22]을 활용합니다. 우리는 프레임별 활성화를 조건화하기 위해 시작 출력을 사용하는 이전 작업[21]을 따릅니다. 3.4. 멀티태스크 학습 손실 제안된 모델을 학습하기 위한 손실 함수를 공식화합니다.JL = Σ(lonset + 1 frame) j=(1) 여기서 l은 기준 진실과 예측 간의 이진 교차 엔트로피 손실이고, onset 및 lo frame은 각각 계측기 j에 대한 onset 및 frame 활성화 손실입니다. 모든 J 계측기의 손실은 해당 계측기가 학습 샘플에서 활성화되었는지 여부에 관계없이 계산해야 합니다. 따라서 샘플에 없는 계측기의 경우 출력이 0이 될 것으로 예상됩니다. 4.1. 데이터 세트 4.
--- EXPERIMENT ---
s, 우리는 다중 작업 학습 방식으로 12개 악기 클래스와 보컬을 모델링하기 위해 Perceiver TF를 훈련합니다. 우리의 결과는 제안된 시스템이 다양한 공개 데이터 세트에서 최첨단 대응 제품(예: MT3 및 SpecTNT)보다 성능이 우수함을 보여줍니다. 색인 용어 시간-주파수, Perceiver, 자동 음악 전사, 다중 작업 학습, 랜덤 믹싱 증강. 1. 소개 자동 음악 전사(AMT)는 음악 오디오 입력을 각 음표에 시작, 피치, 지속 시간 및 속도 속성이 포함된 일련의 음악 음표로 전사하는 것을 목표로 하는 음악 정보 검색(MIR) 작업입니다. 출력은 일반적으로 MIDI 형식으로 제공됩니다. 다중 트랙 설정에서 AMT 시스템은 입력에 있는 모든 악기를 식별하고 연관된 음표를 그에 따라 MIDI 출력의 채널로 추정해야 합니다. 이상적으로 말하면, 각 해당 채널에 대해 식별된 악기를 사용하여 출력 MIDI의 합성 오디오 믹스는 음악적으로 그럴듯한 방식으로 원래 입력 오디오와 유사해야 합니다. 최근 몇 년 동안 딥 러닝 기법[1, 2]을 사용하여 상당한 진전을 이루었지만, 저희의 분석과 검토에 따르면 모델 확장성과 악기 구별이라는 두 가지 주요 과제가 아직 효과적으로 해결되지 않은 것으로 나타났습니다. 멀티트랙 AMT는 일반적으로 매우 어려운 작업으로 간주됩니다. 일반적으로 사용되는 악기의 수는 최대 100개에 달할 수 있습니다. 그 중에서도 기타, 바이올린, 신시사이저와 같은 일반 악기의 음표는 음색, 표현력, 연주 기법의 엄청난 변화로 인해 특성화하기 어렵습니다. 그 외에도 일반적으로 가장 우세한 악기인 보컬은 가사와 표현을 전달하기 위해 음색과 피치를 변경합니다. 모든 악기를 동시에 처리하려면 더 나은 모델 확장성이 필요합니다. 기존 멀티트랙 AMT 시스템에 대한 관찰 결과 피아노와 기타와 같은 인기 있는 피치 악기에 대해 많은 거짓 양성 음표가 발생하는 경우가 많습니다. 예를 들어, 현악 합주단의 음표는 피아노로 대량 캡처됩니다. 이는 시스템이 명확한 음색에 따른 특징을 제공하지 않거나 다양한 악기 간의 음색 변화에 강하지 않기 때문일 수 있습니다. 이 문제는 시스템이 추론을 하는 동안 각 악기 소스를 믹스에서 구별할 수 있다면 완화될 수 있다고 생각합니다. 모델 확장성을 해결하기 위해 Perceiver [3]의 증강된 변형인 Perceiver TF를 제안합니다. Perceiver는 Transformer 제품군에서 고차원 데이터 입력을 처리하기 위한 더 나은 확장성으로 잘 알려져 있습니다. 이 작업에서 오디오 입력을 위한 스펙트로그램을 채택하는데, T와 F는 각각 시간 축과 주파수 축의 길이를 나타냅니다. 다중 트랙 AMT의 경우 여러 악기의 음색에 따른 피치를 모델링하는 기능이 매우 중요하므로 고해상도 주파수 축을 따라 유용한 특징을 포착하기 위해 보다 포괄적인 작업이 필요합니다. 최근 이 목적을 위해 SpecTNT 아키텍처 [4]가 제안되었고 보컬 멜로디 추출(AMT의 하위 작업)에서 최첨단 성능을 달성했습니다. SpecTNT는 계층 구조의 두 개의 Transformer로 구성되며, 하위 레벨 Transformer는 프레임의 스펙트럼에서 직접 셀프 어텐션을 수행합니다. 그러나 이러한 설계는 어텐션 계산의 3차 복잡도, 즉 O(TF² + T²)로 이어져 더 복잡한 작업에 대한 확장성이 제한됩니다. 이를 위해 Perceiver와 SpecTNT의 비사소한 조합을 구상합니다. Perceiver를 계층적으로 확장합니다. 그 결과 Perceiver TF는 교차 어텐션을 활용하여 각 프레임의 잠재적 병목 현상으로 스펙트럼 특징을 추출하고 시간 축을 따라 셀프 어텐션을 위한 추가 Transformer를 추가하여 전체적으로 O(TF+T²)의 2차 복잡도를 생성합니다. F는 일반적으로 크기 때문에 이러한 복잡도 감소가 상당하여 모델이 더 많은 악기를 동시에 처리할 수 있습니다. 악기 구별을 해결하기 위해 음악 소스 분리(MSS) [5, 6]에서 배운 랜덤 믹싱 증강 기술을 채택하여 각 악기 스템을 입력 오디오 믹스 [7]에서 분리하는 것을 목표로 합니다. 또한, 우리는 각 하위 작업이 악기의 전사를 모델링하는 다중 작업 학습 방식으로 AMT 모델을 훈련합니다. 이 다중 작업 설계와 랜덤 혼합 기법을 함께 사용하면 엄청난 양의 증강된 훈련 샘플로 훈련할 때 더 많은 유연성을 얻을 수 있습니다. 우리의 전략은 악기 인식[8] 또는 MSS[9]와 함께 AMT 작업을 공동으로 훈련하여 악기 종속 기능의 모델을 알리는 이전 연구와 다릅니다. 우리가 아는 한, 랜덤 혼합 기법을 사용하여 다중 트랙 AMT를 개선한 연구는 거의 없습니다. 2. 관련 연구 다중 악기 AMT는 여러 이전 연구에서 탐구되었습니다. Wu 등[10]과 Hung 등[8]은 다중 작업 학습 방식으로 관련 작업이 있는 전사 모델을 훈련했습니다. Tanaka 등은 클러스터링 접근 방식을 사용하여 전사된 악기를 분리[11]하는 반면 Cheuk 등은 영어: 비지도 학습 기술을 사용하여 imPositional embedding Spectral Cross-Attention Spectral Cross-Attention Ex N Ex N Latent Transformer Latent Transformer Temporal Transformer ST-OT-Spectral Cross-Attention 0-Ex N Latent Transformer x M x L 그림 1. Perceiver TF 모듈의 블록 다이어그램.위치 임베딩은 먼저 Oo로 표시된 잠재 배열에 추가됩니다.Spectral Cross-Attention 모듈은 스펙트럼 입력 St를 et에 투사한 다음 Latent Transformer 모듈이 투사합니다.Temporal Transformer는 모든 시간 단계의 0/4을 처리하여 시간적 일관성을 모델링합니다.자세한 내용은 섹션 3.2에서 설명합니다.저자원 데이터 세트에서 전사를 증명합니다[1, 12].이러한 이전 예는 피아노롤 표현을 기반으로 하는 모델이 악기에 따라 음표의 시작, 피치 및 지속 시간을 포착할 수 있음을 보여주었습니다.피아노롤 접근 방식과 달리 Gardner et al. [2]는 다중 트랙 AMT를 처리하기 위해 MT3라는 시퀀스 대 시퀀스 모델을 제안하는 새로운 패러다임을 만들었습니다. 그들은 오디오에서 다중 트랙 MIDI 토큰을 모델링하기 위해 표준 인코더-디코더 Transformer를 훈련시키고 여러 공개 데이터 세트에서 최첨단 성능을 보여주었습니다. 반면에 보컬 전사는 AMT와 동일한 목표를 공유함에도 불구하고 일반적으로 문헌에서 독립적인 작업으로 취급됩니다. 훈련 데이터가 부족하기 때문에 다성 음악 오디오에서 음표 수준 출력을 전사하는 데 초점을 맞춘 연구는 거의 없습니다. 최근 Wang et al.은 500곡의 중국 노래를 포함한 인간이 주석을 단 데이터 세트를 공개했습니다[13]. 그들은 작업의 기준선에 대한 CNN 기반 모델(EFN)을 제공합니다. [14]에서는 보컬의 F0 추정에서 파생된 가상 레이블을 활용하기 위한 교사-학생 훈련 체계가 제안되었습니다. 최근 [15]는 프런트 엔드로 MSS가 필요한 보컬 전사 시스템을 제안했습니다. 이 작업에서 우리는 보컬과 다중 악기 전사를 결합하는 통합 프레임워크를 제안하며, MSS 프런트 엔드와 같은 사전 학습된 모듈에 의존하지 않습니다.3. 방법론 이 작업에서 우리는 두 가지 주요 이유에서 시퀀스-투-시퀀스(seq-to-seq) 접근 방식 대신 피아노롤 접근 방식을 채택합니다.첫째, 부분적으로 레이블이 지정된 데이터에서 학습하기 위해 손실 계산을 조작하는 것이 더 쉽습니다.예를 들어, 반주의 MIDI 기준 진실을 사용할 수 없는 보컬 전사 데이터 세트를 결합하는 seq-to-seq 모델을 학습하는 것은 사소한 일이 아닙니다.둘째, seq-to-seq의 추론 시간 복잡도는 자기 회귀적 특성으로 인해 음표(토큰) 수에 따라 달라집니다.오디오 입력에 복잡하고 밀도가 높은 폴리포닉 음표가 있는 악기가 많이 포함된 경우 추론이 매우 느립니다.제안하는 모델도 Transformer 지향 아키텍처이지만, 우리는 피아노롤을 직접 예측하기 위해 인코더 부분에 집중합니다. 다음 섹션에서는 제안된 모델 아키텍처(섹션 3.1-3.3)와 랜덤 혼합 증강 기술(섹션 3.4)을 설명합니다. 우리 모델은 세 개의 하위 모듈로 구성됩니다. 합성곱 모듈, Perceiver TF 모듈, 출력 모듈. 입력 스펙트로그램은 먼저 로컬 피처 집계를 위해 합성곱 모듈을 통과합니다. 그런 다음 여러 Perceiver TF 블록을 포함하는 Perceiver TF 모듈은 피처를 추출하고 각 시간 단계에서 시간 임베딩을 출력합니다. 마지막으로 출력 모듈은 피아노롤 출력을 위해 시간 임베딩을 원하는 차원으로 투영합니다. 3.1. 합성곱 모듈 Transformer 기반 모델의 프런트 엔드로 합성곱 신경망(CNN)을 사용하는 것은 음성 인식 파이프라인에서 일반적인 설계 선택이 되었습니다[16]. 이전 연구에서도 CNN 프런트엔드가 많은 MIR 작업에서 SpecTNT 및 MIRTransformer에서 중요한 역할을 한다는 것을 발견했습니다[4, 17, 18, 19]. 이 관행에 따라, 우리는 평균 풀링을 사용하여 여러 잔여 단위[20]를 쌓아 주파수 축의 차원을 줄입니다.우리는 결과적인 시간-주파수 표현을 S = [S0, S₁,..., ST-1] Є RTXFXC로 표시합니다.여기서 T, F 및 C는 각각 시간, 주파수 및 채널의 차원을 나타냅니다.3.2. Perceiver TF 모듈 기존의 Perceiver 아키텍처는 두 가지 주요 구성 요소[3]를 포함합니다: (i) 입력 데이터와 잠재 배열을 잠재 배열로 매핑하는 교차 주의 모듈; (ii) 잠재 배열을 잠재 배열로 매핑하는 Transformer 타워.이러한 구조에서 Perceiver를 확장하기 위한 우리의 설계 원리는 두 가지입니다.(1) 우리는 시간 단계 St의 스펙트럼 표현이 피치 및 음색 정보를 전달하는 데 핵심적이라고 생각하므로, 그것은 교차 주의 모듈이 시간 단계 t에 대한 잠재 배열로 스펙트럼 정보를 투사하기 위한 입력 데이터 역할을 합니다.각 잠재 배열은 로컬 스펙트럼 특징을 추출하는 역할을 합니다. (2) 서로 다른 시간 단계의 잠재 배열 시퀀스가 있으므로 시간 축을 따라 로컬 스펙트럼 정보를 교환하여 시간적 코히어런스를 학습하는 Transformer가 필요합니다. Perceiver TF 아키텍처는 그림 1에 나와 있습니다. Perceiver TF 블록에는 스펙트럼 교차 어텐션, 잠재 변환기, 시간 변환기라는 세 가지 Transformer 스타일 모듈이 포함되어 있으며, 각각 스펙트럼, 채널별, 시간 정보를 모델링하는 역할을 합니다. 각각에는 어텐션 메커니즘과 위치별 피드포워드 네트워크가 포함됩니다. 스펙트럼 교차 어텐션(SCA) 모듈은 입력 스펙트럼 표현 St에서 직접 작동하여 키(K) 및 값(V) 행렬에 투영합니다. 기존 Transformer와 달리 Perceiver의 교차 어텐션 모듈은 잠재 배열을 쿼리(Q) 행렬에 매핑한 다음 그에 따라 QKV 셀프 어텐션을 수행합니다. 우리는 K개의 학습 가능한 잠재 배열 0º € RKXD를 초기화하기 위해 Perceiver 설계를 따릅니다. 여기서 K는 인덱스 차원이고 D는 채널 차원입니다. 그런 다음 T번 동안 0°를 반복하고 각각을 시간 단계 t에 연관시킵니다. 그러면 이를 0 = ... 1이 되도록 지정합니다. 즉, 모든 잠재 배열은 시간 축을 따라 동일한 초기화에서 나옵니다. 이 Ot는 첫 번째 Perceiver TF 블록의 스펙트럼 정보를 전체 블록 스택에 전달하는 중요한 역할을 합니다. h번째 반복에서 SCA의 쿼리-키-값(QKV) 어텐션은 다음과 같이 작성할 수 있습니다. fscA : {}, St} → 0 (h+1), 이 프로세스는 Perceiver TF 블록이 반복될 때 반복되어 ½)와 입력 St 사이의 연결을 유지합니다. 교차 어텐션 모듈의 설계는 Perceiver의 계산 확장성을 크게 개선하는 열쇠입니다. 예를 들어, 우리의 SCA는 O(FK)로 이어지고, 이는 SpecTNT [4]의 스펙트럼 변환기의 O(F²)보다 훨씬 저렴합니다. 왜냐하면 K(잠재 배열의 차원)가 일반적으로 작기 때문입니다(즉, K &lt; F). 잠재 변환기 모듈은 SCA 모듈 뒤에 위치합니다. 여기에는 O½의 잠재 배열에서 표준 셀프 어텐션을 수행하기 위한 N개의 변환기 스택이 포함되어 있습니다. 그 결과 복잡도 O(NK²)도 효율적입니다. AMT의 맥락에서 이 프로세스는 온셋, 피치 및 악기 간의 상호 작용이 명시적으로 모델링됨을 의미합니다. 다중 트랙 AMT를 수행하기 위해 Klatent 배열을 초기화하고 각 잠재 배열이 하나의 특정 작업을 처리하도록 학습합니다. [21]에 따라 악기의 경우 두 개의 잠재 배열을 배열하여 각각 온셋 및 프레임별(피치) 활성화를 모델링합니다. 이는 K = 2J로 이어지고, 여기서 J는 대상 악기의 수입니다. 시간 변환기 모듈은 서로 다른 시간 단계의 ½ 쌍 간의 통신을 가능하게 하도록 배치됩니다. 시간 변환기가 각 잠재 배열의 시간적 위치를 이해하도록 하기 위해 초기화 중에 각 O에 훈련 가능한 위치 임베딩을 추가합니다. ½³ (k), k = 0, ……., K−1이 O½의 각 잠재 배열을 나타낸다고 하면, 각각이 잠재 배열의 해당 입력 시퀀스인 [0 (k), 0 (k),..., 01(k)]를 제공하는 K개의 병렬 표준 변환기를 배열합니다. 모듈은 M번 반복되어 O(MT²)의 복잡도를 생성합니다. 마지막으로, 전체 모듈을 형성하기 위해 Perceiver TF 블록을 L번 반복합니다. 원래 Perceiver와 달리 스펙트럼 교차 주의와 잠재 변환기의 가중치는 반복되는 블록에서 공유되지 않는다는 점에 유의하세요. 3.3. 출력 모듈 우리는 각각 시작 및 프레임별 잠재 배열 출력에 대해 시그모이드 활성화 함수를 사용하는 두 개의 GRU 모듈[22]을 활용합니다. 우리는 프레임별 활성화를 조건화하기 위해 시작 출력을 사용하는 이전 작업[21]을 따릅니다. 3.4. 멀티태스크 훈련 손실 우리는 제안된 모델을 훈련하기 위한 손실 함수를 공식화한다: JL = Σ(lonset + 1 frame) j=(1) 여기서 l은 기준 진실과 예측 간의 이진 교차 엔트로피 손실이고, onset 및 lo frame은 각각 악기 j에 대한 onset 및 frame 활성화 손실이다. 모든 J 악기에 대한 손실은 해당 악기가 훈련 샘플에서 활성화되었는지 여부에 관계없이 계산해야 한다는 점에 유의한다. 따라서 샘플에 없는 악기의 경우 출력이 0이 될 것으로 예상된다. 4.1. 데이터 세트 4. 실험 우리는 평가를 위해 4개의 공개 데이터 세트를 사용한다. Slakh2100[23]에는 2100개의 멀티트랙 MIDI와 해당 합성 오디오가 포함되어 있다. MIDI 파일은 Lakh 데이터 세트[24]의 하위 세트이고 오디오 샘플은 전문가급 소프트웨어로 합성되었다. 악기는 Slakh 데이터 세트에 정의된 12개의 MIDI 클래스로 그룹화되었다. 우리는 경험에서 공식적인 훈련/검증/테스트 분할을 사용했습니다.&quot;사운드 효과&quot;, &quot;타악기&quot; 및 &quot;민족&quot; 악기는 없습니다.우리는 &quot;현악기&quot;와 &quot;앙상블&quot;을 하나의 악기 클래스로 그룹화했습니다.MAESTROv3 [25]에는 피아노의 MIDI 캡처링 장치로 수집한 정렬된 음표 주석이 있는 약 200시간 분량의 피아노 솔로 녹음이 포함되어 있습니다.우리는 공식적인 훈련/검증/테스트 분할을 따릅니다.기타 세트 [26]에는 360개의 고품질 기타 녹음과 동기화된 음표 주석이 포함되어 있습니다.이 데이터 세트에 대한 공식적인 분할이 없으므로 [2]의 설정을 따릅니다.각 스타일의 처음 두 진행은 훈련에 사용되고 마지막은 테스트에 사용됩니다.MIR-ST500 [13]에는 리드 보컬 멜로디에 대한 음표 주석이 있는 500개의 중국 팝송이 포함되어 있습니다.우리는 공식적인 훈련-테스트 분할을 사용했습니다.장애 링크로 인해 훈련 세트의 약 10%가 누락되었지만 공정한 비교를 위해 테스트 세트가 완전한지 확인합니다. 4.2. 데이터 증강 다중 트랙 AMT에 대한 데이터 주석은 노동 집약적입니다. 손에 있는 데이터를 더 잘 활용하기 위해 훈련 중에 두 가지 데이터 증강 기술을 적용합니다. 이전 연구 [4, 27]에 따라 피치 시프팅은 훈련 중에 모든 비타악기 악기에 무작위로 수행됩니다. 교차 데이터 세트 랜덤 믹싱(RM) 기술을 소개합니다. 먼저 세 가지 유형의 데이터 세트를 정의해 보겠습니다. • 다중 트랙: 각 샘플에는 다성음 음표가 있는 악기별 오디오 스템의 다중 트랙이 포함되고(예: Slakh), 보컬 신호는 없습니다. • 단일 트랙: 각 샘플에는 다성음 음표가 있는 단일 비보컬 스템만 포함됩니다(예: MAESTRO 및 GuitarSet). • 보컬 믹스처: 각 샘플은 리드 보컬만을 위한 단성음 음표가 있는 음악의 전체 믹스입니다(예: MIR-ST500). MSS 도구 [28]를 사용하여 각 샘플을 보컬 스템과 반주 스템으로 분리합니다. 각 학습 샘플은 원래 노래의 무작위 순간에서 발췌한 것이며, 지속 시간은 모델 입력 길이(예: 6초)에 따라 달라집니다. J개의 악기 클래스를 전사하고, 해당 악기 세트를 = {w; } } =로 표시한다고 가정합니다. 그런 다음 각각 언급된 세 가지 유형의 데이터 세트에 다음과 같이 세 가지 처리를 적용합니다. J-먼저, 다중 트랙 데이터 세트의 학습 샘플 s¿에 대해 악기 템플릿을 μi C로 표시하여 sr에 있는 악기를 나타냅니다. 그런 다음 fli의 각 악기 Wj에 대해 μu의 aw;로 대체될 확률이 ap%입니다. 여기서 iu(즉, 다른 샘플)입니다. 두 번째로, 단일 트랙 데이터 세트의 샘플 s¿에 대해 기존 악기 템플릿 μu(iu)를 배경으로 무작위로 선택합니다. s;의 악기가 μu에 있는 경우 해당 스템은 μu에서 제거됩니다. 예를 들어 s¿가 피아노 솔로인 경우 plu에서 피아노 스템을 제거합니다. 영어: 우리의 예비 실험에서 배경과 혼합하지 않고 솔로 예제를 모델 학습에 제시하면 성능이 저하될 수 있습니다.마지막으로 보컬 믹스 데이터 세트의 샘플 si의 경우 두 가지 방법으로 배경을 대체할 aq%의 확률이 있습니다.(i) 단일 트랙 처리와 마찬가지로 기존 μu(i + u)를 배경으로 무작위로 선택하거나(ii) sv에서 분리된 반주 스템을 무작위로 선택합니다.여기서 iv. 두 번째 방법의 경우 선택한 반주 스템에 기준 진실 노트가 없으므로 악기 출력을 마스크하고 보컬 출력의 손실만 계산합니다(식 1 참조).4.3. 구현 세부 정보 우리는 PyTorch[29]를 사용하여 시스템을 구현했습니다.오디오 파형은 16kHz 샘플링 속도로 다시 샘플링됩니다.모델 입력 길이를 6초로 설정합니다.그런 다음 Hann 윈도우의 2048개 샘플과 샘플의 홉 크기(즉, 20ms)를 사용하여 로그 크기 스펙트로그램을 계산합니다. 합성 모듈에는 3개의 잔여 Slakh All Piano Bass Drums Guitar MT......Ours (No-RM) ......Ours ......Strings Brass Organ Pipe Reed .433 .363 .282 ..632 .562 .578 ..732 .694 .666 .725 .S.lead S.pad C.perc. ........표 2가 포함됩니다. (Mix) 데이터 세트에서 학습하고 Slakh2100에서 테스트한 다양한 모델의 결과. MT3*는 [2]에 악기별 결과가 보고되지 않았으므로 복제본입니다. &quot;All&quot;은 다중 악기 Onset F1 점수를 나타냅니다. 다음 열은 개별 악기의 Onset F1 점수를 보여줍니다. “S.lead”, “S.pad&quot;, 및 &quot;C.perc.&quot;는 각각 Synth Lead, Synth Pad 및 Chromatic Percussion을 의미합니다. 데이터 집합 Slakh MAESTRO GuitarSet SpecTNT(싱글) ..MIRST.MT3(싱글) ...MT3(믹스) ...MT3+(믹스) ...Ours(싱글) ....Ours(믹스+보컬) ....EFN JDCnote(L+U) ..표 1. Onset F1 점수의 결과. MT3*는 복제입니다. (믹스) 또는 (믹스+보컬)이 있는 모델은 데이터 집합의 혼합에서 학습되고 (싱글)이 있는 모델은 단일 데이터 집합에서 학습됩니다. 각각 128개의 채널을 가지고 있으며 시간-주파수 필터가 (1, 2)인 평균 풀링 계층이 뒤따릅니다. Perceiver TF 모듈의 경우 다음 매개변수를 사용합니다(그림 1 참조): (i) 다음에 따라 다양한 실험 구성에서 각각 128의 차원을 사용하는 2J개의 잠재 배열을 초기화합니다.(ii) L = 3개의 Perceiver TF 블록을 쌓습니다.(iii) 각 Perceiver TF 블록에 대해 1개의 스펙트럼 교차 주의 층, N = 2개의 잠재 변환기 층, M = 2개의 시간 변환기 층을 사용합니다. 모든 변환기 층은 다중 헤드 주의를 위한 8개의 헤드와 함께 128의 숨겨진 크기를 갖습니다. 마지막으로, 출력 모듈은 128개의 숨겨진 유닛이 있는 2층 양방향 GRU입니다. Perceiver TF의 모든 변환기 모듈에는 0.15의 비율로 드롭아웃이 포함됩니다. 시작 및 프레임 활성화에 대한 출력 차원은 각각 128과 129이며, 여기서 128은 MIDI 피치에 해당하고 프레임 활성화의 추가 1차원은 무음을 위한 것입니다. 학습 최적화 도구로 AdamW[30]를 사용합니다. 초기 학습률과 가중치 감소율은 각각 103과 5 × 10¯³로 설정됩니다. 최종 출력의 경우 이진 표현을 얻기 위해 시작 및 프레임 확률 출력 모두에 대해 0.25의 임계값을 사용하므로 프레임별 활성화를 병합하여 각 음을 피아노 롤 표현으로 생성할 수 있습니다. 추가 후처리는 적용되지 않습니다. 데이터 증가의 경우, 교육 예제의 모든 비타악기 악기는 최대 3개의 반음만큼 피스 시프트 위아래로 이동할 확률이 100%입니다. 무작위 믹싱의 경우 다중 트랙 및 보컬 믹스 데이터 세트의 데이터에 대해 각각 p = 25% 및 q = 50%를 사용합니다. 입력 샘플을 생성하기 위해 각 교육 예제의 모든 악기 스템을 선형적으로 합산합니다. 4.4. 기준선 두 가지 최신 모델인 MT3[2] 및 SpecTNT[4]가 기준선으로 선택되었습니다. MT3의 경우, 테스트 세트에서 공식 모델 체크포인트 및 추론 파이프라인 2https://github.com/magenta/mt3/blob/main/mt3/colab/ music_transcription_with_transformers.ipynb를 포함하는 [2]²에 따라 모델을 복제했습니다.SpecTNT의 경우 [4]에서 보고된 보컬 멜로디 추출에 사용된 구성을 채택했습니다.예비 실험에서 다중 악기 설정에서 Slakh2100에서 원래 SpecTNT를 성공적으로 훈련하는 것이 쉽지 않다는 것을 알았으므로 이 실험은 건너뜁니다.보컬 필사의 경우 EFN [13] 및 JDCnote (L+U) [14]의 가장 좋은 결과가 보고되었습니다.4.5. 평가 지표 피치와 시작 타임스탬프의 정확성을 나타내는 &quot;Onset F1&quot; 점수를 이전 작업 [2]과 비교하기 위한 평가 지표로 사용합니다. 다중 악기 전사의 성능을 더욱 평가하기 위해 Slakh 데이터 세트에 대한 &quot;다중 악기 Onset F1&quot; 점수를 보고합니다. 복제된 MT3 모델의 출력은 프로그램 번호를 기준으로 12개 악기 클래스로 그룹화됩니다. 사용한 다중 악기 Onset F1 점수는 Onset F1만 계산하며, 이는 MV2H 메트릭[31]과 유사합니다. &quot;Drums&quot; 출력에는 명확한 오프셋 정보가 포함되지 않으므로 [2]에서 사용한 점수와 약간 다를 수 있습니다. 4.6. 결과 및 논의 표 1은 제안된 모델과 베이스라인 간의 Onset F1 측면 비교를 보여줍니다. 제안된 모델과 SpecTNT는 어텐션 메커니즘으로 스펙트럼 입력을 직접 모델링하며 GuitarSet과 같은 단일 데이터 세트의 낮은 리소스에서 학습된 경우에도 더 높은 성능을 보여줍니다. MIR-ST500에서 제안된 모델은 베이스라인보다 상당히 우수한 성능을 보입니다. SpecTNT(단일)는 MAESTRO에서 우리 모델보다 약간 더 나은 성능을 보이지만, 여전히 다음을 고려합니다. Perceiver TF는 더 나은 추론 효율성으로 인해 실제 사용에 더 유리합니다. 표 2는 Slakh2100에서 다중 악기 Onset F1(악기 가중 평균)과 개별 악기 클래스의 Onset F1 점수를 제시하여 악기별 성능을 보여줍니다. MT3*와 비교했을 때, 랜덤 믹싱 증강이 없는 모델(No-RM)은 &quot;Pipe&quot;와 같은 덜 흔한 악기에서 상당히 더 나은 성능을 보입니다(Onset F1 점수가 100% 이상 높음). 훈련에 랜덤 믹싱을 적용하면 모든 경우에서 성능을 더욱 높일 수 있으며, 이는 이 기술이 실제로 서로 다른 악기를 구별하는 모델의 견고성을 향상시킨다는 것을 나타냅니다. 마지막으로, 다중 악기와 보컬 전사를 결합하면 보컬 전사만 개선할 수 있는데, 결합된 모델은 더 무작위로 혼합된 보컬 반주 샘플로 훈련되기 때문입니다. 5.
--- CONCLUSION ---
우리는 다중트랙 AMT에 대한 모델 확장성 문제를 적절히 해결하는 새로운 아키텍처인 Perceiver TF를 제시했습니다. 악기 구별 문제를 해결하기 위해 데이터 세트 전반에서 데이터 사용성을 크게 용이하게 하는 랜덤 믹싱 증강 기술을 제안했습니다. 우리 시스템은 다양한 공개 데이터 세트에서 최첨단 성능을 보여주었습니다. 우리는 Perceiver TF가 일반적이며 다른 유사한 작업에도 적용될 수 있다고 믿습니다. 6. 참고문헌 [1] Kin Wai Cheuk, Dorien Herremans, Li Su, &quot;Reconvat: 저자원 실제 데이터를 위한 반지도 자동 음악 전사 프레임워크&quot;, Proc. ACM Multimedia, 2021, pp. 3918-3926. [2] Josh Gardner, Ian Simon, Ethan Manilow, Curtis Hawthorne 및 Jesse Engel, &quot;MT3: 다중 작업 다중 트랙 음악 전사,&quot; Proc. ICLR, 2021. [3] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman 및 Joao Carreira, &quot;Perceiver: 반복적 주의를 통한 일반 지각,&quot; Proc. ICML, 2021, pp. 4651-4664. [4] Wei-Tsung Lu, Ju-Chiang Wang, Minz Won, Keunwoo Choi 및 Xuchen Song, &quot;SpecTNT: 음악 오디오를 위한 시간-주파수 변환기,&quot; Proc. ISMIR, 2021. [5] Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Naoya Takahashi 및 Yuki Mitsufuji, &quot;데이터 증강 및 네트워크 블렌딩을 통한 딥 신경망 기반 음악 소스 분리 개선&quot;, Proc. ICASSP, 2017, pp. 261–265. [6] Xuchen Song, Qiuqiang Kong, Xingjian Du 및 Yuxuan Wang, &quot;Catnet: 믹스오디오 증강을 갖춘 음악 소스 분리 시스템,&quot; arXiv 사전 인쇄본 arXiv:2102.09966, 2021. [7] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, Derry FitzGerald 및 Bryan Pardo, &quot;음악에서 리드와 반주 분리에 대한 개요,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 논문, 제26권, 제8호, 1307-1335쪽, 2018. [8] Yun-Ning Hung, Yi-An Chen 및 Yi-Hsuan Yang, &quot;프레임 수준 악기 인식을 위한 멀티태스크 학습,&quot; Proc. ICASSP, 2019, 381-385쪽. [9] Andreas Jansson, Rachel M Bittner, Sebastian Ewert, Tillman Weyde, &quot;심층 u-net 아키텍처를 통한 공동 노래 음성 분리 및 fo 추정&quot;, Proc. EUSIPCO, 2019, pp. 1-5. [10] Yu-Te Wu, Berlin Chen, Li Su, &quot;자기 주의 기반 인스턴스 분할을 통한 다중 악기 자동 음악 전사&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, vol. 28, pp. 2796–2809, 2020. [11] Keitaro Tanaka, Takayuki Nakatsuka, Ryo Nishikimi, Kazuyoshi Yoshii, Shigeo Morishima, &quot;스펙트로그램 및 피치그램의 심층 구형 클러스터링을 기반으로 한 다중 악기 음악 전사&quot;, Proc. ISMIR, 2020. [12] Kin Wai Cheuk, Keunwoo Choi, Qiuqiang Kong, Bochen Li, Minz Won, Amy Hung, Ju-Chiang Wang, Dorien Herremans, &quot;Jointist: 다중 악기 전사를 위한 공동 학습 및 그 응용 프로그램,&quot; arXiv 사전 인쇄본 arXiv:2206.10805, 2022. [13] Jun-You Wang 및 Jyh-Shing Roger Jang, &quot;대규모 노래 전사 데이터 세트의 준비 및 검증에 관하여,&quot; Proc. ICASSP, 2021, pp. 276-280. [14] Sangeun Kum, Jongpil Lee, Keunhyoung Luke Kim, Taehyoung Kim, Juhan Nam, &quot;다성음 음악의 노래 전사를 위한 교사-학생 프레임워크에서 프레임 수준에서 음표 수준으로의 가상 레이블 전송,&quot; Proc. 영어: ICASSP, 2022. [15] Jui-Yang Hsu 및 Li Su, &quot;Vocano: 다성음악에서 노래하는 목소리를 위한 음표 전사 프레임워크.&quot; Proc. ISMIR, 2021, pp. 293–300. [16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., &quot;Conformer: 음성 인식을 위한 합성곱 증강 변환기.&quot; Proc. INTERSPEECH, 2020. [17] M. Won, K. Choi, 및 X. Serra, &quot;반지도 음악 태그 변환기,&quot; Proc. ISMIR, 2021, 769-776쪽. [18] Yun-Ning Hung, Ju-Chiang Wang, Xuchen Song, Wei-Tsung Lu, 및 Minz Won, &quot;시간-주파수 변환기를 사용한 비트 및 다운비트 모델링,&quot; Proc. ICASSP, 2022, 401-405쪽. [19] Ju-Chiang Wang, Yun-Ning Hung, 및 Jordan BL Smith, &quot;합창, 구절, 인트로 또는 다른 것을 포착하려면 구조적 기능이 있는 노래 분석,&quot; Proc. 영어: ICASSP, 2022, pp. 416420. [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 및 Jian Sun, &quot;심층 잔여 네트워크에서의 정체성 매핑&quot;, 컴퓨터 비전에 관한 유럽 컨퍼런스에서.Springer, 2016, pp. 630-645. [21] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, 및 Douglas Eck, &quot;Onsets and frames: Dual-objective piano transcription,&quot; Proc. ISMIR, 2018, pp. 50-57. [22] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, 및 Yoshua Bengio, &quot;시퀀스 모델링에서 게이트형 순환 신경망의 실증적 평가&quot;, Proc. NeurIPS, 2014. [23] Ethan Manilow, Gordon Wichern, Prem Seetharaman 및 Jonathan Le Roux, &quot;Slakh에서 음악 소스 분리 절단: 교육 데이터 품질 및 양의 영향을 연구하기 위한 데이터 세트&quot;, Proc. WASPAA에 게재됨. IEEE, 2019. [24] Colin Raffel, &quot;오디오-미디 정렬 및 매칭에 응용되는 시퀀스 비교를 위한 학습 기반 방법&quot;, 2016, 컬럼비아 대학교. [25] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, Douglas Eck, &quot;maestro 데이터 세트를 사용하여 요인화된 피아노 음악 모델링 및 생성 활성화&quot;, Proc. ICLR, 2019. [26] Qingyang Xi, Rachel M Bittner, Johan Pauwels, Xuzhou Ye, Juan Pablo Bello, &quot;Guitarset: 기타 전사를 위한 데이터 세트.&quot;, Proc. ISMIR, 2018, pp. 453–460. [27] Sangeun Kum 및 Juhan Nam, &quot;노래 음성 멜로디의 공동 감지 및 분류를 사용하여 영어: convolutional recurrent neural networks,&quot; Applied Sciences, vol. 9, no. 7, pp. 1324, 2019. [28] Qiuqiang Kong, Yin Cao, Haohe Liu, Keunwoo Choi, and Yuxuan Wang, &quot;Decoupling sizes and phase estimation with deep resunet for music source separation.,&quot; in Proc. ISMIR, 2021. [29] Paszke et al., &quot;Pytorch: An imperative style, high-performance deep learning library,&quot; in Neural Information Processing Systems, 2019, vol. 32. [30] Ilya Loshchilov and Frank Hutter, &quot;Decoupled weight decay regularization,&quot; in Proc. ICLR, 2017. [31] Andrew McLeod and Mark Steedman, &quot;Evaluating automatic polyphonic music transcription.,&quot; in Proc. ISMIR, 2018, 42-49쪽.
