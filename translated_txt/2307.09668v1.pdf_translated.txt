--- ABSTRACT ---
언어 모델과 비전 언어 모델은 최근 텍스트 형태로 인간의 의도, 추론, 장면 이해 및 계획과 같은 행동을 이해하는 측면에서 전례 없는 역량을 보여주었습니다. 이 연구에서 우리는 강화 학습(RL) 에이전트에 이러한 능력을 내장하고 활용하는 방법을 조사합니다. 우리는 언어를 핵심 추론 도구로 사용하는 프레임워크를 설계하여 에이전트가 효율적인 탐색, 경험 데이터 재사용, 스케줄링 기술 및 관찰을 통한 학습과 같은 일련의 기본적인 RL 과제를 해결할 수 있는 방법을 탐구합니다. 이러한 과제는 전통적으로 별도의 수직적으로 설계된 알고리즘이 필요합니다. 우리는 로봇이 일련의 객체를 쌓아야 하는 희소 보상 시뮬레이션 로봇 조작 환경에서 우리의 방법을 테스트합니다. 우리는 탐색 효율성과 오프라인 데이터 세트의 데이터를 재사용하는 능력에서 기준선에 비해 상당한 성능 향상을 보여주고, 학습된 기술을 재사용하여 새로운 과제를 해결하거나 인간 전문가의 비디오를 모방하는 방법을 설명합니다.
--- INTRODUCTION ---
최근 몇 년 동안 문헌에서는 주목할 만한 딥러닝(DL) 성공 사례(3)가 연이어 발표되었으며, 특히 자연어 처리(4; 19; 8; 29)와 컴퓨터 비전(2; 25; 36; 37) 분야에서 획기적인 진전이 있었습니다. 양식은 다르지만 이러한 결과는 공통적인 구조를 공유합니다. 즉, 자체 지도 학습 방법(19; 6)을 사용하여 엄청난 웹 규모 데이터 세트(39; 19)에서 학습한 대규모 신경망(종종 트랜스포머(46))입니다. 구조는 간단하지만, 이 레시피는 놀라울 정도로 효과적인 Large Language Models(LLMs)(4)의 개발로 이어졌습니다. 이는 뛰어난 인간과 같은 기능으로 텍스트를 처리하고 생성할 수 있고, Vision Transformers(ViTs)(25; 13)는 감독 없이 이미지와 비디오에서 의미 있는 표현을 추출할 수 있고(6; 18), VisionLanguage Models(VLMs)(2; 36; 28)는 언어로 시각적 입력을 설명하거나 언어 설명을 시각적 출력으로 변환하는 데이터 모달리티를 연결할 수 있습니다. 이러한 모델의 크기와 기능으로 인해 커뮤니티에서 Foundation Models(3)라는 용어를 만들어냈으며, 이러한 모델을 다양한 입력 모달리티를 포함하는 다운스트림 애플리케이션의 백본으로 사용할 수 있는 방법을 제안했습니다. 이를 통해 다음과 같은 질문이 생겼습니다. (Vision) Language Models의 성능과 기능을 활용하여 보다 효율적이고 일반적인 강화 학습 에이전트를 설계할 수 있을까요? 문헌에서는 웹 스케일 텍스트 및 시각적 데이터로 학습한 후, 이러한 모델에서 상식적 추론, 하위 목표 제안 및 시퀀싱, 시각적 이해 및 기타 속성이 나타나는 것을 관찰했습니다(19; 4; 8; 29). 이는 모두 환경과 상호 작용하고 환경으로부터 학습해야 하지만 시행착오에서 깨끗한 상태로 돌아오는 데 비현실적으로 오랜 시간이 걸릴 수 있는 에이전트의 기본적인 특성입니다. Foundation Models에 저장된 지식을 활용하면 이 프로세스를 엄청나게 부트스트랩할 수 있습니다. 이 아이디어에 동기를 부여받아, 특히 처음부터 학습하는 맥락에서 RL 로봇 에이전트의 핵심에 언어를 두는 프레임워크를 설계합니다. 우리의 핵심 기여와 발견은 다음과 같습니다. *Google DeepMind에서 인턴십을 하는 동안 수행한 작업. ICLRLLM에서 강화 학습 워크숍을 환생시키는 것 obs VLM 언어 LangCond 정책 작업 그림 1: 프레임워크의 상위 수준 설명. 우리는 LLM과 VLM을 활용하는 이 프레임워크가 RL 설정에서 1) 희소 보상 환경을 효율적으로 탐색, 2) 수집된 데이터를 재사용하여 새로운 과제의 학습을 순차적으로 부트스트랩, 3) 학습된 기술을 스케줄링하여 새로운 과제를 해결, 4) 전문가 에이전트의 관찰을 통한 학습과 같은 일련의 근본적인 문제를 해결할 수 있음을 보여줍니다. 최근 문헌에서 이러한 과제는 개별적으로 처리하기 위해 다르고 특별히 설계된 알고리즘이 필요하지만, 우리는 Foundation Models의 기능이 보다 통합된 접근 방식을 개발할 가능성을 열어준다는 것을 보여줍니다. 2
--- RELATED WORK ---
지난 몇 년 동안 모델의 매개변수 수와 학습 데이터 세트의 크기와 다양성을 확장하면서 (Vision) 언어 모델(4; 19; 2; 19; 8)에서 전례 없는 기능이 탄생했습니다. 이는 다시 세상과 상호 작용하는 에이전트 내에서 이러한 모델을 활용하는 여러 애플리케이션으로 이어졌습니다. 이전 작업에서는 시뮬레이션 환경에서 RL 에이전트와 함께 LLM 및 VLM을 사용했지만(12; 44), 에이전트를 학습하기 위해 대량의 데모를 수집하는 데 의존했습니다. 대신, 우리는 처음부터 RL 에이전트를 학습하는 문제에 초점을 맞추고 LLM 및 VLM을 활용하여 진행을 가속화합니다. 이전 작업에서는 로봇 응용 프로그램에 LLM 및 VLM을 활용하는 방법도 살펴보았습니다. 특히(1; 21; 50; 20) LLM을 활용하여 장기적 작업의 하위 목표를 계획하고 VLM을 활용하여 장면을 이해하고 요약했습니다. 그런 다음 이러한 하위 목표를 언어 조건 정책을 통해 동작으로 전환할 수 있습니다(22; 30). 이러한 작업의 대부분은 LLM을 통해 이미 학습한 기술을 배치하고 스케줄링하는 데 초점을 맞추지만, 실제 세계에서는 그렇지 않습니다. 반면, 저희의 작업은 이러한 행동을 처음부터 학습하는 RL 시스템에 초점을 맞추고 있으며, 이러한 모델이 탐색, 이전 및 경험 재사용에 가져다주는 이점을 강조합니다. 여러
--- METHOD ---
로봇이 객체 집합을 쌓아야 하는 희소 보상 시뮬레이션 로봇 조작 환경에서. 우리는 탐색 효율성과 오프라인 데이터 세트의 데이터를 재사용하는 능력에서 기준선에 비해 상당한 성능 향상을 보여주고, 학습한 기술을 재사용하여 새로운 과제를 해결하거나 인간 전문가의 비디오를 모방하는 방법을 설명합니다. 서론 최근 몇 년 동안 문헌에서는 주목할 만한 딥 러닝(DL) 성공 사례(3)가 연이어 발표되었으며, 특히 자연어 처리(4; 19; 8; 29)와 컴퓨터 비전(2; 25; 36; 37) 분야에서 획기적인 진전이 있었습니다. 양식은 다르지만 이러한 결과는 공통된 구조를 공유합니다. 즉, 자기 지도 학습 방법(19; 6)을 사용하여 엄청난 웹 규모 데이터 세트(39; 19)에서 학습한 대규모 신경망(종종 트랜스포머(46))입니다. 구조는 간단하지만, 이 레시피는 놀라울 정도로 효과적인 Large Language Models(LLMs)(4)의 개발로 이어졌습니다. 이는 뛰어난 인간과 같은 기능으로 텍스트를 처리하고 생성할 수 있고, Vision Transformers(ViTs)(25; 13)는 감독 없이 이미지와 비디오에서 의미 있는 표현을 추출할 수 있고(6; 18), VisionLanguage Models(VLMs)(2; 36; 28)는 언어로 시각적 입력을 설명하거나 언어 설명을 시각적 출력으로 변환하는 데이터 모달리티를 연결할 수 있습니다. 이러한 모델의 크기와 기능으로 인해 커뮤니티에서 Foundation Models(3)라는 용어를 만들어냈으며, 이러한 모델을 다양한 입력 모달리티를 포함하는 다운스트림 애플리케이션의 백본으로 사용할 수 있는 방법을 제안했습니다. 이를 통해 다음과 같은 질문이 생겼습니다. (Vision) Language Models의 성능과 기능을 활용하여 보다 효율적이고 일반적인 강화 학습 에이전트를 설계할 수 있을까요? 문헌에서는 웹 스케일 텍스트 및 시각적 데이터로 학습한 후, 이러한 모델에서 상식적 추론, 하위 목표 제안 및 시퀀싱, 시각적 이해 및 기타 속성이 나타나는 것을 관찰했습니다(19; 4; 8; 29). 이는 모두 환경과 상호 작용하고 환경으로부터 학습해야 하지만 시행착오에서 깨끗한 상태로 돌아오는 데 비현실적으로 오랜 시간이 걸릴 수 있는 에이전트의 기본적인 특성입니다. Foundation Models에 저장된 지식을 활용하면 이 프로세스를 엄청나게 부트스트랩할 수 있습니다. 이 아이디어에 동기를 부여받아, 특히 처음부터 학습하는 맥락에서 RL 로봇 에이전트의 핵심에 언어를 두는 프레임워크를 설계합니다. 우리의 핵심 기여와 발견은 다음과 같습니다. *Google DeepMind에서 인턴십을 하는 동안 수행한 작업. ICLRLLM에서 강화 학습 워크숍을 환생시키는 것 obs VLM 언어 LangCond 정책 작업 그림 1: 프레임워크의 상위 수준 설명. 우리는 LLM과 VLM을 활용하는 이 프레임워크가 1) 희소 보상 환경을 효율적으로 탐색, 2) 수집된 데이터를 재사용하여 새로운 작업의 학습을 순차적으로 부트스트랩, 3) 학습된 기술을 스케줄링하여 새로운 작업을 해결, 4) 전문가 에이전트의 관찰을 통한 학습과 같은 RL 설정의 일련의 근본적인 문제를 해결할 수 있음을 보여줍니다.최근 문헌에서 이러한 작업은 개별적으로 처리하기 위해 다르고 특별히 설계된 알고리즘이 필요한 반면, 우리는 Foundation Models의 기능이 보다 통합된 접근 방식을 개발할 가능성을 열어준다는 것을 보여줍니다.2 관련 작업 지난 몇 년 동안 모델의 매개변수 수와 교육 데이터 세트의 크기와 다양성을 확장하면서 (Vision) Language Models에서 전례 없는 기능이 탄생했습니다(4; 19; 2; 19; 8).이로 인해 세상과 상호 작용하는 에이전트 내에서 이러한 모델을 활용하는 여러 응용 프로그램이 탄생했습니다.이전 작업에서는 시뮬레이션 환경에서 RL 에이전트와 함께 LLM과 VLM을 사용했지만(12; 44), 에이전트를 교육하기 위해 대량의 데모를 수집하는 데 의존합니다. 대신, 우리는 처음부터 RL 에이전트를 학습하는 문제에 초점을 맞추고 LLM과 VLM을 활용하여 진행을 가속화합니다. 이전 작업에서는 로봇 응용 프로그램에 LLM과 VLM을 활용하는 것도 살펴보았습니다. 특히(1; 21; 50; 20) LLM을 활용하여 장기적 작업의 하위 목표를 계획하고 VLM을 사용하여 장면을 이해하고 요약했습니다. 이러한 하위 목표는 언어 조건 정책을 통해 행동으로 이어질 수 있습니다(22; 30). 이러한 작업의 대부분은 실제 세계에서 LLM을 통해 이미 학습한 기술을 배포하고 스케줄링하는 데 초점을 맞추지만, 우리의 작업은 이러한 행동을 처음부터 학습하는 RL 시스템에 초점을 맞춰 이러한 모델이 탐색, 전송 및 경험 재사용에 가져다주는 이점을 강조합니다. 커리큘럼 학습(43; 51; 31; 16), 내재적 동기(17; 35) 또는 계층적 분해(32; 27)를 통해 희소 보상 작업을 해결하기 위한 여러 가지 방법이 제안되었습니다. 우리는 LLM이 추가 학습이나 미세 조정 없이 학습 커리큘럼을 제로샷으로 생성할 수 있는 방법과 VLM이 이러한 하위 목표에 대한 보상을 자동으로 제공하여 학습 속도를 크게 향상시킬 수 있는 방법을 보여줍니다. 관련 연구에서는 새로운 작업에 대한 보상 모델을 학습하여 로봇 경험의 대규모 데이터 세트를 재사용하는 방법도 살펴보았습니다(5). 그러나 각 새 작업에 대해 원하는 보상에 대한 수많은 인간 주석을 수집해야 합니다. 대신, 동시 관련 작업(48)에서 보고한 대로, 대상 도메인의 소량 데이터로 미세 조정할 수 있는 VLM을 활용하여 과거 경험을 성공적으로 다시 레이블링하는 방법을 보여줍니다. (15)는 우리 작업과 가장 유사한 방법입니다. 그들은 Minecraft에서 희소 보상 작업을 학습하기 위해 LLM과 VLM 간의 상호 작용을 제안합니다(23; 24). 그러나 몇 가지 주목할 만한 차이점이 있습니다. 그들은 비디오, 게시물 및 튜토리얼의 방대한 인터넷 데이터 세트를 사용하여 모델을 미세 조정하는 반면, 우리는 최소 1000개의 데이터 포인트로 VLM을 효과적으로 미세 조정할 수 있고 기성품 LLM을 사용할 수 있음을 보여줍니다. 또한 우리는 또한 조사하고
--- EXPERIMENT ---
이 프레임워크를 사용하여 데이터 재사용 및 전송, 관찰을 통한 학습, 탐색 및 기술 스케줄링에 어떻게 사용할 수 있는지, 강화 학습의 핵심 과제에 대한 보다 통합된 접근 방식을 제안합니다.예비 실험에는 MuJoCo 물리 시뮬레이터(45)로 모델링한 Lee et al.(26)의 시뮬레이션 로봇 환경을 사용합니다.로봇 팔은 빨간색, 파란색, 바구니에 있는 녹색 물체로 구성된 환경과 상호 작용합니다.이를 마르코프 결정 프로세스(MDP)로 공식화합니다.상태 공간 S는 물체와 엔드 이펙터의 3D 위치를 나타냅니다.로봇은 위치 제어를 통해 제어됩니다.행동 공간 A는 로봇의 알려진 역 운동학을 사용하여 도달하는 x, y 위치로 구성되며, 여기서 로봇 팔은 (49; 40)에서 영감을 받아 물체를 집거나 놓을 수 있습니다.관찰 공간 O는 바구니 가장자리에 고정된 두 카메라에서 나오는 128 × 128 × 3 RGB 이미지로 구성됩니다. 에이전트는 해결해야 할 작업에 대한 언어 설명을 받는데, 이는 두 가지 형태가 있습니다. &quot;X를 Y 위에 쌓기&quot;, 여기서 X와 Y는 {&quot;빨간색 개체&quot;, &quot;녹색 개체&quot;, &quot;파란색 개체&quot;}에서 대체 없이 가져오거나, &quot;세 개의 개체를 모두 쌓기&quot;, 우리는 이를 트리플 스택이라고도 합니다. 에피소드가 성공하면 +1의 긍정적 보상이 제공되고, 다른 경우에는 보상 0이 제공됩니다. 우리는 작업의 희소성을 균일한 분포에서 샘플링된 무작위 작업을 실행할 때 작업을 해결하고 단일 보상을 받는 데 필요한 평균 환경 단계 수로 정의합니다. 우리가 채택한 MDP 설계를 사용하면 두 개의 개체를 쌓는 것은 10³의 희소성을 가지지만, 최적 정책은 2개의 픽 앤 플레이스 작업/단계(49; 40)로 작업을 해결할 수 있습니다. 세 객체를 모두 쌓으면 무작위 정책에서 궤적을 평가하여 측정한 결과 희소성이 106 이상인 반면, 최적 정책은 4단계로 작업을 해결할 수 있습니다.언어 중심 에이전트를 위한 프레임워크 이 작업의 목표는 방대한 이미지 및 텍스트 데이터 세트에서 사전 학습된 Foundation Models(3)를 사용하여 보다 일반적이고 통합된 RL 로봇 에이전트를 설계하는 것입니다. 우리는 LLM 및 VLM의 뛰어난 능력을 사용하여 환경, 작업 및 수행해야 할 작업에 대해 전적으로 언어를 통해 추론할 수 있는 기능을 처음부터 RL 에이전트에 제공하는 프레임워크를 제안합니다. 이를 위해 에이전트는 먼저 시각적 입력을 텍스트 설명에 매핑해야 합니다. 둘째, 이러한 텍스트 설명과 작업에 대한 설명으로 LLM을 프롬프트하여 에이전트에게 언어 지침을 제공해야 합니다. 마지막으로 에이전트는 LLM의 출력을 작업으로 접지해야 합니다. &quot;로봇이 빨간색 물체를 움켜쥐고 있습니다.&quot; CLIP - 텍스트 인코더 CLIP - 이미지 인코더 병렬 점곱 0.-0.0. 유사도 점수 VLM을 사용하여 시각과 언어 연결: RGB 카메라에서 가져온 시각적 입력(3장)을 언어 형태로 설명하기 위해 대규모 대조 시각 언어 모델인 CLIP을 사용합니다(36). CLIP은 이미지 인코더 1과 텍스트 인코더 OT로 구성되며, 노이즈가 있는 쌍을 이룬 이미지와 텍스트 설명의 방대한 데이터 세트에서 학습되었으며, 이를 캡션이라고도 합니다. 각 인코더는 128차원 임베딩 벡터를 출력합니다. 이미지 임베딩과 일치하는 텍스트 설명은 큰 코사인 유사도를 갖도록 최적화됩니다. 환경에서 이미지의 언어 설명을 생성하기 위해 에이전트는 관찰 ot를 1에 공급하고 가능한 캡션 Ɩ를 T에 공급합니다(그림 2). 우리는 임베딩 벡터 간의 점곱을 계산하고 결과가 하이퍼파라미터인 Y보다 큰 경우 설명이 올바른 것으로 간주합니다(실험에서 y = 0.8, 자세한 내용은 부록 참조). 로봇 스태킹 작업에 집중함에 따라 설명은 &quot;로봇이 X를 잡고 있습니다&quot; 또는 &quot;X가 Y 위에 있습니다&quot;의 형태이며, 여기서 X와 Y는 {&quot;빨간색 물체&quot;, &quot;녹색 물체&quot;, &quot;파란색 물체&quot;}에서 중복 없이 가져옵니다. 우리는 시뮬레이션된 스태킹 도메인의 소량의 데이터에 대해 CLIP을 미세 조정합니다. 이것이 작동하는 방식에 대한 자세한 내용과 미세 조정을 위한 데이터 요구 사항에 대한 분석은 부록에 제공됩니다. 그림 2: 관찰과 텍스트 설명 간의 유사성을 점곱으로 계산하는 CLIP의 예. LLM을 통한 언어를 통한 추론: 언어 모델은 언어 형태의 프롬프트를 입력으로 받고 다음 토큰의 확률 분포를 자기 회귀적으로 계산하고 이 분포에서 샘플링하여 언어를 출력으로 생성합니다. 우리의 설정에서 LLM의 목표는 현재 작업을 나타내는 텍스트 지침(예: &quot;파란색 물체 위에 빨간색 물체를 쌓으세요&quot;)을 취하고 로봇이 해결할 하위 목표 세트를 생성하는 것입니다. 우리는 언어 지침 데이터 세트에 대해 미세 조정된 LLM인 FLAN-T5(10)를 사용합니다. 우리가 수행한 정성적 분석에 따르면 지침에 대해 미세 조정되지 않은 LLM보다 약간 더 나은 성능을 보였습니다. 이러한 LLM의 뛰어난 맥락 내 학습 기능 덕분에 도메인 내 미세 조정이 필요 없이 기성품(4; 34)을 사용하고 가능한 한 적은 수의 하위 목표 계획인 &quot;작업: 파란색 물체 위에 빨간색 물체를 쌓으세요.&quot;를 제공하여 동작을 안내할 수 있습니다. LLM &quot;로봇이 빨간색 물체를 잡고 있습니다. 빨간색 물체가 파란색 물체 위에 있습니다.&quot; VLM OT(91) · 1(t) 자체 생성 움켜잡기 보상 &quot;로봇이 빨간색 물체를 움켜잡고 있다&quot; 환경 보상 그림 3: VLM은 LLM이 제안한 언어 목표를 수집된 관찰과 비교하여 내부 보상 모델 역할을 할 수 있습니다. 작업 지시와 원하는 언어 출력의 두 가지 예로: 환경 설정을 설명하고 LLM에 제안된 작업을 해결하는 데 도움이 되는 하위 목표를 찾도록 요청하고 이러한 작업의 두 가지 예와 상대적인 하위 목표 분해를 제공합니다. 이를 통해 LLM은 콘텐츠뿐만 아니라 효율적인 구문 분석을 가능하게 하는 출력 언어의 형식에서도 원하는 동작을 에뮬레이션할 수 있었습니다. 부록에서 사용하는 프롬프트와 LLM의 동작에 대한 보다 자세한 설명을 제공합니다. 지침을 행동으로 전환: LLM이 제공한 언어 목표는 언어 조건 정책 네트워크를 사용하여 행동으로 전환됩니다. 이 네트워크는 Transformer(46)로 매개변수화되며, 언어 하위 목표의 임베딩과 객체와 로봇 엔드 이펙터의 위치를 포함한 타임스텝 t에서의 MDP의 상태를 입력으로 받고, 각각 다른 벡터로 표현하여 로봇이 실행할 동작을 타임스텝 t+1로 출력합니다. 이 네트워크는 아래에서 설명하는 대로 RL 루프 내에서 처음부터 학습합니다. 수집 및 추론 학습 패러다임: 에이전트는 수집 및 추론 패러다임(38)에서 영감을 받은 방법을 통해 환경과의 상호 작용을 통해 학습합니다. 수집 단계에서 에이전트는 환경과 상호 작용하고 상태, 관찰, 동작 및 현재 목표의 형태로 데이터를 (St, Ot, at, 9₁)로 수집하여 정책 네트워크 fø(St, 9i) → at을 통해 동작을 예측합니다. 각 에피소드 후에 에이전트는 VLM을 사용하여 수집된 데이터에서 하위 목표가 발견되었는지 추론하고, 나중에 자세히 설명할 대로 추가 보상을 추출합니다. 에피소드가 보상으로 끝나거나 VLM에서 보상이 제공되면 에이전트는 보상 타임스텝 [(So, 00, ao, gi),..., (ST, -1, OT,-1, aT,-1, 9₁)]까지 에피소드 데이터를 경험 버퍼에 저장합니다.그림 4(왼쪽)에서 이 파이프라인을 설명합니다.이러한 단계는 동일한 경험 버퍼에 데이터를 수집하는 N개의 분산 병렬 에이전트에 의해 실행됩니다(저희 작업에서는 N = 1000).Infer 단계에서는 각 에이전트가 에피소드를 완료한 후(따라서 총 N개의 에피소드) 이 경험 버퍼에서 행동 복제를 통해 정책을 훈련하여 성공적인 에피소드에서 자기 모방의 한 형태를 구현합니다(33; 14; 7).그런 다음 정책의 업데이트된 가중치가 모든 분산 에이전트와 공유되고 프로세스가 반복됩니다.응용 프로그램 및 결과 프레임워크를 구성하는 빌딩 블록을 설명했습니다. 에이전트의 핵심으로 언어를 사용하면 RL에서 일련의 근본적인 과제를 해결하기 위한 통합된 프레임워크를 제공합니다.다음 섹션에서는 탐색, 과거 경험 데이터 재사용, 스케줄링 및 기술 재사용, 관찰을 통한 학습에 중점을 두고 이러한 기여 각각을 조사합니다.전체 프레임워크는 알고리즘 1에도 설명되어 있습니다.5. 탐색 - 언어를 통한 커리큘럼 생성 RL은 신중하게 제작된 밀도 있는 보상으로부터 상당한 이점을 얻습니다(5).그러나 많은 실제 환경에서 밀도 있는 보상의 존재는 드뭅니다.로봇 에이전트는 복잡한 환경에서 광범위한 작업을 학습할 수 있어야 하지만 작업 수가 증가함에 따라 밀도 있는 보상 함수를 엔지니어링하는 데 엄청난 시간이 소요됩니다.따라서 이러한 과제를 극복하고 RL을 확장하려면 효율적이고 일반적인 탐색이 필수적입니다.수년에 걸쳐 희소 보상 환경의 탐색을 해결하기 위해 다양한 방법이 개발되었습니다(43; 51; 31; 16; 17; 35; 32; 27). 많은 사람들이 커리큘럼 생성 및 학습을 통해 긴 수평선 작업을 더 짧고 배우기 쉬운 작업으로 분해하는 것을 제안합니다. 일반적으로 이러한 방법은 ICLR에서 강화 학습 워크숍을 환생시킵니다.수집합니다.상호 작용 데이터를 수집합니다.&quot;Stack red on blue&quot; 작업에 대한 학습 곡선 작업 언어 설명 60% 언어 커리큘럼 환경 보상만 10% actor actor actor 경험 버퍼 LLM VLM 추론 언어 계획에서 보상 추출 30% 성공률 Triple Stack 작업에 대한 학습 곡선 언어 커리큘럼 환경 보상만 아직 보상을 하나도 받지 못했습니다.언어 조건 정책 0% 0% 환경 단계(x1000) 환경 단계(x1000) 그림 4: 왼쪽: Collect &amp; Infer 파이프라인의 그림입니다. 가운데, 오른쪽: Stack Red on Blue 및 Triple Stack 작업에서 프레임워크와 기준선의 학습 곡선입니다. 작업을 처음부터 분해하는 법을 배워야 하며, 전반적인 학습 효율성을 저해합니다. 우리는 LLM을 활용하는 RL 에이전트가 과거 환경 상호 작용 없이 생성된 텍스트 하위 목표의 커리큘럼을 어떻게 활용할 수 있는지 보여줍니다. 탐색을 안내하기 위해 에이전트는 LLM에 작업 설명 Tn을 제공하여 작업을 더 짧은 수평선의 하위 목표로 분해하여 텍스트 형태¹로 go:G 목표의 커리큘럼을 효과적으로 생성하도록 지시합니다. 에이전트는 에서 fo(St, Tn)으로 동작을 선택합니다. 환경은 T가 해결된 경우에만 보상을 제공하지만 VLM은 추가적이고 덜 희소한 보상 모델 역할을 하도록 배치됩니다. 에피소드 동안 수집된 관찰 00:π와 LLM에서 제안한 모든 텍스트 하위 목표 go:G가 주어지면 하위 목표 중 어느 것이든 어느 단계에서 해결되었는지 확인합니다. 우리는 T(9i)·Φι(0+) &gt; γ인 경우 하위 목표 gå에 대한 완료 상태를 나타내는 관찰을 고려합니다. 이 경우 에이전트는 [(So, 00, ao, Tn), . . ., (St−1, Ot−1, at−1, Tn)]을 경험 버퍼에 추가합니다. 이 프로세스는 그림 3, 11(부록)에 설명되어 있습니다. 스택 X on Y 및 트리플 스택에 대한 결과. 그림 4에서 환경 보상을 통해서만 학습하는 기준 에이전트와 프레임워크를 비교합니다. 학습 곡선은 모든 작업에서 기준선보다 우리 방법이 훨씬 더 효율적인 방법을 명확하게 보여줍니다. 주목할 점은 트리플 스택 작업에서 에이전트의 학습 곡선이 빠르게 증가하는 반면, 기준 에이전트는 작업의 희소성이 106이기 때문에 여전히 단일 보상을 받아야 합니다. 부록에서 추출된 하위 목표와 보상의 시각적 예를 제공합니다. 알고리즘 1 언어 중심 에이전트 1: 훈련 시간: 2: 작업에 대해 작업에서 수행 3: 4: 하위 목표 = LLM(작업) //작업 설명이 주어진 텍스트 하위 목표 찾기 exp_buffer.append( VLM(offline_buffer, subgoals)) // 과거 작업에서 수집된 오프라인 버프에서 성공적인 eps 추출(5.2절) (5.1절) 5: 에피소드에서 ер에 대해 수행 6: 7: E [S0:T, 00:T, ao:T, 9i] //ep 수집 trajectory 8: 9: rinternal VLM(E, subgoals) //하위 목표에 대한 추가 보상 추출 10: 11: 12:13: rcollect final reward if r or internal then exp_buffer.append (Eo:T) //reward가 발생할 때까지 타임스텝 추가 if ep% N == 0 then 0BC(episode_buffer) //N eps마다 BC를 사용하여 에이전트 학습 14: 테스트 시간: 15: text_instruction 또는 video_demo 수신 16: if text_instruction then = 17: subgoals LLM(text_instruction) (5.3절) 18: else if video_demo then 19: subgoals = VLM(video_demo) 20: execute(subgoals) (5.4절) (5.3절) 이러한 결과는 주목할 만한 사실을 시사합니다. 작업의 희소성을 작업 수와 비교할 수 있습니다. 그림 5와 같이 특정 성공률에 도달하는 데 필요한 단계 수입니다. 세 가지 중 가장 쉬운 Grasp the Red Object 작업에 대해서도 방법을 학습합니다. 희소성은 10¹ 정도입니다. 프레임워크에서 필요한 단계 수가 작업의 희소성보다 느리게 증가하는 것을 볼 수 있습니다. 이는 일반적으로 강화 학습(35)에서 그 반대이기 때문에 특히 중요한 결과입니다. ¹예를 들어, LLM은 &quot;빨간색 물체를 파란색 물체 위에 쌓는다&quot;를 다음과 같은 하위 목표로 분해합니다. [&quot;로봇이 빨간색 물체를 잡고 있습니다&quot;, &quot;빨간색 물체가 파란색 물체 위에 있습니다&quot;]ICLR에서 강화 학습 워크숍을 환생시킵니다.새로운 작업: &quot;빨간색 물체를 빨간색 물체 위에 쌓는다&quot; LLM 오프라인 데이터 하위 목표 계획: [로봇이 파란색 물체를 잡고 있습니다, 파란색 물체는...] VLM 내적 &gt; y? 아니요 내적 &gt; y? VLM 예 새 버퍼 새 작업을 텍스트 하위 목표로 분해합니다. VLM을 사용하여 새로운 LLM 추출 하위 목표를 사용하여 이전 데이터를 추출하고 다시 레이블을 지정합니다. 모든 성공적인 궤적을 새 버퍼에 추가합니다. 그림 6: 프레임워크는 다른 작업에서 수집한 오프라인 데이터를 재활용하여 현재 진행 중인 새 작업에 대한 성공적인 궤적을 추출하고 정책 학습을 부트스트래핑할 수 있습니다. 작업이 희소해짐에 따라 LLM에서 제안한 하위 목표의 양이 증가하여 가능해진 이러한 느린 성장은 프레임워크가 더 어려운 작업에도 확장하여 다루기 쉽게 만들 수 있음을 시사하며, 탐색 중 언제든지 균일한 분포로 하위 목표를 만날 수 있다고 가정합니다. 또한 신중하게 고안된 내재적 보상이나 기타 탐색 보너스가 필요한 이전 접근 방식과 달리, 저희 접근 방식은 LLM 및 VLM의 이전 지식을 직접 활용하여 탐색을 위한 의미적으로 의미 있는 커리큘럼을 생성하여 희소한 보상 환경에서도 자기 동기 부여 방식으로 탐색하는 일반 에이전트를 위한 길을 열 수 있습니다. 5. 추출 및 전송 - 오프라인 데이터를 재사용하여 효율적인 순차적 작업 학습 에이전트는 환경과 상호 작용할 때 시간이 지남에 따라 일련의 작업을 학습하여 10% 성공률에 도달하는 데 필요한 1e1e1eSteps를 희소성의 함수로 재사용할 수 있어야 합니다.|| | leo le1 - Grasp 1e3 - Stack Sparseness Task 1e6-Triple 그림 5: 프레임워크를 사용하면 특정 성공률에 도달하는 데 필요한 단계 수가 작업의 희소성보다 느리게 증가합니다. 새 작업에 대한 부트스트랩 학습을 시작하기 위해 이전에 수집된 데이터를 사용합니다. 이는 경험에서 학습하는 RL 시스템을 확장하는 기본적인 기능입니다. 최근 연구에서는 작업에 독립적인 오프라인 데이터 세트를 새 작업에 적용하는 기술을 제안했지만, 힘든 인간 주석과 보상 모델 학습이 필요할 수 있습니다(5; 47; 9). 언어 기반 프레임워크를 활용하여 에이전트의 과거 경험에 기반한 부트스트래핑을 선보입니다. 우리는 세 가지 작업을 순서대로 훈련합니다.빨간색 물체를 파란색 물체 위에 쌓기, 파란색 물체를 녹색 물체 위에 쌓기, 녹색 물체를 빨간색 물체 위에 쌓기.이 작업을 [TR, B, TB, G, TG, R]이라고 합니다.직관은 간단합니다.예를 들어 TR, B를 풀기 위해 탐색하는 동안 에이전트는 TB, G 또는 TG, R과 같은 다른 관련 작업을 완전히 또는 부분적으로 해결했을 가능성이 높습니다.따라서 에이전트는 새 작업을 풀 때 이러한 예를 추출하여 처음부터 시작하지 않고 이전 작업에 대해 수집한 모든 탐색 데이터를 재사용할 수 있어야 합니다.4절에서 논의했듯이 에이전트는 상호 작용 데이터의 경험 버퍼를 수집합니다.이제 에이전트에 두 가지 다른 버퍼를 제공합니다.평생 버퍼 또는 오프라인 버퍼로, 에이전트가 각 상호 작용 데이터 에피소드를 저장하고 작업마다 계속 확장합니다.그런 다음 에이전트는 각 새 작업의 시작 부분에서 다시 초기화되고 채워집니다.이는 4절에서와 같이 채워집니다. 5.1, 보상으로 이어지는 궤적은 VLM이 LLM 텍스트 하위 목표(그림 3)를 사용하여 외부 또는 내부적으로 제공합니다. 정책 네트워크는 새로운 작업 버퍼를 사용하여 최적화됩니다. 그러나 이전과 다르게 첫 번째 작업 TR,B가 처음부터 학습되는 동안 에이전트는 작업 n 동안 수집된 데이터를 재사용하여 다음 작업 n+1의 학습을 부트스트랩합니다. LLM은 Tn+을 텍스트 하위 목표 [90, . . ., 9L−1]로 분해합니다. 그런 다음 에이전트는 평생/오프라인 버퍼에서 각 ICLR에서 저장된 에피소드 En = [(SO:T,n, 00:T,n, a0:T,n г,n )]를 추출합니다. 그런 다음 각 에피소드의 관찰 Ot,n을 취하고 VLM을 사용하여 모든 이미지 관찰과 모든 텍스트 하위 목표 간의 점곱 점수를 OT(91) · 1(ot)로 계산합니다. 점수가 임계값보다 크고 에이전트가 t까지의 모든 에피소드 타임스텝을 새 작업 버퍼에 추가하면 [(80:t,n, 00:t,n, a0:t,n)]이 새 작업 버퍼에 추가됩니다. 이 프로세스는 그림 6에 설명되어 있습니다. 이 절차는 학습 시작 시 새 작업마다 반복됩니다. 이 절차에 따라 에이전트는 새 작업 tabula rasa를 학습하지 않습니다. 작업 Tn의 시작 시 현재 경험 버퍼는 To:n에서 추출한 작업을 학습하는 데 유용한 에피소드로 채워집니다. n이 증가하면 To:n에서 추출한 데이터 양도 증가하여 학습 속도가 빨라집니다. 순차적 작업 학습을 위한 경험 재사용 결과. 에이전트는 이 방법을 적용하여 [TR,B, TB,G, TG,R]을 연속적으로 학습합니다. 새 작업의 시작 시 정책 가중치를 다시 초기화합니다. 목표는 프레임워크가 데이터를 추출하고 재사용하는 능력을 조사하는 것이므로 네트워크 일반화로 인해 발생할 수 있는 효과를 분리하여 제거합니다. 그림 7에서 에이전트가 각 새 작업에서 50% 성공률에 도달하기 위해 환경에서 몇 번의 상호 작용 단계를 거쳐야 하는지 표시합니다.실험을 통해 이전 작업을 위해 수집한 데이터를 재사용하여 새 작업의 학습 효율성을 개선하는 데 있어 기술이 효과적임을 명확히 보여줍니다.7 x 1e5 x 1e3 x 1e1x 1e데이터 재사용을 통해 연속적으로 작업을 학습하면서 50% 성공률에 도달하는 데 필요한 단계 작업 1 - 파란색 위의 빨간색 작업 2 - 녹색 위의 파란색 작업 3 - 빨간색 위의 녹색 그림 7: 실험에서 에이전트는 과거 경험 데이터를 재사용하여 작업 n보다 작업 n + 1을 더 빨리 학습할 수 있습니다.이러한 결과는 프레임워크를 사용하여 로봇 에이전트의 평생 학습 역량을 끌어낼 수 있음을 시사합니다.연속적으로 학습하는 작업이 많을수록 다음 작업을 더 빨리 학습합니다.이는 특히 현실 세계에서 개방형 환경에 에이전트를 배치할 때 특히 유용할 수 있습니다.에이전트가 접한 수명 동안의 데이터를 활용함으로써 순전히 처음부터 학습하는 것보다 훨씬 빠르게 새로운 작업을 학습할 수 있어야 합니다. 5. 학습한 기술 스케줄링 및 재사용 프레임워크를 통해 에이전트가 희소 보상 작업을 효율적으로 탐색하고 해결하는 방법을 학습하고, 평생 학습을 위해 데이터를 재사용하고 전송할 수 있는 방법을 설명했습니다. 언어 조건 정책(4절)을 사용하면 에이전트는 언어 목표 go:M(예: &quot;녹색 물체가 빨간색 물체 위에 있습니다&quot; 또는 &quot;로봇이 파란색 물체를 잡고 있습니다&quot;)으로 설명되는 일련의 M 기술을 학습할 수 있습니다. 프레임워크를 통해 에이전트는 학습한 M 기술을 스케줄링하고 재사용하여 에이전트가 훈련 중에 마주친 것 이상의 새로운 작업을 해결할 수 있습니다. 패러다임은 이전 섹션에서 마주친 것과 동일한 단계를 따릅니다. 빨간색 물체 위에 녹색 물체를 쌓으세요와 같은 명령 작업: 녹색 물체 위에 파란색 물체를 쌓으세요. 하위 목표 계획: 로봇이 파란색 물체를 잡고 있습니다. 관찰 결과가 현재 하위 목표와 일치하면 다음 기술/하위 목표를 실행합니다. LLM 파란색 물체가 위에 있습니다... VLM 하위 목표 계획: 로봇이 파란색 물체를 잡고 있습니다. 파란색 물체는 녹색 물체 위에 있습니다.실행할 기술 상태 동작 LangCond 정책 상태 실행할 기술 LangCond 정책 시간 그림 8: 프레임워크는 LLM을 사용하여 작업을 기술 목록으로 분해하고 VLM이 하위 목표에 도달했다고 예측할 때까지 각 기술을 실행할 수 있습니다.ICLRExecute에서 환생 강화 학습 워크숍 학습된 기술/하위 목표:[로봇이 빨간색 물체를 움켜쥐고 있습니다, VLM I(vf) T (9m) 녹색 물체가 빨간색 물체 위에 있습니다]&quot;로봇이 빨간색 물체를 움켜쥐고 있습니다&quot; &quot;로봇이 빨간색 물체를 움켜쥐고 있습니다&quot; &quot;빨간색 물체가 파란색 물체 위에 있습니다&quot; 마주친 하위 목표 목록:[로봇이 빨간색 물체를 움켜쥐고 있습니다, 빨간색 물체가 파란색 물체 위에 있습니다] 그림 9: 프레임워크를 사용하여 관찰을 통해 학습하는 에이전트의 예. 또는 빨간색을 파란색 위에 쌓은 다음 녹색을 빨간색 위에 놓고 LLM에 공급하면 LLM은 이를 더 짧은 수평선 목표 목록인 go:N으로 분해하라는 메시지를 받습니다. 그런 다음 에이전트는 정책 네트워크를 사용하여 이를 fo(St, In) → at로 동작으로 전환할 수 있습니다. n번째 스킬을 실행할 때 VLM은 각 타임스텝에서 ØT(9n) · 01(0t) &gt; ✅인지 계산하여 현재 관찰에서 스킬 목표가 달성되었는지 확인합니다. 이 경우 에이전트는 작업이 해결되지 않는 한 n + 1번째 스킬을 실행하기 시작합니다. 5. 관찰로부터 학습: 비디오를 스킬에 매핑 외부 에이전트를 관찰하여 학습하는 것은 일반 에이전트에게 바람직한 능력이지만, 이를 위해서는 종종 특별히 설계된 알고리즘과 모델이 필요합니다(42; 11; 52). 에이전트는 작업을 수행하는 전문가의 비디오에 따라 조건을 받을 수 있으므로 관찰에서 원샷 학습이 가능합니다. 테스트에서 에이전트는 사람이 손으로 물건을 쌓는 영상을 찍습니다. 이 영상은 F개의 프레임(vo:F)으로 나뉩니다. 그런 다음 에이전트는 VLM을 학습된 기술의 M개 텍스트 설명(하위 목표 go:M으로 표현)과 짝을 이루어 전문가 궤적에서 발생한 하위 목표를 다음과 같이 감지합니다. (1) 에이전트는 각 학습된 기술/하위 목표를 OT(9m)까지 임베드하고 각 비디오 프레임을 1(vf)까지 임베드하고 각 쌍 간의 점곱을 계산합니다. (2) Y보다 큰 유사도를 얻는 모든 하위 목표를 나열하여 궤적 동안 전문가가 발생한 하위 목표의 연대순 목록을 수집합니다. (3) 그림 8에서 설명한 대로 하위 목표 목록을 실행합니다. MuJoCo 시뮬레이션(4절)의 이미지에서만 미세 조정되었음에도 불구하고 VLM은 로봇과 사람의 팔을 모두 묘사한 실제 이미지에서 올바른 텍스트-이미지 대응 관계를 정확하게 예측할 수 있었습니다. 또한 캡션(그림 9)에서 여전히 &quot;로봇&quot;이라고 언급하는 반면 VLM은 그럼에도 불구하고 인간의 손으로 일반화한다는 점에 유의하십시오. 6
--- CONCLUSION ---
우리는 에이전트의 핵심에 언어를 두는 프레임워크를 제안합니다. 일련의 실험을 통해 이 프레임워크가 Foundation Models의 지식과 역량을 활용하여 현재 문헌과 관련하여 보다 통합된 접근 방식을 제공하여 일반적으로 별도의 알고리즘과 모델이 필요한 일련의 핵심 RL 과제를 해결하는 방법을 보여줍니다. 1) 희소 보상 작업에서 탐색 2) 경험 데이터를 재사용하여 새로운 기술에 대한 부트스트랩 학습 3) 학습한 기술을 스케줄링하여 새로운 과제를 해결 4) 전문가 에이전트를 관찰하여 학습. 이러한 초기 결과는 Foundation 모델을 활용하면 다양한 문제를 개선된 효율성과 일반성으로 해결할 수 있는 일반 RL 알고리즘을 얻을 수 있음을 시사합니다. 이러한 모델에 포함된 사전 지식을 활용함으로써 실제 세계에서 직접 어려운 과제를 해결할 수 있는 더 나은 로봇 에이전트를 설계할 수 있습니다. 부록에서 현재의 한계와 향후 작업 목록을 제공합니다.ICL에서의 강화 학습 워크숍 환생 참고문헌 [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 내가 할 수 있는 대로 하라, 내가 말하는 대로 하지 마라: 로봇 활용도에서의 언어 기반.arXiv 사전 인쇄본 arXiv:2204.01691, 2022. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv 사전 인쇄본 arXiv:2204.14198, 2022. [3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill 등. 기초 모델의 기회와 위험에 관하여. arXiv 사전 인쇄본 arXiv:2108.07258, 2021. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등. 언어 모델은 소수 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. [5] Serkan Cabi, Sergio Gómez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, et al. 보상 스케치 및 일괄 강화 학습을 통한 데이터 기반 로봇 확장. arXiv 사전 인쇄본 arXiv:1909.12200, 2019. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 자기 감독 비전 변환기의 새로운 속성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 9650-9660쪽, 2021. [7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch. 결정 변환기: 시퀀스 모델링을 통한 강화 학습. 신경 정보 처리 시스템의 발전, 34:15084-15097, 2021. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 코드에서 학습된 대규모 언어 모델 평가. arXiv 사전 인쇄본 arXiv:2107.03374, 2021. [9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei. 인간 선호도에서 심층 강화 학습. 신경 정보 처리 시스템의 발전, 30, 2017. [10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 명령어 미세 조정 언어 모델 확장. arXiv 사전 인쇄본 arXiv:2210.11416, 2022. [11] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, Franziska Meier. 시각적 데모에서 모델 기반 역 강화 학습. Jens Kober, Fabio Ramos 및 Claire Tomlin 편집, 2020 로봇 학습 컨퍼런스 회의록, 기계 학습 연구 회의록 155권, 1930-1942쪽. PMLR, 2021년 11월 16일. URL https://proceedings.mlr.press/v155/das21a.html. [12] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill 및 Rob Fergus. 체화된 추론을 위한 언어 모델과 협업. arXiv 사전 인쇄본 arXiv:2302.00763, 2023. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020. [14] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, Jeff Clune. Go-explore: 하드 탐색 문제에 대한 새로운 접근 방식. arXiv 사전 인쇄본 arXiv:1901.10995, 2019. ICLR에서의 강화 학습 환생 워크숍[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar. Minedojo: 인터넷 규모의 지식을 갖춘 개방형 체현 에이전트 구축. arXiv 사전 인쇄본 arXiv:2206.08853, 2022. [16] Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel. 강화 학습을 위한 역방향 커리큘럼 생성. 로봇 학습 컨퍼런스, 482-495페이지. PMLR, 2017. [17] Oliver Groth, Markus Wulfmeier, Giulia Vezzani, Vibhavari Dasagi, Tim Hertweck, Roland Hafner, Nicolas Heess, Martin Riedmiller. 호기심이 필요한 전부일까요? 호기심 많은 탐험에서 나타나는 새로운 행동의 유용성에 대해. arXiv 사전 인쇄본 arXiv:2109.08603, 2021. [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16000-16009페이지, 2022. [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 컴퓨팅 최적 대규모 언어 모델 교육. arXiv 사전 인쇄본 arXiv:2203.15556, 2022. [20] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트를 위한 실행 가능한 지식 추출. International Conference on Machine Learning, 9118-9147페이지. PMLR, 2022. [21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 내면의 독백: 언어 모델을 통한 계획을 통한 구체화된 추론. arXiv 사전 인쇄본 arXiv:2207.05608, 2022. [22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn. Bc-z: 로봇 모방 학습을 통한 제로샷 작업 일반화. 로봇 학습 컨퍼런스, 991-1002쪽. PMLR, 2022. [23] Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell. 인공지능 실험을 위한 말뫼 플랫폼. 국제 인공지능 공동 컨퍼런스, 2016. [24] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, et al. Minerl diamond 2021 대회: 개요, 결과 및 얻은 교훈. NeurIPS 2021 대회 및 시범 트랙, 13-28페이지, 2022. [25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby. 빅 전송(비트): 일반적인 시각적 표현 학습. Computer VisionECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 5부 16, 491-507페이지. Springer, 2020. [26] Alex X Lee, Coline Manon Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis, Jost Tobias Springenberg, Arunkumar Byravan, Abbas Abdolmaleki, Nimrod Gileadi, David Khosid, et al. 픽앤플레이스를 넘어서: 다양한 모양의 로봇 스태킹 해결. 제5회 로봇 학습 연례 컨퍼런스, 2021. [27] Andrew Levy, George Konidaris, Robert Platt, Kate Saenko. 후견지명으로 다단계 계층 구조 학습. arXiv 사전 인쇄본 arXiv:1712.00948, 2017. [28] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. [29] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. alphacode를 사용한 경쟁 수준 코드 생성. Science, 378(6624):1092-1097, 2022. [30] Corey Lynch 및 Pierre Sermanet. 비정형 데이터에 대한 언어 조건 모방 학습. arXiv 사전 인쇄본 arXiv:2005.07648, 2020. ICLR에서의 강화 학습 워크숍 환생[31] Tambet Matiisen, Avital Oliver, Taco Cohen 및 John Schulman. 교사-학생 커리큘럼 학습. IEEE 신경망 및 학습 시스템 거래, 31(9):3732-3740, 2019. [32] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, Sergey Levine. 데이터 효율적인 계층적 강화 학습. 신경 정보 처리 시스템의 발전, 31, 2018. [33] Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee. 자기 모방 학습. 기계 학습 국제 컨퍼런스, 3878-3887페이지. PMLR, 2018. [34] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. 맥락 내 학습 및 유도 헤드. arXiv 사전 인쇄본 arXiv:2209.11895, 2022. [35] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell. 자기 감독 예측을 통한 호기심 주도 탐색. 기계 학습에 관한 국제 컨퍼런스에서, 2778-2787페이지. PMLR, 2017. [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763페이지. PMLR, 2021. [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재 데이터를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [38] Martin Riedmiller, Jost Tobias Springenberg, Roland Hafner, Nicolas Heess. 수집 및 추론 - 데이터 효율적 강화 학습에 대한 새로운 시각. 제5회 로봇 학습 연례 컨퍼런스, Blue Sky 제출 트랙, 2021. URL https://openreview.net/forum?id=qscEfLT5VJK. [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: 차세대 이미지 텍스트 모델을 훈련하기 위한 대규모 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2210.08402, 2022. [40] Mohit Shridhar, Lucas Manuelli, Dieter Fox. Cliport: 로봇 조작을 위한 무엇과 어디 경로. 로봇 학습 컨퍼런스, 894-906페이지. PMLR, 2022. [41] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. 일반 강화 학습 알고리즘을 사용하여 자기 플레이로 체스와 쇼기를 마스터하기. arXiv 사전 인쇄본 arXiv:1712.01815, 2017. [42] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, Sergey Levine. Avid: 인간 비디오의 픽셀 수준 변환을 통한 다단계 작업 학습. arXiv 사전 인쇄 arXiv:1912.04443, 2019. [43] Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam 및 Rob Fergus. 비대칭 자기 놀이를 통한 내적 동기 부여 및 자동 커리큘럼. arXiv 사전 인쇄 arXiv:1703.05407, 2017. [44] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus 및 Ishita Dasgupta. 인터넷 규모의 비전 언어 모델을 구체화된 에이전트로 추출합니다. arXiv 사전 인쇄 arXiv:2301.12507, 2023. [45] Emanuel Todorov, Tom Erez 및 Yuval Tassa. Mujoco: 모델 기반 제어를 위한 물리 엔진입니다. 2012 IEEE/RSJ 지능형 로봇 및 시스템 국제 컨퍼런스, 5026-5033페이지, 2012. doi: 10.1109/IROS.2012.6386109. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. ICLR에서의 강화 학습 워크숍 환생[47] Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes Fürnkranz. 선호도 기반 강화 학습 방법 조사. J. Mach. 학습. Res., 18:136:1–136:46, 2017. [48] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, Jonathan Tompson. 시각-언어 모델을 통한 지시 증강을 통한 로봇 기술 습득. arXiv 사전 인쇄본 arXiv:2211.11736, 2022. [49] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: 로봇 조작을 위한 시각 세계 재배열. 로봇 학습 컨퍼런스, 726-747페이지. PMLR, 2021. [50] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 소크라테스 모델: 언어로 제로샷 멀티모달 추론 구성. arXiv 사전 인쇄본 arXiv:2204.00598, 2022. [51] Yunzhi Zhang, Pieter Abbeel, Lerrel Pinto. 가치 불일치를 통한 자동 커리큘럼 학습. 신경 정보 처리 시스템의 발전, 33:7648-7659, 2020. [52] Yuxiang Zhou, Yusuf Aytar, Konstantinos Bousmalis. 시각적 모방을 위한 조작자 독립 표현. arXiv 사전 인쇄본 arXiv:2103.09016, 2021.ICLR7에서의 강화 학습 워크숍 환생 부록 7.1 도메인 내 데이터의 CLIP 미세 조정 실험에서 가능한 캡션의 임베딩과 환경 y = 1(0+) 또는 (li)의 RGB 관찰 간의 점곱은 종종 정보가 없었습니다.올바른 쌍과 틀린 쌍은 매우 유사한 점수를 얻었고 범위에서 너무 적게 변했습니다.저희의 목표는 이미지가 주어진 경우 올바른 설명과 틀린 설명을 인식할 수 있는 임계값을 설정하는 것입니다.따라서 점수에서 더 큰 차이가 필요합니다.이를 해결하기 위해 시뮬레이션의 MuJoCo 상태를 기반으로 하는 자동화된 주석기를 사용하여 객체의 다양한 구성과 해당 언어 설명이 있는 이미지 관찰 데이터 세트를 수집하여 도메인 내 데이터로 CLIP을 미세 조정합니다.오른쪽의 플롯은 저희의 연구 결과에 대한 분석을 제공합니다.정밀도와 재현율은 데이터 세트 크기에 따라 대수적으로 증가하는 경향이 있습니다. 핵심 요점은 CLIP이 약 108개의 이미지에서 학습되었지만, 103개의 도메인 내 쌍만으로도 작업 성능을 개선하기에 충분하다는 것입니다. 0.0.0.0.VLM 캡션 정밀도 및 미세 조정 데이터 세트 크기에 따른 재현율 le정밀도 재현율 le이미지-캡션 쌍 데이터 세트 크기 le그림 10: 캡션 정밀도 및 데이터 세트 크기에 따른 재현율. 미세 조정된 CLIP의 대수적 추세 예측은 약 103개의 이미지-캡션 쌍이 충분한 성능을 발휘함을 시사합니다. y = 0.8로 얻은 값. 우리의 경우 높은 재현율보다 높은 정밀도가 더 바람직합니다. 전자는 긍정적인 보상이 노이즈가 없음을 나타내는 반면, 반대의 경우 학습 과정을 방해할 수 있습니다. 재현율이 낮으면 모델이 모든 성공적인 궤적을 올바르게 식별하지 못할 수 있음을 나타내지만, 이는 단순히 학습을 위해 더 많은 에피소드가 필요하다는 의미일 뿐 학습 과정을 방해하지는 않습니다. 미세 조정 후 y = 0.8의 값이 가장 성능이 좋은 선택임을 발견했습니다. 7.2 현재의 한계와 향후 작업 1) 현재 구현에서 우리는 정책에 대해 단순화된 입력 및 출력 공간, 즉 MDP의 상태 공간(즉, MuJoCo 시뮬레이터에서 제공하는 객체와 엔드 이펙터의 위치)과 섹션 3에 설명된 대로 픽 앤 플레이스 액션 공간을 사용합니다.여기서 정책은 로봇이 픽 앤 플레이스할 ax, y 위치를 출력할 수 있습니다.이 선택은 더 빠른 실험 반복을 위해 채택되었으며 따라서 논문의 주요 기여인 LLM과 VLM과의 상호 작용에 대한 검색에 집중할 수 있었습니다.그럼에도 불구하고 최근 문헌에서는 이 액션 공간 공식을 통해 광범위한 로봇 작업을 실행할 수 있음이 입증되었습니다(49; 40).작업: 세 객체를 모두 쌓습니다.로봇이 녹색 객체를 잡고 있습니다.녹색 객체가 파란색 객체 위에 있습니다.로봇이 빨간색 객체를 잡고 있습니다. 자체 생성 보상 - 환경 보상 환경 보상 그림 11: 하위 목표를 자율적으로 식별하고 해당 보상을 하는 것은 Triple Stack과 같이 작업이 엄청나게 희소해질 때 특히 중요해집니다.ICLRI에서의 환생 강화 학습 워크숍 환경과 상호 작용하는 로봇 팔이라고 상상해 보세요.앞에 빨간색 물체, 파란색 물체, 초록색 물체가 있습니다.작업을 받으면 작업을 해결하게 하는 일련의 하위 목표를 설명하세요.작업: 빨간색 물체를 초록색 물체 위에 쌓습니다.하위 목표: 1) 로봇이 빨간색 물체를 움켜쥐고 있습니다.2) 빨간색 물체가 초록색 물체 위에 있습니다.작업: 빨간색 물체를 움켜쥐세요.하위 목표: 1) 로봇이 빨간색 물체를 움켜쥐고 있습니다.작업: 파란색 물체를 빨간색 물체 위에 쌓습니다.하위 목표: 1) 로봇이 파란색 물체를 움켜쥐고 있습니다.2) 파란색 물체가 빨간색 물체 위에 있습니다.작업: 세 물체를 모두 쌓습니다. 하위 목표: 1) 로봇이 빨간색 물체를 움켜쥐고 있습니다. 2) 빨간색 물체가 녹색 물체 위에 있습니다. 3) 로봇이 파란색 물체를 움켜쥐고 있습니다. 4) 파란색 물체가 빨간색 물체 위에 있습니다. 그림 12: LLM을 조건화하는 데 사용한 프롬프트와 그 출력의 예. 일반 텍스트: 사용자가 삽입한 텍스트, 굵은 텍스트: LLM 출력. 현재 문헌의 많은 작업(26; 41; 5; 15)은 정책이 입력으로 이미지 관찰과 출력으로 엔드 이펙터 속도에 맞게 확장되기 위해 모델에 더 많은 데이터와 상호 작용 시간만 필요하다는 것을 보여줍니다. 우리의 목표는 우리 방법이 가져온 상대적인 성능 개선을 보여주는 것이었기 때문에 MDP 설계를 선택해도 결과의 일반성이 감소하지 않습니다. 우리의 결과는 더 많은 데이터가 필요하더라도 이미지를 입력으로 사용하는 모델에도 적용될 가능성이 큽니다. 2) 작업에 사용한 것과 동일한 객체를 사용하여 도메인 내 데이터에 대한 CLIP을 미세 조정합니다. 향후 작업에서는 실제로 작업에 사용하는 객체를 제외하고 더 많은 객체에서 CLIP의 대규모 미세 조정을 수행할 계획이며, 따라서 VLM 기능을 조사하여 클래스 간 객체로 일반화할 수도 있습니다.현재로서는 상당한 양의 추가 계산과 시간이 필요하므로 이 작업의 범위를 벗어났습니다.3) 시뮬레이션에서만 환경을 학습하고 테스트합니다.실제 환경에서도 프레임워크를 테스트할 계획입니다.결과에 따르면 1) 시뮬레이션에서 얻은 데이터로 CLIP을 미세 조정할 수 있으며 실제 이미지로 일반화되므로(5.4절) 값비싼 인간 주석을 피할 수 있고 2) 프레임워크를 사용하면 희소한 작업도 처음부터 효율적으로 학습할 수 있으므로(5.1절) 로봇 경험을 수집하는 데 훨씬 더 많은 시간이 소요되는 실제 세계에 우리 방법을 적용할 수 있음을 시사합니다.7. LLM의 프롬프트와 출력 그림 12에서 LLM에서 기대하는 동작의 맥락 내 학습을 허용하는 데 사용한 프롬프트를 보여줍니다(34). 두 가지 예와 설정 및 해당 작업에 대한 일반적인 설명만 있으면 LLM은 객체의 새로운 조합과 &quot;세 개의 객체를 모두 쌓기&quot;와 같이 잘 정의되지 않은 새로운 작업까지도 일반화하여 일관된 하위 목표를 출력할 수 있습니다.
