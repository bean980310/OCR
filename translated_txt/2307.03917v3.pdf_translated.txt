--- ABSTRACT ---
대규모 언어 모델(LLM)은 자연어 처리 분야에서 놀라운 성공을 거두었으며, 자연어를 사용하여 더 나은 인간-컴퓨터 상호 작용을 가능하게 했습니다. 그러나 LLM에 음성 신호를 원활하게 통합하는 방법은 잘 연구되지 않았습니다. &quot;디코더 전용&quot; 아키텍처도 음성 처리 작업에 대해 잘 연구되지 않았습니다. 이 연구에서는 음향 정보를 텍스트 기반 대규모 언어 모델에 효과적으로 통합하는 새로운 방식인 Speech-LLAMA를 소개합니다. 저희의 방법은 연결주의 시간 분류와 간단한 오디오 인코더를 활용하여 압축 음향 특징을 LLM의 연속 의미 공간에 매핑합니다. 또한 음성-텍스트 쌍 데이터만으로 더 작은 규모의 무작위로 초기화된 speechLLAMA 모델을 학습하여 음성-텍스트 작업을 위한 디코더 전용 아키텍처를 추가로 조사합니다. 저희는 다국어 음성-텍스트 번역 작업에 대한 실험을 수행하고 강력한 기준선에 비해 상당한 개선을 보여주며 음성-텍스트 변환을 위한 디코더 전용 모델의 잠재적 이점을 강조합니다. 색인 용어 번역 디코더 전용, LLAMA, LORA, 음성 1.
--- INTRODUCTION ---
최근에 대규모 언어 모델(LLM)은 질문 답변, 기계 번역, 언어 이해 등을 포함한 다양한 자연어 벤치마크에서 놀라운 성과를 보였습니다[1, 2, 3, 4, 5]. Transformer 기반 아키텍처[6]를 채택하고 시퀀스 내에서 다가올 토큰을 예상하도록 훈련함으로써 이 언어 모델은 문맥적 학습 능력에서 탁월합니다. 이를 통해 모델링 능력이 크게 향상될 뿐만 아니라, 더 중요한 것은 최첨단 연구와 실제 응용 프로그램을 효과적으로 연결하는 원활한 사용자 상호 작용이 가능하다는 것입니다. 음성은 가장 타고난 본능적인 인간 의사소통 방식이므로 음성과 LLM을 통합하면 인간-기계 상호 작용의 사용자 경험이 더욱 향상될 것입니다. 이러한 직관에 따라 음성 신호와 대규모 언어 모델을 결합하는 시도가 여러 번 수행되었습니다[7, 8, 9, 10]. 그 중에서 계단식 접근 방식이 가장 간단한 솔루션입니다. 이러한 시스템에서 음성 신호는 먼저 기존 자동 음성 인식(ASR) [11] 모델을 통해 단어 토큰으로 변환되고 LLM은 다운스트림 작업을 위해 인식된 단어를 처리합니다. 나중에 LLM에 이미지 정보를 통합하는 것에서 영감을 받아 [12, 13, 14, 15], 연구자들은 음성 신호의 심층적 결합도 탐구했습니다 [9, 10, 16, 17, 18, 19]. [16]에서 저자는 통합 디코더 전용 네트워크를 통해 음성 및 텍스트 작업을 공동으로 모델링할 것을 제안했습니다. 마찬가지로 [19]에서 저자는 기성품 LLM과 함께 오디오 토큰 변환 모듈을 최적화할 것을 제안했습니다. 단어 조각 대신 자체 감독 모델의 음성 표현의 개별 토큰이 [17]에서 사용됩니다. 유망한 결과가 있었지만 음성과 LLM의 통합과 관련된 몇 가지 중요한 과제는 여전히 추가 탐구가 필요합니다. 처음에 사전 훈련된 LLM을 사용하여 두 가지 모달리티(음성과 텍스트)를 정렬하는 것은 텍스트 시퀀스에 비해 일반적으로 음성 신호의 시퀀스 길이가 더 길기 때문에 어려움이 있습니다. 게다가 LLM을 훈련하는 데 비용이 많이 들기 때문에 뛰어난 성능을 유지하면서 전체 통합 비용을 최소화하는 방법을 찾는 것은 여전히 어려운 과제입니다. 더 중요한 것은 LLM의 놀라운 성공을 고려할 때 음성-텍스트 처리를 위한 백본 네트워크 아키텍처로 디코더 전용 모델[3, 16, 20, 21, 22]을 사용하는 미개척 잠재력을 탐구하는 것이 중요하다는 것입니다. 이 연구에서 우리는 음성 및 언어 모델의 효율적인 종단 간 통합을 탐구하여 앞서 언급한 과제를 해결하는 것을 목표로 합니다. 우리의 접근 방식에는 텍스트에서 작동하는 대규모 언어 모델에 음향 임베딩도 통합하는 간단하면서도 효과적인 아키텍처를 설계하는 것이 포함됩니다. 이 통합을 통해 LM은 음향 정보의 전사 또는 번역을 조건화할 수 있습니다. 보다 구체적으로, 제안하는 방법은 기존 LLM을 활용하고 음향 특징 압축기와 음향 인코더를 통합하여 소수의 자유 매개변수만 도입합니다. 음성을 이산화된 토큰으로 변환하는 이전 접근 방식과 달리, 저희 모델은 음성의 연속 표현을 LM이 정의한 의미 공간에 직접 매핑합니다. 처리 단계에서 음성 특징은 음향 압축기에 의해 처음에 압축되어 시퀀스 길이를 줄입니다. 그런 다음 음향 인코더는 압축된 음성 신호를 LLM이 사용할 수 있는 텍스트의 동일한 의미 공간에서 연속 벡터로 변환합니다. 최종 출력은 LLM의 디코딩 프로세스를 통해 생성됩니다. 저희는 적절한 음향 압축기, 어텐션 마스크 및 미세 조정 방법을 선택하는 것과 같은 제안된 모델의 다양한 실용적인 측면을 철저히 조사합니다. 또한, 저희는 제안된 모델을 13개 언어의 음성을 영어(EN) 텍스트로 번역하는 작업에 적용하고 CoVOST 데이터 세트의 강력한 기준과 성능을 비교합니다. 마지막으로, 우리는 음성-텍스트 쌍 데이터만을 사용하여 처음부터 학습한 디코더 전용 모델이 음성 처리에서 일반적으로 사용되는 인코더-디코더 아키텍처보다 상당한 잠재력과 여러 장점을 보인다는 것을 보여줍니다. 이 연구에서 우리의 기여는 다음과 같이 요약될 수 있습니다. • 우리는 기존의 텍스트 기반 대규모 언어 모델을 음성 처리와 효과적으로 통합하는 Speech-LLAMA라는 효율적인 엔드투엔드 통합 방법을 소개합니다. 우리는 다양한 음성 번역(ST) 작업에서 강력한 기준선과 비교하여 번역 성능이 상당히 향상되었습니다. • 우리는 성능 향상에 중요한 제안된 음성-LLM 통합의 다양한 실용적인 측면을 조사합니다. 이러한 측면에는 음향 특징의 음향 압축, 어텐션 마스크 선택 및 미세 조정 전략이 포함됩니다. • 대규모의 다양한 실제 데이터에서 우리는 디코더 전용 아키텍처가 음성-텍스트 작업에 대해 인코더-디코더 아키텍처만큼 경쟁력이 있음을 보여줍니다. 우리는 디코더 전용이 또한 매개변수 효율성이 더 높음을 보여줍니다. 2.
--- RELATED WORK ---
우리 모델은 음성 신호를 대규모 언어 모델에 통합하는 것을 목표로 하며, 연결주의 시간 분류(CTC) 특징 길이 압축 및 로랭크 적응(LORA)과 관련이 있습니다. 다음에서 이러한 주제에 대해 설명합니다. 2.1. 대규모 언어 모델 LLM은 일반적으로 다양한 도메인과 언어에 걸친 방대한 양의 텍스트 데이터에 대해 사전 학습됩니다. 일반적으로 각 출력 토큰을 입력으로 사용하여 다음 단계 토큰을 예측하는 자동 회귀 디코더 전용 아키텍처를 따르는 변환기 계층 스택으로 구성됩니다. 이 작업에서 우리는 제안된 LLM을 구축하기 위한 백본 LLM으로 LLaMA-7B [5]를 선택합니다.
--- METHOD ---
연결주의 시간 분류와 간단한 오디오 인코더를 활용하여 압축 음향 특징을 LLM의 연속 의미 공간에 매핑합니다. 또한, 음성-텍스트 작업의 디코더 전용 아키텍처를 추가로 조사하여 음성-텍스트 쌍 데이터만으로 더 작은 규모의 무작위로 초기화된 speechLLAMA 모델을 학습합니다.
--- EXPERIMENT ---
영어: s는 다국어 음성-텍스트 번역 작업에 대한 강력한 기준선에 비해 상당한 개선을 보여주며, 음성-텍스트 변환을 위한 디코더 전용 모델의 잠재적 이점을 강조합니다. 색인 용어 번역 디코더 전용, LLAMA, LORA, 음성 1. 서론 최근 대규모 언어 모델(LLM)은 질문 답변, 기계 번역, 언어 이해 등을 포함한 다양한 자연어 벤치마크에서 놀라운 성과를 보여주었습니다[1, 2, 3, 4, 5]. Transformer 기반 아키텍처[6]를 채택하고 시퀀스 내에서 다가올 토큰을 예상하도록 훈련함으로써 이 언어 모델은 문맥별 학습 능력에서 탁월합니다. 이를 통해 모델링 능력이 크게 향상될 뿐만 아니라, 더 중요한 것은 최첨단 연구와 실제 응용 프로그램을 효과적으로 연결하는 원활한 사용자 상호 작용이 가능하다는 것입니다. 음성은 가장 타고난 본능적인 인간 의사소통 방식이므로 음성과 LLM을 통합하면 인간-기계 상호 작용의 사용자 경험이 더욱 향상될 것입니다. 이러한 직관을 바탕으로 음성 신호와 대규모 언어 모델을 결합하려는 여러 시도가 수행되었습니다[7, 8, 9, 10]. 그 중에서 계단식 접근 방식이 가장 간단한 솔루션입니다. 이러한 시스템에서 음성 신호는 먼저 기존 자동 음성 인식(ASR)[11] 모델을 통해 단어 토큰으로 변환되고 LLM은 다운스트림 작업을 위해 인식된 단어를 처리합니다. 나중에 LLM에 이미지 정보를 통합한 것[12, 13, 14, 15]에서 영감을 받아 연구자들은 음성 신호의 심층적 결합도 탐구했습니다[9, 10, 16, 17, 18, 19]. [16]에서 저자들은 통합 디코더 전용 네트워크를 통해 음성 및 텍스트 작업을 공동으로 모델링하는 것을 제안했습니다. 마찬가지로 [19]에서 저자들은 기성품 LLM과 함께 오디오 토큰 변환 모듈을 최적화하는 것을 제안했습니다. 단어 조각 대신 [17]에서 자체 감독 모델의 음성 표현의 개별 토큰이 사용됩니다. 유망한 결과가 있었지만 음성과 LLM의 통합과 관련된 몇 가지 중요한 과제는 여전히 추가 탐색이 필요합니다. 처음에 사전 훈련된 LLM을 사용하여 두 가지 모달리티(음성과 텍스트)를 정렬하는 것은 텍스트 시퀀스에 비해 일반적으로 음성 신호의 시퀀스 길이가 더 길기 때문에 과제가 발생합니다. 게다가 LLM을 훈련하는 데 비용이 많이 들기 때문에 뛰어난 성능을 유지하면서 전체 통합 비용을 최소화하는 방법을 찾는 것은 계속해서 어려운 과제입니다. 더 중요한 것은 LLM의 놀라운 성공을 고려할 때 음성-텍스트 처리를 위한 백본 네트워크 아키텍처로 디코더 전용 모델[3, 16, 20, 21, 22]을 사용하는 미개척 잠재력을 탐색하는 것이 중요하다는 것입니다. 이 연구에서 우리는 음성 및 언어 모델의 효율적인 종단 간 통합을 탐색하여 앞서 언급한 과제를 해결하는 것을 목표로 합니다. 우리의 접근 방식은 텍스트에서 작동하는 대규모 언어 모델이 음향 임베딩을 통합하는 간단하면서도 효과적인 아키텍처를 설계하는 것을 포함합니다. 이 통합을 통해 LM은 음향 정보의 전사 또는 번역을 조건화할 수 있습니다. 보다 구체적으로, 제안하는 방법은 기존 LLM을 활용하고 음향 특징 압축기와 음향 인코더를 통합하여 소수의 자유 매개변수만 도입합니다. 음성을 이산화된 토큰으로 변환하는 이전 접근 방식과 달리, 저희 모델은 음성의 연속 표현을 LM이 정의한 의미 공간에 직접 매핑합니다. 처리 단계에서 음성 특징은 음향 압축기에 의해 처음에 압축되어 시퀀스 길이를 줄입니다. 그런 다음 음향 인코더는 압축된 음성 신호를 LLM이 사용할 수 있는 텍스트의 동일한 의미 공간에서 연속 벡터로 변환합니다. 최종 출력은 LLM의 디코딩 프로세스를 통해 생성됩니다. 저희는 적절한 음향 압축기, 어텐션 마스크 및 미세 조정 방법을 선택하는 것과 같은 제안된 모델의 다양한 실용적인 측면을 철저히 조사합니다. 또한, 저희는 제안된 모델을 13개 언어의 음성을 영어(EN) 텍스트로 번역하는 작업에 적용하고 CoVOST 데이터 세트의 강력한 기준과 성능을 비교합니다. 마지막으로, 우리는 음성-텍스트 쌍 데이터만을 사용하여 처음부터 학습한 디코더 전용 모델이 음성 처리에서 일반적으로 사용되는 인코더-디코더 아키텍처에 비해 상당한 잠재력과 여러 장점을 보인다는 것을 보여줍니다. 이 연구에서 우리의 기여는 다음과 같이 요약될 수 있습니다. • 우리는 기존의 텍스트 기반 대규모 언어 모델을 음성 처리와 효과적으로 통합하는 Speech-LLAMA라는 효율적인 엔드투엔드 통합 방법을 소개합니다. 우리는 다양한 음성 번역(ST) 작업에서 강력한 기준선과 비교하여 번역 성능이 상당히 향상되었습니다. • 우리는 성능 향상에 중요한 제안된 음성-LLM 통합의 다양한 실용적인 측면을 조사합니다. 이러한 측면에는 음향 특징의 음향 압축, 어텐션 마스크 선택 및 미세 조정 전략이 포함됩니다. • 대규모의 다양하고 실제적인 데이터에서 우리는 디코더 전용 아키텍처가 음성-텍스트 작업에 대해 인코더-디코더 아키텍처만큼 경쟁력이 있음을 보여줍니다. 우리는 디코더 전용이 또한 매개변수 효율성이 더 높음을 보여줍니다. 2. 관련 연구 우리 모델은 음성 신호를 대규모 언어 모델에 통합하는 것을 목표로 하며, 연결주의 시간 분류(CTC) 특징 길이 압축 및 로랭크 적응(LORA)과 관련이 있습니다. 이러한 주제에 대해서는 다음에서 설명합니다. 2.1. 대규모 언어 모델 LLM은 일반적으로 다양한 도메인과 언어에 걸친 방대한 양의 텍스트 데이터에 대해 사전 학습됩니다. 일반적으로 각 출력 토큰을 입력으로 사용하여 다음 단계 토큰을 예측하는 자동 회귀 디코더 전용 아키텍처를 따르는 변환기 계층의 스택으로 구성됩니다. 이 연구에서는 제안된 방법을 구축하기 위한 백본 LLM으로 LLaMA-7B[5]를 선택합니다. LLaMA-7B 모델은 32개의 헤드와 4096개의 어텐션 차원을 가진 32개의 변환기 인코더 계층으로 구성됩니다. LLAMA 작업의 토크나이저는 언어 그룹을 포괄하는 32,000의 어휘 크기를 갖습니다. 2.2. CTC 압축기 Connectionist Temporal Classification(CTC) 압축기[23]는 기능에서 중복된 정보를 제거하여 시퀀스 길이를 줄이기 위해 제안되었습니다. 음성 번역 작업에 적용되었고 더 나은 메모리 소비와 성능을 제공하는 것으로 나타났습니다. 이 방법은 주요 교차 엔트로피 기준과 함께 공동으로 최적화된 인코더의 중간 계층에 선형 CTC 분기를 추가합니다. 그런 다음 CTC 분기의 숨겨진 표현은 CTC 사후 분포에 따라 압축되어 후속 계층으로 전달됩니다. 저자는 이 시퀀스 길이 압축 방법 내에서 몇 가지 변형을 조사했습니다. 그들은 연속적인 숨겨진 표현(동일한 클래스에 속하는 연속적인 CTC 예측에 해당)을 평균화하면 최상의 성능을 제공한다는 것을 발견했습니다. 2.3. LORA Low-Rank Adaptation(LoRA)[24]은 새로운 데이터 세트 또는 작업에 대해 대규모 모델을 적응시키는 데 일반적으로 사용되는 기술입니다. 모든 원래 모델 매개변수를 동결하는 동안 소스 대규모 모델의 각 Transformer 계층에 소량의 자유 매개변수를 도입합니다. 구체적으로, Transformer 계층의 각 가중치 행렬 W Є Rdxk에 대해 r &lt; min{d, k}가 되도록 2개의 새로운 행렬 Wa Є Rdxr과 Wɩ € R™×가 도입됩니다. 학습 중 각 행렬 곱셈에 대해 입력 x는 먼저 원래 가중치 W와 도입된 저랭크 근사 Wa, W₁와 곱해진 다음, 두 출력을 합산하여 이후 계산을 위한 출력을 형성합니다. 미세 조정 중에는 Wa와 W₁만 업데이트되고 W는 고정되므로 학습 중 메모리 사용량이 크게 줄어듭니다. 3. 우리의 접근 방식 이 작업에서 우리는 SpeechLLAMA라는 아키텍처를 설계하는데, 여기서 텍스트-LLM은 텍스트 생성을 위한 조건부 프롬프트로 텍스트뿐만 아니라 음향 임베딩도 허용할 수 있습니다. 음성 입력을 길이와 의미의 측면에서 텍스트 임베딩의 동일한 공간 내에서 음향 임베딩 시퀀스로 변환함으로써, 사전 훈련된 텍스트 LLM은 컨텍스트 내 학습 용량을 활용하여 음성 신호를 흡수하고 음성 번역 작업을 위한 해당 텍스트를 출력할 수 있습니다. = 전반적으로 텍스트 프롬프트 p와 오디오 신호 x가 주어졌을 때, 텍스트-LLM을 사용하여 해당 텍스트 시퀀스 y {yo, Y1,... YN-1}을 생성하는 것은 다음과 같이 공식화됩니다. Np(y|p, x; OLLM): = II P(Yn\y <n, P, X; OLLM) (1) n=Embedding LLM Audio Encoder Text Prompt CTC Compressor Acoustic Features Predicted Tokens Decoder ↑ Conv2D Encoder <SOS>음향적 특징 예측 토큰 그림 1. LLM을 사용한 제안된 접근 방식의 고수준 아키텍처. 녹색 블록은 LLM의 일부를 나타냅니다. 이 작업에서 우리는 &quot;오디오 인코더&quot;에서만 매개변수를 학습하고 나머지는 모두 고정합니다. 여기서 y
--- CONCLUSION ---
&amp; 향후 작업 이 작업에서 우리는 음향 정보를 기성품 대규모 언어 모델에 주입하는 방법을 제안합니다. 제안된 모델은 음향 표현을 LLM의 의미 공간에 직접 매핑하여 오디오와 LLM 간의 긴밀한 통합을 제공합니다. 또한 음향 특징 압축, 어텐션 마스크 설계 및 어댑터 미세 조정을 포함하여 더 나은 성능을 위한 제안된 모델의 여러 가지 실용적인 측면을 탐구합니다. 우리는 13개 언어에서 영어로의 음성 번역 작업에서 제안된 모델이 강력한 시퀀스 대 시퀀스 기준 모델보다 상당히 우수한 성능을 발휘한다는 것을 보여줍니다. 또한 처음부터 학습된 디코더 전용 아키텍처가 약 40% 더 적은 매개변수로 비슷한 성능을 달성할 수 있음을 보여주며, 이는 일반적인 음성-텍스트 모델링을 위한 디코더 전용 모델의 잠재력을 검증합니다. 7. 참고문헌 [1] OpenAI, &quot;Introducing chatgpt,&quot; OpenAI Blog, 2022. [2] OpenAI, &quot;Gpt-4 기술 보고서,&quot; arXiv 사전 인쇄본 arXiv:2303.08774, 2023. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., &quot;Language models are few-shot learners,&quot; Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020. [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al., “Palm: 경로를 통한 언어 모델링 확장,&quot; arXiv 사전 인쇄본 arXiv:2204.02311, 2022. [5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al., “Llama: 개방적이고 효율적인 기반 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin, &quot;Attention is all you need,&quot; 신경 정보 처리 시스템의 발전, 30권, 2017. [7] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu 등, &quot;Audiogpt: 음성, 음악, 소리 및 말하는 머리 이해 및 생성&quot;, arXiv 사전 인쇄 arXiv:2304.12995, 2023. [8] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu 및 Yueting Zhuang, “Hugginggpt: AI 해결 작업과 chatgpt 및 huggingface의 친구들,” arXiv 사전 인쇄본 arXiv:2303.17580, 2023. [9] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu 및 Bo Xu, “X-llm: 다중 모달리티를 외국어로 처리하여 고급 대규모 언어 모델 부트스트래핑,” arXiv 사전 인쇄본 arXiv:2305.04160, 2023. [10] Soham Deshmukh, Benjamin Elizalde, Rita Singh 및 Huaming Wang, “Pengi: 오디오 작업을 위한 오디오 언어 모델,” arXiv 사전 인쇄본 arXiv:2305.11834, 2023. [11] Jinyu Li, “종단 간 자동 음성 인식의 최근 발전,” APSIPA 신호 및 정보 처리 논문, 제11권, 제1호. 1, 2022. [12] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny, “Minigpt-4: 고급 대형 언어 모델을 통한 비전 언어 이해 향상,&quot; arXiv 사전 인쇄 arXiv:2304.10592, 2023. [13] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, &quot;시각적 명령 조정&quot; arXiv 사전 인쇄 arXiv:2304.08485, 2023. [14] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, 외, &quot;Llama-adapter v2: 매개변수 효율적인 시각적 명령 모델&quot; arXiv 사전 인쇄 arXiv:2304.15010, 2023. [15] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds 외, &quot;Flamingo: few-shot learning을 위한 시각 언어 모델&quot;, 신경 정보 처리 시스템의 발전, 제 2권. 35, pp. 23716–23736, 2022. [16] Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li 및 Furu Wei, &quot;Viola: 음성 인식, 합성 및 번역을 위한 통합 코덱 언어 모델&quot;, arXiv 사전 인쇄 arXiv:2305.16107, 2023. [17] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou 및 Xipeng Qiu, “SpeechGpt: 고유한 교차 모달 대화 능력을 갖춘 대규모 언어 모델 강화,&quot; arXiv 사전 인쇄 arXiv:2305.11000, 2023. [18] Eliya Nachmani, Alon Levkovitch, Julian Salazar, 출라유츠시 Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan 및 Michelle Tadmor Ramanovich, &quot;음성이 있는 Lms: 음성 토큰을 넘어서는 구어체 언어 모델링,&quot; arXiv 사전 인쇄본 arXiv:2305.15255, 2023. [19] Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al., &quot;Audiopalm: 말하고 들을 수 있는 대규모 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2306.12925, 2023. [20] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., &quot;신경 코덱 언어 모델은 음성 합성기에 대한 제로 샷 텍스트입니다.&quot; arXiv 사전 인쇄 arXiv:2301.02111, 2023. [21] Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei, &quot;자신의 목소리로 외국어 말하기: 교차 언어 신경 코덱 언어 모델링&quot;, 2023. [22] Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, Nigel Collier, &quot;디코더 전용 또는 인코더-디코더? 정규화된 인코더-디코더로 언어 모델 해석,&quot; arXiv 사전 인쇄 arXiv:2304.04052, 2023. [23] Marco Gaido, Mauro Cettolo, Matteo Negri 및 Marco Turchi, &quot;직접 음성 번역을 위한 Ctc 기반 압축&quot;, arXiv 사전 인쇄 arXiv:2102.01578, 2021. [24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang 및 Weizhu Chen, &quot;Lora: 대규모 언어 모델의 하위 순위 적응&quot;, arXiv 사전 인쇄 arXiv:2106.09685, 2021. [25] Laura Cross Vila, Carlos Escolano, José AR Fonollosa 및 Marta R Costa-Jussa, &quot;변환기를 사용한 엔드투엔드 음성 번역.,&quot; in 영어: Interspeech 논문집, 2018, 60-63쪽. [26] Matthias Sperber와 Matthias Paulik, &quot;음성 번역과 종단간 약속: 현재 위치 파악&quot;, Association for Computational Linguistics 제58회 연례 회의 논문집, 2020, 7409-7421쪽. [27] Jian Xue, Peidong Wang, Jinyu Li, Eric Sun, &quot;진정한 제로샷 기능을 갖춘 약한 감독 스트리밍 다국어 음성 모델&quot;, arXiv 사전 인쇄본 arXiv:2211.02499, 2022. [28] Jian Xue, Peidong Wang, Jinyu Li, Matt Post, Yashesh Gaur, &quot;신경 변환기를 사용한 대규모 스트리밍 종단간 음성 번역&quot;, INTERSPEECH, vol. abs/2204.05352, 2022. [29] Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long Zhou, Yashesh Gaur, Shujie Liu, Jinyu Li, &quot;Lamassu: 신경 변환기를 사용한 스트리밍 언어 독립적 다국어 음성 인식 및 번역&quot;, INTERSPEECH, 2023. [30] Changhan Wang, Anne Wu, Juan Pino, &quot;Covost 2: 대규모 다국어 음성-텍스트 번역 코퍼스&quot;, 2020. [31] Matt Post, &quot;BLEU 점수 보고의 명확성 요구&quot;, 제3차 기계 번역 회의록: 연구 논문, 벨기에 브뤼셀, 2018년 10월, 186-191쪽, 계산언어학 협회. [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색&quot;, The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [33] A. Berard, O. Pietquin, C. Servan, L. Besacier, &quot;듣고 번역하기: 종단 간 음성-텍스트 번역을 위한 개념 증명&quot;, NIPS 음성 및 오디오 처리를 위한 종단 간 학습 워크숍, 2016. [34] Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, Zhifeng Chen, &quot;시퀀스-투-시퀀스 모델은 외국어 음성을 직접 번역할 수 있습니다&quot;, Proc. Interspeech, pp. 2625-2629, 2017. [35] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, 및 Ilya Sutskever, &quot;대규모 약한 감독을 통한 견고한 음성 인식&quot;, International Conference on Machine Learning에서. PMLR, 2023, pp. 28492-28518. [36] Takaaki Hori, Shinji Watanabe, 및 John Hershey, &quot;종단간 음성 인식을 위한 공동 CTC/주의 디코딩&quot;, 55회 연례 총회록(제1권: 장문 논문), 캐나다 밴쿠버, 2017년 7월, pp. 518-529, Association for Computational Linguistics에서. [37] Noam Shazeer, &quot;Glu 변형이 변압기를 개선하다&quot;, arXiv 사전 인쇄본 arXiv:2002.05202, 2020. [38] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, 및 Yunfeng Liu, &quot;Roformer: 회전 위치 임베딩을 갖춘 향상된 변압기,&quot; arXiv 사전 인쇄본 arXiv:2104.09864, 2021. [39] Ilya Loshchilov 및 Frank Hutter, &quot;분리된 가중치 감소 정규화,&quot; arXiv 사전 인쇄본 arXiv:1711.05101, 2017. [40] Taku Kudo 및 John Richardson, &quot;Sentencepiece: 신경망 텍스트 처리를 위한 간단하고 언어 독립적인 하위 단어 토크나이저 및 디토크나이저,&quot; arXiv 사전 인쇄본 arXiv:1808.06226, 2018. [41] Yuang Li, Yu Wu, Jinyu Li, 및 Shujie Liu, “음성 인식에서 제로 샷 도메인 적응을 위한 대규모 언어 모델 촉진”, arXiv 사전 인쇄본 arXiv:2306.16007, 2023.
