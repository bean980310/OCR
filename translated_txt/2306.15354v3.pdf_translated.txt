--- ABSTRACT ---
음성 발화에서 상관관계가 없는 정보를 풀어내는 것은 음성 커뮤니티 내에서 중요한 연구 주제입니다. 다양한 음성 관련 작업은 다른 상관관계가 없는 정보의 영향을 최소화하면서 고유한 음성 표현을 추출하는 데 중점을 둡니다. 우리는 음성 표현 풀림 연구를 용이하게 하기 위해 대규모 음성 코퍼스를 제시합니다. 3D-Speaker에는 10,000개 이상의 화자가 포함되어 있으며, 각 화자는 여러 장치에서 동시에 녹음되고 서로 다른 거리에 위치하며 일부 화자는 여러 방언을 사용합니다. 다차원 오디오 데이터의 제어된 조합은 다양한 음성 표현 얽힘의 매트릭스를 생성하여 이를 풀어내는 흥미로운 방법을 동기를 부여합니다. 3DSpeaker의 다중 도메인 특성은 또한 대규모 범용 음성 모델을 평가하고 도메인 외부 학습 및 자체 감독 학습 방법을 실험하는 데 적합한 리소스입니다. https://3dspeaker.github.io/ 1
--- INTRODUCTION ---
음성 발화에서 상관관계가 없는 정보를 풀어내는 것은 음성 커뮤니티[1][2] 내에서 중요한 연구 주제입니다. 음성 발화는 일반적으로 음성 내용, 화자 특성, 방언, 녹음 장치, 음원까지의 거리 및 환경과 소음과 같은 기타 정보와 같은 정보가 혼합된 것으로 구성됩니다. 다양한 음성 관련 작업은 상관관계가 없는 정보의 영향을 최소화하면서 관심 있는 특정 정보를 인식하는 것을 목표로 합니다. 예를 들어, 자동 음성 인식(ASR)에서 연구자들은 화자의 음성 특성, 소음 및 기타 상관관계가 없는 정보의 영향을 받지 않고 음성 내용을 인식하는 것을 목표로 합니다. 반면 화자 검증(SV)은 음성 내용과 무관하게 화자의 음성을 식별하는 데 중점을 둡니다. 음성 합성 작업에서 연구자들은 스타일 변환, 교차 언어 합성, 음성 변환 등과 같은 목표를 달성하기 위해 얽힘 풀린 임베딩을 활용합니다. 화자 검증은 화자의 음성이 모든 발화에서 편재하는 특성이지만 음성 내용, 장치, 언어 등과 같은 다른 음성 정보와 복잡하게 섞여 있기 때문에 다양한 음성 관련 정보의 성공적인 얽힘 풀림에서 가장 많은 혜택을 받는 작업 중 하나입니다. 또한 장기적 안정성과 상대적 고유성과 같은 특성을 가지고 있습니다. 인간 음성에서 얽힘 풀린 화자 표현을 추출하는 방법과 기술은 비전 및 자연어 이해에서 글로벌 기능을 추출하는 것과 같이 다른 머신 러닝 분야로 일반화할 수 있습니다. 그러나 음성 표현 얽힘 풀림 연구는 음성의 여러 속성을 특징짓는 명시적 레이블이 포함된 대규모 공개적으로 사용 가능한 데이터 세트가 부족하여 크게 방해를 받았습니다. 영어: 관련 연구를 가속화하기 위해 모든 발화에 여러 개의 레이블이 포함된 3D-Speaker를 소개합니다.표 1: 화자 레이블을 제공하는 여러 무료로 제공되는 오디오 데이터 세트의 비교.여러 개의 레이블 여러 화자의 레이블 방언 샘플링 속도 주석이 달린 장치 거리 텍스트/언어 3D-Speaker 10000+ 예 예 예 16k &amp; 48k 예 VoxCeleb 1&amp;2[17] 7000+ 아니요 아니요 아니요 16k 아니요 CN-Celeb[18] 아니요 아니요 아니요 16k 아니요 Librispeech[19] 아니요 아니요 아니요 16k 예 AliMeeting[25] 아니요 아니요 아니요 16k &amp; 48k 예 AISHELL-4[23] 아니요 아니요 아니요 16k 예 화자 ID, 말한 방언, 녹음 장치 유형, 장치에서 화자까지의 거리와 같은 여러 음성 특성을 나타내는 레이블이 포함되어 있습니다. 3D-Speaker는 지도 학습 및 비지도 학습 방법, 도메인 내 및 도메인 외 학습을 실험하는 데 사용할 수 있습니다. 또한 모든 도메인에서 일반적인 음성 관련 작업을 수행하는 능력을 갖추는 것을 목표로 하는 범용 음성 모델을 평가하는 데 사용할 수도 있습니다. 이전 연구에 따르면, 훈련 데이터의 화자 수를 늘리면 화자 검증 시스템의 성능이 현저히 향상됩니다[3][4]. 우리가 아는 한, 3D-Speaker는 화자 수 측면에서 가장 큰 공개 액세스 가능한 코퍼스입니다. 2
--- RELATED WORK ---
s 스피커의 음성만을 나타내는 스피커 임베딩을 추출하고 관련 없는 정보의 영향을 제거하려는 노력이 많이 있습니다.
--- METHOD ---
s를 사용하여 얽힌 것을 풀어냅니다. 3DSpeaker의 다중 도메인 특성은 또한 대규모 범용 음성 모델을 평가하고
--- EXPERIMENT ---
영어: 도메인 외부 학습 및 자기 감독 학습 방법. https://3dspeaker.github.io/ 1 서론 음성 발화에서 상관관계가 없는 정보를 풀어내는 것은 음성 커뮤니티[1][2] 내에서 중요한 연구 주제입니다. 음성 발화는 일반적으로 음성 내용, 화자 특성, 방언, 녹음 장치, 음원까지의 거리 및 환경 및 소음과 같은 기타 정보와 같은 정보가 혼합되어 구성됩니다. 다양한 음성 관련 작업은 상관관계가 없는 정보의 영향을 최소화하면서 관심 있는 특정 정보를 인식하는 것을 목표로 합니다. 예를 들어, 자동 음성 인식(ASR)에서 연구자는 화자의 음성 특성, 소음 및 기타 상관관계가 없는 정보의 영향을 받지 않고 음성 내용을 인식하는 것을 목표로 합니다. 반면 화자 검증(SV)은 음성 내용과 무관하게 화자의 음성을 식별하는 데 중점을 둡니다. 음성 합성 작업에서 연구자들은 스타일 변환, 교차 언어 합성, 음성 변환 등과 같은 목표를 달성하기 위해 얽힘 풀린 임베딩을 활용합니다. 화자 검증은 화자의 음성이 모든 발화에서 편재하는 특성이지만 음성 내용, 장치, 언어 등과 같은 다른 음성 정보와 복잡하게 섞여 있기 때문에 다양한 음성 관련 정보의 성공적인 얽힘 풀림에서 가장 많은 혜택을 받는 작업 중 하나입니다. 또한 장기적 안정성과 상대적 고유성과 같은 특성을 가지고 있습니다. 인간 음성에서 얽힘 풀린 화자 표현을 추출하는 방법과 기술은 비전 및 자연어 이해에서 글로벌 기능을 추출하는 것과 같이 다른 머신 러닝 분야로 일반화할 수 있습니다. 그러나 음성 표현 얽힘 풀림 연구는 음성의 여러 속성을 특징짓는 명시적 레이블이 포함된 대규모 공개적으로 사용 가능한 데이터 세트가 부족하여 크게 방해를 받았습니다. 영어: 관련 연구를 가속화하기 위해 모든 발화에 여러 개의 레이블이 포함된 3D-Speaker를 소개합니다.표 1: 화자 레이블을 제공하는 여러 무료로 제공되는 오디오 데이터 세트의 비교.여러 개의 레이블 여러 화자의 레이블 방언 샘플링 속도 주석이 달린 장치 거리 텍스트/언어 3D-Speaker 10000+ 예 예 예 16k &amp; 48k 예 VoxCeleb 1&amp;2[17] 7000+ 아니요 아니요 아니요 16k 아니요 CN-Celeb[18] 아니요 아니요 아니요 16k 아니요 Librispeech[19] 아니요 아니요 아니요 16k 예 AliMeeting[25] 아니요 아니요 아니요 16k &amp; 48k 예 AISHELL-4[23] 아니요 아니요 아니요 16k 예 화자 ID, 말한 방언, 녹음 장치 유형, 장치에서 화자까지의 거리와 같은 여러 음성 특성을 나타내는 레이블이 포함되어 있습니다. 3D-Speaker는 지도 학습 및 비지도 학습 방법과 도메인 내/외 학습을 실험하는 데 사용할 수 있습니다. 또한 모든 도메인에서 일반적인 음성 관련 작업을 수행하는 기능을 보유하는 것을 목표로 하는 범용 음성 모델을 평가하는 데 사용할 수도 있습니다. 이전 연구에 따르면 학습 데이터의 화자 수를 늘리면 화자 검증 시스템의 성능이 크게 향상됩니다[3][4]. 저희가 아는 한 3D-Speaker는 화자 수 측면에서 공개적으로 액세스 가능한 가장 큰 코퍼스입니다. 2 관련 연구 상관 관계가 없는 정보의 영향을 제거하고 화자의 음성만을 나타내는 화자 임베딩을 추출하려는 노력이 많이 있습니다. 이러한 방법은 적대적 학습[5][6][7][8][9]에서 데이터 증강 및 일반화[10][11][12][13]와 같은 데이터 기반 접근 방식에 이르기까지 다양합니다. 자체 지도 학습을 기반으로 하는 일부 음성 표현 모델은 다양한 음성 정보를 여러 계층으로 풀 수 있는 기능이 있는 것으로 나타났습니다[14][15][16]. 이전에 공개된 여러 코퍼스는 음성 인식 및 화자 검증 연구를 성공적으로 촉진했습니다. VoxCeleb 1 및 2[17]는 인터넷에서 7,000명 이상의 화자를 수집했으며, 화자는 다양한 인종, 언어 및 연령대에 걸쳐 있습니다. 불행히도 화자 신원 외의 레이블이 누락되어 다른 음성 표현을 풀어내고 도메인 외부 작업을 처리하는 데 효과적이지 않습니다. CN-Celeb[18]는 VoxCeleb와 비슷한 방식으로 약 3,000명의 화자를 수집했습니다. 또한 CN-Celeb는 &quot;장르&quot; 레이블을 제공하여 코퍼스에 더 많은 다양성을 도입하고 잠재적으로 &quot;장르 간&quot; 연구를 허용합니다. 그러나 연극, 영화, 블로그, 드라마와 같은 장르는 직접적인 음성 특성이 아니며 관심 있는 얽힌 음성 표현에 대해 거의 추론하지 못합니다. Librispeech[19]는 오디오북 읽기의 영어 음성 컬렉션입니다. 각 발화에 대한 주석이 달린 텍스트를 포함하는 Librispeech는 음성 인식 및 텍스트-음성 합성 작업에 중요한 코퍼스입니다. 그러나 Librispeech는 데이터 소스, 언어 및 기타 음성 측면 측면에서 다양성이 부족합니다. AliMeeting[20]은 3D-Speaker와 비슷한 방식으로 수집됩니다. 각 녹음 세션 동안 여러 녹음 장치가 스피커 앞에 무작위로 배치됩니다. 그러나 AliMeeting에서는 장치의 레이블과 스피커까지의 거리가 제공되지 않습니다. 500명 미만의 스피커를 포함하는 AliMeeting은 스피커 검증 작업을 위한 훈련 코퍼스로만 사용하기에 적합하지 않습니다. NIST SRE 데이터 세트는 정기적으로 개최되는 평가[21]에서 축적되어 수집됩니다. 그러나 대중이 자유롭게 접근할 수 없습니다. 300명의 화자가 있는 SITW[22], 61명의 화자가 있는 AISHELL-4[23], 630명의 화자가 있는 TIMIT[24] 등을 포함하되 이에 국한되지 않는 화자 신원이 포함된 다른 많은 오디오 데이터 세트가 있습니다. 0.1m 0.3m 1m 2m 09:2.5m 4m 그림 1: 녹음 세션에서 장치 배치의 예. 장치는 각 녹음 세션의 시작 부분에서 무작위로 섞입니다. 3 데이터 세트 설명 훈련 데이터 세트에는 총 10,000명의 화자와 579,013개의 발화가 포함됩니다. 유효한 음성의 총 지속 시간은 1,124시간입니다. 데이터 세트의 특정 발화는 다양한 거리에서 다른 장치를 사용하여 동시에 녹음되었기 때문에 동일한 음성 콘텐츠를 공유한다는 점에 주목할 가치가 있습니다. 또한 이 데이터 세트에는 표준 중국어와 화자가 선택한 지역 방언의 두 가지 방언으로 녹음된 1,200명의 화자가 포함되어 있습니다.3.1 다중 장치 모든 발화는 표 2에서 선택한 여러 다른 장치에서 동시에 녹음됩니다.iPad, Android 휴대전화, iPhone, 마이크 어레이(약칭 어레이), PC 노트북, 녹음 펜(약칭 RP), 단일 방향성 마이크, 전화(지정되지 않음).마이크 어레이는 8개 방향성 마이크로 구성되어 있습니다.우리는 이전에 AliMeeting 데이터 세트[20]와 화자 일화 시스템[27]에서 사용된 [26]에 설명된 차등 원형 어레이의 설계를 따릅니다.표 2: 3D-Speaker의 장치에 대한 자세한 정보. 장치 iPad 발언 수 백분율 11.25% Android 11.26% iPhone 11.26% 배열 15.55% PC 16.63% RP 18.41% 방향성 9.99% 전화(지정되지 않음) 합계 5.65% 100.00% 3.2 다중 거리 각 녹음 세션 동안 다른 장치는 스피커로부터 다양한 거리에 무작위로 배치됩니다. 이러한 특정 거리는 표 3에 분류되어 표시됩니다. 거리 범위는 0.1m에서 4m입니다. 실제 사용 시나리오를 시뮬레이션하기 위해 PC 노트북은 독점적으로 표 3 내에 위치합니다. 3D-Speaker에서 소스와 장치 간 거리에 대한 자세한 정보. 거리(m) 발화 수 지속시간(h) 지속시간 백분율 0.26.2.40% 0.14.1.26% 0.57.5.07% 0.8.0.79% 0.0.0.08%180.16.08% 1.1.0.10% 1.6.0.59%138.12.27% 2.7.0.63%138.12.31%247.21.97% 미지정297.26.44% 총계1124.100.00% 표 4: 3D-Speaker에서 말하는 다양한 방언의 자세한 정보. 방언 화자 수 기간(시간) 장화이 방언 2.간 방언 1.우 방언 8.진 방언 29.민 방언 1.중부 평원 방언 3.객가 방언 2.길루 방언 5.랴오자오 방언 6.북부 방언 0.샹 방언 0.남서부 방언 13.광둥어 총계 2.77.화자로부터 0.3m 이내에 지향성 마이크를 배치합니다.3.3 다중 방언 훈련 세트에는 표 4에 나와 있는 것처럼 여러 방언을 사용하는 1,074명의 화자가 포함됩니다.각 화자는 먼저 표준 만다린어를 말하는 것을 녹음합니다.그런 다음 각자의 지역 방언을 말하도록 요청받습니다.전체 세션은 화자와 다른 거리에 위치한 여러 장치를 통해 녹음됩니다.방언 선택은 서로 및 표준 만다린어와 상당히 다르도록 하는 것을 목표로 수행되었습니다. 이것은 특정 방언을 구사하지 않는 개인이 이해하기 어렵다고 생각할 정도입니다.3.4 평가 세트 표 5는 평가 세트에 대한 설명 정보를 제공합니다.총 240명의 화자와 발화가 있습니다.트레인 세트에는 화자가 포함되지 않습니다.평가 세트에는 11개의 고유한 방언이 포함되며, 이는 모두 트레이닝 세트의 일부 화자가 말합니다.표 5: 평가 세트에 대한 자세한 정보.화자 수 발화 수 15.방언 수 기간(시간) 4 실험 및 벤치마크 마이크로폰 어레이는 8개의 채널로 구성되며 각 채널의 샘플링 속도는 48kHz입니다.기준 시스템에서는 첫 번째 채널만 가져와 16kHz로 다운샘플링합니다.이전 연구에서 8개 채널을 모두 모델링하면 귀중한 정보를 얻을 수 있음을 발견했습니다[28][29].기준 시스템의 경우 CAM++[25], ERes2Net[30], ECAPA-TDNN[31]을 선택했습니다. 결과는 표 6에 나와 있습니다. 모든 실험에 대한 메트릭으로 EER과 minDCF(p_target=0.01,c_miss=1,c_fa=1)를 사용합니다. 4.1 트랙 A: 교차 장치 화자 검증 교차 장치 시험에서 등록 및 테스트 발화가 별도의 장치를 사용하여 기록되도록 보장합니다. 또한 180,000번의 시험 각각에 대해 등록 및 테스트 발화 간의 음성 콘텐츠가 다른지 확인합니다. 이 시험은 음향적 유사성으로 인해 &quot;iPhone&quot;, &quot;Android&quot; 및 &quot;Phone&quot; 범주를 하나로 간주합니다. 표 6: 다양한 트랙에서 기준 시스템의 성능. Cross-Device Cross-Distance Cross-Dialect Method ECAPA-TDNN[31]¹ # of Params EER(%) minDCF EER(%) minDCF EER(%) minDCF 20.8M 8.0.12.0.14.0.CAM++ Base[25]² 7.2M 7.0.11.0.13.0.ERes2Net Base[30]4.6M 7.0.9.0.12.0.ERes2Net Large[30] 18.3M 6.0.9.0.11.0.4.2 트랙 B: Cross-Distance Speaker Verification Cross-distance trial에서 0.8m 이상의 거리는 &quot;원거리&quot;로 간주되고, 0.8m 미만의 거리는 &quot;근거리&quot;로 분류됩니다. 이러한 분류를 염두에 두고, 우리는 175,163건의 모든 시도에서 등록 및 테스트 발화가 다른 분류 범주에서 선택되도록 세심하게 보장합니다. 트랙 A와 유사하게, 우리는 등록 및 테스트 발화의 음성 내용이 서로 다르다는 것을 보장합니다. 4.3 트랙 C: 교차 방언 화자 검증 교차 방언 시도에서는 등록 또는 테스트 발화가 표준 중국어이고 다른 하나는 해당 화자의 지역 방언임이 보장됩니다. 4.4 트랙 D: 언어/방언 식별 언어/방언 식별 작업에서는 테스트 세트의 모든 발화를 사용하고 전반적인 식별 정확도를 추정합니다. 우리는 vanilla CAM++를 사용하여 기준 벤치마크를 제공합니다. 극복하기 위해 ¹구현: https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/ lobes/models/ECAPA_TDNN.py 2공식 구현: https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/ egs/sv-cam%2B%2B 3 공식 구현: https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/ egs/sv-eres2net *공식 구현: https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/ egs/sv-eres2net 표 7: 방언 식별에 대한 기준 시스템의 성능. 학습 정확도(%) 테스트 정확도(%) 기준선[25] 96.29.표 8: 다양한 트랙에서 기준선 자기 지도 학습 시스템의 성능.EER 및 minDCF(Ptarget 0.05, Cmiss = 1, Cfa = 1)는 성능 측정에 사용됩니다.= 교차 장치 교차 거리 교차 방언 방법 RDINO[32]EER(%) minDCF 20.41 0.EER(%) minDCF 21.92 0.EER(%) minDCF 25.0.레이블의 불균형으로 인해 기준선 시스템에서 학습 데이터의 작은 하위 집합만 사용합니다.결과는 표 7에 나와 있습니다.4.5 기타 작업 위에서 설명한 작업 및 벤치마크 외에도 3DSpeaker의 풍부한 다중 도메인 정보를 통해 연구자는 자신만의 작업을 설계하고 필요에 맞게 학습 및 평가 세트를 맞춤화할 수 있습니다.도메인 외부 학습. 3D-Speaker를 사용하면 연구자는 도메인 외부 학습에 대한 실험을 수행할 수 있습니다. 예를 들어, 연구자는 특정 장치의 발화를 학습 세트에서 제거하고 이러한 장치에서 모델 성능을 평가할 수 있습니다. 또한 &quot;근거리&quot; 데이터에서만 모델을 학습하고 &quot;원거리&quot; 데이터에서 평가할 수도 있습니다. 자기 지도 학습. 3D-Speaker의 다양한 특성은 음향 데이터에 대한 자기 지도 학습 방법을 탐색하기에 이상적인 후보가 됩니다. 표 8에서는 RDINO 자기 지도 학습 방법을 사용하는 기준 시스템을 제공하며, 여기서 3D-Speaker의 모든 레이블을 알 수 없는 것으로 처리합니다[32]. 대규모 범용 음성 모델 평가. 3D-Speaker는 대규모 음성 모델의 범용 성능을 평가하기에 적합한 리소스입니다. 대규모 범용 음성 모델은 다양한 도메인에서 상당히 좋은 성능을 보일 것으로 예상됩니다. 5
--- CONCLUSION ---
우리는 음성 표현 풀림 연구를 용이하게 하도록 설계된 대규모 음성 코퍼스인 3D-Speaker를 소개했습니다. 이 코퍼스에서 다차원 오디오 데이터의 제어된 조합은 다양한 음성 표현 얽힘 혼합의 매트릭스를 생성하여 이를 풀기 위한 흥미로운 방법을 동기를 부여합니다. 3D-Speaker의 다중 도메인 특성은 또한 대규모 범용 음성 모델을 평가하고 도메인 외부 학습 및 자체 감독 학습 방법을 실험하는 데 적합한 리소스입니다. 또한 3D-Speaker는 화자 수 측면에서 가장 큰 공개 액세스 가능한 코퍼스로, 화자 검증 시스템 및 기타 음성 관련 작업의 성능을 개선하는 데 사용할 수 있습니다. 전반적으로 3D-Speaker는 음성 관련 분야의 연구를 발전시키는 데 귀중한 리소스를 제공합니다. 6 윤리 우리는 음성이 고유한 신체적 특성이자 중요한 인간 생체 인식이라는 것을 이해합니다. 따라서 3D-Speaker를 수집하는 동안 화자와 상호 합의에 도달하도록 합니다. 발표자들은 녹음된 내용이 학술 연구 목적으로 사용되고 공개적으로 접근 가능하다는 점을 이해합니다. &quot;공식 구현: https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/sv-rdino참조 [1] Wei-Ning Hsu, Yu Zhang, Ron J. Weiss, Yu-An Chung, Yuxuan Wang, Yonghui Wu, and James R. Glass. 데이터 증강 및 적대적 인수분해를 통해 음성 합성을 위한 상관관계가 있는 스피커와 노이즈 분리. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP 2019, 브라이튼, 영국, 2019년 5월 12-17일, 5901-5905페이지. IEEE, 2019. [2] Wei-Ning Hsu, Yu Zhang, and James R. Glass. 순차적 데이터에서 분리되고 해석 가능한 표현의 비지도 학습. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, SVN Vishwanathan, Roman Garnett, 편집자, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017년 12월 4-9일, 미국 캘리포니아주 롱비치, 1878-1889쪽, 2017. [3] Joon Son Chung, Arsha Nagrani, Andrew Zisserman. Voxceleb2: 심층적 화자 인식. Interspeech 2018, 제19회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 인도 하이데라바드, 2018년 9월 2-6일, 1086-1090쪽. ISCA, 2018. [4] Siqi Zheng, Gang Liu, Hongbin Suo, Yun Lei. 도메인 외부 화자 검증을 위한 자동 인코더 기반 반지도 커리큘럼 학습. Interspeech 2019, 제20회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 오스트리아 그라츠, 2019년 9월 15일-9월, 4360-4364쪽. ISCA, 2019. [5] Bengt J. Borgström, Elliot Singer, Douglas A. Reynolds, Seyed Omid Sadjadi. 불충분한 도메인 내 데이터를 사용하여 화자 검증 도메인 적응의 효과성 개선. Interspeech 2017, 제18회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 스웨덴 스톡홀름, 2017년 8월 20-24일, 1557-1561쪽. ISCA, 2017. [6] Saurabh Kataria, Jesús Villalba, Piotr Zelasko, Laureano Moro-Velázquez, Najim Dehak. Deep feature cyclegans: Speaker identity preserving non-parallel microphone-telephone domain adaptation for speaker verification. Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 2021, 1079-1083쪽. ISCA, 2021. [7] Fuchuan Tong, Siqi Zheng, Haodong Zhou, Xingjia Xie, Qingyang Hong, Lin Li. 속도 불변 화자 검증을 위한 심층 표현 분해. Odyssey 2022: The Speaker and Language Recognition Workshop, 2022년 6월 28일-7월 1일, 베이징, 중국, 228-232쪽. ISCA, 2022. [8] Zhengyang Chen, Shuai Wang, Yanmin Qian. 부분적으로 공유된 네트워크를 사용한 화자 검증을 위한 적대적 도메인 적응. Interspeech 2020, 제21회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 가상 이벤트, 중국 상하이, 2020년 10월 25일, 3017-3021쪽. ISCA, 2020. [9] Siqi Zheng, Yun Lei, Hongbin Suo. 단시간 텍스트 독립 화자 검증을 위한 음성 인식 결합 네트워크. Interspeech 2020, 제21회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 가상 이벤트, 중국 상하이, 2020년 10월 25일, 926-930쪽. ISCA, 2020. [10] Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M. Hospedales. 일반화 학습: 도메인 일반화를 위한 메타 학습. 제32회 AAAI 인공지능 컨퍼런스(AAAI-18)의 회의록, 제30회 혁신적 인공지능 응용(IAAI-18), 제8회 AAAI 인공지능 교육 발전 심포지엄(EAAI-18), 미국 루이지애나주 뉴올리언스, 2018년 2월 2-7일, 3490-3497쪽. AAAI 출판부, 2018. [11] S. Shahnawazuddin, Waquar Ahmad, Nagaraj Adiga, Avinash Kumar. 제한된 데이터 시나리오에서 어린이의 화자 검증 시스템을 개선하기 위한 도메인 내 및 도메인 외 데이터 증강. 2020년 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP 2020, 스페인 바르셀로나, 2020년 5월 4-8일, 7554-7558쪽. IEEE, 2020.[12] Guangxing Li, Wangjin Zhou, Sheng Li, Yi Zhao, Jichen Yang, Hao Huang. 화자 검증 작업을 위한 효과적인 도메인 적응 방법 조사. Neural Information Processing - 29th International Conference, ICONIP 2022, Virtual Event, 2022년 11월 22-26일, Proceedings, Part VI, Communications in Computer and Information Science의 1793권, 517-527쪽. Springer, 2022. [13] Hanyi Zhang, Longbiao Wang, Kong Aik Lee, Meng Liu, Jianwu Dang, Hui Chen. 화자 검증을 위한 도메인 불변 변환 학습. IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 2022년 5월 23-27일, 7177-7181쪽. IEEE, 2022. [14] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu 및 Furu Wei. Wavlm: 풀 스택 음성 처리를 위한 대규모 자체 감독 사전 학습입니다. IEEE J. 셀. 맨 위. Signal Process., 16(6):1505-1518, 2022. [15] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov 및 Abdelrahman Mohamed. Hubert: 숨겨진 단위의 마스크 예측을 통한 자기 지도 음성 표현 학습. IEEE ACM 트랜스. 오디오 스피치 랭. Process., 29:3451-3460, 2021. [16] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: 음성 표현의 자기 감독 학습을 위한 프레임워크. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상, 2020. [17] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: 대규모 화자 식별 데이터 세트. Interspeech 2017, 국제 음성 커뮤니케이션 협회 제18차 연례 컨퍼런스, 스웨덴 스톡홀름, 2017년 8월 20-24일, 2616-2620쪽. ISCA, 2017. [18] Y. Fan, JW Kang, LT Li, KC Li, HL Chen, ST Cheng, PY Zhang, ZY Zhou, YQ Cai, D. Wang. Cn-celeb: 까다로운 중국어 화자 인식 데이터 세트. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, ICASSP 2020, 스페인 바르셀로나, 2020년 5월 4-8일, 7604-7608페이지. IEEE, 2020. [19] Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur. Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스. 2015년 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP 2015, 호주 퀸즐랜드주 사우스 브리즈번, 2015년 4월 19-24일, 5206-5210페이지.IEEE, 2015. [20] Fan Yu, Shiliang Zhang, Yihui Fu, Lei Xie, Siqi Zheng, Zhihao Du, Weilong Huang, Pengcheng Guo, Zhijie Yan, Bin Ma, Xin Xu, Hui Bu.M2met: icassp 2022 다중채널 다중당사자 회의 대본 작성 챌린지.IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP 2022, 가상 및 싱가포르, 2022년 5월 23-27일, 6167-6171페이지.IEEE, 2022. [21] 2012년 1차년도 화자 인식 평가 계획. 기술 보고서, 2012. [22] Mitchell McLaren, Luciana Ferrer, Diego Castán, Aaron Lawson. The Speakers in the Wild(SITW) 화자 인식 데이터베이스. Interspeech 2016, 제17회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 미국 캘리포니아주 샌프란시스코, 2016년 9월 8-12일, 818-822쪽. ISCA, 2016. [23] Yihui Fu, Luyao Cheng, Shubo Lv, Yukai Jv, Yuxiang Kong, Zhuo Chen, Yanxin Hu, Lei Xie, Jian Wu, Hui Bu, Xin Xu, Jun Du, Jingdong Chen. AISHELL-4: 회의 시나리오에서 음성 향상, 분리, 인식 및 화자 일기를 위한 오픈 소스 데이터 세트. Hynek Hermansky, Honza Cernocký, Lukás Burget, Lori Lamel, Odette Scharenborg, Petr Motlícek 편집자, Interspeech 2021, 제22회 국제 음성 커뮤니케이션 협회 연례 컨퍼런스, 체코 브르노, 2021년 8월 30일 - 9월 3일, 3665-3669쪽. ISCA, 2021.[24] JS Garofolo, LF Lamel, WM Fisher, JG Fiscus, DS Pallett. Darpa timit 음향 음성 연속 음성 코퍼스 cd-rom. nist 음성 디스크 1-1.1. NASA STI/Recon 기술 보고서, 1993. [25] Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, Qian Chen. CAM++: 컨텍스트 인식 마스킹을 사용한 화자 검증을 위한 빠르고 효율적인 네트워크. CoRR, abs/2303.00332, 2023. [26] Weilong Huang 및 Jinwei Feng. 방향성 마이크가 있는 균일한 원형 어레이를 위한 차등 빔포밍. Interspeech 2020, 국제 음성 커뮤니케이션 협회 제21차 연례 컨퍼런스, 가상 이벤트, 중국 상하이, 2020년 10월 25-29일, 71-75쪽. ISCA, 2020. [27] Siqi Zheng, Weilong Huang, Xianliang Wang, Hongbin Suo, Jinwei Feng, Zhijie Yan. 공간 스펙트럼을 기반으로 한 실시간 화자 다이어라이제이션 시스템. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP 2021, 캐나다 온타리오주 토론토, 2021년 6월 6-11일, 7208-7212쪽. IEEE, 2021. [28] Shiliang Zhang, Siqi Zheng, Weilong Huang, Ming Lei, Hongbin Suo, Jinwei Feng, Zhijie Yan. 다자간 회의에서 중첩 음성 감지를 위한 공간 음향 특징 조사. Interspeech 2021, 국제 음성 커뮤니케이션 협회 제22차 연례 컨퍼런스, 체코 브르노, 2021년 8월 30일 - 9월 3일, 3550-3554쪽. ISCA, 2021. [29] Siqi Zheng, Shiliang Zhang, Weilong Huang, Qian Chen, Hongbin Suo, Ming Lei, Jinwei Feng, Zhijie Yan. 빔 트랜스포머: 마이크로폰 어레이 기반 중첩 음성 감지. CORR, abs/2109.04049, 2021. [30] Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, Jiajun Qi. 화자 검증을 위한 로컬 및 글로벌 기능 융합을 갖춘 향상된 res2net. CoRR, abs/2305.12838, 2023. [31] Brecht Desplanques, Jenthe Thienpondt, Kris Demuynck. ECAPA-TDNN: TDNN 기반 화자 검증에서 강조된 채널 주의, 전파 및 집계. Interspeech 2020, 국제 음성 커뮤니케이션 협회 제21차 연례 컨퍼런스, 가상 이벤트, 중국 상하이, 2020년 10월 25-29일, 3830-3834쪽. ISCA, 2020. [32] Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng 및 Qian Chen. 정규화된 증류 프레임워크를 사용하여 자체 감독 스피커 검증의 한계를 넓히기. CoRR, abs/2211.04168, 2022.
