--- ABSTRACT ---
우리는 대규모 언어 모델(LLM)에 동시에 SEE와 Draw를 할 수 있는 새로운 능력을 부여하는 정교한 이미지 토크나이저인 SEED를 제시합니다. 이미지 토크나이저에 대한 연구는 이전에 교착 상태에 도달했습니다. 양자화된 시각적 토큰을 사용하는 프레임워크가 열악한 성능과 멀티모달 이해(BLIP-2 등과 비교) 또는 생성(안정적 확산 등과 비교)에서의 수렴으로 인해 중요성을 잃었기 때문입니다. 이러한 한계에도 불구하고 우리는 시각적 표현과 텍스트 표현을 통합하여 LLM의 원래 레시피로 확장 가능한 멀티모달 학습을 용이하게 하는 자연스러운 능력에 확신을 가지고 있습니다. 이 연구에서 우리는 LLM과의 후속 정렬을 효과적으로 용이하게 하는 SEED의 아키텍처와 학습을 위한 두 가지 중요한 원칙을 식별합니다. (1) 이미지 토큰은 2D 물리적 패치 위치와 독립적이어야 하며 대신 1D 인과적 종속성으로 생성되어야 하며 LLM의 좌우 자기 회귀 예측 메커니즘과 일치하는 본질적인 상호 의존성을 보입니다. (2) 이미지 토큰은 단어의 의미 추상화 정도와 일치하는 고수준 의미론을 포착해야 하며, 토크나이저 학습 단계에서 차별성과 재구성을 위해 최적화되어야 합니다. 결과적으로 기성형 LLM은 효율적인 LoRA 튜닝을 통해 SEED를 통합하여 이미지-텍스트 및 텍스트-이미지 생성을 모두 수행할 수 있습니다. 개선된 결과를 가져올 수 있는 포괄적인 멀티모달 사전 학습 및 명령어 튜닝은 향후 조사를 위해 예약되어 있습니다. 이 버전의 SEED는 64개의 V100 GPU와 5M개의 공개적으로 사용 가능한 이미지-텍스트 쌍만 사용하여 5.7일 만에 학습되었습니다. 저희의 예비 연구는 다재다능한 멀티모달 LLM에서 이산적인 시각적 토큰의 엄청난 잠재력과 더 광범위한 연구에서 적절한 이미지 토크나이저의 중요성을 강조합니다. 1
--- INTRODUCTION ---
최근 몇 년 동안, 다음 단어 예측과 같은 간단한 학습 목표를 가진 방대한 텍스트 코퍼스에서 사전 학습된 대규모 언어 모델[1, 2, 3](LLM)은 다양한 개방형 작업에서 텍스트를 이해하고, 추론하고, 생성하는 놀라운 능력을 보여주었습니다. 최근 연구에서는 LLM의 강력한 일반성을 더욱 활용하여 시각적 이해 또는 생성 작업을 개선하는데, 이를 통칭하여 멀티모달 LLM(MLLM)이라고 합니다. 예를 들어, 이전 연구[4, 5, 6, 7, 8]는 사전 학습된 이미지 인코더(예: CLIP-VIT)의 시각적 특징을 LLM의 입력 임베딩 공간에 맞춰 개방형 시각적 QA를 수행합니다. GILL[9]은 출력 임베딩 공간을 사전 학습된 안정 확산(SD) 모델[10]과 맞춰 LLM에 이미지 생성 능력을 부여합니다. 이러한 연구가 기술적 발전에 기여했지만, MLLM은 아직 새로운 역량 측면에서 LLM의 놀라운 성공을 이루지 못했습니다. 우리는 멀티모달 기능의 등장에 대한 전제가 텍스트와 이미지를 *동등한 기여*로 표현할 수 있다는 대담한 가정을 했습니다.원본 이미지 의미적으로 일관성 있는 인코딩 2D 피처 생성된 이미지 다음 단어 예측 /S 1D 인과 종속성 토큰화 토큰 해제 이산 비전 코드 대규모 언어 모델 생성 S 비전 노이즈 텍스트 비전 (a) SEED 시각적 토크나이저 (b) SEED 토큰을 사용한 멀티모달 자기 회귀 그림 1: (a) 제안된 SEED는 이산 이미지 토크나이저로, 1D 인과 종속성과 고수준 의미론을 가진 양자화된 시각적 코드를 생성합니다. (b) SEED 시각적 토큰을 사용하면 LLM이 인터리브된 이미지 텍스트 데이터를 사용하여 멀티모달 자기 회귀를 통해 시각적 이해와 생성을 모두 수행하고 통합 자기 회귀 변환기에서 상호 교환하여 처리할 수 있습니다. 다행히도 우리는 동시 작업[11, 12]에서 합의를 찾았는데, 모두 이미지-텍스트 및 텍스트-이미지 생성 작업을 사용하여 시각적 이해와 생성을 하나의 프레임워크에서 통합하는 새로운 능력을 보여주었습니다. 이산적이든 연속적이든, 훈련 패러다임은 시각적 토크나이저 훈련, 멀티모달 사전 훈련, 멀티모달 지침 튜닝의 세 단계로 요약할 수 있습니다. 동시 연구는 주로 멀티모달 훈련(나중 두 단계)을 강조하는 반면, 이 연구는 시각적 토크나이저(첫 번째 단계)에 더 중점을 둡니다. 우리는 적절한 시각적 토크나이저가 (i) 시각적 토큰과 단어 토큰 간의 의미적 정렬을 용이하게 하고 (ii) 시각적 토큰에 대한 특정 적응 없이 멀티모달 데이터에 대한 LLM의 원래 훈련 레시피(즉, 다음 단어 예측)를 활성화함으로써 후속 멀티모달 훈련을 용이하게 할 수 있다고 가정합니다. 이미지를 이산 ID 시퀀스로 표현하는 것은 자연스럽게 LLM의 자기 회귀 훈련 목표와 호환됩니다. 그러나 불행히도 다중 모달 작업에 이산화된 시각 토큰을 활용하는 작업[13, 14]은 일반적으로 이러한 모델이 수렴하기 위해 대규모 훈련에 의존하여 상당한 훈련 비용으로 이어지기 때문에 중요성에서 벗어났습니다.게다가 기존 작업에서 지배적인 토크나이저인 VQ-VAE[15]는 LLM이 다중 모달 이해 작업을 효과적으로 수행하기에는 너무 낮은 수준의 정보를 포착한다는 것을 경험적으로 발견했습니다.기존 이미지 토크나이저는 시각적 이해/생성 작업을 통합하고 다중 모달 훈련을 용이하게 하는 요구 사항을 충족하지 못합니다.이를 위해 그림 1에서 볼 수 있듯이 시각적 이해 및 생성 작업 모두에 필요한 고수준 의미론과 1차원 인과 종속성을 갖는 이산화된 시각 코드를 생성하는 VQ 기반 이미지 토크나이저인 SEED를 소개합니다.기성 LLM은 이산화된 시각 토큰을 새 단어로 처리하고 매핑된 시각 코드로 어휘를 업데이트함으로써 SEED를 쉽게 장착할 수 있습니다. 이 논문에서 우리는 사전 훈련된 LLM을 저랭크 적응(LORA)으로 조정하여 SEED 토크나이저와 효율적으로 정렬하는 MLLM을 제시합니다. 우리는 SEED의 설계 원칙을 강조하고 싶습니다. (1) 왜 인과 의존 토큰인가? 기존의 시각적 토큰(예: VQ-VAE 또는 CLIP-ViT)은 2D 컨텍스트를 사용하여 생성되는데, 이는 지배적인 LLM의 단방향 주의와 양립할 수 없고 래스터 순서 예측이 필요한 텍스트-이미지 작업에 반직관적입니다. 따라서 우리는 2D 래스터 순서 임베딩을 1D 인과 의존성을 가진 일련의 의미 코드로 변환합니다. (2) 왜 고수준 의미론인가? LLM의 시각적 및 텍스트 토큰은 상호 운용이 가능할 것으로 예상되므로(가중치와 학습 목표 공유) 정렬 오류를 방지하기 위해 동일한 수준의 의미론을 포함해야 합니다.즉, 단어에 본질적으로 존재하는 고수준 의미론입니다.* 구체적으로 SEED 토크나이저는 ViT 인코더, Causal Q-Former, VQ Codebook, Reverse Q-Former 및 UNet 디코더로 구성됩니다. ViT 인코더와 UNet 디코더는 각각 사전 학습된 BLIP-2 및 SD 모델에서 직접 파생됩니다. (1) 토큰화: Causal Q-Former는 ViT 인코더에서 생성된 2D 래스터 순서 피처를 일련의 인과적 의미 임베딩으로 변환합니다.* 토큰화 중에 고수준 의미론에 초점을 맞추는 동안 이미지 생성 작업에서 레이아웃 및 마스크 조건과 같은 정확한 공간적 구조 제어를 달성하는 것이 여전히 가능합니다. 이러한 공간적 구조적 프롬프트는 SD [10, 16]의 성공에서 입증된 것처럼 유사하게 토큰화할 수 있으며, 이는 VQ 코드북에 의해 더욱 이산화됩니다.(2) 토큰화 해제: 이산 시각 코드는 역방향 Q-Former를 통해 생성 임베딩으로 디코딩됩니다. 생성 임베딩은 SD의 잠재 공간에 맞춰져 기성품 SD-UNet을 사용하여 일관된 의미론을 가진 사실적인 이미지를 생성할 수 있습니다.SEED 학습 동안에는 인과적 Q-Former, VQ 코드북 및 역방향 Q-Former만 조정할 수 있습니다. 인과적 Q-Former는 이미지-텍스트 대조 손실로 최적화됩니다.VQ 코드북과 역방향 Q-Former는 이중 재구성, 즉 연속 인과적 임베딩과 이산 인과적 코드 간 재구성, 생성 임베딩과 쌍을 이룬 텍스트 특징 간 재구성의 목표를 향해 학습됩니다. 훈련 목표는 SEED가 시각적 이해와 생성에 필수적인 의미론을 캡슐화하도록 보장합니다. 정량적 결과에 따르면 개별 SEED 토큰은 BLIP-2에 비해 텍스트-이미지 검색에서 경쟁력 있는 성능을 보이며, 안정적 확산에 비해 이미지 생성에서 경쟁력 있는 성능을 보입니다. 추가적인 멀티모달 자기 회귀 훈련을 통해 SEED-OPT2.7B(5M 이미지-텍스트 쌍을 사용하여 LoRA를 통해 효율적으로 조정)는 이미지-텍스트 및 텍스트-이미지 작업을 효과적으로 수행하여 제로샷 이미지 캡션 및 시각적 QA에서 유망한 결과를 얻고 고품질 이미지를 생성합니다. 이러한 노력은 개별 시각적 토큰을 사용하여 LLM 내에서 멀티모달 이해 및 생성 작업을 통합하는 것을 목표로 합니다. 적절한 토크나이저 설계에 대한 초기 탐색은 새로운 멀티모달 기능 개발을 촉진하는 데 주력합니다. 향후 작업에서는 더 나은 토크나이저에 대한 훈련을 더욱 확장하고 포괄적인 멀티모달 사전 훈련 및 지침 조정을 위해 더 강력한 LLM(예: LLaMA[1])을 활용할 수 있습니다. 2 SEED Visual Tokenizer 2.1 Baseline Tokenizers의 파일럿 실험 Visual Tokenizer는 이미지를 이산 토큰의 시퀀스로 표현하는 것을 목표로 합니다. 이전 연구[15, 13, 17]는 이미지 픽셀을 재구성하여 벡터 양자화 변분 자동 인코더(VQ-VAE)를 훈련하는 반면, Beit v2[18]는 교사 모델에서 고수준 기능을 재구성하여 시각적 토크나이저를 훈련하기 위해 벡터 양자화 지식 증류(VQ-KD)를 제안합니다. 우리는 CC3M[20] 데이터 세트에서 VQ-VAE와 Beit v2의 이산 표현을 각각 OPT 2.7B[19] 모델과 맞추기 위해 두 가지 실험을 수행합니다. 우리는 COCO[21]에서 제로 샷 이미지 캡션으로 성능을 평가합니다. VQ-VAE는 CIDER 34.0을 달성하는 반면 Beit v2는 42.0을 달성합니다. 실험 결과는 저수준 이미지 세부 정보 대신 이미지의 의미적 표현을 캡처하는 고수준 시각적 토크나이저가 멀티모달 이해에 더 효과적임을 보여줍니다.2.2 아키텍처 이 연구에서는 1D 인과 종속성과 고수준 의미론을 가진 이산적인 시각적 코드를 생성하는 VQ 기반 이미지 토크나이저 SEED를 소개합니다.특히, 그림 2에서 볼 수 있듯이 SEED 토크나이저는 ViT 이미지 인코더[22], 인과적 Q-Former, VQ 코드북, 역방향 Q-Former 및 UNet 디코더[10]로 구성됩니다.ViT 인코더와 UNet 디코더는 각각 사전 학습된 BLIP-2 및 SD 모델에서 직접 파생됩니다.먼저 ViT 인코더에서 생성된 2D 래스터 순서 피처(16×16 토큰)를 일련의 인과적 의미 임베딩(32개 토큰)으로 변환하기 위해 인과적 Q-Former를 먼저 학습합니다. 그런 다음 시각적 코드북을 훈련하여 인과적 임베딩을 인과적 종속성을 가진 양자화된 시각적 코드(32개 토큰)로 이산화합니다.역 Q-Former를 사용하여 시각적 코드를 생성 임베딩(77개 토큰)으로 디코딩하고, 이는 사전 훈련된 안정적 확산(SD) 모델의 잠재 공간에 맞춰집니다.2.2.1 훈련 단계 I: 인과적 Q-Former 그림 2에서 볼 수 있듯이, 학습 가능한 쿼리 임베딩(32개 토큰)의 집합과 사전 훈련된 ViT 이미지 인코더의 피처가 인과적 Q-Former에 공급되어 입력 이미지의 고정된 수의 인과적 임베딩(32개 토큰)을 인코딩합니다.특히, 쿼리 임베딩은 인과 마스크가 있는 셀프 어텐션 계층을 통해 이전 쿼리와만 상호 작용할 수 있으며, 교차 어텐션 계층을 통해 동결된 이미지 피처와 상호 작용할 수 있습니다. 우리는 CC3M [20], Unsplash [23], COCO 데이터 세트 [21]를 포함한 5M 이미지-텍스트 쌍에서 사전 학습된 BLIP-2 Q-Former에서 미세 조정된 Causal Qformer를 최적화하기 위해 대조 학습을 채택합니다. 우리는 대조 손실을 사용하여 다음 간의 유사성을 최대화합니다.Original Image Generated Image SD Decoder SEED De-Tokenize Generation Embeddings Reconstruct Reverse Q-Former 52-Learned Queries Causal Codes Reconstruct Codebook Contrastive Text Embeddings Causal Embeddings Causal Q-Former ViT Encoder Learned Queries SEED Tokenize Text Encoder ↑ &quot;개가 풀밭에 앉아 있습니다&quot; 그림 2: 인과 종속성과 고수준 의미론을 갖춘 이산적인 시각 코드를 생성하는 SEED 토크나이저 개요.표 1: 제로샷 이미지-텍스트 검색 평가. 인과 코드는 양자화된 인과 임베딩입니다. 모델 R@BLIP-2 [5] SEED (인과 emb) SEED (인과 코드) Flickr30K (1K 테스트 세트) 이미지 → 텍스트 텍스트 → 이미지 R@5 R@ 10 R@1 R@5 R@81.9 98.4 99.7 82.4 96.5 98.90.0 99.6 99.9 80.0 95.3 97.86.3 98.6 99.5 75.9 93.2 96.COCO (5K 테스트 세트) R@mean 92.93.91.65.7 88.93.이미지 → 텍스트 텍스트 → 이미지 R@1 R@5 R@ 10 R@1 R@5 R@65.3 89.9 95.3 59.1 82.7 89.71.9 91.1 95.9 56.7 80.7 87.52.5 78.0 86.R@mean 80.80.77.배치 내 다른 캡션의 최종 인과 임베딩과 텍스트 특징 간의 유사성을 최소화하면서 최종 인과 임베딩과 해당 캡션의 텍스트 특징을 최적화합니다.인과 임베딩 평가.BLIP-2에 따라 COCO [21]와 Flickr30K [24] 데이터 세트를 사용하여 제로 샷 이미지-텍스트 검색 작업에서 Causal Q-Former의 성능을 평가합니다.성능은 이미지-텍스트 검색과 텍스트-이미지 검색 모두에 대한 Recall@K(R@K)로 측정합니다.추론을 위해 듀얼 스트림 패러다임을 채택하고 공정한 비교를 위해 BLIP-2에서 image-txtmatching(ITM) 재순위 모듈을 제거합니다.표에서 볼 수 있듯이. 1, 우리의 인과적 Q-former는 집계된 메트릭 Recall@mean의 관점에서 BLIP-2보다 더 나은 결과를 달성합니다. 그것은 인과적 종속성이 있는 출력 쿼리 임베딩이 BLIP-2에서 양방향 어텐션이 있는 출력 임베딩보다 성능을 떨어뜨리지 않는다는 것을 보여줍니다. 2.2.2 훈련 단계 II: 시각적 양자화 및 토큰화 해제 그림 2에서 볼 수 있듯이, 우리는 CC3M, Unsplash 및 COCO 데이터 세트를 포함하는 5M 이미지-텍스트 쌍에서 인과적 임베딩(32개 토큰)을 양자화된 시각적 코드(32개 토큰)로 이산화하기 위해 VQ 코드북을 훈련합니다. 구체적으로, 양자화기는 각 인과적 임베딩에 대한 코드북에서 가장 가까운 이웃을 찾고 해당 코드를 얻습니다. 우리는 다층 Transformer[22]인 디코더를 사용하여 이산 코드에서 연속적인 인과적 임베딩을 재구성합니다. 학습하는 동안 디코더의 출력과 인과 임베딩 간의 코사인 유사도를 최대화합니다. 또한 역 Q-Former를 사용하여 불연속 코드에서 동결된 안정 확산 모델의 텍스트적 특징을 재구성합니다. 학습 가능한 쿼리 임베딩(77개 토큰)의 설정된 수가 역 Q-Former에 입력됩니다. 쿼리 임베딩은 셀프 어텐션 계층을 통해 서로 상호 작용하고, 출력 생성 임베딩(77개 토큰)을 위해 교차 어텐션 계층을 통해 인과 코드(32개 토큰)와 상호 작용합니다. 학습하는 동안 입력 재구성 입력 재구성 그림 3: SEED 토크나이저의 재구성 이미지(즉, 원본 이미지 → SEED 토큰화 → 인과적 시각 코드 → SEED 토큰 해제 → 재구성된 이미지)와 SD의 텍스트 특징 간의 MSE 손실을 최소화합니다. 추론 중에 생성 임베딩을 SD-UNet에 공급하여 현실적인 이미지를 디코딩할 수 있습니다.표 2: CLIP 유사도를 메트릭으로 한 이미지 생성 평가.COCO Flickr30K의 인과 코드 평가.인과 코드에서 재구성된 인과 임베딩을 검색에 사용하여 제로 샷 이미지 텍스트 검색에서 SEED 토크나이저의 성능을 평가합니다.표 1에서 볼 수 있듯이, 이산 SEED 토큰은 BLIP-2에 비해 경쟁력 있는 성능을 보입니다.또한 COCO 및 Flickr30K 데이터 세트에서 이미지 생성을 평가합니다.SEED는 먼저 입력 이미지를 인과 코드(토큰 32개)로 이산화하고 Reverse Q-Former에서 생성 임베딩(토큰 77개)을 얻은 다음 이를 재구성된 이미지에 대한 SD-UNet에 공급합니다.기준 모델 GILL[25] 및 SD[10]의 경우 입력 이미지의 해당 캡션에서 이미지를 생성합니다. 우리는 의미적 일관성을 벤치마킹하기 위한 평가 척도로 CLIP 유사도를 계산하기 위해 GILL[25]을 따릅니다. 표 2에서 볼 수 있듯이, 상한 SD와 비교할 때, 우리의 SEED는 성능이 약간 떨어질 뿐이며 이미지 생성에서 GILL보다 더 뛰어납니다. 모델 GILL [9] SD [10] 67.65.68.65.68.65.SEED이미지-텍스트 쌍 &quot;개가 풀밭에 앉아 있습니다&quot; SEED 토큰화LLM 토큰화개가 풀밭에 앉아 있는 사진 이미지-텍스트 자기회귀 텍스트-이미지 자기회귀 IMG N+5 N+2 N+3 N+1 N+7 /IMG SOS개가 풀밭에 앉아 있는 이미지 생성 SOS 15 11 12 1 42965 EOS +142965 EOS IMG N+5 N+2 N+3 N+1 N+7 IMG 그림 4: 효율적인 LORA 튜닝을 사용하여 SEED-OPT2.7B에 대한 다중 모드 자기회귀 훈련 개요. 64개의 V100 GPU와 5M개의 이미지 캡션 쌍만 사용하여 44시간 만에 훈련되었습니다. 재구성된 이미지의 시각화. 그림 3에서 SEED의 재구성된 이미지를 시각화합니다. 입력 이미지의 인과적 시각 코드에서 생성 임베딩을 얻기 위해 역 Q-Former를 활용함으로써 기성품 SD-UNet을 사용하여 입력 이미지와 일관된 의미론을 유지하는 사실적인 이미지를 생성할 수 있습니다. 위의 평가 및 시각화는 이해 및 생성 작업 모두에 대한 SEED 시각 토큰의 다재다능함을 보여줍니다. 3 SEED 시각 토큰을 사용한 다중 모달 자기 회귀 사전 학습된 SEED 토크나이저를 기반으로 CC3M, Unsplash 및 COCO 데이터 세트를 포함한 5M 이미지-텍스트 쌍이 있는 OPT2.7B [19] 모델에서 저랭크 적응(LORA) 모듈을 미세 조정하여 SEED-OPT2.7B를 제시합니다. 그림 4에서 볼 수 있듯이 통합된 다중 모달 이해 및 생성을 위해 이미지-텍스트 및 텍스트-이미지 자기 회귀 사전 학습을 수행합니다. 이미지-텍스트 자기 회귀. 우리는 먼저 이미지-텍스트 자동 회귀를 수행하여 사전 훈련된 VQ 코드북의 어휘를 OPT2.7B와 정렬합니다. 구체적으로, 우리는 완전 연결(FC) 계층을 사용하여 시각적 토크나이저의 인과 코드를 OPT2.7B의 단어 임베딩과 동일한 차원으로 선형적으로 투영합니다. 투영된 인과 코드와 접두사 &quot;A photo of&quot;의 단어 임베딩은 OPT2.7B의 입력으로 연결됩니다. 해당 캡션의 텍스트 토큰은 생성 대상으로 사용됩니다. OPT2.7B를 동결하고 다음 텍스트 토큰을 예측하는 훈련 목표로 LoRA를 미세 조정합니다. 텍스트-이미지 자기 회귀. 그런 다음 이미지-텍스트 및 텍스트-이미지 자기 회귀를 함께 수행하여 LLM이 텍스트 토큰 외에도 비전 토큰을 생성할 수 있는 기능을 제공합니다. 텍스트-이미지 자기 회귀 사전 학습의 경우 접두사 &quot;Generate an image&quot;와 캡션의 단어 임베딩이 OPT2.7B에 입력됩니다. 사전 학습된 토크나이저의 해당 이미지의 시각적 코드는 생성 대상으로 사용됩니다. OPT2.7B를 동결하고 다음 비전 토큰을 예측하는 훈련 목표로 LoRA를 미세 조정합니다. 추론하는 동안 프롬프트 &quot;Generate an image&quot;와 텍스트 설명, SEED-OPT2.7B는 시각적 토큰을 자기 회귀적으로 예측합니다. 출력 시각적 토큰은 생성 임베딩을 위해 역 Q-Former에 공급되며, 이는 SD-UNet을 통해 사실적인 이미지를 생성하기 위해 디코딩될 수 있습니다. 멀티모달 이해 평가. 우리는 제로 샷 이미지 캡션 및 시각적 질의 응답(vqa)을 사용하여 SEED-OPT2.7B의 성능을 평가합니다. 이미지 캡션의 경우 COCO[21] 테스트 세트와 NoCaps[26] 검증 세트에서 평가하고 프롬프트 &quot;a photo of&quot;와 함께 BLEU@K(B@K), METEOR(M), ROUGEL(R), CIDER(C), SPICE(S)를 보고합니다. 시각적 질의 응답의 경우 VQAv2[27] 검증 세트와 GQA[28] 테스트 세트에서 평가하고 프롬프트 &quot;Question: {} Short answer&quot;와 함께 Top-accuracy를 보고합니다. 표에 표시된 대로. 3, 1억 2,900만 개의 이미지-텍스트 쌍으로 학습된 BLIP-2와 비교했을 때, 5백만 개의 쌍으로 학습된 SEED-OPT 2.7B는 SEED 이산 시각 토큰을 사용한 제로샷 이미지 캡션 및 시각적 질의 응답에서 유망한 결과를 달성합니다. 이미지 캡션을 사용하는 동시 작업 CM3Leon[12]과 다른 점에 유의하세요.표 3: 제로샷 이미지 캡션 및 시각적 질의 응답에서 1억 2,900만 개의 이미지-텍스트 쌍으로 사전 학습된 BLIP-2와 SEED-OPT2.7B(5백만 쌍)의 비교. S: SPICE, M: METEOR, R: ROUGEL, B: BLEU, C: CIDER. NoCaps 모델 전체 SS BLIP-2 OPT2.7B [5] SEED-OPT2.7B 14.4 13.12.12.S 13.4 13.12.2 12.SB@4 M COCO 카르파티 검정 R VQAV2 GQA 39.7 28.9 59.34.6 28.4 56.C 131.0 22.119.0 22.S 상위 1 상위 5 1.32.42.28.11:a 코끼리 등을 타고 있는 사람들의 그룹 서핑보드를 들고 해변에 서 있는 여성 잠자리가 컴퓨터 위에 앉아 있음 수박 한 조각을 먹고 있는 어린 소녀 FREY A: 건물 옆에 Q: 노인 옆에 무엇이 있습니까? Q: 그들은 무엇을 들고 있습니까? A: 공원 벤치에 앉아 있는 흰 개 A: 여성 사진이 있는 케이크 Q: 사람들이 참여하는 이벤트의 종류는? A: 와인 시음 Q: 시계는 어디에 있나요? 그림 5: 이미지 캡션(&quot;사진&quot; 프롬프트 포함)과 개방형 시각적 질문 답변에 대한 SEED-OPT2.7B의 정성적 예. 이 모델은 어떤 VQA 데이터 세트에서도 학습되지 않았습니다. 지도 미세 조정을 위한 VQA 데이터 세트와 &quot;사진&quot; 접두사를 사용하여 이미지-텍스트 자동 회귀로 사전 학습된 SEED-OPT2.7B는 자유형 질문을 이해하고 개방형 답변을 예측하여 제로샷 시각적 질문 답변을 수행할 수 있습니다. 또한 이미지 캡션(프롬프트 &quot;a photo of&quot;)과 vqa에 대한 SEED-OPT‍2.7B의 정성적 예를 보여줍니다. 그림 5에서 보듯이, 저희 모델은 시각적 콘텐츠를 설명하는 캡션을 생성하고 다양한 질문에 답할 수 있습니다. 멀티모달 생성 평가. 그림 6에서 SEED-OPT2.7B를 사용하여 텍스트-이미지 생성 결과의 정성적 예를 보여줍니다. 텍스트 설명이 주어지면 SEED-OPT2.7B는 설명과 의미적으로 관련이 있는 사실적인 이미지를 생성할 수 있습니다. SEED는 시각적 토큰과 LLM 간의 정렬을 용이하게 할 수 있으며, 이는 LoRA 튜닝 후 텍스트-이미지 및 이미지-텍스트 생성 작업을 수행할 수 있는 SEED-OPT 2.7B에서 입증됩니다. 4
--- EXPERIMENT ---
Baseline Tokenizers의 s 시각적 토크나이저는 이미지를 이산 토큰의 시퀀스로 표현하는 것을 목표로 합니다. 이전 작업 [15, 13, 17]은 이미지 픽셀을 재구성하여 벡터 양자화 변분 자동 인코더(VQ-VAE)를 훈련하는 반면, Beit v2 [18]는 교사 모델에서 고수준 기능을 재구성하여 시각적 토크나이저를 훈련하기 위해 벡터 양자화 지식 증류(VQ-KD)를 제안합니다. 우리는 CC3M [20] 데이터 세트에서 VQ-VAE와 Beit v2의 이산 표현을 각각 OPT 2.7B [19] 모델과 맞추기 위해 두 가지 실험을 수행합니다. 우리는 COCO [21]에서 제로 샷 이미지 캡션으로 성능을 평가합니다. VQ-VAE는 CIDER 34.0을 달성하는 반면 Beit v2는 42.0을 달성합니다. 실험 결과는 저수준 이미지 세부 정보 대신 이미지의 의미적 표현을 캡처하는 고수준 시각적 토크나이저가 멀티모달 이해에 더 효과적임을 보여줍니다.2.2 아키텍처 이 연구에서는 1D 인과 종속성과 고수준 의미론을 가진 이산적인 시각적 코드를 생성하는 VQ 기반 이미지 토크나이저 SEED를 소개합니다.특히, 그림 2에서 볼 수 있듯이 SEED 토크나이저는 ViT 이미지 인코더[22], 인과적 Q-Former, VQ 코드북, 역방향 Q-Former 및 UNet 디코더[10]로 구성됩니다.ViT 인코더와 UNet 디코더는 각각 사전 학습된 BLIP-2 및 SD 모델에서 직접 파생됩니다.먼저 ViT 인코더에서 생성된 2D 래스터 순서 피처(16×16 토큰)를 일련의 인과적 의미 임베딩(32개 토큰)으로 변환하기 위해 인과적 Q-Former를 먼저 학습합니다. 그런 다음 시각적 코드북을 훈련하여 인과적 임베딩을 인과적 종속성을 가진 양자화된 시각적 코드(32개 토큰)로 이산화합니다.역 Q-Former를 사용하여 시각적 코드를 생성 임베딩(77개 토큰)으로 디코딩하고, 이는 사전 훈련된 안정적 확산(SD) 모델의 잠재 공간에 맞춰집니다.2.2.1 훈련 단계 I: 인과적 Q-Former 그림 2에서 볼 수 있듯이, 학습 가능한 쿼리 임베딩(32개 토큰)의 집합과 사전 훈련된 ViT 이미지 인코더의 피처가 인과적 Q-Former에 공급되어 입력 이미지의 고정된 수의 인과적 임베딩(32개 토큰)을 인코딩합니다.특히, 쿼리 임베딩은 인과 마스크가 있는 셀프 어텐션 계층을 통해 이전 쿼리와만 상호 작용할 수 있으며, 교차 어텐션 계층을 통해 동결된 이미지 피처와 상호 작용할 수 있습니다. 우리는 CC3M [20], Unsplash [23], COCO 데이터 세트 [21]를 포함한 5M 이미지-텍스트 쌍에서 사전 학습된 BLIP-2 Q-Former에서 미세 조정된 Causal Qformer를 최적화하기 위해 대조 학습을 채택합니다. 우리는 대조 손실을 사용하여 다음 간의 유사성을 최대화합니다.Original Image Generated Image SD Decoder SEED De-Tokenize Generation Embeddings Reconstruct Reverse Q-Former 52-Learned Queries Causal Codes Reconstruct Codebook Contrastive Text Embeddings Causal Embeddings Causal Q-Former ViT Encoder Learned Queries SEED Tokenize Text Encoder ↑ &quot;개가 풀밭에 앉아 있습니다&quot; 그림 2: 인과 종속성과 고수준 의미론을 갖춘 이산적인 시각 코드를 생성하는 SEED 토크나이저 개요.표 1: 제로샷 이미지-텍스트 검색 평가. 인과 코드는 양자화된 인과 임베딩입니다. 모델 R@BLIP-2 [5] SEED(인과성 emb) SEED(인과성 코드) Flickr30K(1K 검정 세트) 이미지 → 텍스트 텍스트 → 이미지 R@5 R@ 10 R@1 R@5 R@81.9 98.4 99.7 82.4 96.5 98.90.0 99.6 99.9 80.0 95.3 97.86.3 98.6 99.5 75.9 93.2 96.COCO(5K 검정 세트) R@mean 92.93.91.65.7 88.93.이미지 → 텍스트 텍스트 → 이미지 R@1 R@5 R@ 10 R@1 R@5 R@65.3 89.9 95.3 59.1 82.7 89.71.9 91.1 95.9 56.7 80.7 87.52.5 78.0 86.R@mean 80.80.77.배치 내 다른 캡션의 최종 인과 임베딩과 텍스트 특징 간의 유사성을 최소화하면서 최종 인과 임베딩과 해당 캡션의 텍스트 특징을 최적화합니다.인과 임베딩 평가.BLIP-2에 따라 COCO [21]와 Flickr30K [24] 데이터 세트를 사용하여 제로 샷 이미지-텍스트 검색 작업에서 Causal Q-Former의 성능을 평가합니다.성능은 이미지-텍스트 검색과 텍스트-이미지 검색 모두에 대한 Recall@K(R@K)로 측정합니다.추론을 위해 듀얼 스트림 패러다임을 채택하고 공정한 비교를 위해 BLIP-2에서 image-txtmatching(ITM) 재순위 모듈을 제거합니다.표에서 볼 수 있듯이. 1, 우리의 인과적 Q-former는 집계된 메트릭 Recall@mean의 관점에서 BLIP-2보다 더 나은 결과를 달성합니다. 그것은 인과적 종속성이 있는 출력 쿼리 임베딩이 BLIP-2에서 양방향 어텐션이 있는 출력 임베딩보다 성능을 떨어뜨리지 않는다는 것을 보여줍니다. 2.2.2 훈련 단계 II: 시각적 양자화 및 토큰화 해제 그림 2에서 볼 수 있듯이, 우리는 CC3M, Unsplash 및 COCO 데이터 세트를 포함하는 5M 이미지-텍스트 쌍에서 인과적 임베딩(32개 토큰)을 양자화된 시각적 코드(32개 토큰)로 이산화하기 위해 VQ 코드북을 훈련합니다. 구체적으로, 양자화기는 각 인과적 임베딩에 대한 코드북에서 가장 가까운 이웃을 찾고 해당 코드를 얻습니다. 우리는 다층 Transformer[22]인 디코더를 사용하여 이산 코드에서 연속적인 인과적 임베딩을 재구성합니다. 학습하는 동안 디코더의 출력과 인과 임베딩 간의 코사인 유사도를 최대화합니다. 또한 역 Q-Former를 사용하여 불연속 코드에서 동결된 안정 확산 모델의 텍스트적 특징을 재구성합니다. 학습 가능한 쿼리 임베딩(77개 토큰)의 설정된 수가 역 Q-Former에 입력됩니다. 쿼리 임베딩은 셀프 어텐션 계층을 통해 서로 상호 작용하고, 출력 생성 임베딩(77개 토큰)을 위해 교차 어텐션 계층을 통해 인과 코드(32개 토큰)와 상호 작용합니다. 학습하는 동안 입력 재구성 입력 재구성 그림 3: SEED 토크나이저의 재구성 이미지(즉, 원본 이미지 → SEED 토큰화 → 인과적 시각 코드 → SEED 토큰 해제 → 재구성된 이미지)와 SD의 텍스트 특징 간의 MSE 손실을 최소화합니다. 추론 중에 생성 임베딩을 SD-UNet에 공급하여 현실적인 이미지를 디코딩할 수 있습니다.표 2: CLIP 유사도를 메트릭으로 한 이미지 생성 평가.COCO Flickr30K의 인과 코드 평가.인과 코드에서 재구성된 인과 임베딩을 검색에 사용하여 제로 샷 이미지 텍스트 검색에서 SEED 토크나이저의 성능을 평가합니다.표 1에서 볼 수 있듯이, 이산 SEED 토큰은 BLIP-2에 비해 경쟁력 있는 성능을 보입니다.또한 COCO 및 Flickr30K 데이터 세트에서 이미지 생성을 평가합니다.SEED는 먼저 입력 이미지를 인과 코드(토큰 32개)로 이산화하고 Reverse Q-Former에서 생성 임베딩(토큰 77개)을 얻은 다음 이를 재구성된 이미지에 대한 SD-UNet에 공급합니다.기준 모델 GILL[25] 및 SD[10]의 경우 입력 이미지의 해당 캡션에서 이미지를 생성합니다. 우리는 의미적 일관성을 벤치마킹하기 위한 평가 척도로 CLIP 유사도를 계산하기 위해 GILL[25]을 따릅니다. 표 2에서 볼 수 있듯이, 상한 SD와 비교할 때, 우리의 SEED는 성능이 약간 떨어질 뿐이며 이미지 생성에서 GILL보다 더 뛰어납니다. 모델 GILL [9] SD [10] 67.65.68.65.68.65.SEED이미지-텍스트 쌍 &quot;개가 풀밭에 앉아 있습니다&quot; SEED 토큰화LLM 토큰화개가 풀밭에 앉아 있는 사진 이미지-텍스트 자기회귀 텍스트-이미지 자기회귀 IMG N+5 N+2 N+3 N+1 N+7 /IMG SOS개가 풀밭에 앉아 있는 이미지 생성 SOS 15 11 12 1 42965 EOS +142965 EOS IMG N+5 N+2 N+3 N+1 N+7 IMG 그림 4: 효율적인 LORA 튜닝을 사용하여 SEED-OPT2.7B에 대한 다중 모드 자기회귀 훈련 개요. 64개의 V100 GPU와 5M개의 이미지 캡션 쌍만 사용하여 44시간 만에 훈련되었습니다. 재구성된 이미지의 시각화. 그림 3에서 SEED의 재구성된 이미지를 시각화합니다. 입력 이미지의 인과적 시각 코드에서 생성 임베딩을 얻기 위해 역 Q-Former를 활용함으로써 기성품 SD-UNet을 사용하여 입력 이미지와 일관된 의미론을 유지하는 사실적인 이미지를 생성할 수 있습니다. 위의 평가 및 시각화는 이해 및 생성 작업 모두에 대한 SEED 시각 토큰의 다재다능함을 보여줍니다. 3 SEED 시각 토큰을 사용한 다중 모달 자기 회귀 사전 학습된 SEED 토크나이저를 기반으로 CC3M, Unsplash 및 COCO 데이터 세트를 포함한 5M 이미지-텍스트 쌍이 있는 OPT2.7B [19] 모델에서 저랭크 적응(LORA) 모듈을 미세 조정하여 SEED-OPT2.7B를 제시합니다. 그림 4에서 볼 수 있듯이 통합된 다중 모달 이해 및 생성을 위해 이미지-텍스트 및 텍스트-이미지 자기 회귀 사전 학습을 수행합니다. 이미지-텍스트 자기 회귀. 우리는 먼저 이미지-텍스트 자동 회귀를 수행하여 사전 훈련된 VQ 코드북의 어휘를 OPT2.7B와 정렬합니다. 구체적으로, 우리는 완전 연결(FC) 계층을 사용하여 시각적 토크나이저의 인과 코드를 OPT2.7B의 단어 임베딩과 동일한 차원으로 선형적으로 투영합니다. 투영된 인과 코드와 접두사 &quot;A photo of&quot;의 단어 임베딩은 OPT2.7B의 입력으로 연결됩니다. 해당 캡션의 텍스트 토큰은 생성 대상으로 사용됩니다. OPT2.7B를 동결하고 다음 텍스트 토큰을 예측하는 훈련 목표로 LoRA를 미세 조정합니다. 텍스트-이미지 자기 회귀. 그런 다음 이미지-텍스트 및 텍스트-이미지 자기 회귀를 함께 수행하여 LLM이 텍스트 토큰 외에도 비전 토큰을 생성할 수 있는 기능을 제공합니다. 텍스트-이미지 자기 회귀 사전 학습의 경우 접두사 &quot;Generate an image&quot;와 캡션의 단어 임베딩이 OPT2.7B에 입력됩니다. 사전 학습된 토크나이저의 해당 이미지의 시각적 코드가 생성 대상으로 사용됩니다. OPT2.7B를 동결하고 다음 비전 토큰을 예측하는 훈련 목표로 LoRA를 미세 조정합니다. 추론하는 동안 &quot;Generate an image&quot;라는 프롬프트와 텍스트 설명, SEED-OPT2.7B는 시각적 토큰을 자기 회귀적으로 예측합니다. 출력 시각적 토큰은 생성 임베딩을 위해 역 Q-Former에 공급되며, 이는 SD-UNet을 통해 사실적인 이미지를 생성하기 위해 디코딩될 수 있습니다. 멀티모달 이해 평가. 우리는 제로 샷 이미지 캡션 및 시각적 질의 응답(vqa)을 사용하여 SEED-OPT2.7B의 성능을 평가합니다. 이미지 캡션의 경우 COCO[21] 테스트 세트와 NoCaps[26] 검증 세트에서 평가하고 프롬프트 &quot;a photo of&quot;와 함께 BLEU@K(B@K), METEOR(M), ROUGEL(R), CIDER(C), SPICE(S)를 보고합니다. 시각적 질의 응답의 경우 VQAv2[27] 검증 세트와 GQA[28] 테스트 세트에서 평가하고 프롬프트 &quot;Question: {} Short answer&quot;와 함께 Top-accuracy를 보고합니다. 표에 표시된 대로. 3, 1억 2,900만 개의 이미지-텍스트 쌍으로 학습된 BLIP-2와 비교했을 때, 5백만 개의 쌍으로 학습된 SEED-OPT 2.7B는 SEED 개별 시각 토큰을 사용한 제로샷 이미지 캡션 및 시각적 질의 응답에서 유망한 결과를 달성합니다. 이미지 캡션을 사용하는 동시 작업 CM3Leon[12]과 다른 점에 유의하세요.표 3: 제로샷 이미지 캡션 및 시각적 질의 응답에서 1억 2,900만 개의 이미지-텍스트 쌍으로 사전 학습된 BLIP-2와 SEED-OPT2.7B(5백만 쌍)의 비교. S: SPICE, M: METEOR, R: ROUGEL, B: BLEU, C: CIDER. NoCaps 모델 전체 SS BLIP-2 OPT2.7B [5] SEED-OPT2.7B 14.4 13.12.12.S 13.4 13.12.2 12.SB@4 M COCO 카르파티 검정 R VQAV2 GQA 39.7 28.9 59.34.6 28.4 56.C 131.0 22.119.0 22.S 상위 1 상위 5 1.32.42.28.11:a 코끼리 등을 타고 있는 사람들의 그룹 서핑보드를 들고 해변에 서 있는 여성 잠자리가 컴퓨터 위에 앉아 있음 수박 한 조각을 먹고 있는 어린 소녀 FREY A: 건물 옆에 Q: 노인 옆에 무엇이 있습니까? Q: 그들은 무엇을 들고 있습니까? A: 공원 벤치에 앉아 있는 흰 개 A: 여성 사진이 있는 케이크 Q: 사람들이 참여하는 이벤트의 종류는? A: 와인 시음 Q: 시계는 어디에 있나요? 그림 5: 이미지 캡션(&quot;사진&quot; 프롬프트 포함)과 개방형 시각적 질문 답변에 대한 SEED-OPT2.7B의 정성적 예. 이 모델은 어떤 VQA 데이터 세트에서도 학습되지 않았습니다. 및 지도 미세 조정을 위한 vqa 데이터 세트와 함께, &quot;사진&quot; 접두사를 사용하여 이미지-텍스트 자동 회귀로 사전 학습된 SEED-OPT2.7B는 자유형 질문을 이해하고 개방형 답변을 예측하여 제로샷 시각적 질문 답변을 수행할 수 있습니다. 또한 이미지 캡션(프롬프트 &quot;사진&quot;)과 vqa에 대한 SEED-OPT‍2.7B의 정성적 예를 보여줍니다. 그림 5에서 보듯이, 우리 모델은 시각적 콘텐츠를 설명하는 캡션을 생성하고 다양한 질문에 답할 수 있습니다. 다중 모달 생성 평가. 그림 6에서 SEED-OPT2.7B를 사용하여 텍스트-이미지 생성 결과의 정성적 예를 보여줍니다. 텍스트 설명이 주어지면 SEED-OPT2.7B는 설명과 의미적으로 관련이 있는 사실적인 이미지를 생성할 수 있습니다. SEED는 시각적 토큰과 LLM 간의 정렬을 용이하게 할 수 있으며, 이는 SEED-OPT 2.7B에서 알 수 있듯이 LoRA 튜닝 후 텍스트-이미지 및 이미지-텍스트 생성 작업을 수행할 수 있습니다. 4 관련 작업 이해를 위한 다중 모달 대규모 언어 모델. 대규모 언어 모델[1, 2, 3](LLM)의 인상적인 성공으로 최근 연구에서는 시각적 LLM의 강력한 일반성을 활용하여 이해. 이전 연구[4, 5, 6, 29, 7, 8, 30, 31]는 사전 훈련된 이미지 인코더의 시각적 특징을 이미지-텍스트 데이터 세트의 LLM과 정렬하고, LLM이 텍스트 설명으로 시각적 정보를 해석할 수 있는 기능을 제공합니다. 그러나 이러한 작업은 일반적으로 다음 텍스트 토큰의 예측을 훈련 목표로 사용하고 시각 데이터에 대한 감독을 하지 않으므로 다중 모드 시각 및 언어 입력이 제공된 텍스트만 출력할 수 있습니다.화창한 하늘 아래의 눈산 푸른 풀밭에 개가 앉아 있습니다 선로 위를 달리는 기차 열린 들판에서 타오르는 불 새가 맨 나무 위에 앉아 있습니다 푸른 식물이 많은 방 수영장이 있는 큰 집 바위 절벽 옆의 해변 건물 앞에 주차된 노란색 차 고양이가 소파에 누워 있습니다. 밤에 도시에서 펼쳐지는 불꽃놀이 거리에서 자전거를 타는 소년 흰 드레스를 입은 여성 숲 아래 벽 앞에 폭포가 흐르고 서 있음 나무 테이블 위에 놓인 커피 한 잔 꽃병에 꽂힌 녹색 잎이 있는 노란 꽃 그림 6: SEED-OPT2.7B 생성을 위한 다중 모달 대규모 언어 모델을 사용하여 추론 시 텍스트-이미지 생성 결과. LLM에 이미지 생성 기능을 제공하기 위해 CogView[14]는 이미지 픽셀을 재구성하여 시각적 토크나이저를 사전 학습하고, 이미지와 텍스트 토큰을 동등하게 처리하는 다음 토큰 예측을 목표로 GPT 모델[2, 32]을 미세 조정합니다. GILL[25]은 LLM의 임베딩과 동결된 사전 학습된 이미지 생성 모델 간의 매핑을 학습합니다. 두 작업 모두 다중 모달 이해를 위해 명시적으로 설계되지는 않았지만 LLM이 있는 이미지를 생성하는 것을 목표로 합니다. 시각적 토크나이저. 시각적 토크나이저는 이미지를 자연어와 유사한 이산 토큰의 시퀀스로 표현하는 것을 목표로 합니다. 이전 작업[15, 13, 17]은 색상, 질감, 모서리와 같은 이미지의 저수준 세부 정보만 캡처하는 입력 이미지의 픽셀을 재구성하여 벡터 양자화 변형 자동 인코더(VQ-VAE)를 시각적 토크나이저로 훈련합니다. Beit v2[18]는 교사 모델에서 고수준 기능을 재구성하여 의미가 풍부한 시각적 토크나이저를 훈련하지만, 비전 변환기[22]의 2D 기능에서 얻은 시각적 코드는 다중 모달 생성을 위한 지배적인 LLM의 단방향 주의와 호환되지 않습니다.5
--- CONCLUSION ---
우리는 LLM과 호환되는 시각적 토큰이 1D 인과 종속성으로 생성되는 동안 고수준 의미론을 포착해야 한다는 전제에 따라 설계된 이산 이미지 토크나이저인 SEED를 제시합니다. SEED를 사용하면 LLM이 성숙하고 확장 가능한 텍스트의 원래 레시피(즉, 다음 단어 예측)에 따라 멀티모달 데이터로 훈련될 수 있습니다. 훈련된 멀티모달 LLM은 이미지-텍스트 및 텍스트-이미지 생성 작업을 모두 수행할 수 있어 새로운 멀티모달 기능을 향한 한 걸음 더 나아갑니다. 우리는 SEED가 시각적 토크나이저에 더 많은 관심을 끌기를 바랍니다. 보다 합리적인 시각적 토크나이저는 멀티모달 LLM 훈련의 비용과 복잡성을 크게 줄여 저탄소, 대규모 모델 훈련을 촉진할 수 있습니다. 게다가 LLM 내에서 비전(상상) 씨앗의 &quot;발아&quot;를 간절히 기대합니다. 이 프로젝트는 아직 진행 중입니다. 더 많은 업데이트를 기대하세요! 감사의 말 Sijie Zhao(Tencent AI Lab)와 Chen Li(ARC Lab, Tencent PCG)에게 흥미로운 토론에 참여해 주셔서 진심으로 감사드립니다.참고문헌 [1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로를 통한 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. [4] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: 모듈화는 다중 모달리티를 갖춘 대규모 언어 모델을 강화합니다. arXiv 사전 인쇄 arXiv:2304.14178, 2023. [5] Junnan Li, Dongxu Li, Silvio Savarese 및 Steven Hoi. Blip-2: 고정 이미지 인코더 및 대규모 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 훈련. arXiv 사전 인쇄 arXiv:2301.12597, 2023. [6] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델을 사용하여 비전 언어 이해를 향상합니다. arXiv 사전 인쇄 arXiv:2304.10592, 2023. [7] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: 매개변수 효율적 시각 지시 모델. arXiv 사전 인쇄본 arXiv:2304.15010, 2023. [8] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee. 시각 지시 튜닝. arXiv 사전 인쇄본 arXiv:2304.08485, 2023. [9] Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov. 다중 모달 언어 모델을 사용한 이미지 생성. arXiv 사전 인쇄본 arXiv:2305.17216, 2023. [10] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 10684-10695, 2022. [11] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang 및 Xinlong Wang. 다중 양식의 생성적 사전 훈련. arXiv 사전 인쇄본 arXiv:2307.05222, 2023. [12] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang Binh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu Jacob, Singer Uriel, Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli, Zettlemoyer Luke, Aghajanyan Armen. 자기 회귀 다중 모달 모델 확장: 사전 학습 및 지침 조정. 2023. [13] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로 샷 텍스트-이미지 생성. 기계 학습에 관한 국제 컨퍼런스, 8821-8831페이지. PMLR, 2021.[14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang 등. Cogview: 변환기를 통해 텍스트-이미지 생성을 마스터합니다. 신경 정보 처리 시스템의 발전, 34:19822-19835, 2021. [15] Aaron Van Den Oord, Oriol Vinyals, et al. 신경 이산 표현 학습. 신경 정보 처리 시스템의 발전, 30, 2017. [16] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li 및 Yong Jae Lee. Gligen: 개방형 기반 텍스트-이미지 생성. 2023. [17] Patrick Esser, Robin Rombach, Bjorn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12873-12883페이지, 2021. [18] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei. Beit v2: 벡터 양자화된 시각적 토크나이저를 사용한 마스크 이미지 모델링. arXiv 사전 인쇄본 arXiv:2208.06366, 2022. [19] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: 사전 학습된 변압기 언어 모델 공개. arXiv 사전 인쇄본 arXiv:2205.01068, 2022. [20] Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. Association for Computational Linguistics(제1권: 장문 논문)의 제56회 연례 회의록, 2556-2565쪽, 2018. [21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 맥락 속의 공통 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014. [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020. [23] Unsplash. https://github.com/unsplash/datasets. [24] Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier. 이미지 설명에서 시각적 의미로: 이벤트 설명에 대한 의미 추론을 위한 새로운 유사성 측정 기준. Association for Computational Linguistics의 거래, 2:67-78, 2014. [25] Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov. 다중 모달 언어 모델을 사용한 이미지 생성. arXiv 사전 인쇄본 arXiv:2305.17216, 2023. [26] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson. Nocaps: 대규모의 새로운 객체 캡션. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 8948-8957페이지, 2019. [27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh. VQA에서 v를 중요하게 만들기: 시각적 질의 응답에서 이미지 이해의 역할 강화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6904-6913페이지, 2017. [28] Drew A Hudson 및 Christopher D Manning. Gqa: 실제 시각적 추론 및 구성적 질의 응답을 위한 새로운 데이터 세트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6700-6709페이지, 2019. [29] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao. Llama-adapter: zero-init attention을 통한 언어 모델의 효율적인 미세 조정. arXiv 사전 인쇄본 arXiv:2303.16199, 2023. [30] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. 신경 정보 처리 시스템의 발전, 35:23716-23736, 2022. [31] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: 구체화된 멀티모달 언어 모델. arXiv 사전 인쇄본 arXiv:2303.03378, 2023. [32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019.
