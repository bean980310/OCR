--- ABSTRACT ---
대규모 언어 모델 사전 학습의 최근 급속한 진전은 다음 토큰 예측 또는 스팬 손상과 같은 자기 지도 언어 모델링 목표를 사용하는 데 의존했습니다. 반면, 기계 번역 시스템은 대부분 소스 언어와 대상 언어 간에 정렬된 데이터가 필요한 교차 언어 지도를 사용하여 학습됩니다. 우리는 자기 지도 언어 모델링 목표와 지도 기계 번역 목표를 혼합하여 대규모 언어 모델을 사전 학습하고 사전 학습 중에 교차 언어 병렬 데이터를 포함하면 더 나은 맥락 내 학습 능력을 가진 모델이 생성됨을 보여줍니다. 사전 학습은 매우 리소스 집약적인 프로세스이며 두 목표 간의 최상의 혼합 비율에 대한 그리드 검색은 엄청나게 비쌀 수 있으므로 사전 학습 중에 학습하기 위한 간단하면서도 효과적인 전략을 제안합니다. 1
--- INTRODUCTION ---
대규모 사전 학습, GPT(Brown 등, 2020), XGLM(Lin 등, 2021), PaLM(Chowdhery 등, 2022) 개발의 급속한 진전으로 인해 컨텍스트 내 학습(일명 few shot) 패러다임을 통해 다양한 작업을 수행할 수 있는 모델이 탄생했습니다(Brown 등, 2020). 추론 시 모델에 주어진 작업에 대한 몇 가지 데모를 제시하면 모델은 새롭고 보이지 않는 예제에서 이러한 데모를 따를 수 있습니다. 따라서 더 이상 다양한 다운스트림 작업에서 이러한 모델을 미세 조정할 필요가 없습니다. 이러한 대규모 언어 모델(LLM)의 사전 학습은 자체 감독에 의존합니다. 즉, 데이터에 주석을 달 필요가 없습니다. 자기 지도(LM) 언어 모델링 목표의 예로는 다음 토큰 예측이 있는데, 이 경우 이전 토큰을 고려하여 다음 토큰을 예측하는 작업이고, 스팬 손상은 주변 환경을 고려하여 누락된 텍스트의 일부를 채우는 작업입니다. 반면, 기계 번역 모델(MTM)은 여전히 교차 언어 지도를 사용하여 훈련되고 있으며, 여기에는 정렬된 병렬 데이터가 필요합니다. 실제로 기계 번역(MT) 목표는 소스 문장을 고려하여 대상 문장을 예측하는 것이므로 소스 언어와 대상 언어 간에 정렬된 텍스트 쌍을 수집해야 합니다. 기계 번역에서 사전 훈련된 LLM은 역사적으로 수백만 개의 지도 예제로만 훈련된 MTM보다 성능이 낮았습니다. 이는 LLM이 컨텍스트 내 학습을 사용하여 평가되거나 병렬 데이터로 미세 조정된 후에 모두 해당됩니다. 그러나 LLM과 MTM 간의 성능 격차는 줄어들고 있습니다. 예를 들어, 자기 감독만을 사용하여 사전 학습된 언어 모델인 최근의 PaLM(Chowdhery et al., 2022)은 이전 기계 번역 벤치마크에서 최신 MTM보다 성능이 뛰어나지만, 최근 벤치마크에서는 감독 MTM보다 뒤처졌습니다(Vilar et al., 2022). 이러한 추세는 자연스러운 질문인 Q를 제기합니다. 교차 언어 감독 데이터에 대한 학습이 여전히 필요하거나 유익할까요? 질문 Q와 관련하여, 우리는 LLM을 사전 학습할 때 병렬 데이터를 포함하는 것이 탐색하기에 가장 유망한 방향이라고 생각합니다. 우리가 선호하는 첫 번째 이유는 LLM과 MTM 간의 MT 벤치마크 격차가 줄어들고 있기 때문입니다. LLM이 가까운 미래에 따라잡을 수 있을 가능성이 매우 높으며, 동시에 MTM보다 훨씬 더 많은 작업을 수행할 수 있습니다. 두 번째 근거는 사전 학습 데이터 세트가 여전히 영어에 의해 지배되고 있다는 것입니다. PaLM의 사전 학습 데이터 세트의 언어 구성을 비교해보세요(Chowdhery et al., 2022): 다른 언어, 특히 리소스가 낮은 언어는 과소 표현됩니다. 따라서 정렬된 교차 언어 데이터가 영어 이외의 다른 언어에서 LLM의 능력을 향상시킬 수 있다는 자연스러운 추측이 있습니다. LLM의 다국어 능력을 평가할 때 개방형 및 폐쇄형 생성 설정을 구별해야 합니다. 폐쇄형 생성에서는 작업이 단일 언어로 수행됩니다. 예를 들어 맥락 문단이 독일어로 제시되고 질문은 독일어로 공식화되고 답변은 독일어로 예상됩니다. 개방형 생성에서는 작업이 두 언어로 수행됩니다. 예를 들어 맥락 문단이 영어로 제시되고 질문은 독일어로 공식화되고 답변은 독일어로 예상됩니다. 이제 사전 학습 중에 교차 언어 데이터를 포함하는 것의 매력은 LLM의 기계 번역 성능을 개선하는 능력뿐만 아니라 언어 간의 교량을 구축하는 데 있습니다. 우리는 교차 언어적 지도가 대표성이 낮은 언어에서 폐쇄적 생성을 개선할 것으로 기대할 수 있지만, 또 다른 자연스러운 추측은 그것이 개방적 생성, 즉 두 언어가 관련된 것을 개선한다는 것입니다.위의 논의에 비추어 우리는 Q를 세분화합니다: LLM을 사전 훈련할 때 교차 언어적 지도 데이터가 유익한가요?특히, 평가를 위해 맥락 내 학습 패러다임을 사용할 때 개방적 생성과 폐쇄적 생성 모두에서 이득이 있습니까?우리는 LLM에서 교차 언어적 지도의 사용을 고려한 최초의 연구는 아닙니다(섹션 2 참조).그러나 우리의 연구는 다음과 같은 측면에서 이전 연구와 다릅니다: 1. 우리는 사전 훈련 단계에서 교차 언어적 지도를 포함합니다.2. 우리는 표준 지도 MT 목표를 사용하여 교차 언어적 지도를 포함합니다.3. 우리는 폐쇄적 생성과 개방적 생성 설정을 모두 고려한 맥락 내 학습으로 결과 모델을 평가합니다.4. 우리는 훈련하는 동안 사용할 병렬 데이터의 양을 학습합니다. 이 작업에서 우리는 먼저 대규모 언어 모델을 사전 학습할 때 일부 교차 언어적 감독을 포함하는 것이 유익하다는 것을 보여 주어 Q에 답합니다. 그런 다음 사용할 최적의 양의 교차 언어적 감독을 학습해야 할 때 자동화된 커리큘럼 학습(Graves et al., 2017)이 여러 번의 학습 실행이 필요하지 않고 정적 정책보다 성능이 우수한 효과적인 전략임을 보여줍니다. 우리는 하이퍼 매개변수 검색에 의존하지 않고 학습하는 동안 병렬 데이터의 양을 학습하는 것의 중요성을 강조합니다. 충분히 많은 토큰에서 LLM을 사전 학습하는 것은 리소스 집약적인 작업입니다. 예를 들어 3.8B-매개변수 모델을 사용한 각 실험에는 5일 동안 256개의 TPU 코어가 필요합니다. 병렬 MT 데이터와 LM 학습 데이터 간의 혼합 비율을 하이퍼 매개변수 \로 처리하면 이론상 추가 하이퍼 매개변수만 있습니다. 그러나 그리드 검색은 엄청나게 비쌉니다. 예를 들어 (Kale et al., 2021)은 MT와 LM 데이터를 혼합하여 100k 단계에 대해 mT5 모델을 미세 조정하는 덜 계산 집약적인 설정을 고려했습니다. 그럼에도 불구하고 그들은 X의 두 값만 비교할 수 있었습니다. 더욱이, \를 하이퍼파라미터로 취급하면 X가 고정된 정적 전략보다 성능이 더 좋은 동적 스케줄링 전략, 즉 시간에 따라 \가 변할 수 있다는 사실을 간과합니다. 2
--- RELATED WORK ---
LLM에서 병렬 교차 언어 데이터 사용을 조사한 것은 우리가 처음은 아닙니다. (Reid 및 Artetxe, 2022)는 3개의 목표로 구성된 손실을 고안하여 병렬 데이터를 활용하는 것을 고려했습니다. 그러나 그들의 기술은 다국어 노이즈 처리 절차를 개발해야 하기 때문에 다소 복잡한 반면, 우리는 표준 MT 목표를 사용하여 교차 언어 데이터를 포함하기로 선택했습니다. (Chi et al., 2021)은 (Xue et al., 2021)의 성공을 바탕으로 더 간단한 목표를 제안했습니다. 즉, mT5를 훈련하는 데 사용된 노이즈 제거 절차에 감독 MT 데이터를 직접 추가하여 교차 언어 생성에서 모델이 mT5보다 우수한 성과를 거두었습니다. 그러나 (Chi et al., 2021)이 사전 훈련 중에 교차 언어 감독을 포함하는 반면, 결과 모델은 맥락 내 학습 능력을 표시하지 않으며 평가는 다운스트림 작업에서 미세 조정을 통해 수행됩니다. (Kale et al., 2021)은 병렬 데이터에서 mT5를 미세 조정하면 어떤 일이 일어나는지 탐구했습니다. 따라서 병렬 데이터는 다운스트림 작업에서 사전 학습과 미세 조정 사이의 중간 단계에서 사용됩니다. 이러한 모든 연구의 한계는 미세 조정에 대한 강조입니다. 이러한 모든 모델은 미세 조정이 필요한데, 이는 few-shot in context 학습과 상당히 다릅니다. 따라서 한 작업의 지도 학습 데이터가 다른 작업의 few-shot 학습에 도움이 될 수 있는지에 대한 질문은 여전히 탐구되지 않았습니다. 3 기본 설정 3.1 학습 데이터 언어 모델링 데이터는 (Chowdhery et al., 2022)의 데이터를 기반으로 하지만 약간 데이터 소스. % 데이터 소스. 데이터 비율 낮음/높음 토큰(B) 소셜 미디어 대화+ 40% 문장 96% 필터링된 웹 페이지 34% 문서 4% GitHub 4% ar 7.3% 높음 16.Books* 15% bn 5.4% 낮음 1.Wikipedia+ 5% de 9.5% 높음 85.News* 2% fi 6.3% 높음 8.fr 9.8% 높음 123.표 1: LM 데이터: 데이터 소스 및 데이터 비율. †는 데이터 소스가 다국어임을 의미하고, ✶는 영어로만 제공됨을 의미합니다. id 7.1% 높음 5.ja 7.9% 높음 27.ko 7.2% 높음 15.ru 8.6% 높음 54.SW 4.8% 낮음 1.te 4.5% 낮음 0.th 6.6% 높음 14.tr 7.9% 높음 11.vi 7.1% 높음 12. 다른 하위 범주 간 비율을 수정합니다(표 1 참조). 초기 실험에서 (Chowdhery et al., 2022)의 고품질 데이터가 더 나은 문맥 내 학습 능력을 보였기 때문에 공개 언어 모델링 데이터 세트(예: MC4(Raffel et al., 2019))를 사용하지 않습니다. 언어 모델링 목표로 최근의 &quot;UL2&quot;(Tay et al., 2022)를 사용하는데, fewshot 설정에서 더 나은 성능을 보였기 때문입니다. MT 데이터의 경우 표 2의 언어를 포함하는 사내 병렬 코퍼스를 사용하는데, 이는 또한 샘플링 비율을 보고하고 언어를 높음 또는 낮음 리소스 설정에서 고려할지 여부를 강조합니다. 학습 데이터에는 항상 영어로 된 소스 또는 타겟이 있습니다. 다국어 지도 학습 모델을 학습할 때 사용하는 표준 접근 방식을 사용합니다. (2xx) + 소스 → 인코더 타겟 디코더, (1) (2) 여기서 소스 문장은 특수 타겟 언어 토큰인 (2xx)로 접두사가 붙고 인코더에 제공되고 타겟은 디코더에 제공됩니다. 3.2 모델 아키텍처 일반적으로 사용되는 LLM 아키텍처는 인코더-디코더 모델(예: T5(Raffel et al., 2019))과 디코더 전용 모델(예: Brown et al., 2020; Chowdhery et al., 2022)입니다. 대부분의 지도 학습 MTM은 인코더-디코더 아키텍처를 사용합니다. 실험에는 처음부터 사전 학습이 필요하고 따라서 많은 리소스가 필요하므로 인코더-디코더라는 하나의 아키텍처만 고려합니다.특히, &quot;대형&quot;(12억) 및 &quot;x1&quot;(38억) 모델 크기에서 mT5(Xue et al., 2021) 아키텍처를 사용합니다.T5X 라이브러리(Roberts et al., 2022)의 기본 설정을 사용하여 12억 개의 모델을 250k 단계로 학습하고 38억 개의 모델을 500k 단계로 학습합니다.배치에서 표 2: MT 데이터 구성 최대 시퀀스 길이는 1024이고 비패딩 토큰의 수는 500k를 약간 넘습니다.mT5는 아키텍처에만 사용되고 mT5 체크포인트나 mT5 학습에 사용된 데이터는 절대 사용하지 않는다는 점을 강조합니다.3.3 평가 원샷 설정을 사용하여 컨텍스트 내 학습으로 모델을 평가합니다.명시적인 예는 부록을 참조하세요. 구체적으로, 각 테스트 입력에는 대상 동작에 대한 원하는 입력을 표시하는 하나의 예가 접두사로 붙습니다. 이렇게 얻은 시퀀스는 인코더에 제공되고 대상은 디코더에 의해 생성됩니다. 질문 답변, 기계 번역 및 요약의 세 가지 작업을 고려합니다. 질문 답변의 경우 두 가지 설정을 고려합니다. 컨텍스트, 질문 및 답변이 동일한 언어인 폐쇄형 생성과 컨텍스트가 한 언어이고 질문과 답변이 다른 언어인 개방형 생성입니다. 폐쇄형 생성 설정의 경우 TyDiQA(Clark et al., 2020)를 사용합니다. 개방형 생성 설정의 경우 TyDiQA의 영어가 아닌 분할을 사용하여 Google Translate API(translate.google.com에서 2022년 11월에 액세스)를 사용하여 컨텍스트를 영어로 번역합니다. 이렇게 얻은 데이터 세트를 XTyDiQA로 표시합니다. 기계 번역의 경우 Flores(Guzmán 등, 2019)를 사용하고 요약의 경우 GEM(Gehrmann 등, 2022) 벤치마크의 분할 및 전처리와 함께 Wikilingua(Ladhak 등, 2020)를 사용합니다.4 두 작업을 스케줄링하는 방법 학습 \에 대한 그리드 탐색은 실행 불가능합니다.언어 모델링과 기계 번역이라는 두 가지 작업이 있으므로 MT 작업의 비율 \을 튜닝할 하이퍼 매개변수로 처리할 수 있습니다.사전 학습이 매우 많은 리소스를 필요로 하기 때문에 에 대한 그리드 탐색은 실행 불가능합니다.(Kale 등, 2021)이 고려한 덜 계산 집약적인 설정에서도 mT5 체크포인트의 지속적인 사전 학습인 λ의 두 값만 비교할 수 있었습니다.따라서 시간이 지남에 따라 변경되는 정책이 일정하게 유지하는 정책보다 성능이 우수할 수 있다는 추가 이점이 있으므로 학습 중에 &gt;를 학습하는 것이 매우 바람직합니다.자동화된 커리큘럼 학습은 자연스러운 접근 방식입니다. 여러 소스의 데이터로 모델을 학습할 때 자동화된 커리큘럼 학습 패러다임(Graves et al., 2017)은 학습하는 동안 데이터 샘플링 일정을 학습할 수 있습니다. 이런 식으로 시간 단계 t의 함수인 동적 람다 λt를 학습할 수 있습니다. 구체적으로 λt는 MT 작업을 샘플링할 확률을 나타내고 1λt는 LM 작업을 샘플링할 확률입니다. 최근 연구(Kreutzer et al., 2021)는 데이터가 여러 도메인 또는 여러 언어에서 나오는 기계 번역 시스템에 이 커리큘럼 접근 방식을 적용할 때 유망한 결과를 보여주었습니다. 예를 들어 (Kreutzer et al., 2021)는 자동화된 커리큘럼 학습에서 사용하는 멀티암드밴딧이 다국어 벤치마크에서 여러 SOTA 휴리스틱에 대해 경쟁력 있게 수행됨을 보여줍니다. 올바른 보상 함수를 찾아야 합니다. MT 및 LM 작업의 동적 스케줄링을 학습하려면 특정 작업을 사용하는 데 대한 보상을 할당해야 합니다. 작업 □ € {MT, LM}을 샘플링한다고 가정합니다. 그런 다음 해당 배치 BÃ를 얻고 경사 하강을 수행하여 모델 매개변수를 ☺에서 ☺&#39;로 업데이트합니다. 따라서 7이라는 특정 선택으로 인해 매개변수가 변경되었으며 얼마나 유용했는지 측정해야 합니다. 다양한 유틸리티 함수를 벤치마킹한 후, Kreutzer et al. (2021)은 신뢰할 수 있는 검증 세트에서 손실 감소 L(O) – L(O&#39;)를 측정할 것을 권장합니다. 그러나 Kreutzer et al. (2021)의 설정에서는 검증 세트를 명확하게 선택했지만, 우리는 컨텍스트 내 학습 패러다임을 사용하여 다운스트림 작업에 적용되는 LLM의 사전 학습에 관심이 있습니다. 따라서 가능한 모든 소수 샷 작업을 나타내는 검증 세트를 구축하는 것은 사소한 일이 아닙니다. 특히 특정 작업 선택에 대한 과적합을 방지하기 위해 완화 전략이 필요합니다. 우리는 내재적 보상 함수를 사용합니다. 초기 실험에서 우리는 각 다운스트림 작업(예: 질의응답)에 의해 할당된 보상을 훈련 작업에 의해 할당된 보상과 대조하였고, 전자의 신호가 크기가 더 작고 분산이 더 크다는 것을 발견했습니다. 따라서 우리는 (사전) 훈련 데이터 자체에서 본질적으로 보상을 측정할 것을 제안합니다. 형식적으로, B에서 그래디언트 단계를 수행한 후, 우리는 보상 작업 p = {MT, LM}을 동일한 확률로 샘플링하고 손실 감소를 측정하는 새로운 배치 B를 얻습니다. 우리는 한 작업을 다른 작업보다 선호하도록 고정하고 싶지 않기 때문에 각 보상 작업에 동일한 확률을 할당합니다. 내재적 보상 함수를 사용하는 것의 명확한 이점 중 하나는 더 이상 검증 데이터 세트를 구성할 필요가 없다는 것입니다. 훈련 작업 자체의 사용은 (Graves et al., 2017; Kreutzer et al., 2021)에서 고려되었지만, 그들은 그래디언트 단계를 수행하는 데 사용된 동일한 배치 BT에서 보상을 측정하는 반면, 우리는 다른 작업에서 독립적인 배치 Bp를 샘플링합니다. p는 50% 확률로 7과 같고 50% 확률로 다른 작업과 같도록 샘플링되므로 작업별 학습과 교차 작업 전송을 모두 측정합니다. 손실 감소는 재조정해야 합니다. LM과 MT의 손실 규모는 학습 중에 다를 수 있으므로 절대 손실 감소 L(O) – L(O&#39;)은 L을 계산하는 데 사용된 작업의 영향을 받습니다. 실제로 기계 번역에서는 모든 정보 콘텐츠가 소스 시퀀스에 제공되므로 번역 작업의 복잡도는 일반적으로 언어 모델링 작업의 복잡도보다 낮습니다. 우리는 보상을 상대적 손실 감소 보상 =_ L(O&#39;, Bp) L(O, B₂) (3) &quot;으로 계산하여 이 문제를 해결합니다. 이는 (Kreutzer et al., 2021)에서 &quot;pgnorm&quot;이라고 불렸습니다. 고전적인 밴딧 알고리즘은 단일 작업에서 샘플링하는 경향이 있습니다. 두 작업에서 샘플링한 정책은 멀티암드 밴딧을 사용하여 학습됩니다(Lattimore and Szepesvári, 2020). 우리는 처음에 (Graves et al., 2017; Kreutzer et al., 2021)에서와 같이 EXP3로 실험했습니다. 그러나 LM 작업은 항상 MT 작업보다 약간 더 큰 보상을 생성한다는 것을 발견했습니다. EXP3는 사후적으로 최상의 단일 암을 선택하도록 설계되었으므로 모델 크기(B) 데이터 선택 TyDiQA En TyDiQA Non-En TyDiQA XTyDiQA 1.LM(100%) 40.23.25.10.1.LM (90%)-MT (10%) 39.25.26.11.1.LM (50%)-MT (50%) 41.29.30.13.1.WARMUP 39.23.25.12.1.EXP42.30.31.16.1.FAIR 41.31.32.18.3.LM (100%) 47.32.34.13.3.3.EXPFAIR 50.42.43.25.47.36.37.26.표 3: EM으로 측정한 TyDiQA 및 XTyDiQA의 성능. 정적 데이터 선택 전략은 자동화된 커리큘럼보다 성능이 뛰어납니다. 병렬 데이터를 추가해도 영어 성능에 영향을 미치지 않으며 다른 언어 및 (교차 언어) 오픈 생성에서 폐쇄형 생성 성능이 크게 향상됩니다. 알고리즘 1 FAIR 요구 사항: 탐색 비율 y, 이동 평균 비율 μ, 팔 수 n -1: 팔 가중치 초기화: wa 10¯&#39; Wa Wa + Y n 2: 정책 계산: &quot;a = (1 − y) 3: 팔 샘플링: a ~π 및 보상 ra 얻기 4: 가중치 업데이트: Wa ← (1 - μ)ωα + μια는 정책을 LM 팔에 중심화하는 경향이 있습니다. 이 문제를 완화하기 위해 주어진 팔에 대한 보상의 이동 평균에 비례하여 샘플링하는 &quot;FAIR&quot; 알고리즘을 제안합니다. 자세한 내용은 알고리즘 1을 참조하십시오. 재현성을 위해 부록에서 커리큘럼 설정에 대한 전체 세부 정보를 제공합니다. 5 실험 결과 5.1 기준선 고려하는 첫 번째 기준선은 LM 데이터(LM(100%))에 대해서만 학습하는 것입니다. LM 및 MT 작업 간의 정적 혼합 비율 \에 대한 그리드 검색은 엄청나게 비용이 많이 들기 때문에 (Kale et al., 2021): λ = 0.(LM(50%) MT(50%)) 및 \ = 0.1(LM(90%) -MT(10%)). MT 목표를 보다 적극적으로 샘플링하는 = 0.5와 보다 보수적으로 샘플링하는 X = 0.1 사이의 중간 동작을 만들기 위해 처음 20k 단계에 \ 0.4를 사용한 다음 기본값인 \ = 0.1로 설정하는 WARMUP 휴리스틱을 고려합니다. 값 \은 학습 시작 시 각 작업의 보상을 검사하여 선택했습니다. = = 0. 모델 크기가 1.2B일 때 병렬 데이터를 추가하면 평가된 작업에서 성능이 향상되는 것을 확인했습니다. 그러나 자동화된 커리큘럼 학습 전략이 다른 기준선보다 성능이 우수했습니다. 따라서 제한된 실험 예산을 감안할 때 모델 크기가 3.8B일 때 LM(100%)만 기준으로 고려합니다. 5.2 질의응답 질의응답에 대한 결과는 표 3에 나와 있습니다. TyDiQA의 경우 병렬 데이터를 추가하면 영어가 아닌 부분에서 성능이 크게 향상되고 영어 성능은 저하되지 않습니다. XTyDiQA에서 병렬 데이터를 추가하면 모델 크기 12억에서 최대 +8 EM 포인트, 매개변수 38억에서 +12 EM 포인트로 상당한 차이를 만들 수 있습니다. 따라서 사전 학습 중에 교차 언어 감독을 포함하면 질의응답 과제에 대한 사전 학습된 모델의 오픈 제너레이션 능력이 향상됩니다. 또한 자동화된 커리큘럼(EXP3 또는 FAIR)이 모델 크기 1.2B에서 다른 모든 데이터 샘플링 전략보다 성능이 뛰어나므로 더 큰 모델 크기 3.8B에서 EXP3와 FAIR만 실험합니다. 5.3 요약 표 4는 Wikilingua에서 요약 과제에 대한 주요 결과를 보여줍니다. 자동화된 커리큘럼 기반 기술을 수동 혼합과 대조했습니다.
--- METHOD ---
s. 질의응답과 비교했을 때 요약 결과는 크게는 아니지만 자동화된 커리큘럼 방법이 바닐라 LM 전용 방법보다 성능이 뛰어나고 제안된 FAIR 방법이 다른 방법보다 성능이 뛰어난 더 큰 모델로 기울어집니다. 흥미롭게도, 자동화된 커리큘럼 방법이 바닐라 LM 전용 방법보다 모델 크기를 확장하는 데 더 많은 이점이 있는 LM(100%)의 매개변수를 12억에서 38억으로 확장할 때 아무런 이득도 관찰하지 못했습니다. 모델 크기(B) 데이터 선택 En Non-En 모두 1.LM(100%) 16.12.37 12.1.LM(90%)-MT(10%) 15.12.12.1.1.LM(50%) MT(50%) WARMUP 14.92 11.12.15.11.12.1.EXP15.12.25 12.1.FAIR 14.11.11.3.LM(100%) 16.12.12.3.3.EXPFAIR 17.08 13.38 13.18.15 14.14.표 4: Wikilingua에서 RougeL로 평가한 요약 성능. 매개변수가 1.2B인 경우 병렬 데이터를 더 추가하면 성능이 약간 떨어질 수 있습니다. 그러나 매개변수가 3.8B인 경우 병렬 데이터를 추가하면 LM 전용 기준선보다 성능이 약간 향상됩니다. 모델 크기(B) 데이터 선택 En→ High High → En En Low Low → En 1.LM(100%) 8.15.0.3.1.LM(90%)-MT(10%) 12.20.2.5.1.LM(50%)-MT(50%) 17.27.5.13.1.WARMUP 10.21.1.6.1.EXP16.26.4.18.1.FAIR 23.31.15.26.3.LM(100%) 12.21.1.5.3.EXP26.34.23.31.3.FAIR 30.36.27.36.표 5: sacreBLEU로 측정한 MT 작업(Flores)의 성능. 병렬 데이터를 추가하면 변환 결과(상황 내 학습으로 평가)가 크게 개선되며, FAIR bandit이 최상의 데이터 선택 전략입니다. 5.4 기계 번역 커리큘럼 학습은 성과를 향상시킵니다. 기계 번역(표 5)의 경우, 분석을 영어에서 영어로(X→En, En→X) 번역, 하이 및 로우 리소스 번역의 네 가지 설정으로 분할했습니다. 자동화된 커리큘럼 방법을 사용하면 상당한 이득을 얻을 수 있으며, 예를 들어 En→ Low 설정에서 LM(50%) - MT(50%) 샘플링에 비해 +10 BLEU 포인트가 증가했습니다. 다른 방법과 비교했을 때, 제안하는 FAIR 알고리즘은 생성 품질도 더욱 향상시킵니다. 제약 조건이 주어진 경우,
--- EXPERIMENT ---
영어: 3.8B-매개변수 모델을 사용하는 s는 5일 동안 256개의 TPUvcore가 필요합니다. 병렬 MT 데이터와 LM 학습 데이터 간의 혼합 비율을 하이퍼 매개변수 \로 취급하면 이론적으로 추가 하이퍼 매개변수가 생깁니다. 그러나 그리드 검색은 엄청나게 비쌉니다. 예를 들어 (Kale et al., 2021)은 MT와 LM 데이터를 혼합하여 100k 단계에 대해 mT5 모델을 미세 조정하는 덜 계산 집약적인 설정을 고려했습니다. 그럼에도 불구하고 X의 두 값만 비교할 수 있었습니다. 또한 \를 하이퍼 매개변수로 취급하면 X가 고정된 정적 전략보다 성능이 뛰어난 동적 스케줄링 전략(즉, 시간이 지남에 따라 \가 변함)이 있을 수 있다는 사실을 간과합니다. 2 관련 연구 LLM에서 병렬 교차 언어 데이터 사용을 조사한 것은 처음이 아닙니다. (Reid 및 Artetxe, 2022)는 3개의 목표로 구성된 손실을 고안하여 병렬 데이터를 활용하는 것을 고려했습니다. 그러나 그들의 기술은 다국어 노이즈 처리 절차의 개발을 필요로 하기 때문에 다소 복잡하지만, 우리는 표준 MT 목적을 사용하여 교차 언어 데이터를 포함하기로 선택했습니다.(Chi et al., 2021)은 (Xue et al., 2021)의 성공을 바탕으로 더 간단한 목적을 제안했습니다. 즉, mT5를 훈련하는 데 사용된 노이즈 제거 절차에 감독 MT 데이터를 직접 추가하여 교차 언어 생성에서 모델이 mT5보다 우수한 성과를 거두었습니다. 그러나 (Chi et al., 2021)이 사전 훈련 중에 교차 언어 감독을 포함하는 반면, 결과 모델은 맥락 내 학습 능력을 표시하지 않으며 평가는 다운스트림 작업에서 미세 조정을 통해 수행됩니다.(Kale et al., 2021)은 병렬 데이터에서 mT5를 미세 조정하면 어떤 일이 발생하는지 탐구했습니다. 따라서 사전 훈련과 다운스트림 작업에서 미세 조정 사이의 중간 단계에서 병렬 데이터를 사용합니다. 이 모든 연구의 한계는 미세 조정에 대한 강조입니다. 이 모든 모델은 미세 조정이 필요하며, 이는 few-shot in-context 학습과는 상당히 다릅니다. 따라서 한 과제의 지도 학습 데이터가 다른 과제의 few-shot 학습에 도움이 될 수 있는지에 대한 질문은 여전히 탐구되지 않았습니다. 3 기본 설정 3.1 학습 데이터 언어 모델링 데이터는 (Chowdhery et al., 2022)의 데이터를 기반으로 하지만, 우리는 약간 데이터 소스. % 데이터 소스. 데이터 비율 낮음/높음 토큰(B) 소셜 미디어 대화+ 40% 문장 96% 필터링된 웹 페이지 34% 문서 4% GitHub 4% ar 7.3% 높음 16.Books* 15% bn 5.4% 낮음 1.Wikipedia+ 5% de 9.5% 높음 85.News* 2% fi 6.3% 높음 8.fr 9.8% 높음 123.표 1: LM 데이터: 데이터 소스 및 데이터 비율. †는 데이터 소스가 다국어임을 의미하고, ✶는 영어로만 제공됨을 의미합니다. id 7.1% 높음 5.ja 7.9% 높음 27.ko 7.2% 높음 15.ru 8.6% 높음 54.SW 4.8% 낮음 1.te 4.5% 낮음 0.th 6.6% 높음 14.tr 7.9% 높음 11.vi 7.1% 높음 12. 다른 하위 범주 간 비율을 수정합니다(표 1 참조). 초기 실험에서 (Chowdhery et al., 2022)의 고품질 데이터가 더 나은 문맥 내 학습 능력을 보였기 때문에 공개 언어 모델링 데이터 세트(예: MC4(Raffel et al., 2019))를 사용하지 않습니다. 언어 모델링 목표로 최근의 &quot;UL2&quot;(Tay et al., 2022)를 사용하는데, fewshot 설정에서 더 나은 성능을 보였기 때문입니다. MT 데이터의 경우 표 2의 언어를 포함하는 사내 병렬 코퍼스를 사용하는데, 이는 또한 샘플링 비율을 보고하고 언어를 높음 또는 낮음 리소스 설정에서 고려할지 여부를 강조합니다. 학습 데이터에는 항상 영어로 된 소스 또는 타겟이 있습니다. 다국어 지도 학습 모델을 학습할 때 사용하는 표준 접근 방식을 사용합니다. (2xx) + 소스 → 인코더 타겟 디코더, (1) (2) 여기서 소스 문장은 특수 타겟 언어 토큰인 (2xx)로 접두사가 붙고 인코더에 제공되고 타겟은 디코더에 제공됩니다. 3.2 모델 아키텍처 일반적으로 사용되는 LLM 아키텍처는 인코더-디코더 모델(예: T5(Raffel et al., 2019))과 디코더 전용 모델(예: Brown et al., 2020; Chowdhery et al., 2022)입니다. 대부분의 지도 학습 MTM은 인코더-디코더 아키텍처를 사용합니다. 실험에는 처음부터 사전 학습이 필요하고 따라서 많은 리소스가 필요하므로 인코더-디코더라는 하나의 아키텍처만 고려합니다.특히, &quot;대형&quot;(12억) 및 &quot;x1&quot;(38억) 모델 크기에서 mT5(Xue et al., 2021) 아키텍처를 사용합니다.T5X 라이브러리(Roberts et al., 2022)의 기본 설정을 사용하여 12억 개의 모델을 250k 단계로 학습하고 38억 개의 모델을 500k 단계로 학습합니다.배치에서 표 2: MT 데이터 구성 최대 시퀀스 길이는 1024이고 비패딩 토큰의 수는 500k를 약간 넘습니다.mT5는 아키텍처에만 사용되고 mT5 체크포인트나 mT5 학습에 사용된 데이터는 절대 사용하지 않는다는 점을 강조합니다.3.3 평가 원샷 설정을 사용하여 컨텍스트 내 학습으로 모델을 평가합니다.명시적인 예는 부록을 참조하세요. 구체적으로, 각 테스트 입력에는 대상 동작에 대한 원하는 입력을 표시하는 하나의 예가 접두사로 붙습니다. 이렇게 얻은 시퀀스는 인코더에 제공되고 대상은 디코더에 의해 생성됩니다. 질문 답변, 기계 번역 및 요약의 세 가지 작업을 고려합니다. 질문 답변의 경우 두 가지 설정을 고려합니다. 컨텍스트, 질문 및 답변이 동일한 언어인 폐쇄형 생성과 컨텍스트가 한 언어이고 질문과 답변이 다른 언어인 개방형 생성입니다. 폐쇄형 생성 설정의 경우 TyDiQA(Clark et al., 2020)를 사용합니다. 개방형 생성 설정의 경우 TyDiQA의 영어가 아닌 분할을 사용하여 Google Translate API(translate.google.com에서 2022년 11월에 액세스)를 사용하여 컨텍스트를 영어로 번역합니다. 이렇게 얻은 데이터 세트를 XTyDiQA로 표시합니다. 기계 번역의 경우 Flores(Guzmán 등, 2019)를 사용하고 요약의 경우 GEM(Gehrmann 등, 2022) 벤치마크의 분할 및 전처리와 함께 Wikilingua(Ladhak 등, 2020)를 사용합니다.4 두 작업을 스케줄링하는 방법 학습 \에 대한 그리드 탐색은 실행 불가능합니다.언어 모델링과 기계 번역이라는 두 가지 작업이 있으므로 MT 작업의 비율 \을 튜닝할 하이퍼 매개변수로 처리할 수 있습니다.사전 학습이 매우 많은 리소스를 필요로 하기 때문에 에 대한 그리드 탐색은 실행 불가능합니다.(Kale 등, 2021)이 고려한 덜 계산 집약적인 설정에서도 mT5 체크포인트의 지속적인 사전 학습인 λ의 두 값만 비교할 수 있었습니다.따라서 시간이 지남에 따라 변경되는 정책이 일정하게 유지하는 정책보다 성능이 우수할 수 있다는 추가 이점이 있으므로 학습 중에 &gt;를 학습하는 것이 매우 바람직합니다.자동화된 커리큘럼 학습은 자연스러운 접근 방식입니다. 여러 소스의 데이터로 모델을 학습할 때 자동화된 커리큘럼 학습 패러다임(Graves et al., 2017)은 학습하는 동안 데이터 샘플링 일정을 학습할 수 있습니다. 이런 식으로 시간 단계 t의 함수인 동적 람다 λt를 학습할 수 있습니다. 구체적으로 λt는 MT 작업을 샘플링할 확률을 나타내고 1λt는 LM 작업을 샘플링할 확률입니다. 최근 연구(Kreutzer et al., 2021)는 데이터가 여러 도메인 또는 여러 언어에서 나오는 기계 번역 시스템에 이 커리큘럼 접근 방식을 적용할 때 유망한 결과를 보여주었습니다. 예를 들어 (Kreutzer et al., 2021)는 자동화된 커리큘럼 학습에서 사용하는 멀티암드밴딧이 다국어 벤치마크에서 여러 SOTA 휴리스틱에 대해 경쟁력 있게 수행됨을 보여줍니다. 올바른 보상 함수를 찾아야 합니다. MT 및 LM 작업의 동적 스케줄링을 학습하려면 특정 작업을 사용하는 데 대한 보상을 할당해야 합니다. 작업 □ € {MT, LM}을 샘플링한다고 가정합니다. 그런 다음 해당 배치 BÃ를 얻고 경사 하강을 수행하여 모델 매개변수를 ☺에서 ☺&#39;로 업데이트합니다. 따라서 7이라는 특정 선택으로 인해 매개변수가 변경되었으며 얼마나 유용했는지 측정해야 합니다. 다양한 유틸리티 함수를 벤치마킹한 후, Kreutzer et al. (2021)은 신뢰할 수 있는 검증 세트에서 손실 감소 L(O) – L(O&#39;)를 측정할 것을 권장합니다. 그러나 Kreutzer et al. (2021)의 설정에서는 검증 세트를 명확하게 선택했지만, 우리는 컨텍스트 내 학습 패러다임을 사용하여 다운스트림 작업에 적용되는 LLM의 사전 학습에 관심이 있습니다. 따라서 가능한 모든 소수 샷 작업을 나타내는 검증 세트를 구축하는 것은 사소한 일이 아닙니다. 특히 특정 작업 선택에 대한 과적합을 방지하기 위해 완화 전략이 필요합니다. 우리는 내재적 보상 함수를 사용합니다. 초기 실험에서 우리는 각 다운스트림 작업(예: 질의응답)에 의해 할당된 보상을 훈련 작업에 의해 할당된 보상과 대조하였고, 전자의 신호가 크기가 더 작고 분산이 더 크다는 것을 발견했습니다. 따라서 우리는 (사전) 훈련 데이터 자체에서 본질적으로 보상을 측정할 것을 제안합니다. 형식적으로, B에서 그래디언트 단계를 수행한 후, 우리는 보상 작업 p = {MT, LM}을 동일한 확률로 샘플링하고 손실 감소를 측정하는 새로운 배치 B를 얻습니다. 우리는 한 작업을 다른 작업보다 선호하도록 고정하고 싶지 않기 때문에 각 보상 작업에 동일한 확률을 할당합니다. 내재적 보상 함수를 사용하는 것의 명확한 이점 중 하나는 더 이상 검증 데이터 세트를 구성할 필요가 없다는 것입니다. 훈련 작업 자체의 사용은 (Graves et al., 2017; Kreutzer et al., 2021)에서 고려되었지만, 그들은 그래디언트 단계를 수행하는 데 사용된 동일한 배치 BT에서 보상을 측정하는 반면, 우리는 다른 작업에서 독립적인 배치 Bp를 샘플링합니다. p는 50% 확률로 7과 같고 50% 확률로 다른 작업과 같도록 샘플링되므로 작업별 학습과 교차 작업 전송을 모두 측정합니다. 손실 감소는 재조정해야 합니다. LM과 MT의 손실 규모는 학습 중에 다를 수 있으므로 절대 손실 감소 L(O) – L(O&#39;)은 L을 계산하는 데 사용된 작업의 영향을 받습니다. 실제로 기계 번역에서는 모든 정보 콘텐츠가 소스 시퀀스에 제공되므로 번역 작업의 복잡도는 일반적으로 언어 모델링 작업의 복잡도보다 낮습니다. 우리는 보상을 상대적 손실 감소 보상 =_ L(O&#39;, Bp) L(O, B₂) (3) &quot;으로 계산하여 이 문제를 해결합니다. 이는 (Kreutzer et al., 2021)에서 &quot;pgnorm&quot;이라고 불렸습니다. 고전적인 밴딧 알고리즘은 단일 작업에서 샘플링하는 경향이 있습니다. 두 작업에서 샘플링한 정책은 멀티암드 밴딧을 사용하여 학습됩니다(Lattimore and Szepesvári, 2020). 우리는 처음에 (Graves et al., 2017; Kreutzer et al., 2021)에서와 같이 EXP3로 실험했습니다. 그러나 LM 작업은 항상 MT 작업보다 약간 더 큰 보상을 생성한다는 것을 발견했습니다. EXP3는 사후적으로 최상의 단일 암을 선택하도록 설계되었으므로 모델 크기(B) 데이터 선택 TyDiQA En TyDiQA Non-En TyDiQA XTyDiQA 1.LM(100%) 40.23.25.10.1.LM (90%)-MT (10%) 39.25.26.11.1.LM (50%)-MT (50%) 41.29.30.13.1.WARMUP 39.23.25.12.1.EXP42.30.31.16.1.FAIR 41.31.32.18.3.LM (100%) 47.32.34.13.3.3.EXPFAIR 50.42.43.25.47.36.37.26.표 3: EM으로 측정한 TyDiQA 및 XTyDiQA의 성능. 정적 데이터 선택 전략은 자동화된 커리큘럼보다 성능이 뛰어납니다. 병렬 데이터를 추가해도 영어 성능에 영향을 미치지 않으며 다른 언어 및 (교차 언어) 오픈 생성에서 폐쇄형 생성 성능이 크게 향상됩니다. 알고리즘 1 FAIR 요구 사항: 탐색 비율 y, 이동 평균 비율 μ, 팔 수 n -1: 팔 가중치 초기화: wa 10¯&#39; Wa Wa + Y n 2: 정책 계산: &quot;a = (1 − y) 3: 팔 샘플링: a ~π 및 보상 ra 얻기 4: 가중치 업데이트: Wa ← (1 - μ)ωα + μια는 정책을 LM 팔에 중심화하는 경향이 있습니다. 이 문제를 완화하기 위해 주어진 팔에 대한 보상의 이동 평균에 비례하여 샘플링하는 &quot;FAIR&quot; 알고리즘을 제안합니다. 자세한 내용은 알고리즘 1을 참조하십시오. 재현성을 위해 부록에서 커리큘럼 설정에 대한 전체 세부 정보를 제공합니다. 5 실험 결과 5.1 기준선 고려하는 첫 번째 기준선은 LM 데이터(LM(100%))에 대해서만 학습하는 것입니다. LM 및 MT 작업 간의 정적 혼합 비율 \에 대한 그리드 검색은 엄청나게 비용이 많이 들기 때문에 (Kale et al., 2021): λ = 0.(LM(50%) MT(50%)) 및 \ = 0.1(LM(90%) -MT(10%)). MT 목표를 보다 적극적으로 샘플링하는 = 0.5와 보다 보수적으로 샘플링하는 X = 0.1 사이의 중간 동작을 만들기 위해 처음 20k 단계에 \ 0.4를 사용한 다음 기본값인 \ = 0.1로 설정하는 WARMUP 휴리스틱을 고려합니다. 값 \은 학습 시작 시 각 작업의 보상을 검사하여 선택했습니다. = = 0. 모델 크기가 1.2B일 때 병렬 데이터를 추가하면 평가된 작업에서 성능이 향상되는 것을 확인했습니다. 그러나 자동화된 커리큘럼 학습 전략이 다른 기준선보다 성능이 우수했습니다. 따라서 제한된 실험 예산을 감안할 때 모델 크기가 3.8B일 때 LM(100%)만 기준으로 고려합니다. 5.2 질의 응답 질의 응답에 대한 결과는 표 3에 나와 있습니다. TyDiQA의 경우 병렬 데이터를 추가하면 영어가 아닌 부분에서 성능이 크게 향상되고 영어 성능은 저하되지 않음을 알 수 있습니다. XTyDiQA의 경우 병렬 데이터를 추가하면 모델 크기 12억에서 최대 +8 EM 포인트, 매개변수 38억에서 +12 EM 포인트로 상당한 차이를 만들 수 있음을 관찰했습니다. 따라서 사전 학습 중에 언어 간 감독을 포함하면 질의 응답 과제에 대한 사전 학습된 모델의 오픈 제너레이션 능력이 향상됨을 알 수 있습니다. 또한 자동화된 커리큘럼(EXP3 또는 FAIR)이 모델 크기 1.2B에서 다른 모든 데이터 샘플링 전략보다 성능이 우수함을 알 수 있으므로 더 큰 모델 크기 3.8B에서 EXP3와 FAIR만 실험합니다. 5.3 요약 표 4는 Wikilingua의 요약 과제에 대한 주요 결과를 보여줍니다. 자동화된 커리큘럼 기반 기술을 수동 혼합 방법과 대조했습니다. 질의응답과 비교했을 때 요약 결과는 크게는 아니지만 자동화된 커리큘럼 방법이 바닐라 LM 전용 방법보다 성능이 뛰어나고 제안된 FAIR 방법이 다른 방법보다 성능이 뛰어난 더 큰 모델 쪽으로 기울어집니다. 흥미롭게도, 자동화된 커리큘럼 방법이 바닐라 LM 전용 방법보다 모델 크기를 확장하는 데 더 많은 이점이 있는 LM(100%)의 매개변수를 12억에서 38억으로 확장할 때 아무런 이득도 관찰하지 못했습니다. 모델 크기(B) 데이터 선택 En Non-En 모두 1.LM(100%) 16.12.37 12.1.LM(90%)-MT(10%) 15.12.12.1.1.LM(50%) MT(50%) WARMUP 14.92 11.12.15.11.12.1.EXP15.12.25 12.1.FAIR 14.11.11.3.LM(100%) 16.12.12.3.3.EXPFAIR 17.08 13.38 13.18.15 14.14.표 4: Wikilingua에서 RougeL로 평가한 요약 성능. 매개변수가 1.2B인 경우 병렬 데이터를 더 추가하면 성능이 약간 떨어질 수 있습니다. 그러나 매개변수가 3.8B인 경우 병렬 데이터를 추가하면 LM 전용 기준선보다 성능이 약간 향상됩니다. 모델 크기(B) 데이터 선택 En→ High High → En En Low Low → En 1.LM(100%) 8.15.0.3.1.LM(90%)-MT(10%) 12.20.2.5.1.LM(50%)-MT(50%) 17.27.5.13.1.WARMUP 10.21.1.6.1.EXP16.26.4.18.1.FAIR 23.31.15.26.3.LM(100%) 12.21.1.5.3.EXP26.34.23.31.3.FAIR 30.36.27.36.표 5: sacreBLEU로 측정한 MT 작업(Flores)의 성능. 병렬 데이터를 추가하면 변환 결과(상황 내 학습으로 평가)가 크게 개선되며, FAIR bandit이 최상의 데이터 선택 전략입니다. 5.4 기계 번역 커리큘럼 학습은 성과를 향상시킵니다. 기계 번역(표 5)의 경우 분석을 네 가지 설정, 즉 영어에서 영어로(X→En, En→X) 번역과 높고 낮은 리소스 번역으로 분할합니다. 자동화된 커리큘럼 방법을 사용하면 상당한 이득을 얻을 수 있으며, En→ Low 설정에서 LM(50%)-MT(50%) 샘플링에 비해 +10 BLEU 포인트가 증가하는 등 상당한 이득을 얻을 수 있습니다. 다른 방법과 비교했을 때, 제안하는 FAIR 알고리즘은 생성 품질도 더욱 향상시킵니다. 제한된 실험 예산을 감안할 때 모델 크기가 3.8B인 자동화된 커리큘럼 전략만 고려했습니다. 제어 토큰을 사용한 번역. 병렬 데이터는 MT 목표와 함께 감독 방식으로 사용되었습니다. 각 언어에 대해 특수 제어 토큰이 소스 문장 앞에 접두사로 붙었습니다(1). 이러한 언어 제어 토큰은 LM 학습 데이터에 나타나지 않으므로 사전 학습된 모델에 (1) 형식으로 데이터를 제공하면 원하는 언어로 번역되는지 여부가 자연스러운 의문입니다. 실제로 그렇습니다. 추론 시점에 사전 학습된 LM은 각 언어 제어 토큰에 해당하는 감독 작업을 수행합니다. 제어 토큰을 사용하는 것보다 문맥 내 학습을 통해 더 나은 번역이 생성됩니다. 자연스러운 질문은 결과 모델이 문맥 내 학습을 통해 더 나은 번역을 생성하는지 아니면 제어 토큰을 사용하는지입니다. 표 6에서 제어 토큰(C)을 사용한 &quot;번역 모드&quot;를 문맥 내 학습을 사용한 모드와 비교합니다. 문맥 내 학습의 경우 원샷 설정(O)을 사용합니다. 작업이 어떻게 구성되는지에 대한 예는 부록을 참조하세요. 원샷 설정이 제어 토큰을 사용한 설정보다 성능이 우수하다는 것을 분명히 알 수 있습니다. 단, En → Low 설정에서는 두 번째 설정이 선호됩니다. MTM 및 LLM 기준선과의 비교. 공정한 비교를 위해 동일한 spBLEU 구현을 사용하여 (Lin et al., 2021)에서 보고된 결과와 결과를 비교합니다. 첫 번째 기준선인 M2M(Fan et al., 2021)은 교차 언어적 감독을 통해 훈련된 MTM입니다. 다른 두 기준선인 XGLM(Lin et al., 2021)과 GPT-3(Brown et al., 2020)은 추가적인 교차 언어적 감독 없이 자체 감독 목표에 대해 훈련된 LLM으로 구성되며, 번역은 few-shot 설정에서 맥락 내 학습을 사용하여 얻습니다. 우리는 또한 병렬 데이터가 있는 언어 쌍을 고려하여 비교를 수행합니다(표 7). 우리 모델의 명확한 장점은 우리가 모든모델 크기 데이터 선택(B) 변환 모드 En High High → En En → Low Low → En 1.LM(100%) 8.15.0.3.1.LM(90%)-MT(10%) C 6.9.2.4.1.LM(90%) - MT(10%) 12.20.2.5.1.1.1.1.LM(50%) - MT(50%) 14.14.15.10.LM(50%) -MT(50%) 17.27.5.13.EXP15.15.13.9.EXP16.26.4.18.1.1.FAIR C 22.19.34.16.FAIR 23.31.15.26.3.LM(100%) O 12.21.1.5.3.EXP19.21.34.18.3.EXP26.34.23.31.3.FAIR C 21.15.32.16.3.FAIR O 30.36.27.36.표 6: sacreBLEU로 측정한 MT 작업(Flores)에서 제어 토큰(C)을 사용한 번역과 원샷 설정(O)을 비교한 결과. 원샷 설정은 제어 토큰을 사용하는 것이 더 나은 En → Low 설정을 제외하고는 더 나은 결과를 제공합니다. 방법은 제어 토큰으로 생성된 번역과 컨텍스트 내 학습(원샷) 사이에서 가장 높은 점수를 받습니다. En→Fi를 제외하고 1.2B와 3.8B 매개변수 규모에서 다른 모든 모델을 능가합니다. 또한 병렬 데이터가 없는 (Lin et al., 2021)에 보고된 언어 쌍도 살펴보았습니다. My와 Ta 언어의 경우 LM 데이터 세트에는 데이터가 거의 없었고 모델은 번역할 수 없었습니다. Ca, Bg, Hi의 경우 시스템에서 번역을 생성하는 것을 발견했습니다. 대상 언어가 영어인 경우 Bg→En, Hi→En, Zh→En 방향에서 다른 시스템보다 성능이 우수하므로 성능이 매우 좋을 수 있습니다. 이 평가에 대한 자세한 내용은 부록에 보고되어 있습니다. 5.5 다국어 데이터를 더 많이 사용하여 이득이 발생했습니까? (Chowdhery et al., 2022)의 데이터의 약 77%가 영어인 점을 감안할 때 병렬 데이터를 추가하면 데이터의 비영어 부분이 늘어나므로 유익하다는 것이 자연스러운 추측입니다. 이 가설을 테스트하기 위해 MT 데이터의 비영어 측면을 가져와 LM 목표를 적용하여 새로운 데이터 세트를 구성합니다. 여전히 자동화된 커리큘럼 학습을 사용하여 LM 데이터의 두 부분을 균형 있게 조정합니다. 표 8에서 두 가지 접근 방식을 비교하고 LM 목표와 함께 MT 데이터를 사용할 때 질의응답과 기계 번역 모두에서 성능이 크게 떨어지는 것을 관찰합니다. 우리는 우리의 MT 데이터가 (Chowdhery et al., 2022)의 더 풍부한 종류의 데이터에 비해 언어 모델링에 덜 유용할 것이라고 추측합니다.
--- CONCLUSION ---
s 우리는 Encoder-Decoder 대규모 언어 모델을 사전 학습할 때 학습 목표에 교차 언어적 감독을 포함하는 것이 유익하다는 것을 보여주었습니다. 특히, 기계 번역 및 질의응답에서 결과 모델을 평가할 때 상당한 이득을 발견했습니다. 병렬 데이터를 포함하는 것의 한 가지 단점은 사용할 이러한 데이터의 백분율을 정량화하는 새로운 하이퍼 매개변수를 도입한다는 것입니다. 일부 교차 언어적 감독을 포함하는 것이 유익하더라도 그리드 검색으로 최적의 양을 결정하는 것은 실행 불가능합니다. 그러나 우리는 멀티암드밴딧(Graves et al., 2017)을 사용하여 자동화된 커리큘럼 학습을 채택하면 좋은 결과를 얻을 수 있음을 보여주었습니다. 더욱이, 제안된 접근 방식에서 학습된 백분율은 학습 중에 조정될 수 있으며 (Kale et al., 2021)의 정적 데이터 샘플링 기준선보다 성능이 우수합니다. 7 제한 사항 컴퓨팅 제한으로 인해 Encoder-Decoder 모델만 조사했습니다. 결과를 Decoderononly 모델로 확장하려면 추가 실험이 필요합니다. 우리의 요약 평가는 매개변수 수를 늘릴수록 개선이 있음을 나타내므로 더 큰 모델을 사용한 추가 실험 모델 변환 모드 Ar→En De→En En→Ar En→De En→Fi En→Fr En→Ko M2M-124(0.6B) 25.35.17.32.24.42.18.GPT-3(6.7B) XGLM(7.5B) 10.40.1.25.10.36.1.27.38.11.27.23.36.12.Ours(1.2B) Ours(1.2B) 20.32.18.34.18.45.10.35.44.19.28.13.40.14.Ours(3.8B) 15.24.20.27.19.34.10.Ours(3.8B) 41.46.25.39.21.49.22.모델 번역 모드 En→Ru En→Sw Fi→En Fr→En Ko→En Ru→En Sw→En M2M-124(0.6B) 27.26.27.37.20.27.30.GPT-3(6.7B) 11.0.25.42.8.28.5.XGLM(7.5B) 우리(1.2B) 24.18.29.40.19.30.31.23.29.16.35.10.22.16.우리(1.2B) 우리(3.8B) 우리(3.8B) 24.16.27.45.27.35.28.20.34.18.25.10.20.15.31.26.35.47.33.37.40.표 7: 훈련 중에 병렬 데이터를 포함한 언어 쌍에 대한 MT 기준선과의 비교(Flores/spmBLEU). 각 언어 쌍에 대해 제어 토큰(C)과 문맥 내 학습(O) 사이에서 최상의 전략을 선택하면 기준선을 능가할 수 있습니다. 데이터 선택. LM Parallel as TyDi QA XTyDi QA En → High High → En En → Low Low → En Wikilingua 25.10.8.15.0.3.12.EXPMT 31.16.16.26.4.18.12.EXPLM 29.12.10.16.1.4.11.FAIR MT 32.18.23.31.15.26.11.FAIR LM 18.8.8.15.1.5.10.Table 8: LM 목적과 함께 병렬 데이터를 사용하는 Ablation(모델 크기 1.2B에서). 각 데이터 선택 전략에 대해 LM 목적과 함께 영어가 아닌 크기의 병렬 데이터를 사용하면 (X)TyDi QA 및 Translation의 성능이 크게 감소합니다. (예: &gt; 8B 매개변수) 사전 학습 중에 병렬 데이터를 추가하여 얻은 이점을 보다 정확하게 정량화하려면 필요할 수 있습니다. 마지막으로, 자동화된 커리큘럼 학습이 단순한 정적 데이터 샘플링 전략보다 성과가 좋았지만, 더 정교한 샘플링 접근 방식이 더 나은 결과를 낼 수도 있습니다. 참고문헌 Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. 2002. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Xia Song, Furu Wei. 2021. mT6: 번역 쌍을 사용한 다국어 사전 학습된 텍스트-텍스트 변환기. 2021 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 1671-1683페이지, 온라인 및 도미니카 공화국 푼타카나. 계산 언어학 협회. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. Jonathan H. Clark, 최은솔, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev 및 Jennimaria Palomaki. 2020. TyDi QA: 유형적으로 다양한 언어로 정보를 찾는 질문에 답하는 벤치마크입니다. Transactions of the Association for Computational Linguistics, 8:454– 470. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, Armand Joulin. 2021. 영어 중심의 다국어 기계 번역을 넘어서. Journal of Machine Learning Research, 22(107):1-48. Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish Upadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You, Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip Ginter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez, João Sedoc, Juraj Juraska, Kaustubh D. 돌, 카야티 Raghavi Chandu, Laura Perez-Beltrachini, Leonardo FR Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz, Michael White, Mihir Sanjay Kale, Moussa Kamal Eddine, Nico Daheim, Nishant Subramani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi, Qi Zhu, Ratish Puduppully, 리노 크리즈, 리파트 샤리야르, 로널드 카르데나스, 사드 마하무드, 살로메 오세이, 사무엘 카야위자야, 산자 스타이너, 세바스티앙 몬텔라, 샤일자 졸리, 사이먼 밀레, 타흐미드 하산, 티안하오 셴, 토신 P. 아마히데우미, 비카스 라우낙, 비풀 라헤자, 비탈리 니콜라예프, 비비안 차이, 야신 제르니테, 잉 쉬(Ying Xu), 이상 이상(Yisi Sang), Yixin Liu, Yufang Hou. 2022. Gemv2: 한 줄의 코드에서 다국어 nlg 벤치마킹. CORR, abs/2206.11249. Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, Koray Kavukcuoglu. 2017. 신경망을 위한 자동화된 커리큘럼 학습. 제34회 국제 기계 학습 컨퍼런스 회의록, 기계 학습 연구 회의록 70권, 1311-1320쪽. PMLR. Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, Marc&#39;Aurelio Ranzato. 2019. 저소득 기계 번역을 위한 FLORES 평가 데이터 세트: 네팔어-영어 및 싱할라어, 영어. 2019년 자연어 처리 경험적 방법에 대한 컨퍼런스와 자연어 처리에 대한 제9회 국제 공동 컨퍼런스(EMNLP-IJCNLP)의 회의록, 6098-6111쪽, 중국 홍콩. 전산 언어학 협회. ProMihir Kale, Aditya Siddhant, Rami Al-Rfou, Linting Xue, Noah Constant, Melvin Johnson. 2021. nmT5 - 병렬 데이터가 대규모 다국어 언어 모델의 사전 학습에 여전히 관련이 있을까? 전산 언어학 협회의 제59회 연례 회의와 자연어 처리에 대한 제11회 국제 공동 컨퍼런스(제2권: 단편 논문)의 회의록, 683691쪽, 온라인. 전산 언어학 협회. Julia Kreutzer, David Vilar, Artem Sokolov. 2021. 산적은 규칙을 따르지 않는다: 다중 측면 기계 번역과 다중 무장 산적의 균형. 계산 언어학 협회의 연구 결과: EMNLP 2021, 3190-3204페이지, 도미니카 공화국 푼타카나. 계산 언어학 협회. Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen McKeown. 2020. WikiLingua: 언어 간 추상 요약을 위한 새로운 벤치마크 데이터 세트. 계산 언어학 협회의 연구 결과: EMNLP 2020, 40344048페이지, 온라인. 계산 언어학 협회. Tor Lattimore와 Csaba Szepesvári. 2020. 산적 알고리즘. 케임브리지 대학교 출판부. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O&#39;Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, 베셀린 스토야노프, 시안 리. 2021. 다국어 언어 모델을 사용한 퓨샷 학습. arXiv 전자 인쇄물(EMNLP에 표시됨), 페이지 arXiv:2112.10668. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li 및 Peter J. Liu. 2019. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. arXiv 전자 인쇄물. Machel Reid와 Mikel Artetxe. 2022. PARADISE: 다국어 시퀀스-시퀀스 사전 학습을 위한 병렬 데이터 활용. 2022년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 800-810쪽, 미국 시애틀. 컴퓨터 언어학 협회. Adam Roberts, 정형원, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James LeeThorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy MaitinShepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan 세파시, 알렉산더 스피리도노프, Joshua Newlan, and Andrea Gesmundo. 2022. t5x와 seqio를 사용한 모델 및 데이터 확장. arXiv 사전 인쇄본 arXiv:2203.17189. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2022. U12: 언어 학습 패러다임 통합. David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. 번역을 위한 PaLM 촉진: 전략 및 성과 평가. arXiv 전자 인쇄본, 페이지 arXiv:2211.09102. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: 대규모 다국어 사전 학습된 텍스트-텍스트 변환기. Association for Computational Linguistics: Human Language Technologies의 북미 지부 2021년 회의록, 483-498쪽, 온라인. Association for Computational Linguistics. 학습 하이퍼 매개변수 1.2B 모델을 250k 단계로 학습하고 3.8B 모델을 500k 단계로 학습합니다. T5X와 SeqIO를 사용합니다. 입력 시퀀스는 각 배치에 대해 500k가 조금 넘는 비패딩 토큰이 있는 패킹을 사용합니다. 학습률은 제곱근 감소를 사용하며 기본 학습률은 1.0이고 워밍업 단계는 10k입니다. T5X 라이브러리의 기본 Adafactor 최적화 도구를 사용합니다.B 자동 커리큘럼 학습을 사용하기 위한 기술적 세부 정보 여기서는 커리큘럼 학습을 올바르게 사용하기 위한 몇 가지 중요한 기술적 세부 정보를 보고합니다.(Graves et al., 2017; Kreutzer et al., 2021) 재조정된 보상 사용: 1. 마지막 T 보상의 우선순위 대기열을 유지합니다. 대기열이 x% 채워지지 않은 동안 재조정된 보상으로 0을 반환합니다(따라서 밴딧 알고리즘은 학습하지 않음).2. 대기열이 x% 채워지면 20번째와 80번째 분위수를 계산합니다.3. 새로운 보상 r이 오면 현재 20번째와 80번째 분위수 사이에 오도록 잘라서 r&#39;을 얻습니다. 그런 다음 r&#39;을 r&quot; E [−1,1]로 선형적으로 재조정합니다. 여기서 −1은 20번째 분위수에 해당하고 +1은 80번째 분위수에 해당합니다. 4. 밴딧 알고리즘에 &quot;을 제공합니다. 5. r을 큐에 넣고 20번째와 80번째 분위수를 다시 계산합니다. 따라서 (Graves et al., 2017; Kreutzer et al., 2021)은 [−1, 1]에서 유효 보상을 사용합니다. 그러나 (Auer et al., 2002; Lattimore and Szepesvári, 2020)에서 EXP3에 대한 수렴 증명은 음의 보상에서는 작동하지 않습니다. 또한 FAIR은 보상이 음수이면 음의 확률 가중치를 생성할 수 있습니다. 따라서 r&quot; Є [0, 1] 및 0이 큐의 20번째 분위수에 해당하도록 보상을 재조정합니다. 우리는 길이가 T 5000이고 x 10%인 대기열을 사용합니다. 또한 (Graves et al., 2017)이 EXP3S를 사용한다고 주장하지만, 매개변수를 선택하면 항상 EXP3로 기본 설정됩니다. 따라서 혼동을 줄 수 있으므로 이 작업에서는 EXP3S를 언급하지 않습니다. _ = C 커리큘럼 학습을 위한 하이퍼 매개변수 EXP3의 경우 학습률을 10-3으로, 탐색률을 25%로 설정합니다. FAIR의 경우 탐색률을 10%로 설정하고 μ를 10-2로 설정합니다. μ는 이동 평균을 업데이트하는 데 작동하므로 우리의 선택은 100단계의 시간적 지평에 해당합니다. D 원샷 작업 형식 우리의 모델은 원샷 패러다임을 사용하여 컨텍스트 내 학습으로 평가됩니다. 다음은 다양한 작업에 대한 공식화의 예입니다. ... 질문에 대한 답변의 경우 입력은 &quot;맥락: 유럽 자칼 ...\n\n 질문: 자칼은 몇 마리입니까?\n 답변: 70,000\n\n 맥락: 최초로 알려진 ... 표본 \n\n 질문: 언제였습니까?\n 답변:&quot; 형식입니다. 굵은 글씨 부분은 맥락 내 학습 패러다임을 사용하기 위해 제공된 단일 예입니다(여기서는 원샷입니다). 기계 번역의 경우 입력 형식은 다음과 같습니다. &quot;독일어: Am 28. Juni wurde Marshall Italo Balbo,...\n 영어: 6월 28일, Marshal Italo Balbo,...\n\n 독일어: Dr. Ehud Ur, Professor für Medizin...\n \n 영어:”. 굵은 글씨 부분은 맥락 내 학습 패러다임을 사용하기 위해 제공된 단일 예입니다(여기서는 원샷). 요약의 경우 입력은 다음과 같습니다. &quot;먼저 단계별 지침을 보여준 다음 지침과 동일한 언어로 모든 단계에 대한 간략한 요약을 작성합니다. \n \n 다음 지침을 요약하세요. 외로움은 \n 요약: 외로움의 유형을 파악하세요. 외로움이 감정이라는 것을 깨달으세요. 자신의 성격을 고려하세요. 외로움을 느끼는 사람이 당신 혼자가 아니라는 것을 인식하세요. \n \n 다음 지침을 요약하세요. 일반적으로 무지개는 ... \n 요약:&quot;. 굵은 글씨 부분은 컨텍스트 내 학습 패러다임을 사용하기 위해 제공된 단일 예입니다(여기서는 원샷). 기울임꼴 부분은 GEM 벤치마크에서 사용하는 프롬프트입니다. E 제어 토큰으로 번역하기 특수 언어 토큰이 입력에 접두사로 붙으면 모델은 문장을 원하는 언어로 번역합니다. 이 경우 입력은 다음과 같은 형식을 갖습니다. “&lt;2en&gt; Dr. Ehud Ur, Professor für Medizin ...&quot;, 여기서 &quot;&lt;2en&gt;&quot;은 영어로 번역하는 작업을 나타냅니다. F 병렬 데이터가 없는 언어의 MTM 및 LLM 기준선과의 비교 작업 X에서 작업 Y로의 0.Reward가 없는 언어에 대한 MTM 및 LLM 기준선과의 비교를 보고합니다.0.LM_to_LM LM_to_MT MT_to_MT MT_to_LMStep 그림 1: 작업 X(범례의 X에서 Y)에 대한 그래디언트 단계에서 발생하는 작업 Y의 보상으로 전이 측정. 학습 시작 시 MT 자체에 대한 보상은 낮고 시간이 지남에 따라 증가합니다. MT에서 LM으로의 상당한 전이가 항상 있습니다. 분산을 줄이기 위해 창 500의 실행 평균을 사용합니다. 표 9에서 학습 중 병렬 데이터를 사용합니다. 학습 시점에 제공된 병렬 데이터에 대상 언어가 없는 경우 언어 제어 토큰으로 시스템을 평가할 수 없다는 점에 유의하세요. 따라서 원샷 설정에서 평가합니다. 시스템은 LM 데이터에서 극히 표현되지 않은 언어 My 및 Ta에 어려움을 겪습니다. 다른 언어의 경우 모델이 번역할 수 있다는 표시가 있습니다.게다가 영어로의 번역 성능은 매우 좋을 수 있으며, 예를 들어 특히 Bg En, Hi→En 및 Zh→En 쌍에서 다른 시스템보다 성능이 뛰어납니다.또한 FAIR로 학습한 모델이 LM 데이터로만 학습한 모델보다 성능이 우수하다는 점에 유의하십시오.따라서 병렬 데이터는 교차 언어 지도 데이터에 없는 언어 쌍으로도 번역을 개선했습니다.G 학습 중 보상 시각화 그림 1에 보상 측면에서 측정된 작업 간 전이를 표시합니다.특히 작업 X에서 기울기 단계를 수행하고 작업 Y에서 보상을 평가했을 때 X에서 Y로의 전이를 측정하여 시간에 따라 표시할 수 있습니다.MT에서 LM 목적으로는 항상 긍정적인 전이가 있지만 LM 목적은 MT 또는 LM에 적용하면 평균적으로 보상이 더 높습니다.흥미롭게도 MT에서 자체로의 전이는 학습 시작 시 낮았지만 시간이 지남에 따라 증가했습니다. 모델 M2M-124(0.6B) Bg→En Ca En En Bg En→Ca En Hi En→My 33.33.37.31.28.3.GPT-3(6.7B) 21.40.5.23.0.0.XGLM(7.5B) 35.41.33.34.19.11.Ours(FAIR, 3.8B) 36.39.16.19.8.0.당사(100% LM, 3.8B) 25.29.12.14.3.0.모델 En→Ta En→Zh | 안녕→엔미엔타→엔 | Zh→En M2M-124(0.6B) 3.19.27.10.8.20.GPT-3(6.7B) 0.12.1.0.21.XGLM(7.5B) 8.15.25.14.16.20.Ours(FAIR, 3.8B) 0.12.28.2.6.25.Ours LM만(100% LM, 3.8B) 0.8.11.2.3.17.표 9: 훈련 중에 병렬 데이터를 포함하지 않은 언어 쌍에 대한 MT 기준선과의 비교(Flores/spmBLEU). 우리 시스템에서 병렬 데이터를 추가하면 병렬 데이터에 포함되지 않은 언어 쌍에서도 원샷 번역 성능이 향상됩니다.
