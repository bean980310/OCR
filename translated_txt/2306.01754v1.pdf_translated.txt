--- ABSTRACT ---
Microsoft Redmond, USA 소프트웨어 취약점은 기업에 상당한 비용을 초래합니다. 소프트웨어 취약점 탐지 방법에 대한 연구 및 개발에 대한 광범위한 노력에도 불구하고, 발견되지 않은 취약점은 소프트웨어 소유자와 사용자를 계속 위험에 빠뜨립니다. 현재 많은 취약점 탐지 방법은 탐지를 시도하기 전에 코드 스니펫을 컴파일하고 빌드할 수 있어야 합니다. 불행히도 이는 취약점이 주입되는 시간과 제거되는 시간 사이에 긴 대기 시간을 초래하여 취약점을 수정하는 데 드는 비용을 상당히 증가시킬 수 있습니다. 우리는 머신 러닝의 현재 발전을 사용하여 개발자가 EditTime에서 코드를 작성할 때 구문적으로 불완전한 코드 스니펫에서 취약점 코드 패턴을 탐지할 수 있다는 것을 알고 있습니다. 이 논문에서는 250개 이상의 취약점 유형의 복잡한 표현을 학습하고 EditTime에서 취약점 코드 패턴을 탐지하기 위해 대규모 취약점 코드 패턴 데이터 집합에서 딥 러닝을 활용하는 실용적인 시스템을 제시합니다. 최첨단 사전 학습된 대규모 언어 모델(LLM)에 대한 제로샷, 퓨샷 및 미세 조정 접근 방식을 논의합니다. 최신 취약성 탐지 모델과 비교했을 때 우리의 접근 방식이 최신 기술을 10% 향상시킨다는 것을 보여줍니다. 또한 코드 LLM별로 자동 생성된 코드에서 취약성을 탐지하는 접근 방식을 평가합니다. 고위험 코드 시나리오의 벤치마크에 대한 평가는 최대 90%의 취약성 감소를 보여줍니다. 키워드 변압기, 소프트웨어 취약성, 취약성 탐지
--- INTRODUCTION ---
많은 도구와 모범 사례가 개발되었음에도 불구하고[3, 4, 12, 47], 발견되지 않은 취약점은 코드에 계속 존재하며[1, 2] 사용자에게 영향을 미치고 회사에 시간과 비용을 낭비하게 합니다[19]. 최근 몇 년 동안 소프트웨어의 취약점을 탐지하기 위한 많은 접근 방식이 개발되었습니다. 이러한 접근 방식은 기존 동적 분석[25, 28, 48], 규칙 기반 정적 분석[9, 51], 기능 기반 머신 러닝 솔루션[33]에서 최근의 딥 러닝 기반 솔루션[8, 30, 41, 53]까지 다양합니다. 동적 분석에 의존하는 접근 방식은 종종 코드 커버리지가 낮은 문제로 어려움을 겪습니다. 규칙 기반 정적 분석 접근 방식은 일반적으로 전문가가 정기적으로 새롭거나 진화된 취약점 패턴을 특성화하고 추가하기 위해 수동으로 작업해야 합니다. 마찬가지로 기능 기반 머신 러닝 접근 방식은 인간 전문가가 직접 만든 기능에 의존합니다. 최근에는 전문가의 개입 없이도 취약점 패턴을 자동으로 학습할 수 있는 딥 러닝 기반 접근 방식이 대안으로 등장했습니다. 그러나 이러한 딥 러닝 접근 방식의 대부분은 분석할 완전한 소스 파일[39], 완전한 함수[23, 24, 41] 또는 완전한 코드 문장[11, 43]이 필요합니다. 결과적으로 우리가 아는 한, 기존의 딥 러닝 접근 방식은 개발자가 코드를 입력하고 있고 코드가 아직 구문적으로 올바르지 않거나 완전하지 않은 EditTime에서 취약점을 찾을 수 없습니다. app.get(&#39;name&#39;, function(req, res, next){ // url 매개변수를 기반으로 계정 쿼리 connection.query(&#39;SELECT * FROM accounts WHERE name=&quot;req.params.name + 그림 1: SQLInjection 취약점에 대해 개발자에게 알리는 가장 좋은 시기는 EditTime, 즉 개발자가 실수를 저지른 직후입니다. 문헌에 따르면 오류를 수정하는 데 드는 비용은 오류 무시 시간(즉, 취약점이 주입되는 시간에서 제거되는 시간)과 양의 상관 관계가 있습니다[5, 6, 20]. 따라서 오류가 현재 프로그래밍 작업과 관련이 있을 때 적시에 개발자에게 오류를 알려야 합니다[22]. 예를 들어, 그림 1에 표시된 코드 조각에서 개발자에게 SQL 주입을 도입한 사실을 알리는 가장 좋은 시기는 코드가 구문적으로 완전하지 않더라도 쿼리의 끝부분입니다. EditTime에서 코드의 버그와 취약점을 대화형으로 식별하는 것의 중요성은 다음과 같이 인식되었습니다. 이전 작업[50]. 저희의 작업은 개발자가 타이핑하는 동안 불완전한 코드 조각에서 다양한 취약점을 탐지하기 위해 딥 러닝의 새로운 발전을 활용하여 이러한 이전 작업 라인을 확장합니다.취약성 탐지 솔루션을 구축하기 위해 오픈 소스 저장소에서 정적 분석기를 실행하여 500K개 이상의 취약한 코드 조각을 수집합니다.그런 다음 사전 훈련된 LLM에 대한 일반적인 학습 접근 방식을 사용하여 6가지 모델 변형을 개발합니다.제로 샷, 퓨 샷 및 미세 조정.이러한 학습 기술을 CodeBERT[15]와 OpenAI에서 제공하는 두 가지 최첨단 사전 훈련된 LLM인 code-davinci-002, text-davinci-003에 적용합니다.이러한 변형에 대한 비교 평가 결과 미세 조정 CodeBERT가 정밀도와 재현율 사이에서 최상의 균형을 제공한다는 것을 보여줍니다(정밀도 59%, 재현율 63%). 그러나 InstructGPT[35] 기반 언어 모델인 최근의 text-davinci-003에 대한 zero-shot 및 few-shot 학습은 모두 더 나은 회수율(78% 및 75%)과 약간 낮은 정밀도(47% 및 49%)를 제공합니다. 우리는 모델 크기와 성능 측면에서 각 학습 접근 방식의 이점과 과제를 논의합니다. 우리는 가장 성능이 좋은 모델을 사용하여 두 가지 실험을 수행합니다. 첫 번째 실험은 이 모델을 4개의 기존 벤치마크 데이터 세트에서 기존 취약성 탐지 접근 방식과 비교합니다. 우리는 기존 접근 방식과 비교할 때 우리의 접근 방식이 회수율을 최대 10%, 정밀도를 최대 8% 향상시킨다는 것을 보여줍니다. 두 번째 실험은 개발자의 수동 코드 편집을 넘어서는 우리 접근 방식의 일반화 가능성을 연구하며, 자동화된 코드 편집에서 취약성을 탐지하는 데 있어 우리 접근 방식의 효과를 조사합니다. 최근 코드 완성 도구[17, 45, 46]가 널리 채택됨에 따라 향후 많은 코드 편집이 코드 LLM에 의해 자동으로 생성될 것으로 예상됩니다. 이전 연구에 따르면 코드 LLM은 개발자와 비슷한 실수를 저지른다고 합니다[14]. 따라서 자동 생성된 코드 편집은 잠재적으로 심각한 보안 문제를 일으킬 수 있습니다. 예를 들어, 특정 시나리오에서 코드 LLM에서 생성된 완성의 최대 40%에 취약한 코드 패턴이 포함되었습니다[38]. Pearce 등이 도입한 벤치마크 변형에서 취약성 탐지 모델을 평가했습니다[38]. 평가 결과 모델을 사용하면 취약성 비율이 90% 감소합니다. 마지막으로 확장 기능으로 편집된 코드에서 취약성 비율이 80% 감소한 VSCode 확장 기능에서 취약성 탐지 모델의 배포 과정에 대해 설명합니다. 이 논문은 다음과 같은 기여를 합니다.(1) 우리는 밀리초 단위로 불완전한 코드 조각에서 취약점을 식별하여 EditTime 개발자에게 제공할 수 있는 프로덕션 품질의 취약점 탐지 시스템을 제시합니다.(2) 취약점 탐지 작업에서 사전 훈련된 LLM을 위한 세 가지 일반적인 학습 방식인 제로샷, 퓨어샷, 파인튜닝의 이점과 과제를 살펴보고 논의합니다.(3) Pearce 등이 도입한 벤치마크를 확장하여 향후 EditTime 벤치마크로 코드 취약점 탐지 커뮤니티에서 사용할 수 있도록 합니다.
--- RELATED WORK ---
우리는 취약성 탐지, 심층 학습 분야에서 우리의 작업이 어떻게 이전 작업을 기반으로 구축되고 확장되는지 논의합니다.
--- METHOD ---
s, 포착되지 않은 취약성은 소프트웨어 소유자와 사용자를 계속 위험에 빠뜨립니다. 많은 현재 취약성 탐지 방법은 탐지를 시도하기 전에 코드 스니펫을 컴파일하고 빌드할 수 있어야 합니다. 불행히도 이는 취약성이 주입되는 시간과 제거되는 시간 사이에 긴 대기 시간을 초래하여 취약성을 수정하는 데 드는 비용을 상당히 증가시킬 수 있습니다. 우리는 머신 러닝의 현재 발전을 사용하여 개발자가 EditTime에서 코드를 작성할 때 구문적으로 불완전한 코드 스니펫에서 취약한 코드 패턴을 탐지할 수 있다는 것을 알고 있습니다. 이 논문에서는 취약한 코드 패턴의 대규모 데이터 집합에서 딥 러닝을 활용하여 250개 이상의 취약성 유형의 복잡한 표현을 학습하고 EditTime에서 취약한 코드 패턴을 탐지하는 실용적인 시스템을 제시합니다. 최첨단 사전 학습된 대규모 언어 모델(LLM)에서 제로샷, 퓨샷 및 미세 조정 접근 방식을 논의합니다. 최첨단 취약성 탐지 모델과 비교할 때 우리의 접근 방식이 최첨단을 10% 향상시킨다는 것을 보여줍니다. 또한 코드 LLM에서 자동 생성된 코드의 취약성을 탐지하는 접근 방식을 평가합니다. 고위험 코드 시나리오의 벤치마크에 대한 평가는 최대 90%의 취약성 감소를 보여줍니다. 키워드 변압기, 소프트웨어 취약성, 취약성 탐지 소개 많은 도구와 모범 사례[3, 4, 12, 47]가 개발되었음에도 불구하고, 발견되지 않은 취약성은 코드[1, 2]에 남아 사용자에게 영향을 미치고 회사에 시간과 비용을 낭비하게 합니다[19]. 최근 몇 년 동안 소프트웨어의 취약성을 탐지하기 위한 많은 접근 방식이 개발되었습니다. 이러한 접근 방식은 기존 동적 분석[25, 28, 48], 규칙 기반 정적 분석[9, 51], 기능 기반 머신 러닝 솔루션[33]에서 최근의 딥 러닝 기반 솔루션[8, 30, 41, 53]까지 다양합니다. 동적 분석에 의존하는 접근 방식은 종종 코드 커버리지가 낮은 문제로 어려움을 겪습니다. 규칙 기반 정적 분석 접근 방식은 일반적으로 전문가가 정기적으로 새롭거나 진화된 취약성 패턴을 특성화하고 추가하기 위한 수동 작업을 포함합니다.마찬가지로, 기능 기반 머신 러닝 접근 방식은 인간 전문가가 수작업으로 만든 기능에 의존합니다.최근에는 전문가의 개입 없이 취약성 패턴을 자동으로 학습할 수 있는 대안으로 딥 러닝 기반 접근 방식이 등장했습니다.그러나 이러한 딥 러닝 접근 방식의 대부분은 분석할 완전한 소스 파일[39], 완전한 함수[23, 24, 41] 또는 완전한 코드 명령문[11, 43]이 필요합니다.결과적으로 우리가 아는 한, 기존의 딥 러닝 접근 방식은 개발자가 타이핑하고 코드가 아직 구문적으로 올바르지 않거나 완전하지 않은 동안 EditTime에서 취약성을 찾을 수 없습니다. app.get(&#39;name&#39;, function(req, res, next){ // url 매개변수를 기반으로 계정 쿼리 connection.query(&#39;SELECT * FROM accounts WHERE name=&quot;req.params.name + 그림 1: SQLInjection 취약점에 대해 개발자에게 알리는 가장 좋은 시기는 EditTime, 즉 개발자가 실수를 저지른 직후입니다. 문헌에 따르면 오류를 수정하는 데 드는 비용은 오류 무시 시간(즉, 취약점이 주입되는 시간에서 제거되는 시간)과 양의 상관 관계가 있습니다[5, 6, 20]. 따라서 오류가 현재 프로그래밍 작업과 관련이 있을 때 적시에 개발자에게 오류를 알려야 합니다[22]. 예를 들어, 그림 1에 표시된 코드 조각에서 개발자에게 SQL 주입을 도입한 사실을 알리는 가장 좋은 시기는 코드가 구문적으로 완전하지 않더라도 쿼리의 끝부분입니다. EditTime에서 코드의 버그와 취약점을 대화형으로 식별하는 것의 중요성은 다음과 같이 인식되었습니다. 이전 작업[50]. 저희의 작업은 개발자가 타이핑하는 동안 불완전한 코드 조각에서 다양한 취약점을 탐지하기 위해 딥 러닝의 새로운 발전을 활용하여 이러한 이전 작업 라인을 확장합니다.취약성 탐지 솔루션을 구축하기 위해 오픈 소스 저장소에서 정적 분석기를 실행하여 500K개 이상의 취약한 코드 조각을 수집합니다.그런 다음 사전 훈련된 LLM에 대한 일반적인 학습 접근 방식을 사용하여 6가지 모델 변형을 개발합니다.제로 샷, 퓨 샷 및 미세 조정.이러한 학습 기술을 CodeBERT[15]와 OpenAI에서 제공하는 두 가지 최첨단 사전 훈련된 LLM인 code-davinci-002, text-davinci-003에 적용합니다.이러한 변형에 대한 비교 평가 결과 미세 조정 CodeBERT가 정밀도와 재현율 사이에서 최상의 균형을 제공한다는 것을 보여줍니다(정밀도 59%, 재현율 63%). 그러나 InstructGPT[35] 기반 언어 모델인 최근의 text-davinci-003에 대한 zero-shot 및 few-shot 학습은 모두 더 나은 재현율(78% 및 75%)과 약간 낮은 정확도(47% 및 49%)를 제공합니다. 우리는 모델 크기와 성능 측면에서 각 학습 접근 방식의 이점과 과제를 논의합니다. 우리는 두 가지를 수행합니다.
--- EXPERIMENT ---
우리의 가장 성능이 좋은 모델을 사용합니다.첫 번째 실험은 이 모델을 기존 벤치마크 데이터 세트 4개에서 기존 취약성 탐지 접근 방식과 비교합니다.기존 접근 방식과 비교할 때, 우리의 접근 방식은 회수율을 최대 10%, 정밀도를 최대 8%까지 개선한다는 것을 보여줍니다.두 번째 실험은 개발자의 수동 코드 편집을 넘어서는 우리 접근 방식의 일반화 가능성을 연구하며, 여기서 우리는 자동화된 코드 편집에서 취약성을 탐지하는 데 있어 우리 접근 방식의 효과를 조사합니다.최근 코드 완성 도구[17, 45, 46]가 널리 채택됨에 따라, 많은 향후 코드 편집이 코드 LLM에 의해 자동으로 생성될 것으로 예상됩니다.이전 연구에 따르면 코드 LLM은 개발자와 유사한 실수를 저지른다고 합니다[14].따라서 자동 생성된 코드 편집은 잠재적으로 심각한 보안 문제를 가질 수 있습니다.예를 들어, 특정 시나리오에서 코드 LLM에 의해 생성된 완성의 최대 40%에 취약한 코드 패턴이 포함되었습니다[38].우리는 Pearce 등이 도입한 벤치마크의 변형에서 우리의 취약성 탐지 모델을 평가했습니다.[38]. 평가 결과, 모델을 사용하면 취약성 비율이 90% 감소합니다. 마지막으로 VSCode 확장 프로그램에서 취약성 탐지 모델을 배포한 과정에 대해 논의합니다. 그 결과 확장 프로그램으로 편집한 코드에서 취약성 비율이 80% 감소했습니다. 이 논문은 다음과 같은 기여를 합니다.(1) 불완전한 코드 조각에서 밀리초 단위로 취약성을 식별하여 EditTime의 개발자에게 제공할 수 있는 프로덕션 품질의 취약성 탐지 시스템을 제시합니다.(2) 취약성 탐지 과제에 대한 사전 학습된 LLM의 세 가지 일반적인 학습 방식인 제로샷, 퓨어샷, 파인튜닝의 이점과 과제를 살펴보고 논의합니다.(3) Pearce 등이 도입한 벤치마크를 확장하여 코드 취약성 탐지 커뮤니티에서 향후 EditTime 벤치마크로 사용할 수 있도록 합니다. 관련 작업 취약성 탐지, 취약성 탐지를 위한 딥 러닝 방법, 자동 생성된 코드의 취약성 탐지 분야에서 이전 작업을 기반으로 작업을 구축하고 확장하는 방법에 대해 논의합니다. 2.1 취약점 탐지 취약점을 검색하기 위해 소스 코드를 분석하는 방법에는 두 가지가 있습니다. 동적 분석은 코드 조각을 실행하고 스택 추적을 검사하여 특정 취약점의 시그니처를 확인합니다. 정적 분석은 실행하지 않고 소스 코드를 분석합니다. 동적 분석의 일반적인 방법 중 하나는 퍼징으로, 프로그램을 여러 다른 입력에서 실행하여 프로그램 실행 추적 공간을 탐색합니다. 이 프로그램 공간을 효율적으로 검색하기 위한 최적화 기술은 활발한 연구 분야입니다[21, 31, 52]. 그러나 임의의 코드를 실행하여 대규모 코드 베이스를 퍼징하는 것은 불가능하며, 이로 인해 정적 분석 기술이 개발되었습니다. 정적 분석기는 코드 베이스에서 버그나 취약점의 의미 패턴을 검색합니다[37]. CodeQL[18]은 이러한 패턴 기반 검색을 위한 인기 있는 확장 가능한 정적 분석기입니다. CodeQL은 특정 안티패턴에 대한 규칙 기반 쿼리를 작성하는 쿼리 언어를 제공합니다. 우리는 최첨단 딥 러닝 접근 방식을 활용하여 인간 전문가를 참여시키지 않고도 다양한 취약성 유형을 학습할 수 있는 취약성 탐지 모델을 개발하여 이러한 연구 라인을 구축합니다.2.2 딥 러닝 취약성 탐지 자연어와 프로그래밍 언어는 모두 순차적 구조를 가지고 있기 때문에 GRU, LSTM, Transformer와 같이 NLP에서 성공적인 모델 아키텍처도 취약성 탐지에 유망한 것으로 나타났습니다[49]. 자연어와 공유되지 않는 소스 코드의 고유한 특성 중 하나는 프로그램의 고유한 그래프 구조입니다.VulDeePecker[30]는 데이터 종속성 그래프에서 코드 가젯을 추출하고 BiLSTM 분류기를 학습하여 두 가지 다른 취약성 유형의 데이터 세트에서 취약성을 탐지합니다.SySeVR[29]은 VulDeePecker를 기반으로 구문 및 의미 정보를 모두 사용하여 취약성 유형의 데이터 세트에서 취약성 분류기를 학습합니다. 다른 연구에서는 언어 모델을 보완하기 위해 그래프 신경망을 활용합니다.IVdetect[27]는 Feature-Attention Graph Convolutional Network(FA-GCN)와 함께 GRU 임베딩을 사용하여 프로그램 종속성 그래프의 취약성을 탐지합니다.LineVul[16]은 메서드 수준 탐지 외에도 줄 수준 취약성 지역화를 수행하기 위해 사전 학습된 CodeBERT[15]를 활용하여 IVdetect를 확장합니다.Devign[53]은 코드 표현 그래프의 임베딩을 학습하고 그래프 수준 예측을 위해 취약성을 탐지하기 위해 게이트 그래프 순환 네트워크를 학습합니다.이 모델을 FFmpeg 및 QEMU의 취약성에 대해 평가합니다.ReVeal[8]은 취약성 탐지 모델의 벤치마크로 실제 소프트웨어 프로젝트의 취약성 데이터 세트를 소개합니다.코드 속성 그래프에서 학습된 게이트 그래프 신경망(GGNN)을 사용하여 이 벤치마크를 보여줍니다. 영어: 저희 연구는 JavaScript, Python, Go, Java, C++, C# 및 Ruby의 7개 언어에 대한 신경 취약성 탐지 모델을 개발하여 취약성 탐지에 대한 이전 작업을 확장하여 EditTime에서 취약성을 탐지할 수 있습니다.2.3 자동 생성된 코드의 취약성 탐지 소스 코드에서 학습된 Transformer 기반 모델은 코드 생성을 위한 최첨단 결과를 달성했습니다[10, 26, 34]. 이러한 모델은 개발자 지원 도구로서 전례 없는 유용성을 가지고 있으며 제품에 배포되고 있습니다[17, 45, 46]. 이러한 코드 LLM은 인간이 작성한 코드의 방대한 데이터 세트에서 학습되었으므로 코드 출력은 인간 개발자의 패턴(또는 안티 패턴)을 따를 가능성이 점점 더 높습니다. 예를 들어, [14]는 Java LeetCode 문제에 대한 Codex를 평가하고 Codex의 기능적으로 잘못된 완성이 인간이 작성한 잘못된 코드와 유사한 안티 패턴을 공유한다는 것을 발견했습니다. 마찬가지로 [38]은 취약성 생성과 관련하여 Codex를 조사하고 특정 시나리오에서 Codex가 실제로 취약한 코드 패턴을 생성한다는 것을 발견했습니다. 다른 연구[44]에서는 HumanEval 데이터 세트를 사용한 기능 테스트 중에 GitHub Copilot이 약 2%의 사례에서 특정 CWE를 생성한다는 것을 보여주었습니다. 이러한 연구에 따르면 코드 LLM은 나쁜 코드와 좋은 코드를 모두 생성하는 법을 배우고 있는 것으로 나타났습니다. 다행히도 취약한 코드 패턴은 인간 코드에서는 드물고 따라서 학습 데이터의 작은 부분을 차지하기 때문에 코드 LLM의 완료에서도 드뭅니다. 예를 들어, 코드 LLM 지원의 효과를 연구한 최근 연구에서는 코드 LLM이 인간보다 취약성을 생성할 가능성이 더 높다는 결정적인 증거를 찾지 못했습니다[42]. 그럼에도 불구하고 코드 취약성과 관련된 높은 비용을 감안할 때 코드 LLM에서 생성된 취약한 코드 패턴을 줄이는 것은 특히 초보 프로그래머에게 장기간 사용이 안전하도록 하는 데 필수적입니다. 이를 위해 저희의 연구는 구문적으로 불완전한 코드에서 작동할 수 있는 모델을 만들어내어 EditTime에서 자동 생성된 코드 조각과 개발자가 작성한 코드 조각의 취약점을 모두 탐지합니다.EDITTIME에서 취약점 탐지 EditTime에서 취약점을 탐지하기 위해 사전 훈련된 LLM에 대한 일반적인 학습 접근 방식인 제로샷, 퓨어샷 및 파인 튜닝을 사용하여 여섯 가지 모델 변형을 개발합니다.아래에서는 먼저 파인 튜닝 프로세스를 위해 수집한 훈련 데이터를 설명합니다.그런 다음 모델 변형과 해당 성능을 설명합니다.3.1 데이터 수집 저희 데이터 세트는 GitHub LGTM 서비스가 공개 GitHub 저장소에서 탐지한 취약한 코드 패턴으로 구성되어 있습니다.GitHub LGTM 서비스는 확장 가능한 정적 분석기인 CodeQL[18]을 공개 GitHub 저장소에서 실행하여 하드 코딩된 자격 증명, SQL 주입 및 경로 주입을 포함한 다양한 취약한 코드 패턴을 식별합니다. 이 작업에서 우리는 7개 언어(JavaScript, Python, Go, Java, C++, C#, Ruby¹)의 각각에서 &quot;Common Weakness Enumeration&quot;(CWE) [32] 세트에 해당하는 탐지된 CodeQL 문제의 하위 세트를 선택합니다.표 1은 큐레이팅된 데이터 세트의 요약 통계를 보여줍니다.표 1: 취약성 탐지 학습 데이터 세트의 요약 통계에는 전체 범위가 포함되어 있습니다.특히, 우리는 명령문(if 및 export 명령문 등), 메서드, 선언 및 절을 찾습니다.이러한 코드 블록의 경우 코드 블록에 취약한 코드 패턴이 포함되어 있으면 탐지된 취약성이 시작되기 전의 특정 문자에서 블록을 무작위로 분할합니다.그렇지 않고 취약성이 탐지되지 않으면 임의의 지점에서 블록을 무작위로 분할합니다.이 분할 프로세스는 EditTime의 가능한 코드 상태를 모방합니다.따라서 이 데이터로 학습된 모델은 구문적으로 잘못되고 불완전한 코드 조각에서 취약한 블록을 탐지할 수 있어야 합니다.블록의 첫 번째 세그먼트는 &quot;컨텍스트&quot;로 레이블이 지정되고 두 번째 세그먼트는 &quot;취약한 블록&quot;으로 레이블이 지정됩니다. 그림 2는 컨텍스트와 취약한 블록 쌍의 예를 보여줍니다. 완성에 취약성이 포함된 경우 해당 취약성 유형이 레이블로 예제에 지정됩니다. 컨텍스트와 취약한 블록을 이렇게 분리하면 모델이 컨텍스트를 고려하여 취약한 블록에 집중하게 됩니다. 저장소 수준에서 85/5/10의 학습-검증-테스트 분할을 사용하여 학습 세트가 저장소의 85%로 구성되고 검증 세트가 저장소의 5%로 구성되고 테스트 세트가 저장소의 10%로 구성되도록 했습니다. 그런 다음 테스트 세트 예제와 일치하는 학습 세트의 모든 예제를 제거했습니다. 컨텍스트: app.get(&#39;name&#39;, function(req, res, next) { // url 매개변수를 기반으로 계정 쿼리 con 취약한 블록: nection.query(&#39;SELECT * FROM accounts WHERE name=&quot;req.params.name + &quot;. function(err, rows, fields) { if (err) { 모델 Javascript Python 적용 범위 취약함 취약하지 않음 70 CWES 266,2,293,next(err); 37 CWES return; 149,1,493,} Go 29 CWES 50,535,res.send(JSON.stringify(rows)); Java 44 CWES }); 33,431,}) C++ 32 CWES 7,215,C# 54 CWES Ruby 19 CWES 3,27,1, 탐지된 각 취약성에는 다음이 포함됩니다. 취약점 제목: CWE 범주에 해당하는 탐지된 취약점 유형 메시지: 탐지된 취약점을 설명하는 자세한 오류 메시지 • 위치: 문제가 시작되고 끝나는 파일 경로, 줄 및 열 3.2 데이터 전처리 수집한 데이터를 기반으로 데이터 전처리 단계의 목표는 다음 세 개의 값을 합성하는 것이었습니다.(Ci, vi, li) 여기서 c는 모델이 컨텍스트로 받아들이는 코드 조각이고, v는 취약한 코드 블록이며, I는 취약성 유형의 레이블입니다. 학습 데이터를 합성하는 프로세스는 다음과 같습니다. 먼저 이러한 문제가 탐지된 모든 8,815개 저장소에서 식별된 취약점이 있는 파일을 수집합니다. 각 파일에 대해 추상 구문 트리(AST)를 추출하고 트리의 노드를 검색하여 1실행한 모든 CodeQL 쿼리의 소스 코드는 https://github.com/github/codeql의 ql/Security 폴더에서 찾을 수 있습니다. 그림 2: 학습 데이터의 샘플 컨텍스트와 취약한 블록. 이 예에서 취약한 블록에는 SQL 주입 취약성이 포함되어 있습니다. 3. 모델 EditTime에서 취약성을 탐지하기 위해 LLM에 대한 세 가지 학습 접근 방식인 제로샷 학습, 퓨샷 학습 및 미세 조정을 사용하여 여섯 가지 모델 변형을 개발합니다. 이러한 학습 접근 방식을 기반으로 세 가지 사전 학습된 LLM에 사용합니다. ⚫ code-davinci-002: GitHub의 소스 코드에서 학습된 전체 크기 Codex 모델. ⚫ text-davinci-003: 인간 피드백(RLHF)에서 강화 학습을 사용하는 InstructGPT [36] 기반 Codex 모델. • CodeBERT: 이중 모드 데이터(코드 및 문서)에서 사전 학습된 트랜스포머 [15]. 그 아키텍처는 12개 레이어, 12개 어텐션 헤드, 768의 숨겨진 크기를 갖춘 ROBERTa-base[13]를 따릅니다. 위의 사전 학습된 모델에 세 가지 학습 접근 방식을 사용하면 표 3에 나열된 여섯 가지 모델이 생성됩니다. 아래에서 이러한 모델을 개발하는 세부 사항을 설명합니다.3.3.제로샷 학습.제로샷 학습에서 우리는 원하는 출력을 지정하는 프롬프트가 있는 사전 학습된 모델을 제공합니다. 우리의 경우, codedavinci-002와 text-davinci-003을 프롬프트합니다. 각 모델에 취약성을 탐지하도록 요청하기 위한 적절한 프롬프트를 얻기 위해, 우리는 모델 자체의 취약성 탐지 정의를 활용합니다. 우리는 먼저 모델에 취약성 탐지 작업을 정의하도록 프롬프트하고, 이 작업 설명을 작업의 최종 프롬프트에 사용합니다. code-davinci-002의 경우, 우리는 모델에 &quot;# We run CodeQL security queries in order to &quot;를 4번 프롬프트하고 상위 결과를 선택합니다. 그러면 다음과 같은 프롬프트 변형이 생성됩니다. &quot;잠재적 보안 취약성 식별&quot;, &quot;잠재적 보안 문제 찾기&quot;, &quot;보안 취약성 찾기&quot;, &quot;보안 취약성 감지&quot;. 우리는 아래 템플릿에 따라 이러한 변형을 제로샷 프롬프트에 배치합니다.<phrase><comment> 코드 조각<comment> <code snippet >Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: <code snippet>Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code><phrase > <code snippet ><code snippet>Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code><Language> <code snippet ><code snippet>Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code><Vulnerability Name> <code snippet ><code snippet>Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code><Language> <code snippet ><code snippet>Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code><issue> <code snippet ><code snippet>Answer (Yes/No, explanation): Here &lt; phrase &gt; corresponds to one of the prompt phrasing variations above, &lt; comment &gt; refers to a language specific comment indicator (eg &quot;#&quot; for Python) and &lt; code snippet &gt; refers to the code snippet in question. Figure 3 shows a sample prompt following this template. In response to this prompt, the model outputs either Yes or No, followed by an explanation which can be used for observations and debugging purposes. We check for the presence of &quot;Yes&quot; or &quot;No&quot; to determine the model&#39;s decision. We refer to this zero-shot approach as CodexZero. // identify potential security vulnerabilities // Code snippet const pg = require(&quot;pg&quot;); const client = new pg. Client({ }); host: &quot;database.server.com&quot;, database: &quot;mydb&quot;, port: 3211, user: &quot;username&quot;, password: &quot;password&quot; //Answer (Yes/No, explanation): Figure 3: A sample prompt created based on our template for zero-shot setting Similarly, for text-davinci-003, we first prompt the model four times at temperature 0.8 with the question, &quot;What would you accomplish by running CodeQL security queries?&quot;. We then ask the model to rephrase its response four times. This yields the following unique phrases: &quot;identify potential security vulnerabilities&quot;, &quot;spot any security weaknesses&quot;, &quot;detect any security risks&quot;, &quot;determine any security issues&quot;. We then try these variations in a zero-shot prompt following a similar template: Answer (Yes/No): We check for the presence of &quot;Yes&quot; or &quot;No&quot; in the model&#39;s response to determine the model&#39;s decision. We refer to this zero-shot approach as TextZero. 3.3.2 Few-shot Learning. Few-shot learning builds on zero-shot learning by providing example input-output pairs, in addition to the zero-shot prompt. For our study, we utilize the best performing prompt variations of code-davinci-002 and textdavinci-from our zero-shot learning in the same template format. We then prepend additional examples in the same template format before finally inserting the code snippet of interest and prompting the model for the answer. To create the examples, we prompt each model with the phrase, &quot;Provide an example in of a code snippet that contains security vulnerability. Output the code only, do not include text:&quot; for each of the languages and types of vulnerabilities in Table 2. We prompt with this template three times for each vulnerability and language pair, yielding 150 vulnerable examples. We then prompt the model with &quot;Provide an example in of a code snippet. Output the code only, do not include text:&quot;, to retrieve non vulnerable samples of code. We manually evaluate each sample to ensure that they were vulnerable or non vulnerable as intended. In total, there were 297 examples. We refer to the models resulting from the above few-shot learning process with code-davinci-and text-davinci-003 as CodexFew and TextFew, respectively. 3.3.3 Fine-tuning. For fine-tuning, we focus on pre-trained code LLMs only and fine-tune CodeBERT and code-davinci-002. For CodeBERT, we add a linear classification head to its BERT trunk in order to build a multi-class classification model. The inputs to the model are the context and vulnerable code block, separated by a special [SEP] token and bounded by a beginning of sequence [BOS] token and end of sequence [EOS] token [BOS], C1, C2, ...Cn, [SEP], V1, V2, ...Um, [EOS] where c1, c2, ...cn denotes a sequence of n tokens of context code surrounding the vulnerability and U1, U2, ...Um denotes a sequence of m tokens of the vulnerable block of code. We distinguish between context and vulnerable block to enable the model to process any given complete or incomplete code snippet. Because our data is vastly imbalanced with less than 10% vulnerable examples, we employ an oversampling technique on the vulnerable examples while training: for each epoch, all vulnerable examples are retained while the non-vulnerable examples are rotated through so that each epoch contains 50% vulnerable and 50% non vulnerable examples. The model is trained with a standard binary cross entropy (BCE) loss. In the rest of this work, we refer to this model as DeepDevVuln. Our second fine-tuned model is a fine-tuned version of codedavinci-002 on 30,000 randomly sampled examples from our training set. We down-size our training data due to the cost of fine-tuning the model on the entirety of the training set. In this case, the context and vulnerable block are concatenated together, then a special classification token is appended to the end of the sequence. Because code-davinci-002 is a GPT decoder model that does not inherently perform classification, we use next-token prediction as a proxy, ie the output is a special token that signifies vulnerable or not. The sequence is as follows: ., Un [CLS][VULN] [BOS], C1, C2, ...Cn, V1, V2, ..., We refer to this variant as CodexVuln.Table 2: Summary statistics of vulnerable issues gathered from Github PRs Vulnerability CWE N SQL InjectionHardcoded CredentialsCode InjectionPath Injectionwe randomly sample 150 files from repositories that CodeQL has scanned and has not detected an issue. We then randomly select chunks of non-overlapping code between 1 to 10 lines from these files to generate nonvulnerable examples. In total, the GitHub PR dataset contains 1,006 examples. // Find if the code snippet has any security vulnerabilities // Code snippet: type groupSourceReq struct { Interface uintPad_cgo_0 [256]byte // Answer (Yes/No, explanation): Yes, the code snippet has security // vulnerabilities. // Explanation: The code snippet has a buffer overflow vulnerability. // The variable &quot;buf&quot; is a fixed size array of 256 bytes. 3.Clear Text Logging Weak Cryptographic Algorithm Incomplete URL Substring SanitizationModel Variant Evaluation To better understand the effect of model architecture and training choices, we compare our six model variants on a dataset we collect from GitHub pull requests. 3.4.1 Metrics. The majority of existing approaches treat vulnerability detection as a classification problem. Given a block of code, the model makes a binary decision on whether the code is vulnerable or not. Therefore, in our evaluations we also use the most common evaluation metrics for a classification task [40] which includes: • Precision: which indicates the correctness of predictions and calculated as true positive / (true positive + false positive) • Recall: which indicates the effectiveness of the prediction and calculated as true positive / (true positive + false negative). • F1-score: which indicates balance between precision and recall and is defined as the geometric mean of the two. Table 3: Performance of DeepDevVuln model on Github PR Vulnerabilities dataset. 3.4.Model DeepDevVuln CodexVuln CodexZero TextZero CodexFew (8 examples) TextFew (6 examples) Precision Recall F1-Score 58.87% 63.00% 60.87% 69.56% 48.00% 56.80% 11.08% 98.00% 19.90% 46.99% 78.00% 58.65% 23.91% 95.00% 37.70% 49.01% 75.00% 59.29% Dataset. To create this dataset, we collect all pull requests that contained &quot;fix &quot; in the pull request title for every combination of issue from Table 2 and language (ie JavaScript, Python, Go, Java, C++, C#, Ruby). To ensure that the retrieved pull requests were relevant to the target vulnerability, we only include the pull requests that contain both removal and addition of code and were less than 100 lines in length. This yields 283 pull requests. We then manually examine each pull request to ensure the vulnerabilities are legitimate. Using this process, we gather a set of 100 vulnerable examples constituting the CWEs in Table 2. We added a set of 906 non vulnerable examples to this dataset. To collect the non vulnerable examples,Figure 4: A sample of CodexZero false positive due to overreach // Find if the code snippet has any security vulnerabilities // Code snippet: *(md++) *(md++) *(md++) *(md++) = (unsigned char) (t&gt;&gt;24); = (unsigned char) (t&gt;&gt;16); = (unsigned char) (t&gt;&gt;8); = (unsigned char) (t); // Answer (Yes/No, explanation): Yes, buffer overflow. // Explanation: The code snippet is vulnerable to buffer overflow. // The code snippet is trying to write 4 bytes of data into the // memory pointed by md. // However, md is a pointer to a memory location that is not allocated. Figure 5: A sample of CodexVuln false positive due to lack of context 3.4.3 Results. Table 3 shows the performance of our model variants on this dataset. For these six model variants, we can see that DeepDevVuln has the best performance with respect to F1 score. Every zero-shot and few-shot variant outperforms DeepDevVuln in terms of recall, however the precision of each variant is significantly lower. One reason might be that the zero-shot and few-shot models have a wider definition of what constitutes vulnerable code. For example, the code snippet presented in Figure 4 below does not have any explicit vulnerabilities, however, CodexZero considers the worst-case scenario that the code may be vulnerable to a buffer overflow in the future. Due to this tendency, it seems that CodexZero will raise an alert even if there is no explicit vulnerability present, which leads to a high recall score but low precision score. Another example further explains CodexZero&#39;s high false positive rate. In Figure 5, we see that the model lacks context around the variable md, and explains that because it was never initialized, accessing the memory at that location could lead to a vulnerability. This explanation extends, in part, to TextZero, which has a more balanced precision and recall. In TextZero&#39;s case, the prompt variation used had a significant effect: for example, the first prompt variation &quot;identify potential security vulnerabilities&quot; had similar results to CodexZero. However, using the phrase &quot;detect any security risks&quot; led to the result in Table 3. This may have encouraged the model to focus on the exact code snippet as-is, rather than to speculate about possible vulnerabilities. Comparing few-shot and zero-shot results, we see the few-shot evaluations surpassed their zero-shot counterparts. One explanation is that the inclusion of examples gives the model a sense of what to expect in terms of code snippet length and area of focus. For instance, when the non-vulnerable examples include uninitialized variables and incomplete context, the model starts to ignore the worst-case scenarios explained above. Finding the best number of examples was a matter of gradually increasing the number of examples prepended to the prompt from 1 to 9. We ran several trials and reported the best results. Among these trials, we observed that, especially for CodexFew, there is an apparent correlation between larger number of examples and the precision and recall achieved as seen in Figure 6. It may be that the exact types of examples selected have a major role in steering the model&#39;s output. A similar trend for precision, but not recall, was observed for TextFew, Figure 7.EXPERIMENTS Below, we explain our experiments designed to answer the following two research questions: • RQ1: How effective is our vulnerability detection model compared to results from state of the art models on established benchmark datasets? • RQ2: To what extent our proposed vulnerability detection model is effective in reducing the vulnerability rate of code LLMs? We perform two experiments to answer these questions. The first experiment focuses on comparing our model against the state of the art vulnerability detection models on common datasets. The second experiment gauges the effectiveness of our model in reducing the vulnerability rate of code LLMs. 4.1 Experiment 1: Performance against Existing Approaches on Benchmark Datasets In this section, we present the experimental evaluation of our vulnerability detection model against existing state of the art approaches on four widely used datasets. Table 4 summarizes the datasets used in this experiment. 4.1.1 Experimental Setup. The granularity of the data, quality of the data, and types of issues covered in each of the datasets in Table 4 is different from our training and test set. Therefore, we followed the approach of Chakraborty et al. [8], where we first remove duplicate examples from each of the datasets. We then finetune DeepDevVuln as a binary classification model on each dataset for 10 epochs. For each dataset, we use a standard 80%/10%/10% train/validation/test split, consistent with the baseline models. 4.1.2 Results. As shown in Table 5, our DeepDevVuln model has the best overall F1-Score for the majority of datasets. This means that our model demonstrates a good balance between precision and recall which is important for vulnerability detection. Additionally, our vulnerability detection model has 10 times fewer parameters than GPT2-Large or Codex, yet still achieves comparable precision and higher recall. Overall this results suggests that our approach allowed our model to adapt to the specific types of issues presentprecision precision 0.230.22n_examples ••0.21-0.0.0.180.17code-davinci-002 Few Shot Evaluation 0.0.0.0.0.0.recall Figure 6: Trials varying examples of CodexFew 0.n_examples ••0.46•0.440.420.40text-davinci-003 Few Shot Evaluation 0.68 0.0.72 0.74 0.76 0.78 0.80 0.recall Figure 7: Trials varying examples of TextFew in these datasets and leverage its knowledge gained through pretraining on our vulnerability dataset to improve upon the state of the art results. 4.2 Experiment 2: Model&#39;s effectiveness in reducing the vulnerability rate of code LLMS In our second experiment, we evaluated the effectiveness of our vulnerability detection model in detecting the vulnerable code completions of four different code LLMs: • CodeGen-2B: a transformer decoder model trained on natural language and code (C, C++, Go, Java, JavaScript, Python) ⚫ code-cushman-001: smaller-size Codex model, trained on source code from GitHub • code-davinci-002: full-size Codex model, trained on source code from GitHub ⚫ text-davinci-003: Codex model based on InstructGPT [36], using reinforcement learning from human feedback (RLHF) Table 4: Summary of common vulnerability detection datasets. Dataset # Programs % Vuln # Duplicates Granularity VulDeePecker [30] 61,28.76% 33,Slice SeVC [29] 420,13.41% 188,Slice ReVeal [8] 22,9.85%Function FFMPeg+Qemu [53] 27,45.61%Function Table 5: Performance of DeepDevVuln model on Vuldeepecker, SeVC, Reveal, and FFMPeg+Qemu datasets. VulDeeDescription It was obtained by preprocessing examples from the National Vulnerability Database (NVD) and the Software Assurance Reference Dataset (SARD) and consists of CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) and CWE-399 (Resource Management Errors). An improvement over the VulDeePecker dataset, covering 126 different types of vulnerabilities and divided into four categories: API Function Call, Arithmetic Expression, Array Usage, and Pointer Usage. It tracks past vulnerabilities from the Linux Debian Kernel and Chromium projects. The dataset reflects real bug reports and has a realistic data imbalance, with only 10% of the examples being vulnerable. It consists of past vulnerabilities and their fixes from two open-source projects. We took the 29 Python scenarios developed by [38] and, following the same process, we added 11 new JavaScript scenarios covering 10 CWEs to the benchmark. Table 6 describes the scenarios we added using the same format as in [38]. “Rank&quot; reflects the CWE ranking in the MITRE list if applicable. CWE-Scn refers to the scenario&#39;s identifier and associated CWE. All of these scenarios are written in JavaScript, originate from the public GitHub CodeQL documentation, and were evaluated with CodeQL. Table 6: Javascript Scenarios covering 10 CWEs in Javascript Dataset VulDeeModel VulDeePecker Pecker Thapa et al. CodeBERT 95.27% Precision Recall F1-Score 82.00% 91.70% 86.6% 95.15% 95.21% CWEThapa et al. GPT-2 Base 93.35% 93.56% Thapa et al. GPT2-Large 95.74% Codex DeepDevVuln VulDeePecker 95.30% Pecker Thapa et al. CodeBERT 94.25% CWEThapa et al. GPT-2 Base 92.97% Thapa et al. GPT2-Large 96.79% Codex 96.69% DeepDevVuln 95.65% Thapa et al. (BERTBase) 93.45% 95.51% 97.45% 93.31% 95.33% 96.74% 95.62% 96.18% 94.60% 86.6% 95.29% 94.76% 94.99% 93.96% 96.90% 96.84% 97.04% 96.87% 97.41% 96.53% 88.73% 87.95% 88.34% 95.28% Rank CWE-Scn. Description SeVC Thapa et al. (GPT-2 Base) Codex 86.88% 82.26% 87.47% DeepDevVuln 95.56% 88.34% 84.34% 83.29% 97.14% 96.35%CWE-SQL Injection Chakraborty et al. 30.91% 60.91% 41.25% Reveal Codex 45.04% 29.80% 35.87%CWE-CodeBERT 48.95% 35.35% 41.06%CWE-DeepDevVuln 41.00% 61.00% 49.29% Chakraborty et al. 56.85% 74.61% 64.42% FFmpeg + Qemu Codex 63.22% 55.64% 59.19% CodeBERT DeepDevVuln 62.94% 57.34% 58.70% 78.06% 66.11% 60.74%CWE-Incomplete Url Substring Sanitization Tainted Path Hardcoded CredentialsCWE-Code InjectionCWE-Client Side Url RedirectionCWE-Server Side Url RedirectionCWE-CWE-CWE-CWE-Clear Text Storage of Sensitive Data Stack Trace Exposure Broken Cryptographic Algorithm Insufficient Password Hash We evaluated the extent to which our model detects the vulnerable code patterns produced by each LLM utilizing the benchmark created by Pearce et al[38]. This benchmark consists of scenarios to evaluate the likelihood of a code LLM generating vulnerabilities. These scenarios are constructed to specifically elicit a completion containing a particular vulnerability. Each scenario is associated with a particular CWE and includes a prompt code snippet and a corresponding CodeQL query. The prompt is used as input to a code LLM. The model-generated completion is appended to the prompt and this completed snippet is then analyzed using the provided CodeQL query. Completions that cannot be analyzed by CodeQL (due to syntactical errors) are considered invalid and excluded from the analysis. CodeQL marks the remaining valid completions as either vulnerable or clean. For each scenario, a model can generate a varying number of valid or vulnerable completions. In the context of code LLMs, a developer may often see only a single completion for a given prompt. Therefore, we evaluate vulnerability rates at the level of scenarios (prompts): we count the number of scenarios that yielded at least one vulnerable completion. For example, suppose there are 10 scenarios and each model generates 5 completions per scenario. For each of the 10 scenarios, we run the corresponding CodeQL query on its 5 completions. Suppose that 9 scenarios have at least one syntactically valid completion. We consider the 9 scenarios withvalid completions and examine how many of the 9 have at least one vulnerable completion. For each model, we generate 25 completions per scenario. We then run our vulnerability detection model on each completion and filter out the completions that our model detects as vulnerable. We then rerun the CodeQL queries from the benchmark on the remaining completions. 4.2.Results. The results of this vulnerability experiment are shown in Table 7. The first two columns corresponds to each code LLM acting alone, while the second two columns includes filtration from our vulnerability detection model. The vulnerability reduction rate is the percentage reduction in the vulnerability rate as a result of filtration. As shown in the table, filtering vulnerable outputs results in a significant decrease in vulnerability rate. In particular, the vulnerability reduction rate is highest for text-davinci-003, which follows InstructGPT&#39;s method of reinforcement learning from human feedback (RLHF). RLHF is known to substantially improve the quality and naturalness of generated text. Therefore, text-davinci-likely generates code that more closely resembles code written by real developers. Since DeepDevVuln is trained on developer-written code, it may be better able to detect vulnerabilities in outputs from text-davinci-003 than other code LLMs.DEPLOYING IN PRODUCTION We deployed our model for detecting vulnerable code patterns in a VSCode extension with ~100K daily users. After each key stroke that a user writes, their incomplete code snippets are sent to our model for verification. To evaluate the effectiveness of our model, we collected and examined the JavaScript code snippets that were sent to our model in a three month period, from November 2022 to January 2023, for a total of ~6.7M code snippets. We chose to focus on JavaScript because it is the most popular language in VSCode. In order to have a baseline for comparison, we ran all the CodeQL security queries for JavaScript on the collected code snippets. Overall, CodeQL detected ~1,284 vulnerable code snippets. However, this number is a lower-bound for the amount of actual vulnerable code snippets. CodeQL queries do not successfully run and detect issues in all of the code snippets, because CodeQL is designed to run on a set of complete files in a repository. Therefore, CodeQL&#39;s vulnerability detection rate drops significantly when executed on syntactically-incorrect code or incomplete code that is presented in a single file as opposed to the repository context. This drop impacts some scenarios more than others, depending on sensitivity of the query to syntax issues and the amount of context required by the query to detect a vulnerability. Of CodeQL&#39;s vulnerability detections, over 58% were from two scenarios which have simple CodeQL queries requiring less context to run successfully. In comparison, DeepDevVuln detections were more uniform. In fact, over 57% of DeepDevVuln detections were from SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect, and Insufficient Password Hash scenarios. This is significant because JavaScript is a dominant language in both client- and server-side web development, and these classes should be more prominent in this domain. Yet, CodeQL detects these scenarios at a rate of less than 1 in 1,000,000. CodeQL&#39;s poor coverage and inability to detectvulnerabilities on this production data highlights the need for deep learning based detection systems in live services. For our evaluation, because CodeQL-detected issues are a lowerbound on the number of issues, we use them to measure recall. Instead of precision, we measure the positive rate (number of detected issues over number of all scanned issues). Figure 8 shows how our DeepDevVuln model performs on recall vs positive rate: it can achieve up to 90% recall with around 1% positive rate. While we optimized for recall for our extension, other applications can find the right balance between recall and positive rate based on their user scenario and feedback. 1.0.80.60.0.0.0.0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.Positive rate Figure 8: DeepDevVuln model&#39;s performance on recall vs. positive rate Overall, we observed that DeepDevVuln&#39;s vulnerability reduction rate (ie the reduction in the rate of vulnerabilities present in developer&#39;s code) for JavaScript was 89.64%.LESSONS LEARNED AND ONGOING WORK In the process of building and deploying our model, we learned a few lessons that have guided our ongoing and future work on vulnerability detection. First, the different learning methods we explored in this work come with trade-offs in model size, inference cost, predictive performance, and maintenance cost. Zero-shot and few-shot learning require sufficiently large models in order to effectively make predictions. Furthermore, predictive performance tends to improve with model size. For a constant model size, the inference cost of zero-shot learning is slightly higher than for fine-tuning, since a prompt must be constructed for each example; the cost for few-shot learning is even higher, since the system must fetch examples for each example. Our results show that a fine-tuning approach yielded the best prediction performance, allowing us to make accurate predictions without incurring high inference cost. However, in order to maintain a fine-tuned model, we must continually monitor and retrain the model on additional vulnerability types. Zero-shot and Without DeepDevVuln Table 7: Vulnerability rate on scenario level for different baselines. With DeepDevVuln Valid Vulnerable Valid Vulnerable Vulnerability Approach Scenarios Scenarios Scenarios Scenarios Reduction Rate CodeGen (6B)7 (100.00%)2 (29.00%) 71.00% code-cushman-25 (100.00%)5 (26.0%) 74.00% code-davinci-text-davinci-24 (92.00%) 21 (78.00%)7 (35.0% 61.96%2 (8.0%) 89.74% few-shot learning, on the other hand, would only require maintenance with regard to prompts and examples, rather than any further training. Second, there is a trade-off between the size of a model and its response time. This work focuses on detecting vulnerabilities at EditTime, while a developer is writing code in an IDE. A large vulnerability detection model requires more time to make predictions, which can result in delayed response and the developer missing the vulnerability. In order to maintain low prediction latency, our vulnerability detection is based on the relatively small CodeBERT-base model and has under 100M parameters. As more powerful hardware is built to run large models and improve the inference time, we expect to be able to run large models in production settings in our future iterations. Third, in many classification problems, a model&#39;s prediction threshold is used to create an appropriate balance between precision and recall. The balance is important because an effective production-ready fault detector must minimize churn and false positives [7]. High churn, where the issues raised vary from one version of the system to another, can cause significant friction with users due to a lack of consistency. False positives can similarly erode developer trust in the usefulness of a system, resulting in developers ignoring the tool. In our case, we do not have the ground truth of all vulnerable code snippets for our live evaluations, and therefore we cannot measure precision In this case, the analogous metrics are positive rate (the fraction of examples that the model predicts as positive) and recall. Our second lesson was in balancing these metrics for a production-scale vulnerability detection system. We tuned our threshold to maintain a 1% positive rate based on initial user&#39;s interactions and feedback. However more long-term studies and monitoring of these metrics are needed to better adjust the balance. Finally we learned that we should periodically retrain our model to expand coverage as new vulnerability types are caught. When the common vulnerabilities are caught early on in the development process, users may start to notice the uncommon vulnerabilities and this may hurt their trust on detection tools overtime. Therefore, to address this challenge we have implemented a retraining pipeline where we constantly find datasets with new vulnerabilities to feed the pipeline and expand the coverage.</code></code>
--- CONCLUSION ---
코드 취약점은 소프트웨어 회사와 사용자에게 계속해서 비용을 초래합니다. 개발자가 코드를 작성하거나 코드 LLM에서 생성할 때 EditTime에서 코드의 취약점을 탐지하는 것은 취약점을 낮은 비용으로 수정하는 데 필수적입니다. 그러나 현재 취약점 탐지 도구의 대부분은 EditTime에서 취약점을 탐지하지 않습니다. 저희의 작업은 불완전한 코드 조각에서 취약점을 탐지하는 취약점 탐지 모델을 제시하여 이러한 격차를 메우고, 따라서 코드 LLM 또는 개발자가 코드를 작성할 때 EditTime에서 취약한 코드 패턴을 탐지하는 데 사용할 수 있습니다. 저희의 평가 결과에 따르면 저희 모델은 최신 탐지 접근 방식을 리콜에서 10%, 정확도에서 8% 향상시킵니다. 또한 저희 모델은 코드 LLM의 취약점 비율을 89% 이상 감소시킵니다. 향후 작업에 대한 즉각적인 방향은 새로운 유형의 취약점을 훈련 세트에 추가하여 취약점 탐지 모델의 적용 범위를 확장하는 것입니다. 또 다른 방향은 VSCode 확장 프로그램을 사용하는 개발자의 전반적인 경험에 대한 취약점 탐지 모델의 장기적인 효과를 측정하는 것입니다. 예를 들어, 측정에는 취약성 감소율, 개발 중인 파일이 단위 테스트 실패를 초래했는지 여부 또는 취약성이 EditTime(예: 빌드 시간) 이후에 파일에서 발견되었는지 여부가 포함될 수 있습니다.참고문헌 [1] 2022. CWE 목록 버전 4.9. https://cwe.mitre.org/data/index.html [2] 2023. 국가 취약성 데이터베이스. https://nvd.nist.gov/ [3] 2023. 보안 개발. https://www.sei.cmu.edu/our-work/securedevelopment/index.cfm [4] Shanai Ardi, David Byers, Per Hakon Meland, Inger Anne Tondel, Nahid Shahmehri. 2007. 개발자는 보안 모델링에서 어떤 이점을 얻을 수 있습니까? 제2회 가용성, 신뢰성 및 보안 국제 컨퍼런스(ARES&#39;07). IEEE, 1017-1025. [5] Boehm Barry et al. 1981. 소프트웨어 엔지니어링 경제학. New York 197(1981). [6] Walter Baziuk. 1995. BNR/NORTEL: 제품 품질, 신뢰성 및 고객 만족도를 개선하는 경로. 제6회 소프트웨어 신뢰성 엔지니어링 국제 심포지엄 회의록. ISSRE&#39;95. IEEE, 256–262. [7] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak 및 Dawson Engler. 2010. 수십억 줄의 코드 이후: 정적 분석을 사용하여 실제 세계의 버그 찾기. Commun. ACM 53, 2(2010), 66-75. [8] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding 및 Baishakhi Ray. 2021. 딥 러닝 기반 취약성 탐지: 아직 거기에 도달하지 못했을까. IEEE 소프트웨어 공학 저널(2021). [9] Mahinthan Chandramohan, Yinxing Xue, Zhengzi Xu, Yang Liu, Chia Yuan Cho 및 Hee Beng Kuan Tan. 2016. 빙고: 교차 아키텍처 교차 OS 이진 검색. 2016년 24회 ACM SIGSOFT 소프트웨어 공학 기초 국제 심포지엄 회의록. 678-689. [10] 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 엔리케 폰데 드 올리베이라 핀토, 자렛 카플란, 하리 에드워즈, 유리 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필리프 틸레, 펠리페 페트로스키 수치, 데이브 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 헵겐 거스, 알렉스 니콜, 알렉스 파이노, 니콜라스 테작, 지에 탕, 이고르 바부슈킨, 수치르 발라지, 샨타누 자인, 윌리엄 손더스, 크리스토퍼 Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 2021. 코드에서 학습된 대규모 언어 모델 평가. https://doi.org/10.48550/ARXIV.2107.[11] Min-je Choi, Sehun Jeong, Hakjoo Oh, Jaegul Choo. 2017. 신경 메모리 네트워크를 통한 원시 소스 코드의 버퍼 오버런에 대한 종단 간 예측. arXiv 사전 인쇄본 arXiv:1703.02458 (2017). [12] Microsoft Corp. 2023. Microsoft 보안 개발 수명 주기. https://www. microsoft.com/en-us/securityengineering/sdl/ [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, NAACL-HLT 2019, 미국 미네소타주 미니애폴리스, 2019년 6월 2-7일, 1권(긴 논문 및 짧은 논문), Jill Burstein, Christy Doran, Thamar Solorio(편). 컴퓨터 언어학 협회, 4171-4186. https://doi.org/10.18653/v1/n19-[14] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, Shin Hwei Tan. 2022. Codex에서 자동 생성된 코드를 Automated Program Repair를 통해 개선. https://doi.org/10.48550/ARXIV.2205.[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou. 2020. CodeBERT: 프로그래밍 및 자연어를 위한 사전 학습된 모델. https://doi.org/ 10.48550/ARXIV.2002.[16] Michael Fu 및 Chakkrit Tantithamthavorn. 2022. LineVul: Transformer 기반 라인 수준 취약성 예측. [17] GitHub. 2022. GitHub Copilot. 귀하의 AI 페어 프로그래머. https://copilot.github. com/ [18] Github Inc. 2021. 연구를 위한 CodeQL. https:securitylab.github.com/tools/ codeql [19] Dan Goodin. 2017. NSA에서 파생된 랜섬웨어 웜이 전 세계의 컴퓨터를 중단시키고 있습니다. https://arstechnica.com/information-technology/2017/05/annsa-derived-ransomware-worm-is-shutting-down-computer-world [20] W Humphrey. 1995. 소프트웨어 엔지니어링 분야 Addison-Wesley. 펜실베이니아주 레딩(1995). [21] James Kukucka, Luís Pina, Paul Ammann, Jonathan Bell. 2022. CONFETTI: 퍼저를 위한 Concolic 지침 확대. 2022년 IEEE/ACM 제44회 소프트웨어 엔지니어링 국제 컨퍼런스(ICSE). 438-450. 한국어: https://doi.org/10.1145/ 3510003.གླུ་ཐ་གླུ [22] Lucas Layman, Laurie Williams, and Robert St Amant. 2007. 오류 수정 시간 단축을 위해: 자동화된 오류 감지 도구 설계를 위한 개발자 행동 이해. First International Symposium on Empirical Software Engineering and Measurement(ESEM 2007)에서. IEEE, 176-185. [23] Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier De Vel, and Lizhen Qu. 2019. 이진 소프트웨어 취약성 감지를 위한 최대 발산 순차 자동 인코더. International Conference on Learning Representations에서. [24] Young Jun Lee, Sang-Hoon Choi, Chulwoo Kim, Seung-Ho Lim, and Ki-Woong Park. 2017. 소프트웨어 취약점을 탐지하기 위한 딥러닝으로 바이너리 코드 학습. KSII에서 열린 제9회 인터넷 국제회의(ICONI) 2017 심포지엄. [25] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu, Alwen Tiu. 2017. Steelix: 프로그램 상태 기반 바이너리 퍼징. 2017년 11회 소프트웨어 공학 기초 공동 회의록. 627–637. [26] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. alphacode를 사용한 경쟁 수준 코드 생성. arXiv 사전 인쇄본 arXiv:2203.07814(2022). [27] Yi Li, Shaohua Wang 및 Tien N. Nguyen. 2021. 세분화된 해석을 통한 취약성 감지. 제29회 ACM 유럽 소프트웨어 공학 컨퍼런스 및 소프트웨어 공학 기초 심포지엄(그리스 아테네)의 진행 중(ESEC/FSE 2021). Association for Computing Machinery, 뉴욕, 뉴욕, 미국, 292-303. https://doi.org/10.1145/3468264.[28] Yuekang Li, Yinxing Xue, Hongxu Chen, Xiuheng Wu, Cen Zhang, Xiaofei Xie, Haijun Wang 및 Yang Liu. 2019. Cerebro: 효과적인 취약성 감지를 위한 상황 인식 적응 퍼징. 2019년 27회 ACM 유럽 소프트웨어 공학 공동 회의 회의록 및 소프트웨어 공학 기초 심포지엄.533-544. [29] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, Zhaoxuan Chen.2021. Sysevr: 소프트웨어 취약성을 탐지하기 위한 딥 러닝을 사용하기 위한 프레임워크.IEEE Transactions on Dependable and Secure Computing 19, 4(2021), 2244-2258. [30] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong.2018. Vuldeepecker: 취약성 탐지를 위한 딥 러닝 기반 시스템.arXiv 사전 인쇄본 arXiv:1801.01681(2018). [31] Ruijie Meng, Zhen Dong, Jialin Li, Ivan Beschastnikh 및 Abhik Roychoudhury. 2021. 선형 시간 시간 논리 유도 Greybox 퍼징. https://doi.org/10. 48550/ARXIV.2109.[32] TMC(MITRE). 2022. CWE - 공통 약점 열거. //cwe.mitre.org https: [33] Stephan Neuhaus, Thomas Zimmermann, Christian Holler 및 Andreas Zeller. 2007. 취약한 소프트웨어 구성 요소 예측. 컴퓨터 및 통신 보안에 관한 제14회 ACM 컨퍼런스 회의록. 529-540. [34] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese 및 Caiming Xiong. 2022. 프로그램 합성을 위한 대화형 패러다임. arXiv 사전 인쇄본 arXiv:2203.13474(2022). [35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. 인간 피드백을 통해 지침을 따르도록 언어 모델 훈련. arXiv 사전 인쇄본 arXiv:2203.02155(2022). [36] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 2022. 인간 피드백을 통해 지침을 따르도록 언어 모델 교육. https://doi.org/ 10.48550/ARXIV.2203.[37] OWASP. 2021. &quot;소스 코드 분석 도구.&quot;. 커뮤니티/소스 코드 분석 도구 https://owasp.org/www[38] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri. 2021. GitHub Copilot의 코드 기여에 대한 실증적 사이버 보안 평가. CoRR abs/2108.09293 (2021). arXiv:2108.09293 https: //arxiv.org/abs/2108.[39] Hao Peng, Lili Mou, Ge Li, Yuxuan Liu, Lu Zhang, Zhi Jin. 2015. 딥 러닝을 위한 프로그램 벡터 표현 구축. Knowledge Science, Engineering and Management: 8th International Conference, KSEM 2015, Chongqing, China, October 28-30, 2015, Proceedings 8. Springer, 547–553. [40] David MW Powers. 2020. 평가: 정밀도, 재현율 및 F-측정에서 ROC, 정보성, 표시성 및 상관 관계까지. arXiv 사전 인쇄본 arXiv:2010.(2020). [41] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, and Marc McConley. 2018. 심층 표현 학습을 사용한 소스 코드의 자동 취약성 감지. 2018년 제17회 IEEE 기계 학습 및 응용 프로그램 국제 컨퍼런스(ICMLA). IEEE, 757-762. [42] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Brendan DolanGavitt, and Siddharth Garg. 2022. 대규모 언어 모델 코드 도우미의 보안 의미: 사용자 연구. arXiv 사전 인쇄본 arXiv:2208.09727(2022). [43] Carson D Sestili, William S Snavely, and Nathan M VanHoudnos. 2018. AI를 이용한 보안 결함 예측. arXiv 사전 인쇄본 arXiv:1808.09897(2018). [44] Mohammed Latif Siddiq, Shafayat H Majumder, Maisha R Mim, Sourov Jajodia, Joanna CS Santos. 2022. Transformer 기반 코드 생성 기술에서의 코드 냄새에 대한 실증적 연구. 키프로스 리마솔, 10월(2022). [45] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan. 2020. Intellicode compose: Transformer를 이용한 코드 생성. 제28회 ACM 유럽 소프트웨어 엔지니어링 컨퍼런스 및 소프트웨어 엔지니어링 기초 심포지엄 회의록. 1433-1443. [46] TabNine. 2023. &quot;소프트웨어 개발자를 위한 AI 어시스턴트&quot;. 한국어: https://www.tabnine.com/ [47] Blair Taylor 및 Shiva Azadegan. 2008. 보안 트랙을 넘어서: cs0 및 cs1에서 보안 통합. 컴퓨터 과학 교육에 관한 제39회 SIGCSE 기술 심포지엄의 진행 중. 320-324. [48] Junjie Wang, Bihuan Chen, Lei Wei 및 Yang Liu. 2019. Superion: Grammaraware greybox fuzzing. 2019년 IEEE/ACM 제41회 소프트웨어 공학 국제 컨퍼런스(ICSE). IEEE, 724-735. [49] Tongshuai Wu, Liwei Chen, Gewangzi Du, Chenguang Zhu 및 Gang Shi. 2021. 효과적인 데이터 표현을 통한 셀프 어텐션 기반 자동 취약점 탐지. 2021 IEEE Intl Conf on Parallel &amp; Distributed Processing with Applications, Big Data &amp; Cloud Computing, Sustainable Computing &amp; Communications, Social Computing &amp; Networking(ISPA/BDCloud/SocialCom/SustainCom). 892-899. https://doi.org/10.1109/ISPA-BDCloud-SocialComSustainCom52081.2021.[50] Jing Xie, Bill Chu, Heather Richter Lipford, and John T. Melton. 2011. ASIDE: IDE Support for Web Application Security. 제27회 연례 컴퓨터 보안 애플리케이션 컨퍼런스(플로리다주 올랜도, 미국)(ACSAC &#39;11)의 회의록에서. Association for Computing Machinery, 뉴욕, 뉴욕, 미국, 267–276. https://doi.org/10.1145/2076732.[51] Zhengzi Xu, Bihuan Chen, Mahinthan Chandramohan, Yang Liu, Fu Song. 2017. 스페인: 통증과 알약을 이해하기 위한 바이너리에 대한 보안 패치 분석. 2017년 IEEE/ACM 제39회 국제 소프트웨어 공학 컨퍼런스(ICSE). IEEE, 462-472. [52] Kunpeng Zhang, Xi Xiao, Xiaogang Zhu, Ruoxi Sun, Minhui Xue, Sheng Wen. 2022. 경로 전환이 더 많은 것을 알려줍니다: 런타임 프로그램 상태를 통한 퍼징 일정 최적화. https://doi.org/10.48550/ARXIV.2201.[53] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, Yang Liu. 2019. Devign: 그래프 신경망을 통해 포괄적인 프로그램 의미론을 학습하여 효과적인 취약성 식별. 신경 정보 처리 시스템의 발전 32(2019).
