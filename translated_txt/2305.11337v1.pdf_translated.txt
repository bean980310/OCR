--- ABSTRACT ---
3D 실내 장면 캡처링 기술은 널리 사용되지만, 생성된 메시는 크게 부족합니다. 이 논문에서는 강력한 자연어를 활용하여 다른 스타일의 새로운 방을 합성하는 &quot;RoomDreamer&quot;를 제안합니다. 기존의 이미지 합성 방법과 달리, 저희의 작업은 입력 장면 구조와 프롬프트에 맞춰 지오메트리와 텍스처를 동시에 합성하는 과제를 다룹니다. 중요한 통찰력은 장면을 장면 텍스처와 지오메트리를 모두 고려하여 전체로 처리해야 한다는 것입니다. 제안된 프레임워크는 지오메트리 유도 확산과 메시 최적화라는 두 가지 중요한 구성 요소로 구성됩니다. 3D 장면을 위한 지오메트리 유도 확산은 전체 장면에 동시에 2D를 적용하여 장면 스타일의 일관성을 보장합니다. 메시 최적화는 지오메트리와 텍스처를 함께 개선하고 스캔된 장면의 아티팩트를 제거합니다. 제안된 방법을 검증하기 위해 스마트폰으로 스캔한 실제 실내 장면을 사용하여 광범위한 실험을 수행하여 저희 방법의 효과를 입증합니다. 1.
--- INTRODUCTION ---
모바일 기기의 상업용 깊이 센서[42]와 LiDAR 센서[9]는 일상 생활에서 수백만 명의 사용자를 위한 3D 장면 캡처의 새로운 시대를 열었습니다. 그러나 이러한 센서에서 얻은 메시의 품질은 구멍, 왜곡된 물체, 흐릿한 질감과 같은 문제가 종종 나타나며 크게 바람직하지 않습니다. 또한 사용자는 일반적으로 주변 환경에 변화가 없다고 생각하고 장면을 추가로 편집하고 스타일을 지정하고 싶어할 수 있습니다. 비디오 결과: https://youtu.be/p4xgwj4QJcQ. 이러한 문제를 해결하기 위해 이 논문에서는 저품질 3D 메시의 기하학과 일치하지만 스타일이 다른 텍스트 프롬프트에서 3D 장면을 만드는 방법을 보여줍니다. 우리의 방법은 특히 확산 모델[16,29,32, 33, 40]인 2D 콘텐츠 생성의 최근 발전에 의해 동기를 부여받았습니다. 확산 모델의 한 가지 이점은 사용자가 제공한 텍스트 프롬프트가 이미지 합성 프로세스를 안내할 수 있으므로 다양한 스타일을 생성하는 데 다재다능하다는 것입니다. 2D 콘텐츠 생성을 고차원 공간으로 확장하는 간단한 방법 중 하나는 3D 장면을 멀티뷰 이미지 모음으로 처리하고 프레임별로 아웃페인팅 방식으로 이미지를 합성하는 것입니다. 그러나 이 방법은 아티팩트가 발생하고 생성된 이미지가 캡처된 장면의 지오메트리와 일치하지 않을 수 있습니다. 3D 장면과 &quot;모던 스타일&quot;과 같은 텍스트 프롬프트가 주어지면, 우리의 작업은 일관된 지오메트리와 텍스처로 텍스트에 맞춰진 새로운 장면을 생성할 수 있습니다. 우리의 접근 방식은 먼저 3D 장면의 텍스처를 생성한 다음 메시 텍스처와 지오메트리를 공동 최적화하는 것을 포함합니다. 우리는 메시의 중심에서 큐브맵(360° 이미지)으로 시작한 다음 탐색되지 않은 영역을 업데이트하여 생성된 텍스처가 장면의 스타일과 일관되도록 합니다. 메시 텍스처와 지오메트리를 공동 최적화하기 위해 생성된 2D 이미지 내에서 매끄러운 영역을 식별하고 그에 따라 메시 지오메트리를 업데이트하는 것을 제안합니다. 그림 1은 우리의 접근 방식의 결과를 보여줍니다. 우리의 방법은 텍스트에서 3D 객체를 생성하는 이전 작업[20, 25, 36]과 2D 이미지에 기반한 3D 콘텐츠를 생성하는 이전 작업[10, 14, 22, 25]과 두 가지 핵심 측면에서 다릅니다. 첫째, 우리는 흔하지만 거의 탐구되지 않은 스캔된 장면의 존재를 가정하기 때문에 참신하고 실용적인 설정을 고려합니다. 우리의 독특한 설정에서 우리는 주로 새로운 지오메트리를 생성하는 데 초점을 맞춘 이전 기술과 달리 기존 지오메트리를 개선하는 것을 목표로 합니다. 둘째, 우리의 접근 방식은 2D 확산 모델에 대한 다른 통찰력에서 동기를 얻었습니다. 우리의 동기는 생성된 각 2D 이미지 뒤에 있는 좋은 기본 지오메트리에 있는 반면, 이전 방법은 확산을 통해 반복적으로 일련의 다중 뷰 2D 이미지를 생성하는 데 동기를 얻었습니다. 메시 기반 표현을 2D 이미지에 쉽게 투영할 수 있지만 2D 입력에서 메시 지오메트리를 개선하는 것이 훨씬 더 어렵습니다. 광범위한 실험을 통해 우리의 접근 방식이 많은 실제 응용 프로그램에서 사용하기에 정확하고 유연하다는 것이 입증되었습니다. 요약하자면, 이 작업의 기여는 세 가지입니다. • 주어진 메시를 편집하기 위해 2D 확산 모델을 사용하는 새로운 프레임워크를 소개합니다. 저희 프레임워크는 텍스트 프롬프트에 따라 지오메트리와 텍스처를 모두 편집하고 스타일화하는 것을 용이하게 합니다. • 확산 모델을 제어하기 위한 2D 확산 방식을 설계하여 입력 메시에 대한 장면 일관되고 구조적으로 정렬된 텍스처를 생성합니다. • 스마트폰으로 스캔한 실제 실내 메시를 사용하여 광범위한 실험을 수행하여 프레임워크의 효과성과 신뢰성을 검증합니다. 2.
--- RELATED WORK ---
s 3D 콘텐츠 생성 분야[16, 32, 33]는 최근 몇 년 동안 상당히 개선되었습니다. 이 분야의 연구를 두 가지 범주로 나눕니다. 첫째, 콘텐츠 생성 프로세스를 지시하기 위한 감독을 위해 3D 기준 진실 콘텐츠를 사용하는 것입니다[5, 13, 23]. 이는 고품질 3D 기준 진실의 가용성으로 인해 제한적입니다. 두 번째 연구 범주는 기존 2D 이미지 생성기의 성능[29]을 사용하여 3D 콘텐츠를 생성하는 데 중점을 둡니다. Poole 등[25]은 확산 모델의 구조를 사용하여 3D 신경장에 감독 신호를 제공하는 Score Distillation Sampling(SDS)을 제안했습니다. 동시에 Wang 등[36]은 3D 생성을 위해 사전 학습된 2D 확산 모델을 리프팅하는 Score Jacobian Chaining을 제안했습니다. Lin 등[20]은 먼저 신경장을 통해 3D 콘텐츠를 표현한 다음 메시를 생성하여 2D 확산 유도 생성의 품질과 효율성을 개선하는 Magic3D를 제시했습니다. Fantasia3D [7]는 3D 자산 생성을 기하학 및 텍스처 생성 문제로 분해하도록 제안했습니다. SDS는 또한 기존 2D 이미지를 3D 모델로 변환하는 데 적용되었습니다 [38,43]. Liu et al. [21]는 기존 2D 확산 모델을 카메라 포즈 인식으로 조정하여 다중 뷰 이미지를 직접 생성할 수 있도록 제안했습니다. Chan et al. [6]는 안정적인 확산 백본을 사용하여 기하학적 사전을 통합하여 단일 입력에서 새로운 뷰를 합성하는 것을 제안합니다. 또 다른 최근 작업인 Text2Room [17]은 2D 확산 모델과 깊이 추정 모델을 사용하여 텍스트 프롬프트에서 텍스처 룸 메시를 생성합니다. 우리의 가장 큰 차이점은
--- METHOD ---
입력 실내 메시에 대한 지오메트리를 공동으로 개선하고 텍스처를 생성하는 것을 목표로 합니다. 위 그림은 파노라마 뷰와 깊이 맵이 있는 실제 방을 보여줍니다. 그런 다음 텍스트 프롬프트(가운데)가 주어지면 모델은 다른 스타일(아래쪽 행)의 새로운 방을 합성할 수 있습니다. 입력 메시는 종종 품질이 낮고, 저희 방법은 텍스처와 지오메트리를 모두 다듬을 수 있습니다. 초록 3D 실내 장면 캡처 기술은 널리 사용되지만 생성된 메시는 크게 부족합니다. 이 논문에서는 강력한 자연어를 활용하여 다른 스타일의 새로운 방을 합성하는 &quot;RoomDreamer&quot;를 제안합니다. 기존의 이미지 합성 방법과 달리 저희 작업은 입력 장면 구조와 프롬프트에 맞춰 지오메트리와 텍스처를 동시에 합성하는 과제를 다룹니다. 중요한 통찰력은 장면이 장면 텍스처와 지오메트리를 모두 고려하여 전체로 처리되어야 한다는 것입니다. 제안된 프레임워크는 지오메트리 가이드 확산과 메시 최적화라는 두 가지 중요한 구성 요소로 구성됩니다. 3D 장면을 위한 지오메트리 가이드 확산은 전체 장면에 동시에 2D를 적용하여 장면 스타일의 일관성을 보장합니다. 메시 최적화는 지오메트리와 텍스처를 함께 개선하고 스캔된 장면의 아티팩트를 제거합니다. 제안된 방법을 검증하기 위해 스마트폰으로 스캔한 실제 실내 장면을 광범위하게 사용합니다.
--- EXPERIMENT ---
s를 통해 방법의 효과가 입증되었습니다. 1. 서론 모바일 기기의 상용 깊이 센서[42]와 LiDAR 센서[9]는 일상 생활에서 수백만 명의 사용자에게 3D 장면 캡처의 새로운 시대를 열었습니다. 그러나 이러한 센서에서 얻은 메시의 품질은 구멍, 왜곡된 물체, 흐릿한 질감과 같은 문제가 종종 나타나며 바람직하지 않습니다. 또한 사용자는 일반적으로 주변 환경에 변화가 없다고 생각하고 장면을 추가로 편집하고 스타일을 지정하고 싶어할 수 있습니다. 비디오 결과: https://youtu.be/p4xgwj4QJcQ. 이러한 문제를 해결하기 위해 이 논문에서는 저품질 3D 메시의 기하학과 일치하지만 스타일이 다른 텍스트 프롬프트에서 3D 장면을 만드는 방법을 보여줍니다. 우리의 방법은 특히 확산 모델[16,29,32, 33, 40]과 같은 2D 콘텐츠 생성의 최근 발전에 의해 동기를 부여받았습니다. 확산 모델의 한 가지 이점은 사용자가 제공한 텍스트 프롬프트가 이미지 합성 프로세스를 안내할 수 있으므로 다양한 스타일을 생성하는 데 다재다능하다는 것입니다. 2D 콘텐츠 생성을 고차원 공간으로 확장하는 간단한 방법 중 하나는 3D 장면을 다중 뷰 이미지 모음으로 처리하고 프레임별로 아웃페인팅 방식으로 이미지를 합성하는 것입니다. 그러나 이 방법은 아티팩트가 발생하고 생성된 이미지가 캡처된 장면의 지오메트리와 일치하지 않을 수 있습니다. 3D 장면과 &quot;모던 스타일&quot;과 같은 텍스트 프롬프트가 주어지면, 우리의 작업은 일관된 지오메트리와 텍스처로 텍스트에 맞춰진 새로운 장면을 생성할 수 있습니다. 우리의 접근 방식은 먼저 3D 장면의 텍스처를 생성한 다음 메시 텍스처와 지오메트리를 공동 최적화하는 것을 포함합니다. 우리는 메시의 중심에서 큐브맵(360도 이미지)으로 시작한 다음 탐색되지 않은 영역을 업데이트하여 생성된 텍스처가 장면의 스타일과 일관되도록 합니다. 메시 텍스처와 지오메트리를 공동 최적화하기 위해 생성된 2D 이미지 내에서 매끄러운 영역을 식별하고 그에 따라 메시 지오메트리를 업데이트하는 것을 제안합니다. 그림 1은 우리 접근 방식의 결과를 보여줍니다. 우리 방법은 텍스트에서 3D 객체를 생성하는 이전 작업[20, 25, 36]과 2D 이미지를 기반으로 한 3D 콘텐츠를 생성하는 작업[10, 14, 22, 25]과 두 가지 핵심 측면에서 다릅니다. 첫째, 우리는 흔하지만 거의 탐구되지 않은 스캔된 장면이 있다고 가정하기 때문에 참신하고 실용적인 설정을 고려합니다. 우리의 독특한 설정에서 우리는 주로 새로운 지오메트리를 생성하는 데 초점을 맞춘 이전 기술과 달리 기존 지오메트리를 개선하는 것을 목표로 합니다. 둘째, 우리의 접근 방식은 2D 확산 모델에 대한 다른 통찰력에서 동기를 얻었습니다. 우리의 동기는 생성된 각 2D 이미지 뒤에 있는 좋은 기본 지오메트리에 있는 반면, 이전 방법은 확산을 통해 반복적으로 일련의 다중 뷰 2D 이미지를 생성하는 데 동기를 얻었습니다. 메시 기반 표현을 2D 이미지에 쉽게 투영할 수 있지만 2D 입력에서 메시 지오메트리를 개선하는 것이 훨씬 더 어렵다는 점에 유의하십시오. 광범위한 실험을 통해 우리의 접근 방식이 정확하고 많은 실제 응용 프로그램에서 사용하기에 유연하다는 것이 입증되었습니다. 요약하자면, 이 작업의 기여는 세 가지입니다. • 주어진 메시를 편집하기 위해 2D 확산 모델을 사용하는 새로운 프레임워크를 소개합니다. 우리의 프레임워크는 텍스트 프롬프트를 기반으로 기하학과 텍스처 모두의 편집과 스타일 지정을 용이하게 합니다. • 우리는 확산 모델을 제어하기 위한 2D 확산 체계를 설계하여 입력 메시에 대해 장면 일관되고 구조적으로 정렬된 텍스처를 생성합니다. • 우리는 스마트폰으로 스캔한 실제 실내 메시를 사용하여 광범위한 실험을 수행하여 프레임워크의 효과와 안정성을 검증합니다. 2. 관련 연구 3D 콘텐츠 생성 영역[16, 32, 33]은 최근 몇 년 동안 상당히 개선되었습니다. 우리는 이 분야의 연구를 두 가지 범주로 간주합니다. 첫째, 콘텐츠 생성 프로세스를 지시하기 위한 감독을 위해 3D 기준 진실 콘텐츠를 사용합니다[5, 13, 23]. 이는 고품질 3D 기준 진실의 가용성으로 인해 제한적입니다. 두 번째 연구 범주는 기존 2D 이미지 생성기[29]의 힘을 사용하여 3D 콘텐츠를 생성하는 데 중점을 둡니다.Poole 등[25]은 확산 모델의 구조를 사용하여 3D 신경장에 감독 신호를 제공하는 Score Distillation Sampling(SDS)을 제안했습니다.동시에 Wang 등[36]은 3D 생성을 위해 사전 학습된 2D 확산 모델을 들어올리는 Score Jacobian Chaining을 제안했습니다.Lin 등[20]은 먼저 신경장을 통해 3D 콘텐츠를 표현한 다음 메시를 생성하여 2D 확산 유도 생성의 품질과 효율성을 개선하는 Magic3D를 제시했습니다.Fantasia3D[7]는 3D 자산 생성을 기하학 및 텍스처 생성 문제로 분해하는 것을 제안했습니다.SDS는 또한 기존 2D 이미지를 3D 모델로 변환하는 데 적용되었습니다[38,43].Liu 등[21]은 기존 2D 확산 모델을 카메라 포즈 인식하도록 조정하여 다중 뷰 이미지를 직접 생성할 수 있도록 제안했습니다. Chan et al. [6]은 안정적인 확산 백본을 사용하여 기하 사전을 통합하여 단일 입력에서 새로운 뷰를 합성하는 것을 제안합니다.또 다른 최근 연구인 Text2Room [17]은 2D 확산 모델과 깊이 추정 모델을 사용하여 텍스트 프롬프트에서 텍스처가 있는 룸 메시를 생성합니다.우리의 방법과 위의 연구 사이의 가장 큰 차이점은 우리 방법이 스캔된 메시에 의해 안내되므로 새로 생성된 3D 장면이 입력 장면과 정확하게 정렬되지만 스타일이 다르다는 것입니다.기존 3D 자산을 편집하기 위해 InstructN2N [14]은 2D 멀티뷰 이미지를 반복적으로 업데이트하는 방법론을 제안했습니다.이는 InstructP2P [4]로 알려진 2D 이미지 편집 모델을 기반으로 했습니다.InstructN2N은 2D 이미지에서 편집을 수행했기 때문에 완전히 새로운 장면을 꿈꾸는 능력이 이미지 기반 편집으로 제한되었을 수 있습니다.반면에 우리의 접근 방식은 기하 제어 2D 확산 생성에 의존하므로 원본 장면의 텍스처에 의해 방해받지 않습니다. 3D 데이터 수집의 한 가지 큰 과제는 불완전한 장면 스캐닝 결과에 있습니다. 모바일 기기의 Lidar는 성능과 해상도가 제한적이기 때문에 장면의 일부가 포인트 클라우드에서 종종 누락됩니다. 자체 감독 생성[8], 검색 기반 생성[30], 스타일 전송[18]과 같이 생성적 사전 확률을 사용하여 3D 재구성을 개선하기 위한 연구가 꽤 많이 있었습니다. 재구성 문제 외에도 일부 이전 연구에서는 3D 실내 장면 생성을 객체 레이아웃 예측 문제로 처리했습니다[24, 28, 37]. 레이아웃을 예측한 후 3D-FRONT[12]와 같은 3D 가구 데이터 세트에서 객체를 검색하여 장면에 배치합니다. Plan2Scene[35]과 같은 다른 접근 방식은 실내 장면의 평면도와 이미지 관찰을 사용하여 전체 장면에 대한 텍스처 메시를 예측합니다. 한편, GSN[11], GAUDI[3], CC3D[1]는 신경 복사장을 사용하여 실내 장면의 이미지를 생성하는 데 중점을 두고 있습니다. 이러한 연구 작업은 우리의 작업에 영감을 주고 있습니다. 실제로 우리는 실내 장면 메시를 개선하는 데 중점을 두고 있으며, 특히 3D 기하학의 부드러움과 기하학과 시각적 텍스처의 일치에 중점을 두고 있습니다. 자세한 내용은 이후 섹션에서 설명하겠습니다. 3. 방법 우리 접근 방식의 입력에는 기하학 및 텍스처 정보가 모두 있는 3D 메시와 사용자가 제공한 텍스트 프롬프트가 포함됩니다. 우리 방법은 두 단계로 구성됩니다. 먼저 3D 장면을 2D 이미지로 렌더링하고 새로운 확산 프로세스를 사용하여 새로운 스타일을 합성합니다. 그런 다음 새로운 텍스처와 세련된 기하학으로 새로운 3D 메시를 재구성합니다. 우리 방법에 대한 개요는 그림 2에 나와 있습니다. 3.1. 3D 장면을 위한 기하학 가이드 확산 2D 이미지를 합성하는 것보다 새로운 3D 장면을 합성하는 것이 더 어려운 이유는 표준 확산 모델[33]이 서로 다른 뷰에서 쉽게 불일치를 만들 수 있기 때문입니다. 2D 이미지 확산 모델을 사용하여 장면 텍스처를 생성하는 간단한 방법은 무작위로 배치된 카메라에서 시작하여 그림 3에 나와 있는 것처럼 주변 카메라를 반복적으로 샘플링하여 관찰되지 않은 영역을 아웃페인팅합니다[31, 39]. 그러나 이 기준선 방법은 눈에 띄는 아티팩트(그림 6)를 생성한다는 것을 관찰했는데, 이는 부분적으로 2D 확산 모델의 제한된 아웃페인팅 기능에 기인할 수 있습니다. 뷰별 아웃페인팅 생성 프로세스에서 발생하는 아티팩트를 피하기 위해 먼저 장면의 중앙 뷰가 있는 360° 이미지를 생성하는 것을 제안합니다. 확산 프로세스를 큐브맵 패치로 확장하여 2D 확산 모델로 파노라마 이미지를 생성할 수 있습니다[2,41]. 텍스트 프롬프트에 따라 조건지어진 기존 확산 모델과 달리, 본 방법은 텍스트 프롬프트 Ctext와 깊이 맵 D 둘 다에 따라 조건지어지므로 확산 단계는 Xt-1 = f(Xt, Ctext, D)입니다. 이전 연구에 따라 확산 모델을 f: (RH 7×³, C) → RH×W×3으로 표시되는 매핑 함수로 표시합니다. 여기서 RH×W×는 크기가 H×W인 이미지에 대한 공간을 나타내고 C는 프롬프트와 이미지 깊이를 모두 포함하는 조건부 사전 공간을 나타냅니다. 또한 X0 = f→0 (Ctext, D)를 무작위 노이즈로부터의 전체 확산 프로세스로 표시하고 텍스트와 깊이에 따라 조건지어집니다. HXWX 그러나 큐브맵 생성을 제어하기 위해 깊이 맵을 직접 사용하면 깊이 값이 카메라 포즈와 상관관계가 있기 때문에 불일치가 발생할 수 있습니다. 카메라 포즈가 다르면 큐브맵 면에서 깊이 값이 일치하지 않습니다. 그림 5(a)는 깊이 맵의 불일치를 보여줍니다. 각 카메라와 연관된 깊이 맵은 카메라와 평면 사이의 거리로 표현됩니다. 결과적으로 깊이 값은 동일한 평면에 대해서도 뷰마다 크게 다를 수 있으며 아티팩트가 발생합니다. 불일치를 더욱 줄이기 위해 점과 카메라 원점 사이의 기하학적 거리를 나타내는 거리 맵 D를 고려합니다. 세계 좌표 P와 연관된 화면 좌표가 있는 점을 (u, v)라고 하면 거리 맵 Â의 (u, v) 픽셀은 ||p – o||입니다. 여기서 o는 카메라 원점의 세계 좌표입니다. 깊이 맵과 거리 맵의 비교는 그림 4에 나와 있습니다. 거리 맵 Ô와 깊이 맵 D는 확산 프로세스를 제어하는 측면에서 서로 다른 속성을 갖습니다. 구조는 D의 RGB 이미지와 잘 정렬되지만 Ô에서는 왜곡됩니다. 예를 들어, 평면의 이미지 라플라시안은 D에서는 0이 되지만 D에서는 그렇지 않습니다. 그러나 큐브맵의 테두리는 D에 따라 더 매끄럽게 변경되며 이는 그림 2(b)에서 관찰할 수 있습니다. 현실적인 기하학적 정렬과 경계 일관성을 모두 달성하기 위해 블렌딩 방식을 제안합니다. 큐브 맵 Ia와 IƖ의 교차점에 있는 이미지 패치 p의 경우, ra와 r을 각각 패치의 I와 I의 픽셀 비율로 합니다. 그런 다음 λ = |ra – rɩ|로 정의합니다. 노이즈 제거 프로세스의 각 단계는 다음 방정식을 사용하여 계산합니다. Xt−1 = λf (Xt, Ctext, Dp)+(1 −λ) ƒ (Xt, Ctext, Ôp), (1) 여기서 Dp와 Dp는 각각 노이즈 제거되는 패치 p에 대한 깊이와 거리 맵입니다. 큐브맵을 생성한 후 미분 가능한 렌더러[19]를 사용하여 메시 텍스처를 업데이트합니다. 그런 다음 장면에서 카메라를 무작위로 샘플링하고 360° 이미지로 캡처되지 않은 영역은 확산 모델을 사용하여 마스크 생성(즉, 아웃페인팅)을 통해 업데이트합니다. 큐브맵으로 캡처된 영역과 그렇지 않은 영역을 판단하기 위해, 우리는 단순히 정점을 이전 카메라에 투사하여 어떤 정점이 가려졌는지 볼 수 있습니다. 3.2. 메시 최적화 RoomDreamer의 입력과 출력은 모두 3D 메시입니다. 메시를 (V, F, Vc)로 표시합니다. 여기서 V, F, Vc는 각각 정점, 면, 정점의 색상입니다. 특정 카메라 π에 대해 이 뷰에서 깊이 맵 D와 RGB 이미지 X를 렌더링할 수 있습니다.X=Rx(V, F, VC, π) D =RD(V, F,T) (2) (3)입력 지오메트리 샘플 큐브맵 카메라 샘플 랜덤 카메라 미분 가능 렌더링 지오메트리 안내 + 프롬프트 &quot;아늑한 나무 오두막...&quot; 2D 확산 아웃페인팅 입력 텍스처 입력 지오메트리 동일 카메라 미분 가능 렌더링 RGB 업데이트 텍스처 L텍스처 단안 깊이 추정기 업데이트 정점 면 깊이 L지오메트리 3D 장면을 위한 안내 확산 가상 깊이 메시 최적화 그림 2. 방법의 전체 프레임워크.먼저 3D 장면을 위한 지오메트리 안내 확산 단계에서 장면을 나타내는 큐브맵을 만든 다음, 3.1절에서 자세히 설명한 대로 큐브맵의 덮이지 않은 영역을 아웃페인팅합니다.그런 다음 메시 텍스처와 지오메트리를 최적화합니다.지오메트리 최적화를 위해 단안 깊이 예측을 가상 감독으로 활용하고 3.1절에서 자세히 설명한 대로 장면의 매끄러운 영역을 정렬합니다. 3.2. far 샘플 뷰샘플 뷰표면 확산 마스크 확산 깊이 근처 깊이 맵 far 최적화 (a) 뷰별 아웃페인팅 샘플 중심의 큐브맵 확산 최적화 카메라 원점 거리 거리 맵 근처 (b) 큐브맵 기반 텍스처 그림 3. 장면 텍스처를 생성하는 방법. 단계 &quot;확산&quot;은 확산 모델을 사용하여 2D 이미지를 생성하는 것을 의미합니다. &quot;최적화&quot;는 생성된 2D 이미지로 메시 텍스처를 업데이트하는 것을 의미합니다(식 (5) 참조). (a) 2D 확산 모델을 사용한 아웃페인팅을 기반으로 하는 간단한 기준선. 아웃페인팅은 마스크 확산을 통해 달성되고 회색 영역은 마스크 영역이 확산을 통해 변경되지 않음을 의미합니다. (b) 장면에 대한 큐브맵을 생성한 다음 메시 텍스처를 최적화합니다. 그림 4. 깊이 맵과 거리 맵의 그림. 깊이 맵은 객체 평면과 화면 평면 사이의 길이를 측정하는 반면, 거리 맵은 점과 카메라 원점 사이의 길이를 측정합니다. 여기서 R은 렌더링 함수를 나타냅니다. 구현에서 우리는 미분 가능한 렌더링[19]을 사용하여 그래디언트를 3D로 역전파하여 합성된 이미지에서 메시를 생성할 수 있습니다. {πk}가 카메라 세트를 나타내면 3.1의 방법을 사용하여 이미지 시퀀스 {X}를 생성할 수 있습니다. 그런 다음 definecubemap face A cubemap face B (a) 깊이 맵을 사용한 확산 cubemap face A cubemap face B 지오메트리도 평면이어야 합니다. 이 관찰을 모델링하기 위해 먼저 기성품 단안 깊이 추정기 E(예: MiDaS [27])를 사용하여 {X}에서 깊이 맵 Den을 재구성합니다. 그런 다음 평면 영역에 대한 조건을 정의합니다. |ADgen (u, v)| &lt; T k (6) 여기서 ▲는 깊이 맵 Dk의 라플라시안이고 7은 매끄러운 영역을 결정하기 위한 임계값입니다. 그런 다음 매끄러운 영역을 P = {(u, v)|▲Ɗgen (u, v)| &lt; τ}로 표시합니다. P에서 재구성된 깊이 맵 D는 가능한 한 매끄러울 것으로 예상합니다. 즉, 라플라시안이 0에 가깝습니다. 따라서 다른 손실 함수를 얻습니다. Lgeometry = |ADk (u, v)|, k (u, v)EP (7) Dk = RD(V, F‚Ã²)인 벽에 투사합니다. (b) 거리 맵을 사용한 확산 그림 5. 깊이 맵과 거리 맵의 다른 제어 효과. 깊이 맵은 큐브 맵의 두 면의 조인트 경계에서 빠른 변화를 보입니다. 반대로 거리 맵은 매끄럽게 변경됩니다. 깊이 제어를 사용하여 일관된 큐브 맵을 생성하는 것은 어려워지는 반면, 거리 맵을 사용하면 더 일관된 텍스처가 생성됩니다. 그러나 거리 맵 O는 확산 모델이 학습 중에 깊이 맵에 따라 조건이 지정되므로 벽의 창문과 같은 아티팩트가 발생합니다. 텍스처 기반 손실: Ltexture Σ ||RX (V, F, VC, T¹³) — Xh³||², (4) k 그러면 경사 하강법을 통해 메시 텍스처를 재구성하는 기준선 방법을 얻습니다. Vc + Vc γ aLtexture avc - Ve-γΣ ORx (V, F, V, Tk) aLtexture Rx (V, F, VC, πk²) OVC k (5) Eq. (5)는 V만 업데이트할 수 있고 기하 V, F는 업데이트할 수 없다는 점에 유의하십시오. 이는 미분 가능한 렌더[19]가 기하, 즉 Rx €0, 0에 대한 경사를 계산할 수 없기 때문입니다. ǝv = aRx ƏF 추가 단계는 V, F를 Vc와 함께 최적화하는 것입니다. 입력 메시는 종종 품질이 낮은 기하(예: 구멍 있음)를 나타내므로 기하 V, F를 조정하여 이미지 시퀀스 {X}와 일치시킬 수 있기를 바랍니다. 필수적인 관찰은 합성된 장면에 평면 모양과 같은 매끄러운 영역이 포함된 경우 재구성된다는 것입니다.기하학적 손실 외에도 생성된 이미지는 다음 손실로 장면의 텍스처를 업데이트하는 데 사용됩니다.장면 메시를 업데이트하는 전반적인 진행 상황은 다음과 같이 표현할 수 있습니다.VeVe Y aLtexture ave Lgeometry VV-Y (8) FF-Yav ƏL geometry OF 알고리즘 개요는 알고리즘 1에 나와 있습니다.4. 실험 문제 설정은 기존 문헌에서 광범위하게 탐구되지 않은 스캔된 룸 메시로 입력하는 것을 가정합니다.저희는 저희 방법의 결과를 두 그룹의 작업과 비교합니다.(1) 기하학 유도 확산, 거리 맵, 매끄러운 영역 정규화 등과 같은 하위 모듈이 없는 저희의 기준선에 대한 절제 연구.(2) Score Distillation Sampling(SDS) [25] 및 InstructN2N [14]을 포함한 NeRF 기반 방법. 입력 메시를 투영하여 NeRF를 재구성하는 데 사용될 다중 뷰 이미지를 생성할 수 있습니다.NeRF 재구성은 계산적으로 비용이 많이 들고 NeRF의 재구성된 지오메트리가 항상 정확하지는 않으므로 메시 입력의 기준 진실 깊이를 사용하여 InstructN2N [14]의 재구성 성능을 높여 공정한 비교를 얻습니다.4.1. 데이터 세트 및 구현 세부 정보 우리는 ARKitScenes 데이터 세트 [9]에서 실험을 수행했습니다.이 데이터 세트는 입력 + &quot;중국 궁전, 황궁&quot; 5 赤血 + &quot;일본식, 선, 다다미&quot; + &quot;목조 오두막&quot; + &quot;군사 기지, 군대&quot;로 캡처한 실제 실내 장면으로 구성되어 있습니다.그림 6. 출력 메시의 정성적 비교.아웃페인팅은 텍스처가 순차적으로 아웃페인팅되는 반면 장면을 전체로 처리한다는 점에서 기준 방법입니다.아웃페인팅 기준선에서 스트립 모양 아티팩트를 관찰할 수 있습니다.iPhone. 우리의 방법은 다양한 객실 유형과 평면도를 포함하는 ARKitScenes의 검증 세트에서 정성적, 정량적으로 평가됩니다. 큐브맵을 생성하기 위해 메시의 중심에 카메라 원점을 설정합니다. 깊이 기반 이미지를 생성하기 위해 ControlNet[40]을 활용하고 안내 스케일 및 확산 단계 수 T와 같은 기본 하이퍼파라미터를 유지합니다. 큐브맵에 포함되지 않은 영역을 포함하기 위해 중심 주변에서 K = 100개의 카메라를 무작위로 선택하고 확산 모델의 마스크 생성 모드를 사용합니다. 우리는 MiDaS[27]를 사용하여 단안 깊이를 예측합니다. 최적화 프로세스 동안 우리는 기하학(V, F)과 정점 색상 Ve를 모두 최적화하기 위해 학습률이 0.001인 Adam 최적화기를 사용합니다. 최적화는 총 N = 1000단계 동안 실행됩니다. 한 장면을 하나의 A100 GPU로 처리하는 데 약 15분이 걸립니다.&quot;나무 오두막&quot; 1.PD &quot;우주 오두막&quot; 입력 우리 SDS [25] InstructN2N [4,14] 그림 7. 다른 2D 확산 기반 3D 편집 방법과의 비교.SDS [25]는 지오메트리를 개선할 수 있지만 생성된 텍스처가 다소 흐릿합니다.InstructN2N [14]은 순전히 이미지 기반 편집 방법인 백본 InstructP2P [4]에 의해 제한되므로 제시된 입력 이미지로 인해 오해의 소지가 있습니다.우리의 방식은 지오메트리와 텍스처 생성을 잘 처리할 수 있습니다. 알고리즘 1 방법의 전체 파이프라인 입력: 시스템 요구 사항: 2D 확산 모델 f, 단안 깊이 추정기 E 사용자로부터: 메시(V, F, Vc), 텍스트 프롬프트 Ctext 출력: 새 스타일로 업데이트된 메시(V*, F*, V*) 1단계: 3D 장면을 위한 기하 기반 확산 1: 장면 중앙에 큐브맵 카메라 설정 2: 큐브맵 깊이 D와 거리 D 획득 3: f, Ctext, D, D 방정식에서 큐브맵 Xcube 생성 (1) ▷ 4 사용: 덮이지 않은 영역에 대해 K개의 임의 카메라 {k} &gt; 샘플링 5: k 1~K에 대해 6: 7: 8: 9: ПК가 X 큐브로 덮이지 않은 영역을 보면 πk에서 깊이 Dk를 획득 f, Ctext, Dk에서 이미지 X½ 생성 end if 10: end for 2단계: 메시 최적화 11: 생성된 모든 이미지의 X에 대해 do 12: 단안 깊이 추정을 사용하여 Den = E(X)를 구함 13: end for 14: n 1~N에 대해 do = 15: Eq. (8)을 사용하여 3D 메시를 업데이트함. 16: end for 4.2. 기준선과 비교 먼저 아웃페인팅 기준선과 대조하여 접근 방식에 대한 정성적 평가를 수행합니다. 이 분석의 결과는 그림 6에 나와 있으며, 여기서 큐브맵 기반 장면 텍스처 생성을 아웃페인팅 기준선과 비교합니다. 분명히, 아웃페인팅으로 생성된 장면은 확산 모델의 마스크 생성 모드의 결함 있는 결과에서 발생하는 스트립과 같은 아티팩트를 보여줍니다. 스트립과 같은 아티팩트의 예는 장면의 벽에서 관찰할 수 있습니다. 반대로, 우리의 기술은 전체 장면에서 유지되는 고품질의 균일한 스타일의 이미지를 일관되게 생성합니다. 이러한 우수성은 큐브맵 텍스처 생성 체계를 사용한 데 기인할 수 있습니다. 나아가, 그림 8에서 큐브맵 확산 중에 거리 맵(참조, 방정식 (1))을 혼합하는 것의 영향을 탐구합니다. 주목할 점은 두 큐브맵 면의 조인트 영역 패치를 나타내는 주황색 상자로 구분된 영역에서 중요한 차이를 관찰할 수 있습니다. 입력 장면을 평가하면 이 영역의 벽이 왜곡되어 확산에 대한 깊이 신호도 왜곡된다는 것을 알 수 있습니다. 구체적으로, 그림의 두 번째 행은 이 영역이 실제로 단일 평면이 아닌 두 벽의 회전 모서리로 처리되었음을 보여줍니다. 생성된 모서리는 깊이 제어 신호의 왜곡으로 인한 결과입니다. 세 번째 그림에서와 같이 확산 중에 거리 맵 제어 노이즈 제거를 통합하면 패치가 단일 벽 평면으로 올바르게 간주됩니다. 조인트 영역 외에도 빨간색 화살표, 행으로 표시된 영역의 정렬과 같은 다른 이점도 관찰합니다. 그림 9에서 원래 입력 장면의 지오메트리와 업데이트된 장면의 지오메트리를 비교합니다. 생성에 사용된 텍스트 프롬프트는 &quot;왕궁&quot;입니다. 이 방법을 사용한 후 메시가 이전보다 더 부드러워진 것을 확인할 수 있습니다. 또한 이 방법은 메시의 일부 구멍을 성공적으로 채웁니다. 거리 R이 없는 입력 장면 거리 큐브맵 면 A 큐브맵 면 BR 큐브맵 면 C 큐브맵 면 D 큐브맵 면 A 그림 8. 거리 맵 블렌딩 단계가 있는 큐브맵 생성 및 없는 큐브맵 생성(식 (1) 참조). 거리 맵 블렌딩 없이 2D 확산은 큐브맵의 경계 영역(즉, 주황색 상자 영역)에 두 개의 평면을 생성하는 경향이 있습니다. 거리 맵 블렌딩을 사용하면 경계 영역이 하나의 평면으로 처리됩니다.원래 기하 구조 업데이트된 기하 구조 그림 9. 기하 구조 편집의 시각화.4.3. NeRF 기반 접근 방식과 비교 이 하위 섹션에서는 우리 방법을 두 가지 주요 2D 확산 기반 3D 생성 방법인 Score Distillation Sampling(SDS) [25] 및 InstructN2N [14]과 비교합니다.2D 확산 모델로 안정적 확산을 사용하는 SDS의 오픈 소스 버전[34]을 사용했습니다.다음으로 그림 7에서 볼 수 있듯이 우리 방법의 성능과 최근에 제안된 2D 기반 3D 편집 방법의 성능을 비교 분석합니다.공정한 비교를 위해 SDS 실험에 깊이 안내 확산 모델(즉, 우리와 동일)을 사용합니다.결과를 검토해 보니 SDS로 인해 흐릿한 효과가 나타날 수 있는데, 이는 SDS를 활용한 2D 이미지 생성에 대한 연구에서 최근 보고된 현상과 유사합니다[15]. InstructN2N[4, 14]의 경우, 그 성능은 주로 백본인 InstructP2P[4]의 효능에 의해 영향을 받는다는 점에 유의합니다. InstructP2P는 입력 이미지에만 의존하고 깊이와 같은 기하학에 따라 조건이 지정되지 않는다는 점을 언급하는 것이 중요합니다. 결과적으로 InstructN2N에서 첫 번째 행의 소파의 흰색 공간과 같이 입력 이미지의 빈 영역(예: 흰색 영역)으로 인해 모델이 오도되는 경우를 관찰합니다. 게다가 순수하게 이미지 기반 편집 방식을 사용하면 두 번째 행의 나무 바닥에서 알 수 있듯이 생성된 이미지의 다양성이 제한될 수 있습니다. 이 그림에 제시된 비교는 제안하는 방식이 최적이 아닌 품질의 입력 메시로 텍스처를 성공적으로 생성할 수 있음을 보여줍니다. 4.4. 양적 평가 양식화된 실내 장면의 품질은 일반적으로 주관적이지만, 생성된 결과의 양적 분석을 위해 [4, 14]의 평가 방식을 채택했습니다. 이 평가 프로세스는 CLIP[26]에서 제공하는 임베딩을 기반으로 합니다. 평가에는 두 가지 지표가 사용됩니다. &quot;텍스트-이미지 유사성&quot;이라고 하는 첫 번째 지표는 주어진 텍스트 프롬프트와 생성된 이미지 간의 내적을 계산합니다. 값이 높을수록 텍스트와 이미지 벡터 간의 유사성이 높아지며, 이는 두 벡터 간의 각도가 작음을 의미합니다. &quot;방향 일관성&quot;이라고 하는 두 번째 지표는 서로 다른 뷰에서 생성된 장면의 일관성을 평가합니다. 점수는 다음과 같이 계산됩니다. oa와 b로 표시된 원래 입력 뷰의 두 CLIP 임베딩과 g₁와 gb로 표시된 생성된 뷰의 두 CLIP 임베딩이 주어지면 점수는 다음과 같습니다. 텍스트-이미지 유사성 InstructN2N [4,14] 0.SDS [25] 0.우리의 0.방향 일관성 0.0.0.표 1. 다른 편집 방법과의 정량적 비교. 두 지표(식 (9) 참조)의 경우 값이 높을수록 성능이 더 우수함을 나타냅니다. 큐브맵 텍스트-이미지 유사도 0.거리 없음 0.지오 최적화 없음 0.뷰 일관성 0.0.0.전체 모델 0.0.표 2. 양적 절제. &quot;w/o Cubemap&quot;은 아웃페인팅 기준선입니다. &quot;w/o Distance&quot;는 거리 맵 기반 블렌딩 체계를 제거하는 것을 의미합니다. &quot;w/o Geo Edit&quot;는 가상 깊이 감독(즉, Lgeometry 없음)으로 지오메트리를 업데이트하지 않는 것을 의미합니다. (ga - oa) · (gb ga . Ob) (9)로 계산됩니다. 점수가 낮을수록 생성 방향이 다른 뷰에서 더 잘 정렬되어 장면 생성에서 더 큰 일관성을 나타냅니다. 성능을 평가하기 위해 검증 세트에서 총 80개의 메시를 선택하고 15개의 텍스트 프롬프트를 만듭니다. 각 메시에 대해 무작위로 4개의 다른 뷰를 선택하여 테스트합니다. 방향 일관성을 계산하기 위해 샘플링된 4개의 뷰와 나머지 1개의 뷰를 사용하여 4개의 원본 생성 쌍을 만듭니다. 따라서 평가 메트릭을 계산하는 데 사용되는 총 4800개의 이미지-텍스트 쌍과 원본 생성 쌍을 얻습니다. 먼저 표 1에서 SDS와 InstructN2N을 사용한 방법을 비교합니다. 이 접근 방식은 두 가지 모두보다 성능이 뛰어납니다. 더 높은 유사도 및 일관성 점수를 가진 SDS와 InstructN2N. SDS의 유사도 점수는 비교적 낮은 것으로 보이는데, 이는 흐림 효과 때문일 수 있습니다. SDS의 높은 방향 일관성 점수는 일관된 흐림 효과를 반영합니다. InstructN2N은 순수 이미지 기반 편집 백본(InstructP2P)의 제한으로 인해 더 낮은 텍스트-이미지 유사도 점수를 얻습니다. 그러나 InstructN2N의 방향 일관성 점수는 양호하여 데이터 세트 업데이트 방식의 효과를 나타냅니다. 그런 다음 표 2에 표시된 대로 일부 절제 연구를 수행합니다. 아웃페인팅 모델은 방향 일관성이 좋지만 텍스트-이미지 유사도가 낮습니다. 거리 맵 블렌딩 방식을 제거하면 텍스트-이미지 유사도 점수에 부정적인 영향을 미칩니다. 또한 지오메트리 최적화를 제거하면 텍스트-이미지 유사도 점수에 눈에 띄게 부정적인 영향을 미칩니다. 이는 스캔된 메시를 사용자 지정하고 스타일을 지정하려는 경우 지오메트리를 최적화해야 함을 나타냅니다. 5.
--- CONCLUSION ---
이 논문에서는 스캔된 실내 메시 입력을 기반으로 텍스트 프롬프트에서 3D 실내 장면을 합성하는 문제를 다룹니다. 2D 확산 텍스트-이미지 생성 모델의 기능을 활용하는 솔루션을 제안합니다. 가장 큰 과제는 2D 생성 사전 확률에서 일관된 3D 지오메트리와 텍스처 정보를 생성하는 것입니다. 전체 장면의 일관된 시각적 모양을 보장하기 위해 먼저 지오메트리 가이드 3D 장면 텍스처 생성 기술을 개발합니다. 핵심 아이디어는 공간의 큐브맵을 생성하여 다양한 뷰에서 일관된 스타일을 얻는 것입니다. 그런 다음 단안 깊이 추정기로 추정된 가상 깊이를 기반으로 메시 지오메트리와 텍스처를 공동으로 최적화합니다. 주장된 기여는 실제 스캔된 메시에 대한 실험을 통해 검증됩니다. 감사의 말 이 작업의 품질을 향상시키는 데 귀중한 지원을 해준 Apple Inc.의 동료들에게 감사드립니다. 참고문헌 [1] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, Andrea Tagliasacchi. Cc3d: 구성적 3D 장면의 레이아웃 조건 생성. arXiv 사전 인쇄본 arXiv:2303.12074, 2023.[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, Tali Dekel. 다중 확산: 제어된 이미지 생성을 위한 확산 경로 융합. arXiv 사전 인쇄본 arXiv:2302.08113, 2023.[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al. Gaudi: 몰입형 3D 장면 생성을 위한 신경 아키텍트. 신경 정보 처리 시스템의 발전, 35:25102-25116, 2022.[4] Tim Brooks, Aleksander Holynski, Alexei A. Efros. Instructpix2pix: 이미지 편집 지침을 따르는 법 배우기. CVPR, 2023. 2, 7, 8,[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. 효율적인 기하학 인식 3D 생성적 적대 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16123-16133페이지, 2022.[6] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, ShaliniDe Mello, Tero Karras, Gordon Wetzstein. 3D 인식 확산 모델을 사용한 생성적 소설 뷰 합성. arXiv 사전 인쇄본 arXiv:2304.02602, 2023.[7] Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia. Fantasia3d: 고품질 텍스트-3D 콘텐츠 생성을 위한 기하학 및 모양 분리. arXiv 사전 인쇄본 arXiv:2303.13873, 2023.[8] Angela Dai, Christian Diller, Matthias Nießner. Sg-nn: rgb-d 스캔의 자체 감독 장면 완성을 위한 희소 생성 신경망. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 849-858페이지, 2020.[9] Afshin Dehghan, Gilad Baruch, Zhuoyuan Chen, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, Elad Shulman. Arkitscenes: 모바일 RGB-D 데이터를 사용한 3D 실내 장면 이해를 위한 다양한 실제 데이터 세트. Joaquin Vanschoren 및 Sai-Kit Yeung 편집, Neural Information Processing Systems Track on Datasets and Benchmarks 1의 회의록, NeurIPS Datasets and Benchmarks 2021, 2021년 12월, 가상, 2021. 1,[10] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerdi: 일반 이미지 사전으로 언어 유도 확산을 사용한 단일 뷰 nerf 합성. arXiv 사전 인쇄본 arXiv:2212.03267, 2022.[11] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W Taylor 및 Joshua M Susskind. 로컬 조건 광도 필드를 사용한 제약 없는 장면 생성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 14304-14313쪽, 2021.[12] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, Hao Zhang. 3d-front: 레이아웃과 의미론이 포함된 3d 가구가 비치된 객실. 2021 IEEE/CVF 국제 컴퓨터 비전 컨퍼런스, ICCV 2021, 캐나다 퀘벡주 몬트리올, 2021년 10월 10-17일, 10913-10922쪽. IEEE, 2021.[13] Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi. Nerfdiff: 3D 인식 확산에서 nerfguided distillation을 사용한 단일 이미지 뷰 합성. arXiv 사전 인쇄본 arXiv:2302.10109, 2023.[14] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, Angjoo Kanazawa. Instruct-nerf2nerf: 지침이 있는 3D 장면 편집. 2023. 2, 5, 7, 8,[15] Amir Hertz, Kfir Aberman, Daniel Cohen-Or. 델타 노이즈 제거 점수. arXiv 사전 인쇄본 arXiv:2304.07090, 2023.[16] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델 노이즈 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020.[17] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nieẞner. Text2room: 2D 텍스트-이미지 모델에서 텍스처가 있는 3D 메시 추출. arXiv 사전 인쇄본 arXiv:2303.11989, 2023.[18] Lukas Höllein, Justin Johnson, Matthias Nießner. Stylemesh: 실내 3D 장면 재구성을 위한 스타일 전송. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6198-6208페이지, 2022.[19] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, Timo Aila. 고성능 미분 가능 렌더링을 위한 모듈식 기본 요소. ACM 그래픽스 저널, 39(6), 2020. 3,4,[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. Magic3d: 고해상도 텍스트-3D 콘텐츠 생성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 2023.[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick. Zero-1-to-3: 제로 샷 한 이미지를 3D 객체로, 2023.[22] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi. Realfusion: 단일 이미지에서 모든 객체를 360°로 재구성. arXiv 전자 인쇄물, arXiv-2302 페이지, 2023.[23] Norman Müller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner. Diffrf: 렌더링 유도 3D 광도장 확산. arXiv 사전 인쇄본 arXiv:2212.01206, 2022.[24] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, Sanja Fidler. Atiss: 실내 장면 합성을 위한 자기 회귀 변환기. 신경 정보 처리 시스템의 발전, 34:12013-12026, 2021.[25] Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall. Dreamfusion: 2d 확산을 사용한 텍스트-3d. arXiv, 2022. 2, 5, 7, 8,[26] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 자연어 감독에서 이전 가능한 시각적 모델 학습. ICML, 2021.[27] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향하여: 제로 샷 교차 데이터 세트 전송을 위한 데이터 세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(3), 2022. 5,[28] Daniel Ritchie, Kai Wang 및 Yu-an Lin. 딥 합성곱 생성 모델을 통한 빠르고 유연한 실내 장면 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 61826190페이지, 2019.[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022.[30] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nieẞner 및 Angela Dai. Retrievalfuse: 데이터베이스를 사용한 신경 3D 장면 재구성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 12568-12577페이지, 2021.[31] Josef Sivic, Biliana Kaneva, Antonio Torralba, Shai Avidan, William T Freeman. 대규모 사실적인 가상 공간 만들기 및 탐색. 2008년 IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식 워크숍 컨퍼런스, 1-8페이지. IEEE, 2008.[32] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델 잡음 제거. 2021년 5월 3-7일 오스트리아에서 열린 제9회 학습 표현 국제 컨퍼런스, ICLR 2021, 가상 이벤트. OpenReview.net, 2021.[33] Yang Song, Stefano Ermon. 데이터 분포의 기울기를 추정하여 생성 모델링. 신경 정보 처리 시스템의 발전, 32, 2019. 2,[34] Jiaxiang Tang. Stable-dreamfusion: Stable-diffusion을 사용한 텍스트-3D 변환, 2022. https://github.com/ashawkey/stabledreamfusion.[35] Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X Chang, Manolis Savva. Plan2scene: 평면도를 3D 장면으로 변환. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10733-10742페이지, 2021.[36] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, Greg Shakhnarovich. 점수 야코비안 체이닝: 3D 생성을 위한 사전 학습된 2D 확산 모델 리프팅. arXiv 사전 인쇄본 arXiv:2212.00774, 2022.[37] Kai Wang, Manolis Savva, Angel X Chang, Daniel Ritchie. 실내 장면 합성을 위한 심층 합성 사전 확률. ACM Transactions on Graphics(TOG), 37(4):1–14, 2018.[38] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang. Neurallift-360: 야외에서 찍은 2D 사진을 360° 뷰가 있는 3D 객체로 들어올리기. 2022.[39] Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, Shuicheng Yan. 아웃페인팅을 통한 매우 긴 자연 풍경 이미지 예측. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 10561-10570페이지, 2019.[40] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023. 2,[41] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen 및 Ming yu Liu. Diffcollage: 확산 모델을 사용한 대용량 콘텐츠의 병렬 생성. CVPR에서, 2023.[42] Zhengyou Zhang. Microsoft Kinect 센서 및 그 효과. IEEE MultiMedia, 19(02):4-10, 2012.[43] Zhizhuo Zhou 및 Shubham Tulsiani. Sparsefusion: 3D 재구성을 위한 뷰 조건 확산 증류. CVPR에서, 2023.
