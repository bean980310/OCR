--- ABSTRACT ---
최근 INSTRUCTEVAL [Chia et al., 2023]이 출시되어 인코더/디코더 또는 디코더 전용 아키텍처를 활용하는 대규모 언어 모델(LLM)의 성능에 대한 귀중한 통찰력이 제공되었습니다. 흥미롭게도 4년 전에 도입되었음에도 불구하고 FLAN-T5와 같은 T5 기반 LLM은 일반적인 문제 해결 기술이 필요한 작업에서 LLAMA 및 VICUNA와 같은 최신 디코더 기반 LLM보다 성능이 계속 우수합니다. 이러한 성능 차이는 세 가지 핵심 요인에 기인할 수 있습니다. (1) 사전 학습 데이터, (2) 백본 아키텍처, (3) 명령어 데이터 세트. 이 기술 보고서에서 우리의 주요 초점은 ChatGPT 대화에서 미세 조정을 거친 LLAMA 기반 대규모 언어 모델인 VICUNA를 활용하여 세 번째 요인의 영향을 조사하는 것입니다. 이 목표를 달성하기 위해 FLAN-MINI라는 사용자 지정 명령어 데이터 세트 컬렉션을 사용하여 VICUNA를 미세 조정했습니다. 이 컬렉션에는 FLAN이라고 알려진 대규모 명령어 데이터 세트의 하위 집합과 ChatGPT/GPT-4에서 파생된 다양한 코드 관련 데이터 세트 및 대화형 데이터 세트가 포함됩니다. 이 데이터 세트는 Preprint. 검토 중입니다. 문제 해결 기술을 요구하는 많은 수의 작업으로 구성됩니다. 실험 결과에 따르면, 모델인 FLACUNA의 향상된 문제 해결 능력은 FLAN 데이터 세트에서 VICUNA를 미세 조정하여 얻을 수 있으며, INSTRUCTEVAL의 수많은 벤치마크 데이터 세트에서 상당한 개선이 이루어졌습니다. FLACUNA는 https://huggingface.co/declare-lab/flacuna-13b-v1.0에서 공개적으로 사용할 수 있습니다. 1
--- EXPERIMENT ---
모든 연구 결과에 따르면, 우리 모델인 FLACUNA의 향상된 문제 해결 능력은 FLAN 데이터 세트에서 VICUNA를 미세 조정하여 얻어졌으며, 이는 INSTRUCTEVAL의 수많은 벤치마크 데이터 세트에서 상당한 개선으로 이어졌습니다. FLACUNA는 https://huggingface.co/declare-lab/flacuna-13b-v1.0에서 공개적으로 사용할 수 있습니다. 1 서론 ChatGPT와 후속 모델인 GPT-4는 대부분의 벤치마킹 작업과 데이터 세트에서 이전 최첨단 모델을 능가했습니다. 그러나 개인 정보를 보호하기 위해 GPT-3와 같은 175B+ 크기의 모델을 기본적으로 실행하는 것은 개인은 물론 대부분의 조직의 역량을 넘어섭니다. 이로 인해 많은 연구자들이 ChatGPT 또는 GPT-4에서 생성된 다양한 지침 예제 세트에서 관리 가능한 크기의 LLM을 7B에서 30B로 미세 조정하게 되었습니다. 이로 인해 Alpaca [Taori et al., 2023] 및 VICUNA [Chiang et al., 2023]와 같은 LLM이 탄생했는데, 이는 LLAMA [Touvron et al., 2023]의 미세 조정된 체크포인트입니다. 이러한 모델은 일부 특정 벤치마킹 작업에서 ChatGPT 수준에 가까운 성능을 달성했지만 전반적인 일반화는 여전히 애매합니다. INSTRUCTEVAL [Chia et al., 2023]과 같은 최근 연구는 미세 조정 데이터 세트가 작업별 성능을 결정한다는 것을 강력하게 암시합니다. 예를 들어, FLAN Collection 명령어 데이터 세트에서 미세 조정된 T5 체크포인트인 FLAN-T5는 강력한 추론 및 문제 해결 기술이 필요한 작업에서 VICUNA 및 Alpaca보다 우수한 성능을 보이는 것으로 관찰되었습니다. 이는 우리가 FLAN-MINI 컬렉션 데이터 세트에서 VICUNA를 미세 조정하도록 자극했으며, INSTRUCTEVAL에서 추론 집약적 작업에 대한 개선을 예상했습니다[Chia et al., 2023]. 이를 위해 먼저 15M 크기의 FLAN 컬렉션 데이터 세트[Longpre et al., 2023]에서 1M 크기의 명령어 데이터 세트를 샘플링하고 코딩 작업과 ChatGPT/GPT-4 정제된 대화를 포함하는 여러 다른 데이터 세트와 결합했습니다. 그 결과 더 작은 데이터 세트인 FLAN-MINI는 VICUNA의 대화 형식으로 캐스팅됩니다. 미세 조정 프로세스에 대한 합리적인 계산 비용을 보장하기 위해 LoRA[Hu et al., 2021] 어댑터를 VICUNA의 LLaMA[Touvron et al., 2023] 디코더-변압기에 개조했습니다. FLAN-MINI에서 VICUNA 체크포인트의 매개변수 효율적인 LoRA 미세 조정에 따라 FLACUNA를 얻었습니다. 예상대로 FLACUNA는 대부분 벤치마크 데이터 세트에서 VICUNA보다 상당한 마진으로 성능이 뛰어나며, 특히 추론 집약적 작업의 경우에 그렇습니다. 그러나 FLACUNA의 성능은 동일한 추론 벤치마크에서 여전히 FLAN-T5보다 낮습니다. 이는 덜 다양한 샘플을 포함할 수 있는 명령어 데이터 세트의 15배 더 작은 데이터 세트 때문일 수 있습니다. 또한 VICUNA의 전체 미세 조정으로 FLAN-T5와의 격차가 줄어들 수 있습니다. 이 작업은 전반적으로 다음과 같은 기여를 합니다. 1. FLAN-MINI에서 매개변수 효율적 미세 조정을 통해 VICUNA의 문제 해결 능력 개선. 2. 다양한 작업과 템플릿 세트로 구성된 명령어 튜닝 데이터 세트 FLAN-MINI 소개. 교육 세부 정보 FLAN-MINI 컬렉션 준비. FLAN 컬렉션의 엄청난 크기[Longpre et al., 2023]를 감안할 때 전체 데이터 세트 크기를 줄이는 동시에 높은 수준의 작업 다양성을 유지하는 신중하게 선택된 하위 집합으로 작업하기로 했습니다. 표 1에서는 FLAN 하위 집합에 포함된 특정 작업과 해당 데이터 세트 크기를 제시합니다. FLAN 컬렉션의 공개 릴리스에는 프로그래밍 작업이 포함되지 않으므로 기존 코드 데이터 세트로 컬렉션을 확장합니다. 구체적으로 CodeContests [Li et al., 2022a], APPS [Hendrycks et al., 2021a] 및 CodeSearchNet [Husain et al., 2019a]을 포함합니다. FLAN 컬렉션의 데이터 처리 파이프라인에 따라 각 데이터 세트에서 고정된 수의 예를 샘플링하고 각 예는 다른 프롬프트 템플릿으로 무작위로 확장합니다. 구체적으로 예는 수작업으로 만든 프롬프트 템플릿 풀로 처리되며 제로 샷 예로 사용하거나 몇 샷 데모와 함께 그룹화할 수 있습니다 [Longpre et al., 2023]. VICUNA의 채팅 기능 유지. VICUNA는 놀라운 채팅 기능을 보여주었으며 ChatGPT 성능의 90%를 달성했습니다. 이는 ChatGPT와 같은 폐쇄형 소스 대규모 언어 모델(LLM)에 대한 오픈 소스 대안으로서 상당한 잠재력을 나타냅니다.데이터세트 이름 FlanPublic Pool of Prompts Natural instruction vCOT Code Search Code Contest Husain et al. [2019b] 소스 데이터세트 크기 FLAN 388K FLAN 320K FLAN 200K FLAN 100K 100K 50K 50K 52K 20K 60K 1.34MH 앱 GPT4-Alpaca Code-Alpaca ShareGPT 총계 Li et al. [2022b] Hendrycks et al. [2021b] GPT-ChatGPT ChatGPT 표 1: FLACUNA를 훈련하는 데 사용된 FLAN-MINI 컬렉션. FLACUNA가 VICUNA의 학습된 지식과 채팅 능력을 유지한다는 점을 고려하여 Alpaca [Taori et al., 2023], Code Alpaca [Chaudhary, 2023], ShareGPT [Chiang et al., 2023]를 포함한 다양한 ChatGPT 데이터 세트를 FLAN 컬렉션에 통합했습니다. 이 세 가지 데이터 세트 중 VICUNA는 원래 ShareGPT 데이터 세트를 사용하여 미세 조정되었습니다. 그런 다음 최종 컬렉션을 사용하여 FLACUNA를 훈련했습니다. 아키텍처. FLAN-MINI 컬렉션에서 미세 조정을 위해 VICUNA 모델에 LORA를 적용했습니다. 모든 쿼리 및 값 투영 계층에 저순위 어댑터를 삽입하여 총 훈련 가능한 매개변수 수가 6.55M이 되었는데, 이는 원래 13B VICUNA 모델의 매개변수 수의 약 0.05%에 불과합니다. 최대 입력 시퀀스 길이는 1280으로 설정되었고 bf16 정밀도를 활용하여 효율적인 훈련이 용이해졌습니다. 하이퍼파라미터 세부 정보. FLACUNA는 1에포크 동안 4×A6000 GPU에서 학습되었습니다. 장치당 배치 크기가 2인 그래디언트 축적 단계를 사용하여 총 배치 크기가 128이 되었습니다. 워밍업 단계는 3000개이고 학습 속도는 2e-5였습니다. 3 평가 과제 및 결과 3.1 문제 해결 평가 지시된 대규모 언어 모델(LLM)의 문제 해결 능력을 평가하기 위해 INSTRUCTEVAL은 다양한 주제를 탐구하는 실제 시험을 포함하는 다양한 벤치마크를 사용합니다. 이러한 벤치마크에는 복잡한 명령어, 산술 문제, 프로그래밍 과제 및 인과 추론 과제가 포함됩니다. 이러한 벤치마크에서 탁월한 성과를 거두려면 모델이 세상에 대한 심오한 이해를 보여주고, 멀티홉 추론 기능을 보여주고, 창의성을 보여주고, 수많은 다른 인지 기술을 사용해야 합니다. 세계 지식. Hendrycks 등의 연구에서 도입된 대규모 멀티태스크 언어 이해(MMLU) 벤치마크. [2021c]는 다양한 과목에 걸쳐 언어 모델의 문제 해결 적성과 세계 지식을 측정하는 평가 도구 역할을 합니다. 제로샷과 퓨샷 설정 모두에서 평가를 제공하여 더욱 어렵고 인간과 유사한 평가 시나리오를 제시합니다. MMLU 벤치마크는 STEM, 인문학, 사회 과학 및 기타 도메인에 걸쳐 57개 과목의 포괄적 범위를 포함합니다. 벤치마크 내 작업의 난이도는 초급에서 고급 전문가 수준까지 다양하여 문제 해결 및 도메인 이해에서 모델의 역량을 포괄적으로 평가합니다. 복잡한 지침. BIG-Bench Hard(BBH)로 알려진 하위 집합은 BIG-Bench 벤치마크에서 신중하게 선택한 23개의 매우 까다로운 작업으로 구성되어 있습니다. [Srivastava et al., 2022] 언어 모델의 현재 역량을 능가하는 것으로 간주되는 작업을 특별히 타겟팅합니다. [Suzgun et al., 2022]. BBH는 탐색, 논리적 추론 및 오류 감지에 대한 고급 기술이 필요한 복잡한 지침을 모델에 제시합니다. 이해 및 산술. Discrete Reasoning Over Paragraphs(DROP)는 수학적 초점을 맞춘 독해 과제입니다. 이 과제는 시스템이 위키피디아 문서에서 추출한 구절을 분석하여 이산 추론에 참여하도록 합니다. DROP 과제에서 탁월하려면 시스템이 질문 내의 참조를 능숙하게 탐색하고 제공된 구절의 적절한 섹션을 식별해야 합니다. 또한 시스템은 덧셈, 계산 또는 정렬과 같은 이산 연산을 수행하는 데 능숙해야 합니다. 프로그래밍. HumanEval은 코드에서 학습된 대규모 언어 모델의 성능을 평가하기 위해 특별히 설계된 문제 해결 벤치마크 역할을 합니다[Chen et al., 2021]. 이 벤치마크는 언어 이해, 알고리즘 및 기본 수학과 같은 분야를 포함하는 164개의 고유한 프로그래밍 문제로 구성되어 있습니다. HumanEval에 포함된 문제 중 일부는 간단한 소프트웨어 면접 질문과 본질적으로 유사합니다. 평가 프로세스에서 모델은 생성된 코드 프로그램의 기능적 정확성을 기반으로 평가되며 정확성 기준은 제공된 docstring에 의해 결정됩니다. HumanEval은 코드 중심 맥락에서 언어 모델의 문제 해결 능력을 평가하기 위한 포괄적인 평가 프레임워크를 제공합니다. 인과성. 반사실적 추론 평가(CRASS) 벤치마크는 대규모 언어 모델의 인과 추론 능력을 평가하기 위해 특별히 개발된 새로운 데이터 세트 및 평가 도구입니다. 반사실적 시나리오를 사용하여 CRASS는 모델의 적절한 인과 설명을 식별하고 선택하는 능력을 테스트합니다. 이 벤치마크는 언어 모델의 인과 추론 능력을 측정하기 위한 독특하고 엄격한 평가 프레임워크를 제공합니다. 3.2 인간적 가치와의 일치 LLM을 인간적 가치에 맞추는 것의 중요성을 인식하고 INSTRUCTEVAL은 Helpful, Honest, and Harmless(HHH) 벤치마크를 통합합니다[Askell et al., 2021]. 이 벤치마크는 인간과 대화형 도우미 간의 매력적인 대화를 보여주며 모델이 가장 적절한 응답을 식별하고 제공하도록 도전합니다. 여기에는 정직 관련, 도움 관련, 무해성 관련 샘플 61개와 &quot;기타&quot; 범주에 속하는 고유한 인스턴스 43개가 포함됩니다. &quot;기타&quot; 범주를 포함하는 것은 정직, 도움 또는 무해성에 의해 명시적으로 다루어지지 않는 가치를 구현하는 예를 설명합니다. 3.3 쓰기 실험 쓰기 실험의 경우 INSTRUCTEVAL에서 쉽게 사용할 수 있는 IMPACT 데이터 세트를 활용했습니다. 이 포괄적인 데이터 세트는 정보적, 전문적, 논증적, 창의적이라는 뚜렷한 범주에 걸친 50개의 프롬프트로 구성되어 있습니다. 그 후 ChatGPT는 관련성(Rel.)과 일관성(Coh.) 측면에서 모델의 응답을 1~5점 척도로 평가하는 책임을 맡았습니다. 이 평가에 대한 보다 포괄적인 정보는 Chia et al. [2023]을 참조하세요. 3.4 결과 비교 기준선. 기준선으로 VICUNA[Zheng et al., 2023]와 STABLEVICUNA¹를 선택했습니다.소규모 문제 해결.문제 해결 작업에 초점을 맞춰 INSTRUCTEVAL 벤치마크의 5개 데이터 세트(표 2 참조)에 대한 FLACUNA의 결과를 제시합니다.5개 작업 중 4개에서 FLACUNA는 VICUNA보다 성능이 우수하여 LLAMA 백본보다 평균 5.6포인트의 성능 향상을 보였습니다.그러나 HumanEval 데이터 세트의 코드 관련 문제 해결 작업에서는 0.6포인트의 차이로 약간 더 나쁜 성능을 보였습니다.전반적으로 VICUNA와 비교한 FLACUNA의 향상도는 5개 작업에 대한 평균 5.1포인트입니다.5개 문제 해결 데이터 세트 중 하나인 DROP은 보류 데이터 세트로 분류됩니다.이것은 FLAN 컬렉션의 일부이며 FLACUNA를 훈련하는 데 활용되었습니다.그 결과 VICUNA와 비교하여 11포인트의 상당한 성능 향상을 관찰했습니다. 나머지 데이터 세트는 보류된 것으로 간주됩니다.¹https://huggingface.co/CarperAI/stable-vicuna-13b-deltaMMLU(5샷) BBH(3샷) DROP(3샷) CRASS(3샷) 모델 HumanEval(0샷) 크기 평균 성능 Δ 성능 A 성능 Δ 성능 Δ 성능 Δ 성능 Δ GPT-86.80.67.ChatGPT 70.49.64.90.48.64.Flan-UL20B 55.44.64.94.0.51.Alpaca-Lora 30B 58.+0.41.+2.0 45.-0.79.+10.18.+4.48.6 +3.OpenAssistant 30B 56.-0.39.-0.1 46.+0.67.+1.23.+9.46.5 +1.OPT-IML 30B 38.+11.31.+3.0 47.+28.0 67.+32.9.+7.38.7 +16.Flan-T11B 54.+29.43.+13.6 67.+49.88.+54.0.+0.50.8 +29.플랜-알파카 11B 50.+25.돌리 V12B 25.-1.23.3 -7.0 62.29.7 +0.2 16.+44.90.+56.0.+0.45.3 +24.-0.35.+1.8.-0.23.2 -0.플랜-T3B 49.+25.채트GLM 모자이크-채트 안정된 비쿠나 6B 36.7B 37.13B 49.+1.40.2 +15.9 56.31.32.0 +1.1 20.+43.91.+60.0.+0.47.4 +29.44.51.3.33.-7.47.+13.17.+7.30.9 +3.VICUNA 13B 50.FLACUNA 13B 51.+3.+4.+5.37.5 +0.34.-1.67.+8.15.+2.40.9 +2.37.6 +0.5 32.39.3 +2.2 43.-3.60.+2.11.-1.38.7 +0.+8.74.+15.11.-2.43.8 +5.표 2: 문제 해결 벤치마크에 대한 평가 결과. 벤치마크 전체의 원래 성과를 Perf.로 표시하고, A는 해당 기초 LLM과 비교한 성과의 변화를 나타냅니다. ⋆는 DROP이 보유된 데이터 세트임을 나타냅니다. 모델 크기 MMLU(0-shot) BBH(0-shot) CRASS(0-shot) Flan-UL20B 54.34.OpenAssistant 30B 52.33.OPT IML 30B 41.17.TK-Instruct 11B 39.17.Flan-T5-XXL 11B 54.39.Dolly V12B 25.22.STABLE VICUNA 13B 47.18.64.VICUNA 13B 48.28.65.FLACUNA 13B 49.32.67.표 3: FLACUNA 및 기타 기준 모델의 0-shot 문제 해결 평가. 0-shot 문제 해결. FLACUNA의 0-shot 성능 평가를 수행하여 VICUNA 및 StableVicuNA와 비교했습니다. 표 3에 제시된 결과는 경쟁 제품에 비해 FLACUNA의 주목할 만한 성능 도약을 보여줍니다. 이러한 개선은 고품질 FLAN 명령어 데이터 세트에서 FLACUNA를 훈련한 데 기인할 수 있습니다. HHH 평가. BBH의 HHH 평가 데이터 세트(표 4 참조)를 사용하여 추가 평가를 수행한 결과, FLACUNA는 VICUNA에 비해 인상적인 11%의 개선을 보였습니다. 특히 명령어 데이터 세트 컬렉션은 VICUNA의 문제 해결 능력을 향상시키는 것을 목표로 했지만 HHH 성능에도 긍정적인 영향을 미쳤습니다. 이러한 관찰은 FLAN에서 미세 조정한 후 T5 백본에 비해 24.2%의 성능 개선을 달성한 FLAN-T5의 경험과 일치합니다. 쓰기 평가. FLACUNA는 주로 문제 해결 작업에서 뛰어나지만, VICUNA의 인상적인 쓰기 및 채팅 능력을 유지하기 위해 노력했습니다. 이를 달성하기 위해 GPT-4-Alpaca 및 ShareGPT와 같은 GPT-4에서 생성한 대화형 데이터 세트를 FLANMINI 컬렉션에 통합했습니다. 그러나 이러한 노력에도 불구하고 FLACUNA의 쓰기 성능에서 특정 문제를 발견했습니다. 어떤 경우에는 코딩과 관련 없는 프롬프트에 응답하여 코드 스니펫을 생성합니다. 이러한 동작은 대화형 데이터 세트가 전체 데이터 혼합물의 8.2%에 불과한 상당한 데이터 불균형에 기인합니다. 프롬프트 엔지니어링 기술은 이러한 문제를 해결하는 데 도움이 될 수 있습니다. 다음 템플릿이 제공되면 FLACUNA가 적절한 품질의 응답을 생성한다는 것을 발견했습니다. &quot;호기심 많은 사용자와 인공 지능 도우미 간의 채팅. 도우미는 사용자의 질문에 도움이 되고 자세하며 정중한 답변을 제공합니다. 사용자: 작업 정의./n/n 질문/n 출력: 모델 크기 무해성 도움성 정직성 기타 평균 Δ Ανg. ChatGPT 90.91.78.86.3 86.플랜-알파카 11B 74.81.77.83.79.1 +26.플랜-T11B 75.75.75.79.6 76.7 +24.Tk-인스트럭트 11B 70.54.62.T11B 46.54.58.50.76.0 65.52.+13.알파카 13B 49.51.51.45.5 49.5 -12.LLAMA 13B 57.61.57.72.61.돌리 V12B 51.59.47.58.54.+9.피티아 12B 41.46.43.49.3 45.안정 비쿠나 13B 61.67.57.79.66.3 +4.비쿠나 13B 62.66.52.플라쿠나 13B 72.71.70.74.4 63.7 +1.83.7 74.5 +12.표 4: 정직, 도움, 무해함(HHH) 벤치마크에서 인간적 가치에 대한 정렬에 대한 평가 결과. Avg.는 평균 성과를 나타내고 A Avg.는 해당 기초 모델과 비교한 평균 개선을 나타냅니다. 정보적 전문가 모델 크기 ChatGPT Flan-Alpaca Flan-TDolly-VSTABLE 비쿠나 비쿠나 플라쿠나 상대적 Coh. 상대적 Coh. 3.3.3.88 3.11B 3.56 3.46 3.54 3.70 3.11B 2.64 3.24 2.62 3.12B 3.54 3.13B 3.54 3.13B 3.60 3.13B 3.02 3. 상대적 논증적 창의성 평균 상대적 상대적 상대적 3.3.3.92 3.94 3.78 3.3.3.70 3.3.51 3.2.3.2.50 2.2.58 3.2.96 3.3.3.3.02 3.3.30 3.2.96 3.3.3.3.74 3.82 3.3.48 3.52 3.3.3.3.02 3.3.82 3.92 3.75 3.3.92 3.80 3.45 3.3.21 3.표 5: 쓰기 기반 과제에 대한 평가 결과. 보조: &quot;. 이 발견은 적절한 프롬프트를 사용하면 FLACUNA의 채팅 성능을 개선할 수 있음을 시사합니다. 그러나 생성된 샘플을 주의 깊게 검토하면 FLACUNA가 쓰기 보조 도구로서 VICUNA보다 성능이 뛰어나지 않다는 것이 분명해집니다. 이 관찰은 표 5에 나와 있듯이 ChatGPT를 사용하여 IMPACT 데이터 세트의 프롬프트에 대한 생성된 응답을 평가하여 강화됩니다. ChatGPT는 지속적으로 VICUNA의 응답을 FLACUNA의 응답보다 상당히 높게 평가합니다. 쓰기 기반 시나리오에서 FLACUNA의 열악한 성능은 여러 요인에 기인할 수 있습니다. 첫째, FLAN에서 대화 데이터 세트의 불균형적인 부족이 이러한 결과에 기여했을 수 있습니다. 또한 LORA와 같은 매개변수 효율적 튜닝 방법은 문제 해결 및 일반적인 쓰기 능력을 모두 학습하는 데 있어 모델의 효과를 제한할 수 있습니다. 따라서 앞으로 LLM을 위한 다른 효율적인 교육 방법을 모색할 수 있습니다[Lv et al., 2023]. 프롬프트와 FLACUNA의 응답의 예가 아래에 나와 있습니다. 프롬프트: 호기심 많은 사용자와 인공지능 도우미 간의 채팅. 도우미는 사용자의 질문에 유익하고 자세하며 정중한 답변을 제공합니다. 사용자: 다음 질문에 대한 전문적 또는 업무적 환경에서 쓰기 능력을 보여주는 과제가 주어집니다. 명확하고 간단한 언어를 사용하여 가전제품의 사용 설명서를 작성하고 작동, 유지 관리 및 문제 해결에 대한 단계별 지침을 제공하세요. 출력: 도우미: FLACUNA의 답변: 1. 소개 XYZ 가전제품의 사용 설명서에 오신 것을 환영합니다. 이 가전제품은 여러분의 삶을 더 쉽고 편리하게 만들기 위해 설계되었습니다. 이 가전제품을 사용하면 빠르고 효율적으로 식사를 준비할 수 있습니다. 2. 작동 기기를 작동하려면 다음 단계를 따르세요. a. 전원 버튼을 눌러 기기를 켭니다. b. 모드 선택기를 사용하여 원하는 조리 모드를 선택합니다. c. 식품을 기기 안에 넣습니다. d. 뚜껑을 닫고 시작 버튼을 누릅니다. e. 기기가 조리 과정을 완료할 때까지 기다립니다. 3. 유지 관리 기기를 양호한 작동 상태로 유지하려면 다음 단계를 따르세요. a. 사용할 때마다 기기를 청소합니다. b. 영어: 기기에 손상이나 마모 흔적이 있는지 확인하십시오.c. 문제가 발견되면 제조업체나 인증된 수리 기술자에게 문의하십시오.4. 문제 해결 기기에 문제가 발생하면 다음 문제 해결 단계를 시도하십시오.a. 전원 코드와 플러그를 확인하여 단단히 연결되었는지 확인하십시오.b. 퓨즈가 끊어지지 않았는지 확인하십시오.c. 기기 설명서에서 문제 해결 팁을 확인하십시오.d. 문제가 지속되면 제조업체나 인증된 수리 기술자에게 문의하십시오.5. 안전 예방 조치 기기를 사용할 때는 항상 다음 안전 예방 조치를 따르십시오.a. 기기를 깨끗하고 이물질이 없도록 유지하십시오.b. 기기가 손상되었거나 마모 흔적이 있는 경우 사용하지 마십시오.c. 작동 중에는 기기를 방치하지 마십시오.d. 항상 제조업체의 지침에 따라 기기를 사용하십시오.6.
--- CONCLUSION ---
새로운 XYZ 가전제품을 축하드립니다!적절하게 사용하고 유지관리하면 이 가전제품은 수년간 안정적으로 사용할 수 있습니다.궁금한 사항이나 우려 사항이 있으면 가전제품 설명서를 참조하거나 제조업체에 문의하세요.4 한계와 향후 과제 FLACUNA가 VICUNA에 비해 발전된 점이 유망하지만, 해결해야 할 몇 가지 문제점이 발견되었습니다.• FLACUNA에 &quot;음주 연령 제한을 낮추는 것에 대한 찬성 또는 반대 주장을 제시하세요&quot;와 같은 질문에 대한 설명적 답변을 제공하도록 요청하면 FLACUNA는 대신 코드 조각을 생성합니다.이 동작은 명령어에 대한 불완전한 이해나 환각 경향 때문일 수 있습니다.• • FLACUNA는 여전히 문제 해결 능력 면에서 FLAN-T5보다 상당히 뒤처져 있습니다.놀랍게도 FLACUNA는 코딩 관련 문제에서 LLAMA와 VICUNA보다 성능이 떨어집니다.이 결과는 우리가 명령어 튜닝 컬렉션에 수많은 코딩 문제 해결 데이터 세트를 통합했다는 점을 고려하면 예상치 못한 일입니다. • FLACUNA는 최대 1280의 입력 시퀀스 길이로 훈련되어 더 긴 입력 시퀀스를 이해하는 능력이 제한됩니다. 이러한 제한 사항과 알려진 문제를 해결하기 위해 다음 단계를 살펴볼 수 있습니다. • 이전 연구에 따르면 LoRA는 30B 또는 65B 매개변수가 있는 것과 같은 더 큰 모델에서 더 나은 성능을 발휘하고 작업별 설정에서 뛰어난 성능을 발휘하는 것으로 나타났습니다[Chia et al., 2023]. 따라서 향후 작업에서는 특히 FLAN 컬렉션에서 LORA 없이 VICUNA를 완전히 미세 조정하여 FLACUNA를 향상시킬 수 있습니다. 또 다른 향후 작업은 더 긴 토큰 길이에서 FLACUNA를 훈련하는 것입니다. • 이 연구에서 사용한 명령어 데이터 세트보다 15배 더 큰 원래 FLAN 컬렉션을 훈련 프로세스에 통합할 수 있습니다. FLAN-T5는 이 광범위한 컬렉션에서 훈련을 거쳤으며 그 결과 놀라운 문제 해결 성능이 나타났습니다. • FLACUNA의 채팅 또는 쓰기 성능은 FLAN-MINI에 더 큰 대화 데이터 세트를 통합한 다음 이를 기반으로 FLACUNA를 훈련함으로써 개선될 수 있습니다. 참고문헌 Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models, 2023. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/stanford_alpaca. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇, 2023년 3월. URL https://vicuna.lmsys.org. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델. ArXiv, abs/2302.13971, 2023. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv 사전 인쇄본 arXiv:2301.13688, 2023. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d&#39;Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals. AlphaCode를 사용한 경쟁 수준 코드 생성. Science, 378(6624):1092–1097, 2022년 12월 a. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126%2Fscience.abq1158. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, Jacob Steinhardt. 앱을 통한 코딩 챌린지 역량 측정. ArXiv, abs/2105.09938, 2021a. Hamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt. Codesearchnet 챌린지: 의미 코드 검색 상태 평가. Arxiv, abs/1909.09436, 2019a. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt. CodeSearchNet 챌린지: 의미론적 코드 검색의 상태 평가. arXiv 사전 인쇄본 arXiv:1909.09436, 2019b. 유지아 리, 데이비드 최, 준영 정, 네이트 쿠쉬먼, 줄리안 슈리트비저, 레미 르블론드, 톰 에클스, 제임스 킬링, 펠릭스 지메노, 아구스틴 달 라고, 토마스 휴버트, 피터 초이, 사이프리앙 드 마송 도뮨, 이고르 바부슈킨, 신윤 첸, 포센 황, 요하네스 웰블, 스벤 고왈, 알렉세이 체레파노프, 제임스 몰로이, 다니엘 맨코비츠, 에스메 서덜랜드 롭슨, 푸시밋 콜리, 난도 드 프레이타스, 코레이 카부크추오글루, 오리올 비냔알스. 알파코드를 사용한 경쟁 수준 코드 생성. arXiv 사전 인쇄본 arXiv:2203.07814, 2022b. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt. 앱을 통한 코딩 과제 역량 측정. NeurIPS, 2021b. Sahil Chaudhary. 코드 알파카: 코드 생성을 위한 지시 따르기 라마 모델. https://github.com/sahil280114/codealpaca, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 대규모 멀티태스크 언어 이해 측정. International Conference on Learning Representations, 2021c. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh 부옹, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris 칼리슨-버치, 크리스 웨이츠, 크리스찬 보이트, 크리스토퍼 D. 매닝, 크리스토퍼 포츠, 신디 라미레즈, 클라라 E. 리베라, 클레멘시아 시로, 콜린 라펠, 코트니 애쉬크래프트, 크리스티나 가르바시아, 데미안 실레오, 댄 가렛, 댄 헨드릭스, 댄 킬먼, 댄 로스, 다니엘 프리먼, 다니엘 카샤비, 다니엘 레비, 다니엘 모세기 곤잘레스, 다니엘 페르시크, 대니 Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, 이동호, 예카테리나 슈토바, 에킨 도구스 큐북, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando 마르티네즈-플루메드, 프란체스카 하페, 프랑수아 숄레, 프리다 롱, 가우라브 미슈라, 젠타 인드라 위나타, 제라드 드 멜로, 게르만 크루제프스키, 지암바티스타 파라스칸돌로, 조르지오 마리아니, 글로리아 왕, 곤잘로 자이모비치-로페즈, 그레고르 베츠, 가이 구르아리, 하나 갈리자세비치, 한나 김, 한나 라슈킨, 한나네 하지쉬르지, 하쉬 Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, 이재훈, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jörg Frohberg, Jos 로젠, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, 마르텐 보스마, 마르텐 삽, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Chi, 이나연, Neta 구르아리 크라코버, 니콜라스 카메론, 니콜라스 로버츠, 닉 도이론, 니키타낭지아, 니클라스 데커스, 니클라스 무엔니호프, 니티시 쉬리쉬 케스카, 니베디타 S. 아이어, 노아 콘스탄트, 노아 피델, 누안 웬, 올리버 장, 오마르 아가, 오마르 엘바그다디, 오머 레비, 오웨인 에반스, 파블로 안토니오 모레노 카사레스, 파르스 도시, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, 래퍼 가브리엘, 라헬 하바커, Ramón Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, 한상현, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, 세피데 사데기, 샤디 함단, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, 이수환, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo 데스보르드, 테오도르 로스차일드, 토마스 판, 티안레 왕, 티베리우스 은키닐리, 티모 쉬크, 티모페이 코르네프, 티모시 텔린-로튼, 티투스 툰두니, 토비아스 게르스텐버그, 트렌턴 장, 트리샬라 니라즈, 투샤르 코트, 타일러 슐츠, 유리 샤함, 베단트 미스라, 베라 뎀버그, 빅토리아 냐마이, 비카스 라우낙, 비나이 라마세시, 비나이 우데이 Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, 최예진, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan 벨린코프, 유후, 유팡 허우, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu. 모방 게임을 넘어서: 언어 모델의 역량 정량화 및 외삽, 2022. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, Jason Wei. 빅벤치 과제에 도전하고 사고의 사슬로 해결할 수 있는지 여부. ArXiv, abs/2210.09261, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 코드로 학습된 대규모 언어 모델 평가. Arxiv, abs/2107.03374, 2021. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan. 정렬을 위한 실험실로서의 일반 언어 보조원, 2021. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. mt-bench 및 챗봇 경기장을 통해 Ilm-as-a-judge 심사, 2023. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo 및 Xipeng Qiu. 리소스가 제한된 대규모 언어 모델을 위한 전체 매개변수 미세 조정(2023년)
