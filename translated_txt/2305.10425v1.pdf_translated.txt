--- ABSTRACT ---
인간 피드백으로부터 학습하는 것은 언어 모델을 인간 선호도에 맞추는 데 효과적인 것으로 나타났습니다. 이전 연구에서는 종종 인간 선호도 데이터로 훈련된 보상 모델에서 할당된 보상 점수를 사용하여 언어 모델을 최적화하는 인간 피드백으로부터의 강화 학습(RLHF)에 의존했습니다. 이 연구에서는 최근에 도입된 시퀀스 가능성 보정(SLIC)을 사용하여 인간 선호도(SLIC-HF)로부터 효과적으로 학습하는 방법을 보여줍니다. 또한 오프 정책 오프라인 RL 데이터와 유사하게 다른 모델에 대해 수집된 인간 피드백 데이터로 이를 수행할 수 있음을 보여줍니다. TL;DR 요약 작업에 대한 자동 및 인간 평가 실험은 SLIC-HF가 감독 미세 조정(SFT) 기준선을 크게 개선한다는 것을 보여줍니다. 또한 SLiC-HF는 이전 연구에서 사용된 PPO RLHF 구현에 비해 경쟁력 있는 대안을 제시하는 동시에 구현이 훨씬 간단하고 조정이 쉽고 실제로 계산 효율성이 더 높습니다. 1
--- INTRODUCTION ---
대규모 확장 모델 매개변수와 Transformer 기반 언어 모델의 학습 컴퓨팅은 인상적인 few-shot in-context 학습[5, 6]으로 이어졌지만, 인간 피드백 미세 조정(RLHF)을 통한 강화 학습은 인간이 판단한 대로 생성 품질을 크게 개선할 수 있습니다. 이는 추상적 요약, 대화, 창의적 글쓰기와 같은 다양한 다운스트림 언어 생성 작업에 대한 모든 모델 규모에서 관찰되었습니다[18, 3, 7, 13]. 특히 요약의 경우, 여러 연구에 따르면 RLHF로 조정된 모델에서 생성된 요약이 일반적으로 사용되는 데이터 세트의 참조 요약보다 선호됩니다[18, 10, 8]. 참조 요약은 종종 웹 문서에서 마이닝되며 가장 높은 품질이나 선호되는 스타일이 아닐 수 있습니다. 결과적으로 순수한 지도 학습, 즉 문서가 주어진 참조 요약의 가능성을 최대화하는 것은 참조 요약의 품질에 의해 제한되므로 추가 피드백을 통해 참조를 넘어 모델을 개선할 수 있습니다. ROUGE[11]와 같이 일반적으로 사용되는 참조 기반 메트릭은 모델에서 생성된 텍스트와 참조 텍스트 간의 유사성만 측정합니다. 이러한 참조 기반 메트릭은 참조 요약을 넘어서는 품질 개선을 측정할 수 없습니다. RLHF를 구현하기 위해 보상 모델 rø(x, y)는 인간 선호도 데이터(x, yo, y₁, i)DHF에 대해 학습합니다. 이는 나란히 배치된 인간 평가를 통해 수집되며, 평가자는 두 요약 yo와 y₁ 중 어느 것이 문서 x에 더 나은지 판단하도록 요청받습니다. 즉, i = {0, 1}. 선호하는 요약을 y, 다른 요약을 y¯로 표시하면 인간 피드백은 (x, y+, ,y)가 됩니다. DHF. 일반적인 옵션 중 하나 사전 인쇄. 검토 중. ~~RLHF에서 사용하는 보상 모델의 학습 손실은 다음과 같습니다. = = (1) 손실 (ro) -E(x,y+,y)~DHF [log(σ (rø(x, y±) — rø(x, y¯¯))] PPO [17]와 같은 강화 학습 알고리즘을 사용하여 보상 모델 rø(x, y) [22, 18]에서 지정한 예상 보상을 최대화하도록 지도 학습 미세 조정 모델(SFT)을 개선합니다. RLHF 모델이 원래 지도 학습 정책에서 너무 멀리 벗어나지 않도록 일반적으로 손실에 KL-페널티 항을 추가합니다. 그러나 RLHF-PPO와 같은 알고리즘은 정책 네트워크와 크기가 비슷할 수 있는 별도의 가치 및 보상 네트워크를 추가하여 학습 프로세스에 상당한 복잡성을 도입합니다. 이러한 네트워크는 일반적으로 메모리에 보관되어 학습 속도를 극대화하며, 주어진 메모리 예산에 대해 학습 가능한 모델의 최대 크기를 크게 줄입니다. 또한 롤아웃을 사용하기 때문에 최적화 단계가 상당히 느립니다. 모델에서 샘플링/디코딩을 포함하는 학습 루프. 하이퍼 매개변수 튜닝 및 PPO 프로세스의 조정도 더 복잡하여 틈새 전문 지식이 필요합니다. 최근, 시퀀스 수준 대조 방법의 또 다른 종류[12, 21]는 모델 우도를 임의의, 아마도 미분 불가능한 보상과 일치시켜 샘플의 예상 보상을 최적화하기 위한 RL에 대한 대안을 제시합니다. Zhao et al.[21]은 참조 시퀀스와의 유사성에 따라 디코딩된 시퀀스에 대한 언어 모델의 시퀀스 우도, po(y|x)를 정렬하기 위해 시퀀스 우도 보정(SLiC)을 제안했습니다. 순위 교정 손실은 양의 시퀀스 y+와 음의 시퀀스 y¯를 대조하여 모델 Po가 음의 시퀀스에 비해 양의 시퀀스에 더 많은 확률 질량을 할당하도록 합니다.Lcal(0) = max(0, ẞ – log P₁(y+|x) + log Pe(y¯¯|x)) (2) 원래 SLiC 작업에서는 순위를 매기는 기준으로 참조와의 유사성을 사용했지만(예: ROUGE[11] 및 모델 임베딩 거리) 임의의 참조 없는 순위 함수 R(yo, y1,x) {0, 1}로 대체할 수 있습니다.특히 이 작업에서는 정책 외 선호도 데이터 D를 직접 사용하거나 D에서 예측 순위 모델 Ro(yo, y1, x)를 학습하여 인간의 선호도를 순위 함수로 사용합니다.이 인간 선호도 순위 함수로 SLiC를 사용하는 것을 SLiC-HF라고 하며 Stiennon et al.[18]에서 수집한 인간 피드백 데이터를 사용하여 적용합니다. 우리의 실험은 SLiC-HF가 인간이 판단할 때 Reddit TL;DR 요약 작업에서 요약 품질을 개선하는 데 도움이 된다는 것을 보여줍니다. 이 피드백은 오프 정책 오프라인 RL과 유사하게 다른 모델에 대해 수집되었습니다. 우리의 T5-Large[15](770M 매개변수) SFT 모델은 Stiennon et al.[18]의 6B 디코더 전용 SFT 모델과 유사한 성능을 보이지만, 우리는 SLiC-HF로 모델을 개선하여 인간이 판단할 때 최소한 Stiennon et al.[18]의 6B RLHF-PPO 모델만큼 좋은 성능을 낼 수 있습니다. 더욱이 SLIC-HF를 T5-XXL 11B 매개변수 SFT 모델[15]에 적용하면 결과가 상당히 개선됩니다. 이 논문의 주요 기여는 다음과 같습니다.• RLHF에 대한 더 간단하고 효율적이며 경쟁력 있는 대안인 인간의 선호도에서 학습하기 위해 SLiC를 적용하는 방법(SLiC-HF)• 다른 모델(정책 외)의 피드백/선호도 데이터는 SLIC-HF에서 효과적으로 활용할 수 있으므로 오픈 소스 T5 모델에 기반한 일반적인 SLiC-HF 레시피를 제공하는 모델에 대한 비용이 많이 드는 새로운 피드백 데이터를 수집할 필요가 없으며 Reddit TL;DR 요약 작업에서 RLHF보다 성능이 우수합니다.방법 이 작업에서 우리는 SLiC[21]를 적용하여 표준 감독 미세 조정 데이터(X, Yref) ~ DSFT 외에도 인간의 선호도 데이터(x, y+, y¯) ~ Dƒà를 사용하여 SFT 모델을 개선합니다.2.1 Zhao et al.에 따른 시퀀스 우도 교정 [21], 우리는 먼저 (X, Yref) ~ DSFT에서 감독 모델 Poft(y|x)를 미세 조정한 다음, 다음 손실을 최적화하는 SLiC 접근 방식을 사용하여 SFT 모델의 시퀀스 우도를 맞춥니다. Greg L(0) = Lcal(0, X, Yref, {ŷ}m) + \Ľ™eg(0,0ƒt; X, Yref) (3) 여기서 0 및 0ft는 현재 및 고정 SFT 모델 가중치이고, Lcal 및 Leg는 교정 및 정규화 손실이며, {ŷ}m은 SFT 모델에서 샘플링된 m개의 후보입니다. 보다 구체적으로, 우리는 단순성과 쌍별 인간 피드백 데이터에 대한 자연스러운 적합성을 위해 순위 교정 손실과 교차 엔트로피 정규화 손실을 선택합니다. 따라서 SLiC-HF의 손실 함수는 다음과 같습니다.L(0) = max(0, 8 — log P₁(y+|x) + log P₁(y¯¯|x)) — \ log Pe(yref|×) _ (4) 첫 번째 항은 교정 손실로, 여기서 x는 입력 시퀀스이고, y+와 y¯는 양수와 음수 시퀀스이며, §는 순위 손실의 마진에 대한 하이퍼 매개변수입니다.두 번째 항은 교차 엔트로피 손실로, 여기서 yref는 일부 대상 시퀀스이고 \는 정규화 가중치입니다.교차 엔트로피 손실은 Stiennon et al. [18]에서 사용된 KL 항과 유사하게 모델이 SFT 모델에 가깝게 유지되도록 장려하지만 SFT 가중치의 추가 사본이 필요하지 않습니다.KL 정규화 항은 Zhao et al. [21]에서도 탐구되었지만 비슷한 성능을 보이는 것으로 나타났습니다.y 및 y의 선택은 하위 섹션 2.2 및 2.3에서 설명합니다. 정규화 대상 yref의 선택은 하위 섹션 2.4에서 논의됩니다.2.2 샘플 및 순위를 사용한 SLIC-HF ~ Zhao et al. [21]은 DSFT의 학습 분할에서 후보 {y}m Poft(yx)를 샘플링하여 (양수, 음수) 쌍을 결정합니다.이 접근 방식을 SLiC-HF-sample-rank라고 합니다.순위를 결정하기 위해 인간의 선호도 데이터 DHF에서 학습한 두 개의 텍스트 대 텍스트 모델을 고려합니다.학습된 점별 보상 모델: Askell et al. [2]과 유사하게 그림 1에서 볼 수 있듯이 순위가 매겨진 각 쌍을 양수 및 음수 시퀀스로 이진화합니다.보상 모델을 학습할 때 입력 시퀀스는 &#39;[Context] ... [Summary] ...&#39; 형식으로 지정되고 대상 시퀀스는 &#39;Good&#39; 또는 &#39;Bad&#39;입니다.추론 시간에 디코더 측에서 토큰 &#39;Good&#39;의 확률을 계산하여 목록에 있는 m개 후보 각각에 점수를 매기고, 이들로부터 m개의 양수/음수 쌍을 샘플링합니다. 훈련된 쌍별 순위 모델: 그림 1에서 볼 수 있듯이, 우리는 인간의 피드백을 텍스트 대 텍스트 형식의 쌍별 순위 문제로 공식화합니다. 순위 모델을 훈련할 때, 입력 시퀀스는 &#39;[컨텍스트] ... [요약 A] ... [요약 B]&#39;로 포맷되고 대상 시퀀스는 &#39;A&#39; 또는 &#39;B&#39; 중 하나입니다. 추론 시간에 우리는 토너먼트 스타일 절차를 사용하여 목록에서 후보를 순위를 매깁니다. 예를 들어, 4명의 후보 C1, C2, C3, C4의 목록이 주어지면, 먼저 C1, C2와 C3, C4를 순위를 매긴 다음, 우승자(c1, c2), 우승자(C3, C4)를 순위를 매깁니다. m개의 후보가 주어지면, 순위 모델을 m 1번 호출하고 m - 1개의 긍정/부정 쌍이 생성됩니다. 보상 모델 [문맥] 문서 [요약] 긍정적 요약 → 양호 [문맥] 문서 [요약] 부정적 요약 → 나쁨 순위 모델 [문맥] 문서 [요약 A] 긍정적 요약 [요약 B] 부정적 요약 → A [문맥] 문서 [요약 A] 부정적 요약 [요약 B] 긍정적 요약 → B 그림 1: 텍스트 대 텍스트 보상 모델 및 순위 모델 학습.2.3 인간 피드백에 직접 기반한 SLIC-HF 순위 또는 보상 모델 없이 인간 피드백 데이터 세트인 DHF의 긍정적 및 부정적 시퀀스를 직접 보정하는 간단한 접근 방식도 고려합니다. 이 접근 방식을 SLIC-HF-direct라고 합니다. 이 접근 방식의 명백한 장점은 순위/보상 모델을 학습하거나 사용하지 않으므로 단순성과 효율성이 향상된 것입니다. SLiC-HF-direct는 SFT 모델에서 디코딩하고 디코드에 레이블을 지정하는 모델을 학습하는 데 추가적인 엔지니어링 비용이 발생하지 않습니다. 단점은 정책 외 인간 피드백 데이터 분포가 SFT 모델의 디코드 분포와 크게 다를 수 있다는 것입니다. 2.4 교정을 위한 정규화 항 교차 엔트로피 정규화를 위해 두 가지 대상 시퀀스 yref 선택을 고려합니다. 첫 번째 선택은 DSFT에서 yref를 정규화 대상으로 사용하는 것입니다. 두 번째 선택은 {ŷ}m에서 가장 높은 순위의 후보를 정규화 대상으로 사용하는 것입니다. 가장 높은 순위의 후보는 순위 모델 또는 보상 모델을 사용하여 선택할 수 있습니다. 3 실험 결과 3.1 데이터 세트 Stiennon et al.의 Reddit TL;DR 요약 데이터 세트에서 SLIC-HF를 연구합니다. [18]. 이 데이터 세트에는 미세 조정 데이터 DsFt, 인간 피드백 데이터 DHF와 함께 SFT 및 RLHF 모델 디코드가 포함되어 있으며, 이를 모델과 비교하는 데 사용합니다. DSFT는 Reddit TL;DR 데이터 세트의 필터링된 버전입니다. [19]. 여기에는 훈련, 검증 및 테스트 분할에서 117k/6k/6k 예제가 포함됩니다. DHF는 여러 모델의 디코드에 대한 64k 인간 선호도로 구성됩니다. 3.2 실험적 하이퍼 매개변수 우리는 모든 실험을 T5x 프레임워크[16]에서 T5 모델[15]을 사용하여 수행합니다. 우리의 절제 연구에서 우리는 생성 모델로 T5-대형 모델(770M)을 선택하고 순위 모델과 보상 모델¹로 T5-XXL(11B)을 선택합니다. 우리는 모든 생성 모델을 배치 크기 32로 훈련하고 순위/보상 모델은 배치 크기 128로 훈련합니다. 둘 다 기본 학습률 10-³로 훈련합니다. 우리는 순위 모델과 보상 모델을 DƒF 훈련 분할에서 훈련하고 DHF 검증 분할에서 가장 높은 정확도를 갖는 체크포인트를 선택했습니다. 우리는 DSFt 훈련 분할에서 T5 모델을 미세 조정하고 DSFT 검증 분할에서 가장 낮은 복잡도를 갖는 체크포인트를 선택합니다. 교정에서 우리는 10-5의 학습률과 1.0의 순위 마진 ẞ을 사용합니다. SLiC-HF-sample-rank를 사용하여 자체 디코드에서 모델을 교정할 때 미세 조정 전용 생성 모델에서 온도가 0.7이고 topk가 40인 디코드 8개를 샘플링합니다. 모델을 평가할 때 빔 크기가 4인 빔 검색을 사용합니다. 자동 평가를 위해 DSFT 검증 데이터 세트에서 T5-XXL 순위 모델로 측정한 인간 참조에 대한 모델 디코드의 승률을 계산합니다. 승률은 순위 모델이 인간 참조와 비교하여 선호하는 모델 디코딩된 요약의 백분율로 정의됩니다. 3.3 보상 모델 및 순위 모델 정확도 인간 피드백 및 인간 평가는 평가자가 두 요약을 비교하여 수행하는데, 이는 점별 평가보다 더 신뢰할 수 있기 때문입니다. 순위 모델은 작업과 더 잘 일치하는 쌍별 특성 때문에 보상 모델보다 이점이 있다고 가설을 세웁니다. T5-XXL 순위 모델과 T5-XXL 보상 모델(하위 섹션 2.2)을 훈련하고 비교합니다. 결과에 따르면, 우리 순위 모델의 DHF 검증 정확도는 73.23%로, 정확도가 71.34%²인 우리 보상 모델보다 약 2% 더 높습니다.3.4 SLIC Ablation 우리는 SLiC-HF 설정을 기준선에 대해 Ablation하기 위해 일련의 실험을 수행합니다.우리는 Stiennon et al. [18]에서 입증된 인간 선호도와의 상관 관계가 더 높기 때문에 순위 모델을 주요 지표로 사용합니다.선택된 설정은 나중에 하위 섹션 3.5에서 인간 평가 실험을 통해 검증합니다.우리는 참조 목적으로만 ROUGE 숫자를 보고하며 모델을 선택하는 데 사용하지 않습니다.참조 텍스트와 유사해지려는 인센티브가 적기 때문에 인간 피드백에서 학습할 때 ROUGE 숫자가 떨어질 것으로 예상됩니다.Stiennon et al. [18]의 RLHF와 유사하게, 우리는 또한 모델의 평균 길이가 증가하는 것을 관찰하고 하위 섹션 3.5에서 길이 통제 연구를 수행합니다. 3.4. SLIC-HF 대 필터링된 데이터에 대한 지속적인 미세 조정 인간 피드백 데이터에서 학습하는 간단한 방법은 이를 SFT 데이터 세트로 변환하고 이에 대한 미세 조정을 계속하는 것입니다. 일반적으로 우리는 통제된 &#39;우리는 더 작은 T5 순위/보상 모델이 우리의 설정에서 안정적으로 수렴하지 않는다는 것을 발견했습니다. 2 우리의 순위 및 보상 모델의 정확도는 Stiennon et al.의 6B 보상 모델과 유사합니다. [18]표 1: 인간 피드백 데이터를 활용하는 다양한 방법을 비교합니다. 랭커 승률은 참조 텍스트보다 모델 디코드를 선택하는 T5-XXL 순위 모델의 선호도입니다. Ablation 방법 인간 피드백 형태 참조 SFT 필터링된 데이터에 대한 SFT 계속 HF 데이터에서 양성 메트릭 정규화 #단어 R1/R2/RL 랭커 승률 27.50% 23.35.1/12.87/26.44.96% 31.22 33.02/11.27/24.51.65% SLIC-HF 최고 디코드, 보상별 최고 디코드, 순위별 SLIC-HF-직접 27.69 35.31/12.41/26.63.24% 28.26 35.39/12.69/26.65.43% SLiC-HF-sample-rank, 보상별 SLIC-HF-sample-rank, 보상별 SLIC-HF-sample-rank, 순위별 SLIC-HF-sample-rank, 순위별 SFT 대상 SFT 대상 최고 디코드 SFT 대상 최고 디코드 41.33.76/11.58/24.82.92% 38.33.87/11.48/24.82.42% 38.34.07/11.59/24.83.52% 37.34.49/11.92/25.86.21% 37.34.69/12.03/25.85.51% 생성 접근 방식[1]이지만 구현이 더 깔끔합니다. 지속적인 미세 조정을 위해 데이터를 필터링하는 세 가지 접근 방식을 고려합니다. • 긍정적인 인간 피드백 시퀀스만 유지하고 부정적인 시퀀스는 삭제합니다. • SFT 모델에서 8개의 요약을 디코딩하고 순위 모델을 사용하여 토너먼트 스타일 순위 접근 방식으로 요약 중 최상의 1개를 선택합니다. • SFT 모델에서 8개의 요약을 디코딩하고 보상 모델을 사용하여 각각에 점수를 매기고 최대 점수를 받은 요약을 선택하여 요약 중 최상의 1개를 선택합니다. 표 1에 표시된 대로 Reddit TL;DR 데이터 세트에서 긍정적인 인간 피드백 데이터에 대한 미세 조정을 계속하면 참조에 대한 모델 승률이 44.96%에서 51.65%로 약간 개선됩니다. 이 실험에서는 더 나은 모델을 위해 필터링 없이 모든 인간 피드백을 사용하기로 했습니다. 이는 품질에 대한 명확한 지식 없이 일부 인간 피드백 데이터에 액세스할 수 있는 실제 시나리오를 모방하기 때문입니다. 8개 중 가장 좋은 1개에 대한 미세 조정을 계속하면 참조에 대한 승률이 60% 이상으로 더욱 개선되고 필터링을 위해 쌍별 순위 모델을 사용하는 것이 점별 보상 모델보다 약간 더 좋습니다. 3.4.2 SLIC-HF-direct를 사용하여 인간 피드백 데이터에 직접 SLiC-HF 적용 예상대로 교정 손실이 감소하더라도 시퀀스 길이는 계속 증가하고 안정적인 값으로 수렴하지 않는 것을 관찰했습니다. 반면 SLIC-HF-samplerank는 견고하게 수렴합니다. SLiC-HF-direct는 인간 피드백 데이터의 다른 모델에서 생성된 분포 외 디코드로 인해 제거된다고 가정합니다. SLiC-HF-direct에 대한 최상의 체크포인트를 선택하기 위해 순위 모델을 사용할 때, 적당한 길이 증가가 있고 참조에 대한 82.92%의 승률을 보이는데, 이는 SLiC-HF-sample-rank에 가깝습니다. SLiC-HF-direct의 엔지니어링 복잡도는 모델을 미세 조정하는 것과 거의 같습니다. 따라서 인간의 피드백에 대한 빠른 실험에 적합한 후보입니다. 3.4.3 순위 모델 디코드에 SLiC-HF 적용 표 1에서 볼 수 있듯이, 순위 모델을 사용하는 SLiC-HF-sample-rank는 보상 모델을 사용하는 SLiC-HF-sample-rank에 비해 참조에 대한 승률이 약 3% 증가했습니다. 이 결과는 순위 모델이 보상 모델보다 인간의 선호도에 더 잘 부합한다는 하위 섹션 3.3의 관찰과 일치합니다. 순위 또는 보상 모델을 사용하는 SLIC-HF-sample-rank의 경우 정규화로 SFT 대상 또는 최고 순위 디코드를 사용하면 큰 차이가 나타나지 않습니다. 이는 SLiC-HF-sample-rank가 실제 기준 참조가 없는 경우에도 적용 가능함을 보여줍니다.표 1의 최고 순위 디코드에 대한 지속적인 미세 조정으로 인한 이득은 SLiC-HF에 가산되지 않습니다.3.5 인간 평가 크라우드 소싱을 사용하여 여러 시스템 간에 나란히 인간 평가를 수행합니다.³ 문서와 2~4개의 요약이 주어지면 평가자는 각 요약에 요점별 전반적인 품질을 할당하고 요약이 사실인지 여부를 선택한 다음 최상의 요약을 선택해야 합니다.각 작업은 3명의 다른 평가자가 복제하여 평가합니다.편견을 제거하기 위해 모든 모델을 익명화하고 각 작업에 대한 요약 순서를 무작위로 섞습니다.3명의 크라우드 워커 전체의 평가를 평균하여 요점별 메트릭을 집계하고 다수결 투표를 사용하여 선택 메트릭을 집계합니다. 인간 평가 템플릿과 평가 지침은 부록 A에서 확인할 수 있습니다.3.5.SLIC-HF 절제 연구 표 1의 절제 결과를 확인하기 위해 4방향 나란히 인간 평가를 수행합니다.검증 세트의 예제는 참조, SFT 모델, 계속 미세 조정 모델 및 SLIC-HF 모델(SLiC-HF-sample-rank, 순위 모델 사용, 최상의 디코드로 정규화)에서 샘플링됩니다.표 3에서 볼 수 있듯이 SLiC-HF는 73%의 시간 동안 최상의 모델로 선택되고 평균 품질이 상당히 높으며 가장 사실적인 모델입니다.일반적으로 평균 품질은 표 1의 랭커 승률과 잘 일치합니다.그림 2는 SFT, 계속 미세 조정 및 SLIC-HF 모델의 길이 제어 품질을 보여주며, 이는 SLiC-HF가 선호됨을 명확히 보여줍니다.길이 제어 품질 연구는 Stiennon et al. [18]에서 수행한 연구와 유사하며, 여기서 평균 점수는 참조에 대한 상대적 길이로 버킷팅된 예제들 사이에서 계산됩니다. 표 2: 참조를 비교하기 위한 4방향 인간 평가, 순위 모델을 사용하여 최상의 디코드에 대한 SFT 계속 SFT, 순위 모델을 사용하여 디코드 쌍이 있는 SLiC-HF. 선호도로 선택됨 % 13% 참조 SFT 5% 계속 SFT 5% 평균 품질이 사실임 % 3.94.16% 3.3.SLiC-HF 73% 3. 동일 4% 94.85% 94.85% 96.56% 4.SFT 4.SLIC-HF 3.3. 참조보다 선호되는 비율 3.3. 계속 SFT 2.-0.-0.-0.0.0.0.0.0.log(요약 길이 / 참조 길이) 그림 2: 다양한 기준선에 대한 SFT 및 SLiC-HF의 길이 버킷 평균 품질. 3.5.2 SLIC-HF 대 RLHF-PPO Stiennon et al. [18]의 RLHF-PPO 알고리즘에 대한 올바른 하이퍼 매개변수를 올바르게 구현하고 조정하는 것은 사소한 작업이 아닙니다. 프레임워크에서 알고리즘을 다시 구현하는 대신 Stiennon et al. [18]의 모델 디코드와 직접 비교합니다.3 Amazon Mechanical Turk를 사용하여 작업을 설정하고 평가자를 고용합니다.선호하는 참조 SFT의 분수 먼저 T5-large SFT 모델을 2방향 나란히 인간 평가에서 6B 디코더 전용 SFT 모델과 벤치마킹합니다.그림 3에서 볼 수 있듯이 SFT의 품질과 승률이 약간 더 높지만 통계적으로 유의미하지는 않습니다.다음으로 T5-large SLiC-HF-sample-rank 모델의 두 가지 변형을 Stiennon et al. [18]의 디코더 전용 6B RLHF-PPO 모델과 벤치마킹합니다. 보상 모델을 사용한 SLiC-HF-sample-rank는 RLHF-PPO와 유사한 성능을 보였고 순위 모델을 사용한 SLIC-HF-sample-rank는 RLHF-PPO보다 우수했습니다. SLiC-HF 모델의 요약은 RLHF-PPO 모델보다 약간 길었고 길이 제어 승률은 그림 3에 표시된 대로 RLHF-PPO와 유사했습니다. 표 3: SFT 기준선을 [18]과 비교하고 SLIC-HF 모델을 RLHF-PPO 모델과 비교하기 위한 세 가지 2방향 나란히 인간 평가. 통계적으로 유의미한 결과는 *로 표시했습니다. 시스템 비교 시스템 A(우리 방식) 방법 시스템 B([18]) 인간 선호도 승률 품질 단어 수 SFT(7억 7천만 세대) SLIC-HF(7억 세대, 11B 순위) SLIC-HF(7억 세대, 11B 보상) 23.SFT(sup6B) 36.RLHF(sup6B_rm6B) 단어 수 24.33.ABAB 38.RLHF(supбB_rm6B) 33.56% 44% 3.59 3.66%* 34%* 3.85* 3.61* 56% 44% 3.78 3.✓ SFT RLHF에 대한 선호도의 분수 0.SLIC-HF 순위 SLIC-HF 보상 0.-0.-0.0.0.0.-0.0.0.log(요약 길이 / 참조 SFT 길이) log(요약 길이 / RLHF-요약 길이) 그림 3: 다양한 기준선에 대한 SFT 및 SLiC-HF의 길이 버킷팅 평균 품질. 3.6 SLIC 확장 표 4: SLiC-HF-sample-rank에 대한 모델 매개변수 및 후보 수를 확장하는 효과. 영어: Ablation method SFT SFT # params m 770M 11B# words 23.24.Metrics R1/R2/RL 35.1/12.87/26.36.45/14.11/28.ranker 승률 44.96% 62.34% SLIC-HF 770M SLIC-HF 770M SLIC-HF 11B 8 37.96 34.49/11.92/25.86.21% 64 40.53 34.14/11.70/25.8 36.90 35.83/12.87/26.86.41% 96.10% 우리는 SLiC-HF-sample-rank를 확장하는 2가지 방법을 연구합니다: (1) 생성 모델 매개변수 확장, (2) 디코딩된 후보 수 m 확장. 표 4에서 보듯이, 생성 모델을 770M에서 11B로 확장하면 SFT 모델과 SLiC-HF 모델이 모두 크게 개선됩니다.반면에 m을 8에서 64로 확장해도 큰 도움이 되지 않습니다.4 SLiC-HF 대 RLHF-PPO에 대한 추가 논의 4.1 컴퓨팅/메모리 효율성 및 병렬성 표 5에서 SLiC-HF와 RLHF-PPO의 컴퓨팅 및 메모리 효율성 차이를 요약했습니다.표 5: 컴퓨팅 및 메모리 효율성 비교.p는 정책 네트워크의 매개변수 수를 나타냅니다. 보조 모델 디코딩된 시퀀스 학습을 위한 매개변수 메모리 사용량 단계당 매개변수 업데이트 병렬 디코딩 병렬 보상 입력 인코딩 캐싱 RLHF-PPO [18] SLIC-HF 디코드 순위 직접 보상, 값, SFT 1M 순위 800k 4p P Р 2p Р Ρ 배치 내 전체 학습 세트 배치 내 아니요 전체 학습 세트 예 RLHF-PPO와 SLiC-HF-sample-rank 모두에서 요약의 품질을 판단하는 데 사용되는 보조 순위 또는 보상 모델을 학습합니다. 그러나 Stiennon et al. [18]은 별도의 정책 및 가치 네트워크를 사용하는 것이 훨씬 더 효과적임을 발견했으며, 따라서 정책 업데이트와 함께 업데이트되는 보상 모델과 동일한 크기의 추가 보조 모델이 제공됩니다. 또한 정책, 값, 보상 및 SFT 모델(Stiennon et al. [18]에서 모두 동일한 크기)이 PPO 학습 루프 내에서 사용됩니다. 이러한 모델은 종종 하드웨어 메모리에 보관되어 학습 단계를 더 빠르게 보장합니다. SLiC-HF에서는 보상을 완전히 병렬 및 오프라인으로 계산할 수 있으므로 학습 중에 모델 가중치에 메모리를 1/4만 사용합니다. 이러한 메모리 절약은 더 큰 모델을 학습하는 데 재사용할 수 있습니다. = Stiennon et al. [18]은 RLHF 학습을 수행하는 데 1M 에피소드를 사용한다고 보고하는데, 이는 SLiC-HF에서 사용된 디코딩된 샘플 수와 거의 동일합니다(학습 예제당 m 8, 예제 123,169개). 그러나 실제로 SLiC-HF 디코딩은 모든 디코딩된 샘플이 동일한 정책을 사용하여 완전히 병렬 디코딩이 가능하기 때문에 훨씬 더 빠를 수 있습니다. 반면 PPO의 경우 정책은 모든 배치에서 업데이트되므로 디코딩 병렬성이 각 배치로 제한됩니다([18]의 512). 이는 후속 디코딩이 정책 업데이트에서 차단되기 때문입니다. 또한 PPO 디코딩은 학습 루프 내에서 발생하여 최적화 단계 시간이 훨씬 길어집니다. 반면 SLiC-HF의 경우 단계 시간은 미세 조정과 비슷하여 학습 루프에 디코딩이 없기 때문에 훨씬 더 빠릅니다. 상당한 디코딩 병렬성 이득 외에도 SLiC-HF는 간단한 입력 인코딩 캐싱 최적화를 사용하여 컴퓨팅을 줄일 수 있습니다. m개의 디코드가 동일한 SFT 정책에서 샘플링되므로 입력 시퀀스 인코딩된 상태를 다시 계산하는 대신 캐싱할 수 있습니다. 긴 컨텍스트를 포함하는 요약 및 기타 작업에서 이는 입력 시퀀스 길이가 출력보다 훨씬 길어지는 경향이 있으므로 중요할 수 있습니다. SLIC-HF는 순위를 학습 루프 내부가 아닌 외부에서 계산할 수 있으므로 RLHF와 비교하여 에피소드당 보상을 계산하는 데 유사한 병렬성 이점이 있습니다. 4.2 쌍별 순위 대 보상 모델 RL 알고리즘은 궤적의 예상 보상을 최대화하려고 하며, 이 경우 모델 요약의 품질에 대한 인간의 판단입니다. 이 보상 함수는 일반적으로 점별로 가정되는 반면, 인간의 선호도 데이터는 신뢰성을 개선하기 위해 쌍별로 수집됩니다. 따라서 쌍별 판단을 점별 보상으로 변환하는 데 노이즈가 발생하며, 이는 하위 섹션 3.3에서와 같이 순위 정확도의 차이로 추정할 수 있습니다. SLiC-HF는 두 요약의 상대적 순위에만 관심이 있으므로 이러한 쌍별-지점별 노이즈는 방지되며 이것이 SLiC-HF에 도움이 될 것이라고 추측합니다(표 1, 그림 3). 4.3 언어에서 상태와 동작의 가치 RL을 사용하여 처리하는 많은 작업의 경우 보상은 궤적의 끝에서 수집될 수 있으며(많은 Atari 게임에서처럼) 최종 보상을 특정 동작에 귀속시키는 것은 작업을 해결하는 방법을 배우는 데 매우 중요할 수 있습니다. 일반적으로 RL이 RLHF 문헌에서처럼 언어에 적용될 때 상태는 현재 텍스트의 접두사이고 동작은 다음 토큰을 선택하는 것과 일치합니다. 가치 함수의 역할은 접두사/입력에서 궤적(예: 요약)의 장점을 추정하는 것이며, 이는 직관적으로 인간 평가자에게 매우 어려운 작업이므로 RL도 가치 함수 추정 노이즈로 인해 어려움을 겪을 수 있습니다. 이와 대조적으로 SLIC-HF는 그러한 하위 모델에 의존하지 않고 더 깨끗한 선호 신호만을 사용하여 매개변수 업데이트를 추진하고 우리가 추측컨대 더 안정적인 최적화로 이어집니다.
--- METHOD ---
s [12, 21]는 모델 우도를 임의의, 아마도 미분 불가능한 보상과 일치시키고 샘플의 기대 보상을 최적화하기 위한 RL에 대한 대안을 제시합니다. Zhao et al. [21]는 디코딩된 시퀀스에 대한 언어 모델의 시퀀스 우도 po(y|x)를 참조 시퀀스와의 유사성에 따라 정렬하기 위해 시퀀스 우도 교정(SLiC)을 제안했습니다. 순위 교정 손실은 양의 시퀀스 y+와 음의 시퀀스 y¯를 대조하여 모델 Po가 음의 시퀀스에 비해 양의 시퀀스에 더 많은 확률 질량을 할당하도록 합니다.Lcal(0) = max(0, ẞ – log P₁(y+|x) + log Pe(y¯¯|x)) (2) 원래 SLiC 작업에서는 순위를 매기는 기준으로 참조와의 유사성을 사용했지만(예: ROUGE[11] 및 모델 임베딩 거리) 임의의 참조 없는 순위 함수 R(yo, y1,x) {0, 1}로 대체할 수 있습니다.특히 이 작업에서는 정책 외 선호도 데이터 D를 직접 사용하거나 D에서 예측 순위 모델 Ro(yo, y1, x)를 학습하여 인간의 선호도를 순위 함수로 사용합니다.이 인간 선호도 순위 함수로 SLiC를 사용하여 SLiC-HF를 호출하고 Stiennon et al.[18]에서 수집한 인간 피드백 데이터를 사용하여 적용합니다.
--- EXPERIMENT ---
TL;DR 요약 작업의 s는 SLIC-HF가 SFT(지도 미세 조정) 기준선을 크게 개선한다는 것을 보여줍니다. 나아가 SLiC-HF는 과거 작업에서 사용된 PPO RLHF 구현에 비해 경쟁력 있는 대안을 제시하는 동시에 구현이 훨씬 간단하고 조정이 쉬우며 실제로 계산 효율성이 더 높습니다. 1 서론 Transformer 기반 언어 모델의 대규모 확장 모델 매개변수와 학습 컴퓨팅으로 인해 인상적인 few-shot in-context 학습[5, 6]이 이루어졌지만, 인간 피드백 미세 조정(RLHF)의 강화 학습은 인간이 판단한 대로 생성 품질을 크게 개선할 수 있습니다. 이는 추상적 요약, 대화 및 창의적 글쓰기와 같은 다양한 다운스트림 언어 생성 작업의 모든 모델 규모에서 관찰되었습니다[18, 3, 7, 13]. 특히 요약의 경우 여러 연구에 따르면 RLHF로 조정된 모델에서 생성된 요약이 일반적으로 사용되는 데이터 세트의 참조 요약보다 선호됩니다[18, 10, 8]. 참조 요약은 종종 웹 문서에서 채굴되며 가장 높은 품질이나 선호하는 스타일이 아닐 수 있습니다. 결과적으로 순수한 지도 학습, 즉 문서가 주어진 참조 요약의 가능성을 최대화하는 것은 참조 요약의 품질에 의해 제한되므로 추가 피드백은 참조를 넘어 모델을 개선할 수 있습니다. ROUGE[11]와 같은 일반적으로 사용되는 참조 기반 메트릭은 모델에서 생성된 텍스트와 참조 텍스트 간의 유사성만 측정합니다. 이러한 참조 기반 메트릭은 참조 요약을 넘어서는 품질 개선을 측정할 수 없습니다. RLHF를 구현하기 위해 보상 모델 rø(x, y)는 인간 선호도 데이터(x, yo, y₁, i)DHF에서 학습합니다. 이는 나란히 배치된 인간 평가를 통해 수집되며, 평가자는 두 요약 yo와 y₁ 중 어느 것이 문서 x에 더 나은지 판단하도록 요청받습니다(즉, i = {0, 1}). 선호하는 요약을 y, 다른 요약을 y¯로 표시하면 인간 피드백은 (x, y+, ,y)가 됩니다. DHF. 일반적인 옵션 중 하나 사전 인쇄. 검토 중. ~~RLHF에서 사용하는 보상 모델의 학습 손실은 다음과 같습니다. = = (1) 손실 (ro) -E(x,y+,y)~DHF [log(σ (rø(x, y±) — rø(x, y¯¯))] PPO [17]와 같은 강화 학습 알고리즘을 사용하여 보상 모델 rø(x, y) [22, 18]에서 지정한 예상 보상을 최대화하도록 지도 학습 미세 조정 모델(SFT)을 개선합니다. RLHF 모델이 원래 지도 학습 정책에서 너무 멀리 벗어나지 않도록 일반적으로 손실에 KL-페널티 항을 추가합니다. 그러나 RLHF-PPO와 같은 알고리즘은 정책 네트워크와 크기가 비슷할 수 있는 별도의 가치 및 보상 네트워크를 추가하여 학습 프로세스에 상당한 복잡성을 도입합니다. 이러한 네트워크는 일반적으로 메모리에 보관되어 학습 속도를 극대화하며, 주어진 메모리 예산에 대해 학습 가능한 모델의 최대 크기를 크게 줄입니다. 또한 롤아웃을 사용하기 때문에 최적화 단계가 상당히 느립니다. 모델에서 샘플링/디코딩을 포함하는 학습 루프. 하이퍼 매개변수 튜닝 및 PPO 프로세스의 조정도 더 복잡하여 틈새 전문 지식이 필요합니다. 최근, 시퀀스 수준 대조 방법의 또 다른 종류[12, 21]는 모델 우도를 임의의, 아마도 미분 불가능한 보상과 일치시켜 샘플의 예상 보상을 최적화하기 위한 RL에 대한 대안을 제시합니다. Zhao et al.[21]은 참조 시퀀스와의 유사성에 따라 디코딩된 시퀀스에 대한 언어 모델의 시퀀스 우도, po(y|x)를 정렬하기 위해 시퀀스 우도 보정(SLiC)을 제안했습니다. 순위 교정 손실은 양의 시퀀스 y+와 음의 시퀀스 y¯를 대조하여 모델 Po가 음의 시퀀스에 비해 양의 시퀀스에 더 많은 확률 질량을 할당하도록 합니다.Lcal(0) = max(0, ẞ – log P₁(y+|x) + log Pe(y¯¯|x)) (2) 원래 SLiC 작업에서는 순위를 매기는 기준으로 참조와의 유사성을 사용했지만(예: ROUGE[11] 및 모델 임베딩 거리) 임의의 참조 없는 순위 함수 R(yo, y1,x) {0, 1}로 대체할 수 있습니다.특히 이 작업에서는 정책 외 선호도 데이터 D를 직접 사용하거나 D에서 예측 순위 모델 Ro(yo, y1, x)를 학습하여 인간의 선호도를 순위 함수로 사용합니다.이 인간 선호도 순위 함수로 SLiC를 사용하는 것을 SLiC-HF라고 하며 Stiennon et al.[18]에서 수집한 인간 피드백 데이터를 사용하여 적용합니다. 우리의 실험은 SLiC-HF가 인간이 판단할 때 Reddit TL;DR 요약 작업에서 요약 품질을 개선하는 데 도움이 된다는 것을 보여줍니다. 이 피드백은 오프 정책 오프라인 RL과 유사하게 다른 모델에 대해 수집되었습니다. 우리의 T5-Large[15](770M 매개변수) SFT 모델은 Stiennon et al.[18]의 6B 디코더 전용 SFT 모델과 유사한 성능을 보이지만, 우리는 SLiC-HF로 모델을 개선하여 인간이 판단할 때 최소한 Stiennon et al.[18]의 6B RLHF-PPO 모델만큼 좋은 성능을 낼 수 있습니다. 더욱이 SLIC-HF를 T5-XXL 11B 매개변수 SFT 모델[15]에 적용하면 결과가 상당히 개선됩니다. 이 논문의 주요 기여는 다음과 같습니다.• RLHF에 대한 더 간단하고 효율적이며 경쟁력 있는 대안인 인간의 선호도에서 학습하기 위해 SLiC를 적용하는 방법(SLiC-HF)• 다른 모델(정책 외)의 피드백/선호도 데이터는 SLIC-HF에서 효과적으로 활용할 수 있으므로 오픈 소스 T5 모델에 기반한 일반적인 SLiC-HF 레시피를 제공하는 모델에 대한 비용이 많이 드는 새로운 피드백 데이터를 수집할 필요가 없으며 Reddit TL;DR 요약 작업에서 RLHF보다 성능이 우수합니다.방법 이 작업에서 우리는 SLiC[21]를 적용하여 표준 감독 미세 조정 데이터(X, Yref) ~ DSFT 외에도 인간의 선호도 데이터(x, y+, y¯) ~ Dƒà를 사용하여 SFT 모델을 개선합니다.2.1 Zhao et al.에 따른 시퀀스 우도 교정 [21], 우리는 먼저 (X, Yref) ~ DSFT에서 감독 모델 Poft(y|x)를 미세 조정한 다음, 다음 손실을 최적화하는 SLiC 접근 방식을 사용하여 SFT 모델의 시퀀스 우도를 맞춥니다. Greg L(0) = Lcal(0, X, Yref, {ŷ}m) + \Ľ™eg(0,0ƒt; X, Yref) (3) 여기서 0 및 0ft는 현재 및 고정 SFT 모델 가중치이고, Lcal 및 Leg는 교정 및 정규화 손실이며, {ŷ}m은 SFT 모델에서 샘플링된 m개의 후보입니다. 보다 구체적으로, 우리는 단순성과 쌍별 인간 피드백 데이터에 대한 자연스러운 적합성을 위해 순위 교정 손실과 교차 엔트로피 정규화 손실을 선택합니다. 따라서 SLiC-HF의 손실 함수는 다음과 같습니다.L(0) = max(0, 8 — log P₁(y+|x) + log P₁(y¯¯|x)) — \ log Pe(yref|×) _ (4) 첫 번째 항은 교정 손실로, 여기서 x는 입력 시퀀스이고, y+와 y¯는 양수와 음수 시퀀스이며, §는 순위 손실의 마진에 대한 하이퍼 매개변수입니다.두 번째 항은 교차 엔트로피 손실로, 여기서 yref는 일부 대상 시퀀스이고 \는 정규화 가중치입니다.교차 엔트로피 손실은 Stiennon et al. [18]에서 사용된 KL 항과 유사하게 모델이 SFT 모델에 가깝게 유지되도록 장려하지만 SFT 가중치의 추가 사본이 필요하지 않습니다.KL 정규화 항은 Zhao et al. [21]에서도 탐구되었지만 비슷한 성능을 보이는 것으로 나타났습니다.y 및 y의 선택은 하위 섹션 2.2 및 2.3에서 설명합니다. 정규화 대상 yref의 선택은 하위 섹션 2.4에서 논의됩니다.2.2 샘플 및 순위를 사용한 SLIC-HF ~ Zhao et al. [21]은 DSFT의 학습 분할에서 후보 {y}m Poft(yx)를 샘플링하여 (양수, 음수) 쌍을 결정합니다.이 접근 방식을 SLiC-HF-sample-rank라고 합니다.순위를 결정하기 위해 인간의 선호도 데이터 DHF에서 학습한 두 개의 텍스트 대 텍스트 모델을 고려합니다.학습된 점별 보상 모델: Askell et al. [2]과 유사하게 그림 1에서 볼 수 있듯이 순위가 매겨진 각 쌍을 양수 및 음수 시퀀스로 이진화합니다.보상 모델을 학습할 때 입력 시퀀스는 &#39;[Context] ... [Summary] ...&#39; 형식으로 지정되고 대상 시퀀스는 &#39;Good&#39; 또는 &#39;Bad&#39;입니다.추론 시간에 디코더 측에서 토큰 &#39;Good&#39;의 확률을 계산하여 목록에 있는 m개 후보 각각에 점수를 매기고, 이들로부터 m개의 양수/음수 쌍을 샘플링합니다. 훈련된 쌍별 순위 모델: 그림 1에서 볼 수 있듯이, 우리는 인간의 피드백을 텍스트 대 텍스트 형식의 쌍별 순위 문제로 공식화합니다. 순위 모델을 훈련할 때, 입력 시퀀스는 &#39;[컨텍스트] ... [요약 A] ... [요약 B]&#39;로 포맷되고 대상 시퀀스는 &#39;A&#39; 또는 &#39;B&#39; 중 하나입니다. 추론 시간에 우리는 토너먼트 스타일 절차를 사용하여 목록에서 후보를 순위를 매깁니다. 예를 들어, 4명의 후보 C1, C2, C3, C4의 목록이 주어지면, 먼저 C1, C2와 C3, C4를 순위를 매긴 다음, 우승자(c1, c2), 우승자(C3, C4)를 순위를 매깁니다. m개의 후보가 주어지면, 순위 모델을 m 1번 호출하고 m - 1개의 긍정/부정 쌍이 생성됩니다. 보상 모델 [문맥] 문서 [요약] 긍정적 요약 → 양호 [문맥] 문서 [요약] 부정적 요약 → 나쁨 순위 모델 [문맥] 문서 [요약 A] 긍정적 요약 [요약 B] 부정적 요약 → A [문맥] 문서 [요약 A] 부정적 요약 [요약 B] 긍정적 요약 → B 그림 1: 텍스트 대 텍스트 보상 모델 및 순위 모델 학습.2.3 인간 피드백에 직접 기반한 SLIC-HF 순위 또는 보상 모델 없이 인간 피드백 데이터 세트인 DHF의 긍정적 및 부정적 시퀀스를 직접 보정하는 간단한 접근 방식도 고려합니다. 이 접근 방식을 SLIC-HF-direct라고 합니다. 이 접근 방식의 명백한 장점은 순위/보상 모델을 학습하거나 사용하지 않으므로 단순성과 효율성이 향상된 것입니다. SLiC-HF-direct는 SFT 모델에서 디코딩하고 디코드에 레이블을 지정하는 모델을 학습하는 데 추가적인 엔지니어링 비용이 발생하지 않습니다. 단점은 정책 외 인간 피드백 데이터 분포가 SFT 모델의 디코드 분포와 크게 다를 수 있다는 것입니다. 2.4 교정을 위한 정규화 항 교차 엔트로피 정규화를 위해 두 가지 대상 시퀀스 yref 선택을 고려합니다. 첫 번째 선택은 DSFT에서 yref를 정규화 대상으로 사용하는 것입니다. 두 번째 선택은 {ŷ}m에서 가장 높은 순위의 후보를 정규화 대상으로 사용하는 것입니다. 가장 높은 순위의 후보는 순위 모델 또는 보상 모델을 사용하여 선택할 수 있습니다. 3 실험 결과 3.1 데이터 세트 Stiennon et al.의 Reddit TL;DR 요약 데이터 세트에서 SLIC-HF를 연구합니다. [18]. 이 데이터 세트에는 미세 조정 데이터 DsFt, 인간 피드백 데이터 DHF와 함께 SFT 및 RLHF 모델 디코드가 포함되어 있으며, 이를 모델과 비교하는 데 사용합니다. DSFT는 Reddit TL;DR 데이터 세트의 필터링된 버전입니다. [19]. 여기에는 훈련, 검증 및 테스트 분할에서 117k/6k/6k 예제가 포함됩니다. DHF는 여러 모델의 디코드에 대한 64k 인간 선호도로 구성됩니다. 3.2 실험적 하이퍼 매개변수 우리는 모든 실험을 T5x 프레임워크[16]에서 T5 모델[15]을 사용하여 수행합니다. 우리의 절제 연구에서 우리는 생성 모델로 T5-대형 모델(770M)을 선택하고 순위 모델과 보상 모델¹로 T5-XXL(11B)을 선택합니다. 우리는 모든 생성 모델을 배치 크기 32로 훈련하고 순위/보상 모델은 배치 크기 128로 훈련합니다. 둘 다 기본 학습률 10-³로 훈련합니다. 우리는 순위 모델과 보상 모델을 DƒF 훈련 분할에서 훈련하고 DHF 검증 분할에서 가장 높은 정확도를 갖는 체크포인트를 선택했습니다. 우리는 DSFt 훈련 분할에서 T5 모델을 미세 조정하고 DSFT 검증 분할에서 가장 낮은 복잡도를 갖는 체크포인트를 선택합니다. 교정에서 우리는 10-5의 학습률과 1.0의 순위 마진 ẞ을 사용합니다. SLiC-HF-sample-rank를 사용하여 자체 디코드에서 모델을 교정할 때 미세 조정 전용 생성 모델에서 온도가 0.7이고 topk가 40인 디코드 8개를 샘플링합니다. 모델을 평가할 때 빔 크기가 4인 빔 검색을 사용합니다. 자동 평가를 위해 DSFT 검증 데이터 세트에서 T5-XXL 순위 모델로 측정한 인간 참조에 대한 모델 디코드의 승률을 계산합니다. 승률은 순위 모델이 인간 참조와 비교하여 선호하는 모델 디코딩된 요약의 백분율로 정의됩니다. 3.3 보상 모델 및 순위 모델 정확도 인간 피드백 및 인간 평가는 평가자가 두 요약을 비교하여 수행하는데, 이는 점별 평가보다 더 신뢰할 수 있기 때문입니다. 순위 모델은 작업과 더 잘 일치하는 쌍별 특성 때문에 보상 모델보다 이점이 있다고 가설을 세웁니다. T5-XXL 순위 모델과 T5-XXL 보상 모델(하위 섹션 2.2)을 훈련하고 비교합니다. 결과에 따르면, 우리 순위 모델의 DHF 검증 정확도는 73.23%로, 정확도가 71.34%²인 우리 보상 모델보다 약 2% 더 높습니다.3.4 SLIC Ablation 우리는 SLiC-HF 설정을 기준선에 대해 Ablation하기 위해 일련의 실험을 수행합니다.우리는 Stiennon et al. [18]에서 입증된 인간 선호도와의 상관 관계가 더 높기 때문에 순위 모델을 주요 지표로 사용합니다.선택된 설정은 나중에 하위 섹션 3.5에서 인간 평가 실험을 통해 검증합니다.우리는 참조 목적으로만 ROUGE 숫자를 보고하며 모델을 선택하는 데 사용하지 않습니다.참조 텍스트와 유사해지려는 인센티브가 적기 때문에 인간 피드백에서 학습할 때 ROUGE 숫자가 떨어질 것으로 예상됩니다.Stiennon et al. [18]의 RLHF와 유사하게, 우리는 또한 모델의 평균 길이가 증가하는 것을 관찰하고 하위 섹션 3.5에서 길이 통제 연구를 수행합니다. 3.4. SLIC-HF 대 필터링된 데이터에 대한 지속적인 미세 조정 인간 피드백 데이터에서 학습하는 간단한 방법은 이를 SFT 데이터 세트로 변환하고 이에 대한 미세 조정을 계속하는 것입니다. 일반적으로 우리는 통제된 &#39;우리는 더 작은 T5 순위/보상 모델이 우리의 설정에서 안정적으로 수렴하지 않는다는 것을 발견했습니다. 2 우리의 순위 및 보상 모델의 정확도는 Stiennon et al.의 6B 보상 모델과 유사합니다. [18]표 1: 인간 피드백 데이터를 활용하는 다양한 방법을 비교합니다. 랭커 승률은 참조 텍스트보다 모델 디코드를 선택하는 T5-XXL 순위 모델의 선호도입니다. Ablation 방법 인간 피드백 형태 참조 SFT 필터링된 데이터에 대한 SFT 계속 HF 데이터에서 양성 메트릭 정규화 #단어 R1/R2/RL 랭커 승률 27.50% 23.35.1/12.87/26.44.96% 31.22 33.02/11.27/24.51.65% SLIC-HF 최고 디코드, 보상별 최고 디코드, 순위별 SLIC-HF-직접 27.69 35.31/12.41/26.63.24% 28.26 35.39/12.69/26.65.43% SLiC-HF-sample-rank, 보상별 SLIC-HF-sample-rank, 보상별 SLIC-HF-sample-rank, 순위별 SLIC-HF-sample-rank, 순위별 SFT 대상 SFT 대상 최고 디코드 SFT 대상 최고 디코드 41.33.76/11.58/24.82.92% 38.33.87/11.48/24.82.42% 38.34.07/11.59/24.83.52% 37.34.49/11.92/25.86.21% 37.34.69/12.03/25.85.51% 생성 접근 방식[1]이지만 구현이 더 깔끔합니다. 지속적인 미세 조정을 위해 데이터를 필터링하는 세 가지 접근 방식을 고려합니다. • 긍정적인 인간 피드백 시퀀스만 유지하고 부정적인 시퀀스는 삭제합니다. • SFT 모델에서 8개의 요약을 디코딩하고 순위 모델을 사용하여 토너먼트 스타일 순위 접근 방식으로 요약 중 최상의 1개를 선택합니다. • SFT 모델에서 8개의 요약을 디코딩하고 보상 모델을 사용하여 각각에 점수를 매기고 최대 점수를 받은 요약을 선택하여 요약 중 최상의 1개를 선택합니다. 표 1에 표시된 대로 Reddit TL;DR 데이터 세트에서 긍정적인 인간 피드백 데이터에 대한 미세 조정을 계속하면 참조에 대한 모델 승률이 44.96%에서 51.65%로 약간 개선됩니다. 이 실험에서는 더 나은 모델을 위해 필터링 없이 모든 인간 피드백을 사용하기로 했습니다. 이는 품질에 대한 명확한 지식 없이 일부 인간 피드백 데이터에 액세스할 수 있는 실제 시나리오를 모방하기 때문입니다. 8개 중 가장 좋은 1개에 대한 미세 조정을 계속하면 참조에 대한 승률이 60% 이상으로 더욱 개선되고 필터링을 위해 쌍별 순위 모델을 사용하는 것이 점별 보상 모델보다 약간 더 좋습니다. 3.4.2 SLIC-HF-direct를 사용하여 인간 피드백 데이터에 직접 SLiC-HF 적용 예상대로 교정 손실이 감소하더라도 시퀀스 길이는 계속 증가하고 안정적인 값으로 수렴하지 않는 것을 관찰했습니다. 반면 SLIC-HF-samplerank는 견고하게 수렴합니다. SLiC-HF-direct는 인간 피드백 데이터의 다른 모델에서 생성된 분포 외 디코드로 인해 제거된다고 가정합니다. SLiC-HF-direct에 대한 최상의 체크포인트를 선택하기 위해 순위 모델을 사용할 때, 적당한 길이 증가가 있고 참조에 대한 82.92%의 승률을 보이는데, 이는 SLiC-HF-sample-rank에 가깝습니다. SLiC-HF-direct의 엔지니어링 복잡도는 모델을 미세 조정하는 것과 거의 같습니다. 따라서 인간의 피드백에 대한 빠른 실험에 적합한 후보입니다. 3.4.3 순위 모델 디코드에 SLiC-HF 적용 표 1에서 볼 수 있듯이, 순위 모델을 사용하는 SLiC-HF-sample-rank는 보상 모델을 사용하는 SLiC-HF-sample-rank에 비해 참조에 대한 승률이 약 3% 증가했습니다. 이 결과는 순위 모델이 보상 모델보다 인간의 선호도에 더 잘 부합한다는 하위 섹션 3.3의 관찰과 일치합니다. 순위 또는 보상 모델을 사용하는 SLIC-HF-sample-rank의 경우 정규화로 SFT 대상 또는 최고 순위 디코드를 사용하면 큰 차이가 나타나지 않습니다. 이는 SLiC-HF-sample-rank가 실제 기준 참조가 없는 경우에도 적용 가능함을 보여줍니다.표 1의 최고 순위 디코드에 대한 지속적인 미세 조정으로 인한 이득은 SLiC-HF에 가산되지 않습니다.3.5 인간 평가 크라우드 소싱을 사용하여 여러 시스템 간에 나란히 인간 평가를 수행합니다.³ 문서와 2~4개의 요약이 주어지면 평가자는 각 요약에 요점별 전반적인 품질을 할당하고 요약이 사실인지 여부를 선택한 다음 최상의 요약을 선택해야 합니다.각 작업은 3명의 다른 평가자가 복제하여 평가합니다.편견을 제거하기 위해 모든 모델을 익명화하고 각 작업에 대한 요약 순서를 무작위로 섞습니다.3명의 크라우드 워커 전체의 평가를 평균하여 요점별 메트릭을 집계하고 다수결 투표를 사용하여 선택 메트릭을 집계합니다. 인간 평가 템플릿과 평가 지침은 부록 A에서 확인할 수 있습니다.3.5.SLIC-HF 절제 연구 표 1의 절제 결과를 확인하기 위해 4방향 나란히 인간 평가를 수행합니다.검증 세트의 예제는 참조, SFT 모델, 계속 미세 조정 모델 및 SLIC-HF 모델(SLiC-HF-sample-rank, 순위 모델 사용, 최상의 디코드로 정규화)에서 샘플링됩니다.표 3에서 볼 수 있듯이 SLiC-HF는 73%의 시간 동안 최상의 모델로 선택되고 평균 품질이 상당히 높으며 가장 사실적인 모델입니다.일반적으로 평균 품질은 표 1의 랭커 승률과 잘 일치합니다.그림 2는 SFT, 계속 미세 조정 및 SLIC-HF 모델의 길이 제어 품질을 보여주며, 이는 SLiC-HF가 선호됨을 명확히 보여줍니다.길이 제어 품질 연구는 Stiennon et al. [18]에서 수행한 연구와 유사하며, 여기서 평균 점수는 참조에 대한 상대적 길이로 버킷팅된 예제들 사이에서 계산됩니다. 표 2: 참조를 비교하기 위한 4방향 인간 평가, 순위 모델을 사용하여 최상의 디코드에 대한 SFT 계속 SFT, 순위 모델을 사용하여 디코드 쌍이 있는 SLiC-HF. 선호도로 선택됨 % 13% 참조 SFT 5% 계속 SFT 5% 평균 품질이 사실임 % 3.94.16% 3.3.SLiC-HF 73% 3. 동일 4% 94.85% 94.85% 96.56% 4.SFT 4.SLIC-HF 3.3. 참조보다 선호되는 비율 3.3. 계속 SFT 2.-0.-0.-0.0.0.0.0.0.log(요약 길이 / 참조 길이) 그림 2: 다양한 기준선에 대한 SFT 및 SLiC-HF의 길이 버킷 평균 품질. 3.5.2 SLIC-HF 대 RLHF-PPO Stiennon et al. [18]의 RLHF-PPO 알고리즘에 대한 올바른 하이퍼 매개변수를 올바르게 구현하고 조정하는 것은 사소한 작업이 아닙니다. 프레임워크에서 알고리즘을 다시 구현하는 대신 Stiennon et al. [18]의 모델 디코드와 직접 비교합니다.3 Amazon Mechanical Turk를 사용하여 작업을 설정하고 평가자를 고용합니다.선호하는 참조 SFT의 분수 먼저 T5-large SFT 모델을 2방향 나란히 인간 평가에서 6B 디코더 전용 SFT 모델과 벤치마킹합니다.그림 3에서 볼 수 있듯이 SFT의 품질과 승률이 약간 더 높지만 통계적으로 유의미하지는 않습니다.다음으로 T5-large SLiC-HF-sample-rank 모델의 두 가지 변형을 Stiennon et al. [18]의 디코더 전용 6B RLHF-PPO 모델과 벤치마킹합니다. 보상 모델을 사용한 SLiC-HF-sample-rank는 RLHF-PPO와 유사한 성능을 보였고 순위 모델을 사용한 SLIC-HF-sample-rank는 RLHF-PPO보다 우수했습니다. SLiC-HF 모델의 요약은 RLHF-PPO 모델보다 약간 길었고 길이 제어 승률은 그림 3에 표시된 대로 RLHF-PPO와 유사했습니다. 표 3: SFT 기준선을 [18]과 비교하고 SLIC-HF 모델을 RLHF-PPO 모델과 비교하기 위한 세 가지 2방향 나란히 인간 평가. 통계적으로 유의미한 결과는 *로 표시했습니다. 시스템 비교 시스템 A(우리 방식) 방법 시스템 B([18]) 인간 선호도 승률 품질 단어 수 SFT(7억 7천만 세대) SLIC-HF(7억 세대, 11B 순위) SLIC-HF(7억 세대, 11B 보상) 23.SFT(sup6B) 36.RLHF(sup6B_rm6B) 단어 수 24.33.ABAB 38.RLHF(supбB_rm6B) 33.56% 44% 3.59 3.66%* 34%* 3.85* 3.61* 56% 44% 3.78 3.✓ SFT RLHF에 대한 선호도의 분수 0.SLIC-HF 순위 SLIC-HF 보상 0.-0.-0.0.0.0.-0.0.0.log(요약 길이 / 참조 SFT 길이) log(요약 길이 / RLHF-요약 길이) 그림 3: 다양한 기준선에 대한 SFT 및 SLiC-HF의 길이 버킷팅 평균 품질. 3.6 SLIC 확장 표 4: SLiC-HF-sample-rank에 대한 모델 매개변수 및 후보 수를 확장하는 효과. 영어: Ablation method SFT SFT # params m 770M 11B# words 23.24.Metrics R1/R2/RL 35.1/12.87/26.36.45/14.11/28.ranker 승률 44.96% 62.34% SLIC-HF 770M SLIC-HF 770M SLIC-HF 11B 8 37.96 34.49/11.92/25.86.21% 64 40.53 34.14/11.70/25.8 36.90 35.83/12.87/26.86.41% 96.10% 우리는 SLiC-HF-sample-rank를 확장하는 2가지 방법을 연구합니다: (1) 생성 모델 매개변수 확장, (2) 디코딩된 후보 수 m 확장. 표 4에서 보듯이, 생성 모델을 770M에서 11B로 확장하면 SFT 모델과 SLiC-HF 모델이 모두 크게 개선됩니다.반면에 m을 8에서 64로 확장해도 큰 도움이 되지 않습니다.4 SLiC-HF 대 RLHF-PPO에 대한 추가 논의 4.1 컴퓨팅/메모리 효율성 및 병렬성 표 5에서 SLiC-HF와 RLHF-PPO의 컴퓨팅 및 메모리 효율성 차이를 요약했습니다.표 5: 컴퓨팅 및 메모리 효율성 비교.p는 정책 네트워크의 매개변수 수를 나타냅니다. 보조 모델 디코딩된 시퀀스 학습을 위한 매개변수 메모리 사용량 단계당 매개변수 업데이트 병렬 디코딩 병렬 보상 입력 인코딩 캐싱 RLHF-PPO [18] SLIC-HF 디코드 순위 직접 보상, 값, SFT 1M 순위 800k 4p P Р 2p Р Ρ 배치 내 전체 학습 세트 배치 내 아니요 전체 학습 세트 예 RLHF-PPO와 SLiC-HF-sample-rank 모두에서 요약의 품질을 판단하는 데 사용되는 보조 순위 또는 보상 모델을 학습합니다. 그러나 Stiennon et al. [18]은 별도의 정책 및 가치 네트워크를 사용하는 것이 훨씬 더 효과적임을 발견했으며, 따라서 정책 업데이트와 함께 업데이트되는 보상 모델과 동일한 크기의 추가 보조 모델이 제공됩니다. 또한 정책, 값, 보상 및 SFT 모델(Stiennon et al. [18]에서 모두 동일한 크기)이 PPO 학습 루프 내에서 사용됩니다. 이러한 모델은 종종 하드웨어 메모리에 보관되어 학습 단계를 더 빠르게 보장합니다. SLiC-HF에서는 보상을 완전히 병렬 및 오프라인으로 계산할 수 있으므로 학습 중에 모델 가중치에 메모리를 1/4만 사용합니다. 이러한 메모리 절약은 더 큰 모델을 학습하는 데 재사용할 수 있습니다. = Stiennon et al. [18]은 RLHF 학습을 수행하는 데 1M 에피소드를 사용한다고 보고하는데, 이는 SLiC-HF에서 사용된 디코딩된 샘플 수와 거의 동일합니다(학습 예제당 m 8, 예제 123,169개). 그러나 실제로 SLiC-HF 디코딩은 모든 디코딩된 샘플이 동일한 정책을 사용하여 완전히 병렬 디코딩이 가능하기 때문에 훨씬 더 빠를 수 있습니다. 반면 PPO의 경우 정책은 모든 배치에서 업데이트되므로 디코딩 병렬성이 각 배치로 제한됩니다([18]의 512). 이는 후속 디코딩이 정책 업데이트에서 차단되기 때문입니다. 또한 PPO 디코딩은 학습 루프 내에서 발생하여 최적화 단계 시간이 훨씬 길어집니다. 반면 SLiC-HF의 경우 단계 시간은 미세 조정과 비슷하여 학습 루프에 디코딩이 없기 때문에 훨씬 더 빠릅니다. 상당한 디코딩 병렬성 이득 외에도 SLiC-HF는 간단한 입력 인코딩 캐싱 최적화를 사용하여 컴퓨팅을 줄일 수 있습니다. m개의 디코드가 동일한 SFT 정책에서 샘플링되므로 입력 시퀀스 인코딩된 상태를 다시 계산하는 대신 캐싱할 수 있습니다. 긴 컨텍스트를 포함하는 요약 및 기타 작업에서 이는 입력 시퀀스 길이가 출력보다 훨씬 길어지는 경향이 있으므로 중요할 수 있습니다. SLIC-HF는 순위를 학습 루프 내부가 아닌 외부에서 계산할 수 있으므로 RLHF와 비교하여 에피소드당 보상을 계산하는 데 유사한 병렬성 이점이 있습니다. 4.2 쌍별 순위 대 보상 모델 RL 알고리즘은 궤적의 예상 보상을 최대화하려고 하며, 이 경우 모델 요약의 품질에 대한 인간의 판단입니다. 이 보상 함수는 일반적으로 점별로 가정되는 반면, 인간의 선호도 데이터는 신뢰성을 개선하기 위해 쌍별로 수집됩니다. 따라서 쌍별 판단을 점별 보상으로 변환하는 데 노이즈가 발생하며, 이는 하위 섹션 3.3에서와 같이 순위 정확도의 차이로 추정할 수 있습니다. SLiC-HF는 두 요약의 상대적 순위에만 관심이 있으므로 이러한 쌍별-지점별 노이즈는 방지되며 이것이 SLiC-HF에 도움이 될 것이라고 추측합니다(표 1, 그림 3). 4.3 언어에서 상태와 동작의 가치 RL을 사용하여 처리하는 많은 작업의 경우 보상은 궤적의 끝에서 수집될 수 있으며(많은 Atari 게임에서처럼) 최종 보상을 특정 동작에 귀속시키는 것은 작업을 해결하는 방법을 배우는 데 매우 중요할 수 있습니다. 일반적으로 RL이 RLHF 문헌에서처럼 언어에 적용될 때 상태는 현재 텍스트의 접두사이고 동작은 다음 토큰을 선택하는 것과 일치합니다. 가치 함수의 역할은 접두사/입력에서 궤적(예: 요약)의 장점을 추정하는 것이며, 이는 직관적으로 인간 평가자에게 매우 어려운 작업이므로 RL도 가치 함수 추정 노이즈로 인해 어려움을 겪을 수 있습니다. 이와 대조적으로 SLIC-HF는 이러한 하위 모델에 의존하지 않고 더 깨끗한 선호도 신호만 사용하여 매개변수 업데이트를 구동하고 더 안정적인 최적화로 이어집니다.관련 연구 RL은 번역을 위한 BLEU[20] 및 요약을 위한 ROUGE[14]와 같이 언어 생성에서 임의의 보상을 최적화하는 데 사용되었습니다.그러나 이러한 메트릭은 개선되었지만 메트릭 부정렬로 인해 품질에 대한 인간의 판단이 저하되었습니다.보상 함수를 인간의 판단과 더 잘 일치시키기 위한 노력의 일환으로 많은 연구에서 RL을 사용하여 언어 모델을 신중하게 수집된 인간 판단을 예측하도록 훈련된 보상 모델[22, 18, 13]과 일치시켰으며, 요약을 초기 개념 증명으로 사용했습니다.Jaques et al.에서 처음 사용된 KL 페널티 항[9]은 조정된 모델이 초기 지도 학습 모델에서 벗어나는 것을 방지하기 위한 정규화로 사용되며 SLIC에서도 사용됩니다[21].Liu et al. [12]는 보상 함수에 따라 모델에서 생성된 디코드를 순위 지정하는 SLiC [21]와 유사한 의도를 가진 BRIO를 제안합니다. BRIO는 생성된 디코드의 길이 정규화된 시퀀스 확률을 ROUGE로 측정한 참조와의 유사성에 맞춰 모델을 학습시킵니다. 이와 대조적으로 RLHF와 유사하게 SLiC-HF는 참조와의 유사성 대신 두 요약을 고려하여 인간의 선호도를 예측하도록 학습된 모델에 맞춰 기술을 조정합니다. Bai et al. [4]는 인간의 선호도 데이터를 대규모 언어 모델의 판단으로 대체하고 이를 AI 피드백(AIF)이라고 합니다. SLIC-HF도 AIF와 정확히 같은 방식으로 사용할 수 있으며 피드백의 AI 또는 인간 출처에 대해 무관합니다. 6
--- CONCLUSION ---
이 작업에서 우리는 인간 피드백 데이터의 시퀀스 가능성을 보정하는 SLiC-HF를 제안했습니다. Reddit TL;DR 요약 작업에 대한 우리의 실험은 SLiC-HF가 감독된 미세 조정(SFT) 기준선을 크게 개선하고, 구현이 더 간단하고, 조정이 더 쉬우며, 계산적으로 효율적이면서도 과거 작업의 RLHF-PPO 구현에 대한 경쟁력 있는 대안을 제시한다는 것을 보여줍니다. 향후 작업에는 다른 보상 함수 및/또는 비인간적 피드백을 사용하여 다른 언어 생성 작업에서 SLiC-HF를 연구하는 것이 포함될 수 있습니다. 참고문헌 [1] Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, Mirella Lapata. 2022. mface: 사실적 일관성 평가를 통한 다국어 요약. [2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan. 2021. 정렬을 위한 실험실로서의 일반 언어 지원자. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. 인간 피드백을 통한 강화 학습을 통해 도움이 되고 무해한 지원자 훈련. arXiv 사전 인쇄본 arXiv:2204.05862. [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon 등. 2022. 헌법적 AI: AI 피드백의 무해함. arXiv 사전 인쇄 arXiv:2212.08073. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc.[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. [7] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker 등 2022. 타깃 인간 판단을 통한 대화 에이전트의 정렬 개선. arXiv 사전 인쇄본 arXiv:2209.14375. [8] Tanya Goyal, Junyi Jessy Li, Greg Durrett. 2022. gpt-3 시대의 뉴스 요약 및 평가. arXiv 사전 인쇄본 arXiv:2209.12356. [9] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E Turner, Douglas Eck. 2017. 시퀀스 튜터: kl-제어를 사용한 시퀀스 생성 모델의 보수적 미세 조정. 기계 학습 국제 컨퍼런스, 1645-1654쪽. PMLR. [10] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. 언어 모델의 전체적 평가. arXiv 사전 인쇄본 arXiv:2211.09110. [11] Chin-Yew Lin. 2004. Rouge: 요약의 자동 평가를 위한 패키지. Text summarization branches out, 74-81페이지. [12] Yixin Liu, Pengfei Liu, Dragomir Radev, Graham Neubig. 2022. BRIO: 추상적 요약에 질서 부여. 60회 연례 총회 의사록(제1권: 장문 논문), 2890-2903쪽, 아일랜드 더블린. 전산 언어학 협회. [13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35:27730-27744. [14] Romain Paulus, Caiming Xiong, Richard Socher. 2017. 추상적 요약을 위한 심층 강화 모델. arXiv 사전 인쇄본 arXiv:1705.04304. [15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li 및 Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. 기계 학습 연구 저널, 21(140):1–67. [16] Adam Roberts, 정형원, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan 세타, 라이언 세파시, 알렉산더 Spiridonov, Joshua Newlan, Andrea Gesmundo. 2022. t5x 및 seqio를 사용하여 모델 및 데이터 확장. arXiv 사전 인쇄본 arXiv:2203.17189. [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 2017. 근접 정책 최적화 알고리즘. arXiv 사전 인쇄본 arXiv:1707.06347.[18] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano. 2020. 인간 피드백을 통한 요약 학습. 신경 정보 처리 시스템의 발전, 33권, 3008-3021페이지. Curran Associates, Inc. [19] Michael Völske, Martin Potthast, Shahbaz Syed, Benno Stein. 2017. TL;DR: 자동 요약을 배우기 위한 Reddit 마이닝. 덴마크 코펜하겐에서 발간된 새로운 요약 분야 워크숍 회의록, 59-63쪽. 계산언어학 협회. [20] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google의 신경망 기계 번역 시스템: 인간 번역과 기계 번역 간의 격차 해소. arXiv 사전 인쇄본 arXiv:1609.08144. [21] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, Peter J Liu. 2023. Calibrating sequence likelihood improves conditional language generation. 제11회 국제 학습 표현 컨퍼런스에서. [22] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving. 2020. Fine-tuning language models from human preferences. 지침: 1. 문서와 아래 요약을 읽고 어느 것이 문서를 더 잘 요약하는지 확인합니다. 2. 각 요약에 대해 언급된 모든 내용이 문서와 사실적으로 일치하는지 확인합니다(말한 모든 내용은 문서 텍스트에서 확인할 수 있음). 3. 요약의 품질을 1~5점 척도로 평가합니다(1 = 요약이 나쁨, 5 = 요약이 좋음) 4. 가장 좋은 요약을 선택합니다. 문서: 간단하게 설명하려고 노력하겠습니다! **배경**✶ * 저는 항상 간헐적(매우 캐주얼!)으로 조깅을 했고, 보통 35km를 달렸습니다. * 무릎이 항상 까다로워서 물리 치료사에게 갔는데, 물리 치료사는 제가 &quot;러너스 무릎&quot;이라고 생각했습니다. * 임신 전에는 &quot;러너스 무릎&quot;이 8-10km 거리 범위에 도달하면 악화되었습니다. 괜찮은 기초가 있어도(C210k 유형 프로그램 수행) **현재 문제** 1년 전에 아기를 낳았기 때문에 전반적으로 약 1.5년 동안 달리지 않았습니다. 저는 꽤 날씬하고 지난 1년 동안 유산소 운동 스타일의 수업을 들었기 때문에 완전히 몸매가 망가진 것은 아닙니다. 체중 운동, 유산소 운동, 자전거, 엘립티컬은 모두 괜찮습니다. 그러나 아주 조금만 달리거나 심지어 장거리 산책이나 하이킹을 하면 골반이 매우 아프고 팽팽해지고 무릎이 매우 빨리 아프기 시작합니다. 저는 이미 일반적인 스쿼트/런지/스트레칭 유형의 운동을 하고 있습니다. 달리기가 더 이상 저에게 맞지 않는 것 같은 느낌이 들기 시작했습니다. 제가 달리기를 정말 좋아하기 때문에 실망스럽습니다! 비슷한 경험을 한 분이 있나요? 도움이 될 만한 스트레칭이나 운동을 추천해 주실 수 있나요? 의사를 만나야 할까요? 아니면 물리 치료사를 만나야 할까요? 어떻게 해야 할지 잘 모르겠습니다. 감사합니다! 요약 0: 임신 때문에 몸에 문제가 생긴 것 같아요. 지금은 골반과 무릎에 통증이 있어서 아무리 조금만 달려도 달릴 수 없습니다. 문제가 제가 완전히 몸매가 망가졌다는 것만은 아니라고 확신합니다. 요약 0은 문서와 관련하여 완전히 사실입니다. O 예 아니요 요약 0 품질: 요약 1: 조금만 달려도 골반과 무릎이 아프고, 달리기가 더 이상 저에게 맞지 않는 것 같은 느낌이 들기 시작했습니다. 요약 1은 문서와 관련하여 완전히 사실입니다.예 아니요 요약 1 품질: 요약 2: 1.5년 동안 달리지 않았고, 달릴 때마다 골반과 무릎이 많이 아픕니다.어떻게 해야 할까요?요약 2는 문서와 관련하여 완전히 사실입니다.예 아니요 요약 2 품질: 요약 3: 조금만 달려도 무릎이 아프고, 달릴 때 골반과 무릎이 매우 빨리 아픕니다.제가 할 수 있는 것이 있을까요?의사를 만나야 할까요?아니면 물리치료사를 만나야 할까요?요약 3은 문서와 관련하여 완전히 사실입니다.O 예 아니요 요약 3 품질: 더 나은 요약을 선택하세요.○ 요약요약선호 사항 없음 요약○ 요약그림 4: 인간 평가 과제의 예.인간 평가 4개의 요약이 있는 인간 평가 과제의 예는 그림 4를 참조하세요.요약은 각 예시에 대해 무작위로 섞이고 모델은 익명 처리됩니다.
