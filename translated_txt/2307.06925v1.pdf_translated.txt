--- INTRODUCTION ---
생성 모델의 급속한 발전은 콘텐츠 생성에 혁명을 일으켜 다양한 아트워크를 손쉽게 생성할 수 있게 했습니다.*이 작업은 NVIDIA에서 인턴십을 하는 동안 수행되었습니다 ¹프로젝트 페이지는 https://datencoder.github.io에서 확인할 수 있습니다.그들의 진정한 잠재력 중 일부는 개인화에 있으며, 사용자가 고유한 개인 개념에 맞게 출력을 조정할 수 있게 합니다.모델을 개인화하는 것은 개인 소지품, 기억 또는 자화상의 고유한 특성을 포착하고 나타내도록 모델을 사용자 지정하는 것을 포함합니다.그러나 초기 개인화 방법[Gal et al. 2022; Ruiz et al. 2022]은 여러 이미지의 가용성에 의존하고 장기적인 최적화가 필요합니다.효과적인 대안은 개념을 타겟팅하기 위한 예측 모델을 사전 학습하는 것입니다.이러한 접근 방식은 인코더를 학습시켜 주어진 원하는 대상 개념을 정확하게 재구성하는 텍스트 임베딩을 예측합니다.획득한 임베딩을 사용하여 주어진 개념을 묘사하는 장면을 생성할 수 있습니다.그래도 이러한 방법은 한계에 직면합니다.첫째, 단일 클래스 도메인에 의존하여 다양한 개념의 롱테일 분포를 포착하는 능력을 제한합니다. 둘째, 일부 접근 방식은 스퓨리어스 배경 특징을 삭제하면서 대상 개념의 특성을 효과적으로 포착하기 위해 분할 마스크나 다중 뷰 입력과 같은 외부 사전 지식이 필요합니다. 이 작업에서 우리는 E4T[Gal et al. 2023]를 따릅니다. 이 접근 방식은 인코더를 간단한(5-15회 반복) 미세 조정을 위한 초기화 형태로 활용합니다. E4T는 각 개별 도메인에 대한 인코더를 학습하고 추론 시간 조정을 위해 약 70GB의 VRAM이 필요합니다. 우리의 접근 방식은 여러 도메인을 처리할 수 있으며 추론 시간 2를 줄입니다. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir 및 Amit H. Bermano 메모리 요구 사항. 우리는 인코더를 설계하는 동안 두 가지 목표를 고려합니다. (1) 대상 개념을 편집하는 기능, (2) 대상의 구별 가능한 특성을 충실하게 포착하는 기능. 우리는 생성 모델의 편집 가능한 영역 내에서 단어를 예측하도록 모델을 정규화하여 첫 번째 목표를 달성합니다. 이전 단일 도메인 방법과 달리, 우리는 타겟 도메인에 대한 거친 설명에 의존하지 않습니다. 대신, 우리는 대조 기반 접근 방식을 사용하여 예측된 임베딩을 단어 임베딩 공간의 의미 있는 영역으로 밀어냅니다. 직관적으로, 예측이 개념 클래스를 의미적으로 설명하는 단어 근처에 있도록 하면 모델의 개념 클래스에 대한 사전 지식을 더 잘 보존할 수 있습니다. 두 번째 목표를 위해, 우리는 더 높은 충실도로 타겟 개념의 독특한 특징을 포착하기 위해 하이퍼 네트워크를 도입합니다. 관리 가능한 모델 크기를 보장하기 위해, 우리는 Hu et al. [2021] 및 Gal et al. [2023]에 설명된 접근 방식에 따라 UNET-denoiser 모델의 가중치에 대한 저랭크 분해를 예측하는 전략을 사용합니다. 마지막으로, 조인트 임베딩 및 하이퍼 네트워크 예측은 12개 이하의 최적화 단계가 필요한 정규화된 LoRA 학습 프로세스를 초기화하는 데 사용됩니다. 중요한 점은 이를 통해 메모리 요구 사항이 약 70GB에서 30GB 미만으로 줄어들고 학습 및 추론 시간이 단축된다는 것입니다. 우리는 우리의 방법을 기존의 인코더와 최적화 기반 접근 방식과 비교하고, 그것이 다양한 도메인에서 높은 품질과 빠른 개인화를 달성할 수 있음을 보여줍니다.
--- RELATED WORK ---
확산 모델을 사용한 텍스트 기반 이미지 생성. 텍스트-이미지 합성은 최근 몇 년 동안 상당한 진전을 이루었으며, 주로 사전 훈련된 확산 모델[Ho et al. 2020a]과 특히 [Schuhmann et al. 2021]과 같은 웹 규모 데이터에서 훈련된 대규모 모델[Balaji et al. 2022; Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022]에 의해 주도되었습니다. 저희의 접근 방식은 이러한 사전 훈련된 모델을 기반으로 어휘를 확장하고 개인화된 개념을 생성합니다. 구체적으로, 저희는 Stable-Diffusion 모델을 사용합니다[Rombach et al. 2022]. 저희는 이것이 유사한 주의 기반 아키텍처를 가진 확산 기반 생성기로 일반화될 것으로 예상합니다[Saharia et al. 2022]. 텍스트 기반 이미지 편집. CLIP 기반[Radford et al. 2021] 편집의 성공에 따라
--- METHOD ---
개인화된 개념에 대한 전문 데이터 세트나 사전 정보가 필요하지 않습니다. 우리는 예측된 임베딩을 잠재 공간의 편집 가능한 영역에 가깝게 유지하면서 타겟 개념 특성에 대한 높은 충실도를 유지하기 위해 새로운 대조 기반 정규화 기술을 도입합니다. 예측된 토큰을 가장 가까운 기존 CLIP 토큰으로 푸시합니다. 우리의
--- EXPERIMENT ---
모든 결과는 우리 접근 방식의 효과를 입증하고 학습된 토큰이 비정규화된 모델에서 예측한 토큰보다 더 의미론적임을 보여줍니다. 이를 통해 최첨단 성능을 달성하는 동시에 이전 방법보다 더 유연한 더 나은 표현이 가능합니다. 1. 서론 생성 모델의 급속한 발전은 콘텐츠 생성에 혁명을 일으켜 다양한 아트워크를 손쉽게 생성할 수 있게 했습니다. *작업은 NVIDIA에서 인턴십을 하는 동안 수행되었습니다. ¹프로젝트 페이지는 https://datencoder.github.io에서 확인할 수 있습니다. 진정한 잠재력의 일부는 개인화에 있으며, 사용자가 고유한 개인 개념에 맞게 출력을 맞춤 설정할 수 있습니다. 모델을 개인화하는 것은 개인 소지품, 기억 또는 자화상의 고유한 특성을 포착하고 나타내도록 모델을 사용자 지정하는 것을 포함합니다. 그러나 초기 개인화 방법[Gal et al. 2022; Ruiz et al. 2022]은 여러 이미지의 가용성에 의존하고 장기적인 최적화가 필요합니다. 효과적인 대안은 개념을 타겟팅하기 위해 예측 모델을 사전 학습하는 것입니다. 이러한 접근 방식은 주어진 원하는 대상 개념을 정확하게 재구성하는 텍스트 임베딩을 예측하도록 인코더를 훈련합니다. 획득한 임베딩을 사용하여 주어진 개념을 묘사하는 장면을 생성할 수 있습니다. 그래도 이러한 방법은 한계에 직면합니다. 첫째, 단일 클래스 도메인에 의존하여 다양한 개념의 롱테일 분포를 포착하는 능력이 제한됩니다. 둘째, 일부 접근 방식은 잘못된 배경 기능을 삭제하면서 대상 개념의 특성을 효과적으로 포착하기 위해 분할 마스크나 다중 뷰 입력과 같은 외부 사전이 필요합니다. 이 작업에서 우리는 E4T[Gal et al. 2023]를 따릅니다. 이는 인코더를 간단한(5-15회 반복) 미세 조정을 위한 초기화 형태로 활용하는 접근 방식입니다. E4T는 각 개별 도메인에 대한 인코더를 훈련하고 추론 시간 조정을 위해 약 70GB의 VRAM이 필요합니다. 우리의 접근 방식은 여러 도메인을 다룰 수 있고 추론 시간 2를 줄일 수 있습니다. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano 메모리 요구 사항. 우리는 인코더를 설계하는 동안 두 가지 목표를 고려합니다. (1) 대상 개념을 편집하는 능력, (2) 대상의 구별 가능한 특성을 충실하게 포착하는 능력. 우리는 생성 모델의 편집 가능한 영역 내에서 단어를 예측하도록 모델을 정규화하여 첫 번째 목표를 달성합니다. 이전의 단일 도메인 방법과 달리 대상 도메인에 대한 대략적인 설명에 의존하지 않습니다. 대신 대조 기반 접근 방식을 사용하여 예측된 임베딩을 단어 임베딩 공간의 의미 있는 영역으로 밀어냅니다. 직관적으로, 예측이 개념 클래스를 의미적으로 설명하는 단어 근처에 있도록 하면 모델의 개념 클래스에 대한 사전 지식을 더 잘 보존할 수 있습니다. 두 번째 목표의 경우, 우리는 더 높은 충실도로 대상 개념의 독특한 특징을 포착하기 위해 하이퍼 네트워크를 도입합니다. 관리 가능한 모델 크기를 보장하기 위해 Hu et al. [2021] 및 Gal et al. [2023]에 설명된 접근 방식에 따라 UNET-denoiser 모델의 가중치에 대한 저랭크 분해를 예측하는 전략을 채택했습니다. 마지막으로, 조인트 임베딩 및 하이퍼네트워크 예측을 사용하여 12개 이하의 최적화 단계가 필요한 정규화된 LoRA 학습 프로세스를 초기화합니다. 중요한 점은 이를 통해 메모리 요구 사항이 약 70GB에서 30GB 미만으로 줄어들고 학습 및 추론 시간이 단축된다는 것입니다. 기존 인코더 및 최적화 기반 접근 방식과 방법을 비교하여 다양한 도메인에서 고품질과 빠른 개인화를 달성할 수 있음을 보여줍니다. 관련 작업 확산 모델을 사용한 텍스트 기반 이미지 생성. 텍스트-이미지 합성은 최근 몇 년 동안 상당한 진전을 이루었으며, 주로 사전 학습된 확산 모델[Ho et al. 2020a]과 특히 대규모 모델[Balaji et al. 2022; Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022]는 [Schuhmann et al. 2021]과 같은 웹 규모 데이터에서 학습되었습니다. 저희의 접근 방식은 이러한 사전 학습된 모델을 기반으로 어휘를 확장하고 개인화된 개념을 생성합니다. 구체적으로, 저희는 Stable-Diffusion 모델을 사용합니다[Rombach et al. 2022]. 저희는 이것이 유사한 주의 기반 아키텍처를 가진 확산 기반 생성기로 일반화될 것으로 예상합니다[Saharia et al. 2022]. 텍스트 기반 이미지 편집. CLIP 기반[Radford et al. 2021] 편집 방법[Bar-Tal et al. 2022; Gal et al. 2021; Michel et al. 2021; Patashnik et al. 2021]의 성공에 따라 최근의 대규모 텍스트-이미지 모델의 힘을 활용하고자 하는 많은 연구가 진행되었습니다[Balaji et al. 2022; Kang et al. 2023; Ramesh et al. 2022; Rombach 등 2022; Saharia 등 2022; Sauer 등 2023]은 텍스트 기반 안내를 사용하여 이미지를 조작합니다.Promptto-Prompt [Hertz 등 2022]는 초기 프롬프트의 주의 마스크를 재사용하여 생성된 이미지를 조작하는 방법을 제안합니다.후속 작업에서 Mokady 등 [2022]은 분류기 없는 안내의 널 조건화 공간으로 인코딩하여 이 접근 방식을 실제 이미지로 확장합니다 [Ho 및 Salimans 2021].Tumanyan 등 [2022]과 Parmar 등 [2023]은 DDIM [Song 등 2020] 기반 재구성을 사용하여 참조 주의 맵 또는 기능을 추출합니다.그런 다음 새로운 프롬프트에서 이미지 구조를 유지하는 데 사용됩니다.다른 사람들은 합성 데이터를 사용하여 명령 안내 이미지 대 이미지 변환 네트워크를 훈련합니다 [Brooks 등 2023] 또는 모델을 조정하여 이미지를 재구성하고 컨디셔닝 공간 워크를 사용하여 이미지를 수정합니다[Kawar et al. 2022]. 이러한 접근 방식은 3D 콘텐츠를 편집하는 데에도 사용할 수 있습니다. 예를 들어 깊이 및 텍스트 유도 확산 모델을 사용하여 모양 텍스처를 수정합니다[Richardson et al. 2023]. 이러한 이미지 편집 접근 방식에서 공통적으로 나타나는 것은 원본 이미지의 콘텐츠를 보존하려는 욕구입니다. 이와 대조적으로, 우리의 방법은 나중에 새로운 장면에서 사용할 개념을 포착하는 것을 목표로 하는 모델 개인화를 다룹니다. 거기서 목표는 이미지에서 주제의 의미와 모양을 배우는 것이지만, 주제의 특정 구조는 아닙니다. 역전. 생성적 적대 신경망(GANS, [Goodfellow et al. 2014])의 맥락에서 역전은 사전 훈련된 생성기를 통과했을 때 특정 이미지를 재생성하는 잠재 표현을 찾는 작업입니다(Xia et al. 2021; Zhu et al. 2016]. 이 방법들은 두 가지 주요 캠프로 나뉩니다. 첫 번째는 최적화 방법으로, 최소한의 재구성 손실로 이미지를 합성하는 코드를 잠재 공간에서 반복적으로 검색합니다[Abdal et al. 2019, 2020; Gu et al. 2020; Zhu et al. 2020a]. 두 번째는 인코더 기반 방법으로, 신경망을 훈련하여 이러한 잠재 표현을 직접 예측합니다[Bai et al. 2022; Parmar et al. 2022; Pidhorskyi et al. 2020; Richardson et al. 2020; Tov et al. 2021; Wang et al. 2022; Zhu et al. 2020b]. 확산 모델을 사용하면 역전 잠재 공간이 나중에 주어진 대상으로 노이즈가 제거되는 초기 노이즈 맵이 될 수 있습니다 [Dhariwal and Nichol 2021; Ramesh et al. 2022; Song et al. 2020]. 더 최근의 작업 라인에서 역전은 주어진 개념의 새로운 이미지를 합성하는 데 사용할 수 있는 컨디셔닝 코드를 찾는 것을 나타내는 데 사용되었습니다 [Gal et al. 2022]. 여기서 목표는 특정 이미지를 재생성하는 것이 아니라 하나 이상의 대상 이미지에 설명된 개념의 의미를 포착하여 나중에 새로운 장면에서 재생성하는 것입니다. 마찬가지로 우리의 접근 방식도 개념을 인코딩하는 것을 목표로 합니다. 개인화. 개인화 방법은 모델을 특정 개별 대상에 맞게 조정하는 것을 목표로 합니다. 종종 목표는 대규모 사전 지식을 최종 사용자와 관련된 고유 정보와 결합하는 것입니다. 여기에는 개인화된 추천 시스템 [Amat et al. 2018; Benhamdi et al. 2017; Cho et al. 2002; Martinez et al. 2009], 연합 학습[Fallah et al. 2020; Jiang et al. 2019; Mansour et al. 2020; Shamsian et al. 2021] 또는 특정 장면이나 개인에 맞춰 조정된 생성 모델 생성[Alaluf et al. 2021; Bau et al. 2019; Cao et al. 2022; Cohen et al. 2022; Dinh et al. 2022; Nitzan et al. 2022; Roich et al. 2021]. 텍스트-이미지 개인화에서 목표는 자연어 프롬프트에 따라 특정 대상 개념의 새로운 이미지를 합성하도록 사전 훈련된 모델을 가르치는 것입니다. 이 분야의 초기 작업에서는 직접적인 최적화 접근 방식을 사용했는데, 개념을 설명하기 위해 텍스트 임베딩 세트를 조정하거나[Gal et al. 2022; Voynov et al. 2023], 노이즈 제거 네트워크 자체를 수정하거나[Han et al. 2023; Ruiz et al. 2022], 또는 두 가지를 혼합하는 방식[sim 2023; Kumari et al. 2022; Tewel et al. 2023]이었습니다. 그러나 이러한 최적화 기반 접근 방식은 긴 학습 세션이 필요하며, 일반적으로 모든 개념에 대해 수십 분이 소요됩니다. 최근에는 인코더 기반 접근 방식이 등장했습니다[Gal et al. 2023; Li et al. 2023; Shi et al. 2023; Wei et al. 2023; Zhou et al. 2023]. 이는 신경망을 학습시켜 개념의 새로운 이미지를 합성하기 위해 네트워크에 주입할 수 있는 잠재적 표현을 예측합니다. 이러한 접근 방식은 피험자별 분할 마스크[Wei et al. 2023] 또는 단일 도메인 학습을 사용하여 모델을 정규화하고 단일 이미지에서 타겟을 추론하도록 할 수 있습니다 [Gal et al. 2023; Shi et al. 2023]. 대안적인 접근 방식에서는 모델을 학습할 수 있습니다 타겟 개념 - I 텍스트-이미지 모델의 빠른 개인화를 위한 도메인 독립적 튜닝 인코더 하이퍼네트워크 저랭크 가중치 업데이트 UNET 노이즈 제거기 E4T 백본 사전 보존 이중 경로 적응 LORA + [S*]의 사진 교차 주의 교차 주의 LORA -&gt; 자기 주의 자기 주의 토큰 임베더 [v*] [S*]의 사진 임베딩 정규화 [S*] 최근접 이웃 조회 말의 사진 CLIP 토큰 실제 미래 물고기 고양이 포인트 hr 말 ml 개 낮잠 월드컵 해안 베리 대조적 손실 말 그림 2. 방법 개요. (위) 저희의 방법은 E4T 접근법을 따르고 개념 이미지의 CLIP-특징과 현재 노이즈 세대의 디노이저 기반 특징을 혼합하여 사용하는 특징 추출 백본으로 구성됩니다. 이러한 특징은 임베딩 예측 헤드와 LoRA 스타일의 어텐션 가중치 오프셋을 예측하는 하이퍼네트워크에 공급됩니다. (아래, 오른쪽) 저희의 임베딩은 실제 단어 쪽으로 밀어내지만 다른 개념의 임베딩에서 멀어지는 최근접 이웃 기반 대조 손실을 사용하여 정규화됩니다. (아래, 왼쪽) 저희는 각 어텐션 브랜치를 두 번 반복하는 이중 경로 적응 접근법을 사용합니다. 한 번은 소프트 임베딩과 하이퍼네트워크 오프셋을 사용하고, 한 번은 바닐라 모델과 임베딩의 최근접 이웃을 포함하는 하드 프롬프트를 사용합니다. 이러한 브랜치는 사전을 더 잘 보존하기 위해 선형적으로 혼합됩니다. 이중 조건에서 새로운 이미지를 합성합니다. 텍스트 프롬프트와 대상을 묘사하는 이미지 세트[Chen et al. 2023]. 그러나 이 접근 방식은 50만 개의 사전 훈련된 개인화된 모델의 출력에 대해 모델을 훈련하는 견습 학습을 기반으로 합니다.따라서 이러한 접근 방식은 약 14 A100 GPU년이 필요하므로 대부분의 실무자에게는 실행 불가능합니다.저희 방법은 인코더 기반 접근 방식을 따르지만 분할 마스크나 추가 레이블을 사용하지 않고 단일 도메인을 넘어 확장합니다.또한 이전 인코더 기반 튜닝 접근 방식[Gal et al. 2023]과 비교할 때 튜닝 단계가 더 빠르고 메모리 오버헤드가 줄었습니다.예비 저희의 기여를 맥락에 맞게 설명하기 위해 저희는 작업의 기초가 되는 두 가지 최근의 텍스트-이미지 개인화 접근 방식인 텍스트 역전[Gal et al. 2022]과 E4T[Gal et al. 2023]에 대한 개요부터 시작합니다. 3.1 텍스트 역전 텍스트 역전(TI)은 텍스트-이미지(T2I) 개인화 주제를 소개했습니다. 여기서 사전 훈련된 T2I 확산 모델은 훈련 중에 보이지 않았던 고유한 사용자 제공 개념에 대해 추론하는 방법을 배웁니다. TI에서 저자는 작은(3-5) 이미지 세트에서 시각화된 개념을 나타내는 새로운 단어 임베딩 v*를 학습하여 이 작업을 해결하는 것을 제안합니다. 이러한 임베딩을 찾기 위해 저자는 간단한 확산 잡음 제거 손실을 활용합니다[Ho et al. 2020b]: LDiffusion == € &#39;z,y,e~N(0,1),t | || — €0 (zt, t, y)||}], · Ez,y, (1) 여기서 Є는 스케일링되지 않은 노이즈 샘플이고, eo는 노이즈 제거 네트워크이며, t는 시간 단계이고, zt는 시간 t에 노이즈가 적용된 이미지 또는 잠재 노이즈이고, c는 임베딩 v*에 매핑된 임의의 문자열 S*를 포함하는 일부 조건 프롬프트입니다. 이 임베딩은 학습되면 향후 프롬프트에서 호출하여(플레이스홀더 S* 포함, 예: &quot;S*의 사진&quot;) 새로운 맥락과 장면에서 개념의 이미지를 생성할 수 있습니다. 3.2 튜닝을 위한 인코더(E4T): TI와 같은 최적화 기반 접근 방식은 대상 개념을 재구성할 수 있지만 수렴하려면 여러 번의 반복이 필요합니다. 실제로 TI로 모델을 개인화하려면 상업용 GPU에서도 일반적으로 수십 분이 필요합니다. 최근에는 신경망을 훈련하여 개념의 이미지를 새로운 임베딩에 직접 매핑하는 인코더 기반 접근 방식이 등장했습니다. 보다 구체적으로, 4. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano 개념을 묘사하는 입력 이미지 Ic가 주어지면, 인코더 E는 적합한 임베딩을 예측하도록 훈련됩니다: ʊ* = E(I; 0). 이 인코더는 방정식 (1)의 동일한 노이즈 제거 목표를 사용하여 많은 이미지 집합에서 사전 훈련될 수 있으며, 이를 통해 나중에 새로운 개념으로 일반화할 수 있습니다. E4T에서 이 인코더는 단일 대상 도메인(예: 인간 얼굴, 고양이 또는 예술적 스타일)에서 사전 훈련됩니다. 그러나 과적합을 방지하고 편집 가능성을 유지하기 위해 단일 도메인을 설명하는 단어(예: &quot;얼굴&quot;, &quot;고양이&quot; 또는 &quot;예술&quot;)의 임베딩에 가까운 영역으로 제한하여 예측된 임베딩을 정규화합니다. 이 정규화는 신원 보존의 비용으로 이루어지며, 저자는 나중에 개념의 단일 대상 이미지와 몇 초의 교육을 사용하여 추론 시간 튜닝 세션을 통해 이를 복원합니다.저희의 목표는 이 인코더 기반 튜닝 접근 방식을 제한 없는 도메인으로 확장하여 사용자가 대규모 교육 세트가 존재하지 않을 수 있는 희귀 개념에 대해서도 모델을 빠르게 개인화할 수 있도록 하는 것입니다.방법 4.아키텍처 디자인 반복적 세분화 디자인을 특징으로 하는 E4T 아키텍처를 채택합니다.특히 사전 학습된 CLIP [Radford et al. 2021] ViT-H-14 비주얼 인코더와 StableDiffusion의 UNET-Encoder를 기능 추출 백본으로 활용합니다.각 백본의 마지막 레이어에서 주어진 입력 이미지의 공간적 기능을 추출합니다.E4T에 따라 UNET-Encoder에서 기능을 추출할 때 빈 프롬프트를 제공합니다.기능은 합성곱 기반 네트워크에서 처리되고 토큰 임베더와 하이퍼네트워크라는 두 개의 예측 헤드 간에 공유됩니다. 토큰 임베더는 타겟 개념 Ic를 표현하는 데 사용될 단어 임베딩을 예측합니다. 하이퍼네트워크는 안정적 확산의 잡음 제거 UNET에 대한 가중치 변조를 예측합니다. 다음으로 각 예측 헤드에 대한 몇 가지 중요한 측면에 대해 논의합니다. 하이퍼네트워크: 토큰 임베딩만 사용하여 타겟 개념의 세부 사항을 포착하는 것은 어렵습니다. 이전 연구에 따르면 잡음 제거기 가중치의 하위 집합을 변조하면 모델의 사전 모델에 약간의 피해를 입히면서 재구성 품질을 개선할 수 있습니다. 따라서 더 나은 정체성 보존을 위해 잡음 제거기를 조정하는 데 도움이 되는 가중치 변조 집합을 예측하고자 합니다. 또한 약 10억 개의 매개변수로 구성된 안정적 확산[Rombach et al. 2022]을 사용합니다. 하이퍼네트워크를 사용하여 그렇게 많은 가중치를 조정하는 것은 계산적으로 불가능합니다. 따라서 기존 기술[sim 2023; Gal et al. 2023; Kumari et al. 2022] 및 Stable Diffusion 레이어의 하위 집합, 특히 어텐션 프로젝션 행렬에 대한 변조를 예측하는 데 중점을 둡니다. 그러나 Stable Diffusion에는 이러한 행렬이 96개 포함되어 있으며 각각 평균 715,946개의 매개변수가 포함되어 있습니다. 이처럼 큰 행렬을 예측하는 것은 여전히 어렵습니다. 대신, 저희는 Low-Rank Adaptation(LORA) [Hu et al. 2021]과 동일한 형태의 분해된 가중치를 예측합니다. 여기서 각 가중치 WЄRDin Dout은 학습 가능한 순위 분해 행렬을 주입하여 변조됩니다. 보다 구체적으로, 각 개념 Ic와 각 프로젝션 행렬 W에 대해 두 행렬 A = R Din xr과 BE RrxDout을 예측합니다. 여기서 r은 분해 순위입니다. 새로운 변조된 행렬은 다음과 같습니다.w&#39; = W+AW = W+AX B (2) 학습 시작 시 모델이 손상되는 것을 방지하기 위해 행렬 B의 예측 계층을 0으로 초기화하고 [Hu et al. 2021]에 따라 AW를 상수 인자로 조정합니다.L2-정규화를 적용하여 가중치 오프셋을 추가로 정규화합니다.4.2 임베딩 정규화 대규모 언어 모델은 토큰으로 구성된 유한 사전에서 학습합니다.특히 이러한 모델은 사전의 토큰 시퀀스로 단어를 나누어 처리한 다음 적절한 임베딩 {T;}}}_₁으로 변환합니다.이 토큰화 프로세스에서 각 단어는 하나 이상의 고차원 벡터에 매핑되고 이는 변환기 기반 모델의 입력으로 사용됩니다.인코더의 목표는 대상 개념 Ic를 가장 잘 설명하는 임베딩 v* = E(IC)를 예측하는 것입니다.이전 작업 [Gal et al. 2023]은 제약이 부족한 설정에서 인코더가 분포 밖 임베딩을 사용하는 경향이 있음을 보여주었습니다. 이는 다른 단어에서 주의를 돌리는 경향이 있으며 [Tewel et al. 2023], 나중에 새로운 프롬프트를 통해 개인화된 개념을 조작하는 능력을 제한합니다. 이러한 주의 과적합을 방지하기 위해 기존 토큰 임베딩을 사용하여 Ic를 설명할 수 있습니다. 이러한 토큰은 훈련 분포 내에 있고 따라서 편집 가능하지만 개인적 개념을 포착하기에 충분히 표현력이 없습니다. 따라서 이러한 하드 제약을 완화하고 기존 토큰에 가까운 임베딩을 예측합니다. 직관적으로 의미적으로 관련된 단어 근처에 E(Ic)를 제약하면 재구성과 편집 간의 균형을 이룹니다. 그러나 도메인에 대한 대략적인 설명이 존재하고 사전에 알려진 단일 도메인 인코더와 달리, 우리의 설정에서는 훈련 데이터에서 서로 다른 개념을 설명하는 의미적으로 다른 단어가 많이 있을 수 있습니다. 게다가 추론 중에 마주치는 도메인은 훈련에서 관찰된 도메인과 다를 수 있습니다. [Huang et al. 2023; Miech et al. 2020]에서 영감을 얻어, 우리는 이중 목표를 가진 &quot;최근접 이웃&quot; 대조 학습 목표를 활용합니다: (1) 예측된 임베딩을 가장 가까운 CLIP 토큰에 가깝게 밀어넣고, (2) 다른 개념 이미지를 다른 임베딩에 매핑합니다. 구체적으로, * = E(Ic)일 때, 코사인 거리 메트릭의 관점에서 v✶에 가장 가까운 CLIP 토큰 집합인 N(0*)을 찾습니다. 이러한 CLIP 토큰 T; € N(*)은 대조 손실에서 양의 예로 사용됩니다. 현재 미니 배치의 다른 모든 이미지 l&#39; Ic에 대해 임베딩 v&#39; E(I&#39;)를 음의 샘플로 사용합니다. 따라서 손실은 다음과 같이 정의됩니다: Lc(*) = -log = . (3) ΣN(v) exp (v Ti/t) ΣN(v) exp (v* Ti/t) + Σv′±v exp (v* · V′/T) 이전 방법[Huang et al. 2023]과 달리, 최근접 이웃 임베딩을 양의 샘플로 사용하면 대상 도메인에 대한 감독이나 사전 지식이 필요하지 않아 사전에 정의된 양수 및 음수 토큰 목록이 필요하지 않습니다. 마지막으로, 임베딩의 규범이 크게 증가하는 것을 방지하기 위해 L2-정규화 항을 추가로 사용합니다.LL2 (0*) = ||0||(4) 텍스트-이미지 모델의 빠른 개인화를 위한 도메인 독립적인 튜닝 인코더 텍스트 반전 입력 프롬프트(Few-shot) DreamBooth(Few-shot) LORA(One-shot) (Few-shot) ELITE(seg Mask 포함) Ours(One-shot)[S] 테마 백팩 [S] 모네 스타일 [S] 픽사 렌더링 [S] 벡터 아트 [S] 후지산 근처 [S] 우키요에 그림 [S*] 모자를 쓴 [S.] 모자를 쓴 [S.] [S] 만화 그림 www BOLL Ban akan CA 그림 3. 기존 방법과의 정성적 비교.우리의 방법은 단 하나의 이미지와 12개 이하의 학습 단계를 사용하여 최첨단 기술과 비슷한 품질을 달성합니다. 특히, 최근 인코더 기반 방법에서는 어려움을 겪는 고유한 객체로 일반화됩니다.6. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano 4.3 하이퍼 가중치 정규화 인코더에는 또한 예측된 가중치가 주어진 이미지에 모델을 과대적합시킬 수 있는 하이퍼네트워크 분기가 포함되어 있습니다[Gal et al. 2023]. 하이퍼네트워크 예측으로 인한 과대적합 문제를 해결하기 위해 UNET 순방향 패스에 대한 수정을 제안합니다.먼저 각 블록을 두 개의 사본으로 복제합니다.첫 번째 블록은 원래 UNET의 가중치를 사용하고 두 번째 블록의 경우 하이퍼네트워크 변조된 가중치를 사용합니다.또한 첫 번째(원래 가중치) 분기에서 예측된 단어 임베딩을 가장 가까운 이웃 토큰의 단어 임베딩으로 대체합니다.그런 다음 두 경로의 출력은 ablend 계수로 선형 혼합됩니다. 이 듀얼 콜 접근법은 한 경로가 주의 과적합에서 자유로움을 보장하고, 따라서 정체성을 포착하고 모델의 사전 지식을 보존하는 것 사이의 균형을 이룰 수 있습니다(그림 2 참조). 구체적으로, 가중치 변조 WA와 인코더 E에서 예측된 단어 임베딩 v*가 주어지면, 먼저 모델의 예측 v*에 가장 가까운 하드 토큰 임베딩 o̟을 식별합니다. 그런 다음 각각 U*와 h로 구성된 두 개의 텍스트 프롬프트 C와 Ch를 구성합니다. 즉, C와 Ch는 동일한 프롬프트에서 파생되지만, 하나는 학습된 임베딩을 사용하는 반면 다른 하나는 실제 토큰(&quot;하드&quot;) 임베딩만 사용합니다. 영어: UNET-denoiser의 각 블록 B에 대해, 피처 맵 f = RkxkxD, 텍스트 조건 C 및 가중치 변조 Wд를 수신하고, 이중 경로 접근 방식을 사용하여 블록을 수정합니다.out = αblend · B(f, C, W^) + (1 - αblend) · B(f, Ch, 0) (5) 추론 튜닝 단계: 추론 시간 튜닝 단계에서, 우리는 단일 전방 패스를 사용하여 텍스트-이미지 모델 적응을 위한 하이퍼 가중치 및 워드 임베딩의 초기 예측을 얻었습니다.그런 다음, 우리는 lr=2e-3의 학습 속도와 ablend = 0의 밸런싱 인자를 사용하여 초기 예측을 최적화했습니다.(식 5 참조). 우리는 LORA-PTI에 권장되는 2,000과 비교하여 최대 12개의 최적화 단계가 다양한 개념에 대해 만족스러운 결과를 얻기에 충분하다는 것을 발견했습니다[sim 2023; Roich et al. 2021]. 평가 지표: TI [Gal et al. 2022]를 따르고 CLIP 텍스트-이미지 유사도 점수를 평가 지표로 사용하여 생성된 이미지와 입력 프롬프트의 근접성을 평가합니다. 동일성 보존을 측정하기 위해 단일 이미지 학습 세트와 생성된 결과 간의 이미지-이미지 CLIP 유사도 손실을 활용했습니다. 보고된 모든 지표는 사전 학습된 ViT-B-16 모델을 기반으로 합니다. 평가 세트에는 이전 작업에서 가져온 17개의 이미지가 포함되어 있습니다 [Gal et al. 2022; Kumari et al. 2022; Ruiz et al. 2022]. 여기에는 반려동물(예: 개)부터 개인 물품(예: 백팩) 및 건물에 이르기까지 다양한 범주가 포함됩니다. 5.2 Reg L2가 없는 대조 정규화의 중요성 reg 전용 NN-reg 대조적 Reg [S*]의 얼음 조각 4. 추론 시간 개인화 마지막 단계로 E4T를 따르고 추론 시간에 짧은 튜닝 단계를 사용합니다. E4T는 추론 시점에 모델과 인코더를 모두 조정하지만, 이 프로세스에는 상당한 메모리(권장 최소 배치 크기 16으로 약 70GB)가 필요하다는 것을 알게 되었습니다.이 요구 사항을 줄이기 위해, 우리 모델은 LoRA에서 사용하는 것과 동일한 임베딩 및 가중치 분해를 예측한다는 점에 유의합니다[sim 2023; Hu et al. 2021].따라서 원래 인코더 예측에 가중치와 임베딩을 가깝게 유지하는 것을 목표로 하는 L2-정규화 항을 추가하여 짧은 LoRA 튜닝 실행을 위한 초기화로 출력을 사용할 수 있습니다.실험 5.1 실험 설정 사전 학습: ImageNet-1K 및 Open-Images 데이터 세트에서 모델을 사전 학습하여 실험을 시작했습니다[Kuznetsova et al. 2020; Russakovsky et al. 2015].ImageNet-1K는 1,000개의 고유한 클래스에서 가져온 100만 개의 학습 이미지로 구성되어 있습니다. OpenImages 데이터 세트의 경우, 여러 피험자를 동시에 학습하는 것을 피하기 위해 각 학습 이미지에서 가장 큰 객체를 잘라냅니다. 학습 데이터는 모두 약 300만 개의 이미지로 구성됩니다. 사전 학습 단계에서는 ViT-H-14 인코더를 백본 아키텍처로 사용하는 사전 학습된 CLIP 모델을 사용했습니다. 토큰 임베더와 하이퍼 네트워크는 선형 워밍업 및 코사인 감쇠 스케줄링과 함께 lr=1e-4의 학습 속도를 사용하여 학습했습니다. 절제 목적으로 학습하는 동안 50,000번의 반복을 수행했습니다. 최종 모델과 기존 기술과의 비교를 위해 학습을 150,000단계로 확장했습니다. 입력 글래디에이터 영화의 [S] 사진 그림 4. 임베딩 정규화를 제거하거나 변경한 효과. 정규화를 제거하면 과적합 또는 모드 붕괴가 발생하여 결과의 품질이 좋지 않습니다. 순진한 정규화는 개념 세부 정보를 보존하는 데 어려움을 겪는 경향이 있습니다. 대조 기반 정규화는 두 가지 사이에서 균형을 이룰 수 있습니다. 우리의 접근 방식은 예측 임베딩의 품질을 개선하기 위해 대조 학습을 활용합니다. 이 정규화의 이점을 시각화하기 위해 우리는 네 가지 설정에서 모델을 학습합니다. 첫째, 정규화 없이. 둘째, 예측 임베딩의 Lloss를 제외한 모든 정규화를 생략합니다. 셋째, 우리는 대조 손실을 예측 임베딩과 가장 가까운 이웃 간의 코사인 거리를 최소화하는 손실로 대체합니다. 이 손실은 VQGAN에서 사용된 코드북 손실에서 영감을 얻었습니다[Esser et al. 2021]. 마지막으로, 우리는 제안한 대조 기반 대안을 사용합니다. 그림 4에서 볼 수 있듯이, 대조 기반 손실을 통합하면 결과가 개선됩니다. 특히, 모든 정규화를 생략하면 입력 이미지에 과대적합되는 경향이 있습니다. 예를 들어, 생성된 이미지인 &quot;[S*]의 검투사 영화 사진&quot;에서 검투사라는 단어가 간과됩니다. 그리고 모델은 예측 토큰에 과대적합합니다. 다른 한편, 텍스트-이미지 모델의 빠른 개인화를 위한 ID 유사성 도메인 독립 튜닝 인코더0.8500.840.820.8250.0.Ours(원샷) LORA(원샷) LORA(few-shot) 0.0.74• DB(원샷) ID 유사성 0.8000.7750.Ours w/o finetuning 0.DB(few-shot) w/o dual-path w/o dual-path and tuning TI(원샷) 0.. w/o hyper-network TI(few-shot) ● 단어 임베딩만 튜닝 하이퍼 가중치만 튜닝 0.T 0.0.240 0.0.0.255 0.260 0.265 0.Prompt-Similarity 0.0.0.0.0.0.Prompt-Similarity(a) (b) 그림 5. 양적 평가 결과. (a) 이전 연구와의 비교. 우리의 방법은 최적화 기반 방법보다 수십 배 더 빠르면서도 동일성 프롬프트 유사성 트레이드오프 곡선에서 매력적인 지점을 제시합니다. (b) 절제 연구 결과. 정규화를 제거하면 일반적으로 편집성이 떨어지는 빠른 과적합이 발생합니다. 미세 조정 단계를 건너뛰면 E4T [Gal et al. 2023]에 따라 동일성 보존에 해를 끼칩니다. 반면, 대조 손실을 사용하면 생성된 사진은 대상 개념(예: 말)의 특징을 보존하면서 입력 프롬프트를 충실하게 설명합니다. 대조 손실 함수는 또한 음수 샘플을 통해 다른 이미지의 토큰을 밀어내어 모드 붕괴를 방지하는 데 도움이 됩니다. 예를 들어, 대조 기반 방법과 달리 최근접 이웃 접근 방식은 모드 붕괴를 해결하지 않습니다. 덜 유리한 결과를 얻습니다(그림 4 참조). 5.3 기존 방법과의 비교 우리는 단일 이미지와 일부 학습 단계를 사용하여 놀라운 수준의 세부 정보를 캡처하는 방법의 능력을 보여주는 정성적 분석으로 평가를 시작합니다.그림 3은 다양한 접근 방식을 비교하여 다중 도메인 개인화의 결과를 보여줍니다.특히, 우리는 우리의 방법을 Textual-Inversion [Gal et al. 2022], Dream-Booth [Ruiz et al. 2022] 및 Stable Diffusion을 위한 대중적인 공개 LoRA 라이브러리 [sim 2023]와 비교합니다.또한 우리의 방법을 최첨단 다중 도메인 개인화 인코더인 ELITE [Wei et al. 2023]와 비교합니다.DreamBooth 및 Textual Inversion의 경우 HuggingFace Diffusers 구현 [Patil and Cuenca 2022]을 사용합니다. 우리의 결과는 전체 튜닝 기반 방법(DreamBooth, LoRA)과 동등하며, 후자가 분할 마스크의 형태로 추가 감독에 액세스할 수 있음에도 불구하고 ELITE의 순수 인코더 기반 접근 방식보다 상당히 우수한 성과를 보입니다. 주목할 점은 모든 튜닝 기반 방법은 대상 개념의 여러 이미지에 액세스해야 하는 반면, 우리의 접근 방식은 단일 입력만 사용합니다. 우리의 방법을 사용하여 생성된 추가 결과는 그림 6에서 찾을 수 있습니다. 다음으로, 우리의 방법을 CLIP 기반 메트릭을 사용하는 튜닝 기반 접근 방식과 비교합니다. 결과는 그림 5a에 나와 있습니다. 우리의 방법은 LoRA보다 더 나은 신원 보존 및 편집 가능성을 달성하지만 DreamBooth와 비교할 때 상충 관계를 보입니다. 주목할 점은 단일 이미지만 사용하여 학습했을 때 모든 베이스라인보다 우수한 성과를 보입니다. 전반적으로 우리의 접근 방식은 단일 이미지와 12번의 튜닝 반복만 사용하면서도 최첨단 기술과 경쟁력이 있습니다. 5. 절제 분석 우리는 우리의 방법에서 각 구성 요소의 중요성을 더 잘 이해하기 위해 절제 연구를 수행합니다. 우리는 다음과 같은 설정을 검토합니다. 이중 경로 정규화 접근 방식 제거, 미세 조정 단계 건너뛰기, 하이퍼네트워크 분기 생략. 우리는 최종 조정 단계가 E4T의 관찰과 일치하여 매우 중요하다는 것을 관찰합니다. 특히 미세 조정 없이 기준선을 사용할 때 객체 유사도 메트릭이 20% 감소하는 것을 목격합니다. 조정 중에 이중 경로를 끄면 프롬프트-이미지 정렬이 거의 30% 손상되어 심각한 과적합을 시사합니다. 따라서 이중 경로 접근 방식이 사전을 성공적으로 보존하고 과적합을 줄일 수 있다는 결론을 내릴 수 있습니다. 우리 방법의 또 다른 중요한 구성 요소는 생성기를 대상 개념으로 교정하기 위해 가중치 변조를 예측하는 하이퍼네트워크입니다. 우리의 절제 연구에서 우리는 훈련 시간에 하이퍼네트워크를 생략하면 생성된 이미지와 텍스트 프롬프트의 정렬에 부정적인 영향을 미친다는 것을 발견했습니다. 우리는 이것이 네트워크가 단어 임베딩에서 객체에 대한 더 많은 정보를 인코딩해야 하기 때문에 방법 섹션에서 설명한 대로 주의 과적합이 발생하기 때문이라고 생각합니다.제한 사항 우리의 접근 방식은 기존 튜닝 인코더를 다중 클래스 도메인으로 확장할 수 있지만 여전히 훈련 데이터에 의해 제한됩니다.따라서 데이터 세트에서 제대로 표현되지 않은 도메인은 인코딩하기 어려울 수 있습니다.따라서 ImageNet에서 훈련된 모델은 복잡한 장면이나 인간 얼굴에 어려움을 겪을 수 있습니다.우리는 LAION [Schuhmann et al. 2021]과 같은 보다 일반적이고 대규모 데이터 세트에서 훈련하면 이러한 제한을 극복할 수 있다고 믿습니다.그러나 이러한 조사는 우리의 자원을 벗어납니다.우리의 방법은 보다 일반적인 도메인에서 작동할 수 있지만 여전히 다운스트림 유사성을 높이기 위한 튜닝 단계가 필요합니다. 그러나 8. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir 및 Amit H. Bermano는 이러한 튜닝 접근 방식에 필요한 메모리 요구 사항과 반복 횟수가 감소함에 따라 합성에 필요한 시간에 비해 무시할 수 있게 됩니다.
--- CONCLUSION ---
우리는 단일 클래스 도메인을 넘어 튜닝 인코더 접근 방식을 일반화하는 방법을 제시했습니다. 우리의 접근 방식은 예측된 임베딩이 실제 단어 도메인에 가깝게 위치하도록 하고, 네트워크가 하드 프롬프트와 소프트 프롬프트의 예측을 혼합하는 듀얼 패스 접근 방식을 활용하여 과적합을 제한합니다. 이를 통해 추론 시간에 모델을 빠르게 개인화하여 최적화 기반 접근 방식에 비해 두 자릿수만큼 개인화 속도를 높일 수 있습니다. 앞으로는 튜닝 요구 사항을 더욱 줄여 소비자 등급 GPU에서 방법을 사용하여 최종 사용자가 자신의 머신에서 모델을 빠르게 개인화할 수 있기를 바랍니다. 감사의 말 첫 번째 저자는 Miriam과 Aaron Gutwirth 장학금의 지원을 받았습니다. 이 작업은 Len Blavatnik과 Blavatnik 가족 재단, Deutsch Foundation, Yandex Initiative in Machine Learning, BSF(보조금 2020280) 및 ISF(보조금 2492/20 및 3441/21)의 지원을 받았습니다. 참고문헌 2023. 빠른 텍스트-이미지 확산 미세 조정을 위한 저순위 적응. Rameen Abdal, Yipeng Qin, Peter Wonka. 2019. Image2stylegan: 이미지를 stylegan 잠재 공간에 임베드하는 방법?. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 진행 중. 4432-4441. Rameen Abdal, Yipeng Qin, Peter Wonka. 2020. Image2stylegan++: 임베드된 이미지를 편집하는 방법?. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 8296-8305. Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano. 2021. HyperStyle: 실제 이미지 편집을 위한 HyperNetworks를 사용한 StyleGAN 역전. arXiv:2111.15666 [cs.CV] Fernando Amat, Ashok Chandrashekar, Tony Jebara, Justin Basilico. 2018. Netflix에서의 아트워크 개인화. 제12회 ACM 추천 시스템 컨퍼런스(캐나다 브리티시 컬럼비아주 밴쿠버)(RecSys &#39;18)의 회의록. Association for Computing Machinery, 뉴욕, 뉴욕, 미국, 487-488. https://doi.org/10.1145/ 3240323. Qingyan Bai, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen. 2022. 패딩 공간을 사용한 고충실도 GAN 역전. 유럽 컴퓨터 비전 컨퍼런스에서. Springer, 36-53. Yogesh Balaji, 나승준, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro 등 2022. ediffi: 전문 노이즈 제거기 앙상블을 갖춘 텍스트-이미지 확산 모델. arXiv 사전 인쇄 arXiv:2211.01324 (2022). Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten 및 Tali Dekel. 2022. Text2LIVE: 텍스트 기반 레이어 이미지 및 비디오 편집. arXiv 사전 인쇄 arXiv:2204.02491 (2022). David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu 및 Antonio Torralba. 2019. 생성적 이미지 사전을 사용한 의미적 사진 조작. 38, 4(2019). https://doi.org/10.1145/3306346.Soulef Benhamdi, Abdesselam Babouri, Raja Chiky. 2017. e러닝 환경을 위한 개인화된 추천 시스템. 교육 및 정보 기술 22, 4(2017), 1455-1477. Tim Brooks, Aleksander Holynski, Alexei A. Efros. 2023. InstructPix2Pix: 이미지 편집 지침을 따르는 법 배우기. CVPR에서. Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shunsuke Saito, Stephen Lombardi, Shih-en Wei, Danielle Belko, Shoou-i Yu, Yaser Sheikh, Jason Saragih. 2022. 전화 스캔을 통한 진정한 체적 아바타. ACM Trans. Graph. (2022). Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, William W. Cohen. 2023. 견습 학습을 통한 주제 중심 텍스트-이미지 생성. ArXiv abs/2304.00186 (2023). Yoon Ho Cho, Jae Kyeong Kim, Soung Hie Kim. 2002. 웹 사용 마이닝 및 의사 결정 트리 유도를 기반으로 한 개인화된 추천 시스템. Expert systems with Applications 23, 3 (2002), 329-342. Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, Yuval Atzmon. 2022. &quot;This is my unicorn, Fluffy&quot;: 동결된 시각-언어 표현 개인화. 유럽 컴퓨터 비전 컨퍼런스(ECCV)에서. Prafulla Dhariwal과 Alexander Nichol. 2021. 확산 모델이 이미지 합성에서 gans를 이긴다. 신경 정보 처리 시스템의 발전 34(2021), 8780-8794. Tan M Dinh, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua. 2022. 하이퍼인버터: 하이퍼네트워크를 통한 stylegan 역전 개선. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 11389-11398. Patrick Esser, Robin Rombach, Björn Ommer. 2021. 고해상도 이미지 합성을 위한 변압기 길들이기. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2021, 가상, 2021년 6월 19-25일. Computer Vision Foundation/IEEE, 12873-12883. https://doi.org/10.1109/CVPR46437.2021.Alireza Fallah, Aryan Mokhtari, Asuman Ozdaglar. 2020. 개인화된 연합 학습: 메타 학습 접근 방식. arXiv 사전 인쇄본 arXiv:2002.07948(2020). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, Daniel Cohen-Or. 2022. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618(2022). Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2023. Designing an encoding for fast personalization of text-to-image models. arXiv 사전 인쇄본 arXiv:2302.12228 (2023). Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. 2021. Stylegan-nada: Clip-guided domain adaptation of image generators. arXiv 사전 인쇄본 arXiv:2108.00946 (2021). Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014). Jinjin Gu, Yujun Shen, and Bolei Zhou. 2020. 다중 코드 GAN Prior를 사용한 이미지 처리. arXiv:1912.07116 [cs.CV] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. 2023. SVDiff: 확산 미세 조정을 위한 컴팩트 매개변수 공간. arXiv:2303.11305 [cs.CV] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. 교차 어텐션 제어를 사용한 프롬프트 간 이미지 편집. (2022). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020a. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전 33(2020), 6840-6851. Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020b. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전 33(2020), 6840-6851. Jonathan Ho와 Tim Salimans. 2021. 분류자 없는 확산 지침. NeurIPS 2021 워크숍에서 심층 생성 모델 및 다운스트림 애플리케이션. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen. 2021. LORA: 대규모 언어 모델의 저순위 적응. ArXiv abs/2106.09685(2021). Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan, Ziwei Liu. 2023. ReVersion: Diffusion-Based Relation Inversion from Images. arXiv 사전 인쇄본 arXiv:2303.13495(2023). Yihan Jiang, Jakub Konečný, Keith Rush, Sreeram Kannan. 2019. Improving federated learning personalization via model agnostic meta learning. arXiv 사전 인쇄본 arXiv:1909.12488(2019). Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park. 2023. Scaling up GANs for Text-to-Image Synthesis. IEEE Conference on Computer Vision and Pattern Recognition(CVPR)의 논문집. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri 및 Michal Irani. 2022. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. arXiv 사전 인쇄 arXiv:2210.09276 (2022). Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman 및 Jun-Yan Zhu. 2022. 텍스트-이미지 확산의 다중 개념 사용자 정의. arXiv 사전 인쇄 arXiv:2212.04488 (2022). Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi PontTuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig 및 Vittorio Ferrari. 2020. Open Images Dataset V4: 대규모 통합 이미지 분류, 객체 감지 및 시각적 관계 감지. IJCV(2020). Dongxu Li, Junnan Li, Steven CH Hoi. 2023. BLIP-Diffusion: 제어 가능한 텍스트-이미지 생성 및 편집을 위한 사전 학습된 주제 표현. arXiv:2305.14720 [cs.CV] Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh. 2020. 연합 학습에 응용할 수 있는 개인화를 위한 세 가지 접근 방식. arXiv 사전 인쇄본 arXiv:2002.10619(2020). Ana Belen Barragans Martinez, Jose J Pazos Arias, Ana Fernandez Vilas, Jorge Garcia Duque, Martin Lopez Nores. 2009. 오늘 밤 TV에 무슨 일이 나올까요? TV 프로그램의 효율적이고 효과적인 개인화된 추천 시스템. IEEE Transactions on Consumer Electronics 55, 1 (2009), 286-294. 텍스트-이미지 모델의 빠른 개인화를 위한 도메인 독립적인 튜닝 인코더 Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka. 2021. Text2Mesh: 메시를 위한 텍스트 기반 신경 스타일화. arXiv 사전 인쇄본 arXiv:2112.03221 (2021). Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, Andrew Zisserman. 2020. 큐레이션되지 않은 교육 비디오에서 시각적 표현의 종단 간 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 9879-9889. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 2022. Nulltext Inversion for Editing Real Images using Guided Diffusion Models. arXiv 사전 인쇄본 arXiv:2211.09794(2022). Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv 사전 인쇄본 arXiv:2112.10741(2021). Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-Or. 2022. MyStyle: 개인화된 생성적 사전. arXiv 사전 인쇄본 arXiv:2203.17272(2022). Gaurav Parmar, Yijun Li, Jingwan Lu, Richard Zhang, Jun-Yan Zhu, and Krishna Kumar Singh. 2022. GAN 반전 및 편집을 위한 공간적 적응형 다층 선택. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 11399-11409. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. 2023. 제로샷 이미지-이미지 변환. arXiv:2302.03027 [cs.CV] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. StyleCLIP: StyleGAN 이미지의 텍스트 기반 조작. arXiv 사전 인쇄본 arXiv:2103.17249(2021). Suraj Patil 및 Pedro Cuenca. 2022. HuggingFace DreamBooth 구현. https: //huggingface.co/docs/diffusers/training/dreambooth. Stanislav Pidhorskyi, Donald A Adjeroh 및 Gianfranco Doretto. 2020. 적대적 잠재 자동 인코더. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 14104-14113. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. arXiv 사전 인쇄본 arXiv:2103.00020(2021). Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125(2022). Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2020. 스타일 인코딩: 이미지-이미지 변환을 위한 StyleGAN 인코더. arXiv 사전 인쇄본 arXiv:2008.00951(2020). Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023. 텍스처: 3D 모양의 텍스트 가이드 텍스처링. arXiv 사전 인쇄본 arXiv:2302.01721(2023). Daniel Roich, Ron Mokady, Amit H Bermano, Daniel Cohen-Or. 2021. 실제 이미지의 잠재 기반 편집을 위한 핵심 튜닝. arXiv 사전 인쇄본 arXiv:2106.05744(2021). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2022, 미국 루이지애나주 뉴올리언스, 2022년 6월 18-24일. IEEE, 10674-10685. https://doi.org/10.1109/CVPR52688.2022.Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. DreamBooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. (2022). Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet 대규모 시각 인식 챌린지. International Journal of Computer Vision (IJCV) 115, 3 (2015), 211-252. https://doi.org/10.1007/s11263-015-0816-y Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. 심층적 언어 이해를 통한 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487(2022). Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Timo Aila. 2023. StyleGANT: 대규모 텍스트-이미지 합성을 위한 GANS의 힘 잠금 해제. 기계 학습 국제 컨퍼런스 abs/2301.09515. https://arxiv.org/abs/ 2301.Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114 (2021). Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. 2021. 하이퍼네트워크를 사용한 개인화된 연합 학습. 국제 기계 학습 컨퍼런스에서. PMLR, 9489-9502. Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2023. Instant Booth: 테스트 시간 미세 조정 없이 개인화된 텍스트-이미지 생성. arXiv:2304.03411 [cs.CV] Jiaming Song, Chenlin Meng, Stefano Ermon. 2020. Denoising Diffusion Implicit Models. 국제 학습 표현 컨퍼런스에서. Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon. 2023. 텍스트-이미지 개인화를 위한 키 잠금 랭크 1 편집. arXiv 사전 인쇄본 arXiv:2305.01644(2023). Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or. 2021. Designing an Encoder for StyleGAN Image Manipulation. arXiv 사전 인쇄본 arXiv:2102.02766(2021). Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel. 2022. 텍스트 기반 이미지-이미지 변환을 위한 플러그 앤 플레이 확산 특징. arXiv 사전 인쇄본 arXiv:2211.12572(2022). Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, Kfir Aberman. 2023. P+: 텍스트-이미지 생성에서 확장된 텍스트 컨디셔닝. arXiv 사전 인쇄본 arXiv:2303.(2023). Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen. 2022. 이미지 속성 편집을 위한 HighFidelity GAN 반전. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록. Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo. 2023. Elite: 사용자 지정 텍스트-이미지 생성을 위한 텍스트 임베딩으로 시각적 개념 인코딩. arXiv 사전 인쇄본 arXiv:2302.13848(2023). Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang. 2021. GAN 역전: 조사. arXiv:2101.05278 [cs.CV] Yufan Zhou, Ruiyi Zhang, Tong Sun, Jinhui Xu. 2023. 사용자 지정 텍스트-이미지 생성을 위한 세부 정보 보존 향상: 정규화 없는 접근 방식. arXiv:2305.13579 [cs.CV] Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou. 2020b. 실제 이미지 편집을 위한 도메인 내 gan 역전. arXiv 사전 인쇄본 arXiv:2004.00049(2020). Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A Efros. 2016. 자연 이미지 매니폴드에 대한 생성적 시각적 조작. 유럽 컴퓨터 비전 컨퍼런스에서. Springer, 597-613. Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka. 2020a. 개선된 StyleGAN 임베딩: 좋은 잠재성은 어디에 있는가? arXiv:2012.09036 [cs.CV]. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano [S*]의 얼음 조각품 [S*]의 벡터 아트 [S] 해변에 있는 [S] 플레이도거로 만든 [S] 어쌔신 크리드의 [S] [S] 후지산 위의 대리석 조각품 [S*]의 만화 그림 [S*]의 추상 미술 [S*]의 유화 그림 [S*]의 아크릴 그림 [S] 목탄 그림 [S] 수채화 물감 [S] 몰디브에서 [S] 모네 스타일의 [S] [S*]의 만화 그림 [S] 에펠탑 근처의 모자를 쓴 [S*]가 있는 이집트 그림 [S] 달 위의 [S*] 자수 [S*]의 귀여운 그림 어쌔신 크리드의 [S*] [S]의 우키요에 그림 그림 6. 우리의 방법을 사용하여 생성된 추가적인 정성적 결과. 가장 왼쪽 열에는 입력 이미지가 표시되고, 그 뒤에 각 주제에 대한 개인화된 4개의 세대가 표시됩니다.
