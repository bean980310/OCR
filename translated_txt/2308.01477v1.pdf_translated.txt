--- ABSTRACT ---
우리는 범주 수준 객체 포즈 추정 및 어포던스 예측을 위한 HANDAL 데이터 세트를 제시합니다. 이전 데이터 세트와 달리, 우리의 데이터 세트는 로봇 조작기가 기능적으로 잡을 수 있는 적절한 크기와 모양의 로봇 조작 가능한 객체, 예를 들어 펜치, 주방 도구, 스크루드라이버에 초점을 맞춥니다. 우리의 주석 프로세스는 간소화되어 기성품 카메라 하나와 반자동 처리만 필요하므로 크라우드 소싱 없이도 고품질 3D 주석을 생성할 수 있습니다. 이 데이터 세트는 17개 범주의 212개 실제 객체에 대한 2.2k 비디오에서 주석이 달린 308k 이미지 프레임으로 구성되어 있습니다. 우리는 로봇 조작기가 단순히 밀거나 무차별적으로 잡는 것 이상으로 환경과 상호 작용해야 하는 실제 시나리오에서 연구를 용이하게 하기 위해 하드웨어 및 주방 도구 객체에 초점을 맞춥니다. 우리는 6-DoF 범주 수준 포즈+스케일 추정 및 관련 작업에 대한 데이터 세트의 유용성을 설명합니다. 또한 모든 객체의 3D 재구성 메시를 제공하고, 이와 같은 데이터 세트 수집을 민주화하기 위해 해결해야 할 병목 현상 중 일부를 설명합니다. 프로젝트 웹사이트: https://nvlabs.github.io/HANDAL/ I.
--- INTRODUCTION ---
로봇이 단순한 밀기와 무차별적인 탑다운 움켜잡기를 넘어서려면 3D 주변 환경에 대한 자세한 인식 기능을 갖춰야 합니다[1]. 이를 위해 로봇에 맞춤화된 고품질 3D 데이터 세트가 네트워크의 훈련 및 테스트에 필요합니다. 많은 대규모 2D 컴퓨터 비전 데이터 세트(예: ImageNet[2], COCO[3], OpenImages[4])에 비해 기존 3D 로봇 데이터 세트는 크기와 범위가 다소 작은 경향이 있습니다. 3D의 주요 과제는 실제 이미지에 주석을 달는 것으로, 깊이 또는 여러 이미지가 필요하므로 주석 프로세스를 확장하기 어렵습니다. BOP 챌린지[5]의 데이터 세트가 인스턴스 수준 객체 포즈 추정을 위한 이러한 격차를 메웠지만, 범주 수준 객체 포즈 추정과 작업 지향적 움켜잡기 및 조작을 위한 객체 핸들과 같은 기능적 가능성을 학습하는 과제는 여전히 남아 있습니다[6][10]. 이러한 문제를 해결하기 위해 HANDAL 데이터 세트를 소개합니다. 기성품 카메라를 사용하여 데이터를 수집하고, 3D로 데이터에 주석을 달기 위한 반자동 파이프라인을 사용하여, 파이프라인이 설정된 후 많은 인간의 노력 없이 상당히 큰 레이블이 지정된 데이터 세트를 만들었습니다.그림 1은 분할, 어포던스(핸들), 6-DoF 포즈, 바운딩 박스를 포함한 정밀한 3D 주석이 있는 이미지 프레임의 예를 보여줍니다.우리는 이 데이터 세트를 레이블이 지정된 3D 데이터 세트 생성을 민주화하기 위한 중요한 단계로 봅니다.이를 위해 수집을 위한 특수 하드웨어나 주석을 위한 크라우드소싱을 사용하지 않습니다.우리의 데이터 세트는 다음과 같은 속성을 가지고 있습니다.• 모든 이미지 프레임에 대한 6-DoF 개체 포즈+스케일 주석이 있는 로봇 기능적 파악 및 조작에 적합한 17개 범주의 개체 인스턴스의 짧은 비디오 클립.각 인스턴스를 여러 번 캡처하여 다양한 배경과 조명을 보장합니다.• 작업에서 영감을 받은 어포던스 주석과 함께 모든 개체의 3D 재구성. • 동적 시나리오에서 작업 지향 분석을 용이하게 하기 위해 인간이 다루는 객체의 추가 비디오. 이러한 동적 비디오에는 정확한 기준 진실 주석도 제공됩니다. 논문의 마지막에 우리와 같은 데이터 세트를 만드는 과정에서 남은 병목 현상에 대한 논의가 포함됩니다. II. 이전 작업 대형 객체 데이터 세트. 현재 작업의 선구자에는 KITTI 3D 객체 감지[11] 및 nuScenes[12]와 같은 자율 주행을 위한 데이터 세트가 포함되며, 여기서 감지할 객체는 차량, 자전거 타는 사람 및 보행자입니다. 마찬가지로 PASCAL3D+[13] 및 SUN RGB-D[14]와 같은 데이터 세트는 테이블, 의자, 소파 및 기타 대형 실내 품목의 3D 객체 감지를 가능하게 합니다. 두 경우 모두 객체가 로봇 조작에는 너무 크고, 수직 방향이므로 일반적으로 전체 6-DoF 포즈 추정이 필요하지 않습니다. 범주 수준 객체 포즈 데이터 세트. 작고 조작 가능한 물체의 실제 이미지에 대한 첫 번째 데이터 세트는 NOCSREAL275[15]로, 병, 그릇, 카메라, 캔, 노트북, 머그잔의 여섯 가지 범주로 구성된 소수의 주석이 달린 RGBD 비디오로 구성되어 있습니다. 이 작업을 기반으로 Wild6D[16]는 비디오와 이미지의 수를 확장하지만 병, 그릇, 카메라, 노트북, 머그잔이라는 유사한 범주를 사용합니다. 더 최근의 데이터 세트[17], [18]는 더 작은 규모에서 우리와 동일한 문제를 다룹니다. PhoCaL[17]에는 병, 상자, 캔, 컵, 리모컨, 찻주전자, 칼 붙이, 유리 제품이라는 여덟 가지 범주의 RGBD 비디오가 포함되어 있습니다. HouseCat6D[18]에는 병, 상자, 캔, 컵, 리모컨, 찻주전자, 칼 붙이, 유리 제품, 신발, 튜브라는 유사한 열 가지 범주의 RGBD 비디오가 포함되어 있습니다. 우리 작업과 유사하게 이러한 범주의 물체는 로봇이 조작할 수 있습니다. Objectron [19]은 객체 인스턴스와 비디오의 수를 상당히 확대하지만, 대부분의 범주는 1&quot;기능적 로봇 파악 및 조작을 위한 가정용 주석 데이터 세트&quot;입니다. 모든 객체에 핸들이 있기 때문에 이 이름도 적합합니다. 조작할 수 없습니다. 따라서 이 데이터 세트는 로봇 공학보다 컴퓨터 비전에 더 적합합니다. 마찬가지로 CO3D [20]에는 많은 범주의 비디오가 많이 포함되어 있으며, 이 중 대부분은 조작할 수 없습니다. 그러나 CO3D는 범주 수준의 객체 포즈 추정보다는 범주별 3D 재구성 및 새로운 뷰 합성에 중점을 둔다는 점에서 다릅니다. 결과적으로 이 데이터 세트는 표준 좌표계에 대한 절대적 크기 및 객체 포즈와 같이 후자에 필요한 몇 가지 핵심 품질을 생략합니다. 이러한 작업과 대조적으로, 우리는 조작 가능한 객체, 특히 로봇 기능적 파악에 적합한 객체에 중점을 둡니다. 따라서 우리는 손잡이가 있는 비교적 작은 물체의 데이터를 수집하고, 그들의 어포던스와 6-DoF 포즈+스케일을 주석으로 달았습니다. 3D 데이터 세트와 비교하려면 표 I을 참조하세요.
--- RELATED WORK ---
s. 범주 수준 객체 포즈 추정. 범주 수준 객체 포즈 추정은 최근 주목을 받고 있는 비교적 최근의 문제입니다. 인스턴스 수준 객체 포즈 추정 [5], [21]–[25]의 자연스러운 확장인 범주 수준 포즈 추정 [15], [26]–[29]은 알려진 객체 인스턴스 세트 대신 광범위한 객체 클래스를 다루므로 로봇 응용 프로그램에 더 유용할 수 있습니다. 위에서 언급한 범주 수준 데이터 세트는 이 중요한 분야에서 최근 연구를 가능하게 했습니다. Wang et al. [15]은 획기적인 작업에서 정규화된 객체 좌표계(NOCS)를 도입했으며, 이를 사용하여 단일 RGBD 이미지에서 3D 감지 및 범주 수준 객체 포즈 추정을 수행했습니다. 이
--- METHOD ---
NOCS-REAL275 데이터 세트에서 평가되었습니다. 후속 작업에서 CPS[30]도 REAL275에서 평가하지만 이 방법은 추론 시점에 RGB 이미지만 필요합니다. 이 방법은 레이블이 지정되지 않은 RGBD 이미지를 사용하여 자체 감독 방식으로 학습되었으며 인스턴스의 모양을 저차원 표현으로 추론합니다. DualPoseNet[31] 방법은 구면 융합과 함께 한 쌍의 디코더를 사용하여 단일 RGBD 이미지를 사용하여 이 문제를 해결합니다. 이 방법은 또한 NOCS-REAL275에서 평가되었습니다. SSP-Pose[32], CenterSnap[33], iCaps[34] 및 ShAPO[35]의 최신 접근 방식은 모두 NOCS-REAL275에서 평가됩니다. MobilePose[36]는 Objectron 데이터 세트에서 평가하여 단일 RGB 이미지에서 객체 포즈를 복구하는 것을 목표로 합니다. CenterPose[37]와 추적 확장인 CenterPoseTrack[38]은 convGRU 모듈로 연결된 키포인트와 히트맵의 조합을 사용하여 동일한 문제를 해결합니다.이 두 가지 방법은 Objectron에서도 평가되었습니다.저희가 아는 한, 이전 방법은 REAL275와 Objectron의 두 데이터 세트에서만 평가되었습니다.저희는 기능적 파악을 용이하게 하는 주석과 함께 로봇 조작에 적합한 범주에 초점을 맞춘 가장 큰 데이터 세트를 제공하여 이 연구 분야를 확장하고자 합니다.III. 데이터 세트 개요 이 데이터 세트를 통한 저희의 목표는 로봇 조작자가 간단한 밀기와 픽앤플레이스를 넘어 실제 작업을 수행할 수 있도록 하는 지각 연구를 지원하는 것입니다.표 I: 범주 수준 개체 데이터 세트 비교.자세한 내용은 섹션 III-B를 참조하십시오.여기서는 Wild6D와 저희 데이터 세트의 주석이 달린 이미지만 고려합니다.PhoCal과 HouseCat6D도 편광 카메라를 사용합니다. #CO3D에는 무차별적으로 잡을 수 있지만 기능적 잡기를 허용하지 않는 5개의 추가 범주가 있습니다.*저희 데이터 세트에는 RGBD 카메라의 동적 비디오도 포함되어 있습니다.데이터 세트 모달리티 cat.manip.obj.vid.img.pose scale 3D recon.stat.dyn.360° OCC.fable.NOCS-REAL275 [15] RGBD8k ✓ ✓ PhoCal [17] RGBD+3k HouseCat6D [18] RGBD+24k Wild6D [16] RGBD10k Objectron [19] RGB17k 14k 4M CO3D [20] HANDAL (저희) RGB7+ 19k 19k 1.5M x XXXX RGB*2k 308k 결과적으로 기능적 목적이 있는 객체 범주를 선택하도록 동기를 부여했습니다.특히 기능적 잡기를 용이하게 하는 손잡이가 있는 범주에 초점을 맞추었습니다. A. 물체 범주 우리는 잡을 수 있는 기능적 물체의 17가지 범주를 선택했습니다: 발톱 망치, 고정 조인트 플라이어, 슬립 조인트 플라이어, 잠금 플라이어, 전동 드릴, 래칫, 스크루드라이버, 조절 렌치, 콤비네이션 렌치, 국자, 계량컵, 머그잔, 냄비/팬, 주걱, 체, 주방용품, 거품기. 높은 수준에서 이러한 범주는 일반적으로 두 가지 더 큰 슈퍼 범주 중 하나에 속합니다: 도구 상자에서 찾을 수 있는 물체(예: 망치)와 주방에서 찾을 수 있는 물체(예: 주걱). 모든 물체는 사람이 다루도록 설계되었기 때문에 로봇이 잡고 조작하기에 적절한 크기이지만 두 가지 단서가 있습니다. 첫째, 일부 물체는 기존 로봇 조작기에 너무 무거울 수 있습니다. 우리는 3D 프린팅 복제품이나 플라스틱 버전이 실제 조작을 대체할 수 있다고 생각합니다.
--- EXPERIMENT ---
s. 둘째, 모든 객체가 기능적 움켜잡기를 지원하기 때문에, 즉 무차별적인 움켜잡기와는 대조적으로 특정 용도를 위한 움켜잡기를 지원하기 때문에 일부 객체는 평행 턱 그리퍼가 아닌 인간형 로봇 손이 필요할 수 있습니다. 저희는 저희 데이터 세트가 이 분야에서 추가 연구의 길을 열 것으로 기대합니다. 객체는 다양한 재료와 기하학으로 구성되어 있으며, 일부는 반사 표면을 가지고 있고 일부는 천공되거나 얇은 표면을 가지고 있습니다. 이러한 속성으로 인해 발생하는 많은 과제는 저희 데이터 세트를 실제 시나리오를 처리하기 위한 지각 연구를 진행하는 데 유용하게 만들 것입니다. 저희 데이터 세트의 일부 객체는 관절을 허용하지만, 저희는 캡처 전에 각 인스턴스에 대한 기본 상태를 선택하여 관절이 있는 객체를 마치 딱딱한 것처럼 취급합니다. 그럼에도 불구하고, 저희는 저희의 주석이 이 중요한 분야에서 미래 연구의 토대를 마련했다고 믿습니다. B. 다른 데이터 세트와의 비교 이 작업의 맥락을 설명하기 위해, 저희 데이터 세트를 표에서 기존의 범주 수준 객체 데이터 세트와 비교합니다. I. 표는 왼쪽에서 오른쪽으로 입력 모달리티, 범주 수, 조작 가능한 범주 수, 객체 인스턴스 수, 비디오 수, 이미지/프레임 수, 객체 포즈가 기록되었는지 여부, 절대적 축척을 사용할 수 있는지 여부, 3D 재구성이 제공되는지 여부, 비디오에 정적(객체가 움직이지 않음) 및/또는 동적(객체가 움직임) 장면이 포함되는지 여부, 비디오가 객체의 360° 보기를 캡처하는지 여부, 일부 보기/장면에서 객체가 부분적으로 가려져 있는지 여부, 객체 어포던스에 주석이 달려 있는지 여부를 포착합니다. 저희 데이터 세트는 인스턴스 수준과 범주 수준 포즈 주석을 모두 포함하고 있기 때문에 고유합니다. 인스턴스당 여러 비디오를 캡처했기 때문입니다. 결과적으로 저희 데이터는 특정 3D 객체 모델과 관련하여 작동하는 인스턴스 수준 포즈 추정기를 학습하는 데에도 사용할 수 있습니다. 또한 객체의 3D 모델을 재구성하고 비디오에 맞춰 정렬할 수 있는 기능이 있어 수동 주석으로 가능한 것보다 훨씬 더 정확한 포즈 주석이 생성됩니다. 객체가 조작 가능한지 여부를 판단하는 것은 다소 주관적입니다. 위 표에서 우리는 카메라나 노트북(NOCS-REAL275 [15], Wild6D [16], Objectron [19])을 포함하지 않았는데, 그것들은 깨지기 쉽고 교체 비용이 많이 들기 때문이고, 자전거나 의자(Objectron [19])도 너무 크기 때문에 포함하지 않았습니다.또한 책(Objectron [19]과 CO3D [20])도 포함하지 않았는데, 그것들은 딱딱하지 않아서 로봇 손 하나로 잡기가 거의 불가능하기 때문입니다.게다가 사과, 당근, 오렌지, 바나나, 공(CO3D [20])은 기능적인 잡기를 지원하지 않기 때문에 포함하지 않았습니다.하지만 픽 앤 플레이스만 고려한다면 이것들도 포함할 수 있습니다.NOCS-REAL275 [15], PhoCaL [17], HouseCat6D [18]은 3D 재구성을 위한 별도의 스캐닝 프로세스를 가지고 있는 반면, 우리의 데이터 세트와 CO3D [20]는 캡처로부터 재구성을 얻습니다. CO3D[20]는 단일 비디오의 각 객체 인스턴스에 대한 카메라 포즈를 포함하지만 비디오 간에는 대응 관계가 없습니다. 이러한 이유로 CO3D의 포즈는 범주 수준의 객체 포즈 추정기를 학습시키기에 충분하지 않습니다. IV. 방법 이 섹션에서는 데이터 세트를 수집하고 주석을 달기 위해 사용한 방법론을 설명합니다. A. 데이터 수집 각 범주에 대해 최소 12개의 다른 객체 인스턴스(즉, 다른 브랜드 또는 크기)를 수집하여 17개 범주에 걸쳐 총 212개의 객체를 수집했습니다. 각 객체에 대해 약 10개의 정적 장면 스캔을 다른 조명 조건, 배경 및 다른 방해 객체와 함께 캡처했습니다. 그림 2에 샘플 이미지가 나와 있습니다. 일부 객체의 경우 추가 참조 스캔(오클루전 및 자체 오클루전을 방지하기 위해 객체를 조심스럽게 배치)을 캡처하여 train rain train train test train test train train train test test train train test Strain train test est test (1) train est train train train $train EMPIRE Strain train How train test train Mi train test rain train train train train test train H TLAS test train tram ill traint train 그림 2: 데이터 세트에서 수집한 100개의 객체 인스턴스. 데이터 주석 단계에서 각 썸네일의 왼쪽 상단에 학습/테스트 분할이 표시됩니다. 참조 스캔을 캡처하지 않은 객체의 경우 가장 잘 재구성된 지오메트리를 가진 장면 메시를 표준 메시로 선택했습니다. 각 객체의 최종 메시는 모든 장면의 모든 이미지 프레임을 사용하여 재구성되었습니다. 내장된 ARKit² 또는 ARCore³ 라이브러리를 사용하여 센서 융합을 통해 대략적인 카메라 포즈를 동시에 추정하면서 후면 모바일 기기 카메라를 사용하여 HD 해상도로 비디오를 캡처했습니다. 다른 처리 전에, 우리는 먼저 비디오의 연속된 k 2개의 프레임 세트에서 가장 선명한 이미지를 선택했습니다. k = [캡처된 n/원하는 n], 여기서 ≈1000, 원하는 n캡처된 n=120입니다. 이 단계에서는 가우시안의 라플라시안 분산을 사용하여 선명도를 측정했습니다. 또한 iPad Pro의 전면 TrueDepth 카메라로 640x480으로 녹화하여 51개의 동적 장면을 캡처했습니다. 깊이 감지에 적합한 것으로 밝혀진 객체 하위 집합을 사용하여 객체당 하나의 비디오를 캡처했습니다. 이러한 캡처는 두 부분으로 나뉩니다. 먼저, BundleSDF[39]를 사용하여 고품질 3D 재구성을 생성하기 위해 가능한 한 많은 각도에서 볼 수 있도록 객체를 정적 카메라 앞에서 회전했습니다. 둘째, 인간이 기능적 방식으로 객체를 잡았습니다(예: 손잡이 잡기)ILVIUU LVINY 그림 3: 손잡이(녹색)를 보여주는 기준 진실 어포던스 주석이 있는 재구성된 메시.그리고 느린 동작으로 기능적 경로를 따라 움직였습니다(예: 망치로 못을 박는 흉내내기).이렇게 자연스러운 방식으로 객체를 조작함으로써 객체 어포던스와 관련된 객체 포즈 추정에 대한 다소 현실적인 시나리오를 제공합니다.이러한 동적 캡처와 정확한 주석은 이 공간의 모든 이전 데이터 세트가 정적 장면으로 제한되어 있기 때문에 우리 데이터 세트에만 고유합니다.B. 정적 장면의 데이터 주석 ARKit/ARCore는 모든 프레임에 대한 카메라 포즈 추정을 제공하지만 3D 재구성 및 주석에는 노이즈가 너무 많다는 것을 알았습니다.4 결과적으로 우리는 대신 COLMAP[40]을 사용하여 이미지에서만 크기가 조정되지 않은 카메라 포즈를 추정했습니다.배경 텍스처를 활용하기 위해 수정되지 않은 이미지에서 COLMAP을 실행했습니다(자르거나 분할하지 않음). 그런 다음 XMem[41]을 실행하여 전경 객체를 배경에서 분할했습니다. 이 단계에서는 수동 개입이 약간 필요했지만(일반적으로 첫 번째 프레임에서 마우스를 한두 번 클릭) 그 외에는 자동으로 진행되었습니다. 그런 다음 분할된 이미지에서 Instant NGP[42]를 실행하여 객체의 신경 3D 표현을 만들고 Marching Cubes를 사용하여 메시를 추출했습니다. 이 프로세스는 모든 비디오/장면에 대해 반복되었습니다. COLMAP에서 생성된 카메라 포즈에는 절대 스케일이나 절대 방향이 없으므로 캡처한 ARKit/ARCore 포즈를 사용하여 COLMAP 포즈의 크기를 조정하고(단일 스칼라로 곱함) 중력을 기준으로 수직 방향을 맞췄습니다. 결과 변환은 3D 재구성된 메시에도 적용했습니다. 각 범주에 대해 인스턴스 간 일관성을 보장하기 위해 표준 좌표 프레임 규칙이 설정되었습니다. 일반적으로 대칭 축은 회전 대칭이 있는 범주(예: 드라이버)의 경우 x축으로 설정되고 대칭 평면은 나머지 범주의 xy 평면으로 설정되었습니다. 우리는 모든 범주를 4가지로 정렬했습니다. 특히, 변환 클라우드를 정렬하고 크기를 고려한 후 AR에서 제공한 카메라 포즈와 COLMAP에서 생성한 포즈 간의 중간 차이는 1.8cm와 11.4도였습니다. 이러한 오류는 Instant-NGP 프로세스가 수렴하지 못할 만큼 컸습니다. 객체의 &quot;앞면&quot;은 +x이고 객체의 &quot;위쪽&quot;은 +y입니다. 예를 들어, 망치는 손잡이가 x축을 따라 있고 망치의 머리가 +x에 있고 머리 자체는 y축을 따라 확장되고 얼굴은 +y이고 발톱은 -y가 되도록 배치되었습니다. 객체 인스턴스에 대한 정식 참조 메시의 좌표 프레임을 설정한 후, 이 참조 메시와 다른 장면의 동일한 객체에 대한 각 메시 간의 변환을 계산했습니다. 캡처의 노이즈로 인해 작은 세부 사항이 가려질 수 있으므로 이 변환을 자동으로, 안정적이고 효율적으로 계산할 수 있는 도구를 찾을 수 없었습니다. 대신, 객체의 지향 경계 상자의 범위를 초기 정렬로 사용하여 최종 정렬을 대화형으로 얻는 반자동 도구를 사용하여 비디오를 정렬하는 것이 더 실용적이라는 것을 알았습니다. 저희의 경험에 따르면 이 정렬을 하는 데 1분도 걸리지 않으며 정렬은 장면당 한 번만 필요합니다. 객체의 COLMAP 좌표 포즈에서 표준 참조 포즈로의 변환과 COLMAP에서 생성한 카메라 포즈를 함께 사용하여 카메라에 대한 객체의 포즈가 모든 프레임에서 계산되었습니다. 가끔씩(일부 장면의 몇 프레임) 카메라 포즈의 오류로 인해 포즈가 올바르지 않았습니다. 전체 데이터 세트에서 포즈 품질을 보장하기 위해 계산된 포즈를 사용하여 메시를 입력 이미지로 다시 투영하고 다시 투영의 겹침이 좋지 않은 프레임을 수동으로 제거했습니다. C. 동적 장면의 데이터 주석 동적 장면에 대한 기준 진실 객체 분할 마스크를 얻기 위해 XMem[41](정적 장면과 동일)을 활용한 다음 수동으로 검증 및 수정했습니다. 정적 장면에서 카메라 포즈 현지화에서 기준 진실 객체 포즈를 쉽게 유추할 수 있는 것과 달리 동적 장면에서 객체 포즈를 결정하는 것은 매우 어렵습니다. 이 문제를 해결하기 위해 이러한 동적 비디오에 BundleSDF[39]를 적용하여 객체의 기하학을 동시에 재구성하고 이미지 프레임 전체에서 객체의 6-DoF 포즈를 추적했습니다. Bundle Track[29]과 비교하여 BundleSDF의 추가 3D 재구성을 통해 정적 장면에서 생성된 모델에 등록하여 포즈 주석을 통합할 수 있습니다. BundleSDF 결과를 평가하기 위해 각 프레임의 출력 포즈에 무작위로 노이즈를 추가한 다음 ICP를 적용하여 메시를 깊이 이미지와 정렬했습니다. 그런 다음 모든 프레임을 수동으로 검사하여 고품질을 보장했습니다. 대부분 프레임의 경우 BundleSDF와 ICP의 출력은 거의 동일하며 둘 중 어느 것이 더 정확한지 평가하기 어렵습니다. 따라서 평균을 내어 기준 진실을 얻습니다. BundleSDF는 비디오 전체를 추적하기 때문에 정적 테이블탑 스캐닝 프로세스에서 일반적으로 해결되지 않는 문제인 밑면을 포함한 전체 객체를 재구성할 수 있습니다.마지막으로, 동일한 객체 인스턴스의 정식 메시에 대한 획득한 메시 간에 동일한 전역 등록 단계(정적 장면에서와 같이)를 반복하여 정식으로 변환을 계산했습니다.Stootty Steatty 그림 4: 극한 조명 조건, 눈부심, 그림자, 폐색, 반사 표면 및 천공된 객체가 있는 까다로운 이미지.원본 이미지(위)와 객체 오버레이(아래)가 표시됩니다.AU 2AJTA TT JAUZIV 2AJTA Py T JAUZIV 2AJTA HTPy T JAUZIV 2AJTA JAUCIV 2AJTA Py 그림 5: 동적 플라이어 시퀀스(왼쪽에서 오른쪽)에서 6-DoF 객체 포즈의 정성적 결과, 여기서 인간 작업자는 먼저 객체를 회전하여 전체 재구성을 한 다음 기능적 가능성을 보여줍니다. 빨간색 상자는 BundleSDF [39]의 원시 출력을 시각화하고 녹색 상자는 정제된 실제 기준 참조 프레임을 시각화합니다.이 프로세스를 통해 비디오의 모든 프레임에서 카메라와 관련된 범주 수준 개체 포즈를 얻었습니다.D. 어포던스에 주석 달기 어포던스의 경우 각 개체 재구성에서 3D 메시의 핸들 영역에 수동으로 레이블을 지정했습니다.그림 3은 핸들에 레이블이 지정된 일부 3D 개체 재구성을 보여줍니다.주석은 개체의 기능적 사용을 포착하는 다른 어포던스 기반 분할 또는 키포인트 레이블을 포함하도록 확장할 수 있습니다.A. 데이터 세트 통계V. 결과 각 개체 인스턴스에 대해 3D 참조 메시, 클러터가 있거나 없는 약 10개의 정적 RGB 캡처, 개체 분할 마스크, 범주에 대한 표준 좌표 프레임을 기준으로 모든 프레임의 6-DoF 포즈 및 어포던스 주석이 있습니다.일부 개체에는 동적 RGBD 비디오도 있습니다. 모든 주석은 표준 형식인 COCO [3] 2D 경계 상자/인스턴스 마스크 및 BOP [5] 6-DoF 포즈로 저장됩니다. 캡처 프로토콜은 각 객체 주변 360°를 기록하도록 설계되었습니다. 이러한 주변 캡처는 데이터 세트에서 일반적이지 않지만 고품질 3D 재구성과 뷰 다양성에 중요합니다. 객체 선택 및 캡처에 극한 조명 조건, 눈부심, 움직이는 그림자, 반짝이는 객체 및 심하게 천공된 객체가 포함되도록 했습니다. 그림 6: CenterPose [37]의 범주 수준 객체 감지 및 6-DoF 포즈 추정 결과(빨간색)는 보이지 않는 객체 인스턴스가 포함된 테스트 이미지와 기준 진실(녹색)을 비교한 것입니다. 그림 4를 참조하세요. 각 범주 내에서 다양한 개별 객체를 의도적으로 선택했습니다. 이로 인해 범주형 포즈 추정기와 객체 감지기가 데이터 세트에서 좋은 성능을 발휘하기 어렵지만 데이터 세트가 이러한 실제 과제를 처리하는 데 대한 추가 연구를 촉진할 것으로 기대합니다. 탭. II는 다음에 설명할 2D 감지 및 6-DoF 포즈 추정의 정량적 결과와 함께 데이터 세트 통계를 제시합니다.주방 용품 하드웨어 도구 표 II: 데이터 세트에는 17개의 객체 범주가 있습니다.통계(왼쪽에서 오른쪽으로): 객체 인스턴스, 비디오 및 주석이 달린 이미지 프레임의 수.(후자는 주석이 없는 프레임을 포함하는 경우 약 10배 증가합니다.) 2D 메트릭: 바운딩 박스 감지의 평균 정밀도(AP) 및 픽셀별 분할의 AP.6-DoF 포즈 메트릭: 50%를 초과하는 3D 교차 오버 유니언(IoU)을 갖는 프레임의 백분율; 평균 거리(ADD)가 0~10cm 범위의 임계값 미만인 입방체 정점의 백분율의 곡선 아래 면적(AUC); ADD의 대칭 버전; 그리고 a√√n 픽셀 내에서 정확한 2D 키포인트(PCK)의 백분율, 여기서 n은 이미지에서 객체가 차지하는 픽셀 수이고, a = 0.2입니다. ADD/ADD-S의 3D IOU 및 AUC의 경우, 첫 번째 숫자는 RGB에서만 가져온 것이고, 두 번째 숫자는 단일 기준 진실 깊이 값을 기반으로 최종 결과를 이동/크기 조정한 것입니다. 2D 감지 메트릭 데이터 세트 통계 6-DoF 포즈 메트릭 범주 망치 인스턴스 비디오 프레임 BBox AP 세그먼트 AP24.0k 0.0.플라이어-고정 조인트22.9k 0.0.3D IoU 0.552 / 0.0.669 / 0.플라이어-슬립 조인트17.8k 0.0.0.094 0.플라이어-잠금18.0k 0.0.0.146 / 0.전동 드릴18.4k 0.0.0.590 / 0.래칫15.7k 0.0.0.218 / 0.스크루드라이버20.9k 0.0.0.1960.렌치-조정.18.1k 0.0.0.205 / 0.렌치-빗.17.6k 0.0.0.256 / 0.ADD 0.568 / 0.0.824 / 0.0.3110.0.3440.0.391 / 0.0.370 / 0.0.378 / 0.0.406 / 0.0.534 / 0.ADD-S PCK 0.581 / 0.0.0.834 / 0.0.0.355 / 0.0.0.385 0.0.0.404 / 0.0.0.409 / 0.0.0.432 / 0.0.457 / 0.0.568 / 0.0.0.0.국자18.0k 0.0.0.419 / 0.계량컵15.3k 0.0.0.2010.0.383 / 0.0.346 / 0.머그18.3k 0.0.0.668 / 0. 냄비 / 프라이팬16.1k 0.0.0.629 / 0. 주걱20.2k 0.0.0.155 / 0. 거름망16.0k 0.0.0.235 / 0. 주방도구14.8k 0.0.0.160/0.0.503 / 0.0.353 / 0.0.211 / 0.0.239 / 0.0.368 / 0. 거품기15.9k 0.0.0.427 / 0.0.370 / 0.0.410 / 0.0.386 0.0.507 / 0.0.373 / 0.0.251 / 0.0.260 / 0.0.405 / 0.0.443 / 0.0.0.0.0.0.0.0.0.Total308.1k 0.0.0.340 / 0.0.407 / 0.0.440 / 0.0.B. 객체 감지 = 데이터 세트를 검증하기 위해 각 범주에서 3개의 객체를 보류하여 테스트 세트를 정의했습니다(예시는 그림 2 참조). 객체가 나타나는 모든 장면이 포함됩니다. 단일 Mask-RCNN[43] 모델을 학습하여 17개 객체 범주에서 인스턴스를 로컬라이즈하고 분할했습니다. COCO 인스턴스 분할 작업에서 사전 학습된 모델 가중치를 사용하여 Detectron2 툴킷으로 학습을 수행했습니다. 2D 객체 감지의 경우 AP 50 0.774 및 AP 75 = 0.733을 달성했습니다. 여기서 APn은 n% IoU에서의 평균 정밀도입니다. 50%에서 95%까지의 임계값을 결합하면 AP = 0.656입니다. 분할을 위해 AP50 0.776, AP75 = 0.657, AP=0.562를 달성했습니다. 이러한 결과는 추가 이미지가 훨씬 더 강력한 결과를 얻는 데 도움이 될 수 있지만 이 분야에서 흥미로운 연구를 뒷받침할 만큼 데이터 세트가 크고 다양하다는 것을 시사합니다. 당사의 훈련 프로세스는 아래에 설명된 대로 추가 작업을 통해 3D 재구성 모델을 통해 용이하게 할 수 있는 합성 데이터로 확장될 수도 있습니다. C. 범주 수준 객체 포즈 추정 또 다른 개념 증명으로, 데이터 세트를 사용하여 범주 수준 6-DoF 포즈 추정을 학습하는 기능을 보여줍니다. 구체적으로 동일한 훈련/테스트 분할에 따라 RGB 기반 범주 수준 포즈 추정 방법 CenterPose[37]를 평가했습니다. 포즈의 다양성이 크기 때문에 객체의 수직 축을 이미지의 수직 축과 가장 잘 일치시키는 90° 증가분으로 이미지를 회전하고 기준 진실 객체 치수에 액세스할 수 있다고 가정합니다. 그림 6에서 정성적 결과의 예를 보여줍니다. Shttps://github.com/facebookresearch/detectronVI. 토론 이 프로젝트에서 우리의 목표 중 하나는 로봇 조작을 위한 고품질 3D 데이터 세트 생성의 민주화를 탐구하는 것이었습니다. 이러한 동기는 기성품 카메라를 사용하고 기존 주석 방법으로는 어려웠던 속성(예: 반사 주방 기구, 천공 여과기, 얇은 털)을 가진 객체를 선택하도록 이끌었습니다. 우리는 주석 파이프라인을 최대한 자동화하여 연구자가 자신의 데이터 세트를 생성하는 것이 현실적으로 가능하도록 하는 것을 목표로 합니다. 그 결과, 우리는 주석을 아웃소싱하거나 대규모 팀을 활용하지 않았습니다. 사실, 파이프라인이 설정되자 2.2k 비디오에서 주석이 달린 308k 프레임으로 구성된 전체 데이터 세트를 몇 주 만에 몇몇 연구자가 캡처하여 완전히 처리했습니다. 전반적으로 단일 정적 장면을 처리하는 데 평균적으로 약 5분의 수동 상호 작용 시간이 걸린다고 추정합니다. 이 추정치에는 캡처 시간과 소프트웨어 프로세스의 인간 개입(XMem 결과를 확인하기 위한 비디오 다시 보기, 좌표 변환 프로세스 감독 등)이 모두 포함됩니다. 나머지 프로세스는 자동화되어 약 20분 정도의 컴퓨팅이 필요합니다. 이 시간 중에서 COLMAP이 가장 컴퓨팅 집약적인 부분입니다. 이러한 시간 추정치를 감안할 때 파이프라인에 익숙한 단일 연구원이 다양한 설정에서 약 15,000개의 주석이 달린 프레임이 있는 약 100개의 장면에 걸쳐 ~12개의 객체로 구성된 단일 범주를 약 12시간 안에 캡처하고 처리하는 것이 현실적입니다. COLMAP의 계산 시간은 제외합니다. 따라서 저희 연구는 이 목표를 향해 상당한 진전을 이루었다고 믿습니다. 저희 파이프라인은 확장 가능한 것으로 나타났지만 여전히 수동 개입이 필요한 병목 현상이 많이 있습니다. 저희는 처음에 3DSmoothNet[44]을 사용하여 3D 포인트 클라우드 매칭을 사용하는 자동 메시 정렬 알고리즘을 만들었지만 너무 느려서 특정 객체의 경우 30분 이상 걸리고 크기 변화에 민감하다는 것을 알게 되었습니다. 이로 인해 메시의 방향성 3D 바운딩 상자를 사용하는 훨씬 빠르지만 수동적인 접근 방식을 채택하게 되었습니다. XMem[41]을 사용한 분할은 현재 수동 개입이 필요한 데이터 주석 파이프라인의 또 다른 단계입니다. 이는 일반적으로 대부분의 장면에서 최소한으로 시작 시 몇 번의 클릭 이상 필요하지 않지만 여전히 1~2분의 감독이 필요합니다. 완전 자동 분할 방법은 이러한 병목 현상을 제거하고 파이프라인의 확장성을 더욱 높일 것입니다. 마지막으로 Instant NGP에서 내보낸 메시는 현재 텍스처가 좋지 않습니다. 이러한 렌더링은 개선할 수 있지만 베이크인 조명으로 인해 새로운 환경에서 객체를 사실적으로 렌더링하기 어렵습니다. 또한 현재 도메인 무작위화를 위해 재료 속성을 편집할 방법이 없습니다. 전반적으로 이러한 요소로 인해 훈련을 위한 사실적이고 고품질의 합성 데이터를 생성할 수 없습니다. 다행히도 신경 재구성 및 렌더링은 빠르게 발전하는 분야입니다. 이 분야에 대한 추가 연구는 실제 데이터의 주석 외에도 합성 데이터 생성에 사용할 수 있도록 하여 파이프라인에 또 다른 차원을 추가할 것입니다. VII.
--- CONCLUSION ---
우리는 로봇공학을 위한 6-DoF 범주 수준 포즈 및 스케일로 주석이 달린 이미지의 대규모 데이터 세트를 제시했습니다. 이 데이터 세트는 이런 종류의 가장 큰 비아웃소싱 데이터 세트입니다. 따라서 다른 연구자들이 미래에 이러한 데이터를 수집하는 방법에 대한 교훈을 제공합니다. 동적 장면, 전체 360° 스캔 및 폐색을 캡처하고 객체 어포던스와 3D 재구성을 제공함으로써, 우리의 데이터 세트는 로봇 조작 및 기능적 파악을 위한 지각 학습을 위한 고유한 특성을 제공합니다. 참고문헌 [1] Z. Tang et al., &quot;충돌 없는 조작기 제어를 위한 테이블탑 장면의 RGB 전용 재구성&quot;, ICRA, 2023. [2] J. Deng et al., &quot;ImageNet: 대규모 계층적 이미지 데이터베이스&quot;, CVPR, 2009. [3] T.-Y. Lin et al., &quot;Microsoft COCO: 컨텍스트의 일반 객체&quot;, ECCV, 2014. [4] A. Kuznetsova et al., &quot;Open Images Dataset V4: 대규모 통합 이미지 분류, 객체 감지 및 시각적 관계 감지&quot;, IJCV, 2020. [5] T. Hodaň et al., &quot;BOP: 6D 객체 포즈 추정을 위한 벤치마크&quot;, ECCV, 2018. [6] B. Wen et al., &quot;CatGrasp: 시뮬레이션에서 클러터에서 범주 수준 작업 관련 파악 학습&quot;, ICRA, 2022. [7] -, &quot;한 번만 시연: 단일 시각적 시연에서 범주 수준 조작&quot;, RSS, 2022. [8] B. Huang et al., &quot;장거리 에피소드 로봇 계획 가속화를 위한 일괄 강체 시뮬레이션을 사용한 병렬 몬테카를로 트리 검색&quot;, IROS, 2022. [9] K. Gao 및 J. Yu, &quot;듀얼 암 테이블탑 객체 재배열을 위한 효율적인 작업 계획을 향해&quot;, IROS, 2022. [10] K. Gao, SW Feng 및 J. Yu, &quot;테이블탑 재배열을 위한 실행 버퍼 수 최소화에 관하여&quot;, RSS, 2021. [11] A. Geiger, P. Lenz, C. Stiller 및 R. Urtasun, &quot;비전이 로봇공학을 만나다: KITTI 데이터세트&quot;, IJRR, 2013. [12] H. Caesar et al., &quot;nuScenes: 자율 주행을 위한 멀티모달 데이터세트&quot;, CVPR, 2020. [13] Y. Xiang, R. Mottaghi 및 S. Savarese, &quot;PASCAL 너머: 야외에서 3D 객체 감지를 위한 벤치마크&quot;, WACV, 2014. [14] S. Song, SP Lichtenberg 및 J. Xiao, &quot;SUN RGB-D: RGB-D 장면 이해 벤치마크 모음,&quot; CVPR, 2015. [15] H. Wang 등, &quot;범주 수준 6D 객체 포즈 및 크기 추정을 위한 정규화된 객체 좌표 공간,&quot; CVPR, 2019. [16] Y. Fu 및 X. Wang, &quot;야생에서의 범주 수준 6D 객체 포즈 추정: 반지도 학습 접근법 및 새로운 데이터 세트,&quot; NeurIPS, 2022. [17] P. Wang 등, &quot;PhoCaL: 광도 측정이 어려운 객체가 있는 범주 수준 객체 포즈 추정을 위한 다중 모달 데이터 세트,&quot; CVPR, 2022. [18] H. Jung 등, &quot;HouseCat6D - 현실적인 시나리오에서 가정용 객체가 있는 대규모 다중 모달 범주 수준 6D 객체 포즈 데이터 세트,&quot; arXiv:2212.10428, 2022. [19] A. Ahmadyan 등, &quot;Objectron: 포즈 주석이 있는 야생의 객체 중심 비디오의 대규모 데이터 세트&quot;, CVPR, 2021. [20] J. Reizenstein 등, &quot;3D의 일반적인 객체: 실제 3D 범주 재구성의 대규모 학습 및 평가&quot;, ICCV, 2021. [21] B. Calli 등, &quot;조작 연구 벤치마킹: Yale-CMU-Berkeley 객체 및 모델 세트 사용&quot;, IEEE Robotics and Automation Magazine, 2015. [22] Y. Xiang 등, &quot;PoseCNN: 복잡한 장면에서 6D 객체 포즈 추정을 위한 합성 신경망&quot;, RSS, 2018. [23] S. Tyree 등, &quot;가정의 6-DoF 포즈 추정 로봇 조작을 위한 객체: 접근 가능한 데이터 세트 및 벤치마크,&quot; IROS, 2022. [24] B. Wen 외, &quot;적응형 손으로 잡은 객체에 대한 견고하고 폐색을 인식하는 포즈 추정,&quot; ICRA, 2020. [25] &quot;se(3)-TrackNet: 합성 도메인에서 이미지 잔차를 보정하여 데이터 기반 6D 포즈 추적,&quot; IROS, 2020. [26] K. Chen 및 Q. Dou, &quot;SGPA: 범주 수준 6D 객체 포즈 추정을 위한 구조 안내 사전 적응,&quot; ICCV, 2021. [27] W. Chen 외, &quot;FS-Net: 분리된 회전 메커니즘을 사용한 범주 수준 6D 객체 포즈 추정을 위한 빠른 모양 기반 네트워크,&quot; CVPR, 2021. [28] T. Lee 외, &quot;TTA-COPE: 범주 수준 객체 포즈에 대한 테스트 시간 적응 영어: estimation,&quot; in CVPR, 2023. [29] B. Wen 및 K. Bekris, &quot;BundleTrack: 인스턴스 또는 범주 수준 3D 모델이 없는 새로운 객체에 대한 6D 포즈 추적,&quot; in IROS, 2021. [30] F. Manhardt 등, &quot;CPS++: 자기 감독 학습을 통해 단안 이미지에서 클래스 수준 6D 포즈 및 모양 추정 개선,&quot; in arXiv:2003.05848, 2020. [31] J. Lin 등, &quot;DualPoseNet: 포즈 일관성에 대한 정교한 학습을 통해 듀얼 포즈 네트워크를 사용한 범주 수준 6D 객체 포즈 및 크기 추정,&quot; in ICCV, 2021. [32] R. Zhang 등, &quot;SSP-Pose: 직접 범주 수준 객체 포즈 추정을 위한 대칭 인식 모양 사전 변형,&quot; in IROS, 2022. [33] MZ Irshad 등, “CenterSnap: 단일 샷 다중 객체 3D 모양 재구성 및 범주형 6D 포즈 및 크기 추정,&quot; ICRA, 2022. [34] X. Deng, J. Geng, T. Bretl, Y. Xiang, D. Fox, “iCaps: 반복적 범주 수준 객체 포즈 및 모양 추정,&quot; RAL, 2022. [35] MZ Irshad 등, “ShAPO: 다중 객체 모양, 모양 및 포즈 최적화를 위한 암묵적 표현,&quot; ECCV, 2022. [36] T. Hou 등, “MobilePose: 약한 모양 감독을 통한 보이지 않는 객체의 실시간 포즈 추정,&quot; arXiv:2003.03522, 2020. [37] Y. Lin 등, “단일 단계 키포인트 기반 영어: 2022년 ICRA에서 &quot;RGB 이미지에서 범주 수준 객체 포즈 추정&quot;을 참조하세요.[38] 2022년 ICRA에서 &quot;불확실성 추정을 통한 RGB 시퀀스에서 키포인트 기반 범주 수준 객체 포즈 추적&quot;을 참조하세요.[39] B. Wen 외, &quot;BundleSDF: 신경 6-DoF 추적 및 알 수 없는 객체의 3D 재구성&quot;을 참조하세요.CVPR에서, 2023년.[40] JL Schönberger 및 J.-M. Frahm, &quot;구조에서 동작으로 재검토&quot;를 참조하세요.CVPR에서, 2016년.[41] HK Cheng 외, &quot;XMem: Atkinson-Shiffrin 메모리 모델을 사용한 장기 비디오 객체 분할&quot;을 참조하세요.ECCV에서, 2022년.[42] T. Müller 외, &quot;다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 기본 요소&quot;를 참조하세요.ACM Trans. 그래프, 2022. [43] K. He et al., “Mask R-CNN,&quot; ICCV, 2017. [44] Z. Gojcic, C. Zhou, JD Wegner 및 W. Andreas, “완벽한 일치: 평활화된 밀도를 갖춘 3D 포인트 클라우드 매칭”, CVPR, 2019.
