--- ABSTRACT ---
우리는 코드 최적화에 대한 대규모 언어 모델의 새로운 응용 프로그램을 탐구합니다. 우리는 코드 크기에 맞게 LLVM 어셈블리를 최적화하기 위해 처음부터 학습된 7B 매개변수 변환기 모델을 제시합니다. 이 모델은 최적화되지 않은 어셈블리를 입력으로 받고 프로그램을 가장 잘 최적화하기 위한 컴파일러 옵션 목록을 출력합니다. 중요한 점은 학습 중에 모델에 최적화 전후의 명령어 수와 최적화된 코드 자체를 예측하도록 요청한다는 것입니다. 이러한 보조 학습 과제는 모델의 최적화 성능을 크게 개선하고 모델의 이해도를 향상시킵니다. 우리는 대규모 테스트 프로그램 모음에서 평가합니다. 우리의 접근 방식은 컴파일러보다 명령어 수를 3.0% 줄이는 데 도움이 되어 수천 번의 컴파일이 필요한 두 가지 최첨단 기준선을 능가합니다. 더욱이 이 모델은 놀라울 정도로 강력한 코드 추론 능력을 보여주며 91%의 시간 동안 컴파일 가능한 코드를 생성하고 70%의 시간 동안 컴파일러의 출력을 완벽하게 에뮬레이션합니다. I.
--- INTRODUCTION ---
코드 생성[1-9], 코드 변환[10-12], 코드 테스트[13-15]와 같은 소프트웨어 엔지니어링 도메인을 위한 대규모 언어 모델(LLM)에 대한 관심이 증가하고 있습니다. Code Llama[9], Codex[8], ChatGPT[16]와 같은 모델은 코드에 대한 통계적 이해가 뛰어나고 미완료 코드에 대한 완성 가능성을 제안하여 소프트웨어 편집 및 생성에 유용합니다. 그러나 코드를 최적화하도록 특별히 훈련되지 않은 것으로 보입니다. 예를 들어 ChatGPT는 레지스터로 저장될 변수에 태그를 지정하는 것과 같은 프로그램에 사소한 조정을 하고 벡터화와 같은 보다 본질적인 최적화를 시도하지만 혼동되기 쉽고 실수를 하여 종종 잘못된 코드가 생성됩니다. 머신 러닝 기반 코드 최적화에 대한 이전 작업에서는 수작업으로 작성한 기능[17-19]을 사용하여 그래프 신경망(GNN)[20, 21]까지 사용했습니다. 그러나 모든 경우에 입력 프로그램이 머신 러닝 알고리즘에 표현되는 방식은 불완전하여 그 과정에서 일부 정보가 손실됩니다. 예를 들어, MLGO[17]는 숫자형 피처를 사용하여 함수 인라이닝에 대한 힌트를 제공하지만 호출 그래프나 제어 흐름 등을 충실하게 재현할 수 없습니다. PrograML[21]은 GNN에 전달할 프로그램 그래프를 형성하지만 상수 값과 일부 유형 정보를 제외하여 충실하게 지침을 재현하지 못합니다. 이 작업에서 우리는 다음과 같은 질문을 던집니다. 대규모 언어 모델이 코드를 최적화하는 법을 배울 수 있을까요? LLM은 완전하고 손실 없는 표현으로 소스 프로그램을 그대로 허용할 수 있습니다. 머신 러닝 최적화 프로그램의 입력 및 출력 표현으로 텍스트를 사용하는 데는 † 핵심 기여자. *연락처: cummins@meta.com 바람직한 속성: 텍스트는 보편적이고 이식 가능하며 액세스 가능한 인터페이스이며 이전 접근 방식과 달리 특정 작업에 특화되지 않습니다. 우리는 산업 표준 LLVM[22] 컴파일러를 타겟으로 컴파일러에 존재하는 최적화 변환을 복제하여 LLM의 코드 최적화 능력에 대한 조사를 시작했습니다. LLVM의 최적화 프로그램은 매우 복잡하며 100만 줄이 넘는 C++ 코드에 수천 개의 규칙, 알고리즘 및 휴리스틱이 포함되어 있습니다. LLM이 자연어 번역 및 코드 생성 작업에서 큰 진전을 보였지만 그러한 복잡한 시스템을 에뮬레이션할 수는 없을 것이라는 것이 우리의 기대였습니다. 컴파일러 최적화를 이해하고 적용하려면 여러 수준의 추론, 산술 계산 기능, 복잡한 데이터 구조 및 그래프 알고리즘 적용이 필요하며 이는 LLM에 부족한 것으로 나타난 기능입니다[23, 24]. 우리는 이것이 LLM의 명백한 결함에 대한 논문이 될 것이며 이러한 결함을 극복하기 위한 미래의 영리한 아이디어에 대한 동기가 될 것이라고 생각했습니다. 우리는 많은 경우 충분히 훈련된 LLM이 입력 코드에 적용할 최상의 최적화를 예측할 수 있을 뿐만 아니라 컴파일러에 전혀 의존하지 않고도 최적화를 직접 수행할 수 있다는 사실을 발견하고 완전히 놀랐습니다!우리의 접근 방식은 간단합니다.LLaMa 2[25]에서 가져온 7B 매개변수 LLM 아키텍처로 시작하여 처음부터 초기화합니다.그런 다음 각 어셈블리에 대한 검색을 통해 찾은 최상의 컴파일러 옵션과 해당 최적화를 수행하여 얻은 결과 어셈블리와 결합된 수백만 개의 LLVM 어셈블리 예제에서 이를 훈련합니다.이러한 예제에서만 모델은 놀라운 정확도로 코드를 최적화하는 방법을 학습합니다.우리의 유일한 기여는 코드 최적화를 목적으로 LLM을 처음 적용한 것입니다.우리는 컴파일러 최적화만을 위해 LLM을 구성하고 컴파일러보다 코드 크기 감소에서 단일 컴파일 3.0% 개선을 달성한다는 것을 보여줍니다.2.5e 컴파일로 5.0%를 달성하는 검색 기반 접근 방식과 회귀를 일으키고 수천 번의 컴파일이 필요한 최신 ML 접근 방식과 비교합니다. 우리는 코드 추론을 위한 LLM의 잠재력과 한계를 더욱 특성화하기 위해 보조 실험과 코드 예제를 제공합니다. 전반적으로 우리는 그 효능이 놀랍다고 생각하며 이러한 결과가 커뮤니티에 흥미로울 것이라고 생각합니다. 학습 단계 프롬프트 다음 LLVM-IR에서 명령어 수를 줄이기 위해 실행할 패스를 알려주세요: <code>define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { %3 alloca 132, align%alloca 132, alignstore 132 %0, 132* %3, align 4. store 132 %1, 132* %4, align%%load 132, 132* %3, alignicmp eq 132 %5,br 11 %6, label %10, label %7: %8 load 132, 132* %3, align%9 icmp eq 132 %8,br 11 %9, label %10, label %Answer Run passes -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading to reduce instruction count from 15 to 4: <code>define dso local void @&quot;iowrite_single&quot; (132 %0, 132 %1) switch 132 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ._crit_edge1: %call 132 @&quot;outb&quot; (132 %0, 132 %1) br label &quot;._crit_edge2&quot; ._crit_edge2: ret void } 10: %load 132, 132* %3, align%12 load 132, 132* %4, align%13 call 132 @&quot;outb&quot; (i32 %11, 132 %12) br label %14: ret void } declare dso local i32 @&quot;outb&quot; (132, 132)</code></code> 추론 단계 답변 실행 패스 -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading 컴파일러 최적화 코드 define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { switch i32 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ... ret void } 그림 1: 학습 및 추론 중 모델 입력(프롬프트)과 출력(답변)을 보여주는 접근 방식 개요. 프롬프트에는 최적화되지 않은 코드가 포함되어 있습니다. 답변에는 최적화 패스 목록, 명령어 수 및 최적화된 코드가 포함되어 있습니다. 추론 중에는 컴파일러에 제공하는 최적화 패스 목록만 생성하여 최적화된 코드가 올바른지 확인합니다. 표 I: 훈련 데이터. 각 LLVM-IR 기능은 자동 조정되어 (Prompt, Answer) 쌍을 만드는 데 사용됩니다. n 토큰 열은 Llama 2[25] 토크나이저를 사용하여 프롬프트가 인코딩될 때 토큰 수를 보여줍니다. 표 II: 테스트 데이터. n개 함수 n개 함수 손으로 쓴 합성 총 610,389,1,000, 최적화되지 않은 명령어 수 8,417,13,775,16,411, 최적화되지 않은 명령어 수 -Oz 디스크의 명령어 수 크기 n개 토큰 653.5MB 352.3MB 1.0GB 214,746,158,435,373,181, AI-SOCO [31] ExeBench [32] POJ-104 [33] Transcoder [12] 8,26,97,386,47,181,8,4,17,289,129, CSmith [34] YARPGen [35] 33,647,138,12,285,144, 총 100,1,716,645,II. LLMS를 사용한 패스 순서 지정 이 작업에서는 컴파일러 패스 순서를 목표로 합니다. 패스 순서 지정 작업은 컴파일러에서 사용 가능한 최적화 변환 패스 세트에서 특정 입력 코드에 대해 최상의 결과를 생성하는 패스 목록을 선택하는 것입니다. 패스 순서를 조작하면 런타임 성능과 코드 크기에 상당한 영향을 미치는 것으로 나타났습니다[19, 26]. 이 작업에 대한 머신 러닝 접근 방식은 이전에 좋은 결과를 보였지만 다양한 프로그램 간에 일반화하는 데 어려움을 겪었습니다[27]. 이전 작업에서는 일반적으로 다양한 구성을 시도하고 최상의 성능 옵션을 찾기 위해 수십 또는 수백 번 새 프로그램을 컴파일해야 하므로 실제 사용에는 비실용적이었습니다. 충분한 추론 능력을 갖춘 대규모 언어 모델이라면 이것이 필요 없이도 좋은 최적화 결정을 내리는 법을 배울 수 있을 것이라고 가정했습니다. 코드에 대한 LLM에 대한 대부분의 이전 작업은 Python과 같은 소스 언어에서 작동합니다. 대신 패스 순서 지정 문제의 경우 중간 표현(IR)이라고 하는 컴파일러 어셈블리의 하위 수준에서 추론이 필요합니다. LLM 사전 학습을 위한 소스 언어의 큐레이트된 데이터 세트가 있지만(예: [28–30]), 컴파일러 IRS는 이러한 데이터 세트의 상당 부분을 차지하지 않으며 ChatGPT와 같은 모델은 이해에 약간의 가능성을 보여주지만 IR에 대한 추론 능력은 소스 언어에 비해 훨씬 떨어집니다. 이전 연구[17, 27]와 마찬가지로 IR 명령어 수를 바이너리 크기에 대한(완벽하지 않은) 프록시로 사용하여 코드 크기에 대한 LLVM 패스 순서를 최적화하는 것을 목표로 합니다. 이 접근 방식은 선택한 컴파일러 및 최적화 메트릭과 무관하며 앞으로 런타임 성능을 목표로 할 것입니다. 지금으로서는 코드 크기를 최적화하면 학습 데이터 수집이 간소화됩니다. A. 프롬프트 모델에 최적화되지 않은 LLVM-IR(예: clang 프런트엔드에서 내보내는 것)을 제시하고 이에 적용해야 하는 최적화 패스 목록을 생성하도록 요청합니다. 그림은 입력 프롬프트와 출력 텍스트의 형식을 보여줍니다. 이 작업에서 우리는 LLVM 10을 목표로 하고 opt의 최적화 플래그를 사용합니다. 선택할 수 있는 최적화 패스는 122개이며 패스는 단일 시퀀스에서 두 번 이상 선택할 수 있습니다. 또한 패스 목록당 한 번만 발생할 수 있는 6개의 메타 플래그(-00, -01, -02, -03, -Oz 및 -Os)도 포함합니다. 패스 목록은 길이에 제한이 없지만 실험에서 일반적으로 약 1018의 조합 검색 공간에 대해 최대 9개의 패스 길이를 발견했습니다. 그림 1에서 볼 수 있듯이 두 가지 보조 작업도 포함합니다. i) 최적화가 적용되기 전과 후에 코드의 명령어 수를 생성하고 ii) 최적화가 적용된 후에 출력 IR을 생성합니다. 이를 통해 코드 최적화의 메커니즘에 대한 심층적인 이해를 강제하여 더 나은 패스 순서 지정 결정을 내릴 수 있을 것이라고 가정합니다. 섹션 VB에서 실험적으로 이를 검증합니다. 모델은 명령어 수와 최적화된 IR을 생성하도록 학습되었지만 배포에는 이러한 보조 작업이 필요하지 않습니다. 컴파일러를 사용하여 실행하는 패스 목록만 생성하면 됩니다. 따라서 우리는 모델의 출력이 신뢰할 수 있어야 하는 기술을 괴롭히는 정확성의 문제를 회피합니다[10–12, 36].B. LLVM-IR 정규화 우리는 다음 규칙을 사용하여 LLM을 훈련하는 데 사용되는 LLVM-IR을 정규화합니다.우리는 주석을 버리고, 메타데이터와 속성을 디버깅하고, 줄바꿈은 유지하지만 다른 공백을 표준화하고 들여쓰기를 제거하는 사용자 정의 렉서를 통해 IR을 공급하여 일관된 공백을 보장합니다.우리는 LLM의 제한된 입력 크기(섹션 III-A)를 최대한 활용하기 위해 LLVM-IR의 길이를 줄이기 위해 이를 수행합니다.그림 1의 코드는 이러한 방식으로 처리되었습니다.III.모델 우리는 유비쿼터스 변압기 아키텍처[37]를 사용합니다.변압기는 고정된 크기의 컨텍스트 창에서 셀프 어텐션을 사용하는 인공 신경망입니다.입력 텍스트는 먼저 단어와 하위 단어 단위로 토큰화됩니다. 이것들은 연속 벡터 표현에 내장되어 변환기의 인코더에 입력으로 제공되며, 여기서 셀프 어텐션 메커니즘은 토큰 간의 문맥적 관계를 포착하여 모델이 입력 텍스트의 의미 구조를 이해하고 처리하도록 장려합니다. 출력 텍스트는 한 번에 하나의 토큰을 반복적으로 생성하여 생성됩니다. 디코더는 이전에 생성된 토큰과 함께 인코딩된 입력을 가져와 셀프 어텐션을 사용하여 시퀀스의 다음 토큰을 예측합니다. 디코딩하는 동안 탐욕적으로 샘플링하여 가장 가능성 있는 토큰 시퀀스를 선택합니다. 이 프로세스는 시퀀스 끝 토큰이 생성되거나 사전 정의된 최대 길이에 도달할 때까지 계속됩니다. A. 모델 아키텍처 Llama 2 [25]와 동일한 모델 아키텍처와 바이트 쌍 인코딩(BPE) [38] 토크나이저를 사용하지만 모델을 처음부터 학습합니다. Llama 2 구성 중 가장 작은 것인 어텐션 헤드, 4,096개의 숨겨진 차원, 32개의 레이어를 사용하여 총 7B개의 매개변수를 사용합니다. (프롬프트, 답변) 쌍의 최대 길이는 시퀀스 길이에 의해 정의됩니다. 이 작업에서 우리는 시퀀스 길이 개선 -Oz 6% 4% 2% 자동 튜너 우리의 접근 방식 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le50% 40% w 30% 20% MAPE 10% (a) 생성된 패스 목록의 성능.최적화되지 않은(입력) 코드 최적화된(출력) 코드 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le1.0.0.0.40.2(b) 명령어 수 예측 정확도.BLEU 코드는 정확한 일치를 컴파일합니다.0.0.0.0.0.1.1.#. 훈련 토큰 le(c) 모델 최적화된 코드 메트릭.그림 2: 훈련 중 홀드아웃 검증 세트에 대한 성능.우리는 250개 훈련 단계(1억 3,100만 훈련 토큰)마다 성능을 평가합니다. -Oz와의 동등성은 393M 토큰에서 달성되고 10.9B 토큰에서 최고 성능을 달성합니다. 2,048개 토큰. Llama 2 토크나이저는 LLVM-IR을 인코딩할 때 토큰당 평균 2.02자를 달성하므로 이는 2KB에서 학습할 수 있는 가장 긴 LLVM-IR에 대한 대략적인 상한을 제공합니다(2KB 프롬프트와 2KB 답변은 약 2,048개 토큰이기 때문). B. 학습 데이터 우리는 표 I에 요약된 최적화되지 않은 LLVM-IR 함수의 대규모 코퍼스를 조립했습니다. 공개적으로 사용 가능한 수기 C/C++ 코드의 데이터 세트에서 함수를 추출하고 C/C++ 컴파일러 테스트 생성기에서 생성한 합성 코드로 이를 보완했습니다. 전체적으로 학습 코퍼스는 1,000,000개의 중복 제거된 IR 함수로 구성되어 총 373M 학습 토큰입니다. 우리는 2,048토큰 시퀀스 길이에 맞출 수 있는 데이터 양을 극대화하기 위해 전체 모듈이 아닌 개별 IR 기능 수준에서 작업합니다. 가장 작은 명령어 수를 생성하는 최적화 패스 목록을 찾기 위해 자동 튜닝을 사용합니다. 우리의 자동 튜너는 Liang et. al. [20]의 작업에서 영감을 받아 랜덤 검색과 함수 간의 전체 대 전체 결과 브로드캐스팅을 결합합니다.표 III: 표 II의 보이지 않는 LLVM-IR 함수 테스트 세트에서 패스 순서를 지정하는 다양한 접근 방식의 성능. 모든 메트릭은 -Oz에 대한 것입니다. 절감된 명령어는 개선된 함수에 대해 합산되고 회귀된 명령어는 회귀된 함수에 대해 합산됩니다. 전반적인 개선은 -Oz에 대한 총 명령어 수 절감 합계입니다. 자동 튜너는 최상의 성능을 달성하지만 25억 개의 추가 컴파일(949 CPU-일)이 필요합니다. 우리의 접근 방식은 컴파일러를 한 번도 호출하지 않고도 자동 튜너의 이득의 60%를 달성합니다. 추가 기능 컴파일 개선된 기능 회귀된 명령어 저장된 명령어 회귀된 전체 개선 Autotuner AutoPhase [39] Coreset-NVP [20] 우리의 접근 방식 2,522,253,6,30,5.03% 4,500,1,8,6,32,-3.85% 442,3,6,16,28,-1.88%4,21,3,3.01% 표 IV: &quot;-Oz 백업&quot;을 사용하여 표 III의 모델 확장. 모델이 -Oz 이외의 통과 목록을 예측하는 경우 -Oz도 평가하고 최상의 것을 선택합니다. 이렇게 하면 추가 컴파일을 희생하여 -Oz에 대한 회귀가 방지됩니다. 추가 컴파일 AutoPhase [39] Coreset-NVP [20] 4,600,542, 우리의 접근 방식 5, 전체 개선 1.02% 2.55% 3.52% 각 기능에 대해 고정된 시간(780초) 동안 무작위 검색을 실행한 다음, 명령어 수에 기여하는지 확인하기 위해 무작위로 선택한 개별 패스를 반복적으로 제거하여 최상의 패스 목록을 최소화합니다. 기여하지 않으면 삭제합니다. 각 함수에서 이를 수행한 후 고유한 최상의 패스 목록 세트를 집계하여 다른 모든 함수에 브로드캐스트합니다. 따라서 패스 목록이 한 함수에서 잘 작동하는 것으로 확인되면 다른 모든 함수에서 시도합니다. 전체적으로 자동 튜너는 각 학습 프로그램을 평균 37,424번 컴파일하여 -Oz에서 제공하는 컴파일러의 기준 고정 패스 순서에 비해 명령어 수 감소에서 5.8%의 개선을 달성했습니다. 우리의 목적상 이 자동 튜닝은 각 함수의 최적화를 위한 황금 표준 역할을 합니다. 자동 튜너가 발견한 명령어 수 절감은 상당하지만 이러한 성과를 달성하기 위한 계산 비용은 9,016 CPU 일이었습니다. 이 작업의 목표는 컴파일러를 수천 번 실행할 필요가 없는 예측 모델을 사용하여 자동 튜너 성능의 일부를 달성하는 것입니다. C. 학습 무작위로 초기화된 가중치에서 시작하여 64개의 V100에서 30,000단계로 모델을 학습하여 총 학습 시간이 620GPU 일이었습니다. ₁ 및 B2 값이 각각 0.9 및 0.95인 AdamW 옵티마이저[40]를 사용합니다. 1,000개의 워밍업 단계, 1e-5의 최대 학습 속도, 최대 학습 속도의 1/10인 최종 학습 속도를 갖는 코사인 학습 속도 일정을 사용합니다. 256의 배치 크기를 사용했고 각 배치에는 총 15.7B개의 학습 토큰에 대해 524,288개의 토큰이 포함됩니다. 전체 30,000단계의 학습은 7.7에포크(학습 코퍼스에 대한 반복)입니다. 학습하는 동안 학습 세트와 동일한 방식으로 처리된 1,000개의 보이지 않는 IRS의 홀드아웃 검증 세트에서 모델을 평가했습니다. 모든 단계를 평가합니다. IV. 평가 이 섹션에서는 보이지 않는 코드에 대한 패스 목록을 생성하고 최적화를 올바르게 수행하는 모델의 능력을 평가합니다.A. 학습 결과 그림 2는 보이지 않는 LLVM-IR 함수 1,000개의 홀드아웃 검증 세트에서 평가할 때 학습 중 성능을 보여줍니다.최고 검증 성능은 모델이 109억 학습 토큰에서 달성했습니다.최고 성능에서 모델 생성 패스 시퀀스를 사용하여 최적화된 코드는 컴파일러의 기본 제공 패스 순서(-Oz)를 사용하여 최적화했을 때보다 4.4% 적은 명령어를 포함합니다.자동 튜너는 5.6%의 더 큰 명령어 수 감소를 달성하지만 이를 위해 검증 세트를 2,700만 번 컴파일해야 했습니다.모델은 컴파일러를 한 번도 호출하지 않고 예측을 수행합니다.그림 2b는 예측된 입력 및 출력 명령어 수의 오류를 보여줍니다.최적화되지 않은 코드에 대한 명령어 수 예측은 거의 완벽한 정확도에 빠르게 접근합니다.출력 명령어 수 예측은 더 어려워서 평균 평균 백분율 오류(MAPE)가 5.9%에 도달합니다. 그림 2c는 세 가지 지표를 사용하여 생성된 코드의 품질을 평가합니다.BLEU[41] 점수는 모델 생성 코드와 생성된 패스 목록을 사용하여 컴파일러에서 생성한 참조 기준 진실 코드 간의 유사성을 보여줍니다.코드 컴파일은 모델 생성 코드가 오류 없이 컴파일되는 빈도입니다.정확한 일치는 생성된 패스 목록을 사용하여 최적화할 때 모델 생성 코드가 컴파일러 생성 코드와 문자별로 일치하는 빈도(즉, BLEU=1인 횟수)를 추적합니다.최대 성능에서 모델은 오류 없이 컴파일되는 코드를 생성하는 인상적인 90.5% 비율을 달성합니다.또한 BLEU 점수 0.952는 모델 최적화된 코드가 컴파일러의 코드와 매우 유사하고 정확한 일치 빈도가 70%임을 보여줍니다.비교를 위해 최적화되지 않은 코드를 출력에 복사하는 기준선은 BLEU 점수 0.531과 정확한 일치 빈도 0%를 달성하여 이처럼 높은 점수를 달성하려면 입력 코드를 상당히 조작해야 함을 보여줍니다. 훈련이 끝날 무렵 검증 세트의 성능은 정점에 도달했습니다. 가장 성능이 좋은 체크포인트를 사용하고 나머지 평가에서는 100배 더 큰 규모의 평가로 전환합니다. B. 최신 기술과의 비교 이 실험에서는 기준선과 비교하여 LLM의 통과 목록을 예측하는 능력에 대한 대규모 평가를 수행합니다. 빈도(로그) nالسيسسيد السيسي السبيسZOUAб -instcombine -mem2reg -reg2mem -simplifycfg -memcpyopt Η το POJS -02-Os Hoop-rotate -03-newgvn -loop-deletion. -jump-threading. tailcallelim -로드-스토어-벡터라이저 early-csegvn-hoist – -div-remp -early-cse-memssa -추측-실행 -sip-벡터라이저 싱크 재연결 -00-dse-상관-전파 병합 반환 -break-crit-edges -globalopt -nary-재연결 -instsimplify -loop-simplifycfg -simple-loop-unswitch -loop-unswitch -loop-unroll -aggressive-instcombine -die -bdce -scalarizer -dce → adce Hoop-reroll -flattencfg Hoop-vectorize-constprop -functionattrs woipi-doojAutotuner 우리의 접근 방식 -irce → predicat -mldst-motion Hoop-predication -attributor Hoop-instsimplify hotcoldsplit -Hoop-load-elim -coro-elide -elim-avail-extern -loop-reduce -Hoop-interchange -coro-cleanup Hower-constant-intrinsics -loop-versioning -loop-versioning-licm -partial-inliner -pgo-memop-opt + strip + 1 2 3 4 5 6 7 8 9패스 목록 길이 그림 3: 100,000개 테스트 프로그램 각각에 대한 패스 목록에서 패스가 발생하는 빈도(왼쪽)와 패스 목록의 길이(오른쪽). -Oz는 자동 튜너의 시작점이며 지배적인 결과이며 자동 튜닝된 테스트 프로그램의 93.2%에서 가장 잘 발견된 결과이고 더 긴 시퀀스의 일부로 패스 목록의 추가 0.6%에 나타납니다. 모델에서 생성된 패스 분포는 자동 튜너를 추적하지만 -Oz(94.3%)를 약간 과대 예측하고 자동 튜너가 훈련 세트에는 사용했지만 테스트 세트에는 사용하지 않은 9개의 패스를 포함합니다. 결과는 자동 튜너 주파수가 감소하는 순서로 정렬됩니다. define 132 @f1 (i8 %0) { 2 alloca 132, align3 alloca 18, alignstore 18 %0, 18* %3, align4 load 18, 18* %3, align%5= zext 18 %4 to% 6 = icmp sge 132 %5,br il %6, label %7, label7: 8 load 18, 18* %3, align% 9 = zext 18 %8 to10 icmp sle 132 %9,br il 10, label %11, label %11: 12 load 18, 18* %3, align13 zext 18 %12 to<snip 21 lines...> 33: 34 132 로드, 132* %2, alignret 132 %define 132 @f1 (i8 %0) { %2 = zext 18 %0 to%.off 18 %0,3 추가 icmp ult i8 %.off,br il 3, 레이블 %4, 레이블 %4: %add nsw 132 %2,br 레이블 %6: %.reload16.off = nsw 132 %2,7 추가 icmp ult 132 %.reload16.off,br il 7, 레이블 %10, 레이블 %8: 9 icmp eq 18 %0,%. = select il %9, 132 26, 132br label %10: %.0.reg2mem.0 = phi i32 [%5, %4], [., 8], [%.reload16.off, %6] ret 132.0.reg2mem.define 132 @f1 (i8 %0) { %2 =zext 18 %0에서 %.off로 18 %0,%3 icmp ult 18 %.off,br il %3, label %6, label %._ crit_edge ._crit_edge: %.off24 = 18 %0,%4 icmp ult i8 %.off24,br il 4, label %6, label %.를 추가합니다. crit_edge._crit_edge9: 5 icmp eq 18 %0,spec.select = select il %5, 132 26, 132ret6: } spec.select %.sink = phi 132 [191, %1], [159,._crit_edge] 7 add nsw i32 %.sink, %ret 132 %} (a) 입력 코드(39개 명령어). (b) 패스를 사용한 자동 튜닝 코드(14개 명령어): -reg2mem -inst combine -Os -01. (c) 모델 최적화 코드(13개 명령어) 및 패스 목록: -reg2mem -simplifycfg -mem2reg -jump-threading -Os. 목록 1: 이 코드를 이전에 본 적이 없음에도 불구하고 모델이 자동 튜너보다 더 나은 패스 목록을 제안하는 예제 IR 함수. 이 함수의 경우 자동 튜너는 26,000개의 다른 패스 순서를 시도했습니다. 모델에서 생성한 패스 목록은 1,000,000개의 예제로 구성된 학습 세트에서 5번 나타납니다.데이터 세트 평가를 위해 광범위한 벤치마크 데이터 세트를 집계하여 표 II에 요약했습니다. 학습한 것과 동일한 IR 함수를 중복 제거하고 제외합니다.테스트 데이터는 코딩 대회(AI-SOCO[31], POJ-104[33]), 컴파일러 테스트 케이스 생성기(CSmith[34], YARPGen[35]) 및 기타 공개적으로 사용 가능한 코드(ExeBench[32], Transcoder[12])를 포함한 다양한 도메인의 코드로 구성되어 있습니다.기준선 접근 방식을 AutoPhase[39], Coreset-NVP[20] 및 Autotuner의 세 가지 기준선과 비교합니다.AutoPhase[39]는 고정 길이 에피소드에서 누적 명령어 수 절감을 극대화하는 최적화 패스 시퀀스를 선택하기 위해 Proximal Policy Optimization[42]을 사용하여 에이전트를 학습시키는 강화 학습 접근 방식입니다. 각 단계에서 최적화되는 프로그램은 명령어 수와 기타 속성의 56차원 벡터로 에이전트에 표현됩니다.우리는 [39]의 환경을 복제하지만 에이전트가 100,000개의 에피소드로 훈련되는 [27]의 구현 및 확장된 훈련 체제를 사용합니다.우리는 언어 모델(표 I)과 동일한 데이터에서 에이전트를 훈련하고 보류 검증 세트에서 훈련하는 동안 주기적으로 에이전트 성능을 평가합니다.이전 작업에서와 마찬가지로 45의 동작 공간과 에피소드 길이를 사용합니다.Coreset-NVP[20]는 반복적 검색과 학습된 비용 모델을 결합하는 기술입니다.먼저 17,500개의 벤치마크에서 탐욕적 검색을 실행하여 최상의 패스 목록의 핵심 세트를 결정합니다.그런 다음 그래프 합성곱 네트워크에서 처리한 ProGraML[21] 그래프를 프로그램 표현으로 사용하여 이 검색 결과에 대해 신경 값 예측(NVP)을 훈련합니다. 추론에서 Coreset-NVP는 정규화된 보상을 예측하고 가장 높은 정규화된 보상을 갖는 처음 몇 개의 패스 시퀀스를 시도합니다.-5% -10%0% 30% 15% AutoPhase AutoPhase Coreset-NVP 10% 20% Autotuner Coreset-NVP Autotuner 우리의 접근 방식 우리의 접근 방식 5% 10% ExeBench Transcoder CSmith YARPGen0% -10% TUnoptimized 명령어 수 그림 5: 입력 크기에 따른 -Oz 대비 개선.더 큰 코드는 더 많이 최적화합니다.그림 4: 데이터 세트에 따른 -Oz 대비 개선.수작업으로 작성한 코드는 더 많이 최적화합니다.보상.각 벤치마크에 대해 시도할 수 있는 총 패스 수는 이전 연구에 따라 45입니다.저자가 제공한 모델 가중치를 사용하여 테스트 세트에서 추론을 수행합니다.마지막으로, 이를 학습 데이터를 생성하는 데 사용한 Autotuner와 비교합니다.섹션 III-B에서 설명한 대로 학습 데이터와 동일한 방식으로 테스트 데이터 세트를 자동 튜닝했습니다. 결과 표 III은 결과를 요약한 것입니다. 우리의 접근 방식은 모든 데이터 세트에서 -Oz, AutoPhase, Coreset-NVP보다 성능이 뛰어납니다. 전반적으로 자동 튜너에 제공된 수천 번의 최적화 시도를 통해 가장 성능이 좋은 패스 목록을 발견할 수 있습니다. AutoPhase와 Coreset-NVP는 모두 -Oz보다 성능이 뛰어나지만 회귀가 많아 명령어 수에 전반적으로 부정적인 영향을 미치는 패스 목록을 식별할 수 있습니다. 이를 극복하기 위해 간단한 &quot;-Oz 백업&quot; 확장을 제안합니다. 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 경우 -Oz도 실행하고 두 옵션 중 더 나은 것을 선택합니다. 이렇게 하면 -Oz에 대한 회귀가 방지되지만 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 횟수만큼 추가 컴파일 횟수가 증가합니다. 표 IV는 이러한 방식으로 평가할 때의 기술 결과를 보여줍니다. 이것이 모델이 더 개선되는 데 도움이 되지는 않지만, 회귀가 없기 때문에 AutoPhase와 Coreset-NVP는 이제 -Oz보다 전반적으로 개선되었지만 -Oz 백업이 있거나 없는 LLM보다는 여전히 낮습니다.C. 생성된 패스 목록의 평가 그림 3은 자동 튜너와 이전 실험의 모델이 패스를 선택하는 빈도를 보여줍니다.모델이 선택한 패스의 분포는 자동 튜너를 광범위하게 추적합니다.-Oz가 가장 자주 최적의 패스입니다.-Oz를 제외하고 모델에서 생성된 패스 목록의 평균 길이는 3.4(최대 10)이고 자동 튜너 패스 목록의 평균 길이는 3.1(최대 9)입니다.모델에서 생성된 패스 목록 중 105개는 학습 데이터에 전혀 나타나지 않습니다.710개의 경우에서 모델에서 생성된 패스 목록은 테스트 세트에서 자동 튜너보다 성능이 뛰어나지만 일반적으로 개선 정도는 작습니다. 목록 1은 모델에서 생성된 표 V: 100,000개의 보이지 않는 입력에 대한 모델 최적화 코드의 컴파일러 오류의 예를 보여줍니다. 오류 범주 n 유형 오류 5, 명령어 전방 참조 1, 정의되지 않은 값 1, 잘못된 재정의 구문 오류 상수에 대한 잘못된 값 정의되지 않은 함수 인덱스 오류 기타 9, 전체 패스 목록은 제어 흐름을 더 적은 블록으로 단순화하여 하나의 추가 명령어를 저장합니다. 그림 4는 벤치마크 데이터 집합에 따른 패스 순서 지정에 대한 각 접근 방식의 개선 사항을 분석합니다. -Oz에 비해 가장 큰 개선 사항은 POJ-104 및 Transcoder 데이터 집합에서 발견되는데, 둘 다 대량의 수기 코드를 집계하는 반면 컴파일러를 테스트하기 위한 무작위 프로그램 생성기인 YARPGen은 -Oz에 비해 개선할 수 있는 기회가 가장 적습니다. 우리는 입력 프로그램 크기와 자동 튜너와 모델 모두에서 발견되는 -Oz에 비해 잠재적인 성능 개선 사이에 강력한 상관 관계가 있음을 발견했습니다. 그림 5는 이러한 추세를 나타내며, 더 큰 프로그램이 -Oz에 비해 개선할 수 있는 기회가 더 많다는 것을 명확하게 보여줍니다. D. 생성된 코드 평가 이 섹션에서는 모델에서 생성된 코드의 품질을 평가합니다. 이를 위해 테스트 세트의 모든 100k 함수에 대해 최적화된 코드를 생성하는 보조 학습 작업을 실행했습니다. 이는 이전 섹션에서 평가된 패스 목록을 생성하는 데 필요하지 않습니다. 이 섹션의 코드 샘플에서는 불필요한 명령문을 생략하고 식별자 이름을 줄이는 등 간결성을 위해 사소한 편집을 했습니다. 90.3%의 경우 모델에서 생성된 최적화된 IR이 컴파일되고 68.4%의 경우 출력 IR이 컴파일러에서 생성한 기준 진실과 일치합니다. 생성된 IR이 컴파일되지 않는 9.7%의 경우에 대한 다양한 오류 클래스를 표 V에 분류하고 목록 2에 코드 예제가 나와 있습니다.오류: &#39;15&#39;가 &#39;i32&#39; 유형으로 정의되었지만 &#39;il&#39;을 예상함 %or.cond = or il %14, %(a) 모델은 %15를 정수로 정의했지만 나중에 부울(유형 오류)로 사용하려고 했습니다. 오류: 상수 표현식 유형 불일치 e.str private unnamed_addr 상수 [493 x 18] 정의 132 @f1( 132 %0, 132 %) 정렬 2 { br 레이블 %int f1 (int x, int y) { int i = 2; while (ii &lt; y) { i += 1; return 2; } 3: } % i = phi 132[%7, %6], [2, %2] %4 = mul nsw i32 %i, i %c&quot; <snip 492 chars ', align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type %1 tail call 132 @f1 (float -0.47799998483256463, float -1.8159999847412109) = icmp sgt 132 %4, %br il 5, label %8, label %6:add 132 %i,br label %8: ret 132} (b) Equivalent (hand-written) C code. define 132 @f1( 132 %0, 132 %) align 2 { ret 132} (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates. 1.0.define hidden signext 18 @f1() %230 { %1 alloca 164, align0.store 164 3718042838174166437, 164* %1, align%2 load 164, 164* %1, align0.%trunc i64 %2 to} ret 18 %0.BLEU Code compiles Optimized instcount error I (a) Input unoptimized code. define hidden signext@f1() %30 { ret 18define hidden signext@f1() #230 { ret 18} } (c) Model-generated code. (b) Desired optimized code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. 0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Run passes -inst combine -simplifycfg to reduce instruction count from 14 to 7: define dso_local 132 @f1 (132 %0) { } % 2 = load 164, 164* getelementptr inbounds ( struct.t2, %struct.t2* @gvar, 164 0, 132 0), align3 icmp eq 164 %2, O %4= icmp eq 132 %0,%or.cond or il %3, %5 load 132, 132* @S64_MAX, align%6 = select il %or.cond, 132 %5, 132 %ret 132 %Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRS is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The modelFrequency (log) 0.0.0.-name-anon-globals -Hoop-versioning-licm -strip -strip-dead-prototypes. -indvars -loop-reroll -strip-nondebug. -loop-unroll Hoop-unswitch. -instcombine. -loop-instsimplify-Hoop-idiom -Hoop-deletion -Hoop-rotate Hoop-unroll-and-jam-loop-guard-widening -Hoop-predication. -die -irce 1.0.75globalopt -01. -simple-loop-unswitch -loop-interchange -Os --Oz. -licm -loop-sink-loop-load-elim -loop-simplifycfg -loop-vectorize --sroa Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. gvn-hoist . -loop-simplify -tailcallelim -speculative-execution. -early-cse -dse. -jump-threading -loop-reduce -sink -simplifycfg Improvement over -Oz 5% 4% 3% 2% 1%100% data 50% data 25% data 0% 0.0.0.0.#. Train Tokens 0.1.1.100% data, No Aux T 1.leFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. overall improvement n training examples generate optimized code? 1,000,500,250,1,000,× 4.95% (-) 3.91% (-21%) 3.74% (-24%) 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMS [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. of We observe an interesting connection between the quality pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%.илбмәи gvn . early-cse-memssa. -bdce-correlated-propagation Hcssa. - бәлгшәшadce -break-crit-edges. -ipsccp-dce-globaldce aggressive-instcombine. -instsimplify -sccp -reassociate -loop-fusion Regressed Improved Optimize the following LLVM-IR using -name-anon-globals: @0 private @anon. 2ef3bda806391c61822366a2a59f2569.0 = private @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0 = private unnamed_addr constant [14 x 18] c"<snip> &quot;, aligndefine dso_local 132 @f1 (18* %0) { %2 = call i32 @f2 (18* %0, 18* getelementptr inbounds ( [14 x 18], [14 x 18] * } -20, @anon. 2ef3bda806391c61822366a2a59f2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, 164 0, 164 0)) ret 132 %(a) 불완전한 정보로 인한 실패. -name-anon-globals 패스는 모듈 이름을 사용하여 해시를 계산합니다. 이것이 없으면 모델은 무작위 해시를 환각합니다. -inst combine을 사용하여 다음 LLVM-IR을 최적화하세요. @var 12 = external dso_local 글로벌 164, align@var 13 외부 dso_local 글로벌 132, align@var_14 = 외부 dso_local 글로벌 132, aligndefine dso_local void @f1 (164 %arg) { tmp = 할당 164, 정렬 저장소 164 %arg, 164* %tmp, aligntmp1 = 로드 164, 164* %tmp, aligntmp2 = 하위 164 0, %tmptmp3 = 하위 164 0, %tmp 저장소 164 %tmp3, 164* @var_12, 정렬 저장소 164 %arg, 164*2B @var_12, 정렬 저장소 164 0, 164* @var 12, 정렬 저장소 i32 1, 132* @var_13, alignstore 132 0, 132* @var_14, alignret void -Oz를 사용하여 다음 LLVM-IR을 최적화합니다. %s1 = type { 132 } @&quot;llvm.used&quot; = 전역 추가 [1 x 18*] [18* 비트캐스트(i32(%s1*) * @fl에서 18*로)], 섹션 &quot;llvm.metadata&quot; define dso_local 132 @f1 (%sl* %0) { %alloca 132, alignalloca %sl*, align%alloca 132, alignstore %s %0, %s1** %3, align5 로드 sl*, %s1** %3, align%6 tail call 132 @f2 (%s1* %5) 스토어 i32 %6, 132* %4, 정렬 4. %7=로드 132, 132* %4, 정렬 4. %8 = icmp slt i32 %7,br il %8, 레이블 %9, 레이블 %9: %%%tail() %%%10 132, 132* %4 로드, 정렬저장 132 %10, 132* %2, 정렬br 레이블 %%%&lt;11: %%%저장 132 0, 132* %2, 정렬 4. br 레이블 %ret 12: } %13 132, 132* %2 로드, 정렬ret 132 %%(a) 모델 프롬프트. tail() %
--- RELATED WORK ---
성능을 위한 컴파일러 패스 순서는 수십 년 동안 활용되어 왔습니다[26, 52, 53]. 수년에 걸쳐 머신 러닝을 사용하는 여러 가지 접근 방식이 있었습니다[18-20, 39, 54, 55]. 컴파일러에서 머신 러닝을 적용하는 것은 패스 순서에 국한되지 않으며 다른 많은 문제에 적용되었습니다[17, 56-59]. 아무도 패스 순서 문제에 LLM을 적용한 적이 없으며, 우리가 처음입니다. 신경망 기계 번역은 언어 모델을 사용하여 코드를 한 언어에서 다른 언어로 변환하는 새로운 분야입니다. 이전 예로는 C에서 어셈블리로 컴파일[11], 어셈블리에서 C로 컴파일[36, 60], 소스 간 변환[10]이 있습니다. 이러한 작업에서는 코드의 정확성을 보장할 수 없습니다. 저희 작업에서는 코드 생성을 보조 학습 과제로만 사용합니다. 정확성은 컴파일러에서 제공합니다. 언어 모델은 코딩 과제에 널리 채택되었지만 컴파일러 IR 수준에서 작동하는 모델은 거의 없습니다. Gallagher et al. 영어: 코드 취약점 식별[61]을 목적으로 LLVM-IR에서 ROBERTA 아키텍처를 훈련하고 Transcoder-IR[12]은 소스 간 변환을 위한 피벗 포인트로 LLVM-IR을 사용합니다. 둘 다 우리처럼 최적화를 위해 LLM을 사용하지 않습니다. CodeBERT[62], GraphCodeBERT[63], CodeT5[64]를 포함하여 많은 언어 모델이 소스 코드에서 훈련되었으며, 이는 코드 검색, 코드 요약, 문서 생성을 포함한 여러 작업을 수행하도록 훈련되었습니다. 소스 코드에서 훈련된 LLM은 프로그램 퍼징[13, 14, 65], 테스트 생성[15], 자동화된 프로그램 복구[66–68]에도 사용되었습니다. 언어 모델에 대해 많은 유용한 응용 프로그램이 탐색되었지만, 이는 LLM이 특별히 코드 최적화에 사용된 최초의 작업입니다. 대부분의 LLM은 최소한 부분적으로 코드에서 훈련됩니다[3, 5, 25, 69]. 일부 LLM은 일반 모델과 유사하게 훈련되지만 특히 프로그래밍 언어를 대상으로 하며 Copilot [70]을 구동하는 Codex [8]와 같은 코드 완성에 사용될 수 있습니다.중간 채우기 기능의 도입은 실제 코드 완성 사용 사례에 특히 유용하며 InCoder [6], SantaCoder [4], StarCoder [1] 및 Code Llama [9]와 같은 최근 코드 모델에서 일반화되었습니다.Code Llama는 또한 지침을 따르고 코드를 생성하고 기능을 설명하도록 훈련되었습니다.이러한 모델의 멀티 테라바이트 훈련 코퍼스에는 일부 어셈블리가 포함되어 있지만 컴파일러 도메인에서 LLM의 가치에 대한 집중적인 탐구가 커뮤니티에 가치가 있을 것이라고 믿습니다.이 논문은 그것을 제공하는 것을 목표로 합니다.VIII. 결론 우리는 코드 최적화를 위한 LLM에 대한 첫 번째 단계를 제시합니다.우리는 보이지 않는 LLVM-IR에 대한 좋은 최적화 전략을 예측할 수 있는 모델을 구성합니다. 결과는 유망하지만, 우리는 작은 프로그램 조각에 대한 작업으로 제한하는 시퀀스 길이와 최적화 결과를 예측하는 모델의 능력을 제한하는 산술 추론에서 문제에 직면합니다.우리는 연구 커뮤니티가 간단한 최대 우도 코드 생성을 위한 LLM을 넘어 성능 인식 코드 최적화로 나아가도록 영감을 주기를 바랍니다.참고문헌 [16] [17] OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. &quot;MLGO: a Machine Learning Guided Compiler Optimizations Framework&quot;. in: arXiv:2101.04808 (2021). [18] Z. Wang and M. O&#39;Boyle. &quot;Machine Learning in Compiler Optimisation&quot;. in: arXiv:1805.03441 (2018). [19] H. Leather and C. Cummins. &quot;컴파일러에서의 머신 러닝: 과거, 현재, 미래&quot;. FDL에 게재됨. 2020. [20] Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, Y. Tian. &quot;코어셋과 정규화된 값 예측을 사용하여 컴파일러 패스 순서 학습&quot;. ICML에 게재됨. 2023. [21] C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O&#39;Boyle, H. Leather. &quot;ProGraML: 데이터 흐름 분석 및 컴파일러 최적화를 위한 그래프 기반 프로그램 표현&quot;. ICML에 게재됨. 2021. [1] R. Li, LB Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. &quot;StarCoder: 소스가 함께하길 바랍니다!&quot; [22] In: arXiv:2305.06161 (2023). [2] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, AD Lago, T. Hubert, P. Choy 등 &quot;AlphaCode를 사용한 경쟁 수준 코드 생성&quot;. 에서: 과학 378.6624 (2022). [3] 오픈AI. “GPT-4 기술 보고서”. 출처: arXiv:2303.(2023). [4] LB Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, CM Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, LK Umapathi, CJ Anderson, et al. &quot;SantaCoder: 별에 손대지 마세요!&quot; 출처: arXiv:2301.03988 (2023). [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. &quot;PaLM: 경로로 언어 모델링 확장&quot; 출처: arXiv:2204.02311 (2022). [6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer 및 M. 루이스. &quot;InCoder: 코드 채우기 및 합성을 위한 생성 모델&quot;. In: arXiv:2204.05999 (2023). [7] S. Gunasekar, Y. Zhang, J. Aneja, CCT Mendes, AD Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah 등. &quot;교과서는 당신에게 필요한 전부입니다&quot;. In: arXiv:2306.11644 (2023). [8] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri 외. &quot;코드로 훈련된 대규모 언어 모델 평가&quot;. arXiv:2107.03374(2021)에 게재. [9] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, XE Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov 외. &quot;Code Llama: Open Foundation Models for Code&quot;. arXiv:2308.12950(2023)에 게재. [10] M.-A. Lachaux, B. Roziere, L. Chanussot, G. Lample. &quot;프로그래밍 언어의 비지도 번역&quot;. arXiv:2006.03511(2020)에 게재. [11] J. Armengol-Estapé 및 MF O&#39;Boyle. &quot;C에서 x86으로의 번역 학습: 신경 컴파일 실험&quot;. arXiv:2108.07639(2021)에 게재. [12] M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut 및 G. Synnaeve. &quot;컴파일러 표현을 사용한 코드 번역&quot;. arXiv:2207.03578(2022)에 게재. [13] G. Ye, Z. Tang, SH Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang 및 Z. Wang. &quot;심층 컴파일러 퍼징을 통한 JavaScript 엔진에 대한 자동화된 적합성 테스트&quot;. PLDI에 게재. 2021. Y. Deng, CS Xia, H. Peng, C. Yang, L. Zhang. &quot;대규모 언어 모델은 제로샷 퍼저입니다. 대규모 언어 모델을 통한 퍼징 딥러닝 라이브러리&quot;. ISSTA에서. 2023. [14] [15] M. Schäfer, S. Nadi, A. Eghbali, F. Tip. &quot;대규모 언어 모델을 사용한 적응형 테스트 생성&quot;. arXiv:2302에서.(2023). C. Lattner와 V. Adve. &quot;LLVM: 평생 프로그램 분석 및 변환을 위한 컴파일 프레임워크&quot;. CGO에서. 2004. [23] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, S. Paul. &quot;언어 모델을 사용한 학습의 한계&quot;. arXiv:2306에서.(2023). [24] J. Qian, H. Wang, Z. Li, S. Li 및 X. Yan. 영어: &quot;산술 및 기호 귀납에서 언어 모델의 한계&quot;. arXiv:2208.05051(2022)에 게재됨. [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 외. &quot;Llama 2: Open Foundation 및 Fine-Tuned Chat Models&quot;. arXiv:2307.09288(2023)에 게재됨. [26] GG Fursin, MFP O&#39;Boyle, PMW Knijnenburg. &quot;반복 컴파일 평가&quot;. LCPC에 게재됨. 2005. [27] C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, H. Leather. &quot;CompilerGym: AI 연구를 위한 강력하고 성능이 뛰어난 컴파일러 최적화 환경&quot;. CGO에서. 2022. [28] D. Kocetkov, R. Li, LB Allal, J. Li, C. Mou, CM Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. &quot;스택: 허용 라이선스가 부여된 3TB 소스 코드&quot;. arXiv:2211.15533(2022)에서. [29] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. &quot;파일: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트&quot;. arXiv:2101.00027(2020)에 게재됨. [30] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt. &quot;CodeSearchNet 챌린지: 의미 코드 검색 상태 평가&quot;. arXiv:1909.09436(2019)에 게재됨. A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, P. Rosso. &quot;소스 코드의 저자 식별(AI-SOCO)에 대한 PAN@FIRE 2020 과제 개요&quot;. FIRE에 게재됨. 2020. [31] [32] J. Armengol-Estapé, J. Woodruff, A. Brauckmann, JW d. S. Magalhães, M. O&#39;Boyle. &quot;ExeBench: 실행 가능한 C 함수의 ML 규모 데이터 세트&quot;. MAPS에서. 2022. [33] L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin. &quot;프로그래밍 언어 처리를 위한 트리 구조에 대한 합성 신경망&quot;. AAAI에서. 2016. [34] X. Yang, Y. Chen, E. Eide, J. Regehr. &quot;C 컴파일러의 버그 찾기 및 이해&quot;. PLDI에서. 2011. [35] V. Livinskii, D. Babokin, J. Regehr. &quot;YARPGen을 사용한 C 및 C++ 컴파일러의 임의 테스트&quot;. OOPSLA에서. 2020. [36] J. Armengol-Estapé, J. Woodruff, C. Cummins, MF O&#39;Boyle. &quot;SLaDe: 최적화된 어셈블러를 위한 휴대용 소규모 언어 모델 디컴파일러&quot;. arXiv에서:2305.12520(2023)에서. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. &quot;주의만 있으면 됩니다&quot;. NeurIPS에서(2017).[38] P. Gage. &quot;데이터 압축을 위한 새로운 알고리즘&quot;. C Users Journal 12.2(1994)에 실림. [39] A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, I. Stoica. &quot;AutoPhase: 심층 강화 학습을 통한 랜덤 포레스트에서 HLS 위상 순서 조정&quot;. MLSys에 실림. 2020. [56] AH Ashouri, M. Elhoushi, Y. Hua, X. Wang, MA Manzoor, B. Chan, Y. Gao. &quot;MLGOPerf: 성능 최적화를 위한 ML 가이드 인라이너&quot;. arXiv:2207.08389(2022)에 실림. A. Haj-Ali, NK Ahmed, T. Willke, S. Shao, K. Asanovic, I. Stoica. &quot;NeuroVectorizer: 심층 강화 학습을 통한 종단 간 벡터화&quot;. CGO에서. 2020. [57] [40] I. Loshchilov 및 F. Hutter. &quot;분리된 가중치 감소 규칙&quot;. [58] C. Cummins, P. Petoumenos, Z. Wang 및 H. Leather. &quot;Endlarization&quot;. arXiv:1711.05101 (2017)에서. [41] K. Papineni, S. Roukos, T. Ward 및 W.-J. Zhu. &quot;BLEU: A
--- EXPERIMENT ---
코드 추론을 위한 LLM의 잠재력과 한계를 더욱 특성화하기 위한 s 및 코드 예제. 전반적으로 우리는 그들의 효능이 놀랍다고 생각하며 이러한 결과가 커뮤니티에 흥미로울 것이라고 생각합니다. 학습 단계 프롬프트 다음 LLVM-IR에서 명령어 수를 줄이기 위해 실행할 패스를 알려주세요: <code>define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { %3 alloca 132, align%alloca 132, alignstore 132 %0, 132* %3, align 4. store 132 %1, 132* %4, align%%load 132, 132* %3, alignicmp eq 132 %5,br 11 %6, label %10, label %7: %8 load 132, 132* %3, align%9 icmp eq 132 %8,br 11 %9, label %10, label %Answer Run passes -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading to reduce instruction count from 15 to 4: <code>define dso local void @&quot;iowrite_single&quot; (132 %0, 132 %1) switch 132 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ._crit_edge1: %call 132 @&quot;outb&quot; (132 %0, 132 %1) br label &quot;._crit_edge2&quot; ._crit_edge2: ret void } 10: %load 132, 132* %3, align%12 load 132, 132* %4, align%13 call 132 @&quot;outb&quot; (i32 %11, 132 %12) br label %14: ret void } declare dso local i32 @&quot;outb&quot; (132, 132)</code></code> 추론 단계 답변 실행 패스 -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading 컴파일러 최적화 코드 define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { switch i32 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ... ret void } 그림 1: 학습 및 추론 중 모델 입력(프롬프트)과 출력(답변)을 보여주는 접근 방식 개요. 프롬프트에는 최적화되지 않은 코드가 포함되어 있습니다. 답변에는 최적화 패스 목록, 명령어 수 및 최적화된 코드가 포함되어 있습니다. 추론 중에는 컴파일러에 제공하는 최적화 패스 목록만 생성하여 최적화된 코드가 올바른지 확인합니다. 표 I: 훈련 데이터. 각 LLVM-IR 기능은 자동 조정되어 (Prompt, Answer) 쌍을 만드는 데 사용됩니다. n 토큰 열은 Llama 2[25] 토크나이저를 사용하여 프롬프트가 인코딩될 때 토큰 수를 보여줍니다. 표 II: 테스트 데이터. n개 함수 n개 함수 손으로 쓴 합성 총 610,389,1,000, 최적화되지 않은 명령어 수 8,417,13,775,16,411, 최적화되지 않은 명령어 수 -Oz 디스크의 명령어 수 크기 n개 토큰 653.5MB 352.3MB 1.0GB 214,746,158,435,373,181, AI-SOCO [31] ExeBench [32] POJ-104 [33] Transcoder [12] 8,26,97,386,47,181,8,4,17,289,129, CSmith [34] YARPGen [35] 33,647,138,12,285,144, 총 100,1,716,645,II. LLMS를 사용한 패스 순서 지정 이 작업에서는 컴파일러 패스 순서를 목표로 합니다. 패스 순서 지정 작업은 컴파일러에서 사용 가능한 최적화 변환 패스 세트에서 특정 입력 코드에 대해 최상의 결과를 생성하는 패스 목록을 선택하는 것입니다. 패스 순서를 조작하면 런타임 성능과 코드 크기에 상당한 영향을 미치는 것으로 나타났습니다[19, 26]. 이 작업에 대한 머신 러닝 접근 방식은 이전에 좋은 결과를 보였지만 다양한 프로그램 간에 일반화하는 데 어려움을 겪었습니다[27]. 이전 작업에서는 일반적으로 다양한 구성을 시도하고 최상의 성능 옵션을 찾기 위해 수십 또는 수백 번 새 프로그램을 컴파일해야 하므로 실제 사용에는 비실용적이었습니다. 충분한 추론 능력을 갖춘 대규모 언어 모델이라면 이것이 필요 없이도 좋은 최적화 결정을 내리는 법을 배울 수 있을 것이라고 가정했습니다. 코드에 대한 LLM에 대한 대부분의 이전 작업은 Python과 같은 소스 언어에서 작동합니다. 대신 패스 순서 지정 문제의 경우 중간 표현(IR)이라고 하는 컴파일러 어셈블리의 하위 수준에서 추론이 필요합니다. LLM 사전 학습을 위한 소스 언어의 큐레이트된 데이터 세트가 있지만(예: [28–30]), 컴파일러 IRS는 이러한 데이터 세트의 상당 부분을 차지하지 않으며 ChatGPT와 같은 모델은 이해에 약간의 가능성을 보여주지만 IR에 대한 추론 능력은 소스 언어에 비해 훨씬 떨어집니다. 이전 연구[17, 27]와 마찬가지로 IR 명령어 수를 바이너리 크기에 대한(완벽하지 않은) 프록시로 사용하여 코드 크기에 대한 LLVM 패스 순서를 최적화하는 것을 목표로 합니다. 이 접근 방식은 선택한 컴파일러 및 최적화 메트릭과 무관하며 앞으로 런타임 성능을 목표로 할 것입니다. 지금으로서는 코드 크기를 최적화하면 학습 데이터 수집이 간소화됩니다. A. 프롬프트 모델에 최적화되지 않은 LLVM-IR(예: clang 프런트엔드에서 내보내는 것)을 제시하고 이에 적용해야 하는 최적화 패스 목록을 생성하도록 요청합니다. 그림은 입력 프롬프트와 출력 텍스트의 형식을 보여줍니다. 이 작업에서 우리는 LLVM 10을 목표로 하고 opt의 최적화 플래그를 사용합니다. 선택할 수 있는 최적화 패스는 122개이며 패스는 단일 시퀀스에서 두 번 이상 선택할 수 있습니다. 또한 패스 목록당 한 번만 발생할 수 있는 6개의 메타 플래그(-00, -01, -02, -03, -Oz 및 -Os)도 포함합니다. 패스 목록은 길이에 제한이 없지만 실험에서 일반적으로 약 1018의 조합 검색 공간에 대해 최대 9개의 패스 길이를 발견했습니다. 그림 1에서 볼 수 있듯이 두 가지 보조 작업도 포함합니다. i) 최적화가 적용되기 전과 후에 코드의 명령어 수를 생성하고 ii) 최적화가 적용된 후에 출력 IR을 생성합니다. 이를 통해 코드 최적화의 메커니즘에 대한 심층적인 이해를 강제하여 더 나은 패스 순서 지정 결정을 내릴 수 있을 것이라고 가정합니다. 섹션 VB에서 실험적으로 이를 검증합니다. 모델은 명령어 수와 최적화된 IR을 생성하도록 학습되었지만 배포에는 이러한 보조 작업이 필요하지 않습니다. 컴파일러를 사용하여 실행하는 패스 목록만 생성하면 됩니다. 따라서 우리는 모델의 출력이 신뢰할 수 있어야 하는 기술을 괴롭히는 정확성의 문제를 회피합니다[10–12, 36].B. LLVM-IR 정규화 우리는 다음 규칙을 사용하여 LLM을 훈련하는 데 사용되는 LLVM-IR을 정규화합니다.우리는 주석을 버리고, 메타데이터와 속성을 디버깅하고, 줄바꿈은 유지하지만 다른 공백을 표준화하고 들여쓰기를 제거하는 사용자 정의 렉서를 통해 IR을 공급하여 일관된 공백을 보장합니다.우리는 LLM의 제한된 입력 크기(섹션 III-A)를 최대한 활용하기 위해 LLVM-IR의 길이를 줄이기 위해 이를 수행합니다.그림 1의 코드는 이러한 방식으로 처리되었습니다.III.모델 우리는 유비쿼터스 변압기 아키텍처[37]를 사용합니다.변압기는 고정된 크기의 컨텍스트 창에서 셀프 어텐션을 사용하는 인공 신경망입니다.입력 텍스트는 먼저 단어와 하위 단어 단위로 토큰화됩니다. 이것들은 연속 벡터 표현에 내장되어 변환기의 인코더에 입력으로 제공되며, 여기서 셀프 어텐션 메커니즘은 토큰 간의 문맥적 관계를 포착하여 모델이 입력 텍스트의 의미 구조를 이해하고 처리하도록 장려합니다. 출력 텍스트는 한 번에 하나의 토큰을 반복적으로 생성하여 생성됩니다. 디코더는 이전에 생성된 토큰과 함께 인코딩된 입력을 가져와 셀프 어텐션을 사용하여 시퀀스의 다음 토큰을 예측합니다. 디코딩하는 동안 탐욕적으로 샘플링하여 가장 가능성 있는 토큰 시퀀스를 선택합니다. 이 프로세스는 시퀀스 끝 토큰이 생성되거나 사전 정의된 최대 길이에 도달할 때까지 계속됩니다. A. 모델 아키텍처 Llama 2 [25]와 동일한 모델 아키텍처와 바이트 쌍 인코딩(BPE) [38] 토크나이저를 사용하지만 모델을 처음부터 학습합니다. Llama 2 구성 중 가장 작은 것인 어텐션 헤드, 4,096개의 숨겨진 차원, 32개의 레이어를 사용하여 총 7B개의 매개변수를 사용합니다. (프롬프트, 답변) 쌍의 최대 길이는 시퀀스 길이에 의해 정의됩니다. 이 작업에서 우리는 시퀀스 길이 개선 -Oz 6% 4% 2% 자동 튜너 우리의 접근 방식 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le50% 40% w 30% 20% MAPE 10% (a) 생성된 패스 목록의 성능.최적화되지 않은(입력) 코드 최적화된(출력) 코드 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le1.0.0.0.40.2(b) 명령어 수 예측 정확도.BLEU 코드는 정확한 일치를 컴파일합니다.0.0.0.0.0.1.1.#. 훈련 토큰 le(c) 모델 최적화된 코드 메트릭.그림 2: 훈련 중 홀드아웃 검증 세트에 대한 성능.우리는 250개 훈련 단계(1억 3,100만 훈련 토큰)마다 성능을 평가합니다. -Oz와의 동등성은 393M 토큰에서 달성되고 10.9B 토큰에서 최고 성능을 달성합니다. 2,048개 토큰. Llama 2 토크나이저는 LLVM-IR을 인코딩할 때 토큰당 평균 2.02자를 달성하므로 이는 2KB에서 학습할 수 있는 가장 긴 LLVM-IR에 대한 대략적인 상한을 제공합니다(2KB 프롬프트와 2KB 답변은 약 2,048개 토큰이기 때문). B. 학습 데이터 우리는 표 I에 요약된 최적화되지 않은 LLVM-IR 함수의 대규모 코퍼스를 조립했습니다. 공개적으로 사용 가능한 수기 C/C++ 코드의 데이터 세트에서 함수를 추출하고 C/C++ 컴파일러 테스트 생성기에서 생성한 합성 코드로 이를 보완했습니다. 전체적으로 학습 코퍼스는 1,000,000개의 중복 제거된 IR 함수로 구성되어 총 373M 학습 토큰입니다. 우리는 2,048토큰 시퀀스 길이에 맞출 수 있는 데이터 양을 극대화하기 위해 전체 모듈이 아닌 개별 IR 기능 수준에서 작업합니다. 가장 작은 명령어 수를 생성하는 최적화 패스 목록을 찾기 위해 자동 튜닝을 사용합니다. 우리의 자동 튜너는 Liang et. al. [20]의 작업에서 영감을 받아 랜덤 검색과 함수 간의 전체 대 전체 결과 브로드캐스팅을 결합합니다.표 III: 표 II의 보이지 않는 LLVM-IR 함수 테스트 세트에서 패스 순서를 지정하는 다양한 접근 방식의 성능. 모든 메트릭은 -Oz에 대한 것입니다. 절감된 명령어는 개선된 함수에 대해 합산되고 회귀된 명령어는 회귀된 함수에 대해 합산됩니다. 전반적인 개선은 -Oz에 대한 총 명령어 수 절감 합계입니다. 자동 튜너는 최상의 성능을 달성하지만 25억 개의 추가 컴파일(949 CPU-일)이 필요합니다. 우리의 접근 방식은 컴파일러를 한 번도 호출하지 않고도 자동 튜너의 이득의 60%를 달성합니다. 추가 기능 컴파일 개선된 기능 회귀된 명령어 저장된 명령어 회귀된 전체 개선 Autotuner AutoPhase [39] Coreset-NVP [20] 우리의 접근 방식 2,522,253,6,30,5.03% 4,500,1,8,6,32,-3.85% 442,3,6,16,28,-1.88%4,21,3,3.01% 표 IV: &quot;-Oz 백업&quot;을 사용하여 표 III의 모델 확장. 모델이 -Oz 이외의 통과 목록을 예측하는 경우 -Oz도 평가하고 최상의 것을 선택합니다. 이렇게 하면 추가 컴파일을 희생하여 -Oz에 대한 회귀가 방지됩니다. 추가 컴파일 AutoPhase [39] Coreset-NVP [20] 4,600,542, 우리의 접근 방식 5, 전체 개선 1.02% 2.55% 3.52% 각 기능에 대해 고정된 시간(780초) 동안 무작위 검색을 실행한 다음, 명령어 수에 기여하는지 확인하기 위해 무작위로 선택한 개별 패스를 반복적으로 제거하여 최상의 패스 목록을 최소화합니다. 기여하지 않으면 삭제합니다. 각 함수에서 이를 수행한 후 고유한 최상의 패스 목록 세트를 집계하여 다른 모든 함수에 브로드캐스트합니다. 따라서 패스 목록이 한 함수에서 잘 작동하는 것으로 확인되면 다른 모든 함수에서 시도합니다. 전체적으로 자동 튜너는 각 학습 프로그램을 평균 37,424번 컴파일하여 -Oz에서 제공하는 컴파일러의 기준 고정 패스 순서에 비해 명령어 수 감소에서 5.8%의 개선을 달성했습니다. 우리의 목적상 이 자동 튜닝은 각 함수의 최적화를 위한 황금 표준 역할을 합니다. 자동 튜너가 발견한 명령어 수 절감은 상당하지만 이러한 성과를 달성하기 위한 계산 비용은 9,016 CPU 일이었습니다. 이 작업의 목표는 컴파일러를 수천 번 실행할 필요가 없는 예측 모델을 사용하여 자동 튜너 성능의 일부를 달성하는 것입니다. C. 학습 무작위로 초기화된 가중치에서 시작하여 64개의 V100에서 30,000단계로 모델을 학습하여 총 학습 시간이 620GPU 일이었습니다. ₁ 및 B2 값이 각각 0.9 및 0.95인 AdamW 옵티마이저[40]를 사용합니다. 1,000개의 워밍업 단계, 1e-5의 최대 학습 속도, 최대 학습 속도의 1/10인 최종 학습 속도를 갖는 코사인 학습 속도 일정을 사용합니다. 256의 배치 크기를 사용했고 각 배치에는 총 15.7B개의 학습 토큰에 대해 524,288개의 토큰이 포함됩니다. 전체 30,000단계의 학습은 7.7에포크(학습 코퍼스에 대한 반복)입니다. 학습하는 동안 학습 세트와 동일한 방식으로 처리된 1,000개의 보이지 않는 IRS의 홀드아웃 검증 세트에서 모델을 평가했습니다. 모든 단계를 평가합니다. IV. 평가 이 섹션에서는 보이지 않는 코드에 대한 패스 목록을 생성하고 최적화를 올바르게 수행하는 모델의 능력을 평가합니다.A. 학습 결과 그림 2는 보이지 않는 LLVM-IR 함수 1,000개의 홀드아웃 검증 세트에서 평가할 때 학습 중 성능을 보여줍니다.최고 검증 성능은 모델이 109억 학습 토큰에서 달성했습니다.최고 성능에서 모델 생성 패스 시퀀스를 사용하여 최적화된 코드는 컴파일러의 기본 제공 패스 순서(-Oz)를 사용하여 최적화했을 때보다 4.4% 적은 명령어를 포함합니다.자동 튜너는 5.6%의 더 큰 명령어 수 감소를 달성하지만 이를 위해 검증 세트를 2,700만 번 컴파일해야 했습니다.모델은 컴파일러를 한 번도 호출하지 않고 예측을 수행합니다.그림 2b는 예측된 입력 및 출력 명령어 수의 오류를 보여줍니다.최적화되지 않은 코드에 대한 명령어 수 예측은 거의 완벽한 정확도에 빠르게 접근합니다.출력 명령어 수 예측은 더 어려워서 평균 평균 백분율 오류(MAPE)가 5.9%에 도달합니다. 그림 2c는 세 가지 지표를 사용하여 생성된 코드의 품질을 평가합니다.BLEU[41] 점수는 모델 생성 코드와 생성된 패스 목록을 사용하여 컴파일러에서 생성한 참조 기준 진실 코드 간의 유사성을 보여줍니다.코드 컴파일은 모델 생성 코드가 오류 없이 컴파일되는 빈도입니다.정확한 일치는 생성된 패스 목록을 사용하여 최적화할 때 모델 생성 코드가 컴파일러 생성 코드와 문자별로 일치하는 빈도(즉, BLEU=1인 횟수)를 추적합니다.최대 성능에서 모델은 오류 없이 컴파일되는 코드를 생성하는 인상적인 90.5% 비율을 달성합니다.또한 BLEU 점수 0.952는 모델 최적화된 코드가 컴파일러의 코드와 매우 유사하고 정확한 일치 빈도가 70%임을 보여줍니다.비교를 위해 최적화되지 않은 코드를 출력에 복사하는 기준선은 BLEU 점수 0.531과 정확한 일치 빈도 0%를 달성하여 이처럼 높은 점수를 달성하려면 입력 코드를 상당히 조작해야 함을 보여줍니다. 훈련이 끝날 무렵 검증 세트의 성능은 정점에 도달했습니다. 가장 성능이 좋은 체크포인트를 사용하고 나머지 평가에서는 100배 더 큰 규모의 평가로 전환합니다. B. 최신 기술과의 비교 이 실험에서는 기준선과 비교하여 LLM의 통과 목록을 예측하는 능력에 대한 대규모 평가를 수행합니다. 빈도(로그) nالسيسسيد السيسي السبيسZOUAб -instcombine -mem2reg -reg2mem -simplifycfg -memcpyopt Η το POJS -02-Os Hoop-rotate -03-newgvn -loop-deletion. -jump-threading. tailcallelim -로드-스토어-벡터라이저 early-csegvn-hoist – -div-remp -early-cse-memssa -추측-실행 -sip-벡터라이저 싱크 재연결 -00-dse-상관-전파 병합 반환 -break-crit-edges -globalopt -nary-재연결 -instsimplify -loop-simplifycfg -simple-loop-unswitch -loop-unswitch -loop-unroll -aggressive-instcombine -die -bdce -scalarizer -dce → adce Hoop-reroll -flattencfg Hoop-vectorize-constprop -functionattrs woipi-doojAutotuner 우리의 접근 방식 -irce → predicat -mldst-motion Hoop-predication -attributor Hoop-instsimplify hotcoldsplit -Hoop-load-elim -coro-elide -elim-avail-extern -loop-reduce -Hoop-interchange -coro-cleanup Hower-constant-intrinsics -loop-versioning -loop-versioning-licm -partial-inliner -pgo-memop-opt + strip + 1 2 3 4 5 6 7 8 9패스 목록 길이 그림 3: 100,000개 테스트 프로그램 각각에 대한 패스 목록에서 패스가 발생하는 빈도(왼쪽)와 패스 목록의 길이(오른쪽). -Oz는 자동 튜너의 시작점이며 지배적인 결과이며 자동 튜닝된 테스트 프로그램의 93.2%에서 가장 잘 발견된 결과이고 더 긴 시퀀스의 일부로 패스 목록의 추가 0.6%에 나타납니다. 모델에서 생성된 패스 분포는 자동 튜너를 추적하지만 -Oz(94.3%)를 약간 과대 예측하고 자동 튜너가 훈련 세트에는 사용했지만 테스트 세트에는 사용하지 않은 9개의 패스를 포함합니다. 결과는 자동 튜너 주파수가 감소하는 순서로 정렬됩니다. define 132 @f1 (i8 %0) { 2 alloca 132, align3 alloca 18, alignstore 18 %0, 18* %3, align4 load 18, 18* %3, align%5= zext 18 %4 to% 6 = icmp sge 132 %5,br il %6, label %7, label7: 8 load 18, 18* %3, align% 9 = zext 18 %8 to10 icmp sle 132 %9,br il 10, label %11, label %11: 12 load 18, 18* %3, align13 zext 18 %12 to<snip 21 lines...> 33: 34 132 로드, 132* %2, alignret 132 %define 132 @f1 (i8 %0) { %2 = zext 18 %0 to%.off 18 %0,3 추가 icmp ult i8 %.off,br il 3, 레이블 %4, 레이블 %4: %add nsw 132 %2,br 레이블 %6: %.reload16.off = nsw 132 %2,7 추가 icmp ult 132 %.reload16.off,br il 7, 레이블 %10, 레이블 %8: 9 icmp eq 18 %0,%. = select il %9, 132 26, 132br label %10: %.0.reg2mem.0 = phi i32 [%5, %4], [., 8], [%.reload16.off, %6] ret 132.0.reg2mem.define 132 @f1 (i8 %0) { %2 =zext 18 %0에서 %.off로 18 %0,%3 icmp ult 18 %.off,br il %3, label %6, label %._ crit_edge ._crit_edge: %.off24 = 18 %0,%4 icmp ult i8 %.off24,br il 4, label %6, label %.를 추가합니다. crit_edge._crit_edge9: 5 icmp eq 18 %0,spec.select = select il %5, 132 26, 132ret6: } spec.select %.sink = phi 132 [191, %1], [159,._crit_edge] 7 add nsw i32 %.sink, %ret 132 %} (a) 입력 코드(39개 명령어). (b) 패스를 사용한 자동 튜닝 코드(14개 명령어): -reg2mem -inst combine -Os -01. (c) 모델 최적화 코드(13개 명령어) 및 패스 목록: -reg2mem -simplifycfg -mem2reg -jump-threading -Os. 목록 1: 이 코드를 이전에 본 적이 없음에도 불구하고 모델이 자동 튜너보다 더 나은 패스 목록을 제안하는 예제 IR 함수. 이 함수의 경우 자동 튜너는 26,000개의 다른 패스 순서를 시도했습니다. 모델에서 생성한 패스 목록은 1,000,000개의 예제로 구성된 학습 세트에서 5번 나타납니다.데이터 세트 평가를 위해 광범위한 벤치마크 데이터 세트를 집계하여 표 II에 요약했습니다. 학습한 것과 동일한 IR 함수를 중복 제거하고 제외합니다.테스트 데이터는 코딩 대회(AI-SOCO[31], POJ-104[33]), 컴파일러 테스트 케이스 생성기(CSmith[34], YARPGen[35]) 및 기타 공개적으로 사용 가능한 코드(ExeBench[32], Transcoder[12])를 포함한 다양한 도메인의 코드로 구성되어 있습니다.기준선 접근 방식을 AutoPhase[39], Coreset-NVP[20] 및 Autotuner의 세 가지 기준선과 비교합니다.AutoPhase[39]는 고정 길이 에피소드에서 누적 명령어 수 절감을 극대화하는 최적화 패스 시퀀스를 선택하기 위해 Proximal Policy Optimization[42]을 사용하여 에이전트를 학습시키는 강화 학습 접근 방식입니다. 각 단계에서 최적화되는 프로그램은 명령어 수와 기타 속성의 56차원 벡터로 에이전트에 표현됩니다.우리는 [39]의 환경을 복제하지만 에이전트가 100,000개의 에피소드로 훈련되는 [27]의 구현 및 확장된 훈련 체제를 사용합니다.우리는 언어 모델(표 I)과 동일한 데이터에서 에이전트를 훈련하고 보류 검증 세트에서 훈련하는 동안 주기적으로 에이전트 성능을 평가합니다.이전 작업에서와 마찬가지로 45의 동작 공간과 에피소드 길이를 사용합니다.Coreset-NVP[20]는 반복적 검색과 학습된 비용 모델을 결합하는 기술입니다.먼저 17,500개의 벤치마크에서 탐욕적 검색을 실행하여 최상의 패스 목록의 핵심 세트를 결정합니다.그런 다음 그래프 합성곱 네트워크에서 처리한 ProGraML[21] 그래프를 프로그램 표현으로 사용하여 이 검색 결과에 대해 신경 값 예측(NVP)을 훈련합니다. 추론에서 Coreset-NVP는 정규화된 보상을 예측하고 가장 높은 정규화된 보상을 갖는 처음 몇 개의 패스 시퀀스를 시도합니다.-5% -10%0% 30% 15% AutoPhase AutoPhase Coreset-NVP 10% 20% Autotuner Coreset-NVP Autotuner 우리의 접근 방식 우리의 접근 방식 5% 10% ExeBench Transcoder CSmith YARPGen0% -10% TUnoptimized 명령어 수 그림 5: 입력 크기에 따른 -Oz 대비 개선.더 큰 코드는 더 많이 최적화합니다.그림 4: 데이터 세트에 따른 -Oz 대비 개선.수작업으로 작성한 코드는 더 많이 최적화합니다.보상.각 벤치마크에 대해 시도할 수 있는 총 패스 수는 이전 연구에 따라 45입니다.저자가 제공한 모델 가중치를 사용하여 테스트 세트에서 추론을 수행합니다.마지막으로, 이를 학습 데이터를 생성하는 데 사용한 Autotuner와 비교합니다.섹션 III-B에서 설명한 대로 학습 데이터와 동일한 방식으로 테스트 데이터 세트를 자동 튜닝했습니다. 결과 표 III은 결과를 요약한 것입니다. 우리의 접근 방식은 모든 데이터 세트에서 -Oz, AutoPhase, Coreset-NVP보다 성능이 뛰어납니다. 전반적으로 자동 튜너에 제공된 수천 번의 최적화 시도를 통해 가장 성능이 좋은 패스 목록을 발견할 수 있습니다. AutoPhase와 Coreset-NVP는 모두 -Oz보다 성능이 뛰어나지만 회귀가 많아 명령어 수에 전반적으로 부정적인 영향을 미치는 패스 목록을 식별할 수 있습니다. 이를 극복하기 위해 간단한 &quot;-Oz 백업&quot; 확장을 제안합니다. 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 경우 -Oz도 실행하고 두 옵션 중 더 나은 것을 선택합니다. 이렇게 하면 -Oz에 대한 회귀가 방지되지만 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 횟수만큼 추가 컴파일 횟수가 증가합니다. 표 IV는 이러한 방식으로 평가할 때의 기술 결과를 보여줍니다. 이것이 모델이 더 개선되는 데 도움이 되지는 않지만, 회귀가 없기 때문에 AutoPhase와 Coreset-NVP는 이제 -Oz보다 전반적으로 개선되었지만 -Oz 백업이 있거나 없는 LLM보다는 여전히 낮습니다.C. 생성된 패스 목록의 평가 그림 3은 자동 튜너와 이전 실험의 모델이 패스를 선택하는 빈도를 보여줍니다.모델이 선택한 패스의 분포는 자동 튜너를 광범위하게 추적합니다.-Oz가 가장 자주 최적의 패스입니다.-Oz를 제외하고 모델에서 생성된 패스 목록의 평균 길이는 3.4(최대 10)이고 자동 튜너 패스 목록의 평균 길이는 3.1(최대 9)입니다.모델에서 생성된 패스 목록 중 105개는 학습 데이터에 전혀 나타나지 않습니다.710개의 경우에서 모델에서 생성된 패스 목록은 테스트 세트에서 자동 튜너보다 성능이 뛰어나지만 일반적으로 개선 정도는 작습니다. 목록 1은 모델에서 생성된 표 V: 100,000개의 보이지 않는 입력에 대한 모델 최적화 코드의 컴파일러 오류의 예를 보여줍니다. 오류 범주 n 유형 오류 5, 명령어 전방 참조 1, 정의되지 않은 값 1, 잘못된 재정의 구문 오류 상수에 대한 잘못된 값 정의되지 않은 함수 인덱스 오류 기타 9, 전체 패스 목록은 제어 흐름을 더 적은 블록으로 단순화하여 하나의 추가 명령어를 저장합니다. 그림 4는 벤치마크 데이터 집합에 따른 패스 순서 지정에 대한 각 접근 방식의 개선 사항을 분석합니다. -Oz에 비해 가장 큰 개선 사항은 POJ-104 및 Transcoder 데이터 집합에서 발견되는데, 둘 다 대량의 수기 코드를 집계하는 반면 컴파일러를 테스트하기 위한 무작위 프로그램 생성기인 YARPGen은 -Oz에 비해 개선할 수 있는 기회가 가장 적습니다. 우리는 입력 프로그램 크기와 자동 튜너와 모델 모두에서 발견되는 -Oz에 비해 잠재적인 성능 개선 사이에 강력한 상관 관계가 있음을 발견했습니다. 그림 5는 이러한 추세를 나타내며, 더 큰 프로그램이 -Oz에 비해 개선할 수 있는 기회가 더 많다는 것을 명확하게 보여줍니다. D. 생성된 코드 평가 이 섹션에서는 모델에서 생성된 코드의 품질을 평가합니다. 이를 위해 테스트 세트의 모든 100k 함수에 대해 최적화된 코드를 생성하는 보조 학습 작업을 실행했습니다. 이는 이전 섹션에서 평가된 패스 목록을 생성하는 데 필요하지 않습니다. 이 섹션의 코드 샘플에서는 불필요한 명령문을 생략하고 식별자 이름을 줄이는 등 간결성을 위해 사소한 편집을 했습니다. 90.3%의 경우 모델에서 생성된 최적화된 IR이 컴파일되고 68.4%의 경우 출력 IR이 컴파일러에서 생성한 기준 진실과 일치합니다. 생성된 IR이 컴파일되지 않는 9.7%의 경우에 대한 다양한 오류 클래스를 표 V에 분류하고 목록 2에 코드 예제가 나와 있습니다.오류: &#39;15&#39;가 &#39;i32&#39; 유형으로 정의되었지만 &#39;il&#39;을 예상함 %or.cond = or il %14, %(a) 모델은 %15를 정수로 정의했지만 나중에 부울(유형 오류)로 사용하려고 했습니다. 오류: 상수 표현식 유형 불일치 e.str private unnamed_addr 상수 [493 x 18] 정의 132 @f1( 132 %0, 132 %) 정렬 2 { br 레이블 %int f1 (int x, int y) { int i = 2; while (ii &lt; y) { i += 1; return 2; } 3: } % i = phi 132[%7, %6], [2, %2] %4 = mul nsw i32 %i, i %c&quot; <snip 492 chars ', align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type %1 tail call 132 @f1 (float -0.47799998483256463, float -1.8159999847412109) = icmp sgt 132 %4, %br il 5, label %8, label %6:add 132 %i,br label %8: ret 132} (b) Equivalent (hand-written) C code. define 132 @f1( 132 %0, 132 %) align 2 { ret 132} (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates. 1.0.define hidden signext 18 @f1() %230 { %1 alloca 164, align0.store 164 3718042838174166437, 164* %1, align%2 load 164, 164* %1, align0.%trunc i64 %2 to} ret 18 %0.BLEU Code compiles Optimized instcount error I (a) Input unoptimized code. define hidden signext@f1() %30 { ret 18define hidden signext@f1() #230 { ret 18} } (c) Model-generated code. (b) Desired optimized code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. 0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Run passes -inst combine -simplifycfg to reduce instruction count from 14 to 7: define dso_local 132 @f1 (132 %0) { } % 2 = load 164, 164* getelementptr inbounds ( struct.t2, %struct.t2* @gvar, 164 0, 132 0), align3 icmp eq 164 %2, O %4= icmp eq 132 %0,%or.cond or il %3, %5 load 132, 132* @S64_MAX, align%6 = select il %or.cond, 132 %5, 132 %ret 132 %Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRS is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The modelFrequency (log) 0.0.0.-name-anon-globals -Hoop-versioning-licm -strip -strip-dead-prototypes. -indvars -loop-reroll -strip-nondebug. -loop-unroll Hoop-unswitch. -instcombine. -loop-instsimplify-Hoop-idiom -Hoop-deletion -Hoop-rotate Hoop-unroll-and-jam-loop-guard-widening -Hoop-predication. -die -irce 1.0.75globalopt -01. -simple-loop-unswitch -loop-interchange -Os --Oz. -licm -loop-sink-loop-load-elim -loop-simplifycfg -loop-vectorize --sroa Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. gvn-hoist . -loop-simplify -tailcallelim -speculative-execution. -early-cse -dse. -jump-threading -loop-reduce -sink -simplifycfg Improvement over -Oz 5% 4% 3% 2% 1%100% data 50% data 25% data 0% 0.0.0.0.#. Train Tokens 0.1.1.100% data, No Aux T 1.leFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. overall improvement n training examples generate optimized code? 1,000,500,250,1,000,× 4.95% (-) 3.91% (-21%) 3.74% (-24%) 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMS [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. of We observe an interesting connection between the quality pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%.илбмәи gvn . early-cse-memssa. -bdce-correlated-propagation Hcssa. - бәлгшәшadce -break-crit-edges. -ipsccp-dce-globaldce aggressive-instcombine. -instsimplify -sccp -reassociate -loop-fusion Regressed Improved Optimize the following LLVM-IR using -name-anon-globals: @0 private @anon. 2ef3bda806391c61822366a2a59f2569.0 = private @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0 = private unnamed_addr constant [14 x 18] c"<snip> &quot;, aligndefine dso_local 132 @f1 (18* %0) { %2 = call i32 @f2 (18* %0, 18* getelementptr inbounds ( [14 x 18], [14 x 18] * } -20, @anon. 2ef3bda806391c61822366a2a59f2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, 164 0, 164 0)) ret 132 %(a) 불완전한 정보로 인한 실패. -name-anon-globals 패스는 모듈 이름을 사용하여 해시를 계산합니다. 이것이 없으면 모델은 무작위 해시를 환각합니다. -inst combine을 사용하여 다음 LLVM-IR을 최적화하세요. @var 12 = external dso_local 글로벌 164, align@var 13 외부 dso_local 글로벌 132, align@var_14 = 외부 dso_local 글로벌 132, aligndefine dso_local void @f1 (164 %arg) { tmp = 할당 164, 정렬 저장소 164 %arg, 164* %tmp, aligntmp1 = 로드 164, 164* %tmp, aligntmp2 = 하위 164 0, %tmptmp3 = 하위 164 0, %tmp 저장소 164 %tmp3, 164* @var_12, 정렬 저장소 164 %arg, 164*2B @var_12, 정렬 저장소 164 0, 164* @var 12, 정렬 저장소 i32 1, 132* @var_13, alignstore 132 0, 132* @var_14, alignret void -Oz를 사용하여 다음 LLVM-IR을 최적화합니다. %s1 = type { 132 } @&quot;llvm.used&quot; = 전역 추가 [1 x 18*] [18* 비트캐스트(i32(%s1*) * @fl에서 18*로)], 섹션 &quot;llvm.metadata&quot; define dso_local 132 @f1 (%sl* %0) { %alloca 132, alignalloca %sl*, align%alloca 132, alignstore %s %0, %s1** %3, align5 로드 sl*, %s1** %3, align%6 tail call 132 @f2 (%s1* %5) 스토어 i32 %6, 132* %4, 정렬 4. %7=로드 132, 132* %4, 정렬 4. %8 = icmp slt i32 %7,br il %8, 레이블 %9, 레이블 %9: %%%tail() %%%10 132, 132* %4 로드, 정렬저장 132 %10, 132* %2, 정렬br 레이블 %%%&lt;11: %%%저장 132 0, 132* %2, 정렬 4. br 레이블 %ret 12: } %13 132, 132* %2 로드, 정렬ret 132 %%(a) 모델 프롬프트. tail() %
--- CONCLUSION ---
S 우리는 코드 최적화를 위한 LLM에 대한 첫 번째 단계를 제시합니다. 우리는 보이지 않는 LLVM-IR에 대한 좋은 최적화 전략을 예측할 수 있는 모델을 구성합니다. 결과는 유망하지만, 우리는 작은 프로그램 조각에 대한 작업으로 제한하는 시퀀스 길이와 최적화 결과를 예측하는 모델의 능력을 제한하는 산술 추론에서 어려움에 직면합니다. 우리는 연구 커뮤니티가 간단한 최대 우도 코드 생성을 위한 LLM을 넘어 성능 인식 코드 최적화로 나아가도록 영감을 주고자 합니다. 참고문헌 [16] [17] OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. &quot;MLGO: a Machine Learning Guided Compiler Optimizations Framework&quot;. in: arXiv:2101.04808 (2021). [18] Z. Wang and M. O&#39;Boyle. &quot;컴파일러 최적화에서의 머신 러닝&quot;. arXiv:1805.03441(2018)에 게재됨. [19] H. Leather와 C. Cummins. &quot;컴파일러에서의 머신 러닝: 과거, 현재, 미래&quot;. FDL에 게재됨. 2020. [20] Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, Y. Tian. &quot;코어셋과 정규화된 값 예측을 사용하여 컴파일러 패스 순서 학습&quot;. ICML에 게재됨. 2023. [21] C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O&#39;Boyle, H. Leather. &quot;ProGraML: 데이터 흐름 분석 및 컴파일러 최적화를 위한 그래프 기반 프로그램 표현&quot;. 에서: ICML. 2021. [1] R. Li, LB Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. &quot;StarCoder: 소스가 함께하길 바랍니다!&quot; [22] In: arXiv:2305.06161 (2023). [2] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, AD Lago, T. Hubert, P. Choy 등 &quot;AlphaCode를 사용한 경쟁 수준 코드 생성&quot;. 에서: 과학 378.6624 (2022). [3] 오픈AI. “GPT-4 기술 보고서”. 출처: arXiv:2303.(2023). [4] LB Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, CM Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, LK Umapathi, CJ Anderson, et al. &quot;SantaCoder: 별에 손대지 마세요!&quot; 출처: arXiv:2301.03988 (2023). [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. &quot;PaLM: 경로로 언어 모델링 확장&quot; 출처: arXiv:2204.02311 (2022). [6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer 및 M. 루이스. &quot;InCoder: 코드 채우기 및 합성을 위한 생성 모델&quot;. In: arXiv:2204.05999 (2023). [7] S. Gunasekar, Y. Zhang, J. Aneja, CCT Mendes, AD Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah 등. &quot;교과서는 당신에게 필요한 전부입니다&quot;. In: arXiv:2306.11644 (2023). [8] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri 외. &quot;코드로 훈련된 대규모 언어 모델 평가&quot;. arXiv:2107.03374(2021)에 게재. [9] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, XE Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov 외. &quot;Code Llama: Open Foundation Models for Code&quot;. arXiv:2308.12950(2023)에 게재. [10] M.-A. Lachaux, B. Roziere, L. Chanussot, G. Lample. &quot;프로그래밍 언어의 비지도 번역&quot;. arXiv:2006.03511(2020)에 게재. [11] J. Armengol-Estapé 및 MF O&#39;Boyle. &quot;C에서 x86으로의 번역 학습: 신경 컴파일 실험&quot;. arXiv:2108.07639(2021)에 게재. [12] M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut 및 G. Synnaeve. &quot;컴파일러 표현을 사용한 코드 번역&quot;. arXiv:2207.03578(2022)에 게재. [13] G. Ye, Z. Tang, SH Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang 및 Z. Wang. &quot;심층 컴파일러 퍼징을 통한 JavaScript 엔진에 대한 자동화된 적합성 테스트&quot;. PLDI에 게재. 2021. Y. Deng, CS Xia, H. Peng, C. Yang, L. Zhang. &quot;대규모 언어 모델은 제로샷 퍼저입니다. 대규모 언어 모델을 통한 퍼징 딥러닝 라이브러리&quot;. ISSTA에서. 2023. [14] [15] M. Schäfer, S. Nadi, A. Eghbali, F. Tip. &quot;대규모 언어 모델을 사용한 적응형 테스트 생성&quot;. arXiv:2302에서.(2023). C. Lattner와 V. Adve. &quot;LLVM: 평생 프로그램 분석 및 변환을 위한 컴파일 프레임워크&quot;. CGO에서. 2004. [23] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, S. Paul. &quot;언어 모델을 사용한 학습의 한계&quot;. arXiv:2306에서.(2023). [24] J. Qian, H. Wang, Z. Li, S. Li 및 X. Yan. 영어: &quot;산술 및 기호 귀납에서 언어 모델의 한계&quot;. arXiv:2208.05051(2022)에 게재됨. [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 외. &quot;Llama 2: Open Foundation 및 Fine-Tuned Chat Models&quot;. arXiv:2307.09288(2023)에 게재됨. [26] GG Fursin, MFP O&#39;Boyle, PMW Knijnenburg. &quot;반복 컴파일 평가&quot;. LCPC에 게재됨. 2005. [27] C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, H. Leather. &quot;CompilerGym: AI 연구를 위한 강력하고 성능이 뛰어난 컴파일러 최적화 환경&quot;. CGO에서. 2022. [28] D. Kocetkov, R. Li, LB Allal, J. Li, C. Mou, CM Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. &quot;스택: 허용 라이선스가 부여된 3TB 소스 코드&quot;. arXiv:2211.15533(2022)에서. [29] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. &quot;파일: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트&quot;. arXiv:2101.00027(2020)에 게재됨. [30] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt. &quot;CodeSearchNet 챌린지: 의미 코드 검색 상태 평가&quot;. arXiv:1909.09436(2019)에 게재됨. A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, P. Rosso. &quot;소스 코드의 저자 식별(AI-SOCO)에 대한 PAN@FIRE 2020 과제 개요&quot;. FIRE에 게재됨. 2020. [31] [32] J. Armengol-Estapé, J. Woodruff, A. Brauckmann, JW d. S. Magalhães, M. O&#39;Boyle. &quot;ExeBench: 실행 가능한 C 함수의 ML 규모 데이터 세트&quot;. MAPS에서. 2022. [33] L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin. &quot;프로그래밍 언어 처리를 위한 트리 구조에 대한 합성 신경망&quot;. AAAI에서. 2016. [34] X. Yang, Y. Chen, E. Eide, J. Regehr. &quot;C 컴파일러의 버그 찾기 및 이해&quot;. PLDI에서. 2011. [35] V. Livinskii, D. Babokin, J. Regehr. &quot;YARPGen을 사용한 C 및 C++ 컴파일러의 임의 테스트&quot;. OOPSLA에서. 2020. [36] J. Armengol-Estapé, J. Woodruff, C. Cummins, MF O&#39;Boyle. &quot;SLaDe: 최적화된 어셈블러를 위한 휴대용 소규모 언어 모델 디컴파일러&quot;. arXiv에서:2305.12520(2023)에서. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. &quot;주의만 있으면 됩니다&quot;. NeurIPS에서(2017).[38] P. Gage. &quot;데이터 압축을 위한 새로운 알고리즘&quot;. C Users Journal 12.2(1994)에 실림. [39] A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, I. Stoica. &quot;AutoPhase: 심층 강화 학습을 통한 랜덤 포레스트에서 HLS 위상 순서 조정&quot;. MLSys에 실림. 2020. [56] AH Ashouri, M. Elhoushi, Y. Hua, X. Wang, MA Manzoor, B. Chan, Y. Gao. &quot;MLGOPerf: 성능 최적화를 위한 ML 가이드 인라이너&quot;. arXiv:2207.08389(2022)에 실림. A. Haj-Ali, NK Ahmed, T. Willke, S. Shao, K. Asanovic, I. Stoica. 영어: &quot;NeuroVectorizer: 심층 강화 학습을 통한 종단 간 벡터화&quot;. CGO에서. 2020. [57] [40] I. Loshchilov 및 F. Hutter. &quot;분리된 가중치 감소 규칙&quot;. arXiv에서: 1711.05101(2017). [41] K. Papineni, S. Roukos, T. Ward 및 W.-J. Zhu. &quot;BLEU: 기계 번역의 자동 평가를 위한 방법&quot;. ACL에서. 2002. [42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 및 O. Klimov. &quot;근위 정책 최적화 알고리즘&quot;. 출처: arXiv:1707.06347 (2017). [43] M. Paszkowski. LLVM Canon. https://github.com/michalpaszkowski/LLVM-Canon. [44] WM McKeeman. &quot;소프트웨어에 대한 차등 테스트&quot;. 출처: Digital Technical Journal 10.1 (1998). [45] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei. &quot;LongNet: 1,000,000개 토큰으로 변환기 확장&quot;. 출처: arXiv:2307.02486 (2023). [47] [46] S. Chen, S. Wong, L. Chen, Y. Tian. &quot;위치 보간을 통한 대규모 언어 모델의 컨텍스트 창 확장&quot;. In: arXiv:2306.15595 (2023). Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song 및 F. Wei. &quot;길이 외삽 가능한 변환기&quot;. In: arXiv:2212.10554 (2022). [48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, QV Le, D. Zhou 등. &quot;사슬 사고 촉진은 대규모 언어 모델에서 추론을 이끌어냅니다&quot;. NeurIPS에서. 2022. [49] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, G. Neubig. &quot;Pal: 프로그램 지원 언어 모델&quot;. ICML에서. 2023. [50] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. &quot;수학 단어 문제를 해결하기 위한 검증자 훈련&quot;. arXiv에서. 2110.14168(2021). [51] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han. &quot;SmoothQuant: 정확하고 영어: 대규모 언어 모델을 위한 효율적인 사후 학습 양자화&quot;. ICML에서. 2023. [52] F. Bodin, T. Kisuki, P. Knijnenburg, M. O&#39;Boyle, E. Rohou. &quot;비선형 최적화 공간에서의 반복 컴파일&quot;. FDO에서. 1998. [53] T. Kisuki, P. Knijnenburg, M. O&#39;Boyle. &quot;반복 컴파일을 사용한 타일 크기 및 언롤 요인의 결합 선택&quot;. PACT에서. 2000. [54] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. O&#39;Boyle, J. Thomson, M. Toussaint, C. Williams. &quot;기계 학습을 사용하여 반복적 최적화에 집중하기&quot;. CGO에서. 2006. [55] WF Ogilvie, P. Petoumenos, Z. Wang, H. Leather. &quot;능동 학습을 통한 반복 컴파일 비용 최소화&quot;. CGO에서. 2017. [59] 최적화 휴리스틱의 최종 심층 학습&quot;. PACT에서. 2017. PM Pothilimthana, A. Sabne, N. Sarda, KS Murthy, Y. Zhou, C. Angermueller, M. Burrows, S. Roy, K. Mandke, R. Farahani, et al. &quot;멀티패스 기계 학습 컴파일러의 자동 튜닝에 대한 유연한 접근 방식&quot;. PACT에서. 2021. [60] I. Hosseini 및 B. Dolan-Gavitt. &quot;C를 넘어서: 신경망 기계 번역을 사용한 리타겟팅 가능 디컴파일&quot;. 출처: arXiv:2212.08950 (2022). [61] SK Gallagher, WE Klieber 및 D. Svoboda. 코드 약점 식별을 위한 LLVM 중간 표현. 2022. [62] [63] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al. “CodeBERT: 프로그래밍 및 자연어를 위한 사전 학습된 모델”. arXiv:2002.08155(2020)에 게재됨. D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, SK Deng, C. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, M. Zhou. “GraphCodeBERT: 데이터 흐름을 통한 코드 표현 사전 학습”. arXiv:2009.08366(2021)에 게재됨. [64] Y. Wang, W. Wang, S. Joty, SC Hoi. “CodeT5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델”. 출처: arXiv:2109.00859 (2021). CS Xia, M. Paltenghi, JL Tian, M. Pradel 및 L. Zhang. &quot;대규모 언어 모델을 통한 범용 퍼징&quot;. 출처: arXiv:2308.04748 (2023). [65] [66] CS Xia 및 L. Zhang. 영어: “Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-shot Learning”. ArXiv:2207.08281(2022)에 게재됨. [67] CS Xia, Y. Wei, L. Zhang. &quot;대규모 사전 학습된 언어 모델 시대의 자동 프로그램 복구&quot;. ICSE에 게재됨. 2023. [68] CS Xia, L. Zhang. “Keep the Conversation Going: Fixing 162 of 337 bugs for $0.42 each”. ArXiv:2304.00385(2023)에 게재됨. [69] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. &quot;Llama: 개방적이고 효율적인 기초 언어 모델&quot;. in: arXiv 사전 인쇄본 arXiv:2302.13971 (2023). GitHub. Copilot. https://copilot.github.com/. [70]
