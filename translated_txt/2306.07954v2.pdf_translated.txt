--- ABSTRACT ---
대규모 텍스트-이미지 확산 모델은 고품질 이미지를 생성하는 데 인상적인 능력을 보여주었습니다. 그러나 이러한 모델을 비디오 도메인에 적용할 때 비디오 프레임 간에 시간적 일관성을 보장하는 것은 여전히 어려운 과제입니다. 이 논문에서는 이미지 모델을 비디오에 맞게 조정하기 위한 새로운 제로샷 텍스트 가이드 비디오-비디오 변환 프레임워크를 제안합니다. 이 프레임워크는 키 프레임 변환과 전체 비디오 변환의 두 부분으로 구성됩니다. 첫 번째 부분은 조정된 확산 모델을 사용하여 키 프레임을 생성하고, 계층적 교차 프레임 제약 조건을 적용하여 모양, 텍스처 및 색상의 일관성을 강화합니다. 두 번째 부분은 시간 인식 패치 매칭 및 프레임 블렌딩을 사용하여 키 프레임을 다른 프레임으로 전파합니다. 저희 프레임워크는 낮은 비용(재교육 또는 최적화 없이)으로 글로벌 스타일과 로컬 텍스처 시간적 일관성을 달성합니다. 이 적응은 기존 이미지 확산 기술과 호환되므로 저희 프레임워크가 LORA로 특정 주제를 사용자 지정하고 ControlNet으로 추가 공간 가이드를 도입하는 등 이러한 기술을 활용할 수 있습니다. 광범위한 실험 결과는 고품질의 시간적으로 일관된 비디오를 렌더링하는 데 있어 기존 방법보다 제안된 프레임워크의 효과를 보여줍니다. 코드는 프로젝트 페이지에서 제공됩니다: https: //www.mmlab-ntu.com/project/rerender/ 1.
--- INTRODUCTION ---
DALLE2[30], Imagen[34], Stable Diffusion[32]과 같은 최근의 텍스트-이미지 확산 모델은 자연어로 안내되는 다양하고 고품질의 이미지를 생성하는 뛰어난 능력을 보여줍니다. 이를 기반으로 사용자 지정 객체 생성을 위한 모델 미세 조정[33], 이미지-이미지 변환[23], 이미지 인페인팅[1], 객체 편집[12]을 포함한 수많은 이미지 편집 방법이 등장했습니다. 이러한 애플리케이션을 사용하면 사용자는 통합된 확산 프레임워크 내에서 자연어를 사용하여 이미지를 손쉽게 합성하고 편집하여 생성 효율성을 크게 향상시킬 수 있습니다. 소셜 미디어 플랫폼에서 비디오 콘텐츠의 인기가 급증함에 따라 보다 간소화된 비디오 제작 도구에 대한 수요도 동시에 증가했습니다. 그러나 중요한 과제가 남아 있습니다. 기존 이미지 확산 모델을 비디오에 직접 적용하면 심각한 깜빡임 문제가 발생합니다. 연구자들은 최근 텍스트 안내 비디오 확산 모델로 전환하여 세 가지 솔루션을 제안했습니다. 첫 번째 솔루션은 대규모 비디오 데이터에서 비디오 모델을 학습하는 것[14]으로, 상당한 컴퓨팅 리소스가 필요합니다. 또한, 재설계된 비디오 모델은 기존의 기성 이미지 모델과 호환되지 않습니다.두 번째 솔루션은 단일 비디오에서 이미지 모델을 미세 조정하는 것[40]으로, 긴 비디오에는 효율성이 떨어집니다.단일 비디오에 과적합하면 원본a 잘생긴 남자, 지브리 스타일 일반 모델 사용자 지정 모델 일반 모델 사용자 지정 모델 일반 모델의 아름다운 산 중국 수묵화 사용자 지정 모델 일반 모델(a) 입력(b) ControlNet 없이 변환 결과 사용자 지정 모델(c) ControlNet을 사용한 변환 결과 그림 2. 사용자 지정 모델과 ControlNet은 프롬프트와 콘텐츠 모두에서 더 나은 일관성을 가진 고품질 결과를 생성합니다.우리의 방법은 이러한 기존 이미지 확산 기술과 호환되도록 설계되었으므로 이를 활용하여 스타일(프롬프트)과 콘텐츠 간에 좋은 균형을 이룰 수 있습니다.모델.세 번째 솔루션은 훈련이 필요 없는 제로 샷 방법[20]을 포함합니다.확산 샘플링 프로세스 동안 시간적 일관성을 위해 잠재적 특징에 교차 프레임 제약 조건이 부과됩니다. 제로 샷 전략은 컴퓨팅 리소스가 덜 필요하고 대부분 기존 이미지 모델과 호환되어 유망한 잠재력을 보여줍니다. 그러나 현재의 크로스 프레임 제약은 글로벌 스타일로 제한되어 있으며 저수준 일관성을 유지할 수 없습니다. 예를 들어, 전체 스타일은 일관될 수 있지만 로컬 구조와 텍스처는 여전히 깜빡일 수 있습니다. 비디오 도메인에 이미지 확산 모델을 성공적으로 적용하는 것은 어려운 작업입니다. 1) 시간적 일관성: 저수준 일관성을 위한 크로스 프레임 제약; 2) 제로 샷: 학습이나 미세 조정이 필요 없음; 3) 유연성: 사용자 지정 생성을 위한 기성 이미지 모델과 호환. 위에서 언급했듯이 이미지 모델은 일반 모델보다 대상 스타일을 더 정확하게 캡처하기 위해 특정 객체에서 미세 조정하여 사용자 지정할 수 있습니다. 그림 2는 두 가지 예를 보여줍니다. 이를 활용하기 위해 이 논문에서는 모델 호환성을 위한 제로 샷 전략을 사용하고 저수준 시간적 일관성을 유지하는 이 전략의 핵심 문제를 더욱 해결하는 것을 목표로 합니다. 이러한 목표를 달성하기 위해, 우리는 사전 훈련된 이미지 모델에 대한 새로운 계층적 교차 프레임 제약 조건을 제안하여 일관된 비디오 프레임을 생성합니다. 우리의 핵심 아이디어는 광학 흐름을 사용하여 밀도가 높은 교차 프레임 제약 조건을 적용하는 것입니다. 이전에 렌더링된 프레임은 현재 프레임에 대한 저수준 참조 역할을 하고, 첫 번째 렌더링된 프레임은 렌더링 프로세스를 조절하여 초기 모양과의 편차를 방지하는 앵커 역할을 합니다. 계층적 교차 프레임 제약 조건은 확산 샘플링의 여러 단계에서 실현됩니다. 글로벌 스타일 일관성 외에도, 우리의 방법은 각각 초기, 중기 및 후기 단계에서 모양, 텍스처 및 색상의 일관성을 적용합니다. 이 혁신적이고 가벼운 수정은 글로벌 및 로컬 시간적 일관성을 모두 달성합니다. 그림 1은 여섯 가지 고유한 스타일에 맞게 사용자 정의된 기성품 이미지 모델에 대한 일관된 비디오 변환 결과를 보여줍니다. 이러한 통찰력을 바탕으로, 이 논문은 텍스트 가이드 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크를 소개합니다. 이는 키 프레임 변환과 전체 비디오 변환의 두 부분으로 구성됩니다. 첫 번째 부분에서는 키 프레임을 생성하기 위해 계층적 교차 프레임 제약을 사용하여 사전 학습된 이미지 확산 모델을 조정합니다. 두 번째 부분에서는 시간 인식 패치 매칭 및 프레임 블렌딩을 사용하여 렌더링된 키 프레임을 다른 프레임으로 전파합니다. 확산 기반 생성은 콘텐츠 생성에 뛰어나지만 다단계 샘플링 프로세스는 비효율적입니다. 반면 패치 기반 전파는 픽셀 수준의 일관된 프레임을 효율적으로 추론할 수 있지만 새로운 콘텐츠를 생성할 수 없습니다. 이 두 부분을 결합하여 프레임워크는 품질과 효율성 간의 균형을 이룹니다. 요약하자면, 우리의 주요 기여는 다음과 같습니다. • 전역 및 로컬 시간적 일관성을 모두 달성하고, 학습이 필요하지 않으며, 사전 학습된 이미지 확산 모델과 호환되는 텍스트 기반 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크. • 이미지 확산 모델을 비디오에 적용하는 모양, 텍스처 및 색상의 시간적 일관성을 강제하는 계층적 교차 프레임 일관성 제약. • • 품질과 효율성의 균형을 맞추기 위해 하이브리드 확산 기반 생성과 패치 기반 전파를 사용합니다. 2.
--- RELATED WORK ---
2.1. 텍스트 기반 이미지 생성 설명적 문장이 있는 이미지를 생성하는 것은 직관적이고 유연합니다.초기 시도에서는 사실적인 이미지를 합성하기 위해 GAN[42-44, 46]을 탐구합니다.Transformer[38]의 강력한 표현력으로 자기 회귀 모델[6,9,31]이 제안되어 각 픽셀 간에 자기 회귀 종속성이 있는 시퀀스로 이미지 픽셀을 모델링합니다.DALL-E[31]와 CogView[6]는 이미지 및 텍스트 토큰에 대한 자기 회귀 변환기를 학습합니다.Make-A-Scene[9]은 분할 마스크를 조건으로 추가로 고려합니다.최근 연구는 점진적인 노이즈 제거 프로세스를 통해 이미지를 합성하는 텍스트-이미지 생성을 위한 확산 모델[15]에 중점을 둡니다.DALLE-2[30]와 Imagen[34]은 텍스트 인코더로 사전 학습된 대규모 언어 모델[28, 29]을 도입하여 이미지와 텍스트를 더 잘 정렬하고 고해상도 이미지 생성을 위한 계단식 확산 모델을 도입합니다. GLIDE[26]는 텍스트 컨디셔닝을 개선하기 위해 분류기 없는 안내를 도입합니다. 이미지 공간에서 노이즈 제거를 적용하는 대신, Latent Diffusion Models[32]는 VQ-GAN[7]의 저해상도 잠재 공간을 사용하여 효율성을 개선합니다. 철저한 조사는 [4]를 참조하십시오. 일반 이미지에 대한 확산 모델 외에도 사용자 지정 모델이 연구됩니다. Textual Inversion[10]과 DreamBooth[33]는 특수 토큰을 학습하여 새로운 개념을 포착하고 소수의 예제 이미지가 주어진 경우 관련 이미지를 생성합니다. LORA[17]는 기존 가중치에 추가된 저순위 가중치 행렬을 학습하여 대규모 모델의 미세 조정을 가속화합니다. ControlNet[45]은 새로운 제어 경로를 미세 조정하여 에지 맵 및 포즈와 같은 픽셀 수준 조건을 제공하여 세분화된 이미지 생성을 가능하게 합니다.
--- METHOD ---
영어: 고품질이고 시간적으로 일관된 비디오를 렌더링하는 s. 코드는 프로젝트 페이지에서 확인할 수 있습니다: https://www.mmlab-ntu.com/project/rerender/ 1. 서론 DALLE2[30], Imagen[34], Stable Diffusion[32]과 같은 최근의 텍스트-이미지 확산 모델은 자연어로 안내되는 다양하고 고품질의 이미지를 생성하는 뛰어난 능력을 보여줍니다. 이를 기반으로 사용자 지정 객체 생성을 위한 모델 미세 조정[33], 이미지-이미지 변환[23], 이미지 인페인팅[1], 객체 편집[12]을 포함한 수많은 이미지 편집 방법이 등장했습니다. 이러한 애플리케이션을 사용하면 사용자는 통합된 확산 프레임워크 내에서 자연어를 사용하여 이미지를 손쉽게 합성하고 편집하여 생성 효율성을 크게 개선할 수 있습니다. 소셜 미디어 플랫폼에서 비디오 콘텐츠의 인기가 급증함에 따라 보다 간소화된 비디오 제작 도구에 대한 수요도 동시에 증가했습니다. 그러나 중요한 과제가 남아 있습니다. 기존 이미지 확산 모델을 비디오에 직접 적용하면 심각한 깜빡임 문제가 발생합니다. 연구자들은 최근 텍스트 가이드 비디오 확산 모델을 사용하여 세 가지 솔루션을 제안했습니다.첫 번째 솔루션은 대규모 비디오 데이터에서 비디오 모델을 학습하는 것[14]으로, 상당한 컴퓨팅 리소스가 필요합니다.또한, 재설계된 비디오 모델은 기존의 기성형 이미지 모델과 호환되지 않습니다.두 번째 솔루션은 단일 비디오에서 이미지 모델을 미세 조정하는 것[40]으로, 긴 비디오에는 효율성이 떨어집니다.단일 비디오에 과적합하면 원본 모델의 성능이 저하될 수도 있습니다.잘생긴 남자, 지브리 스타일 일반 모델 사용자 지정 모델 일반 모델 사용자 지정 모델 일반 모델의 아름다운 산 중국 수묵화 사용자 지정 모델 일반 모델 (a) 입력 (b) ControlNet이 없는 변환 결과 사용자 지정 모델 (c) ControlNet이 있는 변환 결과 그림 2. 사용자 지정 모델과 ControlNet은 프롬프트와 콘텐츠 모두에서 더 나은 일관성을 가진 고품질 결과를 생성합니다.우리의 방법은 이러한 기존 이미지 확산 기술과 호환되도록 설계되었으므로 이를 활용하여 스타일(프롬프트)과 콘텐츠 간의 좋은 균형을 이룰 수 있습니다. 세 번째 솔루션은 학습이 필요 없는 제로샷 방법[20]을 포함합니다. 확산 샘플링 프로세스 동안 시간적 일관성을 위해 잠재 특징에 교차 프레임 제약이 부과됩니다. 제로샷 전략은 컴퓨팅 리소스가 덜 필요하고 대부분 기존 이미지 모델과 호환되어 유망한 잠재력을 보여줍니다. 그러나 현재의 교차 프레임 제약은 글로벌 스타일로 제한되어 있으며 저수준 일관성을 유지할 수 없습니다. 예를 들어 전체 스타일은 일관될 수 있지만 로컬 구조와 텍스처는 여전히 깜빡일 수 있습니다. 비디오 도메인에 이미지 확산 모델을 성공적으로 적용하는 것은 어려운 작업입니다. 1) 시간적 일관성: 저수준 일관성을 위한 교차 프레임 제약; 2) 제로샷: 학습이나 미세 조정 필요 없음; 3) 유연성: 사용자 지정 생성을 위한 기성 이미지 모델과 호환. 위에서 언급했듯이 이미지 모델은 일반 모델보다 대상 스타일을 더 정확하게 캡처하기 위해 특정 객체에 대한 미세 조정을 통해 사용자 지정할 수 있습니다. 그림 2는 두 가지 예를 보여줍니다. 이를 활용하기 위해 이 논문에서는 모델 호환성을 위한 제로샷 전략을 채택하고 저수준 시간적 일관성을 유지하는 이 전략의 핵심 문제를 더욱 해결하는 것을 목표로 합니다. 이 목표를 달성하기 위해 사전 훈련된 이미지 모델에 대한 새로운 계층적 교차 프레임 제약 조건을 제안하여 일관된 비디오 프레임을 생성합니다. 우리의 핵심 아이디어는 광학 흐름을 사용하여 밀도가 높은 교차 프레임 제약 조건을 적용하고, 이전에 렌더링된 프레임은 현재 프레임에 대한 저수준 참조 역할을 하고, 첫 번째 렌더링된 프레임은 렌더링 프로세스를 조절하여 초기 모양과의 편차를 방지하는 앵커 역할을 하는 것입니다. 계층적 교차 프레임 제약 조건은 확산 샘플링의 여러 단계에서 실현됩니다. 글로벌 스타일 일관성 외에도 우리의 방법은 각각 초기, 중기 및 후기 단계에서 모양, 텍스처 및 색상의 일관성을 적용합니다. 이 혁신적이고 가벼운 수정은 글로벌 및 로컬 시간적 일관성을 모두 달성합니다. 그림 1은 6가지 고유한 스타일로 사용자 지정된 기성 이미지 모델에 대한 일관된 비디오 변환 결과를 보여줍니다. 이러한 통찰력을 바탕으로 이 논문에서는 텍스트 기반 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크를 소개합니다.이 프레임워크는 키 프레임 변환과 전체 비디오 변환의 두 부분으로 구성됩니다.첫 번째 부분에서는 키 프레임을 생성하기 위해 계층적 교차 프레임 제약 조건을 사용하여 사전 훈련된 이미지 확산 모델을 적용합니다.두 번째 부분에서는 시간 인식 패치 매칭과 프레임 블렌딩을 사용하여 렌더링된 키 프레임을 다른 프레임으로 전파합니다.확산 기반 생성은 콘텐츠 생성에 뛰어나지만 다단계 샘플링 프로세스는 비효율적입니다.반면 패치 기반 전파는 픽셀 수준의 일관된 프레임을 효율적으로 추론할 수 있지만 새로운 콘텐츠를 생성할 수는 없습니다.이 두 부분을 결합하여 프레임워크는 품질과 효율성 간의 균형을 이룹니다.요약하면 주요 기여 사항은 다음과 같습니다.• 글로벌 및 로컬 시간적 일관성을 모두 달성하고, 훈련이 필요하지 않으며, 사전 훈련된 이미지 확산 모델과 호환되는 텍스트 기반 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크. • 모양, 질감 및 색상의 시간적 일관성을 강제하기 위한 계층적 크로스 프레임 일관성 제약 조건으로, 이미지 확산 모델을 비디오에 적용합니다.• • 품질과 효율성 간의 균형을 맞추기 위한 하이브리드 확산 기반 생성 및 패치 기반 전파.2. 관련 연구 2.1. 텍스트 기반 이미지 생성 설명적 문장으로 이미지를 생성하는 것은 직관적이고 유연합니다.초기 시도에서는 사실적인 이미지를 합성하기 위해 GAN[42-44, 46]을 탐구합니다.Transformer[38]의 강력한 표현력으로, 자기 회귀 모델[6,9,31]이 제안되어 각 픽셀 간에 자기 회귀 종속성이 있는 시퀀스로 이미지 픽셀을 모델링합니다.DALL-E[31] 및 CogView[6]는 이미지 및 텍스트 토큰에 대한 자기 회귀 변환기를 학습합니다.Make-A-Scene[9]은 분할 마스크를 조건으로 추가로 고려합니다.최근 연구는 점진적인 노이즈 제거 프로세스를 통해 이미지를 합성하는 텍스트-이미지 생성을 위한 확산 모델[15]에 초점을 맞춥니다. DALLE-2[30]와 Imagen[34]은 텍스트 인코더로 사전 학습된 대규모 언어 모델[28, 29]을 도입하여 이미지를 텍스트와 더 잘 정렬하고 고해상도 이미지 생성을 위한 캐스케이드 확산 모델을 도입합니다. GLIDE[26]는 분류기 없는 안내를 도입하여 텍스트 컨디셔닝을 개선합니다. 이미지 공간에서 노이즈 제거를 적용하는 대신 Latent Diffusion Models[32]는 VQ-GAN[7]의 저해상도 잠재 공간을 사용하여 효율성을 개선합니다. 철저한 조사는 [4]를 참조하십시오. 일반 이미지에 대한 확산 모델 외에도 사용자 지정 모델이 연구됩니다. Textual Inversion[10]과 DreamBooth[33]는 특수 토큰을 학습하여 새로운 개념을 포착하고 소수의 예제 이미지가 주어진 경우 관련 이미지를 생성합니다. LORA[17]는 기존 가중치에 추가된 저순위 가중치 행렬을 학습하여 대규모 모델의 미세 조정을 가속화합니다. ControlNet[45]은 에지 맵 및 포즈와 같은 픽셀 수준 조건을 제공하기 위해 새로운 제어 경로를 미세 조정하여 세분화된 이미지 생성을 가능하게 합니다.우리의 방법은 사전 훈련된 모델을 변경하지 않으므로 이러한 기존 기술과 직교합니다.이를 통해 그림 2와 같이 더 나은 사용자 지정 비디오 변환을 위해 DreamBooth 및 LoRA를 활용하고 ControlNet을 사용하여 시간적으로 일관된 구조 안내를 할 수 있습니다.2.2. 확산 모델을 사용한 비디오 편집 비디오 확산 텍스트에서 비디오로 생성하기 위해 Model[16]은 이미지 모델의 2D U-Net을 인수분해된 공간-시간 UNet으로 확장하는 것을 제안합니다.Imagen Video[14]는 공간 및 시간 비디오 초고해상도 모델의 캐스케이드를 사용하여 비디오 확산 모델을 확장하며, 이는 Dreamix[25]에 의해 비디오 편집으로 더욱 확장됩니다.Make-A-Video[36]는 비지도 방식으로 비디오 데이터를 활용하여 이미지 모델을 구동하는 움직임을 학습합니다.위의 방법은 유망하지만 훈련을 위해 대규모 비디오 데이터가 필요합니다. Tune-A-Video[40]는 대신 이미지 확산 모델을 교차 프레임 어텐션이 있는 비디오 모델로 부풀리고, 단일 비디오에서 미세 조정하여 관련 동작이 있는 비디오를 생성합니다. 이를 기반으로 Edit-A-Video[35], VideoP2P[22] 및 vid2vid-zero[39]는 편집되지 않은 영역을 보존하기 위한 정확한 역전을 위해 Null-Text Inversion[24]을 활용합니다. 그러나 이러한 모델은 사전 학습된 모델의 미세 조정이나 입력 비디오에 대한 최적화가 필요하며, 이는 덜 효율적입니다. 최근 개발에서는 설계상 어떠한 학습 단계 없이 작동하는 zeroshot 방법이 도입되었습니다. 따라서 이러한 방법은 깊이 및 에지와 같은 보다 유연한 조건을 수용하기 위해 InstructPix2Pix[2] 또는 ControlNet과 같은 사전 학습된 확산 변형과 자연스럽게 호환됩니다. 보존할 채널 및 공간 영역을 나타내는 Prompt2Prompt[12]에서 감지한 편집 마스크를 기반으로 FateZero[27]는 편집 전후의 어텐션 기능을 혼합합니다. Text2Video-Zero[20]는 잠재 객체를 변환하여 동작을 직접 시뮬레이션하고 Pix2Video[3]는 현재 프레임의 잠재 객체를 이전 프레임의 잠재 객체와 일치시킵니다.위의 모든 방법은 시간적 일관성을 개선하기 위해 주로 프레임 간 주의와 초기 단계 잠재 융합에 의존합니다.그러나 나중에 보여드리겠지만 이러한 전략은 주로 고수준 스타일과 모양에 맞춰져 있으며 텍스처와 디테일 수준에서 프레임 간 일관성을 유지하는 데 덜 효과적입니다.이러한 접근 방식과 달리, 본 방법은 픽셀 수준의 시간적 일관성을 사소하지 않게 달성하는 새로운 픽셀 인식 프레임 간 잠재 융합을 제안합니다.또 다른 제로샷 솔루션은 하나 이상의 확산 편집된 프레임을 기반으로 비디오를 추론하기 위해 프레임 보간을 적용하는 것입니다.이미지 유추의 선구적 작업[13]은 스타일 효과를 예시 쌍에서 다른 이미지로 이전합니다.I 키 프레임 변환 Ik 전체 비디오 변환 첫 번째 프레임;첫 번째 키 프레임;앵커 프레임 프레임 간 주의; 픽셀 인식 퓨전 12K 키 프레임 비키 프레임 프레임 보간 → 크로스 프레임 주의; 모양 인식 퓨전; 픽셀 인식 퓨전; 색상 AdaIN 그림 3. 패치 매칭을 사용하여 프레임워크에 시간적 제약을 부과하기 위한 서로 다른 프레임 간의 상호 작용에 대한 설명. Fišer et al. [8]은 얼굴 특징을 안내하여 얼굴 비디오 변환으로 이미지 유추를 확장합니다. 나중에 Jamrivška et al. [19]는 새로운 시간 블렌딩 접근 방식을 사용하여 여러 샘플 프레임을 기반으로 하는 일반 비디오 변환을 위한 개선된 EbSynth를 제안합니다. 이러한 패치 기반 방법은 미세한 세부 사항을 보존할 수 있지만 시간적 일관성은 주로 샘플 프레임 전체의 일관성에 달려 있습니다. 따라서 그림 11에서 나중에 보여주듯이 일관된 프레임을 생성하기 위한 적응형 확산 모델이 이러한 방법에 적합합니다. 이 논문에서는 더 나은 시간적 일관성을 달성하고 추가 학습 없이 추론을 가속화하기 위해 제로 샷 EbSynth를 프레임워크에 통합합니다. 3. 예비 지식: 확산 모델 안정적 확산 안정적 확산은 오토인코더 D(E())의 잠재 공간에서 작동하는 잠재 확산 모델입니다.여기서 &amp; 및 는 각각 인코더와 디코더입니다.특히 잠재 특징 X 0 = E(I)를 갖는 이미지 I의 경우 확산 순방향 프로세스는 잠재 _ q(2t|2t−1)=N(2t;Vatt-1,(1 - a)I), (1)에 반복적으로 노이즈를 추가합니다.여기서 t = 1, ..., T는 시간 단계이고, q(xt|xt−1)은 주어진 xt의 조건부 밀도입니다.It-1이고 at는 하이퍼파라미터입니다.또는 xo에서 임의의 시간 단계에서 xt를 직접 샘플링할 수 있습니다.q(xt|xo) = N(xt; √ātxo, (1 — āt)Ī), 여기서 ā₁ = [[±1 αi. (2) 그런 다음 확산 역방향 프로세스인 U-Net €0가 잠재 잡음의 노이즈를 예측하도록 훈련되어 xT에서 x를 반복적으로 복구합니다. 큰 T가 주어지면 xo는 순방향 프로세스에서 완전히 파괴되어 x가 표준 가우시안 분포에 근접합니다. 따라서 e는 무작위 가우시안 노이즈에서 유효한 xo를 유추하는 방법을 그에 따라 학습합니다. 훈련이 완료되면 결정적 DDIM 샘플링[37]을 사용하여 xt를 기반으로 xt-1을 샘플링할 수 있습니다. –1€0(2t,t,Cp), (3) x+-DDIM 샘플링 T를 가리키는 예측된 xo 방향 4,xt→x}-1,x橈=&quot;지브리 스타일의 아름다운 소녀&quot;, DDPM 순방향 E 사전 훈련된 이미지 확산 모델 ε 워프 Sec. 4.1.Cross-Frame Attention Sec. 4.1.Sec. 4.1.형태 인식 AdaIN 잠재 융합 11, 12, ..., IN 사전 학습된 이미지 확산 모델 ε &quot;지브리 스타일의 아름다운 소녀&quot;, Sec. 4.1.픽셀 인식 잠재 융합 Sec. 4.1.E 충실도 지향 이미지 인코딩 워프 I; 및 Mi 1,12,..., IN D 집계 x²-I 프레임 간 주의 T Tpo 형태 인식 잠재 융합 픽셀 인식 잠재 융합 Τρι T₂ 적응적 잠재 조정 (a) 계층적 프레임 간 일관성 제약이 있는 확산 기반 키 프레임 변환 (b) 샘플링 파이프라인 그림 4. 제안된 제로 샷 텍스트 가이드 비디오 변환의 프레임워크. (a) 계층적 프레임 간 제약이 있는 사전 학습된 이미지 확산 모델(Stable Diffusion + ControlNet)을 조정하여 일관된 프레임을 렌더링합니다. 빨간색 점선은 원본 이미지 확산 모델의 샘플링 프로세스를 나타냅니다. 검은색 선은 비디오 변환을 위해 조정된 프로세스를 나타냅니다. (b) 서로 다른 샘플링 단계에서 서로 다른 제약 조건을 적용합니다. 여기서 t는 시간 단계 t에서 예측된 xo, xt→t(x+- √1-α+€(xt, t, Cp))/√√αt이고 0(xt, t, Cp)는 시간 단계 t와 텍스트 프롬프트 조건 Cp를 기반으로 한 xt의 예측 노이즈입니다. = ~ 표 1. 표기법 요약. 설명 안정적 확산의 이미지 인코더 및 디코더 i번째 키 프레임(4.1절); i번째 비디오 프레임(4.2절) I의 변환 결과; 제안된 방법을 사용한 I의 변환 결과; PA 융합 없이(그림 11(b)) (4) 표기법 E, DE* 제안된 충실도 지향 이미지 인코더 Ii II&#39; I, Mi I&#39; x x² wi, M I의 잠재 특징; 확산 역방향 잡음 제거 단계 t에서 I의 추정된 x; 확산 역방향 잡음 제거 단계 t에서 I의 잠재 특징 확산 순방향 샘플링 단계 t에서 I의 광학 흐름 및 폐색 마스크; Ii로 추론하는 동안 DDIM 샘플링을 사용하여 표준 가우시안 잡음 XT ZT, ZT N(0, 1)에서 유효한 xo를 샘플링하고 xo를 최종 생성된 이미지 I&#39; = D(x0)로 디코딩할 수 있습니다.ControlNet 자연어는 유연하지만 출력에 대한 공간 제어가 제한적입니다.공간적 제어성을 개선하기 위해 [45]는 모서리, 깊이 및 인간 포즈와 같은 추가 조건을 허용하기 위해 ControlNet이라는 측면 경로를 Stable Diffusion에 도입합니다.cf가 추가 조건이면 ControlNet을 사용한 U-Net의 잡음 예측은 €0(xt, t, Cp, Cf)가 됩니다.InstructPix2Pix와 비교할 때 ControlNet은 사용자 지정 Stable Diffusion 모델과 직교합니다. 일반적인 제로샷 V2V 프레임워크를 구축하기 위해 ControlNet을 사용하여 입력 비디오에서 구조적 안내를 제공하여 시간적 일관성을 개선합니다.4. 제로샷 텍스트 가이드 비디오 변환 N N개의 프레임 {I}0이 있는 비디오가 주어지면, 우리의 목표는 텍스트 프롬프트 및/또는 기성품 맞춤형 안정적 확산 모델로 지정된 다른 예술적 표현으로 이를 새로운 비디오 {I}로 렌더링하는 것입니다.우리의 프레임워크는 두 부분으로 구성됩니다.키 프레임 변환(4.1절)과 전체 비디오 변환(4.2절).첫 번째 부분에서는 사전 학습된 이미지 확산 모델에 4개의 계층적 교차 프레임 제약 조건을 도입하여 그림 3에서 설명한 대로 앵커 및 이전 키 프레임을 사용하여 일관된 키 프레임의 렌더링을 안내합니다.그런 다음 두 번째 부분에서는 인접한 두 개의 키 프레임을 기반으로 키가 아닌 프레임을 보간합니다.따라서 우리의 프레임워크는 서로 다른 프레임 간의 관계를 최대한 활용하여 출력의 시간적 일관성을 개선할 수 있습니다. I와 I₁를 I로 워핑한 결과와, 변환된 키 프레임 I&#39;₁를 I₂로 전파한 폐색 마스크 결과; E 4.1로 인코딩됨.키 프레임 변환 ~ 그림 4는 키 프레임 변환을 위한 T-단계 샘플링 파이프라인을 보여줍니다.SDEdit [23]에 따라 파이프라인은 XT = √āτx0+ (1āT)ZT, ZT N(0, 1)로 시작합니다.이는 순수한 가우시안 노이즈가 아닌 입력 비디오 프레임의 노이즈가 있는 잠재 코드입니다.사용자는 T를 조정하여 입력 프레임의 세부 정보가 출력에서 얼마나 보존되는지 결정할 수 있습니다.즉, 더 작은 T가 더 많은 세부 정보를 유지합니다.그런 다음 각 프레임을 샘플링하는 동안 첫 번째 프레임을 앵커 프레임으로 사용하고 이전 프레임을 사용하여 전역 스타일 일관성과 지역 시간 일관성을 제한합니다.특히, 모든 샘플링 단계에 프레임 간 주의 [40]를 적용하여 전역 스타일 일관성을 유지합니다(4.1.1절). 또한 초기 단계에서 잠재 특징을 이전 프레임의 정렬된 잠재 특징과 융합하여 대략적인 모양 정렬을 달성합니다(4.1.2절). 그런 다음 중간 단계에서 인코딩된 워핑 앵커와 이전 출력이 있는 잠재 특징을 사용하여 미세한 텍스처 정렬을 실현합니다(4.1.3절). 마지막으로 후반 단계에서 색상 일관성을 위해 잠재 특징 분포를 조정합니다(4.1.4절). 단순화를 위해 이 섹션에서는 키 프레임을 참조하기 위해 {I}를 사용합니다. 표 1.4.1.1.1에서 중요한 표기법을 요약합니다. 스타일 인식 크로스 프레임 어텐션 다른 제로 샷 비디오 편집 방법[3, 20]과 유사하게 U-Net의 셀프 어텐션 레이어를 크로스 프레임 어텐션 레이어로 대체하여 I의 글로벌 스타일을 I½ 및 I&#39;½{_₁와 일치하도록 정규화합니다. 안정적 확산에서 각 셀프 어텐션 레이어는 I¿의 잠재 특징 vi(단순화를 위해 시간 단계 t를 생략)를 받고 v¿를 쿼리, 키 및 값 Q, K, V에 선형 투영하여 Self Attn(Q, K, V) = Softmax(QK)로 출력을 생성합니다.V(a) 입력 이미지(b) D(ε(•)) x(c) D(ε&#39;()) ×(d) D(ε* (•)) ×IQ = Wavi, K WK vi V WV vi (5) = 여기서 WQ, WK, WV는 특징 투영을 위해 사전 학습된 행렬입니다. 이에 비해 크로스 프레임 어텐션은 다른 프레임의 키 K&#39;와 값 V&#39;를 사용합니다(첫 번째와 이전 프레임 사용).즉, CrossFrame Attn(Q, K&#39;, V&#39;) Softmax().V&#39;(√d Q = Wºvi, K&#39; = WK [v1; Vi−1], V&#39; = WV [v1; Vi−1]. (6) 직관적으로, 자기 주의는 단일 프레임 내에서 패치 매칭 및 투표로 생각할 수 있는 반면, 프레임 간 주의는 유사한 패치를 찾고 다른 프레임에서 해당 패치를 융합합니다. 즉, I의 스타일은 I와 I½{-14.1.2 모양 인식 프레임 간 잠재 융합 프레임 간 주의는 글로벌 스타일로 제한됩니다. 프레임 간 로컬 모양과 텍스처 일관성을 제한하기 위해 광학 흐름을 사용하여 잠재 특징을 워프하고 융합합니다. w¹); 및 M이 각각 I;에서 Iį까지의 광학 흐름과 오클루전 마스크를 나타낸다고 합니다. x가 시간 단계 t에서 I½에 대한 잠재 특징이라고 합니다. Eq. (3)에서 예측된 t→o를 →0← M² · 0+ (1 − M²) · w¹³ (x0)으로 업데이트합니다. (7) = w 및 M은 x의 해상도와 일치하도록 다운샘플링됩니다(본 논문에서는 단순화를 위해 다운샘플링 작업을 생략합니다). 참조 프레임 I;의 경우,
--- EXPERIMENT ---
모든 결과는 기존 방법보다 제안된 프레임워크가 고품질이고 시간적으로 일관된 비디오를 렌더링하는 데 효과적임을 보여줍니다. 코드는 프로젝트 페이지에서 확인할 수 있습니다: https://www.mmlab-ntu.com/project/rerender/ 1. 서론 DALLE2[30], Imagen[34], Stable Diffusion[32]과 같은 최근의 텍스트-이미지 확산 모델은 자연어로 안내되는 다양하고 고품질의 이미지를 생성하는 뛰어난 능력을 보여줍니다. 이를 기반으로 사용자 지정 객체 생성을 위한 모델 미세 조정[33], 이미지-이미지 변환[23], 이미지 인페인팅[1], 객체 편집[12]을 포함한 다양한 이미지 편집 방법이 등장했습니다. 이러한 애플리케이션을 사용하면 사용자는 통합된 확산 프레임워크 내에서 자연어를 사용하여 이미지를 손쉽게 합성하고 편집하여 생성 효율성을 크게 개선할 수 있습니다. 소셜 미디어 플랫폼에서 비디오 콘텐츠의 인기가 급증함에 따라 보다 간소화된 비디오 제작 도구에 대한 수요도 동시에 증가했습니다. 그러나 중요한 과제가 남아 있습니다.기존 이미지 확산 모델을 비디오에 직접 적용하면 심각한 깜빡임 문제가 발생합니다.연구자들은 최근 텍스트 가이드 비디오 확산 모델을 사용하여 세 가지 솔루션을 제안했습니다.첫 번째 솔루션은 대규모 비디오 데이터에서 비디오 모델을 학습하는 것[14]으로, 상당한 컴퓨팅 리소스가 필요합니다.또한 재설계된 비디오 모델은 기존의 기성 이미지 모델과 호환되지 않습니다.두 번째 솔루션은 단일 비디오에서 이미지 모델을 미세 조정하는 것[40]으로, 긴 비디오에는 효율성이 떨어집니다.단일 비디오에 과적합하면 원본의 성능이 저하될 수도 있습니다.잘생긴 남자, 지브리 스타일 일반 모델 사용자 지정 모델 일반 모델 사용자 지정 모델 일반 모델의 아름다운 산 중국 수묵화 사용자 지정 모델 일반 모델 (a) 입력 (b) ControlNet이 없는 번역 결과 사용자 지정 모델 (c) ControlNet이 있는 번역 결과 그림 2. 사용자 지정 모델과 ControlNet은 프롬프트와 내용 모두에서 더 나은 일관성을 갖춘 고품질 결과를 생성합니다. 우리의 방법은 이러한 기존의 이미지 확산 기술과 호환되도록 설계되었으므로 스타일(프롬프트)과 콘텐츠 모델 간에 적절한 균형을 이루기 위해 이를 활용할 수 있습니다.세 번째 솔루션에는 훈련이 필요 없는 제로샷 방법[20]이 포함됩니다.확산 샘플링 프로세스 동안 시간적 일관성을 위해 잠재 특징에 교차 프레임 제약 조건이 부과됩니다.제로샷 전략은 컴퓨팅 리소스가 덜 필요하고 대부분 기존 이미지 모델과 호환되어 유망한 잠재력을 보여줍니다.그러나 현재의 교차 프레임 제약 조건은 글로벌 스타일로 제한되며 저수준 일관성을 유지할 수 없습니다.예를 들어, 전체 스타일은 일관될 수 있지만 로컬 구조와 텍스처는 여전히 깜빡일 수 있습니다.비디오 도메인에 이미지 확산 모델을 성공적으로 적용하는 것은 어려운 작업입니다.다음이 필요합니다.1) 시간적 일관성: 저수준 일관성을 위한 교차 프레임 제약 조건;2) 제로샷: 훈련이나 미세 조정 필요 없음;3) 유연성: 사용자 지정 생성을 위한 기성 이미지 모델과 호환. 위에서 언급했듯이, 이미지 모델은 일반 모델보다 대상 스타일을 더 정확하게 포착하기 위해 특정 객체에서 미세 조정하여 사용자 정의할 수 있습니다. 그림 2는 두 가지 예를 보여줍니다. 이를 활용하기 위해 이 논문에서는 모델 호환성을 위한 제로 샷 전략을 사용하고 저수준 시간적 일관성을 유지하는 이 전략의 핵심 문제를 더욱 해결하는 것을 목표로 합니다. 이 목표를 달성하기 위해 사전 학습된 이미지 모델에 대한 새로운 계층적 교차 프레임 제약 조건을 제안하여 일관된 비디오 프레임을 생성합니다. 우리의 핵심 아이디어는 광학 흐름을 사용하여 밀도가 높은 교차 프레임 제약 조건을 적용하는 것입니다. 이전에 렌더링된 프레임은 현재 프레임에 대한 저수준 참조 역할을 하고 첫 번째 렌더링된 프레임은 렌더링 프로세스를 조절하여 초기 모양과의 편차를 방지하는 앵커 역할을 합니다. 계층적 교차 프레임 제약 조건은 확산 샘플링의 여러 단계에서 실현됩니다. 글로벌 스타일 일관성 외에도 우리의 방법은 각각 초기, 중기 및 후기 단계에서 모양, 텍스처 및 색상의 일관성을 적용합니다. 이 혁신적이고 가벼운 수정은 글로벌 및 로컬 시간적 일관성을 모두 달성합니다. 그림 1은 6가지 고유한 스타일로 사용자 정의된 기성 이미지 모델에 대한 일관된 비디오 변환 결과를 보여줍니다. 이 논문은 통찰력을 바탕으로 텍스트 가이드 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크를 소개합니다. 이 프레임워크는 키 프레임 변환과 전체 비디오 변환의 두 부분으로 구성됩니다. 첫 번째 부분에서는 키 프레임을 생성하기 위해 계층적 크로스 프레임 제약 조건을 사용하여 사전 학습된 이미지 확산 모델을 조정합니다. 두 번째 부분에서는 시간 인식 패치 매칭과 프레임 블렌딩을 사용하여 렌더링된 키 프레임을 다른 프레임으로 전파합니다. 확산 기반 생성은 콘텐츠 생성에 뛰어나지만 다단계 샘플링 프로세스는 비효율적입니다. 반면 패치 기반 전파는 픽셀 수준의 일관된 프레임을 효율적으로 추론할 수 있지만 새로운 콘텐츠를 만들 수는 없습니다. 이 두 부분을 결합함으로써 프레임워크는 품질과 효율성 간의 균형을 이룹니다. 요약하자면, 우리의 주요 공헌은 다음과 같습니다. • 전역적 및 지역적 시간적 일관성을 달성하고, 학습이 필요 없으며, 사전 학습된 이미지 확산 모델과 호환되는 텍스트 기반 비디오-비디오 변환을 위한 새로운 제로샷 프레임워크. • 모양, 텍스처 및 색상의 시간적 일관성을 강제하기 위한 계층적 크로스 프레임 일관성 제약 조건으로, 이미지 확산 모델을 비디오에 적용합니다. • • 품질과 효율성 간의 균형을 맞추기 위한 하이브리드 확산 기반 생성 및 패치 기반 전파. 2. 관련 연구 2.1. 텍스트 기반 이미지 생성 설명적 문장이 있는 이미지를 생성하는 것은 직관적이고 유연합니다. 초기 시도에서는 사실적인 이미지를 합성하기 위해 GAN[42-44, 46]을 탐구합니다. Transformer[38]의 강력한 표현력으로, 각 픽셀 간에 자기 회귀 종속성이 있는 시퀀스로 이미지 픽셀을 모델링하기 위해 자기 회귀 모델[6,9,31]이 제안됩니다. DALL-E[31] 및 CogView[6]는 이미지 및 텍스트 토큰에 대한 자기 회귀 변환기를 학습합니다. Make-A-Scene [9]은 분할 마스크를 조건으로 추가로 고려합니다. 최근 연구는 점진적인 노이즈 제거 프로세스를 통해 이미지를 합성하는 텍스트-이미지 생성을 위한 확산 모델 [15]에 초점을 맞춥니다. DALLE-2 [30] 및 Imagen [34]은 텍스트 인코더로 사전 학습된 대규모 언어 모델 [28, 29]을 도입하여 이미지를 텍스트와 더 잘 정렬하고 고해상도 이미지 생성을 위한 캐스케이드 확산 모델을 도입합니다. GLIDE [26]는 텍스트 컨디셔닝을 개선하기 위해 분류기 없는 안내를 도입합니다. 이미지 공간에서 노이즈 제거를 적용하는 대신 Latent Diffusion Models [32]는 VQ-GAN [7]의 저해상도 잠재 공간을 사용하여 효율성을 개선합니다. 철저한 조사는 [4]를 참조하십시오. 일반 이미지에 대한 확산 모델 외에도 사용자 지정 모델이 연구됩니다. Textual Inversion [10] 및 DreamBooth [33]는 특수 토큰을 학습하여 새로운 개념을 포착하고 소수의 예제 이미지가 주어진 경우 관련 이미지를 생성합니다. LORA[17]는 기존 가중치에 추가된 저랭크 가중치 행렬을 학습하여 대형 모델의 미세 조정을 가속화합니다.ControlNet[45]은 에지 맵 및 포즈와 같은 픽셀 수준 조건을 제공하기 위해 새로운 제어 경로를 미세 조정하여 세분화된 이미지 생성을 가능하게 합니다.우리의 방법은 사전 학습된 모델을 변경하지 않으므로 이러한 기존 기술과 직교합니다.이를 통해 우리 방법은 더 나은 사용자 지정 비디오 변환을 위해 DreamBooth 및 LoRA를 활용하고 그림 2와 같이 시간적으로 일관된 구조 안내에 ControlNet을 사용할 수 있습니다.2.2. 확산 모델을 사용한 비디오 편집 비디오 확산 텍스트-비디오 생성을 위해 Model[16]은 이미지 모델의 2D U-Net을 인수분해된 공간-시간 UNet으로 확장하는 것을 제안합니다.Imagen Video[14]는 공간 및 시간 비디오 초고해상도 모델의 캐스케이드로 비디오 확산 모델을 확장하며, 이는 Dreamix[25]에 의해 비디오 편집으로 더욱 확장됩니다. Make-A-Video[36]는 비지도 방식으로 비디오 데이터를 활용하여 이미지 모델을 구동하는 움직임을 학습합니다. 유망하기는 하지만 위의 방법은 학습을 위해 대규모 비디오 데이터가 필요합니다. Tune-A-Video[40]는 대신 이미지 확산 모델을 교차 프레임 주의가 있는 비디오 모델로 팽창시키고 단일 비디오에서 미세 조정하여 관련 동작이 있는 비디오를 생성합니다. 이를 기반으로 Edit-A-Video[35], VideoP2P[22] 및 vid2vid-zero[39]는 편집되지 않은 영역을 보존하기 위해 정확한 반전을 위해 Null-Text Inversion[24]을 사용합니다. 그러나 이러한 모델은 사전 학습된 모델의 미세 조정이나 입력 비디오에 대한 최적화가 필요하므로 효율성이 떨어집니다. 최근 개발에서는 설계상 학습 단계 없이 작동하는 zeroshot 방법이 도입되었습니다. 따라서 이러한 방법은 깊이 및 모서리와 같은 보다 유연한 조건을 수용하기 위해 InstructPix2Pix[2] 또는 ControlNet과 같은 사전 학습된 확산 변형과 자연스럽게 호환됩니다. Prompt2Prompt [12]에서 감지한 편집 마스크에 따라 보존할 채널과 공간 영역을 나타내며, FateZero [27]는 편집 전후의 주의 특징을 혼합합니다.Text2Video-Zero [20]는 잠재 객체를 변환하여 동작을 직접 시뮬레이션하고 Pix2Video [3]는 현재 프레임의 잠재 객체를 이전 프레임의 잠재 객체와 일치시킵니다.위의 모든 방법은 시간적 일관성을 개선하기 위해 주로 프레임 간 주의와 초기 단계 잠재 객체 융합에 의존합니다.그러나 나중에 보여드리겠지만 이러한 전략은 주로 고수준 스타일과 모양에 맞춰져 있으며 텍스처와 디테일 수준에서 프레임 간 일관성을 유지하는 데 덜 효과적입니다.이러한 접근 방식과 달리, 본 방법에서는 픽셀 수준의 시간적 일관성을 비사소하게 달성하는 새로운 픽셀 인식 프레임 간 잠재 객체 융합을 제안합니다.또 다른 제로샷 솔루션은 하나 이상의 확산 편집된 프레임을 기반으로 비디오를 추론하기 위해 프레임 보간을 적용하는 것입니다. 이미지 유추의 선구적 작업[13]은 샘플 쌍의 스타일 효과를 다른 이미지로 이전합니다.I 키 프레임 변환 Ik 전체 비디오 변환 첫 번째 프레임;첫 번째 키 프레임;앵커 프레임 교차 프레임 주의;픽셀 인식 퓨전 12K 키 프레임 비키 프레임 프레임 보간 → 교차 프레임 주의;모양 인식 퓨전;픽셀 인식 퓨전;색상 AdaIN 그림 3. 패치 매칭을 사용하여 프레임워크에 시간적 제약을 부과하기 위한 다른 프레임 간의 상호 작용에 대한 설명.Fišer et al.[8]은 얼굴 특징을 안내하여 이미지 유추를 얼굴 비디오 변환으로 확장합니다.나중에 Jamrivška et al.[19]은 새로운 시간 블렌딩 접근 방식을 사용하여 여러 샘플 프레임을 기반으로 하는 일반 비디오 변환을 위한 개선된 EbSynth를 제안합니다.이러한 패치 기반 방법은 미세한 세부 사항을 보존할 수 있지만 시간적 일관성은 주로 샘플 프레임 전체의 응집성에 의존합니다. 따라서 코히어런트 프레임을 생성하기 위한 우리의 적응형 확산 모델은 이러한 방법에 매우 적합하며, 이는 그림 11에서 나중에 보여줄 것입니다. 이 논문에서 우리는 더 나은 시간적 일관성을 달성하고 추가 학습 없이 추론을 가속화하기 위해 제로샷 EbSynth를 프레임워크에 통합합니다. 3. 예비: 확산 모델 안정적 확산 안정적 확산은 오토인코더 D(E())의 잠재 공간에서 작동하는 잠재 확산 모델입니다. 여기서 &amp; 및 는 각각 인코더와 디코더입니다. 구체적으로, 잠재 특징 X 0 = E(I)를 갖는 이미지 I의 경우, 확산 순방향 프로세스는 잠재 _ q(2t|2t−1)=N(2t;Vatt-1,(1 - a)I), (1)에 노이즈를 반복적으로 추가합니다. 여기서 t = 1, ..., T는 시간 단계이고, q(xt|xt−1)은 주어진 xt의 조건부 밀도입니다. It-1이고 at는 하이퍼파라미터입니다. 또는, 우리는 q(xt|xo) = N(xt; √ātxo, (1 — āt)Ī)인 xo에서 임의의 시간 단계에서 xt를 직접 샘플링할 수 있습니다. 여기서 ā₁ = [[±1 αi. (2) 그런 다음 확산 역방향 프로세스인 U-Net €0가 xT에서 x를 반복적으로 복구하기 위해 잠재 노이즈를 예측하도록 학습됩니다. 큰 T가 주어지면 xo는 순방향 프로세스에서 완전히 파괴되어 x가 표준 가우시안 분포에 근접합니다. 따라서 e는 무작위 가우시안 노이즈에서 유효한 xo를 유추하는 법을 그에 따라 학습합니다. 영어: 일단 훈련되면, 결정론적 DDIM 샘플링[37]을 사용하여 xt를 기반으로 xt-1을 샘플링할 수 있습니다: –1€0(2t,t,Cp), (3) x+-DDIM 샘플링 T를 가리키는 예측된 xo 방향 4,xt→x}-1,x橈=&quot;지브리 스타일의 아름다운 소녀&quot;, DDPM 순방향 E 사전 훈련된 이미지 확산 모델 ε 워프 Sec. 4.1.크로스 프레임 어텐션 Sec. 4.1.Sec. 4.1.모양 인식 AdaIN 잠재 융합 11, 12, ..., IN 사전 훈련된 이미지 확산 모델 ε &quot;지브리 스타일의 아름다운 소녀&quot;, Sec. 4.1.픽셀 인식 잠재 융합 Sec. 4.1.E 충실도 지향 이미지 인코딩 워프 I; 및 Mi 1,12,..., IN D 집계 x²-I 교차 프레임 주의 T Tpo 모양 인식 잠재 융합 픽셀 인식 잠재 융합 Τρι T₂ 적응 잠재 조정 (a) 계층적 교차 프레임 일관성 제약이 있는 확산 기반 키 프레임 변환 (b) 샘플링 파이프라인 그림 4. 제안된 제로 샷 텍스트 가이드 비디오 변환의 프레임워크. (a) 계층적 교차 프레임 제약을 사용하여 사전 학습된 이미지 확산 모델(Stable Diffusion + ControlNet)을 조정하여 일관된 프레임을 렌더링합니다. 빨간색 점선은 원본 이미지 확산 모델의 샘플링 프로세스를 나타냅니다. 검은색 선은 비디오 변환을 위해 조정된 프로세스를 나타냅니다. (b) 다른 샘플링 단계에서 다른 제약을 적용합니다. 여기서 to는 시간 단계 t에서 예측된 xo, xt→t(x+- √1-α+€(xt, t, Cp))/√√αt이고, 0(xt, t, Cp)는 시간 단계 t와 텍스트 프롬프트 조건 Cp를 기반으로 한 xt의 예측 노이즈입니다. = ~ 표 1. 표기법 요약. 설명 안정적 확산의 이미지 인코더 및 디코더 i번째 키 프레임(4.1절); i번째 비디오 프레임(4.2절) I의 변환 결과; 제안된 방법을 사용한 I의 변환 결과; PA 융합 없이(그림 11(b)) (4) 표기법 E, DE* 제안된 충실도 지향 이미지 인코더 Ii II&#39; I, Mi I&#39; x x² wi, M I의 잠재 특징; 확산 역방향 노이즈 제거 단계 t에서 I의 추정된 x; 확산 역방향 잡음 제거 단계 t에서 I의 잠재 특징 확산 순방향 샘플링 단계 t에서 I에서 Ii로의 광학 흐름 및 폐색 마스크 추론하는 동안 DDIM 샘플링을 사용하여 표준 가우시안 잡음 XT ZT, ZT N(0, 1)에서 유효한 xo를 샘플링하고 xo를 최종 생성된 이미지 I&#39; = D(x0)로 디코딩할 수 있습니다.ControlNet 자연어는 유연하지만 출력에 대한 공간 제어가 제한적입니다.공간적 제어성을 개선하기 위해 [45]는 모서리, 깊이 및 인간 포즈와 같은 추가 조건을 허용하기 위해 ControlNet이라는 측면 경로를 Stable Diffusion에 도입합니다.cf를 추가 조건으로 하면 ControlNet을 사용한 U-Net의 잡음 예측은 €0(xt, t, Cp, Cf)가 됩니다.InstructPix2Pix에 비해 ControlNet은 사용자 지정 Stable Diffusion 모델과 직교합니다.일반적인 제로샷 V2V 프레임워크를 구축하기 위해 ControlNet을 사용하여 입력 비디오에서 구조 지침을 제공하여 시간적 일관성을 개선합니다. 4. 제로 샷 텍스트 가이드 비디오 변환 N N개의 프레임 {I}0이 있는 비디오가 주어졌을 때, 우리의 목표는 텍스트 프롬프트 및/또는 기성품 맞춤형 안정적 확산 모델에 의해 지정된 다른 예술적 표현으로 그것을 새로운 비디오 {I}로 렌더링하는 것입니다.우리의 프레임워크는 두 부분으로 구성됩니다.키 프레임 변환(4.1절)과 전체 비디오 변환(4.2절)입니다.첫 번째 부분에서는 사전 학습된 이미지 확산 모델에 4개의 계층적 교차 프레임 제약 조건을 도입하여 그림 3에서 설명한 대로 앵커 및 이전 키 프레임을 사용하여 일관된 키 프레임의 렌더링을 안내합니다.그런 다음 두 번째 부분에서는 인접한 두 키 프레임을 기반으로 키가 아닌 프레임을 보간합니다.따라서 우리의 프레임워크는 서로 다른 프레임 간의 관계를 최대한 활용하여 출력의 시간적 일관성을 향상시킬 수 있습니다.I와 I₁를 I로 워핑한 결과와 변환된 키 프레임 I&#39;₁를 I₂의 잠재 특징으로 전파한 오클루전 마스크 결과입니다. E 4.1로 인코딩됨. 키 프레임 변환 ~ 그림 4는 키 프레임 변환을 위한 T-단계 샘플링 파이프라인을 보여줍니다. SDEdit [23]에 따라 파이프라인은 XT = √āτx0+ (1āT)ZT, ZT N(0, 1)로 시작합니다. 이는 순수한 가우시안 노이즈가 아닌 입력 비디오 프레임의 노이즈가 있는 잠재 코드입니다. 사용자는 T를 조정하여 입력 프레임의 세부 정보가 출력에서 얼마나 보존되는지 결정할 수 있습니다. 즉, 더 작은 T가 더 많은 세부 정보를 유지합니다. 그런 다음 각 프레임을 샘플링하는 동안 첫 번째 프레임을 앵커 프레임으로 사용하고 이전 프레임을 사용하여 전역 스타일 일관성과 로컬 시간 일관성을 제한합니다. 구체적으로, 모든 샘플링 단계에 프레임 간 주의 [40]를 적용하여 전역 스타일 일관성을 유지합니다(4.1.1절). 또한 초기 단계에서 잠재 특징을 이전 프레임의 정렬된 잠재 특징과 융합하여 대략적인 모양 정렬을 달성합니다(4.1.2절). 그런 다음 중간 단계에서 인코딩된 워핑 앵커와 이전 출력이 있는 잠재 특징을 사용하여 미세한 텍스처 정렬을 실현합니다(4.1.3절). 마지막으로 후반 단계에서 색상 일관성을 위해 잠재 특징 분포를 조정합니다(4.1.4절). 단순화를 위해 이 섹션에서는 키 프레임을 나타내는 데 {I}를 사용합니다. 표 1.4.1.1에 중요한 표기법을 요약했습니다. 스타일 인식 크로스 프레임 어텐션 다른 제로 샷 비디오 편집 방법[3, 20]과 유사하게 U-Net의 셀프 어텐션 레이어를 크로스 프레임 어텐션 레이어로 대체하여 I의 글로벌 스타일을 I½ 및 I&#39;½{_₁의 스타일과 일치하도록 정규화합니다. 안정적 확산에서 각 셀프 어텐션 레이어는 I¿의 잠재 특징 vi(단순화를 위해 시간 단계 t를 생략)를 받고 v¿를 쿼리, 키 및 값 Q, K, V에 선형적으로 투영하여 Self Attn(Q, K, V) = Softmax(QK)에 의해 출력을 생성합니다. V with (a) 입력 이미지 (b) D(ε(•)) x(c) D(ε&#39;()) ×(d) D(ε* (•)) ×IQ = Wavi, K WK vi V WV vi (5) = 여기서 WQ, WK, WV는 특징 투영을 위한 사전 학습된 행렬입니다. 비교해 보면 크로스 프레임 어텐션은 다른 프레임의 키 K&#39;와 값 V&#39;를 사용합니다(첫 번째와 이전 프레임 사용). 즉, CrossFrame Attn(Q, K&#39;, V&#39;) Softmax()입니다. V&#39;는 √d Q = Wºvi, K&#39; = WK [v1; Vi−1], V&#39; = WV [v1; Vi−1]입니다. (6) 직관적으로 자기 주의는 단일 프레임 내에서 패치 매칭 및 투표로 생각할 수 있는 반면, 프레임 간 주의는 유사한 패치를 찾고 다른 프레임에서 해당 패치를 융합하므로 I의 스타일은 I 및 I½{-14.1.2 모양을 인식하는 프레임 간 잠재 융합 프레임 간 주의는 전역 스타일로 제한됩니다. 프레임 간 로컬 모양과 텍스처 일관성을 제한하기 위해 광학 흐름을 사용하여 잠재 특징을 워프하고 융합합니다. w¹); 및 M을 각각 I;에서 Iį까지의 광학 흐름 및 오클루전 마스크를 나타냅니다. x를 시간 단계 t에서 I½에 대한 잠재 특징이라고 합니다. Eq. (3)에서 예측된 t→o를→0← M² · 0+ (1 − M²) · w¹³ (x0)으로 업데이트합니다. (7) = w 및 M은 x의 해상도와 일치하도록 다운샘플링됩니다(본 논문에서는 단순화를 위해 다운샘플링 작업을 생략합니다). 참조 프레임 I;의 경우 앵커 프레임(j 0)이 이전 프레임(ji - 1)보다 더 나은 안내를 제공한다는 것을 실험적으로 발견했습니다. 잠재 공간에서 요소를 보간하면 후반 단계에서 흐릿함과 모양 왜곡이 발생할 수 있음을 관찰했습니다. 따라서 대략적인 모양 안내를 위해 융합을 초반 단계로만 제한합니다. 4.1.3 픽셀 인식 크로스 프레임 잠재 융합 중간 단계에서 저수준 텍스처 특징을 제한하기 위해 잠재 특징을 워핑하는 대신 이전 프레임을 워핑하여 다시 잠재 공간으로 인코딩하여 인페인팅 방식으로 융합할 수 있습니다. 그러나 손실 자동 인코더는 프레임 시퀀스를 따라 쉽게 축적되는 왜곡과 색상 편향을 도입합니다. 그림 5(b)는 E&#39;(e) Mε(g) (c)의 오차 맵(h) (d)의 오차 맵(f) (b)의 오차 맵 그림 5. 충실도 지향 이미지 인코딩. D 방정식. (9) E +x D ε* Extr D(E&#39;(I)) Mε Eq. (8)++ E&#39; (I)-D는 낮은 오류 마스크를 계산합니다.그림 6. 충실도 지향 이미지 인코딩의 파이프라인.10번 인코딩 및 디코딩한 후의 왜곡된 결과의 예.[1]은 각 이미지에 맞게 디코더의 가중치를 미세 조정하여 이 문제를 해결했지만 긴 비디오에는 비실용적입니다.이 문제를 효율적으로 해결하기 위해 새로운 충실도 지향 제로 샷 이미지 인코딩 방법을 제안합니다.충실도 지향 이미지 인코딩 우리의 주요 통찰력은 반복적 자동 인코딩 프로세스에서 매번 손실되는 정보의 양이 일관적이라는 관찰입니다.따라서 보상을 위해 정보 손실을 예측할 수 있습니다.특히, 임의의 이미지 I에 대해 두 번 인코딩하고 디코딩하여 x E(I), Ir D(x) 및 x = E(Ir), Irr D(x)를 얻습니다.대상 무손실 xo에서 x로의 손실은 x에서 x로의 손실에 선형이라고 가정합니다. 그런 다음 보상을 적용한 인코딩 &amp;&#39;을 = = E&#39; (I) = x + ε (x − x), =로 정의합니다. 여기서 선형 계수 ε = = (8) 1이 잘 작동합니다. 보상으로 인해 발생하는 가능한 아티팩트(예: 그림 5(c)의 눈 근처의 파란색 아티팩트)를 방지하기 위해 마스크 Mε를 추가합니다. Mε는 I와 D(E&#39;(I)) 사이의 오차가 사전 정의된 임계값 아래에 있는 경우를 나타냅니다. 그런 다음 새로운 충실도 지향 이미지 인코딩 ε*는 E* (I) := x² + Mɛ · λɛ(x − x˜˜¯ )의 형태를 취합니다. (9) 인코딩 파이프라인은 그림 6에 요약되어 있습니다. 그림 5(d)에서 볼 수 있듯이, 우리의 방법은 10번 인코딩 및 디코딩한 후에도 이미지 정보를 잘 보존합니다. 구조 기반 인페인팅 그림 7에 나와 있듯이 픽셀 수준의 일관성을 위해 앵커 프레임 I와 I&#39;, {-M-1, M 합집합 I를 워프하고 오버레이합니다.(식 (10)) 샘플(식 (2)) X-Mi 다운샘플 x-DDIM 샘플링 × T x 인페인트(식 (3)) 업데이트 x-그림 7. 픽셀 인식 잠재 융합의 파이프라인. -이전 프레임 I½{_₁에서 i번째 프레임까지 그리고 픽셀 인식 교차 프레임 잠재 융합 없이 얻은 대략적으로 렌더링된 프레임 I에 Mổ (M_r I+(1–M_1) _1(I_1))+(1−M%) (6) i-(10)으로 중첩합니다. 그 결과 융합된 프레임 Ï½는 I 샘플링을 위한 픽셀 참조를 제공합니다. 즉, I가 마스크 영역 M¿ = M₁¦ M²_₁ 외부에서 I와 일치하고 Mi 내부의 ControlNet에서 구조 안내와 일치하기를 원합니다. 이를 구조 안내 인페인팅 작업으로 공식화하고 [1]에 따라 Eq. (3)의 x²-1을 -x²-1 ← Mi • x² −1 + (1 − M¿) ⋅ xț −1, (11) E* (I) 기반, 여기서 x1은 샘플링된 xt-on Eq. (2)입니다. from xo = 4.1.4 색상 인식 적응적 잠재 조정 마지막으로, 우리는 AdaIN [18]을 현명한 평균과 분산에 적용하여 ✰ 를 늦은 단계에서 채널t→와 일치시킵니다. 그것은 또한 전체 키 프레임에서 색상 스타일을 일관되게 유지할 수 있습니다. 4.2. 전체 비디오 변환 유사한 내용이 있는 프레임의 경우 Ebsynth [19]와 같은 기존 프레임 보간 방법은 렌더링된 프레임을 이웃 프레임으로 효율적으로 전파하여 그럴듯한 결과를 생성할 수 있습니다. 그러나 프레임 보간은 확산 모델과 비교할 때 새로운 내용을 만들 수 없습니다. 품질과 효율성의 균형을 맞추기 위해 우리는 적응된 확산 모델과 Ebsynth를 사용하여 키 프레임과 다른 프레임을 각각 렌더링하는 하이브리드 프레임워크를 제안합니다. 구체적으로, 우리는 모든 K 프레임, 즉 Io, IK, I2K,...에 대해 키 프레임을 균일하게 샘플링하고 적응된 확산 모델을 통해 To, IK, 2K...로 렌더링합니다. 그런 다음 나머지 비키 프레임을 렌더링합니다. 예를 들어 I¿ (0 &lt; i &lt; K)를 취하면 Ebsynth를 채택하여 I½를 인접한 양식화된 키 프레임 I 및 I와 보간합니다.Ebsynth에는 프레임 전파와 프레임 혼합의 두 단계가 있습니다.다음에서 이 두 단계의 주요 아이디어를 간략히 소개하고 Ebsynth를 프레임워크에 적용하는 방법에 대해 설명합니다.구현 세부 정보는 [19]를 참조하세요.4.2.1 단일 키 프레임 전파 프레임 전파는 양식화된 키 프레임을 밀집 대응 관계를 기반으로 인접한 비키 프레임으로 워핑하는 것을 목표로 합니다.Ebsynth를 직접 따라 색상, 위치, 가장자리 및 시간적 안내를 사용하여 밀집 대응 예측 및 프레임 워핑을 위한 가이드 경로 매칭 알고리즘을 채택합니다.프레임워크는 각 키 프레임을 이전 K 1 및 다음 K - 1 프레임으로 전파합니다.I&#39;를 전파한 결과를 표시합니다. 영어: I¿를 I로 지정합니다. I¿(0 &lt; i &lt; K)의 경우 근처 키 프레임 I와 I&#39;κk에서 두 개의 결과 I와 IK를 얻습니다. 4.2.2 시간 인식 블렌딩 프레임 블렌딩은 I/O와 I를 블렌딩하여 최종 결과 I{를 얻는 것을 목표로 합니다. Ebsynth는 3단계 블렌딩 방식을 제안합니다. 1) 각 위치에 대한 패치 매칭(4.2.1절) 중에 오류가 가장 낮은 것을 선택하여 I10과 I!K의 색상과 그래디언트를 결합합니다. 2) 결합된 색상 이미지를 히스토그램 참조로 사용하여 I 및 IK에 대한 대비 보존 블렌딩[11]을 통해 초기 블렌딩 이미지를 생성합니다. 3) 결합된 그래디언트를 그래디언트 참조로 사용하여 초기 블렌딩 이미지에 대한 스크린 포아송 블렌딩[5]을 통해 최종 결과를 얻습니다. 다르게, 우리 프레임워크는 처음 두 블렌딩 단계만 채택하고 초기 블렌딩 이미지를 I½로 사용합니다. 우리는 때때로 평평하지 않은 영역에서 아티팩트를 발생시키고 비교적 시간이 많이 걸리는 푸아송 블렌딩을 적용하지 않습니다.5. 실험 결과 5.1. 구현 세부 정보 = = = = 실험은 하나의 NVIDIA Tesla V100 GPU에서 수행됩니다.우리는 https://civitai.com/의 Stable Diffusion 1.5를 기반으로 하는 fine-tuned 및 LoRA 모델을 사용합니다.우리는 Stable Diffusion을 사용하며 원래 Tmax 1000단계를 사용합니다.그림 4(b)의 샘플링 파이프라인의 경우 기본적으로 Ts를 0.1Tmax, Tpo를 0.5Tmax, Tp를 0.8Tmax, Ta를 0.8Tmax로 설정하고 DDIM 샘플링의 20단계를 사용합니다.우리는 각 비디오에 대해 T를 조정합니다.ControlNet[45]은 각 비디오에 대해 조정된 제어 가중치와 함께 에지 측면에서 구조 지침을 제공하는 데 사용됩니다.우리는 광학 흐름 추정에 GMFlow[41]를 사용하고 전방-후방 일관성 검사를 통해 오클루전 마스크를 계산합니다. 전체 비디오 변환의 경우 기본적으로 K = 10 프레임마다 키 프레임을 샘플링합니다. 테스트 비디오는 https://www.pexels.com/ 및 https://pixabay.com/에서 가져온 것이며, 짧은 면은 512로 크기가 조정되었습니다. 512x512 비디오의 실행 시간 측면에서 키 프레임 및 비키 프레임 변환은 각각 프레임당 약 14.23초 및 1.49초가 걸립니다. (I) 저희의 (e) (P) Text2Video-Zero Pix2Video (၁) Fate Zero (9) vid2vid-zero (a) RRRR 프롬프트: 흰색 고대 그리스 조각품, 밀로의 비너스, 밝은 분홍색과 파란색 배경 프롬프트: 중국 수묵화 속의 백조, 단색 그림 8. 제로 샷 비디오 변환 방법과의 시각적 비교. 자홍색 상자는 일관되지 않은 영역을 나타냅니다. Text2 VideoZero와 저희 방법의 경우 픽셀 수준의 일관성을 더 잘 시각화하기 위해 영역을 더 확대합니다. 각각 프레임당. 전반적으로 전체 비디오 변환은 프레임당 약 (14.23 +1.49(K − 1))/K = 1.49+12.74/Ks가 걸립니다. 논문이 출판되면 코드를 공개하겠습니다. 5.2. 최신 방법과의 비교 = K 5를 사용한 키 프레임 변환에서 vid2vid-zero [39], Fate Zero [27], Pix2Video [3], Text2 Video-Zero [20]의 네 가지 최신 제로샷 방법과 비교합니다. 처음 세 가지 방법의 공식 코드는 ControlNet을 지원하지 않으며 사용자 지정 모델을 로드할 때 그럴듯한 결과를 생성하지 못하는 것을 발견했습니다. 예를 들어 vid2vid-zero는 입력과 완전히 다른 프레임을 생성합니다. 따라서 Text2Video-Zero와 우리 방법만 ControlNet을 사용하여 사용자 지정 모델을 사용합니다. 그림 8과 그림 9는 시각적 결과를 보여줍니다. FateZero는 입력 프레임을 성공적으로 재구성하지만 프롬프트와 일치하도록 조정하지 못합니다. 반면, vid2vid-zero와 Pix2 Video는 입력 프레임을 과도하게 수정하여 프레임 간에 상당한 모양 왜곡과 불연속성을 초래합니다. Text2Video-Zero에서 생성된 각 프레임은 고품질을 보이지만 검은색 상자로 표시된 것처럼 로컬 텍스처의 일관성이 부족합니다. 마지막으로, 제안된 방법은 출력 품질, 콘텐츠 및 프롬프트 매칭 및 시간적 일관성 측면에서 명확한 우수성을 보여줍니다. 정량적 평가를 위해 우리는 FateZero와 Pix2Video를 따라 Fram-Acc(CLIP 기반 프레임별 Table 2. 정량적 비교 및 사용자 선호도. 지표 v2v-zero Fate Zero Pix2Video T2V-Zero Ours Fram-Acc 0.0.0.0.0.Tem-Con 0.0.0.0.0.Pixel-MSE 0.0.0.0.0.User-Balance 3.8% 5.9% 9.2% 15.4% 65.8% User-Temporal 3.8% User-Overall 9.6% 4.2% 10.8% 71.6% 2.9% 4.2% 4.2% 15.0% 73.7% 편집 정확도), Tmp-Con(연속 프레임 간의 CLIP 기반 코사인 유사도), Pixel-MSE(평균 제곱 평균 픽셀 오류)를 보고합니다. 영어: 정렬된 연속 프레임 사이) 표 2에서. 우리의 방법은 가장 좋은 시간적 일관성과 두 번째로 좋은 프레임 편집 정확도를 달성합니다. 우리는 또한 30명의 참가자를 대상으로 사용자 연구를 수행합니다. 참가자들은 세 가지 기준에 따라 5가지 방법 중에서 가장 좋은 결과를 선택하도록 요청받습니다. 1) 프롬프트와 입력 프레임 간의 결과 균형, 2) 결과의 시간적 일관성, 3) 비디오 변환의 전반적인 품질. 표 2는 8개의 테스트 비디오에 대한 평균 선호도 비율을 제시하며, 우리의 방법은 세 가지 지표 모두에서 가장 높은 비율을 달성합니다. 5.3. 절제 연구 계층적 교차 프레임 일관성 제약 조건 그림 10은 다른 키 프레임 샘플링 간격 K가 있는 경우와 없는 경우의 결과를 비교합니다.표 3. 키 프레임 샘플링 간격 K의 효과 (c) 우리의 (b) Text2Video-Zero (a) 입력 프롬프트: 만화 스타일의 나비 (a) 입력 프롬프트: CG 스타일의 아름다운 여성 프롬프트: 깨끗하고 단순한 흰색 옥 조각품 (b) Text2Video-Zero (c) 우리의 그림 9. Text2Video-Zero와의 시각적 비교. Text2VideoZero와 저희 방법은 공정한 비교를 위해 동일한 사용자 지정 모델과 ControlNet을 사용합니다. 저희 방법은 로컬 텍스처 시간적 일관성 측면에서 Text2VideoZero보다 성능이 뛰어납니다. 빨간색 상자는 일관되지 않은 영역을 나타냅니다. 프레임 간 일관성 제약. 저희는 전반부에 간단한 변환 동작이 포함된 비디오와 후반부에 복잡한 3D 회전 변환이 포함된 비디오에서 저희 접근 방식의 효능을 보여줍니다. 시간적 일관성을 더 잘 평가하기 위해 독자들에게 프로젝트 웹페이지에서 비디오를 시청할 것을 권장합니다. 프레임 간 주의는 글로벌 스타일의 일관성을 보장하는 반면, 4.1.4절의 적응적 잠재 조정은 첫 번째 프레임과 동일한 머리 색깔을 유지하거나 머리 색깔이 입력 프레임을 따라 어두워집니다. 적응적 잠재 조정은 사용자가 어떤 색상을 따를지 결정할 수 있도록 하는 선택 사항입니다. 위의 두 가지 글로벌 제약은 로컬 동작을 포착할 수 없습니다. 4.1.2절의 모양 인식 잠재 융합(SA 융합)은 잠재 특징을 변환하여 넥 링을 변환하여 이를 해결하지만 복잡한 동작에 대해 픽셀 수준의 일관성을 유지할 수 없습니다. 제안된 픽셀 인식 잠재 융합(PA 융합)만이 헤어 스타일과 여드름과 같은 국소적 세부 사항을 일관되게 렌더링할 수 있습니다. 그림 11-12에서 PA 융합의 효과를 보여주기 위해 추가 예를 제공합니다. ControlNet은 구조를 잘 안내할 수 있지만 노이즈 추가 및 노이즈 제거로 인해 발생하는 고유한 무작위성으로 인해 국소적 텍스처에서 일관성을 유지하기 어려워 요소가 누락되고 세부 사항이 변경됩니다. 제안된 PA 융합은 이전 프레임의 해당 픽셀 정보를 활용하여 이러한 세부 사항을 복원합니다. 더욱이 키 프레임 간의 이러한 일관성은 보간된 비키 프레임의 고스팅 아티팩트를 효과적으로 줄일 수 있습니다. 충실도 중심 이미지 인코딩 그림에서 충실도 중심 이미지 인코딩에 대한 자세한 분석을 제시합니다. 13Metric K =Fram-Acc 1.000 1.000 1.Tem-Con 0.992 0.Pixel-MSE 0.037 0.028 0.K =K =K =K =K =1.0.0.0.0.0.0.0.0.15, 그림 5 외에도. Stable Diffusion의 공식적으로 출시된 두 개의 자동 인코더인 미세 조정된 f8-ft-MSE VAE와 원래의 손실이 더 큰 kl-f8 VAE가 우리 방법을 테스트하는 데 사용됩니다. 미세 조정된 VAE는 아티팩트를 도입하고 원래의 VAE는 그림 13(b)와 같이 큰 색상 편향을 초래합니다. 제안하는 충실도 중심 이미지 인코딩은 이러한 문제를 효과적으로 완화합니다. 정량적 평가를 위해, MS-COCO [21] 검증 세트의 처음 1,000개 이미지를 사용하여 그림 14에서 여러 번의 인코딩과 디코딩 후 입력 이미지와 재구성된 결과 간의 MSE를 보고합니다. 결과는 시각적 관찰과 일치합니다. 제안하는 방법은 원시 인코딩 방법에 비해 오류 누적을 크게 줄입니다. 마지막으로, 그림 15(b)(c)에서 비디오 변환 프로세스에서 인코딩 방법을 검증합니다. 여기서는 Eq. (10)에서 앵커 프레임 없이 이전 프레임만 사용하여 오류 누적을 더 잘 시각화합니다. 우리 방법은 주로 손실 인코딩으로 인해 발생하는 세부 정보 손실과 색상 편향을 줄입니다. 게다가 파이프라인에는 앵커 프레임과 적응적 잠재 조정이 포함되어 그림 15(d)에 표시된 것처럼 변환을 추가로 조절하는데, 여기서는 명백한 오류가 관찰되지 않습니다. 키 프레임 K의 빈도 표 3에서 다른 K에서 그림 10(a)의 정량적 전체 비디오 변환 결과를 보고합니다. 큰 K를 사용하면 프레임 보간이 많을수록 픽셀 수준의 시간적 일관성이 향상되지만 품질이 떨어져 Fram-Acc가 낮아집니다. 균형을 위해 K = [5,20]의 넓은 범위가 권장됩니다.5.4. 추가 결과 유연한 구조 및 색상 제어 제안된 파이프라인은 x의 초기화를 통해 콘텐츠 보존을 유연하게 제어할 수 있습니다.xã를 가우시안 노이즈로 설정하는 대신(그림 16(b)), 세부 정보를 더 잘 보존하기 위해 입력 프레임의 노이즈가 있는 잠재 버전을 사용합니다(그림 16(c)).사용자는 T 값을 조정하여 콘텐츠와 프롬프트의 균형을 맞출 수 있습니다.또한 입력 프레임에 원치 않는 색상 편향(예: 중국 잉크 그림의 푸른 하늘)이 도입되는 경우 색상 보정 옵션이 제공됩니다.입력 프레임은 XT = ZT로 생성된 프레임의 색상 히스토그램과 일치하도록 조정됩니다(그림 16(b)).조정된 프레임을 입력으로 사용하면(그림 16(a)의 아랫줄) 렌더링된 결과(그림 16(c)-(f)의 아랫줄)가 프롬프트에서 지정한 색상과 더 잘 일치합니다.응용 프로그램 그림 17은 우리 방법의 몇 가지 응용 프로그램을 보여줍니다. &#39;귀여운 고양이/여우/햄스터/토끼&#39;라는 프롬프트를 사용하여, PA 융합 없이 we(b) (a) 전체 BEBEBE →&gt; Legend 여드름은 보존됨 목 고리는 목과 함께 변환됨 일관된 헤어 스타일 일관된 귀 옆의 머리카락 가닥 (a) 입력 비디오 (b) 기준선(이미지 모델)(c) (b)+크로스 프레임 주의 (d) (c)+AdaIN (e) (d)+SA 융합 (f) 전체 그림 10. 제안된 계층적 크로스 프레임 제약 조건의 효과. (a) 입력 프레임 #1, #55, #94. (b) 이미지 확산 모델은 각 프레임을 독립적으로 렌더링합니다. (c) 크로스 프레임 주의는 전반적인 스타일을 일관되게 유지합니다. (d) AdaIN은 머리 색상을 보존합니다. (e) 모양 인식 잠재 융합은 객체의 전반적인 움직임을 일관되게 유지합니다. (f) 픽셀 인식 잠재 융합은 픽셀 수준의 시간적 일관성을 달성합니다. 키 프레임 - 전파된 프레임. 키 프레임 ##83 ##87 #그림 11. 프레임 전파에 대한 픽셀 인식 잠재 융합의 효과. 제안된 픽셀 인식 잠재 융합은 일관된 키 프레임을 생성하는 데 도움이 됩니다. 이것이 없으면 키 프레임 간의 픽셀 수준 불일치로 인해 프레임 블렌딩 중에 키가 아닌 프레임에 고스팅 아티팩트가 발생합니다. 그림 17(a)에서 개를 다른 종류의 애완동물로 변환하기 위해 텍스트 안내 편집을 수행할 수 있습니다. 만화나 사진을 생성하기 위해 사용자 지정 모드를 사용하여 각각 그림 17(b)와 그림 17(c)(d)에서 비사실적 및 사실적 렌더링을 달성할 수 있습니다. 그림 18에서는 실제 인간 비디오와 외모를 설명하는 프롬프트를 기반으로 소설과 만화의 합성된 동적 가상 캐릭터를 제시합니다. 추가 결과는 그림 19에 나와 있습니다. 5.5. 제한 사항 그림 20-22는 우리 방법의 일반적인 실패 사례를 보여줍니다. 첫째, 우리의 방법은 광학 흐름에 의존하기 때문에 부정확한 광학 흐름은 아티팩트로 이어질 수 있습니다.그림 20에서 우리의 방법은 크로스 프레임 대응이 가능한 경우에만 자수를 보존할 수 있습니다.그렇지 않으면 제안된 PA 융합은 효과가 없습니다.둘째, 우리의 방법은 광학 흐름이 변환 전후에 변경되지 않는다고 가정하는데, 이는 그림 21(b)와 같이 결과적인 움직임이 잘못될 수 있는 상당한 외관 변화에는 해당되지 않을 수 있습니다.더 작은 T를 설정하면 이 문제를 해결할 수 있지만 원하는 스타일에 영향을 줄 수 있습니다.한편 (a) 입력 (b) 프레임 #1의 결과 (c) PA 융합 없음 (d) 전체 그림 12. 픽셀 인식 잠재 융합의 효과.프롬프트(위에서 아래로): &#39;비밀스러운 스타일, 잘생긴 남자&#39;, &#39;러빙 빈센트, 하이킹, 잔디&#39;, &#39;디스코 엘리시움, 거리 풍경&#39;.지역 영역은 확대되어 오른쪽 상단에 표시됩니다. 광학 흐름의 불일치는 변환된 키 프레임의 불일치를 의미하며, 이는 시간 인식 블렌딩 후 고스팅 아티팩트(그림 21(d))로 이어질 수 있습니다. 또한 액세서리 및 눈의 움직임과 같은 작은 세부 사항과 미묘한 동작은 변환 중에 잘 보존될 수 없습니다. 마지막으로 키 프레임을 균일하게 샘플링하는데, 이는 최적이 아닐 수 있습니다. 이상적으로 키 프레임에는 다음이 포함되어야 합니다.Original kl-f8 VAE f8-ft-MSE VAE Mean Squared Error Original kl-f8 VAE f8-ft-MSE VAE (a) D(ε()) (b) D(ε(-)) x(c) D(ε*(•)) 그림 13. 두 VAES에서의 충실도 지향 이미지 인코딩. 0.D(ε(•)) kl-f8 VAE 0.D(ε&#39;(•)) kl-f8 VAE 0.0.D(ε* (*)) kl-f8 VAE ☛ D(ε(•)) f8-ft-MSE VAE D(&#39;()) f8-ft-MSE VAE D(E*()) f8-ft-MSE VAE 0.0.祝賀 麗麗 (a) 텍스트 가이드 편집 (d) D(ε*()) ×0.1234 5 6 7 8 9인코딩-디코딩 반복 그림 14. 이미지 인코딩 방식의 양적 평가. (a) 프레임 #(b) D(ε())에 의한 프레임 #6 (c) D(ε*(-))에 의한 프레임 #6 (d) (c)+AdaIN+앵커 그림 15. 오류 누적을 방지하기 위한 다양한 제약 조건. (c) 2D-to-real (b) real-to-2D (d) 3D-to-real 그림 17. 제안된 방법의 응용 프로그램. 입력 갈라드리엘, 왕실 엘프, 은빛 황금색 머리카락 헤르미온느 그레인저, 갈색 머리카락과 눈 원더우먼, 검은 머리카락 입력 갈라드리엘, 왕실 엘프, 은빛 황금색 머리카락 헤르미온느 그레인저, 갈색 머리카락과 눈 원더우먼, 검은 머리카락 입력 มา w/ color correction w/o color correction (a) 입력 프레임 (b) x = ZT (c) T = Tmax (d) T = 0.9Tmax (e) T=0.8Tmax 그림 16. xT 초기화의 효과. 프롬프트: 중국 수묵화의 전통 산. 제안된 프레임워크는 T와 색상 보정을 조정하여 유연한 콘텐츠 및 색상 제어를 가능하게 합니다. 모든 고유한 객체; 그렇지 않으면 전파는 그림 22(b)의 손과 같은 보이지 않는 콘텐츠를 만들 수 없습니다. 한 가지 잠재적인 해결책은 사용자 상호 작용 번역으로, 사용자는 이전 결과에 따라 새로운 키 프레임을 수동으로 할당할 수 있습니다. T&#39;Challa, Black Panther 그림 18. 응용 프로그램: 텍스트 가이드 가상 캐릭터 생성. 결과는 단일 이미지 확산 모델로 생성됩니다. 6.
--- CONCLUSION ---
이 논문은 비디오 변환을 위한 이미지 확산 모델을 적용하기 위한 제로샷 프레임워크를 제시합니다. 저희의 방법은 계층적 크로스 프레임 제약을 활용하여 글로벌 스타일과 저수준 텍스처 모두에서 시간적 일관성을 강화하고 주요 광학 흐름을 활용합니다. 기존 이미지 확산 기술과의 호환성은 저희의 아이디어가 비디오 초고해상도 및 인페인팅과 같은 다른 텍스트 기반 비디오 편집 작업에 적용될 수 있음을 나타냅니다. 또한, 저희가 제안한 충실도 중심 이미지 인코딩은 기존 확산 기반 방법에 도움이 될 수 있습니다. 저희는 저희의 접근 방식이 고품질의 Input Prompt 아름다운 여성 + 만화 스타일 중국 잉크 그림 스타일 지브리 만화 스타일 input 잘생긴 남자 + 반 고흐 그림 그림 19. 응용 프로그램: 비디오 스타일화. 호환되는 디자인 덕분에 저희의 방법은 다양한 스타일에 맞게 사용자 정의된 기성품 사전 학습된 이미지 모델을 사용하여 비디오를 정확하게 스타일화할 수 있습니다. (a) 프레임 #프레임 #프레임 #프레임 #프레임 #(b) 픽셀 인식 융합이 있는 결과 (c) 픽셀 인식 융합이 없는 결과 그림 20. 한계: 큰 동작으로 인한 광학 흐름 실패. 이 방법은 광학 흐름을 추정하기 어려운 비디오를 처리하는 데 적합하지 않습니다. 다시 렌더링된 프레임 입력 프레임 (a) 입력 프레임 (b) T=0.75Tmax인 결과 (c) T=0.6Tax인 결과 (d) 고스팅 그림 21. 한계: 콘텐츠와 프롬프트 간의 균형. 키 프레임키가 아닌 프레임 전파 (a) (b) 새로운 키 프레임 전파 (c) (d) 그림 22. 한계: 좋은 키 프레임 없이는 전파 실패. 시간적으로 일관된 비디오를 제공하고 이 분야에서 추가 연구에 영감을 줍니다. 감사의 말. 이 연구는 RIE2020 산업 정렬 기금 산업 협업 프로젝트(IAF-ICP) 자금 이니셔티브와 산업 파트너의 현금 및 현물 기여로 지원되었습니다. 또한 싱가포르 MOE ACRF Tier 2(MOE-T2EP20221-0011, MOE-T2EP20221-0012) 및 NTU NAP의 지원을 받았습니다. 참고문헌 [1] Omri Avrahami, Ohad Fried, Dani Lischinski. 혼합 잠복 확산. arXiv 사전 인쇄본 arXiv:2206.02779, 2022. 1, 5,[2] Tim Brooks, Aleksander Holynski, Alexei A Efros. InstructPix2Pix: 이미지 편집 지침을 따르는 법 배우기. arXiv 사전 인쇄본 arXiv:2211.09800, 2022.[3] Duygu Ceylan, Chun-Hao Paul Huang, Niloy J Mitra. Pix2video: 이미지 확산을 사용한 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.12688, 2023. 3, 5,[4] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah. 비전의 확산 모델: 조사. IEEE 패턴 분석 및 머신 인텔리전스 저널, 2023.[5] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B Goldman, Pradeep Sen. 이미지 멜딩: 패치 기반 합성을 사용하여 일관되지 않은 이미지 결합. ACM 그래픽스 저널, 31(4):82–1, 2012.[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: 변환기를 통한 텍스트-이미지 생성 마스터링. 신경 정보 처리 시스템의 발전, 34권, 19822-19835페이지, 2021년.[7] Patrick Esser, Robin Rombach, Bjorn Ommer. 고해상도 이미지 합성을 위한 변환기 길들이기. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스, 12873-12883페이지, 2021년.[8] Jakub Fišer, Ondřej Jamriška, David Simons, Eli Shechtman, Jingwan Lu, Paul Asente, Michal Lukáč, Daniel Sykora. 양식화된 얼굴 애니메이션의 예제 기반 합성. ACM Transactions on Graphics(TOG), 36(4):1–11, 2017.[9] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: 인간의 사전 정보를 사용한 장면 기반 텍스트-이미지 생성. Proc. European Conf. Computer Vision, 89–106페이지. Springer, 2022.[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022.[11] Eric Heitz and Fabrice Neyret. 히스토그램 보존 블렌딩 연산자를 사용한 고성능 byexample 노이즈. ACM on Computer Graphics and Interactive Techniques의 회의록, 1(2):1-25, 2018.[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626, 2022. 1,[13] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian Curless, David H. Salesin. 이미지 유추. Proc. Conf. Computer Graphics and Interactive Techniques, 327-340페이지, 2001.[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. 1,[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전, 33권, 6840~6851페이지, 2020.[16] Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. 비디오 확산 모델. 신경 정보 처리 시스템의 발전, 2022.[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: 대규모 언어 모델의 Lowrank 적응. Proc. Int&#39;l Conf. Learning Representations, 2021.[18] Xun Huang 및 Serge Belongie. 적응형 인스턴스 정규화를 통한 실시간 임의 스타일 전송. Proc. Int&#39;l Conf. Computer Vision, 1510-1519페이지, 2017.[19] Ondřej Jamriška, Šárka Sochorová, Ondřej Texler, Michal Lukáč, Jakub Fišer, Jingwan Lu, Eli Shechtman 및 Daniel Sýkora. 예제를 통한 비디오 스타일 지정. ACM Transactions on Graphics, 38(4):1-11, 2019. 3,[20] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi. Text2video-zero: 텍스트-이미지 확산 모델은 제로샷 비디오 생성기입니다. arXiv 사전 인쇄본 arXiv:2303.13439, 2023. 2, 3, 5,[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트의 일반 객체. Proc. European Conf. Computer Vision, 740-755페이지. Springer, 2014.[22] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia. Video-p2p: 교차 주의 제어를 통한 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.04761, 2023.[23] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon. Sdedit: 확률적 미분 방정식을 통한 가이드 이미지 합성 및 편집. Proc. Int&#39;l Conf. Learning Representations, 2021. 1,[24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 가이드 확산 모델을 사용하여 실제 이미지를 편집하기 위한 널 텍스트 반전. arXiv 사전 인쇄본 arXiv:2211.09794, 2022.[25] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen. Dreamix: 비디오 확산 모델은 일반 비디오 편집기입니다. arXiv 사전 인쇄본 arXiv:2302.01329, 2023.[26] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen. Glide: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. Proc. IEEE Int&#39;l Conf. Machine Learning, 16784-16804페이지, 2022.[27] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen. Fatezero: 제로 샷 텍스트 기반 비디오 편집을 위한 주의 융합. arXiv 사전 인쇄본 arXiv:2303.09535, 2023. 3,[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각적 모델 학습. Proc. IEEE Int&#39;l Conf. Machine Learning, 8748-8763페이지. PMLR, 2021.[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. Journal of Machine Learning Research, 21(1):5485-5551, 2020.[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. 1,[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로 샷 텍스트-이미지 생성. Proc. IEEE Int&#39;l Conf. Machine Learning, 8821-8831페이지, 2021.[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. Proc. IEEE Int&#39;l Conf. Computer Vision and Pattern Recognition, 10684-10695쪽, 2022. 1,[33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. 1,[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 영어: Advances in Neural Information Processing Systems, volume 35, pages 36479-36494, 2022. 1,[35] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon. Edit-a-video: 객체 인식 일관성을 갖춘 단일 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.07945, 2023.[36] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. Proc. Int&#39;l Conf. Learning Representations, 2023.[37] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델의 잡음 제거. Proc. Int&#39;l Conf. Learning Representations, 2021.[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30권, 2017년.[39] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen. 기성품 이미지 확산 모델을 사용한 제로샷 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.17599, 2023. 3,[40] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 튜닝.arXiv 사전 인쇄본 arXiv:2212.11565, 2022. 1, 3,[41] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Dacheng Tao.Gmflow: 글로벌 매칭을 통한 광학 흐름 학습.Proc.IEEE Int&#39;l Conf. Computer Vision and Pattern Recognition, 8121-8130페이지, 2022.[42] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He.Attngan: 주의 생성적 적대적 네트워크를 사용한 세분화된 텍스트-이미지 생성.Proc.IEEE Int&#39;l Conf. Computer Vision and Pattern Recognition, 1316-1324쪽, 2018.[43] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang. 텍스트-이미지 생성을 위한 크로스 모달 대조 학습. Proc. IEEE Int&#39;l Conf. Computer Vision and Pattern Recognition, 833-842쪽, 2021.[44] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas. StackGAN: 스택 생성적 적대적 네트워크를 사용한 텍스트-사실적 이미지 합성. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 59075915쪽, 2017.[45] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023. 3, 4,[46] Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang. DMGAN: 텍스트-이미지 합성을 위한 동적 메모리 생성적 적대 네트워크. Proc. IEEE Int&#39;l Conf. Computer Vision and Pattern Recognition, 5802-5810쪽, 2019.
