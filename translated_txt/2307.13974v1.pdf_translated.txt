--- ABSTRACT ---
시각적 객체 추적은 컴퓨터 비전에서 기본적인 비디오 작업입니다. 최근, 지각 알고리즘의 눈에 띄게 증가하는 힘은 단일/다중 객체 및 상자/마스크 기반 추적의 통합을 가능하게 합니다. 그 중에서 Segment Anything Model(SAM)이 많은 주목을 받고 있습니다. 이 보고서에서는 비디오에서 무엇이든 고품질로 추적하기 위한 프레임워크인 HQTrack을 제안합니다. HQTrack은 주로 비디오 다중 객체 세그먼터(VMOS)와 마스크 리파인더(MR)로 구성됩니다. 비디오의 초기 프레임에서 추적할 객체가 주어지면 VMOS는 객체 마스크를 현재 프레임으로 전파합니다. 이 단계에서 마스크 결과는 VMOS가 복잡하고 모서리 장면으로 일반화하는 능력이 제한적인 여러 개의 근접 세트 비디오 객체 세그먼테이션(VOS) 데이터 세트에서 학습되었기 때문에 충분히 정확하지 않습니다. 추적 마스크의 품질을 더욱 개선하기 위해 사전 학습된 MR 모델을 사용하여 추적 결과를 개선합니다. 테스트 시간 데이터 증강 및 모델 앙상블과 같은 트릭을 사용하지 않고도 패러다임의 효과에 대한 설득력 있는 증거로서, HQTrack은 Visual Object Tracking and Segmentation(VOTS2023) 챌린지에서 2위를 차지했습니다. 코드와 모델은 https://github.com/jiawen-zhu/HQTrack에서 제공됩니다. 1.
--- METHOD ---
s는 온라인 업데이트 추적기[3,9]와 Siamese 추적기[2,31]로 그룹화할 수 있습니다. 최근 Transformer[29]가 컴퓨터 비전을 휩쓸고 있으며, 주요 추적 방법은 Transformer 기반 추적기[5, 8, 35, 39]입니다. TransT[5]는 오랫동안 사용된 상관 관계 계산을 대체하기 위해 Transformer 기반 ECA 및 CFA 모듈을 제안합니다. Transformer의 뛰어난 장거리 모델링 기능을 활용하여 TransT는 선형 모델링의 유능한 클래스인 이전 상관 관계 모듈보다 성능이 뛰어납니다. 최근 일부 추적기[8, 39]는 순수한 Transformer 아키텍처를 도입하고 특징 추출 및 템플릿 검색 영역 상호 작용이 단일 백본에서 완료되고 추적 성능은 새 레코드로 푸시됩니다. 이러한 추적기는 주로 단일 객체 추적에 초점을 맞추고 성능 평가를 위해 경계 상자를 출력합니다. 따라서 단순히 SOT 추적기를 사용하는 것은 VOTS2023 챌린지에 적합하지 않습니다. 비디오 객체 분할은 비디오 시퀀스에서 관심 있는 특정 객체를 분할하는 것을 목표로 합니다. VOT와 유사하게 반지도 비디오 객체 분할도 첫 번째 프레임에 주석이 달린 것을 수동으로 제공합니다. 가장 큰 차이점은 VOS 작업이 보다 세분화된 마스크 주석을 제공한다는 것입니다. 초기 VOS 방법은 동작 단서를 통해 비디오 프레임에 객체 마스크를 전파하거나 [6,28] 온라인 학습 전략을 채택합니다 [4,20]. 최근, 공간-시간 메모리(STM) 네트워크 [24, 32]는 메모리 뱅크에서 시공간적 맥락을 추출하여 모양 변화와 폐색을 처리하여 반지도 비디오 객체 분할에 대한 유망한 솔루션을 제공합니다. 다중 객체 분할의 경우 이러한 방법은 객체를 하나씩 분할하고 최종 결과는 포스트 앙상블에 의해 병합된 마스크입니다. AOT [37]는 여러 객체를 동시에 인코딩, 매칭 및 분할할 수 있는 식별 메커니즘을 제안합니다. AOT[37]를 기반으로 DeAOT[38]는 이전 프레임에서 현재 프레임으로 객체 독립적 및 객체별 임베딩의 계층적 전파를 분리하여 VOS 정확도를 더욱 향상시킵니다. 위의 VOS 방법은 다중 객체 및 마스크 출력으로 추적 작업을 처리할 수 있지만 VOTS2023 벤치마크의 과제는 여전히 남아 있습니다. (i) VOTS 비디오에는 많은 수의 장기 시퀀스가 포함되어 있으며, 가장 긴 시퀀스는 10,000개 프레임을 초과하므로 추적기는 객체 모양의 급격한 변화를 구별하고 환경의 변화에 적응할 수 있어야 합니다. 동시에 장기 비디오 시퀀스는 일부 메모리 기반 방법이 메모리 뱅크 공간 과제에 직면하게 합니다. (ii) VOTS 비디오에서 대상은 시야를 벗어난 다음 다시 돌아옵니다. 추적기는 대상의 사라짐과 나타남을 수용하기 위해 추가 설계가 필요합니다. (iii) 빠른 움직임, 잦은 폐색, 방해 요소, 작은 물체와 같은 일련의 과제도 이 작업을 더 어렵게 만듭니다. 이 작업에서 우리는 주로 비디오 다중 객체 분할기(VMOS)와 마스크 정제기(MR)로 구성된 고품질의 모든 것 추적(HQTrack)을 제안합니다. VMOS는 DeAOT[38]의 개선된 변형으로, 복잡한 시나리오에서 작은 객체를 인식하기 위한 1/8 스케일 게이트 전파 모듈(GPM)을 캐스케이드합니다. 또한, Intern-T[33]를 객체 구별 기능을 향상시키기 위한 특징 추출기로 사용합니다. 메모리 사용량을 절약하기 위해 VMOS에서 고정 길이의 장기 메모리를 사용하여 초기 프레임을 제외하고 초기 프레임의 메모리는 버려집니다. 반면에 추적 마스크를 정제하기 위해 대규모 분할 모델을 적용하는 것이 유익할 것입니다. SAM[15]은 복잡한 구조의 객체를 예측할 때 실패하기 쉽고[14], 이러한 어려운 경우는 VOTS 과제에서 흔합니다. 추적 마스크의 품질을 더욱 개선하기 위해 사전 학습된 HQ-SAM[14] 모델을 사용하여 추적 마스크를 정제합니다. 우리는 VMOS에서 예측된 마스크의 바깥쪽 상자를 상자 프롬프트로 계산하고 이를 원본 이미지와 함께 HQ-SAM에 공급하여 정제된 마스크를 얻고, 최종 추적 결과는 VMOS와 MR에서 선택합니다. 마지막으로 HQTrack은 VOTS2023 테스트 세트에서 인상적인 0.615 품질 점수를 얻어 VOTS2023 챌린지에서 준우승을 차지했습니다. 2. 방법 이 섹션에서는 HQTrack을 자세히 설명합니다. 먼저 방법의 파이프라인을 소개합니다. 그런 다음 프레임워크의 각 구성 요소를 소개합니다. 마지막으로 학습 및 추론 세부 정보를 설명합니다. 2.1. 파이프라인 제안된 HQTrack의 파이프라인은 그림 1에 나와 있습니다. 비디오와 첫 번째 프레임 참조(마스크 주석이 달린)가 주어지면 HQTrack은 먼저 VMOS를 통해 각 프레임의 대상 객체를 분할합니다. 현재 프레임의 분할 결과는 모양/식별 정보 및 장기/단기 메모리 모델링을 활용하여 시간 차원을 따라 첫 번째 프레임을 전파한 결과입니다. VMOS는 DeAOT[38]의 변형으로 단일 전파 프로세스 내에서 장면의 여러 객체 모델링을 수행할 수 있습니다.더 나아가 HQSAM[14]을 MR로 사용하여 VMOS의 분할 마스크를 개선합니다.HQ-SAM은 SAM[15]의 변형으로 SAM보다 더 복잡한 구조를 가진 객체를 처리할 수 있습니다.먼저 VMOS에서 예측한 대상 마스크에서 바운딩 박스 추출을 수행하고 이를 박스 프롬프트로 HQ-SAM 모델에 입력합니다.마지막으로 VMOS 및 MR에서 최종 결과를 선택하기 위한 마스크 선택기를 설계합니다.2.2. 비디오 다중 객체 분할기(VMOS) VMOS는 DeAOT[38]의 변형으로 이 하위 섹션에서는 먼저 VMOS의 기준선인 DeAOT를 간략하게 다시 살펴본 다음 VMOS의 설계를 깊이 있게 살펴봅니다.DeAOT. AOT[37]는 통합 임베딩 공간에서 여러 객체를 연관시키는 식별 메커니즘을 통합하여 단일 전파에서 여러 객체를 처리할 수 있도록 제안합니다.DeAOT는 AOT와 유사한 계층적 전파를 갖춘 비디오 객체 분할 모델입니다.심층 전파 계층에서 객체에 독립적인 시각 정보의 손실을 완화하기 위해 DeAOT는 시각 및 식별 임베딩의 전파를 이중 분기 게이트 전파 모듈(GPM)로 분리할 것을 제안합니다.GPM은 계층적 전파를 구성하기 위한 단일 헤드 어텐션을 갖춘 효율적인 모듈입니다.VMOS.HQTrack의 비디오 다중 객체 분할기(VMOS)는 DeAOT의 변형입니다.그림 1의 왼쪽에 표시된 것처럼 특히 작은 객체를 인식하는 분할 성능을 개선하기 위해 8배 크기의 GPM을 계단식으로 연결하고 전파 프로세스를 여러 배로 확장합니다.원래 DeAOT는 16배 크기의 시각 및 식별 특징에 대해서만 전파 작업을 수행합니다. 이 규모에서는 많은 세부적인 객체 단서가 손실되고, 특히 아주 작은 객체의 경우 16배 규모의 특징은 정확한 비디오 객체 분할에 충분하지 않습니다. VMOS에서는 메모리 사용과 모델 효율성을 고려하여 업샘플링과 선형 투영만 사용하여 전파 특징을 4배 규모로 업스케일합니다. 다중 규모 전파 특징은 마스크 예측을 위해 다중 규모 인코더 특징과 함께 디코더에 입력됩니다. 디코더는 간단한 FPN입니다[21]. 또한 새로운 대규모 CNN 기반 기반 모델인 Internimage[33]는 변형 가능한 합성곱을 핵심 연산자로 사용하여 FrameFrame t-Frame t Refined t Encoder Encoder Encoder Select T Propagation hmem → Propagation Reference GPM HQ-SAM GPM Up &amp; Proj J ID Emb Decoder ID Emb hmem VMOS GPM Up &amp; Proj h_sh_s4 h_sDecoder Box Prompt MR Predt -Pred t 그림 1. HQTrack 개요. 주로 비디오 다중 객체 분할기(VMOS)와 마스크 정제기(MR)로 구성됩니다. 객체 감지 및 분할과 같은 다양한 대표적 작업. VMOS에서 Intern-T는 객체 구별 기능을 향상시키기 위한 인코더로 사용됩니다. 2.3. Mask Refiner(MR) MR은 사전 훈련된 HQ-SAM[14]입니다. 이 섹션에서는 먼저 SAM[15]의 변형인 HQ-SAM 방법을 다시 살펴본 다음 HQ-SAM의 사용법을 제공합니다. SAM과 HQ-SAM. Segment anything model(SAM)은 최근 이미지 분할 분야에서 큰 주목을 받았으며, 연구자들은 SAM을 활용하여 많은 놀라운 결과를 얻은 일련의 작업(분할을 포함하되 이에 국한되지 않음)을 보조했습니다. SAM은 11억 개의 마스크가 포함된 고품질 주석이 달린 데이터 세트로 훈련하여 분할 모델을 확장합니다. 대규모 훈련이 제공하는 강력한 제로샷 기능 외에도 SAM에는 다양한 프롬프트 형식으로 달성되는 유연한 인간 상호 작용 메커니즘이 포함됩니다. 그러나 처리된 이미지에 복잡한 구조의 객체가 포함된 경우 SAM의 예측 마스크는 부족한 경향이 있습니다. 이러한 문제를 해결하고 SAM의 원래 프롬프트 가능 디자인, 효율성 및 제로샷 일반화 가능성을 유지하기 위해 Ke et al. HQ-SAM을 제안합니다[14]. HQ-SAM은 사전 학습된 SAM 모델에 몇 가지 추가 매개변수를 도입합니다. 고품질 마스크는 SAM의 마스크 디코더에 학습 출력 토큰을 주입하여 얻습니다. MR. HQTrack은 위의 HQ-SAM을 마스크 정제기로 사용합니다. 그림 1의 오른쪽에 표시된 것처럼 VMOS의 예측 마스크를 MR의 입력으로 사용합니다. VMOS 모델은 규모가 제한된 근접 집합 데이터 세트에서 학습되었으므로 VMOS의 1단계 마스크는 특히 일부 복잡한 시나리오를 처리하는 데 품질이 부족할 수 있습니다. 따라서 대규모 학습된 분할 알고리즘을 사용하여 1차 분할 결과를 정제하면 성능이 상당히 향상됩니다. 구체적으로, VMOS의 예측 마스크의 바깥쪽 둘러싼 상자를 상자 프롬프트로 계산하여 원본 이미지와 함께 HQ-SAM에 공급하여 정제된 마스크를 얻습니다. 여기의 HQ-SAM은 ViT-H 백본이 있는 버전입니다. 마지막으로, VMOS와 HQ-SAM의 마스크 결과에서 HQTrack의 출력 마스크가 선택됩니다. 구체적으로, 동일한 대상 객체에 대해 HQ-SAM에서 정제된 마스크가 때때로 VMOS의 예측 마스크와 완전히 다르고(매우 낮은 IoU 점수) 대신 분할 성능에 해를 끼칩니다. 이는 HQ-SAM과 참조 주석 간의 객체에 대한 이해와 정의가 다르기 때문일 수 있습니다. 따라서 최종 출력으로 사용할 마스크를 결정하기 위해 IoU 임계값(VMOS와 HQ-SAM의 마스크 사이)을 설정합니다. 우리의 경우 IoU 점수가 7보다 높으면 정제된 마스크를 선택합니다. 이 프로세스는 HQ-SAM이 다른 대상 객체를 다시 예측하는 대신 현재 객체 마스크를 정제하는 데 집중하도록 제한합니다. 3. 구현 세부 정보 HQTrack의 VMOS에서 InternImage-T[33]는 정확도와 효율성 간의 균형을 위해 이미지 인코더의 백본으로 사용됩니다. 16× 및 8× 스케일의 GMP 층 번호는 3 및 1로 설정됩니다.4× 스케일0.566 0.645 0.782 0.097 0.121 0.방법 AUC A MS AOT(분리) MS AOT(조인트) R NRE↓ DRE↓ ADQ 0.552 0.625 0.831 0.063 0.106 0.G = AUC A0.R NRE↓ 0.668 0.807 0.DRE↓ ADQ 0.083 0.0.607 0.65 0.806 0.0.074 0.0.626 0.0.813 0.0.060 0.0.650 0.681 0.886 0.0.0.0.669 0.0.885 0.0.0.0.653 0.0.889 0.0.0.표 1. VOTS2023 검증 세트에서 개별 추적 대 공동 추적 패러다임의 절제 연구. ↓로 표시된 메트릭은 작을수록 더 좋고 그 반대의 경우도 마찬가지임을 나타냅니다. NRE: 보고되지 않은 오류. DRE: 드리프트 비율 오류. ADQ: 부재 감지 품질. 평가 메트릭에 대한 자세한 내용은 [17]을 참조하십시오. # 방법 1 기준 AUC AR NRE DRE↓ ADQ 0.576 0.675 0.77 0.122 0.108 0.2 w/InternImage-T 0.611 0.656 0.809 0.137 0.054 0.3 VMOS 0.650 0.681 0.886 0.059 0.055 0.표 2. VOTSvalidation 세트의 VMOS 구성 요소에 대한 절제 연구. 기준 방법으로 DeAOT[38]를 학습합니다. 전파 특징은 업샘플링되고 8배 크기에서 투영 특징이 있습니다. 장기 및 단기 메모리는 세그먼터에서 장기 비디오 시퀀스의 개체 모양 변경을 처리하는 데 사용됩니다. 메모리 사용량을 절약하기 위해 초기 프레임을 제외하고 8의 고정 길이의 장기 메모리를 사용하고 초기 메모리는 삭제됩니다. 모델 학습. 학습 과정은 이전 방법[37, 38]에 따라 두 단계로 구성됩니다.첫 번째 단계에서는 정적 이미지 데이터 세트[7, 11, 12, 22, 27]에서 생성된 합성 비디오 시퀀스에서 VMOS를 사전 학습합니다.두 번째 단계에서 VMOS는 다중 객체 분할 데이터 세트를 사용하여 여러 객체 간의 관계를 더 잘 이해하기 위한 학습을 수행합니다.DAVIS[25], YoutubeVOS[34], VIPSeg[23], BURST[1], MOTS[30], OVIS[26]의 학습 분할이 VMOS를 학습하는 데 선택되었으며, 여기서 OVIS는 가려진 객체를 처리하는 추적기의 견고성을 개선하는 데 사용됩니다.VMOS를 학습하기 위해 글로벌 배치 크기가 있는 NVIDIA Tesla A100 GPU를 사용합니다.사전 학습 단계에서는 100,000단계에 대해 초기 학습률 4 × 10-4를 사용합니다. 두 번째 단계에서는 150,단계에 대해 2 × 10-4의 초기 학습률을 사용합니다. 학습률은 다항식 방식으로 점차 1 × 10-5로 감소합니다[36]. 추론. 추론 프로세스는 파이프라인에 설명된 대로입니다. 뒤집기, 다중 스케일 테스트, 모델 앙상블과 같은 테스트 시간 증가(TTA)를 사용하지 않습니다. 4.
--- EXPERIMENT ---
4.1. 절제 연구 개별 추적 대 공동 추적.우리는 다른 추적 패러다임에 대한 절제 연구를 수행합니다.별도 추적은 각 대상 객체에 대해 별도의 추적기를 초기화하고 여러 객체 추적을 위해 여러 번의 추론을 실행하는 것을 의미합니다.공동 추적은 모든 tar0.656 0.688 0.865 0.052 0.082 0을 공동 추적하는 것을 의미합니다.표 3. VOTS2023 검증 세트에서 장기 메모리 갭(G)에 대한 절제 연구.단일 추적기로 객체를 가져옵니다.MS_AOT[16](Mixformer[8] 제거)를 기준으로 선택합니다.VOTS2023 검증 세트에 대한 결과는 표 1에 나와 있습니다.공동 추적이 개별 추적보다 더 나은 성능을 보이는 것을 알 수 있습니다.공동 추적 시 추적기가 대상 객체 간의 관계를 더 잘 이해하여 방해 요인 간섭에 대한 더 나은 견고성을 얻을 수 있습니다.VMOS에 대한 구성 요소별 분석. 표 2는 VMOS에 대한 구성 요소별 연구 결과를 보여줍니다. #1은 훈련된 기준선 방법 DeAOT[38]입니다. #2에서 원래 ResNet50[13] 백본을 InternImage-T[33]로 바꾸면 AUC 점수가 0.611로 증가합니다. 그런 다음 #3에서 보고한 대로 섹션 2.2에 설명된 대로 다중 스케일 전파 메커니즘을 추가하면 AUC 점수 측면에서 성능이 0.650으로 향상되어 3.9%의 현저한 개선이 이루어져 효과를 보여줍니다. 장기 메모리 갭. VOTS 비디오 시퀀스는 길어지는 경향이 있으므로(가장 긴 것이 10,프레임을 초과함) VOS 벤치마크의 테스트 시간에 대한 원래 장기 메모리 갭 매개변수는 적합하지 않습니다. 따라서 표 3에 표시된 대로 장기 메모리 갭(G) 매개변수에 대한 세척 연구를 수행합니다. 메모리 갭이 50일 때 가장 좋은 성능을 보입니다. 마스크 리파이너(MR)에 대한 분석. 2.3절에서 논의했듯이 모든 분할 마스크를 직접 정제하는 것은 최적이 아닙니다.그림 3에서 VMOS와 VMOS + SAM을 비교합니다.VMOS + SAM의 경우 SAM-h[15]를 사용하여 VMOS의 모든 객체 마스크를 정제합니다.SAM으로 정제하면 상당한 개선이 이루어질 수 있음을 알 수 있습니다.그러나 품질이 낮은 이러한 마스크(실제 IoU 점수가 낮음)의 경우 SAM이 대신 성능을 해칩니다.따라서 VMOS와 SAM에서 마스크 결과를 선택하는 것을 제안합니다.VMOS와 SAM의 마스크 사이에서 IoU 점수를 계산합니다.IoU 점수가 7보다 높으면 정제된 마스크를 최종 출력으로 선택합니다. 우리는 VOTS2023 검증 세트에서 MR의 임계값 7의 영향을 평가하고 그 결과를 표 4에 나타냈다. 7 = 0.1은 가장 유망한 결과를 제공하며 HQTrack에서 이 설정을 선택했다.FrameTOYOTA TOYOTA TOYOTA TDK TDK Frame T TOYOTA TOYOTA TOYOTA WMTHIPS BE WM100m Men 37.6 kph 9.CR WR 9.99 CR 9.7.SEIKO WR 9.그림 2. VOTS2023 테스트 세트의 비디오에 대한 HQTrack의 정성적 결과. T = AUC AR NRE↓ DRE↓ ADQ 0 0.702 0.756 0.866 0.072 0.062 0.0.1 0.708 0.753 0.878 0.072 0.050 0.0.2 0.707 0.753 0.878 0.072 0.050 0.0.3 0.704 0.750 0.878 0.072 0.050 0.0.4 0.701 0.745 0.878 0.072 0.050 0.0.5 0.695 0.739 0.878 0.072 0.050 0.표 4. 다른 임계값 7에 대한 추적 성능 VOTS2023 검증 세트. 마스크 정제기(MR)는 SAM_H 모델입니다. 4.2. 과제 결과 VOTS2023 테스트 세트의 결과는 표 5에 나와 있습니다. VMOS 인코더를 ResNet50[13]에서 InternImage-T[33]로 교체한 후 AUC 점수가 3.2% 증가했습니다. SAM_H를 사용하여 VMOS의 마스크를 정제할 때 Method VMOS (Res50) VMOS AUC AR NRE DRE↓ ADQ 0.564 0.693 0.759 0.155 0.086 0.0.596 0.724 0.765 0.159 0.075 0.VMOS + SAM_H 0.610 0.751 0.757 0.159 0.084 0.HQTrack 0.615 0.752 0.766 0.155 0.079 0.표 5. VOTS2023 테스트 세트의 성능. AUC 측면에서 성능이 1.4% 증가했습니다. HQ-SAM_H를 마스크 정제 모듈로 사용한 후 AUC 점수는 0.615로 상승하여 VMOS보다 0.9% 더 뛰어납니다. 그림 4는 VMOS와 HQtrack 간의 품질 플롯 비교를 제공합니다. 그림 3과 비교해보면 MR의 처리된 결과를 선택적으로 취하면 낮은 IoU 객체로 인한 성능 저하를 효과적으로 방지할 수 있습니다. 마지막으로, HQTrack은 2개의 https://eu.aihub.ml/competitions/201 # 결과에서 2위²를 차지했고, VOTS2023 벤치마크는 챌린지 이후 제출을 허용하기 위해 개방되었습니다.성공 품질 플롯 - 플롯 1.0.VMOS+SAM, T=0.0.0.2VMOS 0.0.0.0.0.0.1.임계값 성공 0.품질 플롯 - 플롯 1.0.80.HQTrack 0.4VMOS 0.0.0.0,0.0.1.임계값 그림 3. VOST2023 검증 세트에서 VMOS 대 VMOS + SAM. SAM은 VMOS의 모든 마스크를 정제하는 데 사용됩니다. 시각적 객체 추적 및 분할 챌린지. 4.3. 시각화 그림 2는 어려운 비디오 시퀀스에 대한 몇 가지 대표적인 시각적 결과를 제공합니다. 표시된 대로 HQTrack은 강력한 추적 기능을 보여줍니다. 장기 객체 추적 시나리오를 안정적으로 처리하고, 동시에 여러 객체를 추적하고, 방해 요소가 많더라도 대상 객체를 정확하게 캡처할 수 있습니다. HQ-SAM의 도움으로 객체 모양 변경, 빠른 동작, 크기 변경과 같은 과제에 직면했을 때 정확한 마스크를 분할할 수도 있습니다. 5.
--- CONCLUSION ---
이 보고서에서는 Tracking Anything in High Quality(HQTrack)를 제안합니다. HQTrack은 주로 비디오 다중 객체 세그먼터(VMOS)와 마스크 리파인더(MR)로 구성됩니다. VMOS는 비디오 프레임에서 여러 대상을 전파하는 역할을 하고 MR은 세그먼테이션 마스크를 리파인하는 역할을 하는 대규모 사전 학습된 세그먼테이션 모델입니다. HQTrack은 강력한 객체 추적 및 세그먼테이션 기능을 보여줍니다. 마지막으로 HQTrack은 Visual Object Tracking and Segmentation(VOTS2023) 챌린지에서 2위를 차지했습니다. 참고문헌 [1] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, Deva Ramanan. Burst: 비디오에서 객체 인식, 세그먼테이션 및 추적을 통합하기 위한 벤치마크. IEEE/CVF Winter Conference on Applications of Computer Vision의 진행 사항, 1674-1683쪽, 2023.[2] Luca Bertinetto, Jack Valmadre, João F Henriques, Andrea Vedaldi, Philip HS Torr. 객체 추적을 위한 완전 합성곱 시암 네트워크. ECCVW, 2016년.[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte. 추적을 위한 판별 모델 예측 학습. ICCV, 2019년.그림 4. VOST2023 테스트 세트에서 VMOS 대 HQTrack. [4] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, Luc Van Gool. 원샷 비디오 객체 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 221-230페이지, 2017년.[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu. Transformer tracking. CVPR, 2021.[6] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, MingHsuan Yang. Segflow: 비디오 객체 분할 및 광학 흐름을 위한 공동 학습. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 686-695페이지, 2017.[7] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, Shi-Min Hu. 전역 대비 기반 두드러진 영역 감지. IEEE 패턴 분석 및 머신 인텔리전스 거래, 37(3):569-582, 2014.[8] Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu. Mixformer: 반복적 혼합 주의를 통한 종단 간 추적. CVPR, 페이지 13608-13618, 2022. 1,[9] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan 및 Michael Felsberg. ECO: 추적을 위한 효율적인 컨볼루션 연산자. CVPR, 2017.[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit 및 Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기입니다. ICLR, 2021.[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn 및 Andrew Zisserman. 파스칼 시각적 객체 클래스(voc) 챌린지. International journal of computer vision, 88:303-338, 2010.[12] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, Jitendra Malik. 역 검출기의 의미적 윤곽. 2011년 국제 컴퓨터 비전 컨퍼런스, 991-998페이지. IEEE, 2011.[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 이미지 인식을 위한 심층적 잔여 학습. CVPR, 2016. 1, 4,[14] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu. 고품질의 모든 것을 세분화합니다. arXiv 사전 인쇄 arXiv:2306.01567, 2023. 2,[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분류하세요. arXiv 사전 인쇄 arXiv:2304.02643, 2023. 2, 3,[16] Matej Kristan, Aleš Leonardis, Jiří Matas, Michael Felsberg, Roman Pflugfelder, Joni-Kristian Kämäräinen, 장형진, Martin Danelljan, Luka čehovin Zajc, Alan Lukežič, Ondrej Drbohlav, et al. 제10회 시각 객체 추적 vot2022 챌린지 결과, 2022. 1,[17] Matej Kristan, Jiri Matas, Martin Danelljan, Luka Čehovin Zajc, Alan Lukezic. vots2023 챌린지 성과 측정.[18] Matej Kristan, Jiří Matas, Aleš Leonardis, Michael Felsberg, Roman Pflugfelder, Joni-Kristian Kämäräinen, Hyung Jin Chang, Martin Danelljan, Luka Cehovin, Alan Lukežič, et al. 제9회 시각 객체 추적 vot2021 챌린지 결과. ICCVW, 2711-2738페이지, 2021.[19] Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton. 딥 합성곱 신경망을 사용한 Imagenet 분류. NIPS, 2012.[20] Xiaoxiao Li 및 Chen Change Loy. 공동 재식별 및 주의 인식 마스크 전파를 사용한 비디오 객체 분할. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 90-105페이지, 2018년.[21] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan 및 Serge Belongie. 객체 감지를 위한 피처 피라미드 네트워크. CVPR, 2117-2125페이지, 2017년.[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár 및 C Lawrence Zitnick. Microsoft coco: 컨텍스트 내 공통 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6일-12일, 회의록, 5부 13, 740-755쪽. Springer, 2014.[23] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, Yi Yang. 야외에서의 대규모 비디오 파노라마 분할: 벤치마크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 21033-21043쪽, 2022.[24] Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim. 시공간 메모리 네트워크를 사용한 비디오 객체 분할. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 9226-9235쪽, 2019.[25] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung 및 Luc Van Gool. 비디오 객체 분할에 대한 Thedavis 챌린지. arXiv 사전 인쇄 arXiv:1704.00675, 2017.[26] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr 및 Song Bai. 가려진 비디오 인스턴스 분할: 벤치마크. 국제 컴퓨터 비전 저널, 130(8):2022-2039, 2022.[27] Jianping Shi, Qiong Yan, Li Xu 및 Jiaya Jia. 확장 CSS에서 계층적 이미지 돌출 감지. TPAMI, 38(4):717-729, 2015.[28] 차이수안, 양밍수안, 마이클 J 블랙. 객체 흐름을 통한 비디오 분할. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 컨퍼런스 진행, 페이지 3899-3908, 2016.[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. NIPS에서, 2017.[30] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger 및 Bastian Leibe. Mots: 다중 객체 추적 및 분할. 컴퓨터 비전 및 패턴 인식에 관한 ieee/cvf 컨퍼런스의 회의록, 7942-7951페이지, 2019.[31] Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, Bastian Leibe. Siam R-CNN: 재검출을 통한 시각 추적. CVPR, 2020.[32] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, Xiang Ruan. 이미지 수준 감독을 통한 뛰어난 객체 감지 학습. CVPR, 136-145페이지, 2017.[33] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: 변형 가능한 합성곱을 사용하여 대규모 비전 기반 모델 탐색. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 14408-14419, 2023. 2, 3,4,[34] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang 및 Thomas Huang. Youtube-vos: 대규모 비디오 객체 분할 벤치마크입니다. arXiv 사전 인쇄 arXiv:1809.03327, 2018.[35] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang 및 Huchuan Lu. 시각적 추적을 위한 시공간 변환기를 학습합니다. ICCV에서, 2021.[36] 양종신, 웨이윤차오, 이양. 전경-배경 통합을 통한 협업 비디오 객체 분할. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 5부, 332-348페이지. Springer, 2020.[37] Zongxin Yang, Yunchao Wei, Yi Yang. 비디오 객체 분할을 위한 변환기와 객체 연관. 신경 정보 처리 시스템의 발전, 34:24912502, 2021. 2,[38] Zongxin Yang과 Yi Yang. 비디오 객체 분할을 위한 계층적 전파의 기능 분리. 신경 정보 처리 시스템의 발전, 35:36324-36336, 2022. 2,[39] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen. 추적을 위한 공동 기능 학습 및 관계 모델링: 단일 스트림 프레임워크. ECCV, 341-357쪽. Springer, 2022.
