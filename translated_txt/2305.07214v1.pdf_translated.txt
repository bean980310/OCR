--- ABSTRACT ---
이 논문에서 우리는 자기중심적 행동 인식의 새로운 문제를 연구하는데, 이를 &quot;다중 모달 일반화&quot;(MMG)라고 부릅니다. MMG는 특정 모달리티의 데이터가 제한적이거나 완전히 누락된 경우 시스템이 어떻게 일반화할 수 있는지 연구하는 것을 목표로 합니다. 우리는 표준 감독 행동 인식과 새로운 행동 범주를 학습하기 위한 더 어려운 few-shot 설정의 맥락에서 MMG를 철저히 조사합니다. MMG는 실제 응용 프로그램에서 보안 및 효율성 고려 사항을 지원하도록 설계된 두 가지 새로운 시나리오로 구성됩니다. (1) 학습 시간 동안 존재했던 일부 모달리티가 추론 시간 동안 누락되는 누락된 모달리티 일반화, (2) 추론 시간과 학습 시간 동안 존재하는 모달리티가 분리된 교차 모달 제로샷 일반화. 이 조사를 가능하게 하기 위해 비디오, 오디오 및 관성 모션 센서(IMU) 모달리티가 있는 데이터 포인트를 포함하는 새로운 데이터 세트 MMG-Ego4D를 구성합니다. 우리의 데이터 세트는 Ego4D [27] 데이터 세트에서 파생되었지만, MMG 문제에 대한 연구를 용이하게 하기 위해 인간 전문가가 처리하고 철저히 재주석을 달았습니다. 우리는 MMG-Ego4D에서 다양한 모델을 평가하고 일반화 능력이 향상된 새로운 방법을 제안합니다. 특히, 우리는 더 나은 few-shot 성능을 위한 모달 드롭아웃 훈련, 대조 기반 정렬 훈련 및 새로운 교차 모달 프로토타입 손실을 갖춘 새로운 융합 모듈을 소개합니다. 우리는 이 연구가 벤치마크 역할을 하고 멀티모달 일반화 문제에 대한 미래 연구를 안내하기를 바랍니다. 벤치마크와 코드는 https://github.com/facebookresearch/MMG_Ego4D 1에서 사용할 수 있습니다.
--- INTRODUCTION ---
동작 인식 시스템은 일반적으로 3인칭 또는 관객 관점에서 수집한 데이터를 통해 훈련됩니다[37, 56]. 그러나 로봇공학 및 증강 현실과 같은 분야에서는 에이전트의 눈을 통해, 즉 1인칭 또는 자기중심적 관점에서 데이터를 수집합니다. 머리로*동등한 기여*Meta Reality Labs에서 인턴십 기간 동안 수행한 작업.(a) 일반 평가 비디오 오디오 훈련 oo oo oo oŎ어텐션 퓨전 비디오 오디오 테스트IMU IMU ဝားက멀티모달 네트워크 비디오 (b) 누락된 HD 모달 평가 오디오 어텐션 퓨전 IMU 멀티모달 네트워크 오디오 (c) 크로스 모달 제로샷 평가 IMU 어텐션 퓨전 멀티모달 네트워크 |오디오 IMU 어텐션 퓨전 멀티모달 네트워크 어텐션 퓨전 멀티모달 네트워크 어텐션 퓨전 멀티모달 네트워크 그림 1. MMG-Ego4D 챌린지 개요. 일반적인 평가 설정에서 (a) 네트워크는 모든 모달리티의 데이터가 있는 훈련/지원 세트를 사용하여 감독 설정 또는 few-shot 설정에 대해 훈련되고 모든 모달리티의 데이터 포인트에서 평가됩니다. 그러나 훈련 및 테스트 모달리티 간에 종종 불일치가 있을 수 있습니다. 제안된 과제에는 이러한 설정을 모방하기 위한 두 가지 과제가 포함되어 있습니다. (b) 누락된 모달리티 평가에서 모델은 추론을 위해 훈련 모달리티의 하위 집합만 사용할 수 있습니다. (c) 크로스모달 제로샷 평가에서 모델은 훈련 중에 보이지 않는 모달리티에 있습니다. Ray-Ban Stories와 같은 장착된 장치가 인기를 얻으면서 자기중심적 비디오의 동작 인식은 상황에 맞는 추천 또는 알림과 같은 다운스트림 애플리케이션을 활성화하는 데 중요합니다. 그러나 자기중심적 동작 인식은 근본적으로 다르고 더 어렵습니다[6, 7, 43, 55]. 3인칭 비디오 클립은 종종 큐레이션되는 반면, 자기중심적 비디오 클립은 큐레이션되지 않았으며 머리 움직임으로 인한 큰 모션 블러와 같은 저수준 손상이 있습니다. 또한, 자기중심적 인식에는 카메라 착용자의 물리적 주변 환경에 대한 신중한 이해가 필요하며, 착용자의 관점에서 사물과 상호작용을 해석해야 합니다.자기중심적 활동을 오직 하나의 비디오 오디오 IMU 비디오 오디오 ↑ ↑ ↑ 다른 도구로 풀을 깎기 쓰레기통에 쓰레기 넣기 ☑ 테이블 닦기 X 바닥에 떨어진 마른 잎 모으기 모달리티 초당 데이터 메모리(KB) 일반 모델 FLOPS(G) 비디오 오디오 IMU 593.92 62.76 9.70.50 42.08 1.표 1. 다양한 모달리티에 대한 계산 및 메모리 비용. 각 모달리티에 대해 초당 사용되는 메모리는 Ego4D[27]에서 무작위로 추출한 1000개 데이터 포인트에서 사용한 메모리를 평균하여 계산합니다. 제공된 계산 번호는 비디오의 MVIT[15], 오디오의 AST[26] 및 IMU 데이터의 ViT[11] 기반 변환기 모델의 순방향 패스 비용에 해당합니다. 단일 모드 네트워크 주의 융합 IMU 다중 모드 네트워크 그림 2. 다중 모드 데이터는 자기중심적 지각에 필수적입니다. 입력 데이터는 비디오, 오디오, IMU의 세 가지 모달리티로 구성됩니다. (위) 비디오 동작 인식은 도구가 있는 클립과 배경에 많은 풀을 다른 도구로 풀을 다듬는 클래스로 식별합니다. 오디오 동작 인식 시스템은 주기적인 문지르는 소리를 쓰레기통에 쓰레기를 넣는 것으로 분류합니다. IMU 모델은 머리 움직임 동작을 테이블 닦기 클래스로 분류합니다. (아래) 다중 모드 동작 인식 시스템은 비디오 피드와 오디오 피드를 올바르게 결합하고 활동을 땅에 마른 잎을 모으는 것으로 식별합니다. 모달리티는 종종 모호할 수 있습니다. 이는 카메라 피드가 포착하는 내용 대신 장치 착용자가 수행하는 내용을 인식하고자 하기 때문입니다. 이를 위해 다중 모드 정보는 사용자의 의도나 동작을 이해하고 모호성을 해소하는 데 중요할 수 있습니다. 그림 2의 예를 통해 이를 보여줍니다. 이 예에서 비디오 피드는 초원 배경에 있는 도구를 보여줍니다. 비디오에만 기반한 활동 인식 모델은 다른 도구를 사용하여 잔디를 다듬는 수업으로 인식합니다. 마찬가지로 오디오에만 기반한 모델은 클립에서 문지르는 소리를 수업이 쓰레기통에 쓰레기를 넣는 것으로 식별하고 IMU 모델은 머리 동작을 테이블 닦는 것으로 오인합니다. 그러나 멀티모달 시스템은 비디오, 오디오 및 IMU 신호를 결합하여 수업을 땅에 마른 잎을 모으는 것으로 올바르게 식별합니다. 멀티모달 정보를 사용하는 것은 성능 상태를 달성하는 데 필수적이지만 보안 또는 효율성 고려 사항으로 인해 실제 세계에서 모든 모달리티를 사용할 수 없는 고유한 과제도 제시합니다. 예를 들어, 사용자가 민감한 환경에 있고 보안 문제로 인해 카메라를 끄기로 결정할 수 있습니다. 마찬가지로 사용자는 음성이 들리지 않도록 마이크를 끌 수 있습니다. 이러한 상황에서 멀티모달 시스템은 누락된 모달리티(그림 1(b))로 일반화할 수 있어야 합니다. 즉, 추론 시 불완전한 모달리티 세트로 작업하고 견고한 예측을 수행해야 합니다. 이러한 과제는 추론 시간에만 국한되지 않고 훈련 중 제한으로 나타날 수 있습니다. 예를 들어, 사용자가 시스템을 훈련해야 하는 경우(종종 몇 번의 샷 설정에서) 비디오와 같은 계산적으로 비싼 모달리티는 클라우드에서 가장 잘 훈련됩니다. 그러나 사용자는 데이터가 장치에 남아 있기를 선호할 수 있습니다. 그러나 비디오는 IMU와 같은 저렴한 모달리티에 비해 60배 더 많은 스토리지와 43배 더 많은 컴퓨팅을 소모하므로(표 1 참조) 컴퓨팅과 스토리지가 제한된 장치에서 훈련하는 데 어려움이 크게 증가합니다. 이 상황에서는 비디오와 같은 보다 정보적인 모달리티에서 추론을 수행하는 유연성을 유지하면서 오디오와 같은 계산적으로 덜 까다로운 모달리티로 훈련을 가능하게 하려고 할 수 있습니다. 멀티모달 시스템은 모달리티 간에 강력하게 일반화해야 합니다. 이 작업에서 우리는 자기중심적 활동 인식 모델의 일반화 능력을 측정하도록 설계된 과제인 MMG-Ego4D를 제안합니다. 우리의 과제는 두 가지 새로운 과제로 구성되어 있습니다. (1) 불완전한 모달리티 집합에서 평가할 때 모델의 일반화 능력을 측정하는 것을 목표로 하는 누락된 모달리티 일반화(그림(b) 참조), (2) 테스트 시간 동안 보이지 않는 모달리티로 일반화하는 모델의 일반화 능력을 측정하는 것을 목표로 하는 교차 모달 제로샷 일반화(그림 1(c) 참조). 이 벤치마크를 사용하여 널리 사용되는 여러 아키텍처를 평가하고 MMG-Ego4D 과제에서 일반화 능력을 향상시키고 표준 전체 모달리티 설정에서 성능을 개선하는 새로운 접근 방식을 소개합니다. 우리의 주요 기여는 다음과 같습니다. • MMG 문제. 우리는 다중 모달 동작 인식 모델의 일반화 능력을 평가하기 위해 누락된 모달리티 일반화와 교차 모달 제로샷 일반화의 두 가지 과제가 있는 새롭고 실용적인 문제인 MMG를 제시합니다. 이러한 과제는 실제 보안 및 효율성 고려 사항을 지원하도록 설계되었으며, 지도 학습 및 더 어려운 fewshot 설정에서 모두 정의합니다. • MMG-Ego4D 데이터 세트. 자기중심적 행동 인식 과제에서 MMG 문제 연구를 용이하게 하기 위해, 우리는 Ego4D[27] 데이터 세트에서 데이터 포인트를 사전 처리하고 인간 전문가가 과제에 맞게 철저히 재주석하여 파생한 새로운 데이터 세트인 MMG-Ego4d를 소개합니다. 우리가 아는 한, 이것은 이러한 새로운 평가 과제와 그 종류의 벤치마크 과제를 소개한 최초의 작업입니다. 강력한 기준선. 우리는 일반화 능력 벤치마크에서 강력한 성능을 달성하고 일반적인 전체 모달리티 설정에서 성능도 개선하는 새로운 방법을 제시합니다. 우리의 방법은 다양한 모달리티의 유연한 입력을 허용하는 Transformer 기반 퓨전 모듈을 사용합니다. 우리는 다양한 모달리티의 특징을 통합된 공간으로 투사하기 위해 교차 모달 대조 정렬 손실을 사용합니다. 마지막으로, 교차 모달 프로토타입 손실이라고 하는 새로운 손실 함수가 도입되어 다중 모달 few-shot 설정에서 최첨단 결과를 달성합니다. 제안된 각 구성 요소의 기여도를 파악하기 위해 광범위한 절제 연구가 수행됩니다. 2.
--- RELATED WORK ---
다중 모드 자기중심적 행동 인식. 행동 인식 시스템은 일반적으로 비디오에서 훈련됩니다[15–19, 25, 65, 70]. 그러나 자기중심적 활동 인식(즉, 1인칭 관점 또는 캡처링 장치를 착용한 사용자가 수행하는 활동 인식)의 경우 올바른 활동을 식별하기 위해 보완적인 다중 모드 정보가 필수적입니다(그림 2 참조). 이전
--- METHOD ---
영어: 일반화 능력이 향상된 s. 특히, 모달 드롭아웃 훈련, 대조 기반 정렬 훈련 및 더 나은 few-shot 성능을 위한 새로운 교차 모달 프로토타입 손실을 갖춘 새로운 퓨전 모듈을 소개합니다. 이 연구가 벤치마크 역할을 하고 멀티모달 일반화 문제에 대한 미래 연구를 안내하기를 바랍니다. 벤치마크와 코드는 https://github.com/facebookresearch/MMG_Ego4D에서 사용할 수 있습니다. 1. 서론 동작 인식 시스템은 일반적으로 3인칭 또는 관객 관점에서 수집한 데이터로 훈련됩니다[37, 56]. 그러나 로봇공학 및 증강 현실과 같은 분야에서는 에이전트의 눈을 통해, 즉 1인칭 또는 자기중심적 관점에서 데이터를 수집합니다. 헤드*동등한 기여*Meta Reality Labs에서 인턴십 기간 동안 수행한 작업. (a) 일반 평가 비디오 오디오 훈련 oo oo oo oŎAttention Fusion 비디오 오디오 TestingIMU IMU ဝားကMultimodal Network 비디오 (b) 누락된 HD 모달리티 평가 오디오 Attention Fusion IMU Multimodal Network 오디오 (c) 크로스 모달 제로샷 평가 IMU Attention Fusion Multimodal Network | 오디오 IMU Attention Fusion Multimodal Network Attention Fusion Multimodal Network Attention Fusion Multimodal Network 그림 1. MMG-Ego4D 챌린지 개요. 일반적인 평가 설정에서 (a) 네트워크는 모든 모달리티의 데이터가 있는 훈련/지원 세트를 사용하여 감독 설정 또는 few-shot 설정에 대해 훈련되고 모든 모달리티의 데이터 포인트에서 평가됩니다. 그러나 훈련 및 테스트 모달리티 간에 불일치가 있는 경우가 많습니다. 제안된 챌린지에는 이러한 설정을 모방하기 위한 두 가지 작업이 포함되어 있습니다. (b) 누락된 모달리티 평가에서 모델은 추론을 위해 훈련 모달리티의 하위 집합만 사용할 수 있습니다. (c) 크로스모달 제로샷 평가에서 모델은 훈련 중에 보지 못한 모달리티에 있습니다. Ray-Ban Stories와 같은 장착된 장치가 인기를 얻으면서, 자기중심적 비디오의 동작 인식은 상황에 맞는 추천이나 알림과 같은 다운스트림 애플리케이션을 활성화하는 데 중요합니다. 그러나 자기중심적 동작 인식은 근본적으로 다르고 더 어렵습니다[6, 7, 43, 55]. 3인칭 비디오 클립은 종종 큐레이션되는 반면, 자기중심적 비디오 클립은 큐레이션되지 않았으며 머리 움직임으로 인한 큰 모션 블러와 같은 저수준의 손상이 있습니다. 게다가, 자기중심적 인식에는 카메라 착용자의 물리적 주변 환경을 신중하게 이해해야 하며 착용자의 관점에서 객체와 상호 작용을 해석해야 합니다. 자기중심적 활동을 오직 하나의 비디오 오디오 IMU에서만 인식 비디오 오디오 ↑ ↑ ↑ 다른 도구로 잔디 깎기 쓰레기통에 쓰레기 버리기 ☑ 테이블 닦기 X 바닥에 마른 잎 모으기 모달리티 초당 데이터 메모리(KB) 일반 모델 FLOPS(G) 비디오 오디오 IMU 593.92 62.76 9.70.50 42.08 1.표 1. 다양한 모달리티에 대한 계산 및 메모리 비용. 각 모달리티에 대해 초당 사용되는 메모리는 Ego4D[27]에서 무작위로 추출한 1000개 데이터 포인트가 사용하는 메모리를 평균하여 계산합니다. 제공된 계산 번호는 비디오의 MVIT[15], 오디오의 AST[26] 및 IMU 데이터의 ViT[11] 기반 변환기 모델의 전방 전달 비용에 해당합니다. 단일 모달 네트워크 주의 융합 IMU 다중 모달 네트워크 그림 2. 다중 모달 데이터는 자기중심적 지각에 필수적입니다. 입력 데이터는 비디오, 오디오 및 IMU의 세 가지 모달리티로 구성됩니다. (위) 비디오 동작 인식은 도구와 배경에 많은 풀이 있는 클립을 다른 도구로 풀을 다듬는 클래스로 식별합니다. 오디오 동작 인식 시스템은 주기적인 문지르는 소리를 쓰레기통에 쓰레기를 넣는 것으로 분류합니다. IMU 모델은 머리 움직임 동작을 테이블 닦기 클래스로 분류합니다. (아래) 다중 모드 동작 인식 시스템은 비디오 피드와 오디오 피드를 올바르게 결합하여 활동을 땅에 마른 잎을 모으는 것으로 식별합니다. 모달리티는 종종 모호할 수 있습니다. 이는 카메라 피드가 캡처하는 내용 대신 장치 착용자가 수행하는 작업을 인식하고자 하기 때문입니다. 이를 위해 다중 모드 정보는 사용자의 의도나 동작을 이해하고 모호성을 해소하는 데 중요할 수 있습니다. 그림 2의 예를 통해 이를 보여줍니다. 이 예에서 비디오 피드는 초원 배경에 있는 도구를 보여줍니다. 비디오에만 기반한 활동 인식 모델은 이를 다른 도구로 풀을 다듬는 클래스로 인식합니다. 마찬가지로 오디오에만 훈련된 모델은 클립의 문지르는 소리를 쓰레기통에 쓰레기를 넣는 클래스로 식별하고 IMU 모델은 머리 동작을 테이블 닦기로 오인합니다. 그러나 멀티모달 시스템은 비디오, 오디오 및 IMU 신호를 결합하여 지상에서 마른 잎을 모으는 클래스를 올바르게 식별합니다. 멀티모달 정보를 사용하는 것은 성능 상태를 달성하는 데 필수적이지만 보안 또는 효율성 고려 사항으로 인해 실제 세계에서 모든 모달리티를 사용할 수 없는 고유한 과제도 제시합니다. 예를 들어, 사용자가 민감한 환경에 있고 보안 문제로 인해 카메라를 끄기로 결정할 수 있습니다. 마찬가지로 사용자는 음성이 들리지 않도록 마이크를 끌 수 있습니다. 이러한 상황에서 멀티모달 시스템은 누락된 모달리티(그림 1(b))로 일반화할 수 있어야 합니다. 즉, 추론 시 불완전한 모달리티 세트로 작업하고 견고한 예측을 수행해야 합니다. 이러한 과제는 추론 시간에만 국한되지 않고 학습 중 제한으로 나타날 수 있습니다. 예를 들어, 사용자가 종종 몇 번의 샷 설정에서 시스템을 학습해야 하는 경우 비디오와 같은 계산적으로 비싼 모달리티는 클라우드에서 가장 잘 학습됩니다. 그러나 사용자는 데이터가 장치에 남아 있기를 원할 수 있습니다. 그러나 비디오는 IMU와 같은 저렴한 모달리티에 비해 60배 더 많은 저장 공간과 43배 더 많은 컴퓨팅을 소모하므로(표 1 참조), 컴퓨팅과 저장 공간이 제한된 장치에서 학습하는 데 어려움이 크게 증가합니다. 이러한 상황에서는 비디오와 같은 보다 많은 정보를 제공하는 모달리티에 대한 추론을 수행하는 유연성을 유지하면서 오디오와 같은 계산적으로 덜 까다로운 모달리티로 학습할 수 있도록 해야 할 수 있습니다. 다중 모달 시스템은 모달리티 간에 강력하게 일반화해야 합니다. 이 연구에서는 자기중심적 활동 인식 모델의 일반화 능력을 측정하도록 설계된 챌린지인 MMG-Ego4D를 제안합니다. 저희의 챌린지는 두 가지 새로운 과제로 구성되어 있습니다. (1) 불완전한 모달리티 집합에서 평가할 때 모델의 일반화 능력을 측정하는 것을 목표로 하는 누락된 모달 일반화(그림(b) 참조), (2) 테스트 시간 동안 보이지 않는 모달리티로 일반화할 때 모델의 일반화 능력을 측정하는 것을 목표로 하는 교차 모달 제로샷 일반화(그림 1(c) 참조). 이 벤치마크를 사용하여 널리 사용되는 여러 아키텍처를 평가하고 MMG-Ego4D 챌린지에서 일반화 기능을 향상시키고 표준 풀모달리티 설정에서 성능을 개선하는 새로운 접근 방식을 소개합니다. 우리의 주요 기여는 다음과 같습니다. • MMG 문제. 우리는 다중 모달 액션 인식 모델의 일반화 능력을 평가하기 위해 누락된 모달리티 일반화와 크로스 모달 제로샷 일반화의 두 가지 과제가 있는 새롭고 실용적인 문제인 MMG를 제시합니다. 이러한 과제는 실제 보안 및 효율성 고려 사항을 지원하도록 설계되었으며, 우리는 이를 감독 및 더 어려운 fewshot 설정에서 모두 정의합니다. • MMG-Ego4D 데이터 세트. 자기 중심 액션 인식 과제에서 MMG 문제 연구를 용이하게 하기 위해, 우리는 데이터 포인트를 전처리하고 인간 전문가가 과제에 맞게 철저히 다시 주석을 달아 Ego4D [27] 데이터 세트에서 파생된 새로운 데이터 세트인 MMG-Ego4d를 소개합니다. 우리가 아는 한, 이것은 이러한 새로운 평가 과제와 그 종류의 벤치마크 챌린지를 소개한 최초의 작업입니다. 강력한 기준선. 우리는 일반화 능력 벤치마크에서 강력한 성능을 달성하고 일반적인 전체 모달리티 설정에서 성능도 개선하는 새로운 방법을 제시합니다.우리의 방법은 Transformer 기반 퓨전 모듈을 사용하여 다양한 모달리티의 유연한 입력이 가능합니다.우리는 교차 모달 대조 정렬 손실을 사용하여 다양한 모달리티의 특징을 통합된 공간으로 투사합니다.마지막으로, 교차 모달 프로토타입 손실이라고 하는 새로운 손실 함수가 도입되어 다중 모달 소수 샷 설정에서 최첨단 결과를 얻습니다.각 제안된 구성 요소의 기여도를 식별하기 위해 광범위한 절제 연구가 수행됩니다.2. 관련 연구 다중 모달 자기중심적 행동 인식.행동 인식 시스템은 일반적으로 비디오에서 훈련됩니다[15–19, 25, 65, 70].그러나 자기중심적 활동 인식(즉, 1인칭 관점 또는 캡처링 장치를 착용한 사용자가 수행하는 활동 인식)의 경우 올바른 활동을 식별하기 위해 보완적인 다중 모달 정보가 필수적입니다(그림 2 참조). 자기중심적 활동 인식에서 다중 모달 융합을 위한 이전 방법은 단순 연결[57, 73]에서 텐서 분해[45]에 이르기까지 다양했으며, 최근 연구에서는 유망한 결과를 보여준 트랜스포머 기반 아키텍처[1,38,46,49]를 채택했습니다. 이 연구에서는 트랜스포머 기반 융합 모듈과 모달리티 드롭아웃 학습 전략을 활용하여 MMG 작업의 성능을 더욱 개선합니다. 다중 모달 모델의 일반화. MMG 문제에 속하는 작업 중 하나인 누락된 모달리티 문제는 최근 몇몇 연구에서 연구되었습니다[40, 46, 47,53,66,67]. 그러나 대부분의 연구는 이중 모달 상황에 초점을 맞춥니다. [67]은 인수분해된 다중 모달 표현을 학습하여 다중 모달 이미지(두 도메인) 분류 문제를 해결합니다. [47]은 베이지안 메타 학습 프레임워크를 활용하여 시청각 분류 문제를 다룹니다. [46]은 특히 텍스트-시각적 분류 작업에서 누락된 모달 데이터에 대한 멀티모달 변환기 모델의 견고성을 조사하고 멀티태스크 학습과 검색된 최적 융합 전략을 통해 견고성을 개선합니다.크로스 모달 제로샷 액션 인식은 여전히 충분히 탐구되지 않은 새로운 문제입니다.이는 크로스 모달 검색 문제[63, 74, 75, 77]와 관련이 있는 반면, 후자는 다양한 모달리티에서 기능 유사성을 측정하는 방법에 초점을 맞춥니다.멀티모달 퓨샷 학습.멀티모달 퓨샷 학습[13, 48, 52, 53, 68]은 머신 러닝 모델이 여러 모달리티의 제한된 예를 기반으로 새로운 객체를 인식하고 분류할 수 있도록 하는 것을 목표로 하는 새로운 연구 분야입니다. 영어: 기존의 few-shot learning 연구는 이미지[5, 8, 12, 23, 24, 41, 50, 59, 62, 64, 78]나 언어[2, 29, 71, 72, 76]와 같이 단일 모달리티에 주로 초점을 맞췄습니다. 그러나 few-shot learning을 멀티모달 시나리오로 확장하는 데 대한 관심이 증가하고 있습니다. 이 분야의 선구적인 작업에는 [52,53]에서 보여준 것처럼 환각 이미지를 통해 텍스트 조건부 GAN을 사용하여 데이터를 증강하는 것이 포함됩니다. Eloff et al.[13]은 음성 및 이미지 모달리티에 대한 원샷 크로스 모달 매칭 문제에 샴 네트워크를 활용합니다. 데이터 세트 및 벤치마크. 데이터 세트의 가용성과 명확하게 정의된 벤치마크 작업은 분류[9], 감지[14], 분할[44] 및 동작 인식[3, 14, 28, 37]과 같은 사용 사례에서 성능을 개선하는 데 중요한 요인이었습니다. 이러한 작업의 성과가 종종 인간의 성과를 능가했지만[31], 연구자들은 최첨단 방법이 종종 취약하고[35] 손상[10, 21]이나 적대적 예[4, 39]와 같이 약간 다른 데이터 포인트에 일반화되지 않는다는 것을 보여주었습니다. 일반화 능력을 측정하기 위한 벤치마크, 데이터 세트 및 작업을 명확하게 정의한 것은 견고성 연구를 추진하는 데 크게 기여했습니다[32-34]. 제안하는 벤치마크와 데이터 세트인 Ego4D-MMG는 자기중심적 행동 인식에서 다중 모달 일반화 능력을 측정하도록 특별히 설계된 최초의 벤치마크입니다. 이 벤치마크가 MMG 작업의 진전을 촉진하고 안전을 인식하는 일반화 가능 모델의 개발을 장려하기를 바랍니다. 3. 제안된 벤치마크: MMG-Ego4D 3.1. 개요 예비 단계. 우리는 &quot;지도 설정&quot;이라는 용어를 많은 수의 레이블이 지정된 학습 데이터(학습 세트)를 사용할 수 있는 일반적인 액션 인식 작업을 지칭하는 데 사용합니다. 테스트 시간 동안 목표는 각 테스트 데이터 포인트(테스트 세트)를 하나의 학습 레이블로 분류하는 것입니다. 반면, few-shot 설정에서는 몇 개의 레이블이 지정된 학습 데이터만 사용할 수 있습니다. 테스트 시간의 목표는 지도 설정과 동일합니다. 실제로 few-shot 설정의 학습 및 테스트 세트를 각각 지원 및 쿼리 세트라고 합니다. 지원 및 쿼리 세트를 함께 모은 것을 &quot;에피소드&quot;라고 합니다[20,61,69]. 학습 모달리티 및 테스트 모달리티라는 용어는 각각 지도 설정에서 지도 학습 및 테스트 중에 사용 가능한 모달리티를 지칭합니다. few-shot 설정에서는 지원 및 쿼리 모달리티를 지칭합니다. 데이터. MMG-Ego4D 데이터 세트는 비디오, 오디오 및 관성 모션 센서(IMU)의 세 가지 모달리티를 가진 데이터 포인트로 구성되어 있습니다. Ego4D 데이터 세트[27](다른 데이터 세트를 선택하지 않는 이유는 보충 설명에서 설명합니다).IMU 데이터에는 가속도계와 자이로스코프에서 얻은 신호가 포함되어 있습니다.우리는 Ego4D 데이터 세트에서 약 시간 분량의 데이터를 사용하여 벤치마크를 만듭니다.167시간 분량의 레이블이 지정되지 않은 시간 정렬 비디오-오디오-IMU 데이터와 35시간 분량의 레이블이 지정된 시간 정렬 데이터입니다.우리는 벤치마크에 적합한 데이터를 만들기 위해 여러 단계를 수행합니다.먼저, 활동이 발생한 데이터에서 타임스탬프를 식별하고 각 데이터 포인트를 5초로 표준화하여 Ego4D Moments 트랙[27]에서 데이터를 가져옵니다.IMU 및 비디오 데이터는 200Hz 및 4FPS로 하위 샘플링됩니다.둘째, Ego4D의 여러 데이터 포인트에는 여러 레이블이 포함되어 있으며, 이는 주로 (1) 수행되는 여러 활동 및 (2) 활동이 계층 구조에서 서로 관련되어 있기 때문입니다(예: 재료 섞기 대 요리). 우리는 WordNet 계층 구조를 휴리스틱으로 사용하여 레이블 공간을 통합하고, 인간 주석자가 레이블을 면밀히 조사하도록 했습니다. 주석자가 하나의 올바른 레이블을 확실히 식별하지 못하면, 우리는 벤치마크에서 해당 데이터 포인트를 버렸습니다. 마지막으로, 우리는 두 가지 주요 기준을 사용하여 MMG-Ego4D few-shot 벤치마크를 만들었습니다. 첫째, 두 개의 의미적으로 분리된 클래스 세트, 즉 기본 클래스와 새로운 클래스로 구성되어야 합니다. 둘째, 동일한 원본 클립의 데이터 포인트가 기본 클래스와 새로운 클래스에 모두 존재할 수 없습니다. 우리는 두 단계로 이를 달성했습니다. 처음에 주석자는 79개 레이블을 65개 기본 클래스와 새로운 클래스로 수동으로 분할하여 few-shot 평가 벤치마크에 의미적으로 유사한 레이블이 포함되지 않도록 했습니다. 그런 다음, 동일한 기본 클립의 데이터 포인트가 기본 클래스와 새로운 클래스에 모두 존재하지 않는다는 것을 확인했습니다. 우리는 기본 클래스를 감독 작업의 훈련 세트로 사용했고, Ego4D Moments 트랙에서 추가 데이터를 추출하여 감독 작업의 해당 테스트 세트를 형성했습니다. 3.2. 제안된 MMG 작업 MMG-Ego4D의 목표는 학습 및 테스트 모드 간에 불일치가 있는 상황에서 머신 러닝 알고리즘의 일반화 능력을 평가하는 것입니다. 인간은 누락된 모드를 아주 잘 처리할 수 있습니다. 예를 들어, 비디오에서만 동작을 식별할 수 있습니다. 마찬가지로 비디오만 사용하여 개념을 소개하더라도 오디오와 같은 다른 모드를 사용하여 이 개념을 식별할 수 있는 경우가 많습니다(예: 우는 아기). 이 섹션에서는 다중 모드 활동 인식 시스템의 일반화 능력을 평가하도록 설계된 두 가지 새로운 작업에 대해 설명합니다. 이러한 작업은 웨어러블 장치를 사용하는 동안의 실제 보안 고려 사항을 반영합니다. 또한 이러한 작업의 성능은 현재 다중 모드 머신 인식이 인간 인식과 얼마나 가까운지 측정할 수도 있습니다. MMG 작업의 개요는 그림 1에 나와 있습니다. 누락된 모드 평가. 이 작업은 모델이 학습에 사용된 모드의 하위 집합만을 사용하여 추론을 얼마나 잘 수행할 수 있는지 측정합니다. 추론하는 동안 전력이나 계산적 제약으로 인해 평가를 수행하기 위해 모드의 하위 집합만 사용할 수 있습니다. 이는 가변적인 평가 설정을 제시하며, 벤치마크에서 결과를 보고하기 위해 그 중 일부를 선택합니다. few-shot 설정의 맥락에서 이 작업은 지원 모달리티의 하위 집합을 쿼리 모달리티로 사용하는 것으로 축소됩니다. Zero-Shot Cross-Modal 일반화. 이 작업은 모델이 보이지 않는 모달리티로 얼마나 잘 일반화할 수 있는지 측정합니다. 학습 및 테스트 모달리티는 분리되어 있습니다. 저희 맥락에서 모델은 학습을 위해 IMU와 오디오만 사용할 수 있지만(비디오 모델 학습은 매우 비쌈) 테스트 시간에 비디오 데이터를 사용할 수 있습니다(비디오 추론에 대한 예산이 허용 가능하고 더 나은 결과를 얻을 수 있음). 마찬가지로 few-shot 학습의 맥락에서 이 작업에서는 지원 및 쿼리 모달리티가 분리되어 있습니다. 4. 일반화 성능 개선 이 섹션에서는 제안된 MMG-Ego4D 벤치마크에서 높은 성능을 달성하는 강력한 기준선을 소개합니다. 저희의 방법은 멀티모달 시스템의 일반화 능력을 개선하도록 설계된 세 가지 새로운 구성 요소로 구성되어 있습니다. 우리는 제안하는 방법에 대한 개요를 제시하고, 각 구성 요소의 중요성을 강조하고, 그 구현에 대한 자세한 설명을 제공하는 것으로 시작합니다.4.1. 방법 개요 이 섹션에서는 제안하는 방법 파이프라인의 개요를 설명합니다.Few-shot 설정에서 모든 평가 작업은 세 단계로 구성된 동일한 학습 파이프라인을 채택합니다.(1) 단봉형 지도 사전 학습: 각 모달리티에 대한 특징 추출기는 별도로 학습됩니다.(2) 다중 모달 지도 사전 학습: 융합 모듈은 단봉형 네트워크의 끝에 연결되어 다중 모달 시스템을 형성한 다음 교차 엔트로피 손실과 교차 모달 대조 정렬 손실로 학습합니다.후자의 손실 항은 모든 모달리티에 대한 통합된 특징 공간을 구성하여 모델의 다중 모달 일반화 가능성을 향상시키는 것을 목표로 합니다.(3) 다중 모달 메타 학습: 다중 모달 네트워크는 모델의 교차 모달 일반화 가능성을 더욱 향상시키기 위해 프로토타입 기반 손실로 메타 학습됩니다. 위의 few-shot 설정의 학습 파이프라인에서 모든 모달리티의 데이터가 사용된다는 점에 주목할 가치가 있습니다.MMG-Ego4D 작업의 모달리티 제한은 few-shot 평가 동안 지원 및 쿼리 세트에만 적용됩니다.지도 학습 설정에서 일반 및 누락된 모달리티 평가 설정은 동일한 학습 파이프라인을 채택하여 (1) 단모달 지도 사전 학습 및 (2) 멀티모달 지도 사전 학습을 포함하며, few-shot 설정 학습 파이프라인의 처음 두 단계와 동일합니다.반대로, zero-shot 교차 모달 설정은 다른 2단계 학습 파이프라인을 갖습니다.(1) 다중 모달 비지도 사전 학습: 다중 모달 네트워크는 레이블이 지정되지 않은 데이터를 사용하여 교차 모달 대조 정렬 손실로 학습하여 모달리티에 독립적인 통합 기능 공간을 확립합니다. (2) 멀티모달 지도 사전 학습: 멀티모달 네트워크는 멀티모달 메타 학습 설정 작업 멀티모달 유니모달 멀티모달 비지도 학습 지도 사전 학습 사전 학습 학습 지도 학습 누락 모달 일반 제로 샷 Lalign Few-shot 일반 누락 모달 제로 샷 ССЕ LCE + Lalign ССЕ LCE + Lalign LCE ССЕ LCE + Lalign Lproto ССЕ LCE + Lalign ССЕ LCE + Lalign Lproto Lproto 표 2. 지도 학습 및 소수 샷 설정의 학습 파이프라인. LCE는 교차 엔트로피 손실을 나타냅니다. Lalign 및 proto는 교차 모달 대조 정렬 손실 및 교차 모달 프로토타입 손실이며, 4.3절 및 4.4절에서 설명합니다. 영어: 이전 설정에서 사용된 대조적 정렬 손실 항 없이 교차 엔트로피 손실을 사용하여 학습했습니다.이 설정의 제한으로 인해 레이블이 지정된 데이터에는 평가 모달리티가 없기 때문입니다.학습 모달리티 간에 정렬을 구성하는 것은 의미가 없습니다.따라서 이 설정의 규칙을 위반하지 않는 모달리티-완전 레이블이 지정되지 않은 데이터를 사용하여 이러한 정렬을 구축하기로 했습니다.MMG-Ego4D 작업의 모달리티 제한은 멀티모달 감독 사전 학습 단계와 평가 단계에서 사용되는 레이블이 지정된 학습 데이터에 적용된다는 점에 유의해야 합니다.이는 few-shot 설정과 다릅니다.표 2에 학습 파이프라인을 요약했습니다.4.2. Transformer 기반 퓨전 모듈을 갖춘 멀티모달 네트워크 제안하는 멀티모달 네트워크는 단모달 백본과 Transformer 기반 퓨전 모듈이라는 두 가지 주요 구성 요소로 구성됩니다.단모달 백본은 서로 다른 입력 모달리티에서 피처를 추출하는 세 개의 개별 피처 추출기로 구성됩니다. 융합 모듈은 단일 모달 백본에서 다양한 모달리티의 특징을 융합하고 집계하여 융합된 특징을 출력하는 것을 목표로 합니다. 모달리티를 융합하는 데 널리 사용되는 옵션은 두 가지입니다. MLP를 사용하여 다양한 모달리티의 연결된 표현을 처리하는 것[51, 54, 57] 또는 Transformer 기반 융합 모듈을 사용하여 다양한 모달리티에서 일련의 토큰을 가져오는 것입니다[46,49,60]. Transformer 기반 융합 설계는 어텐션 모듈을 사용하여 임의의 수의 입력 토큰으로 쉽게 확장할 수 있으므로 채택합니다. 이는 제안된 작업의 맥락에서 멀티모달 모델이 다양한 수의 모달리티가 있는 데이터를 처리해야 하기 때문에 특히 중요합니다. 융합 모듈의 최종 출력은 CLS 토큰을 사용하는 대신 출력 토큰을 평균화하여 얻습니다[11,46]. 형식적으로 퓨전 모듈 fuse의 출력은 다음과 같이 작성할 수 있습니다.zfuse = mf ([output &#39;output +e™;]m = {audio, video, IMU}]), (1) 여기서 x는 모달리티 m에 대한 피처 추출기의 출력 표현을 나타냅니다.f는 다양한 모달리티에서 일련의 입력 토큰을 가져오는 퓨전 모듈입니다.각 모달리티의 토큰은 모달리티별 학습 가능 임베딩 em으로 증강되며, 이는 입력 토큰의 모달리티 정보를 모호하지 않게 하는 데 사용됩니다.퓨전 모듈의 학습 중에 모달리티 드롭이라는 기술을 적용했습니다.학습 중에 모달리티의 하위 집합을 확률 p로 무작위로 삭제하여 퓨전 모듈이 다양한 수의 입력 모달리티에 강인하도록 합니다.4.3. 크로스 모달 정렬 멀티모달 학습 제로샷 크로스 모달 설정에서 멀티모달 모델은 분리된 모달리티에서 학습하고 추론해야 합니다. 이를 달성하기 위한 한 가지 접근 방식은 다양한 모달리티의 표현을 포착하는 통합된 피처 공간을 구성하는 것입니다. 피처 공간은 동일한 데이터 포인트이지만 다른 모달리티의 피처가 서로 가까이 있도록 해야 합니다. 이를 통해 한 모달리티에서 학습한 지식을 다른 모달리티의 추론에 적용할 수 있습니다. 이를 달성하기 위해 대조 손실을 사용하여 다중 모달 학습에서 동일한 데이터 포인트이지만 다른 모달리티의 피처를 정렬하는 것을 제안합니다. 구체적으로, 퓨전 모듈의 단모달 피처 출력은 다음과 같이 표현됩니다. zm = f(xutput em), m = {audio, video, IMU}, (2) 이는 통합된 피처 공간에 있을 것으로 예상됩니다. 비디오-오디오-IMU 데이터의 다른 타임 스탬프에서 가져온 비디오-오디오 및 비디오-IMU 쌍을 정렬하기 위해 잡음 대조 추정(NCE) [58] 손실을 부과합니다. 양의 쌍은 동일한 시간적 위치에서 온 다른 모달리티 쌍으로 구성되는 반면, 음의 쌍은 다른 시간적 위치에서 온 것입니다. NCE 정렬 손실 Lalign은 다음과 같이 작성됩니다. Lalign(Zvideo, Zm) ― log Σ mЄ{audio, IMU} exp(z videoZm/T) exp(z video zm/T) + Σz&#39;en exp(z video z&#39;m/T) 여기서 N은 배치의 음수 쌍입니다. NCS 손실에서 특징 거리 측정 메트릭으로 코사인 유사도를 사용합니다. T는 부드러움을 제어하는 온도 매개변수입니다. 계층적 공통 공간[1]을 구축하는 이전 방법과 달리, 우리의 접근 방식은 모든 모달리티에 대한 통합된 특징 공간을 정의합니다. 4.4. 교차 모달 프로토타입 손실 어떤 속성이 몇 샷 작업에서 표현을 더 잘 일반화하는 데 도움이 될 수 있습니까? 우리는 다양한 모달리티의 피처 간 정렬을 고려하는 원형 손실[61]의 새로운 확장을 설계합니다. 원형 손실은 지원 임베딩의 중심과 피처 공간에서 쿼리 임베딩 간 거리를 최소화하는 것을 목표로 하며, 여기서 쿼리 데이터 포인트의 레이블은 모든 지원 중심까지의 거리에 따라 지정됩니다. 제안하는 방식에서 지원 및 쿼리 예제는 서로 다른 모달리티에 속할 수 있으므로 모달 간 정렬이 가능합니다(그림 3 참조). zk를 클래스 k의 통합 공간 지원 피처를 나타내는 데 사용하고 n을 통합 공간 쿼리 피처를 나타내는 데 사용합니다. 여기서 이들은 서로 다른 모달리티 m에 속할 수 있습니다(n = {오디오, 비디오, IMU}). 중심 통합 공간 지원 피처 ch는 다음을 평균하여 계산합니다. km = Σ .k Zm zmzm m (4) 여기서 Z는 모달리티 m을 가진 클래스 k의 지원 피처 집합입니다. &#39;n 쿼리 예제 z가 클래스 k에 속할 예측 확률은 쿼리 피처와 클래스 k에 대한 통합 공간 지원 피처의 중심 간 12 거리 d의 음의 지수를 사용하여 계산됩니다. ☐ ☐ ☐ ☐ ☐ ☐ ☐ ☐ ☐ ☐ 여성이 누군가와 대화하는 모습을 보여주는 비디오 클립. class여성들이 채팅하고 있습니다. class☐ ☐ ☐ ☐ ☐ 한 남자가 나무에서 과일을 수확하고 있습니다. ☐ ☐ ☐ ☐ ☐ Cs Cㅁㅁㅁㅁㅁㅁ class남자가 옷걸이에 옷을 걸고 있습니다. ○ 비디오 모달의 지원 피처. 오디오 모달의 쿼리 피처. 그림 3. 교차 모달 프로토타입 손실. 지원 예제의 피처를 평균화하여 계산한 소수 샷 프로토타입 중심 Ck. 바닐라 프로토타입 손실과 달리, 우리의 접근 방식은 지원 및 쿼리 예제가 서로 다른 모달리티에 속할 수 있도록 합니다. 그림은 지원 예제가 비디오 데이터이고 쿼리 예제가 오디오 데이터인 예를 보여줍니다. 모델 FLOPS(G) 매개변수(M) 모달 5 Way 5 Shot 정확도 MVIT-B [15] AST [26] 70.36.video 58.Top-Accuracy 52.IMU Transformer 42.1.87.audio 31.39.15.IMU 40.29.Pk exp(-d (n, c)) m, n = {audio, video, IMU} Σ exp(-d (n, ch)) (5) 제안하는 교차 모달 프로토타입 손실 proto는 쿼리 예제 ŷ에 대한 예측 확률과 기준 진실 클래스 간의 음의 로그 가능도 손실로 공식화됩니다. Lproto = NLL (log [Po, P1, ... PN-1],ŷ). (6) 요약하자면, 우리의 교차 모달 프로토타입 손실은 통합 피처 공간에서 지원 및 쿼리 피처 간의 교차 모달 정렬을 가능하게 함으로써 프로토타입 손실을 확장합니다. 이 손실은 few-shot 설정에서 zero-shot 교차 모달 작업에서 표현의 일반화 능력을 향상시킬 수 있습니다. 5.
--- EXPERIMENT ---
al 설정 5.1. 아키텍처 세부 정보 단봉형 백본. 비디오 모달리티의 특징 추출기로 MVIT-B(16×4)[15]를 사용하며, Kinetics-400[37]에서 사전 학습되었습니다. 오디오 스펙트로그램 변환기(AST)[26]를 오디오 특징 추출기로 사용하며, AudioSet[22]에서 사전 학습되었습니다. IMU 특징 추출기의 경우 ViT[11] 기반 변환기 네트워크를 설계했습니다. 퓨전 모듈. 퓨전 모듈은 두 개의 레이어가 있는 변환기 네트워크입니다. 각 레이어에는 12개의 헤드가 있는 셀프 어텐션 블록이 있습니다. 임베딩 차원은 768입니다. 5.2. 학습 및 평가 세부 정보 모델 학습 및 평가의 몇 가지 기본 세부 정보를 설명합니다. 학습률 및 배치 크기와 같은 하이퍼 매개변수는 보충 자료에 자세히 나와 있습니다. 표 3. 단봉형 few-shot 및 지도 평가 결과. 네트워크는 각 모달리티에서 독립적으로 학습됩니다. 비디오는 더 많은 계산 리소스를 소모하면서도 최상의 성능을 달성합니다.지도 학습 설정. 모델은 멀티모달 지도 학습을 위해 MMG-Ego4D 기본 클래스를 사용합니다.제로샷 교차 모달 설정에서 모델은 또한 멀티모달 비지도 사전 학습을 수행하기 위해 MMGEgo4D 레이블이 지정되지 않은 데이터를 활용합니다.모델 성능을 측정하기 위해 Top-1 정확도를 사용합니다.Few-Shot 설정.finetune 기반 방법을 사용하여 few-shot 평가를 수행합니다.여기서 작은 신경망은 지원 세트에서 학습되고 쿼리 세트의 데이터 포인트를 분류하는 데 사용됩니다[30,36,42].평가 설정으로 표준 Nway K-shot 설정[20,69]을 채택합니다.모델 성능을 측정하기 위해 Top-Accuracy를 사용합니다.최종 숫자는 한 에피소드의 결과를 평균하여 얻습니다.6. MMG-Ego4D 벤치마크 결과 6.1. MMG-Ego4D Few-Shot 설정 결과 멀티모달 시스템은 유니모달 시스템보다 상당히 우수한 성능을 보입니다. 표 3은 개별 모달리티에 대한 few-shot 분류 결과를 보여줍니다. 특히 비디오 모달리티는 가장 높은 정확도를 달성하는데, 이는 대부분의 클래스가 시각적 정보를 사용하여 쉽게 인식될 수 있기 때문에 예상됩니다. 그러나 그림 2에서 볼 수 있듯이, 다양한 모달리티의 정보를 융합하는 것은 더 나은 per-Eval을 달성하는 데 중요합니다. 설정 지원 모달리티 쿼리 모달리티 5 Way 5 Shot 정확도 평가. 설정 기차 모달리티 비디오 오디오 IMU 비디오 오디오 IMU 테스트 모달리티 비디오 오디오 IMU 비디오 오디오 IMU 최고 정확도 일반 ✓ ✓ ✓ ✓ ✓ ✓ 63.일반 ✓ ✓ ✓ ✓ ✓ ✓ ✓ 55.61.누락된 모달리티 ✓ 50.62.누락된 62.✓ 43.47.46.42.제로샷 평가 제로샷 평가 모달리티55.37.54.30.20.25.43.35.50.41.44.46.✓ 49.표 4. 다중 모달 소수 샷 평가 결과.이 결과는 세 가지 평가 설정 모두에서 작동하는 단일 네트워크로 얻었습니다.첫 번째 블록에서 일반 평가 결과를 보여줍니다.여기서 모델은 모든 모달리티로 훈련되고 평가됩니다.두 번째 블록은 누락된 모달리티 결과를 제시합니다.여기서 모델은 모든 모달리티로 훈련되지만 하위 집합으로만 평가됩니다. 마지막 블록은 훈련 및 평가 모달리티가 분리된 교차 모달 제로샷 평가의 결과입니다. 모든 결과는 동일한 모델 가중치를 사용하여 얻은 것입니다. 보충 자료에서는 더 많은 훈련 및 테스트 모달리티 구성으로 결과를 제공합니다. 자기중심적 행동 감지에서의 성능. 제안하는 멀티모달 시스템은 정확도 측면에서 최고 성능을 내는 유니모달 시스템보다 4.11배 더 우수한 성능을 보입니다(표 4 블록 1). 누락된 모달리티 일반화. 표 4의 두 번째 블록에 누락된 모달리티 평가 결과를 제시합니다. 여기서 쿼리 모달리티는 지원 모달리티의 하위 집합입니다. 평가 중에 일부 모달리티가 누락된 경우에도 우리 모델은 우수한 일반화성을 보여 견고한 정확도를 달성합니다. 특히 쿼리 세트에 비디오 모달리티를 포함하면 멀티모달 경우에 비해 성능이 약간 변경됩니다. 비디오 모달리티를 포함하지 않으면 정확도가 19.41% 떨어져 비디오 모달리티가 가장 유익함을 나타냅니다. 놀랍게도 쿼리에 저렴한 모달리티(오디오 또는 IMU)가 하나만 있는 경우, 우리 방법은 IMU에서 19.24%, 오디오 모달리티에서 40.53%라는 큰 차이로 단봉 결과(표 3)를 능가하여 우리 접근 방식의 효과를 보여줍니다.제로샷 크로스 모달 일반화.이 작업은 지원 및 쿼리 모달리티가 분리되어 있기 때문에 누락된 모달리티 일반화보다 더 큰 과제를 제시합니다.몇 가지 조합을 선택하여 표 4의 마지막 블록에 결과를 제시합니다.효율적인 학습을 가능하게 하기 위해 IMU 및 오디오와 같이 지원 모달리티가 계산적으로 저렴한 반면 비디오와 같이 쿼리 모달리티가 비교적 더 많은 정보를 제공하는 설정을 선택하여 높은 성능을 달성합니다.이 평가 세트를 사용하여 우리 모델은 오디오 및 IMU 단봉 설정보다 상당히 우수한 성능을 보입니다.표 5. 지도 설정 평가 결과.결과는 표 4와 동일한 구조에 따라 구성됩니다.이 모델은 일반 및 누락된 모달리티 평가에서 동일한 가중치를 갖습니다. 또한 비디오를 지원 모달리티로 사용하고 IMU 및/또는 오디오를 쿼리 모달리티로 사용한 결과를 제시하는데, 여기서도 우리 모델은 여전히 적절한 정확도를 얻습니다. 공간 제한으로 인해 모든 지원-쿼리 모달리티 조합을 포함하지는 않았지만, 독자는 추가 결과에 대해 논문 보충 자료를 참조할 수 있습니다. 6.2. MMG-Ego4D 지도 설정 결과 지도 설정의 결과는 표 5에 나와 있습니다. 우리의 멀티모달 모델은 일반 설정에서 표 3의 각 유니모달 모델보다 상당히 우수한 성능을 보입니다. 누락된 모달리티 평가와 관련하여, 우리 방법은 누락된 모달리티가 있는 경우에도 강력한 일반화 능력을 보여줍니다. 평가 모달리티에서 비디오 모달리티가 유지되는 경우 성능이 약간 떨어질 뿐입니다. 그러나 평가 중에 비디오 데이터가 누락되면 성능이 약 33% 떨어져 비디오 모달리티가 다른 두 모달리티보다 더 많은 정보를 제공한다는 것을 알 수 있습니다. 표 5의 마지막 블록은 제로 샷 크로스 모달 결과를 보여줍니다. 영어: 우리는 두 가지 사례를 살펴본다. 학습에는 값비싼 모달리티를 사용하고 추론에는 값싼 모달리티를 사용하는 경우와 학습에는 값싼 모달리티를 사용하고 추론에는 값비싼 모달리티를 사용하는 경우이다. 우리는 후자의 경우 모델이 더 나은 성능을 보이는 것을 관찰했는데, 이는 정보적 모달리티에서 학습하는 것이 모델에 더 많은 이점을 준다는 것을 나타낸다. 6.3. Ablation 연구의 통찰력 이 섹션에서는 일반, 누락 모달리티, 교차 모달 제로샷 평가를 포함한 다양한 평가 설정에서 설계된 멀티모달 시스템의 각 구성 요소의 효과를 신중하게 제거합니다. 퓨전 모듈. 이 연구에서는 멀티모달 네트워크에서 서로 다른 모달리티의 정보를 통합하는 대체 방법으로 Transformer 기반 퓨전 모듈을 사용하는 것을 제안한다. 성능을 평가하기 위해 diverseEval의 표현을 연결하는 MLP 기반 퓨전 모듈과 비교 분석을 수행한다. 설정 비디오 학습/지원 모달. 모듈 테스트/쿼리 모달. 퓨전 대조적 상단 정렬 정확도 교차 모달 프로토타입. 손실 오디오 IMU 비디오 오디오 IMU 5 방식 5 샷 정확도 주의 55.63.일반 ✓ 주의 52.61.MLP 52.58.주의 62.주의 37.누락된 모달 50.주의 21.40.MLP 32.49.주의 × 50.주의 25.03* 51.제로 샷 크로스 모달 주의 MLP 주의 2.24.54* 33.51.50.표 6. 감독 및 소수 샷 설정에서 각 설계 구성 요소의 절제 연구. 제안된 구성 요소는 모든 평가 설정에서 성능을 개선합니다. 크로스 모달 프로토타입 손실은 소수 샷 설정에서만 적용됩니다. *다른 설정과 달리, 교차 모달 대조 정렬 손실은 지도 제로 샷 교차 모달 설정의 비지도 멀티모달 사전 학습 단계에서 적용됩니다. 모달리티를 처리하고 MLP를 사용하여 처리합니다. 공정한 비교를 보장하기 위해 두 모듈의 입력 및 출력 표현의 차원을 일관되게 유지하고 매개변수 수를 비슷하게 유지합니다. MLP 기반 퓨전 모듈의 입력에 일부 모달리티가 없는 상황에서는 해당 표현을 제로 벡터로 대체합니다. 표 6에 제시된 절제 연구 결과는 Transformer 기반 퓨전 모듈이 모든 작업에서 few-shot 및 지도 학습 시나리오에서 MLP 기반 퓨전 모듈보다 성능이 우수함을 보여줍니다. 또한 세 가지 결정 선택을 경험적으로 조사합니다. 구체적으로 CLS 토큰을 사용하거나 최종 예측을 위해 모든 출력 토큰을 평균화하는 것의 효능을 검토합니다. 모든 출력 토큰을 평균화하면 성능이 더 좋아진다는 것을 알게 되었습니다. 또한, 융합 전에 모달리티별 임베딩을 포함하는 것을 평가하고, 그것이 모달리티를 구별하는 모델의 능력을 돕는 데 효과적이라는 것을 발견했습니다. 마지막으로, 모달리티 드롭아웃에 대한 다양한 드롭아웃 비율(p)로 실험하고, p = 0.6에서 가장 좋은 결과를 얻은 값 범위(0.~0.8)에 걸쳐 일관된 성능을 얻는다는 것을 발견했습니다. 교차 모달 대조 정렬 손실. 교차 모달 정렬 손실을 파이프라인에 통합하려는 동기는 교차 모달 제로샷 일반화 성능을 향상시키고자 하는 욕구에 기인합니다. 표 6에서 이 구성 요소를 포함하면 지도 학습 및 소수 샷 학습 설정에서 각각 교차 모달 제로샷 일반화 성능이 22.66 및 17.47만큼 현저하게 향상되었습니다. 또한, 교차 모달 정렬 손실을 통합하면 일반 및 누락된 모달 작업에서도 성능이 향상되는 것을 관찰했습니다. 이러한 결과는 MMG-Ego4D 벤치마크에서 성공하는 데 교차 모달 정렬의 중요성을 강조합니다. 교차 모달 프로토타입 손실. 저희 연구에서는 MMG 작업에서 few-shot 성능을 향상시키는 수단으로 교차 모달 프로토타입 손실을 통합하는 것을 제안했습니다. 표 6에서 보여준 것처럼 저희의 실험 결과는 이 새로운 구성 요소가 누락된 모달과 제로샷 시나리오에서 각각 0.74와 0.6포인트의 성능 향상에 기여하는 동시에 일반 모달 완전 평가 설정에서 0.63포인트의 향상을 가져온다는 것을 보여줍니다. 이러한 결과는 MMG 작업 성능 최적화 전략에 귀중한 추가 사항으로서 교차 모달 프로토타입 손실의 효능을 증명합니다. 7.
--- CONCLUSION ---
s 이 논문에서 우리는 멀티모달 일반화(MMG)를 위한 최초의 포괄적 벤치마크를 소개하고 모델의 일반화 성능을 개선하기 위한 세 가지 구성 요소를 제안했습니다. 우리의 벤치마크인 MMG-Ego4D에는 두 가지 새로운 작업과 새로운 데이터 세트가 포함됩니다. 다양한 베이스라인 아키텍처의 평가는 현재 시스템의 일반화 능력이 제한적임을 보여주었습니다. 따라서 벤치마킹과 일반화 능력 개선은 특히 모델이 더 민감한 사용 사례에 배포됨에 따라 주목할 만합니다. 광범위한 실험과 절제 연구를 통해 모달 드롭아웃 훈련과 융합 중 단모달 표현의 정렬을 사용한 제안된 주의 기반 융합 메커니즘이 MMG-Ego4D에서 감독 및 퓨어샷 작업의 성능을 개선할 수 있음을 보여주었습니다. 우리가 제안한 크로스 모달 프로토타입 손실은 또한 MMG-Ego4D에서 퓨어샷 작업의 성능을 개선합니다. 우리는 새로운 데이터 세트를 만들고 멀티모달 일반화 문제에 대한 엄격한 연구를 위한 새로운 실험을 도입했습니다. 이러한 방법은 일반화 가능성을 높일 수 있으며 보안 환경이 중요한 실제 환경에 필수적입니다.참고문헌 [1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong.Vatt: 원시 비디오, 오디오 및 텍스트에서 다중 모드 자기 감독 학습을 위한 변환기.신경 정보 처리 시스템의 발전, 34:24206-24221, 2021.3,[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.언어 모델은 소수 샷 학습기입니다.신경 정보 처리 시스템의 발전, 33:1877-1901, 2020.[3] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Juan Carlos Niebles. Activitynet: 인간 활동 이해를 위한 대규모 비디오 벤치마크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 961-970페이지, 2015년.[4] Nicholas Carlini, Guy Katz, Clark Barrett, David L Dill. 최소한으로 왜곡된 입증 가능한 적대적 사례. arXiv 사전 인쇄본 arXiv:1709.10207, 2017년.[5] Zitian Chen, Yanwei Fu, Yu-Xiong Wang, Lin Ma, Wei Liu, Martial Hebert. 원샷 학습을 위한 이미지 변형 메타 네트워크. CVPR, 2019년.[6] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price 등 자기중심적 비전 확장: epic-kitchens 데이터세트. 컴퓨터 비전에 관한 유럽 회의(ECCV) 간행물, 페이지 720-736, 2018.[7] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price 등 자기중심적 비전 재조정: epic-kitchens-100을 위한 수집, 파이프라인 및 과제. 국제 컴퓨터 비전 저널, 130(1):33–55, 2022.[8] Rajshekhar Das, Yu-Xiong Wang, Jose MF Moura. few-shot 분류를 위한 디스트랙터의 중요성에 관하여. ICCV, 2021.[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스. 2009년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 248~255페이지. IEEE, 2009.[10] Samuel Dodge와 Lina Karam. 시각적 왜곡 하에서 인간과 딥 러닝 인식 성능에 대한 연구 및 비교. 2017년 제26회 국제 컴퓨터 통신 및 네트워크 컨퍼런스(ICCCN), 1~7페이지. IEEE, 2017.[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 규모에 따른 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020. 2, 5,[12] Nikita Dvornik, Cordelia Schmid, Julien Mairal. 협력을 통한 다양성: 소수 분류를 위한 앙상블 방법. ICCV에서, 2019.[13] Ryan Eloff, Herman A Engelbrecht, Herman Kamper. 음성 및 이미지의 다중 모드 원샷 학습. ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 8623-8627페이지. IEEE, 2019.[14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman. Pascal Visual Object Classes(VOC) Challenge. International journal of computer vision, 88(2):303-338, 2010.[15] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer. Multiscale vision transformers. IEEE/CVF International Conference on Computer Vision의 회의록, 6824-6835페이지, 2021. 2, 3,[16] Christoph Feichtenhofer. X3d: 효율적인 비디오 인식을 위한 확장 아키텍처. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 203-213페이지, 2020.[17] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He. 비디오 인식을 위한 Slowfast 네트워크. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 6202-6211페이지, 2019.[18] Christoph Feichtenhofer, Axel Pinz, Richard P Wildes. 비디오 동작 인식을 위한 시공간적 곱셈기 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4768-4777페이지, 2017.[19] Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman. 비디오 동작 인식을 위한 합성곱 2스트림 네트워크 융합. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1933-1941페이지, 2016.[20] Chelsea Finn, Pieter Abbeel, Sergey Levine. 모델 독립적인 메타 학습을 통한 딥 네트워크의 빠른 적응. 기계 학습에 관한 국제 컨퍼런스, 1126-1135쪽. PMLR, 2017. 3,[21] Robert Geirhos, David HJ Janssen, Heiko H Schütt, Jonas Rauber, Matthias Bethge, Felix A Wichmann. 딥 신경망과 인간을 비교: 신호가 약해졌을 때의 객체 인식. arXiv 사전 인쇄본 arXiv:1706.06969, 2017.[22] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, Marvin Ritter. 오디오 세트: 오디오 이벤트를 위한 온톨로지 및 휴먼 레이블 데이터 세트. 2017년 IEEE 음향, 음성 및 신호 처리(ICASSP) 국제 컨퍼런스, 776-780쪽. IEEE, 2017.[23] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, Matthieu Cord. 자기 감독을 통한 few-shot 시각적 학습 강화. ICCV에서, 2019.[24] Spyros Gidaris와 Nikos Komodakis. few-shot 학습을 위한 gnn 노이즈 제거 자동 인코더로 분류 가중치 생성. CVPR에서, 2019.[25] Xinyu Gong, Heng Wang, Mike Zheng Shou, Matt Feiszli, Zhangyang Wang, Zhicheng Yan. 비디오 인식을 위한 다변수 공간에서 twostream 모델 검색. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 8033-8042페이지, 2021.[26] Yuan Gong, Yu-An Chung, James Glass. Ast: 오디오 스펙트로그램 변환기. arXiv 사전 인쇄 arXiv:2104.01778, 2021. 2,[27] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: 3,000시간 분량의 자기 중심적 영상으로 전 세계를 여행합니다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 18995-19012, 2022. 1, 2, 3,[28] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar 등 알. Ava: 공간-시간적으로 국소화된 원자 시각적 동작의 비디오 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 60476056페이지, 2018.[29] Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang. Ppt: few-shot 학습을 위한 사전 훈련된 프롬프트 튜닝. arXiv 사전 인쇄본 arXiv:2109.04332, 2021.[30] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, Rogerio Feris. 교차 도메인 few-shot 학습에 대한 광범위한 연구. 유럽 컴퓨터 비전 컨퍼런스, 124-141페이지. Springer, 2020.[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 정류기 심층 탐구: 이미지넷 분류에서 인간 수준 성능을 능가. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 1026-1034페이지, 2015년.[32] Dan Hendrycks 및 Thomas Dietterich. 일반적인 손상 및 교란에 대한 신경망 견고성 벤치마킹. arXiv 사전 인쇄본 arXiv:1903.12261, 2019년.[33] Guenter Hirsch. 대규모 어휘 작업에서 음성 인식 프런트엔드의 성능 평가를 위한 실험 프레임워크. ETSI STQ Aurora DSR 작업 그룹, 2002년 12월, 2002년.[34] Hans-Günter Hirsch 및 David Pearce. 잡음이 많은 조건에서 음성 인식 시스템의 성능 평가를 위한 오로라 실험 프레임워크. ASR2000 자동 음성 인식: 새로운 밀레니엄 ISCA 튜토리얼 및 연구 워크숍(ITRW)의 과제, 2000.[35] Hossein Hosseini, Baicen Xiao, Radha Poovendran. Google의 클라우드 비전 API는 노이즈에 강하지 않습니다. 2017년 제16회 IEEE 기계 학습 및 응용 프로그램 국제 컨퍼런스(ICMLA), 101-105페이지. IEEE, 2017.[36] Shell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, Timothy M Hospedales. 간단한 파이프라인의 한계를 뛰어넘는 몇 가지 샷 학습: 외부 데이터와 미세 조정이 차이를 만듭니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 90689077페이지, 2022.[37] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 운동학 인간 행동 비디오 데이터 세트. arXiv 사전 인쇄본 arXiv:1705.06950, 2017. 1, 3,[38] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: 합성곱 또는 영역 감독이 없는 비전 및 언어 변환기. International Conference on Machine Learning, 5583-5594페이지. PMLR, 2021.[39] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 규모에 따른 적대적 머신 러닝. arXiv 사전 인쇄본 arXiv:1611.01236, 2016.[40] Hu-Cheng Lee, Chih-Yu Lin, Pin-Chun Hsu, Winston H Hsu. 비디오 동작 인식에서 누락된 모달리티 문제를 위한 오디오 기능 생성. ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 3956-3960페이지. IEEE, 2019.[41] Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, Liwei Wang. 글로벌 클래스 표현을 사용한 Few-shot 학습. ICCV, 2019.[42] Wei-Hong Li, Xialei Liu, Hakan Bilen. 교차 도메인 few-shot 학습을 위한 작업 적응 개선. arXiv 사전 인쇄본 arXiv:2107.00358, 2021.[43] Yin Li, Miao Liu, James M Rehg. 보는 사람의 눈으로: 1인칭 비디오에서 시선과 동작의 공동 학습. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 619-635쪽, 2018년.[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 맥락 속의 공통 객체. 유럽 컴퓨터 비전 컨퍼런스, 740-755쪽. Springer, 2014년.[45] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency. 모달리티별 요인을 사용한 효율적인 저순위 멀티모달 융합. arXiv 사전 인쇄본 arXiv:1806.00064, 2018년.[46] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, Xi Peng. 멀티모달 트랜스포머가 누락된 모달리티에 강인한가? IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1817718186페이지, 2022. 3,[47] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi Peng. Smil: 심각하게 누락된 모달리티를 사용한 멀티모달 학습. AAAI 인공지능 컨퍼런스 회의록, 35권, 23022310페이지, 2021.[48] Yao Ma, Shilin Zhao, Weixiao Wang, Yaoman Li, Irwin King. 메타 학습의 멀티모달리티: 포괄적 조사. 지식 기반 시스템, 108976페이지, 2022.[49] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, Chen Sun. 다중 모달 융합을 위한 주의 병목 현상. 신경 정보 처리 시스템의 발전, 34:14200-14213, 2021. 3,[50] Alex Nichol, Joshua Achiam 및 John Schulman. 1차 메타 학습 알고리즘에 관하여. arXiv, 2018.[51] Juan DS Ortega, Mohammed Senoussaoui, Eric Granger, Marco Pedersoli, Patrick Cardinal 및 Alessandro L Koerich. 오디오-비디오 감정 인식을 위한 딥 신경망을 사용한 다중 모달 융합. arXiv 사전 인쇄본 arXiv:1907.03196, 2019.[52] Frederik Pahde, Patrick Jähnichen, Tassilo Klein 및 Moin Nabi. 소수 샷 미세 인식을 위한 교차 모달 환각. arXiv 사전 인쇄본 arXiv:1806.05147, 2018.[53] Frederik Pahde, Oleksiy Ostapenko, Patrick Jä Hnichen, Tassilo Klein, Moin Nabi. 다중 모드 few-shot 학습을 위한 자기 주도적 적대적 훈련. 2019년 IEEE Winter Conference on Applications of Computer Vision(WACV), 218-226페이지. IEEE, 2019.[54] Yagya Raj Pandeya 및 Joonwhoan Lee. 뮤직 비디오의 감정 분류를 위한 다중 모드 정보의 딥 러닝 기반 후기 융합. 멀티미디어 도구 및 응용 프로그램, 80(2):2887-2905, 2021.[55] Hamed Pirsiavash 및 Deva Ramanan. 1인칭 카메라 뷰에서 일상 생활 활동 감지. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2847-2854페이지. IEEE, 2012.[56] Ronald Poppe. 시각 기반 인간 행동 인식에 대한 조사. 이미지 및 비전 컴퓨팅, 28(6):976–990, 2010.[57] Soujanya Poria, Iti Chaturvedi, Erik Cambria, Amir Hussain. 합성곱 mkl 기반 다중 모드 감정 인식 및 감정 분석. 2016년 IEEE 16회 국제 데이터 마이닝 컨퍼런스(ICDM), 439–448페이지. IEEE, 2016. 3,[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 국제 기계 학습 컨퍼런스, 8748-8763페이지. PMLR, 2021.[59] Avinash Ravichandran, Rahul Bhotika, Stefano Soatto. 임베디드 클래스 모델과 샷 없는 메타 학습을 사용한 Few-shot 학습. ICCV, 2019.[60] Tim Siebert, Kai Norman Clasen, Mahdyar Ravanbakhsh, Begüm Demir. 원격 감지에서 시각적 질의 응답을 위한 다중 모달 퓨전 변환기. Image and Signal Processing for Remote Sensing XXVIII, 12267권, 162-170페이지. SPIE, 2022.[61] Jake Snell, Kevin Swersky, Richard Zemel. Few-shot 학습을 위한 프로토타입 네트워크. 신경 정보 처리 시스템의 발전, 30, 2017. 3,[62] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele. Few-shot 학습을 위한 메타 전이 학습. CVPR, 2019.[63] Christopher Thomas 및 Adriana Kovashka. 비문자적 교차 모달 검색을 위한 보완 샘플 강조. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 46324641페이지, 2022.[64] Pavel Tokmakov, Yu-Xiong Wang 및 Martial Hebert. few-shot 인식을 위한 구성적 표현 학습. ICCV, 2019.[65] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun 및 Manohar Paluri. 동작 인식을 위한 시공간적 합성에 대한 자세한 살펴보기. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6450-6459페이지, 2018.[66] Luan Tran, Xiaoming Liu, Jiayu Zhou, Rong Jin. 계단식 잔여 자동 인코더를 통한 누락된 모달리티 임퓨테이션. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1405-1414페이지, 2017.[67] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, LouisPhilippe Morency, Ruslan Salakhutdinov. 요인화된 다중 모달 표현 학습. arXiv 사전 인쇄본 arXiv:1806.06176, 2018.[68] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, Felix Hill. 동결된 언어 모델을 사용한 다중 모달 퓨샷 학습. 신경 정보 처리 시스템의 발전, 34:200-212, 2021.[69] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra 외. 원샷 학습을 위한 매칭 네트워크. 신경 정보 처리 시스템의 발전, 29, 2016. 3,[70] Limin Wang, Wei Li, Wen Li, Luc Van Gool. 비디오 분류를 위한 모양 및 관계 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1430-1439페이지, 2018.[71] Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma. 몇 번의 학습자로서의 Entailment. arXiv 사전 인쇄본 arXiv:2104.14690, 2021.[72] Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung. 언어 모델은 몇 번의 학습으로서의 다국어 학습자입니다. arXiv 사전 인쇄본 arXiv:2109.07684, 2021.[73] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, Christoph Feichtenhofer. 비디오 인식을 위한 시청각적 느리고 빠른 네트워크. arXiv 사전 인쇄본 arXiv:2001.08740, 2020.[74] Hong Xuan 및 Xi Stephen Chen. 이미지-텍스트 검색을 위한 심층적 메트릭 학습 손실 분석. IEEE/CVF 컴퓨터 비전 응용 프로그램 겨울 컨퍼런스 회의록, 2164-2173페이지, 2023.[75] Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, Wangchunshu Zhou. X²-vlm: 시각-언어 작업을 위한 올인원 사전 학습 모델. arXiv 사전 인쇄본 arXiv:2211.12402, 2022.[76] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. 사용 전 보정: 언어 모델의 few-shot 성능 개선. 기계 학습 국제 컨퍼런스, 12697-12706페이지. PMLR, 2021.[77] Liangli Zhen, Peng Hu, Xu Wang, Dezhong Peng. 심층 감독 교차 모달 검색. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10394-10403페이지, 2019.[78] Xiatian Zhu, Antoine Toisoul, Juan Perez-Rua, Li Zhang, Brais Martinez, Tao Xiang. 프로토타입 중심 주의 학습을 통한 몇 가지 샷 액션 인식. BMVC, 2021년.
