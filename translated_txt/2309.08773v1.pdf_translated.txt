--- ABSTRACT ---
이 논문은 모델 학습 중에 오디오와 텍스트 표현 간의 정렬을 강조하여 오디오 생성에 대한 제어를 강화하는 혁신적인 접근 방식을 제시합니다. 언어 모델 기반 오디오 생성의 맥락에서 이 모델은 텍스트 및 오디오 토큰 표현 모두의 입력을 활용하여 후속 오디오 토큰을 예측합니다. 그러나 현재 구성에는 선택한 텍스트 표현과 언어 모델의 예측 간의 정렬을 보장하기 위한 명시적 정규화가 없습니다. 제안에는 오디오 및 텍스트 표현 정규화를 통합하는 것이 포함되며, 특히 언어 모델 학습 중에 텍스트 조건이 교차 주의에서 제외되는 분류자 없는 안내(CFG) 단계에서 그렇습니다. 제안된 표현 정규화의 목적은 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사성의 불일치를 최소화하는 것입니다. 음악 및 오디오 생성 작업 모두에 대한 실험 결과는 제안된 방법이 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선과 오디오 생성에 대한 인간의 지각의 향상으로 이어진다는 것을 보여줍니다. 색인 용어 오디오 생성, 음악 생성, 표현 정규화 1.
--- INTRODUCTION ---
특정 요구 사항을 충족시키기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발 및 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다. 최근 신경 생성 모델의 출현은 디지털 콘텐츠 생성 환경에 혁신적인 변화를 가져왔습니다. 이미지 생성의 놀라운 발전에서 영감을 얻어[1, 2], 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델로 전환되는 패러다임 변화를 겪었습니다[3, 4, 5, 6, 7, 8, 9, 10]. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 영역에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께, 오디오 생성 작업에서도 뛰어난 성능을 보인 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 고품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 그런 다음 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 개별 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대용량 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위한 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 훈련됩니다. 그러나 전체 훈련 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징입니다.&quot;라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 탁구공이 튀는 소리가 여러 개 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 캡처하기 위해 생성 모델의 훈련을 개선하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화하여 달성됩니다. 언어 모델 훈련은 텍스트 조건 훈련과 분류자 없는 안내(CFG) 훈련의 두 가지 모드로 구성됩니다[26, 6]. CFG에서 텍스트 조건은 언어 모델 훈련 중에 생략됩니다. 동일한 훈련 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사도의 불일치를 줄임으로써 오디오 및 텍스트 표현 유사도를 향상시킵니다. 음악 및 음향 효과 생성의 실험 결과는 제안된 접근 방식의 효과를 보여주며, VGG 분류기를 사용한 Frechet 오디오 거리(FAD)[27], PaSST 모델을 사용한 kullback-leibler(KL) 발산[28], 대조 언어 오디오 사전 훈련 모델(CLAP)[29]을 기반으로 한 텍스트 및 오디오 정렬 점수, 오디오 생성을 위한 인간의 주관적 평가에서 개선이 나타났습니다.
--- RELATED WORK ---
이 연구는 [20, 21, 22, 8, 6, 7]과 같은 연구에 제시된 언어 모델 접근 방식을 적용하는데, 여기서 압축 모델은 오디오를 토큰으로 분리하여 학습한 다음 이 토큰을 오디오로 디코딩합니다. 언어 모델은 오디오 토큰을 생성하는 방법을 학습합니다. 그러나 우리의 강조점은 제공된 텍스트 설명과 생성된 오디오 간의 의미적 상관 관계를 증강하는 데 있습니다. 이 향상은 언어 모델 기반 오디오 생성을 위한 MusicGen [8] 및 AudioGen [6]의 기초 위에 구축되었습니다. 텍스트와 오디오 간의 표현 유사성을 모델링하기 위해 관련 작업 중 하나는 대조 손실을 사용하는 CLAP [29]입니다. 그러나 우리는 생성 모델 학습을 위해 CLAP에서 대조 손실을 사용해도 성능이 향상되지 않는다는 것을 발견했습니다. 대신, 우리는 먼저 서로 다른 샘플 간의 오디오와 텍스트의 표현 유사성을 계산하는 새로운 접근 방식을 제안합니다. 그런 다음 오디오의 유사성과 텍스트의 유사성 간의 불일치를 최소화합니다. 또한, 개별 시간 단계 출력에서 시퀀스 수준 표현을 얻는 데 있어 최대 풀링이 평균 풀링보다 더 나은 것을 발견했습니다.3. 표현 정규화 텍스트 입력 텍스트 인코더 텍스트 표현 표현 정규화 각 임베딩 테이블은 하나의 요리책에 해당합니다.변압기 스택 오디오 표현 임베딩 테이블 Encodec 오디오 토큰 한 단계 이동 선형 투영 선형 투영 선형 투영 선형 투영 각 선형 투영은 하나의 요리책에 해당합니다.CE 손실 그림 1. 교차 엔트로피 손실과 표현 정규화를 사용한 언어 모델 학습 그림.3.1. 언어 모델 기반 오디오 생성 언어 모델 기반 오디오 생성 모델은 그림 1에 표시된 것처럼 여러 핵심 요소로 구성됩니다.첫째, EnCodec 모델[30, 23]과 같은 압축 모델을 사용하여 원시 오디오 데이터를 토큰 ak,i의 개별 다중 스트림 시퀀스로 인코딩합니다. 여기서 i Є [1, Ta]이고 Ta는 오디오 토큰 시퀀스의 길이이고, k = [1, K]는 k번째로 인덱싱된 특정 코드북을 나타냅니다. 또한, 이 모델은 사전 훈련된 텍스트 인코더를 통합하여 텍스트 입력을 v;로 식별된 임베딩 표현의 시퀀스로 변환합니다. 여기서 j Є [1,T₂], T₁는 텍스트 임베딩 표현을 포함하는 시퀀스의 길이에 해당합니다. 마지막으로, Transformer 계층의 스택인 언어 모델 구성 요소가 있습니다. 언어 모델은 텍스트 임베딩 표현과 이전 오디오 토큰을 모두 활용하여 후속 오디오 토큰에 대한 확률 분포를 po(ak,i+1|ak,1, ..., ak,i, V1, ..., VT)로 생성합니다. 오디오 생성을 보다 관리하기 쉽게 만들기 위해 다중 스트림 오디오 토큰의 생성은 병렬로 훈련되어 모델 훈련 중에 효과적인 시퀀스 길이가 상당히 줄어듭니다. 언어 모델의 손실은 각 스트림 k에 대한 교차 엔트로피 손실의 합계입니다. Lcond K Ta -Σ log(pe (ak,i+1|ak,1, ..., ak,i, V1, ..., UT₁)) (1) k=1 i=3.2. 표현 정규화 그러나 언어 모델의 교차 엔트로피 손실은 오디오 토큰 예측이 제공된 텍스트 조건과 일치하도록 강제하는 명시적 메커니즘이 부족합니다. 더욱이 텍스트와 오디오 간의 상관 관계는 분류자 없는 안내(CFG)로 인해 더욱 느슨해집니다.
--- METHOD ---
s는 오디오 및 음악 생성을 위한 객관적인 지표의 개선과 오디오 생성을 위한 인간의 지각의 향상으로 이어진다. 색인 용어 오디오 생성, 음악 생성, 표현 정규화 1. 서론 특정 요구 사항을 충족하기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발, 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다. 최근 신경 생성 모델의 출현으로 디지털 콘텐츠 생성 환경에 획기적인 변화가 일어났습니다. 이미지 생성의 놀라운 발전[1, 2]에서 영감을 얻어 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델[3, 4, 5, 6, 7, 8, 9, 10]로 전환되는 패러다임 변화를 겪었습니다. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 분야에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었으며, 이는 오디오 생성 작업에서도 뛰어난 성능을 보였습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서는 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 높은 품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 이산 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대규모 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위해 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 학습됩니다. 그러나 전체 학습 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징&quot;이라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 여러 개의 탁구공이 튀는 소리가 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 포착하기 위해 생성 모델의 학습을 개선하는 것을 목표로 하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화함으로써 달성됩니다. 언어 모델 학습은 텍스트 조건 학습과 분류자 없는 안내(CFG) 학습의 두 가지 모드로 구성됩니다[26, 6]. CFG에서는 언어 모델 학습 중에 텍스트 조건이 생략됩니다. 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오와 텍스트 유사성의 불일치를 줄임으로써 오디오와 텍스트 표현 유사성을 향상시킵니다.
--- EXPERIMENT ---
음악 및 오디오 생성 작업 모두에 대한 모든 결과는 제안된 방법이 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선과 오디오 생성에 대한 인간의 지각의 향상으로 이어진다는 것을 보여줍니다.색인 용어 오디오 생성, 음악 생성, 표현 정규화 1. 서론 특정 요구 사항을 충족하기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발, 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다.최근 신경 생성 모델의 출현으로 디지털 콘텐츠 생성 환경에 획기적인 변화가 일어났습니다.이미지 생성의 놀라운 발전[1, 2]에서 영감을 얻어 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델[3, 4, 5, 6, 7, 8, 9, 10]로 전환되는 패러다임 변화를 겪었습니다. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 분야에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었으며, 이는 오디오 생성 작업에서도 뛰어난 성능을 보였습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서는 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 높은 품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 이산 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대규모 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위해 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 학습됩니다. 그러나 전체 학습 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징&quot;이라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 여러 개의 탁구공이 튀는 소리가 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 포착하기 위해 생성 모델의 학습을 개선하는 것을 목표로 하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화함으로써 달성됩니다. 언어 모델 학습은 텍스트 조건 학습과 분류자 없는 안내(CFG) 학습의 두 가지 모드로 구성됩니다[26, 6]. CFG에서는 언어 모델 학습 중에 텍스트 조건이 생략됩니다. 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오와 텍스트 유사성의 불일치를 줄임으로써 오디오와 텍스트 표현 유사성을 향상시킵니다. 음악 및 음향 효과 생성의 실험 결과는 제안된 접근 방식의 효과를 보여주며, VGG 분류기[27]를 사용한 Frechet 오디오 거리(FAD), PaSST 모델을 사용한 kullback-leibler(KL) 발산[28], 대조 언어 오디오 사전 학습 모델(CLAP)[29]을 기반으로 한 텍스트 및 오디오 정렬 점수, 오디오 생성을 위한 인간의 주관적 평가에서 개선이 나타났습니다. 2. 관련 연구 이 연구는 [20, 21, 22, 8, 6, 7]과 같은 연구에 제시된 언어 모델 접근 방식을 적용합니다. 여기서 압축 모델은 오디오를 토큰으로 분리하여 학습한 다음 이러한 토큰을 오디오로 디코딩합니다. 언어 모델은 오디오 토큰을 생성하는 방법을 학습합니다. 그러나 우리의 강조점은 제공된 텍스트 설명과 생성된 오디오 간의 의미적 상관 관계를 증강하는 데 있습니다. 이 향상은 언어 모델 기반 오디오 생성을 위한 MusicGen[8] 및 AudioGen[6]의 기초 위에 구축되었습니다. 텍스트와 오디오 간의 표현 유사성을 모델링하기 위해 관련 작업 중 하나는 대조 손실을 사용하는 CLAP[29]입니다. 그러나 생성 모델 학습을 위해 CLAP에서 대조 손실을 사용해도 성능이 향상되지 않는다는 것을 발견했습니다. 대신, 서로 다른 샘플 간의 오디오와 텍스트의 표현 유사성을 먼저 계산하는 새로운 접근 방식을 제안합니다. 그런 다음 오디오의 유사성과 텍스트의 유사성 간의 불일치를 최소화합니다. 또한 개별 시간 단계 출력에서 시퀀스 수준 표현을 얻는 데 최대 풀링이 평균 풀링보다 더 나은 것을 발견했습니다. 3. 표현 정규화 텍스트 입력 텍스트 인코더 텍스트 표현 표현 정규화 각 임베딩 테이블은 하나의 요리책에 해당합니다. 변압기 스택 오디오 표현 임베딩 테이블 인코딩 오디오 토큰 한 단계 이동 선형 투영 선형 투영 선형 투영 선형 투영 각 선형 투영은 하나의 요리책에 해당합니다. CE 손실 그림 1. 교차 엔트로피 손실과 표현 정규화를 사용한 언어 모델 학습의 예. 3.1. 언어 모델 기반 오디오 생성 언어 모델 기반 오디오 생성 모델은 그림 1에 표시된 대로 몇 가지 핵심 요소로 구성됩니다. 첫째, EnCodec 모델[30, 23]과 같은 압축 모델을 사용하여 원시 오디오 데이터를 토큰 ak,i의 개별 다중 스트림 시퀀스로 인코딩합니다. 여기서 i Є [1, Ta]이고 Ta는 오디오 토큰 시퀀스의 길이이고 k = [1, K]는 k번째로 색인된 특정 코드북을 나타냅니다. 또한 이 모델은 사전 학습된 텍스트 인코더를 통합하여 텍스트 입력을 v;로 식별된 임베딩 표현의 시퀀스로 변환합니다. 여기서 j Є [1,T₂], T₁는 텍스트 임베딩 표현이 포함된 시퀀스의 길이에 해당합니다. 마지막으로 Transformer 계층의 스택인 언어 모델 구성 요소가 있습니다. 언어 모델은 텍스트 임베딩 표현과 이전 오디오 토큰을 모두 활용하여 후속 오디오 토큰에 대한 확률 분포를 po(ak,i+1|ak,1, ..., ak,i, V1, ..., VT)로 생성합니다. 오디오 생성을 보다 관리하기 쉽게 하기 위해 다중 스트림 오디오 토큰 생성은 병렬로 학습되어 모델 학습 중에 효과적인 시퀀스 길이가 상당히 줄어듭니다. 언어 모델의 손실은 각 스트림 k에 대한 교차 엔트로피 손실의 합입니다. Lcond K Ta -Σ log(pe (ak,i+1|ak,1, ..., ak,i, V1, ..., UT₁)) (1) k=1 i=3.2. 표현 정규화 그러나 언어 모델의 교차 엔트로피 손실에는 오디오 토큰 예측이 제공된 텍스트 조건과 일치하도록 강제하는 명시적 메커니즘이 없습니다. 또한, 샘플 품질과 다양성 간의 균형을 조절하기 위해 훈련에 분류자 없는 안내(CFG) 방법[26, 6, 8]을 사용함에 따라 텍스트와 오디오 간의 상관관계가 더욱 느슨해집니다.CFG를 사용하면 언어 모델을 조건부 및 무조건부 모두로 훈련할 수 있습니다.AudioGen[6]과 유사하게, 언어 모델 훈련 중에 훈련 샘플의 10%에 수반되는 텍스트가 생략됩니다.무조건부 상황에서 손실은 단순히 Luncond K Ta -Σ log(pe (ak,i+1|Ak,1, ..., Ak,i)) k=1 i=입니다.이 작업에서 제안된 표현 정규화는 CFG 방법이 텍스트에 대해 무조건적으로 언어 모델을 훈련하는 효과를 유지하면서 오디오 표현과 텍스트 표현 간의 상관관계를 강화합니다.훈련 샘플 배치가 주어지면 풀링 방법 F를 사용하여 배치의 특정 샘플 b에 대해 텍스트 시퀀스 표현을 T₁ = F(v₁₁, ..., v)로, 오디오 시퀀스 표현을 A³ = F(u), ..., u)로 얻습니다. 실험에서 최대 풀링이 가장 좋은 결과를 얻었습니다.텍스트와 오디오 표현을 동일한 공간에 직접 매핑하고 CLAP[29]처럼 오디오와 텍스트 간의 유사성을 최대화하는 대신, 다음과 같이 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사성의 불일치를 최소화하도록 제안합니다.ты, б = ть жто ||Tb ||||T|| (3) Ab✶ A¹ ||4b|||| A6 || Lrr Eo:=ô(TB, A,B*(B − 1) (4) (5) 여기서 Tb는 샘플 b와 6의 텍스트 입력 간의 표현 유사도를 나타냅니다. 그리고 Ab, b는 샘플 b와 ô의 오디오 간의 표현 유사도를 나타냅니다. B는 배치 크기입니다. Lrr은 한 샘플의 텍스트와 오디오가 다른 샘플과 관련하여 동일한 차이를 갖도록 합니다. 이 연구에서 제안된 표현 정규화는 CFG 단계에서만 적용됩니다. 완전한 모델 학습 손실은 다음과 같이 정의됩니다. 여기서 L = Luncond + XLrr CFG를 사용하는 경우 CFG를 사용하지 않는 경우 Lcond (6)은 표현 정규화에 대한 가중치를 나타냅니다. 표현 정규화는 CFG가 사용 중일 때 일반적인 학습 단계에서만 사용됩니다. 또한 비 CFG 시나리오에서 표현 정규화를 포함하는 실험을 수행했지만 이러한 실험은 객관적 지표에서 개선을 가져오지 못했습니다. 표현 정규화가 텍스트 표현을 복사하여 언어 모델 학습을 방해할 가능성이 있기 때문에 저하가 발생할 수 있다고 생각합니다. 영어: non-CFG에서 오디오 표현으로서의 교차 주의.4. 실험 이 작업에서 우리는 음향 효과 생성과 음악 생성을 포함하는 두 세트의 실험을 사용하여 제안된 방법의 효과를 검증한다.4.1. 데이터 세트 음악 생성에서 우리는 총 20,000시간의 라이선스 음악을 활용하는데, 이는 고품질의 10,000개 음악 트랙과 ShutterStock¹과 Pond52의 390,000개 악기 전용 음악 트랙의 내부 컴파일로 구성된다.모든 데이터 세트는 32kHz 샘플링 속도의 전체 길이의 음악이며, 텍스트 설명, 장르 분류, BPM, 태그와 같은 포괄적인 메타데이터가 함께 제공된다.우리의 평가는 MusicCaps 벤치마크[7]를 사용한다.MusicCaps 벤치마크는 다양한 장르에 걸쳐 균형을 이룬 1,000개 샘플의 하위 세트를 포함하여 5.5,000개 샘플로 구성된다.우리는 [8]과 같이 불균형한 하위 세트에 대한 객관적인 지표를 보고한다.사운드 효과 모델 학습을 위해 4,000시간의 학습 데이터를 포함하는 데이터 세트가 사용된다. 이 데이터 세트 1www.shutterstock.com/music 2www.pond5.com에는 AudioSet [31], BBC sound effects³, AudioCaps[32], Clotho v2 [33], VGG-Sound [34], FSD50K [35] 및 Free To Use Sounds 4와 같은 리소스가 통합되어 있습니다. 모든 오디오 파일은 16kHz의 속도로 샘플링됩니다. 텍스트 설명에 대해 [6]과 유사한 전처리 방법론을 채택합니다. 시작으로 AudioSet, VGG-Sound, FSD50K와 같은 데이터 세트의 다중 레이블 주석을 활용합니다. 오디오 샘플과 연결된 태그 목록을 연결하여 가상 문장을 구성합니다. 그런 다음 불용어와 숫자를 제거하고 AudioCaps, Clotho v2, Free To Use Sounds 및 BBC Sound Effects를 포함한 데이터 세트에서 사용 가능한 자연어 캡션을 레마타이징합니다. 마지막으로, 태그나 캡션에 &quot;음성&quot;이라는 용어가 포함된 샘플은 음성이 데이터에서 우세하다는 점을 고려하여 필터링합니다.4.2. 설정 우리의 접근 방식에는 음악 생성에 맞게 조정된 비인과적 5계층 EnCodec 모델이 포함되며, 모노포닉 음악의 경우 32kHz, 사운드 효과 생성의 경우 16kHz에서 작동합니다.이러한 EnCodec 모델은 초기 숨겨진 크기 64에서 시작하여 50Hz의 프레임 속도를 유지하며, 이는 모델의 5개 계층에서 두 배가 됩니다.임베딩은 각각 2048의 코드북 크기를 특징으로 하는 4개의 양자화기로 구성된 RVQ를 사용하여 양자화됩니다.이러한 EnCodec 모델은 언어 모델 학습과 동일한 오디오 데이터를 사용하여 학습됩니다.이 작업에서 사용된 변환기 모델은 300M 매개변수를 갖습니다.긴 시퀀스의 효율성을 높이기 위해 xFormers 패키지[37]의 메모리 효율적인 Flash Attention[36]을 사용하여 속도와 메모리 활용도를 모두 개선합니다.절제의 경우 사운드 효과 생성 모델 설정을 일관되게 사용합니다. 음악 생성 모델 학습을 위해 전체 트랙에서 무작위로 샘플링한 30초 오디오 세그먼트를 사용합니다.사운드 효과 생성 학습에서는 10초 오디오 클립을 사용합니다.모델 학습은 AdamW 최적화 도구[38], 192개 예제의 배치 크기, B1 0.9, 62 = 0.95, 분리된 가중치 감소 0.1, 그래디언트 클리핑 1.0을 활용하여 100K 단계에 걸쳐 진행됩니다.4k 단계의 워밍업과 함께 코사인 학습률 일정이 사용됩니다.또한 감소 계수 0.99가 특징인 지수 이동 평균이 적용됩니다.모델 학습은 Fully Sharded Data Parallel(FSDP) bfloat16을 사용한 혼합 정밀도를 사용합니다.우리는 사운드 효과 생성과 음악 생성 학습에 각각 16개의 GPU와 32개의 GPU를 사용했습니다.추론을 위한 샘플링 프로세스에서 우리는 상위 250개 토큰을 유지하고 온도 1.0을 적용하는 상위 k 샘플링[39]을 채택합니다. = 4.3. Ablation Study = 표 1은 AudioCaps 데이터 세트를 사용하여 음향 효과 생성 모델에서 수행한 Ablation Study 결과를 보여줍니다. 최적 모델은 최대 풀링에 기반한 표현 3 https://sound-effects.bbcrewind.co.uk/ 4https://www.freetousesounds.com/all-in-one-bundle/ 정규화로 학습되었으며, 가중치 매개변수는 = 3.0이고 학습 데이터의 10%를 CFG 학습에 할당했습니다. 반면, 평균 풀링 기반 시퀀스 표현 정규화를 사용한 경우 기준선에 비해 개선이 나타나지 않았습니다. 나아가 표 1은 CFG 학습이 FAD와 KL 점수를 모두 줄이는 데 중요한 역할을 한다는 것을 재확인합니다. FAD, KL 및 CLAP에서 CFG 入 FAD(↓) KL(↓) CLAP(†) 모델 성능. AudioGen의 결과는 github³을 참조합니다. FAD(↓) KL(↓) CLAP(↑) 방법 AudioGen [6] 1.1.0.ours w/o rr ours w/ rr 1.1.0.1.1.0.pool max 0.11.1.0.max 0.11.1.0.max 0.11.1.0.max 0.11.1.0.0.1.1.0.0.1.1.0.0.1.0.max 0.1.1.0.average 0.11.1.0.표 1. AudioCaps 기반 음향 효과 생성을 사용한 Ablation 연구. 열 &#39;pool&#39;은 오디오와 텍스트 표현에 대한 시퀀스 수준 표현을 구하기 위한 풀링 방법을 나타냅니다. 열 &#39;CFG&#39;는 학습에서 CFG를 사용하는 비율을 나타냅니다. 열 &#39;X&#39;는 표현 정규화에 사용된 가중치를 나타냅니다. 4.4. 음악 생성 표 2는 MusicCaps 데이터에 대한 객관적인 지표를 나타냅니다. 우리는 멜로디가 없는 MuiscLM, Noise2Music 및 MusicGen 1.5B 모델에 대한 원래 메트릭을 보고합니다. 특히, 제안된 표현 정규화를 도입하면 모든 메트릭에서 향상이 이루어집니다. 표현 정규화를 통합한 300M 매개변수 모델은 FAD 및 CLAP 측면에서 MusicGen 1.5B 매개변수 모델의 성능을 능가합니다. FAD(↓) KL(↓) CLAP(†) 방법 MusicLM [7] 4.Noise2Music[40] 2.MusicGen 1.5B[8] 5.1.0.ours 300M w/o rr 5.1.0.ours 300M w/ rr 4.1.0.표 2. MusicCaps를 사용한 음악 생성. &#39;w/ rr&#39; 및 &#39;w/o rr&#39;은 각각 표현 정규화가 있는 경우와 없는 경우를 의미합니다. 4.5. 음향 효과 생성 AudioCaps의 음향 효과 생성 결과는 표 3에 나와 있습니다. 추세는 음악 생성 실험과 동일합니다. 표현 정규화는 표 3. AudioCaps를 사용한 음향 효과 생성을 개선합니다. &#39;w/ rr&#39; 및 &#39;w/o rr&#39;은 각각 표현 정규화가 있는 경우와 없는 경우를 의미합니다. 4.6. 인간의 선호도 평가 표 4는 사운드 및 음악 생성 모델에 대한 주관적 지표를 제공합니다. 주관적 평가는 맹검 쌍대 비교 테스트를 사용했으며, 평가자에게 동일한 텍스트 프롬프트를 기반으로 하는 서로 다른 모델에서 생성한 두 샘플을 제공했습니다. 이 비교는 20개의 텍스트 프롬프트에 걸쳐 수행되었으며, 8명의 인간 평가자는 각 쌍에서 제공된 프롬프트와 더 나은 품질과 더 나은 일치를 보인다고 생각되는 샘플에 대한 선호도를 결정하는 작업을 맡았습니다. 주목할 점은 표현 정규화를 통합했을 때 음악 및 음향 효과 생성 모두에서 사용자 선호도 평가가 더 높았습니다. 음향 효과 생성에서 더 중요한 추세에 대한 가능한 설명은 음악이 음향 효과보다 더 추상적인 경향이 있기 때문입니다. 결과적으로 제공된 텍스트와 일치하는 불일치 사항은 인간 평가자에게 쉽게 드러나지 않을 수 있습니다. 방법 음악 음향 효과 우리의 w/o rr 우리의 w/ rr 48% 52% 33% 67% 표 4. 인간의 선호도 평가 5.
--- CONCLUSION ---
이 논문은 모델 학습 중에 오디오와 텍스트 표현 간의 정렬을 우선시하여 오디오 생성에 대한 제어성을 개선하기 위해 표현 정규화를 도입했습니다. 제안된 방법은 특히 분류자 없는 안내(CFG) 단계에서 오디오와 텍스트 유사성 정규화를 통합했으며, 여기서 텍스트 조건은 언어 모델 학습 중에 교차 주의에서 제외됩니다. 다양한 오디오 및 음악 생성 작업에서 수행된 실험 결과는 제안된 표현 정규화가 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선으로 이어졌음을 보여줍니다. 게다가 이러한 개선은 오디오 생성 품질과 정렬에 대한 인간의 지각에서 눈에 띄는 향상으로 이어졌습니다. 5https://github.com/facebookresearch/audiocraft/blob/main/model_cards 6. 참고문헌 [1] Robin Rombach, Andreas Blattmann 외, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, CVPR, 2022. [2] Aditya Ramesh, Prafulla Dhariwal 외, &quot;CLIP 잠재 모델을 사용한 계층적 텍스트 조건부 이미지 생성&quot;, arXiv, 2022. [3] Yang Song, Jascha Sohl-Dickstein 외, &quot;확률적 미분 방정식을 통한 점수 기반 생성 모델링&quot;, arXiv, 2020. [4] Haohe Liu, Qiao Tian 외, &quot;AudioLDM 2: 자기 감독 사전 학습을 통한 전체적인 오디오 생성 학습&quot;, arXiv, 2023년 8월. [5] Haohe Liu, Zehua Chen 등, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; arXiv, 2023. [6] Felix Kreuk, Gabriel Synnaeve 등, &quot;AudioGen: 텍스트로 안내되는 오디오 생성,&quot; arXiv, 2022. [7] Andrea Agostinelli, Timo I Denk 등, &quot;MusicLM: 텍스트에서 음악 생성,&quot; arXiv, 2023. [8] Jade Copet, Felix Kreuk 등, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv, 2023. [9] Matthew Le, Apoorv Vyas 등, &quot;Voicebox: 대규모 텍스트 안내 다국어 범용 음성 생성,&quot; arXiv, 2023. [10] Max WY Lam, Qiao Tian 등, 영어: “효율적인 신경 음악 생성,&quot; arXiv, 2023. [11] Prafulla Dhariwal 및 Alexander Nichol, “확산 모델이 이미지 합성에서 간을 이긴다,&quot; Adv. Neural Inf. Process. Syst., 2021. [12] Jonathan Ho, Ajay Jain 및 Pieter Abbeel, “잡음 제거 확산 확률적 모델,&quot; Adv. Neural Inf. Process. Syst., 2020. [13] Diederik Kingma, Tim Salimans 외, “변형 확산 모델,&quot; Adv. Neural Inf. Process. Syst., 2021. [14] Rongjie Huang, Max WY Lam, et al., “FastDiff: 고품질 음성 합성을 위한 빠른 조건부 확산 모델,&quot; arXiv, 2022. [15] Sungwon Kim, Heeseung Kim, and Sungroh Yoon, “GuidedTTS 2: 비전사 데이터를 사용한 고품질 적응형 Text-to-Speech를 위한 확산 모델,&quot; arXiv, 2022. [16] Kai Shen, Zeqian Ju, et al., “NaturalSpeech 2: 잠재 확산 모델은 자연스러운 Zero-Shot 음성 및 노래 합성기,&quot; arXiv, 2023. [17] Rongjie Huang, Jiawei Huang, et al., “Make-An-Audio: Prompt-Enhanced 확산 모델을 사용한 TextTo-Audio 생성,&quot; arXiv, 2023. [18] Flavio Schneider, Zhijing Jin 및 Bernhard Schölkopf, &quot;Moûsai: Long-Context 잠재 확산을 통한 텍스트-음악 생성,&quot; arXiv, 2023. [19] Ashish Vaswani, Noam Shazeer 외, &quot;주의만 있으면 됩니다.&quot; Adv. Neural Inf. Process. Syst., 2017. [20] Zalán Borsos, Raphaël Marinier 외, &quot;AudioLM: 오디오 생성을 위한 언어 모델링 접근법&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2023. [21] Ewan Dunbar, Mathieu Bernard 외, &quot;제로 리소스 음성 챌린지 2021: 구어체 모델링&quot;, arXiv, 2021. [22] Kushal Lakhotia, Eugene Kharitonov 외, &quot;원시 오디오에서 생성 구어체 모델링에 관하여&quot;, 계산 언어학 협회 저널, 2021. [23] Neil Zeghidour, Alejandro Luebs 외, &quot;SoundStream: 종단 간 신경 오디오 코덱&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2022. [24] Alexandre Défossez, Jade Copet 외, &quot;고충실도 신경 오디오 압축&quot;, arXiv, 2022. [25] Colin Raffel, Noam Shazeer 외, &quot;통합 텍스트 대 텍스트 변환기를 사용한 전이 학습의 한계 탐색&quot;, arXiv, 2019. [26] Jonathan Ho 및 Tim Salimans, 지침&quot;, arXiv, 2022. &quot;분류자 없는 확산 [27] Shawn Hershey, Sourish Chaudhuri 외, &quot;대규모 오디오 분류를 위한 CNN 아키텍처&quot;, ICASSP, 2017. [28] Khaled Koutini, Jan Schlüter 외, &quot;패치아웃을 사용한 오디오 변환기의 효율적인 학습&quot;, arXiv, 2021. [29] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail 및 Huaming Wang, “CLAP: 자연어 감독에서 오디오 개념 학습,” arXiv, 2022. [30] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi, “고충실도 신경 오디오 압축,” arXiv, 2022. [31] Jort F Gemmeke, Daniel PW Ellis, et al., “오디오 세트: 오디오 이벤트에 대한 온톨로지 및 인간 레이블 데이터 세트,” ICASSP, 2017. [32] Chris Dongjoo Kim, Byeongchang Kim, et al., “AudioCaps: 야생 오디오에 대한 캡션 생성,” NAACL, 2019. [33] Konstantinos Drossos, Samuel Lipping, Tuomas Virtanen, “Clotho: 오디오 캡션 데이터 세트,” ICASSP, 2020. [34] Honglie Chen, Weidi Xie, et al., “Vggsound: 대규모 오디오-비주얼 데이터 세트,&quot; ICASSP, 2020. [35] Eduardo Fonseca, Xavier Favory, et al., “FSD50K: 인간이 레이블을 지정한 사운드 이벤트의 오픈 데이터 세트,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2022. [36] Tri Dao, Daniel Y Fu, et al., &quot;FlashAttention: IO 인식을 통한 빠르고 메모리 효율적인 정확한 어텐션,&quot; arXiv, 2022. [37] Benjamin Lefaudeux, Francisco Massa, et al., “xformers: 모듈식 및 해킹 가능한 변압기 모델링 라이브러리,&quot; 2021. [38] Ilya Loshchilov 및 Frank Hutter, “분리된 가중치 감소 정규화,&quot; arXiv, 2017. [39] Angela Fan, Mike Lewis 및 Yann Dauphin, &quot;계층적 신경 스토리 세대,&quot; arXiv, 2018. [40] Qingqing Huang, Daniel S Park 외, &quot;Noise2Music: 확산 모델을 사용한 텍스트 조건 음악 생성,&quot; arXiv, 2023.
