--- ABSTRACT ---
대규모 언어 모델(LLM)은 방대한 양의 텍스트 데이터에 대한 사전 학습으로 인해 다양한 작업에서 소수/제로 샷 학습자로서 상당한 보편적 역량을 입증했습니다. GPT-3가 InstrctGPT 및 ChatGPT로 부스트되어 자연어 명령을 효과적으로 따라 실제 작업을 완료하는 것이 그 예입니다. 이 논문에서는 Flamingo 모델의 업스트림 인터리브 형식 사전 학습 데이터 세트에서 영감을 받아 다중 모달 모델에 명령어 튜닝을 도입하는 것을 제안합니다. 유사한 접근 방식을 채택하여 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 데이터 세트를 구성합니다. 그런 다음 MIMIC-IT에서 학습하고 향상된 명령어 따르기 능력과 컨텍스트 내 학습을 보여주는 OpenFlamingo(DeepMind의 Flamingo 오픈 소스 버전) 기반 다중 모달 모델인 Otter를 소개합니다. 또한 연구자들을 위해 OpenFlamingo의 구현을 최적화하여 필요한 교육 리소스를 1× A100 GPU에서 4× RTX-GPU로 민주화하고, 더 많은 연구자들이 모델을 사용자 지정 교육 및 추론 파이프라인에 통합할 수 있도록 OpenFlamingo와 Otter를 모두 Huggingface Transformers에 통합합니다. 1
--- INTRODUCTION ---
&amp; 동기 부여 대규모 언어 모델(LLM)은 다양한 작업을 소수/제로 샷 학습기로 수행하는 데 있어 상당한 보편적 역량을 입증했습니다. 이러한 모델은 방대한 양의 텍스트 데이터로 사전 학습되었으며 GPT-2[25] 및 GPT-3[6]과 같은 최근 연구에서 선보였습니다. 최근 연구에서는 GPT-3[6]을 InstrctGPT[22] 및 ChatGPT[20]로 강화하여 자연어 명령을 효과적으로 따라 실제 작업을 수행하고 다운스트림 미세 조정 중에 작업별 규칙을 명령으로 사용자 지정할 수 있도록 하여 사전 학습된 모델이 사용자 의도를 보다 효과적으로 이해하고 정확하고 관련성 있는 응답을 생성할 수 있도록 하는 것처럼 LLM에 권한을 부여하는 데 있어 명령 조정의 중요성을 강조했습니다. 유사한 시도가 다중 모달 모델에도 도입되었습니다. LLaMA-Adapter[38]는 추가 어댑터 모듈과 다중 모달 프롬프트를 추가하여 LLAMA[33]를 명령 따르기 모델로 적용하는 것을 목표로 합니다. Mini-GPT4[39]는 BLIP-2[15]의 아키텍처를 따르지만 언어 디코더를 더 긴 답변을 지원하는 Vicuna[9]로 대체합니다.LLaVA[17]는 동일한 CLIP[23] 비전 인코더와 Vicuna[9] 언어 디코더를 사용하고 GPT-4[19]에서 큐레이팅한 고품질 명령어 데이터 세트를 미세 조정합니다.이러한 작업은 뛰어난 결과를 달성하고 귀중한 통찰력을 제공했지만 사소한 공통 문제가 있습니다.특히 Equal Contribution Corresponding Author Technical Report에서 전체 모델이나 연결 부분을 미세 조정합니다.MMC텍스트가 섞인 수십억 규모의 이미지 코퍼스 OpenFlamingo MIMIC-IT 다중 모달 명령어 튜닝 데이터 세트와 컨텍스트 내 예제 Otter 그림 1: Otter 개요.Otter는 OpenFlamingo를 기반으로 제안한 MIMIC-IT 데이터 세트에서 미세 조정된 다중 모달 모델입니다.Otter 모델은 주어진 지침을 따르고 컨텍스트 내 예제를 활용하여 작업을 실행하는 향상된 기능을 보여줍니다. 작업별 데이터. 예를 들어, 일반적인 관행은 캡션[16] 또는 VQA[11] 작업의 이미지-텍스트 데이터 쌍을 사용하여 시각 및 언어 모듈을 정렬하는 것입니다. 이런 방식으로 언어 모델에 시각 정보를 내장하는 것이 효과적일 수 있지만, 정렬 모듈을 훈련하는 데 데이터가 사용되는 작업에 의존하기 때문에 이 관행이 본질적으로 작업에 따라 달라지는지 의문을 제기합니다. 성찰한 결과, DeepMind Flamingo[1]의 업스트림 사전 훈련 데이터 세트인 MultiModal MassiveWeb(M3W)은 보다 자연스러운 방식으로 시각 및 언어 정보를 정렬하는 데 상당한 가치가 있음을 발견했습니다. 이 데이터 세트는 모든 이미지와 텍스트가 인터리브 형식으로 배열된 HTML 웹 페이지로 구성됩니다. 구체적으로, 텍스트 조각은 위나 아래의 이미지(또는 비디오)를 설명할 수 있으며, 인접한 위치의 이미지(또는 비디오)와 텍스트 사이에 상관 관계가 있을 수 있습니다. 이러한 자연스러운 맥락 구성은 텍스트가 해당 이미지만 설명하는 캡션 데이터 세트보다 더 풍부한 정보를 제공합니다. 이 데이터 세트에서 학습된 Flamingo는 제로 및 퓨샷 일반화와 컨텍스트 내 학습 능력을 달성하여 멀티 모달 도메인에서 GPT-3 모멘트가 되었습니다. 그러나 DeepMind는 모델의 뛰어난 성능으로 인해 조기에 놀라움을 유발할 수 있기 때문에 Flamingo 모델과 M3W 데이터 세트를 대중에게 공개하지 않았습니다. 그럼에도 불구하고 LAION-AI의 OpenFlamingo 프로젝트[4]가 최근 공개되어 더 큰 규모로 동일한 인터리브 형식으로 해당 MMC4[40] 데이터 세트에 대한 액세스를 제공했습니다. 결과적으로 커뮤니티 연구자는 멀티 모달 모델 분야에서 추가 연구 및 개발을 위해 Flamingo 시리즈 작업을 계속 따를 수 있습니다. OpenFlamingo 모델은 인상적인 멀티 모달 컨텍스트 내 학습 능력을 보여주고 주어진 컨텍스트 내 예제로 작업을 실행하지만 업스트림 사전 학습된 모델로서 다운스트림 작업을 보다 효과적으로 수행하려면 여전히 명령어 튜닝이 필요합니다. 논문에서 우리는 MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트를 제안하고 3.1절에서 구성에 대한 세부 정보를 제공합니다. 그런 다음 OpenFlamingo를 기반으로 하는 컨텍스트 내 명령어 튜닝을 갖춘 멀티 모달 모델인 Otter를 소개합니다. 그림 1에서 Otter와 OpenFlamingo의 관계를 설명합니다. MIMIC-IT 데이터 세트에서 미세 조정된 Otter 모델은 4.1절의 정성적 분석에서 볼 수 있듯이 OpenFlamingo에 비해 명령어 수행 능력이 향상되었습니다. 한편, Otter는 4.2절에서 볼 수 있듯이 제공된 컨텍스트 내 학습 예제를 사용하여 명령어를 실행하는 법을 학습할 수 있습니다. 엔지니어링 관점에서 우리는 OpenFlamingo의 구현을 최적화하여 연구자들이 더 쉽게 접근할 수 있도록 했습니다. 최적화에는 최소 1× A100 GPU에서 4× RTX3090 GPU로 학습 요구 사항을 최적화하고 이를 Hugging Face Transformers[34]에 통합하여 몇 줄의 코드로 학습 및 추론을 간소화하는 것이 포함됩니다. 저희의 기여는 다중 모달 모델 분야에서 추가 연구 및 개발을 용이하게 합니다. 주요 기여 사항을 요약하면 다음과 같습니다. • MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트를 제공합니다. 각 데이터 샘플에는 명령어-이미지-답변 트리플릿과 해당 컨텍스트 내 예제가 포함됩니다. • OpenFlamingo를 기반으로 컨텍스트 내 명령어 튜닝을 갖춘 다중 모달 모델인 Otter를 소개합니다. 이 모델은 몇 가지 컨텍스트 내 학습 예제로 명령어를 따르고 새 명령어를 실행할 수 있습니다. • OpenFlamingo 구현에 대한 최적화에는 학습 요구 사항을 4× RTX3090 GPU로 줄이고 이를 Huggingface Transformers에 통합하여 몇 줄의 코드로 학습 및 추론을 간소화하는 것이 포함됩니다.2
--- RELATED WORK ---
2.1 대규모 멀티모달 모델 ChatGPT[20], GPT-4[19] 및 기타 대규모 언어 모델[33, 32, 9]의 최근 성공으로 최근 연구에서는 사전 학습된 언어 모델을 기반으로 다른 모달리티의 정보를 통합하는 것을 탐색하기 시작했습니다. 이러한 연구는 언어 모델의 기능을 더 많은 작업과 모달리티로 확장하며 두 가지 관점으로 분류할 수 있습니다. 시스템 설계 관점. 이 관점은 ChatGPT[20]를 디스패치 스케줄러로 사용하고 이를 통해 다양한 전문가 모델을 연결하여 다양한 시각적 작업을 허용하는 것을 포함합니다. 언어 프롬프트는 해당 작업 도메인 내에서 전문가 시각 언어 모델을 호출하기 위한 인터페이스 역할을 합니다. 이 범주의 작업에는 VisualChatGPT[35], HuggingGPT[29], Cola[8], XGPT[42], MM-REACT[37] 및 ViperGPT[31]가 있습니다. 이 접근 방식은 각 모델이 새 작업에 대해 개별적으로 훈련될 수 없다는 한계가 있으며, ChatGPT[20]를 강력한 명령어 전송 도구로 사용하면 API 쿼리 비용이 높아질 수 있습니다.종단 간 학습 가능 모델 관점.이 관점은 다양한 모달리티의 모델을 통합된 종단 간 학습 가능 모델(다중 모달 기반 모델이라고도 함)로 연결하는 데 중점을 둡니다.이 분야의 초기 작업으로는 언어와 비전을 모델링하기 위한 통합 아키텍처를 제안하고 나중에 LAIONAI에서 OpenFlamingo[4]로 오픈 소스화한 Flamingo[1]가 있습니다.기존의 다른 작업으로는 가벼운 쿼리 변환기와 2단계 부트스트랩 사전 학습을 사용하여 이미지의 정보를 텍스트 모달리티에 연결하는 BLIP-2[15]가 있습니다. GPT-4[19]의 인기로 2023년 이후 이 분야에 대한 관심이 증가했습니다. 엔터프라이즈 수준의 제품 모델에는 아직 출시되지 않은 OpenAI의 GPT-4[19] 비전 언어 버전, Google의 PaLM-E[10], Baidu의 ERNIE[5], Alibaba의 Tongyi Qianwen[2], Sensetime의 Senseova[27]가 포함됩니다. 학술적 멀티모달 노력에는 LLaMA-Adapters[38], Mini-GPT4[39], LLaVA[17]와 같은 다양한 모델이 포함됩니다. LLaMA-Adapters는 추가 어댑터 모듈과 멀티모달 프롬프트가 있는 명령어 따르기 모델로 LLaMA[33]를 적용하는 것을 목표로 합니다. Mini-GPT4는 BLIP-2의[15] 아키텍처를 따르지만 언어 디코더를 Vicuna[9]로 대체하여 더 긴 응답과 다중 라운드 대화를 더 잘 지원합니다. LLaVA는 훈련 가능한 프로젝터 매트릭스를 통해 텍스트와 이미지 모달리티를 연결합니다.이것은 간단한 경량 선형 계층입니다.그러나 LLaVA는 비전 인코더와 언어 디코더를 모두 지시 튜닝 데이터 세트에서 훈련하기 때문에 다른 모델에 비해 비용이 상대적으로 높습니다.반대로 Otter는 Flamingo 모델을 기반으로 몇 개의 교차 게이트 주의 계층을 훈련하여 시각 및 언어 정보를 연결하고 맥락 내 예제 간에 주의를 확립하여 비전 인코더와 언어 디코더를 고정시킵니다.2.2 다중 모달 지시 튜닝 데이터 세트 다중 모달 모델에서 지시 튜닝이라는 개념은 Multi-Instruct [36]에서 처음 소개되었으며, 11개의 광범위한 범주를 포괄하는 47개의 다양한 다중 모달 작업이 구성되었습니다.각 작업은 기존 오픈 소스 데이터 세트의 최소 5,000개 인스턴스(입력-출력 쌍)와 5개의 전문가가 작성한 지침으로 구성됩니다. Multi-Instruct는 Visual Question Answering[11, 41, 30], Image Captioning[16], Image Generation[7], Visual Relationship Understanding[14] 등과 같이 시각적 이해와 멀티모달 추론이 필요한 대부분의 멀티모달 작업을 포괄합니다.마찬가지로 MiniGPT4[39]는 Conceptual Caption[28, 7], SBU[21], LAION[26]을 손으로 쓴 지침 템플릿과 결합하여 지침 따르기 데이터 세트를 구성합니다.최근에 LLAVA[17]는 GPT-4[19]를 사용하여 손으로 쓴 시드 지침이 있는 COCO[16] 이미지의 원래 캡션을 확장하여 더 자세한 설명과 다중 라운드 대화를 제공함으로써 지침 튜닝 데이터 세트의 품질을 더 높은 수준으로 끌어올렸습니다.우리의 지식에 따르면, 위에 언급된 작업은 멀티모달 모델에서 지침 튜닝을 고려한 유일한 몇 안 되는 작업입니다. 우리의 접근 방식은 여러 유사한 명령어를 그룹화하여 문맥 내 예제 세트를 형성함으로써 명령어 튜닝에 문맥 내 예제를 통합한다는 점에서 그들과 더욱 다릅니다. 우리는 멀티모달 모델에서 문맥 내 명령어 튜닝 패러다임을 제안하고 해당 멀티모달 문맥 내 명령어 튜닝 데이터 세트를 구축한 최초의 기업입니다.3
--- METHOD ---
이 섹션에서는 Sec. 3.1에서 MIMIC-IT 데이터 세트의 세부 사항, Sec. 3.2에서 Otter의 훈련 세부 사항, Sec. 3.3에서 Hugging Fance 생태계와의 통합을 소개합니다. 3.1 OpenFlamingo 프레임워크의 다중 모달 문맥 내 명령어 튜닝은 인터리브된 다중 모달 MMC4 데이터 세트를 활용하여 몇 번의 샷, 문맥 내 학습 기능을 구현합니다. MMC4 데이터 세트는 개별 HTML 파일에서 파생된 이미지-텍스트 쌍으로 구성되며, 그림 2(a)에서 볼 수 있듯이 서로 다른 쌍 간에 중요한 문맥적 관계가 있습니다. MMC4 훈련 데이터 샘플에는 (i) 일반적으로 이미지를 설명하는 텍스트인 쿼리된 이미지-텍스트 쌍과 (ii) 동일한 HTML 파일의 나머지 이미지-텍스트 쌍을 포함하는 컨텍스트가 포함됩니다. OpenFlamingo의 주요 학습 목표는 쿼리된 이미지-텍스트 쌍에 대한 텍스트를 생성하는 것이며, 컨텍스트 내 예제에 따라 쿼리 텍스트를 생성하는 패러다임은 추론 단계 동안 OpenFlamingo의 컨텍스트 내 학습 용량을 보장합니다. 당사의 MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트는 컨텍스트 내 학습 용량을 보존하는 동시에 OpenFlamingo의 명령어 이해 기능을 증강하는 것을 목표로 합니다. OpenFlamingo의 명령어 추종 잠재력을 최대한 발휘하기 위해 시각 언어 작업의 데이터를 이미지-명령-답변 3중항으로 컴파일합니다. 동시에 OpenFlamingo의 컨텍스트 내 학습 용량을 유지하기 위해 VQAv2[3]의 시각적 질문-답변 데이터 샘플과 같이 종종 상관 관계가 없는 각 3중항의 컨텍스트 내 예제를 검색합니다. 구체적으로, 각 MIMIC-IT 데이터 샘플은 (i) 이미지에 맞게 조정된 명령-답변이 포함된 쿼리된 이미지-명령-답변 3중항과 (ii) 컨텍스트로 구성됩니다. 컨텍스트에는 쿼리된 3중항과 컨텍스트적으로 상관관계가 있는 일련의 이미지-명령-답변 3중항이 포함되어 있으며, MMC4 데이터 세트에서 발견된 쿼리된 이미지-텍스트 쌍과 컨텍스트 간의 관계를 에뮬레이션합니다. MIMIC-IT의 학습 목표는 쿼리된 이미지-명령-답변 3중항 내에서 답을 생성하는 것입니다. 이미지-명령-답변 3중항은 (i) 시각적 질문-답변 데이터 세트, 즉 VQAv2 [3] 및 GQA [13], (ii) LLaVA [17]와 같은 시각적 명령 데이터 세트, (iii) PVSG 저장소의 진행 중인 고품질 파노라마 비디오 장면 그래프 데이터 세트에서 파생됩니다. 각 비디오에 대해 LLaVA 데이터 세트를 참조로 사용하여 명령-추종 주석을 위한 4-8개 프레임을 선택합니다. 그림 2(b)에 나와 있듯이, 우리는 각 이미지-명령-답변 3중 구조에 대한 맥락을 구성하기 위해 세 가지 휴리스틱을 개발했습니다.3.2 학습 세부 정보 우리의 접근 방식은 OpenFlamingo 학습 패러다임을 채택하여 Otter 모델을 학습합니다.사전 학습된 OpenFlamingo 모델은 LLaMA-7B[33] 언어 인코더와 CLIP ViT-L/14[24] 비전 인코더로 구성됩니다.과잉 맞춤을 방지하고 사전 학습된 지식을 활용하기 위해 두 인코더를 모두 동결하고 Perceiver 리샘플러 모듈, 언어 인코더에 삽입된 교차 주의 계층 및 언어 인코더의 입출력 임베딩만 미세 조정합니다.이를 통해 Otter 모델에 대해 약 10억 개의 학습 가능한 매개변수가 생성됩니다.모델을 최적화하기 위해 시작 학습 속도가 10-5이고 배치 크기가 4인 AdamW 최적화 프로그램[18]을 사용합니다.코사인 어닐링 스케줄러를 사용하여 학습 속도를 예약하여 6개 에포크 동안 Otter를 학습합니다. 또한 폭발하는 그래디언트를 방지하기 위해 임계값 1.0의 그래디언트 클리핑을 사용합니다. 훈련하는 동안 특정 형식을 따라 훈련 데이터를 준비합니다. 형식에는 이미지, 사용자 지침, &quot;GPT&quot; 생성 답변 1, [endof chunk] 토큰이라는 특수 토큰의 조합이 포함됩니다. 훈련 데이터 형식은 다음과 같습니다.<context> [이미지] 사용자:<instruction> GPT: [답변]<answer> . [endof chunk] 여기서 [image], [answer], [endof chunk] 토큰은 고유하며 특정 목적을 갖습니다. 우리는 모델의 지시 따르기와 대화 일반화를 개선하기 위해 모델을 훈련하기 위해 이러한 챗봇과 같은 형식을 설계합니다. [image] 및 [endof chunk] 토큰은 원래 OpenFlamingo 훈련 패러다임에서 가져온 반면 [answer] 토큰은 Otter 훈련에서 우리가 새롭게 도입한 것입니다. [answer] 토큰은 답변과 지시를 분리하므로 훈련 중에 [answer] 토큰 뒤의 모든 토큰을 마스크하고 모델의 예측 목표로 설정합니다. 우리는 교차 엔트로피 손실을 사용하여 모델을 훈련합니다. ¹사용자-지원 대화를 지원하기 위해, 어휘에서 특정 의미적 의미가 없기 때문에 &quot;GPT&quot;를 역할 레이블로 채택했습니다.제목: 호두와 블루치즈를 넣은 버섯 제목: HONDA: 안에서 문을 (보이지 않게) 보기 텍스트: &quot;이 호두와 블루치즈를 넣은 버섯 레시피 Fisher Natural은 Fisher Nuts에서 후원합니다.&quot;, 텍스트: &quot;마스터 잠금 스위치를 사용하여 운전석 도어와 테일게이트를 잠그거나 잠금 해제하면 다른 모든 도어가 동시에 잠기거나 잠금 해제됩니다.&quot; 잠금 - 잠금 탭 잠금 해제 텍스트: &quot;버섯을 채우는 아이디어는 무궁무진하여, 가지고 놀 수 있는 조합이 너무나 많은데, 개인적으로 가장 좋아하는 것 중 몇 가지는 지중해식 버섯과 시금치 텍스트: &quot;운전석 문의 잠금 탭을 사용하여 문을 잠그면 다른 모든 문과 테일게이트가 동시에 잠깁니다.&quot;, 구운 소나무 열매 버섯입니다.&quot;, (a) 멀티모달 C4(MMC4) 잠금 해제/ - 잠금 마스터 도어 잠금 스위치 동일한 지침, 다른 이미지의 문맥 예 지침: 스노보더가 이런 트릭을 수행하는 데 필요한 기술은 무엇입니까? 지침: 이 선수가 경기에서 가져야 하는 기술은 무엇이며, 그 이유는 무엇입니까? 쿼리 지침: 이미지에 따르면, 선수가 이 스포츠에서 탁월해지려면 어떤 기술이 필요합니까? 동일한 이미지, 다른 지침의 문맥 예 지침: 야구 선수가 손에 무엇을 들고 있습니까? 지침: 야구 선수의 유니폼은 무슨 색입니까? 쿼리 지침: 야구 선수가 어느 팀에 속합니까? ANGEL ANGEL ANGEL ANGEL 정답: 정답: 트릭을 수행하려면 ... 정답: 테니스 선수가 정답: 관련 위험을 최소화하려면 ...경기에서 승리해야 합니다. 성과를 개선하고 ...팀을 이겨야 합니다. 야구 선수가 손에 야구 배트를 들고 있습니다. 연속적인 이미지, 다른 지침 정답: 야구 선수의 유니폼은 회색입니다. 정답: 야구 선수는 엔젤스 야구 팀에 속합니다. 문맥별 예 설명: 이 그림에서 주로 일어나는 일은 무엇입니까? 설명: 공격하던 빨간색 옷을 입은 선수가 왜 땅에 쓰러졌습니까? 설명: 빨간색 유니폼을 입은 남자가 왜 땅에서 일어나려고 합니까? 설명: 비디오의 유머러스한 순간에 대한 설명은 무엇입니까? 쿼리 설명: 전체 비디오가 왜 유머러스합니까? FALKE 정답: ...공격하고 기회를 찾아 답: 슛을 했기 때문입니다. 페널티를 만들려고 땅을 쳤습니다. 정답: ...공이 들어갔고 페널티를 만들려고 가장할 필요가 없었습니다. 정답: 경기장에 있는 한 남자가 슛 후 쓰러지고 ... \팀원들과 함께. 답변: 재밌는 건 그의 부상이 순식간에 아물어가는 게 재밌다는 거예요.(b) 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 그림 2: MMC4 및 MIMIC-IT의 예시 데이터 형식에 대한 설명.(a) OpenFlamingo에서 사용하는 MMC4 데이터 세트의 데이터 형식에 대한 설명.(b) 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 데이터 세트를 빌드하는 세 가지 휴리스틱.3.3 Hugging Face와의 통합 Otter를 Hugging Face Transformers[34]에 통합하고 Hugging Face Accelerator²를 사용하여 학습시켰습니다.이를 통해 모델 가중치를 다른 GPU 장치에 자동으로 매핑하고 오버플로된 가중치를 CPU 또는 디스크로 오프로드할 수 있습니다.또한 학습 중에 bf16 혼합 정밀도를 사용합니다.전체 최적화를 통해 각각 24GB 메모리가 있는 4×RTX-3090 GPU에서 모델을 학습할 수 있습니다. 한편, Otter가 Hugging Face Transformers에 통합되었기 때문에 5줄 미만의 코드로 재사용할 수 있어 연구자들이 각자의 훈련 및 추론 파이프라인에 통합하기가 훨씬 쉬워졌습니다(원래 OpenFlamingo 구현과 비교).또한 Fully Sharded Data Parallel(FSDP) 및 DeepSpeed를 지원하여 훈련 효율성을 높이고 메모리 소비를 줄입니다.향후 연구와 편의성을 위해 원래 OpenFlamingo-9B 체크포인트를 Hugging Face Model 형식으로 변환하는 스크립트도 제공합니다.변환된 체크포인트와 Otter 모델은 각각 Hugging Face 모델 허브인 luodian/openflamingo-9b-hf 및 luodian/otter-9b-hf에 업로드되어 제공됩니다.데모 이 섹션에서는 Otter의 두 가지 유형의 데모에 대한 몇 가지 예를 보여줍니다.(1) Sec. 4.1의 지침을 따르는 기능, (2) Sec. 4.1에서 제공된 컨텍스트 내 예제에 따라 새 지침을 실행하는 방법을 학습하는 기능입니다. 4.2. OpenFlamingo와 비교했을 때, 이러한 결과는 컨텍스트 내 명령어 튜닝과 Otter의 개선의 중요성을 보여줍니다. 4.1 사용자 명령어 따르기 3.1절에서 우리는 시각적 명령어 쌍에서 Otter를 미세 조정하여 강력한 명령어 팔로워로 변환하는 방법을 논의했습니다. 우리의 결과는
--- EXPERIMENT ---
그림 3에서 Otter가 이미지에 대한 보다 자세한 설명을 제공하고 사용자 지시를 보다 정확하게 따를 수 있음을 관찰할 수 있습니다. Otter의 이러한 특징은 강력한 언어 디코더의 일반화 능력과 MIMIC-IT 데이터 세트에 있는 다양한 지시를 활용하는 모델과 데이터의 공동 설계에 기인합니다. 시각적 지시 쌍을 미세 조정함으로써 Otter는 인간 언어의 뉘앙스를 학습하고 시각적 입력에 정확하게 적용할 수 있습니다. Otter의 추론 기능을 평가하기 위해 상황에 대한 더 깊은 이해와 보다 진보된 상식적 추론이 필요한 복잡한 시나리오를 처리하는 능력을 테스트하는 일련의 실험을 설계했습니다. 그림 4에서 OpenFlamingo가 이미지에서 중요한 단서를 식별하지 못하는 반면 Otter는 장면에 대한 더 깊은 이해를 보여주고 관련 상식적 지식을 적용하여 주어진 질문에 답할 수 있는 예를 제시합니다. 이 특정 예에서 Otter는 교통 신호 주변의 혼란을 올바르게 해석하는 동시에 사고, 교통 체증, 교통 흐름 방해와 같은 관련 맥락적 요인을 고려할 수 있습니다. 이 수준의 추론 능력은 표준 이미지 캡션 모델에는 존재하지 않으며 컴퓨터 비전 및 자연어 처리 분야에서 상당한 진전을 나타냅니다. 4.2 다중 모달 맥락 내 학습 Otter는 Flamingo [1] 및 OpenFlamingo [4]와 유사한 패턴으로 다중 모달 맥락 내 학습을 지원하도록 설계되었으며, 여기에는 캡션이나 질의-답변 쌍에 해당하는 이미지와 같은 해당 미디어에 언어 모델을 조건화하는 것이 포함됩니다. 그림 5에서 Otter가 시각적 질의-답변 작업을 수행하는 기능을 보여줍니다. 여기서 사용자는 지시-답변 쌍과 이미지를 맥락적 예로 제공하여 밈이나 스포츠의 위험을 설명하는 것과 같은 하위 작업을 모델에 촉구합니다. 이러한 시나리오에서 사용자 입력이 종종 짧고 제한적이라는 사실에도 불구하고 Otter는 보다 포괄적인 답변을 제공할 수 있습니다. 이미지에 포함된 정보와 다른 맥락적 예를 활용하여 Otter는 질문의 근본적인 의미와 맥락을 더 잘 이해하여 더 정확하고 유익한 답변을 얻을 수 있습니다. 2Hugging Face Accelerator 지침: 이 이미지의 주요 특징을 설명해 주시겠습니까? OpenFlamingo: 남자가 연을 날리고 있습니다. Otter: 이미지는 하늘을 날고 있는 연 앞에 서 있는 사람을 보여줍니다. 그 사람은 검은색 셔츠를 입고 연의 왼쪽에 위치하고 있습니다. 연은 매우 크고 그 사람의 머리 위로 보입니다. 장면은 야외에서 보이는데, 그 사람은 아마도 연을 던질 준비를 하고 있거나 연이 바람에 날리는 순간을 즐기고 있을 것입니다. 배경에는 두 명의 다른 사람이 있는데, 한 명은 오른쪽에 있고 다른 한 명은 가장 오른쪽에 부분적으로 보입니다. 이 야외 환경에 여러 사람이 있는 것은 그곳이 모임이거나 연 날리기와 여가 활동을 위한 인기 있는 장소일 수 있음을 시사합니다. 지침: 주어진 이미지에 대한 자세한 설명을 작성하세요? OpenFlamingo: 베이컨과 메이플 시럽을 곁들인 팬케이크. 수달: 이 이미지에는 다양한 품목이 놓인 나무 테이블이 있습니다. 반으로 잘린 샌드위치와 접시 근처에 놓인 시리얼 한 그릇이 있습니다. 또한 커피와 같은 음료가 들어 있는 컵이 있습니다. 테이블에는 &quot;살아있는 박테리아&quot; 용기와 머그잔이 있어 아늑하고 편안한 분위기를 보여줍니다. 이 장면은 샌드위치, 시리얼, 음료가 테이블의 주요 품목인 아침 식사 또는 간식 시간 구성의 일부일 수 있습니다. 그림 3: OpenFlamingo와 Otter의 이미지 캡션 결과 비교. Otter는 이미지에 대한 보다 자세하고 정확한 설명을 제공합니다. 지침: 운전자가 이 교차로의 교통 신호에 혼란스러워하면 어떻게 될까요? OpenFlamingo: 비에 갇힐 수 있습니다. Otter: 운전자가 교차로의 교통 신호에 혼란스러워하면 다음과 같은 다양한 잠재적 시나리오가 발생할 수 있습니다. 1. 빨간불: 빨간불 신호가 켜져 있고 차가 움직이고 있습니다. 이 경우 운전자는 운전을 계속하여 적신호 위반 사고를 일으킬 가능성이 있으며, 이는 보행자와 다른 차량에게 위험할 수 있습니다.2. 녹색 신호: 차량이 이미 움직이기 시작한 상태에서 녹색 신호가 나타나면 차량이 계속 움직여 운전자가 녹색 신호에서 멈추지 않아 다른 차량이나 횡단보도와 충돌할 위험이 있는 녹색 신호 위반 사고로 이어질 수 있습니다.3. 신호에 대한 혼란: 운전자가 신호등에 대해 확신이 없으면 주저하거나 예측할 수 없는 기동을 할 수 있으며, 이는 사고뿐만 아니라 교통 체증과 교통 흐름의 전반적인 방해로 이어질 수 있습니다.그림 4: OpenFlamingo와 Otter의 상황 이해 결과 비교.Otter는 현장에 대한 더 깊은 이해를 보여주고 관련 상식적 지식을 적용하여 주어진 질문에 답할 수 있습니다.사용자가 제공한 예제 Otter 반응 지침: 이 밈은 보통 무슨 용도로 사용하나요?답변: 사람들에게 두 번 생각하고 어리석은 짓을 하지 말라고 요청하는 것사용자가 제공한 예제 지침: 이 밈은 보통 무슨 용도로 사용하나요? 답변: 사용자의 슬픔이나 감동을 표현합니다.설명: 이 밈은 보통 무엇에 사용하나요? 수달: 이 밈은 보통 슬픈 표정의 아기를 메인 이미지로 사용하고, 텍스트는 사람들이 아기가 되는 것을 멈추고 강하고 멋진 사람이 되라고 격려합니다. 이미지와 텍스트는 귀엽고 연약한 아기와 강하고 터프한 사람 사이에 장난기 있고 유머러스한 대조를 만들어냅니다. 수달 반응 영상 설명: 이 스포츠의 위험은 무엇인가요? 답변: 선수가 테니스 공에 맞을 수 있습니다.설명: 이 스포츠의 위험은 무엇인가요? 답변: 야구공을 쫓는 동안 선수가 실수로 다른 선수와 충돌할 수 있습니다.설명: 이 스포츠의 위험은 무엇인가요? 수달: 이 스포츠는 다른 선수들이 풀밭에 서 있는 동안 선수들이 달리고 공을 잡으려고 하는 것으로, 충돌이나 사고로 이어질 수 있습니다. 이러한 충돌은 관련 선수에게 부상을 입힐 수 있습니다. 선수들은 주변 환경을 인식하고 서로 안전한 거리를 유지하여 경기 중 사고 위험을 최소화하는 것이 필수적입니다. 그림 5: Otter의 맥락 내 학습 사례. 지시-답변 쌍의 두 가지 사례와 이미지가 주어지면 Otter는 더 포괄적인 답변을 제공할 수 있습니다. 5
--- CONCLUSION ---
이 연구에서 우리는 명령어 튜닝을 사용한 다중 모달 문맥 내 학습 기반 모델인 Otter를 제안한다. MIMIC-IT 데이터 세트에 대한 부분적 미세 조정을 통해 Otter가 OpenFlamingo를 강력한 문맥 내 학습 능력을 갖춘 제로샷 시각적 명령어 모델로 변환할 수 있음을 관찰했다. 이미지와 비디오의 풍부한 명령어의 도움으로 Otter는 일반화하여 더 나은 명령어 따르기 및 상황 이해 성능을 달성한다. 5.1 한계 언어 환각. Otter는 LLAMA에 의존하는 OpenFlamingo를 기반으로 구축되었으므로 LLAMA의 환각 문제가 Otter에 계승되었다. 또한 현재 Otter 모델은 이미지와 관련이 없는 언어를 환각할 수 있다. 이 문제는 훈련 데이터에 부정적인 예를 도입하면 해결될 수 있다. 5.2 향후 지원 향후에는 보다 효율적인 훈련 스키마(예: LoRA[12]와 같은 매개변수 효율적 미세 조정)와 더 많은 모달리티(예: 3D 비전)를 통합하는 것을 탐색할 계획이다. 감사의 말. 우리는 Chunyuan Li와 Jack Hessel에게 조언과 지원에 감사드리고, 오픈소스 커뮤니티에 크게 기여한 OpenFlamingo 팀에도 감사드립니다.참고문헌 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 2, 3,[2] Alibaba. Tongyi qianwen. 2023.[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh. Vqa: 시각적 질의응답. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 2425-2433페이지, 2015.[4] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt. Openflamingo, 2023년 3월. 2, 3,[5] Baidu. Ernie bot: 지식 통합을 통한 향상된 표현. 2023.[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020.[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut. 개념적 12m: 긴 꼬리 시각적 개념을 인식하기 위한 웹 스케일 이미지-텍스트 사전 학습 추진. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3558-3568페이지, 2021.[8] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu. 언어 모델은 시각적 추론 조정자입니다. ICLR 2023 기초 모델의 수학적 및 경험적 이해에 대한 워크숍, 2023.[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇, 2023년 3월. 1,[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: 구체화된 멀티모달 언어 모델. arXiv 사전 인쇄본 arXiv:2303.03378, 2023.[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh. vqa에서 v를 중요하게 만들기: 시각적 질의 응답에서 이미지 이해의 역할 강화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6904-6913페이지, 2017. 2,[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021.[13] Drew A Hudson 및 Christopher D Manning. Gqa: 실제 시각적 추론 및 구성적 질의 응답을 위한 새로운 데이터 세트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6700-6709페이지, 2019.[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image comments. International journal of computer vision, 123:32-73, 2017.[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. 1,[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트의 일반 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014. 2,[17] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee. 시각 지침 튜닝. arXiv 사전 인쇄본 arXiv:2304.08485, 2023. 1, 3,[18] Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. arXiv 사전 인쇄본 arXiv:1711.05101, 2017.[19] OpenAI. Gpt-4 기술 보고서. 2023. 1,[20] OpenAI. Chatgpt 소개. 2023. 1,[21] Vicente Ordonez, Girish Kulkarni, Tamara Berg. Im2text: 수백만 개의 캡션이 있는 사진을 사용하여 이미지 설명. 신경 정보 처리 시스템의 발전, 24, 2011.[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지침을 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35:27730-27744, 2022.[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독을 통해 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021.[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독을 통해 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021.[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 1(8):9, 2019.[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114, 2021.[27] SenseTime. Sense nova. 2023.[28] Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. 56회 연례 총회 의사록(제1권: 장문 논문), 2556-2565쪽, 2018년.[29] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. Hugginggpt: huggingface에서 chatgpt 및 그 친구들을 사용하여 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580, 2023년.[30] Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi. 시각적 추론을 위한 자연어 코퍼스. 55회 연례 총회 의사록(제2권: 단편 논문), 217-223쪽, 2017년.[31] 디닥 수리스(Dídac Surís), 사치트 메논(Sachit Menon), 칼 본드릭(Carl Vondrick). Vipergpt: 추론을 위한 Python 실행을 통한 시각적 추론. arXiv 사전 인쇄 arXiv:2303.08128, 2023.[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang 및 Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델입니다. https://github.com/tatsu-lab/stanford_alpaca, 2023.[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. 1, 3,[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush. Transformers: State-of-the-the-heart 자연어 처리. 2020 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록: 시스템 시연, 38-45페이지, 온라인, 2020년 10월. 계산 언어학 협회. 2,[35] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang 및 Nan Duan. 시각적 chatgpt: 시각적 기초 모델을 사용하여 말하고, 그리고, 편집합니다. arXiv 사전 인쇄 arXiv:2303.04671, 2023.[36] Zhiyang Xu, Ying Shen, Lifu Huang. Multiinstruct: 명령어 튜닝을 통해 다중 모드 제로샷 학습을 개선합니다. arXiv 사전 인쇄 arXiv:2212.10773, 2022.[37] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 및 Lijuan Wang. Mm-react: 다중 모달 추론 및 작업을 위해 chatgpt에 메시지를 표시합니다. arXiv 사전 인쇄 arXiv:2303.11381, 2023.[38] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023. 1,[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델을 사용하여 비전 언어 이해를 향상합니다. arXiv 사전 인쇄본 arXiv:2304.10592, 2023. 1,[40] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi. 멀티모달 C4: 텍스트가 섞인 개방형 10억 규모 이미지 코퍼스. arXiv 사전 인쇄본 arXiv:2304.06939, 2023.[41] Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei. Visual7w: 이미지에서 근거 있는 질문 답변. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4995-5004페이지, 2016.[42] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan 등. 픽셀, 이미지, 언어에 대한 일반화된 디코딩. arXiv 사전 인쇄 arXiv:2212.11270, 2022.
