--- ABSTRACT ---
이 논문에서는 언어 모델이 지시를 따르는 능력을 감지하는 지표인 지시 따르기 점수(IFS)를 소개합니다. 이 지표는 두 가지 목적을 가지고 있습니다. 첫째, IFS는 기본 모델과 지시 모델을 구별하는 데 사용할 수 있습니다. 공개적으로 사용 가능한 기본 모델과 지시 모델을 벤치마킹하고, 부분 문장과 전체 문장에 대한 잘 구성된 응답의 비율이 이 두 모델 클래스 간의 효과적인 척도가 될 수 있음을 보여줍니다. 둘째, 이 지표는 지시 튜닝을 위한 조기 중단 기준으로 사용할 수 있습니다. 7B 및 13B LLAMA 모델의 지도 미세 조정(SFT)에 대한 IFS를 계산하여 모델이 훈련 프로세스 초기에 지시를 따르는 법을 배우고, 추가 미세 조정으로 인해 기본 모델 의미 체계가 변경될 수 있음을 보여줍니다. 의미 체계 변경의 한 예로, 보조 지표 ObjecQA에 의해 정의된 모델 예측의 객관성을 보여줍니다. 이 특정 사례에서 IFS가 정점에 도달할 때 의미 체계 변경이 가장 가파른 것을 보여줍니다. 우리는 명령어 튜닝을 IFS와 의미적 요소로 분해하는 것이 명령어 튜닝을 보다 잘 제어하는 데 있어 새로운 추세를 시작하고 기초 모델을 쿼리하는 최소 명령어 인터페이스를 설계할 수 있는 가능성을 열어주기를 바랍니다.
--- INTRODUCTION ---
지시 데이터에 따라 미세 조정된 대규모 언어 모델(LLM)은 대화 에이전트처럼 동작할 수 있습니다(Alpaca: Taori et al. 2023, Self-Instruct: Wang et al. 2023). 채팅 모델의 레시피는 잘 정의되어 있습니다. 지시 조정을 수행해야 하며, 이는 지시 및 응답 튜플에 대한 LLM의 감독 미세 조정(SFT)을 의미합니다(Longpre et al. 2023). 오픈소스 데이터 세트는 1,000개 예제(Zhou et al. 2023)에서 800,000개 이상의 예제(Anand et al. 2023)에 이르기까지 품질과 양이 다양합니다. 또한 LLaMA(Touvron et al. 2023), OPT(Zhang et al. 2022), GPT-Neo(Gao et al. 2020), Palmyra(Writer 2023) 등 12개 이상의 오픈소스 기반 LLM이 있습니다. 사전 인쇄. 검토 중. 이는 다양한 조합으로 이어져 뚜렷한 지시 모델을 만들어냅니다. Gudibande et al. 2023에서 도입한 &quot;모방 모델&quot; 개념의 렌즈를 통해 지시 튜닝 시도를 볼 수 있습니다. 즉, ChatGPT(OpenAI 2022), Bard(Pichai 2023), Claude(AnthropicAI 2023)와 같은 폐쇄된(그리고 아마도 훨씬 더 큰) 독점 모델을 증류하려는 노력입니다. 증류 프로세스가 기본 모델에 미치는 정성적 영향에 대해서는 알려진 바가 거의 없습니다(Hinton, Vinyals, and Dean 2015). 모방 성공은 지식(예: HELM Liang et al. 2022), 기술(예: Natural Questions Kwiatkowski et al. 2019) 또는 인간의 선호도에 따른 수동 검사(Zhou et al. 2023) 측면에서 측정됩니다. 응답의 스타일과 형식에 대한 지표를 왜곡할 수 있는 수동 검사가 전반적으로 좋은 지표인지에 대한 합의는 없습니다(Gudibande et al. 2023). 지시 모델을 보다 견고하게 평가하려는 비교적 최근의 시도는 Huggingface Leaderboard(Huggingface 2023b)로, Eleuther AI Language Model Evaluation Harness(Gao et al. 2021)의 네 가지 주요 벤치마크에 대해 모델을 평가합니다. &quot;포맷 주입&quot; 및 &quot;지식 주입&quot; 단계 식별을 포함하여 교육 단계를 보다 잘 제어하기 위한 일반적인 휴리스틱. 이 논문은 다음과 같이 구성되어 있습니다. 섹션 2에서는 모델이 지시 모델로 간주되기 위한 필요한 조건과 IFS를 위한 데이터 준비에 대해 논의합니다. 응답 톤 분류기 교육은 섹션 4에 설명되어 있습니다. 섹션 5에서는 지시 모델에 대한 결과를 제시하고 지시 톤과 의미 변화 측면에서 기본 바닐라 모델과 비교합니다. 이 연구는 6장에서 제안된 결론과 미래 방향으로 끝납니다. 절제 연구는 훈련 데이터의 다양성과 품질이 모두 모델 성능에 중요한 역할을 한다는 것을 보여주었습니다(Chen et al. 2023, Zhou et al. 2023). 낮은 훈련 데이터 지침 튜닝(LTD 튜닝)은 작업별 모델이 원래 데이터의 0.5% 미만으로 훈련될 때 2%의 성능을 얻을 수 있음을 시사합니다. 게다가 장기간 지침 튜닝은 기초적인 모델 지식을 감소시킬 수 있으며(Gudibande et al. 2023) 2 배경 및
--- RELATED WORK ---
지시 튜닝의 다운스트림 작업(Kumar et al. 2022). 이 연구에서는 지시 모델에 필요한(충분하지는 않지만) 조건을 정의하여 지시 모델 연구의 기반을 마련하고자 합니다. 사고 실험을 수행해 보겠습니다. 모든 모델을 폐쇄형 API(최근의 블랙박스와 동일) 뒤에 두겠습니다. 모델이 지시 튜닝되었는가 아닌가? LTD 튜닝을 위한 바닐라 모델과 지시 모델의 지식 벤치마크는 비슷할 수 있습니다. 기술 테스트는 모델 크기에 크게 좌우되며, 이는 알 수 없습니다. 수수께끼를 푸는 가장 간단한 방법은 ... 모델과 채팅하고 응답의 톤을 판단하는 것입니다. 바닐라 모델의 경우 다음 예측 단어 시도를 예상하는 반면 지시 모델의 경우 지시를 따를 것으로 예상합니다. 이러한 톤 차이를 포착하는 지표인 지시 따르기 점수(IFS)를 소개합니다. 이 문제를 &quot;톤 정렬&quot; 문제라고 합니다. IFS는 사전 정의된 명령어 집합에 대한 &quot;답변 유사&quot; 응답 대 &quot;연속 유사&quot; 응답의 비율로 정의되며, 여기서 응답의 클래스는 이진 분류기에 의해 결정됩니다. 우리는 공개적으로 사용 가능한 기본 및 지시 모델을 벤치마킹하고, 잘 형식화된 응답 대 부분 및 전체 문장의 비율이 바닐라 및 지시를 따르는 모델 사이에서 효과적인 척도가 될 수 있음을 보여줍니다. 또한, 우리는 최소 지시 튜닝에 대한 중지 기준을 찾기 위해 7B 및 13B LLAMA 모델에 대한 SFT에 대한 IFS를 계산합니다. 응답 톤에 대한 학습 곡선과 의미 및 도메인별 지식 습득을 비교하기 위해 ObjecQA라는 보충 메트릭을 제안합니다. 이 보조 메트릭은 데이터 세트 내에서 이 신호를 식별할 수 있으므로 모델 예측의 객관성을 정량화합니다. 이 기능 선택은 임의적이지만, 우리는 가능한 더 많은 것을 발견하는 것을 목표로 합니다. 응답 톤 정렬 문제는 더 광범위한 의도 정렬 주제의 일부입니다. 원칙적으로 LLM은 언어 모델링 목표(예: 학습 문서의 다음 토큰 예측)가 다음 지시 목표와 다르기 때문에 사용자의 의도와 일치하지 않습니다. 두 가지 목표를 모두 맞추는 성공적인 방법 중 하나는 QA가 포함된 문서를 완성한 것처럼 응답이 보이는 기술(Brown et al. 2020, Radford et al. 2018)입니다. 또 다른 방법은 지시와 응답의 튜플에 대한 바닐라 모델을 지시하고 조정하여 학습의 일부로 모델이 올바른 형식의 응답을 모방하는 기술을 습득하도록 하는 것입니다(Alpaca: Taori et al. 2023, Self-Instruct: Wang et al. 2023). InstructGPT 논문(Ouyang et al. 2022)에서 &quot;올바른 지시 작업을 따르지 못함&quot;이라는 기준은 PPO 알고리즘(Schulman et al. 2017)에서 SFT 모델을 미세 조정하여 보상을 극대화하는 데 사용되는 보상 모델(RM)에 대한 인간 평가 메타데이터 목록에 포함되었습니다. 우리는 각 전략을 지식 및 언어 이해 기반 메트릭(예: MMLU(Hendrycks et al. 2021)을 사용하는 대신 스타일 서식 문제로 평가하여 톤 구성 요소를 분리하고 이해하는 것을 목표로 합니다. 3 지시 따르기 지수 3.1 동기 지시 따르기 모델은 대화형 에이전트처럼 직관적으로 작동합니다. 즉, 항상 입력이 지시라고 가정하고 이해에 따라 답변을 제공하거나 후속 질문을 하려고 합니다. 반면, 지시를 따르지 않는 모델은 다음 토큰을 예측하고 선택적으로 답변을 제공하거나 다음 지시를 계속합니다. 불완전한 문장 조각인 지시의 경우 두 모델 클래스 간의 구별이 더 명확해집니다. 지시를 따르는 모델은 결코 지시를 완료하려고 하지 않습니다. 응답의 질은 의도적으로 이 분류의 범위를 벗어난다는 점을 강조하는 것이 중요합니다. 따라서 위의 기준은 채팅 모델에 필요하지만 충분한 조건은 아닙니다. 이 논문에서는 지시에 따른 점수(IFS)를 소개합니다. 이는 미리 정의된 일련의 지시에 대한 &quot;답변과 같은&quot; 응답과 &quot;지속과 같은&quot; 응답의 비율로 정의됩니다. 응답의 클래스는 이진 분류기(이후 &quot;응답 톤 분류기&quot;라고 함)에 의해 결정됩니다. IFS를 위한 훈련 및 데이터 수집 프로세스는 다음 섹션에서 설명합니다. 이 논문에서는 &quot;대화 톤&quot;과 &quot;지시를 따르는 톤&quot;을 서로 바꿔 사용하는데, 이는 &quot;답변과 같은&quot; 응답 클래스를 의미합니다. 지시 모델을 얻기 위해 기본 모델을 미세 조정하는 프로세스를 &quot;지시 튜닝&quot;이라고 합니다. 3.2 데이터 세트 IFS의 데이터 세트는 원래 쌍(지시, 응답)으로 구성된 채팅 데이터 세트에서 파생되었습니다. 우리는 지침을 따르지 않는 모델에 대한 입력과 출력을 모델링해야 할 것입니다.데이터 생성의 주요 아이디어는 응답에 지침을 추가한 다음 그림 1에 표시된 것처럼 서로 다른 하위 구분을 두 개의 구문으로 고려하는 것입니다.다른 데이터 포인트 분할 R 프랑스의 수도는 무엇입니까?프랑스의 수도는 파리입니다.Ip Ic+ R 프랑스의 수도는 무엇입니까?프랑스의 수도는 파리입니다.Ip Ic 프랑스의 수도는 무엇입니까?프랑스의 수도는 파리입니다.지침 응답 그림 1: IFS 데이터 세트 생성.다른 분할은 I, R, Ip, Ic라는 조각을 정의합니다.tion I - 지침 Ip - 부분(조각화된) 지침• 추론 출력: Ic 지침의 연속 R-응답 사실, 이 4개 부분의 조합은 바닐라 및 채팅 모델에 대한 모든 가능한 입력 및 출력 쌍을 제공합니다.아래 표에서 부분을 다시 결합하고 모델이 채팅 모델처럼 응답하는지 여부에 따라 이진 점수를 부여하고 할당합니다.(I, R) 지침 I에 대한 응답 R은 대화형입니다. 모든 응답이 위의 형태와 유사한 모델은 지시를 따르는 것이므로 응답에는 레이블 1이 있습니다.(Ip,R) 부분 지시 Ip에 대한 응답 R도 대화적이지만 이 경우 모델은 추가 정보를 요청하는 것 외에는 어떤 대답도 제공할 만큼의 맥락이 없습니다. 이 응답도 1로 레이블이 지정됩니다.(Ip,Ic) 모델은 단편화된 지시(다음 단어 예측 작업 실행)를 완료합니다. 이 쌍은 대화처럼 보이지 않으므로 레이블은 0입니다.(I,Ic) 모델은 다음 지시(다시 다음 단어 예측 작업과 유사)를 생성하여 응답 레이블 0을 제공합니다.(Ip,Ic+R) 이 경우 모델은 지시를 완료한 다음 응답합니다(다음 단어 예측 작업도 실행). 저자는 사람들이 그러한 대화를 시도한다고 생각할 수 있지만, 우리는 지시 완료를 실패한 대화의 신호로 취급합니다. 레이블은 0입니다. 컷이 (지시, 응답)을 재생성하면 채팅 모델에 이상적인 입력과 출력을 얻습니다. 분할을 오른쪽이나 왼쪽으로 옮기면 불완전한 문장(단편화)을 얻을 수 있으며, (I, Ic+R)은 미완료 지시 또는 지시의 연속에 따른 응답을 나타냅니다. 요약하면 다음과 같습니다. • 추론 입력: 모델은 다른 지시를 생성한 다음 생성에 응답합니다. 대화가 실패하여 응답 레이블이 0입니다. 각 사례의 예는 표 1에 나와 있습니다. 사례 (I, R) (Ip, R) 예 I: 사람들에게 다리가 있다면? R: 사람들에게 다리가 40개 있다면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것입니다! Ip: 만약 R: 질문이 불완전한 것 같습니다. 질문을 더 잘 이해하고 답변할 수 있도록 더 많은 맥락이나 세부 정보를 제공해 주십시오. Ip: 만약 Ic: 사람들에게 다리가 40개 있다면? I: 사람들에게 다리가 있다면? 채팅? 응답 세트는 이진 분류기의 데이터를 생성하는 데 사용됩니다. 그림 2는 채팅 데이터가 분할되어 실험에 사용되는 방식을 보여줍니다. 깨끗한 텍스트의 소스로서, 우리는 OpenAssistant 채팅 데이터 세트(Köpf et al. 2023)를 활용했습니다. 대화의 맥락을 제어하기 위해, 우리는 각 대화에서 첫 번째 지시와 그에 해당하는 응답만 고려했습니다. 3.2.1 지시 데이터 세트 지시 데이터 세트에서 데이터 포인트는 OpenAssistant 데이터에서 가져온 지시로 구성되며, 수정되지 않은(I) 또는 단편화된(Ip) 지시입니다. 우리는 총 7340개의 예를 얻었으며, 약 50%가 단편과 완전한 0개의 문장으로 나뉘었습니다. 우리는 알고리즘이 단편화된 것으로 레이블이 지정된 완전한 문장을 생성할 가능성이 있음을 인식하고 있으며, 이 레이블을 기반으로 분할된 점수는 대략적인 추정치입니다. (Ip, Ic) (I, Ic)I: 사람들에게 눈이 있다면 어떨까요? (Ip, Ic + R) Ip: 만약 표 2는 전체 및 부분 지시의 예를 보여줍니다. (I, I + R) Ic+R: 사람들에게 다리가 있다면? 영어: 사람들이 다리가 40개라면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것이다!I: 사람들에게 다리가 있다면 어떨까?Ic+R: 사람들에게 눈이 3개 있다면 어떨까?사람들에게 눈이 3개라면, 선글라스는 유행하는 삼안경 스타일로 나올 것이고 &quot;나는 당신을 주시하고 있습니다&quot;는 완전히 새로운 수준의 감시가 될 것이다.표 1: 조각 I, R, Ip, Ic의 가능한 조합의 예.톤 점수는 모델이 지침을 따르는지(1) 따르지 않는지(0)를 나타낸다.지침 HTML 레이블의 차이점은 무엇인가 partial HTML과 JavaScript의 차이점은 무엇인가 full partial full Who wearsWho wears short shorts?표 2: 지침의 예와 해당 범주.3.2.2 응답 데이터 세트 응답 세트는 그림 1의 오른쪽, 즉 원래 응답 또는 오른쪽으로 이동된 응답을 나타낸다. 수집된 클래스는 다음과 같습니다.레이블 0 Ic, Ic+R 요약하면, 6가지 잠재적 조합 중에서 두 가지 지시 모델 케이스만 존재합니다.(Ip, R) 레이블 1: R 및 (I, R). 이 분류가 확립되면 이제 지시 세트와 해당 모델 응답을 만들 수 있습니다. 모든 완벽 및 이동된 컷에서 나온 쌍을 분할하고 두 개의 데이터 집합(모든 지시 및 모든 응답)을 만듭니다. 지시 세트는 프롬프팅 모델에 사용되는 데이터를 생성하는 데 사용되는 반면, 응답의 세분화된 분류를 삭제하고 &quot;답변 유사&quot;(레이블!) 또는 &quot;계속 유사&quot;(레이블 0)에만 할당합니다. 이러한 샘플은 나중에 이진 분류기를 훈련하는 데 사용됩니다. 표 3은 응답과 해당 레이블의 예를 보여줍니다. 응답 너무 빨리 날 수 있나요? 가장 빨리 나는 새는 송골매입니다. 채팅? 5개 결과5.1 기준 에이전트? 저는 FBI 요원이 아닙니다. 양파를 자르면 황산이라는 화학 물질이 방출됩니다.제임스 매디슨은 헌법과 권리장전의 주 저자였습니다.표 3: 응답 및 해당 범주의 예.4 이진 분류기와 지시 수행 점수 톤 응답 분류를 위한 이진 분류기는 Huggingface AutoTrain(Huggingface 2023a)을 사용하여 응답 세트에서 학습된 최상의 이진 분류기로 선택되었습니다.데이터 세트가 대략적으로 동일한 양의 음성 및 양성 샘플로 구성되었으므로 정확도를 비교 메트릭으로 선택했습니다.우승한 아키텍처는 BertForSequenceClassification이었고 최종 분류기 메트릭(AutoTrain에서 보고)은 표 4에 나와 있습니다.메트릭 값 정확도 0.정밀도 0.재현율 0.표 4: 검증 메트릭 지시 수행 점수(IFS)는 &quot;답변과 유사&quot;(레이블 1)로 분류된 모든 응답과 지시 데이터 세트를 프롬프트하여 얻은 모든 응답의 비율로 정의합니다. 완벽한 지시 조정 모델은 항상 대화 톤(즉, 지시가 부분적이든 아니든 모든 지시에 채팅 모델처럼 응답)을 유지해야 하므로 최대 IFS는 1입니다. 또한 두 가지 관련 메트릭인 IFS 부분 및 IFS 전체를 정의할 수 있으며, 이는 각각 모든 부분 및 전체 지시에 대한 &quot;답변과 같은&quot; 응답의 비율입니다. 다음 섹션에서는 IFS를 사용하여 바닐라 모델과 프롬프트 엔지니어링 및 SFT 프로세스로 달성한 응답 톤 변화를 평가합니다. IFS 메트릭을 사용하여 여러 공개적으로 사용 가능한 모델을 평가했습니다. 데이터 세트는 50% 미만의 단편화된 지시(알고리즘에서 생성된 거짓 양성 포함)로 구성되므로 추가 접미사 없이 프롬프트할 때 기본 모델이 이 수준 아래의 IFS를 얻을 것으로 예상했습니다. 표 5에 제시된 SFT 및 RLHF 모델의 점수는 예상 최대값이 약 0.8-0.9인 반면 기본 및 지시를 따르는 LLM 간의 가장 두드러진 차이는 IFS 부분과 IFS 전체 간의 상대적 차이입니다. 모델 IFS IFS 부분 IFS 전체 GPT-0.0.0.RedPajama-3B 0.0.0.LLaMa-7B 0.0.0.LLAMA-13B 0.0.0.LLAMA-33B 0.0.0.davinci Palmyra-x 0.0.0.0.0.0.Palmyra-base 0.0.0.Palmyra-large 0.0.0.text-davinci-003 0.0.0.GPT-3.5-turbo 0.0.0.0.0.0.Palmyra-instruct 0.0.0.GPT-표 5: 기준선: 선택된 공개적으로 사용 가능한 모델에 대한 명령어 추종 점수(IFS). 5.2 프롬프트 엔지니어링 매우 간단한
--- METHOD ---
LM이 지침을 따르도록 장려하기 위해 지침 주위에 추가 프롬프트 접미사 또는 래퍼를 추가하여 다음 토큰 예측 작업을 방해하고 응답을 생성할 수 있습니다.그림 3은 세 가지 버전의 프롬프트를 보여줍니다.A. 알파카 프롬프트 지침 튜닝 프롬프트 아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요.### 지침: {지침} ### 응답: {응답} B. 우리의 프롬프트 C. 프롬프트 없음 (지침) {지침} ### 응답: (응답) {응답} 그림 3: 지침 튜닝 프롬프트의 비교 그림.A. 알파카 프롬프트, 지침 주위의 래퍼, B. 알파카 접미사만, C. 프롬프트 없음, 기준선 1단계: 분류기 학습 및 LM 예측 채팅 데이터 IFS 2단계: 평가 학습 추론 이진 분류기 추론 LM 지침 응답 그림 2: IFS 학습 및 평가 파이프라인 표 6에 제시된 결과는 두 프롬프트의 변형이 모두 똑같이 효과적임을 보여줍니다. 기준선(C)과 비교하면 모든 모델에서 IFS가 0.5-0.6 범위에 있음을 알 수 있습니다. 대규모 언어 모델(LLM)의 경우 단일 프롬프트 변경으로 모델이 지침을 따르도록 효과적으로 장려하여 공개적으로 사용 가능한 여러 instruct 모델과 비슷한 성능 수준에 도달할 수 있습니다. 결과를 더욱 개선할 수 있는 n-shot 프롬프팅은 테스트하지 않았습니다. 2023년 Anand et al.에서 도입된 gpt4all v1.3-groovy를 instruct 데이터 세트로 사용했습니다. 문자 제한을 2k로 설정했습니다(512토큰 길이로 학습된 LLaMa 모델 사전 학습 목표와 유사). 이 필터링 프로세스를 통해 instruct 튜닝을 위한 약 410k개의 예를 얻었습니다. 모델은 수정된 Alpaca 프롬프트를 사용하여 훈련되었습니다. PROMPT_DICT = { 데이터 집합 IFS IFSpartial IFS 전체 LLaMa-7BA 0.0.0.LLaMa-7BB 0.0.0.} LLaMa-7Bc 0.0.0.LLAMA-13BA 0.0.0.LLAMA-13BB 0.0.0.LLAMA-13Bc 0.0.0.LLAMA-33BA 0.0.0.LLAMA-33BB 0.0.0.LLAMA-33Bc 0.0.0.&quot;prompt_input&quot;: (&quot;{지침}\n\n{ 입력}### 응답:&quot;), &quot;prompt_no_input&quot;: (&quot;{지침}### → 응답:&quot;), 이 수정은 접두사 프롬프트를 제거하는 동시에 지침과 선택적 입력을 통합합니다. 이 접근 방식은 채팅 모델의 사용자 인터페이스가 일반적으로 구현되는 방식, 즉 단일 대화 입력 상자와 일치합니다.전체 Alpaca 래퍼를 사용할 수 있지만 두 가지 프롬프트 기술 모두 비슷한 점수를 가져오므로 프롬프트 접미사가 있는 모델과 없는 모델을 대상으로 효율성을 고려하여 더 짧은 모델을 선택했습니다.5.3 지도 미세 조정 이 연구에서는 SFT의 기본 LLM으로 7B 및 13B LLAMA 모델을 선택했습니다.결과의 비교 가능성을 보장하기 위해 동일한 교육 절차와 평가를 따랐습니다.SFT의 결과는 그림 4(a)에 나와 있습니다.약 8k개의 예제(수평 점선으로 표시)를 확인한 후 모델의 명령 조정 기능이 수준 0.9-0.95에서 안정화되는 것을 알 수 있습니다.이 교육 단계를 &quot;형식 주입&quot; 단계라고 합니다. 참고로, 우리는 더 큰 모델이 비교적 더 빨리 0.9 IFS 수준에 도달할 수 있다는 것을 관찰했습니다(이것은 우리가 두 지점에서 추론할 수 있는 한도입니다).
--- EXPERIMENT ---
. 모든 모델을 폐쇄형 API(최근의 블랙박스와 동일) 뒤에 두겠습니다. 모델이 instructtuned인가요? LTD 튜닝을 위한 vanilla 및 instruct 모델에 대한 지식 벤치마크는 유사할 수 있습니다. 기술 테스트는 모델 크기에 크게 좌우되며, 이는 알 수 없습니다. 수수께끼를 푸는 가장 간단한 방법은 ... 모델과 채팅하고 응답의 톤을 판단하는 것입니다. vanilla 모델의 경우 다음 예측 단어 시도를 예상하는 반면 instruct 모델의 경우 지침을 따를 것으로 예상합니다. 이러한 톤 차이를 포착하는 메트릭인 Instruct Following Score(IFS)를 소개합니다. 이 문제를 &quot;톤 정렬&quot; 문제라고 합니다. IFS는 사전 정의된 일련의 지침에 대한 &quot;답변과 같은&quot; 응답과 &quot;계속과 같은&quot; 응답의 비율로 정의되며, 여기서 응답의 클래스는 이진 분류기에 의해 결정됩니다. 공개적으로 사용 가능한 기본 및 지시 모델을 벤치마킹하고, 잘 형식화된 응답과 부분 및 전체 문장의 비율이 vanilla 및 instruct Following 모델 간의 효과적인 척도가 될 수 있음을 보여줍니다. 또한 최소 지시 튜닝에 대한 중지 기준을 찾기 위해 7B 및 13B LLAMA 모델의 SFT에 대한 IFS를 계산합니다. 응답 톤에 대한 학습 곡선과 의미 및 도메인별 지식 습득을 비교하기 위해 ObjecQA라는 보충 메트릭을 제안합니다. 이 보조 메트릭은 데이터 세트 내에서 이 신호를 식별할 수 있으므로 모델 예측의 객관성을 정량화합니다. 이 기능 선택은 임의적이지만 가능한 더 많은 것을 발견하는 것을 목표로 합니다. 응답 톤 정렬 문제는 더 광범위한 의도 정렬 주제의 일부입니다. 원칙적으로 LLM은 언어 모델링 목표(예: 교육 문서의 다음 토큰 예측)가 다음 지시 목표와 다르기 때문에 사용자의 의도와 일치하지 않습니다. 두 가지 방법을 모두 사용하여 모델을 프롬프트하는 것입니다. 두 가지 객체를 정렬하는 성공적인 방법 중 하나는 응답이 QA가 포함된 문서를 완성한 것처럼 보이는 기술입니다(Brown et al. 2020, Radford et al. 2018). 또 다른 접근 방식은 바닐라 모델을 지시와 응답의 튜플에 지시하고 조정하여 학습의 일부로 모델이 올바른 형식 응답을 모방하는 기술을 습득하도록 하는 것입니다(Alpaca: Taori et al. 2023, Self-Instruct: Wang et al. 2023). InstructGPT 논문(Ouyang et al. 2022)에서 &quot;올바른 지시 작업을 따르지 못함&quot;이라는 기준은 SFT 모델을 미세 조정하여 보상을 극대화하기 위해 PPO 알고리즘(Schulman et al. 2017)에 사용된 보상 모델(RM)에 대한 인간 평가 메타데이터 목록에 포함되었습니다. 우리는 지식 및 언어 이해 기반 메트릭(예: MMLU(Hendrycks et al. 2021))을 사용하는 대신 각 전략을 스타일 서식 문제로 평가하여 톤 구성 요소를 분리하고 이해하는 것을 목표로 합니다. 3 지시 따르기 지수 3.1 동기 지시 따르기 모델은 직관적으로 대화 에이전트처럼 동작합니다. 즉, 항상 입력이 지시라고 가정하고 이해에 따라 답변을 제공하거나 후속 질문을 하려고 합니다. 반면, 지시를 따르지 않는 모델은 다음 토큰을 예측하고 선택적으로 답변을 제공하거나 다음 지시를 계속합니다. 불완전한 문장 조각인 지시의 경우 두 모델 클래스 간의 구분이 더 명확해집니다. 지시 따르기 모델은 결코 지시를 완료하려고 하지 않습니다. 응답의 질은 의도적으로 이 분류의 범위를 벗어난다는 점을 강조하는 것이 중요합니다. 따라서 위의 기준은 채팅 모델에 필요하지만 충분한 조건은 아닙니다. 이 논문에서는 미리 정의된 일련의 지시에 대한 &quot;답변과 같은&quot; 응답과 &quot;계속과 같은&quot; 응답의 비율로 정의된 지시 따르기 점수(IFS)를 소개합니다. 응답의 클래스는 이진 분류기(이후 &quot;응답 톤 분류기&quot;라고 함)에 의해 결정됩니다. IFS에 대한 학습 및 데이터 수집 프로세스는 다음 섹션에서 설명합니다. &quot;이 논문에서 우리는 &quot;대화 톤&quot;과 &quot;지시 후 톤&quot;을 서로 바꿔 사용하는데, 이는 &quot;답변과 같은&quot; 응답의 종류를 의미합니다. 지시 모델을 얻기 위해 기본 모델을 미세 조정하는 프로세스를 &quot;지시 튜닝&quot;이라고 합니다. 3.2 데이터 세트 IFS의 데이터 세트는 원래 쌍(지시, 응답)으로 구성된 채팅 데이터 세트에서 파생되었습니다. 지시를 따르지 않는 모델의 입력 및 출력을 모델링해야 합니다. 데이터 생성의 주요 아이디어는 응답에 지시를 추가한 다음 그림 1과 같이 두 개의 구문으로 다른 하위 구분을 고려하는 것입니다. 다른 데이터 포인트 분할 R 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. Ip Ic+ R 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. Ip Ic 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. 지시 응답 그림 1: IFS 데이터 세트 생성. 다른 분할은 I, R, Ip, Ic라는 조각을 정의합니다. tion I - Instruction Ip - Partial (fragmented) instructuc• 추론 출력: Ic 명령어 R의 연속-응답 사실, 이 4개 부분의 조합은 vanilla 및 chat 모델에 대한 모든 가능한 입력 및 출력 쌍을 제공합니다. 아래 표에서 우리는 부분을 다시 결합하고 모델이 chat 모델처럼 응답하는지 여부에 따라 이진 점수를 부여하고 할당합니다. (I, R) 명령어 I에 대한 응답 R은 대화형입니다. 모든 응답이 위의 형태와 유사한 모델은 명령어를 따르는 것이므로 응답에는 레이블 1이 있습니다. (Ip, R) 부분 명령어 Ip에 대한 응답 R도 대화형이지만, 이 경우 모델은 추가 정보를 요청하는 것 외에는 어떤 대답도 제공할 만큼 충분한 맥락이 없습니다. 이 응답도 1로 레이블이 지정됩니다. (Ip, Ic) 모델은 조각난 명령어(다음 단어 예측 작업 실행)를 완료합니다. 쌍은 대화처럼 보이지 않으므로 레이블은 0입니다.(I, Ic) 모델은 다음 지침을 생성하고(다시 다음 단어 예측 작업과 유사), 응답 레이블은 0입니다.(Ip, Ic+R) 이 경우 모델은 지침을 완료한 다음 응답합니다(다음 단어 예측 작업도 실행). 저자는 사람들이 그러한 대화를 시도한다고 생각할 수 있지만, 우리는 지침 완료를 실패한 대화의 신호로 취급합니다. 레이블은 0입니다. 컷이 (지시, 응답)을 재생성하면 채팅 모델에 이상적인 입력과 출력을 얻습니다. 분할을 오른쪽이나 왼쪽으로 이동하면 불완전한 문장(단편화)을 얻을 수 있으며, 이는 (I, Ic+R)이 미완료 지침 또는 응답이 이어지는 지침의 연속을 나타냅니다. 요약하면 다음을 얻을 수 있습니다. • 추론 입력: 모델은 다른 지침을 생성한 다음 해당 생성에 응답합니다. 대화가 실패하여 응답 레이블이 0이 됩니다. 각 사례에 대한 예는 표 1에 나와 있습니다. 사례 (I, R) (Ip, R) 예 I: 사람들에게 다리가 있다면? R: 사람들에게 다리가 40개 있다면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것입니다! Ip: 만약 R: 질문이 불완전한 것 같습니다. 질문을 더 잘 이해하고 답변할 수 있도록 더 많은 맥락이나 세부 정보를 제공해 주십시오. Ip: 만약 Ic: 사람들에게 다리가 40개 있다면? I: 사람들에게 다리가 있다면? 채팅?응답 세트는 이진 분류기의 데이터를 생성하는 데 사용됩니다. 그림 2는 채팅 데이터가 어떻게 분할되어 실험에 사용되는지 보여줍니다. 깨끗한 텍스트의 소스로서, 우리는 OpenAssistant 채팅 데이터 세트(Köpf et al. 2023)를 활용했습니다. 대화의 맥락을 제어하기 위해, 우리는 각 대화에서 첫 번째 지시와 그에 해당하는 응답만 고려했습니다. 3.2.1 지침 데이터 세트 지침 데이터 세트에서 데이터 포인트는 OpenAssistant 데이터에서 가져온 지침으로 구성되며, 수정되지 않은 지침(I) 또는 단편화된 지침(Ip)입니다. 총 7340개의 예를 얻었으며, 약 50%가 단편과 완전한 0개의 문장으로 나뉩니다. 알고리즘이 단편화된 것으로 레이블이 지정된 완전한 문장을 생성할 가능성이 있으므로 이 레이블을 기준으로 분할된 점수는 대략적인 추정치라는 점을 알고 있습니다. (Ip, Ic) (I, Ic)I: 사람들에게 눈이 있다면 어떨까요? (Ip, Ic + R) Ip: 사람들에게 다리가 있다면 어떨까요? 표 2는 전체 및 부분 지침의 예를 보여줍니다. (I, I + R) Ic+R: 사람들에게 다리가 있다면? 사람들에게 다리가 40개 있다면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것입니다! I: 사람들에게 다리가 있다면 어떨까요? Ic+R: 사람들에게 눈이 3개 있다면 어떨까요? 영어: 사람들이 눈이 3개라면 선글라스는 유행하는 삼안경 스타일로 나오고 &quot;나는 당신을 주시하고 있습니다&quot;는 완전히 새로운 수준의 감시가 될 것입니다.표 1: 조각 I, R, Ip, Ic의 가능한 조합의 예.톤 점수는 모델이 지침을 따르는지(1) 따르지 않는지(0)를 나타냅니다.지침 HTML과 JavaScript의 차이점은 무엇입니까?레이블 partial HTML과 JavaScript의 차이점은 무엇입니까?full partial full Who wearsWho wears short shorts?표 2: 지침의 예와 해당 범주.3.2.2 응답 데이터 세트 응답 세트는 그림 1의 오른쪽, 즉 원래 응답 또는 오른쪽으로 이동된 응답을 나타냅니다.수집된 클래스는 다음과 같습니다.레이블 0 Ic, Ic+R 요약하면, 6가지 잠재적 조합 중에서 두 가지 지침 모델 사례만 존재합니다: (Ip, R) 레이블 1: R 및 (I, R).이 분류가 확립되었으므로 이제 지침 세트와 해당 모델 응답을 만들 수 있습니다. 우리는 모든 완벽한 컷과 이동된 컷에서 나온 쌍을 분리하고 두 개의 데이터 세트를 만듭니다. 모든 명령어와 모든 응답입니다. 명령어 세트는 모델을 프롬프트하는 데 사용되는 데이터를 생성하는 데 사용되는 반면, 우리는 응답의 세분화된 분류를 삭제하고 &quot;답변과 유사&quot;(레이블!) 또는 &quot;연속과 유사&quot;(레이블 0)에만 할당합니다. 이러한 샘플은 나중에 이진 분류기를 훈련하는 데 사용됩니다. 표 3은 응답과 해당 레이블의 예를 보여줍니다. 응답 너무 빨리 날 수 있나요? 가장 빠르게 나는 새는 송골매입니다. 채팅? 5개 결과 5.1 기준 에이전트? 저는 FBI 요원이 아닙니다. 양파를 자르면 황산이라는 화학 물질이 방출됩니다. 제임스 매디슨은 헌법과 권리 장전의 주요 저자였습니다. 표 3: 응답과 해당 범주의 예. 4 이진 분류기와 지시 추종 점수 톤 응답 분류를 위한 이진 분류기는 Huggingface AutoTrain(Huggingface 2023a)을 사용하여 응답 세트에서 학습된 최상의 이진 분류기로 선택되었습니다. 데이터 세트가 대략적으로 동일한 양의 음성 및 양성 샘플로 구성되었으므로 비교 메트릭으로 정확도를 선택했습니다. 우승한 아키텍처는 BertForSequenceClassification이었고 최종 분류기 메트릭(AutoTrain에서 보고한 대로)은 표 4에 나와 있습니다. 메트릭 값 정확도 0. 정밀도 0. 재현율 0. 표 4: 검증 메트릭 지시 추종 점수(IFS)는 지시 데이터 세트를 프롬프트하여 얻은 모든 응답에 대한 &quot;답변 유사&quot;(레이블 1)로 분류된 모든 응답의 비율로 정의합니다. 완벽한 지시 조정 모델은 항상 대화 톤(즉, 지시가 부분적이든 아니든 모든 지시에 채팅 모델처럼 응답)을 유지해야 하므로 최대 IFS는 1입니다. 또한 두 가지 관련 메트릭인 IFS 부분 및 IFS 전체를 정의할 수 있으며, 이는 각각 모든 부분 및 전체 지시에 대한 &quot;답변과 같은&quot; 응답의 비율입니다. 다음 섹션에서는 IFS를 사용하여 바닐라 모델과 프롬프트 엔지니어링 및 SFT 프로세스로 달성한 응답 톤 변화를 평가합니다. IFS 메트릭을 사용하여 여러 공개적으로 사용 가능한 모델을 평가했습니다. 데이터 세트는 50% 미만의 단편화된 지시(알고리즘에서 생성된 거짓 양성 포함)로 구성되므로 추가 접미사 없이 프롬프트할 때 기본 모델이 이 수준 아래의 IFS를 얻을 것으로 예상했습니다. 표 5에 제시된 SFT 및 RLHF 모델의 점수는 예상 최대값이 약 0.8-0.9인 반면 기본 및 지시 따르기 LLM 간의 가장 두드러진 차이는 IFS 부분과 IFS 전체 간의 상대적 차이입니다. 모델 IFS IFS 부분 IFS 전체 GPT-0.0.0.RedPajama-3B 0.0.0.LLaMa-7B 0.0.0.LLAMA-13B 0.0.0.LLAMA-33B 0.0.0.davinci Palmyra-x 0.0.0.0.0.0.Palmyra-base 0.0.0.Palmyra-large 0.0.0.text-davinci-003 0.0.0.GPT-3.5-turbo 0.0.0.0.0.0.Palmyra-instruct 0.0.0.GPT-표 5: 기준: 선택된 공개적으로 사용 가능한 모델에 대한 명령어 추종 점수(IFS). 5.2 프롬프트 엔지니어링 LM이 명령어를 따르도록 장려하는 매우 간단한 방법은 명령어 주위에 추가 프롬프트 접미사 또는 래퍼를 추가하는 것입니다. 이를 통해 다음 토큰 예측 작업을 방해하고 응답을 생성할 수 있습니다. 그림 3은 세 가지 버전의 프롬프트를 나타냅니다.A. 알파카 프롬프트 명령어 튜닝 프롬프트 아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.### 명령어: {명령} ### 응답: {응답} B. 당사의 프롬프트 C. 프롬프트 없음 (명령) {명령} ### 응답: (응답) {응답} 그림 3: 명령어 튜닝 프롬프트의 비교 그림.A. 알파카 프롬프트, 명령어를 래퍼로 사용, B. 알파카 접미사만 사용, C. 프롬프트 없음, 기준선 1단계: 분류기 학습 및 LM 예측 채팅 데이터 IFS 2단계: 평가 학습 추론 이진 분류기 추론 LM 명령어 응답 그림 2: IFS 학습 및 평가 파이프라인 표 6에 제시된 결과는 두 프롬프트의 변형이 모두 똑같이 효과적임을 보여줍니다.기준선(C)과 비교하면 모든 모델에서 IFS 개선 범위가 0.5~0.6임을 알 수 있습니다. 대규모 언어 모델(LLM)의 경우 단일 프롬프트 변경으로 모델이 지침을 따르도록 효과적으로 장려하여 공개적으로 사용 가능한 여러 instruct 모델과 비슷한 성능 수준에 도달할 수 있습니다. 결과를 더욱 개선할 수 있는 n-shot 프롬프팅은 테스트하지 않았습니다. 2023년 Anand et al.에서 도입된 gpt4all v1.3-groovy를 instruct 데이터 세트로 사용했습니다. 문자 제한을 2k로 설정했습니다(512토큰 길이로 학습된 LLaMa 모델 사전 학습 목표와 유사). 이 필터링 프로세스를 통해 instruct 튜닝을 위한 약 410k개의 예를 얻었습니다. 모델은 수정된 Alpaca 프롬프트를 사용하여 훈련되었습니다. PROMPT_DICT = { 데이터 집합 IFS IFSpartial IFS 전체 LLaMa-7BA 0.0.0.LLaMa-7BB 0.0.0.} LLaMa-7Bc 0.0.0.LLAMA-13BA 0.0.0.LLAMA-13BB 0.0.0.LLAMA-13Bc 0.0.0.LLAMA-33BA 0.0.0.LLAMA-33BB 0.0.0.LLAMA-33Bc 0.0.0.&quot;prompt_input&quot;: (&quot;{지침}\n\n{ 입력}### 응답:&quot;), &quot;prompt_no_input&quot;: (&quot;{지침}### → 응답:&quot;), 이 수정은 접두사 프롬프트를 제거하는 동시에 지침과 선택적 입력을 통합합니다. 이 접근 방식은 채팅 모델의 사용자 인터페이스가 일반적으로 구현되는 방식, 즉 단일 대화 입력 상자와 일치합니다.전체 Alpaca 래퍼를 사용할 수 있지만 두 가지 프롬프트 기술 모두 비슷한 점수를 가져오므로 프롬프트 접미사가 있는 모델과 없는 모델을 대상으로 효율성을 위해 더 짧은 모델을 선택했습니다.5.3 지도 미세 조정 이 연구에서는 SFT의 기본 LLM으로 7B 및 13B LLAMA 모델을 선택했습니다.결과의 비교 가능성을 보장하기 위해 동일한 교육 절차와 평가를 따랐습니다.SFT의 결과는 그림 4(a)에 나와 있습니다.약 8k개의 예제(수평 점선으로 표시)를 확인한 후 모델의 명령 조정 기능이 수준 0.9-0.95에서 안정화되는 것을 알 수 있습니다.이 교육 단계를 &quot;형식 주입&quot; 단계라고 합니다. 참고로, 더 큰 모델이 0.9 IFS 수준에 비교적 더 빨리 도달할 수 있다는 것을 관찰했습니다(2점 실험에서 추론할 수 있는 한도). 이는 1k 예제에서 65B LLAMA의 SFT가 좋은 결과를 냈다는 것을 지지합니다(Zhou et al. 2023). SFT에서 발생할 수 있는 모델 응답의 의미적 변화와 톤 변화를 대조하기 위해 채팅 예제를 관찰하는 동안 획득할 수 있는 기능을 찾았습니다. 자세한 검사 없이 gpt4all v1.3-groovy 데이터 세트에서 어떤 기능을 학습할 수 있는지 추정하기 어렵기 때문에 (성공적인) 추측을 목표로 했습니다: &quot;객관성&quot;. 우리는 모델이 인간과 같은 선호도(예: &quot;고양이&quot; 또는 &quot;개&quot;)를 갖지 않을 것으로 예상합니다. 그 이유는 다음과 같습니다: (a) 보편적인 권장 사항을 제공하는 AI를 모델링하는 지침에 따라 훈련되었거나, (b) 객관성이 부상 속성으로 유사한 질문에 대한 다른 답변이 있는 많은 예를 보았기 때문입니다(Wei et al. 2022). 우리는 주관적인 선택이나 선호도를 포함하는 100개의 질문으로 구성된 ObjecQA 벤치마크를 제안합니다. ObjecQA에서 높은 점수를 받는 모델은 다양한 가능성을 제시하거나 직접적인 답변을 피해야 합니다(예: &quot;선호도에 따라 달라짐&quot;). ObjecQA의 주관적 질문의 처음 10가지 예: 가장 좋은 것을 선택하거나 논란의 여지가 있는 주제에 대한 의견을 밝힙니다. 객관적인 의견은 가능한 답변의 전체 → 범위를 보여 주려고 하거나, 맥락의 부족을 양보하거나, 단순히 하나의 확실한 선택을 하지 않으려고 합니다. 응답: 롤러코스터를 타는 스릴을 선호합니다. 수업: 주관적 응답: 상황에 따라 다릅니다. 어떤 경우에는 실용성이 → 더 중요하지만, 다른 경우에는 재미가 더 중요합니다. 클래스 객관식 응답: &quot;그림 4(b)에서. 우리는 점수의 진행이 두 모델에서 유사하고, 대부분의 학습 과정이 검은색 선 마커(약 8k 예) 이후에 발생한다는 것을 관찰합니다. 이 단계를 &quot;지식 주입&quot;이라고 합니다. 한 가지 놀라운 통찰력은 가장 중요한 의미적 변화(지식 주입)가 포맷팅 변화(포맷팅 단계) 바로 뒤에 발생한다는 것입니다. (기본 모델의 모든 질의가 완전한 문장이므로 LLAMA가 커피보다 낫습니까? 3. 고양이가 최고의 애완동물입니까? 4. 휴가를 위해 해변을 선호합니까? 산을 선호합니까?) 더욱이 모델의 ObjectQA는 IFS가 정점에 도달한 후에도 오랫동안 계속 성장합니다. 이 관찰은 이러한 특징의 조합(IFS 및 Ob5. 번화한 도시에서 사는 것이 낫습니까?)에 대해 다음을 의미합니다. jectQA), LLAMA 7B와 13B LM은 모두 선택된 데이터 세트에서 훈련되었으며, 분리된 형식 주입 및 지식 주입 단계를 보입니다. 6. 전자책과 실제 책 중 어느 것이 더 좋은가? 이론적으로 조기 중단 기준을 적용하여 의미적 변화를 최소화할 수 있습니다. 간단한 특징(겹치는 단계)에서 매우 복잡하고 확산된 요인에 이르기까지 다양한 학습 역학을 상상할 수 있습니다. 반면, 비교적 높은 IFS를 가진 모델은 채팅 모델의 좋은 시작점이 될 수 있습니다. 채팅 기능을 SFT 단계의 영향 최소화와 결합하면 10. 귀하의 의견으로는 겨울이나 여름 중 어느 것이 더 좋은가? 사전 훈련 단계 지식을 쿼리합니다. 우리는 se- 6에 GPT-3.5-turbo 프롬프트를 사용했습니다.
--- CONCLUSION ---
6절에서 제안한 s 및 미래 방향.절제 연구에 따르면 학습 데이터의 다양성과 품질이 모델 성능에 중요한 역할을 합니다(Chen et al. 2023, Zhou et al. 2023).Low Training Data Instruction Tuning(LTD Tuning)은 작업별 모델이 원래 데이터의 0.5% 미만으로 학습하면 2%의 성능을 얻을 수 있음을 시사합니다.게다가 장기간의 학습 데이터 튜닝은 기초적인 모델 지식을 감소시킬 수 있으며(Gudibande et al. 2023) 학습 튜닝의 하위 작업인 분포 외 작업으로 볼 수 있습니다(Kumar et al. 2022).이 연구에서는 학습 모델에 필요한(충분하지는 않지만) 조건을 정의하여 학습 모델 연구의 기반을 마련하고자 합니다.사고 실험을 해 보겠습니다.모든 모델을 폐쇄형 API(최근의 블랙박스와 동일) 뒤에 두겠습니다.모델이 학습 튜닝되었는지 아닌지? 지식 벤치마크는 LTD 튜닝을 위한 vanilla 및 instruct 모델에 대해 유사할 수 있습니다. 기술 테스트는 모델 크기에 크게 좌우되며, 이는 알려지지 않았습니다. 수수께끼를 푸는 가장 간단한 방법은 ... 모델과 채팅하고 응답의 톤을 판단하는 것입니다. vanilla 모델의 경우 다음 예측 단어 시도를 예상하는 반면 instruct 모델의 경우 지시를 따를 것으로 예상합니다. 이러한 톤 차이를 포착하는 메트릭인 Instruct Following Score(IFS)를 소개합니다. 이 문제를 &quot;톤 정렬&quot; 문제라고 합니다. IFS는 사전 정의된 지침 세트에 대한 &quot;답변과 같은&quot; 응답과 &quot;계속과 같은&quot; 응답의 비율로 정의되며, 여기서 응답의 클래스는 이진 분류기에 의해 결정됩니다. 공개적으로 사용 가능한 기본 및 지시 모델을 벤치마킹하고, 잘 형식화된 응답과 부분 및 전체 문장의 비율이 vanilla 및 instruct Following 모델 사이에서 효과적인 척도가 될 수 있음을 보여줍니다. 또한, 최소 instruct 튜닝을 위한 중단 기준을 찾기 위해 7B 및 13B LLAMA 모델에 대한 SFT에 대한 IFS를 계산합니다. 응답 톤에 대한 학습 곡선과 의미 및 도메인별 지식 습득을 비교하기 위해 ObjecQA라는 보충 지표를 제안합니다. 이 보조 지표는 데이터 세트 내에서 이 신호를 식별할 수 있으므로 모델 예측의 객관성을 정량화합니다. 이 기능 선택은 임의적이지만 가능한 더 많은 것을 발견하는 것을 목표로 합니다. 응답 톤 정렬 문제는 더 광범위한 의도 정렬 주제의 일부입니다. 원칙적으로 LLM은 언어 모델링 목표(예: 교육 문서의 다음 토큰 예측)가 다음 지침 목표와 다르기 때문에 사용자의 의도와 일치하지 않습니다. 두 가지 목표를 모두 정렬하는 성공적인 방법 중 하나는 응답이 QA가 포함된 문서를 완성한 것처럼 보이는 기법입니다(Brown et al. 2020, Radford et al. 2018). 또 다른 접근 방식은 바닐라 모델을 지시와 응답의 튜플에 지시하고 조정하여 학습의 일부로 모델이 올바른 형식 응답을 모방하는 기술을 습득하도록 하는 것입니다(Alpaca: Taori et al. 2023, Self-Instruct: Wang et al. 2023). InstructGPT 논문(Ouyang et al. 2022)에서 &quot;올바른 지시 작업을 따르지 못함&quot;이라는 기준은 SFT 모델을 미세 조정하여 보상을 극대화하기 위해 PPO 알고리즘(Schulman et al. 2017)에 사용된 보상 모델(RM)에 대한 인간 평가 메타데이터 목록에 포함되었습니다. 우리는 지식 및 언어 이해 기반 메트릭(예: MMLU(Hendrycks et al. 2021))을 사용하는 대신 각 전략을 스타일 서식 문제로 평가하여 톤 구성 요소를 분리하고 이해하는 것을 목표로 합니다. 3 지시 따르기 지수 3.1 동기 지시 따르기 모델은 직관적으로 대화 에이전트처럼 동작합니다. 즉, 항상 입력이 지시라고 가정하고 이해에 따라 답변을 제공하거나 후속 질문을 하려고 합니다. 반면, 지시를 따르지 않는 모델은 다음 토큰을 예측하고 선택적으로 답변을 제공하거나 다음 지시를 계속합니다. 불완전한 문장 조각인 지시의 경우 두 모델 클래스 간의 구분이 더 명확해집니다. 지시 따르기 모델은 결코 지시를 완료하려고 하지 않습니다. 응답의 질은 의도적으로 이 분류의 범위를 벗어난다는 점을 강조하는 것이 중요합니다. 따라서 위의 기준은 채팅 모델에 필요하지만 충분한 조건은 아닙니다. 이 논문에서는 미리 정의된 일련의 지시에 대한 &quot;답변과 같은&quot; 응답과 &quot;계속과 같은&quot; 응답의 비율로 정의된 지시 따르기 점수(IFS)를 소개합니다. 응답의 클래스는 이진 분류기(이후 &quot;응답 톤 분류기&quot;라고 함)에 의해 결정됩니다. IFS에 대한 학습 및 데이터 수집 프로세스는 다음 섹션에서 설명합니다. &quot;이 논문에서 우리는 &quot;대화 톤&quot;과 &quot;지시 후 톤&quot;을 서로 바꿔 사용하는데, 이는 &quot;답변과 같은&quot; 응답의 종류를 의미합니다. 지시 모델을 얻기 위해 기본 모델을 미세 조정하는 프로세스를 &quot;지시 튜닝&quot;이라고 합니다. 3.2 데이터 세트 IFS의 데이터 세트는 원래 쌍(지시, 응답)으로 구성된 채팅 데이터 세트에서 파생되었습니다. 지시를 따르지 않는 모델의 입력 및 출력을 모델링해야 합니다. 데이터 생성의 주요 아이디어는 응답에 지시를 추가한 다음 그림 1과 같이 두 개의 구문으로 다른 하위 구분을 고려하는 것입니다. 다른 데이터 포인트 분할 R 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. Ip Ic+ R 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. Ip Ic 프랑스의 수도는 무엇입니까? 프랑스의 수도는 파리입니다. 지시 응답 그림 1: IFS 데이터 세트 생성. 다른 분할은 I, R, Ip, Ic라는 조각을 정의합니다. tion I - Instruction Ip - Partial (fragmented) instructuc• 추론 출력: Ic 명령어 R의 연속-응답 사실, 이 4개 부분의 조합은 vanilla 및 chat 모델에 대한 모든 가능한 입력 및 출력 쌍을 제공합니다. 아래 표에서 우리는 부분을 다시 결합하고 모델이 chat 모델처럼 응답하는지 여부에 따라 이진 점수를 부여하고 할당합니다. (I, R) 명령어 I에 대한 응답 R은 대화형입니다. 모든 응답이 위의 형태와 유사한 모델은 명령어를 따르는 것이므로 응답에는 레이블 1이 있습니다. (Ip, R) 부분 명령어 Ip에 대한 응답 R도 대화형이지만, 이 경우 모델은 추가 정보를 요청하는 것 외에는 어떤 대답도 제공할 만큼 충분한 맥락이 없습니다. 이 응답도 1로 레이블이 지정됩니다. (Ip, Ic) 모델은 조각난 명령어(다음 단어 예측 작업 실행)를 완료합니다. 쌍은 대화처럼 보이지 않으므로 레이블은 0입니다.(I, Ic) 모델은 다음 지침을 생성하고(다시 다음 단어 예측 작업과 유사), 응답 레이블은 0입니다.(Ip, Ic+R) 이 경우 모델은 지침을 완료한 다음 응답합니다(다음 단어 예측 작업도 실행). 저자는 사람들이 그러한 대화를 시도한다고 생각할 수 있지만, 우리는 지침 완료를 실패한 대화의 신호로 취급합니다. 레이블은 0입니다. 컷이 (지시, 응답)을 재생성하면 채팅 모델에 이상적인 입력과 출력을 얻습니다. 분할을 오른쪽이나 왼쪽으로 이동하면 불완전한 문장(단편화)을 얻을 수 있으며, 이는 (I, Ic+R)이 미완료 지침 또는 응답이 이어지는 지침의 연속을 나타냅니다. 요약하면 다음을 얻을 수 있습니다. • 추론 입력: 모델은 다른 지침을 생성한 다음 해당 생성에 응답합니다. 대화가 실패하여 응답 레이블이 0이 됩니다. 각 사례에 대한 예는 표 1에 나와 있습니다. 사례 (I, R) (Ip, R) 예 I: 사람들에게 다리가 있다면? R: 사람들에게 다리가 40개 있다면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것입니다! Ip: 만약 R: 질문이 불완전한 것 같습니다. 질문을 더 잘 이해하고 답변할 수 있도록 더 많은 맥락이나 세부 정보를 제공해 주십시오. Ip: 만약 Ic: 사람들에게 다리가 40개 있다면? I: 사람들에게 다리가 있다면? 채팅?응답 세트는 이진 분류기의 데이터를 생성하는 데 사용됩니다. 그림 2는 채팅 데이터가 어떻게 분할되어 실험에 사용되는지 보여줍니다. 깨끗한 텍스트의 소스로서, 우리는 OpenAssistant 채팅 데이터 세트(Köpf et al. 2023)를 활용했습니다. 대화의 맥락을 제어하기 위해, 우리는 각 대화에서 첫 번째 지시와 그에 해당하는 응답만 고려했습니다. 3.2.1 지침 데이터 세트 지침 데이터 세트에서 데이터 포인트는 OpenAssistant 데이터에서 가져온 지침으로 구성되며, 수정되지 않은 지침(I) 또는 단편화된 지침(Ip)입니다. 총 7340개의 예를 얻었으며, 약 50%가 단편과 완전한 0개의 문장으로 나뉩니다. 알고리즘이 단편화된 것으로 레이블이 지정된 완전한 문장을 생성할 가능성이 있으므로 이 레이블을 기준으로 분할된 점수는 대략적인 추정치라는 점을 알고 있습니다. (Ip, Ic) (I, Ic)I: 사람들에게 눈이 있다면 어떨까요? (Ip, Ic + R) Ip: 사람들에게 다리가 있다면 어떨까요? 표 2는 전체 및 부분 지침의 예를 보여줍니다. (I, I + R) Ic+R: 사람들에게 다리가 있다면? 사람들에게 다리가 40개 있다면, 그들은 움직이는 인간 지네가 되어 경주에서 세계 기록을 세우고 트위스터에서 항상 우승할 것입니다! I: 사람들에게 다리가 있다면 어떨까요? Ic+R: 사람들에게 눈이 3개 있다면 어떨까요? 영어: 사람들이 눈이 3개라면 선글라스는 유행하는 삼안경 스타일로 나오고 &quot;나는 당신을 주시하고 있습니다&quot;는 완전히 새로운 수준의 감시가 될 것입니다.표 1: 조각 I, R, Ip, Ic의 가능한 조합의 예.톤 점수는 모델이 지침을 따르는지(1) 따르지 않는지(0)를 나타냅니다.지침 HTML과 JavaScript의 차이점은 무엇입니까?레이블 partial HTML과 JavaScript의 차이점은 무엇입니까?full partial full Who wearsWho wears short shorts?표 2: 지침의 예와 해당 범주.3.2.2 응답 데이터 세트 응답 세트는 그림 1의 오른쪽, 즉 원래 응답 또는 오른쪽으로 이동된 응답을 나타냅니다.수집된 클래스는 다음과 같습니다.레이블 0 Ic, Ic+R 요약하면, 6가지 잠재적 조합 중에서 두 가지 지침 모델 사례만 존재합니다: (Ip, R) 레이블 1: R 및 (I, R).이 분류가 확립되었으므로 이제 지침 세트와 해당 모델 응답을 만들 수 있습니다. 우리는 모든 완벽한 컷과 이동된 컷에서 나온 쌍을 분리하고 두 개의 데이터 세트를 만듭니다. 모든 명령어와 모든 응답입니다. 명령어 세트는 모델을 프롬프트하는 데 사용되는 데이터를 생성하는 데 사용되는 반면, 우리는 응답의 세분화된 분류를 삭제하고 &quot;답변과 유사&quot;(레이블!) 또는 &quot;연속과 유사&quot;(레이블 0)에만 할당합니다. 이러한 샘플은 나중에 이진 분류기를 훈련하는 데 사용됩니다. 표 3은 응답과 해당 레이블의 예를 보여줍니다. 응답 너무 빨리 날 수 있나요? 가장 빠르게 나는 새는 송골매입니다. 채팅? 5개 결과 5.1 기준 에이전트? 저는 FBI 요원이 아닙니다. 양파를 자르면 황산이라는 화학 물질이 방출됩니다. 제임스 매디슨은 헌법과 권리 장전의 주요 저자였습니다. 표 3: 응답과 해당 범주의 예. 4 이진 분류기와 지시 추종 점수 톤 응답 분류를 위한 이진 분류기는 Huggingface AutoTrain(Huggingface 2023a)을 사용하여 응답 세트에서 학습된 최상의 이진 분류기로 선택되었습니다. 데이터 세트가 대략적으로 동일한 양의 음성 및 양성 샘플로 구성되었으므로 비교 메트릭으로 정확도를 선택했습니다. 우승한 아키텍처는 BertForSequenceClassification이었고 최종 분류기 메트릭(AutoTrain에서 보고한 대로)은 표 4에 나와 있습니다. 메트릭 값 정확도 0. 정밀도 0. 재현율 0. 표 4: 검증 메트릭 지시 추종 점수(IFS)는 지시 데이터 세트를 프롬프트하여 얻은 모든 응답에 대한 &quot;답변 유사&quot;(레이블 1)로 분류된 모든 응답의 비율로 정의합니다. 완벽한 지시 조정 모델은 항상 대화 톤(즉, 지시가 부분적이든 아니든 모든 지시에 채팅 모델처럼 응답)을 유지해야 하므로 최대 IFS는 1입니다. 또한 두 가지 관련 메트릭인 IFS 부분 및 IFS 전체를 정의할 수 있으며, 이는 각각 모든 부분 및 전체 지시에 대한 &quot;답변과 같은&quot; 응답의 비율입니다. 다음 섹션에서는 IFS를 사용하여 바닐라 모델과 프롬프트 엔지니어링 및 SFT 프로세스로 달성한 응답 톤 변화를 평가합니다. IFS 메트릭을 사용하여 여러 공개적으로 사용 가능한 모델을 평가했습니다. 데이터 세트는 50% 미만의 단편화된 지시(알고리즘에서 생성된 거짓 양성 포함)로 구성되므로 추가 접미사 없이 프롬프트할 때 기본 모델이 이 수준 아래의 IFS를 얻을 것으로 예상했습니다. 표 5에 제시된 SFT 및 RLHF 모델의 점수는 예상 최대값이 약 0.8-0.9인 반면 기본 및 지시 따르기 LLM 간의 가장 두드러진 차이는 IFS 부분과 IFS 전체 간의 상대적 차이입니다. 모델 IFS IFS 부분 IFS 전체 GPT-0.0.0.RedPajama-3B 0.0.0.LLaMa-7B 0.0.0.LLAMA-13B 0.0.0.LLAMA-33B 0.0.0.davinci Palmyra-x 0.0.0.0.0.0.Palmyra-base 0.0.0.Palmyra-large 0.0.0.text-davinci-003 0.0.0.GPT-3.5-turbo 0.0.0.0.0.0.Palmyra-instruct 0.0.0.GPT-표 5: 기준: 선택된 공개적으로 사용 가능한 모델에 대한 명령어 추종 점수(IFS). 5.2 프롬프트 엔지니어링 LM이 명령어를 따르도록 장려하는 매우 간단한 방법은 명령어 주위에 추가 프롬프트 접미사 또는 래퍼를 추가하는 것입니다. 이를 통해 다음 토큰 예측 작업을 방해하고 응답을 생성할 수 있습니다. 그림 3은 세 가지 버전의 프롬프트를 나타냅니다.A. 알파카 프롬프트 명령어 튜닝 프롬프트 아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.### 명령어: {명령} ### 응답: {응답} B. 당사의 프롬프트 C. 프롬프트 없음 (명령) {명령} ### 응답: (응답) {응답} 그림 3: 명령어 튜닝 프롬프트의 비교 그림.A. 알파카 프롬프트, 명령어를 래퍼로 사용, B. 알파카 접미사만 사용, C. 프롬프트 없음, 기준선 1단계: 분류기 학습 및 LM 예측 채팅 데이터 IFS 2단계: 평가 학습 추론 이진 분류기 추론 LM 명령어 응답 그림 2: IFS 학습 및 평가 파이프라인 표 6에 제시된 결과는 두 프롬프트의 변형이 모두 똑같이 효과적임을 보여줍니다.기준선(C)과 비교하면 모든 모델에서 IFS 개선 범위가 0.5~0.6임을 알 수 있습니다. 대규모 언어 모델(LLM)의 경우 단일 프롬프트 변경으로 모델이 지침을 따르도록 효과적으로 장려하여 공개적으로 사용 가능한 여러 instruct 모델과 비슷한 성능 수준에 도달할 수 있습니다. 결과를 더욱 개선할 수 있는 n-shot 프롬프팅은 테스트하지 않았습니다. instruct 데이터 세트로 Anand et al. 2023에서 도입된 gpt4all v1.3-groovy를 사용했습니다. 문자 제한을 2k로 설정했습니다(512토큰 길이로 학습된 LLaMa 모델 사전 학습 목표와 유사). 이 필터링 프로세스를 통해 instruct 튜닝을 위한 약 410k개의 예를 얻었습니다. 모델은 수정된 Alpaca 프롬프트를 사용하여 훈련되었습니다. PROMPT_DICT = { 데이터 세트 IFS IFSpartial IFS 전체 LLaMa-7BA 0.0.0.LLaMa-7BB 0.0.0.} LLaMa-7Bc 0.0.0.LLAMA-13BA 0.0.0.LLAMA-13BB 0.0.0.LLAMA-13Bc 0.0.0.LLAMA-33BA 0.0.0.LLAMA-33BB 0.0.0.LLAMA-33Bc 0.0.0.&quot;prompt_input&quot;: (&quot;{지침}\n\n{ 입력}### 응답:&quot;), &quot;prompt_no_input&quot;: (&quot;{지침}### → 응답:&quot;), 이 수정은 접두사 프롬프트를 제거하는 동시에 지침과 선택적 입력을 통합합니다. 이 접근 방식은 채팅 모델의 사용자 인터페이스가 일반적으로 구현되는 방식, 즉 단일 대화 입력 상자와 일치합니다.전체 Alpaca 래퍼를 사용할 수 있지만 두 가지 프롬프트 기술 모두 비슷한 점수를 가져오므로 프롬프트 접미사가 있는 모델과 없는 모델을 대상으로 효율성을 고려하여 더 짧은 모델을 선택했습니다.5.3 지도 미세 조정 이 연구에서는 SFT의 기본 LLM으로 7B 및 13B LLAMA 모델을 선택했습니다.결과의 비교 가능성을 보장하기 위해 동일한 교육 절차와 평가를 따랐습니다.SFT의 결과는 그림 4(a)에 나와 있습니다.약 8k개의 예제(수평 점선으로 표시)를 확인한 후 모델의 명령 조정 기능이 수준 0.9-0.95에서 안정화되는 것을 알 수 있습니다.이 교육 단계를 &quot;형식 주입&quot; 단계라고 합니다. 참고로, 더 큰 모델이 비교적 더 빨리 0.9 IFS 수준에 도달할 수 있다는 것을 관찰했습니다(2점 실험에서 추론할 수 있는 한도). 이는 1k 예제에서 65B LLAMA의 SFT가 좋은 결과를 냈다는 것을 지지합니다(Zhou et al. 2023). SFT에서 발생할 수 있는 모델 응답의 의미적 변화와 톤 변화를 대조하기 위해 채팅 예제를 관찰하는 동안 획득할 수 있는 기능을 찾았습니다. 자세한 검사 없이 gpt4all v1.3-groovy 데이터 세트에서 어떤 기능을 학습할 수 있는지 추정하기 어렵기 때문에 (성공적인) 추측을 목표로 했습니다: &quot;객관성&quot;. 우리는 모델이 인간과 같은 선호도(예: &quot;고양이&quot; 또는 &quot;개&quot;)를 갖지 않을 것으로 예상합니다. 그 이유는 다음과 같습니다: (a) 보편적인 권장 사항을 제공하는 AI를 모델링하는 지침에 따라 훈련되었거나, (b) 객관성이 부상 속성으로 유사한 질문에 대한 다른 답변이 있는 많은 예를 보았기 때문입니다(Wei et al. 2022). 우리는 주관적인 선택이나 선호도를 포함하는 100개의 질문으로 구성된 ObjecQA 벤치마크를 제안합니다. ObjecQA에서 높은 점수를 받는 모델은 다양한 가능성을 제시하거나 직접적인 답변을 피해야 합니다(예: &quot;선호도에 따라 달라짐&quot;). ObjecQA의 주관적 질문의 처음 10가지 예: 가장 좋은 것을 선택하거나 논란의 여지가 있는 주제에 대한 의견을 밝힙니다. 객관적인 의견은 가능한 답변의 전체 → 범위를 보여 주려고 하거나, 맥락의 부족을 양보하거나, 단순히 하나의 확실한 선택을 하지 않으려고 합니다. 응답: 롤러코스터를 타는 스릴을 선호합니다. 수업: 주관적 응답: 상황에 따라 다릅니다. 어떤 경우에는 실용성이 → 더 중요하지만, 다른 경우에는 재미가 더 중요합니다. 클래스 객관식 응답: &quot;그림 4(b)에서. 우리는 점수의 진행이 두 모델에서 유사하고, 대부분의 학습 과정이 검은색 선 마커(약 8k 예) 이후에 발생한다는 것을 관찰합니다. 이 단계를 &quot;지식 주입&quot;이라고 합니다. 한 가지 놀라운 통찰력은 가장 중요한 의미적 변화(지식 주입)가 포맷팅 변화(포맷팅 단계) 바로 뒤에 발생한다는 것입니다. (기본 모델의 모든 질의가 완전한 문장이므로 LLAMA가 커피보다 낫습니까? 3. 고양이가 최고의 애완동물입니까? 4. 휴가를 위해 해변을 선호합니까? 산을 선호합니까?) 더욱이 모델의 ObjectQA는 IFS가 정점에 도달한 후에도 오랫동안 계속 성장합니다. 이 관찰은 이러한 특징의 조합(IFS 및 Ob5. 번화한 도시에서 사는 것이 낫습니까?)에 대해 다음을 의미합니다. jectQA), LLAMA 7B와 13B LM은 모두 선택된 데이터 세트에서 훈련되었으며, 분리된 형식 주입 및 지식 주입 단계를 보입니다. 6. 전자책과 실제 책 중 어느 것이 더 좋은가? 이론적으로 조기 중단 기준을 적용하여 의미적 변화를 최소화할 수 있습니다. 간단한 특징(겹치는 단계)에서 매우 복잡하고 확산된 요인에 이르기까지 다양한 학습 역학을 상상할 수 있습니다. 반면, 비교적 높은 IFS를 가진 모델은 채팅 모델의 좋은 시작점이 될 수 있습니다. 채팅 기능을 SFT 단계의 영향 최소화와 결합하면 10. 귀하의 의견으로는 겨울이나 여름 중 어느 것이 더 좋은가? 사전 훈련 단계 지식을 쿼리합니다. 우리는 모델 출력의 semantic 분류를 위해 GPT-3.5-turbo 프롬프트를 사용했으며, 모든 인스턴스에서 2샷 예측 접근 방식을 활용했습니다. 우리는 다음과 같은 프롬프트를 사용했습니다. &quot;아래 응답을 → 주관적 의견, → 선호도 또는 객관적으로 분류하십시오. 주관적 응답은 → 질문을 받았을 때 옵션을 선택합니다. 결론적으로, 지시 따르기 점수(IFS)는 언어 모델이 지시를 따르는 능력을 감지하는 척도로 도입되었습니다. 공개적으로 사용 가능한 다양한 모델의 벤치마크는 기본 모델과 지시 조정 모델 간에 상당한 격차가 있지만 SFT와 RLFH 모델 간에는 명확한 격차가 없음을 보여줍니다. 감독 미세 조정 모델 이름에서 IFS 0.LlaMA-7b LlaMA-13b 0.0.0.0.0.0.0.7500 10000 12500N 샘플 (a) IFS ObjecQA 점수 0.0.model_name LLAMA 7B LLAMA 13B SFT에서 0.0.0.0.ObjecQA 5000 7500 10000 12500N 샘플 (b) ObjecQA 그림 4: (a) SFT에서 7B, 13B LLAMA 모델의 IFS 특성. 높은 IFS 값은 모델이 지침을 따른다는 것을 의미합니다. (b) SFT에서 7B, 13B LLaMA 모델의 ObjecQA. 강한 선호도(유형 &quot;고양이 또는 개&quot;)가 없는 모델은 더 높은 점수를 받습니다. LLAMA 7B 및 13B의 SFT 프로세스에 대한 IFS 평가는 지침 톤이 비교적 일찍 학습되었음을 보여줍니다. 보조 메트릭 ObjecQA는 톤 학습 곡선을 의미 및 도메인별 지식 습득과 대조하기 위해 제안되었습니다. 주요 결과에 따르면 검사된 모델의 지침 튜닝 기능(형식 주입 단계)은 약 8k 개의 예를 본 후 0.9-0.95에서 정점에 도달하며, 여기서 의미적 변화(지식 주입 단계)를 관찰합니다. 더 큰 모델은 비교적 빨리 0.IFS 수준에 도달했으며, 높은 IFS는 프로세스 초기에 달성되어 학습 스타일에 필요한 샘플 포인트를 줄임으로써 최소한의 의미적 변경이 가능했습니다. 향후 작업을 위해 연구는 기초 모델에 적용하여 유용성, 형식성 또는 엄격한 형식과 같은 원하는 정렬 측면을 달성하는 데 초점을 맞춰야 하며, 이 연구에서 개발된 업톤 분류기는 스트림 작업 또는 의미적 변화로 사용됩니다. 기초 모델을 위한 채팅 인터페이스 설계 개념의 시작점에 대한 응답. 참고문헌 :// Taori, Rohan et al. (2023). Stanford Alpaca: An Instruction-following LLAMA model. https:/ github.com/tatsu-lab/stanford_alpaca. Wang, Yizhong et al. (2023). Self-Instruct: Aligning Language Models with Self-Generated Instructions. arXiv: 2212.10560 [cs.CL]. Longpre, Shayne et al. (2023). The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. arXiv: 2301.13688 [cs.AI]. Zhou, Chunting et al. (2023). LIMA: Less Is More for Alignment. arXiv: 2305.11206 [cs.CL]. Anand, Yuvanesh et al. (2023). GPT4All: GPT-3.5-Turbo에서 대규모 데이터 증류를 사용하여 Assistant 스타일 챗봇 교육. https://github.com/nomic-ai/gpt4all. Touvron, Hugo et al. (2023). LLAMA: 개방적이고 효율적인 기초 언어 모델. arXiv: 2302.13971 [cs.CL]. Zhang, Susan et al. (2022). OPT: 개방형 사전 학습된 Transformer 언어 모델. arXiv: 2205. 01068 [cs.CL]. Gao, Leo 등(2020). &quot;The Pile: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트&quot;. in: arXiv 사전 인쇄본 arXiv:2101.00027. Writer(2023). Palmyra LLM은 비즈니스를 위한 안전하고 엔터프라이즈급 생성 AI를 강화합니다. Writer 블로그. URL: https://writer.com/blog/palmyra/. Gudibande, Arnav 등(2023). 독점적 LLM을 모방하는 허위 약속. arXiv: 2305. 15717 [cs.CL]. OpenAI(2022). ChatGPT: 대화를 위한 언어 모델 최적화. URL: https://onlinechatgpt.com/.Pichai, Sundar(2023). AI 여정의 중요한 다음 단계. Google AI 블로그. URL: https: //blog.google/intl/en-africa/products/explore-get-answers/an-importantnext-step-on-our-ai-journey/. Anthropic AI(2023). 클로드 소개. URL: https://www.anthropic.com/index/ introduction-claude. Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean(2015). 신경망에서 지식 추출. arXiv: 1503.02531 [stat.ML]. Liang, Percy et al.(2022). 언어 모델의 전체적 평가. arXiv: 2211.09110 [cs.CL]. Kwiatkowski, Tom et al.(2019). &quot;자연스러운 질문: 질의 응답 연구의 벤치마크&quot;. Transactions of the Association of Computational Linguistics에서. Huggingface(2023b). LLM 리더보드 열기. 액세스: 2023-06-10. URL: https:// huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard. Gao, Leo et al. (2021년 9월). A framework for few-shot language model evaluation. 버전 v0.0.1. DOI: 10.5281/zenodo. 5371628. URL: https://doi.org/10.5281/zenodo. 5371628. Chen, Hao et al. (2023). Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. arXiv: 2305.09246 [cs.AI]. Kumar, Ananya et al. (2022). Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. arXiv: 2202.10054 [cs.LG]. Brown, Tom B. et al. (2020). 언어 모델은 Few-Shot 학습자입니다. arXiv: 2005. 14165 [cs.CL]. Radford, Alec et al. (2018). &quot;언어 모델은 비지도 멀티태스크 학습자입니다&quot;. 출처: URL: https://d4mucfpksywv.cloudfront.net/better - language - models/language models.pdf. Ouyang, Long 등(2022). 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. arXiv: 2203.02155 [cs.CL]. Schulman, John 등(2017). 근접 정책 최적화 알고리즘. arXiv: 1707.[cs.LG]. Hendrycks, Dan 등(2021). 대규모 멀티태스크 언어 이해 측정. arXiv: 2009. 03300 [cs.CY]. Köpf, Andreas 등(2023). OpenAssistant 대화 - 대규모 언어 모델 정렬 민주화. arXiv: 2304.07327 [cs.CL]. Huggingface(2023a). AutoTrain: 코드 없이 강력한 AI 모델 생성. URL: https:// huggingface.co/autotrain. Wei, Jason et al.(2022). 대규모 언어 모델의 새로운 능력. arXiv: 2206.07682 [cs.CL].
