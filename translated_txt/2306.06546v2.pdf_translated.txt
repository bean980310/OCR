--- ABSTRACT ---
언어 모델은 이미지, 음성, 음악과 같은 자연 신호를 모델링하는 데 성공적으로 사용되었습니다. 이러한 모델의 핵심 구성 요소는 고차원 자연 신호를 저차원 이산 토큰으로 압축할 수 있는 고품질 신경 압축 모델입니다. 이를 위해 44.1KHz 오디오를 8kbps 대역폭에서 토큰으로 90배 압축하는 고충실도 범용 신경 오디오 압축 알고리즘을 소개합니다. 고충실도 오디오 생성의 발전과 이미지 도메인의 더 나은 벡터 양자화 기법, 개선된 적대적 손실 및 재구성 손실을 결합하여 이를 달성합니다. 모든 도메인(음성, 환경, 음악 등)을 단일 범용 모델로 압축하여 모든 오디오의 생성 모델링에 널리 적용할 수 있습니다. 경쟁 오디오 압축 알고리즘과 비교한 결과, 저희 방법이 훨씬 더 우수한 것으로 나타났습니다. 모든 설계 선택에 대한 철저한 절제와 오픈 소스 코드, 훈련된 모델 가중치를 제공합니다. 저희의 작업이 차세대 고충실도 오디오 모델링의 기반을 마련할 수 있기를 바랍니다. 1
--- INTRODUCTION ---
고해상도 오디오의 생성 모델링은 높은 차원성(초당 약 44,100개 오디오 샘플)[24, 19]과 단기 및 장기 종속성이 있는 다양한 시간 척도에 구조가 존재하기 때문에 어렵습니다. 이 문제를 완화하기 위해 오디오 생성은 일반적으로 두 단계로 나뉩니다. 1) 멜 스펙트로그램[24, 28, 19, 30]과 같은 일부 중간 표현을 조건으로 오디오를 예측하고 2) 텍스트[35, 34]와 같은 일부 조건 정보를 고려하여 중간 표현을 예측합니다. 이는 관찰된 중간 변수가 있는 계층적 생성 모델로 해석할 수 있습니다. 당연히 대체 공식은 VAE(변형 자동 인코더) 프레임워크를 사용하여 중간 변수를 학습하고 학습된 조건 사전을 사용하여 일부 조건이 주어진 잠재 변수를 예측하는 것입니다. 연속 잠재 변수를 사용하고 정규화 흐름을 사용하여 표현 사전을 학습하는 이 공식은 음성 합성에 매우 성공적이었습니다[17, 36]. 밀접하게 관련된 아이디어는 VQ-VAE를 사용하여 이산 잠재 변수로 동일한 변이 자동 인코더를 훈련하는 것입니다[38]. 이산 잠재 변수는 표현적 사전 확률을 이산 변수에 대한 분포를 모델링하기 위해 개발된 강력한 자기 회귀 모델을 사용하여 훈련할 수 있기 때문에 더 나은 선택이라고 할 수 있습니다[27]. 구체적으로, 변환기 언어 모델[39]은 이미 텍스트[6], 이미지[12, 44], 오디오[5, 41], 음악[1] 등과 같은 임의로 복잡한 분포를 학습하는 데이터와 모델 용량에 따라 확장할 수 있는 능력을 보여주었습니다. 사전 확률을 모델링하는 것은 간단하지만 양자화된 자동 인코더를 사용하여 이산 잠재 코드를 모델링하는 것은 여전히 어려운 일입니다. * 이 작업에 동등한 기여를 합니다. papers@descript.com으로 서신을 보내거나 https://github.com/descriptinc/descript-audio-codec에서 문제를 제기하세요. 37th Conference on Neural Information Processing Systems(NeurIPS 2023). 이러한 이산 코드를 학습하는 것은 손실 압축 작업으로 해석될 수 있으며, 여기서 오디오 신호는 고정 길이 코드북을 사용하여 자동 인코더의 표현을 벡터 양자화하여 이산 잠재 공간으로 압축됩니다. 이 오디오 압축 모델은 다음 속성을 충족해야 합니다. 1) 고충실도 및 아티팩트 없이 오디오를 재구성합니다. 2) 시간적 다운스케일링과 함께 높은 수준의 압축을 달성하여 고수준 구조를 유지하면서 저수준의 감지할 수 없는 세부 사항을 삭제하는 컴팩트한 표현을 학습합니다. [38, 33] 3) 단일 범용 모델을 사용하여 음성, 음악, 환경 소리, 다양한 오디오 인코딩(예: mp3) 및 다양한 샘플링 속도와 같은 모든 유형의 오디오를 처리합니다. SoundStream[46] 및 EnCodec[8]과 같은 최근의 신경 오디오 압축 알고리즘은 이러한 속성을 부분적으로 충족하지만 종종 GAN 기반 생성 모델을 괴롭히는 동일한 문제로 어려움을 겪습니다. 특히 이러한 모델은 음조 아티팩트[29], 피치 및 주기 아티팩트[25]와 같은 오디오 아티팩트를 나타내고, 원본과 명확히 구별되는 오디오로 이어지는 고주파를 불완전하게 모델링합니다.이러한 모델은 종종 음성이나 음악과 같은 특정 유형의 오디오 신호에 맞게 조정되며 일반적인 사운드를 모델링하는 데 어려움을 겪습니다.우리는 다음과 같은 기여를 합니다.• 우리는 최소한의 품질 손실과 더 적은 아티팩트로 44.1KHz 오디오를 8kbps 비트 전송률(~90배 압축)에서 개별 코드로 압축할 수 있는 고충실도 범용 오디오 압축 모델인 개선된 RVQGAN을 소개합니다.우리의 모델은 정량적 지표와 정성적 청취 테스트로 평가할 때 낮은 비트 전송률(더 높은 압축)에서도 최첨단 방법보다 훨씬 우수한 성능을 보입니다.• 우리는 코드북 붕괴(일부 코드가 사용되지 않는 경우)로 인해 전체 대역폭을 활용하지 못하는 기존 모델에서 중요한 문제를 식별하고 개선된 코드북 학습 기술을 사용하여 이를 수정합니다. • 우리는 양자화기 드롭아웃의 부작용을 식별합니다. 이 기술은 단일 모델이 가변 비트레이트를 지원하도록 설계되었지만 실제로는 전체 대역폭 오디오 품질을 손상시키고 이를 완화하기 위한 솔루션을 제안합니다. • 우리는 주기적 유도 바이어스, 다중 스케일 STFT 판별기, 다중 스케일 멜 손실을 추가하여 기존 신경 오디오 코덱에 영향력 있는 설계 변경을 하고 이를 동기를 부여하기 위한 철저한 절제와 직관을 제공합니다. •⚫ 제안된 방법은 음성, 음악, 환경 소리, 다양한 샘플링 속도 및 오디오 인코딩 형식을 처리할 수 있는 범용 오디오 압축 모델입니다. 우리는 독자가 들어보도록 권장하는 코드 ¹, 모델 및 오디오 샘플 2를 제공합니다. 2
--- RELATED WORK ---
고충실도 신경 오디오 합성: 최근, 생성적 적대 신경망(GAN)은 피드포워드(병렬) 생성기로 인해 빠른 추론 속도로 고품질 오디오를 생성하는 솔루션으로 등장했습니다. MelGAN[19]은 GAN 기반 스펙트로그램 역전(신경 보코딩) 모델을 성공적으로 훈련합니다. 이는 다양한 오디오 해상도에서 구조를 페널티하는 다중 스케일 파형 판별기(MSD)와 실제 및 합성 오디오의 판별기 특징 맵 간 L1 거리를 최소화하는 특징 매칭 손실을 도입합니다. HifiGAN[18]은 고충실도 합성을 위한 다중 주기 파형 판별기(MPD)를 도입하고 빠른 훈련을 위한 보조 멜 재구성 손실을 추가하여 이 방법을 개선합니다. UnivNet[16]은 선명한 스펙트로그램을 가진 오디오를 생성하는 다중 해상도 스펙트로그램 판별기(MRSD)를 도입합니다. BigVGAN[21]은 Snake 활성화 함수[47]를 사용하여 주기적 유도 바이어스를 도입하여 HifiGAN 레시피를 확장합니다. 또한 HifiGAN의 MSD를 MRSD로 대체하여 오디오 품질을 개선하고 피치, 주기적 아티팩트를 줄입니다[25]. 이러한 GAN 기반 학습 기술은 보코딩에 사용되지만 이러한 레시피는 신경 오디오 압축에 쉽게 적용할 수 있습니다. 개선된 RVQGAN 모델은 몇 가지 주요 변경 사항을 통해 BigVGAN 학습 레시피를 밀접하게 따릅니다. 당사 모델은 앨리어싱 아티팩트를 완화하는 새로운 다중 대역 다중 스케일 STFT 판별기와 빠른 과도 현상을 더 잘 모델링하는 다중 스케일 멜 재구성 손실을 사용합니다. 신경 오디오 압축 모델: VQ-VAES[38]는 신경 오디오 코덱을 학습하는 주요 패러다임이었습니다. 최초의 VQ-VAE 기반 음성 코덱은 1.6kbps에서 작동하는 [13]에서 제안되었습니다. 이 모델은 합성 인코더와 자기 회귀 ¹https://github.com/descriptinc/descript-audio-codec 2 https://descript. notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814dwavenet [27] 디코더를 갖춘 [38]의 원래 아키텍처를 사용했습니다. SoundStream [46]은 단일 모델을 사용하여 다양한 비트 전송률을 지원하면서 다양한 오디오 유형을 처리할 수 있는 최초의 범용 압축 모델 중 하나입니다. 이들은 완전한 인과 합성 인코더 및 디코더 네트워크를 사용하고 잔여 벡터 양자화(RVQ)를 수행합니다. 이 모델은 다중 스케일 스펙트럼 재구성 손실과 함께 적대적 및 특징 매칭 손실을 추가하여 VQ-GAN [12] 공식을 사용하여 학습됩니다. EnCodec [8]은 개선된 품질로 이어지는 몇 가지 수정 사항이 있는 SoundStream 레시피를 밀접하게 따릅니다. EnCodec은 다중 스케일 스펙트럼 재구성 손실이 있는 다중 스케일 STFT 판별기를 사용합니다. 그들은 판별기에서 나오는 그래디언트의 다양한 스케일에 따라 손실 가중치를 조정하는 손실 밸런서를 사용합니다. 제안된
--- METHOD ---
성능이 상당히 뛰어납니다. 모든 설계 선택에 대한 철저한 절제와 오픈 소스 코드, 학습된 모델 가중치를 제공합니다. 저희의 작업이 차세대 고충실도 오디오 모델링의 기반을 마련할 수 있기를 바랍니다. 1 서론 고해상도 오디오의 생성 모델링은 차원성이 높고(초당 약 44,100개 오디오 샘플) [24, 19], 단기 및 장기 종속성이 있는 다양한 시간 척도에 구조가 존재하기 때문에 어렵습니다. 이 문제를 완화하기 위해 오디오 생성은 일반적으로 두 단계로 나뉩니다. 1) 멜 스펙트로그램 [24, 28, 19, 30]과 같은 일부 중간 표현에 따라 오디오를 예측하고 2) 텍스트 [35, 34]와 같은 일부 조건 정보를 고려하여 중간 표현을 예측합니다. 이는 관찰된 중간 변수가 있는 계층적 생성 모델로 해석할 수 있습니다. 자연스럽게 대체 공식은 VAE(변형 자동 인코더) 프레임워크를 사용하여 중간 변수를 학습하고, 학습된 조건 사전을 사용하여 일부 조건이 주어진 잠재 변수를 예측하는 것입니다. 연속 잠재 변수를 사용하고 정규화 흐름을 사용하여 표현 사전을 학습하는 이 공식은 음성 합성에 매우 성공적이었습니다[17, 36]. 밀접하게 관련된 아이디어는 VQ-VAE를 사용하여 이산 잠재 변수로 동일한 변형 자동 인코더를 학습하는 것입니다[38]. 이산 잠재 변수가 더 나은 선택이라고 주장할 수 있는데, 표현 사전은 이산 변수에 대한 분포를 모델링하기 위해 개발된 강력한 자기 회귀 모델을 사용하여 학습할 수 있기 때문입니다[27]. 구체적으로, 변환기 언어 모델[39]은 이미 텍스트[6], 이미지[12, 44], 오디오[5, 41], 음악[1] 등과 같은 임의로 복잡한 분포를 학습하는 데이터와 모델 용량에 따라 확장할 수 있는 능력을 보여주었습니다. 사전 모델링은 간단하지만 양자화된 자동 인코더를 사용하여 이산 잠재 코드를 모델링하는 것은 여전히 어려운 일입니다. * 이 작업에 대한 동등한 기여. papers@descript.com으로 서신을 보내거나 https://github.com/descriptinc/descript-audio-codec에서 문제를 제기하세요. 제37회 신경 정보 처리 시스템 컨퍼런스(NeurIPS 2023). 이러한 이산 코드를 학습하는 것은 손실 압축 작업으로 해석할 수 있으며, 여기서 오디오 신호는 고정 길이 코드북을 사용하여 자동 인코더의 표현을 벡터 양자화하여 이산 잠재 공간으로 압축됩니다. 이 오디오 압축 모델은 다음과 같은 속성을 충족해야 합니다. 1) 고충실도 및 아티팩트 없는 오디오 재구성 2) 낮은 수준의 감지할 수 없는 세부 사항을 버리고 높은 수준의 구조를 보존하는 컴팩트한 표현을 학습하기 위해 시간적 다운스케일링과 함께 높은 수준의 압축 달성 [38, 33] 3) 단일 범용 모델을 사용하여 음성, 음악, 환경음, 다양한 오디오 인코딩(예: mp3) 및 다양한 샘플링 속도와 같은 모든 유형의 오디오 처리. SoundStream [46] 및 EnCodec [8]과 같은 최근의 신경 오디오 압축 알고리즘은 이러한 속성을 부분적으로 충족하지만 종종 GAN 기반 생성 모델을 괴롭히는 동일한 문제로 어려움을 겪습니다. 특히 이러한 모델은 음조 아티팩트 [29], 피치 및 주기 아티팩트 [25]와 같은 오디오 아티팩트를 나타내고 고주파수를 불완전하게 모델링하여 원본과 명확히 구별되는 오디오를 생성합니다. 이러한 모델은 종종 음성이나 음악과 같은 특정 유형의 오디오 신호에 맞게 조정되며 일반적인 소리를 모델링하는 데 어려움을 겪습니다. 우리는 다음과 같은 기여를 합니다. • 우리는 44.1KHz 오디오를 8kbps 비트 전송률(~90배 압축)에서 최소한의 품질 손실과 더 적은 아티팩트로 이산 코드로 압축할 수 있는 고충실도 범용 오디오 압축 모델인 개선된 RVQGAN을 소개합니다. 우리 모델은 양적 지표와 질적 청취 테스트로 평가할 때 더 낮은 비트 전송률(더 높은 압축률)에서도 최첨단 방법보다 큰 폭으로 성능이 뛰어납니다. • 우리는 코드북 붕괴(일부 코드가 사용되지 않는 경우)로 인해 전체 대역폭을 활용하지 못하는 기존 모델에서 중요한 문제를 식별하고 개선된 코드북 학습 기술을 사용하여 이를 수정합니다. • 우리는 단일 모델이 가변 비트 전송률을 지원하도록 설계된 기술인 양자화기 드롭아웃의 부작용을 식별하지만 실제로는 전체 대역폭 오디오 품질에 해를 끼치고 이를 완화하기 위한 솔루션을 제안합니다. • 주기적 유도 바이어스, 다중 스케일 STFT 판별기, 다중 스케일 멜 손실을 추가하여 기존 신경 오디오 코덱에 영향력 있는 설계 변경을 하고 이를 동기를 부여하는 철저한 절제와 직관을 제공합니다.•⚫ 제안하는 방법은 음성, 음악, 환경 소리, 다양한 샘플링 속도 및 오디오 인코딩 형식을 처리할 수 있는 범용 오디오 압축 모델입니다. 독자가 들어보시기를 권장하는 코드 ¹, 모델 및 오디오 샘플 2를 제공합니다.2 관련 작업 고충실도 신경 오디오 합성: 최근 피드포워드(병렬) 생성기로 인해 생성적 적대 신경망(GAN)이 빠른 추론 속도로 고품질 오디오를 생성하는 솔루션으로 등장했습니다.MelGAN[19]은 GAN 기반 스펙트로그램 역전(신경 보코딩) 모델을 성공적으로 학습시킵니다.다양한 오디오 해상도에서 구조를 페널티하는 다중 스케일 파형 판별기(MSD)와 실제 및 합성 오디오의 판별기 기능 맵 간의 L1 거리를 최소화하는 기능 매칭 손실을 도입합니다. HifiGAN[18]은 고충실도 합성을 위한 다중 주기 파형 판별기(MPD)를 도입하고 빠른 학습을 위한 보조 멜 재구성 손실을 추가하여 이 방법을 개선합니다. UnivNet[16]은 선명한 스펙트로그램이 있는 오디오를 생성하기 위해 다중 해상도 스펙트로그램 판별기(MRSD)를 도입합니다. BigVGAN[21]은 Snake 활성화 함수[47]를 사용하여 주기적 유도 바이어스를 도입하여 HifiGAN 방법을 확장합니다. 또한 HifiGAN의 MSD를 MRSD로 대체하여 오디오 품질을 개선하고 피치, 주기 아티팩트를 줄입니다[25]. 이러한 GAN 기반 학습 기술은 보코딩에 사용되지만 이러한 방법은 신경 오디오 압축에 쉽게 적용할 수 있습니다. 개선된 RVQGAN 모델은 몇 가지 주요 변경 사항과 함께 BigVGAN 학습 방법을 밀접하게 따릅니다. 우리 모델은 앨리어싱 아티팩트를 완화하는 새로운 다중 대역 다중 스케일 STFT 판별기와 빠른 과도 현상을 더 잘 모델링하는 다중 스케일 멜 재구성 손실을 사용합니다.신경 오디오 압축 모델: VQ-VAES[38]는 신경 오디오 코덱을 훈련하는 주요 패러다임이었습니다.최초의 VQ-VAE 기반 음성 코덱은 1.6kbps에서 작동하는 [13]에서 제안되었습니다.이 모델은 합성 인코더와 자기 회귀 ¹https://github.com/descriptinc/descript-audio-codec 2 https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814dwavenet [27] 디코더가 있는 [38]의 원래 아키텍처를 사용했습니다.SoundStream[46]은 단일 모델을 사용하여 다양한 비트 전송률을 지원하면서 다양한 오디오 유형을 처리할 수 있는 최초의 범용 압축 모델 중 하나입니다. 그들은 완전 인과적 합성 인코더 및 디코더 네트워크를 사용하고 잔여 벡터 양자화(RVQ)를 수행합니다. 이 모델은 다중 스케일 스펙트럼 재구성 손실과 함께 적대적 및 특징 매칭 손실을 추가하여 VQ-GAN[12] 공식을 사용하여 학습됩니다. EnCodec[8]은 몇 가지 수정을 거쳐 품질이 향상된 SoundStream 레시피를 밀접하게 따릅니다. EnCodec은 다중 스케일 스펙트럼 재구성 손실이 있는 다중 스케일 STFT 판별기를 사용합니다. 그들은 판별기에서 나오는 그래디언트의 다양한 스케일에 따라 손실 가중치를 조정하는 손실 밸런서를 사용합니다. 제안하는 방법은 또한 합성 인코더-디코더 아키텍처, 잔여 벡터 양자화 및 적대적, 지각적 손실을 사용합니다. 그러나 우리의 레시피는 다음과 같은 주요 차이점이 있습니다.1) Snake 활성화를 사용하여 주기적 귀납적 바이어스를 도입합니다[47, 21].2) 인코딩을 저차원 공간으로 투영하여 코드북 학습을 개선합니다[44].3) 고정된 손실 가중치와 정교한 손실 밸런서가 필요하지 않은 적대적 및 지각적 손실 설계에 대한 모범 사례를 사용하여 안정적인 교육 레시피를 얻습니다.우리는 우리의 변경 사항이 거의 최적의 효과적인 대역폭 사용으로 이어진다는 것을 발견했습니다.이를 통해 우리 모델은 비트 전송률이 3배 낮더라도 EnCodec보다 우수한 성능을 발휘할 수 있습니다.자연 신호의 언어 모델링: 신경 언어 모델은 컨텍스트 내 학습 기능이 있는 개방형 텍스트 생성[6]과 같은 다양한 작업에서 큰 성공을 거두었습니다.이러한 모델의 핵심 구성 요소는 자기 주의[39]로, 복잡하고 장거리 종속성을 모델링할 수 있지만 시퀀스 길이에 따라 2차 계산 비용이 발생합니다. 이 비용은 매우 높은 차원을 가진 이미지 및 오디오와 같은 자연 신호에는 받아들일 수 없으며, 이산 표현 공간으로의 컴팩트한 매핑이 필요합니다. 이 매핑은 일반적으로 VQ-GANS[12, 44]를 사용하여 학습한 다음 이산 토큰에서 자기 회귀 변환기를 학습합니다. 이 접근 방식은 이미지[45, 44, 32], 오디오[5, 41], 비디오 및 음악[9, 1] 도메인에서 성공을 거두었습니다. SoundStream 및 EnCodec과 같은 코덱은 이미 AudioLM[5], MusicLM[1], VALL-E[41]와 같은 생성 오디오 모델에서 사용되었습니다. 제안하는 모델은 이러한 방법에서 사용되는 오디오 토큰화 모델을 대체하는 드롭인 모델로 사용할 수 있어 매우 뛰어난 오디오 충실도와 최대 엔트로피 코드 표현으로 인한 보다 효율적인 학습이 가능합니다. 3 개선된 RVQGAN 모델 샘플링 속도(kHz) (a) a 스트라이딩 계수 프레임 속도(Hz) 10비트 코드북의 58%23 압축 계수 당사 모델은 SoundStream [46] 및 EnCodec [8]과 동일한 패턴을 따르는 VQ-GANS 프레임워크를 기반으로 구축됩니다. 당사 모델은 선택한 스트라이딩 계수로 시간적 다운스케일링을 수행하는 SoundStream의 완전 합성 인코더-디코더 네트워크를 사용합니다. 최근 문헌에 따라 초기 양자화 단계에 따라 별도의 코드북을 사용하여 잔차를 재귀적으로 양자화하는 방법인 잔차 벡터 양자화(RVQ)를 사용하여 인코딩을 양자화합니다. 양자화기 드롭아웃은 여러 대상 비트레이트에서 작동할 수 있는 단일 모델을 활성화하기 위해 학습 중에 적용됩니다. 당사 모델은 표 1: 압축 방식 비교를 사용하여 유사하게 학습됩니다. 주파수 영역 재구성 손실과 적대적 손실 및 지각적 손실을 함께 사용합니다. Codec Proposed 44.EnCodec SoundStream51291.24 24 32048 24 320 150 1624 6 320 75 8 샘플링 속도 fs(Hz), 인코더 스트라이드 계수 M, RVQ의 Ng 계층을 갖는 오디오 신호는 S × N₁ 모양의 이산 코드 행렬을 생성합니다. 여기서 S&#39;는 fs/M으로 정의된 프레임 속도입니다. 표 1은 제안된 모델을 베이스라인과 비교하여 압축 계수와 잠재 코드의 프레임 속도를 대조합니다. 언급된 목표 비트레이트는 모든 모델이 가변 비트레이트를 지원하므로 상한입니다. 나중에 보여드리겠지만, 저희 모델은 오디오 품질 면에서 더 우수한 성능을 보이는 동시에 모든 베이스라인 방법에 비해 더 높은 압축 계수를 달성합니다. 마지막으로, 이산 코드에서 언어 모델을 학습할 때는 더 낮은 프레임 속도가 바람직한데, 이는 시퀀스가 더 짧아지기 때문입니다. 3.1 주기적 활성화 함수 오디오 파형은 높은 주기성을 보이는 것으로 알려져 있습니다(특히 음성 구성 요소, 음악 등). 현재의 비자기 회귀 오디오 생성 아키텍처는 고충실도 오디오를 생성할 수 있지만 종종 갑작스러운 피치 및 주기성 아티팩트를 보입니다[25]. 게다가 공통 신경망 활성화(예: Leaky ReLU)는 주기적 신호를 외삽하는 데 어려움을 겪고 오디오 합성에 대한 분포 외 일반화가 좋지 않은 것으로 알려져 있습니다[21]. 생성기에 주기적 유도 바이어스를 추가하기 위해 Liu 등이 제안하고[47] BigVGAN 신경 보코딩 모델[21]의 오디오 도메인에 도입한 Snake 활성화 함수를 채택합니다. 이 함수는 snake(x) = x + 1½-½ sin²(ax)로 정의되며, 여기서 a는 신호의 주기적 구성 요소의 주파수를 제어합니다. 우리의
--- EXPERIMENT ---
s, Leaky ReLU 활성화를 Snake 함수로 대체하는 것이 오디오 충실도를 크게 개선하는 영향력 있는 변경 사항임을 발견했습니다(표 2). 3.2 개선된 잔여 벡터 양자화 엔트로피(비트) 10.9.9.9.9.9.8.제안(24kbps) -- 제안 EnCodec 제안(EMA 포함) 코드북 인덱스 그림 1: 대규모 테스트 세트에서 코드 사용 통계를 사용하여 계산한 각 코드북의 엔트로피. 벡터 양자화(VQ)는 이산 자동 인코더를 훈련하는 데 널리 사용되는 방법이지만, 이를 훈련할 때는 많은 실질적인 어려움이 있습니다. Vanilla VQ-VAES는 초기화가 제대로 되지 않아 코드북 사용량이 낮아 코드북의 상당 부분이 사용되지 않아 어려움을 겪습니다. 효과적인 코드북 크기가 감소하면 대상 비트 전송률이 암묵적으로 감소하여 재구성 품질이 저하됩니다. 이를 완화하기 위해 최근 오디오 코덱 방법은 kmeans 클러스터링을 사용하여 코드북 벡터를 초기화하고 특정 코드북이 여러 배치에서 사용되지 않을 때 무작위 재시작[9]을 수동으로 사용합니다. 그러나 24kbps 대상 비트 전송률에서 학습된 EnCodec 모델과 동일한 코드북 학습 방법을 사용하는 제안 모델(EMA 포함)은 여전히 코드북 활용도가 낮은 것으로 나타났습니다(그림 1). 이 문제를 해결하기 위해 개선된 VQGAN 이미지 모델[44]에 도입된 두 가지 핵심 기술인 인수분해 코드와 L2 정규화 코드를 사용하여 코드북 사용을 개선합니다. 인수분해는 저차원 공간(8d 또는 32d)에서 코드 조회를 수행하는 반면 코드 임베딩은 고차원 공간(1024d)에 있으므로 코드 조회와 코드 임베딩을 분리합니다. 직관적으로 이는 데이터의 분산을 최대로 설명하는 입력 벡터의 주성분만을 사용하는 코드 조회로 해석할 수 있습니다. 인코딩된 벡터와 코드북 벡터의 L2-정규화는 유클리드 거리를 코사인 유사도로 변환하여 안정성과 품질에 도움이 됩니다[44]. 이 두 가지 트릭과 전체 모델 레시피는 코드북 사용을 크게 개선하고, 따라서 비트 전송률 효율성(그림 1)과 재구성 품질(표 2)을 개선하는 동시에 구현이 더 간단합니다. 우리 모델은 k-평균 초기화나 무작위 재시작 없이 원래의 VQ-VAE 코드북과 커밋먼트 손실[38]을 사용하여 학습할 수 있습니다. 수정된 코드북 학습 절차에 대한 방정식은 부록 A에 기록되어 있습니다. 3.3 양자화기 드롭아웃 비율 ~ 양자화기 드롭아웃은 가변 비트 전송률로 단일 압축 모델을 학습하기 위해 SoundStream[46]에 도입되었습니다. 양자화기 수 Nå는 비트 전송률을 결정하므로 각 입력 예제에서 무작위로 n{1, 2,..., Ng}을 샘플링하고 학습하는 동안 처음 ng개의 양자화기만 사용합니다. 그러나 양자화기 드롭아웃을 적용하면 전체 대역폭에서 오디오 재구성 품질이 저하되는 것을 발견했습니다(그림 2). 이 문제를 해결하기 위해, 우리는 대신 각 입력 예제에 확률 p로 양자화기 드롭아웃을 적용합니다. 흥미롭게도, 드롭아웃 확률 p = 0.5가 낮은 비트레이트에서 베이스라인의 재구성 품질과 거의 일치하는 반면, 양자화기 드롭아웃 없이 학습된 모델의 전체 대역폭 품질과의 격차를 좁히는 것을 발견했습니다(p = 0.0). Mel 재구성 손실 3.3.2.2.†1. 드롭아웃 확률 -0.-#- 0.--- 0.1.1. 비트레이트(kbps) 그림 2: 양자화기 드롭아웃이 오디오 품질에 미치는 영향 대 비트레이트. 또한, 양자화기 드롭아웃의 실제 동작과 RVQ와의 상호 작용에 대한 추가적인 통찰력을 제공합니다. 첫째, 이러한 기술을 함께 사용하면 양자화된 코드가 각 추가 양자화기에서 가장 중요한 정보 비트에서 가장 덜 중요한 정보 비트로 학습하게 됩니다. 코드가 1... Na 코드북으로 재구성되면 각 코드북이 점점 더 많은 양의 미세 스케일 세부 정보를 추가하는 것을 볼 수 있습니다. 우리는 이러한 코드[5, 41, 1] 위에 계층적 생성 모델을 훈련할 때 이러한 상호작용이 유익하다고 믿습니다.예를 들어, 코드를 &quot;거친&quot; 토큰(가장 중요한 코드를 나타냄)과 &quot;미세한&quot; 토큰으로 분할하는 데 유용합니다.3.4 판별기 설계 이전 작업과 마찬가지로 다중 스케일(MSD) 및 다중 주기 파형 판별기(MPD)를 사용하여 오디오 충실도를 개선합니다.그러나 생성된 오디오의 스펙트로그램은 여전히 흐릿하게 나타날 수 있으며 고주파에서 과도하게 평활화된 아티팩트가 나타납니다[16].다중 해상도 스펙트로그램 판별기(MRSD)는 UnivNet에서 이러한 아티팩트를 수정하기 위해 제안되었으며 BigVGAN[21]은 피치 및 주기 아티팩트를 줄이는 데에도 도움이 된다는 것을 발견했습니다.그러나 크기 스펙트로그램을 사용하면 판별기가 위상 모델링 오류를 처벌하기 위해 사용할 수 있는 위상 정보가 삭제됩니다.또한 고주파 모델링은 특히 높은 샘플링 속도에서 이러한 모델에 여전히 어려운 문제라는 것을 발견했습니다. 이러한 문제를 해결하기 위해 여러 시간 척도[8]에서 복합 STFT 판별기[46]를 사용했고 실제로 더 잘 작동하고 위상 모델링이 개선되는 것을 발견했습니다.또한 STFT를 하위 대역으로 분할하면 판별기가 특정 하위 대역에 대한 판별 기능을 학습하고 생성기에 더 강한 그래디언트 신호를 제공할 수 있으므로 고주파 예측이 약간 개선되고 앨리어싱 아티팩트가 완화되는 것을 발견했습니다.다중 대역 처리가 이전에 [43]에서 제안되어 하위 대역의 오디오를 예측한 다음 이를 합산하여 전체 대역 오디오를 생성했습니다.3.5 손실 함수 주파수 영역 재구성 손실: 멜 재구성 손실[18]은 안정성, 충실도 및 수렴 속도를 개선하는 것으로 알려져 있지만 다중 대역 스펙트럼 손실[42, 11, 15]은 여러 시간 척도에서 주파수 모델링을 장려합니다. 우리 모델에서는 윈도우 길이 [32, 64, 128, 256, 512, 1024, 2048]로 계산된 멜 스펙트로그램에서 L 손실을 사용하고 홉 길이를 window_length / 4로 설정하여 두 가지 방법을 결합합니다. 특히 가장 낮은 홉 크기 8을 사용하면 음악 도메인에서 특히 흔한 매우 빠른 과도 현상의 모델링이 개선된다는 것을 발견했습니다. EnCodec [8]은 유사한 손실 공식을 사용하지만 L1 및 L2 손실 항과 고정된 멜 빈 크기 64를 사용합니다. 멜 빈 크기를 고정하면 특히 낮은 필터 길이에서 스펙트로그램에 홀이 생기는 것을 발견했습니다. 따라서 수동 검사를 통해 올바른 것으로 확인된 위의 필터 길이에 해당하는 멜 빈 크기 [5, 10, 20, 40, 80, 160, 320]를 사용합니다. 적대적 손실: 우리 모델은 파형 구별을 위해 다중 주기 판별기[18]를 사용하고, 주파수 영역에는 제안된 다중 대역 다중 스케일 STFT 판별기를 사용합니다.우리는 HingeGAN[22] 적대적 손실 공식을 사용하고 L1 특징 매칭 손실[19]을 적용합니다.코드북 학습: 우리는 원래 VQ-VAE 공식[38]의 정지 기울기를 가진 단순 코드북 및 커밋먼트 손실을 사용하고, 직접 추정기[3]를 사용하여 코드북 조회를 통해 기울기를 역전파합니다.손실 가중치: 우리는 다중 스케일 멜 손실에 대해 15.0, 특징 매칭 손실에 대해 2.0, 적대적 손실에 대해 1.0, 코드북 및 커밋먼트 손실에 대해 각각 1.0, 0.25의 손실 가중치를 사용합니다. 이러한 손실 가중치는 최근 연구[18, 21](멜 손실에 45.0 가중치 사용)와 일치하지만, 멜 손실을 계산하는 데 사용한 여러 스케일과 log 10 기반을 설명하기 위해 간단히 재조정되었습니다. EnCodec[8]에서 제안한 손실 밸런서는 사용하지 않습니다. 4 실험 4.1 데이터 소스 우리는 음성, 음악, 환경 소리로 편집된 대규모 데이터 세트에서 모델을 훈련합니다. 음성의 경우 DAPS 데이터 세트[26], DNS Challenge 4[10]의 깨끗한 음성 세그먼트, Common Voice 데이터 세트[2], VCTK 데이터 세트[40]를 사용합니다. 음악의 경우 MUSDB 데이터 세트[31]와 Jamendo 데이터 세트[4]를 사용합니다. 마지막으로 환경 소리의 경우 AudioSet[14]의 균형 잡힌 및 불균형한 훈련 세그먼트를 모두 사용합니다. 모든 오디오는 44kHz로 리샘플링됩니다. 훈련 중에 각 오디오 파일에서 짧은 발췌 부분을 추출하여 -24dB LUFS로 정규화합니다. 적용하는 유일한 데이터 증가는 발췌 부분의 위상을 무작위로 균일하게 이동하는 것입니다. 평가를 위해 AudioSet [14]의 평가 세그먼트, 음성을 위해 DAPS [26]에서 보류된 두 명의 화자(F10, M10), MUSDB [31]의 테스트 분할을 사용합니다. 테스트 세트로 3000개의 10초 세그먼트(각 도메인에서 1000개)를 추출합니다. 4.2 균형 잡힌 데이터 샘플링 데이터 세트에서 샘플링하는 방법에는 특별한 주의를 기울입니다. 데이터 세트는 44kHz로 리샘플링되었지만 그 안의 데이터는 어떤 식으로든 대역 제한이 있을 수 있습니다. 즉, 일부 오디오는 원래 샘플링 속도가 44kHz보다 훨씬 낮았을 수 있습니다. 이는 기본 데이터의 실제 샘플링 속도가 크게 달라질 수 있는 음성 데이터에서 특히 흔합니다(예: Common Voice 데이터는 일반적으로 8-16kHz). 다양한 샘플링 속도로 모델을 훈련했을 때, 결과 모델이 종종 특정 주파수 이상의 데이터를 재구성하지 못한다는 것을 발견했습니다. 조사해보니 이 임계 주파수가 데이터 세트의 평균 실제 샘플링 속도와 일치한다는 것을 발견했습니다. 이를 해결하기 위해 균형 잡힌 데이터 샘플링 기법을 도입했습니다. 먼저 데이터 세트를 전체 대역인 것으로 알려진 데이터 소스(코덱의 원하는 나이퀴스트 주파수(22.05kHz)까지의 주파수에서 에너지를 포함하는 것으로 확인됨)와 최대 주파수를 보장할 수 없는 데이터 소스로 분할했습니다. 배치를 샘플링할 때 전체 대역 항목이 샘플링되었는지 확인합니다. 마지막으로 각 배치에 음성, 음악 및 환경 소리의 각 도메인에서 동일한 수의 항목이 있는지 확인합니다. 절제 연구에서 이 균형 잡힌 샘플링 기법이 모델 성능에 어떤 영향을 미치는지 살펴봅니다. 4.3 모델 및 훈련 레시피 모델은 합성곱 인코더, 잔차 벡터 양자화기, 합성곱 디코더로 구성됩니다. 네트워크의 기본 구성 요소는 스트라이드를 사용하여 업샘플링 또는 다운샘플링하는 합성곱 계층이며, 그 뒤에 비선형 Snake 활성화로 끼워 넣은 합성곱 계층으로 구성된 잔여 계층이 옵니다. 인코더에는 이러한 계층이 4개 있으며, 각각은 입력 오디오 파형을 [2,4,8,8]의 속도로 다운샘플링합니다. 디코더에는 [8,8,4,2]의 속도로 업샘플링하는 해당 계층이 4개 있습니다. 디코더 차원을 1536으로 설정했습니다. 총 76M개의 매개변수가 있으며, 인코더에 22M, 디코더에 54M이 있습니다. 또한 512(31M 매개변수)와 1024(49M 매개변수)의 디코더 차원도 살펴봅니다. 다중 주기 판별기[18]와 복소수 다중 스케일 STFT 판별기를 사용합니다. 첫 번째로, 우리는 [2, 3, 5, 7, 11]의 주기를 사용하고 두 번째로 우리는 [2048, 1024, 512]의 윈도우 길이를 사용하는데, 홉 길이는 윈도우 길이의 1/4입니다. STFT의 대역 분할을 위해 우리는 대역 한계 [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]을 사용합니다. 재구성 손실의 경우, 우리는 [32, 64, 128, 256, 512, 1024, 2048]의 윈도우 길이를 갖는 log-mel 스펙트로그램 사이의 거리를 사용하는데, 각각의 멜 수는 [5, 10, 20, 40, 80, 160, 320]입니다. 홉 길이는 윈도우 길이의 1/4입니다. 우리는 섹션 3.5에서 설명한 대로 특징 매칭과 코드북 손실을 사용합니다. 우리의 절제 연구를 위해, 우리는 250k 반복을 위해 12의 배치 크기로 각 모델을 훈련합니다. 실제로, 이것은 단일 GPU에서 훈련하는 데 약 30시간이 걸립니다. 우리의 최종 모델은 400k 반복을 위해 72의 배치 크기로 훈련합니다. 우리는 0.38초 동안의 발췌문으로 훈련합니다. 우리는 생성기와 판별기 모두에 대해 학습 속도가 le- - 4, ẞ1 0.8, 2 = 0.9인 AdamW 최적화 도구[23]를 사용합니다. 우리는 Y = = 0.999996인 모든 단계에서 학습 속도를 감소시킵니다. 4.4 객관적 및 주관적 지표 우리는 우리의 모델을 평가하기 위해 다음과 같은 객관적 지표를 사용합니다: 1. ViSQOL[7]: 평균 의견 점수를 추정하기 위해 기준 진실과의 스펙트럼 유사성을 사용하는 침입적 지각적 품질 지표. 2. 멜 거리: 재구성된 파형과 실제 파형의 로그 멜 스펙트로그램 사이의 거리. 이 손실의 구성은 3.5에서 설명한 것과 동일합니다. 디코더 희미함. 활성화 다중 주기 × 단일 스케일 STFT 대역 수 다중 스케일 멜. ∞ 아키텍처의 잠재 희미한 소거 1536 snake ✓ × 5✓ 512 snake ✓1024 snake X 5✓ 1536 relu ✓ X✓ 판별기 1536 snake XXX ✓ 1536 snake ✓ X 1536 snake X X1536 snake재구성 손실 1536 snake ☑✓X X1536 snake 1536 snake ✓ ✓ 잠재 희미1536 snake X1536 snake ✓☑ 1536 snake ✔ X 양자화 설정 1536 snake ✓ X 1536 snake ✓ X 1536 snake ✓ X 1536 snake ✓ ☑데이터 1536 snake ✓ ☑ކ xކ ކ 양적 드롭아웃 비트 전송률(kbps) ☐ 균형 잡힌 샘플. 멜 거리↓ o STFT 거리↓ ViSQOL ↑ SI-SDR ↑ 비트레이트 효율 ↑ 8 Proj. 1.01.1.8| 프로젝트. 8 프로젝트. 8 프로젝트. 1.99998 프로젝트. 1.8 프로젝트. 1.8 프로젝트. 1.8 프로젝트. 1.8| 프로젝트. 1.2 프로젝트. 1.4 프로젝트. 1.프로젝트 1.256 프로젝트. 1.✓ 1.09 1.82 3.96 9.12 99% 1.11 1.83 3.91 8.72 99% 1.07 1.82 3.96 9.07 99% 1.17 1.81 3.83 6.92 99% 1.13 1.92 4.1.62% 1.07 1.80 3.9.99% ✓ 1.07 1.81 3.97 9.04 99% 1.08 1.82 3.8.51 99%1.10 1.87 4.7.68 99% 1.44 2.08 3.2.22 84%8 EMA 1.08 프로젝트. 0.08 프로젝트 0.258 프로젝트. 0.58 프로젝트. 1.0ke |✓ X 5|✓| 8| 프로젝트. 1.✓ 1.20 1.89 3.86 7.15 97% ✓ 1.10 1.84 3.95 9.05 98% 1.31 1.97 3.79 5.09 59% 1.11 1.84 3.94 8.33 97% 0.98 1.70 4.09 이 1.09 1.94 3.89 8.89 99% 표 2: 제안된 코덱에 대한 절제 연구 결과. 최종 모델은 기준선(맨 위 행)과 동일한 구성으로 학습되었지만 양자화 드롭아웃은 0.5입니다. 3. STFT 거리: 재구성된 파형과 기준 진실 파형의 로그 크기 스펙트로그램 간 거리입니다. 창 길이[2048, 512]를 사용합니다. 이 메트릭은 멜 거리보다 높은 주파수의 충실도를 더 잘 포착합니다. 4. 스케일 불변 소스 대 왜곡 비율(SI-SDR)[20]: 스케일 차이에 불변하도록 수정된 신호 대 잡음 비율과 유사한 파형 간 거리입니다. 스펙트럼 메트릭과 함께 고려할 때 SI-SDR은 오디오의 위상 재구성 품질을 나타냅니다. 5. 비트 전송률 효율성: 우리는 모든 코드북의 비트 수로 나눈 대규모 테스트 세트에 적용될 때 각 코드북의 엔트로피(비트)의 합으로 비트 전송률 효율성을 계산합니다. 효율적인 비트 전송률 활용을 위해 이는 100%가 되어야 하며, 낮은 백분율은 비트 전송률이 과소 활용되고 있음을 나타냅니다. 우리는 또한 숨겨진 참조가 있지만 저역 통과 앵커가 없는 MUSHRA에서 영감을 받은 청취 테스트를 수행합니다. 이 테스트에서 10명의 전문 청취자가 평가 세트에서 무작위로 선택한 10초 샘플 12개를 평가했습니다. 각 도메인(음성, 음악 및 환경 소리)에서 4개씩입니다. 우리는 2.67kbps, 5.33kbps 및 8kbps에서 제안하는 시스템을 3kbps, 6kbps 및 12kbps에서 EnCodec과 비교합니다. 4.5 절제 연구 우리는 훈련 레시피의 구성 요소와 모델 구성을 하나씩 변경하여 모델에 대한 철저한 절제 연구를 수행합니다. 모델을 비교하기 위해 섹션 4.4에 설명된 네 가지 객관적인 메트릭을 사용합니다. 우리의 절제 연구 결과는 표 2에서 볼 수 있습니다.아키텍처: 디코더 차원을 변경하면 성능에 어느 정도 영향을 미치는데, 작은 모델일수록 지속적으로 더 나쁜 지표를 보입니다.그러나 디코더 차원이 있는 모델은 기준선과 유사한 성능을 보여 작은 모델이 여전히 경쟁력을 가질 수 있음을 나타냅니다.가장 큰 영향을 미친 변경 사항은 relu 활성화를 snake 활성화로 전환하는 것이었습니다.이 변경으로 SI-SDR 및 기타 지표가 훨씬 좋아졌습니다.BigVGAN[21]의 결과와 유사하게 snake 활성화의 주기적 유도 바이어스가 파형 생성에 도움이 된다는 것을 알았습니다.최종 모델에서는 가장 큰 디코더 차원(1536)과 snake 활성화를 사용합니다.판별기: 다음으로, 최종 결과에 미치는 영향을 확인하기 위해 판별기를 하나씩 제거하거나 변경했습니다.먼저, 다중 대역 STFT 판별기는 SI-SDR을 제외하고는 상당히 더 나은 지표를 생성하지 않는다는 것을 알았습니다.SI-SDR은 약간 더 좋습니다. 그러나 생성된 파형의 스펙트로그램을 검사할 때 다중 대역 판별기가 고주파의 앨리어싱을 완화한다는 것을 알게 되었습니다. 디코더의 업샘플링 계층은 상당한 앨리어싱 아티팩트를 도입합니다[29]. 다중 대역 판별기는 이러한 앨리어싱 아티팩트를 더 쉽게 감지하고 생성기에 피드백을 제공하여 제거할 수 있습니다. 앨리어싱 아티팩트는 크기 면에서 매우 작기 때문에 객관적인 지표에 미치는 영향은 최소화됩니다. 따라서 다중 대역 판별기를 유지합니다. 적대적 손실은 출력 오디오의 품질과 비트 전송률 효율성 모두에 중요하다는 것을 알게 되었습니다. 재구성 손실만으로 학습할 때 비트 전송률 효율성은 99%에서 62%로 떨어지고 SI-SDR은 9.12에서 1.07로 떨어집니다. 다른 지표는 스펙트럼 거리를 포착하며 비교적 영향을 받지 않습니다. 그러나 이 모델의 오디오에는 위상을 재구성하는 법을 배우지 않았기 때문에 윙윙거리는 소리를 포함한 많은 아티팩트가 있습니다. 마지막으로, MelGAN [19]에서 제안된 단일 스케일 파형 판별기를 다중 주기 판별기로 바꾸면 SI-SDR이 더 나빠지는 것을 발견했습니다.우리는 다중 주기 판별기를 유지합니다.저홉 재구성 손실의 영향: 저희는 홉 재구성이 파형 손실과 빠른 과도 현상 및 고주파수 모델링에 모두 중요하다는 것을 발견했습니다.단일 스케일 고홉 멜 재구성(80멜, 윈도우 길이 512)으로 대체하면 SI-SDR이 상당히 낮아집니다(9.12에서 7.68).주관적으로, 이 모델은 심벌 크래시, 비프음 및 알람, 노래하는 보컬과 같은 특정 종류의 소리를 훨씬 더 잘 포착한다는 것을 발견했습니다.저희는 최종 레시피에서 다중 스케일 멜 재구성 손실을 유지합니다.코드북의 잠재 차원: 코드북의 잠재 차원은 비트 전송률 효율성과 결과적으로 재구성 품질에 상당한 영향을 미칩니다. 너무 낮거나 너무 높게 설정하면(예: 2, 256), 정량적 지표가 상당히 나빠지고 비트 전송률 효율성이 크게 떨어집니다. 비트 전송률 효율성이 낮으면 대역폭이 효과적으로 낮아져 생성기의 모델링 기능이 손상됩니다. 생성기가 약해지면 판별기가 &quot;이기는&quot; 경향이 있어 생성기가 높은 오디오 품질의 오디오를 생성하는 방법을 학습하지 못합니다. 잠재 차원에 대해 8이 최적임을 알 수 있습니다. 양자화 설정: EnCodec[8]에서와 같이 지수 이동 평균을 코드북 학습 방법으로 사용하면 특히 SI-SDR의 지표가 나빠집니다. 또한 모든 코드북에서 코드북 활용도가 떨어집니다(그림 1). 구현 복잡성이 증가한 것(K-Means 초기화 및 임의 재시작 필요)을 고려하면 커밋먼트 손실과 함께 코드북 학습을 위한 더 간단한 투영 조회 방법을 유지합니다. 다음으로, 양자화 드롭아웃 비율이 정량적 지표에 상당한 영향을 미친다는 점에 유의합니다. 그러나 그림 2에서 볼 수 있듯이 드롭아웃이 0이면 코드북이 적어 재구성이 제대로 이루어지지 않습니다. 이로 인해 다운스트림 생성 모델링 작업에서 코덱 사용이 어려워지므로 최종 모델에서 드롭아웃 비율을 0.5로 변경했습니다. 이를 통해 전체 비트레이트에서 오디오 품질과 낮은 비트레이트 간에 적절한 균형을 이룰 수 있습니다. 마지막으로 모델의 최대 비트레이트를 8kbps에서 24kbps로 늘리고 다른 모든 모델 구성을 능가하는 뛰어난 오디오 품질을 얻을 수 있음을 보여줍니다. 그러나 최종 모델의 경우 압축률을 최대한 높이기 위해 낮은 비트레이트에서 학습합니다. 균형 잡힌 데이터 샘플링: 이를 제거하면 전반적으로 메트릭이 나빠집니다. 경험적으로 균형 잡힌 데이터 샘플링이 없으면 모델이 최대 주파수가 약 18kHz인 파형을 생성합니다. 이는 MPEG와 같은 다양한 오디오 압축 알고리즘에서 보존하는 최대 주파수에 해당하며, 이는 대부분의 데이터 세트를 구성합니다. 균형 잡힌 데이터 샘플링을 통해 알려지지 않은 품질의 데이터 세트(예: Common Voice)의 대역 제한 오디오만큼 고품질 데이터 세트(예: DAPS)의 전체 대역 오디오를 샘플링합니다. 이를 통해 문제가 완화되어 코덱이 대역 제한 오디오뿐만 아니라 전체 대역 오디오도 재구성할 수 있습니다. 4.6 다른 방법과의 비교 이제 최종 모델의 성능을 경쟁 기준선인 EnCodec[8], Lyra[46], 인기 있는 오픈소스 오디오 코덱인 Opus[37]와 비교합니다. EnCodec, Lyra 및 Opus의 경우 저자가 제공한 공개적으로 사용 가능한 오픈소스 구현을 사용합니다. 다양한 비트 전송률에서 객관적 및 주관적 평가를 모두 사용하여 비교합니다. 결과는 표 3에 나와 있습니다. 제안된 코덱은 22kHz의 훨씬 더 넓은 대역폭을 모델링하면서도 객관적, 주관적 지표 측면에서 모든 비트 전송률에서 경쟁 코덱보다 성능이 우수하다는 것을 알 수 있습니다. 비트 전송률(kbps) 대역폭(kHz) Mel 거리↓ STFT 거리↓ ViSQOL ↑ SI-SDR ↑ MUSHRA 점수 HN WA U+-------+-코덱 제안 1.78 22.05 1.39 1.95 3.76 2.2.67 22.05 1.28 1.85 3.90 4.5.33 22.05 1.07 1.69 4.09 8.8 22.05 0.93 1.60 4.18 10.1.EnCodecReference-- 제안12 2.11 4.30 2.82 -0.12 1.97 4.19 2.94 2.12 1.83 4.10 3.05 5.12 1.70 4.02 3.13 8.12 1.61 3.97 3.16 9.EnCodecLyra 9.8 2.71 4.86 2.19 -14.비트 전송률(kbps)Opus4 3.60 5.72 2.06 5.16 1.23 2.14 4.02 8.16 0.88 1.90 4.15 11.그림 3: 44KHz에서의 청취 테스트: MUSHRA 점수, EnCodec, 제안하는 접근 방식 및 참조에 대한 비트 전송률 대비 95% 신뢰 구간. 표 3: 다양한 비트레이트에서 제안된 코덱에 대한 객관적 평가와 경쟁 접근 방식의 결과. 그림 3에서는 다양한 비트레이트에서 EnCodec과 제안된 코덱을 비교하는 MUSHRA 연구 결과를 보여줍니다. 모든 비트레이트에서 코덱이 EnCodec보다 훨씬 높은 MUSHRA 점수를 달성한다는 것을 발견했습니다. 그러나 가장 높은 비트레이트에서도 여전히 참조 MUSHRA 점수에 미치지 못하여 개선의 여지가 있음을 나타냅니다. 최종 모델의 메트릭은 표 2에서 볼 수 있듯이 절제 연구에서 학습한 24kbps 모델보다 여전히 낮습니다. 이는 최대 비트레이트를 증가시키면 남은 성능 격차가 메워질 수 있음을 나타냅니다. 그림 4와 표 4에서는 EnCodec과 정확히 동일한 구성(24KHz 샘플링 속도, 24kbps 비트레이트, 320 스트라이드, 각각 10비트의 코드북 32개)으로 학습한 제안 모델을 정량적 및 정성적 메트릭 모두에서 기존 기준과 비교합니다. 그림 5에서 우리는 사운드 카테고리별 정성적 결과를 보여줍니다. MUSHRA 점수 참조 - 제안된 EnCodecBitrate(kbps) 그림 4: 24KHz에서의 청취 테스트: EnCodec, 동일한 구성을 가진 제안된 접근 방식 및 참조에 대한 비트레이트 대비 95% 신뢰 구간을 갖는 MUSHRA 점수. 여기서 비교 대상인 모든 샘플은 24KHz로 리샘플링됩니다. Codec Proposed@24kHz EnCodec 비트 전송률(kbps) (Н) Чірімрия S Mel 거리↓ STFT 거리 VISQOL ↑ SI-SDR ↑ 1.5 12 1.48 2.24 4.04 0.3 12 1.24 2.01 4.23 4.6 12 1.00 1.78 4.38 8.12 12 0.74 1.54 4.51 12.24 12 0.49 1.33 4.61 16.1.5 12 1.63 2.69 3.98 0.3 12 1.46 2.54 4.16 2.6 12 1.30 2.39 4.30 6.12 12 1.15 2.28 4.39 8.24 12 1.05 2.21 4.42 9.표 4: Encodec 구성: 다양한 비트 전송률에서 EnCodec과 동일한 구성으로 학습한 제안 모델의 객관적 평가와 EnCodec의 결과. MUSHRA 점수MUSHRA 점수참조(음성) 제안(음성) EnCodec(음성)ཤྩ བྷྱཿ སྔེ ཥ བྷྲ ཋ RReference(음악) --제안(음악) EnCodec(음악)비트 전송률(kbps) 비트 전송률(kbps) MUSHRA 점수བྷྱཿ ྨ ཇཱ ྴ བྷྲ ཋ ཀྵུ ཋ 。참조(env) --제안(env) ---EnCodec(env)비트 전송률(kbps) 그림 5: 범주별 MUSHRA: 제안 모델, EnCodec 및 참조에 대한 비트 전송률 대비 95% 신뢰 구간을 사용한 MUSHRA 점수. 5
--- CONCLUSION ---
우리는 다양한 유형의 오디오 데이터에서 오디오 품질을 유지하면서도 놀라운 압축률을 달성하는 고충실도 범용 신경 오디오 압축 알고리즘을 제시했습니다. 우리의 방법은 오디오 생성, 벡터 양자화 기술, 개선된 적대적 손실 및 재구성 손실의 최신 발전을 결합합니다. 기존 오디오 압축 알고리즘에 대한 광범위한 평가는 우리 접근 방식의 우수성을 입증하여 미래의 고충실도 오디오 모델링을 위한 유망한 기반을 제공합니다. 철저한 절제, 오픈 소스 코드, 훈련된 모델 가중치를 통해 생성 오디오 모델링 커뮤니티에 유용한 중심을 제공하고자 합니다. 더 광범위한 영향과 한계: 우리 모델은 풀 밴드 오디오의 생성 모델링을 훨씬 더 쉽게 수행할 수 있는 기능을 갖추고 있습니다. 이를 통해 미디어 편집, 텍스트-음성 합성, 음악 합성 등과 같은 많은 유용한 응용 프로그램이 잠금 해제되지만 딥페이크와 같은 유해한 응용 프로그램으로 이어질 수도 있습니다. 이러한 응용 프로그램을 피하기 위해 주의해야 합니다. 한 가지 가능성은 워터마킹을 추가하거나 코덱이 적용되었는지 여부를 감지할 수 있는 분류기를 학습시켜 코덱을 기반으로 생성된 합성 미디어를 감지할 수 있도록 하는 것입니다.또한, 우리 모델은 완벽하지 않으며 여전히 어려운 오디오를 재구성하는 데 어려움이 있습니다.결과를 도메인별로 분할하면 제안된 코덱이 모든 도메인에서 경쟁 접근 방식보다 성능이 뛰어나지만 음성에 가장 적합하고 환경 소리에 더 많은 문제가 있음을 알 수 있습니다.마지막으로 글로켄슈필이나 신시사이저 소리와 같이 일부 악기를 완벽하게 모델링하지 않는다는 점을 알 수 있습니다.참고문헌 [1] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: 텍스트에서 음악 생성. arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [2] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, Gregor Weber. 공통 음성: 대규모 다국어 음성 코퍼스. arXiv 사전 인쇄본 arXiv:1912.06670, 2019. [3] Yoshua Bengio, Nicholas Léonard, Aaron Courville. 조건부 계산을 위한 확률적 뉴런을 통한 기울기 추정 또는 전파. arXiv 사전 인쇄본 arXiv:1308.3432, 2013. [4] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, Xavier Serra. 자동 음악 태그를 위한 mtgjamendo 데이터 세트. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning(ICML 2019), Long Beach, CA, United States, 2019. URL http://hdl.handle.net/10230/42015. [5] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour. Audiolm: 오디오 생성을 위한 언어 모델링 접근 방식. arXiv 사전 인쇄본 arXiv:2209.03143, 2022. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. [7] Michael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev, Feargus O&#39;Gorman, Andrew Hines. Visqol v3: 오픈 소스 프로덕션 준비 대상 음성 및 오디오 메트릭. 2020년 제12회 멀티미디어 경험 품질(QoMEX) 국제 컨퍼런스, 1–6페이지. IEEE, 2020. [8] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi. 고충실도 신경 오디오 압축. arXiv 사전 인쇄본 arXiv:2210.13438, 2022. [9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. 주크박스: 음악을 위한 생성 모델. arXiv 사전 인쇄본 arXiv:2005.00341, 2020. [10] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sergiy Matusevych, Sebastian Braun, Emre Sefik Eskimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper, Robert Aichner. Icassp 2022 심층 노이즈 억제 챌린지. ICASSP, 2022. [11] Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, Adam Roberts. Ddsp: 미분 가능 디지털 신호 처리. arXiv 사전 인쇄본 arXiv:2001.04643, 2020. [12] Patrick Esser, Robin Rombach, Bjorn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12873-12883페이지, 2021. [13] Cristina Gârbacea, Aäron van den Oord, Yazhe Li, Felicia SC Lim, Alejandro Luebs, Oriol Vinyals, Thomas C Walters. vq-vae 및 wavenet 디코더를 사용한 저비트율 음성 코딩. ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 735-739페이지. IEEE, 2019. [14] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, Marvin Ritter. 오디오 세트: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트. 2017 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 776-780쪽. IEEE, 2017. [15] Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, Nal Kalchbrenner. 병렬 음성 합성을 위한 스펙트럼 에너지 거리. 신경 정보 처리 시스템의 발전, 33:13062-13072, 2020. [16] Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, Juntae Kim. Univnet: 고충실도 파형 생성을 위한 다중 해상도 스펙트로그램 판별기를 갖춘 신경 보코더. arXiv 사전 인쇄본 arXiv:2106.07889, 2021.[17] Jaehyeon Kim, Jungil Kong, Juhee Son. 영어: 종단간 텍스트-음성을 위한 적대적 학습을 갖춘 조건부 변형 자동 인코더. 기계 학습 국제 컨퍼런스, 5530-5540쪽. PMLR, 2021. [18] 공정일, 김재현, 배재경. Hifi-gan: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크. 신경 정보 처리 시스템의 발전, 33:17022-17033, 2020. [19] 쿤단 쿠마르, 리테시 쿠마르, 티보 드 부아시에르, 루카스 게스틴, 웨이 젠 테오, 호세 소텔로, 알렉상드르 드 브레비송, 요슈아 벤지오, 에런 C 쿠르빌. 멜간: 조건부 파형 합성을 위한 생성적 적대적 네트워크. 신경 정보 처리 시스템의 발전, 32, 2019. [20] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, John R Hershey. Sdr-half-baked or well done? ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 626-630페이지. IEEE, 2019. [21] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon. Bigvgan: 대규모 학습이 가능한 범용 신경 보코더. arXiv 사전 인쇄본 arXiv:2206.04658, 2022. [22] Jae Hyun Lim, Jong Chul Ye. Geometric gan. arXiv 사전 인쇄본 arXiv:1705.02894, 2017. [23] Ilya Loshchilov, Frank Hutter. 분리된 가중치 감소 정규화.arXiv 사전 인쇄본 arXiv:1711.05101, 2017. [24] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, Yoshua Bengio. Samplernn: 무조건적 엔드투엔드 신경 오디오 생성 모델.arXiv 사전 인쇄본 arXiv:1612.07837, 2016. [25] Max Morrison, Rithesh Kumar, Kundan Kumar, Prem Seetharaman, Aaron Courville, Yoshua Bengio. 조건부 파형 합성을 위한 청크 자기 회귀 gan.arXiv 사전 인쇄본 arXiv:2110.10139, 2021. [26] Gautham J Mysore. 실제 환경에서 일반 소비자 기기에 녹음된 음성을 전문가 수준의 제작 품질 음성으로 자동 변환할 수 있을까?―데이터 세트, 통찰력 및 과제.IEEE Signal Processing Letters, 22(8):1006–1010, 2014. [27] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior 및 Koray Kavukcuoglu. Wavenet: 원시 오디오를 위한 생성 모델.arXiv 사전 인쇄본 arXiv:1609.03499, 2016. [28] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman 및 John Miller. 심층 음성 3: 합성 시퀀스 학습을 통한 텍스트 음성 변환 확장. arXiv 사전 인쇄본 arXiv:1710.07654, 2017. [29] Jordi Pons, Santiago Pascual, Giulio Cengarle, Joan Serrà. 신경 오디오 합성의 업샘플링 아티팩트. ICASSP 2021-2021 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 3005-3009페이지. IEEE, 2021. [30] Ryan Prenger, Rafael Valle, Bryan Catanzaro. Waveglow: 음성 합성을 위한 흐름 기반 생성 네트워크. ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 3617-3621페이지. IEEE, 2019. [31] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, Rachel Bittner. 음악 분리를 위한 musdb18 코퍼스, 2017. [32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로샷 텍스트-이미지 생성. 기계 학습 국제 컨퍼런스, 8821-8831페이지. PMLR, 2021. [33] Ali Razavi, Aaron Van den Oord, Oriol Vinyals. vq-vae-2를 사용하여 다양한 고화질 이미지 생성. 신경 정보 처리 시스템의 발전, 32, 2019. [34] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트 음성 변환. arXiv 사전 인쇄본 arXiv: 2006.04558, 2020.[35] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Mel 스펙트로그램 예측에 대한 웨이브넷 조건화를 통한 자연스러운 TTS 합성. 2018년 IEEE 음향, 음성 및 신호 처리(ICASSP) 국제 컨퍼런스, 4779-4783페이지. IEEE, 2018. [36] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. Naturalspeech: 인간 수준의 품질을 갖춘 종단 간 텍스트-음성 합성. arXiv 사전 인쇄본 arXiv:2205.04421, 2022. [37] Jean-Marc Valin, Koen Vos, Timothy Terriberry. Opus 오디오 코덱 정의. 기술 보고서, 2012. [38] Aaron Van Den Oord, Oriol Vinyals, et al. 신경 이산 표현 학습. 신경 정보 처리 시스템의 발전, 30, 2017. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다.신경 정보 처리 시스템의 발전, 30, 2017. [40] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald 외.Cstr vctk 코퍼스: cstr 음성 복제 툴킷을 위한 영어 다중 화자 코퍼스.에든버러 대학교.음성 기술 연구 센터(CSTR), 2017. [41] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li 외. 신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다.arXiv 사전 인쇄본 arXiv:2301.02111, 2023. [42] Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim. Parallel wavegan: 다중 해상도 스펙트로그램을 갖춘 생성적 적대적 네트워크를 기반으로 하는 빠른 파형 생성 모델.ICASSP 2020-2020 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 6199-6203페이지.IEEE, 2020. [43] Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, Lei Xie. 다중 대역 melgan: 고품질 텍스트 음성 합성을 위한 더 빠른 파형 생성.2021 IEEE Spoken Language Technology Workshop(SLT), 492-498페이지. IEEE, 2021. [44] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge 및 Yonghui Wu. 향상된 vqgan을 사용한 벡터 양자화 이미지 모델링. arXiv 사전 인쇄 arXiv:2110.04627, 2021. [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 콘텐츠가 풍부한 텍스트-이미지 생성을 위한 자동회귀 모델을 확장합니다. arXiv 사전 인쇄본 arXiv:2206.10789, 2022. [46] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund 및 Marco Tagliasacchi. Soundstream: 엔드투엔드 신경 오디오 코덱. IEEE/ACM 오디오, 음성 및 언어 처리 거래, 30:495–507, 2021. [47] Liu Ziyin, Tilman Hartwig 및 Masahito Ueda. 신경망은 주기 함수를 학습하지 못하고 이를 수정하는 방법. 신경 정보 처리 시스템의 발전, 33:1583–1594, 2020.A 부록 수정된 코드북 학습 알고리즘 우리 작업에서는 다음과 같이 주어지는 수정된 양자화 연산을 사용합니다. Zq(x) = Woutek, 여기서 k = arg min ||l2(Wine (x)) — l2(e;)||j 여기서 Win과 Wout은 투영 행렬이고, Win은 인코더의 출력을 중간 표현으로 매핑하고 Wout은 이 중간 표현을 양자화된 표현 Zq(x)로 매핑합니다. 구체적으로, D는 인코더의 출력 차원이고, M은 M « D인 코드북 차원인 Win Є RDXM 및 Wout Є RMXD입니다. 벡터 양자화기 손실 함수는 재구성 오류를 측정하도록 정의되며 다음과 같습니다. 여기서 Zproj(x) = Win ze(x) LvQ = ||sg|l2(zproj(x))] − l2(ek)||¾½ + B||l2(zproj(x)) − sg[l2(ek)]|| sg는 ek를 통한 그래디언트의 역전파를 방지하는 정지 그래디언트 연산자이고, ẞ는 손실 함수에서 두 항 간의 균형을 제어하는 하이퍼파라미터입니다.
