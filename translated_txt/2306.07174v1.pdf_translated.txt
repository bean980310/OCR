--- ABSTRACT ---
기존의 대규모 언어 모델(LLM)은 입력 길이 제한으로 인해 고정 크기의 입력만 허용할 수 있어 과거 입력의 풍부한 장기 컨텍스트 정보를 활용할 수 없습니다. 이를 해결하기 위해 LLM이 장기 이력을 기억할 수 있도록 하는 프레임워크인 장기 메모리가 증강된 언어 모델(LONGMEM)을 제안합니다. 원래 백본 LLM을 메모리 인코더로 동결하고 적응형 잔여 사이드 네트워크를 메모리 검색기 및 판독기로 사용하여 새로운 분리된 네트워크 아키텍처를 설계합니다. 이러한 분리된 메모리 설계는 메모리가 오래되어도 메모리 검색을 위해 장기 과거 컨텍스트를 쉽게 캐시하고 업데이트할 수 있습니다. 메모리 증강 적응 훈련으로 강화된 LONGMEM은 장기 과거 컨텍스트를 기억하고 장기 메모리를 언어 모델링에 사용할 수 있습니다. 제안된 메모리 검색 모듈은 메모리 뱅크에서 무제한 길이의 컨텍스트를 처리하여 다양한 다운스트림 작업에 이점을 제공할 수 있습니다. 일반적으로 LONGMEM은 장문 메모리를 65k 토큰으로 확장할 수 있으므로 많은 샷 추가 데모 사례를 컨텍스트 내 학습을 위한 장문 메모리로 캐시할 수 있습니다. 실험 결과, 저희 방법은 까다로운 장문 모델링 벤치마크인 ChapterBreak에서 강력한 장문 모델보다 성능이 뛰어나며 LLM에 비해 메모리 증강 컨텍스트 내 학습에서 현저한 개선을 이룹니다. 결과는 제안된 방법이 언어 모델이 장문 콘텐츠를 기억하고 활용하는 데 효과적임을 보여줍니다. 저희 코드는 https://aka.ms/Long Mem에서 오픈 소스로 제공됩니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 다양한 이해 및 생성 작업에서 최첨단 기술을 발전시키는 데 큰 성공을 거두며 자연어 처리에 혁명을 일으켰습니다[DCLT19, RWC+19, LOG+19, YDY+19, BMR+20, RSR+20]. 대부분의 LLM은 고정 크기의 로컬 컨텍스트에서 지식을 수확하여 대규모 코퍼스에 대한 자체 감독 학습의 이점을 누리며, 예를 들어 제로 샷 프롬프팅[RWC+19], 컨텍스트 내 학습[BMR+20], Chain-of-Thought(COT) 추론[WWS+22]과 같은 새로운 능력을 보여줍니다. 그럼에도 불구하고 기존 LLM의 입력 길이 제한으로 인해 고정 크기 세션을 넘어서는 장문 정보 처리 능력이 중요한 실제 시나리오(예: 긴 수평 계획)로 일반화할 수 없습니다. 길이 제한 문제를 해결하기 위한 가장 간단한 방법은 입력 컨텍스트 길이를 단순히 확장하는 것입니다. 예를 들어, GPT-3 [BMR+20]은 더 나은 장거리 종속성을 포착하기 위해 입력 길이를 GPT-2 [RWC+19]의 1k에서 2k 토큰으로 늘립니다. 그러나 이 접근 방식은 일반적으로 처음부터 계산 집약적 학습을 초래하고 컨텍스트 내 밀집 주의는 여전히 Transformer 자기 주의 [VSP+17]의 2차 계산 복잡도에 의해 크게 제한됩니다. 대신 최근의 또 다른 연구 분야 [BPC20, ZGD+20]는 자기 주의의 2차 비용을 피하기 위해 컨텍스트 내 희소 주의 개발에 중점을 두고 있으며, 여전히 대부분 처음부터 학습이 필요합니다. 대조적으로, 저명한 연구인 Memorizing Transformer(MemTRM) [WRHS22]는 키, 값 쌍을 사용하여 컨텍스트 내 캐시된 메모리 뱅크를 근사합니다.장기 메모리 검색→ 검색된 Attn 키 및 값 Attn 키 Attn 키 및 값 및 값(세그 A)(세그 B) Attn 키 및 값(세그 Z) 현재 입력의 어텐션 쿼리 메모리 퓨전 대규모 언어 모델(고정) 잔여 잔여 SideNet 연결(학습 가능) ABD 긴 시퀀스 입력 YZ 그림 1: LONGMEM의 메모리 캐싱 및 검색 흐름 개요. 긴 텍스트 시퀀스는 고정 길이 세그먼트로 분할된 다음 각 세그먼트가 대규모 언어 모델을 통해 전달되고 m번째 계층의 어텐션 키 및 값 벡터가 장기 메모리 뱅크에 캐시됩니다. 미래의 입력을 위해, 어텐션 쿼리 키 기반 검색을 통해 장기 메모리의 상위 k 어텐션 키-값 쌍이 검색되어 언어 모델링으로 융합됩니다. 트랜스포머를 위한 미분 불가능한 메모리에서 검색된 컨텍스트 내 토큰과 기억된 토큰 모두에 대한 밀도 있는 어텐션을 통한 희소한 어텐션. 따라서 MemTRM은 최대 65k 토큰을 처리하도록 결과 언어 모델을 확장하고 전체 길이의 책이나 긴 논문을 모델링하는 데 상당한 복잡도 이득을 얻습니다. 그러나 MemTRM은 메모리 인코딩과 언어 모델링을 위한 메모리 융합에 단일 모델을 사용하는 결합된 메모리 설계로 인해 훈련 중에 메모리 진부화 문제에 직면합니다. 즉, 모델 매개변수가 업데이트됨에 따라 메모리에 캐시된 이전 표현은 최신 모델의 표현과 분포가 다를 수 있으므로 메모리 증강의 효과가 제한됩니다. 이 논문에서 우리는 언어 모델이 장문의 이전 맥락이나 지식을 미분 불가능한 메모리 뱅크에 캐시하고, 분리된 메모리 모듈을 통해 이를 활용하여 메모리 진부화 문제를 해결할 수 있는 장기 메모리로 증강된 언어 모델(LONGMEM) 프레임워크를 제안합니다. 분리된 메모리를 달성하기 위해 새로운 잔여 사이드 네트워크(SideNet)를 설계합니다. 이전 맥락의 페어링된 어텐션 키와 값은 동결된 백본 LLM을 사용하여 메모리 뱅크로 추출됩니다. SideNet의 메모리 증강 계층에서 현재 입력의 생성된 어텐션 쿼리는 메모리에서 이전 맥락의 캐시된 (키, 값)을 검색하는 데 사용되고, 해당 메모리 증강은 조인트 어텐션 메커니즘을 통해 학습된 숨겨진 상태로 융합됩니다. 또한, SideNet과 동결된 백본 LLM 간의 새로 설계된 교차 네트워크 잔여 연결은 사전 학습된 백본 LLM에서 더 나은 지식 전송을 가능하게 합니다. 잔여 SideNet을 지속적으로 훈련하여 메모리 증강된 긴 컨텍스트를 검색하고 융합함으로써 사전 훈련된 LLM을 조정하여 개선된 모델링을 위해 긴 컨텍스트 메모리를 활용할 수 있습니다. 자세한 메모리 캐시, 검색 및 융합 프로세스는 그림 1에 나와 있습니다. 분리된 메모리 설계는 두 가지 주요 이점을 제공합니다. 첫째, 제안된 아키텍처는 이전 입력을 메모리로 인코딩하는 프로세스와 분리된 동결된 백본 LLM 및 SideNet을 통한 메모리 검색 및 융합 프로세스를 분리합니다. 이런 방식으로 백본 LLM은 긴 컨텍스트 지식 인코더로만 작동하는 반면, 잔여 SideNet은 메모리 검색기 및 판독기로 작동하여 메모리 진부화 문제를 효과적으로 해결합니다. 둘째, 메모리 증강으로 전체 LLM을 직접 조정하는 것은 계산적으로 비효율적이며 치명적인 망각으로 어려움을 겪습니다. 백본 LLM은 효율적인 메모리 증강 적응 단계에서 동결되므로 LONGMEM은 사전 훈련된 지식을 활용할 수 있을 뿐만 아니라 치명적인 망각도 피할 수 있습니다. LONGMEM은 다운스트림 작업에 따라 다양한 유형의 장문 텍스트와 지식을 메모리 뱅크로 가져올 수 있습니다. 여기서는 전체 길이의 책 맥락을 사용한 언어 모델링과 수천 개의 작업 관련 데모 예제를 사용한 메모리 증강 맥락 내 학습이라는 두 가지 대표적인 사례를 고려합니다. 구체적으로, 다양한 장문 언어 모델링과 언어 이해를 위한 메모리 증강 맥락 내 학습에 대한 제안된 LONGMEM의 효과를 평가합니다. 실험 결과는 우리 모델이 장문 모델링과 맥락 내 학습 능력 측면에서 강력한 기준선을 지속적으로 능가한다는 것을 보여줍니다. 영어: 우리 방법은 실질적으로 키, 값 쌍이 있는 캐시된 메모리 뱅크 캐시 Attn 키와 값 g 언어 모델 헤드 메모리 검색 및 퓨전 LLM 디코더 계층 HL K LLM 디코더 계층 V SideNet MemAug 계층 LLM 디코더 계층 메모리 퓨전 Q LLM 디코더 계층 SideNet 계층 h²+임베딩 계층 gh 현재 입력 동결 계층 학습 가능 계층 키, 값 쌍이 있는 캐시된 메모리 뱅크 Ꮎ 잔여 토큰-청크 검색 {k} {V} ht 그림 2: LONGMEM 아키텍처 개요. &quot;MemAug&quot;는 메모리 증강 계층을 나타냅니다. Gutenberg-2022 코퍼스의 다양한 길이 분할에 대해 LLM의 긴 컨텍스트 언어 모델링 기능을 -1.38~-1.62 복잡도만큼 개선합니다. 놀랍게도, 우리 모델은 까다로운 장문맥 모델링 벤치마크인 ChapterBreak에서 40.5%의 식별 정확도라는 최첨단 성능을 달성하여 기존의 강력한 x-former 베이스라인을 크게 능가합니다. 마지막으로, 메모리에 2k 데모 예제가 있는 LONGMEM은 MemTRM 및 비메모리 증강 베이스라인과 비교하여 인기 있는 NLU 작업에서 현저한 컨텍스트 내 학습 개선을 보여줍니다. 2가지 방법 LLM이 메모리의 과거 장문맥에서 관련 정보를 수집할 수 있도록 하기 위해, 우리는 분리된 메모리 모듈로 동결된 백본 LLM을 증강하는 것을 제안합니다. 메모리 컨텍스트 정보를 융합하기 위해, 효율적인 방식으로 지속적으로 학습할 수 있는 새로운 경량 잔여 SideNet을 설계합니다. 다음에서, 우리는 먼저 메모리 증강을 통한 언어 모델링의 문제 공식화에 대해 논의합니다. 그런 다음, 우리는 동결된 사전 학습된 LLM을 로컬 입력 컨텍스트와 검색된 메모리 컨텍스트에 공동으로 참석하도록 조정하기 위한 효율적인 잔여 SideNet을 공식적으로 소개합니다. 마지막으로, 언어 모델링을 위해 과거 기억이 인코딩, 저장, 회상 및 융합되는 방식에 대한 설계된 프로세스를 제공합니다.2.1 장기 기억으로 증강된 언어 모델 여기서는 고수준 문제 설정에 초점을 맞추고 더 많은 구성 요소 세부 정보는 나중 섹션으로 미룹니다.사전 훈련된 LLM에 널리 채택됨에 따라 LONGMEM 모델은 Transformer 아키텍처[VSP+17]를 기반으로 구축됩니다.LONGMEM의 경우 세 가지 핵심 구성 요소가 있습니다.동결 백본 LLM, SideNet 및 캐시 메모리 뱅크입니다.대부분의 기존 사전 훈련된 LLM은 고정 크기의 입력만 받을 수 있으므로 길이 제한에 맞는 긴 시퀀스(예: 책)의 입력 세그먼트만 대부분의 기존 자기 회귀 언어 모델에서 수행되는 것처럼 현재 입력으로 표시됩니다.맞을 수 없는 이전 세그먼트는 이전 입력으로 표시되며 이는 메모리 증강에 사용됩니다.사전 훈련된 LLM의 학습된 지식을 활용하기 위해 이전 및 현재 입력은 모두 동결 백본 LLM을 사용하여 인코딩되지만 다른 표현이 추출됩니다. 이전 입력의 경우 m번째 계층의 Transformer 셀프 어텐션에서 나온 키-값 쌍은 캐시 메모리 뱅크에 저장되는 반면, 현재 입력에 대한 각 LLM 디코더 계층의 숨겨진 상태는 유지되어 SideNet으로 전송됩니다. 각 현재 입력 토큰의 경우, 가장 관련 있는 키-값 벡터 쌍이 언어 모델링을 위한 메모리 증강으로 검색됩니다. SideNet 모듈은 분리된 메모리에서 현재 입력 컨텍스트와 관련 캐시된 이전 컨텍스트를 융합하도록 훈련된 효율적인 적응 모델로 볼 수 있습니다. 공식적으로, 고정 크기의 입력 텍스트 시퀀스 {x;}!(현재 입력)의 경우, LONGMEM은 먼저 기울기 계산 없이 백본 LLM(그림 2에서 파란색으로 표시)을 사용하여 순방향 패스를 수행합니다. 백본 LLM의 임베딩 계층은 먼저 입력 {x;}을 임베딩 공간으로 인코딩하고 초기 숨겨진 상태 HOLM Є Rª|×E를 출력합니다. 여기서 E는 숨겨진 차원입니다. 그런 다음 동결된 백본 LLM의 각 연속적인 Transformer 디코더 계층은 이전 계층의 숨겨진 상태를 사용하여 새로운 숨겨진 상태 i=를 계산합니다.HLM = LLM fou (LLM), VI′ = [1, L′]이고 L&#39;은 백본 LLM의 총 계층 수입니다.모든 이전 입력에 대한 백본 LLM을 사용한 순방향 패스 동안 m번째 Transformer 디코더 계층에서 셀프 어텐션에 사용된 키-값 쌍은 캐시된 메모리 뱅크(그림 2의 왼쪽 상단 모서리에 주황색으로 표시)에 저장되며 나중에 미래 입력에 대한 메모리 증강으로 호출됩니다.Cached Memory Bank는 최신 M 이전 입력 K, Ñ € RH×|x|×d의 어텐션 키-값 쌍을 유지하는 캐시된 헤드별 벡터 큐 Zk, Zv € RH×M×d입니다.여기서 H, d는 각각 어텐션 헤드 수와 헤드당 차원을 나타냅니다. 메모리 검색 및 융합(§2.3) 후, 메모리 뱅크는 가장 오래된 시퀀스의 키-값 쌍을 제거하고 현재 시퀀스를 캐시된 벡터 뱅크에 추가합니다. 따라서 이러한 업데이트 메커니즘은 시퀀스 수준에서 언어 모델링 인과 관계를 보장하고 메모리 뱅크가 항상 현재 입력에 대한 가장 가까운 이전 컨텍스트의 레코드를 유지할 수 있도록 합니다. 백본 LLM을 사용한 포워드 패스 후, SideNet 모듈은 백본 LLM {HLM}\₁에서 모든 현재 입력 숨겨진 상태와 캐시된 메모리 뱅크의 과거 키-값 쌍을 가져와 메모리 증강 표현을 계산합니다. 구체적으로, LONGMEM의 SideNet은 (L1) 일반 Transformer 디코더 계층과 하나의 특수 메모리 증강 디코더 계층으로 구성됩니다. 효율적인 목적을 위해, 우리는 주로 SideNet의 #layers L이 백본 LLM의 #layers L보다 작은 경우, 즉 L &lt; L&#39;인 경우를 고려합니다. SideNet은 (L – 1)개의 일반 Transformer 디코더 계층과 특수한 메모리 증강 계층을 통해 Hº를 메모리 증강 컨텍스트 표현으로 인코딩합니다.ms Side j=ms-Side 메모리 증강 계층은 메모리 증강 입력을 받는 vanilla Transformer 디코더 계층의 확장으로, 메모리의 가장 관련 있는 키-값 쌍과 현재 입력의 숨겨진 상태를 모두 포함합니다.여기서 캐시된 키-값 쌍은 토큰 기반 메모리 검색 모듈(§2.3)을 사용하여 회수됩니다.각 현재 입력 토큰에 대해 메모리 검색 모듈 srt(:)는 메모리 뱅크 {kij, ~ij}} \₁ = Srt(xi)에서 가장 관련 있는 K개의 키-값 쌍을 검색합니다. 그런 다음 SideNet은 메모리 증강 입력 Hode = ƒØMem (Hode¯¹‚, {{kij‚ Ñij}-1}!¦ª¦₁)을 사용하여 출력을 계산합니다. 여기서 는 메모리 증강 계층을 주입하는 계층 인덱스입니다. 마지막으로 토큰 확률은 마지막 SideNet 숨김 상태 P(xi|×1, ..., Xi−1) = softmax(WH)를 사용하여 계산합니다. 여기서 W는 백본 LLM과 SideNet이 공유하는 동결된 출력 임베딩 가중치입니다. 분리된 메모리를 활용하기 위해 LONGMEM에 대한 메모리 증강 적응 학습을 수행합니다. 생성적 비지도 사전 학습[RNSS18]에 따라 LONGMEM의 학습 목표는 표준적인 왼쪽에서 오른쪽으로의 언어 모델링 목표이며, 이는 왼쪽 맥락을 기반으로 다음 토큰의 가능성을 최대화합니다. max Σ RED Σx log P(xi|×1, ·‚×i–여기서 x는 사전 학습 텍스트 코퍼스 D에서 무작위로 샘플링된 문장입니다. ... (i-1), 2.2 잔차 SideNet Side SideNet 아키텍처 및 초기화. 여기서도 Transformer [VSP+17]에 기반한 SideNet을 구현합니다. 여기서 SideNet의 디코더 계층 수 L은 백본 LLM의 계층 수 L&#39;을 감소 계수(이 작업 전체에서 계층 감소 계수 2 L&#39; = 2L)로 나눈 것과 같습니다. SideNet의 각 디코더 계층의 가중치는 동일한 깊이를 가진 백본 LLM의 해당 사전 학습된 디코더 계층에서 초기화됩니다. 03 de = 0 LM. 그림 2에서 볼 수 있듯이, SideNet은 백본 LLM의 임베딩 계층의 출력을 가져와 백본 LLM의 언어 모델링 헤드 계층을 재사용하는데, 이 계층은 지속적인 적응 단계에서도 동결됩니다. 메모리 증강 적응 단계에서는 SideNet의 다른 모든 매개변수가 학습 신호에 따라 적절히 업데이트됩니다. 이런 식으로 가벼운 SideNet은 사전 학습된 매개변수에서 전송된 지식으로 빠른 수렴을 달성합니다. 교차 네트워크 잔여 연결. 사전 학습된 백본 LLM의 지식을 활용하기 위해 제안된 교차 네트워크 잔여 연결을 사용하여 백본 LLM의 표현을 SideNet으로 융합합니다. 구체적으로, 백본 LLM의 21번째와 (212)번째 계층에서 출력 숨겨진 상태의 차이를 SideNet의 1번째 계층에서 출력 숨겨진 상태에 대한 잔여 연결로 추가합니다. 그런 다음, SideNet의 다음 (1 + 1)번째 계층에 대한 입력은 (H)에 대한 이전 계층을 통해 전달된 원래의 숨겨진 상태와 백본 LLM Hside = forside (H&#39; side) + (H²LM − H²LM²), VE [1, L], (1)에서 숨겨진 상태 차이의 교차 네트워크 Side Sideresidual 연결의 합입니다.여기서 Ho는 임베딩 계층의 출력입니다.디코더 계층 [VSP+17]의 셀프 어텐션 및 피드포워드 네트워크 이후의 잔여 연결은 fo(H)에서 정상적으로 수행되고 제안된 교차 네트워크 잔여 연결과 병렬로 수행된다는 점에 주목할 가치가 있습니다.Side Side 2.3 메모리 검색 및 융합 LONGMEM의 장기 메모리 기능은 검색 및 융합을 위한 메모리 증강 모듈을 통해 달성됩니다.토큰-청크 메모리 검색. 토큰 대 토큰 검색을 수행하는 대신, 가속 및 무결성을 위해 토큰 대 청크 검색에 집중합니다. 텍스트 청크는 청크 크기 csz 개수의 연속 토큰으로 구성된 n-gram 구조를 말합니다. 메모리 뱅크는 토큰 청크 수준에서 캐시된 키-값 쌍을 저장합니다. 메모리 뱅크를 M/csz 어텐션 키-값 쌍 청크로 나누고 청크 크기 차원의 평균 풀링 벡터를 사용하여 검색을 위한 키 벡터를 구합니다. 그런 다음 현재 입력 토큰의 어텐션 쿼리와 후보 청크의 평균 풀링 어텐션 키 간의 점곱에 대한 상위(K/csz) 어텐션 키-값 청크를 검색합니다. 마지막으로 검색된 키-값 쌍 청크에 대한 청크 크기 차원을 압축하고 토큰 수준 {K¿‚ Ñ;}Ķ1에서 K 키-값 쌍으로 평면화합니다. 토큰-청크 검색을 채택하면 검색 인덱스의 크기 jj=1*이 줄어들고 프로세스가 가속화됩니다. 한편, 검색 정확도는 더욱 향상될 수 있으며, 이는 [LGW+23] 및 [BMH+21]에서도 관찰됩니다. 하이퍼파라미터 청크 크기 csz는 검색된 컨텍스트의 세분성을 제어하며, 이는 다운스트림 작업에 따라 경험적으로 조정할 수 있습니다. 예를 들어, 컨텍스트 내 학습에는 메모리에 캐시된 데모 예제에서 더 세분화된 레이블 토큰이 필요한데, 이때 더 작은 csz가 유용합니다. 메모리 융합. 메모리 융합은 특수 메모리 증강 계층 내에서 수행됩니다. 기존 Transformer 디코더 계층은 멀티헤드 셀프 어텐션 [VSP+17]을 사용하므로 [WRHS22]에 따라 이를 조인트 어텐션 메커니즘으로 확장하고 각 토큰이 로컬 컨텍스트와 검색된 메모리 컨텍스트에 모두 참여할 수 있도록 장기 메모리 융합 프로세스를 제안합니다. 이전 계층 H¹-¹ € R|×|×d의 헤드별 숨김 상태 출력과 해당 검색된 어텐션 키-값 쌍이 {K¿‚ Ñ¿}¦ª¦₁ € R×××d인 경우 1번째 메모리 증강 계층 H²의 출력 숨김 상태는 다음과 같이 계산됩니다. 1-QKT QiKT √d iJi=1&#39; (2) A softmax( -)V, M = Concat{softmax(H = sigmoid (g) A + (1 − sigmoid(g)) · M, (3) 여기서 Q, K, V, A, M Є R||×d, K는 각 토큰에 대한 캐시된 메모리에서 검색된 어텐션 키-값 쌍의 수이고 g는 학습 가능한 헤드별 게이팅 벡터입니다. 이전 계층 H(1-1)의 숨김 상태 출력은 세 가지를 통해 어텐션 쿼리, 키 및 값 Q, K, V로 선형적으로 투영됩니다. 행렬 WQ, WK, WV Ĕ Rdxd. 캐시된 메모리에서 검색된 어텐션 키-값 쌍은 각 토큰마다 다르다는 점에 유의해야 합니다.3 실험 Є 우리는 요구되는 메모리 내 긴 컨텍스트를 기반으로 다양한 작업에서 제안된 LONGMEM 모델을 평가합니다.a) 과거 긴 컨텍스트를 캐시된 메모리에 로드할 때 긴 텍스트 언어 모델링 및 언어 이해;b) 대량의 데모 예제를 캐시된 메모리에 로드할 때 무한 길이의 컨텍스트 내 학습.3.1 훈련 설정 훈련 코퍼스 일괄 처리.대규모 코퍼스에 대한 기존 일괄 처리 프로세스는 패딩 없이 전체 코퍼스를 연속된 고정 길이 텍스트 세그먼트로 자르고 모든 세그먼트를 셔플하여 미니 배치를 구성합니다[RWC+19].반대로, LONGMEM은 글로벌 셔플링을 비활성화하고 세그먼트 수준에서 글로벌 인과성을 보장해야 합니다. 첫째, 훈련 기업의 모든 긴 문서를 동일한 길이의 배치 크기 수의 문서 그룹으로 나눈 다음 각 그룹 내에서 문서 수준 셔플링을 수행합니다. 그런 다음 한 그룹 내에서 셔플링된 문서를 연결하고 정렬된 세그먼트로 잘라냅니다. 배치화 후 한 긴 문서의 두 연속 세그먼트가 두 개의 연속된 입력 배치에 분배되도록 하기 위해 동일한 내부 그룹 인덱스를 가진 배치 크기 수의 문서 그룹에서 하나의 세그먼트를 선택합니다. 따라서 배치 크기 수의 세그먼트를 가진 미니 배치는 정확히 배치 크기 수의 문서 그룹에서 구성됩니다. 이런 식으로 훈련 반복 단계로서 메모리 뱅크에 캐시된 어텐션 키-값 쌍은 동일한 문서 내의 현재 입력의 정확히 이전 컨텍스트입니다. 일괄 처리 프로세스는 그림 3에 설명되어 있습니다.긴 문서 문서 그룹화 잘린 세그먼트 일괄 처리 S11 S12 $13 SSS12 SS₁N DocumentS15 $S1N S21 S22 SS2N ... DocumentSS32 SS3N S21 S22 SSSSSSAN DocumentBatch 1 Batch 2 BatchBatch N $SS2N 캐시된 메모리 5번째 반복 중에 업데이트 DocumentSS32 SSS₁₁ S12 S13 S14 S12 S13 S14 SDocumentSSS3N SS22 S23 SSS23 S24 S♡ S31 S32 SS$S33 S34 SSS42 S43 SS45 SS4N Document Z ... 个 가장 오래된 것 제거 가장 최근 것 추가 그림 3: 각 문서 내의 연속된 세그먼트가 연속된 일괄 처리로 분배되도록 큰 텍스트 코퍼스를 일괄 처리합니다. = 훈련 코퍼스와 하이퍼파라미터. BookCorpus2, Books3, OpenWebText2, Stack Exchange, Wikipedia, Gutenberg(PG-19), NIH ExPorter, Pile-CC 데이터 세트를 포함하여 Pile [GBB+20]의 하위 집합을 훈련 코퍼스로 샘플링합니다. 원래 GPT-2 [RWC+19]가 LLM이 장거리 종속성을 학습할 수 있도록 하는 데 성능이 좋지 않은 것으로 밝혀진 절대 위치 임베딩을 채택했기 때문에 GPT-2(407M-파라미터)를 Alibi [PSL21] 위치 임베딩이 있는 사전 훈련된 백본 LLM으로 재생성합니다. [DYY+19] 백본 LLM은 L&#39; 24, H16, d = 64 아키텍처를 사용합니다. SideNet은 L = 12, H = 16, d = 64 아키텍처를 사용합니다. 메모리 증강 적응을 위한 훈련은 26B 토큰에서 반복되며, 글로벌 배치 크기는 256이고 시퀀스 길이는 1024입니다. 청크 크기 csz는 토큰 4개이고 메모리 크기 M은 토큰의 키-값 쌍 65k개입니다. 각 토큰에 대해 증강을 위해 K=64개의 어텐션 키-값 쌍을 검색하는데, 이는 K/csz=16개의 텍스트 청크입니다. 메모리 증강 계층은 SideNet의 9번째 계층입니다. 백본 LLM의 18번째 계층의 어텐션 키와 값은 메모리에 캐시되어 향후 검색에 사용됩니다. 기타 훈련 세부 정보는 부록 C. 메모리 검색 모듈에 나와 있습니다. 한 GPU의 캐시된 메모리 뱅크의 고정 메모리 크기는 토큰의 키-값 쌍입니다. 효율성을 위해 각 GPU가 자체 메모리 검색 모듈을 구성하고 업데이트할 수 있도록 합니다. 효율적인 토큰-청크 검색을 구현하기 위해 faiss [JDJ21] 툴킷을 사용하여 GPU에서 정확한 검색 인덱스를 구성하여 텍스트 청크의 평균 풀링된 어텐션 키를 저장하고 효율적인 검색을 수행합니다. faiss 인덱스는 고정된 M/csz 키를 유지하고 내적에 대한 효율적인 정확한 검색을 제공합니다. 검색에는 1k 토큰당 약 15ms가 소요되며 이는 백본 LLM 전달 패스의 55% 시간 비용입니다. 정확한 검색 인덱스를 근사 검색 인덱스로 쉽게 조정하여 검색 효율성을 높일 수 있습니다. 기준선. 사전 훈련된 GPT-2*의 기준선 외에도 또 다른 메모리 증강 적응 기준선으로 MemTRM(Memorizing Transformer) [WRHS22]을 재생성합니다. MemTRM은 사전 훈련된 LLM을 조정하여 외부 메모리를 사용하도록 쉽게 조정할 수 있습니다. MemTRM에서 제안한 knn-증강 계층을 LLM 디코더의 동일한 18번째 계층으로 삽입합니다. MemTRM 기준선도 동일한 하이퍼파라미터 설정에서 동일한 수의 토큰에 대해 훈련됩니다. 3.2 장문맥 언어 모델링 장문맥 언어 모델링은 검색된 어텐션 키-값에 저장된 지식이 유용한 역할을 할 수 있는 과거 장문맥의 증강된 분리된 메모리에서 쉽게 이점을 얻을 수 있습니다.데이터세트 PG-ArXiv 분할 SSLen. 범위 5K-10K #문서 평균 #토큰 7.6K 10K-100K47.6K S100K-500K140K SS500K-1M &gt;1M640K &lt;60K1.2M 15.4K 표 1: 길이 범위 및 Arxiv에 따른 PG-22의 5개 분할에 대한 데이터세트 통계. 컨텍스트 내 메모리 PG-모델 ArXiv 길이 길이 5K-10K 10K-100K GPT-2* 1k 없음 22.MemTRM 1k 65K 21.24.23.23.24.100K-500K 500K-1M &gt;1M 24.24.97 18.07 11.17.39 10.LONGMEM 1k 65k 21.23.22.23.16.71 10.표 2: 긴 컨텍스트 언어 모델링 데이터 세트에 대한 평가 결과. 모든 데이터 세트에 대해 토큰 수준 복잡도(PPL)(낮을수록 좋음)를 보고합니다. 긴 컨텍스트 언어 모델링에서 모델이 더 나은 성능을 발휘하도록 돕기 위해 중요한 배경 및 맥락 정보를 제공하는 역할. 예를 들어, 장문 텍스트를 정확하게 모델링하려고 할 때 이전 배경 및 캐릭터 관계에서 지식을 습득하면 그에 따른 스토리를 모델링하는 데 도움이 될 수 있습니다.평가 설정. 먼저 LONGMEM과 3개의 장문 컨텍스트 모델링 데이터 세트인 Project Gutenberg 2020-2022, ArXiv, ChapterBreak의 베이스라인을 비교합니다. 이러한 데이터 세트에 포함된 대부분의 책이나 논문은 길이가 최소 16k 토큰입니다. 나열된 모든 데이터 세트는 작업별 튜닝 없이 제로샷 방식으로 평가됩니다. 3개 데이터 세트에 대한 자세한 평가 설정은 다음과 같습니다. • Project Gutenberg 2020-2022 언어 모델링 데이터 세트. Project Gutenberg Library¹에서 2020년과 2022년 사이에 출판된 책을 크롤링하고 정리하여 PG-22라는 완전히 새로운 장문 모델링 데이터 세트를 구축했습니다. PG-19의 훈련 하위 집합과는 도메인과 글쓰기 스타일 면에서 크게 차별화됩니다.PG-19 [RPJL19]의 책은 1919년 이전에 출판되었기 때문입니다.길이 범위에 따라 PG-22의 다른 검증 분할을 제공하고, 데이터 통계는 표 1에 제시되어 있습니다.• ArXiv 데이터 집합.ArXiv 데이터 집합에는 수학, 컴퓨터 과학, 물리학 분야의 논문이 포함됩니다.Pile 코퍼스 [GBB+20]에서 ArXiv 논문 하위 집합의 검증 분할을 선택합니다.Pile의 ArXiv 하위 집합은 훈련에서 제외되었으며 분포 외부 데이터 집합입니다.PG-22 및 Arxiv의 장 컨텍스트 언어 모델링 벤치마크에 대한 토큰 수준 언어 모델링 복잡도를 보고합니다.• ChapterBreak 벤치마크. ChapterBreak는 [STI22]에서 LLM이 이전 장의 긴 맥락을 고려하여 같은 책에서 샘플링한 일련의 하드 네거티브 세그먼트와 다음 장의 기본 진실의 시작을 구별해야 하는 까다로운 접미사 식별 데이터 세트로 제안되었습니다. ChapterBreak는 올바른 접미사를 이해하고 식별하기 위해 글로벌 긴 맥락을 처리해야 합니다. [STI22]는 장문 처리를 위한 최첨단 x-former조차도 ChapterBreak에서 좋은 성능을 발휘하기 위해 장거리 맥락을 효과적으로 활용하지 못한다는 것을 보여주었습니다. 우리는 AO3에서 추출한 팬 픽션이 포함된 ChapterBreak의 Archive of Our Own(AO3) 하위 세트를 선택했습니다. ChapterBreak는 다양한 모델의 길이 제한에 맞게 0.5k에서 8k 토큰까지 접두사 길이를 기반으로 8개의 분할을 제공합니다. 4k, 6k 및 8k 접두사의 분할이 평가를 위해 선택되었습니다. 4k 이상의 토큰을 처리할 수 없는 LLM의 경우, LLM의 최대 입력 길이를 충족하기 위해 앞부분 접두사를 버립니다. MemTRM 및 LONGMEM 모델의 경우, 먼저 주어진 4k/6k/8k 접두사 컨텍스트를 캐시된 메모리에 로드한 다음 스코어링을 수행합니다. 제로 샷 평가 방식으로 각 후보 접미사 세그먼트에 대한 스코어러로 퍼플렉시티를 사용합니다. 그런 다음 퍼플렉시티가 낮은 접미사 세그먼트가 레이블로 선택됩니다. 접미사 식별 정확도가 평가 지표로 사용됩니다. 결과. 평가된 긴 컨텍스트 데이터 세트에 대한 주요 결과는 표 2에 요약되어 있습니다. 제안된 LONGMEM 모델은 긴 텍스트 언어 모델링에서 고려된 모든 기준선을 상당히 능가합니다. https://www.gutenberg.org/ 컨텍스트 내 메모리 내 모델 #Params Len. Len. ChapterBreakactx-4k ctx-6k ctx-8k GPT-2-XL* [RWC+19] 1.5B 1K 없음 24% 24% 24% GPT-3* [BMR+20] 175B 2K 없음 28% 28% 28% LocalTRM [RSVG21] 516M 8K 없음 24% 24% 24% RoutTRM [RSVG21] 490M 8K 없음 25% 24% 24% Bigbird [ZGD+20] 128M 4K 없음 26% 26% 26% GPT-2* 407M 1K 없음 18.4% 18.4% 18.4% MemTRM 407M 1K ∞ LONGMEM 558M 1K ∞ 28.3% 28.7% 28.7% 37.7% 39.4% 40.5% 표 3: ChapterBreak의 AO3 하위 집합에 대한 제로샷 접미사 식별 정확도. †로 표시된 기준선은 [STI22]에서 직접 인용한 것입니다. MemTRM 및 LONGMEM은 주어진 4k/6k/8k 접두사 컨텍스트를 캐시된 메모리에 로드하는 반면 로컬 컨텍스트에 대한 입력 길이는 여전히 1k 토큰입니다. 데이터 세트, PG-22의 다른 길이 분할에서 -1.38에서 -1.62의 복잡도 개선 및 ARXIV 데이터 세트에서 -1.ppl. 놀랍게도 제안된 방법은 ChapterBreakA03 접미사 식별 벤치마크에서 40.5%의 정확도라는 최첨단 성능을 달성하고 강력한 롱 컨텍스트 변환기와 313배 더 큰 매개변수를 사용하는 최신 LLM GPT-3를 모두 능가합니다. 이러한 데이터 세트에 대한 상당한 개선은 LONGMEM이 캐시된 메모리에서 과거의 롱 컨텍스트를 이해하여 미래 입력에 대한 언어 모델링을 잘 완료할 수 있음을 보여줍니다. 3.3 메모리 증강 인컨텍스트 학습 LLM은 로컬 컨텍스트에서 몇 가지 샷 데모 예제에서 비모수적으로 지식을 학습하여 인컨텍스트 학습(ICL)의 새로운 기능을 가지고 있습니다. 그러나 기존의 인컨텍스트 학습은 입력 컨텍스트 길이에 의해 크게 제한되어 학습 세트에서 충분한 데모 예제에서 감독을 흡수하는 데 효과적이지 않습니다. 제안된 무제한 길이 메모리 증강을 통해 LONGMEM 방법은 로컬 컨텍스트에서 데모 예제 수의 제한을 극복하고 캐시된 메모리에 로드하여 전체 학습 세트에 참여할 수도 있습니다. 이런 방식으로 LONGMEM은 기존의 few-shot in-context 학습을 넘어서 수천 개의 보조 데모 예제를 통해 메모리 증강 in-context 학습을 실현했습니다.평가 설정.여기서 우리는 다섯 가지 자연어 이해(NLU) 데이터 세트인 SST-2 [SPW+13], MPQA [WWC05], MR [ABK+07], Subj [PL04] 및 SST-5 [SPW+13]에 대한 기준선과 제안된 LONGMEM 모델의 in-context 학습 기능을 평가합니다.우리는 두 가지 few-shot 설정인 4-shot 및 20-shot에서 모델을 평가합니다.4-shot 데모는 데이터가 부족한 시나리오인 반면, 20-shot 데모는 1k 입력 길이를 거의 충족하고 충분한 맥락적 자체 감독을 제공할 수 있습니다. 우리는 고정된 텍스트 템플릿을 통해 k-shot 예시를 의미적으로 의미 있는 데모 예시로 변환합니다. 즉, d¿=&quot;리뷰: x¿ 감정: y₁&quot;,\{(xi, yi)}=-1 Є 감정 분석 작업을 위한 Dtrain입니다. 또한, 우리는 개방형 생성 설정에서 SQUAD [RZLL16]의 질문-답변 작업에 대한 3-shot ICL을 평가합니다. 모든 프롬프트 템플릿의 세부 정보는 부록 D에 나와 있습니다. 그런 다음 줄 바꿈으로 데모 예시를 연결하여 구분합니다. 예측 레이블은 컨텍스트에서 데모 예시와 테스트 사례를 고려하여 탐욕적 디코딩을 사용하여 직접 생성됩니다. 예측 정확도는 평가 지표로 사용됩니다. 우리는 k-shot 데모 예시를 선택할 때의 무작위성을 극복하기 위해 서로 다른 난수 시드를 사용한 6회 실행의 평균과 표준 편차를 보고합니다. 앞서 설명한 대로 청크 크기는 검색된 텍스트 청크의 세분성을 제어합니다. 선택된 NLU 데이터 세트는 캐시된 메모리에서 세분화된 레이블을 검색해야 하므로 SST-2의 검증 세트에서 하이퍼 매개변수 선택을 수행하고 최상의 청크 크기 2를 사용하여 MemTRM 및 모델에 대한 결과를 보고합니다. 결과. 컨텍스트 내 학습에 대한 결과는 표 5와 표 4에 요약되어 있습니다. LONGMEM은 20샷 충분한 컨텍스트 내 설정에서 모든 NLU 작업에서 현저한 개선을 달성하여 사전 학습된 GPT-2* 및 MemTRM보다 평균 점수가 +8.0 증가했습니다. 한편, LONGMEM은 로컬 컨텍스트에서 4샷 데모 시나리오에서도 성능이 향상되었습니다. 또한 LONGMEM은 SQUAD에서 +4.5 EM 점수 증가로 개방형 생성 작업에서 LLM의 컨텍스트 내 학습 기능을 개선합니다. 컨텍스트 내 메모리 내 SST-MR 모델 #Demons. #Demons. ACC↑ ACC↑ 다수 해당 없음/해당 없음 50.50.50.GPT-2*해당 없음 68.311.MemTRM67.512.LONGMEM71.814.GPT-2*MemTRMLONGMEMN/A68.211.5 63.45.65.19.78.014.64.712.5 51.94.64.611.3 53.26.0 29.64.65.111.0 53.83.7 36.06.57.610.2 33.66.20.31.44.Subj SST-5 MPQA ACC↑ АССТ ACC↑ 50.평균 44.61.511.8 55.63.012.55.65.412.58.70.87.58.78.63.65.19.3 58.210.6 31.96.3 72.77.65.68.5 36.57.5 74.67.3 66.58.표 5: 5개 NLU 작업(SST-2, mr, subj, SST-5, mpqa)에 대한 4샷 및 20샷 ICL의 정확도[%]. 2000개의 추가 데모 예제를 샘플링하여 캐시된 메모리에 로드합니다. 아래 첨자는 6회 실행에 대한 표준 편차입니다. Avg.는 5개 데이터 세트에 대한 평균 정확도를 나타냅니다. 결과에 따르면 캐시된 메모리에 로드된 데모 예제는 맥락 내 학습에 도움이 되는 보조적 맥락적 데모로 간주될 수 있습니다. LONGMEM 모델은 더 나은 맥락 내 학습을 위해 로컬 맥락적 데모와 메모리 내 증강 데모에서 작업 관련 지식을 모두 수집할 수 있습니다. 3. 절제 연구 모델 GPT-2* MemTRM LONGMEM EM F22.282.3 30.782.22.843.5 32.652.26.772.3 35.702. 표 4: SQUAD에서 3-샷(약 1k 토큰) 맥락 내 학습의 정확한 일치(EM) 및 F1 점수. LONGMEM은 200개의 추가 데모 예제를 캐시된 메모리에 로드합니다. 지금까지 우리는 긴 맥락 모델링, 긴 맥락 이해 및 많은 샷 맥락 내 학습을 위해 캐시된 메모리를 활용하는 데 있어 LONGMEM의 효과와 우수성을 경험적으로 검증했습니다. 캐시된 메모리 뱅크의 설계에는 메모리 크기 msz 및 청크 크기 csz와 같은 많은 하이퍼파라미터가 포함되므로 이러한 하이퍼파라미터가 작업 성능에 미치는 영향을 평가하기 위해 일련의 절제 연구를 수행합니다.청크 크기의 효과.앞서 분석한 것처럼 청크 크기 csz는 검색의 세분성을 제어하므로 컨텍스트 내 학습과 같이 세분화된 검색이 필요한 작업에 차이를 만들 수 있습니다.컨텍스트 내 학습에 대한 다양한 청크 크기 csz Є {2,4,8}의 효과에 대한 절제 연구를 수행하고 그 결과를 그림 4(a)에 표시합니다.청크 크기 2는 5개의 NLU 데이터 세트에서 컨텍스트 내 학습 작업에서 가장 좋은 성능을 발휘하는데, 이는 세분화된 검색과 분류 레이블 토큰에 대한 융합이 필요한 NLU 작업의 속성과 일치합니다.메모리 크기의 효과.메모리 크기(msz)는 메모리 뱅크의 용량을 제어합니다. 일반적으로 메모리 크기는 문서 또는 컨텍스트의 평균 길이와 호환되어야 합니다. 즉, 평균 16k 토큰이 있는 책 세트는 캐시된 메모리에 16k 토큰의 메모리 크기를 배포해야 합니다. 65개 토큰의 학습 msz는 전체 접두사 컨텍스트 길이가 65k 토큰을 초과하지 않기 때문에 ChapterBreak와 같은 다운스트림 작업에 과도합니다. 따라서 추론 단계에서 메모리 크기 msz = {8k, 16k, 32k, 65k}의 효과에 대한 절제 연구를 PG 언어 모델링 데이터 세트에서 수행했으며 그 결과는 그림 4(b)에 나와 있습니다. 평균 8k-50k 길이의 책을 모델링하기 위해 대상 책의 평균 길이와 일치하는 더 작은 메모리 크기 16k가 가장 좋은 복잡도를 제공합니다. 4
--- RELATED WORK ---
대규모 언어 모델. 대규모 언어 모델, 즉 GPT-2[RWC+19], GPT-3[BMR+20], OPT[ZRG+22], BLOOM[SFA+22]은 NLP 연구에 큰 혁명을 일으켰고 다양한 언어 이해, 언어 생성[WZG+22], 심지어 시각 언어 작업[WDC+22]의 최첨단 기술을 촉진했습니다. 또한, 모델 매개변수를 조정함으로써 LLM은 소수의 문맥 내 학습[BMR+20], 다단계 추론[WWS+22], 코드 완성 등과 같은 &quot;새로운 능력&quot;[WTB+22]을 보입니다.정확도78 77.78.675.72.71.874.672.65.59.8 61.34.436.534.SST-MR 과목 sst-MPQA ■csz=■csz=csz=APPL 0.10.0-0.15-10K 10-100K 0.1-0.5M PG 분할-msz=65k msz=32k msz=16k msz=8k 0.5-1M (a) (b) 그림 4: (a) 추론 중에 다른 청크 크기가 주어진 5개 NLU 데이터 세트에 대한 정확도; (b) 추론 중에 메모리 크기가 다른 PG-22의 4개 분할에 대한 APerplexity. msz=65k일 때의 perplexity가 기준선으로 사용됩니다. x-formers. 변환기가 더 긴 컨텍스트에 주의를 기울일 수 있도록 하기 위해 &quot;x-formers&quot;의 많은 변형이 제안됩니다. Transformer-XL [DYY+19]은 과거 세그먼트의 어텐션 키와 값을 캐시하고 이를 반복적으로 재사용하는 것을 제안합니다. LinFormer [WLK+20], LongFormer [BPC20], Routing Transformer [RSVG21]를 포함한 x-formers의 최근 선구적 연구는 O(n²) 복잡도를 O(n log n) 또는 O(n)으로 줄이기 위한 다양한 희소 어텐션 메커니즘을 제안했습니다. BigBird [ZGD+20]는 컨텍스트 토큰의 하위 집합에 주의를 기울여 4k 시퀀스 길이를 달성합니다. 이러한 x-formers는 상당한 효율성 개선을 달성하지만 책 수준 길이에 걸친 시퀀스를 모델링할 때 이러한 효율성 향상은 눈에 띄지 않습니다. 게다가 이러한 x-formers의 가장 큰 시퀀스 길이는
--- METHOD ---
ChapterBreak에서 강력한 장문맥 모델을 능가하며, LLM에 비해 메모리 증강형 문맥 내 학습에서 현저한 개선을 이룹니다. 결과는 제안된 방법이 언어 모델이 장문 콘텐츠를 기억하고 활용하는 데 효과적임을 보여줍니다. 저희 코드는 https://aka.ms/Long Mem에서 오픈 소스로 제공됩니다. 소개 대규모 언어 모델(LLM)은 다양한 이해 및 생성 작업에서 최첨단 기술을 발전시키는 데 큰 성공을 거두며 자연어 처리에 혁명을 일으켰습니다[DCLT19, RWC+19, LOG+19, YDY+19, BMR+20, RSR+20]. 대부분의 LLM은 고정 크기의 로컬 컨텍스트에서 지식을 수확하여 대규모 코퍼스에 대한 자기 지도 학습의 이점을 얻고, 예를 들어 제로 샷 프롬핑[RWC+19], 컨텍스트 내 학습[BMR+20], 사고의 사슬(COT) 추론[WWS+22]과 같은 새로운 능력을 보여줍니다. 그럼에도 불구하고 기존 LLM의 입력 길이 제한으로 인해 고정 크기 세션을 넘어서는 장형 정보 처리 능력이 중요한 실제 시나리오(예: 긴 수평 계획)로 일반화할 수 없습니다. 길이 제한 문제를 해결하기 위한 가장 간단한 방법은 입력 컨텍스트 길이를 단순히 확장하는 것입니다. 예를 들어, GPT-3[BMR+20]은 더 나은 장거리 종속성을 캡처하기 위해 입력 길이를 GPT-2[RWC+19]의 1k에서 2k 토큰으로 늘립니다. 그러나 이 접근 방식은 일반적으로 처음부터 계산 집약적 학습을 초래하고 맥락 내 밀도 주의는 여전히 Transformer 자기 주의의 2차 계산 복잡도에 의해 크게 제한됩니다[VSP+17]. 다른 최근 작업 라인[BPC20, ZGD+20]은 대신 자기 주의의 2차 비용을 피하기 위해 맥락 내 희소 주의 개발에 초점을 맞추고 있으며, 여전히 대부분 처음부터 학습이 필요합니다. 대조적으로, 저명한 연구인 Memorizing Transformer(MemTRM) [WRHS22]는 키, 값 쌍을 사용하여 컨텍스트 내 캐시된 메모리 뱅크를 근사합니다.장기 메모리 검색→ 검색된 Attn 키 및 값 Attn 키 Attn 키 및 값 및 값(세그 A)(세그 B) Attn 키 및 값(세그 Z) 현재 입력의 어텐션 쿼리 메모리 퓨전 대규모 언어 모델(고정) 잔여 잔여 SideNet 연결(학습 가능) ABD 긴 시퀀스 입력 YZ 그림 1: LONGMEM의 메모리 캐싱 및 검색 흐름 개요. 긴 텍스트 시퀀스는 고정 길이 세그먼트로 분할된 다음 각 세그먼트가 대규모 언어 모델을 통해 전달되고 m번째 계층의 어텐션 키 및 값 벡터가 장기 메모리 뱅크에 캐시됩니다. 미래의 입력을 위해, 어텐션 쿼리 키 기반 검색을 통해 장기 메모리의 상위 k 어텐션 키-값 쌍이 검색되어 언어 모델링으로 융합됩니다. 트랜스포머를 위한 미분 불가능한 메모리에서 검색된 컨텍스트 내 토큰과 기억된 토큰 모두에 대한 밀도 있는 어텐션을 통한 희소한 어텐션. 따라서 MemTRM은 최대 65k 토큰을 처리하도록 결과 언어 모델을 확장하고 전체 길이의 책이나 긴 논문을 모델링하는 데 상당한 복잡도 이득을 얻습니다. 그러나 MemTRM은 메모리 인코딩과 언어 모델링을 위한 메모리 융합에 단일 모델을 사용하는 결합된 메모리 설계로 인해 훈련 중에 메모리 진부화 문제에 직면합니다. 즉, 모델 매개변수가 업데이트됨에 따라 메모리에 캐시된 이전 표현은 최신 모델의 표현과 분포가 다를 수 있으므로 메모리 증강의 효과가 제한됩니다. 이 논문에서 우리는 언어 모델이 장문의 이전 맥락이나 지식을 미분 불가능한 메모리 뱅크에 캐시하고, 분리된 메모리 모듈을 통해 이를 활용하여 메모리 진부화 문제를 해결할 수 있는 장기 메모리로 증강된 언어 모델(LONGMEM) 프레임워크를 제안합니다. 분리된 메모리를 달성하기 위해 새로운 잔여 사이드 네트워크(SideNet)를 설계합니다. 이전 맥락의 페어링된 어텐션 키와 값은 동결된 백본 LLM을 사용하여 메모리 뱅크로 추출됩니다. SideNet의 메모리 증강 계층에서 현재 입력의 생성된 어텐션 쿼리는 메모리에서 이전 맥락의 캐시된 (키, 값)을 검색하는 데 사용되고, 해당 메모리 증강은 조인트 어텐션 메커니즘을 통해 학습된 숨겨진 상태로 융합됩니다. 또한, SideNet과 동결된 백본 LLM 간의 새로 설계된 교차 네트워크 잔여 연결은 사전 학습된 백본 LLM에서 더 나은 지식 전송을 가능하게 합니다. 잔여 SideNet을 지속적으로 훈련하여 메모리 증강된 긴 컨텍스트를 검색하고 융합함으로써 사전 훈련된 LLM을 조정하여 개선된 모델링을 위해 긴 컨텍스트 메모리를 활용할 수 있습니다. 자세한 메모리 캐시, 검색 및 융합 프로세스는 그림 1에 나와 있습니다. 분리된 메모리 설계는 두 가지 주요 이점을 제공합니다. 첫째, 제안된 아키텍처는 이전 입력을 메모리로 인코딩하는 프로세스와 분리된 동결된 백본 LLM 및 SideNet을 통한 메모리 검색 및 융합 프로세스를 분리합니다. 이런 방식으로 백본 LLM은 긴 컨텍스트 지식 인코더로만 작동하는 반면, 잔여 SideNet은 메모리 검색기 및 판독기로 작동하여 메모리 진부화 문제를 효과적으로 해결합니다. 둘째, 메모리 증강으로 전체 LLM을 직접 조정하는 것은 계산적으로 비효율적이며 치명적인 망각으로 어려움을 겪습니다. 백본 LLM은 효율적인 메모리 증강 적응 단계에서 동결되므로 LONGMEM은 사전 훈련된 지식을 활용할 수 있을 뿐만 아니라 치명적인 망각도 피할 수 있습니다. LONGMEM은 다운스트림 작업에 따라 다양한 유형의 장문 텍스트와 지식을 메모리 뱅크로 가져올 수 있습니다. 여기서는 전체 길이의 책 맥락을 사용한 언어 모델링과 수천 개의 작업 관련 데모 예제를 사용한 메모리 증강 맥락 내 학습이라는 두 가지 대표적인 사례를 고려합니다. 구체적으로, 다양한 장문 언어 모델링과 언어 이해를 위한 메모리 증강 맥락 내 학습에 대한 제안된 LONGMEM의 효과를 평가합니다.
--- EXPERIMENT ---
s는 우리의 방법이 까다로운 장문맥 모델링 벤치마크인 ChapterBreak에서 강력한 장문맥 모델을 능가하고 LLM에 비해 메모리 증강형 문맥 내 학습에서 현저한 개선을 이룬다는 것을 보여줍니다. 결과는 제안된 방법이 언어 모델이 장문 내용을 기억하고 활용하는 데 효과적임을 보여줍니다. 우리의 코드는 https://aka.ms/Long Mem에서 오픈 소스로 제공됩니다. 서론 대규모 언어 모델(LLM)은 다양한 이해 및 생성 작업에서 최첨단 기술을 발전시키는 데 큰 성공을 거두며 자연어 처리에 혁명을 일으켰습니다[DCLT19, RWC+19, LOG+19, YDY+19, BMR+20, RSR+20]. 대부분의 LLM은 고정 크기의 로컬 컨텍스트에서 지식을 수확하여 대규모 코퍼스에 대한 자기 지도 학습의 이점을 얻고, 예를 들어 제로 샷 프롬핑[RWC+19], 컨텍스트 내 학습[BMR+20], 사고의 사슬(COT) 추론[WWS+22]과 같은 새로운 능력을 보여줍니다. 그럼에도 불구하고 기존 LLM의 입력 길이 제한으로 인해 고정 크기 세션을 넘어서는 장형 정보 처리 능력이 중요한 실제 시나리오(예: 긴 수평 계획)로 일반화할 수 없습니다. 길이 제한 문제를 해결하기 위한 가장 간단한 방법은 입력 컨텍스트 길이를 단순히 확장하는 것입니다. 예를 들어, GPT-3[BMR+20]은 더 나은 장거리 종속성을 캡처하기 위해 입력 길이를 GPT-2[RWC+19]의 1k에서 2k 토큰으로 늘립니다. 그러나 이 접근 방식은 일반적으로 처음부터 계산 집약적 학습을 초래하고 맥락 내 밀도 주의는 여전히 Transformer 자기 주의의 2차 계산 복잡도에 의해 크게 제한됩니다[VSP+17]. 다른 최근 작업 라인[BPC20, ZGD+20]은 대신 자기 주의의 2차 비용을 피하기 위해 맥락 내 희소 주의 개발에 초점을 맞추고 있으며, 여전히 대부분 처음부터 학습이 필요합니다. 대조적으로, 저명한 연구인 Memorizing Transformer(MemTRM) [WRHS22]는 키, 값 쌍을 사용하여 컨텍스트 내 캐시된 메모리 뱅크를 근사합니다.장기 메모리 검색→ 검색된 Attn 키 및 값 Attn 키 Attn 키 및 값 및 값(세그 A)(세그 B) Attn 키 및 값(세그 Z) 현재 입력의 어텐션 쿼리 메모리 퓨전 대규모 언어 모델(고정) 잔여 잔여 SideNet 연결(학습 가능) ABD 긴 시퀀스 입력 YZ 그림 1: LONGMEM의 메모리 캐싱 및 검색 흐름 개요. 긴 텍스트 시퀀스는 고정 길이 세그먼트로 분할된 다음 각 세그먼트가 대규모 언어 모델을 통해 전달되고 m번째 계층의 어텐션 키 및 값 벡터가 장기 메모리 뱅크에 캐시됩니다. 미래의 입력을 위해, 어텐션 쿼리 키 기반 검색을 통해 장기 메모리의 상위 k 어텐션 키-값 쌍이 검색되어 언어 모델링으로 융합됩니다. 트랜스포머를 위한 미분 불가능한 메모리에서 검색된 컨텍스트 내 토큰과 기억된 토큰 모두에 대한 밀도 있는 어텐션을 통한 희소한 어텐션. 따라서 MemTRM은 최대 65k 토큰을 처리하도록 결과 언어 모델을 확장하고 전체 길이의 책이나 긴 논문을 모델링하는 데 상당한 복잡도 이득을 얻습니다. 그러나 MemTRM은 메모리 인코딩과 언어 모델링을 위한 메모리 융합에 단일 모델을 사용하는 결합된 메모리 설계로 인해 훈련 중에 메모리 진부화 문제에 직면합니다. 즉, 모델 매개변수가 업데이트됨에 따라 메모리에 캐시된 이전 표현은 최신 모델의 표현과 분포가 다를 수 있으므로 메모리 증강의 효과가 제한됩니다. 이 논문에서 우리는 언어 모델이 장문의 이전 맥락이나 지식을 미분 불가능한 메모리 뱅크에 캐시하고, 분리된 메모리 모듈을 통해 이를 활용하여 메모리 진부화 문제를 해결할 수 있는 장기 메모리로 증강된 언어 모델(LONGMEM) 프레임워크를 제안합니다. 분리된 메모리를 달성하기 위해 새로운 잔여 사이드 네트워크(SideNet)를 설계합니다. 이전 맥락의 페어링된 어텐션 키와 값은 동결된 백본 LLM을 사용하여 메모리 뱅크로 추출됩니다. SideNet의 메모리 증강 계층에서 현재 입력의 생성된 어텐션 쿼리는 메모리에서 이전 맥락의 캐시된 (키, 값)을 검색하는 데 사용되고, 해당 메모리 증강은 조인트 어텐션 메커니즘을 통해 학습된 숨겨진 상태로 융합됩니다. 또한, SideNet과 동결된 백본 LLM 간의 새로 설계된 교차 네트워크 잔여 연결은 사전 학습된 백본 LLM에서 더 나은 지식 전송을 가능하게 합니다. 잔여 SideNet을 지속적으로 훈련하여 메모리 증강된 긴 컨텍스트를 검색하고 융합함으로써 사전 훈련된 LLM을 조정하여 개선된 모델링을 위해 긴 컨텍스트 메모리를 활용할 수 있습니다. 자세한 메모리 캐시, 검색 및 융합 프로세스는 그림 1에 나와 있습니다. 분리된 메모리 설계는 두 가지 주요 이점을 제공합니다. 첫째, 제안된 아키텍처는 이전 입력을 메모리로 인코딩하는 프로세스와 분리된 동결된 백본 LLM 및 SideNet을 통한 메모리 검색 및 융합 프로세스를 분리합니다. 이런 방식으로 백본 LLM은 긴 컨텍스트 지식 인코더로만 작동하는 반면, 잔여 SideNet은 메모리 검색기 및 판독기로 작동하여 메모리 진부화 문제를 효과적으로 해결합니다. 둘째, 메모리 증강으로 전체 LLM을 직접 조정하는 것은 계산적으로 비효율적이며 치명적인 망각으로 어려움을 겪습니다. 백본 LLM은 효율적인 메모리 증강 적응 단계에서 동결되므로 LONGMEM은 사전 훈련된 지식을 활용할 수 있을 뿐만 아니라 치명적인 망각도 피할 수 있습니다. LONGMEM은 다운스트림 작업에 따라 다양한 유형의 장문 텍스트와 지식을 메모리 뱅크로 가져올 수 있습니다. 여기서는 전체 길이의 책 맥락을 사용한 언어 모델링과 수천 개의 작업 관련 데모 예제를 사용한 메모리 증강 맥락 내 학습이라는 두 가지 대표적인 사례를 고려합니다. 구체적으로, 다양한 장문 언어 모델링과 언어 이해를 위한 메모리 증강 맥락 내 학습에 대한 제안된 LONGMEM의 효과를 평가합니다. 실험 결과는 우리 모델이 장문 모델링과 맥락 내 학습 능력 측면에서 강력한 기준선을 지속적으로 능가한다는 것을 보여줍니다. 영어: 우리 방법은 실질적으로 키, 값 쌍이 있는 캐시된 메모리 뱅크 캐시 Attn 키와 값 g 언어 모델 헤드 메모리 검색 및 퓨전 LLM 디코더 계층 HL K LLM 디코더 계층 V SideNet MemAug 계층 LLM 디코더 계층 메모리 퓨전 Q LLM 디코더 계층 SideNet 계층 h²+임베딩 계층 gh 현재 입력 동결 계층 학습 가능 계층 키, 값 쌍이 있는 캐시된 메모리 뱅크 Ꮎ 잔여 토큰-청크 검색 {k} {V} ht 그림 2: LONGMEM 아키텍처 개요. &quot;MemAug&quot;는 메모리 증강 계층을 나타냅니다. Gutenberg-2022 코퍼스의 다양한 길이 분할에 대해 LLM의 긴 컨텍스트 언어 모델링 기능을 -1.38~-1.62 복잡도만큼 개선합니다. 놀랍게도, 우리 모델은 까다로운 장문맥 모델링 벤치마크인 ChapterBreak에서 40.5%의 식별 정확도라는 최첨단 성능을 달성하여 기존의 강력한 x-former 베이스라인을 크게 능가합니다. 마지막으로, 메모리에 2k 데모 예제가 있는 LONGMEM은 MemTRM 및 비메모리 증강 베이스라인과 비교하여 인기 있는 NLU 작업에서 현저한 컨텍스트 내 학습 개선을 보여줍니다. 2가지 방법 LLM이 메모리의 과거 장문맥에서 관련 정보를 수집할 수 있도록 하기 위해, 우리는 분리된 메모리 모듈로 동결된 백본 LLM을 증강하는 것을 제안합니다. 메모리 컨텍스트 정보를 융합하기 위해, 효율적인 방식으로 지속적으로 학습할 수 있는 새로운 경량 잔여 SideNet을 설계합니다. 다음에서, 우리는 먼저 메모리 증강을 통한 언어 모델링의 문제 공식화에 대해 논의합니다. 그런 다음, 우리는 동결된 사전 학습된 LLM을 로컬 입력 컨텍스트와 검색된 메모리 컨텍스트에 공동으로 참석하도록 조정하기 위한 효율적인 잔여 SideNet을 공식적으로 소개합니다. 마지막으로, 언어 모델링을 위해 과거 기억이 인코딩, 저장, 회상 및 융합되는 방식에 대한 설계된 프로세스를 제공합니다.2.1 장기 기억으로 증강된 언어 모델 여기서는 고수준 문제 설정에 초점을 맞추고 더 많은 구성 요소 세부 정보는 나중 섹션으로 미룹니다.사전 훈련된 LLM에 널리 채택됨에 따라 LONGMEM 모델은 Transformer 아키텍처[VSP+17]를 기반으로 구축됩니다.LONGMEM의 경우 세 가지 핵심 구성 요소가 있습니다.동결 백본 LLM, SideNet 및 캐시 메모리 뱅크입니다.대부분의 기존 사전 훈련된 LLM은 고정 크기의 입력만 받을 수 있으므로 길이 제한에 맞는 긴 시퀀스(예: 책)의 입력 세그먼트만 대부분의 기존 자기 회귀 언어 모델에서 수행되는 것처럼 현재 입력으로 표시됩니다.맞을 수 없는 이전 세그먼트는 이전 입력으로 표시되며 이는 메모리 증강에 사용됩니다.사전 훈련된 LLM의 학습된 지식을 활용하기 위해 이전 및 현재 입력은 모두 동결 백본 LLM을 사용하여 인코딩되지만 다른 표현이 추출됩니다. 이전 입력의 경우 m번째 계층의 Transformer 셀프 어텐션에서 나온 키-값 쌍은 캐시 메모리 뱅크에 저장되는 반면, 현재 입력에 대한 각 LLM 디코더 계층의 숨겨진 상태는 유지되어 SideNet으로 전송됩니다. 각 현재 입력 토큰의 경우, 가장 관련 있는 키-값 벡터 쌍이 언어 모델링을 위한 메모리 증강으로 검색됩니다. SideNet 모듈은 분리된 메모리에서 현재 입력 컨텍스트와 관련 캐시된 이전 컨텍스트를 융합하도록 훈련된 효율적인 적응 모델로 볼 수 있습니다. 공식적으로, 고정 크기의 입력 텍스트 시퀀스 {x;}!(현재 입력)의 경우, LONGMEM은 먼저 기울기 계산 없이 백본 LLM(그림 2에서 파란색으로 표시)을 사용하여 순방향 패스를 수행합니다. 백본 LLM의 임베딩 계층은 먼저 입력 {x;}을 임베딩 공간으로 인코딩하고 초기 숨겨진 상태 HOLM Є Rª|×E를 출력합니다. 여기서 E는 숨겨진 차원입니다. 그런 다음 동결된 백본 LLM의 각 연속적인 Transformer 디코더 계층은 이전 계층의 숨겨진 상태를 사용하여 새로운 숨겨진 상태 i=를 계산합니다.HLM = LLM fou (LLM), VI′ = [1, L′]이고 L&#39;은 백본 LLM의 총 계층 수입니다.모든 이전 입력에 대한 백본 LLM을 사용한 순방향 패스 동안 m번째 Transformer 디코더 계층에서 셀프 어텐션에 사용된 키-값 쌍은 캐시된 메모리 뱅크(그림 2의 왼쪽 상단 모서리에 주황색으로 표시)에 저장되며 나중에 미래 입력에 대한 메모리 증강으로 호출됩니다.Cached Memory Bank는 최신 M 이전 입력 K, Ñ € RH×|x|×d의 어텐션 키-값 쌍을 유지하는 캐시된 헤드별 벡터 큐 Zk, Zv € RH×M×d입니다.여기서 H, d는 각각 어텐션 헤드 수와 헤드당 차원을 나타냅니다. 메모리 검색 및 융합(§2.3) 후, 메모리 뱅크는 가장 오래된 시퀀스의 키-값 쌍을 제거하고 현재 시퀀스를 캐시된 벡터 뱅크에 추가합니다. 따라서 이러한 업데이트 메커니즘은 시퀀스 수준에서 언어 모델링 인과 관계를 보장하고 메모리 뱅크가 항상 현재 입력에 대한 가장 가까운 이전 컨텍스트의 레코드를 유지할 수 있도록 합니다. 백본 LLM을 사용한 포워드 패스 후, SideNet 모듈은 백본 LLM {HLM}\₁에서 모든 현재 입력 숨겨진 상태와 캐시된 메모리 뱅크의 과거 키-값 쌍을 가져와 메모리 증강 표현을 계산합니다. 구체적으로, LONGMEM의 SideNet은 (L1) 일반 Transformer 디코더 계층과 하나의 특수 메모리 증강 디코더 계층으로 구성됩니다. 효율적인 목적을 위해, 우리는 주로 SideNet의 #layers L이 백본 LLM의 #layers L보다 작은 경우, 즉 L &lt; L&#39;인 경우를 고려합니다. SideNet은 (L – 1)개의 일반 Transformer 디코더 계층과 특수한 메모리 증강 계층을 통해 Hº를 메모리 증강 컨텍스트 표현으로 인코딩합니다.ms Side j=ms-Side 메모리 증강 계층은 메모리 증강 입력을 받는 vanilla Transformer 디코더 계층의 확장으로, 메모리의 가장 관련 있는 키-값 쌍과 현재 입력의 숨겨진 상태를 모두 포함합니다.여기서 캐시된 키-값 쌍은 토큰 기반 메모리 검색 모듈(§2.3)을 사용하여 회수됩니다.각 현재 입력 토큰에 대해 메모리 검색 모듈 srt(:)는 메모리 뱅크 {kij, ~ij}} \₁ = Srt(xi)에서 가장 관련 있는 K개의 키-값 쌍을 검색합니다. 그런 다음 SideNet은 메모리 증강 입력 Hode = ƒØMem (Hode¯¹‚, {{kij‚ Ñij}-1}!¦ª¦₁)을 사용하여 출력을 계산합니다. 여기서 는 메모리 증강 계층을 주입하는 계층 인덱스입니다. 마지막으로 토큰 확률은 마지막 SideNet 숨김 상태 P(xi|×1, ..., Xi−1) = softmax(WH)를 사용하여 계산합니다. 여기서 W는 백본 LLM과 SideNet이 공유하는 동결된 출력 임베딩 가중치입니다. 분리된 메모리를 활용하기 위해 LONGMEM에 대한 메모리 증강 적응 학습을 수행합니다. 생성적 비지도 사전 학습[RNSS18]에 따라 LONGMEM의 학습 목표는 표준적인 왼쪽에서 오른쪽으로의 언어 모델링 목표이며, 이는 왼쪽 맥락을 기반으로 다음 토큰의 가능성을 최대화합니다. max Σ RED Σx log P(xi|×1, ·‚×i–여기서 x는 사전 학습 텍스트 코퍼스 D에서 무작위로 샘플링된 문장입니다. ... (i-1), 2.2 잔차 SideNet Side SideNet 아키텍처 및 초기화. 여기서도 Transformer [VSP+17]에 기반한 SideNet을 구현합니다. 여기서 SideNet의 디코더 계층 수 L은 백본 LLM의 계층 수 L&#39;을 감소 계수(이 작업 전체에서 계층 감소 계수 2 L&#39; = 2L)로 나눈 것과 같습니다. SideNet의 각 디코더 계층의 가중치는 동일한 깊이를 가진 백본 LLM의 해당 사전 학습된 디코더 계층에서 초기화됩니다. 03 de = 0 LM. 그림 2에서 볼 수 있듯이, SideNet은 백본 LLM의 임베딩 계층의 출력을 가져와 백본 LLM의 언어 모델링 헤드 계층을 재사용하는데, 이 계층은 지속적인 적응 단계에서도 동결됩니다. 메모리 증강 적응 단계에서는 SideNet의 다른 모든 매개변수가 학습 신호에 따라 적절히 업데이트됩니다. 이런 식으로 가벼운 SideNet은 사전 학습된 매개변수에서 전송된 지식으로 빠른 수렴을 달성합니다. 교차 네트워크 잔여 연결. 사전 학습된 백본 LLM의 지식을 활용하기 위해 제안된 교차 네트워크 잔여 연결을 사용하여 백본 LLM의 표현을 SideNet으로 융합합니다. 구체적으로, 백본 LLM의 21번째와 (212)번째 계층에서 출력 숨겨진 상태의 차이를 SideNet의 1번째 계층에서 출력 숨겨진 상태에 대한 잔여 연결로 추가합니다. 그런 다음, SideNet의 다음 (1 + 1)번째 계층에 대한 입력은 (H)에 대한 이전 계층을 통해 전달된 원래의 숨겨진 상태와 백본 LLM Hside = forside (H&#39; side) + (H²LM − H²LM²), VE [1, L], (1)에서 숨겨진 상태 차이의 교차 네트워크 Side Sideresidual 연결의 합입니다.여기서 Ho는 임베딩 계층의 출력입니다.디코더 계층 [VSP+17]의 셀프 어텐션 및 피드포워드 네트워크 이후의 잔여 연결은 fo(H)에서 정상적으로 수행되고 제안된 교차 네트워크 잔여 연결과 병렬로 수행된다는 점에 주목할 가치가 있습니다.Side Side 2.3 메모리 검색 및 융합 LONGMEM의 장기 메모리 기능은 검색 및 융합을 위한 메모리 증강 모듈을 통해 달성됩니다.토큰-청크 메모리 검색. 토큰 대 토큰 검색을 수행하는 대신, 가속 및 무결성을 위해 토큰 대 청크 검색에 집중합니다. 텍스트 청크는 청크 크기 csz 개수의 연속 토큰으로 구성된 n-gram 구조를 말합니다. 메모리 뱅크는 토큰 청크 수준에서 캐시된 키-값 쌍을 저장합니다. 메모리 뱅크를 M/csz 어텐션 키-값 쌍 청크로 나누고 청크 크기 차원의 평균 풀링 벡터를 사용하여 검색을 위한 키 벡터를 구합니다. 그런 다음 현재 입력 토큰의 어텐션 쿼리와 후보 청크의 평균 풀링 어텐션 키 간의 점곱에 대한 상위(K/csz) 어텐션 키-값 청크를 검색합니다. 마지막으로 검색된 키-값 쌍 청크에 대한 청크 크기 차원을 압축하고 토큰 수준 {K¿‚ Ñ;}Ķ1에서 K 키-값 쌍으로 평면화합니다. 토큰-청크 검색을 채택하면 검색 인덱스의 크기 jj=1*이 줄어들고 프로세스가 가속화됩니다. 한편, 검색 정확도는 더욱 향상될 수 있으며, 이는 [LGW+23] 및 [BMH+21]에서도 관찰됩니다. 하이퍼파라미터 청크 크기 csz는 검색된 컨텍스트의 세분성을 제어하며, 이는 다운스트림 작업에 따라 경험적으로 조정할 수 있습니다. 예를 들어, 컨텍스트 내 학습에는 메모리에 캐시된 데모 예제에서 더 세분화된 레이블 토큰이 필요한데, 이때 더 작은 csz가 유용합니다. 메모리 융합. 메모리 융합은 특수 메모리 증강 계층 내에서 수행됩니다. 기존 Transformer 디코더 계층은 멀티헤드 셀프 어텐션 [VSP+17]을 사용하므로 [WRHS22]에 따라 이를 조인트 어텐션 메커니즘으로 확장하고 각 토큰이 로컬 컨텍스트와 검색된 메모리 컨텍스트에 모두 참여할 수 있도록 장기 메모리 융합 프로세스를 제안합니다. 이전 계층 H¹-¹ € R|×|×d의 헤드별 숨김 상태 출력과 해당 검색된 어텐션 키-값 쌍이 {K¿‚ Ñ¿}¦ª¦₁ € R×××d인 경우 1번째 메모리 증강 계층 H²의 출력 숨김 상태는 다음과 같이 계산됩니다. 1-QKT QiKT √d iJi=1&#39; (2) A softmax( -)V, M = Concat{softmax(H = sigmoid (g) A + (1 − sigmoid(g)) · M, (3) 여기서 Q, K, V, A, M Є R||×d, K는 각 토큰에 대한 캐시된 메모리에서 검색된 어텐션 키-값 쌍의 수이고 g는 학습 가능한 헤드별 게이팅 벡터입니다. 이전 계층 H(1-1)의 숨김 상태 출력은 세 가지를 통해 어텐션 쿼리, 키 및 값 Q, K, V로 선형적으로 투영됩니다. 행렬 WQ, WK, WV Ĕ Rdxd. 캐시된 메모리에서 검색된 어텐션 키-값 쌍은 각 토큰마다 다르다는 점에 유의해야 합니다.3 실험 Є 우리는 요구되는 메모리 내 긴 컨텍스트를 기반으로 다양한 작업에서 제안된 LONGMEM 모델을 평가합니다.a) 과거 긴 컨텍스트를 캐시된 메모리에 로드할 때 긴 텍스트 언어 모델링 및 언어 이해;b) 대량의 데모 예제를 캐시된 메모리에 로드할 때 무한 길이의 컨텍스트 내 학습.3.1 훈련 설정 훈련 코퍼스 일괄 처리.대규모 코퍼스에 대한 기존 일괄 처리 프로세스는 패딩 없이 전체 코퍼스를 연속된 고정 길이 텍스트 세그먼트로 자르고 모든 세그먼트를 셔플하여 미니 배치를 구성합니다[RWC+19].반대로, LONGMEM은 글로벌 셔플링을 비활성화하고 세그먼트 수준에서 글로벌 인과성을 보장해야 합니다. 첫째, 훈련 기업의 모든 긴 문서를 동일한 길이의 배치 크기 수의 문서 그룹으로 나눈 다음 각 그룹 내에서 문서 수준 셔플링을 수행합니다. 그런 다음 한 그룹 내에서 셔플링된 문서를 연결하고 정렬된 세그먼트로 잘라냅니다. 배치화 후 한 긴 문서의 두 연속 세그먼트가 두 개의 연속된 입력 배치에 분배되도록 하기 위해 동일한 내부 그룹 인덱스를 가진 배치 크기 수의 문서 그룹에서 하나의 세그먼트를 선택합니다. 따라서 배치 크기 수의 세그먼트를 가진 미니 배치는 정확히 배치 크기 수의 문서 그룹에서 구성됩니다. 이런 식으로 훈련 반복 단계로서 메모리 뱅크에 캐시된 어텐션 키-값 쌍은 동일한 문서 내의 현재 입력의 정확히 이전 컨텍스트입니다. 일괄 처리 프로세스는 그림 3에 설명되어 있습니다.긴 문서 문서 그룹화 잘린 세그먼트 일괄 처리 S11 S12 $13 SSS12 SS₁N DocumentS15 $S1N S21 S22 SS2N ... DocumentSS32 SS3N S21 S22 SSSSSSAN DocumentBatch 1 Batch 2 BatchBatch N $SS2N 캐시된 메모리 5번째 반복 중에 업데이트 DocumentSS32 SSS₁₁ S12 S13 S14 S12 S13 S14 SDocumentSSS3N SS22 S23 SSS23 S24 S♡ S31 S32 SS$S33 S34 SSS42 S43 SS45 SS4N Document Z ... 个 가장 오래된 것 제거 가장 최근 것 추가 그림 3: 각 문서 내의 연속된 세그먼트가 연속된 일괄 처리로 분배되도록 큰 텍스트 코퍼스를 일괄 처리합니다. = 훈련 코퍼스와 하이퍼파라미터. BookCorpus2, Books3, OpenWebText2, Stack Exchange, Wikipedia, Gutenberg(PG-19), NIH ExPorter, Pile-CC 데이터 세트를 포함하여 Pile [GBB+20]의 하위 집합을 훈련 코퍼스로 샘플링합니다. 원래 GPT-2 [RWC+19]가 LLM이 장거리 종속성을 학습할 수 있도록 하는 데 성능이 좋지 않은 것으로 밝혀진 절대 위치 임베딩을 채택했기 때문에 GPT-2(407M-파라미터)를 Alibi [PSL21] 위치 임베딩이 있는 사전 훈련된 백본 LLM으로 재생성합니다. [DYY+19] 백본 LLM은 L&#39; 24, H16, d = 64 아키텍처를 사용합니다. SideNet은 L = 12, H = 16, d = 64 아키텍처를 사용합니다. 메모리 증강 적응을 위한 훈련은 26B 토큰에서 반복되며, 글로벌 배치 크기는 256이고 시퀀스 길이는 1024입니다. 청크 크기 csz는 토큰 4개이고 메모리 크기 M은 토큰의 키-값 쌍 65k개입니다. 각 토큰에 대해 증강을 위해 K=64개의 어텐션 키-값 쌍을 검색하는데, 이는 K/csz=16개의 텍스트 청크입니다. 메모리 증강 계층은 SideNet의 9번째 계층입니다. 백본 LLM의 18번째 계층의 어텐션 키와 값은 메모리에 캐시되어 향후 검색에 사용됩니다. 기타 훈련 세부 정보는 부록 C. 메모리 검색 모듈에 나와 있습니다. 한 GPU의 캐시된 메모리 뱅크의 고정 메모리 크기는 토큰의 키-값 쌍입니다. 효율성을 위해 각 GPU가 자체 메모리 검색 모듈을 구성하고 업데이트할 수 있도록 합니다. 효율적인 토큰-청크 검색을 구현하기 위해 faiss [JDJ21] 툴킷을 사용하여 GPU에서 정확한 검색 인덱스를 구성하여 텍스트 청크의 평균 풀링된 어텐션 키를 저장하고 효율적인 검색을 수행합니다. faiss 인덱스는 고정된 M/csz 키를 유지하고 내적에 대한 효율적인 정확한 검색을 제공합니다. 검색에는 1k 토큰당 약 15ms가 소요되며 이는 백본 LLM 전달 패스의 55% 시간 비용입니다. 정확한 검색 인덱스를 근사 검색 인덱스로 쉽게 조정하여 검색 효율성을 높일 수 있습니다. 기준선. 사전 훈련된 GPT-2*의 기준선 외에도 메모리 증강 적응 기준선으로 MemTRM(Memorizing Transformer) [WRHS22]을 재생성합니다. MemTRM은 사전 훈련된 LLM을 조정하여 외부 메모리를 사용하도록 쉽게 조정할 수 있습니다. MemTRM에서 제안한 knn-증강 계층을 LLM 디코더의 동일한 18번째 계층으로 삽입합니다. MemTRM 기준선도 동일한 하이퍼파라미터 설정에서 동일한 수의 토큰에 대해 훈련됩니다. 3.2 장문맥 언어 모델링 장문맥 언어 모델링은 검색된 어텐션 키-값에 저장된 지식이 유용한 역할을 할 수 있는 과거 장문맥의 증강된 분리된 메모리에서 쉽게 이점을 얻을 수 있습니다.데이터세트 PG-ArXiv 분할 SSLen. 범위 5K-10K #문서 평균 #토큰 7.6K 10K-100K47.6K S100K-500K140K SS500K-1M &gt;1M640K &lt;60K1.2M 15.4K 표 1: 길이 범위 및 Arxiv에 따른 PG-22의 5개 분할에 대한 데이터세트 통계. 컨텍스트 내 메모리 PG-모델 ArXiv 길이 길이 5K-10K 10K-100K GPT-2* 1k 없음 22.MemTRM 1k 65K 21.24.23.23.24.100K-500K 500K-1M &gt;1M 24.24.97 18.07 11.17.39 10.LONGMEM 1k 65k 21.23.22.23.16.71 10.표 2: 긴 컨텍스트 언어 모델링 데이터 세트에 대한 평가 결과. 모든 데이터 세트에 대해 토큰 수준 복잡도(PPL)(낮을수록 좋음)를 보고합니다. 긴 컨텍스트 언어 모델링에서 모델이 더 나은 성능을 발휘하도록 돕기 위해 중요한 배경 및 맥락 정보를 제공하는 역할. 예를 들어, 장문 텍스트를 정확하게 모델링하려고 할 때 이전 배경 및 캐릭터 관계에서 지식을 습득하면 그에 따른 스토리를 모델링하는 데 도움이 될 수 있습니다.평가 설정. 먼저 LONGMEM과 3개의 장문 컨텍스트 모델링 데이터 세트인 Project Gutenberg 2020-2022, ArXiv, ChapterBreak의 베이스라인을 비교합니다. 이러한 데이터 세트에 포함된 대부분의 책이나 논문은 길이가 최소 16k 토큰입니다. 나열된 모든 데이터 세트는 작업별 튜닝 없이 제로샷 방식으로 평가됩니다. 3개 데이터 세트에 대한 자세한 평가 설정은 다음과 같습니다. • Project Gutenberg 2020-2022 언어 모델링 데이터 세트. Project Gutenberg Library¹에서 2020년과 2022년 사이에 출판된 책을 크롤링하고 정리하여 PG-22라는 완전히 새로운 장문 모델링 데이터 세트를 구축했습니다. PG-19의 훈련 하위 집합과는 도메인과 글쓰기 스타일 면에서 크게 차별화됩니다.PG-19 [RPJL19]의 책은 1919년 이전에 출판되었기 때문입니다.길이 범위에 따라 PG-22의 다른 검증 분할을 제공하고, 데이터 통계는 표 1에 제시되어 있습니다.• ArXiv 데이터 집합.ArXiv 데이터 집합에는 수학, 컴퓨터 과학, 물리학 분야의 논문이 포함됩니다.Pile 코퍼스 [GBB+20]에서 ArXiv 논문 하위 집합의 검증 분할을 선택합니다.Pile의 ArXiv 하위 집합은 훈련에서 제외되었으며 분포 외부 데이터 집합입니다.PG-22 및 Arxiv의 장 컨텍스트 언어 모델링 벤치마크에 대한 토큰 수준 언어 모델링 복잡도를 보고합니다.• ChapterBreak 벤치마크. ChapterBreak는 [STI22]에서 LLM이 이전 장의 긴 맥락을 고려하여 같은 책에서 샘플링한 일련의 하드 네거티브 세그먼트와 다음 장의 기본 진실의 시작을 구별해야 하는 까다로운 접미사 식별 데이터 세트로 제안되었습니다. ChapterBreak는 올바른 접미사를 이해하고 식별하기 위해 글로벌 긴 맥락을 처리해야 합니다. [STI22]는 장문 처리를 위한 최첨단 x-former조차도 ChapterBreak에서 좋은 성능을 발휘하기 위해 장거리 맥락을 효과적으로 활용하지 못한다는 것을 보여주었습니다. 우리는 AO3에서 추출한 팬 픽션이 포함된 ChapterBreak의 Archive of Our Own(AO3) 하위 세트를 선택했습니다. ChapterBreak는 다양한 모델의 길이 제한에 맞게 0.5k에서 8k 토큰까지 접두사 길이를 기반으로 8개의 분할을 제공합니다. 4k, 6k 및 8k 접두사의 분할이 평가를 위해 선택되었습니다. 4k 이상의 토큰을 처리할 수 없는 LLM의 경우, LLM의 최대 입력 길이를 충족하기 위해 앞부분 접두사를 버립니다. MemTRM 및 LONGMEM 모델의 경우, 먼저 주어진 4k/6k/8k 접두사 컨텍스트를 캐시된 메모리에 로드한 다음 스코어링을 수행합니다. 제로 샷 평가 방식으로 각 후보 접미사 세그먼트에 대한 스코어러로 퍼플렉시티를 사용합니다. 그런 다음 퍼플렉시티가 낮은 접미사 세그먼트가 레이블로 선택됩니다. 접미사 식별 정확도가 평가 지표로 사용됩니다. 결과. 평가된 긴 컨텍스트 데이터 세트에 대한 주요 결과는 표 2에 요약되어 있습니다. 제안된 LONGMEM 모델은 긴 텍스트 언어 모델링에서 고려된 모든 기준선을 상당히 능가합니다. https://www.gutenberg.org/ 컨텍스트 내 메모리 내 모델 #Params Len. Len. ChapterBreakactx-4k ctx-6k ctx-8k GPT-2-XL* [RWC+19] 1.5B 1K 없음 24% 24% 24% GPT-3* [BMR+20] 175B 2K 없음 28% 28% 28% LocalTRM [RSVG21] 516M 8K 없음 24% 24% 24% RoutTRM [RSVG21] 490M 8K 없음 25% 24% 24% Bigbird [ZGD+20] 128M 4K 없음 26% 26% 26% GPT-2* 407M 1K 없음 18.4% 18.4% 18.4% MemTRM 407M 1K ∞ LONGMEM 558M 1K ∞ 28.3% 28.7% 28.7% 37.7% 39.4% 40.5% 표 3: ChapterBreak의 AO3 하위 집합에 대한 제로샷 접미사 식별 정확도. †로 표시된 기준선은 [STI22]에서 직접 인용한 것입니다. MemTRM 및 LONGMEM은 주어진 4k/6k/8k 접두사 컨텍스트를 캐시된 메모리에 로드하는 반면 로컬 컨텍스트에 대한 입력 길이는 여전히 1k 토큰입니다. 데이터 세트, PG-22의 다른 길이 분할에서 -1.38에서 -1.62의 복잡도 개선 및 ARXIV 데이터 세트에서 -1.ppl. 놀랍게도 제안된 방법은 ChapterBreakA03 접미사 식별 벤치마크에서 40.5%의 정확도라는 최첨단 성능을 달성하고 강력한 롱 컨텍스트 변환기와 313배 더 큰 매개변수를 사용하는 최신 LLM GPT-3를 모두 능가합니다. 이러한 데이터 세트에 대한 상당한 개선은 LONGMEM이 캐시된 메모리에서 과거의 롱 컨텍스트를 이해하여 미래 입력에 대한 언어 모델링을 잘 완료할 수 있음을 보여줍니다. 3.3 메모리 증강 인컨텍스트 학습 LLM은 로컬 컨텍스트에서 몇 가지 샷 데모 예제에서 비모수적으로 지식을 학습하여 인컨텍스트 학습(ICL)의 새로운 기능을 갖추고 있습니다. 그러나 기존의 인컨텍스트 학습은 입력 컨텍스트 길이에 의해 크게 제한되어 학습 세트에서 충분한 데모 예제에서 감독을 흡수하는 데 효과적이지 않습니다. 제안된 무제한 길이 메모리 증강을 통해 LONGMEM 방법은 로컬 컨텍스트에서 데모 예제 수의 제한을 극복하고 캐시된 메모리에 로드하여 전체 학습 세트에 참여할 수도 있습니다. 이런 방식으로 LONGMEM은 기존의 few-shot in-context 학습을 넘어서 수천 개의 보조 데모 예제를 통해 메모리 증강 in-context 학습을 실현했습니다.평가 설정.여기서 우리는 다섯 가지 자연어 이해(NLU) 데이터 세트인 SST-2 [SPW+13], MPQA [WWC05], MR [ABK+07], Subj [PL04] 및 SST-5 [SPW+13]에 대한 기준선과 제안된 LONGMEM 모델의 in-context 학습 기능을 평가합니다.우리는 두 가지 few-shot 설정인 4-shot 및 20-shot에서 모델을 평가합니다.4-shot 데모는 데이터가 부족한 시나리오인 반면, 20-shot 데모는 1k 입력 길이를 거의 충족하고 충분한 맥락적 자체 감독을 제공할 수 있습니다. 우리는 고정된 텍스트 템플릿을 통해 k-shot 예시를 의미적으로 의미 있는 데모 예시로 변환합니다. 즉, d¿=&quot;리뷰: x¿ 감정: y₁&quot;,\{(xi, yi)}=-1 Є 감정 분석 작업을 위한 Dtrain입니다. 또한, 우리는 개방형 생성 설정에서 SQUAD [RZLL16]의 질문-답변 작업에 대한 3-shot ICL을 평가합니다. 모든 프롬프트 템플릿의 세부 정보는 부록 D에 나와 있습니다. 그런 다음 줄 바꿈으로 데모 예시를 연결하여 구분합니다. 예측 레이블은 컨텍스트에서 데모 예시와 테스트 사례를 고려하여 탐욕적 디코딩을 사용하여 직접 생성됩니다. 예측 정확도는 평가 지표로 사용됩니다. 우리는 k-shot 데모 예시를 선택할 때의 무작위성을 극복하기 위해 서로 다른 난수 시드를 사용한 6회 실행의 평균과 표준 편차를 보고합니다. 앞서 설명한 대로 청크 크기는 검색된 텍스트 청크의 세분성을 제어합니다. 선택된 NLU 데이터 세트는 캐시된 메모리에서 세분화된 레이블을 검색해야 하므로 SST-2의 검증 세트에서 하이퍼 매개변수 선택을 수행하고 최상의 청크 크기 2를 사용하여 MemTRM 및 모델에 대한 결과를 보고합니다. 결과. 컨텍스트 내 학습에 대한 결과는 표 5와 표 4에 요약되어 있습니다. LONGMEM은 20샷 충분한 컨텍스트 내 설정에서 모든 NLU 작업에서 현저한 개선을 달성하여 사전 학습된 GPT-2* 및 MemTRM보다 평균 점수가 +8.0 증가했습니다. 한편, LONGMEM은 로컬 컨텍스트에서 4샷 데모 시나리오에서도 성능이 향상되었습니다. 또한 LONGMEM은 SQUAD에서 +4.5 EM 점수 증가로 개방형 생성 작업에서 LLM의 컨텍스트 내 학습 기능을 개선합니다. 컨텍스트 내 메모리 내 SST-MR 모델 #Demons. #Demons. ACC↑ ACC↑ 다수 해당 없음/해당 없음 50.50.50.GPT-2*해당 없음 68.311.MemTRM67.512.LONGMEM71.814.GPT-2*MemTRMLONGMEMN/A68.211.5 63.45.65.19.78.014.64.712.5 51.94.64.611.3 53.26.0 29.64.65.111.0 53.83.7 36.06.57.610.2 33.66.20.31.44.Subj SST-5 MPQA ACC↑ АССТ ACC↑ 50.평균 44.61.511.8 55.63.012.55.65.412.58.70.87.58.78.63.65.19.3 58.210.6 31.96.3 72.77.65.68.5 36.57.5 74.67.3 66.58.표 5: 5개 NLU 작업(SST-2, mr, subj, SST-5, mpqa)에 대한 4샷 및 20샷 ICL의 정확도[%]. 2000개의 추가 데모 예제를 샘플링하여 캐시된 메모리에 로드합니다. 아래 첨자는 6회 실행에 대한 표준 편차입니다. Avg.는 5개 데이터 세트에 대한 평균 정확도를 나타냅니다. 결과에 따르면 캐시된 메모리에 로드된 데모 예제는 맥락 내 학습에 도움이 되는 보조적 맥락적 데모로 간주될 수 있습니다. LONGMEM 모델은 더 나은 맥락 내 학습을 위해 로컬 맥락적 데모와 메모리 내 증강 데모에서 작업 관련 지식을 모두 수집할 수 있습니다. 3. 절제 연구 모델 GPT-2* MemTRM LONGMEM EM F22.282.3 30.782.22.843.5 32.652.26.772.3 35.702. 표 4: SQUAD에서 3-샷(약 1k 토큰) 맥락 내 학습의 정확한 일치(EM) 및 F1 점수. LONGMEM은 200개의 추가 데모 예제를 캐시된 메모리에 로드합니다. 지금까지 우리는 긴 맥락 모델링, 긴 맥락 이해 및 많은 샷 맥락 내 학습을 위해 캐시된 메모리를 활용하는 데 있어 LONGMEM의 효과와 우수성을 경험적으로 검증했습니다. 캐시된 메모리 뱅크의 설계에는 메모리 크기 msz 및 청크 크기 csz와 같은 많은 하이퍼파라미터가 포함되므로 이러한 하이퍼파라미터가 작업 성능에 미치는 영향을 평가하기 위해 일련의 절제 연구를 수행합니다.청크 크기의 효과.앞서 분석한 것처럼 청크 크기 csz는 검색의 세분성을 제어하므로 컨텍스트 내 학습과 같이 세분화된 검색이 필요한 작업에 차이를 만들 수 있습니다.컨텍스트 내 학습에 대한 다양한 청크 크기 csz Є {2,4,8}의 효과에 대한 절제 연구를 수행하고 그 결과를 그림 4(a)에 표시합니다.청크 크기 2는 5개의 NLU 데이터 세트에서 컨텍스트 내 학습 작업에서 가장 좋은 성능을 발휘하는데, 이는 세분화된 검색과 분류 레이블 토큰에 대한 융합이 필요한 NLU 작업의 속성과 일치합니다.메모리 크기의 효과.메모리 크기(msz)는 메모리 뱅크의 용량을 제어합니다. 일반적으로 메모리 크기는 문서 또는 컨텍스트의 평균 길이와 호환되어야 합니다.즉, 평균 16k 토큰이 있는 책 세트는 캐시된 메모리에 16k 토큰의 메모리 크기를 배포해야 합니다.65개 토큰의 학습 msz는 전체 접두사 컨텍스트 길이가 65k 토큰을 초과하지 않기 때문에 ChapterBreak와 같은 다운스트림 작업에 과도합니다.따라서 추론 단계에서 메모리 크기 msz = {8k, 16k, 32k, 65k}의 효과에 대한 절제 연구를 PG 언어 모델링 데이터 세트에 수행하고 그 결과를 그림 4(b)에 표시합니다.평균 8k-50k 길이의 책을 모델링하기 위해 대상 책의 평균 길이와 일치하는 더 작은 메모리 크기 16k가 가장 좋은 복잡도를 생성합니다.4 관련 작업 대규모 언어 모델. 대규모 언어 모델(예: GPT-2[RWC+19], GPT-3[BMR+20], OPT[ZRG+22], BLOOM[SFA+22])은 NLP 연구에 큰 혁명을 일으켰고 다양한 언어 이해, 언어 생성[WZG+22], 심지어 시각 언어 작업[WDC+22]의 최첨단 기술을 촉진했습니다. 또한, 모델 매개변수를 조정함으로써 LLM은 소수의 문맥 내 학습[BMR+20], 다단계 추론[WWS+22], 코드 완성 등과 같은 &quot;새로운 능력&quot;[WTB+22]을 보입니다.정확도78 77.78.675.72.71.874.672.65.59.8 61.34.436.534.SST-MR 과목 sst-MPQA ■csz=■csz=csz=APPL 0.10.0-0.15-10K 10-100K 0.1-0.5M PG 분할-msz=65k msz=32k msz=16k msz=8k 0.5-1M (a) (b) 그림 4: (a) 추론 중에 다른 청크 크기가 주어진 5개 NLU 데이터 세트에 대한 정확도; (b) 추론 중 메모리 크기가 다른 PG-22의 4개 분할에 대한 APerplexity. msz=65k일 때의 perplexity가 기준선으로 사용됩니다. x-formers. 변환기가 더 긴 컨텍스트에 주의를 기울일 수 있도록 하기 위해 &quot;x-formers&quot;의 많은 변형이 제안됩니다. Transformer-XL [DYY+19]은 과거 세그먼트의 어텐션 키와 값을 캐시하고 이를 반복적으로 재사용하는 것을 제안합니다. LinFormer [WLK+20], LongFormer [BPC20], Routing Transformer [RSVG21]를 포함한 x-formers의 최근 선구적 작업은 O(n²) 복잡도를 O(n log n) 또는 O(n)으로 줄이기 위한 다양한 희소 어텐션 메커니즘을 제안했습니다. BigBird [ZGD+20]는 컨텍스트 토큰의 하위 집합에 주의를 기울여 4k 시퀀스 길이를 달성합니다. 이러한 x-formers는 상당한 효율성 개선을 달성하지만 책 수준 길이에 걸친 시퀀스를 모델링할 때 이러한 효율성 이득은 눈에 띄지 않습니다. 게다가, 이러한 방법의 가장 큰 시퀀스 길이는 여전히 16k 토큰으로 상한이 정해져 있어 책이나 위키피디아 페이지 수준에서 긴 시퀀스를 모델링하는 데는 유효하지 않습니다(즉, PG19 데이터 세트의 전체 길이 책에 대한 평균 70k 토큰[RPJL19]). 사이드 튜닝. 사이드 튜닝 방법[ZSZ+20, SCB22]은 합산을 통해 고정된 사전 학습된 네트워크와 융합된 가벼운 사이드 네트워크를 학습하여 사전 학습된 모델을 위한 작업별 튜닝 방법입니다. 이 방법은 사이드 네트워크를 채택한다는 아이디어를 계승했지만 학습 목표와 교차 네트워크 융합 방식 측면에서 사이드 튜닝 방법을 구별합니다. LONGMEM은 작업별 튜닝을 포함하지 않는, 오래 전 입력을 기억하기 위해 분리된 메모리로 LLM을 증강하는 것을 제안합니다. LONGMEM이 제안한 교차 네트워크 잔여 연결은 참신하며 사이드 튜닝의 바닐라 합산과 다릅니다. 5
--- CONCLUSION ---
이 논문에서 우리는 LLM에 장기 메모리를 증강하여 장문 맥락을 기억하고 장문 기억을 얻을 수 있도록 하는 것을 제안합니다. 설계된 분리된 메모리 모듈은 과거 입력의 주의 키와 값 쌍을 미래의 검색 및 융합을 위해 캐시할 수 있습니다. 분리된 잔여 SideNet이 메모리 검색기 및 판독기로 도입되는 반면, LLM 자체는 동결되어 지식 및 메모리 인코더로 작동합니다. 다양한 장문맥 언어 모델링 데이터 세트에 대한 실험은 다른 메모리 증강 기준선에 비해 우리 모델의 효과를 보여줍니다. 제안된 방법은 또한 수천 개의 보조 데모 예제를 메모리에 캐싱하여 문맥 길이에 의해 제약되는 문맥 내 데모 예제의 제한된 수를 극복하기 위해 LLM의 문맥 내 학습을 가능하게 할 수 있습니다. 참고문헌 [ABK+07] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives. Dbpedia: 오픈 데이터 웹의 핵심. 의미 웹, 722-735페이지. Springer, 2007. [BMH+21] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, TW Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, L. Sifre. 수조 개의 토큰을 검색하여 언어 모델 개선. ArXiv, abs/2112.04426, 2021. [BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, TJ Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습자입니다. ArXiv, abs/2005.14165, 2020. [BPC20] Iz Beltagy, Matthew E Peters, Arman Cohan. Longformer: 긴 문서 변환기. arXiv 사전 인쇄본 arXiv:2004.05150, 2020. [DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. NAACL, 2019. [DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov. Transformer-xl: 고정 길이 컨텍스트를 넘어서는 주의 깊은 언어 모델. arXiv 사전 인쇄본 arXiv:1901.02860, 2019. [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트. arXiv 사전 인쇄본 arXiv:2101.00027, 2020. [JDJ21] Jeff Johnson, Matthijs Douze, and Hervé Jégou. GPU를 사용한 10억 규모 유사도 검색. IEEE Transactions on Big Data, 7:535-547, 2021. [KB15] Diederik P. Kingma and Jimmy Ba. Adam: 확률적 최적화 방법. CORR, abs/1412.6980, 2015. [LGW+23] Rui Lv, Junliang Guo, Rui Wang, Xu Tan, Qi Liu, Tao Qin. N-gram 최근접 이웃 기계 번역. arXiv 사전 인쇄본 arXiv:2301.12866, 2023. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. ROBERTa: 강력하게 최적화된 bert 사전 학습 접근법. ArXiv, abs/1907.11692, 2019. [PL04] Bo Pang과 Lillian Lee. 감성 교육: 최소 컷을 기반으로 한 주관적 요약을 사용한 감성 분석. arXiv 사전 인쇄본 cs/0409058, 2004. [PSL21] Ofir Press, Noah A Smith, Mike Lewis. 짧은 학습, 긴 테스트: 선형 편향이 있는 주의는 입력 길이 외삽을 가능하게 함. arXiv 사전 인쇄본 arXiv:2108.12409, 2021. [RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 비지도 학습을 통한 언어 이해 향상. 2018. [RPJL19] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Timothy P Lillicrap. 장거리 시퀀스 모델링을 위한 압축 변압기. arXiv 사전 인쇄본 arXiv:1911.05507, 2019. [RSR+20] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색. ArXiv, abs/1910.10683, 2020. [RSVG21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier. 라우팅 변환기를 사용한 효율적인 콘텐츠 기반 희소 주의. Association for Computational Linguistics의 거래, 9:53-68, 2021. [RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 언어 모델은 비지도 멀티태스크 학습자입니다. 2019. [RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev 및 Percy Liang. SQUAD: 텍스트의 기계 이해를 위한 100,000개 이상의 질문. arXiv 전자 인쇄본, 페이지 arXiv:1606.05250, 2016. [SCB22] Yi-Lin Sung, Jaemin Cho 및 Mohit Bansal. Lst: 매개변수 및 메모리 효율적 전이 학습을 위한 사다리 측면 튜닝. arXiv 사전 인쇄본 arXiv:2206.06522, 2022. [SFA+22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: 176bparameter 오픈 액세스 다국어 언어 모델. arXiv 사전 인쇄본 arXiv:2211.05100, 2022. [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 감정 트리뱅크에 대한 의미적 구성성을 위한 재귀적 심층 모델. 2013년 자연어 처리의 경험적 방법에 관한 컨퍼런스의 회의록, 1631-1642페이지, 2013.[STI22] Simeng Sun, Katherine Thai, Mohit Iyyer. 장 구분: 장거리 언어 모델을 위한 챌린지 데이터 세트. arXiv 사전 인쇄본 arXiv:2204.10878, 2022. [VSP+17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. NIPS, 2017. [WDC+22] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei. 시각적으로 증강된 언어 모델링. arXiv 사전 인쇄 arXiv:2205.10178, 2022. [WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang 및 Hao Ma. Linformer: 선형적 복잡성을 지닌 Self-attention. arXiv 사전 인쇄 arXiv:2006.04768, 2020. [WRHS22] Yuhuai Wu, Markus N. Rabe, DeLesley S. Hutchins 및 Christian Szegedy. 변압기를 기억합니다. ArXiv, abs/2203.08913, 2022. [WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler 등 대규모 언어 모델의 새로운 능력. arXiv 사전 인쇄본 arXiv:2206.07682, 2022. [WWC05] Janyce Wiebe, Theresa Wilson, Claire Cardie. 언어에서 의견과 감정 표현에 대한 주석 달기. 언어 리소스 및 평가, 39(2):165-210, 2005. [WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou. 사고의 사슬을 촉진하는 것은 대규모 언어 모델에서 추론을 이끌어냅니다. arXiv 사전 인쇄본 arXiv:2201.11903, 2022. [WZG+22] Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, Weihua Luo. 자연어 생성으로서의 과제 지향 대화 시스템. 2022년 제45회 국제 ACM SIGIR 정보 검색 연구 개발 컨퍼런스 논문집, 2698-2703쪽. [YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, Quoc V. Le. XLNet: 언어 이해를 위한 일반화된 자기 회귀 사전 학습. NeurIPS, 2019. [ZGD+20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: 더 긴 시퀀스를 위한 변압기. 신경 정보 처리 시스템의 발전, 33:17283–17297, 2020. [ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: 개방형 사전 학습된 변환기 언어 모델. arXiv 사전 인쇄본 arXiv:2205.01068, 2022. [ZSZ+20] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra Malik. 사이드 튜닝: 가산적 사이드 네트워크를 통한 네트워크 적응을 위한 기준선. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 3부 16, 698-714쪽. Springer, 2020.A 추론 효율성 및 GPU 메모리 효율성 모델이 긴 시퀀스를 이해해야 할 때 제안된 방법인 LONGMEM은 경계 밖 입력을 이전 컨텍스트로 캐시된 메모리에 로드할 수 있습니다.따라서 바닐라 셀프 어텐션 기반 모델에 비해 메모리 사용량과 추론 속도를 크게 개선할 수 있습니다.효율성 측면에서 자세한 통계는 표 6에 나와 있습니다.추론 속도 GPU 메모리 사용량 컨텍스트 내 메모리 내 모델 길이 길이(토큰/초)↑ GPT-2* 4k N/ALONGMEM 1k 3kGPT-2* 8k N/ALONGMEM 1k 7k(MBS)↓표 6: 추론 속도 및 GPU 메모리 활용 측면에서 완전 밀도 셀프 어텐션(GPT-2*)보다 우리 방법이 우수한 점. B 학습 세부 정보 재생산된 GPT-2*의 사전 학습은 총 117B 토큰에서 반복되며, 배치 크기는 512이고 세그먼트 길이는 고정된 토큰은 1024입니다. 메모리 증강 적응 학습에는 Adam 옵티마이저[KB15]가 채택되었습니다. 사전 학습 및 적응은 16개의 32GB-Tesla-V100 GPU에서 학습되었습니다. 기타 자세한 학습 하이퍼 매개변수 및 설정은 표 7에 나와 있습니다. 하이퍼 매개변수 LONGMEM 재생산된 GPT-2* 백본 LLM 하이퍼 매개변수 매개변수 정밀도 407M floatLayersHidden dim.Attention headsHead DimVocab 크기 52k 시퀀스 길이Position emb.Alibi Tied embedding False SideNet 하이퍼 매개변수 매개변수 정밀도 레이어 Hidden dim. 주의 헤드 헤드 Dim 시퀀스 길이 151M float 메모리 증강 적응 하이퍼 매개변수 글로벌 배치 크기 학습 속도 총 토큰 워밍업 토큰 LR 감소 스타일 Adam(B1, B2) Adam eps 가중치 감소 그래디언트 클리핑 2.0e-26B 다항식(0.9, 0.98) 1e-0.2.표 7: 메모리 증강 적응 및 아키텍처 하이퍼 매개변수.C 프롬프트 템플릿 5개 NLU 데이터 세트와 Squad QA 데이터 세트에 대한 모든 수작업으로 만든 컨텍스트 내 학습 프롬프트 템플릿과 레이블을 표 8에 제시합니다.작업 SST 프롬프트 검토: [문장] 감정: [레이블] MR MPQA SST-| 리뷰: [문장] 감정: [레이블] 리뷰: [문장] 감정: [레이블] 입력: [문장] 유형: [레이블] 제목 입력: [문장] 유형: [레이블] Squad | 구절: [구절]\n 질문: [질문] 답변: [대답] 레이블 {긍정적, 부정적} {긍정적, 부정적} {긍정적, 부정적} {끔찍함, 나쁨, 괜찮음, 좋음, 훌륭함} {객관적, 주관적} 표 8: 5개 NLU 데이터 세트와 1개 질의응답 데이터 세트 Squad의 제로샷 평가에 대한 모델 예측을 쿼리하는 데 사용된 수작업 프롬프트.
