--- ABSTRACT ---
최근 몇 년 동안 이미지 생성은 성능 면에서 큰 도약을 보였으며, 여기서 확산 모델이 중심적인 역할을 합니다. 고품질 이미지를 생성하지만 이러한 모델은 주로 텍스트 설명에 따라 달라집니다. 이는 다음과 같은 의문을 제기합니다. 이러한 모델을 다른 모달리티에 따라 조건화하도록 어떻게 채택할 수 있을까요? 이 논문에서는 텍스트-이미지 생성을 위해 훈련된 잠재 확산 모델을 활용하여 오디오 녹음에 따라 조건화된 이미지를 생성하는 새로운 방법을 제안합니다. 사전 훈련된 오디오 인코딩 모델을 사용하여 제안된 방법은 오디오를 새 토큰으로 인코딩하는데, 이는 오디오와 텍스트 표현 사이의 적응 계층으로 간주될 수 있습니다. 이러한 모델링 패러다임에는 적은 수의 훈련 가능한 매개변수가 필요하므로 제안된 접근 방식은 가벼운 최적화에 매력적입니다. 결과에 따르면 제안된 방법은 객관적이고 주관적인 지표를 고려할 때 평가된 기준 방법보다 우수합니다. 코드와 샘플은 https://pages.cs.huji.ac.il/ adiyoss-lab/AudioToken에서 제공됩니다. 색인 용어: 확산 모델, 오디오-이미지. 1.
--- INTRODUCTION ---
신경 생성 모델은 디지털 콘텐츠를 소비하는 방식을 바꾸어 놓았습니다. 고품질 이미지 생성[1, 2, 3]부터 긴 텍스트의 일관성[4, 5, 6], 음성 및 오디오[7, 8, 9, 10]까지 다양합니다. 최근 몇 년 동안 확산 기반 생성 모델이 선호되는 접근 방식으로 등장하여 다양한 작업에서 유망한 결과를 보여주었습니다[11]. 확산 프로세스 동안 모델은 사전 정의된 노이즈 분포를 대상 데이터 분포에 매핑하는 방법을 학습합니다. 확산 프로세스의 모든 단계에서 모델은 주어진 단계에서 노이즈를 예측하는 방법을 학습하여 최종적으로 대상 분포에서 신호를 생성합니다[12, 13, 14]. 확산 모델은 원시 입력[15, 12], 잠재적 표현[16] 등 다양한 형태의 데이터 표현에서 작동합니다. 제어 가능한 생성 모델을 고려할 때 요즘 일반적인 관행은 입력 데이터의 텍스트 설명에 따라 생성을 조건 지정하는 것입니다. 이것은 특히 이미지 생성에서 두드러진다[1, 17, 18]. 최근, 이미지-오디오[19, 20], 이미지-음성[21, 22], 이미지-텍스트[23, 24] 또는 오디오-오디오[25, 26]와 같이 생성 프로세스를 조건화하기 위해 다양한 모달리티를 사용하는 여러 방법이 제안되었다. 그러나 이러한 연구 방향은 커뮤니티에서 덜 탐구되었다. 이 연구에서는 오디오-이미지 생성 작업에 초점을 맞춘다. 오디오 샘플에 임의의 소리가 포함되어 있다고 가정하면 음향 장면을 나타내는 고품질 이미지를 생성하는 것을 목표로 한다. 사전 훈련된 텍스트-이미지 *Equal Contribution을 활용하는 것을 제안한다. 그림 1: 제안된 방법에서 생성된 이미지(오른쪽)와 입력 스펙트로그램(왼쪽). 이 모델은 오디오 녹음을 입력으로 받고, 표현을 추출하고, 텍스트 잠재 공간에 투사하여 사전 훈련된 텍스트 조건 확산 생성 모델에 공급한다. 사전 훈련된 오디오 표현 모델과 함께 생성 모델을 사용하여 출력과 입력 간의 적응 계층 매핑을 학습합니다. 특히, 텍스트 반전에 대한 최근 연구[27]에서 영감을 얻어 오디오 표현을 임베딩 벡터로 매핑하는 전용 오디오 토큰을 학습하는 것을 제안합니다. 그런 다음 이러한 벡터는 새로운 단어 임베딩을 반영하는 연속 표현으로 네트워크로 전달됩니다. 이미지 입력에서 오디오를 생성하는 여러 가지 방법이 이전 작업에서 제안되었습니다. [28, 29]의 저자는 생성적 적대 신경망(GAN) 기반 방법을 사용하여 오디오 녹음을 기반으로 이미지를 생성하는 것을 제안했습니다. 제안된 방법과 달리 [28]에서 저자는 MNIST 숫자만 생성하는 결과를 제시하고 일반 오디오 사운드로 일반화하지 않았습니다. [29]에서 저자는 일반 오디오에서 이미지를 생성했습니다. 그러나 이는 품질이 낮은 이미지로 바뀌었습니다. 가장 관련성이 높은
--- RELATED WORK ---
우리에게는 Wav2Clip[30]이 있는데, 여기서 저자는 먼저 오디오-이미지 쌍에 대한 모델과 같은 대조 언어-이미지 사전 훈련(CLIP)[31]을 학습합니다. 그런 다음 나중에 이러한 표현을 사용하여 VQ-GAN CLIP[33] 프레임워크에서 VQGAN[32]을 사용하여 이미지를 생성할 수 있습니다. 텍스트가 아닌 오디오 신호를 이미지 생성의 조건으로 사용하는 이유는 무엇입니까? 텍스트 기반 생성 모델은 인상적인 이미지를 생성할 수 있지만 텍스트 설명은 이미지와 자연스럽게 쌍을 이루지 않습니다. 즉, 텍스트 설명은 종종 수동으로 추가됩니다. 반면, 비디오, 오디오, 이미지가 동일한 장면을 캡처하고 표현하는 것을 고려할 때, e(&#39;a&#39;) e(&#39;photo&#39;) &quot;A photo of a&quot; Text-Tokenizer e(&#39;of&#39;) Text-Encoder e(&#39;a&#39;) eaudio Embedder 사전 훈련된 오디오 인코더 Atten-Pooling (a) MLP 생성 모델 그림 2: 아키텍처 개요: 사전 훈련된 오디오 인코더를 통해 오디오 녹음을 전달한 다음 Embedder 네트워크를 통해 전달합니다. 사전 훈련된 텍스트 인코더는 토크나이저와 오디오 토큰이 만든 토큰을 추출합니다. 마지막으로 생성 모델에 표현의 연결된 텐서가 공급됩니다. 이 프로세스 동안 Embedder 매개변수만 훈련된다는 점에 유의하는 것이 중요합니다. 따라서 자연스럽게 쌍을 이룹니다. 또한 오디오 신호는 동일한 악기의 다른 유형(예: 클래식 기타, 일렉트릭 기타 등) 또는 동일한 객체의 다른 장면(예: 스튜디오에서 녹음된 클래식 기타 대 라이브 쇼)과 같은 복잡한 장면과 객체를 나타낼 수 있습니다. 다양한 객체의 이렇게 세밀한 세부 사항에 주석을 달기란 노동 집약적이어서 확장하기 어렵습니다. 요약하자면, 우리의 기여는 다음과 같습니다. 우리는 새로운 것을 제안합니다.
--- METHOD ---
영어: 텍스트-이미지 생성을 위해 훈련된 잠재 확산 모델을 활용하여 오디오 녹음에 따라 이미지를 생성합니다. 사전 훈련된 오디오 인코딩 모델을 사용하여 제안된 방법은 오디오를 새 토큰으로 인코딩하며, 이는 오디오와 텍스트 표현 사이의 적응 계층으로 간주될 수 있습니다. 이러한 모델링 패러다임에는 소수의 훈련 가능한 매개변수가 필요하므로 제안된 접근 방식은 가벼운 최적화에 매력적입니다. 결과에 따르면 제안된 방법은 객관적이고 주관적인 지표를 고려할 때 평가된 기준 방법보다 우수합니다. 코드와 샘플은 https://pages.cs.huji.ac.il/ adiyoss-lab/AudioToken에서 제공됩니다. 색인 용어: 확산 모델, 오디오-이미지. 1. 서론 신경 생성 모델은 디지털 콘텐츠를 소비하는 방식을 바꾸어 놓았습니다. 고품질 이미지 생성[1, 2, 3]부터 긴 텍스트의 일관성[4, 5, 6], 음성 및 오디오[7, 8, 9, 10]까지 다양합니다. 최근 몇 년 동안 확산 기반 생성 모델이 선호되는 접근 방식으로 등장하여 다양한 작업에서 유망한 결과를 보여주었습니다[11]. 확산 프로세스 동안 모델은 사전 정의된 노이즈 분포를 대상 데이터 분포에 매핑하는 방법을 학습합니다. 확산 프로세스의 모든 단계에서 모델은 주어진 단계에서 노이즈를 예측하는 방법을 학습하여 최종적으로 대상 분포에서 신호를 생성합니다[12, 13, 14]. 확산 모델은 원시 입력[15, 12], 잠재 표현[16] 등 다양한 형태의 데이터 표현에서 작동합니다. 제어 가능한 생성 모델을 고려할 때 요즘 일반적인 관행은 입력 데이터의 텍스트 설명에 따라 생성을 조건으로 지정하는 것입니다. 이는 특히 이미지 생성에서 두드러집니다[1, 17, 18]. 최근, 이미지-오디오[19, 20], 이미지-음성[21, 22], 이미지-텍스트[23, 24] 또는 오디오-오디오[25, 26]와 같이 생성 프로세스를 조건화하는 다양한 모달리티를 사용하는 여러 방법이 제안되었습니다. 그러나 이러한 연구 방향은 커뮤니티에서 덜 탐구되었습니다. 이 연구에서는 오디오-이미지 생성 작업에 중점을 둡니다. 오디오 샘플에 임의의 소리가 포함되어 있는 경우 음향 장면을 나타내는 고품질 이미지를 생성하는 것을 목표로 합니다. 사전 훈련된 텍스트-이미지*Equal Contribution을 활용하는 것을 제안합니다. 그림 1: 제안된 방법에서 생성된 이미지(오른쪽)와 입력 스펙트로그램(왼쪽). 모델은 오디오 녹음을 입력으로 받고 표현을 추출하여 텍스트 잠재 공간에 투사하여 사전 훈련된 텍스트 조건 확산 생성 모델에 공급합니다. 사전 훈련된 오디오 표현 모델과 함께 생성 모델을 사용하여 출력과 입력 간의 적응 계층 매핑을 학습합니다. 특히, 텍스트 역전[27]에 대한 최근 연구에서 영감을 얻어 오디오 표현을 임베딩 벡터로 매핑하는 전용 오디오 토큰을 학습하는 것을 제안합니다. 그런 다음 이러한 벡터는 새로운 단어 임베딩을 반영하는 연속 표현으로 네트워크에 전달됩니다. 이미지 입력에서 오디오를 생성하는 여러 가지 방법이 이전 작업에서 제안되었습니다. [28, 29]의 저자는 생성적 적대 신경망(GAN) 기반 방법을 사용하여 오디오 녹음을 기반으로 이미지를 생성하는 것을 제안했습니다. 제안된 방법과 달리 [28]에서 저자는 MNIST 숫자만 생성하는 결과를 제시하고 일반 오디오 사운드로 일반화하지 않았습니다. [29]에서 저자는 일반 오디오에서 이미지를 생성했습니다. 그러나 이는 낮은 품질의 이미지로 바뀌었습니다. 우리와 가장 관련성이 높은 관련 작업은 Wav2Clip[30]으로, 저자는 먼저 오디오-이미지 쌍에 대한 모델과 같은 대조적 언어-이미지 사전 학습(CLIP)[31]을 학습합니다. 그런 다음 나중에 이러한 표현을 사용하여 VQ-GAN CLIP [33] 프레임워크에서 VQGAN [32]을 사용하여 이미지를 생성할 수 있습니다.텍스트가 아닌 오디오 신호를 이미지 생성의 조건으로 사용하는 이유는 무엇입니까?텍스트 기반 생성 모델은 인상적인 이미지를 생성할 수 있지만 텍스트 설명은 이미지와 자연스럽게 짝을 이루지 않습니다.즉, 텍스트 설명은 종종 수동으로 추가됩니다.반면에 비디오, 오디오 및 이미지가 동일한 장면을 캡처하고 표현하는 것을 고려할 때, e(&#39;a&#39;) e(&#39;photo&#39;) &quot;A photo of a&quot; Text-Tokenizer e(&#39;of&#39;) Text-Encoder e(&#39;a&#39;) eaudio Embedder 사전 학습된 오디오 인코더 Atten-Pooling (a) MLP 생성 모델 그림 2: 아키텍처 개요: 사전 학습된 오디오 인코더를 통해 오디오 녹음을 전달한 다음 Embedder 네트워크를 통해 전달합니다.사전 학습된 텍스트 인코더는 토크나이저와 오디오 토큰이 만든 토큰을 추출합니다. 마지막으로, 생성 모델은 표현의 연결된 텐서로 공급됩니다. 이 프로세스 동안 Embedder 매개변수만 학습된다는 점에 유의하는 것이 중요합니다. 따라서 자연스럽게 쌍을 이룹니다. 게다가 오디오 신호는 동일한 악기의 다른 유형(예: 클래식 기타, 일렉트릭 기타 등) 또는 동일한 객체의 다른 장면(예: 스튜디오에서 녹음된 클래식 기타 대 라이브 쇼)과 같은 복잡한 장면과 객체를 나타낼 수 있습니다. 다양한 객체의 이러한 세밀한 세부 정보에 주석을 달는 것은 노동 집약적이므로 확장하기 어렵습니다. 요약하면, 우리의 기여는 다음과 같습니다. 사전 학습된 텍스트-이미지 확산 모델과 사전 학습된 오디오 인코더를 함께 활용하여 오디오-이미지 생성을 위한 새로운 방법인 AUDIOTOKEN을 제안합니다. 오디오-이미지 생성 작업에 특별히 전념하는 일련의 평가 메트릭을 제안합니다. 광범위한
--- EXPERIMENT ---
s, 우리는 우리의 방법이 오디오 장면을 기반으로 고품질의 다양한 이미지 세트를 생성할 수 있음을 보여준다.2. 텍스트 조건 모델의 적응 확산 모델은 데이터 분포 p(x)의 기본 확률 모델을 학습하는 경향이 있는 모델 패밀리이다.이것은 길이 T의 역 마르코프 프로세스를 학습하여 이루어진다.타임스탬프 t = [0,1]이 주어지면, 잡음 제거 함수 eŋ : Rd Rd는 훈련 분포 S = {x1,.., xm}에서 섭동된 xt의 깨끗한 버전을 예측하는 법을 학습한다: LDM Ex~S,t~U (0,1),e~N(0,1) [||€ — €0(xt, t)||¾½] . (1) 경험적 결과에 따르면 자동 인코더의 잠재 공간 위에 확산 모델을 학습하면 원시 입력에서 훈련된 결과보다 더 높은 품질의 결과를 생성할 수 있다[16]. 직관적으로, 이 프로세스는 인코더-디코더 아키텍처의 잠재 표현에서 수행될 수 있다.잠재 확산은 인코더 f가 제공하는 표현 위에서 작동한다: LLDM Ex~S,t~U (0,1),€~N(0,1) [||€ – €o(f(xt), t)||¾½]. (2) 확산의 출력은 나중에 디코더를 통해 전달되어 원시 결과(예: 오디오, 이미지, 텍스트)를 얻을 수 있다.최신 생성 모델의 중요한 구성 요소는 컨디셔닝이다.이를 통해 생성 프로세스를 주어진 입력, 즉 y가 데이터 입력인 p(x|y) 모델링에 조건화할 수 있다.예를 들어, 텍스트 기반 시각적 생성에서 생성 프로세스는 텍스트에 조건화된다.텍스트, 시간, 스타일 등 많은 유형의 컨디셔닝이 있다.[16].일반적으로 컨디셔닝 구성 요소는 인코더 7에서 ee의 어텐션 메커니즘으로 조건 표현을 주입하여 수행된다. 확산 과정을 조건화하면 다음과 같은 확산 과정이 생성됩니다. LCLDM E(x,y)~S,t~U(0,1),e~N(0,1) [||€ – €0(f(x+), t, T(y))||]. (3) 다음에서 오디오 장면을 기반으로 하는 고품질의 다양한 이미지를 생성하기 위해 조건 생성 모델을 활용하는 방법을 제안합니다. 2.1. AUDIOTOKEN 오디오 신호에는 오디오 신호를 생성한 장면을 상상하는 데 도움이 되는 정보가 포함되어 있습니다. 이로 인해 오디오 녹음을 조건으로 하는 생성 모델을 사용하여 장면을 생성하고 싶어집니다. 그러나 고품질 이미지를 생성하는 모델은 일반적으로 대규모 텍스트-이미지 쌍에 의존하여 텍스트를 사용하여 이미지를 생성합니다. 따라서 오디오 신호를 텍스트 공간으로 효과적으로 투사하는 AUDIOTOKEN이라는 방법을 제안하여 기존의 텍스트 조건 모델을 활용하여 오디오 기반 토큰을 기반으로 이미지를 생성할 수 있습니다. 우리의 목표는 모든 오디오 신호를 텍스트 컨디셔닝을 위한 추가 토큰으로 적합한 전용 표현으로 직접 인코딩하는 것의 타당성을 조사하는 것입니다. 그렇게 함으로써 기존의 사전 훈련된 모델을 활용할 수 있으며 오디오-비주얼 쌍을 사용하여 새로운 생성 모델을 학습하지 않아도 됩니다. 더욱이, 우리는 각 오디오 클래스나 장면 유형에 대해 새로운 토큰을 학습할 필요가 없습니다(텍스트 반전 기반 방법[27]과 대조적으로). 대신, 우리는 광범위한 다양한 개념을 처리할 수 있는 오디오-이미지 생성기를 개발합니다. 우리 방법에 대한 입력은 짧은 비디오 입력(i, a)으로, 여기서 i는 비디오의 프레임을 나타내고 a는 해당 오디오 녹음을 나타냅니다. 우리는 오디오 컨디셔닝된 생성 프로세스, 즉 p(i|a)를 만드는 것을 목표로 합니다. 이를 달성하기 위해, 우리는 텍스트 컨디셔닝된 생성 모델을 활용합니다. 따라서 우리는 오디오 신호 a를 텍스트 컨디셔닝과 연관시켜야 합니다. 이 프로세스는 초기 프롬프트 &quot;A photo of a&quot;를 표현 etext Є R4xda로 인코딩하는 변환기 모델로 시작합니다. 여기서 da는 텍스트 입력의 임베딩 차원입니다. 그런 다음 오디오 신호의 추가 잠재 표현인 Caudio Є Rda를 etext에 연결합니다. 사전 훈련된 오디오 인코딩 네트워크와 작은 투영 네트워크로 구성된 Embedder를 활용합니다. 그 결과: Caudio = Embedder(a). (4) 다음으로 Embedder 네트워크와 방법의 최적화 프로세스를 설명합니다. FANALS FAY A Benlo INT BESTUNN 그림 3: Wav2Clip(첫 번째 행), ImageBind(두 번째 행), AUDIOTOKEN(세 번째 행) 및 원래 참조 이미지(마지막 행)에 대한 정성적 결과. 오디오 인코딩: Embedder는 사전 훈련된 오디오 분류 네트워크를 활용하여 오디오를 표현합니다. 판별 네트워크의 마지막 층은 일반적으로 분류에 사용되므로 판별 작업과 관련 없는 중요한 오디오 정보를 감소시키는 경향이 있습니다.따라서 우리는 이전 층과 마지막 숨겨진 층을 연결합니다(특히 총 12개 층 중 4번째, 8번째, 12번째 층을 선택합니다).이로 인해 오디오의 시간 임베딩(a) Є Rdxna가 생성됩니다.여기서 na는 시간 오디오 차원입니다.그런 다음 텍스트 임베딩 공간으로의 투영을 학습하기 위해 GELU 함수를 사이에 둔 두 개의 선형 층(a)을 전달합니다.Caudio = W20 (W10(a)), 여기서 W₁ = Rdxd, W₁ Є Rdx daudio, σ는 GELU 비선형성입니다[34]. 마지막으로, 오디오 신호의 시간적 차원을 줄이는 주의 풀링 계층[35]을 적용합니다.즉, Caudio = Atten-Pooling(audio). (6) 최적화: 학습하는 동안 최적화 프로세스 동안 Embedder 네트워크 내의 선형 및 주의 풀링 계층의 가중치만 업데이트합니다. 사전 학습된 오디오 네트워크와 생성 네트워크는 고정된 상태로 유지됩니다. 원래 모델 CLDM(식 2)에서 사용한 손실 함수를 채택하여 학습 체계의 일관성을 유지합니다. 또한 원래의 손실 함수를 보완하는 추가 손실 함수를 도입하는데, 여기에는 lЄ Rnda로 표시되는 비디오 레이블을 인코딩하는 것이 포함되며, 여기서 nɩ는 레이블의 길이를 나타냅니다(예: &#39;어쿠스틱 기타&#39; 레이블의 크기는 2입니다). 레이블은 생성 모델의 텍스트 인코더를 사용하여 인코딩된 다음 평균 poolAvg-Pooling(1)을 사용하여 공간 차원이 줄어듭니다. 분류 손실은 다음과 같이 정의됩니다. = LCL = (1. (Caudio, 1) || Caudio ||||||(7) 직관적으로 이 항은 오디오 임베딩이 비디오 개념에 가깝게 유지되도록 보장하여 더 빠르고 안정적인 수렴을 용이하게 합니다. 마지막으로 인코딩된 오디오 토큰에 ₁ 정규화를 추가하여 오디오 토큰이 더 균등하게 분산되도록 합니다. AUDIOTOKEN에 최적화된 전체 손실은 L=LLDM + 1 || eaudio || 1로 주어집니다. (8) 분류 손실이 있는 AUDIOTOKEN에 최적화된 전체 손실은 L = LLDM e1 || Caudio || 1 + ACLLCL로 주어집니다. 2.2. 평가 함수 오디오 장면에서 시각적 생성을 평가하는 것은 아직 열려 있습니다. 이러한 평가 설정은 잘 수행된 모델이 (i) 오디오 녹음에서 가장 눈에 띄는 개체를 캡처하는 이미지를 생성하도록 예외가 있기 때문에 어렵습니다. (ii) 입력 오디오와 의미적으로 상관관계가 있어야 하며(iii) &quot;기준 진실&quot;/대상 이미지와 의미적으로 유사해야 합니다. 마지막으로 생성된 이미지의 일반적인 품질을 평가해야 합니다. 이를 완화하기 위해 다음 평가 함수를 사용할 것을 제안합니다. 오디오-이미지 유사도(AIS)는 이상적으로 의미 입력 오디오와 생성된 이미지 기능 간의 유사도를 측정합니다. Wav2CLIP 모델[30]을 사용합니다. Wav2CLIP 모델을 사용하면 오디오와 이미지 쌍의 표현 간의 유사도를 측정할 수 있습니다. 이를 통해 생성된 이미지가 오디오를 어느 정도 설명하는지 정량화할 수 있습니다. 상관 관계 점수만 정량화하면 점수 척도가 다를 수 있으므로 전체 상황을 알 수 없습니다. 따라서 좋은 점수가 무엇인지 불분명합니다. 대신 생성된 이미지와 입력 오디오 간의 유사도와 생성된 이미지와 데이터의 임의 오디오 간의 유사도를 비교합니다. 그런 다음 AIS 점수를 검증 세트의 모든 데이터 항목에 대해 평균화합니다. 이미지-이미지 유사도(IIS)는 생성된 이미지와 &quot;실제&quot; 이미지 간의 의미적 유사도를 측정합니다. 이 정보는 &quot;실제&quot; 장면과의 의미적 유사도를 정량화할 수 있기 때문에 중요합니다. AIS 메트릭과 동일한 참조 기반 방법을 사용합니다. 따라서 (i) 생성된 이미지와 &quot;실제&quot; 및 (ii) 생성된 이미지와 데이터의 임의 이미지 간의 CLIP [31] 점수를 측정합니다. 그런 다음 IIS 점수를 검증 세트의 모든 데이터 항목에 대해 평균화합니다. 오디오-이미지 콘텐츠(AIC). 이미지 콘텐츠를 설명하기 위해 이미지 분류기의 예측 클래스와 실제 오디오 레이블 간의 일치 수준을 측정합니다. 그러나 이미지 분류기 클래스와 오디오 레이블 간에 완전한 상관 관계가 없을 수 있으므로 추가 CLIP 기반 점수를 사용하여 일치 여부를 확인합니다. CLIP 기반 일치 점수가 임계값 0.75를 초과하면 이미지와 오디오 클래스가 일치하는 것으로 간주됩니다. 프레셰 인셉션 거리(FID). 생성된 이미지의 품질을 평가하기 위해 표준 FID 점수[36]를 채택했습니다. 이러한 참조 없는 메트릭은 표 1을 사용하여 생성된 이미지의 분포를 원본 이미지와 비교합니다. Wav2Clip과 함께 AUDIOTOKEN(분류 손실(CL) 포함 및 미포함)에 대한 AIC, FID, AIS 및 IIS를 보고합니다. 참고로 원본 이미지(참조)와 텍스트 레이블이 있는 Stable Diffusion(SD)으로 생성한 이미지에 대한 결과도 보고합니다. 방법 참조 SD(텍스트) 메트릭 AIC ↑ FID↓↓ 54. AIS ↑ IIS ↑ 71.28 52. Wav2Clip [30] ImageBind [37] CL이 있는 AUDIOTOKEN AUDIOTOKEN 29.32 99.89 47.76 51.39.15 67.42 67.48 75.48.01 66.08 62.28 76.45.48 56.65 68.23 76. 사전 학습된 모델에서 얻은 내부 표현입니다. 이 작업에서는 Inception 모델을 사용합니다. 인간 평가. 마지막으로 주관적 테스트를 실행하여 생성된 이미지가 레이블에 얼마나 잘 부합하는지 평가합니다. 각 방법에 대해 주석자에게 생성된 이미지를 보여주고 1~5점 척도로 주어진 레이블과의 관련성을 평가하도록 요청했습니다. 3. 결과 이하에서 우리는 객관적이고 주관적인 관점에서 우리의 방법을 연구한다. 우리는 실험 설정에 대한 세부 사항을 설명하는 것으로 시작한다. 그런 다음, 섹션 2.2에서 제안한 평가 프레임워크를 사용하여 우리의 방법과 기준선에 대한 결과를 보고한다. 우리는 우리의 방법이 현재 기준선보다 성능이 우수함을 보여준다. 우리는 마지막으로 주관적으로 평가하고 주석자들이 우리의 방법이 오디오를 가장 잘 설명한다는 데 동의한다는 것을 알아낸다. 기준선. Wav2Clip[30]은 오디오-텍스트 쌍에 대해 CLIP 기반 손실을 사용한다. 그런 다음, 그들은 이 표현을 사용하여 VQ-GAN[32]을 사용하여 오디오와 높은 상관관계가 있는 텍스트에서 이미지를 생성한다. ImageBind[37]는 여섯 가지 다른 모달리티(텍스트, 이미지/비디오, 오디오, 깊이, 열 및 관성 측정 단위(IMU))의 정보를 단일 표현 공간으로 결합한다. 우리는 오디오 샘플에서 이미지를 생성하기 위해 ImageBind의 통합 잠재성을 stable-diffusion-2-1-unclip¹과 함께 사용했다.데이터. 우리는 YouTube 비디오 컬렉션과 해당 오디오비주얼 데이터에서 파생된 VGGSound[38] 데이터 세트를 사용합니다. 이 데이터 세트에는 각각 10초 길이의 200,000개가 포함되어 있습니다. 이 데이터 세트에는 또한 309개의 클래스가 주석으로 달려 있습니다. 하이퍼파라미터. Embedder 네트워크는 3개의 레이어로 구성되어 있으며, 248개의 시퀀스에 어텐션 풀링이 적용됩니다. 생성 모델의 경우 Stable Diffusion[16]을 사용합니다. 그 결과 8,853,507개의 매개변수 모델이 생성됩니다. 학습하는 동안 5초 오디오 클립을 무작위로 자르고 VGGSound 레이블에 해당하는 가장 높은 CLIP 점수를 가진 프레임을 선택합니다. 또한 이미지 및 오디오 분류기에서 분류가 일관되지 않은 프레임을 필터링합니다. Nvidia A6000 GPU에서 8e-5의 학습 속도와 8의 배치 크기로 60,000단계로 모델을 학습합니다. 객관적인 평가. 우리는 제안된 방법을 분류 손실(CL)이 있는 경우와 없는 경우, FID, AIS, AIC 및 IIS를 고려하여 Wav2Clip 및 ImageBind와 비교하는 것으로 시작합니다. 참고로, 우리는 또한 안정적 확산(SD)을 사용하여 텍스트 설명(텍스트 레이블)에서 직접 이미지를 생성한 결과의 최상위 행을 포함합니다. 결과는 표 1에 보고되어 있습니다. https://github.com/Zeqiang-Lai/ Anything2 이미지 그림 4: AUDIOTOKEN(첫 번째 행) 및 참조 이미지(두 번째 행)에 대한 스피커 생성의 정성적 결과. 결과에 따르면 모든 평가 지표를 고려할 때 AUDIOTOKEN이 Wav2Clip 및 ImageBind보다 우수합니다. 흥미롭게도 AUDIOTOKEN은 유사도 점수를 얻기 위해 Wav2Clip 및 ImageBind 모델을 활용하는 AIS 지표를 고려할 때도 더 나은 성능을 보입니다. 이 결과는 정확한 오디오 세부 정보 식별(예: 다양한 기타 구별)을 보여주고 여러 엔터티(예: 여러 비행 비행기 또는 단일 비행기)를 고려합니다. 예상대로 텍스트 레이블을 사용하면 정확도가 더 높아지고 모델은 대상 비디오와 상관관계가 낮지만 차별성이 더 강한 표현을 학습하는 쪽으로 이동합니다. 모든 방법에서 생성된 이미지는 그림 3에서 볼 수 있습니다. 주관적 평가. AUDIOTOKEN을 Wav2Clip과 비교하고 텍스트 설명을 사용하여 SD를 비교합니다. 테스트 세트에서 무작위로 15개 이미지를 샘플링하여 인간 주석자에게 텍스트 레이블과의 관련성을 1~5점 척도로 순위를 매기도록 요청합니다. 평가된 각 이미지에 대해 최소 17개 주석을 적용하고 표준 편차와 함께 평균 점수를 계산합니다. AUDIOTOKEN은 Wav2Clip보다 성능이 우수합니다(4.07±0.83 대 1.85±0.46). 텍스트 레이블을 사용하여 SD와 비교할 때 AUDIOTOKEN은 비슷한 성능에 도달하고 약간 더 나쁜 주관적 점수를 산출합니다(4.07±0. 대 4.58±0.60). 이러한 결과는 특히 고무적입니다.사용자가 AUDIOTOKEN에서 생성한 이미지가 텍스트 레이블을 사용하는 것과 유사하게 오디오 장면의 주요 객체를 포착한다는 것을 시사하기 때문입니다.텍스트 레이블은 상단 라인 역할을 합니다.화자 이미지 생성.다양한 화자의 비주얼을 만드는 잠재력을 조사합니다.이 목표를 달성하기 위해 Barack Obama, Donald Trump, Emma Watson, David Beckham을 소개하는 사람당 30분 분량의 비디오 두 개에서 샘플을 수집하고 XVector [39]에서 오디오 표현을 추출했습니다.그림 4의 결과는 우리의 접근 방식이 Barack Obama와 Donald Trump를 정확하게 표현한다는 것을 나타냅니다.이는 그들의 독특한 목소리 때문일 수 있다고 가정합니다.그러나 Emma Watson과 David Beckham의 경우 이 방법은 주로 그들의 성별을 포착합니다.4.
--- CONCLUSION ---
s 이 논문에서는 오디오 기반 컨디셔닝을 위해 텍스트 조건 생성 모델을 활용하는 방법을 제시합니다. 저희의 방법은 오디오 녹음의 장면을 설명하는 고품질 이미지를 생성합니다. 또한 생성된 이미지의 의미를 고려하는 포괄적인 평가 프레임워크를 제안합니다. 저희의 방법은 오디오 조건 이미지 생성을 향한 첫 번째 단계를 제시합니다. 오디오의 숨겨진 정보는 텍스트에서 관찰된 정보보다 풍부합니다. 따라서 저희는 이 문제가 흥미롭고 커뮤니티에서 더 많은 관심을 받아야 한다고 생각합니다. 5. 참고문헌 [1] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, SKS Ghasemipour, BK Ayan, SS Mahdavi, RG Lopes et al., &quot;심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델&quot;, NeurIPS에 게재. [2] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, Y. Taigman, &quot;Make-a-scene: 인간의 사전 지식을 이용한 장면 기반 텍스트-이미지 생성&quot;, ECCV, 2022. [3] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, WT Freeman, M. Rubinstein 등, &quot;Muse: 마스크 처리된 생성 변환기를 통한 텍스트-이미지 생성&quot;, arXiv 사전 인쇄본 arXiv:2301.00704, 2023. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar 등, 영어: &quot;Llama: 개방적이고 효율적인 기초 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [5] TL Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, AS Luccioni, F. Yvon, M. Gallé et al., &quot;Bloom: 176b-매개변수 오픈 액세스 다국어 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2211.05100, 2022. [6] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., &quot;언어 모델은 소수의 샷 학습기입니다,&quot; 신경 정보 처리 시스템의 발전, vol. 33, pp. 1877-1901, 2020. [7] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman 및 Y. Adi, &quot;Audiogen: 텍스트 안내 오디오 생성&quot;, arXiv 사전 인쇄 arXiv:2209.15352, 2022. [8] F. Kreuk, Y. Taigman, A. Polyak, J. Copet, G. Synnaeve, A. Défossez 및 Y. Adi, &quot;지각 유도 이산 표현을 사용한 오디오 언어 모델링&quot;, arXiv 사전 인쇄 arXiv:2211.01223, 2022. [9] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed 등, &quot;원시 오디오로부터의 생성적 구어 언어 모델링에 관하여,&quot; 계산언어학 협회 논문집, 9권, 1336-1354쪽, 2021. [10] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li 등, &quot;신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다,&quot; arXiv 사전 인쇄본 arXiv:2301.02111, 2023. [11] H. Cao, C. Tan, Z. Gao, G. Chen, P.-A. 영어: Heng 및 SZ Li, &quot;생성 확산 모델에 대한 조사,&quot; arXiv 사전 인쇄본 arXiv:2209.02646, 2022. [12] J. Ho, A. Jain 및 P. Abbeel, &quot;잡음 제거 확산 확률적 모델,&quot; NeurIPS, 2020. [13] P. Dhariwal 및 A. Nichol, &quot;확산 모델은 이미지 합성에서 간을 이긴다,&quot; 신경 정보 처리 시스템의 발전, 제 13권, 2호, 1999, 236-242쪽. 34, pp. 8780-8794, 2021. [14] AQ Nichol 및 P. Dhariwal, &quot;개선된 잡음 제거 확산 확률적 모델&quot;, 국제 기계 학습 컨퍼런스에서. PMLR, 2021, pp. 8162-8171. [15] Z. Kong, W. Ping, J. Huang, K. Zhao 및 B. Catanzaro, &quot;Diffwave: 오디오 합성을 위한 다재다능한 확산 모델&quot;, arXiv 사전 인쇄본 arXiv:2009.09761, 2020. [16] R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, CVPR에서, 2022. [17] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu 및 M. Chen, &quot;클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성,&quot; arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [18] AQ Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever 및 M. Chen, &quot;글라이드: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해,&quot; 국제 기계 학습 컨퍼런스. PMLR, 2022, pp. 16 784-16804. [19] R. Sheffer 및 Y. Adi, &quot;I hear your true colors: Image Guided audio generation,&quot; arXiv 사전 인쇄본 arXiv:2211.03089, 2022. [20] V. Iashin 및 E. Rahtu, &quot;Taming visualguided sound generation,&quot; British Machine Vision Conference(BMVC), 2021. [21] R. Gao 및 K. Grauman, &quot;Visualvoice: Audio-visual speech separation with cross-modal consistency,&quot; 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR). IEEE, 2021, pp. 15 490–15 500. [22] W.-N. Hsu, T. Remez, B. Shi, J. Donley, and Y. Adi, “Revise: Self-supervised speech resynthesis with visual input for universal and generalized speech enhancement,&quot; arXiv preprint arXiv:2212.11377, 2022. [23] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf, &quot;Zerocap: Zeroshot image-to-text generation for visual-semantic arithmetic,&quot; in CVPR, 2022. [24] J. Li, D. Li, S. Savarese, and S. Hoi, &quot;Blip-2: Bootstrapping language-image pre-training with frozen image decoders and large language models,&quot; arXiv preprint arXiv:2301.12597, 2023. [25] F. Kreuk, A. Polyak, J. Copet, E. Kharitonov, TA Nguyen, M. Rivière, W.-N. Hsu, A. Mohamed, E. Dupoux, 및 Y. Adi, &quot;이산 및 분해된 표현을 사용한 텍스트 없는 음성 감정 변환&quot;, EMNLP, 2022. [26] G. Maimon 및 Y. Adi, &quot;이산 자체 감독 단위를 사용한 말하기 스타일 변환&quot;, arXiv 사전 인쇄본 arXiv:2212.09730, 2022. [27] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, AH Bermano, G. Chechik, 및 D. Cohen-Or, &quot;이미지는 한 단어의 가치가 있습니다. 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화&quot;, arXiv 사전 인쇄본 arXiv:2208.01618, 2022. [28] M. Żelaszczyk 및 J. Mańdziuk, “오디오-이미지 크로스 모달 생성,&quot; IJCNN, 2022. [29] C.-H. Wan, S.-P. Chuang 및 H.-Y. Lee, “생성적 적대 신경망을 이용한 오디오에서 장면 이미지 합성으로”, ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리(ICASSP) 국제 컨퍼런스. IEEE, 2019, pp. 496–500. [30] H.-H. Wu, P. Seetharaman, K. Kumar, JP Bello, &quot;Wav2clip: 클립에서 강력한 오디오 표현 학습&quot;, ICASSP, 2022. [31] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, ICML, 2021. [32] P. Esser, R. Rombach, B. Ommer, &quot;고해상도 이미지를 위한 변환기 길들이기&quot; 합성,” IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021, pp. 12873-12883. [33] K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan, L. Castricato, 및 E. Raff, &quot;Vqgan-clip: 자연어 안내를 통한 오픈 도메인 이미지 생성 및 편집,&quot; Computer Vision-ECCV 2022: 제17회 유럽 컨퍼런스, 이스라엘 텔아비브, 2022년 10월 23-27일, 회의록, XXXVII부. Springer, 2022, pp. 88-105. [34] D. Hendrycks 및 K. Gimpel, &quot;가우스 오차 선형 단위(gelus)&quot;, arXiv 사전 인쇄본 arXiv:1606.08415, 2016. [35] I. Schwartz, S. Yu, T. Hazan 및 AG Schwing, &quot;요인 그래프 주의&quot;, CVPR, 2019. [36] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler 및 S. Hochreiter, &quot;2개 시간 척도 업데이트 규칙으로 학습된 Gans는 로컬 내쉬 균형으로 수렴합니다.&quot; NeurIPS, 2017. [37] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, KV Alwala, A. Joulin 및 I. Misra, &quot;Imagebind: 모든 것을 바인딩할 수 있는 하나의 임베딩 공간&quot;, CVPR, 2023. [38] H. Chen, W. Xie, A. Vedaldi, 및 A. Zisserman, &quot;Vggsound: 대규모 오디오-비주얼 데이터세트,&quot; ICASSP, 2020. [39] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, 및 S. Khudanpur, &quot;X-벡터: 화자 인식을 위한 강력한 dnn 임베딩,&quot; 2018 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2018, 5329-5333쪽.
