--- ABSTRACT ---
LLM의 새로운 시대로 접어들면서 LLM의 역량, 한계, 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 이 방향으로 더 나아가기 위해 대규모 LLM(예: ChatGPT)과 규모가 작지만 효과적인 오픈소스 LLM 및 이들의 정제된 대응물 간의 격차를 더 깊이 이해하기 위해 노력합니다. 이를 위해 우리는 특히 장문 질의응답(LFQA)에 집중합니다. 왜냐하면 이 방식은 여러 가지 실용적이고 영향력 있는 응용 프로그램(예: 문제 해결, 고객 서비스 등)이 있지만 여전히 LLM에게 연구가 부족하고 어려운 방식이기 때문입니다. 우리는 추상적 요약에서 질문 생성 방법을 제안하고 긴 문서의 요약에서 후속 질문을 생성하면 LLM이 긴 맥락에서 추론하고 추론하기에 어려운 환경을 만들 수 있음을 보여줍니다. 실험 결과는 다음과 같습니다. (1) 추상 요약에서 질문을 생성하는 제안된 방법은 LLM에 어려운 설정을 제시하며 ChatGPT와 같은 LLM과 오픈 소스 LLM(Alpaca, Llama) 사이에 성능 격차를 보여줍니다. (2) 오픈 소스 LLM은 원본 문서에서 생성된 질문에 대한 컨텍스트 의존도가 감소하지만 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. 특히 긴 컨텍스트(&gt;1024 토큰)의 경우 그렇습니다. 1
--- INTRODUCTION ---
ChatGPT, GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)이 다양한 벤치마크에서 우수한 성능을 보인 반면, 오픈소스 노력도 MMLU(Hendrycks 등, 2021), OpenLLMBoard(Anil 등, 2023; Beeching 등, 2023; Touvron 등, 2023)와 같은 다양한 애플리케이션과 벤치마크에서 따라잡기 위해 빠르게 진행되고 있습니다. 새로운 모델과 기술에 대한 빠른 속도로 진행되는 LLM의 새로운 시대로 접어들면서 이들 간의 기능, 한계 및 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 일관된 텍스트를 생성할 수 있는 LLM은 요약과 같은 작업에서 좋은 성능을 보이는 것으로 입증되었지만(Ouyang 등, 2022), LFQA에서의 성능은 상대적으로 덜 알려져 있습니다. 장문 질의응답(LFQA)은 다양하고 영향력 있는 실제 세계 응용 프로그램(예: 도움말 포럼, 문제 해결, 고객 서비스 등)에서 해결되지 않은 중요한 과제 중 하나입니다. 이러한 질문에 답하려면 종종 원본 문서에 분산된 정보 범위에 걸쳐 질의와 추론을 이해하는 복잡한 추론 능력이 필요합니다. 추상적 요약은 압축된 형태로 기사의 핵심을 포함합니다(Fabbri et al., 2020). 이러한 요약의 후속 질문은 소스 문서의 다른 부분을 연결하는 주제에 대한 더 깊은 이해가 필요할 것이라고 가정합니다. 게다가 Pang et al.(2022)은 긴 문서의 3분의 1 이상을 이해해야 하는 답변은 종종 인간에 의해 &quot;HARD&quot;로 평가된다는 것을 보여줍니다. 따라서 우리는 대규모 LLM과 작지만 입증된 성공적인 기본 LLM(예: Llama-7B, 13B) 및 이들의 정제된 버전(예: Alpaca-7B, 13B)의 차이를 분석하고 연구하기 위한 확장 가능한 평가 방법을 제안합니다. 이를 위해, 우리는 문서 요약에서 복잡한 질문을 생성하기 위한 구체적인 지침으로 ChatGPT를 촉구할 것을 제안합니다. 생성된 질문의 복잡성과 오픈소스 LLM의 답변 품질이라는 두 가지 측면에 대한 우리의 경험적 분석은 요약에서 생성된 후속 질문이 LLM의 추론 능력을 테스트하기 위한 도전적이면서도 더 현실적인 설정을 제시한다는 것을 보여줍니다. 장문 QA에 대한 인간 평가에 전적으로 의존하는 것은 비용이 많이 들고 확장하기 어렵기 때문에(Pagnoni et al., 2021), 우리는 대신 GPT-4를 활용하여 이전 연구에 따라 일관성, 관련성, 사실적 일관성 및 정확성에 대한 답변 품질을 평가합니다(Fabbri et al., 2020; Fan et al., 2019). 그러나 우리는 또한 더 작은 규모의 인간 평가를 수행하는데, 이는 GPT가 인간 평가와 높은 상관관계를 가지고 있어 우리의 평가를 신뢰할 수 있음을 보여줍니다. 이 연구의 주요 결과는 다음과 같습니다. • 추상적 요약에서 질문을 생성하는 우리의 제안된 방법은 20% 이상의 시간 동안 맥락을 여러 번 통과하는 더 긴 맥락에서 추론해야 합니다. • 증류된 LLM(Alpaca-7B, 13B)은 원본 문서에서 생성된 질문에 대한 맥락에 덜 의존하는 경향이 있지만, 문서 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. • 증류된 LLM에서 생성된 답변은 다른 설정에서 일관성이 있을 수 있지만 질문에서 벗어나 요약에서 생성된 질문에 대해 반복적이고 부분적으로 올바른 답변을 생성하는 경향이 있습니다(&gt;16.8%) • Alpaca-7B, 13B는 기본 LLM(Llama)보다 의미 있는 답변을 대부분 생성하지만 긴 맥락(&gt;1024 토큰)에 민감합니다. 2
--- RELATED WORK ---
긴 문서에 대한 추론: LLM은 상식적 추론(Talmor 등, 2019), 수학적 및 기호 추론(Huang 및 Chang, 2023; Cobbe 등, 2021), SQuaD, HotpotQA와 같은 질의 응답 작업과 같은 여러 작업에 대해 추론하는 놀라운 능력을 보여주었습니다. 그러나 이러한 작업의 대부분은 긴 맥락을 필요로 하지 않으며 답변은 종종 맥락에서 짧은 문구나 텍스트 범위입니다. 이 연구에서 우리는 LLM이 긴 문서의 요약에서 후속 질문을 생성하도록 LLM(ChatGPT)에게 촉구하여 더 깊은 이해 능력과 더 긴 맥락이 필요한 긴 문서에 대해 추론하는 것을 평가합니다. 모델 기반 평가: 이전 연구에서는 학습된 모델을 사용하여 자동 평가 지표를 제안했습니다(Zhang* 등, 2020; Laban 등, 2022). 특히 요약(Fabbri et al., 2020; Kryscinski et al., 2020)과 같은 장문 텍스트 생성 작업의 경우 일관성과 응집성이 소스 문서와 생성된 요약 간의 함축으로 측정됩니다. 최근 Liu et al.(2023)은 GPT-4가 인간과 가장 높은 상관 관계를 가지고 있으며 다른 모든 자동 평가를 능가한다는 것을 보여주었습니다.
--- METHOD ---
추상적 요약에서 긴 문서의 요약에서 후속 질문을 생성하는 것이 LLM이 긴 맥락에서 추론하고 추론할 수 있는 도전적인 환경을 만들 수 있음을 보여줍니다.
--- EXPERIMENT ---
모든 결과는 다음 사실을 확인합니다. (1) 추상 요약에서 질문을 생성하는 제안하는 방법은 LLM에 까다로운 설정을 제시하며 ChatGPT와 같은 LLM과 오픈소스 LLM(알파카, 라마) 사이에 성능 격차를 보입니다. (2) 오픈소스 LLM은 원본 문서에서 생성된 질문에 대한 컨텍스트 의존도가 낮지만 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. 특히 긴 컨텍스트(&gt;1024개 토큰)의 경우 그렇습니다. 1 서론 ChatGPT, GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)이 다양한 벤치마크에서 우수한 성능을 보인 반면, MMLU(Hendrycks 등, 2021), OpenLLMBoard(Anil 등, 2023; Beeching 등, 2023; Touvron 등, 2023)와 같은 오픈소스 노력도 다양한 애플리케이션과 벤치마크에서 따라잡기 위해 빠르게 진행되고 있습니다. 새로운 모델과 기술에 대한 빠른 진전과 함께 LLM의 새로운 시대로 접어들면서 이들 간의 역량, 한계, 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 일관된 텍스트를 생성할 수 있는 LLM은 요약과 같은 작업에서 좋은 성과를 보이는 것으로 입증되었지만(Ouyang et al., 2022), LFQA에서의 성과는 상대적으로 덜 알려져 있습니다. 장문 질의응답(LFQA)은 다양하고 영향력 있는 실제 세계 응용 프로그램(예: 도움말 포럼, 문제 해결, 고객 서비스 등)에서 중요한 미해결 과제 중 하나입니다. 이러한 질문에 답하려면 종종 원본 문서에 분산된 정보 범위에 걸쳐 쿼리와 추론을 이해하는 복잡한 추론 능력이 필요합니다. 추상적 요약은 압축된 형태로 기사의 핵심을 포함합니다(Fabbri et al., 2020). 이러한 요약의 후속 질문은 소스 문서의 여러 부분을 연결하는 주제에 대한 더 깊은 이해가 필요할 것이라고 가설을 세웁니다. 또한 Pang et al. (2022)는 긴 문서의 3분의 1 이상을 이해해야 하는 답변은 종종 인간에 의해 &quot;HARD&quot;로 평가된다는 것을 보여줍니다. 따라서 우리는 대규모 LLM과 작지만 입증된 성공적인 기본 LLM(예: Llama-7B, 13B) 및 이들의 정제된 버전(예: Alpaca-7B, 13B)의 차이를 분석하고 연구하기 위한 확장 가능한 평가 방법을 제안합니다. 이를 위해 우리는 문서 요약에서 복잡한 질문을 생성하기 위한 특정 지침으로 ChatGPT를 촉구할 것을 제안합니다. 두 가지 측면(생성된 질문의 복잡성과 오픈소스 LLM의 답변 품질)에 대한 우리의 경험적 분석은 요약에서 생성된 후속 질문이 LLM의 추론 능력을 테스트하기 위한 도전적이지만 더 현실적인 설정을 제시한다는 것을 보여줍니다. 장문 QA에 대한 인간 평가에 전적으로 의존하는 것은 비용이 많이 들고 확장하기 어렵기 때문에(Pagnoni et al., 2021), 대신 GPT-4를 활용하여 이전 연구(Fabbri et al., 2020; Fan et al., 2019)에 따라 일관성, 관련성, 사실적 일관성 및 정확성에 대한 답변 품질을 평가합니다. 그러나 더 작은 규모의 인간 평가도 수행하여 GPT가 인간 평가와 높은 상관관계를 가지고 있어 평가를 신뢰할 수 있음을 보여줍니다. 이 연구에서 얻은 주요 결과는 다음과 같습니다. • 추상 요약에서 질문을 생성하는 제안 방법은 더 긴 맥락에서 추론해야 하며, 맥락을 20% 이상 여러 번 통과해야 합니다. • 증류된 LLM(Alpaca-7B, 13B)은 원본 문서에서 생성된 질문에 대한 맥락에 덜 의존하는 경향이 있지만, 문서 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. • 증류된 LLM에서 생성된 답변은 다양한 설정에서 일관될 수 있습니다. 그러나 질문에서 벗어나는 경향이 있으며 요약에서 생성된 질문에 대해 반복적이고 부분적으로 올바른 답변을 생성합니다(&gt;16.8%) • Alpaca-7B, 13B는 대부분 기본 LLM(Llama)보다 의미 있는 답변을 생성하지만 긴 맥락(&gt;1024개 토큰)에 민감합니다.2 관련 작업 긴 문서에 대한 추론: LLM은 상식적 추론(Talmor 등, 2019), 수학적 및 기호 추론(Huang 및 Chang, 2023; Cobbe 등, 2021), SQuaD, HotpotQA와 같은 질의 응답 작업과 같은 여러 작업에 대해 추론하는 놀라운 능력을 보여주었습니다.그러나 이러한 작업의 대부분은 긴 맥락을 필요로 하지 않으며 답변은 종종 맥락의 짧은 문구나 텍스트 범위입니다.이 작업에서 우리는 LLM(ChatGPT)이 긴 문서의 요약에서 후속 질문을 생성하도록 촉구하여 더 깊은 이해 능력과 더 긴 맥락이 필요한 긴 문서에 대해 추론하도록 LLM을 평가합니다. 모델 기반 평가: 이전 연구에서는 학습된 모델을 사용한 자동 평가 지표를 제안했습니다(Zhang* 등, 2020; Laban 등, 2022). 특히 요약과 같은 장문 텍스트 생성 작업(Fabbri 등, 2020; Kryscinski 등, 2020)의 경우 일관성과 응집성이 소스 문서와 생성된 요약 간의 함의로 측정됩니다. 최근 Liu 등(2023)은 GPT-4가 인간과 가장 높은 상관 관계를 가지고 있으며 요약 작업에서 다른 모든 자동 평가 방법을 능가한다는 것을 보여주었습니다. 우리는 이러한 작업에서 영감을 얻어 평가 프롬프트를 설계하고 GPT-4를 생성된 답변의 평가자로 사용합니다. 3 제안된 평가 방법 3. 데이터 소스 다양하고 널리 사용 가능한 데이터 세트를 만들기 위해 Wikipedia 문서를 사용합니다. Wikipedia API의 Category 목록 메타데이터를 사용하여 주제의 다양성을 제어하고 다음 9개 도메인에서 문서를 수집합니다.물리학, 엔터테인먼트, 역사, 컴퓨터 과학, 사회 과학, 사회, 경제, 의학, 스포츠.문서 페이지는 종종 LLM의 맥락에 맞게 길어질 수 있습니다.따라서 SpaCy 토크나이저를 사용하여 최소 256개 토큰 길이의 문서 페이지에서 섹션별 데이터를 추출하고 짧은 섹션을 결합합니다.다른 모델 간의 공정한 비교를 위해 실험에서 최대 2k 토큰의 맥락 길이를 설정했습니다.또한 사전 처리 필터를 사용하여 정보가 없는 문서를 필터링합니다.자세한 내용은 부록 A.5에서 확인할 수 있습니다.3.2 ChatGPT를 사용한 질문 생성 질문 생성 방법을 2단계 프로세스로 공식화합니다.(1) 요약 및 (2) 요약에서 질문 생성.요약 먼저 3.1절에 설명된 대로 Wikipedia에서 섹션별 구절을 수집합니다. 그런 다음 ChatGPT(gpt-turbo-3.5)를 사용하여 원본 문서의 요약을 생성합니다. ChatGPT에 더 많은 맥락을 제공하기 위해 구절에서 기사의 제목과 도메인에 대한 정보를 제공합니다. 요약에서 질문 생성 이 단계에서는 ChatGPT가 문서 요약을 맥락으로 사용하여 질문을 생성하도록 합니다. 무작위 순서로 질문이 생성되는 것을 피하기 위해 ChatGPT에 답할 상위 3개의 복잡한 질문을 제공하도록 지시합니다. 질문 생성 프로세스의 유용성을 보여주기 위해 동일한 지침으로 질문이 구절에서 직접 생성되는 기준선도 설정합니다. 설정에 사용된 프롬프트에 대한 부록 A.1을 참조하세요. 요약하면 각 도메인에서 50개 구절에 대해 3개의 질문을 생성하여 각 설정에 대해 총 1350개의 질문을 생성합니다. 3.3 생성된 질문 복잡성 평가 Pang et al.(2022)은 질문의 복잡성을 평가하기 위해 광범위한 주석 지침을 설계했습니다. 인간이 &#39;HARD&#39;로 평가한 질문 중 26.7%의 질문(쉬운 질문보다 20.2% 높음)이 주어진 맥락에서 질문에 답할 수 있고 모호하지 않습니까?Q2. 질문에 답하기 위해 얼마나 많은 맥락이 본문에서 필요합니까?Q3: 질문이 본문을 여러 번 통과해야 합니까?선택지 A. 예 B. 아니요 A. 한두 문장만 B. 2문장 이상이지만 한 단락 미만 C. 전체 본문의 최소 1/3 D. 본문 대부분 A. 예 B. 아니요 표 1: 생성된 질문의 복잡성을 평가하기 위해 설계된 프롬프트. 주어진 정보의 최소 1/3 이상이 답변에 필요했습니다.생성된 질문의 질을 평가하기 위해 ChatGPT에 다음의 질문(표 1)을 프롬프트합니다.(1) 본문에서(QG-Passage) (2) 요약에서(QG-Summary). 이전 작업에 따라 다수결 투표를 통해 ChatGPT에서 답변 불가능한 것으로 평가된 질문을 제외하기 위해 다른 top_p {0.8, 0.9, 1}로 질문을 제시했습니다. 필터링 후 각 설정에서 생성된 질문 1278개가 있습니다. Q1: 명확성 Q2. 맥락 길이: = QG - 구절 QG - 요약 96.6% 94.7% 문장 또는 단락 미만 79.3% 구절의 1/3 이상 또는 대부분 Q3: 구절의 다중 통과 20.7% 75.7 % 24.3% 24.4% 31% 표 2: 생성된 질문의 복잡성을 평가하도록 설계된 프롬프트 4 결과 및 분석 4.1 실험 설정 맥락 길이로 인해 우리 설정에서는 few-shot 설정이 실행 불가능하므로 zero-shot 평가에 대한 모델 성능을 비교합니다. 우리는 최종 평가 데이터 세트에 대한 답변으로 자유형 텍스트를 생성하기 위해 다음 모델을 촉구합니다: ChatGPT(OpenAI, 2023), Alpaca-7B, 13B(Taori et al., 2023), LLaMa-7B, 13B(Touvron et al., 2023). 우리는 ChatGPT에 OpenAI API를 사용하고 HuggingFace 1의 오픈소스 LLM에 대한 체크포인트를 로드합니다. 답변 생성에 사용된 프롬프트는 부록 A에 있습니다. 다른 모델과의 공정한 비교를 위해 실험에서는 2k 시퀀스 길이를 넘는 입력은 고려하지 않습니다. 또한 Alpaca에서 질문을 생성하는 것을 테스트한 결과 지침을 따르지 않고 종종 관련 없는 콘텐츠를 생성하는 것으로 나타났습니다. 자세한 분석은 부록 A.2에서 확인할 수 있습니다. GPT-4는 평가자로서 장문 텍스트 생성 작업(요약(Liu et al., 2023))에서 인간 평가와 높은 상관관계를 보였으며, ROUGE 및 BLEU 점수와 같은 다른 자동 평가 지표를 능가했습니다. LLM은 설정에 대한 자유형 답변을 생성할 것으로 예상되므로 장문 텍스트 생성 지표(Fabbri et al., 2020)에 대한 이전 작업에서 영감을 얻어 일관성, 정확성 및 관련성에 대한 평가에 적용했습니다. 기본적으로 인간 평가에 대한 지침으로 사용되는 정의를 아래와 같이 방법에 적용했습니다. 일관성: 답변은 잘 구성되고 잘 정리되어야 하며 관련 정보의 더미가 되어서는 안 됩니다. 관련성: 답변은 질문과 맥락과 관련이 있어야 합니다. 답변은 간결해야 하며 질문에서 벗어나지 않아야 합니다. 사실적 일관성: 맥락이 답변의 주요 출처여야 합니다. 답변에는 조작된 사실이 포함되어서는 안 되며 맥락에 있는 정보를 수반해야 합니다. 정확도: 답변은 만족스럽고 질문에 대한 완전해야 합니다. 답변이 제시된 질문에 대한 답변인지 확인하여 답변의 정확성을 측정합니다. GPT-4가 4가지 지표 모두에서 0~3점(높을수록 좋음)의 척도로 답변을 평가하도록 합니다. GPT-4에서 얻은 모든 평가의 평균을 내고 결과를 표 3에 제시합니다. 평가 프롬프트는 부록 A.3.1에서 찾을 수 있습니다. 최적의 프롬프트는 항상 인간의 답변을 선호해야 하며 모델에서 생성된 답변에 편향되어서는 안 된다고 가정합니다. Laskar et al. (2023)은 ChatGPT와 같은 LLM이 TruthfulQA 데이터 세트에서 여전히 인간보다 성과가 낮음을 보여줍니다(Lin et al., 2022). 따라서 평가 프롬프트의 신뢰성과 충실성을 확인하기 위해 TruthfulQA 데이터 세트에서 GPT-4로 대리 테스트를 수행합니다. 우리는 ChatGPT와 오픈소스 LLM에서 생성된 답변을 무작위로 샘플링한 50개의 테스트 인스턴스에서 실제 사실과 비교 테스트하고, GPT-4 프롬프트를 사용한 평가 프롬프트가 90% 이상의 시간 동안 모델에서 생성된 답변보다 사실적 일관성과 정확성 측면에서 인간이 작성한 답변을 선호한다는 것을 발견했습니다. 또한, 우리는 LLM에서 생성된 답변에 대한 인간 평가도 수행하고, 섹션 A.4에서 GPT-4 평가와 인간 평가의 상관 관계를 논의합니다. 4.2 결과 실험 결과에 따르면 ChatGPT는 모든 지표에서 다른 LLM보다 22.4% -40.1%로 큰 폭으로 우수한 성과를 보였습니다.Passage 2.922.3.1.1.1.2.1,971.Summary 2.1.1.2.0.1.1.1.0.1.0.1.Passage 1.1.2.112.llama7B 0.1.alpaca7B llama13B 0.alpaca13B chatgpt 0.llama7B alpaca7B llama13B alpaca13B chatgpt 모델 QG-Passage QG-Summary 3. 컨텍스트 없음 컨텍스트 있음 컨텍스트 없음 컨텍스트 있음 2.ChatGPT 2.2.2.2.2.Alpaca-13B 2.2.2.2.LlaMa-13B 1.1.0.1.1.0.알파카-7B 2.1.1.1.0.LlaMa-7B 0.1.0.0. <Summary Passage 1.1.961.1,1.2,182.2.1.1,911.3.Summary 2.ارآرا ارت ارتيا لارتريا 0.alpaca13B chatgpt 0.0. llama 7B alpaca7B llama13B alpaca13B chatgpt 2.3.2.Summary 2.1.Passage 1.741.1.1.2.2.0.Figure 1: Graphs showing the breakdown of models with respect to different metrics used in evaluation: (a) Coherence (b) Relevance (c) Answer Accuracy (d) Factual Consistency 0.0.llama7B alpaca7B <Context Length <Table 3: Performance of different models based on GPT4 evaluation. The table shows average ratings across all metrics: accuracy, coherency, consistency, relevance. best performing LLM (Alpaca-13B). However; all the models including ChatGPT generate less accurate and relevant answers for QG-Summary when compared to QG-Passage; while the gap is much larger in open-source LLMs. We also find that most of the LLMs find context important in order to generate answers; however, the gap is much smaller for QG-Passage (avg. gap of 0.12 v.s. 0.2). Surprisingly, Alpaca-7B, 13B models perform better w/o context for QG-Passage. We hypothesize that questions directly generated from the context passage can be simple that could be directly answered from the parametric knowledge of LLMs without additional context. On further analysis, we observe that Alpaca-7B,13B performance drops significantly in longer contexts (Figure 2). We hypothesize that in a constrained sequence length setting, adding supporting context (even gold passage) may not be always helpful. We leave further analysis for future work and hope our analysis will motivate future research directions to study when to add/not add context for different tasks. Performance of LLMs on different metrics Figure 1 presents the performance of models across different metrics for QG-Summary. We observe two trends: (1) Open-source base LLMs (Llama7B,13B) suffer at all fronts significantly on generated answer quality whereas distilled models perform better than their counterparts (Llama) on all the settings. (2) QG-Summary provides a more challenging setting for all the LLMs: specifically, we notice that degradation in coherency score is negligent on ChatGPT and Alpaca-13B while other metrics like relevance, answer accuIlama7B alpaca 7B llama13B alpaca13B+ chatgpt Figure 2: Performance (avg. ratings) of LLMs across different context length. racy and factual consistency degrade consistently. We find open-source LLMs to drift from the question, generate partially correct answers and repeat more frequently in QG-Summary setting leading to lower scores. This further confirms that our proposed evaluation method QG-Summary challenges LLMs for deeper reasoning capabilities. // Context Length Analysis We analyze the effect of context length across LLMs in our proposed setting (QG-Summary). As expected, ChatGPT remains robust to context length until 2k tokens with Llama variants performing worse than other models (Figure 2). Interestingly, we find distilled models (Alpaca) being consistent until 1024 tokens, however beyond > 1024토큰의 경우 라마보다 성능이 더 빨리 저하됩니다.5
--- CONCLUSION ---
ChatGPT와 같은 LLM과 오픈소스 성공 LLM이 등장하면서 다양한 LLM의 역량과 한계를 이해하는 것이 매우 중요합니다. 더 긴 맥락을 참조하여 LLM의 더 깊은 추론 능력을 테스트하기 위해 긴 문서 요약에 대한 ChatGPT에서 생성한 질문에 대해 LLM에서 생성한 답변을 평가합니다. 결과에 따르면 제안한 질문 생성 방법은 LLM에 도전적인 설정을 제시하고 대규모 LLM과 오픈소스 LLM 간의 성과 격차를 밝혀냅니다. 분석이 제한된 시퀀스 길이 설정에서 더 긴 맥락을 활용하고 소규모 LLM에 더 나은 장문 텍스트 생성을 개발하는 것과 같은 미래 연구 방향에 동기를 부여하기를 바랍니다. 6 한계 이 연구에서는 요약에서 질문을 생성하는 자동 평가 설정을 제안하고 LLM에서 생성된 답변은 다양한 메트릭에 대해 GPT-4를 사용하여 평가합니다. 실험 결과에 따르면 제안한 평가 설정은 LLM에 도전적인 설정인 것으로 나타났습니다. 그러나 연구에는 몇 가지 한계가 있을 수 있습니다. 평가자로서의 GPT-4 GPT-4는 장문 텍스트 생성을 위한 인간 평가와 높은 상관관계를 보였지만(Liu et al., 2023), 평가를 위해 GPT-4를 사용하는 기능은 그 자체로 활발한 연구 분야입니다. 따라서 우리의 결과는 GPT-4의 발견되지 않은 기능에 의해 제한될 수 있습니다. 질문 생성을 위한 ChatGPT ChatGPT에서 유도된 질문에 대한 답변을 생성하면 ChatGPT의 낙관적인 결과로 이어질 수 있습니다. 그러나 의미 있는 질문을 생성하기 위한 다른 기준에는 한계가 있습니다. 질문 생성을 위해 다른 LLM을 사용하는 것에 대한 광범위한 분석을 보여줍니다(부록 A.2). 알려지지 않은 교육 데이터 ChatGPT와 같은 대규모 LLM의 교육 데이터 분포에 대해서는 알려진 바가 거의 없습니다. 다른 방법과 데이터 분포로 교육된 모델은 공정한 비교를 위한 평가를 더 어렵게 만듭니다. 참고문헌 Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, 임현택, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant 미스라, 메이삼 무살렘, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov 및 Yonghui Wu. 2023. Palm 2 기술 보고서. Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf. 2023. Open Ilm 리더보드. https://huggingface.co/spaces/ HuggingFaceH4/open_11m_leaderboard. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman. 2021. 수학 단어 문제를 풀기 위한 검증자 훈련. Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, Dragomir Radev. 2020. Summeval: 요약 평가 재평가. arXiv 사전 인쇄본 arXiv:2007.12626. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli. 2019. ELI5: 장문 질의응답. 이탈리아 피렌체에서 열린 제57회 전산언어학 협회 연례 회의록, 3558-3567쪽. 전산언어학 협회. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 2021. 대규모 멀티태스크 언어 이해 측정. 국제 학습 표현 컨퍼런스(ICLR) 회의록. Jie Huang, Kevin Chen-Chuan Chang. 2023. 대규모 언어 모델에서의 추론을 향하여: 조사. Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette. 2018. NarrativeQA 독해 능력 챌린지. Association for Computational Linguistics의 트랜잭션, 6:317–328. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher. 2020. 추상적 텍스트 요약의 사실적 일관성 평가. 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 9332-9346페이지, 온라인. Association for Computational Linguistics. Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst. 2022. SummaC: 요약에서 불일치 감지를 위한 NLI 기반 모델 재방문. Association for Computational Linguistics의 트랜잭션, 10:163–177. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. 벤치마크 데이터 세트에 대한 chatgpt의 체계적 연구 및 종합적 평가. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: 모델이 인간의 거짓을 어떻게 모방하는지 측정. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: 더 나은 인간 정렬을 갖춘 gpt-4를 사용한 Nlg 평가. OpenAI. 2023. Gpt-4 기술 보고서. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 2022. 인간 피드백을 통해 지시를 따르도록 언어 모델 훈련. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov. 2021. FRANK를 사용한 추상 요약의 사실성 이해: 사실성 지표의 벤치마크. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 4812-4829쪽, 온라인. 컴퓨터 언어학 협회. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel Bowman. 2022. 품질: 긴 입력 텍스트로 질문에 답하기, 맞아요! 2022년 북미 컴퓨터 언어 학회 회의록: 인간 언어 기술, 5336~5358쪽, 미국 시애틀. 컴퓨터 언어 학회. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant. 2019. CommonsenseQA: 상식적 지식을 목표로 하는 질문에 답하기 챌린지. 2019년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4149-4158쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학회. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, Yoav Artzi. 2020. Bertscore: bert를 사용한 텍스트 생성 평가. International Conference on Learning Representations에서. 부록 A.1 질문 생성에 사용된 프롬프트 이전 연구(Kočiský et al., 2018; Pang et al., 2022)의 분석에 따라 질문 생성 방법을 2단계 프로세스로 공식화했습니다. (1) 요약 및 (2) 요약에서 질문 생성. 첫 번째 단계에서는 아래와 같이 요약을 생성하기 위한 프롬프트를 설계합니다. {domain}에서 {title}의 맥락에서 아래 문단을 요약합니다. 다음 단계에서는 아래와 같이 ChatGPT에 요약에서 질문을 생성하도록 요청합니다. 아래의 맥락을 사용하여 후속 질문을 생각해 냅니다. 생성된 질문을 답변하기 위한 복잡성이 감소하는 순서로 순위를 매기고 상위 3개만 표시합니다. {context} 질문 생성 프로세스의 유용성을 보여주기 위해 동일한 지침으로 질문이 구절에서 직접 생성되는 기준선도 설정합니다. 기준선에 사용된 프롬프트는 다음과 같습니다. 아래의 맥락을 사용하여 세 개의 질문을 생각해 냅니다. 생성된 질문을 답변하기 어려운 순서대로 순위를 매기고 상위 3개만 표시합니다.{context} A.2 오픈 소스 LLM을 사용한 질문 생성 공정한 평가 설정을 만들기 위해 Alpaca-7B,13B 모델에 50개 인스턴스에 대한 질문을 요약하고 생성하도록 요청합니다. 지침에 맞춰지지 않은 모델(예: Llama)에서 생성된 질문은 고려하지 않습니다. 4절에서 설명한 생성된 질문에 대한 평가 방법을 통해 Alpaca에서 생성된 질문은 답변할 수 없고(맥락에 존재하지 않음) 80% 이상의 경우 횡설수설하는 내용이 포함되어 있음을 발견했습니다. 아래 표는 Alpaca에서 생성된 질문에 대한 평가를 보여줍니다. A.2.1 ChatGPT를 사용한 질문 생성 평가 4절에서 설명한 대로 생성된 질문의 복잡성을 확인하기 위해 다음 프롬프트로 ChatGPT를 요청합니다. 생성된 질문의 복잡성을 결정하는 것에 대한 피드백을 요청하고자 합니다. &#39; &#39; 평가 지표 Q1: 명확성 Q2. 문맥 길이: 문장 또는 단락 미만 최소한 구절의 1/3 또는 대부분 Q3: 구절의 다중 통과 QG - 구절 12.5% QG 요약 8.3% 98.8% 98.5% 1.2% 0% 1.5% 0% 표 4: 위에 표시된 맥락과 관련하여 AI 보조자가 Alpaca에서 생성한 질문의 복잡성을 평가하도록 설계된 프롬프트. \n\n 각 질문에 대해 모호성, 맥락 및 추론 능력의 차원에 대한 각 생성된 질문의 복잡성을 평가하세요. \n\n Q1: 질문에 주어진 맥락에서 답할 수 있고 모호하지 않습니까? A. 예 B. 아니요 \n\n Q2. 질문에 답하기 위해 구절의 얼마만큼이 맥락으로 필요합니까? A. 본문에서 한두 문장만 B. 두 문장 이상이지만 한 문단 미만 C. 주어진 전체 맥락의 최소 1/3 D. 주어진 대부분의 맥락 \n\n Q3: 이 질문은 본문을 여러 번 통과해야 합니까? A. 예 B. 아니요. 주어진 맥락 외에 주제에 대한 사전 지식이 없다고 가정합니다. 사전 형태로 선택 사항을 출력하세요. (예: &#39;Q1&#39;:<your answer choice for Q1> &#39;, &#39;Q2&#39;:<your answer choice for Q2> &#39;, &#39;Q3&#39;: &#39;<your answer choice for Q3> &#39;, &#39;Q4&#39;:<your answer choice for Q4> &#39;). \n\n 다음 줄에서 평가에 대한 포괄적인 설명을 제공하여 잠재적 편견을 피하고 응답이 제시된 순서가 판단에 영향을 미치지 않도록 하십시오. LLM은 서로 다른 교육 데이터 혼합으로 교육되므로, 우리는 ChatGPT에 주어진 맥락에 따라서만 질문에 답하도록 특별히 요청합니다. A.2.2 답변 생성을 위한 프롬프트 LLM에서 생성된 질문에 대한 답변을 생성하기 위해 다음을 프롬프트합니다. 생성된 모든 질문에 대해 다음과 같이 모델을 프롬프트합니다. 맥락을 고려하여 아래 질문에 답합니다. 맥락: {맥락} 질문: {질문} 답변: {답변} 지표 코헨-카파 점수 일관성 0.관련성 0.정확도 0.0.A.3 평가자로서의 GPT-4 A.3.1 GPT-4 평가에 사용되는 프롬프트 LLMS에서 생성된 답변을 평가하기 위해 GPT-4에 다음 네 가지 지표(일관성, 관련성, 정확성 및 사실적 일관성)에 대해 리커트 척도(0~3점, 높을수록 좋음)로 답변을 평가하도록 요청합니다. GPT-4의 프롬프트로 사용된 평가 프롬프트는 아래와 같습니다. 시스템 프롬프트: 귀하는 다음 세로형에서 답변의 질을 확인하는 데 유용하고 정확한 조수입니다. 일관성, 관련성, 사실적 일관성, 정확성. prompt : 위에 표시된 맥락을 기반으로 사용자 질문에 답변한 두 AI 어시스턴트의 성과에 대한 점수와 피드백을 요청하고자 합니다. 일관성, 관련성, 사실적 일관성 및 정확성의 4가지 지표에 대한 답변 품질을 평가해 주세요. 각 지표의 정의는 귀하에게 제공됩니다. 일관성 - 답변은 잘 구성되고 잘 정리되어야 합니다. 관련성 답변은 질문과 맥락과 관련이 있어야 합니다. 또한 답변은 묻는 질문에서 벗어나지 않아야 합니다. 사실적 일관성 맥락은 답변의 주요 출처여야 합니다. 답변에는 조작된 사실이 포함되어서는 안 되며 맥락에 있는 정보를 수반해야 합니다. 정확성 어시스턴트가 제공한 답변이 간결한 방식으로 질문에 올바르게 답변합니까? 위에 정의된 각 지표에 대한 각 AI 어시스턴트 응답에 0~3점 척도로 점수를 부여하세요. 점수가 높을수록 성과가 더 좋습니다. AI 어시스턴트 간의 성과를 비교하지 말고 개별적으로 평가하세요. 두 개의 새 줄을 입력하세요. 다음으로, 둘 중 선호하는 답변과 평가에 대한 포괄적인 설명을 제공하여 잠재적인 편견을 피하고 응답이 제시된 순서가 판단에 영향을 미치지 않도록 하십시오. 점수는 제공된 예와 같이 사전 형식이어야 합니다. 예: &#39;Assistant 1&#39;: &#39;coherence&#39;:<int score> , &#39;관련성&#39;:<int score> , &#39;사실적 일관성&#39;:<int score> , 정확성&#39;:<int score> , &#39;도우미 &#39;설명&#39;: &#39;<Preference: Assistant 1 or Assistant 2 or Both or Neither> .<your explanation> &#39;. 2&#39;: ... A.4 사례 연구: 인간 평가 대 GPT-우리는 GPT-4 등급과 무관하게 LLM이 생성한 답변의 질을 평가하기 위해 50개의 QA 쌍에 주석을 달았습니다. 두 명의 주석 작성자에게 섹션 4에 설명된 것과 유사한 지침과 GPT4(섹션 A.3.1)와 같은 설문지가 제공되었습니다. 공정한 평가를 위해 주석 작성자에게 답변을 생성한 모델을 공개하지 않았습니다. 표 5에는 GPT-4와 인간 평가의 일치 점수가 포함되어 있습니다. 자유형 텍스트 생성에 대한 다양한 지표에서 높은 일치 점수는 GPT-4를 사용한 평가 방법이 신뢰할 수 있음을 보여줍니다. 또한 부록(표 6)에 몇 가지 정성적 예를 제시합니다. A.5 데이터 수집 우리는 Wikipedia API를 사용하여 기사를 수집하여 평가 설정을 만듭니다. 그러나 모든 기사가 설정에 유용하지는 않을 수 있다는 것을 알게 되었습니다. 영어: 참조 링크, 주제 목록, 용어집 등이 포함된 Wikipedia 문서와 같은 경우 두 가지 수준의 필터링을 적용합니다.(1) 토큰이 2k 미만인 모든 문서를 제거합니다.(2) 정보가 없는 섹션 목록을 정리하고 이 연구에서 해당 섹션을 고려 대상에서 필터링합니다.[&#39;참조&#39;, &#39;참조&#39;, &#39;메모&#39;, &#39;출처&#39;, &#39;외부 링크&#39;, &#39;역사&#39;, &#39;추가 자료&#39;] A.6 정성적 예 아래 표는 인간 주석 작업의 정성적 예를 제시하고 인간과 GPT-4의 일부 평가를 보여줍니다.정성적 예 질문 1: 학제간 연구와 단일 학문 전문화의 차이점은 무엇입니까?평가: 문제 - 없음, 인간 점수 - 3, GPT-4 -ChatGPT: 학제간 연구는 여러 학문의 방법과 접근 방식을 한 주제에 적용하는 것을 포함하는 반면, 단일 학문 전문화는 단일 주제에 대한 심층 연구와 전문성을 포함합니다. 평가: 문제 - 반복, 인간 점수 - 2, GPT-4 -Alpaca-13B: 학제간 연구는 두 개 이상의 학문을 결합하여 새로운 연구 분야를 만드는 것입니다. 전문화는 단일 학문에 초점을 맞추고 해당 분야의 전문가가 되는 것을 목표로 합니다. 학제간 연구는 두 개 이상의 학문을 결합하여 새로운 연구 분야를 만드는 것입니다. 전문화는 단일 학문에 초점을 맞추고 해당 분야의 전문가가 되는 것을 목표로 합니다. 해당 분야에서. 질문 2: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 어떻게 필요합니까? 평가: 문제 - 없음, 인간 점수 - 3, GPT-4 -Alpaca-13B: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 필요합니다. 리더십은 전략적 우선순위를 설정하고 조직이 목표에 부합하도록 하는 데 필요합니다. 조직이 목표와 이를 달성하기 위한 전략을 인식하도록 하는 데 커뮤니케이션이 필요합니다. 조직이 목표를 달성하기 위해 함께 일하고 있는지 확인하는 데 협업이 필요합니다. 조직이 지속적으로 성과를 개선하기 위해 노력하고 있는지 확인하려면 지속적인 개선이 필요합니다.평가: 문제 - 정확성, 인간 점수 - 1, GPT-4 -Alpaca-7B: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 필요합니다.질문 3: CSAB가 컴퓨터 과학 분야에서 중요하다고 생각하는 네 가지 영역은 무엇입니까?평가: 문제 - 사실적 일관성, 인간 점수 - 1, GPT-4 -Alpaca-7B: 1. 계산 이론 2. 계산 가능성 이론 3. 프로그래밍 언어 이론 4. 형식적 방법 ChatGPT: CSAB가 컴퓨터 과학 분야에서 중요하다고 생각하는 네 가지 영역은 계산 이론, 알고리즘 및 데이터 구조, 프로그래밍 방법론 및 언어, 컴퓨터 요소 및 아키텍처입니다.표 6: GPT-4 및 인간 평가가 있는 LLM에서 생성된 답변의 몇 가지 예 범례 LLM, 모델에서 놓친 불일치 GPT-4에서 감지된 불일치.
