--- ABSTRACT ---
자기 회귀 대규모 언어 모델(LLM)은 다양한 자연어 생성 작업에서 놀라운 진전을 이루었습니다. 그러나 자기 회귀 토큰별 생성으로 인해 높은 계산 비용과 지연이 발생합니다. 이 문제를 해결하기 위해 조기 종료 전략을 사용하여 계산 비용을 줄이는 여러 가지 방법이 제안되었습니다. 이러한 전략을 사용하면 각 토큰에 전체 계산 그래프를 적용하지 않고도 감소된 계산을 사용하여 더 빠른 텍스트 생성이 가능합니다. 기존의 토큰 수준 조기 종료 방법은 온라인 추론에 유망한 결과를 보여주지만 일괄 추론 및 키-값 캐싱에는 쉽게 적용할 수 없습니다. 이는 일괄 처리의 마지막 토큰이 종료될 때까지 기다려야 계산을 중지할 수 있기 때문입니다. 이는 이러한 기술의 실제 적용을 심각하게 제한합니다. 이 논문에서는 일괄 추론 및 KV 캐싱과 원활하게 작동하도록 설계된 간단하고 효과적인 토큰 수준 조기 종료 방법인 SkipDecode를 제안합니다. 이는 각 시퀀스 위치에서 일괄 처리의 모든 토큰에 대해 단일 종료 지점을 설정하여 이전의 제약 조건을 극복합니다. 또한 종료 지점의 단조로운 감소를 보장하여 이전 토큰에 대한 KV 캐시를 다시 계산할 필요성을 제거합니다. 이전 작업에서처럼 계산을 조기에 종료하는 대신, 저희의 접근 방식은 하위 계층에서 중간 계층으로 우회하여 대부분의 계산 리소스를 상위 계층에 할당하여 이후 토큰이 이전 토큰의 계산 지출에서 이익을 얻을 수 있도록 합니다. 저희의 실험 결과에 따르면 SkipDecode는 다양한 작업에서 무시할 수 있는 회귀로 2배에서 5배의 추론 속도 향상을 얻을 수 있습니다. 이는 13억 개와 60억 개의 매개변수를 가진 OPT 모델을 사용하여 달성되며, 동시에 배칭 및 KV 캐싱 최적화 기술과 직접 호환됩니다. 1
--- INTRODUCTION ---
GPT[14] 및 OPT[23] 패밀리와 같은 자기 회귀 대규모 언어 모델(LLM)은 광범위한 작업에서 강력한 성능을 보였습니다[15, 16, 1]. 그러나 토큰별 생성으로 인해 높은 계산 비용과 대기 시간 요구 사항도 있습니다. 토큰 수준 조기 종료[17, 18]는 숨겨진 상태가 포화 상태에 도달하자마자 토큰이 계산을 중단할 수 있도록 하여 이러한 제한을 완화하는 유망한 기술로 등장했습니다[17]. 현재 방법론은 이론적 이점을 보여주지만 실제로 추론 속도를 높이기 위해 널리 사용되는 일괄 추론 및 KV 캐싱 기술과 호환되지 않기 때문에 실제 구현은 다소 제한적입니다. 이는 주로 배치에서 각 위치의 최종 토큰이 완전히 처리될 때까지 계산을 연장해야 하기 때문입니다. 사전 인쇄. 검토 중. 토큰당 손실토큰당 손실토큰 위치 토큰 위치 (a) OpenWebText의 OPT-350m. 평균 손실(b) Reddit-TLDR의 OPT-350m(미세 조정). 토큰 위치당 평균 손실은 토큰 위치당 강력한 단조적 손실을 보여줍니다. 감소 추세이지만 일반 텍스트의 경우 감소 추세입니다. 다른 함수가 있습니다. 그림 1: 일반 및 작업별 데이터 세트에서 OPT-350m 모델의 포워드 패스 중 토큰 위치당 평균 손실(검정색). 회색은 평균에 대한 95% 신뢰 구간을 나타냅니다. 이는 가장 계산량이 많은 토큰의 종료 위치에 대한 개선을 효과적으로 제한합니다. 또한 종료 지점을 정의하기 위해 학습된 분류기와 같은 동적 지표에 의존하는 토큰 수준 종료 전략은 전체 크기 네트워크에서 수행한 계산에 대한 최악의 비용과 같은 계산 비용에 대한 어떠한 보장도 제공하지 않습니다. 이전 토큰의 키-값(KV) 캐싱의 형태로 추가적인 실질적인 어려움이 발생하는데, 현재 토큰이 다른 토큰보다 늦게 종료되는 경우 업데이트가 필요합니다. 이 논문에서는 SkipDecode라는 새로운 토큰 수준 조기 종료 방법을 제시합니다. 이 방법은 이러한 제한을 극복하고 제어된 계산 예산을 유지합니다. 저희의 접근 방식은 특정 시퀀스 위치에서 배치 내의 모든 토큰에 대해 통합 종료 지점을 설정합니다. 저희는 또한 시퀀스의 끝 부분에 있는 단어가 일반적으로 더 많은 문맥 정보로 인해 예측하기 더 쉽다는 관찰을 활용합니다. 이를 통해 후속 토큰이 덜 계산적 노력을 요구한다는 가정 하에 시퀀스가 진행됨에 따라 단조롭게 감소하는 종료 지점으로 생성 정책을 설계할 수 있습니다. 그림 1은 토큰 위치당 감소하는 손실을 보여줍니다. 시퀀스 시작 부분의 예측은 나중에 나타나는 토큰과 대조적으로 더 높은 엔트로피를 등록합니다. 이는 실수를 최소화하고 오류의 연쇄를 방지하기 위해 사전에 증가된 계산적 노력을 사용하도록 동기를 부여하는 반면 시퀀스가 더 예측 가능해짐에 따라 나중에 계산 처리를 줄일 수 있습니다. 단조롭게 감소하는 종료 지점을 사용하는 저희의 전략은 이전 토큰에 대한 키-값(KV) 캐시를 다시 계산할 필요성을 없애 계산 비용을 크게 줄입니다. 개별 레이어에서 종료되는 토큰은 나중에 종료되는 이전 토큰에서 생성된 모든 정보의 이점을 얻을 수 없으므로 계산이 낭비되고 컨텍스트 정보가 손실됩니다. 이 문제를 해결하기 위해 SkipDecode의 조기 종료 메커니즘은 모든 토큰에서 수행되는 전체 계산을 활용하여 속도 향상 작업 성능 트레이드오프가 상당히 개선되었습니다. 계산을 갑자기 종료하는 대신, 저희의 접근 방식은 하위 레이어를 우회하고 주로 계산 예산을 상위 레이어에 할당하여 오른쪽 토큰이 왼쪽 토큰에서 사용하는 계산 리소스의 이점을 효과적으로 얻을 수 있도록 합니다. 저희의 기술 SkipDecode(그림 2 및 표 1의 개요)는 숨겨진 상태 포화 지점까지 성능 저하를 피할 수 있습니다. 저희는 3개의 벤치마크 생성 데이터 세트에서 13억 개와 67억 개의 OPT Transformer 모델에서 최대 5배의 속도 향상을 실험합니다. 또한 제어되고 예측 가능한 계산 예산을 유지하면서 배칭 및 KV 캐싱과 같은 실질적인 문제를 해결합니다. 우리의 방법은 리소스가 제한된 장치에서 LLM을 사용하기 쉽게 만들고 AI를 민주화하는 데 도움이 됩니다.방법 모델 생성 토큰 배칭 KVТуре 수준 전체 캐싱 주의 제어된 비용 CALM Enc-Dec SkipDecode Dec 전용 ✓ ✗ ✓ ✗ ☑ 표 1: CALM과 SkipDecode의 비교.SkipDecode는 제어된 계산 예산으로 추론 효율성을 높이기 위해 배칭 및 KV 캐싱을 지원합니다.2 토큰 수준 조기 종료 전략의 실제적 과제 해결 이 섹션에서는 LLM에서 토큰 수준 조기 종료 방법과 관련된 실제적 과제에 대해 논의하고 이러한 제한을 해결하기 위해 SkipDecode를 제시합니다.실용적 블로커(기존): ⚫ 배칭: 마지막 종료 토큰에 의해 정의된 계산 비용 ⚫ KV 캐싱: 다음 토큰이 이전 토큰보다 늦게 종료되는 경우 이전 토큰의 KV 값을 다시 계산해야 합니다. ⚫ 계산 효율성: 다음 토큰이 더 일찍 종료되면 이전 토큰의 전체 계산에 참여하지 않습니다.⚫ 비용 불확실성: 최악의 시나리오(예: 마지막 계층에서 잘못된 토큰 종료)는 전체 네트워크를 처리하는 것과 동일합니다.Cat 계층 n 솔루션(저희): • 배칭: 배치당 위치당 종료(열 기준). • KV 캐싱: 다음 열은 이전 열보다 일찍 종료되어야 합니다.왼쪽 토큰은 생성하기가 더 어렵습니다.⚫ 계산 효율성: 대부분의 계산 예산을 최상위 계층에 사용합니다.암묵적으로 이전 토큰의 전체 계산에 참여합니다.비용 불확실성: 정적 정책(놀라운 일 없음), 계산 비용은 미리 정의됩니다.표 아래<end> 레벨 Cat 언어는 LLM 표 아래에 있는 ΑΙ입니다.<end><end> 레이어 n 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 고양이는 테이블 아래에 있습니다.다음 고양이 레벨은 테이블 LLM이 있는 ΑΙ 아래의 언어입니다.(a) 조기 종료 (b) 건너뛰기 그림 2: 언어 생성을 위한 토큰 수준 조기 종료의 실질적인 차단기 극복.2.1 통합 종료 지점을 사용하여 일괄 처리 추론 최적화 일괄 처리 추론은 여러 입력 샘플을 동시에 처리하여 계산 효율성을 높이는 데 널리 사용됩니다.이 접근 방식은 GPU 및 TPU와 같은 하드웨어가 제공하는 병렬성을 활용하여 대기 시간을 줄이고 처리량을 개선합니다.그러나 기존 토큰 수준 조기 종료 전략[17]을 일괄 처리 추론에 적용할 때 주로 배치 내 토큰의 종료 지점이 다양하기 때문에 문제가 발생합니다.토큰이 다양한 레이어에서 종료되므로 각 배치 멤버와 각 위치의 최종 토큰이 처리될 때까지 계산을 지속해야 합니다.이렇게 하면 배치 크기가 1을 초과할 때 실현될 수 있는 이점이 감소하여 병렬 계산의 잠재적 이점이 훼손됩니다. 이를 해결하기 위해, 우리는 주어진 시퀀스 위치에서 배치의 모든 토큰에 대해 고정된 위치별 종료 지점을 지정하는 방법을 제안합니다. 이 전략은 특정 시퀀스 위치에서 배치된 모든 토큰의 처리가 동시에 완료되도록 보장합니다. 결과적으로, 평가 중에 관찰된 모든 이론적 이점이 사소하지 않은 배치 시나리오에 대한 생성 중에 완전히 실현되도록 보장합니다. B를 배치 크기, N을 시퀀스 길이라고 합니다. 우리는 모든 인스턴스에서 특정 위치의 토큰을 사용하여 열별로 배치를 구성합니다. ts를 위치 n의 시퀀스에 있는 토큰으로 간주할 때, 주어진 배치는 t(.),n의 토큰으로 구성됩니다. L(tb,n)을 토큰 tɩ,n이 종료되는 계층이라고 합니다. 우리는 Vn = [1, N], Vb1, b2 € [1, B], L(tb₁,n) = L(tb2,n)임을 보장합니다. 또한 토큰 생성을 위한 자기 회귀 디코딩은 열이 왼쪽에서 오른쪽으로 처리되어 위치 n의 토큰에 대한 계산이 이전 위치 n의 토큰을 처리하는 모든 네트워크 계산을 활용할 수 있도록 합니다.1. 2.2 KV 캐싱 및 단조 감소 종료 지점 키-값(KV) KV 캐싱은 Transformer 모델에서 어텐션 메커니즘을 효율적으로 실행하기 위한 중요한 최적화 기술입니다. 이전에 처리된 토큰에 대한 계산된 키와 값을 저장함으로써 모델은 후속 단계에서 동일한 컨텍스트에 주의를 기울일 때 중복 계산을 크게 줄일 수 있습니다. 이 최적화는 더 빠른 추론 시간으로 이어집니다. 그러나 토큰 수준의 조기 종료 전략을 활용할 때 시퀀스에서 토큰의 다른 종료 지점은 또 다른 과제를 제기합니다. 구체적으로 현재 토큰이 상위 계층에서 종료되는 경우 이전 토큰에 대한 키-값(KV) 캐시를 다시 계산해야 합니다. 이 필요한 재계산은 계산 작업 부하를 증가시키고 조기 종료 방법의 이점을 훼손합니다. 각 이전 토큰의 계산은 이후 토큰의 계산에 의해 제한되기 때문입니다. 우리가 제안하는 솔루션은 주어진 위치에서 일괄 처리의 모든 토큰에 통합 종료점을 할당하여 이러한 과제를 효과적으로 해결합니다. 일괄 처리된 종료점이 시퀀스가 진행됨에 따라 단조롭게 감소하도록 함으로써 이전 토큰이 현재 토큰만큼 최소한 계산을 수행했음을 보장하여 추가 계산의 필요성을 간단히 피할 수 있습니다. 그림 2의 오른쪽 플롯은 모든 계층이 아키텍처를 재계산하거나 변경하지 않고도 왼쪽 어텐션 계층에 주의를 기울일 수 있는 방법을 보여줍니다. 근본적인 이유는 시퀀스 시작 부분에서 다음 단어 예측이 제한된 컨텍스트로 인해 더 어렵고 따라서 이전 토큰이 계산 그래프에서 나중 종료점에서 이점을 얻을 수 있기 때문입니다. 이전 작업[17]에서는 노이즈나 섭동이 자기 회귀 생성으로 인해 오류가 연쇄적으로 발생하는 이전 토큰에서 발생할 때 전체 작업 성능에 더 큰 영향을 미친다는 것을 이미 보여주었습니다. 게다가 컨텍스트가 시퀀스와 함께 커짐에 따라 나중 토큰은 더 예측 가능해지고 컨텍스트가 더 많아져 숨겨진 상태가 더 빨리 포화됩니다(즉, 숨겨진 상태는 계층 간에 분산이 제한됨). 따라서 나중 토큰은 계산이 덜 필요하고 따라서 계산 리소스를 더 효율적으로 사용할 수 있습니다[8]. 그림 1에서 이러한 직관을 보여줍니다. 시퀀스에서 더 이른 토큰은 손실이 더 크고 나중에 나타나는 예측 가능한 토큰과 대조적으로 생성하기 어렵습니다. 2.3 일괄 종료 지점으로 계산 예산 제어 기존의 조기 종료 기술은 일반적으로 개별 토큰의 종료 지점을 학습합니다[17]. 그러나 이전 하위 섹션에서 언급한 제한 사항 외에도 계산 예산을 제어하는 것은 어려울 수 있습니다. 일반적으로 분류기는 토큰이 특정 계층에서 종료되어야 하는지 여부를 결정하는 데 사용되며, 최악의 경우 계산 예산 시나리오는 전체 네트워크를 사용하는 비용에 가깝습니다(예: 마지막 계층에 가까운 잘못된 종료 지점). 이 문제는 최대 및 최소 종료점(각 토큰이 통과해야 하는 최대 및 최소 레이어 수)을 미리 지정하여 해결합니다. 이는 활성 모델 매개변수 수를 통해 계산 비용을 제어합니다. 시퀀스 전체의 종료점은 토큰이 최대값을 초과하거나 최소 종료점 아래로 떨어지지 않도록 할당하여 총 계산 비용을 제한합니다. 또한 앞서 설명한 대로 시퀀스 전체의 종료점 할당은 단조롭게 감소해야 합니다. 즉, 첫 번째 토큰에 최대 종료점이 할당되고 마지막 토큰은 최대 길이 매개변수에 따라 최소 종료점이 할당됩니다. 미리 정의된 함수는 시퀀스의 토큰에 종료점을 점진적으로 지정합니다. 이 함수는 여러 형태를 채택할 수 있으며 모델의 추가 하이퍼파라미터 역할을 하여 계산 비용을 관리합니다. 평가 단계에서는 최소 및 최대 레이어 수로 제한된 선형 감소를 사용하여 테스트를 수행합니다. 다른 함수(예: 거듭제곱 법칙)를 사용하면 더 큰 가속이 발생할 수 있으며 향후 연구의 주제가 될 것입니다. 형식적으로, 하이퍼 매개변수인 sequence_length, min_exit_layer, max_exit_layer, num_decoder_layers 및 prompt_size가 있는 시퀀스와 네트워크를 고려합니다. 배열 token_idx를 다음과 같이 정의합니다. i &lt; prompt_size인 경우 token_idx[i] = {num_decodex_exit_layer + t; × min_exit_layer i ≥ prompt_size인 경우 ti i-prompt_size sequence_length-prompt_size* 종료 계층 대 시퀀스 위치 num_decoder_layers max_layer 종료 계층 min_layer 계산 예산 prompt_len 시퀀스 위치 max_len 그림 3: 시퀀스 위치에 대한 종료 계층의 선형 감소. 위의 설계에서 네트워크의 전체 계산 능력(즉, 모든 디코더 계층 사용)으로 프롬프트의 모든 토큰을 처리할 수 있습니다. 프롬프트 처리에는 생성이 포함되지 않으므로 배칭으로 효율적으로 수행할 수 있습니다. prompt_len + 1에서 자기회귀 토큰 생성을 시작하자마자, 계산 예산에 의해 미리 지정된 최대 및 최소 계층으로 제한된 모든 토큰에 작용하는 활성 계층의 수를 감소시키기 시작합니다.2.4 건너뛰기 대 조기 종료 조기 종료 기반 방법을 사용하면 토큰의 숨겨진 상태가 포화되면 토큰이 모델을 조기에 종료할 수 있습니다[17]. 그러나 토큰 수준의 조기 종료는 이전 토큰이 나중 토큰보다 일찍 종료될 때 생성 모델에 문제를 일으킬 수 있습니다.이 시나리오에서 나중 토큰은 어텐션 메커니즘을 통해 이전 토큰이 수행한 추가 계산의 이점을 얻을 수 없으며, 효과적으로 사용 가능한 컨텍스트를 활용하지 못합니다.이러한 제한을 극복하기 위해 조기 종료 대신 건너뛰기를 수행하는 것을 제안합니다.각 토큰의 계산 예산이 모델의 상위 계층에 할당되도록 합니다.이제 종료 지점에 관계없이 토큰은 모든 이전 토큰의 최상위 계층에 참석하여 모든 사용 가능한 컨텍스트에 효과적으로 참석할 수 있습니다.초기 임베딩 계층과 최상위 계층 간의 표현 격차를 메우기 위해 워밍업 계층의 개념을 도입합니다. 워밍업 레이어는 나머지 계산 예산을 소진하기 위해 최상위 y 레이어로 건너뛰기 전에 x개의 하위 레이어에서 수행되는 초기 계산을 나타냅니다.실험에서 이 접근 방식이 토큰 임베딩과 최상위 레이어의 숨겨진 상태 간의 거리를 효과적으로 줄이는 것을 관찰했습니다.실험적 평가에서 워밍업 레이어의 수는 모든 설정에서 가장 잘 작동하는 1개라는 것을 지속적으로 발견했습니다.3 평가 1.3b 및 6.7b 매개변수(각각 24개 및 32개 레이어)의 OPT[23] 디코더 전용 언어 모델을 사용하여 세 가지 벤치마크 텍스트 생성 데이터 세트인 E2E[13], Reddit-TLDR[21], CNN-DM[6]에서 기술을 시연합니다.metaseq 코드베이스¹를 사용하여 SkipDecode를 구현합니다.3.1 실험 설계 1배 속도가 향상된 사전 학습된 LLM(기본 모델)이 주어졌을 때, SkipDecode를 사용하여 추론 중에 네트워크가 수행하는 계산량을 줄이는 것이 목표입니다. 우리는 2×, 3×, 4× 및 5× 속도 향상에 해당하는 구성으로 방법을 평가합니다. 속도 향상은 배칭 및 KV 캐싱을 본질적으로 지원하는 기본 모델과 관련하여 보고됩니다. 이 속도 향상 비교는 배치 크기가 1이고 KV 캐싱이 없는 약한 기본 모델을 고려하는 이전의 조기 종료 작업과 다릅니다. 섹션 2.3에 설명된 대로 토큰당 최대 및 최소 계층의 다른 구성은 다른 양의 속도 향상을 달성할 수 있습니다. 우리는 e2e 데이터 세트에서 하이퍼 매개변수 튜닝을 통해 지정된 각 속도 향상에 대해 최대 및 최소 계층과 워밍업 계층 및 학습률의 최적 조합을 결정합니다. 우리는 검증 세트의 퍼플렉시티 메트릭을 기반으로 최적의 것을 선택합니다. 모델이 생성할 토큰 수를 미리 예측하는 것은 불가능하므로 실제 속도 향상은 생성 중에 약간 다를 수 있다는 점에 유의해야 합니다. 그러나 계산 예산은 각 데이터 세트의 최대 시퀀스 길이에 할당된 최소 계층에 의해 엄격하게 제한됩니다. 사용된 구성은 표 2에 나와 있습니다. 학습을 위해 모든 인스턴스에 대해 각 데이터 세트의 중간 학습 프롬프트 길이를 사용하여 모든 계층이 그림 3에 나와 있는 것처럼 원하는 생성 동작을 모방하도록 처리되도록 했습니다. 우리의 접근 방식은 효과적이면서도 간단하고 구현하기 쉽다는 점에 주목할 가치가 있습니다. 토큰 건너뛰기 정책 외에도 학습이나 생성 중에 변압기 아키텍처에 대한 추가 수정이 필요하지 않습니다. https://github.com/facebookresearch/metaseqOriginal / Base Number of Layers Target Speedup (x) #Target Avg #Warm up #Min #Max Layer Layer Layer 32 (6.7B)6.24 (1.3B)표 2: 가장 작은 복잡도에 해당하는 E2E 검증 세트를 사용하여 얻은 Base OPT(1.3B 및 6.7B)에 대한 다양한 대상 속도 향상에 대한 SkipDecode 구성. 3.2 데이터 세트 우리는 세 가지 벤치마크 데이터 세트에 대한 실험을 수행합니다. 각 데이터 세트에 대한 생성의 예는 표 3에 나와 있습니다. 생성을 위해 모든 경우에 빔 1, 탑 샘플링 0.7, 온도 0.3을 사용합니다. 학습을 위해 2e-4에서 8e-6 범위의 학습률을 스윕합니다. 데이터 세트 E2E Reddit-TLDR CNN-DM 컨텍스트 이름 [블루 스파이스], eatType[커피숍], 고객 평가 [평균], [버거킹] 근처 &quot;서브레드: r/relationships 제목: 내가 가볍게 사귀었던 이 남자 [18M]는 가을에 대학에 진학하기 때문에 나와 사귀고 싶어하지 않는다 [18F] POST: 여러분을 위한 약간의 맥락이 있습니다: 우리 둘 다 학교 연극 프로그램에서 신입생 때 만났습니다. 신입생 때 저는... (CNN) 테러 집단 알샤바브가 케냐 동부의 가리사 대학교를 공격했다고 주장하며, 이 공격으로 많은 사람이 사망하고 더 많은 사람이 인질로 잡혔습니다. 이 공격은 테러 집단의 활동이 계속 확대되는 데 있어 또 다른 단계이며, 동아프리카의 안보 상황이 빠르게 악화되고 있음을 분명히 보여줍니다. 소말리아에 본사를 둔 알샤바브는 케냐에서 최근 발생한 일련의 공격의 배후에 있으며, 가장 잘 알려진 사건 중 하나는 2013년 나이로비의 웨스트게이트 쇼핑센터에서 일어난 학살입니다. 그러나 이 단체가 케냐에 저지른 국경 간 습격은 2011년으로 거슬러 올라갑니다. 알샤바브의 침입으로 군사적... 응답 2x 버거킹 근처에 있는 블루 스파이스 커피숍은 고객들에게 평균 평가를 받았습니다. 제가 가볍게 사귀던 남자가 가을에 대학에 진학하고 저는 1년 더 고등학교에 다녀야 하기 때문에 저와 헤어지고 싶어합니다. 알샤바브가 케냐의 가리사 대학교를 공격했다고 주장합니다. 이 공격은 테러 집단의 활동이 계속 확대되는 과정에서 또 다른 단계입니다. 알샤바브는 최근 케냐에서 일어난 일련의 공격의 배후에 있습니다. 이 단체는 알카에다와 마찬가지로 코란에 대한 급진적인 해석에 의해 주로 움직입니다. 표 3: 데이터 세트 및 모델 응답의 스냅샷. 응답 5x 블루 스파이스는 버거킹 근처에 있는 커피숍입니다. 고객 평가가 평균이며 버거킹 근처에 있습니다. 내가 사귀고 있는 남자는 꽤 오랫동안 사귀고 있고, 그는 가을에 대학에 갈 예정이고, 나는 몹시 상심했고 어떻게 해야 할지 모르겠어.알샤바브가 케냐의 가리사 대학교를 공격했다고 주장했다.알샤바브는 케냐에서 일어난 일련의 최근 공격의 배후에 있다.알샤바브는 이 지역에서 일어난 일련의 최근 공격의 배후에 있다.E2E [13]. 작업은 키-값 쌍의 구조화된 정보를 유창한 텍스트로 변환하는 것이다. 비교적 작으며, 42061개의 훈련 샘플, 4672개의 평가 샘플, 4693개의 테스트 샘플로 구성되어 있다. 중간 프롬프트에는 38개의 토큰이 포함되어 있다. 훈련과 생성을 위해 최대 시퀀스 길이를 160으로, 최대 프롬프트 길이를 60으로 설정했다. 효과적인 배치 크기는 256이다. 650개의 워밍업 단계와 8개의 에포크로 프롬프트와 완료를 구분하기 위해 구분선을 사용한다. Reddit-TLDR [21]. 117,000개 샘플의 훈련 크기, 6450개 평가 크기, 6550개 샘플의 테스트 크기를 포함하는 요약 데이터 세트입니다. 중간값 훈련 프롬프트는 348개 토큰 길이입니다. 200개 워밍업 단계, 32개의 효과적인 훈련 배치, 3개의 에포크를 활용합니다. 최대 프롬프트 길이는 512개 토큰으로 설정되고 최대 시퀀스 길이는 1024개 토큰으로 설정됩니다. 컨텍스트와 완료 사이의 구분 기호는 &quot;\nTl;dr\n&quot;입니다. CNN Daily Mail[6]. 입력으로 주어진 기사에 대한 요약을 작성해야 합니다. 287,113개 샘플의 훈련 크기, 13,368개 평가 크기, 11,490개 샘플의 테스트 크기를 갖는 대규모 데이터 세트입니다. 중간값 훈련 프롬프트 길이는 788개 토큰입니다. 최대 시퀀스 길이를 2048로, 최대 프롬프트 길이를 1024로 설정했습니다. 워밍업 업데이트는 650으로 설정하고, 효과적인 배치 크기는 32이며, 2에포크 동안 훈련합니다. 컨텍스트와 완료 사이의 구분 기호는 &quot;\nTl;dr\n&quot;입니다. 3.3 주요 결과 결과는 표 4에 나와 있습니다. SkipDecode는 각 데이터 세트와 모델 크기에 대해 계산 효율성이 상당히 향상되었음을 보여줍니다. 그림 4에 나와 있듯이, 눈에 띄는 E2E: 다양한 크기에 대한 Rouge-L 대 속도 향상 Reddit: 다양한 크기에 대한 Rouge-L 대 속도 향상 80.1.3b 1.3b - 6.7b 6.7b 77.72.67.565.62.Reddit: 다양한 크기에 대한 Rouge-L 대 속도 향상 1.3b 6.7b 60.i 목표 속도 향상 (a) E2E 목표 속도 향상 (b) Reddit 목표 속도 향상 (c) CNN DM 그림 4: 1.3B 및 6.7B OPT 모델에 대한 Rouge-L 대 추론 속도 향상. 속도 향상은 배칭을 기준으로 하지 않는 약한 기본 모델을 고려한 이전 작업과 달리 배칭 및 KV 캐싱을 본질적으로 지원하는 기본 모델(1×)에 대해 계산됩니다. 1×(기본 모델)에서 2× 속도 향상으로 성능 저하가 발생한 후 속도가 증가함에 따라 성능이 꾸준히 감소합니다. 이는 토큰이 숨겨진 상태 포화점에 도달하여 더 이상의 계산 감소로 인해 성능이 저하되기 때문이라고 가정합니다. 이 패턴은 데이터 세트 전체에서 일관됩니다. E2E에서 지연된 저하가 나타나는 반면 CNN-DM은 상대적인 난이도로 인해 더 빨리 저하되기 시작합니다. E2E SkipDecode 목표 속도 향상이 1×에서 5×로 증가함에 따라 생성 프로세스에서 활성화된 디코더 계층의 평균 수가 감소하여 계산 부하가 감소함을 나타냅니다. 흥미롭게도 Bleu, Rouge-L 및 Bert-F 점수에 해당하는 모든 작업 측정값은 목표 속도 향상이 증가함에 따라 약간 감소하면서 비교적 안정적으로 유지됩니다. 이는 우리 방법이 특정 작업 설정에 대해 최소한의 저하로 상당한 속도 향상을 달성할 수 있음을 나타냅니다. Reddit 다른 것과 마찬가지로 평균 생성 계층은 목표 속도 향상이 증가함에 따라 감소합니다. 그러나 Bleu, Rouge-L 및 Bert-F 점수와 같은 성능 지표는 이 작업의 상대적인 난이도를 감안할 때 E2E 데이터 세트에 비해 더 상당한 감소를 보입니다. 영어: 저희 방법이 여전히 상당한 속도 향상을 달성하더라도 작업 성능 측면에서의 트레이드오프는 더 눈에 띕니다.CNN-DM 결과는 이전과 유사한 추세를 따릅니다.목표 속도 향상이 증가함에 따라 평균 생성 계층이 감소하여 계산 요구 사항이 감소함을 나타냅니다.그러나 Bleu, Rouge-L 및 Bert-F 점수와 같은 성능 메트릭은 목표 속도 향상이 증가함에 따라 더 크게 떨어집니다.저희 접근 방식은 상당한 속도 향상을 달성할 수 있지만 숨겨진 상태 포화에 더 일찍 도달함에 따라 작업 성능의 트레이드오프가 더 두드러집니다.결론적으로 저희 방법은 모든 데이터 세트와 모델 크기에서 계산 요구 사항을 줄이는 능력을 지속적으로 보여주어 숨겨진 상태 포화 지점을 효과적으로 결정합니다.Bleu, Rouge-L 및 Bert-F 점수로 측정한 작업 성능에 대한 영향은 특정 데이터 세트에 따라 다릅니다.그러나 모든 경우에서 저희 방법은 속도 향상과 작업 성능 간에 유리한 균형을 보여 모든 경우에서 거의 저하 없이 2배의 속도 향상에 도달합니다. 이러한 균형은 우리의 접근 방식이 배칭 및 KV 캐싱과 같은 실질적인 과제를 능숙하게 처리하면서도 제어되고 예측 가능한 계산 예산을 유지하기 때문에 효과적으로 활용할 수 있습니다.3.4 다른 방법과의 비교 SkipDecode를 벤치마킹하기 위해 디코더 전용 모델에서 작동하도록 CALM 프레임워크의 두 가지 개념을 채택했지만 현재 SkipDecode와 직접 대응하는 방법은 없습니다.두 경우 모두 OPT 1.3b를 기본 모델로 사용합니다.먼저 [17]에 설명된 방법에 따라 다중 계층 종료 네트워크를 훈련합니다.여기서 단일 모델 헤드가 각 계층에서 종료되도록 훈련됩니다.이 접근 방식은 고정 정책으로 작동하여 시퀀스 내의 모든 토큰에 적용되는 미리 결정된 종료 계층까지 실행되므로 잘림이 있는 조기 종료 방법에 더 가깝습니다.특히 이 모델은 배칭과 KV 캐싱을 지원합니다.두 번째 방법은 동일한 모델을 사용하지만 CALM의 숨겨진 상태 포화 개념을 추가로 적용하여 디코더 전용 네트워크(CALM-DEC라고 함)에서 작동하도록 조정했습니다. 그러나 이 네트워크는 배치 크기에 제한을 두어 배치 및 KV 캐싱을 배제합니다. 결과적으로 모델은 필요에 따라 이전 토큰에 대한 모든 KV 값을 &#39;백필&#39;해야 합니다. 데이터 세트 크기 대상 속도 향상 #대상 평균 #세대 평균 블루 루즈-L 베르트-F 계층 계층 E2E 1.3b 6.7b 레드잇 1.3b 6.7b CNN-DM 1.3b 6.7b 2286525-03 22865 85-8622005 24-65.67.70.14.66.67.67.9.66.68.67.6.65.66.66.5.64.66.65.64.66.70.20.65.68.67 .65.68.67.9.66.67.67.7.64.65.65.9.27.31.15.8.27.32.9.7.25.22.6.3.21.11.5.3.19. 7.9.28.33.19.9.27.32.13.8.26.25.9.5.21.9.6.4.19.7.15.29.35.15.15.28.34.8.7.23.20.6.3.18.2.5.4.18.2.16.30.37.21.15.29.35.11.4.21.17.8.5.20.7.6.6.4.18.2.표 4: 다양한 속도 향상 및 기본 모델 크기에 대한 다양한 데이터 세트의 SkipDecode 성능(이 경우 해당 계층에서 마지막으로 알려진 숨겨진 상태를 투영하여)은 상당한 시스템 오버헤드를 추가합니다. 이 접근 방식의 최악의 경우 계산 비용은 전체 네트워크 비용과 동일합니다. 이 네트워크의 적응형 은닉 상태 포화 정책은 배칭과 계산/시간 추정 모두에 대해 고정되지 않은 정책의 표준적인 단점이 있습니다.또한 다음과 같은 이유로 특히 대형 디코더 전용 모델에서 속도가 빨라질수록 성능이 크게 저하됩니다.KV 백필은 이러한 작업에 매우 중요한 프롬프트 인코딩에 영향을 미칩니다.CALM[17]의 T5 모델과 같은 인코더-디코더 아키텍처에서 KV 백필은 프롬프트 인코딩을 유지합니다.디코더 전용 아키텍처는 과거 상태를 동시에 인코딩하고 디코딩하는 반면, 조기 종료는 네트워크가 이전 컨텍스트를 이해하는 데 영향을 미칠 가능성이 더 큽니다(부록 참조).이로 인해 CALM 구현은 원래 T5 인코더-디코더 구현과 달리 디코더 전용 모델에서 상당한 저하를 보입니다. Speedup E2E Reddit-TLDR SkipDecode 다중 계층 CALM-DEC SkipDecode 다중 계층67.68.68.27.26.67.65.35.27.17.68.61.32.25.12.66.50.27.21.7.66.46.22.19.6.표 5: SkipDecode, 다중 계층 및 CALM-DEC 간의 성능 비교.표 5에서 볼 수 있듯이 SkipDecode는 다른 접근 방식보다 우수한 성능을 보입니다. 이는 속도 향상 계수가 증가함에 따라 두 데이터 세트에서 작업 성능 저하가 현저히 적다는 사실로 입증됩니다. 이는 속도 향상 증가에 대한 우리 방법의 견고성을 보여줍니다.4
--- RELATED WORK ---
모델 압축: 대규모 언어 모델(LLM)의 추론 효율성을 개선하는 기술을 개발하기 위해 모델 압축에 대한 광범위한 연구가 있었습니다. 가장 두드러진 작업 라인 중 하나는 지식 증류(KD)[7]를 활용하여 숨겨진 상태 및 주의 상태와 같은 교사로서의 LLM의 표현을 사용하여 더 빠른 추론으로 더 작은 학생 모델을 훈련합니다[10, 19]. 모델 압축의 또 다른 작업 라인은 양자화[3], 저정밀도 훈련 및 네트워크 가지치기[5], 매개변수 공유 및 인수분해를 사용하여 메모리 풋프린트와 네트워크 지연 시간을 줄입니다[4, 20]. 주목할 점은 위의 모델 압축 연구 대부분이 자연어 이해 작업을 위한 인코더 전용 모델에 초점을 맞추었다는 것입니다. 조기 종료: 모든 입력에 대해 동일한 계산이 호출되는 정적 계산을 사용하는 위의 작업과 달리, 우리는 입력의 다른 부분에 대해 가변 계산을 사용하는 적응형 계산에 중점을 둡니다. 기존의 적응형 계산 기술은 주로 입력의 토큰이 네트워크의 다른 계층에서 종료하는 방법을 학습하는 조기 종료 전략[25, 24, 22, 12, 11, 9]에 의존합니다.KD의 작업과 유사하게 이러한 기술의 대부분은 자연어 이해(NLU) 작업을 위한 BERT[2]와 같은 인코더 전용 모델용으로 개발되었습니다.시퀀스 전체를 처리해야 하는 NLU 작업과 달리 생성 작업은 토큰별 생성에 대한 자기회귀적 특성으로 인해 더 복잡합니다.최근 작업인 CALM[17]은 사용할 신뢰 측정 기준, 시퀀스 수준 제약 조건을 로컬 토큰별 종료 결정에 연결하는 방법, 이전 토큰의 조기 종료로 인해 누락된 숨겨진 표현을 다시 처리하는 방법 측면에서 생성 작업에 대한 토큰 수준 조기 종료 전략을 연구합니다. 그러나 이전의 모든 조기 종료 작업과 마찬가지로 CALM은 실제로 추론 속도를 높이기 위해 널리 사용되는 배칭(배치 크기 1만 지원) 및 KV 캐싱과 관련된 몇 가지 주요 실질적 차단 요인으로 어려움을 겪습니다.또한 이러한 모든 종료 전략에 대한 최악의 시나리오(예: 모든 토큰의 최상위 계층에 더 가까운 종료 지점)는 전체 네트워크를 사용하여 예측할 수 없는 시스템 부하와 일관되지 않은 처리량을 초래할 수 있습니다.이러한 과제를 해결하기 위해 효율적인 추론을 위한 비사소한 배칭 및 KV 캐싱을 지원하고 놀라움 없이 예측 가능한 계산 부하를 보장하는 SkipDecode를 개발했습니다.5 제한 사항 및 향후 방향 SkipDecode는 배칭 및 키-값(KV) 캐싱이 기존 토큰 수준 조기 종료 전략과 본질적으로 호환되지 않는다는 핵심 문제를 해결합니다.그러나 감소 정책의 도입으로 새로운 제한 사항이 추가되었습니다.생성이 진행되고 배치의 샘플이 계산을 마치면 현재 위치가 나머지 요소의 위치와 일치하는 경우에만 새 샘플을 배치에 포함할 수 있습니다. 따라서 우리의
--- METHOD ---
s는 온라인 추론에 유망한 결과를 보여주지만, 배치 추론 및 키-값 캐싱에 쉽게 적용할 수 없습니다. 이는 배치의 마지막 토큰이 종료될 때까지 기다려야 컴퓨팅을 중지할 수 있기 때문입니다. 이는 이러한 기술의 실제 적용을 심각하게 제한합니다. 이 논문에서는 배치 추론 및 KV 캐싱과 원활하게 작동하도록 설계된 간단하고 효과적인 토큰 수준 조기 종료 방법인 SkipDecode를 제안합니다. 이는 각 시퀀스 위치에서 배치의 모든 토큰에 대해 단일 종료 지점을 설정하여 이전 제약 조건을 극복합니다. 또한 종료 지점의 단조로운 감소를 보장하여 이전 토큰에 대한 KV 캐시를 다시 계산할 필요성을 제거합니다. 이전 작업에서처럼 조기에 계산을 종료하는 대신, 우리의 접근 방식은 하위 계층에서 중간 계층으로 우회하여 대부분의 계산 리소스를 상위 계층에 할당하여 이후 토큰이 이전 토큰의 계산 지출로부터 이익을 얻을 수 있도록 합니다. 우리의
--- EXPERIMENT ---
모든 결과에 따르면 SkipDecode는 다양한 작업에서 무시할 만한 회귀로 2~5배의 추론 속도 향상을 얻을 수 있습니다. 이것은 13억 개와 60억 개의 매개변수를 갖는 OPT 모델을 사용하여 달성되며, 배칭 및 KV 캐싱 최적화 기술과 직접 호환됩니다. 1 서론 GPT[14] 및 OPT[23] 패밀리와 같은 자기 회귀 대규모 언어 모델(LLM)은 광범위한 작업에서 강력한 성능을 보였습니다[15, 16, 1]. 그러나 토큰별 생성으로 인해 높은 계산 비용과 대기 시간 요구 사항도 있습니다. 토큰 수준 조기 종료[17, 18]는 토큰의 숨겨진 상태가 포화 상태에 도달하자마자 계산을 중단할 수 있도록 하여 이러한 제한을 완화하는 유망한 기술로 등장했습니다[17]. 현재 방법론은 이론적 이점을 보여주지만, 실제로 추론 속도를 높이기 위해 널리 사용되는 배치 추론 및 KV 캐싱 기술과 호환되지 않기 때문에 실제 구현은 다소 제한적입니다. 이는 주로 각 위치에 대한 배치의 최종 토큰이 완전히 처리될 때까지 계산을 연장해야 하기 때문입니다. 사전 인쇄. 검토 중. 토큰당 손실토큰당 손실토큰 위치 토큰 위치 (a) OpenWebText의 OPT-350m. 평균 손실 (b) Reddit-TLDR의 OPT-350m(미세 조정). 토큰 위치당 평균은 토큰 위치당 강력한 단조적 손실을 보여줍니다. 감소 추세이지만 일반 텍스트의 경우 감소 추세입니다. 다른 함수로. 그림 1: 일반 및 작업별 데이터 세트에서 OPT-350m 모델의 전방 패스 중 토큰 위치당 평균 손실(검정색). 회색은 평균에 대한 95% 신뢰 구간을 나타냅니다. 이는 계산량이 가장 많은 토큰의 종료 위치에 대한 개선을 효과적으로 제한합니다. 또한 종료 지점을 정의하기 위해 학습된 분류기와 같은 동적 지표에 의존하는 토큰 수준 종료 전략은 전체 크기 네트워크에서 수행한 계산에 대한 최악의 비용과 같은 계산 비용에 대한 어떠한 보장도 제공하지 않습니다. 이전 토큰의 키-값(KV) 캐싱의 형태로 추가적인 실질적인 어려움이 발생하는데, 현재 토큰이 다른 토큰보다 늦게 종료되는 경우 업데이트가 필요합니다. 이 논문에서는 제어된 계산 예산을 유지하면서 이러한 제한을 극복하는 SkipDecode라는 새로운 토큰 수준 조기 종료 방법을 제시합니다. 저희의 접근 방식은 특정 시퀀스 위치에서 배치 내의 모든 토큰에 대한 통합 종료 지점을 설정합니다. 저희는 시퀀스의 끝 부분에 있는 단어가 일반적으로 더 맥락적인 정보로 인해 예측하기 더 쉽다는 관찰을 더욱 활용합니다. 이를 통해 후속 토큰이 덜 계산적 노력을 요구한다는 가정 하에 시퀀스가 진행됨에 따라 단조롭게 감소하는 종료 지점으로 생성 정책을 설계할 수 있습니다. 그림 1은 토큰 위치당 감소하는 손실을 보여줍니다. 시퀀스 시작 부분의 예측은 나중에 나타나는 토큰과 대조적으로 더 높은 엔트로피를 등록합니다. 이는 실수를 최소화하고 오류의 연쇄를 방지하기 위해 사전에 증가된 계산 노력을 사용하도록 동기를 부여하는 반면, 시퀀스가 더 예측 가능해짐에 따라 나중에 계산 처리를 줄일 수 있습니다. 단조롭게 감소하는 종료 지점을 사용하는 전략은 이전 토큰에 대한 키-값(KV) 캐시를 다시 계산할 필요성을 없애 계산 비용을 크게 줄입니다. 별도의 계층에서 종료되는 토큰은 나중에 종료되는 이전 토큰에서 생성된 모든 정보의 이점을 얻을 수 없으므로 계산이 낭비되고 컨텍스트 정보가 손실됩니다. 이 문제를 해결하기 위해 SkipDecode의 조기 종료 메커니즘은 모든 토큰에서 수행되는 전체 계산을 활용하여 속도 향상 작업 성능 트레이드오프를 상당히 개선합니다. 계산을 갑자기 종료하는 대신, 저희의 접근 방식은 하위 계층을 우회하고 주로 상위 계층에 계산 예산을 할당하여 오른쪽 토큰이 왼쪽 토큰에서 사용하는 계산 리소스의 이점을 효과적으로 얻을 수 있도록 합니다. 우리의 기술 SkipDecode(그림 2 및 표 1의 개요)는 숨겨진 상태 포화 지점까지 성능 저하를 피할 수 있습니다. 우리는 3개의 벤치마크 생성 데이터 세트에서 13억 개와 67억 개의 OPT Transformer 모델에 대해 최대 5배의 속도 향상을 실험합니다. 또한 제어되고 예측 가능한 계산 예산을 유지하면서 배칭 및 KV 캐싱과 같은 실질적인 문제를 해결합니다. 우리의 방법은 리소스가 제한된 장치에서 LLM을 사용하기 쉽게 만들고 AI를 민주화하는 데 도움이 됩니다. 방법 모델 생성 토큰 배칭 KVТуре 수준 전체 캐싱 주의 제어된 Comp. 비용 CALM Enc-Dec SkipDecode Dec 전용 ✓ ✗ ✓ ✗ ☑ 표 1: CALM과 SkipDecode 비교. SkipDecode는 제어된 계산 예산으로 추론 효율성을 높이기 위해 배칭 및 KV 캐싱을 지원합니다. 2 토큰 수준 조기 종료 전략의 실제적 과제 해결 이 섹션에서는 LLM에서 토큰 수준 조기 종료 방법과 관련된 실제적 과제를 논의하고 이러한 제한 사항을 해결하기 위한 SkipDecode를 제시합니다.실용적 블로커(기존): ⚫ 배칭: 마지막 종료 토큰에 의해 정의된 계산 비용 ⚫ KV 캐싱: 다음 토큰이 이전 토큰보다 늦게 종료되는 경우 이전 토큰에 대한 KV 값을 다시 계산해야 합니다.⚫ 계산 효율성: 다음 토큰이 더 일찍 종료되는 경우 이전 토큰의 전체 계산을 수행하지 않습니다.⚫ 비용 불확실성: 최악의 시나리오(예: 마지막 계층에서 잘못된 토큰 종료)는 전체 네트워크를 처리하는 것과 동일합니다.Cat 계층 n 솔루션(저희): • 배칭: 배치당 위치당 종료(열 기준).• KV 캐싱: 다음 열은 이전 열보다 일찍 종료되어야 합니다.왼쪽으로 향하는 토큰은 생성하기가 더 어렵습니다. ⚫ 계산 효율성: 계산 예산의 대부분을 상위 계층에 사용합니다. 이전 토큰의 전체 계산을 암묵적으로 처리합니다. 비용 불확실성: 정적 정책(놀라움 없음), 계산 비용은 미리 정의됩니다. 표 아래<end> 레벨 Cat 언어는 LLM 표 아래에 있는 ΑΙ입니다.<end><end> 레이어 n 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 레이어 고양이는 테이블 아래에 있습니다.다음 고양이 레벨은 테이블 LLM이 있는 ΑΙ 아래의 언어입니다.(a) 조기 종료 (b) 건너뛰기 그림 2: 언어 생성을 위한 토큰 수준 조기 종료의 실질적인 차단기 극복.2.1 통합 종료 지점을 사용하여 일괄 처리 추론 최적화 일괄 처리 추론은 여러 입력 샘플을 동시에 처리하여 계산 효율성을 높이는 데 널리 사용됩니다.이 접근 방식은 GPU 및 TPU와 같은 하드웨어가 제공하는 병렬성을 활용하여 대기 시간을 줄이고 처리량을 개선합니다.그러나 기존 토큰 수준 조기 종료 전략[17]을 일괄 처리 추론에 적용할 때 주로 배치 내 토큰의 종료 지점이 다양하기 때문에 문제가 발생합니다.토큰이 다양한 레이어에서 종료되므로 각 배치 멤버와 각 위치의 최종 토큰이 처리될 때까지 계산을 지속해야 합니다.이렇게 하면 배치 크기가 1을 초과할 때 실현될 수 있는 이점이 감소하여 병렬 계산의 잠재적 이점이 훼손됩니다. 이를 해결하기 위해, 우리는 주어진 시퀀스 위치에서 배치의 모든 토큰에 대해 고정된 위치별 종료 지점을 지정하는 방법을 제안합니다. 이 전략은 특정 시퀀스 위치에서 배치된 모든 토큰의 처리가 동시에 완료되도록 보장합니다. 결과적으로, 평가 중에 관찰된 모든 이론적 이점이 사소하지 않은 배치 시나리오에 대한 생성 중에 완전히 실현되도록 보장합니다. B를 배치 크기, N을 시퀀스 길이라고 합니다. 우리는 모든 인스턴스에서 특정 위치의 토큰을 사용하여 열별로 배치를 구성합니다. ts를 위치 n의 시퀀스에 있는 토큰으로 간주할 때, 주어진 배치는 t(.),n의 토큰으로 구성됩니다. L(tb,n)을 토큰 tɩ,n이 종료되는 계층이라고 합니다. 우리는 Vn = [1, N], Vb1, b2 € [1, B], L(tb₁,n) = L(tb2,n)임을 보장합니다. 또한 토큰 생성을 위한 자기 회귀 디코딩은 열이 왼쪽에서 오른쪽으로 처리되어 위치 n의 토큰에 대한 계산이 이전 위치 n의 토큰을 처리하는 모든 네트워크 계산을 활용할 수 있도록 합니다.1. 2.2 KV 캐싱 및 단조 감소 종료 지점 키-값(KV) KV 캐싱은 Transformer 모델에서 어텐션 메커니즘을 효율적으로 실행하기 위한 중요한 최적화 기술입니다. 이전에 처리된 토큰에 대한 계산된 키와 값을 저장함으로써 모델은 후속 단계에서 동일한 컨텍스트에 주의를 기울일 때 중복 계산을 크게 줄일 수 있습니다. 이 최적화는 더 빠른 추론 시간으로 이어집니다. 그러나 토큰 수준의 조기 종료 전략을 활용할 때 시퀀스에서 토큰의 다른 종료 지점은 또 다른 과제를 제기합니다. 구체적으로 현재 토큰이 상위 계층에서 종료되는 경우 이전 토큰에 대한 키-값(KV) 캐시를 다시 계산해야 합니다. 이 필요한 재계산은 계산 작업 부하를 증가시키고 조기 종료 방법의 이점을 훼손합니다. 각 이전 토큰의 계산은 이후 토큰의 계산에 의해 제한되기 때문입니다. 우리가 제안하는 솔루션은 주어진 위치에서 일괄 처리의 모든 토큰에 통합 종료점을 할당하여 이러한 과제를 효과적으로 해결합니다. 일괄 처리된 종료점이 시퀀스가 진행됨에 따라 단조롭게 감소하도록 함으로써 이전 토큰이 현재 토큰만큼 최소한 계산을 수행했음을 보장하여 추가 계산의 필요성을 간단히 피할 수 있습니다. 그림 2의 오른쪽 플롯은 모든 계층이 아키텍처를 재계산하거나 변경하지 않고도 왼쪽 어텐션 계층에 주의를 기울일 수 있는 방법을 보여줍니다. 근본적인 이유는 시퀀스 시작 부분에서 다음 단어 예측이 제한된 컨텍스트로 인해 더 어렵고 따라서 이전 토큰이 계산 그래프에서 나중 종료점에서 이점을 얻을 수 있기 때문입니다. 이전 작업[17]에서는 노이즈나 섭동이 자기 회귀 생성으로 인해 오류가 연쇄적으로 발생하는 이전 토큰에서 발생할 때 전체 작업 성능에 더 큰 영향을 미친다는 것을 이미 보여주었습니다. 게다가 컨텍스트가 시퀀스와 함께 커짐에 따라 나중 토큰은 더 예측 가능해지고 컨텍스트가 더 많아져 숨겨진 상태가 더 빨리 포화됩니다(즉, 숨겨진 상태는 계층 간에 분산이 제한됨). 따라서 나중 토큰은 계산이 덜 필요하고 따라서 계산 리소스를 더 효율적으로 사용할 수 있습니다[8]. 그림 1에서 이러한 직관을 보여줍니다. 시퀀스에서 더 이른 토큰은 손실이 더 크고 나중에 나타나는 예측 가능한 토큰과 대조적으로 생성하기 더 어렵습니다. 2.3 일괄 종료 지점으로 계산 예산 제어 기존의 조기 종료 기술은 일반적으로 개별 토큰의 종료 지점을 학습합니다[17]. 그러나 이전 하위 섹션에서 언급한 제한 사항 외에도 계산 예산을 제어하는 것은 어려울 수 있습니다. 일반적으로 분류기는 토큰이 특정 계층에서 종료되어야 하는지 여부를 결정하는 데 사용되며, 최악의 경우 계산 예산 시나리오는 전체 네트워크를 사용하는 비용에 가깝습니다(예: 마지막 계층에 가까운 잘못된 종료 지점). 이 문제는 최대 및 최소 종료점(각 토큰이 통과해야 하는 최대 및 최소 레이어 수)을 미리 지정하여 해결합니다. 이는 활성 모델 매개변수 수를 통해 계산 비용을 제어합니다. 시퀀스 전체의 종료점은 토큰이 최대값을 초과하거나 최소 종료점 아래로 떨어지지 않도록 할당하여 총 계산 비용을 제한합니다. 또한 앞서 설명한 대로 시퀀스 전체의 종료점 할당은 단조롭게 감소해야 합니다. 즉, 첫 번째 토큰에 최대 종료점이 할당되고 마지막 토큰은 최대 길이 매개변수에 따라 최소 종료점이 할당됩니다. 미리 정의된 함수는 시퀀스의 토큰에 종료점을 점진적으로 지정합니다. 이 함수는 여러 형태를 채택할 수 있으며 모델의 추가 하이퍼파라미터 역할을 하여 계산 비용을 관리합니다. 평가 단계에서는 최소 및 최대 레이어 수로 제한된 선형 감소를 사용하여 테스트를 수행합니다. 다른 함수(예: 거듭제곱 법칙)를 사용하면 더 큰 가속이 발생할 수 있으며 향후 연구의 주제가 될 것입니다. 형식적으로, 하이퍼 매개변수인 sequence_length, min_exit_layer, max_exit_layer, num_decoder_layers 및 prompt_size가 있는 시퀀스와 네트워크를 고려합니다. 배열 token_idx를 다음과 같이 정의합니다. i &lt; prompt_size인 경우 token_idx[i] = {num_decodex_exit_layer + t; × min_exit_layer i ≥ prompt_size인 경우 ti i-prompt_size sequence_length-prompt_size* 종료 계층 대 시퀀스 위치 num_decoder_layers max_layer 종료 계층 min_layer 계산 예산 prompt_len 시퀀스 위치 max_len 그림 3: 시퀀스 위치에 대한 종료 계층의 선형 감소. 위의 설계에서 네트워크의 전체 계산 능력(즉, 모든 디코더 계층 사용)으로 프롬프트의 모든 토큰을 처리할 수 있습니다. 프롬프트 처리에는 생성이 포함되지 않으므로 배칭으로 효율적으로 수행할 수 있습니다. prompt_len + 1에서 자기회귀 토큰 생성을 시작하자마자, 계산 예산에 의해 미리 지정된 최대 및 최소 계층으로 제한된 모든 토큰에 작용하는 활성 계층의 수를 감소시키기 시작합니다.2.4 건너뛰기 대 조기 종료 조기 종료 기반 방법을 사용하면 토큰의 숨겨진 상태가 포화되면 토큰이 모델을 조기에 종료할 수 있습니다[17]. 그러나 토큰 수준의 조기 종료는 이전 토큰이 나중 토큰보다 일찍 종료될 때 생성 모델에 문제를 일으킬 수 있습니다.이 시나리오에서 나중 토큰은 어텐션 메커니즘을 통해 이전 토큰이 수행한 추가 계산의 이점을 얻을 수 없으며, 효과적으로 사용 가능한 컨텍스트를 활용하지 못합니다.이러한 제한을 극복하기 위해 조기 종료 대신 건너뛰기를 수행하는 것을 제안합니다.각 토큰의 계산 예산이 모델의 상위 계층에 할당되도록 합니다.이제 종료 지점에 관계없이 토큰은 모든 이전 토큰의 최상위 계층에 참석하여 모든 사용 가능한 컨텍스트에 효과적으로 참석할 수 있습니다.초기 임베딩 계층과 최상위 계층 간의 표현 격차를 메우기 위해 워밍업 계층의 개념을 도입합니다. 워밍업 레이어는 나머지 계산 예산을 소진하기 위해 최상위 y 레이어로 건너뛰기 전에 x개의 하위 레이어에서 수행되는 초기 계산을 나타냅니다.실험에서 이 접근 방식이 토큰 임베딩과 최상위 레이어의 숨겨진 상태 간의 거리를 효과적으로 줄이는 것을 관찰했습니다.실험적 평가에서 워밍업 레이어의 수는 모든 설정에서 가장 잘 작동하는 1개라는 것을 지속적으로 발견했습니다.3 평가 1.3b 및 6.7b 매개변수(각각 24개 및 32개 레이어)의 OPT[23] 디코더 전용 언어 모델을 사용하여 세 가지 벤치마크 텍스트 생성 데이터 세트인 E2E[13], Reddit-TLDR[21], CNN-DM[6]에서 기술을 시연합니다.metaseq 코드베이스¹를 사용하여 SkipDecode를 구현합니다.3.1 실험 설계 1배 속도가 향상된 사전 학습된 LLM(기본 모델)이 주어졌을 때, SkipDecode를 사용하여 추론 중에 네트워크가 수행하는 계산량을 줄이는 것이 목표입니다. 우리는 2×, 3×, 4× 및 5× 속도 향상에 해당하는 구성으로 방법을 평가합니다. 속도 향상은 배칭 및 KV 캐싱을 본질적으로 지원하는 기본 모델과 관련하여 보고됩니다. 이 속도 향상 비교는 배치 크기가 1이고 KV 캐싱이 없는 약한 기본 모델을 고려하는 이전의 조기 종료 작업과 다릅니다. 섹션 2.3에 설명된 대로 토큰당 최대 및 최소 계층의 다른 구성은 다른 양의 속도 향상을 달성할 수 있습니다. 우리는 e2e 데이터 세트에서 하이퍼 매개변수 튜닝을 통해 지정된 각 속도 향상에 대해 최대 및 최소 계층과 워밍업 계층 및 학습률의 최적 조합을 결정합니다. 우리는 검증 세트의 퍼플렉시티 메트릭을 기반으로 최적의 것을 선택합니다. 모델이 생성할 토큰 수를 미리 예측하는 것은 불가능하므로 실제 속도 향상은 생성 중에 약간 다를 수 있다는 점에 유의해야 합니다. 그러나 계산 예산은 각 데이터 세트의 최대 시퀀스 길이에 할당된 최소 계층에 의해 엄격하게 제한됩니다. 사용된 구성은 표 2에 나와 있습니다. 학습을 위해 모든 인스턴스에 대해 각 데이터 세트의 중간 학습 프롬프트 길이를 사용하여 모든 계층이 그림 3에 나와 있는 것처럼 원하는 생성 동작을 모방하도록 처리되도록 했습니다. 우리의 접근 방식은 효과적이면서도 간단하고 구현하기 쉽다는 점에 주목할 가치가 있습니다. 토큰 건너뛰기 정책 외에도 학습이나 생성 중에 변압기 아키텍처에 대한 추가 수정이 필요하지 않습니다. https://github.com/facebookresearch/metaseqOriginal / Base Number of Layers Target Speedup (x) #Target Avg #Warm up #Min #Max Layer Layer Layer 32 (6.7B)6.24 (1.3B)표 2: 가장 작은 복잡도에 해당하는 E2E 검증 세트를 사용하여 얻은 Base OPT(1.3B 및 6.7B)에 대한 다양한 대상 속도 향상에 대한 SkipDecode 구성. 3.2 데이터 세트 우리는 세 가지 벤치마크 데이터 세트에 대한 실험을 수행합니다. 각 데이터 세트에 대한 생성의 예는 표 3에 나와 있습니다. 생성을 위해 모든 경우에 빔 1, 탑 샘플링 0.7, 온도 0.3을 사용합니다. 학습을 위해 2e-4에서 8e-6 범위의 학습률을 스윕합니다. 데이터 세트 E2E Reddit-TLDR CNN-DM 컨텍스트 이름 [블루 스파이스], eatType[커피숍], 고객 평가 [평균], [버거킹] 근처 &quot;서브레드: r/relationships 제목: 내가 가볍게 사귀었던 이 남자 [18M]는 가을에 대학에 진학하기 때문에 나와 사귀고 싶어하지 않는다 [18F] POST: 여러분을 위한 약간의 맥락이 있습니다: 우리 둘 다 학교 연극 프로그램에서 신입생 때 만났습니다. 신입생 때 저는... (CNN) 테러 집단 알샤바브가 케냐 동부의 가리사 대학교를 공격했다고 주장하며, 이 공격으로 많은 사람이 사망하고 더 많은 사람이 인질로 잡혔습니다. 이 공격은 테러 집단의 활동이 계속 확대되는 데 있어 또 다른 단계이며, 동아프리카의 안보 상황이 빠르게 악화되고 있음을 분명히 보여줍니다. 소말리아에 본사를 둔 알샤바브는 케냐에서 최근 발생한 일련의 공격의 배후에 있으며, 가장 잘 알려진 사건 중 하나는 2013년 나이로비의 웨스트게이트 쇼핑센터에서 일어난 학살입니다. 그러나 이 단체가 케냐에 저지른 국경 간 습격은 2011년으로 거슬러 올라갑니다. 알샤바브의 침입으로 군사적... 응답 2x 버거킹 근처에 있는 블루 스파이스 커피숍은 고객들에게 평균 평가를 받았습니다. 제가 가볍게 사귀던 남자가 가을에 대학에 진학하고 저는 1년 더 고등학교에 다녀야 하기 때문에 저와 헤어지고 싶어합니다. 알샤바브가 케냐의 가리사 대학교를 공격했다고 주장합니다. 이 공격은 테러 집단의 활동이 계속 확대되는 과정에서 또 다른 단계입니다. 알샤바브는 최근 케냐에서 일어난 일련의 공격의 배후에 있습니다. 이 단체는 알카에다와 마찬가지로 코란에 대한 급진적인 해석에 의해 주로 움직입니다. 표 3: 데이터 세트 및 모델 응답의 스냅샷. 응답 5x 블루 스파이스는 버거킹 근처에 있는 커피숍입니다. 고객 평가가 평균이며 버거킹 근처에 있습니다. 내가 사귀고 있는 남자는 꽤 오랫동안 사귀고 있고, 그는 가을에 대학에 갈 예정이고, 나는 몹시 상심했고 어떻게 해야 할지 모르겠어.알샤바브가 케냐의 가리사 대학교를 공격했다고 주장했다.알샤바브는 케냐에서 일어난 일련의 최근 공격의 배후에 있다.알샤바브는 이 지역에서 일어난 일련의 최근 공격의 배후에 있다.E2E [13]. 작업은 키-값 쌍의 구조화된 정보를 유창한 텍스트로 변환하는 것이다. 비교적 작으며, 42061개의 훈련 샘플, 4672개의 평가 샘플, 4693개의 테스트 샘플로 구성되어 있다. 중간 프롬프트에는 38개의 토큰이 포함되어 있다. 훈련과 생성을 위해 최대 시퀀스 길이를 160으로, 최대 프롬프트 길이를 60으로 설정했다. 효과적인 배치 크기는 256이다. 650개의 워밍업 단계와 8개의 에포크로 프롬프트와 완료를 구분하기 위해 구분선을 사용한다. Reddit-TLDR [21]. 117,000개 샘플의 훈련 크기, 6450개 평가 크기, 6550개 샘플의 테스트 크기를 포함하는 요약 데이터 세트입니다. 중간값 훈련 프롬프트는 348개 토큰 길이입니다. 200개 워밍업 단계, 32개의 효과적인 훈련 배치, 3개의 에포크를 활용합니다. 최대 프롬프트 길이는 512개 토큰으로 설정되고 최대 시퀀스 길이는 1024개 토큰으로 설정됩니다. 컨텍스트와 완료 사이의 구분 기호는 &quot;\nTl;dr\n&quot;입니다. CNN Daily Mail[6]. 입력으로 주어진 기사에 대한 요약을 작성해야 합니다. 287,113개 샘플의 훈련 크기, 13,368개 평가 크기, 11,490개 샘플의 테스트 크기를 갖는 대규모 데이터 세트입니다. 중간값 훈련 프롬프트 길이는 788개 토큰입니다. 최대 시퀀스 길이를 2048로, 최대 프롬프트 길이를 1024로 설정했습니다. 워밍업 업데이트는 650으로 설정하고, 효과적인 배치 크기는 32이며, 2에포크 동안 훈련합니다. 컨텍스트와 완료 사이의 구분 기호는 &quot;\nTl;dr\n&quot;입니다. 3.3 주요 결과 결과는 표 4에 나와 있습니다. SkipDecode는 각 데이터 세트와 모델 크기에 대해 계산 효율성이 상당히 향상되었음을 보여줍니다. 그림 4에 나와 있듯이, 눈에 띄는 E2E: 다양한 크기에 대한 Rouge-L 대 속도 향상 Reddit: 다양한 크기에 대한 Rouge-L 대 속도 향상 80.1.3b 1.3b - 6.7b 6.7b 77.72.67.565.62.Reddit: 다양한 크기에 대한 Rouge-L 대 속도 향상 1.3b 6.7b 60.i 목표 속도 향상 (a) E2E 목표 속도 향상 (b) Reddit 목표 속도 향상 (c) CNN DM 그림 4: 1.3B 및 6.7B OPT 모델에 대한 Rouge-L 대 추론 속도 향상. 속도 향상은 배칭을 기준으로 하지 않는 약한 기본 모델을 고려한 이전 작업과 달리 배칭 및 KV 캐싱을 본질적으로 지원하는 기본 모델(1×)에 대해 계산됩니다. 1×(기본 모델)에서 2× 속도 향상으로 성능 저하가 발생한 후 속도가 증가함에 따라 성능이 꾸준히 감소합니다. 이는 토큰이 숨겨진 상태 포화점에 도달하여 더 이상의 계산 감소로 인해 성능이 저하되기 때문이라고 가정합니다. 이 패턴은 데이터 세트 전체에서 일관됩니다. E2E에서 지연된 저하가 나타나는 반면 CNN-DM은 상대적인 난이도로 인해 더 빨리 저하되기 시작합니다. E2E SkipDecode 목표 속도 향상이 1×에서 5×로 증가함에 따라 생성 프로세스에서 활성화된 디코더 계층의 평균 수가 감소하여 계산 부하가 감소함을 나타냅니다. 흥미롭게도 Bleu, Rouge-L 및 Bert-F 점수에 해당하는 모든 작업 측정값은 목표 속도 향상이 증가함에 따라 약간 감소하면서 비교적 안정적으로 유지됩니다. 이는 우리 방법이 특정 작업 설정에 대해 최소한의 저하로 상당한 속도 향상을 달성할 수 있음을 나타냅니다. Reddit 다른 것과 마찬가지로 평균 생성 계층은 목표 속도 향상이 증가함에 따라 감소합니다. 그러나 Bleu, Rouge-L 및 Bert-F 점수와 같은 성능 지표는 이 작업의 상대적인 난이도를 감안할 때 E2E 데이터 세트에 비해 더 상당한 감소를 보입니다. 우리의 방법이 여전히 상당한 속도 향상을 달성하더라도 작업 성능 측면에서의 트레이드오프는 더 눈에 띕니다. CNN-DM 결과는 이전과 유사한 추세를 따릅니다. 목표 속도 향상이 증가함에 따라 평균 생성 계층이 감소하여 계산 요구 사항이 감소함을 나타냅니다. 그러나 Bleu, Rouge-L 및 Bert-F 점수와 같은 성능 지표는 목표 속도 향상이 증가함에 따라 더 크게 떨어집니다. 우리의 접근 방식은 상당한 속도 향상을 달성할 수 있지만 숨겨진 상태 포화에 더 일찍 도달함에 따라 작업 성능의 트레이드오프가 더 두드러집니다.
--- CONCLUSION ---
, 우리의 방법은 모든 데이터 세트와 모델 크기에서 계산 수요를 줄이는 능력을 지속적으로 보여주며, 숨겨진 상태 포화점을 효과적으로 결정합니다. Bleu, Rouge-L 및 Bert-F 점수로 측정한 작업 성능에 미치는 영향은 특정 데이터 세트에 따라 다릅니다. 그러나 모든 인스턴스에서 우리의 방법은 속도 향상과 작업 성능 간에 유리한 균형을 보여주며, 모든 경우에서 거의 저하 없이 2배의 속도 향상에 도달합니다. 이 균형은 우리의 접근 방식이 제어되고 예측 가능한 계산 예산을 유지하면서 배칭 및 KV 캐싱과 같은 실질적인 과제를 능숙하게 처리하기 때문에 효과적으로 활용할 수 있습니다. 3.4 다른 방법과의 비교 SkipDecode를 벤치마킹하기 위해 디코더 전용 모델에서 작동하도록 CALM 프레임워크의 두 가지 개념을 채택했지만 현재 SkipDecode와 직접 대응하는 방법은 없습니다. 두 경우 모두 OPT 1.3b를 기본 모델로 사용합니다. 먼저 [17]에 설명된 방법에 따라 다층 종료 네트워크를 학습합니다. 여기서 단일 모델 헤드가 각 계층에서 종료되도록 학습됩니다. 이 접근 방식은 고정된 정책으로 작동하고 시퀀스 내의 모든 토큰에 적용되는 미리 결정된 종료 계층까지 실행되므로 잘림이 있는 조기 종료 방법과 더 유사합니다. 특히 이 모델은 배칭과 KV 캐싱을 지원합니다. 두 번째 방법은 동일한 모델을 사용하지만 CALM의 숨겨진 상태 포화 개념을 추가로 적용하여 디코더 전용 네트워크(CALM-DEC라고 함)에서 작동하도록 조정했습니다. 그러나 이 네트워크는 배치 크기를 하나로 제한하여 배칭과 KV 캐싱을 배제합니다. 결과적으로 모델은 필요에 따라 이전 토큰의 모든 KV 값을 &#39;백필&#39;해야 합니다. 데이터 세트 크기 대상 속도 향상 #대상 평균 #세대 평균 블루 루즈-L 베르트-F 계층 계층 E2E 1.3b 6.7b 레드잇 1.3b 6.7b CNN-DM 1.3b 6.7b 2286525-03 22865 85-8622005 24-65.67.70.14.66.67.67.9.66.68.67.6.65.66.66.5.64.66.65.64.66.70.20.65.68.67 .65.68.67.9.66.67.67.7.64.65.65.9.27.31.15.8.27.32.9.7.25.22.6.3.21.11.5.3.19. 7.9.28.33.19.9.27.32.13.8.26.25.9.5.21.9.6.4.19.7.15.29.35.15.15.28.34.8.7.23.20.6.3.18.2.5.4.18.2.16.30.37.21.15.29.35.11.4.21.17.8.5.20.7.6.6.4.18.2.표 4: 다양한 속도 향상 및 기본 모델 크기에 대한 다양한 데이터 세트의 SkipDecode 성능(이 경우 해당 계층에서 마지막으로 알려진 숨겨진 상태를 투영하여)은 상당한 시스템 오버헤드를 추가합니다. 이 접근 방식의 최악의 경우 계산 비용은 전체 네트워크 비용과 동일합니다. 이 네트워크의 적응형 은닉 상태 포화 정책은 배칭과 계산/시간 추정 모두에 대해 고정되지 않은 정책의 표준적인 단점이 있습니다.또한 다음과 같은 이유로 특히 대형 디코더 전용 모델에서 속도가 빨라질수록 성능이 크게 저하됩니다.KV 백필은 이러한 작업에 매우 중요한 프롬프트 인코딩에 영향을 미칩니다.CALM[17]의 T5 모델과 같은 인코더-디코더 아키텍처에서 KV 백필은 프롬프트 인코딩을 유지합니다.디코더 전용 아키텍처는 과거 상태를 동시에 인코딩하고 디코딩하는 반면, 조기 종료는 네트워크가 이전 컨텍스트를 이해하는 데 영향을 미칠 가능성이 더 큽니다(부록 참조).이로 인해 CALM 구현은 원래 T5 인코더-디코더 구현과 달리 디코더 전용 모델에서 상당한 저하를 보입니다. Speedup E2E Reddit-TLDR SkipDecode 다중 계층 CALM-DEC SkipDecode 다중 계층67.68.68.27.26.67.65.35.27.17.68.61.32.25.12.66.50.27.21.7.66.46.22.19.6.표 5: SkipDecode, 다중 계층 및 CALM-DEC의 성능 비교.표 5에서 볼 수 있듯이 SkipDecode는 다른 접근 방식보다 우수한 성능을 보입니다.이는 속도 향상 요소가 증가함에 따라 두 데이터 세트 모두에서 작업 성능 저하가 현저히 적다는 사실로 입증됩니다.이는 속도 향상 증가에 대한 우리 방법의 견고성을 보여줍니다.4 관련 작업 모델 압축: 대규모 언어 모델(LLM)의 추론 효율성을 개선하는 기술을 개발하기 위해 모델 압축에 대한 광범위한 연구가 있었습니다. 가장 두드러진 작업 라인 중 하나는 지식 증류(KD) [7]를 활용하여 숨겨진 상태 및 주의 상태와 같은 교사인 LLM의 표현을 사용하여 더 빠른 추론으로 더 작은 학생 모델을 훈련합니다 [10, 19]. 모델 압축의 또 다른 작업 라인은 양자화 [3], 저정밀도 훈련 및 네트워크 가지치기 [5], 매개변수 공유 및 인수 분해를 사용하여 메모리 공간과 네트워크 지연 시간을 줄입니다 [4, 20]. 주목할 점은 위의 모델 압축 연구 대부분이 자연어 이해 작업을 위한 인코더 전용 모델에 초점을 맞추었다는 것입니다.조기 종료: 모든 입력에 대해 동일한 계산이 호출되는 정적 계산을 사용하는 위의 작업과 달리, 우리는 입력의 다른 부분에 대해 가변 계산을 사용하는 적응형 계산에 중점을 둡니다.기존 적응형 계산 기술은 주로 조기 종료 전략 [25, 24, 22, 12, 11, 9]에 의존하는데, 여기서 입력의 토큰은 네트워크의 다른 계층에서 종료하는 방법을 학습합니다. KD의 작업과 유사하게, 이러한 기술의 대부분은 자연어 이해(NLU) 작업을 위한 BERT [2]와 같은 인코더 전용 모델용으로 개발되었습니다. 시퀀스 전체를 처리해야 하는 NLU 작업과 달리, 토큰별 생성에 대한 자기회귀적 특성으로 인해 생성 작업은 더 복잡합니다. 최근 작업인 CALM [17]은 사용할 신뢰도 측정, 시퀀스 수준 제약 조건을 로컬 토큰별 종료 결정에 연결하는 방법, 이전 토큰의 조기 종료로 인해 누락된 숨겨진 표현을 처리하는 방법 측면에서 생성 작업에 대한 토큰 수준 조기 종료 전략을 연구합니다. 그러나 이전의 모든 조기 종료 작업과 유사하게 CALM은 실제로 추론 속도를 높이기 위해 널리 사용되는 배칭(배치 크기 1만 지원) 및 KV 캐싱과 관련된 몇 가지 주요 실질적 차단 요인으로 어려움을 겪습니다. 또한 이러한 모든 종료 전략에 대한 최악의 시나리오(예: 모든 토큰의 최상위 계층에 더 가까운 종료 지점)는 전체 네트워크를 사용하여 예측할 수 없는 시스템 부하와 일관되지 않은 처리량을 초래할 수 있습니다. 이러한 과제를 해결하기 위해, 우리는 효율적인 추론을 위한 사소하지 않은 배칭과 KV 캐싱을 지원하고, 놀라움 없이 예측 가능한 계산 부하를 보장하는 SkipDecode를 개발합니다.5 한계와 미래 방향 SkipDecode는 배칭과 키-값(KV) 캐싱이 기존 토큰 수준 조기 종료 전략과 본질적으로 호환되지 않는 것과 같은 핵심적인 문제를 해결합니다.그러나 감소 정책의 도입은 새로운 한계를 추가합니다.생성이 진행되고 배치의 샘플이 계산을 마치면 현재 위치가 나머지 요소의 위치와 일치하는 경우에만 새 샘플을 배치에 포함할 수 있습니다.따라서 우리의 방법은 자연스럽게 &#39;무한 루프&#39; 추론 모드를 지원하지 않습니다.예비 실험에서 거듭제곱 법칙 감소 함수는 이 연구에서 사용된 선형 감소보다 개선되지 않았습니다.특히, 이전 연구에서는 토큰 종료 수준에 대한 거듭제곱 법칙 분포가 있음을 나타냅니다[17].그림 1에 나와 있는 Oracle 탐색 실험은 이러한 관찰을 뒷받침합니다.대체 감소 함수를 조사하는 것은 향후 작업에 대한 흥미로운 길을 제시합니다. 또 다른 유망한 연구 방향은 프롬프트에 대한 감소 정책의 영향을 조사하는 것입니다. 이전 연구에 따라 프롬프트에 대한 전체 네트워크를 활용했습니다. 앞서 언급했듯이 정책을 프롬프트로 확장하고 보다 공격적인 감소 기능을 구현하면 추가적인 속도 향상을 얻을 수 있습니다. 이를 통해 보다 효율적이고 다재다능한 토큰 수준 조기 종료 전략을 위한 길을 열 수 있습니다. 6 결론 SkipDecode는 토큰 수준 조기 종료의 이론적 이점과 실제 애플리케이션 요구 사항 간의 격차를 메웁니다. 일괄 처리 및 키값 캐싱과 같은 실질적인 과제를 능숙하게 해결합니다. 또한 제어된 계산 예산으로 숨겨진 상태의 포화점을 식별하여 계산 요구 사항을 줄이는 기능을 지속적으로 보여줍니다. 이를 통해 효율성이 향상될 뿐만 아니라 보다 접근 가능하고 지속 가능한 AI 생태계가 촉진됩니다. 이러한 개선 사항을 더욱 확대하기 위해 향후 노력은 동적 배칭을 향상시키고 붕괴 함수의 동작을 더 깊이 파고드는 데 집중할 수 있습니다.참고문헌 [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습기입니다. H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집, 신경 정보 처리 시스템의 발전, 33권, 1877~1901페이지. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 4171-4186쪽, 2019. [3] Yunchao Gong, Liu Liu, Ming Yang, Lubomir D. Bourdev. 벡터 양자화를 사용한 딥 합성곱 네트워크 압축. CORR, abs/1412.6115, 2014. [4] Manish Gupta, Puneet Agrawal. 텍스트에 대한 딥 러닝 모델 압축: 조사. ACM Trans. Knowl. Discov. Data, 16(4):61:1-61:55, 2022. doi: 10.1145/3487045. URL https://doi.org/10.1145/3487045. [5] Song Han, Huizi Mao, William J. Dally. 심층 압축: 가지치기, 훈련된 양자화 및 허프만 코딩을 사용한 심층 신경망 압축.ICLR, 2016. [6] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman 및 Phil Blunsom. 기계에 읽고 이해하도록 가르치기.신경 정보 처리 시스템의 발전, 28, 2015. [7] Geoffrey E. Hinton, Oriol Vinyals 및 Jeffrey Dean.신경망에서 지식 추출.CORR, abs/1503.02531, 2015. [8] Ari Holtzman, Jan Buys, Maxwell Forbes 및 Yejin Choi.신경 텍스트 퇴화의 호기심 많은 사례. CORR, abs/1904.09751, 2019. [9] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu. Dynabert: 적응형 폭과 깊이를 갖춘 동적 BERT. Hugo Larochelle, Marc&#39;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin 편집자, 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html. [10] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang 및 Qun Liu. Tinybert: 자연어 이해를 위한 버트 증류, 2019. [11] Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou 및 Xu Sun. Cascadebert: 보정된 전체 모델 캐스케이드를 통해 사전 훈련된 언어 모델의 추론을 가속화합니다. Marie-Francine Moens, Xuanjing Huang, Lucia Specia 및 Scott Wen-tau Yih 편집자, 전산 언어학 협회 조사 결과: EMNLP 2021, 가상 이벤트/도미니카 공화국 푼타 카나, 2021년 11월 16~20일, 475~486페이지. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.43. URL https: //doi.org/10.18653/v1/2021.findings-emnlp.43. [12] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju. Fastbert: 적응 추론 시간을 갖춘 자체 증류 BERT. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel R. Tetreault 편집자, Association for Computational Linguistics 제58회 연례 회의록, ACL 2020, 온라인, 2020년 7월 5-10일, 6035-6044쪽. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.537. URL https: //doi.org/10.18653/v1/2020.acl-main.537.[13] Jekaterina Novikova, Ondřej Dušek, Verena Rieser. e2e 데이터 세트: 종단 간 생성을 위한 새로운 과제. arXiv 사전 인쇄본 arXiv:1706.09254, 2017. [14] Alec Radford와 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. [15] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 언어 모델은 비지도 멀티태스크 학습기입니다. 2019. [16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21(1), 2020년 1월. ISSN 1532-4435. [17] Tal Schuster, Adam Fisch, Jai Prakash Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, Donald Metzler. 확신에 찬 적응 언어 모델링. CoRR, abs/2207.07061, 2022. [18] Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling Wu, Yilong He, Yuan Ni, Guotong Xie, Xuanjing Huang, Xipeng Qiu. 언어 이해 및 생성을 위한 간단한 해시 기반 조기 종료 접근 방식. Smaranda Muresan, Preslav Nakov, Aline Villavicencio 편집자, Findings of the Association for Computational Linguistics: ACL 2022, 더블린, 아일랜드, 2022년 5월 22-27일, 2409-2421페이지. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.189. URL https://doi.org/10. 18653/v1/2022. findings-acl. 189. [19] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou. Mobilebert: 리소스가 제한된 장치를 위한 컴팩트한 작업 독립적 bert. arXiv 사전 인쇄본 arXiv:2004.02984, 2020. [20] Marcos V. Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Pedro Henrique Martins, André FT Martins, Peter A. Milder, Colin Raffel, Edwin Simpson, Noam Slonim, Niranjan Balasubramanian, Leon Derczynski, Roy Schwartz. 자연어 처리를 위한 효율적인 방법: 설문 조사. CoRR, abs/2209.00099, 2022. doi: 10.48550/arXiv.2209.00099. URL https: //doi.org/10.48550/arxiv. 2209.00099. [21] Michael Völske, Martin Potthast, Shahbaz Syed, Benno Stein. Tl; dr: 자동 요약을 배우기 위한 reddit 마이닝. 요약의 새로운 전선 워크숍 회의록, 59-63페이지, 2017. [22] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin. Deebert: BERT 추론 가속화를 위한 동적 조기 종료. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel R. Tetreault 편집, Association for Computational Linguistics의 제58회 연례 회의록, ACL 2020, 온라인, 2020년 7월 5-10일, 2246-2251쪽. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.204. URL https: //doi.org/10.18653/v1/2020.acl-main.204. [23] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. Opt: 사전 학습된 변환기 언어 모델 개방. ArXiv, abs/2205.01068, 2022. [24] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, Furu Wei. BERT가 인내심을 잃음: 조기 종료로 빠르고 강력한 추론. Hugo Larochelle, Marc&#39;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan 및 Hsuan-Tien Lin 편집자, 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3- Abstract.html. [25] Wei Zhu. Leebert: 교차 수준 최적화를 통해 BERT에 대한 조기 종료를 학습했습니다. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli 편집, Association for Computational Linguistics의 제59회 연례 회의록 및 자연어 처리에 대한 제11회 국제 공동 컨퍼런스, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, 2021년 8월 1-6일, 2968-2980페이지. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.231. URL https://doi.org/10.18653/v1/2021. acl-long. 231.
