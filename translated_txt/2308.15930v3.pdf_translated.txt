--- ABSTRACT ---
멀티모달 대규모 언어 모델은 최근 상당한 관심을 얻었습니다. 하지만 대부분의 연구는 시각-언어 멀티모달 모델에 초점을 맞추어 시각-언어 지침을 따르는 강력한 기능을 제공합니다. 그러나 우리는 음성도 인간이 세상과 상호 작용하는 중요한 모달리티라고 주장합니다. 따라서 범용 보조자가 멀티모달 음성-언어 지침을 따를 수 있는 것이 중요합니다. 이 연구에서 우리는 대규모 언어 및 음성 모델(LLASM)을 제안합니다. LLaSM은 음성-언어 지침을 따를 수 있는 크로스 모달 대화 능력을 갖춘 엔드투엔드 훈련된 대규모 멀티모달 음성-언어 모델입니다. 우리의 초기 실험은 LLaSM이 인간이 인공 지능과 상호 작용하는 더 편리하고 자연스러운 방법을 보여준다는 것을 보여줍니다. 특히, 우리는 또한 대규모 음성 지시 따르기 데이터 세트 LLaSM-Audio-Instructions를 출시합니다. 코드와 데모는 https://github.com/LinkSoul-AI/LLASM 및 https:// huggingface.co/spaces/LinkSoul/LLaSM에서 제공됩니다. LLaSM-Audio-Instructions 데이터 세트는 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions에서 제공됩니다. 1
--- INTRODUCTION ---
음성은 의미 정보를 포함하고 동시에 음조와 같은 준언어적 정보를 포함하며 텍스트보다 더 많은 양의 정보를 전달합니다.또한 음성은 인간이 인공 지능과 상호 작용하는 더 편리하고 자연스러운 방법입니다.따라서 범용 보조 장치를 개발할 때 음성 및 언어 지침을 따르는 것이 중요합니다.그러나 대부분의 대규모 언어 모델[1, 2, 3]은 텍스트 입력만 받으므로 대규모 언어 모델의 기능이 제한됩니다.시각 및 언어 다중 모달 모델[4, 5, 6, 7, 8, 9]은 시각 정보를 이해하는 기능을 제공하여 일반 인공 지능(AGI)을 향한 큰 진전을 이루었지만 인간이 텍스트 지침을 입력하여 작업을 입력하는 것은 여전히 불편합니다.계단식 패러다임 방법[10, 11]은 자동 음성 인식(ASR) 모델을 사용하여 음성 입력을 텍스트 입력으로 변환한 다음 모델은 텍스트 입력으로 작업을 처리할 수 있습니다. 그러나 여전히 음성에서 텍스트로 모달 변환하는 동안 정보 소비로 이어지고 ASR 시스템의 실수를 가져올 수 있습니다.최근에 대규모 언어 모델을 사용하여 음성과 텍스트를 처리하고 생성하는 데 중점을 둔 음성 언어 다중 모달 모델[12, 13]은 다중 모달 콘텐츠를 이해하고 생성할 수 있습니다.음성 신호는 개별 토큰으로 인코딩된 다음 개별 음성 토큰이 LLM의 어휘로 확장됩니다.이런 식으로 LLM은 풍부한 다중 모달 데이터와 거대한 컴퓨팅 리소스로 재교육되어야 합니다.이 논문에서는 음성 및 언어 지침을 이해하고 따를 수 있는 교차 모달 대화 능력을 갖춘 대규모 음성 및 언어 모델인 LLaSM을 제안합니다.LLaVA[6]의 방식에 따라 잘 훈련된 음성 모달 인코더와 LLM을 활용하여 LLaSM을 리소스 친화적으로 만듭니다.특히 Whisper[14]를 음성 인코더로 사용하여 음성 신호를 임베딩으로 인코딩합니다. 그런 다음 모달 어댑터는 A PREPRINT - SEPTEMBER 19, 음성 임베딩을 대규모 언어 모델의 입력 텍스트 임베딩과 정렬하는 방법을 학습합니다. 음성 임베딩과 텍스트 임베딩을 연결하여 인터리브 시퀀스를 형성한 다음 인터리브 시퀀스를 LLM에 입력하여 감독 미세 조정을 수행합니다. 학습 과정은 두 단계로 나뉩니다. 첫 번째 단계에서는 모달 적응 사전 학습을 위해 공개 ASR 데이터 세트를 사용합니다. 음성 인코더와 LLM은 고정되고 모달 어댑터만 음성 및 텍스트 임베딩을 정렬하도록 학습합니다. 대부분의 모델 매개변수는 고정된 상태로 유지되므로 이 단계에서는 모달 어댑터의 매개변수 중 일부만 학습되므로 리소스를 많이 소모하지 않습니다. 두 번째 단계에서는 모델에 크로스 모달 대화를 처리하고 다중 모달 지침을 처리할 수 있는 용량을 제공하기 위해 학습에 크로스 모달 지침 데이터를 사용합니다. 모달 어댑터와 언어 모델의 매개변수가 교차 모달 지침 미세 조정을 위해 업데이트되는 동안 음성 인코더는 동결됩니다. 기존의 오픈 소스 음성-텍스트 교차 모달 지침 따르기 데이터 세트가 부족하다는 점에 유의해야 하므로 음성 텍스트 교차 모달 지침 따르기 데이터 세트 LLaSM-Audio-Instructions를 빌드하여 릴리스합니다. 이 데이터 세트는 GPT4-LLM[15], ShareGPT[16], WizardLM[17]에서 대화를 신중하게 선택하고 텍스트-음성 기술을 사용하여 대량의 대화 오디오 데이터를 생성하여 구성되었습니다. 총 199k 대화가 포함되어 있으며, 여기에는 80k개의 중국어 오디오 샘플과 428k개의 영어 오디오 샘플이 있습니다. 이는 우리가 아는 한 가장 큰 중국어 및 영어 음성-텍스트 교차 모달 지침 따르기 데이터 세트입니다. 저희 논문은 다음과 같은 기여를 합니다. • 음성 언어 명령을 이해하고 따를 수 있는 음성 언어 멀티모달 모델을 구축하여 인간이 인공 지능과 상호 작용하는 데 더 편리하고 자연스러운 방법을 제공합니다. • 대규모 중국어 및 영어 음성 텍스트 크로스모달 명령 따르기 데이터 세트인 LLaSM-Audio-Instrutions를 구성하여 릴리스합니다. https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions에서 데이터를 릴리스합니다. • https://github.com/LinkSoul-AI/LLASM에서 코드를 릴리스하고 데모는 https://huggingface.co/spaces/LinkSoul/LLaSM에서 보여줍니다. 2
--- RELATED WORK ---
Vision Large Language Model은 최근 상당한 주목을 받고 있습니다[4, 5, 6, 7, 8, 9]. 대부분은 사전 훈련된 LLM과 비전 인코더를 활용하여 비전 작업을 수행합니다. Flamingo[18]는 게이트형 교차 주의를 사용하여 사전 훈련된 비전 인코더와 언어 모델을 정렬하고 수십억 개의 이미지-텍스트 쌍으로 훈련합니다. BLIP-2[19]는 Q-Former가 있는 Flan-T5[20]를 사용하여 시각적 특징을 언어 모델과 효율적으로 정렬합니다. 수십억 개의 매개변수를 특징으로 하는 Palm-E[5]는 540B PaLM[2]과 22B Vision Transformer[21]를 가장 큰 비전-언어 모델로 통합합니다. LLAVA[6]는 사전 훈련된 CLIP[22] 시각적 인코더와 LLAMA[3]를 활용하고 GPT4 지원 시각적 지침 데이터에 대한 지침 튜닝을 수행합니다. GPT-4[4]도 강력한 시각적 이해 및 추론 능력을 보여줍니다. 시각 도메인에서 멀티모달 대규모 언어 모델의 성공은 음성 도메인 연구에도 많은 영감을 가져왔습니다. 음성 대규모 언어 모델은 시각 멀티모달 LLM의 성공에 대한 관심이 점점 더 커지고 있습니다. 계단식 패러다임
--- METHOD ---
s [10, 11]는 자동 음성 인식(ASR) 모델을 사용하여 음성 입력을 텍스트 입력으로 변환한 다음 모델은 텍스트 입력으로 작업을 처리할 수 있습니다. 그러나 여전히 음성에서 텍스트로 모달 변환하는 동안 정보 소비로 이어지고 ASR 시스템의 실수를 가져올 수 있습니다. 최근에는 대규모 언어 모델을 사용하여 음성과 텍스트를 처리하고 생성하는 데 중점을 둔 음성 언어 다중 모달 모델[12, 13]이 다중 모달 콘텐츠를 이해하고 생성할 수 있습니다. 음성 신호는 개별 토큰으로 인코딩된 다음 개별 음성 토큰이 LLM의 어휘로 확장됩니다. 이런 식으로 LLM은 풍부한 다중 모달 데이터와 방대한 컴퓨팅 리소스로 재교육되어야 합니다. 이 논문에서는 음성 및 언어 지침을 이해하고 따를 수 있는 교차 모달 대화 능력을 갖춘 대규모 음성 및 언어 모델인 LLaSM을 제안합니다. LLaVA [6]의 방식에 따라, 우리는 잘 훈련된 음성 모달 인코더와 LLM을 활용하여 LLaSM을 더욱 리소스 친화적으로 만듭니다. 구체적으로, 우리는 Whisper [14]를 음성 인코더로 사용하여 음성 신호를 임베딩으로 인코딩합니다. 그런 다음 모달 어댑터는 대규모 언어 모델의 입력 텍스트 임베딩과 음성 임베딩을 정렬하는 방법을 학습합니다. 음성 임베딩과 텍스트 임베딩은 서로 연결되어 인터리브 시퀀스를 형성한 다음, 인터리브 시퀀스는 LLM에 입력되어 감독 미세 조정됩니다. 훈련 과정은 두 단계로 나뉩니다. 첫 번째 단계에서는 모달 적응 사전 훈련을 위해 공개 ASR 데이터 세트를 사용합니다. 음성 인코더와 LLM은 동결되고 모달 어댑터만 음성 및 텍스트 임베딩을 정렬하도록 훈련됩니다. 대부분의 모델 매개변수가 고정되어 있으므로 이 단계에서는 모달 어댑터의 매개변수 중 일부만 학습하므로 리소스를 많이 소모하지 않습니다.두 번째 단계에서는 모델에 모달 간 대화를 처리하고 다중 모달 지침을 처리할 수 있는 용량을 제공하기 위해 모달 간 지침 데이터를 사용하여 학습합니다.모달 어댑터와 언어 모델의 매개변수가 모달 간 지침 미세 조정을 위해 업데이트되는 동안 음성 인코더는 고정됩니다.기존 오픈소스 음성-텍스트 모달 간 지침 따르기 데이터 세트가 부족하다는 점에 유의해야 하므로 음성 텍스트 모달 간 지침 따르기 데이터 세트인 LLaSM-Audio-Instructions를 빌드하여 릴리스합니다.이 데이터 세트는 GPT4-LLM[15], ShareGPT[16], WizardLM[17]에서 대화를 신중하게 선택하고 텍스트-음성 변환 기술을 사용하여 대량의 대화 오디오 데이터를 생성하여 구성되었습니다. 총 199k개의 대화가 포함되어 있으며, 여기에는 80k개의 중국어 오디오 샘플과 428k개의 영어 오디오 샘플이 있습니다. 이는 우리가 아는 가장 큰 중국어 및 영어 음성-텍스트 교차 모달 지시-추종 데이터 세트입니다. 저희 논문은 다음과 같은 기여를 합니다. • 우리는 음성-언어 지시를 이해하고 따를 수 있는 음성-언어 다중 모달 모델을 구축하여 인간이 인공 지능과 상호 작용하는 보다 편리하고 자연스러운 방법을 제공합니다. • 우리는 대규모 중국어 및 영어 음성-텍스트 교차 모달 지시-추종 데이터 세트인 LLaSM-Audio-Instrutions를 구축하여 릴리스합니다. 저희는 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions에서 데이터를 릴리스합니다. • 저희는 https://github.com/LinkSoul-AI/LLASM에서 코드를 릴리스하고 데모는 https://huggingface.co/spaces/LinkSoul/LLaSM에서 보여줍니다. 2 관련 작업 Vision Large Language Model은 최근 상당한 주목을 받고 있습니다[4, 5, 6, 7, 8, 9]. 대부분은 사전 훈련된 LLM과 비전 인코더를 활용하여 비전 작업을 수행합니다. Flamingo[18]는 게이트형 교차 주의를 사용하여 사전 훈련된 비전 인코더와 언어 모델을 정렬하고 수십억 개의 이미지-텍스트 쌍으로 훈련합니다. BLIP-2[19]는 Q-Former가 있는 Flan-T5[20]를 사용하여 시각적 특징을 언어 모델에 효율적으로 정렬합니다. 수십억 개의 매개변수를 특징으로 하는 Palm-E[5]는 540B PaLM[2]과 22B Vision Transformer[21]를 가장 큰 비전-언어 모델로 통합합니다. LLAVA[6]는 사전 훈련된 CLIP[22] 시각 인코더와 LLAMA[3]를 활용하고 GPT4 지원 시각 지침 데이터에 대한 지침 튜닝을 수행합니다. GPT-4[4]도 강력한 시각적 이해 및 추론 능력을 보여줍니다. 시각 도메인에서 다중 모달 대규모 언어 모델의 성공은 음성 도메인 연구에도 많은 영감을 불어넣었습니다. 음성 대규모 언어 모델은 시각 다중 모달 LLM의 성공으로 인해 점점 더 많은 관심을 얻었습니다. 캐스케이딩 패러다임 방법[10, 11]은 자동 음성 인식(ASR) 모델을 사용하여 음성 입력을 텍스트 입력으로 변환하는데, 이는 여전히 정보 소비로 이어지고 ASR 시스템의 실수를 가져올 수 있습니다. 최근에는 대규모 언어 모델을 사용하여 음성과 텍스트를 처리하고 생성하는 데 중점을 둔 음성 언어 다중 모달 모델[12, 13]이 다중 모달 콘텐츠를 이해하고 생성할 수 있습니다. 음성 신호는 개별 토큰으로 인코딩된 다음 개별 음성 토큰이 LLM의 어휘로 확장됩니다. 이런 식으로 LLM은 풍부한 다중 모달 데이터와 방대한 컴퓨팅 리소스로 재교육되어야 합니다. 3 접근 방식 3.1 모델 멀티모달 모델을 훈련하는 초점은 멀티모달의 크로스모달 보완 정보를 융합하고 잘 훈련된 대규모 언어 모델의 기능을 효과적으로 활용하는 것입니다.LLaSM 모델 아키텍처는 그림 1에 나와 있습니다.우리는 Whisper[14]를 사용하여 먼저 원시 오디오 데이터를 임베딩으로 인코딩한 다음, 사전 훈련 단계에서 모달 어댑터를 훈련하여 오디오 임베딩과 텍스트 임베딩을 맞춥니다.오디오 임베딩과 텍스트 임베딩은 서로 연결되어 대규모 언어 모델에 입력할 인터리브된 입력 시퀀스를 형성합니다.우리는 중국어와 영어 모두에 대한 기능을 갖춘 Chinese-LLAMA2-7B[23]를 LLM으로 선택했습니다. 크로스 모달 명령어 미세 조정 단계에서 모달 어댑터와 LLM은 다중 작업으로 훈련됩니다.오디오 텍스트 TOTT 10. 모달 인코더 모달 어댑터 토큰화 및 임베드 + 사전 인쇄 - 9월 19일, 임베딩 언어 모델 임베딩 그림 1: LLaSM의 모델 프레임워크 사전 훈련 단계. 이 단계에서 모달 인코더와 LLM은 동결된 상태로 유지됩니다. LLM이 모달 인코더의 오디오 임베딩을 이해할 수 있도록 모달 어댑터는 공개 ASR 데이터로 훈련되어 텍스트와 오디오 임베딩을 정렬합니다. ASR 데이터의 데이터 샘플(오디오 데이터, 텍스트 레이블)은 (간단한 명령어, 오디오 데이터, 텍스트 레이블)의 튜플로 포맷되며, 여기서 간단한 명령어는 자동 음성 인식 명령어입니다. 오디오 데이터의 언어에 따라 그림에 나열된 영어 간단한 명령어나 그림 3에 나열된 중국어 간단한 명령어가 선택됩니다. 사전 학습 멀티모달 시퀀스 Xsample의 통합된 형식은 그림 4에 나와 있습니다. 각 데이터 샘플은 X 샘플로 포맷된 다음, 텍스트 시퀀스의 오디오 패치 임베딩을 모달 어댑터의 오디오 임베딩으로 바꿉니다. 마지막으로 인터리브된 입력 임베딩은 대규모 언어 모델에 입력됩니다. 학습 목표는 각 데이터 샘플의 텍스트 레이블을 예측하는 것입니다. • &quot;다음 음성을 텍스트로 필사합니다.&quot; • &quot;말한 단어를 서면 텍스트로 변환합니다.&quot; &quot;말을 서면 필사본으로 변환합니다.&quot; &quot;구두 의사소통을 서면 텍스트로 녹음합니다.&quot; &quot;구두 메시지를 서면 형태로 렌더링합니다.&quot; &quot;말한 언어를 서면 스크립트로 전치합니다.&quot; &quot;말한 단어를 서면 텍스트로 번역합니다.&quot; &quot;말한 단어를 적습니다.&quot; • &quot;말한 담화를 서면 문서로 변환합니다.&quot; • &quot;말한 발화를 서면 필사본으로 변환합니다.&quot; &quot;말한 음성의 서면 기록을 만듭니다.&quot; 그림 2: 간단한 영어 지침. &quot; “请将以下语음을转译为文本&quot; “请将这段语음을 转换成文字。 &quot;能否把这段语语转成文字字?&quot; &quot;把这段语转写成文字 . “将此录음을 转写成文字 . “把这段음을 频转换成文字 . &quot;能否把这段录音转成文字?” “把这段语음을 转화한 문자본입니다. 그림 3: 중국어 간단한 지침. 교차 모달 명령 미세 조정. 이 단계에서는 모달 인코더만 고정되고 모달 어댑터와 LLM은 다중 작업으로 공동 훈련됩니다. 우리는 여러 대화 데이터를 사용하여 복잡한 교차 모달 지침을 구축합니다. Microsoft Azure TTS(텍스트 음성 변환) API를 사용하여 인간의 질문을 오디오 데이터로 생성한 다음, 챗봇의 응답을 예측하는 것이 훈련 목표입니다. 한 라운드의 질문과 답변은 다중 모달 시퀀스 X 샘플로 처리되며, 여러 라운드의 질문과 답변은 EOS 토큰과 연결됩니다. 통합된 크로스 모달 지시 미세 조정 시퀀스 형식은 그림 5에 나와 있습니다. 다중 작업 지시가 있는 텍스트 전용 대화 데이터의 효과성은 여러 오픈 소스 언어 전용 지시 조정 작업[15, 16, 17]에서 입증되었으므로 크로스 모달 지시는 다중 모달 지시를 따르는 용량을 개선할 수 있습니다. 3.2 데이터 수집 LLM이 오디오 신호를 이해할 수 있도록 여러 공개 ASR 데이터 세트를 수집하여 자동 음성 인식의 간단한 지시로 모달 적응 사전 학습 데이터를 형성합니다. 그리고 크로스 모달 지시 조정을 위해 여러 오픈 소스 언어 전용 지시 조정 데이터 세트를 사용하여 음성 데이터를 생성하여 크로스 모달 지시 미세 조정 데이터를 구축합니다. 세부 사항은 다음과 같습니다. 모달 적응 사전 학습 데이터. 텍스트와 오디오의 임베딩을 정렬하기 위해 Aishell[24], LibriSpeech[25], Magicdata[26] 및 Primewords[27]를 포함하여 영어와 중국어로 된 여러 공개 ASR 데이터 세트를 수집합니다. ASR 데이터의 데이터 샘플은 일반적으로 음성 오디오와 텍스트 발화의 쌍으로 구성되며, 특히 작업 지침으로 데이터 샘플에 간단한 지침을 추가할 때 그렇습니다. 이러한 간단한 지침은 그림 2와 그림 3에 나열되어 있으며, 이는 영어와 중국어로 된 자동 음성 인식 작업의 다른 표현입니다. 사전 학습하는 동안 간단한 지침과 오디오 데이터는 오디오 데이터의 텍스트 레이블을 예측하기 위해 모델에 입력됩니다.A 사전 인쇄 - 9월 19일, X 샘플 콘텐츠 사용자 = {audiotoken} + &quot;\n&quot; + Isimple(또는 콘텐츠 사용자 = = B_INST + B_SYS + SYSTEM + E_SYS + {contentuser} + E_INST + {content gpt} Isimple + &quot;\n&quot; + {audio token}) 콘텐츠 gpt 오디오 토큰 = TEXT_LABEL = AUDIO_START_TOKEN + AUDIO_PATCH_TOKEN * {audio_token_len} + AUDIO_END_TOKEN 그림 4: 사전 학습을 위한 샘플 시퀀스 형식. Llama-2의 방식을 따르고 B_INST = &#39;[INST]&#39;, E_INST = &#39;[/INST]&#39;, B_SYS = &#39;&lt;<SYS> &gt; \n&#39;, E_SYS = &#39;\n &lt;</SYS> &gt; \n\n&#39;. SYSTEM = &#39;당신은 도움이 되는 언어 및 음성 조수입니다. 사용자가 제공하는 음성 내용을 이해하고 자연어를 사용하여 다양한 작업을 수행할 수 있습니다.&#39;, 그리고 TEXT_LABEL은 ASR 데이터 샘플의 텍스트 레이블입니다. audio_token_len은 기본적으로 64로 설정됩니다. 특수 오디오 토큰이 사용됩니다. AUDIO_START_TOKEN = &#39;<au_start> &#39;, 오디오_엔드_토큰 = &#39;<au_end> &#39;, 오디오_패치_토큰 = &#39;<au_patch> &#39;. 콘텐츠 사용자는 오디오 토큰과 Isimple로 구성되며, 여기서 Isimple은 간단한 명령어이며 오디오 토큰 앞이나 뒤에 무작위로 배치됩니다. BOS 토큰과 EOS 토큰을 훈련하는 동안 시퀀스의 시작과 끝에 각 샘플에 추가되지만, 녹색 토큰만 손실을 계산하는 데 사용됩니다. Xinstruction X 샘플 콘텐츠 사용자 오디오 토큰 =<BOS> + X 샘플 +<EOS> + X 샘플 +<EOS> + *** +<EOS> = B_INST + B_SYS + SYSTEM + E_SYS + {content user} + E_INST + {content gpt} = {audiotoken}, content gpt = TEXT_RESPONSE = AUDIO_START_TOKEN + AUDIO_PATCH_TOKEN * {audio_token_len} + AUDIO_END_TOKEN 그림 5: 교차 모달 명령어 미세 조정을 위한 샘플 시퀀스 형식. Llama-2의 방식을 따르고, B_INST = &#39;[INST]&#39;, E_INST = &#39;[/INST]&#39;, B_SYS=&#39;&lt;<SYS> &gt; \n&#39;, E_SYS = &#39;\n &lt;</SYS> &gt; \n\n&#39;. SYSTEM = &#39;당신은 도움이 되는 언어 및 음성 조수입니다. 사용자가 제공하는 음성 내용을 이해하고 자연어를 사용하여 다양한 작업을 수행할 수 있습니다.&#39;, 그리고 TEXT_RESPONSE는 챗봇의 텍스트 응답입니다. audio_token_len은 기본적으로 64로 설정됩니다. 특수 오디오 토큰이 사용됩니다. AUDIO_START_TOKEN = &#39;<au_start> &#39;, 오디오_엔드_토큰 = &#39;<au_end> &#39;, 오디오_패치_토큰 = &#39;<au_patch> &#39;. 콘텐츠 사용자는 훈련 중에 오디오 임베딩으로 대체될 오디오 토큰입니다. 질문과 답변의 각 라운드는 X 샘플로 포맷되며 EOS 토큰과 함께 연결됩니다. 훈련하는 동안 BOS 토큰은 시퀀스의 시작 부분에 추가되고 EOS 토큰은 시퀀스의 끝에 추가되며 녹색 토큰만 손실을 계산하는 데 사용됩니다. 교차 모달 지시 미세 조정 데이터. 오픈 소스 언어 전용 지시 조정 데이터 세트의 효과는 이전 작업[15, 16, 17]에서 입증되었으므로 이러한 언어 전용 데이터 세트의 오디오 데이터를 생성하여 교차 모달 지시 조정 데이터를 구축하는 것이 자연스러운 아이디어입니다. 이 데이터 세트를 구축하는 과정에서 먼저 코드, 많은 수의 기호, URL 및 기타 읽을 수 없는 텍스트를 포함하여 발성에 적합하지 않은 대화를 제거하여 모든 대화 데이터를 신중하게 필터링했습니다. 데이터 품질을 보장하기 위해 두 번째 단계에서는 대화에서 챗봇의 모든 답변을 다시 필터링합니다. 가치 있는 정보가 없는 답변은 삭제합니다. 세 번째 단계에서는 Microsoft Azure 텍스트 음성 변환 API[28]를 사용하여 이러한 데이터 세트에서 사람의 음성 데이터를 생성합니다. 사람의 음성 데이터는 복잡한 지침으로 사용되고 지침 미세 조정 중에 챗봇의 응답이 예측됩니다. 구체적으로 160k 샘플을 포함하는 80k 대화 데이터는 WizardLM[17]에서 선택되고, 155k 샘플을 포함하는 23k 대화 데이터는 ShareGPT[16]에서 선택되고, 192k 샘플을 포함하는 96k 대화 데이터는 GPT-4-LLM[15]에서 선택됩니다. 표 1은 총 199k 대화 데이터와 508k 샘플을 포함하는 데이터 세트의 구체적인 세부 정보를 보여줍니다. 그림 6에 데이터 세트의 몇 가지 예가 나와 있습니다. 이 데이터 세트를 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.A에서 LLaSM-Audio-Instructions로 출시합니다. 사전 인쇄 - 9월 19일, 표 1: LLaSM-Audio-Instructions 데이터. LLaSM-Audio-Instructions 대화 샘플 영어 샘플 160k 159k 중국어 샘플 BITTT 출처 WizardLM ShareGPT GPT-4-LLM 80k 23k 96k 155k 192k 총계 199k 508k 140k 128k 64k 3대 기본색은 무엇입니까? 인간: /dataset/1038/1.mpAgent: 3대 기본색은 빨간색, 파란색, 노란색입니다. 이러한 색상은 다른 색상을 혼합하여 만들 수 없고 다른 모든 색상은 다양한 비율로 조합하여 만들 수 있기 때문에 기본 색상이라고 합니다. 빛에 사용되는 가산 색상 시스템에서 기본 색상은 빨간색, 녹색, 파란색(RGB)입니다. 셀룰러 네트워크는 실험실에서 성능을 위해 어떻게 테스트됩니까? 인간: /dataset/1203/1.mp에이전트: 帮我找一个适合我兄弟的礼物。他 →是一个狂热的体育迷。 인간: /dataset/19791/1.mpAgent: 에이전트: 셀룰러 네트워크는 일반적으로 실험실에서 다음과 같은 특수 장비를 사용하여 테스트됩니다. 셀룰러 네트워크 시뮬레이터 및 테스트 모바일 장치로 사용됩니다. 네트워크 시뮬레이터는 시뮬레이션된 네트워크 트래픽과 조건을 생성하고, 테스트 모바일 장치는 테스트 중인 네트워크의 성능을 측정하는 데 사용됩니다. 일반적으로 측정되는 성능 지표에는 처리량, 대기 시간, 신호 강도 및 오류율이 포함됩니다. 또한 드라이브 테스트 및 워크 테스트와 같은 다양한 테스트도 수행되어 네트워크 성능을 측정합니다. 셀룰러 네트워크의 맥락에서 드라이브 테스트란 무엇입니까? 2.运动装备:根据他喜欢的运动,您可以为他购买一些상关的运动装备,如篮球, 足球, 棒球等. 3.球票:如果可能的话,您可以为他购买他最喜欢的体育比赛的门票, 让他有机4. 运动类书籍/杂志/纪录 Pictures:您可以为他购买一些关于运动的书籍、杂志或纪录 5. 运动类游戏:您可以为他购买一些运动类电子游戏,让他能够가 가중에서 也能享受 运动的乐趣。 인간: /dataset/1203/3.mpDrive 테스트, &quot;드라이브 바이 테스트&quot;라고도 알려진 이 테스트는 차량이 움직이는 동안 셀룰러 네트워크의 성능을 측정하는 데 사용되는 일종의 현장 테스트입니다. 테스트 차량에는 데이터 로거 및 테스트 모바일 장치와 같은 특수 테스트 장비가 장착되어 있으며, 이는 차량이 네트워크 범위의 여러 영역을 이동할 때 네트워크의 성능을 측정하는 데 사용됩니다. 테스트 차량은 일반적으로 미리 정해진 경로를 따라 주행하며, 테스트 장비는 신호 강도, 통화 품질 및 데이터 처리량과 같은 주요 성능 지표에 대한 데이터를 수집하는 데 사용됩니다. 그런 다음 이 데이터를 분석하여 성능이 좋지 않거나 용량 문제가 발생할 수 있는 네트워크 영역을 식별합니다. 드라이브 테스트는 실제 상황에서 셀룰러 네트워크의 성능을 측정하는 데 유용하며 네트워크 운영자가 네트워크 범위, 용량 및 품질과 관련된 문제를 식별하고 해결하는 데 도움이 될 수 있습니다. 그림 6: LLaSM-Audio-Instructions의 데이터 샘플.北京 0:A 사전 인쇄 - 9월 19일, 아프리카의 어느 나라가 다이아몬드로 유명합니까?北京是中國的首道,位于华北地区,是一个历史悠久、文化 底蕴深厚得city市。它拥有丰富的人文景观와历史遗产,如故宫、天安门广场、长city等。此外,North京还是一个现代化得大 city,拥有许多现代化得建筑和设施,如北京塔、國家大剧 다이아몬드는 주로 아프리카, 특히 보츠와나, 민주당과 같은 국가에서 발견됩니다. 콩고공화국, 남아프리카공화국, 시에라리온. 이 다이아몬드는 다이아몬드가 포함된 지질 구조인 킴벌라이트 파이프에서 채굴됩니다. 如果去北京旅游,你有什么建议吗 0:如果您计划去北京旅游,我可以为您提供以下建议: 1.游览北京的著name景点,如故宫、天安门广场、长city等。 2. 品尝北京的特color美食,如烤鸭、豆汁、炸酱面等。 3. 逛逛北京的夜市와商场,购买一些纪念product或者当地特产。 4. 参观一些历史博物馆和文化遗址,了解京的历史화文化。 5. 6. 如果您对中國文化感兴趣,可以参加一些文化体验活动,如school习书法、剪纸、京剧等。 希望这些建议能够对您有所帮助, 如果您需要更多信息, 请 随时告诉我.第一次와 女孩约会有什么建议吗? 에서 以下是一些建议可以考虑的活动: 1.约会前,可以过社交媒体或短信聊天了解对方的兴趣2.约会时间可以选择一个适合双方的时间및地点,例如咖 3. 可以选择一些共同的兴趣爱好进行活动,例如看电影, 演唱会, 运动等。 4.可以尝试一些new鲜的活动,例如射箭、攀岩、烹饪课等。 5.可以选择一些文化活动,例如博物馆、画廊、历史遗迹等。 6. 可以选择一些轻松愉快的活动,例如逛街、逛公园、看 日落等。 성룡은 누구입니까? 0:最重要的是,要根据对方的喜好和兴趣选择活动,并尽weight 让约会过程中的气氛轻松愉快。 Jackie Chan은 홍콩 출신의 유명한 배우이자 스턴트맨입니다. 그는 영화에서 자신의 스턴트를 연기하는 것으로 유명하며 액션 영화 장르에서 가장 인정받고 성공적인 배우 중 한 명이 되었습니다. 그는 오버필름에 출연했으며 세계에서 가장 유명하고 인지도가 높은 유명인 중 한 명입니다. 세계에서 가장 부유한 사람은 누구입니까? 0:중국의 유명인사? 中國最著名的诗人之一是李白(701年-762年),他被誉为 “诗仙”,是唐代最伟大的诗人之一。他的诗歌清new豪放,充满想象력과 浪漫主义情怀,对后world诗歌创작품产生了深远 的影响。 他的代表有작품哪些? 0:0:0:0: 현재 세계에서 가장 부유한 사람은 사업가이자 산업 디자이너, 엔지니어인 일론 머스크입니다. 그는 SpaceX의 설립자 겸 CEO, Tesla Inc.의 공동 설립자 겸 CEO, Neuralink의 공동 설립자, The Boring Company의 설립자입니다. 2021년 현재 머스크의 순자산은 약 2,510억 달러로 추산되어 세계에서 가장 부유한 사람이 되었습니다. 이백의 대작곡이 많음,其中最著名的诗歌包括 《将进酒》, 《庐山谣寄卢侍御虚舟》, 《夜泊牛渚怀古》, 《早发白帝》城》等。这些诗歌充满了豪放不羁的情怀和奔放的诗风,被 誉为李白的代表작품. 그림 7: 예
--- EXPERIMENT ---
s는 LLaSM이 인간이 인공 지능과 상호 작용하는 보다 편리하고 자연스러운 방법을 보여준다는 것을 보여줍니다.특히, 우리는 또한 대규모 음성 지시 따르기 데이터 세트 LLaSM-Audio-Instructions를 릴리스합니다.코드와 데모는 https://github.com/LinkSoul-AI/LLASM 및 https://huggingface.co/spaces/LinkSoul/LLaSM에서 사용할 수 있습니다.LLaSM-Audio-Instructions 데이터 세트는 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions에서 사용할 수 있습니다.1 서론 음성에는 의미 정보가 포함되어 있고 동시에 억양과 같은 준언어 정보가 포함되어 있으며 텍스트보다 더 많은 양의 정보를 전달합니다.또한 음성은 인간이 인공 지능과 상호 작용하는 보다 편리하고 자연스러운 방법입니다.따라서 범용 보조 장치를 개발할 때 음성 및 언어 지시를 따르는 것이 중요합니다.그러나 대부분의 대규모 언어 모델[1, 2, 3]은 텍스트 입력만 받기 때문에 대규모 언어 모델의 기능이 제한됩니다. 시각-언어 다중 모달 모델[4, 5, 6, 7, 8, 9]은 시각 정보를 이해하는 기능을 제공하여 일반 인공 지능(AGI)을 향한 큰 발걸음을 내디뎠지만, 사람이 텍스트 지침을 입력하여 작업을 입력하는 것은 여전히 불편합니다. 계단식 패러다임 방법[10, 11]은 자동 음성 인식(ASR) 모델을 사용하여 음성 입력을 텍스트 입력으로 변환한 다음, 모델은 텍스트 입력으로 작업을 처리할 수 있습니다. 그러나 여전히 음성에서 텍스트로 모달 변환하는 동안 정보 소비로 이어지고 ASR 시스템의 실수를 가져올 수 있습니다. 최근에는 대규모 언어 모델을 사용하여 음성과 텍스트를 처리하고 생성하는 데 중점을 둔 음성-언어 다중 모달 모델[12, 13]이 다중 모달 콘텐츠를 이해하고 생성할 수 있습니다. 음성 신호는 개별 토큰으로 인코딩된 다음 개별 음성 토큰이 LLM의 어휘로 확장됩니다. 이런 식으로 LLM은 풍부한 멀티모달 데이터와 방대한 컴퓨팅 리소스로 재교육되어야 합니다. 이 논문에서 우리는 크로스 모달 대화 능력을 갖춘 대규모 음성 및 언어 모델인 LLaSM을 제안합니다. 이 모델은 음성 및 언어 지침을 이해하고 따를 수 있습니다. LLaVA [6]의 방식에 따라 잘 훈련된 음성 모달 인코더와 LLM을 활용하여 LLaSM을 리소스 친화적으로 만듭니다. 구체적으로, 우리는 Whisper [14]를 음성 인코더로 사용하여 음성 신호를 임베딩으로 인코딩합니다. 그런 다음 모달 어댑터는 A PREPRINT - SEPTEMBER 19, 음성 임베딩을 대규모 언어 모델의 입력 텍스트 임베딩과 정렬하는 방법을 학습합니다. 음성 임베딩과 텍스트 임베딩을 연결하여 인터리브 시퀀스를 형성한 다음 인터리브 시퀀스를 LLM에 입력하여 감독 미세 조정을 수행합니다. 교육 과정은 두 단계로 나뉩니다. 첫 번째 단계에서는 모달 적응 사전 학습을 위해 공개 ASR 데이터 세트를 사용합니다. 음성 인코더와 LLM은 동결되고 모달 어댑터만 음성 및 텍스트 임베딩을 정렬하도록 학습됩니다. 대부분의 모델 매개변수가 동결된 상태로 유지되므로 이 단계에서는 모달 어댑터의 매개변수 중 일부만 학습되므로 리소스를 많이 소모하지 않습니다. 두 번째 단계에서는 모델에 모달 대화를 처리하고 다중 모달 지침을 처리할 수 있는 용량을 제공하기 위해 모달 간 지침 데이터를 사용하여 학습합니다. 모달 어댑터와 언어 모델의 매개변수가 모달 간 지침 미세 조정을 위해 업데이트되는 동안 음성 인코더는 동결됩니다. 기존의 오픈 소스 음성-텍스트 모달 간 지침 따르기 데이터 세트가 부족하다는 점에 유의해야 하므로 음성 텍스트 모달 간 지침 따르기 데이터 세트 LLaSM-Audio-Instructions를 빌드하여 릴리스합니다. 데이터 세트는 GPT4-LLM[15], ShareGPT[16], WizardLM[17]에서 대화를 신중하게 선택하고 텍스트 음성 변환 기술을 사용하여 방대한 대화 오디오 데이터를 생성하여 구성되었습니다. 총 199k 대화가 포함되어 있으며, 여기에는 80k 중국어 오디오 샘플과 428k 영어 오디오 샘플이 있습니다. 이는 우리가 아는 가장 큰 중국어 및 영어 음성-텍스트 교차 모달 지시 수행 데이터 세트입니다. 저희 논문은 다음과 같은 기여를 합니다. • 우리는 음성 언어 지시를 이해하고 따를 수 있는 음성 언어 다중 모달 모델을 구축하여 인간이 인공 지능과 상호 작용할 수 있는 보다 편리하고 자연스러운 방법을 제공합니다. • 우리는 대규모 중국어 및 영어 음성-텍스트 교차 모달 지시 수행 데이터 세트인 LLaSM-Audio-Instructions를 구축하여 릴리스합니다. 저희는 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions에서 데이터를 릴리스합니다. • https://github.com/LinkSoul-AI/LLASM에서 코드를 공개하고 https://huggingface.co/spaces/LinkSoul/LLaSM에서 데모를 볼 수 있습니다.2 관련 작업 Vision Large Language Model은 최근 상당한 주목을 받았습니다[4, 5, 6, 7, 8, 9]. 대부분은 사전 훈련된 LLM과 비전 인코더를 활용하여 비전 작업을 수행합니다. Flamingo[18]는 게이트 교차 주의를 사용하여 사전 훈련된 비전 인코더와 언어 모델을 정렬하고 수십억 개의 이미지-텍스트 쌍으로 훈련합니다. BLIP-2[19]는 Q-Former가 있는 Flan-T5[20]를 사용하여 시각적 특징을 언어 모델과 효율적으로 정렬합니다. 수십억 개의 매개변수를 특징으로 하는 Palm-E[5]는 540B PaLM[2]과 22B Vision Transformer[21]를 가장 큰 비전-언어 모델로 통합합니다. LLAVA[6]는 사전 훈련된 CLIP[22] 시각적 인코더와 LLAMA[3]를 활용하고 GPT4 지원 시각적 지침 데이터에 대한 지침 튜닝을 수행합니다. GPT-4[4]도 강력한 시각적 이해 및 추론 능력을 보여줍니다. 시각 도메인에서 다중 모달 대규모 언어 모델의 성공은 음성 도메인 연구에도 많은 영감을 불어넣었습니다. 음성 대규모 언어 모델은 시각 다중 모달 LLM의 성공으로 인해 점점 더 많은 관심을 얻었습니다. 계단식 패러다임 방법[10, 11]은 자동 음성 인식(ASR) 모델을 사용하여 음성 입력을 텍스트 입력으로 변환하는데, 이는 여전히 정보 소비로 이어지고 ASR 시스템의 실수를 가져올 수 있습니다. 최근에는 대규모 언어 모델을 사용하여 음성과 텍스트를 처리하고 생성하는 데 중점을 둔 음성 언어 다중 모달 모델[12, 13]이 다중 모달 콘텐츠를 이해하고 생성할 수 있습니다. 음성 신호는 이산 토큰으로 인코딩되고, 이산 음성 토큰은 LLM의 어휘로 확장됩니다. 이런 식으로 LLM은 풍부한 멀티모달 데이터와 방대한 컴퓨팅 리소스로 재교육되어야 합니다. 3 접근 방식 3.1 모델 멀티모달 모델을 교육하는 초점은 멀티모달의 크로스 모달 보완 정보를 융합하고 잘 훈련된 대규모 언어 모델의 기능을 효과적으로 활용하는 것입니다. LLaSM 모델 아키텍처는 그림 1에 나와 있습니다. 먼저 Whisper[14]를 사용하여 원시 오디오 데이터를 임베딩으로 인코딩한 다음, 사전 교육 단계에서 모달 어댑터를 교육하여 오디오 임베딩과 텍스트 임베딩을 맞춥니다. 오디오 임베딩과 텍스트 임베딩은 서로 연결되어 대규모 언어 모델에 입력할 인터리브된 입력 시퀀스를 형성합니다. 중국어와 영어 모두에 대한 기능을 갖춘 Chinese-LLAMA2-7B[23]를 LLM으로 선택했습니다. 크로스 모달 명령어 미세 조정 단계에서 모달 어댑터와 LLM은 다중 작업으로 훈련됩니다.오디오 텍스트 TOTT 10. 모달 인코더 모달 어댑터 토큰화 및 임베드 + 사전 인쇄 - 9월 19일, 임베딩 언어 모델 임베딩 그림 1: LLaSM의 모델 프레임워크 사전 훈련 단계. 이 단계에서 모달 인코더와 LLM은 동결된 상태로 유지됩니다. LLM이 모달 인코더의 오디오 임베딩을 이해할 수 있도록 모달 어댑터는 공개 ASR 데이터로 훈련되어 텍스트와 오디오 임베딩을 정렬합니다. ASR 데이터의 데이터 샘플(오디오 데이터, 텍스트 레이블)은 (간단한 명령어, 오디오 데이터, 텍스트 레이블)의 튜플로 포맷되며, 여기서 간단한 명령어는 자동 음성 인식 명령어입니다. 오디오 데이터의 언어에 따라 그림에 나열된 영어 간단한 명령어나 그림 3에 나열된 중국어 간단한 명령어가 선택됩니다. 사전 학습 멀티모달 시퀀스 Xsample의 통합된 형식은 그림 4에 나와 있습니다. 각 데이터 샘플은 X 샘플로 포맷된 다음, 텍스트 시퀀스의 오디오 패치 임베딩을 모달 어댑터의 오디오 임베딩으로 바꿉니다. 마지막으로 인터리브된 입력 임베딩은 대규모 언어 모델에 입력됩니다. 학습 목표는 각 데이터 샘플의 텍스트 레이블을 예측하는 것입니다. • &quot;다음 음성을 텍스트로 필사합니다.&quot; • &quot;말한 단어를 서면 텍스트로 변환합니다.&quot; &quot;말을 서면 필사본으로 변환합니다.&quot; &quot;구두 의사소통을 서면 텍스트로 녹음합니다.&quot; &quot;구두 메시지를 서면 형태로 렌더링합니다.&quot; &quot;말한 언어를 서면 스크립트로 전치합니다.&quot; &quot;말한 단어를 서면 텍스트로 번역합니다.&quot; &quot;말한 단어를 적습니다.&quot; • &quot;말한 담화를 서면 문서로 변환합니다.&quot; • &quot;말한 발화를 서면 필사본으로 변환합니다.&quot; &quot;말한 음성의 서면 기록을 만듭니다.&quot; 그림 2: 간단한 영어 지침. &quot; “请将以下语음을转译为文本&quot; “请将这段语음을 转换成文字。 &quot;能否把这段语语转成文字字?&quot; &quot;把这段语转写成文字 . “将此录음을 转写成文字 . “把这段음을 频转换成文字 . &quot;能否把这段录音转成文字?” “把这段语음을 转화한 문자본입니다. 그림 3: 중국어 간단한 지침. 교차 모달 명령 미세 조정. 이 단계에서는 모달 인코더만 고정되고 모달 어댑터와 LLM은 다중 작업으로 공동 훈련됩니다. 우리는 여러 대화 데이터를 사용하여 복잡한 교차 모달 지침을 구축합니다. Microsoft Azure TTS(텍스트 음성 변환) API를 사용하여 인간의 질문을 오디오 데이터로 생성한 다음, 챗봇의 응답을 예측하는 것이 훈련 목표입니다. 한 라운드의 질문과 답변은 다중 모달 시퀀스 X 샘플로 처리되며, 여러 라운드의 질문과 답변은 EOS 토큰과 연결됩니다. 통합된 크로스 모달 지시 미세 조정 시퀀스 형식은 그림 5에 나와 있습니다. 다중 작업 지시가 있는 텍스트 전용 대화 데이터의 효과성은 여러 오픈 소스 언어 전용 지시 조정 작업[15, 16, 17]에서 입증되었으므로 크로스 모달 지시는 다중 모달 지시를 따르는 용량을 개선할 수 있습니다. 3.2 데이터 수집 LLM이 오디오 신호를 이해할 수 있도록 여러 공개 ASR 데이터 세트를 수집하여 자동 음성 인식의 간단한 지시로 모달 적응 사전 학습 데이터를 형성합니다. 그리고 크로스 모달 지시 조정을 위해 여러 오픈 소스 언어 전용 지시 조정 데이터 세트를 사용하여 음성 데이터를 생성하여 크로스 모달 지시 미세 조정 데이터를 구축합니다. 세부 사항은 다음과 같습니다. 모달 적응 사전 학습 데이터. 텍스트와 오디오의 임베딩을 정렬하기 위해 Aishell[24], LibriSpeech[25], Magicdata[26] 및 Primewords[27]를 포함하여 영어와 중국어로 된 여러 공개 ASR 데이터 세트를 수집합니다. ASR 데이터의 데이터 샘플은 일반적으로 음성 오디오와 텍스트 발화의 쌍으로 구성되며, 특히 작업 지침으로 데이터 샘플에 간단한 지침을 추가할 때 그렇습니다. 이러한 간단한 지침은 그림 2와 그림 3에 나열되어 있으며, 이는 영어와 중국어로 된 자동 음성 인식 작업의 다른 표현입니다. 사전 학습하는 동안 간단한 지침과 오디오 데이터는 오디오 데이터의 텍스트 레이블을 예측하기 위해 모델에 입력됩니다.A 사전 인쇄 - 9월 19일, X 샘플 콘텐츠 사용자 = {audiotoken} + &quot;\n&quot; + Isimple(또는 콘텐츠 사용자 = = B_INST + B_SYS + SYSTEM + E_SYS + {contentuser} + E_INST + {content gpt} Isimple + &quot;\n&quot; + {audio token}) 콘텐츠 gpt 오디오 토큰 = TEXT_LABEL = AUDIO_START_TOKEN + AUDIO_PATCH_TOKEN * {audio_token_len} + AUDIO_END_TOKEN 그림 4: 사전 학습을 위한 샘플 시퀀스 형식. Llama-2의 방식을 따르고 B_INST = &#39;[INST]&#39;, E_INST = &#39;[/INST]&#39;, B_SYS = &#39;&lt;<SYS> &gt; \n&#39;, E_SYS = &#39;\n &lt;</SYS> &gt; \n\n&#39;. SYSTEM = &#39;당신은 도움이 되는 언어 및 음성 조수입니다. 사용자가 제공하는 음성 내용을 이해하고 자연어를 사용하여 다양한 작업을 수행할 수 있습니다.&#39;, 그리고 TEXT_LABEL은 ASR 데이터 샘플의 텍스트 레이블입니다. audio_token_len은 기본적으로 64로 설정됩니다. 특수 오디오 토큰이 사용됩니다. AUDIO_START_TOKEN = &#39;<au_start> &#39;, 오디오_엔드_토큰 = &#39;<au_end> &#39;, 오디오_패치_토큰 = &#39;<au_patch> &#39;. 콘텐츠 사용자는 오디오 토큰과 Isimple로 구성되며, 여기서 Isimple은 간단한 명령어이며 오디오 토큰 앞이나 뒤에 무작위로 배치됩니다. BOS 토큰과 EOS 토큰을 훈련하는 동안 시퀀스의 시작과 끝에 각 샘플에 추가되지만, 녹색 토큰만 손실을 계산하는 데 사용됩니다. Xinstruction X 샘플 콘텐츠 사용자 오디오 토큰 =<BOS> + X 샘플 +<EOS> + X 샘플 +<EOS> + *** +<EOS> = B_INST + B_SYS + SYSTEM + E_SYS + {content user} + E_INST + {content gpt} = {audiotoken}, content gpt = TEXT_RESPONSE = AUDIO_START_TOKEN + AUDIO_PATCH_TOKEN * {audio_token_len} + AUDIO_END_TOKEN 그림 5: 교차 모달 명령어 미세 조정을 위한 샘플 시퀀스 형식. Llama-2의 방식을 따르고, B_INST = &#39;[INST]&#39;, E_INST = &#39;[/INST]&#39;, B_SYS=&#39;&lt;<SYS> &gt; \n&#39;, E_SYS = &#39;\n &lt;</SYS> &gt; \n\n&#39;. SYSTEM = &#39;당신은 도움이 되는 언어 및 음성 조수입니다. 사용자가 제공하는 음성 내용을 이해하고 자연어를 사용하여 다양한 작업을 수행할 수 있습니다.&#39;, 그리고 TEXT_RESPONSE는 챗봇의 텍스트 응답입니다. audio_token_len은 기본적으로 64로 설정됩니다. 특수 오디오 토큰이 사용됩니다. AUDIO_START_TOKEN = &#39;<au_start> &#39;, 오디오_엔드_토큰 = &#39;<au_end> &#39;, 오디오_패치_토큰 = &#39;<au_patch> &#39;. 콘텐츠 사용자는 훈련 중에 오디오 임베딩으로 대체될 오디오 토큰입니다. 질문과 답변의 각 라운드는 X 샘플로 포맷되며 EOS 토큰과 함께 연결됩니다. 훈련하는 동안 BOS 토큰은 시퀀스의 시작 부분에 추가되고 EOS 토큰은 시퀀스의 끝에 추가되며 녹색 토큰만 손실을 계산하는 데 사용됩니다. 교차 모달 지시 미세 조정 데이터. 오픈 소스 언어 전용 지시 조정 데이터 세트의 효과는 이전 작업[15, 16, 17]에서 입증되었으므로 이러한 언어 전용 데이터 세트의 오디오 데이터를 생성하여 교차 모달 지시 조정 데이터를 구축하는 것이 자연스러운 아이디어입니다. 이 데이터 세트를 구축하는 과정에서 먼저 코드, 많은 수의 기호, URL 및 기타 읽을 수 없는 텍스트를 포함하여 발성에 적합하지 않은 대화를 제거하여 모든 대화 데이터를 신중하게 필터링했습니다. 데이터 품질을 보장하기 위해 두 번째 단계에서는 대화에서 챗봇의 모든 답변을 다시 필터링합니다. 가치 있는 정보가 없는 답변은 삭제합니다. 세 번째 단계에서는 Microsoft Azure 텍스트 음성 변환 API[28]를 사용하여 이러한 데이터 세트에서 사람의 음성 데이터를 생성합니다. 사람의 음성 데이터는 복잡한 지침으로 사용되고 지침 미세 조정 중에 챗봇의 응답이 예측됩니다. 구체적으로 160k 샘플을 포함하는 80k 대화 데이터는 WizardLM[17]에서 선택되고, 155k 샘플을 포함하는 23k 대화 데이터는 ShareGPT[16]에서 선택되고, 192k 샘플을 포함하는 96k 대화 데이터는 GPT-4-LLM[15]에서 선택됩니다. 표 1은 총 199k 대화 데이터와 508k 샘플을 포함하는 데이터 세트의 구체적인 세부 정보를 보여줍니다. 그림 6에 데이터 세트의 몇 가지 예가 나와 있습니다. 이 데이터 세트를 https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.A에서 LLaSM-Audio-Instructions로 출시합니다. 사전 인쇄 - 9월 19일, 표 1: LLaSM-Audio-Instructions 데이터. LLaSM-Audio-Instructions 대화 샘플 영어 샘플 160k 159k 중국어 샘플 BITTT 출처 WizardLM ShareGPT GPT-4-LLM 80k 23k 96k 155k 192k 총계 199k 508k 140k 128k 64k 3대 기본색은 무엇입니까? 인간: /dataset/1038/1.mpAgent: 3대 기본색은 빨간색, 파란색, 노란색입니다. 이러한 색상은 다른 색상을 혼합하여 만들 수 없고 다른 모든 색상은 다양한 비율로 조합하여 만들 수 있기 때문에 기본 색상이라고 합니다. 빛에 사용되는 가산 색상 시스템에서 기본 색상은 빨간색, 녹색, 파란색(RGB)입니다. 셀룰러 네트워크는 실험실에서 성능을 위해 어떻게 테스트됩니까? 인간: /dataset/1203/1.mp에이전트: 帮我找一个适合我兄弟的礼物。他 →是一个狂热的体育迷。 인간: /dataset/19791/1.mpAgent: 에이전트: 셀룰러 네트워크는 일반적으로 실험실에서 다음과 같은 특수 장비를 사용하여 테스트됩니다. 셀룰러 네트워크 시뮬레이터 및 테스트 모바일 장치로 사용됩니다. 네트워크 시뮬레이터는 시뮬레이션된 네트워크 트래픽과 조건을 생성하고, 테스트 모바일 장치는 테스트 중인 네트워크의 성능을 측정하는 데 사용됩니다. 일반적으로 측정되는 성능 지표에는 처리량, 대기 시간, 신호 강도 및 오류율이 포함됩니다. 또한 드라이브 테스트 및 워크 테스트와 같은 다양한 테스트도 수행되어 네트워크 성능을 측정합니다. 셀룰러 네트워크의 맥락에서 드라이브 테스트란 무엇입니까? 2.运动装备:根据他喜欢的运动,您可以为他购买一些상关的运动装备,如篮球, 足球, 棒球等. 3.球票:如果可能的话,您可以为他购买他最喜欢的体育比赛的门票, 让他有机4. 运动类书籍/杂志/纪录 Pictures:您可以为他购买一些关于运动的书籍、杂志或纪录 5. 运动类游戏:您可以为他购买一些运动类电子游戏,让他能够가 가중에서 也能享受 运动的乐趣。 인간: /dataset/1203/3.mpDrive 테스트, &quot;드라이브 바이 테스트&quot;라고도 알려진, 차량이 움직이는 동안 셀룰러 네트워크의 성능을 측정하는 데 사용되는 일종의 현장 테스트입니다. 테스트 차량에는 데이터 로거 및 테스트 모바일 장치와 같은 특수 테스트 장비가 장착되어 있으며, 이는 차량이 네트워크 범위의 여러 영역을 이동할 때 네트워크의 성능을 측정하는 데 사용됩니다. 테스트 차량은 일반적으로 미리 정해진 경로를 따라 주행하며, 테스트 장비는 신호 강도, 통화 품질 및 데이터 처리량과 같은 주요 성능 지표에 대한 데이터를 수집하는 데 사용됩니다. 그런 다음 이 데이터를 분석하여 성능이 좋지 않거나 용량 문제가 발생할 수 있는 네트워크 영역을 식별합니다. 드라이브 테스트는 실제 상황에서 셀룰러 네트워크의 성능을 측정하는 데 유용하며 네트워크 운영자가 네트워크 범위, 용량 및 품질과 관련된 문제를 식별하고 해결하는 데 도움이 될 수 있습니다. 그림 6: LLaSM-Audio-Instructions의 데이터 샘플.北京 0:A 사전 인쇄 - 9월 19일, 아프리카의 어느 나라가 다이아몬드로 유명합니까?北京是中國的首道,位于华北地区,是一个历史悠久、文化 底蕴深厚得城市。它拥有丰富的人文景观와历史遗产,如故宫、天安门广场、长city等。此外,North京还是一个现代化得大 city,拥有许多现代化得建筑和设施,如北京塔、國家大剧 다이아몬드는 주로 아프리카, 특히 보츠와나, 민주당과 같은 국가에서 발견됩니다. 콩고공화국, 남아프리카공화국, 시에라리온. 이 다이아몬드는 다이아몬드가 포함된 지질 구조인 킴벌라이트 파이프에서 채굴됩니다. 如果去北京旅游,你有什么建议吗 0:如果您计划去北京旅游,我可以为您提供以下建议: 1.游览北京的著name景点,如故宫、天安门广场、长city等。 2. 品尝北京的特color美食,如烤鸭、豆汁、炸酱面等。 3. 逛逛北京的夜市와商场,购买一些纪念product或者当地特产。 4. 参观一些历史博物馆和文化遗址,了解京的历史화文化。 5. 6. 如果您对中國文化感兴趣,可以参加一些文化体验活动,如school习书法、剪纸、京剧等。 希望这些建议能够对您有所帮助, 如果您需要更多信息, 请 随时告诉我.第一次와 女孩约会有什么建议吗? 에서 以下是一些建议可以考虑的活动: 1.约会前,可以过社交媒体或短信聊天了解对方的兴趣2.约会时间可以选择一个适合双方的时间및地点,例如咖 3. 可以选择一些共同的兴趣爱好进行活动,例如看电影, 演唱会, 运动等。 4.可以尝试一些new鲜的活动,例如射箭、攀岩、烹饪课等。 5.可以选择一些文化活动,例如博物馆、画廊、历史遗迹等。 6. 可以选择一些轻松愉快的活动,例如逛街、逛公园、看 日落等。 성룡은 누구입니까? 0:最重要的是,要根据对方的喜好和兴趣选择活动,并尽weight 让约会过程中的气氛轻松愉快。 Jackie Chan은 홍콩 출신의 유명한 배우이자 스턴트맨입니다. 그는 자신의 영화에서 스턴트를 직접 수행하는 것으로 유명하며 액션 영화 장르에서 가장 인정받고 성공적인 배우 중 한 명이 되었습니다. 그는 오버필름에 출연했으며 세계에서 가장 유명하고 인지도가 높은 유명인 중 한 명입니다. 세계에서 가장 부유한 사람은 누구입니까? 0:중국의 유명인사? 中國最著名的诗人之一是李白(701年-762年),他被誉为 “诗仙”,是唐代最伟大的诗人之一。他的诗歌清new豪放,充满想象력과 浪漫主义情怀,对后world诗歌创작품产生了深远 的影响。 他的代表有작품哪些? 0:0:0:0: 현재 세계에서 가장 부유한 사람은 사업가이자 산업 디자이너, 엔지니어인 일론 머스크입니다. 그는 SpaceX의 설립자 겸 CEO, Tesla Inc.의 공동 설립자 겸 CEO, Neuralink의 공동 설립자, The Boring Company의 설립자입니다. 2021년 현재 머스크의 순자산은 약 2,510억 달러로 추산되어 세계에서 가장 부유한 사람이 되었습니다. 이백의 대작곡이 많음,其中最著名的诗歌包括 《将进酒》, 《庐山谣寄卢侍御虚舟》, 《夜泊牛渚怀古》, 《早发白帝》城》等。这些诗歌充满了豪放不羁的情怀和奔放的诗风,被 誉为李白的代表작품. 그림 7: 실험의 예. A PREPRINT - 9월 19,4 실험 그림 7에서 볼 수 있듯이 제안된 모델인 LLaSM은 중국어와 영어 음성을 적응적으로 인식하고 응답할 수 있습니다. 그림 7은 이중 언어 환경에서 LLaSM의 효과를 더욱 잘 보여줍니다. 사전 처리 단계로 음성-텍스트 변환에 의존하는 기존 모델과 달리 LLaSM은 음성 입력을 직접 처리할 수 있어 실행 효율성이 향상됩니다. 또한 LLaSM은 여러 언어와 시나리오를 지원할 수 있어 적용 범위가 확장됩니다. 따라서 LLaSM은 편리하고 상호 작용하는 인간-인공지능 커뮤니케이션을 위한 유망한 모델입니다. 5
--- CONCLUSION ---
이 연구는 음성 및 언어 지침을 이해하고 따를 수 있는 크로스 모달 대화 능력을 갖춘 대규모 언어 모델인 LLaSM을 제시합니다. 실험 결과 LLaSM은 인간이 인공 지능과 상호 작용하는 데 더 편리하고 자연스러운 방법을 보여줍니다. 구체적으로 크로스 모달 음성 및 언어 지침 데이터의 부족을 완화하기 위해 대규모 음성 지침 따르기 데이터 세트 LLaSM-Audio-Instructions를 구축합니다. 이는 우리가 아는 가장 큰 중국어 및 영어 음성-텍스트 크로스 모달 지침 따르기 데이터 세트입니다. 마지막으로 LLaSM에 시각적 기능을 쉽게 제공할 수 있는 시각적 모달 인코더를 채택하여 향후 작업에서 시각 및 오디오 모달리티를 결합하는 것을 탐구할 것입니다. 참고문헌 [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 및 Dario Amodei. 언어 모델은 소수 학습자입니다, 2020. [2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, 다프네 이폴리토, 데이비드 Luan, 임현택, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. Palm: 경로를 사용한 확장 언어 모델링, 2022. [3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, 이젠야, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 및 Thomas Scialom. Llama 2: 개방형 기반 및 미세 조정된 채팅 모델, 2023년. [4] OpenAI. Gpt-4 기술 보고서, 2023. [5] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence. Palm-e: 구체화된 멀티모달 언어 모델, 2023. [6] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee. 시각 지시 튜닝, 2023. [7] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델을 통한 비전 언어 이해 향상, 2023.A 사전 인쇄 - 9월 19일,[8] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin 및 Ishan Misra. 이미지 바인딩: 모든 것을 묶는 하나의 임베딩 공간, 2023. [9] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang 및 Xinlong Wang. 다중 모드의 생성적 사전 훈련, 2023. [10] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu 및 Yueting Zhuang. Hugginggpt: 포옹 얼굴에서 chatgpt와 그 친구들과 함께 AI 작업 해결, 2023. [11] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao 및 Shinji Watanabe. Audiogpt: 음성, 음악, 소리 및 말하는 머리 이해 및 생성, 2023. [12] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou 및 Xipeng Qiu. Speechgpt: 고유한 교차 모달 대화 능력으로 대규모 언어 모델 강화, 2023. [13] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka 및 Christian Frank. Audiopalm: 말하고 들을 수 있는 대규모 언어 모델, 2023. [14] Alec Radford, 김종욱, Tao Xu, Greg Brockman, Christine McLeavey 및 Ilya Sutskever. 대규모 약한 감독을 통한 강력한 음성 인식, 2022. [15] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley 및 Jianfeng Gao. gpt-4를 사용한 명령 튜닝, 2023. [16] Dom Eccleston. 공유 https://github.com/domeccleston/sharegpt, 2023. [17] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao 및 Daxin Jiang. Wizardlm: 복잡한 지침을 따르도록 대규모 언어 모델 강화, 2023. [18] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan. Flamingo: few-shot 학습을 위한 시각적 언어 모델, 2022. [19] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 고정 이미지 인코더 및 대규모 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 훈련, 2023. [20] Hong Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le 및 Jason Wei. 스케일링 명령어 미세 조정 언어 모델, 2022. [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다: 규모에 따른 이미지 인식을 위한 변압기, 2021. [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 자연어 감독을 통한 학습 가능한 시각적 모델, 2021. [23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Yemin Shi. Chinese llama2 7b. arXiv, 2023. [24] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, Hao Zheng. Aishell-1: 오픈소스 중국어 음성 코퍼스 및 음성 인식 기준선. Oriental COCOSDA 2017, 제출된 페이지, 2017. [25] Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur. Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스. Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, 5206-5210페이지. IEEE, 2015. [26] Magic data technology co., ltd., 2019. [27] Ltd. Primewords Information Technology Co. Primewords 중국어 코퍼스 세트 1, 2018. https://www. primewords.cn.[28] Microsoft. Microsoft azure text-to-speech api. ai-services/ai-speech. 사전 인쇄 - 9월 19일, https://azure.microsoft.com/en-us/products/
