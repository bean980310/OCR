--- ABSTRACT ---
ive Tri-agent 생성 파이프라인에 의한 요약 Wen Xiao+ Yujia Xie Giuseppe Carenini† Pengcheng He *University of British Columbia, Vancouver, Canada # Microsoft Azure AI {carenini}@cs.ubc.ca, {wxiao, yujiaxie, penhe}@microsoft.com 초록 ChatGPT와 같은 대규모 언어 모델의 출력을 암묵적 사용자 선호도에 맞게 조정하는 것은 인상적인 생성 기능에도 불구하고 여전히 어려운 과제입니다. 이 논문에서는 생성기, 강사, 편집기로 구성된 3중 에이전트 생성 파이프라인을 제안하여 출력 개인화를 향상시킵니다. 생성기는 초기 출력을 생성하고 강사는 사용자 선호도에 따라 편집 지침을 자동으로 생성하며 편집기는 해당 선호도에 맞게 출력을 세부적으로 조정합니다. 추론 전용 대규모 언어 모델(ChatGPT)은 생성기와 편집기 역할을 모두 하며, 더 작은 모델이 출력 생성을 안내하는 강사 역할을 합니다. 대규모 편집기 모델의 피드백을 활용하여 지침 생성을 최적화하는 편집기 주도 강화 학습을 사용하여 강사를 훈련시킵니다. 두 개의 추상 요약 데이터 세트에 대한 실험 결과는 사용자 기대에 더 잘 부합하는 출력을 생성하는 데 있어서 우리 접근 방식의 효과를 보여줍니다.1
--- INTRODUCTION ---
InstructGPT(Ouyang et al., 2022) 및 ChatGPT2와 같은 뛰어난 모델에서 예시되는 대규모 언어 모델은 자연어 처리(NLP) 분야에서 필수적인 리소스로 부상했습니다. 이러한 모델은 기계 번역, 질의 응답 및 텍스트 요약을 포함한 광범위한 NLP 작업에서 뛰어난 수준의 능력을 보여주었습니다. 언어 기반 기술에서 추가 혁신을 주도할 수 있는 잠재력에 비추어 연구 커뮤니티는 대규모 언어 모델을 탐색하고 발전시키는 데 점점 더 열정을 보였습니다. 그러나 이러한 모델이 달성한 인상적인 생성 품질에도 불구하고 사용자의 선호도에 맞게 출력을 조정하는 데 지속적인 과제가 있습니다(Liu et al., 2022b). https://github.com/에서 ¹코드는 Wendy-Xiao/chatgpt_editing_summ에서 사용할 수 있습니다 2 https://openai.com/blog/chatgpt 일반 답변 수정된 답변 개인화된 답변 반복 반복 편집기 교정자 지침 훈련된 강사 생성기 생성기 생성기 일회성 생성 자체 교정 쿼리 Tri-Agent 그림 1: 다양한 생성 패러다임 간 비교. 왼쪽은 일반적인 일회성 생성 프로세스이고 가운데는 Welleck et al. (2022)에서 제공하는 것으로, 훈련된 교정자를 사용하여 생성된 텍스트를 교정하며 일반적으로 환각이나 독성 제거와 같은 특정 문제를 다루고 오른쪽은 제안된 tri-agent 파이프라인입니다. 여러 시나리오에서 언어 모델의 출력이 사용자의 선호도나 기대를 일관되게 충족하지 못하는 것으로 관찰되었습니다(Bubeck et al., 2023). 이러한 한계를 해결하기 위한 널리 퍼진 접근 방식에는 사용자의 목표에 더 잘 부합하는 출력을 생성하도록 모델을 조종하기 위한 프롬프트를 신중하게 만드는 것이 포함됩니다. 그럼에도 불구하고 기존 연구(Reid와 Neubig, 2022)에서 언급했듯이 언어 모델의 기존 일회성 왼쪽에서 오른쪽으로 생성 프로세스는 인간이 일반적으로 사용하는 반복적 정제 및 편집 접근 방식과 대조됩니다. 나아가 이전 연구(Gu et al., 2019; Reid와 Zhong, 2021)는 단일 편집 반복을 사용하더라도 일회성 생성에 비해 생성 및 편집 프로세스의 효능을 입증했습니다. 이러한 결과에 동기를 부여받은 이 논문은 대규모 언어 모델(ChatGPT)을 자동 반복 편집 파이프라인에 통합하는 방법을 살펴봅니다. 질의 답변 수정 부활절에 핫크로스번을 좋아하지 않는 사람이 있을까요? 영국에서는 십자가가 얹힌 매운 달콤한 롤빵을 너무 좋아해서 일년 내내 먹곤 했습니다. 엘리자베스 여왕 1세가 십자가가 너무 가톨릭적이라고 생각하여 금지하려고 할 때까지는 말입니다. 이러한 움직임은 큰 반향을 불러일으켰고, 그래서 그녀는 타협하여 그것들은 성금요일, 크리스마스 및 장례식에만 판매할 수 있다고 말했고, 그 이후로 부활절의 주요 음식이 되었습니다.요즘 슈퍼마켓은 고객을 유혹하기 위해 미니어처 크기부터 토피 퍼지 청크, 사과와 계피와 같은 새로운 맛까지 점점 더 많은 변형을 내놓습니다.이번 부활절에 가장 좋은 배치는 다음과 같습니다...... TL;DR: Harriet Arkell은 이번 부활절에 제공되는 최고의 핫 크로스 번을 테스트했습니다.최고의 제품은 다음과 같습니다.M&amp;S 토피 퍼지 &amp; 벨기에 초콜릿 핫 크로스 번, Fortnum &amp; Mason Sir Nigel&#39;s Marmalade 핫 크로스 번, Betty&#39;s of Harrogate Large Chocolate &amp; Orange 핫 크로스 번, Morrisons Chocolate 핫 크로스 번, Tesco Finest Double Belgian Chocolate &amp; Cornish Fudge 핫 크로스 번, Waitrose LOV Generator 강사 요약을 다시 작성하십시오. 엘리자베스 여왕 1세와 관련된 내용을 추가하고, Tescost Belgian Chocolate, Morrisons Cross B, M&amp;S Toffee Fudge와 관련된 내용을 제거합니다.... 새로운 요약: ...... 최종 출력 핫크로스번은 영국에서 사랑받는 부활절 간식으로, 향신료가 들어간 달콤한 롤빵과 위에 십자가가 얹혀 있습니다.엘리자베스 여왕 1세는 종교적 의미 때문에 이를 금지하려고 했지만 결국 타협하여 성금요일, 크리스마스, 장례식에 판매하도록 허용했습니다.올해 부활절에 슈퍼마켓에서는 다양한 맛과 사이즈를 출시하여 고객을 유혹했습니다.Harriet Arkell은 이번 부활절에 제공되는 최고의 핫크로스번을 테스트했습니다.O 편집자 그림 2: 제안된 3중 에이전트 생성 파이프라인의 그림.질의가 주어지면 생성자는 먼저 초기 답변을 생성하고, 강사는 사용자의 선호도에 맞게 답변을 더 맞춤화하는 방법에 대한 지침을 제공하고, 마지막으로 편집자는 제공된 지침에 따라 개인화된 답변을 생성합니다.Welleck 등이 취한 접근 방식과는 대조적입니다. (2022), 생성 프로세스가 생성기와 교정기로 분해되는 경우, 저희 방법론은 생성기, 강사, 편집자로 구성된 3가지 구성 요소 분해를 포함합니다(그림 1 참조). 이 구조를 사용하면 복잡한 콘텐츠 생성 및 교정 작업에 추론 전용 대형 모델을 활용하는 동시에 사용자별 편집 지침을 생성하는 간단한 작업에 더 작은 모델을 활용할 수 있습니다. 강사는 생성기의 초기 출력을 편집하고 정제하기 위한 타겟팅된 지침을 제공하도록 설계되었습니다. 사용자 행동의 기록을 통해 얻을 수 있는 인간이 작성한 지침 또는 오라클 지침에 대한 학습을 통해 초기화됩니다. 그런 다음 모델은 편집자 주도 강화 학습을 통해 미세 조정됩니다. 여기서 보상 함수는 편집자가 편집한 출력이 사용자 선호도와 일치하는 정도를 직접 정량화하여 모델과 편집자의 호환성을 향상시킵니다. 저희는 이 새로운 프레임워크를 평가하기 위한 초점 작업으로 텍스트 요약을 선택했으며, 이는 주어진 문서에 대한 간결하고 유익한 요약을 생성하는 것입니다. 이 논문에서 우리는 두 가지 요약 데이터 세트(DeFacto(Liu et al., 2022b)와 CNNDM(Nallapati et al., 2016))에 대한 실험적 평가를 수행하며, 사실적 일관성 및 적용 범위와 관련된 사용자 선호도에 초점을 맞춥니다. 우리는 ChatGPT를 생성기와 편집자 모델로 사용합니다. 우리의 실험은 소규모 강사 모델에서 생성된 지침을 사용하면 편집된 출력이 두 데이터 세트 모두에 대한 사용자 선호도와 더 잘 일치한다는 것을 나타냅니다. 반복적 편집에 대한 추가 실험은 편집을 더 많이 반복할수록 출력이 사용자의 요구를 더 잘 충족할 수 있음을 보여줍니다. 전체 파이프라인 생성 파이프라인의 유연성을 향상하고 강력한 대규모 언어 모델과의 호환성을 최적화하기 위해 그림 2에서 설명한 대로 생성 프로세스를 세 가지 고유한 구성 요소로 새롭게 분해하는 것을 제안합니다. 이러한 구성 요소에는 다음이 포함됩니다. (1) 초기 출력을 생성하는 생성기, (2) 초기 출력의 편집을 사용자 선호도 방향으로 안내하는 자연어 지침을 생성하는 작업을 맡은 강사, (3) 제공된 지침에 따라 초기 출력을 정제하는 편집기. 대규모 언어 모델이 생성기와 편집기 모델로 모두 작동할 수 있다는 것이 입증되었으므로 추론 전용 대규모 언어 모델, 특히 ChatGPT를 생성기 및 편집기로 활용하기로 했습니다. 이러한 대규모 언어 모델을 더욱 미세 조정할 수 있지만 편집됨 f(Sedit) 요약 + TL;DR: 초안 요약 f(Sinit) 참조 생성기 보상: f(Sedit) - f(Sinit) 강사 편집기 프롬프트 문서:<document> 요약:<summary> 지침:<instruction> 요약은 지침에 따라서만 편집합니다.새로운 요약: 편집자 주도 강화 학습 편집 지침 그림 3: 강사를 위한 편집자 주도 강화 학습.편집자 주도 강화 학습을 사용하여 강사를 미세 조정하여 편집자의 예상 성능을 극대화합니다(예: ChatGPT).강사 역할을 하려면 계산 리소스(Touvron 등, 2023) 및 액세스 제한(Ouyang 등, 2022)과 같은 실질적인 제한으로 인해 이전 작업(Welleck 등, 2022; Liu 등, 2022a)에서 수행한 것처럼 직접적인 미세 조정이 불가능할 수 있습니다.따라서 편집자 주도 강화 학습을 사용하여 더 작은 모델을 사용자별 강사(섹션 3에서 소개)로 훈련하여 편집자가 인간의 기대에 더 잘 부합하도록 초기 출력을 수정하도록 안내합니다. 3 편집자 중심 강사 위에서 소개한 대로 제안된 강사의 중심 목적은 대규모 언어 모델이 원래 요약을 수정하여 사용자의 선호도에 더 가깝게 정렬할 수 있도록 안내할 수 있는 정확하고 실행 가능한 지침을 생성하는 것입니다. 이를 달성하기 위해 강사가 대규모 언어 모델과 협력하여 작업할 수 있도록 설계된 2단계 교육 프로세스를 사용합니다. 구체적으로, 문서 D가 주어지면 생성기(요약 모델 또는 대규모 언어 모델)를 사용하여 Sinit으로 표시되는 초기 요약이 생성됩니다. 강사의 목적은 D와 Sinit을 입력으로 사용하여 지침 세트 I = {11, 12, ik}를 생성하여 편집자 모델이 사용자의 선호도에 더 가깝게 정렬된 편집된 요약을 생성하도록 안내하는 것입니다. 마지막으로 편집자는 D, Sinit 및 I를 입력으로 사용하여 제공된 지침에 따라 수정된 요약 Sedit을 생성합니다. .... 3.1 1단계: 지도 학습 초기 학습 단계에서는 요약 수정에 대한 사용자의 과거 선호도에 맞춰 조정된 오라클 명령어 세트를 생성합니다.³ 이러한 오라클 명령어는 강사가 생성해야 하는 명령어의 이상적인 예가 됩니다.그런 다음 음의 로그 가능도 손실을 사용하여 지도 방식으로 강사 모델을 학습합니다.즉, L = P(i1, 2, ..., ik|D, Sinit).k 이 단계의 목표는 강사가 입력(소스 문서 및 초기 요약)과 원하는 출력(오라클 명령어) 간의 관계를 학습하도록 하여 사용자의 기대에 부합하는 명령어를 생성할 수 있는 견고한 기반을 구축하는 것입니다.3.2 2단계: 편집자 주도 강화 학습 두 번째 단계에서는 편집자 주도 강화 학습 기술(그림 3 참조)을 사용하여 강사 모델을 더욱 미세 조정합니다.특히 NLPO 알고리즘(Ramamurthy et al., 2023)을 사용합니다. 이 단계의 핵심 측면은 RL 기반 미세 조정 프로세스에 대한 안내 신호 역할을 하는 보상 함수의 설계입니다. 생성된 지침이 호환되는지 확인하기 위해 3 이러한 오라클 지침은 각 소스의 고유한 요약 선호도를 반영하는 인간이 작성한 요약을 참고로 사용하여 사용자의 선호도를 시뮬레이션하여 구성됩니다. 예를 들어 CNN과 DailyMail은 뉴스 기사에 대해 생성하는 요약에서 특정 경향을 보일 수 있습니다. 편집자 모델과 호환되고 의미 있는 요약 수정으로 이어지도록 보상 함수는 소스 문서, 초기 요약 및 강사 모델에서 제공한 편집 지침을 포함하는 프롬프트를 사용하여 편집자 모델에서 생성한 편집된 요약을 기반으로 공식화됩니다(그림 3의 오른쪽 하단에 표시된 예제 프롬프트 참조). 편집된 요약의 품질을 정량화하기 위해 요약이 사용자의 선호도를 충족하는 정도를 측정하는 스코어링 함수 f(.)를 사용합니다. 생성된 요약의 적용 범위와 사실적 일관성을 사용자 요구 사항으로 중점적으로 다루면서, 스코어링 함수 f(.)를 ROUGE 점수와 지식 적용 범위의 합으로 설정합니다.이는 엔티티 수준 적용 범위와 참조 요약의 유사성을 측정합니다.f(S) = aROUGE(S, Sref) + ẞCov(S, Sref).보상 신호 자체는 초기 요약과 편집된 요약 간의 점수 차이로 정의되며, 요약 품질의 개선을 포착하도록 설계되었으며, 더 높은 보상은 보다 실질적인 개선에 해당합니다.보상 = f(Sedit) — f(Sinit).이 단계는 사용자 요구 사항을 준수할 뿐만 아니라 대규모 언어 모델이 개선된 요약을 생성하도록 효과적으로 안내하는 지침을 생성하는 모델의 기능을 향상시키는 것을 목표로 합니다.4 실험 사용자 선호도의 다른 측면을 각각 포착하는 두 개의 서로 다른 데이터 세트에 대한 실험을 수행합니다. 4.1 시나리오 1: DeFacto에서의 사실적 일관성 초기 실험 시나리오에서 우리는 사실적 일관성을 사용자의 요약 선호도에 대한 주요 기준으로 강조하기로 했습니다.4 우리는 DeFacto 데이터 세트(Liu et al., 2022b)를 사용합니다.이 리소스는 인간이 주석을 단 데모와 피드백을 포함하여 기계에서 생성된 요약의 사실적 일관성을 향상시키기 위해 특별히 큐레이팅되었습니다.이 데이터 세트는 각각 훈련/검증/테스트 세트의 701/341/*사실적 일관성이 일반적으로 요약자에게 기준이 될 수 있지만, 우리는 강사를 활용하여 요약의 사실적 일관성을 향상시키는 특정 지침을 작성하는 능력을 습득합니다.5 DeFacto 데이터 세트의 각 데이터 항목은 소스 문서와 PEGASUS에서 생성한 초기 요약으로 구성됩니다(Zhang et al., 2020).주석 작성자는 사실적 일관성을 향상시키기 위해 초기 요약을 수정하는 지침을 제공해야 합니다. 또한 주석자는 제공된 지침을 준수하고 개선된 사실적 일관성을 보여주는 개정된 요약을 생성합니다. 시스템에서 생성된 지침과 인간이 작성한 지침 간의 일치성을 평가하기 위해 ROUGE 점수를 평가 지표로 사용합니다. 또한 ROUGE 점수와 사실성 점수를 포함한 지표를 조합하여 생성된 요약의 품질을 인간의 기대와 사실적 정확성과 관련하여 평가합니다. 구체적으로 DAE(Dependency Arc Entailment) 지표(Goyal 및 Durrett, 2021)와 QFE(Question-answering for Factual Evaluation) 지표(Fabbri et al., 2022)를 사용하여 생성된 요약의 사실성을 정량화합니다. 이러한 지표는 인간의 기대와의 일치성과 사실적 정확성 준수 측면에서 요약 품질에 대한 포괄적인 평가를 제공합니다. 설정 강사의 백본 모델로 FlanT5-large(700M)(Chung et al., 2022)를 사용합니다. 강사를 위한 교육 과정은 섹션 3에 자세히 설명된 대로 두 단계로 실행됩니다.결과 우선, 우리는 ChatGPT가 인간이 제공한 지침에 따라 요약을 수정할 수 있는 편집자 모델 역할을 할 수 있는 잠재력을 평가합니다.표 1에 제시된 이 평가 결과는 ChatGPT가 소스 문서, 초기 요약 및 인간이 작성한 편집 지침을 입력으로 제공했을 때 감독 모델과 비슷한 성능을 보임을 나타내며, 이는 비슷한 ROUGE 점수와 사실성 점수로 입증됩니다.이러한 결과는 적절한 편집 지침이 제공될 때 ChatGPT가 요약 편집기로 효과적임을 확인합니다.그런 다음, 인간이 작성한 지침과 비교하여 시스템에서 생성한 지침을 평가합니다.우리의 목표는 ChatGPT와 훈련된 강사가 사용자 요구 사항을 정확하게 식별하고 그에 따라 해당 지침을 생성할 수 있는 정도를 파악하는 것입니다.결과 원래 논문에 따라 모든 실험은 오류가 표시된 예제에서 수행됩니다. 편집자 DAE QFE RRRL 초기 요약 인간 편집자 0.699 1.837 76.0.906 2.66.74.TOPP-D+S+I(Sup) ChatGPT(10-shot) 0.904 2.470 88.74 83.0.884 2.568 88.48 81.41 86.87.표 1: DeFacto에서 인간이 작성한 지침이 있는 편집된 요약의 ROUGE 점수 및 사실적 일관성 점수는 인간이 편집한 요약과 비교한 것입니다.TOPP-D+S+I(Sup)는 소스 문서, 초기 요약 및 지침을 입력으로 하는 지도 모델입니다(Liu et al., 2022b). 모델 RRRL ChatGPT(제로샷) ChatGPT(10샷) 36.37.22.98 30.24.94 32.FlanT5(Sup) FlanT5(RL) 49.04 34.37 47.48.05 32.94 46.표 2: DeFacto에서 생성된 명령어와 인간이 작성한 명령어 간의 ROUGE 점수. 이 평가의 결과는 표 2에 제시되어 있습니다. 주목할 점은 ChatGPT에서 생성된 명령어가 인간이 작성한 명령어와 효과적으로 일치하지 않는다는 것입니다. 이는 제로샷 및 퓨샷 설정에서 최적이 아닌 성능에서 입증됩니다. 우리가 사용한 강사 모델은 ChatGPT(700M 대 175B)보다 훨씬 작지만 사용자의 요구 사항에 더 잘 맞는 명령어를 생성할 수 있는 능력을 보여줍니다. 표 3에 제시된 최종 실험 세트에서 우리는 훈련되고 RL 미세 조정된 강사와 함께 편집 모델(ChatGPT)의 성능과 ChatGPT가 few-shot 설정에서 생성한 지침을 평가합니다. 결과에 따르면 10-shot 프롬프트와 훈련된 강사의 지침을 활용할 때 ChatGPT가 편집한 요약은 원래 요약에 비해 사실성(DAE/QFE로 측정)에서 큰 개선을 보입니다. ChatGPT에서 파생된 보상을 통합한 강화 학습을 구현하면 요약 품질이 추가로 향상됩니다. 또한 ChatGPT가 생성한 지침을 활용하여 실험을 수행합니다. 이러한 지침은 인간이 작성한 지침과 최적이 아니지만 QFE 메트릭으로 측정한 사실성 측면에서 예상치 못하게 높은 점수를 얻습니다. 그러나 다른 방법에 비해 ROUGE 점수가 현저히 감소하는 것으로 나타났습니다. 이러한 결과는 ChatGPT가 구체적이고 잘 정의된 측면(예: 사실적 불일치 해결)을 타겟으로 하는 지침을 생성하는 능력을 가지고 있지만, 더 광범위한 인간의 기대를 정확하게 식별하고 충족하는 데 어려움을 겪을 수 있음을 시사합니다. 4.2 시나리오 2: CNNDM에서의 적용 범위 ChatGPT는 뉴스 기사의 유창하고 유익한 요약을 생성하는 능력을 보여주었습니다(Goyal et al., 2022). 일관된 요약을 생성하는 데 능숙함에도 불구하고, 사용자가 기대하는 대로 주요 주제에 대한 원하는 적용 범위를 달성하지 못할 수 있습니다. 이러한 과제에 대응하여, 사용자의 이력을 기반으로 지식 적용 범위를 개선하기 위해 요약 편집을 안내하도록 특별히 설계된 강사 모델을 훈련하고 평가하는 실험을 수행합니다. 강사는 현재 요약에 추가되거나 제거될 키워드를 예측하여 요약을 사용자 선호도에 더 가깝게 정렬하기 위한 실행 가능한 지침을 제공합니다. 실제로, 생성된 요약이 키워드 콘텐츠 측면에서 참조 요약과 일치하는 정도에 따라 지식 적용 범위를 평가합니다. 우리는 이 실험의 벤치마크로 CNNDM 데이터 세트(Nallapati et al., 2016)를 사용하는데, 여기에는 기사와 참고 요약의 쌍이 포함되어 있으며, 원래 참고 요약은 커버리지에 대한 사용자 선호도의 대상 표현으로 사용됩니다. 우리는 최근 연구(Goyal et al., 2022)에 따르면 CNNDM 데이터 세트의 참고 요약은 일관성이 떨어지는 등 품질에 한계가 있을 수 있음을 인정합니다. 그러나 이 실험에서 우리의 주요 초점은 요약의 품질보다는 지식 커버리지에 있습니다. 우리는 생성된 요약이 참고의 주요 엔터티를 얼마나 포착하는지 평가하는 데 관심이 있습니다. 지식 커버리지를 측정하기 위해 엔터티 수준 매칭 메트릭 Knlg F1을 도입합니다. Egen은 생성된 요약에서 언급된 엔터티이고 Eref는 참고 요약에서 언급된 엔터티입니다. 우리는 강사 DAE QFE RRRL 초기 요약 0.1.837 76.03 66.34 74.FLAN T5(Sup) 0.FLAN T5(RL) 2.093 72.60 61.96 71.0.803 2.198 74.77 64.73 73.ChatGPT(10-shot) 0.834 2.583 56.54 41.29 53 사이의 중복 정도를 정량화합니다.표 3: DeFacto에서 다른 강사가 생성한 지침이 있는 편집된 요약의 ROUGE 점수 및 사실적 일관성 점수. 우리는 표에 표시된 모든 결과에 대한 편집자 모델로 ChatGPT(10-shot)를 사용합니다. 강사 Knlg F초기 요약 FLAN T5(Sup) FLAN T5(RL) ChatGPT(5-shot)* Oracle R44.15 40.28 16.65 33.47.44 41.04 16.72 33.47.99 41.21 16.80 33.43.43 39.46 15.43 32.RRL 모델 초기 요약 편집 Iter편집 Iter편집 Iter편집 Iter 1(1&amp;2) 편집 Iter 2(1&amp;2) 편집 Iter 3(1&amp;2) Knlg F1 RR44.15 40.28 16.65 33.47.99 41.21 16.80 33.48.65 41.18 16.69 33.48.99 41.14 16.63 33.48.08 41.25 16.91 33.48.87 40.62 16.60 33.49.20 41.15 16.87 33.RL 60.80 43.08 18.37 35.표 4: CNNDM에서 다른 강사가 생성한 지침이 있는 편집된 요약의 지식 범위 및 ROUGE 점수. 생성기 모델(초기 요약 생성) 및 편집자 모델로 ChatGPT(zeroshot)를 사용합니다.* 길이 제한(4k 토큰)을 초과하는 경우 프롬프트의 예제 수를 줄입니다.표 5: CNNDM의 반복 편집.두 번째 블록은 첫 번째 반복에서만 데이터에 대해 미세 조정된 모델의 결과를 보여주고, 아래 블록은 첫 번째 및 두 번째 반복에서 데이터에 대해 미세 조정된 모델의 결과를 보여줍니다. 두 개는 Knlg F = Knlgp 2Knlgp × Knlgr Knlgp + Knlgr Eref Egen | Egen 여기서 &quot; Knlgr Eref Egen | Eref 강사는 이 중복을 최대화함으로써 참조에서 표시된 대로 관련 정보를 효과적으로 포괄하는 요약을 생성하는 것을 목표로 합니다. 설정: ChatGPT에서 생성된 요약을 초기 요약으로 사용합니다. 6. 그리고 FlanT5-large(700M)를 강사 모델로 채택하여 원본 문서와 ChatGPT에서 생성된 초기 요약을 모두 입력으로 사용하여 키워드를 예측합니다. 지도 학습은 추가 및 제거할 키워드를 지정하는 Oracle 키워드 목록을 사용하여 수행됩니다. 그런 다음 모델은 효율성을 위해 데이터 세트에서 10,000개의 학습 예제 하위 집합을 사용하여 섹션 3에 자세히 설명된 대로 편집자 주도 강화 학습 미세 조정을 거칩니다. 결과: 표 4에 제시된 실험 결과는 &quot;데이터 세트가 릴리스되었으며 Github 저장소에서 찾을 수 있습니다. 지식 범위를 향상시키는 강사 모델의 결과는 엔티티 매칭과 ROUGE 점수로 나타납니다. 제로샷 설정에서 ChatGPT는 요약자로서 강력한 성능을 보여줍니다. 중요한 점은 Oracle 명령어가 제공될 때 ChatGPT는 지정된 명령어에 따라 초기 요약을 수정하고 정제하는 강력한 기능을 보여준다는 것입니다. 훈련된 강사 모델에서 생성된 명령어를 통합하면 지식 범위가 현저히 향상되어 요약이 사용자 선호도와 더 잘 일치함을 나타냅니다(FLAN T5(Sup)와 초기 요약 비교). 게다가 강화 학습 미세 조정 프로세스(FLAN T5(RL))가 모델의 성능을 더욱 향상시켜 평가된 메트릭에서 적당하지만 의미 있는 이득을 얻는 것을 관찰했습니다. 반면, 훈련된 강사를 사용하지 않고 ChatGPT가 직접 명령어를 생성하는 몇 가지 샷 설정(ChatGPT(5-shot))을 탐색하면 편집된 요약의 성능이 저하됩니다. 구체적으로, Knlg F1과 ROUGE 점수는 모두 초기 요약보다 낮아서 ChatGPT가 요약 편집에 대한 효과적인 지침을 생성하여 사용자의 선호도에 더 잘 맞출 수 있는 능력에 한계가 있음을 시사합니다. 전반적으로 이러한 결과는 초기 요약 Oracle 지침 인간이 작성한 참조 예측 지침 ChatGPT 편집 요약의 가치를 강조합니다. 전직 교도관은 비행기에서 한 청년이 욕설 사용을 중단하라고 한 후 그에게 주먹을 맞았습니다. 그런 다음 전직 교도관은 청년을 내려놓고 경찰이 도착할 때까지 그를 붙잡아 두었습니다. 출처: Daily Mail<Add> 채드 허스트<remove> Daily Mail 유타주 솔트레이크시티의 채드 허스트는 일요일에 도시에 착륙했을 때 비행기 승객에게 갑작스러운 펀치를 맞았습니다. 허스트가 비행기에서 내린 후 청년에게 욕설을 하지 말라고 한 후였습니다. 전 교도관인 허스트는 그 남자를 잡아눕히고 법 집행을 기다리는 동안 그의 팔을 등 뒤로 고정했습니다. 아직 경찰에서 이름을 밝히지 않은 그 청년은 폭행과 공공장소 취기 혐의로 기소되었습니다.<Add> 채드 허스트<remove> Daily Mail 유타주 솔트레이크시티의 전 교도관인 채드 허스트는 비행기에서 한 청년이 욕설을 그만하라고 한 후 그에게 주먹을 맞았습니다.허스트는 침착하게 그 청년을 잡아눕히고 경찰이 도착할 때까지 붙잡아 두었습니다.그 청년은 폭행과 공공장소 음주 혐의로 기소되었습니다.허스트는 전 교도관으로서의 훈련을 통해 결코 주먹으로 맞받아치지 말고 상황을 통제하고 그 사람을 잡아눕히도록 배웠습니다.표 6: CNNDM 데이터 세트의 예.사용자 선호도를 보다 면밀히 준수하도록 요약을 편집할 때 ChatGPT와 같은 대규모 언어 모델을 안내하는 강력한 중개자로서의 강사.5 토론 5.1 반복 편집 단일 단계 편집을 수행하는 것 외에도 CNNDM 데이터 세트에서 반복 편집의 효과를 알아보기 위한 실험을 수행했습니다.반복 편집 실험의 결과는 표 5에 나와 있습니다.첫 번째 반복의 데이터에만 기반한 강화 학습(RL) 훈련을 활용하여 반복 편집 프로세스에서 편집된 요약의 적용 범위가 향상되는 것을 관찰했습니다. 우리는 첫 번째와 두 번째 반복에서 얻은 데이터를 혼합하여 모델을 더욱 미세 조정했고, 이는 반복적으로 편집된 요약에서 향상된 지식 F1에서 입증된 것처럼 향상된 성능으로 이어졌습니다. 5.2 정성적 예제 표 6에서 CNNDM 데이터 세트의 예를 보여줍니다. 강사 모델은 사용자의 기대를 올바르게 감지하고 편집 지침을 생성할 수 있습니다. ChatGPT는 제공된 지침에 따라 초기 요약을 편집하여 편집자 역할을 할 수 있습니다.&quot; 우리는 DeFacto 데이터 세트에서 유사한 실험을 수행하지 않았습니다. 대부분의 데이터 예제에서 초기 요약에서 사람이 편집한 요약으로 전환하는 데 필요한 편집 단계가 하나뿐이기 때문입니다. DeFacto의 예제는 부록에 나와 있습니다. 6
--- RELATED WORK ---
6.1 텍스트 편집 포스트 편집 기술은 문장 융합(Malmi et al., 2019), 스타일 전송(Reid and Zhong, 2021), 위키 편집(Reid and Neubig, 2022; Faltings et al., 2021)을 포함한 다양한 NLP 작업에서 광범위하게 연구되었습니다.
--- METHOD ---
ology는 생성기, 강사, 편집자로 구성된 3가지 구성 요소 분해를 포함합니다(그림 1 참조). 이 구조를 사용하면 복잡한 콘텐츠 생성 및 수정 작업에 추론 전용 대형 모델을 활용하는 동시에 사용자별 편집 지침을 생성하는 간단한 작업에 더 작은 모델을 활용할 수 있습니다. 강사는 생성기의 초기 출력을 편집하고 정제하기 위한 타겟팅된 지침을 제공하도록 설계되었습니다. 사용자 행동의 기록을 통해 얻을 수 있는 인간이 작성한 지침 또는 오라클 지침에 대한 학습을 통해 초기화됩니다. 그런 다음 모델은 편집자 주도 강화 학습을 통해 미세 조정되며, 보상 함수는 편집자가 편집한 출력이 사용자 선호도와 일치하는 정도를 직접 정량화하여 모델과 편집자의 호환성을 향상시킵니다. 이 새로운 프레임워크를 평가하기 위한 초점 작업으로 텍스트 요약을 선택했는데, 이는 주어진 문서에 대한 간결하고 유익한 요약을 생성하는 것입니다. 이 논문에서 우리는 다음을 수행합니다.
--- EXPERIMENT ---
두 개의 추상 요약 데이터 세트에 대한 모든 결과는 사용자 기대에 더 잘 부합하는 출력을 생성하는 데 있어 우리 접근 방식의 효과성을 보여줍니다.1 서론 InstructGPT(Ouyang et al., 2022) 및 ChatGPT2와 같은 뛰어난 모델에서 예시되는 대규모 언어 모델은 자연어 처리(NLP) 분야에서 필수적인 리소스로 부상했습니다. 이러한 모델은 기계 번역, 질의 응답 및 텍스트 요약을 포함한 광범위한 NLP 작업에서 놀라운 수준의 능숙도를 보여주었습니다. 언어 기반 기술에서 추가 혁신을 주도할 수 있는 잠재력에 비추어 연구 커뮤니티는 대규모 언어 모델을 탐구하고 발전시키는 데 대한 열정이 커지고 있습니다. 그러나 이러한 모델이 달성한 인상적인 생성 품질에도 불구하고 사용자의 선호도에 맞게 출력을 조정하는 데 지속적인 과제가 있습니다(Liu et al., 2022b). https://github.com/에서 ¹코드는 Wendy-Xiao/chatgpt_editing_summ에서 사용할 수 있습니다 2 https://openai.com/blog/chatgpt 일반 답변 수정된 답변 개인화된 답변 반복 반복 편집기 교정자 지침 훈련된 강사 생성기 생성기 생성기 일회성 생성 자체 교정 쿼리 Tri-Agent 그림 1: 다양한 생성 패러다임 간 비교. 왼쪽은 일반적인 일회성 생성 프로세스이고 가운데는 Welleck et al. (2022)에서 제공하는 것으로, 훈련된 교정자를 사용하여 생성된 텍스트를 교정하며 일반적으로 환각이나 독성 제거와 같은 특정 문제를 다루고 오른쪽은 제안된 tri-agent 파이프라인입니다. 여러 시나리오에서 언어 모델의 출력이 사용자의 선호도나 기대를 일관되게 충족하지 못하는 것으로 관찰되었습니다(Bubeck et al., 2023). 이러한 한계를 해결하기 위한 널리 퍼진 접근 방식에는 사용자의 목표에 더 잘 부합하는 출력을 생성하도록 모델을 조종하기 위한 프롬프트를 신중하게 만드는 것이 포함됩니다. 그럼에도 불구하고 기존 연구(Reid와 Neubig, 2022)에서 언급했듯이 언어 모델의 기존 일회성 왼쪽에서 오른쪽으로 생성 프로세스는 인간이 일반적으로 사용하는 반복적 정제 및 편집 접근 방식과 대조됩니다. 나아가 이전 연구(Gu et al., 2019; Reid와 Zhong, 2021)는 단일 편집 반복을 사용하더라도 일회성 생성에 비해 생성 및 편집 프로세스의 효능을 입증했습니다. 이러한 결과에 동기를 부여받은 이 논문은 대규모 언어 모델(ChatGPT)을 자동 반복 편집 파이프라인에 통합하는 방법을 살펴봅니다. 질의 답변 수정 부활절에 핫크로스번을 좋아하지 않는 사람이 있을까요? 영국에서는 십자가가 얹힌 매운 달콤한 롤빵을 너무 좋아해서 일년 내내 먹곤 했습니다. 엘리자베스 여왕 1세가 십자가가 너무 가톨릭적이라고 생각하여 금지하려고 할 때까지는 말입니다. 이러한 움직임은 큰 반향을 불러일으켰고, 그래서 그녀는 타협하여 그것들은 성금요일, 크리스마스 및 장례식에만 판매할 수 있다고 말했고, 그 이후로 부활절의 주요 음식이 되었습니다.요즘 슈퍼마켓은 고객을 유혹하기 위해 미니어처 크기부터 토피 퍼지 청크, 사과와 계피와 같은 새로운 맛까지 점점 더 많은 변형을 내놓습니다.이번 부활절에 가장 좋은 배치는 다음과 같습니다...... TL;DR: Harriet Arkell은 이번 부활절에 제공되는 최고의 핫 크로스 번을 테스트했습니다.최고의 제품은 다음과 같습니다.M&amp;S 토피 퍼지 &amp; 벨기에 초콜릿 핫 크로스 번, Fortnum &amp; Mason Sir Nigel&#39;s Marmalade 핫 크로스 번, Betty&#39;s of Harrogate Large Chocolate &amp; Orange 핫 크로스 번, Morrisons Chocolate 핫 크로스 번, Tesco Finest Double Belgian Chocolate &amp; Cornish Fudge 핫 크로스 번, Waitrose LOV Generator 강사 요약을 다시 작성하십시오. 엘리자베스 여왕 1세와 관련된 내용을 추가하고, Tescost Belgian Chocolate, Morrisons Cross B, M&amp;S Toffee Fudge와 관련된 내용을 제거합니다.... 새로운 요약: ...... 최종 출력 핫크로스번은 영국에서 사랑받는 부활절 간식으로, 향신료가 들어간 달콤한 롤빵과 위에 십자가가 얹혀 있습니다.엘리자베스 여왕 1세는 종교적 의미 때문에 이를 금지하려고 했지만 결국 타협하여 성금요일, 크리스마스, 장례식에 판매하도록 허용했습니다.올해 부활절에 슈퍼마켓에서는 다양한 맛과 사이즈를 출시하여 고객을 유혹했습니다.Harriet Arkell은 이번 부활절에 제공되는 최고의 핫크로스번을 테스트했습니다.O 편집자 그림 2: 제안된 3중 에이전트 생성 파이프라인의 그림.질의가 주어지면 생성자는 먼저 초기 답변을 생성하고, 강사는 사용자의 선호도에 맞게 답변을 더 맞춤화하는 방법에 대한 지침을 제공하고, 마지막으로 편집자는 제공된 지침에 따라 개인화된 답변을 생성합니다.Welleck 등이 취한 접근 방식과는 대조적입니다. (2022), 생성 프로세스가 생성기와 교정기로 분해되는 경우, 저희 방법론은 생성기, 강사, 편집자로 구성된 3가지 구성 요소 분해를 포함합니다(그림 1 참조). 이 구조를 사용하면 복잡한 콘텐츠 생성 및 교정 작업에 추론 전용 대형 모델을 활용하는 동시에 사용자별 편집 지침을 생성하는 간단한 작업에 더 작은 모델을 활용할 수 있습니다. 강사는 생성기의 초기 출력을 편집하고 정제하기 위한 타겟팅된 지침을 제공하도록 설계되었습니다. 사용자 행동의 기록을 통해 얻을 수 있는 인간이 작성한 지침 또는 오라클 지침에 대한 학습을 통해 초기화됩니다. 그런 다음 모델은 편집자 주도 강화 학습을 통해 미세 조정됩니다. 여기서 보상 함수는 편집자가 편집한 출력이 사용자 선호도와 일치하는 정도를 직접 정량화하여 모델과 편집자의 호환성을 향상시킵니다. 저희는 이 새로운 프레임워크를 평가하기 위한 초점 작업으로 텍스트 요약을 선택했으며, 이는 주어진 문서에 대한 간결하고 유익한 요약을 생성하는 것입니다. 이 논문에서 우리는 두 가지 요약 데이터 세트(DeFacto(Liu et al., 2022b)와 CNNDM(Nallapati et al., 2016))에 대한 실험적 평가를 수행하며, 사실적 일관성 및 적용 범위와 관련된 사용자 선호도에 초점을 맞춥니다. 우리는 ChatGPT를 생성기와 편집자 모델로 사용합니다. 우리의 실험은 소규모 강사 모델에서 생성된 지침을 사용하면 편집된 출력이 두 데이터 세트 모두에 대한 사용자 선호도와 더 잘 일치한다는 것을 나타냅니다. 반복적 편집에 대한 추가 실험은 편집을 더 많이 반복할수록 출력이 사용자의 요구를 더 잘 충족할 수 있음을 보여줍니다. 전체 파이프라인 생성 파이프라인의 유연성을 향상하고 강력한 대규모 언어 모델과의 호환성을 최적화하기 위해 그림 2에서 설명한 대로 생성 프로세스를 세 가지 고유한 구성 요소로 새롭게 분해하는 것을 제안합니다. 이러한 구성 요소에는 다음이 포함됩니다. (1) 초기 출력을 생성하는 생성기, (2) 초기 출력의 편집을 사용자 선호도 방향으로 안내하는 자연어 지침을 생성하는 작업을 맡은 강사, (3) 제공된 지침에 따라 초기 출력을 정제하는 편집기. 대규모 언어 모델이 생성기와 편집기 모델로 모두 작동할 수 있다는 것이 입증되었으므로 추론 전용 대규모 언어 모델, 특히 ChatGPT를 생성기 및 편집기로 활용하기로 했습니다. 이러한 대규모 언어 모델을 더욱 미세 조정할 수 있지만 편집됨 f(Sedit) 요약 + TL;DR: 초안 요약 f(Sinit) 참조 생성기 보상: f(Sedit) - f(Sinit) 강사 편집기 프롬프트 문서:<document> 요약:<summary> 지침:<instruction> 요약은 지침에 따라서만 편집합니다.새로운 요약: 편집자 주도 강화 학습 편집 지침 그림 3: 강사를 위한 편집자 주도 강화 학습.편집자 주도 강화 학습을 사용하여 강사를 미세 조정하여 편집자의 예상 성능을 극대화합니다(예: ChatGPT).강사 역할을 하려면 계산 리소스(Touvron et al., 2023) 및 액세스 제한(Ouyang et al., 2022)과 같은 실질적인 제한으로 인해 이전 작업(Welleck et al., 2022; Liu et al., 2022a)에서 수행된 것처럼 직접적인 미세 조정이 불가능할 수 있습니다.따라서 편집자 주도 강화 학습을 사용하여 더 작은 모델을 사용자별 강사(섹션 3에서 소개)로 훈련하여 편집자가 인간의 기대에 더 잘 부합하도록 초기 출력을 수정하도록 안내합니다. 3 편집자 중심 강사 위에서 소개한 대로 제안된 강사의 중심 목적은 대규모 언어 모델이 원래 요약을 수정하여 사용자의 선호도에 더 가깝게 정렬할 수 있도록 안내할 수 있는 정확하고 실행 가능한 지침을 생성하는 것입니다. 이를 달성하기 위해 강사가 대규모 언어 모델과 협력하여 작업할 수 있도록 설계된 2단계 교육 프로세스를 사용합니다. 구체적으로, 문서 D가 주어지면 생성기(요약 모델 또는 대규모 언어 모델)를 사용하여 Sinit으로 표시되는 초기 요약이 생성됩니다. 강사의 목적은 D와 Sinit을 입력으로 사용하여 지침 세트 I = {11, 12, ik}를 생성하여 편집자 모델이 사용자의 선호도에 더 가깝게 정렬된 편집된 요약을 생성하도록 안내하는 것입니다. 마지막으로 편집자는 D, Sinit 및 I를 입력으로 사용하여 제공된 지침에 따라 수정된 요약 Sedit을 생성합니다. .... 3.1 1단계: 지도 학습 초기 학습 단계에서는 요약 수정에 대한 사용자의 과거 선호도에 맞춰 조정된 오라클 명령어 세트를 생성합니다.³ 이러한 오라클 명령어는 강사가 생성해야 하는 명령어의 이상적인 예가 됩니다.그런 다음 음의 로그 가능도 손실을 사용하여 지도 방식으로 강사 모델을 학습합니다.즉, L = P(i1, 2, ..., ik|D, Sinit).k 이 단계의 목표는 강사가 입력(소스 문서 및 초기 요약)과 원하는 출력(오라클 명령어) 간의 관계를 학습하도록 하여 사용자의 기대에 부합하는 명령어를 생성할 수 있는 견고한 기반을 구축하는 것입니다.3.2 2단계: 편집자 주도 강화 학습 두 번째 단계에서는 편집자 주도 강화 학습 기술(그림 3 참조)을 사용하여 강사 모델을 더욱 미세 조정합니다.특히 NLPO 알고리즘(Ramamurthy et al., 2023)을 사용합니다. 이 단계의 핵심 측면은 RL 기반 미세 조정 프로세스에 대한 안내 신호 역할을 하는 보상 함수의 설계입니다. 생성된 지침이 호환되는지 확인하기 위해 3 이러한 오라클 지침은 각 소스의 고유한 요약 선호도를 반영하는 인간이 작성한 요약을 참고로 사용하여 사용자의 선호도를 시뮬레이션하여 구성됩니다. 예를 들어 CNN과 DailyMail은 뉴스 기사에 대해 생성하는 요약에서 특정 경향을 보일 수 있습니다. 편집자 모델과 호환되고 의미 있는 요약 수정으로 이어지도록 보상 함수는 소스 문서, 초기 요약 및 강사 모델에서 제공한 편집 지침을 포함하는 프롬프트를 사용하여 편집자 모델에서 생성한 편집된 요약을 기반으로 공식화됩니다(그림 3의 오른쪽 하단에 표시된 예제 프롬프트 참조). 편집된 요약의 품질을 정량화하기 위해 요약이 사용자의 선호도를 충족하는 정도를 측정하는 스코어링 함수 f(.)를 사용합니다. 생성된 요약의 적용 범위와 사실적 일관성을 사용자 요구 사항으로 중점적으로 다루면서, 스코어링 함수 f(.)를 ROUGE 점수와 지식 적용 범위의 합으로 설정합니다.이는 엔티티 수준 적용 범위와 참조 요약의 유사성을 측정합니다.f(S) = aROUGE(S, Sref) + ẞCov(S, Sref).보상 신호 자체는 초기 요약과 편집된 요약 간의 점수 차이로 정의되며, 요약 품질의 개선을 포착하도록 설계되었으며, 더 높은 보상은 보다 실질적인 개선에 해당합니다.보상 = f(Sedit) — f(Sinit).이 단계는 사용자 요구 사항을 준수할 뿐만 아니라 대규모 언어 모델이 개선된 요약을 생성하도록 효과적으로 안내하는 지침을 생성하는 모델의 기능을 향상시키는 것을 목표로 합니다.4 실험 사용자 선호도의 다른 측면을 각각 포착하는 두 개의 서로 다른 데이터 세트에 대한 실험을 수행합니다. 4.1 시나리오 1: DeFacto에서의 사실적 일관성 초기 실험 시나리오에서 우리는 사실적 일관성을 사용자의 요약 선호도에 대한 주요 기준으로 강조하기로 했습니다.4 우리는 DeFacto 데이터 세트(Liu et al., 2022b)를 사용합니다.이 리소스는 인간이 주석을 단 데모와 피드백을 포함하여 기계에서 생성된 요약의 사실적 일관성을 향상시키기 위해 특별히 큐레이팅되었습니다.이 데이터 세트는 각각 훈련/검증/테스트 세트의 701/341/*사실적 일관성이 일반적으로 요약자에게 기준이 될 수 있지만, 우리는 강사를 활용하여 요약의 사실적 일관성을 향상시키는 특정 지침을 작성하는 능력을 습득합니다.5 DeFacto 데이터 세트의 각 데이터 항목은 소스 문서와 PEGASUS에서 생성한 초기 요약으로 구성됩니다(Zhang et al., 2020).주석 작성자는 사실적 일관성을 향상시키기 위해 초기 요약을 수정하는 지침을 제공해야 합니다. 또한 주석자는 제공된 지침을 준수하고 개선된 사실적 일관성을 보여주는 개정된 요약을 생성합니다. 시스템에서 생성된 지침과 인간이 작성한 지침 간의 일치성을 평가하기 위해 ROUGE 점수를 평가 지표로 사용합니다. 또한 ROUGE 점수와 사실성 점수를 포함한 지표를 조합하여 생성된 요약의 품질을 인간의 기대와 사실적 정확성과 관련하여 평가합니다. 구체적으로 DAE(Dependency Arc Entailment) 지표(Goyal 및 Durrett, 2021)와 QFE(Question-answering for Factual Evaluation) 지표(Fabbri et al., 2022)를 사용하여 생성된 요약의 사실성을 정량화합니다. 이러한 지표는 인간의 기대와의 일치성과 사실적 정확성 준수 측면에서 요약 품질에 대한 포괄적인 평가를 제공합니다. 설정 강사의 백본 모델로 FlanT5-large(700M)(Chung et al., 2022)를 사용합니다. 강사를 위한 교육 과정은 섹션 3에 자세히 설명된 대로 두 단계로 실행됩니다.결과 우선, 우리는 ChatGPT가 인간이 제공한 지침에 따라 요약을 수정할 수 있는 편집자 모델 역할을 할 수 있는 잠재력을 평가합니다.표 1에 제시된 이 평가 결과는 ChatGPT가 소스 문서, 초기 요약 및 인간이 작성한 편집 지침을 입력으로 제공했을 때 감독 모델과 비슷한 성능을 보임을 나타내며, 이는 비슷한 ROUGE 점수와 사실성 점수로 입증됩니다.이러한 결과는 적절한 편집 지침이 제공될 때 ChatGPT가 요약 편집기로 효과적임을 확인합니다.그런 다음, 인간이 작성한 지침과 비교하여 시스템에서 생성한 지침을 평가합니다.우리의 목표는 ChatGPT와 훈련된 강사가 사용자 요구 사항을 정확하게 식별하고 그에 따라 해당 지침을 생성할 수 있는 정도를 파악하는 것입니다.결과 원래 논문에 따라 모든 실험은 오류가 표시된 예제에서 수행됩니다. 편집자 DAE QFE RRRL 초기 요약 인간 편집자 0.699 1.837 76.0.906 2.66.74.TOPP-D+S+I(Sup) ChatGPT(10-shot) 0.904 2.470 88.74 83.0.884 2.568 88.48 81.41 86.87.표 1: DeFacto에서 인간이 작성한 지침이 있는 편집된 요약의 ROUGE 점수 및 사실적 일관성 점수는 인간이 편집한 요약과 비교한 것입니다.TOPP-D+S+I(Sup)는 소스 문서, 초기 요약 및 지침을 입력으로 하는 지도 모델입니다(Liu et al., 2022b). 모델 RRRL ChatGPT(제로샷) ChatGPT(10샷) 36.37.22.98 30.24.94 32.FlanT5(Sup) FlanT5(RL) 49.04 34.37 47.48.05 32.94 46.표 2: DeFacto에서 생성된 명령어와 인간이 작성한 명령어 간의 ROUGE 점수. 이 평가의 결과는 표 2에 제시되어 있습니다. 주목할 점은 ChatGPT에서 생성된 명령어가 인간이 작성한 명령어와 효과적으로 일치하지 않는다는 것입니다. 이는 제로샷 및 퓨샷 설정에서 최적이 아닌 성능에서 입증됩니다. 우리가 사용한 강사 모델은 ChatGPT(700M 대 175B)보다 훨씬 작지만 사용자의 요구 사항에 더 잘 맞는 명령어를 생성할 수 있는 능력을 보여줍니다. 표 3에 제시된 최종 실험 세트에서 우리는 훈련되고 RL 미세 조정된 강사와 함께 편집 모델(ChatGPT)의 성능과 ChatGPT가 few-shot 설정에서 생성한 지침을 평가합니다. 결과에 따르면 10-shot 프롬프트와 훈련된 강사의 지침을 활용할 때 ChatGPT가 편집한 요약은 원래 요약에 비해 사실성(DAE/QFE로 측정)에서 큰 개선을 보입니다. ChatGPT에서 파생된 보상을 통합한 강화 학습을 구현하면 요약 품질이 추가로 향상됩니다. 또한 ChatGPT가 생성한 지침을 활용하여 실험을 수행합니다. 이러한 지침은 인간이 작성한 지침과 최적이 아니지만 QFE 메트릭으로 측정한 사실성 측면에서 예상치 못하게 높은 점수를 얻습니다. 그러나 다른 방법에 비해 ROUGE 점수가 현저히 감소하는 것으로 나타났습니다. 이러한 결과는 ChatGPT가 구체적이고 잘 정의된 측면(예: 사실적 불일치 해결)을 타겟으로 하는 지침을 생성하는 능력을 가지고 있지만, 더 광범위한 인간의 기대를 정확하게 식별하고 충족하는 데 어려움을 겪을 수 있음을 시사합니다. 4.2 시나리오 2: CNNDM에서의 적용 범위 ChatGPT는 뉴스 기사의 유창하고 유익한 요약을 생성하는 능력을 보여주었습니다(Goyal et al., 2022). 일관된 요약을 생성하는 데 능숙함에도 불구하고, 사용자가 기대하는 대로 주요 주제에 대한 원하는 적용 범위를 달성하지 못할 수 있습니다. 이러한 과제에 대응하여, 사용자의 이력을 기반으로 지식 적용 범위를 개선하기 위해 요약 편집을 안내하도록 특별히 설계된 강사 모델을 훈련하고 평가하는 실험을 수행합니다. 강사는 현재 요약에 추가되거나 제거될 키워드를 예측하여 요약을 사용자 선호도에 더 가깝게 정렬하기 위한 실행 가능한 지침을 제공합니다. 실제로, 생성된 요약이 키워드 콘텐츠 측면에서 참조 요약과 일치하는 정도에 따라 지식 적용 범위를 평가합니다. 우리는 이 실험의 벤치마크로 CNNDM 데이터 세트(Nallapati et al., 2016)를 사용하는데, 여기에는 기사와 참고 요약의 쌍이 포함되어 있으며, 원래 참고 요약은 커버리지에 대한 사용자 선호도의 대상 표현으로 사용됩니다. 우리는 최근 연구(Goyal et al., 2022)에 따르면 CNNDM 데이터 세트의 참고 요약은 일관성이 떨어지는 등 품질에 한계가 있을 수 있음을 인정합니다. 그러나 이 실험에서 우리의 주요 초점은 요약의 품질보다는 지식 커버리지에 있습니다. 우리는 생성된 요약이 참고의 주요 엔터티를 얼마나 포착하는지 평가하는 데 관심이 있습니다. 지식 커버리지를 측정하기 위해 엔터티 수준 매칭 메트릭 Knlg F1을 도입합니다. Egen은 생성된 요약에서 언급된 엔터티이고 Eref는 참고 요약에서 언급된 엔터티입니다. 우리는 강사 DAE QFE RRRL 초기 요약 0.1.837 76.03 66.34 74.FLAN T5(Sup) 0.FLAN T5(RL) 2.093 72.60 61.96 71.0.803 2.198 74.77 64.73 73.ChatGPT(10-shot) 0.834 2.583 56.54 41.29 53 사이의 중복 정도를 정량화합니다.표 3: DeFacto에서 다른 강사가 생성한 지침이 있는 편집된 요약의 ROUGE 점수 및 사실적 일관성 점수. 우리는 표에 표시된 모든 결과에 대한 편집자 모델로 ChatGPT(10-shot)를 사용합니다. 강사 Knlg F초기 요약 FLAN T5(Sup) FLAN T5(RL) ChatGPT(5-shot)* Oracle R44.15 40.28 16.65 33.47.44 41.04 16.72 33.47.99 41.21 16.80 33.43.43 39.46 15.43 32.RRL 모델 초기 요약 편집 Iter편집 Iter편집 Iter편집 Iter 1(1&amp;2) 편집 Iter 2(1&amp;2) 편집 Iter 3(1&amp;2) Knlg F1 RR44.15 40.28 16.65 33.47.99 41.21 16.80 33.48.65 41.18 16.69 33.48.99 41.14 16.63 33.48.08 41.25 16.91 33.48.87 40.62 16.60 33.49.20 41.15 16.87 33.RL 60.80 43.08 18.37 35.표 4: CNNDM에서 다른 강사가 생성한 지침이 있는 편집된 요약의 지식 범위 및 ROUGE 점수. 생성기 모델(초기 요약 생성) 및 편집자 모델로 ChatGPT(zeroshot)를 사용합니다.* 길이 제한(4k 토큰)을 초과하는 경우 프롬프트의 예제 수를 줄입니다.표 5: CNNDM의 반복 편집.두 번째 블록은 첫 번째 반복에서만 데이터에 대해 미세 조정된 모델의 결과를 보여주고, 아래 블록은 첫 번째 및 두 번째 반복에서 데이터에 대해 미세 조정된 모델의 결과를 보여줍니다. 두 개는 Knlg F = Knlgp 2Knlgp × Knlgr Knlgp + Knlgr Eref Egen | Egen 여기서 &quot; Knlgr Eref Egen | Eref 강사는 이 중복을 최대화함으로써 참조에서 표시된 대로 관련 정보를 효과적으로 포괄하는 요약을 생성하는 것을 목표로 합니다. 설정: ChatGPT에서 생성된 요약을 초기 요약으로 사용합니다. 6. 그리고 FlanT5-large(700M)를 강사 모델로 채택하여 원본 문서와 ChatGPT에서 생성된 초기 요약을 모두 입력으로 사용하여 키워드를 예측합니다. 지도 학습은 추가 및 제거할 키워드를 지정하는 Oracle 키워드 목록을 사용하여 수행됩니다. 그런 다음 모델은 효율성을 위해 데이터 세트에서 10,000개의 학습 예제 하위 집합을 사용하여 섹션 3에 자세히 설명된 대로 편집자 주도 강화 학습 미세 조정을 거칩니다. 결과: 표 4에 제시된 실험 결과는 &quot;데이터 세트가 릴리스되었으며 Github 저장소에서 찾을 수 있습니다. 지식 범위를 향상시키는 강사 모델의 결과는 엔티티 매칭과 ROUGE 점수로 나타납니다. 제로샷 설정에서 ChatGPT는 요약자로서 강력한 성능을 보여줍니다. 중요한 점은 Oracle 명령어가 제공될 때 ChatGPT는 지정된 명령어에 따라 초기 요약을 수정하고 정제하는 강력한 기능을 보여준다는 것입니다. 훈련된 강사 모델에서 생성된 명령어를 통합하면 지식 범위가 현저히 향상되어 요약이 사용자 선호도와 더 잘 일치함을 나타냅니다(FLAN T5(Sup)와 초기 요약 비교). 게다가 강화 학습 미세 조정 프로세스(FLAN T5(RL))가 모델의 성능을 더욱 향상시켜 평가된 메트릭에서 적당하지만 의미 있는 이득을 얻는 것을 관찰했습니다. 반면, 훈련된 강사를 사용하지 않고 ChatGPT가 직접 명령어를 생성하는 몇 가지 샷 설정(ChatGPT(5-shot))을 탐색하면 편집된 요약의 성능이 저하됩니다. 구체적으로, Knlg F1과 ROUGE 점수는 모두 초기 요약보다 낮아서 ChatGPT가 요약 편집에 대한 효과적인 지침을 생성하여 사용자의 선호도에 더 잘 맞출 수 있는 능력에 한계가 있음을 시사합니다. 전반적으로 이러한 결과는 초기 요약 Oracle 지침 인간이 작성한 참조 예측 지침 ChatGPT 편집 요약의 가치를 강조합니다. 전직 교도관은 비행기에서 한 청년이 욕설 사용을 중단하라고 한 후 그에게 주먹을 맞았습니다. 그런 다음 전직 교도관은 청년을 내려놓고 경찰이 도착할 때까지 그를 붙잡아 두었습니다. 출처: Daily Mail<Add> 채드 허스트<remove> Daily Mail 유타주 솔트레이크시티의 채드 허스트는 일요일에 도시에 착륙했을 때 비행기 승객에게 갑작스러운 펀치를 맞았습니다. 허스트가 비행기에서 내린 후 청년에게 욕설을 하지 말라고 한 후였습니다. 전 교도관인 허스트는 그 남자를 잡아눕히고 법 집행을 기다리는 동안 그의 팔을 등 뒤로 고정했습니다. 아직 경찰에서 이름을 밝히지 않은 그 청년은 폭행과 공공장소 취기 혐의로 기소되었습니다.<Add> 채드 허스트<remove> Daily Mail 유타주 솔트레이크시티의 전 교도관인 채드 허스트는 비행기에서 한 청년이 욕설을 그만하라고 한 후 그에게 주먹을 맞았습니다.허스트는 침착하게 그 청년을 잡아눕히고 경찰이 도착할 때까지 붙잡아 두었습니다.그 청년은 폭행과 공공장소 음주 혐의로 기소되었습니다.허스트는 전 교도관으로서의 훈련을 통해 결코 주먹으로 맞받아치지 말고 상황을 통제하고 그 사람을 잡아눕히도록 배웠습니다.표 6: CNNDM 데이터 세트의 예.사용자 선호도를 보다 면밀히 준수하도록 요약을 편집할 때 ChatGPT와 같은 대규모 언어 모델을 안내하는 강력한 중개자로서의 강사.5 토론 5.1 반복 편집 단일 단계 편집을 수행하는 것 외에도 CNNDM 데이터 세트에서 반복 편집의 효과를 알아보기 위한 실험을 수행했습니다.반복 편집 실험의 결과는 표 5에 나와 있습니다.첫 번째 반복의 데이터에만 기반한 강화 학습(RL) 훈련을 활용하여 반복 편집 프로세스에서 편집된 요약의 적용 범위가 향상되는 것을 관찰했습니다. 우리는 첫 번째와 두 번째 반복에서 얻은 데이터를 혼합하여 모델을 더욱 세부적으로 조정했으며, 이를 통해 반복적으로 편집된 요약에서 향상된 지식 F1을 통해 입증된 바와 같이 성능이 향상되었습니다. 5.2 정성적 예제 표 6에서 CNNDM 데이터 세트의 예를 보여줍니다. 강사 모델은 사용자의 기대를 올바르게 감지하고 편집 지침을 생성할 수 있습니다. ChatGPT는 주어진 지침에 따라 초기 요약을 편집하여 편집자 역할을 할 수 있습니다.&quot;우리는 DeFacto 데이터 세트에 대해 유사한 실험을 수행하지 않았습니다.대부분의 데이터 예제에서 초기 요약에서 인간이 편집한 요약으로 전환하는 데 편집 단계가 하나만 필요하기 때문입니다.DeFacto의 예는 부록에 나와 있습니다.6 관련 연구 6.1 텍스트 편집 사후 편집 기술은 문장 융합(Malmi et al., 2019), 스타일 전송(Reid and Zhong, 2021), 위키 편집(Reid and Neubig, 2022; Faltings et al., 2021)을 포함한 다양한 NLP 작업에서 광범위하게 연구되었습니다.이러한 방법에는 삽입, 삭제 및 바꾸기와 같은 미세하게 정의된 작업이 포함됩니다.그러나 종종 상당한 양의 인간이 레이블을 지정한 데이터 또는 복잡한 편집 체인이 필요합니다.반대로, 우리의 연구는 ChatGPT와 같은 대규모 언어 모델의 기능을 활용하여 자연어 지침을 사용하는 추상 수준의 텍스트 편집에 중점을 둡니다.마찬가지로 Liu et al. (2022b)는 피드백 생성을 위한 비평 모델과 초기 요약을 수정하기 위한 편집자 모델을 포함하는 접근 방식을 제안합니다. 우리는 이 접근 방식을 반복적 편집 파이프라인으로 공식화하고 추론 전용 언어 모델과 편집자 주도 강사로 향상시켜 확장합니다. 최근 (Liu et al., 2022a)는 동적 프로그래밍에서 파생된 편집 체인을 통해 생성된 텍스트를 인간의 가치와 일치시키는 새로운 교육 패러다임을 도입했습니다. 그러나 이 방법은 언어 모델의 추가 미세 조정이 필요하며, 이는 리소스와 접근성이 제한된 모델에는 비실용적일 수 있습니다. 또 다른 작업 라인에서 Welleck et al. (2022)은 원래 생성 프로세스를 생성자와 교정자 구성 요소로 분해하는 프레임워크를 제안했습니다. 여기서 교정자는 온라인 교육을 통해 불완전한 생성을 반복적으로 개선하도록 교육됩니다. 우리의 작업은 생성 프로세스를 생성자, 강사, 편집자의 세 가지 구성 요소로 분해하여 그들과 다릅니다. 이 분해를 통해 복잡한 생성 및 수정 작업에 대규모 모델을 활용하는 동시에 더 작은 모델을 사용하여 사용자별 편집 지침을 예측할 수 있습니다.Madaan 등(2023)은 연구와 병행하여 생성된 출력을 반복적으로 개선하는 것을 목표로 하는 유사한 생성 파이프라인을 제안합니다.그러나 그들의 접근 방식은 초기 출력을 생성하고, 피드백을 제공하고, 사용자별 피드백을 고려하지 않고 수신된 피드백을 기반으로 출력을 편집하기 위해 동일한 대규모 언어 모델(다양한 프롬프트 포함)을 사용한다는 점에서 다릅니다.반대로 이 논문에서는 훈련된 강사의 안내에 따라 생성된 출력을 사용자 요구 사항에 더 가깝게 맞추는 데 중점을 둡니다.6.2 대규모 언어 모델 자연어 처리 분야는 대규모 언어 모델(LLM) 영역에서 상당한 발전을 이루었습니다(Chowdhery 등, 2022; Zhang 등, 2022; Thoppilan 등, 2022).이를 통해 뛰어난 언어 처리 기능을 보이는 모델이 만들어졌습니다. 이러한 모델 중에서 GPT 패밀리(Brown et al., 2020)는 두드러진 사례로, 다양한 언어 관련 과제에서 다양한 성능을 보이는 것으로 널리 인정받고 있습니다. 명령어 튜닝(Wei et al., 2021)의 도입은 언어 모델의 향상을 더욱 촉진시켰으며, 특히 인간 명령어로 훈련할 때 더욱 그렇습니다(Sanh et al., 2021). 주목할 점은 이러한 접근 방식으로 상당한 개선이 이루어졌으며, 특히 제로샷 및 퓨샷 학습의 맥락에서 그렇습니다. 인간 피드백을 통한 강화 학습(RLHF) 훈련 패러다임을 사용하는 InstructGPT(Ouyang et al., 2022)는 이러한 추세를 잘 보여주며, 모델이 인간 명령어를 효과적으로 따를 수 있게 하고 현재 작업의 기초적인 토대를 제공합니다. 최근 LLAMA(Touvron et al., 2023)가 출시되면서 연구자들이 GPT 모델(Wang et al., 2022)에 의해 작업 증강 데이터 세트를 사용하여 모델을 훈련하거나 미세 조정하기 시작하면서 이 분야의 탐색 기회가 더욱 확대되었습니다. 앞서 언급한 연구 노력과 달리, 저희의 작업은 다운스트림 작업을 위해 대규모 언어 모델의 기능을 활용하는 새로운 패러다임인 트라이 에이전트 파이프라인을 도입합니다. 저희의 접근 방식은 컴퓨팅 리소스 수요를 최소화하고 대규모 언어 모델에 대한 제한된 액세스(예: API 전용 액세스)를 수용하면서 성능을 최적화하도록 설계되었습니다. 6.3 LLM을 사용한 요약 LLM이 등장하기 전에 텍스트 요약 작업에 대한 널리 퍼진 접근 방식은 작업 중심 목표를 사용하여 상당한 코퍼스에서 모델을 사전 훈련한 다음 작업별 데이터 세트에서 미세 조정하는 것이었습니다. 이 패러다임은 텍스트 요약에서 효과적임을 입증했으며 PEGASUS(Zhang 등, 2020), Primera(Xiao 등, 2021), Z-Code++(He 등, 2022)와 같은 모델에서 채택되었습니다. 그러나 최근 연구(Goyal 등, 2022; Zhang 등, 2023)에 따르면 GPT-3(Brown 등, 2020)와 InstructGPT(Ouyang 등, 2022)를 제로샷 설정의 뉴스 요약 작업에 적용하면 인간 평가자가 감독 모델보다 선호할 뿐만 아니라 참조 요약 자체보다 더 유리한 결과가 도출됩니다. 이러한 결과는 텍스트 요약 작업의 방향을 시사합니다. 잠재적으로 최적이 아닌 참조 요약에 대해 감독 요약자를 훈련하는 것보다 LLM을 활용하고 사용자 요구 사항에 맞게 출력을 편집하는 데 집중하는 것이 더 효율적일 수 있습니다. 이는 이 연구에서 제안하는 3중 에이전트 파이프라인과도 일치합니다. 7
--- CONCLUSION ---
및 향후 작업 이 논문에서는 생성 프로세스를 생성기, 강사, 편집자의 세 가지 고유한 구성 요소로 분해하는 새로운 생성 패러다임을 소개합니다. 저희의 접근 방식은 제한된 액세스 및 계산 리소스와 같은 제약 조건을 고려하면서 대규모 언어 모델의 기능을 활용하고 생성된 콘텐츠를 사용자 선호도에 맞게 사용자 정의할 수 있도록 특별히 설계되었습니다. 텍스트 요약 작업에 대한 일련의 파일럿 실험을 통해 ChatGPT로 예시되는 대규모 언어 모델이 효과적으로 편집자 역할을 할 수 있으며 인간이 작성한 지침이 제공될 때 감독 편집 모델과 비슷한 성능 수준을 달성할 수 있음을 발견했습니다. 그럼에도 불구하고 대규모 언어 모델이 인간이 작성한 지침과 잘 일치하는 지침을 생성하는 것은 여전히 어렵습니다. 이 과제를 해결하기 위해 저희는 편집자 주도 강화 학습(RL)으로 훈련되고 편집된 요약의 품질에 따라 보상이 제공되는 소규모 모델을 강사로 사용합니다. 저희의 실험 결과는 이 접근 방식이 편집자(ChatGPT)가 사용자 기대치와 더 긴밀하게 일치하는 요약을 생성하도록 안내하는 데 효과적임을 보여줍니다. 앞으로의 작업에는 위키 편집(Reid and Neubig, 2022), 뉴스 편집(Spangher et al., 2022), 수학적 문제 합성(Welleck et al., 2022)과 같은 다른 작업으로 실험을 확장하는 것이 포함될 것입니다. 또한, 더 나은 강사를 교육하기 위해 자체 지시 기술(Wang et al., 2022)을 사용하여 더 많은 지침 데이터를 생성할 수 있습니다. 제한 사항 제안된 생성 파이프라인은 사용자 선호도와 일치하는 대규모 언어 모델 출력의 정렬을 개선하는 것을 목표로 하지만, 연구에서 리소스 제약의 한계를 인정합니다. 결과적으로, 다양한 작업에서 최고의 성능을 입증한 ChatGPT에만 실험을 집중합니다. 그러나 향후 작업에서는 다른 대규모 언어 모델에 대한 적용 가능성과 성능을 탐구해야 합니다. 또한, 모든 대규모 언어 모델과 마찬가지로 시스템의 출력에는 여전히 환각과 편향과 같은 문제가 있을 수 있다는 점에 유의하는 것이 중요합니다. 파이프라인이 이러한 문제를 부분적으로 해결하지만, 결과가 환각과 편향에서 완전히 자유롭다고 보장할 수는 없습니다. 참고문헌 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. CORR, abs/2005.14165. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro 및 Yi Zhang. 2023. 일반 인공 지능의 불꽃: gpt-4를 사용한 초기 실험. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le 및 Jason Wei. 2022. 지침 미세 조정 언어 모델 확장. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu 및 Caiming Xiong. 2022. QAFactEval: 요약을 위한 QA 기반 사실적 일관성 평가가 개선되었습니다. 2022년 북미 컴퓨터 언어학회 학술 대회 회의록: 인간 언어 기술, 2587-2601쪽, 미국 시애틀. 펠릭스 팔팅스, 미셸 갤리, 제럴드 힌츠, 크리스 브로켓, 크리스 퀴크, 지안펭 가오, 빌 돌란. 2021. 명령에 의한 텍스트 편집. 2021년 북미 컴퓨터 언어학회 학술 대회 회의록: 인간 언어 기술, 5259-5274쪽, 온라인. 에세이, 타냐 고얄, 그렉 더렛. 2021. 요약에서 세분화된 사실성에 대한 주석 달기 및 모델링. 2021년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술. Tanya Goyal, Junyi Jessy Li, Greg Durrett. 2022. GPT-3 시대의 뉴스 요약 및 평가. Jiatao Gu, Changhan Wang, Junbo Zhao. 2019. Levenshtein transformer. 신경 정보 처리 시스템의 발전, 32권. Curran Associates, Inc. Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, Xuedong Huang. 2022. Zcode++: 추상적 요약을 위해 최적화된 사전 학습된 언어 모델. Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony Liu, Soroush Vosoughi. 2022a. 두 번째 생각이 가장 좋다: 텍스트 편집을 통해 인간의 가치와 다시 정렬하는 법. 신경 정보 처리 시스템의 발전, 35권, 181~196페이지. Curran Associates, Inc. Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Halfaker, Dragomir Radev, Ahmed H. Awadallah. 2022b. 자연어 피드백을 통한 요약 사실적 일관성 개선에 관하여. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. 자체 개선: 자체 피드백을 통한 반복적 개선. Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, and Aliaksei Severyn. 2019. 인코딩, 태그, 실현: 고정밀 텍스트 편집. 2019년 자연어 처리 경험적 방법 컨퍼런스 및 제9회 자연어 처리 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 5054~5065쪽, 홍콩, 중국. Association for Computational Linguistics. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang. 2016. 시퀀스-투-시퀀스 RNN 및 그 이상을 사용한 추상 텍스트 요약. 20th SIGNLL Conference on Computational Natural Language Learning의 회의록, 280-290쪽, 베를린, 독일. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. 인간의 피드백을 통해 지침을 따르도록 언어 모델을 훈련합니다. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi. 2023. 강화 학습이 자연어 처리를 위한 것이 아닌가: 자연어 정책 최적화를 위한 벤치마크, 기준선 및 빌딩 블록. Machel Reid와 Graham Neubig. 2022. 편집 프로세스를 모델링하는 방법. Association for Computational Linguistics: EMNLP 2022의 연구 결과, 3822-3832페이지, 아랍에미리트 아부다비. Association for Computational Linguistics. Machel Reid와 Victor Zhong. 2021. LEWIS: 비지도 텍스트 스타일 전송을 위한 Levenshtein 편집. 계산 언어학 협회의 연구 결과: ACL-IJCNLP 2021, 3932-3944페이지, 온라인. 계산 언어학 협회. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, 김태운, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, 그리고 알렉산더 M. 러쉬. 2021. 멀티태스크 유도 학습은 제로샷 태스크 일반화를 가능하게 함. CoRR, abs/2110.08207. Alexander Spangher, Xiang Ren, Jonathan May, Nanyun Peng. 2022. Newsedits: 뉴스 기사 수정 데이터 세트와 문서 수준 추론 챌린지. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben 허친슨, 크리스틴 Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed H. Chi 및 Quoc Le. 2022. Lamda: 대화 상자 애플리케이션을 위한 언어 모델. CoRR, ABS/2201.08239. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. 2022. 자체 지시: 언어 모델을 자체 생성 지침과 정렬. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le. 2021. 미세 조정된 언어 모델은 제로 샷 학습자입니다. CORR, abs/2109.01652. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi. 2022. 자체 수정을 학습하여 시퀀스 생성. Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan. 2021. PRIMER: 다중 문서 요약을 위한 피라미드 기반 마스크 문장 사전 학습. CORR, abs/2110.08499. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. Pegasus: 추출된 갭 문장을 사용한 사전 학습을 통한 추상적 요약. 제37회 국제 머신 러닝 컨퍼런스 회의록, ICML&#39;20. JMLR.org. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: 사전 학습된 트랜스포머 언어 모델 열기. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, Tatsunori B. Hashimoto. 2023. 뉴스 요약을 위한 대규모 언어 모델 벤치마킹. A 프롬프트 요약 편집과 지침 생성에 사용된 프롬프트를 각각 표 7과 표 8에 표시합니다. CNNDM 요약: [초기 요약] 문서: [기사] 문서에 대한 요약을 다시 작성합니다. [지침] 새 요약: DeFacto 문서: [기사] 요약: [초기 요약] 지침: [지침] 지침에 따라서만 요약을 편집하고 수정된 요약만 출력합니다. 새 요약: 표 7: 요약 편집에 사용된 프롬프트. CNNDM few-shot prompts ×N, 최대 길이 제한 문서: [기사]į 요약: [초기 요약]į 지침: [지침]į 문서: [기사] 요약: [초기 요약] 요약은 중요한 내용을 다루지 않을 수 있으므로 요약이 중요한 내용에 초점을 맞추도록 지침을 생성하세요. 지침은 다음 형식에서 선택해야 합니다. 관련 내용 삭제 관련 내용 추가 작업이 필요하지 않습니다. 수정된 요약 없이 지침만 출력하고 지침을 보수적으로 작성하세요. 지침: 사실상 few-shot prompts ×문서: [기사]į 요약: [초기 요약]i 요약에 사실 오류가 있을 수 있으므로 요약을 수정하는 지침을 생성하세요. 지침: 문서: [기사] 요약: [초기 요약] 요약에 사실 오류가 있을 수 있으므로 요약을 수정하는 지침을 생성하세요. 지침은 다음 형식에서 선택해야 합니다. 정보 제거 정보 추가 요약에서 정보 바꾸기. 정보 수정 요약을 완전히 다시 작성하세요. 요약의 정보를 사용하여. 수정된 요약 없이 지침만 출력하고, 지침을 보수적으로 작성합니다. 지침: 표 8: 지침 생성에 사용된 프롬프트 B 정성적 예제 표 6에서 DeFacto 데이터 세트의 예를 보여줍니다. 강사 모델은 사용자의 기대를 올바르게 감지하고 편집 지침을 생성할 수 있습니다. ChatGPT는 제공된 지침에 따라 초기 요약을 편집하여 편집자 역할을 할 수 있습니다. 초기 요약 인간이 작성한 지침 인간이 편집한 요약 예측 지침 ChatGPT 편집 요약 초기 요약 인간이 작성한 지침 인간이 편집한 요약 예측 지침 ChatGPT 편집 요약 인도 타밀나두 주의 논란이 되고 있는 쿠단칼룸 원자력 발전소가 전기를 생산하기 시작했습니다. 요약에서 인도 타밀나두 주의 위치에 대한 정보를 제거하십시오. 논란이 되고 있는 쿠단칼룸 원자력 발전소가 전기를 생산하기 시작했습니다. 요약에서 타밀나두에 대한 정보를 제거하십시오. 논란이 되고 있는 쿠단칼룸 원자력 발전소가 전기를 생산하기 시작했습니다. 총소리가 코트디부아르의 두 번째 도시인 부아케에서 들렸는데, 이는 군인들이 급여를 놓고 반란을 일으킨 지 하루 후입니다. 요약에서 두 번째에 대한 정보를 제거하세요. 총소리가 코트디부아르의 부아케에서 들렸는데, 이는 군인들이 급여를 놓고 반란을 일으킨 지 하루 후입니다. 요약에서 두 번째에 대한 정보를 제거하세요. 총소리가 코트디부아르의 부아케에서 들렸는데, 이는 군인들이 급여를 놓고 반란을 일으킨 지 하루 후입니다. C 소프트웨어 및 라이선스 표 9: DeFacto 데이터 세트의 예. 저희 코드는 Apache 라이선스 2.0에 따라 라이선스가 부여되었습니다. 프레임워크 종속성은 다음과 같습니다.• HuggingFace Datasets, Apache 2.• NLTK 10, Apache 2.• Numpy¹¹, BSD 3-Clause &quot;New&quot; 또는 &quot;Revised&quot; • Transformers 12, Apache 2.• Pytorch¹³, Misc • ROUGE 14, Apache 2.• Flan T5 15, Apache 2.• ChatGPT 16, Proprietary https://github.com/huggingface/datasets/blob/ master/LICENSE 10https://github.com/nltk/nltk https://github.com/numpy/numpy/blob/main/ LICENSE.txt 12 https://github.com/huggingface/transformers/ blob/master/LICENSE 13 https://github.com/pytorch/pytorch/blob/ master/LICENSE 14 https://github.com/google-research/ google-research/tree/master/rouge 15https://huggingface.co/google/flan-t5-large 16 https://openai.com/chatgpt
