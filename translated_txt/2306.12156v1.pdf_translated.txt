--- ABSTRACT ---
최근 제안된 세그먼트 무엇이든 모델(SAM)은 많은 컴퓨터 비전 작업에 상당한 영향을 미쳤습니다. 이는 이미지 분할, 이미지 캡션, 이미지 편집과 같은 많은 고급 작업의 기초 단계가 되고 있습니다. 그러나 엄청난 계산 비용으로 인해 산업 시나리오에서 더 광범위하게 적용할 수 없습니다. 계산은 주로 고해상도 입력에서 Transformer 아키텍처에서 이루어집니다. 이 논문에서는 비슷한 성능을 가진 이 기본 작업에 대한 속도 향상 대안 방법을 제안합니다. 작업을 세그먼트 생성 및 프롬프트로 재구성하여 인스턴스 분할 분기가 있는 일반 CNN 감지기도 이 작업을 잘 수행할 수 있음을 발견했습니다. 구체적으로 이 작업을 잘 연구된 인스턴스 분할 작업으로 변환하고 SAM 작성자가 게시한 SA-1B 데이터 세트의 1/50만 사용하여 기존 인스턴스 분할 방법을 직접 학습합니다. 이 방법을 사용하면 50배 더 높은 런타임 속도에서 SAM 방법과 비슷한 성능을 달성합니다. 효과를 입증하기에 충분한 실험 결과를 제공합니다. 코드와 데모는 https://github.com/CASIA-IVA-Lab/FastSAM에서 공개됩니다. 50% 정밀도에서의 재현율(%) ㅁ BSDS500 R50 평가 대 FPS 속도 SAM VIT-H E(16x16) (852ms/img)(པ) FastSAM 40ms/img SAM 2099ms/img (a) FastSAM (TRT) YOLOv8x (12ms/img) ☐ FastSAM YOLOV8x (40ms/img) 실시간 초당 10¹프레임(img/s) (b) 박스 평균 재현율@1000 (%) 75.72.COCO 박스 AR@1000 평가 대 FPS 속도 VIT-HE(64x64) (6972ms/img) SAM 67.5 A 65.FastSAM (TRT) YOLOV8x (12ms/img) A Δ FastSAM YOLOV8x (40ms/img) 50 × 더 빠름 62.SAM* VIT-H E(32×32) (2099ms/img) 60.A SAM 57.VIT-B E(32×32) (1383ms/img) 실시간 55.초당 프레임(img/s) (c) 그림 1. FastSAM과 SAM의 비교 분석. (a) 단일 NVIDIA GeForce RTX 3090에서 FastSAM과 SAM의 속도 비교. (b) 에지 감지를 위한 BSDS500 데이터 세트[1,28]에서의 비교. (c) 객체 제안을 위한 COCO 데이터 세트[25]에서 FastSAM과 SAM의 Box AR@ 1000 평가. SAM과 FastSAM은 모두 추론을 위해 PyTorch를 사용하여 테스트되었지만 FastSAM(TRT)은 추론을 위해 TensorRT를 사용합니다. 1.
--- METHOD ---
이 기본 작업에 대해 비슷한 성능을 제공합니다. 작업을 세그먼트 생성 및 프롬프트로 재구성하면 인스턴스 분할 분기가 있는 일반 CNN 감지기도 이 작업을 잘 수행할 수 있음을 알 수 있습니다. 구체적으로 이 작업을 잘 연구된 인스턴스 분할 작업으로 변환하고 SAM 작성자가 게시한 SA-1B 데이터 세트의 1/50만 사용하여 기존 인스턴스 분할 방법을 직접 학습합니다. 저희 방법을 사용하면 50배 더 높은 런타임 속도에서 SAM 방법과 비슷한 성능을 달성합니다. 충분한
--- EXPERIMENT ---
모든 결과는 그 효과를 입증합니다. 코드와 데모는 https://github.com/CASIA-IVA-Lab/FastSAM에서 공개됩니다. 50% 정밀도에서의 재현율(%) ㅁ BSDS500 R50 평가 대 FPS 속도 SAM VIT-H E(16x16) (852ms/img)(པ) FastSAM 40ms/img SAM 2099ms/img (a) FastSAM (TRT) YOLOv8x (12ms/img) ☐ FastSAM YOLOV8x (40ms/img) 실시간 초당 10¹프레임(img/s) (b) 박스 평균 재현율@1000 (%) 75.72.COCO 박스 AR@1000 평가 대 FPS 속도 VIT-HE(64x64) (6972ms/img) SAM 67.5 A 65.FastSAM (TRT) YOLOV8x (12ms/img) A Δ FastSAM YOLOV8x (40ms/img) 50 × 더 빠름 62.SAM* VIT-H E(32×32) (2099ms/img) 60.A SAM 57.VIT-B E(32×32) (1383ms/img) 실시간 55.초당 프레임(img/s) (c) 그림 1. FastSAM과 SAM의 비교 분석. (a) 단일 NVIDIA GeForce RTX 3090에서 FastSAM과 SAM의 속도 비교. (b) 에지 감지를 위한 BSDS500 데이터 세트[1,28]에서의 비교. (c) 객체 제안을 위한 COCO 데이터 세트[25]에서 FastSAM과 SAM의 Box AR@ 1000 평가. SAM과 FastSAM은 모두 추론을 위해 PyTorch를 사용하여 테스트되었지만 FastSAM(TRT)은 추론을 위해 TensorRT를 사용합니다. 1. 서론 최근, SAM(Segment Anything Model)[19]이 제안되었습니다. 이는 이정표적 비전 기반 모델로 간주됩니다. 이는 다양한 가능한 사용자 상호 작용 프롬프트에 따라 이미지 내의 모든 객체를 분할할 수 있습니다. SAM은 광범위한 SA-1B 데이터 세트에서 학습된 Transformer 모델을 활용하여 광범위한 장면과 객체를 능숙하게 처리할 수 있는 기능을 제공합니다. SAM은 Segment Anything이라는 흥미로운 새로운 작업으로의 문을 엽니다. 이 작업은 일반화 가능성과 잠재력으로 인해 광범위한 미래 비전 작업의 초석이 될 수 있는 모든 요소를 갖추고 있습니다. 그러나 이러한 발전과 SAM 및 후속 모델이 Segment Anything 작업을 처리하는 데 보여준 유망한 결과에도 불구하고 실제 적용은 여전히 어렵습니다. 가장 눈에 띄는 문제는 SAM 아키텍처의 주요 부분인 Transformer(VIT) 모델과 관련된 상당한 계산 리소스 요구 사항입니다. 합성곱 대응 모델과 비교할 때 ViT는 많은 계산 리소스 요구 사항으로 두드러지며, 이는 특히 실시간 애플리케이션에서 실제 배포에 장애물이 됩니다. 이러한 제한은 결과적으로 세그먼트 무엇이든 작업의 진행과 잠재력을 방해합니다.산업용 애플리케이션에서 세그먼트 무엇이든 모델에 대한 수요가 높아 이 논문에서 세그먼트 무엇이든 작업에 대한 실시간 솔루션인 FastSAM을 설계합니다.세그먼트 무엇이든 작업을 모든 인스턴스 분할과 프롬프트 가이드 선택이라는 두 가지 연속적인 단계로 분리합니다.첫 번째 단계는 CNN(Convolutional Neural Network) 기반 감지기의 구현에 달려 있습니다.이미지의 모든 인스턴스에 대한 분할 마스크를 생성합니다.그런 다음 두 번째 단계에서 프롬프트에 해당하는 관심 영역을 출력합니다.CNN의 계산 효율성을 활용하여 성능 품질을 크게 손상시키지 않고도 실시간 세그먼트 무엇이든 모델을 달성할 수 있음을 보여줍니다.제안된 방법이 기본 작업인 무엇이든 분할의 산업적 애플리케이션을 용이하게 하기를 바랍니다.제안된 FastSAM은 YOLACT[4] 방법을 활용하는 인스턴스 분할 분기가 장착된 객체 감지기인 YOLOv8-seg[16]를 기반으로 합니다. 또한 SAM에서 발행한 광범위한 SA-1B 데이터 세트를 채택했습니다. 이 CNN 감지기를 SA-1B 데이터 세트의 2%(1/50)에 대해서만 직접 학습시켜 SAM과 비슷한 성능을 달성했지만, 계산 및 리소스 요구 사항이 대폭 감소하여 실시간 적용이 가능했습니다. 또한 여러 다운스트림 분할 작업에 적용하여 일반화 성능을 보여주었습니다. MS COCO[13]의 객체 제안 작업에서 AR1000에서 63.7을 달성했는데, 이는 32×32포인트 프롬프트 입력을 사용하는 SAM보다 1.2포인트 더 높았지만 단일 NVIDIA RTX 3090에서 50배 더 빠르게 실행되었습니다. 실시간 분할 모델은 산업용 애플리케이션에 가치가 있습니다. 많은 시나리오에 적용할 수 있습니다. 제안된 접근 방식은 많은 수의 비전 작업에 대한 새롭고 실용적인 솔루션을 제공할 뿐만 아니라 현재 방법보다 수십 또는 수백 배 더 빠른 속도로 이를 수행합니다. 또한 일반 비전 작업을 위한 대규모 모델 아키텍처에 대한 새로운 뷰를 제공합니다. 우리는 특정 작업의 경우 특정 모델이 여전히 더 나은 효율성 정확도 균형을 얻기 위해 이점을 활용한다고 생각합니다.그런 다음 모델 압축의 의미에서 우리의 접근 방식은 구조에 인공 사전을 도입하여 계산 노력을 크게 줄일 수 있는 경로의 타당성을 보여줍니다.우리의 기여는 다음과 같이 요약할 수 있습니다.• Segment Anything 작업을 위한 새로운 실시간 CNN 기반 솔루션이 소개되어 경쟁력 있는 성능을 유지하면서 계산 요구 사항을 크게 줄입니다.• 이 작업은 segment anything 작업에 CNN 감지기를 적용하는 첫 번째 연구를 제시하여 복잡한 비전 작업에서 가벼운 CNN 모델의 잠재력에 대한 통찰력을 제공합니다.• 여러 벤치마크에서 제안된 방법과 SAM 간의 비교 평가는 segment anything 도메인에서 접근 방식의 강점과 약점에 대한 통찰력을 제공합니다.2. 예비 이 섹션에서는 segment anything 모델을 검토하고 segment anything 작업에 대한 명확한 정의를 제공합니다.Segment Anything 모델. 진화하는 이미지 분할 분야에서 SAM(Segment Anything Model)[19]은 제안된 학습 방법론과 대규모 시각적 데이터 세트에서의 성능으로 인해 중요한 혁신입니다.SAM은 고정밀, 클래스 독립적인 분할 성능을 제공하여 제로샷 작업에서 뚜렷한 역량을 보여줍니다.기반 모델로서 강력한 대화형 분할 방법뿐만 아니라 다양한 분할 작업에서 뛰어난 적응성을 보여줌으로써 컴퓨터 비전의 지평을 넓힙니다.SAM은 오픈 월드 이미지 이해를 위한 기반 모델의 잠재력을 보여주는 두드러진 예입니다.그러나 모델의 성능이 만족스럽기는 하지만 SAM은 실시간 처리 기능의 부족이라는 상당한 한계에 직면해 있다는 점에 주목할 가치가 있습니다.이로 인해 즉각적인 분할 결과가 중요한 시나리오에서의 광범위한 적용이 제한됩니다.Segment Anything 작업.Segment Anything 작업은 프롬프트의 모든 형태가 주어졌을 때 효과적인 분할 마스크가 생성되는 프로세스로 정의됩니다. 이러한 프롬프트는 전경/배경 포인트 세트, 거친 상자 또는 마스크, 자유형 텍스트 또는 이미지 내에서 분할할 콘텐츠를 나타내는 모든 정보에 이르기까지 다양합니다.우리는 대부분의 실제 응용 프로그램에서 무엇이든 분할하는 작업을 효과적으로 두 단계로 나눌 수 있음을 발견했습니다.첫 번째 단계는 파노라마 분할[18] 프로세스와 같이 이미지의 모든 객체를 감지하고 분할하는 것입니다.두 번째 단계는 제공된 프롬프트에 따라 분할된 파노라마에서 관심 있는 특정 객체를 분리합니다.이 작업을 분리하면 복잡성이 크게 줄어들어 무엇이든 모델의 실시간 분할을 제안할 가능성이 제공됩니다.3. 방법론 3.1. 개요 그림 2는 제안된 방법인 FastSAM의 개요를 보여줍니다.이 방법은 Allinstance 분할과 프롬프트 안내 선택의 두 단계로 구성됩니다.전자 단계는 기초이고 두 번째 단계는 본질적으로 작업 지향 후처리입니다. 엔드투엔드 변환기[7,8, 19]와 달리 전체 방법은 로컬 컨볼루션 연결 및 Detect Branch NMS PMask Coeff와 같은 비전 분할 작업과 일치하는 많은 인간 사전 확률을 도입합니다.Detect PMask Coeff.CNN FPN Crop Detect PBackbone Mask Coeff.Pred Detect ProtoNet Mask Branch +1+0.-0.+0.-1Mask Coeff.Point-prompt Box-prompt Text-prompt &quot;The black dog&quot; L₁T₁ 12-T₂ 임계값 ↓ Text Image Text Encoder Encoder INT₁ T₁ 715-CLIP 그림 2. FastSAM의 프레임워크.여기에는 모든 인스턴스 분할(AIS)과 프롬프트 안내 선택(PGS)의 두 단계가 포함됩니다.YOLOV8-seg[16]를 사용하여 이미지의 모든 객체 또는 영역을 분할합니다. 그런 다음 다양한 프롬프트를 사용하여 관심 있는 특정 객체를 식별합니다. 주로 포인트 프롬프트, 상자 프롬프트 및 텍스트 프롬프트를 활용합니다. 텍스트 프롬프트는 수용 필드 관련 객체 할당 전략인 CLIP[31]을 기반으로 합니다. 이를 통해 비전 분할 작업에 맞게 조정되며 더 적은 수의 매개변수로 더 빠르게 수렴할 수 있습니다. 3.2. 모든 인스턴스 분할 모델 아키텍처. YOLOv8[16]의 아키텍처는 이전 버전인 YOLOv5[15]에서 발전하여 YOLOX[10], YOLOV6[22] 및 YOLOv7[35]과 같은 최신 알고리즘의 주요 설계 측면을 통합합니다. YOLOv8의 백본 네트워크와 넥 모듈은 YOLOv5의 C3 모듈을 C2f 모듈로 대체합니다. 업데이트된 헤드 모듈은 분리된 구조를 채택하여 분류 및 감지 헤드를 분리하고 앵커 기반에서 앵커 없는 방식으로 전환합니다. 인스턴스 분할. YOLOV8-seg는 인스턴스 분할에 YOLACT [4] 원리를 적용합니다. 백본 네트워크와 Feature Pyramid Network(FPN) [24]를 통해 이미지에서 피처 추출로 시작하여 다양한 크기의 피처를 통합합니다. 출력은 감지 및 분할 분기로 구성됩니다. 감지 분기는 범주와 경계 상자를 출력하는 반면 분할 분기는 k개의 프로토타입(FastSAM에서는 기본적으로 32개)과 k개의 마스크 계수를 출력합니다. 분할 및 감지 작업은 병렬로 계산됩니다. 분할 분기는 고해상도 피처 맵을 입력하고 공간 세부 정보를 보존하며 의미 정보도 포함합니다. 이 맵은 합성곱 계층을 통해 처리되고 업스케일된 다음 두 개의 합성곱 계층을 거쳐 마스크를 출력합니다. 감지 헤드의 분류 분기와 유사한 마스크 계수는 -1과 1 사이의 범위입니다. 인스턴스 분할 결과는 마스크 계수를 프로토타입과 곱한 다음 합산하여 얻습니다. YOLOV8은 다양한 객체 감지 작업에 사용할 수 있습니다. 인스턴스 분할 브랜치를 통해 YOLOV8Seg는 이미지의 모든 객체나 영역을 객체 범주에 관계없이 정확하게 감지하고 분할하는 것을 목표로 하는 모든 것 분할 작업에 적합하게 탄생했습니다.프로토타입과 마스크 계수는 신속한 안내를 위한 많은 확장성을 제공합니다.간단한 예로, 간단한 프롬프트 인코더와 디코더 구조가 추가로 학습되고 다양한 프롬프트와 이미지 피처 임베딩이 입력으로, 마스크 계수가 출력으로 사용됩니다.FastSAM에서 우리는 모든 인스턴스 분할 단계에 YOLOV8-seg 방법을 직접 사용합니다.더 인공적인 디자인은 추가적인 개선을 가져올 수 있지만, 우리는 그것을 이 작업의 범위를 벗어나는 것으로 간주하고 향후 연구로 남겨둡니다.3.3. 프롬프트 안내 선택 YOLOv8을 사용하여 이미지의 모든 객체나 영역을 성공적으로 분할한 후, 모든 것 분할 작업의 두 번째 단계는 다양한 프롬프트를 사용하여 관심 있는 특정 객체를 식별하는 것입니다.주로 포인트 프롬프트, 상자 프롬프트 및 텍스트 프롬프트를 활용합니다.포인트 프롬프트. 포인트 프롬프트는 선택된 포인트를 첫 번째 단계에서 얻은 다양한 마스크와 일치시키는 것으로 구성됩니다. 목표는 포인트가 위치한 마스크를 결정하는 것입니다. SAM과 유사하게, 우리는 우리의 접근 방식에서 프롬프트로 전경/배경 포인트를 사용합니다. 전경 포인트가 여러 마스크에 있는 경우, 배경 포인트를 활용하여 해당 작업과 관련 없는 마스크를 필터링할 수 있습니다. 전경/배경 포인트 세트를 사용하여 관심 영역 내에서 여러 마스크를 선택할 수 있습니다. 이러한 마스크는 관심 객체를 완전히 표시하기 위해 단일 마스크로 병합됩니다. 또한, 우리는 형태학적 연산을 활용하여 마스크 병합의 성능을 개선합니다. 상자 프롬프트. 상자 프롬프트는 선택된 상자와 첫 번째 단계의 다양한 마스크에 해당하는 경계 상자 간에 IoU(Intersection over Union) 매칭을 수행하는 것을 포함합니다. 목표는 선택된 상자와 가장 높은 IoU 점수를 가진 마스크를 식별하여 관심 객체를 선택하는 것입니다. 텍스트 프롬프트. 텍스트 프롬프트의 경우, 텍스트의 해당 텍스트 임베딩은 CLIP [31] 모델을 사용하여 추출됩니다. 그런 다음 각각의 이미지 임베딩을 결정하고 유사도 메트릭을 사용하여 각 마스크의 내재적 특징과 일치시킵니다. 그런 다음 텍스트 프롬프트의 이미지 임베딩과 가장 높은 유사도 점수를 가진 마스크가 선택됩니다. 이러한 프롬프트 가이드 선택 기술을 신중하게 구현함으로써 FastSAM은 분할된 이미지에서 관심 있는 특정 객체를 안정적으로 선택할 수 있습니다. 위의 접근 방식은 실시간으로 무엇이든 분할하는 작업을 수행하는 효율적인 방법을 제공하므로 복잡한 이미지 분할 작업에 대한 YOLOv8 모델의 유용성을 크게 향상시킵니다. 보다 효과적인 프롬프트 가이드 선택 기술은 향후 탐색을 위해 남겨둡니다. 4. 실험 이 섹션에서는 먼저 FastSAM의 런타임 효율성을 분석합니다. 그런 다음 실제 시나리오의 응용 프로그램, 효율성 및 배포와 함께 4개의 제로샷 작업을 실험합니다. 실험의 첫 번째 부분에서는 FastSAM과 SAM 간의 기능 유사성을 테스트하는 것이 목표입니다. SAM에 이어서 우리는 또한 서로 다른 수준의 4가지 작업을 실험한다: (1) 저수준: 에지 감지, (2) 중수준: 객체 제안 생성, (3) 고수준: 인스턴스 분할, 마지막으로 (4) 고수준: 자유형 텍스트 입력을 사용한 객체 분할. 우리의 실험은 또한 실제 응용 프로그램과 속도로 FastSAM의 기능을 더욱 검증한다. 구현 세부 정보. 달리 명시되지 않는 한 다음 조건이 적용된다: (1) FastSAM은 입력 크기가 1024인 아키텍처의 주요 부분으로 YOLOV8-x [16] 모델을 사용한다; (2) FastSAM의 훈련은 SA-1B 데이터 세트 [19]의 2%에서 수행되었다; (3) 우리는 큰 인스턴스를 예측하기 위해 경계 상자 회귀 모듈의 reg_max를 16에서 26으로 변경하는 것을 제외하고는 기본 하이퍼 매개변수 설정을 사용하여 100개 에포크 동안 모델을 훈련한다. 4.1. 런타임 효율성 평가 SAM은 Transformer 아키텍처를 사용하여 종단 간 알고리즘을 구성합니다. Transformer는 다양한 작업의 많은 형태의 매핑 함수를 표현할 수 있는 범용 아키텍처입니다. 무엇이든 분할하기 위해 SAM은 대규모 데이터에 대한 학습 프로세스를 통해 비전 지향 귀납적 편향을 학습합니다. 반대로 구조 설계에 대한 인간의 사전 지식을 통해 FastSAM은 비교적 컴팩트한 모델을 얻습니다. 그림 3에서 FastSAM은 비교적 만족스러운 결과를 생성합니다. 표 1에서 단일 NVIDIA GeForce RTX 3090 GPU에서 SAM과 FastSAM의 실행 속도를 보고합니다. FastSAM이 모든 프롬프트 번호에서 SAM을 능가하는 것을 볼 수 있습니다. 게다가 FastSAM의 실행 속도는 프롬프트에 따라 변하지 않으므로 Everything 모드에 더 나은 선택입니다. 4.2. Zero-Shot Edge Detection Approach. FastSAM은 BSDS500[1,28]을 사용하여 기본적인 저수준 작업인 엣지 감지에 대해 평가됩니다. 구체적으로, FastSAM의 모든 인스턴스 분할 단계의 결과에서 마스크 확률 맵을 선택합니다. 그 후, 모든 마스크 확률 맵에 Sobel 필터링[33]을 적용하여 에지 맵을 생성합니다. 마지막으로 에지 NMS[6] 단계로 마무리합니다. 결과. 대표적인 에지 맵은 그림 4에 나와 있습니다. 정성적 관찰을 통해 FastSAM의 매개변수가 상당히 적음에도 불구하고(68M에 불과함) 일반적으로 좋은 에지 맵을 생성한다는 것이 분명해집니다. 기준 진실과 비교했을 때, FastSAM과 SAM은 모두 BSDS500에 주석이 없는 일부 논리적 에지를 포함하여 더 많은 수의 에지를 예측하는 경향이 있습니다. 이 바이어스 방법 매개변수 SAM-H [20] 0.6G SAM-B [20] 136M 다른 지점 프롬프트 번호에서의 실행 속도(ms) E(16×16) E(32×32*) E(64x64) FastSAM(저희) 68M 표 1. 다른 지점 프롬프트 번호에서의 SAM 및 FastSAM의 실행 속도(ms/이미지). E: SAM의 모든 모드. *: 32×는 많은 작업에서 SAM의 기본 설정입니다¹. COUNTY NORTHUMBERLA SLAVERY ABOLITION ACT 방법 그림 3. FastSAM의 분할 결과 년도 ODS OIS AP RHED [37]....EDETR [30]....제로샷 전송 방법: Sobel 필터.Canny [6]...Felz-Hutt [9]...SAM [19]...FastSAM.....표 2. BSDS500에서 에지 감지로의 제로샷 전송. 다른 방법의 평가 데이터는 [20]에서 가져온 것입니다. 표 2에 정량적으로 반영되어 있습니다. 표 2는 SAM으로 유사한 성능을 달성했음을 보여주며, 특히 더 높은 R50과 더 낮은 AP입니다. 4.3. 제로샷 객체 제안 생성 배경. 객체 제안 생성은 일반 객체 감지, 퓨샷 객체 감지 및 이미지 이해를 포함한 많은 컴퓨터 비전 작업의 기본적인 전처리 단계였습니다. 많은 유명한 제안 생성 방법은 지난 수십 년 동안 시각적 인식 방법의 기본 단계의 역할로 시각적 인식의 진화를 목격합니다. 이러한 제안 생성 방법에는 EdgeBox[38], Geodesic[21], Selective Search¹https://github.com/facebookresearch/segment anything[34], MCG[2]가 포함됩니다. 최근 DeepMask[29], OLN[17]과 같은 많은 딥 러닝 기반 방법이 제안되었습니다. 예를 들어, RCNN 시리즈 객체 감지 방법[11, 12]은 Seletive Search 방법을 채택하고 최근 제안된 오픈 월드 감지기인 UniDetector[36]는 OLN 방법을 채택합니다. RPN[32]은 대부분의 기존 객체 감지기에서 사용되지만 학습된 범주의 객체 제안만 생성할 수 있어 오픈 어휘 인식 작업에 적용하는 데 제한이 있습니다. 따라서 제로 샷 객체 제안 생성이 매우 중요합니다. 이러한 시각적 인식 작업의 좋은 성능을 위해서는 좋은 제안 방법이 중요합니다. 우리는 FastSAM의 첫 번째 단계에서 생성된 경계 상자를 객체 제안으로 직접 사용합니다. 성능을 평가하기 위해 기존 평가 전략에 따라 LVIS[13] 및 COCO[25] 데이터 세트에서 테스트합니다. 이 외에도 SAM의 실험 설정에 따라 첫 번째 단계의 모든 인스턴스 마스크를 사용하여 마스크 제안 정확도도 테스트합니다. 세부 정보. LVIS 데이터 세트에서 SAM, ViTDet[23] 및 FastSAM의 결과를 보고합니다. SAM은 자세한 평가 코드를 공개하지 않으므로 공식 LVIS 평가 코드[13]를 사용하여 범주 독립적인 마스크 및 상자 리콜을 재생성했습니다. 그러나 SAM의 논문[20]에 제시된 ViTDet 및 SAM의 마스크 AR 결과를 재생성하지 못했습니다. 그럼에도 불구하고 평가 결과는 여전히 SAM과 비교하여 FastSAM의 여러 기능을 반영한다고 생각합니다. SAM 기준 진실 이미지 로로로로 FastSAM그림 4. BSDS500에서의 제로 샷 에지 예측. FastSAM은 SAM과 비슷한 결과를 얻습니다. 결과. 결과는 표 3, 4 및 5에 나와 있습니다. 결과에 따르면 우리 방법은 상자 제안 생성 작업에서 상당한 이점이 있습니다. 표 3은 COCO 검증 세트에서 다양한 방법의 평균 재현율(AR)을 나타냅니다. 이 중 EdgeBoxes[38], Geodesic[21], Sel.Search[34] 및 MCG[2]는 학습이 필요하지 않은 방법인 반면 DeepMask[29] 및 OLN[17]은 COCO 학습 세트 내의 VOC 범주에 대해 학습한 다음 모든 범주에 대해 테스트하는 지도 학습 방법입니다. 반면에 우리 방법과 SAM[20]은 완전한 제로 샷 전송을 구현합니다. 표에서 우리 방법과 SAM[20]은 OLN[17]과 같은 이전의 지도 학습 방법에 비해 AR@10 정밀도에서 성능이 좋지 않다는 것을 알 수 있습니다. 그러나 AR@ 1000에서 우리 방법은 OLN[17]보다 상당히 우수한 성능을 보입니다. 그 이유는 이전 방법이 COCO의 특정 범주에 대해 학습되었기 때문에 추론 중에 이러한 범주에 대한 신뢰 수준이 더 높았기 때문입니다.그러나 저희 방법과 SAM은 제로샷이기 때문에 다양한 범주에 걸쳐 균형 잡힌 신뢰 수준이 나타나 COCO에 없는 더 많은 범주를 회상합니다.그림 5에서 더 많은 비교를 볼 수 있습니다.표 4에서는 LVIS v1 데이터 세트에 대한 VitDet-H [23], SAM [20] 및 저희 방법의 bbox AR@1000 결과를 보고합니다.저희 방법은 SAM의 가장 계산 집약적인 모델인 SAM-H E64보다 5% 이상 상당히 뛰어납니다.그러나 LVIS 데이터 세트에서 학습한 VitDet-H [23]에 비하면 부족합니다.이러한 결과의 이유는 학습 프로세스 동안 기준 진실(gt) 경계 상자(bbox) 정보를 감독 신호로 사용했기 때문입니다. 반면 SAM[20]은 마스크를 감독 신호로만 사용했으며 추론 시 bbox는 마스크에서 바깥쪽 상자를 추출하여 생성했습니다. 표 5에서 볼 수 있듯이, 우리의 마스크 제안 생성은 Recall에서 비교적 낮습니다. 우리는 이것이 주로 작은 크기의 객체에 대한 우리의 분할 마스크가 fineMCG [2] AR10 AR100 AR1000 AUC EdgeBoxes [38] 7.17.33.13.Geodesic [21] 4.18.35.12.Sel.Search [34] 5.16.35.12.10.24.39.18.DeepMask [29] 13.28.43.21.OLN-Box [17] 27.46.55.34.SAM-H E15.45.67.32.SAM-H E18.49.62.33.SAM-B E11.39.59.27.FastSAM(우리의 것) 15.47.63.32.표 3. 모든 범주의 COCO에서 학습 없는 방법과의 비교. 우리는 학습 자유 방법, 딥 러닝 방법(VOC에서 훈련됨), 그리고 모든 일반화에 대한 우리의 방법 대 SAM의 평균 리콜(AR)과 AUC를 보고합니다. 경쟁 방법의 점수는 모든 COCO 클래스에서 객체 제안 방법을 테스트하는 [17]에서 가져왔습니다. §OLN FastSAM SAM-H E60-COCO ARMax_dets 그림 5. OLN [17] 및 SAM-H [20]와의 비교. 우리는 충분히 세분화된 모든 80개 COCO 클래스에서 객체 제안 방법을 테스트합니다. 섹션 6에서 더 자세히 논의합니다. bbox AR@method all small med. 큰 ViTDet-H [23] 65.53.83.91.제로샷 전송 방법: SAM-H E52.36.75.88.SAM-H E50.33.76.2 89.SAM-B E45.29.68.80.44.3 77.85.FastSAM(저희) 57.표 4. LVIS v1에서 객체 제안 생성. FastSAM 및 SAM은 제로샷에 적용됩니다. 즉, 객체 제안 생성을 위해 훈련되지 않았으며 LVIS 이미지나 주석에 액세스하지 않았습니다. 방법 SAM 논문에 보고된 모든 작은 결과: 마스크 AR@med. 큰 주파수. com. 희귀 ViTDet-H [23] 63.0 51.80.SAM [20] 단일 아웃. 54.9 42.76.7 74.SAM[20] 59.3 45.5 81.6 86.87.0 63.1 63.3 58.54.7 59.8 62.59.63.9 65. 복제 후 결과: ViTDet-H[23] 59.9 48.3 78.1 84.SAM-H E54.39.6 77.9 83.SAM-H E51.35.2 78.7 85.SAM-B EFastSAM(저희) 45.8 31.1 70.5 73.49.7 35.6 72.7 77.표 5. LVIS v1에서 객체 제안 생성. FastSAM과 SAM은 제로 샷으로 적용됩니다.즉, 객체 제안 생성을 위해 훈련되지 않았고 LVIS 이미지나 주석에 액세스하지 않았습니다.이미지 텍스트 프롬프트: &quot;노란색 개&quot; 텍스트 프롬프트: &quot;검은색 개&quot; 그림 6. 텍스트 프롬프트를 사용한 분할 결과 4.4. 제로 샷 인스턴스 분할 접근 방식.SAM 방법과 유사하게 ViTDet[23]에서 생성한 바운딩 박스(bbox)를 프롬프트로 사용하여 인스턴스 분할 작업을 수행합니다.3.3절에서 설명한 대로 예측 마스크로 bbox를 사용하여 가장 높은 IoU(Intersection over Union)를 갖는 마스크를 선택합니다.결과.표 6에 평가 결과가 나와 있습니다.이 작업에서는 높은 AP를 달성하지 못했습니다.이는 주로 분할 마스크 정확도 또는 상자 기반 마스크 선택 전략 때문이라고 추론합니다.6절에 몇 가지 예가 나와 있습니다.4.5. 텍스트 프롬프트 접근 방식을 사용한 제로 샷 객체 현지화. 마지막으로, 자유형 텍스트에 의한 객체 분할이라는 훨씬 더 높은 수준의 작업을 고려합니다.이 실험은 SAM과 같이 텍스트 프롬프트를 처리하는 FastSAM의 능력을 보여주기 위한 것입니다.SAM과 달리 FastSAM은 AP COCO [26] LVIS v1 [13] APS APM APL 방법 APS APM APL AP ViTDet-H [23] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.제로 샷 전송 방법(분할 모듈만 해당): SAM FastSAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.37.9 23.9 43.4 50.0 34.5 24.6 46.2 50을 수정할 필요가 없습니다.표 6. 인스턴스 분할 결과. Fastsam은 ViTDet 상자로 프롬프트되어 제로 샷 분할을 수행합니다. 완전 감독 ViTDet은 SAM보다 성능이 뛰어나지만, 더 높은 품질의 LVIS 마스크에서는 격차가 줄어듭니다. 훈련 절차. CLIP의 텍스트 인코더를 통해 텍스트를 직접 실행한 다음 결과 텍스트 임베딩을 사용하여 추론 시간에 가장 유사한 마스크를 찾습니다. 결과. 그림 6에서 정성적 결과를 보여줍니다. FastSAM은 텍스트 프롬프트에 따라 객체를 잘 분할할 수 있습니다. 그럼에도 불구하고 텍스트-마스크 분할의 실행 속도는 만족스럽지 않습니다. 각 마스크 영역을 CLIP 기능 추출기에 입력해야 하기 때문입니다. CLIP 임베딩 추출기를 FastSAM의 백본 네트워크에 결합하는 방법은 모델 압축과 관련하여 흥미로운 문제로 남아 있습니다. 5. 실제 응용 프로그램 이 섹션에서는 다양한 응용 프로그램 시나리오에서 FastSAM의 성능을 평가하고 장점과 한계를 분석합니다. FastSAM의 분할을 point-prompt, box-prompt 및 everything 모드를 사용하여 시각화하여 보여주고 SAM 및 ground truths와 비교합니다.이상 감지. [3]에서 자세히 설명한 대로 이상 감지는 제조 시 불량 샘플과 정상 샘플을 구별하는 작업입니다.FastSAM은 MVTec AD 데이터 세트[3]를 사용하여 평가되며 결과는 그림 7에 표시됩니다.모든 모드에서 FastSAM은 SAM과 유사한 거의 모든 영역을 분할할 수 있지만 SAM에 비해 정밀도 수준이 낮습니다.또한 배경 마스크가 전체 배경을 완전히 덮지 않는데, 이는 YOLACT[4]의 고유한 특성입니다.전경/배경 포인트(FastSAM-point에서 각각 노란색 및 자홍색 포인트) 또는 상자 안내 선택을 통해 FastSAM은 정확한 불량 영역을 분할할 수 있습니다.눈에 띄는 객체 분할.눈에 띄는 객체 분할[5]의 목적은 이미지에서 가장 주의를 끄는 객체를 분할하는 것입니다. 이 작업은 클래스와 무관하여 의미적 분할과 구별됩니다.잘 알려진 뛰어난 데이터 세트인 ReDWeb-S[27]에 FastSAM을 적용합니다.그림 8에 표시된 것처럼 FastSAM은 작업과 관련 없는 배경 개체를 덜 분할하기 때문에 모든 모드에서 SAM과 사소한 차이만 보였습니다.포인트 기반 선택을 통해 이러한Cactavis Cactavis Cactavis actavis 원본 이미지 SAM-포인트 SAM-박스 SAM-모든 것CactavisCactavisCactavis Pground truth FastSAM-포인트 FastSAM-박스 FastSAM-모든 것 그림 7. SAM-포인트/박스/모든 것은 각각 포인트-프롬프트, 박스-프롬프트 및 모든 것 모드를 사용하는 것을 의미하는 이상 감지에 대한 응용 프로그램입니다. 영어: original image SAM-point SAM-box SAM-everything ground truth FastSAM-point FastSAM-box FastSAM-everything 그림 8. SAM-point/box/everything은 각각 point-prompt, box-prompt 및 everything 모드를 사용하는 것을 의미하는 두드러진 객체 분할에 대한 응용 프로그램입니다. FastSAM-point에서 노란색 점으로 모든 관심 객체의 마스크를 얻을 수 있습니다. FastSAMpoint의 분할 결과는 SAM-point 및 ground truth의 분할 결과와 거의 동일하며 가장자리에서 사소한 세부 정보만 손실되었습니다. 관심 객체는 FastSAM-box의 녹색 상자와 같이 상자 프롬프트로도 선택할 수 있습니다. 그러나 SAM-box조차도 실현할 수 없는 단일 상자로 여러 객체를 선택하는 것은 불가능합니다. 건물 추출. 광학 원격 감지 영상에서 건물을 추출하는 것은 도시 계획과 같은 광범위한 응용 프로그램이 있습니다. [14]에서 제안한 데이터 세트에서 FastSAM을 평가합니다. 그림 9에서 보듯이 FastSAM은 규칙적인 모양의 객체를 분할하는 데 좋은 성과를 보이지만 SAM에 비해 그림자와 관련된 영역을 덜 분할합니다. FastSAM-point 및 FastSAM-box에서 제시된 것처럼 point-prompt 및 boxprompt를 사용하여 관심 영역을 선택할 수도 있습니다. FastSAM-point에서 그림자 영역에 점을 배치한다는 점에 유의해야 합니다. 그러나 이 점을 기준으로 병합하여 건물에 대한 올바른 마스크를 여전히 얻을 수 있습니다. 이는 우리 방법이 어느 정도 노이즈 간섭에 저항할 수 있음을 나타냅니다. 원본 이미지 SAM-point SAM-box SAM-everything 지상 진실 FastSAM-point FastSAM-box FastSAM-everything 그림 9. 건물 추출에 대한 응용 프로그램, 여기서 SAM-point/box/everything은 각각 point-prompt, box-prompt 및 everything 모드를 사용하는 것을 의미합니다. image FastSAM SAM 6. 토론 그림 10. FastSAM은 대형 물체의 좁은 영역에서 더 미세한 분할 마스크를 생성합니다. 일반적으로 제안된 FastSAM은 SAM과 비슷한 성능을 달성하며 SAM(32×32)보다 50배, SAM(64×64)보다 170배 더 빠르게 실행됩니다. 실행 속도 덕분에 도로 장애물 감지, 비디오 인스턴스 추적, 이미지 조작과 같은 산업용 애플리케이션에 적합합니다. 일부 이미지에서는 FastSAM이 그림 10에서 보듯이 대형 물체에 대해 더 나은 마스크를 생성하기도 합니다. 약점. 그러나 실험에서 제시된 것처럼 상자 생성에는 상당한 이점이 있지만 마스크 생성 성능은 SAM보다 낮습니다. 이러한 예를 그림 11에 시각화했습니다. FastSAM에는 다음과 같은 특징이 있습니다. • 품질이 낮은 소형 분할 마스크는 신뢰도 점수가 큽니다. 신뢰도 점수가 마스크 품질과 크게 관련이 없는 YOLOV8의 bbox 점수로 정의되기 때문이라고 생각합니다. 네트워크를 수정하여 마스크 IoU 또는 기타 품질 지표를 예측하는 것은 이를 개선하는 방법입니다. • 일부 작은 크기의 개체의 마스크는 정사각형에 가까운 경향이 있습니다. 게다가 큰 개체의 마스크는 경계 상자의 테두리에 일부 아티팩트가 있을 수 있습니다. 이것이 YOLACT 방법의 약점입니다. 마스크 프로토타입의 용량을 향상시키거나 마스크 생성기를 재구성하면 문제가 해결될 것으로 예상됩니다. 게다가 모든 SA-1B 데이터 세트의 1/50만 사용하므로 더 많은 학습 데이터를 활용하면 모델의 성능을 더욱 향상시킬 수도 있습니다. 7.
--- CONCLUSION ---
이 논문에서 우리는 모든 것의 세그먼트 작업과 모델 아키텍처 선택을 재고하고 OCDE DODD보다 50배 더 빠른 실행 속도를 가진 대체 솔루션을 제안합니다(그림 11). FastSAM의 나쁜 경우에 대한 몇 가지 예. SAM-VIT-H(32×32). 실험 결과 FastSAM은 여러 다운스트림 작업을 잘 해결할 수 있음을 보여줍니다. 그래도 FastSAM의 경우 스코어링 메커니즘과 인스턴스 마스크 생성 패러다임과 같이 개선할 수 있는 몇 가지 약점이 있습니다. 이러한 문제는 향후 연구 과제로 남겨둡니다. 참고문헌 [1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, Jitendra Malik. 윤곽선 감지 및 계층적 이미지 분할. IEEE 패턴 분석 및 머신 인텔리전스 거래, 33(5):898-916, 2010. 1,[2] Pablo Arbeláez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, Jitendra Malik. 다중 규모 조합 그룹화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 328-335페이지, 2014. 5,[3] Paul Bergmann, Michael Fauser, David Sattlegger 및 Carsten Steger. Mvtec ad-비지도 이상 탐지를 위한 포괄적인 실제 세계 데이터 세트. CVPR, 9592-9600페이지, 2019.[4] Daniel Bolya, Chong Zhou, Fanyi Xiao 및 Yong Jae Lee. Yolact: 실시간 인스턴스 분할. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 9157-9166페이지, 2019. 2, 3,[5] Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang 및 Jia Li. 눈에 띄는 객체 탐지: 조사. Computational Visual Media, 5:117–150, 2019.[6] John Canny. 에지 감지에 대한 계산적 접근 방식. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션, (6):679–698, 1986. 4,[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. 변압기를 사용한 종단 간 객체 감지. Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, 2020년 8월 23-28일, Proceedings, Part I 16, 213-229페이지. Springer, 2020.[8] Bowen Cheng, Alex Schwing, Alexander Kirillov. 픽셀당 분류는 의미적 분할에 필요한 전부가 아닙니다. 신경 정보 처리 시스템의 발전, 34:17864-17875, 2021.[9] Pedro F Felzenszwalb 및 Daniel P Huttenlocher. 효율적인 그래프 기반 이미지 분할. 국제 컴퓨터 비전 저널, 59:167–181, 2004.[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li 및 Jian Sun. Yolox: 2021년 yolo 시리즈 초과. arXiv 사전 인쇄본 arXiv:2107.08430, 2021.[11] Ross Girshick. Fast r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 1440–1448페이지, 2015.[12] Ross Girshick, Jeff Donahue, Trevor Darrell 및 Jitendra Malik. 정확한 객체 감지 및 분할을 위한 지역 기반 합성곱 네트워크. IEEE 패턴 분석 및 머신 인텔리전스 거래, 38(1):142–158, 2015.[13] Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5356-5364페이지, 2019. 2, 5,[14] Shunping Ji, Shiqing Wei, Meng Lu. 개방형 항공 및 위성 이미지 데이터 세트에서 다중 소스 건물 추출을 위한 완전 합성곱 네트워크. IEEE 지구 과학 및 원격 감지 거래, 57(1):574–586, 2018.[15] Glenn Jocher. Yolov5 by ultralytics, 2020. https://github.com/ultralytics/yolov5.[16] Glenn Jocher, Ayush Chaurasia, Jing Qiu. ultralytics의 Yolo, 2023. https://github.com/ultralytics/ ultralytics. 2, 3,[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, Weicheng Kuo. 분류를 배우지 않고 오픈 월드 객체 제안 학습. IEEE Robotics and Automation Letters, 7(2):5453-5460, 2022. 5,[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413페이지, 2019.[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo 등. 무엇이든 분할하세요. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. 1, 2, 4,[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo 등. 무엇이든 분할하세요. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. 5, 6,[21] Philipp Krähenbühl 및 Vladlen Koltun. 지오데식 객체 제안. Computer Vision-ECCV 2014: 제13차 유럽 회의, 스위스 취리히, 2014년 9월 6~12일, 절차, 파트 V 13, 725~739페이지. Springer, 2014. 5,[22] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu 및 Xiangxiang Chu. Yolov6 v3.0: 본격적인 재로딩, 2023년.[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang 및 Li Cheng. 약한 감독을 받는 시간적 동작 위치 파악을 위해 잡음이 제거된 교차 비디오 대비를 탐색합니다. CVPR, 19914-19924, 2022. 5, 6,[24] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie. 객체 감지를 위한 피처 피라미드 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2117-2125페이지, 2017.[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트 내 공통 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014. 1,[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트의 공통 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014.[27] Nian Liu, Ni Zhang, Ling Shao, Junwei Han. rgb-d saliency 감지를 위한 선택적 상호 주의 및 대비 학습. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(12):9026-9042, 2021.[28] David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik. 인간이 분할한 자연 이미지의 데이터베이스와 분할 알고리즘 평가 및 생태 통계 측정에 대한 응용. IEEE 제8회 국제 컴퓨터 비전 컨퍼런스 회의록. ICCV 2001, 2권, 416-423페이지. IEEE, 2001. 1,[29] Pedro OO Pinheiro, Ronan Collobert, Piotr Dollár. 객체 후보 분할 학습. 신경 정보 처리 시스템의 발전, 28, 2015. 5,[30] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, Haibin Ling. Edter: 변압기를 사용한 에지 감지. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1402-1412페이지, 2022.[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763쪽. PMLR, 2021. 3,[32] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 더 빠른 r-cnn: 영역 제안 네트워크를 사용한 실시간 객체 감지를 향해. 신경 정보 처리 시스템의 발전, 28, 2015.[33] Irwin Sobel, Gary Feldman, et al. 이미지 처리를 위한 3x3 등방성 기울기 연산자. Stanford Artificial Project에서 한 강연, 271-272쪽, 1968.[34] Jasper RR_ Uijlings, Koen EA Van De Sande, Theo Gevers, Arnold WM Smeulders. 객체 인식을 위한 선택적 검색. 국제 컴퓨터 비전 저널, 104:154-171, 2013. 5,[35] Chien-Yao Wang, Alexey Bochkovskiy, HongYuan Mark Liao. YOLOV7: 학습 가능한 무료 가방은 실시간 객체 감지기를 위한 새로운 최첨단 기술을 설정합니다. arXiv 사전 인쇄본 arXiv:2207.02696, 2022.[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, Shengjin Wang. 열린 세상에서 모든 것을 감지: 보편적인 객체 감지를 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 11433-11443페이지, 2023.[37] Saining Xie 및 Zhuowen Tu. 전체론적 중첩 에지 감지. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 1395-1403페이지, 2015.[38] C Lawrence Zitnick 및 Piotr Dollár. 에지 상자: 에지에서 객체 제안 찾기. Computer VisionECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 391-405페이지. Springer, 2014. 5,
