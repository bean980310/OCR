--- ABSTRACT ---
이 논문에서 우리는 대규모 언어 모델(LLM)의 추론 능력에 대한 철저한 조사를 수행하며, 특히 이러한 모델을 대표하는 Open Pretrained Transformers(OPT) 모델에 초점을 맞춥니다. 우리의 연구는 신중하게 큐레이팅된 추론 코퍼스에서 세 가지 다른 크기의 OPT를 미세 조정하여 설명 없이 미세 조정된 OPT-R과 설명이 있는 미세 조정된 OPT-RE의 두 가지 미세 조정 모델 세트를 생성합니다. 그런 다음 세 가지 프롬프트 기술을 활용하여 26가지 고유한 추론 기술을 포괄하는 SUPERNATURALINSTRUCTIONS 벤치마크에서 추출한 도메인 외부 작업에서 모든 모델을 평가합니다. 27가지 구성과 6,156개의 테스트 평가로 구성된 포괄적인 그리드를 통해 미세 조정, 프롬프트 및 규모의 차원을 조사하여 다양한 추론 기술에 대한 설명의 역할을 이해합니다. 우리의 연구 결과에 따르면 fewshot exemplar에 설명이 있는 경우 모델이 미세 조정될 때 모델의 성능에 큰 영향을 미치지 않지만 미세 조정되지 않은 대응 제품에는 긍정적인 영향을 미칩니다. 게다가 우리는 각각 프롬프팅과 미세조정 중에 설명을 통합함에 따라 분류 정확도가 약간이지만 지속적으로 증가하는 것을 관찰합니다. 마지막으로, 우리는 수치적(+20.4%) 및 유추적(+13.9%) 추론과 같이 미세조정 및 프롬프팅 중에 설명을 통합함으로써 어떤 기술이 가장 많은 이점을 얻는지에 대한 통찰력을 제공하며, 무시할 수 있거나 부정적인 효과를 보이는 기술도 제공합니다. 1
--- INTRODUCTION ---
최근 산업 및 학술 기관에서 대규모 언어 모델(LLM)을 출시하는 경우가 급증하고 있습니다. 이러한 모델은 OPT(Zhang 등, 2022) 및 LLAMA(Touvron 등, 2023)와 같은 오픈 소스 릴리스에서 GPT-3(Brown 등, 2020) 및 PALM(Chowdhery 등, 2022)과 같은 폐쇄형 소스 릴리스까지 다양합니다. 또한 연구자들은 이러한 기본 모델 위에 미세 조정된 모델을 개발하여 13B 6.7B 규모 1.3B OPT OPT-R OPT-RE Zeroshot Fewshot Fewshot-E 프롬프팅 미세 조정 그림 1: 미세 조정, 프롬프팅 및 규모의 3차원 그리드. 각 차원은 축으로 표시되며 각 축에는 미세 조정, 프롬프팅 및 규모의 세 가지 수준이 표시됩니다. 결과 그리드는 다양한 추론 과제에서 평가된 27가지 다른 조합으로 구성됩니다. 4가지 구성 요소로 구성된 숨겨진 차원인 채점 기능이 있다는 점에 유의해야 합니다. 그 결과 총 6,156개의 평가가 종합적으로 이루어집니다. OPT-IML(Iyer et al., 2022) 및 Alpaca(Taori et al., 2023)와 같은 지침을 따릅니다. 자연어 처리(NLP) 과제에서 LLM의 성과가 현저히 향상되었음에도 불구하고 추론은 여전히 어려운 분야입니다. 예를 들어, 이전 연구에 따르면 LLM은 상식적 추론(West et al., 2022) 및 산술적 추론(Hendrycks et al., 2021)에 어려움을 겪는 것으로 나타났습니다. 최근의 노력에서는 맥락 내 학습(Wei 등, 2022b; Kojima 등, 2022)이나 미세 조정(Chung 등, 2022; Wei 등, 2021a)을 사용하여 답변을 단계별 추론 체인으로 분해하여 LLM의 추론 성능을 개선하려고 시도했습니다. 이러한 접근 방식은 GSM8K(Cobbe 등, 2021)와 같은 벤치마크에서 어느 정도 개선을 보였지만 이러한 설명이 미세 조정, 프롬프트 또는 {작업 정의} 답변과 간단한 추론을 함께 제공하세요. {맥락 내 예} 입력: {입력} 옵션: {옵션} 출력: 답변은 {답변}입니다. {설명} 그림 2: 학습과 추론 모두에 사용되는 템플릿입니다. 모델은 답변을 예측한 다음 설명을 예측해야 합니다. 바닐라 OPT 모델과 비교했을 때(표 2 참조). 그러나 우리는 또한 이 접근 방식이 다른 세 가지 추론 기술의 성능을 상당히 방해한다는 것을 발견했습니다(표 3 참조). 또한 fewshot 프롬프팅 중에 설명을 통합하는 것의 영향을 조사하고 각 모델의 추론 기술에 걸쳐 두 프롬프팅 방법 간 차이의 분산으로 측정한 결과 미세 조정된 모델의 성능에 상당한 영향을 미치지 않는다는 것을 발견했습니다. 그러나 표 5에서 볼 수 있듯이 vanilla OPT 모델의 성능에 더 눈에 띄는 효과가 있음을 알았습니다. 또한 Fewshot에서 Fewshot-E까지, 그리고 OPT에서 OPT-R, OPT-RE 모델까지 모든 작업에서 평균 성능이 지속적으로 증가하는 것을 관찰하여 설명이 미세 조정과 프롬프팅 모두에서 성능에 미치는 영향이 작음을 나타냅니다. 마지막으로 표 4는 결과 요약을 제시하여 미세 조정 또는 프롬프팅 중에 설명을 통합하여 어떤 추론 기술이 향상되었는지, 어떤 기술이 부정적인 영향을 미치는지, 어떤 기술이 설명과 관련하여 무시할 만한 영향을 미치는지 나타냅니다. 이들의 조합. 동시 작업에서는 미세 조정 중에 마주치는 것 이상의 추론 기술에 대한 이러한 모델의 일반화 기능을 조사했지만(Yu et al., 2022), 추론 기술과 관련하여 미세 조정 및 프롬프트 중 설명의 역할에 대한 포괄적인 평가는 여전히 부족합니다.2 OPT-R: 추론 기술에 대한 미세 조정 설명이 있는 추론 데이터 세트 2. 이 논문에서는 이러한 격차를 해소하는 것을 목표로 합니다. 이러한 모델을 대표하는 OPT(Zhang et al., 2022)를 조사하고 이를 기본 모델로 활용합니다. 각 인스턴스에 대한 설명이 포함된 신중하게 큐레이팅된 오픈 소스 추론 데이터 세트 컬렉션에서 OPT를 미세 조정하여 SUPER-NATURALINSTRUCTIONS 벤치마크(Wang et al., 2022)에서 가져온 57개 작업에 대한 성능을 평가하여 26가지 추론 기술을 다룹니다. 실험은 미세 조정, 프롬프트, 규모의 세 가지 핵심 차원을 중심으로 구성되며 각각은 세 가지 고유한 구성 요소로 구성됩니다(그림 1 참조). 미세 조정: (1) 미세 조정되지 않은 (바닐라) OPT 모델; (2) 설명이 없는 미세 조정된 OPT 모델(OPT-R); 그리고 (3) 설명이 있는 미세 조정된 OPT 모델(OPT-RE). 프롬프팅: (1) 제로 샷 프롬프팅; (2) 설명이 없는 퓨샷 프롬프팅; 그리고 (3) 설명이 있는 퓨샷 프롬프팅. 마지막으로, 스케일: (1) 1.3B; (2) 6.7B; 그리고 (3) 13B. 따라서 우리는 27개의 서로 다른 구성 요소로 구성된 그리드를 생성하여 다양한 모델 스케일에서 미세 조정 및 추론 중 설명의 영향을 측정하는 자세한 분석을 제공합니다. 샘플 수 연구 결과에 따르면 추론 데이터 세트에 대한 미세 조정은 수치적, 유추적 및 객체 추론을 포함한 7가지 추론 기술에서 통계적으로 유의미한 개선으로 이어지며, 물리적, 계산 및 텍스트적 함의는 OPT-RE 모델에서만 유의미한 효과를 보였으며, 이는 fewshot 프롬프트 조건 모두에서 나타났습니다.AQUA-RAT COQA COS-E ECQA ESNLI GSM8K ProofWriter StrategyQA 데이터 세트 그림 3: 학습 코퍼스의 각 데이터 세트에 있는 샘플 수. Y축은 로그 스케일입니다. OPT를 정제하는 데 사용된 미세 조정 코퍼스는 다양한 추론 데이터 세트로 구성되며, 각각에는 답변에 대한 해당 설명 또는 근거가 포함됩니다. 이러한 근거는 일련의 작은 단계(예: 사고의 사슬) 또는 답변의 추론을 설명하는 자유형 텍스트로 구성될 수 있습니다. 그림 2에서 볼 수 있듯이 학습 프로세스 동안 모든 작업에 대해 균일한 템플릿을 사용합니다. 모델에 대한 입력은 작업 정의로 시작하여, 그 다음에 답을 제공하라는 지시와 간단한 추론이 이어집니다. 다음으로, 각 인스턴스에 대한 학습 내내 일정하게 유지되는 두 개의 무작위 인컨텍스트 예제를 학습 세트에서 균일하게 추출합니다. 그런 다음 현재 학습 인스턴스에 대한 입력이 각 작업에 고유한 형식으로 제공됩니다. 그런 다음 답에 대한 옵션이 입력에 포함되지만 인컨텍스트 예제에는 포함되지 않습니다(작업별 정의 및 옵션에 대한 자세한 내용은 부록 A 참조). 옵션은 각 학습 인스턴스에 대해 미리 섞입니다. 마지막으로 모델에 &quot;출력: 답은&quot;이라는 답 접두사가 제공되고 답을 예측하는 작업이 수행되고, 그 다음에 OPT-RE가 미세 조정되고 있는지에 대한 설명이 이어집니다. 마찬가지로, 인컨텍스트 예제는 OPT-RE를 학습할 때만 설명을 포함합니다. 아래는 미세 조정 중에 사용된 각 데이터 세트에 대한 간략한 설명입니다. 각 데이터 세트의 상대적 크기는 그림 3을 참조하세요. AQUA-RAT 대수 문제 해결을 위한 합리적 근거 데이터 세트(Ling et al., 2017)는 문제를 일련의 작은 단계로 나누어 대수 단어 문제를 푸는 작업을 보다 실현 가능하게 만듭니다. 그들은 자연어로 된 질문, 답변 및 합리적 근거와 최종 답을 도출하는 데 사용할 수 있는 사람이 읽을 수 있는 수학적 표현식을 포함하는 100k 샘플 데이터 세트를 만듭니다. CoQA 대화형 질문 답변 데이터 세트 Reddy et al. (2019). 7개 다른 도메인의 구절에 대한 8k 대화에서 수집한 127k 질문과 답변으로 구성되어 있습니다. 대화가 포함된 구절이 주어지면 모델은 구절에서 해당 증거를 강조 표시하여 질문에 답하는 작업을 맡습니다. CoS-E 상식적 추론을 사용하여 언어 모델을 유도하는 상식적 설명 데이터 세트 Rajani et al. (2019). 이 데이터 세트에서 모델은 질문과 선택지 집합이 주어지고 제공된 선택지 중 하나를 선택하고 그 선택지가 옳은 이유를 자연어로 설명하는 작업을 맡습니다.ECQA 상식적 질문 답변에 대한 설명 데이터 세트 Aggarwal et al. (2021). 모델이 제공된 옵션 중 하나를 선택하여 주어진 질문에 답하고 설명도 제공해야 하기 때문에 CoS-E와 유사합니다.ESNLI 설명이 포함된 Stanford 자연어 추론 데이터 세트 Camburu et al. (2018)은 모델을 훈련하여 결정에 대한 해석 가능하고 견고한 설명을 제공합니다.저자는 SNLI 데이터 세트(Bowman et al., 2015)를 인간이 주석을 단 설명으로 확장합니다.모든 NLI 작업과 마찬가지로 모델에 전제와 가설이 주어지고 작업은 가설 문장이 주어진 전제와 관련하여 수반되는지, 모순되는지 또는 중립적인지 여부를 판별하는 것입니다.GSM8K 초등학교 수학 데이터 세트 Cobbe et al. (2021) 다단계 수학적 추론을 더 잘 수행하기 위한 모델을 훈련합니다. 8.5k개의 언어적으로 다양한 초등학교 수학 단어 문제로 구성되어 있습니다. 따라서 모델의 과제는 최종 답을 얻기 위해 일련의 산술 연산을 수행하여 질문에 답하는 동시에 추론 단계를 설명하는 것입니다. ProofWriter ProofWriter 데이터 세트 Tafjord et al. (2021)은 RuleTaker 데이터 세트(Clark et al., 2020)의 이론의 의미와 이를 뒷받침하는 자연어 증명을 모두 생성합니다. 구체적으로, 일련의 사실과 규칙이 주어지면 모델은 &quot;예&quot;, &quot;아니요&quot; 또는 &quot;알 수 없음&quot;을 사용하여 질문에 답하고 제공된 사실과 규칙을 참조하여 추론 경로를 제공해야 합니다. 우리는 추론 깊이가 최대 5인 질문이 있는 Rule Taker의 오픈 월드 가정 하위 집합을 고려합니다. StrategyQA Strategy Question Answering 데이터 세트 Geva et al. (2021)은 필요한 추론 단계가 질문에 내재되어 있는 질문에 대한 멀티홉 추론을 개선합니다. 따라서 모델의 과제는 &quot;예&quot; 또는 &quot;아니요&quot;를 사용하여 질문에 답한 다음 여러 단계로 분해하여 답변을 설명하는 전략을 제공하는 것입니다. 2.2 미세 조정 절차 OPT Open Pretrained Transformers(OPT) 모델은 Zhang et al. (2022)이 발표한 125M~175B 매개변수 범위의 디코더 전용 사전 학습된 변환기 모음입니다. 이 작업에서 우리는 크기가 1.3B인 세 개의 OPT 모델을 사용합니다. 6.7B 및 13B. 각 모델 아키텍처, 사전 학습 코퍼스 및 학습 구성(예: 가중치 초기화, 최적화, 토크나이저, 하이퍼파라미터 등)의 세부 사항은 Zhang et al.(2022)에서 찾을 수 있습니다. 추론 기술 귀납적 추론 유추적 추론 논증 추론 인과 추론 상식적 추론 상식적 추론 → 수치적 상식 ... 상식적 추론 → 물리적 추론 상식적 추론 → 사회적 상황 상식적 추론 → 공간적 추론 윤리 과제 ID tasktask 1287, tasktasktask279, task156, tasktasktask580, task937, tasktask082, tasktask221, task1568, tasktask667, task724, tasktask1712, task052, tasktask717, task211, task연역적 추론 문법적 추론 논리적 추론 논리적 추론 → 기호 추론 task923, task수학 → 계산 task523, task멀티홉 추론 task1297, task수치적 추론 task621, taskReasoning on Objects task1583, taskReasoning on Social Interactions task609, task881, taskReasoning on Strings 관계적 추론 과학적 추론 시간적 추론 텍스트적 함의 텍스트적 함의 → 유추적 추론 텍스트적 함의 → 연역적 추론 tasktask1380, task472, tasktask1431, task228, tasktask018, task1549, tasktask738, task890, tasktasktask1612, task534, task표 1: 각 추론 기술에 사용된 SUP-NATINST(Wang et al., 2022)의 평가 과제.구현 세부 정보선택된 모델을 미세 조정하기 위해 다른 코드베이스(Zhang et al., 2022)에 비해 더 높은 학습 효율성을 제공하는 metaseq¹ 구현을 활용했습니다. 각 모델은 10개의 에포크에 대해 두 번 미세 조정되며, 한 번은 설명이 있고 한 번은 설명이 없습니다(즉, 각각 OPT-RE 대 OPT-R). 모델은 각 에포크가 끝날 때 선택한 SUPERNATURALINSTRUCTIONS 검증 작업 세트에 대해 평가되고, 가장 좋은 성능을 보이는 체크포인트가 테스트 작업에서 평가를 위해 선택됩니다. 손실은 추론 중에 모델이 예측하도록 작업된 토큰에 대해서만 계산되며, 전체 입력에 대해서는 계산되지 않습니다. 이를 (Iyer et al., 2022)에서 레이블 손실이라고 합니다. 모든 데이터 세트의 샘플은 학습 중에 섞입니다. 또한 추론 시간 후속과 일치하도록 작업 정의 외에도 미세 조정 중에 모델에 두 개의 컨텍스트 내 예가 제공됩니다(Wang et al., 2022). &#39;https://github.com/facebookresearch/metaseq 3 모델 평가 3.1 SUPER-NATURALINSTRUCTIONS 작업 이 연구에서는 Wang et al.(2022)이 제안한 SUPERNATURALINSTRUCTIONS 벤치마크 버전 2(약칭 SUP-NATINST)의 하위 집합에 초점을 맞춥니다. 여기에는 1,616개의 다양한 NLP 작업이 포함되며 작업 유형, 도메인과 이 작업에서 더 중요한 기본 추론 기술과 같은 각 작업에 대한 메타 레이블이 포함됩니다. 구체적으로 두 가지 핵심 기준을 충족하는 작업 하위 집합을 선택합니다. (i) 작업은 단일 추론 기술에 초점을 맞추어 특정 원자 기술을 평가할 수 있고 (ii) 작업은 섹션 3.2에서 자세히 설명한 대로 분류 모드를 사용하여 테스트할 수 있습니다. 미세 조정 데이터와 평가 벤치마크 사이에 데이터 오염이 없음에 유의하세요. 2 https://github.com/allenai/natural-instructions/tree/v2.6에서 데이터를 다운로드했습니다. 정확도(%) Fewshot-E Fewshot Zeroshot1.3B 6.7B 13B 1.3B 6.7B 13B 1.3B 6.7B 13B 크기 크기 크기 모델 OPT OPT-R OPT-RE 그림 4: 이 연구에서 분석한 세 가지 주요 차원인 미세 조정, 촉구 및 척도의 함수로서 모든 작업에서 달성한 결과.벤치마크 분할 작업 선택 프로세스에 따라 테스트 세트 내의 다양성을 보장하기 위해 무작위 표집 기법을 적용합니다.특히 각 추론 기술에서 최대 3개의 작업을 선택하고 나머지 작업을 검증 세트에 할당합니다.특히, 이 접근 방식을 사용하면 테스트를 위해 선택된 추론 기술의 대표 샘플을 얻을 수 있는 동시에 모델의 성능이 특정 작업 하위 집합의 영향을 받지 않도록 할 수 있습니다.표 1은 각 추론 기술에 대한 미세 조정된 모델을 평가하는 데 사용된 작업의 전체 목록을 보여줍니다. 3.2 평가 설정 앞서, 우리는 미세 조정된 모델을 평가하기 위해 SUP-NATINST에서 26가지 추론 기술에 걸친 57개 과제를 선택했다고 언급했습니다. 섹션 3.1에 자세히 설명된 대로 기준을 충족하기 위해 각 과제는 두 가지 조건을 충족해야 했습니다. 두 번째 조건은 과제가 분류 과제로 간주될 수 있어야 한다는 것입니다. 즉, 이산적인 후보 집합(그 중 하나가 정답)이 있고, 따라서 가장 높은 점수를 받은 후보가 답으로 간주되는 분류 문제로 취급한다는 것을 의미합니다. 이를 보장하기 위해 우리는 간단한 휴리스틱을 활용했습니다. 가능한 후보 답이 10개를 넘지 않는 과제만 샘플링했습니다. 분류 방법 정답을 결정하기 위해 우리는 각 잠재적 후보 답에 대해 포워드 패스를 수행하고 Brown et al. (2020)과 유사하게 후보 토큰이 입력을 따를 가능성을 측정하는 채점 함수를 사용합니다. 이 프로세스는 후속 단락에서 자세히 설명한 대로 별도의 채점 함수를 사용하여 4번 반복됩니다. 4가지 채점 함수에서 가장 높은 정확도 점수는 과제의 결과로 간주됩니다. 점수 함수 이는 각 작업을 네 가지 다른 점수 함수를 사용하여 평가하고 그 결과로 최대 정확도를 취하기 때문에 이 작업의 네 번째 차원으로 간주됩니다. 사용된 네 가지 점수 함수는 다음과 같습니다. (1) 평균, 후보 토큰의 로그 확률의 평균을 계산하는 것을 포함하며 토큰 점수라고도 합니다. (2) 무조건적 노름, 이전 토큰에 의해 조건화되지 않았을 때 후보의 토큰 점수 합계와 이전 입력에 의해 조건화되었을 때 후보 토큰 점수 합계의 차이를 계산합니다. (3) 접미사, 조건화된 후보의 토큰 점수만의 합계를 계산합니다. 마지막으로, (4) 합계, 모델에 전달된 모든 토큰 점수의 합계를 계산하는 것을 포함합니다. 서로 다른 함수를 사용한 이유는 특정 작업에 대해 한 점수 함수를 다른 점수 함수보다 사용할 때 성능이 상당히 향상되는 것을 관찰했기 때문입니다. 따라서 모든 작업에 대한 공정성을 보장하기 위해 각 작업에 대해 모든 점수 함수에 대해 가장 높은 정확도를 선택했습니다. 4 결과 및 발견 이 섹션에서는 실험의 결과와 결과를 제시합니다. 먼저, 그림 4에서 finetuned 모델의 효과성에 대한 평가 결과를 vanilla OPT 모델과 비교하여 설명합니다. 설명이 있는 fewshot 프롬핑과 설명이 없는 fewshot 프롬핑을 모두 사용할 때 세 가지 다른 척도에서 평가합니다. 나아가, 두 프롬핑 조건에서 척도를 증가시키면서 각 모델의 성능이 단조롭게 증가하는 것을 관찰했는데, 이는 모델의 용량과 전반적인 성능 사이에 양의 상관관계가 있음을 나타냅니다. 그러나 이러한 추세는 zeroshot 프롬핑 방법에는 적용되지 않는다는 점에 유의합니다. 분포 밖 작업을 테스트하고 finetuned 모델은 컨텍스트에서 fewshot 예시를 사용하여 학습했기 때문입니다. 따라서 나머지 평가에서는 설명이 있는 fewshot 프롬핑 방법과 설명이 없는 fewshot 프롬핑 방법에만 초점을 맞춥니다. 구체적으로, vanilla OPT 모델과 비교하여 추론 데이터 세트에 대한 OPT 모델의 finetuning 영향을 조사하고 추론 기술 측면에서 finetuning과 프롬핑 중 설명의 효과를 탐구합니다. 4.1 추론 기술에 대한 모델 성능 이 섹션과 다음 섹션에서 보고하는 결과는 모델 크기 및 fewshot 프롬핑 방법과 같은 다양한 조건에서 각 추론 기술의 분류 정확도입니다. 표는 Welch의 t-검정으로 측정한 바와 같이 OPT-RE 또는 OPT-R이 vanilla OPT 모델보다 상당히 더 나은 추론 기술을 보여줍니다(p &lt; 0.05). 반대로, 표 3은 vanilla OPT 모델이 미세 조정된 두 모델보다 상당히 더 나은 추론 기술을 보여줍니다. 기술 OPT OPT-R OPT-RE 64.7* 60.8* 수치적 44.8 65.2* 유추적 49.0 62.9* 계산 19.8 13.31.3* 신체적 38.2 37.49.1* 함축적 사회적 Int 사물 42.6 47.34.1 43.0* 54.62.6* 51.6* 40.59.9* 표 2: 추론 기술에 따른 성과. OPT-RE 또는 OPT-R은 웰치 t-테스트(p &lt; 0.05)로 측정한 OPT 모델보다 상당히 더 나은 성과를 보이며, * 기호로 표시했습니다. 성과는 Fewshot 및 Fewshot-E 프롬프트, 세 가지 다른 척도 및 해당 추론 기술에 따른 과제에서 측정했습니다. 가장 좋은 결과는 굵은 글씨로 표시했습니다. 결과에 따르면, OPT 모델의 미세 조정된 변형은 7가지 고유한 추론 기술에서 상당한 개선을 보였으며, 특히 수치적 및 유추적 추론 과제에 중점을 두었습니다. 구체적으로, 수학적 기술 논증 TE - 연역적 상식 OPT OPT-R OPT-RE 57.9 46.136.0 29.033.4 29.48.729.428.8표 3: 추론 기술에 따른 성과. OPT는 웰치 t-검정(p &lt; 0.05)으로 측정한 대로 OPT-R 또는 OPT-RE보다 상당히 우수한 성과를 보였습니다. 성과는 해당 추론 기술에 속하는 세 가지 다른 척도와 과제인 Fewshot 및 Fewshot-E 프롬프트에서 측정되었습니다. TE는 텍스트적 함의입니다. 영어: 계산 기술에서 OPT-RE 변형은 OPT-R 및 OPT 모델보다 성능이 뛰어나 수학적 데이터 세트의 미세 조정 프로세스 중에 설명을 통합하는 것이 중요함을 강조합니다. 마찬가지로 물리적 추론 과제도 유사한 추세를 보입니다. 반면에 논증, 연역적 텍스트적 함의 및 상식 기술의 경우 미세 조정되지 않은 버전이 상당히 우수한 성능을 보이는 것을 볼 수 있습니다. 4.2 미세 조정 기술 분석 표 4는 세 가지 모델에서 얻은 분류 정확도 결과를 추론 기술 및 사용된 few-shot 프롬프트 방법과 관련하여 보여줍니다. 각 추론 기술에 대한 최상의 정확도 값은 굵은 글씨로 표시되고 셀은 녹색에서 흰색까지의 색상으로 음영 처리되어 각 추론 기술의 정확도 스펙트럼에서 위치를 나타냅니다. 다른 모델에서 유사한 성능을 보이는 기술은 더 밝은 녹색 음영이 지정되어 모델 간 성능 차이가 더 큰 다른 기술보다 색상 스펙트럼이 일찍 끝남을 나타냅니다. 이 표는 추론 기술에 대한 미세 조정 및 프롬프트 방법의 효과를 구분하기 위해 4개의 블록으로 나뉩니다. 첫 번째 블록은 미세 조정(OPT-RE 및 OPT-R) 모델이 바닐라 OPT 모델보다 성능이 뛰어난 기술을 보여주고, 두 번째 블록은 OPT-RE가 다른 모델보다 정확도가 더 높은 기술을 강조하여 이러한 기술에 대한 설명에서 미세 조정의 중요성을 보여줍니다. 세 번째 블록은 OPT가 다른 모델보다 성능이 뛰어난 기술을 표시하여 미세 조정이 이 경우 실제로 성능을 저하시킨다는 것을 보여주고, 네 번째 블록은 모델 또는 프롬프트 방법의 선택이 전체 성능에 거의 영향을 미치지 않는 기술을 식별합니다. OPT OPT-R OPT-RE 기술 수치적 Fewshot Fewshot-E Fewshot Fewshot-E Fewshot Fewshot-E 39.49.65.65.64.64.유추적 51.46.63.62.60.60.사물 53.55.61.63.60.59.사회적 상호 작용 33.34.43.42.40.40.텍스트적 함의 43.42.47.47.51.51.문법적 54.55.61.60.62.63.멀티홉 36.31.38.39.39.37.기호 44.47.51.51.51.52.공간적 44.47.49.51.49.49.사회적 상황 46.46.53.53.51.52.계산 19.20.13.12.29.32.물리적 35.40.36.38.48.50.논리적 31.33.33.34.36.38.시간적 50.49.43.46.48.38.주장 55.60.46.45.48.48.TE - 연역적 33.38.27.30.29.29.관계적 47.51.47.47.44.44.상식 35.31.29.29.28.29.TE - 유추적 16.18.18.20.18.18.귀추적 33.36.36.34.34.35.윤리 26.25.26.25.26.27.연역적 39.40.39.40.40.41.인과적 50.50.49.48.50.50.과학적 23.23.24.24.25.24.수치적 상식 59.59.59.59.59.59.문자열 60.60.61.61.60.60.표 4: 추론 기술과 사용된 few-shot prompting 방법에 따른 다양한 모델에서 얻은 분류 정확도 결과. 각 추론 기술에 대해 얻은 최상의 정확도는 굵은 글씨로 강조 표시됩니다. 셀은 정확도 스펙트럼에서 위치를 나타내기 위해 녹색에서 흰색까지의 색상으로 음영 처리됩니다. 달성된 결과에서 분산이 작은 추론 기술은 모델 간의 유사성 정도를 전달하기 위해 더 밝은 녹색 음영이 지정됩니다. 첫 번째 블록은 미세 조정된 모델이 바닐라 OPT보다 현저히 더 나은 성과를 보이는 기술을 강조합니다. 두 번째 블록은 OPT-RE가 다른 모델보다 더 나은 성과를 보이는 기술을 강조합니다. 반면, 세 번째 블록은 OPT가 다른 모델보다 더 나은 성과를 보이는 기술을 보여줍니다. 마지막으로, 네 번째 블록은 모델이나 프롬프팅 방법의 선택이 전반적인 성과에 거의 영향을 미치지 않는 기술을 식별합니다. 설명의 효과 이 연구에서 조사하고자 했던 핵심 질문 중 하나는 미세 조정 및 프롬프팅 중에 설명이 OPT 모델의 추론 능력을 향상시키는 데 어느 정도 역할을 하는지입니다. 표 5에 제시된 결과는 프롬프팅에 사용된 fewshot 예제에서 설명의 존재 또는 부재가 추론 데이터 세트에서 모델을 미세 조정할 때 모델의 성능에 큰 영향을 미치지 않는다는 것을 시사합니다. 구체적으로, 표 5에서 이상치로 식별된 시간적 기술을 제외하여 추론 기술 전체에 걸친 각 모델의 절대 정확도 차이의 분산을 제시합니다. 구체적으로, 우리는 표 4에서 각 모델에 대한 두 개의 해당 열 사이의 차이를 계산합니다. 이러한 값은 프롬프팅 중에 설명을 포함하는 것이 모델 성능에 미치는 영향에 대한 통찰력을 제공합니다. 우리의 연구 결과에 따르면 OPT-R 및 OPT-RE 모델의 차이는 무시할 수 있으며, 프롬프팅 방법의 선택이 모델의 정확도에 큰 영향을 미치지 않는다는 것을 시사합니다. 그러나 vanilla OPT 모델의 경우 차이가 더 크며, fewshot 프롬프팅 중에 설명을 사용하는 것의 중요성을 강조합니다. 그러나 서로 다른 fewshot 프롬프팅 방법에 걸친 각 모델의 평균 성능은 Fewshot에서 Fewshot-E(설명 통합)로, 그리고 OPT에서 OPT-R로, OPT-RE 모델로 분류 정확도가 약간이지만 일관되게 증가했음을 보여주며, 이는 설명이 미세 조정과 프롬프팅 중에 성능에 미치는 영향이 작음을 보여줍니다. 모델 OPT OPT-R OPT-RE Std (F-FE) Avg(F) Avg(FE) 41.43.2.40.0.43.0.44.44.표 5: 첫 번째 열은 Fewshot(F) 및 Fewshot-E(FE) 프롬프팅 방법을 사용할 때 각 모델의 다양한 추론 기술에 대한 정확도의 절대 차이의 분산을 보여줍니다. 두 번째 및 세 번째 열은 각 프롬프팅 방법에 대한 각 모델의 평균 성능을 보여줍니다. 이상치 Temporal 기술을 제거한 후 결과를 얻습니다. 5
--- RELATED WORK ---
추론 LLM LLM은 NLP 및 관련 분야에서 상당한 진전을 이루었습니다(Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022). 특히 사전 학습, 프롬프트 및 예측 패러다임(Liu et al., 2021)의 출현으로 더욱 그렇습니다. 이 패러다임은 이러한 모델이 지침을 사용하여 컨텍스트 내 fewshot 또는 zeroshot 학습을 통해 다양한 작업을 해결할 수 있도록 했습니다(Wei et al., 2021b; Iyer et al., 2022). 그러나 최근 문헌에서 추론 능력에 대한 논쟁이 있었습니다(Huang and Chang, 2022; AlKhamissi et al., 2022). 여러 연구에서 동일한 다음 토큰 예측을 통해 학습된 LM의 크기를 늘리면
--- METHOD ---
각 모델의 추론 기술에 걸쳐 s. 그러나 표 5에서 볼 수 있듯이 바닐라 OPT 모델의 성능에 더 눈에 띄는 효과가 있음을 알았습니다. 또한 Fewshot에서 Fewshot-E로, OPT에서 OPT-R로, OPT-RE 모델로 모든 작업에서 평균 성능이 지속적으로 증가하는 것을 관찰하여 설명이 미세 조정과 프롬프트 중 성능에 미치는 영향이 작음을 나타냅니다. 마지막으로 표 4는 결과 요약을 제시하여 미세 조정 또는 프롬프트 중 설명을 통합하여 어떤 추론 기술이 향상되는지, 어떤 기술이 부정적인 영향을 미치는지, 어떤 기술이 설명과 관련하여 무시할 수 있는 영향을 미치는지 나타냅니다. 이들의 조합. 동시 연구에서는 미세 조정 중에 발생하는 것 이상의 추론 기술에 대한 이러한 모델의 일반화 능력을 조사했지만(Yu et al., 2022), 추론 기술과 관련하여 미세 조정 및 프롬프트 중 설명의 역할에 대한 포괄적인 평가는 여전히 부족합니다. 2 OPT-R: 추론 기술에 대한 미세 조정 추론 데이터 세트와 설명 2. 이 논문에서 우리는 이러한 격차를 해소하고자 합니다. 우리는 이러한 모델의 대표로서 OPT(Zhang et al., 2022)를 조사하고 이를 기본 모델로 활용합니다. 각 인스턴스에 대한 설명이 포함된 신중하게 큐레이팅된 오픈소스 추론 데이터 세트 컬렉션에서 OPT를 미세 조정하여 SUPER-NATURALINSTRUCTIONS 벤치마크(Wang et al., 2022)에서 추출한 57개 작업에 대한 성능을 평가합니다. 여기에는 26가지 추론 기술이 포함됩니다. 우리의
--- EXPERIMENT ---
s는 세 가지 핵심 차원, 즉 미세 조정, 촉구, 규모를 중심으로 구성되며, 각각은 세 가지 고유한 구성 요소로 구성됩니다(그림 1 참조). 미세 조정: (1) (바닐라) 미세 조정되지 않은 OPT 모델; (2) 설명이 없는 미세 조정된 OPT 모델(OPT-R); (3) 설명이 있는 미세 조정된 OPT 모델(OPT-RE). 촉구: (1) 제로 샷 촉구; (2) 설명이 없는 퓨샷 촉구; (3) 설명이 있는 퓨샷 촉구. 마지막으로 규모: (1) 1.3B; (2) 6.7B; (3) 13B. 따라서 27개의 서로 다른 구성 요소로 구성된 그리드를 만들어 다양한 모델 규모에서 미세 조정 및 추론 중 설명의 영향을 측정하는 자세한 분석을 제공합니다. 샘플 수 연구 결과에 따르면 추론 데이터 세트에 대한 미세 조정은 수치적, 유추적 및 객체 추론을 포함한 7가지 추론 기술에서 통계적으로 유의미한 개선으로 이어지며, 물리적, 계산 및 텍스트적 함의는 OPT-RE 모델에서만 유의미한 효과를 보였으며, 이는 fewshot 프롬프트 조건 모두에서 나타났습니다.AQUA-RAT COQA COS-E ECQA ESNLI GSM8K ProofWriter StrategyQA 데이터 세트 그림 3: 학습 코퍼스의 각 데이터 세트에 있는 샘플 수. Y축은 로그 스케일입니다. OPT를 정제하는 데 사용된 미세 조정 코퍼스는 다양한 추론 데이터 세트로 구성되며, 각각에는 답변에 대한 해당 설명 또는 근거가 포함됩니다. 이러한 근거는 일련의 작은 단계(예: 사고의 사슬) 또는 답변의 추론을 설명하는 자유형 텍스트로 구성될 수 있습니다. 그림 2에서 볼 수 있듯이 학습 프로세스 동안 모든 작업에 대해 균일한 템플릿을 사용합니다. 모델에 대한 입력은 작업 정의로 시작하여, 그 다음에 답을 제공하라는 지시와 간단한 추론이 이어집니다. 다음으로, 각 인스턴스에 대한 학습 내내 일정하게 유지되는 두 개의 무작위 인컨텍스트 예제를 학습 세트에서 균일하게 추출합니다. 그런 다음 현재 학습 인스턴스에 대한 입력이 각 작업에 고유한 형식으로 제공됩니다. 그런 다음 답에 대한 옵션이 입력에 포함되지만 인컨텍스트 예제에는 포함되지 않습니다(작업별 정의 및 옵션에 대한 자세한 내용은 부록 A 참조). 옵션은 각 학습 인스턴스에 대해 미리 섞입니다. 마지막으로 모델에 &quot;출력: 답은&quot;이라는 답 접두사가 제공되고 답을 예측하는 작업이 수행되고, 그 다음에 OPT-RE가 미세 조정되고 있는지에 대한 설명이 이어집니다. 마찬가지로, 인컨텍스트 예제는 OPT-RE를 학습할 때만 설명을 포함합니다. 아래는 미세 조정 중에 사용된 각 데이터 세트에 대한 간략한 설명입니다. 각 데이터 세트의 상대적 크기는 그림 3을 참조하세요. AQUA-RAT 대수 문제 해결을 위한 합리적 근거 데이터 세트(Ling et al., 2017)는 문제를 일련의 작은 단계로 나누어 대수 단어 문제를 푸는 작업을 보다 실현 가능하게 만듭니다. 그들은 자연어로 된 질문, 답변 및 합리적 근거와 최종 답을 도출하는 데 사용할 수 있는 사람이 읽을 수 있는 수학적 표현식을 포함하는 100k 샘플 데이터 세트를 만듭니다. CoQA 대화형 질문 답변 데이터 세트 Reddy et al. (2019). 7개 다른 도메인의 구절에 대한 8k 대화에서 수집한 127k 질문과 답변으로 구성되어 있습니다. 대화가 포함된 구절이 주어지면 모델은 구절에서 해당 증거를 강조 표시하여 질문에 답하는 작업을 맡습니다. CoS-E 상식적 추론을 사용하여 언어 모델을 유도하는 상식적 설명 데이터 세트 Rajani et al. (2019). 이 데이터 세트에서 모델은 질문과 선택지 집합이 주어지고 제공된 선택지 중 하나를 선택하고 그 선택지가 옳은 이유를 자연어로 설명하는 작업을 맡습니다.ECQA 상식적 질문 답변에 대한 설명 데이터 세트 Aggarwal et al. (2021). 모델이 제공된 옵션 중 하나를 선택하여 주어진 질문에 답하고 설명도 제공해야 하기 때문에 CoS-E와 유사합니다.ESNLI 설명이 포함된 Stanford 자연어 추론 데이터 세트 Camburu et al. (2018)은 모델을 훈련하여 결정에 대한 해석 가능하고 견고한 설명을 제공합니다.저자는 SNLI 데이터 세트(Bowman et al., 2015)를 인간이 주석을 단 설명으로 확장합니다.모든 NLI 작업과 마찬가지로 모델에 전제와 가설이 주어지고 작업은 가설 문장이 주어진 전제와 관련하여 수반되는지, 모순되는지 또는 중립적인지 여부를 판별하는 것입니다.GSM8K 초등학교 수학 데이터 세트 Cobbe et al. (2021) 다단계 수학적 추론을 더 잘 수행하기 위한 모델을 훈련합니다. 8.5k개의 언어적으로 다양한 초등학교 수학 단어 문제로 구성되어 있습니다. 따라서 모델의 과제는 최종 답을 얻기 위해 일련의 산술 연산을 수행하여 질문에 답하는 동시에 추론 단계를 설명하는 것입니다. ProofWriter ProofWriter 데이터 세트 Tafjord et al. (2021)은 RuleTaker 데이터 세트(Clark et al., 2020)의 이론의 의미와 이를 뒷받침하는 자연어 증명을 모두 생성합니다. 구체적으로, 일련의 사실과 규칙이 주어지면 모델은 &quot;예&quot;, &quot;아니요&quot; 또는 &quot;알 수 없음&quot;을 사용하여 질문에 답하고 제공된 사실과 규칙을 참조하여 추론 경로를 제공해야 합니다. 우리는 추론이 최대 5단계까지 필요한 질문이 있는 Rule Taker의 오픈 월드 가정 하위 집합을 고려합니다. StrategyQA Strategy Question Answering 데이터 세트 Geva et al. (2021)은 필요한 추론 단계가 질문에 내재되어 있는 질문에 대한 멀티홉 추론을 개선합니다. 따라서 모델의 과제는 &quot;예&quot; 또는 &quot;아니요&quot;를 사용하여 질문에 답한 다음 여러 단계로 분해하여 답변을 설명하는 전략을 제공하는 것입니다. 2.2 미세 조정 절차 OPT Open Pretrained Transformers(OPT) 모델은 Zhang et al. (2022)이 발표한 125M~175B 매개변수 범위의 디코더 전용 사전 학습된 변환기 모음입니다. 이 작업에서 우리는 크기가 1.3B인 세 개의 OPT 모델을 사용합니다. 6.7B 및 13B. 각 모델 아키텍처, 사전 학습 코퍼스 및 학습 구성(예: 가중치 초기화, 최적화, 토크나이저, 하이퍼파라미터 등)의 세부 사항은 Zhang et al.(2022)에서 찾을 수 있습니다. 추론 기술 귀납적 추론 유추적 추론 논증 추론 인과 추론 상식적 추론 상식적 추론 → 수치적 상식 ... 상식적 추론 → 물리적 추론 상식적 추론 → 사회적 상황 상식적 추론 → 공간적 추론 윤리 과제 ID tasktask 1287, tasktasktask279, task156, tasktasktask580, task937, tasktask082, tasktask221, task1568, tasktask667, task724, tasktask1712, task052, tasktask717, task211, task연역적 추론 문법적 추론 논리적 추론 논리적 추론 → 기호 추론 task923, task수학 → 계산 task523, task멀티홉 추론 task1297, task수치적 추론 task621, taskReasoning on Objects task1583, taskReasoning on Social Interactions task609, task881, taskReasoning on Strings 관계적 추론 과학적 추론 시간적 추론 텍스트적 함의 텍스트적 함의 → 유추적 추론 텍스트적 함의 → 연역적 추론 tasktask1380, task472, tasktask1431, task228, tasktask018, task1549, tasktask738, task890, tasktasktask1612, task534, task표 1: 각 추론 기술에 사용된 SUP-NATINST(Wang et al., 2022)의 평가 과제.구현 세부 정보선택된 모델을 미세 조정하기 위해 다른 코드베이스(Zhang et al., 2022)에 비해 더 높은 학습 효율성을 제공하는 metaseq¹ 구현을 활용했습니다. 각 모델은 10개의 에포크에 대해 두 번 미세 조정되며, 한 번은 설명이 있고 한 번은 설명이 없습니다(즉, 각각 OPT-RE 대 OPT-R). 모델은 각 에포크가 끝날 때 선택한 SUPERNATURALINSTRUCTIONS 검증 작업 세트에 대해 평가되고, 가장 좋은 성능을 보이는 체크포인트가 테스트 작업에서 평가를 위해 선택됩니다. 손실은 추론 중에 모델이 예측하도록 작업된 토큰에 대해서만 계산되며, 전체 입력에 대해서는 계산되지 않습니다. 이를 (Iyer et al., 2022)에서 레이블 손실이라고 합니다. 모든 데이터 세트의 샘플은 학습 중에 섞입니다. 또한 추론 시간 후속과 일치하도록 작업 정의 외에도 미세 조정 중에 모델에 두 개의 컨텍스트 내 예가 제공됩니다(Wang et al., 2022). &#39;https://github.com/facebookresearch/metaseq 3 모델 평가 3.1 SUPER-NATURALINSTRUCTIONS 작업 이 연구에서는 Wang et al.(2022)이 제안한 SUPERNATURALINSTRUCTIONS 벤치마크 버전 2(약칭 SUP-NATINST)의 하위 집합에 초점을 맞춥니다. 여기에는 1,616개의 다양한 NLP 작업이 포함되며 작업 유형, 도메인과 이 작업에서 더 중요한 기본 추론 기술과 같은 각 작업에 대한 메타 레이블이 포함됩니다. 구체적으로 두 가지 핵심 기준을 충족하는 작업 하위 집합을 선택합니다. (i) 작업은 단일 추론 기술에 초점을 맞추어 특정 원자 기술을 평가할 수 있고 (ii) 작업은 섹션 3.2에서 자세히 설명한 대로 분류 모드를 사용하여 테스트할 수 있습니다. 미세 조정 데이터와 평가 벤치마크 사이에 데이터 오염이 없음에 유의하세요. 2 https://github.com/allenai/natural-instructions/tree/v2.6에서 데이터를 다운로드했습니다. 정확도(%) Fewshot-E Fewshot Zeroshot1.3B 6.7B 13B 1.3B 6.7B 13B 1.3B 6.7B 13B 크기 크기 크기 모델 OPT OPT-R OPT-RE 그림 4: 이 연구에서 분석한 세 가지 주요 차원인 미세 조정, 촉구 및 척도의 함수로서 모든 작업에서 달성한 결과.벤치마크 분할 작업 선택 프로세스에 따라 테스트 세트 내의 다양성을 보장하기 위해 무작위 표집 기법을 적용합니다.특히 각 추론 기술에서 최대 3개의 작업을 선택하고 나머지 작업을 검증 세트에 할당합니다.특히, 이 접근 방식을 사용하면 테스트를 위해 선택된 추론 기술의 대표 샘플을 얻을 수 있는 동시에 모델의 성능이 특정 작업 하위 집합의 영향을 받지 않도록 할 수 있습니다.표 1은 각 추론 기술에 대한 미세 조정된 모델을 평가하는 데 사용된 작업의 전체 목록을 보여줍니다. 3.2 평가 설정 앞서, 우리는 미세 조정된 모델을 평가하기 위해 SUP-NATINST에서 26가지 추론 기술에 걸친 57개 과제를 선택했다고 언급했습니다. 섹션 3.1에 자세히 설명된 대로 기준을 충족하기 위해 각 과제는 두 가지 조건을 충족해야 했습니다. 두 번째 조건은 과제가 분류 과제로 간주될 수 있어야 한다는 것입니다. 즉, 이산적인 후보 집합(그 중 하나가 정답)이 있고, 따라서 가장 높은 점수를 받은 후보가 답으로 간주되는 분류 문제로 취급한다는 것을 의미합니다. 이를 보장하기 위해 우리는 간단한 휴리스틱을 활용했습니다. 가능한 후보 답이 10개를 넘지 않는 과제만 샘플링했습니다. 분류 방법 정답을 결정하기 위해 우리는 각 잠재적 후보 답에 대해 포워드 패스를 수행하고 Brown et al. (2020)과 유사하게 후보 토큰이 입력을 따를 가능성을 측정하는 채점 함수를 사용합니다. 이 프로세스는 후속 단락에서 자세히 설명한 대로 별도의 채점 함수를 사용하여 4번 반복됩니다. 4가지 채점 함수에서 가장 높은 정확도 점수는 과제의 결과로 간주됩니다. 점수 함수 이는 각 작업을 네 가지 다른 점수 함수를 사용하여 평가하고 그 결과로 최대 정확도를 취하기 때문에 이 작업의 네 번째 차원으로 간주됩니다. 사용된 네 가지 점수 함수는 다음과 같습니다. (1) 평균, 후보 토큰의 로그 확률의 평균을 계산하는 것을 포함하며 토큰 점수라고도 합니다. (2) 무조건적 노름, 이전 토큰에 의해 조건화되지 않았을 때 후보의 토큰 점수 합계와 이전 입력에 의해 조건화되었을 때 후보 토큰 점수 합계의 차이를 계산합니다. (3) 접미사, 조건화된 후보의 토큰 점수만의 합계를 계산합니다. 마지막으로, (4) 합계, 모델에 전달된 모든 토큰 점수의 합계를 계산하는 것을 포함합니다. 서로 다른 함수를 사용한 이유는 특정 작업에 대해 한 점수 함수를 다른 점수 함수보다 사용할 때 성능이 상당히 향상되는 것을 관찰했기 때문입니다. 따라서 모든 작업에 대한 공정성을 보장하기 위해 각 작업에 대해 모든 점수 함수에 대해 가장 높은 정확도를 선택했습니다. 4 결과 및 발견 이 섹션에서는 실험의 결과와 결과를 제시합니다. 먼저, 그림 4에서 finetuned 모델의 효과성에 대한 평가 결과를 vanilla OPT 모델과 비교하여 설명합니다. 설명이 있는 fewshot 프롬핑과 설명이 없는 fewshot 프롬핑을 모두 사용할 때 세 가지 다른 척도에서 평가합니다. 나아가, 두 프롬핑 조건에서 척도를 증가시키면서 각 모델의 성능이 단조롭게 증가하는 것을 관찰했는데, 이는 모델의 용량과 전반적인 성능 사이에 양의 상관관계가 있음을 나타냅니다. 그러나 이러한 추세는 zeroshot 프롬핑 방법에는 적용되지 않는다는 점에 유의합니다. 분포 밖 작업을 테스트하고 finetuned 모델은 컨텍스트에서 fewshot 예시를 사용하여 학습했기 때문입니다. 따라서 나머지 평가에서는 설명이 있는 fewshot 프롬핑 방법과 설명이 없는 fewshot 프롬핑 방법에만 초점을 맞춥니다. 구체적으로, vanilla OPT 모델과 비교하여 추론 데이터 세트에 대한 OPT 모델의 finetuning 영향을 조사하고 추론 기술 측면에서 finetuning과 프롬핑 중 설명의 효과를 탐구합니다. 4.1 추론 기술에 대한 모델 성능 이 섹션과 다음 섹션에서 보고하는 결과는 모델 크기 및 fewshot 프롬핑 방법과 같은 다양한 조건에서 각 추론 기술의 분류 정확도입니다. 표는 Welch의 t-검정으로 측정한 바와 같이 OPT-RE 또는 OPT-R이 vanilla OPT 모델보다 상당히 더 나은 추론 기술을 보여줍니다(p &lt; 0.05). 반대로, 표 3은 vanilla OPT 모델이 미세 조정된 두 모델보다 상당히 더 나은 추론 기술을 보여줍니다. 기술 OPT OPT-R OPT-RE 64.7* 60.8* 수치적 44.8 65.2* 유추적 49.0 62.9* 계산 19.8 13.31.3* 신체적 38.2 37.49.1* 함축적 사회적 Int 사물 42.6 47.34.1 43.0* 54.62.6* 51.6* 40.59.9* 표 2: 추론 기술에 따른 성과. OPT-RE 또는 OPT-R은 웰치 t-테스트(p &lt; 0.05)로 측정한 OPT 모델보다 상당히 더 나은 성과를 보이며, * 기호로 표시했습니다. 성과는 Fewshot 및 Fewshot-E 프롬프트, 세 가지 다른 척도 및 해당 추론 기술에 따른 과제에서 측정했습니다. 가장 좋은 결과는 굵은 글씨로 표시했습니다. 결과에 따르면, OPT 모델의 미세 조정된 변형은 7가지 고유한 추론 기술에서 상당한 개선을 보였으며, 특히 수치적 및 유추적 추론 과제에 중점을 두었습니다. 구체적으로, 수학적 기술 논증 TE - 연역적 상식 OPT OPT-R OPT-RE 57.9 46.136.0 29.033.4 29.48.729.428.8표 3: 추론 기술의 함수로서의 성과, 여기서 OPT는 웰치의 t-검정(p &lt; 0.05)으로 측정한 대로 OPT-R 또는 OPT-RE보다 상당히 더 나은 성과를 보였습니다. 성과는 해당 추론 기술에 속하는 세 가지 다른 척도와 과제인 Fewshot 및 Fewshot-E 프롬프트에서 측정되었습니다. TE는 텍스트적 함의입니다. 영어: 계산 기술에서 OPT-RE 변형은 OPT-R 및 OPT 모델보다 성능이 뛰어나 수학적 데이터 세트의 미세 조정 프로세스 중에 설명을 통합하는 것이 중요함을 강조합니다. 마찬가지로 물리적 추론 과제도 유사한 추세를 보입니다. 반면에 논증, 연역적 텍스트적 함의 및 상식 기술의 경우 미세 조정되지 않은 버전이 상당히 우수한 성능을 보이는 것을 볼 수 있습니다. 4.2 미세 조정 기술 분석 표 4는 세 가지 모델에서 얻은 분류 정확도 결과를 추론 기술 및 사용된 few-shot 프롬프트 방법과 관련하여 보여줍니다. 각 추론 기술에 대한 최상의 정확도 값은 굵은 글씨로 표시되고 셀은 녹색에서 흰색까지의 색상으로 음영 처리되어 각 추론 기술의 정확도 스펙트럼에서 위치를 나타냅니다. 다른 모델에서 유사한 성능을 보이는 기술은 더 밝은 녹색 음영이 지정되어 모델 간 성능 차이가 더 큰 다른 기술보다 색상 스펙트럼이 일찍 끝남을 나타냅니다. 이 표는 추론 기술에 대한 미세 조정 및 프롬프트 방법의 효과를 구분하기 위해 4개의 블록으로 나뉩니다. 첫 번째 블록은 미세 조정(OPT-RE 및 OPT-R) 모델이 바닐라 OPT 모델보다 성능이 뛰어난 기술을 보여주고, 두 번째 블록은 OPT-RE가 다른 모델보다 정확도가 더 높은 기술을 강조하여 이러한 기술에 대한 설명에서 미세 조정의 중요성을 보여줍니다. 세 번째 블록은 OPT가 다른 모델보다 성능이 뛰어난 기술을 표시하여 미세 조정이 이 경우 실제로 성능을 저하시킨다는 것을 보여주고, 네 번째 블록은 모델 또는 프롬프트 방법의 선택이 전체 성능에 거의 영향을 미치지 않는 기술을 식별합니다. OPT OPT-R OPT-RE 기술 수치적 Fewshot Fewshot-E Fewshot Fewshot-E Fewshot Fewshot-E 39.49.65.65.64.64.유추적 51.46.63.62.60.60.사물 53.55.61.63.60.59.사회적 상호 작용 33.34.43.42.40.40.텍스트적 함의 43.42.47.47.51.51.문법적 54.55.61.60.62.63.멀티홉 36.31.38.39.39.37.기호 44.47.51.51.51.52.공간적 44.47.49.51.49.49.사회적 상황 46.46.53.53.51.52.계산 19.20.13.12.29.32.물리적 35.40.36.38.48.50.논리적 31.33.33.34.36.38.시간적 50.49.43.46.48.38.주장 55.60.46.45.48.48.TE - 연역적 33.38.27.30.29.29.관계적 47.51.47.47.44.44.상식 35.31.29.29.28.29.TE - 유추적 16.18.18.20.18.18.귀추적 33.36.36.34.34.35.윤리 26.25.26.25.26.27.연역적 39.40.39.40.40.41.인과적 50.50.49.48.50.50.과학적 23.23.24.24.25.24.수치적 상식 59.59.59.59.59.59.문자열 60.60.61.61.60.60.표 4: 추론 기술과 사용된 few-shot prompting 방법에 따른 다양한 모델에서 얻은 분류 정확도 결과. 각 추론 기술에 대해 얻은 최상의 정확도는 굵은 글씨로 강조 표시됩니다. 셀은 정확도 스펙트럼에서 위치를 나타내기 위해 녹색에서 흰색까지의 색상으로 음영 처리됩니다. 달성된 결과에서 분산이 작은 추론 기술은 모델 간의 유사성 정도를 전달하기 위해 더 밝은 녹색 음영이 지정됩니다. 첫 번째 블록은 미세 조정된 모델이 바닐라 OPT보다 현저히 더 나은 성과를 보이는 기술을 강조합니다. 두 번째 블록은 OPT-RE가 다른 모델보다 더 나은 성과를 보이는 기술을 강조합니다. 반면, 세 번째 블록은 OPT가 다른 모델보다 더 나은 성과를 보이는 기술을 보여줍니다. 마지막으로, 네 번째 블록은 모델이나 프롬프팅 방법의 선택이 전반적인 성과에 거의 영향을 미치지 않는 기술을 식별합니다. 설명의 효과 이 연구에서 조사하고자 했던 핵심 질문 중 하나는 미세 조정 및 프롬프팅 중에 설명이 OPT 모델의 추론 능력을 향상시키는 데 어느 정도 역할을 하는지입니다. 표 5에 제시된 결과는 프롬프팅에 사용된 fewshot 예제에서 설명의 존재 또는 부재가 추론 데이터 세트에서 모델을 미세 조정할 때 모델의 성능에 큰 영향을 미치지 않는다는 것을 시사합니다. 구체적으로, 표 5에서 이상치로 식별된 시간적 기술을 제외하여 추론 기술 전체에서 각 모델의 절대 정확도 차이의 분산을 제시합니다. 구체적으로, 우리는 표 4에서 각 모델에 대한 두 개의 해당 열 사이의 차이를 계산합니다. 이러한 값은 프롬프팅 중에 설명을 포함하는 것이 모델 성능에 미치는 영향에 대한 통찰력을 제공합니다. 우리의 연구 결과에 따르면 OPT-R 및 OPT-RE 모델의 차이는 무시할 수 있으며, 프롬프팅 방법의 선택이 모델의 정확도에 큰 영향을 미치지 않는다는 것을 시사합니다. 그러나 vanilla OPT 모델의 경우 차이가 더 크며, fewshot 프롬프팅 중에 설명을 사용하는 것의 중요성을 강조합니다. 그러나 서로 다른 fewshot 프롬프팅 방법에 걸친 각 모델의 평균 성능은 Fewshot에서 Fewshot-E(설명 통합)로, 그리고 OPT에서 OPT-R로, OPT-RE 모델로 분류 정확도가 약간이지만 일관되게 증가했음을 보여주며, 이는 설명이 미세 조정과 프롬프팅 중에 성능에 미치는 영향이 작음을 보여줍니다. 모델 OPT OPT-R OPT-RE Std (F-FE) Avg(F) Avg(FE) 41.43.2.40.0.43.0.44.44.표 5: 첫 번째 열은 Fewshot(F) 및 Fewshot-E(FE) 프롬프팅 방법을 사용할 때 다양한 추론 기술에 걸쳐 각 모델의 정확도의 절대 차이의 분산을 보여줍니다. 두 번째와 세 번째 열은 각 프롬프팅 방법에 대한 각 모델의 평균 성능을 보여줍니다. 결과는 이상치인 Temporal 기술을 제거한 후 얻어집니다. 5 관련 연구 추론 LLM LLM은 NLP 및 관련 분야(Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022)에서 상당한 진전을 이루었으며, 특히 사전 학습, 프롬프트 및 예측 패러다임(Liu et al., 2021)의 출현으로 더욱 그렇습니다. 이 패러다임은 이러한 모델이 지침을 사용하여 컨텍스트 내 fewshot 또는 zeroshot 학습을 통해 다양한 작업을 해결할 수 있도록 했습니다(Wei et al., 2021b; Iyer et al., 2022). 그러나 그들의 추론 능력은 최근 문헌에서 논쟁의 대상이 되었습니다(Huang and Chang, 2022; AlKhamissi et al., 2022). 여러 연구에 따르면 동일한 다음 토큰 예측 방법을 통해 학습된 LM의 크기를 늘리면 추론을 포함한 복잡한 행동이 나타날 수 있습니다(Wei et al., 2022a). 예를 들어, 일부 연구에서는 충분히 큰 LM이 생각의 사슬 프롬프트(Wei et al., 2022b)를 사용하여 인간과 같은 추론을 시뮬레이션할 수 있음을 보여주었습니다. 다른 연구에서는 &quot;단계별로 생각해 보자&quot;(Kojima et al., 2022)와 같은 간단한 프롬프트를 추가하면 최종 답을 디코딩하기 전에 명확한 추론 단계를 생성하여 LLM에서 추론 능력을 이끌어낼 수 있음을 보여주었습니다. 그러나 일부 연구자들은 인간의 추론 사고 과정을 모방하는 것이 모델이 진정으로 추론할 수 있다고 주장하는 것과 다르다고 주장합니다(Wei et al., 2022b). 미세 조정된 LLM 동시 연구에서는 LLM이 지침을 따르도록 미세 조정하여 제로 및 퓨샷 학습을 통해 보이지 않는 작업에 대한 일반화 능력을 향상시켰습니다(Iyer et al., 2022; Chung et al., 2022). 그러나 우리의 접근 방식은 각 인스턴스에 대한 설명을 제공하는 선택된 수의 오픈 소스 데이터 세트에 대해서만 미세 조정한다는 점에서 다릅니다. 이를 통해 추론 기술의 맥락에서 미세 조정 중 설명의 중요성에 집중할 수 있습니다. (Iyer et al., 2022; Wang et al., 2022)와 같은 동시 연구에서는 미세 조정 및 추론 중에 다양한 프롬프트 방법을 실험했지만, 저희 연구는 주로 추론 기술 집합에 걸쳐 미세 조정된 모델의 추론 능력을 평가하는 데 중점을 둡니다. 다른 동시 연구에서는 보류된 추론 과제 집합에 대한 미세 조정의 영향을 탐구했지만(Yu et al., 2022), 답변 생성을 포함하는 평가 접근 방식은 디코딩 전략, 디코딩 매개변수 및 프롬프트 템플릿과 같은 다양한 요인의 영향을 받을 수 있습니다. 이와 대조적으로 저희는 (Brown et al., 2020)과 유사한 순위 분류 접근 방식을 채택했는데, 이는 더 많은 수의 추론 기술과 과제를 포괄하는 것 외에도 평가 중인 모델의 추론 성능을 더 잘 포착합니다. 6
--- CONCLUSION ---
이 연구에서 우리는 OPT 모델의 세 가지 다른 크기에 대한 미세 조정 및 프롬프트 중 설명을 통합하는 영향을 조사했습니다. 세 가지 핵심 차원을 고려한 체계적이고 포괄적인 평가 프로세스를 통해 설명이 성능에 약간의 개선을 제공했지만 미세 조정된 모델에 대한 추론 중 맥락 내 데모에 통합했을 때 그 효과는 크지 않다는 것을 발견했습니다. 또한, 우리의 결과는 두 미세 조정된 모델 모두 수치적, 유추적 및 사물에 대한 추론과 같은 추론 기술에서 상당한 개선을 보였다는 것을 보여주었습니다. 게다가, 우리는 물리적, 계산 및 텍스트적 함의와 같은 기술이 미세 조정 프로세스 중에 설명을 통합함으로써 이점을 얻었다는 것을 보여주었습니다. 전반적으로, 우리의 연구 결과는 LLM의 추론 능력에 대한 설명을 통합하는 영향에 대한 통찰력을 제공하고 미세 조정 및 프롬프트 중에 설명을 포함하거나 제외함으로써 어떤 추론 기술이 가장 큰 이점을 얻을 수 있는지에 대한 지침을 제공합니다. 제한 사항 저희 연구는 추론 성능에 대한 미세 조정의 영향과 다양한 추론 기술과 관련하여 미세 조정 및 프롬프트 중 설명의 역할에 대한 귀중한 통찰력을 제공하지만 저희 작업에는 몇 가지 제한 사항이 있습니다. 첫째, 저희는 기본 모델로 단일 LLM인 OPT만 고려합니다. 저희의 결과는 다른 아키텍처나 사전 학습 목표를 가진 다른 LLM으로 일반화되지 않을 수 있습니다. 둘째, 설명이 있는 오픈 소스 데이터 세트의 가용성이 제한적이기 때문에 미세 조정을 위해 제한된 추론 데이터 세트만 사용합니다. 그러나 저희의 결과가 실제 시나리오에서 일반적으로 볼 수 있는 더 큰 폐쇄 데이터 세트에서 미세 조정된 모델에는 적용되지 않을 수 있습니다. 셋째, 저희의 실험은 계산 예산의 제한으로 인해 제한된 범위의 모델 크기만 다루므로 저희의 결과가 훨씬 더 큰 모델에는 적용되지 않을 수 있습니다. 마지막으로 저희는 실험에서 fewshot 프롬프트 조건을 사용한 미세 조정만 고려하며, 저희의 결과가 컨텍스트 내 예시 없이 미세 조정된 모델에는 적용되지 않을 수 있습니다. 전반적으로, 저희 연구는 추론 성과에 대한 미세 조정 및 설명의 영향에 대한 귀중한 통찰력을 제공하지만, 더 광범위한 모델, 데이터 세트 및 미세 조정 전략에서 이러한 요소를 조사하기 위한 추가 연구가 필요합니다. 윤리 성명 이 연구는 기존 공개 데이터 세트를 사용하여 추론 과제에서 LLM의 성과를 분석하고 평가하는 데 기초합니다. 이 연구에서는 개인 식별 정보나 민감한 데이터를 수집하거나 사용하지 않았습니다. 저희는 잘못된 정보 확산, 원치 않는 콘텐츠 생성 및 데이터 세트의 기존 편견 악화에 대한 잠재적 영향을 포함하여 LLM 개발의 잠재적 위험을 인정합니다. 저희의 연구는 LLM이 특정 추론 기술에 맞게 최적화될 수 있는 방법에 대한 투명성과 이해를 개선하는 데 기여하는 것을 목표로 합니다. 저희는 저희의 연구 결과가 LLM 개발 및 배포를 위한 윤리적이고 책임감 있는 접근 방식을 개발하는 추가 연구에 영감을 줄 수 있기를 바랍니다. 참고 문헌 Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla 및 Dinesh Garg. 2021. CommonsenseQA에 대한 설명: 새로운 데이터 세트와 모델. 제59회 계산 언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 3050-3065쪽, 온라인. 계산 언어학 협회. Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, Marjan Ghazvininejad. 2022. 지식 기반으로서의 언어 모델에 대한 리뷰. ArXiv, abs/2204.06031. Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning. 2015. 자연어 추론 학습을 위한 대규모 주석 코퍼스. 2015년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록. 계산 언어학 협회. 톰 브라운, 벤자민 맨, 닉 라이더, 멜라니 수비아, 재러드 D 카플란, 프라풀라 다리왈, 아빈드 닐라칸탄, 프라나브 샤얌, 기리쉬 사스트리, 아만다 애스켈 외. 2020. 언어 모델은 소수 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. 오아나-마리아 캄부루, 팀 록테셸, 토마스 루카시에비츠, 필 블룬섬. 2018. e-snli: 자연어 설명을 통한 자연어 추론. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett 편집자, Advances in Neural Information Processing Systems 31, 9539-9549쪽. Curran Associates, Inc. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with paths. arXiv 사전 인쇄본 arXiv:2204.02311. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv 사전 인쇄본 arXiv:2210.11416. Peter Clark, Oyvind Tafjord, Kyle Richardson. 2020. 언어에 대한 소프트 추론기로서의 변압기. IJCAI에서. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman. 2021. 수학 단어 문제를 풀기 위한 검증자 훈련. ArXiv, abs/2110.14168. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant. 2021. 아리스토텔레스는 노트북을 사용했을까? 암묵적 추론 전략을 사용한 질문 답변 벤치마크. 계산 언어학 협회의 거래, 9:346361. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt. 2021. 수학 데이터 세트를 사용한 수학적 문제 해결 측정. NeurIPS. Jie Huang 및 Kevin Chen-Chuan Chang. 2022. 대규모 언어 모델에서 추론을 향해: 조사. ArXiv, abs/2212.10403. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: 일반화 렌즈를 통한 언어 모델 지시 메타 학습 확장. arXiv 사전 인쇄본 arXiv:2212.12017. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa. 2022. 대규모 언어 모델은 제로샷 추론기입니다. arXiv 사전 인쇄본 arXiv:2205.11916. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom. 2017. 합리적 생성을 통한 프로그램 유도: 대수적 단어 문제를 풀고 설명하는 법 배우기. Association for Computational Linguistics(제1권: 장문 논문)의 제55회 연례 회의록, 158-167쪽, 캐나다 밴쿠버. Association for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. 2021. 사전 학습, 프롬프트 및 예측: 자연어 처리에서 프롬프트 방법에 대한 체계적 조사. ACM Computing Surveys, 55:1 - 35. Nazneen Rajani, Bryan McCann, Caiming Xiong, Richard Socher. 2019. Explain yourself! 상식적 추론을 위한 언어 모델 활용. ACL에서. Siva Reddy, Danqi Chen, Christopher D. Manning. 2019. CoQA: 대화형 질문 답변 챌린지. Association for Computational Linguistics의 거래, 7:249–266. Oyvind Tafjord, Bhavana Dalvi, Peter Clark. 2021. ProofWriter: 자연어에 대한 함의, 증명 및 추론적 진술 생성. Association for Computational Linguistics의 결과: ACL-IJCNLP 2021, 3621–3634페이지, 온라인. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang 및 Tatsunori B. Hashimoto. 2023. 스탠포드 알파카: 지시를 따르는 라마 모델. https://github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur&#39;elien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. Arxiv, ABS/2302.13971. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit 및 Xudong Shen. 2022. Super-NaturalInstructions: 1600개 이상의 NLP 작업에 대한 선언적 지침을 통한 일반화. 2022년 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 5085-5109페이지, 아랍에미리트 아부다비. 계산언어학 협회. 제이슨 웨이, 마르텐 보스마, 빈센트 이 자오, 켈빈 구, 애덤스 웨이 유, 브라이언 레스터, 난 두, 앤드류 M 다이, 콰크 V 르. 2021a. 미세 조정된 언어 모델은 제로 샷 학습기입니다. arXiv 사전 인쇄본 arXiv:2109.01652. 제이슨 웨이, 마르텐 보스마, 빈센트 이 자오, 켈빈 구, 애덤스 웨이 유, 브라이언 레스터, 난 두, 앤드류 M 다이, 콰크 V 르. 2021b. 미세 조정된 언어 모델은 제로 샷 학습기입니다. arXiv 사전 인쇄 arXiv:2109.01652. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean 및 William Fedus. 2022a. 대규모 언어 모델의 새로운 능력. ArXiv, ABS/2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le 및 Denny Zhou. 2022b. 생각의 연쇄 유도는 대규모 언어 모델에서 추론을 이끌어냅니다. arXiv 사전 인쇄 arXiv:2201.11903. Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi. 2022. 상징적 지식 증류: 일반 언어 모델에서 상식 모델로. Ping Yu, Tianlu Wang, O. Yu. Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona Diab, Asli Celikyilmaz. 2022. 알림: 추론 작업에 언어 모델 적용. ArXiv, abs/2212.08286. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: 사전 학습된 트랜스포머 언어 모델 열기. arXiv 사전 인쇄본 arXiv:2205.01068. 데이터 세트 AQUA COQA COS-E ECQA ESNLI GSM8K ProofWriter StrategyQA 과제 정의 대수 단어 문제가 주어집니다. 이 과제의 문제는 종종 최종 답을 얻기 위해 일련의 산술 연산을 실행해야 합니다. 또한 5개의 답변 옵션(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;와 연관됨)이 주어집니다. 다음 문자 중 하나와 해당 설명 외에는 아무것도 생성하지 마십시오. &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;. 대화와 질문이 포함된 구절이 주어집니다. 과제는 질문에 답하고 구절에서 해당 증거를 강조하는 설명을 제공하는 것입니다. 문장과 질문이 포함된 구절이 주어집니다. 과제는 제공된 선택지 중 하나를 선택하여 질문에 답하는 것입니다. 상식적 추론이 필요한 질문이 주어집니다. 과제는 제공된 선택지 중 하나를 선택하여 질문에 답하는 것입니다. 전제와 가설 문장이 제시됩니다. 과제는 가설 문장이 주어진 전제 문장과 관련하여 함축(암시), 모순(반대), 중립적인지 여부를 판단하는 것입니다. &quot;모순&quot;, &quot;중립&quot; 또는 &quot;함축&quot;으로 답하십시오. 초등학교 수학 단어 문제가 포함된 구절이 제시됩니다. 과제는 일련의 산술 연산을 수행하여 최종 답을 얻어 질문에 답하는 것입니다. 일련의 사실과 규칙이 주어진 다음 질문이 주어집니다. 과제는 &quot;예&quot;, &quot;아니요&quot; 또는 &quot;알 수 없음&quot;을 사용하여 질문에 답하는 것입니다. 문장과 질문이 주어집니다. 필요한 추론 단계는 질문에 암묵적으로 포함되어 있습니다. 과제는 &quot;예&quot; 또는 &quot;아니요&quot;를 사용하여 질문에 답한 다음 여러 단계로 분해하여 답을 설명하는 전략을 제공하는 것입니다. 옵션 -A -B -C -D -E 자유형 텍스트 제공된 선택 사항 중 하나를 선택하세요 제공된 선택 사항 중 하나를 선택하세요 표 6: 각 미세 조정 추론 데이터 세트에 사용된 작업 정의 및 옵션. A 미세 조정 작업 정의 및 옵션 표 6은 추론 데이터 세트에서 OPT 모델을 미세 조정하는 동안 그림에 표시된 템플릿에 대한 입력으로 제공된 작업 정의 및 옵션을 보여줍니다. -모순 -중립 -함축 번호 -예 -아니요 -알 수 없음 -예 -아니요
