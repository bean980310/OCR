--- ABSTRACT ---
언어 모델링 기반 음악 생성에서 생성된 파형은 코드북 패턴에 따라 자동 회귀 방식이나 병렬로 디코딩할 수 있는 계층적 토큰 스택의 시퀀스로 표현됩니다. 특히 코드북을 평탄화하는 것은 가장 높은 품질의 디코딩 전략을 나타내지만, 엄청나게 느립니다. 이를 위해 우리는 플랫 패턴 디코딩을 개선하기 위해 새로운 스택-앤-딜레이 스타일의 디코딩 전략을 제안합니다. 여기서 생성 속도는 바닐라 플랫 디코딩보다 4배 빠릅니다. 이를 통해 추론 시간이 지연 디코딩 전략의 추론 시간과 비슷해지고, 작은 배치 크기에 대해 GPU에서 더 빠른 추론이 가능합니다. 지연 패턴과 동일한 추론 효율 예산의 경우, 제안된 접근 방식이 객관적인 평가에서 더 나은 성능을 보이며, 품질 면에서 플랫 패턴과의 격차를 거의 줄였습니다. 결과는 주관적인 평가에 의해 입증되며, 이는 새로운 모델에서 생성된 샘플이 동일한 텍스트 프롬프트가 주어진 경우 경쟁 모델에서 생성된 샘플보다 약간 더 선호된다는 것을 보여줍니다. 색인 용어 음악 생성, 오디오 생성, 효율적인 디코딩, 변압기 디코더 1.
--- METHOD ---
병렬 디코딩이라고도 할 수 있는 반면 후자는 일반적으로 자기 회귀적입니다. 품질 수준이 원래 노래 수준에 가까워지면서 배치 크기가 일반적으로 작은 개인화된 온디바이스 음악 생성과 같은 새로운 상업적 사용 사례로 나아가는 길이 열리고 있습니다. 그러나 이러한 모델은 종종 품질 상쇄와 함께 제공됩니다. 품질이 높을수록 생성 속도가 느려지고 그 반대의 경우도 마찬가지입니다[3, 6]. 추론 중에 디코딩 전략, 하드웨어 및 모델 크기는 생성 속도에 영향을 미칩니다.[4]는 최근 압축된 이산 음악 표현(즉, 오디오 압축 모델로 계산된 토큰[11])의 시퀀스를 모델링하는 단일 단계 자기 회귀 Transformer 디코더를 제안했습니다. 저자들은 이산 토큰 시퀀스 모델링을 위한 여러 코드북 패턴을 탐구했습니다. 특히, 그들은 가장 성능이 좋은 패턴이 토큰 스택을 평탄화하는 데 의존한다는 것을 보여주었습니다(나머지 논문에서는 이를 플랫 패턴이라고 합니다). 실제로 생성된 파형의 각 부분은 하나의 토큰이 아니라 여러 토큰으로 표현되며, 이는 압축 모델의 Residual Vector Quantizer(RVQ) [12] 모듈에서 잔여 투영의 C 수에 해당합니다.토큰 스택을 평탄화하면 C 배 더 긴 시퀀스를 생성(및 학습)하는 비용이 발생하여 상당히 높은 실시간 계수(RTF)가 발생하여 모델을 실제로 대화형 사용자 경험에 사용할 수 없게 됩니다.이 문제를 극복하기 위해 제안된 지연 패턴[4]은 속도와 품질 간의 좋은 균형으로 나타났습니다.이 논문에서는 지연 패턴이 효율성에도 불구하고 설계상 고품질 샘플을 생성하는 모델의 능력에 영향을 미칠 수 있다는 가설을 세웠습니다.더 강력하지만 느린 플랫 패턴에서 시작하여 훨씬 더 높은 품질로 원래 지연 전략만큼 빠르게 음악을 생성할 수 있는 stackdelay라는 새로운 전략을 제안합니다.이 논문의 기여는 다음과 같습니다.• 추론 중에 이전 키/값 스트리밍 캐시 공간을 줄여 더 빠르고 메모리 효율적이면서도 플랫의 품질을 상속하는 새로운 스택 코드북 패턴. • 새로운 스택 지연 패턴: 생성을 위해 지연 패턴만큼 빠르면서도 스택 패턴의 장점을 활용합니다. - 객관적이고 주관적인 평가를 통해 입증된 것처럼 지연보다 더 높은 품질의 음악을 생성합니다. • 모델이 충분한 컨텍스트를 갖기 전까지 인접한 위치를 디코딩하지 못하도록 하는 디코딩된 위치를 끼워 넣는 새로운 디코딩 일정. 2. 스택 지연 코드북 패턴 2.1. 음악 생성 텍스트 설명이 주어지면 T5 인코더[13]에서 계산한 텍스트 임베딩 시퀀스가 컨디셔닝 신호로 사용됩니다.CCCCo ☐ ☐ ~ 11 12 13 ts ~ 11 12 13 14☐ ~ 11 12 13 14 15to 11 12 13 14 15 16So S1 S2 S3 S4 S5 S6 Sto tits tots ~ 12 t5 ~ to to to tz t5 ~ to to to to to tttག་ ttt3 ttg t12 t7 t10 t13to ttt2 t5 ttttg t†t7 10 13 16 Ct12 t7 410 413 16 Cمی آن آی دی t7 t10 t13to ttt₂ t5 t8 t3 t6 tg t12 t7 t10 t[DELAY to t₁ tt₂ t5 t8 t3 t6 tg t12 tz t10 t[CCto Cto11 to 13 to to t₁₂ t7 totto to t1 tCCtotototo 아닌 Cto t1 tCo to to to to to ti titit Co ☐ to tht₂ t5 t8 t3 t6 tg t12 t7 t10 t13to tits to to to to to to to 12 to 10 13to 1s 13 to toSo S1 S2 S3 S4 S5 S6So S1 SS3 S4 SSS7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 $STACK-DELAY STACK 그림 1. 제안된 스택 지연 패턴(오른쪽)과 지연(왼쪽 위) 및 스택(왼쪽 아래)의 비교. 스택 지연 패턴에서 토큰은 병렬로 다중 스트림 방식으로 생성됩니다. 시간 단계는 순열 방식으로 디코딩됩니다. 최상위 스트림의 키/값 임베딩만 장기 스트리밍 캐시에 저장되므로 스택 패턴에서 더 나은 품질을 유지하면서도 지연만큼 효율적으로 추론할 수 있습니다. C-Transformer 디코더 모델(교차 어텐션 사용). 이 모델은 CNN 디코딩을 통해 오디오 파형으로 디코딩되는 EnCodec [11] 토큰 스택 {Cit}의 시퀀스를 생성합니다. i는 토큰 수준을 나타내고 t는 생성된 시퀀스의 시간 단계를 나타냅니다. 이 논문에서는 이전에 생성된 토큰(Transformer 디코더의 인과적 셀프 어텐션)에 따라 토큰 공간에 대한 확률 분포를 방출하는 자기 회귀 Transformer 디코더 아키텍처[9]만 고려합니다. 추론 중에 과거 셀프 어텐션 키와 값은 생성 시간을 최적화하기 위해 스트리밍 캐시에 저장됩니다.토큰화기 프레임 속도 f(예: f 50Hz), 생성할 오디오의 지속 시간 d 및 토큰 스택 크기 C(예: C = 4)에 따라 모델은 토큰 코드북 패턴과 디코딩 일정에 따라 지정된 양의 디코딩 단계에서 f× C × d 토큰을 생성해야 합니다.디코딩 일정은 각 Cit 2.2에 대한 디코딩 단계를 정의하는 함수 G(i, t)로 공식화할 수 있습니다.코드북 패턴 텍스트 도메인과 달리 오디오 세그먼트는 단일 토큰으로 표현되지 않고 CNN 자동 인코더[11]의 잠재 임베딩을 양자화[12]하여 계산된 계층적 토큰 스택으로 표현됩니다.이는 일반적으로 스택에서 토큰이 낮을수록 더 많은 정보를 전달함을 의미합니다. 계층적 방식으로 토큰을 예측하는 문제를 해결하기 위해 여러 코드북 인터리빙 패턴이 탐색되었습니다[14, 4, 15].가장 낮은 레벨의 토큰을 먼저 디코딩한 다음 더 높은 레벨을 추가 디코딩 단계에서 처리하는 것이 공통적인 아이디어인데, 이는 자기 회귀(AR)[4] 및 비자기 회귀(NAR)[6] 디코딩 아키텍처 모두에 해당합니다.즉, 디코딩 일정은 다음과 같이 제한됩니다.2.2.1. 지연 G(0,t) &lt; G(i,t), Vi Є [1, C[ (1) 음악 생성과 관련하여 지연 인터리빙 패턴(그림 1의 좌측 상단에 표시)은 품질과 AR 디코딩 단계 수 사이에서 좋은 절충안으로 나타났습니다.지연 패턴에서 C 코드북 레벨은 병렬로 예측되지만 디코딩된 시간 단계에서 이동합니다.즉, G(i,t) = t + i. 이는 시퀀스의 각 후속 시간 단계가 이전 인접 시간 단계에 대한 부분적인 지식만으로 디코딩되기 시작한다는 것을 의미합니다.예를 들어, 그림에서 디코딩 단계 s₁에서 cot₁의 예측은 이전에 so에서 디코딩된 Coto에만 조건이 지정되고 시간 단계 to의 더 높은 수준 {c;};--¹에는 조건이 지정되지 않습니다.2.2.2. 스택 C-[4]는 최고의 음악 품질을 얻기 위해 코드북을 평탄화하는 것이 C 배 더 많은 디코딩 단계를 희생하더라도 가장 좋은 성능을 보였다는 것을 보여주었습니다.G(i,t) =Cxt + i &lt; C × T (2) 이는 후속 디코딩된 시간 단계가 이전 단계의 전체 컨텍스트에서 이점을 얻는다는 사실로 쉽게 설명할 수 있습니다. 이 경우 cot+1의 예측은 효과적으로 C[0,C-1][0,t]에 따라 조건지어집니다.컨텍스트 길이는 패턴 디코딩 단계 컨텍스트 길이 지연 플랫 TT Tx C TX C 스택 스택 지연 Tx CT T+CT보다 C 배 더 큽니다.표 1. 추론 중 스트리밍 캐시의 필요한 디코딩 단계 수와 최대 컨텍스트 길이는 T dxf를 생성하는 시퀀스 길이와 지연보다 큰 코드북 수준 C. = 수의 함수입니다.왜냐하면 추론 중 최대 C × T의 과거 Transformer 셀프 어텐션 키/값 표현이 스트리밍 캐시에 저장되기 때문입니다.캐시 크기를 줄이기 위해 그림 1에서 볼 수 있듯이 디코딩 프로세스 전반에 걸쳐 하위 수준 토큰을 유지하고 스태킹하여 플랫 패턴을 적용합니다.주어진 시간 단계에 대해 전체 스택이 디코딩되면 전체 스택에 필요한 모든 정보가 포함되어 있으므로 부분 스택을 스트리밍 캐시에서 지울 수 있습니다.이렇게 하면 최대 캐시 길이가 C×T 대신 C+T만 됩니다.스택 패턴은 추론 동적 캐싱 동작을 시뮬레이션하는 교육 중에 사용자 지정 어텐션 마스크가 필요합니다. 그러나 여전히 지연보다 C배 더 많은 디코딩 단계가 필요합니다.2.2.3. 스택 지연 스택 패턴의 증가된 디코딩 단계 수(즉, 추론 시간)를 보상하기 위해 그림 1의 오른쪽 부분에 표시된 스택 지연 패턴이라고 하는 C 병렬 디코딩 스트림을 도입하기로 제안합니다.C 병렬 스트림이 C배 더 긴 시퀀스를 디코딩한다는 것은 전체적으로 디코딩 단계의 총 수가 지연 패턴(즉, T)과 동일하다는 것을 의미합니다.지연과의 주요 차이점은 더 이상 다른 시간 단계의 토큰을 스택하지 않고 항상 동일한 시간 단계의 토큰을 스택한다는 것입니다.또한 이를 통해 위치 인코딩에서 디코딩된 시간 단계뿐만 아니라 디코딩된 토큰 수준도 인코딩할 수 있으므로 모델에 어떤 시간 단계와 수준이 디코딩될 것인지에 대한 힌트를 제공할 수 있습니다.병렬 최적화된 컴퓨팅 하드웨어를 사용하기 때문에 지연과 동일한 추론 효율성 예산으로 전체 모델 성능이 향상되기를 바랍니다.각 패턴에 대한 디코딩 단계 수와 최대 컨텍스트 길이를 표 1에 보고합니다.2.2.4. 시간 단계 인터리빙 마지막으로 디코딩 일정에 시간 단계 순열을 도입합니다. 디코딩은 자기 회귀적이지만 모델은 시간 단계 순열 순서로 토큰 시퀀스를 예측하도록 학습됩니다. 이는 인접한 시간 단계 디코딩에 대한 더 많은 컨텍스트를 제공하는 것을 목표로 합니다. 이러한 인터리빙 패턴의 예는 그림 1의 오른쪽에 표시되어 있으며, 이는 k = 3인 방정식 3에 정의된 디코딩 일정에 해당합니다. 방정식에 따르면 지연 패턴 디코딩 일정은 k = 1인 경우에 해당합니다. G(i,t) = t+(t mod (k + 1)) × (k − 1) + i (3) 3.
--- EXPERIMENT ---
AL 설정 대부분의 실험 설정은 MusicGen [4]의 설정을 따르며, 자세한 내용은 해당 설정을 참조하세요.3.1. 모델 토크나이저는 CNN 자동 인코더와 파형의 잠재 표현에 적용된 잔여 벡터 양자화 모듈로 구성된 EnCodec 모델[11]입니다.RVQ 모듈은 C = 4 양자화기로 구성되며 각각 코드북 크기가 2048입니다.32kHz 모노포닉 오디오를 20ms(50Hz 프레임 속도)마다 토큰 스택으로 인코딩합니다.Transformer 디코더는 audiocraft¹의 사용자 지정 버전으로 구현된 300M 매개변수로 구성됩니다.최적화된 메모리 풋프린트로 더 빠른 학습과 생성을 위해 Pytorch 2.0² 플래시 어텐션을 사용합니다.모델은 전체 트랙의 30초 무작위 크롭에서 학습됩니다. 모델은 AdamW 옵티마이저, 192의 배치 크기, ẞ₁ = 0.9, 62 = 0.95, 0.1의 분리된 가중치 감소, 그래디언트 클리핑 없음으로 200개 에포크(400k 단계) 동안 학습되었습니다. 학습 시작 시 4000단계의 워밍업이 있는 코사인 학습률 일정이 사용됩니다. 모델은 0.99 감소의 지수 이동 평균으로 학습됩니다. 학습은 24개의 A100 GPU에서 fp16 혼합 정밀도 및 분산 데이터 병렬 처리를 사용합니다. 3.2. 생성 각 디코딩 단계에서 Transformer 디코더는 디코딩 일정에 따라 디코딩할 시간 단계 및 수준에 대한 토큰 공간에 대한 확률 분포를 방출합니다. 토큰은 k = 250개 토큰과 1.0의 온도를 사용하여 상위 k 핵 샘플링을 사용하여 분포에서 샘플링됩니다. 모델의 로짓에서 샘플링할 때 분류기 없는 안내 [16]를 적용하며 안내 척도는 3.0입니다. 기준 모델은 [4]의 지연 코드북 패턴을 사용합니다. 이는 30초 분량의 오디오를 T = 500 자기 회귀 단계로 변환합니다. 텍스트 컨디셔닝의 경우 T5 [13] 텍스트 인코더를 사용합니다. 학습하는 동안 텍스트 조건을 확률 0.1로 삭제합니다. 플랫, 스택 및 스택 딜레이 코드북 패턴을 실험합니다. 3.3. 데이터 20,000시간 분량의 라이선스 음악으로 모델을 학습합니다. 10,000개의 고품질 음악 트랙과 각각 25,000개와 365,000개의 악기 전용 녹음이 있는 ShutterStock 및 Pond5 음악 데이터 컬렉션³입니다. 모든 녹음은 32kHz로 샘플링되며 텍스트 설명이 함께 제공됩니다. 이 모델은 [4]와 다른 도메인 내 분할과 MusicCaps 데이터 세트 [17]에서 평가됩니다. ¹https://github.com/facebookresearch/audiocraft 2https://pytorch.org/ 3 www.shutterstock.com/music 및 www.pond5.com MusicCaps RTF FAD(A100) 도메인 내 패턴 FAD KLD CLAP 지연 플랫 0.0.48 0.4.1.0.42 0.0.5.4.stack 0.0.48 0.5.4.4.1.stack-delay 0.48 0.48 0.표 2. 30초 동안 생성된 트랙에 대한 제안된 토큰 시퀀스 패턴의 품질/효율성 균형. 디코딩 일정 G(i, t) t+i(지연) t+i(스택 지연) t+(t mod 3) × 1 + i 0.0.FAD KLD CLAP 0.45 0.50 0.0.43 0.51 0.0.t+(t mod 4) × 2+ i 0.0.0.t(t mod 5) × 3+ i 0.0.0.표 3. 도메인 내 평가 데이터 세트에서 10초 샘플에 대한 스택 지연 패턴의 디코딩 일정에서 타임스텝을 순열하는 효과에 대한 절제 연구. 3.4. 평가 다양한 모델은 평가 텍스트 프롬프트 목록에서 생성된 샘플 세트를 통해 평가됩니다. 객관적인 평가를 위해 VGG 분류기[18]를 사용하여 Frechet Audio Distance(FAD), PaSST 모델[19]을 사용하여 Kullback-Leibler divergence(KLD), CLAP 유사도 점수[20]를 계산합니다. 주관적인 평가를 위해 동일한 텍스트 프롬프트를 사용하여 두 모델에서 생성한 두 샘플을 평가자에게 제시하는 맹검 쌍별 비교 테스트를 실행합니다. 20개의 텍스트 프롬프트 목록입니다. 인간 평가자에게는 인식된 품질에 따라 각 쌍에서 추출된 샘플을 선택하도록 요청합니다. 마지막으로 한 샘플을 생성할 때 A100 GPU에서 계산된 RTF를 보고합니다(분류기 없는 안내로 인해 모델 관점에서 효과적인 배치 크기 2). 4. 결과 4.1. 기준선 - 플랫 및 지연 패턴 사전 우리는 두 가지 기준선을 고려합니다. 플랫은 지연보다 훨씬 더 많은 컴퓨팅이 필요하지만 최고 품질의 오디오를 생성하는 것으로 알려져 있고, 지연은 속도와 성능의 좋은 절충안으로 RTF를 1에 가깝게 달성하여 스트리밍 시나리오를 잠재적으로 열어줍니다. flat은 0.42의 도메인 내 FAD를 달성하여 delay보다 39% 낮은 반면 KLD와 CLAP은 비슷한 수준을 유지합니다. 더 높은 품질에도 불구하고 RTF는 4 이상입니다. 4.2. 스택 패턴 먼저 스택 패턴을 (지금까지) 최첨단 flat의 대체 패턴으로 조사했습니다. 결과에 따르면 flat과 경쟁력이 있으며 유사한 RTF로 0.38의 FAD 점수를 앞지릅니다. 더 나은 FAD 점수는 생성에 필요한 컨텍스트 길이가 짧을수록 긴 샘플 생성 시 음악 품질에 긍정적인 영향을 미칠 수 있음을 나타냅니다. 4.3. 스택 지연 패턴 스택 지연 패턴을 고려할 때 결과에 따르면 스택에서 낮지는 않지만 delay와 거의 같은 RTF로 훨씬 더 효율적이어서 기준선보다 더 나은 품질의 잠재적 실시간 스트리밍 시나리오를 열어줍니다. 주관적인 평가를 위해 스택 지연과 지연 버전만 비교합니다. 결과는 스택 지연으로 생성된 샘플이 지연에 비해 51.3%의 시간 동안 선호된다는 것을 나타냅니다. 주관적인 평가의 규모가 작기 때문에 이처럼 작은 차이는 예상할 수 있습니다. 4.4. 삭제 - 디코딩된 시간 단계 순열 마지막으로 2.2.4절에서 정의한 인터리브 시간 단계 디코딩 일정을 살펴봅니다. 삭제 결과는 스택 지연 패턴에 적용된 4가지 다른 일정을 비교하고 지연 기준선을 포함하는 표 3에 나와 있습니다. 이 표는 인접한 위치를 구분하는 디코딩 단계 수가 높을수록 FAD가 낮고 KLD 및 CLAP 점수가 비슷한 범위에 있음을 보여줍니다. 이는 스택 지연 패턴에서 시간 단계를 순열하는 이점을 보여줍니다. 순열 없이(즉, 지연과 동일한 오름차순 시간 단계 일정을 따름) 스택 지연 패턴은 미미한 개선만 달성합니다. 또한 동일한 순열 디코딩 일정에 지연 패턴을 적용해 보았는데, 성능은 기준선과 비슷한 수준이었습니다. 즉, 더 나은 성능을 위해서는 제안된 패턴과 순열 디코딩 일정을 결합하는 것이 필수적입니다.
--- CONCLUSION ---
우리는 이산 음악 토큰을 쌓고, 후속 레벨의 디코딩을 지연/이동하고, 디코딩 일정에서 디코딩할 시간 단계의 순서를 순열하는 새로운 코드북 패턴을 소개합니다. 세 가지를 결합하면 동일한 추론 효율 예산에 대해 병렬 디코딩으로 인해 증가된 시퀀스 길이를 보상하여 45%의 도메인 내 FAD 감소로 지연 기준선보다 품질 면에서 성능이 뛰어납니다. 또한 가장 높은 품질이 우선순위일 때 토큰을 평평하게 하는 것보다 토큰을 쌓는 것이 더 선호되어야 함을 보여줍니다. 마지막으로 절제 연구는 시간 단계 순열이 최적의 성능을 달성하는 데 중요하다는 것을 보여주며, 이전 시간 단계에 대한 부분적인 지식만 있는 인접한 위치를 디코딩하면 지연 패턴의 성능에 영향을 미칠 가능성이 있음을 나타냅니다. 전반적으로 우리의 연구 결과가 앞으로 더 나은 비자기 회귀 디코딩 전략을 설계하는 데 도움이 되기를 바랍니다. 6. 참고 자료 [1] Flavio Schneider, Zhijing Jin 및 Bernhard Schölkopf, &quot;Mo\usai: 긴 맥락 잠재 확산을 통한 텍스트-음악 생성&quot;, arXiv 사전 인쇄 arXiv:2301.11757, 2023. [2] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank 등, &quot;Noise2music: 확산 모델을 사용한 텍스트 조건 음악 생성&quot;, arXiv 사전 인쇄 arXiv:2302.03917, 2023. [3] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen 송 등 al., “효율적인 신경 음악 생성,” arXiv 사전 인쇄본 arXiv:2305.15719, 2023. [4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez, “간단하고 제어 가능한 음악 생성,” arXiv 사전 인쇄본 arXiv:2306.05284, 2023. [5] Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, Alex Wang, “Jen-1: 전방위 확산 모델을 사용한 텍스트 기반 범용 음악 생성,” arXiv 사전 인쇄본 arXiv:2308.04729, 2023. [6] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo, “Vampnet: Music generation via 마스크 처리된 음향 토큰 모델링,&quot; arXiv 사전 인쇄본 arXiv:2307.04686, 2023. [7] Prafulla Dhariwal 및 Alexander Nichol, &quot;확산 모델은 이미지 합성에서 간을 이긴다&quot;, 신경 정보 처리 시스템의 발전, vol. 34, pp. 87808794, 2021. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu 및 William T Freeman, &quot;Maskgit: 마스크 처리된 생성 이미지 변환기&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022, pp. 11315-11325. [9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 외, &quot;언어 모델은 비지도 멀티태스크 학습자입니다.&quot; OpenAI 블로그, 1권, 8호, 9쪽, 2019. [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar 외, &quot;Llama: 개방적이고 효율적인 기초 언어 모델&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [11] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi, &quot;고충실도 신경 오디오 압축&quot; arXiv 사전 인쇄본 arXiv:2210.13438, 2022. [12] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund 및 Marco Tagliasacchi, &quot;Soundstream: 종단 간 신경 오디오 코덱&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 거래, 제 12권, 1호(2013년 12월), 1999년 11월. 30, pp. 495–507, 2021. [13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색&quot;, The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [14] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., &quot;신경 코덱 언어 모델은 제로샷 텍스트-음성 합성기&quot;, arXiv 사전 인쇄본 arXiv:2301.02111, 2023. [15] Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour 및 Marco Tagliasacchi, &quot;Soundstorm: 효율적인 병렬 오디오 생성&quot;, arXiv 사전 인쇄 arXiv:2305.09636, 2023. [16] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman 및 Yossi Adi, &quot;Audiogen: 텍스트 안내 오디오 생성&quot;, 학습 표현에 관한 제11차 국제 컨퍼런스, 2022. [17] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi 등 al., &quot;Musiclm: 음악 생성하기 텍스트에서,” arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [18] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al., “대규모 오디오 분류를 위한 CNN 아키텍처,” 2017 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(icassp). IEEE, 2017, pp. 131-135. [19] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh 및 Gerhard Widmer, &quot;패치아웃을 통한 오디오 변압기의 효율적 학습&quot;, arXiv 사전 인쇄본 arXiv:2110.05069, 2021. [20] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail 및 Huaming Wang, &quot;자연어 감독을 통한 박수 학습 오디오 개념&quot;, ICASSP 2023-2023 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2023, pp. 1-5.
