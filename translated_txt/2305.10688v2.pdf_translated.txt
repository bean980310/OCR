--- ABSTRACT ---
생성적 사전 학습된 트랜스포머(GPT)는 자연어 처리에서 큰 성공을 거두었으며 관련 기술은 분자 모델링에 적용되었습니다. 텍스트가 과학적 발견에 가장 중요한 기록이라는 점을 고려하여 이 논문에서는 텍스트로 래핑된 SMILES(분자의 시퀀스 표현)에서 사전 학습된 텍스트와 분자의 통합 언어 모델인 MolXPT를 제안합니다. 간단히 말해, 각 시퀀스에서 분자 이름을 감지하여 해당 SMILES로 바꿉니다. 이런 방식으로 SMILES는 주변 텍스트의 정보를 활용할 수 있으며 그 반대의 경우도 마찬가지입니다. 위의 래핑된 시퀀스, PubMed의 텍스트 시퀀스, PubChem의 SMILES 시퀀스는 모두 사전 학습을 위한 언어 모델에 입력됩니다. 실험 결과에 따르면 MolXPT는 MoleculeNet에서 분자 속성 예측의 강력한 기준선을 능가하고 매개변수의 절반도 사용하지 않으면서 텍스트-분자 번역에서 최상의 모델과 비슷한 성능을 보이며 미세 조정 없이 제로샷 분자 생성이 가능합니다. 1
--- METHOD ---
MolXPT는 과학적 텍스트, SMILES 시퀀스, SMILES와 텍스트 사이의 &quot;래핑된&quot; 시퀀스를 포함한 이기종 데이터에 대해 사전 학습된 언어 모델입니다. 유연한 입력 덕분에 다양한 텍스트 및 분자 작업에 맞게 미세 조정할 수 있습니다. MolXPT의 프레임워크는 그림 1에 나와 있습니다. 2.1 사전 학습 코퍼스 과학적 텍스트의 경우 PubMed¹에서 3,000만 개의 논문 제목과 초록을 사용합니다. 분자 SMILES의 경우 PubChem²에서 3,000만 개의 분자를 무작위로 선택합니다(Kim et al., 2022). 래핑된 시퀀스는 &quot;탐지 및 대체&quot; 파이프라인을 통해 구성됩니다. 먼저 생물의학적 목적으로 널리 사용되는 명명된 엔터티 인식(NER) 도구인 BERN(Sung et al., 2022)을 사용하여 분자에 대한 모든 언급을 탐지하고 ChEBI와 같은 공개 지식 기반의 엔터티에 연결합니다(https://ftp.ncbi.nlm.nih.gov/pubmed/). 2https://pubchem.ncbi.nlm.nih.gov/ (Hastings et al., 2016). 그런 다음, 일치하는 엔터티의 분자 SMILES를 검색할 수 있습니다. 마지막으로, 분자 언급을 해당 SMILES로 바꿉니다. 그림 1의 왼쪽 패널에 예가 나와 있습니다. 래핑된 시퀀스에는 최소한 하나의 분자 SMILES가 포함되어야 합니다. 결국 총 8M개의 래핑된 시퀀스를 얻습니다. 텍스트와 SMILES는 별도로 토큰화됩니다. 텍스트의 경우 바이트 쌍 인코딩(BPE)(Sennrich et al., 2016)을 사용하여 단어를 하위 단어로 분할합니다. BPE 병합 작업의 수는 40k입니다. SMILES 시퀀스(래핑된 시퀀스의 시퀀스 포함)의 경우 (Schwaller et al., 2018)의 정규 표현식으로 토큰화합니다. 각 SMILES 시퀀스 S에 대해 S의 시작 부분에 분자 시작 토큰(som)을 추가하고 S의 끝에 분자 끝 토큰(eom)을 추가합니다. 2.2 모델 및 학습 모델 아키텍처: MolXPT는 GPT 모델과 동일한 아키텍처를 가지고 있습니다(Radford et al., 2019). 계산 리소스 제한으로 인해 이 논문에서는 24개 레이어, 1024개 은닉 크기 및 16개 어텐션 헤드가 있는 GPT-2 매체 구성을 따릅니다. 처리할 수 있는 최대 입력 길이는 2048이고 어휘 크기는 44536입니다. 총 모델은 3억 5천만 개의 매개변수를 가지고 있습니다. 사전 학습: MolXPT의 사전 학습 목적 함수는 음의 로그 가능도입니다. 수학적으로 D {x}i가 세 가지 유형의 데이터 시퀀스 모음을 나타내고 xi = (Si,1, Si,2, ……. Si,n)은 ni 토큰이 있는 i번째 시퀀스입니다. 학습 목적 함수는 다음과 같습니다: min |D| Σ ni i=1 j== &quot; log P(Si,j si j−1, 8i,j−2, ***, 81). P(Si,j|Si,j−1, 사전 훈련 세부 사항은 부록 B에 남아 있습니다. 데이터 세트 BBBP ToxClinTox HIV BACE SIDER Avg #MoleculesG-Contextual 70.3 ± 1.75.2±0.59.9 ± 8.75.9±0.79.2±0.58.4±0.69.G-모티프 66.43.73.2±0.77.8 ± 2.73.81.73.4 ± 4.60.6 ± 1.70.GROVER 베이스 70.0±0.74.3±0.81.23.GROVER 대형 69.5±0.73.5±0.76.2± 3. 그래프MVP 72.4 1.75.9 ± 0. MGSSL 70.5 ± 1. GEM 72.4 ± 0.76.5 ± 0.78.1 ± 0.79.1 2.80.7 ± 2.90.1 ± 1. KV-PLM 74.6 ± 0.72.7 ± 0. 갤럭티카 MoMu MolXPT 66.70.5 ± 2.68.75.6 ± 0.80.0 ± 0.77.1 ± 0.82.79.9 ± 4.95.3 ± 0.62.5 ± 0.68.21.77.01.79.5 1.80.6 ± 0.74.01.74.76.2 ± 0.82.60.64.8±0.72.81.01.81.2±0.79.7±0.85.61.65.4±0.72.63.9±1.74.61.8±0.74.67.2±0.79.61.5±1.61.77.1±1.63.60.5±0.69.73.78.1±0.88.4±1.71.7±0.81.표 1: MoleculeNet의 결과. 평가 지표는 ROC-AUC입니다. 굵은 글꼴은 최상의 결과를 나타냅니다. 프롬프트 기반 미세 조정: MolXPT는 분자 및 텍스트에 대한 다운스트림 작업에 대해 미세 조정할 수 있습니다. 사전 훈련된 백본 모델에 분류 또는 회귀 헤드를 추가하면 사전 훈련과 미세 조정 사이에 격차가 발생합니다(Brown et al., 2020; Chen et al., 2022; Gu et al., 2022). 따라서 우리는 프롬프트 기반 미세 조정(Gao et al., 2021)을 채택하여 다양한 작업을 시퀀스 생성 작업으로 통합하는데, 이는 사전 훈련 목표와 일치합니다. 간단히 말해, 작업이 주어지면 입력과 출력을 텍스트 및/또는 SMILES 시퀀스로 변환하고 시퀀스에 작업별 프롬프트를 장착하고 언어 모델링 손실을 사용하여 미세 조정합니다. MoleculeNet 및 텍스트-분자 번역을 위한 프롬프트는 각각 섹션 3.1과 3.2에 소개되어 있습니다. 논의: 일부 연구에서는 텍스트와 분자를 공동으로 모델링하려고 합니다. Zeng et al.(2022)은 사전 훈련을 위해 분자 이름 뒤에 SMILES 시퀀스를 추가하는 KV-PLM을 제안합니다. Su et al.(2022)은 텍스트와 분자 그래프 간의 대조 학습을 사용합니다. 우리의 MolXPT는 생성 모델인 반면 위의 두 모델은 그렇지 않습니다. 둘 다 과학 문헌을 위한 BERT 모델(Devlin et al., 2019)인 SciBERT(Beltagy et al., 2019)를 기반으로 구축되었습니다. MolXPT는 이를 보완합니다. 3
--- EXPERIMENT ---
모든 결과는 MolXPT가 MoleculeNet에서 분자 속성 예측의 강력한 기준선을 능가하고, 매개변수의 절반도 사용하지 않고도 텍스트-분자 번역에서 최고 모델과 비슷한 성능을 보이며, 미세 조정 없이 제로샷 분자 생성을 가능하게 한다는 것을 보여줍니다.1 서론 GPT-3(Brown et al., 2020) 및 ChatGPT(OpenAI, 2022)와 같은 생성적 사전 학습된 Transformer(GPT)는 자연어 처리에서 큰 성공을 거두었습니다. 이들은 일반적으로 수십억 개의 매개변수를 가지고 있으며 대규모 코퍼스에서 학습됩니다(Taylor et al., 2022; Singhal et al., 2022). 사람들은 그들의 위대한 힘을 목격하면서 언어 모델을 화학(Bagal et al., 2022) 및 생물학적 영역(Ferruz et al., 2022)으로 옮기기 시작했습니다. 예를 들어, 소분자(예: 경구 약물)는 단순화된 분자 입력 라인 입력 시스템(SMILES)(Weininger, 1988)을 사용하여 표현할 수 있는데, 이는 깊이 우선 탐색과 여러 규칙을 사용하여 분자 그래프를 횡단하여 얻은 시퀀스입니다. *동등 기여. 이 작업은 Z. Liu와 W. Zhang이 Microsoft Research AI4Science에서 인턴으로 있을 때 수행되었습니다. *책임 저자. 분기, 방향족성 등에 대해. 분자를 직렬화한 후, 사람들은 SMILES(Bagal et al., 2022; Tong et al., 2021; Frey et al., 2022)에서 언어 모델을 사전 훈련하고 분자 생성에 대한 유망한 결과를 얻습니다. 텍스트는 분자 과학과 더 일반적으로 과학적 발견에 가장 중요한 기록입니다(Beltagy et al., 2019). 분자의 자세한 속성, 예를 들어 분자를 합성하는 방법(Feng et al., 2016), 분자가 독성이 있는지 여부(Juurlink et al., 2003) 등을 설명합니다. BioGPT(Luo et al., 2022)와 PubMedGPT(Bolton et al., 2022)는 생물의학 문헌에서 훈련된 두 가지 언어 모델입니다. 최근 새로운 추세는 SMILES와 과학적 텍스트를 공동으로 모델링하여 두 모달리티에서 공유된 표현을 얻는 것입니다. MolT는 T5와 유사한(Raffel et al., 2020) 모델로, 텍스트/SMILES의 여러 범위가 인코더에서 마스크 처리되고 디코더에서 재구성되어야 합니다. Galactica(Taylor et al., 2022)는 텍스트, SMILES, 단백질 시퀀스 등 다양한 유형의 입력에 대해 사전 학습된 GPT 유사(Brown et al., 2020) 모델입니다. 이러한 모델은 예측 및 생성 작업에서 진전을 보여주지만 분자와 텍스트 간의 관계를 명시적으로 활용하지는 않습니다. 과학 문헌에서 분자 이름이 문장에 나타날 때 주변 맥락은 분자에 대한 설명일 수 있다는 직감이 있습니다. 이는 공동 학습에 유용한 정보여야 하지만 이러한 모델에서는 무시됩니다. 이러한 관계를 활용하기 위해 이 작업에서 &quot;래핑된&quot; 시퀀스에서 학습된 새로운 분자-텍스트 언어 모델(MolXPT)을 제안합니다. 문장이 주어지면 명명된 엔터티 인식 도구로 분자 이름을 감지하고, 있으면 해당 SMILES로 대체하고 SMILES와 텍스트 간의 &quot;래핑된&quot; 시퀀스를 얻습니다. 우리는 8M 래핑 시퀀스와 PubChem의 30M SMILES에서 24-레이어 MolXPT(350M 매개변수 포함)를 사전 훈련했습니다. Rolipram은 실험적 자가면역 뇌척수염 NER 및 엔터티 연결 ChEBI에서 NF-kappaB 활동과 MMP-9 발현을 손상시킵니다. Rolipram: CHEBI id =SMILES=COC1ccc(cc10C1CCCC1) C1CNC(=O)CSMILES로 분자 교체 사전 훈련 코퍼스(som)COC1=C(C=C(C=C1) C2CC(=O) NC2)OC3CCCC3(eom)은 실험적 자가면역 뇌척수염에서 NF-kappaB 활동과 MMP-9 발현을 손상시킵니다. 사전 훈련 PubMed 텍스트(30M 문서) SSą eos 래핑된 시퀀스(8M) PubChem SMILES (30M) $S₁ MOIXPT(24개 레이어, 350M 매개변수) Sn 미세 조정 분자 특성 예측 혈액-뇌 장벽 침투: 진정한 분자-텍스트 번역 Rolipram은 4위치에 3-(시클로펜틸옥시)-4메톡시페닐 치환기를 갖는 피롤리딘-2-온의 Iclass에 속합니다. 그림 1: MolXPT의 프레임워크. MolXPT는 PubMed의 텍스트, PubChem의 SMILES 및 SMILES와 텍스트 사이의 래핑된 시퀀스에서 사전 학습되었습니다. 래핑된 시퀀스는 NER 및 텍스트에 엔터티 링크를 적용한 다음 일치하는 분자 언급을 SMILES로 대체하여 얻습니다. MolXPT는 분자 특성 예측 및 분자-텍스트 번역과 같은 다양한 텍스트 및 분자 다운스트림 작업에 대해 미세 조정할 수 있습니다. (Kim et al., 2022) 및 PubMed(인기 있는 생물의학 문헌 검색 엔진)의 3,000만 개의 제목과 초록. 사전 학습 후, MoleculeNet(분자 속성 예측에 대한 벤치마크)(Wu et al., 2018)에서 MolXPT를 미세 조정하고 프롬프트 기반 미세 조정을 사용하여 분자-텍스트 번역(Edwards et al., 2022)을 미세 조정합니다. MoleculeNet에서 MolXPT는 GEM(Fang et al., 2022)과 같은 정교한 설계를 갖춘 강력한 기준선을 능가합니다. 텍스트-분자 번역에서 MolXPT는 최첨단 모델인 MolT5-large(Edwards et al., 2022)와 비슷한 성능을 보입니다. MolT5-large는 800M 개의 매개변수를 가지고 있는 반면 MolXPT는 매개변수의 44%만 사용합니다. 또한 MolXPT가 텍스트-분자 생성에서 제로샷 기능이 있는지 확인합니다. 2. 방법 MolXPT는 과학적 텍스트, SMILES 시퀀스, SMILES와 텍스트 사이의 &quot;래핑된&quot; 시퀀스를 포함한 이기종 데이터에 대해 사전 학습된 언어 모델입니다. 유연한 입력 덕분에 다양한 텍스트 및 분자 작업에 맞게 미세 조정할 수 있습니다. MolXPT의 프레임워크는 그림 1에 나와 있습니다. 2.1 사전 학습 코퍼스 과학적 텍스트의 경우 PubMed¹에서 3,000만 개의 논문 제목과 초록을 사용합니다. 분자 SMILES의 경우 PubChem²에서 3,000만 개의 분자를 무작위로 선택합니다(Kim et al., 2022). 래핑된 시퀀스는 &quot;탐지 및 대체&quot; 파이프라인을 통해 구성됩니다. 먼저 생물의학적 목적으로 널리 사용되는 명명된 엔터티 인식(NER) 도구인 BERN(Sung et al., 2022)을 사용하여 분자에 대한 모든 언급을 탐지하고 ChEBI와 같은 공개 지식 기반의 엔터티에 연결합니다(https://ftp.ncbi.nlm.nih.gov/pubmed/). 2https://pubchem.ncbi.nlm.nih.gov/ (Hastings et al., 2016). 그런 다음, 일치하는 엔터티의 분자 SMILES를 검색할 수 있습니다. 마지막으로, 분자 언급을 해당 SMILES로 바꿉니다. 그림 1의 왼쪽 패널에 예가 나와 있습니다. 래핑된 시퀀스에는 최소한 하나의 분자 SMILES가 포함되어야 합니다. 결국 총 8M개의 래핑된 시퀀스를 얻습니다. 텍스트와 SMILES는 별도로 토큰화됩니다. 텍스트의 경우 바이트 쌍 인코딩(BPE)(Sennrich et al., 2016)을 사용하여 단어를 하위 단어로 분할합니다. BPE 병합 작업의 수는 40k입니다. SMILES 시퀀스(래핑된 시퀀스의 시퀀스 포함)의 경우 (Schwaller et al., 2018)의 정규 표현식으로 토큰화합니다. 각 SMILES 시퀀스 S에 대해 S의 시작 부분에 분자 시작 토큰(som)을 추가하고 S의 끝에 분자 끝 토큰(eom)을 추가합니다. 2.2 모델 및 학습 모델 아키텍처: MolXPT는 GPT 모델과 동일한 아키텍처를 가지고 있습니다(Radford et al., 2019). 계산 리소스 제한으로 인해 이 논문에서는 24개 레이어, 1024개 은닉 크기 및 16개 어텐션 헤드가 있는 GPT-2 매체 구성을 따릅니다. 처리할 수 있는 최대 입력 길이는 2048이고 어휘 크기는 44536입니다. 총 모델은 3억 5천만 개의 매개변수를 가지고 있습니다. 사전 학습: MolXPT의 사전 학습 목적 함수는 음의 로그 가능도입니다. 수학적으로 D {x}i가 세 가지 유형의 데이터 시퀀스 모음을 나타내고 xi = (Si,1, Si,2, ……. Si,n)은 ni 토큰이 있는 i번째 시퀀스입니다. 학습 목적 함수는 다음과 같습니다: min |D| Σ ni i=1 j== &quot; log P(Si,j si j−1, 8i,j−2, ***, 81). P(Si,j|Si,j−1, 사전 훈련 세부 사항은 부록 B에 남아 있습니다. 데이터 세트 BBBP ToxClinTox HIV BACE SIDER Avg #MoleculesG-Contextual 70.3 ± 1.75.2±0.59.9 ± 8.75.9±0.79.2±0.58.4±0.69.G-모티프 66.43.73.2±0.77.8 ± 2.73.81.73.4 ± 4.60.6 ± 1.70.GROVER 베이스 70.0±0.74.3±0.81.23.GROVER 대형 69.5±0.73.5±0.76.2± 3. 그래프MVP 72.4 1.75.9 ± 0. MGSSL 70.5 ± 1. GEM 72.4 ± 0.76.5 ± 0.78.1 ± 0.79.1 2.80.7 ± 2.90.1 ± 1. KV-PLM 74.6 ± 0.72.7 ± 0. 갤럭티카 MoMu MolXPT 66.70.5 ± 2.68.75.6 ± 0.80.0 ± 0.77.1 ± 0.82.79.9 ± 4.95.3 ± 0.62.5 ± 0.68.21.77.01.79.5 1.80.6 ± 0.74.01.74.76.2 ± 0.82.60.64.8±0.72.81.01.81.2±0.79.7±0.85.61.65.4±0.72.63.9±1.74.61.8±0.74.67.2±0.79.61.5±1.61.77.1±1.63.60.5±0.69.73.78.1±0.88.4±1.71.7±0.81.표 1: MoleculeNet의 결과. 평가 지표는 ROC-AUC입니다. 굵은 글꼴은 최상의 결과를 나타냅니다. 프롬프트 기반 미세 조정: MolXPT는 분자 및 텍스트에 대한 다운스트림 작업에 대해 미세 조정할 수 있습니다. 사전 훈련된 백본 모델에 분류 또는 회귀 헤드를 추가하면 사전 훈련과 미세 조정 사이에 격차가 발생합니다(Brown et al., 2020; Chen et al., 2022; Gu et al., 2022). 따라서 우리는 프롬프트 기반 미세 조정(Gao et al., 2021)을 채택하여 다양한 작업을 시퀀스 생성 작업으로 통합하는데, 이는 사전 훈련 목표와 일치합니다. 간단히 말해, 작업이 주어지면 입력과 출력을 텍스트 및/또는 SMILES 시퀀스로 변환하고 시퀀스에 작업별 프롬프트를 장착하고 언어 모델링 손실을 사용하여 미세 조정합니다. MoleculeNet 및 텍스트-분자 번역을 위한 프롬프트는 각각 섹션 3.1과 3.2에 소개되어 있습니다. 논의: 일부 연구에서는 텍스트와 분자를 공동으로 모델링하려고 합니다. Zeng et al.(2022)은 사전 훈련을 위해 분자 이름 뒤에 SMILES 시퀀스를 추가하는 KV-PLM을 제안합니다. Su et al.(2022)은 텍스트와 분자 그래프 간의 대조 학습을 사용합니다. 우리의 MolXPT는 생성 모델인 반면 위의 두 모델은 그렇지 않습니다. 둘 다 과학 문헌을 위한 BERT 모델(Devlin et al., 2019)인 SciBERT(Beltagy et al., 2019)를 기반으로 구축되었습니다.MolXPT는 이를 보완합니다.3 실험 우리는 두 가지 다운스트림 작업에서 MolXPT를 평가했습니다.(1) MoleculeNet(Wu et al., 2018)에서 분자 속성 예측은 주어진 분자가 특정 속성을 가지고 있는지 예측하는 것입니다.(2) 텍스트 설명과 분자 간 생성(Edwards et al., 2022)은 분자와 텍스트를 모두 고려해야 합니다.이 섹션에서는 작업 정의, 프롬프트 설계 및 결과를 소개하는 데 중점을 두고 자세한 미세 조정 하이퍼 매개 변수는 부록 C에 남겨둡니다.3.1 MoleculeNet의 결과 MoleculeNet(Wu et al., 2018)은 다양한 속성에 대한 700,000개 이상의 화합물을 보유한 널리 사용되는 분자 모델링 벤치마크입니다. 우리는 평가를 위해 BBBP, Tox21, ClinTox, HIV, BACE 및 SIDER인 6개의 분자 분류 작업을 선택했습니다. 자세한 내용은 부록 A에 나와 있습니다. 우리는 GEM(Fang et al., 2022)을 따라 데이터를 스캐폴드를 기반으로 훈련/검증/테스트 세트로 분할합니다. 이러한 작업의 경우 입력은 SMILES이고 출력은 이진 레이블입니다. 미세 조정 전략: 이전의 분자 속성 예측 모델은 주로 SMILES 시퀀스나 분자 그래프를 입력으로 사용했지만, 우리는 &quot;래핑된&quot; 시퀀스를 사용할 수 있습니다. 예를 들어, 한 가지 과제는 분자의 혈액-뇌 장벽 침투(BBBP)를 예측하는 것입니다. 따라서 프롬프트는 &quot;(som) (SMILES) (eom)의 BBB 침투는 (태그)라는 결론을 내릴 수 있습니다.&quot;입니다. 여기서 (SMILES)는 분자 SMILES를 나타내고 (태그)는 분류 결과를 나타냅니다. BBBP 과제의 경우 (태그)를 &quot;참&quot; 또는 &quot;거짓&quot;으로 설계하여 화합물이 BBB를 통과할 수 있는지 여부를 나타냅니다. ... 과제마다 프롬프트가 다르지만(부록 C.1 참조) 모든 과제에 대해 프롬프트의 마지막 토큰에 태그를 넣습니다. (Si,1, Si,2, · · ·, Si‚T;)가 Tį 토큰이 있는 다운스트림 과제의 i번째 래핑된 시퀀스를 나타냅니다. 여기서 si,T₂는 시퀀스의 태그입니다. N 미세 조정을 위한 샘플. 미세 조정 전략은 Nmin N i=log P(Si,T;\Si,
--- CONCLUSION ---
s 및 향후 작업 우리는 과학적 텍스트, 분자 SMILES 및 분자-텍스트에 대해 사전 학습된 생성 모델인 MolXPT를 제안합니다.BLEU-2 BLEU-4 Rouge-1 Rouge-2 Rouge-L METEOR Text2Mol MolT5-small(77M) 0.0.0.0.0.0.0.MolT5-base(250M) 0.0.0.0.0.0.0.0.MolT5-Large(800M) 0.0.0.0.0.0.0.0.MolXPT(350M) 0.0.0.0.0.0.0.0.Text-to-mol Exact↑ MACCS↑ RDK↑ Morgan↑ FCD↓ Text2mol↑ Validity↑ MolT5-small 0.0.0.0.2.0.0.MolT5-medium 0.0.0.0.2.0.0.MolT5-large 0.0.0.0.1.0.0.MolXPT 0.0.0.0.0.0.0.0.표 2: 분자-텍스트(위) 및 텍스트-분자 생성(아래)의 결과. FCD의 경우 작을수록 좋습니다. 나머지 메트릭의 경우 클수록 좋습니다. MolT5 결과는 (Edwards et al., 2022)의 표 1 및 2에서 가져온 것입니다. MolT5 매개변수는 https://github.com/blender-nlp/MolT5에서 가져온 것입니다. 굵은 글꼴은 최상의 결과를 나타냅니다. 제로샷(상위 1) 제로샷(상위 5) 전체 데이터(상위 1) MACCS RDK Morgan 0.540 0.383 0.0.580 0.423 0.0.841 0.746 0.표 3: 제로샷 텍스트-분자 생성. 래핑된 시퀀스. 3억 5천만 개의 매개변수를 사용하여 24층 MolXPT를 훈련합니다. 프롬프트 기반 미세 조정을 통해 MoleculeNet의 강력한 기준선을 개선하고 분자-텍스트 번역에서 최상의 모델과 비슷한 결과를 얻지만 훨씬 적은 매개변수를 사용합니다. 향후 작업을 위해 먼저 더 큰 MolXPT를 훈련하여 다양한 작업과 제로샷/맥락 내(Xie et al., 2022) 학습 능력에 대한 성능을 추가로 검증할 것입니다. 둘째, 분자와 텍스트 간의 상호 작용을 더욱 향상시키는 방법(예: 대조 학습을 사용하여 일관성 향상)을 연구해야 합니다. 셋째, MolXPT를 텍스트 기반 분자 최적화와 같은 다른 분자 및 텍스트 작업에 효과적으로 적용하는 방법은 탐구해야 할 또 다른 방향입니다. 참고문헌 Viraj Bagal, Rishal Aggarwal, PK Vinod, and U. Deva Priyakumar. 2022. Molgpt: Molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064–2076. Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judges. ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 65–72쪽. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. 2019년 자연어 처리 경험적 방법 컨퍼런스 및 제9회 자연어 처리 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 36153620쪽, 중국 홍콩. 계산언어학 협회. 엘리엇 볼튼, 데이비드 홀, 미치히로 야스나가, 토니 리, 크리스 매닝, 퍼시 리앙. 2022. PubMedGPT 2.7B. 톰 브라운, 벤저민 맨, 닉 라이더, 멜라니 수비아, 재러드 D 카플란, 프라풀라 다리왈, 아빈드 닐라칸탄, 프라나브 샤얌, 기리쉬 사스트리, 아만다 애스켈, 산디니 아가왈, 아리엘 허버트-보스, 그레첸 크루거, 톰 헤니건, 리원 차일드, 아디티아 라메시, 다니엘 지글러, 제프리 우, 클레멘스 윈터, 크리스 헤세, 마크 첸, 에릭 시글러, 마테우시 리트윈, 스콧 그레이, 벤저민 체스, 잭 클라크, 크리스토퍼 버너, 샘 맥캔들리시, 알렉 래드포드, 일리아 수츠케버, 다리오 아모데이. 2020. 언어 모델은 몇 번의 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, Yue Zhang. 2022. Adaprompt: prompt 기반 nlp를 위한 적응형 모델 학습. arXiv 사전 인쇄본 arXiv:2202.04824. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학 협회. Joseph L Durant, Burton A Leland, Douglas R Henry, James G Nourse. 2002. 약물 발견에 사용하기 위한 mdl 키의 재최적화. 화학 정보 및 컴퓨터 과학 저널, 42(6):12731280. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Heng Ji. 2022. 분자와 자연어 간 번역. arXiv 사전 인쇄본 arXiv:2204.11817. Carl Edwards, ChengXiang Zhai, Heng Ji. 2021. Text2mol: 자연어 쿼리를 사용한 교차 모달 분자 검색. 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 595-607페이지. Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, Haifeng Wang. 2022. 속성 예측을 위한 기하학 강화 분자 표현 학습. Nature Machine Intelligence, 4(2):127–134. Minghao Feng, Bingqing Tang, Steven H Liang, Xuefeng Jiang. 2016. 약물의 유황 함유 스캐폴드: 의약 화학에서의 합성 및 응용. Current topics in medicine chemistry, 16(11):1200-1216. Noelia Ferruz, Steffen Schmidt, Birte Höcker. 2022. Protgpt2는 단백질 설계를 위한 딥 비지도 언어 모델입니다. Nature Communications, 13(1):4348. Nathan Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor Coley, Vijay Gadepally. 2022. 딥 화학 모델의 신경 스케일링. ChemRxiv. Tianyu Gao, Adam Fisch, Danqi Chen. 2021. 사전 훈련된 언어 모델을 더 나은 few-shot 학습자로 만들기. 제59회 계산 언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 긴 논문)의 진행 상황, 3816-3830쪽. Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang. 2022. Ppt: few-shot 학습을 위한 사전 훈련된 프롬프트 튜닝. 제60회 계산 언어학 협회 연례 회의록(제1권: 긴 논문)의 진행 상황, 8410-8423쪽. Janna Hastings, Gareth Owen, Adriano Dekker, Marcus Ennis, Namrata Kale, Venkatesh Muthukrishnan, Steve Turner, Neil Swainston, Pedro Mendes, Christoph Steinbeck. 2016. Chebi in 2016: 개선된 서비스와 확장되는 대사체 컬렉션. Nucleic Acids Research, 44(D1):D1214-D1219. David N Juurlink, Muhammad Mamdani, Alexander Kopp, Andreas Laupacis, and Donald A Redelmeier. 2003. 약물 독성으로 입원한 노인 환자 간의 약물-약물 상호 작용. Jama, 289(13):1652– 1658. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Leonid Zaslavsky, Jian Zhang, and Evan E Bolton. 2022. PubChem 2023 업데이트. Nucleic Acids Research, 51(D1):D1373-D1380. Diederik P Kingma와 Jimmy Ba. 2015. Adam: 확률적 최적화를 위한 방법. ICLR(포스터)에서. Chin-Yew Lin. 2004. Rouge: 요약 자동 평가 패키지. Text summarization branches out에서, 74-81페이지. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang. 2022. 3D 기하학을 사용한 분자 그래프 표현 사전 학습. International Conference on Learning Representations에서. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu. 2022. BioGPT: 생물의학 텍스트 생성 및 마이닝을 위한 생성적 사전 학습된 변환기. Briefings in Bioinformatics, 23(6). OpenAI. 2022. Chatgpt: 대화를 위한 언어 모델 최적화. 기술 블로그. Kishore Papineni, Salim Roukos, Todd Ward, WeiJing Zhu. 2002. Bleu: 기계 번역의 자동 평가를 위한 방법. ACL, 311-318쪽. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer. 2018. Frechet chemnet distance: 약물 발견 분자의 생성 모델을 위한 메트릭. Journal of chemical information and modeling, 58(9):17361741. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색. Journal of Machine Learning Research, 21(140):1-67. David Rogers and Mathew Hahn. 2010. 확장 연결성 지문. Journal of chemical information and modeling, 50(5):742–754. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. 2020. 대규모 분자 데이터에 대한 자체 감독 그래프 변환기. Advances in Neural Information Processing Systems, 33:12559-12571. Nadine Schneider, Roger A Sayle, and Gregory A Landrum. 2015. Get youratoms in order: An opensource implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):2111-2120. Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. 2018. &quot;found in translation&quot;: 신경 시퀀스-투-시퀀스 모델을 사용하여 복잡한 유기 화학 반응의 결과 예측. Chemical science, 9(28):60916098. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. 하위 단어 단위가 있는 희귀 단어의 신경 기계 번역. Association for Computational Linguistics(제1권: 장문 논문)의 제54회 연례 회의록, 1715-1725쪽, 베를린, 독일. Association for Computational Linguistics. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. 대규모 언어 모델은 임상 지식을 인코딩합니다. arXiv 사전 인쇄본 arXiv:2212.13138. Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, JiRong Wen. 2022. 분자 그래프를 자연어와 연결하는 분자적 멀티모달 기반 모델. arXiv 사전 인쇄본 arXiv:2209.05481. Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, Jaewoo Kang. 2022. Bern2: 고급 신경 생물의학 명명된 엔터티 인식 및 정규화 도구. arXiv 사전 인쇄 arXiv:2201.02080. Ross Taylor, Marcin Kardas, Guillem Cucurulll, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez 및 Robert Stojnic. 2022. Galactica: 과학을 위한 대규모 언어 모델. arXiv 사전 인쇄 arXiv:2211.09085. Xiaochu Tong, Xiaohong Liu, Xiaoqin Tan, Xutong Li, Jiaxin Jiang, Zhaoping Xiong, Tingyang Xu, Hualiang Jiang, Nan Qiao 및 Mingyue Zheng. 2021. 새로운 약물 설계를 위한 생성 모델. 의약화학 저널, 64(19):14011-14027. 데이비드 웨닝거. 1988. Smiles, 화학 언어 및 정보 시스템. 1. 방법론 및 인코딩 규칙 소개. 화학 정보 및 컴퓨터 과학 저널, 28(1):31–36. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, Vijay Pande. 2018. Moleculenet: 분자 기계 학습의 벤치마크. 화학 과학, 9(2):513–530. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma. 2022. 암묵적 베이지안 추론으로서의 맥락 내 학습에 대한 설명. 국제 학습 표현 컨퍼런스에서. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun. 2022. 인간 전문가와 비교할 수 있는 이해력을 갖춘 분자 구조와 생물의학 텍스트를 연결하는 딥 러닝 시스템. Nature Communications, 13(1):862. Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee. 2021. 분자 속성 예측을 위한 모티프 기반 그래프 자기 지도 학습. 신경 정보 처리 시스템의 발전, 34:15870-15882. 부록 A MoleculeNet의 데이터 세트 및 기준선 평가를 위해 MoleculeNet의 다음 작업을 선택했습니다. (1) BBBP에는 혈액-뇌 장벽 침투에 대한 이진 레이블이 있는 화합물이 포함됩니다. (2) Tox21은 12가지 다른 표적에 대한 화합물의 인간 독성을 예측하기 위한 데이터 세트입니다. (3) ClinTox는 FDA에서 승인한 약물과 독성 이유로 임상 시험에 실패한 약물을 포함합니다. (4) HIV는 약물이 HIV 복제를 억제할 수 있는지 예측하는 것을 목표로 합니다. (5) BACE는 인간 ẞ-secretase 1 억제제 세트에 대한 결합 결과를 설명합니다. (6) SIDER에는 27가지 범주의 부작용이 있는 시판 의약품에 사용되는 화합물이 있습니다. 우리는 MolXPT를 다음 기준선과 비교합니다: (1) GROVER는 자기 지도 사전 학습된 그래프 변환기 모델입니다. G-Contextual과 G-Motif는 문맥적 속성 예측 작업과 모티프 예측 작업으로 사전 학습된 두 가지 변형입니다. (2) GraphMVP는 분자의 2D 위상 구조와 3D 기하학적 관점을 모두 사용하는 자기 지도 사전 학습된 GNN 모델입니다. (3) MGSSL은 역합성 기반 알고리즘 BRICS와 추가 규칙을 활용하여 모티프를 찾고 모티프 레이어와 원자 레이어를 결합합니다. (4) GEM은 기하학이 강화된 사전 학습된 GNN 모델입니다. (5) Galactica는 대규모 과학 코퍼스와 SMILES와 같은 많은 자연 시퀀스에서 학습된 GPT와 유사한 모델입니다. 우리는 Galactica-120B의 결과를 보고합니다. (6) KV-PLM은 사전 학습을 위해 SMILES 시퀀스가 분자 이름 뒤에 추가되는 BERT와 유사한 모델입니다. (7) MoMu는 대조 학습을 사용하여 텍스트용 BERT 모델과 분자용 GNN 모델을 공동으로 사전 학습합니다.B 사전 학습 하이퍼 매개변수 MolXPT는 8개의 A100 GPU에서 200k 단계로 사전 학습됩니다. 배치 크기는 GPU당 2048개 토큰입니다. 그래디언트는 업데이트하기 전에 16단계 동안 누적됩니다. 최적화를 위해 Adam(Kingma 및 Ba, 2015) 최적화기를 사용합니다. 최대 학습률은 0.0005이고 워밍업 단계는 20000입니다. 학습률 스케줄러는 역제곱근 감소 스케줄러입니다. 드롭아웃은 0.1입니다.C 다운스트림 작업의 세부 사항 미세 조정 C.1 MoleculeNet 미세 조정을 위한 프롬프트 (1) BBBP: &quot;[som] (SMILES) (eom)의 BBB 침투가 참/거짓이라는 결론을 내릴 수 있습니다.&quot; (2) Tox21: &quot;(som) (SMILES) (eom)의 (대상) 활동 결과가 활성/비활성이라고 결론 내릴 수 있습니다. 여기서 (대상)은 각 하위 작업에 해당하는 수용체 또는 효소를 나타냅니다. 예를 들어 하위 작업 &quot;AR&quot;의 (대상)은 &quot;안드로겐 수용체&quot;입니다.&quot; (3) ClinTox: &quot;(som) (SMILES) (eom)의 임상 시험 독성은 참/거짓이라고 결론 내릴 수 있습니다.&quot; 하위 작업 CT_TOX의 경우 및 &quot;(som) (SMILES) (eom)의 FDA 승인 상태는 참/거짓이라고 결론 내릴 수 있습니다.&quot; 하위 작업 FDA_APPROVED의 경우. (4) HIV: &quot;(som) (SMILES) (eom)의 HIV 복제를 억제하는 능력에 대한 스크리닝 결과는 활성/비활성이라고 결론 내릴 수 있습니다.&quot; (5) BACE: &quot;(som) (SMILES) (eom)의 베타-시크레타제 1에 대한 결합 결과는 참/거짓이라고 결론 내릴 수 있습니다.&quot; (6) SIDER: &quot;(som) (SMILES) (eom)이 (부작용)의 부작용을 가져올 수 있다는 결론은 참/거짓입니다.&quot; 여기서 (부작용)은 각 하위 작업에 해당하는 부작용을 나타냅니다. C.2 MoleculeNet 미세 조정 세부 정보 다음 하이퍼 매개변수를 그리드 검색합니다. 학습률 {3 × 10−5,5 × 10−5}; 드롭아웃 {0.1, 0.3}; 총 에포크 {30, 50}. 모델은 검증 성능에 따라 선택됩니다. C.3 텍스트 분자 생성 미세 조정 세부 정보 텍스트 분자 생성을 위해 MolXPT는 장치당 1024개 토큰과 16개 누적 단계로 하나의 P40 GPU에서 100단계로 미세 조정됩니다. 모델은 100에포크 동안 미세 조정됩니다. 학습률은 0.0001이고 드롭아웃 비율은 [0.1, 0.2, 0.3, 0.4, 0.5]에서 그리드 탐색됩니다. 드롭아웃 비율을 각각 0. 및 0.5로 설정하면 분자-텍스트 생성 및 텍스트-분자 생성에서 가장 우수한 검증 성능을 얻을 수 있습니다. 해당 모델을 테스트에 사용합니다. C.4 MoleculeNet 미세 조정 전략 선택 Eqn.(1) 및 Eqn.(2)에서 두 가지 미세 조정 전략을 제공합니다. 결과는 표 4에 보고되었습니다. 결과는 유사하고 Eqn.(1)이 약간 더 좋습니다. D 제로샷 텍스트-분자 생성 주어진 K 생성된 분자 m1, M2, · · ·‚ Mк 및 참조 분자 m, 상위 K 지문 유사도는 max similarity(m, mi)입니다. iЄ[K] (3) MolXPT는 미세 조정 없이 참조 분자와 정확히 일치할 수 있는 33개의 분자를 생성합니다. 그림 2는 세 가지 경우를 보여줍니다. 입력 텍스트 분자는 세스퀴테르펜 락톤이며 피버퓨(Tanacetum parthenium)의 활성 원리입니다. 비스테로이드성 항염제, 비마취성 진통제, 말초 신경계 약물, 억제제 및 약물 알레르겐 역할을 합니다. 분자는 메발론산의 (R)-거울상 이성질체입니다. (R)-메발로네이트의 공액산입니다. (S)메발론산의 거울상 이성질체입니다. 분자는 우르소콜산의 담즙산 타우린 공액물입니다. 인간 대사산물과 쥐 대사산물 역할을 합니다. 우르소콜산에서 유래합니다. 타우로르소콜레이트의 공액산입니다. 생성된 분자 -OH HO OH 그림 2: 제로샷 텍스트-분자 생성의 예. 우리는 MolXPT가 미세 조정 없이 참조 분자를 성공적으로 생성할 수 있는 세 가지 사례를 무작위로 선택합니다. 데이터 세트 BBBP ToxClinTox Devfull prompt Devtags만 98.8±0.2 78.8±0.1 98.8±0.98.9±0.78.8±0.97.7±0.HIV 82.91.85.3±0.BACE SIDER Avg 78.4±0.67.7±0.84.Testfull prompt 78.1±0.Testtags만 80.0±0.77.2±0.77.10.93.4±0.1 78.1±0.95.3±0.2 78.1±0.75.8±0.87.9±0.88.41.69.4±0.84.70.0±0.71.7±0.2 81.80.표 4: 비교 MoleculeNet의 다양한 미세 조정 전략. &quot;Dev&quot;와 &quot;Test&quot;는 각각 검증 세트와 테스트 세트를 나타냅니다. 아래 첨자는 미세 조정 전체 프롬프트(Eqn.(2)) 또는 태그만(Eqn.(1))을 각각 나타냅니다. 평가 지표는 ROC-AUC입니다.
