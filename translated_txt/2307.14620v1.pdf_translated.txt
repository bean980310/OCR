--- ABSTRACT ---
우리는 포즈된 RGB 이미지를 입력으로 사용하는 실내 3D 감지를 위한 새로운 방법인 NeRF-Det을 제시합니다. 장면 지오메트리를 모델링하는 데 어려움을 겪는 기존의 실내 3D 감지 방법과 달리, 우리의 방법은 엔드투엔드 방식으로 NeRF를 새롭게 사용하여 3D 지오메트리를 명시적으로 추정하고, 이를 통해 3D 감지 성능을 개선합니다. 구체적으로, NeRF의 장면별 최적화와 관련된 상당한 추가 지연 시간을 피하기 위해, 우리는 NeRF-MLP의 일반화 가능성을 향상시키기 위해 충분한 지오메트리 사전을 도입합니다. 또한, 우리는 공유된 MLP를 통해 감지와 NeRF 분기를 미묘하게 연결하여 NeRF를 감지에 효율적으로 적용하고 3D 감지를 위한 지오메트리 인식 체적 표현을 생성합니다. 우리의 방법은 각각 ScanNet 및 ARKITScenes 벤치마크에서 최첨단 기술보다 3.9mAP 및 3.1mAP 더 우수한 성능을 보입니다. 우리는 NeRF-Det이 어떻게 작동하는지 밝히기 위해 광범위한 분석을 제공합니다. 우리의 공동 훈련 설계의 결과로, NeRF-Det은 이 작업은 Chenfeng이 Meta에서 인턴으로 있을 때 수행되었습니다. 장면별 최적화가 필요 없이 객체 감지, 뷰 합성 및 깊이 추정 작업을 위해 보이지 않는 장면으로 일반화할 수 있습니다. 코드는 https://github.com/facebookresearch/NeRF-Det에서 제공됩니다. 1.
--- INTRODUCTION ---
이 논문에서는 포즈를 취한 RGB 이미지를 사용하여 실내 3D 객체 감지 작업에 초점을 맞춥니다. 3D 객체 감지는 로봇 공학 및 AR/VR과 같은 많은 컴퓨터 비전 응용 프로그램의 기본적인 작업입니다. 알고리즘 설계는 입력 센서에 따라 달라집니다. 지난 몇 년 동안 대부분의 3D 감지 작업은 RGB 이미지와 깊이 측정(깊이 이미지, 포인트 클라우드 등)에 중점을 두었습니다. 깊이 센서는 자율 주행과 같은 응용 프로그램에서 널리 채택되고 있지만 비용, 전력 소모 및 폼 팩터 제약으로 인해 대부분의 AR/VR 헤드셋과 모바일 폰에서 쉽게 사용할 수 없습니다. 그러나 깊이 입력을 제외하면 RGB 전용 이미지에서 의미론뿐만 아니라 기본 장면 기하학도 이해해야 하기 때문에 3D 객체 감지가 훨씬 더 어려워집니다. 기하학의 부재를 완화하기 위한 간단한 솔루션 중 하나는 깊이를 추정하는 것입니다. 그러나 깊이 추정 자체는 어렵고 미해결 문제입니다. 예를 들어, 대부분의 단안 깊이 추정 알고리즘은 정확한 메트릭 깊이 또는 다중 뷰 일관성을 제공할 수 없습니다[12, 35, 18, 32]. 다중 뷰 깊이 추정 알고리즘은 텍스처가 있고 가려지지 않은 영역에서만 신뢰할 수 있는 깊이를 추정할 수 있습니다[10, 37]. 또는 ImVoxelNet[34]은 2D 이미지에서 특징을 추출하고 이를 투영하여 3D 볼륨 표현을 구축하여 장면 지오메트리를 암묵적으로 모델링합니다. 그러나 이러한 지오메트리 표현은 본질적으로 모호하고 부정확한 감지로 이어집니다. 반면 Neural Radiance Field(NeRF)[24, 4, 4]는 지오메트리 모델링을 위한 강력한 표현으로 입증되었습니다. 그러나 NeRF를 3D 감지 파이프라인에 통합하는 것은 여러 가지 이유로 복잡한 작업입니다. (i) NeRF를 렌더링하려면 앨리어싱 문제를 방지하기 위해 공간을 고주파로 샘플링해야 하는데[24], 이는 제한된 해상도 볼륨으로 인해 3D 감지 파이프라인에서 어렵습니다. (ii) 기존 NeRF는 장면별로 최적화되며 상당한 지연 시간이 발생하기 때문에 이미지 기반 3D 감지라는 목표와 양립할 수 없습니다. (iii) NeRF는 훈련 중에 기하학을 더 잘 학습하기 위해 다중 뷰 일관성을 최대한 활용합니다. 그러나 먼저 NeRF를 한 다음 인식하는 단순한 스티치[40, 16, 17](즉, 재구성 후 감지)는 감지 파이프라인에 다중 뷰 일관성의 이점을 제공하지 않습니다. 모호한 장면 기하학 문제를 완화하기 위해 NeRF 분기를 3D 감지 파이프라인과 함께 훈련하여 장면 기하학을 불투명도 필드로 명시적으로 모델링하는 NeRF-Det를 제안합니다. 구체적으로, [44, 53]에서 영감을 얻어 광선 샘플을 이미지 평면에 투사하고 저해상도 볼륨이 아닌 고해상도 이미지 특징 맵에서 특징을 추출하여 고해상도 볼륨의 필요성을 극복합니다. 보이지 않는 장면에 대한 NeRF 모델의 일반화를 더욱 향상시키기 위해 NeRF MLP에 대한 입력으로 더 많은 사전 확률로 이미지 특징을 증강하여 NeRF 모델링을 위한 더 구별 가능한 특징을 얻습니다. NeRF-then-perception의 간단한 스티치를 구축하는 이전 작업과 달리, 우리는 밀도 필드를 예측하는 공유 MLP를 통해 NeRF 분기와 감지 분기를 연결하여 NeRF 분기의 그래디언트가 이미지 피처로 역전파되어 훈련 중에 감지 분기에 도움이 되도록 합니다. 그런 다음 볼륨의 균일한 분포를 활용하여 밀도 필드를 불투명도 필드로 변환하고 볼륨 피처와 곱합니다. 이렇게 하면 볼륨 피처의 빈 공간 가중치가 줄어듭니다. 그런 다음 지오메트리 인식 볼륨 피처가 3D 바운딩 박스 회귀를 위해 감지 헤드에 공급됩니다. 추론 중에 NeRF 분기가 제거되어 원래 감지기의 추가 오버헤드가 최소화된다는 점에 주목할 가치가 있습니다. 실험 결과 지오메트리를 불투명도 필드로 명시적으로 모델링하면 훨씬 더 나은 볼륨 표현을 구축하여 3D 감지 성능을 크게 개선할 수 있음을 보여줍니다. 훈련에 깊이 측정을 사용하지 않고도 ScanNet 및 ARKITScenes 데이터 세트에서 각각 3.9 및 3.1 mAP만큼 최첨단 기술을 개선합니다. 선택적으로, 학습을 위해 깊이 측정도 가능한 경우, 추론 모델에 여전히 깊이 센서가 필요하지 않은 상태에서 깊이를 더 활용하여 성능을 개선할 수 있습니다. 마지막으로, 새로운 뷰 합성과 깊이 추정이 우리의 초점이 아니지만, 분석 결과, 우리의 방법은 적절한 새로운 뷰 이미지를 합성하고 장면 최적화 없이 깊이 예측을 수행할 수 있으며, 이는 우리의 3D 볼륨 피처가 장면 지오메트리를 더 잘 표현할 수 있음을 입증합니다. 2.
--- RELATED WORK ---
실내 장면에서의 3D 감지. 3D 감지는 다양한
--- METHOD ---
입력으로 포즈를 취한 RGB 이미지를 사용하여 실내 3D 감지를 위한 것입니다. 장면 지오메트리를 모델링하는 데 어려움을 겪는 기존의 실내 3D 감지 방법과 달리, 저희 방법은 NeRF를 엔드투엔드 방식으로 새롭게 사용하여 3D 지오메트리를 명시적으로 추정하고, 이를 통해 3D 감지 성능을 개선합니다. 특히 NeRF의 장면별 최적화와 관련된 상당한 추가 지연 시간을 피하기 위해 충분한 지오메트리 사전 확률을 도입하여 NeRF-MLP의 일반화 가능성을 향상시킵니다. 또한 공유 MLP를 통해 감지 및 NeRF 분기를 미묘하게 연결하여 NeRF를 감지에 효율적으로 적용하고 3D 감지를 위한 지오메트리 인식 체적 표현을 생성합니다. 저희 방법은 각각 ScanNet 및 ARKITScenes 벤치마크에서 최첨단 기술보다 3.9mAP 및 3.1mAP 더 우수한 성능을 보입니다. 저희는 NeRF-Det의 작동 방식을 밝히기 위해 광범위한 분석을 제공합니다. 저희의 공동 훈련 설계의 결과로 NeRF-Det은 이 작업은 Chenfeng이 Meta에서 인턴으로 일할 때 수행되었습니다. 영어: 장면별 최적화가 필요 없이 객체 감지, 뷰 합성 및 깊이 추정 작업을 위해 보이지 않는 장면으로 일반화합니다. 코드는 https://github.com/facebookresearch/NeRF-Det에서 제공됩니다. 1. 서론 이 논문에서는 포즈를 취한 RGB 이미지를 사용하여 실내 3D 객체 감지 작업에 중점을 둡니다. 3D 객체 감지는 로봇 공학 및 AR/VR과 같은 많은 컴퓨터 비전 응용 프로그램의 기본적인 작업입니다. 알고리즘 설계는 입력 센서에 따라 달라집니다. 지난 몇 년 동안 대부분의 3D 감지 작업은 RGB 이미지와 깊이 측정(깊이 이미지, 포인트 클라우드 등)에 중점을 두었습니다. 깊이 센서는 자율 주행과 같은 응용 프로그램에 널리 채택되었지만 비용, 전력 소모 및 폼 팩터 제약으로 인해 대부분의 AR/VR 헤드셋 및 모바일 폰에서 쉽게 사용할 수 없습니다. 그러나 깊이 입력을 제외하면 RGB 전용 이미지에서 의미론뿐만 아니라 기본 장면 지오메트리도 이해해야 하기 때문에 3D 객체 감지가 훨씬 더 어려워집니다. 지오메트리의 부재를 완화하기 위한 간단한 솔루션 중 하나는 깊이를 추정하는 것입니다. 그러나 깊이 추정 자체는 어렵고 미해결 문제입니다. 예를 들어, 대부분의 단안 깊이 추정 알고리즘은 정확한 메트릭 깊이 또는 다중 뷰 일관성을 제공할 수 없습니다[12, 35, 18, 32]. 다중 뷰 깊이 추정 알고리즘은 텍스처가 있고 가려지지 않은 영역에서만 신뢰할 수 있는 깊이를 추정할 수 있습니다[10, 37]. 또는 ImVoxelNet[34]은 2D 이미지에서 특징을 추출하고 이를 투영하여 3D 볼륨 표현을 구축하여 장면 지오메트리를 암묵적으로 모델링합니다. 그러나 이러한 기하학적 표현은 본질적으로 모호하여 부정확한 감지로 이어집니다. 반면에 Neural Radiance Field(NeRF)[24, 4, 4]는 지오메트리 모델링을 위한 강력한 표현으로 입증되었습니다. 그러나 NeRF를 3D 감지 파이프라인에 통합하는 것은 여러 가지 이유로 복잡한 작업입니다.(i) NeRF를 렌더링하려면 앨리어싱 문제[24]를 방지하기 위해 공간을 고주파로 샘플링해야 하는데, 이는 제한된 해상도 볼륨으로 인해 3D 감지 파이프라인에서는 어렵습니다.(ii) 기존 NeRF는 장면별로 최적화되는데, 이는 상당한 지연 시간으로 인해 이미지 기반 3D 감지라는 목표와 양립할 수 없습니다.(iii) NeRF는 학습 중에 기하학을 더 잘 학습하기 위해 다중 뷰 일관성을 최대한 활용합니다. 그러나 먼저 NeRF를 수행한 다음 인식하는 간단한 스티치[40, 16, 17](즉, 재구성 후 감지)는 감지 파이프라인에 다중 뷰 일관성의 이점을 제공하지 않습니다. 모호한 장면 기하학 문제를 완화하기 위해 NeRF 분기와 3D 감지 파이프라인을 공동으로 학습하여 장면 기하학을 불투명도 필드로 명시적으로 모델링하는 NeRF-Det를 제안합니다. 구체적으로, 우리는 [44, 53]에서 영감을 얻어 레이 샘플을 이미지 평면에 투사하고 저해상도 볼륨이 아닌 고해상도 이미지 피처 맵에서 피처를 추출하여 고해상도 볼륨의 필요성을 극복했습니다. 보이지 않는 장면에 대한 NeRF 모델의 일반화를 더욱 향상시키기 위해 NeRF MLP에 대한 입력으로 더 많은 사전 확률로 이미지 피처를 증강하여 NeRF 모델링을 위한 더 구별 가능한 피처를 얻습니다. NeRF-then-perception의 간단한 스티치를 구축하는 이전 작업과 달리, 우리는 밀도 필드를 예측하는 공유 MLP를 통해 NeRF 분기를 감지 분기와 연결하여 NeRF 분기의 그래디언트가 이미지 피처로 역전파되어 훈련 중에 감지 분기에 도움이 되도록 미묘하게 허용합니다. 그런 다음 볼륨의 균일한 분포를 활용하여 밀도 필드를 불투명도 필드로 변환하고 볼륨 피처와 곱합니다. 이렇게 하면 볼륨 피처의 빈 공간 가중치가 줄어듭니다. 그런 다음 지오메트리 인식 볼륨 피처가 3D 바운딩 박스 회귀를 위해 감지 헤드에 공급됩니다. 추론 중에 NeRF 분기가 제거되어 원래 감지기의 추가 오버헤드가 최소화된다는 점에 유의해야 합니다.
--- EXPERIMENT ---
s는 지오메트리를 불투명도 필드로 명시적으로 모델링함으로써 훨씬 더 나은 볼륨 표현을 구축하고 그로 인해 3D 감지 성능을 크게 개선할 수 있음을 보여줍니다. 학습을 위해 깊이 측정을 사용하지 않으면서 ScanNet 및 ARKITScenes 데이터 세트에서 각각 3.9 및 3.1 mAP만큼 최첨단 기술을 개선합니다. 선택적으로 학습을 위해 깊이 측정도 사용할 수 있는 경우 깊이를 더욱 활용하여 성능을 개선할 수 있지만 추론 모델에는 여전히 깊이 센서가 필요하지 않습니다. 마지막으로, 새로운 뷰 합성 및 깊이 추정이 우리의 초점이 아니지만, 우리의 분석에 따르면 우리의 방법은 합리적인 새로운 뷰 이미지를 합성하고 perscene 최적화 없이 깊이 예측을 수행할 수 있으며, 이는 우리의 3D 볼륨 피처가 장면 지오메트리를 더 잘 표현할 수 있음을 검증합니다. 2. 관련 작업 실내 장면에서의 3D 감지. 3D 감지는 입력에 따라 다양한 방법을 활용하며, 포인트 클라우드[25, 28, 29, 31, 54]와 폭셀 표현[51, 14, 13, 8, 15]에서 큰 성공을 거두었습니다. 3D-SIS[13]는 앵커를 사용하여 색상 및 기하학적 특징으로 융합된 폭셀에서 3D 바운딩 박스를 예측합니다. 널리 사용되는 VoteNet[30]은 포인트 특징에서 바운딩 박스 매개변수를 회귀시키기 위해 허프 투표를 제안합니다. 그러나 깊이 센서는 VR/AR 헤드셋과 같이 엄청난 전력 소모로 인해 많은 장치에서 항상 쉽게 사용할 수 있는 것은 아닙니다. 센서 깊이를 제거하기 위해 Panoptic3D[6]는 예측된 깊이에서 추출된 포인트 클라우드에서 작동합니다. Cube R-CNN[3]은 단일 2D 이미지에서 3D 바운딩 박스를 직접 회귀시킵니다. 비교해 보면, 다중 뷰 접근 방식은 깊이 센서에 의해 제한되지 않고 더 정확합니다. 그러나 현재의 최첨단 멀티뷰 방식[34]은 광선을 따라 픽셀 특징을 복제하여 이미지를 순진하게 융합하는데, 이는 충분한 양의 기하학적 단서를 통합하지 못합니다.이를 해결하기 위해 NeRF를 활용하여 볼륨에 기하학을 내장하여 더 나은 3D 감지를 수행합니다.신경 광도장을 사용한 3D 재구성.신경 광도장(NeRF)[24]은 3년 전에 등장한 획기적인 3D 장면 표현으로, 멀티뷰 이미지에서 3D 기하학을 재구성하는 데 강력한 것으로 입증되었습니다[24, 1, 50, 55, 42].초기 작업[24, 1, 20, 27, 52]은 미분 가능한 체적 렌더링[23]을 사용하여 장면별 밀도 필드를 직접 최적화합니다.나중에 NeuS[43]와 VolSDF[50]는 기하학 표현으로 밀도 대신 SDF를 사용하여 재구성 품질을 개선합니다. 최근 Ref-NeRF[39]는 광택 객체의 더 나은 기하학을 위해 반사 방향을 사용하는 것을 제안합니다.앞서 언급한 장면별 최적화 방법 외에도 IBRNet[44] 및 MVS-NeRF[4]와 같이 여러 장면에서 일반화 가능한 NeRF를 학습하는 것을 목표로 하는 작업도 있습니다.이는 픽셀 정렬된 이미지 피처에 따라 각 3D 위치에서 밀도와 색상을 예측합니다.스캐닝 궤적 이미지의 다중 뷰 이미지 새로운 카메라 뷰의 백본 광선 새로운 뷰 피처 그리드 3D 감지 분기 불투명도 그리드 pos enc G-MLP (σ, ĥ) α 공유 매개변수 pos enc 샘플링된 피처 d G-MLP (σ, ĥ) C-MLP 체적 렌더링 Det 헤드 NeRF 분기 그림 2: NeRF-Det의 프레임워크.우리의 방법은 불투명도 그리드를 추정하여 장면 기하학을 학습하기 위해 NeRF를 활용합니다. 공유 지오메트리-MLP(G-MLP)를 사용하면 감지 분기에서 불투명도 필드를 추정하는 데 NeRF의 이점을 얻을 수 있으므로 자유 공간을 마스크하고 피처 볼륨의 모호성을 완화할 수 있습니다.놀라운 진전으로, 이러한 모든 방법은 새로운 뷰 합성 또는 표면 재구성 중 하나의 작업에만 집중합니다.반대로, 우리는 3D 감지를 개선하기 위해 NeRF를 원활하게 통합하는 새로운 방법을 제안합니다.지각을 위한 NERF.정확한 지오메트리를 포착할 수 있는 NeRF는 분류[17], 분할[40, 56], 감지[16], 인스턴스 분할[19] 및 파노라마 분할[9]과 같은 인식 작업에 점차 통합되었습니다.그러나 대부분[17, 16, 40]은 먼저 NeRF를 사용한 다음 인식하는 파이프라인을 따르는데, 이는 인식 작업에 추가 비용이 발생할 뿐만 아니라 훈련 중에 인식에 이점을 제공하기 위해 체적 렌더링을 충분히 사용하지 않습니다. 또한 [56, 9]은 NeRF가 다중 뷰 일관성과 기하 제약을 보장함으로써 의미적 분할에 대한 레이블 효율성을 크게 개선할 수 있음을 보여줍니다. 제안하는 NeRF-Det 방법은 NERF를 통합하여 3D 감지를 위한 다중 뷰 일관성을 보장합니다. NeRF와 감지를 위한 공동 엔드투엔드 학습을 통해 추론 중에 추가 오버헤드가 발생하지 않습니다. 3. 방법 NeRF-Det이라고 하는 저희 방법은 포즈를 취한 RGB 이미지를 사용하여 이미지 특징을 추출하고 3D 볼륨으로 투영하여 실내 3D 객체 감지를 수행합니다. NERF를 활용하여 2D 관찰에서 장면 기하를 추론합니다. 이를 달성하기 위해 3D 객체 감지와 NERF를 공유 MLP와 얽힙니다. 이를 통해 NeRF의 다중 뷰 제약이 그림 2와 같이 감지를 위한 기하 추정을 향상시킬 수 있습니다. 3.1. 3D 감지 분기 3D 감지 분기에서 포즈를 취한 RGB 프레임을 2D 이미지 백본에 입력하고 이미지를 I¿ Є RHixWix로 표시하고 해당 내재적 행렬과 외재적 행렬을 K € R³×³ 및 Ri Є R³×4로 표시합니다.여기서 i = 1, 2, 3,..., T이고 T는 총 뷰 수입니다.다단계 피처를 융합하고 F₁ = RC×H/4×W/4로 표시되는 고해상도 피처를 사용하여 3D 피처 볼륨을 생성하는 [34]를 따릅니다.각 이미지의 2D 피처를 3D의 해당 위치에 연결하여 3D 피처 볼륨을 생성합니다.z축이 높이를 나타내고 x축과 y축이 두 개의 수평 차원을 나타내는 3D 좌표계를 설정합니다.그런 다음 Nxx Nyx Nz 폭셀로 구성된 3D 그리드를 구축합니다. 좌표 p(x, y, z)를 갖는 각 복셀 중심의 경우, 뷰-i에 투영하여 2D 좌표를 얻습니다. _ T(u&#39;₁, vi, di) = K&#39; × Ri × (p, 1)ª, 여기서 (ui, vi) = (1) (u/di, vi/di)는 뷰-i에 있는 p의 픽셀 좌표입니다. K&#39;는 피처 맵 다운샘플링을 고려한 스케일링된 고유 행렬입니다. 이러한 대응 관계를 구축한 후, 3D 피처를 Vi(p) = interpolate((Ui, Vi), Fi), (2)로 할당합니다. 여기서 interpolate()는 위치 (u¿, v¿)에서 F¿의 피처를 찾습니다. 여기서는 최근접 이웃 보간을 사용합니다. 이미지 경계 밖이나 이미지 평면 뒤에 투사된 P의 경우, 해당 피처를 버리고 Vi(p): 0으로 설정합니다. 직관적으로 이는 광선을 쏘는 것과 같습니다 = 다중 뷰 이미지 기반 샘플링 경계 밖 3D 포인트의 증강 피처 피처 그리드 기반 샘플링 이미지 피처의 평균 이미지 피처의 Var 이미지 색상의 평균 이미지 색상의 Var 그림 3: 다양한 피처 샘플링 전략. 새로운 뷰에서 광선을 제공하면 왼쪽 부분에 표시된 것처럼 광선의 3D 포인트를 다중 뷰 이미지 피처로 투사하고 해당 RGB와 함께 평균/분산을 3D 포인트에 첨부할 수 있습니다. 반면 오른쪽 부분에 표시된 것처럼 피처 그리드에서 피처를 샘플링할 수도 있습니다. 카메라-i의 원점에서 픽셀(ui, vi)까지, 광선에 있는 모든 폭셀에 대해 이미지 피처를 Vi의 폭셀로 분산합니다. 다음으로, ImVoxelNet[34]에서 수행한 것처럼 모든 효과적인 기능을 간단히 평균화하여 다중 뷰 기능을 집계합니다. Mp가 효과적인 2D 투영의 수를 나타내도록 하여 Vavg(p) = Vi(p)/Mp를 계산합니다. 그러나 이런 방식으로 생성된 볼륨 피처는 빈 공간과 다른 기하학적 제약 조건을 고려하지 않고 투영 광선을 &quot;과도하게 채웁니다&quot;. 이로 인해 3D 표현이 모호해지고 감지 오류가 발생합니다. 이 문제를 완화하기 위해 NeRF 분기를 통합하여 감지 분기의 학습 기하학을 개선하는 것을 제안합니다. 3.2. NeRF 분기 피처 샘플링. NeRF[24]는 새로운 뷰 합성을 위한 신경 렌더링 방법입니다. 그러나 이전 NeRF는 1283[24]과 같은 고해상도로 3D 볼륨에서 피처를 샘플링합니다. 3D 감지에서는 앨리어싱 문제가 발생하고 기하학 학습이 저하되는 40×40×16의 낮은 그리드 해상도를 사용합니다. 이를 완화하기 위해 [44, 53]에서 영감을 얻어 그림 3과 같이 일반적으로 크기가 160×120인 고해상도 2D 이미지 피처 맵에서 피처를 샘플링합니다. 구체적으로, 먼저 카메라에서 시작된 광선을 따라 점을 샘플링합니다. 즉, r(t) = o+tx d 여기서 o는 광선 원점이고 d는 시야 방향입니다. 광선에서 샘플링된 좌표 p와 시야 방향 d의 경우 색상 c(p, d)와 밀도 σ(p)를 다음과 같이 계산할 수 있습니다. σ(p), ĥ(p) = G-MLP(Ñ(p), Ŷ(p)), c(p, d) = C-MLP(ĥ(p), d). (3) (4) V(p)는 다중 시점 특징에서 집계 및 증강된 광선 특징이고 y(p)는 [24]와 동일한 위치 인코딩이고 ĥn은 잠재 특징입니다. 첫 번째 MLP는 기하를 추정하기 위해 G-MLP라고 하며 두 번째 MLP는 색상을 추정하기 위해 C-MLP라고 합니다. 활성화의 경우 [24]를 따르고 밀도 σ(p)에 ReLU를 사용하고 색상 c(p, d)에 시그모이드를 사용합니다. 특징 증강. [44, 53]과 이미지 특징을 사용하는 것과 유사하지만, 탐지 분기에서 하는 것처럼 다중 뷰 특징의 특징을 단순히 평균화하여 G-MLP가 다양한 장면에서 정확한 지오메트리를 추정하도록 하는 것은 여전히 어렵습니다.따라서 G-MLP를 최적화하는 데 도움이 되는 더 많은 사전 확률을 통합하는 것을 제안합니다.투영된 특징을 평균화하는 것 외에도 샘플링된 특징을 다중 뷰의 분산 Vvar(p) = ar(p) = Σ(Vi(p) - Vavg(p))2/Mp로 증가시킵니다.색상 특징의 분산은 다중 뷰 스테레오 깊이 추정에서 비용 볼륨으로 널리 사용된 3D 필드의 점유율을 대략적으로 설명할 수 있습니다[49].3D 위치 p가 점유된 경우 장면에 램버시안 표면만 포함되어 있다는 가정 하에 관찰된 특징의 분산은 낮아야 합니다.반면 위치가 자유 공간에 있는 경우 다른 뷰에서 다른 모양이 관찰되므로 색상 특징의 분산이 높아집니다. 순진한 특징의 평균과 비교했을 때 분산은 장면 지오메트리를 추정하기 위한 더 나은 사전 확률을 제공합니다. 미묘한 모양 변화에 불변하도록 훈련된 추출된 딥 특징 외에도, 우리는 또한 레이의 샘플링된 특징으로 픽셀 RGB 값을 증가시킵니다. 이것은 더 나은 모양 모델링을 위해 픽셀 RGB를 MLP의 입력에 첨부하는 IBRNet[44]에서 영감을 얻었습니다. 우리는 딥 특징과 같은 방식으로 픽셀 RGB 값의 평균과 분산을 계산합니다. 전체적으로 증가된 특징 V는 그림 3에 표시된 것처럼 {Vavg, Vvar, RGB avg, RGBvar}의 연결로 표현됩니다. 샘플링된 증가된 특징은 NeRF MLP(식 3)로 전달되어 밀도와 색상을 생성합니다. 우리는 체적 렌더링을 사용하여 최종 픽셀 색상과 깊이, Np Ĉ(r) = Σ Tiαici, D(r) = 를 생성합니다.여기서 Ti = i=exp(-σjt), ai Np ΣΤaiti, (5) i=exp(-σidt), tį는 카메라 사이의 샘플링된 i번째 지점 사이의 거리이고, ɗ는 샘플링된 광선 지점 사이의 거리입니다.3.3. 장면 지오메트리 추정 우리는 불투명도 필드를 사용하여 장면 지오메트리를 모델링합니다.불투명도 필드는 특정 영역에 객체가 존재할 확률을 반영하는 체적 표현입니다.즉, 볼 수 없는 객체가 있는 경우 해당 영역의 불투명도 필드는 1.0이 됩니다.불투명도 필드를 생성하기 위해 NeRF 분기에서 하는 것과 동일한 감지 분기의 기능을 증강하는 프로세스를 따릅니다.이 접근 방식의 핵심 요소는 NeRF 분기에서 학습한 G-MLP를 감지 분기와 공유하는 것입니다.이를 통해 두 가지 중요한 기능이 가능해집니다. 첫째, 공유 G-MLP는 두 가지 분기를 미묘하게 연결하여 NeRF 분기의 그래디언트가 역전파되어 훈련 중에 감지 분기에 도움이 됩니다.둘째, 추론 중에 두 가지 분기의 두 입력이 모두 증강된 특징이기 때문에 3D 감지의 증강된 볼륨 특징을 공유 G-MLP에 직접 입력할 수 있습니다.G-MLP의 출력은 σ(p) G-MLP(V(p), y(p))로 표현된 밀도입니다.여기서 σ(p)는 [0, ∞]입니다.p는 감지 분기의 볼륨에서 각 폭셀의 중심 위치입니다.= 다음으로 밀도 필드를 a(p) = 1 exp(-σ(p) × St)로 불투명도 필드로 변환하는 것을 목표로 합니다.그러나 감지 분기에서 광선 방향을 결정하고 무향 볼륨 내에서 점 거리를 계산할 수 없으므로 St를 계산하는 것은 불가능합니다.여기서 우리는 공간에서 폭셀의 균일한 분포를 미묘하게 활용합니다. 따라서 체적 렌더링 방정식의 St는 상수이므로 취소할 수 있습니다.따라서 불투명도를 얻는 것을 a(p) 1 exp(-σ(p))로 줄일 수 있습니다.불투명도 필드를 생성한 후 3차원 감지를 위한 피처 그리드 Vavg와 곱합니다.이를 a(p) × Vavg(p) = 3.4로 표시합니다.3D 감지 헤드 및 교육 목표 우리의 기하학 인식 체적 표현은 대부분의 감지 헤드에 적합할 수 있습니다.공정한 비교와 단순성을 위해 Im VoxelNet[34]과 동일한 실내 감지 헤드를 사용합니다.여기서 각 객체에 대해 27개의 위치 후보를 선택하고 3개의 합성곱 계층을 사용하여 범주, 위치 및 중심성을 예측합니다.우리는 감지 및 NeRF 분기를 종단 간에 공동으로 교육합니다.테스트 시간에는 NeRF 분기에 대한 장면별 최적화가 수행되지 않습니다. 탐지 분기의 경우, Im VoxelNet [34]에 따라 기준 진실 3D 경계 상자로 학습을 감독합니다. 여기서는 분류 Lels에 대한 초점 손실, 중심성 손실 Lentr, 지역화 손실 Lloc의 세 가지 손실을 계산합니다. NeRF 분기의 경우 광도 손실 Le = ||Ĉ(r) Ĉgt(r)||2를 사용합니다. 깊이 기준 진실을 사용하면 Ld = ||D(r) – Dgt (r)||로 장면 지오메트리의 예상 깊이를 추가로 감독할 수 있습니다. 여기서 D(r)은 Equ. 5로 계산됩니다. 최종 손실 L은 L = Lels + Lentr + Lloc + Lc + Ld로 주어집니다. (6) 학습 중에 선택적으로 깊이를 사용하지만 추론에는 필요하지 않습니다. 또한 학습된 네트워크는 학습 중에 전혀 보이지 않는 새로운 장면으로 일반화할 수 있습니다. 4. 실험 결과 구현 세부 정보. 영어: 저희의 감지 브랜치는 백본, 감지 헤드, 해상도 및 훈련 레시피 등을 포함하여 주로 Im VoxelNet을 따릅니다. 자세한 내용은 보충 자료를 참조하십시오. 저희의 구현은 MMDetection3D[5]에 기반합니다. 저희가 아는 한, 저희는 MMDetection3D에서 NERF를 구현한 최초의 기업입니다. 또한 저희는 전체 ScanNet 데이터 세트에 대해 NeRF 스타일의 새로운 뷰 합성 및 깊이 추정을 수행한 최초의 기업인 반면, 이전 연구는 소수의 장면에서만 테스트했습니다[48, 47]. 코드는 향후 연구를 위해 공개될 예정입니다. 4.1. 주요 결과 정량적 결과. 표에서 볼 수 있듯이 NeRF-Det를 포인트 클라우드 기반 방법[46, 13, 30], RGB-D 기반 방법[13, 11] 및 ScanNet에서 최첨단 RGB 전용 방법인 Im VoxelNet[34]과 비교합니다. 1. ResNet50을 이미지 백본으로 사용하여 NeRF-Det-R50-1x가 ImVoxelNet-R50-1x보다 2.0 mAP 더 우수한 성능을 보이는 것을 관찰했습니다. 그에 더하여 NeRF-Det-R50-1x*로 표시되는 깊이 감독이 있는 NeRF-Det는 RGB 감독만 있는 NeRF-Det-R50-1x에 비해 감지 성능을 0.6 mAP 더 향상시킵니다. 표 1에서 공식 코드에서 ImVoxelNet의 총 학습 반복 횟수를 1x로 표시했습니다. 그러나 1x 설정은 각 장면을 약 36회만 반복하는데, 이는 [24, 44, 1, 20]에서 알 수 있듯이 한 장면을 최적화하는 데 수천 번의 반복이 필요한 NeRF 분기를 최적화하기에 충분하지 않습니다. 따라서 NeRF의 잠재력을 최대한 활용하기 위해 2배의 학습 반복으로 실험을 추가로 수행한 결과, NeRF-Det-R50-2x가 52.0mAP에 도달하여 동일한 설정(ImVoxelNet-R50-2x)에서 ImVoxelNet보다 3.6mAP 더 높은 것을 확인했습니다. 이러한 개선을 얻기 위해 추가 데이터/레이블을 도입하지 않았다는 점은 언급할 가치가 있습니다. NeRF 분기(NeRF-Det-R50-2x+)를 학습하기 위해 깊이 감독을 추가로 사용하면 표 3에 표시된 대로 감지 분기가 mAP@.50에서 1.3만큼 더 개선됩니다. 이는 더 나은 지오메트리 모델링(깊이 감독을 통해)이 3D 감지 작업에 도움이 될 수 있음을 입증합니다. NeRF-Det은 학습 프로세스 중에 깊이 감독을 통합하는 효율적인 방법을 제공하지만 ImVoxelNet에 깊이 감독을 도입하는 것은 어렵습니다. 또한 ResNet50을 ResNet101로 대체하면 ScanNet에서 52.9 mAP@.25를 달성하여 동일한 설정에서 Im VoxelNet보다 3.9포인트 더 우수한 성능을 보입니다. NeRF-Det-R101-2x*는 깊이 감독을 통해 RGB 기반 방법 ImVoxelNet[34]과 포인트 클라우드 기반 방법 VoteNet[30] 간의 격차를 절반(MAP 5 mAP에서)으로 줄입니다. 그 외에도 ARKitScenes에서 실험을 수행합니다(표 2 참조). 3.1 mAP의 개선은 제안하는 방법의 효과를 더욱 잘 보여줍니다. 정성적 결과. 그림 4와 같이 NeRF-Det-R50-2x에서 예측된 3D 바운딩 상자를 장면 메시에 시각화합니다. 제안하는 방법이 exTable 1: ScanNet에서 다중 뷰 RGB 입력을 사용한 3D 감지에서도 정확한 감지 예측을 얻는 것을 관찰했습니다. 표의 첫 번째 블록에는 포인트 클라우드 기반 및 RGBD 기반 방법이 포함되어 있으며 나머지는 다중 보기 RGB 전용 감지 방법입니다. †는 공식 코드를 사용하여 ImVoxelNet[34]을 재생산한 것을 의미합니다. *는 깊이 감독이 있는 NeRF-Det를 나타냅니다. 1x 및 2x는 각각 ImVoxelNet의 원래 반복에 대해 동일한 학습 반복과 두 배의 학습 반복으로 모델을 학습한다는 것을 나타냅니다. 방법 Seg-Cluster [46] Mask R-CNN [11] SGPN [46] 3D-SIS [13] 3D-SIS (w/ RGB) [13] 13.8 11.1 11.5 11.0.12.11.11.16.15.13.26.8.2.2.30.10.27.VoteNet [30] FCAF3D [33] CAGroup3D [41] 89.6 58.92.47.38.1 44.70.61.60.60.4 93.0 95.3 92.69.67.63.cab 침대 의자 소파 테이블 문 바람 bkshf pic cntr 책상 curt 냉장고 showr toil 싱크대 욕실 ofurn MAP@.11.8 13.5 18.9 14.13.7 12.2 12.4 11.2 18.19.5 18.9 16.4 12.15.7 15.4 16.4 16.2 14.19.5 13.7 14.4 14.7 21.6 18.5 25.0 24.5 24.5 16.20.7 31.5 31.6 40.6 31.0.0 17.4 14.1 22.2 0.0 0.0 72.9 52.4 0.0 18.12.8 63.1 66.0 46.0.0 6.9 33.3 2.5 10.4 12.2 74.5 22.9 58.19.8 69.7 66.2 71.8 36.0.0 10.0 46.9 14.1 53.8 36.0 87.6 43.0 84.36.3 87.9 88.7.8 56.1 71.7 47.2 45.4 57.1 94.9 54.7 92.57.2 87.0 95.64.5 29.9 64.3 71.5 60.1 52.4 83.9 99.9 84.7 86.67.3 40.73.0 100.0 79.7 87.13.17.22.7.25.16.40.37.58.65.71.77.0 83.9 69.4 65.66.75.임 VoxelNet-R50-1x 임 VoxelNet-R50-1x NeRF-Det-R50-1x 28.5 84.31.6 81.8 74.4 69.32.7 82.6 74.3 67.6 52.32.7 84.73.1 70.51.32.15.34.2 1.29.7 66.1 23.57.43.92.4 54.1 74.0 34.48.53.29.12.50.34.17.40.1.2.74.6 62.7 52.35.17.48.4 0.NeRF-Det-R50-1x* 임 VoxelNet-R50-2x 34.5 83.6 72.6 71.6 54.NeRF-Det-R50-2x 377 841 745 718 542 342 174 516 04 542 1713 107 545 5540 921 507 738 341 518 (134) NeRF-Det-R50-2x* VoxelNett-R101-2x에서 NeRF-Det-R101-2x NeRF-Det-R101-2x* 36.8 85.0 77.0 73.5 56.9 36.7 14.3 48.1 0.8 49.7 68.3 23.5 54.0 60.0 96.5 49.3 78.4 38.37.6 84.9 76.2 76.7 57.5 36.4 17.8 47.0 2.5 49.2 52.0 29.2 68.2 49.3 97.1 57.6 83.52.9 (+3.9) 35.53.3 (+4.3) 32.6 69.2 12.49.2 67.4 20.49.8 64.6 18.54.31.57.2 41.60.3 48.93.1 55.8 68.2 31.47.30.3 14.8 42.6 0.8 40.8 65.3 18.3 52.2 40.9 90.90.9 52.3 74.0 33.90.7 51.0 76.8 30.53.3 74.9 33.49.5 (+2.0) 50.1 (+2.6) 48. 방법 cab 표 2: ARKITScenes 검증 세트의 &quot;전체 장면&quot; 비교 실험. 냉장고 선반 스토브 침대 싱크대 wshr tolt bthtb 오븐 dshwshr | frplce stool chr tble Im VoxelNet-R50 32.2 34.3 4.2 0.0 64.7 20.5 15.8 68.9.9 | 4.NeRF-Det-R50 36.1 40.7 4.9 0.0 69.3 24.4 17.3 75.14.0 7.80.84.10.10.TV 소파 0.4 5.2 11.6 3.1 35.0.2 4.0 14.2 5.3 44.MAP@.23.26.7 (+3.1) 원시 데이터 예측 실제 결과 원시 데이터 예측 실제 결과 그림 4: NeRF-Det-R50-2x 위에 예측된 3D 바운딩 상자의 정성적 결과. 이 접근 방식은 포즈를 취한 RGB 이미지만 입력으로 사용하고 재구성된 메시는 시각화 목적만을 위한 것입니다. 매우 밀집된 장면(예: 첫 번째 행과 세 번째 행의 왼쪽). 의자는 방 안에 빽빽이 들어차 있고, 일부는 테이블에 끼워져 있어 심하게 가려져 있습니다. 그러나 우리의 방법은 그것들을 정확하게 감지할 수 있습니다.반면에 NeRF-Det는 두 번째 행과 세 번째 행의 오른쪽에 표시된 것처럼 쓰레기통, 의자, 테이블, 문, 창문, 소파 등과 같이 크기가 다른 변형 객체가 있는 여러 스케일도 처리할 수 있습니다.표 3: 장면 지오메트리 모델링의 소거.GT-Depth는 3D 볼륨에 2D 피처를 배치하기 위한 기준 진실 깊이를 나타냅니다.NeuralRecon-Depth는 ScanNetV2에서 사전 학습된 NeuralRecon[38]을 사용하여 깊이를 예측함을 나타냅니다.깊이 예측은 훈련과 추론 모두에 사용됩니다. MAP@.Methods MAP@.Im VoxelNet-R50-2x(기준선) GT-Depth-R50-2x(상한) 48.23.54.5(+6.1) 28.2(+4.5) NeuralRecon-Depth-R50-2x 48.8(+0.4) 21.4(-2.3) Cost-Volume-R50-2x(시그모이드) 49.3(+0.9) 24.4(+0.7) NeRF-Det-R50-2x(NeRF 없음) NeRF-Det-R50-2x NeRF-Det-R50-2x* 49.2(+0.8) 24.6(+0.9) 52.0(+3.6) 26.1(+2.5) 51.8(+3.4) 27.4(+3.7) 장면 기하 모델링 분석. 방법 섹션에서 언급했듯이 불투명도 필드를 학습하여 볼륨 표현의 모호성을 완화합니다. 나아가 표 3에서 깊이 맵을 사용하는 것부터 비용 볼륨[38, 49]까지 다양한 장면 기하 모델링 방법을 탐구합니다. 깊이 맵 사용. 이 실험에서는 훈련과 추론 중에 깊이 맵에 액세스할 수 있다고 가정합니다. 폭셀 피처 그리드를 구축할 때 광선을 따라 모든 지점에 피처를 흩뜨리는 대신 깊이 맵에 따라 단일 폭셀 셀에만 피처를 배치합니다. 직관적으로 이렇게 하면 볼륨 표현의 모호성이 줄어들어 감지 성능이 향상됩니다. 개념 증명으로 먼저 데이터 세트와 함께 제공되는 기준 진실 깊이를 사용합니다. 이는 완벽한 기하 모델링을 제공하므로 NeRF-Det의 상한으로 사용됩니다. 실제로 54.mAP@.25 및 28.2 mAP@.50의 높은 감지 정확도를 달성하여(두 번째 행 참조) 기준선을 6.1 mAP@.25 및 4.5 mAP@.40만큼 개선했습니다. 하지만 실제로는 기준 진실 깊이 맵에 액세스할 수 없습니다. 따라서 대신 NeuralRecon[38]의 기본 기하 재구성을 사용하여 깊이 맵을 렌더링하려고 합니다. 결과는 표 3의 세 번째 행에 나와 있습니다. NeuralRecon의 깊이 추정은 일반 ImVoxelNet과 비교할 때 감지 성능을 2.mAP@.50만큼 크게 저하시키는 것을 관찰하여 깊이 맵의 추정 오류가 감지 파이프라인을 통해 해당 부정확성을 전파한다는 것을 보여줍니다. 비용 볼륨. 다음으로, 우리의 방법을 비용 볼륨 기반 방법[49, 26]과 비교합니다. 비용 볼륨을 계산하는 일반적인 방법은 소스 뷰와 참조 뷰 간의 공분산[49]을 사용하는 것입니다. 이는 여러 입력 뷰의 분산을 계산하는 우리 방법과 유사합니다.[49]에 따라 먼저 여러 3D 합성곱 레이어를 사용하여 비용 볼륨을 인코딩한 다음 시그모이드를 통해 확률 볼륨을 구하고 이를 피처 볼륨 Vavg와 곱합니다.결과는 표 3의 네 번째 행에 있습니다.비용-볼륨 기반 방법이 Im VoxelNet을 각각 0.mAP@.25 및 0.7 mAP@.50만큼 개선하는 것을 볼 수 있습니다.NeRF 분기를 제거하면 우리 방법이 difis를 사용한 비용-볼륨 기반 방법과 매우 유사하다는 점을 언급하는 것이 주목할 만합니다.표 4: 첫 번째 그룹은 NeRF-RPN 학습 세트를 사용하여 표현하고 두 번째 그룹은 ScanNet 학습 세트를 사용하여 표현합니다.지연 시간은 하나의 V100에서 측정됩니다.방법 지연 시간 ~846.8초 AP25 | APNeRF-RPN-R50[16] (NeRF-then-det) 33.13 5.NeRF-Det-R50 (공동 NeRF-and-Det) 35.NeRF-Det-R50+ (공동 NeRF-and-Det) 61.48 25.45 0.554s 7.0.554s 참조는 다음과 같습니다. 1) 평균 볼륨과 색상 볼륨으로 비용 볼륨의 분산을 증가시킵니다. 2) 시그모이드 대신 MLP 및 불투명도 함수를 사용하여 장면 지오메트리를 모델링합니다. 결과는 표 3의 다섯 번째 행에 나와 있습니다. 결과가 비용-볼륨 기반 방법에 매우 가깝고 우리 방법과 비용-볼륨 방법 모두 Im VoxelNet보다 개선되었음을 알 수 있습니다. 깊이와 비용 볼륨을 명시적으로 추정하는 것과 대조적으로 NeRF를 활용하여 장면 지오메트리에 대한 불투명도 필드를 추정합니다. 표 3의 회색 부분에 표시된 대로 NeRF를 사용하여 불투명도 필드를 모델링하면 기준선과 비교하여 성능이 +3.6 mAP@.및 +2.5 mAP@.50만큼 크게 향상됩니다. 깊이 감독을 사용한 후 NeRF-Det는 +3.7 mAP@.50(마지막 행)으로 더 큰 개선을 달성할 수 있습니다. 표 3에서 볼 수 있듯이 NeRF를 사용하여 장면 지오메트리를 모델링하는 방법은 예측된 깊이 또는 비용 볼륨을 사용하는 것보다 더 효과적입니다. NeRF-then-Det 방법과의 비교. 표 4에서 볼 수 있듯이 NeRF-and-Det 공동 방법인 제안된 NeRF-Det를 NeRF-then-Det 방법인 NeRF-RPN[16]과 비교합니다. NeRF-RPN과 ScanNet 학습 세트에 모두 포함되지 않은 4개의 장면을 검증 세트로 선택합니다. 공식 코드를 사용하고 NeRF-RPN의 사전 학습된 모델을 제공하여 AP를 평가합니다. 첫 번째 그룹의 실험은 제안한 조인트-NeRF-and-Det 패러다임이 훨씬 빠른 속도로 first-NeRF-then-Det 방법 NeRF-RPN보다 효과적임을 보여줍니다.표 4의 두 번째 그룹은 우리 모델(표 1의 NeRF-Det-R50-2x)을 직접 사용하면 NeRF-RPN에 비해 엄청난 개선이 있음을 보여줍니다.우리 모델은 대규모 데이터 세트에서 학습되었지만 무거운 오버헤드를 감안할 때 전체 ScanNet(약 1500개 장면)에 NeRF-RPN을 적용하기 어렵기 때문에 이것이 우리의 장점이라고 강조합니다.NeRF 분기는 장면 지오메트리를 학습할 수 있을까요?우리는 더 나은 감지 성능이 더 나은 지오메트리에서 나온다고 가정합니다.이를 확인하기 위해 NeRF 분기의 예측에서 새로운 뷰 합성 및 깊이 추정을 수행합니다.기본 가정은 모델이 장면 지오메트리를 올바르게 추론했다면 새로운 카메라 위치에서 RGB 및 깊이 뷰를 합성할 수 있어야 한다는 것입니다. 먼저 그림 5에서 합성된 이미지와 깊이 추정 중 일부를 시각화합니다.이미지와 깊이 맵 품질 PSNR: 19.PSNR: 28.PSNR: 23.PSNR: 25.PSNR: 20.PSNR: 25.그림 5: NeRF-Det-R50-2x* 위의 새로운 뷰 합성 결과.각 트리플릿 그룹에 대해 왼쪽 그림은 합성된 결과이고 가운데 그림은 기준 진실 RGB 이미지이며 오른쪽 부분은 추정된 깊이 맵입니다.시각화는 훈련 중에는 전혀 볼 수 없는 테스트 세트에서 가져온 것입니다.합당해 보이고 장면 기하학을 잘 반영합니다.정량적으로 IBRNet[44]의 프로토콜을 따라 새로운 뷰 합성과 깊이 추정을 평가합니다.10개의 새로운 뷰를 선택하고 무작위로 근처 소스 뷰 50개를 샘플링합니다. 그런 다음 각 장면에 대한 10개의 새로운 뷰의 평가 결과를 평균하고 마지막으로 표 8에 표시된 대로 검증 세트의 모든 312개 장면에 대한 평균 결과를 보고합니다.이 논문의 주요 초점은 새로운 뷰 합성과 깊이 추정이 아니지만 장면별 학습 없이 새로운 뷰 합성에 대해 평균 20+ PSNR을 달성합니다.깊이 추정의 경우 모든 장면에서 0.756의 RMSE를 달성합니다.이 성능은 최첨단 깊이 추정 방법보다 뒤처지지만 [47]이 ScanNet에서 선택된 장면에 대한 깊이 추정에 Colmap[36]과 vanilla NeRF[24]를 사용하여 평균 RMSE 1.0125와 1.0901을 보고한 것을 알 수 있습니다.이 비교는 우리 방법이 합리적인 깊이를 렌더링할 수 있음을 확인합니다.4.2. 영어: Ablation Study 우리는 다양한 구성 요소가 NeRF-Det의 성능에 어떤 영향을 미치는지에 대한 여러 가지 Ablation Study를 수행하는데, 여기에는 다양한 특징 샘플링 전략, G-MLP 공유 여부, 다양한 손실 및 다양한 특징이 G-MLP에 공급되는 것이 포함됩니다. 게다가, 이 논문에서 새로운 관점 합성이 우리의 초점은 아니지만, NeRF 분기에서 나오는 새로운 관점 합성 결과에 대한 몇 가지 분석을 제공하고, NeRF 분기가 공동 훈련 중에 감지 분기의 영향을 받는 방식을 보여줍니다. Ablation Study의 모든 실험은 NeRF-Det-R50-1x*를 기반으로 합니다. G-MLP 및 특징 샘플링 전략에 대한 Ablation. 3.2절에서 설명한 대로 파이프라인의 핵심 요소는 공유 G-MLP로, 이를 통해 다중 관점 일관성의 제약 조건을 NeRF 분기에서 감지 분기로 전파할 수 있습니다. 표에 표시된 대로 Ablation Study를 수행합니다. 5. 공유 G-MLP가 없으면 성능은 mAP@.25에서 50.1에서 48.1로 크게 떨어지며, 이는 다섯 번째 행 표 5: NeRF 분기에서 광선에 피처를 샘플링하는 다양한 방법 및 G-MLP에 대한 절제 연구에서 보여줍니다.공유 G-MLP 샘플 소스 MAP@.25 MAP@.3D 볼륨 49.24.다중 뷰 2D 피처 50.24.3D 볼륨 48.23.다중 뷰 2D 피처 48.23.표 6: 손실에 대한 절제 연구.광도 손실 깊이 손실 MAP@.MAP@.50.24.49.24.50.24.48.23.표 3. 이 경우 다중 뷰 일관성의 영향은 이미지 백본으로만 전파되어 NeRF로 인한 개선 효과가 상당히 제한됩니다.또한 Sec.에서 언급한 대로 3.2에서, 저해상도 볼륨 피처 대신 다중 뷰 이미지 피처에서 광선을 따라 포인트 피처를 샘플링합니다. 이 절제는 표 3에 나와 있습니다. 공유 G-MLP를 사용하면 두 접근 방식 모두 기준 ImVoxelNet보다 성능이 뛰어나고 이미지 피처에서 샘플링하면 볼륨 피처에서 샘플링하는 것보다 더 나은 성능(+0.in mAP@0.25)을 얻을 수 있습니다. NeRF 분기를 사용하는 새로운 뷰 합성 작업의 경우 이미지 피처에서 샘플링하면 볼륨 샘플링을 사용한 경우 18.93에 비해 PSNR에서 20.51을 달성합니다. NeRF 분기의 성능이 감지 분기의 성능에 비례한다는 사실은 더 나은 NeRF 최적화가 더 나은 감지 결과로 이어질 수 있음을 나타냅니다. 다양한 손실에 대한 절제 연구. 표 6에 나와 있듯이 NeRF 분기에서 다양한 손실이 어떻게 작동하는지 연구합니다. 광도 손실만 사용하면 성능이 표 7: 증강된 피처에 대한 절제 연구의 깊이 감독(세 번째 행)을 순수하게 사용하는 것과 비슷하다는 것을 보여줍니다. 평균 Var Color MAP@.MAP@.50.24.49.24.49.23.표 8: 검출 브랜치가 테스트 세트에서 새로운 뷰 합성(NVS)과 깊이 추정(DE)에 영향을 미치는 방식에 대한 소거.SSIM(NVS) ↑ RMSE(DE)↓↓ 방법 PSNR(NVS) ↑ w/ Det branch w/o Det branch 20.20.0.0.0.0.mAP@.50의 항은 다중 뷰 RGB 일관성이 이미 NeRF 브랜치가 지오메트리를 학습할 수 있도록 충분한 지오메트리 큐를 제공함을 나타냅니다.광도 손실과 깊이 손실을 모두 사용하는 경우 성능을 더욱 개선할 수 있습니다.광도 손실이나 깊이 손실을 사용하지 않는 경우(마지막 행), 성능은 비용-부피 기반 방법의 성능으로 떨어집니다.성능은 1.2 mAP@.25 및 0.5 mAP@.50만큼 떨어지며, 이는 NeRF 브랜치가 더 효과적임을 보여줍니다.다양한 기능에 대한 소거 연구. 그런 다음 표 7에 표시된 대로 다양한 기능이 성능에 어떤 영향을 미치는지 연구합니다. 실험 결과 분산 기능을 도입하면 평균 기능만 사용할 때와 비교하여 성능이 상당히 향상됩니다(0.7 mAP@.및 0.6 mAP@.50 이상). 이는 분산 기능이 실제로 우수한 기하 사전을 제공함을 보여줍니다. 더욱이 이미지 기능을 통합하면 성능이 향상되어 저수준 색상 정보도 우수한 기하 단서를 제공함을 나타냅니다. 새로운 뷰 합성에 영향을 미치는 감지 분기에 대한 절제 연구. 감지 분기가 있거나 없는 대상 뷰와 소스 뷰를 동일하게 유지합니다. 결과는 표 8에 나와 있습니다. NeRF 분기는 장면 기하 모델링을 개선하여 3D 감지를 상당히 개선하지만 감지 분기는 NeRF 분기에 도움이 되지 않습니다. 사실 감지 분기를 비활성화하면 0.43db가 향상됩니다. 감지 분기는 NeRF에 필요한 저수준 세부 정보를 지우는 경향이 있으며 이는 향후 작업에서 해결하고자 하는 사항이라고 가정합니다. 5.
--- CONCLUSION ---
이 논문에서는 NeRF를 사용하여 포즈를 취한 RGB 이미지에서 3D 감지를 위한 지오메트리 인식 체적 표현을 학습하는 새로운 방법인 NeRF-Det을 제시합니다. 미묘한 공유 지오메트리 MLP를 통해 NeRF 분기의 다중 뷰 지오메트리 제약 조건을 3D 감지에 깊이 통합합니다. 장면별 최적화의 큰 오버헤드를 피하기 위해 NeRF-MLP의 일반화 가능성을 향상시키기 위해 증강 이미지 피처를 사전으로 활용하는 것을 제안합니다. 또한 NeRF에서 고해상도 이미지에 대한 필요성을 해결하기 위해 볼륨 대신 고해상도 이미지에서 피처를 샘플링합니다. ScanNet 및 ARKITScene 데이터 세트에 대한 광범위한 실험은 RGB 입력을 사용하여 실내 3D 감지를 위한 최첨단 성능을 달성하여 접근 방식의 효과를 보여줍니다. 특히 NeRF 분기는 보이지 않는 장면에도 잘 일반화된다는 것을 관찰합니다. 나아가, 우리의 결과는 3D 감지를 위한 NERF의 중요성을 강조하고 이 방향에서 최적의 성능을 달성하기 위한 핵심 요소에 대한 통찰력을 제공합니다. 6. 감사의 말 NeuralRecon 실험에 대한 큰 도움을 준 Chaojian Li, NeRF-RPN 실험에 대한 귀중한 조언을 해준 Benran Hu, 통찰력 있는 토론을 해준 Feng (Jeff) Liang과 Hang Gao, 그리고 논문 교정을 도와준 Matthew Yu와 Jinhyung Park에게 진심으로 감사드립니다.참고문헌 [1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan. Mip-nerf: 신경 복사장의 안티앨리어싱을 위한 다중 스케일 표현. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 5855-5864페이지, 2021. 2,[2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, Elad Shulman. Arkitscenes - 모바일 rgb-d 데이터를 사용하여 3D 실내 장면 이해를 위한 다양한 실제 세계 데이터 세트. NeurIPS, 2021.[3] Garrick Brazil, Julian Straub, Nikhila Ravi, Justin Johnson, Georgia Gkioxari. Omni3d: 야생에서 3D 객체 감지를 위한 대규모 벤치마크 및 모델. arXiv 사전 인쇄본 arXiv:2207.10660, 2022.[4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su. Mvsnerf: 다중 시점 스테레오에서 고속 일반화 가능한 광도장 재구성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 14124-14133페이지, 2021.[5] MMDetection3D 기여자. MMDetection3D: 일반 3D 객체 감지를 위한 OpenMMLab 차세대 플랫폼. https://github.com/open-mmlab/ mmdetection 3d, 2020. 5,[6] Manuel Dahnert, Ji Hou, Matthias Nießner, Angela Dai. 단일 RGB 이미지에서 파노라마 3D 장면 재구성. 신경 정보 처리 시스템의 발전, 34, 2021.[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner. Scannet: Richly-annotated 3D reconstructions of indoor scene. CVPR에서, 2017.[8] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, Matthias Nießner. 3D-MPA: 3D 의미적 인스턴스 분할을 위한 다중 제안 집계. CVPR에서, 2020.[9] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao. Panoptic nerf: Panoptic 도시 장면 분할을 위한 3D-2D 레이블 전송. arXiv 사전 인쇄본 arXiv:2203.15224, 2022.[10] Yasutaka Furukawa 및 Carlos Hernández. 다중 뷰 스테레오: 튜토리얼. 컴퓨터 그래픽 및 비전의 기초 및 추세Ⓡ, 9(1-2):1–148, 2015.[11] Kaiming He, Georgia Gkioxari, Piotr Dollár 및 Ross Girshick. 마스크 R-CNN. ICCV, 2017. 5,[12] Derek Hoiem, Alexei A Efros 및 Martial Hebert. 자동 사진 팝업. ACM SIGGRAPH 2005 논문, 577-584페이지.2005.[13] Ji Hou, Angela Dai 및 Matthias Nießner. 3D-SIS: RGB-D 스캔의 3D 의미적 인스턴스 분할. CVPR, 2019. 2, 5, 6,[14] Ji Hou, Angela Dai 및 Matthias Nießner. RevealNet: RGB-D 스캔에서 객체 뒤를 보기. CVPR, 2020.[15] Ji Hou, Benjamin Graham, Matthias Nießner 및 Saining Xie. 대조적인 장면 컨텍스트를 사용하여 데이터 효율적인 3D 장면 이해 탐색. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 15587-15597페이지, 2021.[16] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai 및 Chi-Keung Tang. Nerf-rpn: nerf에서 객체 감지를 위한 일반 프레임워크. arXiv 사전 인쇄본 arXiv:2211.11646, 2022. 2,3,7,[17] Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Chris Choy, Anima Anandkumar, Minsu Cho, Jaesik Park. Perfception: Perception using radiance fields. 36회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스, 2022. 2,[18] Kevin Karsch, Ce Liu, Sing Bing Kang. Depth transfer: 비모수 샘플링을 사용한 비디오에서 Depth 추출. IEEE 패턴 분석 및 머신 인텔리전스 거래, 36(11):2144-2158, 2014.[19] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, Thomas Funkhouser. Panoptic Neural Fields: 의미적 객체 인식 신경 장면 표현. CVPR, 2022.[20] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jurgen Gall, Angjoo Kanazawa, Christoph Lassner. Tava: 템플릿 없는 애니메이션 가능한 체적 액터. arXiv 사전 인쇄본 arXiv:2206.08929, 2022. 2,[21] Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, Zeming Li. Bevstereo: 동적 시간 스테레오를 사용하여 다중 뷰 3D 객체 감지에서 깊이 추정 향상. arXiv 사전 인쇄본 arXiv:2209.10248, 2022.[22] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie. 객체 감지를 위한 피처 피라미드 네트워크. CVPR, 2017.[23] Nelson Max. 직접 볼륨 렌더링을 위한 광학 모델. IEEE 시각화 및 컴퓨터 그래픽스 저널, 1(2):99-108, 1995.[24] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ACM 커뮤니케이션, 65(1):99–106, 2021. 2, 4, 5, 8,[25] Yinyu Nie, Ji Hou, Xiaoguang Han 및 Matthias Nieẞner. Rfd-net: 의미론적 인스턴스 재구성을 통한 포인트 장면 이해. CVPR에서는 2021.[26] 박진형, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris Kitani, Masayoshi Tomizuka, Wei Zhan. 시간이 말해 줄 것입니다: 시간적 다중 뷰 3D 객체 감지를 위한 새로운 전망과 기준선. arXiv 사전 인쇄 arXiv:2210.02443, 2022. 7,[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou 및 Hujun Bao. 동적 인체 모델링을 위한 애니메이션 가능한 신경 복사장. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 14314-14323페이지, 2021.[28] Charles R Qi, Xinlei Chen, Or Litany, Leonidas J Guibas. Imvotenet: 이미지 투표를 사용하여 포인트 클라우드에서 3D 객체 감지 향상. CVPR, 2020.[29] Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas. 포인트 클라우드에서 3D 객체 감지를 위한 딥 허프 투표. ICCV, 2019.[30] Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas. 포인트 클라우드에서 3D 객체 감지를 위한 딥 허프 투표. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 9277-9286페이지, 2019. 2, 5, 6,[31] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J Guibas. rgb-d 데이터에서 3D 객체 감지를 위한 Frustum 포인트넷. CVPR, 2018.[32] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향하여: 제로샷 교차 데이터세트 전송을 위한 데이터세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(3), 2022.[33] Danila Rukhovich, Anna Vorontsova, Anton Konushin. Fcaf3d: 앵커 없는 완전 합성 3D 객체 감지. European Conference on Computer Vision, 477-493페이지. Springer, 2022.[34] Danila Rukhovich, Anna Vorontsova, Anton Konushin. Imvoxelnet: 단안 및 다중 뷰 범용 3D 객체 감지를 위한 이미지 대 폭셀 투영. IEEE/CVF Winter Conference on Applications of Computer Vision의 진행 사항, 2397-2406페이지, 2022. 2, 3, 4, 5, 6,[35] Ashutosh Saxena, Min Sun, Andrew Y Ng. Make3d: 단일 정지 이미지에서 3D 장면 구조 학습. IEEE 패턴 분석 및 머신 인텔리전스 거래, 31(5):824-840, 2008.[36] Johannes Lutz Schönberger 및 Jan-Michael Frahm. 동작에서 구조 재검토. Conference on Computer Vision and Pattern Recognition(CVPR), 2016.[37] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, Richard Szeliski. 다중 시점 스테레오 재구성 알고리즘의 비교 및 평가. IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식(CVPR&#39;06) 컨퍼런스, 1권, 519-528페이지. IEEE, 2006.[38] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, Hujun Bao. NeuralRecon: 단안 비디오에서 실시간으로 일관된 3D 재구성. CVPR, 2021.[39] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, Pratul P Srinivasan. Ref-nerf: 신경 광도장에 대한 구조화된 시점 종속 모양. 2022년 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 5481-5490페이지. IEEE, 2022.[40] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, Daniel Duckworth. Nesf: 3D 장면의 일반화 가능한 의미 분할을 위한 신경 의미 필드. arXiv 사전 인쇄본 arXiv:2111.13260, 2021. 2, 3,[41] Haiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhenguo Li, Liwei Wang. CAGroup3d: 포인트 클라우드에서 3D 객체 감지를 위한 클래스 인식 그룹화. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022.[42] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, Wenping Wang. Neuris: 정규 사전 확률을 사용한 실내 장면의 신경 재구성. arXiv 사전 인쇄본 arXiv:2206.13597, 2022.[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암묵적 표면 학습. arXiv 사전 인쇄본 arXiv:2106.10689, 2021.[44] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser. Ibrnet: 다중 뷰 이미지 기반 렌더링 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4690-4699페이지, 2021. 2, 4, 5, 8,[45] Tai Wang, Jiangmiao Pang, Dahua Lin. 동작에서 깊이를 사용한 단안 3D 객체 감지. Computer Vision-ECCV 2022: 제17회 유럽 컨퍼런스, 텔아비브, 이스라엘, 2022년 10월 23-27일, 회의록, IX부, 386-403페이지. Springer, 2022.[46] Weiyue Wang, Ronald Yu, Qiangui Huang 및 Ulrich Neumann. Sgpn: 3D 포인트 클라우드 인스턴스 분할을 위한 유사성 그룹 제안 네트워크. CVPR, 2018. 5,[47] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu 및 Jie Zhou. Nerfingmvs: 실내 다중 뷰 스테레오를 위한 신경 광도장의 안내 최적화. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 5610-5619페이지, 2021. 5,[48] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli 및 Ulrich Neumann. Point-nerf: 포인트 기반 신경 광도장. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5438-5448페이지, 2022.[49] Jiayu Yang, Wei Mao, Jose M Alvarez, Miaomiao Liu. 다중 뷰 스테레오를 위한 비용 볼륨 피라미드 기반 깊이 추론. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4877-4886페이지, 2020. 4,[50] Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman. 신경 암묵적 표면의 볼륨 렌더링. 신경 정보 처리 시스템의 발전, 34:4805-4815, 2021.[51] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas. GSPN: 포인트 클라우드에서 3D 인스턴스 분할을 위한 생성 모양 제안 네트워크. CVPR, 2019.[52] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa. 신경 복사장의 실시간 렌더링을 위한 Plenoctrees. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 57525761페이지, 2021.[53] Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa. pixelNeRF: 하나 또는 몇 개의 이미지에서 신경 복사장. CVPR, 2021. 2,[54] Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang. H3dnet: 하이브리드 기하학적 기본 요소를 사용한 3D 객체 감지. 유럽 컴퓨터 비전 컨퍼런스, 311-329페이지. Springer, 2020.[55] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye Wu, Lan Xu, et al. 신경 애니메이션 메시를 통한 인간 성능 모델링 및 렌더링. arXiv 사전 인쇄본 arXiv:2209.08468, 2022.[56] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J Davison. 암묵적 장면 표현을 통한 현장 장면 레이블링 및 이해. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 15838-15847페이지, 2021.A. 데이터 세트 및 구현 세부 정보 데이터 세트. 실험은 ScanNetV[7] 및 ARKITScenes 데이터 세트[2]에서 수행됩니다. ScanNetV2 데이터 세트는 약 250만 개의 RGB-D 프레임이 있는 1513개의 복잡한 장면을 포함하고 18개의 객체 범주에 대한 의미 및 인스턴스 분할로 주석이 달린 까다로운 데이터 세트입니다. ScanNetV2는 비모달 또는 지향성 바운딩 박스 주석을 제공하지 않으므로 [13, 30, 34]에서와 같이 대신 축에 맞춰진 바운딩 박스를 예측합니다. 우리는 주로 mAP@.25 및 mAP@.50으로 표시되는 0.25 IoU 및 0.5 IoU 임계값을 사용하여 방법을 평가합니다. ARKITScenes 데이터 세트에는 5000개 이상의 스캔이 있는 약 1.6K개의 방이 포함되어 있습니다. 각 스캔에는 일련의 RGBD 포즈 이미지가 포함됩니다. 실험에서 우리는 저해상도 이미지가 있는 데이터 세트의 하위 집합을 활용합니다. 이 하위 집합에는 841개의 고유한 장면에 대한 2,257개의 스캔이 포함되어 있으며, 스캔의 각 이미지 크기는 256 x 192입니다. 공식 저장소 1에서 제공하는 데이터 세트 설정을 따릅니다. 주로 다음과 같이 0.25 IoU를 사용하여 mAP로 방법을 평가합니다[2]. 감지 분기. ImVoxelNet을 따르고, 주로 FPN을 백본으로 하는 ResNet50을 사용하고 감지 헤드는 각각 분류, 위치 및 중심성을 위한 3개의 3D 합성곱 레이어로 구성됩니다. ARKITScenes에 대한 실험의 경우 회전도 예측합니다. 동일한 크기인 40 x 40 x 16의 폭셀을 사용하며, 각 폭셀은 0.16m, 0.16m, 0.2m의 큐브를 나타냅니다. 그 외에도 https://github.com/apple/ARKitScenes/tree/main/threedod에서 ImVoxelNet과 동일한 훈련 레시피를 유지합니다. 학습하는 동안 기본적으로 ScanNet 데이터 세트의 20개 이미지와 ARKITScenes 데이터 세트의 이미지를 사용합니다.테스트하는 동안 ScanNet 데이터 세트와 ARKITScenes 데이터 세트의 각각 50개 이미지와 100개 이미지를 사용합니다.네트워크는 Adam 최적화 도구로 최적화되었으며 초기 학습률은 0.0002로 설정되었고 가중치 감소는 0.0001이며 12개 에포크 동안 학습되었으며 8번째와 11번째 에포크 이후에는 학습률이 10배로 감소했습니다.NeRF 분기.NeRF 분기에서 2048개 광선은 감독을 위해 10개의 새로운 뷰에서 각 반복에서 무작위로 샘플링됩니다.10개의 새로운 뷰는 학습과 추론 모두에서 감지 분기에 입력된 뷰와 달라야 합니다.근거리-원거리 범위를 (0.2m - 8m)로 설정하고 각 광선을 따라 64개 지점을 균일하게 샘플링합니다. 체적 렌더링 중에 광선의 8개 이상의 점이 빈 공간에 투사되면 이를 버리고 광선 손실을 계산하지 않습니다.기하학 MLP(G-MLP)는 256개의 은닉 유닛과 스킵 연결을 갖춘 4계층 MLP입니다.색상 MLP(C-MLP)는 256개의 은닉 유닛을 갖춘 1계층 MLP입니다.실험은 GPU당 16G 메모리가 있는 8개의 V100 GPU에서 수행되었습니다.학습 중에 각 GPU가 단일 장면을 수행하도록 데이터를 배치했습니다.학습 중에 두 분기는 엔드투엔드 공동 학습됩니다.추론 중에 두 분기 중 하나를 원하는 작업에 유지할 수 있습니다.전체 구현은 MMDetection3D[5]를 기반으로 합니다.B. 새로운 뷰 합성 및 깊이 추정의 평가 프로토콜. 새로운 뷰 합성과 깊이 추정 성능을 평가하기 위해 각 장면의 10개 뷰를 새로운 뷰(IBRNet[44]에서 대상 뷰로 표시)로 무작위로 선택하고, 인근 50개 뷰를 지원 뷰로 선택합니다. 10개 새로운 뷰의 RGB와 깊이를 렌더링하기 위해 새로운 뷰의 픽셀에서 나온 각 점을 모든 지원 뷰로 투영하여 피처를 샘플링한 다음 NeRF MLP로 투영합니다(방법론 설명 참조). 본문의 표 6에 있는 두 설정 모두에 대해 동일한 새로운 뷰와 지원 뷰를 유지합니다. 평가는 학습 중에는 전혀 볼 수 없는 ScanNet 데이터 세트의 테스트 세트에서 수행됩니다. 또한, 비사소한 결과는 제안된 지오메트리 인식 체적 표현의 생성 가능성을 보여줍니다. C. 추가 결과 뷰 수에 대한 소거 연구. 표 9에 표시된 대로 뷰 수가 3D 감지 성능에 어떤 영향을 미치는지 분석을 수행했습니다. 구체적으로 동일한 수의 학습 이미지(20개 이미지)를 사용했습니다. 표 9: 뷰 수에 대한 소거. GPU 메모리 제한으로 인해 100개 뷰에서 실험을 수행할 때 이미지 해상도를 2배로 다운샘플링합니다(Im VoxelNet-R50-2x&#39; 및 NeRF-Det-R50-2x&#39;로 표시됨). 각 설정에 대한 실험은 3번 실행됩니다. 실험의 평균과 표준 편차를 보고합니다. 방법 Im VoxelNet-R50-2x (10회 조회) Im VoxelNet-R50-2x (20회 조회) Im VoxelNet-R50-2x (50회 조회) Im VoxelNet-R50-2x&#39; (100회 조회) NeRF-Det-R50-2x (10회 조회) NeRF-Det-R50-2x (20회 조회) NeRF-Det-R50-2x (50회 조회) NeRF-Det-R50-2x&#39; (100회 조회) MAP@.mAP@.37.8±1.17.5±1.46.5±0.21.1±0.48.4±0.23.7±0.48.1±0.41.4±1.0 (+3.6) 50.2±0.5 (+3.7) 24.7±0.19.2±0.9 (+1.7) 51.8±0.2 (+3.4) 52.2±0.1 (+4.1) 23.6±0.4 (+2.5) 26.0±0.1 (+2.3) 27.4±0.1 (+2.7) 다양한 개수의 이미지로 테스트했습니다. 제안한 NeRF-Det-R50-2x는 뷰 수가 증가함에 따라 성능이 크게 향상되었습니다. 반면 ImVoxelNet-R50-2x의 성능은 개선이 제한적이었고, 더 나쁜 것은 뷰 수가 100개로 늘어나자 성능이 감소했습니다. NeRF-Det의 성능 향상은 효과적인 장면 모델링에 기인합니다. NeRF는 뷰 수가 증가함에 따라 성능이 더 좋으며, 일반적으로 객체에 대해 100개 이상의 뷰가 필요합니다[24]. 제안된 NeRF-Det는 이러한 이점을 계승하여 100개 뷰에서 4.1 mAP@.및 2.7 mAP@.50의 극적인 성능 향상을 가져왔습니다.전반적으로, 우리의 분석은 3D 감지를 위한 다중 뷰 관찰을 활용하는 데 있어서 제안된 NeRF-Det의 효과성과 장면 지오메트리를 효과적으로 모델링할 수 있는 방법을 활용하는 것의 중요성을 보여줍니다.더욱 정성적인 결과그림 6에서 볼 수 있듯이, 새로운 뷰 합성과 깊이 추정의 더 많은 시각화 결과를 제공합니다.결과는 ScanNet의 테스트 세트에서 나왔습니다.제안된 방법이 테스트 장면에서 잘 일반화되는 것을 볼 수 있습니다.놀랍게도, 비교적 어려운 사례에서 사소하지 않은 결과를 얻습니다.예를 들어, 두 번째 행의 왼쪽에는 다채로운 책이 가득한 책장이 표시되어 있으며, 우리의 방법은 합리적인 새로운 뷰 합성 결과를 제공할 수 있습니다.반면에 다섯 번째 행의 왼쪽의 경우 장면에 매우 빽빽한 의자가 배열되어 있으며, 이 방법이 정확한 지오메트리를 예측할 수 있음을 관찰할 수 있습니다. D. 야외 3D 감지에 대한 논의 우리는 야외 장면에서 NeRF-Det와 다른 3D 감지 작업의 차이점을 강조합니다. 제안하는 NeRFDet은 기하학적 인식 표현을 학습하려는 [26, 45, 21]과 같은 많은 야외 3D 감지 작업과 유사한 직관을 공유합니다. 그러나 제안하는 NeRF-Det과 다른 작업은 본질적으로 다릅니다. 야외 3D 감지 작업 [26, 45, 21]은 비용 볼륨 또는 명시적으로 예측된 깊이를 사용하여 장면 기하학을 모델링하는 것을 제안합니다. 대신 NeRF-Det는 PSNR: 25.PSNR: 29.PSNR: 19.SNR: 21.PSNR: 22.PSNR: 26.PSNR: 25.PSNR: 21.SK 20.PSNR: 22.PSNR: 20의 불일치를 활용합니다.그림 6: NeRF-Det-R50-2x* 위에 있는 새로운 뷰 합성 결과. 각 트리플릿 그룹의 경우 왼쪽 그림은 합성된 결과이고 가운데 그림은 기준 진실 RGB 이미지이며 오른쪽 부분은 추정된 깊이 맵입니다. 시각화는 훈련 중에 결코 볼 수 없는 테스트 세트에서 가져온 것입니다. 다중 뷰 관찰, 즉 방법 섹션에서 증강된 분산 특징을 NeRF-MLP 입력의 사전 확률로 사용합니다. 비용 볼륨을 넘어, 우리는 사진 사실주의 원리를 활용하여 밀도 필드를 예측한 다음 불투명도 필드로 변환합니다. 이러한 기하적 표현은 3D 감지 작업에서는 새로운 것입니다. 실험 부분의 분석은 또한 제안된 불투명도 필드의 장점을 보여줍니다. 장면 기하를 모델링하는 다른 방법 외에도 NeRF와 3D 감지를 엔드투엔드 방식으로 결합하는 설계를 통해 NeRF의 그래디언트를 역전파하여 3D 감지 분기에 이점을 제공할 수 있습니다. 이는 이전의 NeRF-then-perception 작업[16, 40]과도 다릅니다. NeRF-Det는 대부분 정적 객체인 실내 장면에서 3D 감지를 위해 특별히 설계되었습니다. 실외 장면은 움직이는 객체, 무제한 장면 볼륨, NeRF 학습을 안내하는 데 사용되는 RGB 값의 정확도에 영향을 미칠 수 있는 빠르게 변화하는 조명 조건으로 인해 다중 뷰 일관성을 보장하는 데 어려움을 포함하여 고유한 과제가 있습니다. 이러한 문제를 해결하고 향후 작업에서 NeRF-Det를 실외 3D 감지에 적용할 계획입니다.
