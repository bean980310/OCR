--- ABSTRACT ---
대규모 언어 모델(LLM)은 최근 인간의 의도를 이해하고 추론에 참여하며 계획과 같은 행동을 설계하는 놀라운 능력을 보여주었습니다. 복잡한 작업을 수행하기 위한 LLM의 힘을 더욱 발휘하기 위해 ChatGPT와 같은 LLM에 방대한 외부 API에 연결할 수 있는 도구 사용 기능을 제공하는 에이전트 프레임워크를 구축하려는 추세가 커지고 있습니다. 이 연구에서는 오픈소스 LLM을 컨트롤러로 사용하는 실제 애플리케이션을 위한 일반적이고 사용자 정의 가능한 에이전트 프레임워크인 ModelScope-Agent를 소개합니다. 사용자 친화적인 시스템 라이브러리를 제공하며, 여러 오픈소스 LLM에 대한 모델 학습을 지원하는 사용자 정의 가능한 엔진 설계를 제공하는 동시에 모델 API와 일반 API를 통합된 방식으로 원활하게 통합할 수 있습니다. LLM에 도구 사용 기능을 제공하기 위해 도구 사용 데이터 수집, 도구 검색, 도구 등록, 메모리 제어, 사용자 정의 모델 학습 및 실제 애플리케이션에 대한 평가에 걸친 포괄적인 프레임워크가 제안되었습니다. 마지막으로, ModelScope-Agent 프레임워크를 기반으로 하는 ModelScope Community의 실제 지능형 어시스턴트인 ModelScopeGPT를 선보입니다. 이 어시스턴트는 오픈소스 LLM을 ModelScope의 1000개 이상의 공개 AI 모델과 지역화된 커뮤니티 지식에 연결할 수 있습니다. ModelScope-Agent 라이브러리¹와 온라인 데모²가 이제 공개적으로 제공됩니다. 1
--- INTRODUCTION ---
대규모 언어 모델(OpenAI, 2022, 2023; Touvron et al., 2023; Chowdhery et al., 2022)은 점차 인간의 의도를 이해하고, 복잡한 추론 작업을 수행하고, 콘텐츠 생성을 가능하게 하는 데 큰 잠재력을 보여주는 일반적인 AI 어시스턴트가 되었습니다. 오픈소스 LLM(예: LLAMA(Touvron et al., 2023) 및 ChatGLM(THUDM, 2023))의 급속한 발전에도 불구하고, 외부 도구를 사용하고 최신 정보를 수집하기 위한 사용자 지침을 따르는 것과 같은 복잡한 작업을 수행하는 데 여전히 제한적입니다. 실제 실용적 응용 프로그램을 위한 LLM의 힘을 더욱 발휘하기 위해 현재 연구의 상승 추세(Schick 등, 2023; Shen 등, 2023; Yang 등, 2023; Qin 등, 2023; Patil 등, 2023)는 도구 사용 능력이 있는 LLM이 AI 에이전트를 구축할 수 있도록 하기 시작했습니다. 여기에는 HuggingFace 모델과 연결하기 위한 HuggingGPT(Shen 등, 2023), Visual-ChatGPT(Wu 등, 2023) 및 Gorilla(Patil 등, 2023), 날씨 예보 및 검색 엔진과 같은 방대한 공통 API를 사용하기 위한 ToolAlpaca(Tang 등, 2023) 및 ToolLLAMA(Qin 등, 2023)가 포함됩니다. 이러한 방법은 ChatGPT와 같은 폐쇄형 소스 대응 제품에 직접 의존하거나 특정 유형의 API 도구에 초점을 맞춥니다. 최근에는 Auto-GPT³, LangChain, Transformers Agent(Huggingface, 2023)와 같은 AI 에이전트가 공개 릴리스되어 ChatGPT나 GPT-4와 같은 LLM이 도구를 사용하고 복잡한 AI 작업을 해결할 수 있게 되었습니다. 그러나 이러한 에이전트는 주로 폐쇄형 소스 LLMS로 구축되었으며 오픈 소스 LLM을 사용하여 사용자 정의 가능한 에이전트 시스템을 구축하는 방법은 여전히 크게 탐구되지 않았습니다. 이 연구에서는 오픈 소스 LLM을 컨트롤러로 사용하는 실제 애플리케이션을 위한 일반적이고 사용자 정의 가능한 에이전트 시스템인 ModelScope-Agent를 제시합니다. ModelScope³는 AI 커뮤니티에서 가장 진보된 머신 러닝 모델을 하나로 모으고 실제 애플리케이션에서 AI 모델을 활용하는 프로세스를 간소화하는 공개 ML 커뮤니티입니다. ModelScopeAgent는 사용자 정의 가능한 엔진 설계를 통해 유연하고 사용자 친화적인 시스템 라이브러리를 제공합니다. *연락처:<ym119608@alibaba-inc.com> https://github.com/modelscope/modelscope-agent 2https://modelscope.cn/studios/damo/ModelScopeGPT/summary &quot;https://modelscope.cn/models https://github.com/Significant-Gravitas/Auto-GPT https://github.com/langchain-ai/langchain은 여러 오픈소스 LLM에 대한 모델 학습을 지원하는 동시에 모델 API와 일반 API를 통합된 방식으로 원활하게 통합할 수 있습니다. 오픈소스 LLM을 핵심 컨트롤러로 포함하고 도구 사용 모듈과 메모리 모듈과 상호 작용하여 복잡한 작업을 수행하는 LLM 중심 시스템 설계가 특징입니다. ModelScope-Agent의 핵심에서 라이브러리는 LLAMA(Touvron et al., 2023), ChatGLM(THUDM, 2023), ChatPLUG(Tian et al., 2023) 및 기타 사용자 지정 LLM과 같은 다양한 오픈소스 LLM에 대한 유연한 선택 및 학습을 지원합니다. ModelScope. 도구 사용을 위해 ModelScopeAgent는 NLP, CV, 오디오 및 다중 모델 분야에서 다양한 AI 모델 API와 검색 엔진과 같은 방대한 공통 API를 지원하는 기본 도구 라이브러리를 제공합니다. 또한 대규모 도구 라이브러리에서 새로운 자체 정의 API 플러그인과 자동 API 검색을 등록하는 것도 지원합니다. 사용자는 가장 적합한 LLM, 로컬 API 도구 및 기능을 사용자 지정하여 실제 애플리케이션을 개발하기 쉽습니다. 또한 시스템 메시지, 사용자 기록, 컨텍스트 내 예제, 도구 메시지 및 지역화된 지식을 더 잘 저장하고 관리하기 위한 메모리 모듈도 도입되었습니다. 오픈 소스 LLM이 전체 에이전트 시스템을 더 잘 제어할 수 있도록 도구 사용 데이터 수집, 사용자 지정 모델 교육, 평가 및 배포에 대한 포괄적인 프레임워크를 추가로 제안합니다. 특히 다양한 API 범주, 다중 턴 API 호출, API 지향 QA 및 영어와 중국어로 된 API 독립적 지침이 포함된 포괄적인 도구 강화 데이터 세트 MSAgent-Bench를 출시합니다. API 이름과 매개변수 생성의 훈련을 강화하는 Weighted LM의 간단한 훈련 전략은 API 호출의 정확성을 보다 잘 보장하기 위해 사용됩니다. 게다가, 우리 라이브러리에서는 훈련된 모델의 도구 사용 능력을 다양한 측면에서 검사하기 위한 평가 프레임워크도 지원합니다. 나아가, 우리는 ModelScope-Agent를 ModelScope Community의 실제 애플리케이션인 ModelScopeGPT에 적용했는데, 이는 오픈소스 LLM을 1000개 이상의 공개 AI 모델과 연결하고 커뮤니티 QA를 위해 ModelScope에서 지역화된 커뮤니티 지식에 액세스할 수 있습니다. 요약하자면, ModelScope-Agent는 개발자가 오픈소스 LLM의 힘을 활용할 수 있도록 설계된 일반적이고 사용자 정의 가능한 에이전트 시스템입니다. 이 라이브러리는 다음과 같은 목표를 목표로 합니다. • • 오픈소스 LLM 기반 에이전트: ModelScope-Agent의 컨트롤러는 우리의 에이전트 훈련 프레임워크를 통해 최적화된 오픈소스 LLM에서 유연하게 선택할 수 있습니다. 다양한 도구의 지원 및 사용자 정의: 기본적으로 수십 개의 다양한 모델 API와 공통 API가 제공됩니다. 라이브러리는 툴셋에서 새로운 자체 정의 API와 자동 API 검색을 등록하는 것을 지원합니다.• 사용자 정의 가능한 애플리케이션: ModelScopeAgent는 다양한 산업 애플리케이션에 유연하게 적용할 수 있습니다. 에이전트와 교육 프레임워크는 사용, 구성 및 최적화를 설명하는 문서로 작성됩니다. ModelScope-Agent는 ModelScope의 엔지니어가 지속적으로 개발하고 있으며 Apache 2.0 라이선스에 따라 출시됩니다. 전체 문서는 프로젝트 웹사이트에서 제공됩니다. 2 ModelScope Agent ModelScope-Agent는 개발자가 오픈 소스 LLM을 기반으로 사용자 정의 가능한 에이전트 시스템을 구축하는 데 도움이 되도록 설계되었습니다. 전체 시스템 아키텍처는 그림 1에 나와 있습니다. 컨트롤러로 오픈 소스 LLM, 도구 사용 모듈 및 상호 작용할 메모리 모듈이 포함됩니다. 인간의 지시가 주어지면 선택된 LLM을 컨트롤러로 채택하는 Agent는 자동으로 작업을 계획하고, 도구를 선택적으로 사용하고, 메모리의 지식을 활용하고, 마지막으로 사용자에게 유용한 응답을 제공합니다. 2.1 두뇌로서의 LLM LLM은 에이전트의 두뇌 역할을 하며, 사용자 요청을 계획하고 분해하고, 도구를 선택적으로 호출하고, 검색을 수행하고, 이전 단계의 모든 정보를 통합하여 최종 응답을 생성합니다. 사용자가 자신의 LLM으로 에이전트를 더 쉽게 사용자 정의할 수 있도록 기본적으로 LLAMA, ChatGLM 및 ChatPLUG와 같은 다양한 오픈 소스 LLM에 대한 지원을 추가했으며, 이는 도구 학습 파이프라인을 통해 최적화되었습니다. 학습 전략 및 도구 사용 데이터 세트에 대한 자세한 내용은 섹션 3을 참조하세요. ModelScope-Agent는 ModelScope 커뮤니티의 LLM 추론 파이프라인을 통합했으며, model_name 및 model_config를 설정하기만 하면 LLM을 교체할 수 있습니다. model_config에서 model_id, model_revision 및 최대 시퀀스 길이와 같은 모델 매개변수 설정을 구성해야 합니다. 교육 프레임워크 데이터 수집 모델 API 일반 API API 지향 QA ⚫ API 독립적 에이전트 파이프라인 에이전트 실행 도구 검색 LLM 교육 0% 메모리 제어 LLM을 뇌 작업 계획으로 사용 ChatGLM 배포 가중 LM 도구 사용 평가 자동 평가 EM Rouge-L FHuman 평가 API 실행 응답 생성 메모리 제어 지식 검색 프롬프트 생성기 임베딩 시스템 프롬프트 API 스키마 • 지식 시스템 모듈 도구 사용 도구 라이브러리 AI 모델 • 텍스트-이미지 • 텍스트-비디오 • 텍스트-오디오 •이미지 채팅 • 텍스트 번역 • Universal-IE 일반 API • 날씨 웹 검색 • 계산기 • 지도 • 음악 플레이어 쇼핑 도구 검색 도구 사용자 정의 그림 1: ModelScope-Agent의 전체 시스템 아키텍처. # LLM config &quot;cfg_file&quot; from modelscope.utils.config import Config model_cfg = Config.from_file(cfg_file) 11m 로컬 LLM(model_name, model_cfg) 또한 ModelScope-Agent는 새로운 LLM을 통합하는 표준적인 방법도 제공합니다. 사용자는 LLM 파이프라인을 ModelScope에 통합하여 자신의 LLM을 추가할 수 있습니다. 그런 다음 에이전트는 학습 및 추론을 위해 새로운 LLM을 선택할 수 있습니다. 2.2 도구 사용 도구 라이브러리 도구 라이브러리는 에이전트에서 사용되는 다양한 API 컬렉션을 구성하고 관리하는 데 사용됩니다. ModelScope-Agent는 ModelScope 및 HuggingFace의 NLP, CV, 오디오 및 멀티모달 모델에서 검색 API와 같은 일반적인 API와 AI 모델 API를 모두 광범위하게 지원할 수 있습니다. 각 도구 API는 API 이름, 설명, 매개변수 및 요청 함수로 구성됩니다. 사용자는 라이브러리에서 적절한 API를 쉽게 선택하고 구성하여 자신의 에이전트를 빌드할 수 있습니다. 라이브러리에서 지원하는 기본 API는 부록 A.1을 참조할 수 있습니다. #23 도구 기본 구성 파일 &quot;default_file&quot; tool_cfg = Config.from_file(default_file) 새 도구 등록 및 사용자 정의 에이전트를 사용하면 사용자가 새 도구를 등록하고 사용자 정의할 수 있으며, 새로 등록된 도구를 에이전트에 빠르게 통합하여 LLM이 특정 애플리케이션에 대해 추가로 자체 정의된 도구를 선택적으로 사용할 수 있습니다.이는 기본 클래스인 Tool에서 상속하고 API 이름, 설명, 매개변수 및 요청 함수의 API 관련 스키마로 새 CustomTool을 정의하여 간단히 수행할 수 있습니다.Custom Tool에 대한 자세한 내용은 부록 A.2에서 참조할 수 있습니다.from modelscope_agent.tools import Tool class Custom Tool(Tool): #23 여기에 논리 추가 # 부록 A의 예제 참조.tool_list = {&#39;customo-tool&#39;: Custom Tool() } 도구 검색 및 실행 도구 라이브러리에 도구 API가 많기 때문에 각 지침 프롬프트에 적합한 API를 추천하기 위해 도구 검색 모듈이 추가로 도입되었습니다. 구체적으로, 우리는 통합 다국어 텍스트 임베딩 API 6에 기반한 밀집 벡터 검색 방법을 사용합니다. 우리는 텍스트 임베딩 API를 사용하여 API의 텍스트 설명과 지시 프롬프트를 모두 벡터화합니다. 가장 높은 벡터 곱 점수를 가진 상위 3개의 가장 관련성 있는 API가 도구 사용을 위해 선택됩니다. 결과적으로, 검색된 API의 스키마 정보는 후속 메모리 모듈의 다른 시스템 프롬프트와 연결되어 LLM에 입력으로 전송됩니다. 연결된 지시 프롬프트를 사용하여 LLM은 에이전트가 실행할 API 요청을 계획하고 생성합니다. 그런 다음 에이전트는 지속적인 생성을 위해 LLM에 결과를 반환합니다. 2.3 메모리 제어 메모리 모듈은 LLM에 대한 입력으로 일련의 상황 정보를 검색하고 조립하는 데 사용됩니다. 이는 지식 검색 하위 모듈과 프롬프트 생성기 하위 모듈로 구성되며, 각각 외부 지식 검색과 지시 프롬프트 생성을 담당합니다. &quot;https://help.aliyun.com/zh/dashscope/getting-started-Knowledge Retrieval 에이전트가 쿼리 프롬프트와 관련된 최신의 지역화된 정보에 액세스할 수 있도록 하여 LLM을 동적이고 도메인별 지식으로 보강합니다. 이전 도구 검색 모듈과 동일한 고밀도 벡터 검색 방법을 따르고 지역화된 문서 코퍼스에서 대규모 지식 검색을 지원합니다. 마찬가지로 사용자가 다른 오픈 소스 검색 프레임워크로 변경하여 사용자 정의할 수 있습니다. 프롬프트 생성기 프롬프트 생성기는 시스템 프롬프트, API 스키마, 검색된 지식, 대화 기록 및 몇 가지 샷 예제와 같은 모든 사용 가능한 상황 정보를 조립하는 데 사용됩니다. 사용자 쿼리 유형과 LLM의 최대 길이에 따라 사용자는 적절한 상황 정보를 선택적으로 선택하고 LLM에 필요한 입력을 조립할 수 있습니다. 에이전트에서 프롬프트 생성기는 에이전트를 구성하기 전에 정의해야 합니다. 2.4 에이전트 파이프라인 요약하면 모든 모듈을 결합하여 에이전트를 빌드합니다. LLM 컨트롤러, 도구 사용 모듈 및 메모리 모듈. agent.run을 사용하면 에이전트가 한 단계 생성으로 효율적으로 명령을 실행하고 완료할 수 있습니다. 먼저 에이전트는 도구 검색을 통해 쿼리 관련 도구를 검색하고 검색된 API 스키마를 메모리 모듈의 다른 상황별 프롬프트와 결합하여 새로운 명령 프롬프트를 구성합니다. 그런 다음 에이전트는 이 새로운 프롬프트를 LLM에 보내고 LLM은 호출할지 여부와 어떤 API를 호출할지 계획하고 API 요청을 생성합니다. 그런 다음 에이전트는 추출된 API 매개변수로 선택한 API를 실행하고 API 결과를 LLM에 반환하고 LLM은 다른 API를 호출할지 여부를 계속 계획합니다. 다른 API 호출이 필요한 경우 프로세스가 반복되고 그렇지 않으면 LLM이 최종 응답을 생성하고 에이전트는 최종 결과를 사용자에게 반환합니다. agent = AgentExecutor (1lm, tool_cfg, additional_tool_list = tool_list) agent.run(&quot;Draw a logo image of agent&quot;) 3 Training 3.1 Dataset 사용자 참여의 최적 수준을 유지하면서 도구를 사용할 수 있는 에이전트를 구축하기 쉽게 하기 위해 ChatGPT 합성 데이터와 기존 지시 수행 데이터 세트를 활용하는 포괄적인 도구 데이터 세트 MSAgent-Bench7을 릴리스합니다. 릴리스된 데이터 세트에는 598k 대화가 포함됩니다. 표 1은 릴리스된 데이터 세트와 다른 공개적으로 사용 가능한 도구 학습 데이터 세트 간의 주요 차이점을 간략하게 설명하고, 데이터 세트의 데이터 분포는 그림 2에 나와 있습니다. 표와 그림에서 볼 수 있듯이 에이전트의 효과적인 교육을 가능하게 하는 포괄적인 데이터 세트를 구성하기 위해 노력했습니다. 다국어: 중국어와 영어 모두로 인스턴스를 수집하여 교육된 에이전트가 두 언어로 작동할 수 있도록 합니다. 다양한 API 범주: 데이터 세트는 사용자가 등록하거나 온라인 API 플랫폼을 통해 적용한 공통 API와 신경 모델을 호출할 수 있는 모델 API를 지원합니다. 멀티 턴 대화: 실제 상황에서 에이전트는 작업을 완료하거나 이전 작업을 완료한 후 추가 지침을 받기 위해 사용자에게 더 구체적인 설명을 요청해야 할 수 있습니다. 당사 데이터 세트는 이러한 시나리오를 고려하고 도구를 사용할 때 멀티 턴 사용자-에이전트 상호 작용을 지원합니다. API 중심 QA: 효과적인 에이전트는 API에 대한 지식을 보유해야 합니다. 당사 데이터 세트는 API 문서 QA 작업과 작업 계획 작업을 통합하여 에이전트가 복잡한 작업을 해결하기 위해 다양한 API를 사용하는 방법에 대한 적절한 제안을 사용자에게 제공해야 합니다. API에 독립적인 지침: 에이전트가 일반적인 지침을 따르고 사용자 참여를 늘리는 능력을 향상시키기 위해 데이터 세트에 중국어와 영어 API에 독립적인 지침을 모두 통합했습니다. 이러한 지침은 API 호출에 대한 의존성보다는 에이전트의 고유한 기능에 더 큰 중점을 둡니다. 데이터는 ChatGPT(gpt-3.5-turbo)를 프롬프트하여 API 호출 결과에 따라 지침, API 요청 및 답변을 생성하여 수집되었습니다. 자세한 내용은 부록 D에서 확인할 수 있습니다. 3.2 모델 학습 LLAMA(Touvron et al., 2023), Qwen(QwenLM, 2023), ChatPLUG(Tian et al., 2023) 등을 포함하여 여러 오픈소스 LLM을 미세 조정하기 위해 MSAgent-Bench를 사용합니다. 다중 라운드 대화 모드에서 모든 오픈소스 LLM을 학습하고 모든 프롬프트와 답변을 연결합니다. Com&quot;https://modelscope.cn/datasets/damo/MSAgentBench/summary 데이터 세트 언어 인스턴스 유형 인스턴스 수 API 유형 평균 회전 평균 단계 API-Bank(Li et al., 2023) 영어 도구 사용 ToolAlpaca(Tang et al., 2023) 영어 도구 사용 3.9K 일반 API 3.1. 일반 API 1.Gorilla(Patil et al., 2023) 영어 도구 사용 16.4K 모델 API GPT4Tools(Yang et al., 2023) 영어 도구 사용 71.4K 모델 API ToolBench(Qin et al., 2023) MSAgent-Bench(저희) 영어 도구 사용 26.9K 영어 + 중국어 도구 사용 + 일반 채팅 598K 일반 API 일반 API + 모델 API 4.1.1.표 1: MSAgent-Bench 및 기타 기존 도구 학습 데이터 세트의 통계. 모델 API 텍스트-이미지 텍스트-비디오 API 지향 QA 문서 QA 작업 계획 • • 텍스트-오디오 번역 이미지 채팅 범용 IE 공통 API .날씨 웹 검색 계산기 맵 MSAgentBench • API 독립적인 지침 스토리 생성 오픈 QA 코드 잡담 의역 STEM 롤 플레이 그림 2: 수집된 MSAgent-Bench의 인스턴스 유형 및 분포. 일반적인 지침 튜닝 데이터와 비교하여 도구 학습 샘플은 도구 선택의 정확성과 API 매개변수 예측에 더 중점을 둡니다.따라서 API 이름 및 매개변수 생성의 교육을 강화하고 사용자 프롬프트와 도구 실행에서 토큰 손실을 없애는 간단한 교육 전략인 Weighted LM을 제안합니다.자세한 내용은 부록 B.3을 참조하세요.kwargs = dict(model = model, ...) trainer: Epoch Based Trainer = build_trainer는 생성된 응답과 골드 답변 간의 유사성을 측정합니다.또한 API 요청의 품질을 완전히 평가하기 위한 인수 F1이라는 새로운 메트릭을 소개합니다. 인수 F1을 계산하기 위해 에이전트의 API 요청에 있는 인수를 두 가지 경우로 분류합니다. 즉, 올바른 인수이지만 잘못된 값을 갖는 절반 일치(HM)와 올바른 인수이지만 올바른 값을 갖는 완전 일치(FM)입니다. API의 골드 인수 번호가 |A|이고 에이전트 API 요청의 인수 수(name=args.trainer, default_args=kwargs)가 |A*|라고 가정하면 새로운 Recall and Precision 트레이너를 계산합니다. train() 4 평가 평가 시스템인 MSAgent-Eval은 두 가지 모듈로 구성됩니다. 에이전트의 API 사용성을 종합적으로 평가하는 자동 평가 프레임워크와 인간 사용자의 선호도를 반영하는 에이전트 경기장에서 구현된 인간 평가 프레임워크입니다. 4.1 자동 평가 프레임워크 자동 평가에서는 주로 API 호출 결과에 따라 정확한 API 요청과 적절한 답변을 생성하는 에이전트의 능력을 평가하는 데 중점을 둡니다. 구체적으로, 우리는 에이전트가 올바른 API를 참조 골드 API로 사용하는지 여부를 측정하는 정확히 일치 점수(Action EM)와 다음과 같은 ROUGE-L 점수를 사용합니다. R = (0.5 x #HM + # FM)/|A| P = (0.5 × # HM + # FM)/|A*| (1) (2) 그리고 최종 인수 F1은 다음과 같이 계산됩니다. F1 = 2(RP)/(R+P). (3) 에이전트의 자동 평가를 위한 샘플 코드는 아래와 같습니다. from tool_agent_finetune import evaluation EM, F1, ROUGE = evaluation (refs, preds) 전문 주석자는 다양한 지침을 제공하고, 올바른 API 호출 요청을 수동으로 문서화하고, 적절한 응답을 작성하는 작업으로 평가 인스턴스에 주석을 달았습니다. 현재 조립된 테스트 데이터의 통계는 부록 B.1에 있으며, 훈련된 에이전트의 자동 평가 점수는 Vision Pro VR 안경에 대한 짧은 홍보 카피를 20단어로 작성하고 생성된 콘텐츠를 여성 음성으로 읽을 수 있습니까? Vision Pro VR 안경으로 꿈에서나 볼 법한 세상을 경험하세요. &quot;modelscope speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. 0:00/С 이미지를 생성하기 위해 이미지 생성 API를 호출하겠습니다. {그리고 VR 안경 이미지를 그려줄 수 있나요?&quot;modelscope_image-generation&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. -API 등록 [1v 2 &quot;plugin_name&quot;: &quot;RenewInstance&quot;, 3v plugin_schema_for_model&quot;: { &quot;name&quot;: &quot;RenewInstance&quot;, &quot;description&quot;: &quot;ECS 구독 갱신&quot; url: &quot;http://123.56.163.126:8120/&quot;, &quot;paths&quot;: [ &quot;name&quot;: &quot;RenewInstance&quot;, &quot;method&quot;: &quot;post&quot;, &quot;description&quot;: &quot;ECS 구독 갱신&quot; parameters&quot;: [ &quot;name&quot;: &quot;instance_id&quot;, &quot;description&quot;: &quot;ECS 인스턴스 ID, &quot;required&quot;: true&quot;name&quot;: &quot;period&quot;,&quot;description&quot;: &quot;갱신 기간 &quot;required&quot;: true 인스턴스 ID가 irj90a7e840y5csv9nh2a인 ECS 인스턴스를 10개월 동안 갱신하도록 도와주세요. ▸ &quot;RenewInstance&quot; API를 선택하면 현재 API가 실행 중... API 호출이 완료되었습니다. 인스턴스 ID가 irj90a7e840y5csv9nh2a인 ECS 인스턴스를 10개월 동안 갱신하는 데 도움을 드리겠습니다. 갱신 프로세스가 성공적으로 완료되었습니다. 다른 ECS 인스턴스, ID: i-rj90a7e840y5csv9nh4b, 12개월 ▸ &quot;RenewInstance&quot; API를 선택하면 API가 현재 실행 중입니다... ►API 호출이 완료되었습니다. 인스턴스 ID가 irj90a7e840y5csv9nh4b인 ECS 인스턴스를 12개월 동안 갱신하는 데 도움을 드리겠습니다. 갱신 프로세스가 성공적으로 완료되었습니다. 이미지 지우기 보내기 재생성 (a) ModelScope Intelligent Assistant 그림 3: ModelScope-Agent 기반 ModelScopeGPT의 데모 사례. (b) Alibaba Cloud에서 새 도구 등록 및 사용은 부록 B.2에서 확인할 수 있습니다. 또한 사용자가 사용자 지정 시나리오에서 에이전트의 성능을 정확하게 평가하기 위해 자신의 주석이 달린 테스트 예제를 업로드하도록 보장합니다.4.2 Agent Arena를 통한 인간 평가 ChatBots를 위한 Arena(Zheng et al., 2023)에서 영감을 받아 제공된 API를 기반으로 사용자가 두 명의 익명 에이전트에게 지침을 제공할 수 있는 액세스 가능한 Agent Arena를 구축했습니다. 그런 다음 사용자는 제공된 API를 사용하여 지침을 처리하는 데 어느 에이전트가 더 나은 성과를 보이는지 투표할 기회를 갖습니다. Zheng et al.(2023)에서 제시한 프레임워크에 따라 참여 에이전트에 대한 ELO 등급 및 리더보드 유지 관리 시스템을 채택합니다.5 ModelScopeGPT의 사용 예 이 섹션에서는 ModelScope-Agent를 기반으로 ModelScope Community인 ModelScopeGPT9의 성공적인 응용 프로그램을 소개합니다.ModelScope Intelligent Assistant ModelScope-Agent를 기반으로 ModelScope Community를 위한 지능형 어시스턴트인 ModelScopeGPT를 개발했습니다. 이는 LLM을 컨트롤러로 사용하여 ModelScope 오픈소스 커뮤니티에서 수십 개의 도메인별 AI 모델을 연결하며, NLP, CV, 오디오 및 멀티모달 분야를 포함합니다. 파이프라인을 보다 실용적으로 만들기 위해 적절한 API를 자동으로 선택하고 로컬 ModelScope 지식에 액세스할 수 있는 API 검색 및 지식 검색 도구를 포함했습니다. 그림 3a에서 볼 수 있듯이 ModelScopeGPT는 다중 턴 대화에서 API 호출을 지원하고 이전 대화의 정보를 사용하여 올바른 API https://modelscope.cn/studios/LLMZOO/ChineseArena/summary &quot;https://modelscope.cn/studios/damo/ModelScopeGPT /summary 호출 매개변수를 생성할 수 있습니다. 더 많은 사례는 부록 C를 참조할 수 있습니다. 그 결과 ModelScopeGPT는 출시 후 한 달 만에 40,000명의 사용자 방문에서 170,000개가 넘는 총 요청 수를 달성했습니다. 새 도구 등록 및 사용 에이전트의 또 다른 주요 기능은 보이지 않는 API에 대한 일반화 기능입니다. 이를 통해 사용자는 자신의 API를 빠르게 등록하고 특정 애플리케이션을 사용자 정의할 수 있습니다. 따라서 Alibaba Cloud 애플리케이션 시나리오에 적용하여 ModelScopeGPT의 일반화 기능을 테스트합니다. 그림 3b에서 볼 수 있듯이 먼저 Alibaba Cloud에서 ECS 인스턴스를 갱신하기 위한 API를 찾았습니다. 그런 다음 도구 라이브러리에 정의된 API 스키마를 에이전트에 등록했습니다. 마지막으로 &quot;도와주세요&quot;라는 프롬프트를 입력했습니다. 데모에서 ECS를 갱신하세요...&quot;. 에이전트는 계획을 통해 요청을 생성하고, 적절한 API를 선택하고, API를 호출하여 인스턴스를 성공적으로 갱신하고, 갱신이 완료되었음을 사용자에게 알리는 답변을 제공했습니다. 이 테스트는 릴리스된 API 데이터 세트를 기반으로 최적화된 오픈소스 LLM이 보이지 않는 API에 대한 강력한 일반화 능력을 가지고 있음을 보여줍니다. 6 결론 ModelScope-Agent는 유연한 시스템 설계, 데이터 수집, 모델 교육, 평가 및 실제 애플리케이션에서의 사용 예를 포함하는 일반적이고 사용자 정의 가능한 에이전트 프레임워크를 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하는 것을 목표로 합니다. 오픈소스 LLM을 사용하여 에이전트 시스템을 구축하기 위한 AI 에이전트 학습 및 모범 사례를 위한 오픈소스 커뮤니티 중심 라이브러리를 제공합니다. ModelScope-Agent가 AI 에이전트의 새로운 시대로 나아가는 길을 닦는 데 도움이 되기를 바랍니다. 윤리 선언문 의도된 사용. ModelScope-Agent는 일반적이고 사용자 정의 가능한 에이전트 시스템을 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하도록 설계되었습니다. 잠재적인 오용. 도구 사용 데이터 세트로만 훈련하고 특정 데이터 필터링 규칙을 거쳤지만, 사용자 지정 모델이 편향되고, 가짜이며, 안전하지 않은 정보를 생성할 가능성이 여전히 있습니다. 에이전트 프레임워크는 또한 사용자에게 적절한 LLM을 선택하고 훈련을 위해 자신의 깨끗한 데이터를 업로드할 수 있는 자유를 제공합니다. 또한 미래에 에이전트 프레임워크의 안전성을 개선하기 위한 구체적인 방법을 설계하는 것도 중요합니다. 참고문헌 마이클 안, 앤서니 브로한, 노아 브라운, 예브겐 체보타르, 오마르 코르테스, 바이런 데이비드, 첼시 핀, 추위안 푸, 키르타나 고팔라크리슈난, 카롤 하우스만, 알렉스 헤르초크, 다니엘 호, 재스민 쉬, 줄리안 이바르즈, 브라이언 이크터, 알렉스 이르판, 에릭 장, 로사리오 하우레기 루아노, 카일 제프리, 샐리 제스먼스, 니킬 J 조시, 라이언 줄리안, 드미트리 칼라시니코프, 위헝 쿠앙, 쿠앙휴이 리, 세르게이 레빈, 야오 루, 린다 루, 캐롤라이나 파라다, 피터 파스터, 조넬 키암바오, 카니슈카 라오, 자렉 레팅하우스, 디에고 레이예스, 피에르 세르마네, 니콜라스 시버스, 클레이튼 탄, 알렉산더 토셰프, 빈센트 반호우케, 페이 샤, 테드 샤오, 펭 쉬, 쓰춘 쉬, 멍위안 얀, 앤디 젱. 2022. 내가 말하는 대로가 아니라 할 수 있는 대로 하세요: 로봇 어포던스의 언어 기반 마련. arXiv 사전 인쇄 arXiv:2204.01691. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic 등 2023. Falcon40b: 최첨단 성능을 갖춘 개방형 대형 언어 모델. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스, NeurIPS 2020, 2020년 12월 6일-12일, 가상으로 개최. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. CORR, ABS/2204.02311. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training Compute-Optimal Large Language Models. arXiv 사전 인쇄본 arXiv:2203.15556. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Enbodied reasoning through planning with language models. 로봇 학습에 관한 제6차 컨퍼런스의 회의록, 기계 학습 연구 회의록 205권, 1769-1782쪽. PMLR. 허깅페이스. 2023. 트랜스포머 에이전트. 웹사이트. https://huggingface.co/docs/transformers/ transformers_agents. 밍하오 리, 페이판 송, 보웬 유, 하이양 유, 저우준 리, 페이 황, 용빈 리. 2023. Apibank: 도구 증강 llms의 벤치마크. arXiv 사전 인쇄본 arXiv:2304.08244. 니클라스 뮤니히호프, 토마스 왕, 린탕 수타위카, 애덤 로버츠, 스텔라 비더만, 테븐 르 스카오, M 사이풀 바리, 쉔 쉔, 정신 용, 헤일리 쇼엘코프 외. 2022. 멀티태스크 미세 조정을 통한 언어 간 일반화. arXiv 사전 인쇄본 arXiv:2211.01786. OpenAI. 2022. Chatgpt: 대화를 위한 언어 모델 최적화. OpenAI. 2023. abs/2303.08774. GPT-4 기술 보고서. CORR, Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez. 2023. Gorilla: 대규모 API와 연결된 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2305.15334. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu 및 Maosong Sun. 2023. 기초 모델을 사용한 도구 학습. arXiv 사전 인쇄 arXiv:2304.08354. QwenLM. 2023. Qwen-7b. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. arXiv 사전 인쇄본 arXiv:2112.11446. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. 2023. 도구 형성자: 언어 모델은 스스로 도구를 사용하도록 학습할 수 있습니다. arXiv 사전 인쇄본 arXiv:2302.04761. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. 2023. Hugginggpt: Chatgpt와 친구들을 사용하여 hugging face에서 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun. 2023. Toolalpaca: 시뮬레이션된 사례를 사용한 언어 모델을 위한 일반화된 도구 학습. arXiv 사전 인쇄본 arXiv:2306.05301. THUDM. 2023. Chatglm. https://github.com/ THUDM/ChatGLM-6B. Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qinghao Ye, Jiejing Zhang, Ji Zhang, Fei Huang 및 Jingren Zhou. 2023. Chatplug: 디지털 휴먼을 위한 인터넷 증강 명령 튜닝 기능을 갖춘 개방형 도메인 생성 대화 시스템. arXiv 사전 인쇄 arXiv:2304.07849. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄 arXiv:2302.13971. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang 및 Nan Duan. 2023. Visual chatgpt: 시각적 기초 모델을 사용하여 말하고, 그리고, 편집합니다. arXiv 사전 인쇄 arXiv:2303.04671. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li 및 Ying Shan. 2023. Gpt4tools: 자가 학습을 통해 도구를 사용하도록 대규모 언어 모델 교육. arXiv 사전 인쇄 arXiv:2305.18752. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv 사전 인쇄본 arXiv:2306.05685. A 라이브러리 A.1 도구 목록 API 이름(언어) 텍스트-이미지(en) 텍스트-이미지(zh) 텍스트-비디오(en) 텍스트-오디오(en) 텍스트-오디오(zh) 이미지-채팅(en) 번역-zh2en 번역-en2zh Universal-IE(zh) 텍스트-지리(zh) NER(zh) API-검색 모델 범위-검색 설명 텍스트를 이미지로 변환합니다. 텍스트를 이미지로 변환합니다. 텍스트를 비디오로 변환합니다. туре 모델 API 모델 API 데이터 세트는 에이전트 예측과 비교할 참조로 텍스트 스니펫이 있는 360개의 대화로 구성되었으며, 이는 이전 호출 결과에 따른 798개의 API 요청과 1261개의 일반 텍스트 답변으로 구성됩니다. 텍스트를 오디오로 변환합니다. 텍스트를 오디오로 변환합니다. 이미지 채팅. 중국어 텍스트를 영어로 번역합니다. 영어 텍스트를 중국어로 번역합니다. 구조화된 정보를 추출합니다. 지리적 정보를 추출합니다. 텍스트에서 명명된 엔터티를 인식합니다. 관련 API를 검색합니다. 모델스코프 문서를 검색합니다. 모델 API 모델 API 모델 API 모델 API B.2 평가 결과 모델 API 모델 API 모델 API 모델 API 모델 ROUGE-L 모델 API 공통 API 공통 API 동작 EM 인수 FChatGPT(2-shot)* 36.34.25.LLAMA 39.58.44.ChatPLUG¹¹ MSAgent-Qwen¹ 46.68.55.51.87.68.표 2: 기본 도구 목록의 통계. API에 지원되는 입력 언어는 괄호 안에 나열되어 있습니다. A. 사용자 정의 도구 사용자는 기본 도구를 상속하고 미리 정의된 스키마에 따라 도구 이름, 설명 및 매개변수를 정의하여 고유한 도구를 사용자 정의할 수 있습니다. 또한 특정 요구 사항에 따라 _local_call() 또는 _remote_call()을 구현할 수 있습니다. 예를 들어, 아래는 사용자 정의 도구의 예입니다.class Custom Tool (Tool): description = &#39;xxx&#39; name = &#39;XXX&#39; parameters: list = }] &#39;name&#39;: &#39;XXX&#39; [ { &#39;description&#39;: &#39;xxx&#39;, &#39;required&#39;: True def _local_call(): def _remote_call(): 표 3: 자동 평가 결과.*는 ChatGPT를 미세 조정하지 않고 2개의 데모를 통해 컨텍스트 내 학습을 사용한다는 것을 나타냅니다.제안한 ModelScopeGPT에서 학습한 모델을 비교합니다.자동화 평가 결과는 표 3에 나와 있습니다.실험에서 얻은 결과에 따르면 컨텍스트 내 학습을 적용한 ChatGPT는 미세 조정을 거친 다른 모델에 비해 열등한 결과를 보였습니다.또한 LLAMA는 다른 미세 조정 모델에 비해 성능이 떨어졌습니다.오류 연구에 따르면 ChatGPT와 LLAMA의 성능이 낮은 것은 테스트 세트에서 중국어 테스트 사례가 많은 비중을 차지하기 때문일 수 있습니다. 더 나은 성과를 보인 모델(ChatPLUG, Qwen)은 주로 중국어 데이터에 초점을 맞춘 모델이었습니다. 저희 조사에 따르면 ChatGPT와 LLAMA는 사용자 의도 인식에 한계가 있어 궁극적으로 Action EM에서 최적이 아닌 성능을 보였습니다. 검토한 모델 중에서 Qwen이 가장 좋은 성과를 보였는데, 이는 기본 모델의 우수한 성능에 기인할 수 있습니다. B 실험 설정 평가 벤치마크 B. 훈련된 에이전트의 일반화를 평가하기 위해 ModelScope-Agent의 훈련 세트에 나타나는 10개의 도메인 내 API와 실제로 보이지 않는 10개의 API를 포함했습니다. 10. 또한 평가 벤치마크에서 여러 다중 턴 시나리오에 주석을 달아 에이전트의 다중 턴 능력을 고려했습니다. 저희의 테스트 인스턴스는 인간 전문가에게 먼저 다양한 지침을 작성하도록 요청하여 주석을 달았습니다. 그런 다음 인간 전문가에게 JSON API 요청을 작성하고 API 호출 결과를 얻은 후 지침에 적절하게 대답하도록 요청했습니다. 저희의 최종 테스트 10 진행 중이며, 앞으로 더 많은 API를 포함할 것입니다. B.3 가중 LM 우리는 훈련 전략인 가중 LM의 예를 들어보겠습니다. 그림 4에서 볼 수 있듯이, 색상이 다른 토큰은 손실 가중치가 다릅니다. 사용자 입력 프롬프트의 경우 손실 가중치를 0으로 설정하여 모델이 프롬프트에 대한 손실을 계산하지 않도록 합니다. 어시스턴트의 API-Agnostic 텍스트의 경우 손실 가중치를 1로 유지합니다. 마지막으로 API 이름, 매개변수, URL 등과 같은 API 호출의 중요한 텍스트의 경우 손실 가중치를 2로 설정하여 API 호출의 생성 정확도를 향상시킬 수 있습니다. 사용자: VR 안경에 대한 간략한 소개를 10단어로 작성하세요. 어시스턴트: 몰입형 가상 현실 경험을 위한 웨어러블 기기입니다. 사용자: 여성 목소리로 큰 소리로 읽어주세요. 어시스턴트<startofthink> { &quot;api_name&quot;: &quot;modelscope_speech-generation&quot;, &quot;url&quot;: &quot;http://33.57.174.141:5000/damo/speech samberthifigan tts zh-cn 16k&quot;, &quot;parameters&quot;:{&quot;text&quot;: &quot;몰입형 가상 현실 경험을 위한 웨어러블 디바이스&quot;, &quot;gender&quot;: &quot;여성&quot;}}<endofthink><starofexec> {&quot;결과&quot;:&quot;<audio id=audio controls= preload=none><source id=wav src=ursfe2.wav></audio> &quot;}<endofexec><audio id audio controls= preload=none><source id=wav src=ursfe2.wav></audio> 손실 가중치 0.1.2.그림 4: 가중 LM에 대한 훈련 전략의 예. 다른 색상의 토큰은 다른 손실 가중치를 갖습니다. 비디오 생성: 두 마리의 고양이가 놀고 있습니다. ModelScope의 비디오 생성 모델 API를 호출하여 비디오를 생성합니다. ▸ &quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 선명한 이미지 生成一两个猫는 玩耍的视频 國目前 ModelScope 只有支持英文视频生成模型, 所以我将首先把您的问题利 用 ModelScope的中英翻译模型转化为英文,然后再调用英文的视频生成模型 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 현재 API가 실행 중입니다... ► API 호출이 완료되었습니다. 번역 후의 단추: 두 마리 고양이가 놀고 있는 비디오 调用视频生成模型生成视频……………… ▶ &quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다... ► API 호출이 완료되었습니다. send regenerate clear image send regenerate 그림 5: 단일 단계 도구 사용 지침, 텍스트-비디오 사례. 표시할 비디오의 몇 프레임을 캡처했습니다. 영어(왼쪽)와 중국어(오른쪽)에서 동일한 의미적 지침을 사용하여 모델을 테스트합니다. ModelScope의 멀티모달 대화 상자 모델 API를 호출합니다. &lt;-&gt; 이 그림에는 무엇이 있습니까? ▸ &quot;modelscope_image-chat&quot; API를 선택하면 현재 API가 실행 중입니다... ►API 호출이 완료되었습니다. 이미지는 파란색과 보라색 픽셀화된 얼굴과 크고 흰 눈을 특징으로 합니다. 현재 ModelScope는 영어와 호환되는 다국어 ChatGPT 모델의 LLM만 지원하며, 이 모델의 이름은 mPLUG-owl입니다. 따라서 먼저 ModelScope의 중국어-영어 번역 모델을 사용하여 프롬프트를 영어로 변환한 다음 영어 mPLUG-wl 모델을 호출합니다. 중국어-영어 번역 모델을 호출하여 프롬프트를 영어로 변환합니다. &lt;-&gt; 이전에 ModelScope에 대해 더 많은 정보가 있는 ChatGPT의 LLM, 이름은 mPLUG-owl, 더 보기的中英翻译模型转 化为英文,然后再调用英文的 mPLUG-Owl 模型 调用中英翻译模型转化 프롬프트为英文 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 API가 현재 실행 중입니다... ►API 호출에 완료되었습니다. N: 이 사진에는 무엇이 있나요? 然後调用 mPLUG-owl 반환 결과 ►&quot;modelscope_image-chat&quot; API를 선택하면 현재 API가 실행 중입니다... ►API 호출이 완료되었습니다.이미지에는 만화처럼 보이는 파란색과 보라색 픽셀화된 얼굴이 있습니다.clear image send regenerate clear image send regenerate 그림 6: 단일 단계 도구 사용 지침, 이미지 채팅 사례.영어(왼쪽)와 중국어(오른쪽)로 동일한 의미적 지침을 사용하여 모델 테스트.C 사례 이 섹션에서는 ModelScope-Agent를 기반으로 하는 ModelScopeGPT 구현에 대한 정성적 결과를 보여줍니다.단일 단계 도구 사용 그림 5와 6에서 볼 수 있듯이 지침은 모델이 각각 비디오를 생성하고 이미지에 대해 채팅할 것으로 예상합니다.이러한 지침은 단일 단계의 도구 사용으로 완료할 수 있습니다.다단계 도구 사용 그림 7에서 볼 수 있듯이 지침은 모델이 먼저 홍보 카피를 작성한 다음 읽고 마지막으로 비디오를 생성할 것으로 예상합니다. 영어: 이러한 지침은 모델이 여러 단계의 도구 사용 기능을 가지고 있어야 합니다. 중국어 사례에서, 우리 모델은 3단계 도구 사용을 정확하게 완료했습니다. 여러 차례의 도구 사용 그림 8에서 볼 수 있듯이, 지침은 모델이 여러 차례의 대화를 하고 과거 대화를 사용할 수 있어야 합니다. 우리 모델은 API를 정확하게 호출하고 이전 대화의 내용을 캡처하여 API 매개변수를 생성할 수 있습니다. Vision Pro VR에 대한 20자 홍보 문구를 쓰고, 여성 음성으로 읽게 한 다음, 영상을 만들어서 어떻게 보이는지 확인하세요. Vision Pro VR로 완전히 새로운 세상을 경험하세요. 한계에 작별인사를 하고 생생하고 사실적인 영상에 푹 빠져보세요. 오늘 이 최첨단 안경을 손에 넣으세요! ►&quot;modelscope_speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다.... API 호출이 완료되었습니다. ► 0:00 / 0:►&quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다.... API 호출이 완료되었습니다. Vision Pro VR 안경: 사실적인 장면, 몰입감 넘치는 경험, 특별한 감각! ② Vision Pro VR 안경: 진실된 공간, 신체와 정신, 초강체!! ►&quot;modelscope.speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다.. API 호출이 완료되었습니다. 0:00/0:현재 ModelScope는 영어에 대한 비디오 생성 모델만 지원하므로 먼저 ModelScope의 영어-중국어 번역 모델을 사용하여 질문을 영어로 번역한 다음 영어 비디오 생성 모델을 호출합니다.目前 ModelScope 只有支持英文的视频生成模型,所以我将首先把您的问题利用 Modelscope 的中英翻译模型转化为英文,然后再调用英文的视频生成模型 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 현재 API가 실행중입니다... API 호출이 완료되었습니다. KO: Vision Pro VR 안경: 실제 장면, 몰입감 있고 특별한 경험! 번역된 지침 비디오 생성 모델을 호출하여 비디오를 생성합니다. 调用视频生成模型生成视频中 &quot;modelscope_video- Generation&quot; API를 선택하시면 현재 API가 실행중입니다... API 호출이 완료되었습니다. send regenerate send Fegenerate clear image clean image 그림 7: 다단계 도구 사용 지침. 비디오의 몇 프레임을 캡처하여 표시했습니다. 영어(왼쪽)와 중국어(오른쪽) 모두에서 동일한 의미 지침을 사용하여 모델을 테스트합니다. 20개 단어만 사용하여 간단한 스토리를 작성합니다. 20자 写一个 20 字左右简短小故事 도시에서 길을 잃은 그는 그녀를 만났습니다. 그들은 함께 역경을 극복하고 집으로 돌아가는 길을 찾았습니다.从前,有一只聪밝은狐狸,它善于骗取其他动동물적식물.一天,它看到了一只可爱的 小兔子,于是它决定装成受伤的样子,向小兔子求救。 小兔子心地善良,把狐狸带回家给它治疗。狐狸趁机逃走,并骗取了小兔子的먹물。 여자 목소리로 이야기 읽기 &quot;modelscope_speech- Generation&quot; API를 선택하면 현재 API가 실행되고 있습니다... 옛날 옛적에 시간, 다른 동물을 속여 음식을 얻는 데 능숙한 영리한 여우가 있었습니다. 어느 날, 귀여운 토끼를 보고 다친 척하고 토끼에게 도움을 요청하기로 했습니다. 친절한 토끼는 여우를 집으로 데려가 부상을 치료했습니다. 그러나 여우는 상황을 이용하여 도망가 토끼의 음식을 훔쳤습니다. 用女生念出来 API 호출이 완료되었습니다. 0:00/0:►&quot;modelscope_speech-generation&quot; API를 선택하는 중, 현재 API가 실행 중입니다... API 호출이 완료되었습니다. ► 0:00 / 0:그리고 스토리 그림을 그려 주시겠습니까? ►&quot;modelscope_image-generation&quot; API를 선택하는 중, 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 명확한 이미지 给这个故事配一张图 好的,我将调用 ModelScope 魔搭社区的图文生成模型为您作画 ►&quot;modelscope_image-generation&quot; API를 선택하시면 현재 API가 실행중입니다... API 호출이 완료되었습니다. 보내기 재생성 선명한 이미지 보내기 재생성 그림 8: 다중 회전 도구 사용 지침, 텍스트 음성 변환 및 텍스트 이미지 변환 사례. 영어(왼쪽)와 중국어(오른쪽) 모두에서 동일한 의미론적 명령을 사용하여 모델을 테스트합니다. 介绍下魔搭社区 ModelScope 소개 ►&quot;modelscope_search&quot; API를 선택하시면 현재 API가 실행중입니다... ►API 호출이 완료되었습니다. ModelScope 社区成立于2022년 6월,是一个模型开源社区及创新平台,由阿里巴巴达摩院,联合 CCF开源发展委员会,共同작동작为项目发起方. ModelScope 커뮤니티는 2022년 6월에 다음과 같이 설립되었습니다. 모델 개발을 위한 오픈 소스 커뮤니티 및 혁신 플랫폼입니다. 이는 Alibaba DAMO Academy와 중국 컴퓨터 연맹(CCF) 오픈 소스 개발 위원회에 의해 시작되었습니다. 커뮤니티가 협력합니다. ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. ModelScope에는 어떤 모델이 있나요? 魔搭社区是一个开源的模型开源社区, 旨재통합회사, 构建深titudeschool习开源,并开源源, 并开源源关模型服务创新技术,推动模型应用生态的繁荣发設. ModelScope는 개방형 커뮤니티 협력을 통해 오픈 소스 딥 러닝 모델을 구축하는 것을 목표로 하는 오픈 소스 모델 개발 커뮤니티입니다. 또한 모델 서비스와 관련된 혁신적인 기술을 오픈소싱함으로써 모델 애플리케이션 생태계의 성공적인 발전을 촉진합니다. &quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 음성 합성 모델을 사용할 수 있나요? 확실히, 일관되고 리드미컬한 음성을 생성하는 데 탁월한 SAMBERT 모델을 사용하는 것이 좋습니다. 음성 합성 모델 링크 좀 주세요 ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 魔搭社区有开放域对话模型吗 ModelScope에 공개 도메인 대화 상자가 있습니까? ►&quot;modelscope.search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 当然有,我们有 ChatPLUG 模型,它可以接收用户对话历史and对话关的외부지식, 用户人设, 模型人设예, 우리는 사용자의 대화 기록, 대화와 관련된 외부 지식, 사용자 페르소나 및 모델 페르소나 정보를 가져와 적절한 응답을 출력할 수 있는 ChatPLUG 모델을 보유하고 있습니다. 사용 가능한 코드가 있나요? ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. 확실히, 우리는 이 모델에 대한 귀하의 필요와 실제 상황에 따라 사용할 수 있는 샘플 코드를 제공합니다. 예를 들어: modelscope.pipelines의 예제 코드 import Pipelines from modelscope.utils.constant import Tasks from modelscope.models import Model model_id = &#39;damo/ChatPLUG-3.7B 파이프라인_ins 파이프라인(Tasks.fid_dialogue, model-model_id,model_revision=&#39;v1.preprocess_params = { &#39;max_encoder_length&#39;: 380, 인코더 KAKE发个语음을합형성模型链接 &#39;context_turn&#39;: 3 # context input str = &#39;Q:我想去北京旅游,有什么好的推荐吗? result Pipeline_ins (입력, **kwargs) print (결과) 한국어: https://www.modelscope.cn/models/damo/speech_sambert-hifigan_tts_zhishuo_zhcn 16k/summary. clear image send regenerate image send regenerati 그림 9: 멀티 턴 도구 사용 지침, 텍스트-음성 및 텍스트-이미지 사례. 영어(왼쪽)와 중국어(오른쪽)에서 동일한 의미적 지침을 사용하여 모델 테스트. 도메인 내 지식 QA 그림 9에서 볼 수 있듯이 지침은 모델이 도메인 내 지식을 검색하고 검색된 지식을 사용하여 질문에 답하도록 요구합니다. 사용자로서의 지침 또는 설명 API 요청 에이전트로서의 API 갤러리 EE CAP 후속 또는 최종 답변 루프는 에이전트가 최종 답변으로 대화를 종료하는 것이 적절하다고 판단할 때까지 계속됩니다. 원시 데이터 세트를 획득한 후, ChatGPT가 검색된 API에 없는 환각적인 API 이름과 매개변수를 포함하는 API 요청을 생성하는 인스턴스를 제거하기 위해 필터링 메커니즘을 적용했습니다. 또한 ChatGPT가 불법 API 요청을 생성하는 인스턴스를 제외하여 정제되고 최종화된 데이터 세트를 생성했습니다.섹션 3.1에서 소개한 대로 다양한 언어와 주제에 걸쳐 인스턴스를 수집하며 수집된 데이터의 자세한 통계는 표 4에 나와 있습니다.결과 인스턴스 유형 중국어 # 인스턴스 532, 영어 66, 일반 API 211, 모델 API 58, 5, 329, 그림 10: MSAgentBench의 데이터 수집 절차.D 데이터 수집 절차 프롬프트 엔지니어를 사용하여 두 개의 ChatGPT(gpt-3.5-turbo)로 에이전트 시나리오를 시뮬레이션하여 데이터 세트를 수집했습니다.하나의 ChatGPT는 사용자 역할을 하도록 프롬프트되었고 다른 하나는 에이전트 역할을 하도록 할당되었습니다.기존의 실제 API가 아닌 교육 데이터에 제시된 API의 도메인과 기능을 확장하기 위해 ChatGPT에서 생성한 여러 합성 API도 포함했습니다. 이러한 합성 API가 대화에 통합되었을 때, 우리는 다른 ChatGPT가 API 역할을 하고 관련 호출 결과를 반환하도록 했습니다. 데이터 수집 절차는 그림 10에 나와 있습니다. 처음에는 일련의 무작위 컨텍스트 데모가 ChatGPT에 제공되어 명령을 생성했습니다. 이 명령은 제공된 데모에 따라 일반적인 명령이거나 API로 해결해야 하는 명령일 수 있습니다. 그 후, ChatGPT는 먼저 어떤 작업을 수행할지 생각하여 에이전트 역할을 하도록 촉구되었습니다. API 호출이 필요하지 않거나 사용자 설명이 필요한 경우 에이전트는 사용자에게 후속 응답으로 응답합니다. 그렇지 않으면 에이전트는 API 갤러리에 API 요청을 보냅니다. API 호출 결과를 받은 후 에이전트는 상황을 평가하고 다음 작업을 결정합니다. &quot;사용자-에이전트-API&quot; E API 지향 QA API 독립적 명령 표 4의 이 반복적 프로세스: 수집된 데이터 세트의 통계.
--- RELATED WORK ---
E.1 대규모 언어 모델 최근 몇 년 동안 대규모 언어 모델(LLM) 분야에서 급속한 발전이 있었습니다. GPT3(Brown 등, 2020), Gopher(Rae 등, 2021), Chinchilla(Hoffmann 등, 2022), PaLM(Chowdhery 등, 2022) 및 LLAMA(Touvron 등, 2023)와 같은 전형적인 모델은 모델과 데이터 크기를 확장하여 광범위한 NLP 작업에서 인상적인 제로샷 및 퓨샷 일반화 능력을 보여주었습니다. 주목할 만한 이정표는 AI 개발의 패러다임을 크게 혁신한 ChatGPT(OpenAI, 2022) 또는 GPT4(OpenAI, 2023)의 출시입니다. 그 결과, ChatGPT 및 Claude와 같은 폐쇄형 소스 대응 제품에 도전하고 따라잡기 위해 오픈 소스 LLM의 상승 추세가 나타났습니다. 여기에는 BLOOM(Muennighoff 등, 2022), LLAMA(Touvron 등, 2023), Falcon(Almazrouei 등, 2023), ChatGLM(THUDM, 2023)이 있습니다. 획기적인 진전에도 불구하고 LLM은 일반 텍스트 코퍼스를 통한 텍스트 생성기로 훈련되므로 멀티모달 작업과 같은 다른 작업에서는 성능이 떨어집니다. 또한 사전 훈련 데이터를 넘어서는 최신 정보가 필요한 작업에서도 성능이 떨어집니다. 도구나 외부 API를 사용하면 한계를 극복하고 LLM의 힘을 활용하여 다운스트림 애플리케이션과 원활하게 연결할 수 있습니다. ModelScope-Agent에서는 오픈 소스 LLM이 도구와 외부 API를 사용할 수 있도록 하는 에이전트 시스템을 구축하기 위한 전체 사용자 정의 프레임워크와 모범 사례를 제공합니다. E.2 에이전트 및 도구 학습 에이전트 시스템을 구성하기 위한 컨트롤러로 대규모 언어 모델(LLM)을 활용하는 것이 중요한 연구 분야로 부상했습니다. 여러 관련 연구에서는 ChatGPT(OpenAI, 2022) 및 Claude와 같은 폐쇄형 소스 LLM에 대한 신속한 엔지니어링 기술을 사용하여 특정 도메인에 적용할 수 있도록 합니다. 예를 들어, VisualChatGPT(Wu et al., 2023) 및 HuggingGPT(Shen et al., 2023)는 OpenAI LLM에서 사용할 수 있는 HuggingFace 모델 호출을 용이하게 합니다. SayCan(Ahn et al., 2022) 및 내면 독백(Huang et al., 2023)은 로봇과 LLM을 통합하여 로봇 시스템을 구현합니다. 특히 Langchain 및 Auto-GPT와 같은 최근 연구는 공통 API 및 신경 모델을 포함한 광범위한 도구를 포함하고 작업을 해결하는 동안 장기 추론 및 인간-에이전트 상호 작용을 향상시켜 일반화된 에이전트를 구축할 수 있는 엄청난 잠재력을 보여줍니다. 오픈소스 LLM이 도구를 활용할 수 있도록 하기 위한 수많은 노력도 이루어졌습니다. 예를 들어, Gorilla(Patil et al., 2023)와 GPT4Tools(Yang et al., 2023)는 자기 학습 기법을 사용하여 학습 데이터를 생성하여 오픈소스 LLM이 신경 모델을 효과적으로 활용하도록 학습시킵니다. ToolAlpaca(Tang et al., 2023)와 ToolLLaMA(Qin et al., 2023)는 공통 API를 사용하여 LLAMA를 학습시키는데, ToolAlpaca는 LLMS의 합성 API를 사용하는 반면 ToolLLAMA는 실제 API를 사용한다는 점이 다릅니다. 전반적으로 위에서 언급한 것과 비교했을 때
--- METHOD ---
s는 ChatGPT와 같은 폐쇄형 소스 대응 제품에 직접 의존하거나 특정 유형의 API 도구에 초점을 맞춥니다. 최근에는 ChatGPT 또는 GPT-4와 같은 LLM이 도구를 사용하고 복잡한 AI 작업을 해결할 수 있도록 하는 Auto-GPT³, LangChain 및 Transformers Agent(Huggingface, 2023)와 같은 AI 에이전트의 공개 릴리스도 있었습니다. 그러나 이러한 에이전트는 주로 폐쇄형 소스 LLMS로 구축되었으며 오픈 소스 LLM을 사용하여 사용자 정의 가능한 에이전트 시스템을 구축하는 방법은 여전히 크게 탐구되지 않았습니다. 이 작업에서는 오픈 소스 LLM을 컨트롤러로 사용하는 실제 애플리케이션을 위한 일반적이고 사용자 정의 가능한 에이전트 시스템인 ModelScope-Agent를 제시합니다. ModelScope³는 AI 커뮤니티에서 가장 진보된 머신 러닝 모델을 하나로 모으고 실제 애플리케이션에서 AI 모델을 활용하는 프로세스를 간소화하려는 공개 ML 커뮤니티입니다. ModelScopeAgent는 사용자 정의 가능한 엔진 설계를 통해 유연하고 사용자 친화적인 시스템 라이브러리를 제공합니다. *연락처:<ym119608@alibaba-inc.com> https://github.com/modelscope/modelscope-agent 2https://modelscope.cn/studios/damo/ModelScopeGPT/summary &quot;https://modelscope.cn/models https://github.com/Significant-Gravitas/Auto-GPT https://github.com/langchain-ai/langchain은 여러 오픈소스 LLM에 대한 모델 학습을 지원하는 동시에 모델 API와 일반 API를 통합된 방식으로 원활하게 통합할 수 있습니다. 오픈소스 LLM을 핵심 컨트롤러로 포함하고 도구 사용 모듈과 메모리 모듈과 상호 작용하여 복잡한 작업을 수행하는 LLM 중심 시스템 설계가 특징입니다. ModelScope-Agent의 핵심에서 라이브러리는 LLAMA(Touvron et al., 2023), ChatGLM(THUDM, 2023), ChatPLUG(Tian et al., 2023) 및 기타 사용자 지정 LLM과 같은 다양한 오픈소스 LLM에 대한 유연한 선택 및 학습을 지원합니다. ModelScope. 도구 사용을 위해 ModelScopeAgent는 NLP, CV, 오디오 및 다중 모델 분야에서 다양한 AI 모델 API와 검색 엔진과 같은 방대한 공통 API를 지원하는 기본 도구 라이브러리를 제공합니다. 또한 대규모 도구 라이브러리에서 새로운 자체 정의 API 플러그인과 자동 API 검색을 등록하는 것도 지원합니다. 사용자는 가장 적합한 LLM, 로컬 API 도구 및 기능을 사용자 지정하여 실제 애플리케이션을 개발하기 쉽습니다. 또한 시스템 메시지, 사용자 기록, 컨텍스트 내 예제, 도구 메시지 및 지역화된 지식을 더 잘 저장하고 관리하기 위한 메모리 모듈도 도입되었습니다. 오픈 소스 LLM이 전체 에이전트 시스템을 더 잘 제어할 수 있도록 도구 사용 데이터 수집, 사용자 지정 모델 교육, 평가 및 배포에 대한 포괄적인 프레임워크를 추가로 제안합니다. 특히 다양한 API 범주, 다중 턴 API 호출, API 지향 QA 및 영어와 중국어로 된 API 독립적 지침이 포함된 포괄적인 도구 강화 데이터 세트 MSAgent-Bench를 출시합니다. API 이름과 매개변수 생성의 훈련을 강화하는 Weighted LM의 간단한 훈련 전략은 API 호출의 정확성을 보다 잘 보장하기 위해 사용됩니다. 게다가, 우리 라이브러리에서는 훈련된 모델의 도구 사용 능력을 다양한 측면에서 검사하기 위한 평가 프레임워크도 지원합니다. 나아가, 우리는 ModelScope-Agent를 ModelScope Community의 실제 애플리케이션인 ModelScopeGPT에 적용했는데, 이는 오픈소스 LLM을 1000개 이상의 공개 AI 모델과 연결하고 커뮤니티 QA를 위해 ModelScope에서 지역화된 커뮤니티 지식에 액세스할 수 있습니다. 요약하자면, ModelScope-Agent는 개발자가 오픈소스 LLM의 힘을 활용할 수 있도록 설계된 일반적이고 사용자 정의 가능한 에이전트 시스템입니다. 이 라이브러리는 다음과 같은 목표를 목표로 합니다. • • 오픈소스 LLM 기반 에이전트: ModelScope-Agent의 컨트롤러는 우리의 에이전트 훈련 프레임워크를 통해 최적화된 오픈소스 LLM에서 유연하게 선택할 수 있습니다. 다양한 도구의 지원 및 사용자 정의: 기본적으로 수십 개의 다양한 모델 API와 공통 API가 제공됩니다. 라이브러리는 툴셋에서 새로운 자체 정의 API와 자동 API 검색을 등록하는 것을 지원합니다.• 사용자 정의 가능한 애플리케이션: ModelScopeAgent는 다양한 산업 애플리케이션에 유연하게 적용할 수 있습니다. 에이전트와 교육 프레임워크는 사용, 구성 및 최적화를 설명하는 문서로 작성됩니다. ModelScope-Agent는 ModelScope의 엔지니어가 지속적으로 개발하고 있으며 Apache 2.0 라이선스에 따라 출시됩니다. 전체 문서는 프로젝트 웹사이트에서 제공됩니다. 2 ModelScope Agent ModelScope-Agent는 개발자가 오픈 소스 LLM을 기반으로 사용자 정의 가능한 에이전트 시스템을 구축하는 데 도움이 되도록 설계되었습니다. 전체 시스템 아키텍처는 그림 1에 나와 있습니다. 컨트롤러로 오픈 소스 LLM, 도구 사용 모듈 및 상호 작용할 메모리 모듈이 포함됩니다. 인간의 지시가 주어지면 선택된 LLM을 컨트롤러로 채택하는 Agent는 자동으로 작업을 계획하고, 도구를 선택적으로 사용하고, 메모리의 지식을 활용하고, 마지막으로 사용자에게 유용한 응답을 제공합니다. 2.1 두뇌로서의 LLM LLM은 에이전트의 두뇌 역할을 하며, 사용자 요청을 계획하고 분해하고, 도구를 선택적으로 호출하고, 검색을 수행하고, 이전 단계의 모든 정보를 통합하여 최종 응답을 생성합니다. 사용자가 자신의 LLM으로 에이전트를 더 쉽게 사용자 정의할 수 있도록 기본적으로 LLAMA, ChatGLM 및 ChatPLUG와 같은 다양한 오픈 소스 LLM에 대한 지원을 추가했으며, 이는 도구 학습 파이프라인을 통해 최적화되었습니다. 학습 전략 및 도구 사용 데이터 세트에 대한 자세한 내용은 섹션 3을 참조하세요. ModelScope-Agent는 ModelScope 커뮤니티의 LLM 추론 파이프라인을 통합했으며, model_name 및 model_config를 설정하기만 하면 LLM을 교체할 수 있습니다. model_config에서 model_id, model_revision 및 최대 시퀀스 길이와 같은 모델 매개변수 설정을 구성해야 합니다. 교육 프레임워크 데이터 수집 모델 API 일반 API API 지향 QA ⚫ API 독립적 에이전트 파이프라인 에이전트 실행 도구 검색 LLM 교육 0% 메모리 제어 LLM을 뇌 작업 계획으로 사용 ChatGLM 배포 가중 LM 도구 사용 평가 자동 평가 EM Rouge-L FHuman 평가 API 실행 응답 생성 메모리 제어 지식 검색 프롬프트 생성기 임베딩 시스템 프롬프트 API 스키마 • 지식 시스템 모듈 도구 사용 도구 라이브러리 AI 모델 • 텍스트-이미지 • 텍스트-비디오 • 텍스트-오디오 •이미지 채팅 • 텍스트 번역 • Universal-IE 일반 API • 날씨 웹 검색 • 계산기 • 지도 • 음악 플레이어 쇼핑 도구 검색 도구 사용자 정의 그림 1: ModelScope-Agent의 전체 시스템 아키텍처. # LLM config &quot;cfg_file&quot; from modelscope.utils.config import Config model_cfg = Config.from_file(cfg_file) 11m 로컬 LLM(model_name, model_cfg) 또한 ModelScope-Agent는 새로운 LLM을 통합하는 표준적인 방법도 제공합니다. 사용자는 LLM 파이프라인을 ModelScope에 통합하여 자신의 LLM을 추가할 수 있습니다. 그런 다음 에이전트는 학습 및 추론을 위해 새로운 LLM을 선택할 수 있습니다. 2.2 도구 사용 도구 라이브러리 도구 라이브러리는 에이전트에서 사용되는 다양한 API 컬렉션을 구성하고 관리하는 데 사용됩니다. ModelScope-Agent는 ModelScope 및 HuggingFace의 NLP, CV, 오디오 및 멀티모달 모델에서 검색 API와 같은 일반적인 API와 AI 모델 API를 모두 광범위하게 지원할 수 있습니다. 각 도구 API는 API 이름, 설명, 매개변수 및 요청 함수로 구성됩니다. 사용자는 라이브러리에서 적절한 API를 쉽게 선택하고 구성하여 자신의 에이전트를 빌드할 수 있습니다. 라이브러리에서 지원하는 기본 API는 부록 A.1을 참조할 수 있습니다. #23 도구 기본 구성 파일 &quot;default_file&quot; tool_cfg = Config.from_file(default_file) 새 도구 등록 및 사용자 정의 에이전트를 사용하면 사용자가 새 도구를 등록하고 사용자 정의할 수 있으며, 새로 등록된 도구를 에이전트에 빠르게 통합하여 LLM이 특정 애플리케이션에 대해 추가로 자체 정의된 도구를 선택적으로 사용할 수 있습니다.이는 기본 클래스인 Tool에서 상속하고 API 이름, 설명, 매개변수 및 요청 함수의 API 관련 스키마로 새 CustomTool을 정의하여 간단히 수행할 수 있습니다.Custom Tool에 대한 자세한 내용은 부록 A.2에서 참조할 수 있습니다.from modelscope_agent.tools import Tool class Custom Tool(Tool): #23 여기에 논리 추가 # 부록 A의 예제 참조.tool_list = {&#39;customo-tool&#39;: Custom Tool() } 도구 검색 및 실행 도구 라이브러리에 도구 API가 많기 때문에 각 지침 프롬프트에 적합한 API를 추천하기 위해 도구 검색 모듈이 추가로 도입되었습니다. 구체적으로, 우리는 통합 다국어 텍스트 임베딩 API 6에 기반한 밀집 벡터 검색 방법을 사용합니다. 우리는 텍스트 임베딩 API를 사용하여 API의 텍스트 설명과 지시 프롬프트를 모두 벡터화합니다. 가장 높은 벡터 곱 점수를 가진 상위 3개의 가장 관련성 있는 API가 도구 사용을 위해 선택됩니다. 결과적으로, 검색된 API의 스키마 정보는 후속 메모리 모듈의 다른 시스템 프롬프트와 연결되어 LLM에 입력으로 전송됩니다. 연결된 지시 프롬프트를 사용하여 LLM은 에이전트가 실행할 API 요청을 계획하고 생성합니다. 그런 다음 에이전트는 지속적인 생성을 위해 LLM에 결과를 반환합니다. 2.3 메모리 제어 메모리 모듈은 LLM에 대한 입력으로 일련의 상황 정보를 검색하고 조립하는 데 사용됩니다. 이는 지식 검색 하위 모듈과 프롬프트 생성기 하위 모듈로 구성되며, 각각 외부 지식 검색과 지시 프롬프트 생성을 담당합니다. &quot;https://help.aliyun.com/zh/dashscope/getting-started-Knowledge Retrieval 에이전트가 쿼리 프롬프트와 관련된 최신의 지역화된 정보에 액세스할 수 있도록 하여 LLM을 동적이고 도메인별 지식으로 보강합니다. 이전 도구 검색 모듈과 동일한 고밀도 벡터 검색 방법을 따르고 지역화된 문서 코퍼스에서 대규모 지식 검색을 지원합니다. 마찬가지로 사용자가 다른 오픈 소스 검색 프레임워크로 변경하여 사용자 정의할 수 있습니다. 프롬프트 생성기 프롬프트 생성기는 시스템 프롬프트, API 스키마, 검색된 지식, 대화 기록 및 몇 가지 샷 예제와 같은 모든 사용 가능한 상황 정보를 조립하는 데 사용됩니다. 사용자 쿼리 유형과 LLM의 최대 길이에 따라 사용자는 적절한 상황 정보를 선택적으로 선택하고 LLM에 필요한 입력을 조립할 수 있습니다. 에이전트에서 프롬프트 생성기는 에이전트를 구성하기 전에 정의해야 합니다. 2.4 에이전트 파이프라인 요약하면 모든 모듈을 결합하여 에이전트를 빌드합니다. LLM 컨트롤러, 도구 사용 모듈 및 메모리 모듈. agent.run을 사용하면 에이전트가 한 단계 생성으로 효율적으로 명령을 실행하고 완료할 수 있습니다. 먼저 에이전트는 도구 검색을 통해 쿼리 관련 도구를 검색하고 검색된 API 스키마를 메모리 모듈의 다른 상황별 프롬프트와 결합하여 새로운 명령 프롬프트를 구성합니다. 그런 다음 에이전트는 이 새로운 프롬프트를 LLM에 보내고 LLM은 호출할지 여부와 어떤 API를 호출할지 계획하고 API 요청을 생성합니다. 그런 다음 에이전트는 추출된 API 매개변수로 선택한 API를 실행하고 API 결과를 LLM에 반환하고 LLM은 다른 API를 호출할지 여부를 계속 계획합니다. 다른 API 호출이 필요한 경우 프로세스가 반복되고 그렇지 않으면 LLM이 최종 응답을 생성하고 에이전트는 최종 결과를 사용자에게 반환합니다. agent = AgentExecutor (1lm, tool_cfg, additional_tool_list = tool_list) agent.run(&quot;Draw a logo image of agent&quot;) 3 Training 3.1 Dataset 사용자 참여의 최적 수준을 유지하면서 도구를 사용할 수 있는 에이전트를 구축하기 쉽게 하기 위해 ChatGPT 합성 데이터와 기존 지시 수행 데이터 세트를 활용하는 포괄적인 도구 데이터 세트 MSAgent-Bench7을 릴리스합니다. 릴리스된 데이터 세트에는 598k 대화가 포함됩니다. 표 1은 릴리스된 데이터 세트와 다른 공개적으로 사용 가능한 도구 학습 데이터 세트 간의 주요 차이점을 간략하게 설명하고, 데이터 세트의 데이터 분포는 그림 2에 나와 있습니다. 표와 그림에서 볼 수 있듯이 에이전트의 효과적인 교육을 가능하게 하는 포괄적인 데이터 세트를 구성하기 위해 노력했습니다. 다국어: 중국어와 영어 모두로 인스턴스를 수집하여 교육된 에이전트가 두 언어로 작동할 수 있도록 합니다. 다양한 API 범주: 데이터 세트는 사용자가 등록하거나 온라인 API 플랫폼을 통해 적용한 공통 API와 신경 모델을 호출할 수 있는 모델 API를 지원합니다. 멀티 턴 대화: 실제 상황에서 에이전트는 작업을 완료하거나 이전 작업을 완료한 후 추가 지침을 받기 위해 사용자에게 더 구체적인 설명을 요청해야 할 수 있습니다. 당사 데이터 세트는 이러한 시나리오를 고려하고 도구를 사용할 때 멀티 턴 사용자-에이전트 상호 작용을 지원합니다. API 중심 QA: 효과적인 에이전트는 API에 대한 지식을 보유해야 합니다. 당사 데이터 세트는 API 문서 QA 작업과 작업 계획 작업을 통합하여 에이전트가 복잡한 작업을 해결하기 위해 다양한 API를 사용하는 방법에 대한 적절한 제안을 사용자에게 제공해야 합니다. API에 독립적인 지침: 에이전트가 일반적인 지침을 따르고 사용자 참여를 늘리는 능력을 향상시키기 위해 데이터 세트에 중국어와 영어 API에 독립적인 지침을 모두 통합했습니다. 이러한 지침은 API 호출에 대한 의존성보다는 에이전트의 고유한 기능에 더 큰 중점을 둡니다. 데이터는 ChatGPT(gpt-3.5-turbo)를 프롬프트하여 API 호출 결과에 따라 지침, API 요청 및 답변을 생성하여 수집되었습니다. 자세한 내용은 부록 D에서 확인할 수 있습니다. 3.2 모델 학습 LLAMA(Touvron et al., 2023), Qwen(QwenLM, 2023), ChatPLUG(Tian et al., 2023) 등을 포함하여 여러 오픈소스 LLM을 미세 조정하기 위해 MSAgent-Bench를 사용합니다. 다중 라운드 대화 모드에서 모든 오픈소스 LLM을 학습하고 모든 프롬프트와 답변을 연결합니다. Com&quot;https://modelscope.cn/datasets/damo/MSAgentBench/summary 데이터 세트 언어 인스턴스 유형 인스턴스 수 API 유형 평균 회전 평균 단계 API-Bank(Li et al., 2023) 영어 도구 사용 ToolAlpaca(Tang et al., 2023) 영어 도구 사용 3.9K 일반 API 3.1. 일반 API 1.Gorilla(Patil et al., 2023) 영어 도구 사용 16.4K 모델 API GPT4Tools(Yang et al., 2023) 영어 도구 사용 71.4K 모델 API ToolBench(Qin et al., 2023) MSAgent-Bench(저희) 영어 도구 사용 26.9K 영어 + 중국어 도구 사용 + 일반 채팅 598K 일반 API 일반 API + 모델 API 4.1.1.표 1: MSAgent-Bench 및 기타 기존 도구 학습 데이터 세트의 통계. 모델 API 텍스트-이미지 텍스트-비디오 API 지향 QA 문서 QA 작업 계획 • • 텍스트-오디오 번역 이미지 채팅 범용 IE 공통 API .날씨 웹 검색 계산기 맵 MSAgentBench • API 독립적인 지침 스토리 생성 오픈 QA 코드 잡담 의역 STEM 롤 플레이 그림 2: 수집된 MSAgent-Bench의 인스턴스 유형 및 분포. 일반적인 지침 튜닝 데이터와 비교하여 도구 학습 샘플은 도구 선택의 정확성과 API 매개변수 예측에 더 중점을 둡니다.따라서 API 이름 및 매개변수 생성의 교육을 강화하고 사용자 프롬프트와 도구 실행에서 토큰 손실을 없애는 간단한 교육 전략인 Weighted LM을 제안합니다.자세한 내용은 부록 B.3을 참조하세요.kwargs = dict(model = model, ...) trainer: Epoch Based Trainer = build_trainer는 생성된 응답과 골드 답변 간의 유사성을 측정합니다.또한 API 요청의 품질을 완전히 평가하기 위한 인수 F1이라는 새로운 메트릭을 소개합니다. 인수 F1을 계산하기 위해 에이전트의 API 요청에 있는 인수를 두 가지 경우로 분류합니다. 즉, 올바른 인수이지만 잘못된 값을 갖는 절반 일치(HM)와 올바른 인수이지만 올바른 값을 갖는 완전 일치(FM)입니다. API의 골드 인수 번호가 |A|이고 에이전트 API 요청의 인수 수(name=args.trainer, default_args=kwargs)가 |A*|라고 가정하면 새로운 Recall and Precision 트레이너를 계산합니다. train() 4 평가 평가 시스템인 MSAgent-Eval은 두 가지 모듈로 구성됩니다. 에이전트의 API 사용성을 종합적으로 평가하는 자동 평가 프레임워크와 인간 사용자의 선호도를 반영하는 에이전트 경기장에서 구현된 인간 평가 프레임워크입니다. 4.1 자동 평가 프레임워크 자동 평가에서는 주로 API 호출 결과에 따라 정확한 API 요청과 적절한 답변을 생성하는 에이전트의 능력을 평가하는 데 중점을 둡니다. 구체적으로, 우리는 에이전트가 올바른 API를 참조 골드 API로 사용하는지 여부를 측정하는 정확히 일치 점수(Action EM)와 다음과 같은 ROUGE-L 점수를 사용합니다. R = (0.5 x #HM + # FM)/|A| P = (0.5 × # HM + # FM)/|A*| (1) (2) 그리고 최종 인수 F1은 다음과 같이 계산됩니다. F1 = 2(RP)/(R+P). (3) 에이전트의 자동 평가를 위한 샘플 코드는 아래와 같습니다. from tool_agent_finetune import evaluation EM, F1, ROUGE = evaluation (refs, preds) 전문 주석자는 다양한 지침을 제공하고, 올바른 API 호출 요청을 수동으로 문서화하고, 적절한 응답을 작성하는 작업으로 평가 인스턴스에 주석을 달았습니다. 현재 조립된 테스트 데이터의 통계는 부록 B.1에 있으며, 훈련된 에이전트의 자동 평가 점수는 Vision Pro VR 안경에 대한 짧은 홍보 카피를 20단어로 작성하고 생성된 콘텐츠를 여성 음성으로 읽을 수 있습니까? Vision Pro VR 안경으로 꿈에서나 볼 법한 세상을 경험하세요. &quot;modelscope speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. 0:00/С 이미지를 생성하기 위해 이미지 생성 API를 호출하겠습니다. {그리고 VR 안경 이미지를 그려줄 수 있나요?&quot;modelscope_image-generation&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. -API 등록 [1v 2 &quot;plugin_name&quot;: &quot;RenewInstance&quot;, 3v plugin_schema_for_model&quot;: { &quot;name&quot;: &quot;RenewInstance&quot;, &quot;description&quot;: &quot;ECS 구독 갱신&quot; url: &quot;http://123.56.163.126:8120/&quot;, &quot;paths&quot;: [ &quot;name&quot;: &quot;RenewInstance&quot;, &quot;method&quot;: &quot;post&quot;, &quot;description&quot;: &quot;ECS 구독 갱신&quot; parameters&quot;: [ &quot;name&quot;: &quot;instance_id&quot;, &quot;description&quot;: &quot;ECS 인스턴스 ID, &quot;required&quot;: true&quot;name&quot;: &quot;period&quot;,&quot;description&quot;: &quot;갱신 기간 &quot;required&quot;: true 인스턴스 ID가 irj90a7e840y5csv9nh2a인 ECS 인스턴스를 10개월 동안 갱신하도록 도와주세요. ▸ &quot;RenewInstance&quot; API를 선택하면 현재 API가 실행 중... API 호출이 완료되었습니다. 인스턴스 ID가 irj90a7e840y5csv9nh2a인 ECS 인스턴스를 10개월 동안 갱신하는 데 도움을 드리겠습니다. 갱신 프로세스가 성공적으로 완료되었습니다. 다른 ECS 인스턴스, ID: i-rj90a7e840y5csv9nh4b, 12개월 ▸ &quot;RenewInstance&quot; API를 선택하면 API가 현재 실행 중입니다... ►API 호출이 완료되었습니다. 인스턴스 ID가 irj90a7e840y5csv9nh4b인 ECS 인스턴스를 12개월 동안 갱신하는 데 도움을 드리겠습니다. 갱신 프로세스가 성공적으로 완료되었습니다. 이미지 지우기 보내기 재생성 (a) ModelScope Intelligent Assistant 그림 3: ModelScope-Agent 기반 ModelScopeGPT의 데모 사례. (b) Alibaba Cloud에서 새 도구 등록 및 사용은 부록 B.2에서 확인할 수 있습니다. 또한 사용자가 사용자 지정 시나리오에서 에이전트의 성능을 정확하게 평가하기 위해 자신의 주석이 달린 테스트 예제를 업로드하도록 보장합니다.4.2 Agent Arena를 통한 인간 평가 ChatBots를 위한 Arena(Zheng et al., 2023)에서 영감을 받아 제공된 API를 기반으로 사용자가 두 명의 익명 에이전트에게 지침을 제공할 수 있는 액세스 가능한 Agent Arena를 구축했습니다. 그런 다음 사용자는 제공된 API를 사용하여 지침을 처리하는 데 어느 에이전트가 더 나은 성과를 보이는지 투표할 기회를 갖습니다. Zheng et al.(2023)에서 제시한 프레임워크에 따라 참여 에이전트에 대한 ELO 등급 및 리더보드 유지 관리 시스템을 채택합니다.5 ModelScopeGPT의 사용 예 이 섹션에서는 ModelScope-Agent를 기반으로 ModelScope Community인 ModelScopeGPT9의 성공적인 응용 프로그램을 소개합니다.ModelScope Intelligent Assistant ModelScope-Agent를 기반으로 ModelScope Community를 위한 지능형 어시스턴트인 ModelScopeGPT를 개발했습니다. 이는 LLM을 컨트롤러로 사용하여 ModelScope 오픈소스 커뮤니티에서 수십 개의 도메인별 AI 모델을 연결하며, NLP, CV, 오디오 및 멀티모달 분야를 포함합니다. 파이프라인을 보다 실용적으로 만들기 위해 적절한 API를 자동으로 선택하고 로컬 ModelScope 지식에 액세스할 수 있는 API 검색 및 지식 검색 도구를 포함했습니다. 그림 3a에서 볼 수 있듯이 ModelScopeGPT는 다중 턴 대화에서 API 호출을 지원하고 이전 대화의 정보를 사용하여 올바른 API https://modelscope.cn/studios/LLMZOO/ChineseArena/summary &quot;https://modelscope.cn/studios/damo/ModelScopeGPT /summary 호출 매개변수를 생성할 수 있습니다. 더 많은 사례는 부록 C를 참조할 수 있습니다. 그 결과 ModelScopeGPT는 출시 후 한 달 만에 40,000명의 사용자 방문에서 170,000개가 넘는 총 요청 수를 달성했습니다. 새 도구 등록 및 사용 에이전트의 또 다른 주요 기능은 보이지 않는 API에 대한 일반화 기능입니다. 이를 통해 사용자는 자신의 API를 빠르게 등록하고 특정 애플리케이션을 사용자 정의할 수 있습니다. 따라서 Alibaba Cloud 애플리케이션 시나리오에 적용하여 ModelScopeGPT의 일반화 기능을 테스트합니다. 그림 3b에서 볼 수 있듯이 먼저 Alibaba Cloud에서 ECS 인스턴스를 갱신하기 위한 API를 찾았습니다. 그런 다음 도구 라이브러리에 정의된 API 스키마를 에이전트에 등록했습니다. 마지막으로 &quot;도와주세요&quot;라는 프롬프트를 입력했습니다. 데모에서 ECS를 갱신하세요...&quot;. 에이전트는 계획을 통해 요청을 생성하고, 적절한 API를 선택하고, API를 호출하여 인스턴스를 성공적으로 갱신하고, 갱신이 완료되었음을 사용자에게 알리는 답변을 제공했습니다. 이 테스트는 릴리스된 API 데이터 세트를 기반으로 최적화된 오픈소스 LLM이 보이지 않는 API에 대한 강력한 일반화 능력을 가지고 있음을 보여줍니다. 6 결론 ModelScope-Agent는 유연한 시스템 설계, 데이터 수집, 모델 교육, 평가 및 실제 애플리케이션에서의 사용 예를 포함하는 일반적이고 사용자 정의 가능한 에이전트 프레임워크를 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하는 것을 목표로 합니다. 오픈소스 LLM을 사용하여 에이전트 시스템을 구축하기 위한 AI 에이전트 학습 및 모범 사례를 위한 오픈소스 커뮤니티 중심 라이브러리를 제공합니다. ModelScope-Agent가 AI 에이전트의 새로운 시대로 나아가는 길을 닦는 데 도움이 되기를 바랍니다. 윤리 선언문 의도된 사용. ModelScope-Agent는 일반적이고 사용자 정의 가능한 에이전트 시스템을 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하도록 설계되었습니다. 잠재적인 오용. 도구 사용 데이터 세트로만 훈련하고 특정 데이터 필터링 규칙을 거쳤지만, 사용자 지정 모델이 편향되고, 가짜이며, 안전하지 않은 정보를 생성할 가능성이 여전히 있습니다. 에이전트 프레임워크는 또한 사용자에게 적절한 LLM을 선택하고 훈련을 위해 자신의 깨끗한 데이터를 업로드할 수 있는 자유를 제공합니다. 또한 미래에 에이전트 프레임워크의 안전성을 개선하기 위한 구체적인 방법을 설계하는 것도 중요합니다. 참고문헌 마이클 안, 앤서니 브로한, 노아 브라운, 예브겐 체보타르, 오마르 코르테스, 바이런 데이비드, 첼시 핀, 추위안 푸, 키르타나 고팔라크리슈난, 카롤 하우스만, 알렉스 헤르초크, 다니엘 호, 재스민 쉬, 줄리안 이바르즈, 브라이언 이크터, 알렉스 이르판, 에릭 장, 로사리오 하우레기 루아노, 카일 제프리, 샐리 제스먼스, 니킬 J 조시, 라이언 줄리안, 드미트리 칼라시니코프, 위헝 쿠앙, 쿠앙휴이 리, 세르게이 레빈, 야오 루, 린다 루, 캐롤라이나 파라다, 피터 파스터, 조넬 키암바오, 카니슈카 라오, 자렉 레팅하우스, 디에고 레이예스, 피에르 세르마네, 니콜라스 시버스, 클레이튼 탄, 알렉산더 토셰프, 빈센트 반호우케, 페이 샤, 테드 샤오, 펭 쉬, 쓰춘 쉬, 멍위안 얀, 앤디 젱. 2022. 내가 말하는 대로가 아니라 할 수 있는 대로 하세요: 로봇 어포던스의 언어 기반 마련. arXiv 사전 인쇄 arXiv:2204.01691. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic 등 2023. Falcon40b: 최첨단 성능을 갖춘 개방형 대형 언어 모델. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스, NeurIPS 2020, 2020년 12월 6일-12일, 가상으로 개최. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. CORR, ABS/2204.02311. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training Compute-Optimal Large Language Models. arXiv 사전 인쇄본 arXiv:2203.15556. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Enbodied reasoning through planning with language models. 로봇 학습에 관한 제6차 컨퍼런스의 회의록, 기계 학습 연구 회의록 205권, 1769-1782쪽. PMLR. 허깅페이스. 2023. 트랜스포머 에이전트. 웹사이트. https://huggingface.co/docs/transformers/ transformers_agents. 밍하오 리, 페이판 송, 보웬 유, 하이양 유, 저우준 리, 페이 황, 용빈 리. 2023. Apibank: 도구 증강 llms의 벤치마크. arXiv 사전 인쇄본 arXiv:2304.08244. 니클라스 뮤니히호프, 토마스 왕, 린탕 수타위카, 애덤 로버츠, 스텔라 비더만, 테븐 르 스카오, M 사이풀 바리, 쉔 쉔, 정신 용, 헤일리 쇼엘코프 외. 2022. 멀티태스크 미세 조정을 통한 언어 간 일반화. arXiv 사전 인쇄본 arXiv:2211.01786. OpenAI. 2022. Chatgpt: 대화를 위한 언어 모델 최적화. OpenAI. 2023. abs/2303.08774. GPT-4 기술 보고서. CORR, Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez. 2023. Gorilla: 대규모 API와 연결된 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2305.15334. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu 및 Maosong Sun. 2023. 기초 모델을 사용한 도구 학습. arXiv 사전 인쇄 arXiv:2304.08354. QwenLM. 2023. Qwen-7b. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. arXiv 사전 인쇄본 arXiv:2112.11446. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. 2023. 도구 형성자: 언어 모델은 스스로 도구를 사용하도록 학습할 수 있습니다. arXiv 사전 인쇄본 arXiv:2302.04761. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. 2023. Hugginggpt: Chatgpt와 친구들을 사용하여 hugging face에서 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun. 2023. Toolalpaca: 시뮬레이션된 사례를 사용한 언어 모델을 위한 일반화된 도구 학습. arXiv 사전 인쇄본 arXiv:2306.05301. THUDM. 2023. Chatglm. https://github.com/ THUDM/ChatGLM-6B. Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qinghao Ye, Jiejing Zhang, Ji Zhang, Fei Huang 및 Jingren Zhou. 2023. Chatplug: 디지털 휴먼을 위한 인터넷 증강 명령 튜닝 기능을 갖춘 개방형 도메인 생성 대화 시스템. arXiv 사전 인쇄 arXiv:2304.07849. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄 arXiv:2302.13971. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang 및 Nan Duan. 2023. Visual chatgpt: 시각적 기초 모델을 사용하여 말하고, 그리고, 편집합니다. arXiv 사전 인쇄 arXiv:2303.04671. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li 및 Ying Shan. 2023. Gpt4tools: 자가 학습을 통해 도구를 사용하도록 대규모 언어 모델 교육. arXiv 사전 인쇄 arXiv:2305.18752. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv 사전 인쇄본 arXiv:2306.05685. A 라이브러리 A.1 도구 목록 API 이름(언어) 텍스트-이미지(en) 텍스트-이미지(zh) 텍스트-비디오(en) 텍스트-오디오(en) 텍스트-오디오(zh) 이미지-채팅(en) 번역-zh2en 번역-en2zh Universal-IE(zh) 텍스트-지리(zh) NER(zh) API-검색 모델 범위-검색 설명 텍스트를 이미지로 변환합니다. 텍스트를 이미지로 변환합니다. 텍스트를 비디오로 변환합니다. туре 모델 API 모델 API 데이터 세트는 에이전트 예측과 비교할 참조로 텍스트 스니펫이 있는 360개의 대화로 구성되었으며, 이는 이전 호출 결과에 따른 798개의 API 요청과 1261개의 일반 텍스트 답변으로 구성됩니다. 텍스트를 오디오로 변환합니다. 텍스트를 오디오로 변환합니다. 이미지 채팅. 중국어 텍스트를 영어로 번역합니다. 영어 텍스트를 중국어로 번역합니다. 구조화된 정보를 추출합니다. 지리적 정보를 추출합니다. 텍스트에서 명명된 엔터티를 인식합니다. 관련 API를 검색합니다. 모델스코프 문서를 검색합니다. 모델 API 모델 API 모델 API 모델 API B.2 평가 결과 모델 API 모델 API 모델 API 모델 API 모델 ROUGE-L 모델 API 공통 API 공통 API 동작 EM 인수 FChatGPT(2-shot)* 36.34.25.LLAMA 39.58.44.ChatPLUG¹¹ MSAgent-Qwen¹ 46.68.55.51.87.68.표 2: 기본 도구 목록의 통계. API에 지원되는 입력 언어는 괄호 안에 나열되어 있습니다. A. 사용자 정의 도구 사용자는 기본 도구를 상속하고 미리 정의된 스키마에 따라 도구 이름, 설명 및 매개변수를 정의하여 고유한 도구를 사용자 정의할 수 있습니다. 또한 특정 요구 사항에 따라 _local_call() 또는 _remote_call()을 구현할 수 있습니다. 예를 들어, 아래는 사용자 정의 도구의 예입니다. class Custom Tool (Tool): description = &#39;xxx&#39; name = &#39;XXX&#39; parameters: list = }] &#39;name&#39;: &#39;XXX&#39; [ { &#39;description&#39;: &#39;xxx&#39;, &#39;required&#39;: True def _local_call(): def _remote_call(): 표 3: 자동 평가 결과. *는 ChatGPT를 미세 조정하지 않고 2개의 데모를 통해 컨텍스트 내 학습을 사용한다는 것을 나타냅니다. 제안한 ModelScopeGPT에서 학습한 모델을 비교합니다. 자동화 평가 결과는 표 3에 나와 있습니다. 우리의 결과를 바탕으로
--- CONCLUSION ---
ModelScope-Agent는 유연한 시스템 설계, 데이터 수집, 모델 교육, 평가 및 실제 응용 프로그램에서의 사용 예를 포함하는 일반적이고 사용자 정의 가능한 에이전트 프레임워크를 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하는 것을 목표로 합니다. 오픈소스 LLM을 사용하여 에이전트 시스템을 구축하기 위한 AI 에이전트 학습 및 모범 사례를 위한 오픈소스 커뮤니티 중심 라이브러리를 제공합니다. ModelScope-Agent가 AI 에이전트의 새로운 시대로 나아가는 길을 닦는 데 도움이 되기를 바랍니다. 윤리 선언문 의도된 사용. ModelScope-Agent는 일반적이고 사용자 정의 가능한 에이전트 시스템을 제공함으로써 오픈소스 LLM을 기반으로 AI 에이전트 애플리케이션과 연구를 구축하는 것을 용이하게 하도록 설계되었습니다. 잠재적 오용. 도구 사용 데이터 세트로만 교육하고 특정 데이터 필터링 규칙을 거쳤지만 사용자 정의 모델이 편향되고 가짜이며 안전하지 않은 정보를 생성할 가능성이 여전히 있습니다. 에이전트 프레임워크는 또한 사용자에게 적절한 LLM을 선택하고 교육을 위해 자체 정리된 데이터를 업로드할 수 있는 자유를 제공합니다. 또한 향후 에이전트 프레임워크의 안전성을 개선하기 위한 구체적인 방법을 설계하는 것도 중요합니다. 참고문헌 마이클 안, 앤서니 브로한, 노아 브라운, 예브겐 체보타르, 오마르 코르테스, 바이런 데이비드, 첼시 핀, 추위안 푸, 키르타나 고팔라크리슈난, 카롤 하우스만, 알렉스 헤르초크, 다니엘 호, 재스민 쉬, 줄리안 이바르즈, 브라이언 이크터, 알렉스 이르판, 에릭 장, 로사리오 하우레기 루아노, 카일 제프리, 샐리 제스먼스, 니킬 J 조시, 라이언 줄리안, 드미트리 칼라시니코프, 위헝 쿠앙, 쿠앙휴이 리, 세르게이 레빈, 야오 루, 린다 루, 캐롤라이나 파라다, 피터 파스터, 조넬 키암바오, 카니슈카 라오, 자렉 레팅하우스, 디에고 레이예스, 피에르 세르마네, 니콜라스 시버스, 클레이튼 탄, 알렉산더 토셰프, 빈센트 반호우케, 페이 샤, 테드 샤오, 펭 쉬, 쓰춘 쉬, 멍위안 얀, 앤디 젱. 2022. 내가 말하는 대로가 아니라 할 수 있는 대로 하세요: 로봇 어포던스의 언어 기반 마련. arXiv 사전 인쇄 arXiv:2204.01691. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic 등 2023. Falcon40b: 최첨단 성능을 갖춘 개방형 대형 언어 모델. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스, NeurIPS 2020, 2020년 12월 6일-12일, 가상으로 개최. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. CORR, ABS/2204.02311. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training Compute-Optimal Large Language Models. arXiv 사전 인쇄본 arXiv:2203.15556. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Enbodied reasoning through planning with language models. 로봇 학습에 관한 제6차 컨퍼런스의 회의록, 기계 학습 연구 회의록 205권, 1769-1782쪽. PMLR. 허깅페이스. 2023. 트랜스포머 에이전트. 웹사이트. https://huggingface.co/docs/transformers/ transformers_agents. 밍하오 리, 페이판 송, 보웬 유, 하이양 유, 저우준 리, 페이 황, 용빈 리. 2023. Apibank: 도구 증강 llms의 벤치마크. arXiv 사전 인쇄본 arXiv:2304.08244. 니클라스 뮤니히호프, 토마스 왕, 린탕 수타위카, 애덤 로버츠, 스텔라 비더만, 테븐 르 스카오, M 사이풀 바리, 쉔 쉔, 정신 용, 헤일리 쇼엘코프 외. 2022. 멀티태스크 미세 조정을 통한 언어 간 일반화. arXiv 사전 인쇄본 arXiv:2211.01786. OpenAI. 2022. Chatgpt: 대화를 위한 언어 모델 최적화. OpenAI. 2023. abs/2303.08774. GPT-4 기술 보고서. CORR, Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez. 2023. Gorilla: 대규모 API와 연결된 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2305.15334. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu 및 Maosong Sun. 2023. 기초 모델을 사용한 도구 학습. arXiv 사전 인쇄 arXiv:2304.08354. QwenLM. 2023. Qwen-7b. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. arXiv 사전 인쇄본 arXiv:2112.11446. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. 2023. 도구 형성자: 언어 모델은 스스로 도구를 사용하도록 학습할 수 있습니다. arXiv 사전 인쇄본 arXiv:2302.04761. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. 2023. Hugginggpt: Chatgpt와 친구들을 사용하여 hugging face에서 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun. 2023. Toolalpaca: 시뮬레이션된 사례를 사용한 언어 모델을 위한 일반화된 도구 학습. arXiv 사전 인쇄본 arXiv:2306.05301. THUDM. 2023. Chatglm. https://github.com/ THUDM/ChatGLM-6B. Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qinghao Ye, Jiejing Zhang, Ji Zhang, Fei Huang 및 Jingren Zhou. 2023. Chatplug: 디지털 휴먼을 위한 인터넷 증강 명령 튜닝 기능을 갖춘 개방형 도메인 생성 대화 시스템. arXiv 사전 인쇄 arXiv:2304.07849. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄 arXiv:2302.13971. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang 및 Nan Duan. 2023. Visual chatgpt: 시각적 기초 모델을 사용하여 말하고, 그리고, 편집합니다. arXiv 사전 인쇄 arXiv:2303.04671. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li 및 Ying Shan. 2023. Gpt4tools: 자가 학습을 통해 도구를 사용하도록 대규모 언어 모델 교육. arXiv 사전 인쇄 arXiv:2305.18752. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv 사전 인쇄본 arXiv:2306.05685. A 라이브러리 A.1 도구 목록 API 이름(언어) 텍스트-이미지(en) 텍스트-이미지(zh) 텍스트-비디오(en) 텍스트-오디오(en) 텍스트-오디오(zh) 이미지-채팅(en) 번역-zh2en 번역-en2zh Universal-IE(zh) 텍스트-지리(zh) NER(zh) API-검색 모델 범위-검색 설명 텍스트를 이미지로 변환합니다. 텍스트를 이미지로 변환합니다. 텍스트를 비디오로 변환합니다. туре 모델 API 모델 API 데이터 세트는 에이전트 예측과 비교할 참조로 텍스트 스니펫이 있는 360개의 대화로 구성되었으며, 이는 이전 호출 결과에 따른 798개의 API 요청과 1261개의 일반 텍스트 답변으로 구성됩니다. 텍스트를 오디오로 변환합니다. 텍스트를 오디오로 변환합니다. 이미지 채팅. 중국어 텍스트를 영어로 번역합니다. 영어 텍스트를 중국어로 번역합니다. 구조화된 정보를 추출합니다. 지리적 정보를 추출합니다. 텍스트에서 명명된 엔터티를 인식합니다. 관련 API를 검색합니다. 모델스코프 문서를 검색합니다. 모델 API 모델 API 모델 API 모델 API B.2 평가 결과 모델 API 모델 API 모델 API 모델 API 모델 ROUGE-L 모델 API 공통 API 공통 API 동작 EM 인수 FChatGPT(2-shot)* 36.34.25.LLAMA 39.58.44.ChatPLUG¹¹ MSAgent-Qwen¹ 46.68.55.51.87.68.표 2: 기본 도구 목록의 통계. API에 지원되는 입력 언어는 괄호 안에 나열되어 있습니다. A. 사용자 정의 도구 사용자는 기본 도구를 상속하고 미리 정의된 스키마에 따라 도구 이름, 설명 및 매개변수를 정의하여 고유한 도구를 사용자 정의할 수 있습니다. 또한 특정 요구 사항에 따라 _local_call() 또는 _remote_call()을 구현할 수 있습니다. 예를 들어, 아래는 사용자 정의 도구의 예입니다.class Custom Tool (Tool): description = &#39;xxx&#39; name = &#39;XXX&#39; parameters: list = }] &#39;name&#39;: &#39;XXX&#39; [ { &#39;description&#39;: &#39;xxx&#39;, &#39;required&#39;: True def _local_call(): def _remote_call(): 표 3: 자동 평가 결과.*는 ChatGPT를 미세 조정하지 않고 2개의 데모를 통해 컨텍스트 내 학습을 사용한다는 것을 나타냅니다.제안한 ModelScopeGPT에서 학습한 모델을 비교합니다.자동화 평가 결과는 표 3에 나와 있습니다.실험에서 얻은 결과에 따르면 컨텍스트 내 학습을 적용한 ChatGPT는 미세 조정을 거친 다른 모델에 비해 열등한 결과를 보였습니다.또한 LLAMA는 다른 미세 조정 모델에 비해 성능이 떨어졌습니다.오류 연구에 따르면 ChatGPT와 LLAMA의 성능이 낮은 것은 테스트 세트에서 중국어 테스트 사례가 많은 비중을 차지하기 때문일 수 있습니다. 더 나은 성과를 보인 모델(ChatPLUG, Qwen)은 주로 중국어 데이터에 초점을 맞춘 모델이었습니다. 저희 조사에 따르면 ChatGPT와 LLAMA는 사용자 의도 인식에 한계가 있어 궁극적으로 Action EM에서 최적이 아닌 성능을 보였습니다. 검토한 모델 중에서 Qwen이 가장 좋은 성과를 보였는데, 이는 기본 모델의 우수한 성능에 기인할 수 있습니다. B 실험 설정 평가 벤치마크 B. 훈련된 에이전트의 일반화를 평가하기 위해 ModelScope-Agent의 훈련 세트에 나타나는 10개의 도메인 내 API와 실제로 보이지 않는 10개의 API를 포함했습니다. 10. 또한 평가 벤치마크에서 여러 다중 턴 시나리오에 주석을 달아 에이전트의 다중 턴 능력을 고려했습니다. 저희의 테스트 인스턴스는 인간 전문가에게 먼저 다양한 지침을 작성하도록 요청하여 주석을 달았습니다. 그런 다음 인간 전문가에게 JSON API 요청을 작성하고 API 호출 결과를 얻은 후 지침에 적절하게 대답하도록 요청했습니다. 저희의 최종 테스트 10 진행 중이며, 앞으로 더 많은 API를 포함할 것입니다. B.3 가중 LM 우리는 훈련 전략인 가중 LM의 예를 들어보겠습니다. 그림 4에서 볼 수 있듯이, 색상이 다른 토큰은 손실 가중치가 다릅니다. 사용자 입력 프롬프트의 경우 손실 가중치를 0으로 설정하여 모델이 프롬프트에 대한 손실을 계산하지 않도록 합니다. 어시스턴트의 API-Agnostic 텍스트의 경우 손실 가중치를 1로 유지합니다. 마지막으로 API 이름, 매개변수, URL 등과 같은 API 호출의 중요한 텍스트의 경우 손실 가중치를 2로 설정하여 API 호출의 생성 정확도를 향상시킬 수 있습니다. 사용자: VR 안경에 대한 간략한 소개를 10단어로 작성하세요. 어시스턴트: 몰입형 가상 현실 경험을 위한 웨어러블 기기입니다. 사용자: 여성 목소리로 큰 소리로 읽어주세요. 어시스턴트<startofthink> { &quot;api_name&quot;: &quot;modelscope_speech-generation&quot;, &quot;url&quot;: &quot;http://33.57.174.141:5000/damo/speech samberthifigan tts zh-cn 16k&quot;, &quot;parameters&quot;:{&quot;text&quot;: &quot;몰입형 가상 현실 경험을 위한 웨어러블 디바이스&quot;, &quot;gender&quot;: &quot;여성&quot;}}<endofthink><starofexec> {&quot;결과&quot;:&quot;<audio id=audio controls= preload=none><source id=wav src=ursfe2.wav></audio> &quot;}<endofexec><audio id audio controls= preload=none><source id=wav src=ursfe2.wav></audio> 손실 가중치 0.1.2.그림 4: 가중 LM에 대한 훈련 전략의 예. 다른 색상의 토큰은 다른 손실 가중치를 갖습니다. 비디오 생성: 두 마리의 고양이가 놀고 있습니다. ModelScope의 비디오 생성 모델 API를 호출하여 비디오를 생성합니다. ▸ &quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 선명한 이미지 生成一两个猫는 玩耍的视频 國目前 ModelScope 只有支持英文视频生成模型, 所以我将首先把您的问题利 用 ModelScope的中英翻译模型转化为英文,然后再调用英文的视频生成模型 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 현재 API가 실행 중입니다... ► API 호출이 완료되었습니다. 번역 후의 단추: 두 마리 고양이가 놀고 있는 비디오 调用视频生成模型生成视频……………… ▶ &quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다... ► API 호출이 완료되었습니다. send regenerate clear image send regenerate 그림 5: 단일 단계 도구 사용 지침, 텍스트-비디오 사례. 표시할 비디오의 몇 프레임을 캡처했습니다. 영어(왼쪽)와 중국어(오른쪽)에서 동일한 의미적 지침을 사용하여 모델을 테스트합니다. ModelScope의 멀티모달 대화 상자 모델 API를 호출합니다. &lt;-&gt; 이 그림에는 무엇이 있습니까? ▸ &quot;modelscope_image-chat&quot; API를 선택하면 현재 API가 실행 중입니다... ►API 호출이 완료되었습니다. 이미지는 파란색과 보라색 픽셀화된 얼굴과 크고 흰 눈을 특징으로 합니다. 현재 ModelScope는 영어와 호환되는 다국어 ChatGPT 모델의 LLM만 지원하며, 이 모델의 이름은 mPLUG-owl입니다. 따라서 먼저 ModelScope의 중국어-영어 번역 모델을 사용하여 프롬프트를 영어로 변환한 다음 영어 mPLUG-wl 모델을 호출합니다. 중국어-영어 번역 모델을 호출하여 프롬프트를 영어로 변환합니다. &lt;-&gt; 이전에 ModelScope에 대해 더 많은 정보가 있는 ChatGPT의 LLM, 이름은 mPLUG-owl, 더 보기的中英翻译模型转 化为英文,然后再调用英文的 mPLUG-Owl 模型 调用中英翻译模型转化 프롬프트为英文 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 API가 현재 실행 중입니다... ►API 호출에 완료되었습니다. N: 이 사진에는 무엇이 있나요? 然後调用 mPLUG-owl 반환 결과 ►&quot;modelscope_image-chat&quot; API를 선택하면 현재 API가 실행 중입니다... ►API 호출이 완료되었습니다.이미지에는 만화처럼 보이는 파란색과 보라색 픽셀화된 얼굴이 있습니다.clear image send regenerate clear image send regenerate 그림 6: 단일 단계 도구 사용 지침, 이미지 채팅 사례.영어(왼쪽)와 중국어(오른쪽) 모두에서 동일한 의미적 지침을 사용하여 모델 테스트.C 사례 이 섹션에서는 ModelScope-Agent를 기반으로 하는 ModelScopeGPT 구현에 대한 정성적 결과를 보여줍니다.단일 단계 도구 사용 그림 5와 6에서 볼 수 있듯이 지침은 모델이 각각 비디오를 생성하고 이미지에 대해 채팅할 것으로 예상합니다.이러한 지침은 단일 단계의 도구 사용으로 완료할 수 있습니다.다단계 도구 사용 그림 7에서 볼 수 있듯이 지침은 모델이 먼저 홍보 카피를 작성한 다음 읽고 마지막으로 비디오를 생성할 것으로 예상합니다. 영어: 이러한 지침은 모델이 여러 단계의 도구 사용 기능을 가지고 있어야 합니다. 중국어 사례에서, 우리 모델은 3단계 도구 사용을 정확하게 완료했습니다. 여러 차례의 도구 사용 그림 8에서 볼 수 있듯이, 지침은 모델이 여러 차례의 대화를 하고 과거 대화를 사용할 수 있어야 합니다. 우리 모델은 API를 정확하게 호출하고 이전 대화의 내용을 캡처하여 API 매개변수를 생성할 수 있습니다. Vision Pro VR에 대한 20자 홍보 문구를 쓰고, 여성 음성으로 읽게 한 다음, 영상을 만들어서 어떻게 보이는지 확인하세요. Vision Pro VR로 완전히 새로운 세상을 경험하세요. 한계에 작별인사를 하고 생생하고 사실적인 영상에 푹 빠져보세요. 오늘 이 최첨단 안경을 손에 넣으세요! ►&quot;modelscope_speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다.... API 호출이 완료되었습니다. ► 0:00 / 0:►&quot;modelscope_video-generation&quot; API를 선택하면 현재 API가 실행 중입니다.... API 호출이 완료되었습니다. Vision Pro VR 안경: 사실적인 장면, 몰입감 넘치는 경험, 특별한 감각! ② Vision Pro VR 안경: 진실된 공간, 신체와 정신, 초강체!! ►&quot;modelscope.speech-generation&quot; API를 선택하면 현재 API가 실행 중입니다.. API 호출이 완료되었습니다. 0:00/0:현재 ModelScope는 영어에 대한 비디오 생성 모델만 지원하므로 먼저 ModelScope의 영어-중국어 번역 모델을 사용하여 질문을 영어로 번역한 다음 영어 비디오 생성 모델을 호출합니다.目前 ModelScope 只有支持英文的视频生成模型,所以我将首先把您的问题利用 Modelscope 的中英翻译模型转化为英文,然后再调用英文的视频生成模型 ▸ &quot;modelscope_text-translation-zh2en&quot; API를 선택하면 현재 API가 실행중입니다... API 호출이 완료되었습니다. EN: Vision Pro VR 안경: 실제 장면, 몰입감, 특별한 경험! 번역된 지침 비디오 생성 모델을 호출하여 비디오를 생성합니다. 调用视频生成模型生成视频中 &quot;modelscope_video- Generation&quot; API를 선택하시면 현재 API가 실행중입니다... API 호출이 완료되었습니다. send regenerate send Fegenerate clear image clean image 그림 7: 다단계 도구 사용 지침. 비디오의 몇 프레임을 캡처하여 표시했습니다. 영어(왼쪽)와 중국어(오른쪽) 모두에서 동일한 의미 지침을 사용하여 모델을 테스트합니다. 20개 단어만 사용하여 간단한 스토리를 작성합니다. 20자 写一个 20 字左右简短小故事 도시에서 길을 잃은 그는 그녀를 만났습니다. 그들은 함께 역경을 극복하고 집으로 돌아가는 길을 찾았습니다.从前,有一只聪밝은狐狸,它善于骗取其他动동물적식물.一天,它看到了一只可爱的 小兔子,于是它决定装成受伤的样子,向小兔子求救。 小兔子心地善良,把狐狸带回家给它治疗。狐狸趁机逃走,并骗取了小兔子的먹물。 여자 목소리로 이야기 읽기 &quot;modelscope_speech- Generation&quot; API를 선택하면 현재 API가 실행되고 있습니다... 옛날 옛적에 시간, 다른 동물을 속여 음식을 얻는 데 능숙한 영리한 여우가 있었습니다. 어느 날, 귀여운 토끼를 보고 다친 척하고 토끼에게 도움을 요청하기로 했습니다. 친절한 토끼는 여우를 집으로 데려가 부상을 치료했습니다. 그러나 여우는 상황을 이용하여 도망가 토끼의 음식을 훔쳤습니다. 用女生念出来 API 호출이 완료되었습니다. 0:00/0:►&quot;modelscope_speech-generation&quot; API를 선택하는 중, 현재 API가 실행 중입니다... API 호출이 완료되었습니다. ► 0:00 / 0:그리고 스토리 그림을 그려 주시겠습니까? ►&quot;modelscope_image-generation&quot; API를 선택하는 중, 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 명확한 이미지 给这个故事配一张图 好的,我将调用 ModelScope 魔搭社区的图文生成模型为您作画 ►&quot;modelscope_image-generation&quot; API를 선택하시면 현재 API가 실행중입니다... API 호출이 완료되었습니다. 보내기 재생성 선명한 이미지 보내기 재생성 그림 8: 다중 회전 도구 사용 지침, 텍스트 음성 변환 및 텍스트 이미지 변환 사례. 영어(왼쪽)와 중국어(오른쪽) 모두에서 동일한 의미론적 명령을 사용하여 모델을 테스트합니다. 介绍下魔搭社区 ModelScope 소개 ►&quot;modelscope_search&quot; API를 선택하시면 현재 API가 실행중입니다... ►API 호출이 완료되었습니다. ModelScope 社区成立于2022년 6월,是一个模型开源社区及创新平台,由阿里巴巴达摩院,联合 CCF开源发展委员会,共同작동작为项目发起方. ModelScope 커뮤니티는 2022년 6월에 다음과 같이 설립되었습니다. 모델 개발을 위한 오픈 소스 커뮤니티 및 혁신 플랫폼입니다. 이는 Alibaba DAMO Academy와 중국 컴퓨터 연맹(CCF) 오픈 소스 개발 위원회에 의해 시작되었습니다. 커뮤니티가 협력합니다. ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. ModelScope에는 어떤 모델이 있나요? 魔搭社区是一个开源的模型开源社区, 旨재통용회사开源社区, 构建深titudeschool习开源,并开源源, 并开源源源关模型服务创新技术,推动模型应用生态的繁荣发記.이제 魔搭社区上, 你可以找到许多领域多模型, 例如自 ModelScope는 개방형 커뮤니티 협력을 통해 오픈 소스 딥 러닝 모델을 구축하는 것을 목표로 하는 오픈 소스 모델 개발 커뮤니티입니다. 또한 모델 서비스와 관련된 혁신적인 기술을 오픈소싱하여 모델 애플리케이션 생태계의 성공적인 발전을 촉진합니다. &quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 음성 합성 모델을 사용할 수 있나요? 확실히, 일관되고 리드미컬한 음성을 생성하는 데 탁월한 SAMBERT 모델을 사용하는 것이 좋습니다. 음성 합성 모델 링크 좀 주세요 ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 魔搭社区有开放域对话模型吗 ModelScope에 공개 도메인 대화 상자가 있습니까? ►&quot;modelscope.search&quot; API를 선택하면 현재 API가 실행 중입니다... API 호출이 완료되었습니다. 当然有,我们有 ChatPLUG 模型,它可以接收用户对话历史and对话关的외부지식, 用户人设, 模型人设예, 우리는 사용자의 대화 기록, 대화와 관련된 외부 지식, 사용자 페르소나 및 모델 페르소나 정보를 가져와 적절한 응답을 출력할 수 있는 ChatPLUG 모델을 보유하고 있습니다. 사용 가능한 코드가 있나요? ►&quot;modelscope_search&quot; API를 선택하면 현재 API가 실행 중입니다. API 호출이 완료되었습니다. 확실히, 우리는 이 모델에 대한 귀하의 필요와 실제 상황에 따라 사용할 수 있는 샘플 코드를 제공합니다. 예를 들어: modelscope.pipelines의 예제 코드 import Pipelines from modelscope.utils.constant import Tasks from modelscope.models import Model model_id = &#39;damo/ChatPLUG-3.7B 파이프라인_ins 파이프라인(Tasks.fid_dialogue, model-model_id,model_revision=&#39;v1.preprocess_params = { &#39;max_encoder_length&#39;: 380, 인코더 KAKE发个语음을합형성模型链接 &#39;context_turn&#39;: 3 # context input str = &#39;Q:我想去北京旅游,有什么好的推荐吗? result Pipeline_ins (입력, **kwargs) print (결과) 한국어: https://www.modelscope.cn/models/damo/speech_sambert-hifigan_tts_zhishuo_zhcn 16k/summary. clear image send regenerate image send regenerati 그림 9: 멀티 턴 도구 사용 지침, 텍스트-음성 및 텍스트-이미지 사례. 영어(왼쪽)와 중국어(오른쪽)에서 동일한 의미적 지침을 사용하여 모델 테스트. 도메인 내 지식 QA 그림 9에서 볼 수 있듯이 지침은 모델이 도메인 내 지식을 검색하고 검색된 지식을 사용하여 질문에 답하도록 요구합니다. 사용자로서의 지침 또는 설명 API 요청 에이전트로서의 API 갤러리 EE CAP 후속 또는 최종 답변 루프는 에이전트가 최종 답변으로 대화를 종료하는 것이 적절하다고 판단할 때까지 계속됩니다. 원시 데이터 세트를 획득한 후, ChatGPT가 검색된 API에 없는 환각적인 API 이름과 매개변수를 포함하는 API 요청을 생성하는 인스턴스를 제거하기 위해 필터링 메커니즘을 적용했습니다. 또한 ChatGPT가 불법 API 요청을 생성하는 인스턴스를 제외하여 정제되고 최종화된 데이터 세트를 생성했습니다.섹션 3.1에서 소개한 대로 다양한 언어와 주제에 걸쳐 인스턴스를 수집하며 수집된 데이터의 자세한 통계는 표 4에 나와 있습니다.결과 인스턴스 유형 중국어 # 인스턴스 532, 영어 66, 일반 API 211, 모델 API 58, 5, 329, 그림 10: MSAgentBench의 데이터 수집 절차.D 데이터 수집 절차 프롬프트 엔지니어를 사용하여 두 개의 ChatGPT(gpt-3.5-turbo)로 에이전트 시나리오를 시뮬레이션하여 데이터 세트를 수집했습니다.하나의 ChatGPT는 사용자 역할을 하도록 프롬프트되었고 다른 하나는 에이전트 역할을 하도록 할당되었습니다.기존의 실제 API가 아닌 교육 데이터에 제시된 API의 도메인과 기능을 확장하기 위해 ChatGPT에서 생성한 여러 합성 API도 포함했습니다. 이러한 합성 API가 대화에 통합되었을 때, 우리는 다른 ChatGPT가 API 역할을 하고 관련 호출 결과를 반환하도록 했습니다. 데이터 수집 절차는 그림 10에 나와 있습니다. 처음에는 일련의 무작위 컨텍스트 데모가 ChatGPT에 제공되어 명령을 생성했습니다. 이 명령은 제공된 데모에 따라 일반적인 명령이거나 API로 해결해야 하는 명령일 수 있습니다. 그 후, ChatGPT는 먼저 어떤 작업을 수행할지 생각하여 에이전트 역할을 하도록 촉구되었습니다. API 호출이 필요하지 않거나 사용자 설명이 필요한 경우 에이전트는 사용자에게 후속 응답으로 응답합니다. 그렇지 않으면 에이전트는 API 갤러리에 API 요청을 보냅니다. API 호출 결과를 받은 후 에이전트는 상황을 평가하고 다음 작업을 결정합니다. &quot;사용자-에이전트-API&quot; E API 지향 QA API 독립적 명령 표 4의 이 반복적 프로세스: 수집된 데이터 세트의 통계. 관련 연구 E.1 대규모 언어 모델 최근 몇 년 동안 대규모 언어 모델(LLM) 분야에서 급속한 발전이 있었습니다. GPT3(Brown 등, 2020), Gopher(Rae 등, 2021), Chinchilla(Hoffmann 등, 2022), PaLM(Chowdhery 등, 2022) 및 LLAMA(Touvron 등, 2023)와 같은 전형적인 모델은 모델과 데이터 크기를 확장하여 광범위한 NLP 작업에서 인상적인 제로샷 및 퓨샷 일반화 능력을 보여주었습니다. 주목할 만한 이정표는 AI 개발의 패러다임을 크게 혁신한 ChatGPT(OpenAI, 2022) 또는 GPT4(OpenAI, 2023)의 출시입니다. 그 결과, ChatGPT 및 Claude와 같은 폐쇄형 소스 대응 제품에 도전하고 따라잡기 위해 오픈 소스 LLM의 상승 추세가 나타났습니다. 여기에는 BLOOM(Muennighoff 등, 2022), LLAMA(Touvron 등, 2023), Falcon(Almazrouei 등, 2023), ChatGLM(THUDM, 2023)이 있습니다. 획기적인 진전에도 불구하고 LLM은 일반 텍스트 코퍼스를 통한 텍스트 생성기로 훈련되므로 멀티모달 작업과 같은 다른 작업에서는 성능이 떨어집니다. 또한 사전 훈련 데이터를 넘어서는 최신 정보가 필요한 작업에서도 성능이 떨어집니다. 도구나 외부 API를 사용하면 한계를 극복하고 LLM의 힘을 활용하여 다운스트림 애플리케이션과 원활하게 연결할 수 있습니다. ModelScope-Agent에서는 오픈 소스 LLM이 도구와 외부 API를 사용할 수 있도록 하는 에이전트 시스템을 구축하기 위한 전체 사용자 정의 프레임워크와 모범 사례를 제공합니다. E.2 에이전트 및 도구 학습 에이전트 시스템을 구성하기 위한 컨트롤러로 대규모 언어 모델(LLM)을 활용하는 것이 중요한 연구 분야로 부상했습니다. 여러 관련 연구에서는 ChatGPT(OpenAI, 2022) 및 Claude와 같은 폐쇄형 소스 LLM에 대한 신속한 엔지니어링 기술을 사용하여 특정 도메인에 적용할 수 있도록 합니다. 예를 들어, VisualChatGPT(Wu et al., 2023) 및 HuggingGPT(Shen et al., 2023)는 OpenAI LLM에서 사용할 수 있는 HuggingFace 모델 호출을 용이하게 합니다. SayCan(Ahn et al., 2022) 및 내면 독백(Huang et al., 2023)은 로봇과 LLM을 통합하여 로봇 시스템을 구현합니다. 특히 Langchain 및 Auto-GPT와 같은 최근 연구는 공통 API 및 신경 모델을 포함한 광범위한 도구를 포함하고 작업을 해결하는 동안 장기 추론 및 인간-에이전트 상호 작용을 향상시켜 일반화된 에이전트를 구축할 수 있는 엄청난 잠재력을 보여줍니다. 오픈소스 LLM이 도구를 활용할 수 있도록 하기 위한 수많은 노력도 이루어졌습니다. 예를 들어, Gorilla(Patil 등, 2023)와 GPT4Tools(Yang 등, 2023)는 자기 학습 기법을 사용하여 학습 데이터를 생성하여 오픈소스 LLM이 신경 모델을 효과적으로 활용하도록 학습시킵니다. ToolAlpaca(Tang 등, 2023)와 ToolLLaMA(Qin 등, 2023)는 공통 API를 사용하여 LLAMA를 학습시키는데, ToolAlpaca는 LLMS의 합성 API를 사용하는 반면 ToolLLAMA는 실제 API를 활용한다는 점이 다릅니다. 전반적으로 위에서 언급한 방법과 비교했을 때 ModelScope-Agent는 다음과 같은 측면에서 다릅니다. 첫째, 우리의 방법은 산업적 요구를 충족시키기 위해 오픈소스 모델에 대한 사용자 정의 에이전트 학습을 지원하는 범용 학습 프레임워크를 포함합니다. 둘째, ModelScope-Agent는 모델 API와 공통 API를 포함하여 다양한 분야의 다양한 API를 지원할 수 있는 반면, 이전 작업은 특정한 특정 API만 지원합니다.
