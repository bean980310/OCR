--- ABSTRACT ---
우리는 인간과 멀티라운드 대화를 수행하기 위한 MultiModal-GPT라는 비전과 언어 모델을 제시합니다. MultiModal-GPT는 자세한 캡션 생성, 특정 개체 계산, 사용자가 제기한 일반적인 문의 처리 등 다양한 지시를 따를 수 있습니다. 이 모델은 OpenFlamingo에서 효율적으로 미세 조정되었으며, 언어 모델의 게이트 교차 주의 및 자기 주의 구성 요소 모두에 Low-rank Adapter(LORA)가 통합되었습니다. 우리의 접근 방식에는 멀티 모달리티 지시 조정을 위해 시각 및 언어 데이터를 통합하는 지시 템플릿을 구성하여 모델이 인간의 지시를 이해하고 준수할 수 있도록 하는 것이 포함됩니다. 우리는 짧은 응답이 있는 제한된 데이터 세트로 인해 모델이 모든 지시에 대한 간단한 답변을 생성할 수 있으므로 효과적인 대화 성능을 위해서는 훈련 데이터의 품질이 매우 중요하다는 것을 관찰했습니다. MultiModal-GPT의 대화 능력을 더욱 향상시키기 위해 언어 전용 지시 따르기 데이터를 시각 언어 지시와 함께 공동 훈련에 사용합니다. 두 유형의 데이터에 동일한 지시 템플릿을 사용하면 대화 성능이 크게 향상됩니다. 저희의 실험은 MultiModal-GPT가 인간과 지속적인 대화를 유지하는 데 능숙하다는 것을 보여줍니다. 코드, 데이터 세트 및 데모는 https://github.com/open-mmlab/Multimodal-GPT에서 찾을 수 있습니다. 1
--- METHOD ---
3.1 아키텍처 제안된 MultiModal-GPT는 오픈 플라밍고 모델[1]을 기반으로 한다. 그림 1에서 보듯이, MultiModal-GPT는 CLIP[13]의 비전 인코더, 비전 인코더에서 공간적 특징을 수신하는 지각자 리샘플러, 언어 디코더 LLaMA[16]로 구성된다. 언어 디코더는 시각의 특징을 텍스트로 인코딩하기 위해 교차 주의에 의해 지각자 리샘플러의 공간적 특징에 따라 조건지어진다. 모델 아키텍처에 대한 자세한 내용은 [1]을 참조한다. 3.2 공동 훈련 언어 전용 지시 따르기 데이터와 시각 및 언어 지시 따르기 데이터를 모두 사용하여 MultiModal-GPT를 공동으로 훈련한다. 그림 1에서 보듯이, 전체 오픈 플라밍고 모델을 동결하고 언어 디코더의 자기 주의, 교차 주의 및 FFW 부분에 LoRA[4]를 추가하여 MultiModal-GPT를 미세 조정한다. MultiModal-GPT는 텍스트의 다음 토큰과 {response}만을 예측하여 학습됩니다.<EOS> 입력 시퀀스의 토큰은 손실 계산에 관련됩니다. 4
