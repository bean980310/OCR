--- ABSTRACT ---
생성자-판별자 사전 학습 프레임워크인 ELECTRA(Clark et al., 2020)는 다양한 다운스트림 작업 중에서 인상적인 의미 구성 역량을 달성했습니다. 설득력 있는 성능에도 불구하고 ELECTRA는 여전히 단조로운 학습과 부족한 상호 작용이라는 과제에 직면해 있습니다. 마스크 언어 모델링(MLM)만 있는 생성자는 판별자에 대한 편향된 학습과 레이블 불균형으로 이어져 학습 효율성이 감소합니다. 판별자에서 생성자로의 명확한 피드백 루프가 없으면 이 두 구성 요소 사이에 틈이 생겨 과정 학습을 충분히 활용하지 못합니다. 이 연구에서는 샘플 효율적인 사전 학습을 위해 여러 각도와 시각 각도를 가져오고 생성자와 판별자 간의 관계를 최대한 활용하기 위해 다중 관점 과정 학습(MCL) 방법을 제안합니다. 구체적으로 세 가지 자체 감독 과정은 MLM의 고유한 결함을 완화하고 다중 관점 방식으로 레이블을 균형 있게 조정하도록 설계되었습니다. 또한, 두 인코더 간의 틈을 메우기 위해 2차 감독을 위한 &quot;수정 노트북&quot;을 만드는 두 가지 자체 수정 코스가 제안되었습니다. 더욱이, MCL의 &quot;줄다리기&quot; 역학 문제를 해결하기 위해 코스 수프 시험을 실시하여 더 강력한 사전 학습된 모델을 진화시켰습니다. 실험 결과에 따르면, 저희 방법은 GLUE 및 SQUAD 2.0 벤치마크에서 각각 ELECTRA의 평균 성능을 2.8% 및 3.2% 절대 포인트만큼 크게 향상시키고 동일한 설정에서 최근의 고급 ELECTRAstyle 모델을 능가합니다. 사전 학습된 MCL 모델은 https://huggingface.co/McmanusChen/MCLbase에서 제공됩니다.
--- METHOD ---
샘플 효율적인 사전 학습을 위해 많은 각도와 시각 각도를 가져오고 생성기와 판별기 간의 관계를 최대한 활용하도록 제안되었습니다. 구체적으로, 세 가지 자기 감독 과정은 MLM의 고유한 결함을 완화하고 다중 관점 방식으로 레이블을 균형 잡도록 설계되었습니다. 또한, 두 가지 자기 수정 과정은 2차 감독을 위한 &quot;수정 노트북&quot;을 만들어 두 인코더 간의 틈을 메우는 데 제안되었습니다. 더욱이, MCL의 &quot;줄다리기&quot; 역학 문제를 해결하고 더 강력한 사전 학습된 모델을 진화시키기 위해 과정 수프 시험을 실시합니다.
--- EXPERIMENT ---
모든 결과에 따르면, 저희의 방법은 GLUE와 SQUAD 2.0 벤치마크에서 각각 ELECTRA의 평균 성능을 2.8%와 3.2% 절대 포인트만큼 크게 향상시켰으며, 동일한 설정에서 최근의 고급 ELECTRAstyle 모델을 압도합니다. 사전 훈련된 MCL 모델은 https://huggingface.co/McmanusChen/MCLbase에서 제공됩니다. 서론 언어 모델 사전 훈련(Radford 등, 2018, 2019; Devlin 등, 2019; Liu 등, 2019)은 *Microsoft 인턴십 기간 동안 기여. + 책임 저자. 다양한 다운스트림 NLP 작업을 이해하고 처리하는 능력을 기계에 부여하는 데 큰 성공을 거두었습니다. 광범위한 사전 훈련 전략이 제안되었는데, 그 중 가장 널리 사용되는 것은 마스크 언어 모델링(MLM)입니다(Devlin 등, 2019). 이러한 자동 인코딩 언어 모델링 객체(Vincent et al., 2008)는 일반적으로 먼저 마스크된 토큰으로 특정 비율의 훈련 코퍼스를 무작위로 손상시킨 다음 인코더가 원래 코퍼스를 복원하도록 권장합니다. 사전 훈련의 무작위성을 줄이고 샘플 효율적인 방법을 생성하기 위해 ELECTRAstyle 프레임워크(Clark et al., 2020)는 MLM으로 훈련하는 Transformer 기반(Vaswani et al., 2017) 생성기를 활용하여 유사한 구조로 판별자를 위한 도전적인 ennoising 문장을 구축하여 denoising 절차를 수행합니다. 일반적으로 ELECTRA 스타일 훈련에서 생성자는 먼저 MLM 훈련을 통해 의미 표현을 구성하고 이러한 마스크된 문장을 가상 단어로 닫습니다. 그 사이에 판별자는 이전 생성자로부터 정보를 상속받고 모든 토큰의 독창성을 구별하는데, 이는 단계별 과정 학습과 같습니다. 그러나 MLM 기반 생성기 학습만이 데이터의 단조로운 학습으로 이어질 수 있으며, 이는 판별자에 대한 손상된 문장의 이해 불가능한 생성 및 불균형한 레이블로 이어집니다(Hao et al., 2021). 게다가 두 인코더 간의 상호 작용은 임베딩 레이어 공유를 제외하고는 갑자기 중단됩니다(Xu et al., 2020; Meng et al., 2021). 판별자에서 생성자로의 직접적인 피드백 루프가 없기 때문입니다. 이 연구에서는 학습 데이터의 효율성을 높이고 생성자와 판별자의 관계를 적절히 활용하기 위해 다중 관점 코스 학습(MCL)이라는 샘플 효율적인 방법을 제안합니다. MCL의 첫 번째 단계에서는 초기 의미 구성을 촉진하기 위해 많은 각도와 시각 각도를 가져오기 위해 빈칸 테스트, 단어 재배열 및 슬롯 감지를 포함한 세 가지 자체 감독 코스가 설계되었습니다. 이러한 과정은 언어 모델이 ELECTRA 스타일 프레임워크에 따라 다양한 관점에서 정확히 동일한 코퍼스를 분해하고 분석하도록 지시합니다. 두 번째 단계에서는 두 개의 자체 수정 과정에서 생성자와 판별자를 모두 개선하는 작업이 할당됩니다. 판별자가 각 문장을 인식하는 것과 관련된 혼동 행렬을 분석하여 수정 코퍼스를 구성하는 데 적용합니다. 이전 과정 학습의 결함에 대응하여 두 구성 요소에 대한 2차 학습이 수행됩니다. 마지막으로 모델은 여러 관점에서 동일한 데이터 배치를 마이닝하고 자체 수정 과정을 통해 점진적인 의미 학습을 구현합니다. 가장 널리 받아들여진 벤치마크인 GLUE(Wang et al., 2019) 및 SQUAD 2.0(Rajpurkar et al., 2018)에 대한 실험은 제안된 MCL의 효과를 보여줍니다. 이전의 고급 시스템과 비교했을 때 MCL은 다양한 다운스트림 작업에서 강력한 이점을 얻었습니다. 풍부한 절제 연구는 다중 관점 과정이 모델이 샘플 효율적인 방식으로 데이터를 학습하도록 장려한다는 것을 확인합니다. 또한, 다중 관점 학습의 핵심을 더욱 해석하고 분석하기 위해 과정 수프 시험을 실시하여 사전 학습 효율성과 성능을 향상시키는 새로운 접근 방식을 제공합니다.2 예비 이 작업에서 우리는 ELECTRA 스타일 프레임워크를 기반으로 시스템을 구축했습니다.따라서 ELECTRA의 프레임워크를 검토합니다.MLM으로 학습된 하나의 변압기 인코더만 사용하는 BERT(Devlin et al., 2019)와 달리 ELECTRA는 생성기 G와 판별기 D의 두 개의 변압기 인코더로 학습합니다.G는 MLM으로 학습되고 입력 시퀀스에서 마스크된 토큰을 대체하기 위해 모호한 토큰을 생성하는 데 사용됩니다.그런 다음 수정된 입력 시퀀스가 D에 공급되고 해당 토큰이 원래 토큰인지 또는 생성기로 대체된 토큰인지 판별해야 합니다. = 생성기 학습 공식적으로, 입력 시퀀스 X [x1, x2, ..., xn]이 주어지면 마스크 연산을 수행하여 위치 집합 r에서 토큰을 [MASK]로 무작위로 대체합니다.1 그리고 마스크된 문장 Xmask = [x1, x2, ..., [MASK]i, , ..., xn]이 생성기에 입력되어 문맥화된 표현 {h}?1을 생성합니다.G는 손실 MLM에 따라 학습하여 마스크된 위치에서 어휘 V의 원래 토큰을 예측합니다.PMLM(xt|hi) exp(xhi) (1) exp(xh) t&#39;=LMLM = E Σ log PMLM(xi hi) (zi/hi)). (2) iЄr 여기서 {x}는 [MASK]로 대체되는 토큰의 임베딩입니다.마스크된 언어 모델링은 마스크된 위치에서만 수행합니다.판별기 학습. G는 마스크 처리된 토큰의 원래 정체성을 예측하는 경향이 있으므로 Xrtd는 마스크 처리된 토큰을 생성기 샘플로 대체하여 생성됩니다: rtd ~ PMLM (x|hi), if iЄr; x = xi, else. (3) D는 대체 토큰 감지(RTD) 손실 LRTD를 통해 Xrtd의 토큰이 G로 대체되었는지 구별하도록 학습됩니다: PRTD (xxih;) = LRTD E = exp(whi) (1 + exp(whi))&#39; (4) Σ log PRTD (x = x; h;) x=xi Σ log (1 - PRTD (x xdxi = Xi Bi/h,))). (5) 여기서 w는 학습 가능한 가중치 벡터입니다. 이 최적화는 모든 토큰에서 수행됩니다. 전반적인 사전 학습 목표는 다음과 같이 정의됩니다: LELECTRA = LMLM XLRTD. (6) 여기서 (일반적으로 50)은 G와 D의 학습 속도를 균형 잡는 데 사용되는 하이퍼파라미터입니다. 사전 학습 후 다운스트림 작업에서는 D만 미세 조정됩니다. 3가지 과제 편향된 학습 ELECTRA 학습 방법은 간단하고 효과적이지만, 단일 관점에서 코퍼스를 처리하면 편향된 학습이 발생할 수 있습니다. MLM과 RTD의 진행과 관련하여 G가 [MASK] 위치에서 적절하지만 원래 토큰이 아닌 토큰을 예측할 수 있다는 고유한 결함이 있으며, 이러한 적절한 표현은 여전히 D에 의한 대체로 판단되어야 합니다. 예를 들어, 원래 시퀀스인 &quot;Alan buys an apple&quot;이 &quot;Alan [MASK] an apple&quot;로 마스크 처리되면 [MASK]를 대체할 수 있는 &quot;eats, peels, cuts&quot;와 같은 후보 단어가 너무 많아 조화로운 맥락을 형성할 수 없습니다. 따라서 D에게 대체된 토큰을 계속 구별하도록 요청하는 것은 어렵고 어색한 작업입니다. 동일한 데이터를 단일 방식으로 처리하면 학습의 효과가 감소합니다. G에서 생성된 코퍼스의 분포에 관해서는, 라벨 불균형은 G의 MLM 훈련과 함께 점차적으로 나타날 수 있으며, 이는 D의 RTD 훈련을 방해할 수 있습니다. G의 의미적 구성이 사전 훈련으로 번창함에 따라 [MASK]의 의사 토큰이 더 합리적이 되고 심지어 원래 단어와 일치합니다. 따라서 D의 훈련 문장에서 대체된 토큰의 비율이 붕괴되어 이진 분류 작업을 심각하게 방해합니다. 이 작업에서는 세 개의 자기 감독 과정이 다중 관점 방식으로 모델을 훈련하여 데이터의 학습 효율성을 개선하고 라벨 분포를 균형 있게 조정하도록 과제를 부여했습니다. 부족한 상호 작용 자기 감독 훈련의 핵심은 원래 코퍼스에서 라벨이 지정된 데이터를 독창적으로 설계하고 구성하는 것입니다. BERT와 같이 랜덤 마스킹에서 진화한 ELECTRA는 생성기 샘플로 보다 현실적인 코퍼스를 제공하여 G와 D가 서로 경쟁하도록 장려합니다. 그러나 D에서 G로의 명시적 피드백 루프가 없으므로 G의 사전 훈련은 이전과 마찬가지로 MLM에 의해 실질적으로 지배됩니다. 이 두 구성 요소 사이의 틈을 메우기 위해 이 작업에서 D의 판별 결과를 활용하여 G와 D 모두에 대한 &quot;수정 노트북&quot;을 만들고, 2차 감독을 제공하기 위한 두 가지 자체 수정 과정을 제안합니다. 수정 훈련은 두 인코더의 관계와 특성을 최대한 활용하여 훈련의 질을 높입니다. 4 방법론 이 섹션에서는 모델이 다중 관점 방식으로 데이터를 처리하도록 장려하는 세 가지 자체 감독 과정을 공식화하는 것으로 시작합니다. 그런 다음 G와 D 사이의 과정과 같은 관계에서 파생된 두 가지 자체 수정 과정을 정교화합니다. 이러한 다양한 과정은 다중 관점 과정 학습 방법 전체에 짜넣어집니다. 무작위 MASK AB<m> 디<m> FG ABCDEFG LRTD ABCDEFG 랜덤 스왑 랜덤 삽입 AFCB EDG ABC<m> 디<m> EFG 생성기 ABCDEFG ABCM DNEFG LMLM LSLM 판별기 LSTD LITD 그림 1: 자체 감독 과정의 전반적인 구조.<m> [MASK]를 나타냅니다. 대문자는 토큰을 나타내고 빨간색 글자는 작동 위치를 나타냅니다. 4.1 자기 감독 과정 대규모 데이터 사전 학습의 필수성은 의심할 여지 없이 방대한 원시 코퍼스를 최대한 활용할 방법을 고안하는 것입니다. ELECTRA는 모델이 ennoising 및 denosing을 통해 의미 표현을 구성하는 데 적용 가능한 패러다임을 제공했습니다. 이 프레임워크를 기반으로 모델이 시퀀스를 살펴본다는 관점을 확장하고 학습 효율성을 개선하고 편향된 학습을 완화하며 레이블 분포를 균형 잡기 위해 세 가지 이진 분류 작업을 제안합니다. 4.1.1 대체 토큰 감지 = = 마스크 언어 모델링을 사용한 사전 학습 언어 모델의 뛰어난 성능으로 인해 ELECTRA의 대체 토큰 감지 작업을 유지합니다. 이전 심볼 설정에 따라 원래 입력 시퀀스 X [x1, x2, ,xn]이 주어지면 먼저 Xmask [x1, x2, ..., [MASK] i, ..., , xn]으로 마스크 아웃한 다음 G에 입력하여 생성기 샘플을 통해 채우기 시퀀스 Xrtd = [x1, x2, xrtd, ..., xn]을 얻습니다. 마지막으로 D는 어떤 토큰이 원래 토큰인지 또는 교체된 토큰인지 파악하는 작업을 맡습니다. 섹션 2에서 설명한 것처럼 G와 D는 각각 LMLM과 LRTD로 학습합니다. MLM은 G에 빈칸 채우기 테스트를 통해 기본적인 맥락적 의미 구성을 부여하고 RTD는 D가 모델이 의사 시퀀스 Xrtd에서 불협화음을 찾기 위해 맥락을 파고들 수 있도록 하는 상위 레벨 과정입니다. 4.1.2 교환된 토큰 감지 직관적으로 재조합 작업은 시퀀스 관련 학습에 기여합니다. 섹션 3에서 언급했듯이 [MASK] 위치에 정보가 없으면 생성된 의사 단어의 신뢰성이 떨어집니다. 채워진 샘플이 적절한지 여부에 관계없이, 편향된 학습이 발생하여 D의 학습을 방해합니다. 따라서 작업의 난이도를 낮추지 않고 정확한 예측을 위해 원시 메시지를 예약하기 위해 단어 재배열 작업을 통해 모델의 구조 인식 능력을 향상시키는 교환 토큰 감지(STD) 과정을 제시합니다. 입력 문장 X = [x1, x2, ,xn]의 경우, 교환 작업을 위해 랜덤 위치 집합 s가 선택됩니다.² 정확히 말해서, 선택된 위치의 토큰이 추출되고, 재정렬되어 문장에 다시 채워집니다. G는 교환된 문장 X 교환을 X로 복원하는 데 필요하고, 인접한 D는 생성기 샘플에 의해 X std에서 어떤 토큰이 교환되었는지 구별하는 작업을 맡습니다. G의 문맥화된 표현을 {h}1로 주목하면, 교환 언어 모델링(SLM)의 훈련 과정은 아래와 같이 공식화된다. = exp(xhi) s&#39;= exp(xh) (7) PSLM (xshi) (hi)) (8) LSLM E Σ log PSLM (xi|hi) s=iЄs 여기서 {x}는 교환된 위치에 있는 토큰의 임베딩이다. 어휘 V는 모든 과정에서 여전히 동일하다는 점에 유의한다. 이는 SLM 중에 정답이 보류 중인 시퀀스에 있더라도 일관되고 자연스러운 환경에서 G를 생성하는 데 도움이 되기 때문이다. SLM은 교환된 위치의 토큰에서만 수행된다. SLM은 G가 교환된 위치에 대해 합리적이고 독창적인 예측을 하도록 하여 훈련의 주의를 단일 단어 추측에서 전체 시퀀스의 구조와 논리를 포괄적으로 이해하는 것으로 전환한다. D의 교환된 토큰 감지(STD) 과정은 자연스럽게 데자뷰 이진 분류로 형성된다. X std는 스왑된 위치를 생성기 샘플로 대체하여 생성됩니다: std ~ PSLM (x|hi), if iЄs; xstd = xi, else. (9) D는 스왑된 토큰 감지(RTD) 손실을 통해 X std의 토큰이 원본인지 아닌지를 구별하도록 학습됩니다: PSTD (xstd = xi ihi) = sigmoid (whi), (10) 2 특수 지침이 없는 경우 비율은 15%로 설정됩니다. LSTD E Σ log PSTD (x Std = xi|hi) std = xi - Σ log (1 - PSTD (std std +xi xi = 2.))). ihi)) (11) 여기서 ws는 각 코스가 자체 이진 분류 헤드를 사용하기 때문에 w에서 독립적으로 학습 가능한 매개변수입니다. 4.1.3 삽입 토큰 감지 MLM 및 SLM을 사용한 사전 학습 속도로 인해 G는 필연적으로 의미 학습의 완성을 위해 훨씬 더 조화로운 시퀀스를 생성할 수 있습니다. 한편, G가 제공한 손상된 문장의 레이블 분포는 거의 모든 토큰이 원래 문장의 단어와 정확히 일치하기 때문에 마법처럼 불균형해집니다. 따라서 D의 학습은 심각한 간섭과 효율성 부족에 직면합니다. 학습 레이블의 경향성은 D의 판단 경향성으로 이어집니다. 레이블 불균형 문제를 완화하고 데이터 처리에 대한 다른 관점을 모색하기 위해 삽입 토큰 감지(ITD) 과정을 제안합니다. 주어진 문장 X = [x1, x2,..., , xn]에 대해 [MASK]가 삽입 위치 집합 i의 시퀀스에 무작위로 삽입됩니다. 확장된 문장 X in에는 G의 예측을 기다리는 여러 개의 허황된 공석이 포함되어 있습니다. 그런 다음 D는 다음 손실의 학습을 통해 생성된 문장 Xitd에 어떤 토큰이 표시되어서는 안 되는지 파악해야 합니다. PITD (x = x²n|hi) = sigmoid(wh₁), (12) LITD = E itd Σ log PITD (x = x^|h;) in -Σ log (1 - Pro (id = &quot;/h;))). td #rin in (13) 한편, 실제 단어와 삽입된 단어의 비율은 고정되어 레이블 불균형을 어느 정도 해결합니다. 반면에 공백 위치에 대한 학습은 모델의 생성 능력을 향상시킵니다. 제안된 자체 감독 과정의 전체 구조는 그림 1에 나와 있습니다. 모든 Predict\Label 원본 교체 마스크 ☑ 원본 pospos✓ 교체 revisek pospos재생성. 재차별 a X xmask [mask] Xrta 출력 원본 원본 교체 원본 교체 check X pos√ pos¹ ฟ √ posi the chef chef chef thanked the chef cooking the meal chef [mask] the [mask] meal posx pos³ [mask] the cooking the meal 표 1: D의 출력 토큰의 혼동 행렬. ✓는 D가 올바른 판단을 내렸음을 나타내고, 반대로 ✗는 잘못된 판단의 상황을 나타냅니다. 과정은 동일한 데이터 및 컴퓨팅 단계 내에서 공동으로 수행됩니다. 4.2 자체 교정 과정 위의 자체 감독 과정에 따르면 G와 D 사이의 경쟁 메커니즘이 형성되는 것으로 보입니다. 동일한 데이터 조각에 직면하여 G는 여러 가지 방법으로 시퀀스를 재구성하려고 시도하는 반면 D는 이전에 발생한 모든 조작을 파악하려고 합니다. 그러나 이 두 인코더의 공유 임베딩 계층은 유일한 통신 브리지가 되며 이는 분명히 충분하지 않습니다. 두 구성 요소 간의 연결을 강화하고 사전 학습에 대한 더 많은 감독 정보를 제공하기 위해 G와 D 간의 관계를 면밀히 분석합니다. 예를 들어 RTD 절차를 살펴보겠습니다. 손상된 문장 X¹td의 각 토큰 xrd에 대해, 이후 D에 입력된 후, X의 해당 위치에서 토큰 x;와 비교하여 레이블을 식별하고 문서화합니다. D의 구별 과정 후, 이 토큰은 원래 토큰인지 대체된 토큰인지 이진 분류됩니다. 표 1에서 볼 수 있듯이, xi에 대한 구별 결과의 상황은 네 가지가 있습니다. pos₁: G가 [MASK] 위치에서 정답을 예측하고 D가 성공적으로 올바른 판단을 내리는 경우, 이러한 종류의 토큰에 대해 추가 작업을 수행할 필요가 없습니다. pos2: G가 원래 토큰을 대체할 대안을 채우고 D가 그것을 원래 토큰으로 부정확하게 보는 경우, 이는 G가 섹션 3에서 언급한 대로 조화로운 맥락을 형성하는 적절한 표현을 생성한다는 것을 의미하므로 D가 구별하기 어렵게 만듭니다. pos3: D가 원래 토큰을 대체된 것으로 잘못 주석하는 서투른 실수를 합니다. pos: G가 [MASK] 위치에서 부적절한 토큰을 채우고 D가 쉽게 알아냅니다. 요약하자면, 한편으로 G는 D에 대해 초기 대안이 부적절하고 도전적이지 않기 때문에 pos4에서 토큰을 재생성해야 합니다.그림 2: RTD의 자체 수정 과정에 대한 예.그림 2에서 보듯이 정보가 풍부한 중요한 위치에 너무 많은 [MASK]가 배치되어 &quot;감사합니다&quot;가 불규칙하게 생성됩니다.같은 시퀀스의 다른 [MASK]가 pos4에서 토큰 생성을 방해할 수 있다는 점을 고려하여 재생성 프로세스의 편의를 위해 다른 [MASK]를 원래 토큰으로 복원합니다.반면에 D는 pos2와 pos3에서 토큰을 다시 구별할 것으로 예상됩니다.시퀀스에서 posд에 토큰이 있는 경우 이러한 부적절한 토큰은 다른 토큰에 대한 D의 결정을 심각하게 방해하여 결과적으로 pos2와 Take pos3을 초래할 수 있습니다.예를 들어 그림 2의 문장 &quot;감사합니다&quot;는 D가 &quot;meal&quot;을 대체된 것으로 잘못 판단하게 합니다. 따라서 이런 종류의 간섭을 완화하기 위해 Xrtd의 pos에 있는 토큰을 원래 토큰으로 대체하고, pos2와 pos3에서 D에 대한 재차별 훈련을 수행합니다. 오류를 분류하고 분석하여 G와 D에 대한 &quot;수정 노트북&quot;이 작성되어 재생성 및 재차별 훈련을 안내합니다. 그러나 단순히 문제를 다시 하는 것이 아니라 각 종류의 문제에 대한 맥락을 재설계한다는 점에 유의하세요. 따라서 Cre-MLM 및 Cre-RTD는 RTD의 자체 수정 과정에 대한 학습 목표로 설계되었습니다. 마찬가지로 Lre-SLM 및 Cre-STD는 STD의 훈련 손실 자체 수정 과정을 제시합니다.³ 삽입된 [MASK] 위치에 원래 토큰이 없기 때문에 ITD에 대한 수정이 수행되지 않습니다. 제안된 두 가지 자체 수정 과정은 내성과 개선을 통해 G와 D 사이의 틈을 메우고 동일한 데이터에 대한 샘플 효율적인 2차 감독을 제공합니다. 마지막으로 G와 D는 세 가지 자체 감독 과정과 두 가지 자체 수정 과정과 함께 공동 훈련됩니다. 제안된 MCL은 동일한 3 방정식은 형식이 이전 텍스트와 일치하므로 나열되지 않았습니다. 시퀀스를 심층적으로 분석합니다. 그리고 추가적인 추론이나 메모리 비용 없이 종합적으로.5 실험 5.1 설정 사전 학습 설정 우리는 두 가지 설정, 즉 base와 tiny에서 실험을 구현합니다.Base는 BERT Base의 표준 학습 구성입니다(Devlin et al., 2019).이 모델은 영어 위키피디아와 BookCorpus(Zhu et al., 2015)에서 사전 학습되었으며, 2억 5,600만 개의 샘플이 있는 16GB의 텍스트가 포함되어 있습니다.입력 시퀀스의 최대 길이를 512로 설정하고 학습률은 5e-4입니다.학습은 2,048 배치 크기로 125,000단계 동안 진행됩니다.우리는 CoCo-LM(Meng et al., 2021)과 동일한 코퍼스와 64,000개의 cased SentencePiece 어휘(Kudo and Richardson, 2018)를 사용합니다. 사전 학습의 하이퍼파라미터 세부 정보는 부록 A에 나와 있습니다. Tiny는 기본 설정과 동일한 구성으로 동일한 코퍼스에서 절제 실험을 수행하지만 배치 크기는 512입니다. 모델 아키텍처 모델 아키텍처의 레이아웃은 기본 및 tiny 설정에서 모두 (Meng et al., 2021)과 동일하게 유지됩니다. D는 12개 레이어 Transformer, 768개의 숨겨진 크기, Trelative 위치 인코딩(Raffel et al., 2020)으로 구성됩니다. G는 동일한 숨겨진 크기와 위치 인코딩을 사용하는 얕은 4개 레이어 Transformer입니다. 사전 학습 후 G를 삭제하고 BERT와 동일한 방식으로 D를 사용하며 다운스트림 작업을 위한 분류 레이어가 있습니다. 다운스트림 작업 제안된 방법의 효과를 확인하기 위해 다양한 다운스트림 작업에 대한 평가 실험을 수행합니다. 우리는 General Language Understanding Evaluation(GLUE) 벤치마크(Wang et al., 2019)와 Stanford Question Answering 2.0(SQUAD 2.0) 데이터 세트(Rajpurkar et al., 2018)를 평가합니다. GLUE 과제의 평가 지표에 관해서는 STS의 경우 스피어만 상관관계, COLA의 경우 매튜스 상관관계, 나머지의 경우 정확도를 채택합니다. 일부 질문은 본문에서 답할 수 없기 때문에 ExactMatch(EM)와 F1 점수의 표준 평가 지표를 채택합니다. GLUE와 SQUAD 2.0 벤치마크에 대한 자세한 내용은 부록 B에 나와 있습니다. 기준선 다양한 사전 학습된 모델이 나열되어 기본 설정에서 비교됩니다. 모든 숫자는 최근 연구에서 보고된 결과에서 가져온 것입니다. 여러 논문에서 동일한 방법에 대해 서로 다른 점수를 보고하는 경우 비교를 위해 가장 높은 점수를 사용합니다. 구현 세부 정보 우리의 구현은 fairseq(Ott et al., 2019)의 오픈 소스 구현을 기반으로 합니다. 128개의 A100(40GB 메모리)을 사용하면 기본 설정에서 사전 학습을 한 번 실행하는 데 약 24시간이 걸립니다. 미세 조정 비용은 BERT와 상대적 긍정 인코딩의 경우와 동일합니다. 미세 조정에 대한 자세한 내용은 부록 C에 나와 있습니다. 5.2 평가 결과 먼저 제안된 MCL 방법으로 모델을 사전 학습한 다음 GLUE 벤치마크에서 8개의 단일 작업으로 구성된 학습 세트로 미세 조정했습니다. 모든 다운스트림 작업에 대해 하이퍼파라미터 검색을 수행했으며 5개의 랜덤 시드 간 평균 점수를 보고합니다. 결과는 표 2의 상단 절반에 자세히 나와 있습니다. 제안된 MCL은 ELECTRA를 분명히 향상시키고 기본 설정에서 GLUE 벤치마크에서 최첨단 사전 학습된 언어 모델에 비해 최소 1.1%의 절대적 전체 개선을 달성합니다. 가장 널리 보고된 작업인 MNLI의 경우, 우리 모델은 일치/불일치(m/mm) 세트에서 88.5/88.6점을 달성하여 ELECTRA에 비해 1.6/1.8의 절대적 개선을 얻습니다. 모든 GLUE 단일 작업을 더 광범위하게 살펴보면, MCL은 CoCo-LM이 근소한 차이로 앞서는 RTE 작업을 제외한 모든 이전 모델을 압도합니다. 또한, 기계가 질문과 함께 문서가 주어진 답변 범위를 추출해야 하는 중요한 독해 데이터 세트인 SQUAD 2.0 데이터 세트에서 제안된 MCL을 평가했습니다. 정확한 일치(EM) 및 F1 점수(F1)의 결과는 표 3의 상단 절반에 표시됩니다. 일관되게, 우리 모델은 ELECTRA 기준선을 상당히 개선하고 다른 동일 크기 모델과 비교하여 배너 점수를 달성합니다. 구체적으로, 기본 설정에서 제안된 MCL은 ELECTRA에 비해 절대적 성능을 3.2점(EM) 및 3.3점(F1)만큼 개선합니다. 또한, 우리 모델은 다른 모든 이전 모델보다 명백한 마진으로 성능이 뛰어납니다. 설득력 있는 결과는 제안된 MCL의 효과를 보여줍니다. 동일한 양의 훈련 코퍼스와 약간의 전방 전파 컴퓨팅 비용으로 MCL은 ELECTRA 기준선을 엄청나게 발전시켜 샘플 효율성의 특성을 보여주었습니다. 다시 말해, 다중 관점 과정 학습은 모델에 코퍼스의 기본 의미에 대한 더 깊고 포괄적인 통찰력을 제공하여 사전 훈련 프로세스에 더 가치 있는 정보를 제공합니다. GLUE 단일 작업 모델 MNLI -m/-mm QQP QNLI SST-2 COLA RTE MRPC STS-B AVG Acc Acc Acc MCC Acc Acc PCC 기본 설정: BERT 기본 크기, Wikipedia + Book Corpus BERT(Devlin 등, 2019) 84.5/91.3 91.93.58.68.87.89.83.XLNet(Yang 등, 2019) 85.8/85.92.ROBERTA(Liu 등, 2019) 85.8/85.91.3 92.93.60.68.87.88.83.DeBERTA(He 등, 2021) 86.3/86.TUPE(Ke 등, 2021) 86.2/86.91.3 92.93.63.73.89.89.84.MC-BERT(Xu 외, 2020) 85.7/85.89.7 91.92.62.75.86.88.83.ELECTRA(Clark 외, 2020) 86.9/86.91.9 92.93.66.75.88.89.85.+HPLOSS+Focal(Hao 외, 2021) 87.0/86.91.7 92.92.66.81.90.91.86.CoCo-LM(Meng 외, 2021) 88.5/88.92.0 93.93.63.84.91.90.87.MCL 88.5/88.92.2 93.4 94.1 70.84.91.91.3 88. 작은 설정: 절제 연구를 위한 훈련 플롭의 1/4, 위키피디아 + 도서 코퍼스 ELECTRA(재구현) 85.80/85.77 91.63 92.03 92.70 65.74.87.89.02 84.+STD 86.97/86.97 92.07 92.93.70.25 82.91.90.72 87.+ITD 87.37/87.33 91.87 92.93.68.81.90.90.52 87. 자체 감독 87.27/87.33 91.97 92.93.67.86 82.90.90.81 87.87.57/87.50 92.07 92.92.69.80 83.91.90.71 87.93.71.25 82.91.90.95 87.+ re-RTD + re-STD MCL 87.80/87.77 91.97 92.87.90/87.83 92.13 93.00 93.47 68.81 83.03 91.67 90.93 87.표 2: 비교를 위한 GLUE 데이터 세트에 대한 모든 평가 결과. Acc, MCC, PCC는 각각 정확도, Matthews 상관관계, Spearman 상관관계를 나타냅니다. 보고된 결과는 5개의 난수 시드에 대한 중앙값입니다. 5. 절제 연구 제안된 MCL에서 각 구성 요소의 역할을 깊이 파고들기 위해 작은 설정에서 절제 연구가 수행됩니다. GLUE 및 SQUAD 2.0 데이터 세트가 모두 평가에 활용되고 절제 결과는 표의 하단과 표 3에 나열됩니다. 사전 훈련 중 손실 및 정확도와 관련된 여러 곡선 그래프에 의해 뒷받침되어 모든 코스가 아래에서 논의됩니다. RTD 가장 기본적인 구성 요소이며 ELCETRA 자체도 나타냅니다. 그 성능은 다른 추가 사항과 비교하기 위한 기준선으로 사용됩니다. 점수뿐만 아니라 곡선도 중요한 기준으로 사용됩니다. STD 이 코스는 보다 조화로운 맥락적 이해를 통해 모델이 더 나은 구조 인식 능력을 얻도록 돕는 것이 목표입니다. STD는 GLUE 및 SQUAD 2.0 데이터 세트의 모든 작업에서 ELECTRA를 개선합니다. CoLA 작업의 점수가 놀랍게도 군중 속에서 돋보인다는 점은 주목할 가치가 있습니다. 언어 수용성 코퍼스(COLA)는 영어 문장이 언어적으로 수용 가능한지 여부를 예측하는 데 사용됩니다. 4 공간 제약으로 인해 사전 학습과 구체적 지침의 곡선 그래프는 부록 D에서 대체됩니다. 분명히 단어 재배열에 대한 사전 학습은 실제로 모델의 전반적인 지능을 높여 단어 예측보다는 구조와 논리에 더 집중하게 합니다. 71.25라는 최고 CoLA 결과조차도 STD의 효과를 더욱 구체화하는 재STD 과정에서 나왔습니다. ITD 이 과정은 라벨 불균형을 완화하는 것이 과제입니다. 그림 5에서 보듯이, 대체율은 G의 예측 정확도를 반영합니다. MLM 및 SLM과 함께 G는 [MASK] 위치에서 더 정확한 단어를 예측하여 D의 학습을 위해 &quot;대체된&quot; 레이블이 부족해집니다. 삽입된 [MASK]를 추가하면 대체율이 삽입된 비율에 해당하는 고정된 하한을 가지므로 레이블이 균형 있게 분포됩니다. 게다가 ITD는 특히 SST-2 데이터 세트에서 ELECTRA보다 크게 개선되었습니다. Stanford Sentiment Treebank(SST-2)는 영화 리뷰에서 추출한 문장의 감정이 긍정적인지 부정적인지 판단해야 하는 감정 분류를 위한 데이터 세트를 제공합니다. 허황된 [MASK]에 대한 예측은 모델이 내용 이해에 더 집중하게 하여 감정 분류에 도움이 될 수 있습니다. 자체 수정 과정 개정은 항상 어려운 과제로 작용합니다. 모델 SQUAD 2.EM FBase 설정 BERT(Devlin et al., 2019) 73.76.XLNet(Yang et al., 2019) 78.81.ROBERTA(Liu et al., 2019) 77.80.80 80MNLI 평균 정확도88.88.88.88.87.* * ** х *86.47 86.86.86.× 85.DeBERTa(He et al., 2021) 79.3 82.25k 50k ELECTRA(Clark et al., 2020) 79.82.MCL +HPLOSS CoCo-LM(Meng et al., 2021) +Focal(Hao et al., 2021) 82.85.MCL ELECTRA 75k 100k 학습 단계 125k 82.85.82.9 85.그림 3: 사전 학습의 MNLI 비교 효율성. 절제 연구를 위한 작은 설정 ELECTRA(재구현) +STD +ITD 자기 감독 + 재RTD + 재STD MCL 79.37 81.81.73 84.81.43 84.81.87 84.81.70 84.81.81 84.82.84.표 3: SQUAD 2.0 데이터 세트에 대한 모든 평가 결과. 고정관념을 뒤집습니다. 그림 5에서 볼 수 있듯이 자기 교정 훈련 중 G와 D의 손실은 일반적으로 자기 감독 훈련 중 손실을 초과하여 어려움을 보여줍니다. 그러나 재RTD 과정의 대체 정확도는 기준선보다 높아 효과를 증명합니다. 자기 교정 훈련이 모든 하위 작업에서 다른 구성 요소보다 성능이 뛰어나지만 &quot;줄다리기&quot; 역학의 현상은 탐구할 가치가 있습니다. 표 2의 마지막 세 행에 나열된 점수는 거의 서로 닿아 있으며, 모든 단일 작업의 최적 결과가 항상 동일한 모델에서 나타나는 것은 아닙니다. 즉, 다중 관점 과정은 매개변수를 다른 방향으로 끌어오려는 시도에서 서로 간섭할 수 있으며, 이는 2차 샘플이 부트스트래핑에 적합하게 잘 설계된 자체 수정 과정에서 더욱 분명하게 나타납니다. 이 상황을 완화하고 학습의 효과를 더욱 개선하기 위해 섹션 5.5에 자세히 설명된 실행 가능한 솔루션을 찾았습니다. 5.4 샘플 효율성 비교 제안된 MCL이 샘플 효율적임을 보여주기 위해 MCL과 ELECTRA 간의 비교 시험을 수행합니다. 그림 3에서 볼 수 있듯이 널리 퍼진 작업 MNLI가 평가를 위해 선택되었습니다. 사전 학습의 25,000단계마다 모델을 예약하고 섹션 5.1에서 언급한 것과 동일한 구성으로 미세 조정했습니다. 분명히 MCL은 모든 GLUE 데이터 세트의 평균 점수 88.88.88.88.88.88.MCL 88.1088.88.weightIngredients of the soups88.uniform에서 ELECTRA 기준선보다 우세하며, 25,000단계에서 87.8점을 얻어 작은 코퍼스 조각에서도 엄청난 학습 효율성을 보여줍니다. 5.5 Course Soups Trial 미세 조정 중 하이퍼파라미터 스윕에서 많은 모델의 평균을 내는 모델 수프(Wortsman et al., 2022)에서 영감을 얻어 유사점을 발견하고 사전 학습 중 작업 스윕에서 이 아이디어를 MCL에 적용했습니다. 다른 코스는 모델이 다른 낮은 오류 분지에 놓이게 하며, 여러 코스를 공동 학습하면 &quot;줄다리기&quot; 역학이 생성될 수 있습니다. 학습 갈등을 해결하고 이후 사전 학습 단계에서 모델의 학습 효율성을 더욱 개선하기 위해 &quot;코스 수프&quot; 시험을 수행합니다. 수프의 성분에 대해 자체 수정 코스에서 4가지 손실의 모든 조합을 정리하여 자체 감독 코스의 구조를 유지하면서 14개의 단일 모델로 학습합니다. 그런 다음 모든 성분을 균일하고 가중된 통합을 통해 병합합니다. 결과는 그림 4에 나와 있습니다. 가중 수프를 통해 얻은 최적의 결과는 최상의 모델 MCL에 비해 평균 GLUE 점수를 0.19 절대 포인트만큼 향상시킵니다. 결과에 따르면 코스 수프는 여러 목표를 분리하고 마지막에 결합하여 모델의 이후 학습을 안내하는 효과적인 방법을 제안합니다. 자세한 점수는 부록 E에 나와 있습니다. 6
--- CONCLUSION ---
이 논문은 학습 효율성을 개선하고 레이블 분포를 균형 잡기 위한 3개의 자기 감독 과정과 수정 훈련을 위한 &quot;수정 노트&quot;를 만드는 2개의 자기 교정 과정을 포함하는 다중 관점 과정 학습 방법을 제안합니다. 게다가, 과정 수프 방법은 효율적인 사전 훈련을 위한 새로운 접근 방식을 알아내기 위해 설계되었습니다. 실험 결과, 우리의 방법은 ELECTRA의 성능을 크게 개선하고 동일한 설정에서 여러 고급 모델을 가려 MCL의 효과를 검증합니다. 한계 제안된 방법은 ELECTRA 스타일 사전 훈련 모델에서 일반적인 문제인 편향된 학습과 부족한 상호 작용의 문제를 완화하는 데 큰 성과를 보였지만, 제안된 방법은 여전히 더 개선될 수 있음을 깨달아야 합니다. 예를 들어, 섹션 3에서 언급한 RTD의 고유한 결함은 해결하기보다는 완화될 수밖에 없었습니다. 이 문제와 관련된 임무 설계에 대해 더 자세히 연구할 가치가 있습니다. 게다가, 결과가 큰 성과를 보여주지만, 각 과정의 숨겨진 영향을 탐구하기 위한 더 많은 노력이 필요하며, 이는 미래에 제안된 모델을 적용하는 데 도움이 될 것입니다. 참고문헌 Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancouver, Canada. Association for Computational Linguistics. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: 텍스트 인코더를 생성기가 아닌 판별자로 사전 학습. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 학회 컴퓨터 언어학: 인간 언어 기술 컨퍼런스 회의록, NAACL-HLT 2019, 미네소타주 미니애폴리스, 미국, 2019년 6월 2-7일, 1권(장문 및 단편 논문), 4171-4186쪽. 컴퓨터 언어학 협회. William B. Dolan and Chris Brockett. 2005. 문장 의역의 코퍼스 자동 구성. 2005년 10월, 한국 제주도에서 열린 제3회 국제 의역 워크숍 회의록, IWP@IJCNLP 2005, 2005년. 아시아 자연어 처리 연맹. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, Bill Dolan. 2007. 세 번째 PASCAL 인식 텍스트적 함의 도전. ACL-PASCAL@ACL 2007 텍스트적 함의 및 의역 워크숍 회의록, 체코 프라하, 2007년 6월 28-29일, 1-9페이지. Association for Computational Linguistics. Yaru Hao, Li Dong, Hangbo Bao, Ke Xu, Furu Wei. 2021. ELECTRA 사전 훈련을 위한 샘플링 대체 학습. Association for Computational Linguistics: ACL/IJCNLP 2021의 결과, 온라인 이벤트, 2021년 8월 1-6일, Findings of ACL의 ACL/IJCNLP 2021 권, 4495-4506페이지. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. 2021. Deberta: disentangled attention을 사용한 디코딩 강화 bert. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. 2012. 특징 감지기의 공동 적응을 방지하여 신경망 개선. CORR, abs/1207.0580. Shankar Iyer, Nikhil Dandekar, Kornél Csernai. 2017. 첫 번째 Quora 데이터 세트 릴리스: 질문 쌍. Guolin Ke, Di He, Tie-Yan Liu. 2021. Rethinking positional encoding in language pre-training. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 11월 4, 2018, 66-71쪽. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 BERT 사전 학습 접근법. CoRR, abs/1907.11692. Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. 2021. COCO-LM: 언어 모델 사전 학습을 위한 텍스트 시퀀스 수정 및 대조. Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 614, 2021, virtual, pages 23102-23114. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. 2019. fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 툴킷. 2019년 북미 컴퓨터 언어학 협회 지부 회의록: 인간 언어 기술, NAACL-HLT 2019, 미네소타주 미니애폴리스, 2019년 6월 2-7일, 데모, 48-53페이지. 컴퓨터 언어학 협회. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 2018. 생성적 사전 학습을 통한 언어 이해 향상. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 2019. 언어 모델은 비지도 멀티태스크 학습자입니다. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21:140:1-140:67. Pranav Rajpurkar, Robin Jia, Percy Liang. 2018. 모르는 것을 아세요: 분대에 대한 답할 수 없는 질문. 2018년 7월 15일~20일 호주 멜버른에서 열린 제56회 전산 언어학 협회 연례 회의록, ACL 2018, 제2권: 단편 논문, 784~789쪽. 전산 언어학 협회. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, 2383-2392쪽. The Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. 2013년 자연어 처리 경험적 방법에 대한 컨퍼런스의 진행 사항, EMNLP 2013, 2013년 10월 18-21일, 그랜드 하얏트 시애틀, 워싱턴주 시애틀, 미국, ACL의 특수 관심 그룹인 SIGDAT의 회의, 1631-1642쪽. ACL. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017. 주의만 기울이면 됩니다. 신경 정보 처리 시스템의 발전 30: 신경 정보 처리 시스템 연례 컨퍼런스 2017, 2017년 12월 49일, 미국 캘리포니아주 롱비치, 5998-6008쪽. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol. 2008. Denoising Autoencoders를 사용한 강력한 기능 추출 및 구성. Machine Learning, 제25회 국제 학술 대회(ICML 2008) 회의록, 핀란드 헬싱키, 2008년 6월 5일-9일, ACM 국제 학술 대회 회의록 시리즈 307권, 1096-1103쪽. ACM. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. 2019. GLUE: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. 제7회 국제 학습 표현 회의, ICLR 2019, 미국 루이지애나주 뉴올리언스, 2019년 5월 6일-9일. OpenReview.net. Alex Warstadt, Amanpreet Singh, Samuel R. Bowman. 2019. 신경망 수용성 판단. Trans. Assoc. Comput. Linguistics, 7:625–641. Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. 모델 수프: 여러 개의 미세 조정된 모델의 가중치를 평균화하면 추론 시간을 늘리지 않고도 정확도가 향상됩니다. International Conference on Machine Learning, ICML 2022, 1723 July 2022, Baltimore, Maryland, USA, Proceedings of Machine Learning Research의 162권, 23965-23998페이지. PMLR. Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, Tie-Yan Liu. 2020. MC-BERT: 메타 컨트롤러를 통한 효율적인 언어 사전 학습. CORR, abs/2006.05744. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, Quoc V. Le. 2019. Xlnet: 언어 이해를 위한 일반화된 자기 회귀 사전 학습. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754-5764. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19–27. IEEE Computer Society. A 사전 학습을 위한 하이퍼파라미터 표 4에서 볼 수 있듯이 기본 설정에서 MCL 사전 학습에 사용된 하이퍼파라미터를 제시합니다. 비교를 위해 CoCo-LM(Meng et al., 2021)의 최적화 하이퍼파라미터를 따릅니다. D에서 수행된 모든 손실은 G와 D의 학습 속도를 균형 잡는 데 사용되는 하이퍼파라미터인 X(50으로 설정)로 곱해집니다. 레이어 숨김 크기 FFN 내부 숨김 크기 어텐션 헤드 어텐션 헤드 크기 최대 상대 위치 학습 단계 배치 크기 Adam &amp; (Kingma and Ba, 2015) 125K 1e-Adam B(0.9, 0.98) 학습률 5e-학습률 일정 선형 워밍업 단계 10K 그래디언트 클리핑 2. 드롭아웃(Hinton et al., 2012) 0. 가중치 감소 0. 표 4: 사전 학습을 위한 하이퍼파라미터 B 하류 작업의 세부 정보 GLUE에는 텍스트적 함의를 포함하는 광범위한 작업이 포함되어 있습니다: RTE(Giampiccolo 등, 2007) 및 MNLI(Williams 등, 2018), 질의-답변 함의: QNLI(Rajpurkar 등, 2016), 의역: MRPC(Dolan 및 Brockett, 2005), 질문 의역: QQP(Iyer 등, 2017), 텍스트적 유사성: STS(Cer 등, 2017), 감정: SST(Socher 등, 2013), 언어적 수용성 COLA(Warstadt 등, 2019). 자연어 추론에는 두 문장을 읽고 함의, 중립 및 모순과 같은 의미 간의 관계를 판단하는 것이 포함됩니다. 우리는 Multi-Genre Natural Language Inference(MNLI), Question Natural Language Inference(QNLI), Recognizing Textual Entailment(RTE)를 포함한 세 가지 다양한 데이터 세트를 평가합니다. 의미적 유사성 작업은 두 문장이 의미적으로 동일한지 여부를 예측하는 것을 목표로 합니다. 과제는 개념의 재구성을 인식하고, 부정을 이해하고, synDataset을 처리하는 것입니다. #Train/#Dev/#Test 단일 문장 분류 COLA(수용 가능성) SST-2(감정) 8.5k/1k/1k 67k/872/1.8k 쌍별 텍스트 분류 MNLI(NLI) RTE(NLI) QNLI(NLI) WNLI(NLI) 393k/20k/20k 2.5k/276/3k 105k/5.5k/5.5k 634/71/364k/40k/391k 판별자 손실 rete 교체 0.0.0.0.0.0.0.0.0.0.0.0.0.0.20k 40k 60k 80k 100k 120k RTD20k 40k 60k 80k 100k 120k ITD 0.0.0.0.0.0.20k 40k 60k 80k 100k RTD 120k 0 20k 40k 60k 80k 100k 120k re-RTD QQP(의역) MRPC(의역) 3.7k/408/1.7k 텍스트 유사도 STS-B(유사도) 7k/1.5k/1.4k 표 5: GLUE 벤치마크 요약. 전술적 모호성. Microsoft Paraphrase corpus(MRPC), Quora Question Pairs(QQP) 데이터 세트, Semantic Textual Similarity 벤치마크(STS-B)를 포함한 세 가지 데이터 세트를 사용했습니다. 분류 언어 수용성 코퍼스(COLA)는 영어 문장이 언어적으로 수용 가능한지 여부를 예측하는 데 사용됩니다. 스탠포드 감정 트리뱅크(SST-2)는 영화 리뷰에서 추출한 문장의 감정이 긍정적인지 부정적인지 판단해야 하는 감정 분류를 위한 데이터 세트를 제공합니다. 널리 사용되는 MRC 벤치마크 데이터 세트인 SQUAD 2.0은 기계가 질문과 함께 문서가 주어진 답변 범위를 추출해야 하는 독해 데이터 세트입니다. 순수한 범위 추출 성능에 초점을 맞추기 위해 v2.0 버전을 선택했습니다. 모델 성능을 평가하는 데 두 가지 공식 지표, 즉 정확한 일치(EM)와 토큰 수준에서 예측과 기준 정답 간의 평균 중복을 측정하는 보다 부드러운 지표 F1 점수를 사용합니다. 일반 언어 이해 평가(GLUE) 벤치마크(Wang 등, 2019)에 대한 요약은 표 5에 나와 있습니다.C 미세 조정을 위한 하이퍼 파라미터 표 6은 공정한 비교를 위해 SQUAD v2.0(Rajpurkar 등, 2018) 및 CoCo-LM에 따른 GLUE 벤치마크(Wang 등, 2019)에 대한 미세 조정에 사용된 하이퍼 파라미터를 제시합니다.개발 세트에서 하이퍼 파라미터는 5회 실행의 평균 성능을 기준으로 검색됩니다.교체 정확도 0.0.0.0.0.IN0 20k 40k 60k 80k 100k 120k RTD 0.0.0.0.0.0.0.20k 40k 60k 80k 100k 120k 재RTD 그림 5: 절제 연구에 대한 교체율, 사전 학습 손실 및 교체 정확도의 곡선. 모든 그림은 Tensorboard에서 그렸습니다.절제 연구를 위한 D 곡선 그림 5에서 보듯이 사전 학습의 질을 평가하기 위해 세 가지 지표를 선택했습니다.대체 비율 이 지표는 손상된 문장 Xrtd에서 대체된 토큰의 비율을 나타냅니다.이 비율이 낮을수록 G의 사전 학습이 더 좋고 D의 레이블 분포가 더 고르지 않습니다.다이어그램의 첫 번째 행에서 ITD를 사용한 대체 비율의 하한이 RTD를 사용한 대체 비율의 하한을 분명히 초과하는 것을 볼 수 있으며, 이는 ITD가 실제로 레이블 불균형 문제를 완화한다는 것을 보여줍니다.손실 및 대체 정확도 학습 손실은 사전 학습 평가를 반영합니다.자체 수정 과정 중 하나인 재RTD는 RTD를 사용한 경우보다 손실이 더 높아 수정 학습의 어려움을 보여줍니다.대체 정확도는 대체된 토큰의 예측 정확도를 나타냅니다.그림 5의 세 번째 줄에서 보듯이 재RTD는 RTD에 비해 상당한 마진으로 대체 정확도가 더 뛰어나 자체 수정 과정의 효과를 보여줍니다. E 코스 수프의 자세한 점수 표 7은 코스 수프 시험의 GLUE 벤치마크에 대한 모든 점수를 나열합니다. 단일 과제에 대한 최적의 결과는 수프 재료에 무작위로 분포된 것으로 보입니다. 균일하고 가중된 통합을 통해 가장 높은 평균 GLUE 점수를 가진 최상의 모델이 등장합니다. 매개변수 최대 에포크 피크 학습률 배치 크기 학습률 감소 워밍업 비율 GLUE 소규모 작업 {2, 3, 5, 10} {2e-5, 3e-5, 4e-5, 5e-5} {16, 32} GLUE 대규모 작업 {2, 3, 5} SQUAD 2.{2, 3} {1e-5, 2e-5, 3e-5, 4e-5}{2e-5, 3e-5, 4e-5, 5e-5} {16, 32} 선형 선형 {6%, 10%} 6% 시퀀스 길이 선형 {6%, 10%} Adam € 1e-1e-le-Adam (B1, B2) (0.9, 0.98) (0.9, 0.98) (0.9, 0.98) 클립 규범 드롭아웃 가중치 감소 0.0.0.0.0.0.표 6: GLUE 및 SQUAD 2.0에서 미세 조정을 위해 검색된 하이퍼파라미터 범위. GLUE 소규모 작업에는 COLA, RTE, MRPC 및 STS-B가 포함됩니다. GLUE 대규모 작업에는 MNLI, QQP, QNLI 및 SST-2가 포함됩니다. GLUE 단일 작업 모델 MNLI -m/-mm Acc 기본 설정: BERT 기본 크기, Wikipedia + Book Corpus Acc QQP QNLI SST-2 COLA RTE MRPC STS-B AVG Acc MCC Acc PCC Acc MCL Lre-MLM Lre-RTD 88.47/88.43 92.23 93.Lre-SLM 88.43/88.43 92.23 93.94.Lre-STD 88.43/88.43 92.23 93.94.Lre-MLM+RTD 88.43/88.33 92.20 93.94.Lre-MLM+SLM 88.50/88.43 92.17 93.94.Lre-MLM+STD 88.43/88.43 92.23 93.94.Lre-RTD+SLM Lre-RTD+STD 88.47/88.43 92.27 93.88.50/88.50 92.23 93.Lre-SLM+STD Lre-SLM+RTD+STD Lre-MLM+SLM+STD 88.47/88.53 92.Lre-MLM+RTD+STD 88.50/88.47 92.27 93.88.47/88.43 92.27 93.93.88.50/88.47 92.23 93.94.94.Lre-MLM+RTD+SLM 88.43/88.40 92.23 93.94.uniform soups weight soups 88.53/88.43 92.23 93.94.88.47/88.43 92.20 93.57 94.88.47/88.47 92.23 93.37 94.13 70.76 84.00 91.57 91.32 88.88.53/88.50 92.23 93.40 94.33 70.53 84.00 92.00 91.18 88.94.13 70.77 83.63 92.40 91.20 88.70.53 83.77 92.00 91.29 88.70.53 83.77 92.00 91.29 88.70.88 83.77 92.17 91.30 88.71.12 83.40 92.40 91.24 88.71.09 83.77 92.17 91.24 88.94.23 70.84 83.27 92.23 91.25 88.94.30 71.00 84.03 92.17 91.25 88.94.13 70.53 83.63 92.94.07 71.61 83.77 92.70.79 84.91.24 88.70.80 83.53 91.93 91.15 88.70.75 84.13 91.77 91.25 88.71.40 83.53 92.23 91.24 88.71.61 84.00 92.40 91.22 88.91.36 88.91.21 88.91.표 7: 코스 수프 시험에 대한 GLUE 데이터 세트의 모든 평가 결과. Acc, MCC, PCC는 각각 정확도, Matthews 상관 관계, Spearman 상관 관계를 나타냅니다. 보고된 결과는 5개의 난수에 대한 중앙값입니다.
