--- ABSTRACT ---
완전 자동화된 객체 재구성 파이프라인은 디지털 콘텐츠 생성에 필수적입니다. 3D 재구성 분야는 엄청난 발전을 이루었지만, 깨끗한 객체 모델을 얻기 위해 배경을 제거하는 것은 여전히 경계 상자 레이블 지정, 마스크 주석 및 메시 조작과 같은 다양한 형태의 수동 작업에 의존합니다. 이 논문에서는 다중 뷰 이미지에서 객체를 자동으로 발견하고 재구성하기 위한 AutoRecon이라는 새로운 프레임워크를 제안합니다. 자체 감독 2D 비전 변환기 기능을 활용하여 전경 객체를 SfM 포인트 클라우드에서 견고하게 찾고 분할할 수 있음을 보여줍니다. 그런 다음 분해된 포인트 클라우드가 제공하는 고밀도 감독을 통해 분해된 신경 장면 표현을 재구성하여 정확한 객체 재구성 및 분할을 수행합니다. DTU, BlendedMVS 및 CO3D-V2 데이터 세트에 대한 실험은 AutoRecon의 효과성과 견고성을 보여줍니다. 코드와 보충 자료는 프로젝트 페이지(https://zju3dv.github.io/autorecon/)에서 확인할 수 있습니다. 구조-모션 변환. 입력 이미지 Reg. SDF Radiance Field DINO 포인트 클라우드 거친 분할 전경 재구성 전경 분할 그림 1. 완전 자동화된 파이프라인과 결과 개요. 객체 중심 비디오가 주어지면, 우리는 점별로 집계된 2D DINO 피처[3]를 사용하여 반밀도 SfM 포인트 클라우드에서 눈에 띄는 전경 객체를 분할하여 거친 분해를 달성합니다. 그런 다음 거친 분해 결과의 도움으로 다중 뷰 이미지에서 분해된 신경 장면 표현을 학습하여 전경 객체를 재구성하고 다중 뷰 일관된 고품질 전경 마스크를 렌더링합니다. 1.
--- INTRODUCTION ---
3D 객체 재구성은 컴퓨터 비전에서 오랫동안 조사되어 왔습니다. 이 연구에서는 다중 시점 이미지에서 눈에 띄는 전경 객체를 재구성하고 주석 없이 배경에서 객체를 자동으로 분할하는 특정 설정에 초점을 맞춥니다. 이를 통해 VR/AR을 위한 확장 가능한 3D 콘텐츠 생성이 가능해지고 감독 학습 작업을 위해 대규모로 무료 2D 및 3D 객체 주석을 생성할 가능성이 열릴 수 있습니다. 기존의 다중 시점 스테레오[8, 32]와 최근의 신경 장면 재구성 방법[40, 46]은 인상적인 재구성 품질을 달성했습니다. 그러나 이러한 방법은 객체를 식별할 수 없으며 재구성된 객체 모델은 일반적으로 주변 배경과 결합됩니다. 간단한 솔루션은 전경 객체 마스크를 사용하여 깨끗한 전경 객체 모델을 얻는 것입니다. 저자는 ZJU-SenseTime Joint Lab of 3D Vision에 소속되어 있습니다. *연락처 저자: Xiaowei Zhou. 그러나 정확한 2D 객체 마스크는 주석을 달기 비용이 많이 들고, 두드러진 객체 분할 기술[21,34,41]은 일반적으로 세분성이 제한된 마스크를 생성하여 특히 얇은 구조가 있는 객체의 경우 재구성 품질을 저하시킵니다.최근 일부 방법[23,30,50]은 3D 객체 경계 상자, 낙서 또는 픽셀 레이블과 같은 최소한의 인간 주석이 주어진 경우 3D 장면에서 객체를 자동으로 분해하려고 시도합니다.그러나 수동 주석이 필요하기 때문에 확장성이 더 뛰어난 3D 콘텐츠 생성의 실현 가능성이 제한됩니다.이 논문에서는 그림 1에서 설명한 것처럼 두드러진 객체의 완전 자동화된 3D 재구성을 위한 새로운 2단계 프레임워크를 제안합니다.먼저 대략적 분해를 수행하여 전경 SfM 포인트 클라우드를 자동으로 분할한 다음, 대략적 분해에서 명시적 감독 하에 암묵적 신경 장면 표현을 학습하여 전경 객체 지오메트리를 재구성합니다. 우리의 거친 분해의 핵심 아이디어는 자기 감독 2D 비전 변환기(ViT) [3]가 제공하는 의미적 특징을 활용하는 것입니다. 구체적으로, 우리는 입력 이미지에서 다중 뷰 ViT 특징을 SfM 포인트 클라우드로 집계한 다음 포인트 클라우드 분할 변환기로 눈에 띄는 전경 포인트를 분할합니다. 대규모 레이블이 지정되지 않은 데이터에서 변환기를 학습하기 위해, 우리는 정규화된 컷 [33]을 기반으로 하는 의사 지상 진실 생성 파이프라인을 고안하고 학습 시 정확한 분할과 3D 경계 상자를 생성하는 기능을 보여줍니다. 객체 재구성을 위해, 우리는 다중 뷰 이미지에서 추정된 전경 경계 상자 내에서 신경 장면 표현을 학습합니다. 우리의 주요 아이디어는 이전에 분해된 포인트 클라우드가 제공하는 명시적 정규화의 도움으로 분해된 장면 표현을 재구성하는 것입니다. 마지막으로, 우리는 깨끗한 객체 모델을 추출하고 전경 전용 렌더링으로 고품질 객체 마스크를 얻을 수 있습니다. 우리는 제안된 파이프라인의 효과를 검증하기 위해 CO3D [29], BlendedMVS [45], DTU [12] 데이터 세트에 대한 실험을 수행했습니다. 실험 결과는 우리의 접근 방식이 복잡한 배경에서도 RGB 비디오에서 정확한 3D 객체 모델과 고품질 분할 마스크를 자동으로 강력하게 복구할 수 있음을 보여줍니다. 요약하면, 우리는 다음과 같은 기여를 합니다. • 우리는 주석 없이 다중 뷰 이미지에서 배경 없는 객체 모델을 재구성하기 위한 완전 자동화된 프레임워크를 제안합니다. • 우리는 먼저 SfM 포인트 클라우드 형태로 장면을 분해하여 장면 분해를 위한 대략적-정밀 파이프라인을 제안합니다. 그런 다음 이를 통해 신경 장면 표현의 분해가 안내됩니다. • 우리는 SfM 포인트 클라우드 분할 변환기를 제안하고 이를 훈련하기 위한 비지도 가상 지상 진실 생성 파이프라인을 고안합니다. • 우리는 3D 모델, 3D 경계 상자, 2D 분할 마스크가 있는 객체 데이터 세트를 자동으로 생성할 가능성을 보여줍니다. 2.
--- RELATED WORK ---
멀티뷰 3D 객체 재구성. 멀티뷰 이미지에서 3D 객체를 재구성하는 것은 오랫동안 광범위한 응용 프로그램을 통해 연구되어 왔습니다. 멀티뷰 이미지 외에도 관심 객체를 주변과 분리하고 선택적으로 추가적인 기하학적 제약을 제공하기 위해 정확한 객체 마스크가 필요합니다. 멀티뷰 스테레오(MVS)
--- METHOD ---
s [40, 46]는 인상적인 재구성 품질을 달성했습니다. 그러나 이러한 방법은 객체를 식별할 수 없으며 재구성된 객체 모델은 일반적으로 주변 배경과 결합됩니다. 간단한 솔루션은 전경 객체 마스크를 사용하여 깨끗한 전경 객체 모델을 얻는 것입니다. 저자는 ZJU-SenseTime Joint Lab of 3D Vision에 소속되어 있습니다. *연락 저자: Xiaowei Zhou. 그러나 정확한 2D 객체 마스크는 주석을 달기 비용이 많이 들고 눈에 띄는 객체 분할 기술 [21,34,41]은 일반적으로 세분성이 제한된 마스크를 생성하여 특히 얇은 구조가 있는 객체의 경우 재구성 품질을 저하시킵니다. 최근 일부 방법 [23,30,50]은 3D 객체 경계 상자, 낙서 또는 픽셀 레이블과 같은 최소한의 인간 주석이 주어진 경우 3D 장면에서 객체를 자동으로 분해하려고 시도합니다. 그러나 수동 주석 요구 사항으로 인해 확장성이 더 뛰어난 3D 콘텐츠 생성의 실현 가능성이 제한됩니다. 이 논문에서 우리는 그림 1에 나와 있는 것처럼 두드러진 객체의 완전 자동화된 3D 재구성을 위한 새로운 2단계 프레임워크를 제안합니다. 우리는 먼저 전경 SfM 포인트 클라우드를 자동으로 분할하기 위해 대략적 분해를 수행한 다음, 대략적 분해에서 명시적 감독 하에 암묵적 신경 장면 표현을 학습하여 전경 객체 지오메트리를 재구성합니다. 우리의 대략적 분해의 핵심 아이디어는 자체 감독 2D 비전 변환기(ViT) [3]에서 제공하는 의미적 특징을 활용하는 것입니다. 구체적으로, 우리는 입력 이미지에서 SfM 포인트 클라우드로 멀티뷰 ViT 특징을 집계한 다음, 포인트 클라우드 분할 변환기로 두드러진 전경 포인트를 분할합니다. 대규모 레이블이 지정되지 않은 데이터에서 변환기를 학습하기 위해, 우리는 Normalized Cut [33]을 기반으로 하는 의사 지상 진실 생성 파이프라인을 고안하고 학습 시 정확한 분할과 3D 경계 상자를 생성하는 기능을 보여줍니다. 객체 재구성을 위해, 우리는 멀티뷰 이미지에서 추정된 전경 경계 상자 내의 신경 장면 표현을 학습합니다. 우리의 주요 아이디어는 이전에 분해된 포인트 클라우드가 제공하는 명시적 정규화의 도움으로 분해된 장면 표현을 재구성하는 것입니다. 마지막으로, 깨끗한 객체 모델을 추출하고 전경 전용 렌더링으로 고품질 객체 마스크를 얻을 수 있습니다. 우리는 다음을 수행합니다.
--- EXPERIMENT ---
DTU, BlendedMVS 및 CO3D-V2 데이터 세트의 s는 AutoRecon의 효과성과 견고성을 보여줍니다. 코드와 보충 자료는 프로젝트 페이지(https://zju3dv.github.io/autorecon/)에서 사용할 수 있습니다. 구조-모션 변환. 입력 이미지 Reg. SDF Radiance Field DINO 포인트 클라우드 거친 분할 전경 재구성 전경 분할 그림 1. 완전 자동화된 파이프라인 및 결과 개요. 객체 중심 비디오가 주어지면 점별로 집계된 2D DINO 피처[3]를 사용하여 반밀도 SfM 포인트 클라우드에서 눈에 띄는 전경 객체를 분할하여 거친 분해를 달성합니다. 그런 다음 거친 분해 결과의 도움으로 다중 뷰 이미지에서 분해된 신경 장면 표현을 학습하여 전경 객체를 재구성하고 다중 뷰 일관된 고품질 전경 마스크를 렌더링합니다. 1. 소개 3D 객체 재구성은 컴퓨터 비전에서 오랫동안 조사되어 왔습니다. 이 연구에서는 다중 시점 이미지에서 눈에 띄는 전경 객체를 재구성하고 주석 없이 배경에서 객체를 자동으로 분할하는 특정 설정에 초점을 맞춥니다.이를 통해 VR/AR을 위한 확장 가능한 3D 콘텐츠 생성이 가능해지고 감독 학습 작업을 위해 대규모로 무료 2D 및 3D 객체 주석을 생성할 가능성이 열릴 수 있습니다.기존의 다중 시점 스테레오[8, 32]와 최근의 신경 장면 재구성 방법[40, 46]은 인상적인 재구성 품질을 달성했습니다.그러나 이러한 방법은 객체를 식별할 수 없으며 재구성된 객체 모델은 일반적으로 주변 배경과 결합됩니다.간단한 솔루션은 전경 객체 마스크를 사용하여 깨끗한 전경 객체 모델을 얻는 것입니다.저자는 ZJU-SenseTime Joint Lab of 3D Vision에 소속되어 있습니다.*연락처 저자: Xiaowei Zhou. 그러나 정확한 2D 객체 마스크는 주석을 달기 비용이 많이 들고, 두드러진 객체 분할 기술[21,34,41]은 일반적으로 세분성이 제한된 마스크를 생성하여 특히 얇은 구조가 있는 객체의 경우 재구성 품질을 저하시킵니다.최근 일부 방법[23,30,50]은 3D 객체 경계 상자, 낙서 또는 픽셀 레이블과 같은 최소한의 인간 주석이 주어진 경우 3D 장면에서 객체를 자동으로 분해하려고 시도합니다.그러나 수동 주석이 필요하기 때문에 확장성이 더 뛰어난 3D 콘텐츠 생성의 실현 가능성이 제한됩니다.이 논문에서는 그림 1에서 설명한 것처럼 두드러진 객체의 완전 자동화된 3D 재구성을 위한 새로운 2단계 프레임워크를 제안합니다.먼저 대략적 분해를 수행하여 전경 SfM 포인트 클라우드를 자동으로 분할한 다음, 대략적 분해에서 명시적 감독 하에 암묵적 신경 장면 표현을 학습하여 전경 객체 지오메트리를 재구성합니다. 우리의 거친 분해의 핵심 아이디어는 자기 감독 2D 비전 변환기(ViT) [3]가 제공하는 의미적 특징을 활용하는 것입니다. 구체적으로, 우리는 입력 이미지에서 다중 뷰 ViT 특징을 SfM 포인트 클라우드로 집계한 다음 포인트 클라우드 분할 변환기로 눈에 띄는 전경 포인트를 분할합니다. 대규모 레이블이 지정되지 않은 데이터에서 변환기를 학습하기 위해, 우리는 정규화된 컷 [33]을 기반으로 하는 의사 지상 진실 생성 파이프라인을 고안하고 학습 시 정확한 분할과 3D 경계 상자를 생성하는 기능을 보여줍니다. 객체 재구성을 위해, 우리는 다중 뷰 이미지에서 추정된 전경 경계 상자 내에서 신경 장면 표현을 학습합니다. 우리의 주요 아이디어는 이전에 분해된 포인트 클라우드가 제공하는 명시적 정규화의 도움으로 분해된 장면 표현을 재구성하는 것입니다. 마지막으로, 우리는 깨끗한 객체 모델을 추출하고 전경 전용 렌더링으로 고품질 객체 마스크를 얻을 수 있습니다. 우리는 제안된 파이프라인의 효과를 검증하기 위해 CO3D [29], BlendedMVS [45], DTU [12] 데이터 세트에 대한 실험을 수행했습니다. 실험 결과는 우리의 접근 방식이 복잡한 배경에서도 RGB 비디오에서 정확한 3D 객체 모델과 고품질 분할 마스크를 자동으로 견고하게 복구할 수 있음을 보여줍니다. 요약하면, 우리는 다음과 같은 기여를 합니다. • 우리는 주석 없이 다중 뷰 이미지에서 배경 없는 객체 모델을 재구성하기 위한 완전 자동화된 프레임워크를 제안합니다. • 우리는 먼저 SfM 포인트 클라우드 형태로 장면을 분해하여 장면 분해를 위한 대략적-정밀 파이프라인을 제안합니다. 그런 다음 이를 통해 신경 장면 표현의 분해가 안내됩니다. • 우리는 SfM 포인트 클라우드 분할 변환기를 제안하고 이를 훈련하기 위한 비지도 가상 지상 진실 생성 파이프라인을 고안합니다. • 우리는 3D 모델, 3D 경계 상자, 2D 분할 마스크가 있는 객체 데이터 세트를 자동으로 생성할 가능성을 보여줍니다. 2. 관련 작업 다중 뷰 3D 객체 재구성. 다중 뷰 이미지에서 3D 객체를 재구성하는 것은 오랫동안 광범위한 응용 프로그램을 통해 연구되어 왔습니다. 다중 뷰 이미지 외에도 관심 객체를 주변에서 분리하고 선택적으로 추가적인 기하학적 제약 조건을 제공하기 위해 정확한 객체 마스크가 필요합니다. 다중 뷰 스테레오(MVS) 방법[32,44]은 프레임별 깊이 맵을 복구한 다음 객체 마스크 내에서만 깊이를 융합하여 배경 없는 재구성을 복구합니다. 최근, 미분 가능한 신경 렌더러와 장면 표현을 기반으로 하는 신경 재구성 방법이 엄청난 발전을 이루었습니다. 표면 렌더링 기반 방법[26,47]은 3D 감독을 없애지만 여전히 대체 기하학적 제약 조건으로 객체 마스크에 의존합니다. 최근의 볼륨 렌더링 기반 재구성 방법[27, 40, 46]은 마스크 없는 학습을 허용하지만 여전히 배경 없는 객체 모델을 생성하려면 객체 마스크 감독이 필요합니다. 객체 마스크 외에도 기존 방법은 전경 객체의 3D 공간적 범위에 대한 수동 주석도 필요합니다. 대신, 우리는 인간 레이블이 없는 완전 자동화된 객체 재구성 파이프라인을 제안하는데, 이는 3D 객체 재구성의 유용성과 확장성을 더욱 개선합니다.신경 장면 표현의 분해.최근의 많은 연구에서 신경 장면 표현(NSR)을 분해하려고 시도합니다.필요한 주석을 기준으로 관련 작업을 분류합니다.명시적인 3D 기하학적 기본 요소는 다양한 엔터티의 간단하지만 효과적인 분해를 제공합니다.NeRF++[50]는 구로 전경과 배경을 분리합니다.범주별 모델에 의해 수동으로 주석이 달리거나 예측된 3D 경계 상자는 정적 및 동적 장면의 분해된 모델링에 사용됩니다[16,24,28].다중 뷰 분할 마스크는 장면 분해를 위한 고밀도 주석을 제공합니다.의미 필드는 의미적 장면 분해를 위해 다중 뷰 의미적 마스크[15,39,51]로 학습될 수 있음이 밝혀졌습니다.게다가 분해된 객체 표현은 다중 뷰 객체 마스크[42, 43]에서도 구축할 수 있습니다. 다중 뷰 분할의 주석 비용을 완화하기 위해 인간 상호작용[52]과 낙서[30] 및 시드 포인트[23]와 같은 다양한 형태의 희소 인간 주석에 의존하는 방법이 제안됩니다.분해는 수동으로 지정된 엔터티를 구별하기 위해 다양한 소스의 수작업 비전용 기능에 의존하기 때문에 덜 안정적입니다.DFF[14] 및 N3F[38]는 불연속적인 의미 레이블을 학습하는 것 외에도 쿼리 기반 장면 분해를 위해 2D 기능을 신경 장면 표현으로 정제합니다.그러나 여전히 수동으로 제공된 쿼리가 필요하고 쿼리 기반 특성은 로컬 편집에 더 적합하며 눈에 띄는 객체의 분해와 같이 장면에 대한 전역 추론이 필요한 애플리케이션을 방해합니다.기존 접근 방식과 달리 파이프라인은 주석이 필요 없고 전역 추론을 용이하게 합니다.비지도 객체 발견.비지도 객체 발견(UOD)은 대규모 데이터 세트에서 객체 개념의 비지도 학습을 목표로 합니다. 최근 많은 연구가 구성적 생성 모델링[2,6,9]을 사용하여 UOD를 시도하고 있습니다.Slot Attention[18]은 이미지에서 직접 객체 중심 표현을 추론하는 것을 용이하게 합니다.이 아이디어는 신경 광도장 또는 광장을 사용한 3D 인식 모델링 및 추론으로 더욱 확장됩니다[31, 35, 36, 48].그러나 이러한 연구는 합성 데이터 세트에서만 작동하는 것으로 나타났으며 복잡한 실제 상황에는 적용할 수 없습니다.우리의 방법은 분해된 단일 객체 표현의 복구에 초점을 맞추고 캐주얼 비디오 캡처와 같은 실제 데이터에서 작동하는 것으로 나타났습니다.또 다른 최근 트렌드는 객체 로컬라이제이션[34], 두드러진 감지 및 의미 분할[49]과 같은 다양한 형태의 비지도 객체 발견을 위해 자체 감독 시각적 표현을 사용합니다.TokenCut[41] 및 DSM[21]은 스펙트럼 클러스터링을 사용하여 두드러진 객체를 로컬라이제이션하고 분할하여 유망한 결과를 보여줍니다. 그러나 2D 특성으로 인해 객체 중심 비디오에 적용하면 다중 뷰 불일치와 불안정한 결과가 발생합니다. 이러한 한계를 극복하기 위해 로컬 2D 관찰에 대한 많은 고립된 추론 대신 글로벌 표현에 대한 일관된 두드러진 객체 발견을 용이하게 하는 3D 비디오에서 비지도 객체 발견을 수행하는 것을 제안합니다. 3. 예비 단계 이 섹션에서는 다음과 같은 예비 단계를 간략히 살펴봅니다. 포인트 클라우드를 분할하는 데 사용된 자체 지도 ViT 기능, 가상 분할 레이블을 생성하는 데 사용된 정규화 절단 알고리즘, 객체 재구성에 사용된 신경 표면 재구성 방법 NeuS. 자체 지도 ViT. Vision Transformer[5]는 H x W 크기의 이미지 I를 P × P 크기의 2D 패치 Ip 시퀀스로 평면화합니다. 각 이미지 패치에는 학습 가능한 선형 투영이 포함되고 위치 임베딩이 추가됩니다. 특별한 학습 가능한 [CLS] 토큰은 일반적으로 글로벌 및 상황 정보를 모델링하기 위해 패치 시퀀스에 추가됩니다. 토큰 임베딩의 1D 시퀀스는 다중 헤드 자기 주의(MSA) 및 MLP 층으로 구성된 여러 Transformer 인코더 블록에 공급됩니다. z = MSA(LN(z−1)) + ze ze e-&quot; z² = MLP(LN(z&quot;)) + z&quot;, (1) 여기서 z는 l번째 Transformer 인코더 층의 출력입니다. [3]에서 자기 감독 ViT 피처에는 장면 레이아웃 및 객체 경계와 같은 명확한 의미 정보가 포함되어 있으며, 이는 감독된 대응물에서 찾을 수 없다는 것이 밝혀졌습니다. 정규화된 절단 알고리즘(NCut) [33]. 스펙트럼 클러스터링은 그래프 분할에서 유래한 널리 사용되는 클러스터링 기술입니다. 데이터 포인트 집합 x¿가 주어지면 스펙트럼 클러스터링은 무향 그래프 G = (V, E)를 빌드하고 두 개의 분리된 집합 A, B로 분할합니다. 각 데이터 포인트 x¿는 정점 v¿에 해당하고 각 그래프 에지의 가중치 w(i, j)는 두 데이터 포인트. 정규화된 컷(NCut)은 스펙트럼 클러스터링에 널리 사용되는 기준으로, [33]에 표시된 대로 일반화된 고유값 문제를 풀면 효율적으로 최소화할 수 있습니다. NeuS를 사용한 신경 표면 재구성. NeuS [40]는 부호 거리 함수(SDF) ƒ : R3 R의 0레벨 집합을 사용하여 표면 S {x = R³ | f(x) = 0}을 나타내고 광도장 c(x, v)로 모양을 모델링합니다. SDF 기반 광도장은 볼륨 렌더링을 통해 렌더링됩니다. 카메라 중심을 o로 나타내고 뷰 방향을 v로 나타내는 광선 {r(t) = 0 + tv| t &gt; 0}이 주어지면 색상 Ĉ를 ĉ = [* w(t) c(r(t), v)dt로 렌더링할 수 있습니다. 여기서 w(t)는 [40]에 자세히 설명된 대로 편향되지 않고 폐색을 인식하는 가중치 함수입니다. 특히, 관심 있는 전경 객체의 공간적 범위는 수동으로 주석을 달아야 하며, 단위 구로 축척하고 SDF 기반 광도 필드로 표현합니다. 구 밖의 배경 영역은 NeRF++로 표현합니다[50]. 관심 객체를 단일 구로만 둘러쌀 수 없기 때문에 재구성된 객체 모델에는 배경 지오메트리가 포함되어 이를 제거하기 위해 수동 사후 처리가 필요합니다. 4. 방법 파이프라인 개요는 그림 1에 나와 있습니다. 객체 중심 비디오가 주어지면 재구성된 지오메트리에서 고품질 2D 마스크를 렌더링할 수 있는 눈에 띄는 전경 객체를 자동으로 분해하고 재구성하는 것을 목표로 합니다. 이 목표를 달성하기 위해 포인트 클라우드 분해의 도움으로 신경 장면 표현을 분해하는 새로운 대략-미세 파이프라인을 제안합니다. 대략 분해 단계는 장면 수준 SfM 포인트 클라우드에서 전경 객체를 분할하고 컴팩트한 3D 경계 상자를 추정합니다(4.1절). 그런 다음, 전경 객체의 분해된 신경 장면 표현은 대략 분해(4.2절)의 명시적 감독 하에 복구됩니다.4.1. 두드러진 객체의 대략 분해 전경 객체를 대략 분해하기 위해 먼저 SfM 포인트 클라우드를 재구성하고 다중 뷰 DINO[3] 기능을 융합합니다.그런 다음, 포인트 클라우드는 가벼운 3D 분할 변환기로 분할되고, 여기서 두드러진 전경 객체의 3D 경계 상자가 생성됩니다.대략 분해 파이프라인은 그림 2에 나와 있습니다.수동 주석을 사용할 수 없다고 가정하므로, 그림 3에 나와 있는 것처럼 가상 지상 진실 분할을 생성하는 비지도 포인트 클라우드 분할 파이프라인을 고안합니다.3D 분할 변환기는 학습 후 비지도 파이프라인보다 성능이 뛰어나 더 큰 규모의 포인트 클라우드에 적용할 수 있습니다.신경 포인트 클라우드 재구성. 우리는 SfM이 일반적으로 카메라 포즈 복구를 위한 고밀도 재구성 이전에 수행되기 때문에 효율적인 거친 분해를 위해 SfM 포인트 클라우드를 활용합니다.특히, 우리는 SfM을 위해 최근의 반고밀도 이미지 매처 LoFTR[37]을 사용하여 재구성합니다.2D DINO 특징 입력 이미지 추출 SfM 평균[CLS] 0!2D 특징을 포인트 클라우드로 집계 DINO 특징이 있는 포인트 클라우드 선형 변환.가상 GTS 분할 레이블 추정된 경계 상자 지면 1. 신경 포인트 클라우드 재구성 2. 3D 분할 변환기 그림 2. 거친 분해.객체 중심 이미지 시퀀스가 주어지면 먼저 반고밀도 구조-움직임(SfM) 포인트 클라우드를 재구성하고 PCA 투영 색상에서 알 수 있듯이 의미가 풍부한 다중 뷰 2D DINO 특징을 집계하여 포인트별 특징을 추출합니다. 그런 다음, 가벼운 3D Transformer를 사용하여 SfM 포인트 클라우드에서 전경 객체를 분할합니다.이 변환기는 포인트별 피처(☐☐)와 글로벌 [CLS] 피처()를 입력으로 사용하여 포인트별 레이블(10)을 예측합니다.마지막으로, 분해된 포인트 클라우드에서 객체의 3D 경계 상자와 선택적 지면 평면을 추정합니다.반밀도 포인트 클라우드.[11]에서 설명하는 것처럼 텍스처가 낮은 객체라도 전경 객체의 전체 지오메트리를 복구할 수 있습니다.이 기능은 희소 키포인트 기반 SfM에서는 신뢰성이 떨어지는 객체의 전체 공간적 범위를 견고하게 찾는 데 매력적입니다.3D 분할을 용이하게 하기 위해 자체 감독 2D ViT 피처를 3D로 끌어올립니다.더 구체적으로, SfM이 유지하는 명시적 3D-2D 대응 관계 덕분에 프레임별 DINO-ViT 피처가 추출되어 반밀도 포인트 클라우드에 집계됩니다. 우리는 다중 뷰 피처를 간단한 평균화 연산과 융합하는 것이 우리 작업에 충분하다는 것을 발견했습니다. 또한, 각 프레임을 전역적으로 설명하는 [CLS] 토큰의 프레임별 피처도 포인트 클라우드의 전역 설명으로 융합되고 제안된 3D Transformer에서 분할 프로토타입으로 추가로 사용됩니다. Transformer를 사용한 포인트 클라우드 분할. 그림 2에서 볼 수 있듯이 신경 포인트 클라우드에는 이미 전경과 배경을 구분하는 차별적 의미적 피처가 포함되어 있습니다. 따라서 적절한 귀납적 편향이 있고 제한된 감독으로 훈련된 간결한 네트워크가 우리 작업에서 눈에 띄는 객체를 탐색하기에 충분하다고 가정합니다. 우리는 두 개의 Transformer 인코더 레이어와 선형 어텐션만 있는 효율적인 포인트 클라우드 Transformer를 구축합니다[13]. 이전에 구축한 신경 포인트 클라우드에서 얻은 전역 [CLS] 토큰과 포인트별 토큰은 위치 인코딩과 함께 추가되고 인코더에 의해 변환됩니다. 그런 다음 변환된 [CLS] 토큰은 입력 종속 분할 프로토타입으로 처리되며, 이는 포인트별 피처와 상관되어 분할 마스크를 생성합니다. 영어: 저희의 설계는 [CLS] 토큰의 전역 정보를 최대한 활용하여 중요한 객체와 분할을 위한 다른 점별 피처와의 전역-지역 관계에 대해 추론합니다. 사전 훈련된 2D ViT 피처를 사용하면 Transformers를 훈련하는 데 대규모 데이터에 대한 의존도를 줄일 수 있습니다. 비지도 분할을 사용한 데이터 세트 생성. 3D Transformer에 대한 훈련 데이터를 생성하기 위해 강력한 분할을 생성할 수 있지만 계산적으로 더 집약적인 비지도 SfM 분할 파이프라인을 제안합니다. 장면 수준의 전역 추론을 용이하게 하기 때문에 이전에 구축된 신경 점 클라우드에 NCut을 적용할 것을 제안합니다. 제안된 파이프라인을 사용하면 의사 기준 진실 분할이 있는 대규모 데이터 세트를 자동으로 생성할 수 있습니다. 개요는 그림 3에 나와 있습니다. 3D 분할을 위해 신경 점 클라우드에 NCut 알고리즘을 적용하기 위해 신경 점 클라우드를 사용하여 완전히 연결된 그래프 G = (V,E)를 구축합니다. 여기서 각 그래프 정점 V;는 3D 점에 해당합니다. V₂와 Vj 사이의 에지 가중치 w(i, j)를 모델링할 때 피처 유사성과 공간적 친화성을 결합합니다. DINO 피처는 의미적으로 풍부하지만 계층적이며 추론된 눈에 띄는 객체는 부분-전체 계층 구조에서 위치 측면에서 모호할 때가 있습니다. 특히 복잡한 구조를 가진 객체의 경우 특정 객체 부분에 의해 지배되는 눈에 띄는 현상을 피하기 위해 그룹화된 코사인 유사성을 제안합니다. 공식적으로 MSA 모듈의 h개 헤드에서 다중 헤드 어텐션 피처 Z₁ = {z,..., z½-1}의 그룹을 표시하면 Zi와 Zj 사이의 그룹화된 코사인 유사성 S*를 계산합니다. S* (Zi, Zj) = max S(z, z), (3) kЄ{0,...,h-1} 여기서 S는 코사인 유사성입니다. 직관은 다중 헤드 피처 그룹 간에 최대 유사성을 취하는 것은 어떤 측면에서든 유사하다면 두 개의 높은 유사성을 갖는 점을 할당하여, 객체의 국소 부분에 의해서만 두드러짐이 지배될 가능성을 줄입니다.정규화된 컷 거친 분할 및 경계 상자 +000-000SDF 필드 색상 필드 S, C SDF 광도 필드 전경 렌더링 점 구름 정규화.지면 평면 정규화.마스크 렌더링 양수 O-무시됨 o 음수 Q 가상 레이블 생성 가상 GT 그림 3. 가상 지상 진실 생성 및 레이블 정의.레이블이 지정되지 않은 데이터로 점 구름 분할 변환기를 학습하기 위해 가상 레이블을 생성하는 비지도 파이프라인을 제안합니다.정규화된 컷[33](NCut)으로 다운샘플링된 신경 점 구름을 분할하고 전경 점에 대한 경계 상자를 추정합니다. 분할 노이즈를 고려하여 NCut의 전경 분할을 양의 샘플(O)로, 경계 상자 외부의 배경 점을 음의 샘플(O)로 처리합니다. 경계 상자 내에 있는 배경 분할은 분할 노이즈(O)로 간주되므로 학습에서 무시합니다. 그런 다음 포인트 클라우드를 위에서 정의한 그래프에서 NCut으로 분할합니다. 그런 다음 전경 포인트 클라우드의 평면 정렬 주성분 분석을 기반으로 방향이 지정된 3D 경계 상자를 추론합니다. 파이프라인은 그림 3에 나와 있습니다. 무감독 분할 파이프라인에 대한 자세한 내용은 부록에 나와 있습니다. 4.2. 배경 없는 두드러진 객체 재구성 배경 없는 두드러진 객체 모델을 재구성하기 위해 거친 분해 결과로 장면을 명시적으로 분할하고 각 분할을 다중 뷰 포즈 이미지에 대해 학습된 신경 장면 표현[22, 40]으로 별도로 모델링합니다. 또한 여러 제약 조건을 최적화에 통합하여 주변 환경에서 전경 객체를 수렴하고 분해하는 것을 용이하게 합니다. 그림 4는 전경 모델링을 보여줍니다. 분해된 장면 모델링. 이전 방법[40,47]은 전경 객체를 공간적 범위에 대한 수동 주석과 함께 단위 구로 크기 조정하고, 마스크 컬링이나 재구성된 메시의 수동 조작에 더 의존하여 배경 기하 구조를 제거합니다. 대신, 4.1절에서 추정된 객체 경계 상자 덕분에 수동 주석 없이 장면을 더 세밀하게 세 부분으로 명시적으로 분할합니다. 보다 구체적으로, 객체 경계 상자 내의 영역을 나타내기 위해 SDF 기반 광도장[40]을 사용하고, 상자 외부의 영역에는 NeRF를 사용합니다. addiForeground Modeling 그림 4. 두드러진 객체 재구성 및 2D 마스크 렌더링. SDF 기반 광도장[40]으로 거친 경계 상자에 둘러싸인 두드러진 전경 객체를 모델링합니다. 경계 상자 내부, 경계 상자 외부 및 지면 근처 영역에 대한 별도의 필드로 구성된 분해된 장면 표현을 사용합니다. 우리는 더욱 강력한 전경 분해를 위해, 거친 분해 결과, 즉 분할된 전경 SfM 포인트 클라우드와 추정된 지면 평면을 사용하여 SDF 기반 광도장의 최적화를 정규화합니다. 재구성 후, 우리는 고품질의 다중 뷰 일관성 있는 2D 객체 마스크를 렌더링할 수 있습니다. 객체를 지지하는 지면 평면 주변 영역을 모델링하기 위해 일반적으로 작은 NeRF를 사용합니다. 이 영역은 객체 경계 상자의 바닥 평면에서 찾을 수 있습니다. 내부 영역과 지면 평면 영역 사이에 겹치는 부분이 있지만, NeRF는 일반적으로 SDF 기반 광도장보다 더 빠르게 수렴하므로 겹친 영역을 차지하기 위한 귀납적 편향이 있습니다. 우리는 MipNeRF-360 [1]에서 L∞ 규범을 가진 수축 함수의 전경-객체 인식 버전을 사용하여 무제한 장면을 모델링합니다. 자세한 내용은 부록에서 제공합니다. 거친 분해를 사용한 명시적 정규화. 우리는 경험적으로 분해된 모델링만으로는 전경 객체를 주변으로부터 견고하게 분리할 수 없다는 것을 발견했는데, 특히 얇은 구조와 닫힌 접촉 영역에 대해서 그렇다. 따라서 우리는 분할된 전경 SfM 포인트 클라우드와 추정된 지면 평면을 포함한 거친 분해 결과의 기하학적 단서를 활용하여 SDF 기반 광도장을 훈련하기 위한 추가 정규화를 제공한다. 첫째, 추정된 지면 평면 Pg에 위치한 SfM 포인트 x Є Pg의 부호 없는 거리 |f(x)|는 하한 0(x)보다 크게 제한된다: Lg = Σ max(0(x) = |f(x)\,0), Ng XЄPg . 0(x) = µ(x) + λ · σ(x), (4) 여기서 µ(x)와 σ(x)는 포인트 x와 가장 가까운 K 이웃 사이의 부호 없는 거리의 평균과 표준 편차이다. 이 제약으로 인해 전경 네트워크가 지면 평면을 모델링하지 못한다. 또한 전경 SfM 포인트 클라우드는 Geo-NeuS [7]와 유사하게 부호 거리장을 정규화하기 전에 거친 표면으로 간주됩니다.이 정규화는 수렴 속도를 높이고 모양-광도 모호성을 완화하며 얇은 구조의 재구성 품질을 개선할 수 있습니다.[7]과 같이 각 SfM 포인트의 SDF 값을 0으로 직접 제한하는 대신 SfM 포인트 클라우드의 노이즈를 고려합니다.특히 전경 SfM 포인트 클라우드 Pfg에서 각 포인트 x = Pƒg의 위치 불확실성 T(x)를 Eq. (4)의 0(x)와 유사한 이웃 포인트까지의 거리로 모델링합니다.그런 다음 x의 부호 없는 거리 |f(x)를 7(x)보다 작게 제한합니다.=Σ max(|f(x)| -7(x),0). Lfg Nfg xEP fg (5) 경계가 뚜렷한 고품질 전경 렌더링을 더욱 향상시키기 위해 개체 경계 상자와 교차하는 각 광선 r = Rfg의 누적 가중치 O(r)에 사전 [19] Lbin 베타 분포를 추가합니다. 마지막으로 샘플링된 전경 포인트에 eikonal 항 Leik [10]을 사용합니다. 총 손실은 다음과 같습니다. L = : Lcolor + αLeik + ß£g + V£ fg + SLbin. (6) 전경 렌더링 및 두드러진 개체 추출. 전경 개체의 재구성된 SDF 기반 광도 필드를 사용하여 다중 뷰 일관된 2D 마스크를 쉽게 렌더링하고 메시를 추출할 수 있습니다. 재구성에서 전경 개체와 배경을 다른 필드로 모델링하므로 개체 경계 상자와 교차하는 각 광선을 따라 전경 필드의 누적 가중치를 임계값 0.5로 이진화한 개체 마스크로 계산합니다. Marching Cubes [20]를 사용하여 개체 메시를 추출합니다. 우리는 분해된 장면 모델링 덕분에 후처리 없이 배경이 없는 객체 3D 모델을 얻을 수 있습니다.4.3. 구현 세부 정보 SfM 재구성에 사용된 입력 이미지는 최대 720,000 영역으로 크기가 조정됩니다.DINO-VIT의 ViT-S/8 버전에서 프레임별 2D 특징을 추출합니다.데이터 생성 파이프라인을 사용하여 다양한 범주를 포함하는 CO3D-V2 데이터 세트에서 880개의 객체 중심 비디오를 처리합니다.모든 의자 객체는 검증을 위한 홀드아웃 세트로 보관합니다.이를 통해 학습을 위한 800개의 객체와 검증을 위한 80개의 객체가 생성됩니다.3D 분할 변환기를 20개의 에포크 동안 학습합니다.다중 해상도 해시 인코딩[25]과 장면 표현의 모든 필드에서 별도의 제안 MLP[1]를 사용합니다.단일 NVIDIA V100 GPU에서 2시간이 걸리는 60k 반복 동안 장면 표현을 학습합니다.식 (6)의 모든 손실 가중치는 0.1로 설정됩니다. 명시적 정규화 항목은 초기 15k 반복 동안에만 적용되며 손실 가중치는 점차적으로 0으로 어닐링됩니다.CO3D BlendedMVS 참조.이미지 추출 메시 Fg.렌더링 그림 5. 배경 없는 돌출 객체 재구성 결과.5. 실험 5.1. 데이터 세트 CO3D-V2 [29], BlendedMVS [45] 및 DTU [12] 데이터 세트에서 제안된 방법을 평가합니다.CO3D에는 50개 MS-COCO 범주의 객체에 대한 19,000개 비디오 시퀀스가 포함되어 있습니다.CO3D의 많은 객체에는 얇은 구조가 포함되어 있어 SfM 포인트 클라우드에서 감지 및 분할하기 어렵습니다.CO3D 데이터 세트를 사용하여 3D 돌출 객체 감지 및 2D 분할을 평가하여 어려운 객체와 캐주얼 캡처에 대한 방법의 기능을 보여줍니다.3D 감지를 평가하기 위해 주어진 MVS 포인트 클라우드를 기반으로 의자 범주의 객체에 대한 기준 진실 3D 경계 상자에 수동으로 주석을 달았습니다. 또한 5개 객체의 자세한 2D 전경 마스크에 주석을 달아 2D 분할을 평가합니다. BlendedMVS와 DTU 데이터 세트는 3D 재구성에 널리 사용됩니다. 이러한 데이터 세트를 사용하여 3D 두드러진 객체 감지, 재구성 및 2D 분할을 평가합니다. BlendedMVS에서 제공하는 메시에는 배경이 포함되므로 전경 메시를 수동으로 분할하고 5개 객체의 멀티뷰 마스크를 렌더링하여 평가합니다. 전경 객체 메시는 또한 기준 진실 3D 경계 상자를 생성하는 데 사용됩니다. 재구성 결과를 평가할 때 모든 메시는 객체 마스크로 사전 처리됩니다. 5.2. 3D 두드러진 객체 감지 이 부분에서는 분할된 전경 포인트 클라우드에서 추론된 3D 경계 상자를 기반으로 거친 분해 결과를 평가합니다. 경계 상자 생성에 대한 자세한 내용은 부록 TokenCut Seg. Agg에서 확인할 수 있습니다. Ours NCut (ablation) Ours Transformer CO3D BlendedMVS DTU AP@0.5 AP@0.7 AP@0.5 AP@0.7 AP@0.5 AP@0.0.816 0.204 0.875 0.0.867 0.306 1.00 0.0.908 0.581 1.00 1.0.500 0.1.00 0.0.833 0.CO3D 표 1. 3D 돌출 객체 감지의 정량적 결과. 우리의 방법은 다른 임계값을 가진 3D 바운딩 박스 IoU의 평균 정밀도(AP)를 사용하여 베이스라인과 비교됩니다. 베이스라인. 우리가 아는 한, 모델 학습을 위한 수동 주석 없이 SfM 포인트 클라우드에서 3D 돌출 객체를 감지하는 우리의 거친 분해 파이프라인과 동일한 설정을 유지하는 기존 베이스라인은 없습니다. 따라서 우리는 우리 설계의 효과를 보여주기 위해 비교할 두 가지 간단한 파이프라인을 고안했습니다.첫 번째 기준선은 TokenCut + 분할 집계(TokenCut Seg. Agg.)입니다.먼저 각 이미지에서 2D 두드러진 객체 분할을 위해 TokenCut[41]을 사용한 다음 평균을 내어 다중 뷰 분할 신뢰도를 SfM 포인트 클라우드에 집계합니다.마지막으로, 두드러진 객체의 3D 영역을 결정하기 위해 임계값 0.5로 SfM 포인트 클라우드를 분할합니다.또 다른 기준선은 신경 포인트 클라우드 + NCut 기반 분할(Ours NCut)로, Ours Transformer를 훈련하기 위한 가상 GT를 생성하는 데 사용됩니다.평가 지표.경계 상자 정확도를 평가하기 위해 임계값 0.5와 0.0을 사용하는 Intersection-overUnion(IoU) 지표를 사용합니다.비교를 위해 평균 정밀도(AP)를 사용합니다.결과. 표 1에 나와 있듯이, 우리의 접근 방식은 모든 데이터 세트에서, 특히 까다로운 CO3D[29] 데이터 세트에서 훨씬 더 나은 두드러진 객체 감지 성능을 달성합니다. TokenCut + Seg. Agg.에서처럼 2D 이미지를 개별적으로 분할하는 대신, 다중 뷰 2D 피처를 집계하고 3D에서 분할을 수행하는 우리의 전략은 두드러진 객체에 대한 전역적 추론을 용이하게 하고 다중 뷰 불일치를 제거합니다. 제안된 Ours Transformer는 Ours NCut에서 생성한 가상 GTT에서 학습되었지만 대부분의 데이터 세트와 메트릭에서 Ours NCut 기준선보다 성능이 뛰어납니다. 이러한 개선은 Our Transformer가 더 높은 밀도의 포인트 클라우드를 입력으로 허용하는 기능, 전역 종속성을 포착하는 기능, Ours NCut에서 생성한 데이터 세트에 대한 추가 학습에 기인합니다. 5.3. 객체 재구성 및 2D 분할 우리는 재구성된 객체 지오메트리와 2D 전경 마스크 렌더링을 평가하여 복잡한 객체를 재구성하고 분할하는 우리 접근 방식의 역량을 보여줍니다. 기준선. 3D 재구성을 위해, 우리는 우리 방법을 신경 표면 재구성 베이스라인 NeuS [40]와 비교한다. 2D 분할의 평가와 관련하여, 제안된 방법은 두 가지 범주에서 다음 베이스라인과 비교된다: 1) 단일 뷰 이미지 분할 베이스라인 TokenBlendedMVS NeuS Ours 정규화 없이 Ours 그림 6. 두드러진 객체 재구성의 정성적 결과. 우리 방법은 CO3D 및 BlendedMVS 데이터 세트에서 NeuS와 비교된다. 우리는 효과를 보여주기 위해 명시적 정규화가 있는 경우와 없는 경우의 결과를 제시합니다.평균 스캔 ID NeuS(주석이 달린 fg. 영역 포함) 0.390 0.216 0.245 0.223 0.345 0.271 0.우리의 결과(완전 자동화) 0.411 0.200 0.240 0.218 0.379 0.264 0.표 2. BlendedMVS 데이터 세트의 정량적 결과.GT 메시를 정규화하여 가장 긴 변이 1이 되도록 합니다.Chamfer 12 거리의 결과는 백분율로 표시합니다.각 이미지에서 2D 두드러진 객체 분할을 수행하고 다중 뷰 정보를 고려하지 않는 Cut[41].2) 노이즈가 있는 마스크와 신경 필드를 융합하고 신경 렌더링으로 고품질 마스크를 생성하는 다중 뷰 이미지 분할 기준선 SemanticNeRF[51]. 구체적으로, 우리는 TokenCut의 분할을 SemanticNeRF의 입력으로 사용하고 마스크 렌더링을 평가합니다.평가 지표.우리는 Chamfer 12 거리에서 3D 재구성을 평가합니다.Mask IoU와 Boundary IoU[4] 지표는 2D 분할을 평가하는 데 사용되며, 전자는 전반적인 분할 품질을 반영하고 후자는 경계 품질에 초점을 맞춥니다.이러한 지표의 정의는 부록에서 찾을 수 있습니다.결과.전경 객체 재구성의 경우 정성적 결과는 그림 5와 6에 표시되고 BlendedMVS 데이터 세트의 정량적 결과는 표 2에 나와 있습니다.제안된 완전 자동화된 파이프라인은 수동으로 주석이 달린 객체 영역이 제공되고 배경 제거를 위해 수동 후처리가 필요한 NeuS와 비교하여 동등하거나 더 나은 재구성 품질을 달성합니다.우리의 파이프라인은 이러한 지루한 노동을 제거하고 따라서 대규모 데이터 세트를 자동으로 생성할 수 있는 잠재력을 보여줍니다. 영어: 또한 우리의 방법은 TableScan IDCO3D5 Mean BlendedMVSMask IoU 5 MeanDTU5 Mean Ours TokenCut(단일 보기) TokenCut + SemanticNeRF Ours 0.933 0.951 0.958 0.962 0.934 0.947 0.959 0.987 0.916 0.936 0.977 0.955 0.931 0.969 0.961 0.959 0.903 0.0.784 0.888 0.976 0.975 0.966 0.0.825 0.861 0.952 0.980 0.914에 표시된 대로 대부분의 평가된 스캔에서 더 나은 2D 분할 정확도를 달성합니다. 0.0.785 0.904 0.919 0.855 0.943 0.0.972 0.906 0.924 0.877 0.941 0.경계 IoU 0.816 0.914 0.767 0.896 0.817 0.0.493 0.562 0.664 0.688 0.695 0.0.829 0.921 0.905 0.955 0.971 0.0.828 0.921 0.907 0.957 0.975 0.0.628 0.842 0.752 0.707 0.613 0.0.572 0.693 0.525 0.636 0.803 0.0.912 0.937 0.839 0.771 0.843 0.TokenCut(단일 보기) 0.635 0.832 0.877 0.839 0.887 0.TokenCut + SemanticNeRF 0.701 0.819 0.847 0.822 0.769 0.792 0.512 0.578 0.699 0.730 0.642 0.632 0.539 0.633 0.522 0.661 0.836 0.표 3. 2D 분할의 정량적 결과. 우리는 CO3D, BlendedMVS, DTU를 포함한 여러 데이터 세트에서 Mask IoU 및 Boundary IoU 메트릭의 기준선과 전경 마스크 렌더링을 비교합니다.AutoRecon은 대부분 스캔에서 기준선보다 성능이 우수합니다.CO3D BMVS 참조.이미지 TokenCut TokenCut + Semantic NeRF 우리의 그림 7. 2D 분할의 정성적 결과.우리는 CO3D의 까다로운 의자 범주와 복잡한 지오메트리를 가진 BlendedMVS의 객체에 대한 전경 분할을 보여줍니다.그림 7에 시각화되어 있습니다.2D 두드러진 객체 분할 기준선 TokenCut의 결과는 다중 뷰 일관성이 부족하고 복잡한 배경이 있는 스캔에서는 노이즈가 많습니다.SemanticNeRF는 일부 스캔에서 초기 TokenCut 분할을 개선할 수 있습니다.제안된 방법은 복잡한 객체와 배경을 처리할 수 있으며 Boundary IoU 메트릭에서 이러한 기준선보다 상당히 성능이 뛰어나 고품질 분할을 생성할 수 있음을 보여줍니다.5.4. 절제 연구 우리는 분해된 신경 장면 표현을 훈련하기 위한 거친 분해 및 정규화 용어에 대한 포인트 클라우드 분할 변환기의 효과를 검증하기 위한 실험을 수행합니다. 보충 자료에서 더 많은 절제 연구를 제공합니다. 거친 분해를 위한 분할 변환기. 우리는 표 1에 표시된 것처럼 여러 데이터 세트에서 더 높은 3D 감지 AP에서 NCut 기반 파이프라인에 대한 3D 분할 변환기의 효과를 보여줍니다. 결과에 따르면 가상 레이블로 훈련되었지만 3D 감지 정확도가 상당히 향상되었으며 특히 CO3D 데이터 세트에서 향상되었습니다. 게다가 Ours Transformer는 Ours NCut보다 약 100배 더 빠르게 실행되어 10k 포인트가 있는 포인트 클라우드를 분할하며 대규모 포인트 클라우드에 적용할 수 있습니다. 분해된 신경 장면 표현을 훈련하기 위한 명시적 정규화. 그림의 정성적 결과는 전경 객체를 주변에서 분리하는 데 명시적 정규화의 효과를 보여줍니다. 거친 분해가 제공하는 정규화는 의자 예에서 표시된 것처럼 모양-광도 모호성을 완화합니다. 6.
--- CONCLUSION ---
우리는 인간의 주석 없이 다중 뷰 이미지에서 완전 자동화된 객체 발견 및 재구성을 위한 새로운 파이프라인을 제시합니다. 여러 실제 데이터 세트에서 수행한 실험은 고품질의 배경 없는 객체 모델을 구축하는 데 있어서 우리 방법의 효과를 보여줍니다. 또한 2D 지도 학습에 직접 적용할 수 있는 고품질 분할 마스크를 생성하는 파이프라인의 기능을 보여줍니다. 한계 및 향후 작업. 신경 재구성 방법에서 직면하는 문제는 그림자와 일시적인 폐색자에 대한 민감도 및 얇은 구조 및 비 램버시안 객체에 대한 저하된 결과와 같이 파이프라인에 남아 있습니다. 다중 뷰 ViT 기능을 저장하는 것은 메모리 집약적이므로 거리 보존 압축 기술을 통해 완화될 것으로 예상합니다. SfM 포인트 클라우드의 재구성 품질은 [11, 17]과 같은 정제 방법을 통해 더욱 개선될 수 있으며, 이는 표면 재구성의 품질을 더욱 개선하고 재구성 모호성을 잠재적으로 제거할 수 있습니다. 자동화된 객체 재구성 파이프라인은 2D 분할 네트워크 및 3D 생성 모델 학습과 같은 그래픽 및 인식 작업을 위한 대규모 3D 객체 데이터 세트를 만드는 데 사용할 수 있습니다.감사의 말.이 연구는 NSFC(No. 62172364), ZJU-SenseTime Joint Lab of 3D Vision, 정보기술센터 및 CAD&amp;CG 국가중점연구소, 저장대학교의 지원을 받았습니다.참고문헌 [1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, Peter Hedman.Mip-nerf 360: Unbounded anti-aliased neural radiance fields.CVPR, 2022. 5,[2] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, Alexander Lerchner.Monet: 비지도 장면 분해 및 표현. arXiv:1901.11390, 2019.[3] 마틸드 카론, 휴고 투브롱, 이샨 미스라, 에르베 제구, 줄리앙 마이랄, 표트르 보자노프스키, 아르망 줄랭. 자체 감독 비전 변환기의 새로운 속성. CVPR, 2021. 1, 2,[4] Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg 및 Alexander Kirillov. 경계 iou: 객체 중심 이미지 분할 평가 개선. CVPR에서는 2021.[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit 및 Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다: 규모에 따른 이미지 인식을 위한 변환기. ICLR에서, 2021.[6] Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner. GENESIS: 객체 중심 잠재 표현을 사용한 생성적 장면 추론 및 샘플링. ICLR에서, 2020.[7] Qiancheng Fu, Qingshan Xu, Yew-Soon Ong, Wenbing Tao. Geo-neus: 다중 뷰 재구성을 위한 기하학 일관성 신경 암묵적 표면 학습. NeurIPS에서, 2022.[8] Yasutaka Furukawa, Brian Curless, Steven M. Seitz, Richard Szeliski. 인터넷 규모의 다중 뷰 스테레오를 향하여. CVPR에서, 2010.[9] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner. 반복적 변분 추론을 통한 다중 객체 표현 학습. ICML, 2019.[10] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman. 모양 학습을 위한 암묵적 기하학적 정규화. ICML, 2020.[11] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, Xiaowei Zhou. Onepose++: CAD 모델 없이 키포인트 없는 원샷 객체 포즈 추정. NeurIPS, 2022. 4,[12] Rasmus Ramsbøl Jensen, Anders Lindbjerg Dahl, George Vogiatzis, Engil Tola, Henrik Aanæs. 대규모 다중 시점 입체시 평가. CVPR, 2014. 2,[13] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret. Transformers are rnns: 선형 주의가 있는 고속 자기 회귀 변환기. ICML, 2020.[14] Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann. 특징 필드 증류를 통한 편집을 위한 nerf 분해. NeurIPS, 2022.[15] Amit Pal Singh Kohli, Vincent Sitzmann, Gordon Wetzstein. 반지도 학습을 통한 의미적 암묵적 신경 장면 표현. 3DV, 2020.[16] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi, Frank Dellaert, Thomas Funkhouser. 파노라마 신경 필드: 의미적 객체 인식 신경 장면 표현. CVPR, 2022.[17] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, Marc Pollefeys. 특징적 세분화를 통한 픽셀 완벽한 구조-동작. ICCV, 2021.[18] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf. 슬롯 어텐션을 통한 객체 중심 학습. NeurIPS, 2020.[19] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh. 신경 볼륨: 이미지에서 동적 렌더링 가능 볼륨 학습. ACM TOG, 2019.[20] William E. Lorensen, Harvey E. Cline. Marching cubes: 고해상도 3D 표면 구성 알고리즘. SIGGRAPH Comput. Graph., 1987.[21] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi. 심층 스펙트럼 방법: 비지도 의미 분할 및 지역화를 위한 놀라울 정도로 강력한 기준선. CVPR, 2022. 1,[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도장으로 장면 표현. ACM 커뮤니케이션, 2021.[23] Ashkan Mirzaei, Yash Kant, Jonathan Kelly, Igor Gilitschenski. Laterf: 레이블 및 텍스트 기반 객체 광도장. ECCV에서, 2022. 1,[24] Norman Müller, Andrea Simonelli, Lorenzo Porzi, Samuel Rota Bulò, Matthias Nießner, Peter Kontschieder. Autorf: 단일 뷰 관찰에서 3D 객체 광도장 학습. CVPR에서, 2022.[25] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM ToG, 2022.[26] Michael Niemeyer, Lars M. Mescheder, Michael Oechsle, Andreas Geiger. 미분 가능한 체적 렌더링: 3차원 감독 없이 암묵적 3차원 표현 학습. CVPR, 2020.[27] Michael Oechsle, Songyou Peng, Andreas Geiger. Unisurf: 다중 뷰 재구성을 위한 신경 암묵적 표면과 광도장 통합. ICCV, 2021.[28] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide. 동적 장면을 위한 신경 장면 그래프. CVPR, 2021.[29] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny. 3차원의 공통 객체: 실제 3차원 범주 재구성의 대규모 학습 및 평가. ICCV, 2021. 2, 6,[30] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G Schwing, Oliver Wang. 신경 체적 객체 선택. CVPR, 2022. 1,[31] Mehdi SM Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Pavetic, Mario Lucic, Leonidas Guibas, Klaus Greff, Thomas Kipf. 객체 장면 표현 변환기. NeurIPS, 2022.[32] Johannes L. Schönberger, Enliang Zheng, Jan-Michael Frahm, Marc Pollefeys. 비정형 다중 뷰 스테레오를 위한 픽셀별 뷰 선택. ECCV. 2016. 1,[33] Jianbo Shi와 Jitendra Malik. 정규화된 컷과 이미지 분할. IEEE TPAMI, 2000. 2, 3,[34] Oriane Siméoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce. 자체 감독 변환기와 레이블 없이 객체 현지화. BMVC에서, 2021. 1,[35] Cameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B. Tenenbaum, Jiajun Wu, Vincent Sitzmann. 객체 빛 필드의 비지도 발견 및 구성. arXiv:2205.03923, 2022.[36] Karl Stelzner, Kristian Kersting, Adam R Kosiorek. 비지도 볼륨 분할을 통해 3D 장면을 객체로 분해. arXiv:2104.01148, 2021.[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, Xiaowei Zhou. Loftr: 변압기를 사용한 검출기 없는 로컬 피처 매칭. CVPR에서, 2021.[38] Vadim Tschernezki, Iro Laina, Diane Larlus, Andrea Vedaldi. 신경 피처 융합 필드: 자기 감독 2D 이미지 표현의 3D 증류. 3DV에서, 2022.[39] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, Daniel Duckworth. NeSF: 3D 장면의 일반화 가능한 의미 분할을 위한 신경 의미 필드. arXiv:2111.13260, 2021.[40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암시적 표면 학습. NeurIPS에서, 2021. 1, 2, 3, 5,[41] Yangtao Wang, Xi Shen, Shell Hu, Yuan Yuan, James Crowley, Dominique Vaufreydaz. 정규화된 컷을 사용한 비지도 객체 발견을 위한 자체 감독 변환기. CVPR에서, 2022. 1, 3,[42] Qianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, Jianmin Zheng. 객체 구성 신경 암시적 표면. ECCV에서, 2022.[43] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang 및 Zhaopeng Cui. 편집 가능한 장면 렌더링을 위한 객체 구성 신경 복사 필드 학습. ICCV, 2021.[44] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang 및 Long Quan. Mvsnet: 구조화되지 않은 다중 뷰 스테레오에 대한 깊이 추론. ECCV에서, 2018.[45] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang 및 Long Quan. Blendedmvs: 일반화된 다중 뷰 스테레오 네트워크를 위한 대규모 데이터세트입니다. CVPR, 2020. 2,[46] Lior Yariv, Jiatao Gu, Yoni Kasten 및 Yaron Lipman. 신경 암시적 표면의 볼륨 렌더링. NeurIPS, 2021. 1,[47] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman. 기하학과 모양을 풀어서 다중 뷰 신경 표면 재구성. NeuIPS, 2020. 2,[48] Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu. 객체 광도장의 비지도 발견. ICLR, 2022.[49] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, Thomas Brox. 자기 지도 객체 중심 표현을 사용한 비지도 의미 분할. ICLR, 2023.[50] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. NeRF++: 신경 복사장 분석 및 개선. arXiv:2010.07492, 2020. 1, 2,[51] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J Davison. 암묵적 장면 표현을 통한 현장 장면 레이블링 및 이해. ICCV에서, 2021. 2,[52] Shuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton, Tristan Laidlow, Andrew J Davison. ilabel: 신경장에서 객체 표시. IEEE Robot. Autom. Lett., 2022.
