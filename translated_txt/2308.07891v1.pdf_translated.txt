--- ABSTRACT ---
새로운 개념으로 맥락에서 학습하고 적절한 응답을 제공하는 능력은 인간 대화에서 필수적입니다. 현재 멀티모달 대규모 언어 모델(MLLM)과 대규모 언어 모델(LLM)이 대규모 데이터 세트에서 훈련되고 있음에도 불구하고, 훈련 없이 보이지 않는 이미지를 인식하거나 새로운 개념을 이해하는 것은 여전히 어려운 일입니다. 문맥 내 학습(ICL)은 모델이 제한된 작업에서 &quot;학습하는 법을 배우도록&quot; 장려하고 보이지 않는 작업으로 일반화하는 훈련 없는 소수 샷 학습을 탐구합니다. 이 연구에서 우리는 MLLM의 학습 능력을 증강하기 위해 &quot;원인과 결과로부터 추론&quot;을 강조하는 링크 문맥 학습(LCL)을 제안합니다. LCL은 지원 세트와 쿼리 세트 간의 인과 관계를 명시적으로 강화함으로써 기존 ICL을 넘어섭니다. 인과 관계를 사용한 데모를 제공함으로써 LCL은 모델이 데이터 포인트 간의 유추뿐만 아니라 근본적인 인과 관계도 식별하도록 안내하여 MLLM이 보이지 않는 이미지를 인식하고 새로운 개념을 보다 효과적으로 이해할 수 있도록 합니다. 이 새로운 접근 방식의 평가를 용이하게 하기 위해 *동등한 기술 기여. *프로젝트 책임자. 링크 문맥 학습을 위해 설계된 보이지 않는 생성된 이미지-레이블 쌍으로만 구성된 ISEKAI 데이터 세트를 소개합니다. 광범위한 실험을 통해 LCL-MLLM이 바닐라 MLLM보다 새로운 개념에 대한 강력한 링크 문맥 학습 능력을 보인다는 것이 밝혀졌습니다. 코드와 데이터는 https://github.com/isekai-portal/Link-Context-Learning에서 공개됩니다. 1
--- INTRODUCTION ---
(가까운 미래에 인류는 마침내 항성간 여행을 하여 센타우로스 별자리에 도착할 수 있게 됩니다.) 인간과 MLLM이 우주선에서 내립니다. 인간: &quot;우리가 해냈어! 봐! 지역 주민들이 왔어.&quot; 지역 주민: 안녕하세요, 우리를 &#39;RockFlock&#39;이라고 부르세요. MLLM: &quot;안녕, 양!&quot; 인간: 인간과 MLLM 간의 위의 대화는 MLLM이 실제로 대화하는 동안 시범을 통해 배우려고 애쓰는 모습을 유머러스하게 표현한 것입니다. &#39;RockFlock&#39;은 우리가 직접 만든 종으로, 시범 추론 맥락 내 학습 Q: 이게 뭐야? Q: 이게 뭐야? A: 고양이. A: 판달 링크 맥락 학습 Q: 이게 뭐야? A: 오카피. Q: 이게 뭐야? A: 판다, 오카피가 아니야. 무관 관련 Q: 이게 뭐야? A: 호랑이. Q: 이게 뭐예요?A: 오카피 시범 추론 그림 2: 링크-컨텍스트 학습과 컨텍스트 내 학습의 차이점. 컨텍스트 내 학습은 시범을 위해 무관한 작업을 제공하는 반면, 링크-컨텍스트 학습의 시범 단계와 추론 단계 사이에는 직접적인 인과 관계가 있습니다. 그림 1에서 보듯이 사람과 같은 몸과 양과 같은 머리. 현재의 MLLM은 보이지 않는 이미지-레이블 쌍을 연결하여 단일 대화에서 새로운 객체를 인식하지 못합니다. 이러한 한계를 해결하기 위해 모델에 소수 샷 학습 능력을 부여하는 것은 MLLM 시대 이전에도 컴퓨터 비전 분야에서 오랜 주제였습니다. 이 접근 방식을 통해 모델은 제한된 예제에서 학습하고 문제를 효과적으로 완화할 수 있습니다. MLLM이 시범에서 학습하는 주요 방법은 컨텍스트 내 학습으로 알려져 있으며, 여기서 모델은 몇 가지 입력-레이블 쌍에 노출된 후 다운스트림 작업에서 현저한 개선을 보입니다. 그러나 현재의 MLLM은 주로 메타 작업에서 &quot;학습&quot;한 후 새로운 작업을 처리하는 능력을 습득하도록 모델을 안내하는 데 중점을 두고 있기 때문에 컨텍스트 내 학습에서 매우 제한적인 이점을 얻습니다. 그러나 메타 작업에서 제공된 답변이 모두 틀렸더라도 모델의 성능에는 영향을 미치지 않습니다.[1] 따라서 MLLM이 데모에서 &quot;학습&quot;한 것은 이미지-레이블 쌍 간의 인과 관계를 이해하는 것보다 특정 형식으로 질문에 답하는 데 있습니다. MLLM이 이미지와 레이블 쌍 간의 인과 관계에 더 집중할 수 있도록 Frozen 방법[2]은 알려진 이미지에 다른 레이블을 바인딩합니다. 그러나 MLLM이 이미지와 레이블이 모두 보이지 않는 완전히 새로운 시나리오에 직면하면 상당한 과제가 발생합니다. 그러한 경우 데모에서 근본 원인과 결과를 추출하고 이 새롭게 발견된 지식을 기반으로 정확한 예측을 하는 작업은 미해결 퍼즐로 남아 있습니다. 그림 1에 표시된 &#39;RockFlock&#39;(보이지 않는 이미지와 새로운 개념)은 이전 방법에서는 잘못 인식되지만, 우리 모델은 시범을 통해 &#39;RockFlock&#39; 개념을 학습하고 정확하게 응답합니다. 더욱이 새로운 개념의 습득은 기존 지식을 방해하지 않아 모델이 원래 이미지와 새로 학습한 이미지를 효과적으로 구별할 수 있습니다. 맥락 내 학습(이하 ICL이라고 함)에서 영감을 얻어 MLLM이 대화에서 새로운 개념에 대한 지식을 습득하고 정확한 질의응답을 위해 기존 지식을 유지해야 하는 링크 맥락 학습(이하 LCL이라고 함)을 제안합니다. 그림 2에서 볼 수 있듯이, MLLM의 현재 맥락 내 학습은 인과 무관한 시범의 이점을 강조합니다. 영어: HowAccuracy 1.0.0.0.0.0.Our Method (7B) OpenFlamingo (98) Otter (98) AVE wingeda stripehopper hornedephantthornbrawlersaquaroverbrawnger mushroomhaven shellfortresslavasnakepineapplepalacerockflockgildedfloatshipcactihog airstoneblazefrostskyquill그림 3: ISEKAI 데이터 세트의 여러 범주에 대한 결과 개요: 저희 모델은 거의 모든 범주에서 OpenFlamingo(9B) [6]와 Otter(9B) [5]보다 우수한 성능을 보이며, 전혀 보이지 않는 이미지가 포함된 시나리오에서 뛰어난 성능을 보여줍니다. 그러나 링크 컨텍스트 학습의 경우 데모와 최종 작업은 인과적으로 연결됩니다. (예: 데모에서 &#39;사과&#39;의 이름을 &#39;오렌지&#39;로 바꾸면 모델은 추론 중에 사과를 &#39;오렌지&#39;라고 불러야 합니다.) 이러한 기능을 통해 MLLM은 유연한 방식으로 few-shot 학습을 지원할 수 있습니다. 대규모 언어 모델 시대에, 이러한 모델이 방대한 양의 실제 데이터에 대해 광범위하게 학습되기 때문에 few-shot 학습에서 모델의 성능을 평가하는 것은 과제가 됩니다. 이 문제를 해결하고 링크 컨텍스트 학습에 대한 포괄적인 평가를 제공하기 위해 ISEKAI 데이터 세트를 소개합니다. 이 데이터 세트는 MLLM에게 전혀 새로운 보이지 않는 이미지와 개념으로 구성되어 있으며, 사실성의 경계를 넘어섭니다. 데이터 세트의 모든 이미지는 Stable Diffusion[3]과 Midjourney[4]에서 생성되었으며, 모든 레이블이나 개념도 만들어졌습니다. 그림 3은 ISEKAI 데이터 세트에서 우리 모델과 Otter[5], OpenFlamingo[6]를 비교한 것입니다. 이 논문에서는 MLLM이 대화에서 잠재적인 인과 관계를 이해하고 보이지 않는 이미지와 개념을 처리할 수 있는 기능을 부여하는 설정인 링크 컨텍스트 학습(LCL)을 제시합니다. ICL이 주로 다양한 과제를 가진 영감을 주는 모델에 초점을 맞추는 것과 달리, LCL은 모델이 소스와 타겟 간의 매핑을 설정하도록 권한을 부여하여 전반적인 성능을 향상시킴으로써 한 걸음 더 나아갑니다. 이 작업의 기여는 다음과 같이 요약할 수 있습니다. • • 링크-컨텍스트 학습: MLLM이 진행 중인 대화에서 새로운 개념을 동화하고 정확한 질의응답을 위해 이 지식을 유지하도록 도전하는 새로운 인과 관련 소수 샷 학습 설정을 소개합니다. 링크-컨텍스트 학습에서 MLLM이 데모에서 소스와 타겟 간의 인과 관계를 파악할 수 있도록 권한을 부여합니다. ISEKAI 데이터 세트: 대부분의 실제 데이터는 MLLM에게 완전히 알려지지 않았기 때문에, MLLM의 성능을 평가하기 위해 새로운 이미지-개념 쌍이 도입된 도전적인 인위적 데이터 세트를 대중에게 공개합니다. 2
--- RELATED WORK ---
s 멀티모달 대규모 언어 모델[7-11]은 보편적 생성 또는 인식 작업에서 상당한 역량을 입증했습니다. MLLMS의 새로운 패러다임에 따라 다양한 시각적 작업을 훈련이 필요 없는 제로샷 방식[12, 13]으로 달성할 수 있으며, 이는 무거운 사전 훈련 및 미세 조정 프로세스에서 벗어납니다. 그러나 단일 모델을 통해 임의의 콘텐츠를 인식하는 것은 일반적으로 매우 어려운 것으로 간주됩니다. 저렴한 비용으로 야외에서 MLLM의 인식 역량을 향상시키는 방법이 최근 연구 초점으로 등장했습니다. 멀티모달 프롬프트 튜닝 멀티모달 프롬프트 튜닝(M-PT)은 CLIP[12]과 같은 대조 학습 기반 멀티모달 대규모 모델에서 일반적으로 사용됩니다. 훈련 프로세스에서 프롬프트 튜닝은 일반적으로 모델의 대부분 매개변수를 동결하고 소수의 매개변수만 업데이트하여 미세 조정과 유사한 결과를 얻습니다[14-17]. PT[14]는 인코더와 디코더의 각 계층에 조정 가능한 프롬프트 임베딩을 추가하고, 추가된 임베딩의 가중치만 학습 중에 업데이트됩니다. VPT[18]는 모델을 조정하기 위해 특정 위치에 학습 가능한 매개변수 세트를 추가했습니다. CoOp[15]와 UPT[19]는 CLIP을 백본으로 사용하여 few-shot 설정에 맞게 프롬프트했습니다. CoCoOp[16], POMP[20] 및 MaPLe[21]는 프롬프트 조정을 오픈 어휘 시각 인식 작업으로 확장합니다. 그러나 기존의 프롬프트 조정
--- METHOD ---
MLLM이 데모에서 학습하는 것을 컨텍스트 내 학습이라고 하며, 여기서 모델은 몇 가지 입력-레이블 쌍에 노출된 후 다운스트림 작업에서 현저한 개선을 보입니다. 그러나 현재 MLLM은 메타 작업에서 &quot;학습&quot;한 후 새로운 작업을 처리하는 능력을 습득하도록 모델을 안내하는 데 주로 중점을 두고 있기 때문에 컨텍스트 내 학습에서 매우 제한적인 이점을 얻습니다. 그러나 메타 작업에서 제공된 답변이 모두 틀렸더라도 모델의 성능에는 영향을 미치지 않습니다. [1] 따라서 MLLM이 데모에서 &quot;학습&quot;한 것은 이미지-레이블 쌍 간의 인과 관계를 이해하는 것보다 특정 형식으로 질문에 답하는 데 있습니다. MLLM이 이미지와 레이블 쌍 간의 인과 관계에 더 집중할 수 있도록 Frozen 방법[2]은 알려진 이미지에 다른 레이블을 바인딩합니다. 그러나 MLLM이 이미지와 레이블이 모두 보이지 않는 완전히 새로운 시나리오에 직면하면 상당한 문제가 발생합니다. 이러한 경우, 시범에서 근본 원인과 결과를 추출하고 이 새롭게 발견한 지식을 기반으로 정확한 예측을 하는 작업은 여전히 풀리지 않은 퍼즐로 남아 있습니다.그림 1에 표시된 &#39;RockFlock&#39;(보이지 않는 이미지와 새로운 개념)은 이전 방법에서는 잘못 인식되지만, 우리 모델은 시범에서 &#39;RockFlock&#39; 개념을 학습하고 정확하게 응답합니다.또한 새로운 개념의 습득은 기존 지식을 방해하지 않으므로 모델은 원래 이미지와 새로 학습한 이미지를 효과적으로 구별할 수 있습니다.상황 내 학습(이하 ICL이라고 함)에서 영감을 얻어 MLLM이 대화에서 새로운 개념에 대한 지식을 습득하고 정확한 질의응답을 위해 기존 지식을 유지해야 하는 링크-상황 학습(이하 LCL이라고 함)을 제안합니다.그림 2에서 볼 수 있듯이, MLLM의 현재 상황 내 학습은 인과 무관한 시범의 이점을 강조합니다. 영어: HowAccuracy 1.0.0.0.0.0.Our Method (7B) OpenFlamingo (98) Otter (98) AVE wingeda stripehopper hornedephantthornbrawlersaquaroverbrawnger mushroomhaven shellfortresslavasnakepineapplepalacerockflockgildedfloatshipcactihog airstoneblazefrostskyquill그림 3: ISEKAI 데이터 세트의 여러 범주에 대한 결과 개요: 저희 모델은 거의 모든 범주에서 OpenFlamingo(9B) [6]와 Otter(9B) [5]보다 우수한 성능을 보이며, 전혀 보이지 않는 이미지가 포함된 시나리오에서 뛰어난 성능을 보여줍니다. 그러나 링크 컨텍스트 학습의 경우 데모와 최종 작업은 인과적으로 연결됩니다. (예: 데모에서 &#39;사과&#39;의 이름을 &#39;오렌지&#39;로 바꾸면 모델은 추론 중에 사과를 &#39;오렌지&#39;라고 불러야 합니다.) 이러한 기능을 통해 MLLM은 유연한 방식으로 few-shot 학습을 지원할 수 있습니다. 대규모 언어 모델 시대에, 이러한 모델이 방대한 양의 실제 데이터에 대해 광범위하게 학습되기 때문에 few-shot 학습에서 모델의 성능을 평가하는 것은 과제가 됩니다. 이 문제를 해결하고 링크 컨텍스트 학습에 대한 포괄적인 평가를 제공하기 위해 ISEKAI 데이터 세트를 소개합니다. 이 데이터 세트는 MLLM에게 전혀 새로운 보이지 않는 이미지와 개념으로 구성되어 있으며, 사실성의 경계를 넘어섭니다. 데이터 세트의 모든 이미지는 Stable Diffusion[3]과 Midjourney[4]에서 생성되었으며, 모든 레이블이나 개념도 만들어졌습니다. 그림 3은 ISEKAI 데이터 세트에서 우리 모델과 Otter[5], OpenFlamingo[6]를 비교한 것입니다. 이 논문에서는 MLLM이 대화에서 잠재적인 인과 관계를 이해하고 보이지 않는 이미지와 개념을 처리할 수 있는 기능을 부여하는 설정인 링크 컨텍스트 학습(LCL)을 제시합니다. ICL이 주로 다양한 작업을 통해 영감을 주는 모델에 초점을 맞추는 것과 달리 LCL은 한 걸음 더 나아가 모델이 소스와 타겟 간의 매핑을 설정하여 전반적인 성능을 향상시키도록 지원합니다.이 작업의 기여는 다음과 같이 요약할 수 있습니다.• • 링크 컨텍스트 학습: MLLM이 진행 중인 대화에서 새로운 개념을 동화하고 정확한 질의 응답을 위해 이 지식을 유지하도록 도전하는 새로운 인과 관련 소수 샷 학습 설정을 소개합니다.링크 컨텍스트 학습에서 MLLM이 데모에서 소스와 타겟 간의 인과 관계를 파악할 수 있도록 지원합니다.ISEKAI 데이터 세트: 대부분의 실제 데이터는 MLLM에게 전혀 알려지지 않았으므로 MLLM의 성능을 평가하기 위해 새로운 이미지-개념 쌍이 도입된 까다로운 인위적 데이터 세트를 대중에게 공개합니다.2 관련 연구 다중 모달 대규모 언어 모델[7-11]은 보편적인 생성 또는 인식 작업에서 상당한 역량을 보여주었습니다. MLLMS의 새로운 패러다임에 따라 다양한 시각적 작업을 훈련이 필요 없는 제로샷 방식으로 달성할 수 있으며[12, 13], 무거운 사전 훈련 및 미세 조정 프로세스에서 벗어날 수 있습니다. 그러나 단일 모델을 통해 임의의 콘텐츠를 인식하는 것은 일반적으로 매우 어려운 것으로 간주됩니다. 저렴한 비용으로 야외에서 MLLM의 인식 기능을 향상시키는 방법이 최근 연구 초점으로 등장했습니다. 다중 모달 프롬프트 튜닝 다중 모달 프롬프트 튜닝(M-PT)은 CLIP[12]과 같은 대조 학습 기반 다중 모달 대규모 모델에서 일반적으로 사용됩니다. 훈련 프로세스에서 프롬프트 튜닝은 일반적으로 대부분의 모델 매개변수를 동결하고 미세 조정과 유사한 결과를 얻기 위해 소수의 매개변수만 업데이트합니다[14-17]. PT[14]는 인코더 및 디코더의 각 계층에 튜닝 가능한 프롬프트 임베딩을 추가하고 추가된 임베딩의 가중치만 훈련 중에 업데이트됩니다. VPT[18]는 모델을 튜닝하기 위해 특정 위치에 학습 가능한 매개변수 세트를 추가했습니다. CoOp[15]와 UPT[19]는 CLIP을 백본으로 사용하여 few-shot 설정에 맞게 프롬프트했습니다.CoCoOp[16], POMP[20] 및 MaPLe[21]는 프롬프트 튜닝을 오픈 어휘 시각적 인식 작업으로 확장합니다.그러나 기존의 프롬프트 튜닝 방법은 강력한 생성적 멀티모달 대규모 언어 모델에 적합하지 않습니다.멀티모달 명령어 튜닝 멀티모달 명령어 튜닝(M-IT)은 명령어 설명 기반 데이터 세트[7,8, 11, 22, 23]에서 미세 조정하여 보이지 않는 작업에서 MLLM의 제로샷 기능을 향상시킵니다.MiniGPT-4[24] 및 LLAVA[11]는 시각적 인코더를 고정하고 언어 모델을 조정하여 명령어 튜닝을 멀티모달로 확장합니다.mPLUG-Owl[25]은 시각적 및 텍스트 인코더를 두 단계로 별도로 조정하고 시각 관련 명령어 튜닝을 평가하기 위한 평가 데이터 세트를 제안했습니다. InstructBLIP[26]은 여러 데이터 세트에서 명령어 튜닝을 수행하여 제로샷 기능을 향상시킵니다.Shikra[27]와 Kosmos-2[28]는 경계 상자 좌표가 있는 명령어를 사용하여 MLLM을 시각적 접지 작업으로 확장했습니다.이러한 연구에서 뛰어난 제로샷 기능이 입증되었지만 모델 학습 프로세스 중에 보이지 않은 클래스는 여전히 인식할 수 없습니다.다중 모달 문맥 내 학습 대규모 언어 모델(LLM)은 문맥 샘플에서 학습하는 데 뛰어난 기능을 보여주었습니다.다중 모달 문맥 내 학습(M-ICL) 설정에서 MLLM은 입력 이미지 샘플과 선택적 명령어에 따라 몇 번의 샷 방식으로 새로운 작업 패턴을 학습할 수 있습니다[29-32].Flamingo[33]는 사전 학습 프로세스 중에 문맥 내 학습을 고려하여 모델이 문맥 내 학습을 지원하는 기능을 보유할 수 있도록 합니다.Otter[5]는 Flamingo를 따르고 명령어 튜닝 단계에서 ICL 기능을 진행하여 새로운 문맥 내 학습 데이터 세트를 제안했습니다. 이전 방법과 달리, 제안하는 링크 컨텍스트 학습은 지원과 쿼리 세트 간의 인과 관계를 확립할 수 있습니다. 구체적으로, 소수의 샷 클래스별 이미지와 텍스트 프롬프트를 사용하여 LCL은 프롬프트와 추론 샘플을 연결하고, 이전에 보지 못한 이미지를 새로운 개념과 연관시킬 수도 있습니다. 3 링크 컨텍스트 학습 이 섹션에서는 먼저 컨텍스트 내 학습에 대한 간략한 소개를 제공하고, 예비에서 링크 컨텍스트 학습과의 주요 제한 사항과 차이점을 밝힙니다. 다음으로, 링크 컨텍스트 학습을 MLLM에 가져오기에서 링크 컨텍스트 학습의 힘을 MLLM에 적용합니다. 3.1 예비 = 문맥 내 학습 공식적으로 문맥 내 학습[34]은 다음을 의미합니다. 모델은 쿼리 입력 x가 주어졌을 때, 다양한 작업에서 가져온 여러 입력 라벨 쌍으로 구성된 지원 세트 S를 조건으로, 후보 답변 세트 Y{Y1, Y2,, Yn}에서 가장 높은 예측 점수를 가진 답변을 선택해야 합니다. 여기서 S{(x1, Y1), (x2, Y2), ..., (xn, Yn)}입니다. (쿼리와 S의 샘플은 다른 작업에 속해야 합니다.) 다른 관점에서, 문맥 내 학습은 훈련이 필요 없는 few-shot 학습으로 표시될 수 있습니다. few-shot 학습의 훈련 단계를 대규모 언어 모델에 대한 데모 입력으로 변환하기 때문입니다. ICL[34]은 FSL과 일관성이 있으며, 데모(훈련) 단계와 추론(쿼리) 단계의 작업이 다릅니다. = 링크-컨텍스트 학습 기본적으로 링크-컨텍스트 학습(LCL)은 학습이 필요 없는 인과 연결 소수 학습의 한 형태를 나타냅니다. 이 접근 방식에서 지원 세트 S(x1,y1), (x2,y2), ..., (xn, Yn)이 쿼리 세트 Q의 쿼리 샘플 x와 함께 제공되며, 여기서 지원 세트의 데이터 쌍은 쿼리 세트와 인과적으로 연결됩니다. 모델은 쿼리와 지원 세트 간의 인과 연결 관계를 기반으로 답을 예측하는 작업을 맡습니다. 더 명확하게 설명하기 위해 링크-컨텍스트 학습은 지원 세트와 쿼리 세트 간의 인과 관계를 크게 강화합니다. 예를 들어: 1). 새로운 산술 규칙: 이 시나리오에서 지원 세트는 (1<op> 2 = 3), (2<op> 3 = 5), 쿼리 샘플은 4입니다.<op> 5 =?. 여기서, &quot;<op> &quot;는 우리가 데모를 통해 모델에 가르치고자 하는 새로운 산술 규칙을 나타냅니다. 2). 새로운 이미지 분류: 이 경우 지원 세트에는 다음과 같은 쌍이 포함됩니다.<unseen image> :<novel cls A> ), (<unseen image><novel cls B> ), 쿼리 샘플은 (<unseen image> 영어: belonging to?). 이 예제는 모델이 데모를 기반으로 보이지 않는 이미지를 지정된 새로운 클래스 중 하나로 올바르게 분류하는 방법을 보여줍니다. 본질적으로 링크 컨텍스트 학습은 지원 세트와 쿼리 세트 간의 인과 관계를 효과적으로 설정하여 새로운 개념과 관계를 파악하는 모델의 능력을 향상시킵니다. 이 설정은 LLM과 MLLM 모두에 적용할 수 있지만 이 논문의 주요 초점은 특히 MLLM에서 링크 컨텍스트 학습을 적용하는 것입니다. MLLM에 집중함으로써 다중 모드 모델에서 이 접근 방식의 잠재력과 학습 기능을 향상시키는 데 미치는 영향을 보여주고자 합니다. 3.2 MLLM에 링크 컨텍스트 학습 적용 이 섹션에서는 MLLM 영역에 링크 컨텍스트 학습(LCL)을 소개하는 것이 주요 목표입니다. ICL 방식으로 학습된 현재 MLLM이 LCL 작업에서 탁월하지 않을 수 있음을 인식하고 MLLM을 미세 조정하기 위한 새로운 학습 전략을 제안합니다. 이 접근 방식은 모델에 맥락에서 인과 관계를 효과적으로 파악할 수 있는 기능을 제공하는 것을 목표로 합니다. 이 새로운 훈련 전략을 활용하여 MLLM이 추론과 인과 관계 이해가 필요한 작업에서 탁월해지도록 하여 기능 범위를 넓히고 전반적인 성과를 개선하는 것을 목표로 합니다. 더 구체적으로, 우리는 Shikra[27]를 기준으로 선택하고 ImageNet1k를 클래스별로 ImageNet-900과 ImageNet-100으로 나눕니다. 이는 훈련 데이터 세트에서 자세히 설명합니다. 또한 훈련 전략에서 설명한 대로 대비 학습 개념을 훈련 전략에 통합합니다. 이를 통해 모델은 같은 종류의 샘플 간에 공유되는 특성과 다른 종류의 샘플 간의 차이점을 이해하는 데 도움이 됩니다. 3.2.1 훈련 데이터 세트 광범위한 훈련 데이터가 필요한 기존 작업과 달리 LCL은 데모에서 소스-대상 쌍 간의 링크를 찾고 쿼리 샘플로 일반화하는 기능을 습득하는 데 집중합니다. 따라서 다양한 이미지 범주를 적절히 표현하는 것은 MLLM이 인과 관계를 효과적이고 효율적으로 파악할 수 있도록 하는 데 필수적입니다. ImageNetlk[35]는 일반적으로 이미지 분류 작업에 사용되며, 모든 범주에서 인식 능력을 향상시키기 위해 전체 데이터 세트에서 모델을 훈련하는 것이 일반적입니다. 반면, LCL의 훈련 구성 내에서는 각 범주에서 제한된 수의 샘플만 무작위로 선택합니다. 그런 다음 각 범주에 대해 유사도가 감소하는 관련 범주 집합을 정렬합니다. 이를 &quot;이웃&quot;이라고 합니다. 구체적으로, 훈련 데이터 세트 내의 다른 클래스 간의 유사도를 계산하기 위해 CLIP[12]을 채택했습니다. 먼저 각 클래스에서 무작위로 100개의 이미지를 선택하고 각 클래스의 평균 이미지 특징을 계산합니다. 그런 다음 모든 클래스의 텍스트 이름을 인코딩하여 해당 특징 벡터를 얻습니다. 궁극적으로 이미지 대 이미지, 이미지 대 텍스트, 텍스트 대 텍스트 상관 관계를 포함하는 고유한 클래스 쌍에 대한 가중 유사도를 계산합니다. 특정 범주의 경우 유사도를 기준으로 다른 모든 범주를 정렬하고 N개의 간격으로 나눕니다. 그런 다음 각 간격 내에서 무작위로 범주를 선택하여 총 N개의 &quot;이웃&quot; 집합을 구성합니다. 3.2.2 학습 전략 MLLM이 지원 집합과 쿼리 샘플 간의 인과 관계와 지원 집합의 입력-레이블 쌍 간의 인과 관계를 이해하도록 하기 위해 모델이 비교를 통해 학습하도록 촉구하는 양수-음수 쌍을 구축합니다. 지원 집합을 S = {S1, S2, ..., Sn}으로 표시합니다. 샘플 간의 상관 관계를 기반으로 지원 집합을 C = {C1, C2, ..., Cm}으로 재정의할 수 있습니다. 여기서 각 cm은 S의 샘플 클러스터를 나타내는 프로토타입 역할을 합니다. 이러한 프로토타입은 S 내 샘플 간의 필수적인 관계와 유사성을 포착합니다. 쿼리 x가 주어지면 우도를 최대화하기 위해 0을 학습합니다. log pe(y|x) = log pe(yı|x, C, Y1, Y2, Yl-1), (1) 여기서 는 언어 모델의 매개변수를 나타냅니다. 시각적 인코더의 매개변수는 학습하는 동안 고정됩니다.= = = [2방향] 전략: 이 전략에서 우리는 이진 이미지 분류를 위한 MLLM을 학습합니다.여기서 C {C1, C2}.더 구체적으로, 여기서 C₁과 c₂는 두 클래스의 프로토타입을 나타냅니다.우리는 학습 클래스 집합을 T {t1, t2,..., t100}으로 표시하고, 우리는 클래스 ti를 양성 클래스로 무작위로 샘플링합니다.여기서 이웃 클래스 집합 Nti {nti, n,..., no} (nt는 t;와 가장 유사한 클래스이고 n100은 가장 유사하지 않음).그런 다음 우리는 확률 pj로 Nti에서 음성 클래스 nt를 샘플링하는 하드 네거티브 마이닝 전략을 적용합니다.이 설정은 16개 샷으로 학습하도록 고정되어 있습니다. ti = 101-jm=m [2-way-random] 전략: 이 전략에서 우리는 먼저 [2-way] 전략에 따라 고정된 16개의 샷으로 MLLM을 훈련한 다음, 10개의 에포크 동안 2-16개의 샷에서 샘플링된 샷 평균으로 모델을 추가로 훈련합니다.[2-way-weight] 전략: 이 전략 내에서 우리는 처음에 [2-way] 접근 방식을 고수하면서 고정된 16개의 샷 체계를 사용하여 MLLM을 훈련합니다.그런 다음, 각 샷의 확률을 p로 표시하여 2-16 범위에서 샘플링된 샷으로 추가 훈련을 통해 모델을 개선합니다.= e³m=em [mix] 전략: 모델의 일반화를 높이기 위해 [2-way] 작업과 Shikra의 [27] 원래 작업을 모두 포함하는 미세 조정 프로세스를 수행합니다.각 반복 동안 훈련 샘플은 [2way] 작업과 원래 작업에서 균등하게 샘플링됩니다. 이 균형 잡힌 접근 방식은 모델이 새로 도입된 링크 컨텍스트 학습 과제와 Shikra의 기존 과제[27]에서 모두 능숙해지도록 보장합니다.4 ISEKAI 데이터 세트 LCL을 통해 MLLM이 새로운 개념을 학습하는 능력을 객관적으로 평가하기 위해 그림 4에 표시된 ISEKAI 데이터 세트를 만들었습니다.관련 개념은 비현실적이며 전설, 신화 또는 허구 미디어에서 거의 볼 수 없습니다.따라서 MLLM이 이러한 개념에 노출되는 정도는 최소화됩니다. &quot;이세계&quot;라는 용어는 Cactihog AirStone CrystalStag OctoVac Pineapple Palace Shellfortress ISEKAI World Real World ISEKIT에서 유래했습니다. 그림 4: ISEKAI 데이터 세트 개요: 이 데이터 세트는 완전히 생성된 이미지로 구성되어 있으며, &quot;ISEKAI World&quot;의 이미지는 현실에서 존재하지 않는 반면 &quot;Real World&quot;의 이미지는 현실에서 가져온 것입니다. 애니메이션의 판타지 하위 장르입니다. 플롯에는 일반적으로 판타지 영역이나 가상 세계와 같이 다른 세계로 이동하는 캐릭터가 포함됩니다. 관객은 주인공의 탐험을 통해 새로운 세계를 점차적으로 이해하는데, 이는 MLLM이 새로운 지식 영역으로 여행하는 것과 유사합니다. 데이터 세트의 이미지는 잘 만들어진 지침을 사용하여 Midjourney의 [4] 텍스트-이미지 모델에 의해 생성됩니다. 핵심 개념의 일관성을 보장하기 위해 이미지를 수동으로 선택했습니다. 데이터 세트는 현재 20개의 그룹과 총 40개의 범주로 구성되어 있습니다(계속 증가). 각 그룹은 새로운 개념과 &quot;문어 진공 청소기&quot;와 같은 관련된 현실 세계 개념을 짝지어 놓습니다. &quot;문어.&quot; 이들은 서로에 대한 도전적인 부정적 샘플 역할을 할 수 있습니다. 각 개념에는 최소 32개의 이미지가 있어 다중 샷 예제를 지원합니다. 이러한 기능을 통해 ISEKAI는 모델의 LCL 기능을 종합적으로 평가할 수 있습니다. 또한 각 개념의 모양과 이름에 대한 텍스트 설명을 제공하여 LCL을 넘어서는 평가에 기여합니다. 이 논문에서는 ISEKAI에서 다양한 모델의 성능을 평가했습니다. 자세한 내용은 ISEKAI 결과를 참조하세요. 5
--- EXPERIMENT ---
s는 LCL-MLLM이 바닐라 MLLM보다 새로운 개념에 대한 강력한 링크-컨텍스트 학습 기능을 보여준다는 것을 보여줍니다. 코드와 데이터는 https://github.com/isekai-portal/Link-Context-Learning에서 공개됩니다. 1 서론 (가까운 미래에 인류는 마침내 항성간을 여행하고 켄타우로스 별자리에 도달할 수 있게 됩니다.) 인간과 MLLM이 우주선에서 내립니다. 인간: &quot;우리가 해냈어! 봐! 지역 주민들이 왔어.&quot; 지역 주민: 안녕하세요, 우리를 &#39;RockFlock&#39;이라고 부르세요. MLLM: &quot;안녕, 양!&quot; 인간: 인간과 MLLM 간의 위의 대화는 MLLM이 대화 중에 실제 시범을 통해 학습하는 데 어려움을 겪는 방식을 유머러스하게 표현한 것입니다. &#39;RockFlock&#39;은 우리가 직접 만든 종으로, 시범 추론 컨텍스트 내 학습 Q: 이게 뭐야? Q: 이게 뭐야? A: 고양이 A: 판달 링크-컨텍스트 학습 Q: 이게 뭐야? A: 오카피. Q: 이게 뭐지? A: 판다지, 오카피가 아니야. 관련 없음 관련 있음 Q: 이게 뭐지? A: 호랑이. Q: 이게 뭐지? A: 오카피 시범 추론 그림 2: 링크-컨텍스트 학습과 컨텍스트 내 학습의 차이. 컨텍스트 내 학습은 시범을 위해 관련 없는 작업을 제공하는 반면, 링크-컨텍스트 학습의 시범 단계와 추론 단계 사이에는 직접적인 인과 관계가 있습니다. 그림 1에서 보듯이 사람과 같은 몸과 양과 같은 머리. 현재의 MLLM은 보이지 않는 이미지-레이블 쌍을 연결하여 단일 대화에서 새로운 객체를 인식하지 못합니다. 이러한 한계를 해결하기 위해 모델에 소수 샷 학습 능력을 부여하는 것은 MLLM 시대 이전에도 컴퓨터 비전 분야에서 오랜 주제였습니다. 이 접근 방식을 통해 모델은 제한된 예제에서 학습하고 문제를 효과적으로 완화할 수 있습니다. MLLM이 데모에서 학습하는 주요 방법은 컨텍스트 내 학습으로 알려져 있으며, 여기서 모델은 몇 가지 입력-레이블 쌍에 노출된 후 다운스트림 작업에서 현저한 개선을 보입니다. 그러나 현재 MLLM은 메타 작업에서 &quot;학습&quot;한 후 새로운 작업을 처리하는 능력을 습득하도록 모델을 안내하는 데 주로 중점을 두고 있기 때문에 컨텍스트 내 학습에서 매우 제한적인 이점을 얻습니다. 그러나 메타 작업에서 제공된 답변이 모두 틀렸더라도 모델의 성능에는 영향을 미치지 않습니다. [1] 따라서 MLLM이 데모에서 &quot;학습&quot;한 것은 이미지-레이블 쌍 간의 인과 관계를 이해하는 것보다 특정 형식으로 질문에 답하는 데 있습니다. MLLM이 이미지와 레이블 쌍 간의 인과 관계에 더 집중할 수 있도록 Frozen 방법[2]은 알려진 이미지에 다른 레이블을 바인딩합니다. 그러나 MLLM이 이미지와 레이블이 모두 보이지 않는 완전히 새로운 시나리오에 직면하면 상당한 문제가 발생합니다. 이러한 경우, 시범에서 근본 원인과 결과를 추출하고 이 새롭게 발견한 지식을 기반으로 정확한 예측을 하는 작업은 여전히 풀리지 않은 퍼즐로 남아 있습니다.그림 1에 표시된 &#39;RockFlock&#39;(보이지 않는 이미지와 새로운 개념)은 이전 방법에서는 잘못 인식되지만, 우리 모델은 시범에서 &#39;RockFlock&#39; 개념을 학습하고 정확하게 응답합니다.또한 새로운 개념의 습득은 기존 지식을 방해하지 않으므로 모델은 원래 이미지와 새로 학습한 이미지를 효과적으로 구별할 수 있습니다.상황 내 학습(이하 ICL이라고 함)에서 영감을 얻어 MLLM이 대화에서 새로운 개념에 대한 지식을 습득하고 정확한 질의응답을 위해 기존 지식을 유지해야 하는 링크-상황 학습(이하 LCL이라고 함)을 제안합니다.그림 2에서 볼 수 있듯이, MLLM의 현재 상황 내 학습은 인과 무관한 시범의 이점을 강조합니다. 영어: HowAccuracy 1.0.0.0.0.0.Our Method (7B) OpenFlamingo (98) Otter (98) AVE wingeda stripehopper hornedephantthornbrawlersaquaroverbrawnger mushroomhaven shellfortresslavasnakepineapplepalacerockflockgildedfloatshipcactihog airstoneblazefrostskyquill그림 3: ISEKAI 데이터 세트의 여러 범주에 대한 결과 개요: 저희 모델은 거의 모든 범주에서 OpenFlamingo(9B) [6]와 Otter(9B) [5]보다 우수한 성능을 보이며, 전혀 보이지 않는 이미지가 포함된 시나리오에서 뛰어난 성능을 보여줍니다. 그러나 링크 컨텍스트 학습의 경우 데모와 최종 작업은 인과적으로 연결됩니다. (예: 데모에서 &#39;사과&#39;의 이름을 &#39;오렌지&#39;로 바꾸면 모델은 추론 중에 사과를 &#39;오렌지&#39;라고 불러야 합니다.) 이러한 기능을 통해 MLLM은 유연한 방식으로 few-shot 학습을 지원할 수 있습니다. 대규모 언어 모델 시대에, 이러한 모델이 방대한 양의 실제 데이터에 대해 광범위하게 학습되기 때문에 few-shot 학습에서 모델의 성능을 평가하는 것은 과제가 됩니다. 이 문제를 해결하고 링크 컨텍스트 학습에 대한 포괄적인 평가를 제공하기 위해 ISEKAI 데이터 세트를 소개합니다. 이 데이터 세트는 MLLM에게 전혀 새로운 보이지 않는 이미지와 개념으로 구성되어 있으며, 사실성의 경계를 넘어섭니다. 데이터 세트의 모든 이미지는 Stable Diffusion[3]과 Midjourney[4]에서 생성되었으며, 모든 레이블이나 개념도 만들어졌습니다. 그림 3은 ISEKAI 데이터 세트에서 우리 모델과 Otter[5], OpenFlamingo[6]를 비교한 것입니다. 이 논문에서는 MLLM이 대화에서 잠재적인 인과 관계를 이해하고 보이지 않는 이미지와 개념을 처리할 수 있는 기능을 부여하는 설정인 링크 컨텍스트 학습(LCL)을 제시합니다. ICL이 주로 다양한 작업을 통해 영감을 주는 모델에 초점을 맞추는 것과 달리 LCL은 한 걸음 더 나아가 모델이 소스와 타겟 간의 매핑을 설정하여 전반적인 성능을 향상시키도록 지원합니다.이 작업의 기여는 다음과 같이 요약할 수 있습니다.• • 링크 컨텍스트 학습: MLLM이 진행 중인 대화에서 새로운 개념을 동화하고 정확한 질의 응답을 위해 이 지식을 유지하도록 도전하는 새로운 인과 관련 소수 샷 학습 설정을 소개합니다.링크 컨텍스트 학습에서 MLLM이 데모에서 소스와 타겟 간의 인과 관계를 파악할 수 있도록 지원합니다.ISEKAI 데이터 세트: 대부분의 실제 데이터는 MLLM에게 전혀 알려지지 않았으므로 MLLM의 성능을 평가하기 위해 새로운 이미지-개념 쌍이 도입된 까다로운 인위적 데이터 세트를 대중에게 공개합니다.2 관련 연구 다중 모달 대규모 언어 모델[7-11]은 보편적인 생성 또는 인식 작업에서 상당한 역량을 보여주었습니다. MLLMS의 새로운 패러다임에 따라 다양한 시각적 작업을 훈련이 필요 없는 제로샷 방식으로 달성할 수 있으며[12, 13], 무거운 사전 훈련 및 미세 조정 프로세스에서 벗어날 수 있습니다. 그러나 단일 모델을 통해 임의의 콘텐츠를 인식하는 것은 일반적으로 매우 어려운 것으로 간주됩니다. 저렴한 비용으로 야외에서 MLLM의 인식 기능을 향상시키는 방법이 최근 연구 초점으로 등장했습니다. 다중 모달 프롬프트 튜닝 다중 모달 프롬프트 튜닝(M-PT)은 CLIP[12]과 같은 대조 학습 기반 다중 모달 대규모 모델에서 일반적으로 사용됩니다. 훈련 프로세스에서 프롬프트 튜닝은 일반적으로 대부분의 모델 매개변수를 동결하고 미세 조정과 유사한 결과를 얻기 위해 소수의 매개변수만 업데이트합니다[14-17]. PT[14]는 인코더 및 디코더의 각 계층에 튜닝 가능한 프롬프트 임베딩을 추가하고 추가된 임베딩의 가중치만 훈련 중에 업데이트됩니다. VPT[18]는 모델을 튜닝하기 위해 특정 위치에 학습 가능한 매개변수 세트를 추가했습니다. CoOp[15]와 UPT[19]는 CLIP을 백본으로 사용하여 few-shot 설정에 맞게 프롬프트했습니다.CoCoOp[16], POMP[20] 및 MaPLe[21]는 프롬프트 튜닝을 오픈 어휘 시각적 인식 작업으로 확장합니다.그러나 기존의 프롬프트 튜닝 방법은 강력한 생성적 멀티모달 대규모 언어 모델에 적합하지 않습니다.멀티모달 명령어 튜닝 멀티모달 명령어 튜닝(M-IT)은 명령어 설명 기반 데이터 세트[7,8, 11, 22, 23]에서 미세 조정하여 보이지 않는 작업에서 MLLM의 제로샷 기능을 향상시킵니다.MiniGPT-4[24] 및 LLAVA[11]는 시각적 인코더를 고정하고 언어 모델을 조정하여 명령어 튜닝을 멀티모달로 확장합니다.mPLUG-Owl[25]은 시각적 및 텍스트 인코더를 두 단계로 별도로 조정하고 시각 관련 명령어 튜닝을 평가하기 위한 평가 데이터 세트를 제안했습니다. InstructBLIP[26]은 여러 데이터 세트에서 명령어 튜닝을 수행하여 제로샷 기능을 향상시킵니다.Shikra[27]와 Kosmos-2[28]는 경계 상자 좌표가 있는 명령어를 사용하여 MLLM을 시각적 접지 작업으로 확장했습니다.이러한 연구에서 뛰어난 제로샷 기능이 입증되었지만 모델 학습 프로세스 중에 보이지 않은 클래스는 여전히 인식할 수 없습니다.다중 모달 문맥 내 학습 대규모 언어 모델(LLM)은 문맥 샘플에서 학습하는 데 뛰어난 기능을 보여주었습니다.다중 모달 문맥 내 학습(M-ICL) 설정에서 MLLM은 입력 이미지 샘플과 선택적 명령어에 따라 몇 번의 샷 방식으로 새로운 작업 패턴을 학습할 수 있습니다[29-32].Flamingo[33]는 사전 학습 프로세스 중에 문맥 내 학습을 고려하여 모델이 문맥 내 학습을 지원하는 기능을 보유할 수 있도록 합니다.Otter[5]는 Flamingo를 따르고 명령어 튜닝 단계에서 ICL 기능을 진행하여 새로운 문맥 내 학습 데이터 세트를 제안했습니다. 이전 방법과 달리, 제안하는 링크 컨텍스트 학습은 지원과 쿼리 세트 간의 인과 관계를 확립할 수 있습니다. 구체적으로, 소수의 샷 클래스별 이미지와 텍스트 프롬프트를 사용하여 LCL은 프롬프트와 추론 샘플을 연결하고, 이전에 보지 못한 이미지를 새로운 개념과 연관시킬 수도 있습니다. 3 링크 컨텍스트 학습 이 섹션에서는 먼저 컨텍스트 내 학습에 대한 간략한 소개를 제공하고, 예비에서 링크 컨텍스트 학습과의 주요 제한 사항과 차이점을 밝힙니다. 다음으로, 링크 컨텍스트 학습을 MLLM에 가져오기에서 링크 컨텍스트 학습의 힘을 MLLM에 적용합니다. 3.1 예비 = 문맥 내 학습 공식적으로 문맥 내 학습[34]은 다음을 의미합니다. 모델은 쿼리 입력 x가 주어졌을 때, 다양한 작업에서 가져온 여러 입력 라벨 쌍으로 구성된 지원 세트 S를 조건으로, 후보 답변 세트 Y{Y1, Y2,, Yn}에서 가장 높은 예측 점수를 가진 답변을 선택해야 합니다. 여기서 S{(x1, Y1), (x2, Y2), ..., (xn, Yn)}입니다. (쿼리와 S의 샘플은 다른 작업에 속해야 합니다.) 다른 관점에서, 문맥 내 학습은 훈련이 필요 없는 few-shot 학습으로 표시될 수 있습니다. few-shot 학습의 훈련 단계를 대규모 언어 모델에 대한 데모 입력으로 변환하기 때문입니다. ICL[34]은 FSL과 일관성이 있으며, 데모(훈련) 단계와 추론(쿼리) 단계의 작업이 다릅니다. = 링크-컨텍스트 학습 기본적으로 링크-컨텍스트 학습(LCL)은 학습이 필요 없는 인과 연결 소수 학습의 한 형태를 나타냅니다. 이 접근 방식에서 지원 세트 S(x1,y1), (x2,y2), ..., (xn, Yn)이 쿼리 세트 Q의 쿼리 샘플 x와 함께 제공되며, 여기서 지원 세트의 데이터 쌍은 쿼리 세트와 인과적으로 연결됩니다. 모델은 쿼리와 지원 세트 간의 인과 연결 관계를 기반으로 답을 예측하는 작업을 맡습니다. 더 명확하게 설명하기 위해 링크-컨텍스트 학습은 지원 세트와 쿼리 세트 간의 인과 관계를 크게 강화합니다. 예를 들어: 1). 새로운 산술 규칙: 이 시나리오에서 지원 세트는 (1<op> 2 = 3), (2<op> 3 = 5), 쿼리 샘플은 4입니다.<op> 5 =?. 여기서, &quot;<op> &quot;는 우리가 데모를 통해 모델에 가르치고자 하는 새로운 산술 규칙을 나타냅니다. 2). 새로운 이미지 분류: 이 경우 지원 세트에는 다음과 같은 쌍이 포함됩니다.<unseen image> :<novel cls A> ), (<unseen image><novel cls B> ), 쿼리 샘플은 (<unseen image> 영어: belonging to?). 이 예제는 모델이 데모를 기반으로 보이지 않는 이미지를 지정된 새로운 클래스 중 하나로 올바르게 분류하는 방법을 보여줍니다. 본질적으로 링크 컨텍스트 학습은 지원 세트와 쿼리 세트 간의 인과 관계를 효과적으로 설정하여 새로운 개념과 관계를 파악하는 모델의 능력을 향상시킵니다. 이 설정은 LLM과 MLLM 모두에 적용할 수 있지만 이 논문의 주요 초점은 특히 MLLM에서 링크 컨텍스트 학습을 적용하는 것입니다. MLLM에 집중함으로써 다중 모드 모델에서 이 접근 방식의 잠재력과 학습 기능을 향상시키는 데 미치는 영향을 보여주고자 합니다. 3.2 MLLM에 링크 컨텍스트 학습 적용 이 섹션에서는 MLLM 영역에 링크 컨텍스트 학습(LCL)을 소개하는 것이 주요 목표입니다. ICL 방식으로 학습된 현재 MLLM이 LCL 작업에서 탁월하지 않을 수 있음을 인식하고 MLLM을 미세 조정하기 위한 새로운 학습 전략을 제안합니다. 이 접근 방식은 모델에 맥락에서 인과 관계를 효과적으로 파악할 수 있는 기능을 제공하는 것을 목표로 합니다. 이 새로운 훈련 전략을 활용하여 MLLM이 추론과 인과 관계 이해가 필요한 작업에서 탁월해지도록 하여 기능 범위를 넓히고 전반적인 성과를 개선하는 것을 목표로 합니다. 더 구체적으로, 우리는 Shikra[27]를 기준으로 선택하고 ImageNet1k를 클래스별로 ImageNet-900과 ImageNet-100으로 나눕니다. 이는 훈련 데이터 세트에서 자세히 설명합니다. 또한 훈련 전략에서 설명한 대로 대비 학습 개념을 훈련 전략에 통합합니다. 이를 통해 모델은 같은 종류의 샘플 간에 공유되는 특성과 다른 종류의 샘플 간의 차이점을 이해하는 데 도움이 됩니다. 3.2.1 훈련 데이터 세트 광범위한 훈련 데이터가 필요한 기존 작업과 달리 LCL은 데모에서 소스-대상 쌍 간의 링크를 찾고 쿼리 샘플로 일반화하는 기능을 습득하는 데 집중합니다. 따라서 다양한 이미지 범주를 적절히 표현하는 것은 MLLM이 인과 관계를 효과적이고 효율적으로 파악할 수 있도록 하는 데 필수적입니다. ImageNetlk[35]는 일반적으로 이미지 분류 작업에 사용되며, 모든 범주에서 인식 능력을 향상시키기 위해 전체 데이터 세트에서 모델을 훈련하는 것이 일반적입니다. 반면, LCL의 훈련 구성 내에서는 각 범주에서 제한된 수의 샘플만 무작위로 선택합니다. 그런 다음 각 범주에 대해 유사도가 감소하는 관련 범주 집합을 정렬합니다. 이를 &quot;이웃&quot;이라고 합니다. 구체적으로, 훈련 데이터 세트 내의 다른 클래스 간의 유사도를 계산하기 위해 CLIP[12]을 채택했습니다. 먼저 각 클래스에서 무작위로 100개의 이미지를 선택하고 각 클래스의 평균 이미지 특징을 계산합니다. 그런 다음 모든 클래스의 텍스트 이름을 인코딩하여 해당 특징 벡터를 얻습니다. 궁극적으로 이미지 대 이미지, 이미지 대 텍스트, 텍스트 대 텍스트 상관 관계를 포함하는 고유한 클래스 쌍에 대한 가중 유사도를 계산합니다. 특정 범주의 경우 유사도를 기준으로 다른 모든 범주를 정렬하고 N개의 간격으로 나눕니다. 그런 다음 각 간격 내에서 무작위로 범주를 선택하여 총 N개의 &quot;이웃&quot; 집합을 구성합니다. 3.2.2 학습 전략 MLLM이 지원 집합과 쿼리 샘플 간의 인과 관계와 지원 집합의 입력-레이블 쌍 간의 인과 관계를 이해하도록 하기 위해 모델이 비교를 통해 학습하도록 촉구하는 양수-음수 쌍을 구축합니다. 지원 집합을 S = {S1, S2, ..., Sn}으로 표시합니다. 샘플 간의 상관 관계를 기반으로 지원 집합을 C = {C1, C2, ..., Cm}으로 재정의할 수 있습니다. 여기서 각 cm은 S의 샘플 클러스터를 나타내는 프로토타입 역할을 합니다. 이러한 프로토타입은 S 내 샘플 간의 필수적인 관계와 유사성을 포착합니다. 쿼리 x가 주어지면 우도를 최대화하기 위해 0을 학습합니다. log pe(y|x) = log pe(yı|x, C, Y1, Y2, Yl-1), (1) 여기서 는 언어 모델의 매개변수를 나타냅니다. 시각적 인코더의 매개변수는 학습하는 동안 고정됩니다.= = = [2방향] 전략: 이 전략에서 우리는 이진 이미지 분류를 위한 MLLM을 학습합니다.여기서 C {C1, C2}.더 구체적으로, 여기서 C₁과 c₂는 두 클래스의 프로토타입을 나타냅니다.우리는 학습 클래스 집합을 T {t1, t2,..., t100}으로 표시하고, 우리는 클래스 ti를 양성 클래스로 무작위로 샘플링합니다.여기서 이웃 클래스 집합 Nti {nti, n,..., no} (nt는 t;와 가장 유사한 클래스이고 n100은 가장 유사하지 않음).그런 다음 우리는 확률 pj로 Nti에서 음성 클래스 nt를 샘플링하는 하드 네거티브 마이닝 전략을 적용합니다.이 설정은 16개 샷으로 학습하도록 고정되어 있습니다. ti = 101-jm=m [2-way-random] 전략: 이 전략에서 우리는 먼저 [2-way] 전략에 따라 고정된 16개의 샷으로 MLLM을 훈련한 다음, 10개의 에포크 동안 2-16개의 샷에서 샘플링된 샷 평균으로 모델을 추가로 훈련합니다.[2-way-weight] 전략: 이 전략 내에서 우리는 처음에 [2-way] 접근 방식을 고수하면서 고정된 16개의 샷 체계를 사용하여 MLLM을 훈련합니다.그런 다음, 각 샷의 확률을 p로 표시하여 2-16 범위에서 샘플링된 샷으로 추가 훈련을 통해 모델을 개선합니다.= e³m=em [mix] 전략: 모델의 일반화를 높이기 위해 [2-way] 작업과 Shikra의 [27] 원래 작업을 모두 포함하는 미세 조정 프로세스를 수행합니다.각 반복 동안 훈련 샘플은 [2way] 작업과 원래 작업에서 균등하게 샘플링됩니다. 이 균형 잡힌 접근 방식은 모델이 새로 도입된 링크 컨텍스트 학습 과제와 Shikra의 기존 과제[27]에서 모두 능숙해지도록 보장합니다.4 ISEKAI 데이터 세트 LCL을 통해 MLLM이 새로운 개념을 학습하는 능력을 객관적으로 평가하기 위해 그림 4에 표시된 ISEKAI 데이터 세트를 만들었습니다.관련 개념은 비현실적이며 전설, 신화 또는 허구 미디어에서 거의 볼 수 없습니다.따라서 MLLM이 이러한 개념에 노출되는 정도는 최소화됩니다. &quot;이세계&quot;라는 용어는 Cactihog AirStone CrystalStag OctoVac Pineapple Palace Shellfortress ISEKAI World Real World ISEKIT에서 유래했습니다. 그림 4: ISEKAI 데이터 세트 개요: 이 데이터 세트는 완전히 생성된 이미지로 구성되어 있으며, &quot;ISEKAI World&quot;의 이미지는 현실에서 존재하지 않는 반면 &quot;Real World&quot;의 이미지는 현실에서 가져온 것입니다. 애니메이션의 판타지 하위 장르입니다. 플롯에는 일반적으로 판타지 영역이나 가상 세계와 같이 다른 세계로 이동하는 캐릭터가 포함됩니다. 관객은 주인공의 탐험을 통해 새로운 세계를 점차적으로 이해하는데, 이는 MLLM이 새로운 지식 영역으로 여행하는 것과 유사합니다. 데이터 세트의 이미지는 잘 만들어진 지침을 사용하여 Midjourney의 [4] 텍스트-이미지 모델에 의해 생성됩니다. 핵심 개념의 일관성을 보장하기 위해 이미지를 수동으로 선택했습니다. 데이터 세트는 현재 20개의 그룹과 총 40개의 범주로 구성되어 있습니다(계속 증가). 각 그룹은 새로운 개념과 &quot;문어 진공 청소기&quot;와 같은 관련된 현실 세계 개념을 짝지어 놓습니다. &quot;문어.&quot; 이들은 서로에 대한 도전적인 부정적 샘플 역할을 할 수 있습니다. 각 개념에는 최소 32개의 이미지가 있어 다중 샷 예제를 지원합니다. 이러한 기능을 통해 ISEKAI는 모델의 LCL 기능을 종합적으로 평가할 수 있습니다. 또한 각 개념의 모양과 이름에 대한 텍스트 설명을 제공하여 LCL을 넘어서는 평가에 기여합니다. 이 논문에서는 ISEKAI에서 다양한 모델의 성능을 평가했습니다. 자세한 내용은 ISEKAI 결과를 참조하세요. 5 실험 이 섹션에서는 제안된 방법의 효과를 보여주기 위해 실험 결과를 제시합니다. 접근 방식(링크 컨텍스트 학습 기반)과 다른 컨텍스트 내 학습 기반 MLLM 간의 포괄적인 비교를 수행합니다. 5.1 ISEKAI 결과 링크 컨텍스트 학습의 성능을 정량적으로 평가하기 위해 다양한 전략의 방법을 두 가지 도전 데이터 세트(ISEKAI-10 및 ISEKAI-pair)에서 기준선(Shikra [27]) 및 ICL 방법(Otter 및 OpenFlamingo)과 비교합니다. ISEKAI-10 평가: 까다로운 긍정-부정 이미지 쌍의 10개 클래스로 구성된 ISEKAI-10은 긍정 클래스가 현실 세계에 전혀 존재하지 않지만 우리 현실의 흔한 동물이나 사물로 구성된 부정 클래스와 특정 특성을 공유하는 시나리오를 제시합니다.표 1의 상단 섹션은 vanilla-shikra[27]가 어려움을 겪은 ISEKAI-10 데이터 세트의 결과를 보여줍니다.우리 모델은 모든 샷 번호에서 OpenFlamingo[6] 및 Otter[5]와 비교하여 경쟁력 있는 성능을 보여줍니다. ISEKAI-페어 평가: ISEKAI-페어 평가에서 모든 이미지 설정 방법을 사용하여 양수 및 음수 페어를 구성합니다.2-샷 4-샷 6-샷 8-샷 10-샷 12-샷 14-샷 16-샷 OpenFlamingo [6] 0.0.0.0.0.0.0.0.0.Otter [5] 0.0.0.0.0.0.0.0.0.ISEKAI-Vanilla-Shikra [27] 0.0.0.0.0.0.0.0.0.0.Ours-[2-way-random] 0.0.0.0.0.0.0.0.0.Ours-[mix] 0.0.0.0.0.0.0.0.0.OpenFlamingo [6] 0.0.0.0.0.0.0.0.0.Otter [5] 0.0.0.0.0.0.0.0.ISEKAI-쌍 Vanilla-Shikra [27] 0.0.0.0.0.0.0.0.0.Ours-[mix] 0.0.0.0.0.0.0.0.Ours-[2-way-random] 0.43 0.0.0.0.0.0.0.0.0.표 1: 정확도로 측정한 0샷에서 16샷까지의 ISEKAI에 대한 정량적 평가. Otter [5] 및 OpenFlamingo [6]와 비교하여 가장 좋은 결과를 얻었습니다. 방법 zero-shot 2-shot 4-shot 6-shot 8-shot 10-shot 12-shot 14-shot 16-shot OpenFlamingo [6] 0.0.0.0.0.0.0.0.0.0.Otter [5] 0.0.0.0.0.0.0.0.0.0.Vanilla-Shikra [27] 0.0.0.0.0.0.0.0.0.0.Ours-[mix] 0.0.0.0.0.0.0.0.0.0.Ours-[2-way] 0.0.0.0.0.0.0.0.0.0.Ours-[2-way-random] 0.0.0.0.0.0.0.0.0.0.0.Ours-[2-way-weight] 0.0.0.0.0.0.0.0.0.0.0.Table 2: 정량적 평가 ImageNet-100은 정확도로 측정한 0샷에서 16샷까지입니다. Otter [5] 및 OpenFlamingo [6]와 비교하여 가장 좋은 결과를 얻습니다. 실제 세계에 존재하지 않는 범주입니다. 각 개별 이미지는 다른 범주의 모든 이미지와 페어링되어 포괄적인 평가가 용이합니다. 이 평가는 다양한 조합을 통해 완전한 미지수를 처리하는 모델의 기능에 대한 현실적인 척도를 제공합니다. 표 1의 아래 섹션은 이 맥락에서 OpenFlamingo [6] 및 Otter [5]보다 모델이 우수함을 강조합니다. 정성적 결과: 그림 1은 모델과 OpenFlamingo [6] 및 Otter [5]를 시각적으로 비교합니다. 특히, 우리 모델은 새로운 개념을 정확하게 이해하고 익숙하지 않은 개체와 유사한 개체를 효과적으로 구별하는 능력을 보여줍니다. 이 관찰은 데모에서 소스 도메인과 대상 도메인 간의 인과 관계를 포착하는 우리 모델의 능력을 강조합니다. 5.2 ImageNet 결과-우리는 훈련 단계에서 전혀 없었던 100개의 클래스를 포함하는 ImageNet100에서 우리 모델의 성능을 평가합니다.결과는 6샷에서 83%의 가장 높은 정확도를 달성하는 혼합 전략의 효능을 강조합니다.반대로 Otter는 25%의 최대 정확도를 달성하고 OpenFlamingo의 성능은 78%에 이릅니다.ISEKAI 데이터 세트와 달리 ImageNet-do의 이미지는 실제 엔터티에 해당합니다.5.3 절제 연구 기준 진실 입력-레이블 매핑이 존재합니까?우리는 데모(지원 세트) 내의 레이블의 정확성에 대한 절제 분석을 수행합니다.이미지 도메인 Xc ERH×W×3과 레이블 도메인 CE R³ 세트가 주어지면 각 이미지를 해당 레이블과 연관시키는 매핑 ƒ : Xc → C가 있습니다. 우리는 여러 이미지-레이블 쌍 {(x²₁, C₁), (x²₁, C₁), ..., (x², c₁)}을 사용하는데, 여기서 x₁₂ = Xc₂를 지원 세트로 사용합니다. 모델은 후보 세트 Y에서 정답을 예측합니다: ŷ = arg max P(yi|x, f), YiЄY 여기서 예측은 매핑 ƒ에 따라 달라집니다. 결과적으로 지원 세트 내의 매핑 관계를 의도적으로 끊으면 모델이 잘못된 답을 제공하게 되는데, 이는 정확한 예측을 위해 지원 세트의 이미지-레이블 쌍 간의 정확한 연관성에 크게 의존하기 때문입니다. 그림 7에서 볼 수 있듯이, 지원 세트에 점차적으로 거짓 레이블을 삽입하여 매핑 f를 방해하고 레이블의 정확성이 100%에서 0%로 떨어지면 정확도가 0.78에서 0.00으로 떨어집니다. 이러한 결과는 지원 세트 내의 이미지-레이블 쌍 간의 정확한 연관성을 유지하는 것이 링크 컨텍스트 학습에서 중요한 역할을 한다는 것을 분명히 보여줍니다. 모델이 더 큰 샷을 사용하면 이점이 있을까요?쿼리 세트 지원 세트 사용자: 이미지에 무엇이 있나요?사용자: 이미지에 무엇이 있나요?답변: 선인장.답변: 고슴도치.사용자: 이미지에 무엇이 있나요?사용자: 이미지에 무엇이 있나요?답변: 버섯.사용자: 이미지에 무엇이 있나요?수달: 이 이미지는 작고 녹색이며 귀여운 동물입니다.OpenFlamingo: 선인장 이미지입니다.우리: 선인장.사용자: 이미지에 무엇이 있나요?수달: 꽃밭에 있는 고슴도치입니다.OpenFlamingo: 고슴도치 이미지입니다.우리: 고슴도치.사용자: 이미지에 무엇이 있나요?사용자: 이미지에 무엇이 있나요?수달: 파란색 지붕과 문이 있는 버섯입니다.Open Flamingo: 버섯집 이미지입니다.우리: 버섯.수달: 물방울이 맺힌 버섯입니다.OpenFlamingo: 버섯 이미지입니다.우리: 버섯. 그림 5: 새로운 이미지 이해 결과의 정성적 비교. 우리와 OpenFlamingo [6], Otter [5] 간의 결과. &quot;Cactihog&quot;라는 이름은 &quot;선인장&quot;과 &quot;고슴도치&quot;의 합성어로, 이 두 생물의 주요 특징을 결합한 것입니다. &quot;MushroomHaven&quot;이라는 이름은 거대한 버섯이 특징인 거주지를 암시합니다. 지도 학습과 매우 유사하게, 모델의 정확도는 학습 데이터 양이 증가함에 따라 초기 속도가 빠르게 증가하다가 결국 정점에 도달합니다. 이 단계에서는 보다 대표적인 샘플을 선택하는 것이 중요해집니다. 그림 6은 두 가지 결과를 보여줍니다. 하나는 고정된 샷(그림의 회색 막대)에서 별도의 학습을 통해 얻은 모델 정확도를 나타내고, 다른 하나는 다양한 샷에서 샘플링을 통해 얻은 모델 성능을 보여줍니다(그림의 빨간색 선). 결과에 따르면 고정된 샷 학습이 낮을수록 약간의 이득을 얻고 랜덤 샷 학습에서 일관된 성능을 보입니다. 특히, 랜덤 및 고정 설정 모두에서 정확도는 8샷 임계값을 지난 후 정점에 도달하거나 점진적으로 증가합니다. 다중 샷의 경우 모델의 의사 결정은 무엇에 따라 달라집니까?그림 8에서 보듯이, 다른 위치의 라벨을 교란할 때 16샷의 모델의 정확도는 다르게 떨어지며, 이는 모델이 다른 위치를 선호하는 정도를 반영합니다.모델이 시작과 중간 위치에 크게 의존한다는 것을 관찰했습니다.다른 측면에서, 이는 모델이 더 많은 샷에서 고원에 도달하는 이유를 설명합니다.마찬가지로, 이 현상은 LLM[36]에서도 존재하는데, 언어 모델은 긴 맥락을 처리할 때 &quot;중간에서 길을 잃는&quot; 경향이 있습니다.또한 맥락이 길어질수록 모델의 성능이 계속 감소한다는 것을 보여줍니다.다른 훈련 전략의 차이점은 무엇입니까?표 2는 네 가지 고유한 훈련 전략을 통해 달성한 결과에 대한 포괄적인 관점을 보여줍니다.혼합 전략은 제로 샷 정확도를 5%에서 16%로 높이고 6샷에서 놀라운 83%의 정확도를 달성하여 두드러지지만, 성능은 16샷에서 57%로 감소합니다. 이와 대조적으로 16발 훈련에 고정된 2방향 전략은 2발에서 51%의 정확도로 시작하여 점진적으로 16발에서 79%로 상승합니다. 흥미롭게도, 2방향 전략의 정확도 추세는 단순히 사격 수의 증가에 기인하는 것이 아니라 훈련된 패턴과 더 긴밀하게 일치하는 데서 비롯된다는 것을 관찰했습니다. 이를 검증하기 위해 2방향 랜덤 및 2방향 가중치라는 두 가지 추가 설정을 도입했습니다. 이러한 설정은 초기화를 위해 고정 사격 훈련을 거친 다음, 각각 랜덤 및 가중치 접근 방식으로 2~16발에 걸쳐 미세 조정합니다. 둘 다 정확도 샘플링 고정 Ours OpenFlamingo Otter --- Origin Ours 0.0.0.0.0.0.0.0.80.0.750.70.0.70 0.0.60.650.60 정확도 0.0.4Q.0.0.0.0.750 정확도 0.0.0.0.0.0.0.30.550.0.6500.0.0.0.0.6250.0.1st 4th 8th 12th 16th 0.450.400.Modified Position 0.0% 25% 2샷 4샷 8샷 12샷 16샷 50% 거짓 비율 75% 100% 그림 6: 샷 번호에 대한 절제 연구. 회색 막대는 각 샷 번호에 대해 달성한 가장 높은 정확도를 보여주며, 특정 샷 기반 학습을 나타냅니다. 빨간색 선은 샘플링된 전략을 사용하여 학습한 모델의 성능을 보여줍니다. 주목할 점은 두 시나리오 모두 8샷 마크에 도달한 후 정확도가 정점에 도달한다는 것입니다. 그림 7: 거짓 비율에 대한 절제 연구. 거짓 비율 100%에서 정확도 38%를 유지하는 OpenFlamingo[6]와 대조적으로, 우리 모델은 동일한 조건에서 정확도 0%를 달성합니다. 이 결과는 우리 모델이 지원 세트와 쿼리 간의 정확한 연결을 유지하는 능력을 강조합니다. 그림 8: 서로 다른 위치에서 레이블 수정의 효과. 파선으로 된 파란색 선은 원래 정확도에 대한 참조 역할을 하는 반면, 빨간색 선은 특정 위치에서 레이블이 수정된 후 모델의 정확도를 나타냅니다. 상당한 정확도 저하는 위치 종속성을 반영하는 반면, 사소한 변화는 모델의 의사 결정에서 위치가 중요하지 않음을 나타냅니다. 낮은 샷에서 상당한 정확도 향상. 특히, 무작위 전략으로 미세 조정된 더 높은 샷의 정확도는 떨어지는 반면, 혼합 전략의 행동을 반영하는 관찰이 있습니다. 이러한 결과는 대규모 언어 모델의 잠재력을 활용하는 데 있어 균등하고 지속적이며 일반화된 훈련 접근 방식의 효능을 강조하여 이전의 관찰과 일관되게 &quot;중간에서 잃어버린&quot; 현상의 출현을 보여줍니다. 훈련이 제로 샷 성능에 해를 끼치나요? 표 3은 shikra-13B [27]를 사용한 -7B 모델과 Imagenet-100 및 VQAv2에서 일부 이전 SOTA 방법을 비교한 것입니다. 결과에서 혼합 훈련 전략이 모델의 제로 샷 성능에 해를 끼치지 않는다는 결론을 내렸습니다. 6 토론 6.1 제한 사항 이 작업이 MLLM과 LLM 모두에게 어렵고 유망한 환경을 제공한다고 믿습니다. 그러나 이 논문의 주요 초점은 MLLM 컨텍스트 내에서 링크 컨텍스트 학습에 있으며, 특히 이미지 분류와 같은 기본 작업을 검증하는 것입니다. 결과적으로 이 연구는 링크-컨텍스트 학습의 잠재력을 탐구하기 위한 기초적인 기준으로 간주되어야 합니다. 앞으로의 연구 방향은 지원 샘플 간의 인과 관계의 복잡성과, 가장 중요하게는 지원 세트와 질의 간의 복잡성을 탐구하는 보다 심층적인 이론적 분석을 포함합니다. 이러한 인과 관계의 복잡성을 이해하고 풀어내는 것은 모델의 추론, 학습 및 새로운 시나리오에 대한 적응 능력을 크게 향상시킬 수 있는 의미 있는 탐구의 길을 나타냅니다. 분야가 진전됨에 따라, 우리는 더 많은 조사와 개선을 기대합니다 ImageNet-100 VQAV2dev VQAV2std 방법 OpenFlamingo [6] Flamingo-80B [33] Flamingo-9B [33] BLIP2 [9] 0.56.51.65.Otter [5] 0.Shikra-13B [27] Ours-7B-[mix] 0.77.77.0.75.75.표 3: 정량적 평가는 제로샷 접근 방식을 사용하여 ImageNet-100 및 VQAv2 데이터 세트 모두에서 수행되었습니다. 결과는 우리의 훈련 전략이 제로샷 성능에 부정적인 영향을 미치지 않는다는 것을 입증합니다. 이는 링크 컨텍스트 학습에 대한 이해를 풍부하게 할 뿐만 아니라 MLLM 및 LLM에 대한 컨텍스트 내 학습을 통합된 방식으로 구현할 것입니다. 6.2
--- CONCLUSION ---
결론적으로, 이 논문은 인과 관련성이 있는 few-shot 학습의 획기적인 패러다임을 소개하고, 단일 대화의 맥락에서 다중 모드 대규모 언어 모델(MLLM)의 역량을 크게 확장합니다. 세심한 실험과 신중하게 고안된 훈련 전략을 통해 MLLM이 실제 입력-레이블 쌍 간의 매핑을 능숙하게 수립하여 이전에 접하지 못했던 이미지와 새로운 개념으로 이 역량을 원활하게 일반화하는 능력을 습득할 수 있음을 보여줍니다. 이 중요한 발전은 MLLM을 미지의 영역으로 밀어붙여 지식을 습득할 뿐만 아니라 인간의 인지에 더 가까운 방식으로 적용할 수 있게 합니다. 참고문헌 [1] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer. 시연의 역할 재고: 맥락 내 학습을 작동하게 하는 요소는 무엇인가? arXiv 사전 인쇄본 arXiv:2202.12837, 2022. [2] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, Felix Hill. 동결된 언어 모델을 사용한 다중 모드 few-shot 학습. 신경 정보 처리 시스템의 발전, 34:200-212, 2021. [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. 컴퓨터 비전 및 패턴 인식에 대한 IEEE/CVF 컨퍼런스 회의록, 10684-10695페이지, 2022. [4] Midjourney. Midjourney. https://www.midjourney.com, 2023. [5] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu. Otter: 컨텍스트 내 명령어 튜닝을 갖춘 다중 모달 모델. CoRR, abs/2305.03726, 2023. [6] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt. Openflamingo, 2023년 3월. [7] OpenAI. Gpt-4 기술 보고서, 2023. [8] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. Blip: 통합 시각 언어 이해 및 생성을 위한 언어 이미지 사전 학습 부트스트래핑. 기계 학습 국제 컨퍼런스, 12888-12900페이지. PMLR, 2022. [9] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip2: 동결된 이미지 인코더 및 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. [10] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao. gpt-4를 사용한 명령어 튜닝. arXiv 사전 인쇄본 arXiv:2304.03277, 2023. [11] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 시각 지시 튜닝. arXiv 사전 인쇄본 arXiv:2304.08485, 2023. [12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763쪽. PMLR, 2021. [13] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, 외. 기반 언어 이미지 사전 훈련. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 1096510975, 2022. [14] Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou 및 Hongxia Yang. 생성적 다중 모드 사전 학습 모델을 위한 즉각적인 조정. arXiv 사전 인쇄 arXiv:2208.02532, 2022. [15] Kaiyang Zhou, Jingkang Yang, Chen Change Loy 및 Ziwei Liu. 비전 언어 모델을 요청하는 방법을 학습합니다. International Journal of Computer Vision, 130(9):2337-2348, 2022. [16] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu. 시각 언어 모델을 위한 조건부 프롬프트 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16816-16825페이지, 2022. [17] Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo, Fei Tan. 깊이 결합된 교차 모달 프롬프트 학습. arXiv 사전 인쇄본 arXiv:2305.17903, 2023. [18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim. 시각적 프롬프트 튜닝. European Conference on Computer Vision, 709-727쪽.Springer, 2022. [19] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy. 통합 비전 및 언어 프롬프트 학습, 2022. [20] Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alex Smola, Xu Sun. 오픈 어휘 시각 인식을 위한 2만 개의 클래스를 사용한 프롬프트 사전 학습, 2023. [21] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan. Maple: 다중 모달 프롬프트 학습, 2023. [22] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V Le. 미세 조정된 언어 모델은 제로샷 학습자입니다. 학습 표현에 관한 국제 컨퍼런스, 2022. [23] OpenAI. ChatGPT를 소개합니다. https://openai.com/blog/ chatgpt, 2023. [24] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델을 통한 시각 언어 이해 향상, 2023. [25] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang 및 Fei Huang. mplug-owl: 모듈화는 다중 양식으로 대규모 언어 모델을 지원합니다(2023년). [26] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung 및 Steven Hoi. Instructblip: 지침 조정을 통한 범용 비전 언어 모델을 향하여, 2023. [27] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu 및 Rui Zhao. Shikra: 멀티모달 LLM의 참조 대화 마법 해방, 2023. [28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma 및 Furu Wei. Kosmos-2: 다중 모드 대형 언어 모델을 세계에 접지, 2023년. [29] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li 및 Zhifang Sui. 상황 내 학습에 관한 설문 조사, 2023. [30] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 및 Lijuan Wang. Mm-react: 다중 모달 추론 및 행동을 위한 chatgpt 프롬프트, 2023. [31] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu 및 Jianfeng Gao. 카멜레온: 대규모 언어 모델을 사용한 플러그 앤 플레이 구성 추론, 2023. [32] Tanmay Gupta 및 Aniruddha Kembhavi. 시각적 프로그래밍: 훈련 없는 구성 시각적 추론, 2022. [33] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikołaj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman 및 Karén Simonyan. 플라밍고: few-shot 학습을 위한 시각적 언어 모델. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh 편집자, 신경 정보 처리 시스템의 발전, 35권, 23716-23736페이지. Curran Associates, Inc., 2022. [34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. [35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei. ImageNet 대규모 시각 인식 챌린지. International Journal of Computer Vision(IJCV), 115(3):211-252, 2015. [36] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang. 중간에서 길을 잃다: 언어 모델이 긴 컨텍스트를 사용하는 방법. arXiv 사전 인쇄본 arXiv:2307.03172, 2023.
