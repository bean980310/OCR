--- ABSTRACT ---
우리는 시각적 질의응답을 모듈식 코드 생성으로 공식화하는 프레임워크를 제시합니다. VQA에 대한 모듈식 접근 방식에 대한 이전 작업과 달리, 우리의 접근 방식은 추가 학습이 필요하지 않으며 사전 학습된 언어 모델(LMS), 이미지-캡션 쌍에 대해 사전 학습된 시각적 모델, 컨텍스트 내 학습에 사용되는 50개의 VQA 예제에 의존합니다. 생성된 Python 프로그램은 산술 및 조건 논리를 사용하여 시각적 모델의 출력을 호출하고 구성합니다. 우리의 접근 방식은 코드 생성을 사용하지 않는 few-shot 베이스라인과 비교하여 COVR 데이터 세트의 정확도를 최소 3%, GQA 데이터 세트의 정확도를 약 2% 향상시킵니다. 1
--- INTRODUCTION ---
시각적 질의응답(VQA)에 필요한 추론 범위는 방대하며, 언어 기반에서 픽셀(Goyal 등, 2017; Radford 등, 2021; Zhai 등, 2022) 및 공간 추론(Hudson 및 Manning, 2019)에서 상식 및 지식 기반 추론(Marino 등, 2019)에 이르기까지 많은 기술의 종합이 필요합니다. &quot;마차가 말의 오른쪽에 있습니까?&quot;라는 질문을 생각해 보세요. 이러한 질문에 지속적으로 올바르게 답하려면 시스템이 해당 질문이 &quot;말이 있습니까?&quot;와 &quot;마차가 말의 오른쪽에 있습니까?&quot;라는 두 하위 질문의 결합이라는 것을 인식해야 합니다. 일반적인 미세 조정 패러다임을 모든 가능한 추론 기술 조합으로 확장하는 것은 주석 비용이 엄청나게 비싸고 이미 훈련된 시스템에 기술을 추가하는 것을 어렵게 만듭니다. 반면, 모듈식 접근법은 고전적 방법(Krishnamurthy 및 Kollar, 2013)에서 미분 가능 신경 모듈 네트워크(NMNS)(Andreas 등, 2016; Hu 등, 2017; Saqur 및 Narasimhan, 2020))에 이르기까지 일반화 수단으로 시각적 추론의 구성적 특성을 활용하고 확장할 수 있는 잠재적 경로를 제공합니다. 즉, 유한한 수단의 무한한 사용입니다. 그러나 NMN의 모듈은 여전히 대규모 데이터 세트에서 공동으로 학습해야 하며 (i) 모듈이 시스템에 추가되거나 제거되는 경우 수정해야 하는 파서가 필요하고 (ii) 모듈을 교체하는 경우 재학습이 필요하다는 점에서 제한적입니다. 이 작업에서 우리는 모듈형 VQA 접근 방식의 대안적 클래스를 조사합니다. 이를 통해 최신의 고성능 아웃 오브 더 박스 언어 모델(LMS)(Chen et al., 2021; Ouyang et al., 2022)과 시각 언어 모델(VLM)(Li et al., 2022)의 출현을 바탕으로 VQA를 프로그램 합성 문제로 공식화하는 시스템을 개발합니다. 구체적으로, 그림 1에 나와 있는 방법인 CodeVQA는 코드 작성 LM을 사용하여 질문을 입력으로 받고, (i) VLM을 감싸는 일련의 시각적 기본 API를 조정하여 이미지에서 특정 시각 정보(예: 캡션, 엔터티의 픽셀 위치 또는 이미지-텍스트 유사도 점수)를 조사하고, (ii) Python 코드의 전체 표현(예: 산술, 논리 구조, 피드백 루프 등)으로 해당 정보에 대해 추론하여 답을 도출합니다. 실용적인 관점에서 CodeVQA의 모듈성과 LM의 few-shot 프롬핑 기능을 결합하면 추가 모델 학습 없이도 다양한 원하는 VQA 레이블 분포에 적응할 수 있으며, 사용 가능해짐에 따라 개별 모듈을 개선된 버전으로 교체하는 데 이점이 있습니다. 저희는 많은 최근 작업이 진행된 few-shot VQA 설정에서 CodeVQA를 평가합니다(Alayrac et al., 2022; Jin et al., 2021; Yang et al., 2021; Tiong et al., 2022). 저희 방법은 여러 이미지에 대한 추론이 필요한 COVR 데이터 세트(Bogin et al., 2021)에서 이전 접근 방식보다 최소 3%, GQA 데이터 세트(Hudson and Manning, 2019)에서 약 2% 더 우수한 성능을 보입니다. 우리의 결과는 최근의 기성형 모델을 사용한 모듈성의 이점이 추가 모델 학습 없이 VQA에서 실현될 수 있음을 시사합니다.¹ ¹저희 코드와 주석이 달린 프로그램은 https://github.com/sanjayss34/codevqa에서 제공됩니다. 질문: 마차가 말의 오른쪽에 있습니까? Codex(Few-Shot Prompting) 맥락 내 예제 # 이미지 1 그림의 어느 쪽에 양탄자가 있습니까? img = open_image(&quot;Image1.jpg&quot;) rug_pos_x, rug_pos_y = get_pos(img, &quot;rug&quot;) if rug_pos_x &lt; (LEFT+RIGHT)/2: answer &quot;left&quot; else: answer = &quot;right&quot; 코드 생성 horse_exists = query(img, &quot;말이 있나요?&quot;) answer=&quot;no&quot; if horse_exists == &quot;yes&quot;: carriage_pos_x, carriage_pos y = get_pos (img, &quot;carriage&quot;) horse_pos_x, horse_pos_y = get_pos (img, &quot;horse&quot;) if carriage_pos_x &gt; horse_pos_x: answer &quot;yes&quot;&gt; 코드 실행 query(img, &quot;말이 있나요?&quot;) 캡션: 1. &#39;소방관이 마차를 타고 끄는 경찰 말&#39;, 2. &#39;경찰관 옆에서 말을 끄는 마차를 탄 남자&#39;, ... &quot;yes&quot; 반환 정답: 아니요 get_pos (img, &quot;carriage&quot;) get_pos (img, &quot;horse&quot;)는 5를 반환하고, 12를 반환하고, carriage_pos_x &lt; horse_pos_x 그림 1: CodeVQA 개요. CodeVQA는 먼저 주어진 질문을 Python 코드로 분해하는 맥락 내 예제로 Codex를 프롬프트합니다. 질문만 사용하여 Codex는 조건 논리, 산술 등을 사용하여 미리 정의된 시각적 모듈을 구성하는 실행 가능한 프로그램을 생성합니다. 시각적 모듈인 쿼리는 이미지에 캡션을 지정하고 캡션을 기반으로 LM을 사용하여 질문에 답합니다. get_pos는 객체의 위치를 검색합니다. 여기서 CodeVQA는 질문을 쿼리와 공간 비교의 결합으로 올바르게 식별하고 올바른 답에 도달합니다. 2
--- RELATED WORK ---
추론 과제에 대한 최근의 여러 접근 방식은 프로그램을 작성하는 LM과 이러한 프로그램을 위한 인터프리터로 구성됩니다.Liang et al.(2022)은 이 접근 방식을 로봇 공학에 적용합니다.Cheng et al.(2023)은 이미지 캡션으로 표현된 이미지, 텍스트 및 이미지에 대한 공동 추론을 위한 프레임워크를 소개합니다.Subramanian et al.(2022)은 LM 대신 구문 분석기와 하드 코딩된 규칙을 사용하여 제로 샷 참조 표현 이해를 위해 CLIP(Radford et al., 2021)의 출력을 집계했습니다.CLIP이 공간 키워드에 유용하지 않다는 그들의 발견은 공간 추론에 대한 우리의 코드 생성 접근 방식의 동기를 부여합니다.우리의 작업과 동시에 다른 논문에서도 멀티홉 VQA에 대한 유사한 프레임워크를 도입했습니다(Gupta and Kembhavi, 2023; Surís et al., 2023). 이 논문들은 프로그램 합성의 이점을 LM, 컨텍스트 내 예제, 원시로 사용되는 비전 모델의 이점과 혼동합니다. 이와 대조적으로, 우리는 동일한 컨텍스트 내 예제 선택을 사용하여 CodeVQA를 강력한 LM 기반 few-shot 베이스라인과 비교하여 프로그램 합성의 효과를 분석합니다.
--- METHOD ---
s(Krishnamurthy 및 Kollar, 2013), 미분 가능 신경 모듈 네트워크(NMNS)(Andreas 등, 2016; Hu 등, 2017; Saqur 및 Narasimhan, 2020)) - 일반화 수단으로 시각적 추론의 구성적 특성을 활용하고 확장할 수 있는 잠재적 경로를 제공합니다. 즉, 유한한 수단의 무한한 사용입니다. 그러나 NMN의 모듈은 여전히 대규모 데이터 세트에서 공동으로 학습해야 하며 (i) 모듈이 시스템에 추가되거나 제거되는 경우 수정해야 하는 파서가 필요하고 (ii) 모듈을 교체하는 경우 재학습이 필요하다는 점에서 제한이 있습니다. 이 작업에서 우리는 모듈형 VQA 접근 방식의 대안적 클래스를 조사합니다. 이를 통해 최신의 고성능 아웃 오브 더 박스 언어 모델(LMS)(Chen et al., 2021; Ouyang et al., 2022)과 시각 언어 모델(VLM)(Li et al., 2022)의 출현을 바탕으로 VQA를 프로그램 합성 문제로 공식화하는 시스템을 개발합니다. 구체적으로, 그림 1에 나와 있는 방법인 CodeVQA는 코드 작성 LM을 사용하여 질문을 입력으로 받고, (i) VLM을 감싸는 일련의 시각적 기본 API를 조정하여 이미지에서 특정 시각 정보(예: 캡션, 엔터티의 픽셀 위치 또는 이미지-텍스트 유사도 점수)를 조사하고, (ii) Python 코드의 전체 표현(예: 산술, 논리 구조, 피드백 루프 등)으로 해당 정보에 대해 추론하여 답을 도출합니다. 실용적인 관점에서 CodeVQA의 모듈성과 LM의 few-shot 프롬핑 기능을 결합하면 추가 모델 학습 없이도 다양한 원하는 VQA 레이블 분포에 적응할 수 있으며, 사용 가능해짐에 따라 개별 모듈을 개선된 버전으로 교체하는 데 이점이 있습니다. 저희는 많은 최근 작업이 진행된 few-shot VQA 설정에서 CodeVQA를 평가합니다(Alayrac et al., 2022; Jin et al., 2021; Yang et al., 2021; Tiong et al., 2022). 저희 방법은 여러 이미지에 대한 추론이 필요한 COVR 데이터 세트(Bogin et al., 2021)에서 이전 접근 방식보다 최소 3%, GQA 데이터 세트(Hudson and Manning, 2019)에서 약 2% 더 우수한 성능을 보입니다. 우리의 결과는 최근의 기성형 모델을 사용한 모듈성의 이점이 추가 모델 학습 없이 VQA에서 실현될 수 있음을 시사합니다.¹ ¹저희 코드와 주석이 달린 프로그램은 https://github.com/sanjayss34/codevqa에서 제공됩니다. 질문: 마차가 말의 오른쪽에 있습니까? Codex(Few-Shot Prompting) 맥락 내 예제 # 이미지 1 그림의 어느 쪽에 양탄자가 있습니까? img = open_image(&quot;Image1.jpg&quot;) rug_pos_x, rug_pos_y = get_pos(img, &quot;rug&quot;) if rug_pos_x &lt; (LEFT+RIGHT)/2: answer &quot;left&quot; else: answer = &quot;right&quot; 코드 생성 horse_exists = query(img, &quot;말이 있나요?&quot;) answer=&quot;no&quot; if horse_exists == &quot;yes&quot;: carriage_pos_x, carriage_pos y = get_pos (img, &quot;carriage&quot;) horse_pos_x, horse_pos_y = get_pos (img, &quot;horse&quot;) if carriage_pos_x &gt; horse_pos_x: answer &quot;yes&quot;&gt; 코드 실행 query(img, &quot;말이 있나요?&quot;) 캡션: 1. &#39;소방관이 마차를 타고 끄는 경찰 말&#39;, 2. &#39;경찰관 옆에서 말을 끄는 마차를 탄 남자&#39;, ... &quot;yes&quot; 반환 정답: 아니요 get_pos (img, &quot;carriage&quot;) get_pos (img, &quot;horse&quot;)는 5를 반환하고, 12를 반환하고, carriage_pos_x &lt; horse_pos_x 그림 1: CodeVQA 개요. CodeVQA는 먼저 주어진 질문을 Python 코드로 분해하는 맥락 내 예제로 Codex에 프롬프트를 표시합니다. 질문만 사용하여 Codex는 조건 논리, 산술 등을 사용하여 미리 정의된 시각적 모듈을 구성하는 실행 가능한 프로그램을 생성합니다. 시각적 모듈인 query는 이미지에 캡션을 지정하고 캡션을 기반으로 LM을 사용하여 답함으로써 질문에 답합니다. get_pos는 객체의 위치를 검색합니다. 여기서 CodeVQA는 질문을 쿼리와 공간 비교의 결합으로 올바르게 식별하고 올바른 답에 도달합니다. 2 관련 작업 추론 작업을 위한 최근의 여러 접근 방식은 프로그램을 작성하는 LM과 이러한 프로그램을 위한 인터프리터로 구성됩니다. Liang et al. (2022)은 이 접근 방식을 로봇 공학에 적용합니다. Cheng et al. (2023)은 이미지 캡션으로 표현된 이미지에서 표, 텍스트 및 이미지에 대한 공동 추론을 위한 프레임워크를 소개합니다.Subramanian 등(2022)은 LM 대신 구문 분석기와 하드 코딩된 규칙을 사용하여 제로 샷 참조 표현 이해를 위해 CLIP(Radford 등, 2021)의 출력을 집계했습니다.CLIP이 공간 키워드에 유용하지 않다는 그들의 발견은 공간 추론에 대한 코드 생성 접근 방식의 동기를 부여합니다.우리의 작업과 동시에 다른 논문에서도 멀티홉 VQA에 대한 유사한 프레임워크를 도입했습니다(Gupta 및 Kembhavi, 2023; Surís 등, 2023).이러한 논문은 프로그램 합성의 이점을 LM, 컨텍스트 내 예제 및 원시로 사용되는 비전 모델의 이점과 혼동합니다.반대로, 우리는 동일한 컨텍스트 내 예제 선택 방법을 사용하여 CodeVQA를 강력한 LM 기반 few-shot 베이스라인과 비교하여 프로그램 합성의 효과를 분석합니다. 또한 이러한 프레임워크가 지도 학습 VQA 또는 객체 감지 모델에 의존하는 반면, LM과 이미지-텍스트 쌍에 대해 사전 학습된 모델만 사용하여 비슷한 성능(GQA 데이터 세트에서)을 얻을 수 있음을 보여줍니다.3 코드 생성을 통한 소수 샷 VQA 시각적 질의 응답(VQA)에서 시스템의 입력은 이미지와 질문이고 출력은 텍스트 답변입니다.시스템이 소수(50)의 인간이 주석을 단 VQA 인스턴스에만 액세스할 수 있는 소수 샷 VQA 설정을 고려합니다.개요.그림 1은 우리의 접근 방식을 보여줍니다.이미지와 해당 질문이 주어지면 CodeVQA는 먼저 질문만 사용하여 Python 프로그램을 생성합니다.그런 다음 필요한 경우 이미지를 사용하여 이 프로그램을 실행하여 답변을 예측합니다.먼저 시스템에서 사용하는 코드 기본 요소 집합을 정의합니다(§3.1).그런 다음 질문을 기반으로 이러한 기본 요소를 구성하는 프로그램을 생성하는 방법을 설명합니다(§ 3.2). 마지막으로, 우리가 사용하는 사전 학습된 모델을 열거합니다(§ 3.3). 3.1 코드 기본 요소 기본 요소는 VQA에 종종 유용한 이미지 또는 텍스트에 대한 기본 연산을 정의합니다. CodeVQA에서 우리는 아래에 정의된 세 가지 기본 요소를 사용합니다. 이러한 기본 요소는 각각 이미지-텍스트 매칭(ITM), 이미지-텍스트 대조(ITC) 및 이미지 캡션 모델을 사용하여 구현되며, 각각 이미지-캡션 쌍으로만 학습할 수 있습니다. ITM과 ITC의 차이점은 ITC가 별도의 이미지 및 텍스트 임베딩을 계산하고 내적을 취하는 반면 ITM은 이미지 및 텍스트 피처에 대한 초기 융합을 수행하므로 계산 비용이 더 많이 든다는 것입니다. 우리의 프레임워크는 이러한 기본 요소 선택에 얽매이지 않으며 프로그래밍 언어 및 타사 라이브러리의 다른 측면을 활용할 수 있는 다른, 더 복잡한 기본 요소를 지원할 수 있습니다. query(image, question) 이 함수는 주어진 이미지에 대한 질문에 답합니다. 이 기능에 대한 우리의 구현은 PnP-VQA(Tiong et al., 2022)와 PICa(Yang et al., 2021)를 기반으로 하며 다음 단계로 구현됩니다. (1) ITM 모델을 사용하여 질문과 이미지 사이의 GradCAM(Selvaraju et al., 2016)을 계산합니다(질문 토큰에 대해 평균화). (2) GradCAM 점수에 따라 K = 20 이미지 패치를 샘플링합니다. (3) 캡션 모델을 사용하여 샘플링된 패치에서 캡션을 생성합니다. (4) C개의 고유 캡션이 생성될 때까지 단계(2)와 (3)을 반복합니다. (5) 질문, 캡션 및 맥락 내 예제가 있는 LM을 프롬프트하여 답을 예측합니다. 단계(5)의 맥락 내 예제는 § 3.2에서 설명한 대로 선택됩니다. 데이터 세트에 여러 이미지에 대한 추론이 포함되는 경우 각 맥락 내 예제에는 모든 이미지에 대한 캡션이 있습니다. get_pos(image, text) 이 함수는 ITM 모델을 사용하여 주어진 텍스트 토큰과 이미지 사이의 GradCAM을 계산하고 GradCAM 값을 최대화하는 (x, y) 쌍을 반환합니다. 모든 질문 토큰에 대해 평균을 내지 않기 때문에 이 GradCAM 응용 프로그램은 쿼리의 응용 프로그램과 다릅니다. GradCAM 맵을 계산하는 방법에 대한 자세한 내용은 부록 B를 참조하세요. find_matching_image(images, text) 각 질문과 여러 이미지가 연관된 설정에서 특정 이미지를 참조하는 질문이 있습니다(예: &quot;여자가 들고 있는 것은 무엇입니까?&quot;). 이 함수를 사용하여 세트에서 가장 관련성 있는 이미지를 선택할 수 있습니다. ITC 모델을 사용하여 텍스트가 있는 각 이미지에 점수를 매기고 가장 높은 점수를 받은 이미지를 선택하여 구현합니다. 3.2 코드 생성 CodeVQA의 첫 번째 단계에서는 질문을 기반으로 Python 프로그램을 생성합니다. 도메인 특정 언어 대신 Python을 사용하는 것이 유리한 이유는 (1) 루프 및 if 문(Liang et al., 2022)을 포함한 산술과 제어 흐름을 모두 지원하기 때문입니다.이 모든 것은 프로그램에서 사용합니다.또한 (2) 코드 생성을 위한 대규모 LM(예: Codex(Chen et al., 2021))이 대량의 Python 코드에서 학습되었기 때문입니다.명령어, 이미지의 크기를 정의하는 상수, 사용 가능한 함수를 지정하는 import 문 및 API 문서(코드 주석)로 구성된 프롬프트를 구성합니다.LM에 대한 입력에는 프롬프트 외에도 여러 컨텍스트 내 예제에 대한 전문가 주석이 달린 프로그램도 포함됩니다.COVR 데이터 세트에 대한 few-shot 프롬프팅에 대한 컨텍스트 내 예제는 아래와 같습니다(질문은 회색이고 프로그램은 강조 표시됨).# 이미지 세트 1: 분홍색 신발이 정확히 2개 들어 있는 이미지가 몇 개나 있나요?? images = open_images(&quot;ImageSet1.jpg&quot;) count =for image in images: two_pink_shoes = query(image, &quot;분홍색 신발이 정확히 2개 있나요?&quot;) if two_pink_shoes == &quot;yes&quot;: count +=answer = count LM 프롬프트의 나머지 예는 부록 A를 참조하세요. 생성된 프로그램을 실행하면 런타임 오류가 발생하면 이미지와 원래 질문(인스턴스에 여러 이미지가 포함된 경우 모든 이미지에 대한 캡션 포함)에 대해 call query를 반환합니다. 주석이 달린 모든 프로그램이 모델의 단일 입력에 맞을 수 없으므로 각 테스트 질문에 대한 컨텍스트 내 예로 사용할 프로그램을 선택해야 합니다. Wang et al. (2022)에 따라 문장 임베딩²을 사용하여 각 테스트 질문에 대해 가장 유사한 질문을 검색합니다. 3.3 구성 요소 모델 저희의 접근 방식은 사전 학습된 네 가지 모델, 즉 코드 생성 모델, ITM 모델, ITC 모델, IC 모델, 캡션을 기반으로 질문에 답하기 위한 질의응답 LM에 의존합니다. 우리는 OpenAI API를 통해 프로그램 생성과 질의응답을 위해 code-davinci-002 모델(Chen et al., 2021)을 사용합니다. 우리는 ITM, ITC 및 캡션을 위해 미세 조정된 BLIP 모델(Li et al., 2022)을 사용합니다. 4
--- EXPERIMENT ---
s 4. 구현 세부 정보 구현 세부 정보는 부록 C를 참조하십시오. 4.2 데이터 세트 GQA 데이터 세트(Hudson 및 Manning, 2019)에는 Visual Genome의 개별 이미지에 대한 인간이 주석을 단 장면 그래프에서 생성된 멀티홉 질문이 포함되어 있습니다(Krishna 등, 2016). COVR 데이터 세트(Bogin 등, 2021)에는 Visual Genome 및 imSitu(Yatskar 등, 2016) 데이터 세트의 이미지 세트에 대한 멀티홉 질문이 포함되어 있습니다. 이러한 질문은 템플릿에서 합성적으로 생성된 후 인간이 의역합니다. 달리 지정하지 않는 한 의역된 질문에 대한 결과를 제시합니다. NLVR2 데이터 세트(Suhr 2https://huggingface.co/sentence-transformers/all-mpnetbase-vModel Acc. Acc. GQA COVR NLVRAcc. Finetuned VisualBERT 57.67.Vin VL-Base 65.83.Zero-shot Few VLM 29.PnP-VQA 42.BLIP-v2* 44.| | | Few-shot Few VLM VisProg* ViperGPT* 35.50.5+ 48.Few-shot PnP-VQA CodeVQA(저희) 46.6 45.49.0 50.62.63.64.표 1: CodeVQA, Few-shot PnP-VQA, 이전 작업 VisualBERT의 GQA(테스트 개발), COVR(테스트), NLVR2(테스트 공개) 데이터 세트에 대한 결과 (Li et al., 2019), Vin VL-Base (Zhang et al., 2021), FewVLM (Jin et al., 2021), PnP-VQA (Tiong et al., 2022) 및 동시 작업(*로 표시) BLIP-v2 (Li et al., 2023), VisProg (Gupta and Kembhavi, 2023) 및 ViperGPT (Surís et al., 2023). 저희의 방법은 이전 작업의 모든 few-shot 방법보다 성능이 뛰어납니다. 각 전체 데이터 세트에 대한 가장 높은 few-shot 점수는 굵은 글씨로 표시되어 있습니다. †는 전체 테스트 세트의 결과와 직접 비교할 수 없는 테스트 세트의 계층화된 샘플에 대한 평가를 나타냅니다. et al., 2019)에는 이미지 쌍에 대한 진술이 포함되어 있으며, 각 진술이 참인지 거짓인지 판별하는 것이 과제입니다(저희는 평가하는 방법에 제공하기 전에 진술을 질문으로 다시 표현합니다). 부록 H에 데이터 세트에 대한 자세한 내용이 나와 있습니다. 세 가지 데이터 세트 각각에 대해 해당 훈련 세트에서 무작위로 샘플링한 50개 질문에 대한 프로그램을 작성했습니다. 달리 명시되지 않는 한, 단일 이미지 데이터 세트의 프롬프트에 12개의 문맥 내 예를 넣고 다중 이미지 데이터 세트의 프롬프트에 6개의 문맥 내 예를 넣었습니다(여러 이미지에 대한 캡션을 포함하면 각 예에 필요한 문맥 크기가 늘어남). 소문자로 된 답변의 정확한 일치 정확도를 보고합니다. 4.3 기준선 기본 기준선은 PnP-VQA(Tiong et al., 2022)를 few-shot 설정에 적용한 것입니다. 이를 &quot;Few-shot PnP-VQA&quot;라고 합니다. 이 기준선은 § 3.1에 설명된 5단계 쿼리 절차를 모든 질문에 대해 실행하는 것과 같습니다. 또한 이전 및 동시 작업의 zero-shot 및 few-shot 방법과 비교합니다. 부록 D에는 이러한 방법에 대한 자세한 내용이 나와 있습니다. 4.4 결과 표 1은 세 가지 데이터 세트에 대한 결과를 보여줍니다. CodeVQA는 fewshot 기술 중에서 가장 높은 정확도를 보입니다. Few-shot PnP-VQA와 비교했을 때 COVR에서 현저히 더 나은 성능을 보이는데, 이는 이 데이터 세트에서 기준 접근 방식은 단일 프롬프트가 주어졌을 때 여러 이미지에 대한 이미지 캡션의 정보를 결합해야 하기 때문입니다. 반면에, 저희 방법은 이미지를 반복하면서 한 번에 하나의 이미지를 쿼리하거나 질문과 가장 관련성이 높은 이미지를 선택합니다. 실제로 표 3은 CodeVQA가 5개 이상의 이미지가 포함된 인스턴스에서 가장 큰 이점을 제공한다는 것을 보여줍니다. 프로그램 합성을 사용하는 동시 작업과 비교했을 때, CodeVQA는 일반적으로 더 나은 성능을 보이는데, 이는 저희의 컨텍스트 내 예제 검색과 같은 방법론적 차이 또는 구현 세부 사항 때문일 수 있습니다. 그림 2는 COVR 데이터 세트에서 CodeVQA와 기준 Few-shot PnP-VQA의 정성적 비교를 보여줍니다. CodeVQA는 각 이미지에 대해 더 간단한 질문에 답하고 답을 비교하여 질문에 올바르게 답하는 반면, Few-shot PnP-VQA는 필요한 정보가 포함된 캡션을 생성했음에도 불구하고 틀리게 답합니다. 4.5 절제 표 2는 문맥 내 예제의 임베딩 기반 검색을 임의 검색과 비교합니다. 문맥 내 예제가 임베딩을 통해 검색될 때 CodeVQA가 Few-shot PnP-VQA보다 더 뛰어납니다. 임베딩 기반 검색은 Gupta와 Kembhavi(2023)에서처럼 단일 예제 세트를 큐레이팅하는 대신 관련 문맥 내 예제를 수집하는 체계적인 방법을 제공합니다. 부록 F에서는 질문-답변 LM과 프롬프트의 샷 수에 대한 절제와 검증 세트의 결과를 포함합니다. 표 4는 질문-답변 LM으로 code-davinci-002 또는 text-davinci-003을 사용할 때 CodeVQA가 Few-shot PnP-VQA보다 향상됨을 보여줍니다. 표 5는 문맥 내 예제의 수가 변함에 따라 대략 일정한 정확도를 보여줍니다. 4.6 분석 그림 3은 질문 유형별 정확도를 분석합니다. CodeVQA의 가장 큰 개선(약 30%)은 왼쪽/오른쪽 또는 위/아래 개체 위치에 대한 질문으로 구성된 하위 집합에 있습니다. &quot;and&quot; 및 &quot;or&quot; 질문도 개선되었습니다. 이 질문: 플랫폼 옆에 있는 기차와 눈 근처에 있는 기차가 같은 색인가요? 정답: 아니요 CodeVQA images = open_images(&quot;ImageSet7.jpg&quot;) platform_image = find_matching_image(images, &quot;train next to a platform&quot;) snow_image = find_matching_image(images, &quot;train near the snow&quot;) platform_train_color = query (platform_image, &quot;What color is the train?&quot;) snow_train_color = query (snow_image, &quot;What color is the train?&quot;) if platform_train_color == snow_train_color: answer = &quot;yes&quot; else: answer &quot;no&quot; 정답: 아니요 맥락: 소수점 PnP-VQA 이미지 1: 큰 노란색 역에 정차한 기차. 기차가 승강장에서 기차역으로 들어오고 있습니다. 노란색 기차가 기차 선착장에 주차되어 있습니다. 그림 2: 눈 덮인 길을 따라 움직이는 기관차. 기차 기차 빨간 상자 기차 기차 기차 기차 기차 위로 기차 화물 기차 빨간 기차 기차 빨간 기차. 빨간 신호등 옆에서 눈 덮인 도시를 달리는 기차. === 질문: 승강장 옆의 기차와 눈 근처의 기차가 같은 색이라는 것은 사실입니까? 답변: 정답: 예 그림 2: 정성적 비교. CodeVQA는 이 질문을 더 간단한 질문으로 나누고 각각 별도로 답한 다음 답을 비교하여 COVR의 이 질문에 올바르게 답합니다. 소수 샷 PnP-VQA는 캡션에 필요한 정보가 포함되어 있음에도 불구하고 틀렸습니다. (CodeVQA는 캡션도 생성하지만, 여기서는 표시되지 않습니다.) 검색 방법 Few-shot PnP-VQA CodeVQA text-davinci-Random 48.49.Embedding 49.52.code-davinci-Random Embedding 49.50.52.55.Accuracy표 2: 2000개의 GQA 검증 사례에 대한 예시 검색 기술 비교. 기울임체로 표시된 GPT 모델 이름은 질문-답변 LM으로 사용된 모델을 나타냅니다. 개선은 LM이 멀티홉 질문을 싱글홉 질문으로 변환하는 데 이점이 있다는 최근 발견과 관련이 있을 수 있습니다(Press et al., 2022).³ 인스턴스 수 소수 샷 PnP-VQA CodeVQA 이미지 수12 915 828 69691.7 51.5 48.3 47.0 46.7 5.0 53.3 48.7 53.2 53.표 3: COVR 검증 세트에서 인스턴스당 이미지 수에 따른 정확도. CodeVQA에서 CodeVQA가 틀리게 답한 COVR 검증 세트의 예제에 대한 CodeVQA의 오류 소스를 분석했습니다. 무관한 캡션(31%), find_matching_image의 실수(12%), 프로그램 생성 오류(14%), 질의응답 오류(25%), 예측된 답변을 옳다고 간주할 수 있음(14%), 기준 진실이 불분명/잘못됨(16%), 수치 오류(1%). 이러한 범주는 상호 배타적이지 않으며 100개 예제 중 13개가 여러 범주로 표시되었습니다. 따라서 프로그램 생성보다 모듈 실행으로 인해 오류가 더 많습니다. 3 이러한 종류의 질문에 대한 정확도는 LM을 개선하여 개선할 수도 있습니다. 예를 들어, QA에 대한 LM으로 text-davinci-를 사용하면 GQA의 &quot;and&quot; 질문에서 Few-shot PnP-VQA와 CodeVQA 간의 격차가 줄어듭니다. CodeVQA Few-shot PnP-VQA 공간 및 기타 그림 3: GQA 테스트 세트의 질문 유형별 정확도. CodeVQA(파란색)는 공간, and, or 질문에서 Few-shot PnP-VQA(주황색)보다 성능이 뛰어납니다. &quot;공간&quot;은 좌우 또는 위/아래 관계 또는 객체 위치에 초점을 맞춘 질문을 말합니다. 5
--- CONCLUSION ---
이 논문에서 우리는 모듈식 few-shot VQA를 위한 프레임워크를 소개했습니다. 우리의 접근 방식은 LM이 사전 학습된 시각적 모듈을 호출하고 이러한 모듈의 출력을 구성하여 답을 예측하는 Python 프로그램을 생성하도록 합니다. 이전의 모듈식 VQA 기술과 달리 이 프레임워크는 (재)학습 모듈이나 파서가 필요하지 않습니다. 또한 이전의 모듈식 접근 방식에서 해석 가능한 모듈 출력을 얻는 것은 사소한 일이 아니지만(Subramanian et al., 2020), 우리의 접근 방식에서는 모듈이 동결되어 해석 가능합니다. CodeVQA는 파이프라인 시스템(Zeng et al., 2022)을 코드의 전체 표현으로 확장하는 것으로 볼 수도 있습니다. 우리의 접근 방식은 경험적 이득을 보여주며, 모듈식 few-shot VQA에 대한 향후 작업을 촉진합니다. 6 제한 사항 초기 결과는 유망하지만, 우리 방법의 정확도는 인간 VQA 정확도 및 VQA 데이터 세트에서 미세 조정된 모델보다 여전히 낮아서 코드 합성을 사용한 few-shot VQA 방법이 실제 응용 프로그램에 유용하기 전에 상당한 진전이 필요할 수 있음을 시사합니다. 또한 부록 G의 결과에 따르면 그렇게 해도 항상 기준 방법보다 개선되는 것은 아니므로 프레임워크를 추가 기본 요소로 확장하기 위한 추가 작업이 필요합니다. 우리 접근 방식의 또 다른 제한 사항은 컴퓨팅 요구 사항이나 비용(예: 사용 가능한 API를 통해)으로 인해 사용이 제한될 수 있는 대규모 유능한 LMS에 의존한다는 것입니다. 또한 이 작업에서는 영어를 기본 언어로 하는 VQA 기능 벤치마킹에 중점을 두고 있으며 향후 작업에서는 다국어 LM을 통해 이를 다른 언어로 확장할 수 있습니다. 7 감사의 말 버클리 NLP 그룹 구성원(특히 Eric Wallace), Grace Luo 및 익명의 검토자에게 이 논문의 초기 초안에 대한 피드백에 감사드립니다. 우리는 CodeVQA와 Few-shot PnP-VQA를 개인 COVR 테스트 세트에서 평가하는 데 도움을 준 Ben Bogin과 Shivanshu Gupta에게 감사드립니다. SS, MN, TD는 NDSEG 펠로우십(SS용)과 DARPA의 LwLL, PTG, 및/또는 SemaFor 프로그램을 포함하여 DoD, NSF 및/또는 Berkeley Artificial Intelligence Research(BAIR) 산업 연합 프로그램에서 부분적으로 지원되었습니다. 참고문헌 Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, Miles Brundage. 2021. 클립 평가: 광범위한 역량과 다운스트림 영향의 특성화를 향해. Arxiv, abs/2108.02818. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman 및 Karen Simonyan. 2022. Flamingo: 퓨샷 학습을 위한 시각적 언어 모델. ArXiv, ABS/2204.14198. 제이콥 안드레아스, 마커스 로르바흐, 트레버 대럴, 댄 클라인. 2016. 질문 답변을 위한 신경망 구성 방법을 학습합니다. 2016년 북미 컴퓨터 언어학회 회의록: 인간 언어 기술, 1545-1554쪽, 캘리포니아주 샌디에이고. 컴퓨터 언어학회. 벤 보긴, 시반슈 굽타, 맷 가드너, 조나단 베란트. 2021. Covr: 실제 이미지를 사용한 시각적 기반 구성 일반화를 위한 테스트 베드. 자연어 처리의 경험적 방법에 대한 회의. 마크 첸, 제리 트워렉, 희우 준, 치밍 위안, 헨리크 폰데, 자렛 카플란, 해리슨 에드워즈, 유라 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필리프 틸렛, 펠리페 페트로스키 수치, 데이비드 W. 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 H. 거스, 알렉스 니콜, 이고르 바부슈킨, S. 아룬 발라지, 샨타누 자인, 앤드류 카, 얀 레이케, 조슈아 아키암, 베단트 미스라, 에반 모리카와, 알렉 Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 2021. 코드에서 학습된 대규모 언어 모델 평가. ArXiv, abs/2107.03374. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu. 2023. 기호 언어에서 언어 모델 바인딩. 제11회 학습 표현 국제 컨퍼런스에서. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh. 2017. vqa에서 v를 중요하게 만들기: 시각적 질의응답에서 이미지 이해의 역할 강화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6904-6913페이지. Tanmay Gupta와 Aniruddha Kembhavi. 2023. 시각적 프로그래밍: 훈련 없이 구성적 시각적 추론. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR). Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko. 2017. 추론 학습: 시각적 질의응답을 위한 종단 간 모듈 네트워크. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 804-813페이지. Drew A. Hudson과 Christopher D. Manning. 2019. Gqa: 실제 시각적 추론 및 구성적 질문 답변을 위한 새로운 데이터 세트.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 6693-6702쪽.우정 진, 유 청, 옘롱 센, 웨이주 첸, 샹 렌.2021.좋은 프롬프트는 수백만 개의 매개변수의 가치가 있다: 시각-언어 모델을 위한 저리소스 프롬프트 기반 학습.Arxiv, abs/2110.08484.란제이 크리슈나, 유케 주, 올리버 그로스, 저스틴 존슨, 켄지 하타, 조슈아 크래비츠, 스테파니 첸, 야니스 칼란티디스, 리자 리, 데이비드 A. 샤마, 마이클 S. 번스타인, 리 페이페이.2016.시각적 게놈: 크라우드소싱된 고밀도 이미지 주석을 사용하여 언어와 시각 연결.국제 컴퓨터 비전 저널, 123:32-73. Jayant Krishnamurthy와 Thomas Kollar. 2013. 공동으로 구문 분석하고 인식하는 법 배우기: 자연어를 물리적 세계에 연결하기. Association for Computational Linguistics의 거래, 1:193– 206. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. 2023. BLIP-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. ICML에서. Junnan Li, Dongxu Li, Caiming Xiong, Steven CH Hoi. 2022. Blip: 통합된 시각-언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. ICML에서. Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, Steven CH Hoi. 2021. 융합 전 정렬: 모멘텀 증류를 사용한 시각 및 언어 표현 학습. 신경 정보 처리 시스템에서. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh 및 Kai-Wei Chang. 2019. Visualbert: 시각과 언어를 위한 간단하고 성능이 뛰어난 기준선입니다. ArXiv, ABS/1908.03557. J. Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence 및 Andy Zeng. 2022. 정책으로서의 코드: 구체화된 제어를 위한 언어 모델 프로그램. ArXiv, ABS/2209.07753. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu 등. 2023. 공룡 접지: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합. arXiv 사전 인쇄본 arXiv:2303.05499. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi. 2019. Ok-vqa: 외부 지식이 필요한 시각적 질문 답변 벤치마크. 2019 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 3190-3199페이지. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J. Lowe. 2022. 인간의 피드백을 통해 지침을 따르도록 언어 모델 훈련. ArXiv, abs/2203.02155. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. 언어 모델에서 구성성 갭 측정 및 좁히기. ArXiv, abs/2210.03350. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. International Conference on Machine Learning, 8748-8763쪽. PMLR. Candace Ross, Boris Katz, and Andrei Barbu. 2020. 접지된 비전과 언어 임베딩에서 사회적 편향 측정. Association for Computational Linguistics의 북미 지부에서. Raeid Saqur와 Karthik Narasimhan. 2020. 시각적 질의응답에서 구성적 일반화를 위한 다중 모드 그래프 네트워크. Neural Information Processing Systems에서. Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra. 2016. Grad-cam: 그래디언트 기반 로컬라이제이션을 통한 딥 네트워크의 시각적 설명. International Journal of Computer Vision, 128:336359. Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner. 2020. 구성적 신경망에서 충실한 해석 얻기. Association for Computational Linguistics 연례 회의에서. Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, Anna Rohrbach. 2022. ReCLIP: 표현 이해를 참조하기 위한 강력한 제로샷 기준선. Association for Computational Linguistics(제1권: 장문 논문)의 제60회 연례 회의록, 5198-5215쪽, 더블린, 아일랜드. Association for Computational Linguistics. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi. 2019. 사진을 기반으로 한 자연어 추론을 위한 코퍼스. Association for Computational Linguistics(제57회 연례 회의록, 6418-6428쪽, 이탈리아 피렌체. Association for Computational Linguistics. Dídac Surís, Sachit Menon, Carl Vondrick. 2023. Vipergpt: 추론을 위한 파이썬 실행을 통한 시각적 추론. arXiv 사전 인쇄본 arXiv:2303.08128. Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven CH Hoi. 2022. 플러그 앤 플레이 vqa: 제로 샷 vqa by conjoining large pretrained models with zero training. ACL의 결과: EMNLP. Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, and Heng Ji. 2022. 이미지 설명자가 있는 언어 모델은 강력한 few-shot 비디오 언어 학습기입니다. ArXiv, abs/2205.10747. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2021. An empirical study of gpt-3 for few-shot knowledgebased vqa. AAAI Conference on Artificial Intelligence에서. Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. 2016. Situation awareness: Visual semantic role labeling for image understanding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 5534-5542쪽. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv 사전 인쇄 arXiv:2204.00598. &quot;&quot;&quot;각 이미지에 대한 질문에 답하는 Python 코드를 작성하세요. 11 11# 전역 상수 #min x 좌표 LEFT =#min y 좌표 BOTTOM = 0 #23 max x 좌표 RIGHT = 24 #23 max y 좌표 TOP = 24 from PIL import Image from utils import open_images, query, find_matching_image, get_posAPI 참조: -&gt; str open_image (path: str) -&gt; Image 경로에서 이미지를 열고 Image 객체로 반환합니다. query(img: Image, question: str) 이미지 쿼리 질문에 대한 답변을 반환합니다. get_pos(img: Image, object: str) -&gt;(float, float) 이미지에서 객체의 위치를 반환합니다.# 이미지 1: 벤치가 은색과 금속색으로 보입니까? img = open_image(&quot;Image1.jpg&quot;) is_silver = query(img, &quot;벤치가 은색과 금속색으로 보입니까?&quot;) is_metallic = query(img, &quot;벤치가 금속색으로 보입니까?&quot;) if is_silver == &quot;yes&quot; 및 is_metallic == &quot;yes&quot;: answer = &quot;yes&quot; else: Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022. Lit: 잠긴 이미지 텍스트 튜닝을 사용한 제로 샷 전송. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18123-18133페이지. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. Vinvl: 시각 언어 모델에서 시각적 표현 재검토. 2021 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 5575-5584페이지. 코드 생성 프롬프트 답변 = &quot;아니요&quot; A.1 GQA 프롬프트의 서문(회색)-지시문, 상수, 가져오기 명령문 및 API 설명서 포함-과 단일 문맥 내 예가 아래에 나와 있습니다(질문은 녹색, 프로그램은 강조 표시됨). 주요 GQA 실험에서 각 평가 예제에 12개의 문맥 내 예가 사용되었습니다. A.2 COVR 프롬프트의 서문(회색)-지시문, 상수, 가져오기 명령문 및 API 설명서 포함-과 단일 문맥 내 예(질문은 녹색, 프로그램은 강조 표시됨)가 아래에 나와 있습니다. COVR 실험에서 각 평가 예제에 6개의 문맥 내 예가 사용되었습니다. &quot;&quot;&quot;각 이미지에 대한 질문에 답하는 Python 코드를 작성하세요. # 전역 상수 11.#min x 좌표 LEFT =#min y 좌표 BOTTOM = 0 % max x 좌표 RIGHT = 24 #23 max y 좌표 TOP = 24 from PIL import Image from utils import open_images, query, find_matching_image, get_posAPI 참조: open_image (path: str) -&gt; List[Image] 주어진 디렉토리에 있는 이미지를 열고 Image 객체 목록으로 반환합니다. query(img: Image, question: str) -&gt; str 주어진 좌표에서 이미지의 영역을 쿼리하고 답을 반환합니다. find_matching_image(images: List[Image], text: str) Image는 텍스트와 가장 잘 일치하는 이미지를 반환합니다. get_pos(img: Image, object: str) -&gt; (float, float)는 이미지에서 객체의 위치를 반환합니다.# 이미지 세트 1: 검은색 셔츠를 입은 여성이 남성보다 많은 것은 사실일까요? images = open_images(&quot;ImageSet1.jpg&quot;) ladies_total =men_total =for image in images: ladies_exist = query(image, &quot;숙녀가 있나요?&quot;) if ladies_exist == &quot;yes&quot;: ladies_count = int (query(image, &quot;검은색 셔츠를 입은 숙녀가 몇 명이나 있나요?&quot;)) ladies_total += ladies_count man_exist = query(image, &quot;남자가 있나요?&quot;) if men_exist == &quot;yes&quot;: men_count = int(query(image, &quot;검은색 셔츠를 입은 남자가 몇 명이나 있나요?&quot;)) men_total += men_count if ladies_total &gt; men_total: else: answer = &quot;yes&quot; answer = &quot;no&quot; B GradCAM .... GradCAM에 대한 우리의 계산은 비전 변환기(Tiong et al., 2022; Li et al., 2021)를 사용하는 이전 작업을 따릅니다. 토큰 91, IT가 있는 질문과 KK 패치로 토큰화된 이미지가 제공됩니다. 우리는 Tiong et al. (2022)에 따라 L = 6 계층을 사용하여 GradCAM을 계산합니다. 우리는 다음과 같이 각 토큰에 대한 GradCAM 맵을 계산합니다. CЄ RTxK²를 계층 L의 교차 어텐션 맵이라고 합니다. G = RT×K²를 C에 대한 이미지-텍스트 매칭 점수의 그래디언트라고 합니다. 그러면 토큰 i에 대한 GradCAM 맵은 CORELU(G))의 i번째 행으로 주어집니다. 여기서 :D:D:D:D:D:D:D:_:_:_:_:_:_:_:_test 요소별 곱셈입니다. 섹션 3.1에서 설명한 대로 쿼리 기본형의 경우 모든 질문 토큰에서 평균 GradCAM 맵을 취하는 반면 get_pos 기본형의 경우 입력 텍스트 토큰(질문 토큰의 일부)에서 평균 GradCAM 맵을 취합니다. C 구현 세부 정보 각 데이터 세트에서 컨텍스트 내 예제에 대한 캡션을 생성하기 위해 컨텍스트 내 예제 데이터베이스의 각 질문에 대해 1~4단계를 실행합니다. GQA 실험의 경우 이미지당 C = 7개의 캡션을 사용하고, 각 질문이 여러 이미지와 연관된 COVR 실험의 경우 이미지당 C = 3개의 캡션을 사용합니다.4 NLVR2 데이터 세트의 경우 C = 캡션을 사용합니다. 보고된 각 정확도 결과는 해당 평가 세트에 대한 단일 평가 실행을 나타냅니다. NLVR 및 일부 COVR 인스턴스의 경우 텍스트 입력은 진술문(참/거짓으로 분류)입니다. 이러한 각 진술문에 &quot;그게 사실인가요?&quot;라는 접두사를 추가하고 답변을 &quot;예&quot;/&quot;아니요&quot;로 변환하여 질문으로 변환합니다. 질문 임베딩을 사용하여 GQA의 경우 12개의 예와 COVR 및 NLVR2의 경우 6개의 예를 선택합니다. D 기준선에 대한 세부 정보 Few VLM은 GQA의 경우 16개의 few-shot 예를 무작위로 샘플링합니다. VisProg는 프로그램 생성 및 실행 파이프라인을 5번 실행하고 각 반복에서 GQA의 경우 24개의 few-shot 예를 무작위로 샘플링하고 NLVR2의 경우 16개의 few-shot 예를 무작위로 샘플링합니다. ViperGPT는 GQA에 8개의 few-shot 예제를 사용합니다. VisProg는 코드 생성에 text-davinci-002 또는 text-davinci-003을 사용하는 반면(코드 릴리스에 따름), ViperGPT는 code-davinci-002를 사용합니다. E 정성적 비교 그림 5에서 방법 Code VQA와 기준 Few-shot PnPVQA(text-davinci-003)의 정성적 비교를 포함했습니다. 모든 인스턴스에서 PnP-VQA는 질문과 관련 없는 캡션을 생성하여 잘못된 답변을 초래하는 것을 볼 수 있습니다. 반면 CodeVQA는 질문을 Python 코드 블록으로 분해합니다. CodeVQA는 미리 정의된 시각적 모듈 get_pos(image, text)와 함께 if-else 조건을 사용합니다. 4 우리는 이 캡션 수를 샷 수와 davinci 모델의 컨텍스트 크기에 따라 가능한 최대값으로 선택했으며, 이를 예비 실험에서 질문-답변 LM으로 사용했습니다. 정확도 GQA COVR 모델 샷 Val 샘플 테스트 개발 샷 Val 샘플 Val 테스트 Few-shot PnP-VQA49.44.51.w/ text-davinci-CodeVQA(우리의 것)52.46.54.w/ text-davinci-Few-shot PnP-VQA52.46.49.47.8 45.w/ code-davinci-CodeVQA(우리의 것)55.49.54.52.9 50.w/ code-davinci-표 4: GQA 및 COVR에 대한 검증 및 테스트 결과. OpenAI 모델 이름(text-davinci-003 또는 code-davinci-002)은 질문-답변 모델로 사용된 모델을 나타냅니다. GQA 검증 샘플에는 GQA 검증 세트의 2000개 예가 포함되어 있습니다. COVR 검증 샘플에는 COVR 비의역 검증 세트의 1000개 예가 포함되어 있습니다. 가장 높은 점수는 굵은 글씨로 표시되어 있습니다.50 먼저 &quot;테이블 근처 소파 위의 베개&quot;와 &quot;침대 근처 소파 위의 베개&quot;라는 기준을 충족하는 이미지를 찾아 개별 발생 횟수를 세어야 합니다.CodeVQA Few-shot PnP-VQASpatial And Or Other 그림 4: 2000개 GQA 검증 사례에서 질문 유형별 정확도.CodeVQA(파란색)는 공간 및 또는 질문에서 Fewshot PnP-VQA(주황색)보다 성능이 뛰어납니다. &quot;공간&quot;은 좌우 또는 상하 관계나 객체 위치에 초점을 맞춘 질문을 말합니다.그리고 쿼리(이미지, 텍스트)를 사용하여 이미지의 오른쪽 영역에 초점을 맞춰 설명 가능한 방식으로 정답에 도달합니다.그림 6은 NLVR 데이터 세트에서 CodeVQA 방법이 질문에 올바르게 답하는 두 가지 예를 보여줍니다.첫 번째 예에서는 각 이미지에 대해 판다의 수를 쿼리하고 그에 따라 질문에 올바르게 답합니다. 두 번째 예에서, 우리 방법은 질문을 세 개의 더 간단한 질의와 if-else 문으로 나누어 정답에 도달합니다.그림 7은 COVR 데이터 세트의 복잡한 다중 참조 질문에 대한 우리 방법의 올바른 결과를 보여줍니다.CodeVQA는 흰색 접시에 케이크가 있는 이미지의 개수와 흰색 접시에 레몬이 있는 이미지의 개수를 얻기 위한 논리를 분해한 다음 두 개수가 같은지 평가합니다.두 번째 더 복잡한 예에서 우리 방법은 for 루프와 복잡한 if-else 논리 F를 사용합니다.추가 정량적 결과 표 4는 검증 세트에 대한 결과를 보여주고 code-davinci-and text-davinci-003을 질문-답변 LM으로 사용할 때 CodeVQA와 Fewshot PnP-VQA의 정확도를 비교합니다.표 5는 CodeVQA와 Few-shot PnP-VQA의 정확도가 프롬프트의 샷 수에 따라 어떻게 달라지는지 보여줍니다. 그림 4는 breakMethod text-davinci-Few-shot PnP-VQA CodeVQA code-davinci-Few-shot PnP-VQA CodeVQA Number of shots 1248.3 49.4 49.5 2.8 52.5 52.5 0.6 52.1 51.55.1 55.3 55를 보여줍니다.표 5: GQA 검증 사례에서 다양한 샷 수에 따른 정확도.초기 실험에 사용한 2000개 GQA 검증 사례에 대한 질문 유형별 정확도 저하(그림 3과 유사하지만 검증 사례).이 샘플에서 Few-shot PnP-VQA가 &quot;and&quot; 질문에서 유리한 것을 알 수 있습니다.질문: 안경을 낀 여성의 왼쪽에 항아리가 있습니까?오른쪽에 있습니까?참 답변: 왼쪽 호텔 캘리포니아 질문: 그림의 위쪽에 있는 여성이 있습니까? 정답: 아니요 질문: 캡션이 보이나요? &#39;앞치마를 두른 여성들이 주방에서 요리하는 모습&#39;, &#39;모자를 쓴 세 명의 여성과 빨간 모자&#39;, &#39;주방에서 요리를 준비하는 여성들&#39;, &#39;가정 주방에서 요리하는 법을 배우는 여학생들&#39;, ... 정답: 맞는 캡션: &#39;사람들이 건물 옆을 지나가고 몇몇 사람들이 돌아다니는 모습&#39;, &#39;밖에 사람들이 말을 걸고 있는 높은 간판&#39;, &#39;호텔 간판은 빨간색과 흰색 디자인&#39;, &#39;간판이 있는 높은 기둥 옆을 걷는 사람&#39; ... 정답: 네 캡션: &#39;테이블 위에 사과와 물 두 컵&#39;, &#39;사과, 녹색 테이블 접시에 놓인 유리 라임 접시&#39;, &#39;사과, 두 개의 컵, 종이 접시가 테이블에 놓여 있음&#39;, &#39;종이 접시 위에 큰 빨간 사과&#39;, &#39;사과가 두 개의 음식 접시 옆에 놓여 있음&#39;... 사과와 샌드위치 둘 다? 참 정답: 아니요 정답: 예 코드: CodeVQA(저희) img = open_image(&quot;Image13.jpg&quot;) woman_pos_x, woman_pos_y = get_pos (img, &quot;woman&quot;) pot_pos_x, pot_pos_y = get_pos(img, &quot;pot&quot;) if pot_pos_x &lt; woman_pos_x: answer &quot;left&quot; else: answer &quot;right&quot; 정답: 왼쪽 코드: LEFT, RIGHT, BOTTOM, TOP = 0, 24, 0,img = open_image(&quot;Image13.jpg&quot;) woman_pos_x, woman_pos_y = get_pos (img, &quot;woman&quot;) if woman_pos_y &lt; (BOTTOM+TOP)/2: else: 정답: 아니요 코드: img = answer = &quot;no&quot; answer = &quot;yes&quot; open_image(&quot;Image13.jpg&quot;) apple_exists = query(img, &quot;사과 보이시나요?&quot;) sandwich_exists = query(img, &quot;사과 보이시나요?&quot; 샌드위치를 봐요?&quot;) if apple_exists == &quot;yes&quot;: else: == &quot;yes&quot; and sandwich_exists answer = &quot;yes&quot; answer = &quot;no&quot; 정답: 아니요 그림 5: GQA 결과. GQA 데이터 세트에서 우리의 방법인 CodeVQA가 기준 Few-Shot PnP-VQA보다 성능이 뛰어난 예시 결과를 보여줍니다. G 추가 기본 요소를 사용한 실험 또한 객체 계산 또는 지식 검색과 관련된 데이터 세트에서 두 가지 다른 기본 요소를 사용하여 실험합니다. find_object(image, object_description) 이 함수는 주어진 설명과 일치하는 이미지의 객체에 대한 참조 집합을 반환하고, 객체를 계산하는 데 사용합니다. 이 함수는 참조 표현 이해에 대해서도 훈련된 오픈 어휘 객체 감지기인 Grounding DINO(Liu et al., 2023)를 사용하여 구현합니다. 우리는 이 기본 요소를 VQAv2 데이터 세트(Goyal et al., 2017)에서 평가하는데, 여기서는 이 기본 요소와 쿼리만 사용하고 COVR 및 NLVR2 데이터 세트도 사용합니다. 우리는 VQAv2 데이터 세트에 대해 12개의 컨텍스트 내 예제를 사용했습니다. 표 6은 이 모듈을 쿼리가 아닌 계산에 사용하면 혼합된 결과가 생성됨을 나타내는 결과를 보여줍니다. 정성적으로, 우리는 find_object 버전에서 오류에 대한 몇 가지 이유를 관찰합니다. 첫째, 객체 감지기가 항상 정확하지는 않습니다(예: 사람은 있지만 바나나가 없는 경우 &quot;바나나를 든 사람&quot;을 찾는 경우). 둘째, 우리 프로그램은 질문에서 주요 세부 정보를 생략할 수 있습니다(예: &quot;사람이 탄 보트는 몇 척입니까?&quot;의 경우 프로그램은 보트의 수를 전체적으로 센다). 셋째, 우리 프로그램은 질문에 적합하지 않을 때 감지기를 호출할 수 있습니다(예: &quot;소화전을 둘러싼 풀잎은 몇 개입니까?&quot;). 반면에 캡션은 개체 수가 적을 때 개체 수를 전달하는 경우가 많은데, 이는 이러한 데이터 세트에서 매우 일반적이므로 쿼리는 계산에 효과적일 수 있습니다. knowledge_query(question) 이 함수는 세계 지식(예: &quot;어느 축구팀이 슈퍼볼에서 가장 많이 우승했나요?&quot;)에 대한 답변을 반환합니다. 이 함수는 쿼리에 사용된 것과 동일한 LM을 사용하여 구현합니다. OK-VQA 데이터 세트의 형식과 더 잘 일치하도록 다음 토큰의 로짓에 큰 음의 편향을 추가하여 LM이 질문: 판다가 네 마리라는 것은 사실일까요? 코드: panda_count =for image in images: panda_count += int(query(image, &quot;판다가 몇 마리나 있나요?&quot;)) if panda_count == 4: answer = &quot;yes&quot; else: answer = &quot;no&quot; 답변: 예 cp*group 질문: 왼쪽 이미지에 가로로 세 개의 노트북이 있고 열린 노트북 행과 닫힌 노트북 행이 포함되어 있는 것은 사실일까요? 코드: images = open images (&quot;ImageSet7.jpg&quot;) rows_of_three = open_laptops = query(images[0], &quot;가로로 세 개의 노트북이 영어: three?&quot;) query(images[0], &quot;열린 노트북의 행이 있습니까?&quot;) == &quot;yes&quot; 닫힌 노트북 query(images[0], &quot;닫힌 노트북의 행이 있습니까?&quot;) if rows_of_three and open_laptops and closed_laptops: = answer = &quot;yes&quot; == &quot;yes&quot; == else: answer = &quot;no&quot; Answer: No &quot;yes&quot; 그림 6: NLVR2 결과. 방법 CodeVQA의 NLVR-2 데이터 세트에서 얻은 결과 예를 보여줍니다. 생성한 결과: 하이픈, &quot;to&quot; 및 °. 이 선택은 OK-VQA 데이터 세트에 대한 예비 실험을 기반으로 이루어졌습니다. 이 기본 요소를 OK-VQA 데이터 세트(Marino et al., 2019)에서 평가하며, 이 데이터 세트에서는 이 기본 요소와 쿼리만 사용합니다. CodeVQA와 Few-shot VQA의 경우 ViperGPT의 OK-VQA 결과와 일치하도록 7개의 컨텍스트 내 예제를 사용했습니다(Surís et al., 2023). 표 7은 결과를 제공하며, 시각 정보와 일반 지식이 모두 포함된 질문의 경우 이런 방식으로 질문을 분류해도 정확도가 향상되지 않는다는 것을 보여줍니다. VQAv2와 OK-VQA 모두 각 질문에 대한 정답 세트를 고려하는 VQAvdataset과 관련된 표준 평가 방법을 사용합니다. 두 데이터 세트에 대해 보고하는 Flamingo(Alayrac et al., 2022) 결과는 32개의 컨텍스트 내 예제를 사용했습니다. H 라이선스 및 기타 데이터 세트 세부 정보 GQA는 CC-BY-4.0 라이선스(https://creativecommons.org/licenses/by/4.0/)에 따라 라이선스가 부여됩니다. 저장소는 MIT 라이선스에 따라 라이선스가 부여된 COVR(https://github.com/benbogin/covr-dataset)입니다(단, imSitu 이미지는 라이선스가 부여되지 않을 수 있음). 두 데이터 세트의 텍스트는 영어로 작성되었습니다. NLVR의 주석은 CC-BY-4.0 라이선스에 따라 라이선스가 부여되지만 데이터 세트의 이미지는 라이선스가 부여되지 않았습니다. ODESNIK OLYMPUS의 주석 질문: 치마를 입고 라켓을 든 소녀의 사진은 얼마나 될까요? 코드: images = count =open images (&quot;ImageSet7.jpg&quot;) for image in images: girl_exists = query(image, &quot;치마를 입은 소녀가 있나요?&quot;) if girl_exists == &quot;yes&quot;: holding_racket = query(image, &quot;라켓을 든 소녀가 있나요?&quot;) if holding_racket == &quot;yes&quot;: count +=answer = count 답변: 질문: 흰 접시에 케이크가 있는 이미지의 수와 흰 접시에 레몬이 있는 이미지의 수가 같을까요? 코드: images = open_images (&quot;ImageSet7.jpg&quot;) cake_count =lemon_count =for image in images: cake_exists = query(image, &quot;Is there a cake on a white plate?&quot;) lemon_exists = query(image, &quot;Is there a lemon on a white plate?&quot;) if cake_exists == &quot;yes&quot;: cake_count +=if lemon_exists === &quot;yes&quot;: lemon_count +=if cake_count == lemon_count: else: answer = &quot;yes&quot; answer = &quot;no&quot; 정답: 아니요 그림 7: COVR 결과. COVR 데이터 세트에 대한 결과를 보여주는데, 우리 방법은 모든 이미지를 참조하여 질문에 올바르게 답합니다. VQAV2 COVR NLVRZero-shot BLIP-v65.Few-shot Flamingo 67.6* Few-shot PnP-VQA 66.47.CodeVQA 52.CodeVQA 66.52.w/ find_object 63.64.66.표 6: VQAv2(검증 세트에서 추출한 4000개 예제의 임의 샘플), COVR(검증), NLVR2(테스트-공개)에서 객체를 계산하는 데 find_object를 사용한 결과. *는 전체 VQAv2 테스트-개발 세트의 결과를 나타내며, 검증 세트 샘플에서 얻은 결과와 직접 비교할 수 없습니다. OK-VQA 57.제로샷 BLIP-v45.퓨샷 플라밍고 바이퍼GPT 51.퓨샷 PnP-VQA 54.코드VQA 53.w/ knowledge_query 표 7: OK-VQA 검증 세트에서 knowledge_query를 사용한 결과. I 윤리 및 영향 진술 우리 작업의 한 가지 목표는 VQA 시스템의 (재)교육 필요성을 줄이는 것입니다. 이 목표를 달성하면 교육 모델에서 탄소 배출량이 감소합니다. 그러나 대규모 언어 모델을 사용하므로 우리의 접근 방식은 추론 비용도 높습니다. 우리 접근 방식을 채택하기로 결정하려면 이러한 계산 비용과 관련된 환경 영향을 고려해야 합니다. 우리 접근 방식의 또 다른 잠재적인 긍정적 영향은 생성된 프로그램을 통한 향상된 해석 가능성입니다. 이러한 프로그램은 Python에 익숙한 사람들에게 시스템이 주어진 질문에 대해 어떤 시각적 작업을 사용하는지와 시스템이 이러한 작업의 출력을 결합하여 답을 예측하는 방법에 대한 기록을 제공합니다. 우리 시스템은 시각적 질문에 대한 답을 예측하기 위해 사전 교육된 시각 언어 모델에 의존합니다. 이전 연구(Ross et al., 2020; Agarwal et al., 2021)에서는 이미지 캡션으로 학습된 시각 언어 모델에서 사회적 편향의 증거를 발견했습니다. 따라서 저희 시스템에서도 이러한 편향이 나타날 수 있습니다. 실무자는 이러한 위험을 인식해야 하며 이상적으로는 인간의 삶에 영향을 미칠 수 있는 방식으로 이 시스템을 배포하는 것을 고려할 때 이러한 위험을 완화하기 위한 조치를 취해야 합니다. VQAV2는 CC-BY-4.0에 따라 라이선스가 부여됩니다. GQA의 testdev 세트에는 12578개의 인스턴스가 포함되어 있습니다. COVR의 test 세트에는 7024개의 인스턴스가 포함되어 있습니다. COVR의 validation 세트에는 6891개의 인스턴스가 포함되어 있습니다. NLVR2의 public test 세트에는 6967개의 인스턴스가 포함되어 있습니다. OK-VQA의 validation 세트에는 5046개의 인스턴스가 포함되어 있습니다. VQAv2의 경우, validation 세트에서 무작위로 추출한 4000개의 예제를 평가합니다. 본 방법을 개발하고 중간 평가하는 동안, GQA에서 무작위로 추출한 200개의 학습 예제와 무작위로 추출한 2000개의 검증 예제, COVR에서 무작위로 추출한 200개의 학습 예제와 검증 세트, NLVR2에서 무작위로 추출한 2000개의 학습 예제, OK-VQA에서 무작위로 추출한 1200개의 학습 예제, VQAV2에서 무작위로 추출한 2200개의 학습 예제를 사용하여 평가했습니다.
