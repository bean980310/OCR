--- ABSTRACT ---
언어 모델은 광범위한 작업에 걸쳐 일반적인 문제 해결을 위해 점점 더 많이 사용되고 있지만 추론 중 토큰 수준의 왼쪽에서 오른쪽으로 의사 결정 프로세스에 여전히 국한되어 있습니다. 즉, 탐색, 전략적 예측 또는 초기 의사 결정이 중요한 역할을 하는 작업에서는 부족할 수 있습니다. 이러한 과제를 극복하기 위해, 우리는 언어 모델 추론을 위한 새로운 프레임워크인 &quot;생각의 나무&quot;(ToT)를 소개합니다. 이는 언어 모델을 촉구하는 인기 있는 &quot;생각의 사슬&quot; 접근 방식을 일반화하고, 문제 해결을 위한 중간 단계로 작용하는 일관된 텍스트 단위(&quot;생각&quot;)를 탐색할 수 있게 합니다. ToT를 사용하면 LM이 여러 가지 추론 경로와 자체 평가 선택을 고려하여 의도적인 의사 결정을 수행하여 다음 행동 방향을 결정하고, 필요한 경우 앞을 내다보거나 후퇴하여 글로벌 선택을 할 수 있습니다. 실험 결과, ToT는 비사소한 계획이나 검색이 필요한 세 가지 새로운 과제, 즉 24게임, 창의적 글쓰기, 미니 크로스워드에서 언어 모델의 문제 해결 능력을 크게 향상시킵니다. 예를 들어, 24게임에서 생각의 사슬을 촉구하는 GPT-4는 과제의 4%만 해결했지만, 우리의 방법은 74%의 성공률을 달성했습니다. 모든 프롬프트가 있는 코드 리포: https://github.com/princeton-nlp/tree-of-thought-11m. 1
--- INTRODUCTION ---
원래 텍스트를 생성하기 위해 설계된 GPT [25, 26, 1, 23] 및 PaLM [5]과 같은 확장된 버전의 언어 모델(LMS)은 수학적, 기호적, 상식적, 지식적 추론이 필요한 점점 더 광범위한 작업을 수행할 수 있는 것으로 나타났습니다. 이 모든 진전의 근간이 여전히 토큰 수준의 결정을 하나씩 그리고 왼쪽에서 오른쪽으로 내리는 원래의 텍스트 생성 자기 회귀 메커니즘이라는 것은 아마도 놀랍습니다. 이렇게 간단한 메커니즘으로 LM을 일반적인 문제 해결자로 구축하기에 충분할까요? 그렇지 않다면 어떤 문제가 현재 패러다임에 도전할까요? 그리고 대안적인 메커니즘은 무엇일까요? 인간 인지에 대한 문헌은 이러한 질문에 답할 수 있는 몇 가지 단서를 제공합니다. &quot;이중 프로세스&quot; 모델에 대한 연구에 따르면 사람들은 결정을 내리는 데 두 가지 모드, 즉 빠르고 자동적이며 무의식적인 모드(&quot;시스템 1&quot;)와 느리고 의도적이며 의식적인 모드(&quot;시스템 2&quot;)를 가지고 있는 것으로 나타났습니다[30, 31, 16, 15]. 이 두 모드는 이전에 머신 러닝에 사용되는 다양한 수학적 모델과 연결되었습니다. 예를 들어, 인간과 다른 동물의 강화 학습에 대한 연구에서는 연관적 &quot;모델 없는&quot; 학습이나 보다 신중한 &quot;모델 기반&quot; 계획에 참여하는 상황을 탐구했습니다[7]. LM의 간단한 연관 토큰 수준 선택은 또한 &quot;시스템 1&quot;을 연상시키며, 따라서 (1) 현재 37차 신경 정보 처리 시스템 컨퍼런스(NeurIPS 2023)에 대한 다양한 대안을 유지하고 탐색하는 보다 신중한 &quot;시스템 2&quot; 계획 프로세스로 증강하는 것이 유익할 수 있습니다. 입력 입력 입력 입력 생각 ...... ...... -----출력 출력 다수결 투표 출력 (a) 입력-출력 (c) 생각의 사슬 촉구 (10) 촉구(COT) (c) COT와의 자기 일관성(COT-SC) 출력 (d) 생각의 나무(ToT) 그림 1: LLM을 사용한 문제 해결에 대한 다양한 접근 방식을 설명하는 개략도. 각 직사각형 상자는 생각을 나타내며, 이는 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다. 그림 2,4,6에서 생각이 생성, 평가 및 검색되는 구체적인 예를 참조하세요. 하나를 선택하는 대신 선택 사항을 유지하고 (2) 현재 상태를 평가하고 영어: 적극적으로 앞을 내다보거나 후퇴하여 더욱 글로벌한 결정을 내립니다. 이러한 계획 프로세스를 설계하기 위해 우리는 인공 지능(및 인지 과학)의 기원으로 돌아가 Newell, Shaw, Simon이 1950년대에 탐구한 계획 프로세스에서 영감을 얻었습니다[21, 22]. Newell과 동료들은 문제 해결[21]을 나무로 표현된 조합 문제 공간을 검색하는 것으로 특징지었습니다. 따라서 우리는 언어 모델을 사용하여 일반적인 문제 해결을 위한 생각의 나무(TOT) 프레임워크를 제안합니다. 그림 1에서 알 수 있듯이 기존 방법(아래에 자세히 설명)은 문제 해결을 위해 연속적인 언어 시퀀스를 샘플링하는 반면, ToT는 생각의 나무를 적극적으로 유지 관리합니다. 여기서 각 생각은 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(표 1). 이러한 고수준 의미 단위를 통해 LM은 언어로 인스턴스화된 의도적인 추론 프로세스를 통해 문제를 해결하기 위해 다양한 중간 생각이 이루는 진행 상황을 자체 평가할 수 있습니다(그림 2,4,6). LM 자체 평가 및 심의를 통한 이러한 검색 휴리스틱 구현은 이전 검색 휴리스틱이 프로그래밍되거나 학습되었기 때문에 참신합니다. 마지막으로, 우리는 다양한 사고를 생성하고 평가하는 이 언어 기반 기능을 너비 우선 탐색(BFS) 또는 깊이 우선 탐색(DFS)과 같은 검색 알고리즘과 결합하여 룩어헤드 및 백트래킹을 사용하여 사고의 나무를 체계적으로 탐색할 수 있습니다. 경험적으로, 우리는 최첨단 언어 모델인 GPT-4[23]를 사용하더라도 기존 LM 추론 방법에 도전하는 세 가지 새로운 문제를 제안합니다. 24게임, 창의적 글쓰기 및 크로스워드(표 1). 이러한 과제에는 연역적, 수학적, 상식적, 어휘적 추론 능력과 체계적인 계획 또는 검색을 통합하는 방법이 필요합니다. 우리는 ToT가 다양한 수준의 사고, 사고를 생성하고 평가하는 다양한 방법 및 다양한 문제의 특성에 적응하는 다양한 검색 알고리즘을 지원할 만큼 일반적이고 유연하여 세 가지 과제 모두에서 우수한 결과를 얻는다는 것을 보여줍니다. 또한 이러한 선택이 체계적 절제를 통해 모델 성능에 어떤 영향을 미치는지 분석하고 LM을 보다 잘 훈련하고 사용하기 위한 향후 방향에 대해 논의합니다.2 배경 먼저 문제 해결을 위해 대규모 언어 모델을 사용하는 기존 방법을 공식화합니다.이는 저희의 접근 방식이 영감을 받았고 나중에 비교되었습니다.po는 매개변수 0을 갖는 사전 훈련된 LM을 나타내고 소문자 x, y, z, s, …..는 언어 시퀀스 iex = (x[1],, x[n])를 나타냅니다.여기서 각 x[i]는 토큰이므로 po(x) = []=1 Po(x[i]|x[1...i]).대문자 S는 언어 시퀀스 모음을 나타냅니다.입출력(IO) 프롬프팅은 LM을 사용하여 문제 입력 x를 출력 y로 바꾸는 가장 일반적인 방법입니다.y ~ Pe(y prompt 10(x)), 여기서 prompt 10(x)는 입력 x를 작업 지침 및/또는 소수 샷 입출력 예제로 래핑합니다. 단순화를 위해 Pe(출력 | 입력) = Pe(출력 프롬프트(입력))로 표시하겠습니다.그러면 IO 프롬핑을 y ~ ·P(y|x)로 공식화할 수 있습니다.프롬프트 IOCOT 생각의 사슬(CoT) 프롬핑[38]은 입력 x에서 출력 y로의 매핑이 사소하지 않은 경우(예: x가 수학 질문이고 y가 최종 숫자 답변인 경우)를 해결하기 위해 제안되었습니다.핵심 아이디어는 x와 y를 연결하기 위해 생각의 사슬 z₁, …, zn을 도입하는 것입니다.여기서 각 z¿는 문제 해결을 향한 의미 있는 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(예: zi는 수학 QA에 대한 중간 방정식이 될 수 있음).CoT로 문제를 풀기 위해 각 생각 ZipoT(zi | x, 21-1)을 순차적으로 샘플링한 다음 출력 y ~ CoT(y|x, z1...n)를 수행합니다. 실제로, [z1...n, Y] (z1...n, y|x)는 연속적인 언어 시퀀스로 샘플링되고, 사고의 분해(예: 각 z¿가 구, 문장 또는 문단인가)는 모호한 상태로 남습니다.CoT를 통한 자기 일관성(CoT-SC) [36]은 k iid 사고 사슬을 샘플링하는 앙상블 방식입니다. [z\...n, y(i)] CoT (z1...n, y|x) (i = 1... k), 그런 다음 가장 빈번한 출력을 반환합니다. ~ COT Ро = ~ arg maxy #{iy (i) y}. CoT-SC는 일반적으로 동일한 문제에 대해 서로 다른 사고 과정(예: 동일한 정리를 증명하는 서로 다른 방법)이 있고, 더 풍부한 사고 집합을 탐색함으로써 출력 결정이 더 충실할 수 있기 때문에 CoT보다 개선되었습니다. 그러나 각 체인 내에서는 다양한 사고 단계에 대한 로컬 탐색이 없으며, &quot;가장 빈번한&quot; 휴리스틱은 출력 공간이 제한될 때(예: 객관식 QA)에만 적용됩니다.3 사고의 나무: LM을 이용한 의도적인 문제 해결 진정한 문제 해결 프로세스에는 탐색을 시작하기 위해 사용 가능한 정보를 반복적으로 사용하여야 하며, 이를 통해 더 많은 정보가 공개되고, 마침내 해결책을 얻는 방법이 발견됩니다.- Newell et al. [21] 인간의 문제 해결에 대한 연구에 따르면 사람들은 조합적 문제 공간(노드가 부분적 해결책을 나타내고 브랜치가 이를 수정하는 연산자에 해당하는 트리)을 검색한다고 합니다[21, 22]. 어떤 브랜치를 선택할지는 문제 공간을 탐색하고 문제 해결자가 해결책을 찾을 수 있도록 안내하는 휴리스틱에 의해 결정됩니다. 이 관점은 일반적인 문제를 해결하기 위해 LM을 사용하는 기존 접근 방식의 두 가지 주요 단점을 강조합니다.1) 로컬하게는 사고 과정 내의 다양한 연속성, 즉 트리의 브랜치를 탐색하지 않습니다. 2) 전 세계적으로, 그들은 이러한 다양한 옵션을 평가하는 데 도움이 되는 어떤 유형의 계획, 룩어헤드 또는 백트래킹도 통합하지 않습니다.인간의 문제 해결의 특징인 것으로 보이는 일종의 휴리스틱 가이드 검색입니다.이러한 단점을 해결하기 위해 LM이 생각에 대한 여러 추론 경로를 탐색할 수 있도록 하는 패러다임인 Tree of Thoughts(ToT)를 소개합니다(그림 1(c)).ToT는 모든 문제를 트리에 대한 검색으로 구성하며, 각 노드는 입력과 지금까지의 생각의 순서를 갖는 부분 솔루션을 나타내는 상태 s = [x, z1...]입니다.ToT의 구체적인 인스턴스화에는 네 가지 질문에 답하는 것이 포함됩니다.1. 중간 프로세스를 생각 단계로 분해하는 방법;2. 각 상태에서 잠재적인 생각을 생성하는 방법;3. 상태를 휴리스틱하게 평가하는 방법;4. 사용할 검색 알고리즘.1. 생각 분해.CoT가 명시적인 분해 없이 생각을 일관되게 샘플링하는 반면, ToT는 문제 속성을 활용하여 중간 생각 단계를 설계하고 분해합니다. 표에서 보듯이, 다양한 문제에 따라 생각은 몇 개의 단어(크로스워드), 한 줄의 방정식(24게임) 또는 한 단락의 쓰기 계획(창의적 작문)이 될 수 있습니다. 일반적으로 생각은 LM이 유망하고 다양한 샘플을 생성할 수 있을 만큼 &quot;작아야&quot; 하지만(예: 책 전체를 생성하는 것은 일반적으로 너무 &quot;크기&quot; 때문에 일관성이 없음), LM이 문제 해결에 대한 전망을 평가할 수 있을 만큼 &quot;크어야&quot; 합니다(예: 토큰 하나를 생성하는 것은 일반적으로 너무 &quot;작기&quot; 때문에 평가할 수 없음). 2. 생각 생성기 G(po, s, k). 트리 상태 s = [x, z1...]이 주어지면 다음 생각 단계에 대한 k 후보를 생성하는 두 가지 전략을 고려합니다. COT (a) CoT 프롬프트에서 iid 생각 샘플(창의적 작문, 그림 4): 2(j) CoT (zi+1|s) = pco (zi+1|x, z1...i) (j = 1 ……. k). 이 방법은 생각 공간이 풍부할 때(예: 각 생각이 문단일 때) 더 효과적이며, iid 샘플은 다양성으로 이어진다.(b) &quot;제안 프롬프트&quot;(24개 게임, 그림 2; 크로스워드, 그림 6)를 사용하여 생각을 순차적으로 제안한다: [2(1),..., 2(k)] ~ ppr suggests (1... k) (2+1초). 이 방법은 생각 공간이 더 제한적일 때(예: 각 생각이 단어나 줄일 때) 더 효과적이므로 동일한 맥락에서 다른 생각을 제안하면 중복을 피할 수 있다.3. 상태 평가자 V(pe, S). 다양한 상태의 경계가 주어지면 상태 평가자는 문제를 해결하기 위한 진행 상황을 평가하여 탐색 알고리즘이 어떤 상태를 어떤 순서로 계속 탐색할지 결정하는 휴리스틱 역할을 한다.휴리스틱은 탐색 문제를 해결하기 위한 표준적인 접근 방식이지만 일반적으로 프로그래밍(예: DeepBlue [3])되거나 학습(예: AlphaGo [29])된다. 우리는 LM을 사용하여 상태에 대해 의도적으로 추론하는 세 번째 대안을 제안합니다. 해당되는 경우 이러한 의도적인 휴리스틱은 프로그래밍된 규칙보다 더 유연할 수 있으며 학습된 모델보다 샘플 효율성이 더 높을 수 있습니다. 사고 생성기와 유사하게 우리는 상태를 독립적으로 또는 함께 평가하는 두 가지 전략을 고려합니다. (a) 각 상태를 독립적으로 평가합니다. V(po, s)(s) ~ pvalue (vs) Vs ES, 여기서 값 프롬프트는 상태 s에 대해 추론하여 스칼라 값 v(예: 1-10) 또는 휴리스틱하게 값으로 변환될 수 있는 분류(예: 확실/가능성/불가능)를 생성합니다. 이러한 평가적 추론의 기초는 문제와 사고 단계에 따라 다를 수 있습니다. 이 작업에서 우리는 몇 가지 룩어헤드 시뮬레이션을 통한 평가를 탐구합니다(예: 5, 5, 14가 5 + 5 + 14를 통해 24에 도달할 수 있음을 빠르게 확인하거나 &quot;hot_l&quot;이 &quot;_&quot;에 &quot;e&quot;를 채워 &quot;inn&quot;을 의미할 수 있음을 확인합니다). 또한 상식(예: 123은 24에 도달하기에는 너무 작음 또는 어떤 단어도 &quot;tzxc&quot;로 시작할 수 없음)을 고려합니다. 전자가 &quot;좋은&quot; 상태를 촉진할 수 있는 반면, 후자는 &quot;나쁜&quot; 상태를 제거하는 데 도움이 될 수 있습니다. 이러한 평가는 완벽할 필요가 없으며 의사 결정에 대략적으로 도움이 되면 됩니다. (b) 상태 간 투표: V(pe, S)(s) = 1[s = s*], 여기서 &quot;좋은&quot; 상태 s* 투표(s*|S)는 투표 프롬프트에서 S의 다른 상태를 의도적으로 비교하여 ~ 투표에서 제외됩니다. 문제 성공 여부를 직접 평가하기 어려운 경우(예: 구절 일관성) 대신 다른 부분 솔루션을 비교하고 가장 유망한 솔루션에 투표하는 것이 자연스럽습니다. 이는 다음과 유사한 정신입니다. 영어: &quot;단계별&quot; 자기 일관성 전략, 즉 &quot;탐색할 상태&quot;를 다중 선택 QA로 캐스팅하고 LM 샘플을 사용하여 이에 투표합니다. 두 전략 모두 LM에 여러 번 요청하여 값이나 투표 결과를 집계하여 시간/자원/비용을 보다 충실하고 견고한 휴리스틱으로 교환할 수 있습니다. 알고리즘 1 ToT-BFS(x, po, G, k, V,T,b) 필요 사항: 입력 x, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 너비 제한 b. 따라서 {x} = t = 1,, T에 대해 S←{[s, z] | s Є St−1, Zt Є G(pe, s, k)} Vt ← V (po, St) Starg maxscs₁₁|S|=bses Vt(s) end for SES return G(pe, arg maxSEST VT(S), 1) 알고리즘 2 ToT-DFS(s, t, po, G, k, V, T, Uth) 필요: 현재 상태 s, 단계 t, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 임계값 th if t T then 기록 출력 G(pe, s, 1) end if for s&#39;ЄG(pe, s, k) do ▷ 정렬된 후보 if V (pe, {s&#39;})(s) &gt; Vthres then ▷ 가지치기 DFS (s&#39;, t + 1) end if end for 4. 검색 알고리즘. 마지막으로, ToT 프레임워크 내에서 트리 구조에 따라 다양한 검색 알고리즘을 플러그 앤 플레이 방식으로 사용할 수 있습니다. 비교적 간단한 두 가지 검색 알고리즘을 살펴보고, 더 진보된 알고리즘(예: A* [11], MCTS [2])은 추후 작업을 위해 남겨둡니다. (a) 너비 우선 탐색(BFS)(알고리즘 1)은 단계당 가장 유망한 b 상태 집합을 유지합니다. 이것은 트리 깊이가 한계(T3)이고 초기 사고 단계를 평가하여 작은 집합으로 정리할 수 있는(b ≤ 5) 24게임과 창의적 글쓰기에 사용됩니다. (b) 깊이 우선 탐색(DFS)(알고리즘 2)은 가장 유망한 상태를 먼저 탐색하여 최종 출력에 도달할 때까지(t &gt; T) 또는 상태 평가자가 현재 s에서 문제를 해결하는 것이 불가능하다고 판단할 때까지(V(po, {s})(s) ≤ vth, 값 임계값 Vth의 경우) 진행합니다. 후자의 경우, s의 서브 트리를 정리하여 탐색을 활용으로 교환합니다. 두 경우 모두 DFS는 탐색을 계속하기 위해 s의 부모 상태로 후퇴합니다.개념적으로 ToT는 LM을 사용한 일반적인 문제 해결 방법으로서 여러 가지 이점이 있습니다.(1) 일반성.IO, COT, CoT-SC 및 자체 정제는 ToT의 특수한 경우로 볼 수 있습니다(즉, 깊이와 폭이 제한된 트리, 그림 1).(2) 모듈성.기본 LM과 사고 분해, 생성, 평가 및 검색 절차는 모두 독립적으로 다양할 수 있습니다.(3) 적응성.다른 문제 속성, LM 기능 및 리소스 제약 조건을 수용할 수 있습니다.(4) 편의성.추가 교육이 필요하지 않으며 사전 교육된 LM만으로 충분합니다.다음 섹션에서는 이러한 개념적 이점이 어떻게 다양한 문제에서 강력한 경험적 성능으로 변환되는지 보여줍니다.4 실험 표준 IO 프롬프팅 또는 사고 사슬(CoT) 프롬프팅을 사용하여 최첨단 언어 모델인 GPT-4[23]에서 샘플링하더라도 어려운 세 가지 작업을 제안합니다. 우리는 입력 출력 생각 게임 4개 숫자(49 10 13) 도달하는 방정식(13-9)*(10-4)=3 중간 방정식(13-9=4(왼쪽 4,4,10); 104-6(왼쪽 4,6); 4*6=24) #ToT 단계창의적 글쓰기 4개의 무작위 문장 4개 문장으로 끝나는 4개 단락의 구절 짧은 글쓰기 계획(1. 연결하는 책 소개...)5x5 크로스워드 10개 단서(h1. 제시됨;..) 5x5 글자: 표시됨; WIRRA; 사용 가능; ... 단서를 채울 단어: (h1. 표시됨; v5. 사용 가능; ...) 5-10(변수) 표 1: 작업 개요. 입력, 출력, 생각 예는 파란색으로 표시되었습니다. 생각의 나무(ToT)에서 의도적으로 검색하면 더 나은 결과가 나오고, 더 중요한 것은 언어 모델을 사용하여 검색이나 계획이 필요한 문제를 해결하는 흥미롭고 유망한 새로운 방법이 생깁니다. 달리 명시하지 않는 한, 샘플링 온도가 0.7인 Chat Completion 모드 GPT-4¹를 사용하여 실험을 수행합니다. 4.1 Game ofGame of 24는 수학적 추론 과제로, 목표는 4개의 숫자와 기본적인 산술 연산(+-*/)을 사용하여 24를 얻는 것입니다. 예를 들어, 입력 값 &quot;4 9 10 13&quot;이 주어졌을 때, 솔루션 출력은 &quot;(104) * (13 -9) = 24&quot;가 될 수 있습니다. 입력: 4 9 1010-4-(왼쪽: 6913) 14+9=(왼쪽: 10 13 13) (a) 프롬프트 제안 {예시 하나} 입력: 4 9 10가능한 다음 단계: 13-6-(왼쪽:79) 13-9=(왼쪽 4 6) 4+6=(왼쪽: 10) 4*6=(왼쪽: 24) (b) 값 프롬프트 주어진 숫자가 24에 도달할 수 있는지 평가합니다(확실/가능성/불가능) 10 14:10 +14= 24. 물론입니다 {예제 더 보기} 10 13생각 생성 4+9 13 (왼쪽: 10 13 13) LM 10 4 6 (왼쪽: 6913) LM {...줄 더 보기...} 생각 평가(13-10) 13 3 13 =10+13 +13 36 이렇게 큰 숫자로는 24를 얻을 수 없습니다.불가능 그림 2: 24 게임에서의 ToT.LM은 (a) 생각 생성 및 (b) 평가를 요청받습니다.작업 설정.인간의 풀기 시간에 따라 쉬운 것부터 어려운 것까지 정렬된 1,362개의 게임이 있는 4nums.com에서 데이터를 스크래핑하고, 테스트를 위해 901-1,000으로 색인된 비교적 어려운 게임의 하위 집합을 사용합니다. 각 작업에 대해 24와 같고 입력 숫자를 정확히 한 번만 사용하는 유효한 방정식인 경우 출력을 성공으로 간주합니다. 100개 게임에 대한 성공률을 메트릭으로 보고합니다. 기준선. 5개의 컨텍스트 내 예제가 있는 표준 입출력(IO) 프롬프트를 사용합니다. 사고의 사슬(CoT) 프롬프트의 경우 각 입출력 쌍을 나머지 두 숫자에서 작동하는 3개의 중간 방정식으로 확장합니다. 예를 들어, 입력 &quot;4 9 10 13&quot;이 주어지면 생각은 &quot;13 - 9 = 4(왼쪽: 4 4 10); 10 - 4 = 6(왼쪽: 4 6); 4 * 6 = 24(왼쪽: 24)&quot;가 될 수 있습니다. 각 게임에 대해 평균 성능을 위해 100회 IO 및 CoT 프롬프트를 샘플링합니다. 또한 100개의 CoT 샘플에서 대부분의 출력을 가져오는 CoT 자체 일관성 기준선과 최대 10번의 반복에 대한 IO 샘플 기반의 반복적 정제 방식을 고려합니다. 각 반복에서 LM은 출력이 올바르지 않은 경우 &quot;실수를 반성하고 정제된 답을 생성&quot;하도록 모든 이전 기록에 따라 조건이 지정됩니다. 방정식의 정확성에 대한 실제 피드백 신호를 사용한다는 점에 유의하세요. ToT 설정. 24번째 게임을 ToT로 구성하려면 생각을 각각 중간 방정식인 3단계로 분해하는 것이 자연스럽습니다. 그림 2(a)에서 볼 수 있듯이 각 트리 노드에서 나머지 숫자를 추출하고 LM에 다음 가능한 단계를 제안하도록 요청합니다. 모든 3가지 생각 단계에 동일한 &quot;프롬프트 제안&quot;이 사용되지만 4개의 입력 숫자가 있는 예는 하나뿐입니다. ToT에서 너비 우선 탐색(BFS)을 수행하며 각 단계에서 최상의 b = 5 후보를 유지합니다. 그림 2(b)에 표시된 것처럼 ToT에서 의도적인 BFS를 수행하기 위해 LM이 24에 도달하는 것과 관련하여 각 생각 후보를 &quot;확실/어쩌면/불가능&quot;으로 평가하도록 합니다. 목표는 몇 번의 룩어헤드 시도로 판결을 내릴 수 있는 정확한 부분 솔루션을 촉진하고 &quot;너무 크거나 작음&quot; 상식에 따라 불가능한 부분 솔루션을 제거하고 나머지는 &quot;어쩌면&quot;으로 유지하는 것입니다. 각 생각에 대해 3번 값을 샘플링합니다. ¹실험은 2023년 5월 5일~16일 사이에 수행되었습니다.방법 성공 (a) 방문한 노드의 성공률 (b) 각 단계에서 샘플이 실패함 CoT IO 프롬프트 7.3% ToT (b=5) CoT 프롬프트 4.0% 0.0.CoT-SC (k=100) 9.0% ToT(저희) (b=1) 45% 0.0.ToT(저희) (b=5) 74% IO(k 중 최고) IO+ 정제(k=10) 0.0.27% IO(100 중 최고) 33% COT(k 중 최고) TOT(b=1...5) COT(100 중 최고) 49% 0.1 2 3 4 정답 표 2: 24개 게임 결과.그림 3: 24개 게임 (a) 척도 분석 및 (b) 오류 분석.결과. 표 2에서 볼 수 있듯이 IO, CoT, CoT-SC 프롬프팅 방법은 작업에서 성능이 좋지 않아 각각 7.3%, 4.0%, 9.0%의 성공률만 달성합니다. 반면 b = 1의 폭을 가진 ToT는 이미 45%의 성공률을 달성하는 반면 b = 5는 74%를 달성합니다. 또한 k 개의 샘플 중 가장 좋은 것(1 ≤ k ≤ 100)을 사용하여 성공률을 계산하여 IO/COT에 대한 오라클 설정을 고려합니다. IO/COT(k 개의 가장 좋은 것)를 ToT와 비교하기 위해 b = 15에서 ToT에서 작업당 방문한 트리 노드를 계산하고 그림 3(a)에 5개의 성공률을 매핑하여 IO/COT(k 개의 가장 좋은 것)를 밴딧에서 k 개의 노드를 방문하는 것으로 처리합니다. 놀랍지 않게도 CoT는 IO보다 확장성이 뛰어나고 100개의 가장 좋은 CoT 샘플은 49%의 성공률을 달성하지만 ToT에서 더 많은 노드를 탐색하는 것보다는 여전히 훨씬 나쁩니다(b &gt; 1). 오류 분석. 그림 3(b)는 CoT 및 ToT 샘플이 작업에 실패한 단계, 즉 생각(CoT에 있는) 또는 모든 b 생각(ToT에 있는)이 유효하지 않거나 도달할 수 없는 단계를 분석합니다. 24. 주목할 점은 CoT 샘플의 약 60%가 첫 번째 단계 또는 동등하게 처음 세 단어(예: &quot;4 + 9&quot;)를 생성한 후에 이미 작업에 실패했다는 것입니다. 이는 왼쪽에서 오른쪽으로 직접 디코딩하는 데 따른 문제점을 강조합니다. 4. 창의적 쓰기 다음으로, 입력이 4개의 무작위 문장이고 출력이 각각 4개의 입력 문장으로 끝나는 4개의 문단이 있는 일관된 구절이어야 하는 창의적 쓰기 작업을 고안합니다. 이러한 작업은 개방적이고 탐색적이며 창의적 사고와 고수준 계획에 도전합니다. 작업 설정. randomwordgenerator.com에서 무작위 문장을 샘플링하여 100개의 입력을 형성하고 각 입력 제약 조건에 대한 기준 진실 구절은 없습니다. GPT-4가 대부분의 경우 입력 제약 조건을 따를 수 있다는 것을 알게 되었으므로 두 가지 방법으로 구절 일관성을 평가하는 데 중점을 두었습니다. GPT-4 제로샷 프롬프트를 사용하여 1~10점의 스칼라 점수를 제공하거나 인간의 판단을 사용하여 다양한 방법의 출력 쌍을 비교합니다. 전자의 경우 5개 점수를 샘플링하여 각 작업 출력에 대해 평균을 내며 이 5개 점수는 일반적으로 일관성이 있으며 출력 전체에서 평균 약 0.56의 표준 편차가 있음을 발견했습니다. 후자의 경우 저자 하위 집합을 맹검 연구에 사용하여 CoT 대 ToT에서 생성된 구절 쌍의 일관성을 비교했습니다. 여기서 구절의 순서는 100개의 입력에 대해 무작위로 뒤집습니다. 기준선. 작업의 창의적인 특성을 감안할 때 IO 및 CoT 프롬프트는 모두 제로샷입니다. 전자는 LM이 입력 제약 조건을 고려하여 일관된 구절을 직접 생성하도록 하는 반면, 후자는 LM이 먼저 간략한 계획을 세운 다음 구절을 작성하도록 합니다. 즉, 계획은 중간 사고 단계 역할을 합니다. 작업당 10개의 IO 및 CoT 샘플을 생성합니다. 또한 각 작업에 대한 무작위 IO 샘플 위에 반복적-세분화(k≤5) 방법을 고려합니다. 여기서 LM은 입력 제약 조건과 마지막으로 생성된 구절에 따라 구절이 이미 &quot;완벽하게 일관성이 있는지&quot; 여부를 결정하고 그렇지 않으면 정제된 구절을 생성합니다. = LM 첫 번째 5개 구절 1, ToT 설정은 선택 사항이 하나뿐이기 때문입니다. 깊이 2(중간 사고 단계가 1개만 있음)로 ToT를 빌드하여 k 5개 계획을 생성하고 가장 좋은 계획에 투표한 다음(그림 4) 마찬가지로 가장 좋은 계획에 따라 k를 생성한 다음 가장 좋은 계획에 투표합니다. 여기서 폭 제한 b = 단계당 유지됩니다. 간단한 제로샷 투표 프롬프트(&quot;아래의 선택 사항을 분석한 다음 지침에 가장 유망한 것을 결론 내립니다&quot;)를 사용하여 두 단계 모두에서 5개 투표를 샘플링합니다. 결과. 그림 5(a)는 100개 과제에 걸친 평균 GPT-4 점수를 보여주며, ToT(7.56)가 평균적으로 IO(6.19) 및 CoT(6.93)보다 더 일관된 구절을 생성하는 것으로 간주됩니다. 이러한 자동 메트릭은 노이즈가 있을 수 있지만, 그림 5(b)는 100개의 구절 쌍 중 41개에서 인간이 CoT보다 ToT를 선호하는 반면, 21개에서만 CoT를 ToT보다 선호한다는 것을 보여줌으로써 결과를 확인시켜 줍니다(다른 38개 쌍은 &quot;비슷하게 일관성이 있음&quot;으로 발견됨). 마지막으로 반복적 정제는 이 자연어 작업에서 더 효과적입니다.입력 입력 | (b) 계획 I 계획 1 계획 구절 구절 4개의 짧은 단락으로 구성된 일관된 구절을 작성합니다. 각 단락의 마지막 문장은 다음과 같아야 합니다. 1. 그냥 손으로 서 있으면 핸드스탠드를 하는 것은 어렵지 않습니다. 2. 그 공간에서 구운 스테이크 냄새가 난다는 사실에 그는 깜짝 놀랐습니다. 3. 그녀를 들어올리려는 남자가 마음에 들지 않자 수화를 사용하기 시작했습니다. 4. 당신을 아는 사람마다 당신에 대한 인식이 다릅니다. 계획 1. 핸드스탠드를 하는 기술을 소개하고 설명합니다. 2. 다음에 대한 이야기로 전환합니다. 우주인이 우주에 간 첫 경험 3. 여성이 원치 않는 주의를 피하기 위해 수화를 사용하는 상황을 설명합니다. 4. 마지막 문단은 모든 사람이 다른 사람에 대해 서로 다른 인식을 가지고 있는 방법을 설명합니다. 계획 1. 핸드스탠드를 도전을 수용하는 은유로 언급하는 특이한 자기계발 책의 소개. 2. 우주의 냄새를 포함하여 우주인으로부터 배운 예상치 못한 것에 대해 논의합니다. 3. 술집에서 원치 않는 주의를 피하기 위한 여성의 영리한 전략을 설명합니다. 4. 자신에 대한 다른 인식이 어떻게 자신의 정체성을 형성할 수 있는지 생각해 보세요. 계획 3-1.... 2... 0/5 투표 3/5 투표 n/5 투표 투표 각 선택 사항을 자세히 분석합니다. 선택 사항 1은 필수 종료 문장을 통합했지만 문단 간의 명확한 연결이 부족한 것처럼 보입니다. {...} 선택 사항 2는 필수 종료 문장을 사용하여 자기계발 책의 내용을 제시함으로써 흥미로운 관점을 제공합니다. 문단을 자기 계발과 도전 수용이라는 주제로 연결하여 일관된 구절을 만듭니다. {...} 최상의 선택은 2입니다. 그림 4: 무작위로 선택된 창의적 작문 과제에서 의도적인 검색 단계. 입력이 주어지면 LM은 5가지 다른 계획을 샘플링한 다음 5번 투표하여 어느 계획이 가장 좋은지 결정합니다. 다수의 선택은 결과적으로 동일한 샘플 투표 절차로 출력 구절을 작성하는 데 사용됩니다.(a) GPT-4 일관성 점수 (b) 인간의 일관성 비교 방법 성공률(%) 문자 단어 게임IO COT ToT(저희) 38.7 1440.6 15.678 60+최상의 상태 -자르기 -역추적 82.4 67.565.4 41.554.6 2010 COT ToT 10 TOT+개선 +개선 COTTOT 유사한 ToT &gt; COT 표 3: 미니 크로스워드 결과. 그림 5: 창의적 작문 결과. IO 일관성 점수는 6.19에서 7.67로, ToT 일관성 점수는 7.56에서 7.91로 향상되었습니다. ToT 프레임워크에서 생각 생성을 위한 세 번째 접근 방식으로 생각할 수 있다고 생각하는데, iid나 순차적으로 생성되는 대신 오래된 생각을 다듬어서 새로운 생각이 생길 수 있습니다. 4.3 미니 크로스워드 24게임과 창의적 글쓰기에서 ToT는 비교적 얕습니다. 최종 출력에 도달하는 데 최대 3개의 생각 단계가 필요합니다. 여기서는 자연어를 포함하는 더 어려운 검색 문제로 5 × 5 미니 크로스워드를 탐구합니다. 다시 말하지만, 목표는 단순히 작업을 해결하는 것이 아닙니다. 보다 일반적인 크로스워드는 LM 대신 대규모 검색을 활용하는 특수 NLP 파이프라인[34]으로 쉽게 해결할 수 있기 때문입니다. 오히려 의도적인 추론을 휴리스틱으로 사용하여 자신의 생각을 탐구하고 자신의 탐구를 안내하는 일반적인 문제 해결사로서 LM의 한계를 탐구하는 것을 목표로 합니다. 작업 설정. GooBix에서 데이터를 스크래핑하는데, 여기에는 5 x 5 미니 크로스워드 게임 156개가 들어 있습니다. 인접한 게임에 비슷한 단서가 들어 있는 것을 관찰했으므로 테스트를 위해 인덱스 1, 6, ..., 91, 96이 있는 게임 20개를 사용하고 프롬프트를 위해 게임 136, 141, 146, 151, 156을 사용합니다. 각 작업의 경우 입력은 수평 단서와 5개의 수직 단서를 설명하고 출력은 크로스워드를 풀기 위한 5 x 5 == 25글자 보드여야 합니다. 평가를 위해 세 가지 성공 수준을 고려합니다. 정답률(게임당), 단어(게임당 10개), 게임입니다. 기준선. IO 프롬프트에 5개의 예제 입출력 쌍을 제공하고 CoT 프롬프트에 h1..5, v1..5 순서로 중간 단어를 추가로 포함합니다. 각 프롬프트를 샘플로 실행하고 결과의 평균을 냅니다. ToT 설정. 우리는 상태가 더 이상 유망하지 않을 때까지 가장 유망한 후속 단어 단서를 계속 탐색하는 깊이 우선 탐색(알고리즘 2)을 활용하고, 그런 다음 부모 상태로 되돌아가 대안적 생각을 탐색합니다. 탐색을 다루기 쉽게 하기 위해 후속 생각은 채워진 단어나 문자를 변경하지 않도록 제한되므로 ToT는 최대 10개의 중간 단계를 갖습니다. 생각 생성을 위해 각 상태에서 기존의 모든 생각(예: 그림 6(a)의 상태에 대한 &quot;h2.motor; h1.tasks&quot;)을 나머지 단서에 대한 문자 제약 조건(예: &quot;v1.To heap: tm___;...&quot;)으로 변환하고 제안 프롬프트를 5번 프롬프트하여 다음 단어에 채울 위치와 내용에 대한 후보를 제시합니다. 중요한 점은 LM에 다른 생각에 대한 신뢰 수준을 제공하고, aggregatetasks motor 입력 단서 salon h2.motor h1.tasks (b) (역추적) DFS 생각 제안 순서 h4. salon (확실) aggregate h4. salon v5. srdry (낮음) h3. grand v3. string (높음) v3. string ...... 상태 평가자(각 단서 위에) v3. Pretentious; flowery: __ sure h3.grand h4.salon v1. To heap: tm_s_ {..} impossible ...... (서브 트리가 정리됨) v5. 건조기; 더 건조: sr_n_ {...} 아마도 그림 6: 미니 크로스워드에서 (a) 깊이 우선 탐색(DFS)을 위해 우선순위 대기열에 생각을 제안하고 집계하는 방법, (b) 남은 단어 단서를 채울 가능성에 따라 상태를 평가하고 LM에서 채울 수 없는 남은 단서가 있는 경우 정리하는 방법. 그런 다음 DFS는 부모 상태로 백트래킹하여 단서에 대한 다음 유망한 생각을 탐색합니다. 이를 제안에 걸쳐 탐색할 다음 생각의 정렬된 목록을 얻습니다(그림 6(a)). 상태 평가의 경우 마찬가지로 각 상태를 남은 단서에 대한 문자 제약 조건으로 변환한 다음 제약 조건을 고려하여 채울 수 있는지 각 단서를 평가합니다. 남은 단서가 채우기 &quot;불가능&quot;한 것으로 간주되면(예: &quot;v1. 쌓으려면: tm_s_&quot;) 상태의 서브트리 탐색이 정리되고 DFS는 부모 상태로 백트래킹하여 다음 유망한 생각을 탐색합니다. DFS 검색 단계를 100으로 제한하고 가장 깊이 탐색된 상태(여러 개일 경우 처음 탐색된 상태)를 최종 출력으로 렌더링합니다.결과.표 3에서 볼 수 있듯이 IO 및 CoT 프롬프팅 방법은 단어 수준 성공률이 16% 미만으로 성과가 좋지 않은 반면 ToT는 모든 지표를 크게 개선하여 단어 수준 성공률이 60%에 도달하고 20개 게임 중 4개를 해결합니다.IO 및 CoT에는 다른 단서를 시도하거나 결정을 변경하거나 후퇴할 수 있는 메커니즘이 없다는 점을 감안할 때 이러한 개선은 놀라운 일이 아닙니다.오라클 및 절제 연구.작업당 오라클 최상의 DFS 상태(경험적으로 결정된 최상의 상태 대신)에서 출력할 때 ToT 성능은 더 높고 실제로 20개 게임 중 7개를 해결합니다(표 3, &quot;+최상의 상태&quot;).이는 간단한 출력 휴리스틱을 쉽게 개선할 수 있음을 나타냅니다. 흥미롭게도, 때때로 크로스워드 게임이 실제로 풀렸을 때, 상태 평가자는 여전히 일부 단어를 &quot;불가능&quot;하다고 간주하고 정리할 수 있습니다. 아마도 5 × 5 크로스워드는 의도적으로 GPT-4가 인식할 수 없는 희귀하거나 쓸모없는 단어가 있기 때문일 것입니다². 정리 휴리스틱으로서의 상태 평가가 불완전하다는 점을 감안하여, 정리를 제거하는 것도 탐구하고, 일반적으로 성능이 더 나쁘다는 것을 발견했습니다(표 3, &quot;-정리&quot;). 그러나 실제로 20개 게임 중 4개에 대한 올바른 솔루션을 찾을 수 있었습니다(하지만 휴리스틱을 통해 1개만 출력). 그 중 3개는 ToT+정리로 100단계 이내에 해결할 수 없는 게임입니다. 따라서 이 경우 DFS 정리를 위한 더 나은 휴리스틱이 문제 해결에 중요합니다. 마지막으로, 최대 20단계 동안 가장 유망한 단서를 계속 채우는 정리를 실행하여 백트래킹의 중요성을 확인하여 덮어쓰기를 허용합니다. 이것은 b = 1의 폭 제한을 갖는 &quot;탐욕적인&quot; BFS 검색과 유사하며 단어 수준 성공률이 20%에 불과해 성능이 좋지 않습니다(표 3, &quot;-backtrack&quot;). 5
--- RELATED WORK ---
계획 및 의사 결정. 스마트한 계획 및 의사 결정은 미리 정의된 목표를 달성하는 데 중요합니다. 방대한 양의 세계 지식과 인간의 사례에 대해 훈련되기 때문에 LM은 이미 풍부한 상식을 흡수하여 문제 설정 및 환경 상태에 따라 합리적인 계획을 제안할 수 있는 것으로 알려져 있습니다[12, 42, 37, 13, 35, 41, 40]. 제안된 ToT 접근 방식은 각 문제 해결 단계에서 동시에 여러 가지 잠재적으로 실행 가능한 계획을 고려하고 가장 유망한 계획을 진행하여 기존 계획 공식을 확장합니다. 사고 샘플링과 가치 피드백 간의 통합은 계획 및 의사 결정 메커니즘을 유기적으로 통합하여 솔루션 트리 내에서 효과적인 검색을 가능하게 합니다. 반면, 기존의 의사 결정 절차는 일반적으로 강화 학습(예: CHAI[33])에서와 같이 전담 보상 및 정책 모델을 훈련해야 하는 반면, 우리는 LM 자체를 사용하여 의사 결정을 위한 가치 추정치를 제공합니다. RAP [9]는 언어 모델 2를 내부 세계 모델과 함께 계획하는 것으로 취급하는 동시 작업입니다.예를 들어, &quot;agend&quot;는 &quot;agendum&quot;의 구식 형태이지만 GPT-4는 이를 &quot;agenda&quot;의 오타로 간주합니다.외부 검색 또는 웹 상호 작용은 지식 불확실성 하에서 문제 해결을 위한 LM을 증강할 수 있습니다.추론하고 MCTS 기반
--- METHOD ---
74%의 성공률을 달성했습니다. 모든 프롬프트가 있는 코드 리포: https://github.com/princeton-nlp/tree-of-thought-11m. 1 서론 원래 텍스트를 생성하도록 설계된 GPT [25, 26, 1, 23] 및 PaLM [5]과 같은 확장된 버전의 언어 모델(LMS)은 수학적, 기호적, 상식적, 지식적 추론이 필요한 점점 더 광범위한 작업을 수행할 수 있는 것으로 나타났습니다. 이 모든 진전의 기반이 여전히 토큰 수준의 결정을 하나씩 그리고 왼쪽에서 오른쪽으로 내리는 원래의 텍스트 생성을 위한 자기 회귀 메커니즘이라는 것은 아마도 놀랍습니다. 이렇게 간단한 메커니즘으로 LM을 일반적인 문제 해결자로 구축하기에 충분할까요? 그렇지 않다면 어떤 문제가 현재 패러다임에 도전할까요? 그리고 대안적인 메커니즘은 무엇일까요? 인간 인지에 대한 문헌은 이러한 질문에 답할 수 있는 몇 가지 단서를 제공합니다. &quot;이중 프로세스&quot; 모델에 대한 연구에 따르면 사람들은 결정을 내리는 데 두 가지 모드, 즉 빠르고 자동적이며 무의식적인 모드(&quot;시스템 1&quot;)와 느리고 의도적이며 의식적인 모드(&quot;시스템 2&quot;)를 가지고 있는 것으로 나타났습니다[30, 31, 16, 15]. 이 두 모드는 이전에 머신 러닝에 사용되는 다양한 수학적 모델과 연결되었습니다. 예를 들어, 인간과 다른 동물의 강화 학습에 대한 연구에서는 연관적 &quot;모델 없는&quot; 학습이나 보다 신중한 &quot;모델 기반&quot; 계획에 참여하는 상황을 탐구했습니다[7]. LM의 간단한 연관 토큰 수준 선택은 또한 &quot;시스템 1&quot;을 연상시키며, 따라서 (1) 현재 37차 신경 정보 처리 시스템 컨퍼런스(NeurIPS 2023)에 대한 다양한 대안을 유지하고 탐색하는 보다 신중한 &quot;시스템 2&quot; 계획 프로세스로 증강하는 것이 유익할 수 있습니다. 입력 입력 입력 입력 생각 ...... ...... -----출력 출력 다수결 투표 출력 (a) 입력-출력 (c) 생각의 사슬 촉구 (10) 촉구(COT) (c) COT와의 자기 일관성(COT-SC) 출력 (d) 생각의 나무(ToT) 그림 1: LLM을 사용한 문제 해결에 대한 다양한 접근 방식을 설명하는 개략도. 각 직사각형 상자는 생각을 나타내며, 이는 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다. 그림 2,4,6에서 생각이 생성, 평가 및 검색되는 구체적인 예를 참조하세요. 하나를 선택하는 대신 선택 사항을 유지하고 (2) 현재 상태를 평가하고 영어: 적극적으로 앞을 내다보거나 후퇴하여 더욱 글로벌한 결정을 내립니다. 이러한 계획 프로세스를 설계하기 위해 우리는 인공 지능(및 인지 과학)의 기원으로 돌아가 Newell, Shaw, Simon이 1950년대에 탐구한 계획 프로세스에서 영감을 얻었습니다[21, 22]. Newell과 동료들은 문제 해결[21]을 나무로 표현된 조합 문제 공간을 검색하는 것으로 특징지었습니다. 따라서 우리는 언어 모델을 사용하여 일반적인 문제 해결을 위한 생각의 나무(TOT) 프레임워크를 제안합니다. 그림 1에서 알 수 있듯이 기존 방법(아래에 자세히 설명)은 문제 해결을 위해 연속적인 언어 시퀀스를 샘플링하는 반면, ToT는 생각의 나무를 적극적으로 유지 관리합니다. 여기서 각 생각은 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(표 1). 이러한 고수준 의미 단위를 통해 LM은 언어로 인스턴스화된 의도적인 추론 프로세스를 통해 문제를 해결하기 위해 다양한 중간 생각이 이루는 진행 상황을 자체 평가할 수 있습니다(그림 2,4,6). LM 자체 평가 및 심의를 통한 이러한 검색 휴리스틱 구현은 이전 검색 휴리스틱이 프로그래밍되거나 학습되었기 때문에 참신합니다. 마지막으로, 우리는 다양한 사고를 생성하고 평가하는 이 언어 기반 기능을 너비 우선 탐색(BFS) 또는 깊이 우선 탐색(DFS)과 같은 검색 알고리즘과 결합하여 룩어헤드 및 백트래킹을 사용하여 사고의 나무를 체계적으로 탐색할 수 있습니다. 경험적으로, 우리는 최첨단 언어 모델인 GPT-4[23]를 사용하더라도 기존 LM 추론 방법에 도전하는 세 가지 새로운 문제를 제안합니다. 24게임, 창의적 글쓰기 및 크로스워드(표 1). 이러한 과제에는 연역적, 수학적, 상식적, 어휘적 추론 능력과 체계적인 계획 또는 검색을 통합하는 방법이 필요합니다. 우리는 ToT가 다양한 수준의 사고, 사고를 생성하고 평가하는 다양한 방법 및 다양한 문제의 특성에 적응하는 다양한 검색 알고리즘을 지원할 만큼 일반적이고 유연하여 세 가지 과제 모두에서 우수한 결과를 얻는다는 것을 보여줍니다. 또한 이러한 선택이 체계적 절제를 통해 모델 성능에 어떤 영향을 미치는지 분석하고 LM을 보다 잘 훈련하고 사용하기 위한 향후 방향에 대해 논의합니다.2 배경 먼저 문제 해결을 위해 대규모 언어 모델을 사용하는 기존 방법을 공식화합니다.이는 저희의 접근 방식이 영감을 받았고 나중에 비교되었습니다.po는 매개변수 0을 갖는 사전 훈련된 LM을 나타내고 소문자 x, y, z, s, …..는 언어 시퀀스 iex = (x[1],, x[n])를 나타냅니다.여기서 각 x[i]는 토큰이므로 po(x) = []=1 Po(x[i]|x[1...i]).대문자 S는 언어 시퀀스 모음을 나타냅니다.입출력(IO) 프롬프팅은 LM을 사용하여 문제 입력 x를 출력 y로 바꾸는 가장 일반적인 방법입니다.y ~ Pe(y prompt 10(x)), 여기서 prompt 10(x)는 입력 x를 작업 지침 및/또는 소수 샷 입출력 예제로 래핑합니다. 단순화를 위해 Pe(출력 | 입력) = Pe(출력 프롬프트(입력))로 표시하겠습니다.그러면 IO 프롬핑을 y ~ ·P(y|x)로 공식화할 수 있습니다.프롬프트 IOCOT 생각의 사슬(CoT) 프롬핑[38]은 입력 x에서 출력 y로의 매핑이 사소하지 않은 경우(예: x가 수학 질문이고 y가 최종 숫자 답변인 경우)를 해결하기 위해 제안되었습니다.핵심 아이디어는 x와 y를 연결하기 위해 생각의 사슬 z₁, …, zn을 도입하는 것입니다.여기서 각 z¿는 문제 해결을 향한 의미 있는 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(예: zi는 수학 QA에 대한 중간 방정식이 될 수 있음).CoT로 문제를 풀기 위해 각 생각 ZipoT(zi | x, 21-1)을 순차적으로 샘플링한 다음 출력 y ~ CoT(y|x, z1...n)를 수행합니다. 실제로, [z1...n, Y] (z1...n, y|x)는 연속적인 언어 시퀀스로 샘플링되고, 사고의 분해(예: 각 z¿가 구, 문장 또는 문단인가)는 모호한 상태로 남습니다.CoT를 통한 자기 일관성(CoT-SC) [36]은 k iid 사고 사슬을 샘플링하는 앙상블 방식입니다. [z\...n, y(i)] CoT (z1...n, y|x) (i = 1... k), 그런 다음 가장 빈번한 출력을 반환합니다. ~ COT Ро = ~ arg maxy #{iy (i) y}. CoT-SC는 일반적으로 동일한 문제에 대해 서로 다른 사고 과정(예: 동일한 정리를 증명하는 서로 다른 방법)이 있고, 더 풍부한 사고 집합을 탐색함으로써 출력 결정이 더 충실할 수 있기 때문에 CoT보다 개선되었습니다. 그러나 각 체인 내에서는 다양한 사고 단계에 대한 로컬 탐색이 없으며, &quot;가장 빈번한&quot; 휴리스틱은 출력 공간이 제한될 때(예: 객관식 QA)에만 적용됩니다.3 사고의 나무: LM을 이용한 의도적인 문제 해결 진정한 문제 해결 프로세스에는 탐색을 시작하기 위해 사용 가능한 정보를 반복적으로 사용하여야 하며, 이를 통해 더 많은 정보가 공개되고, 마침내 해결책을 얻는 방법이 발견됩니다.- Newell et al. [21] 인간의 문제 해결에 대한 연구에 따르면 사람들은 조합적 문제 공간(노드가 부분적 해결책을 나타내고 브랜치가 이를 수정하는 연산자에 해당하는 트리)을 검색한다고 합니다[21, 22]. 어떤 브랜치를 선택할지는 문제 공간을 탐색하고 문제 해결자가 해결책을 찾을 수 있도록 안내하는 휴리스틱에 의해 결정됩니다. 이 관점은 일반적인 문제를 해결하기 위해 LM을 사용하는 기존 접근 방식의 두 가지 주요 단점을 강조합니다.1) 로컬하게는 사고 과정 내의 다양한 연속성, 즉 트리의 브랜치를 탐색하지 않습니다. 2) 전 세계적으로, 그들은 이러한 다양한 옵션을 평가하는 데 도움이 되는 어떤 유형의 계획, 룩어헤드 또는 백트래킹도 통합하지 않습니다.인간의 문제 해결의 특징인 것으로 보이는 일종의 휴리스틱 가이드 검색입니다.이러한 단점을 해결하기 위해 LM이 생각에 대한 여러 추론 경로를 탐색할 수 있도록 하는 패러다임인 Tree of Thoughts(ToT)를 소개합니다(그림 1(c)).ToT는 모든 문제를 트리에 대한 검색으로 구성하며, 각 노드는 입력과 지금까지의 생각의 순서를 갖는 부분 솔루션을 나타내는 상태 s = [x, z1...]입니다.ToT의 구체적인 인스턴스화에는 네 가지 질문에 답하는 것이 포함됩니다.1. 중간 프로세스를 생각 단계로 분해하는 방법;2. 각 상태에서 잠재적인 생각을 생성하는 방법;3. 상태를 휴리스틱하게 평가하는 방법;4. 사용할 검색 알고리즘.1. 생각 분해.CoT가 명시적인 분해 없이 생각을 일관되게 샘플링하는 반면, ToT는 문제 속성을 활용하여 중간 생각 단계를 설계하고 분해합니다. 표에서 보듯이, 다양한 문제에 따라 생각은 몇 개의 단어(크로스워드), 한 줄의 방정식(24게임) 또는 한 단락의 쓰기 계획(창의적 작문)이 될 수 있습니다. 일반적으로 생각은 LM이 유망하고 다양한 샘플을 생성할 수 있을 만큼 &quot;작아야&quot; 하지만(예: 책 전체를 생성하는 것은 일반적으로 너무 &quot;크기&quot; 때문에 일관성이 없음), LM이 문제 해결에 대한 전망을 평가할 수 있을 만큼 &quot;크어야&quot; 합니다(예: 토큰 하나를 생성하는 것은 일반적으로 너무 &quot;작기&quot; 때문에 평가할 수 없음). 2. 생각 생성기 G(po, s, k). 트리 상태 s = [x, z1...]이 주어지면 다음 생각 단계에 대한 k 후보를 생성하는 두 가지 전략을 고려합니다. COT (a) CoT 프롬프트에서 iid 생각 샘플(창의적 작문, 그림 4): 2(j) CoT (zi+1|s) = pco (zi+1|x, z1...i) (j = 1 ……. k). 이 방법은 생각 공간이 풍부할 때(예: 각 생각이 문단일 때) 더 효과적이며, iid 샘플은 다양성으로 이어진다.(b) &quot;제안 프롬프트&quot;를 사용하여 생각을 순차적으로 제안한다(24개 게임, 그림 2; 크로스워드, 그림 6): [2(1),..., 2(k)] ~ ppr suggests (1... k) (2+1초). 이 방법은 생각 공간이 더 제한적일 때(예: 각 생각이 단어나 줄일 때) 더 효과적이므로 동일한 맥락에서 다른 생각을 제안하면 중복을 피할 수 있다.3. 상태 평가자 V(pe, S). 다양한 상태의 경계가 주어지면 상태 평가자는 문제를 해결하기 위한 진행 상황을 평가하여 탐색 알고리즘이 어떤 상태를 어떤 순서로 계속 탐색할지 결정하는 휴리스틱 역할을 한다.휴리스틱은 탐색 문제를 해결하기 위한 표준적인 접근 방식이지만 일반적으로 프로그래밍(예: DeepBlue [3])되거나 학습(예: AlphaGo [29])된다. 우리는 LM을 사용하여 상태에 대해 의도적으로 추론하는 세 번째 대안을 제안합니다. 해당되는 경우 이러한 의도적인 휴리스틱은 프로그래밍된 규칙보다 더 유연할 수 있으며 학습된 모델보다 샘플 효율성이 더 높을 수 있습니다. 사고 생성기와 유사하게 우리는 상태를 독립적으로 또는 함께 평가하는 두 가지 전략을 고려합니다. (a) 각 상태를 독립적으로 평가합니다. V(po, s)(s) ~ pvalue (vs) Vs ES, 여기서 값 프롬프트는 상태 s에 대해 추론하여 스칼라 값 v(예: 1-10) 또는 휴리스틱하게 값으로 변환될 수 있는 분류(예: 확실/가능성/불가능)를 생성합니다. 이러한 평가적 추론의 기초는 문제와 사고 단계에 따라 다를 수 있습니다. 이 작업에서 우리는 몇 가지 룩어헤드 시뮬레이션을 통한 평가를 탐구합니다(예: 5, 5, 14가 5 + 5 + 14를 통해 24에 도달할 수 있음을 빠르게 확인하거나 &quot;hot_l&quot;이 &quot;_&quot;에 &quot;e&quot;를 채워 &quot;inn&quot;을 의미할 수 있음을 확인합니다). 또한 상식(예: 123은 24에 도달하기에는 너무 작음 또는 어떤 단어도 &quot;tzxc&quot;로 시작할 수 없음)을 고려합니다. 전자가 &quot;좋은&quot; 상태를 촉진할 수 있는 반면, 후자는 &quot;나쁜&quot; 상태를 제거하는 데 도움이 될 수 있습니다. 이러한 평가는 완벽할 필요가 없으며 의사 결정에 대략적으로 도움이 되면 됩니다. (b) 상태 간 투표: V(pe, S)(s) = 1[s = s*], 여기서 &quot;좋은&quot; 상태 s* 투표(s*|S)는 투표 프롬프트에서 S의 다른 상태를 의도적으로 비교하여 ~ 투표에서 제외됩니다. 문제 성공 여부를 직접 평가하기 어려운 경우(예: 구절 일관성) 대신 다른 부분 솔루션을 비교하고 가장 유망한 솔루션에 투표하는 것이 자연스럽습니다. 이는 다음과 유사한 정신입니다. 영어: &quot;단계별&quot; 자기 일관성 전략, 즉 &quot;탐색할 상태&quot;를 다중 선택 QA로 캐스팅하고 LM 샘플을 사용하여 이에 투표합니다. 두 전략 모두 LM에 여러 번 요청하여 값이나 투표 결과를 집계하여 시간/자원/비용을 보다 충실하고 견고한 휴리스틱으로 교환할 수 있습니다. 알고리즘 1 ToT-BFS(x, po, G, k, V,T,b) 필요 사항: 입력 x, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 너비 제한 b. 따라서 {x} = t = 1,, T에 대해 S←{[s, z] | s Є St−1, Zt Є G(pe, s, k)} Vt ← V (po, St) Starg maxscs₁₁|S|=bses Vt(s) end for SES return G(pe, arg maxSEST VT(S), 1) 알고리즘 2 ToT-DFS(s, t, po, G, k, V, T, Uth) 필요: 현재 상태 s, 단계 t, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 임계값 th if t T then 기록 출력 G(pe, s, 1) end if for s&#39;ЄG(pe, s, k) do ▷ 정렬된 후보 if V (pe, {s&#39;})(s) &gt; Vthres then ▷ 가지치기 DFS (s&#39;, t + 1) end if end for 4. 검색 알고리즘. 마지막으로, ToT 프레임워크 내에서 트리 구조에 따라 다양한 검색 알고리즘을 플러그 앤 플레이 방식으로 사용할 수 있습니다. 비교적 간단한 두 가지 검색 알고리즘을 살펴보고, 더 진보된 알고리즘(예: A* [11], MCTS [2])은 추후 작업을 위해 남겨둡니다. (a) 너비 우선 탐색(BFS)(알고리즘 1)은 단계당 가장 유망한 b 상태 집합을 유지합니다. 이것은 트리 깊이가 한계(T3)이고 초기 사고 단계를 평가하여 작은 집합으로 정리할 수 있는(b ≤ 5) 24게임과 창의적 글쓰기에 사용됩니다. (b) 깊이 우선 탐색(DFS)(알고리즘 2)은 가장 유망한 상태를 먼저 탐색하여 최종 출력에 도달할 때까지(t &gt; T) 또는 상태 평가자가 현재 s에서 문제를 해결하는 것이 불가능하다고 판단할 때까지(V(po, {s})(s) ≤ vth, 값 임계값 Vth의 경우) 진행합니다. 후자의 경우, s의 서브 트리를 정리하여 탐색을 활용으로 교환합니다. 두 경우 모두 DFS는 s의 부모 상태로 후퇴하여 탐색을 계속합니다. 개념적으로 ToT는 LM을 사용한 일반적인 문제 해결 방법으로서 여러 가지 이점이 있습니다. (1) 일반성. IO, COT, CoT-SC 및 자체 정제는 ToT의 특수한 경우로 볼 수 있습니다(즉, 깊이와 폭이 제한된 트리, 그림 1). (2) 모듈성. 기본 LM과 사고 분해, 생성, 평가 및 검색 절차는 모두 독립적으로 다양화할 수 있습니다. (3) 적응성. 다양한 문제 속성, LM 기능 및 리소스 제약을 수용할 수 있습니다. (4) 편의성. 추가 교육이 필요하지 않으며 사전 교육된 LM만 있으면 충분합니다. 다음 섹션에서는 이러한 개념적 이점이 다양한 문제에서 강력한 경험적 성과로 어떻게 전환되는지 보여줍니다. 4
--- EXPERIMENT ---
s는 ToT가 비사소한 계획이나 검색이 필요한 세 가지 새로운 과제, 즉 24게임, 창의적 글쓰기, 미니 크로스워드에서 언어 모델의 문제 해결 능력을 크게 향상시킨다는 것을 보여줍니다.예를 들어, 24게임에서 생각의 사슬을 촉구하는 GPT-4는 과제의 4%만 해결한 반면, 우리의 방법은 74%의 성공률을 달성했습니다.모든 프롬프트가 있는 코드 리포: https://github.com/princeton-nlp/tree-of-thought-11m.1 서론 원래 텍스트를 생성하도록 설계된 GPT [25, 26, 1, 23] 및 PaLM [5]과 같은 확장된 버전의 언어 모델(LMS)은 수학적, 기호적, 상식적, 지식적 추론이 필요한 점점 더 광범위한 과제를 수행할 수 있는 것으로 나타났습니다.이 모든 진전의 근간이 여전히 토큰 수준의 결정을 하나씩 그리고 왼쪽에서 오른쪽으로 내리는 원래의 텍스트 생성 자기 회귀 메커니즘이라는 것은 아마도 놀랍습니다. 이렇게 간단한 메커니즘이 LM을 일반적인 문제 해결자로 구축하기에 충분할까요? 그렇지 않다면 어떤 문제가 현재 패러다임에 도전할까요? 그리고 대안적인 메커니즘은 무엇일까요? 인간 인지에 관한 문헌은 이러한 질문에 답할 수 있는 몇 가지 단서를 제공합니다. &quot;이중 프로세스&quot; 모델에 대한 연구에 따르면 사람들은 빠르고 자동적이며 무의식적인 모드(&quot;시스템 1&quot;)와 느리고 의도적이며 의식적인 모드(&quot;시스템 2&quot;)의 두 가지 모드로 의사 결정에 임한다고 합니다[30, 31, 16, 15]. 이 두 가지 모드는 이전에 머신 러닝에 사용되는 다양한 수학적 모델에 연결되었습니다. 예를 들어, 인간과 다른 동물의 강화 학습에 대한 연구에서는 연관적 &quot;모델 없는&quot; 학습이나 보다 신중한 &quot;모델 기반&quot; 계획에 참여하는 상황을 탐구했습니다[7]. LM의 간단한 연관 토큰 수준 선택은 또한 &quot;시스템 1&quot;을 연상시키며, 따라서 (1) 현재 37차 신경 정보 처리 시스템 컨퍼런스(NeurIPS 2023)에 대한 다양한 대안을 유지하고 탐색하는 보다 신중한 &quot;시스템 2&quot; 계획 프로세스로 증강하는 것이 유익할 수 있습니다. 입력 입력 입력 입력 생각 ...... ...... -----출력 출력 다수결 투표 출력 (a) 입력-출력 (c) 생각의 사슬 촉구 (10) 촉구(COT) (c) COT와의 자기 일관성(COT-SC) 출력 (d) 생각의 나무(ToT) 그림 1: LLM을 사용한 문제 해결에 대한 다양한 접근 방식을 설명하는 개략도. 각 직사각형 상자는 생각을 나타내며, 이는 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다. 그림 2,4,6에서 생각이 생성, 평가 및 검색되는 구체적인 예를 참조하세요. 하나를 선택하는 대신 선택 사항을 유지하고 (2) 현재 상태를 평가하고 영어: 적극적으로 앞을 내다보거나 후퇴하여 더욱 글로벌한 결정을 내립니다. 이러한 계획 프로세스를 설계하기 위해 우리는 인공 지능(및 인지 과학)의 기원으로 돌아가 Newell, Shaw, Simon이 1950년대에 탐구한 계획 프로세스에서 영감을 얻었습니다[21, 22]. Newell과 동료들은 문제 해결[21]을 나무로 표현된 조합 문제 공간을 검색하는 것으로 특징지었습니다. 따라서 우리는 언어 모델을 사용하여 일반적인 문제 해결을 위한 생각의 나무(TOT) 프레임워크를 제안합니다. 그림 1에서 알 수 있듯이 기존 방법(아래에 자세히 설명)은 문제 해결을 위해 연속적인 언어 시퀀스를 샘플링하는 반면, ToT는 생각의 나무를 적극적으로 유지 관리합니다. 여기서 각 생각은 문제 해결을 향한 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(표 1). 이러한 고수준 의미 단위를 통해 LM은 언어로 인스턴스화된 의도적인 추론 프로세스를 통해 문제를 해결하기 위해 다양한 중간 생각이 이루는 진행 상황을 자체 평가할 수 있습니다(그림 2,4,6). LM 자체 평가 및 심의를 통한 이러한 검색 휴리스틱 구현은 이전 검색 휴리스틱이 프로그래밍되거나 학습되었기 때문에 참신합니다. 마지막으로, 우리는 다양한 사고를 생성하고 평가하는 이 언어 기반 기능을 너비 우선 탐색(BFS) 또는 깊이 우선 탐색(DFS)과 같은 검색 알고리즘과 결합하여 룩어헤드 및 백트래킹을 사용하여 사고의 나무를 체계적으로 탐색할 수 있습니다. 경험적으로, 우리는 최첨단 언어 모델인 GPT-4[23]를 사용하더라도 기존 LM 추론 방법에 도전하는 세 가지 새로운 문제를 제안합니다. 24게임, 창의적 글쓰기 및 크로스워드(표 1). 이러한 과제에는 연역적, 수학적, 상식적, 어휘적 추론 능력과 체계적인 계획 또는 검색을 통합하는 방법이 필요합니다. 우리는 ToT가 다양한 수준의 사고, 사고를 생성하고 평가하는 다양한 방법 및 다양한 문제의 특성에 적응하는 다양한 검색 알고리즘을 지원할 만큼 일반적이고 유연하여 세 가지 과제 모두에서 우수한 결과를 얻는다는 것을 보여줍니다. 또한 이러한 선택이 체계적 절제를 통해 모델 성능에 어떤 영향을 미치는지 분석하고 LM을 보다 잘 훈련하고 사용하기 위한 향후 방향에 대해 논의합니다.2 배경 먼저 문제 해결을 위해 대규모 언어 모델을 사용하는 기존 방법을 공식화합니다.이는 저희의 접근 방식이 영감을 받았고 나중에 비교되었습니다.po는 매개변수 0을 갖는 사전 훈련된 LM을 나타내고 소문자 x, y, z, s, …..는 언어 시퀀스 iex = (x[1],, x[n])를 나타냅니다.여기서 각 x[i]는 토큰이므로 po(x) = []=1 Po(x[i]|x[1...i]).대문자 S는 언어 시퀀스 모음을 나타냅니다.입출력(IO) 프롬프팅은 LM을 사용하여 문제 입력 x를 출력 y로 바꾸는 가장 일반적인 방법입니다.y ~ Pe(y prompt 10(x)), 여기서 prompt 10(x)는 입력 x를 작업 지침 및/또는 소수 샷 입출력 예제로 래핑합니다. 단순화를 위해 Pe(출력 | 입력) = Pe(출력 프롬프트(입력))로 표시하겠습니다.그러면 IO 프롬핑을 y ~ ·P(y|x)로 공식화할 수 있습니다.프롬프트 IOCOT 생각의 사슬(CoT) 프롬핑[38]은 입력 x에서 출력 y로의 매핑이 사소하지 않은 경우(예: x가 수학 질문이고 y가 최종 숫자 답변인 경우)를 해결하기 위해 제안되었습니다.핵심 아이디어는 x와 y를 연결하기 위해 생각의 사슬 z₁, …, zn을 도입하는 것입니다.여기서 각 z¿는 문제 해결을 향한 의미 있는 중간 단계 역할을 하는 일관된 언어 시퀀스입니다(예: zi는 수학 QA에 대한 중간 방정식이 될 수 있음).CoT로 문제를 풀기 위해 각 생각 ZipoT(zi | x, 21-1)을 순차적으로 샘플링한 다음 출력 y ~ CoT(y|x, z1...n)를 수행합니다. 실제로, [z1...n, Y] (z1...n, y|x)는 연속적인 언어 시퀀스로 샘플링되고, 사고의 분해(예: 각 z¿가 구, 문장 또는 문단인가)는 모호한 상태로 남습니다.CoT를 통한 자기 일관성(CoT-SC) [36]은 k iid 사고 사슬을 샘플링하는 앙상블 방식입니다. [z\...n, y(i)] CoT (z1...n, y|x) (i = 1... k), 그런 다음 가장 빈번한 출력을 반환합니다. ~ COT Ро = ~ arg maxy #{iy (i) y}. CoT-SC는 일반적으로 동일한 문제에 대해 서로 다른 사고 과정(예: 동일한 정리를 증명하는 서로 다른 방법)이 있고, 더 풍부한 사고 집합을 탐색함으로써 출력 결정이 더 충실할 수 있기 때문에 CoT보다 개선되었습니다. 그러나 각 체인 내에서는 다양한 사고 단계에 대한 로컬 탐색이 없으며, &quot;가장 빈번한&quot; 휴리스틱은 출력 공간이 제한될 때(예: 객관식 QA)에만 적용됩니다.3 사고의 나무: LM을 이용한 의도적인 문제 해결 진정한 문제 해결 프로세스에는 탐색을 시작하기 위해 사용 가능한 정보를 반복적으로 사용하여야 하며, 이를 통해 더 많은 정보가 공개되고, 마침내 해결책을 얻는 방법이 발견됩니다.- Newell et al. [21] 인간의 문제 해결에 대한 연구에 따르면 사람들은 조합적 문제 공간(노드가 부분적 해결책을 나타내고 브랜치가 이를 수정하는 연산자에 해당하는 트리)을 검색한다고 합니다[21, 22]. 어떤 브랜치를 선택할지는 문제 공간을 탐색하고 문제 해결자가 해결책을 찾을 수 있도록 안내하는 휴리스틱에 의해 결정됩니다. 이 관점은 일반적인 문제를 해결하기 위해 LM을 사용하는 기존 접근 방식의 두 가지 주요 단점을 강조합니다.1) 로컬하게는 사고 과정 내의 다양한 연속성, 즉 트리의 브랜치를 탐색하지 않습니다. 2) 전 세계적으로, 그들은 이러한 다양한 옵션을 평가하는 데 도움이 되는 어떤 유형의 계획, 룩어헤드 또는 백트래킹도 통합하지 않습니다.인간의 문제 해결의 특징인 것으로 보이는 일종의 휴리스틱 가이드 검색입니다.이러한 단점을 해결하기 위해 LM이 생각에 대한 여러 추론 경로를 탐색할 수 있도록 하는 패러다임인 Tree of Thoughts(ToT)를 소개합니다(그림 1(c)).ToT는 모든 문제를 트리에 대한 검색으로 구성하며, 각 노드는 입력과 지금까지의 생각의 순서를 갖는 부분 솔루션을 나타내는 상태 s = [x, z1...]입니다.ToT의 구체적인 인스턴스화에는 네 가지 질문에 답하는 것이 포함됩니다.1. 중간 프로세스를 생각 단계로 분해하는 방법;2. 각 상태에서 잠재적인 생각을 생성하는 방법;3. 상태를 휴리스틱하게 평가하는 방법;4. 사용할 검색 알고리즘.1. 생각 분해.CoT가 명시적인 분해 없이 생각을 일관되게 샘플링하는 반면, ToT는 문제 속성을 활용하여 중간 생각 단계를 설계하고 분해합니다. 표에서 보듯이, 다양한 문제에 따라 생각은 몇 개의 단어(크로스워드), 한 줄의 방정식(24게임) 또는 한 단락의 쓰기 계획(창의적 작문)이 될 수 있습니다. 일반적으로 생각은 LM이 유망하고 다양한 샘플을 생성할 수 있을 만큼 &quot;작아야&quot; 하지만(예: 책 전체를 생성하는 것은 일반적으로 너무 &quot;크기&quot; 때문에 일관성이 없음), LM이 문제 해결에 대한 전망을 평가할 수 있을 만큼 &quot;크어야&quot; 합니다(예: 토큰 하나를 생성하는 것은 일반적으로 너무 &quot;작기&quot; 때문에 평가할 수 없음). 2. 생각 생성기 G(po, s, k). 트리 상태 s = [x, z1...]이 주어지면 다음 생각 단계에 대한 k 후보를 생성하는 두 가지 전략을 고려합니다. COT (a) CoT 프롬프트에서 iid 생각 샘플(창의적 작문, 그림 4): 2(j) CoT (zi+1|s) = pco (zi+1|x, z1...i) (j = 1 ……. k). 이 방법은 생각 공간이 풍부할 때(예: 각 생각이 문단일 때) 더 효과적이며, iid 샘플은 다양성으로 이어진다.(b) &quot;제안 프롬프트&quot;(24개 게임, 그림 2; 크로스워드, 그림 6)를 사용하여 생각을 순차적으로 제안한다: [2(1),..., 2(k)] ~ ppr suggests (1... k) (2+1초). 이 방법은 생각 공간이 더 제한적일 때(예: 각 생각이 단어나 줄일 때) 더 효과적이므로 동일한 맥락에서 다른 생각을 제안하면 중복을 피할 수 있다.3. 상태 평가자 V(pe, S). 다양한 상태의 경계가 주어지면 상태 평가자는 문제를 해결하기 위한 진행 상황을 평가하여 탐색 알고리즘이 어떤 상태를 어떤 순서로 계속 탐색할지 결정하는 휴리스틱 역할을 한다.휴리스틱은 탐색 문제를 해결하기 위한 표준적인 접근 방식이지만 일반적으로 프로그래밍(예: DeepBlue [3])되거나 학습(예: AlphaGo [29])된다. 우리는 LM을 사용하여 상태에 대해 의도적으로 추론하는 세 번째 대안을 제안합니다. 해당되는 경우 이러한 의도적인 휴리스틱은 프로그래밍된 규칙보다 더 유연할 수 있으며 학습된 모델보다 샘플 효율성이 더 높을 수 있습니다. 사고 생성기와 유사하게 우리는 상태를 독립적으로 또는 함께 평가하는 두 가지 전략을 고려합니다. (a) 각 상태를 독립적으로 평가합니다. V(po, s)(s) ~ pvalue (vs) Vs ES, 여기서 값 프롬프트는 상태 s에 대해 추론하여 스칼라 값 v(예: 1-10) 또는 휴리스틱하게 값으로 변환될 수 있는 분류(예: 확실/가능성/불가능)를 생성합니다. 이러한 평가적 추론의 기초는 문제와 사고 단계에 따라 다를 수 있습니다. 이 작업에서 우리는 몇 가지 룩어헤드 시뮬레이션을 통한 평가를 탐구합니다(예: 5, 5, 14가 5 + 5 + 14를 통해 24에 도달할 수 있음을 빠르게 확인하거나 &quot;hot_l&quot;이 &quot;_&quot;에 &quot;e&quot;를 채워 &quot;inn&quot;을 의미할 수 있음을 확인합니다). 또한 상식(예: 123은 24에 도달하기에는 너무 작음 또는 어떤 단어도 &quot;tzxc&quot;로 시작할 수 없음)을 고려합니다. 전자가 &quot;좋은&quot; 상태를 촉진할 수 있는 반면, 후자는 &quot;나쁜&quot; 상태를 제거하는 데 도움이 될 수 있습니다. 이러한 평가는 완벽할 필요가 없으며 의사 결정에 대략적으로 도움이 되면 됩니다. (b) 상태 간 투표: V(pe, S)(s) = 1[s = s*], 여기서 &quot;좋은&quot; 상태 s* 투표(s*|S)는 투표 프롬프트에서 S의 다른 상태를 의도적으로 비교하여 ~ 투표에서 제외됩니다. 문제 성공 여부를 직접 평가하기 어려운 경우(예: 구절 일관성) 대신 다른 부분 솔루션을 비교하고 가장 유망한 솔루션에 투표하는 것이 자연스럽습니다. 이는 다음과 유사한 정신입니다. 영어: &quot;단계별&quot; 자기 일관성 전략, 즉 &quot;탐색할 상태&quot;를 다중 선택 QA로 캐스팅하고 LM 샘플을 사용하여 이에 투표합니다. 두 전략 모두 LM에 여러 번 요청하여 값이나 투표 결과를 집계하여 시간/자원/비용을 보다 충실하고 견고한 휴리스틱으로 교환할 수 있습니다. 알고리즘 1 ToT-BFS(x, po, G, k, V,T,b) 필요 사항: 입력 x, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 너비 제한 b. 따라서 {x} = t = 1,, T에 대해 S←{[s, z] | s Є St−1, Zt Є G(pe, s, k)} Vt ← V (po, St) Starg maxscs₁₁|S|=bses Vt(s) end for SES return G(pe, arg maxSEST VT(S), 1) 알고리즘 2 ToT-DFS(s, t, po, G, k, V, T, Uth) 필요: 현재 상태 s, 단계 t, LM po, 사고 생성기 G() 및 크기 제한 k, 상태 평가기 V(), 단계 제한 T, 임계값 th if t T then 기록 출력 G(pe, s, 1) end if for s&#39;ЄG(pe, s, k) do ▷ 정렬된 후보 if V (pe, {s&#39;})(s) &gt; Vthres then ▷ 가지치기 DFS (s&#39;, t + 1) end if end for 4. 검색 알고리즘. 마지막으로, ToT 프레임워크 내에서 트리 구조에 따라 다양한 검색 알고리즘을 플러그 앤 플레이 방식으로 사용할 수 있습니다. 비교적 간단한 두 가지 검색 알고리즘을 살펴보고, 더 진보된 알고리즘(예: A* [11], MCTS [2])은 추후 작업을 위해 남겨둡니다. (a) 너비 우선 탐색(BFS)(알고리즘 1)은 단계당 가장 유망한 b 상태 집합을 유지합니다. 이것은 트리 깊이가 한계(T3)이고 초기 사고 단계를 평가하여 작은 집합으로 정리할 수 있는(b ≤ 5) 24게임과 창의적 글쓰기에 사용됩니다. (b) 깊이 우선 탐색(DFS)(알고리즘 2)은 가장 유망한 상태를 먼저 탐색하여 최종 출력에 도달할 때까지(t &gt; T) 또는 상태 평가자가 현재 s에서 문제를 해결하는 것이 불가능하다고 판단할 때까지(V(po, {s})(s) ≤ vth, 값 임계값 Vth의 경우) 진행합니다. 후자의 경우, s의 서브 트리를 정리하여 탐색을 활용으로 교환합니다. 두 경우 모두 DFS는 탐색을 계속하기 위해 s의 부모 상태로 후퇴합니다.개념적으로 ToT는 LM을 사용한 일반적인 문제 해결 방법으로서 여러 가지 이점이 있습니다.(1) 일반성.IO, COT, CoT-SC 및 자체 정제는 ToT의 특수한 경우로 볼 수 있습니다(즉, 깊이와 폭이 제한된 트리, 그림 1).(2) 모듈성.기본 LM과 사고 분해, 생성, 평가 및 검색 절차는 모두 독립적으로 다양할 수 있습니다.(3) 적응성.다른 문제 속성, LM 기능 및 리소스 제약 조건을 수용할 수 있습니다.(4) 편의성.추가 교육이 필요하지 않으며 사전 교육된 LM만으로 충분합니다.다음 섹션에서는 이러한 개념적 이점이 어떻게 다양한 문제에서 강력한 경험적 성능으로 변환되는지 보여줍니다.4 실험 표준 IO 프롬프팅 또는 사고 사슬(CoT) 프롬프팅을 사용하여 최첨단 언어 모델인 GPT-4[23]에서 샘플링하더라도 어려운 세 가지 작업을 제안합니다. 우리는 입력 출력 생각 게임 4개 숫자(49 10 13) 도달하는 방정식(13-9)*(10-4)=3 중간 방정식(13-9=4(왼쪽 4,4,10); 104-6(왼쪽 4,6); 4*6=24) #ToT 단계창의적 글쓰기 4개의 무작위 문장 4개 문장으로 끝나는 4개 단락의 구절 짧은 글쓰기 계획(1. 연결하는 책 소개...)5x5 크로스워드 10개 단서(h1. 제시됨;..) 5x5 글자: 표시됨; WIRRA; 사용 가능; ... 단서를 채울 단어: (h1. 표시됨; v5. 사용 가능; ...) 5-10(변수) 표 1: 작업 개요. 입력, 출력, 생각 예는 파란색으로 표시되었습니다. 생각의 나무(ToT)에서 의도적으로 검색하면 더 나은 결과가 나오고, 더 중요한 것은 언어 모델을 사용하여 검색이나 계획이 필요한 문제를 해결하는 흥미롭고 유망한 새로운 방법이 생깁니다. 달리 명시하지 않는 한, 샘플링 온도가 0.7인 Chat Completion 모드 GPT-4¹를 사용하여 실험을 수행합니다. 4.1 Game ofGame of 24는 수학적 추론 과제로, 목표는 4개의 숫자와 기본적인 산술 연산(+-*/)을 사용하여 24를 얻는 것입니다. 예를 들어, 입력 값 &quot;4 9 10 13&quot;이 주어졌을 때, 솔루션 출력은 &quot;(104) * (13 -9) = 24&quot;가 될 수 있습니다. 입력: 4 9 1010-4-(왼쪽: 6913) 14+9=(왼쪽: 10 13 13) (a) 프롬프트 제안 {예시 하나} 입력: 4 9 10가능한 다음 단계: 13-6-(왼쪽:79) 13-9=(왼쪽 4 6) 4+6=(왼쪽: 10) 4*6=(왼쪽: 24) (b) 값 프롬프트 주어진 숫자가 24에 도달할 수 있는지 평가합니다(확실/가능성/불가능) 10 14:10 +14= 24. 물론입니다 {예제 더 보기} 10 13생각 생성 4+9 13 (왼쪽: 10 13 13) LM 10 4 6 (왼쪽: 6913) LM {...줄 더 보기...} 생각 평가(13-10) 13 3 13 =10+13 +13 36 이렇게 큰 숫자로는 24를 얻을 수 없습니다.불가능 그림 2: 24 게임에서의 ToT.LM은 (a) 생각 생성 및 (b) 평가를 요청받습니다.작업 설정.인간의 풀기 시간에 따라 쉬운 것부터 어려운 것까지 정렬된 1,362개의 게임이 있는 4nums.com에서 데이터를 스크래핑하고, 테스트를 위해 901-1,000으로 색인된 비교적 어려운 게임의 하위 집합을 사용합니다. 각 작업에 대해 24와 같고 입력 숫자를 정확히 한 번만 사용하는 유효한 방정식인 경우 출력을 성공으로 간주합니다. 100개 게임에 대한 성공률을 메트릭으로 보고합니다. 기준선. 5개의 컨텍스트 내 예제가 있는 표준 입출력(IO) 프롬프트를 사용합니다. 사고의 사슬(CoT) 프롬프트의 경우 각 입출력 쌍을 나머지 두 숫자에서 작동하는 3개의 중간 방정식으로 확장합니다. 예를 들어, 입력 &quot;4 9 10 13&quot;이 주어지면 생각은 &quot;13 - 9 = 4(왼쪽: 4 4 10); 10 - 4 = 6(왼쪽: 4 6); 4 * 6 = 24(왼쪽: 24)&quot;가 될 수 있습니다. 각 게임에 대해 평균 성능을 위해 100회 IO 및 CoT 프롬프트를 샘플링합니다. 또한 100개의 CoT 샘플에서 대부분의 출력을 가져오는 CoT 자체 일관성 기준선과 최대 10번의 반복에 대한 IO 샘플 기반의 반복적 정제 방식을 고려합니다. 각 반복에서 LM은 출력이 올바르지 않으면 &quot;실수를 반성하고 정제된 답을 생성&quot;하도록 모든 이전 기록에 따라 조건이 지정됩니다. 방정식의 정확성에 대한 실제 피드백 신호를 사용한다는 점에 유의하세요. ToT 설정. 24번째 게임을 ToT로 구성하려면 생각을 각각 중간 방정식인 3단계로 분해하는 것이 자연스럽습니다. 그림 2(a)에서 볼 수 있듯이 각 트리 노드에서 나머지 숫자를 추출하고 LM에 다음 가능한 단계를 제안하도록 요청합니다. 모든 3가지 생각 단계에 동일한 &quot;프롬프트 제안&quot;을 사용하지만 4개의 입력 숫자가 있는 예가 하나뿐입니다. ToT에서 너비 우선 탐색(BFS)을 수행하며 각 단계에서 최상의 b = 5 후보를 유지합니다. 그림 2(b)에 표시된 것처럼 ToT에서 의도적인 BFS를 수행하기 위해 LM이 24에 도달하는 것과 관련하여 각 생각 후보를 &quot;확실/어쩌면/불가능&quot;으로 평가하도록 합니다. 목표는 몇 번의 룩어헤드 시도로 판결을 내릴 수 있는 정확한 부분 솔루션을 촉진하고 &quot;너무 크거나 작음&quot; 상식에 따라 불가능한 부분 솔루션을 제거하고 나머지는 &quot;어쩌면&quot;으로 유지하는 것입니다. 각 생각에 대해 3번 값을 샘플링합니다. ¹실험은 2023년 5월 5일~16일 사이에 수행되었습니다.방법 성공 (a) 방문한 노드의 성공률 (b) 각 단계에서 샘플이 실패함 CoT IO 프롬프트 7.3% ToT (b=5) CoT 프롬프트 4.0% 0.0.CoT-SC (k=100) 9.0% ToT(저희) (b=1) 45% 0.0.ToT(저희) (b=5) 74% IO(k 중 최고) IO+ 정제(k=10) 0.0.27% IO(100 중 최고) 33% COT(k 중 최고) TOT(b=1...5) COT(100 중 최고) 49% 0.1 2 3 4 정답 표 2: 24개 게임 결과.그림 3: 24개 게임 (a) 척도 분석 및 (b) 오류 분석.결과. 표 2에서 볼 수 있듯이 IO, CoT, CoT-SC 프롬프팅 방법은 작업에서 성능이 좋지 않아 각각 7.3%, 4.0%, 9.0%의 성공률만 달성합니다. 반면 b = 1의 폭을 가진 ToT는 이미 45%의 성공률을 달성하는 반면 b = 5는 74%를 달성합니다. 또한 k 개의 샘플 중 가장 좋은 것(1 ≤ k ≤ 100)을 사용하여 성공률을 계산하여 IO/COT에 대한 오라클 설정을 고려합니다. IO/COT(k 개의 가장 좋은 것)를 ToT와 비교하기 위해 b = 15에서 ToT에서 작업당 방문한 트리 노드를 계산하고 그림 3(a)에 5개의 성공률을 매핑하여 IO/COT(k 개의 가장 좋은 것)를 밴딧에서 k 개의 노드를 방문하는 것으로 처리합니다. 놀랍지 않게도 CoT는 IO보다 확장성이 뛰어나고 100개의 가장 좋은 CoT 샘플은 49%의 성공률을 달성하지만 ToT에서 더 많은 노드를 탐색하는 것보다는 여전히 훨씬 나쁩니다(b &gt; 1). 오류 분석. 그림 3(b)는 CoT 및 ToT 샘플이 작업에 실패한 단계, 즉 생각(CoT에 있는) 또는 모든 b 생각(ToT에 있는)이 유효하지 않거나 도달할 수 없는 단계를 분석합니다. 24. 주목할 점은 CoT 샘플의 약 60%가 첫 번째 단계 또는 동등하게 처음 세 단어(예: &quot;4 + 9&quot;)를 생성한 후에 이미 작업에 실패했다는 것입니다. 이는 왼쪽에서 오른쪽으로 직접 디코딩하는 데 따른 문제점을 강조합니다. 4. 창의적 쓰기 다음으로, 입력이 4개의 무작위 문장이고 출력이 각각 4개의 입력 문장으로 끝나는 4개의 문단이 있는 일관된 구절이어야 하는 창의적 쓰기 작업을 고안합니다. 이러한 작업은 개방적이고 탐색적이며 창의적 사고와 고수준 계획에 도전합니다. 작업 설정. randomwordgenerator.com에서 무작위 문장을 샘플링하여 100개의 입력을 형성하고 각 입력 제약 조건에 대한 기준 진실 구절은 없습니다. GPT-4가 대부분의 경우 입력 제약 조건을 따를 수 있다는 것을 알게 되었으므로 두 가지 방법으로 구절 일관성을 평가하는 데 중점을 두었습니다. GPT-4 제로샷 프롬프트를 사용하여 1~10점의 스칼라 점수를 제공하거나 인간의 판단을 사용하여 다양한 방법의 출력 쌍을 비교합니다. 전자의 경우 5개 점수를 샘플링하여 각 작업 출력에 대해 평균을 내며 이 5개 점수는 일반적으로 일관성이 있으며 출력 전체에서 평균 약 0.56의 표준 편차가 있음을 발견했습니다. 후자의 경우 저자 하위 집합을 맹검 연구에 사용하여 CoT 대 ToT에서 생성된 구절 쌍의 일관성을 비교했습니다. 여기서 구절의 순서는 100개의 입력에 대해 무작위로 뒤집습니다. 기준선. 작업의 창의적인 특성을 감안할 때 IO 및 CoT 프롬프트는 모두 제로샷입니다. 전자는 LM이 입력 제약 조건을 고려하여 일관된 구절을 직접 생성하도록 하는 반면, 후자는 LM이 먼저 간략한 계획을 세운 다음 구절을 쓰도록 합니다. 즉, 계획은 중간 사고 단계 역할을 합니다. 우리는 작업당 10개의 IO 및 CoT 샘플을 생성합니다. 또한 각 작업에 대한 무작위 IO 샘플 위에 반복적-세분화(k≤5) 방법을 고려합니다. 여기서 LM은 입력 제약 조건과 마지막으로 생성된 구절에 따라 구절이 이미 &quot;완벽하게 일관성이 있는지&quot; 여부를 결정하고 그렇지 않으면 정제된 구절을 생성합니다. = LM 첫 번째 5개 구절 1, ToT 설정은 선택 사항이 하나뿐이기 때문입니다. 깊이 2(및 중간 사고 단계가 1개만 있음)로 ToT를 구축하여 k 5개 계획을 생성하고 최상의 계획에 투표한 다음(그림 4) 마찬가지로 최상의 계획에 따라 k를 생성한 다음 최상의 계획에 투표합니다. 여기서 폭 제한 b = 단계당 유지됩니다. 간단한 제로샷 투표 프롬프트(&quot;아래의 선택 사항을 분석한 다음 지침에 가장 유망한 것을 결론 내립니다&quot;)를 사용하여 두 단계 모두에서 5개 투표를 샘플링합니다. 결과. 그림 5(a)는 100개 과제에 걸친 평균 GPT-4 점수를 보여주며, ToT(7.56)가 평균적으로 IO(6.19) 및 CoT(6.93)보다 더 일관된 구절을 생성하는 것으로 간주됩니다. 이러한 자동 메트릭은 노이즈가 있을 수 있지만, 그림 5(b)는 100개의 구절 쌍 중 41개에서 인간이 CoT보다 ToT를 선호하는 반면, 21개에서만 CoT를 ToT보다 선호한다는 것을 보여줌으로써 결과를 확인시켜 줍니다(다른 38개 쌍은 &quot;비슷하게 일관성이 있음&quot;으로 발견됨). 마지막으로 반복적 정제는 이 자연어 작업에서 더 효과적입니다.입력 입력 | (b) 계획 I 계획 1 계획 구절 구절 4개의 짧은 단락으로 구성된 일관된 구절을 작성합니다. 각 단락의 마지막 문장은 다음과 같아야 합니다. 1. 그냥 손으로 서 있으면 핸드스탠드를 하는 것은 어렵지 않습니다. 2. 그 공간에서 구운 스테이크 냄새가 난다는 사실에 그는 깜짝 놀랐습니다. 3. 그녀를 들어올리려는 남자가 마음에 들지 않자 수화를 사용하기 시작했습니다. 4. 당신을 아는 사람마다 당신에 대한 인식이 다릅니다. 계획 1. 핸드스탠드를 하는 기술을 소개하고 설명합니다. 2. 다음에 대한 이야기로 전환합니다. 우주인이 우주에 간 첫 경험 3. 여성이 원치 않는 주의를 피하기 위해 수화를 사용하는 상황을 설명합니다. 4. 마지막 문단은 모든 사람이 다른 사람에 대해 서로 다른 인식을 가지고 있는 방법을 설명합니다. 계획 1. 핸드스탠드를 도전을 수용하는 은유로 언급하는 특이한 자기계발 책의 소개. 2. 우주의 냄새를 포함하여 우주인으로부터 배운 예상치 못한 것에 대해 논의합니다. 3. 술집에서 원치 않는 주의를 피하기 위한 여성의 영리한 전략을 설명합니다. 4. 자신에 대한 다른 인식이 어떻게 자신의 정체성을 형성할 수 있는지 생각해 보세요. 계획 3-1.... 2... 0/5 투표 3/5 투표 n/5 투표 투표 각 선택 사항을 자세히 분석합니다. 선택 사항 1은 필수 종료 문장을 통합했지만 문단 간의 명확한 연결이 부족한 것처럼 보입니다. {...} 선택 사항 2는 필수 종료 문장을 사용하여 자기계발 책의 내용을 제시함으로써 흥미로운 관점을 제공합니다. 문단을 자기 계발과 도전 수용이라는 주제로 연결하여 일관된 구절을 만듭니다. {...} 최상의 선택은 2입니다. 그림 4: 무작위로 선택된 창의적 작문 과제에서 의도적인 검색 단계. 입력이 주어지면 LM은 5가지 다른 계획을 샘플링한 다음 5번 투표하여 어느 계획이 가장 좋은지 결정합니다. 다수의 선택은 결과적으로 동일한 샘플 투표 절차로 출력 구절을 작성하는 데 사용됩니다.(a) GPT-4 일관성 점수 (b) 인간의 일관성 비교 방법 성공률(%) 문자 단어 게임IO COT ToT(저희) 38.7 1440.6 15.678 60+최상의 상태 -자르기 -역추적 82.4 67.565.4 41.554.6 2010 COT ToT 10 TOT+개선 +개선 COTTOT 유사한 ToT &gt; COT 표 3: 미니 크로스워드 결과. 그림 5: 창의적 작문 결과. IO 일관성 점수는 6.19에서 7.67로, ToT 일관성 점수는 7.56에서 7.91로 향상되었습니다. ToT 프레임워크에서 생각 생성을 위한 세 번째 접근 방식으로 생각할 수 있다고 믿습니다. iid나 순차적으로 생성되는 대신 오래된 생각을 다듬어서 새로운 생각이 생길 수 있습니다. 4.3 미니 크로스워드 24게임과 창의적 글쓰기에서 ToT는 비교적 얕습니다. 최종 출력에 도달하는 데 최대 3개의 생각 단계가 필요합니다. 여기서는 자연어를 포함하는 더 어려운 검색 문제로 5 × 5 미니 크로스워드를 탐구합니다. 다시 말하지만, 목표는 단순히 작업을 해결하는 것이 아닙니다. 보다 일반적인 크로스워드는 LM 대신 대규모 검색을 활용하는 특수 NLP 파이프라인[34]으로 쉽게 해결할 수 있습니다. 오히려 의도적인 추론을 휴리스틱으로 사용하여 자체 생각을 탐구하고 자체 탐색을 안내하는 일반적인 문제 해결기로서 LM의 한계를 탐구하는 것을 목표로 합니다. 작업 설정. GooBix에서 데이터를 스크래핑하는데, 여기에는 5 x 5 미니 크로스워드 게임 156개가 들어 있습니다. 인접한 게임에 비슷한 단서가 들어 있는 것을 관찰했으므로 테스트를 위해 인덱스 1, 6, ..., 91, 96이 있는 게임 20개를 사용하고 프롬프트를 위해 게임 136, 141, 146, 151, 156을 사용합니다. 각 작업의 경우 입력은 수평 단서와 5개의 수직 단서를 설명하고 출력은 크로스워드를 풀기 위한 5 x 5 == 25글자 보드여야 합니다. 평가를 위해 세 가지 성공 수준을 고려합니다. 정답률(게임당), 단어(게임당 10개), 게임입니다. 기준선. IO 프롬프트에 5개의 예제 입출력 쌍을 제공하고 CoT 프롬프트에 h1..5, v1..5 순서로 중간 단어를 추가로 포함합니다. 각 프롬프트를 샘플로 실행하고 결과의 평균을 냅니다. ToT 설정. 우리는 상태가 더 이상 유망하지 않을 때까지 가장 유망한 후속 단어 단서를 계속 탐색하는 깊이 우선 탐색(알고리즘 2)을 활용하고, 그런 다음 부모 상태로 되돌아가 대안적 생각을 탐색합니다. 탐색을 다루기 쉽게 하기 위해 후속 생각은 채워진 단어나 문자를 변경하지 않도록 제한되므로 ToT는 최대 10개의 중간 단계를 갖습니다. 생각 생성을 위해 각 상태에서 기존의 모든 생각(예: 그림 6(a)의 상태에 대한 &quot;h2.motor; h1.tasks&quot;)을 나머지 단서에 대한 문자 제약 조건(예: &quot;v1.To heap: tm___;...&quot;)으로 변환하고 제안 프롬프트를 5번 프롬프트하여 다음 단어에 채울 위치와 내용에 대한 후보를 제시합니다. 중요한 점은 LM에 다른 생각에 대한 신뢰 수준을 제공하고, aggregatetasks motor 입력 단서 salon h2.motor h1.tasks (b) (역추적) DFS 생각 제안 순서 h4. salon (확실) aggregate h4. salon v5. srdry (낮음) h3. grand v3. string (높음) v3. string ...... 상태 평가자(각 단서 위에) v3. Pretentious; flowery: __ sure h3.grand h4.salon v1. To heap: tm_s_ {..} impossible ...... (서브 트리가 정리됨) v5. 건조기; 더 건조: sr_n_ {...} 아마도 그림 6: 미니 크로스워드에서 (a) 깊이 우선 탐색(DFS)을 위해 우선순위 대기열에 생각을 제안하고 집계하는 방법, (b) 남은 단어 단서를 채울 가능성에 따라 상태를 평가하고 LM에서 채울 수 없는 남은 단서가 있는 경우 정리하는 방법. 그런 다음 DFS는 부모 상태로 백트래킹하여 단서에 대한 다음 유망한 생각을 탐색합니다. 이를 제안에 걸쳐 탐색할 다음 생각의 정렬된 목록을 얻습니다(그림 6(a)). 상태 평가의 경우 마찬가지로 각 상태를 남은 단서에 대한 문자 제약 조건으로 변환한 다음 제약 조건을 고려하여 채울 수 있는지 각 단서를 평가합니다. 남은 단서가 채우기 &quot;불가능&quot;한 것으로 간주되면(예: &quot;v1. 쌓으려면: tm_s_&quot;) 상태의 서브트리 탐색이 정리되고 DFS는 부모 상태로 백트래킹하여 다음 유망한 생각을 탐색합니다. DFS 검색 단계를 100으로 제한하고 가장 깊이 탐색된 상태(여러 개일 경우 처음 탐색된 상태)를 최종 출력으로 렌더링합니다.결과.표 3에서 볼 수 있듯이 IO 및 CoT 프롬프팅 방법은 단어 수준 성공률이 16% 미만으로 성과가 좋지 않은 반면 ToT는 모든 지표를 크게 개선하여 단어 수준 성공률이 60%에 도달하고 20개 게임 중 4개를 해결합니다.IO 및 CoT에는 다른 단서를 시도하거나 결정을 변경하거나 후퇴할 수 있는 메커니즘이 없다는 점을 감안할 때 이러한 개선은 놀라운 일이 아닙니다.오라클 및 절제 연구.작업당 오라클 최상의 DFS 상태(경험적으로 결정된 최상의 상태 대신)에서 출력할 때 ToT 성능은 더 높고 실제로 20개 게임 중 7개를 해결합니다(표 3, &quot;+최상의 상태&quot;).이는 간단한 출력 휴리스틱을 쉽게 개선할 수 있음을 나타냅니다. 흥미롭게도, 때때로 크로스워드 게임이 실제로 풀렸을 때, 상태 평가자는 여전히 일부 단어를 &quot;불가능&quot;하다고 간주하고 가지치기할 수 있습니다. 아마도 5 × 5 크로스워드는 의도적으로 GPT-4가 인식할 수 없는 희귀하거나 구식 단어가 있기 때문일 것입니다². 가지치기 휴리스틱으로서의 상태 평가가 불완전하다는 점을 감안하여, 우리는 또한 가지치기를 제거하는 것을 탐구했고, 일반적으로 성능이 더 나쁘다는 것을 발견했습니다(표 3, &quot;-prune&quot;). 그러나 실제로 20개 중 4개 게임에 대한 올바른 솔루션을 찾을 수 있었습니다(휴리스틱을 통해 1개만 출력하지만). 그 중 3개는 ToT+pruning으로 100단계 이내에 해결할 수 없는 게임입니다. 따라서 이 경우 DFS 가지치기를 위한 더 나은 휴리스틱이 문제 해결에 중요합니다. 마지막으로, 최대 20단계 동안 가장 유망한 단서를 계속 채우는 제거를 실행하여 백트래킹의 중요성을 확인하여 덮어쓰기를 허용합니다. 이것은 b = 1의 폭 한계를 갖는 &quot;탐욕스러운&quot; BFS 검색과 유사하며, 단어 수준 성공률이 20%에 불과하여 성능이 좋지 않습니다(표 3, &quot;-backtrack&quot;).5 관련 작업 계획 및 의사 결정.스마트한 계획 및 의사 결정은 미리 정의된 목표를 달성하는 데 중요합니다.LM은 방대한 양의 세계 지식과 인간의 사례로 훈련되므로 문제 설정 및 환경 상태에 따라 합리적인 계획을 제안하는 것이 가능한 풍부한 상식을 이미 흡수한 것으로 알려져 있습니다[12, 42, 37, 13, 35, 41, 40]. 제안된 ToT 접근 방식은 각 문제 해결 단계에서 잠재적으로 실행 가능한 여러 계획을 동시에 고려하고 가장 유망한 계획을 진행함으로써 기존 계획 공식화를 확장합니다.사고 샘플링과 가치 피드백 간의 통합은 계획 및 의사 결정 메커니즘을 유기적으로 통합하여 솔루션 트리 내에서 효과적인 검색을 가능하게 합니다. 반면, 전통적인 의사결정 절차는 일반적으로 강화 학습(예: CHAI [33])에서와 같이 전담 보상 및 정책 모델을 교육해야 하는 반면, 우리는 LM 자체를 사용하여 의사결정에 대한 가치 추정치를 제공합니다.RAP [9]는 언어 모델 2를 내부 세계 모델로 계획하는 것으로 취급하는 동시 작업이며, ToT와 유사한 MCTS 기반 방법을 제안합니다.그러나 그 작업은 우리보다 간단하고 프레임워크에는 다양한 트리 검색 알고리즘을 통합할 수 있는 모듈성이 부족합니다.자기 반성.LLM을 사용하여 자체 예측의 실행 가능성을 평가하는 것은 문제 해결에서 점점 더 중요한 절차가 되고 있습니다.[28, 20, 24]는 LM이 생성 후보에게 피드백을 제공하는 &quot;자기 반성&quot; 메커니즘을 도입했습니다. [4]는 LM 자체가 코드 실행 결과를 기반으로 생성한 피드백 메시지를 주입하여 LM의 코드 생성 정확도를 향상시킵니다. 마찬가지로 [17]은 작업 및 상태에 대한 &quot;비평&quot; 또는 검토 단계를 도입하여 컴퓨터 운영 작업을 해결하기 위한 다음 작업을 결정합니다. 영어: 우리와 매우 관련성이 높은 또 다른 최근 연구는 &quot;자체 평가 안내 디코딩&quot;[39]입니다. 우리 방법과 유사하게 자체 평가 디코딩도 확률적 빔 검색 디코딩에서 샘플링된 리프로 트리 검색 절차를 따르며, 이후 LLM 자체가 신중하게 준비된 자체 평가 프롬프트로 이를 평가합니다. 그러나 그들의 접근 방식은 생각을 코드로 표현하는 PAL 공식[8]을 사용하는데, 이는 이 논문에서 고려하는 창의적 글쓰기와 같은 어려운 과제를 해결하기 어렵게 만듭니다. 따라서 우리의 생각 트리 공식은 더 다재다능하며 GPT-4가 표준 프롬프트로 매우 낮은 정확도만 달성하는 어려운 과제를 처리합니다. 프로그램 안내 LLM 생성. 우리의 제안은 또한 체계적인 절차[14, 44, 6, 43] 또는 기호 프로그램 안내로 LM의 행동을 구성하는 최근의 발전과 관련이 있습니다. 예를 들어, Schlag et al.[27]은 알고리즘 검색 절차에 LM을 내장하여 질문에 대한 단계별 답변과 같은 문제를 해결하는 데 도움을 주며, 여기서 검색 트리는 답변을 제공할 수 있는 관련 문단으로 확장됩니다. 그러나 이 접근 방식은 LM 자체의 생각 대신 외부 문단을 샘플링하여 트리를 확장하고 반성이나 투표 단계가 없다는 점에서 우리 방식과 다릅니다.또 다른 접근 방식인 LLM+P[18]는 한 걸음 더 나아가 실제 계획 프로세스를 고전적 플래너에게 위임합니다.고전적 검색 방법.마지막으로, 우리의 접근 방식은 문제 해결을 위한 고전적 검색 방법의 현대적 해석으로 간주될 수 있습니다.예를 들어, LM의 자체 평가에 의해 각 검색 노드의 휴리스틱이 제공되는 A*[10]와 같은 휴리스틱 검색 알고리즘으로 간주될 수 있습니다.이 관점에서 볼 때, 우리의 방법은 A* 검색에서 영감을 받았지만 LM이 빔 검색 또는 상위 k 샘플링 디코딩을 개선하는 데 효율적인 룩어헤드 휴리스틱을 도입한 NeuroLogic A*esque 디코딩[19]과도 관련이 있습니다.그러나 이 방법은 문장 생성 작업에 국한되는 반면, 우리의 프레임워크는 값 피드백으로 보호되는 복잡한 다단계 문제 해결을 위해 설계되었습니다.6 토론 제한 사항 및 향후 방향. ToT와 같은 의도적인 검색은 GPT-4가 이미 탁월한 많은 기존 작업에 필요하지 않을 수 있으며(부록 B.1 참조), 초기 단계로서 이 작업은 GPT-4에 도전하는 비교적 간단한 세 가지 작업만 탐색합니다(일부 GPT-3 실험 결과는 부록 B.2 참조) 및 LM과 통합된 더 나은 검색 및 계획 능력에 대한 요구. 그러나 더 현실적인 의사 결정 애플리케이션(예: 코딩, 데이터 분석, 로봇 공학 등)에 LM을 배포하기 시작하면서 더 복잡한 작업이 등장하여 이러한 연구 질문을 연구할 수 있는 새로운 기회를 제공할 수 있습니다. 또한 ToT와 같은 검색 방법은 작업 성능을 개선하기 위해 샘플링 방법보다 더 많은 리소스(예: GPT-4 API 비용)가 필요하지만 TOT의 모듈식 유연성 덕분에 사용자는 이러한 성능-비용 균형을 사용자 정의할 수 있으며 지속적인 오픈 소스 노력[32]을 통해 가까운 미래에 이러한 비용을 쉽게 줄일 수 있습니다. 비용 및 효율성에 대한 자세한 내용은 부록 B.3에 나와 있습니다. 마지막으로, 이 연구는 기성형 LM을 사용하는 데 중점을 두고 있으며, ToT 스타일의 고수준 반사실적 의사 결정(예: 다음 토큰을 예측하는 대신 다음 문단에 대한 잠재적 선택 사항에 대해 숙고)을 사용하여 LM을 미세 조정하면 LM의 문제 해결 능력을 향상시킬 수 있는 기회가 제공될 수 있습니다.
--- CONCLUSION ---
. LM의 연관적 &quot;시스템 1&quot;은 문제 해결에 대한 가능한 경로의 트리를 검색하는 &quot;시스템 2&quot;에 의해 유익하게 증강될 수 있습니다. 생각의 트리 프레임워크는 문제 해결에 대한 고전적 통찰력을 현대 LM에 대한 실행 가능한 방법으로 변환하는 방법을 제공합니다. 동시에 LM은 이러한 고전적 방법의 약점을 해결하여 창의적 글쓰기와 같이 쉽게 공식화되지 않는 복잡한 문제를 해결하는 방법을 제공합니다. 우리는 AI에 대한 고전적 접근 방식과 LM의 이러한 교차점을 흥미로운 방향으로 봅니다. Broader Impact ToT는 LM이 보다 자율적이고 지능적으로 결정을 내리고 문제를 해결할 수 있도록 하는 프레임워크입니다. 현재 작업은 추론 및 검색 문제로 제한되어 있지만 외부 환경 또는 인간과의 상호 작용을 포함하는 미래의 응용 프로그램은 잠재적인 위험을 초래할 수 있습니다(예: LM의 해로운 사용을 용이하게 함). 반면에 ToT는 결과 표현이 암묵적이고 저수준 토큰 값 대신 읽을 수 있는 고수준 언어 추론이기 때문에 모델 결정의 해석 가능성과 인간의 정렬 기회도 개선합니다. 감사의 말 SY와 KN은 Oracle Collaborative Research 상과 Grant No. 2239363에 따라 National Science Foundation의 지원에 감사드립니다. 이 자료에 표현된 모든 의견, 결과, 결론 또는 권장 사항은 저자의 것이며 반드시 National Science Foundation의 견해를 반영하는 것은 아닙니다. SY는 또한 Princeton의 Harold W. Dodds 펠로우십의 지원을 받고 있습니다. 참고문헌 [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. [2] C. Browne, EJ Powley, D. Whitehouse, SMM Lucas, PI Cowling, P. Rohlfshagen, S. Tavener, DP Liebana, S. Samothrakis, S. Colton. 몬테카를로 트리 탐색 방법 조사. IEEE 게임 분야의 계산 지능 및 AI 저널, 4:1–43, 2012. [3] M. Campbell, AJ Hoane Jr, F.-h. Hsu. 딥 블루. 인공지능, 134(1-2):57-83, 2002. [4] X. Chen, M. Lin, N. Schärli, D. Zhou. 대규모 언어 모델에 자체 디버깅을 가르치기, 2023. [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, et al. Palm: 경로를 통한 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. [6] A. Creswell 및 M. Shanahan. 대규모 언어 모델을 사용한 충실한 추론. arXiv 사전 인쇄본 arXiv:2208.14271, 2022. [7] ND Daw, Y. Niv 및 P. Dayan. 행동 제어를 위한 전두엽과 등외측 편도체 시스템 간의 불확실성 기반 경쟁. Nature neuroscience, 8(12):1704–1711, 2005. [8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Programaided language models, 2023. [9] S. Hao, Y. Gu, H. Ma, JJ Hong, Z. Wang, DZ Wang, and Z. Hu. 언어 모델을 통한 추론은 세계 모델을 통한 계획입니다. arXiv 사전 인쇄본 arXiv:2305.14992, 2023. [10] PE Hart, NJ Nilsson, and B. Raphael. 최소 비용 경로의 휴리스틱 결정을 위한 공식적 기반. IEEE 시스템 과학 및 사이버네틱스 저널, 4(2):100–107, 1968. doi: 10.1109/TSSC.1968.300136. [11] PE Hart, NJ Nilsson, 및 B. Raphael. 최소 비용 경로의 휴리스틱 결정을 위한 공식적 기반.IEEE 시스템 과학 및 사이버네틱스 저널, 4(2):100–107, 1968. [12] W. Huang, P. Abbeel, D. Pathak, 및 I. Mordatch. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트를 위한 실행 가능한 지식 추출, 2022. [13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. 내면의 독백: 언어 모델을 통한 계획을 통한 구체화된 추론.arXiv 사전 인쇄본 arXiv:2207.05608, 2022.[14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, RL Bras, and Y. Choi. 산파적 촉구: 재귀적 설명을 통한 논리적으로 일관된 추론.arXiv 사전 인쇄본 arXiv:2205.11822, 2022.[15] D. Kahneman. 빠르고 느린 생각.Macmillan, 2011.[16] D. Kahneman, S. Frederick, et al. 대표성 재검토: 직관적 판단에서의 속성 대체. 휴리스틱과 편향: 직관적 판단의 심리학, 49(49-81):74, 2002. [17] G. Kim, P. Baldi, S. McAleer. 언어 모델은 컴퓨터 작업을 해결할 수 있습니다, 2023. [18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, P. Stone. Llm+p: 최적의 계획 능력을 갖춘 대규모 언어 모델 강화, 2023. [19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, RL Bras, L. Qin, Y. Yu, R. Zellers, NA Smith, Y. Choi. 신경학적 a*esque 디코딩: 룩어헤드 휴리스틱을 사용한 제약 텍스트 생성. 북미 컴퓨터 언어학회 지부, 2021. [20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, BP Majumder, S. Gupta, A. Yazdanbakhsh, P. Clark. 자체 개선: 자체 피드백을 통한 반복적 개선, 2023. [21] A. Newell, JC Shaw, HA Simon. 일반적인 문제 해결 프로그램 보고서. IFIP 의회, 256권, 64페이지. 펜실베이니아주 피츠버그, 1959. [22] A. Newell, HA Simon 외. 인간의 문제 해결. Prentice-Hall, 1972. [23] OpenAI. Gpt-4 기술 보고서.ArXiv, abs/2303.08774, 2023. [24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, B. Faltings. Refiner: 중간 표현에 대한 추론 피드백, 2023. [25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. 생성적 사전 학습을 통한 언어 이해 향상.OpenAI 블로그, 2018. [26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. [27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li. 대규모 언어 모델 프로그램, 2023. [28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: 동적 메모리와 자기 반성을 갖춘 자율 에이전트, 2023. [29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. 인간의 지식 없이 바둑 게임 마스터하기. nature,(7676):354-359, 2017. [30] SA Sloman. 두 가지 추론 시스템에 대한 경험적 사례. Psychological bulletin, 119(1): 3, 1996. [31] KE Stanovich. Who is Reason? Studies of individual differences in reasoning. Psychology Press, 1999. [32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: 오프라인 강화 학습을 통한 작업 지향 대화를 위한 챗봇 AI. 2022년 북미 계산언어학 협회 회의록: 인간 언어 기술, 4471-4491쪽, 2022.[34] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, D. Klein. 자동 크로스워드 풀기. arXiv 사전 인쇄본 arXiv:2205.09665, 2022. [35] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, RK-W. Lee, E.-P. Lim. Plan-and-solve 프롬프팅: 대규모 언어 모델을 통한 제로샷 사고 사슬 추론 개선, 2023. [36] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, D. Zhou. 자기 일관성은 언어 모델에서 사고의 연쇄 추론을 개선합니다.arXiv 사전 인쇄본 arXiv:2203.11171, 2022. [37] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. 설명, 계획 및 선택: 대규모 언어 모델을 사용한 대화형 계획은 오픈 월드 멀티태스크 에이전트를 가능하게 합니다, 2023. [38] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. 사고의 연쇄 촉발은 대규모 언어 모델에서 추론을 이끌어냅니다.arXiv 사전 인쇄본 arXiv:2201.11903, 2022. [39] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. 분해는 자체 평가 안내 디코딩을 통해 추론을 향상시킵니다, 2023. [40] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, D. Schuurmans. 의사 결정을 위한 기초 모델: 문제, 방법 및 기회, 2023. [41] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, Y. Cao. ReAct: 언어 모델에서 추론과 행동의 시너지 효과. arXiv 사전 인쇄본 arXiv:2210.03629, 2022. [42] S. Zhang, Z. Chen, Y. Shen, M. Ding, JB Tenenbaum, C. Gan. 코드 생성을 위한 대규모 언어 모델로 계획. 2023년 제11회 학습 표현 국제 컨퍼런스에서. URL https: s: //openreview.net/forum?id=Lr8c00tYbfL. [43] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al. 최소에서 최대로의 프롬프트를 통해 대규모 언어 모델에서 복잡한 추론이 가능. arXiv 사전 인쇄본 arXiv:2205.10625, 2022. [44] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. 협력 추론 유도 언어 모델을 통해 수학 단어 문제 해결. arXiv 사전 인쇄본 arXiv:2210.16257, 2022.A 코드, 프롬프트, 궤적 모든 코드는 https://github.com/princeton-nlp/tree-of-thought-11m에서 제공됩니다. 모든 프롬프트는 https://github.com/princeton-nlp/tree-of-thought-11m/tree/master/src/tot/prompts에서 제공됩니다. 궤적은 https://github.com/princeton-nlp/tree-of-thought-11m/tree/master/logs에서 제공됩니다. B 추가 실험 결과 언어 모델의 역량 프런티어를 탐구하고 확장하려는 동기를 감안하여 본 논문의 실험은 최첨단 언어 모델(GPT-4)을 사용한 설정과 이에 도전하기 위해 고안된 세 가지 어려운 작업에 초점을 맞추었습니다. 여기서는 LLM이 약하거나 쉬운 작업을 사용한 추가 실험을 보고하고 비용과 효율성에 대해 논의합니다. GSM8K StrategyQA GPT-GPT-3.GPT-4 GPT-3.IOIO 7.3% 6% IO 6.4.COTCOT 4.0% 3% COT 6.5.TOTTOT 74% 19% TOT 7.6.표 4: 제로 샷 ToT와 GPT-4를 사용한 새로운 작업.표 5: GPT-4와 GPT-3.5를 사용한 24개의 게임.표 6: GPT-4와 GPT-3.5를 사용한 창의적 글쓰기.B.1 제로 샷 ToT를 사용한 새로운 작업(GSM8k, StrategyQA)으로의 확장 더 일반적인 NLP 작업은 GPT-4에 너무 쉽고 ToT가 필요하지 않을 수 있지만(이것이 더 어려운 새로운 작업을 고려한 이유), 새로운 작업에 ToT를 적용하는 것이 간단할 수 있다고 생각합니다. 예를 들어, 우리는 GSM8K와 StrategyQA를 위해 몇 줄의 추가 코드로 창의적 글쓰기와 유사한 간단하고 일반적인 제로샷 ToT-BFS를 구현했습니다(문제 해결 전략 5개를 샘플링한 다음 가장 좋은 전략에 투표하고, 가장 좋은 전략에 따라 해결책 5개를 샘플링한 다음 가장 좋은 전략에 투표). # 새로운 작업의 답변 형식 정의 gsm8k_format = &quot;답은 n입니다&quot; 여기서 n은 숫자입니다&#39; strategyqa_format = &#39;&quot;답은 예입니다&quot; 또는 &quot;답은 아니요입니다&quot;, # 제로샷 io 프롬프트 정의 standard_prompt = &#39;다음 질문에 {format}으로 답하세요: {input}&#39; # 제로샷 cot 및 제로샷 tot에 대한 사고 형식 정의 cot_prompt = &quot;&quot;&quot;다음 질문에 답하세요: {input} 전략을 만든 다음 작성하세요. 출력 형식은 다음과 같아야 합니다. 전략: 질문에 답하는 방법에 대한 전략. 답변: 질문에 대한 답변. {format}으로 끝나야 합니다. &quot;&quot;&quot; # 다음에 사용되는 제로샷 투표 정의 zero-shot tot vote prompt = &quot;&quot;&quot;지시와 여러 선택지가 주어지면 가장 유망한 선택지를 결정하세요. 각 선택지를 자세히 분석한 다음 마지막 줄에서 &quot;가장 좋은 선택지는 {s}입니다&quot;라고 결론을 내리세요. 여기서 s는 선택지의 정수 ID입니다. &quot;&quot;&quot;우리는 100개의 무작위 GSM8K 테스트와 StrategyQA 개발 질문의 하위 집합을 평가했습니다. 표 4에서 볼 수 있듯이 예상대로 ToT는 두 작업 모두에서 CoT보다 향상되었습니다(하지만 GPT-+ CoT가 이미 이러한 작업에서 매우 뛰어나고 StrategyQA의 병목 현상이 추론이 아닌 외부 지식이라는 점을 감안하면 약간만 향상되었습니다). 계산 비용을 고려할 때 기존 NLP 작업의 경우 더 작은 LLM + ToT를 시도하거나 GPT-4 + CoT의 추론에 도전하는 어려운 작업의 경우 GPT-4 + ToT를 시도하는 것이 더 적합합니다. B.2 새로운 LMS로의 확장(GPT-3.5) ToT가 다른 LLM과 어떻게 작동하는지 이해하기 위해 Creative Writing(표 6)과 Game of 24(표 5)에 대해 GPT-3.5-turbo도 실행했습니다. 두 작업 모두에서 &quot;ToT &gt; CoT &gt; IO&quot;는 GPT-3.5에 대해 여전히 사실입니다. Creative Writing에서 GPT-3.5+ToT가 GPT-4+IO보다 성능이 뛰어나고 유사한 GPT-4+CoT로, 이는 ToT가 약한 언어 모델에서도 잘 작동할 수 있음을 시사합니다. 24개의 게임에서(작동하도록 1-샷 제안 프롬프트를 3-샷으로 변경) GPT-3.5+ToT의 19%는 GPT-4+ToT의 74%보다 훨씬 나쁩니다. 생성 대 평가의 중요성을 더 잘 이해하기 위해 GPT-4 생성 + GPT-3.5 평가(64%) 및 GPT-3.5 생성 + GPT-4 평가(31%)를 실행했습니다. 이는 게임의 병목 현상이 사고 생성이며, 다른 생성/평가 언어 모델이 비용을 줄이면서 적절한 결과를 얻을 수 있음을 시사합니다. B.3 비용 및 효율성 ToT를 실행하려면 IO 또는 CoT 프롬프트보다 훨씬 더 많은 계산이 필요합니다. 예를 들어, 24개의 게임(아래 표 7)에서 ToT로 문제를 해결하려면 5.5k 완료 토큰, 약 100개의 CoT 시도(6.7k 토큰)가 필요합니다. 하지만 ToT의 성과는 100개의 독립적인 CoT 시험 중 최고보다 더 좋습니다. Game ofIO(100개 중 최고) 토큰 생성/촉발 사례당 비용 성공 COT(100개 중 최고) TOT 1.8k/1.0k 6.7k/2.2k $0.33% $0.49% 5.5k/1.4k $0.74% 표 7: Game of 24에 대한 비용 분석. 창의적 글쓰기(아래 표 8)에서 ToT는 완료 토큰과 돈 비용의 약 5배가 소요되며 이는 b 5로 직관적이고 대부분 토큰이 생성된 구절임을 발견했습니다. = 창의적 글쓰기 토큰 생성/촉발 IO COT TOT 0.9k / 0.4k 0.9k / 0.4k 4k / 2.9k 사례당 비용 $0.$0.$0.표 8: 24개의 게임에 대한 비용 분석. 따라서 24개의 게임과 창의적 글쓰기의 주요 ToT 실험을 완료하는 데 드는 비용은 약 0.74 × 100+ 0.32 x 100 = 106달러입니다. 크로스워드의 DFS 실험도 100달러 이내여야 합니다. 일반적으로 ToT의 비용과 효율성은 사용된 프롬프트와 검색 알고리즘에 크게 좌우되며 CoT보다 5~100배 더 많은 생성된 토큰이 필요할 수 있습니다. 몇 가지 실행 가능한 통찰력: • • CoT가 어려움을 겪는 의도적인 추론이 필요한 작업에 ToT를 사용하는 것이 좋습니다. ToT의 유연성은 일부 성능-비용 상쇄를 허용합니다.예를 들어, BFS에서 빔 크기 또는 투표 수 변경, few-shot 대 zero-shot 프롬핑, GPT-3.5 대 GPT-4 등입니다.어떤 리소스 제약이나 성능 목표에 따라 설정을 구성할 수 있습니다.• 효율성을 개선할 수 있는 여지가 많습니다.예를 들어, BFS는 솔루션을 찾았을 때 조기에 중지하거나 일부 생각이 &quot;불가능&quot;할 때 빔 크기를 줄일 수 있습니다.• 모델이 더 강력한 인텔리전스를 달성하기 위해서는 실제로 더 많은 계산이 필요하다고 생각하며, 장기적으로 (오픈소스) LM이 훨씬 저렴하고 효율적이 될 것이므로 이는 차단 문제가 되어서는 안 됩니다.또한 생각 생성 및/또는 평가를 위해 LM을 더 잘 훈련/미세 조정하는 방법에 대한 좋은 방향이기도 합니다.
