--- ABSTRACT ---
우리는 이미지-텍스트 사전 학습 방법론인 Contrastive Feature Masking Vision Transformer(CFM-ViT)를 제시합니다. 이는 오픈 어휘 객체 감지(OVD)를 위한 이미지 및 영역 수준 표현의 동시 학습을 달성합니다. 우리의 접근 방식은 마스크 자동 인코더(MAE) 목표를 대조 학습 목표에 결합하여 지역화 작업에 대한 표현을 개선합니다. 표준 MAE와 달리 우리는 고전적인 MAE 방법에서 관례적인 픽셀 공간이 아닌 공동 이미지-텍스트 임베딩 공간에서 재구성을 수행하여 모델이 영역 수준 의미를 더 잘 학습하게 합니다. 또한 우리는 사전 학습 중에 위치 임베딩을 무작위로 삭제하여 이미지-텍스트 사전 학습과 감지 미세 조정 간의 스케일 차이를 해결하기 위해 Positional Embedding Dropout(PED)을 도입합니다. PED는 감지 성능을 개선하고 동결된 ViT 백본을 영역 분류기로 사용할 수 있게 하여 감지 미세 조정 중에 오픈 어휘 지식을 잊어버리는 것을 방지합니다. LVIS 오픈 어휘 탐지 벤치마크에서 CFM-ViT는 최첨단 33.9 APr을 달성하여 최상의 접근 방식을 7.6포인트 능가하고 더 나은 제로샷 탐지 전송을 달성합니다. 마지막으로 CFM-ViT는 강력한 이미지 수준 표현을 획득하여 제로샷 이미지-텍스트 검색 벤치마크에서 8개 지표에서 최첨단을 능가합니다. 1.
--- INTRODUCTION ---
실제 세계에서 광범위한 객체를 감지하는 능력은 컴퓨터 비전과 머신 러닝의 기본입니다.이는 자율 에이전트에서 검색 엔진에 이르기까지 광범위한 응용 프로그램에 동력을 제공합니다.안타깝게도, 현재까지 대부분의 최신 객체 감지기는 수동으로 주석이 달린 영역과 클래스 레이블에 의존하는데, 이는 노동 집약적이고 10³ 범주의 순서를 넘어 확장하는 데 비실용적입니다.오픈 어휘 감지(OVD)라는 새로운 작업은 이미지-텍스트 쌍을 사용하여 학습하고 테스트 시간에 사용자의 텍스트 쿼리를 사용하여 객체 감지의 어휘 제한을 해결하기 위해 도입되었습니다[65].오픈 어휘 감지기는 범주를 개별 클래스 레이블이 아닌 텍스트 임베딩으로 표현하여 객체를 예측할 수 있습니다.사전 학습: 대조적 특징 마스킹(a) 특징 재구성(d) dec(b) 마스크 대조적(b)(c) ViT 텍스트 enc &quot;천장에 걸려 있는 크리스마스 조명&quot; image @ text &quot;두 아이가 연못 옆을 걷고 있다&quot; 미세 조정: 오픈 어휘 감지 init. 사전 학습된 ViT 오픈 어휘 감지 det 헤드 조이스틱: 68% 조이스틱: 44% 조이스틱: 47% Esofa 침대: 61% VIT VIT(미세 조정)(고정) 이미지 페이퍼백 책: 44% 그림 1: 오픈 어휘 감지를 위해 더 많은 픽셀 및 영역 정보를 캡처하기 위해 비전 변환기를 사전 학습하기 위해 CFM-ViT를 제안합니다. CFM-ViT는 대조 이미지-텍스트 사전 학습 위에 마스크된 대조 기능을 예측합니다. (위) (d) 재구성된 이미지 기능(왼쪽 위 참조)과 (e) 쿼리 텍스트 임베딩 간의 유사도 맵(c)을 시각화합니다. CFM-ViT는 (b) 크게 잘린 이미지에서 (c) 전체 이미지 의미를 올바르게 예측합니다. (아래) 오픈 어휘 감지기는 사전 학습된 지식을 유지하기 위해 동결된 ViT 백본을 활용하고 기본 및 새로운 객체 클래스(새로운 클래스만 표시됨)를 감지할 수 있습니다. 훈련 중에는 사용할 수 없습니다. 지식 증류[18, 13], 약한 감독[74], 자가 학습[71, 49, 68], 동결 백본[33]과 같은 다양한 기술이 제안되었습니다. 일반적으로 CNN 백본이 이러한 접근 방식에서 활용됩니다. 비전 변환기가 이미지 이해에서 상당한 인기를 얻었으므로[12, 66, 21, 3] 비전 변환기를 기반으로 하는 오픈 어휘 감지기를 탐색하는 것이 중요합니다[42]. 게다가 우리가 아는 한, 대부분의 현재 OVD 연구는 사전 학습된 VisionLanguage 모델(VLM)(예: CLIP[47])의 가용성을 가정하고 이미지 수준 사전 학습과 객체 수준 미세 조정 간의 차이를 극복하기 위한 적응 또는 미세 조정 기술을 제안합니다[18, 13, 71, 68, 49]. 그러나 이러한 VLM은 일반적으로 분류 및 검색과 같은 이미지 수준 작업에 최적화되어 있기 때문에 다운스트림 오픈 어휘 감지에 중요한 사전 학습 중에 픽셀 및 영역 수준 정보를 적절하게 활용하지 못합니다.우리는 오픈 어휘 객체 감지를 위해 더 자세한 픽셀/영역 정보를 캡처하도록 비전 변환기를 사전 학습하는 간단한 프레임워크인 CFM-ViT(Contrastive Feature Masking Vision Transformer)를 제시합니다(그림 1).MAE[21]에서 영감을 받아 사전 학습 중에 객체 표현을 향상시키기 위해 마스크 자동 인코딩의 개념을 채택했습니다.그러나 MAE와 달리 우리는 대조 이미지 텍스트 학습에 대한 보조 목표로 픽셀 공간이 아닌 공동 이미지-텍스트 임베딩 공간에서 예측을 수행합니다.이 추가 목표는 대조 학습에서 직교 신호를 제공하고 이미지 수준 작업을 손상시키지 않고 다운스트림 감지 작업에 도움이 됩니다.또한 일반적으로 해상도가 낮고 객체 중심의 사전 학습 데이터에 대한 과적합을 해결하기 위해 위치 임베딩 드롭아웃(PED)을 제안합니다. 사전 학습 중에 위치 임베딩을 무작위로 삭제함으로써 PED는 모델이 고해상도 감지 데이터에 더 잘 일반화되는 더욱 견고한 표현을 학습하도록 돕습니다. 게다가 PED는 동결된 ViT 인코더를 오픈 어휘 영역 분류기로 사용할 수 있게 하여 감지 시 오픈 어휘 지식을 잊어버리는 것을 방지합니다. 우리는 널리 사용되는 LVIS 및 COCO 오픈 어휘 감지 벤치마크에서 CFM-ViT를 평가합니다. 우리의 최고 성능 모델은 LVIS에서 33.9 APr을 얻어 시스템 수준에서 이전의 최고 접근 방식보다 7.6 APr 더 높습니다. COCO 벤치마크에서 CFM-ViT는 최초의 ViT 기반 모델을 나타내며 가상 레이블이나 약한 감독을 사용하지 않고도 매우 경쟁력 있는 새로운 AP를 달성합니다. 검색에 최적화되지는 않았지만 CFM-ViT는 12개의 이미지-텍스트 검색 벤치마크 메트릭 중 8개에서 비슷하거나 더 큰 용량의 최첨단 방법보다 성능이 뛰어납니다. 요약: • 우리는 대조적 특징 마스킹을 통해 오픈 어휘 탐지를 위한 지역화 단서를 학습하기 위한 이미지-텍스트 사전 학습 방법론(CFM-ViT)을 제시합니다. • 우리는 이미지-텍스트 사전 학습과 탐지 미세 조정 간의 격차를 메우기 위해 위치 임베딩 드롭아웃(PED)을 제안합니다. 이를 통해 탐지 미세 조정 중에 오픈 어휘 지식을 잊어버리는 것을 방지하기 위해 동결된 ViT 인코더를 사용할 수 있습니다. • CFM-ViT는 LVIS 오픈 어휘 탐지 벤치마크에서 최첨단 APr을 달성하고, COCO와 Objects365로의 제로샷 전송에서 매우 경쟁력 있는 성능을 보이며, 제로샷 이미지-텍스트 검색 벤치마크의 8개 지표에서 SOTA보다 우수한 성능을 보입니다. 이러한 발견이 커뮤니티가 이미지-텍스트 사전 학습의 관점에서 오픈 어휘 탐지를 탐구하도록 격려하기를 바랍니다. 2.
--- RELATED WORK ---
s 언어 감독 개방형 어휘 인식. 개방형 어휘 인식을 위한 학습 표현은 일반 지능의 특징입니다. DeViSE[16] 및 ConSE[43]와 같은 초기 개척 작업은 딥 합성 신경망을 사용하여 제로 샷 인식을 위한 공유 이미지-텍스트 임베딩 공간을 구성했습니다. 원시 인터넷 데이터에서 이미지와 텍스트의 동시 발생을 활용하기 위해 연구자들은 이미지 태그[4, 9, 30], 캡션[8, 24, 50, 55], 대체 텍스트[29, 51], 이미지 검색 쿼리[47], 페이지 제목[5] 또는 이러한 소스의 조합[5]과 같은 다양한 데이터 소스를 탐색했습니다. 모델링 관점에서 대조 학습은 단순성, 확장성 및 제로 샷, 퓨어 샷 및 전체 미세 조정 전송 설정에서의 다양성으로 인해 인기 있는 패러다임이 되었습니다[46, 47, 39, 10, 36]. 대부분의 작업이 이미지 수준 이해에 초점을 맞추는 반면, 우리는 오픈 어휘 탐지 작업에 필수적인 이미지-텍스트 사전 학습에서 영역 수준 정보의 학습을 탐구합니다.자기 감독 객체 표현 학습. 탐지를 위한 주석 확장은 상당한 과제를 제시합니다. 그 결과, 자기 감독 방식으로 객체 표현을 학습하기 위한 많은 노력이 이루어졌습니다. 이러한 접근 방식은 대체로 대조적 또는 생성적이라고 분류할 수 있습니다. 이러한 대조적 접근 방식은 일반적으로 픽셀 또는 영역 수준 대조 학습을 위해 슬라이딩 윈도우[59], 객체 제안[57, 25] 또는 포인트 샘플[1]을 사용합니다. 생성적
--- METHOD ---
이미지 및 영역 수준 표현의 동시 학습을 달성하는 개방형 어휘 객체 감지(OVD)를 위한 ology. 우리의 접근 방식은 마스크 자동 인코더(MAE) 목표를 대조 학습 목표에 결합하여 지역화 작업에 대한 표현을 개선합니다. 표준 MAE와 달리 우리는 고전적인 MAE 방법에서 관례적인 픽셀 공간이 아닌 공동 이미지-텍스트 임베딩 공간에서 재구성을 수행하여 모델이 영역 수준 의미를 더 잘 학습하게 합니다. 또한 사전 학습 중에 위치 임베딩을 무작위로 삭제하여 이미지-텍스트 사전 학습과 감지 미세 조정 간의 스케일 차이를 해결하기 위해 위치 임베딩 삭제(PED)를 도입합니다. PED는 감지 성능을 개선하고 동결된 ViT 백본을 영역 분류기로 사용할 수 있게 하여 감지 미세 조정 중에 개방형 어휘 지식을 잊어버리는 것을 방지합니다. LVIS 오픈 어휘 감지 벤치마크에서 CFM-ViT는 최첨단 33.9 APr을 달성하여 최상의 접근 방식보다 7.6포인트 앞서고 더 나은 제로 샷 감지 전송을 달성합니다. 마지막으로 CFM-ViT는 강력한 이미지 수준 표현을 획득하여 제로 샷 이미지-텍스트 검색 벤치마크에서 8개 지표에서 최첨단보다 우수한 성능을 발휘합니다. 1. 서론 실제 세계에서 광범위한 객체를 감지하는 능력은 컴퓨터 비전과 머신 러닝에 기본이 됩니다. 이는 자율 에이전트에서 검색 엔진에 이르기까지 광범위한 응용 프로그램을 구동합니다. 안타깝게도 현재까지 대부분의 최신 객체 감지기는 수동으로 주석이 달린 영역과 클래스 레이블에 의존하는데, 이는 노동 집약적이며 10³ 범주의 순서를 넘어 확장하는 데 비실용적입니다. 오픈 어휘 감지(OVD)라는 새로운 작업이 도입되어 학습을 위해 이미지-텍스트 쌍을 사용하고 테스트 시간에 사용자의 텍스트 쿼리를 사용하여 객체 감지의 어휘 제한을 해결합니다[65]. 영어: 오픈 어휘 감지기는 범주를 개별 클래스 레이블이 아닌 텍스트 임베딩으로 표현하여 객체를 예측할 수 있습니다 사전 학습: 대조적 특징 마스킹 (a) 특징 재구성 (d) dec (b) 마스크 대조적 (b) (c) ViT 텍스트 enc &quot;천장에 걸려 있는 크리스마스 조명&quot; 이미지 @ 텍스트 &quot;두 아이가 연못 옆을 걷고 있다&quot; 미세 조정: 사전 학습된 ViT를 사용한 오픈 어휘 감지 init. 오픈 어휘 감지 det 헤드 조이스틱: 68% 조이스틱: 44% 조이스틱: 47% 에소파 침대: 61% VIT VIT(미세 조정) (고정) 이미지 페이퍼백 책: 44% 그림 1: 오픈 어휘 감지를 위해 더 많은 픽셀 및 영역 정보를 캡처하기 위해 비전 변환기를 사전 학습하기 위해 CFM-ViT를 제안합니다. CFM-ViT는 대조적 이미지-텍스트 사전 학습 위에 마스크된 대조적 특징을 예측합니다. (위) (d) 재구성된 이미지 특징(왼쪽 위 참조)과 (e) 쿼리 텍스트 임베딩 간의 유사도 맵(c)을 시각화합니다. CFM-ViT는 (b) 크게 잘린 이미지에서 (c) 전체 이미지 의미를 올바르게 예측합니다. (아래) 저희의 오픈 어휘 감지기는 사전 훈련된 지식을 유지하기 위해 동결된 ViT 백본을 활용하고 기본 및 새로운 객체 클래스를 감지할 수 있습니다(새로운 클래스만 표시됨). 훈련 중에는 사용할 수 없습니다. 지식 증류[18, 13], 약한 감독[74], 셀프 트레이닝[71, 49, 68], 동결된 백본[33]과 같은 다양한 기술이 제안되었습니다. 일반적으로 CNN 백본이 이러한 접근 방식에 활용됩니다. 비전 변환기가 이미지 이해에서 상당한 인기를 얻었기 때문에[12, 66, 21, 3], 비전 변환기[42]를 기반으로 하는 오픈 어휘 감지기를 탐색하는 것이 중요합니다. 게다가, 우리가 아는 한, 대부분의 최근 OVD 연구는 사전 학습된 VisionLanguage 모델(VLM)(예: CLIP [47])의 가용성을 가정하고 이미지 수준 사전 학습과 객체 수준 미세 조정 간의 차이를 극복하기 위한 적응 또는 미세 조정 기술을 제안합니다 [18, 13, 71, 68, 49]. 그러나 이러한 VLM은 일반적으로 분류 및 검색과 같은 이미지 수준 작업에 최적화되어 있기 때문에 사전 학습 중에 다운스트림 오픈 어휘 감지에 중요한 픽셀 및 영역 수준 정보를 적절하게 활용하지 못합니다. 우리는 오픈 어휘 객체 감지를 위해 더 자세한 픽셀/영역 정보를 캡처하도록 비전 변환기를 사전 학습하는 간단한 프레임워크인 CFM-ViT(Contrastive Feature Masking Vision Transformer)를 제시합니다(그림 1). MAE [21]에서 영감을 받아 사전 학습 중에 객체 표현을 향상시키기 위해 마스크 자동 인코딩 개념을 채택합니다. 그러나 MAE와 달리 우리는 픽셀 공간이 아닌 공동 이미지-텍스트 임베딩 공간에서 예측을 수행하여 대조적 이미지 텍스트 학습에 대한 보조적 목적으로 사용합니다. 이 추가 목적은 대조적 학습에서 직교 신호를 제공하고 이미지 수준 작업을 손상시키지 않으면서 다운스트림 감지 작업에 도움이 됩니다. 또한 일반적으로 해상도가 낮고 객체 중심의 사전 학습 데이터에 대한 과적합을 해결하기 위해 위치 임베딩 드롭아웃(PED)을 제안합니다. 사전 학습 중에 위치 임베딩을 무작위로 드롭함으로써 PED는 모델이 고해상도 감지 데이터로 더 잘 일반화되는 더욱 견고한 표현을 학습하도록 돕습니다. 게다가 PED는 동결된 ViT 인코더를 개방형 어휘 영역 분류기로 사용할 수 있게 하여 감지 시 개방형 어휘 지식을 잊어버리는 것을 방지합니다. 우리는 널리 사용되는 LVIS 및 COCO 개방형 어휘 감지 벤치마크에서 CFM-ViT를 평가합니다. 최고 성능을 내는 모델은 LVIS에서 33.9 APr을 얻어 시스템 수준에서 이전의 최고 접근 방식보다 7.6 APr을 앞지릅니다. COCO 벤치마크에서 CFM-ViT는 최초의 ViT 기반 모델을 나타내며 가상 레이블이나 약한 감독을 사용하지 않고도 매우 경쟁력 있는 새로운 AP를 달성합니다. 검색에 최적화되지는 않았지만 CFM-ViT는 12개 이미지-텍스트 검색 벤치마크 메트릭 중 8개에서 비슷하거나 더 큰 용량의 최첨단 방법보다 성능이 뛰어납니다. 요약하면 다음과 같습니다. • 대조적 특징 마스킹을 통해 오픈 어휘 감지를 위한 지역화 단서를 학습하는 이미지-텍스트 사전 학습 방법론(CFM-ViT)을 제시합니다. • 이미지-텍스트 사전 학습과 감지 미세 조정 간의 격차를 메우기 위해 위치 임베딩 드롭아웃(PED)을 제안합니다. 이를 통해 동결된 ViT 인코더를 사용하여 감지 미세 조정 중에 오픈 어휘 지식을 잊어버리는 것을 방지할 수 있습니다. • CFM-ViT는 LVIS 오픈 어휘 감지 벤치마크에서 최첨단 APr을 달성하고 COCO 및 Objects365로의 제로샷 전송에서 매우 경쟁력 있는 성능을 보였으며 제로샷 이미지-텍스트 검색 벤치마크의 8가지 지표에서 SOTA보다 우수한 성능을 보였습니다. 이러한 발견이 커뮤니티가 이미지-텍스트 사전 학습의 관점에서 오픈 어휘 감지를 탐색하도록 격려하기를 바랍니다. 2. 관련 연구 언어 감독 오픈 어휘 인식. 오픈 어휘 인식을 위한 학습 표현은 일반 지능의 특징입니다. DeViSE [16] 및 ConSE [43]와 같은 초기 개척 작업은 딥 합성곱 네트워크를 사용하여 제로샷 인식을 위한 공유 이미지-텍스트 임베딩 공간을 구성했습니다. 연구자들은 원시 인터넷 데이터에서 이미지와 텍스트의 동시 발생을 활용하기 위해 이미지 태그[4, 9, 30], 캡션[8, 24, 50, 55], 대체 텍스트[29, 51], 이미지 검색 쿼리[47], 페이지 제목[5] 또는 이러한 소스의 조합[5]과 같은 다양한 데이터 소스를 탐색했습니다. 모델링 관점에서 대조 학습은 단순성, 확장성 및 제로 샷, 퓨어 샷 및 전체 미세 조정 전송 설정에서의 다양성으로 인해 인기 있는 패러다임이 되었습니다[46, 47, 39, 10, 36]. 이러한 작업의 대부분이 이미지 수준 이해에 초점을 맞추는 반면, 우리는 오픈 어휘 감지 작업에 필수적인 이미지 텍스트 사전 학습에서 영역 수준 정보의 학습을 탐구합니다. 자기 감독 객체 표현 학습. 감지를 위한 주석 확장은 상당한 과제를 제시합니다. 결과적으로, 많은 노력이 자기 지도 방식으로 객체 표현을 학습하기 위해 이루어졌습니다. 이러한 접근 방식은 대체로 대조적 또는 생성적이라고 분류할 수 있습니다. 이러한 대조적 접근 방식은 일반적으로 픽셀 또는 영역 수준 대조 학습을 위해 슬라이딩 윈도우[59], 객체 제안[57, 25] 또는 포인트 샘플[1]을 사용합니다. 생성적 방법은 픽셀[21], 저수준[3, 56]/고수준 이미지 특징[6, 73]과 같은 재구성 대상이 있는 마스크된 이미지 모델링을 사용하거나 대조 목적[27]과 결합합니다. 마스크된 이미지를 복원하는 방법을 학습함으로써 모델은 객체와 영역에 대해 학습해야 합니다. 그러나 이러한 자기 지도적 방법은 지역화 작업에 적합하지만 개방형 어휘 인식에 필요한 이미지-텍스트 학습이 부족합니다. 일부 최근 작업[58, 45, 67, 26, 14]은 기성품 CLIP 특징[47]을 예측 대상으로 활용하여 2단계 학습을 통해 마스크된 이미지 모델링을 향상시킵니다. 이 연구에서 우리는 단일 엔드투엔드 학습 단계에서 생성적 자기 지도 학습과 대조적 이미지-텍스트 학습을 공동으로 결합하는 새로운 접근 방식을 제안합니다. 일부 동시 연구에서는 제로샷 이미지 수준 작업이나 완전 지도 미세 조정에 대한 유사한 목표를 탐구했지만[11, 60, 54], 우리의 초점은 오픈 어휘 감지에 있습니다. 오픈 어휘 객체 감지 및 분할. 제로샷 감지는 영역 시각적 표현과 범주 단어 임베딩을 정렬[2, 48, 7, 69]하거나 생성 모델로 시각적 특징을 생성[20, 75]하여 제한된 학습 범주를 넘어 감지 모델을 향상시키는 것을 목표로 합니다. 오픈 어휘 감지[65]는 새로운 범주에 대한 이미지 텍스트 지도를 통합하여 제로샷 감지를 개선합니다. 이미지-텍스트 사전 학습의 출현으로, 많은 연구에서 이러한 사전 학습된 모델을 오픈 어휘 탐지 및 분할에 적용하는 것을 탐구했습니다[18, 71, 17, 35, 72]. 예를 들어, ViLD[18]는 이미지-텍스트 지식을 탐지기로 정제하는 반면, DetPro[13]는 범주 프롬프트 최적화를 통해 ViLD를 개선합니다. 또한, 영역-텍스트 자체 학습은 이미지 캡션 데이터[71], 분류 데이터[49] 및 레이블이 지정되지 않은 데이터[68]에서 시연되었습니다. 구문 접지[37], 약한 감독[74] 및 동결 모델[33] 접근 방식도 탐구되었습니다. 대부분의 방법은 CNN 백본에 의존하지만, 비전 변환기가 추진력을 얻고 있습니다[42, 72, 31, 34, 38]. 이전 연구는 사전 학습된 모델에 대한 미세 조정 또는 적응 전략에 초점을 맞춘 반면, 우리의 연구는 비전 변환기의 마스크 표현을 예측하여 이미지-텍스트 사전 학습을 개선하는 것을 목표로 합니다. 3. 방법 우리는 오픈 어휘 객체 감지 문제를 해결합니다. 훈련하는 동안 모델은 기본 범주의 감지 레이블에 액세스할 수 있지만 추론 단계에서는 일련의 새로운 범주에서 객체를 감지할 수 있어야 합니다. 이를 달성하기 위해 우리는 이전 연구[18, 71, 33]에 따라 사전 훈련된 비전 및 언어 모델(VLM)을 활용합니다. 그러나 기성품 사전 훈련된 VLM을 사용하는 대신 오픈 어휘 감지를 위해 비전 변환기[12]로 VLM을 더 잘 사전 훈련하는 방법을 보여줍니다. 3.1. 예비 단계: 전체 파이프라인 사전 훈련. 우리는 기존 연구[47, 29]에서 널리 사용되는 듀얼 인코더 이미지-텍스트 대조 모델을 채택합니다. 이미지 임베딩 {v}와 텍스트 임베딩 {1}은 이미지 및 텍스트 인코더의 마지막 레이어에서 글로벌 평균 풀링을 통해 얻습니다. 배치 B의 임베딩의 코사인 유사도는 학습 가능한 온도 7로 조정되며 InfoNCE 손실에 대한 입력입니다[44, 47]. 이미지-텍스트(I2T) 대조 손실은 다음과 같이 공식화됩니다. L12T = BB log(: Bi=exp(vili/T) Σexp(vil/7) -). (1) 텍스트-이미지(T2I) 대조 손실은 내부/외부 합산 루프를 교환하여 I2T 손실과 대칭적입니다. 총 대조 손실 Lcon은 Lcon(L12T + LT21)/2에 의해 얻습니다. = 다운스트림 오픈 어휘 감지. 당사의 오픈 어휘 감지 알고리즘은 기존 작업[65, 18, 33, 31]을 따릅니다. 학습 시 감지된 각 영역 i에 대해 해당 영역 임베딩은 RoI-Align 기능입니다. 탐지 점수 pi는 CB의 영역 임베딩과 텍스트 임베딩 사이의 코사인 유사도이며, 소프트맥스가 뒤따릅니다. 텍스트 임베딩은 이미지-텍스트 사전 학습에서 동일한 텍스트 인코더에서 계산됩니다. 테스트 시간에 텍스트 임베딩은 CB에서 CB UCN과 &quot;배경&quot; 임베딩으로 확장됩니다. 또한 ViT 백본의 마지막 특징 맵에서 RoI-Align을 통해 영역 i의 VLM 임베딩을 추출합니다. VLM 점수 zi는 CBUCN 텍스트 임베딩과의 코사인 유사도입니다. 마찬가지로 탐지 점수 pi는 이제 CB UCN 텍스트 임베딩으로 계산됩니다. 오픈 어휘 시나리오에 대한 객체 탐지기는 기본 범주 CB의 레이블에서 학습되지만 테스트 시간에 기본 범주와 신규 범주(CBUCN)의 합집합을 탐지할 수 있어야 합니다. 기존 연구[65, 18]에 따라 고정 크기 분류기 계층을 기본 범주의 텍스트 임베딩으로 대체합니다.이미지-텍스트 사전 학습의 동일한 텍스트 인코더를 사용하여 사전 학습된 오픈 어휘 지식을 유지하기 위해 텍스트 임베딩을 계산합니다.“배경” 문구는 배경 범주를 나타내며, 어떤 CB 주석과도 일치하지 않는 제안은 배경으로 표시됩니다.기하학적 평균[18, 33]을 통해 얻은 앙상블 오픈 어휘 감지 점수 si: ens는 (1-a)입니다.ens Si = (1-ẞ) B · p if i Є CB · Pi if i Є CN (2) 여기서 a, ẞ Є [0, 1]은 기본 및 신규 범주의 가중치를 제어합니다.배경 점수는 감지 점수 pi에서 직접 나오므로 “배경” 문구가 있는 VLM 점수는 신뢰할 수 없는 경향이 있습니다.3.2. 대조적 특징 마스킹 본 방법은 대조적 이미지-텍스트 학습(3.1절)에 대한 보조 목표로 공동 이미지 텍스트 임베딩 공간에서 재구성을 수행합니다(그림 2-왼쪽 참조).마스크된 특징 재구성.MAE[22]에 따라 표현 학습을 위해 많은 이미지 토큰(예: 마스크 비율 75%)을 무작위로 마스크합니다.그러나 MAE와 달리 의미론에 대한 더 나은 학습을 장려하기 위해 원시 픽셀 대신 공동 이미지-텍스트 임베딩을 예측합니다.특히, 글로벌 평균 풀링 이전의 대조적 이미지 인코더의 출력 특징 {f}가 재구성 대상입니다.재구성된 특징 {f}와 마스크되지 않은 이미지 특징 {f} 사이의 코사인 거리를 손실 함수로 사용합니다.M을 마스크된 패치 인덱스의 집합이라 하고 재구성 손실 Lrec는 다음과 같이 마스크된 토큰에 대해서만 계산됩니다.B Lrec =B i=f.sg(f) Σ -), (3) |M| kЄM ||ƒ || · ||sg(ƒ) || 여기서 |M|은 마스크된 토큰의 수이고 sg는 정지 그래디언트를 나타냅니다. CFM-ViT 손실 전체는 Lcon + Lrec• 대조적 특징 마스킹에 의한 사전 학습 디코더 특징 재구성 손실 f + 위치 임베딩 v 대조적 손실 GAP ViT 인코더 가중치 공유 ViT 인코더 텍스트 인코더 마스킹 ↑ 위치 임베딩 드롭아웃 이미지 패치 다운스트림 오픈 어휘 감지 감지 손실 OVD 점수로서의 영역-텍스트 유사성 s 새로운 클래스 임베딩 -&gt; (테스트 시간에만 해당) 영역 감지 점수 p 기본 클래스 임베딩(학습 및 테스트 시간) 영역 VLM 점수 z Rol 정렬 Rol 정렬 감지 헤드 감지된 영역 텍스트 학습 경로 → 추론 경로 ViT 인코더(미세 조정) ViT 인코더(고정) + 위치 임베딩(업샘플링) 이미지 패치 그림 2: CFM-ViT 아키텍처: CFM-ViT의 이미지-텍스트 사전 학습(왼쪽)과 오픈 어휘 감지 미세 조정(오른쪽) 아키텍처를 모두 제시합니다. (왼쪽) 대조 학습을 기반으로, 우리는 조인트 이미지-텍스트 임베딩 공간에서 마스크된 토큰을 재구성하는 방법을 학습합니다. 또한, 우리는 사전 학습 중에 전체 PE를 무작위로 마스크하여 저해상도 위치 임베딩에 대한 과적합을 완화하고, 고해상도 다운스트림 감지 작업에 더 잘 적응하는 위치 임베딩 드롭아웃(PED)을 제안합니다. (오른쪽) 오픈 어휘 감지기는 미세 조정 중에 사전 학습된 ViT 백본으로 초기화됩니다. 감지된 영역 임베딩은 캐시된 카테고리 임베딩과 일치하여 영역 점수를 계산합니다. 추론에서, 우리는 동결된 ViT 백본을 활용하여 VLM 점수 z를 얻고, 이는 감지 점수 p와 결합하여 오픈 어휘 감지 점수 s(색상으로 보는 것이 가장 좋음)로 변환됩니다. 우리의 재구성 인코더는 대조 이미지 인코더와 동일(가중치 공유)하지만, 보이는 마스크되지 않은 토큰(예: 25%)에만 적용됩니다. 디코더는 인코딩된 가시적 토큰과 위치 임베딩이 추가된 학습 가능한 [마스크] 토큰을 사용합니다.대조적 브랜치 마스킹을 통한 더 빠른 학습.특징 재구성 브랜치는 마스킹 비율(예: 75%)에 따라 사전 학습에 계산 부담(예: 25%)을 추가합니다.이 비용은 대조 브랜치에 마스크된 토큰(M)만 공급하여 대조 및 재구성 인코더에 대한 입력 패치가 상호 배타적이고 동일한 재구성 대상 {fkЄM}을 생성함으로써 면제될 수 있습니다.표 5c의 절제 연구는 이 기술이 대조 학습의 학습 효율성을 유지하는 동시에 개방형 어휘 감지에서 기준선보다 상당한 이득을 달성함을 보여줍니다.위치 임베딩 드롭아웃.비전 트랜스포머 인코더에서 위치 임베딩은 첫 번째 패치화 계층 이후의 모든 토큰에 추가되어 이미지에서 각 패치의 위치를 제공합니다. 위치 임베딩은 이미지 분류/검색에 잘 작동하지만, 낮은 해상도의 객체 중심 이미지에 과적합되는 경향이 있으며 일반적으로 감지 작업에 사용되는 고해상도 이미지에서는 어려움을 겪습니다. 또한 감지에서 객체 인식은 이미지 수준이 아닌 영역 수준에서 발생합니다(예: 3.1절에서 영역 i에 대한 VLM 점수 zi 참조). 이는 이미지 수준 작업에 대해서만 학습된 위치 임베딩에 어려움을 초래합니다. 이 문제를 해결하기 위해 위치 임베딩 드롭아웃(PED)이라는 간단하면서도 효과적인 기술을 제안합니다. 이는 학습 중에 전체 위치 임베딩을 무작위로 마스크 아웃합니다(예: 확률 0.5). 이를 통해 모델이 위치 임베딩에 크게 의존하지 않도록 학습하여 고해상도 이미지를 처리하고 더 나은 영역 분류를 수행할 수 있습니다. PED는 기준선과 &#39;위치 임베딩 없음&#39; 변형보다 성능이 뛰어날 뿐만 아니라 동결된 비전 변환기를 사용하여 오픈 어휘 감지를 더욱 개선할 수 있습니다. 3.3. 영어: 오픈 어휘 감지 오픈 어휘 시나리오용 객체 감지기는 기본 범주 CB의 레이블에 대해 학습되지만 테스트 시간에 기본 및 신규 범주(CBUCN)의 결합을 감지할 수 있어야 합니다(3.1절 및 그림 2-오른쪽 참조).베이스라인 아키텍처.저희 감지기는 ViTDet [40]에서 제안된 것처럼 고해상도 이미지를 처리하기 위해 간단한 피처 피라미드와 윈도우 주의를 채택하고 [13, 18, 65, 71, 33]에서와 같이 마스크 R-CNN 헤드와 클래스 독립적 상자 회귀 및 마스크 헤드를 사용합니다.또한 RPN의 이진 분류를 중심성 기반 객체성으로 대체하여 최근의 새로운 객체 제안 방법 [32]을 활용합니다.예측된 객체성 점수 o₂는 si OVD = Oi · Siens로 최종 OVD 점수에 결합됩니다.저희 감지기 백본은 3.1절의 VLM에서 사전 학습된 ViT로 초기화됩니다. 3.2, 그리고 미리 훈련된 검출기 미리 훈련된 검출기 방법 APr AP 방법 새로운 AP AP 모델 백본 모델 백본 ConvNet 기반: ConvNet 기반: DetPro-Cascade [13] ViT-B/R-20.27.ViLD [18] ViT-B/32 R-27.51.Detic-CN2 [74] ViT-B/R-24.32.OV-DETR [64] ViT-B/R-29.52.RegionCLIP [71] R-50xR-50x22.32.VILD-Ens [18] ViT-B/R-18.26.XPM et al. [28] ViLD-Ens [18] ViT-L/EffNet-B7 21.29.ViLD-Ens [18] EffNet-BEffNet-B7 26.29.w/ 가상 상자 레이블: RegionCLIP [71] † PromptDet [15] RR-27.41.R-50xR-50x39.55.ViT-B/32 R-26.50.VL-PLM [68] ViT-B/R-17.27.VL-PLM [68] ViT-B/32 R-34.53.OV-DETR [64] ViT-B/R-17.26.Rasheed et al. [49] ViT-B/R-21.25.PromptDet [15] ViT-B/R-21.4 25.Rasheed et al. [49] 약한 감독 포함: Detic-CN2 [74] ViT-B/32 R-36.51.ViT-B/R-24.32.ViT 기반: ViT 기반:* OWL-ViT [42] ViT-H/ViT-H/14 23.3* 35.3* OWL-VIT [42] ViT-L/ViT-L/25.6* 34.7* CFM-VIT(우리의 것) CFM-VIT(우리의 것) ViT-B/16 ViT-B/ViT-L/16 ViT-L/30.42.34.46.CFM-VIT(우리의 것) ViT-B/CFM-VIT(우리의 것) ViT-L/CFM-VIT(우리의 것) CFM-VIT(우리의 것) ViT-B/ViT-L/ViT-B/16 29.6* 33.8* ViT-L/16 35.6* 38.5* ViT-B/16 28.8 32.ViT-L/16 33.9 36.표 1: LVIS 오픈 어휘 객체 감지.CFM-ViT는 동일한 백본을 사용하여 기존 최고의 접근 방식보다 +7.6 APr, 다른 ViT 기반 접근 방식[42]보다 +10.0 AP 더 우수한 성능을 보입니다.*: 새로 추가된 감지 헤드가 있는 상자 AP를 보고합니다.위치 정보가 감지에 중요하므로 미세 조정 중에 위치 임베딩 드롭아웃(PED)을 적용하지 않습니다.백본 학습 속도.백본의 사전 학습된 지식은 새로운 범주를 인식하는 데 중요하므로 미세 조정 단계에서 잊는 것을 방지하도록 백본 학습 속도를 설정하는 것이 중요합니다.반면에 백본을 완전히 동결하면 감지 작업에 적응하는 능력이 제한됩니다. 우리는 나머지 검출기 층보다 백본 학습률을 낮게(예: 0.5×) 설정하면 트레이드오프에서 이점이 있음을 발견했습니다. 검출 학습이 완료된 후, 다음에 설명하는 대로 테스트 시간에 동결된 ViT 백본을 사용하는 것을 탐색합니다. ViT 백본 동결된 백본 추론은 검출 작업에 적응하지만 사전 학습된 일부 개방형 어휘 지식을 잊는 경향이 있습니다. 따라서 추론을 위해 별도의 동결된 ViT 백본을 개방형 어휘 영역 분류기로 사용하기로 제안합니다. 구체적으로, 영역 VLM 점수 zi(3.1절)를 계산할 때 미세 조정된 백본 대신 동결된 백본을 사용합니다. 동결된 ViT가 강력한 제로 샷 영역 분류기 역할을 하기 위해 위치 임베딩 드롭아웃(PED)으로 사전 학습되는 것이 중요하다는 것을 알게 되었습니다. 우리는 다음을 보여줍니다.
--- EXPERIMENT ---
s는 PED 사전 학습과 동결된 백본 추론을 통합하면 오픈 어휘 감지에서 큰 이득을 제공한다는 것입니다.4. 실험 결과 사전 학습 설정.이미지-텍스트 사전 학습의 경우 널리 사용되는 ViT-B/16 및 ViT-L/16을 이미지 enTable 2로 사용합니다.COCO 오픈 어휘 객체 감지(상자 AP50).CFM-ViT는 최초의 ViT 기반 접근 방식을 나타내며 가상 레이블링이나 약한 감독을 사용하지 않고도 매우 경쟁력 있는 새로운 AP를 보여줍니다.†: RegionCLIP은 사전 학습 중에 기성품 RPN을 사용합니다.: Rasheed et al.은 사전 학습 중에 외부 MVIT 감지기[41]를 사용합니다.*: 다른 ViT 기반 방법[42]은 LVIS에서만 결과를 보고합니다. 영어: 224의 입력 이미지 크기를 가진 코더. 고정된 2D 사인파 위치 임베딩을 사용하고 0.5의 드롭 확률을 가진 위치 임베딩 드롭아웃(PED)을 적용합니다. 이미지 임베딩은 마지막 ViT 레이어에서 전역 평균 풀링을 통해 얻습니다. 텍스트 인코더는 [47, 62]에서와 같이 12레이어 Transformer이며 입력 시퀀스는 고정된 길이인 64개 토큰으로 잘립니다. L2-정규화된 이미지 및 텍스트 임베딩과 학습 가능한 스케일링 온도는 InfoNCE 대조 손실[47]에 대한 입력입니다. 우리의 특징 재구성 디코더는 원시 픽셀 재구성을 위해 설계된 MAE[22]의 8레이어 대응 제품과 달리 2레이어 ViT입니다. 재구성 손실은 코사인 거리이며 손실 계수 2.0으로 스케일링되고 대조 손실에 추가됩니다. 우리는 기본적으로 ALIGN 데이터 세트[29]를 사용하는 반면, LAION 데이터 세트[51]를 사용하면 비슷한 결과가 나타남을 보여줍니다(표 6). 별도로 언급하지 않는 한, 우리는 절제에 4k의 배치 크기를 사용하고 비교에 16k를 사용하고 초기 학습률(LR)이 5e-4이고 선형 LR 감소가 있는 AdamW 최적화 도구를 사용하여 500k 반복으로 학습합니다. 우리는 10k의 워밍업 반복과 0.01의 가중치 감소를 사용합니다. 탐지 미세 조정 설정. 우리는 1024×1024의 이미지 크기를 가진 기본 범주 CB에서 모델을 훈련합니다. 위치 임베딩(PE)은 더 높은 해상도에 맞게 선형 보간됩니다. 우리는 탐지 훈련 동안 PE 드롭아웃을 적용하지 않고 나머지 모델에 비해 백본에 대해 더 낮은 학습률(예: 0.5×)을 설정합니다. 우리는 CLIP 템플릿[47]을 활용하고 각 범주의 평균 텍스트 임베딩을 취합니다. 우리는 배치 크기 128, 모멘텀 0.9의 SGD 옵티마이저, 초기 학습률 0.18/0.02를 사용하고 LVIS/COCO 데이터 세트에서 36.8k/11.3k 반복으로 훈련합니다. 이미지 Flickr30K(1K 테스트 세트) MS COCO(5K 테스트 세트) 인코더 이미지-텍스트 변환 방법 크기 R@CLIP [47] 302M 88.98.99.text-to-image R@5 R@10 R@1 R@68.image-to-text text-to-image R@R@1 R@RR@1 R@R@90.95.58.4 81.88.37.62.72.ALIGN [29] 480M 88.98.99.75.7 93.96.58.6 83.89.45.6 69.78.FLAVA [53] 86M 67.94.65.89.42.7 76.38.67.FILIP [61] 302M 89.99.99.75.0 93.96.61.3 84.90.45.9 70.79. Florence [63] 637M 90.99.76.93.CoCa-Large [62] 303M 91.4 99.99.79.95.97.CFM-VIT(저희) 303M 91.7 99.99.79.6 95.97.64.7 85.65.4 85.66.4 86.91.91.47.2 71.50.1 73.49.8 73.81.81.표 3: Flickr30K 및 COCO 벤치마크에서의 제로 샷 이미지-텍스트 검색 결과. 저희는 사전 학습된 모델을 다른 방법과 비교하여 평가합니다. 우리는 12개 지표 중 8개에서 동일한 백본을 가진 최첨단 CoCa-Large보다 우수한 성과를 거두었습니다.4.1. 주요 결과 LVIS 벤치마크.우리는 다양한 1203개 객체 범주를 포함하는 LVIS [19] 오픈 어휘 탐지 벤치마크에서 다른 방법과 비교했습니다.훈련을 위한 기본 범주 CB는 &#39;빈번한&#39; 및 &#39;일반적인&#39; 범주이고, 새로운 범주 CN은 [18, 70, 13]에서와 같이 테스트를 위해 보류된 &#39;희귀&#39; 범주입니다.주요 지표는 마스크 APr이고, 우리는 재현성을 위해 [18]에 따라 세 번의 실행에 대한 평균을 보고합니다.표 1에 따르면 최상의 CFM-ViT 모델은 33.9 APr을 달성하여 최상의 기존 ViT 기반 방법 OWL-ViT [42]보다 +10.0 APr만큼 상당히 개선되었습니다. 주목할 점은 더 작은 ViT-B/16 백본을 사용하는 CFM-ViT가 ViT-L/14를 사용하는 OWL-ViT보다 +4.0 AP 더 우수한 성능을 보인다는 것입니다. 나아가 EffNet-B7 백본을 사용하는 현재 가장 우수한 접근 방식인 ViLD-Ens와 비교할 때 CFM-ViT는 +7.6 AP 개선을 달성합니다. 주목할 점은 CFM-ViT가 롱테일 인식 손실[42, 71, 74], 지식 증류[18, 13], 약한 감독[74] 또는 의사 상자/마스크 레이블[71, 68, 49]을 사용하지 않고 바닐라 감지 손실[23]만을 사용하는 간단한 미세 조정 방법을 가지고 있다는 것입니다. 이는 모두 현재 오픈 어휘 탐지 방법에서 공통적입니다. COCO 벤치마크. COCO 오픈 어휘 탐지 벤치마크에 대한 비교를 제시합니다. 이 설정은 학습을 위해 48개의 기본 범주와 테스트를 위해 17개의 새로운 범주를 사용합니다[18]. 주요 지표는 신규 범주의 AP50(&#39;신규 AP&#39;)입니다. 훈련 범주가 적기 때문에 CFM-ViT 모델은 바닐라 감지 손실만을 사용하여 이러한 범주에 과적합되는 경향이 있습니다. 이는 CFM-ViT가 이 벤치마크에서 과적합을 상쇄하기 위해 의사 상자/마스크 레이블[28, 15, 71, 68, 49], 지식 증류[18, 13], 약한 감독[74]과 같은 보조 목적을 사용하지 않기 때문입니다. 그러나 표 2는 CFM-ViT가 보조 목적을 활용하는 기존 방법 중에서 여전히 매우 경쟁력이 있음을 보여줍니다. 더욱이 CFM-ViT는 다른 ViT 기반[42] 접근 방식이 LVIS에서만 벤치마크를 수행하기 때문에 이 벤치마크에서 최초의 ViT 기반 방법을 나타냅니다. 제로 샷 이미지-텍스트 검색. 지역 수준 오픈 어휘 감지에 대한 우리의 주요 평가 외에도, 방법은 [18] ViLD [18] 백본 AP APAPR-25.38.28.R-11.18.12.DetPro [13] R-12.18.12.CFM-VIT(우리의 것) CFM-VIT(우리의 것) ViT-B/15.24.17.ViT-L/18.28.20을 감독했습니다.표 4: Objects365(박스 AP)에 대한 전이 감지. 모든 모델은 LVIS 기반 범주에서 학습하고 미세 조정 없이 Objects365 데이터 세트에서 테스트했습니다. 우리는 제로 샷 이미지-텍스트 검색에서 이미지 수준 표현을 평가합니다. 우리는 표 1의 마지막 행과 같은 CFM-ViT 모델(ViT-L, 배치 크기 16k)을 취하고 표준 프로토콜[29, 62]에 따라 추가 40K 반복을 위해 더 높은 해상도(예: 448)에서 사전 학습을 계속합니다.표 3은 Flickr30K 및 MS COCO 벤치마크에서 다른 듀얼 인코더 방법과의 비교를 보여줍니다.CFMViT는 12개 메트릭 중 8개에서 비슷하거나 더 큰 모델 크기의 최첨단 방법보다 성능이 우수합니다.제로 샷 전이 감지.제로 샷 전이 감지에서 CFM-VIT의 일반화 능력을 평가하기 위해 Objects365-v1 검증 분할에서 성능을 테스트합니다[52].LVIS 기반 범주에서 학습된 동일한 감지기를 사용하고(표 1) 미세 조정 없이 전이 감지를 위해 LVIS를 Objects365 어휘 임베딩으로 대체합니다[18, 13]. 모든 범주가 새롭다고 가정하고 Eq. (2)에서 a, ẞ=(0.0, 0.65)를 설정합니다. 최상의 모델은 18.7 AP를 달성하여 VILD보다 +6.9 AP, DetPro보다 +5.6 AP 더 우수한 성능을 보였습니다(표 4 참조). 백본 용량(R50 대 ViT)이 다르기 때문에 이 비교는 주로 CFMViT가 강력한 교차 데이터세트 일반화를 달성할 수 있음을 보여주는 데 사용됩니다. 4.2. Ablation 연구 CFM-ViT의 사전 학습 및 오픈 어휘 감지기 설계를 Ablation합니다. LVIS 오픈 어휘 감지 벤치마크에서 평가합니다. 이미지 인코더는 ViT-L/16이고 대조 배치 크기는 기본적으로 4k입니다. 마스크된 특징 재구성. 표 5a는 제안된 마스크된 이미지-텍스트 사전 학습(3.2절)을 Ablation합니다. 제안된 마스크된 특징 재구성은 명확한 benpretraining 방법 APr AP 사전 학습 방법 APP AP 제어/재구성을 제공합니다.FLOPS APr AP 기준선 27.30.기준선 27.30.100% 0% 1.00× 27.4 30.w/ 특징 재구성 30.7 (+3.3) 34.w/ PED 28.5 (+1.1) 31.100% / 25% 1.23 × 30.7 34.w/ 픽셀 재구성 w/ 1차 레이어 특징 재구성 27.27.31.w/ 특징 재구성 + PED 31.2 (+3.8) 33.30.w/ PE 없음 25.29.100% / 50% 75%/25% 1.44x 1.01 X 29.9 33.30.4 33.w/ 특징 재구성 + PE 없음 27.31.(a) 마스크 재구성. &#39;baseline&#39;은 대조적 이미지-텍스트 사전 학습입니다. 제안하는 마스크 특징 재구성은 +3.APr만큼 개선됩니다. 원시 픽셀 공간이나 첫 번째 레이어 특징 공간에서 재구성하면 이점이 없습니다. (b) 위치 임베딩 드롭아웃(PED)은 베이스라인을 1.1 APr만큼 개선합니다. 마스크 특징 재구성과 함께 사용하면 +2.7의 추가 이득을 얻습니다. PED는 기능 재구성 여부에 관계없이 &#39;PE 없음&#39;보다 3./1.6 AP 더 우수한 성능을 보입니다. (c) 대조 분기를 마스킹하면 성능 저하가 거의 없거나 전혀 없이 학습 효율성을 회복하여 기준선보다 +3.0 APr 더 우수한 성능을 보입니다. bblr APr AP 0.0 9.5 11.0.1 25.8 28.0.5 27.4 30.1.0 26.0 30.w/ PED APr 기준선 27.24.6 (-2.8) AP 30.430.모델 배치 B/16 4k APT AP 24.26.8 (+2.7) 27.6 → 30.w/feat-recon. 기준선 30.27.1 (-3.8) 34.033.B/16 16k 26.28.8 (+2.4) 30.333,w/ feat-recon ✓ ✓ 28.30.5 (+2.0) 31.931.27.30.4 34.31.32.5 (+1.3) 33.7 34.(d) 백본 미세 조정 Ir 비율(bblr) 추가된 검출기 레이어에 대한 값. (e) 동결된 백본 추론. 표준 위치 임베딩을 사용하는 경우 미세 조정된 인코더보다 성능이 떨어집니다. 반면 PED로 사전 학습된 인코더의 경우 동결된 백본 추론은 미세 조정된 인코더보다 +2.0 및 +1.3 APr만큼 성능이 뛰어납니다. L/16 4k L/16 16k 30.32.5 (+5.1) 33.9 (+3.4) 35.9 36.(f) 확장성: 다양한 모델 및 대조 배치 크기에 걸친 &#39;기준선 → CFM-VIT&#39;의 이점. 기준선을 +2.4~+5.1 APr만큼 개선합니다. 표 5: LVIS 개방형 어휘 감지 벤치마크에 대한 절제 연구. 기본(&#39;빈번한&#39; + &#39;일반적인&#39;) 범주에 대해 학습하고, 새로운(&#39;드문&#39;) 범주에 대해 테스트하고 APr을 보고합니다. 달리 언급하지 않는 한 ViT-L/16 백본과 대조 배치 크기 4k를 사용합니다. 대조 이미지-텍스트 사전 학습 기준선에 대해 +3.3 AP의 효과. 이 경우, 피처 재구성 대상은 이미지 인코더의 출력 피처입니다. 다른 재구성 대상인 정규화된 이미지 픽셀[22]과 첫 번째 패치화 계층의 피처와 비교합니다. 우리는 둘 다 기준선보다 개선되지 않는다는 것을 관찰합니다.대조적 사전 학습이 강력한 기준선 표현을 설정하기 때문일 가능성이 큽니다[18, 10, 33].반대로, 제안된 마스크된 특징 재구성은 강력한 기준선을 분명히 개선하고 개방형 어휘 감지에서 이점을 보여줍니다.위치 임베딩 드롭아웃.표 5b에서 위치 임베딩 드롭아웃(&#39;PED&#39;)을 제거합니다.PED는 기준선(드롭아웃 없는 PE)보다 +1.1 AP의 이득을 가져옵니다.이는 PED가 사전 학습 중에 저해상도 전체 이미지 PE에 대한 과적합을 효과적으로 줄여 미세 조정을 통해 고해상도 감지 작업에 더 잘 적응함을 보여줍니다.또한 PED는 마스크된 특징 재구성과 함께 사용할 때 +2.7의 추가 이득을 얻습니다.PED를 ViT 인코더에서 위치 임베딩을 사용하지 않는 다른 기준선(&#39;PE 없음&#39;)과 비교합니다.PED 방법은 특징 재구성 여부에 관계없이 &#39;PE 없음&#39; 기준선보다 3.5 / 1.6 AP 더 우수한 성능을 보입니다. 재구성 디코더의 위치 임베딩[22]이 항상 유지된다는 점에 유의합니다. 마지막으로, PED는 표 5e에 표시된 것처럼 강력한 영역 분류기로 동결된 백본을 사용할 수 있게 합니다. 대조 브랜치를 마스킹하여 더 빠른 학습. 표 5c는 대조 및 재구성 인코더의 이미지 마스킹 비율을 연구합니다. 기본적으로 학습 중에 손상되지 않은 이미지, 즉 100% 토큰에 대조 인코더를 적용합니다. 사전 학습 데이터에 25% 입력이 있는 재구성 타워 추가 ALIGN [29] APr AP 32.34.LAION-2B [51] LAION-400MB [51] 32.34.32.34.표 6: 사전 학습 데이터. ViT-L/16 및 배치 크기 4k가 사용됩니다. 특징 재구성이 있는 PED가 있는 베이스라인. + PED Flickr30K I2T 86.0 72.T2I 86.1 72.87.0 73.MS COCO I2T T2I 59.3 43.59.1 43.60.1 44.표 7: 제로 샷 이미지-텍스트 검색에 대한 사전 학습 평가(Recall@1). Flickr30k 및 COCO 검색 작업에서 사전 학습된 모델의 이미지 수준 표현을 평가합니다. 위치 임베딩 드롭아웃(PED)을 제거하고 마스크된 기능 재구성을 추가합니다. ViT-L/16 및 배치 크기 4k가 사용됩니다. kens는 1.23배 더 많은 학습 비용을 초래합니다. 학습 효율성을 유지하기 위해 재구성 분기 입력에서 상호 배타적인 75% 토큰만 대조 인코더에 공급하는 것을 살펴봅니다. 이 마스킹 기술은 정확도 손실이 거의 없거나 전혀 없이 학습 효율성을 완전히 회복하여 기준선보다 +3.0 APr 더 우수한 성능을 보입니다. 백본 학습률 비율. CFM-ViT는 새로운 범주를 인식하기 위해 백본에서 사전 학습된 지식을 유지해야 합니다. 표 5d는 미세 조정 중에 백본 학습률을 나머지 감지기보다 낮게 설정하는 이점을 보고하며, 비율은 0.5×가 최적 값입니다. 비율이 높을수록 망각이 발생하고 비율이 낮을수록 감지 작업에 적응하는 능력이 제한됩니다. Tn 래그 인형: 82% ar 감: 88%| 래그 인형: 77% 래그 인형: 85% 감: 90% p 래그 인형: 85% 감: 86% 페이퍼웨이트: 81% 래그 인형: 82% 래그 인형: 78% 감: 82% 하드커버 책: 69% 하드커버 책: 72% + 셰퍼드견: 63% 셰퍼드견: 68% 레몬: 79% 와인잔: 77% 통풍구: 69% 에어컨: 70% 에어컨: 75% TV: 60% 상추: 63% 새우: 64% 새우: 92% 새우: 62% 새우: 85% 캐비닛 또는 선반: 80% 콘센트: 65% 캐비닛 또는 선반: 74% 전자레인지: 92% 전자렌지/가스렌지: 81% 싱크대: 64% 램프: 94% 램프: 84% 램프: 69% 의자: 71% 의자: 82% 베개: 88% 베개: 85% 침대: 82% 소화기: 92% 캐비닛 또는 선반: 78% 의자: 70% TV: 88% 전자/가스 스토브: 55% 바나나: 91% 그릇: 59% 계량컵: 56% 바나나: 82% 오븐: 64% 캐비닛 또는 선반: 69% 그림 3: LVIS 신규 카테고리(위)와 Objects365 제로샷 전송 감지(아래)에 대한 정성적 결과. LVIS 결과의 경우 명확성을 위해 신규 카테고리만 표시합니다. CFM-ViT는 LVIS에서 래그 인형, 감, 종이추, 하드커버 책, 셰퍼드견, Objects365에서 새우, 전원 콘센트와 같은 많은 신규 카테고리를 감지합니다. 동결된 백본 추론. 지금까지의 절제 연구에는 동결된 백본 추론이 포함되지 않았습니다. 모든 절제는 미세 조정된 ViT 백본을 사용하여 VLM 점수(3.1절의 pi 및 Eq. (2))를 계산합니다. 표 5e에서 미세 조정된 ViT 인코더를 동결된 ViT 인코더로 대체하여 동결된 백본의 영역 분류기로서의 효능을 평가하고 성능을 분석합니다(그림 2의 가장 오른쪽 부분 참조). 실험 결과, 표준 위치 임베딩을 사용할 때 동결된 백본은 미세 조정된 인코더보다 성능이 떨어지는 것으로 나타났으며, 이는 특징 재구성 손실이 있는 기준선과 없는 기준선 모두에 적용됩니다. 그러나 위치 임베딩 드롭아웃(PED)으로 ViT 인코더를 사전 학습하면 동결된 백본으로 성능이 상당히 향상되어 미세 조정된 백본의 성능보다 +2.0/+1.3 APr 더 뛰어나며 특징 재구성 손실이 있거나 없는 것으로 나타났습니다. 이 결과는 대조 사전 학습과 검출 미세 조정 간의 도메인 갭을 줄이는 데 PED가 효과적임을 보여주며, 따라서 제로 샷 영역 분류가 향상됩니다. 특징 재구성과 결합하면, 전체 방법은 기준선에 비해 전체적으로 +5.1 APr 향상을 달성합니다.모델 크기와 배치 크기.표 5f는 CFM-ViT 사전 학습에서 모델 크기와 배치 크기가 다운스트림 오픈 어휘 감지에 미치는 영향을 연구합니다.배치 크기를 4k에서 16k로 늘리면 ViT-B/L 모두에서 +2.7 / 1.4 APr 향상되고, ViT-B에서 ViT-L로 업그레이드하면 두 배치 크기 모두에서 +5.9/5.APr 향상되는 것을 관찰했습니다.이러한 결과는 더 큰 배치 크기와 모델 크기가 모두 매우 유익하다는 대조 학습 문헌[47, 29, 46]의 관찰 결과와 일치합니다.중요한 점은 CFM-ViT가 테스트된 모든 배치 및 모델 크기에서 기준선보다 +2.4 ~ +5.1 APr 더 우수한 성능을 지속적으로 발휘한다는 것을 발견하여 효능을 더욱 입증한다는 것입니다.사전 학습 데이터. ALIGN 데이터[29] 외에도 표 6의 LAION 데이터 세트[51]로 실험했습니다.LAION-2B/LAION-400M은 32.4/32.2 APr을 생성하여 ALIGN 결과 32.5 APr과 비슷합니다.이미지-텍스트 검색.오픈보컬러리 감지에 대한 절제 외에도 위치 임베딩 드롭아웃과 마스크된 특징 재구성이 제로 샷 이미지 수준 검색에 미치는 영향을 조사하고 Flickr30K 및 MS COCO 데이터 세트에서 Recall @ 1 메트릭 측면에서 결과를 보고합니다.표 7은 위치 임베딩 드롭아웃이 이미지 수준 표현의 품질을 효과적으로 보존하는 반면 마스크된 특징 재구성은 모든 메트릭에서 평균 1% Recall @1의 개선을 보임을 보여줍니다.4.3. 시각화 특징 재구성.그림 1에서 사전 학습(3.2절)에서 얻은 특징 재구성 결과를 보여줍니다. 시각화를 위해 재구성된 이미지 피처(d)와 쿼리 텍스트 임베딩(e) 간의 유사도 맵(c)을 계산합니다. 학습된 피처 재구성이 쿼리된 이미지-텍스트 쌍과 관련하여 의미적으로 타당하다는 것을 관찰합니다. 오픈 어휘 감지 출력. 그림 3에서 LVIS 신규 범주(위쪽 행)에 대한 CFM-ViT 출력과 Objects365(아래쪽 행)에 대한 제로샷 전송 감지를 시각화합니다. 두 시각화 모두 표 1의 마지막 행과 동일한 모델을 사용하며, 이는 LVIS 기본 범주에서 학습되었습니다. 두 데이터 세트 모두에서 CFM-ViT는 학습 중에 사용할 수 없는 많은 신규 범주를 감지할 수 있습니다. 5.
--- CONCLUSION ---
우리는 픽셀/영역 수준 의미론을 이미지-텍스트 사전 학습에 주입하여 오픈 어휘 객체 감지를 위한 Contrastive Feature Masking Vision Transformer(CFM-VIT)를 소개합니다. CFM-ViT는 피처 구성과 위치 임베딩 드롭아웃을 사용하여 간단하고 확장 가능하며, LVIS 오픈 어휘 감지 벤치마크에서 최첨단 기술을 큰 차이로 능가하며, COCO 벤치마크와 Objects365로의 제로샷 전송에서 매우 경쟁력 있는 성능을 보여줍니다. 또한 CFM-ViT는 제로샷 이미지-텍스트 검색 벤치마크의 12개 지표 중 8개에서 최첨단 기술을 능가합니다. CFM-VIT가 커뮤니티에서 오픈 어휘 감지를 위한 이미지-텍스트 사전 학습을 탐색하도록 영감을 줄 수 있기를 바랍니다[31]. 참고문헌[1] Yutong Bai, Xinlei Chen, Alexander Kirillov, Alan Yuille, Alexander C. Berg. 객체 감지 사전 학습을 위한 포인트 수준 영역 대비. CVPR, 16061-16070페이지, 2022년 6월.[2] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, Ajay Divakaran. 제로샷 객체 감지. ECCV, 2018.[3] Hangbo Bao, Li Dong, Furu Wei. Beit: Bert 이미지 변환기 사전 학습. arXiv 사전 인쇄본 arXiv:2106.08254, 2021. 1,[4] Xinlei Chen 및 Abhinav Gupta. 합성곱 신경망의 웹 감독 학습. ICCV, 2015.[5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer 등 Pali: 공동으로 확장된 다국어 언어-이미지 모델입니다. arXiv 사전 인쇄 arXiv:2209.06794, 2022.[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong 및 Qi Tian. Sdae: 자가 증류 마스크 오토인코더. Computer Vision-ECCV 2022: 제17차 유럽 컨퍼런스, 이스라엘 텔아비브, 2022년 10월 23~27일, 절차, 파트 XXX, 108~124페이지. 스프링거, 2022.[7] Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis. 하이브리드 영역 임베딩을 통한 제로샷 객체 감지. BMVC에서, 2018.[8] Karan Desai와 Justin Johnson. Virtex: 텍스트 주석에서 시각적 표현 학습. CVPR에서, 2021.[9] Santosh K Divvala, Ali Farhadi, Carlos Guestrin. 모든 것에 대한 모든 것을 배우기: Webly 감독 시각적 개념 학습. CVPR에서, 2014.[10] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu. Clip 자체는 강력한 미세 조정기입니다. imagenet에서 vit-b와 vit-l로 85.7%와 88.0%의 상위 1 정확도 달성. arXiv 사전 인쇄본 arXiv:2212.06138, 2022. 2,[11] Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen 등. Maskclip: Masked selfdistillation advances contrastive language-image pretraining. arXiv 사전 인쇄본 arXiv:2208.12262, 2022.[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly 등. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변압기. arXiv 사전 인쇄 arXiv:2010.11929, 2020. 1,[13] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao 및 Guoqi Li. 비전 언어 모델을 사용하여 개방형 어휘 개체 감지를 요청하는 방법을 학습합니다. CVPR, 2022. 1, 2, 3, 4, 5,[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang 및 Yue Cao. Eva: 대규모로 마스크된 시각적 표현 학습의 한계를 탐색합니다. arXiv 사전 인쇄 arXiv:2211.07636, 2022.[15] Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, Lin Ma. Promptdet: 큐레이션되지 않은 이미지를 사용한 오픈 어휘 감지를 향해. European Conference on Computer Vision, 701-717페이지. Springer, 2022. 5,[16] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc&#39;Aurelio Ranzato, Tomas Mikolov. Devise: 심층적 시각적 의미 임베딩 모델. NeurIPS, 2013.[17] Golnaz Ghiasi, Xiuye Gu, Yin Cui, Tsung-Yi Lin. 이미지 수준 레이블을 사용한 오픈 어휘 이미지 분할 확장. European Conference on Computer Vision, 540-557페이지. Springer, 2022.[18] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. 비전 및 언어 지식 증류를 통한 오픈 어휘 객체 감지. ICLR, 2022. 1, 2, 3, 4, 5, 6,[19] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. CVPR, 2019.[20] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. 제로 샷 객체 감지를 위한 보이지 않는 것 합성. ACCV, 2020.[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다. CVPR, 16000-16009페이지, 2022년 6월. 1,[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16000-16009페이지, 2022년. 3,5,[23] Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. 마스크 r-cnn. ICCV, 2017년.[24] Xiangteng He 및 Yuxin Peng. 비전과 언어를 결합하여 세분화된 이미지 분류. CVPR, 2017년.[25] Olivier J. Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, João Carreira. 대조 감지를 통한 효율적인 시각적 사전 학습. ICCV, 10086-10096페이지, 2021년 10월.[26] Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, SunYuan Kung. Milan: 언어 지원 표현에 대한 마스크 이미지 사전 학습. arXiv 사전 인쇄본 arXiv:2208.06049, 2022.[27] Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, Jiashi Feng. 대조 마스크 자동 인코더는 더 강력한 시각 학습자입니다. arXiv 사전 인쇄본 arXiv:2207.13532, 2022.[28] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, Ehsan Elhamifar. 강력한 교차 모달 의사 라벨링을 통한 개방형 어휘 인스턴스 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7020-7031페이지, 2022. 5,[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각 및 시각 언어 표현 학습 확장. ICML, 2021. 2, 3, 5, 6,7,[30] Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache. 대규모 약한 감독 데이터에서 시각적 특징 학습. ECCV, 2016.[31] Dahun Kim, Anelia Angelova, Weicheng Kuo. 비전 변환기를 사용한 오픈 어휘 객체 감지를 위한 지역 인식 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 11144-11154쪽, 2023. 3,[32] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, Weicheng Kuo. 분류를 학습하지 않고 오픈 월드 객체 제안 학습. IEEE Robotics and Automation Letters, 7(2):5453-5460, 2022.[33] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, Anelia Angelova. F-vlm: 동결된 시각 및 언어 모델에 대한 개방형 어휘 객체 감지. arXiv 사전 인쇄본 arXiv:2209.15639, 2022. 1, 3, 4,[34] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, et al. Mammut: 다중 모드 작업을 위한 공동 학습을 위한 간단한 아키텍처. arXiv 사전 인쇄본 arXiv:2303.16839, 2023.[35] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl. 언어 기반 의미 분할. arXiv 사전 인쇄본 arXiv:2201.03546, 2022.[36] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP: 통합 시각 언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. 제39회 국제 기계 학습 컨퍼런스 논문집, 기계 학습 연구 논문집, 12888-12900페이지, 2022.[37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao. 접지된 언어-이미지 사전 학습. CVPR, 2022.[38] Runze Li, Dahun Kim, Bir Bhanu, Weicheng Kuo. Reclip: 작은 이미지로 학습하여 리소스 효율적인 클립. arXiv 사전 인쇄본 arXiv:2304.06028, 2023.[39] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He. 마스킹을 통한 언어-이미지 사전 학습 확장. arXiv 사전 인쇄본 arXiv:2212.00794, 2022.[40] Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He. 객체 감지를 위한 일반 비전 변환기 백본 탐색. ECCV, 2022.[41] Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, MingHsuan Yang. 멀티모달 변환기를 사용한 클래스 독립적인 객체 감지. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part X, pages 512-531. Springer, 2022.[42] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby. 비전 변환기를 사용한 간단한 오픈 어휘 객체 감지. ECCV, 2022. 1, 3, 5,[43] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, Jeffrey Dean. 의미 임베딩의 볼록 조합을 통한 제로샷 학습. 2014.[44] Aaron van den Oord, Yazhe Li, Oriol Vinyals. 대조 예측 코딩을 통한 표현 학습. arXiv 사전 인쇄본 arXiv:1807.03748, 2018.[45] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei. Beit v2: 벡터 양자화 시각적 토큰화기를 사용한 마스크 이미지 모델링. arXiv 사전 인쇄본 arXiv:2208.06366, 2022.[46] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, Quoc V. Le. 제로샷 전이 학습을 위한 결합 스케일링. CORR, abs/2111.10050, 2021. 2,[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 자연어 감독에서 전이 가능한 시각 모델 학습. ICML, 2021. 2, 3, 5, 6,[48] Shafin Rahman, Salman Khan, Nick Barnes. 제로샷 객체 감지를 위한 개선된 시각-의미 정렬. AAAI, 2020.[49] Hanoona Rasheed, Muhammad Maaz, Muhammad Uzair Khattak, Salman Khan, Fahad Shahbaz Khan. 오픈 어휘 감지를 위한 객체 및 이미지 수준 표현 간 격차 해소. arXiv 사전 인쇄본 arXiv:2207.03482, 2022. 1, 2, 3, 5,[50] Mert Bulent Sariyildiz, Julien Perez, Diane Larlus. 캡션 주석을 사용한 시각적 표현 학습. ECCV, 2020.[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114, 2021. 2, 5, 7,[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun. Objects365: 객체 감지를 위한 대규모 고품질 데이터 세트. ICCV에서, 2019.[53] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela. Flava: 기초 언어 및 비전 정렬 모델. CVPR에서, 15638~15650페이지, 2022.[54] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, Jifeng Dai. 다중 모달 상호 정보 극대화를 통한 올인원 사전 학습을 향해. arXiv 사전 인쇄본 arXiv:2211.09807, 2022.[55] Josiah Wang, Katja Markert, Mark Everingham, et al. 자연어 설명으로부터 객체 인식을 위한 학습 모델. BMVC에서, 2009.[56] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, Christoph Feichtenhofer. 자기 감독 시각적 사전 학습을 위한 마스크된 특징 예측. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 14668-14678페이지, 2022.[57] Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, Stephen Lin. 객체 수준 대조 학습을 통한 감지를 위한 사전 학습 정렬. NeurIPS, 2021.[58] Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian. Mvp: 다중 모달리티 가이드 시각적 사전 학습. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, 2022년 10월 23-27일, Proceedings, Part XXX, 337-353페이지. Springer, 2022.[59] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, Trevor Darrell. 영역 유사성 표현 학습. ICCV, 10539-10548페이지, 2021년 10월.[60] Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie 및 Xinggang Wang. 언어 의미 공간에서 마스크된 시각적 재구성. arXiv 사전 인쇄 arXiv:2301.06958, 2023.[61] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang 및 Chunjing Xu. Filip: 세분화된 대화형 언어-이미지 사전 훈련. ICLR, 2021.[62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini 및 Yonghui Wu. Coca: 대조 캡션 작성자는 이미지-텍스트 기반 모델입니다. TMLR, 2022. 5,[63] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou 및 Pengchuan Zhang. Florence: 컴퓨터 비전을 위한 새로운 기반 모델입니다. arXiv 사전 인쇄 arXiv:2111.11432, 2021년 11월.[64] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang 및 Chen Change Loy. 조건부 일치를 통한 개방형 어휘 detr. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IX, pages 106–122. Springer, 2022.[65] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. 캡션을 사용한 오픈 어휘 객체 감지. CVPR, 2021. 1, 3,[66] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, and Yifan Liu. Segvit: 일반 시각 변환기를 사용한 의미 분할. arXiv 사전 인쇄본 arXiv:2210.05844, 2022.[67] Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi, Kun Yao, et al. Cae v2: 클립 타겟이 있는 컨텍스트 자동 인코더. arXiv 사전 인쇄본 arXiv:2211.09799, 2022.[68] Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao, Anastasis Stathopoulos, Manmohan Chandraker, Dimitris Metaxas, et al. 객체 감지를 위한 비전 및 언어 모델을 사용하여 레이블이 지정되지 않은 데이터 활용. ECCV에서, 2022. 1, 2, 3,5,[69] Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui. 제로 샷 객체 감지를 위한 배경 학습 가능 카스케이드. ACCV에서, 2020.[70] Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu 및 Yin Li. 자연어 감독을 통해 장면 그래프를 생성하는 방법을 학습합니다. ICCV, 2021.[71] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li 및 Jianfeng Gao. Regionclip: 지역 기반 언어 이미지 사전 훈련. CVPR, 2022. 1, 2, 3, 4, 5,[72] Chong Zhou, Chen Change Loy 및 Bo Dai. 클립에서 무료 밀도 레이블을 추출합니다. ECCV, 2022.[73] Zhou Jinghao, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille 및 Tao Kong. ibot: 온라인 토크나이저를 사용한 이미지 bert 사전 훈련. arXiv 사전 인쇄본 arXiv:2111.07832, 2021.[74] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, Ishan Misra. 이미지 수준 감독을 사용하여 2만 개의 클래스 감지. ECCV에서, 2022. 1, 3, 5,[75] Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama. 한 번도 보지 마세요: 제로 샷 감지를 위한 기능 합성. CVPR에서, 2020.
