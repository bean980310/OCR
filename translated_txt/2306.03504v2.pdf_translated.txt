--- ABSTRACT ---
저희는 새로운 과제, 즉 리소스가 적은 텍스트-대화 아바타에 관심이 있습니다. 오디오 트랙을 훈련 데이터로, 임의의 텍스트를 구동 입력으로 하는 몇 분 길이의 대화하는 사람 비디오만 주어지면, 저희는 입력 텍스트에 해당하는 고품질의 대화 인물 비디오를 합성하는 것을 목표로 합니다. 이 과제는 디지털 인간 산업에서 광범위한 적용 가능성을 가지고 있지만, 두 가지 과제로 인해 아직 기술적으로 달성되지 않았습니다. (1) 기존의 다중 스피커 텍스트-대화 시스템에서 도메인 외부 오디오의 음색을 모방하는 것이 어렵습니다. (2) 제한된 훈련 데이터로 고충실도의 립싱크된 대화 아바타를 렌더링하는 것이 어렵습니다. 이 논문에서는 (1) 텍스트 내용, 음색 및 음조를 잘 풀어내는 일반적인 제로샷 다중 스피커 TTS 모델을 설계하고 (2) 신경 렌더링의 최근 발전을 수용하여 사실적인 오디오 기반 대화 얼굴 비디오 생성을 달성하는 적응형 텍스트-대화 아바타(AdaTTA)를 소개합니다. 이러한 설계를 통해, 우리의 방법은 앞서 언급한 두 가지 과제를 극복하고 신원 보존 음성과 사실적인 화자 비디오를 생성하는 데 성공했습니다. 실험 결과, 우리의 방법은 사실적이고 신원 보존적이며 시청각 동기화된 화자 아바타 비디오를 합성할 수 있음을 보여줍니다. 1.
--- INTRODUCTION ---
최근 몇 년 동안 다양한 분야에서 생성적 인공 지능이 등장했습니다. 예를 들어, 대규모 언어 모델(LLM) 기반 챗봇(Adamopoulou &amp; Moussiades, 2020)을 통해 고품질, 자연스럽고 사실적인 대화 텍스트 콘텐츠를 얻을 수 있습니다. 고급 텍스트 음성 변환(TTS) 시스템(Kim et al., 2021; Ren et al., 2021; Wang et al., 2023; Ye et al., 2023b)을 사용하면 *균등한 기여 #ByteDance 인턴 &#39;저장 대학교 ByteDance. 연락처: Zhou Zhao<zhaozhou@zju.edu.cn> . 2023년 제40회 국제 머신러닝 컨퍼런스 회의록. 저작권 2023년 저자(들) 소유. 참조 오디오와 일반 텍스트가 주어진 개인화된 음성. 신경 렌더링 기술의 개발은 또한 몇 개의 훈련 샘플만으로 고충실도의 사실적인 화자 영상 생성(TFG)(Prajwal et al., 2020; Guo et al., 2021; Ye et al., 2023c)을 달성할 수 있게 합니다. TTS 모델과 TFG 방법을 결합하여 공동 시스템을 통해 사용자가 텍스트 입력만으로 화자 영상을 만들 수 있도록 하는 것은 자연스러운 생각입니다. 이 공동 시스템은 ChatGPT(Ouyang et al., 2022)의 최근 발전을 고려할 때 뉴스 방송, 가상 강의, 화자 챗봇과 같은 광범위한 잠재적 응용 분야를 가지고 있습니다. 그러나 이전의 TTS 및 TFG 시스템은 일반적으로 만족스러운 개인화된 결과를 생성하기 위해 많은 양의 신원별 데이터가 필요했습니다(Ren et al., 2020; Suwajanakorn et al., 2017). 이는 일반적으로 대상 인물의 몇 분 길이의 비디오만 사용할 수 있는 실제 시나리오에 과제를 제기합니다. 이러한 관찰에 동기를 부여받아 저희는 저리소스 텍스트-대화 아바타(TTA)라는 새로운 작업에 관심을 갖게 되었습니다. 훈련 데이터로 필사된 오디오 트랙이 있는 몇 분 길이의 대화 인물 비디오만 주어진 경우, 저희는 주도적인 입력 텍스트가 주어진 경우 신원 보존 및 오디오-립 동기화된 대화 인물 비디오를 합성하는 것을 목표로 합니다. 저희는 먼저 TTS와 TFG의 과제를 각각 고려합니다. TTS 단계의 경우 주요 과제는 입력 오디오의 음색 신원을 적절하게 보존하는 방법입니다(Kharitonov et al., 2023). 순진한 해결책은 주어진 텍스트-오디오 쌍에서 사전 훈련된 TTS 모델을 미세 조정하는 것입니다. 그러나 미세 조정은 큰 지연을 유도하고, 제한된 양의 데이터, 일반화 가능성으로 인해 성능이 보장되지 않습니다.또 다른 해결책은 기성품 툴킷(Jia et al., 2018)을 사용하여 입력 오디오의 스피커 임베딩을 추출하는 것입니다.이는 정보 손실을 도입하는 것으로 알려져 있으며 신원 보존 품질이 만족스럽지 않습니다.TFG 단계의 경우 주요 과제는 제한된 양의 오디오-비디오 쌍을 감안할 때 고충실도 및 오디오-비주얼 동기화를 달성하는 것입니다.일부 제로샷 방법(Prajwal et al., 2020; Zhou et al., 2020; 2021)은 빅 데이터로 모델을 학습하여 좋은 립 동기화를 달성하지만 비디오 품질은 좋지 않습니다. 일부 최근의 신경 렌더링 기반 방법(Guo et al., 2021; Tang et al., 2022; Liu et al., 2022)은 고충실도 목표를 달성하지만 오디오-입술 훈련 쌍의 양이 적어 입술 동기화가 좋지 않습니다.Ada-TTA: 적응형 고품질 텍스트-토킹 아바타 합성을 향하여 이 논문에서는 앞서 언급한 문제를 처리하기 위해 Ada-TTA를 제안합니다.Ada-TTA는 TTS와 TFG의 공동 시스템으로, 각 하위 작업의 최신 발전을 활용합니다.TTS 모델의 신원 보존 능력을 향상시키기 위해 20,000시간 분량의 TTS 데이터 세트에서 훈련된 잘 설계된 제로샷 다중 화자 TTS 모델을 소개합니다.이 모델은 보이지 않는 화자의 짧은 녹음 하나만으로 고품질의 개인화된 음성을 합성할 수 있습니다. 고충실도 및 립싱크된 말하는 얼굴 생성을 달성하기 위해, 우리는 최근 제안된 GeneFace++(Ye et al., 2023a)를 TFG 시스템으로 활용합니다. 이는 고충실도를 유지하면서도 이전 신경 렌더링 기반 방법의 립싱크 및 시스템 효율성을 개선하기 때문입니다. TTS와 TFG에서 이 두 가지 고급 시스템의 장점을 결합한 Ada-TTA는 리소스는 적지만 고품질의 텍스트-말하는 아바타 합성을 달성합니다. 실험은 합성된 음성 및 비디오 측면에서 Ada-TTA의 좋은 성능을 보여줍니다. 또한 Ada-TTA가 객관적 및 주관적 지표의 관점에서 기준선보다 성능이 우수함을 보여줍니다. 2.
--- RELATED WORK ---
s 저희의 작업은 주로 저자원 개인화된 텍스트-음성 변환 및 말하는 얼굴 생성과 관련이 있습니다. 저희는 이 두 분야의 관련 작업을 각각 논의합니다. 이전의 개인화된 음성 생성 접근 방식(제로샷 멀티 스피커 TTS라고도 함)은 스피커 적응 및 스피커 인코딩으로 분류할 수 있습니다.
--- METHOD ---
앞서 언급한 두 가지 과제를 극복하고 신원을 보존하는 음성과 사실적으로 말하는 사람 비디오를 생성하는 데 성공했습니다.
--- EXPERIMENT ---
s는 우리의 방법이 사실적이고, 신원을 보존하고, 시청각적으로 동기화된 대화 아바타 비디오를 합성할 수 있음을 보여줍니다. 1. 서론 최근 몇 년 동안 다양한 분야에서 생성적 인공 지능이 등장했습니다. 예를 들어, 대규모 언어 모델(LLM) 기반 챗봇(Adamopoulou &amp; Moussiades, 2020)을 사용하면 고품질, 자연스럽고 사실적인 대화 텍스트 콘텐츠를 얻을 수 있습니다. 고급 텍스트 음성 변환(TTS) 시스템(Kim et al., 2021; Ren et al., 2021; Wang et al., 2023; Ye et al., 2023b)을 사용하면 *균등한 기여 #ByteDance 인턴 &#39;저장대학교 ByteDance. 연락처: Zhou Zhao<zhaozhou@zju.edu.cn> . 2023년 제40회 국제 머신러닝 컨퍼런스 회의록. 저작권 2023년 저자(들) 소유. 참조 오디오와 일반 텍스트가 주어진 개인화된 음성. 신경 렌더링 기술의 개발은 또한 몇 개의 훈련 샘플만으로 고충실도의 사실적인 화자 영상 생성(TFG)(Prajwal et al., 2020; Guo et al., 2021; Ye et al., 2023c)을 달성할 수 있게 합니다. TTS 모델과 TFG 방법을 결합하여 공동 시스템을 통해 사용자가 텍스트 입력만으로 화자 영상을 만들 수 있도록 하는 것은 자연스러운 생각입니다. 이 공동 시스템은 ChatGPT(Ouyang et al., 2022)의 최근 발전을 고려할 때 뉴스 방송, 가상 강의, 화자 챗봇과 같은 광범위한 잠재적 응용 분야를 가지고 있습니다. 그러나 이전의 TTS 및 TFG 시스템은 일반적으로 만족스러운 개인화된 결과를 생성하기 위해 많은 양의 신원별 데이터가 필요했습니다(Ren et al., 2020; Suwajanakorn et al., 2017). 이는 일반적으로 대상 인물의 몇 분 길이의 비디오만 사용할 수 있는 실제 시나리오에 과제를 제기합니다. 이러한 관찰에 동기를 부여받아 저희는 저리소스 텍스트-대화 아바타(TTA)라는 새로운 작업에 관심을 갖게 되었습니다. 훈련 데이터로 필사된 오디오 트랙이 있는 몇 분 길이의 대화 인물 비디오만 주어진 경우, 저희는 주도적인 입력 텍스트가 주어진 경우 신원 보존 및 오디오-립 동기화된 대화 인물 비디오를 합성하는 것을 목표로 합니다. 저희는 먼저 TTS와 TFG의 과제를 각각 고려합니다. TTS 단계의 경우 주요 과제는 입력 오디오의 음색 신원을 적절하게 보존하는 방법입니다(Kharitonov et al., 2023). 순진한 해결책은 주어진 텍스트-오디오 쌍에서 사전 훈련된 TTS 모델을 미세 조정하는 것입니다. 그러나 미세 조정은 큰 지연을 유도하고, 제한된 양의 데이터, 일반화 가능성으로 인해 성능이 보장되지 않습니다.또 다른 해결책은 기성품 툴킷(Jia et al., 2018)을 사용하여 입력 오디오의 스피커 임베딩을 추출하는 것입니다.이는 정보 손실을 도입하는 것으로 알려져 있으며 신원 보존 품질이 만족스럽지 않습니다.TFG 단계의 경우 주요 과제는 제한된 양의 오디오-비디오 쌍을 감안할 때 고충실도 및 오디오-비주얼 동기화를 달성하는 것입니다.일부 제로샷 방법(Prajwal et al., 2020; Zhou et al., 2020; 2021)은 빅 데이터로 모델을 학습하여 좋은 립 동기화를 달성하지만 비디오 품질은 좋지 않습니다. 일부 최근의 신경 렌더링 기반 방법(Guo et al., 2021; Tang et al., 2022; Liu et al., 2022)은 고충실도 목표를 달성하지만, 오디오-입술 훈련 쌍의 양이 적어 입술 동기화가 좋지 않습니다.Ada-TTA: 적응형 고품질 텍스트-토킹 아바타 합성을 향하여 이 논문에서는 앞서 언급한 문제를 처리하기 위해 Ada-TTA를 제안합니다.Ada-TTA는 TTS와 TFG의 공동 시스템으로, 각 하위 작업의 최신 발전을 활용합니다.TTS 모델의 신원 보존 능력을 향상시키기 위해 20,000시간 분량의 TTS 데이터 세트에서 훈련된 잘 설계된 제로샷 다중 화자 TTS 모델을 소개합니다.이 모델은 보이지 않는 화자의 짧은 녹음 하나만으로 고품질의 개인화된 음성을 합성할 수 있습니다. 영어: 고충실도 및 립싱크된 화자 생성을 달성하기 위해, 우리는 최근 제안된 GeneFace++(Ye et al., 2023a)를 TFG 시스템으로 활용합니다.이는 고충실도를 유지하면서도 이전 신경 렌더링 기반 방법의 립싱크 및 시스템 효율성을 개선하기 때문입니다.TTS와 TFG에서 이 두 가지 고급 시스템의 장점을 결합한 Ada-TTA는 리소스가 적지만 고품질의 텍스트-화자 아바타 합성을 달성합니다.실험은 합성된 음성 및 비디오 측면에서 Ada-TTA의 우수한 성능을 보여줍니다.또한 Ada-TTA가 객관적 및 주관적 지표의 관점에서 기준선보다 성능이 우수함을 보여줍니다.2. 관련 연구 우리의 연구는 주로 리소스가 적은 개인화된 텍스트-음성 및 화자 생성과 관련이 있습니다.우리는 이 두 분야의 관련 연구에 대해 논의합니다.제로샷 다중 화자 TTS라고도 하는 이전의 개인화된 음성 생성 접근 방식은 화자 적응 및 화자 인코딩 방법으로 분류할 수 있습니다. 전통적인 연구(Ren et al., 2019; Casanova et al., 2022; Ye et al., 2022)는 일반적으로 작은 읽기 스타일 데이터 세트에서 학습되며 보이지 않는 화자에 대해 일반화할 수 없습니다. 대규모 다중 도메인 데이터 세트에서 학습된 최근의 일부 연구는 제로샷 시나리오에서 효과를 보여줍니다. 그 중 일부 연구(Wang et al., 2023; Shen et al., 2023)는 신경 오디오 코덱 모델을 사용하여 오디오 파형을 잠재 파형으로 변환하고 음성 생성을 위한 중간 표현으로 간주하는데, 이는 음성 속성의 내재적 속성을 무시하고 열등하거나 제어할 수 없는 결과(예: 저하된 음색 동일성 유사성 및 제어할 수 없는 음성)로 이어질 수 있습니다. 이와 대조적으로 제안된 제로샷 다중 화자 TTS 시스템은 음성을 여러 속성으로 풀어내고 각각을 적절한 귀납적 편향이 있는 아키텍처를 사용하여 모델링하여 동일성 보존 능력과 음성적 자연성을 개선합니다. 제로샷/저리소스 TFG(Prajwal 등, 2020; Zhou 등, 2021; 2020)의 기존 작업은 일반적으로 신원의 참조 이미지가 주어진 GAN 기반 렌더러를 채택하는데, 이는 사실적이고 신원을 보존하는 비디오를 생성하지 못합니다.최근 작업에서는 사실적인 3D 포즈 제어를 가능하게 하고 좋은 비디오를 얻을 수 있기 때문에 신경 광도장(NeRF)(Mildenhall 등, 2020; Guo 등, 2021)을 채택했습니다.오디오 트랙 비디오 프레임 입력 텍스트 &quot;안녕하세요 여러분...&quot; 참조 음색 운율 제로샷 다중 화자 TTS 합성 음성 렌더러 교육 외관 기하학 말하는 얼굴 생성 합성 비디오 그림 1: Ada-TTA의 전체 파이프라인.점선은 프로세스가 교육 단계에서만 실행됨을 나타냅니다.제한된 데이터로 신원의 품질을 향상시킵니다. 그런 다음 일부 연구에서는 샘플 효율적이고 시간 효율적인 학습을 탐구합니다(Liu et al., 2022; Shen et al., 2022). 최근 제안된 방법인 GeneFace++(Ye et al., 2023a)는 OOD 오디오에 대한 입술 동기화를 개선하고 실시간 추론을 달성하여 실제 시나리오에 적용 가능한 NeRF 기반 TFG를 촉진합니다. 3. 제안 방법 제안된 시스템은 두 가지 주요 모듈로 구성됩니다. 제로샷 다중 스피커 TTS 모듈과 TFG 모듈입니다. 먼저 TTS 모델은 대상 ID의 참조 오디오 클립을 얻고 컨텍스트 내 학습 방식으로 음색과 음조 패턴을 추출한 다음 입력 텍스트를 음성 오디오로 변환합니다. 그런 다음 TFG 모듈은 입력 음성과 동기화된 말하는 사람 초상화 비디오를 합성합니다. 시스템의 전체 파이프라인은 그림 1에 나와 있습니다. 3.1. Zero-Shot Multi-Speaker TTS 우리는 내부 zero-shot multi-speaker TTS 모델을 사용합니다. 그림 2에서 볼 수 있듯이, 이것은 VQGAN 기반(Esser et al., 2021) TTS 모델로, 음색 인코더, 텍스트 인코더, 벡터 양자화(VQ) 음소 인코더, 그리고 멜 디코더로 구성되어 멜 스펙트로그램을 음성 속성(예: 음소와 음색)으로 풀어냅니다. 우리는 신중하게 설계된 정보 병목 현상으로 멜 스펙트로그램을 풀어냅니다. 1) 텍스트 인코더를 사용하여 음소 시퀀스를 콘텐츠 표현으로 인코딩합니다. 2) 같은 화자의 다른 음성에서 샘플링한 참조 멜 스펙트로그램을 입력하여 음색과 내용 정보를 풀어내고 음색 인코더의 출력을 시간적으로 평균하여 1차원 글로벌 음색을 구합니다.Ada-TTA: 적응형 고품질 텍스트-대-화상 아바타 합성을 향하여 P-LLM O 멜 디코더 학습 VQ-프로소디 음색 인코더 인코더 텍스트 인코더 참조 멜 &quot;안녕하세요 여러분...&quot; 텍스트 그림 2: 내부 제로샷 다중 화자 TTS 모델의 전체 구조.벡터. 3) 각 멜 스펙트로그램 프레임의 처음 20개 빈을 프로소디 인코더에 입력하는데, 이는 거의 완전한 프로소디와 적은 음색/내용 정보를 포함하고 있기 때문입니다. 또한 정보 흐름을 제한하기 위해 신중하게 조정된 벡터 양자화(VQ) 계층과 음소 수준 다운샘플링 계층을 프로소디 인코더에 도입합니다. 올바르게 설계된 병목 현상은 프로소디 인코더의 출력에서 콘텐츠 정보와 글로벌 음색 정보를 제거하는 방법을 학습하여 얽힘 해소 성능을 보장합니다. 훈련하는 동안 GT 멜 스펙트로그램을 사용하여 프로소디 시퀀스를 추출합니다. 그리고 추론 단계에서는 입력 텍스트가 주어졌을 때 프로소디 시퀀스를 예측해야 합니다. 이를 위해 LLM의 강력한 맥락 내 학습 기능을 활용하여 자기 회귀 방식으로 프로소디 분포에 맞는 프로소디 대규모 언어 모델(P-LLM)을 학습합니다. 구체적으로 P-LLM은 참조 음성의 프로소디 코드를 프롬프트로 사용하여 대상 음성의 프로소디 코드를 생성하는 디코더 전용 변환기 기반 아키텍처입니다. P-LLM이 훈련되면 추론 단계에서 주어진 텍스트 시퀀스의 콘텐츠, 프롬프트 음성에서 추출한 음색, P-LLM에서 예측한 프로소디를 사용하여 제로샷 개인화 음성 합성을 위한 대상 음성을 생성하는 것을 제안합니다. 3.2. 말하는 얼굴 생성 우리는 GeneFace++(Ye et al., 2023a)를 말하는 얼굴 생성 모델로 채택했습니다. 이는 입술 동기화 및 고충실도 말하는 얼굴 비디오를 실시간으로 렌더링할 수 있는 저리소스 TFG 방법입니다. 이는 입력 음성을 얼굴 랜드마크로 변환하는 오디오-모션 모듈과 랜드마크가 주어진 비디오 프레임을 합성하는 모션-비디오 렌더링으로 구성됩니다. 우리는 원시 파형에서 피치 윤곽선과 HUBERT 특징을 추출하여 오디오 표현을 하고, 재구성된 3DMM 메시에서 68개의 얼굴 랜드마크를 모션 표현으로 선택합니다. 오디오-모션 모듈은 일반적인 HuBERT 조건화 VAE와 신원별 포스트넷으로 구성되어 오디오 동기화 및 신원 보존 얼굴 랜드마크를 생성합니다. 모션-비디오 렌더러는 랜드마크 조건화 NeRF로 얼굴 랜드마크와 카메라 포즈를 조정하여 얼굴 표정과 머리 포즈를 자유롭게 제어할 수 있습니다. 시스템 효율성을 개선하기 위해 오디오-모션 모듈은 전체 오디오 시퀀스를 한 번에 처리하는 비자기회귀 네트워크 구조로 구축되었으며, 모션-비디오 렌더러는 이전의 계산적으로 비싼 고밀도 MLP 쿼리를 학습 가능한 그리드의 간단한 바이리니어 인덱싱으로 대체하는 그리드 기반 NeRF의 최근 발전을 채택했습니다.3.3. 학습 및 추론 제로샷 멀티 스피커 TTS 모델의 학습 절차는 두 단계로 구성됩니다.첫 번째 단계에서는 입력 텍스트, GT 프로소디 코드 및 참조 멜스펙트로그램이 주어진 멜스펙트로그램을 재구성하도록 VQGAN 기반 TTS 모델을 학습합니다. VQGAN 기반 TTS의 훈련 손실은 다음과 같습니다. L = ||y − ŷ||² + LvQ + L Adv (1) 여기서 yŷ는 mel-spectrogram의 L2 손실이고, LvQ는 VQVAE 손실 함수(Van Den Oord et al., 2017; Esser et al., 2021)이고, Adv는 LSGAN 스타일의 적대적 손실(Mao et al., 2017)입니다. 그런 다음 두 번째 단계에서 입력 텍스트와 이전 프로소디 코드 시퀀스가 주어진 프로소디 코드를 예측하기 위해 P-LLM을 훈련합니다. P-LLM은 훈련 단계에서 교차 엔트로피 손실을 통해 교사 강제 모드로 훈련됩니다. 말하는 얼굴 생성 모델을 훈련할 때 GeneFace++의 사전 훈련된 오디오-모션 모듈을 사용하고 NeRF 기반 렌더러를 처음부터 훈련합니다. 이미지 품질을 더욱 개선하기 위해 예측 이미지의 입술 부분에 VGG 지각 손실을 채택합니다. 추론하는 동안, 제로샷 멀티 스피커 TTS 모델은 입력 텍스트가 주어지면 신원 보존 음성을 합성한 다음, 말하는 얼굴 생성 모델은 합성된 오디오를 입력으로 사용하여 말하는 초상화 이미지를 생성합니다. 마지막으로, 합성된 오디오 및 비디오 프레임이 최종 비디오에 통합됩니다. 4. 실험 훈련 세부 정보. TTS 모델의 경우 음색과 음조의 일반화 가능성을 개선하기 위해 TTS 모델을 2억 2,250만 개의 매개변수로 확장하고 모델을 훈련하기 위해 10,000시간 분량의 영어 TTS 데이터 세트인 GigaSpeech(Chen et al., 2021)를 채택합니다. 이 대규모 TTS 모델을 각 GPU에 30개 문장의 배치 크기로 8개의 NVIDIA A100 GPU에서 훈련합니다. 수렴에는 420k 단계가 필요하고 furYourTTS+ Wav2Lip Ada-TTA Ada-TTA: 적응형 고품질 텍스트-대-화상 아바타 합성을 향하여 표 1: TTA 시스템의 객관적 평가. Spk-Sim은 코사인 화자 유사도를 나타냅니다. 방법 Spk-Sim↑ FID↓ YourTTS + Wav2Lip 0.9392 55.Ada-TTA(저희) 0.9854 28.everyone together university<silence> 그림 3: TTA 시스템에서 생성한 비디오 프레임. 미세 조정이 필요합니다. 말하는 얼굴 생성의 경우 GeneFace++에서 제공하는 사전 훈련된 오디오-모션 모듈(Ye et al., 2023a)을 사용하여 각 특정 ID에 대해 NeRF 기반 렌더러를 320k 단계로 훈련합니다. 이는 1개의 NVIDIA A100 GPU에서 약 6시간이 걸립니다. 렌더러를 훈련하는 데 사용된 말하는 사람 비디오는 길이가 약 3분입니다. 비교 기준선. 공개적으로 사용 가능한 저리소스 텍스트-아바타 시스템이 없으므로 최근 제안된 제로샷 멀티 스피커 TTS인 YourTTS(Casanova et al., 2022)와 립 동기화 관점에서 최첨단 퓨샷 말하는 얼굴 생성 방법인 Wav2Lip(Prajwal et al., 2020)을 결합하여 기준선을 구성합니다. 기준선을 YourTTS+Wav2Lip으로 표시합니다. 추론 중에 Wav2Lip은 전체 교육 비디오를 입력으로 사용하고 입술 부분만 재생성합니다.반면에, 우리 모델은 전체 프레임을 렌더링합니다.평가 지표.텍스트-아바타 시스템의 성능을 평가하기 위해 CMOS(비교 평균 의견 점수) 테스트를 수행합니다.CMOS를 세 가지 측면에서 분석합니다.CMOS-A(화자 유사성, 음조, 오디오 품질을 포함한 오디오만 분석), CMOS-V(이미지 충실도, 3D 현실성, 신원 보존을 포함한 비디오만 분석), CMOS(합성 비디오의 전체 의견 점수).테스터에게 점수를 매길 때 하나의 해당 측면에 집중하고 다른 측면은 무시하라고 말합니다.주관적인 평가의 경우, 화자 검증을 위해 미세 조정된 WavLM(Chen et al., 2022) 모델을 사용하여 기준 진실 음성과 합성 음성 간의 코사인 유사도 점수를 계산합니다.FID를 사용하여 합성 비디오의 이미지 품질을 측정합니다.실험 결과. 표 1에 표시된 대로, 우리의 방법은 YourTTS보다 더 높은 화자 유사성을 달성하여 입력 참조 오디오의 동일성을 보존하는 데 있어 우리의 제로샷 멀티 화자 TTS 모델의 효과를 증명합니다. 또한 우리의 방법이 렌더링된 비디오 프레임의 더 나은 이미지 품질을 나타내는 Wav2Lip보다 더 나은 FID를 달성한다는 점에 주목합니다. 우리는 또한 CMOS 테스트를 수행하여 인간의 지각 측면에서 성능을 평가합니다. 결과는 표 2: TTA 시스템의 CMOS 결과입니다. 오차 막대는 표준 편차입니다. Y+W는 YourTTS+Wav2Lip을 나타냅니다. 방법 CMOS-A CMOS-V Y+W 0.0.Ada-TTA +0.84 0.50 +0.76±0.CMOS 0.+0.74 ± 0.표 2에 나열되어 있습니다. 우리는 오디오 품질(CMOS-A), 비디오 품질(CMOS-V) 및 전반적인 품질(CMOS) 측면에서 우리의 방법으로 합성된 비디오를 선호한다는 것을 발견했습니다. 더 나은 질적 비교를 위해, 우리는 독자들에게 데모 비디오¹를 시청할 것을 권장합니다. 또한 우리는 Ada-TTA와 기준선의 성능을 비교하기 위해 그림 3에 몇 가지 키 프레임을 제공합니다. 우리는 기준선과 비교했을 때, Ada-TTA가 (1) 높은 음색 유사성과 좋은 오디오 품질 측면에서 더 나은 음성을 생성하고, (2) 높은 립싱크, 좋은 이미지 충실도, 자유로운 머리 자세 제어를 갖춘 더 나은 비디오를 생성한다는 것을 관찰할 수 있습니다. 5.
--- CONCLUSION ---
s 이 논문에서는 적응형 고품질 텍스트-대화 아바타 합성 시스템인 Ada-TTA를 제시합니다. 몇 분 길이의 대화하는 사람 비디오만 학습 데이터로 제공하면 AdaTTA를 사용하여 임의의 입력 텍스트가 주어지면 신원을 보존하는 음성을 합성하고 립싱크 비디오를 생성할 수 있습니다. Ada-TTA 시스템을 구성하는 데 사용된 제로샷 다중 화자 TTS 모델과 고품질 대화 얼굴 생성 방법을 설명합니다. 실험을 통해 제한된 데이터 규모에서 사실적인 음성과 비디오를 합성하는 시스템의 능력을 입증했습니다. 참고문헌 Adamopoulou, E. 및 Moussiades, L. Chatbots: History, technology, and applications. Machine Learning with Applications, 2:100006, 2020. Casanova, E., Weber, J., Shulby, CD, Junior, AC, Gölge, E., 및 Ponti, MA Yourtts: 모든 사람을 위한 제로샷 다중 화자 TTS 및 제로샷 음성 변환을 향해. &#39;데모 비디오 링크는 https://genefaceplusplus.github.io/GeneFace++/ada_tta.mpAda-TTA: 국제 머신 러닝 컨퍼런스에서 적응형 고품질 텍스트-대화 아바타 합성을 향해, pp. 2709-2720. PMLR, 2022. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. Gigaspeech: 10,000시간 분량의 필사 오디오가 포함된 진화하는 다중 도메인 ASR 코퍼스. arXiv 사전 인쇄본 arXiv:2106.06909, 2021. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: 풀 스택 음성 처리를 위한 대규모 자체 감독 사전 학습. IEEE Journal of Selected Topics in Signal Processing, 16(6): 1505–1518, 2022. Esser, P., Rombach, R., and Ommer, B. 고해상도 이미지 합성을 위한 변압기 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 12873-12883, 2021. Guo, Y., Chen, K., Liang, S., Liu, Y.-J., Bao, H., Zhang, J. Ad-nerf: 토킹 헤드 합성을 위한 오디오 구동 신경 복사장. ICCV, pp. 5784-5794, 2021. Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., Ren, F., Nguyen, P., Pang, R., Lopez Moreno, I., Wu, Y., et al. 화자 검증에서 다중 화자 텍스트-음성 합성으로의 전이 학습. 신경 정보 처리 시스템의 발전, 31, 2018. Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., 및 Zeghidour, N. 말하고, 읽고, 촉구하기: 최소한의 감독으로 고품질 텍스트 음성 변환. arXiv 사전 인쇄본 arXiv:2302.03540, 2023. Kim, J., Kong, J., 및 Son, J. 엔드투엔드 텍스트 음성 변환을 위한 적대적 학습을 갖춘 조건부 변분 자동 인코더. ICML에서. PMLR, 2021. Liu, X., Xu, Y., Wu, Q., Zhou, H., Wu, W., 및 Zhou, B. 의미 인식 암묵적 신경 오디오 구동 비디오 초상화 생성. arXiv 사전 인쇄 arXiv:2201.07786, 2022. Mao, X., Li, Q., Xie, H., Lau, RY, Wang, Z. 및 Paul Smolley, S. 최소 제곱 생성적 적대 네트워크. ICCV, 2017. Mildenhall, B., Srinivasan, PP, Tancik, M., Barron, JT, Ramamoorthi, R. 및 Ng, R. Nerf: 장면을 뷰 합성을 위한 신경 방사 필드로 표현. ECCV, 405-421페이지. Springer, 2020. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. NeurIPS, 2022. Prajwal, K., Mukhopadhyay, R., Namboodiri, VP, Jawahar, C. 야생에서 음성 대 입술 생성을 위해서는 립싱크 전문가만 있으면 됩니다. ACM MM, pp. 484–492, 2020. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., Liu, T.-Y. Fastspeech: 빠르고 견고하며 제어 가능한 텍스트 음성 변환. 신경 정보 처리 시스템의 발전, 32, 2019. Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., Liu, T.-Y. Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트 음성 변환. arXiv 사전 인쇄본 arXiv:2006.04558, 2020. Ren, Y., Liu, J., and Zhao, Z. Portaspeech: Portable and high-quality generative text-to-speech. NIPS, 34:1396313974, 2021. Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizer. arXiv 사전 인쇄본 arXiv:2304.09116, 2023. Shen, S., Li, W., Zhu, Z., Duan, Y., Zhou, J., and Lu, J. Learning dynamic facial radiance fields for few-shot talking head synthesis. ECCV, 2022. Suwajanakorn, S., Seitz, SM, 및 KemelmacherShlizerman, I. 합성 오바마: 오디오에서 립싱크 학습. ACM 그래픽스 저널(ToG),(4):1-13, 2017. Tang, J., Wang, K., Zhou, H., Chen, X., He, D., Hu, T., Liu, J., Zeng, G., 및 Wang, J. 오디오 공간 분해를 통한 실시간 신경 광도 대화 초상화 합성. arXiv 사전 인쇄본 arXiv:2211.12368, 2022. Van Den Oord, A., Vinyals, O., et al. 신경 이산 표현 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. 신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다. arXiv 사전 인쇄본 arXiv:2301.02111, 2023. 구문 음성: Ye, Z., Zhao, Z., Ren, Y., 및 Wu, F. 구문 인식 생성적 적대적 텍스트 음성 합성. arXiv 사전 인쇄본 arXiv:2204.11792, 2022. Ye, Z., He, J., Jiang, Z., Huang, R., Huang, J., Liu, J., Ren, Y., Yin, X., Ma, Z., and Zhao, Z. Geneface++: 일반화되고 안정적인 실시간 오디오 구동 3D 화자 생성. arXiv 사전 인쇄본 arXiv:2305.00787, 2023a. Ye, Z., Huang, R., Ren, Y., Jiang, Z., Liu, J., He, J., Yin, X., and Zhao, Z. Clapspeech: 대조적 언어-오디오 사전 학습을 통해 텍스트 맥락에서 음성 학습. arXiv 사전 인쇄본 arXiv:2305.10763, 2023b. Ada-TTA: 적응형 고품질 텍스트-토킹 아바타 합성을 향해 Ye, Z., Jiang, Z., Ren, Y., Liu, J., He, J., and Zhao, Z. Geneface: 일반화되고 고충실도 오디오 구동 3D 토킹 페이스 합성. ICLR, 2023c. Zhou, H., Sun, Y., Wu, W., Loy, CC, Wang, X., and Liu, Z. 암묵적으로 모듈화된 오디오-비주얼 표현을 통한 포즈 제어 가능한 토킹 페이스 생성. CVPR, pp. 4176-4186, 2021. Zhou, Y., Han, X., Shechtman, E., Echevarria, J., Kalogerakis, E., and Li, D. Makelttalk: 화자 인식 토킹헤드 애니메이션. ACM 그래픽스 저널(TOG), 39(6):1-15, 2020.
