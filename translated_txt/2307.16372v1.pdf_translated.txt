--- ABSTRACT ---
주어진 음악 트랙에 대한 자연어 설명을 생성하는 자동 음악 캡션은 방대한 양의 음악 데이터에 대한 이해와 구성을 향상시키는 데 상당한 잠재력을 가지고 있습니다. 그 중요성에도 불구하고, 연구자들은 크기가 제한된 기존 음악 언어 데이터 세트의 비용이 많이 들고 시간이 많이 걸리는 수집 프로세스로 인해 어려움에 직면합니다. 이러한 데이터 부족 문제를 해결하기 위해 대규모 언어 모델(LLM)을 사용하여 대규모 태그 데이터 세트에서 설명 문장을 인공적으로 생성하는 것을 제안합니다. 그 결과 약 220만 개의 캡션과 0.5M 개의 오디오 클립이 페어링됩니다. 이를 대규모 언어 모델 기반 가상 음악 캡션 데이터 세트, 즉 LP-MusicCaps라고 합니다. 자연어 처리 분야에서 사용되는 다양한 양적 평가 지표와 인간 평가를 사용하여 대규모 음악 캡션 데이터 세트에 대한 체계적 평가를 수행합니다. 또한, 데이터 세트로 트랜스포머 기반 음악 캡션 모델을 학습하고 제로 샷 및 전이 학습 설정에서 평가했습니다. 결과는 제안된 접근 방식이 지도 기준 모델보다 성능이 우수함을 보여줍니다.1.
--- METHOD ---
. 2.1 데이터 생성을 위한 대규모 언어 모델 먼저 기존 음악 태그 데이터 세트에서 다중 레이블 태그를 가져옵니다. 태그 목록에 신중하게 작성된 작업 지침이 대규모 언어 모델에 대한 입력(프롬프트)으로 추가됩니다. 그런 다음 모델은 작업 지침이 조건으로 지정하는 방식으로 음악을 설명하는 문장을 생성하여 반환합니다. 표 1은 다중 레이블 태그와 작업 지침에 따라 생성된 캡션의 예를 보여줍니다. 언어 모델의 경우 다양한 작업에서 강력한 성능을 보이는 GPT-3.5 Turbo[23]를 선택했습니다. 학습하는 동안 먼저 대규모 코퍼스와 엄청난 컴퓨팅 성능으로 학습한 다음 주어진 지침과 더 나은 상호 작용을 위해 인간 피드백(RLHF)[24]을 통한 강화 학습으로 미세 조정했습니다. 그 결과 GPT-3.5 Turbo는 자연어 입력에 대한 인간과 유사한 응답을 이해하고 추론하고 생성하는 최첨단 제로샷 능력을 보여줍니다. LLM에는 광범위한 정보가 포함되어 있으므로 아티스트 이름이나 앨범 이름과 같은 일부 유명한 음악 엔터티를 기반으로 음악 캡션을 생성할 수 있습니다. 그러나 LLM은 확실한 어조로 부정확한 텍스트를 생성할 수 있으며, 이는 실제 사실 없이는 감지하기 어렵습니다. 환각으로 알려진 이 문제는 창의적인 목적으로 LLM을 사용할 때 재미있는 측면이 될 수 있습니다[25]. 그러나 결과 캡션이 사실이어야 하므로 저희와 같은 애플리케이션에서는 환각을 피해야 합니다. 따라서 이전 작업과 달리 메타데이터를 사용하지 않습니다[19]. 또한 제안된 평가 체계에 환각을 측정하는 질문을 추가했습니다. 2.2 작업 지침 설계 제안된 캡션 생성은 다음 공식을 따릅니다. ỹcap = fLLM(Ytag, i), 여기서 Ytag와 cap은 각각 다중 레이블 태그와 생성된 캡션을 나타내고 i는 제공된 작업 지침입니다. 동일한 모델과 입력을 사용하더라도 출력은 작업 지침에 따라 달라질 수 있으므로 작업 지침은 데이터 생성의 중요한 측면이 됩니다. 따라서 네 가지 다른 작업을 정의하고 그에 따라 캡션을 생성합니다. 쓰기: 다음 속성을 포함하는 노래 설명 문장을 작성합니다. {입력 태그} 요약: 다음 속성을 가진 노래를 요약하는 단일 문장을 작성합니다. 아티스트 이름이나 앨범 이름은 쓰지 마세요. {입력 태그} 입력 태그 비디오 게임 테마, 가수 없음, 악기 연주, 아날로그 사운드, 작은 키보드, 비트박싱, 장난기 있는, 쾌활한, 그루비 &quot;쓰기&quot; 지침의 결과 이 악기 연주 트랙은 비디오 게임 테마에 완벽한 즐겁고 쾌활한 분위기를 가지고 있습니다. 가수가 없는 아날로그 사운드의 음악은 작은 키보드와 비트박싱을 특징으로 하며, 그루비하고 쾌활한 분위기를 조성합니다. &quot;요약&quot; 지침의 결과 쾌활하고 그루비한 악기 연주 트랙은 장난기 있는 비트박싱과 아날로그 사운드를 특징으로 하며, 작은 키보드와 비디오 게임에서 영감을 받은 테마를 특징으로 합니다. &quot;의역&quot; 지침의 결과 이 경쾌한 악기 연주 곡은 아날로그 사운드의 비트, 장난기 있는 비트박싱 리듬, 그루비한 작은 키보드 멜로디를 통합하여 즐거운 비디오 게임 테마의 본질을 포착했으며, 가수가 필요하지 않습니다. &quot;속성 예측&quot; 지침의 결과 {&quot;new_attribute&quot;: [&quot;8비트 사운드&quot;, &quot;칩튠 스타일&quot;, &quot;레트로 분위기&quot;], &quot;description&quot;: &quot;이 악기 연주곡은 아날로그 사운드의 멜로디와 작은 키보드 울림으로 비디오 게임에서 곧장 나온 것 같습니다. 비트박싱은 그루비하고 쾌활한 분위기에 유쾌한 요소를 더합니다. 고전적인 8비트 사운드와 칩튠 스타일을 연상시키는 이 레트로 분위기는 분명 당신의 얼굴에 미소를 가져다 줄 것입니다.&quot;} Ground Truth 이것은 비디오 게임 테마 커버입니다. 테마는 슈퍼 마리오 프랜차이즈에 속합니다. 메인 테마는 아날로그 사운드의 작은 키보드에서 연주됩니다. 이 버전에는 비트박싱의 리드미컬한 배경이 추가되었습니다. 분위기가 유쾌합니다. 이 작품은 아케이드 게임 소셜 미디어 콘텐츠의 배경에 사용될 수 있습니다. 표 1. MusicCaps 데이터 세트에서 생성된 캡션의 예. 의역: 다음 속성을 포함하는 노래 설명 문장을 작성하세요. 창의적인 의역은 허용됩니다. {input tags} 속성 예측: new_attribute와 description을 키로 하는 Python 사전으로 답을 작성합니다. new_attribute의 경우 다음 속성과 높은 동시 발생률을 보이는 새 속성을 작성합니다. description의 경우 다음 속성과 새 속성을 포함하는 노래 설명 문장을 작성합니다. {input tags} 모든 지침에서 환각을 방지하기 위해 &#39;include / with the following properties&#39;를 추가합니다. &quot;Writing&quot; 작업 지침은 태그를 사용하여 문장을 생성하는 간단한 프롬프트입니다. &quot;Summary&quot; 작업 지침은 정보를 짧은 길이로 압축하는 것을 목표로 합니다. &quot;Paraphrase&quot; 작업 지침은 어휘를 확장합니다. 마지막으로 &quot;Attribute Prediction&quot; 작업 지침은 대규모 코퍼스(즉, GPT-3.5 Turbo의 훈련 데이터)에서 태그 동시 발생률을 기반으로 새 태그를 예측합니다. 이는 기존 태그 데이터 세트에서 높은 거짓 부정률 문제를 해결하고 환각 위험을 완화할 것으로 예상됩니다. 이 지침에서 &#39;새로운 속성&#39;은 설명과 입력을 연결하기 위해 존재하며, 우리는 &#39;설명&#39;만 캡션으로 사용합니다.3. 가상 캡션 평가 생성된 캡션의 품질을 보장하는 것이 중요한데, 특히 캡션은 실제 값으로 사용되어야 하기 때문입니다.이 섹션에서는 객관적이고 주관적인 평가를 포함하는 전체적인 평가 체계와 제안된 방법의 캡션에 대한 결과를 소개합니다.3.1 객관적 평가 MusicCaps 데이터 세트[12]를 사용하여 생성된 캡션에 대한 평가를 수행합니다.이 데이터 세트에는 오디오(x), 태그 목록(tag) 및 실제 값 캡션(cap)이 있습니다.실제 값 캡션(cap)은 섹션 2.2에서 설명한 대로 평가 분할의 모든 항목에 대해 미리 정의된 네 가지 지침으로 생성됩니다.평가하는 동안 생성된 캡션은 n-gram, 신경망 메트릭과 관련하여 실제 값 캡션과 비교됩니다.또한 다양성 메트릭을 보고합니다. 이전 연구[5]에 따라 4개의 ngram 메트릭[26–28]을 측정합니다. BLEU1~4(B1, B2, B3, B4), METEOR(M), ROUGE-L(RL)입니다. 이들은 모두 기준 진실과 생성된 캡션 간의 n-gram 정밀도와 재현율을 기반으로 합니다. 이러한 메트릭은 캡션 품질의 다양한 측면을 포착합니다. BLEU와 METEOR는 생성된 캡션과 기준 진실 캡션 간의 ngram 중복에 초점을 맞추는 반면, ROUGE-L은 두 캡션 간의 가장 긴 공통 부분 시퀀스를 측정합니다. 또한 사전 훈련된 BERT 임베딩을 기반으로 하는 BERT-Score(BERT-S)를 사용하여 생성된 캡션과 관련하여 기준 진실의 토큰을 표현하고 일치시킵니다[29]. 각 토큰의 BERT 임베딩 간의 유사성을 계산함으로써 BERT-Score는 n-gram 메트릭보다 생성된 캡션과 기준 진실 캡션 간의 의미적 유사성을 더 잘 포착할 수 있습니다. 동의어, 의역 및 어순 변형에 더 강력하기 때문입니다. 마지막으로, 생성된 캡션의 다양성을 평가하기 위해 서로 다른 단어가 얼마나 많이 사용되었는지 측정합니다. novel은 훈련 어휘에 없는 생성된 캡션의 새 어휘 비율을 나타냅니다. Vocab은 모든 생성된 캡션에서 사용된 고유한 단어의 수입니다. 다양성 지표는 일반적으로 보조적인 것으로 간주되며 생성된 캡션의 전반적인 품질을 포착하지 못한다는 점에 유의해야 합니다. 3.2 주관적 평가 이전 연구[12]에 따라 참가자에게 10초 분량의 단일 음악 클립과 두 개의 텍스트 설명이 제공되는 A 대 B 인간 평가 작업을 설정했습니다. MusicCaps 평가 데이터 세트에서 240개의 음악 샘플을 무작위로 선택했습니다. 연구 목표는 지도 학습 지표 다양성 지표 길이 방법 LM 매개변수 B1↑ B2↑ B3↑ B4↑ M↑ RL↑ BERT-S↑ Vocab↑ Novely ↑ Avg. 토큰 베이스라인 태그 Concat [2, 13] 템플릿 [14] K2C 8월 [22] T220M 20.25.6.13.57 8.64 5.42 23.24 19.16.3.86.46.20.6±11.10.00 6.15 25.57 21.87.46.25.6±11.1.58 0.85 14.23 17.86.67.14.7+5.제안된 명령어 작성 요약 Pharapase GPT3.5 175B+ GPT3.5 175B+ GPT3.5 175B+ 36.84 19.26.12 14.11.37 6.74 31.8.80 5.52 27.25.89.56.44.4±17.25.89.49.28.6±10.속성 예측 GPT3.5 175B+ 36.51 18.73 10.33 5.87 30.36 23.35.26 18.16 9.69 5.41 34.09 23.19 88.88.59.47.9±18.63.66.2±21.표 2. 기존의 가상 캡션 생성 방법과 제안하는 방법의 성능. LM은 언어 모델을 나타냅니다. Avg.Token은 캡션당 토큰의 평균 수를 나타냅니다. 더 많은 True Positive?LoseTie Winwin+tie lose Less False Positive?win+tie Lose Tie Win 더 많은 True Positive?더 많은 True Positive?더 많은 True Positive?더 많은 True Positive? 영어: Lose Tie Win Lose Lose Lose Tie Tie Tie Win Win win+tie lose False Positive 감소?Lose Tie Win win+tie lose False Positive 감소?Lose Tie Win win+tie lose False Positive 감소?Lose Tie Win win+tie lose False Positive 감소?Lose Tie Win win+tie lose False Positive 감소?Lose Tie Win lose win+tie lose win+tie lose (b) 쓰기 (c) 요약 win+tie (d) 의역 lose (a) K2C 증강 win+tie lose (e) 속성 예측 그림 2. A 대 B 테스트 결과.각 방법은 True Positive가 더 많고 False Positive가 더 적은 측면에서 기준 진실과 비교됩니다.제안된 방법(b, c, d, e)은 기준 진실과 비슷한 win+tie 성능을 보여줍니다.가상 기준 진실로 사용할 수 있는 음악 캡션의 경우 한 설명은 항상 기준 진실에 고정되고 다른 설명은 K2C 증강[22] 및 제안된 네 가지 지침 방법을 포함한 생성된 캡션의 5가지 유형에서 선택됩니다.이렇게 하면 최대 1200(= 240 x 5)개의 질문이 생성됩니다. 우리는 음악 연구자 또는 음악 산업 전문가인 24명의 참여자를 고용했습니다. 그들 각자는 무작위로 선택된 20개의 질문에 평가했습니다. 그 결과, 총 480개의 평가를 수집했습니다. 평가자는 두 가지 다른 측면에서 캡션 품질을 평가하도록 요청받았습니다. (Q1) 더 많은 True Positive: 어떤 캡션이 음악을 더 정확한 속성으로 설명합니까? (Q2) 덜 False Positive: 어떤 캡션이 음악을 덜 잘못 설명합니까? 예를 들어, 어떤 방법이 많은 음악 속성이 있는 길고 다양한 문장을 생성하는 경우 Q1에는 유리하지만 Q2에는 불리할 수 있습니다. 반대로, 어떤 방법이 음악 속성이 적은 짧은 문장을 보수적으로 생성하는 경우 Q2에는 유리하지만 Q1에는 불리할 수 있습니다. 우리는 쌍별 검정에서 승리, 무승부, 패배의 수를 세어 조건의 순위를 결정합니다. 3.3 결과 우리는 LLM 기반 캡션 생성을 두 가지 템플릿 기반 방법(태그 연결, 프롬프트 템플릿 2) 및 K2C 증강[22]과 비교합니다. 표 2에서 MusicCaps[12] 평가 세트에 대한 캡션 결과를 제시합니다. 제안된 방법을 기존의 meth2 템플릿 예제와 비교할 때: 음악은 {입력 태그} ods로 특징지어지며, n-gram 메트릭에서 상당한 차이를 관찰합니다. 이는 태그 연결이 문장 구조를 완성하지 못하기 때문입니다. K2C 증강의 경우 지시가 없기 때문에 입력 태그가 생성된 캡션에서 제외되거나 노래 설명 문장과 관련 없는 문장이 생성됩니다. 반면, 템플릿 기반 모델은 템플릿에 음악적 맥락이 존재하기 때문에 성능이 향상되었습니다. 다음으로 BERT-Score를 사용한 다양성 메트릭을 고려합니다. 제안된 방법은 다양한 어휘를 생성하는 동안 BERT-Score에서 더 높은 값을 보여줍니다. 이는 새로 생성된 어휘가 음악 의미론에 해를 끼치지 않는다는 것을 나타냅니다. 제안된 다양한 작업 지시를 비교하면 각 지시가 다른 역할을 수행한다는 것을 알 수 있습니다. &quot;쓰기&quot;는 입력 태그를 충실하게 사용하여 캡션을 생성하기 때문에 높은 n-gram 성능을 보여줍니다. &quot;요약&quot;은 정보 압축으로 인해 토큰의 평균 수가 가장 적지만 요약에 특화된 ROUGE-L에서 경쟁력 있는 성능을 보이며 BERT-Score도 가장 높습니다. &quot;의역&quot;은 많은 동의어를 생성하여 어휘 크기가 크고 새로운 어휘를 사용합니다. &quot;속성 예측&quot;은 태그의 동시 발생을 기반으로 새로운 태그를 예측합니다. 이 명령어는 BLEU에서는 성능이 낮지만 WordNet과 같은 동의어 사전을 사용하여 유사한 의미를 가진 단어의 정확도 점수를 고려하는 METEOR에서는 경쟁력 있는 결과를 보이며, 새로 예측된 태그는 기준 진실과 유사한 의미를 가지고 있음을 나타냅니다. 그림 2는 주관적인 A 대 B 테스트 결과를 보여줍니다. 각 데이터 세트 # 항목 기간(시간) C/A 평균 토큰 일반 오디오 도메인 AudioCaps [30] 51k 144.9.0±N/A LAION-Audio [22] 630k 4325.1-WavCaps [18] 403k 7568.N/A 7.8±N/A 레이어 표준 음악 도메인 FFN MusicCaps [12] 6k 15.48.9±17.MuLaMCap* [19] 393k 1091.N/A 레이어 표준 LP-MusicCaps-MC 6k 15.44.9+21.LP-MusicCaps-MTT 22k 180.24.8±13.LP-MusicCaps-MSD 514k 4283.37.3±26.Next-토큰 예측 피아노와 레이어 표준 FFN 레이어 표준 표 3이 있는 부드러운 기타 곡. 오디오-캡션 쌍 데이터 세트의 비교. C/A는 오디오당 캡션 수를 의미합니다. *비교를 위해 표에 MuLaMCap을 포함했지만 공개적으로 액세스할 수 없습니다. 이 방법은 더 많은 참 양성(Q1)과 더 적은 거짓 양성(Q2)을 갖는 측면에서 기준 진실과 비교됩니다. 첫 번째 질문의 경우, 기준 K2C 증강과 비교했을 때 지침을 사용하는 제안된 방법은 압도적으로 더 높은 승+무승부 점수를 보여줍니다. 이는 LLM을 활용할 때 음악별 지침의 중요성을 나타냅니다. 특히, &quot;Paraphrase&quot;와 &quot;Attribute Prediction❞&quot;은 기존 어휘와 다른 새로운 정보를 통합하여 높은 승점을 달성합니다. 두 번째 질문에서 &quot;Attribute Prediction&quot;을 제외한 모든 캡션 생성 방법은 패자 점수보다 승+무승부 점수가 더 높습니다. 이는 LLM 기반 캡션 생성이 실제 결과와 비슷하거나 더 낮은 거짓 양성률을 보이기 때문에 신뢰성이 있음을 주장합니다. 가장 긴 평균 길이를 가진 &quot;Attribute Prediction&quot;은 &#39;너무 창의적&#39;이며 실제 결과보다 약간 더 높은 거짓 양성률을 보입니다. 4. 데이터 세트: LP-MusicCaps 제안된 가상 캡션 생성 방법을 기반으로 LLM 기반 가상 음악 캡션 데이터 세트인 LP-MusicCaps를 소개합니다. 기존 다중 레이블 태그 데이터 세트 3개와 작업 지침 4개를 사용하여 음악-캡션 쌍을 구성합니다. 데이터 소스는 MusicCaps[12], Magnatagtune[31], Million Song Dataset[32] ECALS 하위 세트[13]입니다. 각각 이를 MC, MTT, MSD라고 합니다. MC에는 5,521개의 음악 예제가 포함되어 있으며, 각각 3개에 음악 전문가가 작성한 13,219개의 고유한 측면이 레이블되어 있습니다. MTT[31]는 장르, 악기, 보컬, 분위기, 지각적 템포, 기원 및 음향 특성을 포함하여 5,223개의 고유한 노래에서 26,000개의 음악 클립으로 구성되어 있습니다. 전체 188개 태그 어휘를 사용했으며 연관된 태그가 없는 트랙에 대한 캡션은 생성하지 않았습니다(22,000개로 감소). MSD는 52만 개의 30초 클립과 1,054개의 태그 어휘로 구성되어 있습니다[13]. 태그 어휘는 장르, 스타일, 악기, 보컬, 분위기, 테마 및 문화를 포함한 다양한 범주를 포괄합니다. 각 데이터 세트는 각각 가상 캡션을 생성하기 위해 음악 클립당 평균 10.7/3.3/10.2개의 레이블을 사용합니다. 표 3은 LP-MusicCaps 제품군과 다른 오디오 캡션 쌍 3 간의 통계를 비교한 것입니다. 26개 데이터 샘플이 손실되어 총 5495개만 사용합니다. 다중 헤드 자기 주의 x 다중 헤드 교차 자기 주의 계층 규범 사인파 위치 인코딩 마스크된 다중 헤드 자기 주의 스트라이드 Conv1D+GELU Conv1D+GELU x 학습된 위치 인코딩 피아노가 있는 이 부드러운 기타 곡 음악 캡션 10초 로그-멜 스펙트로그램 그림 3. 교차 모달 인코더-디코더 아키텍처. 데이터 세트. 두 도메인을 비교할 때 AudioCaps[30]와 MusicCaps는 고품질의 인간 주석 캡션을 제공하지만 오디오 지속 시간이 짧은 캡션이 적습니다. 대규모 데이터 세트를 비교할 때 음악 도메인은 일반 오디오 도메인(예: LAION-Audio[22] 및 WavCaps[18])에 비해 사용 가능한 데이터 세트가 부족합니다. MuLaMCap은 엄청난 양의 주석이 달린 캡션을 제공하지만 공개적으로 사용할 수 없습니다. 반면 LM-MusicCaps는 공개적으로 사용할 수 있으며 다양한 스케일로 제공됩니다. LP-MusicCaps-MC는 수동으로 작성한 캡션과 비슷한 캡션 길이를 가지고 있지만 오디오당 캡션이 4배 더 많습니다. LPMusicCaps-MTT는 오디오 다운로드 링크가 있는 중간 크기의 데이터 세트이고 LP-MusicCaps-MSD는 음악 도메인 캡션 데이터 세트의 다양한 캡션 중에서 가장 긴 오디오 지속 시간을 가지고 있습니다. 5. 자동 음악 캡션 우리는 음악 캡션 모델을 훈련하고 제로 샷 및 전이 학습 설정에서 평가했습니다. 이 섹션에서는 다음을 보고합니다.
--- EXPERIMENT ---
모든 결과.5.1 인코더-디코더 모델 우리는 그림 3에서와 같이 다양한 자연어 처리 작업[33], 가사 해석[34] 및 음성 인식[35]에서 뛰어난 결과를 얻은 교차 모달 인코더-디코더 변환기 아키텍처를 사용했습니다.Whisper[35]와 유사하게 인코더는 필터 폭이 3이고 GELU[36] 활성화 함수가 있는 6개의 합성곱 계층이 있는 로그멜 스펙트로그램을 사용합니다.첫 번째 계층을 제외하고 각 합성곱 계층의 스트라이드는 2입니다.합성곱 계층의 출력은 사인파 위치 인코딩과 결합된 다음 인코더 변환기 블록에서 처리됩니다.BART 기반 아키텍처에 따라 인코더와 디코더는 모두 폭이 768이고 변환기 블록이 6개입니다.디코더 감독 메트릭 모델 B1↑ B2↑ B3↑ B4↑ M↑ RL↑ BERT-S↑ 어휘↑ 다양성 메트릭 Novely Novel ↑ 길이 평균 토큰 기준선 감독 모델 28.13.7.59 4.79 20.62 19.87.0.69.46.7±16.Zeroshot 캡션 태그 Concat [2, 13] 4.0.Template [14] 7.K2C-Aug [22] 7.LP-MusicCaps(저희) 19.0.26 0.00 3.10 2.1.58 0.46 0.00 5.2.10 0.49 0.10 7.6.70 2.17 0.79.46.100.23.8±12.6.81.45.100.25.8±12.11.82.81.100.19.9±7.12.13.84.47.100.45.3±28.전송 학습 태그 연결 [2, 13] 28.템플릿 [14] 28.14.68 8.14.49 8.59 5.5.21.21.87.3.96.41.8±14.21.21.87.3.96.41.1±13.K2C-8월 [22] 29.50 14.99 8.70 5.73 21.20.87.1.84.44.1±15.LP-음악캡스 (저희) 29.09 14.87 8.93 6.05 22.39 21.87.1.96.42.5±14.표 4. MusicCaps eval-set에서 음악 자막 결과. Avg.Token은 자막당 토큰의 평균 수를 나타냅니다. 인과 관계를 위해 미래 토큰을 숨기는 마스크를 포함하는 멀티헤드 어텐션 모듈이 있는 트랜스포머 블록을 사용하여 토큰화된 텍스트 자막을 처리합니다. 음악 및 캡션 표현은 교차 모달 어텐션 계층에 입력되고 디코더의 언어 모델 헤드는 교차 엔트로피 손실을 사용하여 다음 토큰을 자기 회귀적으로 예측합니다.이 손실은 다음과 같이 공식화됩니다.L – Σ±1 log po(yt | Y1:t−1,x) 여기서 x는 페어링된 오디오 클립이고 yt는 길이 T인 캡션의 시간 t에서의 기준 진실 토큰입니다.= 5.2 실험 설정 t= 제안된 데이터 세트가 음악 캡션 작업에 미치는 영향을 평가하기 위해 MusicCaps [12] 훈련 분할로 훈련된 지도 학습 모델과 LP-MusicCaps-MSD 데이터 세트로 훈련된 사전 학습된 모델을 비교합니다. 사전 학습된 모델의 경우 MusicCaps [12] 데이터 세트를 사용하지 않는 제로 샷 캡션 작업과 MusicCaps [12] 훈련 분할을 사용하여 모델을 업데이트하는 미세 조정 작업을 모두 수행합니다. 다른 가상 캡션 생성 방법과의 비교를 위해 동일한 아키텍처와 오디오 양으로 학습되었지만 가상 캡션이 다른 기준 모델에 대한 결과를 보고합니다.섹션 3.1에서 사용한 모든 메트릭 외에도 학습 세트에 없는 생성된 캡션의 비율인 Novel을 계산합니다[37].캡션 모델이 단순히 학습 데이터를 복사하는지 여부를 측정합니다.모든 실험에서 인코더의 입력은 16kHz 샘플링 속도의 10초 오디오 신호입니다.128멜빈, 한 윈도우가 있는 1024포인트 FFT, 10ms의 홉 크기를 갖는 로그 스케일 멜 스펙트로그램으로 변환됩니다.모든 모델은 1e-4의 학습 속도를 갖는 AdamW를 사용하여 최적화됩니다.처음 1000개 업데이트에 대한 워밍업 후 0으로 감소하는 코사인 학습 속도를 사용합니다.사전 학습 데이터 세트의 경우 256배치 크기를 사용하고 모델은 32,768개 업데이트에 대해 학습됩니다. 우리는 앵커 태그를 먼저 균일하게 샘플링한 다음 주석이 달린 항목을 선택하는 균형 샘플링[38]을 채택합니다.지도 학습 및 전이 학습의 경우 64배치 크기, 100에포크를 사용합니다.모든 모델의 추론을 위해 5개 빔을 사용하는 빔 검색을 사용합니다.5.3 결과 제로 샷 캡션 모델 내에서 비교할 때, 제안된 LP-MusicCaps 데이터 세트로 학습된 모델은 일반적으로 강력한 성능을 보여줍니다.태그 연결을 사용하는 모델은 음악 문장을 생성하지 못해 가장 낮은 성능을 보입니다.프롬프트 템플릿을 사용하는 모델의 경우 BERTScore가 약간 더 높지만 어휘가 제한되어 n-gram 메트릭 측면에서 여전히 성능이 좋지 않습니다.K2C 증강을 사용하는 모델은 다른 두 가지 방법보다 성능이 뛰어나지만 음악적 맥락이 부족하여 여전히 부족합니다.일반적으로 제로 샷 모델은 몇 가지 예외를 제외하고 대부분의 메트릭에서 지도 기준선만큼 성능이 좋지 않습니다. 전이 캡션 모델 중에서 LP-MusicCaps 사전 학습을 거친 모델은 BERT-Score와 대부분의 n-gram 메트릭에서 승리하여 전반적으로 강력한 성능을 달성합니다. 제안한 모델이 지도 학습 모델에 비해 BERT-Score에서 의미 있는 증가를 보인다는 점이 주목할 만합니다. 이러한 개선은 단어 대 단어 매칭보다는 의미적 이해에 성공한 결과일 가능성이 큽니다. 게다가 Novelc의 개선을 통해 LP-MusicCaps 모델은 학습 데이터 세트에서 문구를 반복하는 대신 새로운 캡션을 생성할 수 있음을 보여줍니다. 이러한 이점은 전이 학습 모델의 제로샷 및 지도 학습 작업에서 모두 관찰됩니다. 6.
--- CONCLUSION ---
우리는 자동 음악 캡션의 데이터 부족 문제를 해결하기 위해 대규모 언어 모델을 사용한 태그-투-가상 캡션 생성 방식을 제안했습니다. 우리는 LLM 기반 증강에 대한 체계적 평가를 수행하여 대규모 가상 음악 캡션 데이터 세트인 LP-MusicCaps 데이터 세트를 생성했습니다. 또한 LP-MusicCaps를 사용하여 음악 캡션 모델을 학습하고 개선된 일반화를 보였습니다. 우리가 제안한 방식은 음악-언어 데이터 세트 수집에 필요한 비용과 시간을 크게 줄이고 표현 학습, 캡션, 생성을 포함하여 음악과 언어를 연결하는 분야에서 추가 연구를 용이하게 할 수 있는 잠재력이 있습니다. 그러나 생성된 캡션의 품질과 정확성을 높이기 위해서는 커뮤니티와의 추가 협업과 인간 평가가 필수적입니다. 또한 음악 정보 검색 및 음악 추천에서 다른 주제에 대한 LLM 사용을 탐색하면 새롭고 흥미로운 응용 프로그램이 생길 수 있다고 믿습니다. 7. 참고문헌 [1] I. Manco, E. Benetos, E. Quinton 및 G. Fazekas, &quot;Muscaps: 음악 오디오에 대한 캡션 생성&quot;, 국제 신경망 공동 컨퍼런스(IJCNN). IEEE, 2021. [2] T. Cai, MI Mandel, and D. He, &quot;Music autotagging as captioning,&quot; in Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA), 2020. [3] K. Choi, G. Fazekas, B. McFee, K. Cho, and M. Sandler, &quot;Towards music captioning: Generating music playlist descriptions,&quot; in International Society for Music Information Retrieval Conference (ISMIR), LateBreaking/Demo, 2016. [4] S. Doh, J. Lee, and J. Nam, &quot;Music playlist title generation: A machine-translation approach,&quot; in Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MuSA), 2021. [5] G. Gabbolini, R. Hennequin, and E. Epure, &quot;Dataefficient playlist 음악 및 언어 지식을 활용한 자막 제작&quot;, 2022 자연어 처리 경험적 방법 컨퍼런스(EMNLP) 회의록, 2022. [6] H. Kim, S. Doh, J. Lee, J. Nam, &quot;아티스트 정보를 활용한 음악 플레이리스트 제목 생성&quot;, AAAI-23 모달리티 간 창의적 AI 워크숍 회의록, 2023. [7] D. Bahdanau, K. Cho, Y. Bengio, &quot;공동 학습을 통한 정렬 및 번역을 통한 신경망 기계 번역&quot;, 국제 학습 표현 컨퍼런스(ICLR) 회의록, 2014. [8] M. Won, S. Chun, O. Nieto, X. Serrc, &quot;오디오 표현 학습을 위한 데이터 기반 고조파 필터&quot;, ICASSP 2020-2020 IEEE 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2020. [9] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., &quot;Language models are few-shot learners,&quot; Proceedings of the Advances in neural information processing systems (NeurIPS), 2020. [10] Q. Huang, A. Jansen, J. Lee, R. Ganti, JY Li, DP Ellis, &quot;MuLan: 음악 오디오와 자연어의 공동 임베딩,&quot; International Conference on Music Information Retrieval (ISMIR), 2022. [11] I. Manco, B. Weck, P. Tovstogan, M. Won, D. Bogdanov, &quot;Song describer: a platform for collected textual descriptions of music recordings,&quot; International Conference on Music Information Retrieval (ISMIR), Late-Breaking/Demo 세션, 2022. [12] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., “MusicLM: 텍스트에서 음악 생성,&quot; arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [13] S. Doh, M. Won, K. Choi, J. Nam, “보편적인 텍스트-음악 검색을 향하여,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2023. [14] T. Chen, Y. Xie, S. Zhang, S. Huang, H. Zhou, J. Li, “텍스트 감독에서 음악 시퀀스 표현 학습,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2022. [15] K. Choi, G. Fazekas, K. Cho, and M. Sandler, &quot;음악 태그를 위한 딥 합성곱 신경망에 대한 노이즈 레이블의 효과&quot;, IEEE Transactions on Emerging Topics in Computational Intelligence, 2018. [16] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and PJ Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구&quot;, J. Mach. Learn. Res., vol. 21, no. 1, 2020년 1월. [17] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, C. Leahy, &quot;파일: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트,&quot; 2020. [18] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, MD Plumbley, Y. Zou, W. Wang, &quot;WavCaps: 오디오-언어 멀티모달 연구를 위한 ChatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트,&quot; arXiv 사전 인쇄본 arXiv:2303.17395, 2023. [19] Q. Huang, DS Park, T. Wang, TI Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank 외, &quot;Noise2music: 확산 모델을 사용한 텍스트 조건 음악 생성,&quot; arXiv 사전 인쇄본 arXiv:2302.03917, 2023. [20] F. Gilardi, M. Alizadeh, M. Kubli, &quot;ChatGPT는 텍스트 주석 작업에서 군중 작업자보다 성능이 우수합니다,&quot; arXiv 사전 인쇄본 arXiv:2303.15056, 2023. [21] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du 외, &quot;Lamda: 대화 응용 프로그램을 위한 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2201.08239, 2022. [22] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, S. Dubnov, &quot;기능 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어 오디오 사전 학습&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2023. [23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., &quot;인간 피드백을 통해 지시를 따르도록 언어 모델 학습&quot;, 신경 정보 처리 시스템 진보(NeurIPS) 회의록, 2022. [24] PF Christiano, J. Leike, T. Brown, M. Martic, S. Legg, D. Amodei, &quot;인간 선호도에서 심층 강화 학습&quot;, 신경 정보 처리 시스템 진보(NeurIPS) 회의록, 2017. [25] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, YJ Bang, A. Madotto 및 P. Fung, &quot;자연어 생성에서 환각 조사&quot;, ACM Computing Surveys, 2023. [26] K. Papineni, S. Roukos, T. Ward 및 W.-J. Zhu, “Bleu: 기계 번역의 자동 평가를 위한 방법,&quot; 2002년 제40회 ACL(Association for Computational Linguistics) 연례 회의록. [27] S. Banerjee 및 A. Lavie, “Meteor: 인간 판단과의 상관 관계가 향상된 mt 평가를 위한 자동 메트릭,&quot; 기계 번역 및/또는 요약을 위한 내재적 및 외재적 평가 측정에 관한 ACL 워크숍 회의록, 2005. [28] C.-Y. Lin, “Rouge: 요약의 자동 평가를 위한 패키지,&quot; Text summarization branches out, 2004. [29] T. Zhang, V. Kishore, F. Wu, KQ Weinberger 및 Y. Artzi, &quot;Bertscore: bert를 사용한 텍스트 생성 평가,&quot; 국제 학습 표현 컨퍼런스(ICLR), 2020. [30] CD Kim, B. Kim, H. Lee 및 G. Kim, &quot;AudioCaps: Generating captions for audios in the wild&quot;, 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 2019. [31] E. Law, K. West, MI Mandel, M. Bay 및 JS Downie, &quot;게임을 사용한 알고리즘 평가: 음악 태그의 경우.&quot; in International Conference on Music Information Retrieval (ISMIR), 2009. [32] T. Bertin-Mahieux, DP Ellis, B. Whitman, and P. Lamere, &quot;The million song dataset,&quot; in International Conference on Music Information Retrieval (ISMIR), 2011. [33] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, &quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,&quot; in the 58th Annual Meeting of the Association for Computational Linguistics (ACL)의 회의록, 2020. [34] Y. Zhang, J. Jiang, G. Xia, and S. Dixon, &quot;Interpreting song lyrics with an audio-informed 영어: 사전 훈련된 언어 모델,&quot; 국제 음악 정보 검색 컨퍼런스(ISMIR), 2022. [35] A. Radford, JW Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, &quot;대규모 약한 감독을 통한 견고한 음성 인식,&quot; arXiv 사전 인쇄본 arXiv:2212.04356, 2022. [36] D. Hendrycks and K. Gimpel, &quot;가우스 오차 선형 단위(GELU)&quot;, arXiv 사전 인쇄본 arXiv:1606.08415, 2016. [37] M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli, G. Fiameni, and R. Cucchiara, &quot;From show to tell: a survey on deep learning-based image captioning,&quot; IEEE transaction on pattern analysis and machine intelligence, 2022. [38] M. Won, S. Oramas, O. Nieto, F. Gouyon, and X. Serra, &quot;태그 기반 음악 검색을 위한 멀티모달 메트릭 학습&quot;, ICASSP 2021-2021 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2021.
