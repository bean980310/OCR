--- ABSTRACT ---
Damien Vincent Eugene Kharitonov¹ Neil Zeghidour¹ Marco Tagliasacchi¹ 효율적이고 비자기 회귀적 오디오 생성을 위한 모델인 SoundStorm을 제시합니다. SoundStorm은 AudioLM의 의미 토큰을 입력으로 받고 양방향 어텐션과 신뢰 기반 병렬 디코딩에 의존하여 신경 오디오 코덱의 토큰을 생성합니다. AudioLM의 자기 회귀적 생성 방식과 비교할 때, 저희 모델은 동일한 품질의 오디오를 생성하고 음성 및 음향 조건에서 더 높은 일관성을 유지하면서도 두 자릿수 더 빠릅니다. SoundStorm은 TPU-v4에서 0.5초 만에 30초 분량의 오디오를 생성합니다. 저희는 화자 차례가 주석으로 달린 대본과 화자의 음성이 있는 짧은 프롬프트가 주어졌을 때, 고품질의 자연스러운 대화 세그먼트를 합성하여 오디오 생성을 더 긴 시퀀스로 확장할 수 있는 저희 모델의 능력을 보여줍니다. 오디오 샘플은 https://google-research.github.io/seanet/soundstorm/examples/에서 제공됩니다. 1.
--- INTRODUCTION ---
신경 코덱에서 생성된 오디오의 이산적 표현을 모델링하면(Zeghidour et al., 2022; Défossez et al., 2022) 오디오 생성 작업이 강력한 Transformer 기반 시퀀스-투-시퀀스 모델링 접근 방식(Vaswani et al., 2017)에 적합해집니다. 무조건 및 조건 오디오 생성을 시퀀스-투-시퀀스 모델링으로 캐스팅하면 음성 연속(Borsos et al., 2022), 텍스트-음성(Wang et al., 2023; Kharitonov et al., 2023), 일반 오디오 및 음악 생성(Kreuk et al., 2022; Agostinelli et al., 2023)에서 빠른 진전이 이루어졌습니다. 신경 코덱의 토큰을 모델링하여 고품질 오디오를 생성하려면 이산 표현의 비율을 높여야 하며, 그 결과 코드북 크기나 긴 토큰 시퀀스가 기하급수적으로 증가합니다. 메모리 제한으로 인해 코드북의 기하급수적 증가가 금지되는 반면, 긴 토큰 시퀀스는 자기 회귀 모델에 대한 계산적 과제를 제시합니다. Google Research. 특히 이 작업의 주요 초점인 주의 기반 모델은 자기 주의를 계산하기 위한 시퀀스 길이와 관련하여 2차 런타임 복잡도를 초래합니다. 따라서 지각적 품질과 런타임 간의 균형을 해결하는 것은 오디오 생성의 핵심 과제 중 하나입니다. 긴 오디오 토큰 시퀀스를 생성하는 문제는 적어도 세 가지 직교적 접근 방식 또는 그 조합을 통해 해결할 수 있습니다. i) 효율적인 어텐션 메커니즘(Kitaev 등, 2020; Choromanski 등, 2021; Xiong 등, 2021; Hawthorne 등, 2022), ii) 비자기회귀 병렬 디코딩 방식(Gu 등, 2017; Ghazvininejad 등, 2019; Chang 등, 2022), iii) 신경 오디오 코덱에서 생성된 토큰의 특수 구조에 맞게 조정된 사용자 지정 아키텍처(Kreuk 등, 2022; Wang 등, 2023; Lee 등, 2022). 그러나 신경 오디오 코덱의 토큰 시퀀스를 무조건적으로 모델링하거나 텍스트와 같은 약한 컨디셔닝을 기반으로 모델링하는 맥락에서 길고 고품질의 오디오 세그먼트를 효율적으로 생성하는 것은 여전히 미해결 문제로 남아 있습니다. 우리는 장시퀀스 오디오 모델링의 미래 발전에 가장 큰 희망을 안겨주는 것은 오디오 토큰 시퀀스의 특수 구조라고 믿습니다. 구체적으로, SoundStream(Zeghidour et al., 2022)과 EnCodec(Défossez et al., 2022)은 모두 잔여 벡터 양자화(RVQ)에 의존하는데, 여기서 각 압축 오디오 프레임은 일련의 양자화기에 의해 양자화되고, 각 양자화기는 이전 양자화기의 잔여에 대해 작동하며, 양자화기의 수는 전체 비트 전송률을 제어합니다. 이는 계층적 토큰 구조를 유도하는데, 여기서 더 미세한 RVQ 레벨의 토큰은 지각적 품질에 덜 기여하여 효율적인 인수분해와 토큰 시퀀스의 공동 분포 근사를 허용합니다. 따라서 모델과 디코딩 체계는 효율적인 학습과 추론을 위해 입력의 이러한 특수 구조를 고려해야 합니다. 이 작업에서 우리는 효율적이고 고품질의 오디오 생성을 위한 방법인 SoundStorm을 제시합니다. SoundStorm은 다음을 사용하여 긴 오디오 토큰 시퀀스를 생성하는 문제를 해결합니다.i) 오디오 토큰의 계층 구조에 맞게 조정된 아키텍처, ii) MaskGIT(Chang et al., 2022)에서 영감을 받은 잔여 벡터 양자화 토큰 시퀀스를 위한 병렬, 비자기 회귀, 신뢰 기반 디코딩 방식.SoundStorm: 효율적인 병렬 오디오 생성 SoundStorm은 AudioLM(Borsos et al., 2022)의 의미 토큰과 같은 컨디셔닝 신호가 주어졌을 때 SoundStream에서 생성된 마스크 오디오 토큰을 예측하도록 훈련된 양방향 어텐션 기반 Conformer(Gulati et al., 2020)에 의존합니다.입력 측에서 동일한 SoundStream 프레임에 해당하는 토큰의 임베딩을 합산하여 셀프 어텐션의 내부 시퀀스 길이가 SoundStream 프레임 수와 동일하고 RVQ의 양자화기 수와 무관합니다. 출력 임베딩은 RVQ 레벨당 별도의 헤드에서 처리되어 마스크된 대상 토큰을 예측합니다. 추론 시간에 컨디셔닝 신호가 주어지면 SoundStorm은 모든 오디오 토큰이 마스크된 상태에서 시작하여 여러 반복에 걸쳐 레벨별로 마스크된 토큰 RVQ를 채우고 레벨 내에서 단일 반복 중에 여러 토큰을 병렬로 예측합니다. 이 추론 방식을 지원하기 위해 추론 절차를 모방하는 학습을 위한 마스킹 방식을 제안합니다. SoundStorm이 AudioLM의 음향 생성기 역할을 하여 AudioLM의 2단계(거친 음향 모델)와 3단계(미세 음향 모델)를 모두 대체할 수 있음을 보여줍니다. SoundStorm은 AudioLM의 계층적 자기 회귀 음향 생성기보다 두 자릿수 더 빠르게 오디오를 생성하며, 스피커 정체성과 음향 조건 측면에서 일치하는 품질과 향상된 일관성을 제공합니다. 또한, 우리는 SoundStorm이 SPEAR-TTS(Kharitonov et al., 2023)의 텍스트-의미 모델링 단계와 결합되어 고품질의 자연스러운 대화를 합성할 수 있음을 보여주며, 이를 통해 말하는 내용(대본을 통해), 화자 음성(짧은 음성 프롬프트를 통해) 및 화자 차례(대본 주석을 통해)를 제어할 수 있습니다. 30초의 대화를 합성할 때, 우리는 단일 TPU-v4에서 초의 런타임을 측정합니다(Jouppi et al., 2023). 2.
--- RELATED WORK ---
신경 오디오 코덱의 토큰 모델링. 비지도 음성 임베딩(Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021)은 기본 신호의 낮은 프레임 속도 표현을 제공했으며, 이는 언어 모델이 특정 화자의 이해 가능한 음성을 토큰 시퀀스로 생성할 수 있을 만큼 이산화 후에도 충분히 풍부합니다(Lakhotia et al., 2021). 신경 오디오 코덱(Zeghidour 등, 2022; Défossez 등, 2022)은 매우 낮은 비트레이트에서 고품질 오디오를 재구성하는 기능을 갖추고 있어, 이후 다중 화자 음성 및 피아노(Borsos 등, 2022; Kharitonov 등, 2023), 음악(Agostinelli 등, 2023) 또는 음향 효과(Kreuk 등, 2022)와 같이 다양한 오디오 신호로 이산 모델링을 확장할 수 있었습니다. 특히 AudioLM(Borsos 등, 2022)은 고수준 의미 토큰을 중간 표현으로 생성한 다음, 이를 SoundStream(Zeghidour 등, 2022) 코덱의 토큰을 예측하기 위한 컨디셔닝 신호로 사용하는 계층적 시퀀스-투-시퀀스 방식을 도입했습니다. 이러한 계층적 접근 방식은 음성(Kharitonov 등, 2023) 및 음악 모델링(Agostinelli 등, 2023; Donahue 등, 2023)에 대해 놀라운 결과를 보여주었지만, 셀프 어텐션을 사용하여 평탄화된 SoundStream 토큰을 모델링하는 데 드는 계산 비용은 시퀀스 길이와 신경 코덱의 비트 전송률에 따라 2차적으로 증가하여 이러한 모델이 고품질의 장문 오디오를 생성할 수 없게 합니다. SoundStorm은 신경 코덱의 다중 레벨 토큰을 병렬로 모델링하여 이 문제를 해결하고, 자기 회귀 모델링보다 두 자릿수 속도 향상을 유도하며, 품질과 시퀀스 길이 모두에서 오디오 생성 능력을 확장할 수 있는 기능을 제공합니다. RVQ 인식 아키텍처. RVQ 토큰 시퀀스를 모델링하기 위한 일반적인 설계 선택은 시퀀스 길이를 줄이기 위해 동일한 RVQ 입력 임베딩(프레임)에 해당하는 임베딩을 합산하는 것입니다. 이러한 시퀀스에서 작동하는 AudioGen(Kreuk et al., 2022)은 서로 다른 RVQ 레벨에 대해 Q개의 별도 헤드가 있는 Transformer를 제안하여 RVQ 프레임의 토큰을 병렬로 예측합니다. 추론을 위한 상당한 속도 향상을 제공하는 반면, 저자는 텍스트-오디오 생성의 경우 이 접근 방식이 비트 전송률과 재구성 품질이 비슷하지만 양자화 수준이 단일인 신경 오디오 코덱의 토큰 시퀀스를 모델링하는 것보다 성능이 떨어진다는 것을 발견했습니다. 대신 VALL-E(Wang et al., 2023)는 첫 번째 RVQ 레벨에 해당하는 토큰이 자기 회귀적으로 예측되고 후속 레벨이 비자기 회귀적으로 생성되는 하이브리드 접근 방식에 의존합니다. 후자는 동일한 RVQ 입력 프레임의 임베딩을 합산하고 양방향 자기 주의를 적용하여 레벨 1의 모든 토큰이 주어진 경우 RVQ 레벨 9+의 모든 토큰을 예측하는 모델에 의해 달성됩니다. q, 음향 프롬프트 및 음소 시퀀스. 추론하는 동안 RVQ의 두 번째 레벨에서 시작하는 토큰은 반복적으로 생성되어 탐욕적 디코딩(가장 가능성 있는 토큰 선택)을 레벨별로 수행합니다. 레벨별 탐욕적 디코딩은 우리의 기준선을 나타냅니다.
--- METHOD ---
효율적이고 고품질의 오디오 생성을 위해 SoundStorm은 다음을 사용하여 긴 오디오 토큰 시퀀스를 생성하는 문제를 해결합니다.i) 오디오 토큰의 계층 구조에 맞게 조정된 아키텍처, ii) MaskGIT(Chang et al., 2022)에서 영감을 받은 잔여 벡터 양자화 토큰 시퀀스를 위한 병렬, 비자기 회귀, 신뢰 기반 디코딩 방식.SoundStorm: 효율적인 병렬 오디오 생성 SoundStorm은 AudioLM의 의미 토큰(Borsos et al., 2022)과 같은 컨디셔닝 신호가 주어졌을 때 SoundStream에서 생성된 마스크 오디오 토큰을 예측하도록 훈련된 양방향 어텐션 기반 Conformer(Gulati et al., 2020)에 의존합니다.입력 측에서 동일한 SoundStream 프레임에 해당하는 토큰의 임베딩을 합산하여 셀프 어텐션의 내부 시퀀스 길이가 SoundStream 프레임 수와 동일하고 RVQ의 양자화기 수와 무관합니다. 출력 임베딩은 RVQ 레벨당 별도의 헤드에서 처리되어 마스크된 대상 토큰을 예측합니다. 추론 시간에 컨디셔닝 신호가 주어지면 SoundStorm은 모든 오디오 토큰이 마스크된 상태에서 시작하여 여러 반복에 걸쳐 레벨별로 마스크된 토큰 RVQ를 채우고 레벨 내에서 단일 반복 중에 여러 토큰을 병렬로 예측합니다. 이 추론 방식을 지원하기 위해 추론 절차를 모방하는 학습을 위한 마스킹 방식을 제안합니다. SoundStorm이 AudioLM의 음향 생성기 역할을 하여 AudioLM의 2단계(거친 음향 모델)와 3단계(미세 음향 모델)를 모두 대체할 수 있음을 보여줍니다. SoundStorm은 AudioLM의 계층적 자기 회귀 음향 생성기보다 두 자릿수 더 빠르게 오디오를 생성하며, 스피커 정체성과 음향 조건 측면에서 일치하는 품질과 향상된 일관성을 제공합니다. 또한, SoundStorm을 SPEAR-TTS(Kharitonov et al., 2023)의 텍스트-의미 모델링 단계와 결합하면 고품질의 자연스러운 대화를 합성하여 말한 내용(대본을 통해), 화자 음성(짧은 음성 프롬프트를 통해) 및 화자 차례(대본 주석을 통해)를 제어할 수 있음을 보여줍니다. 30초 대화를 합성할 때 단일 TPU-v4에서 초 단위의 런타임을 측정합니다(Jouppi et al., 2023). 2. 관련 연구 신경 오디오 코덱의 토큰 모델링. 비지도 음성 임베딩(Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021)은 기본 신호의 낮은 프레임 속도 표현을 제공하여 언어 모델이 특정 화자의 이해 가능한 음성을 토큰 시퀀스로 생성할 수 있을 만큼 이산화 후에도 충분히 풍부합니다(Lakhotia et al., 2021). 신경 오디오 코덱(Zeghidour 등, 2022; Défossez 등, 2022)은 매우 낮은 비트레이트에서 고품질 오디오를 재구성하는 기능을 갖추고 있어, 이후 다중 화자 음성 및 피아노(Borsos 등, 2022; Kharitonov 등, 2023), 음악(Agostinelli 등, 2023) 또는 음향 효과(Kreuk 등, 2022)와 같이 다양한 오디오 신호로 이산 모델링을 확장할 수 있었습니다. 특히 AudioLM(Borsos 등, 2022)은 고수준 의미 토큰을 중간 표현으로 생성한 다음, 이를 SoundStream(Zeghidour 등, 2022) 코덱의 토큰을 예측하기 위한 컨디셔닝 신호로 사용하는 계층적 시퀀스-투-시퀀스 방식을 도입했습니다. 이러한 계층적 접근 방식은 음성(Kharitonov 등, 2023) 및 음악 모델링(Agostinelli 등, 2023; Donahue 등, 2023)에 대해 놀라운 결과를 보여주었지만, 셀프 어텐션을 사용하여 평탄화된 SoundStream 토큰을 모델링하는 데 드는 계산 비용은 시퀀스 길이와 신경 코덱의 비트 전송률에 따라 2차적으로 증가하여 이러한 모델이 고품질의 장문 오디오를 생성할 수 없게 합니다. SoundStorm은 신경 코덱의 다중 레벨 토큰을 병렬로 모델링하여 이 문제를 해결하고, 자기 회귀 모델링보다 두 자릿수 속도 향상을 유도하며, 품질과 시퀀스 길이 모두에서 오디오 생성 능력을 확장할 수 있는 기능을 제공합니다. RVQ 인식 아키텍처. RVQ 토큰 시퀀스를 모델링하기 위한 일반적인 설계 선택은 시퀀스 길이를 줄이기 위해 동일한 RVQ 입력 임베딩(프레임)에 해당하는 임베딩을 합산하는 것입니다. 이러한 시퀀스에서 작동하는 AudioGen(Kreuk et al., 2022)은 서로 다른 RVQ 레벨에 대해 Q개의 별도 헤드가 있는 Transformer를 제안하여 RVQ 프레임의 토큰을 병렬로 예측합니다. 추론을 위한 상당한 속도 향상을 제공하는 반면, 저자는 텍스트-오디오 생성의 경우 이 접근 방식이 비트 전송률과 재구성 품질이 비슷하지만 양자화 수준이 단일인 신경 오디오 코덱의 토큰 시퀀스를 모델링하는 것보다 성능이 떨어진다는 것을 발견했습니다. 대신 VALL-E(Wang et al., 2023)는 첫 번째 RVQ 레벨에 해당하는 토큰이 자기 회귀적으로 예측되고 후속 레벨이 비자기 회귀적으로 생성되는 하이브리드 접근 방식에 의존합니다. 후자는 동일한 RVQ 입력 프레임의 임베딩을 합산하고 양방향 자기 주의를 적용하여 레벨 1의 모든 토큰이 주어진 경우 RVQ 레벨 9+의 모든 토큰을 예측하는 모델에 의해 달성됩니다. q, 음향 프롬프트 및 음소 시퀀스. 추론하는 동안 RVQ의 두 번째 레벨에서 시작하는 토큰이 반복적으로 생성되어 탐욕적 디코딩(가장 가능성 있는 토큰 선택)을 레벨별로 수행합니다. 레벨별 탐욕적 디코딩은 우리 방법의 기준선을 나타냅니다. RVQ에서 생성된 모델링 시퀀스는 오디오 이외의 도메인에서도 조사되었습니다. 예를 들어, RQTransformer(Lee et al., 2022)는 동일한 RVQ 입력 프레임에 해당하는 임베딩을 합산하지만 공간 및 깊이 Transformer를 사용하여 전체 조인트 분포를 효율적으로 인수분해하여 각각 RVQ 프레임과 프레임 내의 토큰을 자기 회귀적으로 모델링합니다. 아직 시연되지는 않았지만 이 접근 방식은 잠재적으로 병렬 디코딩 방식과 결합되어 오디오 생성을 위한 유망한 미래 경로입니다. 병렬 디코딩. 추론 시간을 개선하고 입력 시퀀스에 대한 양방향 비인과적 주의를 허용하기 위해 텍스트에 대한 병렬 디코딩 방식이 제안되었습니다(Gu et al., 2017; Ghazvininejad et al., 2019). imSoundStorm: 효율적인 병렬 오디오 생성 SoundStream 토큰 머리 머리 머리 머리|양방향 자기 주의를 갖춘 컨포머 A t₁ た *** 조건화 토큰 마스크된 토큰 손실에서 고려된 토큰 그림 1. 학습을 위한 SoundStorm 아키텍처 및 마스킹 방식(프롬프트 없음). 이 모델은 동일한 SoundStream 프레임에 해당하는 토큰의 임베딩을 합산하여 입력 시퀀스 길이를 줄입니다. 훈련 중에 RVQ 수준 q가 샘플링되고(그림에서 Q = 3 수준 중 q = 2) 수준 q에서 무작위로 샘플링된 토큰의 하위 집합이 RVQ 수준 g+1,..., Q의 모든 토큰과 함께 마스크됩니다. 손실은 수준 q에서 마스크된 토큰에서만 계산됩니다. 연령(Chang et al., 2022) 및 비디오 생성(Villegas et al., 2022). 우리 작업과 특히 관련이 있는 것은 MaskGIT의 병렬 반복 샘플링 체계입니다(Chang et al., 2022). 추론 시간 동안 MaskGIT는 마스크된 토큰에서 시작하여 각 라운드에서 신뢰도 점수에 따라 토큰의 일부를 예측합니다. 각 라운드에서 예측된 토큰의 일부는 일정에 의해 제어되며 일반적으로 반복을 통해 점진적으로 증가합니다. 예측되면 토큰은 고정된 것으로 처리됩니다. 제안된 디코딩 체계는 잔여 양자화에 의해 생성된 토큰 시퀀스에 대한 MaskGIT 디코딩의 확장으로 볼 수 있습니다. 3. 방법 SoundStorm은 컨디셔닝 신호를 나타내는 이산 토큰 시퀀스를 입력으로 받고 SoundStream 토큰 시퀀스를 출력으로 생성하며, 이는 오디오 파형으로 다시 디코딩될 수 있습니다. 컨디셔닝 신호는 SoundStream 프레임과 시간적으로 정렬되거나 동일한 속도로 업샘플링될 수 있다고 가정합니다. 이러한 컨디셔닝 신호는 예를 들어 AudioLM, SPEAR-TTS 또는 MusicLM에서 사용되는 의미 토큰 시퀀스이며, 이는 이러한 모델의 음향 생성기를 대체하는 드롭인 방법을 만듭니다. 우리는 교차 어텐션을 통한 다른 유형의 컨디셔닝 신호 또는 무조건 샘플링에 대한 확장을 향후 작업으로 남겨두고, AudioLM 내의 음향 생성기로서 SoundStorm에 대한 프레젠테이션에 집중하여 AudioLM의 거친 음향 모델링 단계와 미세 음향 모델링 단계를 모두 대체합니다. 3.1. 아키텍처 모델의 아키텍처는 그림 1에 설명되어 있습니다. 입력 측에서 시간 정렬된 컨디셔닝 토큰을 프레임 수준에서 SoundStream 토큰과 인터리빙하고, 결과 시퀀스를 임베드하고, 컨디셔닝 토큰의 임베딩을 포함하여 동일한 프레임에 해당하는 임베딩을 합산하고, 결과 연속 임베딩을 Conformer에 전달합니다. 결과적으로 Conformer에서 양방향 셀프 어텐션의 시퀀스 길이는 SoundStream 프레임 수(일반적으로 초당 50개)에 의해 결정되므로 RVQ 레벨 Q의 수와 무관하여 분 단위 길이의 오디오를 처리할 수 있습니다. 출력 측에서 Q개의 밀집 레이어를 헤드로 사용하여 대상 SoundStream 토큰을 생성합니다. 3.2. 마스킹 마스킹 및 디코딩을 설계하기 위해 MaskGIT(Chang et al., 2022)의 마스킹 및 신뢰 기반 병렬 디코딩 방식을 RVQ에서 생성된 토큰 시퀀스로 확장합니다. 높은 수준에서, 우리의 접근 방식은 RVQ 레벨당 Chang et al. (2022)의 전략을 따르는 것으로 볼 수 있습니다. 거친 것에서 미세한 것까지의 순서입니다. 거친 것에서 미세한 것까지의 순서는 특히 중요합니다. 왜냐하면 RVQ 계층의 레벨 간의 조건부 종속성을 존중할 뿐만 아니라 거친 레벨의 모든 토큰이 주어졌을 때 미세한 레벨의 토큰의 조건부 독립성을 활용하기 때문입니다. 미세한 레벨의 토큰은 지역적이고 미세한 음향 세부 사항을 담당하므로 오디오 품질 손실 없이 병렬로 샘플링할 수 있습니다. 우리는 이에 따라 훈련을 위한 마스킹 체계를 설계합니다. 음성 프롬프트를 활성화하기 위해 무작위로 타임스텝 t = {1,………‚T}를 샘플링합니다. 여기서 T는 최대 시퀀스 길이를 나타내고 이 타임스텝 이전의 토큰은 마스크하지 않습니다. 조건 토큰은 절대 마스크되지 않습니다. YЄ {1,..., C}xQ가 SoundStream 토큰을 나타내도록 합니다. 여기서 C는 Q 레벨 중 각 RVQ 레벨에서 사용된 코드북 크기를 나타냅니다. 마스킹 방식은 다음과 같이 진행됩니다. • 프롬프트 구분 기호 타임스텝 t~ ~U{0,T−1}을 샘플링합니다. 현재 RVQ 레벨 q ~U{1,Q}를 샘플링합니다. • SoundStorm: 효율적인 병렬 오디오 생성 • 코사인 일정(Chang et al., 2022)에 따라 레벨 q에 대한 마스크 M = {0,1}ª을 샘플링합니다. 즉, 마스킹 비율 p = cos(u)를 샘플링합니다. 여기서 u ~ U[0, π/2]이고 iid M¿ ~ Bernoulli(p)를 샘플링합니다. = • 현재 RVQ 레벨 q에서 선택된 비프롬프트 토큰을 마스크합니다(Mt&#39; 1이고 t&#39; &gt; t이면 Yt&#39;¸g를 마스크합니다) 및 더 세부적인 RVQ 레벨(Y&gt;t,&gt;q)에서 모든 비프롬프트 토큰을 마스크합니다. , 마스크된 토큰 시퀀스가 주어지면, 타겟으로 지상 진실 토큰을 사용하여 교차 엔트로피 손실로 모델을 훈련합니다. 여기서 손실은 q번째 RVQ 레벨 내의 마스크된 토큰에서만 계산됩니다. 이 마스킹 방식의 예는 그림 1에 나와 있으며, T = 4, Q = 3, t = 및 q = 2입니다. 3.3. 반복적 병렬 디코딩 조건 신호가 주어지면, 디코딩 방식은 프롬프트(제공된 경우)의 토큰을 제외한 모든 SoundStream 토큰이 마스크된 상태에서 시작합니다. 그런 다음, 토큰 RVQ를 거친 것에서 미세한 것으로 레벨별로 샘플링하여 레벨 1, ..., q에 대한 모든 토큰이 샘플링된 경우에만 레벨 q + 1로 진행합니다. RVQ 레벨 내에서, Chang et al. (2022)의 신뢰 기반 샘플링 방식을 사용합니다. 즉, 여러 차례의 포워드 패스를 수행하고 각 반복 i에서 마스크된 위치의 후보를 샘플링하여 신뢰도 점수에 따라 후보의 på를 유지합니다. 여기서 Pi는 코사인 일정을 따릅니다. Chang et al. (2022)과 비교했을 때, 각 RVQ 레벨 내의 마지막 반복에 신뢰도 기반 샘플링 대신 탐욕적 디코딩을 사용하여 인지되는 오디오 품질을 개선하는 것으로 나타났습니다. 디코딩 RVQ를 레벨별로 수행하면 더 미세한 레벨에서 조건부 독립 가정을 활용할 수 있습니다. 즉, 여러 개의 더 미세한 토큰을 병렬로 샘플링할 수 있습니다. 이는 디코딩 중에 더 미세한 RVQ 레벨로 진행함에 따라 포워드 패스 수를 크게 줄일 수 있음을 의미합니다. 4.
--- EXPERIMENT ---
s 4.1. 모델 학습 및 추론 설정 = = 실험에서 우리는 초당 50프레임을 생성하는 SoundStream 코덱에 의존하고, 레벨당 1024 코드북 크기를 갖는 Q 12 레벨의 RVQ를 사용하여 50-12-log2 1024 6000bps의 비트 전송률을 생성합니다. 우리는 AudioLM의 의미 토큰을 컨디셔닝으로 사용하는데, 이는 1024개의 클러스터 센터가 있는 k-means로 양자화된 w2v-BERT(Chung et al., 2021) 임베딩에서 유래합니다. 이 토큰은 초당 25개의 토큰 속도를 가지므로 SoundStream의 프레임 속도와 일치하도록 복제합니다. 우리는 12개 레이어, 16개 어텐션 헤드, 1024의 임베딩 크기와 모델 차원, 4096의 피드포워드 차원, 5의 합성 커널 크기, 회전 위치 임베딩(Su et al., 2021)을 갖는 350M 매개변수를 갖는 Conformer를 사용합니다. 디코딩하는 동안 우리는 RVQ 레벨에 대해 (16, 1, 1, ..., 1) 반복을 사용합니다. 즉, 첫 번째 레벨에서 16번 반복하고 후속 레벨에서 가장 높은 확률을 갖는 토큰을 레벨별로 탐욕적으로 선택합니다. 이 전략은 모델에서 30초의 오디오를 예측하기 위해 27번의 포워드 패스를 수행하거나 30.50·12 = 18000 SoundStream 토큰과 동일합니다. 우리는 LibriLight(Kahn et al., 2020)(60k 시간)에서 모델을 훈련시키고, 데이터에 대해 10개의 에포크를 적용하고, 각 예제에서 0~30초 길이의 무작위 창을 샘플링합니다.4.2. 음성 명료도, 오디오 품질, 음성 보존 및 음향 일관성 일련의 주관적 평가 실험에서 Borsos et al.(2022)과 Kharitonov et al.(2023)은 AudioLM의 음향 생성 단계가 기준 샘플의 품질과 구별할 수 없는 품질의 오디오를 생성한다는 것을 보여주었습니다.따라서 우리는 AudioLM의 계층적 음향 생성 단계(거친 단계와 미세 단계)를 우리 실험의 기준으로 고려합니다.Borsos et al.(2022)의 AudioLM 음향 단계 실험과 비교했을 때 한 가지 중요한 차이점은 우리는 컨디셔닝 신호를 SoundStream 프레임과 시간적으로 정렬해야 하는 반면, Borsos et al.(2022)의 실험은 중복이 제거된 의미 토큰을 사용한다는 것입니다. 공정한 비교를 위해 두 방법 모두 동일한 컨디셔닝을 사용하도록 Borsos et al.(2022)의 음향 단계 실험을 중복된 의미 토큰을 제거하지 않고 반복하여 컨디셔닝을 강화합니다. 또한 두 방법 모두에 동일한 SoundStream 코덱을 사용합니다. 음성 이해도. ASR로 필사한 후 생성된 오디오의 단어 오류율(WER)과 문자 오류율(CER)을 측정하여 음성 이해도를 정량화합니다. 생성은 LibriSpeech 테스트 클린 분할(Panayotov et al., 2015)의 기준 진실 의미 토큰에 따라 조건화됩니다. 이 실험은 방법이 화자를 무작위로 샘플링할 수 있는 비프롬프트 설정과 방법이 처음 3초에 해당하는 기준 진실 SoundStream 토큰의 형태로 제공된 화자 ID를 존중해야 하는 프롬프트 설정에서 모두 수행합니다. 필사를 위해 Conformer Transducer-L(Gulati et al., 2020) ASR 모델을 사용합니다. 우리는 짧은(4-10초), 중간(10-20초) 및 긴(20-30초) 세그먼트에 대해 별도로 결과를 보고합니다. AudioLM의 음향 생성 단계는 첫 번째 RVQ 레벨의 평탄화된 토큰 시퀀스에서 자기 회귀적이므로 단일 패스에서 30초를 생성하는 것은 엄청나게 비쌉니다. 따라서 10초 청크를 생성하고 마지막 3초를 다음 청크의 프롬프트로 사용하는 슬라이드-앤-프롬프트 방식으로 10초보다 긴 세그먼트를 생성합니다. TableSoundStorm의 결과: 효율적인 병렬 오디오 생성 표 1. AudioLM의 음향 생성기와 SoundStorm의 이해도, 품질, 음성 보존 및 음향 일관성 비교. LibriSpeech test-clean의 &#39;짧은&#39;(4-10초), &#39;중간&#39;(10-20초) 및 &#39;긴&#39;(20-30초) 분할에 대한 메트릭 값을 별도로 보고합니다. SoundStorm은 오디오 품질 면에서 AudioLM의 음향 발생기와 맞먹으며, 음성 명료도와 음향 일관성 면에서는 더 뛰어납니다. 오디오 WER↓ CER↓ 품질 음성 보존 음향 일관성 짧음 중간 길음 짧음 중간 길음 짧음 중간 길음 짧음 중간 길음 짧음 중간 길음 원본 2.62 1.95 2.0.0.0.69 3.72 3.91 3.0.63 0.65 0.66 0.0.0.SoundStream rec. AudioLM SoundStorm 4.65 3.59 4.79 2.3.48 2.55 3.33 1.1.0.스피커 프롬프트 없음 2.30 3.93 4.04 4.1.29 4.01 4.16 4.스피커 프롬프트 있음 AudioLM SoundStorm 3.77 3.40 3.75 1.2.99 2.43 3.36 1.1.0.1.1.3.91 4.06 4.10 0.46 0.48 0.48 0.96 0.91 0.3.81 4.05 4.15 0.57 0.59 0.59 0.96 0.94 0.음향 일관성 1.0.0.Original · SoundStorm AudioLM 0.0.시간(초) 그림 2. LibriSpeech 테스트 클린의 &#39;&#39;긴&#39; 분할에서 샘플의 프롬프트와 생성된 오디오 간의 음향 일관성. 음영 처리된 영역은 사분위 범위를 나타냅니다. SoundStorm이 프롬프트된 시나리오와 프롬프트되지 않은 시나리오 모두에서 모든 분할에서 WER 및 CER 측면에서 AudioLM의 음향 생성보다 상당히 개선되었음을 보여줍니다. 음성 보존. 다음으로 SoundStorm이 프롬프트의 화자 정체성을 유지하는 기능을 측정합니다. 이를 위해 프롬프트와 생성된 오디오에서 화자 임베딩을 추출하고 코사인 유사도를 계산합니다. Wang et al.(2023) 및 Kharitonov et al.(2023)에서와 같이 WavLM(Chen et al., 2022)을 기반으로 하는 공개적으로 사용 가능한¹ 화자 검증 시스템으로 임베딩을 계산합니다. 표 1은 SoundStorm이 AudioLM 기준선보다 상당히 성능이 우수함을 보여줍니다. 음향 일관성 드리프트. 장기 생성의 경우, 프롬프트의 음향 특성(예: 화자 신원, 녹음 조건)이 생성된 오디오에서 시간이 지남에 따라 어느 정도 보존되는지 측정하는 것이 흥미롭습니다. 이를 위해 &#39;https://github.com/microsoft/UniSpeech/ tree/main/downstreams/speaker_verification # 사전 학습된 모델&#39; 두 개의 짧은 세그먼트가 동일한 녹음에서 나왔는지 여부를 평가할 수 있는 모델을 학습합니다. 구체적으로, 각 예에 단일 화자(예: LibriLight)의 녹음이 포함된 음성 코퍼스를 고려하고 2~5초 길이의 두 개의 무작위적이고 겹치지 않는 크롭을 추출합니다. 그런 다음 각 크롭에서 동일한 코퍼스에서 사전 학습된 BEST-RQ 모델(Chiu et al., 2022)의 중간 계층에서 임베딩 시퀀스를 추출합니다. 실험에서는 계층 4를 사용합니다. 각 임베딩 시퀀스는 Conformer 레이어로 구성된 모델에 공급되고, 그 다음에 시간 차원을 따라 전역 평균 풀링과 선형 투영 레이어가 이어지므로 각 크롭은 단일 1024차원 임베딩으로 표현됩니다. 이 모델은 Radford et al. (2021)과 동일한 대조 손실을 사용하여 학습합니다. 음향 일관성 드리프트를 측정하기 위해 3초 프롬프트에서 계산된 임베딩과 시간 축을 따라 3초의 후속 크롭에서 계산된 임베딩 간의 코사인 유사도를 계산합니다. 예를 들어, 그림 2는 Libri Speech test-clean의 &#39;긴&#39; 분할에서 측정된 드리프트를 보여줍니다. SoundStorm에서 생성된 오디오의 경우 음향 일관성 점수가 원래 샘플에서 측정된 점수와 가까운 반면 AudioLM의 경우 시간이 지남에 따라 더 큰 드리프트가 관찰됩니다. 표 1은 &#39;짧은&#39;, &#39;중간&#39; 및 &#39;긴&#39; 분할의 평균 음향 일관성 점수를 보고하며, 여기서 평균은 겹치지 않는 크롭에 대해 수행됩니다. 놀랍지 않게도 개선은 더 긴 오디오 샘플에서 더 분명합니다.오디오 품질.우리는 생성된 샘플의 지각된 오디오 품질을 추정하기 위해 DNSMOS(Reddy et al., 2021)와 유사한 MOS 추정기를 사용합니다.표의 결과는 MOS 추정기에 따르면 SoundStorm이 AudioLM의 음향 생성기와 동등하며, 이는 Borsos et al.(2022) 및 Kharitonov et al.(2023)의 주관적인 연구에서 기준 진실 오디오의 품질과 일치하는 것으로 나타났습니다.런타임(초)10° 10-1. 10-2.SoundStorm: 효율적인 병렬 오디오 생성 SoundStream Dec AudioLM StageAudioLM Stage 2&amp;3|SoundStormSequence 길이(초) 그림 3. TPU-v4에서 SoundStream 디코딩, SoundStorm 및 AudioLM의 다양한 단계의 런타임. 오디오 품질 4.4.14.03.93.3.Long Mid + ShortNumber of iterations 그림 4. 첫 번째 RVQ 레벨에서 디코딩 반복 횟수에 따른 오디오 품질. 4.3. 런타임 및 Ablations 런타임. 단일 TPUv4에서 최대 30초의 오디오를 생성하는 다양한 방법의 런타임을 측정합니다. 그림 3은 SoundStorm이 AudioLM의 음향 생성기(그림의 &quot;AudioLM Stage 2&amp;3&quot;)보다 두 자릿수 더 빠르게 오디오를 생성할 수 있음을 보여주며, SoundStream 디코더로 파형을 디코딩하는 것을 포함하여 실시간 계수는 0.017입니다. 또한 AudioLM의 의미 생성 단계(그림의 &quot;AudioLM 단계 1&quot;)의 런타임을 측정하고 의미 생성 단계를 SoundStream과 결합하면 2초(의미 생성의 경우 1.4초, SoundStorm의 경우 0.5초, SoundStream 디코딩의 경우 0.1초) 내에 30초의 음성 연속을 생성할 수 있다는 결론을 내렸습니다. 디코딩 단계 수. 이전 실험에서 첫 번째 RVQ 레벨을 디코딩하는 데 16번의 반복을 사용했고 후속 레벨에 대해 탐욕적 디코딩을 수행했습니다. 이제 다른 RVQ 레벨에 대한 디코딩 반복 횟수의 효과를 조사합니다. 이를 달성하기 위해 첫 번째 레벨에서 다른 횟수의 디코딩 반복을 사용하여 LibriSpeech test-clean에서 화자 프롬프트 실험을 반복합니다. 그림 4는 오디오 품질 추정기에 따르면 16회 반복을 사용하는 전략이 레벨별 탐욕적 디코딩과 비교했을 때 품질 점수에서 0.1-0.2 증가를 달성하는 반면, 반복 횟수를 더 늘려도 점수가 향상되지 않는다는 것을 보여줍니다. 탐욕적 전략으로 생성된 아티팩트는 분명히 인지할 수 있습니다. 이 전략으로 생성된 샘플을 첨부된 웹페이지에 제공합니다. 또한 RVQ 레벨 2-12에 대한 반복 횟수를 늘리는 실험을 했으며 LibriSpeech 테스트 클린에서 샘플을 합성할 때 오디오 품질 점수에서 통계적으로 유의미한 향상을 발견하지 못했습니다. 이 관찰은 첫 번째 RVQ 레벨에서는 자기 회귀적이고 그 너머에서는 레벨별 탐욕적인 Wang et al.(2023)의 디코딩 전략과 일치한다는 점에 유의합니다. 우리는 의미 토큰과 첫 번째 RVQ 레벨 SoundStream 토큰이 모든 중요한 음향 세부 사항을 포착하지 못하는 음성을 넘어서는 오디오를 생성할 때 더 세부적인 수준에서 여러 번 반복하는 것이 관련성이 있다고 가정합니다.5. 대화 합성 말하기 대화 합성은 여러 화자 턴과 긴 시간 범위에 걸쳐 화자 신원을 유지하는 것이 가장 중요한 응용 프로그램입니다.따라서 SoundStorm의 이상적인 사용 사례가 됩니다.이 섹션에서는 SoundStorm을 텍스트-의미 토큰 모델과 결합하여 최대 30초 길이의 고품질 멀티 턴 대화를 합성할 수 있음을 보여줍니다.이 접근 방식은 SPEAR-TTS(Kharitonov et al., 2023)와 유사합니다.우리는 약 100,000시간 분량의 대화 코퍼스를 수집하여 30초 단위로 분할했습니다.기성 ASR 시스템을 실행하여 각 세그먼트에 대한 대본을 생성했습니다. 이러한 필사본에서 화자 턴은 필사본의 적절한 위치에 턴 마커 기호로 주석이 달려 있습니다. 의미 토큰을 추출하기 위해 이 데이터 세트에서 0.6B 매개변수 BEST-RQ(Chiu et al., 2022)를 학습하고 BEST-RQ 모델의 레이어 13 활성화에 4096개 클러스터 센터가 있는 k-평균을 적용합니다. 그 결과 초당 25개 의미 토큰이 생성되고 코드북 크기는 4096입니다. 24kHz에서 작동하는 SoundStream 코덱을 학습하여 이 코퍼스에서 초당 12개 RVQ로 프레임을 생성합니다. 텍스트-의미 토큰 매핑을 모델링하기 위해 ByT5-large Transformer(Xue et al., 2022)를 학습합니다. 이것은 36개의 인코더와 12개의 디코더 레이어, 1536의 임베딩 크기, 3840의 피드포워드 차원을 갖는 인코더-디코더 모델입니다. 이 모델은 총 1.2B개의 매개변수를 갖습니다. 우리는 디코더만 훈련하고 게시된 ByT5 체크포인트(Xue et al., 2022)에서 텍스트 사전 훈련된 인코더를 사용합니다. 이 모델은 입력으로 텍스트의 바이트 수준 표현을 취하고 중복 제거되지 않은 의미 토큰을 예측합니다. 디코딩은 온도 샘플링으로 수행되며 온도는 0.9이고 top-k는 125로 설정됩니다. 디코더의 크기가 적당하고 대상 시퀀스 길이가 짧기 때문에(30초 동안 750개의 의미 토큰만 있음) 이 모델로 추론을 실행하는 데 TPU-v4에서 1초가 걸립니다. SoundStorm: 효율적인 병렬 오디오 생성 우리는 대화 코퍼스에서 텍스트-의미 모델과 SoundStorm을 모두 10개 에포크 동안 훈련합니다. 추론을 위해 훈련 중에 보지 못한 화자의 짧은 대화를 녹음하여 우리 모델의 프롬프트로 사용했고, 프롬프트의 연장인 텍스트 대본을 만들었습니다. 그런 다음 대본을 텍스트-의미 모델에 공급하고, 그 출력을 SoundStorm에 공급하는 동시에 두 단계 모두에 화자 프롬프트를 사용했습니다. 이 접근 방식은 고품질의 자연스러운 대화 시퀀스를 생성하고, 대본에 필러 단어가 나타날 때 말더듬이를 생성하며, 대본에 턴 마커 기호를 삽입하여 통제된 화자 턴을 허용한다는 것을 발견했습니다. 30초 세그먼트를 합성하는 데 걸리는 총 런타임은 2초입니다. 독자는 첨부된 웹 페이지에서 생성된 샘플을 들어보시기 바랍니다.² 6.
--- CONCLUSION ---
이 논문에서는 이산 컨디셔닝 토큰에서 고품질 오디오를 효율적으로 합성할 수 있는 모델인 SoundStorm을 제시합니다. AudioLM의 음향 생성기와 비교했을 때 SoundStorm은 두 자릿수 더 빠르고 긴 오디오 샘플을 생성할 때 더 높은 시간적 일관성을 달성합니다. SPEAR-TTS와 유사한 텍스트-의미 토큰 모델을 SoundStorm과 결합함으로써 텍스트-음성 합성을 더 긴 컨텍스트로 확장하고 여러 화자 턴이 있는 자연스러운 대화를 생성하여 화자의 음성과 생성된 콘텐츠를 모두 제어할 수 있습니다. 7. 더 광범위한 영향 SoundStorm은 신경 오디오 코덱에서 파생된 오디오 표현을 고품질로 효율적으로 생성하기 위한 모델입니다. 이 작업에서 우리는 이를 AudioLM 및 SPEAR-TTS의 음향 생성 파이프라인을 대체하는 데 사용합니다. 우리는 모델에서 생성된 오디오 샘플이 예를 들어 표현된 악센트와 음성 특성 측면에서 훈련 데이터에 존재하는 편향의 영향을 받을 수 있음을 인정합니다. 생성된 샘플에서 우리는 프롬프팅을 통해 화자 특성을 안정적으로 제어할 수 있음을 보여줍니다. 그러나 모든 훈련 데이터와 그 한계에 대한 보다 철저한 분석은 책임 있는 AI 원칙에 따라 향후 작업 분야가 될 것입니다. 차례로 음성을 모방하는 능력은 생체 인식 식별을 우회하고 사칭을 목적으로 하는 등 수많은 악의적인 응용 프로그램을 가질 수 있습니다. 따라서 잠재적인 오용에 대한 보호 장치를 마련하는 것이 중요합니다. 이를 위해 SoundStorm에서 생성된 오디오가 전용 분류기(Borsos et al.(2022)와 동일한 분류기를 사용하여 98.5% 2https://google-research.github.io/ seanet/soundstorm/examples/)에서 감지 가능한지 확인했습니다. 따라서 더 큰 시스템의 구성 요소로서 SoundStorm은 Borsos et al.(2022) 및 Kharitonov et al.(2023)에서 이전에 논의한 위험에 추가적인 위험을 초래할 가능성이 낮다고 생각합니다. 동시에 AudioLM의 메모리 및 계산 요구 사항을 완화하면 오디오 생성 분야의 연구가 더 광범위한 커뮤니티에서 더 쉽게 접근할 수 있게 됩니다. 앞으로는 합성 음성을 감지하는 다른 접근 방식(예: 오디오 워터마킹)을 탐색하여 이 기술을 잠재적으로 제품에 사용할 때 책임 있는 AI 원칙을 엄격히 따를 계획입니다.8. 감사의 말 저자는 기술적 토론과 도움이 되는 피드백을 제공해 주신 Aren Jansen과 RJ Skerry-Ryan, 음성 샘플을 기부해 주신 Jelena Antić, Brian McWilliams, Paul Rubenstein, Michael Dooley 및 기타 성우 자원봉사자에게 감사드립니다.참고문헌 Agostinelli, A., Denk, TI, Borsos, Z., Engel, JH, Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., Sharifi, M., Zeghidour, N., Frank, CH MusicLM: 텍스트에서 음악 생성. arXiv:2301.11325, 2023. Baevski, A., Zhou, H., Mohamed, A. 및 Auli, M. wav2vec 2.0: 음성 표현의 자기 지도 학습을 위한 프레임워크. arXiv:2006.11477, 2020. Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M. 및 Zeghidour, N. AudioLM: 오디오 생성에 대한 언어 모델링 접근 방식. arXiv:2209.03143, 2022. Chang, H., Zhang, H., Jiang, L., Liu, C. 및 Freeman, WT MaskGIT: 마스크 생성 이미지 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2022. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., and Wei, F. WavLM: 풀 스택 음성 처리를 위한 대규모 자기 지도 사전 학습. IEEE Journal of Selected Topics in Signal Processing, 2022. Chiu, C., Qin, J., Zhang, Y., Yu, J., and Wu, Y. 음성 인식을 위한 랜덤 투영 양자화기를 사용한 자기 지도 학습. 기계 학습 국제 컨퍼런스(ICML), 2022. Choromanski, KM, Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlós, T., Hawkins, P., Davis, JQ, MoSoundStorm: 효율적인 병렬 오디오 생성 hiuddin, A., Kaiser, L., Belanger, DB, Colwell, LJ, and Weller, A. Rethinking attention with Performers. 학습 표현 국제 컨퍼런스(ICLR), 2021. Chung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. w2v-BERT: 대조 학습과 마스크 언어 모델링을 결합하여 자체 감독 음성 사전 학습. IEEE 자동 음성 인식 및 이해 워크숍, ASRU, 2021. Défossez, A., Copet, J., Synnaeve, G., 및 Adi, Y. 고충실도 신경 오디오 압축. arXiv:2210.13438, 2022. Donahue, C., Caillon, A., Roberts, A., Manilow, E., Esling, P., Agostinelli, A., Verzetti, M., Simon, I., Pietquin, O., Zeghidour, N., 및 Engel, JH Singsong: 노래에서 음악 반주 생성. arXiv:2301.12662, 2023. Ghazvininejad, M., Levy, O., Liu, Y., 및 Zettlemoyer, L. 마스크 예측: 조건부 마스크 언어 모델의 병렬 디코딩. arXiv:1904.09324, 2019. Gu, J., Bradbury, J., Xiong, C., Li, VO, and Socher, R. 비자기회귀 신경망 기계 번역. arXiv:1711.02281, 2017. Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang, R. Conformer: 음성 인식을 위한 합성곱 증강 변환기. Interspeech, 2020에서. Hawthorne, C., Jaegle, A., Cangea, C., Borgeaud, S., Nash, C., Malinowski, M., Dieleman, S., Vinyals, O., Botvinick, MM, Simon, I., Sheahan, H., Zeghidour, N., Alayrac, J., Carreira, J., and Engel, JH Perceiver AR을 사용한 범용, 장문맥 자기 회귀 모델링. International Conference on Machine Learning(ICML), 2022에서. Hsu, W., Bolte, B., Tsai, YH, Lakhotia, K., Salakhutdinov, R., and Mohamed, A. HuBERT: 숨겨진 유닛의 마스크 예측을 통한 자기 감독 음성 표현 학습. arXiv:2106.07447, 2021. Jouppi, NP, Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N., Subramanian, S., Swing, A., Towles, B., et al. TPU v4: 임베딩을 위한 하드웨어 지원을 갖춘 기계 학습을 위한 광학적으로 재구성 가능한 슈퍼컴퓨터. arXiv 사전 인쇄 arXiv:2304.01433, 2023. Kahn, J., Rivière, M., Zheng, W., Kharitonov, E., Xu, Q., Mazaré, P., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A. 및 Dupoux, E. Libri-Light: 감독이 제한되거나 없는 ASR에 대한 벤치마크입니다. IEEE 국제 음향, 음성 및 신호 처리(ICASSP) 컨퍼런스, 2020. Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., 및 Zeghidour, N. 말하고, 읽고, 촉구하기: 최소한의 감독으로 고품질 텍스트 음성 변환. arXiv:2301.03540, 2023. Kitaev, N., Kaiser, Ł., 및 Levskaya, A. Reformer: 효율적인 변압기. arXiv:2001.04451, 2020. Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Défossez, A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. AudioGen: 텍스트 기반 오디오 생성. arXiv:2209.15352, 2022. Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., et al. 원시 오디오에서 생성적 구어 모델링에 관하여. Transactions of the Association for Computational Linguistics, 2021. Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S. 잔류 양자화를 사용한 자기회귀 이미지 생성. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 2022. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: 퍼블릭 도메인 오디오북을 기반으로 한 ASR 코퍼스. IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP)에서. IEEE, 2015. Radford, A., Kim, JW, Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervisor. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. Reddy, CKA, Gopal, V., and Cutler, R. DNSMOS: 노이즈 억제기를 평가하기 위한 비침투적 지각적 객관적 음성 품질 지표. In IEEE International Conference on Acoustics, Speech and Signal Processing (DNSMOS), 2021. Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: 회전 위치 임베딩을 갖춘 향상된 변압기. arXiv:2104.09864, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, AN, Kaiser, L. 및 Polosukhin, I. 관심만 있으면 됩니다. 신경 정보 처리 시스템(NeurIPS)의 발전, 2017. Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, MT, Castro, S., Kunze, J. 및 SoundStorm: 효율적인 병렬 오디오 생성 Erhan, D. Phenaki: 오픈 도메인 텍스트 설명에서 가변 길이 비디오 생성. arXiv:2210.02399, 2022. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. 신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다. arXiv:2301.02111, 2023. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nyströmformer: 셀프 어텐션을 근사화하기 위한 Nyström 기반 알고리즘. AAAI 인공지능 컨퍼런스 회의록, 2021. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: 사전 훈련된 바이트-투-바이트 모델을 통한 토큰 없는 미래를 향하여. Association for Computational Linguistics의 트랜잭션, 10:291–306, 2022. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: 엔드투엔드 신경 오디오 코덱. IEEE 오디오, 음성 및 언어 처리 트랜잭션, 2022.
