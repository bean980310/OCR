--- ABSTRACT ---
우리는 긴 입력을 처리하는 데 적합한 다국어의 효율적인 텍스트-텍스트 변환기를 개발하는 작업을 제시합니다. mLongT5라고 하는 이 모델은 LongT5의 아키텍처를 기반으로 구축되는 동시에 mT5 사전 학습에 사용된 다국어 데이터 세트와 UL2의 사전 학습 작업을 활용합니다. 우리는 이 모델을 다양한 다국어 요약 및 질의응답 작업에서 평가하며, 그 결과 mBART 또는 M-BERT와 같은 기존 다국어 모델과 비교했을 때 mLongT5의 성능이 더 강력하다는 것을 보여줍니다.
--- INTRODUCTION ---
최근 몇 년 동안, 더 긴 입력 시퀀스를 처리할 수 있도록 변압기 기반 모델을 보다 효율적으로 만드는 개발이 있었습니다. 그러나 많은 모델이 영어 전용이어서 다른 언어에는 적용할 수 없었습니다. 이 논문에서는 이러한 모델 중 하나를 확장하여 다국어 데이터를 처리할 수 있는 작업을 제시합니다. mLongT5라는 모델은 LongT(Guo et al., 2022)의 효율적인 아키텍처를 활용하고 다국어 mC4 데이터 세트(Xue et al., 2021)에서 사전 학습되어 다국어 작업을 수행할 수 있습니다. 다양한 다국어 요약 및 질의응답 작업에 mLongT5를 적용했으며, 결과에 따르면 mLongT5는 이러한 도메인에서 강력한 성능을 보였습니다. 구성¹과 체크포인트²는 모두 오픈 소스로 공개되었습니다. 2
--- RELATED WORK ---
관련 작업에는 긴 입력을 처리할 수 있는 효율적인 변환기 모델과 다국어 모델의 두 가지 영역이 있습니다. &#39;https://github.com/google/flaxformer/tree/main/flaxformer/t5x/configs/longt5/models 2https://github.com/google-research/longt최근에는 긴 입력을 처리하는 것과 같이 변환기 모델을 보다 효율적으로 만드는 데 많은 관심이 있었습니다. 이러한 예로는 ETC(Ainslie et al., 2020), Big Bird(Zaheer et al., 2020), Long T5(Guo et al., 2022) 및 Longformer(Beltagy et al., 2020)가 있습니다. 이러한 모델은 변환기에서 어텐션 메커니즘의 2차 성장을 해결하기 위해 다양한 접근 방식을 취하는 데 성공했습니다. 그러나 불행히도 이러한 모델은 영어 데이터 세트에서 학습되어 다국어 도메인에서의 사용이 제한됩니다. 다국어 모델과 관련하여 여기에는 mT5(Xue et al., 2021), mBART(Liu et al., 2020) 및 최근 umT5(Chung et al., 2023)가 포함됩니다. 이러한 모델은 영어 모델에서 사용하는 아키텍처를 재사용했지만 더 큰 다국어 코퍼스에서 사전 학습되었으며 mT5와 umT5는 101개 언어로 학습되었고 mBART는 25개 언어로 학습되었습니다. 이러한 모델은 광범위한 언어를 처리할 수 있는 강력한 성능을 보였지만 더 긴 시퀀스로 확장할 수 없다는 점에서 원래 영어 모델과 동일한 제한을 받았습니다. 3 모델 mLongT5는 LongT(Guo et al., 2022)의 아키텍처를 기반으로 합니다. LongT5는 보다 효율적인 어텐션 메커니즘을 활용하여 긴 입력을 효율적으로 처리하도록 개발되었습니다. 이 모델은 다양한 다운스트림 작업에서 강력한 성능을 보이는 것으로 나타났으며 따라서 mLongT5의 기반이 되었습니다. 3. 데이터 세트 mLongT5를 다국어로 만들기 위해 101개 언어로 구성된 다국어 모델 mT5(Xue et al., 2021)를 학습하는 데 사용된 mC4 데이터 세트를 활용합니다. 이 데이터 세트는 Chung et al.(2023)에서 설명한 대로 최근 업데이트되었으며 umT5를 학습하고 새로운 SentencePiece 모델을 만드는 데 사용되었습니다(Kudo and Richardson, 2018). 그런 다음 umT5에 사용된 것과 동일한 SentencePiece 모델을 사용하여 mLongT5가 다국어 입력을 처리할 수 있도록 합니다. 3.2 사전 학습 작업 우리 모델과 LongT5의 주요 차이점 중 하나는 모델 사전 학습 작업을 변경한 것입니다. LongT5는 모델을 사전 학습하기 위해 PEGASUS의 Principle Sentences Generation(PSG)(Zhang et al., 2020)을 사용했습니다. 이것이 다양한 다운스트림 작업에 대해 강력한 성능을 보이는 것으로 나타났지만, PSG의 한 가지 약점은 다국어 학습에 덜 적합하다는 것입니다.PSG는 텍스트를 문장으로 분할할 수 있는 기능에 의존하며, 현재 구현은 라틴어 기반 언어에 가장 적합합니다.101개의 다른 언어에 대해 텍스트를 문장으로 올바르게 분할해야 하므로 다국어 환경에서 사용하기 어려운 작업입니다.이를 극복하기 위해 대신 UL2의 사전 학습 작업(Tay et al., 2022)을 적용하기로 결정했습니다.Mixture-of-Denoisers(MoD)라고 하는 사전 학습 작업은 모델이 여러 작업을 혼합하여 학습하며 T5의 원래 사전 학습 작업(Raffel et al., 2019)보다 더 잘 작동하는 것으로 나타났습니다.더 중요한 것은 MoD가 PSG에 비해 다른 언어에 더 쉽게 적용될 수 있으므로 mLongT5 사전 학습에 이상적이라는 것입니다.3.3 사전 학습 세부 정보 mLongT5 사전 학습은 LongT5의 사전 학습 방식과 많은 유사점이 있습니다. 100만 단계에 대해 사전 학습되었으며, Base, Large, XL의 모델 크기를 사전 학습했습니다. 또한 입력에 대해 4,096, 대상에 대해 910의 동일한 사전 학습 길이를 사용합니다. 한 가지 작은 차이점은 배치 크기를 128에서 256으로 늘려 모델이 mT5와 동일한 수의 토큰에서 학습할 수 있도록 하는 것입니다. mC4 데이터 세트의 경우 Chung et al.(2023)이 업데이트한 버전인 버전 3.1.0을 사용했습니다. 데이터 세트 샘플링의 경우 UniMax 샘플링을 사용합니다.
--- CONCLUSION ---
저희는 새로운 모델 mLongT5를 선보였습니다. 이 모델은 LongT5의 효율적인 아키텍처의 이점을 가지고 있으며, 다국어 입력 및 출력을 처리할 수 있습니다. 저희 보고서에서 보여 주듯이, 이 모델은 다양한 요약 및 질의응답 작업에서 좋은 성과를 낼 수 있습니다. 접근 EM FmT5(베이스 - 512 입력) mT5(베이스 1k 입력) mT5(베이스 - 2k 입력) mT5(베이스 4k 입력) mT5(대형 - 512 입력) mT5(대형 4k 입력) mT5(xl - 512 입력) mT5(xl - 4k 입력) 37.16 49.43.09 56.44.63 58.45.41 58.40.96 54.52.77 66.43.84 56.55.03 68.50.76 62.51.21 63.52.64.54.66.55.68.55.93 68.58.52 70.59.6 71.60.42 72.mLongT5(베이스 - 4k 입력) mLongT5(기본 - 8k 입력) mLongT5(기본 - 16k 입력) mLongT5(대형 - 4k 입력) mLongT5(대형 - 8k 입력) mLongT5(대형 - 16k 입력) mLongT5(xl - 4k 입력) mLongT5(xl - 8k 입력) mLongT5(xl - 16k 입력) 제한 사항 표 4: TyDi QA 결과. mLongT5는 원래 LongT5 모델에서 볼 수 있는 것과 동일한 제한 사항이 있는데, 즉 더 긴 입력에 대한 작업에 더 적합하다는 것입니다. 짧은 입력이 있는 작업은 전체 주의를 활용할 수 있는 mT5 및 umT5와 같은 모델이 더 적합합니다. 참고 문헌 Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: 길고 구조화된 입력을 변환기에 인코딩. arXiv 사전 인쇄본 arXiv:2004.08483. Iz Beltagy, Matthew E. Peters, Arman Cohan. 2020. Longformer: 긴 문서 변환기. Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, Orhan Firat. 2023. UniMax: 대규모 다국어 사전 학습을 위한 보다 공정하고 효과적인 언어 샘플링. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, Jennimaria Palomaki. 2020. TyDi QA: 유형적으로 다양한 언어에서 정보를 찾는 질문에 대한 답변을 위한 벤치마크. Association for Computational Linguistics의 거래, 8:454–470. Jacob Devlin. 2018. 다국어 BERT README. https://github.com/google-research/bert/blob/master/multilingual.md. Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina 맥밀런 메이저, 사이먼 Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, Jiawei Zhou. 2021. GEM 벤치마크: 자연어 생성, 평가 및 메트릭. 제1회 자연어 생성, 평가 및 메트릭 워크숍(GEM 2021) 회의록, 96-120쪽, 온라인. Association for Computational Linguistics. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung 및 Yinfei Yang. 2022. LongT5: 긴 시퀀스를 위한 효율적인 텍스트-텍스트 변환기. 전산언어학협회 조사 결과: NAACL 2022, 페이지 724736, 미국 시애틀. 전산언어학협회. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman 및 Rifat Shahriyar. 2021. XLsum: 44개 언어에 대한 대규모 다국어 추상 요약. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 4693-4703페이지, 온라인. Association for Computational Linguistics. Taku Kudo와 John Richardson. 2018. SentencePiece: 신경 텍스트 처리를 위한 간단하고 언어 독립적인 하위 단어 토크나이저 및 디토크나이저. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 66-71페이지, 벨기에 브뤼셀. Association for Computational Linguistics. Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen McKeown. 2020. WikiLingua: 언어 간 추상 요약을 위한 새로운 벤치마크 데이터 세트. In Findings of the Association for Computational Linguistics: EMNLP 2020, 4034-4048페이지, 온라인. Association for Computational Linguistics. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. 2020. 신경망 기계 번역을 위한 다국어 노이즈 제거 사전 학습. Association for Computational Linguistics의 거래, 8:726-742. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2019. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. CoRR, abs/1910.10683. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano. 2020. MLSUM: 다국어 요약 코퍼스. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 8051-8067쪽, 온라인. Association for Computational Linguistics. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler. 2022. UL2: 언어 학습 패러다임 통합. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. 2021. mT5: 대규모 다국어 사전 학습된 텍스트-텍스트 변환기. 2021년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 483-498쪽, 온라인. 컴퓨터 언어학회. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. 2020. Big Bird: 더 긴 시퀀스를 위한 변환기. 신경 정보 처리 시스템의 발전, 33권, 17283-17297쪽. Curran Associates, Inc. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu. 2020. PEGASUS: 추상적 요약을 위한 추출된 갭 문장을 사용한 사전 학습. 37회 기계 학습 국제 컨퍼런스의 회의록, 기계 학습 연구 회의록 119권, 11328-11339페이지. PMLR. XL-Sum 표 5에 mLongT 모델을 XL-Sum에서 실행한 전체 결과를 보여줍니다. 이 결과는 저자가 업데이트된 데이터 세트와 함께 GitHub³에 업로드한 것입니다. ROUGE 점수를 계산할 때, 중국어, 일본어, 태국어를 제외하고 해당 논문에서 수행한 것과 유사한 계산을 사용합니다. 이러한 언어의 경우, ROUGE를 계산하기 위해 결과의 토큰화를 위해 모델에서 사용한 SPM을 사용합니다. 3 https://github.com/csebuetnlp/xl-sum mT5(기본) 언어 RRRL R-mLongT5(기본) RRL R-mLongT5(대형) RRL R-mLongT5(xl) RRL 암하라어 아랍어 아제르바이잔어 벵골어 20.05 7.41 18.34.91 14.79 29.21.42 9.52 19.29.57 12.11 25.21.버마어 15.96 5.15 14.45.중국어(간체) 중국어(번체) 영어 프랑스어 39.41 17.79 33.37.19 17.14 31.37.60 15.15 29.35.34 16.17 28.38.39.22.32.32.11.25.30.12.24.구자라트어 하우사 힌디어 38.Igbo 31.10.인도네시아어 37.17.21.96 7.74 19.39.44 17.68 31.67 34.61 13.16.88 32.01 34.81 14.24.53 25.82 8.13.19.59 6.30.76 32.일본어 48.23.85 37.36 45.27.키룬디 한국어 23.31.99 14.37 25.83 25.11.45 22.36 20.10.9.키르기스 18.마라티어 22.7.96 16.9.14.5.네팔어 26.65 10.오로모어 파슈토어 페르시아어 피진어 포르투갈어 펀자브어 러시아어 스코틀랜드 게일어 세르비아어(키릴 문자) 세르비아어(라틴 문자) 싱할라어 10.소말리아어 스페인어 스와힐리어 타밀어 텔루구어 태국어 19.86 7.03 17.티그리냐어 터키어 우크라이나어 우르두어 우즈베크어 베트남어 웨일스어 요루바어 9.9.31.79 13.20.68 8.15.11 4.37.40 17.28 28.88 35.98 21.25.32 8.02 21.17 22.27 7.32.93 15.57 29.26 25.52 11.23.99 10.14 20.92 20.97 8.39.56 18.37 32.84 37.11 15.16.83 6.34 15.41 14.60 5.32.88 16.22 26.08 31.58 15.32.66 11.60 26.12 29.96 9.31.66 11.66 25.09 25.87 8.16.70 5.91 14.73 20.29 7.99 18.09 22.37 8.90 19.26.39 11.01 22.45 27.65 12.25 23.57 32.09 15.04 27.17.52 7.10 15.77 19.92 8.80 18.08 22.68 9.89 20.8.18.65 24.69 10.04 21.25 26.83 11.32 22.26.62 34.76 49.07 29.52 38.10 51.60 31.69 40.21.78 32.59 42.62 24.70 35.80 48.42 29.99 41.43.32 25.56 35.95 48.82 30.80 41.35.59 13.63 28.02 39.51 17.00 31.31.88 14.32 25.61 34.82 16.17 28.17.61 22.38 7.94 20.15 25.52 9.92 22.27.30 38.04 16.07 30.32 40.58 18.28.71 37.42 16.71 31.22 40.92 19.20.19 30.41 10.01 23.68 31.31 9.88 24.26.59 35.17 15.23 29.07 38.87 18.00 32.36.51 48.60 29.95 39.00 50.77 32.06 40.20.26 29.36 12.78 23.67 31.67 14.55 25.19.00 23.18 10.42 21.38 25.30 11.63 23.12.46 16.01 6.30 14.14 18.19 7.81 16.19.92 20.33 8.62 18.41 23.35 10.56 21.22 25.90 12.03 23.24.28 23.96 8.94 21.80 26.24 10.33 23.91 28.87 11.18.70 6.17 16.19 14.88 4.38 12.71 17.91 5.65 15.28 19.52 6.38.47 15.55 31.35.01 13.79 28.84 38.63 16.06 32.00 41.37 17.61 33.36.94 16.19 30.07 35.47 14.66 28.40 37.70 16.45 30.49 40.64 18.89 33.37.96 15.12 29.87 33.86 12.01 26.68 35.86 13.72 28.24 38.01 15.08 29.37.17 15.90 28.56 31.67 12.51 24.46 34.04 14.51 26.37.30.70 12.21 25.52 28.61 10.43 23.66 31.92 12.75 26.17 34.45 14.81 28.32.22 13.64 26.17 22.11 8.29 18.62 24.39 10.00 20.54 28.20 12.72 23.29.02 10.99 22.88 26.98 8.87 21.57 29.80 10.64 23.44 31.74 12.61 25.23.78 7.98 20.14 20.30 5.86 16.74 21.92 6.98 18.35 27.51 11.46 23.21.64 6.66 18.23 18.14 4.75 14.96 21.79 6.92 18.14 25.86 10.17 21.27.29 13.38 23.47 22.19.96 25.24 11.52 21.98 27.78 13.20 24.31.56 11.58 24.22 27.21.10 30.29 10.69 23.29 31.64 11.11 24.31.51 11.88 24.07 26.20.47 28.71 10.56 22.04 32.20 13.10 24.37.67 17.85 30.25.67 34.29 15.22 27.82 37.29 17.24.33 11.06 22.18.71 24.08 10.74 21.71 26.81 12.13.48 17.98 6.12 16.10 21.20 7.26.65 38.11 22.92 28.26 40.70 25.18.61 26.30 8.90 22.05 28.53 10.13 24.22.83 28.56 13.25.72 31.33 15.61 28.18.17 23.34 9.74 20.29 27.05 12.16 23.30.14 39.90 18.53 32.75 43.03 21.40 35.13.39 17.26 6.42 15.49 19.18 7.80 17.25.02 34.54 17.63 27.59 38.17 20.23.96 33.66 12.26 27.01 36.49 15.34 29.20.27 29.49 10.50 23.26 32.20 12.34 25.32.34.26.17.17.57 29.30.24.18.30.30.표 5: XL-Sum에 대한 전체 결과.
