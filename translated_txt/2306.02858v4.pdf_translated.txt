--- ABSTRACT ---
우리는 비디오의 시각적 및 청각적 내용을 모두 이해할 수 있는 기능을 갖춘 대규모 언어 모델(LLM)을 강화하는 다중 모달 프레임워크인 Video-LLAMA¹를 제시합니다. Video-LLaMA는 동결된 사전 훈련된 시각 및 오디오 인코더와 동결된 LLM에서 교차 모달 훈련을 부트스트랩합니다. 시각적 또는 오디오 신호만 처리하기 위해 LLM을 보완하는 이전 작업(Zhu et al., 2023; Liu et al., 2023; Huang et al., 2023a)과 달리 Video-LLAMA는 두 가지 과제를 해결하여 비디오 이해를 가능하게 합니다. (1) 시각적 장면의 시간적 변화 포착, (2) 오디오-비주얼 신호 통합. 첫 번째 과제를 해결하기 위해 사전 훈련된 이미지 인코더를 비디오 인코더에 조립하고 비디오-텍스트 생성 작업을 도입하여 비디오-언어 대응 관계를 학습하는 Video Q-former를 제안합니다. 두 번째 과제에서는 다중 모달리티를 정렬하는 범용 임베딩 모델인 ImageBind(Girdhar et al., 2023)를 사전 훈련된 오디오 인코더로 활용하고 ImageBind 위에 Audio Q-former를 도입하여 LLM 모듈에 대한 합리적인 청각 쿼리 임베딩을 학습합니다. 시각 및 오디오 인코더의 출력을 LLM의 임베딩 공간과 맞추기 위해 먼저 방대한 비디오/이미지 캡션 쌍에서 VideoLLAMA를 훈련한 다음 적당한 양이지만 더 높은 품질의 시각-지시 데이터 세트로 모델을 조정합니다. Video-LLAMA는 비디오 콘텐츠를 인식하고 이해하고 비디오에 제시된 시각 및 청각 정보를 기반으로 의미 있는 응답을 생성하는 능력을 보여줍니다. 1
--- INTRODUCTION ---
대규모 언어 모델(LLM)(Chowdhery 등, 2022; Bai 등, 2022; OpenAI, 2023)은 놀라운 이해 능력을 입증했으며, *Xin Li가 해당 저자입니다. &#39;비디오 데모는 사용자의 의도와 지시에 따라 https://youtu.be/RDNYs3Rswhc에서 제공됩니다.234 일반적으로 사용자 요청과 LLM의 해당 응답은 모두 텍스트 형식이지만, 실제 세계 정보는 일반적으로 다중 모달이기 때문에 텍스트 전용 인간-컴퓨터 상호 작용은 많은 응용 시나리오에 충분하지 않습니다. LLM의 잠재력을 더욱 탐구하기 위해 많은 연구자들은 LLM에 다중 모달 콘텐츠를 이해하는 기능을 부여하려고 시도합니다(Huang 등, 2023a; Zhang 등, 2023b; Yin 등, 2023). 이러한 노력 중에는 Alayrac 등(2022b); Wang 등(2022); Huang 등(2023b); Xu 등(2023b); Zhang 등(2023b); Sun 등(2023)은 다중 모달 입력을 수용하기 위해 방대한 인터리브 이미지-텍스트 데이터 또는 음성-텍스트 데이터로 다중 모달 LLM을 사전 학습합니다. 한편, 또 다른 연구 그룹은 다중 모달 이해를 달성하기 위해 기성품 비전 또는 음성 기반 모델로 LLM을 보완하여 매개변수 효율성이 더 높은 방식을 채택합니다(Li 등, 2023b; Zhu 등, 2023; Liu 등, 2023; Ye 등, 2023; Zhang 등, 2023a; Huang 등, 2023a; Wu 등, 2023b; Su 등, 2023; Li 등, 2023a). 이러한 접근 방식은 효과적이지만, 정확히 하나의 추가 모달리티에서 입력을 텍스트(예: 이미지 또는 오디오)와 정렬하는 데 전념하고 있으며, 이는 비디오 이해에 만족스럽지 않습니다. 구체적으로, LLM이 비디오를 이해하도록 하려면 시각적 입력, 청각적 입력 및 텍스트 출력을 포함한 다양한 모달리티에 대한 포괄적인 처리가 필요하며, 이는 이미지 전용 이해 및 오디오 전용 이해 작업보다 더 어렵습니다. 최근 LLM의 비디오 이해 능력을 발휘하려는 시도가 여러 건 있지만(Li et al., 2023c; Maaz et al., 2023; Luo et al., 2023), 이들의 주요 목적은 비디오의 시각적 콘텐츠만 이해하고 청각적 콘텐츠는 사용하지 않는 것입니다. 2https://chat.openai.com/chat https://www.anthropic.com/product 4https://bard.google.com/ 모델 이름 BLIP2(Li et al., 2023b) MiniGPT4(Zhu et al., 2023) LLAVA(Liu et al., 2023) mPLUG-Owl(Ye et al., 2023) VideoChat(Li et al., 2023c) AudioGPT(Huang et al., 2023a) Video-ChatGPT(Maaz et al., 2023) Video-LLAMA 기능 정적 이미지 무음 비디오 오디오 표 1: 인기 있는 멀티모달 대규모 언어 모델과의 비교. Video-LLAMA는 청각 및 시각 정보를 동시에 이해할 수 있는 고유한 기능을 가지고 있습니다. 이 작업에서 우리는 오디오-비주얼 LLM의 공백을 채우기 위해 비디오 입력을 지원하고 일반적으로 여러 비디오 프레임과 오디오로 구성된 사용자가 업로드한 비디오를 중심으로 사용자가 컴퓨터와 채팅할 수 있는 다중 모달 LLM을 구축할 가능성을 조사합니다. 외부 지각 모델을 사용하여 시각/청각 신호를 텍스트 신호로 변환하는 대신(Shen et al., 2023; Li et al., 2023c), 우리는 하나의 프레임워크 내에서 여러 모달리티의 데이터를 처리할 수 있는 엔드투엔드 모델을 구축하기로 했습니다. 구체적으로, 우리는 BLIP-2(Li et al., 2023b)의 아이디어를 채택하여 교차 모달 사전 학습의 효율성을 보장합니다. 비디오에서 시각적 장면의 변화를 명시적으로 포착하기 위해 사전 학습된 시각적 인코더를 사용하여 프레임 표현을 별도로 계산합니다. 그런 다음 프레임 임베딩 계층을 도입하여 시간 정보를 주입하고 비디오 Q-Former를 도입하여 시각적 쿼리 토큰을 생성합니다. 비디오의 오디오 신호에 대해서는 사전 훈련된 오디오 인코더와 오디오 Q-포머를 추가로 활용하여 합리적인 청각 쿼리 임베딩을 학습합니다(그림 1의 오른쪽 부분 참조). 텍스트 출력을 비디오와 맞추기 위해 다중 분기 교차 모달 사전 훈련을 고안하여 시각-언어 대응 및 청각-언어 대응을 학습합니다. 시각-언어 대응의 경우, 먼저 비디오 클립-텍스트 생성 작업을 사용하여 대규모 비디오 캡션 데이터 세트에서 시각 관련 구성 요소를 사전 훈련합니다. 정적 시각적 개념에 대한 이해를 높이기 위해 이 사전 훈련 단계에 이미지 캡션 데이터도 추가합니다. 그런 다음 비디오 기반 대화 데이터 세트에서 이러한 구성 요소를 추가로 미세 조정하여 시각적 지침 튜닝을 실행합니다. 오디오 인코더와 언어 디코더 간의 정렬을 위해 오디오-텍스트 생성 작업을 사용하여 오디오 캡션 데이터 세트에서 오디오 관련 구성 요소를 추가로 사전 훈련합니다. 오디오-언어 대응의 경우, 우리는 Imagebind(Girdhar et al., 2023)를 인코더로 활용하는데, 이는 다양한 모달리티를 공통 임베딩 공간에 정렬하는 데 매우 뛰어난 성능을 발휘합니다. 오디오-텍스트 데이터의 가용성이 제한되어 있기 때문에, 우리는 또한 시각-텍스트 데이터를 활용하여 오디오 관련 구성 요소를 훈련합니다. 이러한 구성 요소는 Imagebind가 제공하는 공통 임베딩 공간을 LLM의 임베딩 공간과 정렬하는 방법을 학습합니다. 오디오-텍스트 데이터로 명시적으로 훈련되지 않았음에도 불구하고, Video-LLAMA는 추론 중에 놀라운 제로샷 오디오 이해 기능을 보여줍니다. 표 1에서 볼 수 있듯이, 우리의 Video-LLAMA는 비디오에서 시청각 모달 정보를 독특하게 포괄적으로 이해한다는 측면에서 다른 기존의 다중 모달 LLM과 차별화됩니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 우리는 LLM이 주어진 비디오의 시각적 및 청각적 콘텐츠를 동시에 처리하고 인간과 대화할 수 있도록 하는 다중 모달 프레임워크인 Video-LLAMA를 제안합니다. • LLM에 비디오 이해 능력을 부여하기 위해 시각 언어 정렬과 오디오 언어 정렬을 모두 달성하기 위한 다중 분기 교차 모달 사전 학습 프레임워크를 제안합니다.• 사전 학습 및 미세 조정을 위한 전체 코드베이스와 모든 Video-LLaMA5 변형의 모델 가중치를 오픈 소스로 제공합니다.또한 비디오 기반 대화 67에 대한 데모도 준비했습니다.2 방법 Video-LLAMA는 동결된 LLM에 비디오의 시각적 및 청각적 콘텐츠를 모두 이해하는 능력을 부여하는 것을 목표로 합니다.그림 1에서 볼 수 있듯이, 각각 비디오 프레임과 오디오 신호를 LLM의 텍스트 입력과 호환되는 쿼리 표현으로 변환하기 위해 시각 언어 분기와 오디오 언어 분기라는 두 가지 분기를 설계합니다.이 섹션에서는 먼저 전체 아키텍처와 각 분기의 구성 요소를 소개합니다.그런 다음 제안된 다중 분기 교차 모달 사전 학습 및 시청각 교육 조정의 절차를 설명합니다. Shttps://github.com/DAMO-NLP-SG/Video-LLAMA https://huggingface.co/spaces/DAMO-NLP-SG/ Video LLAMA https://modelscope.cn/studios/damo/ video-11ama/summary Vision-Language Branch 이 비디오는 밤에 발사대에서 발사되는 로켓의 애니메이션입니다... LLM(비쿠냐/LLAMA) 인간: 비디오 Q-Former 선형 &#39;이 비디오를 설명해 주세요: 오디오 언어 분기 선형 오디오 Q-Former to to t to t to tq to t 시각적 인코더(VIT &amp; Q-Former) 오디오 인코더 비디오 프레임 오디오 신호 2.1 아키텍처 2.1.그림 1: Video-LLaMA의 전체 아키텍처. Vision-Language Branch Vision-Language Branch는 LLM이 시각적 입력을 이해할 수 있도록 설계되었습니다. 그림 1의 왼쪽 부분에 표시된 것처럼 비디오 프레임에서 특징을 추출하기 위한 동결된 사전 학습된 이미지 인코더, 비디오 프레임에 시간 정보를 주입하기 위한 위치 임베딩 계층, 프레임 수준 표현을 집계하기 위한 비디오 Q-former, 출력 비디오 표현을 LLM의 텍스트 임베딩과 동일한 차원으로 투영하기 위한 선형 계층으로 구성됩니다. 하나의 비디오가 N개의 프레임으로 구성되어 있다고 가정하면 이미지 인코더는 먼저 각 프레임/이미지를 Kƒ 이미지 임베딩 벡터로 매핑하여 비디오 프레임 표현 V [V1, V2, VN]을 생성합니다. 여기서 vi Є RKƒxdf는 i번째 프레임에 해당하는 df 차원 이미지 임베딩 집합입니다. = 동결된 이미지 인코더의 프레임 표현 vi는 시간 정보를 고려하지 않고 계산되므로 다른 프레임의 표현에 시간 정보의 지표로 위치 임베딩을 추가로 적용합니다. 그런 다음, BLIP-2(Li et al., 2023b)의 Query Transformer(QFormer)와 동일한 아키텍처를 공유하는 Video Q-former에 위치 인코딩된 프레임 표현을 공급하여 차원 d₁의 ky 비디오 임베딩 벡터를 비디오의 표현 ✩ Є Rkydy로 얻습니다. E 비디오 표현을 LLM의 입력에 맞게 조정하기 위해 선형 계층을 추가하여 비디오 임베딩 벡터를 비디오 쿼리 벡터로 변환합니다. 비디오 쿼리 벡터는 LLM의 텍스트 임베딩과 동일한 차원입니다. 순방향 패스에서 비디오 소프트 프롬프트로 텍스트 임베딩에 연결되고 동결된 LLM이 비디오 콘텐츠를 조건으로 텍스트를 생성하도록 안내합니다. VisionLanguage Branch 구현에 있어서, 우리는 BLIP-2의 사전 훈련된 비전 구성요소(Li et al., 2023b)를 동결된 시각적 인코더로 활용하는데, 여기에는 EVA-CLIP의 ViTG/14(Fang et al., 2022)와 사전 훈련된 Q-former가 포함된다. 위치 임베딩 계층, 비디오 Qformer, 선형 계층을 포함한 나머지 구성요소는 무작위로 초기화되고 최적화되어 동결된 시각적 인코더의 출력을 동결된 LLM에 잘 연결한다. 2.1.2 오디오-언어 분기 주어진 비디오의 청각적 내용을 처리하기 위해 오디오-언어 분기를 도입한다. 구체적으로, 짧은 원점 오디오 세그먼트가 주어진 경우 피처를 계산하는 사전 훈련된 오디오 인코더, 오디오 세그먼트에 시간 정보를 주입하는 위치 임베딩 계층, 다양한 오디오 세그먼트의 피처를 융합하는 오디오 Q-former, 오디오 표현을 LLM의 임베딩 공간에 매핑하는 선형 계층으로 구성된다. 실제로, 우리는 사전 훈련된 Imagebind(Girdhar et al., 2023)를 오디오 인코더로 활용합니다. 우리는 먼저 비디오에서 2초 길이의 짧은 오디오 클립의 M개 세그먼트를 균일하게 샘플링한 다음, mel-spectrogram bin을 사용하여 각 2초 오디오 클립을 스펙트로그램으로 변환합니다. 입력 오디오의 스펙트로그램 목록을 얻은 후, 오디오 인코더는 각 스펙트로그램을 밀집 벡터로 매핑합니다. 따라서 주어진 비디오의 생성된 오디오 표현은 A = [a1, a2, ..., ·, aм]로 표시할 수 있습니다. Video Q-Former와 유사하게 Audio Q-former는 오디오 세그먼트에 학습 가능한 위치 임베딩을 추가하여 시간 정보를 주입합니다. 그런 다음 위치가 인코딩된 오디오 세그먼트에서 상호 작용을 계산하여 고정 길이의 오디오 기능을 생성합니다. Audio Q-Former는 Q-Former와 동일한 아키텍처를 채택합니다. 가변 길이 오디오 표현 목록 A를 고정 길이 시퀀스 Â ERKada로 투영합니다. 여기서 Ka는 오디오 임베딩 벡터의 수이고 do는 각 벡터의 차원입니다. 마지막으로 선형 계층을 사용하여 오디오 기능을 LLM의 임베딩 공간에 매핑합니다. 2. 다중 분기 교차 모달 훈련 시각 언어 및 오디오 언어 분기를 별도로 훈련합니다. 첫 번째 단계에서는 대규모 시각 캡션 데이터 세트를 사용하여 훈련을 수행하고 두 번째 단계에서는 고품질 지침 따르기 데이터 세트를 사용하여 미세 조정합니다. 이미지는 1프레임 비디오로 처리됩니다. 2.2.1 시각 언어 분기 훈련 시각 언어 분기를 사전 훈련하기 위해 스톡 영상 사이트에서 얻은 텍스트 설명이 있는 짧은 비디오의 대규모 데이터 세트인 Webvid-2M(Bain et al., 2021)을 활용했습니다. 또한 CC3M(Sharma et al., 2018)에서 소스화하고 Liu et al.(2023)에서 필터링한 이미지 캡션 데이터 세트 CC595k를 사용했습니다. 사전 훈련 단계에서 비디오-텍스트 생성 작업을 채택하여 비디오 표현이 주어지면 동결된 LLM이 해당 텍스트 설명을 생성하도록 합니다. 텍스트 설명의 상당 부분이 비디오의 전체 내용을 반영하기에 부족하다는 것을 알게 되었습니다. 따라서 비디오의 시각적 의미는 비디오 설명의 텍스트 의미와 완전히 일치하지 않습니다. 그럼에도 불구하고 이 단계는 방대한 양의 데이터를 활용하고 비디오 특징이 가능한 한 많은 시각적 지식을 포함하도록 하는 것을 목표로 했습니다. 다음 단계에서는 시각-텍스트 정렬 및 지시 따르기 기능을 남겨 두었습니다. 사전 훈련 단계 후에 모델은 비디오의 정보에 대한 내용을 생성할 수 있지만 지시를 따르는 능력이 감소했습니다. 따라서 두 번째 단계에서는 고품질 지시 데이터를 사용하여 모델을 미세 조정했습니다. 우리는 MiniGPT4(Zhu et al., 2023)의 이미지-세부-설명 데이터 세트, LLAVA(Liu et al., 2023)의 이미지-지시 데이터 세트, 그리고 Video-Chat(Li et al., 2023c)의 비디오-지시 데이터 세트를 통합했습니다. 미세 조정 후, Video-LLAMA는 지시를 따르고 이미지와 비디오를 이해하는 데 놀라운 능력을 보였습니다. 2.2.2 오디오-언어 분기의 훈련 오디오-텍스트 데이터를 직접 사용하여 오디오-언어 분기를 훈련하는 것은 그러한 데이터가 희귀하기 때문에 매우 어렵습니다. 오디오-언어 분기의 학습 가능한 매개변수의 목적은 동결된 오디오 인코더의 출력 임베딩을 LLM의 임베딩 공간과 맞추는 것입니다. 오디오-텍스트 데이터의 희소성을 감안할 때, 우리는 이 목표를 달성하기 위해 해결 방법을 사용합니다. 오디오 인코더로 사용되는 ImageBind는 다양한 모달리티의 임베딩을 하나의 공통 공간에 정렬하는 놀라운 능력을 가지고 있어 크로스 모달 검색 및 생성 작업에서 인상적인 성능을 보여줍니다. 오디오 텍스트 데이터의 희소성과 시각적 텍스트 데이터의 풍부함을 감안하여, 우리는 시각 브랜치와 동일한 데이터와 프로세스를 따라 시각적 텍스트 데이터를 사용하여 오디오 언어 브랜치를 훈련합니다. ImageBind가 제공하는 공유 임베딩 공간 덕분에 Video-LLAMA는 오디오 인터페이스가 오디오 데이터로 훈련된 적이 없더라도 추론 중에 오디오를 이해하는 능력을 보여줍니다. 3
--- METHOD ---
Video-LLAMA는 동결된 LLM이 비디오에서 시각적 및 청각적 콘텐츠를 모두 이해할 수 있는 능력을 갖추도록 하는 것을 목표로 합니다. 그림 1에서 볼 수 있듯이, 우리는 각각 비디오 프레임과 오디오 신호를 LLM의 텍스트 입력과 호환되는 쿼리 표현으로 변환하기 위해 Vision-Language Branch와 Audio-Language Branch라는 두 가지 분기를 설계합니다. 이 섹션에서는 먼저 각 분기의 전체 아키텍처와 구성 요소를 소개합니다. 그런 다음 제안된 다중 분기 교차 모달 사전 학습 및 시청각 교육 튜닝의 절차를 설명합니다. Shttps://github.com/DAMO-NLP-SG/Video-LLAMA https://huggingface.co/spaces/DAMO-NLP-SG/ Video LLAMA https://modelscope.cn/studios/damo/ video-11ama/summary Vision-Language Branch 이 비디오는 밤에 발사대에서 발사되는 로켓의 애니메이션입니다... LLM(비쿠냐/LLAMA) 인간: 비디오 Q-Former 선형 &#39;이 비디오를 설명해 주세요: 오디오 언어 분기 선형 오디오 Q-Former to to t to t to tq to t 시각적 인코더(VIT &amp; Q-Former) 오디오 인코더 비디오 프레임 오디오 신호 2.1 아키텍처 2.1.그림 1: Video-LLaMA의 전체 아키텍처. Vision-Language Branch Vision-Language Branch는 LLM이 시각적 입력을 이해할 수 있도록 설계되었습니다. 그림 1의 왼쪽 부분에 표시된 것처럼 비디오 프레임에서 특징을 추출하기 위한 동결된 사전 학습된 이미지 인코더, 비디오 프레임에 시간 정보를 주입하기 위한 위치 임베딩 계층, 프레임 수준 표현을 집계하기 위한 비디오 Q-former, 출력 비디오 표현을 LLM의 텍스트 임베딩과 동일한 차원으로 투영하기 위한 선형 계층으로 구성됩니다. 하나의 비디오가 N개의 프레임으로 구성되어 있다고 가정하면 이미지 인코더는 먼저 각 프레임/이미지를 Kƒ 이미지 임베딩 벡터로 매핑하여 비디오 프레임 표현 V [V1, V2, VN]을 생성합니다. 여기서 vi Є RKƒxdf는 i번째 프레임에 해당하는 df 차원 이미지 임베딩 집합입니다. = 동결된 이미지 인코더의 프레임 표현 vi는 시간 정보를 고려하지 않고 계산되므로 다른 프레임의 표현에 시간 정보의 지표로 위치 임베딩을 추가로 적용합니다. 그런 다음, BLIP-2(Li et al., 2023b)의 Query Transformer(QFormer)와 동일한 아키텍처를 공유하는 Video Q-former에 위치 인코딩된 프레임 표현을 공급하여 차원 d₁의 ky 비디오 임베딩 벡터를 비디오의 표현 ✩ Є Rkydy로 얻습니다. E 비디오 표현을 LLM의 입력에 맞게 조정하기 위해 선형 계층을 추가하여 비디오 임베딩 벡터를 비디오 쿼리 벡터로 변환합니다. 비디오 쿼리 벡터는 LLM의 텍스트 임베딩과 동일한 차원입니다. 순방향 패스에서 비디오 소프트 프롬프트로 텍스트 임베딩에 연결되고 동결된 LLM이 비디오 콘텐츠를 조건으로 하는 텍스트를 생성하도록 안내합니다. VisionLanguage Branch 구현에 관해서는 BLIP-2의 사전 훈련된 비전 구성 요소(Li et al., 2023b)를 동결된 시각적 인코더로 활용합니다. 여기에는 EVA-CLIP의 ViTG/14(Fang et al., 2022)와 사전 훈련된 Q-former가 포함됩니다. 위치 임베딩 계층, 비디오 Qformer 및 선형 계층을 포함한 나머지 구성 요소는 무작위로 초기화되고 최적화되어 동결된 시각적 인코더의 출력을 동결된 LLM에 잘 연결합니다. 2.1.2 오디오-언어 분기 주어진 비디오의 청각적 콘텐츠를 처리하기 위해 오디오-언어 분기를 소개합니다. 구체적으로는 짧은 원점 오디오 세그먼트가 주어진 경우 피처를 계산하는 사전 훈련된 오디오 인코더, 오디오 세그먼트에 시간 정보를 주입하는 위치 임베딩 계층, 다양한 오디오 세그먼트의 피처를 융합하는 오디오 Q-former, 오디오 표현을 LLM의 임베딩 공간에 매핑하는 선형 계층으로 구성됩니다. 실제로, 우리는 사전 훈련된 Imagebind(Girdhar et al., 2023)를 오디오 인코더로 활용합니다. 우리는 먼저 비디오에서 2초 길이의 짧은 오디오 클립의 M개 세그먼트를 균일하게 샘플링한 다음, mel-spectrogram bin을 사용하여 각 2초 오디오 클립을 스펙트로그램으로 변환합니다. 입력 오디오의 스펙트로그램 목록을 얻은 후, 오디오 인코더는 각 스펙트로그램을 밀집 벡터로 매핑합니다. 따라서 주어진 비디오의 생성된 오디오 표현은 A = [a1, a2, ..., ·, aм]로 표시할 수 있습니다. Video Q-Former와 유사하게 Audio Q-former는 오디오 세그먼트에 학습 가능한 위치 임베딩을 추가하여 시간 정보를 주입합니다. 그런 다음 위치가 인코딩된 오디오 세그먼트에서 상호 작용을 계산하여 고정 길이의 오디오 기능을 생성합니다. Audio Q-Former는 Q-Former와 동일한 아키텍처를 채택합니다. 가변 길이 오디오 표현 목록 A를 고정 길이 시퀀스 Â ERKada로 투영합니다. 여기서 Ka는 오디오 임베딩 벡터의 수이고 do는 각 벡터의 차원입니다. 마지막으로 선형 계층을 사용하여 오디오 기능을 LLM의 임베딩 공간에 매핑합니다. 2. 다중 분기 교차 모달 훈련 시각 언어 및 오디오 언어 분기를 별도로 훈련합니다. 첫 번째 단계에서는 대규모 시각 캡션 데이터 세트를 사용하여 훈련을 수행하고 두 번째 단계에서는 고품질 지침 따르기 데이터 세트를 사용하여 미세 조정합니다. 이미지는 1프레임 비디오로 처리됩니다. 2.2.1 시각 언어 분기 훈련 시각 언어 분기를 사전 훈련하기 위해 스톡 영상 사이트에서 얻은 텍스트 설명이 있는 짧은 비디오의 대규모 데이터 세트인 Webvid-2M(Bain et al., 2021)을 활용했습니다. 또한 CC3M(Sharma et al., 2018)에서 소스화하고 Liu et al.(2023)에서 필터링한 이미지 캡션 데이터 세트 CC595k를 사용했습니다. 사전 훈련 단계에서 비디오-텍스트 생성 작업을 채택하여 비디오 표현이 주어지면 동결된 LLM이 해당 텍스트 설명을 생성하도록 합니다. 텍스트 설명의 상당 부분이 비디오의 전체 내용을 반영하기에 부족하다는 것을 알게 되었습니다. 따라서 비디오의 시각적 의미는 비디오 설명의 텍스트 의미와 완전히 일치하지 않습니다. 그럼에도 불구하고 이 단계는 방대한 양의 데이터를 활용하고 비디오 특징이 가능한 한 많은 시각적 지식을 포함하도록 하는 것을 목표로 했습니다. 다음 단계에서는 시각-텍스트 정렬 및 지시 따르기 기능을 남겨 두었습니다. 사전 훈련 단계 후에 모델은 비디오의 정보에 대한 내용을 생성할 수 있지만 지시를 따르는 능력이 감소했습니다. 따라서 두 번째 단계에서는 고품질 지시 데이터를 사용하여 모델을 미세 조정했습니다. 우리는 MiniGPT4(Zhu et al., 2023)의 이미지-세부-설명 데이터 세트, LLAVA(Liu et al., 2023)의 이미지-지시 데이터 세트, 그리고 Video-Chat(Li et al., 2023c)의 비디오-지시 데이터 세트를 통합했습니다. 미세 조정 후, Video-LLAMA는 지시를 따르고 이미지와 비디오를 이해하는 데 놀라운 능력을 보였습니다. 2.2.2 오디오-언어 분기의 훈련 오디오-텍스트 데이터를 직접 사용하여 오디오-언어 분기를 훈련하는 것은 그러한 데이터가 희귀하기 때문에 매우 어렵습니다. 오디오-언어 분기의 학습 가능한 매개변수의 목적은 동결된 오디오 인코더의 출력 임베딩을 LLM의 임베딩 공간과 맞추는 것입니다. 오디오-텍스트 데이터의 희소성을 감안할 때, 우리는 이 목표를 달성하기 위해 해결 방법을 사용합니다. 오디오 인코더로 사용되는 ImageBind는 다양한 모달리티의 임베딩을 하나의 공통 공간에 정렬하는 놀라운 능력을 가지고 있어 교차 모달 검색 및 생성 작업에서 인상적인 성능을 보여줍니다. 오디오 텍스트 데이터의 부족과 시각적 텍스트 데이터의 풍부함을 고려하여 시각 브랜치와 동일한 데이터와 프로세스를 따라 시각적 텍스트 데이터를 사용하여 오디오 언어 브랜치를 훈련합니다. ImageBind가 제공하는 공유 임베딩 공간 덕분에 Video-LLAMA는 오디오 인터페이스가 오디오 데이터로 훈련되지 않았더라도 추론 중에 오디오를 이해하는 능력을 보여줍니다. 3 관련 연구 대규모 언어 모델: 대규모 언어 모델(LLM)(Black et al., 2022; Scao et al., 2022; OpenAI, 2023; Tsimpoukelli et al., 2021)은 놀라운 언어 이해 및 추론 능력을 보여주어 기사, 대화, 스토리, 시를 포함한 다양한 도메인에서 고품질 자연어 텍스트를 생성할 수 있었습니다. LLM은 이미 기술 혁명을 일으켰으며 다양한 응용 분야에 널리 적용되었습니다. 게다가 LLAMA(Touvron 등, 2023), BLOOM(Scao 등, 2022) 및 OPT(Zhang 등, 2022)와 같은 일련의 오픈 소스 대형 모델은 기술 발전을 크게 촉진하고 NLP 커뮤니티에 뛰어난 기여를 했습니다. 이러한 LLM을 기반으로 연구자들은 역량을 더욱 확장하고 다양한 NLP 작업에 대한 뛰어난 모델을 개발했습니다. 예로는 Vicuna(Chiang 등, 2023) 및 Baize(Xu 등, 2023a)가 있습니다. 저희의 작업은 이러한 LLM을 기반으로 하며 비디오의 시각적 및 청각적 콘텐츠를 모두 이해할 수 있는 기능을 제공하는 플러그 앤 플레이 플러그인을 제공합니다. 다중 모달 대규모 언어 모델: 연구자들은 다중 모달 입력을 처리하기 위해 LLM을 사용하는 방법을 적극적으로 탐구해 왔습니다(Gao et al., 2023; Li et al., 2023c). 기존 접근 방식은 두 가지 주요 그룹으로 분류할 수 있습니다. 첫 번째 범주는 LLM을 컨트롤러로 사용하고 기존 다중 모달 모델을 도구로 활용하는 것입니다. 이 접근 방식에서 LLM은 사용자의 텍스트 지침을 받을 때 사용자의 의도를 인식하고 어떤 도구를 호출할지 결정합니다. 그런 다음 이러한 기성형 다중 모달 모델에서 얻은 결과를 통합하여 포괄적인 응답을 생성합니다. 예로는 ChatGPT(Wu et al., 2023a), HuggingGPT(Shen et al., 2023), AudioGPT(Huang et al., 2023a)가 있습니다. 두 번째 범주는 기본적인 대규모 다중 모달 모델을 훈련하는 데 중점을 둡니다. 이 작업 라인의 핵심 아이디어는 다른 모달리티에 대한 사전 훈련된 기초 모델을 텍스트 LLM에 맞추는 것입니다. 예를 들어, Flamingo(Alayrac et al., 2022a)는 인식자 리샘플러와 게이트 교차 주의 계층을 활용하여 동결된 이미지 인코더와 LLM을 연결합니다. BLIP2(Li et al., 2023b)는 학습된 이미지 쿼리를 LLM의 텍스트 임베딩 공간에 매핑하기 위해 Q-Former를 도입합니다. (Liu et al., 2023), mPLUG-owl(Ye et al., 2023) 및 MiniGPT4(Zhu et al., 2023)는 이미지 지시 추종 데이터 세트를 사용하여 지시 추종 이미지 LLM을 개발합니다. Video-Chat(Li et al., 2023c)과 Video-ChatGPT(Maaz et al., 2023)는 이미지 인코더를 비디오 인코더로 확장하고 이를 LLM과 연결하여 비디오의 시각적 콘텐츠를 이해합니다.PandaGPT(Su et al., 2023)는 이미지-지시 쌍에 대해서만 학습된 ImageBind의 다중 모달 인코더를 활용하여 대규모 모델이 6가지 모달리티를 이해할 수 있도록 합니다.저희의 작업은 두 번째 범주에 속하며, 기본 모델을 학습하여 비디오의 시각적 및 청각적 콘텐츠를 모두 이해합니다.4가지 예 이 섹션에서는 비디오/오디오/이미지 기반 대화에서 Video-LLAMA의 다중 모달 지시 수행 기능을 보여주는 몇 가지 사례를 보여줍니다.그림 2는 이러한 몇 가지 예를 보여주고 있으며, 부록 A에서 추가 사례를 찾을 수 있습니다.(1) 시청각 통합 인식 능력.그림 2(a)와 그림 3은 청각 및 시각 정보를 동시에 이해하는 Video-LLAMA의 고유한 능력을 보여줍니다. 두 경우 모두 비디오에 오디오가 포함되어 있습니다. 각 대화에서 시각적 및 청각적 내용과 관련된 두 가지 질문을 제기합니다. 모델이 하나의 모달만 수신할 수 있다면 두 질문에 모두 답할 수 없습니다. 그러나 VideoLLAMA가 두 경우 모두 시각적 및 청각적 질문에 정확하게 응답하는 것을 볼 수 있습니다. (2) 비디오에서 시간적 역동성을 포착하는 능력. 그림 2(b)와 그림 4는 Video-LLAMA가 시간 경과에 따른 동작을 식별하는 기능을 보여줍니다. 소녀의 동작과 보트의 이동 방향을 성공적으로 설명합니다. (3) 정적 이미지를 인식하고 이해하는 능력. 그림 2(c)와 그림 5는 VideoLLAMA가 그림을 인식하고 이해하는 능력을 보여줍니다. 그림 2(c)는 Video-LLAMA가 &quot;비정상적&quot;이라는 개념을 이해하고 특히 비정상적인 장면을 설명하는 능력을 보여줍니다. 그림 5에서 Video-LLAMA는 다음을 정확하게 설명할 뿐만 아니라 다음을 설명합니다. 들리는 것을 설명하세요? 배경에서 바닥에서 발소리가 들리고 집에서 개가 짖는 소리도 있습니다. 남자가 안경을 쓰고 있나요? 네, 그는 비디오에서 안경을 쓰고 있습니다. 이 비디오를 한 문장으로 요약하세요. 이 비디오는 물 위에 있는 보트를 따라 흐르는 벚꽃이 늘어선 강의 아름다운 풍경과 배경에 높은 빌딩이 있는 도시 풍경을 보여줍니다. 배는 어느 방향으로 가고 있습니까? 배는 비디오의 오른쪽으로 가고 있습니다. (a) Video-LLAMA가 비디오의 배경 사운드와 시각적 콘텐츠를 기반으로 질문에 답하는 사례 (b) Video-LLAMA가 시간에 따른 동작을 식별하는 능력을 보여주는 사례. 이 이미지에서 특이한 점은 무엇입니까? 이 이미지에서 특이한 점은 노란색 셔츠를 입은 남자가 다리미판을 들고 옷을 다림질한 채 주차된 차 위에 서 있다는 것입니다. 이 사진 속 건물을 아십니까? 이미지 속 건물은 미국 국회의사당으로 보입니다. 미국 수도인 워싱턴 D.C.에 위치한 신고전주의 양식의 건물입니다. 미국 의회의 본거지이며 미국에서 가장 상징적인 랜드마크 중 하나입니다. (c) Video-LLAMA가 정적 이미지를 이해하는 능력을 보여주는 사례. (d) Video-LLAMA가 유명 랜드마크를 인식하는 능력을 보여주는 사례. 그림 2: Video-LLAMA에서 생성한 몇 가지 예. 주요 콘텐츠이지만 개와 사람 사이의 친근한 상호 작용과도 연관시킵니다. (4) 공통 지식 개념 인식 능력. 그림 2(d)와 그림 6은 Video-LLaMA가 시각 신호에서 공통 지식 개념을 인식하는 놀라운 능력을 보여줍니다. Video-LLaMA는 유명 랜드마크와 캐릭터를 성공적으로 인식하고 상식적인 질문에 답할 수 있습니다. 5 결론 이 논문에서는 오디오 및 비디오 이해 기능을 모두 갖춘 대규모 언어 모델을 강화하는 최첨단 멀티모달 프레임워크인 Video-LLAMA를 제시합니다.
--- CONCLUSION ---
이 논문에서는 오디오 및 비디오 이해 기능을 모두 갖춘 대규모 언어 모델을 강화하는 최첨단 멀티모달 프레임워크인 Video-LLAMA를 제시합니다. 저희의 실험은 오디오 및 비디오 기반 대화에서 Video-LLAMA의 인상적인 능력을 보여주었으며, 오디오-비주얼 AI 보조원을 위한 유망한 프로토타입으로서의 잠재력을 강조했습니다. 저희는 개발자가 추가 개발을 위해 코드를 활용하는 데 도움이 되는 자세한 지침과 함께 전체 교육 코드와 다양한 모델 가중치를 오픈 소스로 공개했습니다. 또한 사용자가 Video-LLAMA의 기능을 직접 경험할 수 있도록 온라인 데모 웹사이트와 오프라인 데모 배포 가이드를 제공했습니다. 저희는 Video-LLAMA를 지속적으로 유지 관리하고 개선하는 데 전념하며 오픈 소스 커뮤니티에 계속 기여할 것입니다. 6 제한 사항 Video-LLaMA는 비디오에서 시각적 및 청각적 콘텐츠를 모두 이해하는 데 인상적인 능력을 보여주었지만 아직 초기 단계의 프로토타입이며 다음을 포함한 몇 가지 제한 사항이 있습니다. (1) 제한된 지각 능력: Video-LLAMA의 성능은 현재 교육 데이터 세트의 품질과 규모에 의해 방해를 받습니다. 우리는 모델의 인식 능력을 향상시키기 위해 고품질 오디오-비디오-텍스트 정렬 데이터 세트를 적극적으로 구축하고 있습니다. (2) 긴 비디오를 처리하는 제한된 능력. 긴 비디오(영화, TV 쇼 등)는 방대한 양의 정보를 포함하고 있으며 계산 리소스에 더 많은 요구를 합니다. 이 과제는 연구 커뮤니티가 적극적으로 해결하기 위해 노력하고 있는 중요한 문제로 남아 있습니다. (3) 환각. Video-LLaMA는 동결된 LLM에서 환각 문제를 물려받았습니다. 우리는 이러한 과제를 계속 해결하고 비디오 이해를 위한 더 강력한 버전을 개발할 것입니다. 참고문헌 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022a. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman 및 Karen Simonyan. 2022b. Flamingo: 퓨샷 학습을 위한 시각적 언어 모델입니다. arXiv 사전 인쇄 arXiv:2204.14198. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv 사전 인쇄본 arXiv:2212.08073. Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoding for end-to-end retrieval. IEEE International Conference on Computer Vision에서. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: 오픈소스 자기 회귀 언어 모델. arXiv 사전 인쇄본 arXiv:2204.06745. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang 및 Yue Cao. 2022. Eva: 대규모로 마스크된 시각적 표현 학습의 한계를 탐색합니다. arXiv 사전 인쇄 arXiv:2211.07636. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li 및 Yu Jiao Qiao. 2023. Llama-adapter v2: 매개변수 효율적인 시각적 교육 모델. arXiv 사전 인쇄 arXiv:2304.15010. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin 및 Ishan Misra. 2023. Imagebind: 모든 것을 묶을 하나의 임베딩 공간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 15180-15190페이지. Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2023a. Audiogpt: 음성, 음악, 소리 및 말하는 머리 이해 및 생성. arXiv 사전 인쇄본 arXiv:2304.12995. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023b. 언어만이 필요한 전부는 아니다: 언어 모델에 대한 인식 조정. arXiv 사전 인쇄본 arXiv:2302.14045. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: 맥락 내 지시 조정을 갖춘 다중 모달 모델. arXiv 사전 인쇄본 arXiv:2305.03726. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: 고정 이미지 인코더 및 대규모 언어 모델을 사용하여 언어-이미지 사전 훈련을 부트스트랩합니다. arXiv 사전 인쇄 arXiv:2301.12597. Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang 및 Yu Qiao. 2023c. 영상채팅: 채팅 중심의 영상 이해. arXiv 사전 인쇄 arXiv:2305.06355. Haotian Liu, Chunyuan Li, Qingyang Wu, 그리고 이용재. 2023. 시각적 지시 조정. arXiv 사전 인쇄 arXiv:2304.08485. Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-Hui Qiu, Pengcheng Lu, Tao Wang 및 Zhongyu Wei. 2023. 밸리(Valley): 대규모 언어 모델 강화 능력을 갖춘 비디오 어시스턴트. arXiv 사전 인쇄 arXiv:2306.07207. 무하마드 마즈(Muhammad Maaz), 하누나 라시드(Hanoona Rasheed), 살만 칸(Salman Khan), 파하드 샤바즈 칸(Fahad Shahbaz Khan). 2023. Video-chatgpt: 대규모 비전 및 언어 모델을 통해 상세한 비디오 이해를 지향합니다. arXiv 사전 인쇄 arXiv:2306.05424. 오픈AI. 2023. Gpt-4 기술 보고서. arXiv 사전 인쇄 arXiv:2303.08774. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé 등 2022. Bloom: 176b 매개변수 오픈 액세스 다국어 언어 모델. arXiv 사전 인쇄 arXiv:2211.05100. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 2018. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. Association for Computational Linguistics(제1권: 장문 논문)의 제56회 연례 회의록, 2556-2565쪽. Association for Computational Linguistics. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. 2023. Hugginggpt: huggingface에서 chatgpt와 그 친구들을 사용하여 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, Deng Cai. 2023. Pandagpt: 모든 것을 지시하고 따르는 하나의 모델. arXiv 사전 인쇄본 arXiv:2305.16355. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang. 2023. 다중 모달리티에서의 생성적 사전 학습. arXiv 사전 인쇄본 arXiv:2307.05222. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, Felix Hill. 2021. 동결된 언어 모델을 사용한 다중 모달 few-shot 학습. 신경 정보 처리 시스템의 발전, 34:200-212. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang. 2022. 간단한 시퀀스-투-시퀀스 학습 프레임워크를 통해 아키텍처, 작업 및 모달리티 통합. 기계 학습 국제 컨퍼런스에서. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan. 2023a. Visual chatgpt: Visual Foundation 모델을 사용한 대화, 그림 및 편집. arXiv 사전 인쇄본 arXiv:2303.04671. Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yilun Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu 및 Yu Wu. 2023b. 음성-텍스트 및 대규모 언어 모델 통합을 위한 디코더 전용 아키텍처입니다. arXiv 사전 인쇄 arXiv:2307.03917. Canwen Xu, Daya Guo, Nan Duan 및 Julian McAuley. 2023a. Baize: 셀프 채팅 데이터에 대한 매개변수 효율적인 조정 기능을 갖춘 오픈 소스 채팅 모델입니다. arXiv 사전 인쇄 arXiv:2304.01196. Haiyang Xu, Qinghao Ye, Mingshi Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qiuchen Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Feiran Huang 및 Jingren Zhou. 2023b. mplug-2: 텍스트, 이미지, 비디오 전반에 걸쳐 모듈화된 다중 모드 기반 모델입니다. arXiv 사전 인쇄 arXiv:2302.00402. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang 및 Feiyan Huang. 2023. mplug-owl: 모듈화는 다중 양식을 갖춘 대규모 언어 모델을 강화합니다. arXiv 사전 인쇄 arXiv:2304.14178. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu 및 Enhong Chen. 2023. 다중 모드 대형 언어 모델에 대한 조사. arXiv 사전 인쇄 arXiv:2306.13549. Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu 및 Tat-Seng Chua. 2023a. llms를 통해 시각적 프롬프트 생성기를 전송합니다. arXiv 사전 인쇄 arXiv:23045.01278. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Peng Peng Wang, Yaqian Zhou 및 Xipeng Qiu. 2023b. Speechgpt: 본질적인 교차 모달 대화 능력으로 대규모 언어 모델을 강화합니다. arXiv 사전 인쇄본 arXiv:2305.11000. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: 사전 훈련된 트랜스포머 언어 모델 열기. arXiv 사전 인쇄본 arXiv:2205.01068. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: 고급 대규모 언어 모델을 사용한 시각 언어 이해 향상. arXiv 사전 인쇄본 arXiv:2304.10592. 부록 청중의 반응은 어땠나요? 청중은 정말 열광적이었고 공연자를 위해 큰 박수를 보냈습니다. 이 남자는 무엇을 하고 있을까요? 그는 콘서트 무대에서 색소폰을 연주하고 있습니다. 그림 3: 비디오에서 박수 소리를 식별하고 청중의 긍정적인 반응을 유추하는 Video-LLAMA의 능력을 보여주는 사례. 또한 시각적 콘텐츠를 기반으로 무대에서 색소폰을 연주하는 남성을 유추합니다. shutterstock 이 비디오를 설명하세요. 이 비디오는 줄무늬 셔츠를 입은 젊은 빨간 머리 여성이 회색 배경 앞에서 카메라에 포즈를 취하고 있는 모습을 보여줍니다. 그녀는 조용히 하려는 듯이 손가락을 입에 대고 있으며, 캡션에는 그녀의 긴 머리와 붉은 입술이 언급되어 있습니다. 이 여성은 젊고 피부가 밝으며 차분하고 자신감이 넘쳐 보입니다. 그녀는 손가락을 입에 대고 있는데, 이는 카메라에 무언가를 신호로 보내거나 메시지를 전달하려는 것일 수 있음을 시사합니다. 그림 4: Video-LLAMA가 동적 비디오에서 시각적 콘텐츠에 대한 자세한 설명을 제공하는 사례. 이미지를 자세히 설명하세요. 이 이미지에는 밝은 녹색 잔디밭에서 달리는 아름답고 장난기 많은 골든 리트리버가 등장합니다. 개는 입에 테니스 공을 물고 달리고 있는데, 이 개는 놀고 즐기는 활동적이고 활기찬 동물이기 때문입니다. 배경의 푸른 잔디는 개 털의 황금색과 생생한 대비를 이루며, 장면에 색상과 질감을 더합니다. 개의 털은 잘 정돈되어 있고, 윤기가 나고, 잘 정돈되어 있어 건강과 외모를 돋보이게 합니다. 푸른 잔디의 밝기와 개의 장난기 어린 행동은 즐거운 분위기를 암시하며, 시청자가 이 아름다운 풍경과 개의 활기찬 놀이에 참여하도록 유도합니다. 개가 공을 가지고 놀 때 집중하고 헌신하는 모습은 개의 본능과 사냥 능력을 잘 보여줍니다. 또한 이 이미지는 개와 인간 친구 사이의 즐겁고 따뜻한 유대감의 순간을 포착했는데, 개가 공에 대해 긍정적인 태도를 보이는 것은 주인과 주변 환경과 상호 작용하는 것을 즐기는 모습을 반영하기 때문입니다. 전반적으로 이 이미지는 매력적이고 즐거운 분위기를 전달합니다. 그림 5: Video-LLaMA가 정적 이미지 콘텐츠에 대한 자세한 설명을 제공하는 사례. 오른쪽에 있는 남자가 누구인지 아십니까? 오른쪽에 있는 남자는 TV 시리즈 Game of Thrones에서 Kit Harington이 연기한 Jon Snow입니다.왼쪽에 있는 여자는 누구일까요?왼쪽에 있는 여자는 TV 시리즈 Game of Thrones에서 Emilia Clarke가 연기한 Daenerys Targaryen일 가능성이 큽니다.두 사람의 관계는 무엇일까요?Jon Snow와 Daenerys Targaryen은 모두 TV 시리즈 Game of Thrones에 등장하는 캐릭터입니다.이 드라마에서 두 사람은 로맨틱한 관계를 맺습니다.두 사람은 7번째 시즌에 처음 만나고, 8번째 시즌에 들어서면서 서로에 대한 매력이 커집니다.처음에는 호감을 느꼈지만, 각자의 배경과 시리즈에서 직면한 어려움 때문에 복잡한 관계를 맺게 됩니다.그림 6: Video-LLAMA가 유명 캐릭터를 알아보고 비디오 기반 질의응답에 참여하는 능력을 보여주는 사례
