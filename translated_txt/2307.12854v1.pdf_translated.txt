--- ABSTRACT ---
장기 활동 예측은 관찰된 행동 간의 시간적 관계와 인간 활동의 변동성과 복잡성을 이해해야 하기 때문에 특히 어려운 연구 문제입니다. 값비싼 인간 주석을 통한 강력한 감독에 의존함에도 불구하고 최첨단 예측 접근 방식은 종종 보이지 않는 데이터로 일반화되지 않습니다. 이 문제를 완화하기 위해 다중 스케일 비디오 사전 학습(MVP)을 제안합니다. 이는 여러 시간 척도에 걸쳐 미래 비디오 클립의 맥락화된 표현을 예측하는 방법을 학습하여 예측을 위한 견고한 표현을 학습하는 새로운 자기 감독 사전 학습 접근 방식입니다. MVP는 비디오의 행동이 다중 스케일 특성을 가지고 있다는 관찰에 기반을 두고 있으며, 원자 행동은 일반적으로 짧은 시간 척도에서 발생하고 더 복잡한 행동은 더 긴 시간 척도에 걸쳐 발생할 수 있습니다. 장기 행동 예상 및 비디오 요약 예측을 포함한 다운스트림 장기 예측 작업에서 MVP를 최첨단 자기 감독 비디오 학습 접근 방식과 비교합니다. Ego4D와 Epic-Kitchens-55/100 데이터 세트에 대한 포괄적인 실험은 MVP가 최첨단 방법을 상당한 마진으로 능가한다는 것을 보여줍니다. 특히, MVP는 기존 방법에 비해 비디오 요약 예측에서 20% 이상의 상대적 성능 이득을 얻습니다. 1.
--- INTRODUCTION ---
인간 활동에 대한 장기 예측(그림 1에 설명)은 지능적이고 협업적인 기계를 개발하는 데 필수적인 핵심 역량입니다. 몇 가지 관찰을 바탕으로 미래의 행동에 대해 추론하는 기계는 그에 따라 자신의 행동을 계획하고 동적 환경에서 다른 에이전트와 보다 효과적으로 상호 작용할 수 있습니다. 그러나 미래의 행동을 예측하는 것은 본질적으로 어렵습니다. 시작하려면 모델이 부분적 관찰 가능성 하에서 환경의 현재 상태를 이해해야 합니다. 더 중요한 것은 미래의 비결정적 특성이 다운스트림 장기 예측 작업을 복잡하게 만든다는 것입니다. 부분적으로 관찰된 비디오 입력 미래의 행동을 예측하세요!! 올바른 요약 검색 ☑ Х 요약 3: C는 반죽을 반죽하고 피자를 만듭니다. III 보드 커버 손잡이를 수리하고 닫고 조입니다. 행동 예측 요약 1: C 요약 2: C는 회로를 제거하고 회로를 돌리고 스위치를 켭니다. 도구를 치웁니다. 비디오 요약 예측 -L 그림 1: 장기 활동 예측 작업. 비디오 모델을 사전 학습하고 학습된 표현을 장기 행동 및 비디오 요약 예측으로 전송합니다. &#39;C&#39;는 요약에서 카메라 착용자를 나타냅니다. 시간이 지남에 따라 관찰된 동작과 객체 간의 관계를 추론하고 이러한 관계가 미래에 어떻게 진화할지 예측해야 하는 어려움. 최신 장기 예측 방법(예: [16, 17])은 완전한 주의 모델을 활용하여 비디오에서 장기 시간적 역학을 모델링하기 위한 보다 효과적인 기능을 학습하는 데 중점을 두었지만 [29] 여전히 동작 인식을 위해 개발된 표준 목표를 사용하여 학습된 사전 학습된 시각적 표현에 의존합니다. 그러나 이 목표는 종종 비디오 모델이 비디오의 장기 상호 작용과 역학을 포착하는 대신 짧은 클립의 단기 종속성만 이해하도록 장려합니다. 이는 사전 학습된 시각적 표현의 장기 예측 작업으로의 일반화 가능성을 제한할 수 있습니다. 인간이 주석을 단 동작 레이블의 강력한 학습 감독에 의존함에도 불구하고 위에서 언급한 접근 방식은 여전히 보이지 않는 데이터로 일반화하는 데 적합하지 않습니다. [16] 이는 이론을 뒷받침합니다. 장기 예측을 위한 사전 학습을 개선하기 위해, 우리는 먼저 비디오가 일반적으로 다중 스케일 특성을 가지고 있으며, 액션이 서로 다른 세분성 수준에서 발생하는 하위 액션으로 분해될 수 있다는 것을 관찰했습니다.비디오 인코더 비디오 클립 쌍 사전 학습(이전 작업) 비디오 유사성 극대화 인코더 문맥화된 미래 표현 예측 헤드 비디오 인코더 관찰된 비디오 클립 유사성 극대화 비디오 인코더 다중 스케일 비디오 사전 학습(저희) 비디오 인코더 비디오 인코더 비디오 인코더 유사성 극대화 여러 시간 스케일에 걸친 미래 비디오 클립 그림 2: 다중 스케일 비디오 사전 학습(MVP). 동일한 비디오의 클립 표현 간 유사성을 극대화하는 이전의 자체 감독 방법[24, 13]과 달리 MVP는 다양한 시간 스케일에 걸쳐 미래 문맥 정보를 예측하는 모델을 학습시켜 장기 예측 작업에 더 잘 일반화하는 데 도움이 됩니다.식사. 가장 높은 수준의 추상화에서 오믈렛을 만드는 복잡한 동작은 일반적으로 계란 깨기와 기름 넣기와 같이 더 짧은 시간 척도에서 발생하는 여러 동작으로 구성됩니다. 이 구조를 이해하는 법을 배우는 것이 관련된 에이전트의 기본 목표와 의도를 유추하는 데 중요할 수 있으며, 따라서 후속 동작에 대한 보다 정확한 예측을 용이하게 할 수 있다고 가정합니다. 사전 학습 중에 동작의 다중 스케일 특성을 학습된 비디오 표현으로 자체 감독 방식으로 인코딩하려고 노력하며, 이는 다운스트림 장기 예측 작업으로 보다 효과적으로 일반화될 것입니다. 이를 위해 부분적으로 관찰된 비디오 시퀀스의 정보를 사용하여 다양한 시간 척도에 걸쳐 집계된 미래 비디오 클립의 맥락화된 표현을 예측하는 법을 비디오 모델이 학습하도록 장려하는 새로운 다중 스케일 비디오 사전 학습(MVP) 접근 방식(그림 2 참조)을 소개합니다. MVP는 장기 예측 작업에 필요한 기능에서 영감을 얻었으며, 관찰된 행동의 공간적 및 시간적 구조에 대해 추론하고 여러 척도와 시간적 해상도에서 발생하는 미래 이벤트를 예측할 수 있어야 합니다. 사전 학습 중에 MVP는 미래 클립에 포함된 맥락적 정보를 예측하는 데 필요한 관찰된 클립 시퀀스에서 지식을 추론하는 방법을 학습합니다. 자체 감독 공식에 기준 진실 레이블이 없으므로 미래 비디오 클립의 맥락적 표현을 계산하여 예측 대상을 생성합니다. MVP의 이 핵심 측면은 동일한 비디오에서 샘플링한 다른 클립의 표현 간 유사성을 최대화하는 최첨단 비디오 사전 학습 목표와 구별됩니다[24, 13](그림 2 상단). Feichtenhofer 등[13]은 후자의 목표가 동일한 비디오의 다른 클립이 시공간적 차원에서 유사한 표현을 갖도록 장려한다는 것을 보여줍니다. 클립 불변 비디오 표현을 학습하는 것은 단기 액션 인식 작업에 유익할 수 있지만, 관찰된 비디오의 고수준 의미 구조를 인코딩하지는 않습니다. 반면, MVP 학습 목표는 비디오 모델을 훈련하여 관찰된 비디오 시퀀스에서 여러 스케일로 미래 정보를 외삽합니다. 긴 비디오에서 다양한 수준의 세분성으로 다양한 액션 간의 관계를 인식함으로써 비디오 모델은 비디오의 기본 구조를 더 잘 이해하고 다음에 무슨 일이 일어날지 더 정확하게 예측할 수 있습니다. 우리는 사전 훈련된 표현을 순서 무관 및 특정 액션 예측(그림 1)을 포함한 다운스트림 장기 예측 작업으로 전송하여 MVP의 효과를 평가합니다. 또한 비디오 요약 예측의 새로운 멀티모달 작업을 소개합니다. 여기서 목표는 일련의 방해 요소에서 관찰된 활동과 미래 활동의 해당 텍스트 요약을 검색하는 것입니다. MVP는 Ego4D 및 Epic-Kitchens55/100 데이터 세트에서 최첨단 비디오 사전 훈련 접근 방식보다 상당히 우수한 성능을 보입니다. 더 중요한 점은 광범위한 절제 연구를 통해 MVP의 다양한 측면이 기여하는 것에 대한 주요 통찰력을 추출한다는 것입니다. 이는 다중 스케일 비디오 표현을 학습하는 미래의 작업에 유익할 것으로 기대합니다.
--- RELATED WORK ---
자기 감독 비디오 사전 학습. 자기 감독 비디오 사전 학습[13, 18, 31]은 Kinetics-400/600[2, 3, 21], HMDB-51[22] 및 UCF101[28]을 포함한 대상 데이터 세트에서 활동 인식[10, 12, 13, 18, 19, 25, 31], 비디오 객체 분할[20], 조기 행동 예측[27] 및 의도치 않은 행동 감지[18, 19]와 같은 다운스트림 작업의 성능을 개선하는 데 유익한 것으로 입증되었습니다. 이미지 기반 자기 감독 사전 학습 목표[4, 5, 6]에서 영감을 얻은 최첨단 비디오 접근 방식[13, 24, 31, 33]은 종종 동일한 비디오에서 샘플링된 두 클립의 표현 간 유사성을 최대화하는 유사한 학습 목표를 사용합니다. Contrastive Video Representation Learning(CVRL) [24] 접근법은 또한 최적의 성능을 위해 적용된 변환이 모든 프레임에서 일관되어야 함을 보여줍니다.Feichtenhofer et al. [13]은 또한 비디오 클립 불변 표현을 학습하는 이 목표가 클립 쌍을 넘어 확장될 수 있으며, 이를 통해 학습된 표현의 다운스트림 작업인 동작 인식에 대한 견고성이 더욱 향상됨을 보여줍니다.또한 Contrastive Predictive Coding(CPC) [23] 및 Dense Predictive Coding(DPC) [18] 접근법도 정신이 유사하며, 각각 맥락을 위한 관찰된 클립 시퀀스가 주어졌을 때 미래 클립의 거친 클립 수준 및 세밀한 시공간 영역 표현을 예측하는 것입니다.Han et al. [19]은 미래를 예측하는 비결정적 특성을 설명하기 위해 학습 가능한 벡터의 메모리 뱅크를 도입하여 이를 더욱 발전시켰습니다. 그러나 우리의 MVP 접근 방식과 달리, 앞서 언급한 접근 방식은 관찰된 시퀀스 바로 뒤에 오는 미래 클립의 정보를 예측하는 법을 배웁니다. 더 중요한 것은, 미래 비디오 클립의 기본 표현만을 예측하고, 맥락화된 표현은 예측하지 않는다는 것입니다. 맥락화된 표현에서 정보는 인과적 방식으로 모든 이전 미래 클립에 대해 집계되었습니다. 또한, BraVe[25]와 LSTCL[31]은 동일한 비디오의 짧은 클립과 긴 클립 한 쌍 사이의 유사성을 최대화하여 클립 수준 표현에서 장기 시간적 단서를 인코딩하는 법을 학습한다는 유사한 아이디어를 구현합니다. MVP의 다중 스케일 측면은 BraVe와 LSTCL과 구별됩니다. 이러한
--- METHOD ---
s는 상당한 마진으로. 특히 MVP는 기존 방법에 비해 비디오 요약 예측에서 20% 이상의 상대적 성능 이득을 얻습니다. 1. 서론 인간 활동에 대한 장기 예측(그림 1에 설명됨)은 지능적이고 협력적인 기계를 개발하는 데 필수적인 핵심 기능입니다. 몇 가지 관찰을 바탕으로 미래 행동에 대해 추론하는 기계는 그에 따라 자신의 행동을 더 잘 계획하고 동적 환경에서 다른 에이전트와 더 효과적으로 상호 작용할 수 있습니다. 그러나 미래 행동을 예측하는 것은 본질적으로 어렵습니다. 시작하려면 모델이 부분적 관찰 가능성 하에서 환경의 현재 상태를 이해해야 합니다. 더 중요한 것은 미래의 비결정적 특성이 다운스트림 장기 예측 작업을 복잡하게 만든다는 것입니다. 부분적으로 관찰된 비디오 입력 미래 행동 예측!! 올바른 요약 검색 ☑ Х 요약 3: CI는 반죽을 반죽하고 피자를 만듭니다. III 보드 커버 손잡이를 조여 닫고 수리합니다. 작업 예측 요약 1: C 요약 2: C는 회로를 제거하고 회로를 돌리고 스위치를 켭니다. 도구를 치웁니다. 비디오 요약 예측 -L 그림 1: 장기 활동 예측 작업. 비디오 모델을 사전 학습하고 학습된 표현을 장기 액션 및 비디오 요약 예측으로 전환합니다. &#39;C&#39;는 요약에서 카메라 착용자를 나타냅니다. 시간이 지남에 따라 관찰된 액션과 객체 간의 관계를 추론하고 이러한 관계가 미래에 어떻게 진화할지 예측해야 하는 어려움. 최신 장기 예측 방법(예: [16, 17])은 완전한 주의 모델[29]을 활용하여 비디오에서 장기 시간적 역학을 모델링하기 위한 보다 효과적인 기능을 학습하는 데 중점을 두었지만 여전히 액션 인식을 위해 개발된 표준 목표를 사용하여 학습된 사전 학습된 시각적 표현에 의존합니다. 그러나 이 목표는 종종 비디오 모델이 비디오의 장기적 상호 작용과 역학을 포착하는 대신 짧은 클립에서 단기 종속성만 이해하도록 권장합니다. 이는 사전 학습된 시각적 표현의 장기 예측 작업으로의 일반화 가능성을 제한할 수 있습니다. 영어: 인간이 주석을 단 액션 레이블의 강력한 학습 감독에 의존함에도 불구하고, 위에서 언급한 접근 방식은 여전히 보이지 않는 데이터[16]에 대한 일반화가 잘 되지 않아 이론을 뒷받침합니다. 장기 예측을 위한 사전 학습을 개선하기 위해, 우리는 먼저 비디오가 일반적으로 다중 스케일 특성을 가지고 있으며, 액션이 서로 다른 세분성 수준에서 발생하는 하위 액션으로 분해될 수 있다는 관찰을 합니다. 비디오 인코더 비디오 클립 쌍 사전 학습(이전 작업) 비디오 유사성 최대화 인코더 맥락화된 미래 표현 예측 헤드 비디오 인코더 관찰된 비디오 클립 유사성 최대화 비디오 인코더 다중 스케일 비디오 사전 학습(우리의 것) 비디오 인코더 비디오 인코더 비디오 인코더 유사성 최대화 여러 시간 스케일에 걸친 미래 비디오 클립 그림 2: 다중 스케일 비디오 사전 학습(MVP). 동일한 비디오의 클립 표현 간 유사성을 극대화하는 이전의 자기 감독 방식[24, 13]과 달리 MVP는 다양한 시간 척도에 걸쳐 미래의 맥락적 정보를 예측하는 모델을 학습시켜 장기 예측 작업에 더 잘 일반화하는 데 도움이 됩니다. 식사. 가장 높은 수준의 추상화에서 오믈렛을 만드는 복잡한 동작은 일반적으로 계란 깨기와 기름 넣기와 같이 더 짧은 시간 척도에서 발생하는 여러 동작으로 구성됩니다. 이 구조를 이해하는 법을 배우는 것이 관련된 에이전트의 기본 목표와 의도를 유추하는 데 중요할 수 있으며, 따라서 후속 동작에 대한 보다 정확한 예측을 용이하게 할 것이라고 가정합니다. 사전 학습 중에 자기 감독 방식으로 학습된 비디오 표현에 동작의 다중 척도 특성을 인코딩하려고 노력하며, 이는 다운스트림 장기 예측 작업으로 더 효과적으로 일반화될 것입니다. 이를 위해, 우리는 새로운 멀티스케일 비디오 사전 학습(MVP) 접근법(그림 2에 설명됨)을 소개합니다. 이 접근법은 비디오 모델이 부분적으로 관찰된 비디오 시퀀스의 정보를 사용하여 다양한 시간 척도에 걸쳐 집계된 미래 비디오 클립의 맥락화된 표현을 예측하는 법을 배우도록 장려합니다. MVP는 장기 예측 작업에서 필요한 역량에서 영감을 얻었으며, 이를 위해서는 관찰된 행동의 공간적 및 시간적 구조에 대해 추론하고 여러 척도와 시간적 해상도에 걸쳐 발생하는 미래 이벤트를 예측할 수 있어야 합니다. 사전 학습 중에 MVP는 미래 클립에 포함된 맥락적 정보를 예측하는 데 필요한 관찰된 클립 시퀀스에서 지식을 추론하는 법을 학습합니다. 우리의 자기 지도 공식에는 기준 진실 레이블이 없으므로 미래 비디오 클립의 맥락화된 표현을 계산하여 예측 대상을 생성합니다. MVP의 이 핵심 측면은 동일한 비디오에서 샘플링한 서로 다른 클립의 표현 간 유사성을 극대화하는 최첨단 비디오 사전 학습 목표와 구별됩니다[24, 13](그림 2 상단). Feichtenhofer 등[13]은 후자의 목표가 동일한 비디오의 서로 다른 클립이 시공간적 차원에서 유사한 표현을 갖도록 장려한다는 것을 보여줍니다. 클립 불변 비디오 표현을 학습하는 것이 단기 동작 인식 작업에 유익할 수 있지만 관찰된 비디오의 고수준 의미 구조를 인코딩하지 않습니다. 반면 MVP 학습 목표는 비디오 모델이 관찰된 비디오 시퀀스에서 여러 척도로 미래 정보를 외삽하도록 학습합니다. 비디오 모델은 긴 비디오에서 서로 다른 세분성 수준에서 서로 다른 동작 간의 관계를 인식함으로써 비디오의 기본 구조를 더 잘 이해하고 다음에 무슨 일이 일어날지에 대해 더 정확한 예측을 할 수 있습니다. 우리는 MVP의 사전 학습된 표현을 순서 무관 및 특정 행동 예측(그림 1)을 포함한 다운스트림 장기 예측 작업으로 전송하여 MVP의 효과를 평가합니다. 나아가, 우리는 또한 비디오 요약 예측의 새로운 멀티모달 작업을 도입하는데, 여기서 목표는 일련의 방해 요소에서 관찰된 활동과 미래 활동의 해당 텍스트 요약을 검색하는 것입니다. MVP는 Ego4D 및 Epic-Kitchens55/100 데이터 세트에서 최첨단 비디오 사전 학습 접근 방식보다 상당히 우수한 성능을 보입니다. 더 중요한 것은, 우리는 광범위한 절제 연구를 통해 MVP의 다양한 측면의 기여에 대한 핵심 통찰력을 추출하는데, 이는 다중 스케일 비디오 표현을 학습하는 미래 작업에 유익할 것으로 기대합니다. 2. 관련 연구 자기 감독 비디오 사전 학습. 자기 감독 비디오 사전 학습[13, 18, 31]은 Kinetics-400/600[2, 3, 21], HMDB-51[22] 및 UCF101[28]을 포함한 대상 데이터 세트에서 활동 인식[10, 12, 13, 18, 19, 25, 31], 비디오 객체 분할[20], 조기 행동 예측[27] 및 의도치 않은 행동 감지[18, 19]와 같은 다운스트림 작업의 성능을 개선하는 데 유익한 것으로 입증되었습니다. 이미지 기반 자기 감독 사전 학습 목표[4, 5, 6]에서 영감을 받은 최첨단 비디오 접근 방식[13, 24, 31, 33]은 종종 동일한 비디오에서 샘플링된 두 클립의 표현 간 유사성을 최대화하는 유사한 학습 목표를 사용합니다. Contrastive Video Representation Learning(CVRL) [24] 접근법은 또한 최적의 성능을 위해 적용된 변환이 모든 프레임에서 일관되어야 함을 보여줍니다.Feichtenhofer et al. [13]은 또한 비디오 클립 불변 표현을 학습하는 이 목표가 클립 쌍을 넘어 확장될 수 있으며, 이를 통해 학습된 표현의 다운스트림 작업인 동작 인식에 대한 견고성이 더욱 향상됨을 보여줍니다.또한 Contrastive Predictive Coding(CPC) [23] 및 Dense Predictive Coding(DPC) [18] 접근법도 정신이 유사하며, 각각 맥락을 위한 관찰된 클립 시퀀스가 주어졌을 때 미래 클립의 거친 클립 수준 및 세밀한 시공간 영역 표현을 예측하는 것입니다.Han et al. [19]은 미래를 예측하는 비결정적 특성을 설명하기 위해 학습 가능한 벡터의 메모리 뱅크를 도입하여 이를 더욱 발전시켰습니다. 그러나 MVP 접근 방식과 달리 앞서 언급한 접근 방식은 관찰된 시퀀스 바로 뒤에 오는 미래 클립의 정보를 예측하는 법을 학습합니다. 더 중요한 것은, 미래 비디오 클립의 기본 표현만을 예측하고, 맥락화된 표현은 예측하지 않는다는 것입니다. 맥락화된 표현에서 정보는 인과적 방식으로 모든 이전 미래 클립에 집계됩니다. 또한 BraVe[25]와 LSTCL[31]은 동일한 비디오의 짧은 클립과 긴 클립 한 쌍의 유사성을 극대화하여 클립 수준 표현에서 장기 시간적 단서를 인코딩하는 법을 학습하는 유사한 아이디어를 구현합니다. MVP의 다중 스케일 측면은 BraVe 및 LSTCL과 구별됩니다. 이러한 방법은 비디오 모델이 짧은 클립에서 긴 클립에 포함된 맥락적 정보를 외삽하는 데 도움이 되지만, 학습 목표는 맥락적 정보가 다른 기간에 걸쳐 어떻게 변할 수 있는지 이해하도록 명시적으로 장려하지 않습니다. 이는 몇 프레임 내에서 발생하는 짧은 동작과 몇 초 이상에 걸쳐 발생할 수 있는 장기 동작 간의 관계를 이해하는 비디오 모델의 능력을 제한할 수 있습니다. 대조적으로, 다양한 시간 범위에 걸쳐 미래의 맥락적 정보를 예측하는 법을 학습함으로써, MVP는 훈련된 비디오 모델이 다양한 추상화 수준에서의 행동을 더 깊이 이해하고 하위 행동을 식별함으로써 복잡한 행동을 인식할 수 있도록 할 수 있다.행동 예측.최신 접근 방식[8, 15]은 종종 단기 문제 공식화를 다루는 것을 목표로 하며, 그 목표는 7초 동안 관찰된 비디오 시퀀스의 맥락을 사용하여 다음 7초 동안 발생할 행동을 예상하는 것이다.이전 접근 방식에서는 과거 시간적 맥락[14, 26]을 집계하거나 미래 프레임과 비디오 클립의 표현을 예측[30, 32]하여 쿼리 비디오의 무료 추가 정보를 활용하여 이 작업을 해결하는 것으로 제안되었다.Gong et al. [16] 또한 부분적으로 관찰된 비디오에서 장기 시간적 역학에 대한 보다 효과적인 이해를 계산하여 장기 예측의 더 어려운 작업에서 보다 정확한 예측을 생성하기 위해 완전 주의 모델을 활용합니다[8, 11, 15, 17, 26]. 그러나 이러한 강력하게 감독되는 접근 방식은 종종 비디오에서 동작의 다중 스케일 특성을 인코딩하지 않는 사전 학습된 시각적 표현을 활용하여 효과성이 제한됩니다. 따라서 MVP는 다운스트림 장기 예측 작업을 위한 보다 효율적인 기본 표현을 학습하는 것을 목표로 하기 때문에 이러한 방법과 직교합니다. 다중 스케일 표현을 최첨단 예측 접근 방식에 통합하는 것은 향후 작업에 맡깁니다. 3. 다중 스케일 비디오 사전 학습 우리의 목표는 레이블이 지정되지 않은 비디오 세트에서 다운스트림 장기 예측 작업으로 잘 일반화되는 강력한 비디오 표현을 학습하는 것입니다. 이를 위해, 부분적으로 관찰된 클립 시퀀스의 맥락을 바탕으로 비디오 모델이 다가올 비디오 클립의 보다 정확한 세부적인 동작 예측을 생성할 수 있도록 하는 자기 감독형 다중 스케일 비디오 사전 학습(MVP) 목표를 소개합니다. 저희의 접근 방식은 장기 예측에는 여러 시간 스케일(예: 가까운 미래와 먼 미래)에서 미래 이벤트의 발생을 예측하는 핵심 역량이 필요하다는 추론에서 동기를 얻었습니다. 마찬가지로 MVP는 비디오 모델이 관찰된 클립 시퀀스에서 비디오의 초기 맥락을 유추하고 맥락을 활용하여 미래 클립에 포함된 정보에 대한 예측을 조건화해야 합니다. 사전 학습 중에 명확한 주석이 없기 때문에 긴 비디오에서 복잡한 동작의 다중 스케일 특성을 활용하여 의사 감독을 제안합니다. 예를 들어, 오믈렛을 만드는 복잡한 동작은 양파를 자르고 계란을 깨는 것과 같은 더 짧은 원자 동작으로 분해될 수 있습니다. 보다 구체적으로, MVP는 다양한 수의 미래 클립에 대한 정보를 집계하여 맥락화된 미래의 세밀한 시공간적 표현을 예측하도록 비디오 모델을 훈련합니다. 이 목표는 비디오 모델이 여러 시간 범위에 걸쳐 미래 맥락적 정보를 인코딩하는 표현을 학습하도록 장려한다는 가설을 세웁니다. 일반적으로 동일한 비디오의 다른 클립이 유사한 표현을 갖도록 장려하는 최첨단 비디오 사전 학습 방법[13, 23, 24, 31]과 달리 MVP는 비디오 모델이 관찰된 비디오의 공간적 및 시간적 구조를 효과적으로 표현하여 미래의 단기 및 장기 행동에 대한 장기 정보를 외삽하도록 훈련합니다. 직관적으로, 행동의 계층 구조를 이해하면 비디오 모델이 하위 행동을 식별하여 복잡한 행동에 대해 더 잘 추론하고 인식할 수 있습니다. 이러한 이해는 모델이 예측을 조건지을 수 있는 보다 정확한 사전 분포를 계산하는 데 도움이 될 수 있습니다. 3.1. 비디오 클립 시퀀스의 시간적 집계 최신 비디오 사전 학습 방법[13, 24]은 종종 동일한 비디오의 비디오 클립 쌍을 활용하지만, MVP 목표는 대신 비디오 클립 시퀀스 V와 VF의 쌍으로 비디오 모델을 학습합니다. MVP는 비디오 모델이 VO를 관찰하고 여러 시간 척도에서 VF의 클립에 대해 집계된 미래 맥락 정보를 예측하는 데 필요한 지식을 추론하도록 요구합니다. 시작으로, 입력 비디오를 각각 프레임(약 0.8초)의 겹치지 않는 클립으로 분할하고 관찰된 클립 시퀀스와 미래 클립 시퀀스 V° = {√₁°, VF = {VN+ V No +K,···, Vo+K+NÅ }를 무작위로 샘플링합니다. 여기서 No, NF 및 K는 각각 관찰된 클립, 미래 클립 및 시간적 오프셋 클립의 수를 나타냅니다. 또한 시간적 스트라이드 S를 두 시간 척도 사이의 클립 수 차이로 정의합니다. NF 따라서 MVP는 Np 예측을 수행합니다.여기서 Np S = V}이고 비디오 모델(그림 3)은 비디오 클립 미래 비디오 표현 예측 Pred HeadPred HeadPred HeadTemporal Aggregator ho 다중 스케일 예측 유사성 극대화 Temporal Aggregator hμ 비디오 비디오 비디오 비디오 인코더 인코더 인코더 會 비디오 인코더 비디오 인코더 TTT T+K 관찰된 비디오 클립 T+K+S T+K+2S 미래 비디오 클립 비디오 인코더 T+K+3S 그림 3: 다중 스케일 비디오 사전 학습.관찰된 비디오 클립 시퀀스가 주어지면 MVP는 여러 시간 스케일에 걸쳐 미래 비디오 클립의 맥락화된 표현을 예측하는 데 필요한 정보를 추출하는 방법을 학습합니다.인코딩 함수 go와 시간적 맥락 집계 함수 ho 및 hμ. go는 입력 클립을 시공간 영역 표현 세트로 인코딩하는 데 사용되는 반면 ho 및 hμ는 구성 클립 표현에 대한 정보를 결합하여 각각 관찰된 및 미래 클립 시퀀스의 시간적 맥락을 집계하는 데 사용됩니다. MVP 목적의 계산적으로 요구되는 특성으로 인해 수정 없이 가볍지만 강력한 Multiscale Vision Transformers(MViT) [10]를 기본 인코더 ge로 채택했습니다. 이 인코더는 상당히 적은 매개변수를 포함하고 있음에도 불구하고 동작 인식에서 이전 비디오 인코더보다 성능이 뛰어난 것으로 나타났습니다. i번째 비디오 클립을 다음과 같이 인코딩합니다. fi = go(Vi), fi Є RL×H×W×D 여기서 L, H, W 및 D는 각각 시간, 높이, 너비 및 채널 크기를 나타냅니다. 그런 다음 클립에 대한 정보를 집계하여 두 입력 시퀀스에 대한 맥락화된 표현을 계산합니다.zº = No = F = h$(go(V°)), z³ = z¦µ = hµ(go(VF)), (1) 여기서 z와 z는 각각 관찰된 시퀀스와 미래 시퀀스에 대한 맥락화된 표현입니다.3.2. 시공간적 다중 헤드 자기 주의 우리는 공간적 및 시간적 차원에서 세분화된 영역 표현을 예측하는 법을 배우는 것이 글로벌 클립 표현을 예측하는 데 초점을 맞춘 이전 연구[23, 24, 31]와 달리 비디오에서 객체와 동작 간의 상호 작용을 이해하는 데 유익할 수 있다고 주장합니다.이를 위해 여러 시간 척도에 걸쳐 맥락화된 미래 클립 시퀀스의 시공간적 영역 표현을 예측하도록 모델을 훈련합니다.이를 위해서는 시간 집계 함수가 여러 시간 단계에 걸쳐 서로 다른 공간적 영역 간의 맥락적 정보를 계산할 수 있어야 합니다. 직관적으로 이 목표는 시간에 따른 객체의 움직임과 다양한 동작과의 관련성에 대한 강력한 이해를 통해서만 달성할 수 있습니다. 이 기능을 학습하기 위해 널리 채택된 규칙은 비디오 클립 시퀀스의 전체 시공간 영역에 대해 멀티헤드 자기 주의(MHA) [1]를 사용하는 것입니다. 그러나 자기 주의는 2차 복잡도를 가지므로 비디오 클립의 짧은 시퀀스에서도 계산 요구 사항이 빠르게 증가합니다. 이를 해결하기 위해 비디오 클립 시퀀스의 동일한 공간 위치에 있는 모든 영역에 대해 자기 주의만 계산하여 비디오 클립 간의 시간적 맥락 정보만 집계합니다. 이는 자기 주의 연산이 MVIT 모델에서 학습한 최종 비디오 클립 인코딩 함수에서 합성된 암묵적 함수이기 때문에 각 시간 단계에 대한 MVIT의 출력 영역 표현이 다른 공간 영역에 대한 정보를 집계하여 이미 맥락화되었다는 관찰에서 동기를 얻었습니다. MViT 아키텍처에 대한 자세한 내용은 관심 있는 독자에게 [10]을 참조하세요. 시작으로, 입력 시공간 블록 SЄ RLxHxWxD가 주어졌을 때, j번째 공간 위치 S; E RLXD(여기서 j = hw)에 대한 시간 영역 피처 집합을 쿼리, 키 및 값으로 투영합니다.Sj,q = SjWq, Sj‚k = SjWk, Sj,v = SjWv(여기서 Wq, Wk, W₁는 차원 DxD의 쿼리, 키 및 값 투영 가중치입니다.그런 다음 다음과 같이 MHA 연산을 사용하여 시퀀스에 대한 맥락화된 표현을 계산합니다.MHA(Sj,q, Sj,k, Sj,vw) = Sj,v Softmax SqSj,k (3) √D i번째 비디오 클립의 시공간 영역 표현 Zi,t,h,w가 주어졌을 때, 다음과 같이 맥락화된 표현을 계산합니다.Zi,t,h,w MHA(Zi,t,h,w). 마지막으로, 문맥화된 시공간 영역 표현을 2층 다층 퍼셉트론(MLP), 즉 Zi,t,h,w MLPk(2,t,h,w)를 통해 통과시켜 k번째 시간 단계에서 S의 시간적 스트라이드로 j번째 미래 영역 표현을 예측합니다. 예측된 영역 표현의 전체 세트는 섹션 3.3에서 훈련 손실을 계산하는 데 사용됩니다. 각 예측 시간 단계에 대해 다른 예측 헤드를 사용한다는 점에 유의하세요. 3.3. 다중 스케일 대상 및 손실 공식 = 자체 감독을 위한 예측 대상을 계산하기 위해 인과적 방식으로 VF에 집계 함수 hμ를 적용합니다. 즉, 1번째 시간 단계에서 j번째 공간 영역에 대한 문맥화된 공간 영역 표현 St,j 세트는 시간적으로 앞선 영역에만 주의를 기울여 계산됩니다. 샘플링된 배치의 미래 비디오 클립의 b번째 시퀀스에 대해, 우리는 타겟 표현 Z₁ = {¾‚k}의 세트를 추출합니다.여기서 k % S = 0이고 Z₁ € RÑ³×LHW×D¸ 레이블이 지정되지 않은 비디오의 배치가 주어지면, 우리는 비디오 모델을 훈련합니다.사전 학습 다중 사전 학습 Ego4D ↑ EK55 ↑ EK100 ↑ 접근 클립 사용 감독 동사 동작 인식 아니요 CVRL [24] 아니요 자기 강력함 20.25.CPC [23] 예 자기 27.26.명사 14.17.56 18.25.85 25.88 22.26.91 23.평균 동사 명사 평균 동사 명사 평균 11.17.17.LSTCL [31] 예 자기 26.27.18.DPC [18] 예 자기 28.29.19.CVRL [24] 예 자체 CONSTCL [33] 예 자체 MVP(우리의) 예 자체 28.27 29.27.49 29.30.18 32.27.29 23.28.61 24.29.00 23.91 18.28.31 24.47 19.52 22.00 25.31.25 25.83 20.78 23.31 26.69 20.18 23.14.80 18.19.62 22.92 16.20.13 23.16 17.21.05 23.12.46 15.19.20.17.15 20.21.52 25.18.18 21.21.12 24.19.24 22.19.35 22.표 1: 순서 독립적인 장기 예측. 모든 동사와 명사 클래스에 대한 평균 평균 정밀도를 보고합니다. 일반적으로 자기 감독 사전 학습이 동작 인식보다 장기 예측 작업에 더 유익하다는 것을 알 수 있습니다. 대조적 손실[23] 공식을 사용하여 종단 간: A = B Np LHW -ΣΣΣ - 10g b=1 j=1 n=exp(žb,j,n · Zb,j,n/T) exp(žb,j,n · Zb,j,n/T)+ Σ exp(b,j,n Zb&#39;,j&#39;,n&#39; /T) (4) (b&#39;,j&#39;,n&#39;)!=(b,j,n) 여기서 7은 온도 값을 나타냅니다. 4.
--- EXPERIMENT ---
Ego4D 및 Epic-Kitchens-55/100 데이터 세트에서 MVP가 최신 방식보다 상당한 마진으로 성능이 우수함을 보여줍니다. 특히 MVP는 기존 방식보다 비디오 요약 예측에서 20% 이상의 상대적 성능 향상을 얻습니다. 1. 서론 인간 활동에 대한 장기 예측(그림 1에 설명됨)은 지능적이고 협업적인 기계를 개발하는 데 필수적인 핵심 기능입니다. 몇 가지 관찰을 바탕으로 미래 행동에 대해 추론하는 기계는 그에 따라 자신의 행동을 더 잘 계획하고 동적 환경에서 다른 에이전트와 더 효과적으로 상호 작용할 수 있습니다. 그러나 미래 행동을 예측하는 것은 본질적으로 어렵습니다. 시작하려면 모델이 부분적 관찰 가능성 하에서 환경의 현재 상태를 이해해야 합니다. 더 중요한 것은 미래의 비결정적 특성이 다운스트림 장기 예측 작업을 복합화한다는 것입니다. 부분적으로 관찰된 비디오 입력 미래 행동 예측!! 올바른 요약 검색 ☑ Х 요약 3: CI가 반죽을 반죽하고 피자를 만듭니다. III 수리 닫기 조이기 보드 커버 손잡이 동작 예측 요약 1: C 요약 2: C 수리 Il회로를 제거하고 회로를 돌리고 스위치를 켭니다.도구를 치웁니다.비디오 요약 예측 -L 그림 1: 장기 활동 예측 작업.비디오 모델을 사전 학습하고 학습된 표현을 장기 동작 및 비디오 요약 예측으로 전환합니다.&#39;C&#39;는 요약에서 카메라 착용자를 나타냅니다.시간이 지남에 따라 관찰된 동작과 객체 간의 관계를 추론하고 이러한 관계가 미래에 어떻게 진화할지 예측해야 하는 어려움.최신 장기 예측 방법(예: [16, 17])은 완전한 주의 모델[29]을 활용하여 비디오에서 장기 시간적 역학을 모델링하기 위한 보다 효과적인 기능을 학습하는 데 중점을 두었지만 여전히 동작 인식을 위해 개발된 표준 목표를 사용하여 학습된 사전 학습된 시각적 표현에 의존합니다. 그러나 이 목표는 종종 비디오 모델이 비디오의 장기적인 상호작용과 역동성을 포착하는 대신 짧은 클립에서 단기적인 종속성만 이해하도록 장려합니다. 이는 사전 학습된 시각적 표현의 장기 예측 작업에 대한 일반화 가능성을 제한할 수 있습니다. 인간이 주석을 단 액션 레이블의 강력한 학습 감독에 의존함에도 불구하고, 위에서 언급한 접근 방식은 여전히 보이지 않는 데이터로 일반화하는 데 있어 좋지 않습니다[16]. 이는 이론을 뒷받침합니다. 장기 예측을 위한 사전 학습을 개선하기 위해, 우리는 먼저 비디오가 일반적으로 다중 스케일 특성을 가지고 있으며, 액션이 서로 다른 세분성 수준에서 발생하는 하위 액션으로 분해될 수 있다는 것을 관찰합니다. 그림 2를 고려해 보겠습니다.이것은 누군가가 비디오 인코더 비디오 클립 쌍 사전 학습(이전 작업) 비디오 유사성 극대화 인코더 문맥화된 미래 표현 예측 예측 헤드 비디오 인코더 관찰된 비디오 클립 유사성 극대화 비디오 인코더 다중 스케일 비디오 사전 학습(저희) 비디오 인코더 비디오 인코더 비디오 인코더 유사성 극대화 여러 시간 척도에 걸친 미래 비디오 클립 그림 2: 다중 스케일 비디오 사전 학습(MVP). 동일한 비디오의 클립 표현 간 유사성을 극대화하는 이전의 자체 감독 방법[24, 13]과 달리 MVP는 다양한 시간 척도에 걸쳐 미래 문맥 정보를 예측하도록 모델을 학습시켜 장기 예측 작업에 더 잘 일반화하는 데 도움이 됩니다.식사.가장 높은 수준의 추상화에서 오믈렛을 만드는 복잡한 동작은 일반적으로 계란 깨기와 기름 넣기와 같이 더 짧은 시간 척도에서 발생하는 여러 동작으로 구성됩니다. 우리는 이 구조를 이해하는 법을 배우는 것이 관련된 에이전트의 기본 목표와 의도를 추론하는 데 중요할 수 있으며, 따라서 후속 행동에 대한 보다 정확한 예측을 용이하게 할 것이라고 가설을 세웠습니다. 우리는 사전 학습 중에 자기 감독 방식으로 학습된 비디오 표현에 행동의 다중 스케일 특성을 인코딩하려고 노력하며, 이는 다운스트림 장기 예측 작업으로 보다 효과적으로 일반화될 것입니다. 이를 위해 우리는 새로운 다중 스케일 비디오 사전 학습(MVP) 접근 방식(그림 2에 설명됨)을 소개합니다. 이 접근 방식은 비디오 모델이 부분적으로 관찰된 비디오 시퀀스의 정보를 사용하여 다양한 시간 스케일에 걸쳐 집계된 미래 비디오 클립의 맥락화된 표현을 예측하는 법을 학습하도록 장려합니다. MVP는 장기 예측 작업에서 필요한 역량에서 영감을 얻었으며, 관찰된 행동의 공간적 및 시간적 구조에 대해 추론하고 여러 스케일과 시간적 해상도에서 발생하는 미래 이벤트를 예측할 수 있어야 합니다. 사전 학습 중에 MVP는 미래 클립에 포함된 맥락적 정보를 예측하는 데 필요한 관찰된 클립 시퀀스에서 지식을 유추하는 방법을 학습합니다. 자체 지도 공식에 기준 진실 레이블이 없기 때문에 미래 비디오 클립의 맥락적 표현을 계산하여 예측 대상을 생성합니다. MVP의 이 핵심 측면은 동일한 비디오에서 샘플링한 다른 클립의 표현 간 유사성을 극대화하는 최첨단 비디오 사전 학습 목표와 구별됩니다[24, 13](그림 2 상단). Feichtenhofer 등[13]은 후자의 목표가 동일한 비디오의 다른 클립이 시공간적 차원에서 유사한 표현을 갖도록 장려한다는 것을 보여줍니다. 클립 불변 비디오 표현을 학습하는 것은 단기 동작 인식 작업에 유익할 수 있지만 관찰된 비디오의 고수준 의미 구조를 인코딩하지 않습니다. 반면 MVP 학습 목표는 관찰된 비디오 시퀀스에서 여러 스케일로 미래 정보를 외삽하도록 비디오 모델을 학습합니다. 다양한 수준의 세분성에서 긴 비디오에서 다양한 동작 간의 관계를 인식함으로써 비디오 모델은 비디오의 기본 구조를 더 잘 이해하고 다음에 무슨 일이 일어날지에 대해 더 정확한 예측을 할 수 있습니다. 우리는 MVP의 사전 훈련된 표현을 순서 무관 및 특정 동작 예측(그림 1)을 포함한 다운스트림 장기 예측 작업으로 전송하여 MVP의 효과를 평가합니다. 또한, 관찰된 활동과 미래 활동의 해당 텍스트 요약을 일련의 방해 요소에서 검색하는 것이 목표인 새로운 멀티모달 비디오 요약 예측 작업을 소개합니다. MVP는 Ego4D 및 Epic-Kitchens55/100 데이터 세트에서 최첨단 비디오 사전 훈련 접근 방식보다 상당히 우수한 성능을 발휘합니다. 더 중요한 것은, 우리는 광범위한 절제 연구를 통해 MVP의 다양한 측면의 기여에 대한 주요 통찰력을 추출하여 다중 스케일 비디오 표현을 학습하는 향후 작업에 도움이 되기를 바랍니다. 2. 관련 연구 자기 감독 비디오 사전 훈련. 자기 감독 비디오 사전 학습[13, 18, 31]은 Kinetics-400/600[2, 3, 21], HMDB-51[22] 및 UCF101[28]을 포함한 대상 데이터 세트에서 활동 인식[10, 12, 13, 18, 19, 25, 31], 비디오 객체 분할[20], 조기 행동 예측[27] 및 의도치 않은 행동 감지[18, 19]와 같은 다운스트림 작업의 성능을 개선하는 데 유익한 것으로 입증되었습니다. 이미지 기반 자기 감독 사전 학습 목표[4, 5, 6]에서 영감을 받은 최첨단 비디오 접근 방식[13, 24, 31, 33]은 종종 동일한 비디오에서 샘플링된 두 클립의 표현 간 유사성을 최대화하는 유사한 학습 목표를 사용합니다. Contrastive Video Representation Learning(CVRL) [24] 접근법은 또한 최적의 성능을 위해 적용된 변환이 모든 프레임에서 일관되어야 함을 보여줍니다.Feichtenhofer et al. [13]은 또한 비디오 클립 불변 표현을 학습하는 이 목표가 클립 쌍을 넘어 확장될 수 있으며, 이를 통해 학습된 표현의 다운스트림 작업인 동작 인식에 대한 견고성이 더욱 향상됨을 보여줍니다.또한 Contrastive Predictive Coding(CPC) [23] 및 Dense Predictive Coding(DPC) [18] 접근법도 정신이 유사하며, 각각 맥락을 위한 관찰된 클립 시퀀스가 주어졌을 때 미래 클립의 거친 클립 수준 및 세밀한 시공간 영역 표현을 예측하는 것입니다.Han et al. [19]은 미래를 예측하는 비결정적 특성을 설명하기 위해 학습 가능한 벡터의 메모리 뱅크를 도입하여 이를 더욱 발전시켰습니다. 그러나 MVP 접근 방식과 달리 앞서 언급한 접근 방식은 관찰된 시퀀스 바로 뒤에 오는 미래 클립의 정보를 예측하는 법을 학습합니다. 더 중요한 것은, 미래 비디오 클립의 기본 표현만을 예측하고, 맥락화된 표현은 예측하지 않는다는 것입니다. 맥락화된 표현에서 정보는 인과적 방식으로 모든 이전 미래 클립에 집계됩니다. 또한 BraVe[25]와 LSTCL[31]은 동일한 비디오의 짧은 클립과 긴 클립 한 쌍의 유사성을 극대화하여 클립 수준 표현에서 장기 시간적 단서를 인코딩하는 법을 학습하는 유사한 아이디어를 구현합니다. MVP의 다중 스케일 측면은 BraVe 및 LSTCL과 구별됩니다. 이러한 방법은 비디오 모델이 짧은 클립에서 긴 클립에 포함된 맥락적 정보를 외삽하는 데 도움이 되지만, 학습 목표는 맥락적 정보가 다른 기간에 걸쳐 어떻게 변할 수 있는지 이해하도록 명시적으로 장려하지 않습니다. 이는 몇 프레임 내에서 발생하는 짧은 동작과 몇 초 이상에 걸쳐 발생할 수 있는 장기 동작 간의 관계를 이해하는 비디오 모델의 능력을 제한할 수 있습니다. 대조적으로, 다양한 시간 범위에 걸쳐 미래의 맥락적 정보를 예측하는 법을 학습함으로써, MVP는 훈련된 비디오 모델이 다양한 추상화 수준에서의 행동을 더 깊이 이해하고 하위 행동을 식별함으로써 복잡한 행동을 인식할 수 있도록 할 수 있다.행동 예측.최신 접근 방식[8, 15]은 종종 단기 문제 공식화를 다루는 것을 목표로 하며, 그 목표는 7초 동안 관찰된 비디오 시퀀스의 맥락을 사용하여 다음 7초 동안 발생할 행동을 예상하는 것이다.이전 접근 방식에서는 과거 시간적 맥락[14, 26]을 집계하거나 미래 프레임과 비디오 클립의 표현을 예측[30, 32]하여 쿼리 비디오의 무료 추가 정보를 활용하여 이 작업을 해결하는 것으로 제안되었다.Gong et al. [16] 또한 부분적으로 관찰된 비디오에서 장기 시간적 역학에 대한 보다 효과적인 이해를 계산하여 장기 예측의 더 어려운 작업에서 보다 정확한 예측을 생성하기 위해 완전 주의 모델을 활용합니다[8, 11, 15, 17, 26]. 그러나 이러한 강력하게 감독되는 접근 방식은 종종 비디오에서 동작의 다중 스케일 특성을 인코딩하지 않는 사전 학습된 시각적 표현을 활용하여 효과성이 제한됩니다. 따라서 MVP는 다운스트림 장기 예측 작업을 위한 보다 효율적인 기본 표현을 학습하는 것을 목표로 하기 때문에 이러한 방법과 직교합니다. 다중 스케일 표현을 최첨단 예측 접근 방식에 통합하는 것은 향후 작업에 맡깁니다. 3. 다중 스케일 비디오 사전 학습 우리의 목표는 레이블이 지정되지 않은 비디오 세트에서 다운스트림 장기 예측 작업으로 잘 일반화되는 강력한 비디오 표현을 학습하는 것입니다. 이를 위해, 부분적으로 관찰된 클립 시퀀스의 맥락을 바탕으로 비디오 모델이 다가올 비디오 클립의 보다 정확한 세부적인 동작 예측을 생성할 수 있도록 하는 자기 감독형 다중 스케일 비디오 사전 학습(MVP) 목표를 소개합니다. 저희의 접근 방식은 장기 예측에는 여러 시간 스케일(예: 가까운 미래와 먼 미래)에서 미래 이벤트의 발생을 예측하는 핵심 역량이 필요하다는 추론에서 동기를 얻었습니다. 마찬가지로 MVP는 비디오 모델이 관찰된 클립 시퀀스에서 비디오의 초기 맥락을 유추하고 맥락을 활용하여 미래 클립에 포함된 정보에 대한 예측을 조건화해야 합니다. 사전 학습 중에 명확한 주석이 없기 때문에 긴 비디오에서 복잡한 동작의 다중 스케일 특성을 활용하여 의사 감독을 제안합니다. 예를 들어, 오믈렛을 만드는 복잡한 동작은 양파를 자르고 계란을 깨는 것과 같은 더 짧은 원자 동작으로 분해될 수 있습니다. 보다 구체적으로, MVP는 다양한 수의 미래 클립에 대한 정보를 집계하여 맥락화된 미래의 세밀한 시공간적 표현을 예측하도록 비디오 모델을 훈련합니다. 이 목표는 비디오 모델이 여러 시간 범위에 걸쳐 미래 맥락적 정보를 인코딩하는 표현을 학습하도록 장려한다는 가설을 세웁니다. 일반적으로 동일한 비디오의 다른 클립이 유사한 표현을 갖도록 장려하는 최첨단 비디오 사전 학습 방법[13, 23, 24, 31]과 달리 MVP는 비디오 모델이 관찰된 비디오의 공간적 및 시간적 구조를 효과적으로 표현하여 미래의 단기 및 장기 행동에 대한 장기 정보를 외삽하도록 훈련합니다. 직관적으로, 행동의 계층 구조를 이해하면 비디오 모델이 하위 행동을 식별하여 복잡한 행동에 대해 더 잘 추론하고 인식할 수 있습니다. 이러한 이해는 모델이 예측을 조건지을 수 있는 보다 정확한 사전 분포를 계산하는 데 도움이 될 수 있습니다. 3.1. 비디오 클립 시퀀스의 시간적 집계 최신 비디오 사전 학습 방법[13, 24]은 종종 동일한 비디오의 비디오 클립 쌍을 활용하지만, MVP 목표는 대신 비디오 클립 시퀀스 V와 VF의 쌍으로 비디오 모델을 학습합니다. MVP는 비디오 모델이 VO를 관찰하고 여러 시간 척도에서 VF의 클립에 대해 집계된 미래 맥락 정보를 예측하는 데 필요한 지식을 추론하도록 요구합니다. 시작으로, 입력 비디오를 각각 프레임(약 0.8초)의 겹치지 않는 클립으로 분할하고 관찰된 클립 시퀀스와 미래 클립 시퀀스 V° = {√₁°, VF = {VN+ V No +K,···, Vo+K+NÅ }를 무작위로 샘플링합니다. 여기서 No, NF 및 K는 각각 관찰된 클립, 미래 클립 및 시간적 오프셋 클립의 수를 나타냅니다. 또한 시간적 스트라이드 S를 두 시간 척도 사이의 클립 수 차이로 정의합니다. NF 따라서 MVP는 Np 예측을 수행합니다.여기서 Np S = V}이고 비디오 모델(그림 3)은 비디오 클립 미래 비디오 표현 예측 Pred HeadPred HeadPred HeadTemporal Aggregator ho 다중 스케일 예측 유사성 극대화 Temporal Aggregator hμ 비디오 비디오 비디오 비디오 인코더 인코더 인코더 會 비디오 인코더 비디오 인코더 TTT T+K 관찰된 비디오 클립 T+K+S T+K+2S 미래 비디오 클립 비디오 인코더 T+K+3S 그림 3: 다중 스케일 비디오 사전 학습.관찰된 비디오 클립 시퀀스가 주어지면 MVP는 여러 시간 스케일에 걸쳐 미래 비디오 클립의 맥락화된 표현을 예측하는 데 필요한 정보를 추출하는 방법을 학습합니다.인코딩 함수 go와 시간적 맥락 집계 함수 ho 및 hμ. go는 입력 클립을 시공간 영역 표현 세트로 인코딩하는 데 사용되는 반면 ho 및 hμ는 구성 클립 표현에 대한 정보를 결합하여 각각 관찰된 및 미래 클립 시퀀스의 시간적 맥락을 집계하는 데 사용됩니다. MVP 목적의 계산적으로 요구되는 특성으로 인해 수정 없이 가볍지만 강력한 Multiscale Vision Transformers(MViT) [10]를 기본 인코더 ge로 채택했습니다. 이 인코더는 상당히 적은 매개변수를 포함하고 있음에도 불구하고 동작 인식에서 이전 비디오 인코더보다 성능이 뛰어난 것으로 나타났습니다. i번째 비디오 클립을 다음과 같이 인코딩합니다. fi = go(Vi), fi Є RL×H×W×D 여기서 L, H, W 및 D는 각각 시간, 높이, 너비 및 채널 크기를 나타냅니다. 그런 다음 클립에 대한 정보를 집계하여 두 입력 시퀀스에 대한 맥락화된 표현을 계산합니다.zº = No = F = h$(go(V°)), z³ = z¦µ = hµ(go(VF)), (1) 여기서 z와 z는 각각 관찰된 시퀀스와 미래 시퀀스에 대한 맥락화된 표현입니다.3.2. 시공간적 다중 헤드 자기 주의 우리는 공간적 및 시간적 차원에서 세분화된 영역 표현을 예측하는 법을 배우는 것이 글로벌 클립 표현을 예측하는 데 초점을 맞춘 이전 연구[23, 24, 31]와 달리 비디오에서 객체와 동작 간의 상호 작용을 이해하는 데 유익할 수 있다고 주장합니다.이를 위해 여러 시간 척도에 걸쳐 맥락화된 미래 클립 시퀀스의 시공간적 영역 표현을 예측하도록 모델을 훈련합니다.이를 위해서는 시간 집계 함수가 여러 시간 단계에 걸쳐 서로 다른 공간적 영역 간의 맥락적 정보를 계산할 수 있어야 합니다. 직관적으로 이 목표는 시간에 따른 객체의 움직임과 다양한 동작과의 관련성에 대한 강력한 이해를 통해서만 달성할 수 있습니다. 이 기능을 학습하기 위해 널리 채택된 규칙은 비디오 클립 시퀀스의 전체 시공간 영역에 대해 멀티헤드 자기 주의(MHA) [1]를 사용하는 것입니다. 그러나 자기 주의는 2차 복잡도를 가지므로 비디오 클립의 짧은 시퀀스에서도 계산 요구 사항이 빠르게 증가합니다. 이를 해결하기 위해 비디오 클립 시퀀스의 동일한 공간 위치에 있는 모든 영역에 대해 자기 주의만 계산하여 비디오 클립 간의 시간적 맥락 정보만 집계합니다. 이는 자기 주의 연산이 MVIT 모델에서 학습한 최종 비디오 클립 인코딩 함수에서 합성된 암묵적 함수이기 때문에 각 시간 단계에 대한 MVIT의 출력 영역 표현이 다른 공간 영역에 대한 정보를 집계하여 이미 맥락화되었다는 관찰에서 동기를 얻었습니다. MViT 아키텍처에 대한 자세한 내용은 관심 있는 독자에게 [10]을 참조하세요. 시작으로, 입력 시공간 블록 SЄ RLxHxWxD가 주어졌을 때, j번째 공간 위치 S; E RLXD(여기서 j = hw)에 대한 시간 영역 피처 집합을 쿼리, 키 및 값으로 투영합니다.Sj,q = SjWq, Sj‚k = SjWk, Sj,v = SjWv(여기서 Wq, Wk, W₁는 차원 DxD의 쿼리, 키 및 값 투영 가중치입니다.그런 다음 다음과 같이 MHA 연산을 사용하여 시퀀스에 대한 맥락화된 표현을 계산합니다.MHA(Sj,q, Sj,k, Sj,vw) = Sj,v Softmax SqSj,k (3) √D i번째 비디오 클립의 시공간 영역 표현 Zi,t,h,w가 주어졌을 때, 다음과 같이 맥락화된 표현을 계산합니다.Zi,t,h,w MHA(Zi,t,h,w). 마지막으로, 문맥화된 시공간 영역 표현을 2층 다층 퍼셉트론(MLP), 즉 Zi,t,h,w MLPk(2,t,h,w)를 통해 통과시켜 k번째 시간 단계에서 S의 시간적 스트라이드로 j번째 미래 영역 표현을 예측합니다. 예측된 영역 표현의 전체 세트는 섹션 3.3에서 훈련 손실을 계산하는 데 사용됩니다. 각 예측 시간 단계에 대해 다른 예측 헤드를 사용한다는 점에 유의하세요. 3.3. 다중 스케일 대상 및 손실 공식 = 자체 감독을 위한 예측 대상을 계산하기 위해 인과적 방식으로 VF에 집계 함수 hμ를 적용합니다. 즉, 1번째 시간 단계에서 j번째 공간 영역에 대한 문맥화된 공간 영역 표현 St,j 세트는 시간적으로 앞선 영역에만 주의를 기울여 계산됩니다. 샘플링된 배치의 미래 비디오 클립의 b번째 시퀀스에 대해, 우리는 타겟 표현 Z₁ = {¾‚k}의 세트를 추출합니다.여기서 k % S = 0이고 Z₁ € RÑ³×LHW×D¸ 레이블이 지정되지 않은 비디오의 배치가 주어지면, 우리는 비디오 모델을 훈련합니다.사전 학습 다중 사전 학습 Ego4D ↑ EK55 ↑ EK100 ↑ 접근 클립 사용 감독 동사 동작 인식 아니요 CVRL [24] 아니요 자기 강력함 20.25.CPC [23] 예 자기 27.26.명사 14.17.56 18.25.85 25.88 22.26.91 23.평균 동사 명사 평균 동사 명사 평균 11.17.17.LSTCL [31] 예 자기 26.27.18.DPC [18] 예 자기 28.29.19.CVRL [24] 예 자체 CONSTCL [33] 예 자체 MVP(우리의) 예 자체 28.27 29.27.49 29.30.18 32.27.29 23.28.61 24.29.00 23.91 18.28.31 24.47 19.52 22.00 25.31.25 25.83 20.78 23.31 26.69 20.18 23.14.80 18.19.62 22.92 16.20.13 23.16 17.21.05 23.12.46 15.19.20.17.15 20.21.52 25.18.18 21.21.12 24.19.24 22.19.35 22.표 1: 순서 독립적인 장기 예측. 모든 동사와 명사 클래스에 대한 평균 평균 정밀도를 보고합니다. 일반적으로 자기 감독 사전 학습이 동작 인식보다 장기 예측 작업에 더 유익하다는 것을 알 수 있습니다. 대조 손실 [23] 공식을 사용하여 종단 간: A = B Np LHW -ΣΣΣ - 10g b=1 j=1 n=exp(žb,j,n · Zb,j,n/T) exp(žb,j,n · Zb,j,n/T)+ Σ exp(b,j,n Zb&#39;,j&#39;,n&#39; /T) (4) (b&#39;,j&#39;,n&#39;)!=(b,j,n) 여기서 7은 온도 값을 나타냅니다. 4. 실험 4.1. 다운스트림 작업 우리는 순서 무관 및 특정 장기 행동 예측과 비디오 요약 예측 작업에 대한 최첨단 자기 감독 비디오 사전 학습 방법과 멀티스케일 비디오 사전 학습 목표를 비교합니다. 우리는 비디오 모델을 Ego4D[17]에서 사전 학습하고 다운스트림 작업을 위해 Ego4D와 EpicsKitchen-55/100[7, 8]에서 미세 조정합니다. 또한, 우리는 각각 시간적 맥락 집계자 ho와 hμ(섹션 3.1)로 변압기 인코더[29]와 평균 풀링 작업을 사용합니다. 이러한 데이터 세트, 구현 및 기준 모델에 대한 자세한 내용은 보충 자료를 참조하세요. 순서 무관 행동 예측. 순서 무관 장기 예측에서 우리는 지속 시간 T의 비디오의 K%를 관찰하고 나머지 비디오 내에서 행동이 발생할지 예측합니다. Nverb 및 Nnoun 클래스의 어휘가 주어지면, 각 차원이 미래에 클래스가 발생할 확률을 나타내는 Nverb 차원 및 Nnoun 차원 이진 벡터를 예측합니다. 이를 다중 레이블 예측 작업으로 공식화하고 모든 동사 및 명사 클래스에 대해 계산된 이진 교차 엔트로피 손실을 최적화하여 모든 사전 학습된 모델을 미세 조정합니다. 모든 동사 및 명사 클래스에 대해 평균 평균 정밀도(mAP)를 계산합니다. 순서별 동작 예측. 순서별 작업은 훨씬 더 어려운 설정으로, 모델이 올바른 동사 또는 명사를 예측하지만 잘못된 순서로 예측하더라도 페널티를 받습니다. 예측된 동작의 정확도는 시간적 순서에 따라 달라지므로 시퀀스 예측 작업으로 공식화할 수 있습니다. 우리는 모든 시간 단계에 걸쳐 계산된 동사와 명사에 대한 총 교차 엔트로피 손실을 최적화하여 사전 학습된 &quot;모델을 미세 조정합니다. 우리는 예측된 것과 실제 행동 시퀀스가 얼마나 다른지 정량화하기 위해 편집 거리 메트릭[17]을 채택합니다. 비디오 요약 예측. 이 다중 모드 작업에서 T개의 시간 클립과 길이가 TO인 관찰된 하위 시퀀스의 비디오 V에 대해 목표는 일련의 방해 요약에서 해당 요약을 검색하는 것입니다. 비디오 V와 NL 단어가 포함된 요약 L이 주어지면 먼저 관찰된 클립 시퀀스에 대한 맥락화된 표현을 추출합니다: CTO ha(gy(Vo:TO)). 우리는 사전 학습된 BERT-Base[9] 모델을 사용하여 요약에 대한 자연어 표현 fL = RLXDL을 추출합니다: fL = k(L), 여기서 DL은 BERT 모델의 출력 차원이고 ko는 o로 매개변수화된 BERT 모델을 나타냅니다. 우리는 선형 레이어 Wy와 WL을 사용하여 비디오를 투영합니다. 및 언어 표현을 공동 시각-의미 임베딩 공간으로 통합하고 다음의 대조적 손실 공식을 최적화하여 모델을 미세 조정합니다.B = L = Σ - log b = agg exp(Cb,To fb,L/T) еexp(СÛ‚Ã¤ · ƒÛ‚L/T)+ Σexp(ь, fm,L/T) m#b (5) 직관적으로 이 목표는 비디오와 언어 표현 간의 일치를 모델이 학습하도록 장려하여 해당 비디오와 텍스트 요약 쌍 간의 유사성을 최대화합니다.텍스트-비디오 검색[34]의 이전 작업과 일관되게, 기준 진실 요약이 상위 K 검색에서 순위가 매겨지는 횟수의 백분율을 계산하는 Recall @K 메트릭을 채택합니다.4.2. 정량적 결과 4.2.1 순서에 독립적인 장기 예측 우리는 다양한 시간적 범위에 걸쳐 미래 맥락을 인코딩하는 비디오 표현을 학습하는 데 있어 제안된 MVP 사전 학습 접근 방식의 효과를 평가하는 것을 목표로 합니다. 따라서 우리는 다음 8개 시간 단계에 대한 미래 행동을 예측합니다 사전 훈련 접근 방식 행동 인식 아니요 다중 사전 훈련 클립 사용 접근 방식 강력 Ego4D↓ CVRL [24] 아니요 자기 CPC [23] 예 자기 LSTCL [31] 예 자기 DPC [18] 예 자기 CVRL [24] 예 자기 0.CONSTCL [33] 예 자기 MVP(우리의 것) 예 자기 EK55↓ 동사 명사 행동 동사 명사 0.754 0.901 0.977 0.741 0.0.746 0.845 0.960 0.719 0.0.735 0.838 0.956 0.719 0.0.752 0.846 0.963 0.721 0.0.734 0.821 0.950 0.708 0.0.822 0.952 0.719 0.0.735 0.818 0.951 0.704 0.0.724 0.809 0.943 0.690 0.EK100↓ 동작 동사 명사 동작 0.962 0.758 0.952 0.0.948 0.753 0.948 0.0.951 0.746 0.944 0.0.950 0.739 0.939 0.0.946 0.738 0.932 0.0.930 0.0.948 0.0.946 0.0.941 0.721 0.918 0.0.930 0.표 2: 주문별 장기 예측 평가. 편집 거리를 척도로 사용하고 동사, 명사 및 동작 클래스에 대한 성과를 보고합니다. 동작 클래스는 동사와 명사 클래스의 조합입니다. 결과에 따르면 비디오의 다중 스케일 특성을 이해하는 법을 배우는 것이 정확하고 세분화된 예측을 하는 데 필수적입니다. 표 1에서 Ego4D, EK55 및 EK100에 대한 결과를 보고합니다. 자체 감독 비디오 사전 학습은 일반적으로 동작 인식의 강력한 감독 변형(표 1의 첫 번째 행)에 비해 장기 예측의 핵심 기능이 필요한 작업에 더 유익합니다. 사전 학습 중에 인간이 주석을 단 레이블이 필요하지 않음에도 불구하고 제안하는 MVP 방식은 Ego4D 작업 주석에서 미세 조정 시 강력한 감독 대응 방식보다 미래 동사 및 명사 예측에서 약 14%의 개선을 가져옵니다. 미래 클립 표현을 예측하는 학습 목표가 동작 예상에 필수적이라고 가정합니다. 또한 모든 데이터 세트에서 클립 불변 비디오 표현을 학습하는 최첨단 사전 학습 목표[24, 13]가 클립 시퀀스에 대한 효과적인 추론이 필요한 다운스트림 작업으로 일반화되지 않는다는 것을 관찰했습니다. 사실, 동일한 비디오에서 샘플링한 두 클립 시퀀스의 표현 간 유사성을 최대화하기 위해 앞서 언급한 사전 학습 목표를 확장하면 특히 클립의 더 긴 시간적 수평선에서 미래의 액션 예측이 크게 향상됩니다. 또한 MVP 접근 방식은 LSTCL[31]보다 상당한 마진으로 성능이 뛰어납니다(예: Ego4D에서 3-5% 향상). LSTCL은 짧은 클립 시퀀스의 비디오 표현에서 장기 시간적 단서를 인코딩하는 것을 목표로 하므로, 미래 클립 시퀀스의 맥락적 정보를 예측하는 방법을 학습하는 것이 장기 비디오 이해를 위한 효과적인 사전 학습 목표가 될 수 있음을 보여줍니다. 4.2.2 순서별 장기 예측 표 2는 특정 시간 단계에서 동작을 예측하는 더 어려운 작업에 대한 세 가지 데이터 세트에 대한 결과를 보고합니다. 섹션 4.2.1의 순서 미인식 작업에 대한 결과와 유사하게, 제안된 MVP 접근 방식이 정확한 세분화된 예측이 필요한 작업에 더 잘 일반화된다는 것을 관찰했습니다. DPC, CONSTCL 및 우리와 같은 세분화된 영역 수준에서 미래 클립 표현을 예측하는 방법을 학습하는 사전 학습 접근 방식은 CPC 및 CVRL을 포함한 미래 비디오 클립의 전역 표현을 예측하는 변형과 비교할 때 일반적으로 이 어려운 설정에서 더 나은 성능을 발휘합니다. 한 가지 가능한 이유는 미래에 세분화된 시공간적 영역 표현을 예측하는 것이 훨씬 더 어려운 목표이기 때문에 비디오 모델이 트리밍되지 않은 비디오에서 다양한 원자 동작의 구조를 이해해야 하기 때문입니다. 특히 세 가지 데이터 세트에 대한 우리의 성과는 미래 영역 수준 표현을 예측하는 방법을 학습하는 것이 동사 예측에 특히 중요하다는 것을 시사합니다. 이는 명사에 비해 미래 클립의 동사를 예측하는 데 있어 이러한 접근 방식이 달성한 훨씬 더 큰 개선 마진으로 입증됩니다. 예를 들어, MVP는 동사 및 명사 예측에서 각각 0.029 및 0.018만큼 편집 거리를 줄입니다. 순서 무관 작업과 대조적으로 MVP 목표에서 달성한 개선 사항이 더 작다는 것을 알 수 있으며, 이는 특정 타임스텝에서 정확하게 동작을 예측하는 것의 어려움을 더욱 강조합니다. 또한, 다양한 시간적 지평에서 비디오 클립에서 수집된 미래 맥락 정보를 예측하는 학습의 효과를 이해하는 것을 목표로 합니다. 특히, 관찰된 클립 시퀀스의 맥락을 고려하여 미래 비디오 클립 시퀀스의 세밀한 시공간 영역 표현을 재구성하는 것을 목표로 하는 CONSTCL[33]과 비교합니다. 사전 학습된 객체 감지기를 사용하여 위치 사전을 식별하지 않음에도 불구하고, 제안된 MVP 접근 방식은 동사와 명사 예측에서 CONSTCL보다 성능이 우수합니다(예: Ego4D에서 편집 거리를 0.008만큼 줄임). 동시에 밀도가 높은 시공간적 특징 맵만 사용합니다. 통합된 미래 시공간적 영역 표현을 예측하는 사전 학습 목표는 비디오 모델이 다양한 원자적 동작 간의 상관 관계와 비디오에서 포괄적인 목표에 어떻게 기여하는지에 대해 더 잘 추론하는 데 도움이 된다고 가정합니다. 4.2.3 비디오 요약 예측 마지막으로 표 3은 다중 모달 비디오 요약 예측 작업에 대한 결과를 보고합니다. 비디오 전용 작업 외에도 Ego4D에서 순서 인식하지 못하는 예측의 다운스트림 작업인 다중 사전 훈련 접근 방식 클립 동작 인식 감독 없음 R@1↑ R@5↑ R@10+ 강력 0.90 5.00 8.CPC [23] 예 자기 9.70 28.60 41.DPC [18] 예 자기 10.10 29.70 43.CVRL [24] 예 자기 11.00 34.80 49.LSTCL [31] 예 자기 12.70 38.90 53.CONSTCL [33] 예 자기 11.40 41.80 53.CVRL [24] 예 자기 MVP(저희) 예 자기 15.90 40.70 56.19.30 50.70 65. Ego4D에 대한 순서 인식 없는 장기 예측 평균 평균 정밀도(MAP) 동사 명사 표 3: Ego4D 데이터 세트에 대한 비디오 요약 예측. MVP는 비디오 모델이 텍스트 요약 검색의 다중 모드 작업에 대한 이전 작업보다 더 잘 일반화하는 더욱 강력한 표현을 학습하도록 돕습니다. 자체 감독 사전 학습 접근 방식이 강력하게 감독되는 행동 인식 작업보다 언어 모달리티를 포함하는 다운스트림 작업에 훨씬 더 잘 일반화한다는 점에 유의하십시오. 순서 인식 없는 특정 장기 예측의 이전 작업에 대한 결과와 달리 CVRL(단일 및 다중 클립) 및 LSTCL과 같은 클립 불변 비디오 표현을 학습하는 사전 학습 목표는 R@ 1 정확도에서 DPC보다 15%라는 상당한 마진으로 성능이 우수합니다. 이는 DPC 사전 학습 접근 방식이 비디오 모델을 학습하여 미래에 연속적인 비디오 클립의 표현을 예측하기 때문일 수 있다고 가설을 세웁니다. 이와 대조적으로, 앞서 언급한 접근 방식은 동일한 비디오에서 관찰되고 예측된 비디오 클립 시퀀스를 샘플링하지만 무작위로 결정된 시간에 샘플링합니다. 이를 통해 비디오 모델은 DPC 방법의 경우처럼 항상 즉각적인 미래를 예측하는 대신 맥락적 정보를 더 먼 미래로 외삽하는 법을 배우게 될 수 있습니다. 흥미롭게도 사전 학습 중에 세분화된 시공간적 영역 표현을 예측하는 법을 배우는 것이 이전 평가 작업만큼 비디오의 포괄적인 맥락을 이해하는 데 중요하지 않을 수도 있음을 관찰했습니다. 이는 여러 비디오 클립으로 사전 학습된 CVRL이 실제로 R@1 정확도에서 CONSTCL보다 4% 더 우수한 성능을 보인다는 사실에서 입증됩니다. 마지막으로, 제안된 MVP 접근 방식이 CVRL 클립 시퀀스, LSTCL 및 CONSTCL에 비해 R@1 정확도에서 약 3~8%의 성능 향상을 달성한 것은 여러 시간 척도에 걸쳐 집계된 미래 맥락적 정보에 대해 추론하는 법을 배우는 것이 모델이 전체 비디오의 의미를 외삽하는 데 특히 유용하다는 것을 시사합니다. ~ 4.2.4 절제 연구 우리는 MVP 접근법의 다양한 측면을 절제하여 학습된 표현의 견고성에 대한 상대적 기여도를 결정합니다. 구체적으로, 우리는 Ego4D의 45.0 47.60.0 62.50.0 52.5 55.0 57.MVP top-1 정확도에 대한 다양한 모델 변형의 표현의 효과를 비교합니다. 그림 4: MVP의 이점. 우리는 주문에 독립적인 장기 예측에서 자기 감독 사전 학습 예측 정확도와 평균 평균 정밀도 간의 관계를 연구합니다. 시간적 오프셋 K 동사↑23.명사 ↑ 21.평균↑ 22.27.15 26.26.27.26.27.26.25.26.27.26.기하학적 26.25.26.무작위(저희) 30.32.33 31.표 4: Ego4D의 시간적 오프셋 제거. 사전 학습 중 시간적 오프셋이 순서 인식 장기 예측의 하위 작업에 미치는 영향을 제거합니다. MVP의 효과. 사전 학습의 다양한 단계에서 체크포인트를 사용하여 사전 학습 중 비디오 모델의 예측 정확도와 하위 성능 간의 상관 관계를 연구하여 그림 4에서 다중 스케일 비디오 사전 학습 접근 방식의 이점을 평가합니다. MVP는 대조적 공식을 사용하지만 예측 정확도는 실제 정답과 가장 높은 유사성을 갖는 예측 영역의 백분율로 계산합니다. 사전 학습 중 예측 정확도와 모든 동사 및 명사 클래스에 대한 평균 mAP 점수 사이에 직접적인 상관 관계가 있음을 관찰했습니다. 이는 기본 표현에서 비디오의 다중 스케일 특성을 인코딩하는 방법을 배우는 것이 장기 예측 작업에 유익함을 시사합니다. 시간 오프셋 K. 표 4에서 사전 학습 중 K가 증가함에 따라 동사 및 명사 예측 정확도가 증가하는 것을 관찰했습니다. 이는 비디오 모델이 사전 학습 중에 문맥 정보에 대해 더 먼 미래에 추론하는 방법을 학습하여 미래 동작을 더 잘 예측할 수 있어야 하기 때문에 놀라운 일이 아닙니다. 그러나 12개 클립의 시간 오프셋을 사용하면 실제로 성능이 저하되는 것을 알 수 있습니다. 가능한 이유 중 하나는 미래가 비결정적이며 정보를 너무 먼 미래에 예측하면 사전 학습 중에 높은 수준의 노이즈가 발생하기 때문입니다. EMean MAP 집계 스케일 절제 평균 MAP # 예측 단계 = 4, 시간적 보폭 s = 평균 MAP # 입력 클립 = 4, 시간적 보폭 s = 집계 없음 단일 스케일 집계 다중 스케일 집계(저희)(a) # 입력 클립 (b) # 예측 클립 (c) 평균 MAP # 입력 클립 = 4, # 예측 단계 = 시간적 보폭 S (d) 그림 5: MVP의 절제. (a) 결과에 따르면 여러 시간 스케일에서 비디오의 시간적 역학을 모델링하는 방법을 배우는 것이 동작 예측에 필수적입니다. (b) 더 많이 관찰된 비디오 클립으로 더 많은 맥락을 제공하는 것은 일반적으로 더욱 견고한 표현을 학습하는 데 도움이 됩니다. (c) 예측 단계의 수를 늘리면 비디오 모델이 어느 정도 더 정확한 동작 예측을 하는 데 도움이 됩니다. (d) 여러 시간 스케일에 걸쳐 미래 클립 시퀀스의 맥락을 집계하기 위해 작은 시간적 보폭을 사용하는 것이 더 높은 값보다 더 유익합니다. 또한, 다양한 시간적 지평에 걸쳐 미래의 맥락적 정보를 예측하는 것을 학습하는 것은 일종의 정규화 역할을 하며 모델이 일정한 시간적 기간에 걸쳐 예측에 과적합되는 것을 방지하기 때문에 무작위 시간적 오프셋 값을 샘플링하는 것이 가장 효과적이라고 가정합니다.다중 스케일 이점.다운스트림 성능에 대한 사전 학습 중 다중 스케일 집계의 중요성을 조사합니다(그림 5(a)).특히, 미래 클립의 맥락화되지 않은 표현만 예측하는(집계 없음) MVP 변형과 맥락의 집계가 단일 스케일에 걸쳐 계산되는 또 다른 변형으로 비디오 모델을 학습합니다.시작하기 위해, 맥락화된 표현을 예측하는 것의 중요성을 관찰합니다.맥락화되지 않은 클립 표현을 예측하면 평균 mAP가 2 ~ % 감소합니다.더 중요한 것은 여러 시간 스케일에 걸쳐 집계된 미래 클립 표현을 예측하는 것을 학습하면 단일 시간 스케일에 걸쳐 맥락화된 표현만 예측하는 것보다 상당한 개선이 있다는 것을 알 수 있다는 것입니다. 이러한 결과는 다중 스케일 동작의 본질을 이해하는 법을 배우면 비디오 모델이 기본 목표를 더 잘 추론하고 미래 동작을 예상하는 데 도움이 된다는 가설을 뒷받침할 수 있습니다.입력 클립 수 아니요.그림 5(b)에서 사전 학습 중에 관찰된 시퀀스 V의 클립 수를 늘리면 일반적으로 다운스트림 성능이 향상됩니다.그러나 8개의 입력 클립을 사용하면 예측 결과가 떨어지는 것을 알 수 있습니다.한 가지 가능한 이유는 입력 클립을 더 많이 사용하면 관찰된 맥락이 더 많아져 사전 학습 목표의 어려움이 완화되고 결과적으로 다운스트림 예측 작업에 대한 학습된 표현의 견고성이 감소할 수 있기 때문입니다.예측 클립 수 Np 또한 그림 5(c)에서 다운스트림 예측 성능에 대한 사전 학습 중에 예측 클립 수를 변경하는 것의 중요성을 이해하고자 합니다.직관적으로 예측된 미래 클립의 수를 늘리면 비디오가 더 먼 미래의 맥락적 정보를 예측하는 법을 배워야 하기 때문에 MVP 목표의 어려움이 커집니다. 예측된 클립의 수를 늘리는 것이 일반적으로 다운스트림 성능에 도움이 되지만, 8개의 미래 클립을 예측하면 성능이 저하되는 것을 볼 수도 있습니다. 문맥화된 정보는 비결정적이기 때문에 너무 먼 미래로 예측하는 것이 너무 어려울 수 있다고 이론화합니다. 이는 사전 학습 중에 약간의 노이즈를 발생시켜 학습된 비디오 표현에 부정적인 영향을 미칠 수 있습니다. 집계를 위한 시간적 스트라이드 S. 마지막으로, 그림 5(d)에서 사전 학습 중에 시간적 스트라이드 S의 효과를 제거합니다. 시간적 스트라이드를 1에서 2로 늘릴 때 최상의 다운스트림 성능을 얻는데, 이는 시간적 스트라이드가 높을수록 비디오 모델이 장기적인 미래 문맥적 정보를 인코딩하는 법을 배우도록 장려한다는 것을 시사할 수 있습니다. 스트라이드가 클수록 실제로 성능이 상당히 저하되는 것으로 가정합니다. 이는 비디오 모델이 시간적으로 매우 먼 다른 원자적 동작 간의 구조와 관계를 이해하는 법을 배우는 것이 너무 어려울 수 있기 때문입니다. 4.3. 제한 사항 MVP의 대상 표현은 다양한 시간 척도에 대해 고정된 시간적 스트라이드를 사용하여 미래 클립에 대한 정보를 집계하여 계산됩니다. 그러나 이는 다양한 복잡한 동작이 다양한 수의 원자 동작으로 구성될 수 있기 때문에 항상 현실적이지 않을 수 있습니다. 5.
--- CONCLUSION ---
요약하자면, 다운스트림 장기 예측 작업을 위한 견고한 비디오 표현을 학습하는 것을 목표로 하는 자기 감독 방식인 Multiscale Video Pretraining을 소개합니다. 관찰된 비디오 클립 시퀀스가 주어지면, 여러 시간 척도에 걸쳐 미래 클립의 집계된 표현을 예측하도록 비디오 모델을 학습합니다. 미래 맥락적 정보를 인코딩하는 법을 학습하면 비디오 모델이 이전 작업보다 장기 예측 작업에 더 잘 일반화하는 데 도움이 된다는 것을 경험적으로 입증하며, 이는 장기 비디오 이해에 대한 다중 스케일 사전 학습의 중요성을 강조합니다. 마지막으로, 우리는 광범위한 절제 연구를 통해 MVP의 다양한 측면에 대한 핵심 통찰력을 추출합니다. 이는 다중 스케일 비디오 표현 학습에 대한 추가 연구에 도움이 되기를 바랍니다. 향후 작업을 위한 몇 가지 흥미로운 경로에는 동작 인식 및 텍스트-비디오 검색과 같은 다른 비디오 및 다중 모달 작업에 대한 이러한 표현의 기능을 추가로 탐색하는 것이 포함될 수 있습니다. 감사의 말: 이 자료는 DARPA의 계약 번호 HR00112020054에 따라 부분적으로 지원된 작업을 기반으로 합니다. 실험을 위한 컴퓨팅 인프라를 설정하는 데 도움을 준 Gideon Stocek과 Nishanth Alapati에게 감사드리고 싶습니다. 참고문헌 [1] Gedas Bertasius, Heng Wang, Lorenzo Torresani. Is space-time attention all you need for video understanding? ICML, 2권, 4페이지, 2021년. [2] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, Andrew Zisserman. kinetics600에 대한 간략한 메모. arXiv 사전 인쇄본 arXiv:1808.01340, 2018. [3] Joao Carreira, Eric Noland, Chloe Hillier, Andrew Zisserman. kinetics-700 인간 행동 데이터 세트에 대한 간략한 메모. arXiv 사전 인쇄본 arXiv:1907.06987, 2019. [4] Ting Chen, Simon Kornblith, Mohammad Norouzi 및 Geoffrey Hinton. 시각적 표현의 대조 학습을 위한 간단한 프레임워크. 기계 학습에 관한 국제 컨퍼런스에서, 1597-1607페이지. PMLR, 2020. [5] Xinlei Chen, Haoqi Fan, Ross Girshick 및 Kaiming He. 모멘텀 대조 학습을 통한 개선된 기준선. arXiv 사전 인쇄본 arXiv:2003.04297, 2020. [6] Xinlei Chen 및 Kaiming He. 간단한 샴 표현 학습 탐색. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 15750-15758, 2021. [7] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price 및 Michael Wray. 자기중심적 비전 확장: epickitchens 데이터세트. 컴퓨터 비전에 관한 유럽 회의(ECCV), 2018. [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price 등. 자기 중심적 비전을 재조정합니다. arXiv 사전 인쇄본 arXiv:2006.13256, 2020. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. [10] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer. 다중 스케일 비전 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 6824-6835페이지, 2021. [11] Yazan Abu Farha, Qiuhong Ke, Bernt Schiele, Juergen Gall. 주기적 일관성을 갖춘 활동에 대한 장기적 예상. arXiv 사전 인쇄본 arXiv:2009.01142, 2020. [12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He. 비디오 인식을 위한 Slowfast 네트워크. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 6202-6211페이지, 2019. [13] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He. 비지도 시공간 표현 학습에 대한 대규모 연구. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3299-3309페이지, 2021. [14] Antonino Furnari 및 Giovanni Maria Farinella. 1인칭 비디오에서 액션 예상을 위한 Rollingunrolling Istms. IEEE 패턴 분석 및 머신 인텔리전스 거래, 43(11):4021-4036, 2020. [15] Rohit Girdhar 및 Kristen Grauman. 예상 비디오 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 13505-13515페이지, 2021. [16] Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho. 장기적 행동 예상을 위한 미래 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 30523061페이지, 2022. [17] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: 3,000시간 분량의 자기중심적 비디오로 전 세계를 여행하다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18995-19012페이지, 2022년. [18] Tengda Han, Weidi Xie, Andrew Zisserman. 고밀도 예측 코딩을 통한 비디오 표현 학습. IEEE/CVF 컴퓨터 비전 워크숍 국제 컨퍼런스 회의록, 0-0페이지, 2019년. [19] Tengda Han, Weidi Xie, Andrew Zisserman. 비디오 표현 학습을 위한 메모리 증강 고밀도 예측 코딩. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 3부 16, 312-329페이지. Springer, 2020. [20] Allan Jabri, Andrew Owens, Alexei Efros. 대조적 무작위 걷기로서의 공간-시간 대응. 신경 정보 처리 시스템의 발전, 33:19545-19560, 2020. [21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 운동학 인간 행동 비디오 데이터 세트. arXiv 사전 인쇄본 arXiv:1705.06950, 2017. [22] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, Thomas Serre. Hmdb: 인간 동작 인식을 위한 대규모 비디오 데이터베이스. 2011년 컴퓨터 비전 국제 컨퍼런스, 2556-2563쪽. IEEE, 2011. [23] Aaron van den Oord, Yazhe Li 및 Oriol Vinyals. 대조 예측 코딩을 통한 표현 학습. arXiv 사전 인쇄본 arXiv:1807.03748, 2018. [24] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie 및 Yin Cui. 시공간적 대조 비디오 표현 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6964-6974페이지, 2021. [25] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Pătrăucean, Florent Altché, Michal Valko 등. 자기 감독 비디오 학습에 대한 견해를 넓히세요. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 1255-1265페이지, 2021. [26] Fadime Sener, Dipika Singhania, Angela Yao. 장거리 비디오 이해를 위한 시간적 집계 표현. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, Part XVI 16, 154-171페이지. Springer, 2020. [27] Dian Shao, Yue Zhao, Bo Dai, Dahua Lin. Finegym: 세분화된 동작 이해를 위한 계층적 비디오 데이터 세트. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 2616-2625, 2020. [28] Khurram Soomro, Amir Roshan Zamir 및 Mubarak Shah. Ucf101: 야생 동영상의 101개 인간 행동 클래스로 구성된 데이터 세트입니다. arXiv 사전 인쇄 arXiv:1212.0402, 2012. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. 신경 정보 처리 시스템의 발전, 30, 2017. [30] Carl Vondrick, Hamed Pirsiavash 및 Antonio Torralba. 라벨이 지정되지 않은 비디오의 시각적 표현을 예상합니다. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 98-106페이지, 2016년. [31] Jue Wang, Gedas Bertasius, Du Tran, Lorenzo Torresani. 비디오 변환기의 장단기 시간적 대조 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 14010-14020페이지, 2022년. [32] Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, Fei Wu. 상상력을 통해 자기중심적 행동을 예상하는 법 배우기. IEEE 이미지 처리 저널, 30:1143– 1152, 2020년. [33] Liangzhe Yuan, Rui Qian, Yin Cui, Boqing Gong, Florian Schroff, Ming-Hsuan Yang, Hartwig Adam, Ting Liu. 영어: 자기 감독을 통한 맥락화된 시공간 대조 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 13977-13986페이지, 2022년. [34] Luowei Zhou, Chenliang Xu, Jason Corso. 웹 교육 비디오에서 절차를 자동으로 학습하기 위해. AAAI 인공 지능 컨퍼런스 회의록, 32권, 2018년. 이 보충 자료에서 우리는 주요 제출물에 다음과 같은 추가 자료를 제공합니다. A. 훈련 및 평가 데이터 세트 세부 정보 B. 구현 세부 정보 C. 시공간적 구성적 손실 공식화 D. 비교를 위한 기준 모델 A. 데이터 세트 Ego4D[17]는 가정에서 야외 여가 상황에 이르기까지 일상 생활 활동 3600시간 이상을 아우르는 자기중심적 비디오의 가장 큰 데이터 세트입니다. 이 비디오는 9개국에서 온 931명의 카메라 착용자가 수집한 것으로, 다양한 설정에서 일상 활동에 참여하는 동안 대본 없는 상호작용을 기록합니다.기존 비디오 인식 데이터 세트와 달리 Ego4D의 비디오는 일반적으로 Kinetics 400/600의 10초 비디오 클립과 비교하여 1~10시간에 걸쳐 있기 때문에 지속 시간이 훨씬 깁니다[2, 3].또한 Epic-Kitchens 55/100[7, 8]과 같은 기존의 자기중심적 비디오 데이터 세트보다 규모와 활동의 다양성이 훨씬 큽니다.또한 각 비디오에는 사람이 자세히 주석을 달아 비디오의 주목할 만한 상호작용과 요점 요약을 설명하는 주석을 제공합니다.이 데이터 세트는 오디오-비주얼 일기 및 예측과 같은 다양한 다운스트림 작업에서 탐색과 추가 연구를 용이하게 합니다.제공된 주석을 사용하여 장기 예측과 비디오 요약 예측에 대한 제안된 MTPL 접근 방식을 평가합니다. 우리는 Grauman et al. [17]과 동일한 분할을 목표 작업에 대한 훈련 및 평가에 채택합니다. 이 데이터 세트에서 테스트 평가는 챌린지 포털에 제출하여 보류된 세트에서 수행되므로 훈련 및 검증 분할에 대한 평가를 수행합니다. 또한 제공된 3개 분할 모두에 존재하는 동사 및 명사 클래스의 수가 일관되지 않은데, 각 분할에는 다른 분할에 없는 동사 및 명사 클래스가 포함되어 있기 때문입니다. 자세한 내용은 보충 자료를 참조하십시오. EpicsKitchen-55/100. EpicKitchens-100(EK100) [8]은 또 다른 대규모 자기중심적 비디오 데이터 세트입니다. Ego4D와 유사하게 약 100시간 분량의 700개의 긴 대본 없는 자기중심적 비디오도 제공합니다. 참가자가 주방에서 일상적인 활동에만 참여하기 때문에 Ego4D보다 다양성이 떨어집니다. EpicKitchens-55(EK55) [7]는 EK100의 초기 버전이며 규모가 작지만 동일한 유형의 비디오와 주석을 제공합니다.우리는 EK55와 EK100을 사용하여 순서 무관 및 순서별 장기 예측 작업을 평가합니다.B. 구현 세부 정보B.1. 다중 스케일 비디오 사전 학습우리는 Pytorch 딥 러닝 라이브러리를 사용하여 모든 모델과 실험을 구현합니다.우리는 다중 스케일 비전 트랜스포머(MVIT) [10]를 기본 비디오 인코더로 사용하고 1개의 어텐션 헤드가 있는 1개의 트랜스포머 인코더 레이어를 시간적 맥락 집계기로 사용합니다.MVIT 인코더는 일반적으로 16개 프레임의 비디오 클립을 입력으로 받아들이고 분류 토큰의 맥락화된 출력인 글로벌 클립 표현을 출력합니다.그러나 우리의 경우 메모리 제약으로 인해 클립당 프레임 수를 8개로 줄였습니다.또한 사전 학습 중에 분류 토큰을 버리고 시공간적 영역 세분성에서 미래의 기능 예측을 수행합니다. 두 번째 단계의 미세 조정에서 우리는 시공간 영역 표현에 대한 평균 풀링을 수행하여 글로벌 클립 표현을 계산합니다. 초당 10프레임(FPS)으로 비디오 프레임을 샘플링하므로 각 클립의 시간적 지속 시간은 약 0.8초입니다. 각 입력 비디오 클립은 프레임의 높이를 248~280픽셀 사이에서 무작위로 조정하고 224 x픽셀을 잘라내어 사전 처리합니다. Ego4D 데이터 세트에 대한 사전 학습의 첫 번째 단계에서는 무작위 수평 뒤집기 및 색상 지터를 포함하여 비디오 클립에 무작위 증강을 수행합니다. 미래 특징 예측 함수는 비선형 ReLU 연산과 768의 은닉 차원을 가진 2층 다층 퍼셉트론(MLP)으로 표현됩니다. B.2. 다운스트림 장기 예측 작업 그림 6은 사전 학습된 비디오 모델과 학습된 표현이 순서 무관 및 순서별 동작 예측과 비디오 요약 예측으로 어떻게 전환되는지 보여줍니다. 시작으로, 각 작업 V = {V₁, ···VNv }에서 관찰된 Ny개의 비디오 클립 시퀀스가 주어지면, 다음과 같이 마지막 타임스텝의 맥락화된 표현을 추출합니다.ZNv = ho(90(Vz)), ZNv ЄRD (6) 여기서 D는 출력 채널 차원입니다.모든 다운스트림 작업의 경우, 고정된 상태로 유지되는 사전 학습된 비디오 모델 위에 선형 프로브를 미세 조정합니다.순서 독립적인 액션 예측.및 Nnoun 클래스의 어휘가 주어지면, 다음과 같이 Nverb 차원 및 Nnoun 차원 이진 벡터를 예측합니다.Nverb Pverb=fverb (Ny), fnoun (NV), Pnoun (7) 여기서 예측된 벡터의 각 차원은 미래에 동사 또는 명사 클래스가 발생할 확률을 나타냅니다.순서 독립적인 장기 예측 | 동사 클래스 수 예측 헤드 명사 클래스 수 비디오 모델 순서별 장기 예측 동사: &quot;cut&quot; 명사: &quot;banana&quot; 동사: &quot;pour&quot; 명사: &quot;milk&quot; 예측 헤드 예측 헤드 비디오 모델 비디오 요약 예측 예측 동사: &quot;wash&quot; 명사: &quot;spoon&quot; 비디오 요약 표현 예측 헤드 N 텍스트 요약 표현 시각적 의미 투영 시각적 의미 투영 비디오 모델 언어 모델 요약: &quot;카메라 착용자는 스무디를 만들기 전에 설거지를 합니다...&quot; 관찰된 비디오 클립 시퀀스 관찰된 비디오 클립 시퀀스 관찰된 비디오 클립 시퀀스 그림 6: 다운스트림 장기 예측 작업 구현. 우리는 강력한 감독을 통해 대상 데이터 세트에 대한 비디오 요약 예측뿐만 아니라 순서 무관 및 순서별 행동 예측의 다운스트림 작업에 대한 사전 학습된 비디오 모델을 미세 조정합니다. 우리는 이것을 다중 레이블 예측 작업으로 공식화하고 모든 동사와 명사에 대해 계산된 이진 교차 엔트로피 손실을 최적화하여 모든 사전 학습된 모델을 미세 조정합니다. 클래스: 2T+K+S 예측 표현 В Nverb Nnoun L = = -Σ verb, b,¿ log (Pverb,b,i ) + Σ Ynoun,b,i ¿ log(Pnoun,b,¿)), 시간적 부정 b=1 i=i=(8) 긍정적 부정 시간적 공간적 부정 기준 진실 표현 여기서 Yverb,b,i 및 Ynoun,b,i는 각각 기준 진실 동사 및 명사 이진 레이블입니다. 순서별 동작 예측. 이 보다 어려운 설정에서 목표는 특정 시간 단계에서 세분화된 동작 예측을 하는 것입니다. 단순화를 위해 [17]과 동일한 교육 및 평가 설정을 채택하고 다른 시간 단계에 대해 별도의 예측 헤드를 사용합니다. 각 시간 단계에 대해 하위 작업을 동사와 명사 모두에 대한 다중 클래스 예측 문제로 공식화합니다. 따라서 사전 학습된 비디오 모델을 다음 손실 공식을 사용하여 미세 조정합니다.LB Np -&gt;&gt; (verb, b, t log (Pverb, b, t)+(Ynoun, b, t log(Pnoun, b, t)). b=1 t=(9) 비디오 요약 예측. 그림 6(오른쪽)에서 볼 수 있듯이 이 멀티모달 작업을 처리하기 위해 듀얼 인코더 아키텍처를 채택합니다. CLIP 및 ALIGN을 포함한 공동 시각 언어 표현을 학습하는 이전 작업과 유사하게 최종 비디오와 언어 표현 간의 의미적 유사성이 최종 내적 연산을 사용하여 계산되는 후기 융합 메커니즘도 사용합니다. I | I ZT+K ZT+K+S ----ZT+K+2S 그림 7: 시공간 영역 예측. MVP 접근 방식은 세분화된 시공간 영역에 포함된 미래의 맥락적 정보를 예측하도록 비디오를 학습합니다. C. 시공간 구성적 손실 공식 제안된 MVP 목표가 그림 7의 대조 손실 공식을 사용하여 세분화된 시공간 영역 표현을 예측하는 비디오 모델. t번째 타임스텝 2t,j에서 j번째 공간 영역의 예측 표현이 주어지면, 실제 집계 표현 zt,와의 의미적 유사성을 최대화하는 것을 목표로 합니다. 그리고 전체 방해 요소 세트의 부정적 샘플은 동일한 타임스텝의 다른 공간 영역과 같은 하드 네거티브와 샘플링된 배치의 다른 비디오의 클립에 속하는 표현을 포함하는 쉬운 네거티브로 구성됩니다. D. 기준 모델 우리는 평가에서 제안한 MVP 목표를 비교하는 자체 감독 비디오 사전 학습 기준선을 간략히 설명합니다. 대조 예측 코딩(CPC). 대조 예측 코딩(CPC) [23] 접근 방식은 비디오의 다른 클립 간에 공유되는 전역 정보를 인코딩하는 비디오 표현을 학습하는 것을 목표로 합니다. CPC는 관찰된 클립 시퀀스의 컨텍스트를 사용하여 관찰된 시퀀스 바로 뒤에 오는 미래 클립의 미래 비문맥화 정보를 예측합니다. 또한 예측을 시도하는 다양한 타임스텝의 표현에 대해 여러 예측 헤드를 사용합니다.밀집 예측 코딩(DPC).밀집 예측 코딩(DPC)[18] 접근법은 CPC를 기반으로 하여 문맥화되지 않은 정보를 예측하는 비디오 표현을 학습하지만 주어진 타임스텝에 대한 예측을 이전 타임스텝의 예측 정보의 맥락에 따라 조건을 지정합니다.또한 CPC와 달리 DPC 목적은 전역 클립 표현 대신 시공간 표현을 계산하는 것을 목표로 합니다.대조적 비디오 표현 학습(CVRL).또한 MVP를 인기 있는 이미지 기반 자기 감독 사전 학습 목표[4, 5, 6]에서 영감을 받은 대조적 비디오 표현 학습(CVRL)[24] 접근법과 비교합니다.CVRL은 동일한 비디오에서 무작위로 샘플링된 다양한 클립의 표현 간 유사성을 최대화하도록 비디오 모델을 학습합니다. 우리는 비디오 클립 쌍을 사용하는 바닐라 설정의 CVRL과 비교하는 동안, 클립 시퀀스 쌍의 표현 간 유사성을 최대화하는 CVRL의 변형도 훈련하고 평가합니다.장단기 대조 학습(LSTCL). CVRL 접근 방식과 유사하게, 장단기 대조 학습(LSTCL)[31]은 처음에 비디오 클립 쌍의 표현 간 유사성을 최대화하여 비디오 표현을 학습하도록 제안되었습니다. 사전 훈련 동안, 짧은 클립과 이전에 없는 시간 정보가 포함된 또 다른 긴 클립을 입력으로 받습니다. LSTCL은 작은 관찰된 시간 창에서 과거와 미래 정보를 외삽하도록 비디오 모델을 훈련합니다. 또한, 사전 훈련 동안 샘플당 동일한 총 비디오 클립 수를 가진 비디오 클립 시퀀스 쌍에 대해 훈련하도록 LSTCL을 확장하여 공정한 비교를 용이하게 합니다. 자기 감독을 통한 맥락화된 시공간 대조 학습(CONSTCL). 마지막으로, 우리는 또한 자기 감독을 통한 맥락화된 시공간 대조 학습(CONSTCL) [33] 접근 방식과 비교합니다.CONSTCL은 CVRL 목표에 의해 강제되는 시공간 불변성 [13]의 한계를 해결하는 것을 목표로 합니다.CONSTCL 목표는 첫 번째 시퀀스의 맥락이 주어지면 비디오 표현을 한 클립 시퀀스에서 다른 클립 시퀀스로 변환하도록 비디오 모델을 훈련하는 영역 기반 사전 학습 작업을 활용합니다.
