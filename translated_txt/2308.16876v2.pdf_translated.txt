--- ABSTRACT ---
인간 중심 비디오 프레임 보간은 엔터테인먼트 경험을 향상시키고 스포츠 분석 산업에서 상업적 응용 프로그램을 찾는 데 큰 잠재력이 있습니다(예: 슬로우 모션 비디오 합성). 커뮤니티에서 비디오 프레임 보간에 사용할 수 있는 여러 벤치마크 데이터 세트가 있지만, 그 중 어느 것도 인간 중심 시나리오에 전념하지 않습니다. 이러한 격차를 메우기 위해 YouTube에서 얻은 130,000개 이상의 고해상도(≥720p) 슬로우 모션 스포츠 비디오 클립을 특징으로 하는 벤치마크인 Sports SloMo를 소개합니다. 이 벤치마크에서 여러 최첨단 방법을 다시 학습시켰고, 다른 데이터 세트에 비해 정확도가 눈에 띄게 감소하는 것을 관찰했습니다. 이는 벤치마크의 어려움을 강조하며 인체는 매우 변형되기 쉽고 스포츠 비디오에서 폐색이 빈번하기 때문에 가장 성능이 좋은 방법에도 상당한 과제를 제기한다는 것을 시사합니다. 이러한 과제를 해결하기 위해 파노라마 설정에서 인간 분할을 위한 보조 감독과 주요 지점 감지를 추가하는 인간 인식 손실 항목을 제안합니다. 이러한 손실 항목은 모델에 구애받지 않으며 모든 비디오 프레임 보간 접근 방식에 쉽게 연결할 수 있습니다. 실험 결과는 제안된 인간 인식 손실 항목의 효과를 검증하여 기존 모델에 비해 일관된 성능 향상을 이룹니다. 데이터 세트와 코드는 https://neuvi.github.io/SportsSlomol에서 찾을 수 있습니다. 1.
--- INTRODUCTION ---
비디오 프레임 보간(VFI)은 입력 이미지에서 중간 프레임을 합성하여 그렇지 않으면 보기 어려울 수 있는 콘텐츠의 선명도를 향상시키는 기술입니다. 이 기술은 슬로우 모션 비디오 생성[27], 새로운 뷰 합성[39], 비디오 압축[73], 만화 및 렌더링을 포함한 광범위한 응용 프로그램을 찾습니다. * 주로 Jiaben Chen이 Northeastern University에서 인턴으로 있을 때 수행한 작업. (a) (b) (c) (d) der ader 그림 1: 인간 중심 비디오 프레임 보간 결과. 우리는 동작 경계에서 보간 정확도를 개선하기 위해 인간 인식 보조 손실을 제안합니다. 왼쪽에서 오른쪽으로: (a): 오버레이된 입력, (b): 기준 진실, (c): 보간 결과, (d): 제안한 인간 인식 손실 항목이 포함된 보간 결과. 콘텐츠 생성[37, 4] 등. 최근 몇 년 동안 우리는 다양한 벤치마크[1, 65, 57, 66, 79, 14, 64]의 개발로 인해 이 분야에서 상당한 진전을 목격했습니다. 인간은 대부분의 현대 비디오에서 두드러지게 등장합니다. 모바일 기기의 광범위한 사용으로 사람들은 일상의 경험을 가족, 친구 및 동료와 쉽게 녹화하고 공유할 수 있습니다. 한편, 스포츠 경기의 라이브 방송은 많은 시청자를 끌어들입니다. 자동으로 생성된 슬로우 모션 비디오는 실시간으로 놓칠 수 있는 세부 사항과 삶의 귀중한 순간을 강조하여 사용자에게 더욱 몰입적이고 매력적인 경험을 제공할 수 있습니다. 따라서 인간 중심 비디오의 비디오 프레임 보간 결과를 개선하는 것은 엔터테인먼트에서 사용자 경험을 향상시키는 데 큰 잠재력이 있습니다. 인간 중심 비디오 프레임 보간 접근 방식은 다양한 산업에서도 유익할 수 있습니다. 예를 들어 운동 선수와 코치는 슬로우 모션 합성을 사용하여 기술의 결함을 파악하고 개선 영역을 강조하며 다양한 요소가 성공에 어떻게 기여할 수 있는지에 대한 보다 자세한 이해를 얻을 수 있습니다. 새로운 벤치마크. 다양한 COLD 인간 주요 포인트 분할 마스크를 사용할 수 있음에도 불구하고 표 1: 비디오 프레임 보간을 위한 다양한 벤치마크 데이터 세트 비교. (#inter. 프레임은 합성할 중간 프레임의 수를 나타냅니다.) 데이터 세트 #클립 #이미지 #inter. 프레임 해상도 인간 중심 UCF101 [65] 0.4K 1K256xAdobe240fps [66] 0.1K Vimeo90K [79] 70K SNU-FILM [14] |1.2K 3.6K X4K1000FPS [64] 4.4K 286K SportsSloMo 130K 1183K 80K1280×720 ☑ 220K448×256 ☑1280×720 ☑4096×2160 X1280×720 ✓ 그림 2: 인간의 주요 포인트[78]와 파노라마 분할 마스크[10]의 시각화. 비디오 프레임 보간의 벤치마크에서 인간 중심 시나리오에 특별히 맞춤화된 데이터 세트에는 상당한 차이가 있습니다. 이러한 격차를 메우고 이 중요한 방향으로 연구를 촉진하기 위해, Common Creative Licence에 따라 YouTube에서 크롤링한 고해상도(≥720p) 슬로우 모션 스포츠 비디오로 구성된 새로운 데이터 세트인 SportsSloMo를 소개합니다. 이 데이터 세트는 축구, 농구, 야구, 하키 등 다양한 스포츠를 포함합니다. 비디오에는 광고, 전환 프레임, 샷 변경 및 슬로우 모션이 아닌 콘텐츠가 포함될 수 있으므로 이러한 원치 않는 콘텐츠를 제거하기 위해 데이터를 신중하게 큐레이션하고 마지막으로 각 긴 비디오를 9개 프레임의 짧은 슬로우 모션 클립 세트로 분할합니다. 첫 번째와 마지막 프레임은 입력으로 사용되고 나머지 7개의 중간 프레임은 VFI 모델을 훈련하고 평가하기 위한 기준 진실로 예약됩니다. 벤치마크에는 총 130,000개의 비디오 클립과 100만 개 이상의 비디오 프레임이 있습니다. 표 1에 표시된 기존 데이터 세트와 비교할 때, 제안된 SportsSloMo 벤치마크는 고해상도와 인간 중심 시나리오에 초점을 맞춘 지금까지 가장 큰 벤치마크입니다. 이 논문에서는 주로 인간 중심 VFI를 위해 설계되었지만, SportsSloMo 데이터 세트는 비디오 초고해상도[6, 38], 그룹 활동 인식[67, 83, 81], 동적 뷰 합성[39, 17, 75]과 같은 다른 과제에 대한 연구를 지원할 잠재력이 있다고 생각합니다. 이 데이터 세트를 전체 커뮤니티에 공개함으로써 인간 중심 비디오 프레임 보간의 기술적 발전을 촉진하고 연구자들이 다른 인접 분야에서 혁신적인 응용 프로그램을 탐색할 수 있기를 바랍니다. 기존 접근 방식 벤치마킹. 인간 중심 비디오 프레임 보간 방법의 개발 및 평가를 용이하게 하기 위해, Sports SloMo 데이터 세트에서 공개적으로 공개된 코드를 사용하여 여러 최첨단 접근 방식[27, 36, 23, 25, 29, 28, 82]을 재교육합니다. 인체는 변형되기 쉽고 스포츠 비디오에서는 폐색이 빈번하기 때문에 모든 방법의 정확도는 다른 데이터 세트에서의 성능에 비해 떨어집니다. 예를 들어, SportsSloMo에서 가장 성능이 좋은 두 가지 접근 방식인 EBME[29]와 EMA-VFI[82]는 각각 30.과 30.70의 PSNR 점수를 생성합니다. 이는 Viemo90K[79]의 36.19와 36.64, SNU-FILM[14] 벤치마크의 하드 스플릿에서의 30.64와 30.94보다 현저히 낮습니다. 이는 벤치마크의 어려움을 강조하고 해결해야 할 중요한 과제가 있음을 시사합니다. 인간 중심 VFI를 위한 모델 개선. 벤치마크에서 기존 VFI 모델을 개선하기 위해 인간 인식 사전 확률을 도입하여 모델 학습을 개선합니다. 구체적으로, 파노라마 설정에서의 인간 분할[10]과 중간 프레임 합성을 위한 추가 감독으로 인간 키포인트 추정[78]에 기반한 손실 항목을 제안합니다. 그림 2는 데이터 세트에서 감지된 인간 키포인트와 분할 마스크를 시각화한 것입니다. 인간 분할 마스크는 인체 경계를 구분하는 데 도움이 되며, 이는 동작 경계 주변의 고스트 효과를 줄이는 데 도움이 됩니다. 동시에, 인간 키포인트 추정은 각 신체 부위가 어디에 있는지 나타낼 수도 있어 합성된 비디오 프레임에서 일관된 동작 궤적을 적용합니다. 구체적으로, 우리는 합성된 중간 프레임과 실제 중간 프레임으로 입력을 받는 사전 훈련된 파노라마 분할 및 키포인트 감지 모델의 출력을 비교하고 일관성을 감독으로 사용합니다. 그림 1에서 볼 수 있듯이, 제안된 인간 인식 손실 항으로 감독함으로써 큰 동작과 폐색이 있는 시나리오에서 흐릿한 결과가 덜한 동작 경계에서 보간 품질을 개선합니다. 이 두 인간 인식 손실 항은 모두 모델에 독립적이며 모든 비디오 프레임 보간 접근 방식에 쉽게 통합할 수 있습니다. 실험 결과에 따르면 기존 접근 방식 7가지의 정확도를 지속적으로 개선하여 벤치마크에서 강력한 기준을 도출할 수 있습니다. 요약하자면, 이 논문은 다음과 같은 기여를 합니다. • 우리는 많은 양의 슬로우모션 스포츠 비디오로 구성된 새로운 벤치마크 데이터 세트인 SportsSloMo를 소개합니다. 우리가 아는 한, 이것은 인간 중심 비디오 프레임 보간에 맞춰진 최초의 고해상도 데이터 세트로, 여러 중간 프레임의 합성을 지원합니다. • 우리는 새로운 벤치마크에서 최첨단 접근 방식을 벤치마킹하여 인간 중심 비디오 프레임 보간 작업의 과제를 강조합니다. • 우리는 비디오 프레임 보간에 대한 인간 사전 확률을 고려하고 기존 비디오 프레임 보간 접근 방식에 쉽게 연결할 수 있는 두 가지 인간 인식 손실 항을 제안합니다. 실험 결과는 이것이 기존 모델을 지속적으로 개선하여 새로운 벤치마크에서 강력한 기준 모델을 생성할 수 있음을 검증합니다. 2.
--- RELATED WORK ---
2.1. 벤치마크 데이터세트 기존의 공개적으로 사용 가능한 데이터세트는 이미 비디오 프레임 보간을 개발하고 평가하기 위한 귀중한 리소스를 제공합니다.
--- METHOD ---
벤치마크에서 s를 사용했고, 다른 데이터 세트에 비해 정확도가 눈에 띄게 감소한 것을 관찰했습니다. 이는 벤치마크의 어려움을 강조하며, 인체는 매우 변형되기 쉽고 스포츠 비디오에서 폐색이 빈번하기 때문에 가장 성능이 좋은 방법에도 상당한 과제를 제기한다는 것을 시사합니다. 이러한 과제를 해결하기 위해, 파노라마 설정에서 인간 분할을 위한 보조 감독과 키포인트 감지를 추가하는 인간 인식 손실 항목을 제안합니다. 이러한 손실 항목은 모델에 독립적이며 모든 비디오 프레임 보간 방식에 쉽게 연결할 수 있습니다.
--- EXPERIMENT ---
모든 결과는 제안된 인간 인식 손실 항목의 효과를 검증하여 기존 모델에 비해 일관된 성능 향상을 이룹니다. 데이터 세트와 코드는 https://neuvi.github.io/SportsSlomol에서 찾을 수 있습니다. 1. 소개 비디오 프레임 보간(VFI)은 입력 이미지에서 중간 프레임을 합성하여 그렇지 않으면 보기 어려울 수 있는 콘텐츠의 선명도를 향상시키는 기술입니다. 이 기술은 슬로우 모션 비디오 생성[27], 새로운 뷰 합성[39], 비디오 압축[73], 만화 및 렌더링을 포함한 광범위한 응용 프로그램을 찾습니다. * 주로 Jiaben Chen이 Northeastern University에서 인턴으로 있을 때 수행한 작업. (a) (b) (c) (d) der ader 그림 1: 인간 중심 비디오 프레임 보간 결과. 동작 경계에서 보간 정확도를 개선하기 위해 인간 인식 보조 손실을 제안합니다. 왼쪽에서 오른쪽으로: (a): 오버레이된 입력, (b): 기준 진실, (c): 보간 결과, (d): 제안된 인간 인식 손실 항목이 적용된 보간 결과. 콘텐츠 생성[37, 4] 등. 최근 몇 년 동안 우리는 다양한 벤치마크[1, 65, 57, 66, 79, 14, 64]의 개발로 인해 이 분야에서 상당한 진전을 목격했습니다. 인간은 대부분의 현대 비디오에서 두드러지게 등장합니다. 모바일 기기의 광범위한 사용으로 사람들은 일상의 경험을 가족, 친구 및 동료와 쉽게 녹화하고 공유할 수 있습니다. 한편, 스포츠 경기의 라이브 방송은 많은 시청자를 끌어들입니다. 자동으로 생성된 슬로우 모션 비디오는 실시간으로 놓칠 수 있는 세부 사항과 삶의 귀중한 순간을 강조하여 사용자에게 더욱 몰입적이고 매력적인 경험을 제공할 수 있습니다. 따라서 인간 중심 비디오의 비디오 프레임 보간 결과를 개선하는 것은 엔터테인먼트에서 사용자 경험을 향상시키는 데 큰 잠재력이 있습니다. 인간 중심 비디오 프레임 보간 접근 방식은 다양한 산업에서도 유익할 수 있습니다. 예를 들어 운동 선수와 코치는 슬로우 모션 합성을 사용하여 기술의 결함을 파악하고 개선 영역을 강조하며 다양한 요소가 성공에 어떻게 기여할 수 있는지에 대한 보다 자세한 이해를 얻을 수 있습니다. 새로운 벤치마크. 다양한 COLD 인간 주요 포인트 분할 마스크를 사용할 수 있음에도 불구하고 표 1: 비디오 프레임 보간을 위한 다양한 벤치마크 데이터 세트 비교. (#inter. 프레임은 합성할 중간 프레임의 수를 나타냅니다.) 데이터 세트 #클립 #이미지 #inter. 프레임 해상도 인간 중심 UCF101 [65] 0.4K 1K256xAdobe240fps [66] 0.1K Vimeo90K [79] 70K SNU-FILM [14] |1.2K 3.6K X4K1000FPS [64] 4.4K 286K SportsSloMo 130K 1183K 80K1280×720 ☑ 220K448×256 ☑1280×720 ☑4096×2160 X1280×720 ✓ 그림 2: 인간의 주요 포인트[78]와 파노라마 분할 마스크[10]의 시각화. 비디오 프레임 보간의 벤치마크에서 인간 중심 시나리오에 특별히 맞춤화된 데이터 세트에는 상당한 차이가 있습니다. 이러한 격차를 메우고 이 중요한 방향으로 연구를 촉진하기 위해, Common Creative Licence에 따라 YouTube에서 크롤링한 고해상도(≥720p) 슬로우 모션 스포츠 비디오로 구성된 새로운 데이터 세트인 SportsSloMo를 소개합니다. 이 데이터 세트는 축구, 농구, 야구, 하키 등 다양한 스포츠를 포함합니다. 비디오에는 광고, 전환 프레임, 샷 변경 및 슬로우 모션이 아닌 콘텐츠가 포함될 수 있으므로 이러한 원치 않는 콘텐츠를 제거하기 위해 데이터를 신중하게 큐레이션하고 마지막으로 각 긴 비디오를 9개 프레임의 짧은 슬로우 모션 클립 세트로 분할합니다. 첫 번째와 마지막 프레임은 입력으로 사용되고 나머지 7개의 중간 프레임은 VFI 모델을 훈련하고 평가하기 위한 기준 진실로 예약됩니다. 벤치마크에는 총 130,000개의 비디오 클립과 100만 개 이상의 비디오 프레임이 있습니다. 표 1에 표시된 기존 데이터 세트와 비교할 때, 제안된 SportsSloMo 벤치마크는 고해상도와 인간 중심 시나리오에 초점을 맞춘 지금까지 가장 큰 벤치마크입니다. 이 논문에서는 주로 인간 중심 VFI를 위해 설계되었지만, SportsSloMo 데이터 세트는 비디오 초고해상도[6, 38], 그룹 활동 인식[67, 83, 81], 동적 뷰 합성[39, 17, 75]과 같은 다른 과제에 대한 연구를 지원할 잠재력이 있다고 생각합니다. 이 데이터 세트를 전체 커뮤니티에 공개함으로써 인간 중심 비디오 프레임 보간의 기술적 발전을 촉진하고 연구자들이 다른 인접 분야에서 혁신적인 응용 프로그램을 탐색할 수 있기를 바랍니다. 기존 접근 방식 벤치마킹. 인간 중심 비디오 프레임 보간 방법의 개발 및 평가를 용이하게 하기 위해, Sports SloMo 데이터 세트에서 공개적으로 공개된 코드를 사용하여 여러 최첨단 접근 방식[27, 36, 23, 25, 29, 28, 82]을 재교육합니다. 인체는 변형되기 쉽고 스포츠 비디오에서는 폐색이 빈번하기 때문에 모든 방법의 정확도는 다른 데이터 세트에서의 성능에 비해 떨어집니다. 예를 들어, SportsSloMo에서 가장 성능이 좋은 두 가지 접근 방식인 EBME[29]와 EMA-VFI[82]는 각각 30.과 30.70의 PSNR 점수를 생성합니다. 이는 Viemo90K[79]의 36.19와 36.64, SNU-FILM[14] 벤치마크의 하드 스플릿에서의 30.64와 30.94보다 현저히 낮습니다. 이는 벤치마크의 어려움을 강조하고 해결해야 할 중요한 과제가 있음을 시사합니다. 인간 중심 VFI를 위한 모델 개선. 벤치마크에서 기존 VFI 모델을 개선하기 위해 인간 인식 사전 확률을 도입하여 모델 학습을 개선합니다. 구체적으로, 파노라마 설정에서의 인간 분할[10]과 중간 프레임 합성을 위한 추가 감독으로 인간 키포인트 추정[78]에 기반한 손실 항목을 제안합니다. 그림 2는 데이터 세트에서 감지된 인간 키포인트와 분할 마스크를 시각화한 것입니다. 인간 분할 마스크는 인체 경계를 구분하는 데 도움이 되며, 이는 동작 경계 주변의 고스트 효과를 줄이는 데 도움이 됩니다. 동시에, 인간 키포인트 추정은 각 신체 부위가 어디에 있는지 나타낼 수도 있어 합성된 비디오 프레임에서 일관된 동작 궤적을 적용합니다. 구체적으로, 우리는 합성된 중간 프레임과 실제 중간 프레임으로 입력을 받는 사전 훈련된 파노라마 분할 및 키포인트 감지 모델의 출력을 비교하고 일관성을 감독으로 사용합니다. 그림 1에서 볼 수 있듯이, 제안된 인간 인식 손실 항으로 감독함으로써 큰 동작과 폐색이 있는 시나리오에서 흐릿한 결과가 덜한 동작 경계에서 보간 품질을 개선합니다. 이 두 인간 인식 손실 항은 모두 모델에 독립적이며 모든 비디오 프레임 보간 접근 방식에 쉽게 통합할 수 있습니다. 실험 결과에 따르면 기존 접근 방식 7가지의 정확도를 지속적으로 개선하여 벤치마크에서 강력한 기준을 도출할 수 있습니다. 요약하자면, 이 논문은 다음과 같은 기여를 합니다. • 우리는 많은 양의 슬로우모션 스포츠 비디오로 구성된 새로운 벤치마크 데이터 세트인 SportsSloMo를 소개합니다. 우리가 아는 한, 이것은 인간 중심 비디오 프레임 보간에 맞춰진 최초의 고해상도 데이터 세트로, 여러 중간 프레임의 합성을 지원합니다. • 우리는 새로운 벤치마크에서 최첨단 접근 방식을 벤치마킹하여 인간 중심 비디오 프레임 보간 작업의 과제를 강조합니다. • 우리는 비디오 프레임 보간에 대한 인간의 사전 확률을 고려하고 기존 비디오 프레임 보간 접근 방식에 쉽게 연결할 수 있는 두 가지 인간 인식 손실 항목을 제안합니다. 실험 결과는 이것들이 기존 모델을 지속적으로 개선하여 새로운 벤치마크에서 강력한 기준 모델을 생성할 수 있음을 검증합니다. 2. 관련 연구 2.1. 벤치마크 데이터 세트 기존의 공개적으로 사용 가능한 데이터 세트는 이미 비디오 프레임 보간 방법을 개발하고 평가하는 데 귀중한 리소스를 제공합니다. 이 섹션에서는 이러한 데이터 세트를 간략하게 소개하고 몇 가지 한계점을 밝힙니다. 표 1은 기존 데이터 세트와 제안하는 SportsSloMo 데이터 세트를 비교한 것입니다. SNU-FILM[14] 데이터 세트는 VFI 평가를 위한 널리 사용되는 벤치마크로, × 720 해상도의 1240개 프레임 트리플릿을 포함하고 있습니다. 그리고 동작 크기에 따라 Easy, Medium, Hard, Extreme의 네 가지 부분으로 나뉩니다. Middlebury 벤치마크[1]는 또 다른 널리 사용되는 데이터 세트로, 이 데이터 세트의 이미지 해상도는 약 640 x 480입니다. 그러나 일반적으로 8개 시퀀스에 대한 VFI 방법을 평가하는 데만 사용됩니다. UCF101[65]은 원래 인간 행동 인식을 위한 데이터 세트로, 다양한 인간 행동을 포함하고 있습니다. [43]이 구성한 테스트 세트와 함께 VFI 방법을 평가하는 데에도 사용되며, 256 x 256 프레임 크기의 트리플릿 379개를 포함하고 있습니다. 그럼에도 불구하고 규모가 다소 작고 해상도도 낮습니다. 원래 비디오 흐림 제거를 위한 Adobe240fps 데이터 세트[66]는 VFI를 위한 또 다른 널리 사용되는 데이터 세트입니다. 이것은 ×720의 해상도를 가진 높은 프레임 속도 비디오(240fps)로 구성되어 있지만, 비디오는 단지 118개의 클립에서 나온 것입니다. VFI 방법을 훈련하고 평가하는 데 가장 많이 사용되는 데이터 세트는 Vimeo90K 데이터 세트[79]입니다. 여기에는 fps ≤ 30인 실제 비디오 클립에서 추출한 14777개의 비디오 클립의 프레임 트리플릿이 포함되어 있습니다. 그럼에도 불구하고, 주요 단점 중 하나는 원래 고해상도 프레임을 다운스케일링하여 얻은 448 × 256의 낮은 해상도입니다. 게다가, VFI 방법이 최근 몇 년 동안 빠르게 개선되면서 널리 사용되는 Vimeo90K 데이터 세트에서의 성능이 포화 상태에 접근했습니다. 비디오 프레임 보간의 최첨단 기술을 더욱 발전시키기 위해 더 큰 규모, 더 높은 해상도 및 더 어려운 시나리오를 갖춘 새로운 데이터 세트가 필요합니다. X4K1000FPS는 매우 고해상도 비디오에 대한 VFI 연구를 촉진하기 위해 4K 공간 해상도를 가진 최근 출시된 높은 프레임 속도(1000fps)입니다. 결과적으로 기존 데이터 세트에는 고해상도 및 대규모의 풍부한 인간 중심 데이터가 포함되지 않았습니다. 이러한 격차를 메우기 위해 1280x720 해상도의 240fps에서 130K 비디오 클립과 1M 비디오 프레임이 포함된 인간 중심 VFI 데이터 세트인 SportsSloMo 데이터 세트를 소개하여 인간 중심 VFI 연구를 촉진하는 것을 목표로 합니다. 2.2. 비디오 프레임 보간 방법 기존 VFI 방법은 일반적으로 흐름 독립적 방법과 흐름 기반 방법으로 분류할 수 있습니다. 흐름 독립적 접근 방식은 명시적인 중간 동작 표현 없이 VFI를 모델링합니다. 위상 기반 방법[48, 47]은 중간 프레임의 위상 분해를 직접 예측하지만 제한된 범위 내에서만 동작을 처리할 수 있습니다. 커널 기반 방법은 이 범주에서 주류를 이루는 접근 방식으로, 일반적으로 적응형 커널을 학습하여 입력 프레임을 합성하여 중간 프레임을 추정하는 것을 목표로 합니다[53, 54]. 수년에 걸쳐 이 분야에서 변형 가능한 합성곱 사용[11, 12], 분류로서 보간된 동작 추정 공식화[56], 딥 피처 블렌딩[19], 듀얼 프레임 적대적 손실 도입[36], 채널 어텐션 수행[14] 및 3D 시공간 합성곱 활용[32]을 포함한 수많은 개선이 제안되었습니다. 최근 Shi et al.[63]은 어텐션 메커니즘의 도움으로 장거리 종속성을 모델링하는 Transformer 기반 프레임워크를 도입했습니다. 이러한 방법은 픽셀 값을 직접 환각함으로써 특히 빠르게 움직이는 장면에서 흐릿한 결과와 아티팩트를 생성하는 경향이 있습니다[44]. 흐름 기반 접근 방식은 현재 VFI에서 유망한 방향으로 작용하고 있습니다. 일반적으로 흐름 기반 방법은 (1) 흐름 추정 및 (2) 프레임 합성의 2단계 파이프라인 패러다임을 취합니다. 이들은 먼저 입력 프레임 간의 광학 흐름을 추정한 다음 이미지 워핑을 사용하여 중간 프레임을 합성합니다[26]. 대표적인 연구로, Jiang 등의 SuperSlomo[27]는 선형 운동을 가정하여 양방향 광학 흐름을 추정하기 위해 스킵 연결 U-Net을 채택했습니다. 2차[77] 및 3차[13, 69] 궤적 가정도 중간 운동을 근사화하기 위해 만들어졌습니다. 최근 연구에서는 소프트맥스 스플래팅[52, 23]을 통한 전방 워핑, 복셀 흐름[43], 사이클 일관성 손실[61, 41], 작업 지향 흐름 증류 손실[35], 그램 행렬 손실[60], 암묵적 신경 함수[9], 폐색 마스크[3], 앵커 포인트 정렬[64], 특권 증류[25], 피라미드 순환 흐름 추정[28]을 포함하여 중간 흐름 추정 및 보간 정확도를 개선하기 위한 다양한 기술을 탐구했습니다. 문맥적 맵[51], 깊이 맵[2] 및 이벤트 카메라의 보조 시각 정보[70, 21, 8]와 같은 추가 정보를 고려하면 보간 정확도를 더욱 향상시킬 수도 있습니다.Park et al.은 대칭적 양측 모션 필드 추정을 채택하고 비대칭적 양측 모션 필드를 통해 중간 모션 추정 정확도를 더욱 향상시켰습니다[55].Lu et al.[44]은 Transformer 아키텍처[71]를 활용하여 장기 종속성을 모델링했습니다.Jin et al.[29]은 피라미드 구조에서 새로운 양방향 모션 추정기를 제안했습니다.Zhang et al. [82]는 동작과 FIRST RIT MINE UPMC MEDI D Frequency 0.0.0.&gt; 0.Frequency $0.0.III SportsSloMo SNU-FILM X4K1000FPS 0.0.0.0.Flow Magnitude (b) 0.0.0.SportsSloMo SNU-FILM X4K1000FPS 0.0.0.0.0.0.Flow Magnitude (c) 그림 3: SportsSloMo 데이터 세트. (a) 다양한 스포츠 카테고리와 VFI를 위한 까다로운 인간 중심 콘텐츠를 포함하는 샘플링된 프레임. (b) 데이터 세트의 모든 픽셀의 흐름 크기 히스토그램. (c) 데이터 세트의 모든 이미지의 평균 흐름 크기 히스토그램. 하키 기타 9.6% 11.9% 야구 6.7% 농구 17.2% 축구 11.6% 3.6% 미식축구 3.5% 무술 3.0% 3.5% 체조 3.8% 배구 6.4% 배드민턴 탁구 19.3% 테니스 그림 4: SportsSloMo 벤치마크에서 다양한 스포츠 카테고리 분포. 하이브리드 CNN 및 Transformer 아키텍처를 통한 모양 정보. 간단히 말해서, 이전 방법에서는 복잡한 동작과 폐색을 처리하는 성공적인 설계가 제안되었지만, 그 중 어느 것도 인간 중심 장면에 맞게 신중하게 설계되지 않았습니다. 1절에서 논의했듯이, 인간 중심 VFI는 동적 포즈 변화, 복잡한 인간 동작 패턴 및 혼잡한 장면에서의 폐색을 포함한 다양한 과제에 직면합니다. 또한 얼굴 표정과 손짓과 같은 미세한 디테일을 정확하게 합성하는 것은 어려울 수 있습니다. 이를 위해 파노라마 설정에서 인간의 주요 지점 감지와 분할에 추가 감독을 통합하여 인간이 인식하는 손실 항목을 고려할 것을 제안합니다. 3. SportsSloMo 벤치마크 Middlebury [1], GoPro [50], UCF101 [65], DAVIS [57], Adobe240fps [66], Vimeo90K [79], SNU-FILM [14] 및 X4K1000FPS [64]를 포함하여 비디오 프레임 보간에 대한 수많은 벤치마크 데이터 세트가 있습니다. 그러나 이러한 데이터 세트 중 어느 것도 스포츠 장면과 같은 인간 중심의 VFI에 초점을 맞추지 않습니다. 이는 향상된 엔터테인먼트 경험, 상업적 배포 등과 같은 인간 중심 애플리케이션을 대상으로 하는 VFI 방법에 대한 연구를 제한합니다. 이러한 격차를 메우고 향후 연구를 촉진하기 위해 Common Creative License에 따라 YouTube에서 크롤링한 고해상도(≥ 720p) 스포츠 비디오로 구성된 까다로운 데이터 세트인 SportsSloMo를 제안합니다. 광고, 전환 프레임, 샷 변경, 깜박이는 빛 및 슬로우 모션이 아닌 콘텐츠를 포함하여 비디오에서 원치 않는 콘텐츠를 제거하기 위해 신중하게 큐레이션했습니다. 구체적으로, 먼저 Yolov3[62]를 활용하여 모든 비디오에서 인간 감지를 수행하고 감지된 인간이 없는 비디오 프레임을 제거합니다.둘째, 연속 프레임 사이의 밝기 변화에 대한 임계값을 설정하여 깜박이는 빛이 있는 프레임을 제거합니다(큰 밝기 변화는 깜박이는 빛이 있음을 나타냄).셋째, RAFT[68]를 사용하여 동작 크기를 측정하고 슬로우 모션이 아닌 비디오 세그먼트를 삭제하기 위한 임계값을 설정합니다.마지막으로, 추출한 클립을 수동으로 신중하게 큐레이션하여 고품질의 자체 일관성 있는 비디오만 유지합니다.이러한 반자동 큐레이션을 통해 각 프레임의 공간 해상도가 최소 1280×720인 9개 프레임의 짧은 슬로우 모션 클립 세트를 얻습니다.첫 번째와 마지막 프레임은 VFI 방법의 입력으로 사용되고 7개의 중간 프레임은 모델 학습 및 평가를 위한 기준 진실로 사용되며, 대략 30fps(초당 프레임)의 비디오를 240fps로 변환하는 것과 같습니다. 총 259개의 원시 YouTube 비디오에서 131,464개의 비디오 클립과 120만 개의 개별 비디오 프레임을 수집합니다. 여기에는 하키, 베이스플러그인 인간 인식 손실 모듈 파노라마 분할 키포인트 감지 StateFarm Mt Lseg Lkpt Mt 파노라마 분할 키포인트 감지 State Farm R₁ Io 비디오 프레임 보간 → K₁₂ State Lbasic It그림 5: 파노라마 설정에서 인간 분할과 키포인트 감지에 대한 손실로 구성된 제안된 인간 인식 손실 항목의 예입니다. 공, 스케이팅, 농구, 달리기, 배구 등 그림은 다양한 스포츠 카테고리의 비율을 보여줍니다. 무작위로 샘플링된 비디오 프레임의 시각화는 그림 3에서 볼 수 있습니다. 더 많은 결과를 보려면 독자는 보충 자료를 참조하세요. 표 1에서 볼 수 있듯이, 기존 데이터 세트와 비교했을 때, 저희 데이터 세트는 인간 중심 시나리오에 맞춰져 있을 뿐만 아니라, 규모, 해상도, 프레임 속도 측면에서도 다른 데이터 세트를 능가합니다.그림 3은 저희 데이터 세트의 모든 픽셀의 흐름 크기와 모든 이미지의 평균 흐름 크기의 히스토그램(GMFlow [76]를 사용하여 계산)을 보여줍니다. 널리 사용되는 SNU-FILM [14] 데이터 세트(저희와 동일한 공간 해상도를 가짐)와 최근에 도입된 X4k1000FPS [64] 데이터 세트와 비교합니다.보시다시피, 저희의 SportsSloMo 벤치마크는 SNU-FILM 데이터 세트에 비해 더 큰 변위 모션을 포함합니다.예를 들어, 이미지 레벨의 평균 흐름 크기(그림 3(c))의 경우, 저희 데이터 세트에는 평균 흐름 크기가 20픽셀보다 큰 이미지가 훨씬 더 많습니다.또한 SportsSloMo와 X4K1000FPS 데이터 세트 모두에 큰 모션이 포함되어 있음을 확인할 수 있습니다. 우리는 제안된 SportsSloMo 데이터 세트를 각각 115,421개와 16,043개의 비디오 클립을 포함하는 훈련 및 테스트로 분할했습니다. 각 스포츠 카테고리의 경우 비디오는 교차 없이 훈련 및 테스트로 분할되므로 훈련 중에는 테스트 비디오가 전혀 보이지 않습니다. 4. 인간 인식 비디오 프레임 보간 4.1. 개요 두 개의 입력 프레임 I와 I₁가 주어지면 VFI의 목표는 중간 시간 단계 t = (0, 1)에서 중간 프레임 It를 합성하는 것입니다. VFI 방법은 Io와 I₁ 사이의 픽셀별 대응 관계를 찾고 해당 픽셀을 적응적으로 융합하여 It의 각 픽셀을 합성해야 합니다. SuperSloMo[27] 및 EBME[29]와 같은 흐름 기반 접근 방식은 일반적으로 대응 관계를 광학 흐름으로 명시적으로 모델링하는 반면 AdaCof[36]와 같은 흐름 독립적 방법은 학습된 커널을 사용하여 시각적 대응 관계를 암묵적으로 처리합니다. 네트워크 훈련을 감독하기 위해 중간 프레임의 재구성 오류가 손실로 사용됩니다. 예를 들어, [29]에서 기준 진실 중간 프레임 It와 합성 프레임 It 사이의 Charbonnier 손실 [7] Lchar과 인구 조사 손실 [46] Leen의 가중 합은 Lbasic = Lchar (It - Ît) + λcen · Lcen (It, Ît), (1)로 채택되었습니다. 여기서 Lchar(x) = (x² + €²)ª, a = 0.5, € = 10−6입니다. 그러나 이러한 재구성 손실은 고도로 변형 가능한 인간 동작과 잦은 폐색의 과제로 인해 인간 중심 VFI에 대한 충분한 감독을 제공하지 못할 수 있습니다. 이러한 과제를 해결하기 위해 우리는 파노라마 설정에서 인간 분할과 주요 포인트 감지를 기반으로 하는 네트워크 감독에 Lseg와 Lkpt를 통합하여 모델이 인간 경계와 주요 포인트 궤적을 따라 고품질 합성 결과를 생성하도록 강제합니다. 최종 손실은 L = Lbasic Aseg seg + λkptLkpt입니다. 제안하는 인간 인식 손실 항목은 유연하여 섹션 5에서 볼 수 있듯이 다른 흐름 기반(예: SuperSloMo[27] 및 EBME[29]) 또는 흐름과 무관한 VFI 모델(예: AdaCoF[36])에 플러그인할 수 있다는 점에 주목할 가치가 있습니다.4.2. 인간 분할 손실 인간 중심 시나리오에서 인체 경계를 정확하게 추정하는 것이 중요합니다.신체 움직임이 복잡하거나 심한 폐색이 있는 영역에서는 추정된 신체 경계의 부정확성으로 인해 합성된 비디오 프레임에 눈에 띄는 아티팩트가 발생할 수 있습니다.이를 위해 중간 비디오 프레임의 합성을 개선하기 위해 추가 감독으로 인간 분할을 통합하는 것을 제안합니다.인간 분할 마스크는 그림 2에서 볼 수 있듯이 인체 경계가 어디에 있는지 직접 알려주므로 고스트 효과를 줄이는 데 도움이 됩니다.인스턴스 분할과 파노라마 분할은 모두 이 목적에 사용될 수 있지만 경험적으로 파노라마 분할 모델이 인스턴스 분할 대응 제품보다 보간 결과가 더 나은 경향이 있음을 발견했습니다. 구체적으로, 우리는 Detectron2 [74]의 SwinL 백본 [42]이 있는 COCO 파노라마 데이터 세트 [33]에서 훈련된 최신 Mask2Former [10]를 채택하여 기준 진실 중간 프레임 It에서 파노라마 분할 마스크 M₁를 생성합니다. 훈련 중에 동일한 Mask2Former 모델을 사용하여 얻은 합성된 중간 프레임 Ît의 파노라마 분할 결과 Mt를 Mt와 비교합니다. 이상적으로는 합성된 중간 프레임 It가 기준 진실 It와 동일하다면 Mt는 Mt와 같아야 합니다. 그 차이를 비교함으로써 인간 분할 손실은 It와 Ît 사이의 인체 경계의 일관성을 향상시켜 고스트 효과를 줄이는 데 도움이 됩니다. 우리는 Mask2Former의 손실 함수를 따르고 이진 교차 엔트로피 손실 Lce와 주사위 손실 Ldice [49]를 인간 분할 손실 Lseg = \ceLce(Ût, Mt) + \dice ↳dice (ÛÂt, Mt), (3)에 사용합니다. 여기서 Ace = 5.0 및 Adice 5.0으로 설정합니다. 파노라마 분할 모델은 훈련 중에 동결되고 이 손실의 그래디언트는 지각 손실 [31]과 유사한 방식으로만 VFI 모델로 역전파되어 모델이 고품질 합성 결과를 생성하도록 합니다. 4.3. 인간 주요 지점 감지 손실 분할 외에도 추가 감독으로 인간 주요 지점 추정도 고려합니다. 인간 주요 지점의 정확한 추정은 각 신체 부위의 위치를 나타내는 인간 중심 VFI에도 중요하며 동작 추정을 위한 추가 단서를 제공합니다. 따라서 인간의 주요 포인트에 대한 감독을 통합함으로써 일관된 동작 궤적을 적용하여 인간의 동작을 더 잘 보존하고 보간 프로세스를 안내하여 더욱 그럴듯한 중간 프레임을 생성할 수 있습니다. 구체적으로, MAE[20] 사전 학습된 가중치로 초기화된 ViT-L 백본[15]과 함께 COCO[40]에서 학습된 최첨단 인간 자세 추정 모델 ViTPose[78]를 채택합니다. 먼저 기성품 인간 감지기 YOLOV8[30]을 사용하여 실제 중간 프레임 It에서 사람 인스턴스를 감지합니다. 그런 다음 ViTPose를 사용하여 실제 EBME W/ HL EBME 사람 사람 그림 6: 인간 인식 손실(HL) 항목이 있는 VFI 결과. 인간 인식 손실로 인해 향상된 더 나은 인간 분할, 주요 포인트 감지 및 보간 결과를 얻을 수 있습니다. 각 관절의 위치를 포함하는 주요 포인트 히트맵 K₁를 추정합니다. 인간의 분할 손실과 유사하게, 우리는 합성된 중간 프레임 Ît에서 동일한 ViTPose 모델을 사용하여 키포인트 히트맵 Ất를 추정합니다. 그런 다음 Kt와 Kt의 차이를 감독으로 사용하여 VFI 결과를 개선합니다. 우리는 ViTPose의 손실 함수를 따르고 히트맵에 대한 MSE(평균 제곱 오차) 손실을 인간 키포인트 손실 Lkpt로 사용합니다. Lkpt = MSE(Kt, Kt). (4) ViTPose 모델은 학습하는 동안 동결되고 이 손실의 그래디언트는 VFI 모델로 역전파되어 고품질 중간 프레임 합성을 촉진하여 추정된 인간 키포인트가 실제 중간 프레임에서 얻은 것과 가깝도록 합니다. 5. 실험 5.1. VFI 모델 설정. 우리는 SuperSlomo [27], AdaCoF [36], M2MVFI [23], RIFE [25], EBME [29], UPR-Net [28], EMA-VFI [82]를 포함하여 공개적으로 사용 가능한 코드를 사용하여 SportsSloMo 데이터 세트에서 여러 최첨단 방법을 재교육하여 기존 VFI 접근 방식을 벤치마킹합니다.AdaCoF는 흐름에 독립적인 VFI 모델이고 나머지는 흐름 기반입니다.모든 오버레이된 입력 오버레이된 입력 실제 SuperSlomo AdaCoF M2MVFI RIFE EBME EBME w/ HL 그림 7: SportsSloMo 데이터 세트에 대한 정성적 비교.인간 인식(HL) 손실을 향상시키면서 EBME는 더 나은 보간 결과를 얻습니다.표 2: SportsSloMo 데이터 세트에 대한 다양한 접근 방식의 정량적 결과.보시다시피, 제안한 인간 인식 손실(HL) 항목은 모든 VFI 모델의 성능을 일관되게 개선합니다. 방법 SuperSlomo [27] SuperSloMo + HL 장소 CVPRAdaCoF [36] AdaCoFHL M2MVFI [23] M2MVFI + HL RIFE [25] RIFE + HL EBME [29] EBME + HL UPR-Net [28] UPR-Net + HL EMA-VFI [82] EMA-VFI + HL CVPRCVPRECCVWACVCVPRCVPRPSNR SSIM↑ IE↓ 29.77 0.910 9.30.24 0.917 8.28.79 0.926 5.28.94 0.926 5.29.03 0.935 5.29.29 0.936 5.29.69 0.931 5.29.87 0.933 5.30.15 0.941 4.30.48 0.944 4.30.25 0.945 4.30.50 0.945 4.30.70 0.949 4.30.75 0.952 4. 이러한 모델은 SportsSloMo 데이터 세트에서 처음부터 학습되었습니다. 또한 인간 중심 VFI를 개선하기 위해 인간의 사전 정보를 명시적으로 고려하는 효과를 검증하기 위해 제안한 인간 인식 손실 항목을 이러한 모델에 통합했습니다. 평가 지표. 이전 VFI 방법[27, 24]에 따라 널리 사용되는 신호 대 잡음비(PSNR), 구조적 유사성(SSIM)[72] 및 보간 오류(IE)[1]를 채택하여 보간 결과를 평가합니다. PSNR 및 SSIM의 경우 높을수록 성능이 우수함을 나타냅니다. IE의 경우 낮을수록 좋습니다. 구현 세부 정보. 인간 분할 및 키포인트 손실 항목의 경우, 학습 단계에서 배치 정규화(BN) 계층의 영향을 피하기 위해 BN 계층을 동결된 배치 정규화(동결된 BN) 계층으로 대체했습니다. 각 VFI 모델에서 제공하는 기본 하이퍼파라미터를 사용합니다. 학습 및 평가는 모두 8개의 NVIDIA RTX A6000 GPU에서 수행됩니다. 학습하는 동안 SuperSlomo[27], M2MVFI[23], RIFE[25], EBME[29], UPR-Net[28], EMA-VFI[82]와 같은 임의의 시간 프레임 보간을 지원하는 VFI 방법에 대해 첫 번째와 9번째(마지막) 프레임 사이의 프레임을 무작위로 샘플링합니다. AdaCoF[36]와 같이 중간 프레임만 합성할 수 있는 VFI 방법의 경우 대상 프레임이 항상 입력된 두 프레임의 중간에 있도록 9개 프레임 내에서 무작위로 트리플릿을 생성합니다. 평가 중에 임의 시간 지원 VFI 방법은 시간 단계를 입력으로 사용하여 모든 중간 프레임을 보간하는 반면, 단일 프레임 방법은 각 중간 프레임을 재귀적으로 생성합니다.5.2. 기존 방법 벤치마킹 표 2는 제안된 SportsSloMo 데이터 세트에서 기존 VFI 방법 간의 정량적 비교를 보여줍니다.EMA-VFI[82]가 세 가지 평가 지표 모두에서 가장 좋은 성능을 보이는 것을 알 수 있습니다.각 모델의 성능은 인기 있는 데이터 세트의 결과에 비해 떨어진다는 점에 유의해야 합니다.예를 들어, SportsSloMo에서 가장 성능이 좋은 두 가지 방법인 EBME[29]와 EMA-VFI[82]는 각각 30.15와 30.70의 PSNR 점수를 생성하는 반면 Viemo90K[79]에서는 36.19와 36.64, SNUFILM[14] 벤치마크의 하드 스플릿에서는 30.64와 30.94를 생성합니다. 그것은 우리의 표 3: 다양한 보조 손실 항목의 효과성의 어려움을 강조합니다.✓ SuperSloMo PSNR SSIM↑ IE↓ 모델 분할 키포인트 손실 손실 ✓ ☑ ☑ EBME 29.77 0.910 9.30.19 0.916 8.30.22 0.916 8.30.24 0.917 8.30.15 0.941 4.30.26 0.942 4.30.34 0.943 4.30.48 0.4.벤치마크와 해결해야 할 중요한 과제를 시사합니다.보충 자료에서 더 자세한 내용을 제공합니다.5.3. 향상된 인간 인식 VFI 표 2는 또한 제안한 인간 인식 손실의 효과를 보여줍니다.특히, 우리는 제안한 인간 분할 및 키포인트 손실을 각 VFI 모델에 통합합니다. 보시다시피, 그들은 세 가지 평가 지표 모두에서 모든 단일 VFI 방법의 성능을 지속적으로 개선합니다.특히, PSNR 측면에서 제안한 인간 인식 손실은 SuperSloMo[27]와 EBME[29]에 대해 각각 1.6%와 1.1%의 개선으로 이어집니다.IE 측면에서 이 두 방법에 대한 개선은 각각 7.4%와 5.6%입니다.질적으로, 그림 6과 그림 7에서 제안한 인간 인식 손실 항목이 운동선수의 매우 변형 가능한 팔과 폐색 아래의 배경에 대한 보간 결과를 개선할 수 있음을 알 수 있습니다.5.4. 절제 연구 이 섹션에서는 표 3에서 제안한 인간 인식 손실의 설계 선택을 분석하기 위해 SuperSloMo[27]와 EBME[29]에 대한 절제 연구를 제시합니다.보시다시피, 인간 분할과 키포인트 감지 손실은 모두 SuperSloMo와 EBME의 성능을 성공적으로 개선할 수 있습니다. 이는 인간 중심 VFI를 개선하기 위해 인간 인식 사전 확률을 활용하려는 동기를 정당화합니다. 이 두 손실을 결합하면 두 방법 모두에서 가장 큰 성능 이득을 얻을 수 있습니다. 또한 그림 6에서 팔꿈치와 손 주변에서 더 나은 보간 결과를 얻을 수 있음을 분명히 알 수 있습니다. 5.5. 제한 사항 및 논의 제안된 데이터 세트에서 까다로운 인간 중심 시나리오를 처리하기 위해 인간 인식 손실 항목을 도입했지만 향후 탐색을 위한 주목할 만한 제한 사항이 여전히 남아 있습니다. 우선, 인간 중심 장면의 경우 크고 복잡하며 변형되기 쉬운 인간 동작과 군중에 의한 가려짐으로 인해 입력 프레임 사이에서 인간(즉, 광학 흐름)의 대응 관계를 찾는 것이 어려운 작업입니다. 최근 연구[25, 35]에서는 지식 증류[22]가 중간 광학 흐름 추정을 개선하는 데 효과적임을 보여주었습니다. 또한 흐름 기반 VFI 방법(예: SuperSloMo[27], EBME[29])에 대한 추가 감독을 통합하기 위해 이 손실을 탐색했습니다. 구체적으로, 우리는 광학 흐름 벤치마크[16, 45, 5]에서 사전 학습된 최첨단 모델 GMFlow[76]에서 얻은 광학 흐름 결과를 흐름 증류를 수행하는 교사 네트워크로 사용합니다. 그러나 SportsSloMo 벤치마크에서는 엇갈린 결과를 발견했습니다. 실제로 SuperSlomo[27]의 성능을 향상시키지만(PSNR: 29.77 대 30.18), EBME[29]의 성능을 저하시킵니다(PSNR: 30.15 대 29.85). 또한 실제 세계 이미지 위에 배경으로 중첩된 합성 인간 모습으로 구성된 인간 흐름 데이터 세트[59]에서 GMFlow 모델을 미세 조정합니다. 그러나 합성 학습 이미지와 실제 테스트 이미지 사이에 상당한 도메인 갭이 발견되었고, 이로 인해 광학 흐름 추정 결과가 좋지 않았습니다. VFI를 개선하기 위해 인간 중심 광학 흐름을 개선하는 방법은 여전히 어려운 문제입니다. 둘째, 우리는 VFI를 향상시키기 위해 제안된 손실 항목에서 인체의 2D 단서만 활용합니다. 최근, 예를 들어 [18, 80, 58, 34]에서 공간적 정보와 시간적 정보를 모두 고려하여 비디오에서 3D 인체 재구성에 상당한 진전이 있었습니다. 2D 비디오를 3D 공간으로 들어 올리면 폐색을 더 잘 해결할 수 있습니다. 3D 인체 재구성과 관련된 보다 진보된 손실 항목은 탐구할 가치가 있습니다. 우리는 이것을 탐구할 미래의 작업으로 남겨둡니다. 마지막으로, Sports SloMo 데이터 세트에서 다양한 스포츠에는 빠르게 움직이는 공과 스포츠 장비가 포함됩니다. 이 논문에서는 동작 추정을 위해 인간의 경계와 신체 부위만 고려합니다. 그러나 이러한 시나리오에서 빠르게 움직이는 물체는 VFI에도 도전이 됩니다. 이러한 문제에는 더 많은 노력이 필요합니다. 6.
--- CONCLUSION ---
이 논문에서는 인간 중심 비디오 프레임 보간에 초점을 맞춘 새로운 벤치마크인 SportsSloMo를 소개했습니다. 벤치마크에는 YouTube에서 신중하게 큐레이션하여 크롤링한 고해상도(≥ 720p) 슬로우모션 스포츠 비디오에서 얻은 130K 비디오 클립과 1M 이상의 비디오 프레임이 포함되어 있습니다. 복잡하고 변형이 심한 인간 동작과 빈번한 폐색으로 인해 이 벤치마크는 기존 VFI 모델에 상당한 과제를 안겨줍니다. 정확도를 높이기 위해 인간 인식 손실 항목을 도입하여 인간 분할 감독과 키포인트 감지가 통합된 기존 방법을 개선합니다. 손실 항목은 모델에 구애받지 않으며 기존 VFI 방법 7개에 성공적으로 적용되어 지속적으로 더 나은 정확도를 얻었습니다. 벤치마크 데이터 세트와 코드는 인간 중심 VFI의 새로운 흥미로운 방향으로 미래 연구를 촉진하기 위해 공개될 예정입니다. 감사의 말 데이터 수집에 도움을 준 Thuy-Tien Bui와 Devroop Kar에게 감사드립니다. 이 연구는 부분적으로 IIS-2310254 수여에 따라 National Science Foundation에서 지원했습니다. 참고문헌 [1] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, Richard Szeliski. 광학 흐름을 위한 데이터베이스 및 평가 방법론. International journal of computer vision, 92(1):1-31, 2011. 1, 3, 4,[2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang. 깊이 인식 비디오 프레임 보간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 3703-3712페이지, 2019.[3] Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang. Memc-net: 비디오 보간 및 향상을 위한 동작 추정 및 동작 보상 기반 신경망. IEEE 패턴 분석 및 머신 인텔리전스 거래, 43(3):933-948, 2019.[4] Karlis Martins Briedis, Abdelaziz Djelouah, Mark Meyer, Ian McGonigal, Markus H. Gross, Christopher Schroers. 렌더링된 콘텐츠에 대한 신경 프레임 보간. ACM Trans. Graph., 40(6):239:1-239:13, 2021.[5] DJ Butler, J. Wulff, GB Stanley, MJ Black. 광학 흐름 평가를 위한 자연스러운 오픈 소스 영화. ECCV에서, 2012.[6] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, Chen Change Loy. 실제 비디오 초고해상도의 상충 관계 조사. CVPR에서, 2022.[7] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, Michel Barlaud. 계산된 이미징을 위한 두 가지 결정론적 반이차 정규화 알고리즘. 제1회 국제 이미지 처리 컨퍼런스 논문집, 2권, 168-172페이지. IEEE, 1994.[8] Jiaben Chen, Yichen Zhu, Dongze Lian, Jiaqi Yang, Yifu Wang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent Kneip, Shenghua Gao. 이벤트 기반 비디오 프레임 보간 재검토. arXiv 사전 인쇄본 arXiv:2307.12558, 2023.[9] Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, Xiaolong Wang. Videoinr: 연속 시공간 초고해상도를 위한 비디오 암묵적 신경 표현 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2047-2057페이지, 2022년.[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 마스크-어텐션 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1290-1299페이지, 2022년. 2,[11] Xianhang Cheng 및 Zhenzhong Chen. 변형 가능한 분리 가능한 합성곱을 통한 비디오 프레임 보간. AAAI 인공지능 컨퍼런스 회의록, 34권, 10607-10614페이지, 2020년.[12] Xianhang Cheng 및 Zhenzhong Chen. 향상된 변형 가능한 분리 가능한 합성곱을 통한 다중 비디오 프레임 보간. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(10):7029-7045, 2021.[13] Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang, Konstantinos N Plataniotis. All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling. 유럽 컴퓨터 비전 컨퍼런스, 107-123페이지. Springer, 2020.[14] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, Kyoung Mu Lee. 채널 주의만 있으면 비디오 프레임 보간이 가능합니다. AAAI 인공지능 컨퍼런스 회의록, 제34권, 10663-10671페이지, 2020. 1, 2, 3, 4, 5,[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020.[16] Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, et al. FlowNet: 합성곱 네트워크를 사용한 광학 흐름 학습. ICCV, 2015.[17] Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang. 동적 단안 비디오에서 동적 뷰 합성. ICCV, 2021.[18] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa*, Jitendra Malik*. 4D의 인간: 트랜스포머를 사용하여 인간 재구성 및 추적. 국제 컴퓨터 비전 컨퍼런스(ICCV), 2023.[19] Shurui Gui, Chaoyue Wang, Qihua Chen, Dacheng Tao. Featureflow: 구조-텍스처 생성을 통한 견고한 비디오 보간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 14004-14013페이지, 2020.[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. Masked autoencoders are scalable vision learners. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 16000-16009쪽, 2022년.[21] Weihua He, Kaichao You, Zhendong Qiao, Xu Jia, Ziyang Zhang, Wenhui Wang, Huchuan Lu, Yaoyuan Wang, Jianxing Liao. Timereplayer: 비디오 보간을 위한 이벤트 카메라의 잠재력 해제. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 17804-17813쪽, 2022년.[22] Geoffrey Hinton, Oriol Vinyals, Jeff Dean. 신경망에서 지식 추출. arXiv 사전 인쇄본 arXiv:1503.02531, 2015.[23] Ping Hu, Simon Niklaus, Stan Sclaroff, Kate Saenko. 효율적인 비디오 프레임 보간을 위한 다대다 스플래팅. CVPR, 2022. 2, 3, 6,[24] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou. Rife: 비디오 프레임 보간을 위한 실시간 중간 흐름 추정. arXiv 사전 인쇄본 arXiv:2011.06294, 2020.[25] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou. 비디오 프레임 보간을 위한 실시간 중간 흐름 추정. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV, pages 624–642. Springer, 2022. 2, 3, 6, 7,[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. 공간 변환기 네트워크. 신경 정보 처리 시스템의 발전, 28, 2015.[27] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik G. Learned-Miller, Jan Kautz. 슈퍼 슬로모: 비디오 보간을 위한 여러 중간 프레임의 고품질 추정. CVPR, 2018. 1, 2, 3, 5, 6, 7,[28] Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo, and Cheul-hee Hahm. 비디오 프레임 보간을 위한 통합 피라미드 순환 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023. 2, 3, 6,[29] Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen, Jayoon Koo, and Cheul-hee Hahm. 비디오 프레임 보간을 위한 향상된 양방향 동작 추정. IEEE/CVF 컴퓨터 비전 응용 겨울 컨퍼런스 회의록, 5049-5057페이지, 2023. 2, 3, 5, 6, 7,[30] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics의 YOLO, 1 2023.[31] Justin Johnson, Alexandre Alahi, Li Fei-Fei. 실시간 스타일 전송 및 초고해상도를 위한 지각 손실. ECCV에서, 2016.[32] Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran. Flavr: 빠른 프레임 보간을 위한 흐름 독립적인 비디오 표현. IEEE/CVF 컴퓨터 비전 응용 프로그램 겨울 컨퍼런스 회의록, 2071-2082페이지, 2023.[33] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413페이지, 2019.[34] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, Michael J. Black. PARE: 3D 인체 추정을 위한 부분 주의 회귀기. ICCV, 2021.[35] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, Jie Yang. Ifrnet: 효율적인 프레임 보간을 위한 중간 기능 정제 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 1969~1978쪽, 2022. 3,[36] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, Sangyoun Lee. Adacof: 비디오 프레임 보간을 위한 흐름의 적응적 협업. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 5316-5325쪽, 2020. 2, 3, 5, 6,[37] Siyao Li, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N. Metaxas, Chen Change Loy, Ziwei Liu. 야생에서의 딥 애니메이션 비디오 보간. CVPR, 2021.[38] Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, MingHsuan Yang, Peyman Milanfar. Comisr: 압축 정보 비디오 초고해상도. ICCV, 2021.[39] Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang. 동적 장면의 시공간 뷰 합성을 위한 신경 장면 흐름 필드. CVPR, 2021. 1,[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár 및 C Lawrence Zitnick. Microsoft coco: 컨텍스트 내 공통 개체. Computer Vision-ECCV 2014: 제13차 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6~12일, 절차, 파트 V 13, 740~755페이지. 스프링거, 2014.[41] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, Yung-Yu Chuang. 순환 프레임 생성을 사용한 심층 비디오 프레임 보간. 인공 지능에 관한 AAAI 회의 진행, 33권, 페이지 8794-8802, 2019.[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin transformer: 이동된 창을 사용하는 계층적 비전 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 회의록, 10012-10022페이지, 2021.[43] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala. 딥 폭셀 흐름을 사용한 비디오 프레임 합성. IEEE 컴퓨터 비전 국제 컨퍼런스의 회의록, 4463-4471페이지, 2017.[44] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, Jiaya Jia. 변환기를 사용한 비디오 프레임 보간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3532-3542페이지, 2022.[45] Nikolaus Mayer, Eddy Ilg, Philip Häusser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox. 시차, 광학 흐름 및 장면 흐름 추정을 위한 합성 신경망을 훈련하는 대규모 데이터 세트. CVPR, 2016.[46] Simon Meister, Junhwa Hur, Stefan Roth. Unflow: 양방향 인구 조사 손실을 사용한 광학 흐름의 비지도 학습. AAAI 인공지능 컨퍼런스 회의록, 32권, 2018.[47] Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus Gross, Christopher Schroers. 비디오 프레임 보간을 위한 Phasenet. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 498-507페이지, 2018.[48] Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse, Alexander Sorkine-Hornung. 비디오를 위한 위상 기반 프레임 보간. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 1410-1418페이지, 2015.[49] Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi. V-net: 체적 의료 영상 분할을 위한 완전 합성 신경망. 2016년 제4회 3D 비전(3DV) 국제 컨퍼런스, 565-571페이지. IEEE, 2016.[50] Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee. 동적 장면 흐림 제거를 위한 딥 다중 스케일 합성 신경망. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3883-3891페이지, 2017.[51] Simon Niklaus 및 Feng Liu. 비디오 프레임 보간을 위한 컨텍스트 인식 합성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1701-1710페이지, 2018.[52] Simon Niklaus 및 Feng Liu. 비디오 프레임 보간을 위한 Softmax 스플래팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5437-5446페이지, 2020.[53] Simon Niklaus, Long Mai 및 Feng Liu. 적응 합성을 통한 비디오 프레임 보간. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 670-679페이지, 2017.[54] Simon Niklaus, Long Mai, Feng Liu. 적응형 분리형 합성곱을 통한 비디오 프레임 보간. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 261-270페이지, 2017년.[55] Junheum Park, Chul Lee, Chang-Su Kim. 비디오 프레임 보간을 위한 비대칭 양측 모션 추정. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 14539-14548페이지, 2021년.[56] Tomer Peleg, Pablo Szekely, Doron Sabo, Omry Sendik. 고해상도 비디오 프레임 보간을 위한 Im-net. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2398-2407페이지, 2019년.[57] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, Alexander Sorkine-Hornung. 비디오 객체 분할을 위한 벤치마크 데이터 세트 및 평가 방법론. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 724-732페이지, 2016. 1,[58] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Christoph Feichtenhofer, Jitendra Malik. 인간 동작 인식을 위한 3D 포즈 및 추적의 이점에 관하여. CVPR, 2023.[59] Anurag Ranjan, David T. Hoffmann, Dimitrios Tzionas, Siyu Tang, Javier Romero, Michael J. Black. 다중 인간 광학 흐름 학습. Int. J. Comput. Vis., 128(4):873–890, 2020.[60] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru 및 Brian Curless. 필름: 큰 움직임을 위한 프레임 보간. Computer Vision-ECCV 2022: 제17차 유럽 회의, 이스라엘 텔아비브, 2022년 10월 23~27일, 절차, 파트 VII, 250~266페이지. 스프링거, 2022.[61] Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin Liu, Kevin J Shih, Andrew Tao, Jan Kautz 및 Bryan Catanzaro. 주기 일관성을 사용한 비지도 비디오 보간. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 진행 사항, 892-900페이지, 2019.[62] Joseph Redmon 및 Ali Farhadi. Yolov3: 점진적 개선. arXiv 사전 인쇄본 arXiv:1804.02767, 2018.[63] Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen 및 Ming-Hsuan Yang. 비디오 프레임 보간 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항, 17482-17491페이지, 2022.Xvfi: [64] Hyeonjun Sim, Jihyong Oh 및 Munchurl Kim. 극단적인 비디오 프레임 보간. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 14489-14498쪽, 2021. 1, 2, 3, 4,[65] Khurram Soomro, Amir Roshan Zamir 및 Mubarak Shah. Ucf101: 야생 비디오에서 추출한 101개 인간 행동 클래스 데이터 세트. arXiv 사전 인쇄본 arXiv:1212.0402, 2012. 1, 2, 3,[66] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich 및 Oliver Wang. 핸드헬드 카메라를 위한 심층적 비디오 흐림 제거. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1279-1288페이지, 2017. 1, 2, 3,[67] Masato Tamura, Rahul Vishwakarma 및 Ravigopal Vennelakanti. 사회 집단 활동 인식을 위한 변환기를 사용한 사냥 집단 단서. ECCV, 2022.[68] Zachary Teed 및 Jia Deng. Raft: 광학 흐름을 위한 반복적인 모든 쌍 필드 변환. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23일~28일, 회의록, 2부 16, 402~419페이지. Springer, 2020.[69] Stepan Tulyakov, Alfredo Bochicchio, Daniel Gehrig, Stamatios Georgoulis, Yuanyou Li 및 Davide Scaramuzza. 시간 렌즈++: 매개변수적 비선형 흐름 및 다중 스케일 융합을 사용한 이벤트 기반 프레임 보간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 17755-17764페이지, 2022년.[70] Stepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis, Julius Erbach, Mathias Gehrig, Yuanyou Li, Davide Scaramuzza. 시간 렌즈: 이벤트 기반 비디오 프레임 보간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16155-16164페이지, 2021년.[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 기울이면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017.[72] Zhou Wang, Alan C Bovik, Hamid R Sheikh, 및 Eero P Simoncelli. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE 이미지 처리 거래, 13(4):600-612, 2004.[73] Chao-Yuan Wu, Nayan Singhal, 및 Philipp Krähenbühl. 이미지 보간을 통한 비디오 압축. ECCV에서, 2018.[74] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, 및 Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019.[75] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, 및 Changil Kim. 자유 시점 비디오를 위한 시공간 신경 조도 필드. CVPR, 2021.[76] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi 및 Dacheng Tao. Gmflow: 글로벌 매칭을 통한 광학 흐름 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8121-8130페이지, 2022. 5,[77] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin 및 MingHsuan Yang. 2차 비디오 보간. 신경 정보 처리 시스템의 발전, 32, 2019.[78] Yufei Xu, Jing Zhang, Qiming Zhang 및 Dacheng Tao. Vitpose: 인간 포즈 추정을 위한 간단한 비전 변환기 기준선. arXiv 사전 인쇄본 arXiv:2204.12484, 2022. 2,[79] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. 작업 지향 흐름을 통한 비디오 향상. International Journal of Computer Vision, 127(8):1106-1125, 2019. 1, 2, 3, 4,[80] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. 야생에서 비디오에서 인간과 카메라 동작 분리. IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2023년 6월.[81] Hangjie Yuan, Dong Ni, and Mang Wang. 그룹 활동 인식을 위한 시공간적 동적 추론 네트워크. ICCV, 2021.[82] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, Limin Wang. 효율적인 비디오 프레임 보간을 위한 프레임 간 주의를 통한 동작 및 외관 추출. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 56825692쪽, 2023. 2, 3, 6,[83] Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng, Farley Lai, Long Zhao, Ting Liu, Mubbasir Kapadia, Hans Peter Graf. 작곡가: 키포인트 전용 모달리티를 사용한 비디오의 그룹 활동에 대한 구성적 추론. ECCV, 2022.
