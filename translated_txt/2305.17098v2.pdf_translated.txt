--- ABSTRACT ---
이 논문은 텍스트 기반 비디오 편집을 위한 ControlVideo를 제시합니다. 주어진 텍스트와 일치하면서 소스 비디오의 구조를 유지하는 비디오를 생성합니다. 사전 훈련된 텍스트-이미지 확산 모델을 기반으로 하는 ControlVideo는 추가 조건(예: 에지 맵)을 통합하고, 디자인 공간에 대한 심층적인 탐색을 통해 소스 비디오-텍스트 쌍에 대한 키 프레임 및 시간적 주의를 미세 조정하여 충실도와 시간적 일관성을 향상시킵니다. 광범위한 실험 결과에 따르면 ControlVideo는 소스 콘텐츠와 관련하여 높은 충실도와 시간적 일관성을 보이는 비디오를 제공하면서 텍스트와 일치함으로써 다양한 경쟁 기준선보다 성능이 뛰어납니다. 훈련 전에 Lowrank 적응 계층을 모델에 통합함으로써 ControlVideo는 참조 이미지와 완벽하게 일치하는 비디오를 생성할 수 있는 권한이 더욱 강화됩니다. 더 중요한 것은 Control Video가 긴 비디오 편집(예: 수백 개의 프레임)과 같은 더 어려운 작업으로 쉽게 확장될 수 있다는 것입니다. 이 경우 장거리 시간적 일관성을 유지하는 것이 중요합니다. 이를 달성하기 위해 기본 ControlVideo를 겹치는 짧은 비디오 세그먼트와 키 프레임 비디오에 적용한 다음 미리 정의된 가중치 함수로 병합하여 융합된 ControlVideo를 구성하는 것을 제안합니다. 경험적 결과는 140개 프레임에 걸쳐 비디오를 만드는 기능을 검증하는데, 이는 이전 작업에서 달성한 것보다 약 5.83~17.5배 더 많은 것입니다. 코드는 https://github.com/thu-ml/controlvideo에서 사용할 수 있으며 시각화 결과는 HERE에서 사용할 수 있습니다.
--- INTRODUCTION ---
텍스트 기반 비디오 편집의 목적은 텍스트 프롬프트와 기존 비디오 영상에서 파생된 비디오를 생성하여 수동 노동을 줄이는 것입니다. 이 기술은 광고, 마케팅, 소셜 미디어 콘텐츠와 같은 다양한 분야에 상당한 영향을 미칠 것으로 예상됩니다. 이 과정에서 편집된 비디오는 소스 비디오의 콘텐츠를 충실히 보존하고 생성된 프레임 간의 시간적 일관성을 유지하며 제공된 텍스트와 선택적 참조 이미지와 일치시키는 것이 중요합니다. 그러나 이러한 모든 요구 사항을 동시에 충족하려면 상당한 과제가 있습니다. 게다가 일반적으로 수백 개의 프레임으로 구성된 실제 비디오를 처리할 때 또 다른 과제가 발생합니다. 장거리 시간적 일관성을 어떻게 유지할 수 있을까요? 이전 연구[1-4]는 대규모 텍스트-이미지(T2I) 확산의 발전을 활용하여 제로샷 및 원샷 설정에서 텍스트 기반 비디오 편집에서 상당한 진전을 이루었습니다. *연락 저자. 사전 인쇄본. 검토 중. (a) 단일 컨트롤 &quot;자동차, 가을&quot; + Canny Edge Maps &quot;스와로브스키 크리스털 백조가 강에서 수영&quot; HED + Boundary &quot;지프차가 도로를 달리고 있습니다, 눈 덮인 겨울&quot; &quot;셜록 홈즈가 춤추고 있습니다, + Depth + 런던 거리의 포즈, 비가 내립니다&quot; (b) 다중 컨트롤 &quot;판다가 춤추고 있습니다&quot; ControlControlEditing 다중 컨트롤을 사용한 편집 Canny Control을 사용한 편집 포즈 컨트롤을 사용한 편집 (c) 이미지 기반 편집 소스 참조 이미지를 사용한 비디오 편집 I 참조 이미지를 사용한 편집 II (d) 긴 비디오 편집 &quot;드레스를 입은 에반젤리넬리&quot;[140프레임] &quot;구풍 스타일, 여자, 검은 머리, 긴 머리, 보석&quot; 캐나다 QAD 웹사이트 그림 1: (a) 단일 컨트롤, (b) 다중 컨트롤, (c) 이미지 기반 비디오 편집, (d) 긴 비디오 편집. 모델[5, 6] 및 이미지 편집 기술[7-9]. 그러나 이러한 발전에도 불구하고 여전히 앞서 언급한 과제를 해결할 수 없습니다. 첫째, 경험적 증거(그림 6 참조)에 따르면 기존 접근 방식은 시간적 일관성을 유지하면서 출력을 충실하게 제어하는 것과 같이 텍스트 기반 비디오 편집의 세 가지 요구 사항을 동시에 충족하는 데 여전히 어려움을 겪고 있습니다. 둘째, 이러한 접근 방식은 주로 짧은 비디오 편집, 특히 프레임보다 짧은 비디오에 초점을 맞추고 장시간에 걸쳐 시간적 일관성을 유지하는 방법을 탐구하지 않습니다. 첫 번째 과제를 해결하기 위해 사전 훈련된 T2I 확산 모델을 기반으로 충실하고 시간적으로 일관된 비디오 편집을 위한 Control Video를 제시합니다. 충실도를 높이기 위해 에지 맵과 같은 시각적 조건을 T2I 확산 모델에 추가 입력으로 통합하여 소스 비디오의 안내를 증폭하는 것을 제안합니다. ControlNet[10]은 확산 모델과 함께 사전 훈련되었으므로 이러한 시각적 조건을 처리하는 데 활용합니다. 다양한 시각적 조건이 소스 비디오의 다양한 수준의 정보를 포함한다는 것을 인식하고, 우리는 다양한 장면에 대한 다양한 시각적 조건의 적합성에 대한 포괄적인 조사에 착수합니다. 이 탐구는 자연스럽게 여러 제어를 결합하여 각각의 이점을 활용하게 합니다. 나아가, 우리는 원래의 공간적 자기 주의를 키 프레임 주의로 변환하여 모든 프레임을 선택한 프레임과 맞추고, 시간적 주의 모듈을 확산 모델에 추가 분기로 통합하여 충실도와 시간적 일관성을 더욱 개선합니다. 이는 체계적인 경험적 연구에 의해 설계되었습니다. 또한, 제어 비디오는 훈련 전에 확산 모델에 저순위 적응(LORA)[11] 계층을 도입하여 선택적 참조 이미지와 일치하는 비디오를 생성할 수 있습니다. 경험적으로, 우리는 이전 연구[1, 3, 4]와 인터넷에 따라 Davis 데이터 세트에서 수집한 50개의 비디오-텍스트 쌍 데이터에서 우리 방법을 검증합니다. 우리는 객관적인 지표와 사용자 연구에 따라 안정적 확산 및 SOTA 텍스트 기반 비디오 편집 방법[1, 3, 4]과 비교합니다. 특히, [1, 4]에 따라 CLIP [12]을 사용하여 텍스트 정렬 및 시간적 일관성을 측정하고 SSIM을 사용하여 충실도를 평가합니다. 광범위한 결과에 따르면 Control Video는 텍스트 기반 비디오 편집의 세 가지 요구 사항을 동시에 충족하여 다양한 경쟁자보다 성능이 뛰어납니다. 특히 Control Video는 매우 사실적인 시각적 품질의 비디오를 제작하고 텍스트 지침을 따르면서 원본 소스 콘텐츠를 매우 충실하게 보존할 수 있습니다. 예를 들어, Control Video는 모든 기존 방법이 실패하는 동안 정체성을 유지하면서 여성을 성공적으로 만들어낼 수 있습니다(그림 6 참조). 더욱이 Control Video는 앞서 언급한 두 번째 과제, 즉 수백 개의 프레임을 포함하는 긴 비디오의 비디오 편집을 위해 쉽게 확장할 수 있습니다(3.2절 참조). 이를 달성하기 위해 기본 ControlVideo를 겹치는 짧은 비디오와 주요 프레임 비디오에 적용한 다음 각 노이즈 제거 단계에서 정의된 가중치 함수로 병합하여 융합된 ControlVideo를 구성하는 것을 제안합니다. 직관적으로 겹치는 짧은 비디오와의 융합은 겹치는 프레임이 인접한 짧은 비디오의 기능을 병합하도록 장려하여 인접한 비디오 클립 간의 불일치 문제를 효과적으로 완화합니다. 반면, 각 비디오 세그먼트의 첫 번째 프레임을 통합하는 키 프레임 비디오는 전체 비디오에서 글로벌 가이드를 제공하므로, 이를 융합하면 장거리 시간적 일관성을 더욱 개선할 수 있습니다. 경험적 결과는 ControlVideo가 140개 프레임에 걸친 비디오를 생성할 수 있는 능력을 확인시켜 주는데, 이는 이전 작업에서 처리한 것보다 약 5.83~17.5배 더 깁니다. 2 배경 2. 이미지 생성 및 편집을 위한 확산 모델 q(x)를 RD의 데이터 분포라고 합시다. 확산 모델[13–15]은 순방향 확산 과정을 통해 데이터 xo ~ q(x)를 점진적으로 교란합니다.T q(×1:T) = q(x0) II 9(xt|xt−1), q(xt|xt−1) = N(×t; √αtxt−1, ßtĪ), t=(1) 여기서 ẞt는 노이즈 스케줄, at = 1 − ẞt이며 xT ~N(0, 1)을 만족하도록 설계되었습니다. 순방향 과정 {xt}te[0,T]는 다음과 같은 전이 분포를 갖습니다.It|0(xt|xo) = N(xt|√āt×0, (1 – āt)I), 여기서 at = 1 as. 데이터는 역 확산 프로세스를 통해 xτ ~ N(0, 1)에서 시작하여 생성될 수 있으며, 이때 역 전이 커널 q(xt−1 xt)는 가우시안 모델에 의해 학습됩니다: Po(xt-1xt) N(x+ 1;t−1; µØ(×t),σI). Ho et al. [15]은 평균 μ(x+)를 학습하여 평균 제곱 오차 손실을 통해 노이즈 예측 네트워크 €0(x, t)를 학습할 수 있음을 보여줍니다: = min Et,xo,€€€(xt, t)||², Ꮎ (3) 여기서 xt~ Ito (xt |xo), ~N(0, 1). 결정론적 DDIM 샘플링[16]은 다음 반복 규칙을 통해 xT ~N(0, 1)에서 시작하는 샘플을 생성합니다.xt xt-1 = at-√1 – α₁€ (x, t) √at + 1 — αt−1€ (x, t).xt-(4) 고품질 샘플을 생성하는 기능 덕분에 확산 모델은 자연스럽게 이미지 변환 및 이미지 편집에 적용됩니다[7, 17, 18].무조건 생성과 달리 일반적으로 소스 이미지 xo의 내용을 보존해야 합니다.ODE의 가역적 속성을 고려하여 DDIM 역산[16]을 채택하여 위의 프로세스를 반대로 수행하여 충실한 이미지 편집을 위해 실제 이미지 xo를 관련 역산 노이즈 xм로 변환합니다.-xt-1 - √1 — αt-1 €0 (x−1, t − 1) xt = + √√1 − α+€¤ (xt−1, t − 1). (5) αt-2.2 잠재 확산 모델 및 ControlNet = 계산 비용을 줄이기 위해 잠재 확산 모델(LDM, 안정 확산이라고도 함) [5]은 인코더 &amp;를 사용하여 xo를 저차원 잠재 공간 zo E(xo)로 변환합니다.이는 디코더 xo ≈ D(zo)로 재구성할 수 있으며, 그런 다음 잠재 공간에서 노이즈 예측 네트워크 ε0(zt, p, t)를 학습합니다.여기서 p는 텍스트 프롬프트입니다.ε(zt, p, t)의 백본은 여러 기본 블록을 쌓는 UNet(주 UNet이라고 함)입니다.특히 U-Net은 인코더, 중간 블록, 디코더로 구성됩니다.인코더와 디코더는 각각 12개의 블록으로 구성되는 반면 전체 모델은 총 25개의 블록을 포함합니다.이러한 블록 내에서 8개는 다운샘플링 또는 업샘플링 합성곱 계층에 사용되고 나머지 블록은 기본 구성 블록을 구성합니다. 각 기본 블록은 변환기 블록과 잔여 블록으로 구성됩니다.변환기 블록은 셀프 어텐션 계층, 교차 어텐션 계층 및 피드포워드 신경망을 통합합니다.CLIP 텍스트 인코더에서 처리한 텍스트 임베딩은 교차 어텐션 계층을 통해 U-Net에 통합됩니다.모델이 추가 조건 c를 학습할 수 있도록 ControlNet[10]은 인코더의 학습 가능한 사본과 메인 UNet의 중간 블록(ControlNet이라고 함)을 추가하여 잠긴 안정적 확산에 작업별 조건을 통합합니다.그런 다음 ControlNet의 출력 뒤에 0-초기화 합성곱 계층이 나오고, 이는 이후 해당 계층에서 메인 U-Net의 기능에 추가됩니다.3가지 방법 1절에서 언급한 과제를 해결하기 위해 먼저 사전 학습된 T2I 확산 모델을 기반으로 하는 충실하고 시간적으로 일관된 텍스트 기반 비디오 편집을 위한 Control Video를 제시합니다(3.1절 참조). 그런 다음 두 번째 과제인 수백 개의 프레임을 포함하는 긴 비디오의 비디오 편집을 위해 Control Video를 확장합니다(3.2절 참조). 3. Control Video 이 섹션에서는 먼저 디자인 공간에 대한 심층적인 탐색을 통해 ControlVideo의 아키텍처를 소개합니다(3.1.1절 참조). 그림 2에서 볼 수 있듯이 ControlVideo는 추가 조건, 키 프레임 미세 조정 및 시간적 주의를 통합합니다. 3.1.2절에서는 ControlVideo의 학습 및 샘플링 프레임워크를 제시합니다. 나아가 ControlVideo가 3.1.3절에서 Low-rank 적응 계층을 통합하여 선택적 참조 이미지와 일치하는 비디오를 생성하는 방법을 보여줍니다. 3.1. 아키텍처 T2I 확산 모델은 대규모 텍스트-이미지 데이터에서 사전 학습되었으므로 이를 기반으로 주어진 텍스트와 일치하도록 구축합니다. 이전 연구[1, 3]에 따라 먼저 2D 합성곱 계층의 공간 커널(3 × 3)을 3D 커널(1 × 3 × 3)로 대체하여 비디오 입력을 처리합니다. = 시각적 컨트롤 추가. 텍스트 기반 비디오 편집의 핵심 목표는 소스 비디오의 콘텐츠를 충실하게 보존하는 것입니다. 직관적인 접근 방식은 Xo의 정보를 활용하기 위해 방정식 5의 DDIM 역전 Xм에서 시작하여 편집된 비디오를 생성하는 것입니다. 그러나 그림 3에 나와 있는 것처럼 ODE의 가역적 특성에도 불구하고 경험적으로 DDIM 역전과 DDIM 샘플링을 결합하면 소스 비디오의 구조가 크게 붕괴됩니다. 충실도를 높이기 위해 모든 프레임에 대한 에지 맵과 같은 추가적인 시각적 조건 C{1을 주 UNet에 도입하여 초기 시간만이 아니라 각 시간 단계에서 소스 비디오의 안내를 증폭할 것을 제안합니다: (X+, C, p, t). 특히, ControlNet[10]이 Stable Diffusion에서 주 UNet과 함께 사전 학습되었으므로, 우리는 이를 사용하여 이러한 시각적 조건 C를 처리합니다. 형식적으로, h₂ ЄRNxd 및 he ЄRNxd가 각각 주 UNet 및 ControlNet의 동일한 계층의 차원 d를 갖는 숨겨진 특징을 나타낸다고 합시다. 우리는 이러한 특징을 합산하여 결합하여 h huhe를 얻고, 이는 i=1&#39;= A가 제어 스케일 역할을 하는 스킵 연결을 통해 주 UNet의 디코더에 입력됩니다. 그림 3에서 볼 수 있듯이, Xo로부터 구조적 안내를 제공하기 위해 시각적 조건을 도입하면 편집된 비디오의 충실도가 크게 향상됩니다. 또한, 서로 다른 시각적 조건이 Xo에서 파생된 다양한 정도의 정보를 포함한다는 점을 감안할 때, 우리는 서로 다른 조건을 사용하는 이점을 종합적으로 조사합니다. 그림 1에서 볼 수 있듯이, 우리의 연구 결과에 따르면 에지 맵과 같이 Xo에 대한 자세한 통찰력을 제공하는 조건은 얼굴 비디오 편집과 같은 속성 조작에 특히 유리하며, 인간의 정체성을 보존하기 위해 정확한 제어가 필요합니다. 반대로, Xo에 대한 더 거친 통찰력을 제공하는 조건, (a) 인코더 중간 디코더 블록 블록 블록 소스 비디오 컨트롤 (b) 소스 프롬프트 &quot;자동차&quot; 겹치는 짧은 비디오 NSV를 사용한 융합된 특징 KFV를 사용한 융합된 특징 훈련 기본 블록 의사 3D ResNet 예측된 노이즈 키 프레임 어텐션 시간적 어텐션 제로 합성 소스 비디오 교차 어텐션 피드포워드 신경망 시간적 어텐션이 있는 편집된 비디오 키 프레임 어텐션 시간적 어텐션 키, 값 없음 쿼리 시간적 어텐션 [T 프레임] 제어 비디오 퓨전과 이웃하는 짧은 비디오 추론 가우시안 노이즈/ 노이즈가 있는 소스 비디오/ DDIM 반전 제어 비디오 초기 값 DDIM 샘플링 대상 프롬프트 &quot;자동차, 가을&quot; 각 비디오의 첫 번째 프레임 키 프레임 비디오 제어 비디오 퓨전과 키 프레임 비디오 융합 제어 비디오 그림 2: (a) ControlVideo의 개요. 왼쪽: 아키텍처. ControlVideo는 추가 제어를 통합하고, 키 프레임 주의와 시간적 주의를 미세 조정합니다. 주의 모듈은 T2I 확산 모델의 자기 주의 가중치를 사용하여 초기화됩니다. 오른쪽: 추론 프레임워크. 편집 시나리오에 따라 초기 값을 도출하는 세 가지 방법이 있습니다(섹션 3.1.2 참조). (b) 긴 비디오 편집을 위한 확장된 Control Video 개요. NSV와 KFV는 각각 인접한 짧은 비디오와 키 프레임 비디오를 나타냅니다. 포즈 정보와 같은 정보는 모양과 배경에 대한 유연한 조정을 용이하게 합니다. 이 탐색은 자연스럽게 여러 제어를 결합하여 각각의 장점을 활용할 수 있는지에 대한 의문을 제기합니다. 이를 위해 h = huhe로 표시되는 다양한 제어에서 파생된 숨겨진 기능의 가중 합을 계산한 다음 융합된 기능을 주 UNet의 디코더에 입력합니다. 여기서 λ는 i번째 제어와 관련된 제어 스케일을 나타냅니다. 여러 컨트롤이 충돌이나 불일치를 보일 수 있는 상황에서는 SAM[19] 또는 교차 주의 맵[7]을 사용하여 텍스트를 기반으로 마스크를 생성하고 마스크된 컨트롤을 Control Video에 공급하여 컨트롤 시너지를 향상시킬 수 있습니다.그림 1에서 볼 수 있듯이 Canny 에지 맵은 모양 수정에 미치는 영향이 제한적이면서 배경을 보존하는 데 탁월합니다.반대로 포즈 컨트롤은 유연한 모양 조정을 용이하게 하지만 다른 중요한 세부 사항을 간과할 수 있습니다.이러한 컨트롤을 결합하면 배경을 보존하고 모양 수정에 동시에 효과를 줄 수 있어 복잡한 비디오 편집 시나리오에서 여러 컨트롤을 활용하는 것이 가능함을 보여줍니다.키 프레임 주의.T2I 확산 모델은 각 프레임의 기능을 독립적으로 업데이트하고 프레임 간에 상호 작용이 없으므로 시간적 불일치가 발생합니다.이 문제를 해결하고 시간적 일관성을 개선하기 위해 비디오 전체에 걸쳐 정보를 전파하기 위한 참조 역할을 하는 키 프레임을 도입합니다. 구체적으로, 이전 연구[3]에서 영감을 얻어 기본 UNet과 ControlNet 모두에서 공간적 자기 주의력을 키 프레임 주의력으로 변환하여 모든 프레임을 선택된 참조 프레임에 맞춥니다. 형식적으로, v² = Rd가 i번째 프레임의 숨겨진 특징을 나타내고, k = [1, N]이 선택된 키 프레임을 나타낸다고 합니다. 키 프레임 주의 메커니즘은 다음과 같이 정의됩니다. Q = WQv², K = WKyk, ‚V = WV vk, 여기서 WQ, WK, WV는 투영 행렬입니다. T2I 확산 모델의 기능을 최대한 활용하기 위해 원래의 자기 주의력 가중치를 사용하여 이러한 행렬을 초기화합니다. 경험적으로, 우리는(a) [100개 프레임] 소스 비디오 가중치 겹치는 길이 L &quot;자동차&quot; &quot;자동차, 빈센트 반 고흐 스타일&quot; 소스 비디오 짧은 비디오를 독립적으로 편집 이웃 짧은 비디오와의 융합 [100개 프레임] +컨트롤 이웃 짧은 비디오 및 키 프레임 비디오와의 융합 (b) 소스 DDIM +Ker-프레임 +시간적 비디오 반전 &quot;소녀&quot; &quot;풍부한 화장을 한 소녀&quot; 전체 버전 At. At. 소스 비디오 DDIM 반전 +Ker-프레임 +시간적 +컨트롤 전체 버전 At. At. &quot;자동차&quot; → &quot;빨간 자동차&quot; &quot;사람이 춤추고 있다&quot; &quot;판다가 춤추고 있다&quot; 그림 3: (a) 긴 비디오 편집을 위한 키 프레임 비디오 융합을 위한 융합 전략, 겹치는 길이 a 및 가중치 w에 대한 소거 연구. 3.2절 및 5.2.3절에서 자세한 분석을 참조하세요. (b) 제어 비디오의 주요 구성 요소에 대한 소거 연구. At. 주의를 나타냅니다. 3.2절에서 자세한 분석을 참조하세요. 5.2.3. 자기 주의와 미세 조정된 매개변수에서 키 프레임, 키 및 값 선택의 설계를 체계적으로 연구합니다. 자세한 분석은 부록에 제공됩니다. 요약하면, 주의 메커니즘에서 키와 값으로 사용되는 첫 번째 프레임을 키 프레임으로 활용하고, 주의 모듈 내에서 출력 투영 행렬 W°를 미세 조정하여 시간적 일관성을 향상시킵니다. 시간적 주의. 편집된 비디오의 충실도와 시간적 일관성을 모두 향상시키기 위해, 우리는 모든 프레임에서 해당 공간 위치 간의 관계를 포착하는 네트워크의 추가 분기로 시간적 주의 모듈을 도입합니다. 공식적으로, 숨겨진 특징을 v = RN×d라고 하면 시간적 주의는 다음과 같이 정의됩니다. Q = WQv, K = WK v,V = WVv. 이전 연구[20]는 시간적 주의를 훈련하기 위해 광범위한 데이터로부터 이점을 얻었는데, 이는 우리의 원샷 설정에서는 없는 사치입니다. 이 과제를 해결하기 위해, 우리는 서로 다른 주의 메커니즘이 이미지 특징 간의 관계를 모델링하는 일관된 방식에서 영감을 얻습니다. 따라서 원래의 공간적 자기 주의 가중치를 사용하여 시간적 주의를 초기화하고 T2I 확산 모델의 기능을 활용합니다. 각 시간적 주의 모듈 후에 미세 조정 전에 모듈의 출력을 사전 유지하기 위해 제로 합성 계층[10]을 통합합니다. 나아가 시간적 주의를 도입하기 위해 로컬 및 글로벌 위치를 통합하는 것에 대한 포괄적인 연구를 수행합니다. 정성적 결과는 그림 4에 나와 있습니다. 변환기 블록의 로컬 위치와 관련하여 가장 효과적인 배치는 자기 주의 메커니즘의 앞과 내부에 있는 것입니다. 이 선택은 이 두 위치의 입력이 자기 주의의 입력과 일치하여 시간적 주의의 초기 가중치 역할을 한다는 사실에 의해 입증됩니다. 자기 주의 위치는 더 높은 텍스트 정렬을 보여 궁극적으로 선호하는 선택이 됩니다. ControlVideo의 글로벌 위치에 대해 우리의 주요 발견은 위치의 효과성은 캡슐화하는 정보의 양과 상관 관계가 있다는 것입니다. 예를 들어, 이미지 생성을 담당하는 주요 UNet은 정보의 전체 스펙트럼을 유지하여 다른 기능은 무시하고 조건 관련 기능만 추출하는 데 집중하는 ControlNet보다 성능이 뛰어납니다. 결과적으로 중간 블록을 제외한 주요 UNet의 모든 단계에서 자기 주의와 함께 시간적 주의를 통합합니다. 더 자세한 분석은 부록 3.1.2에 나와 있습니다. 학습 및 샘플링 프레임워크 C = {c²}₁은 X에 대한 시각적 조건(예: Canny 에지 맵)을 나타내고 ε(Xt,C,p,t)는 제어 비디오 네트워크를 나타냅니다. p 및 pt는 각각 소스 프롬프트와 대상 프롬프트를 나타냅니다. 방정식과 유사합니다. 3, 우리는 다음과 같이 정의된 평균 제곱 오차 손실을 사용하여 소스 비디오-텍스트 쌍(Xo, ps)에서 ε0(Xt, C, p, t)를 미세 조정합니다. ~ min Et,e||€ – €(Xt, C, ps, t)||², Ꮎ여기서 € ~ N(0, I), Xt Ito(Xt Xo). 훈련하는 동안 우리는 다른 모든 매개변수를 고정한 채로 주의 모듈 내의 매개변수만을 독점적으로 최적화합니다(3.1.1절에서 논의한 대로). 초기 값 XM 선택. ε(Xt, C, p, t)를 기반으로, 대상 프롬프트 pt를 기반으로 DDIM 샘플링[16]을 사용하여 초기 값 XM에서 시작하여 편집된 비디오를 생성할 수 있습니다. Xм의 경우, 속성 조작과 같은 로컬 편집 작업을 위해 Eq. 5에 설명된 대로 DDIM 역전을 사용합니다. 전역 편집의 경우, 이전 작업[1, 3]과 달리, 큰 M과 짝수 XM N(0, 1)을 사용하여 Eq. 2에서 전방 전환 분포를 사용하여 노이즈가 있는 소스 비디오 XM ~ 9M|0(XM|X0)에서 시작하여 시각적 조건이 이미 Xo로부터 구조 지침을 제공했기 때문에 편집 가능성을 개선할 수도 있습니다. 이 프로세스 동안 샘플링된 노이즈는 시간적 일관성을 위해 모든 프레임에서 공유됩니다. 알고리즘 1 긴 비디오 편집을 위한 확장된 ControlVideo ~ 요구 사항: 초기 값 XM, 컨트롤 C, 짧은 비디오 길이 L, 중첩 길이 a, 융합 함수 F(), 가중치 w, 모델 € (·, ·, ·, ·), 프롬프트 pn = [N/(L- a)] + tM에서 1까지 do j = 1 톤까지 do &lt; € (X, C³,p,t) E까지 종료 € KF(E,..., €) € (XK, CK,p,t) €0← wo() + (1 – w)êo Xt-1DDIM_Sampling(€0, Xt, t) Xo로 돌아가기 종료 3.1.3 이미지 기반 비디오 편집 ▶ 짧은 비디오 수 ► 각 짧은 비디오에 대한 ControlVideo는 Eq.를 통해 인접한 짧은 비디오와 융합합니다.► 키 프레임 비디오에 대한 ControlVideo ▶ Eq.를 통해 키 프레임 비디오와 융합합니다.▶ Eq.의 노이즈 제거 단계 특정 시나리오에서 텍스트 설명이 누락될 수 있습니다. 사용자의 원하는 정확한 효과를 완전히 전달하지 못하는 경우. 이러한 경우 사용자는 생성된 비디오가 주어진 참조 이미지와도 일치하기를 원할 수 있습니다. 여기서는 이미지 기반 비디오 편집을 위해 ControlVideo를 확장하는 간단한 방법을 보여줍니다. 구체적으로, 먼저 주 UNet에 Low-rank adaptation(LoRA)[11] 계층을 추가하여 참조 이미지와 관련된 개념의 학습을 용이하게 한 다음 Sec. 3.1.2에 따라 Control Video를 학습하기 위해 이를 동결할 수 있습니다. 참조 이미지와 비디오에 대한 학습은 독립적이므로 CivitAI와 같은 커뮤니티의 모델을 유연하게 활용할 수 있습니다. 3.2 긴 비디오 편집을 위한 확장된 ControlVideo 위 섹션에서 설명한 ControlVideo는 시간적으로 일관된 비디오를 생성하는 매력적인 기능이 있지만 메모리 제한으로 인해 일반적으로 수백 개의 프레임을 포함하는 실제 비디오를 처리하는 것은 여전히 어렵습니다. 이 문제를 해결하는 간단한 방법은 전체 비디오를 여러 개의 겹치지 않는 짧은 세그먼트로 나누고 각 세그먼트에 Control Video를 독립적으로 적용하는 것입니다. 그러나 그림 3에서 볼 수 있듯이 이 방법은 여전히 비디오 클립 간에 시간적 불일치가 발생합니다. 이 문제를 해결하기 위해 우리는 각 노이즈 제거 단계에서 짧은 비디오 사이를 연결하는 프레임의 특징을 융합하는 것을 제안합니다. 이를 달성하기 위해 그림 2에서 보듯이 전체 비디오를 겹치는 짧은 비디오로 분할하고 각 세그먼트에 제어 비디오를 적용한 다음 미리 정의된 가중치 함수를 통해 인접한 짧은 비디오의 겹치는 프레임의 특징을 병합합니다. 여기서 가중치 융합 전략은 이미지 생성 작업에서도 사용됩니다[21]. 또한, 이후의 노이즈 제거 단계에서는 겹치지 않는 것과 겹치는 것(a) 다른 초기화와의 비교 (b) 변압기 블록에서 시간적 주의의 다른 로컬 위치와의 비교 소스 비디오 사전 학습 가중치를 사용한 랜덤 초기화 소스 비디오 자기 주의 이전 자기 주의 이후 자기 주의 이후 교차 주의 이후 FNN &quot;아름다운 풍경이 있는 여성의 뒷모습&quot;&quot;, 별이 빛나는 하늘&quot; &quot;아름다운 풍경이 있는 여성의 뒷모습&quot; &quot;..., 일출, 이른 아침&quot; (c) 시간적 주의의 다른 글로벌 위치와의 비교 ControlNet + 소스 비디오 UNet ControlNet UNet 인코더 UNet 디코더 UNet 블록 1,2, UNet 블록 1, UNet 블록 2, UNet HHHHHHH &quot;사람이 춤추고 있다&quot; &quot;판다가 춤추고 있다&quot; 그림 4: (a) 초기화 방법과 시간적 주의를 도입하기 위한 (b) 로컬 위치 및 (c) 글로벌 위치의 통합에 대한 절제 연구. 녹색은 우리의 선택을 표시했습니다. 짧은 비디오 클립 내의 프레임은 Control Video에 함께 공급되어 겹치지 않는 프레임의 특징이 겹치는 프레임의 특징에 더 가까워지므로 간접적으로 전역 시간적 일관성이 향상됩니다.형식적으로 j번째 짧은 비디오 클립 X와 해당 시각적 조건 C³는 다음과 같이 정의됩니다. &quot; C³ = {c}=(-1)(La)+min((j−1)(La) +L,N) &quot; j Є [1, n] X² = {x}i=(j−1)(La)+imin((j−1)(La) +L,N) (6) 여기서 n = [N/(L − a)] + 1은 짧은 비디오 클립의 수이고, L은 짧은 비디오 클립의 길이이며, a는 겹친 길이입니다. &amp; € RL×D = €0 (X²², C&#39;³‚p, t)는 j번째 짧은 비디오의 ControlVideo를 나타내고 ê0 € RND는 전체 비디오의 융합된 ControlVideo를 나타냅니다. 융합 함수 F(): RnxLxD →RNXD는 다음과 같이 정의됩니다. = F(ε √ √, ..., ε ) = Sum (Normalize(O(w; &amp; 1D)) O(€²)), (7) 여기서 w; ER은 j번째 짧은 비디오의 가중치 벡터이고, 1ɲ Є R³는 1의 벡터이고, ◇는 벡터 외적이며, 는 요소별 곱셈이고 Sum(•)는 행렬의 해당 위치에 요소를 추가합니다. O(.) : RLXD → RND는 0 패딩을 나타냅니다. 예를 들어, O(€)는 j번째 비디오의 해당 프레임 인덱스가 6이고 다른 프레임 인덱스는 0임을 나타냅니다. Normalize()는 해당 위치에서 합으로 행렬 요소를 조정하여 융합 가중치의 합이 1이 되도록 하고 융합 후 값 범위를 유지합니다. 이 작업에서 우리는 정규 난수 변수 w; ~ N(l; L/2,σ²)를 정의합니다. 여기서 σ = 0.1입니다. 대체 가중치 함수가 테스트되었고, 그 결과는 함수 선택에 대한 무감각함을 나타냈습니다(5.2.4절 참조). 그림 3에서 볼 수 있듯이, 이 융합 전략은 짧은 비디오 간의 시간적 일관성을 크게 향상시킵니다. 그러나 이 접근 방식은 인접한 비디오를 직접 융합하여 인접한 비디오 클립 간의 로컬 일관성을 보장하고, 전체 비디오의 글로벌 일관성은 반복적인 노이즈 제거 단계 동안 간접적으로 향상됩니다. 결과적으로 그림 3에서 볼 수 있듯이 비디오 클립이 더 멀리 떨어져 있을 때 시간적 일관성이 저하되며, 이는 검은색 차가 녹색 차로 저하되는 것으로 예시됩니다. 이러한 관찰에 비추어 볼 때 자연스럽게 다음과 같은 의문이 생깁니다. 더 많은 글로벌 기능을 직접 융합하여 장거리 시간적 일관성을 더욱 향상시킬 수 있을까요? 이를 달성하기 위해 각 짧은 비디오 세그먼트의 첫 번째 프레임을 통합하여 키 프레임 비디오를 만들어 글로벌 안내를 직접 제공합니다. ControlVideo는 그런 다음 0.0.Weight 0.0.1.1과 융합되는 이 키 프레임 비디오에 적용됩니다.(a) 다양한 가중치 함수의 시각화 (b) 다양한 가중치 함수의 결과 역제곱 가우시안 소스 가우시안 루트(볼록) 가우시안 선형 상수 코사인 exp 볼록 inverse_square_root 볼록 가우시안l 프레임=프레임=프레임=프레임=코사인 지수 선형 상수 그림 5: (a) 다양한 가중치 함수의 시각화. 여기서는 L = 25를 예로 들어 설명합니다. (b) 다양한 가중치 함수로 편집한 결과. = 이전에 얻은 ĉe. 공식적으로 X : {x(−1)(La)+1}}=1은 키 프레임 비디오를 나타내고 CK = {c(-1)(La)+1}; 21은 해당 시각적 조건을 나타냅니다. 최종 모델 eð는 다음과 같이 정의됩니다. €g = wo(€ỗ) + (1 – w)ĉo, (8) 여기서 w € [0,1]은 가중치이고 € = €(XK, C, p, t)입니다. 여기서 키 프레임 비디오의 프레임은 키 프레임 어텐션의 각 짧은 비디오의 키 프레임으로도 선택되어 전역 시간 일관성이 보장됩니다. 전체 알고리즘은 알고리즘 1에 나와 있습니다. 그림 3에서 볼 수 있듯이 키 프레임 비디오 퓨전 전략을 사용하면 자동차의 색상이 전체 비디오에서 일관되게 유지됩니다. 4
--- RELATED WORK ---
4.1 텍스트 기반 생성 및 이미지 편집을 위한 확산 모델 최근 확산 모델은 생성 인공 지능 분야에서 큰 혁신을 이루었으며, 따라서 텍스트-이미지 생성에 활용됩니다[5, 22]. 이러한 모델은 일반적으로 대규모 이미지-텍스트 쌍 데이터 세트에서 텍스트를 조건으로 하는 확산 모델을 학습합니다. T2I 확산 모델의 이러한 놀라운 발전을 바탕으로 수많은
--- METHOD ---
s, 베이징, 중국 3성수, 베이징, 중국; 4파저우 연구실(황푸), 광저우, 중국 gracezhao1997@gmail.com; wangrz@ruc.edu.cn; bf 19@mails.tsinghua.edu.cn; chongxuanli@ruc.edu.cn; dcszj@tsinghua.edu.cn 초록 이 논문은 텍스트 기반 비디오 편집을 위한 ControlVideo를 제시합니다. 소스 비디오의 구조를 유지하면서 주어진 텍스트와 일치하는 비디오를 생성합니다. 사전 학습된 텍스트-이미지 확산 모델을 기반으로 하는 ControlVideo는 추가 조건(예: 에지 맵)을 통합하고 설계 공간에 대한 심층적 탐색을 통해 소스 비디오-텍스트 쌍에 대한 키 프레임 및 시간적 주의를 미세 조정하여 충실도와 시간적 일관성을 향상시킵니다. 광범위한
--- EXPERIMENT ---
모든 결과는 ControlVideo가 소스 콘텐츠에 대한 높은 충실도와 시간적 일관성을 보이는 비디오를 제공하면서 텍스트와 일치함으로써 다양한 경쟁적 기준선보다 성능이 우수하다는 것을 보여줍니다. 훈련 전에 Lowrank 적응 계층을 모델에 통합함으로써 ControlVideo는 참조 이미지와 매끄럽게 일치하는 비디오를 생성할 수 있는 권한이 더욱 강화됩니다. 더 중요한 것은 ControlVideo가 긴 비디오 편집(예: 수백 개의 프레임)과 같은 더 어려운 작업으로 쉽게 확장될 수 있다는 것입니다. 이 작업에서는 장거리 시간적 일관성을 유지하는 것이 중요합니다. 이를 달성하기 위해 기본 ControlVideo를 겹치는 짧은 비디오 세그먼트와 주요 프레임 비디오에 적용한 다음 미리 정의된 가중치 함수로 병합하여 융합된 ControlVideo를 구성하는 것을 제안합니다. 경험적 결과는 140개 프레임에 걸쳐 비디오를 생성할 수 있는 기능을 검증하는데, 이는 이전 작업에서 달성한 것보다 약 5.83~17.5배 더 많은 것입니다. 코드는 https://github.com/thu-ml/controlvideo에서 사용할 수 있으며 시각화 결과는 HERE에서 사용할 수 있습니다.소개 텍스트 기반 비디오 편집의 목적은 텍스트 프롬프트와 기존 비디오 영상에서 파생된 비디오를 생성하여 수동 노동을 줄이는 것입니다.이 기술은 광고, 마케팅, 소셜 미디어 콘텐츠와 같은 다양한 분야에 상당한 영향을 미칠 것으로 예상됩니다.이 과정에서 편집된 비디오는 소스 비디오의 콘텐츠를 충실히 보존하고 생성된 프레임 간의 시간적 일관성을 유지하며 제공된 텍스트 및 선택적 참조 이미지와 일치시키는 것이 중요합니다.그러나 이러한 모든 요구 사항을 동시에 충족하려면 상당한 과제가 필요합니다.게다가 일반적으로 수백 개의 프레임으로 구성된 실제 비디오를 처리할 때 또 다른 과제가 발생합니다.장거리 시간적 일관성을 어떻게 유지할 수 있을까요?이전 연구[1-4]는 대규모 텍스트-이미지(T2I) 확산의 발전을 활용하여 제로샷 및 원샷 설정에서 텍스트 기반 비디오 편집에서 상당한 진전을 이루었습니다.*연락 저자.사전 인쇄본.검토 중. (a) 단일 컨트롤 &quot;자동차, 가을&quot; + Canny Edge Maps &quot;스와로브스키 크리스털 백조가 강에서 수영&quot; HED + Boundary &quot;지프차가 도로를 달리고 있습니다, 눈 덮인 겨울&quot; &quot;셜록 홈즈가 춤추고 있습니다, + Depth + 런던 거리의 포즈, 비가 내립니다&quot; (b) 다중 컨트롤 &quot;판다가 춤추고 있습니다&quot; ControlControlEditing 다중 컨트롤을 사용한 편집 Canny Control을 사용한 편집 포즈 컨트롤을 사용한 편집 (c) 이미지 기반 편집 소스 참조 이미지를 사용한 비디오 편집 I 참조 이미지를 사용한 편집 II (d) 긴 비디오 편집 &quot;드레스를 입은 에반젤리넬리&quot;[140프레임] &quot;구풍 스타일, 여자, 검은 머리, 긴 머리, 보석&quot; 캐나다 QAD 웹사이트 그림 1: (a) 단일 컨트롤, (b) 다중 컨트롤, (c) 이미지 기반 비디오 편집, (d) 긴 비디오 편집. 모델[5, 6] 및 이미지 편집 기술[7-9]. 그러나 이러한 발전에도 불구하고 여전히 앞서 언급한 과제를 해결할 수 없습니다. 첫째, 경험적 증거(그림 6 참조)에 따르면 기존 접근 방식은 시간적 일관성을 유지하면서 출력을 충실하게 제어하는 것과 같이 텍스트 기반 비디오 편집의 세 가지 요구 사항을 동시에 충족하는 데 여전히 어려움을 겪고 있습니다. 둘째, 이러한 접근 방식은 주로 짧은 비디오 편집, 특히 프레임보다 짧은 비디오에 초점을 맞추고 장시간에 걸쳐 시간적 일관성을 유지하는 방법을 탐구하지 않습니다. 첫 번째 과제를 해결하기 위해 사전 훈련된 T2I 확산 모델을 기반으로 충실하고 시간적으로 일관된 비디오 편집을 위한 Control Video를 제시합니다. 충실도를 높이기 위해 에지 맵과 같은 시각적 조건을 T2I 확산 모델에 추가 입력으로 통합하여 소스 비디오의 안내를 증폭하는 것을 제안합니다. ControlNet[10]은 확산 모델과 함께 사전 훈련되었으므로 이러한 시각적 조건을 처리하는 데 활용합니다. 다양한 시각적 조건이 소스 비디오의 다양한 수준의 정보를 포함한다는 것을 인식하고, 우리는 다양한 장면에 대한 다양한 시각적 조건의 적합성에 대한 포괄적인 조사에 착수합니다. 이 탐구는 자연스럽게 여러 제어를 결합하여 각각의 이점을 활용하게 합니다. 나아가, 우리는 원래의 공간적 자기 주의를 키 프레임 주의로 변환하여 모든 프레임을 선택한 프레임과 맞추고, 시간적 주의 모듈을 확산 모델에 추가 분기로 통합하여 충실도와 시간적 일관성을 더욱 개선합니다. 이는 체계적인 경험적 연구에 의해 설계되었습니다. 또한, 제어 비디오는 훈련 전에 확산 모델에 저순위 적응(LORA)[11] 계층을 도입하여 선택적 참조 이미지와 일치하는 비디오를 생성할 수 있습니다. 경험적으로, 우리는 이전 연구[1, 3, 4]와 인터넷에 따라 Davis 데이터 세트에서 수집한 50개의 비디오-텍스트 쌍 데이터에서 우리 방법을 검증합니다. 우리는 객관적인 지표와 사용자 연구에 따라 안정적 확산 및 SOTA 텍스트 기반 비디오 편집 방법[1, 3, 4]과 비교합니다. 특히, [1, 4]에 따라 CLIP [12]을 사용하여 텍스트 정렬 및 시간적 일관성을 측정하고 SSIM을 사용하여 충실도를 평가합니다. 광범위한 결과에 따르면 Control Video는 텍스트 기반 비디오 편집의 세 가지 요구 사항을 동시에 충족하여 다양한 경쟁자보다 성능이 뛰어납니다. 특히 Control Video는 매우 사실적인 시각적 품질의 비디오를 제작하고 텍스트 지침을 따르면서 원본 소스 콘텐츠를 매우 충실하게 보존할 수 있습니다. 예를 들어, Control Video는 모든 기존 방법이 실패하는 동안 정체성을 유지하면서 여성을 성공적으로 만들어낼 수 있습니다(그림 6 참조). 더욱이 Control Video는 앞서 언급한 두 번째 과제, 즉 수백 개의 프레임을 포함하는 긴 비디오의 비디오 편집을 위해 쉽게 확장할 수 있습니다(3.2절 참조). 이를 달성하기 위해 기본 ControlVideo를 겹치는 짧은 비디오와 주요 프레임 비디오에 적용한 다음 각 노이즈 제거 단계에서 정의된 가중치 함수로 병합하여 융합된 ControlVideo를 구성하는 것을 제안합니다. 직관적으로 겹치는 짧은 비디오와의 융합은 겹치는 프레임이 인접한 짧은 비디오의 기능을 병합하도록 장려하여 인접한 비디오 클립 간의 불일치 문제를 효과적으로 완화합니다. 반면, 각 비디오 세그먼트의 첫 번째 프레임을 통합하는 키 프레임 비디오는 전체 비디오에서 글로벌 가이드를 제공하므로, 이를 융합하면 장거리 시간적 일관성을 더욱 개선할 수 있습니다. 경험적 결과는 ControlVideo가 140개 프레임에 걸친 비디오를 생성할 수 있는 능력을 확인시켜 주는데, 이는 이전 작업에서 처리한 것보다 약 5.83~17.5배 더 깁니다. 2 배경 2. 이미지 생성 및 편집을 위한 확산 모델 q(x)를 RD의 데이터 분포라고 합시다. 확산 모델[13–15]은 순방향 확산 과정을 통해 데이터 xo ~ q(x)를 점진적으로 교란합니다.T q(×1:T) = q(x0) II 9(xt|xt−1), q(xt|xt−1) = N(×t; √αtxt−1, ßtĪ), t=(1) 여기서 ẞt는 노이즈 스케줄, at = 1 − ẞt이며 xT ~N(0, 1)을 만족하도록 설계되었습니다. 순방향 과정 {xt}te[0,T]는 다음과 같은 전이 분포를 갖습니다.It|0(xt|xo) = N(xt|√āt×0, (1 – āt)I), 여기서 at = 1 as. 데이터는 역 확산 프로세스를 통해 xτ ~ N(0, 1)에서 시작하여 생성될 수 있으며, 이때 역 전이 커널 q(xt−1 xt)는 가우시안 모델에 의해 학습됩니다: Po(xt-1xt) N(x+ 1;t−1; µØ(×t),σI). Ho et al. [15]은 평균 μ(x+)를 학습하여 평균 제곱 오차 손실을 통해 노이즈 예측 네트워크 €0(x, t)를 학습할 수 있음을 보여줍니다: = min Et,xo,€€€(xt, t)||², Ꮎ (3) 여기서 xt~ Ito (xt |xo), ~N(0, 1). 결정론적 DDIM 샘플링[16]은 다음 반복 규칙을 통해 xT ~N(0, 1)에서 시작하는 샘플을 생성합니다.xt xt-1 = at-√1 – α₁€ (x, t) √at + 1 — αt−1€ (x, t).xt-(4) 고품질 샘플을 생성하는 기능 덕분에 확산 모델은 자연스럽게 이미지 변환 및 이미지 편집에 적용됩니다[7, 17, 18].무조건 생성과 달리 일반적으로 소스 이미지 xo의 내용을 보존해야 합니다.ODE의 가역적 속성을 고려하여 DDIM 역산[16]을 채택하여 위의 프로세스를 반대로 수행하여 충실한 이미지 편집을 위해 실제 이미지 xo를 관련 역산 노이즈 xм로 변환합니다.-xt-1 - √1 — αt-1 €0 (x−1, t − 1) xt = + √√1 − α+€¤ (xt−1, t − 1). (5) αt-2.2 잠재 확산 모델 및 ControlNet = 계산 비용을 줄이기 위해 잠재 확산 모델(LDM, 안정 확산이라고도 함) [5]은 인코더 &amp;를 사용하여 xo를 저차원 잠재 공간 zo E(xo)로 변환합니다.이는 디코더 xo ≈ D(zo)로 재구성할 수 있으며, 그런 다음 잠재 공간에서 노이즈 예측 네트워크 ε0(zt, p, t)를 학습합니다.여기서 p는 텍스트 프롬프트입니다.ε(zt, p, t)의 백본은 여러 기본 블록을 쌓는 UNet(주 UNet이라고 함)입니다.특히 U-Net은 인코더, 중간 블록, 디코더로 구성됩니다.인코더와 디코더는 각각 12개의 블록으로 구성되는 반면 전체 모델은 총 25개의 블록을 포함합니다.이러한 블록 내에서 8개는 다운샘플링 또는 업샘플링 합성곱 계층에 사용되고 나머지 블록은 기본 구성 블록을 구성합니다. 각 기본 블록은 변환기 블록과 잔여 블록으로 구성됩니다.변환기 블록은 셀프 어텐션 계층, 교차 어텐션 계층 및 피드포워드 신경망을 통합합니다.CLIP 텍스트 인코더에서 처리한 텍스트 임베딩은 교차 어텐션 계층을 통해 U-Net에 통합됩니다.모델이 추가 조건 c를 학습할 수 있도록 ControlNet[10]은 인코더의 학습 가능한 사본과 메인 UNet의 중간 블록(ControlNet이라고 함)을 추가하여 잠긴 안정적 확산에 작업별 조건을 통합합니다.그런 다음 ControlNet의 출력 뒤에 0-초기화 합성곱 계층이 나오고, 이는 이후 해당 계층에서 메인 U-Net의 기능에 추가됩니다.3가지 방법 1절에서 언급한 과제를 해결하기 위해 먼저 사전 학습된 T2I 확산 모델을 기반으로 하는 충실하고 시간적으로 일관된 텍스트 기반 비디오 편집을 위한 Control Video를 제시합니다(3.1절 참조). 그런 다음 두 번째 과제인 수백 개의 프레임을 포함하는 긴 비디오의 비디오 편집을 위해 Control Video를 확장합니다(3.2절 참조).3.Control Video 이 섹션에서는 먼저 디자인 공간에 대한 심층적인 탐색을 통해 ControlVideo의 아키텍처를 소개합니다(3.1.1절 참조).그림 2에서 볼 수 있듯이 ControlVideo는 추가 조건, 키 프레임 미세 조정 및 시간적 주의를 통합합니다.3.1.2절에서는 ControlVideo의 학습 및 샘플링 프레임워크를 제시합니다.또한 3.1.3절에서 저랭크 적응 계층을 통합하여 ControlVideo가 선택적 참조 이미지에 맞춰 비디오를 생성하는 방법을 보여줍니다.3.1.아키텍처 T2I 확산 모델은 대규모 텍스트-이미지 데이터에서 사전 학습되었으므로 주어진 텍스트에 맞춰 이를 구축합니다. 이전 연구[1, 3]에 따라 먼저 2D 합성곱 계층의 공간 커널(3 × 3)을 3D 커널(1 × 3 × 3)로 대체하여 비디오 입력을 처리합니다. = 시각적 컨트롤 추가. 텍스트 기반 비디오 편집의 핵심 목표는 소스 비디오의 콘텐츠를 충실하게 보존하는 것입니다. 직관적인 접근 방식은 Xo의 정보를 활용하기 위해 방정식 5의 DDIM 역전 Xм에서 시작하여 편집된 비디오를 생성하는 것입니다. 그러나 그림 3에 나와 있는 것처럼 ODE의 가역적 특성에도 불구하고 경험적으로 DDIM 역전과 DDIM 샘플링을 결합하면 소스 비디오의 구조가 크게 붕괴됩니다. 충실도를 높이기 위해 모든 프레임에 대한 에지 맵과 같은 추가적인 시각적 조건 C{1을 주 UNet에 도입하여 초기 시간만이 아니라 각 시간 단계에서 소스 비디오의 안내를 증폭할 것을 제안합니다: (X+, C, p, t). 특히, ControlNet[10]이 Stable Diffusion에서 주 UNet과 함께 사전 학습되었으므로, 우리는 이를 사용하여 이러한 시각적 조건 C를 처리합니다. 형식적으로, h₂ ЄRNxd 및 he ЄRNxd가 각각 주 UNet 및 ControlNet의 동일한 계층의 차원 d를 갖는 숨겨진 특징을 나타낸다고 합시다. 우리는 이러한 특징을 합산하여 결합하여 h huhe를 얻고, 이는 i=1&#39;= A가 제어 스케일 역할을 하는 스킵 연결을 통해 주 UNet의 디코더에 입력됩니다. 그림 3에서 볼 수 있듯이, Xo로부터 구조적 안내를 제공하기 위해 시각적 조건을 도입하면 편집된 비디오의 충실도가 크게 향상됩니다. 또한, 서로 다른 시각적 조건이 Xo에서 파생된 다양한 정도의 정보를 포함한다는 점을 감안할 때, 우리는 서로 다른 조건을 사용하는 이점을 종합적으로 조사합니다. 그림 1에서 볼 수 있듯이, 우리의 연구 결과에 따르면 에지 맵과 같이 Xo에 대한 자세한 통찰력을 제공하는 조건은 얼굴 비디오 편집과 같은 속성 조작에 특히 유리하며, 인간의 정체성을 보존하기 위해 정확한 제어가 필요합니다. 반대로, Xo에 대한 더 거친 통찰력을 제공하는 조건, (a) 인코더 중간 디코더 블록 블록 블록 소스 비디오 컨트롤 (b) 소스 프롬프트 &quot;자동차&quot; 겹치는 짧은 비디오 NSV를 사용한 융합된 특징 KFV를 사용한 융합된 특징 훈련 기본 블록 의사 3D ResNet 예측된 노이즈 키 프레임 어텐션 시간적 어텐션 제로 합성 소스 비디오 교차 어텐션 피드포워드 신경망 시간적 어텐션이 있는 편집된 비디오 키 프레임 어텐션 시간적 어텐션 키, 값 없음 쿼리 시간적 어텐션 [T 프레임] 제어 비디오 퓨전과 이웃하는 짧은 비디오 추론 가우시안 노이즈/ 노이즈가 있는 소스 비디오/ DDIM 반전 제어 비디오 초기 값 DDIM 샘플링 대상 프롬프트 &quot;자동차, 가을&quot; 각 비디오의 첫 번째 프레임 키 프레임 비디오 제어 비디오 퓨전과 키 프레임 비디오 융합 제어 비디오 그림 2: (a) ControlVideo의 개요. 왼쪽: 아키텍처. ControlVideo는 추가 제어를 통합하고, 키 프레임 주의와 시간적 주의를 미세 조정합니다. 주의 모듈은 T2I 확산 모델의 자기 주의 가중치를 사용하여 초기화됩니다. 오른쪽: 추론 프레임워크. 편집 시나리오에 따라 초기 값을 도출하는 세 가지 방법이 있습니다(섹션 3.1.2 참조). (b) 긴 비디오 편집을 위한 확장된 Control Video 개요. NSV와 KFV는 각각 인접한 짧은 비디오와 키 프레임 비디오를 나타냅니다. 포즈 정보와 같은 정보는 모양과 배경에 대한 유연한 조정을 용이하게 합니다. 이 탐색은 자연스럽게 여러 제어를 결합하여 각각의 장점을 활용할 수 있는지에 대한 의문을 제기합니다. 이를 위해 h = huhe로 표시되는 다양한 제어에서 파생된 숨겨진 기능의 가중 합을 계산한 다음 융합된 기능을 주 UNet의 디코더에 입력합니다. 여기서 λ는 i번째 제어와 관련된 제어 스케일을 나타냅니다. 여러 컨트롤이 충돌이나 불일치를 보일 수 있는 상황에서는 SAM[19] 또는 교차 주의 맵[7]을 사용하여 텍스트를 기반으로 마스크를 생성하고 마스크된 컨트롤을 Control Video에 공급하여 컨트롤 시너지를 향상시킬 수 있습니다.그림 1에서 볼 수 있듯이 Canny 에지 맵은 모양 수정에 미치는 영향이 제한적이면서 배경을 보존하는 데 탁월합니다.반대로 포즈 컨트롤은 유연한 모양 조정을 용이하게 하지만 다른 중요한 세부 사항을 간과할 수 있습니다.이러한 컨트롤을 결합하면 배경을 보존하고 모양 수정에 동시에 효과를 줄 수 있어 복잡한 비디오 편집 시나리오에서 여러 컨트롤을 활용하는 것이 가능함을 보여줍니다.키 프레임 주의.T2I 확산 모델은 각 프레임의 기능을 독립적으로 업데이트하고 프레임 간에 상호 작용이 없으므로 시간적 불일치가 발생합니다.이 문제를 해결하고 시간적 일관성을 개선하기 위해 비디오 전체에 걸쳐 정보를 전파하기 위한 참조 역할을 하는 키 프레임을 도입합니다. 구체적으로, 이전 연구[3]에서 영감을 얻어 기본 UNet과 ControlNet 모두에서 공간적 자기 주의력을 키 프레임 주의력으로 변환하여 모든 프레임을 선택된 참조 프레임에 맞춥니다. 형식적으로, v² = Rd가 i번째 프레임의 숨겨진 특징을 나타내고, k = [1, N]이 선택된 키 프레임을 나타낸다고 합니다. 키 프레임 주의 메커니즘은 다음과 같이 정의됩니다. Q = WQv², K = WKyk, ‚V = WV vk, 여기서 WQ, WK, WV는 투영 행렬입니다. T2I 확산 모델의 기능을 최대한 활용하기 위해 원래의 자기 주의력 가중치를 사용하여 이러한 행렬을 초기화합니다. 경험적으로, 우리는(a) [100개 프레임] 소스 비디오 가중치 겹치는 길이 L &quot;자동차&quot; &quot;자동차, 빈센트 반 고흐 스타일&quot; 소스 비디오 짧은 비디오를 독립적으로 편집 이웃 짧은 비디오와의 융합 [100개 프레임] +컨트롤 이웃 짧은 비디오 및 키 프레임 비디오와의 융합 (b) 소스 DDIM +Ker-프레임 +시간적 비디오 반전 &quot;소녀&quot; &quot;풍부한 화장을 한 소녀&quot; 전체 버전 At. At. 소스 비디오 DDIM 반전 +Ker-프레임 +시간적 +컨트롤 전체 버전 At. At. &quot;자동차&quot; → &quot;빨간 자동차&quot; &quot;사람이 춤추고 있다&quot; &quot;판다가 춤추고 있다&quot; 그림 3: (a) 긴 비디오 편집을 위한 키 프레임 비디오 융합을 위한 융합 전략, 겹치는 길이 a 및 가중치 w에 대한 소거 연구. 3.2절 및 5.2.3절에서 자세한 분석을 참조하세요. (b) 제어 비디오의 주요 구성 요소에 대한 소거 연구. At. 주의를 나타냅니다. 3.2절에서 자세한 분석을 참조하세요. 5.2.3. 자기 주의와 미세 조정된 매개변수에서 키 프레임, 키 및 값 선택의 설계를 체계적으로 연구합니다. 자세한 분석은 부록에 제공됩니다. 요약하면, 주의 메커니즘에서 키와 값으로 사용되는 첫 번째 프레임을 키 프레임으로 활용하고, 주의 모듈 내에서 출력 투영 행렬 W°를 미세 조정하여 시간적 일관성을 향상시킵니다. 시간적 주의. 편집된 비디오의 충실도와 시간적 일관성을 모두 향상시키기 위해, 우리는 모든 프레임에서 해당 공간 위치 간의 관계를 포착하는 네트워크의 추가 분기로 시간적 주의 모듈을 도입합니다. 공식적으로, 숨겨진 특징을 v = RN×d라고 하면 시간적 주의는 다음과 같이 정의됩니다. Q = WQv, K = WK v,V = WVv. 이전 연구[20]는 시간적 주의를 훈련하기 위해 광범위한 데이터로부터 이점을 얻었는데, 이는 우리의 원샷 설정에서는 없는 사치입니다. 이 과제를 해결하기 위해, 우리는 서로 다른 주의 메커니즘이 이미지 특징 간의 관계를 모델링하는 일관된 방식에서 영감을 얻습니다. 따라서 원래의 공간적 자기 주의 가중치를 사용하여 시간적 주의를 초기화하고 T2I 확산 모델의 기능을 활용합니다. 각 시간적 주의 모듈 후에 미세 조정 전에 모듈의 출력을 사전 유지하기 위해 제로 합성 계층[10]을 통합합니다. 나아가 시간적 주의를 도입하기 위해 로컬 및 글로벌 위치를 통합하는 것에 대한 포괄적인 연구를 수행합니다. 정성적 결과는 그림 4에 나와 있습니다. 변환기 블록의 로컬 위치와 관련하여 가장 효과적인 배치는 자기 주의 메커니즘의 앞과 내부에 있는 것입니다. 이 선택은 이 두 위치의 입력이 자기 주의의 입력과 일치하여 시간적 주의의 초기 가중치 역할을 한다는 사실에 의해 입증됩니다. 자기 주의 위치는 더 높은 텍스트 정렬을 보여 궁극적으로 선호하는 선택이 됩니다. ControlVideo의 글로벌 위치에 대해 우리의 주요 발견은 위치의 효과성은 캡슐화하는 정보의 양과 상관 관계가 있다는 것입니다. 예를 들어, 이미지 생성을 담당하는 주요 UNet은 정보의 전체 스펙트럼을 유지하여 다른 기능은 무시하고 조건 관련 기능만 추출하는 데 집중하는 ControlNet보다 성능이 뛰어납니다. 결과적으로 중간 블록을 제외한 주요 UNet의 모든 단계에서 자기 주의와 함께 시간적 주의를 통합합니다. 더 자세한 분석은 부록 3.1.2에 나와 있습니다. 학습 및 샘플링 프레임워크 C = {c²}₁은 X에 대한 시각적 조건(예: Canny 에지 맵)을 나타내고 ε(Xt,C,p,t)는 제어 비디오 네트워크를 나타냅니다. p 및 pt는 각각 소스 프롬프트와 대상 프롬프트를 나타냅니다. 방정식과 유사합니다. 3, 우리는 다음과 같이 정의된 평균 제곱 오차 손실을 사용하여 소스 비디오-텍스트 쌍(Xo, ps)에서 ε0(Xt, C, p, t)를 미세 조정합니다. ~ min Et,e||€ – €(Xt, C, ps, t)||², Ꮎ여기서 € ~ N(0, I), Xt Ito(Xt Xo). 훈련하는 동안 우리는 다른 모든 매개변수를 고정한 채로 주의 모듈 내의 매개변수만을 독점적으로 최적화합니다(3.1.1절에서 논의한 대로). 초기 값 XM 선택. ε(Xt, C, p, t)를 기반으로, 대상 프롬프트 pt를 기반으로 DDIM 샘플링[16]을 사용하여 초기 값 XM에서 시작하여 편집된 비디오를 생성할 수 있습니다. Xм의 경우, 속성 조작과 같은 로컬 편집 작업을 위해 Eq. 5에 설명된 대로 DDIM 역전을 사용합니다. 전역 편집의 경우, 이전 작업[1, 3]과 달리, 큰 M과 짝수 XM N(0, 1)을 사용하여 Eq. 2에서 전방 전환 분포를 사용하여 노이즈가 있는 소스 비디오 XM ~ 9M|0(XM|X0)에서 시작하여 시각적 조건이 이미 Xo로부터 구조 지침을 제공했기 때문에 편집 가능성을 개선할 수도 있습니다. 이 프로세스 동안 샘플링된 노이즈는 시간적 일관성을 위해 모든 프레임에서 공유됩니다. 알고리즘 1 긴 비디오 편집을 위한 확장된 ControlVideo ~ 요구 사항: 초기 값 XM, 컨트롤 C, 짧은 비디오 길이 L, 중첩 길이 a, 융합 함수 F(), 가중치 w, 모델 € (·, ·, ·, ·), 프롬프트 pn = [N/(L- a)] + tM에서 1까지 do j = 1 톤까지 do &lt; € (X, C³,p,t) E까지 종료 € KF(E,..., €) € (XK, CK,p,t) €0← wo() + (1 – w)êo Xt-1DDIM_Sampling(€0, Xt, t) Xo로 돌아가기 종료 3.1.3 이미지 기반 비디오 편집 ▶ 짧은 비디오 수 ► 각 짧은 비디오에 대한 ControlVideo는 Eq.를 통해 인접한 짧은 비디오와 융합합니다.► 키 프레임 비디오에 대한 ControlVideo ▶ Eq.를 통해 키 프레임 비디오와 융합합니다.▶ Eq.의 노이즈 제거 단계 특정 시나리오에서 텍스트 설명이 누락될 수 있습니다. 사용자의 원하는 정확한 효과를 완전히 전달하지 못하는 경우. 이러한 경우 사용자는 생성된 비디오가 주어진 참조 이미지와도 일치하기를 원할 수 있습니다. 여기서는 이미지 기반 비디오 편집을 위해 ControlVideo를 확장하는 간단한 방법을 보여줍니다. 구체적으로, 먼저 주 UNet에 Low-rank adaptation(LoRA)[11] 계층을 추가하여 참조 이미지와 관련된 개념의 학습을 용이하게 한 다음 Sec. 3.1.2에 따라 Control Video를 학습하기 위해 이를 동결할 수 있습니다. 참조 이미지와 비디오에 대한 학습은 독립적이므로 CivitAI와 같은 커뮤니티의 모델을 유연하게 활용할 수 있습니다. 3.2 긴 비디오 편집을 위한 확장된 ControlVideo 위 섹션에서 설명한 ControlVideo는 시간적으로 일관된 비디오를 생성하는 매력적인 기능이 있지만 메모리 제한으로 인해 일반적으로 수백 개의 프레임을 포함하는 실제 비디오를 처리하는 것은 여전히 어렵습니다. 이 문제를 해결하는 간단한 방법은 전체 비디오를 여러 개의 겹치지 않는 짧은 세그먼트로 나누고 각 세그먼트에 Control Video를 독립적으로 적용하는 것입니다. 그러나 그림 3에서 볼 수 있듯이 이 방법은 여전히 비디오 클립 간에 시간적 불일치가 발생합니다. 이 문제를 해결하기 위해 우리는 각 노이즈 제거 단계에서 짧은 비디오 사이를 연결하는 프레임의 특징을 융합하는 것을 제안합니다. 이를 달성하기 위해 그림 2에서 보듯이 전체 비디오를 겹치는 짧은 비디오로 분할하고 각 세그먼트에 제어 비디오를 적용한 다음 미리 정의된 가중치 함수를 통해 인접한 짧은 비디오에서 겹치는 프레임의 특징을 병합합니다. 여기서 가중치 융합 전략은 이미지 생성 작업에서도 사용됩니다[21]. 또한, 이후의 노이즈 제거 단계에서는 겹치지 않는 것과 겹치는 것(a) 다른 초기화와의 비교 (b) 변압기 블록에서 시간적 주의의 다른 로컬 위치와의 비교 소스 비디오 사전 학습 가중치를 사용한 랜덤 초기화 소스 비디오 자기 주의 이전 자기 주의 이후 자기 주의 이후 교차 주의 이후 FNN &quot;아름다운 풍경이 있는 여성의 뒷모습&quot;&quot;, 별이 빛나는 하늘&quot; &quot;아름다운 풍경이 있는 여성의 뒷모습&quot; &quot;..., 일출, 이른 아침&quot; (c) 시간적 주의의 다른 글로벌 위치와의 비교 ControlNet + 소스 비디오 UNet ControlNet UNet 인코더 UNet 디코더 UNet 블록 1,2, UNet 블록 1, UNet 블록 2, UNet HHHHHHH &quot;사람이 춤추고 있다&quot; &quot;판다가 춤추고 있다&quot; 그림 4: (a) 초기화 방법과 시간적 주의를 도입하기 위한 (b) 로컬 위치 및 (c) 글로벌 위치의 통합에 대한 절제 연구. 녹색은 우리의 선택을 표시했습니다. 짧은 비디오 클립 내의 프레임은 Control Video에 함께 공급되어 겹치지 않는 프레임의 특징이 겹치는 프레임의 특징에 더 가까워지므로 간접적으로 전역 시간적 일관성이 향상됩니다.형식적으로 j번째 짧은 비디오 클립 X와 해당 시각적 조건 C³는 다음과 같이 정의됩니다. &quot; C³ = {c}=(-1)(La)+min((j−1)(La) +L,N) &quot; j Є [1, n] X² = {x}i=(j−1)(La)+imin((j−1)(La) +L,N) (6) 여기서 n = [N/(L − a)] + 1은 짧은 비디오 클립의 수이고, L은 짧은 비디오 클립의 길이이며, a는 겹친 길이입니다. &amp; € RL×D = €0 (X²², C&#39;³‚p, t)는 j번째 짧은 비디오의 ControlVideo를 나타내고 ê0 € RND는 전체 비디오의 융합된 ControlVideo를 나타냅니다. 융합 함수 F(): RnxLxD →RNXD는 다음과 같이 정의됩니다. = F(ε √ √, ..., ε ) = Sum (Normalize(O(w; &amp; 1D)) O(€²)), (7) 여기서 w; ER은 j번째 짧은 비디오의 가중치 벡터이고, 1ɲ Є R³는 1의 벡터이고, ◇는 벡터 외적이며, 는 요소별 곱셈이고 Sum(•)는 행렬의 해당 위치에 요소를 추가합니다. O(.) : RLXD → RND는 0 패딩을 나타냅니다. 예를 들어, O(€)는 j번째 비디오의 해당 프레임 인덱스가 6이고 다른 프레임 인덱스는 0임을 나타냅니다. Normalize()는 해당 위치에서 합으로 행렬 요소를 조정하여 융합 가중치의 합이 1이 되도록 하고 융합 후 값 범위를 유지합니다. 이 작업에서 우리는 정규 난수 변수 w; ~ N(l; L/2,σ²)를 정의합니다. 여기서 σ = 0.1입니다. 대체 가중치 함수가 테스트되었고, 그 결과는 함수 선택에 대한 무감각함을 나타냈습니다(5.2.4절 참조). 그림 3에서 볼 수 있듯이, 이 융합 전략은 짧은 비디오 간의 시간적 일관성을 크게 향상시킵니다. 그러나 이 접근 방식은 인접한 비디오를 직접 융합하여 인접한 비디오 클립 간의 로컬 일관성을 보장하고, 전체 비디오의 글로벌 일관성은 반복적인 노이즈 제거 단계 동안 간접적으로 향상됩니다. 결과적으로 그림 3에서 볼 수 있듯이 비디오 클립이 더 멀리 떨어져 있을 때 시간적 일관성이 저하되며, 이는 검은색 차가 녹색 차로 저하되는 것으로 예시됩니다. 이러한 관찰에 비추어 볼 때 자연스럽게 다음과 같은 의문이 생깁니다. 더 많은 글로벌 기능을 직접 융합하여 장거리 시간적 일관성을 더욱 향상시킬 수 있을까요? 이를 달성하기 위해 각 짧은 비디오 세그먼트의 첫 번째 프레임을 통합하여 키 프레임 비디오를 만들어 글로벌 안내를 직접 제공합니다. ControlVideo는 그런 다음 0.0.Weight 0.0.1.1과 융합되는 이 키 프레임 비디오에 적용됩니다.(a) 다양한 가중치 함수의 시각화 (b) 다양한 가중치 함수의 결과 역제곱 가우시안 소스 가우시안 루트(볼록) 가우시안 선형 상수 코사인 exp 볼록 inverse_square_root 볼록 가우시안l 프레임=프레임=프레임=프레임=코사인 지수 선형 상수 그림 5: (a) 다양한 가중치 함수의 시각화. 여기서는 L = 25를 예로 들어 설명합니다. (b) 다양한 가중치 함수로 편집한 결과. = 이전에 얻은 ĉe. 공식적으로 X : {x(−1)(La)+1}}=1은 키 프레임 비디오를 나타내고 CK = {c(-1)(La)+1}; 21은 해당 시각적 조건을 나타냅니다. 최종 모델 eð는 다음과 같이 정의됩니다. €g = wo(€ỗ) + (1 – w)ĉo, (8) 여기서 w € [0,1]은 가중치, € = €(XK, C, p, t). 여기서 키 프레임 비디오의 프레임은 키 프레임 어텐션의 각 짧은 비디오에서 키 프레임으로 선택되어 전역 시간 일관성이 보장됩니다. 전체 알고리즘은 알고리즘 1에 나와 있습니다. 그림 3에서 볼 수 있듯이 키 프레임 비디오 퓨전 전략을 사용하면 자동차의 색상이 전체 비디오에서 일관되게 유지됩니다. 4 관련 작업 4.1 텍스트 기반 생성 및 이미지 편집을 위한 확산 모델 최근 확산 모델은 생성적 인공 지능 분야에서 큰 획기적인 발전을 이루었으며, 따라서 텍스트-이미지 생성에 사용됩니다[5, 22]. 이러한 모델은 일반적으로 대규모 이미지-텍스트 쌍 데이터 세트에서 텍스트를 조건으로 하는 확산 모델을 학습합니다. T2I 확산 모델의 이러한 놀라운 발전을 바탕으로 수많은 방법이 텍스트 기반 이미지 편집에서 유망한 결과를 보여주었습니다. 특히 Prompt-to-Prompt [7], Plug-andPlay [8], Pix2pix-Zero [9]와 같은 여러 연구에서는 생성된 콘텐츠에 대한 주의 제어를 탐구하고 SOTA 결과를 달성했습니다. 이러한 방법은 일반적으로 DDIM 역전에서 시작하여 생성 프로세스의 주의 맵을 소스 프롬프트의 주의 맵으로 대체하여 소스 이미지의 공간적 레이아웃을 다시 학습합니다. 상당한 발전에도 불구하고 이러한 이미지 편집 방법을 비디오 프레임에 직접 적용하면 시간적 깜빡임이 발생합니다. 4.2 텍스트 기반 비디오 편집을 위한 확산 모델 Gen-1 [23]은 대규모 데이터 세트에서 비디오 확산 모델을 학습하여 인상적인 성능을 달성했습니다. 그러나 값비싼 계산 리소스가 필요합니다. 이를 극복하기 위해 최근 연구에서는 단일 텍스트-비디오 쌍에서 T2I 확산 모델을 기반으로 합니다. 특히, Tune-A-Video[3]는 T2I 확산 모델을 T2V 확산 모델로 팽창시키고 소스 비디오-텍스트 데이터에서 미세 조정합니다.여기에서 영감을 얻은 여러 작업[1, 2, 4]은 이를 어텐션 맵 주입 방법과 결합하여 우수한 성능을 달성했습니다.진보에도 불구하고 경험적 증거는 시간적 일관성을 유지하면서 출력을 충실하고 적절하게 제어하는 데 여전히 어려움을 겪고 있음을 시사합니다.소스 비디오 &quot;스와로브스키 크리스털 백조가 강에서 수영하고 있습니다&quot; &quot;빨간 머리 소녀&quot; &quot;남자가 공원에서 반 고흐 스타일로 춤을 추고 있습니다&quot; 우리의 안정적 확산 Tune-AVideo Vid2vidzero VideoP2P Fate Zero 그림 6: DAVIS의 기준선과 웹사이트에서 수집한 데이터와의 비교.제어 비디오는 세 가지 요구 사항을 동시에 충족시켜 더 나은 시각적 품질을 달성합니다.DDIM 반전이 아닌 가우시안 노이즈에서 시작하여 글로벌 편집에서 편집 가능성을 개선할 수 있습니다(세 번째 예 참조). 5 실험 5.1 설정 5.1.1 구현 세부 정보 짧은 비디오 편집의 경우 이전 연구 [2]에 따라 공정한 비교를 위해 512 × 512 해상도의 8개 프레임을 사용합니다. DAVIS 데이터 세트 [24]와 웹사이트²에서 50개의 비디오-텍스트 쌍 데이터를 수집합니다. 제어 비디오를 안정적 확산과 다음 SOTA 텍스트 기반 비디오 편집 방법인 Tune-A-Video [3], Vid2vid-zero [9], Video-P2P [4] 및 FateZero [1]과 비교합니다. 기본적으로 제어 비디오를 각각 캐니 에지 맵, HED 경계, 깊이 맵 및 포즈에 대해 80, 300, 500 및 1500번의 반복으로 학습 속도 3 × 10−5로 학습합니다. 제어 스케일 A는 1로 설정됩니다. 여러 제어의 경우 기본적으로 λi를 0.5로 설정합니다. 50단계와 12개의 분류기 없는 안내를 갖춘 DDIM 샘플러[16]가 추론에 사용됩니다. 캐니 에지 맵, HED 경계, 깊이 맵 및 포즈가 있는 Stable Diffusion 1.5[5] 및 ControlNet 1.0[10]이 실험에 채택되었습니다. 이미지 기반 비디오 편집의 경우 Civitai의 Lora 가중치를 사용하여 Stable Diffusion에 병합합니다. = 5.1.2 평가 이전 연구[1]에 따라 시간적 일관성을 위해 CLIP-temp를 보고하고 텍스트 정렬을 위해 CLIP-text를 보고합니다. 또한 충실도를 위해 입력-출력 쌍 사이의 편집되지 않은 영역 내에서 SSIM[25]을 보고합니다. 충실도에 대한 메트릭은 편집되지 않은 영역만 고려합니다. 편집되지 않은 영역은 텍스트에 따라 SAM[19]에서 계산합니다. 또한 기준선과 ControlVideo 간의 쌍별 비교를 통해 텍스트 정렬, 시간적 일관성, 충실도 및 전반적인 모든 측면을 정량화하기 위해 사용자 연구를 수행합니다. 이 섹션에는 총 10명의 피험자가 참여했습니다. 충실도를 예로 들면, 소스 비디오가 주어졌을 때, 참가자들은 기준선과 제어 비디오 간의 쌍대 비교에서 어느 편집된 비디오가 소스 비디오에 더 충실한지 선택하도록 지시받습니다. 2https://www.pexels.com0.안정적인 확산 안정적 44% 56% 확산 안정적 확산 18% 82% ● Tune-A-Video • Vid2vid-zero Tune-A36% 64% 비디오 Tune-AVideo 0.24% 76% 비디오-P2P 4% 96% 우리 비디오-P2P 36% 64% 우리 Vid2vid 38% 62% Vid2vid 20% CLIP-text ↑ 0.● Fatezero ● 비디오-P2P 우리 80% 0.Fatezero 32% 68% Fatezero 30% 70% 0.0% 25% 50% 75% 100% 0% 25% 50% 75% 100% 0.50 0.0.0.0.90 1. 텍스트 정렬 시간적 일관성 SSIM ↑ 안정적 확산 Tune-AVideo 12% 88% 비디오-P2P 38% 62% Vid2vid 20% 92% 안정적 확산 Tune-AVideo 10% 90% 0.14% 86% 0. 우리의 비디오-P2P 18% 82% 우리의 80% Vid2vid 20% 80% CLIP-temp ↑ 0.0. 안정적 확산 Tune-A-Video ● Vid2vid-zero • Fatezero ● 비디오-P2P Fatezero 22% 78% Fatezero 26% 74% 0% 25% 50% 75% 100% 0% 25% 충실도 50% 전체 75% 100% 0.0.50 0.(a) 사용자 선호도 • 당사 0.70 0.80 0.90 1.SSIM ↑ (b) 객관적 지표 그림 7: 사용자 연구 및 객관적 지표에 따른 정량적 결과.ControlVideo는 전반적인 측면에서 모든 기준선보다 우수한 성과를 보입니다.5.2.2절에서 자세한 분석을 참조하세요.5.2 결과 5.2. 응용 프로그램 주요 결과는 그림 1에 나와 있습니다.첫째, 다양한 단일 컨트롤의 지침에 따라 ControlVideo는 속성, 스타일 및 배경 편집에서 시각적 사실성이 높은 비디오를 제공합니다.예를 들어, HED 경계 컨트롤은 백조를 스와로브스키 크리스털 백조로 충실하게 변경하는 데 도움이 됩니다.포즈 컨트롤은 남자를 검은 코트를 입은 셜록 홈즈로 변경하여 모양을 유연하게 수정할 수 있습니다.둘째, &quot;사람&quot; → &quot;판다&quot;의 경우 ControlVideo는 여러 컨트롤(캐니 에지 맵 및 포즈 컨트롤)을 결합하여 배경을 보존하고 모양을 동시에 변경하여 다양한 컨트롤 유형의 이점을 활용할 수 있습니다. 또한 이미지 기반 비디오 편집에서 Control Video는 소스 비디오의 여성을 Evangeline Lilly로 성공적으로 변경하여 참조 이미지를 정렬합니다. 마지막으로 수백 개의 프레임에서 여성의 정체성을 보존하여 ControlVideo가 장거리 시간적 일관성을 유지할 수 있음을 보여줍니다. 5.2.2 비교 정량적 및 정성적 결과는 각각 그림 7과 그림 6에 나와 있습니다. 텍스트 기반 비디오 편집은 세 가지 요구 사항을 동시에 충족해야 하며 단일 객관적 지표로는 편집된 결과를 반영할 수 없음을 강조합니다. 예를 들어 SSIM이 높은 Video-P2P는 소스 비디오를 재구성하는 경향이 있고 텍스트를 정렬하지 못합니다. 그림 6에서 볼 수 있듯이 &quot;빨간 머리 소녀&quot; 예에서 머리 색깔을 변경할 수 없습니다. CLIP-text가 높은 Stable Diffusion과 Vid2vid-zero는 눈에 띄는 빨간 머리 소녀를 생성하지만 소스 비디오의 여성의 정체성을 완전히 무시하여 만족스럽지 못한 결과를 초래합니다. 그림 7(a)에 표시된 대로 사용자 연구를 통해 수행한 전반적인 측면에서 우리 방법은 모든 기준선을 크게 능가합니다. 구체적으로, 86%의 사람들이 Tune-A-Video보다 편집된 비디오를 선호합니다. 게다가, 인간 평가는 비디오 편집 작업에 가장 합리적인 정량적 지표이며, Control Video가 모든 측면에서 모든 기준선을 능가하는 것을 관찰할 수 있습니다. 그림 6의 정성적 결과는 정량적 결과와 일치하며, ControlVideo는 머리 색깔을 성공적으로 변경할 뿐만 아니라 모든 기존 방법이 실패하는 반면 여성의 정체성을 그대로 유지합니다. 전반적으로 광범위한 결과는 Control Video가 텍스트 프롬프트와 일치하면서도 시간적으로 일관되고 충실한 비디오를 제공함으로써 모든 기준선을 능가한다는 것을 보여줍니다. 5.2. Control Video의 주요 구성 요소에 대한 절제 연구 그림 3에 표시된 대로 컨트롤을 추가하면 소스 비디오에서 추가 안내가 제공되어 충실도가 크게 향상됩니다. 주요 프레임 주의는 시간적 일관성을 크게 향상시킵니다. 시간적 주의는 충실도와 시간적 일관성을 향상시킵니다. 모든 모듈을 결합하면 최상의 성능을 얻을 수 있습니다. 부록의 정량적 결과는 정성적 결과와 일치한다.5.2.4 긴 비디오 편집의 하이퍼 매개변수에 대한 절제 연구 이 섹션에서는 키 프레임 비디오 융합을 위한 겹치는 길이 a, 가중치 w, 확장된 제어 비디오에서 근처 비디오와의 융합을 위한 가중치 함수에 대한 절제 연구를 수행합니다.그림 3에서 볼 수 있듯이 겹치는 길이 a가 늘어나면 시간적 일관성이 향상된 비디오가 생성됩니다.이 연구에서는 a를 [½, L]로 설정했습니다.더 큰 w는 전체 비디오에서 확장된 시간 시퀀스에 대한 일관성을 촉진합니다.하지만 w가 너무 크면 시간적 깜빡임이 발생할 수 있습니다.이 작업에서는 w를 [0.2, 0.5]로 설정했습니다.또한 근처 비디오와의 융합을 위한 다양한 가중치 함수를 고안했습니다.융합이 비디오의 양쪽 끝에서 발생한다는 점을 감안할 때 L/2에 대해 대칭이고 모든 요소가 0보다 큰 함수를 만드는 것을 선호합니다. 그림 5에 나와 있듯이, 우리는 상수, 선형, 오목(예: 코사인), 볼록(예: 역제곱근) 함수를 포함한 여러 함수 형태를 탐구합니다. 그림 5에 제시된 결과는 편집된 비디오의 품질이 가중치 함수 선택에 크게 영향을 받지 않는다는 것을 나타냅니다. 6
--- CONCLUSION ---
이 논문에서는 충실도와 시간적 일관성을 개선하기 위해 에지 맵, 키 프레임 및 시간적 주의와 같은 추가 조건을 통합한 원샷 비디오 편집을 위한 T2I 확산 모델을 활용하는 일반 프레임워크인 ControlVideo를 제시합니다. 최신 텍스트 기반 비디오 편집 방법보다 성능이 우수하여 그 효과를 보여줍니다. 참고문헌 [1] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan 및 Qifeng Chen. Fatezero: 제로샷 텍스트 기반 비디오 편집을 위한 주의 융합. arXiv 사전 인쇄본 arXiv:2303.09535, 2023. [2] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang 및 Chunhua Shen. 기성품 이미지 확산 모델을 사용한 제로샷 비디오 편집. arXiv 사전 인쇄 arXiv:2303.17599, 2023. [3] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie 및 Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 조정입니다. arXiv 사전 인쇄 arXiv:2212.11565, 2022. [4] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin 및 Jiaya Jia. Video-p2p: 교차 주의 제어를 통한 비디오 편집. arXiv 사전 인쇄 arXiv:2303.04761, 2023. [5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. [6] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. [7] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 교차 주의 제어를 사용한 프롬프트 간 이미지 편집. International Conference on Learning Representations, 2023. [8] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 텍스트 기반 이미지 대 이미지 변환을 위한 플러그 앤 플레이 확산 기능. arXiv 사전 인쇄본 arXiv:2211.12572, 2022.[9] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. 제로 샷 이미지 대 이미지 변환. arXiv 사전 인쇄본 arXiv:2302.03027, 2023. [10] Lvmin Zhang 및 Maneesh Agrawala. 텍스트 대 이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023. [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. [12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763페이지. PMLR, 2021. [13] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. International Conference on Learning Representations에서, 2020. [14] Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang. Analytic-dpm: 확산 확률적 모델에서 최적 역분산의 분석적 추정. International Conference on Learning Representations에서, 2021. [15] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. [16] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암묵적 모델의 잡음 제거. arXiv 사전 인쇄본 arXiv: 2010.02502, 2020. [17] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon. Sdedit: 확률적 미분 방정식을 사용한 이미지 합성 및 편집. 학습 표현 국제 컨퍼런스, 2022. [18] Min Zhao, Fan Bao, Chongxuan Li, Jun Zhu. Egsde: 에너지 유도 확률적 미분 방정식을 통한 비페어 이미지 간 변환. 신경 정보 처리 시스템의 발전, 35:3609-3623, 2022. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 무엇이든 분할하세요. arXiv:2304.02643, 2023. [20] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022. [21] Álvaro Barbero Jiménez. 장면 구성 및 고해상도 이미지 생성을 위한 확산기 혼합. arXiv 사전 인쇄본 arXiv:2302.02412, 2023. [22] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479–36494, 2022. [23] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 안내 비디오 합성. arXiv 사전 인쇄본 arXiv:2302.03011, 2023. [24] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv 사전 인쇄본 arXiv:1704.00675, 2017. [25] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE 이미지 처리 거래, 13(4):600– 612, 2004.
