--- ABSTRACT ---
멀티뷰 스테레오(MVS)의 핵심은 참조 픽셀과 소스 픽셀 간의 매칭 프로세스입니다. 비용 집계는 이 프로세스에서 중요한 역할을 하는 반면, 이전 방법은 CNN을 통해 처리하는 데 중점을 둡니다. 이는 제한된 로컬 수용 필드로 인해 반복적이거나 잘못된 매칭을 구별하지 못하는 CNN의 자연스러운 한계를 물려받을 수 있습니다. 이 문제를 해결하기 위해 Transformer를 비용 집계에 포함시키는 것을 목표로 합니다. 그러나 Transformer로 인해 2차적으로 증가하는 계산 복잡도로 인해 메모리 오버플로 및 추론 지연이 발생하여 또 다른 문제가 발생할 수 있습니다. 이 논문에서는 효율적인 Transformer 기반 비용 집계 네트워크인 CostFormer를 사용하여 이러한 한계를 극복합니다. Residual DepthAware Cost Transformer(RDACT)는 깊이와 공간 차원을 따라 자체 주의 메커니즘을 통해 비용 볼륨에 대한 장거리 기능을 집계하도록 제안됩니다. 또한 Residual Regression Transformer(RRT)는 공간 주의를 향상시키기 위해 제안됩니다. 제안된 방법은 학습 기반 MVS 방법을 개선하기 위한 범용 플러그인입니다. 1
--- INTRODUCTION ---
한 장면의 다양한 시점에서 보정된 일련의 이미지가 주어지면, 멀티뷰 스테레오(MVS)는 관찰된 장면의 3D 정보를 복구하는 것을 목표로 합니다. 이는 컴퓨터 비전의 근본적인 문제이며 로봇 내비게이션, 자율 주행, 증강 현실 등에 널리 적용됩니다. 최근의 학습 기반 MVS 네트워크[Yao et al., 2018; Gu et al., 2020; Wang et al., 2021b]는 3D 재구성의 품질과 효율성 측면에서 모두 고무적인 성공을 거두었습니다. 일반적으로 딥 MVS 접근 방식은 다음의 5단계로 구성됩니다. 공유 가중치가 있는 CNN 네트워크를 통한 멀티뷰 이미지에서 피처 추출, 모든 소스 피처를 참조 시점에 정렬하기 위한 미분 가능한 워핑, 참조 피처와 정렬된 소스 피처에서 일치 비용 계산, 일치 비용 집계 또는 정규화, 깊이 또는 시차 회귀. *이 저자는 이 작업에 동등하게 기여했습니다. *책임 저자. 전체 오류(mm) 0.0.PatchMatchNet MVSNet 0.0.R-MVSNet 0.0.CVP-MVSNet CasMVSNet 0.UCS-Net 당사 0.0.0.0.0.* 0.0.0.0.0.0.1.1.GPU 메모리(GB) 런타임(초) 그림 1: DTU에서 최신 MVS 방법과의 비교. 이미지 크기가 1152x864일 때 오류, GPU 메모리 및 런타임 간의 관계. 학습 기반 MVS의 현재 진행 상황은 주로 재구성 품질[Wei 등, 2021; Yang 등, 2020a], 메모리 소비[Yan 등, 2020; Wei 등, 2021] 및 효율성[Wang 등, 2021b; Wang et al., 2021a]. 이러한 작업의 기본 네트워크 아키텍처는 우아하고 안정적인 기준선을 제공하는 MVSNet[Yao et al., 2018]이라는 선구적인 백본 네트워크를 기반으로 합니다. 그러나 MVSNet[Yao et al., 2018]의 네트워크 설계 원리를 당연하게 받아들이는 대신 MVS 문제의 과제를 밀집 대응 문제[Hosni et al., 2012]로 다시 생각해 볼 수 있습니다. MVS의 핵심은 모든 워핑된 소스 이미지에서 에피폴라 선을 따라 참조 이미지의 특정 픽셀에 해당하는 픽셀을 검색하는 밀집 픽셀별 대응 추정 문제입니다. 이 대응 추정 문제가 어떤 과제에 적용되든 매칭 과제는 고전적인 매칭 파이프라인[Scharstein and Szeliski, 2002]으로 축소될 수 있습니다. (1) 특징 추출, (2) 비용 집계. 학습 기반 MVS 방법에서 기존의 수작업 피처에서 CNN 기반 피처로의 전환은 대규모 데이터에서 학습한 강력한 피처 표현을 제공함으로써 기존 매칭 파이프라인의 이전 단계를 본질적으로 해결합니다. 그러나 사전 지식 없이 피처 간 유사성을 매칭하여 비용 집계 단계를 처리하는 것은 일반적으로 반복적인 패턴이나 배경 클러터로 인해 발생하는 모호성으로 인한 문제에 시달립니다[Cho et al., 2021]. 결과적으로 MVSNet 및 그 변형[Yao et al., 2018; Gu et al., 2020; Wang et al., 2021b]의 일반적인 솔루션은 비용 볼륨의 초기 상관 관계 단서의 품질에 직접 의존하지 않고 3D CNN 또는 RNN을 적용하여 참조 및 소스 뷰 간 비용 볼륨을 정규화하는 것입니다. 이러한 방법은 이전 방법에서 다양하게 공식화되었지만, 심각한 변형에 독립적인 수작업 기술을 사용하거나 CNN의 한계(예: 제한된 수용 필드)를 계승하여 지역적으로 일관된 잘못된 일치를 구별할 수 없습니다.이 연구에서는 비용 볼륨의 비용 집계 단계에 초점을 맞추고 위의 문제를 해결하기 위해 새로운 비용 집계 Transformer(CostFormer)를 제안합니다.우리의 CostFormer는 전역 수용 필드와 장거리 종속 표현으로 유명한 Transformer[Vaswani et al., 2017]를 기반으로 합니다.비용 볼륨에서 일치 비용을 집계함으로써, 우리의 집계 네트워크는 전역 대응 관계를 탐색하고 Transformer의 셀프 어텐션(SA) 메커니즘의 도움으로 모호한 일치 지점을 효과적으로 정제할 수 있습니다.Vision Transformer의 유망한 성능은 많은 응용 프로그램에서 입증되었지만[Dosovitskiy et al., 2020; Sun et al., 2021], 기존 SA에서 키-쿼리 점곱 상호작용의 시간 및 메모리 복잡도는 입력의 공간 해상도에 따라 2차적으로 증가합니다. 따라서 3D CNN을 Transformer로 대체하면 메모리에서 예상치 못한 추가 점유와 추론 지연이 발생할 수 있습니다. [Wang et al., 2021b]에서 영감을 얻어 Transformer 아키텍처를 반복적 다중 스케일 학습 가능 PatchMatch 파이프라인에 추가로 도입합니다. Transformer의 장거리 수용 필드의 장점을 계승하여 재구성 성능을 크게 개선합니다. 동시에 효율성과 성능 간에 균형 잡힌 균형을 유지하여 다른 방법에 비해 추론 속도와 매개변수 크기에서 경쟁력이 있습니다. 우리의 주요 기여는 다음과 같습니다. (1) 이 논문에서는 CostFormer라는 새로운 Transformer 기반 비용 집계 네트워크를 제안합니다. 이는 학습 기반 MVS 방법에 플러그인하여 비용 볼륨을 효과적으로 개선할 수 있습니다. (2) CostFormer는 비용 볼륨에 효율적인 잔여 깊이 인식 비용 변환기를 적용하여 2D 공간 주의를 3D 깊이와 공간 주의로 확장합니다. (3) CostFormer는 비용 집계와 깊이 회귀 사이에 효율적인 잔여 회귀 변환기를 적용하여 공간 주의를 유지합니다. (4) 제안된 CostFormer는 DTU[Aanæs et al., 2016], Tanks &amp; Temples[Knapitsch et al., 2017] ETH3D[Schöps et al., 2017] 및 BlendedMVS[Yao et al., 2020] 데이터 세트를 평가할 때 학습 기반 MVS 방법에 이점을 제공합니다. 2
--- RELATED WORK ---
2.1 학습 기반 MVS
--- METHOD ---
s는 CNN을 통해 처리하는 데 중점을 둡니다. 이는 제한된 로컬 수용 필드로 인해 반복적이거나 잘못된 일치를 구별하지 못하는 CNN의 자연스러운 한계를 상속받을 수 있습니다. 이 문제를 해결하기 위해 Transformer를 비용 집계에 포함시키는 것을 목표로 합니다. 그러나 Transformer로 인해 2차적으로 증가하는 계산 복잡도로 인해 메모리 오버플로 및 추론 지연이 발생하여 또 다른 문제가 발생할 수 있습니다. 이 논문에서는 효율적인 Transformer 기반 비용 집계 네트워크인 CostFormer를 사용하여 이러한 한계를 극복합니다. Residual DepthAware Cost Transformer(RDACT)는 깊이와 공간 차원을 따라 자체 주의 메커니즘을 통해 비용 볼륨에 대한 장거리 기능을 집계하도록 제안됩니다. 또한 Residual Regression Transformer(RRT)는 공간 주의를 향상시키기 위해 제안됩니다. 제안된 방법은 학습 기반 MVS 방법을 개선하기 위한 범용 플러그인입니다. 1 서론 한 장면의 다른 관점에서 보정된 일련의 이미지가 주어지면 Multi-view Stereo(MVS)는 관찰된 장면의 3D 정보를 복구하는 것을 목표로 합니다. 이는 컴퓨터 비전의 근본적인 문제이며 로봇 내비게이션, 자율 주행, 증강 현실 등에 널리 적용됩니다.최근의 학습 기반 MVS 네트워크[Yao et al., 2018; Gu et al., 2020; Wang et al., 2021b]는 3D 재구성의 품질과 효율성 측면에서 모두 고무적인 성공을 거두었습니다.일반적으로 심층 MVS 접근 방식은 다음의 5단계로 구성됩니다.공유 가중치가 있는 CNN 네트워크를 통한 다중 뷰 이미지에서 피처 추출, 모든 소스 피처를 참조 뷰에 정렬하기 위한 미분 가능한 워핑, 참조 피처와 정렬된 소스 피처에서 일치 비용 계산, 일치 비용 집계 또는 정규화, 깊이 또는 불일치 회귀.*이 저자들은 이 작업에 동등하게 기여했습니다.*책임 저자. 전체 오류(mm) 0.0.PatchMatchNet MVSNet 0.0.R-MVSNet 0.0.CVP-MVSNet CasMVSNet 0.UCS-Net 당사 0.0.0.0.0.* 0.0.0.0.0.0.1.1.GPU 메모리(GB) 런타임(초) 그림 1: DTU에서 최신 MVS 방법과의 비교. 이미지 크기가 1152x864일 때 오류, GPU 메모리 및 런타임 간의 관계. 학습 기반 MVS의 현재 진행 상황은 주로 재구성 품질[Wei 등, 2021; Yang 등, 2020a], 메모리 소비[Yan 등, 2020; Wei 등, 2021] 및 효율성[Wang 등, 2021b; Wang et al., 2021a]. 이러한 작업의 기본 네트워크 아키텍처는 우아하고 안정적인 기준선을 제공하는 MVSNet[Yao et al., 2018]이라는 선구적인 백본 네트워크를 기반으로 합니다. 그러나 MVSNet[Yao et al., 2018]의 네트워크 설계 원리를 당연하게 받아들이는 대신 MVS 문제의 과제를 밀집 대응 문제[Hosni et al., 2012]로 다시 생각해 볼 수 있습니다. MVS의 핵심은 모든 워핑된 소스 이미지에서 에피폴라 선을 따라 참조 이미지의 특정 픽셀에 해당하는 픽셀을 검색하는 밀집 픽셀별 대응 추정 문제입니다. 이 대응 추정 문제가 어떤 과제에 적용되든 매칭 과제는 고전적인 매칭 파이프라인[Scharstein and Szeliski, 2002]으로 축소될 수 있습니다. (1) 특징 추출, (2) 비용 집계. 학습 기반 MVS 방법에서 기존의 수작업 피처에서 CNN 기반 피처로의 전환은 대규모 데이터에서 학습한 강력한 피처 표현을 제공함으로써 기존 매칭 파이프라인의 이전 단계를 본질적으로 해결합니다. 그러나 사전 지식 없이 피처 간 유사성을 매칭하여 비용 집계 단계를 처리하는 것은 일반적으로 반복적인 패턴이나 배경 클러터로 인해 발생하는 모호성으로 인한 문제에 시달립니다[Cho et al., 2021]. 결과적으로 MVSNet 및 그 변형[Yao et al., 2018; Gu et al., 2020; Wang et al., 2021b]의 일반적인 솔루션은 비용 볼륨의 초기 상관 관계 단서의 품질에 직접 의존하지 않고 3D CNN 또는 RNN을 적용하여 참조 및 소스 뷰 간 비용 볼륨을 정규화하는 것입니다. 이러한 방법은 이전 방법에서 다양하게 공식화되었지만, 심각한 변형에 독립적인 수작업 기술을 사용하거나 CNN의 한계(예: 제한된 수용 필드)를 계승하여 지역적으로 일관된 잘못된 일치를 구별할 수 없습니다.이 연구에서는 비용 볼륨의 비용 집계 단계에 초점을 맞추고 위의 문제를 해결하기 위해 새로운 비용 집계 Transformer(CostFormer)를 제안합니다.우리의 CostFormer는 전역 수용 필드와 장거리 종속 표현으로 유명한 Transformer[Vaswani et al., 2017]를 기반으로 합니다.비용 볼륨에서 일치 비용을 집계함으로써, 우리의 집계 네트워크는 전역 대응 관계를 탐색하고 Transformer의 셀프 어텐션(SA) 메커니즘의 도움으로 모호한 일치 지점을 효과적으로 정제할 수 있습니다.Vision Transformer의 유망한 성능은 많은 응용 프로그램에서 입증되었지만[Dosovitskiy et al., 2020; Sun et al., 2021], 기존 SA에서 키-쿼리 점곱 상호작용의 시간 및 메모리 복잡도는 입력의 공간 해상도에 따라 2차적으로 증가합니다. 따라서 3D CNN을 Transformer로 대체하면 메모리에서 예상치 못한 추가 점유와 추론 지연이 발생할 수 있습니다. [Wang et al., 2021b]에서 영감을 얻어 Transformer 아키텍처를 반복적 다중 스케일 학습 가능 PatchMatch 파이프라인에 추가로 도입합니다. Transformer의 장거리 수용 필드의 장점을 계승하여 재구성 성능을 크게 개선합니다. 동시에 효율성과 성능 간에 균형 잡힌 균형을 유지하여 다른 방법에 비해 추론 속도와 매개변수 크기에서 경쟁력이 있습니다. 우리의 주요 기여는 다음과 같습니다. (1) 이 논문에서는 CostFormer라는 새로운 Transformer 기반 비용 집계 네트워크를 제안합니다. 이는 학습 기반 MVS 방법에 플러그인하여 비용 볼륨을 효과적으로 개선할 수 있습니다. (2) CostFormer는 비용 볼륨에 효율적인 잔여 깊이 인식 비용 변환기를 적용하여 2D 공간 주의를 3D 깊이 및 공간 주의로 확장합니다.(3) CostFormer는 비용 집계와 깊이 회귀 사이에 효율적인 잔여 회귀 변환기를 적용하여 공간 주의를 유지합니다.(4) 제안된 CostFormer는 DTU[Aanæs et al., 2016], Tanks &amp; Temples[Knapitsch et al., 2017] ETH3D[Schöps et al., 2017] 및 BlendedMVS[Yao et al., 2020] 데이터 세트를 평가할 때 학습 기반 MVS 방법에 이점을 제공합니다.2 관련 연구 2.1 학습 기반 MVS 방법 딥 러닝 기반 기술의 큰 성공에 힘입어 많은 학습 기반 방법이 멀티뷰 스테레오의 성능을 높이기 위해 제안되었습니다. MVSNet[Yao et al., 2018]은 MVS 작업을 위한 각 참조 뷰에서 깊이 맵을 추론하는 엔드투엔드 네트워크의 랜드마크입니다. 각 뷰에서 2D CNN으로 추출한 피처 맵은 분산 기반 비용 볼륨을 구축하기 위해 동일한 참조 뷰로 재투영됩니다. 3D CNN은 깊이 맵을 회귀하는 데 추가로 사용됩니다. 이 선구적인 작업에 따라 속도를 높이고 메모리 점유율을 줄이기 위해 많은 노력이 기울여졌습니다. 막대한 메모리 비용의 부담을 덜기 위해 순환 신경망을 활용하여 AA-RMVSNet[Wei et al., 2021]에서 비용 볼륨을 정규화합니다. 영어: 계산적으로 효율적인 네트워크를 개발하기 위해 거친-정밀 방식을 따른 최근 연구들은 CasMVSNet [Gu et al., 2020], CVP-MVSNet [Yang et al., 2020a], UCSNet [Cheng et al., 2020] 등과 같이 단일 비용 볼륨을 여러 단계에 걸쳐 여러 비용 볼륨으로 분할합니다. 기존 PatchMatch 스테레오 알고리즘에서 영감을 받은 PatchMatchNet [Wang et al., 2021b]은 반복적으로 PatchMatch 스테레오의 파이프라인을 상속받아 학습 기반 엔드투엔드 네트워크로 확장합니다. 2.2 Vision Transformer Transformer [Vaswani et al., 2017]와 그 변형 [Dosovitskiy et al., 2020; Liu et al., 2021]의 성공은 최근 몇 년 동안 신경 언어 처리의 개발을 촉진했습니다. 이러한 작업에서 영감을 얻어 Transformer는 비전 작업으로 성공적으로 확장되었으며 이미지 분류 성능을 높이는 것으로 입증되었습니다[Dosovitskiy et al., 2020]. 선구적인 작업에 이어 Transformer의 강력한 표현 능력으로 다양한 비전 작업의 개발을 촉진하기 위해 많은 노력이 기울여지고 있습니다.[Li et al., 2021]에서는 고전적인 스테레오 시차 추정 작업에서 Transformer를 적용하는 것을 신중하게 조사했습니다.Swin Transformer[Liu et al., 2021]는 계층적 구조를 Vision Transformers에 포함하고 이동된 창으로 표현을 계산합니다.어텐션 메커니즘을 통해 전역 콘텐츠 정보를 추출하는 데 있어 Transformer의 우수성을 고려하여 많은 작업이 피처 매칭 작업에 활용하려고 시도합니다.한 쌍의 이미지가 주어지면 CATS[Cho et al., 2021]는 Transformer에서 추출한 상관 관계 맵 간의 전역적 합의를 탐색하여 셀프 어텐션 메커니즘을 최대한 활용하고 픽셀 간의 장거리 종속성을 모델링할 수 있습니다. LoFTR[Sun et al., 2021]은 또한 조밀한 대응 관계를 모델링하기 위해 조악-미세 방식으로 Transformers를 활용합니다.STTR[Li et al., 2021]은 시퀀스-시퀀스 매칭 관점에서 스테레오 깊이 추정 작업으로 Feature Matching Transformer 아키텍처를 확장합니다.TransMVSNet[Ding et al., 2021]은 Feature Matching Transformer(FMT)를 활용하여 셀프 어텐션과 크로스 어텐션을 활용하여 이미지 내부 및 이미지 간에 장거리 컨텍스트 정보를 집계하는 저희와 비교했을 때 가장 관련성이 높은 동시 작업입니다.특히 TransMVSNet의 초점은 비용 집계 전에 Feature 추출을 향상시키는 데 있는 반면, 제안하는 CostFormer는 비용 볼륨에 대한 비용 집계 프로세스를 개선하는 것을 목표로 합니다.3 방법론 이 섹션에서는 비용 볼륨의 비용 집계 단계에 초점을 맞춘 제안된 CostFormer의 자세한 아키텍처를 소개합니다. CostFormer에는 RDACT(Residual-Depth Aware Cost Transformer) 및 RRT(Residual Regression Transformer)라는 특별히 설계된 두 개의 모듈이 포함되어 있으며, 이는 장거리 내 픽셀 간의 관계와 다른 참조 이미지 소스 이미지 간의 관계를 탐색하는 데 사용됩니다.INIT&amp;PROP STAGEDATL DASTL DATL W →&gt;&gt; →3D → INIT&amp;PROP STAGEW INIT&amp;PROP STAGEW-&gt; DASTL REC RDACT →3D-&gt; → 3D RDACT RRT RDACT RRT RDACT RRT W 워핑 C 비용 계산 3D 3D CNN CA 비용 집계 DD 깊이 회귀 D 워핑된 특징 비용 볼륨 정규화된 비용 볼륨 변환된 비용 볼륨 집계된 비용 볼륨 변환된 집계된 비용 볼륨 INIT&amp;PROP 초기화 및 전파 참조 이미지 RTL RSTL RER RDACT RRT 정제 STAGEO RRT 잔여 깊이 인식 비용 변환기 잔여 회귀 변환기 그림 2: 구조 PatchMatchNet을 기반으로 하는 CostFormer. 평가 프로세스 동안의 심층 가설. 섹션 예비에서 우리는 방법의 파이프라인에 대한 간략한 예비를 제공합니다. 그런 다음 각각 RDACT와 RRT의 구성을 보여줍니다. 마지막으로, 우리는 보여줍니다.
--- EXPERIMENT ---
s. 3. 예비 일반적으로 제안된 RDACT와 RRT는 학습 기반 MVS 네트워크의 임의의 비용 볼륨과 통합될 수 있다. 패치 매치 아키텍처[Wang et al., 2021b]를 기반으로 비용 볼륨의 비용 집계 문제를 추가로 탐구한다. 그림 2에서 볼 수 있듯이 PatchMatchNet[Wang et al., 2021b]을 기반으로 하는 CostFormer는 다중 뷰 이미지에서 피처 맵을 추출하고 초기화 및 전파를 수행하여 소스 뷰의 피처 맵을 참조 뷰로 워핑한다. 참조 뷰의 픽셀 p와 j번째 깊이 가설 dj에 따라 i번째 소스 뷰의 해당 픽셀 på가 주어지면 다음과 같이 정의된다. -Pi,j = Ki · (Ro,i ·(Kõ¹· p⋅ d;) + to,i) . . (1) 여기서 Roi와 to, i는 참조 뷰와 i번째 소스 뷰 간의 회전과 변환을 나타낸다. Ko와 Ki는 참조 및 i번째 소스 뷰의 내재적 행렬입니다. i번째 소스 뷰 Fi(pi,j)에서 워핑된 피처 맵은 원래 해상도를 유지하기 위해 쌍선형 보간됩니다. 그런 다음, 피처 맵의 유사성에서 비용 볼륨이 구성되고 3D CNN이 비용 볼륨을 정규화하는 데 적용됩니다. 모든 소스 뷰의 워핑된 피처는 다음과 같이 그룹별 수정을 통해 가설당 비용 S₁ (p, j)9을 계산하여 각 픽셀 p와 깊이 가설 dj에 대한 단일 비용으로 통합됩니다. G Si(p, j) = &lt; Fo(p)⁹, Fi(pi,j)&#39; &gt;ERG (2) 여기서 G는 그룹 번호, C는 채널 번호, &lt; ·, · &gt;는 내적이며 Fo(p)9와 Fi(pi,j)9는 각각 i번째 뷰에서 그룹화된 참조 피처 맵과 그룹화된 소스 피처 맵입니다. 그런 다음 픽셀별 뷰 가중치 wi(p)로 뷰에 대해 집계하여 S(p, j)를 얻습니다. 비용 집계(CA) 단계에서 Transformer를 고려하지 않고 CA 모듈은 먼저 1×1×1 커널을 사용한 3D 합성곱을 사용하는 작은 네트워크를 활용하여 단일 비용 C Ke k=Ke Є RH×W×D를 얻습니다. Ke 픽셀의 공간 창에 대해 {pk}는 그리드로 구성할 수 있으며, 픽셀당 추가 오프셋 {Apk}k=를 공간 적응을 위해 학습할 수 있습니다. 집계된 공간 비용 C(p, j)는 다음과 같이 정의됩니다. C(p, j) = = Ке-Σ Wkdk C (p + Pk + Apk, j) (3) Σ±1 Wkdk k=Ke k=Ke 여기서 wк와 dê는 특징과 깊이 유사성에 따라 비용 C에 가중치를 둡니다. 샘플링 위치 (p + Pk + Apk) k±1이 주어지면 Fo의 해당 특징은 쌍선형 보간을 통해 추출됩니다. 그런 다음 각 샘플링 위치와 p의 특징 간에 그룹별 상관관계가 적용됩니다. 결과는 1×1×1 커널과 시그모이드 비선형성을 갖는 3D 합성곱 계층이 정규화된 가중치 {k} 1을 출력하는 데 적용되는 볼륨으로 연결됩니다. 각 샘플링 포인트와 j번째 가설이 있는 픽셀 p 간의 역 깊이의 절대 차이가 수집됩니다. 그런 다음 역 차이에 대한 시그모이드 함수 Ke가 적용되어 {d}/k=Ke k= 주목할 만한 점은 이러한 비용 집계가 반복적인 패턴이나 배경 클러터로 인해 발생하는 모호성으로 인해 불가피하게 어려움을 겪는다는 것입니다. 모호성의 로컬 메커니즘은 로컬 전파 및 작고 학습 가능한 약간의 오프셋에 의한 공간 적응과 같은 많은 연산에 존재합니다. CostFormer는 RDACT 및 RRT를 통해 이러한 문제를 상당히 완화합니다. 원래 CA 모듈도 RDACT와 RRT 사이에 재배치됩니다. RRT 후에 소프트 argmin이 적용되어 회귀된 깊이를 얻습니다. 마지막으로 깊이 세분화 모듈은 깊이 회귀를 세분화하도록 설계되었습니다. CascadeMVS 및 기타 캐스케이드 아키텍처의 경우 CostFormer를 비슷하게 플러그인할 수 있습니다.3.2 잔여 깊이 인식 비용 변환기 이 섹션에서는 잔여 깊이 인식 비용 변환기(RDACT)의 세부 정보를 살펴봅니다.각 RDACT는 두 부분으로 구성됩니다.첫 번째 부분은 깊이 인식 변환기 계층(DATL)과 깊이 인식 이동 변환기 계층(DASTL)의 스택으로, 비용 볼륨을 처리하여 관계를 충분히 탐색합니다.두 번째 부분은 ReUCSNet PatchmatchNet Ours Ground TruthTUBORG 10 STK 18&#39; TUF 10 STIC TUFORG 10 STKTUF 10TER TUFORG 10 STTUF TI 그림 3: DTU 평가 세트에서 다양한 방법 비교.여기서 CostFormer의 백본은 PatchMatchNet입니다.첫 번째 부분에서 비용 볼륨을 복구하는 임베딩 비용 계층(REC). 주어진 비용 볼륨 Co ERH×W×D×G¸ 임시 중간 비용 볼륨 C1, C2,..., CL Є RHXWXDXE는 DATL 및 DASTL에 의해 번갈아 추출됩니다. =는 먼저 Ck DASTLk (DATLk (Ck−1)), k = 1, 2, ..., L (4)입니다. 여기서 DATL은 일반 윈도우가 있는 k번째 깊이 인식 변환기 계층이고, DASTLk는 이동된 윈도우가 있는 k번째 깊이 인식 변환기 계층이며, E는 DATLk 및 DASTLk의 임베딩 차원 번호입니다. 그런 다음 재임베딩 비용 계층이 마지막 Ck, 즉 CL에 적용되어 E에서 G를 복구합니다. RDACT의 출력은 다음과 같이 공식화됩니다. Cout = REC(CL) + Co (5) 여기서 REC는 재임베딩 비용 계층이고 G 출력 채널이 있는 3D 합성곱이 될 수 있습니다. E = G인 경우 Cout은 다음과 같이 간단히 공식화할 수 있습니다.Cout=CL+Co (6) 이 잔여 연결은 다양한 수준의 비용 볼륨을 집계할 수 있게 합니다.그러면 Co 대신 Cout이 섹션 3.1에서 설명한 원래 집계 네트워크에 의해 집계됩니다.전체 RDACT는 그림 2의 빨간색 창에 나와 있습니다.DATL과 DASTL의 구성을 소개하기 전에 DepthAware Multi-Head Self-Attention(DA-MSA)과 DepthAware Shifted Multi-Head Self-Attention(DAS-MSA)이라는 핵심 구성의 세부 사항을 살펴보겠습니다.DA-MSA와 DAS-MSA는 모두 Depth-Aware SelfAttention 메커니즘을 기반으로 합니다.Depth-Aware Self-Attention 메커니즘을 설명하기 위해 Depth-Aware Patch Embedding과 Depth-Aware Windows에 대한 지식을 예비적으로 제공합니다. 깊이 인식 패치 임베딩: 분명히 픽셀 수준에서 피처 맵에 대한 어텐션 메커니즘을 직접 적용하는 것은 GPU 메모리 사용 측면에서 상당히 비용이 많이 듭니다. 이 문제를 해결하기 위해, 우리는 높은 메모리 비용을 줄이고 추가적인 정규화를 얻기 위해 깊이 인식 패치 임베딩을 제안합니다. 구체적으로, 집계 CE RHXWXDXG 전에 그룹화된 비용 볼륨이 주어지면 깊이 인식 패치 임베딩이 먼저 C에 적용되어 토큰을 얻습니다. 이것은 커널 크기 hxwxd와 계층 정규화를 사용한 3D 합성곱으로 구성됩니다. 비용 볼륨의 공간 크기를 다운샘플링하고 깊이 가설을 유지하기 위해 h와 w를 1보다 크게 설정하고 d를 1로 설정합니다. 따라서 샘플 비율은 메모리 비용과 런타임에 적응적입니다. 합성곱 전에 비용 볼륨은 공간 크기와 다운샘플링 비율에 맞게 패딩됩니다. 계층 정규화(LN) 후, 이러한 임베딩된 패치는 깊이 인식 창으로 추가로 분할됩니다. 깊이 인식 윈도우: 비선형 및 선형 글로벌 자기 주의 너머로, 윈도우 내의 로컬 자기 주의가 더 효과적이고 효율적인 것으로 입증되었습니다. 2D 윈도우의 예로, Swin Transformer [Liu et al., 2021]는 글로벌 토큰의 큰 계산 복잡성을 피하기 위해 겹치지 않는 2D 윈도우에 멀티헤드 자기 주의 메커니즘을 직접 적용합니다. 2D 공간 윈도우에서 확장된 깊이 정보가 있는 임베디드 비용 볼륨 패치 = RH**W**D* ×G가 겹치지 않는 3D 윈도우로 분할됩니다. 그런 다음 이러한 로컬 윈도우를 전치하여 로컬 비용 토큰으로 재구성합니다. 이러한 윈도우의 크기를 hs × ws × ds라고 가정하면 총 토큰 수는 COLMAP UCSNet PatchmatchNet Ours 2T 37 = 9mm입니다. 그림 4: Tanks&amp;Temples의 다양한 방법 비교. 공식 벤치마크에서 보고한 Recall이 제시됩니다. ատ [*] × [W] × [D]. 이러한 로컬 토큰은 멀티헤드 셀프 어텐션 메커니즘에 의해 추가로 처리됩니다.깊이 인식 셀프 어텐션 메커니즘: 비용 창 토큰 X Є Rh, xwxd, XG의 경우 쿼리, 키 및 값 행렬 Q, K 및 VЄ RhsxwsxdsxG는 다음과 같이 계산됩니다.Q=XPQ, K = XPк,V = XPv (7) 여기서 PQ, PK 및 Pv Є RGXG는 서로 다른 창에서 공유되는 투영 행렬입니다. 각 헤드에 대해 깊이 및 공간 인식 상대적 위치 바이어스 B1 € R(hsxhs)x(wsxws)x(dsxds)를 도입함으로써 3D 로컬 윈도우 내의 깊이 인식 자기 주의(DA-SA1) 행렬은 다음과 같이 계산됩니다. DA-SA1 = Attention 1(Q1, K1, V1) = SoftMax( Q1K1T √G + B1)V(8) 여기서 Q1, K1 및 V1 € Rh¸wsds ×G는 Q, K 및 VЄ RhsxwxdsxG에서 재구성됩니다. LayerNorm(LN)과 현재 레벨의 다중 헤드 DA-SA1을 사용한 DATL 프로세스는 다음과 같이 공식화됩니다.Â¹ = DA-MSA1((LN(X²−¹)) + X¹−(9) 각 헤드에 대해 깊이 인식 상대 위치 바이어스 B2 € Rdsds를 도입함으로써 깊이 차원을 따라 깊이 인식 자기 주의(DASA2) 행렬은 DATL에 대한 대체 모듈이며 다음과 같이 계산됩니다.DA-SA2 = Attention2(Q2, K2, V2) = Soft Max( Q2K2T √G + B2)V(10) 여기서 Q2, K2 및 V2 € Rhs wsxdsxG는 Q, K 및 VЄ Rhsxwxd,×G에서 재구성됩니다.B1 및 B2는 깊이 차원을 따라 있고 [-d, +1, ds - 1] 범위에 있습니다.높이 및 너비 차원을 따라 B1은 범위에 있습니다. [−hs + 1,h¸ − 1] 및 [−ws + 1, Ws 1]. 실제로, 우리는 B1에서 더 작은 크기의 바이어스 행렬 B1 € R(2h-1)x(2ws-1)x(2ds-1)을 매개변수화하고 f번 동안 병렬로 어텐션 함수를 수행한 다음 깊이 인식 멀티헤드 셀프 어텐션(DA-MSA) 출력을 연결합니다. 현재 수준에서 LayerNorm(LN), 멀티헤드 DA-SA1 및 DA-SA2를 사용한 DATL 프로세스는 다음과 같이 공식화됩니다. Î¹ = DA-MSA1(LN(DA-MSA2(LN(X¹¯¹)))) + X²−(11) 그런 다음, GELU 비선형성을 갖는 두 개의 완전 연결 계층이 있는 MLP 모듈이 추가적인 특징 변환에 사용됩니다. X¹ = MLP(LN(Â¹))) + Â¹ (12) 글로벌과 비교 주의, 로컬 주의는 고해상도 계산을 가능하게 합니다. 그러나 고정된 파티션이 있는 로컬 윈도우 간에는 연결이 없습니다. 따라서 일반 및 이동된 윈도우 파티션을 번갈아 사용하여 크로스 윈도우 연결을 가능하게 합니다. 따라서 다음 레벨에서 윈도우 파티션 구성은 높이, 너비 및 깊이 축을 따라 hs Ws 2만큼 이동합니다. 깊이 인식 자체 주의는 이러한 이동된 윈도우(DAS-MSA)에서 2로 계산됩니다. DASTL의 전체 프로세스는 다음과 같이 공식화할 수 있습니다.(쁨, Ŷ²+1 = DAS-MSA1(LN(DAS-MSA2(LN(X¹)))) + X¹ X²+1 = MLP(LN(Â¹+¹)) + Ŷ¹+(13) (14) DAS-MSA1과 DAS-MSA2는 각각 이동된 윈도우 내의 멀티헤드 어텐션과 어텐션2에 해당합니다. 단계의 수가 n이라고 가정하면 CostFormer에 n개의 RDACT 블록이 있습니다. 3.3 잔차 회귀 변환기 집계 후 비용 CE RHXWXD가 깊이 회귀에 사용됩니다. 특정 깊이에서 공간 관계를 더 탐색하기 위해 소프트맥스 전에 변환기 블록이 С에 적용됩니다. RDACT에서 영감을 받아 잔차 회귀 변환기(RRT)의 전체 프로세스는 다음과 같이 공식화할 수 있습니다. Ck = RSTk (RTk (Ck−1)), k = 1, 2, ..., L Cout = RER(CL) + Co (15) (16) 방법 Mean Fam. 프라. MVSNet [Yao 외, 2018] 43.55.28.25.CasMVSNet [Gu 외, 2020] 56.76.37 58.UCS-Net [Cheng 외, 2020] 54.CVP-MVSNet [Yang 외, 2020b] 54.PVA-MVSNet [Yi 외 al., 2020] 54.AA-RMVSNet [Wei et al., 2021] 61.PatchmatchNet [Wang et al., 2021b] 53.64.56.63.중간 그룹(F-score ↑) Hor. 리그. M60 팬. Pla. Tra. 50.79 53.96 50.86 47.90 34.46.26 55.81 56.11 54.06 58.18 49.76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.76.50 47.74 36.34 55.12 57.28 54.28 57.43 47.69.36 46.80 46.01 55.74 57.23 54.75 56.70 49.77.77 59.53 51.53 64.02 64.05 59.47 60.85 54.66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81.20 66.34 53.11 63.46 66.09 64.84 62.23 57.76.92 59.82 50.16 56.73 56.53 51.22 56.58 47.80.92 65.83 56.94 62.54 63.06 60.00 60.20 58.평균 호주 고급 그룹 (F-점수 ↑) Bal.Co.M.P.Tem. 31.19.38.46 29.10 43.27.36 28.56.27(+3.12) 72.46 52.59 54.27 55.83 56.80 50.88 55.05 52.57.10(+3.95) 74.22 56.27 54.41 56.65 54.46 51.45 57.65 51.64.40(+0.04) 81.45 66.22 53.88 62.94 66.12 65.35 61.31 57.64.51(+0.15) 81.31 65.51 55.57 63.46 66.24 65.39 61.27 57.33.20.96 40.15 32.05 46.01 29.28 32.32.23.69 37.73 30.04 41.80 28.31 32.38.28.33 44.36 39.74 52.89 33.80 34.32.22.83 39.33.87 45.46 27.95 27.37.24.84 44.59 34.77 46.49 34.69 36.37.26.68 42.14 35.65 49.37 32.16 39.34.07(+1.76) 24.05 39.20 32.17 43.95 28.62 36.34.31(+2.00) 26.77 39.13 31.58 44.55 28.79 35.39.55(+0.59) 28.61 45.63 40.21 52.81 34.40 35.39.43(+0.47) 29.18 45.21 39.88 53.38 34.07 34.UniMVSNet [Peng 외, 2022] MVSTR [Zhu 외, 2021] TransMVS [Ding 외, 2022] MVSTER [왕외 al., 2022] CostFormer(PatchMatchNet) CostFormer(PatchMatchNet*) CostFormer(UniMVSNet) CostFormer(UniMVSNet*) 표 1: Tanks &amp; Temples 벤치마크에서 다양한 방법에 대한 정량적 결과(높을수록 좋음). *는 DTU에서 사전 학습되고 BlendedMVS에서 미세 조정됩니다. -는 DTU에서 사전 학습되지 않고 Blended MVS에서 처음부터 학습됩니다. 여기서 RT는 일반 윈도우가 있는 k번째 회귀 변환기 계층이고, RSTÅ는 이동된 윈도우가 있는 k번째 회귀 변환기 계층이고, RER은 CL에서 깊이 차원을 복구하기 위한 재임베딩 계층이며, D 출력 채널이 있는 2D 합성곱이 될 수 있습니다. RRT는 또한 로컬 윈도우에서 셀프 어텐션을 계산합니다. RDACT와 비교하여 RRT는 공간 관계에 더 중점을 둡니다. 일반적인 Swin [Liu et al., 2021] Transformer 블록과 비교했을 때 RRT는 깊이를 채널로 처리하고, 채널 수는 실제로 1이고 이 채널은 Transformer 전에 압축됩니다.임베딩 매개변수는 다양한 반복의 비용 집계에 맞게 설정됩니다.임베딩 차원 번호가 D와 같으면 Cout을 다음과 같이 간단히 공식화할 수 있습니다.Cout = CL + Co (17) 단계는 다른 깊이 가설로 여러 번 반복될 수 있으므로 RRT 블록의 수는 반복 횟수와 동일하게 설정해야 합니다.전체 RRT는 그림 2의 노란색 창에 표시되어 있습니다.4 학습 4.1 손실 함수 최종 손실은 모든 단계에서 모든 반복의 손실과 최종 정제 모듈의 손실을 결합합니다.sn 손실 = ΣΣΙ + Lref k=1i=(18) 여기서 L은 k번째 단계에서 i번째 반복의 회귀 또는 통합 손실입니다. Lref는 정제 모듈의 회귀 또는 통합 손실입니다. 정제 모듈이 없으면 Lref 손실은 0으로 설정됩니다. 4.2 일반적인 학습 설정 CostFormer는 Pytorch에서 구현됩니다[Paszke et al., 2019]. RDACT의 경우 3, 2, 1 단계에서 깊이 번호를 4, 2, 2로 설정하고, 높이, 너비 및 깊이 축에서 패치 크기를 4, 4, 1로 설정하고, 높이, 너비 및 깊이 축에서 윈도우 크기를 7, 7, 2로 설정합니다. 백본을 PatchMatchNet으로 설정하면 3, 2, 1 단계에서 임베딩 차원 번호를 8, 8, 4로 설정합니다. RRT의 경우 모든 단계에서 깊이 번호를 2로 설정하고, 모든 축에서 패치 크기를 1로 설정하고, 모든 축에서 윈도우 크기를 8로 설정합니다. 백본을 PatchMatchNet으로 설정하고 반복 2, 2, 단계 3, 2, 1에서의 차원 번호를 32, 64, 16, 16, 8로 임베딩하는 경우. 모든 모델은 Nvidia GTX V100 GPU에서 학습되었습니다. 깊이 추정 후 MVSNet[Yao et al., 2018]과 유사한 포인트 클라우드를 재구성합니다. 실험 이 섹션에서는 여러 MVS 데이터 세트를 소개하고 이러한 데이터 세트에서 방법을 평가합니다. 결과에 대해서는 나중에 자세히 보고합니다. 5. 데이터 세트 평가에 사용된 데이터 세트는 DTU [Aanæs et al., 2016], BlendedMVS [Yao et al., 2020], ETH3D [Schöps et al., 2017], Tanks &amp; Temples [Knapitsch et al., 2017], YFCC-100M [Thomee et al., 2016]입니다. DTU 데이터 세트는 124개의 서로 다른 장면이 있는 실내 다중 뷰 스테레오 데이터 세트로, 한 장면에 7가지 서로 다른 조명 조건에서 49개의 뷰가 있습니다. Tanks &amp; Temples는 보다 복잡하고 사실적인 환경에서 수집되었으며 중급 및 고급 세트로 나뉩니다. ETH3D 벤치마크는 시점 변화가 강한 장면의 보정된 고해상도 이미지로 구성됩니다. 훈련 및 테스트 데이터 세트로 나뉩니다. 훈련 데이터 세트에는 13개의 장면이 포함되어 있는 반면 테스트 데이터 세트에는 12개의 장면이 포함되어 있습니다. BlendedMVS 데이터 세트는 113개의 실내 및 실외 장면으로 구성되고 106개의 훈련 장면과 7개의 검증 장면으로 분할된 대규모 합성 데이터 세트입니다.5. DTU의 주요 설정 및 결과 DTU [Aanæs et al., 2016] 평가 세트에 대한 평가를 위해 DTU 훈련 세트만 사용합니다.훈련 단계에서 이미지 해상도를 640×512로 설정했습니다.우리는 우리의 방법을 CasMVSNet [Gu et al., 2020] 및 PatchMatchNet [Wang et al., 2021b]을 포함한 최근의 학습 기반 MVS 방법과 비교합니다.이들 역시 CostFormer의 백본으로 설정되었습니다.우리는 DTU 데이터 세트에서 제공하는 평가 지표를 따릅니다.DTU 평가 세트에 대한 정량적 결과는 표 2에 요약되어 있으며, 이는 플러그 앤 플레이 CostFormer가 비용 집계를 개선한다는 것을 나타냅니다. 표 2의 부분적 시각화 결과는 그림 3에 나와 있습니다. 복잡도 분석: CostFormer의 복잡도 분석을 위해 이를 PatchMatchNet[Wang et al., 2021b]에 대입하여 먼저 메모리 소비량과 런타임을 전체(mm) 0.방법 정확도(mm) 비교합니다. (mm) Furu [Furukawa 및 Ponce, 2010] 0.0.Tola [Tola 등, 2012] 0.1.0.Gipuma [Galliani 등, 2015] 0.0.0.Colmap [Schönberger 및 Frahm, 2016] 0.0.0.SurfaceNet [Ji 등, 2017] 0.1.0.MVSNet [Yao 외, 2018] 0.0.0.R-MVSNet [Yao 외, 2019] 0.0.0.P-MVSNet [Luo 외, 2019] 0.0.0.Point-MVSNet [Chen 외, 2019] 0.0.0.Fast-MVSNet [Yu 및 Gao, 2020] 0.0.0.CasMVSNet [Gu 외, 2020] 0.0.0.UCS-Net [Cheng 외, 2020] 0.0.0.CVP-MVSNet [Yang 외, 2020b] 0.0.0.PVA-MVSNet [Yi 외, 2020] 0.0.0.PatchMatchNet [Wang et al., 2021b] 0.0.0.AA-RMVSNet [Wei et al., 2021] 0.0.0.UniMVSNet [Peng et al., 2022] 0.0.0.CostFormer(PatchMatchNet 기반) 0.0.0.343 (+0.0093) CostFormer(CasMVSNet 기반) 0.0.CostFormer(기준) UniMVSNet) 0.0.0.345 (+0.0097) 0.312 (+0.0035) 표 2: DTU에 대한 다양한 방법의 정량적 결과. 이 백본. 공정한 비교를 위해 1152 x 864의 고정 입력 크기를 사용하여 NVIDIA Telsa V100의 단일 GPU에서 계산 비용을 평가합니다. PatchMatchNet [Wang et al., 2021b]의 메모리 소비 및 런타임은 2323MB 및 0.169초입니다. 플러그인을 통해서만 2693MB 및 0.231초로 늘어납니다. PatchMatchNet [Wang et al., 2021b]의 보고서를 기반으로 다른 최신 학습 기반 방법의 비교 결과를 얻습니다. 메모리 소비량과 런타임은 CasMVSNet[Gu et al., 2020]에 비해 61.9%와 54.8% 감소했고, UCSNet[Cheng et al., 2020]에 비해 48.8%와 50.7% 감소했으며, CVP-MVSNet[Yang et al., 2020b]에 비해 63.5%와 77.3% 감소했습니다. 결과를 결합한 결과(낮을수록 좋음)는 표 3과 그림 1에 나와 있으며, CostFormer의 GPU 메모리와 런타임은 100%로 설정되었습니다. 방법 전체(mm) 0.0.GPU 메모리(%) CasMVSNet [Gu et al., 2020] UCSNet [Cheng et al., 2020] CVP-MVSNet [Yang et al., 2020b] 262.47% 런타임(%) 221.24% 195.31% 202.84% 273.97% 440.53% 0.저희의 100.00% 100.00% 0.표 3: DTU에서 다른 SOTA 학습 기반 MVS 방법과의 비교.전체 성능, GPU 메모리 및 런타임 간의 관계.Transformers와의 비교 또한 CostFormer를 다른 Transformers [Zhu et al., 2021; Wang et al., 2022; Ding et al., 2021; Liao et al., 2022]는 MVS 방법에서 사용되며 플러그 앤 플레이가 아닙니다. 공정한 비교를 위해 유사한 깊이 가설에서 순수한 Transformers의 직접적인 개선(높을수록 좋음)과 실행 시간의 증가 비용(낮을수록 좋음)만 표 4에 요약되어 있습니다. Blended MVS [Yao et al., 2020] 데이터 세트. 우리는 우리의 방법을 CostFormer의 백본으로 설정된 PatchMatchNet [Wang et al., 2021b] 및 UniMVSNet [Peng et al., 2022]을 포함한 최근의 학습 기반 MVS 방법과 비교합니다. Tanks &amp; Temples [Knapitsch et al., 2017] 세트에 대한 정량적 결과는 CostFormer의 견고성을 나타내는 표 1에 요약되어 있습니다. 표 1의 부분적 시각화 결과가 그림 4에 나와 있습니다.표 1의 UniMVSNet¯은 UniMVSNet 기준선보다 적은 데이터(DTU 없음)를 사용하는 BlendedMVS만을 사용하여 학습한다는 점을 명확히 하고 싶습니다.5.4 ETH3D의 주요 설정 및 결과 PatchMatchNet[Wang et al., 2021b]을 백본으로 사용하고 Tanks &amp; Temples 데이터 세트[Knapitsch et al., 2017]에서 사용된 학습된 모델을 채택하여 ETH3D[Schöps et al., 2017] 데이터 세트를 평가합니다.표 5에서 볼 수 있듯이, 우리 방법은 학습 및 특히 어려운 테스트 데이터 세트에서 다른 방법보다 우수한 성과를 보입니다(높을수록 좋음). 방법 MVE [Fuhrmann et al., 2014] Gipuma [Galliani et al., 2015] 훈련 테스트 F1 점수 ↑ 20.시간↓ 13278.Fl 점수 ↑ 30.시간↓ 10550.36.587.45.689.PMVS [Furukawa and Ponce, 2010] 46.836.44.957.COLMAP [Schönberger and Frahm, 2016] 67.2690.73.1658.PVSNet [Xu and Tao, 2020] 67.72.829.IterMVS [Wang et al., 2021a] 66.74.PatchMatchNet [Wang et al., 2021b] PatchMatch-RL [Lee et al., 2021] CostFormer(Ours) 64.452.73.492.67.68.92(+4.71) 566.72.75.24(+2.12) 547.표 5: ETH3D에서 다양한 방법에 대한 정량적 결과.5.5 BlendedMVS 데이터 세트의 주요 설정 및 결과 ETH3D에서 사용된 모델을 사용합니다. BlendedMVS [Yao et al., 2020] 평가 세트에서 N = 5, 이미지 해상도를 576 x 768로 설정했습니다. 엔드포인트 오류(EPE), 1픽셀 오류(e1), 펙셀 오류(e3)가 평가 지표로 사용됩니다. 다양한 방법의 정량적 결과(낮을수록 좋음)는 표 6에 나와 있습니다. 방법 EPE el(%) e3(%) MVSNet [Yao et al., 2018] 1.21.8.MVSNet-s [Darmon et al., 2021] 1.25.8.CVP-MVSNet [Yang et al., 2020a] 1.19.10.VisMVSNet [Zhang et al., 2020] 1.18.7.CasMVSNet [Gu et al., 2020] 1.15.7.EPPMVSNet [Ma et al., 2021] 1.12.6.TransMVSNet [Ding et al., 2021] 0.8.3.CostFormer(PatchmatchNet 기반) 0.12.4.CostFormer( UniMVSNet) 0.7.2.Method MVSTR [Zhu et al., 2021] TransMVS [Ding et al., 2021] Trans Improvement (mm) +0.Delta Time (s) +0.359s +0.+0.367s Delta Time (%) +78.21% +135.42% WT-MVSNet(CT) [Liao et al., 2022] MVSTER(CNN Fusion) [Wang et al., 2022] CostFormer(CNN Fusion) +0.+0.265s +0.+0.+0.016s +0.062s +13.34% +36.69% 표 6: BlendedMVS에서 다양한 방법에 대한 정량적 결과 표 4: DTU 평가 세트에서 다양한 Transformers의 성능 및 런타임 증가 비용의 정량적 개선. 5.3 탱크 및 사원의 주요 설정 및 결과 탱크 및 사원에 대한 평가를 위해 [Knapitsch et al., 2017], 우리는 DTU [Aanæs et al., 2016] 데이터 세트를 사용합니다.
--- CONCLUSION ---
이 연구에서 우리는 cost Transformer가 비용 집계를 개선할 수 있는지 알아보고 RDACT와 RRT 캐스케이드 모듈을 갖춘 새로운 CostFormer를 제안합니다. DTU [Aanæs et al., 2016], Tanks &amp; Temples [Knapitsch et al., 2017], ETH3D [Schöps et al., 2017], BlendedMVS [Yao et al., 2020]에 대한 실험 결과는 우리의 방법이 경쟁력 있고 효율적이며 플러그 앤 플레이 방식임을 보여줍니다. Cost Transformer는 다중 시점 스테레오에서 더 나은 비용 집계를 위한 여러분의 요구 사항일 수 있습니다. 참고문헌 [Aanæs et al., 2016] Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. 다중 시점 입체시를 위한 대규모 데이터. Int. J. Comput. Vis., 120(2):153–168, 2016. [Chen et al., 2019] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, pages 1538-1547. IEEE, 2019. [Cheng et al., 2020] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. 불확실성 인식을 통한 적응적 얇은 볼륨 표현을 사용한 딥 스테레오. In CVPR, pages 2521–2531. IEEE, 2020. [Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: 시각적 대응을 위한 비용 집계 변환기. 신경 정보 처리 시스템의 발전, 34, 2021. [Darmon et al., 2021] François Darmon, Bénédicte Bascle, Jean-Clément Devaux, Pascal Monasse, Mathieu Aubry. 딥 멀티뷰 스테레오의 야생화. CORR, abs/2104.15119, 2021. [Ding et al., 2021] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, Xiao Liu. Transmvsnet: 변압기를 갖춘 글로벌 컨텍스트 인식 멀티뷰 스테레오 네트워크. arXiv 사전 인쇄본 arXiv:2111.14600, 2021. [Ding et al., 2022] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, Xiao Liu. Transmvsnet: 변압기를 사용한 글로벌 컨텍스트 인식 다중 뷰 스테레오 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8585-8594페이지, 2022. [Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변압기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020. [Fuhrmann et al., 2014] Simon Fuhrmann, Fabian Langguth, and Michael Goesele. Mve - 다중 시점 재구성 환경. GCH, 11-18페이지. Eurographics Association, 2014. [Furukawa and Ponce, 2010] Yasutaka Furukawa and Jean Ponce. 정확하고 밀도가 높으며 견고한 다중 시점 입체시. IEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362–1376, 2010. [Galliani et al., 2015] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. 표면 정상 확산에 의한 대규모 평행 다중 시점 입체시. ICCV, 873-881페이지. IEEE Computer Society, 2015. [Gu et al., 2020] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan. 고해상도 멀티뷰 스테레오 및 스테레오 매칭을 위한 캐스케이드 비용 볼륨. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2495-2504페이지, 2020. [Hosni et al., 2012] Asmaa Hosni, Christoph Rhemann, Michael Bleyer, Carsten Rother, Margrit Gelautz. 시각적 대응 및 그 이상을 위한 빠른 비용-볼륨 필터링. IEEE 패턴 분석 및 머신 인텔리전스 저널, 35(2):504-511, 2012. [Ji et al., 2017] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, Lu Fang. Surfacenet: 다중 시점 입체시를 위한 종단 간 3D 신경망. ICCV, 2326-2334페이지. IEEE Computer Society, 2017. [Knapitsch et al., 2017] Arno Knapitsch, Jaesik Park, QianYi Zhou, Vladlen Koltun. Tanks and temples: benchmarking large-scale scene reconstruction. ACM Trans. Graph., 36(4):78:1-78:13, 2017. [Lee et al., 2021] Jae Yong Lee, Joseph DeGol, Chuhang Zou, Derek Hoiem. Patchmatch-rl: 픽셀 단위 깊이, 일반 및 가시성을 갖춘 딥 mvs. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 2021년 10월. [Li et al., 2021] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, Mathias Unberath. 변압기를 사용하여 시퀀스 간 관점에서 스테레오 깊이 추정 재검토. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 6197-6206페이지, 2021. [Liao et al., 2022] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe Huang, Shihao Ren, Jia Guo, Wensen Feng, Kai Zhang. Wt-mvsnet: 다중 뷰 스테레오를 위한 윈도우 기반 변환기, 2022. [Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin 변환기: 이동된 윈도우를 사용하는 계층적 비전 변환기. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 1001210022페이지, 2021. [Luo et al., 2019] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, Yawei Luo. P-mvsnet: 다중 뷰 스테레오를 위한 패치별 매칭 신뢰도 집계 학습. ICCV, 10451–10460페이지. IEEE, 2019. [Ma et al., 2021] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: 다중 뷰 스테레오를 위한 Epipolarassembling 기반 깊이 예측. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 5732-5740쪽, 2021. [Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: 필수 스타일, 고성능 딥 러닝 라이브러리. H. Wallach, H. Larochelle, A. Beygelzimer, F. d&#39;Alché-Buc, E. Fox, R. Garnett 편집, 신경 정보 처리 시스템의 발전 32, 8024-8035페이지. Curran Associates, Inc., 2019. [Peng et al., 2022] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, Ronggang Wang. 다중 뷰 스테레오를 위한 깊이 추정 재고: 통합 표현. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2022. [Scharstein 및 Szeliski, 2002] Daniel Scharstein 및 Richard Szeliski. 밀집 2프레임 스테레오 대응 알고리즘의 분류 및 평가. 컴퓨터 비전 국제 저널, 47(1):7-42, 2002. [Schönberger 및 Frahm, 2016] Johannes L. Schönberger 및 Jan-Michael Frahm. 모션에서 구조를 재검토했습니다. CVPR, 4104-4113페이지. IEEE 컴퓨터 학회, 2016. [Schöps 외, 2017] Thomas Schöps, Johannes L. Schönberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys 및 Andreas Geiger. 고해상도 이미지와 다중 카메라 비디오를 갖춘 다중 뷰 스테레오 벤치마크입니다. CVPR, 2538-2547페이지. IEEE 컴퓨터 학회, 2017. [Sun 외, 2021] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao 및 Xiaowei Zhou. Loftr: 변압기를 사용한 검출기 없는 로컬 특징 매칭. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 8922-8931쪽, 2021. [Thomee et al., 2016] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li. Yfcc100m: 멀티미디어 연구의 새로운 데이터. Commun. ACM, 59(2):64-73, 2016. [Tola et al., 2012] Engin Tola, Christoph Strecha, Pascal Fua. 초고해상도 이미지 세트를 위한 효율적인 대규모 멀티뷰 스테레오. Mach. Vis. Appl., 23(5):903–920, 2012. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. 신경 정보 처리 시스템의 발전, 30, 2017. [Wang et al., 2021a] Fangjinhua Wang, Silvano Galliani, Christoph Vogel 및 Marc Pollefeys. Iterms: 효율적인 다중 시점 스테레오를 위한 반복 확률 추정. arXiv 사전 인쇄본 arXiv:2112.05126, 2021. [Wang et al., 2021b] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: 학습된 멀티뷰 패치매치 스테레오. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 14194–14203쪽, 2021. [Wang et al., 2022] Xiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang Wang. Mvster: 효율적인 멀티뷰 스테레오를 위한 에피폴라 변압기, 2022. [Wei et al., 2021] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: 적응형 집계 순환 다중 시점 스테레오 네트워크. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 6187-6196쪽, 2021. [Xu 및 Tao, 2020] Qingshan Xu 및 Wenbing Tao. Pvsnet: 픽셀별 가시성 인식 다중 시점 스테레오 네트워크. CORR, abs/2007.07714, 2020. [Yan et al., 2020] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang 및 Yu-Wing Tai. 동적 일관성 검사를 사용한 고밀도 하이브리드 순환 다중 시점 스테레오 네트워크. 유럽 컴퓨터 비전 컨퍼런스, 674-689쪽. Springer, 2020. [Yang et al., 2020a] Jiayu Yang, Wei Mao, Jose M Alvarez, Miaomiao Liu. 다중 뷰 스테레오를 위한 비용 볼륨 피라미드 기반 깊이 추론. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 4877-4886페이지, 2020. [Yang et al., 2020b] Jiayu Yang, Wei Mao, Jose M. Alvarez, Miaomiao Liu. 다중 뷰 스테레오를 위한 비용 볼륨 피라미드 기반 깊이 추론. CVPR, 4876-4885페이지. IEEE, 2020. [Yao et al., 2018] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan. Mvsnet: 비정형 다중 뷰 스테레오를 위한 깊이 추론. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 767-783페이지, 2018. [Yao et al., 2019] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan. 고해상도 다중 시점 스테레오 깊이 추론을 위한 Recurrent mvsnet. CVPR, 5525-5534페이지. Computer Vision Foundation/IEEE, 2019. [Yao et al., 2020] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan. Blendedmvs: 일반화된 다중 시점 스테레오 네트워크를 위한 대규모 데이터 세트. 컴퓨터 비전 및 패턴 인식(CVPR), 2020. [Yi et al., 2020] Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, Yu-Wing Tai. 자체 적응형 뷰 집계를 갖춘 피라미드 다중 뷰 스테레오 넷. Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm 편집자, ECCV(9), Lecture Notes in Computer Science의 12354권, 766782페이지. Springer, 2020. [Yu 및 Gao, 2020] Zehao Yu 및 Shenghua Gao. Fastmvsnet: 학습된 전파 및 가우스-뉴턴 정제를 갖춘 희소-고밀도 다중 뷰 스테레오. CVPR, 1946-1955페이지. IEEE, 2020. [Zhang 외, 2020] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo 및 Tian Fang. 가시성을 인식하는 다중 뷰 스테레오 네트워크. 영국 머신 비전 컨퍼런스(BMVC), 2020. [Zhu et al., 2021] Jie Zhu, Bo Peng, Wanqing Li, Haifeng Shen, Zhe Zhang 및 Jianjun Lei. 변압기가 포함된 멀티뷰 스테레오. ArXiv, ABS/2112.00336, 2021.
