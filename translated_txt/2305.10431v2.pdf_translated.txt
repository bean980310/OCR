--- ABSTRACT ---
확산 모델은 텍스트-이미지 생성에 뛰어나며, 특히 개인화된 이미지를 위한 주제 중심 생성에 뛰어납니다. 그러나 기존 방법은 계산 집약적이고 효율적인 배포를 방해하는 주제별 미세 조정으로 인해 비효율적입니다. 게다가 기존 방법은 종종 주제 간의 정체성을 혼합하기 때문에 다중 주제 생성에 어려움을 겪습니다. 미세 조정 없이 효율적이고 개인화된 다중 주제 텍스트-이미지 생성을 가능하게 하는 FastComposer를 제시합니다. FastComposer는 이미지 인코더에서 추출한 주제 임베딩을 사용하여 확산 모델의 일반 텍스트 컨디셔닝을 증강하여 전방 패스만으로 주제 이미지와 텍스트 지침을 기반으로 개인화된 이미지를 생성할 수 있습니다. 다중 주제 생성에서 정체성 혼합 문제를 해결하기 위해 FastComposer는 훈련 중에 교차 주의 국소화 감독을 제안하여 참조 주제의 주의를 대상 이미지의 올바른 영역에 국한시킵니다. 주제 임베딩에 대한 순진한 컨디셔닝은 주제 과적합으로 이어집니다. FastComposer는 주제 중심 이미지 생성에서 정체성과 편집 가능성을 모두 유지하기 위해 노이즈 제거 단계에서 지연된 주제 컨디셔닝을 제안합니다. FastComposer는 다양한 스타일, 동작, 맥락을 가진 보이지 않는 여러 개인의 이미지를 생성합니다. 미세 조정 기반 방법에 비해 300~2500배의 속도 향상을 달성하며 새로운 피사체에 대한 추가 저장 공간이 전혀 필요하지 않습니다. FastComposer는 효율적이고 개인화되고 고품질의 다중 피사체 이미지 생성을 위한 길을 열어줍니다. 코드, 모델 및 데이터 세트는 재생산을 위해 공개됩니다.
--- INTRODUCTION ---
텍스트-이미지 생성[4, 8, 17, 28]의 최근 발전, 특히 확산 모델[13, 27, 31, 36, 37]은 콘텐츠 생성의 새로운 영역을 열었습니다. 주제 중심 텍스트-이미지 생성은 몇 가지 샘플 이미지[3, 9, 20, 25, 32]가 주어지면 새로운 개인에게 개인화를 허용하여 새로운 장면, 스타일 및 동작의 특정 주제를 특징으로 하는 이미지를 생성할 수 있습니다. 그러나 현재의 주제 중심 텍스트-이미지 생성 방법은 개인화 비용과 여러 주제에 대한 아이덴티티 블렌딩이라는 두 가지 주요 한계가 있습니다. 개인화는 최상의 충실도를 위해 종종 새로운 주제마다 모델 미세 조정이 필요하기 때문에 비용이 많이 듭니다. 주로 메모리 소비[6]와 역전파 계산으로 인해 모델 조정으로 인해 발생하는 계산 오버헤드와 높은 하드웨어 요구 사항은 이러한 모델을 다양한 플랫폼에 적용하는 데 제약이 됩니다. 또한 기존 기술은 모델이 다른 주체의 고유한 특성(주체 A가 주체 B처럼 보이고 그 반대의 경우도 마찬가지)을 결합하는 &quot;정체성 혼합&quot; 문제(왼쪽 그림 2)로 인해 다중 주체 생성(그림 1)에 어려움을 겪습니다. 우리는 튜닝이 필요 없는 개인화된 다중 주체 텍스트-이미지 생성 방법인 FastComposer를 제안합니다. 우리의 핵심 아이디어는 &quot;사람&quot;과 같은 일반적인 단어 토큰을 * Equal Contribution을 포착하는 임베딩으로 대체하는 것입니다. 연락처: Guangxuan Xiao<xgx@mit.edu> , 천위인<tianweiy@mit.edu> . 사전 인쇄본. 검토 중. 참고 문헌 및 해변에 앉아 있는 남자와 남자와 여자가 있는 공원과 함께 그리고 여자 여자와 남자의 일본 목판화 남자와 여자와 남자의 am 여자 StableDiffusion(텍스트만) Midjourney(텍스트만) Textual Inversion CustomDiffusion(미세 조정)(미세 조정) FastComposer(추론만) 그림 1: 다중 피험자 이미지 생성을 위한 기준선과의 비교. 텍스트 전용 방법(SD, MJ)의 텍스트 프롬프트에서 과학자 이름을 사용합니다. 텍스트 전용 방법은 피험자가 훈련 데이터 세트에 있는 경우에만 성능이 좋지만 그렇지 않으면 신원을 유지하는 데 어려움을 겪습니다. 미세 조정 기반 방법은 다른 사람의 신원을 혼합하고(TI 행 1 및 2, CD 행 1, 2, 4), 텍스트 지침에서 벗어나 단일 피험자만 생성하고(TI 행 4), 특정 참조와 유사하지 않은 이미지를 생성합니다(CD 행 3). 문제 1: 아이덴티티 블렌딩 &quot;공원에 앉아 있는 한 남자와 한 남자&quot; 문제 2: 피험자 과적합 &quot;말을 타고 있는 한 여자&quot; 교차 주의가 없는 입력과 교차 주의 국소화 국소화(저희) 지연된 피험자가 없는 입력과 지연된 피험자 조절 조절(저희) 그림 2: 기존의 피험자 중심 이미지 생성 방법이 직면한 두 가지 과제. 첫째, 현재 방법은 오른쪽 그림에서 뉴턴이 아인슈타인과 닮은 모양으로 표시된 다른 피험자의 고유한 특성을 블렌딩합니다(아이덴티티 블렌딩). 교차 주의 국소화(4.2절)는 이 문제를 해결합니다. 둘째, 피험자 과적합으로 인해 입력 이미지에 과적합하고 텍스트 지시를 무시합니다. 지연된 피험자 조절(4.3절)은 이 문제를 해결합니다. 텍스트 조절에서 개인의 고유한 아이덴티티. 비전 인코더를 사용하여 참조된 이미지에서 이 아이덴티티 임베딩을 유도한 다음 이 아이덴티티 임베딩의 특징으로 일반 텍스트 토큰을 증강합니다. 이를 통해 피험자 증강 조절을 기반으로 이미지를 생성할 수 있습니다. 우리의 설계는 전방 패스만으로 지정된 피험자를 특징으로 하는 이미지를 생성할 수 있게 하며, 배포 효율성을 높이기 위해 모델 압축 기술[2, 11, 40]과 더욱 통합될 수 있습니다.다중 피험자 신원 혼합 문제를 해결하기 위해, 우리는 규제되지 않은 교차 주의를 주된 이유로 식별합니다(그림 4).텍스트에 두 개의 &quot;사람&quot; 토큰이 포함되어 있는 경우, 각 토큰의 주의 맵은 각 토큰을 이미지의 개별 사람과 연결하는 대신 이미지의 두 사람 모두에 주의를 기울입니다.이를 해결하기 위해, 우리는 표준 분할 도구[7]를 사용하여 훈련 중에 분할 마스크가 있는 피험자의 교차 주의 맵을 감독하는 것을 제안합니다.이 감독은 모델이 피험자 특징을 이미지의 개별적이고 겹치지 않는 영역에 매핑하도록 명시적으로 안내하여 고품질 다중 피험자 이미지를 생성하는 것을 용이하게 합니다(그림 2 왼쪽).우리는 분할과 교차 주의 국소화가 훈련 단계에서만 필요하다는 점에 주목합니다. 순진하게 주제 증강 컨디셔닝을 적용하면 주제 과적합(오른쪽 그림 2)이 발생하여 텍스트 지시에 따라 주제를 편집하는 사용자의 기능이 제한됩니다. 이를 해결하기 위해 텍스트 지시를 따르는 동안 주제의 정체성을 보존하는 지연 주제 컨디셔닝을 도입했습니다. 초기 노이즈 제거 단계에서 텍스트 전용 컨디셔닝을 사용하여 이미지 레이아웃을 생성한 다음 나머지 노이즈 제거 단계에서 주제 증강 컨디셔닝을 사용하여 주제 모양을 정제합니다. 이 간단한 기술은 편집 가능성을 희생하지 않고도 주제 정체성을 효과적으로 보존합니다(그림 5). FastComposer는 처음으로 다양한 시나리오에서 다중 주제 이미지의 추론 전용 생성을 가능하게 합니다(그림 1). FastComposer는 미세 조정 기반 방법에 비해 300~2500배의 속도 향상과 2.8~6.7배의 메모리 절약을 달성하여 새 주제에 대한 추가 저장 공간이 전혀 필요하지 않습니다. FastComposer는 저렴하고 개인화되고 다재다능한 텍스트-이미지 생성의 길을 열어줍니다. 2
--- RELATED WORK ---
주제 중심 이미지 생성은 초기 훈련 단계에서 특정 주제를 보이지 않게 렌더링하는 것을 목표로 합니다. 주제의 제한된 수의 예시 이미지가 주어지면 다양한 맥락에서 새로운 표현을 합성하려고 합니다. DreamBooth[32], textual-inversion[9] 및 custom-diffusion[20]은 최적화 기반을 사용합니다.
--- METHOD ---
s는 계산 집약적이고 효율적인 배포를 방해하는 주제별 미세 조정으로 인해 비효율적입니다. 게다가 기존 방법은 종종 주제 간의 정체성을 혼합하기 때문에 다중 주제 생성에 어려움을 겪습니다. 미세 조정 없이 효율적이고 개인화된 다중 주제 텍스트-이미지 생성을 가능하게 하는 FastComposer를 제시합니다. FastComposer는 이미지 인코더에서 추출한 주제 임베딩을 사용하여 확산 모델의 일반 텍스트 컨디셔닝을 증강하여 전방 패스만으로 주제 이미지와 텍스트 지침을 기반으로 개인화된 이미지 생성을 가능하게 합니다. 다중 주제 생성에서 정체성 혼합 문제를 해결하기 위해 FastComposer는 훈련 중에 교차 주의 국소화 감독을 제안하여 대상 이미지의 올바른 영역에 국소화된 참조 주제의 주의를 강제합니다. 주제 임베딩에 대한 순진한 컨디셔닝은 주제 과적합으로 이어집니다. FastComposer는 주제 중심 이미지 생성에서 정체성과 편집 가능성을 모두 유지하기 위해 노이즈 제거 단계에서 지연된 주제 컨디셔닝을 제안합니다. FastComposer는 서로 다른 스타일, 동작 및 맥락을 가진 여러 보이지 않는 개인의 이미지를 생성합니다. 미세 조정 기반 방법에 비해 300~2500배의 속도 향상을 달성하며 새로운 피사체에 대한 추가 저장 공간이 전혀 필요하지 않습니다.FastComposer는 효율적이고 개인화된 고품질의 다중 피사체 이미지 생성을 위한 길을 열어줍니다.코드, 모델 및 데이터 세트는 재생산을 위해 공개됩니다.서론 텍스트-이미지 생성[4, 8, 17, 28]의 최근 발전, 특히 확산 모델[13, 27, 31, 36, 37]은 콘텐츠 생성의 새로운 지평을 열었습니다.주체 중심 텍스트-이미지 생성은 몇 가지 샘플 이미지[3, 9, 20, 25, 32]가 주어진 경우 새로운 개인에게 개인화를 허용하여 새로운 장면, 스타일 및 동작에서 특정 피사체가 등장하는 이미지를 생성할 수 있습니다.그러나 현재의 주체 중심 텍스트-이미지 생성 방법은 개인화 비용과 여러 피사체에 대한 아이덴티티 블렌딩이라는 두 가지 주요 한계가 있습니다. 개인화는 최상의 충실도를 위해 종종 새로운 주제마다 모델 미세 조정이 필요하기 때문에 비용이 많이 듭니다. 모델 조정으로 인해 발생하는 계산 오버헤드와 높은 하드웨어 요구 사항은 주로 메모리 소비[6]와 역전파 계산으로 인해 발생하며, 이러한 모델의 다양한 플랫폼 간 적용성을 제한합니다. 또한 기존 기술은 모델이 다른 주제의 고유한 특성(주제 A가 주제 B처럼 보이고 그 반대의 경우도 마찬가지)을 결합하는 &quot;신원 혼합&quot; 문제(왼쪽 그림 2)로 인해 다중 주제 생성(그림 1)에 어려움을 겪습니다. 우리는 조정이 필요 없는 개인화된 다중 주제 텍스트-이미지 생성 방법인 FastComposer를 제안합니다. 우리의 핵심 아이디어는 &quot;사람&quot;과 같은 일반적인 단어 토큰을 * 동등한 기여를 포착하는 임베딩으로 대체하는 것입니다. 문의처: Guangxuan Xiao<xgx@mit.edu> , 천위인<tianweiy@mit.edu> . 사전 인쇄본. 검토 중. 참고 문헌 및 해변에 앉아 있는 남자와 남자와 여자가 있는 공원과 함께 그리고 여자 여자와 남자의 일본 목판화 남자와 여자와 남자의 am 여자 StableDiffusion(텍스트만) Midjourney(텍스트만) Textual Inversion CustomDiffusion(미세 조정)(미세 조정) FastComposer(추론만) 그림 1: 다중 피험자 이미지 생성을 위한 기준선과의 비교. 텍스트 전용 방법(SD, MJ)의 텍스트 프롬프트에서 과학자 이름을 사용합니다. 텍스트 전용 방법은 피험자가 훈련 데이터 세트에 있는 경우에만 성능이 좋지만 그렇지 않으면 신원을 유지하는 데 어려움을 겪습니다. 미세 조정 기반 방법은 다른 사람의 신원을 혼합하고(TI 행 1 및 2, CD 행 1, 2, 4), 텍스트 지침에서 벗어나 단일 피험자만 생성하고(TI 행 4), 특정 참조와 유사하지 않은 이미지를 생성합니다(CD 행 3). 문제 1: 아이덴티티 블렌딩 &quot;공원에 앉아 있는 한 남자와 한 남자&quot; 문제 2: 피험자 과적합 &quot;말을 타고 있는 한 여자&quot; 교차 주의가 없는 입력과 교차 주의 국소화 국소화(저희) 지연된 피험자가 없는 입력과 지연된 피험자 조절 조절(저희) 그림 2: 기존의 피험자 중심 이미지 생성 방법이 직면한 두 가지 과제. 첫째, 현재 방법은 오른쪽 그림에서 뉴턴이 아인슈타인과 닮은 모양으로 표시된 다른 피험자의 고유한 특성을 블렌딩합니다(아이덴티티 블렌딩). 교차 주의 국소화(4.2절)는 이 문제를 해결합니다. 둘째, 피험자 과적합으로 인해 입력 이미지에 과적합하고 텍스트 지시를 무시합니다. 지연된 피험자 조절(4.3절)은 이 문제를 해결합니다. 텍스트 조절에서 개인의 고유한 아이덴티티. 비전 인코더를 사용하여 참조된 이미지에서 이 아이덴티티 임베딩을 유도한 다음 이 아이덴티티 임베딩의 특징으로 일반 텍스트 토큰을 증강합니다. 이를 통해 피험자 증강 조절을 기반으로 이미지를 생성할 수 있습니다. 우리의 설계는 전방 패스만으로 지정된 피험자를 특징으로 하는 이미지를 생성할 수 있게 하며, 배포 효율성을 높이기 위해 모델 압축 기술[2, 11, 40]과 더욱 통합될 수 있습니다.다중 피험자 신원 혼합 문제를 해결하기 위해, 우리는 규제되지 않은 교차 주의를 주된 이유로 식별합니다(그림 4).텍스트에 두 개의 &quot;사람&quot; 토큰이 포함되어 있는 경우, 각 토큰의 주의 맵은 각 토큰을 이미지의 개별 사람과 연결하는 대신 이미지의 두 사람 모두에 주의를 기울입니다.이를 해결하기 위해, 우리는 표준 분할 도구[7]를 사용하여 훈련 중에 분할 마스크가 있는 피험자의 교차 주의 맵을 감독하는 것을 제안합니다.이 감독은 모델이 피험자 특징을 이미지의 개별적이고 겹치지 않는 영역에 매핑하도록 명시적으로 안내하여 고품질 다중 피험자 이미지를 생성하는 것을 용이하게 합니다(그림 2 왼쪽).우리는 분할과 교차 주의 국소화가 훈련 단계에서만 필요하다는 점에 주목합니다. 순진하게 주제 증강 컨디셔닝을 적용하면 주제 과적합(오른쪽 그림 2)이 발생하여 텍스트 지시에 따라 주제를 편집하는 사용자의 기능이 제한됩니다. 이를 해결하기 위해 텍스트 지시를 따르는 동안 주제의 정체성을 보존하는 지연 주제 컨디셔닝을 도입했습니다. 이는 초기 노이즈 제거 단계에서 텍스트 전용 컨디셔닝을 사용하여 이미지 레이아웃을 생성한 다음 나머지 노이즈 제거 단계에서 주제 증강 컨디셔닝을 사용하여 주제 모양을 정제합니다. 이 간단한 기술은 편집 가능성을 희생하지 않고도 주제의 정체성을 효과적으로 보존합니다(그림 5). FastComposer는 처음으로 다양한 시나리오에서 다중 주제 이미지의 추론 전용 생성을 가능하게 합니다(그림 1). FastComposer는 미세 조정 기반 방법에 비해 300~2500배의 속도 향상과 2.8~6.7배의 메모리 절약을 달성하여 새로운 주제에 대한 추가 저장 공간이 전혀 필요하지 않습니다. FastComposer는 저렴하고 개인화되고 다재다능한 텍스트-이미지 생성의 길을 열어줍니다. 2 관련 연구 주제 중심 이미지 생성은 초기 훈련 단계에서 특정 주제를 보이지 않게 렌더링하는 것을 목표로 합니다. 주제의 제한된 수의 예제 이미지가 주어지면 다양한 맥락에서 새로운 렌더링을 합성하려고 합니다. DreamBooth[32], textual-inversion[9], custom-diffusion[20]은 최적화 기반 방법을 사용하여 주제를 확산 모델에 포함합니다. 이는 모델 가중치를 미세 조정[20, 32]하거나 주제 이미지를 주제 신원을 인코딩하는 텍스트 토큰으로 반전하여 달성됩니다[9]. 최근 tuning-encoder[30]는 사전 훈련된 인코더를 사용하여 반전된 세트 잠재 코드를 먼저 생성한 다음 여러 미세 조정 단계를 통해 이러한 코드를 정제하여 주제 신원을 더 잘 보존함으로써 미세 조정 단계의 총 수를 줄입니다. 그러나 이러한 모든 튜닝 기반 방법[9, 10, 20, 32]은 리소스 집약적인 역전파가 필요하며, 하드웨어는 모델을 미세 조정할 수 있어야 하는데, 이는 스마트폰과 같은 에지 장치에서는 실행 가능하지 않고 클라우드 기반 애플리케이션으로 확장할 수도 없습니다.반대로, 새로운 FastComposer는 훈련 단계에서 비용이 많이 드는 주제 튜닝을 상각하여 테스트 시간에 간단한 피드포워드 방법을 사용하여 여러 주제를 즉시 개인화할 수 있습니다.동시에 수행되는 여러 연구에서 튜닝이 필요 없는 방법을 탐구했습니다.X&amp;Fuse[19]는 이미지 컨디셔닝을 위해 참조 이미지와 노이즈가 있는 잠재 이미지를 연결합니다.ELITE[39]와 InstantBooth[35]는 전역 및 지역 매핑 네트워크를 사용하여 참조 이미지를 워드 임베딩에 투사하고 참조 이미지 패치 기능을 교차 주의 계층에 주입하여 지역 세부 정보를 향상시킵니다.단일 객체 사용자 정의에 대한 인상적인 결과에도 불구하고, 생성된 이미지와 참조 입력 이미지 간의 전역 상호 작용에 의존하기 때문에 아키텍처 설계로 인해 여러 주제 설정에 적용 가능성이 제한됩니다. UMM-Diffusion[24]은 저희와 유사한 아키텍처를 공유합니다.그러나 여러 주체로 확장하면 신원 혼합 문제에 직면합니다[24].이에 비해 저희 방법은 교차 주의 국소화 감독 메커니즘(4.2절)을 통해 다중 주체 구성을 지원합니다.다중 주체 이미지 생성.Custom-Diffusion[20]은 다중 개념에 대한 확산 모델을 공동으로 미세 조정하여 다중 개념 구성을 가능하게 합니다.그러나 일반적으로 동물과 관련된 액세서리 또는 배경과 같이 의미적 구분이 명확한 개념을 처리합니다.이 방법은 유사한 범주 내의 주체를 처리할 때 문제에 부딪히며, 종종 두 명의 다른 개인을 구성할 때 같은 사람을 두 번 생성합니다(그림 1).SpaText[1] 및 Collage Diffusion[33]은 레이아웃에서 이미지 생성 프로세스를 통해 다중 객체 구성을 가능하게 합니다.사용자가 제공한 분할 마스크는 최종 레이아웃을 결정한 다음 확산 모델을 사용하여 고해상도 이미지로 변환합니다. 그럼에도 불구하고, 이러한 기술은 사용자 정의 없이 일반 객체를 구성하거나 [1] 인스턴스별 세부 정보를 인코딩하기 위해 비용이 많이 드는 텍스트 반전 프로세스를 요구합니다 [33].또한 이러한 기술은 사용자가 제공한 분할 맵이 필요합니다.반대로 FastComposer는 추론 전용 방식으로 개인화된 다중 주제 이미지를 생성하고 텍스트 프롬프트에서 자동으로 그럴듯한 레이아웃을 파생합니다.3 예비 안정 확산.최신 StableDiffusion(SD) 모델을 백본 네트워크로 사용합니다.SD 모델은 세 가지 구성 요소로 구성됩니다.변형 자동 인코더(VAE), U-Net 및 텍스트 인코더.VAE 인코더&amp;는 이미지 x를 더 작은 잠재 표현 z로 압축하고, 이는 이후 전방 확산 프로세스에서 가우시안 노이즈 ɛ에 의해 교란됩니다.0으로 매개변수화된 U-Net은 노이즈를 예측하여 노이즈가 있는 잠재 표현의 노이즈를 제거합니다. 이러한 잡음 제거 과정은 교차 주의 메커니즘을 통해 텍스트 프롬프트에 따라 달라질 수 있으며, 텍스트 인코더는 텍스트 프롬프트 P를 조건부 임베딩(P)에 매핑합니다. 학습하는 동안 네트워크는 학습 텍스트-이미지 쌍 텍스트 임베딩 &quot;함께 서 있는 여성과 남성&quot; 텍스트 인코더 woman concat 및 Š MLP .............. 추론 주체 임베딩 임베딩 조건부 텍스트 프롬프트 &quot;공원에 앉아 있는 남성과 남성&quot; 텍스트 인코더 참조 주체 이미지 인코더 a man concat 함께 서 있는 주체 세그먼트 이미지 인코더 노이즈가 있는 이미지 노이즈 제거된 이미지 노이즈 분할 마스크 U-Net 교차 주의 맵 여성과 남성이 함께 서 있는 -11-17--concat MLP concat 지연된 주체 조건화(4.3절) t≤aT t&gt; aT U-Net + 생성된 이미지 교차 주의 국소화(4.2절) XT хо 그림 3: FastComposer의 학습 및 추론 파이프라인. 텍스트 설명과 여러 주체의 이미지가 주어지면 FastComposer는 이미지 인코더를 사용하여 주체의 특징을 추출하고 해당 텍스트 토큰을 증강합니다. 확산 모델은 증강된 컨디셔닝을 사용하여 다중 피험자 이미지를 생성하도록 훈련됩니다. 교차 어텐션 로컬라이제이션(4.2절)을 사용하여 다중 피험자 생성 품질을 높이고 지연된 피험자 컨디셔닝을 사용하여 피험자 과적합을 방지합니다(4.3절). 아래 방정식으로 주어진 손실 함수를 최소화하도록 최적화되었습니다. Lnoise = Ez~ε(x), P, E~N(0,1),t [||E — Eo (Zt, t, v(P))||{}], (1) 여기서 zt는 시간 단계 t에서의 잠재 코드입니다. 추론 시간에 랜덤 노이즈 z가 N(0, 1)에서 샘플링되고 U-Net에서 초기 잠재 표현 zo로 반복적으로 노이즈가 제거됩니다. 마지막으로 VAE 디코더 D는 잠재 코드를 픽셀 공간 î = D(zo)로 다시 매핑하여 최종 이미지를 생성합니다. = 교차 어텐션 메커니즘을 통한 텍스트 컨디셔닝. SD 모델에서 U-Net은 교차 어텐션 메커니즘을 사용하여 텍스트 프롬프트에 조건화된 잠재 코드의 노이즈를 제거합니다.간단하게 설명하기 위해 논의에서 단일 헤드 어텐션 메커니즘을 사용합니다.P가 텍스트 프롬프트를 나타내고 텍스트 인코더를 나타내며, 이는 일반적으로 사전 학습된 CLIP 텍스트 인코더입니다.인코더는 P를 d차원 임베딩 목록, &amp;(P) = c = Rnd로 변환합니다.교차 어텐션 계층은 공간 잠재 코드 zЄR(hxw)×f와 텍스트 임베딩 c를 입력으로 허용합니다.그런 다음 잠재 코드와 텍스트 임베딩을 쿼리, 키 및 값 행렬로 투영합니다.Q = W₁z, K Wkc 및 V = Wc.여기서 WЄ Rfxd&#39;, Wk, W₁ = Rdxd&#39;는 세 선형 계층의 가중치 행렬을 나타내고 d&#39;는 쿼리, 키 및 값 임베딩의 차원입니다. 교차 어텐션 레이어는 어텐션 점수 A QKT Softmax(· -) Є [0, 1](h×w)×n을 계산하고, 값 행렬에 대한 가중 합을 구하여 교차 어텐션 출력 Zattn = AVER(hxwxd&#39;를 구합니다. 직관적으로, 교차 어텐션 메커니즘은 텍스트 정보를 2D 잠재 코드 공간으로 &quot;분산&quot;하고, A[i, j, k]는 k번째 텍스트 토큰에서 (i, j) 잠재 픽셀로 흐르는 정보의 양을 나타냅니다. 저희의 방법은 교차 어텐션 맵의 이러한 의미적 해석에 기반하며, 섹션 4.2에서 자세히 논의할 것입니다. 4 FastComposer = √d&#39; 4.1 이미지 인코더를 사용한 튜닝 없는 주체 중심 이미지 생성 = 주체 임베딩을 사용한 텍스트 표현 증강. 튜닝 없는 주체 중심 이미지 생성을 달성하기 위해, 참조 주체 이미지에서 추출한 시각적 특징으로 텍스트 프롬프트를 증강하는 것을 제안합니다. 텍스트 프롬프트 P가 주어지면, {w1, W2,... Wn}, 참조 주제 이미지 목록 S = {81, 82, . . . Sm}, 텍스트 프롬프트에서 어떤 주제가 어떤 단어에 해당하는지 나타내는 인덱스 목록 I = {i1, 12, ... im}, ij Є 1, 2, . . ., n, 먼저 텍스트 프롬프트 P와 참조 주제 S를 사전 학습된 CLIP 텍스트 및 이미지 인코더 &amp; 및 6을 사용하여 임베딩으로 인코딩합니다. 다음으로 다층 퍼셉트론(MLP)을 사용하여 참조 주제에서 추출한 시각적 특징으로 텍스트 임베딩을 증강합니다. 단어 임베딩을 시각적 특징으로 연결하고 그 결과로 증강된 임베딩을 MLP에 공급합니다. 이 과정은 최종 컨디셔닝을 제공합니다.공원에 앉아 있는 남자와 남자 교차 주의 국소화 없이 생성된 이미지 교차 주의 국소화(저희의 이미지) 그림 4: 교차 주의 정규화가 없는 경우(위), 확산 모델은 여러 주체의 입력 토큰에 주의를 기울이고 이들의 정체성을 병합합니다.교차 주의 정규화를 적용함으로써(아래), 확산 모델은 주체를 생성하는 동안 하나의 참조 토큰에만 집중하도록 학습합니다.이렇게 하면 생성된 이미지에서 여러 주체의 특징이 더 분리됩니다.임베딩 c&#39; E Rnxd는 다음과 같이 정의됩니다.c&#39;i = Sv (P)i, i &amp; I [MLP(v(P)i||0(s;)), i = i; ЄI 그림 3은 증강 접근 방식의 구체적인 예를 보여줍니다.(2) 주체 주도 이미지 생성 학습.추론 전용 주체 주도 이미지 생성을 가능하게 하기 위해 노이즈 제거 손실로 이미지 인코더, MLP 모듈 및 U-Net을 학습합니다(그림 3). 우리는 모델을 훈련하기 위해 주체 증강 이미지-텍스트 쌍 데이터 세트를 생성하는데, 여기서 이미지 캡션의 명사 구는 대상 이미지에 나타나는 주체 세그먼트와 쌍을 이룹니다. 우리는 처음에 종속성 구문 분석 모델을 사용하여 이미지 캡션의 모든 명사 구(예: &quot;여성&quot;)를 청크로 나누고 파노라마 분할 모델을 사용하여 이미지에 있는 모든 주체를 분할합니다. 그런 다음 텍스트와 이미지 유사성을 기반으로 하는 탐욕적 매칭 알고리즘[26, 29]을 사용하여 이러한 주체 세그먼트를 캡션의 해당 명사 구와 쌍으로 만듭니다. 주체 증강 이미지-텍스트 데이터 세트를 구성하는 프로세스는 5.1절에 자세히 설명되어 있습니다. 훈련 단계에서 우리는 방정식 2에 설명된 대로 주체 증강 컨디셔닝을 사용하여 교란된 대상 이미지의 노이즈를 제거합니다. 또한 인코딩하기 전에 주체의 배경을 랜덤 노이즈로 마스크하여 주체의 배경이 과적합되는 것을 방지합니다. 결과적으로 FastComposer는 명시적인 배경 분할 없이 추론 중에 자연스러운 주체 이미지를 직접 사용할 수 있습니다. 4.2 주제 분할 마스크를 사용한 교차 주의 맵 국소화 우리는 전통적인 교차 주의 맵이 모든 주제에 동시에 주의를 기울이는 경향이 있음을 관찰했으며, 이는 다중 주제 이미지 생성에서 정체성 혼합으로 이어진다(그림 4 위). 우리는 이 문제를 해결하기 위해 훈련 중에 주제 분할 마스크를 사용하여 교차 주의 맵을 국소화하는 것을 제안한다. 확산 모델에서의 정체성 혼합 이해. 이전 연구[12]에 따르면 확산 모델 내의 교차 주의 메커니즘이 생성된 이미지의 레이아웃을 제어한다. 교차 주의 맵의 점수는 &quot;텍스트 토큰에서 잠재 픽셀로 흐르는 정보의 양&quot;을 나타냅니다. 우리는 단일 잠재 픽셀이 모든 텍스트 토큰에 주의를 기울일 수 있기 때문에 제한 없는 교차 주의 메커니즘에서 정체성 블렌딩이 발생한다고 가정합니다. 한 피험자의 영역이 여러 참조 피험자에 주의를 기울이면 정체성 블렌딩이 발생합니다. 그림 4에서 확산 모델의 U-Net 내에서 평균 교차 주의 맵을 시각화하여 가설을 확인합니다. 정규화되지 않은 모델은 종종 두 개의 참조 피험자 토큰이 동시에 동일한 생성된 사람에게 영향을 미쳐 두 피험자의 특징이 혼합되는 경우가 많습니다. 우리는 적절한 교차 주의 맵이 대상 이미지의 인스턴스 분할과 유사해야 하며, 다른 피험자와 관련된 특징을 명확하게 분리해야 한다고 주장합니다. 이를 달성하기 위해 훈련 중에 피험자 교차 주의 맵에 정규화 항을 추가하여 특정 인스턴스 영역에 집중하도록 장려합니다. 분할 맵과 교차 주의 정규화는 테스트 시간이 아닌 훈련 중에만 사용됩니다. 프롬프트 일관성 ● 편집 가능성 ✰ 정체성 &quot;그림 영어: 남자와 남자의 Vincent van Gogh 스타일의&quot; 28% 64% ° 21% 48% 14% 우리의 선택 32% 7% 더 많은 편집 더 많은 16% 유사한 정체성 보존 &quot;말을 타는 여자&quot; 0% 0% 0 0.2 0.4 0.6 0.8α =a = 0.α = 0.a = 0.a = 0.a = 1.주체 조건화 비율 a 주체 조건화 비율 a 그림 5: 주체 조건화를 위해 다른 비율의 타임스텝을 사용하는 효과. 0.6~0.8 사이의 비율은 좋은 결과를 가져오고 신속한 일관성과 정체성 보존 간의 균형을 이룹니다. 분할 마스크를 사용한 교차 주의 지역화. 3장에서 논의했듯이 교차 주의 맵 A Є [0,1] (hxw)×n은 각 계층의 잠재 픽셀을 조건부 임베딩에 연결합니다. 여기서 A[i, j, k]는 k번째 조건부 토큰에서 (i, j) 잠재 픽셀로의 정보 흐름을 나타냅니다. 이상적으로는 주체 토큰의 주의 맵은 전체 이미지에 퍼지는 것이 아니라 주체 영역에만 초점을 맞춰야 하며 주체 간의 동일성 혼합을 방지해야 합니다. 이를 달성하기 위해 참조 주체의 분할 마스크를 사용하여 교차 주의 맵을 지역화하는 것을 제안합니다. M = {M1, M2,... Mm}이 참조 주체의 분할 마스크를 나타내고 I = {i1, i2, . . . im}이 텍스트 프롬프트의 각 단어에 해당하는 주체를 나타내는 인덱스 목록이고 Ai : A[:, :, i] = [0, 1](h×w)가 i번째 주체 토큰의 교차 주의 맵이라고 합니다. 우리는 교차 어텐션 맵 Ai를 감독하여 j번째 주체 토큰의 분할 마스크 m;, 즉 Ai, mj에 가깝게 합니다. 우리는 균형 잡힌 L1 손실을 사용하여 교차 어텐션 맵과 분할 마스크 간의 거리를 최소화합니다: = mLloc(mean(A¿, [m;]) – mean(A¿, [m;])). mj=(3) FastComposer의 최종 학습 목표는 다음과 같습니다: L = Lnoise Lloc, (4) 하이퍼파라미터 &gt; = 0.001로 제어되는 로컬라이제이션 손실 비율을 사용합니다. [5, 12]에서 영감을 받아 로컬라이제이션 손실을 다운샘플링된 교차 어텐션 맵, 즉 더 많은 의미적 정보를 포함하는 것으로 알려진 U-Net의 중간 5개 블록에 적용합니다. 그림 4에서 볼 수 있듯이, 우리의 로컬라이제이션 기술을 사용하면 모델이 테스트 시간에 참조 주체에 주의를 정확하게 할당할 수 있으므로 주체 간의 동일성 혼합이 방지됩니다. 4.3 반복적 노이즈 제거에서의 지연된 주체 조건화 추론 중에 증강된 텍스트 표현을 직접 사용하면 텍스트 지시를 무시하면서 주체와 매우 유사한 이미지가 생성되는 경우가 많습니다. 이는 이미지 레이아웃이 노이즈 제거 프로세스의 초기 단계에서 형성되고 참조 이미지에서 조기에 증강하면 결과 이미지가 텍스트 지시에서 벗어나기 때문입니다. 이전 방법[10, 30]은 초기 잠재 코드를 생성하고 반복적 모델 미세 조정을 통해 이를 정제하여 이 문제를 완화합니다. 그러나 이 프로세스는 리소스 집약적이며 모델 미세 조정을 위해 고급 장치가 필요합니다. Style Mixing[18]에서 영감을 얻어 간단한 지연된 주체 조건화를 제안합니다. 이를 통해 추론 전용 주체 조건화가 가능하면서도 ID 보존과 편집 가능성 간의 균형을 이룹니다. 구체적으로, 텍스트 전용 프롬프트를 사용하여 레이아웃을 만든 후에만 이미지 증강을 수행합니다. 이 프레임워크에서, 우리의 시간 의존적 노이즈 예측 모델은 다음과 같이 표현될 수 있다. Єt = Jo(zt, t, c) if t&gt; aT, [co (zt, t, c&#39;) else (5) 여기서, c는 원래 텍스트 임베딩을 나타내고 c&#39;는 입력 이미지 임베딩으로 증강된 텍스트 임베딩을 나타낸다. a는 주제 컨디셔닝의 비율을 나타내는 하이퍼파라미터이다. 그림 5에서 다른 a를 사용하는 효과를 제거한다. 경험적으로, a = [0.6, 0.8]은 프롬프트 일관성과 정체성 보존의 균형을 이루는 좋은 결과를 산출하지만, 특정 인스턴스에 맞게 쉽게 조정할 수 있다. 보라색 마법사 의상을 입은 남자 점묘주의 그림 반 고흐 스타일의 여자를 그린 남자 그림 해변의 남자 눈 속의 남자 산타 모자를 쓴 여자 그림 6: 단일 주제 이미지 생성에 대한 다양한 방법 비교. 텍스트 전용 방법(예: StableDiffusion 및 Midjourney)의 경우 텍스트 프롬프트에 과학자의 이름을 사용합니다. 5
--- EXPERIMENT ---
s 5.1 설정 데이터 세트 구축. 우리는 모델을 훈련하기 위해 FFHQ-wild [18] 데이터 세트를 기반으로 한 주제 증강 이미지-텍스트 쌍 데이터 세트를 구축했습니다. 먼저, BLIP-2 모델 [21] blip2-opt-6.7b-coco를 사용하여 모든 이미지에 대한 캡션을 생성합니다. 다음으로, Mask2Former 모델 [7] mask2former-swin-large-coco-panoptic을 사용하여 각 이미지에 대한 파노옵틱 분할 마스크를 생성합니다. 그런 다음 spaCy [15] 라이브러리를 활용하여 이미지 캡션의 모든 명사 구를 청크로 나누고 번호가 매겨진 복수 구(예: &quot;two women&quot;)를 &quot;and&quot;로 연결된 단수 구(예: &quot;a woman and a woman&quot;)로 확장합니다. 마지막으로, 탐욕적 매칭 알고리즘을 사용하여 명사 구를 이미지 세그먼트와 매칭합니다. 우리는 OpenCLIP 모델 [16] CLIP-ViT-H-14-laion2B-s32B-b79K에 의한 이미지-텍스트 유사도 점수와 Sentence-Transformer [29] 모델 st sb-mpnet-base-v2에 의한 레이블-텍스트 유사도 점수의 곱을 고려하여 이를 수행합니다. 우리는 검증 및 테스트 목적으로 1000개의 이미지를 예약합니다. 참고문헌 말을 타는 여성 와인잔을 든 남성 Stable Diffusion(텍스트만) Midjourney(텍스트만) Textual Inversion(미세 조정) DreamBooth(미세 조정) CustomDiffusion(미세 조정) FastComposer(추론만) 표 1: 단일 피험자 이미지 생성에 대한 우리 방법과 기준 접근 방식 간의 비교. StableDiffusion은 피험자 조건화 없이 텍스트만 기준선으로 사용되었습니다. 방법 StableDiffusion ImagesIdentity Preservation ↑ Prompt Consistency ↑ Total Time ↓ Peak Memory↓↓ 3.85% 26.79% 2초 6GB Textual-Inversion DreamBooth29.26% 21.91% 2500초 17GB 27.27% 23.91% 1084초 40GB Custom Diffusion43.37% FastComposer51.41% 23.29% 24.30% 789초 29GB 2초 6GB 학습 세부 정보. StableDiffusion v1-5[31] 모델에서 학습을 시작합니다. 시각적 입력을 인코딩하기 위해 SDv1-5의 텍스트 인코더의 파트너 모델 역할을 하는 OpenAI의 clip-vit-large-patch14 비전 모델을 사용합니다. 학습하는 동안 텍스트 인코더를 정지시키고 U-Net, MLP 모듈, 비전 인코더의 마지막 두 개의 변환기 블록만 학습시킵니다. 1e-5의 일정한 학습 속도와 128의 배치 크기를 사용하여 8개의 NVIDIA A6000 GPU에서 150k 단계로 모델을 학습시킵니다. COCO [22] 레이블이 &quot;사람&quot;인 세그먼트만 증가시키고 학습하는 동안 최대 참조 피험자를 설정하며 각 피험자는 10%의 확률로 삭제됩니다. 모델의 텍스트 전용 생성 기능을 유지하기 위해 샘플의 10%로 텍스트 컨디셔닝만으로 모델을 학습시킵니다. 분류기 없는 안내 샘플링[14]을 용이하게 하기 위해 인스턴스의 10%에서 조건 없이 모델을 학습시킵니다. 학습하는 동안 피험자 영역에서만 손실을 학습 샘플의 절반에 적용하여 피험자 영역의 생성 품질을 향상시킵니다. 평가 지표 신원 보존 및 신속한 일관성에 대한 이미지 생성 품질을 평가합니다. 신원 보존은 MTCNN[41]을 사용하여 참조 및 생성된 이미지에서 얼굴을 감지한 다음 FaceNet[34]을 사용하여 쌍별 신원 유사도를 계산하여 결정됩니다. 다중 피험자 평가의 경우 생성된 이미지 내의 모든 얼굴을 식별하고 생성된 얼굴과 참조 피험자 간에 탐욕적 매칭 절차를 사용합니다. 모든 피험자 간의 최소 유사도 값은 전반적인 신원 보존을 측정합니다. 텍스트 반전[9]에 따른 평균 CLIP-L/14 이미지-텍스트 유사도를 사용하여 신속한 일관성을 평가합니다. 효율성 평가를 위해 미세 조정(조정 기반 방법의 경우) 및 추론을 포함한 사용자 지정에 대한 총 시간을 고려합니다. 또한 전체 절차 동안 최대 메모리 사용량을 측정합니다. 5.2 단일 피험자 이미지 생성 첫 번째 평가는 단일 피험자 이미지 생성의 성능을 목표로 합니다. 튜닝 없는 환경에서 게시된 기준선이 부족하다는 점을 감안하여 DreamBooth[32], Textual-Inversion[9], Custom Diffusion[20]을 포함한 선도적인 최적화 기반 접근 방식과 비교합니다. 우리는 디퓨저 라이브러리[38]의 구현을 사용합니다. 부록 섹션에 자세한 하이퍼파라미터를 제공합니다. Celeb-A 데이터 세트[23]에서 파생된 피험자에 대한 개인화된 콘텐츠를 생성하는 이러한 다양한 방법의 기능을 평가합니다. 평가 벤치마크를 구성하기 위해 재맥락화, 양식화, 액세서리화 및 다양한 동작과 같은 광범위한 시나리오를 캡슐화하는 광범위한 텍스트 프롬프트를 개발합니다. 전체 테스트 세트는 피험자로 구성되며 각각에 할당된 30개의 고유한 텍스트 프롬프트가 있습니다. 텍스트 프롬프트의 전체 목록은 부록에서 제공됩니다. 단일 참조 이미지를 사용할 때 이러한 방법이 과적합되고 단순히 참조 이미지를 재생성한다는 관찰을 바탕으로 최적화 기반 방법을 미세 조정하기 위해 피험자당 5개의 이미지를 사용했습니다. 반면, 당사 모델은 각 피험자에 대해 무작위로 선택된 단일 이미지를 사용합니다. 표 1에 표시된 FastComposer는 모든 기준선을 능가하여 뛰어난 ID 보존 및 프롬프트 일관성을 제공합니다. 놀랍게도 300~1200배의 속도 향상과 2.8배의 메모리 사용량 감소를 달성합니다. 그림 6은 다양한 프롬프트에 걸쳐 다양한 접근 방식을 사용하여 단일 피험자 개인화 비교의 정성적 결과를 보여줍니다. 중요한 점은, 우리 모델이 텍스트 전용 방법의 텍스트 일관성과 일치하고 단일 입력 및 전달 패스만 사용하여 신원 보존 측면에서 모든 기준 전략을 능가한다는 것입니다. 5.3 다중 피험자 이미지 생성 그런 다음 더 복잡한 설정인 다중 객체, 피험자 중심 이미지 생성을 고려합니다. 5.2절에 설명된 15명의 피험자로부터 형성된 모든 가능한 조합(총 105쌍)을 사용하여 다중 피험자 생성의 품질을 검토하고 각 쌍에 평가를 위한 21개의 프롬프트를 할당합니다. 표표 2: 다중 피험자 이미지 생성에 대한 우리 방법과 기준 접근 방식 비교. StableDiffusion은 피험자 조절 없이 텍스트 전용 기준선으로 사용되었습니다. 방법 StableDiffusion ImagesIdentity Preservation ↑ Prompt Consistency ↑ Total Time ↓ Peak Memory↓↓ 1.88% Textual-Inversion13.52% 28.44% 21.08% 2초 6GB 4998초 17GB Custom Diffusion5.37% 25.84% 789초 29GB FastComposer43.11% 24.25% 2초 6GB는 FastComposer를 기준 방법과 대조하는 정량적 분석을 보여줍니다. 최적화 기반 방법[9, 20, 32]은 신원 보존을 유지하는 데 자주 실패하며, 종종 일반적인 이미지나 다양한 참조 주제 간의 신원을 혼합한 이미지를 생성합니다. 반면 FastComposer는 다양한 주제의 고유한 특징을 보존하여 신원 보존 점수를 크게 향상시킵니다. 더욱이, 우리의 신원 일관성은 튜닝 기반 접근 방식[9, 20]과 동등합니다. 그림 1에 정성적 비교가 나와 있습니다. 3명의 피험자 이미지에 대한 더 많은 시각적 예가 그림 7에 나와 있습니다. &quot;함께 저녁을 먹는 여성과 여성과 여성&quot; &quot;함께 저녁을 먹는 남성과 남성과 남성&quot; 표 3: 교차 주의 국소화 감독에 대한 절제 연구. 교차 주의 국소화 없이 동일한 설정에서 훈련된 모델과 비교합니다. 방법 동일성 Pres. ↑ Prompt Cons. ↑ 그림 7: 3명의 피험자가 있는 이미지 생성. 5. 위치 없는 절제 연구 위치 없음(저희) 37.66% 43.11% 25.03% 24.25% 지연된 피험자 조건화. 그림 5는 지연된 피험자 조건화 접근 방식에서 하이퍼파라미터인 피험자 조건화에 할애된 시간 단계 비율을 변경하는 것의 영향을 보여줍니다. 이 비율이 증가함에 따라 모델은 동일성 보존이 향상되지만 편집 가능성이 손실됩니다. 0.6에서 0. 사이의 비율은 트레이드오프 곡선에서 유리한 균형을 이룹니다. 교차 주의 국소화 손실. 표 3은 제안된 교차 주의 국소화 손실에 대한 절제 연구를 제시합니다. 기준선은 동일한 설정에서 훈련되지만 국소화 손실은 제외합니다. 우리의 방법은 정체성 보존 점수의 상당한 향상을 보여줍니다. 그림은 정성적 비교를 보여줍니다. 국소화 손실을 통합하면 모델이 특정 참조 주제에 초점을 맞출 수 있으므로 정체성 블렌딩을 피할 수 있습니다. 6 논의 및
--- CONCLUSION ---
우리는 개인화된 다중 주제 텍스트-이미지 생성을 위한 튜닝 없는 방법인 FastComposer를 제안합니다. 우리는 사전 훈련된 비전 인코더를 사용하여 튜닝 없는 주제 중심 이미지 생성을 달성하여 이 프로세스를 다양한 플랫폼에서 효율적이고 접근 가능하게 만듭니다. FastComposer는 훈련 중에 분할 마스크로 교차 주의 맵을 감독하여 다중 주제 생성에서 신원 혼합 문제를 효과적으로 해결합니다. 우리는 또한 신원 보존과 이미지 편집의 유연성을 균형 있게 조정하기 위한 새로운 지연된 주제 컨디셔닝 기술을 제안합니다. 한계. 첫째, 현재의 훈련 세트는 FFHQ[18]로 작고 주로 인간 얼굴의 헤드샷을 포함합니다. 또한 사람 수에 대한 긴 꼬리 분포를 가지고 있어 세 명 이상의 주제가 있는 이미지를 생성하는 우리의 능력이 제한됩니다. 더 다양한 데이터 세트를 활용하면 FastComposer가 더 광범위한 동작과 시나리오를 생성할 수 있으므로 다재다능함과 적용성이 향상됩니다. 둘째, 우리의 작업은 동물과 같은 다른 주제를 특징으로 하는 대규모 다중 주제 데이터 세트가 부족하기 때문에 주로 인간 중심적입니다. 우리는 다른 범주의 다중 주제 이미지를 통합하여 데이터 세트를 확대하면 모델의 기능이 크게 강화될 것이라고 믿습니다. 마지막으로, Stable Diffusion과 FFHQ를 기반으로 구축된 모델은 이러한 편향도 상속받습니다. 감사의 말 이 연구는 MIT AI 하드웨어 프로그램, NVIDIA 학술 파트너십 어워드, MIT-IBM Watson AI 랩, Amazon 및 MIT Science Hub, Microsoft Turing 학술 프로그램, 싱가포르 DSTA(DST000ECI20300823(시각을 위한 새로운 표현), NSF 보조금 및 NSF CAREER 어워드 1943349의 지원을 받았습니다. 참고문헌 [1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin. Spatext: 제어 가능한 이미지 생성을 위한 공간-텍스트 표현. CVPR, 2023.[2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer 및 Judy Hoffman. 토큰 병합: ViT가 더 빠르지만 더 빠릅니다. 학습 표현에 관한 국제 컨퍼런스, 2023.[3] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal 및 Adriana Romero Soriano. 인스턴스 조건부 간. 신경 정보 처리 시스템의 발전, 34:27517-27529, 2021.[4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein 등 Muse: 마스크된 생성 변환기를 통한 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2301.00704, 2023.[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or. Attend-and-excite: 텍스트-이미지 확산 모델을 위한 주의 기반 의미적 안내. Siggraph, 2023.[6] Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin. 비선형 메모리 비용으로 딥 네트 학습, 2016.[7] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 Maskedattention 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1290-1299페이지, 2022. 2,[8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: 변환기를 통한 텍스트-이미지 생성 마스터링. 신경 정보 처리 시스템의 발전, 34:19822-19835, 2021.[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, Daniel Cohen-Or. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR, 2023. 1, 3, 8,[10] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, Daniel Cohen-Or. 텍스트-이미지 모델의 빠른 개인화를 위한 인코더 설계. Siggraph, 2023. 3,[11] Song Han, Huizi Mao, William J Dally. 심층 압축: 가지치기, 훈련된 양자화 및 허프만 코딩을 사용한 심층 신경망 압축. ICLR, 2016.[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 어텐션 제어를 사용한 프롬프트 간 이미지 편집. ICLR, 2023. 5,[13] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020.[14] Jonathan Ho 및 Tim Salimans. 분류자 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022.[15] Matthew Honnibal 및 Ines Montani. spaCy 2: Bloom 임베딩, 합성곱 신경망 및 증분 구문 분석을 통한 자연어 이해. 2017년에 게재될 예정입니다.[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi 및 Ludwig Schmidt. Openclip, 2021년 7월.[17] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park. 텍스트-이미지 합성을 위한 gans 확장. CVPR, 2023.[18] Tero Karras, Samuli Laine, Timo Aila. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 4401-4410쪽, 2019. 6, 7,[19] Yuval Kirstain, Omer Levy, Adam Polyak. X&amp;fuse: 텍스트-이미지 생성에서 시각 정보 융합. arXiv 사전 인쇄본 arXiv:2303.01000, 2023.[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 정의. CVPR, 2023. 1, 3, 8,[21] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023.[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár. Microsoft coco: 컨텍스트의 일반 객체, 2014.[23] Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang. 야생에서 얼굴 속성 심층 학습. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 3730-3738페이지, 2015.[24] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu 및 Jiaying Liu. 공동 주체 및 텍스트 조건부 이미지 생성을 위한 통합 다중 모달 잠재 확산. arXiv 사전 인쇄본 arXiv:2303.09319, 2023.[25] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch 및 Daniel Cohen-Or. Mystyle: 개인화된 생성 사전. ACM Transactions on Graphics(TOG), 41(6):1-10, 2022.[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스에서, 8748-8763페이지. PMLR, 2021.[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022.[28] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 제로샷 텍스트-이미지 생성. 기계 학습 국제 컨퍼런스에서, 8821-8831페이지. PMLR, 2021.[29] Nils Reimers 및 Iryna Gurevych. Sentence-bert: siamese bert-networks를 사용한 문장 임베딩. EMNLP, 2019. 5,[30] Daniel Roich, Ron Mokady, Amit H Bermano 및 Daniel Cohen-Or. 실제 이미지의 잠재 기반 편집을 위한 피벗 튜닝. ACM Transactions on Graphics(TOG), 42(1):1–13, 2022. 3,[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. 1,[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR, 2023. 1, 3, 8,[33] Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, Kayvon Fatahalian. 콜라주 확산. arXiv 사전 인쇄본 arXiv:2303.00262, 2023.[34] Florian Schroff, Dmitry Kalenichenko, James Philbin. Facenet: 얼굴 인식 및 클러스터링을 위한 통합 임베딩. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 815-823페이지, 2015.[35] Jing Shi, Wei Xiong, Zhe Lin, Hyun Joon Jung. Instantbooth: 테스트 시간 미세 조정 없이 개인화된 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2304.03411, 2023.[36] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 비평형 열역학을 사용한 심층적 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015.[37] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. ICLR, 2021.[38] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/ hugging face/diffusers, 2022.[39] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo. Elite: 사용자 지정 텍스트-이미지 생성을 위한 텍스트 임베딩으로 시각적 개념 인코딩. arXiv 사전 인쇄본 arXiv:2302.13848, 2023.[40] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han. Smoothquant: 대규모 언어 모델을 위한 정확하고 효율적인 사후 학습 양자화. arXiv 사전 인쇄본 arXiv:2211.10438, 2022.[41] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li 및 Yu Qiao. 멀티태스크 계단식 합성 신경망을 사용한 공동 얼굴 감지 및 정렬. IEEE 신호 처리 편지, 23(10):1499–1503, 2016.
