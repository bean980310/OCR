--- ABSTRACT ---
DreamHuman은 텍스트 설명만으로 사실적인 애니메이션 3D 인간 아바타 모델을 생성하는 방법입니다. 최근의 텍스트-3D 방법은 생성에 상당한 진전을 이루었지만 여전히 중요한 측면에서 부족합니다. 제어 및 종종 공간적 해상도는 제한적이며 기존 방법은 애니메이션이 아닌 고정된 3D 인간 모델을 생성하고 사람과 같은 복잡한 구조의 인체 측정적 일관성은 여전히 과제입니다. DreamHuman은 새로운 모델링 및 최적화 프레임워크에서 대규모 텍스트-이미지 합성 모델, 신경 광도장 및 통계적 인체 모델을 연결합니다. 이를 통해 고품질 텍스처와 학습된 인스턴스별 표면 변형을 사용하여 동적 3D 인간 아바타를 생성할 수 있습니다. 이 방법이 텍스트에서 다양한 애니메이션이 가능한 사실적인 3D 인간 모델을 생성할 수 있음을 보여줍니다. 3D 모델은 다양한 외모, 옷, 피부 톤 및 신체 모양을 가지고 있으며 시각적 충실도 면에서 일반적인 텍스트-3D 접근 방식과 이전의 텍스트 기반 3D 아바타 생성기보다 상당히 우수한 성능을 보입니다. 더 많은 결과와 애니메이션을 보려면 https://dream-human.github.io에서 당사 웹사이트를 확인하세요. 1
--- INTRODUCTION ---
대규모 언어 모델[40, 8]의 놀라운 진전은 텍스트에서 광범위한 미디어 모달리티를 생성하는 데 상당한 관심을 불러일으켰습니다. 텍스트-이미지[43, 44, 46, 61, 9, 28], 텍스트-음성[31, 35], 텍스트-음악[2, 15] 및 텍스트-3D[17, 37] 생성 분야에서 상당한 진전이 있었습니다. 텍스트를 조건으로 하는 일부 인기 있는 생성 이미지 방법의 성공에 중요한 것은 확산 모델[46, 44, 49]이었습니다. 최근 연구에서는 이러한 텍스트-이미지 모델을 미분 가능한 신경 3D 장면 표현[5]과 결합하고 텍스트 설명만으로 사실적인 3D 모델을 생성하도록 최적화할 수 있음을 보여주었습니다[17, 37]. 사실적인 3D 인간 모델의 제어 가능한 생성은 오랫동안 연구 커뮤니티의 초점이었습니다. 이것이 또한 우리 작업의 목표입니다.우리는 텍스트 설명만 주어진 사실적이고 애니메이션이 가능한 3D 인간을 생성하고자 합니다.우리의 방법은 추가적인 훈련이나 미세 조정 없이 다양한 포즈로 배치할 수 있는 동적이고 관절화된 3D 모델을 학습하기 때문에 정적 텍스트-3D 생성 방법을 넘어섭니다.우리는 텍스트-3D 생성[37], 신경 광도장[25, 5] 및 인체 모델링[58, 3]의 최근 진전을 활용하여 사실적인 모양과 고품질 지오메트리를 갖춘 3D 인간 모델을 생성합니다.우리는 어떠한 지도식 텍스트-3D 데이터나 이미지 컨디셔닝도 사용하지 않고 이를 달성합니다. 그림 1과 그림 2에서 볼 수 있듯이 텍스트에만 의존하여 사실적이고 애니메이션이 가능한 3D 인간 모델을 만들었습니다. 범용 3D 생성 방법[37]이 인상적이기는 하지만, 생성에 대한 제어가 제한적이어서 비현실적인 신체 비율, 사라진 사지 또는 잘못된 손가락 개수와 같은 바람직하지 않은 시각적 아티팩트가 종종 발생하기 때문에 3D 인간 합성에는 최적이 아니라고 주장합니다. 이러한 불일치는 부분적으로 텍스트-이미지 네트워크의 알려진 문제에 기인할 수 있지만, 더 어려울 수 있는 3D 생성 문제를 고려할 때 훨씬 더 분명해집니다. 애니메이션 기능을 활성화하는 것 외에도 기하학적 및 운동학적 인간 사전 지식이 사전 인쇄. 검토 중. 드레드락을 한 남자 요가 바지를 입은 금발 여성 그림 1: 우리 방법으로 합성되고 포즈를 취한 3D 모델의 예. DreamHuman은 인간의 외모에 대한 텍스트 설명만 주어지면 애니메이션이 가능한 3D 아바타를 생성할 수 있습니다. 테스트 시간에는 추가적인 정제 없이 3D 포즈나 동작 세트에 따라 아바타를 재배치할 수 있습니다. 효과적인 방식으로 인체 측정학적 일관성 문제를 해결합니다. DreamHuman이라는 이름의 제안된 방법은 전문 아티스트와 3D 애니메이터를 위한 강력한 도구가 될 수 있으며, 게임, 특수 효과, 영화 및 콘텐츠 제작과 같은 산업에서 잠재적으로 혁신적인 효과를 낼 수 있는 디자인 프로세스의 복잡한 부분을 자동화할 수 있습니다. 주요 기여 사항은 다음과 같습니다. • 다양한 포즈로 배치할 수 있고, 사실적인 옷 변형이 있으며, 단일 텍스트 설명만 제공되고, 감독된 텍스트-3D 데이터 없이 학습할 수 있는 3D 인간 모델을 생성하는 새로운 방법을 제시합니다. • 당사 모델은 여러 손실을 사용하여 인간 구조, 외관 및 변형의 품질을 보장함으로써 결과 아바타의 생성 및 재배치를 정규화하는 데 필요한 3D 인체 사전을 통합합니다. • 우리는 얼굴과 손과 같은 지각적으로 중요한 신체 부위에 세부 정보를 추가하기 위해 세부적인 프롬프트를 사용하여 의미적 확대/축소를 통해 생성 품질을 개선합니다. 2
--- RELATED WORK ---
확산 모델[52]과 이를 이미지 생성[13, 29, 10, 46, 44, 49, 48] 또는 이미지 편집[18, 47, 12, 26]에 적용하는 것과 관련된 상당한 연구가 있습니다. 저희는 텍스트-3D[17, 37, 41]에 초점을 맞추고 있으며, 특히 텍스트 프롬프트에 따른 사실적인 3D 인간 생성에 중점을 두고 있습니다. 다음 하위 섹션에서는 저희 목표와 관련된 일부 관련 작업을 다시 살펴봅니다. 텍스트-3D 생성. CLIP-Forge[50]는 레이블이 지정된 텍스트-3D 쌍 없이 3D 객체를 생성하기 전에 학습된 3D 모양과 CLIP[39] 텍스트-이미지 임베딩을 결합합니다. DreamFields[17]는 CLIP[39]의 지침을 사용하여 텍스트 프롬프트가 주어진 NeRF 모델을 최적화합니다. CLIP-Mesh[19]도 CLIP을 사용하지만 NeRF를 기본 3D 표현으로 메시로 대체합니다.DreamFusion[37]은 DreamFields를 기반으로 구축되고 확산 기반 텍스트-이미지 모델[48]의 감독을 사용합니다.Latent-NeRF[24]는 DreamFusion과 유사한 전략을 사용하지만 Latent Diffusion 모델[46]의 공간에서 작동하는 NeRF를 최적화합니다.TEXTure[45]는 텍스트 프롬프트와 대상 메시를 모두 입력으로 사용하고 텍스처 맵을 최적화하여 입력 프롬프트와 일치시킵니다.Magic3D[22]는 고해상도 3D 생성을 위해 Neural Radiance Fields와 메시를 결합하는 2단계 전략을 사용합니다.우리와 달리
--- METHOD ---
텍스트 설명만으로 사실적인 애니메이션이 가능한 3D 인간 아바타 모델을 생성합니다. 최근의 텍스트-3D 방법은 생성에 상당한 진전을 이루었지만 여전히 중요한 측면에서 부족합니다. 제어 및 종종 공간적 해상도는 제한적이며 기존 방법은 애니메이션이 아닌 고정된 3D 인간 모델을 생성하고 사람과 같은 복잡한 구조에 대한 인체 측정적 일관성은 여전히 과제입니다. DreamHuman은 새로운 모델링 및 최적화 프레임워크에서 대규모 텍스트-이미지 합성 모델, 신경 광도장 및 통계적 인체 모델을 연결합니다. 이를 통해 고품질 텍스처와 학습된 인스턴스별 표면 변형을 사용하여 동적 3D 인간 아바타를 생성할 수 있습니다. 저희는 저희 방법이 텍스트에서 다양한 애니메이션이 가능한 사실적인 3D 인간 모델을 생성할 수 있음을 보여줍니다. 저희의 3D 모델은 다양한 외모, 옷, 피부 톤 및 신체 모양을 가지고 있으며 시각적 충실도 면에서 일반적인 텍스트-3D 접근 방식과 이전의 텍스트 기반 3D 아바타 생성기보다 상당히 우수한 성능을 보입니다. 더 많은 결과와 애니메이션은 https://dream-human.github.io에서 당사 웹사이트를 확인하세요.1 서론 대규모 언어 모델[40, 8]의 놀라운 진전으로 인해 텍스트에서 다양한 미디어 모달리티를 생성하는 데 상당한 관심이 생겼습니다. 텍스트-이미지[43, 44, 46, 61, 9, 28], 텍스트-음성[31, 35], 텍스트-음악[2, 15] 및 텍스트-3D[17, 37] 생성 분야에서 상당한 진전이 있었습니다. 텍스트를 조건으로 하는 일부 인기 있는 생성 이미지 방법의 성공의 핵심은 확산 모델[46, 44, 49]이었습니다. 최근 연구에서는 이러한 텍스트-이미지 모델을 미분 가능한 신경 3D 장면 표현[5]과 결합하여 텍스트 설명만으로 사실적인 3D 모델을 생성하도록 최적화할 수 있음을 보여주었습니다[17, 37]. 제어 가능한 포토리얼리스틱 3D 인체 모델 생성은 오랫동안 연구 커뮤니티의 초점이었습니다. 이는 또한 저희 작업의 목표입니다. 저희는 텍스트 설명만 주어진 사실적이고 애니메이션이 가능한 3D 인체를 생성하고자 합니다. 저희의 방법은 추가적인 훈련이나 미세 조정 없이 다양한 포즈로 배치할 수 있는 동적이고 관절이 있는 3D 모델을 학습하기 때문에 정적 텍스트-3D 생성 방법을 넘어섭니다. 저희는 텍스트-3D 생성[37], 신경 광도장[25, 5] 및 인체 모델링[58, 3]의 최근 진전을 활용하여 사실적인 모양과 고품질 지오메트리를 갖춘 3D 인체 모델을 생성합니다. 저희는 어떠한 지도식 텍스트-3D 데이터나 이미지 컨디셔닝도 사용하지 않고 이를 달성합니다. 그림 1과 그림 2에서 볼 수 있듯이 텍스트에만 의존하여 사실적이고 애니메이션이 가능한 3D 인간 모델을 만들었습니다. 범용 3D 생성 방법[37]이 인상적이기는 하지만, 생성에 대한 제어가 제한적이어서 비현실적인 신체 비율, 사라진 사지 또는 잘못된 손가락 개수와 같은 바람직하지 않은 시각적 아티팩트가 종종 발생하기 때문에 3D 인간 합성에는 최적이 아니라고 주장합니다. 이러한 불일치는 부분적으로 텍스트-이미지 네트워크의 알려진 문제에 기인할 수 있지만, 더 어려울 수 있는 3D 생성 문제를 고려할 때 훨씬 더 분명해집니다. 애니메이션 기능을 활성화하는 것 외에도 기하학적 및 운동학적 인간 사전 지식이 사전 인쇄. 검토 중. 드레드락을 한 남자 요가 바지를 입은 금발 여성 그림 1: 우리 방법으로 합성되고 포즈를 취한 3D 모델의 예. DreamHuman은 인간의 외모에 대한 텍스트 설명만 주어지면 애니메이션이 가능한 3D 아바타를 생성할 수 있습니다. 테스트 시간에는 추가적인 정제 없이 3D 포즈나 동작 세트에 따라 아바타를 재배치할 수 있습니다. 효과적인 방식으로 인체 측정학적 일관성 문제를 해결합니다. DreamHuman이라는 이름의 제안된 방법은 전문 아티스트와 3D 애니메이터를 위한 강력한 도구가 될 수 있으며, 게임, 특수 효과, 영화 및 콘텐츠 제작과 같은 산업에서 잠재적으로 혁신적인 효과를 낼 수 있는 디자인 프로세스의 복잡한 부분을 자동화할 수 있습니다. 주요 기여 사항은 다음과 같습니다. • 다양한 포즈로 배치할 수 있고, 사실적인 옷 변형이 있으며, 단일 텍스트 설명만 제공되고, 감독된 텍스트-3D 데이터 없이 학습할 수 있는 3D 인간 모델을 생성하는 새로운 방법을 제시합니다. • 당사 모델은 여러 손실을 사용하여 인간 구조, 외관 및 변형의 품질을 보장함으로써 결과 아바타의 생성 및 재배치를 정규화하는 데 필요한 3D 인체 사전을 통합합니다. • 우리는 얼굴과 손과 같이 지각적으로 중요한 신체 영역에 세부 정보를 추가하기 위해 프롬프트를 정제하여 의미적 확대/축소를 통해 생성의 질을 개선합니다.2 관련 작업 확산 모델[52]과 이미지 생성[13, 29, 10, 46, 44, 49, 48] 또는 이미지 편집[18, 47, 12, 26]에 대한 응용 프로그램과 관련된 상당한 작업이 있습니다.우리는 텍스트-3D[17, 37, 41]에 초점을 맞추고 있으며, 특히 텍스트 프롬프트에 따라 사실적인 3D 인간 생성에 중점을 두고 있습니다.다음 하위 섹션에서는 목표와 관련된 일부 관련 작업을 다시 살펴봅니다.텍스트-3D 생성.CLIP-Forge[50]는 레이블이 지정된 텍스트-3D 쌍 없이 3D 객체를 생성하기 전에 학습된 3D 모양과 CLIP[39] 텍스트-이미지 임베딩을 결합합니다. DreamFields[17]는 CLIP[39]의 안내를 사용하여 텍스트 프롬프트가 주어진 NeRF 모델을 최적화합니다.CLIP-Mesh[19]도 CLIP을 사용하지만 NeRF를 기본 3D 표현으로 메시로 대체합니다.DreamFusion[37]은 DreamFields를 기반으로 구축되고 확산 기반 텍스트-이미지 모델[48]의 감독을 사용합니다.Latent-NeRF[24]는 DreamFusion과 유사한 전략을 사용하지만 Latent Diffusion 모델[46]의 공간에서 작동하는 NeRF를 최적화합니다.TEXTure[45]는 텍스트 프롬프트와 대상 메시를 모두 입력으로 사용하고 입력 프롬프트와 일치하도록 텍스처 맵을 최적화합니다.Magic3D[22]는 고해상도 3D 생성을 위해 신경 복사장과 메시를 결합하는 2단계 전략을 사용합니다.우리의 방법과 달리 언급된 모든 작업은 텍스트 프롬프트가 주어진 정적 3D 장면을 생성합니다. 인간 관련 프롬프트로 질의하면 종종 얼굴 세부 정보 누락, 비현실적인 기하학적 비율, 부분적 신체 생성 또는 다리나 손가락과 같은 신체 부위의 잘못된 수와 같은 아티팩트가 결과에 나타납니다. 루프에 3D 인간 사전 정보를 통합하여 정확하고 인간형적으로 일관된 결과를 생성합니다. 텍스트-3D 인간 생성. 여러 방법[34, 54, 4, 20, 11]은 텍스트-MoCap 데이터 세트를 활용하여 텍스트에서 3D 인간 동작을 생성하는 방법을 학습합니다. MotionCLIP[53]은 CLIP을 감독으로 활용하여 페어링된 텍스트-동작 데이터를 사용하지 않고도 3D 인간 동작을 생성하는 방법을 학습합니다. 그러나 이러한 모든 방법은 3D 좌표 또는 인체의 형태로 3D 인간의 동작을 출력합니다.불교 승려 네이비색 정장을 입은 아시아인 남성 짧은 청바지 치마와 크롭 탑을 입은 여성 웨딩 드레스를 입은 여성 갈색 가죽 재킷을 입은 금발 머리의 남성 터틀넥을 입은 청년 임신한 유색인종 마른 마라톤 선수 크리스마스 스웨터를 입은 남성 폴로 셔츠를 입은 흑인 노인 검은 벨트를 매는 가라테 마스터 탱크탑을 입은 보디빌더 광대 파자마를 입은 플러스 사이즈 모델 흰색 옷을 입은 요리사 흑인 여성 외과의 전통 의상을 입은 인도 신부 스키복을 입은 여성 체육복을 입은 흑인 여성 농부 스페인 플라멩코 댄서 TH 다이빙 슈트를 입은 사람 군복을 입은 흑인 봄버 재킷을 입은 남자 육상 선수 베니스 카니발 옷을 입은 사람 후드 티셔츠를 입은 남자 그림 2: 텍스트 프롬프트가 주어진 방법을 사용하여 생성한 3D 인간 아바타. 각 예제를 두 관점에서 임의의 포즈로 렌더링하고 해당 표면 법선 맵과 함께 렌더링합니다.모델 입력 프롬프트 p = 드레스를 입은 여성 포즈 Ot ~ ►L(§, h, ß) = L₁ + Ln + Lm + Lp + Lo + Lsds 의미적 확대 {p}의 전신 사진 반사율 c imGHUM S(x, 0, B) (d, s) NeRF f(Þ,d,s,0, ẞ) {p}의 헤드샷 확산 렌더러 모델 포즈 공간 정준 공간 i (d, 밀도 구면 고조파 h {p}의 오른팔 사진 그림 3: DreamHuman 개요.드레스를 입은 여성과 같은 텍스트 프롬프트가 주어지면 텍스트 설명과 외모와 신체 모양이 일치하는 사실적이고 애니메이션이 가능한 3D 아바타를 생성합니다.파이프라인의 핵심 구성 요소는 imGHUM [3]을 사용하여 학습하고 제한한 변형 가능하고 포즈 조건이 지정된 NeRF 모델입니다. 암묵적 통계적 3D 인간 포즈 및 모양 모델. 각 훈련 단계에서 무작위로 샘플링된 포즈를 기반으로 아바타를 합성하고 무작위 관점에서 렌더링합니다. 아바타 구조의 최적화는 텍스트-이미지 생성 모델[48]에 의해 구동되는 Score Distillation Sampling 손실[37]에 의해 안내됩니다. imGHUM[3]에 의존하여 I 포즈 제어를 추가하고 아바타 최적화 프로세스에 인간형 사전 확률을 주입합니다. 또한 일관된 합성을 보장하기 위해 여러 다른 일반, 마스크 및 방향 기반 손실을 사용합니다. NeRF, 체형 및 구면 고조파 조명 매개변수(빨간색)가 최적화됩니다. 모델 매개변수[23]이며 사실적인 결과를 생성하는 기능이 없습니다. 우리에게 가장 관련성 있는 방법은 AvatarCLIP[14]입니다. 주어진 텍스트 프롬프트에 대해 AvatarCLIP은 SMPL[23]의 나머지 포즈에서 NeRF를 학습한 다음 행진 큐브를 사용하여 메시로 다시 변환합니다. 새로운 메시는 SMPL 템플릿에 맞춰 정렬되고 스키닝 가중치를 사용하여 애니메이션을 적용할 수 있습니다.그러나 재배치 절차는 애니메이션의 전반적인 사실성을 제한하는 고정 스키닝 가중치에 따라 달라집니다.반대로, 저희 방법은 인스턴스별 포즈별 기하학적 변형을 학습하여 훨씬 더 사실적인 의복 모양을 만들어냅니다.AvatarCLIP과 달리 치마와 드레스와 같은 느슨한 의복도 처리할 수 있습니다.변형 가능한 신경 광채 필드.여러 방법이 동적 콘텐츠를 모델링하기 위해 변형 가능한 NeRF를 학습하려고 시도합니다[32, 38, 55, 33, 51].관절이 있는 인체를 표현하는 작업도 있었습니다[59, 30, 57, 63, 16, 60, 21].저희 방법과 더 밀접한 관련이 있는 방법은 암묵적 인체 모델과 NeRF를 결합한 H-NeRF[59]입니다.H-NeRF에 비해 저희 방법은 두 개의 다른 밀도 필드를 렌더링하지 않고 3D에서 직접 일관성을 적용하는 더 간단한 접근 방식을 사용합니다. 또한, 감독을 위해 비디오를 사용하는 H-NeRF의 경우, 우리의 유일한 입력은 텍스트이고, 우리는 비디오에 존재하는 포즈와 관점에 의해 제약받지 않습니다.따라서 우리의 방법은 다양한 포즈와 카메라 관점에서 더 잘 일반화될 수 있습니다.3 방법론 3. 아키텍처 우리는 Neural Radiance Fields(NeRF)[25]를 사용하여 3D 장면을 공간 좌표의 연속 함수로 표현합니다[27]. 우리는 각 공간 지점 x = R³를 RGB 색상 및 밀도 값의 튜플(c, 7)에 매핑하는 다층 퍼셉트론(MLP)을 사용합니다.NeRF를 사용하여 장면을 렌더링하려면 카메라 중심에서 이미지 픽셀을 통과하는 광선을 투사한 다음 각 광선을 따라 예상 색상 C를 계산해야 합니다.실제로는 광선의 지점 x¿를 샘플링한 다음 렌더링 적분을 근사하여 수행됩니다[5] C = Σ Wici, w₁ = arП[(1 − αj), aii j
--- EXPERIMENT ---
s 이 섹션에서는 제안된 방법의 효과성을 설명합니다. 제안된 개별 구성 요소가 어떻게 도움이 되는지, 그리고 최신 최신 방법과 어떻게 비교되는지 보여드립니다. 그림 2는 다양한 포즈의 생성된 3D 인간 모델을 다양하게 보여 주므로 다양한 신체 모양, 피부 톤 및 신체 구성을 설명할 수 있습니다. 공간 제약으로 인해 추가 결과는 보충 자료에서 제공됩니다. 4.1 절제 연구 의미적 확대. 그림 4에서 의미적 확대 전략의 중요성을 보여줍니다. 우리 방법이 신체와 얼굴 모두에 대해 훨씬 더 높은 품질의 텍스처를 생성할 수 있는 방법에 주목하세요. 포즈에 따른 변형. 그림 5에서 사실적인 의상 변형을 학습하는 방법의 예를 보여줍니다. 발레리나의 예에서 다리가 움직이면 치마가 더 자연스럽게 변형되는 것을 볼 수 있습니다. 반면에 비강성 변형이 없는 기준선은 치마 형상을 포착하는 데 어려움을 겪고 다리 주위에 떠 있는 아티팩트가 나타납니다. 반바지를 입은 남자에 대해서도 비슷한 관찰을 할 수 있습니다. 우리는 텍스트-이미지 생성기가 다양한 포즈의 옷을 입은 사람들의 많은 이미지에 대해 학습되었기 때문에 우리 모델이 이를 추론할 수 있다고 가정합니다.따라서,포즈 교정 있음 포즈 교정 없음 포즈 교정 있음 포즈 교정 없음 발레리나 하와이안 셔츠, 선글라스, 반바지를 입은 남자 그림 5: NeRF 모델에서 포즈 종속 변형과 포즈 샘플링의 중요성, f(b,d, s, 0,3).우리의 비강체 포즈 종속 변형은 아바타를 다시 포즈를 취할 때 더 사실적인 옷을 가능하게 합니다.두 가지 예제 프롬프트 각각에 대해 포즈 교정이 있는 아바타와 없는 아바타 두 개를 보여줍니다.아바타를 다시 포즈를 취할 때 치마와 반바지가 더 자연스럽게 움직이는 것을 주목하세요.우리 모델은 비디오를 사용하지 않거나 텍스트-비디오 확산 손실에 의존하지 않지만 옷이 어떻게 늘어지는지에 대한 일반적인 지식을 활용할 수 있습니다. 4.2 최신 기술과의 비교 그림 6에서 DreamFusion과 우리 방법을 정성적으로 비교했습니다. DreamFusion은 생성에 대한 제어가 제한적입니다. 피사체의 전신을 생성하라는 메시지가 표시되었지만 종종 상체 또는 머리의 3D 모델을 생성합니다. 동시에 장면의 다른 객체에서 인간 피사체를 제대로 분리할 수 없어 환경의 일부가 포함된 3D 모델이 생성됩니다. 그러나 더 중요한 것은 비현실적인 신체 비율, 팔다리가 없거나 여러 개 있는 경우, 시점 과적합으로 인해 발생할 수 있는 퇴화된 기하학과 같은 불쾌한 시각적 아티팩트를 매우 자주 생성한다는 것입니다. 우리 방법은 인체 기하학에 대한 강력한 3D 사전 확률을 활용하여 이러한 문제를 극복할 수 있습니다. 더 많은 비교를 위해 독자는 보충 자료를 참조하세요. 또한 일반적인 관행에 따라 렌더링된 3D 모델을 평가하기 위해 CLIP도 사용합니다. 사람에 대한 설명이 있는 총 160개의 프롬프트를 사용합니다. 결과는 표 1에 나와 있습니다. DreamFusion (1) (1) 1: 잘못된 얼굴 형상 및 텍스처 2: 여러 팔 3: 비현실적인 전체 신체 프로 권투 선수 우리의 DreamFusion 1: 여러 얼굴, 얼굴에 세부 정보 없음 2: 팔 없음 3: 일부 신체 생성 여경 우리의 그림 6: DreamFusion과의 비교. 각 예에서 렌더링된 3D 모델과 해당 표면 법선을 보여줍니다. 두 방법 모두 프롬프트에 DSLR 전신 사진을 추가하여 피사체의 전신을 재구성하도록 요청했습니다. 표 1: CLIP을 사용하여 렌더링된 3D 모델 평가. R-Precision과 실제 캡션이 상위 3개와 가장 높은 점수를 받은 캡션 5개에 포함되는지 여부를 보고합니다. 방법 DreamFusion [37] 우리의 R-정밀도 ↑ 상위-3 상위-0.0.0.888 0.0.931 0.그림 7은 DreamHuman과 AvatarCLIP을 비교한 것입니다.우리의 방법이 훨씬 더 나은 지오메트리와 텍스처 품질을 생성할 수 있음을 알 수 있습니다.AvatarCLIP으로 재구성된 아바타의 지오메트리는 사소한 수정을 거쳐 기본 신체 모델 지오메트리와 매우 유사합니다.결과적으로 느슨한 옷, 드레스, 모자와 같은 액세서리를 처리할 수 없습니다.AvatarCLIP의 모델 텍스처도 상당한 아티팩트가 있으며 시도한 모든 예에서 DreamHuman의 사실감과 전반적인 품질과 일치하지 않습니다.AvatarCLIP 우리의 그림 7: AvatarCLIP과의 비교.DreamHuman을 AvatarCLIP [14]과 비교합니다.왼쪽에서 오른쪽으로 다음 프롬프트를 사용했습니다: 우주 비행사, 건설 관리자, 소방관, 정원사, 조종사, 경찰관, 로봇, 노인, 군인, 청소년, 전사, 마녀, 마법사. 우리의 방법이 훨씬 더 사실적인 텍스처와 기하학을 생성한다는 점에 유의하세요. 모든 그림은 기본 A-Pose입니다. 5
--- CONCLUSION ---
우리는 텍스트에서 3D 인간 아바타를 생성하는 새로운 방법인 DreamHuman을 제시했습니다. 우리의 방법은 통계적 3D 인체 모델과 3D 모델링 및 텍스트-3D 생성의 최근 발전을 활용하여 텍스트-3D 감독 없이 애니메이션 가능한 3D 인간 아바타를 만듭니다. 우리는 우리의 방법이 세부적인 지오메트리로 사실적인 결과를 생성할 수 있으며 최첨단 기술보다 훨씬 더 우수하다는 것을 보여주었습니다. 한계와 향후 작업. 우리의 모델은 3D 데이터 없이 학습되었기 때문에 때때로 지오메트리에 기반하여 만드는 대신 알베도 맵을 사용하여 주름과 같은 미세한 디테일을 그립니다. 향후 작업에서는 3D 데이터를 활용하여 일부 재구성 모호성을 해결하여 이를 해결할 수 있습니다. 또한 모델은 때때로 알베도를 음영에서 제대로 분리할 수 없어 구운 반사와 그림자가 발생합니다. 확산 모델의 현재 계산 제약으로 인해 이 방법을 매우 고해상도 텍스처와 머리카락과 같은 기하학적 디테일로 확장할 수 없습니다. 마지막으로, 의류 애니메이션의 사실성은 비디오 모델에서 이점을 얻을 수 있습니다. 더 광범위한 영향. 저희 방법은 추가 학습 데이터를 사용하지 않지만, 때로는 충분히 큐레이팅되지 않은 이미지와 캡션이 포함된 대규모 데이터 세트에서 사전 학습된 텍스트-이미지 확산 모델에 부분적으로 의존합니다[7](참고로 이 논문에서 사용하는 대부분의 모델에서 원치 않는 콘텐츠를 제거하는 것을 보장하기 위한 효과적인 자동화 수준은 상당합니다). 또한 텍스트-이미지 생성기는 큐레이팅되지 않은 인터넷 규모 데이터에서 사전 학습된 텍스트 인코더에 LLM을 사용합니다. 저희 방법은 편향을 제거하기 위해 엄격하게 큐레이팅되고 다양한 데이터를 사용하여 학습한 통계적 3D 인체 모양 모델을 사용하지만, 궁극적으로 저희 생성 프로세스는 종속성에서 일부 편향에 취약할 수 있습니다. 저희 방법의 목표는 가짜 미디어와 관련하여 오용될 가능성이 있는 사람의 3D 모델을 생성하는 것입니다. 그러나 렌더링된 3D 인체 모델은 일반적으로 2D로 생성된 대응 모델보다 덜 사실적이라는 점을 강조하는 것이 중요합니다. 그럼에도 불구하고 실제 환경에서는 입력 텍스트 프롬프트를 필터링하고 모델 렌더링에서 안전하지 않은 콘텐츠를 감지하는 것과 같이 남용을 방지하기 위한 보호 조치를 사용해야 합니다.우리의 방법은 다양한 체형, 외모, 피부색 및 의복을 가진 사람들을 생성할 수 있습니다.이를 통해 인간 관련 작업을 위한 다양한 대규모 합성 3D 데이터 세트를 생성할 수 있으며, 차례로 다양한 그룹에서 더 공정한 결과를 제공하는 훈련 모델을 지원할 수 있습니다.DreamHuman은 잠재적으로 아티스트 및 기타 창의적인 전문가의 작업을 보강할 수 있습니다.생산성을 높이기 위한 보완 도구로 사용될 수 있습니다.또한 현재 전문 지식과 값비싼 독점 소프트웨어가 필요한 3D 콘텐츠 제작을 민주화할 잠재력이 있습니다.참고문헌 [1] http://mocap.cs.cmu.edu/. [2] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: 텍스트에서 음악 생성. arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [3] Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu. imghum: 3D 인간 모양과 관절 포즈의 암묵적 생성 모델. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 5461-5470페이지, 2021. [4] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, Gül Varol. Teach: 3D 인간을 위한 시간적 동작 구성. 국제 3D 비전 컨퍼런스(3DV), 2022년 9월. [5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. Mip-nerf 360: 무제한 앤티 앨리어싱 신경 광도장. CVPR, 2022. [6] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu. Hspace: 복잡한 환경에서 애니메이션화된 합성 매개변수 인간, 2021. [7] Abeba Birhane, Vinay Uday Prabhu, Emmanuel Kahembwe. 다중 모드 데이터 세트: 여성혐오, 포르노, 악성 고정관념. arXiv 사전 인쇄본 arXiv:2110.01963, 2021. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 소수 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. [9] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: 마스크 생성 변환기를 통한 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2301.00704, 2023. [10] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 gans를 이김. 신경 정보 처리 시스템의 발전, 34:8780-8794, 2021. [11] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li 및 Li Cheng. 텍스트에서 다양하고 자연스러운 3D 인간 동작 생성. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 5152-5161페이지, 2022년 6월. [12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. 2022. [13] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020. [14] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, Ziwei Liu. Avatarclip: 3D 아바타의 제로샷 텍스트 기반 생성 및 애니메이션. ACM Transactions on Graphics(TOG),(4):1-19, 2022. [15] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: 확산 모델을 사용한 텍스트 조건 음악 생성. arXiv 사전 인쇄본 arXiv:2302.03917, 2023. [16] Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nieẞner. Humanrf: 움직이는 인간을 위한 고충실도 신경 광도장. ACM Transactions on Graphics(TOG), 42(4):1–12, 2023. doi: 10.1145/3592415. URL https://doi.org/10.1145/ 3592415. [17] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole. 꿈 필드를 사용한 제로샷 텍스트 가이드 객체 생성. 2022. [18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집, 2023. [19] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Popa Tiberiu. Clip-mesh: 사전 학습된 이미지-텍스트 모델을 사용하여 텍스트에서 텍스처 메시 생성. SIGGRAPH Asia 2022 컨퍼런스 논문, 2022년 12월. [20] Jihoon Kim, Jiseob Kim, Sungjoon Choi. Flame: 자유형 언어 기반 동작 합성 및 편집. AAAI, 2023.[21] Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou 및 Yebin Liu. Posevocab: 인간 아바타 모델링을 위한 관절 구조 포즈 임베딩 학습. ACM SIGGRAPH 컨퍼런스 회의록, 2023. [22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu 및 Tsung-Yi Lin. Magic3d: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.10440, 2022. [23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll 및 Michael J Black. Smpl: 스킨이 적용된 다중 사람 선형 모델. ACM 그래픽 거래(TOG), 34(6):1-16, 2015. [24] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or. 3D 모양과 텍스처의 모양 가이드 생성을 위한 Latent-nerf. arXiv 사전 인쇄본 arXiv:2211.07600, 2022. [25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 장면을 신경 광도 필드로 표현. ECCV에서, 2020. [26] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models, 2022. [27] Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah. Differentiable image parametersizations. Distill, 3(7):e12, 2018. [28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv 사전 인쇄본 arXiv:2112.10741, 2021. [29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. 국제 기계 학습 컨퍼런스, 8162-8171페이지. PMLR, 2021. [30] Atsuhiro Noguchi, Xiao Sun, Stephen Lin 및 Tatsuya Harada. 신경 관절 광도장. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 5762-5772페이지, 2021. [31] Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. 병렬 웨이브넷: 고속 고충실도 음성 합성. 기계 학습 국제 컨퍼런스, 3918-3926페이지. PMLR, 2018. [32] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz 및 Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 광도장. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 5865-5874쪽, 2021. [33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz. Hypernerf: 위상적으로 변하는 신경 광도장에 대한 고차원 표현. ACM Trans. Graph., 40(6), 2021년 12월. [34] Mathis Petrovich, Michael J. Black, Gül Varol. TEMOS: 텍스트 설명에서 다양한 인간 동작 생성. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. [35] Wei Ping, Kainan Peng, Jitong Chen. Clarinet: end-to-end text-to-speech에서의 병렬파 생성. arXiv 사전 인쇄본 arXiv:1807.07281, 2018. [36] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, Michael Black. Clothcap: Seamless 4d clothing capture and retargeting. ACM Transactions on Graphics, (Proc. SIGGRAPH), 36(4), 2017. URL http: //dx.doi.org/10.1145/3072959.3073711. 두 명의 첫 번째 저자가 동등하게 기여했습니다. [37] Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall. Dreamfusion: 2d 확산을 사용한 텍스트-3d. ICLR, 2023. [38] Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer. D-nerf: 동적 장면을 위한 신경 복사장. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 10318-10327쪽, 2021. [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763쪽. PMLR, 2021. [40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/ 20-074.html.[41] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani. Dreambooth3d: 주제 중심 텍스트-3D 생성, 2023. [42] Ravi Ramamoorthi 및 Pat Hanrahan. 조도 환경 맵에 대한 효율적인 표현. 2001년 컴퓨터 그래픽 및 대화형 기술에 관한 제28회 연례 컨퍼런스 논문집, 497-500페이지. [43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로샷 텍스트-이미지 생성. 기계 학습 국제 컨퍼런스, 8821-8831페이지, 2021. [44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재 데이터를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [45] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or. 텍스처: 3D 모양의 텍스트 안내 텍스처링. SIGGRAPH, 2023. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. [47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. 2022. [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans 등. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. [49] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, Mohammad Norouzi. 반복적 정제를 통한 이미지 초고해상도. IEEE 패턴 분석 및 머신 인텔리전스 저널, 2022. [50] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan. Clip-forge: 제로 샷 텍스트-모양 생성을 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18603-18613페이지, 2022. [51] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. 텍스트-4D 동적 장면 생성. arXiv 사전 인쇄본 arXiv:2301.11280, 2023. [52] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 비평형 열역학을 사용한 심층 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015. [53] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, Daniel Cohen-Or. Motionclip: 클립 공간에 인간의 동작 생성을 노출. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, 2022년 10월 23-27일, Proceedings, Part XXII, 358-374페이지. Springer, 2022. [54] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, Daniel Cohen-Or. 인간의 동작 확산 모델. ICLR, 2023. [55] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt. 비강체 신경 광도장: 단안 비디오에서 동적 장면의 재구성 및 새로운 시점 합성. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 12959-12970페이지, 2021. [56] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron 및 Pratul P. Srinivasan. Ref-NeRF: 신경 광도장에 대한 구조화된 시점 종속적 모양. CVPR, 2022. [57] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron 및 Ira Kemelmacher-Shlizerman. HumanNeRF: 단안 비디오에서 움직이는 사람의 자유 시점 렌더링. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 16210-16220쪽, 2022년 6월. [58] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T Freeman, Rahul Sukthankar, Cristian Sminchisescu. Ghum &amp; ghuml: 생성적 3D 인체 모양 및 관절 포즈 모델. CVPR, 2020.[59] Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu. H-nerf: 움직이는 인체의 렌더링 및 시간 재구성을 위한 신경 광도장. 신경 정보 처리 시스템의 발전, 34: 14955-14966, 2021. [60] Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Huang Han, Qi Guojun, Yebin Liu. Latentavatar: 표현력이 풍부한 신경 머리 아바타를 위한 잠재 표현 코드 학습. ACM SIGGRAPHConference Proceedings, 2023. [61] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 콘텐츠가 풍부한 텍스트-이미지 생성을 위한 자기 회귀 모델 확장. arXiv 사전 인쇄본 arXiv:2206.10789, 2022. [62] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, William T Freeman, Rahul Sukthankar, Cristian Sminchisescu. 정규화 흐름을 사용한 약한 감독 3D 인간 포즈 및 모양 재구성. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 6부 16, 465-481페이지. Springer, 2020. [63] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, Lan Xu. Humannerf: 희소 입력에서 효율적으로 생성된 인간 광채장. 2022년 6월 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 7743-7753쪽.
