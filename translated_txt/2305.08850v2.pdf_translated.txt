--- ABSTRACT ---
텍스트 기반 이미지 및 비디오 확산 모델은 사실적이고 다양한 콘텐츠를 생성하는 데 있어 전례 없는 성공을 거두었습니다. 최근 확산 기반 생성 모델에서 기존 이미지와 비디오를 편집하고 변형하는 것이 상당한 주목을 받았습니다. 그러나 이전 작업은 텍스트로 콘텐츠를 편집하거나 단일 시각적 단서를 사용하여 거친 개인화를 제공하는 데 국한되어 세분화되고 세부적인 제어가 필요한 설명할 수 없는 콘텐츠에는 적합하지 않습니다. 이와 관련하여 개인이 주인공이 될 수 있도록 하는 목표로 텍스트 및 시각적 단서를 사용하여 비디오를 편집하는 Make-AProtagonist라는 일반적인 비디오 편집 프레임워크를 제안합니다. 구체적으로, 소스 비디오, 대상 시각적 단서 및 텍스트 단서를 통합하기 위해 마스크 가이드 퓨전 방법과 결합된 시각적-텍스트 기반 비디오 생성 모델을 설계합니다. 광범위한 결과는 Make-A-Protagonist의 다재다능하고 놀라운 편집 기능을 보여줍니다. 1.
--- INTRODUCTION ---
&quot;주인공은 전체 스토리의 배경을 설정합니다.&quot; Ben Okri [24] 확산 기반 생성 모델은 텍스트에 따라 사실적이고 다양한 이미지 [27, 28, 31]와 비디오 [34, 38, 41]를 생성하는 데 놀라운 성공을 거두었습니다. 그러나 생성 프로세스는 더 다양하지만 텍스트 설명에만 의존할 때 제어 가능성이 부족합니다. 원하는 콘텐츠를 생성하기 위해 연구자들은 텍스트 조건 생성을 수정하는 두 가지 방법을 모색합니다. 외부 제어 신호 [5, 13, 23, 40]를 통합하고 기존 콘텐츠를 텍스트 정보로 편집합니다 [8, 19, 21, 25]. 그러나 두 방법 모두 원하는 콘텐츠를 전달하기 위해 텍스트 설명에 의존하므로 중요한 질문이 제기됩니다. 텍스트를 통해 콘텐츠를 정확하게 설명할 수 없다면 어떻게 될까요? 예를 들어, 그림 1에 표시된 소스 비디오를 고려해 보겠습니다. 이전 연구[19, 25]에서는 &quot;Suzuki Jimny&quot;를 &quot;Porsche&quot;나 &quot;Mercedes-Benz&quot;와 같이 인식 가능한 브랜드로 대체할 수 있었습니다. 이러한 옵션은 텍스트 모델에서 식별할 수 있기 때문입니다. 그럼에도 불구하고 이름 없는 자동차로는 대체할 수 없습니다. 예를 들어, 그림 1의 2번째 행에 있는 참조 이미지는 텍스트 프롬프트가 &quot;검은색 자동차&quot;인 텍스트-이미지 모델에서 생성되었으므로 언어 모델에서 정확히 인식할 수 있는 해당 이름이 없습니다. 반면에 사람들은 자신의 콘텐츠를 개인화하고 텍스트로 정확하게 설명되지 않을 수 있는 주인공을 변경하려는 강한 동기를 가지고 있습니다. 이러한 동기는 개인화된 모델[22, 30]과 이미지/비디오 변형 기술[5, 27]의 개발에 영감을 주었습니다. 그럼에도 불구하고 개인화된 모델은 여러 대상 이미지를 사용하여 각 특정 대상에 대해 미세 조정이 필요하고 하이퍼파라미터와 교육 구성에 대한 민감성을 보입니다. 또한 이미지 및 비디오 변형 모델은 참조 이미지의 배경에 대한 편향을 보이는 경향이 있습니다. 또한 두 유형의 생성 모델은 일반적으로 단일 시각적 참조 단서를 기반으로 콘텐츠를 생성하는 데 제한되어 여러 주인공을 수정해야 하는 경우 비효과적입니다. 이러한 제한 사항은 텍스트와 임의의 수의 참조 이미지를 사용하여 일반 비디오 편집을 위한 프레임워크를 설계하도록 동기를 부여합니다. 최근 연구 중 하나[38]는 개인화된 모델링[30]과 원샷 비디오 편집을 결합하여 일반 비디오 편집에 적용할 수 있습니다. 구체적으로 이 연구는 먼저 참조 이미지로 텍스트-이미지 생성(T2I) 모델을 미세 조정합니다. 그런 다음 시간 계층으로 T2I 모델을 팽창시키고 소스 비디오에서 최적화합니다. 그러나 이미지-비디오 튜닝 프로세스에는 심각한 한계가 있습니다. 참조 이미지를 변경할 때 두 모델을 다시 학습해야 합니다. 또한 하이퍼 매개변수에 민감한 개인화된 모델링의 문제도 상속받습니다. 이를 위해 이 논문은 개인적으로 편집한 콘텐츠(주인공)를 소스 비디오에서 풀어서 엔드투엔드 원스테이지 일반 비디오 편집을 실현합니다. 일반적인 비디오 편집을 달성하기 위해 프레임워크는 시각적 단서를 활용하고, 텍스트 단서를 활용하고, 소스 동작을 유지하는 기능이 필요합니다. 따라서 우리는 CLIP 이미지와 텍스트 임베딩을 조건으로 사용하는 Stable UnCLIP*을 기반으로 프레임워크를 구축합니다. 이미지 임베딩은 모델 피처에 직접 추가되는 반면 텍스트 임베딩은 교차 주의를 통해 활용됩니다. [34, 38, 41]에 따라 추가 시간 모듈을 포함하여 T2I 모델(Stable UnCLIP)을 원샷 텍스트-비디오 생성(T2V) 모델로 확장합니다. 소스 비디오로 T2V 모델을 조정한 후, 소스 비디오의 주인공 마스크에 시각적 단서, 텍스트 단서 및 소스 동작을 통합하는 새로운 마스크 가이드 퓨전을 제안합니다. 마스크는 사전 훈련된 모델에서 가져오므로 [4, 16, 18] 데이터 주석이 필요 없습니다. 확산 기반 T2V 모델과 효과적인 마스크 유도 융합 방법을 통해, 저희 프레임워크(MakeA-Protagonist)는 배경을 유지하면서 주인공 편집, 소스 콘텐츠의 배경 편집, 주인공을 사용한 텍스트-비디오 편집을 포함하여 기존 및 새로운 비디오 편집 작업에서 칭찬할 만한 성과를 달성합니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • 시각적 및 텍스트적 단서가 있는 일반적인 비디오 편집을 위한 최초의 엔드투엔드 프레임워크를 제시합니다. • 시각적-텍스트 기반 비디오 생성 모델과 새로운 마스크 유도 융합을 설계하여 시각적 단서, 텍스트 단서 및 소스 동작을 통합하여 강력한 비디오 편집 성능을 실현합니다. • 광범위한 결과는 Make-A-Protagonist의 다양한 응용 프로그램과 이전 비디오 편집 작업보다 우수함을 보여줍니다. 2.
--- RELATED WORK ---
시각적 콘텐츠 생성. 콘텐츠 생성은 강력한 생성 모델[14, 37, 39]을 통해 놀라운 진전을 이루었습니다. 최근 인터넷에서 광범위하고 다양한 이미지-텍스트 쌍을 갖춘 확산 기반 생성 모델[27, 28, 31]이 GAN 기반 생성 모델보다 성능이 우수했습니다.
--- METHOD ---
소스 비디오, 타겟 시각적 및 텍스트적 단서를 통합합니다. 광범위한 결과는 Make-A-Protagonist의 다재다능하고 뛰어난 편집 기능을 보여줍니다. 1. 서론 &quot;주인공은 전체 스토리의 배경을 설정합니다.&quot; Ben Okri [24] 확산 기반 생성 모델은 텍스트에 따라 사실적이고 다양한 이미지[27, 28, 31]와 비디오[34, 38, 41]를 생성하는 데 놀라운 성공을 거두었습니다. 그러나 생성 프로세스는 더 다양하지만 텍스트 설명에만 의존할 때 제어 가능성이 부족합니다. 원하는 콘텐츠를 생성하기 위해 연구자들은 텍스트 조건 생성을 수정하는 두 가지 방법을 모색합니다. 외부 제어 신호[5, 13, 23, 40]를 통합하고 기존 콘텐츠를 텍스트 정보로 편집합니다[8, 19, 21, 25]. 그러나 두 방법 모두 원하는 콘텐츠를 전달하기 위해 텍스트 설명에 의존하므로 중요한 질문이 제기됩니다. 콘텐츠를 텍스트로 정확하게 설명할 수 없다면 어떻게 될까요? 예를 들어, 그림 1에 표시된 소스 비디오를 고려해 보겠습니다. 이전 연구[19, 25]에서는 &quot;Suzuki Jimny&quot;를 &quot;Porsche&quot;나 &quot;Mercedes-Benz&quot;와 같이 인식 가능한 브랜드로 대체할 수 있었습니다. 이러한 옵션은 텍스트 모델에서 식별할 수 있기 때문입니다. 그럼에도 불구하고 이름 없는 자동차로는 대체할 수 없습니다. 예를 들어, 그림 1의 2번째 행에 있는 참조 이미지는 텍스트 프롬프트가 &quot;검은색 자동차&quot;인 텍스트-이미지 모델에서 생성되었으므로 언어 모델에서 정확히 인식할 수 있는 해당 이름이 없습니다. 반면에 사람들은 자신의 콘텐츠를 개인화하고 텍스트로 정확하게 설명되지 않을 수 있는 주인공을 변경하려는 강한 동기를 가지고 있습니다. 이러한 동기는 개인화된 모델[22, 30]과 이미지/비디오 변형 기술[5, 27]의 개발에 영감을 주었습니다. 그럼에도 불구하고 개인화된 모델은 여러 대상 이미지를 사용하여 각 특정 대상에 대해 미세 조정이 필요하고 하이퍼파라미터와 교육 구성에 대한 민감성을 보입니다. 또한 이미지 및 비디오 변형 모델은 참조 이미지의 배경에 대한 편향을 보이는 경향이 있습니다. 또한 두 유형의 생성 모델은 일반적으로 단일 시각적 참조 단서를 기반으로 콘텐츠를 생성하는 데 제한되어 여러 주인공을 수정해야 하는 경우 비효과적입니다. 이러한 제한 사항은 텍스트와 임의의 수의 참조 이미지를 사용하여 일반 비디오 편집을 위한 프레임워크를 설계하도록 동기를 부여합니다. 최근 연구 중 하나[38]는 개인화된 모델링[30]과 원샷 비디오 편집을 결합하여 일반 비디오 편집에 적용할 수 있습니다. 구체적으로 이 연구는 먼저 참조 이미지로 텍스트-이미지 생성(T2I) 모델을 미세 조정합니다. 그런 다음 시간 계층으로 T2I 모델을 팽창시키고 소스 비디오에서 최적화합니다. 그러나 이미지-비디오 튜닝 프로세스에는 심각한 한계가 있습니다. 참조 이미지를 변경할 때 두 모델을 다시 학습해야 합니다. 또한 하이퍼 매개변수에 민감한 개인화된 모델링의 문제도 물려받습니다. 이를 위해 이 논문은 개인적으로 편집한 콘텐츠(주인공)를 소스 비디오에서 풀어서 엔드투엔드 원스테이지 일반 비디오 편집을 실현합니다. 일반적인 비디오 편집을 달성하기 위해 프레임워크는 시각적 단서를 활용하고, 텍스트 단서를 활용하고, 소스 동작을 유지하는 기능이 필요합니다. 따라서 우리는 CLIP 이미지와 텍스트 임베딩을 조건으로 사용하는 Stable UnCLIP*을 기반으로 프레임워크를 구축합니다. 이미지 임베딩은 모델 피처에 직접 추가되는 반면 텍스트 임베딩은 교차 주의를 통해 활용됩니다. [34, 38, 41]에 따라 추가 시간 모듈을 포함하여 T2I 모델(Stable UnCLIP)을 원샷 텍스트-비디오 생성(T2V) 모델로 확장합니다. 소스 비디오로 T2V 모델을 조정한 후, 소스 비디오의 주인공 마스크에 시각적 단서, 텍스트 단서 및 소스 동작을 통합하는 새로운 마스크 가이드 퓨전을 제안합니다. 마스크는 사전 훈련된 모델에서 가져오므로 [4, 16, 18] 데이터 주석이 필요 없습니다. 확산 기반 T2V 모델과 효과적인 마스크 유도 융합 방법을 통해, 우리의 프레임워크(MakeA-Protagonist)는 배경을 유지하면서 주인공 편집, 소스 콘텐츠의 배경 편집, 주인공을 사용한 텍스트-비디오 편집을 포함하여 기존 및 새로운 비디오 편집 작업에서 칭찬할 만한 성능을 달성합니다.우리의 기여는 다음과 같이 요약될 수 있습니다.• 시각적 및 텍스트적 단서가 있는 일반적인 비디오 편집을 위한 최초의 종단 간 프레임워크를 제시합니다.• 시각적-텍스트 기반 비디오 생성 모델과 시각적 단서, 텍스트 단서 및 소스 동작을 통합하는 새로운 마스크 유도 융합을 설계하여 강력한 비디오 편집 성능을 실현합니다.• 광범위한 결과는 Make-A-Protagonist의 다양한 응용 분야와 이전 비디오 편집 작업보다 우수함을 보여줍니다.2. 관련 연구 시각적 콘텐츠 생성. 콘텐츠 생성은 강력한 생성 모델[14, 37, 39]을 통해 놀라운 발전을 이루었습니다. 최근, 인터넷에서 방대하고 다양한 이미지-텍스트 쌍을 갖춘 확산 기반 생성 모델[27, 28, 31]은 텍스트-이미지 생성(T2I)에서 GAN 기반 방법을 능가했습니다. T2I에서의 인상적인 성능을 감안할 때, 텍스트-비디오 생성(T2V)[11, 12, 20, 34, 41]이 많은 주목을 받았습니다. T2V 모델은 종종 사전 훈련된 T2I 모델을 사용하여 풍부한 이미지-텍스트 리소스를 활용합니다. 또한, 비디오 표현 학습을 용이하게 하기 위해 시간 모듈[34, 41]이 도입되었습니다. 또한, 시간적 일관성 *https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip-small 소스 비디오 역전 잠재 &quot;산길을 달리는 스즈키 짐니&quot; 주의 Мар CLIP 비전 역전 잠재 소스 키 프레임 CLIP 이미지 임베드 주의 맵 참조 이미지 CLIP 텍스트 임베드 ResBlock 자체 주의 출력 교차 주의 마스크 유도 융합 ControlNet &quot;눈 속을 달리는 차&quot; 그림 2. Make-A-Protagonist의 전체 추론 프레임워크. 마스크와 제어 신호는 소스 비디오에서 추출됩니다. 소스 비디오, 시각적 및 텍스트적 단서는 마스크 유도 융합을 통해 비디오 생성 모델에서 융합되어 일반적인 비디오 편집이 가능합니다. 또한 비디오 생성을 향상시키기 위해 잠재 공간에 설정됩니다[15, 20]. 텍스트를 사용한 시각적 콘텐츠 편집 및 변형. 콘텐츠 생성을 위한 대체 방향은 텍스트 설명에만 기반한 통제되지 않은 생성 대신 기존 이미지[2, 8, 21, 36]와 비디오[1, 19, 25, 33, 38]를 편집하여 텍스트를 사용하는 것입니다.SDEdit[21]는 이미지에 노이즈를 적용하고 편집 목적으로 이미지를 복구합니다.Promptto-prompt[8]와 Plug-and-Play[36]는 텍스트 설명을 변경하여 교차 주의 맵을 수정합니다.비디오 편집의 경우 Text2Live[1]는 비디오를 레이어로 나누고 텍스트 설명을 사용하여 각 레이어를 별도로 편집합니다.TuneA-Video[38]는 단일 비디오에서 T2I 모델을 팽창시키고 미세 조정하여 유사한 동작의 새 비디오를 생성합니다.Video-P2P[19]와 FateZero[25]는 Prompt-toprompt를 비디오 수준으로 확장합니다.TokenFlow[7]는 prompt-to-prompt로 여러 키 프레임을 편집한 다음 기능 매칭을 통해 전체 비디오로 전파합니다. 그러나 이전 방법은 텍스트를 통한 콘텐츠 편집에만 초점을 맞추었기 때문에 설명할 수 없는 콘텐츠에는 적합하지 않았습니다.시각적 콘텐츠 변형 및 개인화.설명할 수 없는 콘텐츠를 처리하기 위해 DALL-E 2[27]와 Gen1[5]은 CLIP[26] 이미지 임베딩을 사용하여 이미지 및 비디오 변형을 수행합니다.그러나 CLIP 비전 모델은 전체 이미지에서 정보를 추출하기 때문에 배경 정보가 변형에 불가피하게 포함됩니다.개인화된 모델[6, 22, 30]은 설명할 수 없는 콘텐츠를 처리하기 위한 대체 접근 방식을 제시합니다.그러나 DreamBooth[30]와 DreamMix[22]는 개념 학습을 위해 T21 및 T2V 모델을 미세 조정하기 위해 여러 이미지가 필요하며 미세 조정 프로세스는 교육 구성에 민감합니다.이전 연구의 한계를 고려하여 이 논문에서는 이미지와 텍스트 설명을 모두 포함하는 일반적인 비디오 편집을 위한 프레임워크를 제시합니다.3. Make-A-Protagonist 3.1. 개요 시각적 및 텍스트적 단서를 모두 사용하여 일반적인 비디오 편집을 실현하기 위해 CLIP 이미지 임베딩과 CLIP 텍스트 임베딩을 모두 조건으로 사용할 수 있는 비디오 생성 모델을 소개합니다(3.2절). 또한 CLIP 이미지 임베딩은 피처에 직접 추가되므로 텍스트 임베딩에서 사용하는 교차 어텐션과 비교하여 공간 제어가 부족합니다. 따라서 소스 비디오에서 주인공의 마스크를 활용하여 추론 중에 공간 위치를 정확하게 제어하는 마스크 유도 융합 기술을 제안합니다(3.3절). 나아가 배경과 주인공의 의미적 및 공간적 정보를 각각 정확하게 제어하기 위해 어텐션 융합과 ControlNet을 도입합니다. 비디오 생성 모델과 마스크 유도 융합을 갖춘 프레임워크는 세 가지 응용 프로그램을 달성합니다(3.4절). 전체 프레임워크는 그림 2에 나와 있습니다. 3.2. 시각적-텍스트 기반 비디오 생성 모델 이미지 임베딩을 사용한 잠재 확산 모델(LDM) LDM은 두 가지 주요 구성 요소로 구성됩니다. 인코더 E를 사용하여 RGB 이미지 x를 잠재 z = E(x)로 인코딩하는 자동 인코더와 z에서 이미지 x D(z)를 재구성하는 디코더 D입니다. 잠재 공간에서 잔여 블록과 자기/교차 주의가 포함된 U-Net[29] ε이 사용됩니다. 소스 비디오 인코더 확산 소스 흐름 텍스트 흐름 참조 흐름 산길을 달리는 스즈키 짐니 CLIP 텍스트 CLIP 비전 재구성 손실 ENLIER 출력 참조 이미지 그림 3. 시각-텍스트 기반 비디오 생성 모델의 학습 프로세스. 모델은 비디오 캡션과 무작위로 샘플링된 참조 프레임으로 학습됩니다. 노이즈를 제거하고 다음 목적을 갖습니다. min Ezo,e,t√√at ε - √√1 - αt ·€0 (zt, t, C, I)||2, (1) 여기서 C와 I는 각각 텍스트와 이미지의 임베딩을 나타냅니다. 잡음 ε는 t 단계에 따라 zo에 추가되어 zt를 얻습니다. 잡음 Є [10]을 직접 예측하는 대신 [5, 11]에 따라 v-매개변수화 [32]를 사용하여 비디오의 색상 일관성을 개선합니다. 텍스트 및 이미지 조건과 관련하여 텍스트 조건은 교차 주의의 키와 값으로 사용되는 반면 이미지 조건은 잔여 블록의 특징에 직접 추가되어 각각 [28] 및 [27]과 일치합니다. DDIM 반전. 결정적 DDIM 샘플링은 T개의 잡음 제거 단계에서 잠재 잡음으로부터 각 프레임을 생성하는 데 사용됩니다. Zt-1 = √at-1 (√√ất Zt + at-1 (√√at ε0 + √√Ī at εe) at Zt), 여기서 t: 1 →T는 타임스탬프를 나타내고 at는 잡음 스케줄링의 매개변수입니다 [35]. 방정식 1과 동일하게 잡음 제거를 위해 v-매개변수화를 사용합니다. Make-A-Protagonist는 비디오 편집에 초점을 맞추므로 추론 중에 무작위 노이즈를 사용하는 대신 DDIM 역전[35]을 사용하여 소스 비디오를 노이즈 제거가 필요한 초기 잠재 코드로 변환합니다. 이 접근 방식은 소스 비디오에 표시된 동작 및 구조 정보를 모두 보존하는 동시에 잠재 공간 내의 시간적 일관성을 향상시킵니다. DDIM 역전은 다음과 같이 설명할 수 있습니다. Zt+1 = √α++1 (√αt zt - at εe) (3) + √√1 - αt+1 (√√αt ε0 + √1 − αt Zt). 비디오 생성 모델. 이 논문에서는 소스 비디오, 시각적 및 텍스트적 단서를 통합하여 일반적인 비디오 편집을 달성하기 위해 LDM(잠재적 차이 모델) 기반의 시각적-텍스트적 비디오 생성 모델을 제안합니다. 비디오 생성 모델의 학습 과정은 그림 3에 나와 있습니다. [34, 38, 41]에 따라 텍스트-이미지 LDM으로 모델을 초기화하고 시간 합성, 시간적 주의, 시공간적 주의를 통해 시간적 상관관계를 포착하고 활용하도록 U-Net을 수정합니다. [38] 비디오 생성 모델은 텍스트와 시각적 단서를 모두 조건 입력으로 사용합니다. 텍스트 단서를 얻기 위해 BLIP-2 [17]를 사용하여 비디오 캡션을 추출합니다. 그런 다음 캡션은 U-Net 내의 교차 주의 메커니즘에서 키와 값으로 사용되는 CLIP 텍스트 모델 [26]을 사용하여 텍스트 임베딩으로 변환됩니다. 이미지 임베딩을 통합하는 기능을 보장하기 위해 각 반복에서 비디오에서 무작위로 하나의 프레임을 참조 프레임으로 선택합니다. 참조 프레임은 CLIP 비전 모델 [26]을 사용하여 인코딩되고 U-Net의 잔여 블록에 추가됩니다. 텍스트 및 이미지 임베딩은 U-Net의 모든 블록에 적용됩니다. 3.3. 마스크 유도 퓨전 소스 비디오 마스크. 시각적 단서(CLIP 이미지 임베딩)가 공간 정보(예: 교차 주의)를 사용한 작업 대신 잔여 블록의 중간 특징에 직접 추가되므로 모델은 참조 객체의 위치를 정확하게 제어할 수 없습니다. 따라서 소스 비디오에서 주인공의 마스크를 활용하여 공간 제어를 위해 전경과 배경을 분리합니다. 구체적으로, 먼저 Segment Anything[16]을 사용하여 첫 번째 프레임에서 주인공 마스크를 추출합니다. 그런 다음 XMem[4]을 채택하여 전체 비디오에서 마스크를 추적합니다. 전체 퓨전 프로세스. 추론 단계에서 비디오 생성 모델은 DDIM 역 잠복 코드를 시작점으로 사용하여 Eq. 2에서 표시한 대로 대상 텍스트 및 시각적 단서를 사용하여 코드의 노이즈를 제거합니다. CLIP Vision Model[26]은 전체 참조 이미지를 인코딩하므로 참조 이미지의 배경이 필연적으로 생성된 결과에 도입됩니다. 이를 해결하기 위해 Grounded-SAM[16, 18]을 통해 원하는 참조 부분을 마스크 아웃합니다. 그러나 그림 2의 &quot;사막에서&quot;와 같이 소스 비디오의 배경을 변경하려고 할 때 마스크된 참조 이미지로 인해 편집 프로세스에서 만족스러운 결과가 나오지 않을 수 있습니다. 그 이유는 피처에 직접 추가된 참조 이미지 임베딩이 교차 주의 메커니즘에서 사용하는 텍스트 단서에 비해 더 지배적인 영향을 미치기 때문입니다. 따라서 텍스트 임베딩을 이미지 임베딩으로 변환할 수 있는 DALL-E 2 Prior[27]를 통합하여 텍스트 단서의 표현을 향상시킵니다. 사전 임베딩과 참조 이미지 임베딩은 소스 마스크와 함께 융합됩니다. 소스 마스크로 주인공과 배경을 분할했음에도 불구하고 소스 마스크에서 도입된 공간 정보가 매우 거칠기 때문에 주인공과 배경이 비디오 전체에서 일관되지 않을 수 있습니다. 대신 셀프 어텐션 맵은 각 프레임의 공간 위치에 대한 좋은 지침 역할을 할 수 있으며 소스 비디오 전체의 공간 위치는 일관됩니다. 따라서 소스 비디오에서 배경 자기 주의 맵을 찾고 이 맵을 사용하여 대상 배경에서 정보를 추출할 수 있습니다.주의 융합 기술은 소스 비디오와 유사한 시간적 일관성을 유지하면서 대상 배경의 표현을 개선할 수 있습니다.주인공 부분의 경우 ControlNet[40]을 추가로 활용하여 보다 정밀한 공간 제어를 제공합니다.특징 융합, 주의 융합 및 ControlNet은 다음과 같이 소개합니다.특징 융합.잔여 블록의 잠재 특징 zЄ RFXCXHXW가 주어지면 이미지 임베딩 IE R1×C가 모든 프레임과 공간 위치에 추가됩니다.비디오 주인공 마스크 MЄ RFXHXW를 활용하여 참조 이미지 임베딩 ZË E R¹×C와 이전에 변환된 텍스트 임베딩 Ip = R¹×C를 융합합니다.2t = F Zt + MIR+ (1 - M) · Tp. . (4) 또한 IR은 주인공의 정보를 나타내므로 피처 융합 연산의 종료 단계를 제어하기 위해 타임스탬프 매개변수 TF를 도입합니다. = √ z²² + M · LR + (1 − M) · Lp F Z+IR ift
--- EXPERIMENT ---
s 4.1. 구현 세부 사항 비디오 생성 모델은 이미지 임베딩[28](Stable UnCLIP)을 사용한 텍스트-이미지(T2I) 잠재 확산 모델에 기반합니다.T2I LDM으로 초기화하고 시간 모듈을 모델에 삽입한 후 비디오 생성 모델은 768 x 768 해상도의 비디오에서 8개 프레임으로 미세 조정됩니다.모델은 3 × 10−5의 학습 속도로 200단계 동안 학습됩니다.추론 동안 분류기 없는 안내[9]가 있는 DDIM 샘플러[35]를 20단계 동안 사용합니다.단일 비디오를 학습하는 데 약 10분, 추론에 30초가 걸립니다.4.2. 기준선과의 비교 기준선. 이전에는 일반적인 비디오 편집을 수행하는 방법이 없었기 때문에 Make-A-Protagonist를 텍스트 기반 편집[25, 38], 비디오 변형[5] 및 우리의 방법과 비교했습니다.표 1. 양적 평가. Make-A-Protagonist는 사용자 연구에서 압도적인 선호도를 보이면서 DreamBooth-V와 비슷한 모델 평가를 달성했습니다.출처 비디오 모델 평가 CLIP-T↑ DINO ↑ 사용자 연구 품질 ↑ 주제 ↑ 프롬프트 ↑ Make-A-Protagonist DreamBooth-V [30, 38] 둘 다 동등하게 0.0.7 1.6% 67.2% 69.8% 0.0.2 1.0% 21.2% 12.4% 7.4% 11.6% 17.8% 강을 오토바이로 달리는 남자 A<man> 타고<motorbike> 해변에서, 애니메이션 스타일 그림 5. 한 비디오에서 두 주인공을 편집하는 예.개인화된 [30] 방법: (1) Tune-A-Video [38]는 단일 비디오에서 팽창된 LDM을 미세 조정하여 관련 콘텐츠를 생성합니다. (2) FateZero [25]는 교차 주의 맵을 제어하여 기존 비디오를 편집합니다. (3) Gen-1 [5]은 참조 이미지나 텍스트를 사용하여 소스 비디오를 변경합니다. Gen-은 하나의 조건(이미지 또는 텍스트)만 사용할 수 있으므로 텍스트 입력 없이 참조 이미지를 사용합니다. (4) DreamBooth-V [30, 38]는 DreamBooth [30]와 Tune-A-Video [38]를 결합한 베이스라인으로, 저희와 유사한 응용 프로그램을 가지고 있습니다. 구체적으로, 먼저 하나의 참조 이미지와 해당 텍스트 토큰 [V]로 개인화된 DreamBooth 모델을 학습한 다음 이 개인화된 모델을 단일 비디오 미세 조정 [38]의 초기화로 사용합니다. 추론하는 동안 텍스트 토큰 [V]를 사용하여 주인공이 있는 콘텐츠를 생성할 수 있습니다. 정성적 평가. 그림 4에서 두 비디오의 기준선을 비교합니다. 첫째, 텍스트 기반 편집 방법[25, 38]은 시각적 단서를 통합할 수 없어 설명할 수 없는 주인공을 생성하는 능력이 제한됩니다. 둘째, Gen-1[5]은 일반적인 편집에 시각적 단서와 텍스트 단서를 동시에 사용할 수 없습니다. 게다가 Gen-1[5]은 생성된 결과에 참조 이미지의 배경 정보를 통합합니다. 셋째, DreamBooth-V[30, 38]는 Make-A-Protagonist로 일반적인 비디오 편집을 수행할 수 있습니다. 그러나 하나의 참조 이미지만으로는 복잡한 주인공(예: 왼쪽 비디오의 남자)에 대한 좋은 DreamBooth 모델을 학습하기에 충분하지 않아 DreamBooth-V는 사실적인 비디오를 생성하는 데 어려움을 겪습니다. 게다가 개인화된 모델이 잘 훈련되었더라도 신중하게 선택된 훈련 구성을 가진 각 주인공에 대해 별도의 모델이 필요합니다. 게다가 DreamBooth-V는 비디오에서 한 명의 주인공만 편집할 수 있는 반면 Make-A-Protagonist는 그림 5에서 여러 주인공을 편집하는 능력을 보여줍니다. 양적 평가. 우리는 일반적인 비디오 편집에서 DreamBooth-V [30, 38]와 정량적 비교를 수행합니다. [30]에 따라 프롬프트 충실도는 CLIPT로 표시되는 프롬프트와 각 비디오 프레임 ViT-B/32 CLIP [26] 임베딩 간의 평균 코사인 유사도로 측정됩니다. 주체 충실도는 DINO로 표시되는 각 생성된 비디오 프레임의 ViT-S/16 DINO [3] 임베딩과 참조 이미지 간의 평균 코사인 유사도로 측정됩니다. 또한 비디오 품질, 주체 충실도 및 프롬프트 충실도 측면에서 두 방법을 비교하기 위해 사용자 연구를 수행합니다. 20명의 사용자에게 25개의 비교 질문으로 구성된 설문지에 답하도록 요청했습니다. 각 비교 질문은 두 방법으로 생성된 참조 이미지, 텍스트 프롬프트 및 비디오를 무작위 순서로 보여줍니다. 비디오 품질의 경우 사용자에게 &quot;두 비디오 중 어느 것이 더 자연스럽고 사실적입니까?&quot;라는 질문에 답하도록 요청하고 &quot;판단할 수 없음/둘 다 동등함&quot; 옵션을 포함했습니다. 마찬가지로, 우리는 &quot;두 비디오 중 어느 것이 참조 이미지의 정체성(예: 아이템 유형 및 세부 정보)을 더 잘 재현하는가?&quot;와 &quot;두 비디오 중 어느 것이 텍스트로 더 잘 설명되는가?(참조 이미지를 고려할 필요 없음)&quot;를 각각 주제 충실도와 프롬프트 충실도에 대해 묻습니다. 표 1에서 볼 수 있듯이, Make-A-Protagonist는 CLIP에서 평가한 프롬프트 충실도가 더 나은 반면 DreamBooth-V 참조 이미지 A<car> 밤에 트랙을 운전하는 것 (a) 소스 비디오 (c) wo DALL-E 2 Prior (e) wo Attention Fusion (b) wo ControlNet (d) wo Feature Fusion (f) 우리의 그림 6. 정성적 절제 연구. 제안된 구성 요소를 제거하면 피험자 충실도, 프롬프트 충실도 및 비디오 품질이 저하됩니다.표 2. 정량적 절제 연구.CLIP-T↑ DINO ↑ wo ControlNet [40] 0.0.wo DALL-E 2 Prior [27] 0.0.wo Feature Fusion 0.0.wo Attention Fusion 0.0.0.0.Make-A-Protagonist는 더 높은 DINO 점수를 얻습니다.그러나 DreamBooth-V가 참조 이미지에 과대적합되는 경향이 있기 때문에 DINO 점수가 생성된 품질을 완전히 반영하지 않는다고 추측합니다.과대적합은 덜 현실적인 생성된 결과이지만 DINO 점수는 더 높습니다(보충 자료의 정성적 비교 참조). 사용자 연구와 관련하여 비디오 품질, 피사체 충실도 및 프롬프트 충실도 측면에서 Make-A-Protagonist에 대한 압도적인 선호도를 발견하여 우리 방법의 우수성을 입증했습니다.4.3. Ablation Studies Make-AProtagonist의 각 구성 요소의 효과를 확인하기 위해 ControlNet[40], DALL-E 2 Prior[27], 기능 융합 및 주의 융합에 대한 절제 연구를 정량적(표 2) 및 정성적(그림 6)으로 수행합니다.ControlNet.ControlNet을 제거하면 주인공 정보를 보존할 수 있지만 동작이 일관되지 않습니다.표 2의 첫 번째 행에서 ControlNet을 제거하면 피사체 충실도(DINO 점수)에 약간의 영향을 미치지만 프롬프트 충실도(CLIP-T 점수)가 손상됩니다.또한 주인공의 정확한 공간 위치가 부족하기 때문에 처음 몇 프레임에서 자동차의 방향이 잘못되었습니다(그림 6(b)).DALL-E 2 Prior. DALL-E 2 Prior를 비활성화하면 텍스트 임베딩이 교차 주의에만 사용됩니다. 결과적으로 텍스트 단서가 시각적 단서에 비해 약해져 텍스트 설명과 일치하지 않는 결과가 생성됩니다. 대신 배경에 대한 CLIP 이미지 임베딩이 0이므로 참조 이미지의 표현에 영향을 미치지 않습니다. 따라서 이미지 유사도가 약간 증가합니다. 특징 융합. 특징 융합은 시각 정보를 유지하는 데 중요하여 DINO 점수가 0.013 향상됩니다. 또한 특징 융합을 제거하는 것은 참조 이미지의 CLIP 이미지 임베딩을 전체 프레임에 통합하는 것과 같으며 DALL-E 2 Prior 임베딩도 제외됩니다. 따라서 특징 융합을 비활성화하면 프롬프트 충실도가 떨어지며 이는 그림 6(d)에서도 입증됩니다. 주의 융합. 주의 융합은 배경에 대한 공간 정보를 제공하여 시간적 일관성을 개선하는 것을 목표로 합니다. 그림 6(e)와 그림 6(f)를 비교하면, 주의 융합은 일관되지 않은 차선 표시를 해결할 수 있습니다. DALL-E 2 Prior와 마스크 유도 융합을 통합하여 Make-A-Protagonist는 양적, 질적 평가에서 모두 놀라운 성과를 보여줍니다. 5.
--- CONCLUSION ---
이 논문에서는 텍스트 및 시각적 단서를 사용하여 일반 비디오 편집을 위한 최초의 엔드투엔드 프레임워크인 Make-A-Protagonist를 소개합니다. 이러한 단서로 주인공과 배경을 편집하기 위해 다양한 정보 소스를 통합하기 위해 마스크 가이드 퓨전 방법과 결합된 시각적-텍스트 기반 비디오 생성 모델을 설계합니다. 퓨전 방식은 소스 비디오의 마스크, 제어 신호 및 주의 맵을 효과적으로 사용하여 주인공과 배경을 모두 편집할 수 있는 정확한 공간 위치를 제공합니다. 비디오 생성 모델을 이 퓨전 방식과 결합함으로써 Make-A-Protagonist는 배경 편집, 주인공 편집 및 주인공을 사용한 텍스트-비디오 편집을 포함하여 다재다능하고 강력한 일반 비디오 편집 애플리케이션을 강화합니다. 한계 및 사회적 영향. 우리 방식의 한 가지 한계는 CLIP 이미지 임베딩으로 시각적 단서를 표현하는 것이 최적이 아닐 수 있다는 것입니다. 주제의 가능한 모든 변형 범위를 포괄하는 데 어려움을 겪을 수 있습니다. 또한 시각적 표현의 효과는 주제에 따라 다릅니다. 예를 들어, 우리 모델은 인간에 비해 자동차에서 더 우수한 성능을 보여줍니다. 사회적 영향과 관련하여 개인화된 이미지 생성은 수많은 이점을 제공하지만 동시에 오용 위험을 높입니다. 따라서 가능한 부정적 영향을 완화하기 위해 소스 비디오와 참조 이미지에 대한 안전 검사를 도입하는 것이 좋습니다. 참고문헌 [1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: 텍스트 기반 계층 이미지 및 비디오 편집. ECCV, 2022.[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: 이미지 편집 지침을 따르는 법 배우기. arXiv 사전 인쇄본 arXiv:2211.09800, 2022.[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 자체 감독 비전 변환기의 새로운 속성. ICCV, 2021. [4] Ho Kei Cheng 및 Alexander G. Schwing. XMem: Atkinson-Shiffrin 메모리 모델을 사용한 장기 비디오 객체 분할. ECCV, 2022. 2,[5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog 및 Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 유도 비디오 합성. arXiv 사전 인쇄본 arXiv:2302.03011, 2023. 2, 3, 4, 6,[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel CohenOr. 이미지는 한 단어의 가치가 있습니다. 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022.[7] Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel. Tokenflow: 일관된 비디오 편집을 위한 일관된 확산 기능. arXiv 사전 인쇄본 arXiv:2307.10373, 2023.[8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626, 2022. 2,[9] Jonathan Ho와 Tim Salimans. 분류자 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022.[10] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 노이즈 제거. NeurIPS에서, 2020.[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. 2,[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang. Cogvideo: 변압기를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868, 2022.[13] Lianghua Huang, Di Chen, Yu Liu, Shen Yujun, Deli Zhao, Zhou Jingren. Composer: 구성 가능한 조건을 사용한 창의적이고 제어 가능한 이미지 합성. arXiv 사전 인쇄본 arxiv:2302.09778, 2023.[14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park. 텍스트-이미지 합성을 위한 gans 확장. arXiv 사전 인쇄본 arXiv:2303.05511, 2023.[15] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi. Text2video-zero: 텍스트-이미지 확산 모델은 제로샷 비디오 생성기입니다. arXiv 사전 인쇄본 arXiv:2303.13439, 2023.[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 무엇이든 분할하세요. arXiv:2304.02643, 2023. 2,4,[17] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023.[18] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 접지 공룡: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합입니다. arXiv 사전 인쇄 arXiv:2303.05499, 2023. 2,[19] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin 및 Jiaya Jia. Video-p2p: 교차 주의 제어를 통한 비디오 편집. arXiv:2303.04761, 2023. 2,[20] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jinren Zhou 및 Tieniu Tan. Videofusion: 고품질 비디오 생성을 위한 분해 확산 모델입니다. arXiv 사전 인쇄본 arXiv:2303.08320, 2023. 2,[21] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, Stefano Ermon. Sdedit: 확률적 미분 방정식을 사용한 이미지 합성 및 편집. arXiv 사전 인쇄본 arXiv:2108.01073, 2021. 2,[22] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen. Dreamix: 비디오 확산 모델은 일반 비디오 편집기입니다. arXiv 사전 인쇄 arXiv:2302.01329, 2023. 2,[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan 및 Xiaohu Qie. T2i 어댑터: 텍스트-이미지 확산 모델에 대해 보다 제어 가능한 기능을 발굴하기 위한 학습 어댑터입니다. arXiv 사전 인쇄 arXiv:2302.08453, 2023.[24] 벤 오크리. 배고픈 길. 조나단 케이프, 1991.[25] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan 및 Qifeng Chen. Fatezero: 제로샷 텍스트 기반 비디오 편집을 위한 주의 집중. arXiv:2303.09535, 2023. 2, 3, 6,[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각 모델 학습. ICML, 2021. 3, 4, 5,[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. 2, 3, 4, 5,[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR, 2022. 2, 4,[29] Olaf Ronneberger, Philipp Fischer 및 Thomas Brox. U-net: 생물의학 이미지 분할을 위한 합성 네트워크. MICCAI, 2015.[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein 및 Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. 2, 3,[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. NeurIPS에서, 2022.[32] Tim Salimans 및 Jonathan Ho. 확산 모델의 빠른 샘플링을 위한 점진적 증류. arXiv 사전 인쇄본 arXiv:2202.00512, 2022.[33] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon. Edit-a-video: 객체 인식 일관성을 갖춘 단일 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.07945, 2023.[34] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022. 2,[35] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델의 노이즈 제거. arXiv 사전 인쇄본 arXiv:2010.02502, 2020. 4,[36] Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel. 텍스트 기반 이미지-이미지 변환을 위한 플러그 앤 플레이 확산 기능. arXiv 사전 인쇄 arXiv:2211.12572, 2022.[37] Aaron Van Den Oord, Oriol Vinyals 등. 신경 이산 표현 학습. NeurIPS, 2017.[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie 및 Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 조정입니다. arXiv 사전 인쇄본 arXiv:2212.11565, 2022. 2, 3, 4, 6,[39] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 콘텐츠가 풍부한 텍스트-이미지 생성을 위한 자기 회귀 모델 확장. arXiv 사전 인쇄본 arXiv:2206.10789, 2022.[40] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543, 2023. 2, 5,[41] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu 및 Jiashi Feng. Magicvideo: 잠재 확산 모델을 사용한 효율적인 비디오 생성. arXiv 사전 인쇄 arXiv:2211.11018, 2022. 2,
