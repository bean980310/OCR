--- ABSTRACT ---
멀티모달 사전 학습의 최근 발전은 3D 모양, 2D 대응물 및 언어 설명에 걸쳐 멀티모달 기능을 정렬하여 3D 표현 학습에서 유망한 효능을 보여주었습니다. 그러나 기존 프레임워크에서 이러한 멀티모달 데이터, 특히 3D 모양에 대한 언어 설명을 큐레이션하는 데 사용하는 방법은 확장 가능하지 않으며 수집된 언어 설명은 다양하지 않습니다. 이를 해결하기 위해 대규모 멀티모달 모델을 활용하여 3D 모양에 대한 전체적인 언어 설명을 자동으로 생성하는 간단하면서도 효과적인 3모달 사전 학습 프레임워크인 ULIP-2를 소개합니다. 입력으로 3D 데이터만 필요하므로 수동 3D 주석이 필요 없으므로 대규모 데이터 세트로 확장할 수 있습니다. ULIP-2는 더 나은 멀티모달 표현 학습을 위한 확장된 백본도 갖추고 있습니다. 우리는 두 개의 대규모 3D 데이터 세트인 Objaverse와 ShapeNet에 대한 실험을 수행하고, ULIP-2를 훈련하기 위해 3D 포인트 클라우드, 이미지 및 언어의 3중 모달 데이터 세트로 이를 보강합니다. 실험 결과 ULIP-2는 세 가지 다운스트림 작업, 즉 제로샷 3D 분류, 미세 조정을 통한 표준 3D 분류 및 3D 캡션(3D-tolanguage 생성)에서 상당한 이점을 보여줍니다. 제로샷 분류에서 Objaverse-LVIS에서 50.6%(상위 1)의 새로운 SOTA와 ModelNet40에서 84.7%(상위 1)를 달성합니다. 표준 미세 조정을 위한 ScanObjectNN 벤치마크에서 ULIP-2는 단 140만 개의 매개변수로 구성된 컴팩트 모델로 91.5%의 전체 정확도에 도달합니다. ULIP-2는 인간 주석 없이 확장 가능한 멀티모달 3D 표현 학습을 위한 새로운 패러다임을 보여주며 기존 기준선에 비해 상당한 개선을 보여줍니다. 코드와 데이터 세트는 https://github.com/salesforce/ULIP에서 공개됩니다. 1.
--- INTRODUCTION ---
영어: 3D 시각적 이해는 최근 몇 년 동안 관심이 급증했습니다[8, 10, 21, 23, 35, 51].* 문의: Ixue@salesforce.com CLIP 렌더링으로 순위 지정 전체 이미지 샘플 포인트 클라우드 대규모 멀티모달 모델 &quot;책과 홀을 든 조각상&quot;, &quot;탁자 위에 왕관을 쓴 인물과 검을 든 조각상&quot;, &quot;책과 필기 도구를 든 작은 석상&quot;을 설명합니다. 텍스트 인코더 제로샷 3D 분류 GOGGO 사전 정렬된 이미지 인코더 3D 인코더 다운스트림 작업 3D 캡션 미세 조정을 통한 3D 분류 그림 1. ULIP-2 사전 학습 프레임워크와 다운스트림 작업 개요. 위의 부분은 ULIP-2 사전 학습 프레임워크이며, ULIP-2는 대규모 멀티모달 모델을 사용하여 3D 모양의 전체적 관점에서 각 2D 렌더링된 이미지에 대한 자세한 설명을 자동으로 생성합니다. ULIP-2는 사전 정렬되고 동결된 시각-언어 기능 공간을 활용하여 전체적 텍스트, 이미지 및 3D 포인트 클라우드의 삼중 모달리티 간의 정렬을 달성합니다. 사전 학습 후 3D 인코더가 다운스트림 작업에서 사용됩니다. 그림에서 볼 수 있듯이 이 사전 학습 프로세스에는 3D 데이터만 필요합니다. 증강 현실과 가상 현실(AR 및 VR) [2, 24, 27, 39, 45], 자율 주행 [20, 54], 로봇 공학 [3, 48]의 응용 분야. 그럼에도 불구하고 3D 데이터의 수집 및 주석은 여전히 비용이 많이 들고 노동 집약적인 프로세스입니다 [4, 42, 50]. 이러한 과제에 대응하여 연구자들은 3D 표현을 학습하기 위한 감독 신호를 제공하기 위해 이미지 및 자연어와 같이 보다 풍부하게 사용 가능한 다른 모달리티로 전환했습니다. 이 접근 방식은 단일 모달 표현을 개선했을 뿐만 아니라 보다 풍부한 다중 모달 표현 기능도 개발했습니다. 결과는 유망했으며 어느 정도 3D 도메인에서 단일 모달 밀도 주석에 대한 필요성을 완화했습니다. 그러나 이 방향의 다중 모달 학습 프레임워크는 일반적으로 3D 애플리케이션을 위한 확장 가능하고 고품질이며 잘 정렬된 다중 모달 데이터를 조립하는 과제에 직면합니다. 우리는 이 프로세스에서 3D 데이터의 언어 모달리티를 중요한 병목 현상으로 식별합니다. 기존 프레임워크는 수동으로 주석이 달린 범주 이름과 메타데이터에서 파생된 짧은 설명을 3D 데이터의 언어 대응물로 사용하는 경향이 있습니다. 그러나 이러한 접근 방식[34, 52]은 데이터 세트 수집 프로세스 중에 항상 어느 정도의 인간 주석에 의존하기 때문에 확장성이 부족하며 이는 확장하기 어려울 것입니다. 더욱이, 기존의 방법은 파생된 언어 정보가 충분한 세부 정보를 제공하지 못하고 변형이 부족하거나 노이즈가 있는 것처럼 보일 수 있으므로 충분히 포괄적이지 않습니다. 이는 확장 가능하고 포괄적인 3D 데이터에 대한 언어 대응물을 제공하는 혁신적인 패러다임의 필요성을 강조하여 멀티모달 학습의 잠재력을 진정으로 발휘합니다. 그러나 3D 모달리티를 위한 언어 데이터를 수집하고 활용하는 최적의 방법은 불분명합니다. 잘 훈련된 인간 주석자는 잠재적으로 3D 객체에 대한 자세한 언어 설명을 제공할 수 있지만 이러한 방법은 비용이 많이 들고 확장성이 부족합니다. 더욱이 3D 모양에 대한 적절한 언어 대응물 모달리티를 식별하는 것은 간단한 작업이 아닙니다. 이러한 문제를 해결하기 위해 먼저 3D 모양에 대한 2D 이미지 대응물 모달리티가 무엇이어야 하는지 재고해 보겠습니다. 의미적으로, 어떤 관점에서든 3D 모양의 2D 이미지를 렌더링할 수 있다면 이러한 모든 렌더링된 이미지의 컬렉션은 이 3D 모양에 대한 모든 정보를 대략적으로 캡슐화해야 하므로 3D에 대한 적절한 이미지 대응물 모달리티를 형성해야 합니다. 유추적으로, 만약 우리가 어떤 관점에서든 3D 모양을 언어적으로 설명할 수 있다면, 모든 관점에서 이 모든 언어 설명을 컴파일하면 이 모양에 대한 모든 언어적으로 표현 가능한 정보도 대략적으로 포괄해야 하므로, 3D 모양에 적합한 언어 모달리티가 형성됩니다. 실제로 효율성을 위해 &quot;모든 관점&quot; 대신 유한하고 고정된 전체적 관점 집합을 샘플링할 수 있습니다. 이미지를 렌더링할 때 언어 모달리티를 만드는 데 동일한 관점 집합을 적용하면, 이 작업은 자연스럽게 주어진 관점에 대한 렌더링된 2D 이미지를 설명하는 것으로 귀결됩니다. 광범위한 언어 및 이미지 데이터로 훈련된 대규모 멀티모달 모델의 발전을 활용하여 렌더링된 이미지에 대한 자세한 언어 설명을 생성하는 기능을 활용합니다. 이 방법을 사용하면 3D 데이터 자체만 필요하고 대규모 멀티모달 모델의 풍부한 지식도 언어 설명으로 추출되므로 확장 가능한 방식으로 프로세스를 자동화할 수 있습니다. 결과적으로, 이 자동화되고 확장 가능한 전략은 자세하고 전체적인 설명으로 언어 모달리티를 풍부하게 하여 멀티모달 3D 표현 학습을 더욱 돕습니다. 앞서 언급한 추론에 비추어, 또한 확장 가능하고 포괄적인 멀티모달 3D 데이터 수집의 과제에 대응하여, 우리는 3D 이해를 위한 잘 정렬된 전체적인 멀티모달 데이터를 생성하는 혁신적인 접근 방식을 포괄하는 새로운 프레임워크인 ULIP-2를 소개합니다. 또한, 이러한 멀티모달 데이터를 견고하게 정렬할 수 있는 효율적인 멀티모달 사전 학습 아키텍처를 결합하여 멀티모달 학습의 모든 잠재력을 활용합니다. 3D 모양이 주어지면, 우리의 첫 번째 단계는 3D 모달리티 입력으로 사용할 3D 포인트 클라우드 데이터를 추출하는 것입니다. 그런 다음 이 모양을 고정된 전체적인 관점 집합에서 일련의 이미지로 렌더링하여 2D 모달리티 입력을 제공합니다. 렌더링된 각 이미지에 대해 우리는 큰 멀티모달 모델을 사용하여 자세한 설명 목록을 생성하여 언어 모달리티를 확립합니다(그림 2 참조). 이 접근 방식은 3D 데이터 자체만 필요하므로 3D에 대한 확장 가능한 멀티모달 데이터를 생성할 수 있습니다. 나아가 포괄적인 전체적 관점 집합에서 설명을 생성함으로써 언어 모달리티의 세부 사항과 포괄성의 이전 문제를 해결합니다. 이 멀티모달 데이터를 정렬하기 위해 효율적인 멀티모달 사전 학습 아키텍처를 채택함으로써 그림 1에 설명된 대로 포괄적인 멀티모달 3D 표현의 학습을 용이하게 합니다. 결과적으로 ULIP-2는 3D 표현 학습을 위한 확장 가능하고 포괄적인 멀티모달 사전 학습을 위한 유망한 솔루션을 제공합니다. ULIP-2는 (1) 포괄적인 멀티모달 학습을 위한 수동 작업이 필요 없는 데이터 생성 패러다임을 제안하고, (2) 이 확장 가능한 패러다임을 활용하여 멀티모달 3D 학습을 더 큰 데이터 세트로 확장하는 동시에 비전 언어와 3D 백본을 확장하고, (3) 동일한 데이터 세트에서 사전 학습할 때 ULIP-2는 모든 다운스트림 작업에서 ULIP보다 인상적인 개선을 제공함으로써 이전 버전인 ULIP보다 더 발전했습니다. 이 논문의 주요 기여는 다음과 같습니다. 1. ULIP-2는 인간 주석이 필요 없이 확장 가능한 멀티모달 사전 학습을 용이하게 하므로 레이블이 지정되지 않은 모든 3D 데이터 세트에 적용할 수 있습니다. 3D 데이터에만 의존하므로 더 광범위한 적용성과 사용 편의성이 가능합니다. 2. ULIP-2는 멀티모달 표현 학습에서 상당한 진전을 이룹니다. 도전적인 오픈 월드 Objaverse-LVIS 벤치마크에서 ULIP-2는 50.6%의 상위 1 정확도를 달성하여 ULIP-2가 더 간단하고 간소화된 프레임워크를 갖고 있음에도 불구하고 현재 SOTA(OpenShape[22])를 3.8%나 크게 능가했습니다. ModelNet40의 제로샷 분류의 경우 ULIP-2는 84.7%에 도달하여 일부 완전 지도 3D 분류 방법[50]보다 성능이 우수했습니다. 또한, &quot;회색의 의자에 앉은 동상&quot;, &quot;매우 정지된 장면에 앉아 있는 동상&quot;, &quot;앉아서 책을 읽는 사람의 동상&quot;, &quot;왕관을 쓴 사람의 동상&quot;, &quot;로브와 왕관을 쓴 동상&quot;, &quot;등받이에 의자가 있는 동상&quot;, ALFONSUEL SABIO &quot;의자에 앉은 남자의 동상&quot;, &quot;받침대에 앉은 작은 조각상&quot;, 360° &quot;왕좌에 앉은 망토를 두른 여성의 작은 동상&quot;, &quot;왕좌에 앉은 사람의 조각상&quot;, &quot;회색 배경의 벤자민 왕의 동상&quot;, &quot;책과 홀을 든 동상&quot;에서 ScanObjectNN 벤치마크에서 전반적인 정확도 91.5%를 확보했습니다. 그림 2. 2D 이미지에서 언어 설명 생성을 보여주는 그림. 이러한 이미지는 3D 객체의 전체적 관점에서 렌더링됩니다. 어떤 관점에서는 의자가 보이지 않고, 다른 관점에서는 홀/검이 보이지 않습니다. 모든 뷰에 대한 설명을 결합하는 것은 모델이 3D 객체에 대한 포괄적이고 전체적인 정보를 학습하는 데 필수적입니다. 메타데이터에서 이 객체의 수동 캡션은 &quot;Estatua de Alfonso X - José Alcoverro(1892)&quot;로, 의미 정보가 많지 않고 ULIP-2의 전체적인 캡션과 달리 멀티모달 사전 학습에 해를 끼칠 수 있습니다. 매개변수는 140만 개에 불과합니다. LLM을 사용한 ULIP-2 인코더의 3D에서 언어 생성 기능도 시연되어 증가하는 LLM 개발과 보조를 맞출 수 있는 잠재력을 강조합니다. 게다가 ULIP-2는 끊임없이 증가하는 3D 데이터 용량과 대규모 멀티모달 모델 개발과 효과적으로 시너지 효과를 낼 수 있습니다. 3. 포인트 클라우드, 이미지, 언어 설명으로 구성된 두 개의 대규모 3모달 데이터 세트인 &quot;ULIPObjaverse&quot;와 &quot;ULIP-ShapeNet&quot; 트리플릿을 출시합니다. 데이터 세트의 통계는 표 2에 자세히 나와 있습니다. 2.
--- RELATED WORK ---
다중 모달 표현 학습. 최근 몇 년 동안 다중 모달 표현 학습은 놀라운 기능과 응용 프로그램으로 인해 인기 있는 연구 주제로 부상했습니다. 대부분의 연구 작업은 언어 및 이미지 모달리티의 두 가지 모달리티에 대한 다중 모달 표현 학습에 초점을 맞추고 있으며, 이는 놀라운 결과를 가져왔습니다. 이 분야의 한 연구 분야는 강력한 예측 기능을 보여주지만 훈련에 컴퓨팅 비용이 많이 드는 Transformer 기반 아키텍처[12, 15, 16, 30, 56]를 사용하여 이미지 영역과 캡션 토큰 간의 상호 작용을 강조합니다. 또는,
--- METHOD ---
기존 프레임워크에서 이러한 멀티모달 데이터, 특히 3D 모양에 대한 언어 설명을 큐레이션하는 데 사용하는 s는 확장 가능하지 않으며 수집된 언어 설명은 다양하지 않습니다. 이를 해결하기 위해 대규모 멀티모달 모델을 활용하여 3D 모양에 대한 전체적인 언어 설명을 자동으로 생성하는 간단하면서도 효과적인 3모달 사전 학습 프레임워크인 ULIP-2를 소개합니다. 입력으로 3D 데이터만 필요하므로 수동 3D 주석이 필요 없으므로 대규모 데이터 세트로 확장할 수 있습니다. ULIP-2는 더 나은 멀티모달 표현 학습을 위해 확장된 백본도 갖추고 있습니다. 우리는 다음을 수행합니다.
--- EXPERIMENT ---
두 개의 대규모 3D 데이터 세트인 Objaverse와 ShapeNet에 대한 s를 사용하고, ULIP-2를 훈련하기 위해 3D 포인트 클라우드, 이미지 및 언어의 3중 모달 데이터 세트로 이를 보강합니다. 실험 결과 ULIP-2는 세 가지 다운스트림 작업, 즉 제로샷 3D 분류, 미세 조정을 통한 표준 3D 분류 및 3D 캡션(3D-언어 생성)에서 상당한 이점을 보여줍니다. 제로샷 분류에서 Objaverse-LVIS에서 50.6%(상위 1)의 새로운 SOTA와 ModelNet40에서 84.7%(상위 1)를 달성합니다. 표준 미세 조정을 위한 ScanObjectNN 벤치마크에서 ULIP-2는 단 140만 개의 매개변수로 구성된 컴팩트 모델로 91.5%의 전체 정확도에 도달합니다. ULIP-2는 인간 주석 없이 확장 가능한 멀티모달 3D 표현 학습을 위한 새로운 패러다임을 보여주며 기존 기준선에 비해 상당한 개선을 보여줍니다. 코드와 데이터 세트는 https://github.com/salesforce/ULIP에서 공개됩니다. 1. 소개 3D 시각적 이해는 최근 몇 년 동안 관심이 급증했습니다[8, 10, 21, 23, 35, 51].* 문의: Ixue@salesforce.com CLIP으로 순위 지정 전체 이미지 샘플 포인트 클라우드 대규모 멀티모달 모델 &quot;책과 홀을 든 조각상&quot;, &quot;탁자 위에 왕관을 쓴 인물과 검을 든 조각상&quot;, &quot;책과 필기 도구를 든 작은 석상&quot;을 설명합니다. 텍스트 인코더 제로샷 3D 분류 GOGGO 사전 정렬된 이미지 인코더 3D 인코더 다운스트림 작업 3D 캡션 미세 조정을 통한 3D 분류 그림 1. ULIP-2 사전 학습 프레임워크와 다운스트림 작업 개요. 위의 부분은 ULIP-2 사전 학습 프레임워크이며, ULIP-2는 대규모 멀티모달 모델을 사용하여 3D 모양의 전체적 관점에서 각 2D 렌더링된 이미지에 대한 자세한 설명을 자동으로 생성합니다. ULIP-2는 사전 정렬되고 동결된 시각-언어 기능 공간을 활용하여 전체적 텍스트, 이미지 및 3D 포인트 클라우드의 삼중 모달리티 간의 정렬을 달성합니다. 사전 학습 후 3D 인코더가 다운스트림 작업에서 사용됩니다. 그림에서 볼 수 있듯이 이 사전 학습 프로세스에는 3D 데이터만 필요합니다. 증강 현실과 가상 현실(AR 및 VR) [2, 24, 27, 39, 45], 자율 주행 [20, 54], 로봇 공학 [3, 48]의 응용 분야. 그럼에도 불구하고 3D 데이터의 수집 및 주석은 여전히 비용이 많이 들고 노동 집약적인 프로세스입니다 [4, 42, 50]. 이러한 과제에 대응하여 연구자들은 3D 표현을 학습하기 위한 감독 신호를 제공하기 위해 이미지 및 자연어와 같이 보다 풍부하게 사용 가능한 다른 모달리티로 전환했습니다. 이 접근 방식은 단일 모달 표현을 개선했을 뿐만 아니라 보다 풍부한 다중 모달 표현 기능도 개발했습니다. 결과는 유망했으며 어느 정도 3D 도메인에서 단일 모달 밀도 주석에 대한 필요성을 완화했습니다. 그러나 이 방향의 다중 모달 학습 프레임워크는 일반적으로 3D 애플리케이션을 위한 확장 가능하고 고품질이며 잘 정렬된 다중 모달 데이터를 조립하는 과제에 직면합니다. 우리는 이 프로세스에서 3D 데이터의 언어 모달리티를 중요한 병목 현상으로 식별합니다. 기존 프레임워크는 수동으로 주석이 달린 범주 이름과 메타데이터에서 파생된 짧은 설명을 3D 데이터의 언어 대응물로 사용하는 경향이 있습니다. 그러나 이러한 접근 방식[34, 52]은 데이터 세트 수집 프로세스 중에 항상 어느 정도의 인간 주석에 의존하기 때문에 확장성이 부족하며 이는 확장하기 어려울 것입니다. 더욱이, 기존의 방법은 파생된 언어 정보가 충분한 세부 정보를 제공하지 못하고 변형이 부족하거나 노이즈가 있는 것처럼 보일 수 있으므로 충분히 포괄적이지 않습니다. 이는 확장 가능하고 포괄적인 3D 데이터에 대한 언어 대응물을 제공하는 혁신적인 패러다임의 필요성을 강조하여 멀티모달 학습의 잠재력을 진정으로 발휘합니다. 그러나 3D 모달리티를 위한 언어 데이터를 수집하고 활용하는 최적의 방법은 불분명합니다. 잘 훈련된 인간 주석자는 잠재적으로 3D 객체에 대한 자세한 언어 설명을 제공할 수 있지만 이러한 방법은 비용이 많이 들고 확장성이 부족합니다. 더욱이 3D 모양에 대한 적절한 언어 대응물 모달리티를 식별하는 것은 간단한 작업이 아닙니다. 이러한 문제를 해결하기 위해 먼저 3D 모양에 대한 2D 이미지 대응물 모달리티가 무엇이어야 하는지 재고해 보겠습니다. 의미적으로, 어떤 관점에서든 3D 모양의 2D 이미지를 렌더링할 수 있다면 이러한 모든 렌더링된 이미지의 컬렉션은 이 3D 모양에 대한 모든 정보를 대략적으로 캡슐화해야 하므로 3D에 대한 적절한 이미지 대응물 모달리티를 형성해야 합니다. 유추적으로, 만약 우리가 어떤 관점에서든 3D 모양을 언어적으로 설명할 수 있다면, 모든 관점에서 이 모든 언어 설명을 컴파일하면 이 모양에 대한 모든 언어적으로 표현 가능한 정보도 대략적으로 포괄해야 하므로, 3D 모양에 적합한 언어 모달리티가 형성됩니다. 실제로 효율성을 위해 &quot;모든 관점&quot; 대신 유한하고 고정된 전체적 관점 집합을 샘플링할 수 있습니다. 이미지를 렌더링할 때 언어 모달리티를 만드는 데 동일한 관점 집합을 적용하면, 이 작업은 자연스럽게 주어진 관점에 대한 렌더링된 2D 이미지를 설명하는 것으로 귀결됩니다. 광범위한 언어 및 이미지 데이터로 훈련된 대규모 멀티모달 모델의 발전을 활용하여 렌더링된 이미지에 대한 자세한 언어 설명을 생성하는 기능을 활용합니다. 이 방법을 사용하면 3D 데이터 자체만 필요하고 대규모 멀티모달 모델의 풍부한 지식도 언어 설명으로 추출되므로 확장 가능한 방식으로 프로세스를 자동화할 수 있습니다. 결과적으로, 이 자동화되고 확장 가능한 전략은 자세하고 전체적인 설명으로 언어 모달리티를 풍부하게 하여 멀티모달 3D 표현 학습을 더욱 돕습니다. 앞서 언급한 추론에 비추어, 또한 확장 가능하고 포괄적인 멀티모달 3D 데이터 수집의 과제에 대응하여, 우리는 3D 이해를 위한 잘 정렬된 전체적인 멀티모달 데이터를 생성하는 혁신적인 접근 방식을 포괄하는 새로운 프레임워크인 ULIP-2를 소개합니다. 또한, 이러한 멀티모달 데이터를 견고하게 정렬할 수 있는 효율적인 멀티모달 사전 학습 아키텍처를 결합하여 멀티모달 학습의 모든 잠재력을 활용합니다. 3D 모양이 주어지면, 우리의 첫 번째 단계는 3D 모달리티 입력으로 사용할 3D 포인트 클라우드 데이터를 추출하는 것입니다. 그런 다음 이 모양을 고정된 전체적인 관점 집합에서 일련의 이미지로 렌더링하여 2D 모달리티 입력을 제공합니다. 렌더링된 각 이미지에 대해 우리는 큰 멀티모달 모델을 사용하여 자세한 설명 목록을 생성하여 언어 모달리티를 확립합니다(그림 2 참조). 이 접근 방식은 3D 데이터 자체만 필요하므로 3D에 대한 확장 가능한 멀티모달 데이터를 생성할 수 있습니다. 나아가 포괄적인 전체적 관점 집합에서 설명을 생성함으로써 언어 모달리티의 세부 사항과 포괄성의 이전 문제를 해결합니다. 이 멀티모달 데이터를 정렬하기 위해 효율적인 멀티모달 사전 학습 아키텍처를 채택함으로써 그림 1에 설명된 대로 포괄적인 멀티모달 3D 표현의 학습을 용이하게 합니다. 결과적으로 ULIP-2는 3D 표현 학습을 위한 확장 가능하고 포괄적인 멀티모달 사전 학습을 위한 유망한 솔루션을 제공합니다. ULIP-2는 (1) 포괄적인 멀티모달 학습을 위한 수동 작업이 필요 없는 데이터 생성 패러다임을 제안하고, (2) 이 확장 가능한 패러다임을 활용하여 멀티모달 3D 학습을 더 큰 데이터 세트로 확장하는 동시에 비전 언어와 3D 백본을 확장하고, (3) 동일한 데이터 세트에서 사전 학습할 때 ULIP-2는 모든 다운스트림 작업에서 ULIP보다 인상적인 개선을 제공함으로써 이전 버전인 ULIP보다 더 발전했습니다. 이 논문의 주요 기여는 다음과 같습니다. 1. ULIP-2는 인간 주석이 필요 없이 확장 가능한 멀티모달 사전 학습을 용이하게 하므로 레이블이 지정되지 않은 모든 3D 데이터 세트에 적용할 수 있습니다. 3D 데이터에만 의존하므로 더 광범위한 적용성과 사용 편의성이 가능합니다. 2. ULIP-2는 멀티모달 표현 학습에서 상당한 진전을 이룹니다. 도전적인 오픈 월드 Objaverse-LVIS 벤치마크에서 ULIP-2는 50.6%의 상위 1 정확도를 달성하여 ULIP-2가 더 간단하고 간소화된 프레임워크를 갖고 있음에도 불구하고 현재 SOTA(OpenShape[22])를 3.8%나 크게 능가했습니다. ModelNet40의 제로샷 분류의 경우 ULIP-2는 84.7%에 도달하여 일부 완전 지도 3D 분류 방법[50]보다 성능이 우수했습니다. 또한, &quot;회색의 의자에 앉은 동상&quot;, &quot;매우 정지된 장면에 앉아 있는 동상&quot;, &quot;앉아서 책을 읽는 사람의 동상&quot;, &quot;왕관을 쓴 사람의 동상&quot;, &quot;로브와 왕관을 쓴 동상&quot;, &quot;등받이에 의자가 있는 동상&quot;, ALFONSUEL SABIO &quot;의자에 앉은 남자의 동상&quot;, &quot;받침대에 앉은 작은 조각상&quot;, 360° &quot;왕좌에 앉은 망토를 두른 여성의 작은 동상&quot;, &quot;왕좌에 앉은 사람의 조각상&quot;, &quot;회색 배경의 벤자민 왕의 동상&quot;, &quot;책과 홀을 든 동상&quot;에서 ScanObjectNN 벤치마크에서 전반적인 정확도 91.5%를 확보했습니다. 그림 2. 2D 이미지에서 언어 설명 생성을 보여주는 그림. 이러한 이미지는 3D 객체의 전체적 관점에서 렌더링됩니다. 어떤 관점에서는 의자가 보이지 않고, 다른 관점에서는 홀/검이 보이지 않습니다. 모든 뷰에 대한 설명을 결합하는 것은 모델이 3D 객체에 대한 포괄적이고 전체적인 정보를 학습하는 데 필수적입니다. 메타데이터에서 이 객체의 수동 캡션은 &quot;Estatua de Alfonso X - José Alcoverro(1892)&quot;로, 의미 정보가 많지 않고 ULIP-2의 전체적인 캡션과 달리 멀티모달 사전 학습에 해를 끼칠 수 있습니다. 매개변수는 140만 개에 불과합니다. LLM을 사용한 ULIP-2 인코더의 3D에서 언어 생성 기능도 시연되어 증가하는 LLM 개발과 보조를 맞출 수 있는 잠재력을 강조합니다. 게다가 ULIP-2는 끊임없이 증가하는 3D 데이터 용량과 대규모 멀티모달 모델 개발과 효과적으로 시너지 효과를 낼 수 있습니다. 3. 포인트 클라우드, 이미지, 언어 설명으로 구성된 두 개의 대규모 3모달 데이터 세트인 &quot;ULIPObjaverse&quot;와 &quot;ULIP-ShapeNet&quot; 트리플릿을 출시합니다. 데이터 세트의 통계는 표 2에 자세히 나와 있습니다. 2. 관련 연구 다중 모달 표현 학습. 최근 몇 년 동안 다중 모달 표현 학습은 놀라운 기능과 응용 프로그램으로 인해 인기 있는 연구 주제로 부상했습니다. 대부분의 연구 작업은 언어 및 이미지 모달리티의 두 가지 모달리티에 대한 다중 모달 표현 학습에 초점을 맞추고 있으며, 이는 놀라운 결과로 이어졌습니다. 이 분야의 한 연구 분야는 강력한 예측 기능을 보이지만 학습에 컴퓨팅 비용이 많이 드는 Transformer 기반 아키텍처[12, 15, 16, 30, 56]를 사용하여 이미지 영역과 캡션 토큰 간의 상호 작용을 강조합니다. 또는 CLIP[37] 및 SLIP[28]과 같은 방법은 이미지와 텍스트에 대한 단일 기능을 독립적으로 생성한 다음 이 두 모달리티를 정렬하는 것을 목표로 합니다. 이 단순화된 아키텍처는 노이즈가 있는 데이터에서도 견고하고 효율적인 대규모 사전 학습을 촉진합니다. 최근 연구에서는 다중 모달 표현 학습을 3D 모달리티로 확장하여 유망한 결과가 입증되었습니다. ULIP[52]는 (3D 포인트 클라우드 - 이미지 - 언어) 삼중항을 만드는 선구적인 작업 중 하나입니다. 이 세 가지 모달리티를 함께 정렬함으로써 ULIP는 3D 표현 학습을 향상시키고 단일 모달 밀도 3D 데이터 주석에 대한 필요성을 완화하여 3D의 데이터 부족 문제를 부분적으로 완화합니다. 최근 작업[59]은 Image-toPoint Masked Autoencoders를 통해 사전 학습된 2D 인코더에서 3D 표현을 학습하려고 합니다. 그러나 이 접근 방식은 언어 모달리티와의 정렬을 포함하지 않으므로 더 복잡한 멀티모달 작업에 대한 용량이 제한될 수 있습니다. [22]의 동시 작업은 더 강력한 성능을 달성하기 위해 ULIP의 프레임워크를 더욱 확장하지만 여전히 3D 데이터의 수동 주석과 복잡한 데이터 엔지니어링 프레임워크에 의존합니다. 그러나 ULIP-2는 제안된 훨씬 더 간단하고 간소화된 프레임워크를 사용하여 까다로운 Objaverse-LVIS 벤치마크에서 SOTA 결과를 달성하고 Objaverse-LVIS 상위 1 정확도에서 OpenShape보다 인상적인 3.8% 더 우수한 성능을 낼 수 있음을 보여줍니다. 단일 모달 밀도 주석 노력을 줄이기 위한 ULIP와 같은 방법을 개발했음에도 불구하고 [22, 34, 52] 여전히 언어 대응 모달을 얻기 위해 데이터 세트 메타데이터와 범주 이름에 의존하기 때문에 확장성 문제에 직면합니다. 또한 이러한 방법으로 생성된 프롬프트 기반 가상 캡션에는 포괄적인 이해에 필요한 세부 정보와 변형이 부족합니다. 반면 ULIP는 최첨단 대규모 멀티모달 모델의 힘과 지식을 활용하여 이러한 한계를 극복합니다. 이 접근 방식은 데이터 요구 사항을 근본적으로 줄이고 사전 학습된 멀티모달 데이터를 풍부하게 하여 더 큰 데이터 세트에서 보다 효율적인 응용 프로그램을 사용하고 훨씬 더 강력한 3D 표현을 생성합니다. 생성적 대규모 멀티모달 모델. GPT에서 GPT-4[29, 43]로의 변압기 모델 확장은 멀티모달 작업에서 규모의 효과를 보여줍니다. [1]에서 유래한 이 접근 방식은 이미지에서 텍스트를 생성하는 데 상당한 진전을 이루었습니다[5, 16–19, 25, 46, 60, 61]. 저희 연구에서는 BLIP-2[17]를 활용하여 3D 모양에 대한 다양한 주석을 생성하여 보다 풍부한 멀티모달 3D 표현을 학습하는 데 도움이 됩니다. 또한 사용된 대형 멀티모달 모델에 대한 5.2에서 절제 연구를 수행하여 ULIP-2가 이 분야의 급속한 개선과 협력하여 대형 멀티모달 모델의 발전으로부터 이점을 얻는다는 것을 보여줍니다. 3D 포인트 클라우드 이해. PointNet[32]은 3D 포인트 클라우드를 직접 처리하는 선구적인 작업입니다[33]. 이를 기반으로 PointNeXt[36]는 가볍고 성능이 뛰어난 변형으로 등장합니다. 포인트 클라우드에 대한 자기 지도 사전 학습의 영역에서 Point-BERT[55]는 변압기 기반 아키텍처로 상당한 진전을 이루었으며, 제로샷 분류 작업에서 주목할 만한 성능을 보여주었습니다. ULIP-2에서 우리는 Point-BERT와 PointNeXt를 모두 3D 인코더로 활용하여 강력한 기능을 활용합니다. 3. 방법 ULIP-2는 ULIP의 사전 학습 프레임워크를 동화하고 확장 가능하고 포괄적인 멀티모달 트리플릿 생성 패러다임을 도입하여 인간 주석의 필요성을 없앨 뿐만 아니라 학습된 멀티모달 3D 표현을 크게 개선합니다. ULIP의 효율적인 멀티모달 사전 학습을 이 확장 가능한 트리플릿 생성 방법과 병합함으로써 ULIP-2는 본질적으로 의사 자기 지도 방식으로 작동하는 대규모 사전 학습의 길을 열었습니다. 우리는 이 방법이 데이터 확장성 문제를 효과적으로 완화하고 동시에 3D 표현 학습 분야를 새로운 수준의 성능으로 발전시킨다는 것을 보여줍니다. 3.1. 예비: ULIP ULIP[52]는 세 가지 모달리티를 포함하는 트리플릿을 구성하는 효율적인 멀티모달 사전 학습 프레임워크를 제시합니다.(1) 3D 포인트 클라우드 데이터를 추출하여 얻은 3D 모달리티;(2) 여러 관점에서 3D 모양의 이미지를 렌더링하여 생성된 이미지 모달리티;(3) 설명적 용어 및 범주 이름과 같은 데이터 세트 메타데이터를 응집력 있는 문장으로 프롬프트하여 파생된 언어 모달리티.ULIP는 사전 학습된 시각 언어 모델이자 CLIP 모델의 변형인 SLIP[28]의 ViT-B 인코더를 활용하여 3D 표현을 학습합니다.이는 3D 모달리티 기능을 언어 및 이미지 모달리티가 공유하는 기능 공간에 맞춰서 달성합니다.ULIP-2는 (이미지, 텍스트, 3D) 모달리티를 정렬하는 데 있어서 ULIP와 유사한 목표를 공유하며, 이는 사전 학습 프레임워크를 채택하도록 합니다. ULIP와 ULIP-2의 설정에서 유사한 점이 많으므로 ULIP를 실험 기준으로 선택했습니다.3.2 확장 가능한 트리플릿 생성 ULIP-2에서 모델은 유사하게 세 가지 입력 모달리티를 활용하지만 3D 객체 데이터 자체만 필요합니다.그림 1에서 볼 수 있듯이 3D 객체가 주어지면 표면에서 3D 포인트 클라우드를 추출하여 3D 인코더에 대한 입력으로 사용하고 다양한 시야각에서 이미지를 생성합니다.그런 다음 최첨단 대형 멀티모달 모델인 BLIP-2[18]를 활용하여 렌더링된 각 2D 이미지에 대한 설명 텍스트를 생성합니다.각 이미지에 대해 문장 세트를 생성하고 CLIP 유사도를 사용하여 순위를 매기고 상위 1개 설명을 집계하여 트리플릿의 언어 모달리티를 형성합니다.이 확장 가능한 트리플릿 생성 방식은 데이터 세트 확장을 용이하게 하여 데이터 세트 메타데이터 수집의 필요성을 없애고 3D 데이터 자체만 필요합니다. 우리 방법은 주석이 없는 3D 데이터 세트에서 3D 표현을 전체적인 이미지 텍스트 쌍과 정렬하여 보다 확장 가능하고 포괄적인 솔루션을 제공할 수 있습니다.3.3. 3중 모드 사전 학습 ULIP-2는 3D 포인트 클라우드, 2D 렌더링 이미지 및 포괄적인 설명의 세 가지를 통합된 feaModel PointCLIP [58] PointCLIPv2 [62] ReCon [34] 사전 학습 데이터 세트 사전 학습 방법 수동 Objaverse-LVIS 캡션? ModelNettop-1 top-5 top-1 top-1.5.19.3 34.4.12.63.6 85.ShapeNet ReCon [34] 1.3.61.78.CLIP2Point [11] ShapeNet CLIP2Point [11] 2.7.49.5 81.Point-BERT [55] ShapeNet OpenShape [22] 10.25.70.91.Point-BERT [55] Objaverse(LVIS 없음) + ShapeNet OpenShape [22] 38.68.83.97.Point-BERT [55] Objaverse + ShapeNet OpenShape [22] 46.76.82.96.Point-BERT [55] Objaverse+ShapeNet + (2개 추가) OpenShape [22] 46.77.84.4 98.ULIP [52] 2.8.60.84.ShapeNet ULIP-Χ 16.34.75.95.Point-BERT [55] ULIP [52] ✓ 21.41.68.86.Objaverse(LVIS 없음) + ShapeNet ULIP-X 46.75.84.97.Objaverse + ShapeNet ULIP [52] ULIP-✓ 34.61.69.6 85.X 50.79.84.97.표 1. Objaverse-LVIS 및 ModelNet40의 제로샷 3D 분류. OpenShape의 강조된 선은 현재 SOTA 접근 방식에서 가져온 것입니다. 저희의 방법은 제로샷 3D 분류에서 최신 기술(SOTA) OpenShape를 능가하여 Objaverse-LVIS에서 3.8% 더 높은 상위 1 정확도를 달성하고, 사전 학습 데이터 세트를 덜 사용했음에도 불구하고 ModelNet40에서 비슷한 성능을 보였습니다. &quot;수동 캡션?&quot;에 체크 표시 열은 사전 학습된 모델이 어느 정도 수동 작업에 의존하는 3D 캡션을 활용한다는 것을 의미하고, 십자가는 그 반대를 의미합니다. ture 공간. 대부분의 실험에 OpenCLIP(VIT-G/14) [13]의 가장 큰 인코더 버전을 채택하고 사전 학습 중에 동결합니다. OpenCLIP에서 이미 사전 정렬한 피처 공간은 3D 모달리티를 통합하려는 대상 공간 역할을 합니다. 3중 모달 사전 학습 중에 3D 모양 O가 주어지면 3D 포인트 클라우드 P를 추출하고 2D 렌더링된 이미지 I render(O)를 무작위로 샘플링하고 BLIP-2에서 생성된 언어 설명 T~ blip2(I)를 포함합니다. 여기서 render는 3D-2D 렌더링 작업이고 blip2는 이미지 설명을 위해 BLIP-2 [18]를 쿼리합니다. 그런 다음 사전 정렬되고 동결된 이미지 인코더 Ę를 기반으로 이미지 피처 f¹ = E₁(I)와 텍스트 피처 fT = E(T)를 추출합니다. OpenCLIP [13]의 텍스트 인코더 ET. 3D 포인트 클라우드 인코더 Ep를 학습하여 3D 특징 fP Ep(P)가 이미지 및 텍스트 특징과 정렬되도록 합니다. CLIP [37]과 유사한 정신으로 대조 손실을 사용하여 3D-이미지 정렬을 공식화합니다. LP2I log = exp(ff/T) Σ108 Σ, exp(f?f}/7) i +log exp(ff/T) Σ, exp(ff/T)&#39; (1) 여기서 i, j는 샘플링 인덱스이고 7은 학습 가능한 온도 매개변수입니다. 첫 번째 항은 3D 특징과 같은 샘플의 이미지 특징의 내적이 이미지 특징이 다른 샘플에서 나온 다른 곱들 중에서 두드러져야 함을 나타냅니다. 마찬가지로 두 번째 항은 3D 특징과 같은 샘플의 이미지 특징의 내적이 3D 특징이 다른 샘플에서 나온 다른 곱들 중에서 두드러져야 함을 나타냅니다. 마찬가지로 3D-텍스트 정렬 손실을 다음과 같이 공식화합니다.CP2T = Σlog +log exp(ff/T) exp(ff/T) exp(ff/T) Σexp(ff/7) (2) 최종 학습 목표는 위의 두 대조 정렬 손실의 합을 최소화하는 3D 인코더 Ep를 학습하는 것입니다.min Lp21 + LP2T.EP (3) 3.4. 3D 멀티모달 학습 확장 보다 강력한 이미지 및 텍스트 인코더가 보다 일반화된 멀티모달 3D 표현을 학습하는 데 도움이 된다는 점을 인식하고 이전에 ULIP에서 활용된 더 작은 ViT-B 모델을 넘어 탐색을 확장합니다.실험은 3모달 정렬 프레임워크에서 이 시각-언어 백본을 업그레이드하는 데 중점을 둡니다.또한 다른 설정은 변경하지 않고 모델 크기를 확장하는 방법을 조사합니다.이러한 수정의 효과는 ModelNet40 및 Objaverse-LVIS 데이터 세트에서 제로샷 분류 작업을 통해 평가됩니다. 표 9 참조. 모달리티 ULIP-Objaverse ~ 800k ULIP-ShapeNet 포인트 클라우드 ~ 1,000만 ~ 1억 ~ 이미지 언어 ~ 52.5k ~ 300만 3,000만 표 2. ULIP-Objaverse 및 ULIP-ShapeNet 트리플릿의 통계. 4. 실험 4.1. ULIP-Objaverse 트리플릿 및 ULIP-ShapeNet 트리플릿 생성 3D 모양의 두 개의 대규모 데이터 세트를 기반으로 3D 포인트 클라우드, 이미지 및 언어 설명의 트리플릿을 추출합니다. 첫 번째 데이터 세트는 최근에 출시된 가장 큰 규모의 사실적 3D 데이터 세트인 Objaverse[6]입니다. 여기에는 ~ 800K 실제 3D 모양이 있으며 각각은 &quot;이름&quot; 필드가 포함된 메타데이터와 연결됩니다. 각 3D 모양에 대해 Blender[14]를 사용하여 360/12도 간격으로 균등하게 배치된 12개의 이미지를 렌더링합니다. 렌더링된 각 이미지에 대해 BLIP-2 [18]의 BLIP-2-opt6.7B를 사용하여 10개의 자세한 설명을 독립적으로 생성한 다음 CLIP-VIT-Large [37] 이미지-텍스트 유사도 점수를 사용하여 순위를 매깁니다. 5.4절의 절제 연구를 기반으로 상위 1개 설명을 언어 모달리티 입력으로 사용합니다. ULIP 및 OpenShape에 따라 각 3D 모양에서 10k, 8k 및 2k 포인트를 사용하여 다양한 다운스트림 작업을 수용합니다. 포괄적인 설명, 2D 렌더링 이미지 및 3D 포인트 클라우드의 잘 페어링된 3중체는 ULIP-Objaverse 3중체로 릴리스됩니다. ~ 두 번째 데이터 세트는 유명한 합성 데이터 세트인 ShapeNet [4]입니다. 55개의 주석이 달린 범주가 있는 52.5K 3D 모양이 있는 공개적으로 사용 가능한 하위 세트를 사용합니다. 각 모양에 대해 ULIP에 따라 30개의 동일 간격의 시야각을 샘플링하고, 각 시야각에 대해 RGB 이미지와 깊이 맵을 렌더링합니다.이미지 설명 생성 방법은 Objaverse와 동일합니다.이러한 트리플릿을 ULIP-ShapeNet 트리플릿으로 릴리스합니다.추가 구현 세부 정보 및 절제 연구는 부록에 포함되어 있습니다.4.2. 다운스트림 작업 ~ ~ ULIP-2를 벤치마킹하기 위해 ModelNet40[4], Objaverse-LVIS[6] 및 ScanObjectNN[41] 데이터 세트를 사용합니다.ModelNet40은 합성 CAD 모델 데이터 세트입니다.여기에는 ~ 9.8k개의 교육 샘플과 2.5k개의 테스트 샘플이 포함되어 있습니다.ObjaverseLVIS는 인간이 검증한 범주 레이블이 있는 Objaverse 데이터 세트의 하위 집합입니다.1.2k개의 범주에 걸쳐 ~ 46k개의 샘플이 있어 더 어려운 오픈월드 제로샷 3D 모양 분류에 적합합니다.ScanObjectNN은 15개 범주에 2.9k개의 샘플이 있는 실제 3D 데이터 세트입니다. 우리는 ULIP 및 OpenShape에서 사용된 것과 동일한 데이터 세트 설정 및 준비 프로토콜을 따르며, 비교에서 일관성을 보장합니다.~ 우리는 세 가지 다운스트림 작업에 대한 실험을 수행합니다: (1) 다중 모달 입력을 포함하는 제로샷 3D 분류 작업, (2) 단일 모달을 포함하는 표준 3D 분류 작업, (3) LLM을 사용한 3D-언어 생성을 포함하는 3D 캡션 작업.평가 지표.우리는 ULIP에서 사용된 것과 동일한 평가 지표를 채택합니다: 제로샷 3D 분류 작업의 경우 상위 1 및 상위 5 정확도, 표준 3D 분류 작업의 경우 전체 정확도 및 클래스 평균 정확도.새로운 다운스트림 작업인 3D-언어 생성의 경우 X-InstructBLIP[40]을 따르고 CIDER[44] 점수를 사용하여 생성된 캡션의 품질을 정량적으로 평가합니다.백본. 우리는 두 개의 대표적인 백본에서 ULIP-2를 사전 학습시켰습니다.Point-BERT[55]는 ULIP의 제로샷 분류 실험에서 강력한 성능을 보이는 변압기 기반 백본입니다.PointNeXt[36]는 PointNet++[33]에 기반한 가벼운 백본을 제안하고 ScanObjectNN 벤치마크에서 유망한 결과를 제공하는 작업입니다.4.3. 기준선과의 비교 제로샷 3D 분류.우리는 제로샷 3D 분류를 위해 ULIP 및 OpenShape와 동일한 절차를 따르고 [11, 22, 34, 52, 58, 62]를 포함한 기존의 제로샷 접근 방식과 비교합니다.우리는 표 1에 Objaverse-LVIS와 ModelNet40 모두에 대한 제로샷 3D 분류 결과를 제시합니다.먼저, 동일한 데이터 세트에서 사전 학습했을 때 ULIP-2 사전 학습의 이점을 얻는 Point-BERT가 ULIP로 사전 학습한 것보다 상당히 더 나은 결과를 얻는다는 것을 관찰했습니다. 구체적으로, ShapeNet에서 ULIP와 ULIP-2를 모두 사전 학습할 때 ULIP-2는 ULIP보다 ModelNet40 상위 1 정확도를 14.8%, Objaverse-LVIS 상위 1 정확도를 13.8% 능가합니다. Objaverse(LVIS 샘플 제외)와 ShapeNet에서 함께 사전 학습할 경우 ULIP-2는 ModelNet40 상위 정확도에서 ULIP를 15.4%, ObjaverseLVIS 상위 1 정확도에서 ULIP를 24.9% 능가합니다. 이러한 이득은 우리 접근 방식의 효능, 특히 사전 학습 표현 기능을 증폭하는 순위가 매겨진 전체적 관점 언어 설명 및 확장 전략을 강조합니다. 대규모 멀티모달 모델에서 생성된 포괄적인 언어 설명은 방대한 양의 언어 및 이미지 데이터에서 얻은 지식을 캡슐화하여 3D 모양 설명의 의미적 풍부함을 풍부하게 하고 언어와 3D 모달리티 간의 정렬을 강화합니다.또한 ULIP-2의 간단하고 간소화된 프레임워크를 통해 동시 OpenShape[22]를 포함한 기존 기준선보다 우수한 성능을 달성합니다.이는 현재 SOTA(Objaverse-LVIS 상위 1위에서 46.8%)이기도 합니다.표준 3D 분류.표준 3D 분류를 위한 ULIP 및 커뮤니티 프로토콜을 준수합니다. 우리는 표 3에 ScanObjectNN 가장 어려운 세트에 대한 3D 분류 결과를 제시합니다. ULIP와 ULIP-2 프레임워크를 모두 사용하여 동일한 3D 인코더 아키텍처에 대해 Objaverse와 ShapeNet에서 공동으로 사전 학습했을 때, ULIP-2(Point-BERT 백본 사용)가 기준선 방법을 개선한다는 것을 관찰했습니다(X-InstructBLIP(ULIP-2 사용)을 사용한 3D 캡션 사용) 접두사 &quot;3d:&quot; 명령어 조정 LLM☑ 3D ☐☐☐ 명령어 임베딩 3D Q-Former ULIP-Encoder 3D 입력 쿼리 ☐ ☐ ☐ ☐ 명령어-응답 I 재사용 사용자: &quot;3D 모델 설명&quot; X-InstructBLIP 3D 포인트 클라우드 입력(ULIP 사용) 그림 3. &quot;테이블과 의자의 3D 모델&quot; X-InstructBLIP(ULIP-2 사용) &quot;테이블 옆에 서 있는 사람의 3D 모델&quot; &quot; &quot;나무 블록 위의 남자 머리 3D 모델&quot; 수염을 기른 남자의 흉상&quot; &quot;창문이 있는 나무 문의 3D 모델&quot; &quot;칼의 3D 모델&quot; &quot;아치형 창문의 3D 모델&quot; &quot;나무 손잡이가 있는 칼의 3D 모델&quot; XInstructBLIP 프레임워크를 사용한 3D-언어 멀티모달 생성 [40]. 모델(M) PointNet [32] 3.68.PointNet++ [33] 1.77.#Params 전체 클래스 평균 정확도 정확도 63.75.DGCNN [49] 1.78.73.MVTN [9] 11.82.RepSurf-U [38] 1.84.Point-MAE [31] 22.85.PointMLP [26] 12.85.84.Point-M2AE [57] 15.86.PointCMT [53] 12.86.84.ACT [7] 22.88.P2P [47] 89.Recon-s [34] 19.89.12.90.Point-BERT(공식) 22.83.Point-BERT(ULIP 포함) 22.88.Point-BERT(ULIP-2 포함) 22.89.PointNeXt(처음부터) 1.87.85.PointNeXt(ULIP 포함) 1.90.89.Point NeXt(ULIP-2 포함) 1.91.90.PointNeXt(ULIP-2 포함)* 1.91.90.I2P-MAE [59] | | | 91.5%의 성능을 보이며 140만 개의 매개변수로 ScanObjectNN 벤치마크에서 새로운 기록을 수립했습니다. 3D-to-Language 생성. 그림 3에 나와 있듯이, 우리는 X-InstructBLIP 방법론[40]을 채택하여 ULIP-2 사전 학습된 인코더를 동결된 대용량 언어 모델(LLM)과 통합하여 3D 데이터로부터 언어를 생성하는 기능을 부여합니다. 3D-언어 생성 능력을 공정하게 비교하기 위해 동결된 Point-BERT 모델을 X-InstructBLIP의[40] 프레임워크에 연결합니다. 이 모델은 동일한 사전 학습 데이터 세트(Objaverse + ShapeNet)를 사용하여 ULIP 및 ULIP-2 프레임워크에서 사전 학습됩니다. 그런 다음 [40]에 따라 3D 캡션 기능을 벤치마킹합니다. 이 평가 동안 다른 모든 변수는 일정하게 유지됩니다. 캡션 성능은 모델의 캡션 성능에 대한 정량적 분석을 제공하는 PyCOCOTools Cider Score[44]를 사용하여 측정됩니다. 표 4는 ULIP-2 사전 학습된 인코더가 캡션 점수를 28.3%만큼 크게 향상시킬 수 있음을 보여주고, 그림 3은 ULIP-2 사전 학습된 모델을 사용하여 생성된 캡션이 더 정확하고 설명적임을 정성적으로 보여줍니다.다중 모달 생성 프레임워크 X-InstructBLIP X-InstructBLIP CIDER 점수 Frozen 3D 인코더 PB w/ ULIP PB w/ ULIP-132.160.표 4. 동일한 Objaverse + ShapeNet 데이터 세트 설정에서 사전 학습된 X-InstructBLIP [40]을 사용한 3D-언어 생성. ULIP-2가 있는 PB는 ULIP-2 프레임워크로 사전 학습된 Point-BERT를 의미합니다.5. 절제 연구 5.1. 생성된 캡션의 효과에 대한 절제 생성된 캡션이 성능에 어떻게 기여하는지 절제하기 위해 ULIP 설정에 맞춰 실험을 수행했지만 언어 모달리티만 하나 주요 수정했습니다. ULIP의 수동 설명을 사용하는 대신 BLIP-2에서 생성된 상위 1위의 전체적 뷰 캡션을 활용했습니다.표 5의 결과는 이러한 생성된 캡션을 사용할 때 ModelNet40에서 제로샷 분류가 상당히 향상되었음을 보여주며, ULIP에서 사용된 수동 캡션과 비교했을 때 중요한 영향을 보여줍니다.사전 학습 언어 모달 ModelNettop-top-Manual 캡션 60.84.상위 1 전체적 BLIP-2 캡션 69.88.표 3. ScanObjectNN의 3D 분류 결과.ULIP-2는 기준선보다 상당히 우수한 성능을 보입니다.*는 투표[26]를 사용한다는 것을 의미합니다.멀티모달 사전 학습보다 6.6% 더 우수합니다. PointNext 백본을 사용하여 ULIP-2는 처음부터 학습하는 것보다 4.0%의 상당한 성능 이득을 얻어 전체적으로 정확한 5. SLIP ViT-B 인코더(ULIP에서 사용됨)가 있는 ShapeNet에서 사전 학습된 ModelNet40의 Point-BERT 제로샷 3D 분류. 5.2. 다양한 대규모 멀티모달 모델 대규모 멀티모달 모델의 언어 설명 품질이 3D 표현 사전 학습에서 중요한 역할을 한다는 점을 고려하여 이러한 두 모델에 대한 절제 연구를 수행합니다. 위의 벤치마킹 실험 전체에서 BLIP-2[18]를 사용합니다. 여기서 ShapeNet에서 사전 학습된 Point-BERT 백본을 사용하는 제로샷 3D 분류 작업에 대해 이를 이전 버전인 BLIP[17]과 비교합니다. 표 6의 결과는 BLIP-2에서 생성된 설명을 사용하면 진화된 이미지 이해 기능 덕분에 BLIP보다 더 나은 결과를 얻을 수 있음을 보여주며, 대규모 멀티모달 모델이 발전함에 따라 ULIP-2의 성능도 그에 따라 향상될 것으로 예상할 수 있다. 대규모 멀티모달 모델 BLIP[17] BLIP-2[18] 상위-1 상위-67.7 88.69.88.표 6. ModelNet40에서의 제로샷 3D 분류.SLIP VIT-B 인코더가 있는 ShapeNet에서 사전 학습됨.선택된 상위 k BLIP-2 캡션정확도 상위-상위-69.88.66.87.66.87.66.85.표 8. SLIP ViT-B가 있는 ShapeNet에서 사전 학습된 ModelNet40에서의 ULIP-2 제로샷 3D 분류. 예를 들어, 선택된 상위 5개 BLIP 캡션은 사전 학습에서 언어 모달리티로 상위 5개 CLIP CLIP 순위 캡션을 앙상블한다는 것을 의미합니다. 구성은 성능과 모델 크기 간의 균형을 맞추기 위해 선택한 설정입니다. 5.3. 3D 객체당 2D 뷰 수 사전 학습에서 상위 1개 BLIP-2 캡션이 있는 전체적 뷰 수와 관련하여 제로 샷 3D 분류 성능에 대한 절제 연구를 추가로 수행합니다. 표 7의 결과는 뷰 수가 증가함에 따라 제로 샷 분류 정확도가 그에 따라 증가함을 보여줍니다. 이는 전체적 뷰에 대한 다양한 언어 설명이 멀티모달 3D 표현 학습에 도움이 된다는 우리의 주장을 입증합니다. # 전체적인 뷰 정확도 상위 1 상위 54.8 77.58.80.69.3 88.69.7 88.표 7. SLIP ViT-B 인코더가 있는 ShapeNet에서 사전 학습된 ModelNet40의 제로샷 3D 분류.5.4. 2D 뷰당 상위 k CLIP 순위 캡션 상위 1 BLIP-2 캡션 전략의 효과를 평가하기 위해 다중 모달 사전 학습을 위해 독립적으로 생성된 10개의 BLIP-2 캡션 중 다른 상위 k를 선택하는 절제 연구를 수행했습니다.표 8의 결과는 상위 1 CLIP 점수 순위 캡션을 사용하면 최상의 성능을 얻을 수 있음을 나타냅니다.이는 직관적으로 말이 됩니다.상위 CLIP 점수 캡션은 노이즈 방지 기능이 더 뛰어나 우리 맥락에서 다중 모달 학습에 유리합니다.5.5. 백본 모델 확장 우리는 CLIP 모델과 3D 백본 모델의 크기를 늘리는 것이 성능에 미치는 효과를 조사했습니다. 표 9에서 알 수 있듯이, 더 큰 CLIP 모델은 결과를 개선합니다. 3D 백본의 경우 성능은 약 32.5M 매개변수에서 최고치를 기록하고, 그 이상에서는 이득이 감소합니다. 따라서 이 CLIP 3D 인코더 ModelNetObjaverse-LVIS 크기 #Params(M) 상위 1 상위 5 상위 1 상위 ViT-B 21.ViT-G 21.71.4 89.76.3 94.1 35.28.52.62.ViT-G 5.75.0 94.7 34.61.ViT-G 21.76.3 94.35.62.ViT-G 32.77.0 94.0 35.62.ViT-G 43.ViT-G 85.76.8 94.8 35.76.5 94.7 35.62.62.표 9. ModelNet40 및 ObjaverseLVIS의 제로샷 3D 분류, 모든 모델은 사전 학습된 Point-BERT 모델입니다. Objaverse(no-LVIS). 강조된 회색 선은 ULIP-2를 더 큰 Objaverse 데이터 세트로 확장하는 데 사용하는 모델 설정입니다. 가장 작은 5.3M 모델은 ShapeNet에서만 사전 학습된 경우에 사용됩니다. 6.
--- CONCLUSION ---
토론 다중 모드 3D 표현 학습을 위한 새로운 프레임워크인 ULIP-2를 제시합니다. 언어 설명 생성을 위한 대규모 다중 모드 모델을 활용하고 다중 모드 3D 사전 학습을 확장함으로써 ULIP-2는 기존 다중 모드 3D 데이터 세트의 품질 및 확장성 과제를 해결할 뿐만 아니라 모든 다운스트림 작업에서 상당한 개선을 보여줍니다. 또한 추가 연구를 촉진하기 위한 두 개의 대규모 삼중 모드 데이터 세트인 &quot;ULIP-Objaverse&quot; 트리플릿과 &quot;ULIP-ShapeNet&quot; 트리플릿을 출시합니다. 한계. ULIP-2의 사전 학습은 주로 개체 수준 3D 모양 데이터 세트를 활용하는데, 이는 본질적으로 분포와 복잡성 면에서 장면 수준 3D 데이터와 다릅니다. 장면 수준 3D 데이터 이해에 ULIP-2 프레임워크를 적용하는 방법을 탐구하고 이를 위해 개체 수준 3D 데이터에서 얻은 지식을 활용하는 것은 미래 연구를 위한 매력적인 경로입니다. 더 광범위한 영향. ULIP-2는 3D 멀티모달 사전 훈련에서 인간 주석을 최소화하여 노동력을 줄이면서도 저숙련 일자리 시장에 영향을 미칠 수 있는 것을 목표로 합니다. AI 발전에 대한 일반적인 우려인 이러한 이중적 영향은 AI 연구에서 보다 광범위한 고려 사항의 필요성을 강조합니다. 참고문헌 [1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang. 이미지 캡션 및 시각적 질의응답을 위한 하향식 및 상향식 주의. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6077-6086쪽, 2018.[2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, Silvio Savarese. 대규모 실내 공간의 3D 의미 구문 분석. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 1534-1543쪽, 2016년.[3] Cesar Cadena, Anthony R Dick, Ian D Reid. 로봇 장면 이해를 위한 공동 추정기로서의 멀티모달 자동 인코더. Robotics: Science and systems, 2016년.[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: 정보가 풍부한 3D 모델 저장소. arXiv 사전 인쇄본 arXiv:1512.03012, 2015년. 1,[5] Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal. 텍스트 생성을 통한 비전 및 언어 작업 통합. International Conference on Machine Learning, 1931-1942쪽. PMLR, 2021.[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi. Objaverse: 주석이 달린 3D 객체의 우주. arXiv 사전 인쇄본 arXiv:2212.08051, 2022.[7] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, Kaisheng Ma. 교차 모달 교사로서의 자동 인코더: 사전 학습된 2D 이미지 변환기가 3D 표현 학습에 도움이 될 수 있을까? arXiv 사전 인쇄본 arXiv:2212.08320, 2022.[8] Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten. 부분 다양체 희소 합성 신경망을 사용한 3차원 의미적 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9224-9232페이지, 2018.[9] Abdullah Hamdi, Silvio Giancola, Bernard Ghanem. Mvtn: 3차원 모양 인식을 위한 다중 시점 변환 네트워크. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 1-11페이지, 2021.[10] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham. Randla-net: 대규모 포인트 클라우드의 효율적인 의미적 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1110811117페이지, 2020.[11] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang, Wangmeng Zuo. Clip2point: 이미지 심도 사전 학습을 통한 포인트 클라우드 분류로 클립 전송. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 2023. 5,[12] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu. 상자 밖에서 보기: 비전 언어 표현 학습을 위한 종단 간 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12976-12985페이지, 2021.[13] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt. Openclip, 2021. 이 소프트웨어를 사용하는 경우 아래와 같이 인용해 주십시오.[14] Brian R Kent. Blender®를 사용한 3D 과학적 시각화. Morgan &amp; Claypool Publishers, 2015.[15] Wonjae Kim, Bokyung Son, Ildoo Kim. Vilt: 합성곱이나 영역 감독이 없는 비전 및 언어 변환기. 기계 학습 국제 컨퍼런스, 5583-5594쪽. PMLR, 2021.[16] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu Hong Hoi. 융합 전 정렬: 모멘텀 증류를 통한 비전 및 언어 표현 학습. 신경 정보 처리 시스템의 발전, 34:9694-9705, 2021. 3,[17] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. Blip: 통합 비전 언어 이해 및 생성을 위한 언어 이미지 사전 학습 부트스트래핑. 기계 학습 국제 컨퍼런스, 12888-12900페이지. PMLR, 2022. 4,[18] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip2: 동결된 이미지 인코더 및 대규모 언어 모델을 사용하여 언어 이미지 사전 학습 부트스트래핑. 기계 학습 국제 컨퍼런스. PMLR, 2023. 4, 5, 6,[19] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang. Unimo: 교차 모달 대조 학습을 통한 통합 모달 이해 및 생성을 향해. Association for Computational Linguistics 연례 회의, 2592-2607쪽, 2021.[20] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al. Deepfusion: 다중 모달 3D 객체 감지를 위한 Lidar-카메라 딥 퓨전. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 17182-17191쪽, 2022.[21] Zhichao Li, Feng Wang, Naiyan Wang. Lidar r-cnn: 효율적이고 보편적인 3D 객체 감지기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7546-7555페이지, 2021.[22] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, Hao Su. Openshape: 오픈 월드 이해를 향한 3D 모양 표현 확장. 신경 정보 처리 시스템의 발전, 2023. 2, 4, 5,[23] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, Chunhong Pan. Densepoint: 효율적인 포인트 클라우드 처리를 위한 밀도 있는 맥락적 표현 학습. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 5239-5248페이지, 2019.[24] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong. 변압기를 통한 그룹 없는 3D 객체 감지. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 2949-2958페이지, 2021.[25] Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh. 신경 아기말. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7219-7228페이지, 2018.[26] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, Yun Fu. 포인트 클라우드에서 네트워크 설계 및 로컬 지오메트리 재고: 간단한 잔여 MLP 프레임워크. arXiv 사전 인쇄본 arXiv:2202.07123, 2022.[27] Ishan Misra, Rohit Girdhar, Armand Joulin. 3D 객체 감지를 위한 엔드투엔드 변압기 모델. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 2906-2917페이지, 2021.[28] Norman Mu, Alexander Kirillov, David Wagner, Saining Xie. Slip: 자기 감독이 언어-이미지 사전 학습을 만납니다. 유럽 컴퓨터 비전 컨퍼런스, 529-544페이지. Springer, 2022. 3,[29] OpenAI. Gpt-4 기술 보고서. OpenAI 블로그, 2023.[30] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles. X-instructblip: x-모달 명령어 인식 표현을 Ilms 및 새로운 교차 모달 추론에 맞추기 위한 프레임워크. arXiv 사전 인쇄본 arXiv:2311.18799, 2023.[31] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, Li Yuan. 포인트 클라우드 자기 지도 학습을 위한 마스크 자동 인코더. arXiv 사전 인쇄본 arXiv:2203.06604, 2022.[32] Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas. Pointnet: 3D 분류 및 분할을 위한 포인트 집합에 대한 딥 러닝. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 652-660페이지, 2017. 4,[33] Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J Guibas. Pointnet++: 메트릭 공간의 포인트 집합에 대한 딥 계층적 특징 학습. 신경 정보 처리 시스템의 발전, 30, 2017. 4, 6,[34] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma 및 Li Yi. 재구성과 대조: 생성적 사전 학습을 통해 안내되는 대조 3D 표현 학습. 기계 학습에 관한 국제 컨퍼런스(ICML), 2023. 2, 4, 5, 6,[35] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin 및 Hengshuang Zhao. Gpt4point: 포인트 언어 이해 및 생성을 위한 통합 프레임워크입니다. arXiv 사전 인쇄 arXiv:2312.02980, 2023.[36] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, Bernard Ghanem. Pointnext: 개선된 학습 및 확장 전략을 사용한 pointnet++ 재검토. arXiv 사전 인쇄본 arXiv:2206.04670, 2022. 4, 6,[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. International Conference on Machine Learning, 8748-8763페이지. PMLR, 2021. 3, 5,[38] Haoxi Ran, Jun Liu, Chengjie Wang. 포인트 클라우드의 표면 표현. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18942-18952페이지, 2022.[39] Manli Shu, Le Xue, Ning Yu, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong 및 Ran Xu. 3D 객체 감지를 위한 모델 독립적 계층적 주의. arXiv 사전 인쇄본 arXiv:2301.02650, 2023.[40] 익명 제출. X-InstructBLIP: X-모달 명령어 인식 표현을 LLM 및 새로운 교차 모달 추론에 맞추기 위한 프레임워크, 2023. 6,[41] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen 및 Sai-Kit Yeung. 포인트 클라우드 분류 재검토: 실제 데이터에 대한 새로운 벤치마크 데이터 세트 및 분류 모델. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 회의록, 2019.[42] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, Sai-Kit Yeung. 포인트 클라우드 분류 재검토: 실제 데이터에 대한 새로운 벤치마크 데이터 세트 및 분류 모델. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 1588-1597페이지, 2019.[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017.[44] Ramakrishna Vedantam, C Lawrence Zitnick, Devi Parikh. Cider: 합의 기반 이미지 설명 평가. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4566-4575페이지, 2015. 6,[45] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, Chang D Yoo. 포인트 클라우드에서 3D 인스턴스 분할을 위한 Softgroup. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2708-2717페이지, 2022.[46] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao. Simvlm: 약한 감독을 통한 간단한 시각 언어 모델 사전 학습. arXiv 사전 인쇄본 arXiv:2108.10904, 2021.[47] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu. P2p: 포인트-픽셀 프롬핑을 사용한 포인트 클라우드 분석을 위한 사전 학습된 이미지 모델 조정. arXiv 사전 인쇄본 arXiv:2208.02812, 2022.[48] Christian Wojek, Stefan Walk, Stefan Roth, Bernt Schiele. 명시적 폐색 추론을 사용한 단안 3D 장면 이해. CVPR 2011, 1993-2000페이지. IEEE, 2011.[49] Bo Wu, Yang Liu, Bo Lang, Lei Huang. Dgcnn: 가우스 혼합 모델을 기반으로 한 무질서 그래프 합성 신경망. Neurocomputing, 321:346–356, 2018.[50] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao. 3d shapenets: 체적 모양을 위한 심층 표현. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1912-1920쪽, 2015. 1,[51] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin. Pointllm: 포인트 클라우드를 이해하기 위한 대규모 언어 모델 강화. arXiv 사전 인쇄본 arXiv:2308.16911, 2023.[52] Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese. Ulip: 3D 이해를 위한 언어, 이미지 및 포인트 클라우드의 통합 표현 학습. arXiv 사전 인쇄 arXiv:2212.05171, 2022. 2, 3, 4, 5, 6,[53] Xu Yan, Heshen Zhan, Chaoda Zheng, Jiantao Gao, Ruimao Zhang, Shuguang Cui 및 Zhen Li. 이미지를 통해 더 많은 것을 얻을 수 있습니다. 형상 분석을 위한 포인트 클라우드 교차 모달 교육. arXiv 사전 인쇄 arXiv:2210.04208, 2022.[54] 티안웨이 인(Tianwei Yin), 저우싱이(Xingyi Zhou), 필립 크라헨불(Philipp Krahenbuhl). 중앙 기반 3D 객체 감지 및 추적. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 11784-11793, 2021.[55] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou 및 Jiwen Lu. Point-bert: 마스크된 포인트 모델링을 사용한 3D 포인트 클라우드 변환기 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 19313-19322페이지, 2022. 4, 5, 6,[56] Yan Zeng, Xinsong Zhang, Hang Li. 다중 그레인 비전 언어 사전 학습: 시각적 개념에 맞게 텍스트 정렬. arXiv 사전 인쇄본 arXiv:2111.08276, 2021.[57] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li. Point-m2ae: 계층적 포인트 클라우드 사전 학습을 위한 다중 스케일 마스크된 자동 인코더. arXiv 사전 인쇄본 arXiv:2205.14401, 2022.[58] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li. Pointclip: 클립을 통한 포인트 클라우드 이해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8552-8562페이지, 2022. 5,[59] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, Hongsheng Li. 이미지-포인트 마스크 자동 인코더를 통해 2d 사전 학습된 모델에서 3d 표현 학습. 사전 인쇄본 arXiv:2212.06785, 2022. 4, arXiv [60] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J Corso, Marcus Rohrbach. 접지된 비디오 설명. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6578-6587페이지, 2019.[61] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso 및 Jianfeng Gao. 이미지 캡셔닝 및 VQA를 위한 통합 비전 언어 사전 학습. AAAI 인공 지능 컨퍼런스 회의록, 2020.[62] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang 및 Peng Gao. Pointclip v2: 강력한 3D 오픈 월드 학습을 위한 프롬프팅 클립 및 gpt. arXiv 사전 인쇄본 arXiv:2211.11682, 2022. 5, ULIP-2: 3D 이해를 위한 확장 가능한 멀티모달 사전 학습을 향하여 보충 자료 A. 부록 A.1. 3D 입력에 대한 소거 3D 입력으로 10k 컬러 포인트 클라우드를 활용하는 Objaverse-LVIS 벤치마크에서 OpenShape와 공정하게 비교하기 위해 OpenShape와 동일한 3D 입력 사전 처리를 채택합니다. 이 소거 연구는 색상 정보가 Objaverse-LVIS에서 제로 샷 분류 결과에 어떤 영향을 미치는지 평가하기 위해 수행됩니다. 이를 위해 Objaverse + ShapeNet에서 사전 학습된 ULIP 사전 학습된 Point-BERT 모델을 평가했습니다. 표 10의 결과에 따르면 색상 정보를 사용하지 않아도 ULIP가 Objaverse-LVIS 제로 샷 분류 작업에서 강력한 성능을 유지합니다. Objaverse-LVIS 3D 인코더 입력 top-top-8k xyz 48.77.10k xyzrgb 50.79.표 10. Objaverse-LVIS에서 ULIP-2 제로샷 3D 분류를 적용한 Point-BERT, OpenCLIP ViT-G 인코더와 함께 Objaverse 및 ShapeNet에서 사전 학습됨.A.2. 다양한 종류의 3D 백본 ULIP-2의 개선이 3D 백본과 무관한지 확인하기 위해 PointNext 백본에서 실험을 수행했습니다.표 11의 결과는 본질적으로 다른 또 다른 종류의 3D 백본을 사용하여 ULIP-2가 여전히 성능을 크게 개선할 수 있음을 보여줍니다.Point-BERT가 확장 친화적인 변압기 기반 아키텍처이고 더 나은 제로샷 분류 결과를 제공한다는 점을 감안할 때, 우리는 모든 실험에서 주로 Point-BERT에서 실험을 수행합니다. 모델 사전 학습 방법 ModelNettop-1 top-PointNeXt [36] ULIP [52] ULIP-56.2 77.72.8 95.Point-BERT [55] ULIP [52] ULIP-60.4 84.75.2 95.표 11. SLIP ViT-B 인코더가 있는 ShapeNet에서 사전 학습된 다양한 3D 백본을 갖춘 ModelNet40의 제로샷 3D 분류.
