--- ABSTRACT ---
텍스트-3D 생성은 최근 수십억 개의 이미지-텍스트 쌍에서 학습된 2D 확산 모델에 힘입어 상당한 주목을 받았습니다. 기존 방법은 주로 스코어 증류에 의존하여 2D 확산 사전 확률을 활용하여 3D 모델(예: NeRF) 생성을 감독합니다. 그러나 스코어 증류는 뷰 불일치 문제가 발생하기 쉽고 암묵적 NeRF 모델링은 임의의 모양으로 이어질 수 있으므로 덜 현실적이고 제어할 수 없는 3D 생성으로 이어질 수 있습니다. 이 연구에서는 2D 및 3D 확산 모델에서 지식을 증류하여 희소하지만 자유롭게 사용할 수 있는 3D 포인트와 현실적인 모양 제어 가능한 3D 생성 간의 격차를 메우는 유연한 포인트-3D 프레임워크를 제안합니다. 포인트-3D의 핵심 아이디어는 텍스트-3D 생성을 안내하기 위해 제어 가능한 희소 3D 포인트를 도입하는 것입니다. 구체적으로, 우리는 3D 확산 모델인 Point-E에서 생성된 희소한 포인트 클라우드를 기하학적 사전으로 사용하고, 단일 참조 이미지를 조건으로 합니다. 더 나은 이 작업의 일부 또는 전부를 개인 또는 교실용으로 디지털 또는 하드 카피로 만드는 것은 이익 또는 상업적 이익을 위해 사본을 만들거나 배포하지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문을 표시하는 경우 무료로 허가됩니다. ACM 이외의 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재발행하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. ACM MM &#39;23 컨퍼런스, 2023년 10월 29일-11월 3일, 캐나다 오타와 © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.https://doi.org/XXXXXXX.XXXXXXX 희소 3D 포인트를 활용하여 NeRF의 지오메트리를 희소 3D 포인트의 모양과 일치하도록 적응적으로 구동하는 효율적인 포인트 클라우드 안내 손실을 제안합니다. 지오메트리를 제어하는 것 외에도 NeRF를 최적화하여 뷰 일관성이 더 높은 모양을 제안합니다. 구체적으로, 학습된 컴팩트 지오메트리의 텍스트와 깊이 맵에 따라 공개적으로 사용 가능한 2D 이미지 확산 모델 ControlNet에 대한 점수 증류를 수행합니다. 정성적 및 정량적 비교를 통해 Points-to-3D가 뷰 일관성을 개선하고 텍스트-to-3D 생성을 위한 양호한 모양 제어성을 달성한다는 것을 보여줍니다. Points-to-3D는 사용자에게 텍스트-to-3D 생성을 개선하고 제어하는 새로운 방법을 제공합니다. CCS 개념 ⚫ 컴퓨팅 방법론 → 가시성; 모양 및 텍스처 표현. 키워드 텍스트-3D, 확산 모델, NeRF, 포인트 클라우드 ACM 참조 형식: Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, Fan Wang, DAMO Academy, Alibaba Group, . 2018. Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation. 회의록에서 올바른 회의 제목을 입력하세요. 권리 회의 ACM MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 오타와 Trovato et al. 확인 이메일(회의 ACM MM &#39;23). ACM, 뉴욕, 뉴욕, 미국, 10페이지. https://doi.org/XXXXXXX.XXXXXXX
--- INTRODUCTION ---
최근, 텍스트-이미지 생성 분야에서 경이로운 발전이 이루어졌는데[38, 39, 42, 44, 59], 주로 대규모 정렬된 이미지-텍스트 데이터 세트[47], 시각-언어 사전 학습 모델[20, 24, 37], 확산 모델[9, 15, 42]에서의 중요한 성과 덕분입니다. 이러한 텍스트-이미지 생성 결과에서 영감을 받아 많은 연구에서 텍스트-조건부 확산 모델을 다른 모달리티(예: 텍스트-비디오[16, 17, 48] 및 텍스트-3D[19, 25, 28, 36, 54])에서 탐구했습니다. 이 연구에서는 3D 콘텐츠를 생성하고 게임, 가상 또는 증강 현실, 로봇 응용 프로그램 등 많은 응용 프로그램에 잠재적으로 적용될 수 있는 텍스트-3D 생성 분야에 특히 초점을 맞춥니다. 텍스트-3D 생성 모델을 훈련하는 것은 2D 이미지에 비해 풍부한 텍스트와 3D 데이터 쌍을 얻는 것이 어렵기 때문에 어려울 수 있습니다. 가장 최근에 DreamFusion[36]은 사전 훈련된 2D 텍스트-이미지 확산 모델[44]에서 스코어 증류를 사용하여 Neural Radiance Fields(NeRF)[29]를 최적화하여 텍스트-3D 합성을 수행하여 이 과제를 처음으로 해결했습니다. 다음 문헌[28, 54]도 스코어 증류 패러다임을 사용합니다. 이러한 방법은 3D 감독이 필요 없이 텍스트-3D 콘텐츠 생성을 위한 솔루션을 제공하고 검증합니다. 상당한 약속에도 불구하고 이러한 방법은 다중면 문제 또는 Janus 문제라고 하는 주목할 만한 문제로 인해 어려움을 겪습니다. 이는 뷰 간 불일치를 초래합니다. 게다가, 텍스트-3D 생성에서 또 다른 중요한 문제는 생성된 3D 객체의 모양에 대한 제어가 부족하다는 것입니다.즉, 이러한 방법은 다른 시드를 설정하여 입력 텍스트의 요구 사항을 충족하는 임의의 모양의 객체를 생성할 수 있습니다.Latent-NeRF[28]는 먼저 스케치 모양 가이드 3D 생성을 도입했는데, 이는 미리 정의된 메시를 대상으로 사용하여 NeRF의 기하 학습을 감독합니다.그러나 이 접근 방식은 매번 각 3D 생성에 대한 메시 모양을 미리 정의해야 하므로 비용이 많이 들고 시간이 많이 걸립니다.이것은 텍스트-3D 생성의 모양과 기하 학습을 모두 안내하기 위해 2D 및 3D 확산 모델 모두에서 사전 지식을 개발할 가능성을 탐구하도록 동기를 부여했습니다. 영어: 텍스트-이미지 확산 모델(예: ControlNet[59] 및 T2I-Adapter[32])의 조건부 제어 패러다임에서 영감을 받아 텍스트 프롬프트와 함께 추가 조건(예: 스케치, 마스크, 깊이)을 사용하여 생성 프로세스를 안내하여 이미지의 제어 가능성과 공간적 일관성을 더욱 높였습니다. 이 조건부 제어 메커니즘을 텍스트-3D 생성에 통합하는 방법을 모색합니다. 이 연구에서는 Points-to-3D라는 새롭고 유연한 프레임워크를 제안하여 뷰 간 뷰 일관성을 개선하고 텍스트-3D 생성을 위한 3D 모양에 대한 유연한 제어 가능성을 달성합니다. Points-to-3D의 핵심 아이디어는 기하학 및 모양 측면에서 텍스트-3D 생성을 안내하는 제어 가능한 희소 3D 포인트를 도입하는 것입니다. 이를 달성하기 위해 Point-E[35]에서 영감을 받아 사전 학습된 3D 포인트 클라우드 확산 모델에서 희소 포인트 클라우드를 기하학 사전으로 추출하는 것을 제안합니다. 이러한 희소 3D 포인트는 사용자가 제공하거나 텍스트-이미지 모델에서 생성할 수 있는 단일 참조 이미지에 따라 조건이 지정됩니다. 그러나 4096개의 3D 포인트만 포함된 생성된 희소 포인트 클라우드를 활용하는 것은 간단하지 않습니다. 이 문제를 극복하기 위해 임의로 초기화된 NeRF의 지오메트리가 참조 이미지에 묘사된 모양과 매우 유사하도록 장려하는 포인트 클라우드 안내 손실을 제안합니다. 지오메트리 외에도 텍스트 프롬프트와 학습된 깊이 맵에 따라 조건 지정된 모양을 최적화할 것을 제안합니다. 보다 구체적으로, 컴팩트한 잠재 공간에서 공개적으로 사용 가능하고 제어하기 쉬운 2D 이미지 확산 모델인 ControlNet[59]에 대한 점수 증류[28, 36]를 수행합니다. Points-to-3D라는 접근 방식은 2D 및 3D 확산 사전에 대한 지식을 증류하여 희소 3D 포인트와 사실적인 모양 제어가 가능한 3D 생성 간의 격차를 메울 수 있습니다. 그림 1에 나와 있듯이, 상상적인 참조 이미지가 주어지면 Points-to-3D는 다양한 텍스트 프롬프트에 따라 달라지는 사실적이고 모양 제어가 가능한 3D 콘텐츠를 생성할 수 있습니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • Points-to-3D라는 새롭고 유연한 텍스트-3D 생성 프레임워크를 제시합니다. 이 프레임워크는 사전 훈련된 2D 및 3D 확산 모델에서 지식을 추출하여 희소 3D 포인트와 보다 사실적이고 모양 제어가 가능한 3D 생성 간의 격차를 메웁니다. • 희소 3D 포인트를 최대한 활용하기 위해 효율적인 포인트 클라우드 유도 손실을 제안하여 NeRF의 지오메트리를 최적화하고 텍스트와 학습된 깊이 맵에 따라 ControlNet을 사용하여 점수 증류를 통해 지오메트리 일관성 있는 모양을 학습합니다. • 실험 결과에 따르면 Points-to-3D는 뷰 간 불일치를 크게 완화하고 텍스트-3D 생성을 위해 3D 모양에 대한 우수한 제어성을 달성할 수 있습니다.
--- RELATED WORK ---
텍스트-이미지 생성. 이미지 생성은 실제 이미지와 구별할 수 없는 이미지를 합성하도록 생성기를 훈련시키는 생성적 적대 신경망(GAN) [13, 21]을 만났을 때 첫 번째 획기적인 결과를 달성합니다. 최근 이미지 생성은 확산 모델 [49]의 개발로 또 다른 경이로운 진전을 이루었습니다. 모델링의 개선 [9, 15]으로 인해 노이즈 제거 확산 모델은 노이즈가 있는 이미지의 노이즈를 반복적으로 제거하여 다양한 고품질 이미지를 생성할 수 있습니다. 이미지 기반 무조건 생성 모델 외에도 확산 모델은 텍스트 설명에서 텍스트 조건 이미지를 생성할 수 있습니다 [38, 44]. 다음 연구에서는 의미 분할 [42], 참조 이미지 [43], 스케치 [53], 깊이 맵 [32, 59] 및 기타 조건 [18, 32, 59]을 포함하여 텍스트-이미지 생성에 더 많은 조건을 추가하는 것을 제안합니다. 이는 텍스트-이미지 생성의 개발 및 응용을 크게 촉진합니다. 텍스트-이미지 확산 모델의 성공에 힘입어 많은 연구에서 텍스트 기반 조작[4], 텍스트-비디오[17, 48], 텍스트-3D[25, 28, 36, 54]와 같은 다른 모달리티에서 텍스트 조건부 확산 모델을 탐구했습니다. 이 연구에서는 텍스트-3D 생성 분야에 중점을 둡니다. 신경 광도장(NeRF). 3D 폭셀 그리드[51], 메시[11], 포인트 클라우드[1, 27, 30, 60], 암묵적 NeRF[29, 34]를 포함하여 3D 장면 표현에 대한 연구가 많이 있습니다. 최근 몇 년 동안 일련의 역 렌더링으로
--- METHOD ---
s는 주로 점수 증류에 의존하여 2D 확산 사전 확률을 활용하여 3D 모델(예: NeRF) 생성을 감독합니다. 그러나 점수 증류는 뷰 불일치 문제가 발생하기 쉽고 암묵적 NeRF 모델링은 임의의 모양으로 이어질 수 있으므로 덜 현실적이고 제어할 수 없는 3D 생성으로 이어질 수 있습니다. 이 연구에서는 2D 및 3D 확산 모델에서 지식을 증류하여 희소하지만 자유롭게 사용할 수 있는 3D 점과 현실적인 모양 제어 가능한 3D 생성 간의 격차를 메우는 Points-to-3D의 유연한 프레임워크를 제안합니다. Points-to-3D의 핵심 아이디어는 제어 가능한 희소 3D 점을 도입하여 텍스트-3D 생성을 안내하는 것입니다. 구체적으로, 단일 참조 이미지에 조건부로 3D 확산 모델인 Point-E에서 생성된 희소 포인트 클라우드를 기하학적 사전 확률로 사용합니다. 더 나은 허가를 위해 이 작품의 일부 또는 전부를 개인 또는 교실용으로 디지털 또는 하드 카피로 만드는 것은 이익 또는 상업적 이익을 위해 사본을 만들거나 배포하지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문을 표시하는 경우 무료로 허가됩니다. ACM이 아닌 다른 사람이 소유한 이 작품의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. ACM MM &#39;23 컨퍼런스, 2023년 10월 29일-11월 3일, 캐나다 오타와 © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.https://doi.org/XXXXXXX.XXXXXXX 희소 3D 포인트를 활용하여 NeRF의 지오메트리를 희소 3D 포인트의 모양과 일치하도록 적응적으로 구동하는 효율적인 포인트 클라우드 안내 손실을 제안합니다. 지오메트리를 제어하는 것 외에도 NeRF를 최적화하여 뷰 일관성이 더 높은 모양을 제안합니다. 구체적으로, 학습된 컴팩트 지오메트리의 텍스트와 깊이 맵에 따라 공개적으로 사용 가능한 2D 이미지 확산 모델 ControlNet에 대한 점수 증류를 수행합니다. 정성적 및 정량적 비교를 통해 Points-to-3D가 뷰 일관성을 개선하고 텍스트-to-3D 생성을 위한 양호한 모양 제어성을 달성한다는 것을 보여줍니다. Points-to-3D는 사용자에게 텍스트-to-3D 생성을 개선하고 제어하는 새로운 방법을 제공합니다. CCS 개념 ⚫ 컴퓨팅 방법론 → 가시성; 모양 및 텍스처 표현. 키워드 text-to-3D, 확산 모델, NeRF, 포인트 클라우드 ACM 참조 형식: Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, Fan Wang, DAMO Academy, Alibaba Group, . 2018. Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation. 회의록에서 올바른 회의 제목을 입력하세요. 권리에서 컨퍼런스 ACM MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 오타와 Trovato et al. 확인 이메일(컨퍼런스 ACM MM &#39;23). ACM, 뉴욕, 뉴욕, 미국, 10페이지. 한국어: https://doi.org/XXXXXXX.XXXXXXX 서 론 최근, 텍스트-이미지 생성 분야에서 경이로운 발전이 이루어졌습니다[38, 39, 42, 44, 59]. 이는 주로 대규모 정렬된 이미지-텍스트 데이터 세트[47], 시각-언어 사전 학습 모델[20, 24, 37], 확산 모델[9, 15, 42]에서의 중요한 성과 덕분입니다. 이러한 텍스트-이미지 생성 결과에서 영감을 받아 많은 연구에서 텍스트-비디오[16, 17, 48] 및 텍스트-3D[19, 25, 28, 36, 54]와 같은 다른 모드에서 텍스트 조건부 확산 모델을 탐구했습니다. 이 작업에서 우리는 3D 콘텐츠를 만들고 게임, 가상 또는 증강 현실, 로봇 응용 프로그램 등 많은 응용 프로그램에 잠재적으로 적용될 수 있는 텍스트-3D 생성 분야에 특히 초점을 맞춥니다. 텍스트-3D 생성 모델을 훈련하는 것은 2D 이미지에 비해 풍부한 텍스트와 3D 데이터 쌍을 얻는 것이 어렵기 때문에 어려울 수 있습니다. 가장 최근에 DreamFusion[36]은 사전 훈련된 2D 텍스트-이미지 확산 모델[44]에서 스코어 증류를 사용하여 Neural Radiance Fields(NeRF)[29]를 최적화하여 텍스트-3D 합성을 수행함으로써 처음으로 이 과제를 해결했습니다. 다음 문헌[28, 54]도 스코어 증류 패러다임을 사용합니다. 이러한 방법은 3D 감독이 필요 없이 텍스트-3D 콘텐츠 생성을 위한 솔루션을 제공하고 검증합니다. 이러한 방법은 상당한 약속에도 불구하고 다중면 문제 또는 Janus 문제라고 알려진 주목할 만한 문제로 인해 어려움을 겪고 있으며, 이는 뷰 간 불일치를 초래합니다.게다가 텍스트-3D 생성에서 또 다른 중요한 문제는 생성된 3D 객체의 모양을 제어할 수 없다는 것입니다.즉, 이러한 방법은 다른 시드를 설정하여 입력 텍스트의 요구 사항을 충족하는 임의의 모양의 객체를 생성할 수 있습니다.Latent-NeRF[28]는 먼저 스케치 모양 가이드 3D 생성을 도입하여 NeRF의 기하 학습을 감독하기 위해 미리 정의된 메시를 대상으로 사용합니다.그러나 이 방법은 매번 각 3D 생성에 대한 메시 모양을 미리 정의해야 하므로 비용이 많이 들고 시간이 많이 걸립니다.이것은 우리가 텍스트-3D 생성의 모양과 기하 학습을 모두 안내하기 위해 2D 및 3D 확산 모델 모두에서 사전 지식을 개발할 가능성을 탐구하도록 동기를 부여했습니다. 영어: 텍스트-이미지 확산 모델(예: ControlNet[59] 및 T2I-Adapter[32])의 조건부 제어 패러다임에서 영감을 받아 텍스트 프롬프트와 함께 추가 조건(예: 스케치, 마스크, 깊이)을 사용하여 생성 프로세스를 안내하여 이미지의 제어 가능성과 공간적 일관성을 더욱 높였습니다. 이 조건부 제어 메커니즘을 텍스트-3D 생성에 통합하는 방법을 모색합니다. 이 연구에서는 Points-to-3D라는 새롭고 유연한 프레임워크를 제안하여 뷰 간 뷰 일관성을 개선하고 텍스트-3D 생성을 위한 3D 모양에 대한 유연한 제어 가능성을 달성합니다. Points-to-3D의 핵심 아이디어는 기하학 및 모양 측면에서 텍스트-3D 생성을 안내하는 제어 가능한 희소 3D 포인트를 도입하는 것입니다. 이를 달성하기 위해 Point-E[35]에서 영감을 받아 사전 학습된 3D 포인트 클라우드 확산 모델에서 희소 포인트 클라우드를 기하학 사전으로 추출하는 것을 제안합니다. 이러한 희소 3D 포인트는 사용자가 제공하거나 텍스트-이미지 모델에서 생성할 수 있는 단일 참조 이미지에 따라 조건이 지정됩니다. 그러나 4096개의 3D 포인트만 포함된 생성된 희소 포인트 클라우드를 활용하는 것은 간단하지 않습니다. 이 문제를 극복하기 위해 임의로 초기화된 NeRF의 지오메트리가 참조 이미지에 묘사된 모양과 매우 유사하도록 장려하는 포인트 클라우드 안내 손실을 제안합니다. 지오메트리 외에도 텍스트 프롬프트와 학습된 깊이 맵에 따라 조건 지정된 모양을 최적화할 것을 제안합니다. 보다 구체적으로, 컴팩트한 잠재 공간에서 공개적으로 사용 가능하고 제어가 더 쉬운 2D 이미지 확산 모델인 ControlNet[59]에 대한 점수 증류[28, 36]를 수행합니다. Points-to-3D라는 접근 방식은 2D 및 3D 확산 사전에 대한 지식을 증류하여 희소 3D 포인트와 사실적인 모양 제어가 가능한 3D 생성 간의 격차를 메울 수 있습니다. 그림 1에 나와 있듯이, 상상적인 참조 이미지가 주어지면 Points-to-3D는 다양한 텍스트 프롬프트에 따라 달라지는 사실적이고 모양 제어가 가능한 3D 콘텐츠를 생성할 수 있습니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • Points-to-3D라는 새롭고 유연한 텍스트-3D 생성 프레임워크를 제시합니다. 이 프레임워크는 사전 훈련된 2D 및 3D 확산 모델에서 지식을 추출하여 희소 3D 포인트와 보다 사실적이고 모양 제어가 가능한 3D 생성 간의 격차를 메웁니다. • 희소 3D 포인트를 최대한 활용하기 위해 효율적인 포인트 클라우드 안내 손실을 제안하여 NeRF의 지오메트리를 최적화하고 텍스트와 학습된 깊이 맵에 따라 ControlNet을 사용하여 점수 증류를 통해 지오메트리와 일관된 모양을 학습합니다. •
--- EXPERIMENT ---
모든 결과는 Points-to-3D가 뷰 간 불일치를 상당히 완화하고 텍스트-3D 생성을 위한 3D 모양에 대한 우수한 제어성을 달성할 수 있음을 보여줍니다.관련 작업 텍스트-이미지 생성.이미지 생성은 생성적 적대 신경망(GAN)[13, 21]을 만났을 때 첫 번째 획기적인 결과를 얻습니다.GAN은 실제 이미지와 구별할 수 없는 이미지를 합성하도록 생성기를 훈련합니다.최근 이미지 생성은 확산 모델[49]의 개발로 또 다른 경이로운 진전을 이루었습니다.모델링의 개선[9, 15]으로 인해 노이즈 제거 확산 모델은 노이즈가 있는 이미지의 노이즈를 반복적으로 제거하여 다양한 고품질 이미지를 생성할 수 있습니다.이미지 기반 무조건 생성 모델 외에도 확산 모델은 텍스트 설명에서 텍스트 조건 이미지를 생성할 수 있습니다[38, 44]. 다음 연구에서는 의미 분할[42], 참조 이미지[43], 스케치[53], 깊이 맵[32, 59] 및 기타 조건[18, 32, 59]을 포함하여 텍스트-이미지 생성에 더 많은 조건을 추가하는 것을 제안하며, 이는 텍스트-이미지 생성의 개발 및 응용을 크게 촉진합니다. 텍스트-이미지 확산 모델의 성공에 힘입어 많은 연구에서 텍스트 기반 조작[4], 텍스트-비디오[17, 48] 및 텍스트-3D[25, 28, 36, 54]와 같은 다른 모드에서 텍스트 조건 확산 모델을 탐구했습니다. 이 연구에서는 텍스트-3D 생성 분야에 중점을 둡니다. 신경 광도장(NeRF). 3D 폭셀 그리드[51], 메시[11], 포인트 클라우드[1, 27, 30, 60], 암묵적 NeRF[29, 34]를 포함하여 3D 장면 표현에 대한 연구가 많이 있습니다. 최근 몇 년 동안 일련의 역 렌더링 방법으로 NeRF 기반 방법이 3D 장면 표현에서 중요한 기술로 등장하여 새로운 뷰를 합성하고 기하 표면을 재구성할 수 있습니다[29, 34, 55]. 구체적으로 NeRF[29]는 신경망 Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation(MLP)을 사용하여 장면을 밀도 및 광도 필드로 표현하여 사실적인 새로운 뷰 합성을 허용합니다. 그러나 3D 공간에서 신경망을 밀집하여 쿼리하는 데 드는 계산 비용은 상당합니다. NeRF의 효율성을 개선하기 위해 최근 연구에서는 NeRF[6, 34, 51]에 기반한 하이브리드 또는 명시적 구조를 설계하여 복사장 재구성을 위한 빠른 수렴을 달성하고 NERF[12, 14, 40]의 렌더링 속도를 가속화하는 방법을 모색했습니다. 이러한 방법의 대부분은 학습을 위해 여러 뷰와 해당 카메라 매개변수가 필요하며, 특히 새로운 텍스트-3D 콘텐츠 생성에서는 항상 충족할 수 없습니다. 이 연구에서는 NeRF를 기본적인 장면 표현 모델로 보고 텍스트-3D 생성을 위한 새로운 프레임워크를 고안하는 데 중점을 둡니다. 단일 이미지 3D 재구성. 이미지에 있는 객체를 재구성하는 것을 목표로 하는 단일 이미지 3D 재구성에는 다양한 접근 방식이 있습니다. 폭셀[7, 57], 폴리곤 메시[56], 포인트 클라우드[10], 그리고 최근에는 NeRF[33, 58]와 같이 재구성된 객체를 표현하는 데 사용할 수 있는 다양한 형식이 있습니다. 그러나 이러한 방법은 일반적으로 특정 3D 데이터 세트[5]에서 훈련되고 평가되므로 충분한 3D 훈련 데이터가 부족하여 일반 3D 재구성으로의 일반화가 어렵습니다.최근 Point-E[35]는 포인트 클라우드 형태로 일반 3D 콘텐츠 생성을 위한 효율적인 방법을 탐구합니다.먼저 사전 훈련된 텍스트-이미지 확산 모델을 사용하여 단일 합성 이미지를 생성한 다음 생성된 이미지에 따라 조건화된 포인트 클라우드 확산 모델을 사용하여 희소(4096개 포인트) 3D 포인트 클라우드를 생성합니다.Point-E의 일반화 능력은 수백만 개의 3D 데이터에 대한 훈련에 기인합니다[35].이 연구에서는 Point-E를 포인트 클라우드 기반 모델로 혁신적으로 활용하여 보다 사실적이고 모양을 제어할 수 있는 텍스트-3D 생성을 위한 희소한 지오메트리 지침을 제공합니다.텍스트-3D 생성.최근 텍스트-이미지 생성 및 3D 장면 모델링의 발전으로 인해 텍스트-3D 콘텐츠 생성에 대한 관심이 커지고 있습니다. CLIPforge[45]와 같은 이전 작업은 모양 코드에 따라 조건화된 암묵적 자동 인코더와 텍스트 입력에서 모양 임베딩을 샘플링하기 위한 정규화 흐름 모델로 구성됩니다. 그러나 실제 응용 프로그램에서 확장하기 어려운 폭셀 표현의 3D 학습 데이터가 필요합니다. PureCLIPNERF[23]는 3D 데이터 세트에 액세스하지 않고도 텍스트에서 3D로 생성하기 위해 장면 표현을 위한 폭셀 그리드 모델로 안내하기 위해 사전 학습된 CLIP[37]을 사용합니다. CLIP-Mesh[31]는 텍스트 프롬프트를 사용하여 제로 샷 3D 생성을 위한 방법을 제시하며, 또한 생성된 3D 모델의 미분적으로 렌더링된 이미지와 입력 텍스트를 비교하는 사전 학습된 CLIP 모델에 의존합니다. DreamFields[19]는 먼저 사전 학습된 CLIP을 안내로 사용하여 NERF[29]의 3D 표현을 최적화하여 NeRF의 모든 렌더링 뷰가 텍스트 프롬프트와 일치하도록 권장합니다. 최근 DreamFusion[36]은 강력한 사전 학습된 2D 텍스트-이미지 확산 모델[44]을 활용하여 텍스트-3D 합성을 수행하는 것을 제안합니다. 그들은 NeRF로 모델링된 3D 객체의 렌더링된 뷰를 감독하기 위해 Score Distillation Sampling(SDS) 손실을 제안합니다. 다음의 Stable-DreamFusion[52], Latent-NeRF[28], SJC[54]는 확산 프로세스를 컴팩트한 잠재 공간에 적용하고 텍스트-3D 생성 개발을 용이하게 하는 공개적으로 사용 가능하고 계산 효율적인 Stable Diffusion 모델[42]에 점수 증류를 적용합니다. 우리는 이러한 작업을 바탕으로 희소한 3D 포인트와 더욱 사실적인 모양 제어가 가능한 3D 콘텐츠 생성 간의 격차를 메움으로써 텍스트-3D 생성을 위한 유연한 Points-to-3D 프레임워크를 제안한다.3. 접근 방식 서론 이 섹션에서는 섹션 3.2에서 제안한 프레임워크를 이해하는 데 필요한 핵심 개념 중 일부를 간략하게 소개합니다.확산 모델.확산 모델은 [49]에서 처음 제안되었으며 최근 [15, 50]에서 홍보되었습니다.확산 모델은 일반적으로 이미지 x = X에 점진적으로 노이즈를 추가하는 순방향 프로세스 q와 노이즈가 있는 데이터에서 점진적으로 노이즈를 제거하는 역방향 프로세스 p로 구성됩니다.순방향 프로세스 q는 다음과 같이 공식화할 수 있습니다.= q(xt|xt−1) = N(xt; √1 − ßtxt−1, ßtī), xt = ~ (1) 여기서 시간 단계 t = [0,T], ẞt는 노이즈 일정을 나타냅니다. DDPM [15]은 노이즈 처리 절차의 주어진 타임스텝을 직접 얻는 것을 제안한다: (2) 여기서 āt 1 - Bt, 및 e N(0, 1). 노이즈 제거 프로세스 Po(xt-1|x+)는 랜덤 노이즈에서 시작하여 노이즈 처리 과정을 천천히 역전한다. DDPM [15]은 추가된 노이즈 e를 모델링하여 분포를 매개변수화하는 것을 제안한다. 최근, 확산 모델의 특정 형태인 잠재 확산 모델(LDM)은 텍스트-이미지 생성에서 큰 진전을 이루었다. 잘 알려진 안정 확산[42]과 ControlNet[59]은 모두 잠재 확산 모델이다. 점수 증류 샘플링(SDS). 점수 증류 샘플링(SDS)은 DreamFusion[36]에서 처음 제안되었는데, 장면 표현 모델[3]과 사전 학습된 텍스트-이미지 확산 모델(Imagen[44])의 두 모듈을 통합하여 텍스트-3D 생성을 달성한다. 학습하는 동안 학습 가능한 NeRF 모델은 먼저 미분 가능한 렌더링 g: x = g(0)로 뷰 합성을 수행하여 주어진 랜덤 카메라 포즈에서 이미지를 렌더링할 수 있습니다.그런 다음 랜덤 노이즈가 x에 추가되고 확산 모델은 노이즈가 있는 이미지 xt, 텍스트 임베딩 y 및 노이즈 레벨 t가 주어진 학습된 노이즈 제거 함수(xt; y, t)를 사용하여 노이즈가 있는 이미지에서 추가된 노이즈 e를 예측합니다.이 점수 함수는 NeRF 매개변수 0을 업데이트하기 위한 그래디언트를 제공하며 다음과 같이 계산됩니다.VLSDs(8, 9(0)) = Et,€ [w(t)(€$(xt; y, t) − e) дх ᎧᎾ (3) 여기서 w(t)는 at에 따라 달라지는 가중치 함수입니다. Stable Diffusion[42]을 사용하는 Stable-DreamFusion[52] 및 Latent-NeRF[28]에서 영감을 얻어 제어가 더 쉬운 LDM인 ControNet[59]으로 스코어 증류를 수행하여 더욱 사실적이고 모양을 제어할 수 있는 3D 콘텐츠를 생성하는 것을 제안합니다.3.Points-to-3D 이 섹션에서는 그림 2에 나와 있는 Points-to-3D 프레임워크에 대해 설명합니다.아키텍처.먼저 Points-to-3D 프레임워크의 아키텍처를 설명합니다.그림 2에서 볼 수 있듯이 Points-to-3D는 주로 장면 표현 모델(좌표 기반 MLP[34]), 텍스트-이미지 2D 확산 모델(ControlNet[59]), 포인트 클라우드 3D 확산 모델(Point-E[35])의 세 가지 모델로 구성됩니다.ACM MM &#39;23 컨퍼런스, 10월 29일-11월 03, 2023, 오타와, 캐나다 기차 참조 이미지 추론 포인트 클라우드 업샘플링 Point-E L포인트 클라우드 σ 깊이 맵 MLP MLP VAE 디코더 Trovato et al. &quot;a Nissan GTR racing car&quot; ControlNet NeRF (c¹, c², c³, c) ‣LSDS+ 그림 2: 텍스트-3D 생성을 위한 제안된 Points-to-3D 프레임워크의 그림. Points-to-3D는 주로 세 부분으로 구성됩니다. 장면 표현 모델(좌표 기반 NeRF [34]), 텍스트-이미지 2D 확산 모델(ControlNet [59]), 포인트 클라우드 3D 확산 모델(Point-E [35])입니다. 학습하는 동안 2D 및 3D 확산 모델은 모두 동결됩니다. • 장면 모델. Neural Radiance Field(NeRF) [29]는 체적 광선 추적기와 MLP로 구성된 장면 표현에 사용되는 중요한 기술이었습니다. 이전 문헌[28, 36, 54]은 NeRF를 텍스트-3D 생성을 위한 장면 표현 모델로 사용했습니다.그 이유는 주로 NeRF 모델이 공간 광도 필드와 렌더링 패러다임으로 인해 서로 다른 뷰 간에 공간적 일관성을 암묵적으로 부과할 수 있기 때문입니다.NeRF 모델은 일반적으로 체적 밀도 σ와 RGB 색상 c를 생성합니다.이 연구에서 우리는 체적 밀도 σ와 4개의 의사 색상 채널 {C = (c¹, c², c³, c¹)} = R64×64×4를 포함하여 5개의 출력을 생성하는 Latent-NeRF[28]의 효율적인 설계를 채택했습니다.이는 잠재 확산 모델[42]의 4개 입력 잠재 특징에 해당합니다.(c1, c2, c³, c4, 0) = MLP(x, y, z, d; 0), (4) 여기서 x, y, z는 3D 좌표를 나타내고 d는 뷰 방향입니다.기본적으로 Instant-NGP[34]를 장면 표현 모델로 사용합니다. • 텍스트-이미지 2D 확산 모델. DreamFusion [36]에서 사용하는 Imagen [44]은 공개적으로 사용할 수 없으므로 기존 문헌 [28, 52, 54]에서 이전에 탐구한 대로 처음에 Stable Diffusion을 텍스트-이미지 확산 모델로 사용합니다. 그러나 원래의 Stable Diffusion v1.5는 추가 입력 조건을 지원하도록 제어할 수 없습니다. 이 작업에서 먼저 Points-to-3D에서 깊이 맵에 조건화된 사전 학습된 ControlNet [59]을 2D 확산 모델로 사용하도록 제안합니다. 그림 2에서 볼 수 있듯이 입력 텍스트 프롬프트(예: &quot;Nissan GTR 경주용 자동차&quot;) 외에도 NeRF 모델의 예측 깊이 맵 M = RH×W×1을 조건 제어로 추가로 도입합니다. 깊이 맵은 다음과 같이 계산됩니다. 단순화를 위해 한 픽셀에 대한 깊이 값 계산만 보여줍니다. K M₁ = Σ wktk k=(5) 및 wk = αk [[(1 − α;), 및 ak = 1 − exp(−σk||tk — tk+1||). (6) j <k where K is the total number of sampling points along a ray, and tk denotes the depth hypothesis at point k. The better and more accurate the predicted depth map M, the more geometrically consistent views ControlNet will synthesize. • Point Cloud 3D Diffusion Model. To control the geometry of NERF for text-to-3D generation, we propose in this paper, for the first time, the distillation of prior knowledge from the pre-trained large point cloud diffusion model, Point-E [35]. Point-E [35] is an efficient 3D diffusion model for generating sparse 3D point clouds (4096 points) from text prompts or images in about 1 minute. As illustrated in Figure 2, we utilize the pre-trained Point-E model to regulate the geometry learning of NeRF. Specifically, the model generates a sparse 3D point cloud consisting of 4096 points, which is conditioned on a reference image and can flexibly represent the object's shape depicted in the image. However, it is not trivial to guide the NeRF's geometry with only sparse 3D points, we propose a sparse point cloud guidance loss point-cloud to address this issue, which is illustrated in the next section. It is worth noting that Points-to-3D enables users to easily control the shape of the generated content by providing a reference image, which can be a real image or a generated image via text-toimage models [32, 42, 59]. Sparse 3D Points Guidance. The core idea of our Points-to-3D is to introduce controllable sparse 3D points to guide the text-to-3D generation. In this section, we elaborate on how to leverage the sparse 3D points. It is challenging to use a sparse 3D point cloud to guide the geometry learning of NeRF. Previous work on improving Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation Conference ACM MM '23, Oct. 29-Nov. 03, 2023, Ottawa, Canada NeRF's geometry uses the depth of sparse points to supervise the predicted depth [8, 41]. However, the 3D points are computed using multiple views via COLMAP [46], and the information about which view each 3D point belongs to has been calculated in advance. In our case, only a single RGB image is used to generate the sparse 3D points, when we project all the points to the current view to attain a sparse depth map, there will be aliasing problems between the front and the rear 3D points.i=In this work, we present a sparse point cloud guidance loss. Specifically, let Ps {(xi, Yi, Zi)} be the original sparse 3D points generated by Point-E [35] conditioned on a reference image. Instead of using Ps directly, we experimentally find that making the sparse point cloud to be dense can provide better geometry supervision and produce more realistic 3D contents. We propose to upsample Ps by iteratively performing 3D points interpolation via a simple rule, i.e., for each point pi, we add a new 3D point at the middle position between each of its nearest q neighbor points and pi. The process is depicted in Figure 3. We set q = 20, n = 2 by default. Now we get the dense 3D points Pd, which contain about 500k points after eliminating duplicate points. Ideally, we want to align the geometry (the volume density σ) of NeRF with the shape of Pd to ensure that the generated 3D content of Points-to-3D closely resembles the reference image. In addition, we also want to provide NeRF with a level of flexibility and adaptability in its geometry to enable the generation of new details while satisfying different text prompts. Instead of using the per-view sparse depth map supervision, which has a front-rear aliasing issue as discussed above, and is also not efficient as it only optimizes the current view's depth, we propose an efficient point cloud guidance loss Lpoint-cloud to directly optimize the whole geometry (σ) in 3D space. Specifically, we encourage the occupancy (a) corresponding to the NeRF points Pnerf that near the point cloud Pd to be close to 1, while the occupancy of the NeRF points that far from the point cloud Pd to be close to 0. Furthermore, we make the geometry capable of generating new details adaptively by ignoring the supervision of some parts of occupancy. We first compute the closest distance between each point in Pnerf and all points in Pd: D = Dist (Pnerf, Pd), DERS×1, where S denotes the number of points in Pnerf. Then, normalize D via: = 0.5 (max(Pnerf)-min(Pnerf)) Finally, The calculation of point-cloud can be formulated as: and D point-cloud = CrossEntropy (α(Pnerf), O(Pnerf)), 1 – Â¡, if 1 – Ô¡ > ˜1; Oi = 0, −1, 그렇지 않으면 1 - Di &lt; t2; 그렇지 않으면; (7) 여기서 O(P, Pnerf).는 모든 NeRF 포인트의 목표 점유율을 나타내고, 1 - D는 가이드 포인트 클라우드 Pd에 대한 근접도를 나타내고, 71, 72는 각각 실험적으로 0.95와 0.9로 설정된 두 개의 하이퍼파라미터입니다. t₂ &lt; 1 − D &lt; T₁인 포인트의 감독을 무시하여 모델이 텍스트 프롬프트와 일치하도록 기하학에 새로운 세부 정보를 적응적으로 추가하고 불완전한 가이드 포인트 클라우드 Pd에 있는 깨진 구멍을 수정할 수 있도록 합니다. 학습 목표. Points-to-3D의 학습 목표는 포인트 클라우드 가이드 손실 포인트 클라우드, StepStep n의 세 부분으로 구성됩니다. 그림 3: 포인트 클라우드 업샘플링 프로세스의 그림. 각 원래 3D 점(예: pi)에 대해 가장 가까운 q 이웃 점(파란색 점)과 점 p 사이에 새로운 3D 점(빨간색 점)을 추가합니다. 각 보간 단계에 대해 점수 증류 샘플링 손실 LSD 및 희소성 손실 Lsparse를 추가합니다. 희소성 손실은 [52]에서 제안한 것으로, 렌더링 가중치를 정규화하여 플로터를 억제할 수 있습니다. Lsparse -Σ (wk log wk + (1 - wk) log(1 – wµ)). (9) k 방정식으로 계산한 깊이 맵 조건 M을 도입하고 다음과 같이 방정식 3에서 점수 증류 샘플링 손실을 업데이트합니다. 4. V₁LSDS (4,9(0)) = Et,e [w(t) (€6(xt; y, M, t) − e) 전체 학습 목표는 다음과 같이 계산됩니다. дх ᎧᎾ ;]. (10) L = λpoint Lpoint-cloud + λSDS LSDS + λsparse sparse.실험 기준선 (11) 우리는 세 가지 텍스트-3D 생성 기준선을 고려합니다: DreamFusion [36, 52], Latent-NeRF [28], 및 SJC [54]. 폐쇄 소스 Imagen [44] 확산 모델을 사용하는 대신, Latent-NeRF와 SJC는 모두 공개적으로 사용 가능한 Stable Diffusion [42]을 사용합니다. 우리는 주로 실험에서 Points-to-3D를 Latent-NeRF 및 SJC와 비교합니다. 우리는 보충 자료에서 DreamFields [19] 및 DreamFusion [36]과의 비교를 포함한 더 많은 결과를 제공합니다. 4.2 구현 세부 정보 우리는 Instant-NGP [34]를 장면 모델로 사용합니다. [36]의 카메라 샘플링 방법에 따라 훈련하는 동안 구면 좌표에서 카메라 위치를 무작위로 샘플링하고 NeRF로 렌더링할 때 FOV도 무작위로 확대합니다.그림 2에 표시된 잠재 공간에서의 훈련 외에도 [28]에 도입된 RGB 공간에서 RGB 미세 조정을 추가로 수행하면 텍스트-3D 생성 결과를 더욱 개선할 수 있음을 실험적으로 발견했습니다.Points-to-3D는 단일 A100 GPU에서 3D 생성을 완료하는 데 텍스트 프롬프트당 50분도 걸리지 않으며 대부분의 시간은 포인트 클라우드를 계산하는 데 사용됩니다.학습 속도가 1e−³인 AdamW 옵티마이저를 사용하여 반복을 훈련합니다.Apoint, ASDS, Asparse의 하이퍼파라미터는 각각 5e-6, 1.0, 5e−4로 설정됩니다.4.3 Ablation 연구 포인트 클라우드 유도 손실의 효과. 이 섹션에서는 제안된 포인트 클라우드 유도 손실 포인트 클라우드를 평가합니다.구체적으로 포인트 클라우드 유도를 제거하여 Points-to-3D를 평가합니다.또한 섹션 3.2에서 설명한 대로 뷰당 희소 깊이 맵 손실을 검증합니다.결과는 그림 4에 나와 있습니다.먼저 &quot;말 위에 배낭을 멘 우주인&quot; &quot;해바라기 꽃병&quot; Trovato et al. &quot;튤립 꽃병&quot;을 생성합니다.그림 6: 동일한 참조 이미지(Stable Diffusion[42]에서 생성)와 해당 희소 3D 포인트이지만 텍스트는 다른 두 3D 모델의 시각화.그림 4: Lpoint-cloud 효과의 그림. 참조 이미지와 텍스트 프롬프트가 주어지면, Lpoint-cloud를 사용한 Points-to-3D(3번째 행)는 뷰당 깊이 맵 손실(2번째 행)과 지오메트리 제약이 없는 것[28](1번째 행)보다 더욱 사실적인 3D 콘텐츠를 생성할 수 있습니다. 그림 5: 각각 지오메트리 가이드로 Ps와 Pd를 사용하여 학습한 모델의 렌더링된 뷰 비교. 텍스트 프롬프트는 &quot;Nissan GTR 레이싱 카&quot;입니다. 텍스트 프롬프트가 있는 참조 이미지: &quot;말에 배낭을 멘 우주인&quot; Stable Diffusion을 사용합니다. 그런 다음 Lpoint-cloud(3번째 행), 설계된 per-view 깊이 맵 손실(2번째 행), 그리고 지오메트리 제약 없이(1번째 행) 각각 동일한 텍스트 프롬프트가 있는 세 개의 모델을 학습합니다. 지오메트리 제약 없이 생성된 콘텐츠가 명백한 뷰 불일치 문제(빨간색 점선 상자)를 겪는 것을 알 수 있습니다. 설계된 per-view 깊이 맵 손실을 지오메트리 감독으로 사용한 결과는 다중 얼굴 문제를 더욱 개선합니다. 그러나 렌더링된 이미지는 점 클라우드의 희소성과 per-view 감독의 비효율성으로 인해 덜 사실적이고 깨지기까지 합니다(노란색 점선 상자). 점 클라우드를 사용한 결과는 &quot;우주인&quot;과 &quot;말&quot; 모두에서 더 많은 세부 정보를 보여준다는 점에 주목할 가치가 있습니다. 즉, 지오메트리 최적화를 위한 Lpoint-cloud를 사용한 Points-to-3D는 더욱 사실적인 3D 콘텐츠를 생성할 수 있습니다. 3D 점 업샘플링의 효과. 이 섹션에서는 생성된 희소 3D 포인트 클라우드를 업샘플링하는 효과를 분석합니다.그림 5에서 보듯이 희소(4096) 3D 포인트 Ps와 업샘플링된 더 밀도가 높은(~500k) 3D 포인트 Pd를 지오메트리 가이드로 사용하여 학습한 Points-to3D의 렌더링된 뷰를 비교합니다.첫 번째 열은 그림 2에 표시된 참조 이미지가 주어진 Point-E [35]에서 생성된 원래 희소 포인트 Ps와 우리가 설계한 규칙을 통해 업샘플링된 포인트 Pd를 나타냅니다.두 번째, 네 번째 열은 그림 7: 첫 번째 열에 표시된 동일한 참조 이미지와 희소 3D 포인트로 학습한 두 개의 3D 모델 비교.첫 번째와 두 번째 행은 각각 Lpoint-cloud에서 적응 설계 없이 및 적응 설계를 사용하여 학습한 것을 나타냅니다.텍스트 프롬프트는 &quot;나무 의자&quot;입니다.세 개의 해당 렌더링된 뷰입니다.Pd로 안내된 결과가 Ps로 안내된 결과에 비해 더 사실적임을 알 수 있습니다. 이는 더 밀도가 높은 포인트 클라우드가 NeRF가 더 간결한 지오메트리를 학습하도록 장려하기 위해 더 많은 감독을 제공할 수 있기 때문입니다.또한, 더 나은 지오메트리(깊이 맵)는 ControlNet[59]이 입력 텍스트 프롬프트와 일치하는 더욱 지오메트리 일관되고 사실적인 이미지를 생성하도록 안내할 수도 있습니다.Lpoint-cloud에서의 적응형 설계 효과.이 섹션에서는 Lpoint-cloud에서 적응형 설계의 효과를 설명합니다.즉, 방정식 7과 방정식 8에서 2 &lt; 1 − &lt; ₁인 NeRF 포인트의 감독을 무시하여 Points-to-3D가 지오메트리를 적응적으로 조정하여 텍스트 프롬프트와 일치하도록 할 것을 제안합니다.이 적응형 설계는 두 가지 주요 목적을 제공합니다.a) 3D 콘텐츠의 주요 모양을 변경하지 않고도 새로운 세부 정보를 생성할 수 있는 기능을 제공합니다.b) 불완전한 포인트 클라우드 Pd의 깨진 구멍을 채울 수 있습니다.그림 6에서 볼 수 있듯이 동일한 참조 이미지와 희소한 포인트 클라우드이지만 텍스트 프롬프트가 다른 두 개의 생성된 3D 콘텐츠를 Points-to-3D를 사용하여 시각화합니다. 마지막 세 열은 각각 동일한 카메라 포즈에서 렌더링된 이미지, 렌더링된 깊이 맵 및 렌더링된 법선을 나타냅니다.Points-to-3D가 동일한 포인트 클라우드 안내를 기반으로 다양한 입력 텍스트 프롬프트와 일치하도록 보다 구체적인 새로운 세부 정보를 생성할 수 있음을 분명히 알 수 있습니다.그림 7에서 불완전한 포인트 클라우드의 구멍을 채우는 적응형 설계의 효과를 분석합니다.참조 이미지가 주어지면 Point-E[35]는 이 경우 의자 등받이에 깨진 구멍과 같이 균일하지 않은 포인트 클라우드를 생성할 수 있습니다.포인트 클라우드에 가까운 모든 NeRF 포인트가 양의 클래스이고 그렇지 않으면 음의 클래스가 되도록 적용하면 모든 3D 콘텐츠에 적절한 거리 임계값을 설정하기 어렵고 구멍이 깨집니다. 예를 들어, 우리는 1차 및 2차 Points-to-3D: 희소 점과 모양 제어 가능 텍스트-3D 생성 간의 격차 해소 참조 이미지 Points-to-3D(저희) Latent-NeRF 레고 맨 참조 이미지 딸기 참조 이미지 빨간색 컨버스 올스타 신발 참조 이미지 GUCCI 백팩 참조 이미지 다람쥐가 오토바이를 타고 있음 SJC 컨퍼런스 ACM MM &#39;23, 10월 29일-11월 03, 2023, 오타와, 캐나다 Points-to-3D (Ours) Latent-NeRF 트럼프의 모습 파란 수정으로 만든 딸기 나이키 러닝화 클래식한 LV 백팩 스파이더맨이 오토바이를 타고 있다 SJC 참조 이미지 금속 쟁반 위의 황금색 찻주전자 쟁반 위의 중국 청화백자 찻주전자 참조 이미지 중세 기사가 나무를 자르고 있다 우주인이 나무를 자르고 있다 참조 이미지 미키 마우스가 튜바를 연주하고 있다 테디가 튜바를 연주하고 있다 그림 8: 단일 객체 생성(1~4번째 행) 및 장면 생성(5~8번째 행)에서 Latnet-NeRF [28] 및 SJC [54]와 정성적 비교. 첫 번째 열은 Points-to-3D에 사용된 참조 이미지를 나타내며, 위쪽 4개는 실제 이미지이고 아래쪽 4개는 각각 Stable Diffusion [42]을 사용하여 생성한 합성 이미지입니다. (확대하여 보는 것이 가장 좋습니다.) 행입니다. Points-to-3D는 기하 구조와 외관 모두에서 깨진 구멍을 자연스럽게 복구할 수 있습니다. 또한 보충 자료에서 깊이 맵 조건의 효과를 분석합니다. 4.4 모양 제어 가능 텍스트-3D 생성 특수 개념과 모양은 일반적으로 텍스트 프롬프트로는 설명하기 어렵지만 이미지로는 쉽기 때문에 이미지를 사용하여 텍스트-3D 콘텐츠 생성을 안내하는 메커니즘이 절실히 필요합니다. 이 섹션에서는 기하 구조를 안내하기 위한 단일 참조 이미지로 뷰 일관성 및 모양 제어 가능 3D 콘텐츠를 생성하는 Points-to-3D를 평가합니다. DreamFusion[36]과 Magic3D[25]가 자체 텍스트-이미지 확산 모델[2, 44]을 사용하고 둘 다 코드를 릴리스하지 않는다는 점을 고려하여 주로 Latent-NeRF[28] 및 SJC[54]와 비교합니다. 그림 8에서 볼 수 있듯이 주로 단일 객체 생성과 장면(여러 객체로 구성됨) 생성의 두 가지 측면을 비교합니다. ~ 단일 객체 생성(1~4행)의 경우 LatentNERF[28]는 뷰 불일치 문제가 발생하기 쉽고, 때로는 적절한 콘텐츠를 생성하지 못합니다. SJC[54]는 생성된 객체의 뷰 일관성 측면에서 Latent-NeRF보다 약간 더 나아 보이지만, 때로는 텍스트 설명과 일치하는 콘텐츠를 생성하지 못합니다(예: 2~4행). Points-to-3D는 자동으로 뷰 일관성이 있고 더 사실적인 단일 객체를 생성할 수 있습니다. Points-to3D는 Converse, Nike, GUCCI, LV 로고와 같이 더 사실적인 세부 정보를 생성할 수 있다는 점에 주목할 가치가 있습니다. 더 어려운 장면 생성(5~8행)의 경우 Latent-NeRF[28]의 고유한 뷰 불일치 문제가 더 심각해집니다(예: 6행에 여러 개의 찻주전자 주둥이, 7행에 여러 개의 손이나 다리). 또한 Latent-NeRF와 SJC는 모두 입력 텍스트 프롬프트의 일부 개념을 쉽게 잃을 수 있습니다.예를 들어, 5번째 행의 &quot;오토바이&quot;, 6번째 행의 &quot;트레이&quot;, 마지막 행의 &quot;튜바&quot;가 있습니다.반면에, Points-to-3D는 뷰 일관성이 있는 3D 콘텐츠를 생성하고 텍스트 프롬프트에 포함된 개념을 보존할 수 있습니다.또한, Points-to-3D를 사용하면 사용자가 참조 이미지와 비슷한 모양의 3D 콘텐츠를 임의로 만들거나 수정할 수 있습니다.보충 자료에서 더 많은 비교를 제공합니다.컨퍼런스 ACM MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 오타와 Points-to-3D(저희) Latent-NeRF 그림 9: Marching Cubes를 통한 메시 비교 [26].테디베어가 차 위에 앉아 있습니다 Trovato et al. 표 1: Latent-NeRF [28], SJC [54] 및 당사의 Points-to-3D의 CLIP R-정밀도를 사용한 양적 비교.방법 VIT-B/16 ↑ VIT-B/32 ↑ ViT-L/14 ↑ Latent-NeRF [28] SJC [54] Points-to-3D (당사) 81.00% 53.00% 59.00% 66.00% 61.00% 57.00% 71.00% 81.00% 90.00% □ SJC Latent-NeRF Points-to-3D (당사) 보기 프롬프트 일관성 관련성 2.2.4.2.2.4.a 테디가 아이스크림을 먹고 있습니다.그림 10: Points-to-3D의 구성 생성. 4.5 기하 비교 우리는 Points-to-3D와 LatentNERF [28]의 학습된 기하를 비교합니다.둘 다 Instant-NGP [34]를 장면 모델로 사용합니다.그림 9에서 볼 수 있듯이, 우리는 두 개의 텍스트 프롬프트 &quot;레고 맨&quot;과 &quot;빨간색 컨버스 올스타 신발&quot;을 사용하여 생성된 두 가지 생성 결과를 보여줍니다.각각에는 렌더링된 RGB 이미지와 두 개의 메시 뷰의 세 가지 뷰가 포함됩니다.메시는 Marching Cubes [26]에 의해 학습된 Instant-NGP의 밀도 필드에서 추출됩니다.Latent-NeRF의 결함 있는 메시와 비교할 때 Points-to-3D는 더 섬세한 메시를 생성할 수 있음을 분명히 알 수 있습니다.즉, 합성 뷰와 일관된 새로운 뷰 외에도 Points-to-3D는 텍스트-to-3D 생성을 위해 제어 가능하고 더 컴팩트한 기하를 학습할 수 있습니다.4.6 구성적 생성 우리는 구성적 3D 콘텐츠를 생성하는 데 있어 Points-to-3D의 효율성을 분석합니다. 그림 10에서 볼 수 있듯이, 여러 참조 이미지의 수동으로 합성된 희소 3D 점을 기하 안내로 사용하여 Points-to-3D는 뷰 일관되고 모양이 제어 가능한 텍스트-3D 생성을 수행할 수 있습니다. 결과에 따르면 Points-to-3D를 사용하면 사용자가 여러 참조 이미지를 사용하여 객체를 자유롭게 합성하고 더욱 창의적인 3D 콘텐츠를 생성할 수 있습니다. 4.7 정량적 비교 CLIP R 정밀도. 이 섹션에서는 Latent-NeRF[28], SJC[54] 및 Points-to3D에 대한 CLIP R 정밀도 메트릭을 계산합니다. 세 개의 CLIP 이미지 인코더(ViT-B/16, ViT-B/32 및 ViT-L/14)를 기반으로 50개의 텍스트 및 3D 모델 쌍(보충 자료에 표시됨)에서 [19]에 따라 CLIP R 정밀도를 계산합니다. 각 3D 생성에 대해 계산을 위해 렌더링된 두 개의 뷰를 무작위로 선택합니다. 결과는 표 1에 보고되어 있으며, Points-to-3D 결과에 대한 점수가 높을수록 3D 모델 출력의 렌더링이 텍스트 프롬프트와 더 정확하게 유사함을 나타냅니다. 사용자 연구. CLIP R-정밀도 메트릭은 렌더링된 뷰와 텍스트 프롬프트의 일치 정도에 초점을 맞추지만 뷰 일관성과 이미지 사실성을 반영하기는 어렵습니다. 사용자 선호도에 따라 다양한 방법을 평가하기 위해 22명의 참가자를 대상으로 사용자 연구를 수행합니다. 참가자에게 각 익명화된 방법의 생성에 대한 뷰 일관성과 프롬프트 관련성 측면에서 선호도 점수(범위 1 ~ 5)를 부여하도록 요청합니다. 그림 11에서 볼 수 있듯이 각 방법의 생성 결과 36개로 구성된 무작위로 구성된 평가 세트의 평균 점수를 보고합니다. Points-to-3D가 뷰 일관성과 프롬프트 관련성 측면에서 Latent-NeRF와 SJC보다 상당히 선호되는 것을 발견했습니다. 사용자 연구에 대한 자세한 정보는 보충 자료를 참조하십시오.제한 사항 Points-to-3D는 유연한 텍스트-3D 생성을 허용하고 사실성, 뷰 일관성 및 모양 제어 가능성 측면에서 이전 작업보다 개선되었지만 몇 가지 제한 사항이 있습니다.첫째, Pointsto-3D는 사전 학습된 2D 이미지 확산 모델[59]과 3D 포인트 클라우드 확산 모델[35]을 기반으로 구축되므로 ControlNet 또는 Point-E가 특정 개체에서 실패할 경우 영향을 받습니다.이 문제는 더 강력한 기초 모델을 개발하면 완화될 수 있습니다.둘째, Points-to-3D는 3D 모양의 우수한 제어 가능성을 달성하는 반면 지오메트리 안내를 위한 단일 참조 이미지가 필요합니다.이 문제는 SAM(Segment Anything Model)[22]을 사용하여 실제 이미지에서 개체를 자르거나 Stable Diffusion, ControlNet과 같은 텍스트-이미지 모델을 사용하여 이미지를 직접 생성하면 완화될 수 있습니다.
--- CONCLUSION ---
S 이 연구에서 우리는 새롭고 유연한 텍스트-3D 생성 프레임워크인 Points-to-3D를 제안합니다. 우리는 뷰 불일치 문제를 완화하고 3D 콘텐츠 생성을 위한 3D 모양의 제어 가능성을 개선하여 프레임워크에 영감을 얻었습니다. 학습된 지오메트리를 제어하기 위해 3D 포인트 클라우드 확산 모델(Point-E)에서 지오메트리 지식(희소 3D 포인트)을 추출하는 것을 혁신적으로 제안합니다. 희소 포인트 클라우드를 더 잘 활용하기 위해 NeRF와 희소 포인트 사이의 지오메트리를 적응적으로 정렬하기 위한 효율적인 포인트 클라우드 안내 손실을 제안합니다. 또한 3D 콘텐츠를 보다 사실적이고 뷰에 일관되게 만들기 위해 텍스트와 학습된 컴팩트 깊이 맵을 모두 조건으로 하는 NeRF 모델인 Points-to-3D: 희소 포인트와 모양 제어 가능 텍스트-3D 생성 간의 격차 해소를 2D 이미지 확산 모델(ControlNet)에 대한 점수 추출을 수행하여 최적화합니다. 정성적 및 정량적 비교는 모두 뷰 일관성 및 모양 제어가 가능한 3D 콘텐츠를 생성하는 데 있어 Points-to-3D의 우수성을 입증합니다.참고문헌 [1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas 및 Leonidas Guibas.2018. 3D 포인트 클라우드에 대한 학습 표현 및 생성 모델.기계 학습에 관한 국제 컨퍼런스에서.PMLR, 40-49. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al.2022. ediffi: 전문가 디노이저 앙상블을 사용한 텍스트-이미지 확산 모델.arXiv 사전 인쇄본 arXiv:2211.01324(2022). [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, Peter Hedman. 2022. Mip-nerf 360: 무한 앤티 앨리어싱 신경 광도장. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 5470-5479. [4] Tim Brooks, Aleksander Holynski, Alexei A Efros. 2022. Instructpix2pix: 이미지 편집 지침을 따르는 법. arXiv 사전 인쇄본 arXiv:2211.(2022). [5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. 2015. Shapenet: 정보가 풍부한 3D 모델 저장소. arXiv 사전 인쇄본 arXiv:1512.03012 (2015). [6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su. 2022. Tensorf: Tensorial radiance fields. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, 2022년 10월 23-27일, Proceedings, Part XXXII. Springer, 333-350. [7] Christopher B Choy, Danfei Xu, Jun Young Gwak, Kevin Chen, Silvio Savarese. 2016. 3d-r2n2: 단일 및 다중 뷰 3D 객체 재구성을 위한 통합 접근 방식. Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, 2016년 10월 11-14일, Proceedings, Part VIII 14. Springer, 628-644. [8] Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan. 2022. Depthsupervised nerf: Fewer views and faster training for free. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록. 12882-12891. [9] Prafulla Dhariwal and Alexander Nichol. 2021. 확산 모델이 이미지 합성에서 gans를 이김. Advances in Neural Information Processing Systems 34(2021), 8780-8794. [10] Haoqiang Fan, Hao Su, Leonidas J Guibas. 2017. 단일 이미지에서 3D 객체를 재구성하기 위한 포인트 세트 생성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 605-613. [11] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler. 2022. Get3d: 이미지에서 학습한 고품질 3D 텍스처 모양의 생성 모델. 신경 정보 처리 시스템의 발전 35(2022), 31841-31854. [12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin. 2021. Fastnerf: 200fps의 고충실도 신경 렌더링. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 진행 중. 14346-14355. [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 2020. 생성적 적대 네트워크. Commun. ACM 63, 11(2020), 139–144. [14] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, Paul Debevec. 2021. 실시간 뷰 합성을 위한 신경 광도 필드 베이킹. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록. 58755884. [15] Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전 33(2020), 6840-6851. [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet. 2022. 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2204.03458(2022). [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang. 2022. Cogvideo: 변압기를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868(2022). [18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, Jingren Zhou. 2023. Composer: 구성 가능한 조건을 갖춘 창의적이고 제어 가능한 이미지 합성. arXiv 사전 인쇄본 arXiv:2302.09778 (2023). [19] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, Ben Poole. 2022. 꿈 필드를 사용한 제로샷 텍스트 가이드 객체 생성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 867-876. [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig. 2021. 노이즈가 많은 텍스트 감독을 통한 시각 및 시각 언어 표현 학습 확장. 기계 학습 국제 컨퍼런스에서. PMLR, 4904-4916. [21] Tero Karras, Samuli Laine, Timo Aila. 2019. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처. IEEE/CVF 컨퍼런스 ACM MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 오타와에서 개최된 컴퓨터 비전 및 패턴 인식 컨퍼런스. 4401-4410. [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 2023. Segment Anything. arXiv:2304.02643 (2023). [23] Han-Hung Lee 및 Angel X Chang. 2022. 폭셀 그리드 너프 모델을 위한 순수 클립 안내 이해. arXiv 사전 인쇄본 arXiv:2209.15172 (2022). [24] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. 2022. Blip: 통합 시각 언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. 기계 학습 국제 컨퍼런스에서. PMLR, 12888–12900. [25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. 2022. Magic3D: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.(2022). [26] William E Lorensen 및 Harvey E Cline. 1987. Marching cubes: 고해상도 3D 표면 구성 알고리즘. ACM siggraph 컴퓨터 그래픽 21, 4(1987), 163-169. [27] Shitong Luo 및 Wei Hu. 2021. 3D 포인트 클라우드 생성을 위한 확산 확률 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 2837-2845. [28] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes 및 Daniel Cohen-Or. 2022. 3D 모양 및 텍스처의 모양 안내 생성을 위한 Latent-NeRF. arXiv 사전 인쇄본 arXiv:2211.07600(2022). [29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. 2021. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. Commun. ACM 65, 1(2021), 99–106. [30] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J Guibas. 2019. Structurenet: 3D 모양 생성을 위한 계층적 그래프 네트워크. arXiv 사전 인쇄본 arXiv:1908.00575(2019). [31] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa. 2022. CLIP-Mesh: 사전 학습된 이미지-텍스트 모델을 사용하여 텍스트에서 텍스처 메시 생성. SIGGRAPH Asia 2022 컨퍼런스 논문. 1-8. [32] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie. 2023. T2i-adapter: 텍스트-이미지 확산 모델에 대한 보다 제어 가능한 기능을 발굴하기 위한 학습 어댑터. arXiv 사전 인쇄본 arXiv:2302.08453(2023). [33] Norman Müller, Andrea Simonelli, Lorenzo Porzi, Samuel Rota Bulò, Matthias Nieẞner, Peter Kontschieder. 2022. Autorf: 단일 뷰 관찰에서 3D 객체 광도장 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 3971-3980. [34] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 2022. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 기본형. ACM Transactions on Graphics(ToG) 41, 4(2022), 1-15. [35] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen. 2022. Point-E: 복잡한 프롬프트에서 3D 포인트 클라우드를 생성하는 시스템. arXiv 사전 인쇄본 arXiv:2212.08751(2022). [36] Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall. 2022. Dreamfusion: 2D 확산을 사용한 텍스트-3D. arXiv 사전 인쇄본 arXiv:2209.14988(2022). [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독으로부터 전이 가능한 시각적 모델 학습. ICML에서. PMLR, 8748-8763. [38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125 (2022). [39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. 제로샷 텍스트-이미지 생성. 기계 학습 국제 컨퍼런스에서. PMLR, 8821–8831. [40] Christian Reiser, Songyou Peng, Yiyi Liao 및 Andreas Geiger. 2021. Kilonerf: 수천 개의 작은 mlps로 신경 광도장 속도 향상. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 진행 사항. 14335-14345. [41] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan 및 Matthias Nießner. 2022. 희소 입력 뷰에서 신경 광도장에 대한 밀집 깊이 사전. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항. 12892-12901. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10684-10695. [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2022. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242(2022). [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전(2022), 36479-36494. [45] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan. 2022. Clip-forge: 제로 샷 텍스트-모양 생성을 향하여. IEEE/CVF 컴퓨터 컨퍼런스 ACM MM &#39;23 회의록, 2023년 10월 29일-11월 3일, 캐나다 오타와 Trovato et al. 비전 및 패턴 인식. 18603-18613. [46] Johannes L Schonberger 및 Jan-Michael Frahm. 2016. 동작에서 구조 재검토. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스록. 4104-4113. [47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman 등. 2022. Laion-5b: 차세대 이미지-텍스트 모델 훈련을 위한 개방형 대규모 데이터세트. arXiv 사전 인쇄 arXiv:2210.08402 (2022). [48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni 등. 2022. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄 arXiv:2209.(2022). [49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan 및 Surya Ganguli. 2015. 비평형 열역학을 사용한 심층적 비지도 학습. 국제 기계 학습 컨퍼런스에서. PMLR, 2256–2265. [50] Yang Song 및 Stefano Ermon. 2019. 데이터 분포의 기울기 추정을 통한 생성 모델링. 신경 정보 처리 시스템의 발전(2019). [51] Cheng Sun, Min Sun 및 Hwann-Tzong Chen. 2022. 직접 복셀 그리드 최적화: 광도장 재구성을 위한 초고속 수렴. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 5459-5469. [52] Jiaxiang Tang. 2022. Stable-dreamfusion: Stable-diffusion을 사용한 텍스트-3D. https://github.com/ashawkey/stable-dreamfusion. [53] Andrey Voynov, Kfir Aberman 및 Daniel Cohen-Or. 2022. 스케치 가이드 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13752(2022). [54] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh 및 Greg Shakhnarovich. 2022. 점수 Jacobian Chaining: 3D 생성을 위한 사전 학습된 2D 확산 모델 리프팅. arXiv 사전 인쇄본 arXiv:2212.00774(2022). [55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura 및 Wenping Wang. 2021. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암시적 표면 학습. arXiv 사전 인쇄본 arXiv:2106.10689(2021). [56] Chao Wen, Yinda Zhang, Zhuwen Li 및 Yanwei Fu. 2019. Pixel2mesh++: 변형을 통한 다중 뷰 3D 메시 생성. 컴퓨터 비전에 관한 IEEE/CVF 국제 컨퍼런스 진행 중. 1042-1051. [57] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou 및 Shengping Zhang. 2019. Pix2vox: 단일 및 다중 뷰 이미지에서 상황 인식 3D 재구성. 컴퓨터 비전에 관한 IEEE/CVF 국제 컨퍼런스 진행 중. 2690-2698. [58] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi 및 Zhangyang Wang. 2022. Sinnerf: 단일 이미지의 복잡한 장면에 대한 신경 복사 필드 훈련. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXII. Springer, 736-753. [59] Lvmin Zhang 및 Maneesh Agrawala. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543 (2023). [60] Linqi Zhou, Yilun Du 및 Jiajun Wu. 2021. 점-복셀 확산을 통한 3D 모양 생성 및 완성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록. 5826-5835.
