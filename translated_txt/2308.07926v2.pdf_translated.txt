--- ABSTRACT ---
우리는 전체 비디오의 정적 콘텐츠를 집계하는 표준 콘텐츠 필드와 표준 이미지(즉, 표준 콘텐츠 필드에서 렌더링됨)에서 시간 축을 따라 각 개별 프레임으로의 변환을 기록하는 시간 변형 필드로 구성된 새로운 유형의 비디오 표현으로 콘텐츠 변형 필드(CoDeF)를 제시합니다. 대상 비디오가 주어지면 이 두 필드는 신중하게 맞춤화된 렌더링 파이프라인을 통해 비디오를 재구성하기 위해 공동으로 최적화됩니다. 우리는 표준 콘텐츠 필드가 비디오에서 의미(예: 개체 모양)를 상속하도록 권장하여 최적화 프로세스에 일부 정규화를 도입합니다. 이러한 설계를 통해 CoDeF는 표준 이미지에 이미지 알고리즘을 적용하고 시간 변형 필드의 도움으로 전체 비디오에 결과를 손쉽게 전파할 수 있다는 의미에서 자연스럽게 비디오 처리를 위한 이미지 리프팅 알고리즘을 지원합니다. 우리는 CoDeF가 아무런 훈련 없이도 이미지-이미지 변환을 비디오-비디오 변환으로, 키포인트 감지를 키포인트 추적으로 리프팅할 수 있음을 실험적으로 보여줍니다. 더 중요한 것은, 알고리즘을 단 하나의 이미지에만 적용하는 리프팅 전략 덕분에, 기존의 비디오-비디오 변환 방식과 비교했을 때 처리된 비디오에서 뛰어난 크로스 프레임 일관성을 달성하고, 물과 스모그와 같은 비강체 물체도 추적할 수 있다는 것입니다. 프로젝트 페이지는 여기에서 찾을 수 있습니다. * 동등한 기여 † 연락 저자1.
--- INTRODUCTION ---
이미지 처리 분야는 광범위한 데이터 세트에서 학습된 생성 모델의 힘에 크게 기인하여 놀라운 발전을 이루었으며, 이는 뛰어난 품질과 정밀도를 제공합니다. 그러나 비디오 콘텐츠 처리에서는 비슷한 진전이 없었습니다. 한 가지 과제는 높은 시간적 일관성을 유지하는 데 있으며, 이는 신경망의 고유한 무작위성으로 인해 복잡해집니다. 또 다른 과제는 비디오 데이터 세트 자체의 특성에서 발생하는데, 이는 종종 이미지 대응물에 비해 품질이 떨어지는 텍스처를 포함하고 더 많은 계산 리소스가 필요합니다. 결과적으로 비디오 기반 알고리즘의 품질은 이미지에 초점을 맞춘 알고리즘보다 상당히 떨어집니다. 이러한 대조는 다음과 같은 의문을 제기합니다. 높은 시간적 일관성을 가진 비디오 콘텐츠에 확립된 이미지 알고리즘을 원활하게 적용하기 위해 비디오를 이미지 형태로 표현하는 것이 가능할까요? 이 목표를 추구하기 위해 연구자들은 딥 러닝 이전 시대에 동적 비디오에서 비디오 모자이크를 생성[40, 47]하고 암묵적 신경 표현을 제안한 후 신경 계층 이미지 아틀라스[16, 23, 66]를 활용할 것을 제안했습니다. 그럼에도 불구하고 이러한 방법은 두 가지 주요 결함을 보인다. 첫째, 특히 비디오 내의 복잡한 세부 사항을 충실하게 재구성하는 데 있어서 이러한 표현의 용량이 제한적이다. 종종 재구성된 비디오는 눈 깜박임이나 가벼운 미소와 같은 미묘한 동작 세부 사항을 간과한다. 두 번째 한계는 추정된 아틀라스의 일반적으로 왜곡된 특성과 관련이 있으며, 결과적으로 의미 정보가 손상된다. 따라서 기존 이미지 처리 알고리즘은 추정된 아틀라스가 충분한 자연스러움이 부족하기 때문에 최적의 성능을 발휘하지 못한다. 우리는 3D 해시 기반 시간 변형 필드와 결합된 2D 해시 기반 이미지 필드를 활용하는 비디오 표현에 대한 새로운 접근 방식을 제안한다. 시간 변형을 표현하기 위해 다중 해상도 해시 인코딩[29]을 통합하면 일반 비디오를 재구성하는 능력이 크게 향상된다. 이 공식은 물과 스모그와 같은 복잡한 엔터티의 변형을 추적하는 데 도움이 된다. 그러나 변형 필드의 강화된 기능은 자연스러운 표준 이미지를 추정하는 데 어려움을 준다. 비자연스러운 표준 이미지도 충실한 재구성으로 해당 변형 필드를 추정할 수 있습니다. 이러한 과제를 해결하기 위해 학습 중에 어닐링 해시를 사용하는 것이 좋습니다. 처음에는 부드러운 변형 그리드를 사용하여 모든 강체 동작에 적용할 수 있는 거친 솔루션을 식별하고 고주파 세부 정보를 점진적으로 추가합니다. 이 거친-미세 학습을 통해 표현은 표준의 자연스러움과 재구성의 충실성 간의 균형을 이룹니다. 이전 방법과 비교하여 재구성 품질이 눈에 띄게 향상되었습니다. 이러한 개선 사항은 표준 이미지의 자연스러움이 관찰 가능하게 증가한 것과 함께 PSNR이 약 4배 증가하는 것으로 정량화됩니다. 최적화 프로세스는 변형 필드로 표준 이미지를 추정하는 데 약 300초가 걸리지만 이전의 암묵적 계층 표현[16]은 10시간 이상 걸립니다. 제안된 콘텐츠 변형 필드를 기반으로 신속한 가이드 이미지 변환, 초고해상도 및 분할과 같은 이미지 처리 작업을 비디오 콘텐츠의 보다 동적인 영역으로 끌어올리는 방법을 설명합니다. 영어: 프롬프트 가이드 비디오 대 비디오 변환에 대한 우리의 접근 방식은 표준 이미지에 ControlNet[69]을 사용하여 학습된 변형을 통해 변환된 콘텐츠를 전파합니다. 변환 프로세스는 단일 표준 이미지에서 수행되며 모든 프레임에서 시간 집약적 추론 모델(예: 확산 모델)이 필요 없습니다. 우리의 변환 출력은 생성 모델을 사용한 최첨단 제로 샷 비디오 변환[36, 64]보다 시간적 일관성과 텍스처 품질이 현저히 향상되었습니다. 신경 계층 아틀라스에 의존하는 Text2Live와 대조적으로, 우리의 모델은 더 복잡한 동작을 처리하고 더 자연스러운 표준 이미지를 생성하여 더 뛰어난 변환 결과를 얻는 데 능숙합니다. 또한 초고해상도, 의미 분할, 키포인트 감지와 같은 이미지 알고리즘의 적용을 표준 이미지로 확장하여 비디오 컨텍스트에서 실제적으로 적용할 수 있도록 합니다. 여기에는 비디오 초고해상도, 비디오 객체 분할, 비디오 키포인트 추적 등이 포함됩니다. 우리가 제안하는 표현 방식은 지속적으로 뛰어난 시간적 일관성과 고충실도의 합성 프레임을 제공하여 비디오 처리의 획기적인 도구로서의 잠재력을 보여줍니다. 2.
--- RELATED WORK ---
암묵적 신경 표현. 좌표 기반 다층 퍼셉트론(MLP)과 결합한 암묵적 표현은 이미지[4, 49, 51], 비디오[16, 21, 49, 66], 3D/4D 표현[26, 27, 31-34, 56]을 정확하게 표현하는 강력한 역량을 보여주었습니다. 이러한 기술은 새로운 뷰 합성[27], 이미지 초고해상도[4], 3D/4D 재구성[56, 62]을 포함한 다양한 응용 분야에 사용되었습니다. 또한, 학습 속도를 높이기 위해 다양한 가속[29, 46] 기술을 탐구하여 원래 푸리에 위치 인코딩을 다중 해상도 피처 그리드나 해시 테이블과 같은 일부 이산 표현으로 대체했습니다. 또한, 암묵적 변형 필드[20,32,33,35]를 채택하면 동적 장면을 과대 맞춤하는 놀라운 역량을 보여주었습니다. 이러한 작업에서 영감을 얻어, 우리의 주요 목표는 비디오 처리 목적으로 의미론을 상속하는 표준 이미지를 활용하여 비디오를 재구성하는 것입니다.일관된 비디오 편집.우리의 연구는 다음과 밀접하게 정렬됩니다.다중 해상도 (x, y, t) (x, y, t) ⑥ ⑦ MLP 표준 필드: I 표준 이미지 리프팅 이미지 알고리즘 비디오로(예: ControlNet, Real-ESRGAN, SAM) BHA RCB 비디오 재구성 ControlNet 비디오 변환용 Real-ESRGAN 비디오 분할용 초고해상도 SAM 변형 필드: D 그림 2. 임의의 비디오를 2D 콘텐츠 표준 필드와 3D 시간 변형 필드로 인수분해하는 제안된 비디오 표현인 CoDeF의 그림. 각 필드는 효율적인 MLP를 사용하여 다중 해상도 2D 또는 3D 해시 테이블로 구현됩니다. 이러한 새로운 유형의 표현은 표준 이미지(즉, 표준 콘텐츠 필드에서 렌더링됨)에 확립된 알고리즘을 직접 적용한 다음 시간 변형 필드를 통해 시간 축을 따라 결과를 전파하는 방식으로 비디오 처리를 위한 리프팅 이미지 알고리즘을 자연스럽게 지원합니다. 일관된 비디오 편집 도메인[15, 16, 19, 23]에서는 주로 전파 기반이라는 두 가지 주요 접근 방식이 특징입니다.
--- METHOD ---
s는 두 가지 주요 결함을 보인다. 첫째, 특히 비디오 내의 복잡한 세부 사항을 충실하게 재구성하는 데 있어서 이러한 표현의 용량이 제한적이다. 종종 재구성된 비디오는 눈 깜박임이나 가벼운 미소와 같은 미묘한 동작 세부 사항을 간과한다. 두 번째 한계는 추정된 아틀라스의 일반적으로 왜곡된 특성과 관련이 있으며, 결과적으로 의미 정보가 손상된다. 따라서 기존 이미지 처리 알고리즘은 추정된 아틀라스가 충분한 자연스러움이 부족하기 때문에 최적의 성능을 발휘하지 못한다. 우리는 3D 해시 기반 시간 변형 필드와 결합된 2D 해시 기반 이미지 필드를 활용하는 비디오 표현에 대한 새로운 접근 방식을 제안한다. 시간 변형을 표현하기 위해 다중 해상도 해시 인코딩[29]을 통합하면 일반 비디오를 재구성하는 능력이 크게 향상된다. 이 공식은 물과 스모그와 같은 복잡한 엔터티의 변형을 추적하는 것을 용이하게 한다. 그러나 변형 필드의 강화된 기능은 자연스러운 표준 이미지를 추정하는 데 어려움을 준다. 비자연스러운 표준 이미지도 충실한 재구성으로 해당 변형 필드를 추정할 수 있습니다. 이러한 과제를 해결하기 위해 학습 중에 어닐링 해시를 사용하는 것이 좋습니다. 처음에는 부드러운 변형 그리드를 사용하여 모든 강체 동작에 적용할 수 있는 거친 솔루션을 식별하고 고주파 세부 정보를 점진적으로 추가합니다. 이 거친-미세 학습을 통해 표현은 표준의 자연스러움과 재구성의 충실성 간의 균형을 이룹니다. 이전 방법과 비교하여 재구성 품질이 눈에 띄게 향상되었습니다. 이러한 개선 사항은 표준 이미지의 자연스러움이 관찰 가능하게 증가한 것과 함께 PSNR이 약 4배 증가하는 것으로 정량화됩니다. 최적화 프로세스는 변형 필드로 표준 이미지를 추정하는 데 약 300초가 걸리지만 이전의 암묵적 계층 표현[16]은 10시간 이상 걸립니다. 제안된 콘텐츠 변형 필드를 기반으로 신속한 가이드 이미지 변환, 초고해상도 및 분할과 같은 이미지 처리 작업을 비디오 콘텐츠의 보다 동적인 영역으로 끌어올리는 방법을 설명합니다. 영어: 프롬프트 가이드 비디오 대 비디오 변환에 대한 우리의 접근 방식은 표준 이미지에 ControlNet[69]을 사용하여 학습된 변형을 통해 변환된 콘텐츠를 전파합니다. 변환 프로세스는 단일 표준 이미지에서 수행되며 모든 프레임에서 시간 집약적 추론 모델(예: 확산 모델)이 필요 없습니다. 우리의 변환 출력은 생성 모델을 사용한 최첨단 제로 샷 비디오 변환[36, 64]보다 시간적 일관성과 텍스처 품질이 현저히 향상되었습니다. 신경 계층 아틀라스에 의존하는 Text2Live와 대조적으로, 우리의 모델은 더 복잡한 동작을 처리하고 더 자연스러운 표준 이미지를 생성하여 더 뛰어난 변환 결과를 얻는 데 능숙합니다. 또한 초고해상도, 의미 분할, 키포인트 감지와 같은 이미지 알고리즘의 적용을 표준 이미지로 확장하여 비디오 컨텍스트에서 실제적으로 적용할 수 있도록 합니다. 여기에는 비디오 초고해상도, 비디오 객체 분할, 비디오 키포인트 추적 등이 포함됩니다. 제안된 표현 방식은 지속적으로 뛰어난 시간적 일관성과 고충실도 합성 프레임을 제공하여 비디오 처리에서 획기적인 도구로서의 잠재력을 보여줍니다.2. 관련 연구 암묵적 신경 표현.좌표 기반 다층 퍼셉트론(MLP)과 결합한 암묵적 표현은 이미지[4, 49, 51], 비디오[16, 21, 49, 66], 3D/4D 표현[26, 27, 31-34,56]을 정확하게 표현하는 강력한 역량을 보여주었습니다.이러한 기술은 새로운 뷰 합성[27], 이미지 초해상도[4], 3D/4D 재구성[56,62]을 포함한 다양한 응용 분야에 사용되었습니다.또한 학습 속도를 높이기 위해 다양한 가속[29, 46] 기술을 탐색하여 원래 푸리에 위치 인코딩을 다중 해상도 피처 그리드 또는 해시 테이블과 같은 일부 이산 표현으로 대체했습니다. 또한 암묵적 변형 필드[20,32,33,35]를 채택하면 동적 장면을 과대적합하는 놀라운 능력이 나타났습니다. 이러한 작업에서 영감을 받아 비디오 처리 목적으로 의미론을 상속하는 표준 이미지를 활용하여 비디오를 재구성하는 것이 주요 목표입니다. 일관된 비디오 편집. 연구는 다음과 밀접하게 정렬됩니다. 다중 해상도(x, y, t) (x, y, t) ⑥ ⑦ MLP 표준 필드: I 표준 이미지 비디오에 대한 이미지 리프팅 알고리즘(예: ControlNet, Real-ESRGAN, SAM) BHA RCB 비디오 변환을 위한 비디오 재구성 ControlNet 비디오를 위한 Real-ESRGAN 비디오 분할을 위한 초고해상도 SAM 변형 필드: D 그림 2. 임의의 비디오를 2D 콘텐츠 표준 필드와 3D 시간 변형 필드로 인수분해하는 제안된 비디오 표현인 CoDeF의 그림입니다. 각 필드는 효율적인 MLP를 사용하여 다중 해상도 2D 또는 3D 해시 테이블로 구현됩니다. 이러한 새로운 유형의 표현은 표준 이미지(즉, 표준 콘텐츠 필드에서 렌더링됨)에 확립된 알고리즘을 직접 적용한 다음 시간 변형 필드를 통해 시간 축을 따라 결과를 전파하는 방식으로 비디오 처리를 위한 리프팅 이미지 알고리즘을 자연스럽게 지원합니다.일관된 비디오 편집 도메인[15, 16, 19, 23]과 함께 주로 전파 기반 방법과 계층화된 표현 기반 기술의 두 가지 주요 접근 방식을 특징으로 합니다.전파 기반 방법[13–15,43, 53,60]은 초기 프레임을 편집한 다음 해당 편집 내용을 비디오 시퀀스 전체에 배포하는 데 중점을 둡니다.이 접근 방식은 계산 효율성과 단순성 측면에서 이점을 제공하지만 특히 복잡한 동작이나 폐색이 특징인 상황에서 편집 내용을 전파하는 동안 부정확성과 불일치가 발생하기 쉽습니다. 반대로, 계층적 표현 기반 기술[16, 23, 24, 40, 47]은 비디오를 여러 개의 개별 레이어로 분해하여 편집 프로세스 중에 더 큰 제어와 유연성을 용이하게 합니다.Text2Live[1]는 텍스트 입력을 사용하여 최적화된 아틀라스[16]를 수정하여 비디오 편집을 위한 CLIP[37] 모델을 적용하여 시간적으로 일관된 비디오 편집 결과를 제공합니다.우리의 작업은 비디오에 대한 최적화된 표현을 사용한다는 맥락에서 Text2Live와 유사합니다.그러나 우리의 방법론은 여러 측면에서 다릅니다.해시 기반 변형 가능한 디자인을 통합하여 의미를 더 잘 인식하는 표준 표현을 최적화하고 더 높은 충실도의 비디오 처리를 달성합니다.생성 모델을 통한 비디오 처리.확산 모델의 발전으로 텍스트-이미지 생성의 합성 품질이 현저히 향상되어[6, 11, 50] 이전 방법론의 성능[25,41,65, 68]을 능가했습니다. GLIDE[30], Dall-E 2[38, 39], Stable Diffusion[42], Imagen[45]과 같은 최신 확산 모델은 수백만 개의 이미지에서 학습되었으며, 그 결과 뛰어난 생성 기능을 갖추게 되었습니다.기존의 텍스트-이미지(T2I) 모델은 자유 텍스트 생성을 가능하게 하지만, 에지, 깊이 맵, 노멀 맵과 같은 추가적인 조건화 요소[2,3,9,28,44,54,58,69]를 통합하는 것은 정밀한 제어를 달성하는 데 필수적입니다.제어 가능성을 향상시키기 위한 노력의 일환으로 연구자들은 여러 가지 접근 방식을 제안했습니다.예를 들어, PITI[58]는 잠재 객체를 T2I 잠재 공간에 매핑하도록 이미지 인코더를 다시 학습하는 것을 포함합니다.반면에 InstructPix2Pix[2]는 합성된 이미지 조건 쌍을 사용하여 T2I 모델을 미세 조정합니다.ControlNet[69]은 보조 분기를 통해 안정적 확산에 대한 추가적인 제어 조건을 도입하여 입력 조건 맵을 충실히 준수하는 이미지를 생성합니다. 최근의 연구 방향은 텍스트-이미지(T2I) 모델을 독점적으로 활용한 비디오 처리에 집중합니다. Tune-AVideo[64], Text2 Video-Zero[17], FateZero[36], Vid2VidZero[59], Video-P2P[22]와 같은 접근 방식은 DDIM[50]의 잠재 공간을 탐색하고 일관된 생성을 용이하게 하기 위해 프레임 간 주의 맵을 통합합니다. 그럼에도 불구하고 이러한 방법은 생성의 본질적인 무작위성으로 인해 시간적 일관성이 손상될 수 있으며 제어 조건이 정밀하게 달성되지 않을 수 있습니다. 텍스트-비디오 생성은 최근 몇 년 동안 중요한 연구 분야로 떠올랐으며, 널리 퍼진 접근 방식에는 광범위한 데이터 세트에 대한 확산 모델 또는 자기 회귀 변환기의 학습이 포함됩니다. NUWA [63], CogVideo [12], Phenaki [55], Make-A-Video [48], Imagen Video [10], Gen-1 [7]과 같은 텍스트-비디오 아키텍처는 입력 텍스트와 의미적으로 일치하는 비디오 프레임을 생성할 수 있지만 상당한 계산 요구로 인해 비디오 조건에 대한 정확한 제어나 낮은 해상도 측면에서 한계가 있을 수 있습니다.3. 방법 문제 공식화. 프레임 {I₁, I2, ..., IN}으로 구성된 비디오 V가 주어지면 해당 비디오 작업에 대해 각 프레임에 개별적으로 이미지 처리 알고리즘 X를 순진하게 적용할 수 있지만 프레임 간에 바람직하지 않은 불일치가 관찰될 수 있습니다.또 다른 전략은 시간 모듈로 알고리즘 X를 향상시키는 것으로, 비디오 데이터에 대한 추가 학습이 필요합니다.그러나 단순히 시간 모듈을 도입하는 것은 이론적 일관성을 보장하기 어렵고 학습 데이터가 부족하여 성능이 저하될 수 있습니다. 이러한 과제에 동기를 부여받아, 우리는 평평한 표준 이미지 Ic와 변형 필드 D를 사용하여 비디오를 표현하는 것을 제안합니다. Ic에 이미지 알고리즘 X를 적용함으로써, 우리는 학습된 변형 필드로 전체 비디오에 효과를 효과적으로 전파할 수 있습니다. 이 새로운 비디오 표현은 이미지 알고리즘과 비디오 작업 간의 중요한 교량 역할을 하여, 최첨단 이미지 방법론을 비디오 애플리케이션에 직접 적용할 수 있습니다. 제안된 표현은 다음과 같은 필수 특성을 보여야 합니다. • 충실한 비디오 재구성을 위한 피팅 기능. 표현은 비디오에서 큰 강체 또는 비강체 변형을 정확하게 피팅할 수 있는 기능을 가져야 합니다. • 표준 이미지의 의미적 정확성. 왜곡되거나 의미적으로 잘못된 표준 이미지는 이미지 처리 성능 저하로 이어질 수 있으며, 특히 이러한 프로세스의 대부분이 자연스러운 이미지 데이터에서 학습된다는 점을 고려할 때 더욱 그렇습니다. • 변형 필드의 부드러움. 변형 필드의 부드러움 보장은 시간적 일관성과 올바른 전파를 보장하는 필수 기능입니다. 3.1. 콘텐츠 변형 필드 동적 NeRF[32, 33]에서 영감을 얻어 비디오를 두 가지 뚜렷한 구성 요소인 표준 필드와 변형 필드로 표현하도록 제안합니다. 이 두 구성 요소는 각각 2D 및 3D 해시 테이블을 사용하여 실현됩니다. 이러한 해시 테이블의 용량을 향상시키기 위해 두 개의 미세 MLP가 통합됩니다. 그림 2에서 설명한 대로 비디오를 재구성하고 처리하기 위한 제안된 표현을 제시합니다. 프레임 {I₁, I2, ..., IN}으로 구성된 비디오 V가 주어지면 이러한 프레임에 맞게 조정된 암묵적 변형 가능 모델을 학습합니다. 이 모델은 두 개의 좌표 기반 MLP인 변형 필드 D와 표준 필드 C로 구성됩니다. 표준 필드 C는 비디오 V에 있는 모든 평탄화된 텍스처를 포함하는 연속 표현으로 사용됩니다. 이는 2D 위치 x : (x,y)를 색상 c (r, g, b)로 매핑하는 함수 F : ✗ → C로 정의됩니다. 학습 속도를 높이고 네트워크가 고주파 세부 정보를 캡처할 수 있도록 다중 해상도 해시 인코딩 Y2D: R² → R²+FXL을 채택하여 좌표 x를 피처 벡터로 매핑합니다.여기서 L은 다중 해상도의 레벨 수이고 F는 레이어당 피처 차원 수입니다.함수 : = Y2D(X) (x, F1(x), ..., FL (x))는 모델이 고주파 세부 정보를 캡처하는 기능을 용이하게 합니다.여기서 Fi(x)는 x에 의해 ¿th 해상도에서 선형 보간된 피처입니다.변형 필드 D는 비디오 내의 모든 프레임에 대한 관찰-표준 변형을 캡처합니다.특정 프레임 Ii의 경우 D는 관찰 위치와 표준 위치 간의 대응 관계를 설정합니다.동적 NeRF[32,33]는 푸리에 위치 인코딩과 추가 학습 가능한 시간 코드를 사용하여 3D 공간에서 변형 필드를 구현합니다.이 구현은 변형 필드의 부드러움을 보장합니다. 그럼에도 불구하고, 이 간단한 구현은 두 가지 이유(즉, 낮은 학습 효율성과 부적절한 대표 기능)로 비디오 표현으로 원활하게 전환할 수 없습니다.따라서 우리는 변형 필드를 작은 MLP가 따르는 3D 해시 테이블로 표현하도록 제안합니다.특히, t번째 프레임의 임의의 위치 x는 먼저 3D 해시 인코딩 함수 3D(x, t)에 의해 인코딩되어 고차원 피처를 얻습니다.그런 다음 작은 MLP D : (√3D(x, t)) → x&#39;는 임베디드 피처를 표준 필드의 해당 위치 x&#39;에 매핑합니다.우리는 다음과 같이 3D 해시 인코딩 기반 변형 필드를 자세히 설명합니다.변형 필드를 위한 3D 해시 인코딩.특히, 비디오의 임의의 지점은 직교 3D 공간 내의 위치 X3D(x, y, t)로 개념화될 수 있습니다.그림 2의 왼쪽에 표시된 것처럼 3D 해시 인코딩 기술을 사용하여 비디오 공간을 표현합니다.이 기술은 3D 공간을 다중 해상도 피처 그리드로 캡슐화합니다. 다중 해상도라는 용어는 다양한 수준의 해상도를 가진 그리드의 구성을 나타내며, 피처 그리드는 각 정점에서 학습 가능한 피처로 채워진 그리드를 나타냅니다. 프레임워크에서 다중 해상도 피처 그리드는 L개의 개별 레벨로 구성됩니다. 학습 가능한 피처의 차원은 F로 표현됩니다. 또한 N₁로 표시되는 7번째 레이어의 해상도는 [Nmin, Nmax]로 집합적으로 표시되는 가장 거친 해상도와 가장 정밀한 해상도 사이의 기하적 진행을 나타내며, N₁ = [Nmin b&#39;], b = exp를 사용합니다. Nmax에서 - Nmin L에서 -(1) 7번째 레이어에서 쿼리된 점 X3D를 고려하면 입력 좌표는 해당 레벨의 그리드 해상도로 크기 조정됩니다. 그리고 X3D의 쿼리된 피처는 8개의 이웃 코너 점(그림 2 참조)에서 삼선형 보간됩니다. x3D의 모서리 점을 얻기 위해 내림과 올림은 먼저 [X 3D = [X3D N₁], [x3D] = [×³D · Nɩ], . 로 연산되고, 각 모서리를 최대 T의 고정 크기를 갖는 레벨의 해당 피처 벡터 배열의 항목에 매핑합니다. 거친 레벨의 경우 저해상도 그리드의 매개변수는 T보다 적으며, 여기서 매핑은 1 : 1입니다. 따라서 인덱스로 피처를 직접 찾을 수 있습니다. 반대로. 더 미세한 해상도의 경우, 점은 해시 함수, h(x¹³3D) = (±²²±1xiπ i ) mod T, (3)로 매핑됩니다. 여기서 ☺는 비트별 XOR 연산을 나타내고 {~¿}는 [29]에 따른 고유한 큰 소수입니다. 프레임 t의 좌표 x에서 출력 색상 값은 c = C(D(Y3D(x, t)))로 계산할 수 있습니다. (4) 이 출력은 입력 프레임에 있는 기준 진실 색상을 사용하여 감독할 수 있습니다. 3.2. 모델 설계 제안된 표현은 임의의 비디오에 대한 표준 콘텐츠와 시간 변형을 모두 효과적으로 모델링하고 재구성할 수 있습니다. 그러나 견고한 비디오 처리에 대한 요구 사항을 충족하는 데 어려움이 있습니다. 특히 3D 해시 변형은 강력한 피팅 기능을 가지고 있지만 시간 변형의 부드러움을 손상시킵니다. 이러한 균형으로 인해 표준 이미지의 고유한 의미론을 유지하는 것이 현저히 어려워져 기존 이미지 알고리즘을 비디오 사용에 적용하는 데 상당한 장벽이 생깁니다. 표준 이미지의 고유한 의미를 보존하면서 정확한 비디오 재구성을 달성하기 위해 어닐링된 다중 해상도 해시 인코딩을 사용할 것을 제안합니다. 변형의 부드러움을 더욱 향상시키기 위해 흐름 기반 일관성을 도입합니다. 큰 폐색이나 복잡한 다중 객체 시나리오와 같은 어려운 경우에는 추가 의미 정보를 활용할 것을 제안합니다. 이는 그룹화된 변형 필드와 함께 의미 마스크를 사용하여 달성할 수 있습니다. 변형을 위한 어닐링 3D 해시 인코딩. 더 세밀한 해상도를 위해 해시 인코딩은 복잡한 변형 피팅 성능을 향상시키지만 표준 필드에 불연속성과 왜곡을 도입합니다(그림 9 참조). 동적 NeRF에서 사용된 어닐링 전략[32]에서 영감을 얻어 변형을 위한 점진적 주파수 필터에 어닐링 해시 인코딩 기술을 사용합니다. 더 구체적으로, 우리는 다른 해상도로 보간된 해당 기능에 대해 점진적 제어 가중치를 사용합니다. 학습 단계 k에서 7번째 계층의 가중치는 1 - cos(π clamp(m(j – Neg)/Nstep, 0, 1)) wj (k) . &quot; (5)로 계산됩니다. 여기서 Noeg는 어닐링을 시작하기 위한 사전 정의된 단계이고 m은 어닐링 속도를 제어하기 위한 하이퍼 매개변수를 나타내며 Nstep은 어닐링 단계의 숫자입니다. 흐름 가이드 일관성 손실. 높은 신뢰도로 흐름에 의해 식별된 해당 지점은 표준 필드에서 동일한 지점이어야 합니다. 흐름 가이드 재구성된 비디오 입력 비디오 표준 이미지 계층화된 신경 아틀라스를 계산합니다. 그림 3. 비디오 표현의 용량을 반영하고 충실한 비디오 처리에서 기본적인 역할을 하는 비디오 재구성과 관련하여 계층화된 신경 아틀라스[16]와 CoDeF 간의 정성적 비교. 세부 사항은 확대하면 가장 잘 이해할 수 있습니다. 이 관찰에 따르면 일관성 손실. 두 개의 연속된 프레임 I¿ 및 I₁+1의 경우 RAFT[52]를 사용하여 전방 흐름을 감지합니다. Fi→i+1 및 역방향 흐름 Fi+1. 프레임 I¿의 신뢰 영역은 Mflow = |Warp(Warp(Ii, Fi→i+1), Fi+1→i)—Ii| &lt; e, (6)로 정의할 수 있습니다. 여기서 €는 오류 임계값에 대한 하이퍼파라미터를 나타냅니다. 이 손실은 다음과 같이 공식화할 수 있습니다. = flow Lflow (3D(x, t)) — D(73D(x + F{\t+1; t + 1)) − F¥\+t+1|| * Mow (7) 여기서 Ft+1 및 Mw는 광학 흐름과 x에서의 흐름 신뢰도입니다. 흐름 손실은 특히 매끄러운 영역에 대해 변형 필드의 부드러움을 효율적으로 정규화합니다. 그룹화된 콘텐츠 변형 필드. 표현은 단일 콘텐츠 변형 필드를 사용하여 비디오를 재구성하는 방법을 학습할 수 있지만 겹치는 여러 객체에서 발생하는 복잡한 동작은 하나의 표준 내에서 충돌을 일으킬 수 있습니다. 결과적으로 경계 영역은 부정확한 재구성으로 인해 어려움을 겪을 수 있습니다. 큰 폐색이 특징인 까다로운 인스턴스의 경우 여러 콘텐츠 변형 필드에 해당하는 레이어를 도입하는 옵션을 제안합니다. 이러한 레이어는 의미 분할을 기반으로 정의되므로 이러한 까다로운 시나리오에서 비디오 재구성의 정확도와 견고성이 향상됩니다. SAMtrack(Segment-Anything-track) [5]을 활용하여 각 비디오 프레임의 분할을 달성합니다. 원본 비디오 Ours Text2Live Tune-A-Video Fate Zero urde urder 텍스트 프롬프트: Makoto Shinkai의 Your name, 잘생긴 남자와 아름다운 여자 텍스트 프롬프트: 사이버펑크 로봇 텍스트 프롬프트: 아이언 맨 그림 4. Text2Live [1], Tune-A-Video [64], FateZero [36] 및 CoDeF를 통한 ControlNet [69] 직접 리프팅을 포함한 다양한 방법에 걸친 텍스트 안내 비디오 대 비디오 변환 작업에 대한 정성적 비교. 시간적 일관성과 합성 품질에 대한 자세한 평가를 위해 프로젝트 페이지의 비디오를 보는 것이 좋습니다. I¿를 마스크 Mo, ..., M²-1을 사용하여 K개의 의미적 층으로 나눕니다. 그리고 각 층에서 표준 필드와 변형 필드의 그룹을 사용하여 서로 다른 객체의 개별 동작을 표현합니다. 이러한 모델은 이후 암묵적 필드의 그룹으로 공식화됩니다. D: {D1, ..., DK}, C: {C1, CK}. 이론적으로 프레임 i의 의미적 층 k의 경우 효율적인 재구성을 위해 M½ 영역에서 픽셀을 샘플링하는 것으로 충분합니다. 그러나 해시 인코딩은 비지도 영역에서 무작위적이고 비구조적 패턴을 초래할 수 있으며, 이는 자연스러운 이미지에서 학습된 이미지 기반 모델의 성능을 저하시킵니다. 이 문제를 해결하기 위해 M½ 영역 외부의 여러 지점을 샘플링하고 기준 진실 색상을 사용하여 L2 손실을 사용하여 학습합니다. 이런 식으로 배경 손실 Lbg로 M을 효과적으로 정규화합니다. 결과적으로 표준 이미지는 더 자연스러운 모양을 갖게 되어 향상된 처리 결과가 나타납니다. 학습 목표. 표현은 목적 함수 rec를 최소화하여 학습합니다. 이 함수는 주어진 좌표 x에 대한 기준 색상과 예측 색상 c 사이의 L2 손실에 해당합니다. 학습 프로세스를 정규화하고 안정화하기 위해 이전에 논의한 대로 추가 정규화 항을 도입합니다. 총 손실은 다음 방정식 L == Lrec + 1 * Lflow, (8)을 사용하여 계산합니다. 여기서 ₁은 손실 가중치에 대한 하이퍼 매개변수를 나타냅니다. 그룹화된 변형 필드를 학습할 때 №2 * Lbg로 표시되는 추가 정규화기를 포함한다는 점에 유의하는 것이 중요합니다. 1번째 프레임 10번째 프레임 20번째 프레임 30번째 프레임 표준 이미지 그림 5. CoDeF로 비디오를 재구성한 후 시간 변형 필드에서 직접 추출한 프레임 간 포인트 대응의 시각화. 3.3. 일관된 비디오 처리에 대한 응용 프로그램 콘텐츠 변형 필드를 최적화하면 모든 포인트의 변형을 0으로 설정하여 표준 이미지 Į를 검색합니다. 원본 이미지 크기보다 더 큰 표준 이미지의 크기를 비디오에서 관찰되는 장면 움직임에 따라 유연하게 조정할 수 있으므로 더 많은 콘텐츠를 포함할 수 있다는 점에 유의하는 것이 중요합니다. 그런 다음 표준 이미지 Ic를 사용하여 일관된 비디오 처리를 위한 다양한 다운스트림 알고리즘을 실행합니다. 다음과 같은 최신(SOTA) 알고리즘을 평가했습니다. (1) ControlNet[69]: 프롬프트 가이드 비디오 간 변환에 사용됩니다. (2) Segment-anything(SAM)[18]: 비디오 객체 추적에 적용됩니다. (3) R-ESRGAN[61]: 비디오 초해상도에 사용됩니다. 또한 표준 이미지를 사용하면 이미지를 직접 수정하여 비디오를 편리하게 편집할 수 있습니다. 여러 수동 비디오 편집 예를 통해 이 기능을 추가로 설명합니다. 4.
--- EXPERIMENT ---
ally는 CoDeF가 이미지 대 이미지 변환을 비디오 대 비디오 변환으로, 키포인트 감지를 키포인트 추적으로 아무런 훈련 없이 끌어올릴 수 있음을 보여줍니다. 더 중요한 것은 알고리즘을 단 하나의 이미지에만 배치하는 리프팅 전략 덕분에 기존 비디오 대 비디오 변환 접근 방식에 비해 처리된 비디오에서 뛰어난 크로스 프레임 일관성을 달성하고 물과 스모그와 같은 비강체 물체도 추적할 수 있다는 것입니다. 프로젝트 페이지는 여기에서 찾을 수 있습니다. * 동등한 기여 † 책임 저자 1. 서론 이미지 처리 분야는 광범위한 데이터 세트에서 훈련된 생성 모델의 힘에 크게 기인하여 놀라운 발전을 이루었으며, 뛰어난 품질과 정밀도를 제공합니다. 그러나 비디오 콘텐츠 처리에서는 비슷한 진전을 이루지 못했습니다. 한 가지 과제는 높은 시간적 일관성을 유지하는 것인데, 이는 신경망의 고유한 무작위성으로 인해 복잡해지는 작업입니다. 또 다른 과제는 비디오 데이터 세트 자체의 특성에서 비롯되는데, 이는 종종 이미지 대응물에 비해 품질이 떨어지는 텍스처를 포함하고 더 많은 계산 리소스가 필요합니다. 결과적으로 비디오 기반 알고리즘의 품질은 이미지에 초점을 맞춘 알고리즘보다 상당히 떨어집니다. 이러한 대조는 다음과 같은 의문을 제기합니다. 높은 시간적 일관성을 가진 비디오 콘텐츠에 확립된 이미지 알고리즘을 원활하게 적용하기 위해 비디오를 이미지 형태로 표현하는 것이 가능할까요? 이 목표를 추구하면서 연구자들은 딥 러닝 이전 시대에 동적 비디오에서 비디오 모자이크를 생성[40, 47]하고 암묵적 신경 표현을 제안한 후 신경 계층 이미지 아틀라스[16, 23, 66]를 활용할 것을 제안했습니다. 그럼에도 불구하고 이러한 방법은 두 가지 주요 결함을 보입니다. 첫째, 특히 비디오 내의 복잡한 세부 사항을 충실하게 재구성하는 데 있어 이러한 표현의 용량이 제한됩니다. 종종 재구성된 비디오는 눈 깜박임이나 가벼운 미소와 같은 미묘한 동작 세부 사항을 간과합니다. 두 번째 제한은 추정된 아틀라스의 일반적으로 왜곡된 특성과 관련이 있으며, 결과적으로 의미 정보가 손상됩니다. 따라서 기존의 이미지 처리 알고리즘은 추정된 아틀라스가 충분한 자연스러움이 부족하기 때문에 최적의 성능을 발휘하지 못합니다. 우리는 3D 해시 기반 시간 변형 필드와 결합된 2D 해시 기반 이미지 필드를 활용하는 비디오 표현에 대한 새로운 접근 방식을 제안합니다. 시간 변형을 표현하기 위한 다중 해상도 해시 인코딩[29]을 통합하면 일반 비디오를 재구성하는 능력이 크게 향상됩니다. 이 공식은 물과 스모그와 같은 복잡한 개체의 변형을 추적하는 데 도움이 됩니다. 그러나 변형 필드의 기능이 향상되어 자연스러운 표준 이미지를 추정하는 데 어려움이 있습니다. 비자연스러운 표준 이미지도 충실한 재구성으로 해당 변형 필드를 추정할 수 있습니다. 이 과제를 해결하기 위해 학습 중에 어닐링 해시를 사용하는 것이 좋습니다. 처음에는 부드러운 변형 그리드를 사용하여 모든 강체 동작에 적용할 수 있는 거친 솔루션을 식별하고 고주파 세부 정보를 점진적으로 추가합니다. 이 거친-미세 학습을 통해 표현은 표준의 자연스러움과 재구성의 충실성 사이의 균형을 이룹니다. 우리는 이전 방법과 비교하여 재구성 품질에서 주목할 만한 향상을 관찰합니다. 이 개선은 약 4배의 PSNR 증가와 표준 이미지의 자연스러움의 관찰 가능한 증가로 정량화됩니다. 우리의 최적화 프로세스는 변형 필드로 표준 이미지를 추정하는 데 불과 약 300초가 필요한 반면, 이전의 암묵적 계층 표현[16]은 10시간 이상 걸립니다. 제안된 콘텐츠 변형 필드를 기반으로, 프롬프트 가이드 이미지 변환, 초고해상도 및 분할과 같은 리프팅 이미지 처리 작업을 비디오 콘텐츠의 보다 동적인 영역으로 설명합니다. 프롬프트 가이드 비디오-비디오 변환에 대한 우리의 접근 방식은 표준 이미지에 ControlNet[69]을 사용하여 학습된 변형을 통해 변환된 콘텐츠를 전파합니다. 변환 프로세스는 단일 표준 이미지에서 수행되며 모든 프레임에서 시간 집약적인 추론 모델(예: 확산 모델)이 필요하지 않습니다. 영어: 저희의 번역 출력은 생성 모델을 사용한 최첨단 제로샷 비디오 번역보다 시간적 일관성과 텍스처 품질에서 현저한 개선을 보여줍니다[36, 64]. 신경 계층 아틀라스에 의존하는 Text2Live와 대조적으로 저희 모델은 더 복잡한 동작을 처리하고, 더 자연스러운 표준 이미지를 생성하고, 그 결과 더 뛰어난 번역 결과를 얻는 데 능숙합니다. 또한 초고해상도, 의미 분할, 키포인트 감지와 같은 이미지 알고리즘의 적용을 표준 이미지로 확장하여 비디오 컨텍스트에서 실제적으로 적용할 수 있도록 합니다. 여기에는 비디오 초고해상도, 비디오 객체 분할, 비디오 키포인트 추적 등이 포함됩니다. 저희가 제안한 표현은 일관되게 뛰어난 시간적 일관성과 고충실도 합성 프레임을 제공하여 비디오 처리에서 획기적인 도구로서의 잠재력을 보여줍니다. 2. 관련 연구 암묵적 신경 표현. 좌표 기반 다층 퍼셉트론(MLP)과 결합한 암시적 표현은 이미지[4, 49, 51], 비디오[16, 21, 49, 66], 3D/4D 표현[26, 27, 31-34, 56]을 정확하게 표현하는 강력한 역량을 보여주었습니다. 이러한 기술은 새로운 뷰 합성[27], 이미지 초해상도[4], 3D/4D 재구성[56, 62]을 포함한 다양한 응용 분야에 사용되었습니다. 또한, 학습 속도를 높이기 위해 다양한 가속[29, 46] 기술을 탐구하여 원래 푸리에 위치 인코딩을 다중 해상도 피처 그리드나 해시 테이블과 같은 일부 이산 표현으로 대체했습니다. 또한 암시적 변형 필드[20,32,33,35]를 채택하면 동적 장면을 과대 맞춤하는 놀라운 역량을 보여주었습니다. 이러한 작업에서 영감을 얻어, 우리의 주요 목표는 비디오 처리 목적으로 의미론을 상속하는 표준 이미지를 활용하여 비디오를 재구성하는 것입니다.일관된 비디오 편집.우리의 연구는 다음과 밀접하게 정렬됩니다.다중 해상도 (x, y, t) (x, y, t) ⑥ ⑦ MLP 표준 필드: I 표준 이미지 리프팅 이미지 알고리즘 비디오로(예: ControlNet, Real-ESRGAN, SAM) BHA RCB 비디오 재구성 ControlNet 비디오 변환용 Real-ESRGAN 비디오 분할용 초고해상도 SAM 변형 필드: D 그림 2. 임의의 비디오를 2D 콘텐츠 표준 필드와 3D 시간 변형 필드로 인수분해하는 제안된 비디오 표현인 CoDeF의 그림. 각 필드는 효율적인 MLP를 사용하여 다중 해상도 2D 또는 3D 해시 테이블로 구현됩니다. 이러한 새로운 유형의 표현은 표준 이미지(즉, 표준 콘텐츠 필드에서 렌더링됨)에 확립된 알고리즘을 직접 적용한 다음 시간 변형 필드를 통해 시간 축을 따라 결과를 전파하는 방식으로 비디오 처리를 위한 리프팅 이미지 알고리즘을 자연스럽게 지원합니다.일관된 비디오 편집 도메인[15, 16, 19, 23]과 함께 주로 전파 기반 방법과 계층화된 표현 기반 기술의 두 가지 주요 접근 방식을 특징으로 합니다.전파 기반 방법[13–15,43, 53,60]은 초기 프레임을 편집한 다음 해당 편집 내용을 비디오 시퀀스 전체에 배포하는 데 중점을 둡니다.이 접근 방식은 계산 효율성과 단순성 측면에서 이점을 제공하지만 특히 복잡한 동작이나 폐색이 특징인 상황에서 편집 내용을 전파하는 동안 부정확성과 불일치가 발생하기 쉽습니다. 반대로, 계층적 표현 기반 기술[16, 23, 24, 40, 47]은 비디오를 여러 개의 개별 레이어로 분해하여 편집 프로세스 중에 더 큰 제어와 유연성을 용이하게 합니다.Text2Live[1]는 텍스트 입력을 사용하여 최적화된 아틀라스[16]를 수정하여 비디오 편집을 위한 CLIP[37] 모델을 적용하여 시간적으로 일관된 비디오 편집 결과를 제공합니다.우리의 작업은 비디오에 대한 최적화된 표현을 사용한다는 맥락에서 Text2Live와 유사합니다.그러나 우리의 방법론은 여러 측면에서 다릅니다.해시 기반 변형 가능한 디자인을 통합하여 의미를 더 잘 인식하는 표준 표현을 최적화하고 더 높은 충실도의 비디오 처리를 달성합니다.생성 모델을 통한 비디오 처리.확산 모델의 발전으로 텍스트-이미지 생성의 합성 품질이 현저히 향상되어[6, 11, 50] 이전 방법론의 성능[25,41,65, 68]을 능가했습니다. GLIDE[30], Dall-E 2[38, 39], Stable Diffusion[42], Imagen[45]과 같은 최신 확산 모델은 수백만 개의 이미지에서 학습되었으며, 그 결과 뛰어난 생성 기능을 갖추게 되었습니다.기존의 텍스트-이미지(T2I) 모델은 자유 텍스트 생성을 가능하게 하지만, 에지, 깊이 맵, 노멀 맵과 같은 추가적인 조건화 요소[2,3,9,28,44,54,58,69]를 통합하는 것은 정밀한 제어를 달성하는 데 필수적입니다.제어 가능성을 향상시키기 위한 노력의 일환으로 연구자들은 여러 가지 접근 방식을 제안했습니다.예를 들어, PITI[58]는 잠재 객체를 T2I 잠재 공간에 매핑하도록 이미지 인코더를 다시 학습하는 것을 포함합니다.반면에 InstructPix2Pix[2]는 합성된 이미지 조건 쌍을 사용하여 T2I 모델을 미세 조정합니다.ControlNet[69]은 보조 분기를 통해 안정적 확산에 대한 추가적인 제어 조건을 도입하여 입력 조건 맵을 충실히 준수하는 이미지를 생성합니다. 최근의 연구 방향은 텍스트-이미지(T2I) 모델을 독점적으로 활용한 비디오 처리에 집중합니다. Tune-AVideo[64], Text2 Video-Zero[17], FateZero[36], Vid2VidZero[59], Video-P2P[22]와 같은 접근 방식은 DDIM[50]의 잠재 공간을 탐색하고 일관된 생성을 용이하게 하기 위해 프레임 간 주의 맵을 통합합니다. 그럼에도 불구하고 이러한 방법은 생성의 본질적인 무작위성으로 인해 시간적 일관성이 손상될 수 있으며 제어 조건이 정밀하게 달성되지 않을 수 있습니다. 텍스트-비디오 생성은 최근 몇 년 동안 중요한 연구 분야로 떠올랐으며, 널리 퍼진 접근 방식에는 광범위한 데이터 세트에 대한 확산 모델 또는 자기 회귀 변환기의 학습이 포함됩니다. NUWA [63], CogVideo [12], Phenaki [55], Make-A-Video [48], Imagen Video [10], Gen-1 [7]과 같은 텍스트-비디오 아키텍처는 입력 텍스트와 의미적으로 일치하는 비디오 프레임을 생성할 수 있지만 상당한 계산 요구로 인해 비디오 조건에 대한 정확한 제어나 낮은 해상도 측면에서 한계가 있을 수 있습니다.3. 방법 문제 공식화. 프레임 {I₁, I2, ..., IN}으로 구성된 비디오 V가 주어지면 해당 비디오 작업에 대해 각 프레임에 개별적으로 이미지 처리 알고리즘 X를 순진하게 적용할 수 있지만 프레임 간에 바람직하지 않은 불일치가 관찰될 수 있습니다.또 다른 전략은 시간 모듈로 알고리즘 X를 향상시키는 것으로, 비디오 데이터에 대한 추가 학습이 필요합니다.그러나 단순히 시간 모듈을 도입하는 것은 이론적 일관성을 보장하기 어렵고 학습 데이터가 부족하여 성능이 저하될 수 있습니다. 이러한 과제에 동기를 부여받아, 우리는 평평한 표준 이미지 Ic와 변형 필드 D를 사용하여 비디오를 표현하는 것을 제안합니다. Ic에 이미지 알고리즘 X를 적용함으로써, 우리는 학습된 변형 필드로 전체 비디오에 효과를 효과적으로 전파할 수 있습니다. 이 새로운 비디오 표현은 이미지 알고리즘과 비디오 작업 간의 중요한 교량 역할을 하여, 최첨단 이미지 방법론을 비디오 애플리케이션에 직접 적용할 수 있습니다. 제안된 표현은 다음과 같은 필수 특성을 보여야 합니다. • 충실한 비디오 재구성을 위한 피팅 기능. 표현은 비디오에서 큰 강체 또는 비강체 변형을 정확하게 피팅할 수 있는 기능을 가져야 합니다. • 표준 이미지의 의미적 정확성. 왜곡되거나 의미적으로 잘못된 표준 이미지는 이미지 처리 성능 저하로 이어질 수 있으며, 특히 이러한 프로세스의 대부분이 자연스러운 이미지 데이터에서 학습된다는 점을 고려할 때 더욱 그렇습니다. • 변형 필드의 부드러움. 변형 필드의 부드러움 보장은 시간적 일관성과 올바른 전파를 보장하는 필수 기능입니다. 3.1. 콘텐츠 변형 필드 동적 NeRF[32, 33]에서 영감을 얻어 비디오를 두 가지 뚜렷한 구성 요소인 표준 필드와 변형 필드로 표현하도록 제안합니다. 이 두 구성 요소는 각각 2D 및 3D 해시 테이블을 사용하여 실현됩니다. 이러한 해시 테이블의 용량을 향상시키기 위해 두 개의 미세 MLP가 통합됩니다. 그림 2에서 설명한 대로 비디오를 재구성하고 처리하기 위한 제안된 표현을 제시합니다. 프레임 {I₁, I2, ..., IN}으로 구성된 비디오 V가 주어지면 이러한 프레임에 맞게 조정된 암묵적 변형 가능 모델을 학습합니다. 이 모델은 두 개의 좌표 기반 MLP인 변형 필드 D와 표준 필드 C로 구성됩니다. 표준 필드 C는 비디오 V에 있는 모든 평탄화된 텍스처를 포함하는 연속 표현으로 사용됩니다. 이는 2D 위치 x : (x,y)를 색상 c (r, g, b)로 매핑하는 함수 F : ✗ → C로 정의됩니다. 학습 속도를 높이고 네트워크가 고주파 세부 정보를 캡처할 수 있도록 다중 해상도 해시 인코딩 Y2D: R² → R²+FXL을 채택하여 좌표 x를 피처 벡터로 매핑합니다.여기서 L은 다중 해상도의 레벨 수이고 F는 레이어당 피처 차원 수입니다.함수 : = Y2D(X) (x, F1(x), ..., FL (x))는 모델이 고주파 세부 정보를 캡처하는 기능을 용이하게 합니다.여기서 Fi(x)는 x에 의해 ¿th 해상도에서 선형 보간된 피처입니다.변형 필드 D는 비디오 내의 모든 프레임에 대한 관찰-표준 변형을 캡처합니다.특정 프레임 Ii의 경우 D는 관찰 위치와 표준 위치 간의 대응 관계를 설정합니다.동적 NeRF[32,33]는 푸리에 위치 인코딩과 추가 학습 가능한 시간 코드를 사용하여 3D 공간에서 변형 필드를 구현합니다.이 구현은 변형 필드의 부드러움을 보장합니다. 그럼에도 불구하고, 이 간단한 구현은 두 가지 이유(즉, 낮은 학습 효율성과 부적절한 대표 기능)로 비디오 표현으로 원활하게 전환할 수 없습니다.따라서 우리는 변형 필드를 작은 MLP가 따르는 3D 해시 테이블로 표현하도록 제안합니다.특히, t번째 프레임의 임의의 위치 x는 먼저 3D 해시 인코딩 함수 3D(x, t)에 의해 인코딩되어 고차원 피처를 얻습니다.그런 다음 작은 MLP D : (√3D(x, t)) → x&#39;는 임베디드 피처를 표준 필드의 해당 위치 x&#39;에 매핑합니다.우리는 다음과 같이 3D 해시 인코딩 기반 변형 필드를 자세히 설명합니다.변형 필드를 위한 3D 해시 인코딩.특히, 비디오의 임의의 지점은 직교 3D 공간 내의 위치 X3D(x, y, t)로 개념화될 수 있습니다.그림 2의 왼쪽에 표시된 것처럼 3D 해시 인코딩 기술을 사용하여 비디오 공간을 표현합니다.이 기술은 3D 공간을 다중 해상도 피처 그리드로 캡슐화합니다. 다중 해상도라는 용어는 다양한 수준의 해상도를 가진 그리드의 구성을 나타내며, 피처 그리드는 각 정점에서 학습 가능한 피처로 채워진 그리드를 나타냅니다. 프레임워크에서 다중 해상도 피처 그리드는 L개의 개별 레벨로 구성됩니다. 학습 가능한 피처의 차원은 F로 표현됩니다. 또한 N₁로 표시되는 7번째 레이어의 해상도는 [Nmin, Nmax]로 집합적으로 표시되는 가장 거친 해상도와 가장 정밀한 해상도 사이의 기하적 진행을 나타내며, N₁ = [Nmin b&#39;], b = exp를 사용합니다. Nmax에서 - Nmin L에서 -(1) 7번째 레이어에서 쿼리된 점 X3D를 고려하면 입력 좌표는 해당 레벨의 그리드 해상도로 크기 조정됩니다. 그리고 X3D의 쿼리된 피처는 8개의 이웃 코너 점(그림 2 참조)에서 삼선형 보간됩니다. x3D의 모서리 점을 얻기 위해 내림과 올림은 먼저 [X 3D = [X3D N₁], [x3D] = [×³D · Nɩ], . 로 연산되고, 각 모서리를 최대 T의 고정 크기를 갖는 레벨의 해당 피처 벡터 배열의 항목에 매핑합니다. 거친 레벨의 경우 저해상도 그리드의 매개변수는 T보다 적으며, 여기서 매핑은 1 : 1입니다. 따라서 인덱스로 피처를 직접 찾을 수 있습니다. 반대로. 더 미세한 해상도의 경우, 점은 해시 함수, h(x¹³3D) = (±²²±1xiπ i ) mod T, (3)로 매핑됩니다. 여기서 ☺는 비트별 XOR 연산을 나타내고 {~¿}는 [29]에 따른 고유한 큰 소수입니다. 프레임 t의 좌표 x에서 출력 색상 값은 c = C(D(Y3D(x, t)))로 계산할 수 있습니다. (4) 이 출력은 입력 프레임에 있는 기준 진실 색상을 사용하여 감독할 수 있습니다. 3.2. 모델 설계 제안된 표현은 임의의 비디오에 대한 표준 콘텐츠와 시간 변형을 모두 효과적으로 모델링하고 재구성할 수 있습니다. 그러나 견고한 비디오 처리에 대한 요구 사항을 충족하는 데 어려움이 있습니다. 특히 3D 해시 변형은 강력한 피팅 기능을 가지고 있지만 시간 변형의 부드러움을 손상시킵니다. 이러한 균형으로 인해 표준 이미지의 고유한 의미론을 유지하는 것이 현저히 어려워져 기존 이미지 알고리즘을 비디오 사용에 적용하는 데 상당한 장벽이 생깁니다. 표준 이미지의 고유한 의미를 보존하면서 정확한 비디오 재구성을 달성하기 위해 어닐링된 다중 해상도 해시 인코딩을 사용할 것을 제안합니다. 변형의 부드러움을 더욱 향상시키기 위해 흐름 기반 일관성을 도입합니다. 큰 폐색이나 복잡한 다중 객체 시나리오와 같은 어려운 경우에는 추가 의미 정보를 활용할 것을 제안합니다. 이는 그룹화된 변형 필드와 함께 의미 마스크를 사용하여 달성할 수 있습니다. 변형을 위한 어닐링 3D 해시 인코딩. 더 세밀한 해상도를 위해 해시 인코딩은 복잡한 변형 피팅 성능을 향상시키지만 표준 필드에 불연속성과 왜곡을 도입합니다(그림 9 참조). 동적 NeRF에서 사용된 어닐링 전략[32]에서 영감을 얻어 변형을 위한 점진적 주파수 필터에 어닐링 해시 인코딩 기술을 사용합니다. 더 구체적으로, 우리는 다른 해상도로 보간된 해당 기능에 대해 점진적 제어 가중치를 사용합니다. 학습 단계 k에서 7번째 계층의 가중치는 1 - cos(π clamp(m(j – Neg)/Nstep, 0, 1)) wj (k) . &quot; (5)로 계산됩니다. 여기서 Noeg는 어닐링을 시작하기 위한 사전 정의된 단계이고 m은 어닐링 속도를 제어하기 위한 하이퍼 매개변수를 나타내며 Nstep은 어닐링 단계의 숫자입니다. 흐름 가이드 일관성 손실. 높은 신뢰도로 흐름에 의해 식별된 해당 지점은 표준 필드에서 동일한 지점이어야 합니다. 흐름 가이드 재구성된 비디오 입력 비디오 표준 이미지 계층화된 신경 아틀라스를 계산합니다. 그림 3. 비디오 표현의 용량을 반영하고 충실한 비디오 처리에서 기본적인 역할을 하는 비디오 재구성과 관련하여 계층화된 신경 아틀라스[16]와 CoDeF 간의 정성적 비교. 세부 사항은 확대하면 가장 잘 이해할 수 있습니다. 이 관찰에 따르면 일관성 손실. 두 개의 연속된 프레임 I¿ 및 I₁+1의 경우 RAFT[52]를 사용하여 전방 흐름을 감지합니다. Fi→i+1 및 역방향 흐름 Fi+1. 프레임 I¿의 신뢰 영역은 Mflow = |Warp(Warp(Ii, Fi→i+1), Fi+1→i)—Ii| &lt; e, (6)로 정의할 수 있습니다. 여기서 €는 오류 임계값에 대한 하이퍼파라미터를 나타냅니다. 이 손실은 다음과 같이 공식화할 수 있습니다. = flow Lflow (3D(x, t)) — D(73D(x + F{\t+1; t + 1)) − F¥\+t+1|| * Mow (7) 여기서 Ft+1 및 Mw는 광학 흐름과 x에서의 흐름 신뢰도입니다. 흐름 손실은 특히 매끄러운 영역에 대해 변형 필드의 부드러움을 효율적으로 정규화합니다. 그룹화된 콘텐츠 변형 필드. 표현은 단일 콘텐츠 변형 필드를 사용하여 비디오를 재구성하는 방법을 학습할 수 있지만 겹치는 여러 객체에서 발생하는 복잡한 동작은 하나의 표준 내에서 충돌을 일으킬 수 있습니다. 결과적으로 경계 영역은 부정확한 재구성으로 인해 어려움을 겪을 수 있습니다. 큰 폐색이 특징인 까다로운 인스턴스의 경우 여러 콘텐츠 변형 필드에 해당하는 레이어를 도입하는 옵션을 제안합니다. 이러한 레이어는 의미 분할을 기반으로 정의되므로 이러한 까다로운 시나리오에서 비디오 재구성의 정확도와 견고성이 향상됩니다. SAMtrack(Segment-Anything-track) [5]을 활용하여 각 비디오 프레임의 분할을 달성합니다. 원본 비디오 Ours Text2Live Tune-A-Video Fate Zero urde urder 텍스트 프롬프트: Makoto Shinkai의 Your name, 잘생긴 남자와 아름다운 여자 텍스트 프롬프트: 사이버펑크 로봇 텍스트 프롬프트: 아이언 맨 그림 4. Text2Live [1], Tune-A-Video [64], FateZero [36] 및 CoDeF를 통한 ControlNet [69] 직접 리프팅을 포함한 다양한 방법에 걸친 텍스트 안내 비디오 대 비디오 변환 작업에 대한 정성적 비교. 시간적 일관성과 합성 품질에 대한 자세한 평가를 위해 프로젝트 페이지의 비디오를 보는 것이 좋습니다. I¿를 마스크 Mo, ..., M²-1을 사용하여 K개의 의미적 층으로 나눕니다. 그리고 각 층에서 표준 필드와 변형 필드의 그룹을 사용하여 서로 다른 객체의 개별 동작을 표현합니다. 이러한 모델은 이후 암묵적 필드의 그룹으로 공식화됩니다. D: {D1, ..., DK}, C: {C1, CK}. 이론적으로 프레임 i의 의미적 층 k의 경우 효율적인 재구성을 위해 M½ 영역에서 픽셀을 샘플링하는 것으로 충분합니다. 그러나 해시 인코딩은 비지도 영역에서 무작위적이고 비구조적 패턴을 초래할 수 있으며, 이는 자연스러운 이미지에서 학습된 이미지 기반 모델의 성능을 저하시킵니다. 이 문제를 해결하기 위해 M½ 영역 외부의 여러 지점을 샘플링하고 기준 진실 색상을 사용하여 L2 손실을 사용하여 학습합니다. 이런 식으로 배경 손실 Lbg로 M을 효과적으로 정규화합니다. 결과적으로 표준 이미지는 더 자연스러운 모양을 갖게 되어 향상된 처리 결과가 나타납니다. 학습 목표. 표현은 목적 함수 rec를 최소화하여 학습합니다. 이 함수는 주어진 좌표 x에 대한 기준 색상과 예측 색상 c 사이의 L2 손실에 해당합니다. 학습 프로세스를 정규화하고 안정화하기 위해 이전에 논의한 대로 추가 정규화 항을 도입합니다. 총 손실은 다음 방정식 L == Lrec + 1 * Lflow, (8)을 사용하여 계산합니다. 여기서 ₁은 손실 가중치에 대한 하이퍼 매개변수를 나타냅니다. 그룹화된 변형 필드를 학습할 때 №2 * Lbg로 표시되는 추가 정규화기를 포함한다는 점에 유의하는 것이 중요합니다. 1번째 프레임 10번째 프레임 20번째 프레임 30번째 프레임 표준 이미지 그림 5. CoDeF로 비디오를 재구성한 후 시간 변형 필드에서 직접 추출한 프레임 간 포인트 대응의 시각화. 3.3. 일관된 비디오 처리에 대한 응용 프로그램 콘텐츠 변형 필드를 최적화하면 모든 포인트의 변형을 0으로 설정하여 표준 이미지 Į를 검색합니다. 원본 이미지 크기보다 더 큰 표준 이미지의 크기를 비디오에서 관찰되는 장면 움직임에 따라 유연하게 조정할 수 있으므로 더 많은 콘텐츠를 포함할 수 있다는 점에 유의하는 것이 중요합니다. 그런 다음 표준 이미지 Ic를 사용하여 일관된 비디오 처리를 위한 다양한 다운스트림 알고리즘을 실행합니다. 다음과 같은 최신(SOTA) 알고리즘을 평가했습니다. (1) ControlNet[69]: 프롬프트 가이드 비디오 간 변환에 사용됩니다. (2) Segment-anything(SAM)[18]: 비디오 객체 추적에 적용됩니다. (3) R-ESRGAN[61]: 비디오 초해상도에 사용됩니다. 또한 표준 이미지를 사용하면 이미지를 직접 수정하여 비디오를 편리하게 편집할 수 있습니다. 여러 수동 비디오 편집 예를 통해 이 기능을 추가로 설명합니다. 4. 실험 4.1 실험 설정 제안하는 방법의 견고성과 다양성을 강조하기 위해 실험을 수행합니다. 우리의 표현은 강체와 비강체 물체, 그리고 스모그와 같은 복잡한 시나리오를 포함하는 다양한 변형에 강합니다.우리 실험의 기본 매개변수는 각각 4000과 8000에서 어닐링 시작 및 종료 단계로 설정됩니다.전체 반복 단계는 10000으로 제한됩니다.단일 NVIDIA A6000 GPU에서 비디오 프레임을 활용할 때 평균 학습 기간은 약 5분입니다.학습 시간은 비디오 길이, 동작 유형, 레이어 수와 같은 여러 요인에 따라 다릅니다.학습 매개변수를 적절히 조정함으로써 최적화 기간을 1분에서 10분까지 다양하게 할 수 있습니다.4.2. 평가 우리의 표현에 대한 평가는 두 가지 주요 측면에 집중되어 있습니다.추정된 표준 이미지가 있는 재구성된 비디오의 품질과 다운스트림 비디오 처리의 품질입니다. 정확한 평가 지표가 부족하기 때문에 정확한 정량적 비디오 객체 추적 입력 비디오 분석을 수행하는 것은 여전히 어렵습니다.그럼에도 불구하고 추가 검토를 위해 정량적 결과를 선택했습니다.재구성 품질.그림 3에서 볼 수 있듯이 Neural Image Atlas와의 비교 분석에서 우리 모델은 비강체 동작에 대한 뛰어난 견고성을 보여 미세한 움직임을 더욱 정밀하게 재구성합니다(예: 눈 깜빡임, 얼굴 질감).정량적으로 수집된 비디오 데이터 세트에서 우리 알고리즘의 비디오 재구성 PSNR은 4.4dB 더 높습니다.아틀라스와 표준 이미지를 비교할 때, 우리 결과는 더 자연스러운 표현을 제공하므로 기존 이미지 알고리즘을 더 쉽게 적용할 수 있습니다. 또한, 저희의 방법은 훈련 효율성에서 상당한 진전을 이루었습니다. 즉, 5분(저희 방법) 대 10시간(아틀라스)입니다. 다운스트림 비디오 처리. 제안된 표현과 관련된 잠재적인 응용 프로그램의 범위를 확장하여 비디오-비디오 변환, 비디오 키포인트 추적, 비디오 객체 추적, 비디오 초고해상도 및 사용자 상호 작용 비디오 편집을 포함합니다. (a) 비디오-비디오 변환. 표준 이미지에 이미지 변환을 적용하여 비디오-비디오 변환을 수행할 수 있습니다. 세 가지 뚜렷한 범주에 속하는 여러 기준 방법을 포함하는 정성적 비교가 제시됩니다. (1) ControlNet[69]과 같은 이미지 변환 모델을 사용한 프레임당 추론; (2) Text-to-live[1]로 대표되는 계층화된 비디오 편집; (3) Tune-A-Video[64] 및 FateZero[36]를 포함한 확산 기반 비디오 변환. 그림 4에서 볼 수 있듯이, 프레임당 이미지 변환 모델은 상당한 깜빡임이 수반되는 고화질 콘텐츠를 생성합니다. 대체 기준선은 손상된 생성 품질 또는 비교적 낮은 시간적 일관성을 보입니다. 제안된 파이프라인은 이미지 변환을 비디오로 효과적으로 끌어올려 상당한 시간적 일관성을 보장하면서 이미지 변환 알고리즘과 관련된 높은 품질을 유지합니다. 철저한 비교는 수반되는 비디오를 보면 더 잘 이해할 수 있습니다. (b) 비디오 키포인트 추적. 각 개별 프레임에 대한 변형 필드를 추정함으로써 표준 공간 내의 한 프레임에서 특정 키포인트의 위치를 쿼리하고 이후 그림 5와 같이 모든 프레임에 있는 해당 포인트를 식별하는 것이 가능합니다. 프로젝트 페이지에서 비디오의 유체와 같은 비강체 객체에서 추적 포인트의 데모를 보여줍니다. (c) 비디오 객체 추적. 표준 이미지의 분할 알고리즘을 사용하여 콘텐츠 변형 필드를 활용하여 모든 비디오 시퀀스에서 마스크 전파를 용이하게 할 수 있습니다. 그림 6에서 볼 수 있듯이 파이프라인은 모든 프레임에서 일관성을 유지하는 마스크를 능숙하게 생성합니다. (d) 비디오 초고해상도. 이미지 초고해상도 알고리즘을 표준 이미지에 직접 적용하여 그림 7과 같이 비디오 초고해상도를 실행하여 고품질 비디오를 생성할 수 있습니다. 변형이 연속 필드로 표현되므로 초고해상도를 적용해도 깜빡임이 발생하지 않습니다. (e) 사용자 상호 작용 비디오 편집. 이 표현을 사용하면 이미지의 다른 부분에 영향을 주지 않고 고유한 스타일의 객체를 사용자가 편집할 수 있습니다. 그림 8에서 예시한 것처럼 사용자는 표준 이미지의 콘텐츠를 수동으로 조정하여 자동 편집 알고리즘이 최적의 결과를 얻지 못하는 영역에서 정확한 편집을 수행할 수 있습니다. 4.3. 절제 연구 제안된 모듈의 효과를 검증하기 위해 절제 연구를 수행했습니다. 3D 해시 인코딩을 위치 인코딩으로 대체하면 비디오의 재구성 PSNR이 3dB만큼 현저히 감소합니다. 어닐링된 해시가 없으면 표준 이미지는 자연스러운 모양을 잃습니다.입력 비디오 비디오 편집 그림 8에서 알 수 있듯이요.사용자 상호 작용 비디오 편집은 단 하나의 이미지만 편집하고 CoDeF를 사용하여 시간 축을 따라 결과를 전파함으로써 달성됩니다.독자 여러분께서 시간적 일관성을 이해하시려면 프로젝트 페이지의 비디오를 보시기를 강력히 권장합니다.전송된 표준 이미지 어닐링이 없는 표준 이미지 그림 9. 어닐링된 해시의 효과에 대한 절제 연구.표준 이미지의 부자연스러움은 다운스트림 작업의 성능에 해를 끼칩니다.그림 9에서 여러 손의 존재.또한 흐름 손실을 통합하지 않으면 매끄러운 영역이 뚜렷한 깜빡임의 영향을 크게 받습니다.더 광범위한 비교는 프로젝트 페이지의 비디오를 참조하세요.5.
--- CONCLUSION ---
및 논의 이 논문에서 우리는 비디오를 콘텐츠 변형 필드로 표현하는 것을 조사했으며, 시간적으로 일관된 비디오 처리를 달성하는 데 중점을 두었습니다. 우리의 접근 방식은 충실도와 시간적 일관성 측면에서 유망한 결과를 보여줍니다. 그러나 향후 작업에서는 해결해야 할 과제가 몇 가지 남아 있습니다. 가장 중요한 문제 중 하나는 방법론에 필요한 장면별 최적화와 관련이 있습니다. 우리는 피드포워드 암묵적 필드 기술[57, 67]의 발전이 잠재적으로 이 방향에 적용될 수 있다고 예상합니다. 또 다른 과제는 시점의 극단적인 변화가 포함된 시나리오에서 발생합니다. 이 문제를 해결하기 위해 3D 사전 지식[8]을 통합하면 추가 정보와 제약 조건을 제공할 수 있으므로 유익할 수 있습니다. 마지막으로, 큰 비강체 변형을 처리하는 것은 여전히 문제입니다. 이를 해결하기 위한 한 가지 잠재적 솔루션은 복잡한 변형을 더 잘 캡처하고 표현할 수 있는 여러 개의 표준 이미지[33]를 사용하는 것입니다. 참고문헌 [1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: 텍스트 기반 계층 이미지 및 비디오 편집. Eur. Conf. Comput. Vis., 2022. [2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: 이미지 편집 지침을 따르는 법 배우기. IEEE Conf. Comput. Vis. Pattern Recog., 2023. [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: 일관된 이미지 합성 및 편집을 위한 튜닝 없는 상호 자기 주의 제어. arXiv 사전 인쇄본 arXiv:2304.08465, 2023. [4] Yinbo Chen, Sifei Liu, and Xiaolong Wang. 로컬 암묵적 이미지 함수를 사용하여 연속 이미지 표현 학습. IEEE Conf. Comput. Vis. Pattern Recog., 2021. [5] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, Yi Yang. 모든 것을 분할하고 추적. arXiv 사전 인쇄본 arXiv:2305.06558, 2023. [6] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 gans를 이겼습니다. Adv. Neural Inform. Process. Syst., 2021. [7] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 안내 비디오 합성. arXiv 사전 인쇄본 arXiv:2302.03011, 2023. [8] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, Angjoo Kanazawa. Instruct-nerf2nerf: 지침이 있는 3D 장면 편집. arXiv 사전 인쇄본 arXiv:2303.12789, 2023. [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626, 2022. [10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 확산 확률적 모델의 노이즈 제거. Adv. Neural Inform. Process. Syst., 2020. [12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: 변압기를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄 arXiv:2205.15868, 2022. [13] Allan Jabri, Andrew Owens 및 Alexei Efros. 대조적인 무작위 보행으로서의 시공간 대응. 고급에서. 신경 정보. 프로세스. Syst., 2020. [14] Varun Jampani, Raghudeep Gadde 및 Peter V Gehler. 비디오 전파 네트워크. IEEE 컨퍼런스에서. 계산. 비스. Pattern Recog., 2017. [15] Ondřej Jamriška, Šárka Sochorová, Ondřej Texler, Michal Lukáč, Jakub Fišer, Jingwan Lu, Eli Shechtman 및 Daniel Sýkora. 예시를 통한 비디오 스타일링. ACM 트랜스. Graph., 38(4):1-11, 2019. [16] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. 일관된 비디오 편집을 위한 계층화된 신경 아틀라스. ACM Trans. Graph., 40(6):1–12, 2021. [17] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: 텍스트-이미지 확산 모델은 제로샷 비디오 생성기입니다. arXiv 사전 인쇄본 arXiv:2303.13439, 2023. [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할하세요. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. [19] Chenyang Lei, Yazhou Xing, Hao Ouyang, 및 Qifeng Chen. 비디오 일관성 및 전파를 위한 딥 비디오 사전.IEEE Trans. Pattern Anal. Mach. Intell., 45(1):356–371, 2022. [20] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. 다중 뷰 비디오의 신경망 3D 비디오 합성.IEEE Conf. Comput. Vis. Pattern Recog., 2022. [21] Zhengqi Li, Simon Niklaus, Noah Snavely, 및 Oliver Wang. 동적 장면의 시공간 뷰 합성을 위한 신경망 장면 흐름 필드.IEEE Conf. Comput. Vis. Pattern Recog., 2021. [22] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia. 비디오-p2p: 교차 주의 제어를 통한 비디오 편집. arXiv 사전 인쇄본 arXiv:2303.04761, 2023. [23] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, Michael Rubinstein. 비디오에서 사람의 타이밍을 재조정하기 위한 계층적 신경 렌더링. arXiv 사전 인쇄본 arXiv:2009.07833, 2020. [24] Erika Lu, Forrester Cole, Weidi Xie, Tali Dekel, Bill Freeman, Andrew Zisserman, Michael Rubinstein. 조정 게임을 통해 비디오에서 객체와 그 효과 연관시키기. Adv. Neural Inform. Process. 영어: Syst., 2022. [25] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov. 주의가 있는 캡션에서 이미지 생성. arXiv 사전 인쇄본 arXiv:1511.02793, 2015. [26] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, Anders Eriksson. 신경망의 레이어로서의 암묵적 표면 표현. Int. Conf. Comput. Vis., 2019. [27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. Eur. Conf. Comput. Vis., 2020. [28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie. T2i-adapter: 텍스트-이미지 확산 모델에 대한 보다 제어 가능한 기능을 발굴하기 위한 학습 어댑터. arXiv 사전 인쇄본 arXiv:2302.08453, 2023. [29] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM Trans. Graph., 41(4):102:1– 102:15, 2022.[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. Glide: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해.arXiv 사전 인쇄본 arXiv:2112.10741, 2021. [31] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove. Deepsdf: 모양 표현을 위한 연속 부호 거리 함수 학습.IEEE Conf. Comput. Vis. Pattern Recog., 2019. [32] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 광도장.Int. Conf. Comput. 영어: Vis., 2021. [33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, Steven M. Seitz. Hypernerf: 위상적으로 변하는 신경 광도장에 대한 고차원 표현. ACM Trans. Graph., 40(6), 2021. [34] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger. 합성곱 점유 네트워크. Eur. Conf. Comput. Vis., 2020. [35] Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer. D-nerf: 동적 장면을 위한 신경 광도장. IEEE Conf. Comput. Vis. Pattern Recog., 2021. [36] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: 제로 샷 텍스트 기반 비디오 편집을 위한 어텐션 융합. arXiv 사전 인쇄본 arXiv:2303.09535, 2023. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각 모델 학습. Int. Conf. Mach. Learn., 2021. [38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로 샷 텍스트-이미지 생성. Int. Conf. Mach. Learn., 2021. [40] Alex Rav-Acha, Pushmeet Kohli, Carsten Rother, Andrew Fitzgibbon. 모자이크 풀기: 비디오 편집을 위한 새로운 표현. SIGGRAPH, 1-11페이지, 2008. [41] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, Honglak Lee. 무엇을 어디에 그릴지 배우기. Adv. Neural Inform. Process. Syst., 2016. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE Conf. Comput. Vis. Pattern Recog., 2022. [43] Manuel Ruder, Alexey Dosovitskiy, Thomas Brox. 비디오의 예술적 스타일 전환. Pattern Recognition: 38th German Conference, GCPR 2016, 하노버, 독일, 2016년 9월 12-15일, Proceedings 38, 26-36페이지. Springer, 2016. [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. IEEE Conf. Comput. Vis. Pattern Recog., 2023. [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델.Adv. Neural Inform. Process. Syst., 2022. [46] Sara Fridovich-Keil 및 Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa. 플레녹셀: 신경망이 없는 광채장.IEEE Conf. Comput. Vis. Pattern Recog., 2022. [47] Jonathan Shade, Steven Gortler, Li-wei He, Richard Szeliski. 계층화된 깊이 이미지. 1998년 컴퓨터 그래픽 및 상호작용 기술에 관한 제25회 연례 컨퍼런스의 회의록에서. [48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터가 없는 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022. [49] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein. 주기적 활성화 함수가 있는 암묵적 신경 표현. Adv. Neural Inform. Process. Syst., 2020. [50] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암묵적 모델의 잡음 제거. arXiv 사전 인쇄 arXiv:2010.02502, 2020. [51] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron 및 Ren Ng. 푸리에 기능을 사용하면 네트워크가 저차원 영역에서 고주파수 함수를 학습할 수 있습니다. 고급에서. 신경 정보. 프로세스. Syst., 2020. [52] Zachary Teed 및 Jia Deng. Raft: 광학 흐름을 위한 반복적인 모든 쌍 필드 변환입니다. 유로에서는. 회의 계산. Vis., 2020. [53] Ondřej Texler, David Futschik, Michal Kučera, Ondřej Jamriška, Šárka Sochorová, Menclei Chai, Sergey Tulyakov 및 Daniel Sýkora. fewshot 패치 기반 훈련을 사용한 대화형 비디오 스타일화. ACM Trans. Graph., 39(4):73–1, 2020. [54] Narek Tumanyan, Michal Geyer, Shai Bagon 및 Tali Dekel. 텍스트 기반 이미지 대 이미지 변환을 위한 플러그 앤 플레이 확산 기능. IEEE Conf. Comput. Vis. Pattern Recog., 2023. [55] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze 및 Dumitru Erhan. Phenaki: 오픈 도메인 텍스트 설명에서 가변 길이 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02399, 2022. [56] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암시적 표면 학습. arXiv 사전 인쇄본 arXiv:2106.10689, 2021.[57] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser. Ibrnet: 다중 뷰 이미지 기반 렌더링 학습. IEEE Conf. Comput. Vis. Pattern Recog., 2021. [58] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, Fang Wen. 사전 학습은 이미지 간 변환에 필요한 전부입니다.arXiv 사전 인쇄본 arXiv:2205.12952, 2022. [59] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen. 기성품 이미지 확산 모델을 사용한 제로샷 비디오 편집.arXiv 사전 인쇄본 arXiv:2303.17599, 2023. [60] Xiaolong Wang, Allan Jabri, Alexei A Efros. 시간의 주기 일관성에서 대응 관계 학습.IEEE Conf. Comput. Vis. 영어: Pattern Recog., 2019. [61] Xintao Wang, Liangbin Xie, Chao Dong 및 Ying Shan. Real-esrgan: 순수 합성 데이터로 실제 세계 블라인드 초해상도 학습. Int. Conf. Comput. Vis., 2021. [62] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt 및 Lingjie Liu. Neus2: 다중 뷰 재구성을 위한 신경 암시적 표면의 빠른 학습. arXiv 사전 인쇄본 arXiv:2212.05231, 2022. [63] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang 및 Nan Duan. Nüwa: 신경 시각 세계 생성을 위한 시각 합성 사전 학습. Eur. Conf. Comput. Vis., 2022. [64] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie 및 Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 조정입니다. arXiv 사전 인쇄 arXiv:2212.11565, 2022. [65] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang 및 Xiaodong He. Attngan: 주의 생성 적대 네트워크를 사용하여 텍스트를 이미지로 세밀하게 생성합니다. IEEE 컨퍼런스에서. 계산. 비스. Pattern Recog., 2018. [66] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa 및 Noah Snavely. 비지도 비디오 분해를 위한 변형 가능한 스프라이트.IEEE Conf. Comput. Vis. Pattern Recog., 2022. [67] Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa. pixelnerf: 하나 또는 몇 개의 이미지에서 추출한 신경 광도장.IEEE Conf. Comput. Vis. Pattern Recog., 2021. [68] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas. Stackgan: 스택형 생성적 적대 네트워크를 사용한 텍스트에서 사진과 같은 사실적인 이미지 합성.Int. Conf. Comput. Vis., 2017. [69] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543, 2023.
