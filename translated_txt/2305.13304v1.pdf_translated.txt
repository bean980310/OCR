--- ABSTRACT ---
Transformer의 고정 크기 컨텍스트는 GPT 모델이 임의로 긴 텍스트를 생성할 수 없게 만듭니다. 이 논문에서는 RNN의 반복 메커니즘을 모사한 언어 기반 시뮬레이션인 RECURRENTGPT를 소개합니다. RECURRENTGPT는 ChatGPT와 같은 대규모 언어 모델(LLM)을 기반으로 하며 자연어를 사용하여 LSTM의 장단기 메모리 메커니즘을 시뮬레이션합니다. 각 타임스텝에서 RECURRENTGPT는 텍스트 단락을 생성하고 하드 드라이브와 프롬프트에 각각 저장된 언어 기반 장단기 메모리를 업데이트합니다. 이 반복 메커니즘을 통해 RECURRENTGPT는 잊지 않고 임의의 길이의 텍스트를 생성할 수 있습니다. 인간 사용자는 자연어 메모리를 쉽게 관찰하고 편집할 수 있으므로 RECURRENTGPT는 해석이 가능하고 긴 텍스트의 대화형 생성이 가능합니다. RECURRENTGPT는 로컬 편집 제안을 넘어 차세대 컴퓨터 지원 쓰기 시스템을 향한 첫 걸음입니다. AI 생성 콘텐츠(AIGC)를 제작하는 것 외에도 RECURRENTGPT를 소비자와 직접 상호 작용하는 대화형 소설로 사용할 수 있는 가능성을 보여줍니다. 우리는 이러한 생성 모델 사용을 &quot;AI as Contents&quot;(AIAC)라고 부르며, 이는 기존 AIGC의 다음 형태라고 믿습니다. 또한 RECURRENTGPT를 사용하여 작가와 상호 작용하는 대신 독자와 직접 상호 작용하는 개인화된 대화형 소설을 만들 수 있는 가능성을 보여줍니다. 더 광범위하게 RECURRENTGPT는 인지 과학 및 딥 러닝에서 인기 있는 모델 설계에서 아이디어를 빌려 LLM을 촉진하는 유용성을 보여줍니다. 코드는 https://github.com/aiwaves-cn/Recurrent GPT에서 사용할 수 있으며 온라인 데모는 https://www.aiwaves.org/recurrent gpt에서 사용할 수 있습니다. 1
--- INTRODUCTION ---
ChatGPT와 같은 대규모 언어 모델(LLM) [1–5]은 이메일과 블로그 게시물을 포함한 다양한 일상적인 쓰기 작업을 지원하는 데 매우 효과적인 도구임이 입증되었습니다. 그럼에도 불구하고 Transformer [6] 아키텍처에 내재된 고정 크기 컨텍스트 설계로 인해 LLM만 프롬프트하여 긴 텍스트(예: 소설)를 생성하는 것은 불가능합니다. 반면, 이론상 순환 신경망(RNN) [7, 8]은 재귀 메커니즘 덕분에 임의 길이의 시퀀스를 생성할 수 있는 기능을 가지고 있습니다. RNN은 각 시간 단계마다 업데이트되는 숨겨진 상태를 유지하여 현재 시간 단계의 출력을 후속 시간 단계의 입력으로 사용합니다. 그러나 실제로 RNN은 사라지고 폭발하는 그래디언트 문제가 있으며 확장하기 어렵습니다. *동등 기여 사전 인쇄본. 진행 중인 작업. (h₁-(xt-Ot (0+1) C++1. A Ot ht x+ A ht+1, LLM x+ -(x1-xt Xt+인간/ 인간 시뮬레이터 Ot 출력 내용 Χρ 선택된 계획 x 후보 계획 ht 단기 메모리 Ct 장기 메모리 시간 단계 t -단기 메모리 h₁-1: 유명한 행성 탐험가인 엘리아스는 수많은 행성을 여행했고 상상할 수 없는 경이로움을 발견했습니다... 선택된 계획 x+-1: 유명한 행성 탐험가인 엘리아스는 최근에 발견된 먼 행성을 탐험하는 새로운 임무에 착수합니다. 그 행성은 독특한 생태계를 가지고 있는 것으로 여겨지며 엘리아스는 그 행성의 비밀을 밝히고 싶어합니다. 출력 내용 0,: 엘리아스는 며칠 동안 유물을 연구하고 비밀을 밝히려고 노력했습니다. 그는 모든 지식과 기술을 사용하여 상형 문자를 해독하고 기술을 이해하기 위해 쉴 새 없이 일했습니다. 그가 더 깊이 파고들면서 유물의 신비 속으로 들어가면서, 그는 그것이 이 세상의 것이 아니라는 것을 깨닫기 시작했습니다. 그것은 한때 은하계를 지배했던 오래전에 사라진 문명의 유물이었습니다. 엘리아스는 자신이 정말 특별한 것을 우연히 발견했다는 것을 알았지만, 그것이 위험하다는 것도 알고 있었습니다. 유물을 작업하면서 엘리아스는 생물과 외계인이 이상하게 행동하는 것을 알아차리기 시작했습니다. 그들은 그를 지켜보고, 연구하고, 무언가를 기다리는 것 같았습니다. 엘리아스는 무언가 잘못되었다는 느낌을 떨칠 수 없었습니다. 그는 조심해야 한다는 것을 알았지만, 지금 멈출 수 없다는 것도 알았습니다. 그는 유물의 비밀을 푸는 데 너무 가까이 있었습니다. 타임스텝 t 단기 기억 ht: 엘리아스와 그의 팀은 그들이 찾은 유물이 은하계 전역에 흩어져 있는 더 큰 유물 네트워크의 일부라는 것을 발견했습니다. 엘리아스와 그의 팀은 다른 유물을 찾고 그것을 자신의 이익을 위해 사용하려는 강력한 존재를 막기로 결심했습니다.... 후보 계획 x: 엘리아스와 그의 팀은 라이벌 팀보다 먼저 다음 유물을 찾기 위해 시간과 경쟁해야 합니다. 그들은 유물을 지키는 위험한 생물이 사는 행성으로 여행을 간다. 그들은 그 생물을 따돌리고 라이벌 팀이 도착하기 전에 유물을 회수해야 한다. 후보 계획 x²: 엘리아스와 그의 팀은 다음 유물이 평화로운 외계 종족이 사는 행성에 있다는 것을 발견한다. 그러나 그들은 곧 외계 종족이 우주 해적단에게 위협을 받고 있다는 것을 깨닫는다. 그들은 외계인과 팀을 이루어 우주 해적을 물리치고 유물을 회수해야 한다. 후보 계획 x: 엘리아스와 그의 팀은 강력한 존재에게 공격받고 있는 행성으로부터 구조 신호를 받는다. 그들은 그 존재가 행성에 위치한 유물을 노리고 있다는 것을 알게 된다. 엘리아스와 그의 팀은 그 존재와 싸워서 너무 늦기 전에 유물을 회수해야 한다. 장기 기억 C+ : 1. 엘리아스는 우주선 조종석에 앉아 앞에 펼쳐진 광활한 우주를 응시했다. 그는 몇 달 동안 여행을 했지만, 새로운 행성을 발견한다는 설렘이 그를 계속 움직이게 했다. 그는 행성에 접근하면서 기대감을 느끼지 않을 수 없었습니다. 앞으로 어떤 경이로움이 기다리고 있을까요? 2. 그 행성은 엘리아스가 전에 본 적이 없는 것과 달랐습니다. 하늘은 짙은 보라색이었고, 나무는 선명한 파란색이었습니다. 우주선에서 내리자 이국적인 꽃 향이 나는 따뜻한 바람이 그를 맞이했습니다. 엘리아스는 이 새로운 세계를 탐험하기를 고대했습니다. 3. 엘리아스가 행성을 계속 탐험하면서 경이로움과 흥분을 느끼지 않을 수 없었습니다. 그가 만난 생물들은 그가 전에 본 적이 없는 것과 달랐습니다. 어떤 것은 작고 털이 많았고, 어떤 것은 거대하고 여러 개의 팔다리가 있었습니다. 엘리아스는 그들의 독특한 특징과 행동에 매료되었습니다. 그는 수 시간 동안 그들을 관찰하고, 메모를 하고, 일기에 스케치했습니다. 4. 행성 깊숙이 들어가면서 엘리아스는 행성에 한때 진보된 문명이 있었음을 시사하는 고대 유적을 우연히 발견했습니다. 그 유적은 그가 전에 본 적이 없는 것과 달랐습니다. 아키텍처는 복잡하고 아름다웠으며 기술은 그가 접한 어떤 것보다 훨씬 더 발전했습니다.그림 1: RECURRENTGPT 프레임워크의 그림.RECURRENTGPT는 자연어 구성 요소를 사용하여 RNN을 시뮬레이션하여 LLM으로 반복적 프롬프트를 활성화하고 프롬프트로 반복 계산 그래프를 정의합니다.이를 위해 여러 연구[9–11]에서 RNN과 유사한 반복 메커니즘을 Transformers에 장착하려고 시도했습니다.긴 텍스트 모델링 및 생성에서 유망한 결과를 달성하는 동안 이러한 반복 증강 Transformer는 확장성이 입증되지 않은 상당한 아키텍처 수정이 필요합니다.현재 LLM의 대부분은 최소한의 변경으로 원래 Transformer 아키텍처를 계속 사용합니다.이 논문에서는 RNN의 반복 메커니즘의 언어 기반 시뮬라크럼인 RECURRENTGPT를 소개합니다. 그림 1에서 볼 수 있듯이, RECURRENTGPT는 장단기 메모리 RNN(LSTM) [8]의 벡터화된 요소(즉, 셀 상태, 숨겨진 상태, 입력 및 출력)를 자연어(즉, 텍스트 문단)로 대체하고, 프롬프트 엔지니어링으로 재귀 메커니즘을 시뮬레이션합니다. 각 타임스텝 t에서 RECURRENTGPT는 t-1 단계에서 생성된 텍스트 문단과 다음 문단의 간략한 계획을 수신합니다. 그런 다음 이전에 생성된 모든 문단의 요약을 포함하고 하드 드라이브에 저장할 수 있는 장기 메모리를 처리하고, 의미 검색을 통해 관련 문단을 검색할 수 있습니다. RECURRENTGPT는 또한 최근 타임스텝 내에서 자연어로 주요 정보를 요약하는 단기 메모리를 유지 관리하고 각 타임스텝에서 업데이트합니다. RECURRENTGPT는 앞서 언급한 모든 입력을 프롬프트에 결합하고 백본 LLM에 새 문단, 다음 문단에 대한 간략한 계획을 생성하도록 요청하고 단기 메모리를 다시 쓰고 출력 문단의 요약을 장기 메모리에 추가하여 장기-단기 메모리를 업데이트합니다. 그런 다음 이러한 구성 요소는 nexttime 단계에서 재사용되어 생성 프로세스에 대한 반복 메커니즘이 생성됩니다. 언어 기반 반복 메커니즘을 통해 RECURRENTGPT는 아키텍처 수정의 필요성을 완화하고 강력한 LLM에 통합할 수 있으므로 고정 크기 컨텍스트 창을 넘어 임의로 긴 텍스트를 생성할 수 있습니다. RECURRENTGPT는 고정 크기 컨텍스트 제한을 뛰어넘는 것 외에도 RNN에 사용된 벡터 기반 반복 메커니즘과 비교하여 반복 메커니즘의 해석 가능성을 향상시킵니다. 이러한 개선은 간단한 검사를 통해 주의를 기울이는 장기 메모리의 특정 세그먼트와 단기 메모리가 업데이트되는 방식을 관찰할 수 있는 능력에서 비롯됩니다. 더 중요한 것은 자연어를 빌딩 블록으로 사용하면 인간이 RECURRENTGPT에 참여할 수 있어 인간이 미래 세대를 위해 기억과 계획을 조작할 수 있다는 것입니다. 또한 인간 상호 작용은 RECURRENTGPT가 원하는 동작에서 벗어나는 것을 방지하는데, 이는 AutoGPT²와 같은 최근의 자율적 GPT 기반 에이전트에서 일반적으로 마주치는 문제입니다. 현재 최첨단 컴퓨터 지원 쓰기 시스템[12, 13]이 주로 지역화된 편집 제안에 초점을 맞추고 LLM을 블랙박스로 취급한다는 점을 감안할 때, RECURRENTGPT는 해석 가능성도 제공하는 대화형 장문 텍스트 생성을 위한 차세대 컴퓨터 지원 쓰기 시스템으로 나아가는 한 걸음이라고 생각합니다. 그런 다음 콘텐츠 제작자와만 상호 작용하는 것이 아니라 소비자와 직접 상호 작용할 수 있는 잠재력을 탐구하여 AI 생성 콘텐츠(AIGC)를 제작하는 도구로서의 RECURRENTGPT의 활용을 확장합니다. 구체적으로, RECURRENTGPT를 개인화된 대화형 소설로 변환하여 후속 작업에 대한 여러 가지 예상 계획을 생성하여 플레이어가 관심을 끄는 작업을 선택하고 탐색할 수 있도록 합니다. 게다가, 모델에서 생성된 계획에서 선택하는 것 외에도, 플레이어는 자신의 계획을 고안할 수 있는 능력을 가지고 있습니다. 이러한 능력은 기존의 대화형 허구에서는 달성할 수 없는데, 내러티브와 옵션은 관례적으로 미리 결정되어 있기 때문입니다. 우리는 이 새로운 패러다임을 &quot;AI As Content&quot;로 명명합니다. 이는 생성적 AI를 콘텐츠 제작자를 위한 단순한 도구 역할에 국한되지 않고 소비자와 적극적으로 상호 작용하는 매체로 활용하는 것을 의미합니다. RECURRENTGPT를 통해 우리는 AI 모델이 결국 우리의 창의적 노력에서 협력 파트너가 될 미래를 향한 예비적 진전을 인식합니다. 우리의 실험에서 우리는 ChatGPT에 RECURRENTGPT를 구축하고 수천 개의 토큰에 걸쳐 놀라울 정도로 광범위한 텍스트를 자율적으로 생성하는 기능을 보여주면서도 일관성과 참여를 유지하는 것을 발견했습니다. 극명하게 대조적으로, vanilla ChatGPT는 반복적인 콘텐츠나 일관성 저하와 같은 문제에 직면하기 전에 수백 개의 토큰을 생성하는 것으로 제한됩니다. 게다가 RECURRENTGPT는 인간 작가가 임의로 긴 텍스트를 쉽게 생성할 수 있도록 돕고 소설과 같은 긴 창의적 텍스트를 쓰는 데 필요한 많은 인간의 노력을 줄일 수 있습니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. • 우리는 언어 기반 RECURRENTGPT를 제안합니다. ChatGPT와 같은 LLM의 고정 크기 컨텍스트 제한을 완화하는 RNN의 반복 메커니즘 시뮬레이션. • RECURRENTGPT가 단독으로 매우 긴 텍스트를 생성하거나 대화형 쓰기 도우미 역할을 하여 인간 작가가 임의로 긴 텍스트를 쓸 수 있도록 도울 수 있음을 보여줍니다. • RECURRENTGPT를 콘텐츠 큐레이션을 위한 개인화된 대화형 소설로 사용하여 콘텐츠 생성 도구로 사용하는 기존 관행과 달리 텍스트 소비자와 직접 상호 작용하기 위해 생성 모델을 사용하는 생성 AI의 새로운 사용 사례를 소개합니다. 또한 RECURRENTGPT가 인지 과학 및 딥 러닝 분야에서 잘 확립된 모델 설계에서 영감을 얻어 LLM의 프롬프트를 통해 긴 형식의 텍스트를 생성할 수 있는 가능성을 보여준다는 점을 강조하는 것이 중요합니다. RECURRENTGPT 이 섹션에서는 RECURRENTGPT를 자세히 설명합니다. RECURRENTGPT는 RNNS의 반복 메커니즘에 대한 자연어 기반 대응물입니다. RECURRENTGPT는 (1) 입력 벡터 xt, 출력 벡터 yt, 숨겨진 상태 ht, 셀 상태 ct를 포함한 LSTM의 모든 벡터 기반 구성 요소를 자연어로 모델링하고, (2) 자연어 프롬프트를 사용하여 LSTM에서 재귀 계산 2https://github.com/Significant-Gravitas/Auto-GPTgraph를 모델링하고, (3) RNN의 학습 가능한 매개변수를 동결된 LLM으로 대체하여 LSTM을 시뮬레이션합니다. 이론적으로 RECURRENTGPT의 백본은 모든 LLM 또는 텍스트-텍스트 모델이 될 수 있지만, 우리는 그 기능과 인기 때문에 ChatGPT를 선택합니다. 형식적으로, 우리는 RECURRENTGPT를 매개변수 0과 프롬프트 템플릿 P를 갖는 LLM에 의해 매개변수화된 계산 함수로 정의합니다. LSTM의 순환 계산 그래프는 다음과 같이 요약될 수 있습니다. (1) Ot+1, ht+1, C++= LSTM(x+1, ht, Ct, 0) 여기서 는 모델 매개변수를 나타내고, x++1은 ot와 같고, ht, Ct는 각각 시간 단계 t에서의 장기/단기 메모리입니다. 유추적으로, 우리 모델의 순환 메커니즘은 다음과 같이 표현될 수 있습니다. Ot+1, xt+1, ht+1, C++= RECURRENTGPT (ot, xt, ht, Ct, O,P) (2) 여기서 Ot, xt, ht, ct는 각각 시간 단계 t에서의 내용, 계획, 단기 메모리, 장기 메모리를 포함하는 자연어 기반 구성 요소를 나타냅니다. 여기서 x++1은 ot와 같지 않고 대신 별도로 생성되는데, 이는 기존 RNN과 다릅니다. 먼저 RECURRENTGPT의 각 구성 요소를 설명한 다음 프롬프트 P가 RECURRENTGPT가 임의로 긴 텍스트를 반복적으로 생성하는 방법을 제시합니다. 2.1 언어 기반 구성 요소 입력/출력 각 타임스텝에서 RECURRENTGPT의 입력 및 출력에는 생성된 최종 텍스트에 추가되는 텍스트 문단과 생성될 다음 문단의 개요가 포함됩니다. 이 두 가지를 각각 &quot;내용&quot;과 &quot;계획&quot;이라고 합니다. 그림 1에서 볼 수 있듯이 내용은 일반적으로 200~400단어로 구성되며 대부분 읽을 준비가 되어 있어야 합니다. 반면 계획은 다음 내용의 개요이며 일반적으로 3~5개의 문장으로 구성됩니다. 각 타임스텝에서 이전 타임스텝에서 생성된 내용과 계획은 RECURRENTGPT의 입력으로 사용되어 반복적인 계산이 가능합니다. RECURRENTGPT는 사용자가 계획을 읽고 편집할 수 있도록 하여 해석 가능성을 높이고 인간-컴퓨터 상호 작용을 용이하게 하므로 콘텐츠 외에도 계획을 생성하도록 설계되었습니다.장단기 메모리 LSTM과 유사하게 RECURRENTGPT는 시간 단계에 걸쳐 장단기 메모리를 유지합니다.그림 1에서 볼 수 있듯이 장기 메모리는 긴 텍스트를 생성할 때 정보 손실을 최소화하기 위해 이전에 생성된 모든 콘텐츠를 요약합니다.생성된 콘텐츠는 임의로 길 수 있으며 LLM의 컨텍스트 크기에 맞지 않기 때문에 각 시간 단계에서 생성된 콘텐츠를 문장 변환기에 임베드하여 VectorDB 방식으로 RECURRENTGPT에서 장기 메모리를 구현합니다[14].이 방법을 사용하면 RECURRENTGPT가 GPU 메모리 대신 디스크 공간에 메모리를 저장할 수 있으므로 이전의 메모리 기반 변환기[9, 11]에 비해 더 긴 메모리를 저장할 수 있습니다.이는 사용자의 장치에 하이엔드 GPU가 없는 여러 사용 사례에서 중요할 수 있습니다. 반면 단기 기억은 최근 타임스텝에 걸쳐 핵심 정보를 요약한 짧은 문단의 텍스트입니다. 단기 기억의 길이는 프롬프트에 맞고 LLM 백본에서 업데이트할 수 있도록 10-20문장으로 제어됩니다. 장단기 기억을 결합함으로써 RECURRENTGPT는 최근에 생성된 콘텐츠와 일관성을 유지하고 오래 전에 생성된 핵심 정보를 회상할 수 있습니다. 바닐라 LLM에서는 입력에서 이전에 생성된 몇 개의 텍스트만 사용할 수 있기 때문에 이는 불가능합니다. RECURRENTGPT는 LLM에 소설의 주제와 기타 배경 정보를 지정하는 텍스트로 앞서 언급한 구성 요소를 생성하도록 지시하는 간단한 프롬프트를 사용하여 초기화할 수 있습니다. RECURRENTGPT를 사용하여 소설을 계속 쓸 때 사용자는 단기 기억과 초기 계획을 적어 두거나(또는 ChatGPT를 프롬프트하여 생성)할 수 있습니다. 2.2 언어 기반 순환 계산 RNN이 계산 그래프에 피드백 루프를 구현하여 순환 계산을 달성하는 반면, RECURRENTGPT는 프롬프트 엔지니어링에 의존하여 순환 계산 체계를 시뮬레이션합니다.그림 1에서 볼 수 있듯이 RECURRENTGPT는 부록의 그림 1에 제시된 프롬프트 템플릿과 몇 가지 간단한 Python 코드를 사용하여 RNN에서 계산 그래프를 시뮬레이션합니다³.3 공간 제약으로 인해 부록 A에 프롬프트를 제시합니다.각 타임스텝에서 RECURRENTGPT는 프롬프트 템플릿을 입력 콘텐츠/플랜과 내부 장단기 메모리로 채워 입력 프롬프트를 구성합니다.특히 장기 메모리는 컨텍스트 크기에 맞지 않기 때문에 입력 플랜을 쿼리로 사용하여 VectorDB 기반 장기 메모리에서 의미 검색을 수행하고 가장 관련성 있는 몇 가지 콘텐츠를 프롬프트에 맞춥니다.그런 다음 프롬프트는 LLM 백본에 새 콘텐츠, 플랜 및 업데이트된 단기 메모리를 생성하도록 지시합니다. 부록의 그림 1에 나와 있듯이, 우리의 프롬프트는 LLM이 더 이상 관련성이 없는 정보를 삭제하고 유용한 새 정보를 추가하는 동시에 항상 컨텍스트 크기에 맞을 수 있는 범위 내에서 길이를 유지하여 단기 메모리를 업데이트하도록 장려합니다. LLM이 여러 개(예: 실험에서는 3개)의 계획을 생성하도록 프롬프트하는 것은 주목할 만합니다. 이를 통해 출력의 다양성이 향상되고 인간 사용자가 가장 적합한 계획을 선택할 수 있으므로 인간-컴퓨터 상호 작용이 더 친화적이 됩니다. 또한 생성된 계획 중 원하는 것이 없을 경우 사용자가 직접 계획을 작성할 수 있는 옵션도 제공합니다. RECURRENTGPT가 인간의 개입 없이 자율적으로 긴 텍스트를 생성할 수 있도록 프롬프트 기반 인간 시뮬레이터를 추가하여 좋은 계획을 선택하고 다음 타임스텝을 위해 수정합니다. 2.3 RECURRENTGPT를 사용한 대화형 긴 텍스트 생성 RECURRENTGPT는 재귀 메커니즘으로 자체적으로 긴 텍스트를 생성할 수 있지만, 언어 기반 계산 체계는 고유한 해석 가능성과 대화형성을 제공합니다. 기존의 컴퓨터 지원 쓰기 시스템이 언어 모델을 블랙박스로 사용하고 다음 구문/문장 제안만 제공하는 것과 비교했을 때, RECURRENTGPT는 다음과 같은 장점을 가지고 있습니다.• 로컬 쓰기 제안 대신 문단/장 수준의 진행을 하기 때문에 인간의 노동을 줄이는 데 더 효율적입니다.• 사용자가 언어 기반 내부 상태를 직접 관찰할 수 있기 때문에 해석 가능합니다.• 인간이 자연어로 구성 요소를 편집할 수 있기 때문에 대화형입니다.• 사용자가 자신의 관심사에 따라 모델을 사용자 정의하기 위해 프롬프트를 쉽게 수정할 수 있기 때문에 사용자 정의가 가능합니다(예: 출력 텍스트의 스타일, 각 타임스텝에서 얼마나 진행해야 하는지 등).또한 인간의 상호 작용은 RECURRENTGPT가 긴 텍스트를 자율적으로 생성할 때 저지른 우발적인 실수를 수정하고 긴 텍스트 생성의 주요 병목 현상인 오류 전파를 방지하는 데 도움이 될 수도 있습니다.실험 3.1 실험 설정 작업 이 섹션에서는 RECURRENTGPT의 경험적 효과를 테스트합니다. 특히, 우리는 다음을 포함한 세 가지 다른 설정에서 RECURRENTGPT를 평가합니다. • • 인간 상호 작용 없이 자율적으로 긴 텍스트 생성. • 인간 작가와 협력하여 긴 텍스트 생성. • 대화형 소설로 텍스트 소비자와 직접 상호 작용. 이러한 각 작업에서 우리는 공상과학, 로맨스, 판타지, 공포, 미스터리, 스릴러 소설을 포함한 다양한 소설 장르로 테스트합니다. 서로 다른 길이의 텍스트에 대한 RECURRENTGPT의 효과를 테스트하기 위해 공포, 미스터리, 스릴러의 경우 중간 길이(~3000단어)의 소설을 생성하고 공상과학, 로맨스, 판타지의 경우 더 긴 소설(~6000단어)을 생성합니다. 기준선 RECURRENTGPT는 LLM을 사용하여 임의로 긴 텍스트를 생성하는 최초의 작업이지만 아래에 나열된 것처럼 일부 합리적인 기준선 및 축소된 변형과 비교할 수 있습니다.• Rolling-ChatGPT는 문학 장르와 일부 개요 또는 배경 설정이 주어지면 ChatGPT가 소설을 쓰기 시작하도록 하는 간단한 기준선이며, 그런 다음 컨텍스트 길이 제한에 도달한 후 ChatGPT가 쓰기를 계속하도록 반복적으로 요청합니다.이 기준선은 Transformers로 긴 텍스트를 생성하기 위해 슬라이딩 컨텍스트 창 트릭을 사용하는 것과 거의 같습니다.• RE³ [15]는 먼저 LLM이 스토리의 개요를 생성하도록 한 다음 일부 순위 지정 및 재작성 파이프라인을 사용하여 개요에 따라 스토리를 생성하는 계층적 긴 스토리 생성 기준선입니다.공정한 비교를 보장하기 위해 ChatGPT로 다시 구현합니다.⚫ DOC [16]는 개요 제어로 RE³를 개선하는 최첨단 긴 스토리 생성 기준선입니다. 우리는 OPT-175B [17]를 ChatGPT로 대체하고 ChatGPT 가중치에 접근할 수 없기 때문에 사용할 수 없는 자세한 컨트롤러를 제거하여 DOC를 다시 구현합니다. 일반적으로 우리는 백본 LLM의 개선으로 인해 다시 구현한 결과가 약간 더 나은 품질을 얻는다는 것을 발견했습니다. 원칙적으로 두 기준선 모두 일관성을 유지하면서 임의로 긴 텍스트를 생성할 수 없다는 점은 주목할 만합니다. 이는 Rolling-ChatGPT 기준선이 이전에 생성된 콘텐츠를 매우 빠르게 잊기 때문입니다. 반면 RE³와 DOC는 첫 번째 단계에서 개요를 수정하여 생성할 스토리의 전체 길이를 제한합니다. 표 1: 다양한 장르의 소설 20권에 대한 기준선과 RECURRENTGPT의 쌍별 비교. 서로 다른 비교의 결과는 서로 비교할 수 없습니다. 굵은 글씨는 p &lt; 0.05인 유의성을 나타냅니다. 소설 장르 ~ 6000단어 SF 로맨스 판타지 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다↑ 흥미롭다 ↑ 일관성 있다 ↑ RECURRENTGPT 94.86.91.84.95.85.Rolling-ChatGPT 7.14.9.18.6.13.RECURRENTGPT 68.65.71.69.63.62.RE31.28.28.25.35.33.RECURRENTGPT 66.59.77.63.61.56.DOC 30.38.25.29.31.40.소설 장르 공포 미스터리 스릴러 ~ 3000단어 흥미롭다 ↑ 일관성 있다↑ 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다↑ RECURRENTGPT 88.84.87.82.91.82.Rolling-ChatGPT 13.17.14.20.11.17.RECURRENTGPT 64.64.66.63.61.61.RE³ 34.30.27.28.38.37.RECURRENTGPT 65.60.72.66.60.58.DOC 29.39.27.25.33.37.평가 지표 평가를 위해 Yang et al. [15]을 따르고 두 가지 차원에 따라 RECURRENTGPT를 기준선과 비교하여 인간 평가를 수행합니다.· 흥미롭다: 생성된 소설이 일반 독자에게 얼마나 흥미로운가?· 일관성: 문단이 얼마나 잘 구성되고 서로 연결되어 있는가?Yang et al.에 따른 &quot;품질&quot; 또는 &quot;인간과 유사한&quot; 지표는 생략합니다. [16] 모든 기준선이 대부분 시간 동안 고품질 텍스트를 생성할 수 있는 ChatGPT에 기반하기 때문입니다.우리는 쌍별 비교를 통해 비교 모델을 평가합니다.특히, 우리는 다른 비교 방법으로 생성된 두 소설(무작위 순서로 A와 B)을 영어 능력이 좋은 인간 주석자에게 제공하고, 그들에게 소설 A와 소설 B 중 어느 것이 더 나은지, 아니면 흥미롭고 일관성 있는 면에서 구별할 수 없는지 레이블을 지정하도록 지시합니다.Yang et al. [16]의 인간 평가 설정에 따라 각 장르에 대해 생성된 소설 20개를 샘플링하고 각 소설에 대해 3명의 주석자를 할당합니다.3.2 결과 표 1에서 볼 수 있듯이, 우리는 RECURRENTGPT가 롤링 윈도우 기준선과 RE³ 및 DOC와 같은 이전 최신 기술에 비해 흥미롭고 일관성 있는 면에서 인간 독자에게 비교적 큰 마진으로 선호된다는 것을 발견했습니다.이는 긴 텍스트 생성에 순환 계산이 중요하다는 우리의 직감을 확인해줍니다.이 격차는 긴 소설에서 더 크며, 이는I | II | | | I 1차 반복 생성된 계획: 1. Aria는 자신이 작업하고 있는 프로젝트가 정부 계획이라는 것을 알게 된 과학자입니다... 2. 뛰어난 유전학자인 Aria는 민간 우주 탐사 회사에 채용되어... 3. 젊고 재능 있는 과학자인 Aria는 명문 연구... 생성된 스토리: Aria는 비커와 시험관이 줄지어 있는 어두운 연구실에 앉아 있었습니다. 그녀는 피곤한 눈을 비비며 앞에 있는 화면에 표시된 데이터에 집중하려고 노력했습니다. 벽에 있는 시계는 오전 2시를 표시했지만 그녀는 몇 시간 전에 시간의 감각을 잃었습니다. 그녀는 몇 달 동안 프로젝트를 진행했지만 무언가 맞지 않았습니다. 계산이 말이 되지 않았습니다. 영어: 그녀는 의자에 기대앉아 깊은 한숨을 쉬었다.그녀는 창밖을 바라보며 어쩔 수 없이 어떤 감정을 느꼈다... [남은 단어 357개] ....기억: 먼 행성으로 우주선을 보내는 프로젝트를 진행하는 과학자 아리아는 그 프로젝트가 초인간적 존재 집단을 만드는 비밀 정부 프로그램을 은폐하기 위한 것이라는 사실을 알게 된다.그녀는 밤늦게까지 일하면서 연구실 밖의 암울한 세상을 점점 더 자각하게 된다.수트를 입은 남자가 그녀에게 말을 걸자 그녀는 무언가 잘못되었다는 것을 깨닫는다.1차 반복 이제 선택을 해야 할 때다.1. 당신은 마지못해 우주비행사로, ...에 의해 강제로 투입되었다.2. 당신은 노련한 천체 고고학자로, 엘리트 ...의 일원이다.3. 당신은 독립적인 연구원이다. 이동 로봇이... RECURRENTGPT가 대화형 쓰기 보조원 2차 반복 생성된 계획: 1. 슈트를 입은 남자가 Aria가 최초의 테스트 대상 중 하나로 선택되었다고 밝힙니다... 2. Aria는 정부가 그녀를 잡기 위해 아무것도 서슴지 않을 것이라는 것을 알고 도망칩니다... 3. Aria는 슈트를 입은 남자에게 답변을 요구하고 정부가... 생성된 스토리: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하기 위해 쉼 없이 노력했습니다. 그들은 큰 위험을 감수하고 있다는 것을 알았지만, 그것이 진실을 밝히는 유일한 방법이라는 것도 알고 있었습니다. Aria는 프로그램에 대한 접근 권한을 사용하여 그들의 시스템에 바이러스를 심어 진행 속도를 늦추고자 했습니다. 또한 그녀는 프로젝트에 참여한 개인에 대한 가능한 한 많은 정보를 수집하여 그들이 악용할 수 있는 약점을 찾고자 했습니다. 한편, 나머지 그룹은 [281단어 남음] ... ... 기억: 과학자 아리아는 트랜스휴먼 그룹을 만들기 위한 정부의 은폐로 밝혀진 프로젝트를 진행 중입니다. 아리아는 프로젝트에 대한 정보를 수집하고 신뢰할 수 있는 친구들로 구성된 소규모 그룹을 구성하여 다음 움직임을 계획합니다. 그들은 트랜스휴먼 프로그램을 내부에서 침투하고 시스템에 바이러스를 심고 적발되지 않고 진실을 밝히기로 계획합니다. RECURRENTGPT as Interactive Fiction 2-nd Iteration 이제 선택을 할 시간입니다.1. 무언가를 감지한 화성 학자를 만납니다... 2. 외계 대도시를 항해하면서 이해할 수 있다는 것을 깨닫습니다... 3. 통치에 대한 봉기의 한가운데에 있습니다... II 3-rd Iteration 생성된 계획: 1. 아리아의 트랜스휴먼 프로그램에 침투하려는 계획이 발각되고 그들은 강제로... 2. 아리아와 그녀의 그룹은 트랜스휴먼 프로그램에 성공적으로 침투하고 충격적인 것을 발견합니다... 3. 아리아의 트랜스휴먼 프로그램을 폭로하려는 계획은 성공하지만 그 결과는... 생성된 스토리: 시위 날이 왔고 아리아와 그녀의 그룹은 긴장감에 휩싸였습니다.그들은 이 순간을 준비하기 위해 수많은 시간을 보냈지만 여전히 많은 것이 걸려 있었습니다.그들이 시위 장소로 향하면서 그들은 어깨에 세상의 무게를 느낄 수 있었습니다. 영어: 참여율은 그들이 기대했던 것보다 좋았고, 모든 계층의 사람들이 그들의 싸움에 합류했습니다.분위기는 전기적이었고, Aria는 ...을 바라보면서 희망의 감각이 그녀를 휩쓸고 지나가는 것을 느꼈습니다.[426단어 남음] ...기억: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하여 시스템에 바이러스를 심고 정보를 수집하기 시작했습니다.그들은 우려를 공유하고 시위를 계획하는 다른 그룹과 동맹을 맺었습니다.그들은 프로그램과 정부의 계획에 대한 정보를 수집했고 그 어느 때보다 목소리를 내기로 결심했습니다.3차 반복 이제 선택을 할 시간입니다.1. Elysia가 Temporal Vault라고 알려진 비밀 장소로 안내합니다... 2. Elysia는 그녀의 가르침에서 대담한 아이디어를 제안합니다.아티팩트는 ... 3. 화성 사회를 더 깊이 파고들면서 그것을 발견합니다.나 | | 나 | 나 | II 생성된 스토리: 당신은 세계 최고의 천체 고고학자 알렉세이 니콜라이 박사입니다. 그는 스핑크스의 실제 연대에 대한 수수께끼를 풀고 신비한 보이니치 사본을 해독한 사람입니다. 당신은 이상하고 고대의 것을 다루는 데 익숙하지만 앞으로 무슨 일이 일어날지 전혀 예상할 수 없습니다. 처음 화성에 발을 디딘 순간 가슴이 쿵쾅거리며 기대와 불안이 뒤섞입니다. 당신은 화성 토양 아래에서 발견된 고대 유적을 조사하기 위해 NASA에서 엄선한 팀에 속해 있습니다. 레이더 이미지에 따르면 유적은 오래전에 사라진 문명의 건축물과 비슷합니다... 생성된 스토리: 외계 도시를 탐색하면서 첨단 기술과 건축물에 감탄합니다. 외계 구조물 사이에는 자석처럼 당신을 끌어들이는 지식의 중심지인 학술 기관이 있습니다. 그리고 그곳에서 시간 연구 학자 엘리시아를 만나 자신을 소개합니다. 그녀는 당신에 대해 다른 무언가를 감지하는 듯하며, 그녀의 호기심 어린 눈은 당신의 변장을 꿰뚫어봅니다. &quot;당신 주변에 파장이 일고 있어요, 낯선 사람아.&quot; 그녀가 말하는데, 그녀의 목소리는 음악적입니다. &quot;당신은 타임 테더에 닿았죠?&quot; 그녀가 유물에 대해 알고 있는 지식은 당신을 놀라게 합니다. 여기에 당신의... 생성된 스토리: 당신과 엘리시아가 화성 기술의 복잡한 부분을 더 깊이 파고들면서, 당신은 시간 조작 장치인 타임 테더의 사용이 금지된 고대 법령을 발견합니다. 화성 문명을 거의 멸망시켰던 과거의 재앙적인 사건을 상기시키는 것입니다. 그 깨달음은 당신을 공포로 채웁니다. 당신이 집으로 가는 티켓이라고 생각했던 바로 그 도구가 사람들에게는 파멸의 상징이 되었습니다. 당신은 중대한 결정을 내려야 합니다. 화성의 법을 무시하고 문명 전체의 분노를 감수할 것인가, 아니면 그들의 법에 따라 일할 것인가? 후자를 선택하여 화성 의회에 자신의 입장을 변론하기로 결정했습니다... 그림 2: RECURRENTGPT를 대화형 쓰기 도우미와 대화형 소설로 사용한 정성적 분석. 강조된 계획 또는 선택 사항은 인간 사용자가 선택한 것입니다. 매우 긴 텍스트를 생성하는 RECURRENTGPT. 마지막으로 인간 주석자는 모든 소설 장르에서 RECURRENTGPT를 선호합니다. 이는 다양한 유형의 긴 텍스트에서의 견고성을 확인합니다. RECURRENTGPT의 효과를 더 잘 이해하기 위해 단기 또는 장기 기억이 없는 절제된 변형과 GPT-4를 백본 모델로 사용하는 변형과 RECURRENTGPT를 비교하여 절제 연구를 수행합니다. 결과는 표 2에 나와 있습니다. 장기/단기 기억이 주로 생성된 텍스트의 일관성에 기여하는 것을 볼 수 있으며, 이는 직관과 잘 일치합니다. GPT-4를 백본 LLM으로 사용하는 RECURRENTGPT는 ChatGPT/GPT-3.5-turbo를 사용하는 대응 모델에 비해 엄청나게 선호됩니다. 이는 더 강력한 LLM을 장착했을 때 RECURRENTGPT의 잠재력을 확인합니다. 정성적 평가를 위해 부록에 RECURRENTGPT에서 생성한 몇 가지 샘플 소설을 제시합니다. 3.3 대화형 쓰기 도우미로서의 RECURRENTGPT 그런 다음 인간-AI 상호 작용 관점에서 대화형 쓰기 도우미로서 RECURRENTGPT의 유용성을 테스트합니다. 그림 2에서 볼 수 있듯이 인간 작가는 먼저 자신이/그녀가 원하는 주제를 선택합니다.표 2: 절제된 변형이 있는 RECURRENTGPT와 GPT-4를 백본 모델로 사용하는 변형의 쌍별 비교. 비교를 위해 서로 다른 장르의 소설 20개를 샘플링했습니다. 서로 다른 비교의 결과는 서로 비교할 수 없습니다. 굵은 글씨는 p &lt; 0.05인 유의성을 나타냅니다. 소설 장르 SF 판타지 ~ 6000단어 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다 ↑ RECURRENTGPT 58.65.55.64.w/o 단기 기억 44.31.47.33.RECURRENTGPT 51.71.57.68.w/o 장기 기억 40.27.46.38.RECURRENTGPT 21.28.27.24.w/ GPT-73.64.71.70.w/를 쓰고 싶어하며 책의 배경과 개요를 설명하는 짧은 문단을 씁니다. 그런 다음 RECURRENTGPT가 자동으로 첫 번째 문단을 생성하고 작가가 스토리를 계속할 수 있는 몇 가지 가능한 옵션을 제공합니다. 작가는 그중 하나를 선택하여 필요한 경우 편집할 수 있습니다. 또한 생성된 계획이 모두 부적절한 경우 다음 몇 단락에 대한 짧은 계획을 직접 작성할 수 있으므로 인간-AI 공동 집필 프로세스가 더 유연해집니다. 부록 B에서 인간 작가가 RECURRENTGPT와 상호 작용하여 다양한 장르의 소설을 쓸 수 있는 Gradio 기반 인터페이스를 보여줍니다. 소규모 인간 사용자 연구에 따르면 RECURRENTGPT는 인간 작가의 생산성을 크게 향상시키며 이러한 향상은 주로 다음에서 비롯됩니다. (1) 짧은 계획을 작성하거나 선택하고 RECURRENTGPT가 실제 텍스트를 생성하도록 하여 긴 텍스트를 입력하는 시간을 줄임; (2) 사용자 피드백에 따라 RECURRENTGPT에서 생성된 계획에서 계획을 선택하여 덜 중요한 플롯을 디자인하는 시간을 줄임. 더욱이 사용자는 RECURRENTGPT가 블랙박스 역할을 하는 기존 AI 쓰기 도우미에 비해 더 해석 가능하고 제어 가능하다고 느낍니다. RECURRENTGPT의 언어 기반 구성 요소는 사용자에게 투명하고 편집 가능하기 때문입니다. 마지막으로, DOC 및 RE³와 같이 긴 텍스트를 계층적으로 생성하는 이전 방법과 비교했을 때, 인간 사용자는 반복적이고 대화형으로 긴 텍스트를 작성하는 것이 더 유연하고 제어 가능하기 때문에 우리 시스템을 선호합니다.마지막으로, 우리 시스템은 구문이나 몇 문장 내에서 로컬 쓰기 제안을 제공하는 데 중점을 두는 반면 RECURRENTGPT는 한 번에 몇 개의 단락을 생성할 수 있기 때문에 대부분의 기존 AI 쓰기 도우미와 매우 다릅니다.3.4 대화형 소설로서의 RECURRENTGPT 또한 RECURRENTGPT를 개인화된 대화형 소설로 사용할 수 있는지 테스트합니다.이 사용 사례는 AI 쓰기 도우미로서의 RECURRENTGPT와 매우 유사합니다.주요 차이점은 그림 2에서 볼 수 있듯이 두 가지입니다.(1) 인간 플레이어에게 몰입감을 조성하기 위한 3인칭 관점에서 1인칭 관점으로의 전환, (2) RECURRENTGPT가 다음 단락에 대한 일반적인 계획이 아닌 주인공에게 중요한 선택이 포함된 계획을 생성하도록 하는 것입니다.이러한 적응은 프롬프트를 약간 수정하여 쉽게 구현할 수 있습니다. 사용자 연구에 따르면 RECURRENTGPT는 인간 플레이어와 상호 작용하고 인간 소비자에게 양질의 콘텐츠를 직접 제공할 수 있습니다. 인간 플레이어는 또한 대화형 소설에서의 행동이 흥미를 크게 향상시키므로 자유형 텍스트를 쓸 수 있는 가능성을 발견합니다. 이는 생성 AI를 콘텐츠 제작 도구로 사용하는 대신 콘텐츠로 직접 사용할 수 있는 잠재력을 확인합니다. 그러나 RECURRENTGPT가 때때로 일관성이 떨어지는 콘텐츠와 관련성이 낮거나 합리적이지 않은 저품질 옵션을 생성한다는 사실도 발견했습니다. 더 강력한 LLM 백본을 사용하거나, 감독된 미세 조정 또는 인간 피드백을 통한 강화 학습으로 LLM 백본을 미세 조정하거나, 더 나은 프롬프트를 설계하면 이를 개선할 수 있다고 생각합니다. 이는 향후 작업으로 남겨둡니다. 4https://gradio.app/ 5더 큰 규모의 사용자 연구를 수행하고 수정된 버전에서 세부 정보와 결과를 제시할 것입니다.4
--- RELATED WORK ---
s 4.1 고정 크기 컨텍스트를 넘어서는 Transformers Transformers의 주요 한계 중 하나는 컨텍스트 크기가 고정되어 긴 텍스트를 처리하고 생성하는 능력을 방해한다는 것입니다. 이전 연구에서는 두 가지 다른 방법으로 이 문제를 해결하려고 시도했습니다. 더 큰 컨텍스트 창을 사용하여 Transformers를 훈련하고 사용하기 위한 효율적인 주의 메커니즘을 설계하는 것[18–21]과 Transformer의 계산 그래프에 메모리 메커니즘을 추가하여 여러 컨텍스트 창에서 정보를 처리할 수 있도록 하는 것입니다[9, 22, 23, 11]. 이러한
--- METHOD ---
영어: 영어 능력이 뛰어난 인간 주석자에게 s를 전달하고, 흥미롭고 일관성 있는 측면에서 소설 A와 소설 B 중 어느 것이 더 나은지, 아니면 구별할 수 없는지 레이블을 지정하도록 지시합니다.Yang et al. [16]의 인간 평가 설정에 따라 각 장르에 대해 생성된 소설 20개를 샘플링하고 각 소설에 대해 주석자 3명을 할당합니다.3.2 결과 표 1에서 볼 수 있듯이 RECURRENTGPT는 흥미와 일관성 면에서 인간 독자에게 선호도가 높았으며 롤링 윈도우 기준선과 RE³ 및 DOC와 같은 기존 최신 기술에 비해 상대적으로 큰 차이가 있었습니다.이를 통해 순환 계산이 긴 텍스트 생성에 중요하다는 직감이 확인되었습니다.긴 소설의 경우 격차가 더 커서I | II | | | I 1차 반복 생성된 계획: 1. Aria는 자신이 작업하고 있는 프로젝트가 정부 계획이라는 것을 알게 된 과학자입니다... 2. 뛰어난 유전학자인 Aria는 민간 우주 탐사 회사에 채용되어... 3. 젊고 재능 있는 과학자인 Aria는 명문 연구... 생성된 스토리: Aria는 비커와 시험관이 줄지어 선 그녀의 어두운 연구실에 앉아 있었습니다. 그녀는 피곤한 눈을 비비며 그녀 앞의 화면에 표시된 데이터에 집중하려고 노력했습니다. 벽에 걸린 시계는 오전 2시를 표시했지만 그녀는 몇 시간 전에 시간의 감각을 잃었습니다. 그녀는 몇 달 동안 프로젝트를 진행했지만 무언가 맞지 않았습니다. 계산이 말이 되지 않았습니다. 영어: 그녀는 의자에 기대앉아 깊은 한숨을 쉬었다.그녀는 창밖을 바라보며 어쩔 수 없이 어떤 감정을 느꼈다... [남은 단어 357개] ....기억: 먼 행성으로 우주선을 보내는 프로젝트를 진행하는 과학자 아리아는 그 프로젝트가 초인간적 존재 집단을 만드는 비밀 정부 프로그램을 은폐하기 위한 것이라는 사실을 알게 된다.그녀는 밤늦게까지 일하면서 연구실 밖의 암울한 세상을 점점 더 자각하게 된다.수트를 입은 남자가 그녀에게 말을 걸자 그녀는 무언가 잘못되었다는 것을 깨닫는다.1차 반복 이제 선택을 해야 할 때다.1. 당신은 마지못해 우주비행사로, ...에 의해 강제로 투입되었다.2. 당신은 노련한 천체 고고학자로, 엘리트 ...의 일원이다.3. 당신은 독립적인 연구원이다. 이동 로봇이... RECURRENTGPT가 대화형 쓰기 보조원 2차 반복 생성된 계획: 1. 슈트를 입은 남자가 Aria가 최초의 테스트 대상 중 하나로 선택되었다고 밝힙니다... 2. Aria는 정부가 그녀를 잡기 위해 아무것도 서슴지 않을 것이라는 것을 알고 도망칩니다... 3. Aria는 슈트를 입은 남자에게 답변을 요구하고 정부가... 생성된 스토리: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하기 위해 쉼 없이 노력했습니다. 그들은 큰 위험을 감수하고 있다는 것을 알았지만, 그것이 진실을 밝히는 유일한 방법이라는 것도 알고 있었습니다. Aria는 프로그램에 대한 접근 권한을 사용하여 그들의 시스템에 바이러스를 심어 진행 속도를 늦추고자 했습니다. 또한 그녀는 프로젝트에 참여한 개인에 대한 가능한 한 많은 정보를 수집하여 그들이 악용할 수 있는 약점을 찾고자 했습니다. 한편, 나머지 그룹은 [281단어 남음] ... ... 기억: 과학자 아리아는 트랜스휴먼 그룹을 만들기 위한 정부의 은폐로 밝혀진 프로젝트를 진행 중입니다. 아리아는 프로젝트에 대한 정보를 수집하고 신뢰할 수 있는 친구들로 구성된 소규모 그룹을 구성하여 다음 움직임을 계획합니다. 그들은 트랜스휴먼 프로그램을 내부에서 침투하고 시스템에 바이러스를 심고 적발되지 않고 진실을 밝히기로 계획합니다. RECURRENTGPT as Interactive Fiction 2-nd Iteration 이제 선택을 할 시간입니다.1. 무언가를 감지한 화성 학자를 만납니다... 2. 외계 대도시를 항해하면서 이해할 수 있다는 것을 깨닫습니다... 3. 통치에 대한 봉기의 한가운데에 있습니다... II 3-rd Iteration 생성된 계획: 1. 아리아의 트랜스휴먼 프로그램에 침투하려는 계획이 발각되고 그들은 강제로... 2. 아리아와 그녀의 그룹은 트랜스휴먼 프로그램에 성공적으로 침투하고 충격적인 것을 발견합니다... 3. 아리아의 트랜스휴먼 프로그램을 폭로하려는 계획은 성공하지만 그 결과는... 생성된 스토리: 시위 날이 왔고 아리아와 그녀의 그룹은 긴장감에 휩싸였습니다.그들은 이 순간을 준비하기 위해 수많은 시간을 보냈지만 여전히 많은 것이 걸려 있었습니다.그들이 시위 장소로 향하면서 그들은 어깨에 세상의 무게를 느낄 수 있었습니다. 영어: 참여율은 그들이 기대했던 것보다 좋았고, 모든 계층의 사람들이 그들의 싸움에 합류했습니다.분위기는 전기적이었고, Aria는 ...을 바라보면서 희망의 감각이 그녀를 휩쓸고 지나가는 것을 느꼈습니다.[426단어 남음] ...기억: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하여 시스템에 바이러스를 심고 정보를 수집하기 시작했습니다.그들은 우려를 공유하고 시위를 계획하는 다른 그룹과 동맹을 맺었습니다.그들은 프로그램과 정부의 계획에 대한 정보를 수집했고 그 어느 때보다 목소리를 내기로 결심했습니다.3차 반복 이제 선택을 할 시간입니다.1. Elysia가 Temporal Vault라고 알려진 비밀 장소로 안내합니다... 2. Elysia는 그녀의 가르침에서 대담한 아이디어를 제안합니다.아티팩트는 ... 3. 화성 사회를 더 깊이 파고들면서 그것을 발견합니다.나 | | 나 | 나 | II 생성된 스토리: 당신은 세계 최고의 천체 고고학자 알렉세이 니콜라이 박사입니다. 그는 스핑크스의 실제 연대에 대한 수수께끼를 풀고 신비한 보이니치 사본을 해독한 사람입니다. 당신은 이상하고 고대의 것을 다루는 데 익숙하지만 앞으로 무슨 일이 일어날지 전혀 예상할 수 없습니다. 처음 화성에 발을 디딘 순간 가슴이 쿵쾅거리며 기대와 불안이 뒤섞입니다. 당신은 화성 토양 아래에서 발견된 고대 유적을 조사하기 위해 NASA에서 엄선한 팀에 속해 있습니다. 레이더 이미지에 따르면 유적은 오래전에 사라진 문명의 건축물과 비슷합니다... 생성된 스토리: 외계 도시를 탐색하면서 첨단 기술과 건축물에 감탄합니다. 외계 구조물 사이에는 자석처럼 당신을 끌어들이는 지식의 중심지인 학술 기관이 있습니다. 그리고 그곳에서 시간 연구 학자 엘리시아를 만나 자신을 소개합니다. 그녀는 당신에 대해 다른 무언가를 감지하는 듯하며, 그녀의 호기심 어린 눈은 당신의 변장을 꿰뚫어봅니다. &quot;당신 주변에 파장이 일고 있어요, 낯선 사람아.&quot; 그녀가 말하는데, 그녀의 목소리는 음악적입니다. &quot;당신은 타임 테더에 닿았죠?&quot; 그녀가 유물에 대해 알고 있는 지식은 당신을 놀라게 합니다. 여기에 당신의... 생성된 스토리: 당신과 엘리시아가 화성 기술의 복잡한 부분을 더 깊이 파고들면서, 당신은 시간 조작 장치인 타임 테더의 사용이 금지된 고대 법령을 발견합니다. 화성 문명을 거의 멸망시켰던 과거의 재앙적인 사건을 상기시키는 것입니다. 그 깨달음은 당신을 공포로 채웁니다. 당신이 집으로 가는 티켓이라고 생각했던 바로 그 도구가 사람들에게는 파멸의 상징이 되었습니다. 당신은 중대한 결정을 내려야 합니다. 화성의 법을 무시하고 문명 전체의 분노를 감수할 것인가, 아니면 그들의 법에 따라 일할 것인가? 후자를 선택하여 화성 의회에 자신의 입장을 변론하기로 결정했습니다... 그림 2: RECURRENTGPT를 대화형 쓰기 도우미와 대화형 소설로 사용한 정성적 분석. 강조된 계획 또는 선택 사항은 인간 사용자가 선택한 것입니다. 매우 긴 텍스트를 생성하는 RECURRENTGPT. 마지막으로 인간 주석자는 모든 소설 장르에서 RECURRENTGPT를 선호합니다. 이는 다양한 유형의 긴 텍스트에서의 견고성을 확인합니다. RECURRENTGPT의 효과를 더 잘 이해하기 위해 단기 또는 장기 기억이 없는 절제된 변형과 GPT-4를 백본 모델로 사용하는 변형과 RECURRENTGPT를 비교하여 절제 연구를 수행합니다. 결과는 표 2에 나와 있습니다. 장기/단기 기억이 주로 생성된 텍스트의 일관성에 기여하는 것을 볼 수 있으며, 이는 직관과 잘 일치합니다. GPT-4를 백본 LLM으로 사용하는 RECURRENTGPT는 ChatGPT/GPT-3.5-turbo를 사용하는 대응 모델에 비해 엄청나게 선호됩니다. 이는 더 강력한 LLM을 장착했을 때 RECURRENTGPT의 잠재력을 확인합니다. 정성적 평가를 위해 부록에 RECURRENTGPT에서 생성한 몇 가지 샘플 소설을 제시합니다. 3.3 대화형 쓰기 도우미로서의 RECURRENTGPT 그런 다음 인간-AI 상호 작용 관점에서 대화형 쓰기 도우미로서 RECURRENTGPT의 유용성을 테스트합니다. 그림 2에서 볼 수 있듯이 인간 작가는 먼저 자신이/그녀가 원하는 주제를 선택합니다.표 2: 절제된 변형이 있는 RECURRENTGPT와 GPT-4를 백본 모델로 사용하는 변형의 쌍별 비교. 비교를 위해 서로 다른 장르의 소설 20개를 샘플링했습니다. 서로 다른 비교의 결과는 서로 비교할 수 없습니다. 굵은 글씨는 p &lt; 0.05인 유의성을 나타냅니다. 소설 장르 SF 판타지 ~ 6000단어 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다 ↑ RECURRENTGPT 58.65.55.64.w/o 단기 기억 44.31.47.33.RECURRENTGPT 51.71.57.68.w/o 장기 기억 40.27.46.38.RECURRENTGPT 21.28.27.24.w/ GPT-73.64.71.70.w/를 쓰고 싶어하며 책의 배경과 개요를 설명하는 짧은 문단을 씁니다. 그런 다음 RECURRENTGPT가 자동으로 첫 번째 문단을 생성하고 작가가 스토리를 계속할 수 있는 몇 가지 가능한 옵션을 제공합니다. 작가는 그중 하나를 선택하여 필요한 경우 편집할 수 있습니다. 또한 생성된 계획이 모두 부적절한 경우 다음 몇 단락에 대한 짧은 계획을 직접 작성할 수 있으므로 인간-AI 공동 집필 프로세스가 더 유연해집니다. 부록 B에서 인간 작가가 RECURRENTGPT와 상호 작용하여 다양한 장르의 소설을 쓸 수 있는 Gradio 기반 인터페이스를 보여줍니다. 소규모 인간 사용자 연구에 따르면 RECURRENTGPT는 인간 작가의 생산성을 크게 향상시키며 이러한 향상은 주로 다음에서 비롯됩니다. (1) 짧은 계획을 작성하거나 선택하고 RECURRENTGPT가 실제 텍스트를 생성하도록 하여 긴 텍스트를 입력하는 시간을 줄임; (2) 사용자 피드백에 따라 RECURRENTGPT에서 생성된 계획에서 계획을 선택하여 덜 중요한 플롯을 디자인하는 시간을 줄임. 더욱이 사용자는 RECURRENTGPT가 블랙박스 역할을 하는 기존 AI 쓰기 도우미에 비해 더 해석 가능하고 제어 가능하다고 느낍니다. RECURRENTGPT의 언어 기반 구성 요소는 사용자에게 투명하고 편집 가능하기 때문입니다. 마지막으로, DOC 및 RE³와 같이 긴 텍스트를 계층적으로 생성하는 이전 방법과 비교했을 때, 인간 사용자는 반복적이고 대화형으로 긴 텍스트를 작성하는 것이 더 유연하고 제어 가능하기 때문에 우리 시스템을 선호합니다.마지막으로, 우리 시스템은 대부분의 기존 AI 쓰기 도우미와 매우 다릅니다.그들은 구문이나 몇 문장 내에서 로컬 쓰기 제안을 제공하는 데 중점을 두는 반면, RECURRENTGPT는 한 번에 몇 개의 단락을 생성할 수 있습니다.3.4 대화형 소설로서의 RECURRENTGPT 또한 RECURRENTGPT를 개인화된 대화형 소설로 사용할 수 있는지 테스트합니다.이 사용 사례는 AI 쓰기 도우미로서의 RECURRENTGPT와 매우 유사합니다.주요 차이점은 그림 2에서 볼 수 있듯이 두 가지입니다.(1) 인간 플레이어에게 몰입감을 조성하기 위한 3인칭 관점에서 1인칭 관점으로의 전환, (2) RECURRENTGPT가 다음 단락에 대한 일반적인 계획이 아닌 주인공에게 중요한 선택이 포함된 계획을 생성하도록 하는 것입니다.이러한 적응은 프롬프트를 약간 수정하여 쉽게 구현할 수 있습니다. 사용자 연구에 따르면 RECURRENTGPT는 인간 플레이어와 상호 작용하고 인간 소비자에게 양질의 콘텐츠를 직접 제공할 수 있습니다. 인간 플레이어는 대화형 소설에서의 행동이 흥미를 크게 향상시키므로 자유형 텍스트를 쓸 수 있는 가능성을 찾습니다. 이는 생성 AI를 콘텐츠 제작 도구로 사용하는 대신 콘텐츠로 직접 사용할 수 있는 잠재력을 확인합니다. 그러나 RECURRENTGPT가 때때로 일관성이 떨어지는 콘텐츠와 관련성이 낮거나 합리적이지 않은 저품질 옵션을 생성한다는 사실도 발견했습니다. 이는 더 강력한 LLM 백본을 사용하거나, 인간 피드백을 통한 감독 미세 조정 또는 강화 학습으로 LLM 백본을 미세 조정하거나, 더 나은 프롬프트를 설계함으로써 개선될 수 있다고 생각합니다. 이는 향후 작업으로 남겨둡니다. 4https://gradio.app/ 5더 큰 규모의 사용자 연구를 수행하고 수정된 버전에서 세부 정보와 결과를 제시할 것입니다.4 관련 연구 4.1 고정 크기 컨텍스트를 넘어서는 트랜스포머 트랜스포머의 주요 한계 중 하나는 컨텍스트 크기가 고정되어 긴 텍스트를 처리하고 생성하는 능력을 방해한다는 것입니다. 이전 연구에서는 두 가지 다른 방법으로 이 문제를 해결하려고 시도했습니다. 더 큰 컨텍스트 창을 사용하여 Transformer를 훈련하고 사용하기 위한 효율적인 어텐션 메커니즘을 설계[18–21]하고 Transformer의 계산 그래프에 메모리 메커니즘을 추가하여 여러 컨텍스트 창에서 정보를 처리할 수 있도록 하는 것입니다[9, 22, 23, 11]. 이러한 방법을 사용하면 Transformer가 매우 긴 텍스트를 처리할 수 있지만 모두 원래 Transformer 아키텍처에 상당한 아키텍처 변경이 필요합니다. 따라서 이러한 접근 방식은 ChatGPT 및 LLAMA와 같은 강력한 사전 훈련된 LLM에 통합할 수 없으므로 유용성이 크게 제한됩니다. 최근 Press et al.[24]은 어텐션에 선형 편향을 추가하여 입력 길이 외삽을 허용하는 ALiBi를 도입했습니다. 그러나 이 방법은 주로 긴 출력 대신 긴 입력을 지원합니다. 또한 모델 매개변수와 추론 코드에 액세스해야 하는데, ChatGPT, GPT-4, PaLM과 같은 최신 LLM이 대부분 폐쇄 소스이기 때문에 종종 불가능합니다. 4.2 긴 텍스트 생성 구조적 수정 외에도 많은 연구에서 계층적 방식으로 긴 텍스트 생성을 조사합니다.Fan et al. [25]은 먼저 짧은 요약을 생성하여 스토리를 생성한 다음 스토리의 술어-인수 구조인 개요를 생성하는 중간 단계를 추가하여 이 방법을 개선하는 것을 제안했습니다.[26] Tan et al. [27]과 Sun et al. [28]은 이러한 종류의 계층적 긴 텍스트 생성 방법을 더욱 개선했습니다.Yao et al. [29]도 먼저 스토리라인을 생성한 다음 스토리를 완성하는 것을 제안했습니다.이 연구 분야는 RE³ [15]와 그 변형인 DOC[16]에 의해 더욱 개선되었으며, 계획 및 작성 방식으로 긴 스토리 생성을 위한 LLM을 재귀적으로 촉구했습니다.그러나 최종 스토리의 플롯과 길이는 여전히 미리 결정된 계획에 의해 제한됩니다. 대조적으로 RECURRENTGPT는 반복적 생성을 통해 위의 한계를 극복하여 효과적인 인간-LM 협업을 가능하게 하고 긴 텍스트 생성의 유연성과 제어성을 개선합니다.4.3 AI 지원 쓰기 시스템 AI 쓰기 도우미는 스토리 완성[12], 에세이 쓰기[30], 시 생성[31]을 포함한 다양한 응용 프로그램에 채택되었습니다.기존 시스템은 대체로 대화형 생성과 자동 생성으로 분류할 수 있습니다.대화형 시스템[32-34]은 주로 구문이나 문장 수준에서 로컬 제안이나 수정을 제공하도록 설계되었습니다.결과적으로 인간 작가의 창의적 부담을 덜어주는 능력이 떨어집니다.반면 자동 생성[26, 35, 36]은 시퀀스-투-시퀀스 프레임워크를 통해 주어진 프롬프트나 주제에 따라 전체 텍스트를 작성하는 것을 목표로 합니다.LLM의 발전은 이러한 시스템에 대한 인상적인 잠재력을 보여주었지만 투명성, 제어 가능성 및 협업 감각이 부족하면 작가의 인식된 소유권과 관련된 사용자 경험에 해를 끼칠 수 있습니다[12, 37]. 게다가 대부분은 여러 구문에서 몇 문장에 이르는 로컬 편집 제안을 제공함으로써 제한을 받는데[38, 39], 이는 부분적으로 NLG 모델의 길이 제한 때문이고 부분적으로는 장거리 일관성을 유지하는 과제 때문입니다.5 제한 사항 이 작업의 한 가지 제한 사항은 RECURRENTGPT가 임의로 긴 텍스트를 생성할 수 있는 반면, 생성된 텍스트가 최대 5000단어 정도인 설정에서만 평가한다는 것입니다.매우 긴 텍스트에 대한 정성적, 정량적 평가가 모두 엄청나게 어렵기 때문입니다.또 다른 제한 사항은 RECURRENTGPT가 ChatGPT 및 GPT-4와 같이 충분히 강력한 백본 LLM에서만 작동한다는 것입니다.더 강력한 소규모 LLM이 개발되면 이 문제가 완화될 수 있다고 생각합니다.마지막으로, AI 쓰기 도우미 및 대화형 소설로서 RECURRENTGPT를 평가하기 위한 사용자 연구는 소규모 연구에 의해 제한됩니다.개정된 버전에서 사용자 연구 전반에 걸쳐 더 크고 더 많은 연구를 추가할 것입니다. 사회적 영향에 있어서 RECURRENTGPT는 AI가 생성한 긴 텍스트의 품질을 개선하고 인간 작가의 생산성을 높일 수 있습니다. 그러나 부정적인 사회적 영향을 초래하는 쓰레기나 유해한 콘텐츠를 생성하는 데 오용될 수도 있습니다. 그러나 이는 생성적 AI의 알려진 한계이며, 우리는 생성적 AI의 책임 있는 사용을 촉진하기 위해 최선을 다할 것입니다.6 결론 우리는 언어 기반 구성 요소를 사용하고 프롬프트 엔지니어링을 통해 반복 계산 그래프를 정의하는 RNN의 반복 메커니즘에 대한 언어 기반 시뮬라크라인 RECURRENTGPT를 제시합니다. RECURRENTGPT는 LLM이 자율적으로 또는 인간 작가와 상호 작용하여 임의로 긴 텍스트를 생성할 수 있도록 합니다. 언어 기반 구성 요소는 해석 가능성과 제어 가능성을 개선하고 프롬프트 기반 계산 그래프는 쉽게 사용자 정의할 수 있게 합니다. RECURRENTGPT를 AI 쓰기 도우미 및 텍스트 기반 게임으로 사용하는 것에 대한 사용자 연구는 지역적 쓰기 제안을 넘어 차세대 AI 쓰기 도우미로의 초기 단계로서의 잠재력을 보여주고 상호 작용을 통해 소비 가능한 콘텐츠로 생성 AI를 직접 사용합니다. 마지막으로, 우리의 연구는 LLM을 사용하여 장문 텍스트를 생성하기 위해 인지 과학 및 딥 러닝 문헌에서 인기 있는 모델 설계에서 아이디어를 빌릴 수 있는 가능성을 보여줍니다. 참고문헌 [1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 생성적 사전 학습을 통한 언어 이해 향상. 2018. [2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 1(8):9, 2019. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 및 Dario Amodei. 언어 모델은 few-shot 학습자입니다. H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집, 신경 정보 처리 시스템의 발전, 33권, 1877~1901페이지. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [4] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id=TG8KACXEON. [5] OpenAI. Gpt-4 기술 보고서, 2023. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett 편집자, 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [7] Jeffrey L. Elman. 시간 속 구조 찾기. 인지 과학, 14(2):179–211, 1990. ISSN 0364-0213. doi: https://doi.org/10.1016/0364-0213(90)90002-E. URL https://www. sciencedirect.com/science/article/pii/036402139090002E. [8] Sepp Hochreiter 및 Jürgen Schmidhuber. 장기 단기 기억. 신경 계산, 9(8): 1735-1780, 1997. [9] Zihang Dai*, Zhilin Yang*, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le 및 Ruslan Salakhutdinov. Transformer-XL: 장기 종속성이 있는 언어 모델링, 2019. URL https://openreview.net/forum?id=HJePno0cYm.[10] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier 및 Timothy P. Lillicrap. 장거리 시퀀스 모델링을 위한 압축 변압기. 국제 학습 표현 컨퍼런스, 2020. URL https://openreview.net/forum?id=SylKikSYDH. [11] Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev. 순환 메모리 변압기. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id= Uynr3iPhksa. [12] Mina Lee, Percy Liang, Qian Yang. 공동 저자: 언어 모델 기능 탐색을 위한 인간-AI 협력 쓰기 데이터 세트 설계. 2022년 CHI 컴퓨팅 시스템의 인간 요인 컨퍼런스 회의록, CHI &#39;22, 뉴욕, 뉴욕, 미국, 2022. 컴퓨팅 기계 협회. ISBN 9781450391573. doi: 10.1145/3491102.3502030. URL https://doi.org/10.1145/3491102.3502030. [13] Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek. 제어보다 선택: 사용자가 다이어제틱 및 비다이어제틱 프롬핑을 사용하여 대규모 언어 모델로 작성하는 방법. 2023년 CHI 컴퓨팅 시스템의 인간 요인 컨퍼런스 회의록, CHI &#39;23, 뉴욕, 뉴욕, 미국, 2023. 컴퓨팅 기계 협회. ISBN 9781450394215. doi: 10.1145/ 3544548.3580969. URL https://doi.org/10.1145/3544548.3580969. [14] Nils Reimers 및 Iryna Gurevych. Sentence-bert: siamese bertnetworks를 사용한 문장 임베딩. 2019년 자연어 처리 경험적 방법 컨퍼런스의 회의록. 계산 언어학 협회, 2019년 11월. URL https://arxiv.org/abs/1908.10084. [15] Kevin Yang, Yuandong Tian, Nanyun Peng 및 Dan Klein. Re3: 재귀적 재프롬프팅 및 수정을 사용하여 더 긴 스토리 생성. 2022년 자연어 처리 경험적 방법에 관한 컨퍼런스의 회의록, 4393-4479쪽, 아랍에미리트 아부다비, 2022년 12월. 계산언어학 협회. URL https://aclanthology. org/2022. emnlp-main. 296. [16] Kevin Yang, Dan Klein, Nanyun Peng, Yuandong Tian. Doc: 세부적인 개요 제어를 통한 긴 스토리 일관성 개선, 2022. [17] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: 사전 학습된 변환기 언어 모델 개방. arXiv 사전 인쇄본 arXiv:2205.01068, 2022. [18] Iz Beltagy, Matthew E. Peters, Arman Cohan. Longformer: 긴 문서 변환기. arXiv:2004.05150, 2020. [19] Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya. Reformer: 효율적 변환기. ICLR에서. OpenReview.net, 2020. [20] Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. 희소 변환기를 사용하여 긴 시퀀스 생성, 2019. [21] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. Big bird: Transformers for longer sequences. NeurIPS에서, 2020. [22] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. R-transformer: Recurrent Neural Network Enhanced Transformer, 2019. [23] Peng Cui and Le Hu. 긴 문서의 추출 요약을 위한 동적 메모리가 있는 슬라이딩 선택기 네트워크. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 5881-5891쪽, 온라인, 2021년 6월. 컴퓨터 언어학 협회. doi: 10.18653/v1/2021. naacl-main.470. URL https://aclanthology.org/2021. naacl-main.470. [24] Ofir Press, Noah A. Smith, and Mike Lewis. 짧게 훈련하고 길게 테스트: 선형 편향이 있는 주의는 입력 길이 외삽을 가능하게 합니다. ICLR에서. OpenReview.net, 2022.[25] Angela Fan, Mike Lewis, Yann Dauphin. 계층적 신경 스토리 생성. Association for Computational Linguistics의 제56회 연례 회의록(제1권: 긴 논문), 889-898페이지, 2018. [26] Angela Fan, Mike Lewis, Yann Dauphin. 스토리 생성 구조화 전략. arXiv 사전 인쇄본 arXiv:1902.01109, 2019. [27] Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, Zhiting Hu. 사전 훈련된 언어 모델을 사용한 긴 텍스트의 점진적 생성. 2021년 북미 계산언어학회 학술대회 논문집: 인간언어기술, 4313-4324쪽, 온라인, 2021년 6월. 계산언어학회. doi: 10.18653/v1/2021.naacl-main.341. URL https://aclanthology.org/ 2021.naacl-main. 341. [28] Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, Chun Fan. 요약, 개요, 세부화: 추출 요약에서 계층적 감독을 통한 장문 텍스트 생성. 2022년 10월 대한민국 경주에서 열린 제29회 국제 계산언어학회 학술대회 논문집, 6392-6402쪽. 국제 계산언어학 위원회. URL https://aclanthology.org/2022. coling-1.556. [29] Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, Rui Yan. Plan-and-write: Towards better automatic storytelling. AAAI, 7378-7385쪽. AAAI Press, 2019. [30] Yuanchao Liu, Bo Pang, Bingquan Liu. 에세이 쓰기에서 우아함을 강화하기 위한 신경 기반 중국어 관용어 추천. 57회 연례 총회록, 5522-5526쪽, 이탈리아 피렌체, 2019년 7월. Association for Computational Linguistics. doi: 10.18653/v1/P19-1552. URL https://aclanthology.org/P19-1552. [31] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight. Hafez: 대화형 시 생성 시스템. ACL 2017 회의록, System Demonstrations, 4348쪽, 캐나다 밴쿠버, 2017년 7월. Association for Computational Linguistics. URL https://aclanthology.org/P17-4008. [32] Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan. Wordcraft: 스토리 쓰기를 위한 인간-AI 협업 편집기. arXiv 사전 인쇄본 arXiv:2107.07430, 2021. [33] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, Minsuk Chang. Talebrush: 생성적 사전 학습 언어 모델을 사용하여 스토리 스케치. 2022년 CHI 컴퓨팅 시스템의 인간적 요인 컨퍼런스 회의록, 1-19페이지, 2022년. [34] Seraphina Goldfarb-Tarrant, Haining Feng, Nanyun Peng. 계획, 작성 및 수정: 오픈 도메인 스토리 생성을 위한 대화형 시스템. arXiv 사전 인쇄본 arXiv:1904.02357, 2019년. [35] Yufei Tian과 Nanyun Peng. 담화 수준 계획 및 미적 특징을 갖춘 제로샷 소네트 생성. 2022년 북미 컴퓨터 언어학 협회 컨퍼런스 회의록: 인간 언어 기술, 3587-3597페이지, 미국 시애틀, 2022년 7월. 컴퓨터 언어학 협회. doi: 10.18653/v1/2022.naacl-main.262. URL https://aclanthology.org/2022. naacl-main.262. [36] Boyang Li, Stephen Lee-Urban, George Johnston, Mark Riedl. 크라우드소싱 플롯 그래프를 사용한 스토리 생성. AAAI 인공지능 컨퍼런스 회의록, 27권, 598-604페이지, 2013. [37] Jeremy Birnholtz, Stephanie Steinhardt, Antonella Pavese. 여기에 쓰세요, 지금 쓰세요!
--- EXPERIMENT ---
s, 우리는 ChatGPT에 RECURRENTGPT를 빌드하고, 일관성과 참여를 유지하면서 수천 개의 토큰에 걸쳐 놀라울 정도로 광범위한 텍스트를 자율적으로 생성하는 기능을 보여줍니다. 극명한 대조적으로, vanilla ChatGPT는 반복적인 콘텐츠나 일관성 저하와 같은 문제에 직면하기 전에 수백 개의 토큰을 생성하는 데 제한됩니다. 게다가 RECURRENTGPT는 인간 작가가 임의로 긴 텍스트를 쉽게 생성할 수 있도록 돕고, 소설과 같은 긴 창작 텍스트를 쓰는 데 필요한 인간의 노력을 크게 줄일 수 있습니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. • 우리는 ChatGPT와 같은 LLM의 고정 크기 컨텍스트 제한을 완화하는 RNN의 반복 메커니즘의 언어 기반 시뮬라크럼인 RECURRENTGPT를 제안합니다. • 우리는 RECURRENTGPT가 단독으로 매우 긴 텍스트를 생성하거나 대화형 쓰기 도우미 역할을 하여 인간 작가가 임의로 긴 텍스트를 쓸 수 있도록 도울 수 있음을 보여줍니다. • 우리는 RECURRENTGPT를 콘텐츠 큐레이션을 위한 개인화된 대화형 소설로 사용하여 콘텐츠 생성을 위한 도구로 사용하는 기존 관행과 달리 텍스트 소비자와 직접 상호 작용하기 위해 생성 모델을 사용하는 생성 AI의 새로운 사용 사례를 소개합니다.또한 RECURRENTGPT는 LLM의 프롬프트를 통해 장문 텍스트를 생성하는 것을 목표로 인지 과학 및 딥 러닝 분야의 잘 확립된 모델 설계에서 영감을 얻을 수 있는 가능성을 보여준다는 점을 강조하는 것이 중요합니다.RECURRENTGPT 이 섹션에서는 RECURRENTGPT에 대해 자세히 설명합니다.RECURRENTGPT는 RNNS의 재귀 메커니즘에 대한 자연어 기반 대응물입니다.RECURRENTGPT는 (1) 입력 벡터 xt, 출력 벡터 yt, 숨겨진 상태 ht 및 셀 상태 ct를 포함한 LSTM의 모든 벡터 기반 구성 요소를 자연어로 모델링하여 LSTM을 시뮬레이션합니다. (2) 자연어 프롬프트가 있는 LSTM에서 순환 계산 2https://github.com/Significant-Gravitas/Auto-GPTgraph를 모델링하고, (3) RNN에서 학습 가능한 매개변수를 동결된 LLM으로 대체합니다. 이론적으로 RECURRENTGPT의 백본은 모든 LLM 또는 텍스트-텍스트 모델이 될 수 있지만, 그 기능과 인기 때문에 ChatGPT를 선택합니다. 형식적으로 RECURRENTGPT를 매개변수 0과 프롬프트 템플릿 P가 있는 LLM으로 매개변수화된 계산 함수로 정의합니다. LSTM의 순환 계산 그래프는 다음과 같이 요약할 수 있습니다. (1) Ot+1, ht+1, C++= LSTM(x+1, ht, Ct, 0) 여기서 는 모델 매개변수를 나타내고, x++1은 ot와 같고, ht, Ct는 각각 타임스텝 t에서 장기/단기 메모리입니다. 유추적으로, 우리 모델의 반복 메커니즘은 다음과 같이 표현할 수 있습니다. Ot+1, xt+1, ht+1, C++= RECURRENTGPT (ot, xt, ht, Ct, O,P) (2) 여기서 Ot, xt, ht, ct는 각각 시간 단계 t에서 내용, 계획, 단기 기억, 장기 기억을 포함하는 자연어 기반 구성 요소를 나타냅니다. 여기서 x++1은 ot와 같지 않고 대신 별도로 생성되며 이는 기존 RNN과 다릅니다. 먼저 RECURRENTGPT의 각 구성 요소를 설명한 다음 프롬프트 P가 RECURRENTGPT가 임의로 긴 텍스트를 반복적으로 생성할 수 있도록 하는 방법을 제시합니다. 2.1 언어 기반 구성 요소 입력/출력 각 시간 단계에서 RECURRENTGPT의 입력 및 출력에는 생성된 최종 텍스트에 추가되는 텍스트 문단과 생성될 다음 문단의 개요가 포함됩니다. 이 둘을 각각 &quot;내용&quot;과 &quot;계획&quot;이라고 합니다. 그림 1에서 볼 수 있듯이, 컨텐츠는 일반적으로 200-400단어로 구성되며 대부분 읽을 준비가 되어 있어야 합니다. 반면 계획은 다음 컨텐츠에 대한 개요이며 일반적으로 3-5개의 문장으로 구성됩니다. 각 타임스텝에서 이전 타임스텝에서 생성된 컨텐츠와 계획은 RECURRENTGPT의 입력으로 사용되어 반복적인 계산이 가능합니다. RECURRENTGPT는 사용자가 계획을 읽고 편집할 수 있도록 하여 해석 가능성을 높이고 인간-컴퓨터 상호 작용을 용이하게 하기 때문에 컨텐츠 외에도 계획을 생성하도록 설계되었습니다. 장단기 메모리 LSTM과 유사하게 RECURRENTGPT는 타임스텝 전반에 걸쳐 장단기 메모리를 유지합니다. 그림 1에서 볼 수 있듯이, 장기 메모리는 긴 텍스트를 생성할 때 정보 손실을 최소화하기 위해 이전에 생성된 모든 컨텐츠를 요약합니다. 생성된 컨텐츠는 임의로 길 수 있고 LLM의 컨텍스트 크기에 맞지 않기 때문에 각 타임스텝에서 생성된 컨텐츠에 문장 변환기를 임베드하여 VectorDB 방식으로 RECURRENTGPT에서 장기 메모리를 구현합니다[14]. 이 접근 방식을 사용하면 RECURRENTGPT가 GPU 메모리 대신 디스크 공간에 메모리를 저장할 수 있으므로 이전의 메모리 기반 Transformers[9, 11]보다 더 긴 메모리를 저장할 수 있습니다. 이는 사용자의 기기에 고성능 GPU가 없는 여러 사용 사례에서 중요할 수 있습니다. 반면 단기 메모리는 최근 타임스텝에서 주요 정보를 요약한 짧은 텍스트 단락입니다. 단기 메모리의 길이는 프롬프트에 맞고 LLM 백본에서 업데이트할 수 있도록 10~20문장으로 제어됩니다. 장단기 메모리를 결합함으로써 RECURRENTGPT는 최근 생성된 콘텐츠와의 일관성을 유지하고 오래 전에 생성된 주요 정보를 회상할 수도 있습니다. 바닐라 LLM에서는 입력에서 이전에 생성된 몇 개의 텍스트만 사용할 수 있기 때문에 불가능합니다. RECURRENTGPT는 LLM에 소설의 주제와 기타 배경 정보를 지정하는 텍스트로 앞서 언급한 구성 요소를 생성하도록 지시하는 간단한 프롬프트를 사용하여 초기화할 수 있습니다. RECURRENTGPT를 사용하여 소설 쓰기를 계속할 때 사용자는 단기 메모리와 초기 계획을 적어 두거나(또는 ChatGPT를 프롬프트하여 생성)할 수 있습니다.2.2 언어 기반 순환 계산 RNN이 계산 그래프에서 피드백 루프를 구현하여 순환 계산을 달성하는 반면, RECURRENTGPT는 프롬프트 엔지니어링에 의존하여 순환 계산 체계를 시뮬레이션합니다.그림 1에서 볼 수 있듯이 RECURRENTGPT는 부록의 그림 1에 제시된 프롬프트 템플릿과 몇 가지 간단한 Python 코드를 사용하여 RNN에서 계산 그래프를 시뮬레이션합니다³.3 공간 제약으로 인해 부록 A에 프롬프트를 제시합니다.각 타임스텝에서 RECURRENTGPT는 프롬프트 템플릿을 입력 콘텐츠/계획과 내부 장단기 메모리로 채워 입력 프롬프트를 구성합니다.특히 장기 메모리는 컨텍스트 크기에 맞지 않기 때문에 입력 계획을 쿼리로 사용하여 VectorDB 기반 장기 메모리에서 의미 검색을 수행하고 가장 관련성 있는 몇 가지 콘텐츠를 프롬프트에 맞춥니다. 그런 다음 프롬프트는 LLM 백본에 새로운 내용, 계획 및 업데이트된 단기 메모리를 생성하도록 지시합니다. 부록의 그림 1에서 설명한 것처럼, 프롬프트는 LLM이 더 이상 관련성이 없는 정보를 삭제하고 유용한 새 정보를 추가하여 단기 메모리를 업데이트하는 동시에 항상 컨텍스트 크기에 맞을 수 있는 범위 내에서 길이를 유지하도록 합니다. LLM에 여러 개(예: 실험에서는 3개)의 계획을 생성하도록 프롬프트하는 것은 주목할 만합니다. 이를 통해 출력의 다양성이 향상되고 인간 사용자가 가장 적합한 계획을 선택할 수 있으므로 인간-컴퓨터 상호 작용이 더욱 친화적이 됩니다. 또한 생성된 계획 중 원하는 것이 없는 경우 사용자에게 직접 계획을 작성할 수 있는 옵션도 제공합니다. RECURRENTGPT가 인간의 개입 없이 자율적으로 긴 텍스트를 생성할 수 있도록 프롬프트 기반 인간 시뮬레이터를 추가하여 좋은 계획을 선택하고 다음 타임스텝을 위해 수정합니다. 2.3 RECURRENTGPT를 사용한 대화형 긴 텍스트 생성 RECURRENTGPT는 반복 메커니즘을 사용하여 스스로 긴 텍스트를 생성할 수 있지만, 언어 기반 계산 체계는 고유한 해석 가능성과 상호 작용을 제공합니다. 언어 모델을 블랙 박스로 사용하고 다음 구문/문장 제안만 제공하는 기존의 컴퓨터 지원 쓰기 시스템과 비교할 때, RECURRENTGPT는 다음과 같은 이점을 제공합니다. • 로컬 쓰기 제안 대신 단락/장 수준의 진행을 하기 때문에 인간의 노동을 줄이는 데 더 효율적입니다. • 사용자가 언어 기반 내부 상태를 직접 관찰할 수 있기 때문에 해석 가능합니다. • 인간이 자연어로 구성 요소를 편집할 수 있기 때문에 상호 작용합니다. • 사용자가 자신의 관심사(예: 출력 텍스트의 스타일, 각 타임스텝에서 얼마나 진행해야 하는지 등)에 따라 모델을 사용자 정의하기 위해 프롬프트를 쉽게 수정할 수 있기 때문에 사용자 정의가 가능합니다.또한, 인간 상호 작용은 RECURRENTGPT가 긴 텍스트를 자율적으로 생성할 때 저지른 우발적 실수를 수정하고 긴 텍스트 생성의 주요 병목 현상인 오류 전파를 방지하는 데 도움이 될 수도 있습니다.실험 3.1 실험 설정 작업 이 섹션에서는 RECURRENTGPT의 경험적 효과를 테스트합니다.특히, 다음을 포함한 세 가지 다른 설정에서 RECURRENTGPT를 평가합니다.• • 인간 상호 작용 없이 긴 텍스트를 자율적으로 생성합니다.인간 작가와 협력하여 긴 텍스트를 생성합니다.• 대화형 소설로 텍스트 소비자와 직접 상호 작용합니다.이러한 각 작업에서 공상 과학, 로맨스, 판타지, 공포, 미스터리, 스릴러 소설을 포함한 다양한 장르의 소설로 테스트합니다. 다양한 길이의 텍스트에 대한 RECURRENTGPT의 효과를 테스트하기 위해 공포, 미스터리, 스릴러의 경우 중간 길이(~3000단어)의 소설을 생성하고 공상과학, 로맨스, 판타지의 경우 더 긴 소설(~6000단어)을 생성합니다. 기준선 RECURRENTGPT는 LLM을 사용하여 임의로 긴 텍스트를 생성하는 최초의 작업이지만 아래에 나열된 것처럼 일부 합리적인 기준선과 축소된 변형과 비교할 수 있습니다. • Rolling-ChatGPT는 문학 장르와 일부 개요 또는 배경 설정이 주어진 경우 ChatGPT가 소설을 쓰기 시작하도록 하는 간단한 기준선이며, 컨텍스트 길이 제한에 도달한 후에도 ChatGPT가 쓰기를 계속하도록 반복적으로 촉구합니다. 이 기준선은 Transformers를 사용하여 긴 텍스트를 생성하기 위해 슬라이딩 컨텍스트 윈도우 트릭을 사용하는 것과 거의 같습니다.• RE³ [15]는 LLM이 스토리의 개요를 생성하도록 먼저 촉구한 다음, 다시 순위를 매기고 다시 쓰는 파이프라인을 사용하여 개요에 따라 스토리를 생성하는 계층적 긴 스토리 생성 기준선입니다. 공정한 비교를 보장하기 위해 ChatGPT로 다시 구현합니다.⚫ DOC [16]는 개요 제어로 RE³를 개선하는 최첨단 긴 스토리 생성 기준선입니다. OPT-175B [17]를 ChatGPT로 대체하고 ChatGPT 가중치에 액세스할 수 없기 때문에 사용할 수 없는 자세한 컨트롤러를 제거하여 DOC를 다시 구현합니다. 일반적으로 백본 LLM의 개선으로 인해 다시 구현한 결과 품질이 약간 더 좋아졌습니다. 원칙적으로 두 기준선 모두 일관성을 유지하면서 임의로 긴 텍스트를 생성할 수 없다는 점이 주목할 만합니다. 이는 Rolling-ChatGPT 기준선이 이전에 생성된 콘텐츠를 매우 빠르게 잊어버리기 때문입니다. 반면 RE³와 DOC는 첫 번째 단계에서 개요를 수정하여 생성할 스토리의 전체 길이를 제한합니다. 표 1: 다양한 장르의 소설 20편에 대한 기준선과 RECURRENTGPT의 쌍별 비교. 서로 다른 비교의 결과는 서로 비교할 수 없습니다. 굵은 글씨는 p &lt; 0.05인 유의성을 나타냅니다. 소설 장르 ~ 6000단어 SF 로맨스 판타지 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다↑ 흥미롭다 ↑ 일관성 있다 ↑ RECURRENTGPT 94.86.91.84.95.85.Rolling-ChatGPT 7.14.9.18.6.13.RECURRENTGPT 68.65.71.69.63.62.RE31.28.28.25.35.33.RECURRENTGPT 66.59.77.63.61.56.DOC 30.38.25.29.31.40.소설 장르 공포 미스터리 스릴러 ~ 3000단어 흥미롭다 ↑ 일관성 있다↑ 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다↑ RECURRENTGPT 88.84.87.82.91.82.Rolling-ChatGPT 13.17.14.20.11.17.RECURRENTGPT 64.64.66.63.61.61.RE³ 34.30.27.28.38.37.RECURRENTGPT 65.60.72.66.60.58.DOC 29.39.27.25.33.37.평가 지표 평가를 위해 Yang et al. [15]을 따르고 두 가지 차원에 따라 RECURRENTGPT를 기준선과 비교하여 인간 평가를 수행합니다.· 흥미롭다: 생성된 소설이 일반 독자에게 얼마나 흥미로운가?· 일관성: 문단이 얼마나 잘 구성되고 서로 연결되어 있는가?Yang et al.에 따른 &quot;품질&quot; 또는 &quot;인간과 유사한&quot; 지표는 생략합니다. [16] 모든 기준선이 대부분 시간 동안 고품질 텍스트를 생성할 수 있는 ChatGPT에 기반하기 때문입니다.우리는 쌍별 비교를 통해 비교 모델을 평가합니다.특히, 우리는 다른 비교 방법으로 생성된 두 소설(무작위 순서로 A와 B)을 영어 능력이 좋은 인간 주석자에게 제공하고, 그들에게 소설 A와 소설 B 중 어느 것이 더 나은지, 아니면 흥미롭고 일관성 있는 면에서 구별할 수 없는지 레이블을 지정하도록 지시합니다.Yang et al. [16]의 인간 평가 설정에 따라 각 장르에 대해 생성된 소설 20개를 샘플링하고 각 소설에 대해 3명의 주석자를 할당합니다.3.2 결과 표 1에서 볼 수 있듯이, 우리는 RECURRENTGPT가 롤링 윈도우 기준선과 RE³ 및 DOC와 같은 이전 최신 기술에 비해 흥미롭고 일관성 있는 면에서 인간 독자에게 비교적 큰 마진으로 선호된다는 것을 발견했습니다.이는 긴 텍스트 생성에 순환 계산이 중요하다는 우리의 직감을 확인해줍니다.이 격차는 긴 소설에서 더 크며, 이는I | II | | | I 1차 반복 생성된 계획: 1. Aria는 자신이 작업하고 있는 프로젝트가 정부 계획이라는 것을 알게 된 과학자입니다... 2. 뛰어난 유전학자인 Aria는 민간 우주 탐사 회사에 채용되어... 3. 젊고 재능 있는 과학자인 Aria는 명문 연구... 생성된 스토리: Aria는 비커와 시험관이 줄지어 있는 어두운 연구실에 앉아 있었습니다. 그녀는 피곤한 눈을 비비며 앞에 있는 화면에 표시된 데이터에 집중하려고 노력했습니다. 벽에 있는 시계는 오전 2시를 표시했지만 그녀는 몇 시간 전에 시간의 감각을 잃었습니다. 그녀는 몇 달 동안 프로젝트를 진행했지만 무언가 맞지 않았습니다. 계산이 말이 되지 않았습니다. 영어: 그녀는 의자에 기대앉아 깊은 한숨을 쉬었다.그녀는 창밖을 바라보며 어쩔 수 없이 어떤 감정을 느꼈다... [남은 단어 357개] ....기억: 먼 행성으로 우주선을 보내는 프로젝트를 진행하는 과학자 아리아는 그 프로젝트가 초인간적 존재 집단을 만드는 비밀 정부 프로그램을 은폐하기 위한 것이라는 사실을 알게 된다.그녀는 밤늦게까지 일하면서 연구실 밖의 암울한 세상을 점점 더 자각하게 된다.수트를 입은 남자가 그녀에게 말을 걸자 그녀는 무언가 잘못되었다는 것을 깨닫는다.1차 반복 이제 선택을 해야 할 때다.1. 당신은 마지못해 우주비행사로, ...에 의해 강제로 투입되었다.2. 당신은 노련한 천체 고고학자로, 엘리트 ...의 일원이다.3. 당신은 독립적인 연구원이다. 이동 로봇이... RECURRENTGPT가 대화형 쓰기 보조원 2차 반복 생성된 계획: 1. 슈트를 입은 남자가 Aria가 최초의 테스트 대상 중 하나로 선택되었다고 밝힙니다... 2. Aria는 정부가 그녀를 잡기 위해 아무것도 서슴지 않을 것이라는 것을 알고 도망칩니다... 3. Aria는 슈트를 입은 남자에게 답변을 요구하고 정부가... 생성된 스토리: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하기 위해 쉼 없이 노력했습니다. 그들은 큰 위험을 감수하고 있다는 것을 알았지만, 그것이 진실을 밝히는 유일한 방법이라는 것도 알고 있었습니다. Aria는 프로그램에 대한 접근 권한을 사용하여 그들의 시스템에 바이러스를 심어 진행 속도를 늦추고자 했습니다. 또한 그녀는 프로젝트에 참여한 개인에 대한 가능한 한 많은 정보를 수집하여 그들이 악용할 수 있는 약점을 찾고자 했습니다. 한편, 나머지 그룹은 [281단어 남음] ... ... 기억: 과학자 아리아는 트랜스휴먼 그룹을 만들기 위한 정부의 은폐로 밝혀진 프로젝트를 진행 중입니다. 아리아는 프로젝트에 대한 정보를 수집하고 신뢰할 수 있는 친구들로 구성된 소규모 그룹을 구성하여 다음 움직임을 계획합니다. 그들은 트랜스휴먼 프로그램을 내부에서 침투하고 시스템에 바이러스를 심고 적발되지 않고 진실을 밝히기로 계획합니다. RECURRENTGPT as Interactive Fiction 2-nd Iteration 이제 선택을 할 시간입니다.1. 무언가를 감지한 화성 학자를 만납니다... 2. 외계 대도시를 항해하면서 이해할 수 있다는 것을 깨닫습니다... 3. 통치에 대한 봉기의 한가운데에 있습니다... II 3-rd Iteration 생성된 계획: 1. 아리아의 트랜스휴먼 프로그램에 침투하려는 계획이 발각되고 그들은 강제로... 2. 아리아와 그녀의 그룹은 트랜스휴먼 프로그램에 성공적으로 침투하고 충격적인 것을 발견합니다... 3. 아리아의 트랜스휴먼 프로그램을 폭로하려는 계획은 성공하지만 그 결과는... 생성된 스토리: 시위 날이 왔고 아리아와 그녀의 그룹은 긴장감에 휩싸였습니다.그들은 이 순간을 준비하기 위해 수많은 시간을 보냈지만 여전히 많은 것이 걸려 있었습니다.그들이 시위 장소로 향하면서 그들은 어깨에 세상의 무게를 느낄 수 있었습니다. 영어: 참여율은 그들이 기대했던 것보다 좋았고, 모든 계층의 사람들이 그들의 싸움에 합류했습니다.분위기는 전기적이었고, Aria는 ...을 바라보면서 희망의 감각이 그녀를 휩쓸고 지나가는 것을 느꼈습니다.[426단어 남음] ...기억: Aria와 그녀의 그룹은 트랜스휴먼 프로그램에 침투하여 시스템에 바이러스를 심고 정보를 수집하기 시작했습니다.그들은 우려를 공유하고 시위를 계획하는 다른 그룹과 동맹을 맺었습니다.그들은 프로그램과 정부의 계획에 대한 정보를 수집했고 그 어느 때보다 목소리를 내기로 결심했습니다.3차 반복 이제 선택을 할 시간입니다.1. Elysia가 Temporal Vault라고 알려진 비밀 장소로 안내합니다... 2. Elysia는 그녀의 가르침에서 대담한 아이디어를 제안합니다.아티팩트는 ... 3. 화성 사회를 더 깊이 파고들면서 그것을 발견합니다.나 | | 나 | 나 | II 생성된 스토리: 당신은 세계 최고의 천체 고고학자 알렉세이 니콜라이 박사입니다. 그는 스핑크스의 실제 연대에 대한 수수께끼를 풀고 신비한 보이니치 사본을 해독한 사람입니다. 당신은 이상하고 고대의 것을 다루는 데 익숙하지만 앞으로 무슨 일이 일어날지 전혀 예상할 수 없습니다. 처음 화성에 발을 디딘 순간 가슴이 쿵쾅거리며 기대와 불안이 뒤섞입니다. 당신은 화성 토양 아래에서 발견된 고대 유적을 조사하기 위해 NASA에서 엄선한 팀에 속해 있습니다. 레이더 이미지에 따르면 유적은 오래전에 사라진 문명의 건축물과 비슷합니다... 생성된 스토리: 외계 도시를 탐색하면서 첨단 기술과 건축물에 감탄합니다. 외계 구조물 사이에는 자석처럼 당신을 끌어들이는 지식의 중심지인 학술 기관이 있습니다. 그리고 그곳에서 시간 연구 학자 엘리시아를 만나 자신을 소개합니다. 그녀는 당신에 대해 다른 무언가를 감지하는 듯하며, 그녀의 호기심 어린 눈은 당신의 변장을 꿰뚫어봅니다. &quot;당신 주변에 파장이 일고 있어요, 낯선 사람아.&quot; 그녀가 말하는데, 그녀의 목소리는 음악적입니다. &quot;당신은 타임 테더에 닿았죠?&quot; 그녀가 유물에 대해 알고 있는 지식은 당신을 놀라게 합니다. 여기에 당신의... 생성된 스토리: 당신과 엘리시아가 화성 기술의 복잡한 부분을 더 깊이 파고들면서, 당신은 시간 조작 장치인 타임 테더의 사용이 금지된 고대 법령을 발견합니다. 화성 문명을 거의 멸망시켰던 과거의 재앙적인 사건을 상기시키는 것입니다. 그 깨달음은 당신을 공포로 채웁니다. 당신이 집으로 가는 티켓이라고 생각했던 바로 그 도구가 사람들에게는 파멸의 상징이 되었습니다. 당신은 중대한 결정을 내려야 합니다. 화성의 법을 무시하고 문명 전체의 분노를 감수할 것인가, 아니면 그들의 법에 따라 일할 것인가? 후자를 선택하여 화성 의회에 자신의 입장을 변론하기로 결정했습니다... 그림 2: RECURRENTGPT를 대화형 쓰기 도우미와 대화형 소설로 사용한 정성적 분석. 강조된 계획 또는 선택 사항은 인간 사용자가 선택한 것입니다. 매우 긴 텍스트를 생성하는 RECURRENTGPT. 마지막으로 인간 주석자는 모든 소설 장르에서 RECURRENTGPT를 선호합니다. 이는 다양한 유형의 긴 텍스트에서의 견고성을 확인합니다. RECURRENTGPT의 효과를 더 잘 이해하기 위해 단기 또는 장기 기억이 없는 절제된 변형과 GPT-4를 백본 모델로 사용하는 변형과 RECURRENTGPT를 비교하여 절제 연구를 수행합니다. 결과는 표 2에 나와 있습니다. 장기/단기 기억이 주로 생성된 텍스트의 일관성에 기여하는 것을 볼 수 있으며, 이는 직관과 잘 일치합니다. GPT-4를 백본 LLM으로 사용하는 RECURRENTGPT는 ChatGPT/GPT-3.5-turbo를 사용하는 대응 모델에 비해 엄청나게 선호됩니다. 이는 더 강력한 LLM을 장착했을 때 RECURRENTGPT의 잠재력을 확인합니다. 정성적 평가를 위해 부록에 RECURRENTGPT에서 생성한 몇 가지 샘플 소설을 제시합니다. 3.3 대화형 쓰기 도우미로서의 RECURRENTGPT 그런 다음 인간-AI 상호 작용 관점에서 대화형 쓰기 도우미로서 RECURRENTGPT의 유용성을 테스트합니다. 그림 2에서 볼 수 있듯이 인간 작가는 먼저 자신이/그녀가 원하는 주제를 선택합니다.표 2: 절제된 변형이 있는 RECURRENTGPT와 GPT-4를 백본 모델로 사용하는 변형의 쌍별 비교. 비교를 위해 서로 다른 장르의 소설 20개를 샘플링했습니다. 서로 다른 비교의 결과는 서로 비교할 수 없습니다. 굵은 글씨는 p &lt; 0.05인 유의성을 나타냅니다. 소설 장르 SF 판타지 ~ 6000단어 흥미롭다 ↑ 일관성 있다 ↑ 흥미롭다 ↑ 일관성 있다 ↑ RECURRENTGPT 58.65.55.64.w/o 단기 기억 44.31.47.33.RECURRENTGPT 51.71.57.68.w/o 장기 기억 40.27.46.38.RECURRENTGPT 21.28.27.24.w/ GPT-73.64.71.70.w/를 쓰고 싶어하며 책의 배경과 개요를 설명하는 짧은 문단을 씁니다. 그런 다음 RECURRENTGPT가 자동으로 첫 번째 문단을 생성하고 작가가 스토리를 계속할 수 있는 몇 가지 가능한 옵션을 제공합니다. 작가는 그중 하나를 선택하여 필요한 경우 편집할 수 있습니다. 또한 생성된 계획이 모두 부적절한 경우 다음 몇 단락에 대한 짧은 계획을 직접 작성할 수 있으므로 인간-AI 공동 집필 프로세스가 더 유연해집니다. 부록 B에서 인간 작가가 RECURRENTGPT와 상호 작용하여 다양한 장르의 소설을 쓸 수 있는 Gradio 기반 인터페이스를 보여줍니다. 소규모 인간 사용자 연구에 따르면 RECURRENTGPT는 인간 작가의 생산성을 크게 향상시키며 이러한 향상은 주로 다음에서 비롯됩니다. (1) 짧은 계획을 작성하거나 선택하고 RECURRENTGPT가 실제 텍스트를 생성하도록 하여 긴 텍스트를 입력하는 시간을 줄임; (2) 사용자 피드백에 따라 RECURRENTGPT에서 생성된 계획에서 계획을 선택하여 덜 중요한 플롯을 디자인하는 시간을 줄임. 더욱이 사용자는 RECURRENTGPT가 블랙박스 역할을 하는 기존 AI 쓰기 도우미에 비해 더 해석 가능하고 제어 가능하다고 느낍니다. RECURRENTGPT의 언어 기반 구성 요소는 사용자에게 투명하고 편집 가능하기 때문입니다. 마지막으로, DOC 및 RE³와 같이 긴 텍스트를 계층적으로 생성하는 이전 방법과 비교했을 때, 인간 사용자는 반복적이고 대화형으로 긴 텍스트를 작성하는 것이 더 유연하고 제어 가능하기 때문에 우리 시스템을 선호합니다.마지막으로, 우리 시스템은 대부분의 기존 AI 쓰기 도우미와 매우 다릅니다.그들은 구문이나 몇 문장 내에서 로컬 쓰기 제안을 제공하는 데 중점을 두는 반면, RECURRENTGPT는 한 번에 몇 개의 단락을 생성할 수 있습니다.3.4 대화형 소설로서의 RECURRENTGPT 또한 RECURRENTGPT를 개인화된 대화형 소설로 사용할 수 있는지 테스트합니다.이 사용 사례는 AI 쓰기 도우미로서의 RECURRENTGPT와 매우 유사합니다.주요 차이점은 그림 2에서 볼 수 있듯이 두 가지입니다.(1) 인간 플레이어에게 몰입감을 조성하기 위한 3인칭 관점에서 1인칭 관점으로의 전환, (2) RECURRENTGPT가 다음 단락에 대한 일반적인 계획이 아닌 주인공에게 중요한 선택이 포함된 계획을 생성하도록 하는 것입니다.이러한 적응은 프롬프트를 약간 수정하여 쉽게 구현할 수 있습니다. 사용자 연구에 따르면 RECURRENTGPT는 인간 플레이어와 상호 작용하고 인간 소비자에게 양질의 콘텐츠를 직접 제공할 수 있습니다. 인간 플레이어는 대화형 소설에서의 행동이 흥미를 크게 향상시키므로 자유형 텍스트를 쓸 수 있는 가능성을 찾습니다. 이는 생성 AI를 콘텐츠 제작 도구로 사용하는 대신 콘텐츠로 직접 사용할 수 있는 잠재력을 확인합니다. 그러나 RECURRENTGPT가 때때로 일관성이 떨어지는 콘텐츠와 관련성이 낮거나 합리적이지 않은 저품질 옵션을 생성한다는 사실도 발견했습니다. 이는 더 강력한 LLM 백본을 사용하거나, 인간 피드백을 통한 감독 미세 조정 또는 강화 학습으로 LLM 백본을 미세 조정하거나, 더 나은 프롬프트를 설계함으로써 개선될 수 있다고 생각합니다. 이는 향후 작업으로 남겨둡니다. 4https://gradio.app/ 5더 큰 규모의 사용자 연구를 수행하고 수정된 버전에서 세부 정보와 결과를 제시할 것입니다.4 관련 연구 4.1 고정 크기 컨텍스트를 넘어서는 트랜스포머 트랜스포머의 주요 한계 중 하나는 컨텍스트 크기가 고정되어 긴 텍스트를 처리하고 생성하는 능력을 방해한다는 것입니다. 이전 연구에서는 두 가지 다른 방법으로 이 문제를 해결하려고 시도했습니다. 더 큰 컨텍스트 창을 사용하여 Transformer를 훈련하고 사용하기 위한 효율적인 어텐션 메커니즘을 설계[18–21]하고 Transformer의 계산 그래프에 메모리 메커니즘을 추가하여 여러 컨텍스트 창에서 정보를 처리할 수 있도록 하는 것입니다[9, 22, 23, 11]. 이러한 방법을 사용하면 Transformer가 매우 긴 텍스트를 처리할 수 있지만 모두 원래 Transformer 아키텍처에 상당한 아키텍처 변경이 필요합니다. 따라서 이러한 접근 방식은 ChatGPT 및 LLAMA와 같은 강력한 사전 훈련된 LLM에 통합할 수 없으므로 유용성이 크게 제한됩니다. 최근 Press et al.[24]은 어텐션에 선형 편향을 추가하여 입력 길이 외삽을 허용하는 ALiBi를 도입했습니다. 그러나 이 방법은 주로 긴 출력 대신 긴 입력을 지원합니다. 또한 모델 매개변수와 추론 코드에 액세스해야 하는데, ChatGPT, GPT-4, PaLM과 같은 최신 LLM이 대부분 폐쇄 소스이기 때문에 종종 불가능합니다. 4.2 긴 텍스트 생성 구조적 수정 외에도 많은 연구에서 계층적 방식으로 긴 텍스트 생성을 조사합니다.Fan et al. [25]은 먼저 짧은 요약을 생성하여 스토리를 생성한 다음 스토리의 술어-인수 구조인 개요를 생성하는 중간 단계를 추가하여 이 방법을 개선하는 것을 제안했습니다.[26] Tan et al. [27]과 Sun et al. [28]은 이러한 종류의 계층적 긴 텍스트 생성 방법을 더욱 개선했습니다.Yao et al. [29]도 먼저 스토리라인을 생성한 다음 스토리를 완성하는 것을 제안했습니다.이 연구 분야는 RE³ [15]와 그 변형인 DOC[16]에 의해 더욱 개선되었으며, 계획 및 작성 방식으로 긴 스토리 생성을 위한 LLM을 재귀적으로 촉구했습니다.그러나 최종 스토리의 플롯과 길이는 여전히 미리 결정된 계획에 의해 제한됩니다. 대조적으로 RECURRENTGPT는 반복적 생성을 통해 위의 한계를 극복하여 효과적인 인간-LM 협업을 가능하게 하고 긴 텍스트 생성의 유연성과 제어성을 개선합니다.4.3 AI 지원 쓰기 시스템 AI 쓰기 도우미는 스토리 완성[12], 에세이 쓰기[30], 시 생성[31]을 포함한 다양한 응용 프로그램에 채택되었습니다.기존 시스템은 대체로 대화형 생성과 자동 생성으로 분류할 수 있습니다.대화형 시스템[32-34]은 주로 구문이나 문장 수준에서 로컬 제안이나 수정을 제공하도록 설계되었습니다.결과적으로 인간 작가의 창의적 부담을 덜어주는 능력이 떨어집니다.반면 자동 생성[26, 35, 36]은 시퀀스-투-시퀀스 프레임워크를 통해 주어진 프롬프트나 주제에 따라 전체 텍스트를 작성하는 것을 목표로 합니다.LLM의 발전은 이러한 시스템에 대한 인상적인 잠재력을 보여주었지만 투명성, 제어 가능성 및 협업 감각이 부족하면 작가의 인식된 소유권과 관련된 사용자 경험에 해를 끼칠 수 있습니다[12, 37]. 게다가 대부분은 여러 구문에서 몇 문장에 이르는 로컬 편집 제안을 제공함으로써 제한을 받는데[38, 39], 이는 부분적으로 NLG 모델의 길이 제한 때문이고 부분적으로는 장거리 일관성을 유지하는 과제 때문입니다.5 제한 사항 이 작업의 한 가지 제한 사항은 RECURRENTGPT가 임의로 긴 텍스트를 생성할 수 있는 반면, 생성된 텍스트가 최대 5000단어 정도인 설정에서만 평가한다는 것입니다.매우 긴 텍스트에 대한 정성적, 정량적 평가가 모두 엄청나게 어렵기 때문입니다.또 다른 제한 사항은 RECURRENTGPT가 ChatGPT 및 GPT-4와 같이 충분히 강력한 백본 LLM에서만 작동한다는 것입니다.더 강력한 소규모 LLM이 개발되면 이 문제가 완화될 수 있다고 생각합니다.마지막으로, AI 쓰기 도우미 및 대화형 소설로서 RECURRENTGPT를 평가하기 위한 사용자 연구는 소규모 연구에 의해 제한됩니다.개정된 버전에서 사용자 연구 전반에 걸쳐 더 크고 더 많은 연구를 추가할 것입니다. 사회적 영향에 관해서는 RECURRENTGPT는 AI가 생성한 긴 텍스트의 품질을 개선하고 인간 작가의 생산성을 높일 수 있습니다. 그러나 부정적인 사회적 영향을 초래하는 쓰레기나 유해한 콘텐츠를 생성하는 데 오용될 수도 있습니다. 그러나 이는 생성적 AI의 알려진 한계이며, 우리는 생성적 AI의 책임 있는 사용을 촉진하기 위해 최선을 다할 것입니다.6
--- CONCLUSION ---
s 우리는 언어 기반 구성 요소를 사용하고 프롬프트 엔지니어링을 통해 반복 계산 그래프를 정의하는 RNN의 반복 메커니즘에 대한 언어 기반 시뮬라크라인 RECURRENTGPT를 제시합니다. RECURRENTGPT는 LLM이 자율적으로 또는 인간 작성자와 상호 작용하여 임의로 긴 텍스트를 생성하도록 합니다. 언어 기반 구성 요소는 해석 가능성과 제어 가능성을 개선하고 프롬프트 기반 계산 그래프는 쉽게 사용자 정의할 수 있게 해줍니다. RECURRENTGPT를 AI 쓰기 도우미 및 텍스트 기반 게임으로 사용하는 것에 대한 사용자 연구는 로컬 쓰기 제안을 넘어 차세대 AI 쓰기 도우미로의 초기 단계로서의 잠재력을 보여주고 상호 작용을 통해 소비 가능한 콘텐츠로 생성 AI를 직접 사용합니다. 마지막으로, 우리의 연구는 LLM을 사용하여 장문 텍스트를 생성하기 위해 인지 과학 및 딥 러닝 문헌에서 인기 있는 모델 설계에서 아이디어를 빌릴 수 있는 가능성을 보여줍니다. 참고문헌 [1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 생성적 사전 학습을 통한 언어 이해 향상. 2018. [2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 1(8):9, 2019. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 및 Dario Amodei. 언어 모델은 few-shot 학습자입니다. H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집, 신경 정보 처리 시스템의 발전, 33권, 1877~1901쪽. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [4] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id=TG8KACXEON. [5] OpenAI. Gpt-4 기술 보고서, 2023. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett 편집자, 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [7] Jeffrey L. Elman. 시간 속 구조 찾기. 인지 과학, 14(2):179–211, 1990. ISSN 0364-0213. doi: https://doi.org/10.1016/0364-0213(90)90002-E. URL https://www. sciencedirect.com/science/article/pii/036402139090002E. [8] Sepp Hochreiter 및 Jürgen Schmidhuber. 장기 단기 기억. 신경 계산, 9(8): 1735-1780, 1997. [9] Zihang Dai*, Zhilin Yang*, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le 및 Ruslan Salakhutdinov. Transformer-XL: 장기 종속성이 있는 언어 모델링, 2019. URL https://openreview.net/forum?id=HJePno0cYm.[10] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier 및 Timothy P. Lillicrap. 장거리 시퀀스 모델링을 위한 압축 변압기. 국제 학습 표현 컨퍼런스, 2020. URL https://openreview.net/forum?id=SylKikSYDH. [11] Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev. 순환 메모리 변압기. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id= Uynr3iPhksa. [12] Mina Lee, Percy Liang, Qian Yang. 공동 저자: 언어 모델 기능 탐색을 위한 인간-AI 협력 쓰기 데이터 세트 설계. 2022년 CHI 컴퓨팅 시스템의 인간 요인 컨퍼런스 회의록, CHI &#39;22, 뉴욕, 뉴욕, 미국, 2022. 컴퓨팅 기계 협회. ISBN 9781450391573. doi: 10.1145/3491102.3502030. URL https://doi.org/10.1145/3491102.3502030. [13] Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek. 제어보다 선택: 사용자가 다이어제틱 및 비다이어제틱 프롬핑을 사용하여 대규모 언어 모델로 작성하는 방법. 2023년 CHI 컴퓨팅 시스템의 인간 요인 컨퍼런스 회의록, CHI &#39;23, 뉴욕, 뉴욕, 미국, 2023. 컴퓨팅 기계 협회. ISBN 9781450394215. doi: 10.1145/ 3544548.3580969. URL https://doi.org/10.1145/3544548.3580969. [14] Nils Reimers 및 Iryna Gurevych. Sentence-bert: siamese bertnetworks를 사용한 문장 임베딩. 2019년 자연어 처리 경험적 방법 컨퍼런스의 회의록. 계산 언어학 협회, 2019년 11월. URL https://arxiv.org/abs/1908.10084. [15] Kevin Yang, Yuandong Tian, Nanyun Peng 및 Dan Klein. Re3: 재귀적 재프롬프팅 및 수정을 사용하여 더 긴 스토리 생성. 2022년 자연어 처리 경험적 방법에 관한 컨퍼런스의 회의록, 4393-4479쪽, 아랍에미리트 아부다비, 2022년 12월. 계산언어학 협회. URL https://aclanthology. org/2022. emnlp-main. 296. [16] Kevin Yang, Dan Klein, Nanyun Peng, Yuandong Tian. Doc: 세부적인 개요 제어를 통한 긴 스토리 일관성 개선, 2022. [17] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: 사전 학습된 변환기 언어 모델 개방. arXiv 사전 인쇄본 arXiv:2205.01068, 2022. [18] Iz Beltagy, Matthew E. Peters, Arman Cohan. Longformer: 긴 문서 변환기. arXiv:2004.05150, 2020. [19] Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya. Reformer: 효율적 변환기. ICLR에서. OpenReview.net, 2020. [20] Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. 희소 변환기를 사용하여 긴 시퀀스 생성, 2019. [21] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. Big bird: Transformers for longer sequences. NeurIPS에서, 2020. [22] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. R-transformer: Recurrent Neural Network Enhanced Transformer, 2019. [23] Peng Cui and Le Hu. 긴 문서의 추출 요약을 위한 동적 메모리가 있는 슬라이딩 선택기 네트워크. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 5881-5891쪽, 온라인, 2021년 6월. 컴퓨터 언어학 협회. doi: 10.18653/v1/2021. naacl-main.470. URL https://aclanthology.org/2021. naacl-main.470. [24] Ofir Press, Noah A. Smith, and Mike Lewis. 짧게 훈련하고 길게 테스트: 선형 편향이 있는 주의는 입력 길이 외삽을 가능하게 합니다. ICLR에서. OpenReview.net, 2022.[25] Angela Fan, Mike Lewis, Yann Dauphin. 계층적 신경 스토리 생성. Association for Computational Linguistics의 제56회 연례 회의록(제1권: 긴 논문), 889-898페이지, 2018. [26] Angela Fan, Mike Lewis, Yann Dauphin. 스토리 생성 구조화 전략. arXiv 사전 인쇄본 arXiv:1902.01109, 2019. [27] Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, Zhiting Hu. 사전 훈련된 언어 모델을 사용한 긴 텍스트의 점진적 생성. 2021년 북미 계산언어학회 학술대회 논문집: 인간언어기술, 4313-4324쪽, 온라인, 2021년 6월. 계산언어학회. doi: 10.18653/v1/2021.naacl-main.341. URL https://aclanthology.org/ 2021.naacl-main. 341. [28] Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, Chun Fan. 요약, 개요, 세부화: 추출 요약에서 계층적 감독을 통한 장문 텍스트 생성. 2022년 10월 대한민국 경주에서 열린 제29회 국제 계산언어학회 학술대회 논문집, 6392-6402쪽. 국제 계산언어학 위원회. URL https://aclanthology.org/2022. coling-1.556. [29] Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, Rui Yan. Plan-and-write: Towards better automatic storytelling. AAAI, 7378-7385쪽. AAAI Press, 2019. [30] Yuanchao Liu, Bo Pang, Bingquan Liu. 에세이 쓰기에서 우아함을 강화하기 위한 신경 기반 중국어 관용어 추천. 57회 연례 총회록, 5522-5526쪽, 이탈리아 피렌체, 2019년 7월. Association for Computational Linguistics. doi: 10.18653/v1/P19-1552. URL https://aclanthology.org/P19-1552. [31] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, Kevin Knight. Hafez: 대화형 시 생성 시스템. ACL 2017 회의록, System Demonstrations, 4348쪽, 캐나다 밴쿠버, 2017년 7월. Association for Computational Linguistics. URL https://aclanthology.org/P17-4008. [32] Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan. Wordcraft: 스토리 쓰기를 위한 인간-AI 협업 편집기. arXiv 사전 인쇄본 arXiv:2107.07430, 2021. [33] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, Minsuk Chang. Talebrush: 생성적 사전 학습 언어 모델을 사용하여 스토리 스케치. 2022년 CHI 컴퓨팅 시스템의 인간적 요인 컨퍼런스 회의록, 1-19페이지, 2022년. [34] Seraphina Goldfarb-Tarrant, Haining Feng, Nanyun Peng. 계획, 작성 및 수정: 오픈 도메인 스토리 생성을 위한 대화형 시스템. arXiv 사전 인쇄본 arXiv:1904.02357, 2019년. [35] Yufei Tian과 Nanyun Peng. 담화 수준 계획 및 미적 특징을 갖춘 제로샷 소네트 생성. 2022년 북미 컴퓨터 언어학 협회 컨퍼런스 회의록: 인간 언어 기술, 3587-3597페이지, 미국 시애틀, 2022년 7월. 컴퓨터 언어학 협회. doi: 10.18653/v1/2022.naacl-main.262. URL https://aclanthology.org/2022. naacl-main.262. [36] Boyang Li, Stephen Lee-Urban, George Johnston, Mark Riedl. 크라우드소싱 플롯 그래프를 사용한 스토리 생성. AAAI 인공지능 컨퍼런스 회의록, 27권, 598-604페이지, 2013년. [37] Jeremy Birnholtz, Stephanie Steinhardt, Antonella Pavese. 여기에 쓰고 지금 쓰세요! 협업적 글쓰기에서 그룹 유지에 대한 실험적 연구. SIGCHI 컴퓨팅 시스템의 인간적 요인 컨퍼런스 회의록, 961-970페이지, 2013년. [38] Rujun Han, Hong Chen, Yufei Tian, Nanyun Peng. 시간 여행: 이벤트 시간적 프롬프트가 있는 스토리에서 플래시백 생성. arXiv 사전 인쇄본 arXiv:2205.01898, 2022. [39] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, Rui Yan. Planand-write: 더 나은 자동 스토리텔링을 향하여. AAAI 인공지능 컨퍼런스 회의록, 33권, 7378-7385페이지, 2019.A 프롬프트 소설을 쓰는 것을 도와주세요. 이제 400단어의 메모리(간단한 요약)를 드리겠습니다. 이를 사용하여 쓰여진 내용의 주요 내용을 저장하여 매우 긴 맥락을 추적할 수 있습니다. 매번 현재 메모리(이전 스토리의 간략한 요약), 이전에 작성된 단락, 다음 단락에서 무엇을 쓸지에 대한 지침을 제공합니다. 다음을 작성해 주세요. 1. 출력 문단: 소설의 다음 문단. 출력 문단은 약 20개의 문장을 포함해야 하며 입력 지침을 따라야 합니다. 2. 출력 메모리: 업데이트된 메모리. 먼저 입력 메모리의 어떤 문장이 더 이상 필요하지 않은지와 그 이유를 설명한 다음, 메모리에 무엇을 추가해야 하는지와 그 이유를 설명해야 합니다. 그런 다음 업데이트된 메모리를 작성해야 합니다. 업데이트된 메모리는 이전에 삭제 또는 추가해야 한다고 생각했던 부분을 제외하고는 입력 메모리와 유사해야 합니다. 업데이트된 메모리는 핵심 정보만 저장해야 합니다. 업데이트된 메모리는 20개의 문장을 초과해서는 안 됩니다! 다음에 무엇을 쓸지에 대한 지침(작성한 내용 다음에). 3개의 다른 지침을 출력해야 하며, 각각은 스토리의 흥미로운 연속이 될 수 있습니다. 각 출력 명령어는 약 5개의 문장을 포함해야 합니다.3. 출력 명령어: 입력은 다음과 같습니다.입력 메모리: (short_memory) 입력 문단: (input_paragraph} 입력 명령어: (input_instruction} 입력 관련 문단: (input_long_term_memory} 이제 쓰기를 시작하고 아래와 같이 출력 형식을 엄격히 따라 출력을 구성하세요.출력 문단:<string of output paragraph> , 약 20개의 문장. 출력 메모리: 합리적:<string that explain how to update the memory> ; 업데이트된 메모리:<string of updated memory> , 약 10~20문장 출력 지시사항: 지시사항 1:<content for instruction 1> , 약 5 문장 지시 2:<content for instruction 2> , 약 5 문장 지시 3:<content for instruction 3> , 약 5개 문장 매우 중요: 업데이트된 메모리는 핵심 정보만 저장해야 합니다. 업데이트된 메모리는 500단어를 넘지 않아야 합니다! 마지막으로, 소설을 쓰고 있다는 것을 기억하세요. 소설가처럼 쓰고 다음 문단의 출력 지침을 작성할 때 너무 빨리 움직이지 마세요. 장은 문단을 포함하고 소설은 100개 이상의 장을 포함할 것이라는 점을 기억하세요. 그리고 이것은 시작에 불과합니다. 다음에 일어날 흥미로운 스태프를 몇 개 쓰세요. 또한 출력 지침을 작성할 때 일반 독자에게 어떤 플롯이 매력적일 수 있는지 생각해보세요. 먼저 입력 메모리의 어떤 문장이 더 이상 필요하지 않은지와 그 이유를 설명한 다음, 메모리에 무엇을 추가해야 하는지와 그 이유를 설명해야 합니다. 그런 다음 입력 메모리를 다시 작성하여 업데이트된 메모리를 얻습니다. 그림 3: 입력(계획, 지침), 출력, 단기 메모리, 장기 메모리를 각각 시뮬레이션하는 RECURRENTGPT 프레임워크의 백본 LLM을 위해 설계된 프롬프트. B 데모 RecurrentGPT 대화형 (임의로) 긴 텍스트 생성, 인간 참여 소설 유형 공상 과학 소설 Init 소설 생성 메모리 모듈 단기 메모리 돔형 도시의 과학자인 마야는 돔 밖에서 사는 사람들을 발견합니다. 그녀는 위험에도 불구하고 그들을 방문하기로 결심하고 황무지를 통한 긴 여행 끝에 마침내 그들의 정착지를 찾습니다. 마야는 집안일을 돕고, 그들의 생존 방법을 배우고, 과학과 기술에 대한 지식을 공유함으로써 사람들의 신뢰를 얻습니다. 그녀는 발견한 사람과 힘을 합쳐 가축 맨틀에 전할 계획을 세웁니다. 서면 단락 이름: 우리 종족의 마지막 개요: 2078년, 지구 대부분을 살 수 없게 만든 대참사로 인해 인류는 멸종 직전에 있습니다. 남은 인구는 돔형 도시에서 살고 있으며 생존을 위해 첨단 기술에 의존합니다. 이 이야기는 돔 밖에서 사는 사람들을 발견한 젊은 과학자 마야의 이야기입니다. 마야는 자신의 사람들에 대한 충성심과 이 그룹과 함께 새로운 시작을 할 가능성 사이에서 갈등합니다. 문단: 마야는 연구실에서 나와 도시의 밝은 불빛에 눈을 맞추었습니다. 그녀는 우뚝 솟은 고층 빌딩과 그녀의 세계를 둘러싼 반짝이는 흰색 돔을 둘러보았습니다. 불과 수십 년 전만 해도 이곳이 황무지였다는 것이 믿기 어려웠습니다. 마야의 생각은 과거로, 모든 것을 바꾼 사건으로 흘러갔습니다. 마야의 일은 도시의 생존을 돕는 새로운 기술을 개발하는 것이었습니다. 그녀는 항상 돔 도시와 그것이 제공하는 안전과 보안을 믿었습니다. 하지만 그녀의 최근 프로젝트는 그녀를 놀라운 발견으로 이끌었습니다. 돔 밖에 사람들이 살고 있다는 것입니다. 마야는 처음에는 믿을 수 없었습니다. 지구 표면 대부분을 덮고 있는 독성 황무지에서 누가 살아남을 수 있을까요? 하지만 증거는 부인할 수 없었습니다. 마야는 방으로 돌아가는 동안 심장이 쿵쾅거렸다. 그녀는 돔 밖에 사는 사람들에 대한 생각을 떨칠 수 없었다. 그들은 누구였을까? 그들은 도시를 둘러싼 독성 황무지에서 어떻게 그렇게 오랫동안 살아남았을까? 마야는 더 많은 것을 알아야 한다는 것을 알았지만, 그에 따른 위험도 알고 있었다. 만약 그녀가 잡히면, 그녀는 엄중한 처벌을 받거나, 더 나쁜 경우, 도시에서 추방당할 수도 있었다. 하지만 그녀는 자신이 발견한 것을 그냥 무시할 수 없었다. 마야의 마음은 나시빌리티로 가득 찼고, 나치 아나 마마는 전보다 더 위험했다. 장기 기억 1을 조심해야 했다. 마야는 연구실에서 나와 도시의 밝은 빛에 눈을 적응시켰다. 그녀는 우뚝 솟은 고층 빌딩과 그녀의 세계를 둘러싼 반짝이는 흰색 돔을 둘러보았다. 불과 수십 년 전만 해도 이곳이 황무지였다는 것이 믿기 어려웠다. 마야의 생각은 과거로, 모든 것을 바꾼 사건으로 흘러갔다. 2. 마야의 일은 도시의 생존을 도울 새로운 기술을 개발하는 것이었습니다. 그녀는 항상 돔형 도시, 그것이 제공하는 안전과 보안을 믿었습니다. 하지만 그녀의 최근 프로젝트는 그녀를 놀라운 발견으로 이끌었습니다. 다마에 스칸라가 살고 있었습니다. 마운은 처음에는 그것을 할리문할 수 없었습니다. 우누는 타비르 와발란드에서 사바나 쿠미운을 할 수 있었습니다. 지침 모듈 지침 1(편집 가능) 마야는 정착지의 지도자들을 만나 그녀의 비밀을 공유합니다. 그들은 조치를 취하기로 결정하고 마야는 그들의 노력에 동참합니다. 선택된 지침 지침 2(편집 가능) 마야의 비밀은 정착지의 나머지 사람들에게 밝혀지고 그들은 충격과 분노로 반응합니다. 마야는 자신을 변호하고 그녀의 행동을 설명해야 합니다. 지침 3(편집 가능) 마야는 돔형 도시를 공격하려는 음모를 발견하고, 사람들에게 경고할지, 아니면 그녀의 집이 된 정착지에 충성할지 결정해야 합니다. 마야는 집안일을 돕고, 생존 방법을 배우고, 과학과 기술에 대한 지식을 공유함으로써 돔 밖에 사는 사람들의 신뢰를 계속 얻습니다. 그녀는 그들의 수완과 독창성에 매료되고, 돔형 도시에 대한 자신의 믿음에 의문을 품기 시작합니다. 그러나 그녀는 자신이 깨달은 핵심에 어려움을 겪고, 공동체의 신뢰를 미워하지 않고 진실을 밝힐 방법을 찾아야 합니다. 예시 공상과학 로맨스 미스터리 판타지 역사 공포 스릴러 서부 청소년 문학 소설 그림 4: RECURRENTGPT.Next Step의 웹 데모
