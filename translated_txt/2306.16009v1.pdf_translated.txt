--- ABSTRACT ---
최근의 엔드투엔드 자동 음성 인식(ASR) 시스템은 종종 높은 프레임 속도로 임베딩을 생성하는 Transformer 기반 음향 인코더를 활용합니다. 그러나 이 설계는 특히 셀프 어텐션의 2차 계산으로 인해 긴 음성 신호의 경우 비효율적입니다. 이를 해결하기 위해, 키 값 간에 높은 유사도 점수를 가진 인접한 토큰을 점진적으로 결합하는 새로운 방법인 Adjacent Token Merging(A-TOMe)을 제안합니다. 이런 식으로 총 시간 단계를 줄일 수 있고, 인코더와 조인트 네트워크의 추론이 가속화됩니다. LibriSpeech에서 실시한 실험에 따르면, 이 방법은 토큰을 57% 줄이고 GPU에서 추론 속도를 70% 향상시킬 수 있으며, 정확도가 크게 떨어지지 않습니다. 또한, A-ToMe가 입력 음성이 여러 발화로 구성된 장문 ASR에서 토큰을 줄이는 효과적인 솔루션임을 보여줍니다. 색인 용어: 음성 인식, 트랜스듀서, 적응적 서브샘플링 1.
--- METHOD ---
, 인접 토큰 병합(A-TOMe)은 키 값 간에 높은 유사도 점수를 가진 인접 토큰을 점진적으로 결합합니다. 이런 식으로 총 시간 단계를 줄일 수 있고 인코더와 조인트 네트워크의 추론이 가속화됩니다.
--- EXPERIMENT ---
LibriSpeech의 s는 우리 방법이 토큰을 57% 줄이고 GPU에서 추론 속도를 70% 향상시킬 수 있으며, 정확도가 크게 떨어지지 않는다는 것을 보여줍니다. 또한 A-ToMe가 입력 음성이 여러 발화로 구성된 장형 ASR에서 토큰을 줄이는 데 효과적인 솔루션임을 보여줍니다. 색인 용어: 음성 인식, 변환기, 적응형 서브샘플링 1. 서론 종단간(E2E) 자동 음성 인식(ASR) 분야는 최근 몇 년 동안 상당한 진전을 이루었습니다[1, 2, 3, 4, 5, 6, 7]. 세 가지 주요 접근 방식이 등장했습니다. 연결주의 시간 분류(CTC)[8], 주의 기반 인코더 디코더(AED)[9], 순환 신경망 변환기(RNN-T)[10]. 이러한 방법은 음성과 텍스트 토큰 간의 정렬을 처리하는 방식이 다릅니다. AED는 교차 주의를 사용하는 반면, CTC와 RNN-T는 &quot;공백&quot;과 같은 중복 기호를 사용합니다. 이 모든 모델의 인코더는 높은 프레임 속도로 세분화된 음향 임베딩을 처리하여 높은 계산 비용이 발생합니다. 음향 토큰의 빈도가 음소나 단어 조각과 같은 텍스트 토큰보다 훨씬 높기 때문에 상당한 중복성이 존재합니다. 따라서 인코더 내의 시퀀스 길이를 줄이는 것은 E2E ASR의 효율성을 개선하는 데 중요합니다. 자연어 처리 분야에서 적응형 하위 샘플링 기술이 광범위하게 연구되었으며, 토큰 가지치기는 가장 인기 있는 접근 방식 중 하나입니다[11, 12, 13, 14]. 토큰 가지치기는 일반적으로 멀티헤드 어텐션 메커니즘에서 누적 어텐션 점수에 의해 결정되는 중요도 점수가 낮은 토큰을 제거하는 것을 포함합니다. 가지치기된 토큰의 양은 고정 구성[13], 학습된 임계값[14] 또는 진화적 탐색[12]을 통해 결정할 수 있습니다. 이러한 방법은 시퀀스-투-시퀀스 작업보다는 시퀀스 수준 분류 작업에서 종종 평가됩니다.ASR의 경우 대부분의 연구는 합성곱 계층을 통한 점진적 다운샘플링과 같은 고정 길이 서브샘플링에 초점을 맞췄습니다[15, 16].Squeezeformer[17]는 업샘플링 계층을 사용한 다음 다운샘플링을 사용하여 성능을 더욱 향상시켰습니다.그러나 고정 길이 서브샘플링은 음향 단위의 지속 시간이 맥락과 화자에 따라 상당히 달라지기 때문에 최적이 아닐 수 있습니다.이 문제를 해결하기 위해 Meng et al.[18]은 음소 경계의 감독과 함께 CIF[19] 모듈을 사용하여 Distill-Hubert[20]에서 적응 속도를 달성할 것을 제안했습니다.Cuervo et al.[21]은 두 수준 사이에 경계 예측기와 평균 풀링 계층이 있는 2단계 CPC 네트워크를 제안했습니다.이 연구에서는 토큰 병합[22]이라는 최근 도입된 적응적 서브샘플링 기술에 집중합니다. 이 방법은 원래 분류 작업을 위한 Vision Transformers에서 사용하기 위해 개발되었습니다. 이 방법은 주의 메커니즘 내의 키 값 간에 높은 코사인 유사도 점수를 갖는 모든 위치에서 토큰을 병합하여 작동합니다. 그러나 토큰의 시간적 순서를 유지하는 것이 중요하기 때문에 ASR 작업에 직접 적용할 수 없습니다. 이 문제를 해결하기 위해 서로 인접한 토큰만 병합하는 Adjacent Token Merging(A-ToMe)이라는 수정된 기술을 제안합니다. 또한 특정 수의 토큰을 병합하는 대신 다양한 입력 길이를 처리하기 위해 고정 병합 비율과 고정 병합 임계값의 두 가지 다른 구성을 도입합니다. 이전 연구와 달리 제안된 방법은 경계를 명시적으로 예측하지 않습니다. 대신 레이어가 깊어짐에 따라 가변 프레임 속도를 달성하기 위해 유사한 토큰을 점진적으로 결합합니다. 실험은 Transformer 변환기[24]를 기준으로 LibriSpeech[23] 데이터 세트에서 수행되었습니다. 병합 비율 또는 임계값을 변경하여 병합된 토큰의 수를 조정했습니다. 대부분의 경우, 전체 병합 비율이 60% 미만일 때 모델은 기준선과 비슷한 단어 오류율(WERS)을 유지하면서 CPU와 GPU에서 각각 최대 35%와 70%의 상대적 추론 속도 향상을 달성할 수 있었습니다. 병합된 토큰의 수가 70% 이상으로 증가함에 따라 WER이 약간 증가했지만 성능은 여전히 합성곱 서브샘플링보다 상당히 우수했습니다. 나아가 실험을 장문 ASR로 확장하여 과거 발화를 현재 발화와 연결하여 맥락 정보를 제공했고 입력 음성이 길어질 때 A-ToMe가 인코더를 가속화하는 데 더욱 중요하다는 것을 보여주었습니다. 마지막으로 고정 임계값으로 학습한 모델이 추론 중에 여러 임계값에 적응할 수 있음을 발견했으며, 이는 향후 주문형 토큰 감소 방향으로 연구를 촉진할 수 있습니다. 2. 방법론 2.1. 변압기 변환기 RNN-T[10]는 인코더, 예측 네트워크 및 조인트 네트워크로 구성됩니다. Transformer 변환기[24]는 음향 특징 X에서 고수준 표현 ht를 효과적으로 추출할 수 있는 Transformer 기반[25] 인코더를 사용하여 RNN-T를 확장합니다(방정식 1). 예측 네트워크는 이전에 예측된 비어 있지 않은 심볼 Yu를 기반으로 임베딩 zu를 생성합니다(방정식 2). 피드포워드 네트워크(FFN)로 구현된 조인트 네트워크는 인코더와 예측 네트워크의 출력을 결합하고, 해당 출력은 Softmax 함수를 통해 토큰 확률로 변환됩니다(방정식 3). ht = fenc(X) Zu = = fpred(Y <u) P(k|ht, Zu) softmax(fjoint (ht, Zu)) = So ลล (3) The "blank" token is used to help the alignment between the acoustic tokens from the encoder output with the text tokens. As there are many more acoustic tokens than text tokens, without token reduction, most output symbols are blank and will be removed in the final prediction. 2.2. Adjacent token merging (a) Single module 0.Key 0.9 0.6 Score (cosine similarity) Merged index X MHSA(b) Multiple modules FFN XmergeMerge {2, 3} Layer3 4Merge {1, 2} {3, 4} Layer40ms 120ms 80ms Figure 1: (a) A-ToMe module inside a Transformer layer. (b) The adaptive frame rate is achieved by stacking multiple modules. As shown in Figure 1 (a), the proposed A-ToMe module is inserted between the multi-head self-attention (MHSA) and FFN of a Transformer layer. This module utilizes the key values used in the self-attention calculation to determine the cosine similarity score between each pair of neighboring tokens. Tokens with high similarity scores are merged by taking their average, and the upper boundary for the merge ratio per layer is 50%, which is equivalent to average pooling. Figure 1 (b) illustrates that multiple layers with the A-ToMe module can be stacked to achieve an adaptive frame rate, as merged tokens can be re-merged in subsequent layers. With n modules and the original token length of 1, the highest possible token length is 2n xl. The A-ToMe is simple and efficient, requiring no additional parameters, and it can be implemented in parallel using PyTorch's [26] built-in functions without any loops. To determine the number of tokens to merge, we employed two strategies that work for inputs of varying lengths: 1) Fixed merge threshold: Tokens with a similarity score above a predefined threshold are merged. This strategy prevents dissimilar tokens from being merged at an earlier stage of the network, minimizing the loss of information. By adjusting the threshold, the number of merged tokens can be controlled, however, it is not possible to predict the exact number of merged tokens before inference. 2) Fixed merge ratio: The similarity scores are ranked and a fixed ratio of tokens with the highest scores are merged. As the layer becomes deeper, the number of tokens decreases, leading to a corresponding decrease in the number of merged tokens. The advantage is that the number of output tokens can be pre-calculated based on the merge ratio. In Section 3.2, we demonstrate that the fixed merge ratio can also be interpreted as using a higher threshold for deeper layers. 2.3. Long-form speech encoder ASR performance improves with longer sequences as more contextual information becomes available [27, 28, 29, 30]. In this paper, we adopted a simple approach to utilize historical utterances by concatenating the acoustic features from historical utterances {✗i-n, i-n ... Xi-1} and the features ✗i of the current utterance in order before the encoder (Equation 4). While the outputs contain both historical and current embeddings, the joint network only considers Hi, the embedding corresponding to the current utterance. = [Hi—n; ...; Hi – 1; H¿] fenc (Xi-r i-n ;…; Xi−1; X]) (4) This approach increases the computational intensity and memory consumption of the encoder due to the quadratic complexity of MHSA. Therefore, A-ToMe can be more important in this case. Additionally, different merging configurations can be applied to current and historical tokens, considering that current tokens may be more crucial. For instance, we can limit merging to only historical tokens or use a higher merge ratio for historical tokens than for current tokens. 3. Experiments 3.1. Experimental setup Evaluations were performed on the LibriSpeech dataset [23], which comprises 960 hours of speech. We report the WERS on dev-clean, dev-other, test-clean, and test-other subsets. Moreover, we measured the average inference latency per utterance for the test-clean subset on GPU and CPU. For GPU latency, we used beam search with a beam size of 16 on NVIDIA Tesla V100 32GB GPU. For CPU latency, we used a single core of Intel Xeon CPU E5-2673 and employed greedy search instead of beam search for efficiency. The WERS reported were obtained using the beam search decoding method. The encoder of the Transformer transducer has a four-layer VGG-like convolutional network that reduces the frame rate by a factor of four, followed by 18 Transformer layers. Each Transformer layer consists of an MHSA with an attention dimension of 512 and eight heads, and an FFN with a hidden dimension of 2048. The encoder takes as input 80-dimensional filterbank features with a frame length of 25 ms and a stride of 10 ms. The prediction network is a two-layer LSTM [31] with a hidden dimension of 1024, and the joint network has an embedding dimension of 512. 5000-wordpiece vocabulary [32] is used as the target. The whole model contains 94 million parameters, with the majority located in the encoder (59 million). We used a multitask loss function [33] including RNN-T, AED, and CTC losses with weights of 1, 0.3, and 0.3 respectively. The model was trained from scratch for 400,000 steps with AdamW [34] optimizer. Specaug [35] was adopted for better generalizations. The Transformer encoder incorporates A-ToMe every three layers, specifically at layers 2, 5, 8, 11, 14, and 17. In addition to presenting results for the un-merged model, we also report the outcomes of a traditional subsampling technique, achieved Method baseline subsampling×Merged Tokens (%) Token Length (ms) Table 1: The comparison between different merging configurations. The average of merged tokens, token length, and latency/speed is calculated based on the test-clean subset. The ratio of merged tokens and the average token length refer to the tokens after the encoder. Latency (s)/Speed CPU GPU WER Dev clean other WER Test clean other3.66 / 1.00× 1.07/1.00× 2.57 5.2.79 6.2.99 / 1.22× 0.67 / 1.59× 2.71 6.08 2.90 6.subsampling×2.16 / 1.70× 0.50/2.16× 3.07 6.77 3.6.A-ToMe (fixed merge ratio) ratio/layer=10%2.86/1.28× ratio/layer=15%ratio/layer=20%2.43 / 1.51x 2.06 / 1.78× 0.74/1.46× 0.62 / 1.73× 0.53 / 2.04× 2.63 5.86 2.2.67 6.02 2.2.80 5.95 2.5.6.6.A-ToMe (fixed merge threshold) threshold=0.3.09 / 1.18× threshold=0.threshold=0.2.70/1.35x 2.20 / 1.66× 0.78/1.37× 0.63/1.70× 0.54 / 1.98× 2.79 5.74 2.6.2.66 5.78 2.2.70 5.97 3.5.6.by adding extra convolutional layers to the VGG-like downsampler, as a baseline comparison. In the long-form ASR experiments, only a small batch size can be used which leads to slow convergence if the transducer is trained from scratch. Hence, we fine-tuned the utterance-based models for 200,000 steps with history utterances. 3.2. Utterance-based ASR results Cosine Similarity Average Threshold Percentage (%) Fixed merge ratio/layer Cosine Similarity Fixed merge threshold +0.90 +0.85 +0.10% 15% 20% 0.0.(a) (b) 0.0.0.0.0.0.0.0.0.0.0.7Layer 0.0.(c) 0.0.0.Merge Ratio (%) 0.(d)Layer(e) (f)2654322 °120 160 200 > 출력 토큰의 길이(ms) 백분율(%)10 13계층8계층80 120 160 200 &gt;출력 토큰의 길이(ms) 그림 2: 테스트-클린 하위 집합에 대한 더 나은 이해를 위해 다양한 병합 구성을 시각화합니다.(a, b) 얕은 계층에서 깊은 계층으로 인접 토큰 간의 코사인 유사도가 변경되었습니다.(c) 고정 병합 비율을 적용할 때 다양한 계층에서의 평균 임계값입니다.(d) 고정 병합 임계값을 사용할 때 다양한 계층에서의 평균 병합 비율입니다.(e, f) 백분율로 나타낸 인코더 출력 토큰 길이의 분포입니다.표 1에서 볼 수 있듯이, 합성곱 서브샘플링은 특히 devother 및 test-other 하위 집합에서 WER이 상당히 증가하는 결과를 가져왔습니다.예를 들어, x2 및 ×4의 서브샘플링 속도가 사용되었을 때 test-other 하위 집합에서 WER이 각각 5% 및 14% 상대적으로 저하되었습니다. 고정된 병합 비율은 병합된 토큰 수가 증가함에 따라 WER이 약간 증가했지만 그 영향은 합성곱 서브샘플링보다 훨씬 작았습니다.전체 병합 토큰이 73%(서브샘플링×4와 비교 가능)에 도달했을 때 테스트-기타의 WER은 상대적으로 3%만 증가했습니다.또한 계층당 병합 비율이 10%이고 전체 병합 토큰이 46%일 때 기준선과 비교하여 눈에 띄는 저하가 없었습니다.속도 측면에서 A-ToMe는 인코더를 가속화하고 디코딩에서 전진 단계를 줄임으로써 기준선과 비교하여 훨씬 낮은 E2E 대기 시간에 기여했습니다.계층당 병합 비율을 10%~20%로 사용할 때 CPU 속도가 1.28~1.78배 빨라졌습니다.작업이 병렬화되지 않았기 때문에 CPU에 대한 평가는 계산을 직접 반영했습니다. GPU 성능의 경우, A-ToMe는 디코딩 중 반복적인 포워드 단계가 병목 현상이고 토큰 수에 따라 증가하고 병렬화하기 어렵기 때문에 1.46~2.01배 더 빠른 속도로 더욱 효과적이었습니다. 고정 비율 병합과 비교할 때, 병합된 토큰 수가 많을 때 고정 병합 임계값을 사용하면 성능이 약간 더 좋았습니다. 예를 들어, 임계값 0.85를 사용할 때 약 57%의 토큰이 무시할 수 있는 성능 저하로 병합되었습니다. 그러나 임계값 0.9의 성능은 만족스럽지 않았고 하위 계층에서 병합된 토큰이 적었기 때문에 CPU에서의 속도 향상이 더 제한적이었습니다. 그림 2는 AToMe를 더 잘 이해하기 위한 시각화를 제공합니다. 그림 2(a, b)의 점선에서 볼 수 있듯이 병합하지 않으면 인접 토큰 간의 코사인 유사도는 계층이 깊어질수록 계속 증가하여 상당한 중복 정보를 나타냅니다. 고정 병합 비율을 사용하면 코사인 유사도가 네트워크 전체에서 비교적 낮은 수준으로 유지되는 반면, 고정 병합 임계값을 사용하면 코사인 유사도가 대부분 깊은 계층에서 감소했습니다.이는 낮은 계층에서 유사도 점수가 임계값 아래에 있어 병합된 토큰이 적었기 때문입니다.그림 2(d)에서 임계값이 0과 0.90일 때 대부분의 토큰이 계층 14와 17에서 병합되었음을 알 수 있습니다.0.8의 낮은 임계값의 경우 계층 8과 같은 얕은 계층에서 더 많은 토큰을 병합할 수 있습니다.또한 고정 병합 비율을 적용하는 것은 얕은 계층에 대해 낮은 임계값을 사용하는 것과 유사함을 의미합니다.표 2: A-ToMe를 활용한 장문 ASR.&#39;병합 기록&#39;은 이전 발화만 병합하는 것을 나타내는 반면, &#39;모두 병합&#39;은 이전과 현재 발화를 모두 병합하는 것을 나타냅니다.기록 병합 및 기록 토큰. 레이어당 20%의 병합 비율을 과거 토큰에 사용하면 현재 토큰에 사용하는 것보다 WERS에 미치는 영향이 작다는 점에 주목할 가치가 있습니다.그림 3은 CPU에서 E2E 지연 시간을 비교한 것입니다.과거 발언의 수가 0에서 2로 증가함에 따라 A-ToMe를 사용하지 않았을 때 지연 시간이 3.66초에서 10.26초로 크게 증가했습니다.인코더 지연 시간이 주로 영향을 받았지만, 나머지 모델은 인코더 다음에 기록 토큰이 제거되었기 때문에 영향을 덜 받았습니다.또한 A-ToMe의 속도 이득은 시퀀스가 길어질수록 향상되며, 주로 인코더 뒤의 계산에 이로운 것에서 인코더 자체에 이로운 것으로 전환됩니다.WER Dev clean other| 2.5.2.2.5.history 2.5.all 2.5.WER Test clean other 6.2.61 5.2.69 5.2.74 5.2.35 5.2.5.3.4. 다양한 임계값 기록을 사용한 주문형 추론 2.5.2.5.all 2.5.2.5.E2E 지연 CPU(초)Enc Latencybaseline 병합 기록 모두 병합 3.663.2.6.5.4.10.7.6.WER(%) 6.6.€ 6.5.5.(a) 학습 임계값 기준 = 0.=평가 임계값 0.900 0.875 0.850 0.825. 0.평가 임계값 병합된 토큰(%)(b)학습 임계값 0.=평가 임계값 0.900 0.875 0.850 0.825 0.평가 임계값 기록=기록=기록=그림 3: 다양한 수의 기록 발언에 따른 CPU의 E2E 지연. 점선 아래의 영역은 인코더가 소요한 지속 시간을 나타내는 반면, 점선 위의 영역은 디코딩 프로세스 동안 예측 및 조인트 네트워크가 소모한 시간을 나타냅니다. 레이어(그림 2(c)). 그림 2(e, f)는 A-ToMe에서 달성한 적응형 토큰 길이를 보여줍니다. 임계값 0.8과 같은 공격적인 병합 구성의 경우 토큰의 20% 이상이 200ms를 넘는 길이를 가졌습니다. 레이어당 10%와 같은 낮은 병합 비율의 경우 토큰의 50% 이상이 40ms에서 변경되지 않았습니다. 이러한 시각화는 접근 방식의 두 가지 주요 장점을 강조합니다. 1) 80ms 및 160ms와 같은 고정 길이 대신 가변 토큰 길이, 2) 한 번에 모두 하위 샘플링하는 대신 점진적인 하위 샘플링. 3.3. 장형 ASR 결과 장형 ASR 실험에서 우리는 더 최신 정보를 보존하면서 히스토리 토큰을 상당히 병합하는 두 가지 구성을 조사했습니다. 첫 번째 구성은 레이어당 20%의 고정 병합 비율로 과거 토큰만 병합하는 것을 포함합니다.두 번째 구성은 레이어당 10% 비율로 현재 토큰을 병합하고 레이어당 20% 비율로 과거 토큰을 병합하는 것을 포함합니다.표 2에서 볼 수 있듯이 컨텍스트가 더 많이 추가됨에 따라 ASR 성능이 향상되었습니다.병합하지 않은 경우 두 개의 과거 발화를 사용했을 때 테스트-기타의 WER은 6.01%에서 5.38%로 감소했습니다.과거 발화가 하나뿐인 경우 두 병합 구성의 WERS가 비슷했으며 병합되지 않은 모델의 결과와 비슷했습니다.과거 발화가 두 개일 때 A-ToMe는 성능에 약간의 영향을 미쳤으며 과거 토큰만 병합하면 현재 두 개를 병합하는 것보다 약간 더 나은 결과를 얻었습니다.그림 4: (a) 추론 중에 서로 다른 임계값을 적용하는 것과 WER(테스트-기타)에 대한 학습을 비교한 영향.(b) 임계값 설정이 다른 병합된 토큰의 비율. 주문형 컴퓨팅 감소[36, 37]는 재교육 없이 추론 시 다양한 컴퓨팅 요구 사항에 맞게 조정할 수 있는 모델을 학습하는 것을 포함합니다.우리는 AToMe의 주문형 기능을 조사하기 위해 예비 실험을 수행했습니다.그림 4(a)는 고정된 임계값 0.85로 학습되었지만 모델이 다른 임계값으로 평가되었을 때의 테스트-다른 WERS를 보여줍니다.그림 4(b)는 임계값이 조정되었을 때의 병합된 토큰의 백분율을 보여줍니다.임계값을 수정하면 특히 높은 임계값에서 우수한 성능을 유지하면서 추론 중 병합된 토큰의 수를 제어할 수 있습니다.그러나 0.8과 같은 낮은 임계값에서는 성능이 좋지 않다는 것을 관찰했습니다.또한 임계값이 0.8~0.9 사이인 주문형 설정을 사용할 때 병합된 토큰의 백분율은 동일한 임계값을 학습 및 평가에 사용한 기존 설정보다 범위가 좁았습니다.4.
--- CONCLUSION ---
이 논문에서 우리는 Transformer 변환기의 인코더에서 토큰의 수를 점진적으로 줄이는 Adjacent Token Merging이라는 새로운 적응적 서브샘플링 방법을 제안했습니다. 우리는 가변 프레임 속도와 점진적 서브샘플링의 중요성을 강조했습니다. 발화 기반 및 장형 ASR에 대한 실험은 우리의 접근 방식이 인식 성능에 미치는 영향을 최소화하면서 추론을 상당히 가속화할 수 있음을 보여주었습니다. 또한, 우리의 접근 방식은 효율적인 ASR 모델과 주문형 신경망을 설계하는 데 더 많은 유연성을 제공할 수 있어 향후 연구를 용이하게 할 것입니다. 앞으로 우리는 보다 정교한 병합 전략을 조사할 계획이며, 우리의 접근 방식을 스트리밍 ASR에 적용할 것입니다. 5. 참고문헌 [1] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-end speech awareness using deep RNN models and WFST-based decoding,” in Proc. ASRU, 2015, pp. 167–174. [2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attention and spell: A neural network for large vocabulary conversational speechrecognition,” in Proc. ICASSP, 2016, pp. 4960-4964. [3] S. Watanabe, T. Hori, S. Kim, JR Hershey, and T. Hayashi, “Hybrid CTC/attention architecture for end-to-end speechrecognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240-1253, 2017. [4] Y. He, TN Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao, D. Rybach et al., “Streaming end-to-end speechrecognition,” in Proc. ICASSP, 2019, pp. 6381-6385. [5] J. Li, R. Zhao, H. Hu, 및 Y. Gong, &quot;종단간 음성 인식을 위한 RNN 변환기 모델링 개선&quot;, Proc. ASRU, 2019. [6] G. Saon, Z. Tüske, D. Bolanos, 및 B. Kingsbury, &quot;음성 인식을 위한 RNN 변환기 기술 발전&quot;, Proc. ICASSP, 2021, 5654-5658쪽. [7] J. Li, &quot;종단간 자동 음성 인식의 최근 발전&quot;, APSIPA 신호 및 정보 처리 저널, 제11권, 제1호. 1, 2022. [8] A. Graves, S. Fernández, F. Gomez, 및 J. Schmidhuber, &quot;연결주의 시간 분류: 순환 신경망을 사용한 분할되지 않은 시퀀스 데이터 라벨링&quot;, 미국 펜실베이니아주 피츠버그에서 열린 Proc. ICML, 2006년 6월, 369-376쪽. [9] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, 및 Y. Bengio, &quot;음성 인식을 위한 주의 기반 모델&quot;, 캐나다 몬트리올에서 열린 Proc. NeurIPS, 2015년 12월, 577-585쪽. [10] A. Graves, &quot;순환 신경망을 사용한 시퀀스 변환&quot;, 미국 Proc. ICML, 스코틀랜드 에든버러, 2012년 6월. [11] S. Goyal, AR Choudhury, S. Raje, V. Chakaravarthy, Y. Sabharwal, A. Verma, &quot;Power-bert: 점진적 단어 벡터 제거를 통한 bert 추론 가속화&quot;, Proc. ICML, 오스트리아 비엔나, 2020년 7월, 3690-3699쪽. [12] G. Kim, K. Cho, &quot;길이 적응형 변환기: 길이 감소로 한 번 학습하고 검색으로 언제든지 사용&quot;, Proc. ACLIJCNLP, 2021년 7월, 6501-6511쪽. [13] H. Wang, Z. Zhang, S. Han, &quot;Spatten: 캐스케이드 토큰 및 헤드 프루닝을 사용한 효율적인 희소 어텐션 아키텍처&quot;, Proc. HPCA, 2021년 2월, pp. 97–110. [14] S. Kim, S. Shen, D. Thorsley, A. Gholami, W. Kwon, J. Hassoun, and K. Keutzer, &quot;변압기를 위한 학습된 토큰 가지치기,&quot; Proc. KDD, 워싱턴 DC, 미국, 2022년 8월, 784–794쪽. [15] M. Burchi and V. Vielzeuf, &quot;효율적인 컨포머: 자동 음성 인식을 위한 점진적 다운샘플링 및 그룹화된 주의,&quot; Proc. ASRU, 카르타헤나, 콜롬비아, 2021년 12월, 815쪽. [16] W. Huang, W. Hu, YT Yeung, and X. Chen, &quot;변환-변압기 변환기: 저지연, 낮은 프레임 속도, 스트리밍 가능한 엔드투엔드 음성 인식,&quot; Proc. Interspeech, 상하이, 중국, 2020년 10월, 5001-5005쪽. [17] S. Kim, A. Gholami, AE Shaw, N. Lee, K. Mangalam, J. Malik, MW Mahoney, K. Keutzer, &quot;Squeezeformer: 자동 음성 인식을 위한 효율적인 변환기&quot;, Proc. NeurIPS, 미국 루이지애나주 뉴올리언스, 2022년 11월. [18] Y. Meng, H.-J. Chen, J. Shi, S. Watanabe, P. Garcia, H.-Y. Lee, H. Tang, &quot;자체 감독 음성 모델을 위한 시퀀스 압축에 관하여&quot;, Proc. SLT, 도하, 카타르, 2023년 1월, 1128-1135쪽. [19] L. Dong 및 B. Xu, &quot;Cif: 엔드투엔드 음성 인식을 위한 연속적 통합 및 발사&quot;, 스페인 바르셀로나에서 열린 Proc. ICASSP, 2020년 5월, 6079-6083쪽. [20] H.-J. Chang, S.-w. Yang 및 H.-y. Lee, &quot;Distilhubert: 숨겨진 단위 bert의 계층별 증류를 통한 음성 표현 학습&quot;, 싱가포르에서 열린 Proc. ICASSP, 2022년 5월, 7087-7091쪽. [21] S. Cuervo, A. Lancucki, R. Marxer, P. Rychlikowski, 및 JK Chorowski, &quot;가변 비율 계층적 cpc는 음성에서 음향 단위 발견으로 이어짐&quot;, Proc. NeurIPS, 루이지애나주 뉴올리언스, 미국, 2022년 11월. [22] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, 및 J. Hoffman, &quot;토큰 병합: ViT가 더 빠르다&quot;, Proc. ICLR, 키갈리, 르완다, 2023년 5월. [23] V. Panayotov, G. Chen, D. Povey 및 S. Khudanpur, “Librispeech: 공개 도메인 오디오 북을 기반으로 한 ASR 코퍼스,&quot; Proc. ICASSP, South Brisbane, Queensland, Australia, 2015년 4월, pp. 5206-5210. [24] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo 및 S. Kumar, &quot;변압기 변환기: 변환기 인코더 및 RNN-T 손실을 갖춘 스트리밍 가능한 음성 인식 모델&quot;, Proc. ICASSP, 스페인 바르셀로나, 2020년 5월, 페이지 7829-7833 [25] A. Vaswani, N. Shazeer, N. Parmar, J. 우슈코레이트, L. 영어: Jones, AN Gomez, Ł. Kaiser, 및 I. Polosukhin, &quot;Attention is all you need,&quot; Proc. NeurIPS, 캘리포니아주 롱비치, 미국, 2017년 12월. [26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., &quot;Pytorch: 필수 스타일, 고성능 딥 러닝 라이브러리,&quot; Proc. NeurIPS, 캐나다 밴쿠버, 2019년 12월, 8026-8037쪽. [27] A. Narayanan, R. Prabhavalkar, C.-C. Chiu, D. Rybach, TN Sainath, 및 T. Strohman, &quot;스트리밍 엔드투엔드 모델을 사용하여 장문 음성 인식,&quot; Proc. 영어: ASRU, 싱가포르 센토사, 2019년 12월, 920-927쪽. [28] A. Schwarz, I. Sklyar, 및 S. Wiesler, &quot;컨텍스트 오디오를 사용하여 RNN-T ASR 정확도 개선&quot;, Proc. Interspeech, 체코 브르노, 2021년 9월. [29] T. Hori, N. Moritz, C. Hori, 및 JL Roux, &quot;컨텍스트 확장 변환기를 사용한 고급 롱컨텍스트 엔드투엔드 음성 인식&quot;, Proc. Interspeech, 체코 브르노, 2021년 9월. [30] R. Masumura, N. Makishima, M. Ihori, A. Takashima, T. Tanaka, S. Orihashi, &quot;대규모 컨텍스트 지식 증류를 갖춘 계층적 변환기 기반 대규모 컨텍스트 엔드투엔드 ASR,&quot; Proc. ICASSP, 캐나다 토론토, 2021년 6월, 5879-5883쪽. [31] S. Hochreiter와 J. Schmidhuber, &quot;장단기 메모리,&quot; 신경 계산, 제9권, 제8호, 1735-1780쪽, 1997년. [32] T. Kudo와 J. Richardson, &quot;문장 조각: 신경 텍스트 처리를 위한 간단하고 언어에 독립적인 하위 단어 토크나이저 및 디토크나이저,&quot; Proc. EMNLP, 벨기에 브뤼셀, 2018년 10월, 66쪽. [33] J.-J. Jeon 및 E. Kim, &quot;Transformer-RNN-Transducer 음성 인식을 위한 멀티태스크 학습 및 공동 최적화&quot;, Proc. ICASSP, 캐나다 토론토, 2021년 6월, 6793-6797쪽. [34] I. Loshchilov 및 F. Hutter, &quot;분리된 가중치 감소 정규화&quot;, Proc. ICLR, 미국 루이지애나주 뉴올리언스, 2019년 5월. [35] DS Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, ED Cubuk 및 QV Le, &quot;Specaugment: 자동 음성 인식을 위한 간단한 데이터 증가 방법&quot;, Proc. Interspeech, 오스트리아 그라츠, 2019년 9월, 2613-2617쪽. [36] H. Cai, C. Gan, T. Wang, Z. Zhang 및 S. Han, &quot;한 번에 모든 것: 하나의 네트워크를 훈련하고 효율적인 배포를 위해 특화하세요.&quot; Proc. ICLR, 아디스아바바, 에티오피아, 2020년 4월. [37] A. Vyas, W.-N. Hsu, M. Auli 및 A. Baevski, &quot;확률적 wav2vec 2.0을 통한 주문형 컴퓨팅 감소&quot;, Proc. Interspeech, 2022년 9월.
