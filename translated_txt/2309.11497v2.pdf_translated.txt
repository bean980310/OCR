--- ABSTRACT ---
이 논문에서 우리는 &quot;무료 점심&quot; 역할을 하는 확산 U-Net의 미개척 잠재력을 발견합니다. 이는 즉석에서 생성 품질을 크게 개선합니다. 우리는 처음에 U-Net 아키텍처가 노이즈 제거 프로세스에 미치는 주요 기여를 조사하고, 주요 백본이 주로 노이즈 제거에 기여하는 반면, 스킵 연결은 주로 디코더 모듈에 고주파 기능을 도입하여 네트워크가 백본 의미론을 간과하게 한다는 것을 확인합니다. 이 발견을 활용하여 추가 학습이나 미세 조정 없이도 생성 품질을 개선하는 간단하면서도 효과적인 방법인 &quot;FreeU&quot;를 제안합니다. 우리의 주요 통찰력은 U-Net의 스킵 연결과 백본 기능 맵에서 얻은 기여를 전략적으로 다시 가중치를 두어 U-Net 아키텍처의 두 구성 요소의 강점을 활용하는 것입니다. 이미지 및 비디오 생성 작업에서 유망한 결과는 FreeU가 기존 확산 모델(예: Stable Diffusion, DreamBooth, ModelScope, Rerender 및 ReVersion)에 쉽게 통합되어 몇 줄의 코드만으로 생성 품질을 개선할 수 있음을 보여줍니다. 추론 중에 두 개의 스케일링 요인만 조정하면 됩니다. 프로젝트 페이지: https://chenyangsi.top/FreeU/. 1.
--- METHOD ---
비용 없이 확산 모델 샘플 품질을 크게 개선합니다. 훈련이 없고, 추가 매개변수가 도입되지 않으며, 메모리나 샘플링 시간이 증가하지 않습니다. 초록 이 논문에서 우리는 &quot;무료 점심&quot; 역할을 하는 확산 U-Net의 미개척 잠재력을 발견합니다. 즉석에서 생성 품질을 크게 개선합니다. 우리는 처음에 U-Net 아키텍처가 노이즈 제거 프로세스에 미치는 주요 기여를 조사하고, 주요 백본이 주로 노이즈 제거에 기여하는 반면, 스킵 연결은 주로 디코더 모듈에 고주파 기능을 도입하여 네트워크가 백본 의미론을 간과하게 한다는 것을 확인합니다. 이 발견을 활용하여 추가 훈련이나 미세 조정 없이 생성 품질을 개선하는 간단하면서도 효과적인 방법인 &quot;FreeU&quot;를 제안합니다. 우리의 주요 통찰력은 U-Net의 스킵 연결과 백본 기능 맵에서 얻은 기여를 전략적으로 다시 가중치를 두어 U-Net 아키텍처의 두 구성 요소의 강점을 활용하는 것입니다. 이미지 및 비디오 생성 작업에서 유망한 결과는 FreeU가 기존 확산 모델(예: Stable Diffusion, DreamBooth, ModelScope, Rerender 및 ReVersion)에 쉽게 통합되어 몇 줄의 코드만으로 생성 품질을 개선할 수 있음을 보여줍니다. 추론 중에 두 개의 스케일링 요인을 조정하기만 하면 됩니다. 프로젝트 페이지: https://chenyangsi.top/FreeU/. 1. 소개 생성 모델의 최첨단 범주인 확산 확률적 모델은 특히 컴퓨터 비전과 관련된 작업에서 연구 분야의 초점이 되었습니다[5, 6, 8, 10, 12, 20, 22, 26, 28, 29, 32]. 변분 자동 인코더(VAE) [21], 생성적 적대 네트워크(GAN) [3, 9, 16-19, 25] 및 벡터 양자화 접근 방식 [7, 34]과 같은 다른 생성 모델 클래스 [3, 7, 9, 16–19, 21, 25, 34, 35]와 달리 확산 모델은 새로운 생성 패러다임을 도입합니다. 이러한 모델은 고정 마르코프 체인을 사용하여 잠재 공간을 매핑하여 데이터 세트 내의 잠재 구조적 복잡성을 포착하는 복잡한 매핑을 용이하게 합니다. 최근, 높은 수준의 세부 정보부터 생성된 예제의 다양성에 이르기까지 인상적인 생성 기능은 이미지 합성[12, 29, 32], 이미지 편집[1, 4, 14, 24], 이미지 대 이미지 변환[4, 31, 36], 텍스트 대 비디오 생성[2, 11, 13, 23, 33, 37, 38, 40]과 같은 다양한 컴퓨터 비전 애플리케이션에서 획기적인 발전을 이루었습니다. 확산 모델은 확산 프로세스와 잡음 제거 프로세스로 구성됩니다. 확산 프로세스 동안 가우시안 잡음이 입력 데이터에 점차적으로 추가되고 결국 거의 순수한 가우시안 잡음으로 손상됩니다. 잡음 제거 프로세스 동안 원래 입력 데이터버거를 먹는 다람쥐 0.StepGenerated image 저주파 A 로그 진폭 0.-0.-1.0-1.StepStepStepStepStepStepStep-2.Ο.Οπ 0.2π 0.4π 0.6π 0.8Ο 1.Οπ 고주파 주파수 그림 2. 노이즈 제거 프로세스. 맨 위 행은 반복에 걸친 이미지의 진행 상황을 보여 주고, 그 뒤의 두 행은 백본 스케일링 계수 b의 진행 상황을 보여 줍니다. b에서 역 푸리에 변환 후에 저주파 및 고주파 성분을 증가시키면 고주파가 억제되어 각 단계와 일치합니다. 저주파 성분은 디노이즈 프로세스 중에 difslowly에 의해 생성된 이미지에서 주파수 성분을 변경하는 반면, 고주파 성분은 더 큰 변화를 보입니다. 일반적으로 U-Net은 각 노이즈 제거 단계에서 제거할 노이즈를 반복적으로 예측하도록 훈련됩니다. 기존 연구는 다운스트림 애플리케이션에 사전 훈련된 확산 U-Net을 활용하는 데 중점을 두고 있는 반면, 확산 U-Net의 내부 속성은 여전히 크게 탐구되지 않았습니다. 이 논문에서는 확산 모델의 적용을 넘어 노이즈 제거 프로세스에 대한 확산 U-Net의 효과를 조사하는 데 관심이 있습니다. 노이즈 제거 프로세스를 더 잘 이해하기 위해 먼저 확산 모델의 생성된 프로세스를 전망하기 위해 푸리에 영역으로의 패러다임 전환을 제시합니다. 이 연구 분야는 사전 조사가 제한적이었습니다. 그림 2에서 볼 수 있듯이 가장 위 행은 점진적 노이즈 제거 프로세스를 제공하여 연속적인 반복에서 생성된 이미지를 보여줍니다. 그 뒤의 두 행은 각 단계에 맞춰 역 푸리에 변환 후 연관된 저주파 및 고주파 공간 영역 정보를 보여줍니다. 그림 2에서 알 수 있듯이 저주파 성분은 점진적으로 변조되어 변화 속도가 둔화되고, 고주파 성분은 노이즈 제거 과정 전반에 걸쳐 더욱 두드러진 역동성을 보입니다. 이러한 결과는 그림 3에서 더욱 입증됩니다. 이는 직관적으로 설명할 수 있습니다. 1) 저주파 성분은 본질적으로 이미지의 글로벌 구조와 특성을 구현하여 글로벌 레이아웃과 부드러운 색상을 포함합니다. 이러한 성분은 이미지의 본질과 표현을 구성하는 기본적인 글로벌 요소를 캡슐화합니다. 노이즈 제거 과정에서 빠른 변화는 일반적으로 비합리적입니다. 이러한 성분을 급격하게 변경하면 이미지의 본질이 근본적으로 바뀔 수 있으며, 이는 일반적으로 노이즈 제거 과정의 목표와 양립할 수 없는 결과입니다. 2) 반대로 고주파 성분은 가장자리와 질감과 같은 이미지의 빠른 변화를 포함합니다. 이러한 미세한 세부 사항은 노이즈에 현저히 민감하여 노이즈가 이미지에 도입되면 종종 무작위 고주파 정보로 나타납니다. 결과적으로, 노이즈 제거 프로세스는 필수적인 복잡한 세부 사항을 유지하면서 노이즈를 제거해야 합니다. 노이즈 제거 프로세스 동안 저주파와 고주파 구성 요소 간의 이러한 관찰에 비추어, 우리는 확산 프레임워크 내에서 U-Net 아키텍처의 특정 기여를 확인하기 위해 조사를 확대합니다. U-Net 디코더의 각 단계에서, 스킵 연결의 스킵 피처와 백본 피처가 함께 연결됩니다. 우리의 조사에 따르면 U-Net의 주요 백본은 주로 노이즈 제거에 기여합니다. 반대로, 스킵 연결은 디코더 모듈에 고주파 피처를 도입하는 것으로 관찰됩니다. 이러한 연결은 세분화된 의미 정보를 전파하여 입력 데이터를 복구하기 쉽게 만듭니다. 그러나 이 전파의 의도치 않은 결과는 추론 단계 동안 백본의 고유한 노이즈 제거 기능이 약화될 가능성이 있다는 것입니다. 이는 그림 1의 첫 번째 행에 나와 있는 것처럼 비정상적인 이미지 세부 정보가 생성될 수 있습니다. 이러한 발견을 바탕으로 &quot;FreeU&quot;라는 새로운 전략을 도입하여 추가적인 훈련이나 미세 조정의 계산 오버헤드 없이도 샘플 품질을 개선할 수 있는 잠재력을 가지고 있습니다. 추론 단계에서는 U-Net 아키텍처의 기본 백본과 스킵 연결에서 피처 기여도를 균형 잡도록 설계된 두 가지 특수 변조 요소를 인스턴스화합니다. 백본 피처 요소라고 하는 첫 번째 요소는 기본 백본의 피처 맵을 증폭하여 노이즈 제거 프로세스를 강화하는 것을 목표로 합니다. 그러나 백본 피처 스케일링 요소를 포함하면 상당한 개선이 이루어지지만 가끔은 텍스처의 바람직하지 않은 과도한 평활화가 발생할 수 있습니다. 이 문제를 완화하기 위해 두 번째 요소인 스킵 피처 스케일링 요소를 도입하여 텍스처 과도한 평활화 문제를 완화하는 것을 목표로 합니다. FreeU 프레임워크는 기존 확산 모델과 통합될 때 원활한 적응성을 보여주며, 텍스트-이미지 생성 및 텍스트-비디오 생성과 같은 애플리케이션을 포함합니다. 우리는 포괄적인
--- EXPERIMENT ---
영어: 우리의 접근 방식에 대한 모든 평가는 벤치마크 비교를 위한 우리의 기초 모델로 Stable Diffusion [29], DreamBooth [30], ReVersion [15], ModelScope [23], 및 Rerender [39]를 사용합니다. 추론 단계에서 FreeU를 사용함으로써 이러한 모델은 생성된 출력의 품질이 눈에 띄게 향상되었음을 나타냅니다. 그림 1에 설명된 시각화는 생성된 이미지 내에서 복잡한 세부 사항과 전반적인 시각적 충실도를 모두 크게 향상시키는 FreeU의 효능을 입증합니다. 우리의 기여는 다음과 같이 요약됩니다. • 우리는 확산 모델 내에서 노이즈 제거를 위한 U-Net 아키텍처의 잠재력을 조사하고 발견하며, 그 주요 백본은 주로 노이즈 제거에 기여하는 반면, 건너뛰기 연결은 디코더 모듈에 고주파 기능을 도입한다는 것을 식별합니다. • 우리는 또한 U-Net 아키텍처의 두 구성 요소의 장점을 활용하여 U-Net의 노이즈 제거 기능을 향상시키는 &quot;FreeU&quot;로 표시되는 간단하면서도 효과적인 방법을 소개합니다. 추가 교육이나 미세 조정 없이도 생성 품질을 크게 개선합니다.• 제안된 FreeU 프레임워크는 다재다능하고 기존 확산 모델과 완벽하게 통합됩니다.우리는 다양한 확산 기반 방법에서 상당한 샘플 품질 개선을 보여주며 추가 비용 없이 FreeU의 효과를 보여줍니다.2. 방법론 2.1. 예비 Denoising Diffusion Probabilistic Models(DDPM)[12]와 같은 확산 모델은 데이터 모델링을 위한 두 가지 기본 프로세스, 즉 확산 프로세스와 노이즈 제거 프로세스를 포함합니다.확산 프로세스는 T 단계의 시퀀스로 특징지어집니다. 각 단계 t에서 가우시안 노이즈가 마르코프 체인을 통해 데이터 분포 xo~ q(x0)에 점진적으로 도입되며, 이는 ẞ₁,..., BT로 표시된 규정된 분산 일정에 따릅니다. q(xt|xt−1) = N(xt; √√1 – ßtxt−1, ßtI) (1) 노이즈 제거 프로세스는 노이즈 입력 x+가 주어진 기본 클린 데이터 xt-1에 대한 위의 확산 프로세스를 역전합니다. (2) Po(xt-1\xt) = N(xt−1; µ₁(xt, t), Σo(xt, t)) μ 및 Σ는 €0로 표시된 노이즈 제거 모델을 포함하는 추정 절차를 통해 결정됩니다. 일반적으로 이 노이즈 제거 모델은 시간 조건부 U-Net 아키텍처를 사용하여 구현됩니다. 생성된 샘플의 전반적인 충실도를 동시에 향상시키는 동시에 데이터 샘플에서 노이즈를 제거하도록 훈련됩니다. 2.2. 확산 U-Net은 어떻게 노이즈 제거를 수행할까요?그림 2와 그림 3에 나와 있는 노이즈 제거 프로세스 전반에서 저주파와 고주파 성분 간에 관찰된 눈에 띄는 차이점을 바탕으로, 우리는 노이즈 제거 프로세스 내에서 U-Net 아키텍처의 구체적인 기여를 구분하고 노이즈 제거 네트워크의 내부 속성을 탐구하기 위해 조사를 확장합니다.그림 4에서 볼 수 있듯이, U-Net 아키텍처는 인코더와 디코더를 모두 포함하는 기본 백본 네트워크와 인코더와 디코더의 해당 계층 간의 정보 전송을 용이하게 하는 스킵 연결을 포함합니다.U-Net의 백본.노이즈 제거 프로세스에서 백본과 측면 스킵 연결의 두드러진 특성을 평가하기 위해, 우리는 b와 s로 표시된 두 개의 곱셈적 스케일링 인수를 도입하여 연결 전에 백본과 스킵 연결에서 생성된 피처 맵을 변조하는 통제된 실험을 수행합니다.그림 5에서 볼 수 있듯이, 백본의 스케일 인수 b를 높이면 생성된 이미지의 품질이 현저히 향상되는 것이 분명합니다. 반대로, 측면 건너뛰기 연결의 영향을 조절하는 스케일링 계수의 변화는 생성된 이미지의 품질에 무시할 만한 영향을 미치는 것으로 보입니다. 이러한 관찰을 바탕으로, 우리는 백본 피처 맵과 관련된 스케일링 계수 b가 증가할 때 이미지 생성 품질이 향상되는 근본적인 메커니즘을 조사했습니다. 우리의 분석에 따르면 이러한 품질 향상은 근본적으로 U-Net 아키텍처의 백본에서 부여된 증폭된 노이즈 제거 기능과 연결되어 있습니다. 그림 6에서 설명한 대로, b가 비례적으로 증가하면 확산 모델에서 생성된 이미지의 고주파 성분이 억제됩니다. 이는 백본 피처를 향상시키면 U-Net 아키텍처의 노이즈 제거 기능이 효과적으로 강화되어 충실도와 세부 정보 보존 측면에서 더 우수한 출력에 기여함을 의미합니다.연결 건너뛰기 백본 피처 건너뛰기 피처 -|H|IHI 피처 건너뛰기(h) FFT IFFT 연결 건너뛰기 백본 피처(x) b (a) UNet 아키텍처 (b) FreeU 연산 그림 4. FreeU 프레임워크.(a) U-Net 피처 건너뛰기 및 백본 피처.U-Net에서 건너뛰기 피처와 백본 피처는 각 디코딩 단계에서 함께 연결됩니다.연결 중에 FreeU 연산을 적용합니다.(b) FreeU 연산.인자 b는 백본 피처 맵 x를 증폭하는 것을 목표로 하는 반면, 인자는 건너뛰기 피처 맵 h를 약화하도록 설계되었습니다. b=0.6, s=1.b=0.8, s 1.b=1.0, s=1.b=1.2, s=1.b=1.4, s=1.b=1.0, s=0.b=1.0, s=0.b=1.0, s=1.b=1.0, s=1.b=1.0, s=1.b=1.0, s=1.A 로그 진폭 0.-0.-1.-1.-1.-2.0.0.1.1.1.Ο.Οπ 0.2π 0.4π 0.6π 0.8π 1.Οπ 주파수 그림 5. 백본 및 스킵 연결 스케일링 계수(b 및 s)의 효과. 그림 6. 백본 스케일링 계수 b를 증가시키면서 푸리에의 상대적 로그 진폭은 이미지 품질을 크게 향상시키지만, 백본 스케일링 계수 b의 변화는 이미지 품질을 크게 향상시킵니다. 건너뛰기 스케일링 계수 s의 증가는 이미지 합성에 무시할 만한 영향을 미치며, b에서 ing은 확산 모델에서 생성된 이미지의 고주파 성분의 품질을 억제하는 결과를 낳습니다. U-Net의 건너뛰기 연결. 반대로, 건너뛰기 연결은 인코더 블록의 이전 계층에서 디코더로 직접 기능을 전달하는 역할을 합니다. 흥미롭게도, 그림 7에서 알 수 있듯이 이러한 기능은 주로 고주파 정보를 구성합니다. 이 관찰에 근거한 우리의 추측은 U-Net 아키텍처를 학습하는 동안 이러한 고주파 기능의 존재가 디코더 모듈 내에서 노이즈 예측으로의 수렴을 의도치 않게 촉진할 수 있다고 가정합니다. 또한, 그림 5에서 건너뛰기 기능을 변조하는 제한적인 영향은 건너뛰기 기능이 주로 디코더의 정보에 기여한다는 것을 나타냅니다. 이 현상은 추론 중에 백본의 고유한 노이즈 제거 기능의 효능을 의도치 않게 약화시킬 수 있습니다. 따라서 이 관찰은 U-Net 프레임워크의 복합 노이즈 제거 성능에서 백본과 스킵 연결이 수행하는 균형 잡힌 역할에 대한 적절한 질문을 촉발합니다.2.3. 확산 U-Net에서의 무상 점심 위의 발견을 활용하여 &quot;FreeU&quot;로 표시되는 간단하면서도 효과적인 방법을 소개하여 U-Net 아키텍처의 노이즈 제거 기능을 효과적으로 강화합니다.A 로그 진폭 백본 여기서 x1은 피처 맵 xɩ의 i번째 채널을 나타냅니다.C는 xɩ의 총 채널 수를 나타냅니다.따라서 백본 인수 맵은 다음과 같이 결정됩니다.0.-2.스킵 융합 -4.-6.Ο.Οπ 0.2π 0.4π 0.6π 0.8П 주파수 1.Οπ 그림 7. 백본, 스킵 및 융합된 피처 맵의 푸리 상대 로그 진폭. 인코더 블록의 이전 계층에서 디코더로 스킵 연결을 통해 직접 전달된 피처에는 대량의 고주파 정보가 포함됩니다.생성된 이미지 피처 맵 생성된 이미지 피처 맵 그림 8. 디코더의 두 번째 단계에서 평균 피처 맵의 시각화.U-Net 아키텍처의 두 구성 요소의 추가 학습이나 미세 조정 없이도 생성 품질을 크게 개선합니다.기술적으로 U-Net 디코더의 1번째 블록의 경우 x1은 이전 블록의 주 백본에서 백본 피처 맵을 나타내고 hɩ는 해당 스킵 연결을 통해 전파된 피처 맵을 나타냅니다.이러한 피처 맵을 변조하기 위해 두 개의 스칼라 인수를 도입합니다.xɩ의 경우 백본 피처 스케일링 인수 b₁와 hɩ의 아직 정의되지 않은 스킵 피처 스케일링 인수 sɩ입니다.특히 인수 by는 백본 피처 맵 xɩ를 증폭하는 것을 목표로 하는 반면 인수 sɩ는 스킵 피처 맵 hɩ를 감쇠하도록 설계되었습니다. 백본 피처의 경우, 구조 관련 스케일링이라는 새로운 방법을 도입합니다.이 방법은 각 샘플에 대한 백본 피처의 스케일링을 동적으로 조정합니다.동일한 채널 내의 모든 샘플이나 위치에 균일하게 적용되는 고정 스케일링 인수와 달리, 우리의 접근 방식은 샘플 피처의 특정 특성에 따라 스케일링 인수를 적응적으로 조정합니다.먼저 채널 차원을 따라 평균 피처 맵을 계산합니다.C (3) xl,i, α = (b₁ — 1) .x₁ - Min(x1) Max(1) Min(xi) +1, (4) 여기서 a는 백본 인수 맵을 나타냅니다.bɩ는 스칼라 상수입니다.그런 다음 실험적 조사를 통해 a와 곱하여 x의 모든 채널을 무차별적으로 증폭하면 합성된 결과 이미지에 지나치게 매끈한 질감이 발생한다는 것을 알아냈습니다.그 이유는 향상된 U-Net이 노이즈를 제거하는 동안 이미지의 고주파 세부 정보를 손상시키기 때문입니다. 따라서 스케일링 작업을 다음과 같이 xɩ의 반 채널로 제한합니다. x1.i x1,i xl,i, Oa, if i &lt; C/otherwise (5) 실제로 그림 8에서 설명한 대로 평균 피처 맵 ɩ에는 본질적으로 귀중한 구조적 정보가 포함되어 있습니다. 결과적으로 백본 인자 맵 oɩ는 백본 피처 맵 xɩ를 구조적 특성과 일치하는 방식으로 증폭하는 데 도움이 됩니다. 이 전략적 접근 방식은 과도한 평활화 문제를 완화하는 데 도움이 됩니다. 중요한 점은 이 전략이 이중의 이점을 제공한다는 것입니다. 첫째, 백본 피처 맵의 노이즈 제거 기능을 향상시켜 노이즈를 보다 효과적으로 필터링할 수 있습니다. 둘째, 전체 피처 맵에 무차별적으로 스케일링을 적용하는 것과 관련된 부정적인 영향을 피하여 노이즈 감소와 텍스처 보존 간에 보다 미묘한 균형을 이룹니다. 노이즈 제거를 강화하여 과도하게 매끈해진 텍스처 문제를 완화하기 위해, 푸리에 영역에서 스펙트럼 변조를 추가로 사용하여 건너뛰기 피처에 대한 저주파 성분을 선택적으로 감소시킵니다. 수학적으로 이 연산은 다음과 같이 수행됩니다. F(hi) = FFT(hı,i) F&#39; (hi) = F(hı,i) © ẞı,i (6) (7) (8) h₁ = IFFT(F&#39; (hı,i)) i 여기서 FFT() 및 IFFT(·)는 푸리에 변환과 역푸리에 변환입니다. 는 요소별 곱셈을 나타내고, B는 푸리에 계수의 크기의 함수로 설계된 푸리에 마스크로, 주파수 종속 스케일링 인자 sɩ를 구현하는 데 사용됩니다. B₁₁i (r)ifr <thresh, otherwise. (9) where r is the radius. thresh is the threshold frequency. Then, the augmented skip feature map hy is then concatenated with the modified backbone feature map x for subsequent layers in the U-Net architecture, as shown in Fig. 4.SD SD + FreeU SD SD + FreeU SD SD + FreeU a blue car is being filmed Mother rabbit is raising baby rabbits A bridge is depicted in the water a baby in a red shirt a attacks an upset cat and is then chased off A teddy bear walking in the snowstorm A cat riding a motorcycle. | A panda standing on a surfboard in the ocean | A boy is playing pokemon Figure 9. Samples generated by Stable Diffusion [29] with or without FreeU. Remarkably, the proposed FreeU framework does not require any task-specific training or fine-tuning. Adding the backbone and skip scaling factors can be easily done with just a few lines of code. Essentially, the parameters of the architecture can be adaptively re-weighted during the inference phase, which allows for a more flexible and potent denoising operation without adding any computational burden. This makes FreeU a highly practical solution that can be seamlessly integrated into existing diffusion models to improve their performance. 3. Experiments 3.1. Implementation details To assess the effectiveness of the proposed FreeU, we systematically conduct a series of experiments, aligning our benchmarks with state-of-the-art methods such as Stable Diffusion [29], DreamBooth [30], ModelScope [23], and Rerender [39]. Importantly, our approach seamlessly integrates with these established methods without imposing any additional computational overhead associated with supplementary training or fine-tuning. We meticulously adhere to the prescribed settings of these methods and exclusively introduce the backbone feature factors and skip feature factors during the inference. 3.2. Text-to-image Stable Diffusion [29] is a latent text-to-image diffusion model renowned for its capability to generate photorealistic images based on textual input. It has consistently demonstrated exceptional performance in various image synthesis tasks. With the integration of our FreeU augmentation into Stable Diffusion, the results, as exemplified in Fig. 9, exhibit a notable enhancement in the model's generative capacity. To elaborate, the incorporation of FreeU into Stable Diffusion [29] yields improvements in both entity portrayal and fine-grained details. For instance, when provided with the prompt "a blue car is being filmed", FreeU refines the image, eliminating rooftop irregularities and enhancing the textural intricacies of the surrounding structures. In the case of "Mother rabbit is raising baby rabbits", FreeU ensures that the generated image portrays a mother rabbit in a normal appearance caring for baby rabbits. Furthermore, In scenarios like "a attacks an upset cat and is then chased off" and "A teddy bear walking in the snowstorm", FreeU helps generate more realistically posed cats and teddy bears. Impressively, in response to the complex prompt "A cat riding a motorcycle", FreeU not only accurately renders the individual entities but also expertly captures the nu-SDXL SDXL+ FreeU SDXL SDXL + FreeU SDXL SDXL+FreeU Figure 10. Samples generated by Stable Diffusion-XL [27] with or without FreeU. anced relationship between them, ensuring that the cat is actively engaged in riding. In Figure 10, we present the generated images based on the SDXL framework [27]. It becomes evident that our proposed FreeU consistently excels in generating realistic images, especially in detail generation. These compelling results serve as a testament to the substantial qualitative enhancements engendered by the synergy of FreeU with the SD[29] or SDXL[27] frameworks. parQuantitative evaluation. We conduct a study withticipants to assess image quality and image-text alignment. Each participant receives a text prompt and two corresponding synthesized images, one from SD and another from SD+FreeU. To ensure fairness, we use the same randomly sampled random seed for generating both images. The image sequence is randomized to eliminate any bias. Participants then select the image they consider superior for image-text alignment and image quality, respectively. We tabulate the votes for SD and SD+FreeU in each category in Table 1. Our analysis reveals that the majority of votes go to SD+FreeU, indicating that FreeU significantly enhances the Stable Diffusion text-to-image model in both evaluated aspects. 3.3. Text-to-video ModelScope [23], an avant-garde text-to-video diffusion model, stands at the forefront of video generation from textual descriptions. The infusion of our FreeU augmentation Table 1. Text-to-Image Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Image-Text refers to Image-Text Alignment. Method SD [29] SD+FreeU Image-Text 14.12% 85.88% Image Quality 14.66% 85.34% Table 2. Text-to-Video Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Video-Text refers to Video-Text Alignment. Method ModelScope [23] ModelScope+FreeU Video-Text 15.29% 84.71% Video Quality 14.33% 85.67% into ModelScope [23] serves to further hone its video synthesis prowess, as substantiated by Fig. 11. For instance, when presented with the prompt "A cinematic view of the ocean, from a cave", FreeU enables ModelScope [23] to generate the perspective “from a cave”, enriching the visual narrative. In the case of "A cartoon of an elephant walking", ModelScope [23] initially generates an elephant with two trunks, but with the incorporation of FreeU, it rectifies this anomaly and produces a correct depiction of an elephant in motion. Moreover, in response to the prompt "An astronaut flying in space", ModelScope [23], with the assistance of FreeU, can generate a clear and vivid portrayal of an astronaut floating in the expanse of outer space.ModelScope ModelScope she sh Folterstock shutterstock nuit Stock A cinematic view of the ocean, from a cave. A cartoon of an elephant walking. ModelScope shutte Shutters MICH UNITY shutterst An astronaut flying in space. Figure 11. Samples generated by ModelScope [23] with or without FreeU. These results underscore the significant improvements achieved through the synergistic application of FreeU with ModelScope [23], resulting in high-quality generated content characterized by clear motion, rich detail, and semantic alignment. Quantitative evaluation. We conduct the quantitative evaluation for FreeU on the text-to-video task in a similar way as text-to-image. The results displayed in Table 2 indicate that most participants prefer the video generated with FreeU. 3.4. Downstream tasks FreeU presents substantial enhancements in the quality of synthesized samples across various diffusion model applications. Our evaluations extend from foundational image and video synthesis models to more specialized downstreamDreamBooth DreamBooth + FreeU Input images M a photo of action figure riding a motorcycle A toy on a beach Figure 12. Samples generated by DreamBooth [30] with or without FreeU. ReVersion ReVersion+FreeU ReVersion ReVersion+FreeU Rerender Rerender+FreeU A dog wearing sunglasses Figure 14. Samples generated by Rerender [39] with or without FreeU. child <R>어린이<R> = &quot;개와 등을 맞대고 앉다&quot;<R> 바구니<R> = &quot;스파이더맨 안에 들어있습니다&quot;<R> 바구니<R> = &quot;고양이 안에 포함되어 있습니다&quot;<R> 오토바이<R> = &quot;ride on&quot; 그림 13. FreeU 응용 프로그램이 있거나 없는 ReVersion [15]에서 생성한 샘플. FreeU를 개인화된 텍스트-이미지 작업에 특화된 확산 모델인 Dreambooth [30]에 통합합니다. 그림 12에서 볼 수 있듯이 향상점은 분명하고 합성된 이미지는 사실감이 현저히 향상되었습니다. 예를 들어 기본 DreamBooth [30] 모델은 &quot;오토바이를 타는 액션 피규어 사진&quot; 프롬프트에서 액션 피규어의 다리 모양을 합성하는 데 어려움을 겪는 반면, FreeU로 증강된 버전은 이 장애물을 능숙하게 극복합니다. 마찬가지로 &quot;해변의 장난감&quot; 프롬프트의 경우 초기 출력은 체형 이상을 보였습니다. FreeU의 통합은 이러한 불완전성을 개선하여 더 정확한 표현을 제공하고 색상 충실도를 개선합니다. 또한 FreeU를 Stable Diffusion 기반 관계 역전 방법인 ReVersion [15]에 통합하여 그림 13에서 보듯이 품질을 향상시켰습니다. 예를 들어, 두 아이 사이에서 &quot;back to back&quot; 관계를 표현해야 할 때 FreeU는 ReVersion이 이 관계를 정확하게 표현하는 능력을 향상시킵니다. &quot;inside&quot; 관계의 경우, 개가 바구니 안에 놓여야 할 때 ReVersion은 때때로 아티팩트가 있는 개를 생성하고 FreeU를 도입하면 이러한 아티팩트를 제거하는 데 도움이 됩니다. ReVersion이 관계 개념을 효과적으로 포착하는 반면, Stable Diffusion은 때때로 U-Net 건너뛰기 피처의 과도한 고주파 노이즈로 인해 관계 개념을 합성하는 데 어려움을 겪을 수 있습니다. FreeU를 추가하면 ReVersion이 학습한 정확히 동일한 관계 프롬프트를 사용하여 엔티티 및 관계 합성 품질을 향상시킬 수 있습니다. 또한, 우리는 제로 샷 텍스트 가이드 비디오 대 비디오 변환에 맞게 조정된 확산 모델인 Rerender [39]에 대한 FreeU의 영향을 평가했습니다. 그림 14는 결과를 보여줍니다. 세부 정보와 사실성이 명확하게 향상되었습니다. 합성된 비디오. 예를 들어, &quot;선글라스를 쓴 개&quot;라는 프롬프트와 입력 비디오가 제공될 때 Rerender[39]는 처음에 &quot;선글라스&quot;와 관련된 아티팩트가 있는 개 비디오를 생성합니다. 그러나 FreeU를 통합하면 이러한 아티팩트가 성공적으로 제거되어 정제된 출력이 생성됩니다. 요약하면, 이러한 결과는 FreeU를 통합하면 정확히 동일한 학습된 프롬프트를 사용하여 향상된 엔터티 표현과 합성 품질이 제공된다는 것을 입증합니다. 3.5. 절제 연구 FreeU의 효과. FreeU는 확산 모델 내에서 U-Net 아키텍처의 노이즈 제거 기능을 향상시키는 주요 목적으로 도입되었습니다. FreeU의 영향을 평가하기 위해 Sta-A 로그 진폭 0.SD 0.FreeU -0.-1.00.SD 0.FreeU -1.5-2.A 로그 진폭 -0.-1.-1.-2.0-2.-2.-·8.On 0.2П 0.4П 0.6П 0.8П를 사용하여 분석 실험을 수행했습니다. 1.Οπ 주파수 단계A 로그 진폭 0.SD FreeU 0.-0.-1.-1.-2.A 로그 진폭 0.0.SD FreeU -0.-1.-1.-2.-2.8.On 0.2П 0.4π 0.6П 0.8П 1.0π -0.0П 0.2π 0.4π 0.6П 0.8П 1.Οπ -0.0П 0.2π 0.4П 0.6П 0.8П 주파수 단계주파수 단계주파수 단계그림 15. 잡음 제거 프로세스 내에서 FreeU가 있거나 없는 안정 확산[29]의 푸리에 상대 로그 진폭.SD SD SD SD SD + FreeU SD + FreeU SD + FreeU SD FreeU 그림 16. FreeU가 있거나 없는 안정 확산[29]에 대한 피처 맵의 시각화. SD SD+FreeU (b) SD+FreeU (b&amp;s) 보라색 가운을 입은 뚱뚱한 토끼가 판타지 풍경을 걷고 있다.노을 속을 길을 걷는 테디베어 바다에 반사되는 물 위로 보이는 신스웨이브 스타일의 일몰, 디지털 아트 그림 17. 백본 스케일링 계수와 스킵 스케일링 계수의 절제 연구.ble Diffusion [29]을 기본 프레임워크로 사용합니다.그림 15에서는 FreeU를 통합한 경우와 통합하지 않은 경우를 비교하여 Stable Diffusion [29]의 푸리에 변환의 상대적 로그 진폭을 시각화합니다.이러한 시각화는 FreeU가 노이즈 제거 프로세스의 각 단계에서 고주파 정보를 줄이는 데 눈에 띄는 영향을 미치는 것을 보여주며, 이는 FreeU의 ef(b) 용량을 나타냅니다.(c) 그림 18. 백본 스케일링 계수의 절제 연구.(a) SD의 생성된 이미지.(b) 상수 계수가 있는 FreeU의 생성된 이미지.(c) 구조 관련 스케일링 계수 맵이 있는 FreeU의 생성된 이미지. 효과적으로 노이즈를 제거합니다. 나아가, 우리는 U-Net 아키텍처의 피처 맵을 시각화하여 분석을 확장했습니다. 그림 16에서 볼 수 있듯이, FreeU에서 생성된 피처 맵에는 더 두드러진 구조적 정보가 포함되어 있음을 관찰했습니다. 이 관찰은 FreeU의 의도된 효과와 일치하며, 복잡한 세부 사항을 보존하는 동시에 효과적으로 노이즈를 제거하여 모델의 노이즈 제거 목표와 조화를 이룹니다. FreeU의 구성 요소 효과. 우리는 제안된 FreeU 전략의 효과를 평가합니다. 즉, 백본 피처 스케일링 계수와 스킵 피처 스케일링 계수를 도입하여 UNet 아키텍처의 기본 백본 및 스킵 연결에서 피처 기여도를 복잡하게 균형 잡습니다. 그림 17에서 평가 결과를 제시합니다. 추론 중에 백본 스케일링 계수가 통합되는 SD+FreeU(b)의 경우, SD[29] 단독에 비해 생생한 세부 사항 생성에서 눈에 띄는 개선을 관찰합니다. 예를 들어, &quot;판타지 풍경을 걷는 보라색 망토를 입은 뚱뚱한 토끼&quot;라는 프롬프트가 주어졌을 때 SD+FreeU(b)는 SD[29]와 달리 정상적인 팔과 귀를 가진 보다 사실적인 토끼를 생성합니다. 그러나 피처 스케일링 계수를 포함하면 상당한 개선이 이루어지지만 가끔은 텍스처의 바람직하지 않은 과도한 평활화로 이어질 수 있다는 점에 유의하는 것이 중요합니다. 이 문제를 완화하기 위해 저주파 정보를 줄이고 텍스처 과도한 평활화 문제를 완화하기 위해 건너뛰기 피처 스케일링 계수를 도입합니다. 그림 17에서 보여 주듯이 SD+FreeU(b &amp; s)에서 백본과 건너뛰기 피처 스케일링 계수를 결합하면 보다 사실적인 이미지가 생성됩니다. 예를 들어, &quot;바다의 반사수 위의 신스웨이브 스타일 일몰, 디지털 아트&quot;라는 프롬프트에서 SD+FreeU(b &amp; s)에서 생성된 일몰 하늘은 SD+FreeU(b)에 비해 사실성이 향상되었습니다. 이는 포괄적인 FreeU 전략이 피처의 균형을 맞추고 문제를 완화하는 데 효과적임을 강조합니다. 텍스처 매끄럽게 하는 것과 관련하여 궁극적으로 보다 충실하고 사실적인 이미지 생성을 가져옵니다. 백본 구조 관련 요소의 효과. 제안된 백본 스케일링 전략인 구조 관련 스케일링이 노이즈 감소와 텍스처 보존 간의 섬세한 균형에 미치는 효과를 평가합니다. 그림 18에 나와 있듯이 SD [29]에서 생성된 결과와 비교할 때 일정한 스케일링 요소를 활용할 때 FreeU에서 생성된 이미지 품질이 상당히 향상되는 것을 관찰합니다. 그러나 고정된 스케일링 요소를 사용하면 텍스처의 현저한 과매끄럽게 하는 것과 바람직하지 않은 색상 과포화로 나타나는 부정적인 결과가 발생할 수 있다는 점을 강조하는 것이 중요합니다. 반대로 구조 관련 스케일링 요소 맵을 사용하는 FreeU는 적응적 스케일링 접근 방식을 사용하여 구조 정보를 활용하여 백본 요소 맵의 할당을 안내합니다. 우리의 관찰 결과에 따르면 구조 관련 스케일링 요소 맵을 사용하는 FreeU는 이러한 문제를 효과적으로 완화하고 생생하고 복잡한 세부 사항을 생성하는 데 상당한 개선을 이룹니다. 4.
--- CONCLUSION ---
이 연구에서는 추가적인 계산 비용 없이 확산 모델의 샘플 품질을 크게 향상시키는 우아하고 간단하면서도 매우 효과적인 FreeU라는 접근 방식을 소개합니다. U-Net 아키텍처에서 스킵 연결과 백본 피처가 수행하는 근본적인 역할에 동기를 부여받아 확산 U-Net에서 이러한 피처의 효과에 대한 심층 분석을 수행합니다. 조사 결과 기본 백본은 주로 노이즈 제거에 기여하는 반면 스킵 연결은 주로 디코더에 고주파 피처를 도입하여 필수적인 백본 의미론을 무시할 가능성이 있음을 보여줍니다. 이를 해결하기 위해 U-Net의 스킵 연결과 백본 피처 맵에서 발생하는 기여를 전략적으로 다시 가중치를 적용합니다. 이 다시 가중치를 적용하는 프로세스는 두 U-Net 구성 요소의 고유한 강점을 활용하여 광범위한 텍스트 프롬프트와 랜덤 시드에서 샘플 품질을 크게 향상시킵니다. 제안하는 FreeU는 다양한 확산 기반 모델과 다운스트림 작업에 원활하게 통합되어 샘플 품질을 향상시키는 다재다능한 수단을 제공합니다. 참고문헌 [1] Omri Avrahami, Dani Lischinski, Ohad Fried. 자연 이미지의 텍스트 기반 편집을 위한 혼합 확산. CVPR에서, 2022.[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis. 잠재체 정렬: 잠재체 확산 모델을 사용한 고해상도 비디오 합성. CVPR에서, 2023.[3] Andrew Brock, Jeff Donahue, Karen Simonyan. 고충실도 자연 이미지 합성을 위한 대규모 GAN 학습. arXiv 사전 인쇄본 arXiv:1809.11096, 2018.[4] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon. ILVR: 확산 확률적 모델의 노이즈 제거를 위한 컨디셔닝 방법. ICCV에서, 2021.[5] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 GANS를 이긴다. NeurIPS, 2021.[6] Patrick Esser, Robin Rombach, Andreas Blattmann 및 Bjorn Ommer. ImageBART: 자기 회귀 이미지 합성을 위한 다항 확산을 사용한 양방향 컨텍스트. NeurIPS, 2021.[7] Patrick Esser, Robin Rombach 및 Bjorn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. CVPR, 2021.[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR, 2023.[9] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio. 생성적 적대적 네트워크. NeurIPS, 2014년.[10] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo. 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. CVPR, 2022년.[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen. 임의 길이의 고화질 비디오 생성을 위한 잠재 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13221, 2022년.[12] Jonathan Ho, Ajay Jain, Pieter Abbeel. 노이즈 제거 확산 확률 모델. NeurIPS, 2020. 1,[13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu 및 Jie Tang. Cog Video: 변압기를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868, 2022.[14] Ziqi Huang, Kelvin CK Chan, Yuming Jiang 및 Ziwei Liu. 다중 모달 얼굴 생성 및 편집을 위한 협력 확산. CVPR, 2023.[15] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan 및 Ziwei Liu. ReVersion: 이미지에서 확산 기반 관계 반전. arXiv 사전 인쇄 arXiv:2303.13495, 2023. 3,[16] Tero Karras, Timo Aila, Samuli Laine 및 Jaakko Lehtinen. 품질, 안정성 및 변형 개선을 위해 GAN을 점진적으로 성장시킵니다. ICLR, 2018.[17] 테로 카라스, 사무리 레인, 티모 아일라. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처입니다. CVPR, 2019. [18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen 및 Timo Aila. StyleGAN의 이미지 품질을 분석하고 개선합니다. CVPR, 2020. [19] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen 및 Timo Aila. 별칭이 없는 생성적 적대 네트워크. NeurIPS, 2021.[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri 및 Michal Irani. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. arXiv 사전 인쇄 arXiv:2210.09276, 2022.[21] 디데릭 P 킹마(Diederik P Kingma)와 맥스 웰링(Max Welling). 자동 인코딩 변형 베이. arXiv 사전 인쇄 arXiv:1312.6114, 2013.[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman 및 Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 정의. arXiv 사전 인쇄 arXiv:2212.04488, 2022.[23] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou 및 Tieniu Tan. VideoFusion: 고품질 비디오 생성을 위한 분해 확산 모델입니다. CVPR, 2023. 1, 3, 6, 7,[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu 및 Stefano Ermon. SDEdit: 확률론적 미분 방정식을 사용한 유도 이미지 합성 및 편집. ICLR, 2022.[25] 메디 미르자와 사이먼 오신데로. 조건부 생성 적대 네트워크. arXiv 사전 인쇄 arXiv:1411.1784, 2014.[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. GLIDE: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. arXiv 사전 인쇄본 arXiv:2112.10741, 2021.[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach. Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 사전 인쇄본 arXiv:2307.01952, 2023.[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. CLIP 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022.[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR, 2022. 1, 3, 6, 7, 10,[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR, 2023. 3,6,[31] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi. Palette: 이미지-이미지 확산 모델. ACM SIGGRAPH, 2022.[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487, 2022.[33] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022.[34] Aaron Van Den Oord, Oriol Vinyals 등. 신경 이산 표현 학습. NeurIPS, 2017.[35] 페이 왕(Pei Wang), 리 이준(Yijun Li), 누노 바스콘셀로스(Nuno Vasconcelos). 이미지 스타일 전송의 견고성을 재고하고 개선합니다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 124-133, 2021.[36] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen 및 Fang Wen. 사전 훈련은 이미지를 이미지로 변환하는 데 필요한 전부입니다. arXiv 사전 인쇄 arXiv:2205.12952, 2022.[37] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang 등. Lavie: 계단식 잠재 확산 모델을 사용한 고품질 비디오 생성. arXiv 사전 인쇄 arXiv:2309.15103, 2023.[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie 및 Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 조정입니다. arXiv 사전 인쇄 arXiv:2212.11565, 2022.[39] Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy. 비디오 다시 렌더링: 제로샷 텍스트 안내 비디오-비디오 변환. arXiv 사전 인쇄 arXiv:2306.07954, 2023. 3, 6,[40] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao 및 Mike Zheng Shou. 쇼 1: 텍스트-비디오 생성을 위한 픽셀 및 잠재 확산 모델 결합, 2023.
