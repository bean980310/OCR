--- ABSTRACT ---
이 논문은 동작을 보존하면서 비디오의 시각적 모양을 수정하는 문제를 다룹니다. MagicProp이라는 새로운 프레임워크가 제안되는데, 이는 비디오 편집 프로세스를 모양 편집과 동작 인식 모양 전파의 두 단계로 나눕니다. 첫 번째 단계에서 MagicProp은 입력 비디오에서 단일 프레임을 선택하고 이미지 편집 기술을 적용하여 프레임의 콘텐츠 및/또는 스타일을 수정합니다. 이러한 기술의 유연성 덕분에 프레임 내의 임의 영역을 편집할 수 있습니다. 두 번째 단계에서 MagicProp은 편집된 프레임을 모양 참조로 사용하고 자기 회귀 렌더링 방식을 사용하여 나머지 프레임을 생성합니다. 이를 달성하기 위해 PropDPM이라는 확산 기반 조건 생성 모델이 개발되어 참조 모양, 대상 동작 및 이전 모양을 조건으로 하여 대상 프레임을 합성합니다. 자기 회귀 편집 방식은 결과 비디오에서 시간적 일관성을 보장합니다. 전반적으로 MagicProp은 이미지 편집 기술의 유연성과 자기 회귀 모델링의 뛰어난 시간적 일관성을 결합하여 프레임 전체에서 좋은 시간적 일관성을 유지하면서 입력 비디오의 임의 영역에서 객체 유형과 미적 스타일을 유연하게 편집할 수 있습니다. 다양한 비디오 편집 시나리오에서 광범위한 실험을 통해 MagicProp의 효과가 입증되었습니다. 1
--- INTRODUCTION ---
콘텐츠 생성에는 종종 비디오 편집이 포함되며, 여기에는 원시 비디오의 모양을 수정하거나 동작을 조정하는 작업이 포함됩니다[Wu et al., 2023; Kasten et al., 2021; Zhao et al., 2023; Wang et al., 2023]. 영화 제작자는 더 나은 미적 품질을 위해 원시 비디오의 노출, 채도 및 대비를 조정해야 할 수 있으며, 광고주는 타겟 청중에게 깊은 인상을 주기 위해 사실적인 비디오를 특정 매혹적인 스타일로 바꾸고 싶을 수 있습니다. 이 논문은 특정 공간 영역에서 로컬하게 또는 전체 비디오에서 글로벌하게 콘텐츠나 스타일을 변경하는 것을 포함하여 비디오 모양을 편집하는 문제를 다룹니다. 기존 연구는 주로 두 가지 관점에서 이 문제를 해결하려고 시도합니다. 이미지 생성 모델을 통해 각 프레임을 개별적으로 편집합니다[Qi et al., 2023; Ceylan et al., 2023; Yang et al., 2023; Khachatryan et al., 2023; Geyer 등, 2023] 또는 모양 변경을 위한 전체 비디오 시퀀스 모델링[Ni 등, 2023; Molad 등, 2023; Karras 등, 2023; Kasten 등, 2021; Esser 등, 2023]. Stable Diffusion[Rombach 등, 2022] 및 ControlNet[Zhang 및 Agrawala, 2023]과 같은 이미지 모델 기반 방법은 임의의 영역의 내용이나 스타일을 유연하게 수정할 수 있지만, 인접한 프레임에서 시간적 일관성을 보장하는 것은 어렵습니다. 이 문제를 완화하기 위해 일부는 구조 안내 모델과 프레임 간 주의를 사용하여 프레임 전체에서 색상과 레이아웃을 맞춥니다[Zhang 및 Agrawala, 2023; Qi 등, 2023; Ceylan 등, 2023]. 다른 방법은 광학 흐름과 같은 프레임 간 대응을 활용하여 편집된 프레임의 특징을 왜곡합니다[Yang et al., 2023; Geyer et al., 2023]. 그러나 편집된 비디오의 시간적 일관성은 여전히 최적이 아닙니다. 연구자들은 이미지 기반 모델을 사용하는 대신 비디오 생성 및 편집을 위한 많은 시퀀스 기반 모델을 개발했습니다[Esser et al., 2023; Couairon et al., 2023]. 신경 계층 아틀라스(NLA)는 먼저 비디오를 과적합한 다음 학습된 해당 아틀라스를 편집하여 전경이나 배경을 변경합니다[Kasten et al., 2021; Bar-Tal et al., 2022]. NLA 기반 방법은 비디오의 모양을 효과적으로 편집할 수 있지만 테스트 시간 최적화는 시간과 리소스가 많이 소모됩니다. 최근, Gen-[Esser et al., 2023], ControlVideo [Zhao et al., 2023; Chen et al., 2023], VideoComposer [Wang et al., 2023]와 같은 구조 인식 비디오 생성을 위한 많은 확산 기반 모델이 제안되었습니다. 이러한 방법은 깊이 또는 스케치 맵과 같은 레이아웃 시퀀스를 조건으로 비디오를 합성하여 결과 비디오의 동작 일관성을 보장할 수 있습니다. 그러나 텍스트 설명의 제한과 사용자 상호 작용의 어려움으로 인해 편집 가능성과 유연성이 손상됩니다. 예를 들어, 주어진 비디오의 특정 부분을 편집할 때 텍스트 프롬프트는 모든 프레임에서 관심 영역을 정확하게 현지화하지 못할 수 있으며 사용자가 모든 프레임에 대한 마스크를 준비하는 것이 어려울 수 있습니다. 시간적 일관성과 편집 유연성 간의 균형은 비디오 편집을 위한 다른 대안 프레임워크를 탐색하도록 영감을 줍니다. 비디오 내의 프레임이 일반적으로 비슷한 장면을 공유한다는 사실에 동기를 부여받아, 비디오 편집을 외관 편집과 동작 인식 외관 전파의 두 단계로 풀어내는 새로운 프레임워크인 MagicProp을 제안합니다. MagicProp은 먼저 주어진 비디오에서 하나의 프레임을 선택하여 외관을 편집합니다. 편집된 프레임은 두 번째 단계에서 외관 참조로 사용됩니다. 그런 다음, MagicProp은 참조 프레임과 동작 시퀀스(예: 주어진 비디오의 깊이 맵)를 조건으로 나머지 프레임을 자기 회귀적으로 렌더링합니다. MagicProp은 자기 회귀 방식으로 비디오를 모델링하여 출력 비디오의 시간적 일관성을 보장합니다. 또한, MagicProp은 참조 편집을 위해 강력한 이미지 확산 모델(선택적으로 추가 마스크 사용)을 사용하여 로컬 영역 또는 전체 비디오의 내용을 유연하게 수정할 수 있습니다. MagicProp의 가장 중요한 구성 요소는 이전 프레임, 대상 깊이 및 참조 외관의 제어 하에 대상 이미지를 합성하는 자기 회귀 조건부 이미지 확산 모델입니다. 우리는 참조 프레임의 의미 수준 및 픽셀 수준 정보를 병합하여 이미지 생성 프로세스에 주입하는 가벼운 어댑터를 설계하여 결과 프레임의 모양이 참조와 잘 일치하도록 합니다. 훈련하는 동안 우리는 훈련과 추론 중 노이즈 일정 간의 격차를 메우는 제로 터미널 신호 대 잡음비(SNR) [Lin et al., 2023] 전략을 따르며, 생성된 프레임의 색상과 스타일이 참조와 더 잘 일치합니다. 우리는 로컬 객체/배경 편집 및 글로벌 스타일화를 포함한 여러 비디오 편집 시나리오에서 광범위한 실험을 수행했습니다. 결과는 MagicProp의 효과와 유연성을 보여줍니다. MagicProp의 기여는 세 가지입니다. • 우리는 비디오 편집을 모양 편집 및 동작 인식 모양 전파로 분리하는 새로운 프레임워크인 MagicProp을 제안했습니다. • 우리는 클래스 및 픽셀 수준 기능을 확산 모델에 주입하는 가벼운 어댑터를 고안했습니다. 우리는 또한 훈련을 위해 제로 터미널 SNR 전략을 적용했습니다. 이러한 기술은 모양의 정렬을 용이하게 합니다. • 광범위한 실험을 통해 MagicProp이 주어진 비디오의 임의의 영역을 유연하게 편집하고 고품질 결과를 생성할 수 있음이 입증되었습니다.2
--- RELATED WORK ---
S 및 예비 연구 이 섹션에서는 먼저 비디오의 모양 편집에 대한 최근 관련 연구를 검토합니다. 이를 이미지 모델을 통해 프레임별로 비디오를 편집하는 것과 편집을 위해 전체 프레임 시퀀스를 모델링하는 것의 두 그룹으로 분류합니다. 그런 다음 확산 확률 모델과 비디오 편집을 위한 표기법에 대한 예비 연구를 소개합니다. 2.1 관련 연구 프레임별 편집 확산 기반 이미지 생성 모델은 이미지 생성 및 편집 작업에서 큰 성공을 거두었습니다[Ho et al., 2020; 2022; Rombach et al., 2022; Blattmann et al., 2023]. 가장 간단한
--- METHOD ---
영어: Stable Diffusion[Rombach et al., 2022] 및 ControlNet[Zhang and Agrawala, 2023]과 같은 이미지 모델을 기반으로 하는 s는 임의의 영역의 내용이나 스타일을 유연하게 수정할 수 있지만 인접 프레임 간에 시간적 일관성을 보장하는 것은 어렵습니다.이 문제를 완화하기 위해 일부는 구조 기반 모델과 프레임 간 주의를 사용하여 프레임 간에 색상과 레이아웃을 맞춥니다[Zhang and Agrawala, 2023; Qi et al., 2023; Ceylan et al., 2023].다른 방법은 광학 흐름과 같은 프레임 간 대응성을 활용하여 편집된 프레임의 기능을 왜곡합니다[Yang et al., 2023; Geyer et al., 2023].그러나 편집된 비디오의 시간적 일관성은 여전히 최적이 아닙니다.연구자들은 이미지 기반 모델을 사용하는 대신 비디오 생성 및 편집을 위한 많은 시퀀스 기반 모델을 개발했습니다[Esser et al., 2023; Couairon 등, 2023]. 신경 계층 아틀라스(NLA)는 먼저 비디오를 과적합한 다음 학습된 해당 아틀라스를 편집하여 전경 또는 배경을 변경합니다[Kasten 등, 2021; Bar-Tal 등, 2022]. NLA 기반 방법은 비디오의 모양을 효과적으로 편집할 수 있지만 테스트 시간 최적화는 시간과 리소스가 많이 소모됩니다. 최근 Gen-[Esser 등, 2023], ControlVideo [Zhao 등, 2023; Chen 등, 2023], VideoComposer [Wang 등, 2023]와 같이 구조 인식 비디오 생성을 위해 많은 확산 기반 모델이 제안되었습니다. 이러한 방법은 깊이 또는 스케치 맵과 같은 레이아웃 시퀀스를 조건으로 비디오를 합성하여 결과 비디오의 동작 일관성을 보장할 수 있습니다. 그러나 텍스트 설명의 제한과 사용자 상호 작용의 어려움으로 인해 편집 가능성과 유연성이 저하될 수 있습니다. 예를 들어, 주어진 비디오의 특정 부분을 편집할 때 텍스트 프롬프트는 모든 프레임에서 관심 영역을 정확하게 현지화하지 못할 수 있으며, 사용자가 모든 프레임에 대한 마스크를 준비하는 것이 어려울 수 있습니다. 시간적 일관성과 편집 유연성 간의 균형은 비디오 편집을 위한 다른 대체 프레임워크를 탐색하도록 영감을 줍니다. 비디오 내의 프레임이 일반적으로 유사한 장면을 공유한다는 사실에 동기를 부여받아 비디오 편집을 모양 편집과 동작 인식 모양 전파의 두 단계로 구분하는 새로운 프레임워크인 MagicProp을 제안합니다. MagicProp은 먼저 주어진 비디오에서 하나의 프레임을 선택하여 모양을 편집합니다. 편집된 프레임은 두 번째 단계에서 모양 참조로 사용됩니다. 그런 다음 MagicProp은 참조 프레임과 동작 시퀀스(예: 주어진 비디오의 깊이 맵)를 조건으로 나머지 프레임을 자기 회귀적으로 렌더링합니다. MagicProp은 자기 회귀 방식으로 비디오를 모델링하여 출력 비디오의 시간적 일관성을 보장합니다. 또한 MagicProp은 참조 편집을 위해 강력한 이미지 확산 모델(선택적으로 추가 마스크 사용)을 사용하여 로컬 영역 또는 전체 비디오의 콘텐츠를 유연하게 수정할 수 있습니다. MagicProp의 가장 중요한 구성 요소는 이전 프레임, 대상 깊이 및 참조 모양의 제어 하에 대상 이미지를 합성하는 자기 회귀 조건부 이미지 확산 모델입니다. 우리는 참조 프레임의 의미 수준 및 픽셀 수준 정보를 병합하여 이미지 생성 프로세스에 주입하는 가벼운 어댑터를 설계하여 결과 프레임의 모양이 참조와 잘 일치하도록 합니다. 훈련하는 동안 우리는 훈련과 추론 중 노이즈 일정 간의 격차를 메우는 제로 터미널 신호 대 잡음비(SNR) [Lin et al., 2023] 전략을 따르며, 이는 생성된 프레임의 색상과 스타일이 참조와 더 잘 일치하도록 합니다. 우리는 광범위한
