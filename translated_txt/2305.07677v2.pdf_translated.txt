--- ABSTRACT ---
Masked Language Models(MLMs)는 자동 음성 인식(ASR) 시스템에서 2차 통과 재채점에 효과적인 것으로 입증되었습니다. 이 연구에서는 MLM의 입력 공간에 음향 표현을 통합하는 다중 모달 마스크 언어 모델 재채점기인 Masked Audio Text Encoder(MATE)를 제안합니다. 우리는 공유 표현을 학습하여 모달리티를 효과적으로 정렬하기 위해 대조 학습을 채택합니다. 우리는 타겟 도메인 데이터를 사용할 수 없는 경우 다중 모달 재채점기를 사용하는 것이 ASR 시스템의 도메인 일반화에 유익함을 보여줍니다. MATE는 텍스트 전용 기준선에 비해 도메인 내 데이터 세트에서 단어 오류율(WER)을 4%-16%, 도메인 외 데이터 세트에서 3%-7% 줄입니다. 또한 매우 제한된 양의 교육 데이터(0.8시간)로 MATE는 1차 통과 기준선에 비해 WER을 8%-23% 줄입니다. 1
--- METHOD ---
설계상 모든 1차 통과 ASR 모델(하이브리드/CTC/트랜스듀서)과 함께 작동할 수 있습니다. 재채점자는 ASR 아키텍처, 학습 메커니즘 및 내부 기능에 독립적이어서 더 나은 일반화 기능을 제공합니다. 저희가 아는 한, 이는 사전 학습된 자체 감독 학습(SSL) 음성 표현 모델(Baevski et al., 2019, 2020; Hsu et al., 2021; Chen et al., 2021)을 2차 통과 재채점에 통합한 최초의 작업입니다. 음향 정보를 LLM에 통합하는 데 있어 한 가지 주요 과제는 음성을 언어 모델에서 수용할 수 있는 형태로 변환하는 것입니다. 저희는 합성곱 신경망(CNN)(LeCun et al., 1989)과 어댑터 네트워크(Houlsby et al., 2019)로 구성된 교차 모달 적응 모듈을 사용하여 이를 극복합니다. 저희는
--- EXPERIMENT ---
영어: 오디오-텍스트 정렬을 위한 다른 보조 정렬 손실을 사용하여 두 모달리티에서 공유된 표현을 효과적으로 학습하고 모델 성능을 크게 개선하는 대조 학습을 채택합니다.경험적으로, MATE가 제로 샷 및 퓨 샷 설정에서 새로운 도메인으로 잘 전환되어 텍스트 전용 베이스라인보다 성능이 우수함을 보여줍니다.LMLM L = LMLM + αLCTR Masked Language Model(BERT) Concatenation LCTR OO Adapter Net CNN Speech Encoder(WavLM) 回归 &quot;It is eighty uh eighty two degrees&quot; 어휘 임베딩 It is eighty uh [MASK] two degrees 그림 1: MATE는 두 가지 손실로 학습됩니다.(1) 연결된 교차 모달 표현을 입력으로 사용하고 마스크된 텍스트 토큰에서 LMLM을 계산하는 MLM.(2) 오디오 및 텍스트 잠재 표현을 정렬하는 LCTR. 2 Approach MATE는 그림 1에서 설명한 대로 사전 훈련된 마스크 언어 모델 BERT, 자체 감독 학습(SSL) 기반 음성 인코더 WavLM(Chen et al., 2021) 및 모달리티 매칭 모듈(CNN 및 어댑터 네트워크)로 구성됩니다. 2.1 시스템 아키텍처 마스크 언어 모델 우리는 사전 훈련된 양방향 MLM인 BERT를 재채점기의 주요 구성 요소로 사용합니다. 이 작업에서 우리는 BERT를 확장하여 텍스트와 함께 음성 데이터를 통합합니다. BERT의 사전 훈련된 임베딩 계층은 텍스트 임베딩 모듈 역할을 하는 반면, 중간 인코더 계층은 음향 및 어휘 표현을 모두 입력으로 받습니다. 사전 훈련된 음성 인코더 음향 표현을 추출하기 위해 마스크 음성 예측 및 음성 잡음 제거 작업에 대해 사전 훈련된 WavLM 모델을 사용하여 다양한 음성 처리 작업에서 최첨단 성능을 달성하고 SUPERB(Yang et al., 2021) 벤치마크에서 Wav2Vec2(Baevski et al., 2020) 및 HuBERT(Hsu et al., 2021)와 같은 다른 모델보다 우수한 성능을 달성했습니다.교차 모달 적응 동일한 특징 공간에서 음향 및 어휘 표현을 정렬하기 위해 교차 모달 적응 모듈을 설계합니다.이는 두 개의 하위 모듈로 구성됩니다.(i) 모달리티 간의 시퀀스 길이를 균형 잡기 위한 CNN(Convolutional Neural Network) 기반 서브 샘플링 구성 요소, (ii) 음향 표현을 BERT 인코더 입력 공간에 투사하기 위한 병목 어댑터 네트워크. 어댑터 네트워크 a와 어휘 임베딩의 출력은 수평으로 al 연결¹되고 BERT 인코더 계층을 통과하여 두 모달리티의 정보를 융합합니다.2.2 정렬 손실 사전 훈련된 마스크 언어 모델은 텍스트 코퍼스에서 훈련됩니다(Devlin et al., 2019). 오디오 및 텍스트 모달리티를 명시적으로 정렬하기 위해 명시적 정렬 손실 함수를 도입하여 교차 모달 학습의 품질을 더욱 향상시킬 것을 제안합니다. 우리는 대조적 손실 함수를 채택하여 음향 표현 a와 어휘 표현 /를 공유 기능 공간에 매핑하도록 강제합니다. 우리는 발화 수준에서 평균 풀링을 수행하고 음향 또는 어휘 표현 a¿와 li에서 각각 풀링된 벡터를 (āį,īj)로 표시합니다. 배치 크기인 음향-어휘 표현 (āi, i)1≤i≤N이 주어지면, 우리는 쌍을 이룬 벡터 (āį, į)를 양성 샘플로 사용하고, 동일한 미니 배치에서 쌍을 이루지 않은 벡터 (āi, īj)i‡¡를 음성 샘플로 사용합니다. 학습 목표는 음의 로그 가능도(NLL) 함수를 사용하여 다음의 대조 손실 LCTR을 최소화하는 것입니다. LCTR N log: i=exp(sim(ār, Ti)) N Σ₁₁ exp(sim(ār, j)) j=(1) 여기서 sim(,)은 실험에서 점곱으로 구현된 유사도 메트릭입니다. 대조 손실은 쌍을 이루지 않은 표현에 비해 쌍을 이룬 음향 및 어휘 표현 간의 더 높은 수준의 유사성을 촉진하여 두 모달리티 간의 정렬을 향상시킵니다. 2.3 학습 및 추론 학습 MATE는 BERT의 사전 학습에 사용된 것과 유사한 MLM 목적 MLM과 대조 손실 LCTR에서 공동으로 학습합니다.L = £MLM+a• LCTR (2) BERT 사전 학습 후 텍스트 시퀀스의 토큰 일부가 예측을 위해 무작위로 선택되고 [MASK] 토큰인 a로 대체됩니다.¹또한 성능이 떨어지는 교차 주의 기반 병합 메커니즘을 실험했습니다.임의 토큰이나 변경하지 않습니다.모델의 성능을 최적화하기 위해 모델은 종단 간에 학습되고 모든 매개변수는 학습 프로세스 중에 업데이트됩니다.= 추론 시퀀스 수준 점수를 계산하기 위해 의사 로그 가능도(PLL) 스코어링(Wang 및 Cho, 2019; Salazar et al., 2020)을 사용합니다. 음향 시퀀스 a(S1,, SR)와 어휘 시퀀스(t1, ..., tT)가 주어지고, l_k(t1,..., tk-1, [MASK], tk+1, ..., tT)라고 하면, PLL 점수는 마스크된 각 어휘 토큰의 조건부 로그 확률 log PMLM(lia, l₁₁)을 합산하여 계산합니다. T = = PLL(1) log PMLM(li|a, l_;) (3) = i= 발화의 최종 점수는 1차 통과 ASR 신뢰도 점수와 2차 통과 PLL 점수의 선형 보간으로 계산되며, 상호 보완적인 정보를 활용하여 성능을 개선하는 동시에 두 점수 간의 균형을 허용합니다. 3 실험 3.1 데이터 세트 교육 세트 교육 코퍼스는 공개 및 사내 데이터 세트에서 샘플링한 10,000시간 이상의 오디오-텍스트 데이터 쌍으로 구성됩니다. 이 데이터 체제는 악센트, 화자, 샘플링 속도 및 배경 소음이 혼합된 다양한 음성 애플리케이션에 사용되는 다양한 ASR 시스템을 대표합니다.데이터의 5% 미만이 AWS Polly Text-to-Speech(TTS) 2 신경 백엔드를 사용하여 생성된 합성 오디오입니다.평가 세트 다양한 도메인의 합성 및 실제 데이터 세트에서 제안된 MATE 접근 방식을 평가합니다.MTDialogue(movie-twitter), LibriSpeech(LS)(Panayotov et al., 2015) 및 VoxPopuli(Wang et al., 2021)는 훈련 세트에 해당 훈련 데이터 분할이 포함되므로 도메인 내 세트입니다.Wall Street Journal(WSJ)(Garofolo et al., 1993), ConvAI(사내), SLURP(Bastianelli et al., 2020) 데이터 세트는 제로샷 평가를 위한 도메인 외부(OOD) 데이터 세트입니다. MTDialogue(movie-twitter)는 영화 자막과 twitter 사용자 상호작용으로 구성된 공개 어휘 대화 코퍼스 3을 기반으로 합니다. 2https://aws.amazon.com/polly/ https://github.com/Phylliida/Dialogue-Datasets 오디오는 TTS 시스템에서 생성됩니다. MTDialogue 데이터 세트는 오픈북 평가를 위한 보이는 데이터 세트입니다. 즉, 모든 데이터 샘플이 훈련 데이터에 포함됩니다. 1.2시간의 하위 집합이 평가를 위해 샘플링됩니다. LibriSpeech(LS)(Panayotov et al., 2015)는 LibriVox 오디오북을 기반으로 하는 읽기 영어 음성 코퍼스입니다. 우리는 각각 5시간 분량의 테스트 오디오가 포함된 두 가지 공식 평가 세트인 test-clean과 test-other를 고려합니다. VoxPopuli(Wang et al., 2021)는 2009-2020년 유럽 의회 행사 녹음에서 샘플링한 공개 정치 연설로 구성되어 있습니다. 평가 목적으로 VoxPopuli English 데이터의 5시간 하위 집합을 활용합니다. 또한 OOD 평가 세트인 ConvAI, WSJ, SLURP에서 MATE를 평가합니다. Wall Street Journal(WSJ)(Garofolo et al., 1993) 코퍼스에는 기자의 기존 받아쓰기와 자발적 받아쓰기가 포함되어 있습니다. 0.4시간의 test_eval93 분할이 평가를 위해 선택되었습니다. ConvAI는 작업 지향 대화형 AI 시스템의 사내 사용자 발화를 기반으로 합니다. 일반적인 사용 시나리오로는 항공편 예약, 음식 주문, 건강 보험 정보 조회 등이 있습니다. 2.0시간 분량의 오디오는 TTS 시스템에서 생성됩니다. SLURP(Bastianelli et al., 2020)는 스마트 홈 가상 비서 개발을 위한 공개 데이터 세트입니다. 주요 사용 시나리오로는 일정 확인, 음악 재생, 시간 묻기 등이 있습니다. 평가를 위해 10시간 테스트 세트를 활용했습니다. 윤리적 고려 사항: 연구 및 논문 출판을 허용하는 모든 공개 데이터 세트의 라이선스를 검토했습니다. 사내 데이터 세트 ConvAI는 연구 목적으로 내부적으로 승인되었습니다. 모든 데이터 세트는 익명성을 보장하기 위해 익명 처리됩니다. 또한 데이터 세트가 다양한 영어 악센트, 화자 및 배경을 포함하는지 확인합니다. 3.2 평가 지표 평가 지표로 단어 오류율(WER)과 내용 단어 오류율(CWER)을 사용합니다. CWER은 내용 단어(예: &quot;피자&quot;, &quot;의회&quot;, &quot;항공사&quot;)에 대해서만 계산되며, 여기서는 규칙 기반 방법을 적용하여 미리 정의된 기능 단어 블록 목록을 필터링합니다. 또한 표준 SLU 지표(정확도 및 F1 점수)를 사용하여 SLURP 데이터 세트에서 구어 이해(SLU) 성능을 평가합니다. SLU 예측(시나리오, 액션 및 엔터티)은 양방향 Long Short-Term Memory(BiLSTM) NLU 모듈(부록 A)에 의해 생성됩니다. 도메인 내 MTDialogue LS 테스트-클린 LS 테스트-다른 Voxpopuli WSJ 도메인 외 ConvAI SLURP(1) 재점수 없음(2) GPT2 텍스트(3) BERT 텍스트(4) LAS 재점수(6) MATE-NA(7) MATE-MSE(8) MATE(우리의) WER CWER WER CWER WER CWER WER CWER WER CWER WER CWER WER CWER 9.47 14.63 6.75 8.07 11.98 15.61 11.06 10.33 8.16 8.75 5.89 9.00 24.91 29.9.32 14.37 6.45 7.78 11.70 15.8.40 5.76 8.66 24.91 29.9.05 13.88 5.50 7.10.14.9.27 14.15 6.7 7.99 11.97 15.59 11.02 10.(5) 멀티모달 GPT2 9.24 14.6.7.69 11.54 14.93 10.56 9.9.05 13.90 5.7.10.75 14.51 10.34 9.7.49 11.41 5.22 6.95 10.31 13.97 10.10 9.7.64 11.70 5.16 6.84 10.30 13.81 9.91 9.10.9.7.10.9.6.8.20 5.8.37 24.48 29.8.01 8.55 5.7.55 8.20 5.6.49 8.10 5.6.10 7.65 5.6.01 7.46 5.8.84 24.91 29.8.59 24.89 29.8.36 24.46 29.7.92 23.84 28.7.91 23.77 28.(9) Frozen-ME 9.21 14.22 5.57 7.3.4 매개변수 효율적 튜닝 9.80 6.55 8.15 5.42 8.34 24.39 29.9.15 14.02 5.(10) WavLM-어댑터 7.41 10.81 14.69 10.23 9.86 6.52 8.05 5.47 8.39 24.56 29.(11) ME-어댑터 9.19 14.12 5.56 7.43 10.79 14.63 10.09 9.60 6.43 8.20 5.42 8.35 24.34 29.표 1: WER↓ 및 CWER↓로 측정한 성능. (2-3)을 제외한 모든 모델은 멀티모달입니다. (2) GPT2-텍스트(Radford et al., 2019a): 코퍼스 전사본 훈련에 대한 GPT2의 전체 미세 조정. (3) BERT-text(Salazar et al., 2020; Devlin et al., 2019): &quot;텍스트 전용 베이스라인&quot;이라고도 하는, 코퍼스 전사를 훈련하는 BERT의 전체 미세 조정. (4) LAS 재점수(Sainath et al., 2019): WavLM으로부터 음향 정보를 수용하는 LAS 헤드 재점수(주의 기반 LSTM 디코더)를 포함한 다중 모달 베이스라인. (5) Multi-modal-GPT2: WavLM으로부터 음향 정보를 수용하는 GPT2를 포함한 다중 모달 단방향 베이스라인. (6) MATE-NA: 추가 정렬 손실 없는 MATE. (7) MATE-MSE: 대조 손실 대신 MSE 손실로 훈련된 MATE. (9) Frozen-ME(Masked Encoder): MLM 목적만 있는 마스크 인코더(BERT) 계층을 제외한 다중 모달 시스템의 모든 매개변수를 미세 조정. (10) WavLM-어댑터: 음성 인코더(WavLM)에 병목 어댑터를 추가하고 WavLM에서 어댑터 튜닝을 수행하며, 다른 모든 매개변수는 고정됩니다.(11) ME(마스크 인코더)-어댑터: 마스크 인코더(BERT)에서 어댑터 튜닝을 수행하며, 다른 모든 매개변수는 고정됩니다.4 결과 및 분석 실험 결과의 관찰 및 분석을 다음과 같이 요약합니다.MATE는 도메인 내 및 도메인 외 일반화에서 모두 뛰어납니다.표 1은 다양한 설정에서 도메인 내 및 OOD 데이터 세트에서 제안된 MATE와 여러 기준 모델의 성능을 요약합니다.전반적으로 제안된 접근 방식(행 8)이 도메인 내 데이터 세트에서 텍스트 전용 기준(행 3)보다 상당히 우수한 성능을 보이는 것을 관찰하여 미세 조정을 위한 충분한 대상 도메인 코퍼스가 있는 경우에도 오디오 정보가 도움이 됨을 나타냅니다. 또한 OOD 데이터 세트의 결과는 MATE가 오디오의 풍부한 정보를 활용하여 텍스트 전용 기준선과 비교할 때 도메인 데이터가 전혀 없는 경우(제로샷 설정)에도 새로운 도메인으로 훨씬 더 잘 일반화됨을 나타냅니다.MLM은 자기 회귀 LM보다 더 효과적인 다중 모달 재점수자입니다.2~5행은 BERT와 자기 회귀 재점수자(LAS/GPT-2) 사이에 상당한 성능 격차가 있음을 나타냅니다.텍스트 전용 기준선인 BERT-Text는 다중 모달 GPT2보다 성능이 뛰어나 격차의 근본 원인이 GPT2에 양방향(좌우) 컨텍스트가 없기 때문이며, 이는 신뢰할 수 있는 *부록 B에는 하이퍼파라미터 및 인프라 설정을 포함한 실험 설정 세부 정보가 포함되어 있습니다.효과적인 LLM 스코어링을 위해 필요하므로 MATE에서 MLM을 선택한 것이 타당합니다.정렬 손실은 성능을 크게 향상시킵니다.정렬 손실의 효과를 연구하기 위해 평균 제곱 오차(MSE) 손실과 대조 손실이라는 두 가지 손실 함수로 다중 모달 재점수자를 훈련합니다. 표 1의 상당한 성능 향상(행 대 행 7-8)은 명시적 정렬 기술이 다중 모달 표현의 학습을 크게 개선한다는 것을 나타냅니다. 구체적으로, 대조 손실은 MSE 손실과 같은 관련 쌍을 정렬할 뿐만 아니라 관련 없는 샘플을 멀어지게 하여 OOD 세트에 대한 일반화를 개선합니다. 매개변수 효율적 미세 조정은 제한된 이득을 가져옵니다. 행 9-11은 다양한 매개변수 효율적 미세 조정 설정에서 다중 모달 재채점기의 성능을 연구합니다. 전체 미세 조정에서 어댑터 조정으로 이동하고 전체 BERT 인코더 계층을 동결하면 성능이 저하되는 것을 관찰하여 BERT 인코더 미세 조정이 성능 개선 측면에서 가장 유익함을 나타냅니다. 예상대로 전체 미세 조정(행 6)이 있는 모델과 비교했을 때 행 9-11은 성능이 낮습니다. 이는 동결 또는 매개변수 효율적 학습 방법이 다중 모달 데이터에 있는 음향 정보를 완전히 활용할 수 있는 모델 용량이 부족할 수 있음을 시사합니다. WERR 30.0% 25.0% 25.6% 23.0% 23.0% 23.8% 22.7% 21.9% 21.6% 20.0% 19.7% 20.5% 20.5% 20.8% 15.0% 10.0% 8.0% 6.6% 5.0% 0.0% 0.0.0.2.8.27.3.9% 9.2% 6.7%- 6.7% 9.2% 6.9% --BERT-text, Voxpopuli -- BERT-text, WSJ -MATE, Voxpopuli --MATE, WSJ 오디오 길이(시간) 그림 2: 도메인별 학습 데이터 크기에 대한 상대적 WER 감소(첫 번째 통과 대비) 시나리오 액션 엔티티 78.01 72.53 53.72.53 53.72.65 53.72.45 53.73.70 54. 재채점 없음 GPT2-텍스트 78. 다중 모달-GPT78. BERT-텍스트 77. MATE 78. 표 2: SLURP SLU 과제에 대한 제로샷 평가: 시나리오/액션에 대한 정확도, 엔티티에 대한 F1 점수. MATE는 가장 효과적인 퓨샷 학습기입니다. 퓨샷 학습의 효과를 연구하기 위해 그림 2에 표시된 것처럼 다양한 리소스 조건에서 Voxpopuli 및 WSJ 데이터 세트에 대한 상대적 WER 감소(WERR)를 플로팅합니다. MATE는 전혀 훈련이나 도메인 데이터가 없는 제로샷 설정에서 새로운 도메인으로 잘 전환되는 것을 관찰합니다. Fewshot 성능은 더 많은 예제로 분명히 개선되고, 제로샷 성능에서 완전한 미세 조정 성능까지의 격차를 줄이는 데 상당히 도움이 됩니다. 또한 MATE는 두 데이터 세트에서 텍스트 전용 기준선보다 지속적으로 우수한 성능을 보이며, 오디오 모달리티의 추가 정보를 활용하여 새로운 도메인에 빠르게 적응할 수 있는 능력을 확인합니다. MATE는 다운스트림 SLU 작업에서 최상의 제로샷 성능 개선을 달성합니다. 대화 시스템의 최종 목표에 대한 제안된 접근 방식의 효과를 평가하기 위해 시나리오/액션 정확도 및 SLURP 데이터 세트의 제로샷 설정에서 엔터티 F1 점수와 같은 메트릭을 사용하여 다른 기준선과 비교합니다. 표 2의 결과에서 MATE는 종단 간 목표에서 다른 기준선보다 지속적으로 우수한 성과를 보이며, 이는 개선이 주로 5SLURP는 스마트 홈 어시스턴트의 노이즈가 많은 사용 사례를 모방하는 까다로운 코퍼스입니다. 따라서 재채점 방법만 개선하여 WER 및 SLU 메트릭에서 절대적인 개선이 2% 미만입니다. 콘텐츠 단어 및 슬롯 엔터티 인식.5
--- CONCLUSION ---
s 우리는 도메인 내 및 OOD 데이터 세트에서 상당한 WER, CWER 감소를 달성하는 새로운 멀티모달 리스코러인 MATE를 제안합니다. 제로샷 및 퓨샷 설정에서 MATE는 보이지 않는 도메인에서 좋은 성능을 발휘하고 제한된 데이터로 빠르게 적응합니다. MATE의 도메인 일반화 기능은 ASR 시스템을 새로운 도메인으로 확장하기 위한 2차 통과 리스코러로 효과적인 선택이 됩니다. 6 제한 사항 우리 접근 방식의 한 가지 제한 사항은 SSL 음성 인코더(예: WavLM)의 음향 기능을 통합하면 1차 통과에 독립형 ASR 모델을 사용하기 때문에 추가 지연 오버헤드가 발생한다는 것입니다. 따라서 우리의 접근 방식은 지연 제약이 매우 낮은 특정 애플리케이션에는 적합하지 않을 수 있습니다. 또 다른 제한 사항은 멀티모달 LLM이 ASR 성능을 개선할 수 있는 잠재력이 있지만 텍스트 전용 LLM보다 더 복잡하고 해석하기 어려울 수 있다는 것입니다. 이로 인해 모델의 의사 결정 프로세스를 이해하거나 잠재적 오류를 디버깅하기가 더 어려워집니다. &quot;부록 E에 정성적 예가 나와 있습니다. 참고문헌 Alexei Baevski, Steffen Schneider, Michael Auli. 2019. vq-wav2vec: 이산 음성 표현의 자기 지도 학습. ArXiv, abs/1910.05453. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli. 2020. wav2vec 2.0: 음성 표현의 자기 지도 학습을 위한 프레임워크. 신경 정보 처리 시스템의 발전, 33권, 12449~12460페이지. Curran Associates, Inc. Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philémon Brakel, Yoshua Bengio. 2016. 종단 간 주의 기반 대규모 어휘 음성 인식. 2016 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스 (ICASSP), 4945-4949쪽. Ankur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati, Ye Jia, Jonathan H Clark, Melvin Johnson, Jason Riesa, Alexis Conneau, Yu Zhang. 2021. Slam: 음성-텍스트 공동 사전 학습을 통한 음성 및 언어 모델링을 위한 통합 인코더. arXiv 사전 인쇄본 arXiv:2110.10329. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, Verena Rieser. 2020. SLURP: 구어 이해 리소스 패키지. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 7252-7262쪽, 온라인. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Jinglun Cai, Mingda Li, Ziyan Jiang, Eunah Cho, Zheng Chen, Yang Liu, Xing Fan, and Chenlei Guo. 2023. Kg-eco: 질의 재작성을 위한 지식 그래프 강화 엔터티 수정. ICASSP 2023IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attention and spell: 대규모 어휘 대화형 음성 인식을 위한 신경망. 2016 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 4960-4964쪽. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Micheal Zeng, and Furu Wei. 2021. Wavlm: 풀 스택 음성 처리를 위한 대규모 자체 감독 사전 학습. IEEE Journal of Selected Topics in Signal Processing, 16:1505-1518. Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ramabhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen. 2022. MAESTRO: 모달리티 매칭을 통한 매칭된 음성 텍스트 표현. Proc. Interspeech 2022, 4093-4097쪽. Ethan A. Chi, Julian Salazar, Katrin Kirchhoff. 2021. Align-refine: 반복적 재정렬을 통한 비자기회귀 음성 인식. 2021년 북미 컴퓨터 언어학회 회의록: 인간 언어 기술, 1920-1927쪽, 온라인. 컴퓨터 언어학회. Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. 2014. 주의 기반 순환 nn을 사용한 종단 간 연속 음성 인식: 첫 번째 결과. 2014년 12월 NIPS 2014 심층 학습 워크숍. Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, Bettina Berendt. 2022. 편향된 통치자를 사용한 공정성 측정: 사전 학습된 언어 모델의 편향 지표에 대한 비교 연구. NAACL 2022: 2022년 북미 컴퓨터 언어학회 지부 컨퍼런스: 인간 언어 기술, 1693~1706쪽. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학회 지부 컨퍼런스 회의록: 인간 언어 기술, 1권(긴 논문과 짧은 논문), 4171~4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학회. Ankur Gandhe와 Ariya Rastrow. 2020. ASR 재채점을 위한 오디오 어텐션 차별 언어 모델. ICASSP 2020에서. Heting Gao, Junrui Ni, Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson. 2022. WavPrompt: 동결된 언어 모델을 사용한 Few-Shot Spoken Language Understanding을 향해. Proc. Interspeech 2022, 2738-2742쪽. John S. Garofolo, David Graff, Doug Paul, David Pallett. 1993. CSR-I(WSJ0) Complete LDC93S6A. Linguistic Data Consortium. Alex Graves. 2012. 순환 신경망을 사용한 시퀀스 변환. ArXiv, abs/1211.3711. Alex Graves와 Navdeep Jaitly. 2014. 순환 신경망을 사용한 엔드투엔드 음성 인식을 향해. 31st International Conference on Machine Learning의 회의록, Proceedings of Machine Learning Research의 32권, 1764-1772페이지. PMLR. Anmol Gulati, James Qin, Chung Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang. 2020. Conformer: 음성 인식을 위한 합성곱 증강 변환기. International Speech Communication Association의 연례 회의록, INTERSPEECH. Demi Guo, Alexander Rush, Yoon Kim. 2021. diff pruning을 사용한 매개변수 효율적 전이 학습. Association for Computational Linguistics의 제59회 연례 회의록 및 제11회 국제 자연어 처리 공동 회의(제1권: 장문 논문), 4884-4896페이지, 온라인. Association for Computational Linguistics. Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep speech: Scaling up endto-end speech awareness. CoRR, abs/1412.5567. Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. 2012. 음성 인식에서 음향 모델링을 위한 딥 신경망. Signal Processing Magazine. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. 2019. nlp를 위한 매개변수 효율적 전이 학습. ICML, Proceedings of Machine Learning Research의 97권, 2790-2799페이지. PMLR. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. 2021. Hubert: 숨겨진 단위의 마스크 예측을 통한 자체 감독 음성 표현 학습. IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 29:3451-3460. Ke Hu, Ruoming Pang, Tara N. Sainath, Trevor Strohman. 2021. 2-패스 음성 인식을 위한 트랜스포머 기반 심의. 2021 IEEE Spoken Language Technology Workshop(SLT), 68-74쪽. Ke Hu, Tara N Sainath, Yanzhang He, Rohit Prabhavalkar, Trevor Strohman, Sepand Mavandadi, Weiran Wang. 2022. 텍스트 전용 및 반지도 학습을 통한 심의 개선. arXiv 사전 인쇄본 arXiv:2206.14716. Ke Hu, Tara N Sainath, Ruoming Pang, Rohit Prabhavalkar. 2020. 심의 모델 기반 2-패스 엔드투엔드 음성 인식. ICASSP 20202020 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 77997803쪽. IEEE. Diederik P. Kingma, Jimmy Ba. 2014. 확률적 최적화를 위한 방법. abs/1412.6980. Adam: CORR, Y. LeCun, B. Boser, JS Denker, D. Henderson, RE Howard, W. Hubbard, LD Jackel. 1989. 손으로 쓴 우편번호 인식에 적용된 역전파. Neural Computation, 1(4):541–551. Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiangyang Li, Edward Lin, Tie-Yan Liu. 2021. Fastcorrect: 자동 음성 인식을 위한 편집 정렬을 통한 빠른 오류 수정. 신경 정보 처리 시스템의 발전, 34권, 21708–21719페이지. Curran Associates, Inc. Erik McDermott, Hasim Sak, Ehsan Variani. 2019. 엔드투엔드 자동 음성 인식에서 언어 모델 융합에 대한 밀도 비율 접근 방식. IEEE 자동 음성 인식 및 이해 워크숍(ASRU), 434-441페이지. Yajie Miao, Mohammad Abdelaziz Gowayyed, Florian Metze. 2015. Eesen: 딥 rnn 모델과 wfst 기반 디코딩을 사용한 엔드투엔드 음성 인식. 2015 IEEE 자동 음성 인식 및 이해 워크숍(ASRU), 167-174페이지. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur. 2015. Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP)에서. A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, Ilya Sutskever. 2019a. 언어 모델은 비지도 멀티태스크 학습기입니다. Alec Radford와 Karthik Narasimhan. 2018. 생성적 사전 학습을 통한 언어 이해 향상. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 외. 2019b. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9. Navid Rekabsaz, Simone Kopeinik, Markus Schedl. 2021. 검색된 콘텐츠의 사회적 편견: bert rankers의 측정 프레임워크와 적대적 완화. 정보 검색 연구 및 개발에 관한 제44회 국제 ACM SIGIR 컨퍼런스 회의록, SIGIR &#39;21, 306-316페이지, 미국 뉴욕. 컴퓨팅 기계 협회. Tara N. Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirkó Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu, Ian McGraw, Chung-Cheng Chiu. 2019. TwoPass End-to-End Speech Recognition. Proc. Interspeech 2019, 2773-2777쪽. Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff. 2020. Masked language model scoring. Association for Computational Linguistics의 제58회 연례 회의록, 2699-2712쪽, 온라인. Association for Computational Linguistics. R. Schwartz와 Steve Austin. 1991. 여러 (n-best) 문장 가설을 찾기 위한 여러 근사 알고리즘 비교. ICASSP 91: 음향, 음성 및 신호 처리 국제 컨퍼런스, 701-704페이지 1권. Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, Adam Coates. 2018. Cold Fusion: 언어 모델과 함께 Seq2Seq 모델 훈련. Proc. Interspeech 2018, 387-391페이지. Tzu-Wei Sung, Jun-You Liu, Hung-yi Lee, Linshan Lee. 2019. 2패스 디코딩을 통한 종단 간 음성-텍스트 변환을 향해. ICASSP-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 71757179페이지. Edmondo Trentin과 Marco Gori. 2001. 자동 음성 인식을 위한 하이브리드 ann/hmm 모델 조사. Neurocomputing, 37(1):91–126. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Ali Eslami, Oriol Vinyals 및 Felix Hill. 2021. 고정된 언어 모델을 사용한 다중 모달 퓨샷 학습. 신경 정보 처리 시스템의 발전, 34권, 200-212페이지. Curran Associates, Inc. 우다가와 타쿠마, 스즈키 마사유키, 쿠라타 가쿠토, 이토 노부야스, 조지 사온. 2022. 경쟁 ASR 시스템에 대한 대규모 언어 모델 재채점의 효과 및 분석. Proc에서 Interspeech 2022, 페이지 3919-3923. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30권.Curran Associates, Inc. Alex Wang과 Kyunghyun Cho.2019. BERT는 입이 있고 말해야 합니다: 마르코프 랜덤 필드 언어 모델로서의 BERT.신경 언어 생성 최적화 및 평가 방법에 대한 워크숍 회의록, 30-36페이지, 미네소타주 미니애폴리스.전산 언어학 협회.Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, Emmanuel Dupoux.2021. VoxPopuli: 표현 학습, 반지도 학습 및 해석을 위한 대규모 다국어 음성 코퍼스.전산 언어학 협회의 제59회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스 회의록. Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass decoding. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan, Ankur Gandhe, Ariya Rastrow, Andreas Stolcke, Ivan Bulyko. 2022. Rescorebert: bert를 사용한 Discrimi 네이티브 음성 인식 재스코어링. ICASSP 2022 - 2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 6117-6121페이지. Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, ShangWen Li, Shinji Watanabe, 압델라흐만 모하메드(Abdelrahman Mohamed), 이흥이(Hung yi Lee). 2021. 우수: 음성 처리 범용 성능 벤치마크. Proc에서 Interspeech 2021, 페이지 1194-1198. Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo 및 Shankar Kumar. 2020. 변환기 변환기: 변환기 인코더 및 rnn-t 손실을 갖춘 스트리밍 가능한 음성 인식 모델입니다. ICASSP 2020-2020 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 7829-7833페이지. Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, Deepti Bhatia, Bo Li, Ruoming Pang. 2019. Shallow-Fusion End-to-End Contextual Biasing. Proc. Interspeech 2019, 1418-1422페이지. Guolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiaodan Liang, Liang Lin. 2021. Wav-BERT: 저소득층 음성 인식을 위한 협력적 음향 및 언어 표현 학습. Findings of the Association for Computational Linguistics: EMNLP 2021, 2765-2777페이지, 도미니카 공화국 푼타카나. 계산언어학 협회. 부록 A SLURP SLU 의미론 및 NLU 모듈 SLURP 데이터 세트는 스마트 홈 가상 비서와의 사용자 상호작용으로 구성됩니다. 의미론은 시나리오, 동작 및 엔터티의 세 가지 수준의 의미론으로 주석이 달려 있습니다. 예를 들어, ASR 대본 &quot;칠면조를 만드는 법&quot;은 &quot;시나리오: 요리 | 동작: 레시피 | 엔터티: [(유형: 음식 | 필러: 칠면조)]&quot;라는 의미론으로 주석이 달려 있습니다. SLU 의미론은 18개의 서로 다른 시나리오, 정의된 동작 및 55개의 서로 다른 엔터티 유형(Bastianelli et al., 2020)에 걸쳐 있습니다. NLU 모듈에서 의미론 예측을 시퀀스 간 문제로 취급합니다. 구체적으로, &quot;칠면조를 만드는 법&quot;을 다시 채점한 후 ASR 대본이 주어지면 목표는 &quot;시나리오: 요리 | 동작: 레시피 | 엔터티: [(유형: 음식 | 필러: 칠면조)]&quot;를 예측하는 것입니다. NLU 모듈은 양방향 장단기 메모리(Bi-LSTM)를 기반으로 하는 인코더-디코더 구조를 가지고 있습니다. 인코더와 디코더 모두 은닉 차원 256을 가지고 있습니다. 인코더는 2개의 레이어를 가지고 있는 반면 디코더는 3개의 레이어를 가지고 있습니다. 우리는 시퀀스 예측을 위한 훈련 목표로 Negative Log-Likelihood(NLL) 손실을 사용합니다. 우리는 지상 진실에 따라 모델을 훈련합니다.<transcript, NLU semantics> SLURP 학습 데이터 세트의 paris. 학습률은 3e-4로 설정되고 학습은 배치 크기 16으로 20개 에포크 동안 수행됩니다. B 실험 설정 MATE에는 총 2억 1,700만 개의 매개변수가 있습니다. 마스크 언어 모델과 음성 인코더 모두 효율성을 위해 기본 크기 모델을 활용합니다(각각 BERT-Base 110M 및 WavLM-Base+ 95M). 합성곱 신경망에는 스트라이드(2, 1, 2)와 커널 너비(3, 1, 1)가 있는 채널이 있는 3개 계층이 있습니다. 병목 어댑터 계층의 압축 계수는 0.5입니다. MATE의 학습 실험은 종단 간으로 수행됩니다. 모든 모듈을 동시에 학습합니다. 학습률의 선형 감소가 있는 Adam 옵티마이저(Kingma 및 Ba, 2014)를 사용합니다. 초기 학습률을 5e-5로, 배치 크기를 32로 설정했습니다. Eq.2에서 하이퍼파라미터 a를 (1.0, 3.0, 10.0)으로 검색했고 최종 값은 1.0으로 설정했습니다. 학습은 88K 단계에 걸쳐 수행했습니다. 모든 실험은 단일 실행에서 NVIDIA Tesla V100 GPU로 수행했습니다. MATE에 대한 학습은 Tesla V100 8-GPU 머신에서 39.7시간이 걸립니다. 첫 번째 패스 ASR 모델은 conformerCTC(Gulati et al., 2020) 아키텍처를 가지고 있으며, 50,000시간 이상의 오디오-대본 쌍 데이터에서 학습했습니다. conformer 인코더는 은닉 차원이 2048인 conformer 블록의 20개 레이어로 구성되어 있는 반면, 얕은 디코더는 은닉 차원이 2048인 단일 Transformer 기반 레이어입니다. conformer-CTC 모델은 약 1억 4천만 개의 매개변수를 가지고 있습니다. WER 및 CWER 평가를 위해 SCTK 패키지를 사용합니다. CWER는 WER 계산과 동일한 논리를 가지고 있지만 함수어를 필터링합니다. SLU 의미 평가를 위해 SLURP 툴킷을 사용합니다. C 어텐션 시각화 그림 3에서 제안된 MATE 모델에서 추출한 학습된 자기 어텐션 플롯을 시각화합니다. 이 모델은 12개의 Transformer 레이어와 각 멀티헤드 자기 어텐션에 헤드가 있습니다. wsj_evaltest 세트의 샘플 발화를 사용하여 총 144개의 어텐션 플롯에서 6개의 대표 플롯을 선택했습니다. 입력 발화에는 음향 특징에 대한 33개의 토큰과 프레임이 있으며, 음향 특징은 BERT 모델에 입력되기 전에 어휘 임베딩에 추가됩니다. 관찰 결과는 다음과 같습니다. • (a) (b) (c) 및 (d) 플롯은 텍스트 입력과 오디오 입력의 경계(위치 32의 수직 직선)를 강조 표시합니다. MATE에 모달리티 경계 정보를 입력하지 않아도 두 모달리티의 경계를 스스로 학습할 수 있다는 결론을 내릴 수 있습니다. (a), (d), (e) 및 (f) 단조로운 오디오-텍스트 위치 정렬이 플롯에서 명확하게 표시됩니다. 이는 음향 및 어휘 표현이 하나의 통합된 특징 공간에 성공적으로 매핑되었음을 나타냅니다. 흥미롭게도 플롯 (a), (e) 및 (f)는 텍스트-오디오 위치 정렬도 MATE에서 학습할 수 있음을 보여줍니다. D 위험 제안된 시스템인 MATE는 사전 학습된 언어 모델(BERT)과 음성 모델(WavLM)을 모두 설계에 통합합니다. 영어: 이러한 사전 학습된 모델은 https://github.com/chinshr/sctk https://github.com/pswietojanski/slurp에 대한 편견과 고정관념을 포함할 수 있습니다.(a) 4층 머리(b) 6층 머리 &quot;복도를 청소해&quot;라는 올바른 발화에 대한 것입니다.80100-(c) 7층 머리(d) 8층 머리20-(e) 11층 머리(f) 11층 머리그림 3: 12층 BERT 인코더의 자기 주의 층에서 선택된 주의 플롯 샘플 발화(wsj_eval93에서 가져옴)에는 총 110개의 프레임이 포함되어 있습니다. 처음 33개 프레임은 어휘 임베딩이고 그 뒤에 음향 임베딩 프레임이 옵니다. 발화는 다음과 같습니다. &quot;작년에 뉴햄프셔는 주 외부 은행이 뉴햄프셔 은행을 인수할 수 있도록 하는 법률을 제정했지만 법안의 제한으로 인해 특정 종교, 인종 및 성별 그룹의 잠재적 구매자가 낙담했습니다(Rekabsaz et al., 2021; Delobelle et al., 2022). E 정성적 사례 제안된 접근 방식인 MATE가 더 정확한 예측을 제공하는 이유를 더 잘 이해하기 위해 평가 세트에서 몇 가지 대표적인 사례를 선택했습니다. 표 3은 MATE가 n-best 목록에 있는 더 많은 어휘 또는 문법 오류를 수정하는 경향이 있음을 분명히 보여줍니다. MATE는 텍스트 정보만으로는 해결할 수 없는 많은 ASR 오류를 수정할 수 있음을 관찰했습니다. SLURP의 예에서 &quot;who in the hallway&quot;와 &quot;hoover the hallway&quot;는 모두 일상 대화의 비공식적인 스타일에서 그럴듯한 발화입니다. 음향 정보의 도움으로 MATE는 더 높은 점수를 할당할 수 있습니다.데이터 집합 SLURP Ground Truth Voxpopuli BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 Ground Truth BERT-text에 의해 1-최고로 재점수됨 MATE에 의해 1-최고로 재점수됨 MTDialogue Ground Truth ConvAI BERT-text에 의해 1-best로 재점수됨 MATE에 의해 1-best로 재점수됨 Ground Truth BERT-text에 의해 1-best로 재점수됨 MATE에 의해 1-best로 재점수됨 Ground Truth BERT-text에 의해 1-best로 재점수됨 MATE에 의해 1-best로 재점수됨 Ground Truth BERT-text에 의해 1-best로 재점수됨 MATE에 의해 1-best로 재점수됨 발화 화요일 오전 9시 알람 제거 오전 9시 알람으로 이동 화요일 오전 9시 알람 제거 복도 청소 복도에 있는 사람 복도 청소 수요일 업무 회의 취소 수요일 협의회 업무 회의 취소 수요일 업무 회의 취소 델타에 다시는 사용하지 않을 거라고 알려줄 수 있어 의사에게 다시는 사용하지 않을 거라고 알려줄 수 있어 델타에 다시는 사용하지 않을 거라고 알려줄 수 있어 피파 세븐틴을 하고 싶어 세븐틴으로 떠나고 싶어 피파 세븐틴을 하고 싶어 프린지에 대해 뭘 알고 있어 내년 에든버러에서 당신은 내년 에든버러의 프랑스어에 대해 얼마나 알고 있습니까?내년 에든버러의 프린지에 대해 무엇을 알고 있습니까?예를 들어 보고서는 법치주의와 부패에 대해 이야기합니다.예를 들어 보고서는 부패에 대한 법치주의에 대해 이야기합니다.예를 들어 보고서는 법치주의와 부패에 대해 이야기합니다.나는 그들을 만났습니다.그들은 젊고 유능하며 비전이 있습니다.나는 그들을 만났습니다.그들은 젊고 유능하며 선교사입니다.나는 그들을 만났습니다.그들은 젊고 유능하며 비전이 있습니다.숨겨졌습니다.숨겨졌습니다.숨겨졌습니다.그녀는 얼마를 지불했습니까?그는 얼마를 지불했습니까?그녀는 얼마를 지불했습니까?그린스버러의 국수 상자가 왜 건강 검사에 떨어졌습니까?그린스버러의 국수 상자가 왜 건강 검사에 떨어졌습니까?그린스버러의 국수 상자가 왜 건강 검사에 떨어졌습니까?면세점에 대해 말해 주세요.면세점에 대해 말해 주세요.면세점에 대해 말해 주세요.표 3: 정성적 예: BERT 텍스트 모델과 MATE의 1-베스트 출력을 기준 진실과 비교하여 대조합니다. MATE가 콘텐츠 단어와 슬롯 엔터티의 인식을 향상시키는 것을 확인할 수 있습니다.
