--- ABSTRACT ---
문서 이해는 웹 페이지와 같은 다양한 유형의 디지털 문서에서 정보를 자동으로 추출, 분석 및 이해하는 것을 말합니다. mPLUG-Owl을 포함한 기존의 다중 모델 대규모 언어 모델(MLLM)은 얕은 OCR 없는 텍스트 인식에서 유망한 제로샷 기능을 보여주었으며, 이는 OCR 없는 문서 이해에 대한 잠재력을 나타냅니다. 그럼에도 불구하고 도메인 내 학습이 없으면 이러한 모델은 OCR 없는 문서 이해에 필수적인 정교한 표나 큰 텍스트 블록과 같은 세분화된 OCR 기능을 무시하는 경향이 있습니다. 이 논문에서는 OCR 없는 문서 이해를 위해 mPLUG-Owl을 기반으로 하는 mPLUG-DocOwl을 제안합니다. 구체적으로, 먼저 광범위한 시각 텍스트 이해 작업을 특징으로 하는 명령어 튜닝 데이터 세트를 구성합니다. 그런 다음 통합 명령어 튜닝 전략으로 언어 전용, 일반 시각 및 언어 및 문서 명령어 튜닝 데이터 세트에서 모델을 공동으로 학습하여 OCR 없는 문서 이해 능력을 강화합니다. 또한 OCR 없는 문서 명령어 이해 평가 세트 LLMDoc를 구축하여 명령어 준수 및 문서 이해에 대한 모델의 기능을 더 잘 비교합니다. 실험 결과에 따르면, 저희 모델은 기존 멀티모달 모델보다 성능이 뛰어나 문서 이해 능력이 뛰어납니다. 게다가, 특정 미세 조정 없이도 mPLUG-DocOwl은 다양한 다운스트림 작업에서 잘 일반화됩니다. 저희 코드, 모델, 학습 데이터, 평가 세트는 https://github.com/X-PLUG/mPLUG-DocOwl에서 제공됩니다. 1
--- INTRODUCTION ---
ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], LLAMA [Touvron et al., 2023]와 같은 대규모 언어 모델(LLM)은 다양한 언어 응용 프로그램에서 인상적인 제로 샷 기능을 자랑하며 일반 인공 지능의 실현을 가능하게 하기 위해 빠르게 개발되었습니다. LLM을 언어 디코더로 사용하여 MiniGPT-4 [Zhu et al., 2023], LLAVA [Liu et al., 2023a], mPLUG-Owl [Ye et al., 2023]과 같은 멀티모달 대규모 언어 모델(MLLM)은 다양한 개방형 비전 및 언어 작업에서 놀라운 제로 샷 성능을 보여주었습니다. 이러한 모델은 사전 훈련 단계에서 텍스트와 이미지를 정렬하도록 훈련된 다음 명령어 튜닝 단계에서 다양한 기능을 촉진하도록 훈련됩니다. 흥미롭게도, 이러한 MLLM은 시각적 텍스트 이해 데이터 세트에 대한 명시적 훈련 없이 피상적인 OCR 없는 텍스트 인식 능력을 보입니다 [Ye et al., 2023, Liu et al., 2023b]. 그럼에도 불구하고, 특정 훈련이 부족하기 때문에 이러한 모델은 차트, 문서 및 웹 페이지와 같은 다양한 유형의 이미지에서 시각적 텍스트와 개체 간의 복잡한 관계를 이해하는 과제에 여전히 직면해 있습니다. *동등한 기여 *연락처 사전 인쇄본.심사 중.mPLUG-Owl [Ye et al., 2023]에서 문서 이해를 위한 통합 명령어 튜닝을 수행함으로써, 우리는 모듈화된 MLLM [Li et al., 2022, Xu et al., 2023b], 즉 mPLUG-DocOwl을 제안합니다. 우리의 접근 방식은 mPLUG-Owl [Ye et al., 2023]과 유사한 모듈화된 프레임워크를 활용하는데, 이는 시각적 추상화 모듈을 통합하여 사전 훈련된 LLM을 시각적 지식 모듈과 연결하여 텍스트와 이미지의 정렬을 달성합니다. 다양한 문서 이해 능력을 향상시키기 위해 동일한 형태의 지침으로 다양한 다운스트림 문서 이해 작업을 재구성합니다. 일반적인 단일/다중 모달 능력을 유지하기 위해 mPLUG-Owl이 mPLUG-DocOwl을 훈련하는 데 사용하는 언어 전용 및 일반적인 시각 및 언어 지침 데이터 세트도 포함합니다. 훈련하는 동안 시각적 지식 모듈과 LLM 디코더는 모두 동결되고 LLM의 시각적 추상화와 Low-Rank Adaption(LoRA) [Hu et al., 2022]만 미세 조정됩니다. mPLUG-DocOwl은 일반적으로 사용되는 여러 문서 이해 데이터 세트에서 OCR 없는 최첨단 성능을 달성합니다. 또한, 신중하게 구축된 문서 지시 이해 평가 세트 LLMDoc에 대한 실험은 mPLUG-DocOwl이 기존 MLMMS보다 다양한 도메인에서 상당히 더 나은 시각적 텍스트 이해 성능을 달성한다는 것을 보여줍니다. 우리의 주요 기여는 다음과 같이 강조될 수 있습니다. • 우리는 언어 전용, 일반 시각 및 언어, 통합 지시 튜닝을 기반으로 한 문서 이해를 균형 있게 조정한 최초의 모듈화된 MLLM인 mPLUG-DocOwl을 제안합니다. •⚫ 우리는 다양한 문서 이해 능력을 평가하기 위해 LLMDoc이라는 인간 평가가 있는 지시 이해 테스트 세트를 신중하게 구축합니다. • 경험적 결과는 우리의 mPLUG-DocOwl이 여러 표준 벤치마크와 LLMDoc를 포함하여 OCR 없는 문서 이해에 대한 기존 방법을 능가한다는 것을 보여줍니다. 2
--- RELATED WORK ---
2. 시각적 텍스트 이해 풍부한 텍스트 정보가 포함된 이미지를 이해하기 위한 두 가지 유형의 모델이 있습니다.첫 번째 종류의 접근 방식[Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021]은 기성품 OCR 모델 또는 API를 사용하여 이미지에서 텍스트를 인식한 다음 사전 학습 작업을 설계하여 시각적 및 텍스트 입력 간의 교차 모달리티 정렬을 용이하게 합니다.반면에 엔드투엔드 접근 방식[Davis et al., 2022, Kim et al., 2022, Lee et al., 2022]은 사전 학습 단계에서 고해상도 이미지 인코더를 사용하여 텍스트 인식을 학습합니다.두 가지 유형의 모델 모두 다른 다운스트림 데이터 세트에 대한 특정 미세 조정에 의존하며 다중 모달 대규모 언어 모델과 같은 오픈 도메인 명령어 이해 성능을 달성할 수 없습니다. 2.2 멀티모달 대규모 언어 모델 대규모 언어 모델(LLM)은 다양한 개방형 작업에서 인상적인 제로샷 능력을 보여주었습니다. 최근 연구에서는 체계적 협업과 종단 간 훈련된 모델이라는 두 가지 패러다임을 활용하여 멀티모달 생성을 위한 LLM 적용도 탐구했습니다. Visual ChatGPT[Wu et al., 2023] 및 MM-REACT[Yang et al., 2023]와 같은 체계적 협업 방식은 다양한 비전 전문가 또는 도구를 활용하여 텍스트 설명으로 시각적 정보를 표현합니다. 그런 다음 ChatGPT[OpenAI, 2022]와 같은 LLM은 에이전트 역할을 하여 시각적 이해를 위한 적절한 전문가와 도구를 선택할 수 있습니다. 마지막으로 LLM은 이러한 전문가의 출력을 요약하여 사용자 쿼리에 답합니다. 반면, MiniGPT-[Zhu et al., 2023], LLAVA [Liu et al., 2023a], mPLUG-Owl [Ye et al., 2023]과 같은 일부 접근 방식은 제한된 연결 매개변수를 사용하여 다중 모달리티에 대한 통합 모델을 구축하기 위해 LLM을 활용합니다. 이러한
--- METHOD ---
s는 여러 표준 벤치마크와 LLMDoc를 포함하여 OCR 없는 문서 이해에 관한 것입니다.2 관련 연구 2. 시각적 텍스트 이해 풍부한 텍스트 정보가 포함된 이미지를 이해하기 위한 모델에는 두 가지 유형이 있습니다.첫 번째 종류의 접근 방식[Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021]은 기성품 OCR 모델 또는 API를 사용하여 이미지에서 텍스트를 인식한 다음 시각적 및 텍스트 입력 간의 교차 모달리티 정렬을 용이하게 하기 위해 사전 학습 작업을 설계합니다.반면에 엔드투엔드 접근 방식[Davis et al., 2022, Kim et al., 2022, Lee et al., 2022]은 사전 학습 단계에서 고해상도 이미지 인코더를 사용하여 텍스트 인식을 학습합니다. 두 가지 유형의 모델 모두 다른 다운스트림 데이터 세트에 대한 특정 미세 조정에 의존하며 멀티모달 대규모 언어 모델과 같은 오픈 도메인 명령어 이해 성능을 달성할 수 없습니다.2.2 멀티모달 대규모 언어 모델 대규모 언어 모델(LLM)은 다양한 오픈 엔드 작업에서 인상적인 제로샷 능력을 보여주었습니다.최근 연구에서는 체계적 협업과 엔드투엔드 훈련 모델이라는 두 가지 패러다임을 활용하여 멀티모달 생성을 위한 LLM 적용도 탐구했습니다.Visual ChatGPT[Wu et al., 2023] 및 MM-REACT[Yang et al., 2023]와 같은 체계적 협업 방식은 다양한 비전 전문가 또는 도구를 활용하여 텍스트 설명으로 시각적 정보를 표현합니다.그러면 ChatGPT[OpenAI, 2022]와 같은 LLM이 에이전트 역할을 하여 시각적 이해를 위한 적절한 전문가와 도구를 선택할 수 있습니다.마지막으로 LLM은 이러한 전문가의 출력을 요약하여 사용자 쿼리에 답합니다. 반면, MiniGPT-[Zhu et al., 2023], LLAVA [Liu et al., 2023a], mPLUG-Owl [Ye et al., 2023]과 같은 일부 접근 방식은 LLM을 활용하여 제한된 연결 매개변수를 사용하여 다중 모달을 위한 통합 모델을 구축합니다. 이러한 방법은 제로 샷 설정에서 피상적인 OCR 없는 텍스트 인식 능력을 보여줍니다. 그러나 도메인 내 훈련이 부족하여 복잡한 문서 이해의 경우 다양한 이미지 유형을 처리하고, 풍부한 텍스트를 인식하고, 시각적 의미와 텍스트 정보 간의 관계를 이해하는 데 어려움을 겪습니다. 이 작업에서 통합 명령어 튜닝을 통해 mPLUG-DocOwl은 훨씬 더 나은 문서 이해 성능을 달성하고 일반적인 단일/다중 모달 능력을 유지합니다. 자연 이미지 문서 차트 CALEA DOROBANTILOR 인간: 2019년 홍콩에서 가장 바빴던 통제점은 어디였습니까? Al: Lo Wu 제어점 인간: CID#에 언급된 값은 무엇인가: AI: 736 00Human 제시된 이미지를 나타내는 간결한 내러티브를 만드십시오.AI: 파란색과 흰색으로 Calea Dorobantilor라고 쓰인 표지판 다중 모드 사전 학습 mPLUG-Owl 통합 지침 튜닝 차트 문서 표 자연스러운 이미지 웹 페이지 일반 V&amp;L 언어 전용 mPLUG-DocOwl 표 인간: juan román riquelme는 2007년 7월 11일, 11월 17일에 같은 점수를 받았습니다. 예 또는 아니요?Al: 예 웹 페이지 인간: 이 프레젠테이션은 AI에 대한 토론을 장려하기 위해 무엇을 목표로 합니까?오픈 데이터 소유권과 다양한 사회적 권리에 대한 토론을 장려하는 것을 목표로 합니다.언어 전용 인간: 복잡한 추론을 요구하고 문제를 여러 단계로 분해해야 하는 자세한 답변을 제공해 주시겠습니까?AI: 네, 물론 귀하의 요청을 충족시킬 수 있습니다. 자세히 설명해 주었으면 하는 구체적인 질문이나 문제를 제공해 주십시오.그림 1: mPLUG-DocOwl의 지침 튜닝 패러다임 요약. 3 mPLUG-DocOwl 3.1 아키텍처 mPLUG-DocOwl의 아키텍처는 인기 있는 멀티모달 언어 모델인 mPLUGOwl[Ye et al., 2023]을 기반으로 하며, 이는 사전 훈련된 시각적 기초 모델, 시각적 추상화기, 언어 기초 모델로 구성됩니다. 시각적 기초 모델은 입력 이미지에서 시각적 특징을 추출하고, 시각적 추상화기는 학습 가능한 토큰 세트를 사용하여 이러한 특징을 추출합니다. 그런 다음 결과 시각적 특징을 입력 문장의 단어 임베딩과 연결하여 언어 모델에 입력하여 응답을 생성합니다. 이 강력한 아키텍처는 정확하고 효율적인 멀티모달 언어 처리를 가능하게 합니다. mPLUG-Owl[Ye et al., 2023]은 눈에 띄는 텍스트가 포함된 이미지가 표시되면 표면적인 OCR 기능을 보여줍니다. 여기에서 영감을 얻어 문서, 표, 차트, 자연스러운 이미지 및 웹 페이지를 포함하여 더 나은 문서 이해 성능을 위해 문서 지침 튜닝 데이터로 모델을 더욱 미세 조정할 것을 제안합니다. 미세 조정하는 동안 시각적 인코더와 언어 모델을 동결하고 시각적 추상화를 훈련합니다. 또한 언어 모델의 능력을 향상시키기 위해 저순위 적응 접근 방식(LORA) [Hu et al., 2022]을 채택합니다. 3.2 명령어 조정 데이터 이 섹션에서는 명령어 조정 데이터의 구성을 자세히 소개합니다. mPLUG-DocOwl의 다재다능함을 보장하기 위해 시각적 질문 답변(VQA) [Antol et al., 2015], 정보 추출(IE), 자연어 추론(NLI) [Bowman et al., 2015] 및 이미지 캡션(IC)을 포함하여 다양한 작업 형식의 다양한 문서 이해 데이터 세트를 수집합니다. mPLUG-Owl [Ye et al., 2023]은 &quot;<image> 인간: {질문} AI: {답변}&quot;. 이 작업에서 우리는 다음과 같이 {질문}과 {답변} 플레이스홀더를 대체하여 다양한 문서 이해 작업을 mPLUG-Owl [Ye et al., 2023]과 동일한 형식으로 변환합니다. 시각적 질의응답 우리는 단순히 원시 질문과 답변을 {질문}과 {답변} 플레이스홀더로 사용합니다. 우리는 ChartQA [Masry et al., 2022], DocVQA [Mathew et al., 2021], InfographicsVQA (InfoVQA) [Mathew et al., 2022], Document Table DocVQA WTQ Chart ChartQA General V&amp;L LLAVA TabFact InfoVQA Language-only Natural Image Alpaca DeepForm WebPage TextVQA Vicuna KLC VisualMRC TextCaps Baize를 포함한 다양한 도메인에서 VQA 데이터 세트를 수집합니다. 그림 2: 학습에 사용되는 다양한 유형의 데이터 세트 mPLUG-DocOwl. WikiTableQuestions(WTQ) [Pasupat 및 Liang, 2015], TextVQA [Singh 등, 2019] 및 VisualMRC [Tanaka 등, 2021]. 정보 추출에는 모델이 입력 이미지에서 키-값 쌍을 추출해야 합니다. &#39;키&#39;(또는 &#39;범주&#39;)는 항상 고정된 집합입니다. 이 작업을 명령어 튜닝 형식으로 변환하기 위해 값을 {answer}로 취급하고 {question}을 {key}의 값은 무엇입니까?&#39;로 구성합니다. 키가 이미지에 없으면 {answer}는 &#39;없음&#39;으로 설정됩니다. DeepForm [Svetlichnaya, 2020] 및 Kleister Charity(KLC) [Stanislawek 등, 2021]에서 정보 추출 데이터를 수집합니다. 자연어 추론은 &#39;Entailed&#39; 및 &#39;Refuted&#39; 레이블이 있는 이진 분류 작업입니다. 주어진 진술에 대해, 우리는 {질문}을 &#39;{진술}, 예 또는 아니오?&#39;로 구성합니다. {답변}은 &#39;예&#39; 또는 &#39;아니오&#39;이며 각각 &#39;수반됨&#39; 또는 &#39;반박됨&#39;을 나타냅니다. 테이블에 대한 자연어 추론 데이터 세트인 TabFact[Chen et al., 2020]가 명령어 튜닝을 위해 선택되었습니다. 이미지 캡션은 유창한 언어로 이미지를 간략하게 설명하는 것을 목표로 합니다. 우리는 캡션을 {답변}으로 취급하고 LLaVa[Liu et al., 2023a]와 같이 프롬프트를 {질문}으로 무작위로 선택합니다. TextCaps[Sidorov et al., 2020]는 텍스트가 있는 자연 이미지에 대한 적절한 캡션 데이터 세트입니다. 언어 전용 및 일반 시각 및 언어 명령어 튜닝. 모델의 언어 이해 및 다중 모달 개방형 대화 능력을 강화하기 위해 mPLUGOwl [Ye et al., 2023]을 따라 언어 전용 및 일반 시각 및 언어 지시 튜닝 데이터 [Taori et al., 2023, Vicuna, 2023, Xu et al., 2023a, Liu et al., 2023a]를 도입했습니다. 그림 2는 데이터 세트 유형별로 그룹화된 지시 튜닝 데이터의 구성을 보여줍니다. 이러한 데이터 세트의 교육 세트를 지시 튜닝 데이터로 사용하고 테스트 세트에서 모델을 평가합니다. 3.3 교육 세부 정보 Vision Transformer와 Language 모델이 동결된 2단계 교육 패러다임을 채택했습니다. 첫 번째 단계에서는 언어 모델의 시각적 추상화기와 LORA [Hu et al., 2022]가 모두 미세 조정됩니다. 첫 번째 단계에서는 문서 이해 데이터만 사용하고 10개의 에포크가 걸립니다. 두 번째 단계에서는 시각적 추상화를 더욱 동결하고 LORA만 훈련합니다. 문서 이해 데이터 외에도 이 단계에서는 언어 전용 및 일반 시각 및 언어 교육 튜닝 데이터가 추가로 도입되고 6회 업샘플링됩니다. 두 번째 단계는 3개의 에포크가 걸립니다. 다른 훈련 하이퍼 매개변수는 mPLUG-Owl과 동일합니다[Ye et al., 2023]. 4
--- EXPERIMENT ---
모든 결과에 따르면 우리 모델은 기존 멀티모달 모델보다 성능이 뛰어나 문서 이해 능력이 강력하다는 것이 입증되었습니다. 게다가 특정 미세 조정 없이도 mPLUG-DocOwl은 다양한 다운스트림 작업에서 일반화됩니다. 코드, 모델, 학습 데이터 및 평가 세트는 https://github.com/X-PLUG/mPLUG-DocOwl에서 제공됩니다. 1 서론 ChatGPT[OpenAI, 2022], BLOOM[Scao et al., 2022], LLAMA[Touvron et al., 2023]와 같은 대규모 언어 모델(LLM)은 다양한 언어 응용 프로그램에서 인상적인 제로샷 기능을 자랑하며 일반 인공 지능을 실현할 수 있도록 빠르게 개발되었습니다. LLM을 언어 디코더로 사용하여 MiniGPT-4 [Zhu et al., 2023], LLAVA [Liu et al., 2023a], mPLUG-Owl [Ye et al., 2023]과 같은 다중 모달 대규모 언어 모델(MLLM)은 다양한 개방형 시각 및 언어 작업에서 놀라운 제로 샷 성능을 보여주었습니다. 이러한 모델은 사전 훈련 단계에서 텍스트와 이미지를 정렬하도록 훈련된 다음 지침 조정 단계에서 다양한 능력을 발휘하도록 훈련됩니다. 흥미롭게도 이러한 MLLM은 시각적 텍스트 이해 데이터 세트에 대한 명시적 훈련 없이도 피상적인 OCR 없는 텍스트 인식 능력을 보입니다 [Ye et al., 2023, Liu et al., 2023b]. 그럼에도 불구하고 이러한 모델은 특정 훈련이 부족하기 때문에 차트, 문서, 웹 페이지와 같은 다양한 유형의 이미지에서 시각적 텍스트와 객체 간의 복잡한 관계를 이해하는 과제에 여전히 직면해 있습니다. *동등한 기여 *연락처 저자 사전 인쇄본. 검토 중. mPLUG-Owl에 대한 문서 이해를 위한 통합 지침 튜닝을 수행함으로써 [Ye et al., 2023], 우리는 또한 모듈화된 MLLM [Li et al., 2022, Xu et al., 2023b], 즉 mPLUG-DocOwl을 제안합니다. 우리의 접근 방식은 mPLUG-Owl [Ye et al., 2023]과 유사한 모듈화된 프레임워크를 활용하는데, 이는 시각적 추상화 모듈을 통합하여 사전 훈련된 LLM을 시각적 지식 모듈과 연결하여 텍스트와 이미지의 정렬을 달성합니다. 다양한 문서 이해 능력을 향상시키기 위해, 우리는 동일한 형태의 지침으로 다양한 다운스트림 문서 이해 작업을 재구성합니다. 일반적인 단일/다중 모달 능력을 유지하기 위해, 우리는 mPLUG-Owl이 mPLUG-DocOwl을 훈련하는 데 사용하는 언어 전용 및 일반적인 시각 및 언어 지침 데이터 세트도 포함합니다. 학습하는 동안 시각적 지식 모듈과 LLM 디코더는 모두 동결되고, LLM의 시각적 추상화기와 Low-Rank Adaption(LoRA) [Hu et al., 2022]만 미세 조정됩니다. mPLUG-DocOwl은 일반적으로 사용되는 여러 문서 이해 데이터 세트에서 OCR 없는 최첨단 성능을 달성합니다. 더욱이, 신중하게 구축된 문서 지시 이해 평가 세트 LLMDoc에 대한 실험은 mPLUG-DocOwl이 기존 MLMMS보다 다양한 도메인에서 상당히 더 나은 시각적 텍스트 이해 성능을 달성한다는 것을 보여줍니다. 우리의 주요 기여는 다음과 같습니다. • 우리는 언어 전용, 일반 시각 및 언어, 통합 지시 조정을 기반으로 한 문서 이해를 균형 있게 조정한 최초의 모듈화된 MLLM인 mPLUG-DocOwl을 제안합니다. •⚫ 우리는 다양한 문서 이해 능력을 평가하기 위해 LLMDoc이라는 인간 평가가 있는 지시 이해 테스트 세트를 신중하게 구성합니다. • 경험적 결과는 우리의 mPLUG-DocOwl이 여러 표준 벤치마크 및 LLMDoc를 포함한 OCR 없는 문서 이해에 대한 기존 방법을 능가한다는 것을 보여줍니다.2 관련 연구 2. 시각적 텍스트 이해 풍부한 텍스트 정보가 포함된 이미지를 이해하기 위한 모델에는 두 가지 유형이 있습니다.첫 번째 종류의 접근 방식[Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021]은 기성품 OCR 모델 또는 API를 사용하여 이미지에서 텍스트를 인식한 다음 시각적 및 텍스트 입력 간의 교차 모달리티 정렬을 용이하게 하기 위해 사전 학습 작업을 설계합니다.반면에 엔드투엔드 접근 방식[Davis et al., 2022, Kim et al., 2022, Lee et al., 2022]은 사전 학습 단계에서 고해상도 이미지 인코더를 사용하여 텍스트 인식을 학습합니다. 두 가지 유형의 모델 모두 다른 다운스트림 데이터 세트에 대한 특정 미세 조정에 의존하며 멀티모달 대규모 언어 모델과 같은 오픈 도메인 명령어 이해 성능을 달성할 수 없습니다.2.2 멀티모달 대규모 언어 모델 대규모 언어 모델(LLM)은 다양한 오픈 엔드 작업에서 인상적인 제로샷 능력을 보여주었습니다.최근 연구에서는 체계적 협업과 엔드투엔드 훈련 모델이라는 두 가지 패러다임을 활용하여 멀티모달 생성을 위한 LLM 적용도 탐구했습니다.Visual ChatGPT[Wu et al., 2023] 및 MM-REACT[Yang et al., 2023]와 같은 체계적 협업 방식은 다양한 비전 전문가 또는 도구를 활용하여 텍스트 설명으로 시각적 정보를 표현합니다.그러면 ChatGPT[OpenAI, 2022]와 같은 LLM이 에이전트 역할을 하여 시각적 이해를 위한 적절한 전문가와 도구를 선택할 수 있습니다.마지막으로 LLM은 이러한 전문가의 출력을 요약하여 사용자 쿼리에 답합니다. 반면, MiniGPT-[Zhu et al., 2023], LLAVA [Liu et al., 2023a], mPLUG-Owl [Ye et al., 2023]과 같은 일부 접근 방식은 LLM을 활용하여 제한된 연결 매개변수를 사용하여 다중 모달을 위한 통합 모델을 구축합니다. 이러한 방법은 제로 샷 설정에서 피상적인 OCR 없는 텍스트 인식 능력을 보여줍니다. 그러나 도메인 내 훈련이 부족하여 복잡한 문서 이해의 경우 다양한 이미지 유형을 처리하고, 풍부한 텍스트를 인식하고, 시각적 의미와 텍스트 정보 간의 관계를 이해하는 데 어려움을 겪습니다. 이 작업에서 통합 명령어 튜닝을 통해 mPLUG-DocOwl은 훨씬 더 나은 문서 이해 성능을 달성하고 일반적인 단일/다중 모달 능력을 유지합니다. 자연 이미지 문서 차트 CALEA DOROBANTILOR 인간: 2019년 홍콩에서 가장 바빴던 통제점은 어디였습니까? Al: Lo Wu 제어점 인간: CID#에 언급된 값은 무엇인가: AI: 736 00Human 제시된 이미지를 나타내는 간결한 내러티브를 만드십시오.AI: 파란색과 흰색으로 Calea Dorobantilor라고 쓰인 표지판 다중 모드 사전 학습 mPLUG-Owl 통합 지침 튜닝 차트 문서 표 자연스러운 이미지 웹 페이지 일반 V&amp;L 언어 전용 mPLUG-DocOwl 표 인간: juan román riquelme는 2007년 7월 11일, 11월 17일에 같은 점수를 받았습니다. 예 또는 아니요?Al: 예 웹 페이지 인간: 이 프레젠테이션은 AI에 대한 토론을 장려하기 위해 무엇을 목표로 합니까?오픈 데이터 소유권과 다양한 사회적 권리에 대한 토론을 장려하는 것을 목표로 합니다.언어 전용 인간: 복잡한 추론을 요구하고 문제를 여러 단계로 분해해야 하는 자세한 답변을 제공해 주시겠습니까?AI: 네, 물론 귀하의 요청을 충족시킬 수 있습니다. 자세히 설명해 주었으면 하는 구체적인 질문이나 문제를 제공해 주십시오.그림 1: mPLUG-DocOwl의 지침 튜닝 패러다임 요약. 3 mPLUG-DocOwl 3.1 아키텍처 mPLUG-DocOwl의 아키텍처는 인기 있는 멀티모달 언어 모델인 mPLUGOwl[Ye et al., 2023]을 기반으로 하며, 이는 사전 훈련된 시각적 기초 모델, 시각적 추상화기, 언어 기초 모델로 구성됩니다. 시각적 기초 모델은 입력 이미지에서 시각적 특징을 추출하고, 시각적 추상화기는 학습 가능한 토큰 세트를 사용하여 이러한 특징을 추출합니다. 그런 다음 결과 시각적 특징을 입력 문장의 단어 임베딩과 연결하여 언어 모델에 입력하여 응답을 생성합니다. 이 강력한 아키텍처는 정확하고 효율적인 멀티모달 언어 처리를 가능하게 합니다. mPLUG-Owl[Ye et al., 2023]은 눈에 띄는 텍스트가 포함된 이미지가 표시되면 표면적인 OCR 기능을 보여줍니다. 여기에서 영감을 얻어 문서, 표, 차트, 자연스러운 이미지 및 웹 페이지를 포함하여 더 나은 문서 이해 성능을 위해 문서 지침 튜닝 데이터로 모델을 더욱 미세 조정할 것을 제안합니다. 미세 조정하는 동안 시각적 인코더와 언어 모델을 동결하고 시각적 추상화를 훈련합니다. 또한 언어 모델의 능력을 향상시키기 위해 저순위 적응 접근 방식(LORA) [Hu et al., 2022]을 채택합니다. 3.2 명령어 조정 데이터 이 섹션에서는 명령어 조정 데이터의 구성을 자세히 소개합니다. mPLUG-DocOwl의 다재다능함을 보장하기 위해 시각적 질문 답변(VQA) [Antol et al., 2015], 정보 추출(IE), 자연어 추론(NLI) [Bowman et al., 2015] 및 이미지 캡션(IC)을 포함하여 다양한 작업 형식의 다양한 문서 이해 데이터 세트를 수집합니다. mPLUG-Owl [Ye et al., 2023]은 &quot;<image> 인간: {질문} AI: {답변}&quot;. 이 작업에서 우리는 다음과 같이 {질문}과 {답변} 플레이스홀더를 대체하여 다양한 문서 이해 작업을 mPLUG-Owl [Ye et al., 2023]과 동일한 형식으로 변환합니다. 시각적 질의응답 우리는 단순히 원시 질문과 답변을 {질문}과 {답변} 플레이스홀더로 사용합니다. 우리는 ChartQA [Masry et al., 2022], DocVQA [Mathew et al., 2021], InfographicsVQA (InfoVQA) [Mathew et al., 2022], Document Table DocVQA WTQ Chart ChartQA General V&amp;L LLAVA TabFact InfoVQA Language-only Natural Image Alpaca DeepForm WebPage TextVQA Vicuna KLC VisualMRC TextCaps Baize를 포함한 다양한 도메인에서 VQA 데이터 세트를 수집합니다. 그림 2: 학습에 사용되는 다양한 유형의 데이터 세트 mPLUG-DocOwl. WikiTableQuestions(WTQ) [Pasupat 및 Liang, 2015], TextVQA [Singh 등, 2019] 및 VisualMRC [Tanaka 등, 2021]. 정보 추출에는 모델이 입력 이미지에서 키-값 쌍을 추출해야 합니다. &#39;키&#39;(또는 &#39;범주&#39;)는 항상 고정된 집합입니다. 이 작업을 명령어 튜닝 형식으로 변환하기 위해 값을 {answer}로 취급하고 {question}을 {key}의 값은 무엇입니까?&#39;로 구성합니다. 키가 이미지에 없으면 {answer}는 &#39;없음&#39;으로 설정됩니다. DeepForm [Svetlichnaya, 2020] 및 Kleister Charity(KLC) [Stanislawek 등, 2021]에서 정보 추출 데이터를 수집합니다. 자연어 추론은 &#39;Entailed&#39; 및 &#39;Refuted&#39; 레이블이 있는 이진 분류 작업입니다. 주어진 진술에 대해, 우리는 {질문}을 &#39;{진술}, 예 또는 아니오?&#39;로 구성합니다. {답변}은 &#39;예&#39; 또는 &#39;아니오&#39;이며 각각 &#39;수반됨&#39; 또는 &#39;반박됨&#39;을 나타냅니다. 테이블에 대한 자연어 추론 데이터 세트인 TabFact[Chen et al., 2020]가 명령어 튜닝을 위해 선택되었습니다. 이미지 캡션은 유창한 언어로 이미지를 간략하게 설명하는 것을 목표로 합니다. 우리는 캡션을 {답변}으로 취급하고 LLaVa[Liu et al., 2023a]와 같이 프롬프트를 {질문}으로 무작위로 선택합니다. TextCaps[Sidorov et al., 2020]는 텍스트가 있는 자연 이미지에 대한 적절한 캡션 데이터 세트입니다. 언어 전용 및 일반 시각 및 언어 명령어 튜닝. 모델의 언어 이해 및 다중 모달 개방형 대화 능력을 강화하기 위해 mPLUGOwl [Ye et al., 2023]을 따라 언어 전용 및 일반 시각 및 언어 지시 튜닝 데이터 [Taori et al., 2023, Vicuna, 2023, Xu et al., 2023a, Liu et al., 2023a]를 도입했습니다. 그림 2는 데이터 세트 유형별로 그룹화된 지시 튜닝 데이터의 구성을 보여줍니다. 이러한 데이터 세트의 교육 세트를 지시 튜닝 데이터로 사용하고 테스트 세트에서 모델을 평가합니다. 3.3 교육 세부 정보 Vision Transformer와 Language 모델이 동결된 2단계 교육 패러다임을 채택했습니다. 첫 번째 단계에서는 언어 모델의 시각적 추상화기와 LORA [Hu et al., 2022]가 모두 미세 조정됩니다. 첫 번째 단계에서는 문서 이해 데이터만 사용하고 10개의 에포크가 걸립니다. 두 번째 단계에서는 시각적 추상화를 더욱 동결하고 LORA만 훈련합니다. 이 단계에서는 문서 이해 데이터 외에도 언어 전용 및 일반 시각 및 언어 지시 튜닝 데이터가 추가로 도입되고 6회 업샘플링됩니다. 두 번째 단계에는 3개의 에포크가 걸립니다. 다른 훈련 하이퍼 매개변수는 mPLUG-Owl과 동일합니다[Ye et al., 2023]. 4 실험 4.1 LLMDoc 기존 벤치마크는 MLMM이 제공하는 개방형 지시 이해 결과를 평가하기 어렵습니다. 문서 도메인에서 지시 이해 성능을 더 잘 비교하기 위해 LLMDoc이라는 인간 평가가 있는 테스트 세트를 추가로 구성합니다. 표 1: DUE-Benchmark에서 OCR 없는 방법과의 비교. 모델 DocVQA 정보 VQA DeepForm KLC WTQ TabFact Dessurt 63.Donut 67.11.61.30.0 18.54.Pix2Structbase 72.38.mPLUG-DocOwl | 62.38.42.30.3 26.60.표 2: 차트, 자연스러운 이미지 및 웹 페이지 이해에 대한 OCR 없는 방법과의 비교. ChartQA TextVQA TextCaps 모델 VisualMRC Donut Pix2Structbase 41.43.74.93.56.88.mPLUG-DocOwl 57.52.111.188.데이터 수집 모델의 능력을 종합적으로 평가하기 위해 테이블(TabFact [Chen et al., 2020]), 차트(ChartQA [Masry et al., 2022]), 문서(DocVQA [Mathew et al., 2021]), 자연 이미지(TextVQA [Singh et al., 2019]) 및 웹 페이지(VisualMRC [Tanaka et al., 2021])를 포함하여 평가 데이터 세트를 구성하는 5가지 시나리오를 고려합니다. 구체적으로 각 데이터 세트에 대해 테스트 분할에서 20개의 이미지를 샘플링합니다. 이러한 이미지 중 10개에 대해 원시 질문을 지침으로 채택합니다. 나머지 10개에 대해서는 주석자에게 요약, 추론, 계산과 같은 더 강력한 기능이 필요한 지침을 작성하도록 요청합니다. 총 100개의 테스트 샘플을 얻습니다. 100% 인간 평가 Self-Instruct [Wang et al., 2022]에서 제안한 평가 기준에 따라 A &gt; B &gt;C&gt;D의 인간 평가를 수행하여 모델의 응답을 평가합니다. 여기서 A는 &#39;정확하고 만족스러운 응답&#39;을 나타내고, B는 &#39;사소한 불완전성이 있는 허용 가능한 응답&#39;을 의미하고, C는 &#39;지침에 대한 응답이지만 상당한 오류가 있음&#39;을 의미하고, D는 &#39;관련성이 없거나 잘못된 응답&#39;을 의미합니다. 우리는 mPLUG-DocOwl을 LLMDoc에서 mPLUG-Owl [Ye et al., 2023] 및 MiniGPT4 [Zhu et al., 2023]를 포함한 다른 대중적인 멀티 모달 대규모 언어 모델과 비교합니다. 그림 3에서 보듯이 mPLUG-DocOwl은 상당히 더 나은 성능을 달성했으며, 37개의 응답이 &quot;A&quot;로 평가되어 다양한 문서 시나리오에서 mPLUGDocOwl의 더 강력한 이해 능력을 보여줍니다. 게다가 모든 모델에서 일부 응답이 &quot;C&quot; 또는 &quot;D&quot;로 평가되어 문서 도메인에서 명령어 이해 성능이 여전히 유망하지 않으며 더 많은 노력이 필요하다는 것을 알 수 있습니다. 4.2 벤치마크 평가 50%0% mPLUG-DocOwl mPLUG-OwlMiniGPT-그림 3: LLMDoc에서 mPLUG-DocOwl, mPLUG-Owl 및 MiniGPT-4에 대한 인간 평가. 인간 평가 외에도 mPLUG-DocOwl을 공개 데이터 세트에서 ocr이 없는 최첨단 문서 이해 모델과 비교합니다. 표 1은 주로 문서와 표에서 텍스트 인식 및 레이아웃 이해 능력이 필요한 DUE-Benchmark [Borchmann et al., 2021]에서 Dessurt [Davis et al., 2022], Donut [Kim et al., 2022] 및 Pix2Struct [Lee et al., 2022]와 비교한 내용을 보여줍니다.또한, 표 2는 차트, 자연스러운 이미지 및 웹페이지 데이터 세트에 대한 평가를 제시하며, 이는 시각적 의미론과 텍스트 정보를 연관시키는 더 강력한 능력을 요구합니다.각 데이터 세트를 미세 조정하지 않고도 mPLUG-DocOwl은 비슷하거나 더 나은 성능을 달성합니다.최소 두 개의 상업적 소프트웨어 매뉴얼 출판사가 이 라이선스를 사용하는 데 관심이 있다고 말했습니다.GNU 운영 체제 출판사가 GNU F를 사용해야 하는 이유 .LEFT ROOM RIGHT ROM RIGHT ROOM RIGHT COMER Toys yous 240 LytoHuman: 얼마나 많은 상업적 출판사가 이 라이선스를 사용하는 데 관심을 보였습니까? mPLUG-DocOwl: 최소 두 개의 상업 출판사가 라이선스 사용에 관심을 보였습니다. Human: 목록의 두 번째 방은 무엇입니까? mPLUG-DocOwl: 오른쪽 방 (a) (b) 상대 주 날짜 인터넷 접속 불가 9월 6일, 9월 13일, 9월 20일, 9월 27일, 10월 4일, 17% 15% 19% 10월 11일, 로스앤젤레스 램스에서 클리블랜드 브라운스에서 마이애미 돌핀스에서 뉴욕 제츠에서 신시내티 벵갈스 시애틀 시호크스 결과 승 27-승 9-관객 63, 79, 패 16-패 33-47, 50.승 17-44.승 35-42, 16% 13% 10월 18일, 뉴잉글랜드 패트리어츠에서 패 38-60, 10월 26일, 피츠버그 스틸러스에서 패 26-52, 전혀 없음 2018년 11월 1일, 1981년. 11월 8일, 11월 15, 11월 22일, 11월 29일, 신시내티 벵갈스 오클랜드 레이더스 L 34-54, W 17-45, 12월 3일, 12월 13일, 12월 20일, 캔자스시티 치프스 뉴올리언스 세인츠 애틀랜타 팔콘스 클리블랜드 브라운스 샌프란시스코 49ers 피츠버그 스틸러스 L 23-73, L 27-49, L 31-40. W 17-44. L 28-55, W 21-41, 인간: 이 그래픽에서 몇 가지 관련 항목을 표시하세요. mPLUG-DocOwl: 2019년에 미국 인구의 13%가 인터넷을 전혀 사용하지 않았다고 보고했습니다. 인간: 가장 많은 점수를 얻은 날짜는? mPLUG-DocOwl: 10월 11일, COP 합의 (c) 가상 폭력 핵심-마크 Rob Allen 씨 RJ Reynolds Tobacco Co.Rick Sanders 씨 게임 구매 또는 대여 시 부모의 91%가 현장에 있음 06.0%의 어린이가 부모의 허락을 받음 1 91% 9% 인간: &#39;CRP 합의&#39;에 관련된 회사의 이름을 말하세요? mPLUG-DocOwl: Cone-Mark &amp; RJ Reynolds Tobacco Co. 인간: 게임 구매 또는 대여 시 부모의 몇 %가 현장에 없습니까? mPLUG-DocOwl: 9% (e) (f) 그림 4: mPLUG-DocOwl의 정성적 결과. 중요한 영역과 해당 단어는 더 명확한 시각화를 위해 동일한 색상으로 주석이 달려 있습니다. 틀린 답은 빨간색으로 표시됩니다. 4.3 정성적 분석 벤치마크 결과. 그림 4에는 다양한 유형의 이미지에 대한 정성적 결과가 나와 있습니다. 중요한 영역과 해당 응답은 동일한 색상으로 주석이 달려 있습니다. 사례 (a)는 mPLUG-DocOwl이 복잡한 내용이 있는 웹페이지 스크린샷에서 답을 정확하게 찾을 수 있음을 보여줍니다. 사례 (b)는 mPLUG-DocOwl이 손으로 그린 표도 이해하고 손으로 쓴 글꼴을 올바르게 인식할 수 있음을 보여줍니다. 사례 (c)에서 mPLUG-DocOwl은 차트에서 주요 요점을 요약할 수 있습니다. 표가 인터넷 사용에 대한 것임을 성공적으로 이해하고 &quot;절대 안 함&quot;은 &quot;인터넷을 사용한 적이 없음&quot;을 의미한다고 추론합니다. 그러나 &quot;미국에서&quot;와 같은 허황된 출력도 생성합니다. 사례 (d)의 질문은 모델이 &quot;결과&quot; 열을 이해하고, 요점을 비교하고, 가장 좋은 결과가 나온 날짜를 반환하도록 요구합니다. 사례 (e)는 모델이 다음을 처리할 수 있음을 보여줍니다. ScandHuman: 표의 각 행을 json 형식으로 변환합니다. mPLUG-DocOwl: {&quot;경기&quot;:1,&quot;날짜&quot;:1,&quot;장소&quot;:&quot;Redland Field&quot;,&quot;시간&quot;: 1:42,&quot;참석&quot;:30,511,&quot;경기&quot;:2,&quot;날짜&quot;:2,&quot;장소&quot;:&quot;Redland Field&quot;,&quot;시간&quot;: 1:42,&quot;참석&quot;:29,698,&quot;경기&quot;:3,&quot;날짜&quot;:3,&quot;장소&quot;:&quot;Comiskey Park 경기 날짜 요약 장소 시간 참석 10월 1일 Redland Field 1:30,10월 Redland Field 1:29,10월 Comiskey Park (1) 1:29,10월 Comiskey Park (1) 1:10월 Comiskey Park (1) 1:10월 Redland Field 10월 Redland Field 2:1:34,34,32,13,(I)&quot;,&quot;시간&quot;: 1:30,&quot;참석&quot;:34,363,&quot;경기 10월 9일 코미스키 파크(1) 2:27 32,&quot;:4,&quot;날짜&quot;:4,&quot;장소&quot;:&quot;코미스키 파크(I)&quot;,&quot;시간&quot;: 1:37,&quot;참석&quot;:34,379,&quot;경기 &quot;:5,&quot;날짜&quot;:6,&quot;장소&quot;:&quot;레드랜드 필드&quot;,&quot;시간&quot;: 1:45,&quot;참석&quot;:32,006,&quot;경기&quot;:6,&quot;날짜&quot;:7,&quot;장소&quot;:&quot;레드랜드 필드&quot;,&quot;시간&quot;:2:06,&quot;참석&quot;: 13,923,&quot;경기&quot;:7,&quot;날짜&quot;:8,&quot;장소&quot;:&quot;레드랜드 필드&quot;,&quot;시간&quot;: 1:47,&quot;출석&quot;:32,930} (a) 점수: B Mini-GPT4: |날짜 시간 점수: D Dickies 의료용 유니폼 SCRUB SHOP 인간: mPLUG-DocOwl의 이름은 무엇입니까: 이 가게는 Scrub Shop이라고 하며 의료용 유니폼을 판매합니다. 점수: 가게? 무엇을 판매합니까? Mini-GPT4: 가게 이름은 Scrub Shop입니다. 이 가게는 드레스, 상의, 바지, 액세서리와 같은 다양한 종류의 의류를 판매합니다. 점수: B (b) 그림 5: LLMDoc에서 mPLUG-DocOwl과 Mini-GPT4의 정성적 비교. 1부. 문서와 구별되는 회사 및 개인 이름. 사례 (f)는 mPLUG-DocOwl이 작고 흐릿한 텍스트를 인식할 수 있을 뿐만 아니라 사용자 의도에 따라 간단한 계산도 수행할 수 있음을 보여줍니다. LLMDoc 결과 그림 5와 그림 6은 LLMDoc에서 mPLUG-DocOwl과 Mini-GPT4를 비교한 것입니다. 그림 5(a)는 모델이 표를 JSON 형식으로 변환해야 합니다. mPLUG-DocOwl은 명령을 올바르게 이해하고 JSON 형식의 문자열을 반환하지만 마지막 행을 놓칩니다. Mini-GPT4는 명령을 이해하지 못하고 표 내의 내용을 이해하지 못합니다. 그림 5(b)에서 mPLUG-DocOwl과 Mini-GPT4는 모두 매장 이름을 올바르게 인식합니다. 그러나 Mini-GPT4는 이 매장의 옷이 의료용 유니폼임을 나타내는 작은 표지판을 간과합니다. 그림 6(c)의 차트 이해와 관련하여 Mini-GPT4는 잘못된 답변과 중복된 응답을 제공하는 반면 mPLUG-DocOwl은 간결하고 올바른 응답을 제공합니다. 그림 6(d)에서 Bernadette의 실제 목적은 Suzy에게 사본을 익일 배송해 줄 것인지 확인하는 것입니다. 이를 위해서는 모델이 텍스트를 정확하게 인식할 뿐만 아니라 관련된 사람들 간의 관계도 이해해야 합니다. mPLUG-DocOwl은 &quot;챕터 사본 요청&quot;이라는 문구를 인식하지만 주어와 목적어를 오해합니다. Mini-GPT4는 이 이미지가 메일 시나리오라는 것만 이해하고 모호하고 환각적인 반응을 제공합니다. 그림 6(e)에서 mPLUG-DocOwl은 두 가지 최신 뉴스에 대한 올바른 요약을 제공하지만 Mini-GPT4는 웹페이지 스크린샷과 관련 없는 뉴스를 생성합니다. 벨라루스의 연령별 뇌졸중 사망률 변화, 100,000명당 사망자 수로 측정한 연령대별 뇌졸중 사망률. 연령 표준화는 인구 연령 및 구조가 일정하다고 가정하여 국가 간 비교와 인구 내 연령 분포 변화(예: 노화)의 영향 없이 시간에 따른 비교를 허용합니다. 70세 이상 50-69세 old _ 201: 모든 연령대 190연령 표준화 5-14세 0.119.1,289.1, 출처: IHME, 질병의 세계적 부담(GBD) CC BY Re: Suzy: Merck Manual Human의 장의 새 사본: 어느 연령대의 사람들이 뇌졸중에 가장 많은 주의를 기울여야 할까요? mPLUG-DocOwl: 70세 이상 점수: A Mini-GPT4: 뇌졸중에 가장 많은 주의가 필요한 연령대는 65세 이상입니다. ... 그리고 노령 인구는 젊은 사람들보다 뇌졸중을 경험할 위험이 더 높습니다. 점수: C Naima가 이 장의 깨끗한 사본이 필요하다는 메시지를 전달했습니다. 그녀는 제가 팩스로 보내달라고 요청했습니다. 사본을 익일 배송받고 싶으시면 알려주십시오. 감사합니다, Bernie Janas Designetto 중요 수정 메시지 즉시 전달하세요 표지지의 페이지 수. 오전 11:05 EST Nimfadedamage가 이 장의 필요성을 나타냄 DIVRITE DIN Human: Bernadette가 이 메시지를 보낸 의도는 무엇입니까? mPLUG-DocOwl: Mini-GPT4: Bernadette가 이메일을 보낸 의도는 수신자의 도움을 요청하는 것입니다 Merck에서 장의 사본을 요청하려면 점수: B 중요한 작업을 완료합니다. 메시지에는 &quot;이 작업을 도와주세요.&quot;라고 적혀 있어 Bernadette가 특정 작업을 완료하는 데 수신자의 도움이 필요하다는 것을 나타냅니다. 점수: C 생성: 2월 10일 최신 뉴스 몰도바 공화국의 Europeana 교육 컬렉션 교육 교사 교육 anGourself moldova 2019년 10월 유럽 문화 유산의 날의 일환으로 몰도바 공화국 국립도서관은 &#39;학생과 교사의 문화적 친구인 Europeana에서 발견할 수 있는 10가지&#39;라는 주제로 일련의 교육 세션을 조직했습니다. 이 게시물에서는 몰도바 공화국의 활동과 유럽 전역의 학생과 교사를 지원하는 Europeana 리소스를 살펴봅니다. 자세히 보기 Creed: 2월Unsplash for Education Collections를 통해 Europeana를 새로운 청중에게 선보이다 Education parmers Europeana에서는 학생과 교육자 모두에게 풍부한 디지털 문화 유산을 제공하고자 하며, 이러한 그룹이 Europeana의 컬렉션과 리소스에 액세스할 수 있도록 파트너와 협력합니다.이 게시물에서는 Europeana가 플랫폼 Unsplash와 협력하여 새로운 교육 청중에게 다가가는 방법을 살펴봅니다.자세히 보기 © Human: 두 가지 최신 뉴스는 무엇입니까?두 가지 최신 mPLUG-DocOwl: Mini-GPT4: 이미지의 두 가지 최신 뉴스는 &#39;Amazon이 미국 상공 회의소에 가입&#39;과 &#39;뉴스는 몰도바 공화국의 유럽 연합과 교육을 위한 유럽 연합 리소스입니다. 미국 농무부는 지적 재산권 침해 혐의로 중국과의 무역을 제한할 것을 제안했습니다.&#39; 자세히 보기 점수: A 점수: C 그림 6: LLMDoc에서 mPLUG-DocOwl과 Mini-GPT4의 정성적 비교. 2부.Vodafone NL 18:채팅 (1) Lucy Lives! Guido, Janneke, Paul, Ti... Guido De Vries: 66% 정보 90.117.84 117.84 117.84 117.Presente! 17:Timo Bestwerk: 확인하세요. 17:Janneke: 난 반쯤 됐어 17:200817:Dan은 모든 걸 다 할 수 있어! Timo Bestwerk:Nee, aanmaakblokjes zijn er dan nog broad wel Janneke: Heus nie 17:Timo Bestwerk: Als jij het zegt... 17:17:130.131.127.04 127.117.84 117.84 117.84 117.84 117.84 117.Stuur Human: 그림에는 몇 개의 서로 다른 스피커가 등장합니까? mPLUG-DocOwl: 그림에는 세 가지 다른 스피커가 있습니다. (a) 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2018 2019추가 정보 ©Statista출처 보기 Human: 2011년과 2011년 사이의 평균 티켓 가격은 얼마였습니까? mPLUG-DocOwl: 117.(b) 그림 7: LLMDoc에서의 실패 사례. 1부. &quot; LLMDoc에는 문서 도메인에서 많은 어려운 명령 이해 사례가 포함되어 있습니다. 그림 7과 그림 8은 mPLUG-DocOwl이 제공한 몇 가지 잘못된 응답을 보여줍니다. 그림 7(a)에서 mPLUG-DocOwl은 그림에 있는 세 개의 이름만 기록하고 사용자 자신도 화자라는 사실을 무시합니다. 그림 7(b)에서 mPLUG-DocOwl은 이미지의 여러 요소에 대한 다단계 계산을 수행하지 못합니다. 그림 8(c)에서 모델은 장면과 그 안의 텍스트를 이해할 수 있지만 존재하지 않는 캐릭터에 대해 환상을 품습니다. 그림 8(d)에서 mPLUG-DocOwl은 뉴스 쓰기에 대한 명령을 이해하지 못하고 태블릿의 텍스트만 읽습니다. 5
--- CONCLUSION ---
이 작업에서 우리는 문서 이해 데이터를 지침 미세 조정에 통합하여 다양한 OCR 없는 문서 이해 기능을 mPLUG-Owl에 주입합니다. 실험 결과는 우리의 mPLUG-DocOwl이 기존 OCR 없는 방식과 동등하거나 더 나은 성능을 달성한다는 것을 보여줍니다. 게다가 언어 전용 및 일반적인 시각 및 언어 지침 조정의 이점을 활용하여 mPLUG-DocOwl은 사용자 지침과 의도를 더 잘 이해하여 더 복잡한 상호 작용을 가능하게 할 수 있습니다. 게다가 LLMDoc에 대한 인간 평가는 mPLUG-DocOwl이 여전히 문서 관련 상식적 추론, 수학적 계산 및 창의적 생성에 어려움을 겪고 있음을 보여줍니다. 이는 미래에 LLM으로 더 강력한 문서 이해 능력을 개발하는 데 대한 귀중한 통찰력을 제공합니다. SHRED Khed SPOOSHIN 메달 표8221231222랭크 국가 금 은 동 총계러시아미국우크라이나일본프랑스이탈리아헝가리중국220OOOOO 독일폴란드캐나다체코공화국에스토니아영국인간: 그림 속 단어는 어떤 장면을 전달합니까?mPLUG-DocOwl: 그림 속 단어는 &quot;K-Rash&quot;와 &quot;Sploosh!&quot;라고 말하는 캐릭터가 있는 만화책이나 만화 같은 장면을 전달합니다. 단어에는 만화책이나 만화의 캐릭터인 사람의 그림이 함께 표시되고, 단어는 말풍선에 있습니다. 캐릭터가 무언가에 과장되거나 예상치 못한 반응을 표현할 가능성이 높으므로 장면은 유머러스하거나 가벼운 순간으로 보입니다. (c) 인간: 뉴스를 쓰세요. mPLUG-DocOwl: 메달 표 랭크 국가 금 | 은 | 동 | 총계 러시아 13 9 8 30 우크라이나 6 4 8 21 일본 338 프랑스 3 1 26 이탈리아 3205 헝가리 2 3 0 5 중국25 독일 0 2 3 5 폴란드 021 4 캐나다 0 1 13 체코 1011 에스토니아 0 1 0 1 영국(d) 그림 8: LLMDoc의 실패 사례. 2부. 참고문헌 S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, CL Zitnick, D. Parikh. Vqa: 시각적 질의 응답. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 2425-2433쪽, 2015. L. Borchmann, M. Pietruszka, T. Stanislawek, D. Jurkiewicz, M. Turski, K. Szyndler, F. Gralinski. DUE: 종단간 문서 이해 벤치마크. NeurIPS Datasets and Benchmarks, 2021. SR Bowman, G. Angeli, C. Potts, CD Manning. 자연어 추론 학습을 위한 대규모 주석 코퍼스. arXiv 사전 인쇄본 arXiv:1508.05326, 2015. W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, WY Wang. Tabfact: 테이블 기반 사실 검증을 위한 대규모 데이터 세트. International Conference on Learning Representations(ICLR), 에티오피아 아디스아바바, 2020년 4월. BL Davis, BS Morse, BL Price, C. Tensmeyer, C. Wigington, VI Morariu. dessurt를 사용한 종단간 문서 인식 및 이해. ECCV 워크숍(4), 컴퓨터 과학 강의 노트 13804권, 280-296쪽. Springer, 2022. A. Hu, S. Chen, Q. Jin. 질문 제어 텍스트 인식 이미지 캡션. ACM 멀티미디어, 3097-3105쪽. ACM, 2021. EJ Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen. Lora: 대규모 언어 모델의 Lowrank 적응. 제10회 국제 학습 표현 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Y. Huang, T. Lv, L. Cui, Y. Lu, F. Wei. Layoutlmv3: 통합 텍스트 및 이미지 마스킹을 사용한 문서 AI 사전 학습. ACM Multimedia, 4083-4091쪽. ACM, 2022. G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. OCR 없는 문서 이해 변환기. ECCV(28), Lecture Notes in Computer Science의 13688권, 498-517쪽. Springer, 2022. K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M. Chang, and K. Toutanova. Pix2struct: 시각 언어 이해를 위한 사전 학습으로서의 스크린샷 파싱. CORR, abs/2210.03347, 2022. C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, J. Zhang, S. Huang, F. Huang, J. Zhou, and L. Si. mplug: 교차 모달 건너뛰기 연결을 통한 효과적이고 효율적인 시각-언어 학습. EMNLP, 7241–7259쪽. Association for Computational Linguistics, 2022. H. Liu, C. Li, Q. Wu, and YJ Lee. 시각 지시 튜닝. CORR, abs/2304.08485, 2023a. Y. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin, et al. 대규모 멀티모달 모델에서 OCR의 숨겨진 미스터리에 관하여. arXiv 사전 인쇄본 arXiv:2305.07895, 2023b. A. Masry, DX Long, JQ Tan, SR Joty, and E. Hoque. Chartqa: 시각적 및 논리적 추론을 통한 차트에 대한 질의 응답을 위한 벤치마크. ACL(Findings), 2263-2279페이지. Association for Computational Linguistics, 2022. M. Mathew, D. Karatzas, and CV Jawahar. Docvqa: 문서 이미지에 대한 VQA 데이터 세트. WACV, 2199-2208페이지. IEEE, 2021. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and CV Jawahar. 인포그래픽vqa. WACV, 2582-2591페이지. IEEE, 2022. OpenAI. chatgpt 소개. https://openai.com/blog/chatgpt, 2022. P. Pasupat 및 P. Liang. 반구조화된 표에 대한 구성적 의미 구문 분석. ACL(1), 1470-1480페이지. 컴퓨터 언어학 협회, 2015.TL Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, AS Luccioni, F. Yvon, M. Gallé, J. Tow, AM Rush, S. Biderman, A. Webson, PS Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, AV del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, PO Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, AF Aji, A. 알파시, A. Rogers, AK Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, DI Adelani, et al. BLOOM: 176b-매개변수 오픈 액세스 다국어 언어 모델. CoRR, abs/2211.05100, 2022. O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: 독해 능력을 갖춘 이미지 캡션을 위한 데이터 세트. ECCV(2), Lecture Notes in Computer Science의 12347권, 742-758페이지. Springer, 2020. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. 읽을 수 있는 VQA 모델을 향해. CVPR, 8317-8326페이지. Computer Vision Foundation/IEEE, 2019. T. Stanislawek, F. Gralinski, A. Wróblewska, D. Lipinski, A. Kaliska, P. Rosalska, B. Topolski, P. Biecek. Kleister: 복잡한 레이아웃이 있는 긴 문서와 관련된 주요 정보 추출 데이터 세트. ICDAR(1), Lecture Notes in Computer Science의 12821권, 564-579페이지. Springer, 2021. S. Svetlichnaya. Deepform: 규모에 맞게 구조화된 문서 이해, 2020. R. Tanaka, K. Nishida, S. Yoshida. Visualmrc: 문서 이미지에 대한 기계 독해 이해. AAAI, 13878-13888페이지. AAAI Press, 2021. Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, and M. Bansal. 범용 문서 처리를 위한 통합 비전, 텍스트 및 레이아웃. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 19254–19264쪽, 2023. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and TB Hashimoto. Stanford alpaca: 지시를 따르는 라마 모델. https://github.com/tatsu-lab/ stanford_alpaca, 2023. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: 개방적이고 효율적인 기반 언어 모델. CoRR, abs/2302.13971, 2023. Vicuna. Vicuna: gpt-4를 감동시키는 개방형 챗봇. https://github.com/1m-sys/FastChat, 2023. Y. Wang, Y. Kordi, S. Mishra, A. Liu, NA Smith, D. Khashabi, and H. Hajishirzi. 자체 지시: 언어 모델을 자체 생성 지침과 정렬. CoRR, abs/2212.10560, 2022. doi: 10.48550/arxiv.2212.10560. URL https://doi.org/10.48550/arxiv. 2212.10560. C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Visual Foundation 모델을 사용한 대화, 그림 그리기 및 편집. CORR, abs/2303.04671, 2023. C. Xu, D. Guo, N. Duan, and JJ McAuley. Baize: 셀프 채팅 데이터에 대한 매개변수 효율적 튜닝을 갖춘 오픈소스 채팅 모델. CORR, abs/2304.01196, 2023a. H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, G. Xu, J. Zhang, S. Huang, F. Huang, and J. Zhou. mplug-2: 텍스트, 이미지 및 비디오에 걸친 모듈화된 멀티모달 기반 모델. CoRR, abs/2302.00402, 2023b. Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou. Layoutlm: 문서 이미지 이해를 위한 텍스트 및 레이아웃 사전 학습. R. Gupta, Y. Liu, J. Tang 및 BA Prakash 편집자, KDD &#39;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, 2020년 8월 23~27일, 페이지 1192~1200. ACM, 2020. doi: 10.1145/3394486.3403172. URL https://doi.org/10.1145/3394486.3403172.Z. Yang, Y. Lu, J. Wang, X. Yin, D. Florêncio, L. Wang, C. Zhang, L. Zhang 및 J. Luo. TAP: text-vqa 및 text-caption을 위한 textaware 사전 학습입니다. CVPR, 페이지 8751-8761. Computer Vision Foundation/IEEE, 2021. Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang. MM-REACT: 다중 모드 추론 및 액션을 위한 chatgpt 프롬핑. CoRR, abs/2303.11381, 2023. Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: 모듈화는 다중 모드성을 갖춘 대규모 언어 모델을 강화합니다. CoRR, abs/2304.14178, 2023. D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: 고급 대규모 언어 모델을 통한 시각-언어 이해 향상, 2023.
