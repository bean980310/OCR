--- ABSTRACT ---
우리는 민첩한 로봇 공학의 벤치마크 과제를 다룹니다. 고속으로 던져진 물체를 잡는 것입니다. 이것은 물체의 시각적 관찰과 로봇의 고유 감각 상태에만 접근하여 던져진 물체를 추적, 가로채기, 안는 것을 포함하는 어려운 과제이며, 이 모든 것이 1초의 일부 내에 이루어집니다. 우리는 두 가지 근본적으로 다른 솔루션 전략의 상대적 장점을 제시합니다. (i) 가속 제약 궤적 최적화를 사용한 모델 예측 제어, (ii) 0차 최적화를 사용한 강화 학습. 우리는 광범위한 하드웨어 실험을 통해 샘플 효율성, 시뮬레이션-실제 전송, 분포 변화에 대한 견고성, 전신 멀티모달리티를 포함한 다양한 성능 트레이드오프에 대한 통찰력을 제공합니다. 우리는 민첩한 로봇 제어를 위한 &quot;고전적&quot; 및 &quot;학습 기반&quot; 기술을 융합하는 것에 대한 제안으로 마무리합니다. 실험 비디오는 여기에서 볼 수 있습니다: https://sites.google.com/view/agile-taking. 그림 1: 라크로스 헤드가 달린 모바일 매니퓰레이터가 1초 이내에 공을 잡습니다. (오른쪽) 요 각도를 제어할 수 있고 속도가 약 5m/s인 자동 공 던지기 장치. 1.
--- INTRODUCTION ---
공중에서 공을 쫓고 극적인 다이빙 캐치를 완료하는 것은 여러 인기 스포츠에서 기억에 남는 운동 능력의 순간이며, 인간의 민첩성의 벤치마크입니다. 이 논문에서는 라크로스 헤드가 장착된 엔드 이펙터가 있는 모바일 조작기 플랫폼(그림 1 참조)에서 고속으로 움직이는 공을 추적, 가로채기 및 잡는 작업을 고려합니다. 1초도 채 안 되는 시간 내에 로봇은 공에 대한 시각적 관찰을 지속적으로 실행 가능한 전신 동작으로 변환하여 베이스와 팔을 모두 조정된 방식으로 제어해야 합니다. 마지막 밀리초에서 © 2023 SAABNMBKCDDDJPSASVSSS.J.-JSaS Tu*. 전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐치 제어 시스템은 지각적 폐색에 강해야 하며 캐치를 안정화하고 튀어나오는 것을 방지하기 위해 크래들링 기동을 실행해야 합니다. 이 작업의 물리학은 놀라울 정도로 복잡할 수 있습니다. 기하학적으로 단순함에도 불구하고, 비행 중인 공은 항력과 마그누스 효과로 인해 예측할 수 없는 방식으로 휘고 휘어질 수 있습니다(Mehta, 1985). 더욱이 공과 변형 가능한 엔드 이펙터 사이의 접촉 상호 작용에는 정확하게 모델링하기 어려운 복잡한 연체 물리학이 관련됩니다. 이 논문에서 우리는 설계 스펙트럼의 두 끝에서 이 작업을 위한 고속 시각적 피드백 컨트롤러를 합성하는 상대적 장점을 연구합니다. &quot;순수 제어&quot; 전략을 나타내는 모델 예측 제어(MPC)(Borrelli et al., 2017; Rawlings, 2000)와 &quot;순수 학습&quot; 접근 방식을 나타내는 블랙박스 정책 최적화(Choromanski et al., 2018a). MPC는 상태 불확실성에 대응하여 실시간으로 로봇 궤적을 최적화합니다. 데이터 요구 사항 측면에서 거의 &quot;제로샷&quot;이며 운동학, 역학 및 작업별 제약 조건을 우아하게 처리하지만 계산 비용이 많이 들고 역학 모델링의 오류에 민감할 수 있습니다. 반면 블랙박스 또는 RL(강화 학습) 방법을 통한 정책 학습은 매우 데이터 비효율적일 수 있지만 원칙적으로 복잡하고 알려지지 않은 실제 세계 역학에 적응할 수 있습니다. 우리의 주요 기여는 반응 시간, 샘플 효율성, 분포 변화에 대한 견고성 및 로봇 민첩성에 대한 통합 실험 평가에서 전신 멀티모달 행동 측면에서 다양성. 우리는 향후 작업에서 &quot;두 세계의 장점&quot;을 결합하는 제안으로 논문을 마무리합니다. 1.1.
--- RELATED WORK ---
두 가지 기술 클래스 모두 이전에 로봇 캐치 작업에 적용되었습니다.공 캐치를 위한 최적화 기반 제어의 예로는 Hove와 Slotine(1991); Hong과 Slotine(1995); Yu 등(2021); Frese 등(2001); Kober 등(2012)이 있습니다.Bäuml 등(2010a)과 Lampariello 등(2011)은 캐치 포인트 선택, 캐치 구성 계산 및 경로 생성을 단일 비선형 최적화 문제로 포괄하는 통합된 접근 방식을 제시합니다(Koç 등(2018), Jia 등(2019) 참조).여러 논문에서 제어 스택의 일부에 대해 인간 시범과 머신 러닝을 활용합니다.Kim 등(2014)은 다양한 실행 가능한 캐치 구성을 확률적으로 예측하고 인간 시범에서 학습한 손-팔 동작을 안내하는 컨트롤러를 개발합니다.Riley와 Atkeson(2002)도 인간 시범에서 동작 기본 요소를 학습하고 새로운 동작을 생성합니다. Dong 등(2020)은 2단계 모션 플래닝과 학습 기반 추적 컨트롤러를 사용합니다. 일부 논문은 명시적으로 소프트 캐치를 목표로 합니다. Salehian 등(2016)은 Kim 등(2014)을 더 확장하여 팔과 원하는 캐치 시간을 제어하는 데 있어 부정확성에 더 탄력적인 소프트 캐치 절차를 제공합니다. Bäuml 등(2011)은 Bäuml 등(2010a)을 더 확장하여 소프트 랜딩을 가능하게 합니다. Hong과 Slotine(1995); Lippiello와 Ruggiero(2012)는 소프트 캐치를 위한 휴리스틱을 추가하여 공의 예상 경로를 따라 손을 움직이면서 공의 속도를 줄여 충격 에너지를 소산시킵니다. 2. 문제 공식화 및 제안된 해결책 우리는 잡을 대상의 궤적을 함수 Fo로 기술하는데, 이 함수는 질의 시간 t = R20을 시간 t에서의 대상의 위치와 속도, 즉 (p。(t), vo(t)) Є R³ × R³에 매핑합니다. 대상의 공기역학적 및 관성적 속성에 따라 Fo는 매우 사소하지 않을 수 있습니다. Fo에 대한 우리의 지식은 알려진 Fo를 통해 인코딩되는데, 이 Fo는 질의 시간 t = R20과 매개변수 집합 0.Є Rd를 시간 t에서의 대상의 위치와 속도에 대한 예측, 즉 (po(t; 0。), î。(t; 0。))에 매핑합니다. 전신 MPC와 블랙박스 정책 학습을 통한 민첩한 잡기 이 작업의 경우, 우리는 범위를 구형의 단단한 공으로 제한하고 고전적인 뉴턴 물리학을 통해 Fo를 구현합니다. 비사소한 공기 역학과 비균일한 모양을 가진 물체를 잡는 것은 향후 작업으로 남겨둡니다.그러나 우리는 두 대의 고정된 카메라를 통해 공의 위치와 속도를 간접적으로 관찰하고, 를 사용하여 비전 시스템의 현재 위치와 속도 추정치를 인코딩합니다.로봇의 경우, q = R7은 관절 구성 벡터를 나타내며, 여기서 q₁ = R은 평행 이동 기본 관절에 해당하고 92:7 R6은 팔 관절 각도를 나타냅니다.또한 FK : q Є R7 ↔ FK(q) = (Ph(9), Rh(9)) = R³ × SO(3)은 관절 구성 벡터 q를 로봇 엔드 이펙터(즉, 라크로스 헤드)의 SE(3) 포즈에 매핑하는 순방향 운동학 변환을 나타냅니다.2.1. 최적 제어를 통한 잡기 우리는 팔의 비선형 조작기 동역학을 보상하는 하위 수준 위치 및/또는 속도 컨트롤러가 있다고 가정합니다. 이 하위 수준 제어 시스템의 폐루프 동작을 추상화하고, q에 대한 2차 적분기 동역학¹을 가정하여 팔의 동작을 계획합니다.즉, ä(t) = ua(t) € R7입니다. 이 가정에 따라 최적 포착 문제(OCP)는 함수 ua(•)와 포착 시간 tƒ에 대한 자유-종료-시간 제약 최적 제어 문제로 공식화할 수 있습니다. Jua, tƒ) := ua (·),tf ƒª&#39; (\ + ||ua(r)||²) dr + ¥ (q(tƒ),ġ(ts),ts), (1)을 최소화합니다. 여기서 λ = R&gt;0은 가중치 상수이고 ¥ : R7 × R7 × R≥0 → 0은 최종 비용입니다. 2차 적분 동역학 ä(t) = ua(t) 및 다음 제약 조건에 따름: VT € [0,tƒ], Ua(T) E [Ua, Ua], q(7) E [¶‚¶], ġ(7) € ġ‚¶], c(q(tƒ), tƒ) ≥ 0. (2) 처음 세 가지 제약 조건은 제어 노력과 조인트 구성 및 속도에 대한 한계를 포착합니다. 최종 비용 및 끝점 제약 조건 함수 c는 잡기에 바람직한 두 가지 속성을 포착합니다: (i) SE(3) 잡기 시간 tƒ에서 공의 위치 및 속도 방향과 라크로스 헤드의 포즈 정렬, (ii) 공의 속도 벡터에 수직인 라크로스 헤드의 모든 잔류 속도 최소화. 우리는 하드 및 소프트 제약 조건을 통해 이러한 두 가지 속성을 포착합니다. 하드 끝점 제약 조건 c. 끝점 잡기 제약 조건은 들어오는 발사체를 받아들이기 위해 라크로스 헤드가 올바른 위치와 방향으로 배치되어야 한다는 요구 사항을 포착합니다. 특히, (po(tƒ), vo(tƒ))를 포획 시간 tƒ에서 물체의 실제 3D 위치와 속도라고 하자. 그러면 다음이 필요하다. vo(ts) Ph(q(t)) Po(t)||≤p, 그리고 (Rh(q(tƒ))e2)T: ||vo(tƒ)|| &gt; COS Єr, (3) 여기서 Єp, Єr Є R&gt;0은 각각 위치 및 각도 오차에 대한 규정된 허용 오차이고, e2 = (0, 1, 0)이다. 두 번째 제약 조건은 라크로스 헤드의 로컬 ŷ-축을 정렬하도록 강제하는데, 이는 그물의 포획 평면과 직교하고, tƒ에서 공의 속도 벡터이다. 1. 하위 수준 제어 시스템에는 지연을 포함하여 일부 사소하지 않은 폐루프 응답 특성이 있을 수 있다는 점에 유의한다. 그러나 이러한 문제는 계획된 (q, q) 궤적에서 명령된 (q, q) 궤적을 조정하여 미리 보상할 수 있습니다.전신 MPC와 블랙박스 정책 학습을 통한 민첩한 캐치 위의 제약 조건은 공의 실제 3D 위치와 속도에 대한 액세스를 가정하여 작성되었습니다. 그러나 매개변수 예측자 Fo(300)를 통해서만 이러한 수량에 대한 예측에 액세스할 수 있으므로 예측된 수량 po(tƒ;00),ûo(tƒ;00)에 대해 위의 제약 조건을 적용하여 엔드포인트 포착 제약 조건 함수 c(q(tƒ), tƒ; 0。)를 0에서 매개변수화합니다.소프트 터미널 비용 V. 위의 하드 제약 조건과 함께 터미널 비용 &amp;는 다음과 같은 형식을 취합니다.¥(q(tƒ),ġ(tƒ), tƒ;00) := Wpp(q(tƒ), tƒ;00) + wvvv (q(tƒ), ġ(tƒ)) p(q(tƒ), tƒ;00) = ||Ph(q(tƒ)) - Po(tƒ;00) ||² + (1 – (R = | (1) ¥v(q(tƒ), ġ(tƒ)) := || Rh(q(tf))vn(q(tf), ġ(tƒ)) — (4) 1- (Rr(q(tƒ))e2)T vo(tf;00) ||vo(tƒ;00)||, (5)(6) 여기서 wp, w₁ Є R≥0은 상수 가중치이고, v (q(tƒ), ġ(tƒ)) € R³는 관성 프레임에서 표현되고 야코비안 벡터 곱 qph (9)q를 통해 계산된 라크로스 헤드의 평행 이동 속도입니다. 상수 ve ER은 원하는 캐치 속도입니다. 따라서 최종 비용 ↳는 (3)에 정의된 캐치 시간 포즈 오류와 캐치 순간에 공의 속도 벡터에 수직인 라크로스 헤드의 움직임에 페널티를 부여합니다. 따라서 전체 OCP는 00에서 매개변수적이며, 공의 3D 예측 함수 Fo의 매개변수와 문제 매개변수 {Єp, Er, Vc, Wp, Wv, \}입니다.3. 순차적 이차 프로그래밍으로의 축소 OCP(1)는 자유 종료 시간 문제에 대한 최적성의 필요 조건을 활용하고 경계값 문제 솔버를 사용하여 해결할 수 있는 비자명한 문제입니다.그러나 이를 위해서는 동역학의 고밀도 이산화와 부등식 제약 조건(예: 배치를 통해)을 사용하여 제어, 상태 및 공동 상태 궤적을 최적화해야 합니다.대신 제한된 솔루션 클래스에서 가속 및 코스팅 단계 시퀀스를 최적화하여 계산 부담을 단순화하고, 그 과정에서 문제를 다단계 이산 시간 궤적 최적화 문제로 변환한 다음 최첨단 슈팅 기반 순차적 이차 프로그래밍(SQP) 솔버(Singh et al., 2022)를 사용하여 해결합니다. 3.1. 다단계 궤적 최적화로의 변환 가속 한계가 대칭 간격 [—ïa, ïa]에 의해 주어진다고 가정합니다.여기서 ïa Є Ro는 고정 벡터입니다.그러면 각 &quot;단계&quot;가 일정 가속 단계 다음에 일정 순항 단계로 구성된 N단계 이산 시간 궤적 최적화 문제를 정의할 수 있습니다.형식적으로 k € {0, ………, N − 1}에 대한 단계 k는 St[k] € R≥0인 St[k]초 동안 지속됩니다.그러면 단계 k의 가속 단계 내에서 관절 i = {1,…,7}은 (9i, ġi) [k]에서 시작하여 ±ga에서 가속하여 dġ¿[k]의 순 속도 변화를 달성합니다.순항 단계에서 관절은 St[k] — (|Sġi[k]\/ġa;)초 동안 ġi[k] + Sġi[k]의 일정 속도로 이동합니다. 위의 단계 전환을 요약하면 다음과 같습니다.복합 상태 x와 제어 u를 정의합니다.x[k] := (q[k],ġ[k],t[k]) € R15, u[k] := (8q[k], 8t[k]) € 전체 신체 MPC와 블랙박스 정책 학습을 통한 RAGILE CATCHING 그런 다음 단계 &quot;역학&quot;은 다음과 같이 작성됩니다.x[k + 1] = [q[k + 1]¯ ġ[k + 1] _t[k + 1] = о [q[k] + (ġ[k] +8ġ[k])6t[k] — (1/2)▲µ³ (8ġ[k] ○ |§ġ[k]|)] ġ[k] +8ġ[k] t[k] + St[k] (7) 여기서 o는 Hadamard 곱을 나타내고 A는 벡터 v의 대각 행렬 형태. u = (u[0],..., u[N - 1])이라 하자. 단계 등가 이산 시간 목적은 다음과 같이 주어진다. NJ(u) = Σ (A&amp;t[k] + ||8ġ[k]||²) + ¥(x[N]). k=(8) 참고 1 (1)의 적분 목적을 단계별 이산 시간 목적으로 정확하게 변환하면 단계 비용이 \St[k] +qz|8ġ[k]] 형태가 된다. 그러나 이것은 위에서 사용된 C² 매끄러운 목적보다 수치적으로 덜 견고한 것으로 밝혀졌다. = Nk= (3)의 최종 비용과 종점 포착 부등식 제약 조건이 직접 이월되어 x[N] = (q(tƒ),ġ(tƒ), tƒ)에 적용된다. 여기서 tƒ St[k]. 이제 (q, q, q)에 대한 한계 제약 조건을 다룬다. 가속의 경우 다음이 필요하다. |§ġ[k]| ≤ ġa&amp;t[k], k = 0,...,N - 1. ġ(t)가 단계 값 ġ[k] 사이를 선형 보간하므로 속도 한계 제약 조건은 단계 값에서만 적용하면 됩니다. ġ≤ġ[k]≤q‚_k=0,..., N. (10) 마지막으로 모든 7 Є [0, tƒ]에 대한 q(7)의 한계 제약 조건을 처리하려면 각 단계 내의 포물선(일정 가속도) 및 선형(순항) 프로필을 모두 고려해야 합니다. • 사례 1: ġi [k](ġi[k] +8ġi[k]) ≥ 0. 이 경우 qi(7)은 모든 □ Є [t[k], t[k + 1]]에 대해 {qi[k], qi[k+1]} 사이를 보간합니다. 따라서 우리는 끝점 qi [k], gik + 1]에만 한계 제약 조건을 적용해야 합니다. • 사례 2: ġi[k](ġi[k] + Sġi[k]) &lt; 0. 이 경우, [t[k], t[k+1]] 내에서 q₁(7)에 대한 로컬 최대/최소값이 존재하며, 여기서 ġ¿ (7) = 0입니다. 이 최대/최소값을 ĝ¿[k]로 표시합니다. 그런 다음 qi[k], qi[k + 1]에서 한계 제약 조건을 적용하는 것 외에도 ĝi [k]에 대한 제약 조건도 적용해야 합니다. i [k]에 대한 표현식은 다음과 같습니다.ĝi [k] = qi [k] + ġi [k]2äai ġi [k]if ġi [k] &gt;2äai if ġi [k] &lt; 0° 이산 시간 &quot;단계&quot; 역학, 최적화 목적 및 제약 조건이 주어지면 기성품 제약 이산 시간 궤적 최적화 솔버를 사용할 수 있습니다.이 작업에서는 Singh et al.(2022)에서 도입한 Dynamic Shooting SQP를 활용합니다.전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐칭 참고 사항 2 각 단계 내의 최대/최소 가속 및 순항 단계의 조합은 혼합 제어 노력/최소 시간 최적 제어 솔루션의 특성을 반영하며, 일반적으로 &quot;뱅오프뱅&quot; 전략이라고 합니다. 최근 연구(Sarkar et al., 2021)에 따르면 단일 제어 입력이 있는 LTI 시스템의 경우 엔드포인트 도달성 제약이 있는 혼합 제어 노력/최소 시간 문제에 대한 최적 솔루션은 일련의 &quot;뱅오프&quot; 단계입니다. 이는 원래 연속 시간 OCP의 이러한 단계적 감소를 사용하는 것을 정당화하며 사다리꼴 속도 프로파일을 사용하여 캐치하는 이전 연구(Bäuml et al., 2010b)와 정신이 비슷합니다. 3.2. 추가 구현 세부 정보 이제 SQP 구현에 대한 추가 구현 세부 정보를 설명합니다. 비동기 구현: 캐치 컨트롤러와 동시에 실행되는 것은 예측 매개변수 00의 업데이트를 생성하는 추정기로, 온라인 재계획이 필요합니다. 우리는 비동기 구현을 통해 이를 달성하는데, 여기서 최적화 문제는 0에 대한 최신 추정치와 현재 로봇 상태(q, q)를 사용하여 별도의 스레드에서 지속적으로 다시 해결됩니다. 로봇의 하위 PD 컨트롤러에 대한 명령된 (q, q)는 가장 최근의 단계별 솔루션을 연속 시간 궤적으로 디코딩하여 계산되므로 일관된 제어 속도가 보장됩니다. 후퇴하는 수평선이 없으므로 재계획은 전통적인 모델 예측 제어와 달리 공의 궤적에 대한 업데이트 추정치에 대한 응답으로 계획을 미세 조정하는 것과 더 유사합니다. 따라서 공의 궤적 예측에 오류가 없는 경우 문제는 재귀적으로 실행 가능한 상태로 유지됩니다. 크래들링: 공의 절편에 따라 q에서 2차 ODE로 모델링된 개방 루프 크래들링 동작 기본 요소를 사용하여 라크로스 헤드를 늦추고 동시에 그물을 위쪽을 가리키도록 회전합니다. 특히 t ≥ tƒ의 경우 ä(t) := uc(t, q(t), ġ(t))로 두고 여기서 u는 아래에 자세히 설명된 가속 컨트롤러입니다. ŷħ(q) := Rh(q)e2, 즉 엔드 이펙터 프레임의 ŷ-축을 정의합니다. 그런 다음 t &gt; tf에 대해 v(t) min{(t − tƒ)/ts, 1}로 설정합니다. 여기서 ts Є R&gt;0은 사용자가 설정한 상수로 &quot;감속&quot; 시간으로 표시됩니다. 그런 다음 라크로스 헤드의 원하는 평행 이동 vd 및 회전 wd 속도를 다음과 같이 정의합니다. === va(q(t),t) := vc(1 − v(t)) cos(πv(t))ŷh(q(t)) wa(q(t)) = −—π (ŷh(q(t)) × €3), 여기서 e3 := (0, 0, 1), ve ER은 (6)에서 정의된 원하는 캐칭 속도입니다. 따라서 원하는 병진 속도 vd는 엔드 이펙터의 ŷ축과 정렬되고 헤드를 vc에서 0으로 늦춥니다. 한편, 원하는 회전 속도는 그물을 위쪽을 가리키도록 정렬하려고 합니다. 이러한 속도의 조합은 공을 감싸 안기 위한 &quot;스쿠핑과 같은&quot; 동작을 초래합니다. 그런 다음 간단한 비례 피드백을 통해 원하는 가속도가 제공됩니다. vh = kv(vd − vn(9,ġ)), wn = kw (wd – wh (9,ġ)), (11) 여기서 wh(q, q)는 관성 프레임에서 표현된 라크로스 헤드의 회전 속도이고 kv, kw R&gt;0은 사용자가 설정한 상수 이득입니다. 이러한 가속도를 각 샘플링된 제어 단계에서 관절 가속도 uc로 변환하기 위해 오일러 적분을 사용하여 한 컨트롤러 시간 단계에 대해 현재(un (9, 9), wh(q, q))에서 (11)을 적분하고 q에서 FK 변환의 야코비안을 역전하여 업데이트된 원하는 관절 속도 ġ+ 세트를 계산합니다. 그런 다음 결과적인 원하는 속도 변화 ġ+ − ġ가 가속도 제약 조건에 따라 클리핑되어 최종 관절 가속도가 생성됩니다. uc. 마지막으로, uc에 대한 영차 유지를 가정하여 q = uc의 한 컨트롤러 시간 단계 적분을 수행하여 (q, ġ)를 업데이트합니다.-전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 포착 4. 블랙박스 그래디언트 감지 최적화 포착 문제(1)는 부분적으로 관찰 가능한 마르코프 결정 과정(POMDP)으로도 공식화할 수 있으며, 이는 튜플 (S, O, A, R, P)로 정의됩니다. 여기서 S는 O에 의해 부분적으로 관찰된 상태 공간, 관찰 공간, A는 동작 공간, R: S×A → R은 보상 함수, P: S× A ⇒ S&#39;는 동역학 함수입니다. 최적화 목표는 예상 총 에피소드 수익 J(0) = Er=(0,0,...,ST) Σt=0 (St, Te(t))을 최대화하는 매개변수화된 정책 To :A를 학습하는 것입니다. 상태 공간은 공의 위치, 속도, 예측된 궤적으로 구성되며, 이는 지각 시스템에서 생성된 원시 관찰과 불완전한 동역학 모델에서 근사되므로 POMDP 분류를 정당화합니다. 이 작업에서 우리는 블랙박스 정책 최적화(Choromanski et al., 2018b, 2019; Mania et al., 2018)를 통해 신경망 정책을 최적화합니다. 4.1. 보상 설계 블랙박스 정책 최적화의 성공을 보장하려면 신중한 보상 설계가 필요합니다. 보상 함수는 각각의 데이터 품질 차이로 인해 시뮬레이션과 실제에서 훈련할 때 다릅니다. 두 경우 모두 에피소드 중에 공에 가까이 다가가는 그물에 보상을 줍니다. 시뮬레이션에서는 캐치 전 방향 정렬과 공을 그물에 유지하는 것에 대한 안정성 보상을 추가로 보상합니다. 실제에서는 성공적인 캐치(센서에서 감지)에 대해 평평한 보상을 사용합니다. 마지막으로 시뮬레이션에서 위치/속도/가속도/갑작스러운 움직임에 대한 페널티를 제공하고 실제에서 하드웨어 한계 위반에 대한 페널티를 제공하여 과도한 움직임을 방지합니다. 모든 용어를 합산하여 순 보상을 산출합니다. 더 자세히 설명하면 다음과 같습니다. • • • • 개체 위치 보상(sim/real): 에피소드 중에 엔드 이펙터가 개체에 가장 가까운 거리를 계산합니다. 가장 가까운 거리는 20cm를 기준으로 지수 곡선으로 조정되며, 이 임계값 아래의 에피소드에 대해 1.0점을 받습니다. 개체 방향 보상(sim): 공이 네트에서 20cm 이내에 있을 때 네트의 방향을 계산합니다. 점수는 개체의 속도 벡터와 네트 축의 내적으로 계산되며 0~1 사이의 값으로 조정됩니다. • 개체 안정성 보상(sim): 이 보상은 개체가 네트에 가까워진 후(네트에서 20cm 이내) 얼마나 안정적으로 유지되는지 계산합니다. 닫기 기준을 입력하고 에피소드가 끝날 때까지 그대로 유지하면 0.2의 평면 보상이 제공됩니다. 안정성 보상의 나머지 0.8 부분은 공이 가까이 있는 동안 0.25초 동안 공의 속도를 측정하여 제공됩니다. 이 기간 동안의 각 시간 단계는 동일하게 기여하며 객체 속도에 따라 지수 곡선에서 점수가 매겨지며 0.2m/s 미만의 속도에서 최고점을 매겨 해당 시간 단계에 대해 전체 점수를 매깁니다. 전체 점수는 전체 0.25초 동안 속도를 0.2m/s 미만으로 유지합니다. 이 보상은 공이 그물에 있거나 가려져 있을 때 실제로 공 추적의 정밀도가 어렵기 때문에 시뮬레이션에서만 사용됩니다. 객체 잡기 보상(실제): 그물에 공이 있는지 여부를 안정적으로 감지할 수 있는 그물 근처에 부착된 근접 센서를 사용합니다. 센서가 0.25초 이상 그물에서 공을 계속 감지하면 공이 잡힌 것으로 선언됩니다. 이는 0 또는 1의 평평한 보상을 제공합니다. • 동적 제약 조건 초과에 대한 페널티(시뮬레이션): 관절 위치, 속도, 가속 및 저크 한계와 같은 로봇 제약 조건 내에서 작동하도록 정책이 학습하도록 하는 데 사용되는 여러 페널티 보상을 사용합니다. 페널티 보상은 에이전트 동작이 제약 조건 내에 있으면 1.0으로 구현되고, 제약 조건을 위반하는 정도에 따라 0으로 감소합니다. 보상은 타임스텝 수와 초과하는 정도에 따라 감소합니다.전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐치 • 동적 제약 조건(실제) 초과에 대한 페널티: 하드웨어는 오류 코드를 생성하고 움직임이 제약 조건을 초과하면 정지합니다.따라서 하드웨어가 오류 코드를 발견하든 발견하지 않든 이진 페널티를 할당합니다.객체 캐치 보상은 캐치 성공의 직접적인 척도이지만, 특히 초기 훈련 단계에서는 최적화를 위한 희소한 메트릭입니다.결과적으로 객체 위치 및 객체 방향 보상 항목은 보다 밀도 있는 지침을 제공하여 정책이 캐치 포즈에 맞게 네트를 올바르게 정렬하고 학습을 시작하도록 장려합니다.객체 안정성 보상은 공이 단순히 네트에서 튀어오르는 상호 작용을 페널티하는 데 필요했습니다.마지막으로, 동적 제약 조건 페널티는 물리적 제약 조건을 준수하기 위해 실제 로봇으로 성공적으로 전송하는 데 필요했습니다. 4.2. 추가 구현 세부 정보 이제 블랙박스 정책 최적화의 추가 구현 세부 정보를 설명합니다. 정책 네트워크: 2개의 타워로 구성된 CNN 신경망을 사용합니다. 첫 번째 타워는 크기가 (nhist, 7)인 이미지로 표현된 과거 조인트 위치를 처리합니다. 여기서 hist는 과거 타임스텝 수입니다. 두 번째 CNN 타워는 크기가 (npred, 6)인 이미지로 표현된 예측 볼 궤적을 처리합니다. 여기서 pred는 예측 타임스텝 수입니다. 두 타워의 출력은 단일 텐서로 연결되어 두 개의 완전히 연결된 레이어에 공급됩니다. 그런 다음 최종 출력은 명령된 조인트 속도로 간주됩니다. 정책 네트워크는 총 3255개의 매개변수를 갖습니다. 블랙박스 그래디언트 센싱 및 시뮬레이션-실제 미세 조정: 정책 신경망 매개변수 0을 최적화하기 위해 블랙박스 그래디언트 센싱(BGS)을 적용합니다. 이는 부분적으로
--- METHOD ---
s는 극도로 데이터 비효율적일 수 있지만, 원칙적으로 복잡하고 알려지지 않은 실제 세계 역학에 적응할 수 있습니다. 우리의 주요 기여는 통합된 전체 신체 다중 모드 행동 측면에서 반응 시간, 샘플 효율성, 분포 변화에 대한 견고성 및 다양성의 미묘한 상충 관계에 대한 통찰력을 제공하는 것입니다.
--- EXPERIMENT ---
씨. 우리는 민첩한 로봇 제어를 위한 &quot;고전적&quot; 및 &quot;학습 기반&quot; 기술을 융합하는 것에 대한 제안으로 마무리합니다. 실험 비디오는 여기에서 볼 수 있습니다: https://sites.google.com/view/agile-taking. 그림 1: 라크로스 헤드가 있는 모바일 매니퓰레이터가 1초 이내에 공을 잡습니다. (오른쪽) 조절 가능한 요 각도와 약 5m/s의 속도를 가진 자동 공 던지기. 1. 서론 공중에서 공을 쫓고 극적인 다이빙 캐치를 완료하는 것은 여러 인기 스포츠에서 기억에 남는 운동 능력의 순간이며 인간의 민첩성의 벤치마크입니다. 이 논문에서는 모바일 매니퓰레이터 플랫폼(그림 1 참조)에서 고속으로 움직이는 공을 추적, 가로채기 및 잡는 작업을 고려합니다. 이 플랫폼의 엔드 이펙터에는 라크로스 헤드가 장착되어 있습니다. 로봇은 1초도 채 안 되는 시간 내에 공에 대한 시각적 관찰을 지속적으로 실행 가능한 전신 동작으로 변환하여 베이스와 팔을 모두 조정된 방식으로 제어해야 합니다. 마지막 밀리초에서 © 2023 SAABNMBKCDDDJPSASVSSS.J.-JSaS Tu*. 전신 MPC와 블랙박스 정책 학습을 통한 민첩한 캐칭 제어 시스템은 지각적 폐색에 강해야 하며, 캐치를 안정화하고 바운스 아웃을 방지하기 위해 크래들링 기동을 실행해야 합니다. 이 작업의 물리학은 놀라울 정도로 복잡할 수 있습니다. 기하학적 단순성에도 불구하고 비행 중인 공은 항력과 마그누스 효과(Mehta, 1985)로 인해 예측할 수 없는 방식으로 스윙하고 휘어질 수 있습니다. 게다가 공과 변형 가능한 엔드 이펙터 간의 접촉 상호 작용에는 정확하게 모델링하기 어려운 복잡한 소프트 바디 물리학이 포함됩니다. 이 논문에서 우리는 설계 스펙트럼의 두 끝에서 이 작업을 위한 고속 시각적 피드백 컨트롤러를 합성하는 상대적 장점을 연구합니다. &quot;순수 제어&quot; 전략을 나타내는 모델 예측 제어(MPC)(Borrelli et al., 2017; Rawlings, 2000)와 &quot;순수 학습&quot; 접근 방식을 나타내는 블랙박스 정책 최적화(Choromanski et al., 2018a). MPC는 상태 불확실성에 대응하여 실시간으로 로봇 궤적을 최적화합니다. 데이터 요구 사항 측면에서 거의 &quot;제로샷&quot;이며 운동학, 역학 및 작업별 제약 조건을 우아하게 처리하지만 계산 비용이 많이 들고 역학 모델링의 오류에 민감할 수 있습니다. 반면 블랙박스 또는 RL(강화 학습) 방법을 통한 정책 학습은 매우 데이터 비효율적일 수 있지만 원칙적으로 복잡하고 알려지지 않은 실제 세계 역학에 적응할 수 있습니다. 우리의 주요 기여는 반응 시간, 샘플 효율성, 분포 변화에 대한 견고성 및 로봇 민첩성에 대한 통합 실험 평가에서 전신 다중 모달 행동 측면에서의 다재다능함이 입증되었습니다. 논문의 결론은 향후 작업에서 &quot;두 세계의 장점&quot;을 결합하는 제안으로 마무리합니다. 1.1. 관련 작업 두 가지 기술 클래스는 이전에 로봇 캐치 작업에 적용되었습니다. 공 캐치를 위한 최적화 기반 제어의 예로는 Hove 및 Slotine(1991); Hong 및 Slotine(1995); Yu et al.(2021); Frese et al.(2001); Kober et al.(2012)이 있습니다. Bäuml et al.(2010a) 및 Lampariello et al.(2011)은 캐치 포인트 선택, 캐치 구성 계산 및 경로 생성을 단일 비선형 최적화 문제로 포함하는 통합된 접근 방식을 제시합니다(Koç et al.(2018), Jia et al.(2019) 참조). 여러 논문에서 제어 스택의 일부에 대한 인간 시연과 머신 러닝을 활용합니다. Kim et al.(2014)은 다양한 실행 가능한 캐치 구성을 확률적으로 예측하고 손-팔 동작을 안내하는 컨트롤러를 개발하여 이를 통해 학습합니다. 인간 시범. Riley와 Atkeson(2002)도 인간 시범에서 동작 기본 요소를 학습하고 새로운 동작을 생성합니다. Dong 등(2020)은 2단계 동작 계획과 학습 기반 추적 컨트롤러를 사용합니다. 일부 논문은 명시적으로 소프트 캐치를 목표로 합니다. Salehian 등(2016)은 Kim 등(2014)을 더 확장하여 팔과 원하는 캐치 시간을 제어하는 데 있어 부정확성에 더 탄력적인 소프트 캐치 절차를 제공합니다. Bäuml 등(2011)은 Bäuml 등(2010a)을 더 확장하여 소프트 랜딩을 가능하게 합니다. Hong과 Slotine(1995); Lippiello와 Ruggiero(2012)는 소프트 캐치를 위한 휴리스틱을 추가하여 공의 예상 경로를 따라 손을 움직이면서 충격 에너지의 소산을 허용하기 위해 속도를 줄입니다. 2. 문제 공식화 및 제안된 해결책 우리는 잡을 대상의 궤적을 함수 Fo로 기술하는데, 이 함수는 질의 시간 t = R20을 시간 t에서의 대상의 위치와 속도, 즉 (p。(t), vo(t)) Є R³ × R³에 매핑합니다. 대상의 공기역학적 및 관성적 속성에 따라 Fo는 매우 사소하지 않을 수 있습니다. Fo에 대한 우리의 지식은 알려진 Fo를 통해 인코딩되는데, 이 Fo는 질의 시간 t = R20과 매개변수 집합 0.Є Rd를 시간 t에서의 대상의 위치와 속도에 대한 예측, 즉 (po(t; 0。), î。(t; 0。))에 매핑합니다. 전신 MPC와 블랙박스 정책 학습을 통한 민첩한 잡기 이 작업의 경우, 우리는 범위를 구형의 단단한 공으로 제한하고 고전적인 뉴턴 물리학을 통해 Fo를 구현합니다. 비사소한 공기 역학과 비균일한 모양을 가진 물체를 잡는 것은 향후 작업으로 남겨둡니다.그러나 우리는 두 대의 고정된 카메라를 통해 공의 위치와 속도를 간접적으로 관찰하고, 를 사용하여 비전 시스템의 현재 위치와 속도 추정치를 인코딩합니다.로봇의 경우, q = R7은 관절 구성 벡터를 나타내며, 여기서 q₁ = R은 평행 이동 기본 관절에 해당하고 92:7 R6은 팔 관절 각도를 나타냅니다.또한 FK : q Є R7 ↔ FK(q) = (Ph(9), Rh(9)) = R³ × SO(3)은 관절 구성 벡터 q를 로봇 엔드 이펙터(즉, 라크로스 헤드)의 SE(3) 포즈에 매핑하는 순방향 운동학 변환을 나타냅니다.2.1. 최적 제어를 통한 잡기 우리는 팔의 비선형 조작기 동역학을 보상하는 하위 수준 위치 및/또는 속도 컨트롤러가 있다고 가정합니다. 이 하위 수준 제어 시스템의 폐루프 동작을 추상화하고, q에 대한 2차 적분기 동역학¹을 가정하여 팔의 동작을 계획합니다.즉, ä(t) = ua(t) € R7입니다. 이 가정에 따라 최적 포착 문제(OCP)는 함수 ua(•)와 포착 시간 tƒ에 대한 자유-종료-시간 제약 최적 제어 문제로 공식화할 수 있습니다. Jua, tƒ) := ua (·),tf ƒª&#39; (\ + ||ua(r)||²) dr + ¥ (q(tƒ),ġ(ts),ts), (1)을 최소화합니다. 여기서 λ = R&gt;0은 가중치 상수이고 ¥ : R7 × R7 × R≥0 → 0은 최종 비용입니다. 2차 적분 동역학 ä(t) = ua(t) 및 다음 제약 조건에 따름: VT € [0,tƒ], Ua(T) E [Ua, Ua], q(7) E [¶‚¶], ġ(7) € ġ‚¶], c(q(tƒ), tƒ) ≥ 0. (2) 처음 세 가지 제약 조건은 제어 노력과 조인트 구성 및 속도에 대한 한계를 포착합니다. 최종 비용 및 끝점 제약 조건 함수 c는 잡기에 바람직한 두 가지 속성을 포착합니다: (i) SE(3) 잡기 시간 tƒ에서 공의 위치 및 속도 방향과 라크로스 헤드의 포즈 정렬, (ii) 공의 속도 벡터에 수직인 라크로스 헤드의 모든 잔류 속도 최소화. 우리는 하드 및 소프트 제약 조건을 통해 이러한 두 가지 속성을 포착합니다. 하드 끝점 제약 조건 c. 끝점 잡기 제약 조건은 들어오는 발사체를 받아들이기 위해 라크로스 헤드가 올바른 위치와 방향으로 배치되어야 한다는 요구 사항을 포착합니다. 특히, (po(tƒ), vo(tƒ))를 포획 시간 tƒ에서 물체의 실제 3D 위치와 속도라고 하자. 그러면 다음이 필요하다. vo(ts) Ph(q(t)) Po(t)||≤p, 그리고 (Rh(q(tƒ))e2)T: ||vo(tƒ)|| &gt; COS Єr, (3) 여기서 Єp, Єr Є R&gt;0은 각각 위치 및 각도 오차에 대한 규정된 허용 오차이고, e2 = (0, 1, 0)이다. 두 번째 제약 조건은 라크로스 헤드의 로컬 ŷ-축을 정렬하도록 강제하는데, 이는 그물의 포획 평면과 직교하고, tƒ에서 공의 속도 벡터이다. 1. 하위 수준 제어 시스템에는 지연을 포함하여 일부 사소하지 않은 폐루프 응답 특성이 있을 수 있다는 점에 유의한다. 그러나 이러한 문제는 계획된 (q, q) 궤적에서 명령된 (q, q) 궤적을 조정하여 미리 보상할 수 있습니다.전신 MPC와 블랙박스 정책 학습을 통한 민첩한 캐치 위의 제약 조건은 공의 실제 3D 위치와 속도에 대한 액세스를 가정하여 작성되었습니다. 그러나 매개변수 예측자 Fo(300)를 통해서만 이러한 수량에 대한 예측에 액세스할 수 있으므로 예측된 수량 po(tƒ;00),ûo(tƒ;00)에 대해 위의 제약 조건을 적용하여 엔드포인트 포착 제약 조건 함수 c(q(tƒ), tƒ; 0。)를 0에서 매개변수화합니다.소프트 터미널 비용 V. 위의 하드 제약 조건과 함께 터미널 비용 &amp;는 다음과 같은 형식을 취합니다.¥(q(tƒ),ġ(tƒ), tƒ;00) := Wpp(q(tƒ), tƒ;00) + wvvv (q(tƒ), ġ(tƒ)) p(q(tƒ), tƒ;00) = ||Ph(q(tƒ)) - Po(tƒ;00) ||² + (1 – (R = | (1) ¥v(q(tƒ), ġ(tƒ)) := || Rh(q(tf))vn(q(tf), ġ(tƒ)) — (4) 1- (Rr(q(tƒ))e2)T vo(tf;00) ||vo(tƒ;00)||, (5)(6) 여기서 wp, w₁ Є R≥0은 상수 가중치이고, v (q(tƒ), ġ(tƒ)) € R³는 관성 프레임에서 표현되고 야코비안 벡터 곱 qph (9)q를 통해 계산된 라크로스 헤드의 평행 이동 속도입니다. 상수 ve ER은 원하는 캐치 속도입니다. 따라서 최종 비용 ↳는 (3)에 정의된 캐치 시간 포즈 오류와 캐치 순간에 공의 속도 벡터에 수직인 라크로스 헤드의 움직임에 페널티를 부여합니다. 따라서 전체 OCP는 00에서 매개변수적이며, 공의 3D 예측 함수 Fo의 매개변수와 문제 매개변수 {Єp, Er, Vc, Wp, Wv, \}입니다.3. 순차적 이차 프로그래밍으로의 축소 OCP(1)는 자유 종료 시간 문제에 대한 최적성의 필요 조건을 활용하고 경계값 문제 솔버를 사용하여 해결할 수 있는 비자명한 문제입니다.그러나 이를 위해서는 동역학의 고밀도 이산화와 부등식 제약 조건(예: 배치를 통해)을 사용하여 제어, 상태 및 공동 상태 궤적을 최적화해야 합니다.대신 제한된 솔루션 클래스에서 가속 및 코스팅 단계 시퀀스를 최적화하여 계산 부담을 단순화하고, 그 과정에서 문제를 다단계 이산 시간 궤적 최적화 문제로 변환한 다음 최첨단 슈팅 기반 순차적 이차 프로그래밍(SQP) 솔버(Singh et al., 2022)를 사용하여 해결합니다. 3.1. 다단계 궤적 최적화로의 변환 가속 한계가 대칭 간격 [—ïa, ïa]에 의해 주어진다고 가정합니다.여기서 ïa Є Ro는 고정 벡터입니다.그러면 각 &quot;단계&quot;가 일정 가속 단계 다음에 일정 순항 단계로 구성된 N단계 이산 시간 궤적 최적화 문제를 정의할 수 있습니다.형식적으로 k € {0, ………, N − 1}에 대한 단계 k는 St[k] € R≥0인 St[k]초 동안 지속됩니다.그러면 단계 k의 가속 단계 내에서 관절 i = {1,…,7}은 (9i, ġi) [k]에서 시작하여 ±ga에서 가속하여 dġ¿[k]의 순 속도 변화를 달성합니다.순항 단계에서 관절은 St[k] — (|Sġi[k]\/ġa;)초 동안 ġi[k] + Sġi[k]의 일정 속도로 이동합니다. 위의 단계 전환을 요약하면 다음과 같습니다.복합 상태 x와 제어 u를 정의합니다.x[k] := (q[k],ġ[k],t[k]) € R15, u[k] := (8q[k], 8t[k]) € 전체 신체 MPC와 블랙박스 정책 학습을 통한 RAGILE CATCHING 그런 다음 단계 &quot;역학&quot;은 다음과 같이 작성됩니다.x[k + 1] = [q[k + 1]¯ ġ[k + 1] _t[k + 1] = о [q[k] + (ġ[k] +8ġ[k])6t[k] — (1/2)▲µ³ (8ġ[k] ○ |§ġ[k]|)] ġ[k] +8ġ[k] t[k] + St[k] (7) 여기서 o는 Hadamard 곱을 나타내고 A는 벡터 v의 대각 행렬 형태. u = (u[0],..., u[N - 1])이라 하자. 단계 등가 이산 시간 목적은 다음과 같이 주어진다. NJ(u) = Σ (A&amp;t[k] + ||8ġ[k]||²) + ¥(x[N]). k=(8) 참고 1 (1)의 적분 목적을 단계별 이산 시간 목적으로 정확하게 변환하면 단계 비용이 \St[k] +qz|8ġ[k]] 형태가 된다. 그러나 이것은 위에서 사용된 C² 매끄러운 목적보다 수치적으로 덜 견고한 것으로 밝혀졌다. = Nk= (3)의 최종 비용과 종점 포착 부등식 제약 조건이 직접 이월되어 x[N] = (q(tƒ),ġ(tƒ), tƒ)에 적용된다. 여기서 tƒ St[k]. 이제 (q, q, q)에 대한 한계 제약 조건을 다룬다. 가속의 경우 다음이 필요하다. |§ġ[k]| ≤ ġa&amp;t[k], k = 0,...,N - 1. ġ(t)가 단계 값 ġ[k] 사이를 선형 보간하므로 속도 한계 제약 조건은 단계 값에서만 적용하면 됩니다. ġ≤ġ[k]≤q‚_k=0,..., N. (10) 마지막으로 모든 7 Є [0, tƒ]에 대한 q(7)의 한계 제약 조건을 처리하려면 각 단계 내의 포물선(일정 가속도) 및 선형(순항) 프로필을 모두 고려해야 합니다. • 사례 1: ġi [k](ġi[k] +8ġi[k]) ≥ 0. 이 경우 qi(7)은 모든 □ Є [t[k], t[k + 1]]에 대해 {qi[k], qi[k+1]} 사이를 보간합니다. 따라서 우리는 끝점 qi [k], gik + 1]에만 한계 제약 조건을 적용해야 합니다. • 사례 2: ġi[k](ġi[k] + Sġi[k]) &lt; 0. 이 경우, [t[k], t[k+1]] 내에서 q₁(7)에 대한 로컬 최대/최소값이 존재하며, 여기서 ġ¿ (7) = 0입니다. 이 최대/최소값을 ĝ¿[k]로 표시합니다. 그런 다음 qi[k], qi[k + 1]에서 한계 제약 조건을 적용하는 것 외에도 ĝi [k]에 대한 제약 조건도 적용해야 합니다. i [k]에 대한 표현식은 다음과 같습니다.ĝi [k] = qi [k] + ġi [k]2äai ġi [k]if ġi [k] &gt;2äai if ġi [k] &lt; 0° 이산 시간 &quot;단계&quot; 역학, 최적화 목적 및 제약 조건이 주어지면 기성품 제약 이산 시간 궤적 최적화 솔버를 사용할 수 있습니다.이 작업에서는 Singh et al.(2022)에서 도입한 Dynamic Shooting SQP를 활용합니다.전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐칭 참고 사항 2 각 단계 내의 최대/최소 가속 및 순항 단계의 조합은 혼합 제어 노력/최소 시간 최적 제어 솔루션의 특성을 반영하며, 일반적으로 &quot;뱅오프뱅&quot; 전략이라고 합니다. 최근 연구(Sarkar et al., 2021)에 따르면 단일 제어 입력이 있는 LTI 시스템의 경우 엔드포인트 도달성 제약이 있는 혼합 제어 노력/최소 시간 문제에 대한 최적 솔루션은 일련의 &quot;뱅오프&quot; 단계입니다. 이는 원래 연속 시간 OCP의 이러한 단계적 감소를 사용하는 것을 정당화하며 사다리꼴 속도 프로파일을 사용하여 캐치하는 이전 연구(Bäuml et al., 2010b)와 정신이 비슷합니다. 3.2. 추가 구현 세부 정보 이제 SQP 구현에 대한 추가 구현 세부 정보를 설명합니다. 비동기 구현: 캐치 컨트롤러와 동시에 실행되는 것은 예측 매개변수 00의 업데이트를 생성하는 추정기로, 온라인 재계획이 필요합니다. 우리는 비동기 구현을 통해 이를 달성하는데, 여기서 최적화 문제는 0에 대한 최신 추정치와 현재 로봇 상태(q, q)를 사용하여 별도의 스레드에서 지속적으로 다시 해결됩니다. 로봇의 하위 PD 컨트롤러에 대한 명령된 (q, q)는 가장 최근의 단계별 솔루션을 연속 시간 궤적으로 디코딩하여 계산되므로 일관된 제어 속도가 보장됩니다. 후퇴하는 수평선이 없으므로 재계획은 전통적인 모델 예측 제어와 달리 공의 궤적에 대한 업데이트 추정치에 대한 응답으로 계획을 미세 조정하는 것과 더 유사합니다. 따라서 공의 궤적 예측에 오류가 없는 경우 문제는 재귀적으로 실행 가능한 상태로 유지됩니다. 크래들링: 공의 절편에 따라 q에서 2차 ODE로 모델링된 개방 루프 크래들링 동작 기본 요소를 사용하여 라크로스 헤드를 늦추고 동시에 그물을 위쪽을 가리키도록 회전합니다. 특히 t ≥ tƒ의 경우 ä(t) := uc(t, q(t), ġ(t))로 두고 여기서 u는 아래에 자세히 설명된 가속 컨트롤러입니다. ŷħ(q) := Rh(q)e2, 즉 엔드 이펙터 프레임의 ŷ-축을 정의합니다. 그런 다음 t &gt; tf에 대해 v(t) min{(t − tƒ)/ts, 1}로 설정합니다. 여기서 ts Є R&gt;0은 사용자가 설정한 상수로 &quot;감속&quot; 시간으로 표시됩니다. 그런 다음 라크로스 헤드의 원하는 평행 이동 vd 및 회전 wd 속도를 다음과 같이 정의합니다. === va(q(t),t) := vc(1 − v(t)) cos(πv(t))ŷh(q(t)) wa(q(t)) = −—π (ŷh(q(t)) × €3), 여기서 e3 := (0, 0, 1), ve ER은 (6)에서 정의된 원하는 캐칭 속도입니다. 따라서 원하는 병진 속도 vd는 엔드 이펙터의 ŷ축과 정렬되고 헤드를 vc에서 0으로 늦춥니다. 한편, 원하는 회전 속도는 그물을 위쪽을 가리키도록 정렬하려고 합니다. 이러한 속도의 조합은 공을 감싸 안기 위한 &quot;스쿠핑과 같은&quot; 동작을 초래합니다. 그런 다음 간단한 비례 피드백을 통해 원하는 가속도가 제공됩니다. vh = kv(vd − vn(9,ġ)), wn = kw (wd – wh (9,ġ)), (11) 여기서 wh(q, q)는 관성 프레임에서 표현된 라크로스 헤드의 회전 속도이고 kv, kw R&gt;0은 사용자가 설정한 상수 이득입니다. 이러한 가속도를 각 샘플링된 제어 단계에서 관절 가속도 uc로 변환하기 위해 오일러 적분을 사용하여 한 컨트롤러 시간 단계에 대해 현재(un (9, 9), wh(q, q))에서 (11)을 적분하고 q에서 FK 변환의 야코비안을 역전하여 업데이트된 원하는 관절 속도 ġ+ 세트를 계산합니다. 그런 다음 결과적인 원하는 속도 변화 ġ+ − ġ가 가속도 제약 조건에 따라 클리핑되어 최종 관절 가속도가 생성됩니다. uc. 마지막으로, uc에 대한 영차 유지를 가정하여 q = uc의 한 컨트롤러 시간 단계 적분을 수행하여 (q, ġ)를 업데이트합니다.-전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 포착 4. 블랙박스 그래디언트 감지 최적화 포착 문제(1)는 부분적으로 관찰 가능한 마르코프 결정 과정(POMDP)으로도 공식화할 수 있으며, 이는 튜플 (S, O, A, R, P)로 정의됩니다. 여기서 S는 O에 의해 부분적으로 관찰된 상태 공간, 관찰 공간, A는 동작 공간, R: S×A → R은 보상 함수, P: S× A ⇒ S&#39;는 동역학 함수입니다. 최적화 목표는 예상 총 에피소드 수익 J(0) = Er=(0,0,...,ST) Σt=0 (St, Te(t))을 최대화하는 매개변수화된 정책 To :A를 학습하는 것입니다. 상태 공간은 공의 위치, 속도, 예측된 궤적으로 구성되며, 이는 지각 시스템에서 생성된 원시 관찰과 불완전한 동역학 모델에서 근사되므로 POMDP 분류를 정당화합니다. 이 작업에서 우리는 블랙박스 정책 최적화(Choromanski et al., 2018b, 2019; Mania et al., 2018)를 통해 신경망 정책을 최적화합니다. 4.1. 보상 설계 블랙박스 정책 최적화의 성공을 보장하려면 신중한 보상 설계가 필요합니다. 보상 함수는 각각의 데이터 품질 차이로 인해 시뮬레이션과 실제에서 훈련할 때 다릅니다. 두 경우 모두 에피소드 중에 공에 가까이 다가가는 그물에 보상을 줍니다. 시뮬레이션에서는 캐치 전 방향 정렬과 공을 그물에 유지하는 것에 대한 안정성 보상을 추가로 보상합니다. 실제에서는 성공적인 캐치(센서에서 감지)에 대해 평평한 보상을 사용합니다. 마지막으로 시뮬레이션에서 위치/속도/가속도/갑작스러운 움직임에 대한 페널티를 제공하고 실제에서 하드웨어 한계 위반에 대한 페널티를 제공하여 과도한 움직임을 방지합니다. 모든 용어를 합산하여 순 보상을 산출합니다. 더 자세히 설명하면 다음과 같습니다. • • • • 개체 위치 보상(sim/real): 에피소드 중에 엔드 이펙터가 개체에 가장 가까운 거리를 계산합니다. 가장 가까운 거리는 20cm를 기준으로 지수 곡선으로 조정되며, 이 임계값 아래의 에피소드에 대해 1.0점을 받습니다. 개체 방향 보상(sim): 공이 네트에서 20cm 이내에 있을 때 네트의 방향을 계산합니다. 점수는 개체의 속도 벡터와 네트 축의 내적으로 계산되며 0~1 사이의 값으로 조정됩니다. • 개체 안정성 보상(sim): 이 보상은 개체가 네트에 가까워진 후(네트에서 20cm 이내) 얼마나 안정적으로 유지되는지 계산합니다. 닫기 기준을 입력하고 에피소드가 끝날 때까지 그대로 유지하면 0.2의 평면 보상이 제공됩니다. 안정성 보상의 나머지 0.8 부분은 공이 가까이 있는 동안 0.25초 동안 공의 속도를 측정하여 제공됩니다. 이 기간 동안의 각 시간 단계는 동일하게 기여하며 객체 속도에 따라 지수 곡선에서 점수가 매겨지며 0.2m/s 미만의 속도에서 최고점을 매겨 해당 시간 단계에 대해 전체 점수를 매깁니다. 전체 점수는 전체 0.25초 동안 속도를 0.2m/s 미만으로 유지합니다. 이 보상은 공이 그물에 있거나 가려져 있을 때 실제로 공 추적의 정밀도가 어렵기 때문에 시뮬레이션에서만 사용됩니다. 객체 잡기 보상(실제): 그물에 공이 있는지 여부를 안정적으로 감지할 수 있는 그물 근처에 부착된 근접 센서를 사용합니다. 센서가 0.25초 이상 그물에서 공을 계속 감지하면 공이 잡힌 것으로 선언됩니다. 이는 0 또는 1의 평평한 보상을 제공합니다. • 동적 제약 조건 초과에 대한 페널티(시뮬레이션): 관절 위치, 속도, 가속 및 저크 한계와 같은 로봇 제약 조건 내에서 작동하도록 정책이 학습하도록 하는 데 사용되는 여러 페널티 보상을 사용합니다. 페널티 보상은 에이전트 동작이 제약 조건 내에 있으면 1.0으로 구현되고, 제약 조건을 위반하는 정도에 따라 0으로 감소합니다. 보상은 타임스텝 수와 초과하는 정도에 따라 감소합니다.전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐치 • 동적 제약 조건(실제) 초과에 대한 페널티: 하드웨어는 오류 코드를 생성하고 움직임이 제약 조건을 초과하면 정지합니다.따라서 하드웨어가 오류 코드를 발견하든 발견하지 않든 이진 페널티를 할당합니다.객체 캐치 보상은 캐치 성공의 직접적인 척도이지만, 특히 초기 훈련 단계에서는 최적화를 위한 희소한 메트릭입니다.결과적으로 객체 위치 및 객체 방향 보상 항목은 보다 밀도 있는 지침을 제공하여 정책이 캐치 포즈에 맞게 네트를 올바르게 정렬하고 학습을 시작하도록 장려합니다.객체 안정성 보상은 공이 단순히 네트에서 튀어오르는 상호 작용을 페널티하는 데 필요했습니다.마지막으로, 동적 제약 조건 페널티는 물리적 제약 조건을 준수하기 위해 실제 로봇으로 성공적으로 전송하는 데 필요했습니다. 4.2. 추가 구현 세부 사항 이제 블랙박스 정책 최적화의 추가 구현 세부 사항을 설명합니다. 정책 네트워크: 2개의 타워로 구성된 CNN 신경망을 사용합니다. 첫 번째 타워는 크기가 (nhist, 7)인 이미지로 표현된 과거 조인트 위치를 처리합니다. 여기서 hist는 과거 타임스텝의 수입니다. 두 번째 CNN 타워는 크기가 (npred, 6)인 이미지로 표현된 예측된 볼 궤적을 처리합니다. 여기서 pred는 예측된 타임스텝의 수입니다. 두 타워의 출력은 단일 텐서로 연결되어 두 개의 완전 연결 계층에 공급됩니다. 그런 다음 최종 출력은 명령된 조인트 속도로 간주됩니다. 총 정책 네트워크에는 3255개의 매개변수가 있습니다. Blackbox Gradient Sensing 및 Sim-to-Real Finetuning: 방법의 단순성과 다양한 로봇 도메인에서의 최근 성공(Abeyruwan et al., 2022) 덕분에 부분적으로 정책 신경망 매개변수 0을 최적화하기 위해 Blackbox Gradient Sensing(BGS)을 적용합니다. 이 알고리즘은 원래 총 보상 목적 J(0)의 평활화된 버전 Jo(0)을 최적화합니다. 이는 다음과 같습니다. Jo(0) = Es~N(0,1a) [J(0 + 08)], 여기서 σ &gt; 0은 평활화의 정밀도를 제어하고 등방성 랜덤 가우시안 벡터입니다. 먼저 PyBullet(Coumans and Bai, 2016–2021)에 구현된 시뮬레이션 환경에서 학습합니다. 시뮬레이션에서 정책이 좋은 성과를 거두면 정책을 실제 로봇으로 옮기고 기계적 던지기를 사용하여 추가 BGS 미세 조정 단계를 실행합니다. 5. 실험 시뮬레이션에서 실제 로봇에서 SQP 및 블랙박스(BB) 에이전트를 모두 평가하고 던지는 사람의 다양한 분포 변화에서 성능을 탐구합니다.SQP 에이전트는 미분 가능한 최적 제어를 위한 JAX 라이브러리인 trajax(Frostig et al., 2021) 위에 구축된 최첨단 SQP 솔버(Singh et al., 2022)를 사용합니다.SQP 에이전트의 경우 단일 단계, 즉 (8)에서 N = 1이 높은 포획 성공률을 달성하기에 충분했지만 (6)의 소프트 터미널 비용에서 정의된 원하는 포획 속도와 일치시키는 데 유연성이 떨어지는 것으로 나타났습니다.BB 에이전트의 경우 Tensorflow Keras에서 구현된 정책 네트워크가 있는 분산 BGS 라이브러리(Choromanski et al., 2018a)를 사용합니다. 사용된 로봇은 1차원 Festo 선형 액추에이터에 장착된 ABB IRB 120T 6-DOF 암의 조합으로, 7-DOF 시스템을 생성합니다. 공의 위치는 훈련된 순환 추적 모델로 실행되는 Ximea MQ013CG-ON 카메라의 스테레오 쌍을 사용하여 결정됩니다. 오차 막대: Clopper-Pearson 방법을 사용하여 이항 신뢰 구간을 계산하여 최소 95% 적용 범위를 제공하는 오차 막대로 실제 로봇의 캐치 성공을 보여줍니다. 전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 캐치 추론 속도 및 반응 시간: BB 에이전트는 7ms(표준 0.160ms)의 시간 내에 단일 정책 작업을 계산하는 반면, SQP는 해결하는 데 43.046ms(표준 21.255ms)가 걸립니다. SQP는 비동기적으로 실행되므로 이 해결 시간이 에이전트를 차단하지 않는다는 점을 기억하세요. 동기 부분은 2.ms(표준 0.212ms)로 실행됩니다. 비전/하드웨어 조인트 데이터 처리에는 약 5ms가 걸립니다. 전체 에이전트는 75Hz에서 동기적으로 실행되도록 설정됩니다. 기계식 던지기는 로봇에서 3.9m 떨어져 있으며 수평 속도만 4.5m/s로 전달합니다. z-구성 요소를 포함하여 속도는 캐치 시간에 5.5m/s입니다. ~ 시뮬레이션에서 현실로의 전환: 그림 2는 SQP 및 BB 에이전트의 실제 로봇 캐치 성능을 강조합니다. 첫째, 시뮬레이션에서 BB 성능이 대부분 단조롭게 증가하는 반면(그림 2, 왼쪽), 이것이 반드시 실제 하드웨어에서 단조로운 개선으로 변환되지는 않는다는 것을 알 수 있습니다(그림 2, 가운데). 둘째, SQP는 실제로 전환할 때 BB에 비해 성능 저하가 적다는 것을 알 수 있습니다. 마지막으로, 미세 조정된 BB 에이전트가 SQP의 실제 성과와 일치하고(결국 이를 초과) 하려면 실제로 미세 조정을 40번 반복해야 합니다(반복당 30번의 공 던지기). 두 방법 모두 기계적 공 던지기에서 약 80~85%의 성공률을 달성합니다. 영어: Sim Catch Success 0.80.60.0.Sim Performance Sim2Real Across Checkpoints Fine-tuning Performance 0.0.Real Catch Success 0.0.0.0.Real Catch Success 0.BB ---SQP 0.0.בקו רקרכש 0.0.0.BB --- SQP BB ---SOP 0.0.0 5000 10000 15000 20000 25000 30000Sim 반복 횟수15000Sim 반복 횟수10 20 30 40 50Fine-tuning 반복 횟수그림 2: (왼쪽) sim에서 에이전트의 성능. (가운데) 미세 조정 없이 실수에서 에이전트의 성능. (오른쪽) 30k 반복 sim 체크포인트에서 시작하여 BB 에이전트를 미세 조정한 후 sim2real 전송의 성능. 각 반복은 30번의 기계적 볼 던지기에 해당합니다.BB 미세 조정 정책의 더 높은 분산은 시뮬레이션(100)에 비해 실수(30)에서 BGS 기울기 단계당 상당히 적은 수의 던지기를 사용한 결과입니다.분포 변화에 대한 견고성: 다음으로, 두 에이전트의 분포 이탈 던지기에 대한 견고성을 살펴봅니다.BB의 경우 이는 실제 정책에 대한 미세 조정 후입니다.세 가지 다른 분포 변화를 고려합니다.(i) 던지는 사람의 속도 변경, (ii) 던지는 사람의 요 각도 변경, (iii) 기계적 던지는 사람을 사용하는 대신 손으로 볼 던지기.처음 두 분포 변화는 그림 3에 표시되어 있습니다.그림 3(왼쪽)에서 BB는 빠른 던지기에 상당히 견고하지만 느린 던지기에서는 성능이 상당히 저하되는 것을 볼 수 있습니다. 이는 빠른 던지기(아마도 계산 병목 현상 때문일 가능성이 높음)에 대해 성능이 적당히 저하되지만 느린 던지기에는 상당히 견고한 SQP 에이전트와 대조적입니다. 그림 3(가운데)에서 두 에이전트 모두 분포 내 요 각도에서 비슷한 성능을 보이지만 분포 외 각도의 경우 SQP가 BB에 비해 성능을 더 잘 유지합니다. 마지막 분포 변화는 기계식 던지기 대신 던지는 사람에게 손으로 던지는 것(로브)과 관련이 있습니다. 손으로 던지는 경우 SQP 에이전트는 68.9%의 캐치 성공률(196회 이상)을 보인 반면 BB 에이전트의 캐치 성능은 2.0%(150회 이상)로 저하됩니다. BB 정책 캐치 성공 0.80.60. 전신 MPC와 블랙박스 정책을 통한 민첩한 캐치 수정된 던지기 속도에서 BB 대 SQP 학습 요 각도에 따른 캐치 성공 1. BB SQP 캐치 성공 0.80.60. SQP 캐치 방식. 1.0 왼쪽 구성 오른쪽 구성 0.0.SQP 캐치의 비율 0.0.0.BB SQP 0.0.00.빠른(~4.7m/s) 훈련(~4.5m/s) 느린(~4.1m/s) -10.0 -7.5 -5.0 -2.5 0.0 2.던지는 사람 요(도) 5.7.0.공 던지는 사람 손 던지기 그림 3: (왼쪽) 던지는 사람 속도가 빠른(~4.7m/s), 훈련(~4.5m/s), 느린(~4.1m/s) 던지기 사이에서 변할 때의 캐치 성능. (가운데) 던지는 사람 요 각도가 -9.5°에서 8°까지 변할 때의 캐치 성능. 훈련 분포가 -6°에서 6.3°(점선 수직 검은색 선으로 표시) 사이에서 변하는 점에 유의하십시오. (오른쪽) SQP 에이전트가 기계식 공 던지기와 핸드 던지기에서 좌우로 잡는 것의 분포. BB 에이전트는 공 던지기 분포의 학습 편향으로 인해 100% 오른쪽으로 잡는다는 점에 유의하세요. 핸드 던지기 분포에서 추가로 미세 조정할 수 있지만 필요한 던지기 횟수는 엄청나게 시간이 많이 걸립니다. 다중 모달리티: 그림 3(오른쪽)에서 SQP 에이전트는 던지는 사람의 편향과 일치하는 상당히 균등한 비율로 좌우 포즈 구성에서 공을 잡을 수 있음을 보여줍니다. 반면, BB 에이전트는 공 던지는 사람 분포가 오른쪽으로 던지는 쪽으로 편향(60/40%)되어 있기 때문에 오른쪽으로만 잡을 수 있습니다. 우리는 초기 학습 단계에서 정책이 던지기 분포의 분할을 활용하여 국소적으로 최적의 오른쪽 잡기 행동을 학습하고 나중에 이 전략을 미세 조정하여 베이스를 이동하여 왼쪽 공을 잡는다는 것을 직감합니다. 6.
--- CONCLUSION ---
및 향후 작업 미세 조정된 블랙박스 에이전트가 가장 높은 캐치 성공 성능을 보이는 반면, SQP 에이전트는 던지는 사람의 분포 변화에 훨씬 더 강인합니다. &quot;두 가지의 장점&quot;을 얻기 위해 블랙박스 최적화와 SQP를 결합한 다음 전략을 조사할 계획입니다. • 블랙박스 최적화(CMA, BGS 등)를 사용하여 조정 가능한 SQP 및 크래들링 매개변수 0 세트를 최적화합니다. • • 공의 이력과 현재 고유 감각에 따라 각 에피소드에 대해 SQP 및 크래들링 매개변수 0을 출력하도록 (더 작은) BB 정책을 최적화합니다. • SQP가 인터셉트 및 크래들링 바로 전에 BB 정책으로 제어가 넘겨지는 스위치 오버 지점까지 따르는 &quot;스위치 오버&quot; 정책을 연구합니다. BB 정책은 또한 언제 스위치해야 하는지를 나타내는 이진 변수를 출력합니다. 이는 BB 정책의 복잡성을 올바른 &quot;크래들링&quot; 동작을 포착하는 데만 국한합니다. 향후 확장에는 상당한 공기 역학(예: 2차 저항 및 Magnus 효과)이 있는 가벼운 공을 잡는 것과 같은 응용 프로그램이나 더 큰 비구형 물체를 잡는 것에 대한 적응적 비선형 동역학 예측의 도구 도입도 포함됩니다.전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 잡기 참고문헌 Saminda Abeyruwan, Laura Graesser, David B D&#39;Ambrosio, Avi Singh, Anish Shankar, Alex Bewley, Deepali Jain, Krzysztof Choromanski 및 Pannag R Sanketi. i-sim2real: 긴밀한 인간-로봇 상호 작용 루프에서 로봇 정책의 강화 학습. arXiv 사전 인쇄본 arXiv:2207.06572, 2022. B. Bäuml, T. Wimböck 및 G. Hirzinger. 핸드암 시스템으로 날아오는 공을 잡는 운동학적으로 최적의 방법. 2010 IEEE/RSJ 국제 지능형 로봇 및 시스템 컨퍼런스, 2592-2599쪽, 2010a. B. Bäuml, Oliver Birbach, T. Wimböck, U. Frese, Alexander Dietrich, G. Hirzinger. 모바일 휴머노이드로 날아오는 공을 잡는 방법: 시스템 개요 및 설계 고려 사항. 2011 제11회 IEEE-RAS 국제 휴머노이드 로봇 컨퍼런스, 513-520쪽, 2011. Berthold Bäuml, Thomas Wimböck, Gerd Hirzinger. 핸드암 시스템으로 날아오는 공을 잡는 운동학적으로 최적의 방법. 2010 IEEE/RSJ 국제 지능형 로봇 및 시스템 컨퍼런스. IEEE, 2010b. Francesco Borrelli, Alberto Bemporad, Manfred Morari. 선형 및 하이브리드 시스템을 위한 예측 제어. 케임브리지 대학교 출판부, 2017. 크시스토프 코로만스키, 마크 로랜드, 비카스 신드와니, 리처드 터너, 에이드리언 웰러. 확장 가능한 정책 최적화를 위한 컴팩트 아키텍처를 사용한 구조적 진화. 기계 학습 국제 컨퍼런스, 970-978페이지. PMLR, 2018a. 크시스토프 코로만스키, 마크 로랜드, 비카스 신드와니, 리처드 E. 터너, 에이드리언 웰러. 확장 가능한 정책 최적화를 위한 컴팩트 아키텍처를 사용한 구조적 진화. 기계 학습 국제 컨퍼런스의 회의록, 969-977페이지. PMLR, 2018b. Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang Yang, Atil Iscen, Jasmine Hsu, Vikas Sindhwani. 강화 학습을 위한 입증 가능한 견고한 블랙박스 최적화. Leslie Pack Kaelbling, Danica Kragic, Komei Sugiura, 편집자, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, 2019년 10월 30일 - 11월 1일, Proceedings, Proceedings of Machine Learning Research의 100권, 683-696쪽. PMLR, 2019. URL http://proceedings.mlr.press/v100/ choromanski20a.html. Erwin Coumans와 Yunfei Bai. Pybullet, 게임, 로봇 및 머신 러닝을 위한 물리 시뮬레이션을 위한 파이썬 모듈. http://pybullet.org, 2016-2021. K. Dong, Karime Pereida, F. Shkurti, and Angela P. Schoellig. Catch the ball: Accurate high-speed motions for mobile manipulators via inverse dynamics learning. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 6718-6725쪽, 2020. U. Frese, B. Bäuml, S. Haidacher, G. Schreiber, Ingo Schäfer, M. Hähnle, and G. Hirzinger. Offthe-shelf vision for a robotic ball catcher. Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium(Cat. No.01 CH37180), 3:1623–1629 vol.3, 2001. 전신 MPC와 블랙박스 정책 학습을 통한 민첩한 캐칭 Roy Frostig, Vikas Sindhwani, Sumeet Singh, Stephen Tu. trajax: 가속기의 미분 가능한 최적 제어, 2021. URL http://github.com/google/trajax. Won Hong과 J. Slotine. 능동적 시각을 사용한 손과 눈의 협응 실험. ISER에서, 1995. Barbara Hove와 J. Slotine. 로봇 캐칭 실험. 1991 American Control Conference, 380-386쪽, 1991. Y. Jia, M. Gardner, Xiaoqian Mu. 비행 중인 물체를 목표물에 치기. 국제 로봇 연구 저널, 38:451-485, 2019. 김승수, A. 슈클라, A. 빌리드. 비행 중인 물체 잡기. IEEE 로봇 거래, 30:1049-1065, 2014. J. 코버, M. 글리슨, M. 미스트리. 휴머노이드 로봇으로 캐치 앤 저글링 하기. 2012 제12회 IEEE-RAS 휴머노이드 로봇 국제 컨퍼런스(Humanoids 2012), 875-881쪽, 2012. 오칸 코치, 길례르미 J. 마에다, 얀 피터스. 로봇 탁구를 위한 온라인 최적 궤적 생성. Robotics Auton. Syst., 105:121–137, 2018. R. Lampariello, D. Nguyen-Tuong, Claudio Castellini, G. Hirzinger, Jan Peters. 실시간으로 최적의 로봇을 잡기 위한 궤적 계획. 2011 IEEE 국제 로봇 및 자동화 컨퍼런스, 3719-3726페이지, 2011. V. Lippiello와 F. Ruggiero. 반복적 궤적 추정 개선을 통한 3D 단안 로봇 볼 잡기. 2012 IEEE 국제 로봇 및 자동화 컨퍼런스, 3950-3955페이지, 2012. Horia Mania, Aurelia Guy, Benjamin Recht. 단순 임의 탐색은 강화 학습에 대한 경쟁적 접근 방식을 제공합니다. NeurIPS, 2018. R. Mehta. 스포츠 볼의 공기 역학. Annual Review of Fluid Mechanics, 17(1):151–189, 1985. James B Rawlings. 모델 예측 제어에 대한 튜토리얼 개요. IEEE 제어 시스템 매거진, 20(3):38-52, 2000. Marcia Riley와 C. Atkeson. 로봇 잡기: 인간-인간형 상호작용을 유도하기 위해. Autonomous Robots, 12:119–128, 2002. Seyed Sina Mirrazavi Salehian, Mahdi Khoramshahi, A. Billard. 비행 물체를 부드럽게 잡기 위한 동적 시스템 접근 방식: 이론과 실험. IEEE Transactions on Robotics, 32: 462-471, 2016. Rajasree Sarkar, Deepak U Patil, Indra Narayan Kar. lti 시스템에 대한 최소 시간-연료 최적 제어의 특성화. arXiv 사전 인쇄본 arXiv:2102.10831, 2021. Sumeet Singh, Jean-Jacques Slotine, Vikas Sindhwani. 폐루프 동적 SQP를 사용한 궤적 최적화. 2022년 국제 로봇 및 자동화 컨퍼런스(ICRA). IEEE, 2022. H. Yu, Dashun Guo, H. Yin, Anzhe Chen, Kechun Xu, Yue Wang, R. Xiong. 비행 중 고르지 않은 물체 포착을 위한 신경 운동 예측. ArXiv, abs/2103.08368, 2021. 전신 MPC 및 블랙박스 정책 학습을 통한 민첩한 포착 부록 A. 저자 기여 Saminda Abeyruwan, Alex Bewley, David D&#39;Ambrosio: 비전 시스템 및 칼만 필터링 구현. Krzysztof Choromanski: 모든 Blackbox 훈련 실행에 적용된 Blackbox Gradient Sensing 알고리즘을 (Deepali와 함께) 설계했습니다. 이 논문의 Blackbox 최적화 섹션을 작성했습니다. Deepali Jain: BB 에이전트를 위한 2타워 CNN 정책 아키텍처를 설계했습니다. 공 궤적 관찰을 위한 적응형 예측기와 BB 정책을 통합하고 시뮬레이션 실험을 실행하여 90%의 캐치 성공률을 달성했습니다. Anish Shankar: 실험 설계 및 실행, 하드웨어, 환경, 에이전트 개선에 대한 통찰력을 위한 결과 분석; 더 나은 에이전트 방향을 위한 연구 루프 반복에 대한 나머지 팀과 협업; 하드웨어 통합과 함께 캐치 환경을 설계 및 개발하고 적절한 에이전트 관찰/보상을 설계했습니다. Sumeet Singh: 최적 제어 공식화와 SQP 감소를 (시뮬레이션에서) 설계, 디버깅 및 다듬었습니다. 동기 SQP 에이전트의 코드를 작성하고 (Stephen Tu와 함께) 비동기 적응을 위해 페어링했습니다. Anish와 함께 실제 실험을 반복했습니다. 문제 공식화, 최적 제어, SQP 이론, 논문의 보관 섹션, 전반적인 편집을 작성했습니다. Pannag Sanketi: 프로젝트를 공동 설립하고 관리했습니다. 연구 방향, 실험 및 논문 스토리에 대해 조언했습니다. 논문의 일부를 작성했습니다. Vikas Sindhwani: SQP 솔버 내에서 사용되는 미분 운동학 및 iLQR 루틴을 작성했습니다. 시뮬레이션 환경 구현에 기여했습니다. 초기 BlackBox 실험을 수행했습니다. 논문 작성에 기여하고 전반적인 프로젝트 지침과 방향을 제공했습니다. Jean-Jacques Slotine: 연구 방향에 대해 조언하고 프로젝트 지침을 제공했습니다. Stephen Tu: 비동기식 SQP 구현을 작성하고 SQP 에이전트에서 성능 문제를 디버깅하여 실시간 사용에 실용적으로 만들었습니다. Anish와 함께 논문의 실험 섹션을 작성했습니다.
