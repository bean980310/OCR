--- ABSTRACT ---
텍스트 기반 확산 모델은 이미지 생성에서 전례 없는 능력을 발휘했지만, 비디오 모델은 시간적 모델링의 과도한 학습 비용으로 인해 여전히 뒤처져 있습니다. 학습 부담 외에도 생성된 비디오는 특히 긴 비디오 합성에서 외관 불일치와 구조적 깜빡임으로 어려움을 겪습니다. 이러한 과제를 해결하기 위해 자연스럽고 효율적인 텍스트-비디오 생성을 가능하게 하는 ControlVideo라는 학습 없는 프레임워크를 설계했습니다. ControlNet에서 변형한 Control Video는 입력 모션 시퀀스의 거친 구조적 일관성을 활용하고 비디오 생성을 개선하기 위한 세 가지 모듈을 도입합니다. 첫째, 프레임 간의 외관 일관성을 보장하기 위해 Control Video는 셀프 어텐션 모듈에서 완전한 크로스 프레임 상호 작용을 추가합니다. 둘째, 깜빡임 효과를 완화하기 위해 교대로 배치된 프레임에서 프레임 보간을 사용하는 인터리브 프레임 스무더를 도입합니다. 마지막으로 긴 비디오를 효율적으로 제작하기 위해 각 짧은 클립을 전체적으로 일관성 있게 합성하는 계층적 샘플러를 활용합니다. 이러한 모듈로 강화된 Control Video는 광범위한 모션 프롬프트 쌍에서 정량적, 질적으로 최첨단 기술을 능가합니다. 특히 효율적인 설계 덕분에 NVIDIA 2080Ti 하나를 사용하여 몇 분 내에 짧은 비디오와 긴 비디오를 모두 생성합니다. 코드는 https://github.com/YBYBZhang/ControlVideo에서 제공됩니다. 1
--- INTRODUCTION ---
대규모 확산 모델은 텍스트-이미지 합성[1, 22,26,29,32]과 이를 창의적으로 응용하는 데 있어 엄청난 돌파구를 마련했습니다[6,8,21,37]. 여러 연구[5,9,11,12,34]에서 이러한 성공을 비디오 분야에서 재현하려고 시도했습니다. 즉, 야생 세계에서 고차원의 복잡한 비디오 분포를 모델링하는 것입니다. 그러나 이러한 텍스트-비디오 모델을 학습하려면 엄청난 양의 고품질 비디오와 계산 리소스가 필요하므로 관련 커뮤니티에서 추가 연구와 응용 프로그램을 제한합니다. 과도한 학습 요구 사항을 줄이기 위해 텍스트-이미지 모델을 사용한 제어 가능한 텍스트-비디오 생성이라는 새롭고 효율적인 형태를 연구합니다. 이 작업의 목표는 텍스트 설명과 동작 시퀀스(예: 깊이 또는 에지 맵)를 모두 조건으로 하는 비디오를 생성하는 것입니다. 그림 1에서 보듯이, 처음부터 비디오 분포를 학습하는 대신 사전 훈련된 텍스트-이미지 생성 모델[26, 29]의 생성 기능과 동작 시퀀스의 대략적인 시간적 일관성을 효율적으로 활용하여 생생한 비디오를 생성할 수 있습니다.최근 연구[15,40]에서는 비디오 생성을 위해 ControlNet[43] 또는 DDIM 역전[35]의 구조적 제어 가능성을 활용하는 것을 탐구했습니다.모든 프레임을 독립적으로 합성하는 대신[15,40] 원래의 자기 주의를 더 희소한 크로스 프레임 주의로 대체하여 외관 일관성을 향상시킵니다.그럼에도 불구하고, 그들의 비디오 품질은 여전히 다음과 같은 측면에서 사실적인 비디오에 크게 뒤처집니다.(i) 일부 프레임 간의 외관 불일치(그림 4(a) 참조), (ii) 큰 동작 비디오의 눈에 띄는 아티팩트(그림 4(b) 참조), (iii) 프레임 간 전환 중 구조적 깜빡임.(i) 및 사전 인쇄본의 경우.검토 중.호수에서 움직이는 백조 ControlNet Temporal Extension ControlVideo 움직이는 백조. 호수에서 매혹적인 플라밍고가 고요한 물 속을 우아하게 헤엄칩니다. 구불구불한 길을 매끈한 검은색 오토바이를 타고 가는 남자. 제임스 본드가 해변에서 문워크를 하는 애니메이션 스타일. 그림 1: 훈련이 필요 없는 제어 가능한 텍스트-비디오 생성. 왼쪽: ControlVideo는 시간 축을 따라 팽창하여 ControlNet을 비디오 대응물에 맞게 조정하여 미세 조정 없이 고품질의 일관된 생성을 직접 상속하는 것을 목표로 합니다. 오른쪽: ControlVideo는 구조와 모양 모두에서 시간적으로 일관된 다양한 동작 시퀀스를 조건으로 하는 사실적인 비디오를 합성할 수 있습니다. 결과는 500% 확대 시 가장 잘 보입니다. (ii) 희소한 크로스 프레임 메커니즘은 쿼리와 키의 셀프 어텐션 모듈 간의 불일치를 증가시켜 사전 훈련된 텍스트-이미지 모델에서 고품질의 일관된 생성을 상속하는 것을 방해합니다. (iii)의 경우 입력 동작 시퀀스는 비디오의 거친 수준 구조만 제공하며 연속된 프레임 간에 원활하게 전환하지 못합니다. 이 작업에서 우리는 구조적 부드러움을 향상시키기 위한 인터리브 프레임 스무더와 함께 고품질의 일관된 제어 가능한 텍스트-비디오 생성을 위한 훈련 없는 ControlVideo를 제안합니다.Control Video는 완전한 크로스 프레임 상호 작용으로 셀프 어텐션을 확장하여 비디오 대응물에 적용하는 동시에 ControlNet[43]에서 아키텍처와 가중치를 직접 상속합니다.이전 작업[15, 40]과 달리, 우리의 완전한 크로스 프레임 상호 작용은 모든 프레임을 연결하여 &quot;더 큰 이미지&quot;가 되므로 ControlNet에서 고품질의 일관된 생성을 직접 상속합니다.인터리브 프레임 스무더는 선택된 순차적 타임스텝에서 인터리브 보간을 통해 전체 비디오의 플리커를 제거합니다.그림 3에서 볼 수 있듯이 각 타임스텝에서의 작업은 중간 프레임을 보간하여 인터리브된 3프레임 클립을 부드럽게 하고, 두 개의 연속된 타임스텝에서 조합하면 전체 비디오가 부드럽게 됩니다.스무딩 작업은 몇 개의 타임스텝에서만 수행되므로 보간된 프레임의 품질과 개별성은 다음의 노이즈 제거 단계를 통해 잘 유지될 수 있습니다. 효율적인 장편 비디오 합성을 가능하게 하기 위해, 장기적 일관성을 갖는 분리된 짧은 클립을 생성하는 계층적 샘플러를 추가로 도입합니다. 구체적으로, 긴 비디오는 먼저 선택된 키 프레임을 갖는 여러 개의 짧은 비디오 클립으로 분할됩니다. 그런 다음, 장거리 일관성을 위해 키 프레임이 완전한 크로스 프레임 어텐션으로 사전 생성됩니다. 키 프레임 쌍을 조건으로, 우리는 전역적 일관성을 갖는 해당 중간 짧은 비디오 클립을 순차적으로 합성합니다. 우리는 광범위하게 수집된 모션 프롬프트 쌍에 대해 실험을 수행합니다. 실험 결과는 우리의 방법이 질적, 양적으로 대안 경쟁자보다 우수하다는 것을 보여줍니다. 효율적인 설계, 즉 xFormers [17] 구현 및 계층적 샘플러 덕분에 ControlVideo는 하나의 NVIDIA 2080Ti를 사용하여 몇 분 안에 짧은 비디오와 긴 비디오를 모두 생성할 수 있습니다. 요약하면, 우리의 기여는 다음과 같습니다. • 우리는 완전한 교차 프레임 상호 작용, 인터리브 프레임 스무더, 계층적 샘플러로 구성된 제어 가능한 텍스트-비디오 생성을 위한 훈련 없는 제어 비디오를 제안합니다. • 완전한 교차 어텐션은 더 높은 비디오 품질과 외관 일관성을 보여주는 반면, 인터리브 프레임 스무더는 전체 비디오에서 구조적 깜빡임을 더욱 줄입니다. • 계층적 샘플러는 상용 GPU에서 효율적인 장편 비디오 생성을 가능하게 합니다. 2 배경 잠재 확산 모델(LDM)[29]은 이미지 공간이 아닌 잠재 공간에서 확산 프로세스를 적용하여 확산 모델[10]의 효율적인 변형입니다. LDM에는 두 가지 주요 구성 요소가 있습니다.또는 호수에서 움직이는 백조 ᏃᎢ ControlNet × T 단계 Smoother Frame InterleavedSelf-Attention Conv Block Z z Attn Block ZN-zł zł .N-Zt Temporal Inflation ZN-zť Zo ... Fully Cross-Frame Text Cross-Attn Fully Cross-Frame Attn Figure 2: ControlVideo 개요.일관된 모양을 위해 Control Video는 Self-Attention 모듈에 완전한 크로스 프레임 상호 작용을 추가하여 ControlNet을 비디오 대응물에 적용합니다.구조상의 깜빡임을 고려하여, 인터리브 프레임 Smoother가 통합되어 인터리브 보간을 통해 모든 프레임 간 전환을 부드럽게 합니다(자세한 내용은 그림 3 참조).먼저 인코더 &amp;를 사용하여 이미지 x를 잠재 코드 z = E(x)로 압축하고 디코더를 사용하여 이 이미지 x ≈ D(z)를 각각 재구성합니다. 둘째, DDPM 공식화[10]에서 이미지 잠재 코드 Zo ~ Pdata(20)의 분포를 학습하는데, 여기에는 전방 및 후방 프로세스가 포함됩니다. 전방 확산 프로세스는 각 타임스텝 t에서 점진적으로 가우시안 노이즈를 추가하여 zt를 얻습니다. q(zt|Zt−1) = N(zt; √√1 – ßtzt−1, ßtI), (1) 여기서 {ẞt}는 노이즈의 스케일이고 T는 확산 타임스텝의 수를 나타냅니다. 후방 노이즈 제거 프로세스는 위의 확산 프로세스를 역으로 수행하여 노이즈가 적은 zt-1을 예측합니다. Po(t−1) = N(Zt−1; μo (Zt, t), Σo (zt, t)). (2) μe와 Σ는 학습 가능한 매개변수 0을 갖는 노이즈 제거 모델 ε로 구현되며, 이는 간단한 목적 함수 — € (z, t) ||]로 학습됩니다. Lsimple := Eɛ(z),e~N(0,1),t[|| € — (3) 새로운 샘플을 생성할 때 z ~ N(0, 1)에서 시작하여 DDIM 샘플링을 사용하여 이전 타임스텝을 예측합니다. Zt-Zt Zt-1 = √√√αt-√√1 — α₁€ (z, t) αt &quot;예측된 zo&#39; +11 ·αt−1 · €0 (zt, t), (4) &quot;z+를 가리키는 방향&quot; 여기서 at = i=(1B). 단순화를 위해 t 타임스텝에서 &quot;예측된 zo&quot;를 표현하기 위해 를 사용합니다. 기본 모델로 안정 확산(SD) €0 (Zt, t, T)을 사용한다는 점에 유의하세요. 이는 수십억 개의 이미지-텍스트 쌍에서 사전 학습된 텍스트 가이드 LDM의 인스턴스화입니다. 7은 텍스트 프롬프트를 나타냅니다. ControlNet[43]을 사용하면 SD가 텍스트-이미지 합성 중에 깊이 맵, 포즈, 모서리 등과 같은 보다 제어 가능한 입력 조건을 지원할 수 있습니다. ControlNet은 SD와 동일한 U-Net[30] 아키텍처를 사용하고 작업별 조건을 지원하도록 가중치를 미세 조정하여 ε(zt, t, σ)를 €0(zt, t, C, T)로 변환합니다. 여기서 c는 추가 조건을 나타냅니다. SD와 ControlNet의 U-Net 아키텍처를 구별하기 위해 전자를 주 U-Net, 후자를 보조 U-Net으로 표시합니다. 3 ControlVideo N-Controllable 텍스트-비디오 생성은 동작 시퀀스 c = {c²}\¯¹과 텍스트 프롬프트 7을 조건으로 길이가 N인 비디오를 생성하는 것을 목표로 합니다. 그림 2에서 볼 수 있듯이 일관되고 효율적인 비디오 생성을 위해 ControlVideo라는 학습이 필요 없는 프레임워크를 제안합니다. 첫째, Control Video는 완전한 크로스 프레임 상호 작용을 채택하여 ControlNet에서 변형되었으며, 이는 품질 저하를 최소화하면서 모양 일관성을 보장합니다. 둘째, 인터리브 프레임 스무더는 순차적인 타임스텝에서 교대로 프레임을 보간하여 전체 비디오의 플리커를 제거합니다. 마지막으로, 계층적 샘플러는 전체적 일관성을 가진 짧은 클립을 개별적으로 생성하여 긴 비디오 합성을 가능하게 합니다.완전한 크로스 프레임 상호 작용.텍스트-이미지 모델을 비디오 대응물에 적용하는 주요 과제는 시간적 일관성을 보장하는 것입니다.ControlNet의 제어 가능성을 활용하여 모션 시퀀스는 구조에서 거친 수준의 일관성을 제공할 수 있습니다.그럼에도 불구하고 동일한 초기 노이즈를 사용하더라도 ControlNet으로 모든 프레임을 개별적으로 생성하면 모양에 엄청난 불일치가 발생합니다(그림 6의 행 2 참조).비디오 모양의 일관성을 유지하기 위해 모든 비디오 프레임을 연결하여 &quot;큰 이미지&quot;가 되므로 프레임 간 상호 작용을 통해 해당 콘텐츠를 공유할 수 있습니다.SD에서 자기 주의가 모양 유사성에 의해 구동된다는 점을 고려하면[40] 주의 기반 완전 크로스 프레임 상호 작용을 추가하여 전체적 일관성을 향상할 것을 제안합니다.특히 ControlVideo는 보조 U-Net을 ControlNet에서 유지하면서 시간 축을 따라 안정적 확산에서 주 U-Net을 팽창시킵니다. [11,15,40]과 유사하게, 3 × 3 커널을 1 × 3 × 3 커널로 대체하여 2D 합성곱 계층을 3D 대응 계층으로 직접 변환합니다.그림(오른쪽)에서 모든 프레임에 걸쳐 상호 작용을 추가하여 셀프 어텐션을 확장합니다.QKT Attention(Q, K, V) = Softmax( -) .V, 여기서 Q = W°zt, K = WK zt, V = W³ zt, = iN-여기서 Zt {z}는 시간 단계 t에서 모든 잠재 프레임을 나타내고 WQ, WK 및 WV는 각각 zł을 쿼리, 키 및 값으로 투영합니다.이전 작업 [15,40]은 일반적으로 셀프 어텐션을 더 희소한 교차 프레임 메커니즘으로 대체합니다.예: 모든 프레임이 첫 번째 프레임에만 주의를 기울입니다.그러나 이러한 메커니즘은 셀프 어텐션 모듈에서 쿼리와 키 간의 불일치를 증가시켜 비디오 품질과 일관성이 저하됩니다. 비교해보면, 우리의 완전 교차 프레임 메커니즘은 모든 프레임을 &quot;큰 이미지&quot;로 결합하고, 텍스트-이미지 모델과의 생성 갭이 적습니다(그림 6의 비교 참조). 게다가, 효율적인 구현을 통해, 완전 교차 프레임 어텐션은 짧은 비디오 생성(&lt; 16프레임)에서 메모리와 허용 가능한 계산 부담을 거의 가져오지 않습니다. 더 매끄럽습니다. 완전 교차 프레임 상호작용으로 생성된 인터리브 프레임 비디오는 외관상 유망하게 일관성이 있지만, 구조상 여전히 눈에 띄게 깜빡입니다. 입력 모션 시퀀스는 합성된 비디오의 구조적 일관성을 거칠게만 보장하지만, 연속된 프레임 간의 매끄러운 전환을 유지하기에는 충분하지 않습니다. 따라서 우리는 구조상 깜빡임 효과를 완화하기 위해 인터리브 프레임 매끄럽게 하는 방법을 추가로 제안합니다. 그림 3에서 보듯이, 우리의 핵심 아이디어는 중간 프레임을 보간하여 각 3프레임 클립을 매끄럽게 한 다음, 인터리브 방식으로 반복하여 전체 비디오를 매끄럽게 하는 것입니다. t-1단계 부드러움 매끄럽게 하기 - 복사 보간 | ... X-1-i-2 i-1 i i+1 i+2 ... iii i+☑ i+2 ... Xt-1-DDIM 노이즈 제거 i-2 ii i+1 i+...} xt→단계 t . 1-3 1-2 1-1 i i+1 +2 +3.x xt→특히, 우리의 인터리브 프레임 스무더는 연속적인 시간 단계에서 예측된 RGB 프레임에서 수행됩니다. 각 시간 단계에서의 작업은 짝수 또는 홀수 프레임을 보간하여 해당 3프레임 클립을 스무딩합니다. 이런 식으로, 두 개의 연속적인 시간 단계에서 스무딩된 3프레임 클립이 서로 겹쳐져 전체 비디오의 플리커가 사라집니다. 시간 단계 t에서 우리의 인터리브 프레임 스무더를 적용하기 전에, 먼저 zł에 따라 깨끗한 비디오 잠복 Zt→0을 예측합니다. 그림 3: 인터리브 프레임 스무더의 그림. 시간 단계 t에서 예측된 RGB 프레임 xt→0은 중간 프레임 보간을 통해 로 스무딩됩니다. 두 개의 순차적 타임스텝을 결합하면 전체 비디오에서 구조적 깜빡임이 줄어듭니다.Zt→Zt — √√1 – α±€ø (Zt, t, C, T) (6)√at 소스 비디오 구조 조건 Tune-A-Video Text2Video-Zero 제어 비디오(저희 비디오) 대담한 남자가 고산 자연 속의 위험하고 험준한 봉우리를 오르고 있습니다.(a) 깊이 맵 호기심 많은 황금빛 개가 바위 산길을 호기심 가득히 돌아다닙니다.(b) Canny Edges 그림 4: 깊이 맵과 canny edge를 조건으로 한 정성적 비교.저희 ControlVideo는 다른 제품보다 (a) 외관 일관성과 (b) 비디오 품질이 더 나은 비디오를 생성합니다.반대로 Tune-A-Video[40]는 소스 비디오의 구조를 상속하지 못하는 반면, Text2 Video-Zero[15]는 큰 동작 비디오에서 눈에 띄는 아티팩트를 가져옵니다.결과는 500% 확대 시 가장 잘 보입니다. zt→0을 RGB 비디오 xt→0 = :D(zt→0)로 투영한 후, 인터리브 프레임 스무더를 사용하여 보다 부드러운 비디오 act→o로 변환합니다. 부드러운 비디오 잠복 t→0 = ε(xt→0)에 따라 Eq. 4에서 DDIM 노이즈 제거에 따라 노이즈가 적은 잠복 zt-1을 계산합니다. Zt−1 = √αt−1žt→0 + √√1 at-1 €· €ð (Zt, t, C, T). (7) 주목할 점은 위의 프로세스가 선택된 중간 타임스텝에서만 수행된다는 점인데, 이는 두 가지 장점이 있습니다. (i) 새로운 계산 부담은 무시할 수 있고 (ii) 보간된 프레임의 개별성과 품질은 다음 노이즈 제거 단계에서 잘 유지됩니다. 계층적 샘플러. 비디오 확산 모델은 프레임 간 상호 작용과 시간적 일관성을 유지해야 하므로, 특히 긴 비디오를 제작할 때 상당한 GPU 메모리와 계산 리소스가 필요한 경우가 많습니다. 효율적이고 일관된 긴 비디오 합성을 용이하게 하기 위해 클립별로 긴 비디오를 제작하는 계층적 샘플러를 도입했습니다. 각 타임스텝에서 긴 비디오 z₁ = {z}}-¹가 선택된 키 프레임 Zt Ν 키 = {zk Ne} Ne k=0&#39; Ni=을 갖는 여러 개의 짧은 비디오 클립으로 분리됩니다.여기서 각 클립의 길이는 NC - 1이고 k번째 클립은 (k+1) Ne-1로 표시됩니다.그런 다음 z = {1}=kNc+• 장거리 코히어런스에 대한 완전한 크로스 프레임 어텐션을 갖는 키 프레임을 미리 생성하고 해당 쿼리, 키, 값은 다음과 같이 계산됩니다.= Qkey WQkey 키 Zt = &quot; Zt WK 키 Vkey &quot; WVzkey (8) 각 키 프레임 쌍을 조건으로 전체적 일관성을 유지하는 해당 클립을 순차적으로 합성합니다.Qk = W°zk, K³ = WK [zkNc, z(k+1)Nc], ŵk = WV [zkNc, z(k+1) Nc]. (9)표 1: 제어 비디오와 다른 방법의 정량적 비교. 일관성 측면에서 동작-프롬프트 쌍에 대해 평가하며, 가장 좋은 결과는 굵은 글씨로 표시했습니다. 방법 구조 조건 프레임 일관성(%) 프롬프트 일관성(%) Tune-A-Video [40] DDIM Inversion [35] 94.31.Text2 Video-Zero [15] Canny Edge 95.30.Control Video Canny Edge 96.30.Text2 Video-Zero [15] 제어 비디오 깊이 맵 깊이 맵 95.31.97.31.4 실험 4.1 실험 설정 구현 세부 정보. 제어 비디오는 ControlNet* [43]에서 가져왔으며, 인터리브 프레임 스무더는 가벼운 RIFE [13]를 사용하여 각 3프레임 클립의 중간 프레임을 보간합니다. 합성된 짧은 비디오는 길이가 15인 반면 긴 비디오는 일반적으로 약 프레임을 포함합니다. 달리 언급하지 않는 한, 해상도는 모두 512×512입니다. 샘플링하는 동안 50개의 타임스텝을 갖는 DDIM 샘플링[35]을 채택하고, 기본적으로 타임스텝 {30, 31}에서 예측된 RGB 프레임에 대해 인터리브 프레임 스무더를 수행합니다. xFormers[17]의 효율적인 구현을 통해, 우리의 Contro Video는 각각 약 10분과 10분 안에 하나의 NVIDIA RTX 2080Ti로 짧은 비디오와 긴 비디오를 모두 생성할 수 있었습니다. 데이터 세트. ControlVideo를 평가하기 위해 DAVIS 데이터 세트[24]에서 25개의 객체 중심 비디오를 수집하고 소스 설명에 수동으로 주석을 달았습니다. 그런 다음 각 소스 설명에 대해 ChatGPT[23]를 사용하여 5개의 편집 프롬프트를 자동으로 생성하여 총 125개의 비디오 프롬프트 쌍을 생성합니다. 마지막으로 Canny와 MiDaS DPT-Hybrid 모델[28]을 사용하여 소스 비디오의 에지와 깊이 맵을 추정하고 125개의 모션 프롬프트 쌍을 평가 데이터 세트로 형성합니다. 자세한 내용은 보충 자료에 나와 있습니다.지표.[5,40]에 따라 두 가지 관점에서 비디오 품질을 평가하기 위해 CLIP[25]을 채택합니다.(i) 프레임 일관성: 연속된 모든 프레임 쌍 간의 평균 코사인 유사도,(ii) 프롬프트 일관성: 입력 프롬프트와 모든 비디오 프레임 간의 평균 코사인 유사도.기준선.ControlVideo를 세 가지 공개적으로 사용 가능한 방법과 비교합니다.(i) Tune-AVideo[40]는 소스 비디오에서 미세 조정하여 안정적 확산을 비디오 대응물로 확장합니다.추론하는 동안 소스 비디오의 DDIM 역 코드를 사용하여 구조 지침을 제공합니다.(ii) Text2Video-Zero[15]는 ControlNet을 기반으로 하며 미세 조정 없이 안정적 확산에 대한 첫 번째 전용 교차 프레임 어텐션을 사용합니다.(iii) Follow-Your-Pose[18]는 안정적 확산으로 초기화되고 인간의 포즈 조건을 지원하기 위해 LAION-Pose[18]에서 미세 조정됩니다. 그 후, 시간적으로 일관된 비디오 생성을 가능하게 하기 위해 수백만 개의 비디오[41]로 학습합니다.4.2 정성적 및 정량적 비교 정성적 결과.그림 4는 먼저 (a) 깊이 맵과 (b) 캐니 에지에 조건부로 합성된 비디오의 시각적 비교를 보여줍니다.그림 4(a)에서 볼 수 있듯이, 우리의 제어 비디오는 대체 경쟁자보다 모양과 구조 모두에서 더 나은 일관성을 보여줍니다.Tune-A-Video는 모양과 세밀한 구조(예: 코트 색상 및 도로 구조)의 시간적 일관성을 유지하지 못합니다.깊이 맵의 동작 정보를 사용하여 Text2VideoZero는 구조에서 유망한 일관성을 달성하지만 여전히 비디오의 불일치한 모양(예: 코트 색상)에는 어려움을 겪습니다.게다가, 우리의 제어 비디오는 대규모 동작 입력을 처리할 때도 더 강력하게 수행됩니다.그림 4(b)에서 볼 수 있듯이, Tune-A-Video는 소스 비디오의 구조 정보를 무시합니다. Text2Video-Zero는 프레임 품질과 외관 일관성을 절충하기 위해 최초 전용 크로스 프레임 메커니즘을 채택하고, 눈에 띄는 아티팩트가 있는 후속 프레임을 생성합니다. 반면, 제안된 완전 크로스 프레임 메커니즘과 인터리브 프레임 스무더를 사용하면, 당사의 Control Video는 큰 움직임을 처리하여 고품질의 일관된 비디오를 생성할 수 있습니다. *https://huggingface.co/lllyasviel/ControlNet표 2: 사용자 선호도 연구. 숫자는 다른 방법보다 당사의 Control Video에서 합성한 비디오를 선호하는 평가자의 백분율을 나타냅니다. 방법 비교 Ours 대 Tune-A-Video [40] Ours 대 Text2Video-Zero [15] 비디오 품질 시간적 일관성 텍스트 정렬 73.6% 76.0% 83.2% 68.0% 81.6% 65.6% 소스 비디오 구조 조건 Tune-A-Video Text2Video-Zero Follow-Your-Pose Canny Edge 개별적 First-only 희소-인과적 완전 크로스 프레임 ControlVideo(Ours) 아이언맨이 도로에서 문워크를 합니다.그림 5: 포즈에 대한 정성적 비교.Tune-A-Video [40]는 원래 인간의 위치만 보존하는 반면, Text2Video-Zero [15]와 Follow-Your-Pose [18]는 외관상 불일치가 있는 프레임(예: 아이언맨의 얼굴 변화)을 생성합니다.Our Control Video는 구조와 외관 모두에서 더 나은 일관성을 달성합니다.완전 크로스 프레임 + 더 부드러움 강력한 코끼리가 험난한 지형을 꾸준히 행진합니다. 그림 6: 교차 프레임 메커니즘과 인터리브 프레임 스무더에 대한 정성적 절제 연구. 첫 번째 행에 캐니 에지가 주어지면, 우리의 완전한 교차 프레임 상호작용은 다른 메커니즘보다 더 높은 품질과 일관성을 가진 비디오 프레임을 생성하고, 우리의 스무더를 추가하면 비디오 부드러움이 더욱 향상됩니다. 그림 5는 인간 포즈에 따른 비교를 추가로 보여줍니다. 그림 5에서 Tune-A-Video는 소스 비디오의 거친 구조, 즉 인간 위치만 유지합니다. Text2Video-Zero와 Follow-Your-Pose는 일관되지 않은 모양의 비디오 프레임을 생성합니다. 예를 들어, 아이언맨의 얼굴이 바뀌는 것(4행)이나 배경에서 사라지는 물체(5행)가 있습니다. 이에 비해 우리의 Control Video는 더 일관된 비디오 생성을 수행하여 우수성을 보여줍니다. 보충 자료에서 더 정성적인 비교를 제공합니다. 정량적 결과. 또한 125개 비디오 프롬프트 쌍에서 우리의 ControlVideo를 기존 방법과 정량적으로 비교했습니다. 표 1에서, 깊이에 따라 조건지어진 우리의 제어 비디오는 프레임 일관성과 프롬프트 일관성 측면에서 최첨단 방법보다 성능이 뛰어나며, 이는 정성적 결과와 일치합니다.반대로, 소스 비디오에서 미세 조정에도 불구하고, Tune-A-Video는 여전히 시간적으로 일관된 비디오를 생성하는 데 어려움을 겪습니다.동일한 구조 정보에 따라 조건지어졌지만, Text2Video-Zero는 우리의 제어 비디오보다 프레임 일관성이 떨어집니다.각 방법에서, 깊이 조건 모델은 깊이 맵이 더 부드러운 동작 정보를 제공하기 때문에 캐니 조건 대응 모델보다 시간적 일관성과 텍스트 충실도가 더 높은 비디오를 생성합니다.4.3 사용자 연구 그런 다음 사용자 연구를 수행하여 깊이 맵에 따라 조건지어진 우리의 제어 비디오를 다른 경쟁 방법과 비교합니다.특히, 우리는 각 평가자에게 구조 시퀀스, 텍스트 프롬프트, 두 가지 다른 방법에서 합성된 비디오(무작위 순서)를 제공합니다. 그런 다음 세 가지 측정 각각에 대해 더 나은 합성 비디오를 선택하도록 요청합니다.(i) 비디오 품질, (ii) 시간적 일관성표 3: 교차 프레임 메커니즘 및 인터리브 프레임 스무더에 대한 정량적 절제 연구. 결과에 따르면 완전 교차 프레임 메커니즘이 다른 메커니즘보다 프레임 일관성이 더 뛰어나고 인터리브 프레임 스무더가 프레임 일관성을 크게 개선합니다.교차 프레임 메커니즘 프레임 일관성(%) 프롬프트 일관성(%) 시간 비용(분) 개별 첫 번째만 희소-인과 완전 완전 스무더 89.94.95.95.96.30.30.30.30.30.1.1.3.3.해변에 있는 증기선, 일몰, 스케치 스타일 프레임프레임프레임프레임프레임프레임프레임프레임프레임프레임프레임프레임프레임그림 7: 계층적 샘플링으로 제작한 긴 비디오.모션 시퀀스는 왼쪽 상단에 표시됩니다. 효율적인 샘플러를 사용하여 ControlVideo는 전체적인 일관성을 갖춘 고품질의 긴 비디오를 생성합니다. 모든 프레임에서 500% 확대 시 가장 잘 보이는 결과, (iii) 프롬프트와 합성 비디오 간의 텍스트 정렬. 평가 세트는 125개의 대표적인 구조-프롬프트 쌍으로 구성됩니다. 각 쌍은 평가자에 의해 평가되고, 우리는 최종 결과에 대해 다수결 투표를 합니다. 표 2에서 평가자는 세 가지 관점 모두에서 특히 시간적 일관성 측면에서 합성 비디오를 강력히 선호합니다. 반면, Tune-A-Video는 구조적 안내를 위한 DDIM 역전만으로 일관되고 고품질의 비디오를 생성하지 못하고, Text2Video-Zero도 품질과 일관성이 낮은 비디오를 생성합니다. 4.4 Ablation 연구 완전 교차 프레임 상호 작용의 효과. 완전한 크로스 프레임 상호작용의 효과를 보여주기 위해 다음 변형과 비교를 수행했습니다.i) 개별: 모든 프레임 간에 상호작용 없음, ii) 첫 번째 전용: 모든 프레임이 첫 번째 프레임에 주의를 기울임, iii) 희소-인과: 각 프레임이 첫 번째 프레임과 이전 프레임에 주의를 기울임, iv) 완전히: 완전한 크로스 프레임, 3절 참조.위의 모든 모델은 미세 조정 없이 ControlNet에서 확장된 것입니다.정성적 및 정량적 결과는 각각 그림 6과 표 3에 나와 있습니다.그림 6에서 개별 크로스 프레임 메커니즘은 심각한 시간적 불일치, 예를 들어 컬러 및 흑백 프레임으로 인해 어려움을 겪습니다.첫 번째 전용 및 희소-인과 메커니즘은 크로스 프레임 상호작용을 추가하여 일부 외관 불일치를 줄입니다.그러나 여전히 구조적 불일치와 눈에 보이는 아티팩트, 예를 들어 코끼리의 방향 및 복제된 코(그림 6의 행)가 있는 비디오를 생성합니다. 대조적으로 ControlNet과의 세대 간격이 적기 때문에, 우리의 완전한 크로스 프레임 상호작용은 더 나은 외관 일관성과 비디오 품질을 수행합니다. 도입된 상호작용은 1~2배의 추가 시간 비용을 가져오지만, 고품질 비디오 생성에는 허용 가능합니다. 인터리브 프레임 스무더의 효과. 우리는 제안된 인터리브 프레임 스무더의 효과를 추가로 분석합니다. 그림 6과 표 3에서, 우리의 인터리브 프레임 스무더는 구조적 깜빡임을 크게 완화하고 비디오 부드러움을 개선합니다. 4.5 긴 비디오 생성으로의 확장 긴 비디오를 생성하려면 일반적으로 메모리가 높은 고급 GPU가 필요합니다. 제안된 계층적 샘플러를 사용하여, 우리의 ControlVideo는 메모리 효율적인 방식으로 긴 비디오 생성(100개 이상의 프레임)을 달성합니다. 그림 7에서 볼 수 있듯이, 우리의 Control Video는 일관되게 높은 품질의 긴 비디오를 생성할 수 있습니다. 특히, 우리의 효율적인 샘플링의 이점을 통해, 하나의 NVIDIA RTX 2080Ti에서 512 x 512 해상도의 100개 프레임을 생성하는 데 약 10분 밖에 걸리지 않습니다. 긴 비디오의 더 많은 시각화는 보충 자료에서 찾을 수 있습니다.5
--- METHOD ---
영어: 대안 경쟁자보다 질적, 양적으로 더 우수한 성능을 발휘합니다. 효율적인 설계, 즉 xFormers[17] 구현 및 계층적 샘플러 덕분에 ControlVideo는 하나의 NVIDIA 2080Ti를 사용하여 몇 분 안에 짧은 비디오와 긴 비디오를 모두 생성할 수 있습니다. 요약하면, 우리의 기여는 다음과 같습니다. • 우리는 완전한 크로스 프레임 상호 작용, 인터리브 프레임 스무더, 계층적 샘플러로 구성된 제어 가능한 텍스트-비디오 생성을 위한 학습 없는 Control Video를 제안합니다. • 완전한 크로스 어텐션은 더 높은 비디오 품질과 외관 일관성을 보여주는 반면, 인터리브 프레임 스무더는 전체 비디오에서 구조적 깜빡임을 더욱 줄입니다. • 계층적 샘플러는 상용 GPU에서 효율적인 긴 비디오 생성을 가능하게 합니다. 2 배경 잠재 확산 모델(LDM)[29]은 이미지 공간이 아닌 잠재 공간에서 확산 프로세스를 적용하여 확산 모델[10]의 효율적인 변형입니다. LDM에는 두 가지 주요 구성 요소가 있습니다.또는 호수에서 움직이는 백조 ᏃᎢ ControlNet × T 단계 Smoother Frame InterleavedSelf-Attention Conv Block Z z Attn Block ZN-zł zł .N-Zt Temporal Inflation ZN-zť Zo ... Fully Cross-Frame Text Cross-Attn Fully Cross-Frame Attn Figure 2: ControlVideo 개요.일관된 모양을 위해 Control Video는 Self-Attention 모듈에 완전한 크로스 프레임 상호 작용을 추가하여 ControlNet을 비디오 대응물에 적용합니다.구조상의 깜빡임을 고려하여, 인터리브 프레임 Smoother가 통합되어 인터리브 보간을 통해 모든 프레임 간 전환을 부드럽게 합니다(자세한 내용은 그림 3 참조).먼저 인코더 &amp;를 사용하여 이미지 x를 잠재 코드 z = E(x)로 압축하고 디코더를 사용하여 이 이미지 x ≈ D(z)를 각각 재구성합니다. 둘째, DDPM 공식화[10]에서 이미지 잠재 코드 Zo ~ Pdata(20)의 분포를 학습하는데, 여기에는 전방 및 후방 프로세스가 포함됩니다. 전방 확산 프로세스는 각 타임스텝 t에서 점진적으로 가우시안 노이즈를 추가하여 zt를 얻습니다. q(zt|Zt−1) = N(zt; √√1 – ßtzt−1, ßtI), (1) 여기서 {ẞt}는 노이즈의 스케일이고 T는 확산 타임스텝의 수를 나타냅니다. 후방 노이즈 제거 프로세스는 위의 확산 프로세스를 역으로 수행하여 노이즈가 적은 zt-1을 예측합니다. Po(t−1) = N(Zt−1; μo (Zt, t), Σo (zt, t)). (2) μe와 Σ는 학습 가능한 매개변수 0을 갖는 노이즈 제거 모델 ε로 구현되며, 이는 간단한 목적 함수 — € (z, t) ||]로 학습됩니다. Lsimple := Eɛ(z),e~N(0,1),t[|| € — (3) 새로운 샘플을 생성할 때 z ~ N(0, 1)에서 시작하여 DDIM 샘플링을 사용하여 이전 타임스텝을 예측합니다. Zt-Zt Zt-1 = √√√αt-√√1 — α₁€ (z, t) αt &quot;예측된 zo&#39; +11 ·αt−1 · €0 (zt, t), (4) &quot;z+를 가리키는 방향&quot; 여기서 at = i=(1B). 단순화를 위해 t 타임스텝에서 &quot;예측된 zo&quot;를 표현하기 위해 를 사용합니다. 기본 모델로 안정 확산(SD) €0 (Zt, t, T)을 사용한다는 점에 유의하세요. 이는 수십억 개의 이미지-텍스트 쌍에서 사전 학습된 텍스트 가이드 LDM의 인스턴스화입니다. 7은 텍스트 프롬프트를 나타냅니다. ControlNet[43]을 사용하면 SD가 텍스트-이미지 합성 중에 깊이 맵, 포즈, 모서리 등과 같은 보다 제어 가능한 입력 조건을 지원할 수 있습니다. ControlNet은 SD와 동일한 U-Net[30] 아키텍처를 사용하고 작업별 조건을 지원하도록 가중치를 미세 조정하여 ε(zt, t, σ)를 €0(zt, t, C, T)로 변환합니다. 여기서 c는 추가 조건을 나타냅니다. SD와 ControlNet의 U-Net 아키텍처를 구별하기 위해 전자를 주 U-Net, 후자를 보조 U-Net으로 표시합니다. 3 ControlVideo N-Controllable 텍스트-비디오 생성은 동작 시퀀스 c = {c²}\¯¹과 텍스트 프롬프트 7을 조건으로 길이가 N인 비디오를 생성하는 것을 목표로 합니다. 그림 2에서 볼 수 있듯이 일관되고 효율적인 비디오 생성을 위해 ControlVideo라는 학습이 필요 없는 프레임워크를 제안합니다. 첫째, Control Video는 완전한 크로스 프레임 상호 작용을 채택하여 ControlNet에서 변형되었으며, 이는 품질 저하를 최소화하면서 모양 일관성을 보장합니다. 둘째, 인터리브 프레임 스무더는 순차적인 타임스텝에서 교대로 프레임을 보간하여 전체 비디오의 플리커를 제거합니다. 마지막으로, 계층적 샘플러는 전체적 일관성을 가진 짧은 클립을 개별적으로 생성하여 긴 비디오 합성을 가능하게 합니다.완전한 크로스 프레임 상호 작용.텍스트-이미지 모델을 비디오 대응물에 적용하는 주요 과제는 시간적 일관성을 보장하는 것입니다.ControlNet의 제어 가능성을 활용하여 모션 시퀀스는 구조에서 거친 수준의 일관성을 제공할 수 있습니다.그럼에도 불구하고 동일한 초기 노이즈를 사용하더라도 ControlNet으로 모든 프레임을 개별적으로 생성하면 모양에 엄청난 불일치가 발생합니다(그림 6의 행 2 참조).비디오 모양의 일관성을 유지하기 위해 모든 비디오 프레임을 연결하여 &quot;큰 이미지&quot;가 되므로 프레임 간 상호 작용을 통해 해당 콘텐츠를 공유할 수 있습니다.SD에서 자기 주의가 모양 유사성에 의해 구동된다는 점을 고려하면[40] 주의 기반 완전 크로스 프레임 상호 작용을 추가하여 전체적 일관성을 향상할 것을 제안합니다.특히 ControlVideo는 보조 U-Net을 ControlNet에서 유지하면서 시간 축을 따라 안정적 확산에서 주 U-Net을 팽창시킵니다. [11,15,40]과 유사하게, 3 × 3 커널을 1 × 3 × 3 커널로 대체하여 2D 합성곱 계층을 3D 대응 계층으로 직접 변환합니다.그림(오른쪽)에서 모든 프레임에 걸쳐 상호 작용을 추가하여 셀프 어텐션을 확장합니다.QKT Attention(Q, K, V) = Softmax( -) .V, 여기서 Q = W°zt, K = WK zt, V = W³ zt, = iN-여기서 Zt {z}는 시간 단계 t에서 모든 잠재 프레임을 나타내고 WQ, WK 및 WV는 각각 zł을 쿼리, 키 및 값으로 투영합니다.이전 작업 [15,40]은 일반적으로 셀프 어텐션을 더 희소한 교차 프레임 메커니즘으로 대체합니다.예: 모든 프레임이 첫 번째 프레임에만 주의를 기울입니다.그러나 이러한 메커니즘은 셀프 어텐션 모듈에서 쿼리와 키 간의 불일치를 증가시켜 비디오 품질과 일관성이 저하됩니다. 비교해보면, 우리의 완전 교차 프레임 메커니즘은 모든 프레임을 &quot;큰 이미지&quot;로 결합하고, 텍스트-이미지 모델과의 생성 갭이 적습니다(그림 6의 비교 참조). 게다가, 효율적인 구현을 통해, 완전 교차 프레임 어텐션은 짧은 비디오 생성(&lt; 16프레임)에서 메모리와 허용 가능한 계산 부담을 거의 가져오지 않습니다. 더 매끄럽습니다. 완전 교차 프레임 상호작용으로 생성된 인터리브 프레임 비디오는 외관상 유망하게 일관성이 있지만, 구조상 여전히 눈에 띄게 깜빡입니다. 입력 모션 시퀀스는 합성된 비디오의 구조적 일관성을 거칠게만 보장하지만, 연속된 프레임 간의 매끄러운 전환을 유지하기에는 충분하지 않습니다. 따라서 우리는 구조상 깜빡임 효과를 완화하기 위해 인터리브 프레임 매끄럽게 하는 방법을 추가로 제안합니다. 그림 3에서 보듯이, 우리의 핵심 아이디어는 중간 프레임을 보간하여 각 3프레임 클립을 매끄럽게 한 다음, 인터리브 방식으로 반복하여 전체 비디오를 매끄럽게 하는 것입니다. t-1단계 부드러움 매끄럽게 하기 - 복사 보간 | ... X-1-i-2 i-1 i i+1 i+2 ... iii i+☑ i+2 ... Xt-1-DDIM 노이즈 제거 i-2 ii i+1 i+...} xt→단계 t . 1-3 1-2 1-1 i i+1 +2 +3.x xt→특히, 우리의 인터리브 프레임 스무더는 연속적인 시간 단계에서 예측된 RGB 프레임에서 수행됩니다. 각 시간 단계에서의 작업은 짝수 또는 홀수 프레임을 보간하여 해당 3프레임 클립을 스무딩합니다. 이런 식으로, 두 개의 연속적인 시간 단계에서 스무딩된 3프레임 클립이 서로 겹쳐져 전체 비디오의 플리커가 사라집니다. 시간 단계 t에서 우리의 인터리브 프레임 스무더를 적용하기 전에, 먼저 zł에 따라 깨끗한 비디오 잠복 Zt→0을 예측합니다. 그림 3: 인터리브 프레임 스무더의 그림. 시간 단계 t에서 예측된 RGB 프레임 xt→0은 중간 프레임 보간을 통해 로 스무딩됩니다. 두 개의 순차적 타임스텝을 결합하면 전체 비디오에서 구조적 깜빡임이 줄어듭니다.Zt→Zt — √√1 – α±€ø (Zt, t, C, T) (6)√at 소스 비디오 구조 조건 Tune-A-Video Text2Video-Zero 제어 비디오(저희 비디오) 대담한 남자가 고산 자연 속의 위험하고 험준한 봉우리를 오르고 있습니다.(a) 깊이 맵 호기심 많은 황금빛 개가 바위 산길을 호기심 가득히 돌아다닙니다.(b) Canny Edges 그림 4: 깊이 맵과 canny edge를 조건으로 한 정성적 비교.저희 ControlVideo는 다른 제품보다 (a) 외관 일관성과 (b) 비디오 품질이 더 나은 비디오를 생성합니다.반대로 Tune-A-Video[40]는 소스 비디오의 구조를 상속하지 못하는 반면, Text2 Video-Zero[15]는 큰 동작 비디오에서 눈에 띄는 아티팩트를 가져옵니다.결과는 500% 확대 시 가장 잘 보입니다. zt→0을 RGB 비디오 xt→0 = :D(zt→0)로 투영한 후, 인터리브 프레임 스무더를 사용하여 보다 부드러운 비디오 act→o로 변환합니다. 부드러운 비디오 잠복 t→0 = ε(xt→0)에 따라 Eq. 4에서 DDIM 노이즈 제거에 따라 노이즈가 적은 잠복 zt-1을 계산합니다. Zt−1 = √αt−1žt→0 + √√1 at-1 €· €ð (Zt, t, C, T). (7) 주목할 점은 위의 프로세스가 선택된 중간 타임스텝에서만 수행된다는 점인데, 이는 두 가지 장점이 있습니다. (i) 새로운 계산 부담은 무시할 수 있고 (ii) 보간된 프레임의 개별성과 품질은 다음 노이즈 제거 단계에서 잘 유지됩니다. 계층적 샘플러. 비디오 확산 모델은 프레임 간 상호 작용과 시간적 일관성을 유지해야 하므로, 특히 긴 비디오를 제작할 때 상당한 GPU 메모리와 계산 리소스가 필요한 경우가 많습니다. 효율적이고 일관된 긴 비디오 합성을 용이하게 하기 위해 클립별로 긴 비디오를 제작하는 계층적 샘플러를 도입했습니다. 각 타임스텝에서 긴 비디오 z₁ = {z}}-¹가 선택된 키 프레임 Zt Ν 키 = {zk Ne} Ne k=0&#39; Ni=을 갖는 여러 개의 짧은 비디오 클립으로 분리됩니다.여기서 각 클립의 길이는 NC - 1이고 k번째 클립은 (k+1) Ne-1로 표시됩니다.그런 다음 z = {1}=kNc+• 장거리 코히어런스에 대한 완전한 크로스 프레임 어텐션을 갖는 키 프레임을 미리 생성하고 해당 쿼리, 키, 값은 다음과 같이 계산됩니다.= Qkey WQkey 키 Zt = &quot; Zt WK 키 Vkey &quot; WVzkey (8) 각 키 프레임 쌍을 조건으로 전체적 일관성을 유지하는 해당 클립을 순차적으로 합성합니다.Qk = W°zk, K³ = WK [zkNc, z(k+1)Nc], ŵk = WV [zkNc, z(k+1) Nc]. (9)표 1: Control Video와 다른 방법의 정량적 비교. 일관성 측면에서 동작-프롬프트 쌍에 대해 평가하며, 가장 좋은 결과는 굵은 글씨로 표시했습니다. 방법 구조 조건 프레임 일관성(%) 프롬프트 일관성(%) Tune-A-Video [40] DDIM Inversion [35] 94.31.Text2 Video-Zero [15] Canny Edge 95.30.Control Video Canny Edge 96.30.Text2 Video-Zero [15] Control Video Depth Map Depth Map 95.31.97.31.4
