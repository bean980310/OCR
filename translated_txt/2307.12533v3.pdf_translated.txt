--- ABSTRACT ---
ChatGPT를 대표로 수많은 회사가 대규모 Transformers 모델을 기반으로 하는 서비스를 제공하기 시작했습니다. 그러나 이러한 서비스를 사용하면 필연적으로 사용자의 프롬프트가 모델 제공자에게 유출됩니다. 이전 연구에서는 모델 매개변수와 클라이언트의 프롬프트가 비밀로 유지되는 안전한 다자간 계산(MPC)을 사용하여 Transformer 모델에 대한 안전한 추론을 연구했습니다. 그럼에도 불구하고 이러한 프레임워크는 여전히 모델 성능, 효율성 및 배포 측면에서 제한적입니다. 이러한 제한 사항을 해결하기 위해 빠르고 안전한 Transformer 모델 추론을 가능하게 하는 프레임워크 PUMA를 제안합니다. 저희 프레임워크는 GeLU 및 softmax와 같은 값비싼 함수에 대한 고품질 근사치를 설계하고 모델 성능을 유지하면서 안전한 추론 비용을 크게 줄입니다. 또한 Transformer 아키텍처를 훼손하지 않고 원하는 기능을 충실히 구현하는 안전한 Embedding 및 LayerNorm 절차를 설계합니다. PUMA는 최첨단 프레임워크인 MPCFORMER(ICLR 2023)보다 약 2배 빠르며 미세 조정 없이도 일반 텍스트 모델과 유사한 정확도를 제공합니다(이전 연구에서는 달성하지 못했습니다). PUMA는 LLaMA-7B를 약 5분 안에 평가하여 토큰 1개를 생성할 수도 있습니다. 저희가 아는 한, 이런 매개변수 크기의 모델을 MPC에서 평가할 수 있는 것은 이번이 처음입니다. PUMA는 SecretFlow-SPU¹의 Github 저장소에서 오픈 소스로 공개되었습니다. 1
--- INTRODUCTION ---
사전 훈련된 Transformer 모델(Vaswani et al., 2017)은 실제 작업에서 높은 성능을 보여 많은 주목을 받았고(Radford &amp; Narasimhan, 2018; Zhuge et al., 2021) Deep Learning as a Service(DLaaS) 패러다임에서 널리 사용되었습니다(Soifer et al., 2019). 그러나 이러한 서비스는 사용자가 서비스 제공자에게 개인 프롬프트를 공개하거나 서비스 제공자가 사용자에게 독점적으로 훈련된 가중치를 공개해야 하는 ChatGPT(Brown et al., 2020)의 경우와 같이 개인 정보 보호 문제를 일으킬 수 있습니다. 한국어: https://github.com/secretflow/spu/tree/main/examples/python/ml/flax_llama7b PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 Transformer 모델 서비스의 개인 정보 보호 문제를 해결하는 한 가지 솔루션은 안전한 다자간 계산(MPC)(Shamir, 1979; Yao, 1986; Goldreich et al., 1987)으로, 추론 중에 데이터와 모델 가중치를 비공개로 유지할 수 있습니다. (Hao et al., 2022; Li et al., 2023; Akimoto et al., 2023; Liang et al., 2023; Liu &amp; Liu, 2023)는 안전한 Transformer 모델 추론을 지원하는 다양한 방법을 제안했지만, 이러한 접근 방식에는 여전히 다음과 같은 단점 중 하나 이상이 있습니다. 추론 비용이 높습니다. GeLU 및 softmax와 같은 비선형 함수는 MPC에서 설계하기 어렵습니다. (Hao et al., 2022)는 이러한 비선형 함수를 충실한 방식으로 계산합니다. 예를 들어, 그들은 (Rathee et al., 2021)이 제안한 일반적인 MPC 지수화 방법을 기반으로 tanh를 사용하여 GeLU를 설계합니다. 그러나 이러한 일반적인 방법은 계산 및 통신 측면에서 매우 비용이 많이 들고 작은 비트 폭(예: 32 미만)에서만 테스트되었습니다. 재교육 필요. 비선형 함수의 비용을 줄이기 위해 여러 연구(Li et al., 2023; Akimoto et al., 2023; Liu &amp; Liu, 2023)는 ReLU 및 이차와 같은 더 간단한 함수를 사용하여 GeLU 및 softmax를 근사화할 것을 제안했습니다. 이러한 함수는 MPC에서 최대 10배 저렴하지만 Transformer 모델에 유틸리티 손실을 초래합니다. 결과적으로 모델 재교육(미세 조정)의 추가 단계가 필요합니다. 그러나 재교육은 데이터가 제한된 참가자에게 적합하지 않으며 만족스러운 성능을 달성하지 못할 수 있습니다(Kumar et al., 2022). 비호환 아키텍처. (Li et al., 2023; Liang et al., 2023)은 Transformer 모델의 아키텍처를 수정하여 보안 추론을 더욱 가속화할 것을 제안했습니다. 예를 들어, 임베딩 절차를 분해하거나 선형 계층을 재구성합니다. 더 나쁜 점은 (Li et al., 2023) 보안 LayerNorm을 지원하지 않고 BatchNorm을 사용하여 비용을 시뮬레이션하여 잘못된 보안 추론 결과를 생성한다는 것입니다. 이러한 수정은 기존 평문 Transformer 시스템과 충돌하며 배포 장애물로 이어질 것입니다. 요약하자면 MPC Transformer 추론 분야에서 모델 성능과 효율성을 모두 달성하는 것은 어려운 일이며 사람들은 다음과 같은 질문을 할 수 있습니다. 사전 학습된 대규모 Transformer 모델을 추가 재학습 없이 평문과 유사한 정확도로 안전하고 효율적으로 평가할 수 있을까요? 이러한 과제를 해결하기 위해 빠르고 정확한 엔드투엔드 보안 Transformer 추론 프레임워크인 PUMA 프레임워크를 제안합니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • 비선형 함수에 대한 새로운 근사치. 우리는 Transformer 모델에서 값비싼 비선형 함수(예: GeLU 및 softmax)에 대해 보다 정확하고 빠른 근사치를 제안합니다. 기존 연구와 달리, 우리는 이러한 비선형 함수의 특수한 속성을 기반으로 근사치를 설계하여 정확성과 효율성을 모두 달성합니다. • 보다 빠르고 정확한 보안 추론. 우리는 6개의 Transformer 모델과 4개의 데이터 세트에 대해 광범위한 실험을 수행했으며, 그 결과 PUMA의 정밀도는 평문과 유사하고 MPCFORMER보다 약 2배 빠릅니다(MPCFORMER는 PUMA와 유사한 정밀도를 달성하지 못함에 유의). PUMA는 약 5분 안에 LLaMA-7B를 평가하여 한 단어를 생성할 수도 있습니다. 우리가 아는 한, 이렇게 큰 언어 모델을 MPC에서 평가할 수 있는 것은 이번이 처음입니다. • 평문과 호환되는 엔드투엔드 프레임워크. 우리는 Transformer에 필요한 모든 계층(다른 작업에서는 누락된 Embedding 및 LayerNorm 계층 포함)을 MPC에서 설계하고 구현합니다. 이를 통해 사전 훈련된 일반 텍스트 Transfomer 모델(예: Hugging face에서 다운로드)을 쉽게 로드하고 안전하게 평가할 수 있습니다. 저희가 아는 한, PUMA는 재훈련과 같은 추가 수정 없이 사전 훈련된 Transformer 모델의 정확한 추론을 지원하는 최초의 오픈 소스 MPC 솔루션입니다. 조직. 요약하면 다음과 같습니다.
--- RELATED WORK ---
§ 2에서 설명하고 § 3에서 배경을 제시합니다. § 4에서 PUMA의 고수준 관점과 구체적인 설계를 제시합니다. § 5에서 실험 결과를 분석하고 § 6에서 이 작업을 마무리합니다. 2 관련 연구 안전한 다자간 계산(MPC)(Yao, 1986; Goldreich et al., 1987)을 사용하면 신뢰할 수 없는 당사자가 입력 내용을 비공개로 유지하면서 공동으로 함수를 계산할 수 있으며, MPC를 사용한 안전한 딥 러닝 추론은 높은 개인 정보 보호 기능으로 인해 많은 주목을 받았습니다. 이러한 연구는 2자 설정(Mohassel &amp; Zhang, 2017; Liu et al., 2017; Mishra et al., 2020; Huang et al., 2022; Patra et al., 2021; Rathee et al., 2020), 3자 설정(Wagh et al., 2019; Mohassel &amp; Rindal, 2018; Wagh et al., 2020; Kumar et al., 2019; Patra &amp; Suresh, 2020; Tan et al., 2021; Dong et al., 2023) 또는 4자 설정(Byali et al., 2020; Dalskov et al., 2021)을 포함한 다양한 모델과 아키텍처에서 작동합니다. 그러나 이러한 접근 방식의 대부분은 합성곱/심층 신경망의 안전한 추론만 고려하며 Transformer 모델을 지원하도록 직접 확장할 수 없습니다. 최근 여러 연구 작업(Hao 등, 2022; Li 등, 2023; Akimoto 등, 2023; Liang 등, 2023; Liu &amp; Liu, 2023)에서 MPC 기반 보안 추론 솔루션인 PUMA: Secure Inference of LLaMA-7B in Five Minutes A PREPRINT for Transformer 모델을 제안했지만 이러한 접근 방식은 여전히 모델 성능, 효율성 및 배포 측면에서 한계가 있습니다. 이러한 작업 중 MPCFORMER(Li 등, 2023)는 오픈 소스로 공개된 유일한 작업으로, 클라이언트와 서버에 대해 상관 관계가 있는 무작위성을 생성하기 위해 비공모 제3자를 사용하는 3자 프레임워크인 CrypTen(Knott 등, 2021)을 기반으로 합니다. 또한 비공모 가정을 갖춘 3자 모델은 다양한 MPC 설정 중에서 가장 구체적인 효율성을 가지고 있습니다. 따라서 우리는 주로 동일한 3자 설정에서 제안된 프레임워크 PUMA를 MPCFORMER와 비교합니다.3 배경 3.1 표기법 주로 사용되는 표기법은 다음과 같습니다.På는 i번째 컴퓨팅 당사자를 나타냅니다.i = {0,1,2}.대문자 굵은 글자 X는 행렬에 사용되고 소문자 굵은 글자 x는 벡터를 나타냅니다.✗[i]는 벡터 x의 i번째 요소를 나타내고 소문자 x는 스칼라 값에 사용됩니다.Ze는 모듈로 2º에 대한 이산 링을 나타내고 R은 실수를 나타냅니다.[]는 3개 중 2개 복제된 비밀 공유에 사용됩니다(Araki et al., 2016; Mohassel &amp; Rindal, 2018). 3.변압기 모델 변압기 모델은 언어 이해(Radford &amp; Narasimhan, 2018; Devlin et al., 2019; Yang et al., 2019; Touvron et al., 2023), 시각 이해(Zhuge et al., 2021; Dong et al., 2022; Chen et al., 2021) 등에서 놀라운 성공을 거두었습니다. 두 가지 인기 있는 변형은 Bert(변압기의 양방향 인코더 표현)(Devlin et al., 2019)와 GPT(생성적 사전 학습 모델)(Radford &amp; Narasimhan, 2018)입니다. 변압기 모델(Vaswani et al., 2017)은 주로 Embedding, Attention, Feed-Forward Network 및 LayerNorm 하위 계층으로 구성됩니다. Attention. 입력 (Q, K, V)이 주어지면, 어텐션 함수는 어텐션 (Q, K, V) = softmax(Q·KT+ M) V로 계산됩니다. 여기서 M은 바이어스 행렬로 볼 수 있습니다. 게다가, (Vaswani et al., 2017)은 다른 위치에서 다른 표현 부분 공간의 정보에 공동으로 주의를 기울이기 위해 Multi-Head Attention을 제안했습니다. . 피드포워드 네트워크(FFN). FFN은 각 위치에 별도로 동일하게 적용됩니다. 이것은 중간에 활성화가 있는 두 개의 선형 변환으로 구성되며, 가장 일반적으로 사용되는 활성화 함수는 GeLU입니다. 입력 x와 매개변수 {W1, b1, W2, b2}가 주어지면, FFN은 FFN(x) = W2GELU (W1x + b₁) + b2로 공식화할 수 있습니다. 선형 변환의 매개변수는 레이어마다 다르다는 점에 유의하세요. = LayerNorm. 벡터 x Є R^이 주어지면, LayerNorm은 다음과 같이 정의됩니다. LayerNorm(x)[i] = y. x + B, 여기서 (Y, B)는 훈련된 매개변수, μ = Σ=[], 및 σ = Σ²²±1(x[i] — µ)²입니다. n 3.3 2/3 복제 비밀 공유 = 비밀 값 x Є Ze는 x = x0+x1+x2(mod 2º)인 세 개의 랜덤 값 x0,x1, x2 € Z2에 의해 공유됩니다. 2/3 복제 비밀 공유([-]-공유로 표시)에서 당사자 P¿는 [x] i (xi, xi+1)을 얻습니다. 특별한 선언 없이 Ze에서 계산하고 간결성을 위해 (mod 2º)를 생략합니다. 산술 연산(예: +, − 및)을 지원하는 l &gt; 1(예: l 64)의 경우 이 유형을 산술 공유라고 하며 표기법 [·]을 사용합니다. 부울 공유([.]³)는 l = 1을 참조하는데, 여기서 (+, -)와 ·는 각각 비트별 및 A. = 추가로 대체됩니다. (C1, C2, C3)을 공개 상수로 하고 ([x], [y])를 두 개의 비밀 공유 값으로 합니다. 그러면 [c1x + C2Y + C3]은 (C1x0 + €20 + C3, C1x1 + C2Y1, C1x2 + C2y2)로 계산할 수 있습니다. 여기서 P¿는 로컬에서 자신의 공유를 계산할 수 있습니다. (c₁ = 1, c2 = 1, C3 = 0)이면 [x + y]를 얻습니다. 곱셈. 보안 곱셈 프로토콜 ПMul에서 두 공유 값 [x]와 [y]가 주어지면 당사자는 다음 단계를 따릅니다. i) 먼저 P¿가 zi = XiYi + Xi+1Yi + XiYi+1을 로컬하게 계산합니다. ii) 그런 다음 당사자는 P₂가 z&#39;i = αi + zi를 Pi−1로 보내도록 하여 재공유를 수행합니다. 여기서 a0 + α1 + a2 = 0입니다(P¿는 Mohassel &amp; Rindal(2018)과 같이 설정 단계에서 a¿를 생성할 수 있음). iii) 마지막으로 {(zo, z1), (z1, z2), (22, zó)}가 [x · y]를 형성합니다. 기본 프로토콜. PUMA는 덧셈과 곱셈 외에도 최신 기술에서 나온 부울 산술 곱셈(ПMula), 제곱 ПISquare, 등식 검정(ПEq), 미만(ПIT), 역수(II Recip), 최대값(ПMax) 및 제곱근의 역수(ПrSqrt) 등 여러 기본 프로토콜에 의존합니다.우리는 이를 블랙박스 방식으로 사용하고 다음과 같이 이러한 프로토콜의 입력 및 출력만 열거합니다.PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 • [2] = 제곱 ([x]), st z = = x² . [z] = [Mulba ([b]³, [x]), st z = b · x • [2] = ПRecip([x]), st z = = 1/x • • [z] = ПrSqrt([x]), st z = 1/√x • [z] = ПMax([x]), st z = maximum(x) [2] B = IIЕq ([x], [y]), st z = 1{x = y} [2]B=LT([x], [y]), st z = = 1{x &lt; y} 1{e}는 조건 e가 참일 때 1을 반환하고 그렇지 않으면 0을 반환합니다. 자세한 프로토콜 구성은 (Mohassel &amp; Rindal, 2018; Lu et al., 2020; Keller, 2020)을 참조하십시오. 고정 소수점 표현 및 잘림. 실수는 유한 링/체로 표현되기 전에 고정 소수점 숫자로 인코딩되어야 합니다. 오버플로를 방지하기 위해 각 고정 소수점 곱셈 후에 II rune을 사용하여 최소 f 비트를 안전하게 잘라내야 합니다.더 간단한 설명을 위해 기본적으로 Пul 및 Пsquare에 II runc를 포함하고 프로토콜 설계에서 명시적으로 언급하지 않습니다.위의 연산은 벡터와 행렬로 쉽게 확장할 수 있으며 단순성을 위해 벡터 및 행렬 연산에 동일한 표기법을 사용합니다.자세한 내용은 (Mohassel &amp; Rindal, 2018; Wagh et al., 2020)을 참조하세요.위협 모델.이전 연구(Mohassel &amp; Rindal, 2018; Li et al., 2023)에 따라 PUMA는 세 컴퓨팅 당사자 중 하나만 손상시키는 반쯤 정직한 적대자로부터 안전합니다.반쯤 정직하다는 것은 이러한 적대자가 프로토콜 사양을 따르지만 프로토콜 중에 다른 사람의 개인 정보를 알아내려고 시도할 수 있음을 의미합니다. PUMA는 추론 결과에 따른 공격으로부터 방어할 수 없으며, 이러한 공격(예: 차등 개인 정보 보호(Abadi et al., 2016))을 완화하는 것은 이 연구의 범위를 벗어난다는 점에 유의하십시오.4 PUMA의 안전한 설계 이 섹션에서는 먼저 PUMA에 대한 개요를 제시하고 PUMA에서 사용하는 안전한 GeLU, softmax, 임베딩 및 LayerNorm에 대한 프로토콜을 제시합니다.행렬 곱셈과 같은 선형 계층은 복제된 비밀 공유에서 간단하므로 이 원고에서는 주로 비선형 계층에 대한 프로토콜을 설명합니다.4.1 PUMA 개요 Transformer 모델의 안전한 추론을 달성하기 위해 PUMA는 세 가지 역할을 정의합니다.한 명의 모델 소유자, 한 명의 클라이언트, 세 명의 컴퓨팅 당사자입니다.모델 소유자와 클라이언트는 비밀 공유 형태로 컴퓨팅 당사자(예: Po, P1 및 P2)에게 모델 또는 입력을 제공한 다음 컴퓨팅 당사자는 MPC 프로토콜을 실행하고 결과를 클라이언트로 다시 보냅니다. 모델 소유자와 클라이언트도 컴퓨팅 당사자 중 한 명으로 작동할 수 있습니다. 일반성을 위해 별도로 설명합니다. 예를 들어 모델 소유자가 Po로 작동하고 클라이언트가 P₁로 작동하고 제3자 딜러가 P2로 작동하는 경우 시스템 모델은 MPCFORMER와 동일해집니다(Li et al., 2023). 안전한 추론 프로세스 동안 키 불변성이 유지됩니다. 모든 계층에서 컴퓨팅 당사자는 항상 이전 계층의 출력과 모델 가중치의 3개 중 2개의 복제된 비밀 공유로 시작하여 이 계층의 출력의 2개의 복제된 비밀 공유로 끝납니다. 공유는 각 당사자에게 정보를 누출하지 않으므로 이를 통해 임의의 깊이에 대해 계층을 순차적으로 결합하여 모든 Transformer 기반 모델에 대한 안전한 계산 체계를 얻을 수 있습니다. 4. 안전한 GeLU를 위한 프로토콜 대부분의 현재 접근 방식은 GeLU 함수를 더 작은 함수의 구성으로 보고 각 부분을 최적화하려고 시도하여 개인 GeLU 전체를 최적화할 기회를 놓치게 합니다. GeLU 함수가 주어졌을 때: GeLU(x) x (1+tanh П (x+0.044715 x³) (1) ≈x 시그모이드 (0.071355 · x³ + 1.595769. x) 이러한 접근 방식(Hao et al., 2022; Wang et al., 2022)은 함수 tanh에 대한 근사 프로토콜을 설계하거나 시그모이드에 대한 지수화 및 역수의 기존 일반 MPC 프로토콜을 사용하는 데 중점을 둡니다. 그러나 현재 접근 방식 중 어느 것도 GeLU 함수가 양쪽에서 거의 선형이라는 사실(즉, x &lt; −4의 경우 GeLU(x) ≈ 0, x &gt; 3의 경우 GeLU(x) ≈ x)을 활용하지 않았습니다. GeLU의 짧은 구간 [−4, 3] 내에서 저차 다항식의 조각별 근사가 securePUMA에 대해 더 효율적이고 구현하기 쉬운 선택이라고 제안합니다. 5분 안에 LLaMA-7B 만들기 사전 인쇄 알고리즘 1 보안 GeLU 프로토콜 II GeLU 입력: Pi는 3개 중 2개의 복제 비밀 공유 [x]]를 보유합니다.i = {0,1,2} 출력: På는 3개 중 2개의 복제 비밀 공유 [y]를 가져옵니다.¿ i = {0, 1, 2}의 경우, y 1: Po, P1 및 P2는 공동으로 다음을 계산합니다.= [bo] = IILT ([x], −4), [b1]³ = HLT([x], −1.95), [b2]B=LT (3, [x]), ▷ bo = 1{x &lt; −4} ▷ b₁ = 1{x &lt; −1.95} ▷ b₂ = 1{3 &lt; x} GeLU(x). BB 및 [20] B = [bo]³ &amp; [b1]³, [21]³ = [b1]³ + [b2]³ + 1, [22]³ = [b2] B를 계산합니다. zo = 1{−4 ≤ x &lt; −1.95}, %1 1{-1.95≤ x ≤ 3}, z2 1{x &gt; 3}에 유의하세요. = 2: [x²] = [Square ([x]), [x³] = [[mu!([x], [x²]), [x4] = ПSquare ([x²]), [x6] = ПSquare ([x³])를 공동으로 계산합니다. 3: {[x], [x²], [x³], [x4], [x6]}를 기반으로 다항식 [Fo(x)] 및 [F1(x)]]를 방정식 (2)로 안전하게 계산합니다. 4: [y] = [[Mulga ([20]³, [Fo(x)]) + IIMulBA ([21]³, [F1(x)]) + IIMulBA ([22]³, [x]). 프로토콜을 반환합니다. 구체적으로, 저희의 조각별 저차수 다항식은 방정식 (2)로 표시됩니다. x &lt; −Fo(x), -4&lt; x &lt; −1.F₁(x), GeLU(x) x, −1.95≤ x ≤x &gt;(2) 여기서 다항식 Fo() 및 F₁()은 라이브러리 numpy.ployfit²에 의해 방정식 (3)으로 계산됩니다. 놀랍게도 위의 간단한 폴리 피팅이 매우 잘 작동하고 최대 오차 &lt; 0.01403, 중간 오차 &lt; 4.41e – 05, 평균 오차 &lt; 0.00168입니다. Fo(x) = -0.011034134030615728x³- 0.11807612951181953x² F1(x) -0.42226581151983866x 0.= 0.0018067462606141187x6 - 0.037688200365904236x+0.3603292692789629x² + 0.5x + 0. 형식적으로 비밀 입력([x])이 주어지면 보안 GeLU 프로토콜 II GeLU는 알고리즘 1로 구성됩니다. 4.3 보안 Softmax 프로토콜 = . (3) 함수 Attention(Q, K, V) softmax(QK + M). V에서 핵심 과제는 함수 softmax를 계산하는 것입니다. 수치적 안정성을 위해 softmax 함수 는 softmax(x)[i] exp(x[i] — — €) Σ exp(x[i] — x - E (4)로 계산됩니다. 여기서 ã는 입력 벡터 x의 최대 요소입니다. 일반 평문 softmax의 경우 € = 0입니다. 2차원 행렬의 경우 각 행 벡터에 방정식(4)를 적용합니다. 형식적으로, 자세한 보안 프로토콜 II softmax는 알고리즘 2에 설명되어 있으며, 여기서 두 가지 최적화를 제안합니다. • 첫 번째 최적화의 경우 방정식 4의 e를 아주 작고 양의 값, 예: € = 10-6으로 설정하여 방정식 4의 지수 연산에 대한 입력이 모두 음수가 되도록 합니다. 가속을 위해 음수 피연산자를 활용합니다. 특히, 간단한 클리핑 negExp(x) = (1+ 2)²², x &lt; Texp x = [Texp, 0]을 사용하여 테일러 급수(Tan et al., 2021)를 사용하여 지수 연산을 계산합니다. (5) 실제로 x &lt; Texp 분기에 대해 보다 작은 것을 적용합니다. 2t로 나누는 것은 II Trunc를 사용하여 수행할 수 있습니다. 입력은 이미 음수입니다. 또한 제곱 함수 Пsquare와 IIrune의 t-단계 시퀀스를 사용하여 2t의 거듭제곱을 계산할 수 있습니다. MPC 프로그램이 18비트 고정 소수점 정밀도를 사용한다고 가정합니다. 그런 다음 Texp = −given exp(-14) &lt; 2−18로 설정하고 경험적으로 t = 5로 설정합니다. 2https://numpy.org/doc/stable/reference/generated/numpy.polyfit.htmlPUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 알고리즘 2 안전한 softmax 프로토콜 II softmax 입력: På는 i = {0, 1, 2}에 대해 3개 중 2개의 복제 비밀 공유 [×]¿를 보유하고 x는 크기가 n인 벡터입니다. 출력: Pi는 3개 중 2개의 복제 비밀 공유 [y]를 얻습니다. i = {0,1,2}에 대해 y = softmax(x)입니다. 1: Po, P₁ 및 P2는 [b]³ = IILT(Texp, [×]) 및 최대값 [x] = ПMax([x])를 공동으로 계산합니다. 2: 당사자는 [x] = [x] – [ã] – €를 로컬하게 계산하고 [z0] = 1 + II Trunc ([^])를 공동으로 계산합니다. 3: j = 1, 2, ..., t에 대해 4를 수행합니다. [zj] = IISquare ([Zj−1]). 5: 끝 6에 대해: 당사자는 [z] = Σ1 [z[i]]를 로컬하게 계산하고 [1/2]] = ПRecip([~])를 공동으로 계산합니다. 7: 당사자는 [z/z] = IIMul([z], [1/2])를 공동으로 계산합니다. 8: [y] = ПMulga ([b]³, [z/z])를 반환합니다. = • 두 번째 최적화는 나누기 횟수를 줄이는 것으로, 궁극적으로 계산 및 통신 비용을 절감합니다.이를 달성하기 위해 크기가 n인 벡터 x에 대해 Div(x, Broadcast(y)) 연산을 x . Broadcast()로 대체했습니다.여기서 y =Σx[i]입니다.이렇게 대체함으로써 n개의 나누기를 단 하나의 역수 연산과 n개의 곱셈으로 효과적으로 줄일 수 있습니다.이 최적화는 소프트맥스 연산의 경우에 특히 유용합니다.소프트맥스 연산의 1/4은 고정 소수점 값에서 충분한 정확도를 유지하기에 충분히 큽니다.결과적으로 이 최적화는 정확한 결과를 제공하면서 계산 및 통신 비용을 크게 줄일 수 있습니다.4.4 보안 임베딩 프로토콜 (Li et al., 2023)에 설명된 현재의 보안 임베딩 절차에서는 클라이언트가 로컬에서 토큰 ID를 사용하여 원핫 벡터를 생성해야 합니다.이는 원핫 벡터가 모델 내부에서 생성되는 평문 Transformer 워크플로와 다릅니다. 결과적으로, 그들은 사전 훈련된 모델에서 원핫 스텝을 조심스럽게 제거하고 클라이언트 측에 스텝을 추가해야 하는데, 이는 배포에 장애물이 될 수 있다.이 문제를 해결하기 위해 다음과 같이 안전한 임베딩 설계를 제안한다.토큰 id = [n]이고 모든 임베딩 벡터가 E(e, e,..., e)로 표시된다고 가정하면, 임베딩은 eid = E[id]로 공식화할 수 있다.(id, E)가 비밀 공유 방식인 경우, 안전한 임베딩 프로토콜 П Embed는 다음과 같이 작동한다.= • 컴퓨팅 당사자는 클라이언트로부터 [id]]를 받은 후 원핫 벡터 [o] B를 안전하게 계산한다.특히, i = [n]에 대해 [o[i]]B = Iɛq(i, [id])이다.• 당사자는 [eid] = [[Mulga ([E], [0] ³)를 통해 임베딩 벡터를 계산할 수 있으며, 여기서는 안전한 잘림이 필요하지 않다. 이런 방식으로, 우리의 ПEmbed는 더 많은 ПEq 및 ПMulBA 연산을 희생하고 평문 Transformer 모델의 워크플로를 명시적으로 수정할 필요가 없습니다.4.5 보안 LayerNorm 프로토콜 크기가 n인 벡터 x가 주어졌을 때, Layer Norm(x)[i] μ Σ=1×³]이고 σ = n νο i== γ .x[i] + B인 것을 기억하세요.여기서 (Y, B)는 훈련된 매개변수(x[i])입니다.MPC에서 핵심 과제는 나누기-제곱근 *(i) = 공식을 평가하는 것입니다.이 공식을 안전하게 평가하기 위해 CrypTen은 제곱근, 역수 및 곱셈의 MPC 프로토콜을 순차적으로 실행합니다. 그러나 *()_&quot;가 (x[i] — µ) · σ¯¹/²와 동일함을 관찰합니다. 그리고 MPC 측에서 역제곱근 σ-1/2를 계산하는 비용은 제곱근 연산과 비슷합니다(Lu et al., 2020). 게다가 § 4.3의 두 번째 최적화에서 영감을 받아 먼저 σ-1/2를 계산한 다음 빠르고 안전한 LayerNorm(x)을 지원하기 위해 (σ-1/2)를 브로드캐스트할 수 있습니다. 그리고 우리의 공식 프로토콜 II LayerNorm은 알고리즘 3에 나와 있습니다. 5 실험 평가 구현. 우리는 C++와 Python에서 SecretFlow-SPU(Ma et al., 2023) 위에 PUMA를 구현합니다. 우리는 18비트 소수 부분이 있는 링 Z264에서 고정 소수점 형태로 데이터를 인코딩합니다. 우리의 실험은 각각 32개 vCPU와 128GB RAM이 있는 3개의 Alibaba Cloud ecs.g7.8xlarge 서버에서 실행됩니다. CPU 모델은 Intel Xeon(Ice Lake) Platinum 8369BPUMA입니다: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 알고리즘 3 보안 LayerNorm 프로토콜 II LayerNorm 입력: Pi는 3개 중 2개의 복제 비밀 공유 [×] ¿를 보유합니다. i = {0, 1, 2}이고 x는 크기가 n인 벡터입니다. 출력: Pi는 3개 중 2개의 복제 비밀 공유 [y]를 얻습니다. i = {0, 1, 2}이고 yn LayerNorm(x)입니다. 1: Po, P1 및 P2는 [µ] = ±²²±1[×[i]] 및 [0] = Σ²±1 Square ([x] - [µ])[i]를 계산합니다. 2: 당사자는 공동으로 [σ¯¹/2] = ПrSqrt([0])를 계산합니다. 3: 당사자는 [c] = ПMul (([×] − [µ]), [σ¯¹/²])를 공동으로 계산합니다. 4: [y] = [Mul([y], [c]) + [ß]를 반환합니다. 표 1: CoLA, RTE 및 QNLI에서 Bert-Base, Roberta-Base 및 Bert-Large의 GLUE 벤치마크 성능, CoLA에 대한 Matthews 상관관계가 보고됩니다. 다른 데이터 세트에 대한 정확도가 보고됩니다. 모델 Bert-Base Roberta-Base Bert-Large TASK COLA RTE QNLI COLA RTE QNLI COLA RTE QNLI CPU 0.616 0.700 0.916 0.629 0.805 0.920 0.686 0.755 0.PUMA 0.613 0.700 0.916 0.618 0.805 0.918 0.690 0.747 0.CPU @ 2.70GHz. Linux 커널 5.4.0-144-generic이 있는 Ubuntu 20.04.6 LTS에서 PUMA를 평가합니다. 대역폭은 약 5Gbps이고 왕복 시간은 약 1ms입니다. 모델 및 데이터 세트. 우리는 7개의 NLP 모델에서 PUMA를 평가합니다: Bert-Base, Roberta-Base, BertLarge(Devlin et al., 2019); GPT2-Base, GPT2-Medium, GPT2-Large(Radford &amp; Narasimhan, 2018); LLAMA-7B(Touvron et al., 2023). 우리는 Corpus of Linguistic Acceptability(COLA), Recognizing Textual Entailment(RTE), GLUE 벤치마크의 Stanford Question Answering Dataset(QNLI)(Wang et al., 2019) 및 Wikitext-103 V1(Merity et al., 2016)에 대한 3개의 NLP 작업에 대한 Bert 성능을 측정합니다. 기준선. 우리는 PUMA를 가장 유사한 이전 작업인 MPCFORMER(Li et al., 2023)와 비교합니다. 하지만 공정한 비교를 위해 다음과 같은 고려 사항이 있습니다.i) MPCFORMER는 사전 학습된 변압기 모델 로딩을 지원하지 않고 LayerNorm을 충실하게 구현하지 않으므로³ 해당 프레임워크를 사용하여 의미 있는 보안 추론 결과를 얻을 수 없습니다.따라서 정밀도 보장을 보여주기 위해 일반 텍스트(부동 소수점)의 성능과 비교합니다.ii) Quad 근사를 사용한 MPCFORMER는 수정된 모델을 재학습해야 합니다.PUMA는 재학습이 필요하지 않으므로 Quad 근사가 없는 MPCFORMER의 비용과 비용을 비교합니다.또한 환경에서 MPCFORMER를 다시 실행합니다.5.1 정밀도 표 1과 2에서 보안 모델 추론 성능을 일반 텍스트(부동 소수점)의 성능과 비교하여 정밀도 보장을 보여줍니다.표 1에서 Bert-Base, Roberta-base 및 Bert-Large에서 일반 텍스트와 PUMA의 Matthews 상관 관계/정확도를 보여줍니다.PUMA가 달성한 정확도는 일반 텍스트 Flax 코드의 정확도와 일치합니다. 구체적으로, 정확도 차이는 모든 데이터 세트에서 0.011을 초과하지 않습니다. 게다가, 표 2에서 우리는 또한 데이터 세트 Wikitext-103 V1에 대한 우리의 복잡도를 GPT2 모델의 평문 기준선과 비교합니다. 결과는 유사하며, 모든 모델에서 복잡도 차이는 0.02를 초과하지 않습니다. 위의 정확도와 복잡도 이점은 우리 프로토콜이 수치적으로 정확하다는 것을 실험적으로 검증합니다. 3 MPCFORMER는 사전 학습된 Transformer 모델 로딩을 지원하지 않기 때문에, 우리는 MPCFORMER처럼 LayerNorm을 BatchNorm으로 대체하는 평문 Bert-Base에서 실험을 했습니다. 그 결과 CoLA 작업의 MCC 점수가 0.616에서 -0.020으로 상당히 떨어졌습니다. 반대로, PUMA는 0.613의 MCC 점수를 달성합니다. 표 2: Wikitext-103 V1에서 GPT2-Base, GPT2-Medium 및 GPT2-Large의 복잡도. 모델 CPU PUMA GPT2-Base 16.16.GPT2-Medium 12.12.GPT2-Large 10.10.PUMA: 5분 안에 LLaMA-7B의 안전한 추론 사전 인쇄 표 3: 길이가 128인 한 문장에 대한 Bert-Base, Roberta-Base 및 Bert-Large의 비용. 시간은 초 단위이고 통신(약칭 Comm.)은 GB 단위이며, 이는 다음 표의 경우에도 동일합니다. Roberta-Base 모델 비용 MPCFORMER PUMA 개선 Bert-Base Bert-Large 시간 통신 시간 통신. 시간 55.320 12.089 57.256 12.373 141.33.913 10.773 41.641 11.463 73.720 27.1.631 × 1.122× 1.375× 1.079× 1.916× 1.195x 통신 32.표 4: GPT2-Base, GPT2-Medium 및 GPT2-Large의 비용. 입력 문장의 길이는 32이고, 모든 비용은 1개 토큰을 생성하는 데 드는 비용입니다. 모델 비용 MPCFORMER GPT2-Base GPT2-Medium GPT2-Large 시간 통신 34.4.PUMA 15.3.Improv. 2.250x 1.325x 2.414x 시간 통신 73.078 11.30.272 7.1.667x 통신 시간 129.095 22.54.154 11.2.383× 1.884× 5.2 추론 비용 PUMA의 추론 비용을 MPCFORMER의 추론 비용과 비교합니다. 비용은 하나의 입력 문장을 처리하는 데 드는 비용입니다. i) Bert 모델의 경우 입력 문장의 길이는 128입니다. ii) GPT2 모델의 경우 입력 길이가 32이고 새로운 단어 1개를 생성합니다. 표 3의 3가지 Bert 모델에서 PUMA는 MPCFORMER보다 1.375 ~ 1.916배 빠르고 통신 효율성은 1.079 ~ 1.195배 더 높습니다. 표 4의 GPT2 모델의 경우 PUMA는 MPCFORMER보다 2.250 ~ 2.414배 빠르고 통신 효율성은 1.325 ~ 1.884배 더 높습니다. PUMA의 개선은 모델 크기가 커질수록 증가하는 것을 관찰할 수 있으며, 특히 GPT2 모델의 경우 그렇습니다. 이러한 추세는 대규모 평가를 처리할 때 특수 최적화가 더 효과적이기 때문입니다. 5.3 확장성 이 하위 섹션에서는 일괄 입력, 가변 길이 입력 및 가변 길이 출력(GPT2-Base에만 해당)에 대해 Bert-Base 및 GPT2-Base 모델에서 PUMA를 평가하는 비용을 측정합니다. 또한 개선 사항을 보여주기 위해 비용을 MPCFORMER의 비용과 비교합니다. 입력 길이 평가. 표 5는 가변 길이 입력에 대한 비용을 보여줍니다.길이 {64, 128, 256}의 입력에서 Bert-Base를 평가하고 길이 {16, 32, 64}의 입력에서 GPT2-Base를 평가합니다.Bert-Base의 경우 PUMA가 1.631 ~ 1.837배 빠르고 GPT2-Base의 경우 PUMA가 1.744 ~ 2.686배 빠릅니다.출력 길이 평가.그림 1은 GPT2-Base에 대한 가변 길이 출력에 대한 비용을 나타냅니다.MPCFORMER에 대한 개선 범위는 1.279 ~ 2.700배입니다.~표 5와 그림 1에서 GPT-2의 경우 입력/출력 토큰이 많을수록 효율성 향상이 감소하는 것을 알 수 있습니다.이는 PUMA가 추가적인 원핫 임베딩 비용을 도입하기 때문입니다(4.4에서 설명). PUMA는 평문 모델과 호환되며 MPCFORMER가 달성하지 못하는 평문 모델과 유사한 정확도를 달성할 수 있다는 점을 다시 한번 강조해야 합니다.표 5: 다양한 입력 길이에 대한 Bert-Base 및 GPT2-Base의 비용(#Input으로 표시).Bert-Base 및 GPT2-Base의 입력 길이는 각각 {64, 128, 256} 및 {16, 32, 64}입니다.GPT2-Base는 토큰 1개를 생성합니다.#Input 비용 64/128/256/시간 Bert GPTComm.시간 통신 MPCFORMER 36.354 5.707 55.320 12.PUMA 21.141 4.881 33.913 10.Improv. 1.720× 1.169× 1.631× 1.122x MPCFORMER 29.695 4.011 34.889 4.PUMA 11.056 1.875 15.506 3.임프로브 2.686× 2.139x 2.250x 1.324× 시간 112.통신 29.61.26.1.837x 1.151× 43.7.24.7.1.744× 0.936×시간(초) PUMA: 5분 안에 LLaMA-7B의 안전한 추론 사전 인쇄 MPC 이전 시간 Puma 시간 그림 1: 다양한 출력 토큰을 생성하기 위한 GPT2-Base의 런타임, 입력 길이는 32입니다. 표 6: LLaMA-7B의 안전한 추론 비용, #입력은 입력 문장의 길이를 나타내고 #출력은 생성된 토큰의 수를 나타냅니다. (#입력, #출력) (4, 1) (8,1) (8,2) 비용 PUMA 시간 122.통신 시간 통신 시간 통신. 0.907 200.473 1.794 364.527 3.5.4 5분 안에 LLaMA-7B 평가하기. 저희 프로토콜은 LLaMA-7B를 포함한 모든 Transformer 기반 모델을 평가하기에 이미 완성되었습니다. 안타깝게도 Protobuf(Varda, 2008) 및 FlatBuffers(van Oortmerssen, 2014)와 같은 기존 직렬화 라이브러리는 최대 2GB 크기의 데이터 트렁크만 지원하는데, 이는 대규모 MPC 작업에 충분하지 않습니다. 이 문제를 해결하기 위해 SecretFlow-SPU에 대한 최적화를 제안합니다. 구체적으로, 시스템은 통신하거나 I/O 작업을 수행할 때 지나치게 큰 비밀 공유 구조를 자동으로 더 작은 청크로 분할하고 직렬화할 수 있습니다. 우리는 각각 128개의 스레드와 1TB RAM, 20GB 대역폭, 0.1ms 왕복 시간을 가진 3개의 Alibaba Cloud ecs.r7.32xlarge 서버에서 PUMA를 사용하여 대규모 언어 모델 LLaMA-7B를 평가했습니다.표 6에서 볼 수 있듯이 PUMA는 합리적인 비용으로 LLaMA-7B의 안전한 추론을 지원할 수 있습니다.예를 들어, 8개의 토큰으로 구성된 입력 문장이 주어지면 PUMA는 1.794GB의 통신 비용으로 약 200초 안에 토큰 하나를 출력할 수 있습니다.저희가 아는 한, LLaMA-7B가 MPC를 사용하여 평가된 것은 이번이 처음입니다.게다가 PUMA는 평문 LLaMA-7B와 똑같은 토큰을 생성할 수 있습니다.예는 부록을 참조하세요.6 결론 우리는 복제된 비밀 공유를 기반으로 하는 Transformer 모델에 대한 안전한 추론을 위한 효율적인 MPC 프레임워크 PUMA를 제안합니다. 안전한 추론의 비용을 줄이기 위해, 우리는 정확한 다항식으로 값비싼 함수를 근사하고, 엔드투엔드 안전한 추론을 지원하기 위해 안전한 Embedding 및 LayerNorm 프로토콜을 제안합니다. 추론 비용은 여전히 상당히 높지만, Transformer 기반 DLaas에서 사용자의 개인 정보 보호 문제를 해결하는 데 한 걸음 더 다가갔습니다. 우리는 PUMA를 양자화와 결합함으로써
--- METHOD ---
(Rathee et al., 2021)이 제안했습니다. 하지만 이러한 일반적인 방법은 계산 및 통신 측면에서 매우 비용이 많이 들고 작은 비트폭(예: 32 미만)에서만 테스트되었습니다. 재교육 필요. 비선형 함수의 비용을 줄이기 위해 여러 연구(Li et al., 2023; Akimoto et al., 2023; Liu &amp; Liu, 2023)에서 ReLU 및 이차 함수와 같은 더 간단한 함수를 사용하여 GeLU 및 softmax를 근사화하는 것을 제안했습니다. 이러한 함수는 MPC에서 최대 10배 저렴하지만 Transformer 모델에 유틸리티 손실을 초래합니다. 결과적으로 모델 재교육(미세 조정)의 추가 단계가 필요합니다. 그러나 재교육은 데이터가 제한된 참가자에게 적합하지 않으며 만족스러운 성능을 달성하지 못할 수 있습니다(Kumar et al., 2022). 호환되지 않는 아키텍처. (Li et al., 2023; Liang et al., 2023)은 Transformer 모델의 아키텍처를 수정하여 보안 추론을 더욱 가속화할 것을 제안했습니다.예를 들어, 임베딩 절차를 분해하거나 선형 계층을 재구성합니다.더 나쁜 점은 (Li et al., 2023)이 보안 LayerNorm을 지원하지 않고 BatchNorm을 사용하여 비용을 시뮬레이션하여 잘못된 보안 추론 결과를 생성한다는 것입니다.이러한 수정은 기존 평문 Transformer 시스템과 충돌하며 배포 장애물로 이어질 것입니다.요약하자면 MPC Transformer 추론 분야에서 모델 성능과 효율성을 모두 달성하는 것은 어려운 일이며 사람들은 다음과 같은 질문을 할 수 있습니다.사전 훈련된 대규모 Transformer 모델을 추가 재훈련 없이 평문과 유사한 정확도로 안전하고 효율적으로 평가할 수 있을까요?이러한 과제를 해결하기 위해 빠르고 정확한 엔드투엔드 보안 Transformer 추론 프레임워크인 PUMA 프레임워크를 제안합니다.저희의 기여는 다음과 같이 요약할 수 있습니다.• 비선형 함수에 대한 새로운 근사치. 우리는 Transformer 모델에서 값비싼 비선형 함수(예: GeLU 및 softmax)에 대한 보다 정확하고 빠른 근사치를 제안합니다. 기존 작업과 달리 우리는 이러한 비선형 함수의 특수한 속성을 기반으로 근사치를 설계하여 정확도와 효율성을 모두 달성합니다. • 보다 빠르고 정확한 보안 추론. 우리는 광범위한
--- EXPERIMENT ---
6개의 트랜스포머 모델과 4개의 데이터 세트에 대한 결과는 PUMA의 정밀도가 평문과 유사하고 MPCFORMER보다 약 2배 빠르다는 것을 보여줍니다(MPCFORMER는 PUMA와 유사한 정밀도를 달성하지 못함에 유의).PUMA는 약 5분 안에 LLaMA-7B를 평가하여 한 단어를 생성할 수도 있습니다.저희가 아는 한, 이렇게 큰 언어 모델을 MPC에서 평가할 수 있는 것은 처음입니다.• 평문과 호환되는 엔드투엔드 프레임워크.다른 작업에서는 누락된 Embedding 및 LayerNorm 계층을 포함하여 Transformer에 필요한 모든 계층을 MPC에서 설계하고 구현합니다.이를 통해 사전 학습된 평문 Transfomer 모델(예: Hugging face에서 다운로드)을 쉽게 로드하고 안전하게 평가할 수 있습니다.저희가 아는 한, PUMA는 재학습과 같은 추가 수정 없이 사전 학습된 Transformer 모델의 정확한 추론을 지원하는 최초의 오픈 소스 MPC 솔루션입니다.조직. §2에서 관련 연구를 요약하고 §3에서 배경을 제시합니다. §4에서 PUMA의 개략적 관점과 구체적인 설계를 제시합니다. §5에서 실험 결과를 분석하고 §6에서 이 연구를 마무리합니다. 2 관련 연구 안전한 다자간 계산(MPC)(Yao, 1986; Goldreich et al., 1987)을 사용하면 신뢰할 수 없는 당사자가 입력 내용을 비공개로 유지하면서 공동으로 함수를 계산할 수 있으며, MPC를 사용한 안전한 딥러닝 추론은 높은 개인 정보 보호 기능으로 인해 많은 주목을 받았습니다. 이러한 연구는 2자 설정(Mohassel &amp; Zhang, 2017; Liu et al., 2017; Mishra et al., 2020; Huang et al., 2022; Patra et al., 2021; Rathee et al., 2020), 3자 설정(Wagh et al., 2019; Mohassel &amp; Rindal, 2018; Wagh et al., 2020; Kumar et al., 2019; Patra &amp; Suresh, 2020; Tan et al., 2021; Dong et al., 2023) 또는 4자 설정(Byali et al., 2020; Dalskov et al., 2021)을 포함한 다양한 모델과 아키텍처에서 작동합니다. 그러나 이러한 접근 방식의 대부분은 합성곱/심층 신경망의 안전한 추론만 고려하며 Transformer 모델을 지원하도록 직접 확장할 수 없습니다. 최근 여러 연구 작업(Hao 등, 2022; Li 등, 2023; Akimoto 등, 2023; Liang 등, 2023; Liu &amp; Liu, 2023)에서 MPC 기반 보안 추론 솔루션인 PUMA: Secure Inference of LLaMA-7B in Five Minutes A PREPRINT for Transformer 모델을 제안했지만 이러한 접근 방식은 여전히 모델 성능, 효율성 및 배포 측면에서 한계가 있습니다. 이러한 작업 중 MPCFORMER(Li 등, 2023)는 오픈 소스로 공개된 유일한 작업으로, 클라이언트와 서버에 대해 상관 관계가 있는 무작위성을 생성하기 위해 비공모 제3자를 사용하는 3자 프레임워크인 CrypTen(Knott 등, 2021)을 기반으로 합니다. 또한 비공모 가정을 갖춘 3자 모델은 다양한 MPC 설정 중에서 가장 구체적인 효율성을 가지고 있습니다. 따라서 우리는 주로 동일한 3자 설정에서 제안된 프레임워크 PUMA를 MPCFORMER와 비교합니다.3 배경 3.1 표기법 주로 사용되는 표기법은 다음과 같습니다.På는 i번째 컴퓨팅 당사자를 나타냅니다.i = {0,1,2}.대문자 굵은 글자 X는 행렬에 사용되고 소문자 굵은 글자 x는 벡터를 나타냅니다.✗[i]는 벡터 x의 i번째 요소를 나타내고 소문자 x는 스칼라 값에 사용됩니다.Ze는 모듈로 2º에 대한 이산 링을 나타내고 R은 실수를 나타냅니다.[]는 3개 중 2개 복제된 비밀 공유에 사용됩니다(Araki et al., 2016; Mohassel &amp; Rindal, 2018). 3.변압기 모델 변압기 모델은 언어 이해(Radford &amp; Narasimhan, 2018; Devlin et al., 2019; Yang et al., 2019; Touvron et al., 2023), 시각 이해(Zhuge et al., 2021; Dong et al., 2022; Chen et al., 2021) 등에서 놀라운 성공을 거두었습니다. 두 가지 인기 있는 변형은 Bert(변압기의 양방향 인코더 표현)(Devlin et al., 2019)와 GPT(생성적 사전 학습 모델)(Radford &amp; Narasimhan, 2018)입니다. 변압기 모델(Vaswani et al., 2017)은 주로 Embedding, Attention, Feed-Forward Network 및 LayerNorm 하위 계층으로 구성됩니다. 입력 (Q, K, V)이 주어지면, 어텐션 함수는 어텐션 (Q, K, V) = softmax(Q·KT+ M) V로 계산됩니다. 여기서 M은 바이어스 행렬로 볼 수 있습니다. 게다가, (Vaswani et al., 2017)은 다른 위치에서 다른 표현 부분 공간의 정보에 공동으로 주의를 기울이기 위해 Multi-Head Attention을 제안했습니다. . 피드포워드 네트워크(FFN). FFN은 각 위치에 별도로 동일하게 적용됩니다. 이것은 중간에 활성화가 있는 두 개의 선형 변환으로 구성되며, 가장 일반적으로 사용되는 활성화 함수는 GeLU입니다. 입력 x와 매개변수 {W1, b1, W2, b2}가 주어지면, FFN은 FFN(x) = W2GELU (W1x + b₁) + b2로 공식화할 수 있습니다. 선형 변환의 매개변수는 레이어마다 다르다는 점에 유의하세요. = LayerNorm. 벡터 x Є R^이 주어지면, LayerNorm은 다음과 같이 정의됩니다. LayerNorm(x)[i] = y. x + B, 여기서 (Y, B)는 훈련된 매개변수, μ = Σ=[], 및 σ = Σ²²±1(x[i] — µ)²입니다. n 3.3 2/3 복제 비밀 공유 = 비밀 값 x Є Ze는 x = x0+x1+x2(mod 2º)인 세 개의 랜덤 값 x0,x1, x2 € Z2에 의해 공유됩니다. 2/3 복제 비밀 공유([-]-공유로 표시)에서 당사자 P¿는 [x] i (xi, xi+1)을 얻습니다. 특별한 선언 없이 Ze에서 계산하고 간결성을 위해 (mod 2º)를 생략합니다. 산술 연산(예: +, − 및)을 지원하는 l &gt; 1(예: l 64)의 경우 이 유형을 산술 공유라고 하며 표기법 [·]을 사용합니다. 부울 공유([.]³)는 l = 1을 참조하는데, 여기서 (+, -)와 ·는 각각 비트별 및 A. = 추가로 대체됩니다. (C1, C2, C3)을 공개 상수로 하고 ([x], [y])를 두 개의 비밀 공유 값으로 합니다. 그러면 [c1x + C2Y + C3]은 (C1x0 + €20 + C3, C1x1 + C2Y1, C1x2 + C2y2)로 계산할 수 있습니다. 여기서 P¿는 로컬에서 자신의 공유를 계산할 수 있습니다. (c₁ = 1, c2 = 1, C3 = 0)이면 [x + y]를 얻습니다. 곱셈. 보안 곱셈 프로토콜 ПMul에서 두 공유 값 [x]와 [y]가 주어지면 당사자는 다음 단계를 따릅니다. i) 먼저 P¿가 zi = XiYi + Xi+1Yi + XiYi+1을 로컬하게 계산합니다. ii) 그런 다음 당사자는 P₂가 z&#39;i = αi + zi를 Pi−1로 보내도록 하여 재공유를 수행합니다. 여기서 a0 + α1 + a2 = 0입니다(P¿는 Mohassel &amp; Rindal(2018)과 같이 설정 단계에서 a¿를 생성할 수 있음). iii) 마지막으로 {(zo, z1), (z1, z2), (22, zó)}가 [x · y]를 형성합니다. 기본 프로토콜. PUMA는 덧셈과 곱셈 외에도 최신 기술에서 나온 부울 산술 곱셈(ПMula), 제곱 ПISquare, 등식 검정(ПEq), 미만(ПIT), 역수(II Recip), 최대값(ПMax) 및 제곱근의 역수(ПrSqrt) 등 여러 기본 프로토콜에 의존합니다.우리는 이를 블랙박스 방식으로 사용하고 다음과 같이 이러한 프로토콜의 입력 및 출력만 열거합니다.PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 • [2] = 제곱 ([x]), st z = = x² . [z] = [Mulba ([b]³, [x]), st z = b · x • [2] = ПRecip([x]), st z = = 1/x • • [z] = ПrSqrt([x]), st z = 1/√x • [z] = ПMax([x]), st z = maximum(x) [2] B = IIЕq ([x], [y]), st z = 1{x = y} [2]B=LT([x], [y]), st z = = 1{x &lt; y} 1{e}는 조건 e가 참일 때 1을 반환하고 그렇지 않으면 0을 반환합니다. 자세한 프로토콜 구성은 (Mohassel &amp; Rindal, 2018; Lu et al., 2020; Keller, 2020)을 참조하십시오. 고정 소수점 표현 및 잘림. 실수는 유한 링/체로 표현되기 전에 고정 소수점 숫자로 인코딩되어야 합니다. 오버플로를 방지하기 위해 각 고정 소수점 곱셈 후에 II rune을 사용하여 최소 f 비트를 안전하게 잘라내야 합니다.더 간단한 설명을 위해 기본적으로 Пul 및 Пsquare에 II runc를 포함하고 프로토콜 설계에서 명시적으로 언급하지 않습니다.위의 연산은 벡터와 행렬로 쉽게 확장할 수 있으며 단순성을 위해 벡터 및 행렬 연산에 동일한 표기법을 사용합니다.자세한 내용은 (Mohassel &amp; Rindal, 2018; Wagh et al., 2020)을 참조하세요.위협 모델.이전 연구(Mohassel &amp; Rindal, 2018; Li et al., 2023)에 따라 PUMA는 세 컴퓨팅 당사자 중 하나만 손상시키는 반쯤 정직한 적대자로부터 안전합니다.반쯤 정직하다는 것은 이러한 적대자가 프로토콜 사양을 따르지만 프로토콜 중에 다른 사람의 개인 정보를 알아내려고 시도할 수 있음을 의미합니다. PUMA는 추론 결과에 따른 공격으로부터 방어할 수 없으며, 이러한 공격(예: 차등 개인 정보 보호(Abadi et al., 2016))을 완화하는 것은 이 연구의 범위를 벗어난다는 점에 유의하십시오.4 PUMA의 안전한 설계 이 섹션에서는 먼저 PUMA에 대한 개요를 제시하고 PUMA에서 사용하는 안전한 GeLU, softmax, 임베딩 및 LayerNorm에 대한 프로토콜을 제시합니다.행렬 곱셈과 같은 선형 계층은 복제된 비밀 공유에서 간단하므로 이 원고에서는 주로 비선형 계층에 대한 프로토콜을 설명합니다.4.1 PUMA 개요 Transformer 모델의 안전한 추론을 달성하기 위해 PUMA는 세 가지 역할을 정의합니다.한 명의 모델 소유자, 한 명의 클라이언트, 세 명의 컴퓨팅 당사자입니다.모델 소유자와 클라이언트는 비밀 공유 형태로 컴퓨팅 당사자(예: Po, P1 및 P2)에게 모델 또는 입력을 제공한 다음 컴퓨팅 당사자는 MPC 프로토콜을 실행하고 결과를 클라이언트로 다시 보냅니다. 모델 소유자와 클라이언트도 컴퓨팅 당사자 중 한 명으로 작동할 수 있습니다. 일반성을 위해 별도로 설명합니다. 예를 들어 모델 소유자가 Po로 작동하고 클라이언트가 P₁로 작동하고 제3자 딜러가 P2로 작동하는 경우 시스템 모델은 MPCFORMER와 동일해집니다(Li et al., 2023). 안전한 추론 프로세스 동안 키 불변성이 유지됩니다. 모든 계층에서 컴퓨팅 당사자는 항상 이전 계층의 출력과 모델 가중치의 3개 중 2개의 복제된 비밀 공유로 시작하여 이 계층의 출력의 2개의 복제된 비밀 공유로 끝납니다. 공유는 각 당사자에게 정보를 누출하지 않으므로 이를 통해 임의의 깊이에 대해 계층을 순차적으로 결합하여 모든 Transformer 기반 모델에 대한 안전한 계산 체계를 얻을 수 있습니다. 4. 안전한 GeLU를 위한 프로토콜 대부분의 현재 접근 방식은 GeLU 함수를 더 작은 함수의 구성으로 보고 각 부분을 최적화하려고 시도하여 개인 GeLU 전체를 최적화할 기회를 놓치게 합니다. GeLU 함수가 주어졌을 때: GeLU(x) x (1+tanh П (x+0.044715 x³) (1) ≈x 시그모이드 (0.071355 · x³ + 1.595769. x) 이러한 접근 방식(Hao et al., 2022; Wang et al., 2022)은 함수 tanh에 대한 근사 프로토콜을 설계하거나 시그모이드에 대한 지수화 및 역수의 기존 일반 MPC 프로토콜을 사용하는 데 중점을 둡니다. 그러나 현재 접근 방식 중 어느 것도 GeLU 함수가 양쪽에서 거의 선형이라는 사실(즉, x &lt; −4의 경우 GeLU(x) ≈ 0, x &gt; 3의 경우 GeLU(x) ≈ x)을 활용하지 않았습니다. GeLU의 짧은 구간 [−4, 3] 내에서 저차 다항식의 조각별 근사가 securePUMA에 대해 더 효율적이고 구현하기 쉬운 선택이라고 제안합니다. 5분 안에 LLaMA-7B 만들기 사전 인쇄 알고리즘 1 보안 GeLU 프로토콜 II GeLU 입력: Pi는 3개 중 2개의 복제 비밀 공유 [x]]를 보유합니다.i = {0,1,2} 출력: På는 3개 중 2개의 복제 비밀 공유 [y]를 가져옵니다.¿ i = {0, 1, 2}의 경우, y 1: Po, P1 및 P2는 공동으로 다음을 계산합니다.= [bo] = IILT ([x], −4), [b1]³ = HLT([x], −1.95), [b2]B=LT (3, [x]), ▷ bo = 1{x &lt; −4} ▷ b₁ = 1{x &lt; −1.95} ▷ b₂ = 1{3 &lt; x} GeLU(x). BB 및 [20] B = [bo]³ &amp; [b1]³, [21]³ = [b1]³ + [b2]³ + 1, [22]³ = [b2] B를 계산합니다. zo = 1{−4 ≤ x &lt; −1.95}, %1 1{-1.95≤ x ≤ 3}, z2 1{x &gt; 3}에 유의하세요. = 2: [x²] = [Square ([x]), [x³] = [[mu!([x], [x²]), [x4] = ПSquare ([x²]), [x6] = ПSquare ([x³])를 공동으로 계산합니다. 3: {[x], [x²], [x³], [x4], [x6]}를 기반으로 다항식 [Fo(x)] 및 [F1(x)]]를 방정식 (2)로 안전하게 계산합니다. 4: [y] = [[Mulga ([20]³, [Fo(x)]) + IIMulBA ([21]³, [F1(x)]) + IIMulBA ([22]³, [x]). 프로토콜을 반환합니다. 구체적으로, 저희의 조각별 저차수 다항식은 방정식 (2)로 표시됩니다. x &lt; −Fo(x), -4&lt; x &lt; −1.F₁(x), GeLU(x) x, −1.95≤ x ≤x &gt;(2) 여기서 다항식 Fo() 및 F₁()은 라이브러리 numpy.ployfit²에 의해 방정식 (3)으로 계산됩니다. 놀랍게도 위의 간단한 폴리 피팅이 매우 잘 작동하고 최대 오차 &lt; 0.01403, 중간 오차 &lt; 4.41e – 05, 평균 오차 &lt; 0.00168입니다. Fo(x) = -0.011034134030615728x³- 0.11807612951181953x² F1(x) -0.42226581151983866x 0.= 0.0018067462606141187x6 - 0.037688200365904236x+0.3603292692789629x² + 0.5x + 0. 형식적으로 비밀 입력([x])이 주어지면 보안 GeLU 프로토콜 II GeLU는 알고리즘 1로 구성됩니다. 4.3 보안 Softmax 프로토콜 = . (3) 함수 Attention(Q, K, V) softmax(QK + M). V에서 핵심 과제는 함수 softmax를 계산하는 것입니다. 수치적 안정성을 위해 softmax 함수 는 softmax(x)[i] exp(x[i] — — €) Σ exp(x[i] — x - E (4)로 계산됩니다. 여기서 ã는 입력 벡터 x의 최대 요소입니다. 일반 평문 softmax의 경우 € = 0입니다. 2차원 행렬의 경우 각 행 벡터에 방정식(4)를 적용합니다. 형식적으로, 자세한 보안 프로토콜 II softmax는 알고리즘 2에 설명되어 있으며, 여기서 두 가지 최적화를 제안합니다. • 첫 번째 최적화의 경우 방정식 4의 e를 아주 작고 양의 값, 예: € = 10-6으로 설정하여 방정식 4의 지수 연산에 대한 입력이 모두 음수가 되도록 합니다. 가속을 위해 음수 피연산자를 활용합니다. 특히, 간단한 클리핑 negExp(x) = (1+ 2)²², x &lt; Texp x = [Texp, 0]을 사용하여 테일러 급수(Tan et al., 2021)를 사용하여 지수 연산을 계산합니다. (5) 실제로 x &lt; Texp 분기에 대해 보다 작은 것을 적용합니다. 2t로 나누는 것은 II Trunc를 사용하여 수행할 수 있습니다. 입력은 이미 음수입니다. 또한 제곱 함수 Пsquare와 IIrune의 t-단계 시퀀스를 사용하여 2t의 거듭제곱을 계산할 수 있습니다. MPC 프로그램이 18비트 고정 소수점 정밀도를 사용한다고 가정합니다. 그런 다음 Texp = −given exp(-14) &lt; 2−18로 설정하고 경험적으로 t = 5로 설정합니다. 2https://numpy.org/doc/stable/reference/generated/numpy.polyfit.htmlPUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 알고리즘 2 안전한 softmax 프로토콜 II softmax 입력: På는 i = {0, 1, 2}에 대해 3개 중 2개의 복제 비밀 공유 [×]¿를 보유하고 x는 크기가 n인 벡터입니다. 출력: Pi는 3개 중 2개의 복제 비밀 공유 [y]를 얻습니다. i = {0,1,2}에 대해 y = softmax(x)입니다. 1: Po, P₁ 및 P2는 [b]³ = IILT(Texp, [×]) 및 최대값 [x] = ПMax([x])를 공동으로 계산합니다. 2: 당사자는 [x] = [x] – [ã] – €를 로컬하게 계산하고 [z0] = 1 + II Trunc ([^])를 공동으로 계산합니다. 3: j = 1, 2, ..., t에 대해 4를 수행합니다. [zj] = IISquare ([Zj−1]). 5: 끝 6에 대해: 당사자는 [z] = Σ1 [z[i]]를 로컬하게 계산하고 [1/2]] = ПRecip([~])를 공동으로 계산합니다. 7: 당사자는 [z/z] = IIMul([z], [1/2])를 공동으로 계산합니다. 8: [y] = ПMulga ([b]³, [z/z])를 반환합니다. = • 두 번째 최적화는 나누기 횟수를 줄이는 것으로, 궁극적으로 계산 및 통신 비용을 절감합니다.이를 달성하기 위해 크기가 n인 벡터 x에 대해 Div(x, Broadcast(y)) 연산을 x . Broadcast()로 대체했습니다.여기서 y =Σx[i]입니다.이렇게 대체함으로써 n개의 나누기를 단 하나의 역수 연산과 n개의 곱셈으로 효과적으로 줄일 수 있습니다.이 최적화는 소프트맥스 연산의 경우에 특히 유용합니다.소프트맥스 연산의 1/4은 고정 소수점 값에서 충분한 정확도를 유지하기에 충분히 큽니다.결과적으로 이 최적화는 정확한 결과를 제공하면서 계산 및 통신 비용을 크게 줄일 수 있습니다.4.4 보안 임베딩 프로토콜 (Li et al., 2023)에 설명된 현재의 보안 임베딩 절차에서는 클라이언트가 로컬에서 토큰 ID를 사용하여 원핫 벡터를 생성해야 합니다.이는 원핫 벡터가 모델 내부에서 생성되는 평문 Transformer 워크플로와 다릅니다. 결과적으로, 그들은 사전 훈련된 모델에서 원핫 스텝을 조심스럽게 제거하고 클라이언트 측에 스텝을 추가해야 하는데, 이는 배포에 장애물이 될 수 있다.이 문제를 해결하기 위해 다음과 같이 안전한 임베딩 설계를 제안한다.토큰 id = [n]이고 모든 임베딩 벡터가 E(e, e,..., e)로 표시된다고 가정하면, 임베딩은 eid = E[id]로 공식화할 수 있다.(id, E)가 비밀 공유 방식인 경우, 안전한 임베딩 프로토콜 П Embed는 다음과 같이 작동한다.= • 컴퓨팅 당사자는 클라이언트로부터 [id]]를 받은 후 원핫 벡터 [o] B를 안전하게 계산한다.특히, i = [n]에 대해 [o[i]]B = Iɛq(i, [id])이다.• 당사자는 [eid] = [[Mulga ([E], [0] ³)를 통해 임베딩 벡터를 계산할 수 있으며, 여기서는 안전한 잘림이 필요하지 않다. 이런 방식으로, 우리의 ПEmbed는 더 많은 ПEq 및 ПMulBA 연산을 희생하고 평문 Transformer 모델의 워크플로를 명시적으로 수정할 필요가 없습니다.4.5 보안 LayerNorm 프로토콜 크기가 n인 벡터 x가 주어졌을 때, Layer Norm(x)[i] μ Σ=1×³]이고 σ = n νο i== γ .x[i] + B인 것을 기억하세요.여기서 (Y, B)는 훈련된 매개변수(x[i])입니다.MPC에서 핵심 과제는 나누기-제곱근 *(i) = 공식을 평가하는 것입니다.이 공식을 안전하게 평가하기 위해 CrypTen은 제곱근, 역수 및 곱셈의 MPC 프로토콜을 순차적으로 실행합니다. 그러나 *()_&quot;가 (x[i] — µ) · σ¯¹/²와 동일함을 관찰합니다. 그리고 MPC 측에서 역제곱근 σ-1/2를 계산하는 비용은 제곱근 연산과 비슷합니다(Lu et al., 2020). 게다가 § 4.3의 두 번째 최적화에서 영감을 받아 먼저 σ-1/2를 계산한 다음 빠르고 안전한 LayerNorm(x)을 지원하기 위해 (σ-1/2)를 브로드캐스트할 수 있습니다. 그리고 우리의 공식 프로토콜 II LayerNorm은 알고리즘 3에 나와 있습니다. 5 실험 평가 구현. 우리는 C++와 Python에서 SecretFlow-SPU(Ma et al., 2023) 위에 PUMA를 구현합니다. 우리는 18비트 소수 부분이 있는 링 Z264에서 고정 소수점 형태로 데이터를 인코딩합니다. 우리의 실험은 각각 32개 vCPU와 128GB RAM이 있는 3개의 Alibaba Cloud ecs.g7.8xlarge 서버에서 실행됩니다. CPU 모델은 Intel Xeon(Ice Lake) Platinum 8369BPUMA입니다: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 알고리즘 3 보안 LayerNorm 프로토콜 II LayerNorm 입력: Pi는 3개 중 2개의 복제 비밀 공유 [×] ¿를 보유합니다. i = {0, 1, 2}이고 x는 크기가 n인 벡터입니다. 출력: Pi는 3개 중 2개의 복제 비밀 공유 [y]를 얻습니다. i = {0, 1, 2}이고 yn LayerNorm(x)입니다. 1: Po, P1 및 P2는 [µ] = ±²²±1[×[i]] 및 [0] = Σ²±1 Square ([x] - [µ])[i]를 계산합니다. 2: 당사자는 공동으로 [σ¯¹/2] = ПrSqrt([0])를 계산합니다. 3: 당사자는 [c] = ПMul (([×] − [µ]), [σ¯¹/²])를 공동으로 계산합니다. 4: [y] = [Mul([y], [c]) + [ß]를 반환합니다. 표 1: CoLA, RTE 및 QNLI에서 Bert-Base, Roberta-Base 및 Bert-Large의 GLUE 벤치마크 성능, CoLA에 대한 Matthews 상관관계가 보고됩니다. 다른 데이터 세트에 대한 정확도가 보고됩니다. 모델 Bert-Base Roberta-Base Bert-Large TASK COLA RTE QNLI COLA RTE QNLI COLA RTE QNLI CPU 0.616 0.700 0.916 0.629 0.805 0.920 0.686 0.755 0.PUMA 0.613 0.700 0.916 0.618 0.805 0.918 0.690 0.747 0.CPU @ 2.70GHz. Linux 커널 5.4.0-144-generic이 있는 Ubuntu 20.04.6 LTS에서 PUMA를 평가합니다. 대역폭은 약 5Gbps이고 왕복 시간은 약 1ms입니다. 모델 및 데이터 세트. 우리는 7개의 NLP 모델에서 PUMA를 평가합니다: Bert-Base, Roberta-Base, BertLarge(Devlin et al., 2019); GPT2-Base, GPT2-Medium, GPT2-Large(Radford &amp; Narasimhan, 2018); LLAMA-7B(Touvron et al., 2023). 우리는 Corpus of Linguistic Acceptability(COLA), Recognizing Textual Entailment(RTE), GLUE 벤치마크의 Stanford Question Answering Dataset(QNLI)(Wang et al., 2019) 및 Wikitext-103 V1(Merity et al., 2016)에 대한 3개의 NLP 작업에 대한 Bert 성능을 측정합니다. 기준선. 우리는 PUMA를 가장 유사한 이전 작업인 MPCFORMER(Li et al., 2023)와 비교합니다. 하지만 공정한 비교를 위해 다음과 같은 고려 사항이 있습니다.i) MPCFORMER는 사전 학습된 변압기 모델 로딩을 지원하지 않고 LayerNorm을 충실하게 구현하지 않으므로³ 해당 프레임워크를 사용하여 의미 있는 보안 추론 결과를 얻을 수 없습니다.따라서 정밀도 보장을 보여주기 위해 일반 텍스트(부동 소수점)의 성능과 비교합니다.ii) Quad 근사를 사용한 MPCFORMER는 수정된 모델을 재학습해야 합니다.PUMA는 재학습이 필요하지 않으므로 Quad 근사가 없는 MPCFORMER의 비용과 비용을 비교합니다.또한 환경에서 MPCFORMER를 다시 실행합니다.5.1 정밀도 표 1과 2에서 보안 모델 추론 성능을 일반 텍스트(부동 소수점)의 성능과 비교하여 정밀도 보장을 보여줍니다.표 1에서 Bert-Base, Roberta-base 및 Bert-Large에서 일반 텍스트와 PUMA의 Matthews 상관 관계/정확도를 보여줍니다.PUMA가 달성한 정확도는 일반 텍스트 Flax 코드의 정확도와 일치합니다. 구체적으로, 정확도 차이는 모든 데이터 세트에서 0.011을 초과하지 않습니다. 게다가 표 2에서 Wikitext-103 V1 데이터 세트에 대한 우리의 복잡도를 GPT2 모델의 일반 텍스트 기준선과 비교합니다. 결과는 비슷하며, 모든 모델에 대한 복잡도 차이는 0.02를 초과하지 않습니다. 위의 정확도와 복잡도 이점은 우리 프로토콜이 수치적으로 정확하다는 것을 실험적으로 검증합니다. 3 MPCFORMER는 사전 학습된 Transformer 모델 로딩을 지원하지 않으므로, MPCFORMER처럼 LayerNorm을 BatchNorm으로 대체하는 일반 텍스트 Bert-Base에서 실험을 수행했습니다. 그 결과 CoLA 작업의 MCC 점수가 0.616에서 -0.020으로 크게 떨어졌습니다. 반대로 PUMA는 0.613의 MCC 점수를 달성합니다. 표 2: Wikitext-103 V1에서 GPT2-Base, GPT2-Medium 및 GPT2-Large의 복잡도. 모델 CPU PUMA GPT2-Base 16.16.GPT2-Medium 12.12.GPT2-Large 10.10.PUMA: 5분 안에 LLaMA-7B의 안전한 추론 사전 인쇄 표 3: 길이가 128인 한 문장에 대한 Bert-Base, Roberta-Base 및 Bert-Large의 비용. 시간은 초 단위이고 통신(약칭 Comm.)은 GB 단위이며, 이는 다음 표의 경우에도 동일합니다. Roberta-Base 모델 비용 MPCFORMER PUMA 개선 Bert-Base Bert-Large 시간 통신 시간 통신. 시간 55.320 12.089 57.256 12.373 141.33.913 10.773 41.641 11.463 73.720 27.1.631 × 1.122× 1.375× 1.079× 1.916× 1.195x 통신 32.표 4: GPT2-Base, GPT2-Medium 및 GPT2-Large의 비용. 입력 문장의 길이는 32이고, 모든 비용은 1개 토큰을 생성하는 데 드는 비용입니다. 모델 비용 MPCFORMER GPT2-Base GPT2-Medium GPT2-Large 시간 통신 34.4.PUMA 15.3.Improv. 2.250x 1.325x 2.414x 시간 통신 73.078 11.30.272 7.1.667x 통신 시간 129.095 22.54.154 11.2.383× 1.884× 5.2 추론 비용 PUMA의 추론 비용을 MPCFORMER의 추론 비용과 비교합니다. 비용은 하나의 입력 문장을 처리하는 데 드는 비용입니다. i) Bert 모델의 경우 입력 문장의 길이는 128입니다. ii) GPT2 모델의 경우 입력 길이가 32이고 새로운 단어 1개를 생성합니다. 표 3의 3가지 Bert 모델에서 PUMA는 MPCFORMER보다 1.375 ~ 1.916배 빠르고 통신 효율성은 1.079 ~ 1.195배 더 높습니다. 표 4의 GPT2 모델의 경우 PUMA는 MPCFORMER보다 2.250 ~ 2.414배 빠르고 통신 효율성은 1.325 ~ 1.884배 더 높습니다. PUMA의 개선은 모델 크기가 커질수록 증가하는 것을 관찰할 수 있으며, 특히 GPT2 모델의 경우 그렇습니다. 이러한 추세는 저희의 전문화된 최적화가 대규모 평가를 처리할 때 더 효과적이기 때문입니다. 5.3 확장성 이 하위 섹션에서는 Bert-Base 및 GPT2-Base 모델에서 일괄 입력, 가변 길이 입력 및 가변 길이 출력(GPT2-Base에만 해당)에 대한 PUMA를 평가하는 비용을 측정합니다. 또한 개선 사항을 보여주기 위해 비용을 MPCFORMER의 비용과 비교합니다. 입력 길이 평가. 표 5는 가변 길이 입력에 대한 비용을 보여줍니다.길이 {64, 128, 256}의 입력에서 Bert-Base를 평가하고 길이 {16, 32, 64}의 입력에서 GPT2-Base를 평가합니다.Bert-Base의 경우 PUMA가 1.631 ~ 1.837배 빠르고 GPT2-Base의 경우 PUMA가 1.744 ~ 2.686배 빠릅니다.출력 길이 평가.그림 1은 GPT2-Base에 대한 가변 길이 출력에 대한 비용을 나타냅니다.MPCFORMER에 대한 개선 범위는 1.279 ~ 2.700배입니다.~표 5와 그림 1에서 GPT-2의 경우 입력/출력 토큰이 많을수록 효율성 향상이 감소하는 것을 알 수 있습니다.이는 PUMA가 추가적인 원핫 임베딩 비용을 도입하기 때문입니다(4.4에서 설명). PUMA는 평문 모델과 호환되며 MPCFORMER가 달성하지 못하는 평문 모델과 유사한 정확도를 달성할 수 있다는 점을 다시 한번 강조해야 합니다.표 5: 다양한 입력 길이에 대한 Bert-Base 및 GPT2-Base의 비용(#Input으로 표시).Bert-Base 및 GPT2-Base의 입력 길이는 각각 {64, 128, 256} 및 {16, 32, 64}입니다.GPT2-Base는 토큰 1개를 생성합니다.#Input 비용 64/128/256/시간 Bert GPTComm.시간 통신 MPCFORMER 36.354 5.707 55.320 12.PUMA 21.141 4.881 33.913 10.Improv. 1.720× 1.169× 1.631× 1.122x MPCFORMER 29.695 4.011 34.889 4.PUMA 11.056 1.875 15.506 3.임프로브 2.686× 2.139x 2.250x 1.324× 시간 112.통신 29.61.26.1.837x 1.151× 43.7.24.7.1.744× 0.936×시간(초) PUMA: 5분 안에 LLaMA-7B의 안전한 추론 사전 인쇄 MPC 이전 시간 Puma 시간 그림 1: 다양한 출력 토큰을 생성하기 위한 GPT2-Base의 런타임, 입력 길이는 32입니다. 표 6: LLaMA-7B의 안전한 추론 비용, #입력은 입력 문장의 길이를 나타내고 #출력은 생성된 토큰의 수를 나타냅니다. (#입력, #출력) (4, 1) (8,1) (8,2) 비용 PUMA 시간 122.통신 시간 통신 시간 통신. 0.907 200.473 1.794 364.527 3.5.4 5분 안에 LLaMA-7B 평가하기. 저희 프로토콜은 LLaMA-7B를 포함한 모든 Transformer 기반 모델을 평가하기에 이미 완성되었습니다. 안타깝게도 Protobuf(Varda, 2008) 및 FlatBuffers(van Oortmerssen, 2014)와 같은 기존 직렬화 라이브러리는 최대 2GB 크기의 데이터 트렁크만 지원하는데, 이는 대규모 MPC 작업에 충분하지 않습니다. 이 문제를 해결하기 위해 SecretFlow-SPU에 대한 최적화를 제안합니다. 구체적으로, 시스템은 통신하거나 I/O 작업을 수행할 때 지나치게 큰 비밀 공유 구조를 자동으로 더 작은 청크로 분할하고 직렬화할 수 있습니다. 우리는 3개의 Alibaba Cloud ecs.r7.32xlarge 서버에서 PUMA를 사용하여 대규모 언어 모델 LLaMA-7B를 평가했습니다. 각각은 128개의 스레드와 1TB RAM, 20GB 대역폭, 0.1ms 왕복 시간을 가지고 있습니다. 표 6에서 볼 수 있듯이 PUMA는 합리적인 비용으로 LLaMA-7B의 안전한 추론을 지원할 수 있습니다. 예를 들어, 8개의 토큰으로 구성된 입력 문장이 주어지면 PUMA는 1.794GB의 통신 비용으로 약 200초 안에 토큰 하나를 출력할 수 있습니다. 우리가 아는 한, 이것은 MPC를 사용하여 LLaMA-7B를 평가한 첫 번째 사례입니다. 게다가 PUMA는 평문 LLaMA-7B와 정확히 동일한 토큰을 생성할 수 있습니다. 예는 부록을 참조하세요. 6
--- CONCLUSION ---
우리는 복제된 비밀 공유에 기반한 Transformer 모델에 대한 안전한 추론을 위한 효율적인 MPC 프레임워크 PUMA를 제안합니다. 안전한 추론의 비용을 줄이기 위해 정확한 다항식으로 값비싼 함수를 근사하고 종단 간 안전한 추론을 지원하기 위해 안전한 Embedding 및 LayerNorm 프로토콜을 제안합니다. 추론 비용은 여전히 상당히 높지만 Transformer 기반 DLaas에서 사용자의 개인 정보 보호 문제를 해결하는 데 한 걸음 더 가까워졌습니다. 우리는 앞으로 PUMA를 양자화 방법 및 하드웨어 가속과 결합함으로써 몇 초 만에 대규모 Transformer 모델을 안전하게 추론하는 것이 더 이상 불가능하지 않다고 믿습니다. 참고 문헌 Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar 및 Li Zhang. 차등 개인 정보 보호를 통한 딥 러닝. 2016 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록, 308-318쪽, 2016. Y. Akimoto, K. Fukuchi, Y. Akimoto, J. Sakuma. Privformer: mpc를 사용한 개인정보 보호 변압기. 2023년 IEEE 8th European Symposium on Security and Privacy(EuroSP), 392-410쪽, 미국 캘리포니아주 로스알라미토스, 2023. IEEE Computer Society. doi:10.1109/EuroSP57164.2023.00031. URL https://doi.ieeecomputersociety.org/10.1109/EuroSP57164.2023.00031.PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 Toshinori Araki, Jun Furukawa, Yehuda Lindell, Ariel Nof, Kazuma Ohara. 정직한 다수결을 통한 고처리량 준정직 안전한 3자 계산. 2016 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록, 805-817쪽, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습기입니다, 2020. Megha Byali, Harsh Chaudhari, Arpita Patra, Ajith Suresh. Flash: 개인 정보 보호 머신 러닝을 위한 빠르고 강력한 프레임워크. Proc. Priv. Enhancing Technol., 2020(2):459-480, 2020. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao. 사전 학습된 이미지 처리 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 12299–12310, 2021. Anders Dalskov, Daniel Escudero, Marcel Keller. Fantastic four: 악의적 보안을 갖춘 정직한 4자 보안 계산. 30th {USENIX} Security Symposium({USENIX} Security 21), 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. ArXiv, abs/1810.04805, 2019. Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. 비전 bert 사전 학습을 위한 부트스트랩 마스크 자동 인코더. European Conference on Computer Vision, pp. 247–264. Springer, 2022. Ye Dong, Chen Xiaojun, Weizhan Jing, Li Kaiyun, and Weiping Wang. Meteor: 온라인 통신 비용을 줄이면서 보안 3자 신경망 추론 개선. ACM 웹 컨퍼런스 2023 회의록, WWW &#39;23, pp. 2087-2098, 뉴욕, 뉴욕, 미국, 2023. Association for Computing Machinery. ISBN 9781450394161. O. Goldreich, S. Micali, A. Wigderson. 모든 정신 게임을 하는 방법. 제19회 ACM 컴퓨팅 이론 심포지엄 회의록, STOC &#39;87, pp. 218-229, 뉴욕, 뉴욕, 미국, 1987. Association for Computing Machinery. ISBN 0897912217. Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, Tianwei Zhang. Iron: 변압기에 대한 사적 추론. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho(편), 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id=deyqjpcTfsG. Zhicong Huang, Wen jie Lu, Cheng Hong, Jiansheng Ding. Cheetah: Lean and fast secure Two-Party deep neural network inference. 제31회 USENIX 보안 심포지엄(USENIX 보안 22), 809-826쪽, 매사추세츠주 보스턴, 2022년 8월. USENIX 협회. ISBN 978-1-939133-31-1. Marcel Keller. Mp-spdz: 다자간 계산을 위한 다재다능한 프레임워크. 2020 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 논문집, pp. 1575–1590, 2020. B. Knott, S. Venkataraman, AY Hannun, S. Sengupta, M. Ibrahim, LJP van der Maaten. Crypten: 안전한 다자간 연산이 머신 러닝을 만난다. arXiv 2109.00984, 2021. Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. 미세 조정은 사전 학습된 기능을 왜곡하고 분포 외에서 성능이 떨어질 수 있다. arXiv 사전 인쇄본 arXiv:2202.10054, 2022. Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem Rastogi, Rahul Sharma. Cryptflow: 안전한 tensorflow 추론. arXiv 사전 인쇄본 arXiv:1909.07814, 2019. Dacheng Li, Hongyi Wang, Rulin Shao, Han Guo, Eric Xing, Hao Zhang. MPCFORMER: MPC를 통한 빠르고, 성능 좋고, 개인적인 트랜스포머 추론. 제11회 학습 표현 국제 컨퍼런스, 2023. URL https://openreview.net/forum?id=CWmvj0EhgH-. Zi Liang, Pinghui Wang, Ruofei Zhang, Lifeng Xing, Nuo Xu, Shuo Zhang. Merge: 빠른 개인 텍스트 생성, 2023. Jian Liu, Mika Juuti, Yao Lu, Nadarajah Asokan. minionn 변환을 통한 무지 신경망 예측. 2017 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록, 619-631쪽, 2017. Xuanqi Liu와 Zhuotao Liu. Llms는 암호화된 프롬프트를 이해할 수 있습니다: 개인 정보 보호 컴퓨팅 친화적인 변환기를 향해, 2023. PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄본 Wen-jie Lu, Yixuan Fang, Zhicong Huang, Cheng Hong, Chaochao Chen, Hunter Qu, Yajin Zhou, Kui Ren. 적응형 경사 하강의 더 빠르고 안전한 다자간 계산. 2020년 개인 정보 보호 머신 러닝 실무 워크숍 회의록, PPMLP&#39;20, 47-49쪽, 뉴욕, 뉴욕, 미국, 2020. 컴퓨팅 기계 협회. ISBN 9781450380881. Junming Ma, Yancheng Zheng, Jun Feng, Derun Zhao, Haoqi Wu, Wenjing Fang, Jin Tan, Chaofan Yu, Benyu Zhang, Lei Wang. SecretFlow-SPU: 개인 정보 보호 머신 러닝을 위한 성능 좋고 사용자 친화적인 프레임워크. 2023 USENIX 연례 기술 컨퍼런스(USENIX ATC 23), 17-33쪽, 매사추세츠주 보스턴, 2023년 7월. USENIX 협회. ISBN 978-1-939133-35-9. https://www.usenix.org/conference/atc23/presentation/ma. URL Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher. 포인터 센티넬 혼합 모델, 2016. Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, Raluca Ada Popa. Delphi: 신경망을 위한 암호화 추론 서비스. 제29회 {USENIX} 보안 심포지엄({USENIX} 보안 20), 2505-2522쪽, 2020. Payman Mohassel과 Peter Rindal. Aby3: 머신 러닝을 위한 혼합 프로토콜 프레임워크. 2018 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록, 35-52쪽, 미국 뉴욕, 2018. Association for Computing Machinery. ISBN 9781450356930. doi:10.1145/3243734.3243760. URL https://doi.org/10.1145/3243734.3243760. Payman Mohassel 및 Yupeng Zhang. Secureml: 확장 가능한 개인 정보 보호 머신 러닝을 위한 시스템. IEEE 보안 및 개인 정보 보호 심포지엄(SP), pp. 19–38. IEEE, 2017. Arpita Patra 및 Ajith Suresh. Blaze: 놀라울 정도로 빠른 개인 정보 보호 머신 러닝. arXiv:2005.09042, 2020. arXiv 사전 인쇄본 Arpita Patra, Thomas Schneider, Ajith Suresh 및 Hossein Yalame. {ABY2. 0}: 개선된 {혼합 프로토콜} 보안 {2자} 계산. 30th USENIX Security Symposium(USENIX Security 21), pp. 2165–2182, 2021. Alec Radford와 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem Rastogi, Rahul Sharma. Cryptflow2: 실용적인 2자 보안 추론. 뉴욕, 뉴욕, 미국, 2020. Association for Computing Machinery. ISBN 9781450370899. URL https://doi.org/10.1145/3372297.3417274. Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya Gupta, Rahul Sharma, Nishanth Chandran, Aseem Rastogi. Sirnn: 안전한 rnn 추론을 위한 수학 라이브러리. arXiv 사전 인쇄본 arXiv:2105.04236, 2021. Adi Shamir. 비밀을 공유하는 방법. Communications of the ACM, 22(11):612–613, 1979. Jonathan Soifer, Jason Li, Mingqin Li, Jeffrey Zhu, Yingnan Li, Yuxiong He, Elton Zheng, Adi Oltean, Maya Mosyak, Chris Barnes 등. Microsoft의 딥 러닝 추론 서비스. 2019 USENIX Operational Machine Learning 컨퍼런스(OpML 19), pp. 15–17, 2019. Sijun Tan, Brian Knott, Yuan Tian, David J Wu. Cryptgpu: GPU에서 개인 정보를 보호하는 빠른 머신 러닝. arXiv 사전 인쇄본 arXiv:2104.10949, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 개방적이고 효율적인 기반 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. Wouter van Oortmerssen. Flatbuffers: 메모리 효율적인 직렬화 라이브러리. 웹 페이지. androiddevelopers.googleblog.com/2014/06/platbuffers-memory-efficient. HTML, 2014. Kenton Varda. 프로토콜 버퍼: Google의 데이터 교환 형식입니다. Google 오픈 소스 블로그, 빠르면 2008년 7월 72시 23분부터 이용 가능. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan 및 R. Garnett(eds.), 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc., 2017. Sameer Wagh, Divya Gupta 및 Nishanth Chandran. Securenn: 신경망 학습을 위한 3자 보안 계산. 개인정보 보호 강화 기술 회의록, 2019(3):26–49, 2019. Sameer Wagh, Shruti Tople, Fabrice Benhamouda, Eyal Kushilevitz, Prateek Mittal, Tal Rabin. Falcon: 개인 딥 러닝을 위한 Honestmajority 악의적 보안 프레임워크. arXiv 사전 인쇄본 arXiv:2004.02229, 2020. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. GLUE: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 Yongqin Wang, G. Edward Suh, Wenjie Xiong, Benjamin Lefaudeux, Brian Knott, Murali Annavaram, Hsien-Hsin S. Lee. 변압기 기반 모델을 위한 mpc 기반 개인 추론의 특성화. IEEE 시스템 및 소프트웨어 성능 분석 국제 심포지엄(ISPASS), pp. 187–197, 2022. doi:10.1109/ISPASS55109.2022.00025. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush. Transformers: State-of-the-art natural language processing. 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations의 회의록, 38-45쪽, 온라인, 2020년 10월. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020. emnlp-demos. 6. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. Xlnet: 언어 이해를 위한 일반화된 자기 회귀 사전 학습. 미국 뉴욕주 레드훅에서 열린 제33회 신경 정보 처리 시스템 국제 컨퍼런스의 회의록, 2019년. Curran Associates Inc. Andrew Chi-Chih Yao. 비밀을 생성하고 교환하는 방법. 제27회 컴퓨터 과학 기초 심포지엄(sfcs 1986), pp. 162–167. IEEE, 1986. Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao. Kaleido-bert: 패션 도메인에서의 비전 언어 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 12647-12657, 2021. 실험 모델 세부 정보 이 섹션에서는 실험 모델의 아키텍처를 간략하게 설명합니다. 자세한 내용은 HuggingFace Transformers 라이브러리(Wolf et al., 2020)를 참조하세요. • Bert-Base: Bert-Base는 Bert 모델의 기본 버전으로, 12개의 Transformer 인코더 레이어, 768개의 숨겨진 크기, 12개의 헤드로 구성됩니다. 매개변수는 1억 1,000만 개이며 레이블이 지정되지 않은 방대한 텍스트 데이터 코퍼스에서 학습됩니다. • Roberta-Base: Bert-base와 유사한 Roberta-base는 Roberta 모델의 기본 버전입니다. Transformer 레이어, 768개의 숨겨진 크기, 12개의 헤드로 구성됩니다. 매개변수는 약 1억 2,500만 개입니다. • Bert-Large: Bert-Large는 24개의 Transformer 인코더 레이어, 1024개의 숨겨진 크기, 16개의 헤드를 갖춘 Bert-base의 확장 버전입니다. 약 3억 4천만 개의 매개변수가 있어 더욱 강력하고 복잡한 언어 패턴을 포착할 수 있습니다. • GPT2-Base: GPT2-Base는 Gpt2 모델의 기본 버전으로 12개의 Transformer 디코더 레이어, 768개의 숨겨진 크기, 12개의 헤드로 구성되어 있습니다. 1억 1천 7백만 개의 매개변수가 있으며 방대한 텍스트 데이터 코퍼스에서 학습되었습니다. GPT2-Base는 주로 텍스트 생성 및 언어 이해와 관련된 작업에 사용됩니다. • GPT2-Medium: GPT2-Medium은 24개의 Transformer 디코더 레이어, 1024개의 숨겨진 크기, 16개의 헤드로 구성되어 있습니다. 약 3억 4천 5백만 개의 매개변수가 있습니다. • GPT2-Large: GPT2-Large는 GPT2 모델의 가장 큰 변형으로, 36개의 Transformer 디코더 레이어, 1280개의 숨겨진 크기, 16개의 헤드를 갖추고 있습니다. 약 7억 7,400만 개의 매개변수가 있습니다. LLaMA-7B를 위한 B PUMA GPT-2 및 Bert와 달리 LLAMA는 GeLU 대신 SiLU를 사용하므로 다른 계수를 가진 유사한 조각별 저차수 다항식을 사용하여 SiLU를 근사할 수 있습니다. 전체 다항식은 flax_llama7b.py에서 찾을 수 있습니다. 그림 2에서는 프롬프트가 주어진 LLamA-7B(고정된 무작위성 포함)의 출력 토큰을 보여줍니다. Q: 가장 큰 동물은 무엇입니까? PUMA가 20개 이상의 토큰을 생성하는 평문에서 LLaMA-7B와 동일한 토큰을 출력하는 것을 볼 수 있습니다. 평문 PUMA: 5분 만에 LLaMA-7B의 안전한 추론 사전 인쇄 PUMA 프롬프트: Q: 가장 큰 동물은 무엇입니까? 출력: A: 가장 큰 동물은 푸른 고래입니다. Q: 가장 작은 동물은 무엇입니까? A: 가장 작은 동물은 벌입니다. 프롬프트: Q: 가장 큰 동물은 무엇입니까? 출력: A: 가장 큰 동물은 푸른 고래입니다. Q: 가장 작은 동물은 무엇입니까? A: 가장 작은 동물은 벌입니다. 그림 2: 평문과 PUMA의 LLaMA-7B 출력.
