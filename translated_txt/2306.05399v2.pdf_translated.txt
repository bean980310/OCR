--- ABSTRACT ---
이 논문에서 우리는 유연하고 대화형 시각적 또는 언어적 사용자 프롬프트 안내를 통해 이미지의 모든 인스턴스의 알파 매트를 추정하기 위한 효율적이고 다재다능한 프레임워크인 Matting Anything Model(MAM)을 제안합니다.MAM은 이전의 특수 이미지 매팅 네트워크에 비해 여러 가지 중요한 이점을 제공합니다.(i) MAM은 단일 모델만으로 의미적, 인스턴스적, 참조적 이미지 매팅을 포함한 다양한 유형의 이미지 매팅을 처리할 수 있습니다.(ii) MAM은 Segment Anything Model(SAM)[21]의 피처 맵을 활용하고 가벼운 Mask-to-Matte(M2M) 모듈을 채택하여 반복적 정제를 통해 알파 매트를 예측하는데, 이는 270만 개의 학습 가능한 매개변수만 있습니다.(iii) MAM은 SAM을 통합함으로써 트리맵에서 상자, 지점 또는 텍스트 프롬프트까지 이미지 매팅의 대화형 사용에 필요한 사용자 개입을 단순화합니다. 우리는 다양한 이미지 매팅 벤치마크에서 MAM의 성능을 평가하고, 실험 결과는 MAM이 각 벤치마크에서 다른 메트릭에서 최첨단 전문 이미지 매팅 모델과 비슷한 성능을 달성한다는 것을 보여줍니다. 전반적으로 MAM은 우수한 일반화 능력을 보여주고 더 적은 매개변수로 다양한 이미지 매팅 작업을 효과적으로 처리할 수 있어 통합 이미지 매팅을 위한 실용적인 솔루션이 됩니다. 우리의 코드와 모델은 https://github.com/SHI-Labs/Matting-Anything에서 오픈 소스로 제공됩니다. 1.
--- INTRODUCTION ---
오래된 컴퓨터 비전 작업인 이미지 매팅은 입력 이미지 I가 주어졌을 때 알파 매트 a를 추정하는 것을 목표로 합니다[45]. 매팅 대상은 주로 의미 수준에서 사람이나 다른 물체 주변입니다[26,41,49]. 최근 연구에서는 이미지 매팅의 범위를 인스턴스 인식 알파 매트 예측이 필요한 이미지 인스턴스 매팅[42]과 자연어 설명이 주어졌을 때 알파 매트를 추출하는 참조 이미지 매팅[28]과 같은 더 복잡한 시나리오로 확장했습니다. 이전의 딥 러닝 기반 이미지 매팅 방법[28, 29, 36, 37, 42, 47, 51, 54, 59]은 해당 벤치마크에서 특정 이미지 매팅 작업을 처리하기 위해 제안되었습니다. 이러한 방법은 개별 데이터 세트에 맞게 조정되었으며 고정된 모델 설계로 인해 다양한 이미지 매팅 작업을 처리할 수 있는 유연성이 부족합니다. 이러한 제한으로 인해 보다 일반화되고 다재다능한 이미지 매팅 모델의 개발이 방해를 받았습니다. 결과적으로, 단일 모델로 다양한 유형의 이미지 매팅 작업을 처리할 수 있는 보다 적응적이고 효율적인 이미지 매팅 프레임워크를 개발하는 데 대한 관심이 커지고 있습니다. 더욱이, 이전의 이미지 매팅 방법은 정확한 알파 매트 예측을 달성하기 위해 보조 입력으로 사용자 안내 트리맵에 의존했습니다. 대신 마스크 안내 또는 배경 이미지를 사용하는 일부 트리맵프리 방법이 제안되었지만[39,55], 이러한 방법은 대화형 사용을 위한 사용자 요청에 따라 대상 인스턴스의 알파 매트를 추정할 수 없습니다. 따라서 사용자 안내 트리맵에 의존하지 않고도 정확한 알파 매트 추정을 달성할 수 있는 모델을 개발하는 것이 중요하며, 대화형 사용을 위해 유연하고 효율적인 방식으로 간단한 사용자 요청을 처리할 수 있어야 합니다. 이러한 모델은 수동 개입에 대한 추가 필요성을 줄임으로써 사용자 경험을 크게 향상시킬 것입니다. 이미지 매팅의 이러한 한계에 동기를 부여받아, 그림 1에 표시된 것처럼 이미지에서 프롬프트 기반 사용자 안내를 통해 모든 대상 인스턴스의 알파 매트를 추정할 수 있는 다재다능한 네트워크인 Matting Anything Model(MAM)을 제안합니다. MAM은 유연한 프롬프팅을 지원하고 대화형 사용을 위해 모든 대상 인스턴스의 분할 마스크를 출력하는 최신 Segment Anything Model(SAM) 프레임워크[21]를 활용합니다. 구체적으로, MAM은 SAM의 피처 맵과 마스크 출력을 입력으로 사용하고 가벼운 Mask-to-Matte(M2M) 모듈을 추가하여 대상 인스턴스의 알파 매트를 예측합니다. 우리는 다양한 클래스의 인스턴스를 포함하는 5개의 이미지 매팅 데이터 세트를 조합하여 MAM을 훈련하여 M2M 모듈이 이미지 매팅을 위한 일반화 가능한 기능을 학습할 수 있도록 했습니다. 훈련하는 동안, 우리는 무작위로 대상 인스턴스를 배경 이미지에 배치하고 사전 훈련된 SAM을 사용하여 해당 인스턴스의 마스크 예측을 출력합니다. 그런 다음 훈련 가능한 M2M 모듈은 다중 스케일 알파 매트를 예측하여 마스크를 세분화합니다. 마스크 또는 알파 매트를 기반으로 하는 반복적 정제 프로세스를 통해 다중 스케일 예측이 병합되어 최종적으로 세심한 알파 매트를 얻습니다. 우리는 의미적 이미지 매팅 벤치마크 PPM-100[40], AM2K[25] PM-10K[25], 인스턴스 이미지 매팅 벤치마크 RWP636[55], HIM2K[42], 참조 이미지 매팅 벤치마크 RefMatte-RW 100[28]을 포함한 6개의 이미지 매팅 벤치마크에서 MAM에 대한 광범위한 평가를 수행했습니다. 우리의 결과는 MAM이 다양한 평가 지표에서 모든 벤치마크에서 최첨단 이미지 매팅 모델과 비슷한 성능을 달성한다는 것을 보여줍니다. 실험 결과는 다양한 이미지 매팅 작업을 대화형이고 효율적인 방식으로 처리하기 위한 제안된 접근 방식의 다재다능함과 효과를 강조합니다. 2.
--- RELATED WORK ---
s 2.1. 이미지 매팅 전경 이미지 F와 계수 알파 매트 a, IaF + (1 - a)B (1)를 갖는 배경 이미지 B의 조합으로 볼 수 있는 이미지 I가 주어지면 이미지 매팅은 주어진 입력으로만 I를 추정하는 것입니다. 전통적인
--- METHOD ---
s [28, 29, 36, 37, 42, 47, 51, 54, 59]는 해당 벤치마크에서 특정 이미지 매팅 작업을 처리하기 위해 제안되었습니다. 이러한 방법은 개별 데이터 세트에 맞게 조정되었으며 고정된 모델 설계로 인해 다양한 이미지 매팅 작업을 처리할 수 있는 유연성이 부족합니다. 이러한 제한으로 인해 보다 일반화되고 다재다능한 이미지 매팅 모델 개발이 방해를 받았습니다. 결과적으로 단일 모델로 다양한 유형의 이미지 매팅 작업을 처리할 수 있는 보다 적응적이고 효율적인 이미지 매팅 프레임워크를 개발하는 데 대한 관심이 커지고 있습니다. 또한 이전 이미지 매팅 방법은 정확한 알파 매트 예측을 달성하기 위해 보조 입력으로 사용자 안내 트리맵에 의존했습니다. 대신 마스크 안내 또는 배경 이미지를 사용하는 일부 트리맵 없는 방법[39,55]이 제안되었지만 대화형 사용에 대한 사용자 요청에 따라 대상 인스턴스의 알파 매트를 추정할 수 없습니다. 따라서 사용자 가이드 트리맵에 의존하지 않고도 정확한 알파 매트 추정을 달성할 수 있는 모델을 개발하는 것이 중요하며, 동시에 대화형 사용을 위해 유연하고 효율적인 방식으로 간단한 사용자 요청을 처리할 수 있어야 합니다. 이러한 모델은 수동 개입에 대한 추가 필요성을 줄임으로써 사용자 경험을 크게 향상시킬 것입니다. 이미지 매팅의 이러한 한계에 동기를 부여받아 그림 1과 같이 이미지에서 프롬프트 기반 사용자 가이드로 모든 대상 인스턴스의 알파 매트를 추정할 수 있는 다재다능한 네트워크인 Matting Anything Model(MAM)을 제안합니다. MAM은 유연한 프롬프팅을 지원하고 대화형 사용을 위해 모든 대상 인스턴스의 분할 마스크를 출력하는 최신 Segment Anything Model(SAM) 프레임워크[21]를 활용합니다. 구체적으로 MAM은 SAM의 피처 맵과 마스크 출력을 입력으로 사용하고 가벼운 Mask-to-Matte(M2M) 모듈을 추가하여 대상 인스턴스의 알파 매트를 예측합니다. 우리는 다양한 인스턴스 클래스를 포함하는 5개의 이미지 매팅 데이터 세트의 조합에서 MAM을 훈련하여 M2M 모듈이 이미지 매팅을 위한 일반화 가능한 기능을 학습할 수 있도록 했습니다.훈련하는 동안, 우리는 무작위로 대상 인스턴스를 배경 이미지에 배치하고 사전 훈련된 SAM을 사용하여 해당 인스턴스의 마스크 예측을 출력합니다.그런 다음 훈련 가능한 M2M 모듈은 다중 스케일 알파 매트를 예측하여 마스크를 정제합니다.마스크 또는 알파 매트를 기반으로 하는 반복적인 정제 프로세스를 통해 다중 스케일 예측이 병합되어 최종적인 세심한 알파 매트를 얻습니다.우리는 의미적 이미지 매팅 벤치마크 PPM-100[40], AM2K[25] PM-10K[25], 인스턴스 이미지 매팅 벤치마크 RWP636[55], HIM2K[42], 참조 이미지 매팅 벤치마크 RefMatte-RW 100[28]을 포함한 6개의 이미지 매팅 벤치마크에서 MAM에 대한 광범위한 평가를 수행했습니다. 우리의 결과는 MAM이 다양한 평가 지표에서 모든 벤치마크에 걸쳐 최첨단 이미지 매팅 모델과 비슷한 성능을 달성한다는 것을 보여줍니다.
--- EXPERIMENT ---
모든 결과는 MAM이 각 벤치마크에서 다른 지표에서 최첨단 전문 이미지 매팅 모델과 비슷한 성능을 달성한다는 것을 보여줍니다. 전반적으로 MAM은 뛰어난 일반화 능력을 보여주고 더 적은 매개변수로 다양한 이미지 매팅 작업을 효과적으로 처리할 수 있어 통합 이미지 매팅을 위한 실용적인 솔루션이 됩니다. 저희의 코드와 모델은 https://github.com/SHI-Labs/Matting-Anything에서 오픈 소스로 제공됩니다. 1. 소개 오래된 컴퓨터 비전 작업인 이미지 매팅은 주어진 입력 이미지 I[45]에서 알파 매트 a를 추정하는 것을 목표로 합니다. 매팅 대상은 주로 의미 수준에서 사람이나 다른 물체 주변입니다[26,41,49]. 최근 연구에서는 이미지 매팅의 범위를 인스턴스 인식 알파 매트 예측이 필요한 이미지 인스턴스 매팅[42]과 자연어 설명이 주어진 알파 매트를 추출하는 참조 이미지 매팅[28]과 같은 보다 복잡한 시나리오로 확장했습니다. 이전의 딥 러닝 기반 이미지 매팅 방법[28, 29, 36, 37, 42, 47, 51, 54, 59]은 해당 벤치마크에서 특정 이미지 매팅 작업을 처리하기 위해 제안되었습니다. 이러한 방법은 개별 데이터 세트에 맞게 조정되었으며 고정된 모델 설계로 인해 다양한 이미지 매팅 작업을 처리할 수 있는 유연성이 부족합니다. 이러한 제한으로 인해 보다 일반화되고 다재다능한 이미지 매팅 모델 개발이 방해를 받았습니다. 결과적으로 단일 모델로 다양한 유형의 이미지 매팅 작업을 처리할 수 있는 보다 적응적이고 효율적인 이미지 매팅 프레임워크를 개발하는 데 대한 관심이 커지고 있습니다. 또한 이전의 이미지 매팅 방법은 정확한 알파 매트 예측을 달성하기 위해 보조 입력으로 사용자 안내 트리맵에 의존했습니다. 대신 마스크 안내 또는 배경 이미지를 사용하는 일부 트리맵 없는 방법[39,55]이 제안되었지만 대화형 사용에 대한 사용자 요청에 따라 대상 인스턴스의 알파 매트를 추정할 수 없습니다. 따라서 사용자 가이드 트리맵에 의존하지 않고도 정확한 알파 매트 추정을 달성할 수 있는 모델을 개발하는 것이 중요하며, 동시에 대화형 사용을 위해 유연하고 효율적인 방식으로 간단한 사용자 요청을 처리할 수 있어야 합니다. 이러한 모델은 수동 개입에 대한 추가 필요성을 줄임으로써 사용자 경험을 크게 향상시킬 것입니다. 이미지 매팅의 이러한 한계에 동기를 부여받아 그림 1과 같이 이미지에서 프롬프트 기반 사용자 가이드로 모든 대상 인스턴스의 알파 매트를 추정할 수 있는 다재다능한 네트워크인 Matting Anything Model(MAM)을 제안합니다. MAM은 유연한 프롬프팅을 지원하고 대화형 사용을 위해 모든 대상 인스턴스의 분할 마스크를 출력하는 최신 Segment Anything Model(SAM) 프레임워크[21]를 활용합니다. 구체적으로 MAM은 SAM의 피처 맵과 마스크 출력을 입력으로 사용하고 가벼운 Mask-to-Matte(M2M) 모듈을 추가하여 대상 인스턴스의 알파 매트를 예측합니다. 우리는 다양한 인스턴스 클래스를 포함하는 5개의 이미지 매팅 데이터 세트의 조합에서 MAM을 훈련하여 M2M 모듈이 이미지 매팅을 위한 일반화 가능한 기능을 학습할 수 있도록 했습니다.훈련하는 동안, 우리는 무작위로 대상 인스턴스를 배경 이미지에 배치하고 사전 훈련된 SAM을 사용하여 해당 인스턴스의 마스크 예측을 출력합니다.그런 다음 훈련 가능한 M2M 모듈은 다중 스케일 알파 매트를 예측하여 마스크를 정제합니다.마스크 또는 알파 매트를 기반으로 하는 반복적인 정제 프로세스를 통해 다중 스케일 예측이 병합되어 최종적인 세심한 알파 매트를 얻습니다.우리는 의미적 이미지 매팅 벤치마크 PPM-100[40], AM2K[25] PM-10K[25], 인스턴스 이미지 매팅 벤치마크 RWP636[55], HIM2K[42], 참조 이미지 매팅 벤치마크 RefMatte-RW 100[28]을 포함한 6개의 이미지 매팅 벤치마크에서 MAM에 대한 광범위한 평가를 수행했습니다. 우리의 결과는 MAM이 다양한 평가 지표에서 모든 벤치마크에서 최첨단 이미지 매팅 모델과 비슷한 성능을 달성한다는 것을 보여줍니다.실험 결과는 다양한 이미지 매팅 작업을 대화형이고 효율적인 방식으로 처리하기 위한 제안된 접근 방식의 다재다능함과 효과성을 강조합니다.2. 관련 연구 2.1. 이미지 매팅 전경 이미지 F와 계수 알파 매트 a, IaF + (1 - a)B를 갖는 배경 이미지 B의 조합으로 볼 수 있는 이미지 I가 주어지면 (1) 이미지 매팅은 주어진 I만을 입력으로 추정하는 것입니다.기존 방법은 절대 전경 영역, 절대 배경 영역 및 전환 영역에 명시적으로 주석을 달는 사용자 안내 트리맵에 의존합니다.그런 다음 샘플링 기반 이미지 매팅 솔루션은 저수준 기능을 사용하여 전경과 배경 이웃 간의 유사성을 측정하여 전환 영역을 구별합니다[1,2,5,8,9,11]. 최근, 딥 러닝 기반 방법[27, 36, 47, 51, 54, 59]은 신경망을 채택하여 보조 입력으로 트리맵을 사용하여 종단 간 방식으로 알파 매트를 추정합니다. 일부 트리맵 없는 방법은 트리맵이 없는 부분을 보완하기 위해 배경 이미지[39], 마스크 안내[32,55] 또는 분할 데이터[4, 30]를 사용합니다. 이미지 I에 여러 인스턴스가 포함된 경우 구성은 NNI = ΣaiFi + (1 - Σai) B ia¿는 인스턴스 i의 알파 매트를 나타내고 InstMatt[42]는 대상 및 참조 마스크를 안내로 채택하여 인스턴스 인식 알파 매트 예측을 수행합니다. 대화형 매팅 방법[28, 48, 52]은 점, 상자 또는 텍스트 입력을 사용하여 대상 인스턴스의 알파 매트를 추정하는 특수 모델을 개발합니다. MatAny[53]는 의미적 이미지 매팅을 위해 SAM도 채택하는 동시 작업입니다. 비디오 매팅의 경우, 실시간 추론을 위해 트리맵 없는 [22–24, 30, 40] 방법이 탐구되는 반면 프레임당 예측 품질은 이미지 매팅 방법과 비교할 수 없습니다. 그러나 이러한 방법은 해당 벤치마크가 있는 특정 시나리오에 맞게 설계되어 다양한 이미지 매팅 작업과 벤치마크를 동시에 처리할 수 있는 잠재력이 제한됩니다. 2.2. 이미지 분할 이미지 분할은 이미지 매팅과 밀접한 연구 분야인 반면, 다양한 세그먼트 내 모든 것 모델(SAM)의 바이너리 마스크를 예측합니다. 이미지 고정 가중치 피처 맵 RB 정제 블록 반복적 정제 프롬프트(상자/점) 검은색/빨간색 스웨터를 입은 소녀 프롬프트(텍스트) 마스크 마스크 대 매트(M2M) 모듈 RB RB RB a osl αaos그림 2. 매팅 모든 것 모델 아키텍처. MAM 아키텍처는 사전 학습된 SAM과 M2M 모듈로 구성됩니다. 입력 이미지 I가 주어지면 SAM은 상자 또는 지점 사용자 프롬프트를 기반으로 대상 인스턴스에 대한 마스크 예측을 생성합니다.M2M 모듈은 이미지, 마스크 및 피처 맵을 포함한 연결된 입력을 가져와 다중 스케일 예측 aos8, dos4 및 Cosl을 생성합니다.섹션 3에서 자세히 설명하는 반복적 정제 프로세스는 다중 스케일 출력의 정보를 통합하여 최종 세심한 알파 매트 a의 정밀도를 점진적으로 향상시킵니다.이미지 매팅과 유사하게 많은 이미지 분할 방법은 의미 분할[3, 15], 인스턴스 분할[13, 46] 및 파노라마 분할[20, 44]과 같이 특정 이미지 분할 작업에 맞게 조정됩니다.최근 작업에서는 통합 이미지 분할을 위한 변환기 기반 프레임워크[6, 12, 16, 17]를 탐색하기 시작했습니다.언어 안내 분할 프레임워크[50,58]는 인스턴스 인식 마스크를 분할하기 위해 텍스트 감독을 찾습니다. OneFormer[16]는 공동 학습 전략으로 학습하기 위해 단일 변압기 모델을 채택하고 의미론적, 인스턴스 및 파노라마 분할에서 범용 분할을 수행하며 특수 모델보다 성능이 뛰어납니다.SAM[21]은 최근 한 단계 더 나아가 사용자가 이미지의 모든 인스턴스를 대화형으로 분할하도록 유연한 프롬프트를 지원합니다.Grounded-SAM[33]은 DINO를 SAM과 통합하여 텍스트 프롬프트 지원을 추가합니다.SAM과 같은 Foundation 모델은 다른 영역에서 다양한 애플리케이션을 지원하는 다재다능한 프레임워크를 개발할 수 있는 기회를 제공합니다.3. Matting Anything 이 섹션에서는 동결된 Segment Anything Model(SAM)과 학습 가능한 Mask-to-Matte(M2M) 모듈의 두 가지 주요 구성 요소로 구성된 Matting Anything Model(MAM) 아키텍처에 대한 개요를 제공합니다.먼저 사용자 안내 프롬프트가 주어진 경우 고품질 인스턴스 분할을 생성하도록 설계된 SAM에 대한 간략한 검토를 제공합니다.그런 다음 이진 마스크를 고품질 알파 매트로 변환할 수 있는 M2M 모듈을 소개합니다. 마지막으로 M2M 모듈을 SAM에 연결하여 종단 간 MAM을 점진적으로 구축하는 방법을 설명합니다.3.1. Segment Anything 모델 Segment Anything은 최근에 제안된 분할 기반 모델입니다.이미지 I Є R³×H×W¸가 주어지면 SAM은 ViT 기반 이미지 인코더를 사용하여 심층 피처 맵 FЄ RC×6×6을 얻습니다.그런 다음 다양한 N 입력 프롬프트가 프롬프트 인코더에 의해 인코딩되어 피처 맵과 함께 마스크 디코더로 전송됩니다.마스크 디코더는 입력 프롬프트에 의해 지정된 마스크 후보 m¿ Є R¹×H×W, i Є N 세트를 반환합니다.SAM은 유연한 프롬프팅 메커니즘을 통해 대화형 사용이 가능하며 다운스트림 작업에 쉽게 적용할 수 있습니다.3.2. Mask-to-Matte Mask-to-Matte(M2M) 모듈은 Matting Anything Model(MAM)의 필수 구성 요소이며 SAM의 인스턴스 인식 마스크 예측을 인스턴스 인식 알파 매트 예측으로 효율적이고 원활하게 변환하도록 설계되었습니다. 이를 달성하기 위해, 우리는 SAM에서 생성된 피처 맵과 마스크 예측을 M2M에 대한 보조 입력으로 활용합니다. 예측의 정확도를 높이기 위해, 우리는 알파 매트를 예측하기 위한 다중 스케일 브랜치를 채택하고 반복적 정제 일정을 통해 이러한 예측을 병합합니다. 다중 스케일 예측: 입력 이미지 I Є R³×HxW가 주어지면 사전 학습된 SAM 모델은 즉각적인 안내를 통해 대상 인스턴스에서 피처 맵 F € RC××와 마스크 예측 m = R¹×HxW를 생성합니다. 우리는 재조정된 이미지, 마스크 및 기능 맵을 연결하여 다음을 형성합니다.작업 벤치마크 메트릭 특수 모델 AM2K 의미 매팅 PM-10K PPM-SADall MADall↓ MSE all↓ 인스턴스 매팅 HIM2K IMQmat IMQ RWPIMQmad IMQmse↑ 참조 매팅 RefMatte-RWSADall↓ MSE all↓ GFM-R [25] 10.6.GFM-D [25] 10.6.MODNet [19] 4.MGMatting [55] 57.71.30.53.InstMatt [42] 70.81.51.73.CLIPMat-B [28] 107.59.CLIPMat-L [28] 85.47.일반화된 모델 SAM [21] 25.MAM 17.25.15.10.4.61.74.49.56.33.17.68.81.54.76.29.15.표 1. 다양한 벤치마크에서 특수 매팅 모델과 MAM 간의 비교. ↑↓는 더 높거나 낮은 값이 해당 메트릭에 대해 더 나은 성능을 나타냄을 의미합니다. 회색 텍스트는 이러한 벤치마크를 위해 특별히 설계된 모델을 나타냅니다. MAM은 SAM보다 명확한 개선 사항과 통합 이미지 매팅 모델로서 뛰어난 일반화 능력을 보여줍니다. 입력 Fm2m Є R(C+4)× ×W를 M2M 모듈에 전달합니다. M2M은 연결된 셀프 어텐션 계층[56], 배치 노름 계층, 활성화 계층을 포함하는 여러 정제 블록[7,55]을 사용하여 1/8 해상도에서 알파 매트 예측을 생성하며, 이를 E R¹× ×W로 표시합니다. 그런 다음 피처 맵을 더 높은 해상도로 업샘플링하여 1/4 및 전체 해상도에서 알파 매트 예측을 수행하고 각각 αos4 € R¹×× 및 ɑos1 € R¹×H×W로 표시합니다. 다중 스케일 예측을 통해 MAM은 다양한 스케일의 객체를 처리하고 세부적인 객체 추출을 위해 더 세분화된 알파 매트를 제공할 수 있습니다. 반복적 정제 전역 및 로컬 예측의 정확도를 높이기 위해 반복적 정제 프로세스를 사용합니다. 먼저 트라이맵과 같이 훈련 중에 이미지의 다른 영역을 강조하는 가중치 맵 Woss, Wos4 및 Wosl을 계산합니다. 이러한 가중치 맵은 각 예측 스케일에 대한 손실을 계산하는 데 사용되며 woss는 aoss 예측의 경우 전체 이미지를 강조하고, wos4는 aos4 예측의 경우 배경을 필터링하고, wos1은 전환 영역에만 초점을 맞춥니다. 추론하는 동안 aos8, Qos4, Cos1의 예측을 SAM의 마스크 예측 m과 점진적으로 병합하여 최종 알파 매트 예측 a Є R¹×HxW 3.3을 얻습니다.매팅 무엇이든 모델 Mask-to-Matte(M2M) 모듈을 개발한 후 Segment Anything Model(SAM)과 통합하여 매팅 무엇이든 모델(MAM)에 대한 종단 간 학습 및 추론을 가능하게 합니다. 이 통합을 통해 특징 추출에서 알파 매트 예측까지 전체 매팅 프로세스를 처리하는 포괄적이고 통합된 프레임워크를 사용할 수 있습니다. 다중 데이터 세트 학습 매팅 무엇이든 모델(MAM)의 견고성과 다양성을 보장하기 위해 다양한 이미지 매팅 데이터 세트의 다양한 전경 인스턴스와 배경 이미지를 포함하는 다중 데이터 세트 학습 방식을 채택합니다. 이 선택을 통해 광범위한 인스턴스 클래스와 배경 시나리오를 포괄하여 모델이 다양한 유형의 인스턴스와 배경을 효과적으로 처리하는 기능을 향상시킬 수 있습니다. 학습 과정에서 전경 인스턴스 F € R³×H×W와 해당 지상 진실 알파 매트 agt E R¹×H×W, 배경 이미지 BЄ R³×H×W를 결합하여 합성 이미지를 생성합니다. 합성은 방정식 I agt F (1 agt) B를 사용하여 수행됩니다. 그런 다음 합성 이미지 내에서 관심 인스턴스를 캡슐화하는 바운딩 박스 (xo, Yo, x1,y1)를 추출합니다. 그런 다음 이미지 I와 바운딩 박스를 사전 학습된 SAM에 프롬프트로 보내면 인스턴스의 마스크 예측이 반환됩니다. 그런 다음 이미지, 마스크 및 피처 맵을 연결하여 M2M 모듈에 보내면 다중 스케일 알파 매트 예측 aos8, Qos4, Qos1이 반환됩니다. 손실 L은 다중 스케일 예측과 기준 진실 agt 사이에서 L(agt, dos1, dos4, Qo88) = AL₁ £1 + ALLap Lap (3)으로 계산됩니다.L₁는 L1 손실이고 LLap은 [14,30,43]에서 사용된 라플라시안 손실입니다.계수 1과 XL Lap은 각각 각 손실 항의 기여도를 제어합니다.두 손실 항 모두 다중 스케일 예측에서 L₁ = L1 (agt, αos1) + L1 (agt, αos4) + L1 (agt, αoss) (4) LLap=LLap (agt, dos1)+L Lap (agt, Qos4) + L Lap (agt, dos8) (5)로 계산됩니다.다중 벤치마크 추론 추론 단계에서 우리는 일반성과 적응성을 평가하기 위해 다중 이미지 매팅 벤치마크에서 Matting Anything Model(MAM)에 대한 광범위한 평가를 수행했습니다. 입력 이미지 I가 주어졌을 때, SAM은 초기 마스크 예측을 생성했습니다.방법 SADall↓ SHM [4] LFM [57] HATT [37] SHMC [32] GFM-R [25] 17.81 / 16.36.12/37.28.01 / 22.61.50/57.6.8/6.11.6 / 15.5.5/3.27.0/29.GFM-D [25] SAM [21] MAM 10.89 / 11.10.26/11.25.00 / 44.17.30/25.2.9/3.2.9/4.10.8/28.3.5/9.10.2/9.21.0/ 15.16.1 13.35.6/34.6.4/6.5.9/6.14.8/25.10.1 / 15.MSE 전체↓ AM2K/PM-10K MAD 전체↓ Gradall↓ 12.54 / 14.SADtri↓ 10.26 / 8.19.68 / 16.13.36/9.35.23/23.9.15 / 8.21.06 / 21.18.29/15.37.00/37.10.00 / 13.8.82 / 12.8.24/7.60.01 / 24.10.65/14.20.72/31.15.67/23.표 2. 의미적 이미지 매팅 벤치마크 AM2K 및 PM-10K의 결과. 아래 첨자로 all과 tri가 있는 메트릭은 전체 이미지와 전환 영역의 평가를 별도로 나타냅니다.↓는 값이 낮을수록 메트릭의 성능이 더 우수함을 의미합니다.방법 DIM [51] MSE all↓ MADall↓ 11.17.FDMPA [59] 10.16.LFM [57] 9.15.SHM [4] 7.15.HATT [37] 6.13.BSHM [32] 6.11.MODNet [19] 4.8.SAM [21] MAM 10.13.4.9.표 3. 의미적 이미지 매팅 벤치마크 PPM100의 결과.m Є R¹×H×W는 인스턴스의 대략적인 경계를 포착했습니다.그 후 M2M은 다중 스케일 예측 aos8, Qos4 및 Qos1을 제공하여 알파 매트 예측의 개선에 기여했습니다. 그런 다음 반복적 개선을 거쳐 마스크 예측 m에서 해당 영역을 양의 가중치 맵을 보여주는 해당 다중 스케일 예측으로 대체하여 예측을 점진적으로 업데이트했으며, 일부 간단한 경우에는 대체가 m 대신 aoss에 직접 수행되었습니다. 이 반복적 개선을 통해 알파 매트 추정을 반복적으로 개선하고 최종 예측 a Є R¹×H×W¸ 4의 정확도를 높일 수 있었습니다. 실험 6가지 다양한 이미지 매팅 벤치마크에서 MAM의 성능을 광범위하게 평가합니다. 다양한 메트릭을 사용한 포괄적인 평가를 통해 각 벤치마크에서 MAM의 성능을 최첨단 이미지 매팅 모델과 비교합니다. 결과에 따르면 MAM은 전문화된 최첨단 모델과 지속적으로 비슷한 성능을 달성하여 통합 이미지 매팅 솔루션으로서의 다재다능함과 효과성을 재확인했습니다. 4.1. 구현 세부 정보 훈련 데이터 세트 훈련 과정에서 다양한 인스턴스 클래스를 보장하기 위해 Adobe Image Matting 데이터 세트[51], Distinctions-646[54], AM2K[25], Human-2K[34], RefMatte[28]를 포함한 여러 이미지 매팅 데이터 세트에서 전경 인스턴스를 무작위로 선택합니다. 배경 이미지의 경우 실제 세계와 합성 배경을 모두 혼합하기 위해 COCO[31]와 BG20K[25]의 두 데이터 세트에서 선택합니다. 평가 벤치마크 MAM의 적응 능력을 평가하기 위해 의미적 이미지 매팅 벤치마크 PPM-100[40], AM2K[25], PM-10K[25], 인스턴스 이미지 매팅 벤치마크 RWP636[55], HIM2K[42], 참조 이미지 매팅 벤치마크 RefMatte-RW100[28]을 포함한 다양한 이미지 매팅 벤치마크에서 테스트합니다. 박스 프롬프트는 모든 벤치마크에 사용되고 포인트 프롬프트는 RefMatte-RW100에서만 사용됩니다.이 포괄적인 평가를 통해 다양한 이미지 매팅 작업 및 벤치마크에서 MAM의 일반화 기능을 평가할 수 있습니다.평가 지표 일반적으로 채택된 평가 지표를 사용하여 MAM에 대한 예측된 알파 매트의 정확도를 평가합니다.특히 평균 절대 차이(MAD), 절대 차이 합(SAD), 평균 제곱 오차(MSE), 기울기(Grad) 및 연결성(Conn)[38]을 해당 평가 지표로 사용합니다.MAD, MSE, 기울기 및 Conn을 각각 103, 103, 10-3 및 10-³로 확장합니다.값이 낮을수록 이러한 지표에 대한 성능이 더 우수함을 나타냅니다.또한 인스턴스 인식 매팅의 경우 인식 및 매팅 정확도를 동시에 고려하는 인스턴스 매팅 품질(IMQ)[42]을 활용합니다.값이 높을수록 IMQ 지표에 대한 성능이 더 우수함을 나타냅니다. 실험 설정 우리는 GPU당 10개 이미지의 배치 크기로 8개의 RTX A6000 GPU를 사용하여 훈련 데이터 세트의 조합에서 MAM을 훈련했습니다. 각 이미지는 무작위로 선택된 전경 인스턴스와 배경 이미지의 조합이었습니다. 이미지는 1024×1024 크기로 자르고 대상 인스턴스의 경계 상자 프롬프트와 함께 사전 훈련된 ViT-B 기반 SAM[21]으로 전송되었습니다. SAM에서 출력한 피처 맵과 마스크는 알파 매트 예측을 위해 M2M 모듈에 입력되었습니다.모델 방법 크기 IMQmad Mask RCNN [13] 44.3M 18.25.Synthetic Subset ↑ IMQmse IMQgrad IMQcon Natural Subset ↑ IMQmad IMQmse IMQgrad IMQconn 0.19.24.33.2.26.CascadePSP [7] 67.7M 40.51.29.43.64.74.60.67.GCA [29] 25.2M 37.51.38.39.45.61.44.48.SIM [41] 46.5M 43.52.40.44.54.66.49.58.FBA [10] 34.7M 36.51.37.38.34.48.36.37.MGMatting[55] + 29.6M 51.67.53.55.57.71.66.60.InstMatt[42] SAM[21] + 29.7M 63.78.64.67.70.81.74.72.93.7M 49.61.4.51.61.74.13.65.MAM + 2.7M 54.68.30.55.68.81.51.72.표 4. 인스턴스 이미지 매팅 벤치마크 HIM2K의 결과. 아래 첨자로 mad, mse, grad 및 conn이 있는 메트릭은 IMQ의 유사성 메트릭이 각각 MAD, MSE, Gradient 및 Connectivity임을 나타냅니다. ↑는 더 높은 값이 IMQ 메트릭에 대한 더 나은 성능을 나타냄을 의미합니다. MAM은 다른 메트릭에서 SAM보다 명확한 개선을 보이며, 2.7M개의 추가 학습 가능 매개변수만 있고, MGMatting 및 InstMatt와 같은 다른 마스크 유도 방법에 비해 훨씬 가볍습니다. 방법 IMQmad IMQmse↑ 마스크 RCNN [13] 20.25.방법 MDETR [18] 프롬프트 SADall↓ MSEall↓ MADall 텍스트 131.67.75.CascadePSP [7] 42.52.CLIPSeg [35] 텍스트 211.117.122.GCA [29] 33.46.CLIPMat [28] 텍스트 107.59.62.SIM [41] 34.46.SAM [21] 텍스트 122.67.69.FBA [10] 35.47.MAM 텍스트 120.65.67.MGMatting [55] 30.53.SAM [21] 포인트 214.123.124.InstMatt [42] 51.73.MAM 포인트 168.89.97.SAM [21] 49.56.SAM [21] 박스 33.17.19.MAM 54.76.MAM 박스 29.15.16.표 5. 인스턴스 매팅 벤치마크 RWP636의 결과. tion. 우리는 ẞ₁ = 0. 및 B2 = 0.99인 Adam 옵티마이저를 채택하여 처음 4,000회 반복은 워밍업으로 20,000회 반복 동안 학습했습니다. 가중치 맵 woss는 학습하는 동안 모든 픽셀에서 항상 1인 반면, wos4는 4,000회 반복 후 SAM의 마스크 안내로 변경되고 Wos1은 4,000회 반복 후 os4의 경계로 변경됩니다. 우리는 aos8 예측을 위해 3개의 정제 블록, dos4 예측을 위해 3개의 정제 블록, 그리고 osl 예측을 위해 2개의 정제 블록을 설정했습니다. 결과적으로 MAM의 총 학습 가능한 매개변수는 270만 개입니다. 우리는 학습 중에 초기 학습률을 0.001로 하여 코사인 학습률 감소를 적용했습니다. 추론 중에 배치 크기가 1인 단일 GPU를 사용했습니다. 각 이미지는 긴 면이 1024픽셀이 되도록 크기를 조정했고 짧은 면은 대상 인스턴스의 알파 매트 예측을 위해 MAM으로 전송되기 전에 1024픽셀로 패딩했습니다. 4.2. 주요 결과 특수 모델 대 통합 모델 우리는 표 1에서 의미, 인스턴스 및 참조 이미지 매팅 벤치마크에 대한 특수 이미지 매팅 모델과 MAM 간의 상위 수준 비교를 제시합니다. 그것은 MAM이 모든 벤치마크에서 SAM보다 뚜렷이 개선되었음을 보여줍니다. 또한 MAM은 각 전문화된 표 6. 참조 이미지 매팅 벤치마크 RefMatte-RW 100에 대한 결과와 비슷한 성능을 보여줍니다. 상자 프롬프트가 있는 MAM은 텍스트 프롬프트를 사용하는 경우보다 상당히 더 나은 성능을 달성할 수 있으며 HIM2K, RWP635 및 RefMatte-RW100에서도 더 나은 성능을 달성하여 통합 이미지 매팅에 대한 실용적이고 실행 가능한 솔루션이 됩니다. 의미적 이미지 매팅 표 3과 표 2에 제시된 대로 PPM100[40], AM2K[25] 및 PM-10K[25]의 세 가지 의미적 이미지 매팅 벤치마크에서 MAM의 성능을 평가합니다. 반복적 정제 프로세스는 세 가지 벤치마크 모두에 대한 doss 예측을 기반으로 합니다. PPM-100 벤치마크에서 MAM은 SAM 전체에서 6.2 MSEall 및 3.9 MAD의 개선을 달성합니다. 마찬가지로 AM2K 벤치마크에서 MAM은 7.64 SAD all, 4.4 MSEall, 4.5 MADall, 41.Gradall, 7.59 SADtri의 향상으로 SAM보다 우수한 성능을 보였습니다.인스턴스 이미지 매팅 표 4와 표 5에서 MAM을 두 가지 인스턴스 이미지 매팅 벤치마크인 HIM2K[42]와 RWP636[55]에서 평가합니다.HIM2K의 경우 반복적 세분화는 이미지당 여러 인스턴스를 포함하고 m에서 시작하여 거짓 양성 예측을 제거하기 때문에 예측 마스크 m을 기반으로 합니다. HIM2K에서 다른 최신 방법과 비교했을 때 MAM은 단 2.7M개의 추가 학습 가능 매개변수로 비슷한 성능에 도달합니다.모델 자연 부분 집합 방법 크기 IMQmad IMQmse SAM [21] 93.7M 50.61.+ 마스크 선택 93.7M 61.74.MAM 기준선 1.0M 52.71.+ 다중 스케일 예측 2.7M 60.74.+ 반복적 개선 + 다중 데이터 세트 학습 2.7M 65.78.2.7M 68.81.표 7. HIM2K 벤치마크에서 MAM의 절제 연구.MAM 기준선은 상자 프롬프트가 있는 SAM 모델을 기반으로 구축됩니다.다른 전략은 MAM 기준선에 점진적으로 추가되고 결국 2.7M개의 추가 학습 가능 매개변수로 끝납니다. MGMatting 및 InstMatt와 같은 특수 모델의 10%에 불과하며, 이들은 Mask RCNN의 마스크 안내를 사용합니다. RWP636 벤치마크에서 aos8의 반복적 정제를 적용하고 MAM은 54.40 IMQmad와 76.45 IMQmse로 최신 상태에 도달합니다. 참조 이미지 매팅 표 6에서는 최근 도입된 참조 이미지 매팅 벤치마크인 RefMatte-RW 100 벤치마크[28]에서 MAM의 평가를 제시합니다. 이전 방법은 참조 이미지 매팅에 텍스트 프롬프트에 의존하는 반면, 우리는 SAM의 프롬프트로 경계 상자와 텍스트 설명을 활용합니다. SAM의 텍스트 프롬프트가 아직 출시되지 않았으므로 Grounded-SAM[33]을 사용하여 텍스트 프롬프트 안내를 지원합니다. 놀랍게도 MAM은 SAM의 프롬프트로 경계 상자를 활용할 때 뛰어난 성능을 달성하여 텍스트 기반 방법인 CLIPSeg 및 CLIPMat을 상당한 차이로 능가합니다. 또한, 경계 상자를 프롬프트로 사용하면 고정된 텍스트 단락을 구성하는 것보다 경계 상자를 제공하는 것이 더 쉽다고 생각하기 때문에 사용자 친화적이고 직관적인 상호 작용을 제공합니다.이 관찰 결과는 경계 상자 프롬프트가 참조 이미지 매팅을 위한 텍스트 또는 포인트 프롬프트보다 대화형 이미지 매팅에 더 효과적임을 시사합니다.4.3. 절제 연구 SAM이 훈련 프로세스 동안 고정된 상태를 유지한다는 점을 고려하여 MAM의 M2M 모듈에 대한 포괄적인 절제 연구를 수행합니다.MAM의 성능을 평가하기 위해 HIM2K 벤치마크의 실제 하위 집합을 선택합니다.HIM2K에서의 SAM 경계 상자와 포인트를 대상 인스턴스의 프롬프트로 사용하여 사전 훈련된 ViT-B 기반 SAM을 평가하는 것으로 시작합니다.상자 기반 프롬프트가 있는 SAM은 포인트 기반 프롬프트보다 상당히 성능이 뛰어나며 최종 마스크 출력은 경계 상자와의 IoU(Intersection over Union) 점수가 가장 높은 마스크를 기준으로 선택됩니다. SAM은 HIM2K 벤치마크에서 강력한 성능을 보이며 자연 하위 집합에서 IMQmad 61.15와 IMQmse 74.01을 달성했습니다.MAM 구축 그런 다음 D Image GT D MAM MG[51] 그림 3. MGMatting 및 MAM의 알파 매트 예측 시각화. 개선 사항은 빨간색 상자에 강조 표시되어 있습니다.SAM의 마스크 및 피처 맵과 이미지를 입력으로 사용하는 M2M 모듈을 통합하여 M2M 베이스라인을 구축합니다.3개의 연결된 정제 블록으로 구성되고 1/16 해상도로 예측하는 이 베이스라인은 저해상도 예측에는 알파 매트의 세부 정보가 부족하기 때문에 SAM에 비해 성능이 떨어집니다.그러나 섹션 3.2에서 설명한 대로 다중 스케일 예측과 반복적 정제를 점진적으로 통합함으로써 MAM의 성능이 향상됩니다. 또한, 섹션 3.3에 설명된 대로 다중 데이터 세트 학습을 채택하면 성능이 더욱 향상되어 자연 하위 집합에서 68.37 IMQmad 및 81.56 IMQmse가 생성됩니다. 그런 다음 일반성과 적응성을 검증하기 위해 재학습 없이 다른 벤치마크에서 MAM의 성능을 평가합니다. 4.4. 시각화 그림 3에서는 여러 인스턴스가 포함된 이미지의 MGMatting과 MAM 간의 매팅 성능을 비교합니다. 둘 다 SAM의 마스크 안내를 활용합니다. MAM은 동일한 마스크 안내에서 MGMatting에 비해 10%의 매개변수만으로 더 정확한 알파 매트 예측을 제공할 수 있음을 보여줍니다. 또한 다른 인스턴스에서 거짓 긍정 예측이 더 적습니다. 그림 4에서는 SAM과 MAM의 마스크 및 알파 매트 예측을 시각화합니다. 이러한 이미지는 의미적 이미지 매팅 벤치마크에서 선택되었습니다. MAM SAM[21] GT Image MAM SAM[21] GT Image 8 tc 意! FFF 그림 4. SAM 및 MAM의 마스크 및 알파 매트 예측 시각화. 개선 사항은 빨간색 상자에 강조 표시되어 있습니다. 그리고 사람, 동물 또는 투명한 물체가 될 수 있는 단일 인스턴스를 포함합니다. 시각화는 MAM이 트리맵 안내 없이 전환 영역에서 상당히 향상된 예측을 달성한다는 것을 보여주며, 이는 알파 매트 예측의 품질을 개선하고 향상시키는 데 있어 MAM의 뛰어난 성능을 강조합니다. 5.
--- CONCLUSION ---
이 논문에서는 Segment Anything Model(SAM)을 안내 모듈로 사용하고 가벼운 Mask-to-Matte(M2M) 모듈을 사용하여 마스크 출력을 대상 인스턴스의 알파 매트로 정제하는 Matting Anything Model(MAM)을 소개합니다. M2M은 점, 상자, 텍스트를 포함한 사용자 프롬프트를 기반으로 하는 단일 모델을 사용하여 의미론적, 인스턴스적, 참조적 이미지 매팅을 포함한 다양한 이미지 매팅 작업을 처리하도록 설계되었습니다. 우리는 6가지 이미지 매팅 벤치마크에서 MAM을 평가하고 다양한 평가 지표에서 전문화된 최첨단 방법과 비슷한 성능을 달성함을 보여줍니다. 제안된 모델은 대화형 및 통합 이미지 매팅을 위한 보다 다재다능하고 효율적인 솔루션을 제공합니다. 참고문헌 [1] Yagiz Aksoy, Tunc Ozan Aydin, Marc Pollefeys. 자연스러운 이미지 매팅을 위한 효과적인 픽셀 간 정보 흐름 설계. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2017.[2] Xue Bai 및 Guillermo Sapiro. 빠른 상호작용 이미지 및 비디오 분할 및 매팅을 위한 지오데식 프레임워크. 2007년 IEEE 11th International Conference on Computer Vision. IEEE, 2007.[3] Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam. 의미적 이미지 분할을 위한 아트로스 합성 재고. arXiv 사전 인쇄본 arXiv:1706.05587, 2017.[4] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai. 의미적 인간 매팅. 2018년 제26회 ACM 국제 멀티미디어 회의록. 2,[5] Qifeng Chen, Dingzeyu Li, Chi-Keung Tang. Knn 매팅. IEEE 패턴 분석 및 머신 인텔리전스 거래, 2013.[6] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 Masked-attention 마스크 변환기. arXiv 사전 인쇄본 arXiv:2112.01527, 2021.[7] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, Chi-Keung Tang. Cascadepsp: 전역 및 로컬 정제를 통한 클래스 독립적 및 매우 고해상도 분할을 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2020. 4,[8] Yung-Yu Chuang, Brian Curless, David H Salesin, Richard Szeliski. 디지털 매팅에 대한 베이지안 접근 방식. 2001 IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. CVPR 2001. IEEE, 2001.[9] Xiaoxue Feng, Xiaohui Liang, Zili Zhang. 희소 코딩을 통한 이미지 매팅을 위한 클러스터 샘플링 방법. European Conference on Computer Vision에서. Springer, 2016.[10] Marco Forte와 François Pitié. ƒ, b, 알파 매팅. arXiv 사전 인쇄본 arXiv:2003.07711, 2020.[11] Leo Grady, Thomas Schiwietz, Shmuel Aharon, Rüdiger Westermann. 대화형 알파 매팅을 위한 무작위 워크. VIIP 회의록, 2005.[12] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, Humphrey Shi. Neighborhood attention transformer. IEEE/CVF Conference on Computer Vision and Pattern Recognition 회의록, 2023.[13] Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. Mask r-cnn. ICCV, 2017. 3,[14] Qiqi Hou 및 Feng Liu. 동시 전경 및 알파 추정을 위한 컨텍스트 인식 이미지 매팅. ICCV, 2019.[15] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang. Ccnet: 의미 분할을 위한 교차 어텐션. TPAMI, 2020.[16] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. Oneformer: 범용 이미지 분할을 지배하는 하나의 변압기. CVPR, 2023.[17] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, Humphrey Shi. Semask: 의미 분할을 위한 의미 마스크 변환기. arXiv 사전 인쇄본 arXiv:2112.12782, 2021.[18] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion. 엔드투엔드 멀티모달 이해를 위한 변조 감지. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 1780-1790페이지, 2021.[19] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, Rynson WH Lau. Modnet: 객관적 분해를 통한 실시간 트라이맵 없는 초상화 매팅. AAAI 인공지능 컨퍼런스 회의록, 제36권, 1140-1147페이지, 2022. 4,[20] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2019.[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할하세요. arXiv 사전 인쇄 arXiv:2304.02643, 2023. 1, 2, 3, 4, 5, 6,[22] Jiachen Li, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Yunchao Wei 및 Humphrey Shi. Vmformer: Transformer를 사용한 엔드투엔드 비디오 매트입니다. arXiv 사전 인쇄 arXiv:2208.12801, 2022.[23] Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan 및 Humphrey Shi. 비디오 인스턴스 매트. arXiv 사전 인쇄 arXiv:2311.04212, 2023.[24] Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant Navasardyan, Yunchao Wei 및 Humphrey Shi. VideoMatt: 접근 가능한 실시간 비디오 매팅을 위한 간단한 기준선. CVPR 워크숍, 2023.[25] Jizhizi Li, Jing Zhang, Stephen J Maybank, Dacheng Tao. 합성 및 실제 연결: 엔드투엔드 딥 이미지 매팅을 향하여. International Journal of Computer Vision, 2022. 2, 4, 5,[26] Jizhizi Li, Jing Zhang, Dacheng Tao. 딥 자동 자연 이미지 매팅. arXiv 사전 인쇄본 arXiv:2107.07235, 2021.[27] Jizhizi Li, Jing Zhang, Dacheng Tao. 딥 이미지 매팅: 포괄적 조사. ArXiv, 2023.[28] Jizhizi Li, Jing Zhang, Dacheng Tao. 참조 이미지 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2244822457페이지, 2023. 2, 4, 5, 6,[29] Yaoyi Li 및 Hongtao Lu. 안내된 맥락적 주의를 통한 자연스러운 이미지 매팅. AAAI 인공 지능 컨퍼런스 논문집, 2020. 2,[30] Shanchuan Lin, Linjie Yang, Imran Saleemi 및 Soumyadip Sengupta. 시간적 안내를 통한 견고한 고해상도 비디오 매팅. arXiv 사전 인쇄본 arXiv:2108.11515, 2021. 2,[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár 및 C Lawrence Zitnick. Microsoft coco: 맥락 속의 공통 객체. 유럽 컴퓨터 비전 컨퍼런스에서. 스프링거, 2014.[32] Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang 및 Xian-sheng Hua. 거친 주석을 사용하여 의미론적 인간 매트를 강화합니다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행 중, 2020. 2,[33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 접지 공룡: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합입니다. arXiv 사전 인쇄본 arXiv:2303.05499, 2023. 3,[34] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, Xin Yang. 이미지 매팅을 위한 3자 정보 마이닝 및 통합. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 2021.[35] Timo Lüddecke 및 Alexander Ecker. 텍스트 및 이미지 프롬프트를 사용한 이미지 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7086-7096페이지, 2022.[36] GyuTae Park, SungJoon Son, Jae Young Yoo, SeHo Kim, Nojun Kwak. Matteformer: 사전 토큰을 통한 Transformer 기반 이미지 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 2022년 6월.[37] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, Xiaopeng Wei. 이미지 매팅을 위한 주의 유도 계층 구조 집계. IEEE/CVF 컴퓨터 비전 및 패턴 인식 회의록, 2020. 2,[38] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, Pamela Rott. 이미지 매팅을 위한 지각적 동기 온라인 벤치마크. IEEE 컴퓨터 비전 및 패턴 인식 회의록. IEEE, 2009.[39] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, Ira Kemelmacher-Shlizerman. 배경 매팅: 세상은 당신의 그린 스크린입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2020.[40] Jiayu Sun, Zhanghan Ke, Lihe Zhang, Huchuan Lu, Rynson WH Lau. Modnet-v: 배경 복원을 통한 인물 비디오 매팅 개선. arXiv 사전 인쇄본 arXiv:2109.11818, 2021. 2, 5,[41] Yanan Sun, Chi-Keung Tang, Yu-Wing Tai. 의미적 이미지 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021. 1,[42] Yanan Sun, Chi-Keung Tang, Yu-Wing Tai. 상호 안내 및 다중 인스턴스 정제를 통한 인간 인스턴스 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022. 2, 4, 5,[43] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, Yu-Wing Tai. 시공간적 정렬 및 집계를 통한 딥 비디오 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021.[44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen. Max-deeplab: 마스크 변환기를 사용한 엔드투엔드 파노라마 분할. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021.[45] Jue Wang 및 Michael F Cohen. 이미지 및 비디오 매팅: 조사. 2008.[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen. Solov2: 동적 및 빠른 인스턴스 분할. 신경 정보 처리 시스템의 발전, 2020.[47] Yu Wang, Yi Niu, Peiyong Duan, Jianwei Lin, Yuanjie Zheng. 심층 전파 기반 이미지 매팅. IJCAI, 2018.[48] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao, Weiming Zhang, Nenghai Yu. 실시간 사용자 클릭 및 불확실성 추정을 통한 개선된 이미지 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 15374-15383페이지, 2021.[49] Bo Xu, Jiake Xie, Han Huang, Ziwen Li, Cheng Lu, Yong Tang, Yandong Guo. 상황 인식 유도 이미지 매팅. 2022년 제30회 ACM 국제 멀티미디어 컨퍼런스 논문집.[50] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang. Groupvit: 의미적 분할이 텍스트 감독에서 등장. 2022년 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집.[51] Ning Xu, Brian Price, Scott Cohen, Thomas Huang. 심층 이미지 매팅. 2017년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 2,[52] Stephen DH Yang, Bin Wang, Weijia Li, YiQi Lin, Conghui He. 통합 대화형 이미지 매팅. arXiv 사전 인쇄 arXiv:2205.08324, 2022.[53] Jingfeng Yao, Xinggang Wang, Lang Ye 및 Wenyu Liu. 무엇이든 매트: 세그먼트 모든 모델을 사용한 대화형 자연 이미지 매트입니다. arXiv 사전 인쇄 arXiv:2306.04121, 2023.[54] 유 하이차오(Hachao Yu), 닝 쉬(Ning Xu), 황자룡(Zilong Huang), 주우첸(Yuqian Zhou), 험프리 시(Humphrey Shi). 고해상도 딥 이미지 매트. AAAI, 2021. 2,[55] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai 및 Alan Yuille. 진보적인 개선 네트워크를 통한 마스크 유도 매트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021. 2, 4, 5,[56] Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena. 자기 주의 생성적 적대 네트워크. 기계 학습 국제 컨퍼런스에서. PMLR, 2019.[57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, Weiwei Xu. 디지털 매팅을 위한 후기 융합 CNN. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7469-7478페이지, 2019.[58] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Lmseg: 언어 가이드 multiarXiv 사전 인쇄본 arXiv:2302.13495, Wang, and Fan Wang. 데이터 세트 분할. 2023.[59] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, and Ming Tang. 모바일 폰에서 인물 애니메이션을 위한 빠른 딥 매팅. 2017년 제25회 ACM 국제 멀티미디어 컨퍼런스 회의록. 2,
