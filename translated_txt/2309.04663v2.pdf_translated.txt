--- ABSTRACT ---
대규모 언어 모델(LLM)에 대한 학습 패러다임은 현재 맥락 내 학습(ICL) 또는 전체 미세 조정에 속하는 경향이 있습니다. 이러한 각 패러다임은 사용 가능한 데이터, 모델 크기, 컴퓨팅 비용, 사용 편의성 및 최종 품질에 따라 고유한 상충 관계가 있으며, 두 솔루션 모두 전반적으로 좋은 성과를 거두지 못했습니다. 이 글에서는 먼저 ICL과 미세 조정 패러다임을 자연스러운 연결을 강조하는 방식으로 설명합니다. 이러한 연결을 기반으로 이러한 패러다임의 장점을 결합하여² 가장 큰 모델에서 신속한 엔지니어링 지침과 사고의 사슬 추론을 가능하게 하는 동시에 유사한 방법을 사용하여 매개변수 효율적인 조정으로 적당한 크기의 LLM에서 매개변수 업데이트를 수행합니다. 다양한 다국어 작업³에서 FIAT의 효과를 평가하고 FIAT가 100~10,000개의 학습 예제 범위에서 ICL과 미세 조정보다 더 나은 성과를 거두는 것을 관찰했습니다. 우리는 FIAT가 학습 패러다임 사이에서 어려운 선택을 하지 않고도 LLM의 잠재력을 최대한 활용할 수 있는 실용적인 방법을 제공하기를 바랍니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 새로운 과제와 언어에 대한 인상적인 일반화 능력을 보여줍니다. 문제를 해결하기 위한 논리적 추론을 생성하는 것과 같은 가장 흥미로운 기능 중 일부는 모델 크기가 특정 임계값, 종종 수천억 개의 매개변수를 초과할 때만 나타나는 것으로 밝혀졌습니다(Wei et al., 2022b;a). 이러한 모델의 인상적인 기능은 과제별 튜닝 없이도 고품질 응답을 생성할 수 있으며, 이러한 모델을 추가로 튜닝하는 데 드는 비용이 매우 높기 때문에 최근 많은 작업이 모델의 입력에 몇 가지 과제별 예와 지침을 배치하는 컨텍스트 내 학습(ICL) 패러다임에 초점을 맞추게 되었습니다(Brown et al., 2020; Chowdhery et al., 2022; Google et al., 2023; OpenAI, 2023). 이전 연구에서는 작업 데이터에 대한 모델을 미세 조정하면 ICL에 비해 다운스트림 작업에서 더 우수한 성능을 낼 수 있는 경우가 많다는 것을 확인했지만(Scao &amp; Rush, 2021; Schick &amp; Schütze, 2020a;b; Asai et al., 2023), 데이터가 제한된 작업에 대한 모델을 미세 조정하려는 최근의 노력은 상당히 적습니다.아마도 매우 큰 모델을 조정하는 데 따른 시간과 컴퓨팅 비용으로 인해 실무자들이 더 작은 모델을 선호하게 되어 새로운 모델 기능을 활용할 수 없게 되었기 때문일 것입니다.ICL과 모델 미세 조정은 각각 고유한 상충 관계가 있습니다.ICL은 학습 비용이 발생하지 않으며 가장 유능한 LLM을 활용할 수 있습니다(Schick &amp; Schütze, 2020b; OpenAI, 2023).그러나 ICL은 소수의 주석이 달린 예제로 많은 작업에서 경쟁력 있는 성능을 달성할 수 있지만 잘 작동하려면 종종 매우 큰 모델이 필요하고 컨텍스트 창에 맞지 않으면 추가 학습 예제를 활용할 수 없습니다. 많은 작업에서 이로 인해 잠재적으로 유용한 훈련 사례의 상당수가 무시됩니다. 반면, 미세 조정은 학습 예제를 모델의 입력에 맞춰야 할 필요성에 의해 제약받지 않으며, 매우 효과적일 수 있습니다. &quot;FIAT라는 이름은 Fusing Learning Paradigms with Instruction Accelerated Tuning에서 따왔습니다. 2 FIAT는 학습 패러다임뿐만 아니라 모델 자체도 융합합니다. 3 이러한 작업은 추가 데이터가 없기 때문에 자연스럽게 데이터가 적다고 하며, 더 많은 데이터를 얻는 것도 쉽지 않습니다. 이를 대규모 데이터가 있지만 무시되는 인위적으로 데이터가 적은 시나리오와 대조합니다. 인간이 프롬프트 엔지니어링을 통해 업데이트한 명령어 조정 LLM X 입력 시퀀스 몇 가지 샷 컨텍스트 내 학습 명령어 조정 LLM CoT 이유 IB x θρ 입력 시퀀스 인간이 프롬프트 엔지니어링을 통해 업데이트한 yx 작업 데이터에 대한 경사 하강을 통해 업데이트한 LLM 전체 미세 조정 작은 매개변수가 작업 데이터에 대한 경사 하강을 통해 업데이트한 명령어 조정 LLM ŷB X PEFT 0. y 입력 시퀀스 y 두 패러다임을 사용한 FIAT 그림 1: FIAT의 전반적인 흐름과 ICL 및 미세 조정과 비교한 방식. 색상이 지정된 구성 요소는 FIAT의 작업별 인스턴스를 빌드하고 학습하는 동안 업데이트되는 반면, 다른 구성 요소는 고정됩니다.0B는 더 큰 LLM의 매개변수이고 I ẞ는 추론을 유도하는 데 사용되는 명령어입니다.+는 조정할 중간 크기의 LLM의 매개변수이고 I는 모델이 올바른 최종 답을 예측하는 데 도움이 되는 명령어입니다.더 작은 언어 모델에서도 마찬가지입니다.이러한 상충 관계로 인해 실무자는 최상의 접근 방식을 선택하기 위해 임의로 패러다임을 선택하거나 이러한 서로 다른 방법에 대한 비용이 많이 드는 실험을 수행하는 경향이 있습니다.대신 이 두 모델 학습 패러다임은 실제로 상호 보완적이라고 생각합니다.이를 위해 매우 큰 모델에서 ICL을 활용하고 중간 크기의 LLM에서 매개변수 조정을 활용하면서 각 패러다임과 관련된 공통 기술을 융합하는 FIAT-학습 패러다임과 지침 가속 조정(FIAT) 융합을 제안합니다. FIAT는 매우 큰 모델에서 사고의 사슬 추론을 이끌어내는 수작업 엔지니어링 지시 프롬프트를 사용하는 동시에 생성된 추론 및 지시 프롬프트를 사용하여 매개변수 효율적인 튜닝으로 중간 크기의 LLM을 튜닝합니다. 그림 1은 FIAT의 워크플로와 ICL 및 미세 조정과 비교한 방식을 보여줍니다. 이 기사의 나머지 부분에서는 ICL과 미세 조정 간의 연결과 각 패러다임 내에서 개발된 다양한 기술을 공식적으로 설명합니다(§2). 이러한 것들 중 가장 좋은 것을 결합하고 각 개인의 많은 함정을 피하는 FIAT를 제안합니다(§2.3). 우리는 100~10,000개 예제의 데이터 시나리오에서 FIAT가 두 학습 패러다임보다 어떻게 개선되는지 보여주는 실험을 제시하고 이러한 이득이 어디에서 나오는지 자세히 설명하는 절제를 제시합니다(§3).LLMS를 위한 학습 패러다임 이 섹션에서는 LLM을 위한 두 가지 인기 있는 학습 패러다임(§2.1의 ICL과 §2.2의 매개변수 조정)을 검토하면서 각각의 강점과 약점을 고려합니다.이러한 강점과 약점은 FIAT로 직접 이어집니다(§2.3).2. 맥락 내 학습 지시된 ICL은 LLM의 매개변수를 고정된 상태로 유지하지만 대신 다운스트림 작업의 정확도를 개선하기 위해 명령 프롬프트(종종 수동 최적화를 통해)를 선택합니다. 형식적으로, 모델 예측은 고정된 0과 텍스트 명령 I로 매개변수화된 매우 큰 사전 학습된 LLM을 샘플링하여 수행됩니다.P(y|x; 0, 1) (1) * 일반적으로 샘플링은 온도가 0인 간단한 argmax이지만 다수결 투표와 같은 기술에서와 같이 항상 그런 것은 아닙니다.명령 I은 실제로 모델 입력 x에 접두사로 붙지만, 우리는 의도적으로 이를 모델의 인수로 표시하여 이것이 개념화된 방식을 더 잘 반영한다고 주장합니다.나중에 이를 기반으로 구축할 것입니다.사고의 사슬 추론은 I를 제작하여 모델의 출력에서 단계별 추론을 유도하여 모델이 올바른 예측에 도달하는 능력을 향상시킴으로써 지시된 ICL을 한 단계 더 발전시킵니다(Wei et al., 2022b).이를 통해 자기 회귀 추론을 통해 입력에 대한 관찰을 출력하거나 최종 답을 예측할 때 향후 디코딩 단계에서 활용할 수 있는 전체 작업의 하위 문제를 해결할 수 있습니다. 또한 사전 학습 중에 모델이 본 텍스트 패턴을 이끌어낼 수도 있는데, 이는 그렇지 않으면 모델의 잠재 특징 공간에서 접근하기 어려울 것입니다(예: 미세 조정을 통해).소수 샷 ICL 소수 샷 ICL은 지시 I가 지시를 통해 모델에 텍스트 입력으로 포맷된 학습 예제 D 중에서 선택한 소수의 예제로 구성된다는 점에서 지시 ICL과 다릅니다.지시 조정 기반 모델 FLAN 및 TO(Sanh et al., 2021; Chung et al., 2022; Longpre et al., 2023)와 같은 지시 조정 모델은 종종 사전 학습된 모델을 사용하는 것에 비해 ICL에서 상당한 개선을 제공합니다.이는 지시 조정이 본질적으로 분포가 다운스트림 작업에 더 가까운 멀티태스크 데이터 세트를 사용하는 2단계 사전 학습이기 때문입니다.ICL 패러다임은 주석이 달린 예제가 없거나 소수에 불과한 다양한 작업에서 경쟁력 있는 결과를 얻습니다. ICL은 추가적인 모델 튜닝 비용이 발생하지 않지만, 특히 chain-of-thought와 같은 기술을 사용할 때 잘 작동하려면 특정 크기 이상의 LLM이 필요하기 때문에 추론 비용이 높은 경우가 많습니다.또한 모델의 컨텍스트 창에 맞는 것 이상의 추가 작업 데이터를 활용할 수 없습니다.2.2 매개변수 튜닝 전체 매개변수 미세 조정 튜닝할 LLM의 사전 훈련된 매개변수 0이 주어지면, 표준 미세 조정은 다음에 따라 작업별 감독 학습 데이터 D에서 모델의 모든 매개변수를 간단히 최적화합니다.P(y|x; 0) (2) 0의 최적화는 ICL에서 I의 인간 프롬프트 엔지니어링 프로세스와 목적이 비슷합니다.모델 미세 조정은 학습 데이터를 모델의 컨텍스트 창에 맞출 필요가 없으므로 사용 가능한 학습 예제가 약간 더 많을 때 더 효과적입니다.미세 조정은 충분한 학습 예제가 있는 더 작은 언어 모델에서도 잘 작동하여 추론 속도가 빨라집니다. 그러나 미세 조정은 추가적인 학습 비용이 발생하고 모델 매개변수에 대한 액세스가 필요한 반면, 가장 유능한 LLM 중 일부는 추론 전용 API 액세스에 사용할 수 있습니다. 모델은 특히 데이터가 제한된 작업의 경우 재앙적 망각(Goodfellow et al., 2013)으로 인해 학습 예제에 쉽게 과적합될 수 있습니다. 매개변수 효율적 미세 조정(PEFT)은 |0 PEFT | &lt;&lt;&lt; |0|인 학습 매개변수화 PEFT를 사용하여 조정 절차를 개선합니다. 과적합의 위험을 줄이는 것 외에도 이 학습 기술은 학습 세트를 넘어 일반화에 유용할 수 있는 기능을 잊지 않도록 합니다. 마찬가지로 ICL은 매개변수를 고정한 채로 모델의 입력만 수정하여 재앙적 망각을 방지합니다. 2.3 FIAT를 사용한 학습 패러다임 융합 이 섹션에서는 모델링 기능 측면에서 각 설계 선택의 목적을 설명하면서 FIAT를 구성합니다. ICL과 미세 조정은 각각 매력적인 강점과 함정을 가지고 있으며, 이를 표 1에 요약했습니다. 높은 수준에서 이러한 속성은 대체로 상호 보완적임을 관찰했습니다. 5 실제로 |0|은 ICL보다 미세 조정에서 훨씬 더 작은 경향이 있습니다.ICL 미세 조정 강점 작은 모델에서 잘 작동 아니요 예 대량의 학습 데이터 지원 아니요 예 사고의 사슬 추론 지원 예 아니요 지시 프롬프트 사용 예 아니요 과제 매개변수 업데이트 없음 예 치명적인 망각 방지 예 ZZ 아니요 아니요 표 1: 일반적인 사용 패턴에 따른 ICL 및 미세 조정 학습 패러다임 비교.• ICL과 미세 조정의 이러한 능력을 반영하여 다음을 수행할 수 있는 접근 방식을 모색합니다.• • 지시 따르기: 인간이 설계한 지침을 따라 고품질 예측을 달성합니다.사고의 사슬 추론: 모델이 올바른 예측을 향해 나아가는 데 도움이 되는 중간 텍스트를 생성합니다.• 매개변수 조정: 중간에서 많은 수의 지도 학습 예제에 맞게 내부 표현을 정제합니다. 그리고 • 데이터 스케일링: 100개에서 1000개의 예제에 이르는 데이터 스케일로 고품질 모델을 제공합니다. CoT 증강 튜닝을 통한 모델 스태킹 우리는 사고의 사슬 프롬프트가 일반적으로 감독되지 않고 신중하게 작성된 지침을 통해 유도된다는 관찰에서 시작합니다. 이를 동기로 학습 및 추론을 위한 두 가지 모델을 융합합니다. LLM의 가장 강력한 새로운 역량을 모두 갖춘 큰 모델 ẞ과 관심 있는 작업의 용량 요구 사항에 따라 크기를 유연하게 선택할 수 있는 조정 가능한 모델 7입니다. 사고의 사슬 추론의 책임을 ẞ에 할당한 다음 텍스트 예측 ŷß을 조정 가능한 모델에 제공합니다. 그런 다음 이러한 입력(예: 사고의 사슬 설명)을 감독된 출력을 예측하는 데 얼마나 유용한지에 따라 가장 잘 사용하는 방법을 학습할 수 있습니다. 매개 변수 0ẞ는 하위 작업에 대한 직접 감독 데이터가 없고 필요하지 않기 때문에 고정된 상태로 유지됩니다. 명령어 증강 튜닝 우수한 명령어 프롬프트를 만드는 것은 고품질 ICL 성능에 필수적인 것으로 알려져 있으므로, 추론과 설명을 생성하는 명령어 Iẞ를 첫 번째 단계로 자연스럽게 포함합니다. 명령어는 일반적으로 소규모 튜닝 가능 모델 IT에 사용되지 않지만, 명령어가 튜닝에도 도움이 될 수 있는 잠재력이 있음을 관찰했습니다. 명령어가 작업의 입력을 사전 학습 중에 보이는 분포와 더 잘 일치시켜 모델이 더 빠르게 수렴할 수 있을 뿐만 아니라 매개변수 업데이트도 줄일 수 있다고 추측합니다. 이를 통해 과도한 매개변수 업데이트와 관련된 치명적인 망각의 위험을 피할 수 있습니다. 따라서 FIAT는 튜닝 가능 모델에 대해 별도의 명령어 I도 제공합니다. 퍼베이시브 명령어 튜닝 모델 이미 명령어 튜닝 모델은 ICL의 표준이 되었습니다. 우리는 모든 실험에서 0ẞ와 같은 모델을 사용합니다. 그러나 FIAT의 Instructionaugmented Tuning 사용을 감안할 때, 우리는 주로 span corruption 목표에 대해 사전 학습된 모델에서 시작하는 미세 조정의 일반적인 관행에서 벗어나 대신 instruction-tuned checkpoint로 초기화합니다(Longpre et al., 2023). 이렇게 하면 모델이 이미 명령을 예상하고 있기 때문에 최적화가 더 쉬워집니다. 이는 제한된 학습 데이터 시나리오에서 특히 유용할 수 있습니다. 매개변수 효율적 튜닝 지금까지 우리는 FIAT의 설계에 사고의 사슬 추론, 튜닝에서의 명령 따르기, 명령 조정 초기화를 추가했으며, 이는 모두 원하는 출력의 확률을 높이는 측면에서 사전 튜닝 모델과 작업 정의를 서로에게 가깝게 움직입니다. 우리는 매개변수 효율적 튜닝이 FIAT에서 훈련 데이터에 대한 최적화에 특히 적합하다고 가정합니다. 모델 매개변수 0에 대한 큰 변경은 &quot;FIAT에서 명령어는 이전의 통계적 학습 방법에서 베이지안 사전과 유사한 목적을 제공하는 것으로 볼 수 있습니다. 이를 통해 매개변수를 경험적으로 추정하는 감독 데이터와 함께 학습 절차에 인간 지식을 인코딩할 수 있습니다. 그러나 텍스트 명령어는 Dirichlet의 하이퍼 매개변수보다 훨씬 더 자연스러운 방법입니다. 알고리즘 1: FIAT를 사용한 모델 구축 입력: 0, 0, D 출력: 0, Iẞ, IT // 추론 명령어 작성 및 예시 선택. IB = PROMPTENGINEERING (D, 0B) // 큰 모델을 기반으로 튜닝 명령어 작성. IT PROMPTENGINEERING (D, 0B) // 매개변수 효율적 튜닝 초기화. OPEFT T ← INIT(07) // 예제 또는 데이터 배치 반복. for x, y ED do end // 확장, 설명, 추론을 생성합니다.Ув = arg max, P(y|x; 0ß, Iß) // 매개변수 효율적 업데이트를 사용하여 최적화합니다.97 = ▼ PEFTP(y|x, ŷß; 07, 0PEFT, IT) OPEFT ← UPDATE (OPEFT, gr) // 최종 조정된 모델에 PEFT 업데이트를 적용합니다.04 — 07 OPEFT 알고리즘 2: FIAT를 사용한 추론 입력: x, I, I, 0ß, 출력: y // 확장, 설명, 추론을 생성합니다.= arg max, P(y|x; ¤ß, Iß) B₂ // 조정된 모델을 사용하여 최종 출력을 추론합니다.y = arg max, P(y|x, ŷß; ¤, I+) 그림 2: FIAT를 사용한 모델 구축 및 추론 왼쪽: FIAT를 사용한 모델 구축은 지침 I의 대화형 프롬프트 엔지니어링으로 시작됩니다. Iẞ는 대규모 주석이 없는 08-ie 동작에 대해 few-shot exemplar를 사용하여 추론을 수행하는 방법을 지정하는 반면, I는 생성된 추론 및 입력을 사용하여 최종 출력을 생성하기 위한 조정된 모델 0에 대한 지침을 지정합니다. 03과 0은 모두 지침 조정된 모델이며 매개변수 효율적 조정을 통해 훈련하는 동안 ¤만 업데이트됩니다. 오른쪽: FIAT를 사용한 추론은 매우 간단하며 다음만 필요합니다. (1) 고정된 사전 훈련된 매개변수 0ẞ 및 추론 지침 IB를 사용하여 대규모 생성 모델에 대한 호출, (2) 적절한 초기화가 주어진 경우 필요한 연관된 작업 지침 I와 함께 조정된 모델 0에 대한 호출.7 위의 모든 수정 사항을 공식화하면 알고리즘 1 및 알고리즘에서 미세 조정 및 추론에 사용되는 FIAT의 최종 공식에 도달합니다. 2. 실험 데이터 세트 다양한 훈련 데이터 크기를 자연스럽게 포괄하는 데이터 세트를 선택하는 주요 목표 중 하나입니다. 분류에서 모델의 짧은 답변 생성 능력 연습에 이르는 다양한 작업을 고려하며, 방법의 일반성을 평가하기 위해 다양한 언어를 포함합니다. 먼저, XOR-ATTRIQA(Muller et al., 2023)를 사용합니다. 이는 모델이 질문에 대한 제공된 답변이 주어진 구절 맥락에서 지원되는지 여부를 예측하도록 요청하는 분류 작업으로, 총 262개의 예가 있는 5개 언어를 포함합니다. 이를 O(100) 데이터 시나리오라고 합니다. 또한 XTREME-UP(Ruder et al., 2023)의 언어 간 QA 작업에서 FIAT의 동작을 연구합니다. 이 데이터는 TyDi QA(Clark et al., 2020) 데이터 세트의 언어 간 변형인 XOR QA³ 데이터 세트(Asai et al., 2020)를 확장한 것입니다. 이 과제는 영어가 아닌 질문과 영어 답변 구절이 주어졌을 때 올바른 영어 답변 범위를 예측하는 모델을 요구합니다. 이 과제에는 구절에 올바른 답변이 포함되지 않을 가능성도 포함되어 있어 더 어렵습니다. 언어 간 QA는 답변 내용이 거의 없는 언어에 특히 중요한 과제로, 언어 내 내용만 사용하여 답변이 불가능할 질문에 대한 답변을 제공할 수 있기 때문입니다. 두 가지 초점 세트에 대한 결과를 제공합니다. 첫째, 각 언어에 약 300개의 예가 있는 XTREME-UP 언어 간 QA에서 20개 인도어 하위 집합을 사용하여 7개의 시나리오를 연구할 수 있습니다. FIAT에서는 추가적인 추론 비용을 유발하지 않기 때문에 LoRA(Hu et al., 2021)를 사용하여 튜닝 절차를 매개변수화합니다. 향후 작업에서는 소프트 프롬프트 튜닝(Lester et al., 2021)과 같은 다른 방법을 고려해야 합니다. 8XOR QA는 언어 간 개방형 검색 질문 답변을 의미합니다. XOR QA와 XOR-ATTRIQA의 차이점을 주목하세요.XOR-ATTRIQA XTREME-UP 언어 간 QA(인도식) XTREME-UP 언어 간 QA(전체)ߨв 방법 O(100) Acc/AUC-PR O(1000) O(10000) FFL ICL 78.6/—† 68.69. 미세 조정 90.5/52.63.75.XS L FIAT 94.0/78.73.77. 미세 조정 SL FIAT 90.6/54.93.9/77.67.77.77.79. 최상의 기준선에 대한 이득 +3.5/+26.0(S 미세 조정 대비) +8.4(ICL 대비) +1.5(S 미세 조정 대비) 표 2: 전반적인 결과 FIAT 및 일반적인 기준선. 최상의 기준선과 관련하여 개선 사항을 제공하는 한편, 최상의 기준선은 특히 더 작은 모델 크기에서 ICL과 미세 조정 사이에서 종종 다르다는 점도 지적합니다. 따라서 실무자는 경험적으로 최상의 조치 과정을 결정해야 합니다. 출력이 텍스트 전용이기 때문에 ICL의 경우 AUC-PR이 계산되지 않습니다. 중간 데이터. 이를 O(1000) 데이터 시나리오라고 합니다. 또한 27개 언어에 걸쳐 22,500개의 예가 있는 전체 XTREME-UP 교차 언어 QA 작업을 연구하는데, 여기서 5개의 고자원 언어는 각각 2,500개 이상의 예가 있습니다. 이를 O(10,000) 데이터 시나리오라고 합니다.⁹ 이러한 작업을 함께 수행하면 100개 정도의 작은 것부터 20,000개의 예를 과도하게 학습하는 것까지 세 가지 다른 데이터 크기 시나리오에서 방법을 테스트할 수 있습니다. 언어 및 데이터 세트 크기에 대한 자세한 내용은 App. A.1에서 확인할 수 있습니다. 모델 PaLM-2(Google 등, 2023)를 기본 모델로 사용하고 FLAN 혼합(Chung 등, 2022)을 사용하여 명령어 조정 모델을 실험합니다.PaLM-2 L을 MB로 사용하고 PaLM-2 XS 및 S를 Mr. 기준선 ICL과 미세 조정 기준선과 비교합니다.ICL의 경우 chainof-thought 추론을 사용하는 PaLM-2 L을 사용합니다(Wei 등, 2022b).XOR-ATTRIQA 귀속 작업에서 5개 언어 각각에 대해 영어로 손으로 쓴 chainofthought 설명이 있는 4개의 few-shot 예시를 포함합니다.총 20개의 예시에 대해 10개입니다.그러나 XTREME-UP 교차 언어 QA의 경우 27개 언어 각각에 대한 프롬프트를 수동으로 엔지니어링하는 것이 불가능했습니다. 따라서 벵골어 예시를 기반으로 4가지 사고의 사슬 설명을 직접 작성하고¹¹ 모든 20개 언어에 동일한 ICL 예시를 사용합니다.3.1 결과 우리는 표 2에서 세 가지 데이터 설정에 대한 기준선(ICL 및 미세 조정)과 FIAT 프레임워크의 성능을 제시합니다.간단성을 위해 각 데이터 세트의 모든 언어에 대한 평균 점수를 표시하고 각 언어에 대한 결과를 앱 A.2에 제공합니다.기준선을 살펴보면 PaLM-2 L 모델을 사용하는 few-shot ICL은 추가 모델 조정 없이도 상당히 경쟁력이 있지만 비교적 적은 양의 작업 데이터에서 미세 조정된 PaLM-2 S보다 여전히 뒤처집니다.그러나 최상의 기준선은 다양한 작업과 데이터 크기 설정에서 ICL과 미세 조정 PaLM-2 XS 간에 다르다는 것을 발견했습니다.ICL만 사용하거나 미세 조정하는 경우 이러한 불일치로 인해 경험적 비교 없이 최상의 조치 과정을 결정하기 어렵습니다. 반면, FIAT는 ICL과 미세 조정의 장점을 결합하여 최고의 성능을 제공합니다. 4. 절삭 및 분석 이 섹션에서는 FIAT 내의 개별 설계 결정의 효과를 연구하고 표 3에 결과를 제시하고 아래에서 결론을 도출합니다. 결국, 우리는 특정 디자인 &quot;우리는 XTREME-UP 벤치마크의 권장 사항에 따라 과소 표현된 언어에 대한 평균 결과를 보고합니다. 1º 수동 프롬프트 엔지니어링 동안 Google Translate를 사용하여 설명 주석을 도왔습니다. 11 예시에 벵골어 질문이 있지만 모델에 영어로 추론을 수행하도록 지시합니다. XOR-ATTRIQA XTREME-UP XTREME-UP 교차 언어 QA: Indices 교차 언어 QA: Fullߨв 방법 O(100) Acc/AUC-PR O(1000) O(10000) FFL Few-shot ICL 78.6/68.69.L FIAT 94.0/78.73.77.XS w/o CoT-augmentated tuning w/o Instruction-augmented tuning 94.0/ 80.70.76.93.5/72.69.76.L w/o 매개변수 효율적 튜닝 w/o 명령어 튜닝 기본 모델 FIAT 93.7/69.67.75.90.5 / 52.63.75.93.9/77.77.79.w/o CoT 증강 튜닝 94.7/80.76.79.S w/o 명령어 증강 튜닝 94.1/71.75.79.w/o 매개변수 효율적 튜닝 w/o 명령어 튜닝 기본 모델 94.7/76.90.6/54.72.78.67.77.표 3: FIAT 레시피 내 각 수정의 기여도를 보여주는 소거. 각 제거는 위의 제거와 누적됩니다. 각 수정은 적어도 하나의 시나리오에 상당한 긍정적 영향을 미치는 경향이 있음을 관찰했습니다. 각 블록의 하단 라인은 기존의 미세 조정과 동일합니다. 선택 사항은 일부 설정에 다른 설정보다 더 큰 영향을 미치는 경향이 있으며, 각각은 일부 영역에서 상당한 기여를 하는 경향이 있으며, 전체적으로 모델링 레시피는 전체적으로 매우 효과적입니다. 지시 조정된 기본 모델은 미세 조정된 모델의 최종 품질을 개선합니다. 지시 조정된 Flan XS 모델은 모든 데이터 세트에서 기본 모델보다 개선되며, 특히 XOR-ATTRIQA 및 XTREME-UP Cross-lingual QA Indic에서 총 작업 데이터 양이 O(100)에서 O(1000) 정도입니다. 이는 지시 조정된 모델이 ICL에만 도움이 되는 것이 아니라 제한된 데이터에 대한 미세 조정에도 도움이 될 수 있음을 나타냅니다(Longpre et al., 2023). 그러나 XTREME-UP Cross-lingual QA에서 instruction-tuned 모델의 이점은 Indic(O(1000)개의 학습 예제)에서 Full(O(10000)개의 학습 예제)로 감소하여 instruction-tuned 모델이 fine-tuning 데이터 세트가 클 때 덜 도움이 됨을 나타냅니다. Instruction-augmented Tuning은 일반적으로 상당한 개선으로 이어집니다. 작업 데이터에 적절한 prompted 형식을 추가하면 일반적으로 모든 작업에 유익합니다. 이 결과는 prompt engineering이 direct few-shot ICL에 도움이 될 뿐만 아니라 모델 fine-tuning에도 긍정적인 영향을 미친다는 것을 나타냅니다. prompted tuning은 작업 데이터의 양이 매우 제한적인 XOR-ATTRIQA 및 XTREME-UP Crosslingual QA Indic에 특히 유용합니다. 이는 prompt 형식이 다운스트림 작업의 분포를 모델 사전 학습 분포에 더 가깝게 정렬하여 사전 학습된 모델이 적은 양의 작업 예제로 다운스트림 작업으로 일반화할 수 있기 때문입니다. CoT-augmentated Tuning은 대부분의 작업에 유용합니다. CoT 증강 튜닝은 Xtreme-UP 언어 간 QA 인도어 작업에 큰 개선을 가져올 수 있습니다. 놀랍게도 XORATTRIQA에는 도움이 되지 않는데, 이는 설명이 분류 작업에 특히 도움이 될 수 있다는 것을 보여주는 이전 연구의 결과와 모순됩니다(Hsieh et al., 2023; Zhou et al., 2023). 이는 모델이 설명에 액세스하지 않고도 XOR-ATTRIQA에서 이미 상당히 좋은 성능을 보이고(정확도 90% 이상) 이 작업이 포화점에 도달했을 수 있기 때문이라고 가정합니다. CoT 증강 튜닝은 성능이 낮은 작업과 언어에 더욱 유용합니다. CoT 증강 튜닝이 XTREMEUP 언어 간 QA 작업에 가져온 이득 간의 관계를 분석합니다. 그림 3은 CoT 증강 튜닝이 없는 기준 모델의 F1 점수 대비 다양한 언어의 F1 점수 개선을 보여줍니다. CoT 증강 튜닝의 이점과 기준 모델 점수 사이에 역상관 관계가 있음을 알 수 있는데, 이는 CoT가 CoT 증강 없이는 모델이 좋은 성능을 낼 수 없는 어려운 작업이나 언어에 더 이롭다는 것을 나타냅니다. 즉, 전체적으로 의미 있는 이득을 볼 수 있지만 개별 언어(또는 보다 일반적으로 개별 작업 및 사용 사례)의 경우 CoT가 품질에 엄청난 영향을 미칠 수 있음을 의미합니다. FA F1 점수와 CoT 10.07.5.2.0.F1 점수 그림 3: CoT 증강 튜닝을 사용한 XTREME-UP Crosslingual QA에서 F1의 이득. 성능이 낮은 언어는 CoT 증강에서 더 많은 이점을 얻는 경향이 있습니다. 방법 FGains Baseline 70.Distilled CoT(Hsieh et al., 2023) CoT 증강 튜닝 72.+1.73.+2.그림 4: CoT가 없는 기준과 비교한 XTREME-UP 교차 언어 QA 지표의 성능. CoT 증강 튜닝 방법은 CoT 증류에서 이전 방법보다 상당히 우수한 성과를 보입니다. 기준에 대한 70-BasePromptedStepGain 1.1.XOR QA 지표 귀속 기준 FLAN 이득 기준 0.5Base FLAN 0.FAUC-PR 그림 5: Instruction-augmented Tuning이 있는 방법과 없는 방법에 대한 XTREME-UP 교차 언어 QA에 대한 학습 전체에 걸친 검증 F1 점수. Instruction-augmented Tuning은 기준보다 성능이 뛰어나며 모델 최적화 전 단계 0에서 성능이 훨씬 더 좋습니다. 그림 6: instruction-tuning이 있는 모델과 없는 모델에 대한 Instruction-augmented Tuning으로 인한 개선. Instructionaugmented Tuning은 일반적으로 두 유형의 모델에 모두 도움이 되며 instruction-tuned 모델에 더 유익한 경향이 있습니다.CoT-augmented Tuning은 CoT 증류보다 더 나은 품질을 제공합니다.최근 연구에서는 설명을 멀티태스크 출력 대상으로 사용하는 증류 CoT를 제안하여 모델이 테스트 시간에 추가 설명을 생성할 필요가 없도록 했습니다(Hsieh et al., 2023).여기서 CoT 설명을 사용하는 이 두 가지 다른 방법의 성능을 비교하고 그림 4에 교차 언어 QA 작업에 대한 성능을 나열합니다.더 높은 추론 비용이 발생함에도 불구하고 CoT 증강 방법은 더 어려운 XTREME-UP 교차 언어 QA 인도어 작업에서 증류 CoT보다 훨씬 더 우수한 성능을 보입니다.일반적으로 증류는 품질보다 효율성을 목표로 하는 FIAT에 대한 직교 기술로 봅니다.튜닝에 지침을 추가하면 처음부터 끝까지 도움이 됩니다. 그림 5에서 명령어 증강 튜닝을 적용한 경우와 적용하지 않은 경우의 Flan PaLM-2 S 모델의 학습 곡선을 표시합니다. 튜닝에 명령어를 추가하면 모델 최적화 전인 단계 0에서 성능이 훨씬 향상되는 것을 볼 수 있습니다. 이는 미세 튜닝 12 중에 작업 데이터에 명령어를 추가하면 모델의 제로샷 성능이 크게 향상될 수 있음을 나타내며, 아마도 작업을 12 더 잘 만들기 때문일 것입니다. 명령어 증강 튜닝이라는 용어를 사용하여 나중에 특정 작업의 명령어를 더 잘 따를 수 있는 기본 모델을 만드는 명령어 튜닝 기본 모델과 명령어 튜닝 단계에서 사용된 데이터와 더 유사한 소프트 프롬프트 임베딩을 학습하는 프롬프트 튜닝이라는 별도의 개념과 구별합니다. 중요한 점은 모델 매개변수가 동일한 수준의 품질을 달성하기 위해 시작점에서 멀리 이동할 필요가 없다는 것을 의미하므로 치명적인 망각의 위험이 줄어듭니다. 그러나 모델은 더 적은 단계로 동일한 수준의 품질에 도달할 뿐만 아니라 명령어가 없는 모델의 품질을 능가합니다. 명령어 증강 튜닝은 명령어 튜닝 기반 모델에 더 많은 도움이 됩니다. 우리는 명령어 튜닝이 있는 모델과 없는 모델에서 프롬프트 튜닝의 효과를 비교합니다. 그림 6은 프롬프트 튜닝이 명령어 튜닝이 없는 기본 모델과 명령어 튜닝이 있는 플랜 모델 모두에 일반적으로 개선을 가져오는 반면, 명령어 튜닝 플랜 모델의 이득은 약간 더 크고 더 일관되는 경향이 있음을 보여줍니다. 이는 프롬프트 튜닝에 사용한 데이터 형식(작업 명령어 다음에 입력)이 명령어 튜닝에 사용된 플랜 데이터 혼합과 더 유사하기 때문일 가능성이 큽니다. 5
--- RELATED WORK ---
지시 튜닝 지시 튜닝 모델(Wei et al., 2021; Longpre et al., 2023)은 다양한 작업 세트에 대해 미세 조정되어 이미 지시를 따르도록 준비되었기 때문에 기본 언어 모델보다 few-shot ICL 작업에서 더 나은 성능을 보이는 경우가 많습니다. 지시 튜닝 모델을 사용하는 것은 FIAT의 핵심 구성 요소입니다. 맥락 내 학습 맥락 내 학습에서 LLM의 매개변수는 고정된 상태로 유지되고 추론 단계와 함께 몇 가지 예가 포함된 프롬프트를 사용하여 유사한 작업을 해결하도록 모델을 준비합니다(Nye et al., 2021; Wei et al., 2022b). 맥락 내 학습은 대규모 언어 모델에 가장 적합합니다. FIAT는 미세 조정과 함께 대규모 언어 모델의 이러한 기능을 사용하여 저데이터 체제에서 소규모 언어 모델을 구동합니다. 대규모 LLM에서 소규모 LLM으로의 지식 전달 인기 있는 사전
--- METHOD ---
s는 매개변수 효율적 튜닝을 통해 적당한 크기의 LLM에서 매개변수 업데이트를 수행합니다. 다양한 다국어 작업³에서 FIAT의 효과를 평가하고 FIAT가 100~10,000개의 학습 예제 범위에서 ICL과 미세 조정보다 더 나은 성능을 보이는 것을 관찰합니다. FIAT가 학습 패러다임 사이에서 어려운 선택을 할 필요 없이 LLM의 모든 잠재력을 활용하는 실용적인 방법을 제공하기를 바랍니다. 서론 대규모 언어 모델(LLM)은 새로운 작업과 언어에 대해 인상적인 일반화 능력을 보여줍니다. 문제를 해결하기 위한 논리적 추론을 생성하는 것과 같은 가장 흥미로운 기능 중 일부는 모델 크기가 특정 임계값, 종종 수천억 개의 매개변수를 초과할 때만 나타나는 것으로 밝혀졌습니다(Wei et al., 2022b;a). 이러한 모델이 작업별 튜닝 없이도 고품질 응답을 생성할 수 있는 인상적인 기능과 이러한 모델을 추가로 튜닝하는 데 드는 매우 높은 비용으로 인해 최근 많은 작업이 In-Context Learning(ICL) 패러다임에 초점을 맞추게 되었습니다. 즉, 몇 가지 작업별 예와 지침을 모델의 입력에 배치하는 것입니다(Brown et al., 2020; Chowdhery et al., 2022; Google et al., 2023; OpenAI, 2023). 이전 작업에서는 작업 데이터에 따라 모델을 미세 조정하면 ICL에 비해 다운스트림 작업에서 더 우수한 성능을 낼 수 있는 경우가 많았지만(Scao &amp; Rush, 2021; Schick &amp; Schütze, 2020a;b; Asai et al., 2023), 데이터가 제한된 작업에 대한 모델을 미세 조정하려는 최근의 노력은 상당히 적습니다. 아마도 매우 큰 모델을 튜닝하는 데 드는 시간과 컴퓨팅 비용으로 인해 실무자들이 더 작은 모델을 선호하고 새로운 모델 기능을 활용할 수 있는 능력을 포기했기 때문일 것입니다. ICL과 모델 미세 조정은 각각 고유한 상충 관계를 가지고 있습니다. ICL은 학습 비용이 발생하지 않으며 가장 유능한 LLM을 활용할 수 있습니다(Schick &amp; Schütze, 2020b; OpenAI, 2023). 그러나 ICL은 소수의 주석이 달린 예제로 많은 작업에서 경쟁력 있는 성능을 달성할 수 있지만, 잘 작동하려면 종종 매우 큰 모델이 필요하고 컨텍스트 창에 맞지 않으면 추가 학습 예제를 활용할 수 없습니다. 많은 작업에서 이는 잠재적으로 유용한 학습 예제를 상당수 무시하는 결과를 초래합니다. 반면, 미세 조정은 학습 예제를 모델의 입력에 맞춰야 할 필요성에 의해 제약받지 않으며, 매우 효과적일 수 있습니다. &quot;FIAT라는 이름은 Fusing Learning Paradigms with Instruction Accelerated Tuning에서 따왔습니다. 2 FIAT는 학습 패러다임뿐만 아니라 모델 자체도 융합합니다. 3 이러한 작업은 추가 데이터가 없기 때문에 자연스럽게 데이터가 적다고 하며, 더 많은 데이터를 얻는 것도 쉽지 않습니다. 이를 대규모 데이터가 있지만 무시되는 인위적으로 데이터가 적은 시나리오와 대조합니다. 인간이 프롬프트 엔지니어링을 통해 업데이트한 명령어 조정 LLM X 입력 시퀀스 몇 가지 샷 컨텍스트 내 학습 명령어 조정 LLM CoT 이유 IB x θρ 입력 시퀀스 인간이 프롬프트 엔지니어링을 통해 업데이트한 yx 작업 데이터에 대한 경사 하강을 통해 업데이트한 LLM 전체 미세 조정 작은 매개변수가 작업 데이터에 대한 경사 하강을 통해 업데이트한 명령어 조정 LLM ŷB X PEFT 0. y 입력 시퀀스 y FIAT는 두 패러다임을 모두 사용 그림 1: FIAT의 전반적인 흐름과 ICL 및 미세 조정과 비교. 색상이 지정된 구성 요소는 FIAT의 작업별 인스턴스를 빌드하고 학습하는 동안 업데이트되는 반면, 다른 구성 요소는 고정됩니다.0B는 더 큰 LLM의 매개변수이고 I ẞ는 추론을 유도하는 데 사용되는 지침입니다.+는 조정할 중간 크기의 LLM의 매개변수이고 I는 모델이 올바른 최종 답을 예측하는 데 도움이 되는 지침입니다. 더 작은 언어 모델에서도 마찬가지입니다. 이러한 상충은 실무자가 임의로 패러다임을 선택하거나 비용이 많이 드는
--- EXPERIMENT ---
영어: 최상의 접근 방식을 선택하기 위해 이러한 이질적인 방법에 대한 s를 사용합니다. 대신 우리는 이 두 모델 학습 패러다임이 실제로 상호 보완적이라는 견해를 취합니다. 이를 위해 우리는 FIAT-학습 패러다임과 지침 가속 튜닝(FIAT)을 융합하는 것을 제안합니다. 이는 매우 큰 모델에서 ICL을 활용하고 중간 크기의 LLM에서 매개변수 튜닝을 활용하면서 각 패러다임과 관련된 공통 기술을 융합합니다. FIAT는 매우 큰 모델에서 사고의 사슬 추론을 이끌어내는 수작업 엔지니어링 지침 프롬프트를 사용하는 동시에 생성된 추론 및 지침 프롬프트를 사용하여 매개변수 효율적인 튜닝으로 중간 크기의 LLM을 튜닝합니다. 그림 1은 FIAT의 워크플로와 ICL 및 미세 조정과 비교한 방식을 보여줍니다. 이 기사의 나머지 부분에서는 각 패러다임 내에서 개발된 다양한 기술과 함께 ICL과 미세 조정 간의 연결을 공식적으로 설명합니다(§2). 우리는 이들 중 가장 좋은 것을 함께 융합하고 각 개인의 많은 함정을 피하는 FIAT를 제안한다(§2.3); 우리는 100-10,000개 예제의 데이터 시나리오에서 FIAT가 두 학습 패러다임보다 어떻게 개선되는지 보여주는 실험을 제시하고 이러한 이득이 어디에서 나오는지 자세히 설명하는 절제를 제시한다(§3).LLMS를 위한 학습 패러다임 이 섹션에서는 LLM을 위한 두 가지 인기 있는 학습 패러다임(§2.1의 ICL 및 §2.2의 매개변수 튜닝)을 검토하면서 FIAT로 직접 이어지는 강점과 약점을 고려한다(§2.3).2. 맥락 내 학습 지시된 ICL은 LLM의 매개변수를 고정된 상태로 유지하지만 대신 다운스트림 작업의 정확도를 개선하기 위해 명령 프롬프트(종종 수동 최적화를 통해)를 선택합니다. 형식적으로, 모델 예측은 고정된 0과 텍스트 명령 I로 매개변수화된 매우 큰 사전 학습된 LLM을 샘플링하여 수행됩니다.P(y|x; 0, 1) (1) * 일반적으로 샘플링은 온도가 0인 간단한 argmax이지만 다수결 투표와 같은 기술에서와 같이 항상 그런 것은 아닙니다.명령 I은 실제로 모델 입력 x에 접두사로 붙지만, 우리는 의도적으로 이를 모델의 인수로 표시하여 이것이 개념화된 방식을 더 잘 반영한다고 주장합니다.나중에 이를 기반으로 구축할 것입니다.사고의 사슬 추론은 I를 제작하여 모델의 출력에서 단계별 추론을 유도하여 모델이 올바른 예측에 도달하는 능력을 향상시킴으로써 지시된 ICL을 한 단계 더 발전시킵니다(Wei et al., 2022b).이를 통해 자기 회귀 추론을 통해 입력에 대한 관찰을 출력하거나 최종 답을 예측할 때 향후 디코딩 단계에서 활용할 수 있는 전체 작업의 하위 문제를 해결할 수 있습니다. 또한 사전 학습 중에 모델이 본 텍스트 패턴을 이끌어낼 수도 있는데, 이는 그렇지 않으면 모델의 잠재 특징 공간에서 접근하기 어려울 것입니다(예: 미세 조정을 통해).소수 샷 ICL 소수 샷 ICL은 지시 I가 지시를 통해 모델에 텍스트 입력으로 포맷된 학습 예제 D 중에서 선택한 소수의 예제로 구성된다는 점에서 지시 ICL과 다릅니다.지시 조정 기반 모델 FLAN 및 TO(Sanh et al., 2021; Chung et al., 2022; Longpre et al., 2023)와 같은 지시 조정 모델은 종종 사전 학습된 모델을 사용하는 것에 비해 ICL에서 상당한 개선을 제공합니다.이는 지시 조정이 본질적으로 분포가 다운스트림 작업에 더 가까운 멀티태스크 데이터 세트를 사용하는 2단계 사전 학습이기 때문입니다.ICL 패러다임은 주석이 달린 예제가 없거나 소수에 불과한 다양한 작업에서 경쟁력 있는 결과를 얻습니다. ICL은 추가적인 모델 튜닝 비용이 발생하지 않지만, 특히 chain-of-thought와 같은 기술을 사용할 때 잘 작동하려면 특정 크기 이상의 LLM이 필요하기 때문에 추론 비용이 높은 경우가 많습니다.또한 모델의 컨텍스트 창에 맞는 것 이상의 추가 작업 데이터를 활용할 수 없습니다.2.2 매개변수 튜닝 전체 매개변수 미세 조정 튜닝할 LLM의 사전 훈련된 매개변수 0이 주어지면, 표준 미세 조정은 다음에 따라 작업별 감독 학습 데이터 D에서 모델의 모든 매개변수를 간단히 최적화합니다.P(y|x; 0) (2) 0의 최적화는 ICL에서 I의 인간 프롬프트 엔지니어링 프로세스와 목적이 비슷합니다.모델 미세 조정은 학습 데이터를 모델의 컨텍스트 창에 맞출 필요가 없으므로 사용 가능한 학습 예제가 약간 더 많을 때 더 효과적입니다.미세 조정은 충분한 학습 예제가 있는 더 작은 언어 모델에서도 잘 작동하여 추론 속도가 빨라집니다. 그러나 미세 조정은 추가적인 학습 비용이 발생하고 모델 매개변수에 대한 액세스가 필요한 반면, 가장 유능한 LLM 중 일부는 추론 전용 API 액세스에 사용할 수 있습니다. 모델은 특히 데이터가 제한된 작업의 경우 재앙적 망각(Goodfellow et al., 2013)으로 인해 학습 예제에 쉽게 과적합될 수 있습니다. 매개변수 효율적 미세 조정(PEFT)은 |0 PEFT | &lt;&lt;&lt; |0|인 학습 매개변수화 PEFT를 사용하여 조정 절차를 개선합니다. 과적합의 위험을 줄이는 것 외에도 이 학습 기술은 학습 세트를 넘어 일반화에 유용할 수 있는 기능을 잊지 않도록 합니다. 마찬가지로 ICL은 매개변수를 고정한 채로 모델의 입력만 수정하여 재앙적 망각을 방지합니다. 2.3 FIAT를 사용한 학습 패러다임 융합 이 섹션에서는 모델링 기능 측면에서 각 설계 선택의 목적을 설명하면서 FIAT를 구성합니다. ICL과 미세 조정은 각각 매력적인 강점과 함정을 가지고 있으며, 이를 표 1에 요약했습니다. 높은 수준에서 이러한 속성은 대체로 상호 보완적임을 관찰했습니다. 5 실제로 |0|은 ICL보다 미세 조정에서 훨씬 더 작은 경향이 있습니다.ICL 미세 조정 강점 작은 모델에서 잘 작동 아니요 예 대량의 학습 데이터 지원 아니요 예 사고의 사슬 추론 지원 예 아니요 지시 프롬프트 사용 예 아니요 과제 매개변수 업데이트 없음 예 치명적인 망각 방지 예 ZZ 아니요 아니요 표 1: 일반적인 사용 패턴에 따른 ICL 및 미세 조정 학습 패러다임 비교.• ICL과 미세 조정의 이러한 능력을 반영하여 다음을 수행할 수 있는 접근 방식을 모색합니다.• • 지시 따르기: 인간이 설계한 지침을 따라 고품질 예측을 달성합니다.사고의 사슬 추론: 모델이 올바른 예측을 향해 나아가는 데 도움이 되는 중간 텍스트를 생성합니다.• 매개변수 조정: 중간에서 많은 수의 지도 학습 예제에 맞게 내부 표현을 정제합니다. 그리고 • 데이터 스케일링: 100개에서 1000개의 예제에 이르는 데이터 스케일로 고품질 모델을 제공합니다. CoT 증강 튜닝을 통한 모델 스태킹 우리는 사고의 사슬 프롬프트가 일반적으로 감독되지 않고 신중하게 작성된 지침을 통해 유도된다는 관찰에서 시작합니다. 이를 동기로 학습 및 추론을 위한 두 가지 모델을 융합합니다. LLM의 가장 강력한 새로운 역량을 모두 갖춘 큰 모델 ẞ과 관심 있는 작업의 용량 요구 사항에 따라 크기를 유연하게 선택할 수 있는 조정 가능한 모델 7입니다. 사고의 사슬 추론의 책임을 ẞ에 할당한 다음 텍스트 예측 ŷß을 조정 가능한 모델에 제공합니다. 그런 다음 이러한 입력(예: 사고의 사슬 설명)을 감독된 출력을 예측하는 데 얼마나 유용한지에 따라 가장 잘 사용하는 방법을 학습할 수 있습니다. 매개 변수 0ẞ는 하위 작업에 대한 직접 감독 데이터가 없고 필요하지 않기 때문에 고정된 상태로 유지됩니다. 명령어 증강 튜닝 우수한 명령어 프롬프트를 만드는 것은 고품질 ICL 성능에 필수적인 것으로 알려져 있으므로, 추론과 설명을 생성하는 명령어 Iẞ를 첫 번째 단계로 자연스럽게 포함합니다. 명령어는 일반적으로 소규모 튜닝 가능 모델 IT에 사용되지 않지만, 명령어가 튜닝에도 도움이 될 수 있는 잠재력이 있음을 관찰했습니다. 명령어가 작업의 입력을 사전 학습 중에 보이는 분포와 더 잘 일치시켜 모델이 더 빠르게 수렴할 수 있을 뿐만 아니라 매개변수 업데이트도 줄일 수 있다고 추측합니다. 이를 통해 과도한 매개변수 업데이트와 관련된 치명적인 망각의 위험을 피할 수 있습니다. 따라서 FIAT는 튜닝 가능 모델에 대해 별도의 명령어 I도 제공합니다. 퍼베이시브 명령어 튜닝 모델 이미 명령어 튜닝 모델은 ICL의 표준이 되었습니다. 우리는 모든 실험에서 0ẞ와 같은 모델을 사용합니다. 그러나 FIAT의 Instructionaugmented Tuning 사용을 감안할 때, 우리는 주로 span corruption 목표에 대해 사전 학습된 모델에서 시작하는 미세 조정의 일반적인 관행에서 벗어나 대신 instruction-tuned checkpoint로 초기화합니다(Longpre et al., 2023). 이렇게 하면 모델이 이미 명령을 예상하고 있기 때문에 최적화가 더 쉬워집니다. 이는 제한된 학습 데이터 시나리오에서 특히 유용할 수 있습니다. 매개변수 효율적 튜닝 지금까지 우리는 FIAT의 설계에 사고의 사슬 추론, 튜닝에서의 명령 따르기, 명령 조정 초기화를 추가했으며, 이는 모두 원하는 출력의 확률을 높이는 측면에서 사전 튜닝 모델과 작업 정의를 서로에게 가깝게 움직입니다. 우리는 매개변수 효율적 튜닝이 FIAT에서 훈련 데이터에 대한 최적화에 특히 적합하다고 가정합니다. 모델 매개변수 0에 대한 큰 변경은 &quot;FIAT에서 명령어는 이전의 통계적 학습 방법에서 베이지안 사전과 유사한 목적을 제공하는 것으로 볼 수 있습니다. 이를 통해 매개변수를 경험적으로 추정하는 감독 데이터와 함께 학습 절차에 인간 지식을 인코딩할 수 있습니다. 그러나 텍스트 명령어는 Dirichlet의 하이퍼 매개변수보다 훨씬 더 자연스러운 방법입니다. 알고리즘 1: FIAT를 사용한 모델 구축 입력: 0, 0, D 출력: 0, Iẞ, IT // 추론 명령어 작성 및 예시 선택. IB = PROMPTENGINEERING (D, 0B) // 큰 모델을 기반으로 튜닝 명령어 작성. IT PROMPTENGINEERING (D, 0B) // 매개변수 효율적 튜닝 초기화. OPEFT T ← INIT(07) // 예제 또는 데이터 배치 반복. for x, y ED do end // 확장, 설명, 추론을 생성합니다.Ув = arg max, P(y|x; 0ß, Iß) // 매개변수 효율적 업데이트를 사용하여 최적화합니다.97 = ▼ PEFTP(y|x, ŷß; 07, 0PEFT, IT) OPEFT ← UPDATE (OPEFT, gr) // 최종 조정된 모델에 PEFT 업데이트를 적용합니다.04 — 07 OPEFT 알고리즘 2: FIAT를 사용한 추론 입력: x, I, I, 0ß, 출력: y // 확장, 설명, 추론을 생성합니다.= arg max, P(y|x; ¤ß, Iß) B₂ // 조정된 모델을 사용하여 최종 출력을 추론합니다.y = arg max, P(y|x, ŷß; ¤, I+) 그림 2: FIAT를 사용한 모델 구축 및 추론 왼쪽: FIAT를 사용한 모델 구축은 지침 I의 대화형 프롬프트 엔지니어링으로 시작됩니다. Iẞ는 대규모 주석이 없는 08-ie 동작에 대해 few-shot exemplar를 사용하여 추론을 수행하는 방법을 지정하는 반면, I는 생성된 추론 및 입력을 사용하여 최종 출력을 생성하기 위한 조정된 모델 0에 대한 지침을 지정합니다. 03과 0은 모두 지침 조정된 모델이며 매개변수 효율적 조정을 통해 훈련하는 동안 ¤만 업데이트됩니다. 오른쪽: FIAT를 사용한 추론은 매우 간단하며 다음만 필요합니다. (1) 고정된 사전 훈련된 매개변수 0ẞ 및 추론 지침 IB를 사용하여 대규모 생성 모델에 대한 호출, (2) 적절한 초기화가 주어진 경우 필요한 연관된 작업 지침 I와 함께 조정된 모델 0에 대한 호출.7 위의 모든 수정 사항을 공식화하면 알고리즘 1 및 알고리즘에서 미세 조정 및 추론에 사용되는 FIAT의 최종 공식에 도달합니다. 2. 실험 데이터 세트 다양한 훈련 데이터 크기를 자연스럽게 포괄하는 데이터 세트를 선택하는 주요 목표 중 하나입니다. 분류에서 모델의 짧은 답변 생성 능력 연습에 이르는 다양한 작업을 고려하며, 방법의 일반성을 평가하기 위해 다양한 언어를 포함합니다. 먼저, XOR-ATTRIQA(Muller et al., 2023)를 사용합니다. 이는 모델이 질문에 대한 제공된 답변이 주어진 구절 맥락에서 지원되는지 여부를 예측하도록 요청하는 분류 작업으로, 총 262개의 예가 있는 5개 언어를 포함합니다. 이를 O(100) 데이터 시나리오라고 합니다. 또한 XTREME-UP(Ruder et al., 2023)의 언어 간 QA 작업에서 FIAT의 동작을 연구합니다. 이 데이터는 TyDi QA(Clark et al., 2020) 데이터 세트의 언어 간 변형인 XOR QA³ 데이터 세트(Asai et al., 2020)를 확장한 것입니다. 이 과제는 영어가 아닌 질문과 영어 답변 구절이 주어졌을 때 올바른 영어 답변 범위를 예측하는 모델을 요구합니다. 이 과제에는 구절에 올바른 답변이 포함되지 않을 가능성도 포함되어 있어 더 어렵습니다. 언어 간 QA는 답변 내용이 거의 없는 언어에 특히 중요한 과제로, 언어 내 내용만 사용하여 답변이 불가능할 질문에 대한 답변을 제공할 수 있기 때문입니다. 두 가지 초점 세트에 대한 결과를 제공합니다. 첫째, 각 언어에 약 300개의 예가 있는 XTREME-UP 언어 간 QA에서 20개 인도어 하위 집합을 사용하여 7개의 시나리오를 연구할 수 있습니다. FIAT에서는 추가적인 추론 비용을 유발하지 않기 때문에 LoRA(Hu et al., 2021)를 사용하여 튜닝 절차를 매개변수화합니다. 향후 작업에서는 소프트 프롬프트 튜닝(Lester et al., 2021)과 같은 다른 방법을 고려해야 합니다. 8XOR QA는 언어 간 개방형 검색 질문 답변을 의미합니다. XOR QA와 XOR-ATTRIQA의 차이점을 주목하세요.XOR-ATTRIQA XTREME-UP 언어 간 QA(인도식) XTREME-UP 언어 간 QA(전체)ߨв 방법 O(100) Acc/AUC-PR O(1000) O(10000) FFL ICL 78.6/—† 68.69. 미세 조정 90.5/52.63.75.XS L FIAT 94.0/78.73.77. 미세 조정 SL FIAT 90.6/54.93.9/77.67.77.77.79. 최상의 기준선에 대한 이득 +3.5/+26.0(S 미세 조정 대비) +8.4(ICL 대비) +1.5(S 미세 조정 대비) 표 2: 전반적인 결과 FIAT 및 일반적인 기준선. 최상의 기준선과 관련하여 개선 사항을 제공하는 한편, 최상의 기준선은 특히 더 작은 모델 크기에서 ICL과 미세 조정 사이에서 종종 다르다는 점도 지적합니다. 따라서 실무자는 경험적으로 최상의 조치 과정을 결정해야 합니다. 출력이 텍스트 전용이기 때문에 ICL의 경우 AUC-PR이 계산되지 않습니다. 중간 데이터. 이를 O(1000) 데이터 시나리오라고 합니다. 또한 27개 언어에 걸쳐 22,500개의 예가 있는 전체 XTREME-UP 교차 언어 QA 작업을 연구하는데, 여기서 5개의 고자원 언어는 각각 2,500개 이상의 예가 있습니다. 이를 O(10,000) 데이터 시나리오라고 합니다.⁹ 이러한 작업을 함께 수행하면 100개 정도의 작은 것부터 20,000개의 예를 과도하게 학습하는 것까지 세 가지 다른 데이터 크기 시나리오에서 방법을 테스트할 수 있습니다. 언어 및 데이터 세트 크기에 대한 자세한 내용은 App. A.1에서 확인할 수 있습니다. 모델 PaLM-2(Google 등, 2023)를 기본 모델로 사용하고 FLAN 혼합(Chung 등, 2022)을 사용하여 명령어 조정 모델을 실험합니다.PaLM-2 L을 MB로 사용하고 PaLM-2 XS 및 S를 Mr. 기준선 ICL과 미세 조정 기준선과 비교합니다.ICL의 경우 chainof-thought 추론을 사용하는 PaLM-2 L을 사용합니다(Wei 등, 2022b).XOR-ATTRIQA 귀속 작업에서 5개 언어 각각에 대해 영어로 손으로 쓴 chainofthought 설명이 있는 4개의 few-shot 예시를 포함합니다.총 20개의 예시에 대해 10개입니다.그러나 XTREME-UP 교차 언어 QA의 경우 27개 언어 각각에 대한 프롬프트를 수동으로 엔지니어링하는 것이 불가능했습니다. 따라서 벵골어 예시를 기반으로 4가지 사고의 사슬 설명을 직접 작성하고¹¹ 모든 20개 언어에 동일한 ICL 예시를 사용합니다.3.1 결과 우리는 표 2에서 세 가지 데이터 설정에 대한 기준선(ICL 및 미세 조정)과 FIAT 프레임워크의 성능을 제시합니다.간단성을 위해 각 데이터 세트의 모든 언어에 대한 평균 점수를 표시하고 각 언어에 대한 결과를 앱 A.2에 제공합니다.기준선을 살펴보면 PaLM-2 L 모델을 사용하는 few-shot ICL은 추가 모델 조정 없이도 상당히 경쟁력이 있지만 비교적 적은 양의 작업 데이터에서 미세 조정된 PaLM-2 S보다 여전히 뒤처집니다.그러나 최상의 기준선은 다양한 작업과 데이터 크기 설정에서 ICL과 미세 조정 PaLM-2 XS 간에 다르다는 것을 발견했습니다.ICL만 사용하거나 미세 조정하는 경우 이러한 불일치로 인해 경험적 비교 없이 최상의 조치 과정을 결정하기 어렵습니다. 반면, FIAT는 ICL과 미세 조정의 장점을 결합하여 최상의 성능을 제공합니다. 4 ABLATION 및 분석 이 섹션에서는 FIAT 내 개별 설계 결정의 효과를 연구하고 표 3에 결과를 제시하고 그림을 그립니다.
--- CONCLUSION ---
아래에서 그들로부터 s를 찾을 수 있습니다. 결국, 우리는 특정 디자인 &quot;우리는 XTREME-UP 벤치마크의 권장 사항에 따라 과소 표현된 언어에 대한 평균 결과를 보고합니다. 1º 수동 프롬프트 엔지니어링 동안, 우리는 설명 주석을 돕기 위해 Google Translate를 사용했습니다. 11 예시에 벵골어 질문이 있지만, 우리는 모델에 영어로 추론을 수행하도록 지시합니다. XOR-ATTRIQA XTREME-UP XTREME-UP 교차 언어 QA: Indices 교차 언어 QA: Fullߨв Method O(100) Acc/AUC-PR O(1000) O(10000) FFL Few-shot ICL 78.6/68.69.L FIAT 94.0/78.73.77.XS w/o CoT-augmentated tuning w/o Instruction-augmented tuning 94.0/ 80.70.76.93.5/72.69.76.L w/o 매개변수 효율적 튜닝 w/o 명령어 튜닝 기본 모델 FIAT 93.7/69.67.75.90.5 / 52.63.75.93.9/77.77.79.w/o CoT 증강 튜닝 94.7/80.76.79.S w/o 명령어 증강 튜닝 94.1/71.75.79.w/o 매개변수 효율적 튜닝 w/o 명령어 튜닝 기본 모델 94.7/76.90.6/54.72.78.67.77.표 3: FIAT 레시피 내 각 수정의 기여도를 보여주는 소거. 각 제거는 위의 제거와 누적됩니다. 각 수정은 적어도 하나의 시나리오에 상당한 긍정적 영향을 미치는 경향이 있음을 관찰했습니다. 각 블록의 하단 라인은 기존의 미세 조정과 동일합니다. 선택 사항은 일부 설정에 다른 설정보다 더 큰 영향을 미치는 경향이 있으며, 각각은 일부 영역에서 상당한 기여를 하는 경향이 있으며, 전체적으로 모델링 레시피는 전체적으로 매우 효과적입니다. 지시 조정된 기본 모델은 미세 조정된 모델의 최종 품질을 개선합니다. 지시 조정된 Flan XS 모델은 모든 데이터 세트에서 기본 모델보다 개선되며, 특히 XOR-ATTRIQA 및 XTREME-UP Cross-lingual QA Indic에서 총 작업 데이터 양이 O(100)에서 O(1000) 정도입니다. 이는 지시 조정된 모델이 ICL에만 도움이 되는 것이 아니라 제한된 데이터에 대한 미세 조정에도 도움이 될 수 있음을 나타냅니다(Longpre et al., 2023). 그러나 XTREME-UP Cross-lingual QA에서 instruction-tuned 모델의 이점은 Indic(O(1000)개의 학습 예제)에서 Full(O(10000)개의 학습 예제)로 감소하여 instruction-tuned 모델이 fine-tuning 데이터 세트가 클 때 덜 도움이 됨을 나타냅니다. Instruction-augmented Tuning은 일반적으로 상당한 개선으로 이어집니다. 작업 데이터에 적절한 prompted 형식을 추가하면 일반적으로 모든 작업에 유익합니다. 이 결과는 prompt engineering이 direct few-shot ICL에 도움이 될 뿐만 아니라 모델 fine-tuning에도 긍정적인 영향을 미친다는 것을 나타냅니다. prompted tuning은 작업 데이터의 양이 매우 제한적인 XOR-ATTRIQA 및 XTREME-UP Crosslingual QA Indic에 특히 유용합니다. 이는 prompt 형식이 다운스트림 작업의 분포를 모델 사전 학습 분포에 더 가깝게 정렬하여 사전 학습된 모델이 적은 양의 작업 예제로 다운스트림 작업으로 일반화할 수 있기 때문입니다. CoT-augmentated Tuning은 대부분의 작업에 유용합니다. CoT 증강 튜닝은 Xtreme-UP 언어 간 QA 인도어 작업에 큰 개선을 가져올 수 있습니다. 놀랍게도 XORATTRIQA에는 도움이 되지 않는데, 이는 설명이 분류 작업에 특히 도움이 될 수 있다는 것을 보여주는 이전 연구의 결과와 모순됩니다(Hsieh et al., 2023; Zhou et al., 2023). 이는 모델이 설명에 액세스하지 않고도 XOR-ATTRIQA에서 이미 상당히 좋은 성능을 보이고(정확도 90% 이상) 이 작업이 포화점에 도달했을 수 있기 때문이라고 가정합니다. CoT 증강 튜닝은 성능이 낮은 작업과 언어에 더욱 유용합니다. CoT 증강 튜닝이 XTREMEUP 언어 간 QA 작업에 가져온 이득 간의 관계를 분석합니다. 그림 3은 CoT 증강 튜닝이 없는 기준 모델의 F1 점수 대비 다양한 언어의 F1 점수 개선을 보여줍니다. CoT 증강 튜닝의 이점과 기준 모델 점수 사이에 역상관 관계가 있음을 알 수 있는데, 이는 CoT가 CoT 증강 없이는 모델이 좋은 성능을 낼 수 없는 어려운 작업이나 언어에 더 이롭다는 것을 나타냅니다. 즉, 전체적으로 의미 있는 이득을 볼 수 있지만 개별 언어(또는 보다 일반적으로 개별 작업 및 사용 사례)의 경우 CoT가 품질에 엄청난 영향을 미칠 수 있음을 의미합니다. FA F1 점수와 CoT 10.07.5.2.0.F1 점수 그림 3: CoT 증강 튜닝을 사용한 XTREME-UP Crosslingual QA에서 F1의 이득. 성능이 낮은 언어는 CoT 증강에서 더 많은 이점을 얻는 경향이 있습니다. 방법 FGains Baseline 70.Distilled CoT(Hsieh et al., 2023) CoT 증강 튜닝 72.+1.73.+2.그림 4: CoT가 없는 기준과 비교한 XTREME-UP 교차 언어 QA 지표의 성능. CoT 증강 튜닝 방법은 CoT 증류에서 이전 방법보다 상당히 우수한 성과를 보입니다. 기준에 대한 70-BasePromptedStepGain 1.1.XOR QA 지표 귀속 기준 FLAN 이득 기준 0.5Base FLAN 0.FAUC-PR 그림 5: Instruction-augmented Tuning이 있는 방법과 없는 방법에 대한 XTREME-UP 교차 언어 QA에 대한 학습 전체에 걸친 검증 F1 점수. Instruction-augmented Tuning은 기준보다 성능이 뛰어나며 모델 최적화 전 단계 0에서 성능이 훨씬 더 좋습니다. 그림 6: instruction-tuning이 있는 모델과 없는 모델에 대한 Instruction-augmented Tuning으로 인한 개선. Instructionaugmented Tuning은 일반적으로 두 유형의 모델에 모두 도움이 되며 instruction-tuned 모델에 더 유익한 경향이 있습니다.CoT-augmented Tuning은 CoT 증류보다 더 나은 품질을 제공합니다.최근 연구에서는 설명을 멀티태스크 출력 대상으로 사용하는 증류 CoT를 제안하여 모델이 테스트 시간에 추가 설명을 생성할 필요가 없도록 했습니다(Hsieh et al., 2023).여기서 CoT 설명을 사용하는 이 두 가지 다른 방법의 성능을 비교하고 그림 4에 교차 언어 QA 작업에 대한 성능을 나열합니다.더 높은 추론 비용이 발생함에도 불구하고 CoT 증강 방법은 더 어려운 XTREME-UP 교차 언어 QA 인도어 작업에서 증류 CoT보다 훨씬 더 우수한 성능을 보입니다.일반적으로 증류는 품질보다 효율성을 목표로 하는 FIAT에 대한 직교 기술로 봅니다.튜닝에 지침을 추가하면 처음부터 끝까지 도움이 됩니다. 그림 5에서 명령어 증강 튜닝을 적용한 경우와 적용하지 않은 경우의 Flan PaLM-2 S 모델의 학습 곡선을 표시합니다. 튜닝에 명령어를 추가하면 모델 최적화 전인 단계 0에서 성능이 훨씬 향상되는 것을 볼 수 있습니다. 이는 미세 튜닝 12 중에 작업 데이터에 명령어를 추가하면 모델의 제로샷 성능이 크게 향상될 수 있음을 나타내며, 아마도 작업을 12 더 잘 따르게 하기 때문일 것입니다. 명령어 증강 튜닝이라는 용어를 사용하여 나중에 특정 작업의 명령어를 더 잘 따를 수 있는 기본 모델을 만드는 명령어 튜닝 기본 모델과 명령어 튜닝 단계에서 사용된 데이터와 더 유사한 소프트 프롬프트 임베딩을 학습하는 프롬프트 튜닝이라는 별도의 개념과 구별합니다. 중요한 점은 모델 매개변수가 동일한 수준의 품질을 달성하기 위해 시작점에서 멀리 이동할 필요가 없다는 것을 의미하므로 치명적인 망각의 위험이 줄어듭니다. 그러나 모델은 더 적은 단계로 동일한 수준의 품질에 도달할 뿐만 아니라 명령어가 없는 모델의 품질을 능가합니다. 명령어 증강 튜닝은 명령어 튜닝 기본 모델에 더 많은 도움이 됩니다. 명령어 튜닝이 있는 모델과 없는 모델에서 프롬프트 튜닝의 효과를 비교합니다. 그림 6은 프롬프트 튜닝이 일반적으로 명령어 튜닝이 없는 기본 모델과 명령어 튜닝이 있는 플랜 모델 모두에 개선을 가져오는 반면, 명령어 튜닝 플랜 모델의 이득은 약간 더 크고 더 일관되는 경향이 있음을 보여줍니다. 이는 프롬프트 튜닝에 사용한 데이터 형식(입력에 따른 작업 명령어)이 명령어 튜닝에 사용된 플랜 데이터 혼합과 더 유사하기 때문일 가능성이 큽니다. 5 관련 작업 명령어 튜닝 명령어 튜닝 모델(Wei et al., 2021; Longpre et al., 2023)은 다양한 작업 세트에 대해 미세 조정되어 이미 명령어를 따르도록 준비되어 있기 때문에 기본 언어 모델보다 few-shot ICL 작업에 대한 성능이 더 우수한 경우가 많습니다. 명령어 튜닝 모델을 사용하는 것은 FIAT의 핵심 구성 요소입니다. 문맥 내 학습 문맥 내 학습에서 LLM의 매개변수는 고정된 상태로 유지되고 추론 단계와 함께 몇 가지 예가 포함된 프롬프트를 사용하여 유사한 작업을 해결하도록 모델을 준비시킵니다(Nye et al., 2021; Wei et al., 2022b). 문맥 내 학습은 대규모 언어 모델에 가장 적합합니다. FIAT는 미세 조정과 함께 대규모 언어 모델의 이러한 기능을 사용하여 저데이터 영역에서 소규모 언어 모델을 구동합니다. 대규모 LLM에서 소규모 LLM으로의 지식 전달 대규모 모델에서 소규모 모델로 지식을 전달하기 위한 널리 알려진 기존 방법은 모델 증류(Hinton et al., 2015)로, 대규모 모델의 출력을 소규모 모델의 훈련 신호로 사용합니다. 다른 접근 방식으로는 대규모 언어 모델을 사용하여 데이터를 생성한 다음 이 데이터를 사용하여 소규모 모델을 훈련하는 것이 있습니다. 더 최근에는 후자의 접근 방식이 확장되어 더 작은 언어 모델에 대한 미세 조정 데이터로 제공되는 추론 단계를 생성합니다(Magister 등, 2022; Huang 등, 2022; Li 등, 2022; Ho 등, 2023; Hsieh 등, 2023; Fu 등, 2023; Zhu 등, 2023; Li 등, 2023). 과소 표현된 언어 대규모 언어 모델을 훈련하고 다운스트림 작업에 사용하는 대부분의 작업은 영어나 대규모의 쉽게 구할 수 있는 코퍼스가 있는 약 100개 언어 모음에 초점을 맞춥니다(ImaniGooghari 등, 2023). 꼬리 언어는 사용 가능한 코퍼스가 부족하여 언어 기술에서 종종 무시되었습니다(Nayak &amp; Joshi, 2022). 최근 연구는 이러한 헤드 랭귀지 이외의 테일 랭귀지에 초점을 맞추었습니다(Bapna et al., 2022; Ruder et al., 2023). 이 연구에서 저희는 특히 테일 랭귀지에 유용한 저데이터 체제를 저희 노력의 초점으로 삼았습니다. 더 작은 LLM 미세 조정 마스크된 언어 모델링 목표로 사전 학습된 인코더에 대해 프롬프트를 통한 미세 조정이 연구되었지만(Scao &amp; Rush, 2021), 저희는 생성 언어 모델을 미세 조정하는 데에도 중요하다는 것을 보여주었습니다. 예를 들어, 일부 연구에서는 더 작은 언어 모델을 미세 조정하는 것이 few-shot ICL보다 실용적인 저데이터 학습 문제에 대해 더 경쟁력 있고 효율적인 방법임을 보여줍니다(Asai et al., 2023; Ruder et al., 2023). Agrawal et al. (2022)은 매우 큰 LLM에서 생성된 QA 데이터를 합성하여 더 작은 모델의 성능을 개선할 것을 제안합니다. 6 결론 우리는 ICL과 미세 조정 학습 패러다임을 융합하고 100~10,000개의 학습 예제에 이르는 다양한 데이터 시나리오에서 개선된 모델 예측으로 이어지는 방법인 FIAT를 제시했습니다. 우리는 FIAT가 학습 패러다임 사이에서 어려운 선택을 할 필요 없이 LLM의 모든 잠재력을 활용하는 실용적인 방법을 제공하기를 바랍니다.참고문헌 Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev, Dipanjan Das, Mirella Lapata. Qameleon: 예제가 5개뿐인 다국어 qa. arXiv 사전 인쇄본 arXiv:2211.08264, 2022. Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, Hannaneh Hajishirzi. Xor qa: 언어 간 개방형 검색 질문 답변. arXiv 사전 인쇄본 arXiv:2010.11856, 2020. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh Hajishirzi. Buffet: few-shot 교차 언어 전송을 위한 대규모 언어 모델 벤치마킹. arXiv 사전 인쇄본 arXiv:2305.14857, 2023. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. 다음 천 개 언어를 위한 기계 번역 시스템 구축. arXiv 사전 인쇄본 arXiv:2205.03983, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄 arXiv:2204.02311, 2022. 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 명령어를 미세 조정한 언어 모델을 확장합니다. arXiv 사전 인쇄 arXiv:2210.11416, 2022. Jonathan H Clark, 최은솔, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev 및 Jennimaria Palomaki. Tydi qa: 다양한 유형의 언어로 정보를 찾는 질문에 답하는 벤치마크입니다. Transactions of the Association for Computational Linguistics, 8: 454-470, 2020. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 다단계 추론을 위한 소규모 언어 모델 전문화. arXiv 사전 인쇄본 arXiv:2301.12726, 2023. Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 그래디언트 기반 신경망에서 치명적인 망각에 대한 실증적 조사. arXiv 사전 인쇄본 arXiv:1312.6211, 2013. Google, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 기술 보고서. arXiv 사전 인쇄본 arXiv:2305.10403, 2023. Geoffrey Hinton, Oriol Vinyals, Jeff Dean. 신경망에서 지식 추출. arXiv 사전 인쇄본 arXiv:1503.02531, 2015. Namgyu Ho, Laura Schmid, Se-Young Yun. 대규모 언어 모델은 추론 교사입니다. Association for Computational Linguistics의 제61회 연례 회의록(제1권: 장문 논문), pp. 14852–14882, 캐나다 토론토, 2023년 7월. Association for Computational Linguistics. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister. 단계별로 증류! 더 적은 훈련 데이터와 더 작은 모델 크기로 더 큰 언어 모델보다 성능이 뛰어납니다. arXiv 사전 인쇄 arXiv:2305.02301, 2023. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang 및 Weizhu Chen. Lora: 대규모 언어 모델의 낮은 순위 적응. arXiv 사전 인쇄 arXiv:2106.09685, 2021. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu 및 Jiawei Han. 대규모 언어 모델은 자체적으로 개선될 수 있습니다. arXiv 사전 인쇄본 arXiv:2210.11610, 2022.Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André FT Martins, François Yvon, et al. Glot500: 다국어 코퍼스 및 언어 모델을 500개 언어로 확장. arXiv 사전 인쇄본 arXiv:2305.12182, 2023. Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적 프롬프트 튜닝을 위한 스케일의 힘. arXiv 사전 인쇄본 arXiv:2104.08691, 2021. Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 상징적 사고의 사슬 증류: 작은 모델도 단계별로 &quot;생각&quot;할 수 있습니다. arXiv 사전 인쇄본 arXiv:2306.14050, 2023. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 큰 언어 모델의 설명은 작은 추론자를 더 좋게 만듭니다. arXiv 사전 인쇄본 arXiv:2210.06726, 2022. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 플랜 컬렉션: 효과적인 교육 튜닝을 위한 데이터 및 방법 설계. arXiv 사전 인쇄본 arXiv:2301.13688, 2023. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn. 추론하도록 작은 언어 모델 가르치기. arXiv 사전 인쇄본 arXiv:2212.08410, 2022. Benjamin Muller, John Wieting, Jonathan H Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, Xinyi Wang. 언어 간 질의 응답을 위한 속성 평가 및 모델링. arXiv 사전 인쇄본 arXiv:2305.14332, 2023. Ravindra Nayak 및 Raviraj Joshi. L3Cube-HingCorpus 및 HingBERT: 코드 혼합 HindiEnglish 데이터 세트 및 BERT 언어 모델. 제13차 언어 자원 및 평가 회의 내 WILDRE-6 워크숍 진행, pp. 7–12, 프랑스 마르세유, 2022년 6월. 유럽 언어 자원 협회. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan 등 작업 표시: 언어 모델을 사용한 중간 계산을 위한 스크래치 패드. arXiv 사전 인쇄 arXiv:2112.00114, 2021. OpenAI. Gpt-4 기술 보고서, 2023년. Sebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel A Sarr, Xinyi Wang 등 Xtreme-up: 대표성이 낮은 언어를 위한 사용자 중심의 희소 데이터 벤치마크. arXiv 사전 인쇄본 arXiv:2305.11938, 2023. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 멀티태스크 프롬프트 학습은 제로샷 태스크 일반화를 가능하게 합니다. arXiv 사전 인쇄본 arXiv: 2110.08207, 2021. Teven Le Scao와 Alexander M Rush. 프롬프트의 데이터 포인트는 몇 개입니까? NAACL, 2021. Timo Schick와 Hinrich Schütze. 몇 가지 샷 텍스트 분류 및 자연어 추론을 위한 빈칸 채우기 질문 활용. arXiv 사전 인쇄본 arXiv:2001.07676, 2020a. 티모 쉬크(Timo Schick)와 힌리히 슈체(Hinrich Schütze). 중요한 것은 단지 크기만이 아닙니다. 소규모 언어 모델은 소수의 학습자이기도 합니다. arXiv 사전 인쇄 arXiv:2009.07118, 2020b. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai 및 Quoc V Le. 미세 조정된 언어 모델은 제로샷 학습자입니다. arXiv 사전 인쇄 arXiv:2109.01652, 2021. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler 등. 대규모 언어 모델의 새로운 능력. arXiv 사전 인쇄본 arXiv:2206.07682, 2022a.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022b. Yangqiaoyu Zhou, Yiming Zhang, Chenhao Tan. Flame: 자연어 설명을 통한 Few-shot learning. Association for Computational Linguistics의 제61회 연례 회의록, 2023. Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen Zhou. Pad: Program-aided distillation은 추론에서 대형 모델을 전문으로 합니다. arXiv 사전 인쇄본 arXiv:2305.13888, 2023.Split bn fi ja ru te TrainValidationTest 150 578 1362822 1318 1908 1268표 4: XOR-ATTRIQA에 대한 데이터 세트 크기. bho brx gbm gom gu hi hne kn mai ml mni mr mwr or pa ps sa ta ur ar bn fi ja ko ru te 검증 356 358 359 365 366 370 310 372 373 360 375 330 335 386 386 385 384 385 386 387 388 3468 272 361 2530표 5: XTREME-UP 교차 언어 QA를 위한 데이터 세트 크기. 부록 A.1 각 작업에 대한 언어 목록 표 4와 표 5에 각 작업에 대한 학습, 검증 및 테스트 예제의 수를 제공합니다.A. 결과의 언어별 분석 표 6, 표 7 및 표 8에 각 언어에 대한 성능을 제공합니다.Mr MB 방법 L Few-shot ICL L FIAT XS w/o CoT-Augmented 튜닝 w/o 명령어-Augmented 튜닝 w/o 매개변수 효율적 튜닝 w/o 명령어-튜닝 기본 모델 L FIAT S w/o CoT-Augmented 튜닝 w/o 명령어-Augmented 튜닝 w/o 매개변수 효율적 튜닝 w/o 명령어-튜닝 기본 모델 bn fi ja Acc/AUC-PR ru te 85.9/- 78.5/92.6/81.1 91.0/85.92.5/84.85.4/-96.3 / 66.91.8/85.96.2/70.84.5/94.8/84.9 95.3/72.94.6/84.1 95.0/76.58.9/91.7/74.92.6/73.89.4 / 65.92.3/81.93.0/84.93.1/75.92.7/76.90.9 / 66.91.2/81.95.9/53.93.8/77.4 94.8/75.92.0/76.95.0/55.94.2/74.1 94.7/68.88.9/65.9 94.3 / 42.90.1/58.6 89.7/28.92.184.0 96.2/62.4 94.6/84.9 94.0/93.94.4/81.2 95.5/58.8 98.8/87.4 95.3/78.92.7/82.9 95.0/51.3 94.6/78.93.2/83.6 96.3/59.0 95.1/83.88.6/67.7 93.2/41.0 89.7/57.95.2/70.96.5/78.90.3 / 40.표 6: XOR-ATTRIQA.M, MB에 대한 각 언어의 결과 방법 L Few-shot ICL L FIAT XS w/o CoT-Augmented Tuning w/o Instruction-Augmented Tuning w/o Parameter-efficient Tuning w/o Instruction-tuned 기본 모델 L FIAT S w/o CoT-augmented Tuning w/o Instruction-Augmented Tuning w/o Parameter-efficient Tuning w/o Instruction-tuned base model _ as bho brx gbm gom gu hi han kn mai Filmni mr mwr or pa ps sata 당신 72.5 61.8 43.0 60.3 72.3 70.6 61.5 70.8 72.9 73.3 72.2 57.1 71.5 69.5 71.4 73.7 70.6 72.6 71.5 69.75.9 73.9 47.2 72.7 76.1 76.1 79.3 76.2 76.6 75.5 76.3 61.1 75.4 73.3 76.0 75.6 76.6 77.4 75.4 73.65.6 64.7 49.3 60.3 62.6 65.73.2 71.5 39.1 67.8 71.7 73.7 78.5 70.3 74.0 71.2 74.7 50.1 73.9 71.4 70.9 72.2 72.8 71.8 74.5 72.70.7 69.5 49.2 65.7 70.7 80.5 67.4 69.69.7 70.51.6 70.0 67.8 66.8 69.5 69.7 68.7 70.9 69.8 67.76.9 63.2 65.2 63.7 65.4 52.8 64.2 63.5 63.8 65.8 64.3 63.7 65.4 64.80.2 77.8 52.2 77.2 78.3 80.6 82.2 79.5 79.7 78.8 79.8 64.5 79.4 77.4 79.4 80.7 80.0 80.4 79.8 78.79.1 78.4 50.3 75.6 78.7 79.9 84.6 77.8 79.2 78.3 79.2 62.4 77.8 77.7 79.6 79.2 78.8 79.9 80.1 78.74.3 71.2 506 717 72.7 746 818 727 75.1 74.1 74.9 51.9 73.9 721 758 73.5 73.5 726 73.6표 7: XTREME-UP Cross-lingual QA Indic에 대한 각 언어별 결과 ра ps sa ta ur ar fi ja ko ru te 70.8 72.9 73.3 72.2 57.1 71.5 69.5 714 73.7 70.6 72.6 71.5 69.4 66.0 75.2 65.5 60.3 61.2 66.9 68.80.5 80.8 79.0 79.6 65.6 79.6 78.7 79.8 79.1 80.1 79.5 79.8 78.3 83.7 84.6 82.8 83.7 86.3 81.6 82.75 4 79.0 55.2 77.8 75.9 75.8 78.7 78.1 78.3 80.76.1 80 4 58.3 78.7 76.77 3 78.8 61.975.bho brx gbm gom gu hi hne kn mai ml mni mr mwr MМа 방법 FL Few-shot ICL L FIAT w/o CoT-augmented Tuning XS w/o 명령어-augmented Tuning 78.w/o 매개변수-효율적 튜닝 78.72.5 61.8 43.0 60.3 723 70.6 61.80.1 80.4 52.6 77.0 78.9 80.7 85.79.8 76.8 49.1 71.9 76.5 78.1 8477.8 49.2 72.8 77.0 78.775.6 55.77.5 79.76.8 80.75.0 78.0 84.9 76.5 78w/o 명령어 조정 기반 모델 76.9 76.4 56.6 73.742 76.8 84.7 75.4 77.78.1 62.8 77.5 74.78.1 83.77.1 78.6 76.8 79.1 79.4 79 4 84.75.9 78.4 76.9 76.6 79.8 77.8 84.74.7 77.5 76.5 75.3 77.5 75.8 82.L FIAT S CoT 증강 튜닝 없음 명령어 증강 튜닝 없음 매개변수 효율적 튜닝 없음 명령어 튜닝 없음 기본 모델 85.0 82.1 82.3 85.9 80.8 81.84 6 81.5 82.6 87.0 81.7 8083.5 819 83.2 88.1 82.0 81.84 2 81.2 82.8 88.1 80.4 80.86.2 82.0 83.7 87.1 83.3 86.8383.8 86.4 84.84.9 892 85.81.6 80.5 51.9 78.3 80.2 82.3 85.8 81.2 82.4 82.1 81.5 67.0 82.1 80.2 81.6 80.9 81.5 82.2 82.3 79.5 82.82.8 80.5 49.9 78.0 80.0 834 859 80.4 82.7 80.5 83.7 64 9 81.5 80.2 82.0 82.0 83.0 82.4 80.0 84.2 86.6 81.9 82.4 87.81.3 80.0 51.2 78.3 7882.0 85.7 80.5 81.2 80.3 81 8 64.8 81.0 79.7 81.2 80.5 80.7 80.5 81.6 794 82.8 85.7 83.79.5 77.5 61.5 77.3 78.3 80.1 85.3 79.0 79.9 79.0 80.5 68.9 79.0 78.4 79.8 78.8 78.7 78.78.3 83.3 85.1 84.79.5 77.4 55.4 75.6 79.1 79.9 85.5 77.5 80.7 78.5 80.3 63.4 79.5 77.8 78.8 78.6 78.7 78.8 80.7 77.7 81.9 85.8 84.80.8482.85.0 88.8 91.9 82.표 8: XTREME-UP Cross-lingual QA 전체에 대한 각 언어별 결과.
