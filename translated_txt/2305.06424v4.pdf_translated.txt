--- ABSTRACT ---
GPT-4와 같은 대규모 언어 모델(LLM)은 최근 자연어 이해 및 생성에서 인상적인 역량을 보여주었습니다. 그러나 사기 또는 서비스 거부 공격과 같은 악의적인 목적으로 오용될 수 있다는 우려가 있습니다. 따라서 대화에 참여한 당사자가 봇인지 사람인지 감지하는 방법을 개발하는 것이 중요합니다. 이 논문에서는 FLAIR(Finding Large Language Model Authenticity via a Single Inquiry and Response)라는 프레임워크를 제안하여 온라인 방식으로 대화형 봇을 감지합니다. 구체적으로, 우리는 인간 사용자와 봇을 효과적으로 구별할 수 있는 단일 질문 시나리오를 목표로 합니다. 질문은 두 가지 범주로 나뉩니다. 사람에게는 쉽지만 봇에게는 어려운 질문(예: 계산, 대체, 검색 및 ASCII 아트 추론)과 봇에게는 쉽지만 사람에게는 어려운 질문(예: 암기 및 계산)입니다. 우리의 접근 방식은 이러한 질문의 효과성 측면에서 서로 다른 강점을 보여주며 온라인 서비스 제공자가 악의적인 활동으로부터 자신을 보호할 수 있는 새로운 방법을 제공합니다. 저희의 코드와 질문 세트는 https://github.com/hongwang600/FLAIR에서 볼 수 있습니다. 1
--- INTRODUCTION ---
최근 GPT-4(OpenAI, 2023) 및 LLAMA-2(Touvron et al., 2023a)와 같은 대규모 언어 모델(LLM)의 개발은 자연어 처리에 상당한 진전을 가져왔고 언어 이해(Chowdhery et al., 2022), 질의 응답(Su et al., 2019), 대화 시스템(Wang et al., 2022b; Qian &amp; Yan, 2023) 및 다중 모드 추론(Wang et al., 2022a)의 하위 작업에서 우수한 성능을 달성했습니다. 그러나 이러한 모델이 확산되면서 악의적인 목적으로 오용될 가능성에 대한 우려가 제기되었습니다. 가장 심각한 위협 중 하나는 대규모 언어 모델을 사용하여 인간 사용자를 사칭하고 사기, 스패밍 또는 서비스 거부 공격과 같은 불법 활동에 참여하는 것입니다. 예를 들어, LLM 에이전트는 해커가 전자 상거래, 항공사, 은행과 같은 다양한 기업의 모든 고객 서비스 채널을 점유하는 데 사용될 수 있습니다. 게다가 텍스트 음성 변환(TTS) 기술의 도움으로 기계에서 생성된 음성이 911과 같은 공공 서비스 회선을 점유하여 심각한 공공 위기를 초래할 수도 있습니다(Wang et al., 2021). 이러한 공격은 온라인 서비스 제공자와 사용자에게 상당한 피해를 입혀 온라인 상호 작용의 신뢰와 무결성을 침식할 수 있습니다. 이러한 과제에 대응하여 인간 사용자와 악의적인 LLM 기반 봇을 확실하게 구별해야 할 시급한 요구 사항이 있습니다. CAPTCHA(Von Ahn et al., 2003) 사용과 같은 기존 기술은 봇 스패밍과 레이딩을 방지하기 위해 사용자가 인간인지 봇인지 판별하기 위해 개발되었습니다. 일반적으로 사용되는 CAPTCHA 방법은 사용자에게 왜곡된 문자와 숫자를 인식하도록 요청하는 것입니다. 그러나 이러한 접근 방식은 텍스트만 포함하는 챗봇을 감지하는 데 있어 상당한 과제에 직면합니다. 여기서 GPT-4와 같은 대규모 언어 모델의 등장이 채팅봇 감지 문제를 더욱 복잡하게 만들었습니다.이러한 모델은 고품질의 인간과 유사한 텍스트를 생성하고 상당한 정도로 인간의 행동을 모방할 수 있기 때문입니다.최근 COLMHumans good at | Humans not good at Bots good at × memorization computing symbolic Manipulation randomness Bots not good at searching graphic understanding Table 1: 봇과 인간이 (잘) 하지 못하는 작업 활용.DetectGPT(Mitchell et al., 2023)와 같은 연구에서는 텍스트가 ChatGPT에서 생성되었는지 여부를 분류하는 방법을 제안했지만 오프라인 설정에 초점을 맞춥니다.최근 연구(Sadasivan et al., 2023b)에 따르면 이러한 감지기는 생성 텍스트 모델 위에 가벼운 의역기가 적용되는 의역 공격에는 신뢰할 수 없습니다.이러한 제한은 대규모 언어 모델과 인간 사용자를 구별하고 온라인 채팅 상호 작용에서 이들의 존재를 감지하기 위한 보다 견고하고 정확한 방법에 대한 필요성을 강조합니다. 이 논문에서 우리는 LLM 기반 대화형 봇 감지를 위한 LLM의 강점과 약점을 최대한 활용하기 위해 FLAIR라는 새로운 프레임워크를 제안합니다. 구체적으로, 우리는 봇과 인간 간에 뚜렷한 반응을 유도하는 신중하게 설계된 일련의 질문을 소개합니다. 이러한 질문은 봇과 인간이 언어를 처리하고 생성하는 방식의 차이를 활용하도록 맞춤화되었습니다. 표 1에서 볼 수 있듯이, 기호 조작, 무작위성, 검색 및 그래픽 이해 분야의 특정 질문은 봇에게는 어렵지만 인간에게는 비교적 쉽습니다. 예로는 계산, 대체, 검색 및 ASCII 아트 추론이 있습니다. 반면에 암기와 계산은 봇에게는 비교적 쉽지만 인간에게는 어려웠습니다. 우리의 실험 결과는 FLAIR가 기존 CAPTCHAS에 대한 실행 가능한 대안을 제공한다는 것을 보여줍니다. 구체적으로, 인간과 LLM은 강점 분야 내 작업에서 높은 정확도로 뛰어난 반면, 덜 능숙한 작업에서는 성과가 크게 떨어져 종종 매우 낮은 수준(~0%)으로 떨어집니다. 이러한 성능의 뚜렷한 차이는 단 하나의 질문으로 인간과 LLM 응답자를 구별할 수 있게 해줍니다. 제안된 접근 방식은 봇을 빠르게 감지하고 온라인 상호작용을 보호하는 보다 견고하고 정확한 방법을 개발하는 데 유망함을 보여줍니다. 2
--- RELATED WORK ---
2.1 CAPTCHA CAPTCHA(Von Ahn et al., 2003)는 사전 공격, 이메일 스패밍, 웹 크롤러, 피싱 공격 등과 같은 악성 애플리케이션을 차단하는 데 사용되는 일반적인 기술입니다. CAPTCHA에는 여러 유형이 있습니다. 텍스트 기반 CAPTCHA는 사용자가 왜곡된 형태의 문자와 숫자를 인식하도록 요구하고(Chew &amp; Baird, 2003; Mori &amp; Malik, 2003; Yan &amp; El Ahmad, 2008), 이미지 기반 CAPTCHA(Gossweiler et al., 2009)는 사용자가 신호등과 같이 유사한 속성을 가진 이미지를 선택하도록 요구합니다. 비디오 기반 CAPTCHA(Kluever, 2008)는 사용자가 비디오를 설명하는 세 단어를 선택하도록 요구하고, 오디오 기반 CAPTCHA(Gao et al., 2010)는 사용자에게 오디오를 듣고 언급된 단어를 제출하도록 요청합니다(Saini &amp; Bala, 2013). 퍼즐 CAPTCHAS(Singh &amp; Pal, 2014)는 사용자가 세그먼트를 결합하여 완전한 그림을 형성하도록 요구합니다. 이러한 기술은 인간 사용자와 봇을 구별하여 악의적인 활동을 방지하는 데 사용됩니다. 2.2 LLM 오프라인 감지. 도입 이후 대규모 언어 모델(LLM)은 널리 사용되었으며 잠재적인 오용에 대한 대중의 우려를 불러일으켰습니다. 예를 들어, 학생들은 ChatGPT(OpenAI, 2023)를 사용하여 서면 과제를 완료할 수 있으므로 강사가 학생 학습을 정확하게 평가하기 어려울 수 있습니다. 결과적으로 텍스트가 ChatGPT에서 작성되었는지 감지해야 할 필요성이 커지고 있습니다. 이 문제를 해결하기 위해 DetectGPT(Mitchell et al., 2023)는 원래 구절의 로그 확률과 동일한 구절의 섭동의 로그 확률을 비교하여 솔루션을 제안합니다. 이것의 이면에 있는 가설
--- METHOD ---
영어: 대화에 참여한 당사자가 봇인지 사람인지 감지하는 s입니다. 이 논문에서는 온라인 방식으로 대화형 봇을 감지하기 위해 FLAIR(Finding Large Language Model Authenticity via a Single Inquiry and Response)이라는 프레임워크를 제안합니다. 구체적으로, 우리는 인간 사용자와 봇을 효과적으로 구별할 수 있는 단일 질문 시나리오를 목표로 합니다. 질문은 두 가지 범주로 나뉩니다. 사람에게는 쉽지만 봇에게는 어려운 질문(예: 계산, 대체, 검색 및 ASCII 아트 추론)과 봇에게는 쉽지만 사람에게는 어려운 질문(예: 암기 및 계산)입니다. 우리의 접근 방식은 이러한 질문의 효과성 측면에서 서로 다른 강점을 보여주며, 온라인 서비스 제공자가 사악한 활동으로부터 자신을 보호할 수 있는 새로운 방법을 제공합니다. 우리의 코드와 질문 세트는 https://github.com/hongwang600/FLAIR에서 제공됩니다. 1 서론 최근 GPT-4(OpenAI, 2023) 및 LLAMA-2(Touvron et al., 2023a)와 같은 대규모 언어 모델(LLM)의 개발은 자연어 처리에 상당한 진전을 가져왔고 언어 이해(Chowdhery et al., 2022), 질의 응답(Su et al., 2019), 대화 시스템(Wang et al., 2022b; Qian &amp; Yan, 2023) 및 다중 모드 추론(Wang et al., 2022a)의 하위 작업에서 우수한 성능을 달성했습니다. 그러나 이러한 모델이 확산되면서 악의적인 목적으로 오용될 가능성에 대한 우려가 제기되었습니다. 가장 심각한 위협 중 하나는 대규모 언어 모델을 사용하여 인간 사용자를 사칭하고 사기, 스패밍 또는 서비스 거부 공격과 같은 불법 활동에 참여하는 것입니다. 예를 들어, LLM 에이전트는 해커가 전자 상거래, 항공사, 은행과 같은 다양한 기업의 모든 고객 서비스 채널을 점유하는 데 사용될 수 있습니다. 게다가 텍스트 음성 변환(TTS) 기술의 도움으로 기계에서 생성된 음성이 911과 같은 공공 서비스 회선을 점유하여 심각한 공공 위기를 초래할 수도 있습니다(Wang et al., 2021). 이러한 공격은 온라인 서비스 제공자와 사용자에게 상당한 피해를 입혀 온라인 상호 작용의 신뢰와 무결성을 침식할 수 있습니다. 이러한 과제에 대응하여 인간 사용자와 악의적인 LLM 기반 봇을 확실하게 구별해야 할 시급한 요구 사항이 있습니다. CAPTCHA(Von Ahn et al., 2003) 사용과 같은 기존 기술은 봇 스패밍과 레이딩을 방지하기 위해 사용자가 인간인지 봇인지 판별하기 위해 개발되었습니다. 일반적으로 사용되는 CAPTCHA 방법은 사용자에게 왜곡된 문자와 숫자를 인식하도록 요청하는 것입니다. 그러나 이러한 접근 방식은 텍스트만 포함하는 챗봇을 감지하는 데 있어 상당한 과제에 직면합니다. 여기서 GPT-4와 같은 대규모 언어 모델의 등장이 채팅봇 감지 문제를 더욱 복잡하게 만들었습니다.이러한 모델은 고품질의 인간과 유사한 텍스트를 생성하고 상당한 정도로 인간의 행동을 모방할 수 있기 때문입니다.최근 COLMHumans good at | Humans not good at Bots good at × memorization computing symbolic Manipulation randomness Bots not good at searching graphic understanding Table 1: 봇과 인간이 (잘) 하지 못하는 작업 활용.DetectGPT(Mitchell et al., 2023)와 같은 연구에서는 텍스트가 ChatGPT에서 생성되었는지 여부를 분류하는 방법을 제안했지만 오프라인 설정에 초점을 맞춥니다.최근 연구(Sadasivan et al., 2023b)에 따르면 이러한 감지기는 생성 텍스트 모델 위에 가벼운 의역기가 적용되는 의역 공격에는 신뢰할 수 없습니다.이러한 제한은 대규모 언어 모델과 인간 사용자를 구별하고 온라인 채팅 상호 작용에서 이들의 존재를 감지하기 위한 보다 견고하고 정확한 방법에 대한 필요성을 강조합니다. 이 논문에서 우리는 LLM 기반 대화형 봇 감지를 위한 LLM의 강점과 약점을 최대한 활용하기 위해 FLAIR, Finding LLM Authenticity with a Single Inquiry and Response라는 새로운 프레임워크를 제안합니다. 구체적으로, 우리는 봇과 인간 사이에 뚜렷한 반응을 유도하는 신중하게 설계된 일련의 질문을 소개합니다. 이러한 질문은 봇과 인간이 언어를 처리하고 생성하는 방식의 차이점을 활용하도록 맞춤화되었습니다. 표 1에서 볼 수 있듯이, 기호 조작, 무작위성, 검색 및 그래픽 이해 분야의 특정 질문은 봇에게는 어렵지만 인간에게는 비교적 쉽습니다. 예로는 계산, 대체, 검색 및 ASCII 아트 추론이 있습니다. 반면에 암기와 계산은 봇에게는 비교적 쉽지만 인간에게는 어려웠습니다. 우리의
--- EXPERIMENT ---
모든 결과는 FLAIR가 기존 CAPTCHAS에 대한 실행 가능한 대안을 제공함을 보여줍니다. 구체적으로, 인간과 LLM은 자신의 강점 분야 내 작업에서 높은 정확도로 뛰어난 반면, 덜 능숙한 작업에서는 성과가 크게 떨어져 종종 매우 낮은 수준(~0%)으로 떨어집니다. 이러한 성과의 뚜렷한 차이는 단 하나의 질문으로 인간과 LLM 응답자를 구별할 수 있게 합니다. 제안된 접근 방식은 봇을 빠르게 감지하고 온라인 상호 작용을 보호하는 보다 견고하고 정확한 방법을 개발하는 데 유망함을 보여줍니다. 2 관련 연구 2.1 CAPTCHA CAPTCHA(Von Ahn et al., 2003)는 사전 공격, 이메일 스패밍, 웹 크롤러, 피싱 공격 등과 같은 악성 애플리케이션을 차단하는 데 사용되는 일반적인 기술입니다. 여러 유형의 CAPTCHA가 있습니다. 텍스트 기반 CAPTCHA는 사용자가 왜곡된 형태의 문자와 숫자를 인식하도록 요구하고(Chew &amp; Baird, 2003; Mori &amp; Malik, 2003; Yan &amp; El Ahmad, 2008), 이미지 기반 CAPTCHA(Gossweiler et al., 2009)는 사용자가 신호등과 같이 유사한 속성을 가진 이미지를 선택하도록 요구합니다.비디오 기반 CAPTCHA(Kluever, 2008)는 사용자가 비디오를 설명하는 세 단어를 선택하도록 요구하고, 오디오 기반 CAPTCHA(Gao et al., 2010)는 사용자에게 오디오를 듣고 언급된 단어를 제출하도록 요구합니다(Saini &amp; Bala, 2013).퍼즐 CAPTCHA(Singh &amp; Pal, 2014)는 사용자가 세그먼트를 결합하여 완전한 그림을 형성하도록 요구합니다.이러한 기술은 인간 사용자와 봇을 구별하여 악의적인 활동을 방지하는 데 사용됩니다.2.2 LLM 오프라인 감지. 대규모 언어 모델(LLM)은 도입 이후 널리 사용되었으며 잠재적인 오용에 대한 대중의 우려를 불러일으켰습니다. 예를 들어, 학생들은 ChatGPT(OpenAI, 2023)를 사용하여 서면 과제를 완료할 수 있으므로 강사가 학생 학습을 정확하게 평가하기 어렵습니다. 결과적으로 텍스트가 ChatGPT에서 작성되었는지 감지해야 할 필요성이 커지고 있습니다. 이 문제를 해결하기 위해 DetectGPT(Mitchell et al., 2023)는 원래 구절의 로그 확률과 같은 구절의 섭동의 로그 확률을 비교하여 솔루션을 제안합니다. 이 방법의 기본 가설은 모델에서 생성한 텍스트를 약간 다시 쓰면 원래 샘플에 비해 로그 확률이 낮아지는 반면 사람이 쓴 텍스트를 약간 다시 쓰면 로그 확률이 높아지거나 낮아질 수 있다는 것입니다. 또 다른 연구 분야는 이 문제를 이진 분류 문제로 모델링하고 지도 학습 데이터를 사용하여 다른 모델을 미세 조정합니다(Bakhtin et al., 2019). 가장 최근에 Mitrović 등(2023)은 Transformer 기반 모델을 미세 조정하여 예측을 수행한 다음 SHAP(Lundberg &amp; Lee, 2017)을 사용하여 설명했습니다.또 다른 연구 분야는 AI가 생성한 텍스트에 워터마크를 추가하여 식별을 용이하게 하는 데 중점을 두고 있으며, 이는 텍스트에 특정 패턴을 각인하여 감지하기 쉽게 만드는 것을 포함합니다(Zhao 등, 2023).Kirchenbauer 등(2023)이 제안한 소프트 워터마킹은 이러한 패턴을 만들기 위해 토큰을 녹색 및 빨간색 목록으로 나누는 것을 포함합니다.텍스트를 생성할 때 워터마크가 있는 LLM은 접두사 토큰에 의해 결정되는 녹색 목록에서 토큰을 선택할 가능성이 더 높습니다.이러한 워터마크는 종종 미묘하고 사람이 알아차리기 어렵습니다.그러나 Sadasivan 등에서 보여준 것처럼 (2023a), 워터마킹 방식, 신경망 기반 감지기, 제로샷 분류기를 포함한 다양한 감지 방법은 패러프레이징 공격으로 쉽게 무력화할 수 있습니다. 이러한 공격에는 언어 모델에서 생성된 텍스트에 가벼운 패러프레이저를 적용하는 것이 포함됩니다. 또한 이론적 분석에 따르면 최상의 감지기조차도 충분히 좋은 언어 모델을 처리할 때 무작위 분류기보다 약간 더 나은 성능을 보일 수 있습니다. 이는 고급 언어 모델에서 생성된 텍스트의 오프라인 감지에서 근본적인 과제를 강조하는데, 이는 인간이 쓴 텍스트와 사실상 구별할 수 없는 글을 생성할 수 있습니다. 따라서 사용자가 시스템과 라이브 채팅 상호 작용을 하는 온라인 감지 설정으로 초점을 옮기는 것이 더 의미 있고 중요합니다. 3 LLM의 약점 활용 이 섹션에서는 계산, 대체, 무작위 편집, 검색 및 ASCII 아트 추론과 같은 특정 작업을 살펴봅니다. 이러한 작업은 인간에게는 간단해 보이지만 대규모 언어 모델(LLM)에게는 상당한 과제를 제시합니다. 3.1 세기 최첨단 LLM은 문자열의 문자를 정확하게 세기 위해 노력하지 않지만(Qian et al., 2022), 인간은 쉽게 그렇게 할 수 있습니다. LLM의 이러한 한계는 인간과 LLM을 구별하기 위해 세기 FLAIR의 설계에 영감을 주었습니다. 참가자들은 주어진 문자열에 특정 문자가 나타나는 횟수를 세도록 요청받습니다. 설명 없이 질문에 답하세요. eeoeotetto GPT-3.5에서 t의 개수를 세어 주세요. &quot;eeooeotetto&quot;에는 t가 4개 있습니다. 인간: 이 예에서 보여지듯이, GPT-3.5는 문자열 내의 지정된 문자를 정확하게 세는 데 어려움을 겪습니다. 3.2 대체 LLM은 종종 맥락과 일치하지 않는 내용을 출력한다는 것이 알려져 있습니다(Elazar et al., 2021; Wang et al., 2022c). 이는 현재 LLM의 공통된 약점입니다. LLM에게 주어진 대체 규칙에 따라 임의의 단어를 철자하도록 요청하여 규칙을 일관되게 따를 수 있는지 테스트합니다.임의 단어는 5~10자 길이이며 사전에서 무작위로 샘플링합니다.다음은 예입니다.COLM에서 학회 논문으로 게시 설명 없이 질문에 답하세요: p를 대체하려면 m을 사용하고, e를 대체하려면 a를 사용하고, a를 대체하려면 n을 사용하고, c를 대체하려면 g를 사용하고, h를 대체하려면 o를 사용하고, 이 규칙에 따라 peach를 철자하려면 어떻게 해야 합니까?GPT-3.5: mnong Human: mango LLM은 위의 대체 작업에서와 같이 이 규칙을 일관되고 반복적으로 적용하는 데 어려움을 겪습니다.이 테스트 개념은 문자열이 규칙에 따라 변환되는 암호화 체계로 더 일반화될 수 있습니다.3.임의 편집 임의 편집은 노이즈가 많은 입력에 대한 자연어 처리 모델의 견고성을 평가하는 데 사용되는 기술입니다.이 기술은 모델에 문자열([0,1] 집합에서 샘플링)을 제시하고 일부 문자를 무작위로 삭제, 삽입, 교환 또는 대체하는 것을 포함합니다. 모델이 이러한 임의 연산을 수행할 수 있는지 확인하려면 동일한 조건에서 세 가지 다른 출력을 생성해야 합니다. 세 가지 출력이 서로 다른지 확인하면 모델이 임의 연산을 적용하고 있음을 확인할 수 있습니다. 임의 삭제의 경우, 모델은 문자열에서 주어진 문자 c의 k개 발생을 임의로 제거하도록 요청받습니다. 여기서 k는 난수이고 c는 문자열에 나타나는 임의 문자입니다. 임의 삽입의 경우, 모델은 문자열의 임의 위치에 주어진 문자 c의 k개 발생을 임의로 삽입하도록 요청받습니다. 임의 스왑의 경우, 모델은 문자열에서 문자 c와 d의 k개 쌍을 임의로 스왑하도록 요청받습니다. 여기서 c와 d는 문자열에 나타나는 서로 다른 문자입니다. 마지막으로, 임의 대체의 경우, 모델은 문자열에서 문자 c의 k개 발생을 다른 문자 d로 임의로 바꾸도록 요청받습니다. 아래에서는 임의 삭제 작업에서 LLM과 인간 출력의 예를 보여줍니다. 설명 없이 질문에 답하세요: 문자열에서 1 두 개를 임의로 삭제하세요: 0110010011. 세 가지 다른 출력을 주세요. GPT-3.5: 01001011, 010010011, 인간: 00010011, 01000011, 인간은 지정된 대로 직관적이고 올바르게 문자를 조작할 수 있는 반면, GPT-3.은 작업을 일관되게 올바르게 수행하는 데 어려움을 겪었습니다. 이는 자연어 처리 작업에서 동적으로 지정된 무작위 입력 변환을 처리하는 데 잠재적인 한계가 있음을 시사합니다. 3.4 검색 검색은 인간의 고유한 능력으로, 우리는 마음 속으로 다양한 잠재적 결과를 상상하고 그에 따라 결정을 내릴 수 있습니다. 반면, 언어 모델은 순차적 생성 프로세스에 의해 제약을 받아 백트래킹을 통해 다양한 결과를 탐색하는 능력이 제한됩니다. 토큰이 생성되면 이러한 모델은 후속 통찰력이나 정보에 따라 철회하거나 수정할 수 없으므로 초기 처리 중에 간과되었을 수 있는 대체 경로를 탐색할 수 없습니다. 이를 테스트하기 위해 기본적인 검색 시나리오를 검토합니다. 2D 지도에서 섬의 수를 세는 작업입니다. 이 작업은 일반적으로 깊이 우선 탐색(DFS) 또는 너비 우선 탐색(BFS) 알고리즘을 사용하여 해결됩니다. 이 과제에서는 그리드에서 &#39;■&#39;(육지를 나타냄)의 연결된 구성 요소를 식별하고 세어야 합니다. 수평 또는 수직으로 인접한 &#39;■&#39;는 섬을 형성합니다. 섬의 수를 정확하게 파악하려면 체계적으로 지도를 횡단하여 방문한 육지 셀을 표시하여 같은 섬을 다시 세지 않도록 해야 합니다. 다음은 예입니다. COLM에서 학술 대회 논문으로 게시 고양이가 포함된 모든 ASCII 아트를 선택하십시오. ASCII 아트를 올바른 방향으로 회전하십시오. 누락된 영역과 가장 잘 일치하는 샘플을 선택하십시오. AB 사용자 응답 사용자 응답 +180] 사용자 응답 A-.. BB: +(a) +(b) (c) D 그림 1: ASCII 추론에 대한 예. (a) X가 포함된 ASCII 아트를 선택합니다. (b) ASCII 아트를 적절한 방향으로 회전합니다. (c) 잘린 부분과 가장 정확하게 일치하는 것을 선택합니다. 설명 없이 질문에 답합니다: 검은색 블록이 육지를 나타내고 공백이 물을 나타내는 주어진 2D 지도에서 섬의 수를 세어 보세요. 결과로 단일 숫자를 출력합니다. 지도: ווה GPT-3.5:Human: 예에서 보여준 것처럼 GPT-3.5의 출력은 잘못된 반면, 사람은 연결된 영역을 탐색하여 섬의 수를 직관적이고 정확하게 셀 수 있습니다. 이 작업은 검색 및 백트랙 메커니즘에서 언어 모델의 고유한 한계를 강조합니다. 3.5 ASCII 아트 추론 사람은 이미지 형식으로 표현되든 텍스트 문자열로 표현되든 시각적 요소를 이해할 수 있습니다. 예를 들어, ASCII 아트로 변환된 사과는 추상적 표현에도 불구하고 사람이 여전히 인식할 수 있습니다. ASCII 아트를 이해하는 이러한 능력에는 현재 언어 모델에 없는 수준의 시각적 추상화가 필요합니다. 이를 조사하기 위해 자연 이미지를 ASCII 형식으로 변환하고 일반적인 추론 과제와 관련된 실험을 수행했습니다. 이 접근 방식을 기존 CAPTCHA 시스템과 통합하기 위해 세 가지 추론 과제에 집중했습니다. 첫 번째 과제는 고양이와 같은 특정 요소를 포함하는 ASCII 아트를 식별하는 것입니다. 그림 1(a)는 참가자가 reCAPTCHA의 기능을 모방하여 고양이를 묘사한 ASCII 아트를 선택해야 하는 예를 보여줍니다. 참가자는 고양이가 있는 이미지를 클릭합니다. 언어 모델의 경우 지정된 인덱스(A, B, C, D)가 있는 문자열 형식으로 이러한 이미지를 받습니다. 그런 다음 언어 모델은 고양이가 있는 ASCII 아트에 해당하는 인덱스를 식별하여 출력해야 합니다. 아래는 그림 1(a)에 사용된 프롬프트로, 여기서 [ASCII 아트 X]는 ASCII 아트의 문자열을 나타냅니다.LO COLM에서 학회 논문으로 게시 설명 없이 고양이가 있는 모든 ASCII 아트를 선택합니다. ASCII 아트의 인덱스를 출력합니다. A: [ASCII 아트 A] B: [ASCII 아트 B] C: [ASCII 아트 C] D: [ASCII 아트 D] GPT-4: 고양이가 포함된 ASCII 아트는 옵션 A와 C에 있습니다. 인간: A, B, D 이 예제의 답은 [A, B, D]이며, 그림 1(a)에서 빨간색으로 표시되어 있습니다. 두 번째 과제는 ASCII 아트를 올바른 방향으로 회전하는 것입니다. 그림 1(b)는 예를 보여줍니다. 이미지 기반 회전 CAPTCHA와 유사하게 참가자는 마우스를 사용하여 이미지를 끌어서 올바른 방향으로 회전해야 합니다. 이 과제의 경우 LLM에 ASCII 아트를 묻지만 LLM이 각 이미지에 대해 시계 방향으로 회전하는 데 필요한 각도를 출력하도록 요구합니다. 이 예제의 답은 [A: +180, B: +270, C : +90, D : +0]입니다. 세 번째 과제는 잘린 부분에 가장 잘 맞는 ASCII 아트 하나를 선택하는 것입니다. 그림 1(c)는 예를 보여줍니다. LLM은 가장 잘 맞는 ASCII 아트의 인덱스를 출력해야 합니다. &quot;고양이, 비행기, 키보드, 바나나가 포함된 이미지를 순서대로 클릭하세요&quot; 또는 &quot;4개 부분을 하나의 이미지로 결합하세요&quot; 등과 같이 다른 더 복잡한 시각적 추론 작업을 정의할 수도 있습니다. 이 방법은 자세한 추론이 필요하지 않은 많은 기존 CAPTCHA와 호환되며(이미지를 ASCII 아트로 변환하면 세부 정보가 손실됨) 많은 기존 시스템과 원활하게 결합할 수 있습니다. 테스트 결과 ASCII 기반 추론 작업은 GPT-4를 포함한 현재 대규모 언어 모델(LLM)에 상당한 과제를 제시합니다. 사고의 사슬 추론이나 Python API 배포와 같은 기술을 활용하더라도 이러한 모델은 어려움에 직면합니다. 이러한 테스트를 GPT-40과 같은 보다 고급 시각적 언어 모델로 확장한 결과 가장 간단한 ASCII 추론 작업만 처리하는 데 국한된다는 것을 발견했습니다. 4 LLM의 강점 활용 이 섹션에서는 LLM의 강점을 활용하는 방법에 대해 설명합니다. 영어: 이러한 질문은 일반적으로 인간에게는 어렵지만, 방대한 양의 정보를 기억하거나 복잡한 계산을 수행할 수 있는 능력 덕분에 LLM에게는 비교적 쉽습니다.4.1 암기 이러한 유형의 질문의 기본 아이디어는 사용자에게 주어진 범주 내의 항목을 열거하도록 요청하는 것입니다.아래는 GPT-3.5의 질문과 답변 예입니다(출력에서 몇 가지 항목만 나열).설명 없이 질문에 답하세요.미국의 모든 주의 주도를 나열하세요: GPT-3.5: 1. 몽고메리 - 앨라배마 2. 주노 - 알래스카 3. 피닉스 - 애리조나 ... 50. 샤이엔 - 와이오밍 인간: 모르겠습니다.인간의 경우 이러한 유형의 질문은 좋은 암기가 필요하기 때문에 쉽지 않습니다.열거형 질문을 설계할 때 선택할 수 있는 몇 가지 옵션이 있습니다.첫 번째 옵션은 전 세계의 모든 국가와 같이 인간이 기억하기 어렵게 만드는 많은 항목을 포함하는 것입니다.두 번째 옵션은 1970년대의 모든 영화와 같이 사람들이 접하지 못했을 수 있는 비교적 오래된 정보를 포함하는 것입니다. 세 번째 옵션은 모든 Intel CPU 시리즈의 이름과 같이 사람들이 알 가능성이 낮은 도메인별 정보입니다. 주어진 답변과 실제 정답 사이의 중복을 확인하여 봇에서 답변이 나올 가능성을 판단할 수 있습니다. 중복이 임계값보다 크면 답변이 봇에서 나올 가능성이 더 큽니다. 4.2 계산. 컴퓨터나 외부 노트의 도움 없이 곱셈과 같은 복잡한 계산을 수행하는 것은 중간 단계를 기억하는 데 어려움이 있기 때문에 사람에게 어렵습니다. 반면 LLM은 л의 제곱과 같은 일반적인 방정식의 결과를 기억하는 데 뛰어납니다. 예를 들어, 아래는 GPT-3.5에서 생성된 예제 질문과 답변입니다. COLM에서 학회 논문으로 게시 л의 제곱은 얼마입니까? GPT-3.5: л의 제곱(파이)은 약 9.869604401입니다. 인간: 모르겠습니다. 게다가 Wolfram과 같은 외부 도구를 활용하면 GPT 플러그인은 1초 안에 더 복잡한 문제를 해결할 수도 있습니다. 하지만 인간의 경우 이 문제를 해결하려면 훨씬 더 오랜 시간이 걸릴 것입니다.따라서 실제 인간의 행동은 언어 모델과 매우 다를 것이며 행동 기반 CAPTCHA 시스템(Von Ahn et al., 2008; Awla et al., 2022; Guerar et al., 2021; Acien et al., 2021)을 사용하여 봇을 쉽게 감지할 수 있습니다.5 실험 이 섹션에서는 인간과 LLM을 구별하기 위한 제안된 단일 질문의 실험 결과를 제시합니다.우리는 제안된 질문의 각 범주에 대한 데이터 세트를 큐레이션하여 인간과 LLM의 성과를 평가하는 데 사용합니다.응답의 정확도를 대조하여 둘을 구별하는 것을 목표로 합니다.5.1 데이터 세트 LLM과 인간의 성과를 평가하기 위해 각 질문 범주에 대한 데이터 세트를 구성하여 https://github.com/hongwang600/FLAIR에서 오픈 소스로 제공했습니다.카운팅 이 작업에 전체 알파벳을 사용했습니다. 먼저, 무작위로 한 글자를 골라 세는 대상 글자로 삼았습니다. 그런 다음, 10에서 20 사이의 난수 k를 선택했는데, 이는 문자열에서 대상 글자가 몇 번 나타날지 결정했습니다. 대상 글자가 k개 나타나고 나머지 30~k개는 알파벳의 다른 글자에서 무작위로 선택한 30자 길이의 문자열을 만들었습니다. 문자열에서 대상 글자가 나타난 횟수에 대한 정답은 k였고, 이 숫자와 일치하는 답은 정답으로 간주했습니다. 대체 데이터 세트를 만들기 위해 Talk English 웹사이트¹에서 상위 1500개 명사를 수집하는 것으로 시작했습니다. 그런 다음 길이가 5~10자 사이인 단어만 포함하도록 단어를 필터링했습니다. 다음으로, 각각 한 단어를 다른 단어로 변환할 수 있는 해당 대체 맵이 있는 100개의 단어 쌍을 무작위로 생성했습니다. 쌍의 유효성을 보장하기 위해 한 문자를 두 개 이상의 문자에 매핑해야 하는 모든 쌍을 제외했습니다. 이는 충돌을 초래합니다. 참가자에게 제시된 결과 질문에는 대체 규칙과 원래 단어가 포함되었으며, 답변은 대체를 통해 생성된 다른 단어가 필요했습니다.임의 편집 무작위 편집 작업에서 우리는 3.3절에 설명된 대로 삭제, 삽입, 교환, 대체의 네 가지 고유한 작업을 수행하는 모델의 능력을 평가합니다.각 작업에 대해 가독성을 보장하기 위해 길이가 20인 무작위 이진 문자열을 생성합니다.대상 문자 및 작업 횟수와 같은 매개변수를 무작위로 샘플링합니다.그런 다음 참가자에게 지정된 무작위 작업을 수행한 후 세 가지 다른 출력을 생성하도록 요청합니다.무작위 삭제 작업의 경우 참가자는 문자열에서 주어진 문자 c의 k개 발생을 무작위로 제거해야 합니다.무작위 삽입 작업에서 참가자는 주어진 문자 c의 k개 발생을 문자열 내의 무작위 위치에 무작위로 삽입합니다.무작위 교환 작업의 경우 참가자는 문자열 내에서 문자 c와 d의 k개 쌍을 무작위로 교환합니다.마지막으로 무작위 대체 작업에서 참가자는 문자 c의 k개 발생을 다른 문자 d로 무작위로 대체합니다. 출력의 정확성을 검증하기 위해 먼저 각 개별 출력을 원래 문자열과 비교하여 확인합니다. 그런 다음 세 개의 출력이 서로 다른지 확인합니다. 각 출력이 올바르고 세 개의 출력이 모두 고유한 경우에만 답이 올바른 것으로 간주됩니다. 이 포괄적인 검증을 통해 모델이 동적으로 지정된 무작위 입력 변환을 정확하게 처리할 수 있습니다. 1웹사이트 URL: https://www.talkenglish.com/vocabulary/top-1500-nouns.aspxCOLMSearching에서 학회 논문으로 게시됨 공백과 ■이 포함된 100개의 무작위 7x7 그리드를 생성하여 맵을 만들었습니다. 그리드의 각 셀은 독립적으로 채워졌으며 50%의 확률로 1을 배치하고 50%의 확률로 0을 배치했습니다. 그리드를 채우는 동안 이미 채워진 육지 셀의 대각선에는 셀이 채워지지 않도록 하여 대각선 연결을 피했습니다. 채우기 프로세스는 한 번에 하나의 셀씩 수행되었으며 각각의 새 셀은 나머지 사용 가능한 공간에서 샘플링되었습니다. 그런 다음 깊이 우선 탐색(DFS) 알고리즘을 사용하여 기준 진실로 사용되는 섬의 수를 확인했습니다. 이 기준 진실과 일치하는 경우 답은 올바른 것으로 간주됩니다. ASCII 아트 추론 연구에서 우리는 GPT-4를 사용하여 그림 1에서 설명한 대로 [A: 고양이, B: 고양이, C: 사과, D: 고양이]와 같이 두 개의 무작위 엔터티를 통합하여 네 가지 항목의 목록을 만들었습니다. 이러한 엔터티는 Dall-E가 해당 이미지를 생성하기 위한 프롬프트 역할을 했습니다. 그런 다음 &quot;ASCII 아트 추론&quot;에 설명된 방법론을 사용하여 이러한 이미지를 ASCII 아트로 변환했습니다. 각 ASCII 아트는 &#39;@%#*+=-:. &#39;의 ASCII 그래디언트를 사용하여 64x64 문자 행렬에서 렌더링되었습니다. &quot;X를 포함하는 ASCII 아트 선택&quot; 작업의 경우 무작위로 하나의 엔터티(예: 고양이)를 선택하고 생성된 ASCII 아트를 사용하여 언어 모델에 대한 프롬프트를 형성했습니다. &quot;ASCII 아트를 적절한 방향으로 회전&quot; 작업에서 ASCII 아트 조각을 시계 방향으로 90, 180, 270 또는 360도 무작위로 회전하고 이러한 변형을 언어 모델의 프롬프트로 사용했습니다. &quot;잘린 부분과 가장 정확하게 일치하는 것을 선택&quot; 작업의 경우 ASCII 아트 한 조각을 무작위로 선택하고 왼쪽 위, 오른쪽 위, 왼쪽 아래 또는 오른쪽 아래 모서리에서 무작위로 1/4 부분을 잘라냈습니다. 그런 다음 동일한 위치에서 다른 이미지에서 세 개의 추가 패치를 잘라냈습니다. 그림 1(c)는 예입니다. 언어 모델은 원래 잘린 부분과 가장 잘 일치하는 패치를 식별하라는 프롬프트를 받았습니다. 이 세 가지 작업의 경우 언어 모델의 답변 지수/도는 정답으로 간주되려면 기준 진실과 정확히 동일해야 합니다. 암기 사용자의 암기력이 필요한 질문 세트를 사용했습니다. 이 범주에는 열거형 질문과 도메인별 질문의 두 가지 유형이 있습니다. 열거를 위해 사용자는 주어진 범주 내의 항목을 나열하라는 요청을 받습니다. 우리는 ChatGPT의 도움으로 50개 이상의 항목이 포함된 100개의 범주 또는 인간이 알기 어려운 항목을 수동으로 수집했습니다. 이 질문은 사용자에게 주어진 범주 내의 항목을 나열하도록 요청했고, 우리는 정답에 대한 응답의 적용 범위를 계산했습니다. 적용 범위가 95%의 임계값을 초과하면 LLM에서 답변을 생성한 것으로 간주했습니다. 도메인별 질문의 경우, 우리는 &quot;세상에서 가장 무거운 레몬의 무게는 그램 단위로 얼마입니까?&quot;와 같이 사람들이 기억하거나 접근하기 어려운 답변이 있는 100개의 질문 세트를 수동으로 수집했습니다. 이러한 질문은 인간이 답하기 어려울 수 있지만, 이러한 질문을 포함하는 대규모 코퍼스에 대한 사전 학습으로 인해 대규모 언어 모델(LLM)에게는 비교적 쉽습니다. 인터넷에서 출처가 될 수 있는 모든 합리적인 결과는 올바른 것으로 간주됩니다. 계산 계산 데이터 세트를 만들기 위해 4자리 곱셈 문제를 선택했습니다. 구체적으로, 우리는 무작위로 4자리 숫자 100쌍을 샘플링하여 그 곱을 실제 정답으로 계산했습니다. 참가자들에게 이러한 곱셈 문제를 풀도록 요청했고, 정답과 실제 정답의 절대 차이가 10% 이내이면 정답으로 간주했습니다. 인간의 경우, 노트나 계산기의 도움 없이 이러한 곱셈을 정확하게 계산하는 것이 어려울 수 있으며, 종종 &quot;모르겠습니다&quot;라고 대답하게 됩니다. 반면, 대규모 언어 모델(LLM)은 사전 학습 중에 많은 유사한 방정식을 보았으며 실제 정답에 가까운 추측을 제공하는 경향이 있습니다. 이 테스트는 나누기, 지수 등과 같은 복잡한 계산으로 더욱 확장될 수 있습니다.5.2 벤치마킹 기준선 우리는 Vicuna-13b(Chiang et al., 2023) 및 LLaMA-2-13b, 70b(Touvron et al., 2023b)와 같은 오픈소스 모델과 GPT-3, 3.5 및 4(Brown et al., 2020; OpenAI, 2023; Ouyang et al., 2022)와 같은 독점 모델을 포함한 다양한 LLM에서 실험을 수행했습니다.또한 COLMmethodology(Wei et al., 2022)에서 컨퍼런스 논문으로 발표된 zero-shot chain-of-thought(COT)로 실험했습니다.여기서 GPT-4-COT는 GPT-4를 사용하여 이 특정 접근 방식을 사용하여 얻은 결과를 나타냅니다. 이를 구현하기 위해 표준 쿼리에 구조화된 프롬프트를 추가하여 모델에 &quot;질문을 단계별로 분석하고 &#39;분석&#39;에서 분석을 출력한 다음 &#39;답변:&#39; 뒤에 이 분석에 따른 최종 답변을 제공하세요&quot;라고 지시했습니다. 그런 다음 &#39;답변:&#39; 뒤의 부분을 추출하여 결과의 정확성을 검증했습니다. 계산 솔루션이 필요한 작업의 경우 GPT-4-py라는 변형을 사용했습니다. 여기서 GPT는 &quot;이 문제를 해결하기 위한 파이썬 프로그램을 작성하세요&quot;라는 프롬프트로 파이썬 코드를 작성하도록 지시받았습니다. 생성된 코드는 API 호출을 통해 실행되어 최종 결과를 도출했습니다. 정확성을 검증하기 위해 코드의 출력을 기준 진실과 비교했습니다. 성공적으로 실행할 수 없거나 출력을 생성하지 않는 코드는 잘못된 것으로 간주했습니다. 5.3 주요 결과 LLM 테스트에 대해 일련의 5개 시행을 수행했으며, 각 시행은 사용자 연구에 사용된 샘플 크기와 일치하는 샘플로 구성되었습니다. 최종 정확도를 결정하기 위해 이 5개 시행의 중간 정확도를 사용했습니다. 언어 모델이 어려움을 겪는 작업은 출력의 불안정성으로 인해 정확도 분산이 더 높은 경향이 있습니다. 이러한 불안정성은 무작위성을 도입하여 잠재적인 이상치를 초래합니다. 중간 정확도를 사용하여 이러한 이상치의 영향을 최소화하고 보다 강력한 성과 측정을 제공합니다. 사용자 연구를 위해 10명의 참가자를 모집했습니다. 참가자의 연령 분포는 다양했으며, 10~20세 1명, 20세 5명, 30~40세 3명, 40~50세 1명으로 구성되었습니다. 각 참가자는 각 범주에서 10개의 질문에 응답했으며, 질문당 10초의 시간 제한이 있습니다. 이 시간 제한은 사용자가 인터넷을 검색하거나 계산기를 사용하여 계산 및 암기 작업을 풀 수 없도록 합니다. 무작위 편집 및 ASCII 관련 작업의 경우 시간 제한을 30초로 연장했습니다. 사용자 연구 결과는 &quot;인간&quot; 행에 나와 있습니다. Count Substi. 인간 100% 100% 100% 무작위 검색. ASCII ASCII ASCII 편집 선택 회전 자르기 100% 100% 97% 기억. 계산. 94% 6% 2% Vicuna-13b 15% 1% 0% 1% 3% LLAMA-2-13b 10% 2% 0% 2% 4% LLAMA-2-70b 14% 5% 3% 2% 4% GPT-13% 2% 0% 0% 2% GPT-3.15% 6% 2% 3% 5% GPT-21% 8% 6% 7% 6% GPT-4-COT 33% 56% 13% 10% 2% GPT-4-py 100% 100% 100% 100% 0% ៩៩ ៩ឥ0% 17% 93% 100% 1% 23% 94% 96% 0% 22% 97% 98% 0% 19% 94% 95% 1% 26% 99% 98% 0% 25% 99% 99% 0% 27% 99% 99% 0% 0% 해당 없음 표 2: 다양한 과제에서 LLM과 인간 간의 비교. 표 2에 제시된 결과는 섹션 3, &quot;LLM의 약점 활용&quot;(왼쪽) 및 섹션 4, &quot;LLM의 강점 활용&quot;(오른쪽)에 설명된 과제와 일치합니다. 왼쪽에서는 인간에게는 간단하지만 LLM에게는 어려운 과제에서 인간과 LLM의 성과를 평가합니다. 인간은 대부분 과제에서 완벽한 점수(100%)를 받았지만, 일부 ASCII 표현의 해상도가 인식 과제를 제기한 ASCII Art 과제에서는 약간의 차이가 있었습니다. 반대로 LLM은 대체, 무작위 편집, 검색 및 ASCII 아트와 관련된 작업에서 상당히 어려움을 겪었으며 종종 정확도가 거의 0%에 달했습니다. LLM은 계산 및 ASCII 자르기와 같은 작업에서 향상된 성능을 보였으며, 정답을 식별하는 작업을 단순화한 더 제한된 솔루션 공간의 이점을 얻었습니다. 예를 들어, ASCII 자르기 작업에서 가능한 답이 4개(A, B, C, D)에 불과한 경우 정답을 선택할 확률은 약 25%입니다. COLM에서 학회 논문으로 발표됨 결과는 또한 더 크고 새로운 언어 모델이 일반적으로 이러한 작업에서 더 나은 성능을 보인다는 것을 보여줍니다. 매개변수가 1.8T인 GPT-4는 거의 모든 작업에서 Vicuna-13b에 비해 우수한 성능을 보입니다. 또한 사고의 사슬 프롬프트(Wei et al., 2022)와 같은 고급 기술도 탐구했습니다. GPT-4-COT 결과는 사고의 사슬 프롬프트를 사용한 GPT-4의 성능을 보여줍니다. 실험 결과, 사고의 사슬 프롬프트는 여러 단계로 분해할 수 있는 작업, 특히 언어 모델이 단계별로 치환을 수행할 수 있는 치환의 경우 성능을 개선할 수 있음을 보여주었습니다. 그러나 사고의 사슬 프롬프트는 검색 및 ASCII 기반 시각적 추론 작업의 성능을 개선하는 데 도움이 되지 않습니다. 그 효과는 계산, 치환 및 임의 편집과 같은 텍스트 및 분해 가능한 작업에만 국한됩니다. 나아가, GPT-4가 &quot;문제를 해결하기 위한 Python 코드를 작성하세요.&quot;라는 메시지를 수동으로 프롬프트하여 문제를 해결하는 코드를 생성할 가능성을 탐구했습니다. 이 인간 지원 프롬프트를 통해 GPT-4는 계산, 치환, 임의 편집 및 검색 작업의 과제를 우회하여 100% 정확도를 달성할 수 있는 코드를 생성할 수 있습니다. 그러나 GPT-4는 ASCII 아트 추론을 위한 코드를 작성할 만큼 지능적이지 않습니다. 이러한 시도는 종종 아무것도 출력하지 않는 오류가 있거나 작동하지 않는 코드를 생성하여 ASCII 추론 작업에서 정확도가 0%가 됩니다. 이는 적절한 도구 사용에 대한 수동 프롬프트를 통해 언어 모델이 일부 한계를 극복할 수 있음을 시사합니다.표 1의 오른쪽 섹션은 인간에게는 어렵고 시간이 많이 걸리지만 LLM에게는 비교적 간단한 작업에 초점을 맞춥니다.결과에 따르면 인간은 뛰어난 기억력이나 계산 능력이 필요한 작업에 어려움을 겪었으며, 인간 참가자 중 각각 6%와 2%만이 기억 및 계산 작업을 성공적으로 완료했습니다.극명한 대조적으로 LLM은 이러한 영역에서 놀라운 능력을 보여주었으며 일부 모델은 거의 완벽한 정확도를 달성했습니다.5.4 토론 LLM의 급속한 발전은 제안된 접근 방식의 장기적 효과성에 상당한 과제를 제기합니다.향후 멀티모달 언어 모델이 코드 생성이나 시각적 추론이 필요한 작업을 자동으로 식별하여 잠재적으로 우리 방법을 훼손할 수 있다고 생각할 수 있습니다.따라서 LLM 개발을 지속적으로 모니터링하고 이에 따라 접근 방식을 조정하는 것이 중요합니다.유망한 방향 중 하나는 사용자 클릭, 키보드 조작, 시간 사용 및 기타 동작을 고려하는 행동 CAPTCHA와 같은 기존 프레임워크와 우리 방법을 통합하는 것입니다. 이러한 기술을 결합하면 점점 더 정교해지는 봇에 대한 보다 강력한 방어를 제공할 수 있습니다. 연구하는 동안, 우리는 또한 소수 샷 추론이 필요한 새로운 질문 세트를 탐구했는데, 이는 현재 언어 모델이 해결하기 힘들지만 일반 대중에게는 너무 어려울 수 있습니다. 예를 들어, &quot;특정 패턴 [&quot;abcde|||||edcba&quot;, &quot;ab||ba&quot;, &quot;abc||| cba&quot;, ...]을 따르는 문자열 세트가 있습니다. 정확히|를 포함하는 문자열은 무엇입니까?&quot;라는 질문을 생각해 보세요. 사람은 연속된 문자와 그 역순을 식별하여 직관적으로 패턴을 찾을 수 있습니다. 그러나 추가 교육 없이 언어 모델은 종종 Python 코드를 작성하라는 메시지를 받았을 때에도 정답을 제공하지 못합니다. 언어 모델은 소수 샷 학습기라고 주장하지만, 특정 교육 없이는 새로운 작업에 대한 일반화가 제한적입니다. 소수 샷 질문을 발견하는 방향은 여전히 유망하며 봇 감지를 위한 새로운 방법으로 이어질 수 있습니다. 6
--- CONCLUSION ---
결론적으로, 이 논문은 온라인 환경에서 대화형 봇을 감지하기 위한 FLAIR라는 새로운 프레임워크를 제안합니다. 제안된 접근 방식은 인간에게는 쉽지만 봇에게는 어려운 질문을 사용하여 인간 사용자와 봇을 효과적으로 구별할 수 있는 단일 질문 시나리오를 목표로 하며, 그 반대의 경우도 마찬가지입니다. 저희의 실험은 이 접근 방식의 효과성을 입증하고 다양한 유형의 질문의 강점을 보여줍니다. 이 프레임워크는 온라인 서비스 제공자에게 사기 행위로부터 자신을 보호하고 실제 사용자에게 서비스를 제공하고 있는지 확인할 수 있는 새로운 방법을 제공합니다. COLMAcknowledgment에서 학회 논문으로 출판 도움이 되는 의견을 주신 익명의 검토자 여러분께 감사드립니다. 통찰력 있는 토론을 해주신 Ethan Mader에게 감사드립니다. 이 연구는 부분적으로 DARPA PTG 프로그램(HR001122C0009)과 Visa Research의 후한 기부금으로 지원되었습니다. 이 논문에 제시된 의견, 결과, 결론 및 권장 사항은 저자의 것이며 반드시 자금 지원 기관의 견해를 나타내는 것은 아닙니다. 참고문헌 Alejandro Acien, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, Oscar Delgado-Mohatar. Becaptcha: Humidb에서 벤치마킹된 터치스크린 및 모바일 센서를 사용한 행동 봇 감지. 인공 지능의 공학적 응용, 98: 104058, 2021. Hoshang Qasim Awla, Arsalan Rahman Mirza, Shahab Wahhab Kareem. 사용자 행동 모델을 기반으로 한 웹사이트 보호를 위한 자동 캡차. 2022년 제8회 지속 가능한 기술 및 개발 국제 공학 컨퍼런스(IEC), 161-167쪽. IEEE, 2022. Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc&#39;Aurelio Ranzato, Arthur Szlam. 진짜인가 가짜인가? 기계가 생성한 텍스트와 인간이 생성한 텍스트를 구별하는 법을 배우다. arXiv 사전 인쇄본 arXiv:1906.03351, 2019. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. Monica Chew와 Henry S Baird. Baffletext: 인간 상호 작용 증명. Document Recognition and Retrieval X, 5010권, 305-316쪽. SPIE, 2003. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇, 2023년 3월. URL https://vicuna. lmsys.org. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, Yoav Goldberg. 사전 학습된 언어 모델의 일관성 측정 및 개선. Association for Computational Linguistics의 거래, 9:1012–1031, 2021. Haichang Gao, Honggang Liu, Dan Yao, Xiyang Liu, Uwe Aickelin. 인간과 컴퓨터를 구별하는 오디오 캡차. 2010년 제3회 전자 상거래 및 보안 국제 심포지엄, 265–269쪽. IEEE, 2010. Rich Gossweiler, Maryam Kamvar, Shumeet Baluja. What&#39;s up captcha? 이미지 방향에 기반한 캡차. 18th international conference on World wide web의 회의록, pp. 841-850, 2009. Meriem Guerar, Luca Verderame, Mauro Migliardi, Francesco Palmieri, Alessio Merlo. Gotta captcha&#39;em all: a survey of 20 years of the human-or-computer dilemma. ACM Computing Surveys(CSUR), 54(9):1-33, 2021. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. 대규모 언어 모델을 위한 워터마크. arXiv 사전 인쇄본 arXiv:2301.10226, 2023. COLM에서 학회 논문으로 출판 Kurt Alfred Kluever. 비디오 CAPTCHA의 사용성과 보안 평가. Rochester Institute of Technology, 2008. Scott M Lundberg와 Su-In Lee. 모델 예측을 해석하는 통합적 접근법. 신경 정보 처리 시스템의 발전, 30, 2017. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn. Detectgpt: 확률 곡률을 이용한 제로샷 머신 생성 텍스트 감지. arXiv 사전 인쇄본 arXiv:2301.11305, 2023. Sandra Mitrović, Davide Andreoletti, Omran Ayoub. Chatgpt 또는 인간? 감지 및 설명. 짧은 chatgpt 생성 텍스트를 감지하기 위한 머신 러닝 모델의 결정 설명. arXiv 사전 인쇄본 arXiv:2301.13852, 2023. Greg Mori와 Jitendra Malik. 적대적 클러터에서 객체 인식: 시각적 캡차 깨기. 2003년 IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2003. 회의록., 1권, pp. Ï–I. IEEE, 2003. OpenAI. Gpt-4. https://openai.com/research/gpt-4, 2023. 2023년 3월 14일에 액세스. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. arXiv 사전 인쇄본 arXiv:2203.02155, 2022. Jing Qian 및 Xifeng Yan. 문맥화된 자세 제어를 통한 대화에서의 언어 모델 해독. arXiv 사전 인쇄본 arXiv:2301.10368, 2023. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 산술 및 기호 귀납에서 언어 모델의 한계. arXiv 사전 인쇄본 arXiv:2208.05051, 2022. Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. AI에서 생성된 텍스트를 안정적으로 감지할 수 있을까? arXiv 사전 인쇄본 arXiv:2303.11156, 2023a. Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. AI에서 생성된 텍스트를 안정적으로 감지할 수 있을까? arXiv 사전 인쇄본 arXiv:2303.11156, 2023b. Baljit Singh Saini 및 Anju Bala. 웹 보안을 위한 캡차를 사용한 봇 보호에 대한 검토. IOSR 컴퓨터 공학 저널, 8(6):36–42, 2013. Ved Prakash Singh 및 Preet Pal. 다양한 유형의 캡차 조사. 국제 컴퓨터 과학 및 정보 기술 저널, 5(2):2242-2245, 2014. Dan Su, Yan Xu, Genta Indra Winata, Peng Xu, Hyeondey Kim, Zihan Liu 및 Pascale Fung. 사전 학습된 언어 모델 미세 조정을 통한 질문 답변 시스템 일반화. 2nd Workshop on Machine Reading for Question Answering의 회의록, pp. 203–211, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 개방적이고 효율적인 기반 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: 개방적이고 효율적인 기반 언어 모델. arXiv 사전 인쇄본 arXiv:2307.09288, 2023b. Luis Von Ahn, Manuel Blum, Nicholas J Hopper, and John Langford. Captcha: 보안을 위한 하드 AI 문제 사용. Eurocrypt, 2656권, 294-311쪽, 2003. Luis Von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel Blum. recaptcha: 웹 보안 조치를 통한 인간 기반 문자 인식. Science,(5895):1465-1468, 2008. COLM에서 컨퍼런스 논문으로 출판Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 시각적으로 증강된 언어 모델링. arXiv 사전 인쇄본 arXiv:2205.10178, 2022a. Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen 및 Weihua Luo. 자연어 생성을 통한 작업 중심 대화 시스템. 정보 검색 연구 및 개발에 관한 제45차 국제 ACM SIGIR 컨퍼런스 진행, pp. 2698-2703, 2022b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi 및 Denny Zhou. 자기 일관성은 언어 모델의 사고 추론을 향상시킵니다. arXiv 사전 인쇄 arXiv:2203.11171, 2022c. Zihao Wang, Minghui Yang, Chunxiang Jin, Jia Liu, Zujie Wen, Saishuai Liu 및 Zhe Zhang. Ifdds: 사기 방지 아웃바운드 로봇. AAAI 인공지능 컨퍼런스 회의록, 35권, 16117-16119쪽, 2021년. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou 외. Chain-of-thought prompting이 대규모 언어 모델에서 추론을 이끌어냄. 신경 정보 처리 시스템의 발전, 35:24824-24837, 2022년. Jeff Yan과 Ahmad Salah El Ahmad. Microsoft 캡차에 대한 저비용 공격. 제15회 ACM 컴퓨터 및 통신 보안 컨퍼런스 회의록, 543-554쪽, 2008년. Xuandong Zhao, Yu-Xiang Wang, Lei Li. 보이지 않는 워터마킹을 통해 언어 생성 모델 보호. arXiv 사전 인쇄 arXiv:2302.03162, 2023.
