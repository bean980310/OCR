--- ABSTRACT ---
딥 러닝 소프트웨어 라이브러리의 개발은 사용자가 모델링에 집중할 수 있게 함으로써 이 분야에서 상당한 진전을 이룰 수 있게 했고, 라이브러리는 현대 하드웨어 가속기의 실행을 최적화하는 지루하고 시간 소모적인 작업을 처리할 수 있게 되었습니다. 그러나 이는 Transformers와 같은 특정 유형의 딥 러닝 모델에만 도움이 되었으며, 그 기본 요소는 벡터화된 계산에 쉽게 매핑됩니다. 트리 및 분할과 같은 구조화된 객체를 명시적으로 고려하는 모델은 벡터화된 형태로 구현하기 어려운 사용자 지정 알고리즘이 필요하기 때문에 동등한 이점을 얻지 못했습니다. SynJax는 정렬, 태그 지정, 분할, 구성 트리 및 스패닝 트리를 포함하는 구조화된 분포에 대한 추론 알고리즘의 효율적인 벡터화된 구현을 제공함으로써 이 문제를 직접 해결합니다. 이는 자동 미분과 확률적 추론을 위한 알고리즘 간의 연결을 활용하여 수행됩니다. SynJax를 사용하면 데이터의 구조를 명시적으로 모델링하는 대규모 미분 가능 모델을 빌드할 수 있습니다. 코드는 https://github.com/google-deepmind/synjax에서 사용할 수 있습니다. 1
--- CONCLUSION ---
구조화된 분포에 대한 딥 신경 모델을 만드는 데 있어 주요 과제 중 하나는 최신 하드웨어 가속기에서 구현하기 어렵다는 것입니다. SynJax는 이 문제를 해결하고 JAX에서 구조화된 모델의 대규모 학습을 실행 가능하고 쉽게 만듭니다. 이를 통해 구조화된 데이터의 자기 회귀 모델링에 대한 대안을 찾는 연구가 촉진되기를 바랍니다. 제한 사항 SynJax는 매우 빠르지만 개선이 필요한 영역이 여전히 있습니다. 주요 속도 및 메모리 병목 현상 중 하나는 로그 분할 함수 계산에 필요한 동적 프로그래밍 알고리즘에서 큰 임시 텐서를 사용하는 것입니다. 이는 Pallas로 작성된 사용자 정의 커널로 최적화할 수 있습니다.2 개념적으로 간단할 수 있는 속도 향상 2https://jax.readthedocs.io/en/latest/pallas가 있지만 특수 하드웨어가 있어야 합니다. 예를 들어, 반링을 사용한 행렬 곱셈은 현재 GPU의 TensorCore와 같은 행렬 곱셈에 하드웨어 가속을 사용하지 않고 대신 일반 CUDA 코어로 계산을 수행합니다. 우리는 log-einsum-exp 트릭(Peharz et al., 2020)으로 이 문제를 해결하려고 시도했지만, 결과적인 계산은 브로드캐스팅을 사용한 일반적인 log-semiring을 사용하는 것보다 수치적으로 덜 정확했습니다.최대 신장 트리 알고리즘은 현재 최적화된 Numba CPU 코드로 실행 중이므로 벡터화할 수 있다면 훨씬 더 빠를 것입니다.감사의 말 이 작업의 초안에 대한 초기 의견을 제공해 준 Chris Dyer, Aida Nematzadeh 및 Google DeepMind의 언어 팀의 다른 멤버에게 감사드립니다.SynJax 개발을 훨씬 더 쉽게 만들어 준 Patrick Kidger의 Equinox 및 Jaxtyping 작업에 감사드립니다.또한 Sasha Rush가 SynJax의 많은 측면에 영향을 준 라이브러리인 Torch-Struct를 오픈 소스로 공개한 것에 감사드립니다.참고 문헌 Ossama Abdel-Hamid, Li Deng, Dong Yu 및 Hui Jiang.2013. 음성 인식을 위한 심층 세그먼트 신경망.Interspeech, 36권에서.Wilker Aziz. 2015. Grasp: Randomised Semiring Parsing. 프라하 수학 언어학 게시판, 104:51-62. 이고르 바부슈킨, 케이트 바움리, 앨리슨 벨, 수리야 부파티라주, 제이크 브루스, 피터 부클롭스키, 데이비드 버든, 트레버 카이, 에이던 클라크, 이보 다니헬카, 앙트완 드듀, 클라우디오 판타치, 조나단 고드윈, 크리스 존스, 로스 헴슬리, 톰 헤니건, 마테오 헤셀, 샤오보 호우, 스티븐 캅투로프스키, 토마스 케크, 유리 케마에프, 마이클 킹, 마르쿠스 쿠네쉬, 레나 마르텐스, 함자 메르지치, 블라디미르 미쿨릭, 타마라 노먼, 조지 파파마카리오스, 존 콴, 로만 링, 프란시스코 루이스, 알바로 산체스, 로랑 사르트란, 로살리아 슈나이더, 에렌 세제너, 스티븐 스펜서, 스리바찬 스리니바산, 밀로스 스타노예비치, 보이치에흐 스토코비에츠, 루위 왕, 광야오 저우, 파비오 비올라. 2020. DeepMind JAX 생태계. Jasmijn Bastings, Wilker Aziz, Ivan Titov. 2019. 미분 가능한 이진 변수를 사용한 해석 가능한 신경 예측. 이탈리아 피렌체에서 열린 제57회 전산 언어학 협회 연례 회의록, 2963-2977쪽. 전산 언어학 협회. Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima&#39;an. 2017. 구문 인식 신경 기계 번역을 위한 그래프 합성 인코더. 덴마크 코펜하겐에서 열린 2017년 자연어 처리 경험적 방법 컨퍼런스록, 1957-1967쪽. 전산 언어학 협회. Yonatan Bisk, Ke Tran. 2018. 신경 기계 번역을 사용한 문법 유도. 2nd Workshop on Neural Machine Translation and Generation의 회의록, 25-35페이지, 호주 멜버른. Association for Computational Linguistics. Oscar Chang, Dongseong Hwang, and Olivier Siohan. 2023. Revisiting the Entropy Semiring for Neural Speech Recognition. 제11회 International Conference on Learning Representations. Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy. 2022. Dynamax. Do Kook Choe and Eugene Charniak. 2016. Parsing as language modeling. Conference on Empirical Methods in Natural Language Processing의 회의록, 2331-2336페이지, 텍사스 오스틴. Association for Computational Linguistics. Shay B. Cohen, Giorgio Satta, and Michael Collins. 2013. 텐서 분해를 사용한 대략적인 PCFG 파싱. 2013년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 487-496쪽, 조지아주 애틀랜타. 컴퓨터 언어학 협회. Charles J. Colbourn, Wendy J. Myrvold, Eugene Neufeld. 1996. 수목의 순위를 매기지 않는 두 가지 알고리즘. 알고리즘 저널, 20(2):268–281. Caio Corro와 Ivan Titov. 2019. Differentiable Perturband-Parse: Structured Variational Autoencoder를 사용한 반지도 파싱. 국제 학습 표현 회의. Corinna Cortes, Mehryar Mohri, Ashish Rastogi, Michael Riley. 2008. 확률적 오토마타의 상대적 엔트로피 계산에 관하여. International Journal of Foundations of Computer Science, 19(01):219-242. David F Crouse. 2016. 2D 직사각형 할당 알고리즘 구현에 관하여. IEEE Transactions on Aerospace and Electronic Systems, 52(4):1679–1696. Marco Cuturi and Mathieu Blondel. 2017. Soft-DTW: 시계열에 대한 미분 가능 손실 함수. 제34회 국제 기계 학습 컨퍼런스 회의록 - 제70권, ICML&#39; 17, 894-903페이지. JMLR.org. Adnan Darwiche. 2003. 베이지안 네트워크에서 추론에 대한 미분적 접근 방식. J. ACM, 50(3):280–305. Nathan Day, Andrew Hemmaplardh, Robert E. Thurman, John A. Stamatoyannopoulos, and William S. Noble. 2007. 연속 게놈 데이터의 무감독 분할. Bioinformatics, 23(11):1424– 1426. Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous. 2017. TensorFlow 분포. Chris Dyer, Adhguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. 순환 신경망 문법. 2016년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 199–209쪽, 캘리포니아주 샌디에이고. Association for Computational Linguistics. Jason Eisner. 1996. 종속성 구문 분석을 위한 새로운 확률적 모델 3개: 탐색. COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics. Jason Eisner. 2002. 확률적 유한 상태 변환기의 매개변수 추정. Association for Computational Linguistics의 40번째 연례 회의록, 1-8페이지, 미국 펜실베이니아주 필라델피아. Association for Computational Linguistics. Jason Eisner. 2016. Inside-Outside 및 ForwardBackward 알고리즘은 Backprop에 불과하다(튜토리얼 논문). NLP를 위한 구조적 예측 워크숍의 회의록, 1-17페이지, 텍사스주 오스틴. Association for Computational Linguistics. Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, and Alexander M. Rush. 2020. Latent template induction with gumbel-crfs. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&#39; 20, Red Hook, NY, USA. Curran Associates Inc. Joshua Goodman. 1999. Semiring Parsing. Computational Linguistics, 25(4):573–606. Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006. Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In Proceedings of the 23rd International Conference on Machine Learning, 369–376쪽. Andreas Griewank. 1992. 역 자동 미분에서 시간적 및 공간적 복잡성의 대수적 증가 달성. 최적화 방법 및 소프트웨어, 1(1):35–54. Awni Hannun. 2017. CTC를 사용한 시퀀스 모델링. Distill. https://distill.pub/2017/ctc. Serhii Havrylov, Germán Kruszewski, Armand Joulin. 2019. 분리된 구문 및 의미론의 협력 학습. 2019년 북미 전산 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문 및 짧은 논문), 1118~1128쪽, 미네소타주 미니애폴리스. 전산 언어학 협회. Julia Hockenmaier, Aravind K Joshi, Ken A Dill. 2007. 경로는 트리입니다. 단백질 접힘에 대한 구문 분석 관점. 단백질: 구조, 기능 및 생물정보학, 66(1):1–15. Liang Huang, He Zhang, Dezhong Deng, Kai Zhao, Kaibo Liu, David A Hendrix, David H Mathews. 2019. LinearFold: 5&#39;-to-3&#39; 동적 프로그래밍 및 빔 검색을 통한 선형 시간 근사 RNA 폴딩. 생물정보학, 35(14):i295-i304. Rebecca Hwa. 2000. 통계적 문법 유도를 위한 샘플 선택. 2000년 공동 SIGDAT 자연어 처리 및 초대형 코퍼스의 경험적 방법에 대한 컨퍼런스, 45-52페이지, 홍콩, 중국. 계산언어학 협회. Patrick Kidger 및 Cristian Garcia. 2021. Equinox: 호출 가능 PyTrees 및 필터링된 변환을 통한 JAX의 신경망. Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. 2017. 구조화된 주의 네트워크. 국제 학습 표현 컨퍼런스에서. 윤 킴, 크리스 다이어, 알렉산더 러시. 2019. 문법 유도를 위한 복합 확률적 문맥 자유 문법. 이탈리아 피렌체에서 열린 제57회 전산 언어학 협회 연례 회의록, 2369-2385쪽. 전산 언어학 협회. 니키타 키타에프, 스티븐 카오, 댄 클라인. 2019. 자기 주의와 사전 학습을 통한 다국어 구성 요소 구문 분석. 이탈리아 피렌체에서 열린 제57회 전산 언어학 협회 연례 회의록, 3499-3505쪽. 전산 언어학 협회. 테리 쿠, 아미르 글로버슨, 자비에르 카레라스, 마이클 콜린스. 2007. Matrix-Tree 정리를 통한 구조적 예측 모델. 2007년 자연어 처리 및 계산 자연어 학습(EMNLP-CONLL)의 경험적 방법에 대한 공동 컨퍼런스 회의록, 141-150쪽, 체코 프라하. 계산 언어학 협회. Marco Kuhlmann, Carlos Gómez-Rodríguez, Giorgio Satta. 2011. 전환 기반 종속성 파서를 위한 동적 프로그래밍 알고리즘. 계산 언어학 협회: 인간 언어 기술의 제49회 연례 회의록, 673-682쪽, 미국 오리건주 포틀랜드. 계산 언어학 협회. John D. Lafferty, Andrew McCallum, Fernando CN Pereira. 2001. 조건부 랜덤 필드: 시퀀스 데이터 분할 및 레이블 지정을 위한 확률적 모델. 미국 캘리포니아주 샌프란시스코에서 열린 제18회 기계 학습 국제 컨퍼런스의 회의록, ICML &#39;01, 282-289페이지. Morgan Kaufmann Publishers Inc. Siu Kwan Lam, Antoine Pitrou, Stanley Seibert. 2015. Numba: llvm 기반 파이썬 jit 컴파일러. 미국 뉴욕에서 열린 HPC의 LLVM 컴파일러 인프라에 대한 제2회 워크숍의 회의록, LLVM &#39;15. Association for Computing Machinery. Zhifei Li와 Jason Eisner. 2009. 번역 숲에서 최소 위험 학습에 대한 응용 프로그램을 갖춘 1차 및 2차 기대 반고리. 2009년 자연어 처리 경험적 방법 컨퍼런스의 회의록, 40-51페이지, 싱가포르. Association for Computational Linguistics. Liang Lu, Lingpeng Kong, Chris Dyer, Noah A Smith, Steve Renals. 2016. 종단간 음성 인식을 위한 세그먼트 순환 신경망. 제17회 국제 음성 커뮤니케이션 협회 연례 회의록(INTERSPEECH 2016). Chunchuan Lyu와 Ivan Titov. 2018. 잠재 정렬을 사용한 그래프 예측으로서의 AMR 구문 분석. Association for Computational Linguistics 제56회 연례 회의록(제1권: 장문 논문), 397-407쪽, 멜버른, 호주. Association for Computational Linguistics. Xuezhe Ma와 Eduard Hovy. 2016. 양방향 LSTM-CNNS-CRF를 통한 종단간 시퀀스 라벨링. 제54회 Association for Computational Linguistics 연례 회의록(제1권: 장문 논문), 1064-1074쪽, 독일 베를린. Association for Computational Linguistics. André Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mário Figueiredo. 2010. Turbo parsers: Dependency parsing by Approachal variational inference. 2010 Conference on Empirical Methods in Natural Language Processing, 3444쪽, 매사추세츠주 케임브리지. Association for Computational Linguistics. Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. 2018. Learning Latent Permutations with Gumbel-Sinkhorn Networks. In International Conference on Learning Representations. Arthur Mensch and Mathieu Blondel. 2018. 구조적 예측 및 주의를 위한 미분 가능 동적 프로그래밍. 제35회 기계 학습 국제 컨퍼런스 회의록, 기계 학습 연구 회의록, 3462-3471페이지. PMLR. Tsvetomila Mihaylova, Vlad Niculae, and André FT Martins. 2020. SPIGOT의 메커니즘 이해: 잠재 구조 학습을 위한 대체 기울기. 2020년 자연어 처리 경험적 방법 컨퍼런스 회의록(EMNLP), 2186-2202페이지, 온라인. 계산 언어학 협회. Kevin P. Murphy. 2012. 기계 학습: 확률적 관점. 적응형 계산 및 기계 학습 시리즈. MIT 출판부. Maria Nădejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, Alexandra Birch. 2017. 대상 언어 CCG 슈퍼태그 예측으로 신경망 기계 번역 개선. 덴마크 코펜하겐에서 열린 제2회 기계 번역 회의록, 68-79쪽. 계산언어학 협회. SB Needleman과 CD Wunsch. 1970. 두 단백질의 아미노산 서열에서 유사성을 검색하는 데 적용할 수 있는 일반적인 방법. Journal of Molecular Biology, 48:443–453. Mathias Niepert, Pasquale Minervini, Luca Franceschi. 2021. 암묵적 mle: 이산 지수 패밀리 분포를 통한 역전파. 신경 정보 처리 시스템의 발전, 34:14567-14579. Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, Chris J Maddison. 2020. 확률적 소프트맥스 트릭을 사용한 그래디언트 추정. 신경 정보 처리 시스템의 발전, 33권, 5691-5704페이지. Curran Associates, Inc. Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van Den Broeck, Kristian Kersting, Zoubin Ghahramani. 2020. Einsum 네트워크: 추적 가능한 확률 회로의 빠르고 확장 가능한 학습. 제37회 기계 학습 국제 컨퍼런스, ICML&#39;20의 회의록. JMLR.org. Du Phan, Neeraj Pradhan, Martin Jankowiak. 2019. NumPyro에서 유연하고 가속화된 확률적 프로그래밍을 위한 구성 가능한 효과. 2019년 NeurIPS에서 열린 ML 워크숍의 Program Transformations에서. Marin Vlastelica Pogančić, Anselm Paulus, Vit Musil, Georg Martius, Michal Rolinek. 2020. Differentiation of Blackbox Combinatorial Solvers. International Conference on Learning Representations에서. Lawrence R. Rabiner. 1990. A tutorial on hidden markov models and selected applications in speech awareness. Readings in Speech Recognition, 267-296쪽. Elsevier. Alexander Rush. 2020. Torch-Struct: Deep Structured Prediction Library. Association for Computational Linguistics: System Demonstrations의 제58회 연례 회의록, 335-342쪽, 온라인. Association for Computational Linguistics. Sakakibara, Underwood, Mian, Haussler. 1994. RNA 모델링을 위한 확률적 문맥 자유 문법. 1994년 제27회 하와이 국제 시스템 과학 컨퍼런스 회의록, 5권, 284-293페이지. IEEE. Sunita Sarawagi와 William W Cohen. 2004. 정보 추출을 위한 Semimarkov 조건부 난수 필드. 신경 정보 처리 시스템의 발전, 17권. MIT 출판부. Laurent Sartran, Samuel Barrett, Adhigana Kuncoro, Miloš Stanojević, Phil Blunsom, Chris Dyer. 2022. Transformer Grammars: 대규모 구문적 귀납적 편향을 가진 Transformer 언어 모델 증강. 계산 언어학 협회의 거래, 10:1423-1439. David A. Smith와 Noah A. Smith. 2007. 비투영 종속 트리의 확률적 모델. 2007년 자연어 처리 및 계산 자연어 학습(EMNLPCoNLL)에 대한 경험적 방법에 관한 공동 컨퍼런스의 회의록, 132-140페이지, 체코 프라하. 계산 언어학 협회. Miloš Stanojević. 2022. 종속성 트리의 편향되지 않고 효율적인 샘플링. 2022년 자연어 처리에 대한 경험적 방법에 관한 컨퍼런스의 회의록, 1691-1706페이지, 아랍에미리트 아부다비. 계산 언어학 협회. Miloš Stanojević 및 Shay B. Cohen. 2021. 문제의 근원: 단일 루트 종속성 구문 분석 최적화. 2021년 자연어 처리 경험적 방법 컨퍼런스 회의록, 10540-10557쪽, 온라인 및 도미니카 공화국 푼타카나. Association for Computational Linguistics. Mitchell Stern, Jacob Andreas, Dan Klein. 2017. 최소 범위 기반 신경 구성 요소 파서. Association for Computational Linguistics 제55회 연례 회의 회의록(제1권: 장문 논문), 818-827쪽, 캐나다 밴쿠버. Association for Computational Linguistics. Charles Sutton, Andrew McCallum. 2007. 관계 학습을 위한 조건부 난수 필드 소개. 통계적 관계 학습 소개, 93쪽. Simo Särkkä, Ángel F. García-Fernández. 2021. 베이지안 스무더의 시간적 병렬화. IEEE 자동 제어 거래, 66(1):299– 306. RE Tarjan. 1977. 최적 분기 찾기. 네트워크, 7(1):25-35. Ke M. Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, Kevin Knight. 2016. 비지도 신경망 은닉 마르코프 모델. NLP를 위한 구조적 예측 워크숍 회의록, 63-71페이지, 텍사스 오스틴. 계산 언어학 협회. WT Tutte. 1984. 그래프 이론, 수학 백과사전 및 응용 프로그램 21권. AddisonWesley, 캘리포니아 멘로 파크. LG Valiant. 1979. 영구 계산의 복잡성. 이론 컴퓨터 과학, 8(2):189–201. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew RJ Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, EA Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt 및 SciPy 1.0 기여자. 2020. SciPy 1.0: Python에서 과학적 컴퓨팅을 위한 기본 알고리즘. Nature Methods, 17:261–272. Zhiyong Wang 및 Jinbo Xu. 2011. RNA 시퀀스-구조 관계 모델링 및 형태 샘플링을 위한 조건부 랜덤 필드 방법. Bioinformatics, 27(13):i102-i110. Ronald J. Williams. 1992. 연결주의 강화 학습을 위한 간단한 통계적 기울기 추종 알고리즘. Mach. Learn., 8(3-4):229–256. David Bruce Wilson. 1996. 커버 타임보다 더 빠르게 랜덤 스패닝 트리 생성. 제28회 ACM 컴퓨팅 이론 심포지엄 회의록, STOC &#39;96, 296-303페이지, 뉴욕, 뉴욕, 미국. Association for Computing Machinery. Songlin Yang, Wei Liu, and Kewei Tu. 2022. Dynamic programming in rank space: Scaling structured inference with low-rank HMMs and PCFGs. 2022년 북미 전산언어학회: 인간언어기술 학회 회의록, 4797-4809쪽, 미국 시애틀. 전산언어학회. Guangyao Zhou, Antoine Dedieu, Nishanth Kumar, Wolfgang Lehrach, Miguel Lázaro-Gredilla, Shrinu Kushagra, and Dileep George. 2023. Pgmax: jax에서 이산 확률적 그래픽 모델과 루프 신념 전파를 위한 요인 그래프. Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2020. Please Mind the Root: Dependency Parsing을 위한 Arborescences 디코딩. 자연어 처리 경험적 방법(EMNLP)에 대한 회의록, 4809-4819페이지, 온라인. 계산 언어학 협회. 란 지미그로드, 팀 비에이라, 라이언 코터렐. 2021. 스패닝 트리 분포 하에서 기대치의 효율적 계산. 9:675-690. 분포 매개변수 CTC b= 64, n=124, 1=정렬 CRF 반마르코프 CRF 트리 CRF b= 16, n=256, m=b= b= 128, n=128, nt=1, n= 64, nt= 32, k=선형 체인 CRF PCFG HMM b= 128, n=256, nt=b= 1, n= 48, nt= 64, pt=b= 1, n=128, nt=비투영 CRF 투사 CRF b=1024, n=b= 128, n=표 2: 테스트된 분포의 크기. 경험적 비교 A.1 Torch-Struct와의 비교 2022년 1월 30일의 가장 최근 Torch-Struct³ 커밋과 비교했습니다. Torch-Struct를 더 빠르게 실행하기 위해 2021년 10월 11일의 가장 최근 커밋에서 준링 행렬 곱셈 genbmm을 위한 특수 커널도 설치했습니다. Torch-Struct는 SynJax와 동일한 배포판 중 일부를 지원하지만 모든 배포판에 대한 속도 비교는 하지 못했습니다. 예를 들어, Torch-Struct의 AlignmentCRF는 PyTorch, Torch-Struct 및 genbmm 변경 사항의 인플레이스 업데이트에 대한 불일치로 인해 충돌했습니다. jax.jit으로 SynJax를 컴파일하고 벤치마킹하는 동안 컴파일에 걸리는 시간을 계산하지 않습니다. 한 번만 하면 되기 때문입니다. 또한 추적을 통해 TorchScript를 사용하여 Torch-Struct를 컴파일하려고 했지만 기본적으로 작동하지 않았습니다. 비교는 Colab Pro+에서 A100 GPU에서 수행되었습니다. 결과는 본문의 표 1에 나와 있습니다. 표 2는 테스트 중인 분포의 크기를 보여줍니다. A.2 Zmigrod 등과의 비교 비사영 스패닝 트리는 알고리즘에 관련된 동적 구조로 인해 쉽게 벡터화할 수 없기 때문에 특별한 과제를 제시합니다. 이 유형의 트리를 구문 분석하기 위한 주요 알고리즘과 라이브러리는 Zmigrod et al. (2020)5 및 Stanojević and Cohen (2021)6에서 제공합니다. 첫 번째는 재귀 알고리즘으로 표현되는 반면 두 번째는 반복적인 방식으로 고정된 크기의 배열에서 작동합니다. 이로 인해 Stanojević and Cohen 알고리즘은 Numba 최적화에 훨씬 더 적합합니다. 우리는 그 코드를 가져와서 Numba 기본 요소로 https://github.com/harvardnlp/pytorch-struct https://github.com/harvardnlp/genbmm Shttps://github.com/rycolab/spanningtrees https://github.com/stanojevic/ Fast-MST-Algorithm에 주석을 달았습니다. 이렇게 하면 알고리즘이 상당히 빨라졌고, 특히 큰 그래프의 경우 그림 3에서 볼 수 있듯이 더 빨라졌습니다. 그래프 라이브러리 Stanojević-Cohen Zmigrod-et-al Synjax의 0.0100.005-노드 그림 3: 비사영 신장 트리 라이브러리의 속도 비교. A.3 최대 사영 신장 트리 알고리즘의 비교 Eisner 알고리즘은 전이 기반 파서를 세지 않으면 사실상 유일하게 적극적으로 사용되는 사영 파싱 알고리즘입니다. Eisner 알고리즘을 Kuhlmann et al.로 대체하면 (2011) arc-hybrid 알고리즘의 표는 CPU와 GPU 모두에서 큰 속도 이득을 제공할 수 있습니다. 그림 4를 참조하세요. 이 구현에서 그래프 크기는 벡터화된 방식으로 구현되어 대부분의 작업이 병렬화되므로 큰 차이가 없습니다. 그래프 라이브러리 CPU Synjax Eisner CPU Synjax Kuhlmann GPU Synjax Eisner GPU Synjax Kuhlmann의 0.080.060.040.020-노드 그림 4: Projective Maximum Spanning Tree 알고리즘의 속도 비교.
