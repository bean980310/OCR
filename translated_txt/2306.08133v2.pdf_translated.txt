--- ABSTRACT ---
이 연구에서 우리는 YouTube 비디오의 자동 음성 인식(ASR)에 대한 대규모 언어 모델(LLM)의 영향을 연구합니다. 우리는 이를 장문 ASR의 소스로 사용합니다. 우리는 미국 영어(en-us) 및 코드 전환 인도 영어(en-in) 장문 ASR 테스트 세트에서 단어 오류율(WER)이 최대 8% 감소하고 최대 엔트로피 기반 언어 모델을 사용하는 강력한 첫 번째 통과 기준선에 비해 Salient Term Error Rate(STER)가 최대 30% 감소하는 것을 보여줍니다. 적절한(비트리) 디그래프 토폴로지를 갖는 격자와 이전 세그먼트의 1-최적 가설에서 컨텍스트를 전달하는 격자 처리가 개선되어 LLM으로 재채점하는 데 상당한 이점이 있습니다. 또한 우리는 방대한 양의 사용 가능한 데이터(예: C4[1])로 훈련된 LLM과 기존 신경 LM의 조합에서 얻은 성능 향상이 가산적이며 최대 엔트로피 LM을 갖춘 강력한 첫 번째 통과 기준선보다 상당히 뛰어나다는 것을 발견했습니다. 색인 용어: 대규모 언어 모델, N-best 재채점, Finetuning 1.
--- INTRODUCTION ---
BERT[2], T5[3], GPT-3[4], PaLM[5]과 같은 대규모 언어 모델(LLM)은 질의 응답, 텍스트 요약 및 기타 Zero Shot 학습 애플리케이션과 같은 자연어 처리(NLP) 작업에서 성공한 것으로 입증되었습니다. 이러한 모델은 방대한 양의 텍스트 데이터로 학습되었으며 여러 NLP 및 검색 작업에서 최첨단 결과를 얻었습니다. 그러나 자동 음성 인식(ASR)에서 이러한 LLM을 사용하는 연구는 매우 제한적입니다. 최근 연구에서는 소량의 도메인 내 데이터로 GPT, GPT-2 및 BERT 모델을 미세 조정하는 데 중점을 두었으며 이러한 모델이 동일한 데이터로 학습된 변압기 LM과 같은 기존 신경망 LM의 성능을 능가하는 경향이 있음을 보여줍니다[6]. [7]의 저자는 의사 가능도 점수 사용을 제안하고 ASR 모델에서 N-best 가설을 재채점하면 Librispeech에서 상당한 승리를 거둘 수 있지만 도메인 내 모델링과 훨씬 더 많은 텍스트로 훈련된 모델의 미세 조정 간에는 항상 상충 관계가 있음을 보여줍니다. 오라클 가설을 직접 예측하는 또 다른 접근 방식은 원래 [8]에서 제안되었으며 [9]에서 BERT의 점수를 사용하여 N-best 가설을 재순위 지정하는 데 사용되었습니다. 이 논문에서는 장편 ASR의 소스로 사용하는 YouTube 비디오에서 LLMS 사용을 ASR로 확장합니다. *동등한 기여를 보여줍니다. © 2023 IEEE. 이 자료의 개인적 사용은 허용됩니다. 광고 또는 홍보 목적으로 이 자료를 재인쇄/재출판하거나, 새로운 공동 작품을 만들거나, 서버나 목록에 재판매 또는 재배포하거나, 이 작품의 저작권이 있는 구성 요소를 다른 작품에서 재사용하는 것을 포함하여 현재 또는 미래의 모든 미디어에서 다른 모든 용도에 대해서는 IEEE의 허가를 받아야 합니다. 장문 ASR에 대한 격자 품질과 문맥 증강의 중요성을 설명하고, 단어 오류율(WER)과 두드러진 용어 오류율(STER)이라는 두 가지 지표를 사용하여 LLM의 성능을 다른 신경 및 최대 엔트로피 기반 LM과 비교합니다. 2.
--- RELATED WORK ---
여러 개의
--- METHOD ---
문헌에서는 LM을 엔드투엔드 시퀀스 모델에 통합하는 방법이 제안되었습니다. 디코딩 알고리즘[10, 11, 12]은 얕은[13], 차가운[14], 깊은[15] 및 구성 요소[16] 융합과 같은 융합 전략을 사용합니다. 그러나 이러한 방식으로 LM을 통합하여 얻은 이점은 대규모 ASR의 경우 비교적 작았습니다[17]. 인코더-디코더 모델에 대해 [18]에 도입된 하이브리드 자기 회귀 변환기(HAT) 모델은 정량화되고 외부 언어 모델(ELM)로 적절히 보간될 수 있는 내부 언어 모델 구성 요소를 계산할 수 있도록 했습니다. [19]에 제안된 밀도 비율 방법은 소스 도메인에서 음향 가능도 점수와 내부 LM 점수를 분리하면서 외부 언어 모델을 활용하는 이론적으로 근거 있는 솔루션을 제공합니다. 이 모듈식 프레임워크는 LM 재점수 및 적응의 원칙적인 접근 방식에 적합하여 앞서 언급한 LM 통합 전략의 단점 중 일부를 극복합니다[18, 20]. ASR 시스템은 훈련 데이터가 대상 도메인과 일치할 때 최상의 성능을 발휘합니다. 그러나 엔드투엔드 ASR 모델은 사용 가능한 대량의 음성 데이터에서 훈련되고 LM은 대상 도메인에서 사용 가능한 제한된 텍스트 데이터에서 훈련되므로 도메인 간 전송이 가능합니다. 또는 대용량 LM은 방대한 양의 텍스트에서 훈련된 후 대상 도메인 텍스트에서 미세 조정됩니다. 두 시나리오 모두에서 엔드투엔드 ASR 모델과 암묵적으로 훈련된 내부 LM 및 외부 LM의 최적 조합을 찾는 것이 대상 도메인에서 최상의 성능을 발휘하는 데 중요합니다. Neural Oracle Search는 외부 LM을 사용하여 LM 재점수를 위한 HAT 인수분해를 활용하여 오라클 가설을 직접 선택하는 반면[8], 다른 사람들은 온디바이스 신경 및 바이어싱 LM 통합[21]을 탐구하고 재점수와 심의를 비교하여[22] 모든 작업에서 승리를 입증했습니다. 이 논문에서는 장문 ASR에 대한 HAT 프레임워크 내 LLM의 영향을 연구합니다. 미국 영어(en-us)와 힌디어 및 기타 인도 언어와 크게 코드 전환되는 인도 영어(en-in)의 두 가지 서로 다른 소스의 데이터를 사용하여 최대 엔트로피 기반 언어 모델을 사용하는 강력한 첫 번째 통과 기준선에 비해 Salient Term Error Rate(STER)에서 최대 30%까지 감소하는 동시에 장문 ASR에서 최대 8%의 상대적 승리를 얻을 수 있음을 보여줍니다. 또한 이전 세그먼트의 1best 가설에서 컨텍스트를 전달하고 적절한(비-트리) 디그래프 토폴로지를 갖는 격자를 생성하는 개선된 격자 품질의 중요성을 보여줍니다.이는 LLM에서 최상의 성능을 얻습니다.텍스트 대 텍스트 전송 변환기(T5)[3]와 다국어 대응물인 MT5[23]는 모두 기존 신경 LM을 보완하고 최대 엔트로피 LM을 활용하는 강력한 첫 번째 통과 기준선보다 성능이 우수합니다.3. 대규모 언어 모델 지금까지 다양한 NLP 작업에서 상당한 개선이 이루어진 여러 LLM이 제안되었습니다.이 작업에서 우리는 주로 3B에서 540B 매개변수 범위의 두 가지 LLMS인 T5와 PaLM에 초점을 맞춥니다(표 1에 요약).비교에 사용된 기존 신경 LM은 섹션 4.4에 설명된 70M 매개변수로 구성된 컨포머 LM입니다. 기존 크기 T5 [3] 크기 MT5 [23] 크기 PaLM [5] 크기 SMS LM 신경망 LM MaxEnt 70M 4.5BMS 60M 220M L 770M L 8B 62B 540B XL 3B XL 3.7B 13B XXL 11B XXL 표 1: LM 크기 비교.3.1. T5 및 PaLM 인코더-디코더 변환기 기반 아키텍처를 기반으로 하는 T5는 입력이 주어진 대상 텍스트의 로그 가능도를 최적화하여 입력에서 대상으로의 매핑을 학습합니다.T5가 스팬 손상 작업에 대해 사전 학습된 반면, LM 및 접두사 LM은 언어 모델링에 사용되는 두 가지 미세 조정 작업입니다.LM 작업은 널 컨텍스트 입력이 있는 대상 시퀀스를 예측하는 반면, 접두사 LM 작업은 텍스트를 무작위로 두 개의 절반으로 분할하여 전반부를 입력으로 사용하여 후반부를 예측합니다. 이러한 미세 조정 작업은 마스크된 LM에 대해 [2]에서 처음 제안된 것처럼 의사 로그 우도 추정 대신 대상 텍스트의 로그 우도를 직접 계산할 수 있게 합니다. 따라서 LM 작업과 유사하게 텍스트 시퀀스 Y가 주어지면 다음 방정식을 사용하여 빈 문자열 &amp;를 입력으로 사용하고 텍스트 시퀀스 Y를 대상으로 사용하여 T5 점수 ST5(Y)를 계산할 수 있습니다. STS(Y) = log PTS(Y|E; OTS). (1) 더 긴 시퀀스의 경우 이전 컨텍스트를 더 잘 활용하고 반자기 회귀 방식으로 점수를 계산할 수 있습니다. 따라서 Y를 여러 세그먼트 Y₁... Ys로 분할할 수 있고 현재 세그먼트의 로그 우도는 이전 세그먼트의 컨텍스트를 사용하여 계산할 수 있습니다. S STS(Y) = Σ log PTS(Ys|Ys−1;©TS), 여기서 Yo는 €입니다. s=PaLM은 디코더 전용 아키텍처를 갖춘 자기 회귀 LM입니다. 따라서 텍스트 시퀀스의 점수는 간단하게 계산할 수 있습니다.3.2. ASR 모델과의 통합 이 작업에서 우리는 HAT 분해[18]를 사용하는 컨포머 아키텍처[24]에 기반한 첫 번째 통과 모델을 사용합니다.HAT 모델은 사후 점수 SHAT(Y|X)를 제공할 뿐만 아니라 내부 LM(ILM) 점수도 추정합니다.2절에서 언급했듯이 재점수 또는 얕은 융합 중에 외부 LM을 보간할 때 내부 LM 점수를 추정하고 빼면 승리합니다.따라서 추론 검색은 다음을 최대화합니다.S(Y, X) = SHAT(Y|X) — µSILM(Y) + VSELM(Y), (3) 여기서 및 는 조정 가능한 하이퍼파라미터입니다.μ 4.1. 데이터 4.
--- EXPERIMENT ---
S 우리는 enus와 en-in의 두 언어 로캘에서 얻은 데이터로 실험을 수행합니다. 이 논문에서 사용한 다중 도메인 ASR 모델은 YouTube 비디오[25]에서 파생된 수천 시간 분량의 장문 발화와 익명화되고 손으로 필사되어 Google 음성 검색 트래픽을 나타내는 단문 발화[26]로 훈련되었습니다. 테스트 세트에는 30분 길이의 YouTube 비디오에서 파생된 장문 발화가 포함됩니다. 우리는 테스트 발화의 5%를 포함하는 하위 세트를 하이퍼파라미터를 조정하기 위한 개발 테스트로 따로 두었습니다. T5를 훈련하는 데 사용된 사전 훈련 코퍼스는 공개적으로 사용 가능한 Colossal Clean Crawled Corpus(C4)이고, MT5는 다국어 변형인 MC4[23]로 사전 훈련되었습니다. en-in [27]에서 발견되는 코드 전환을 해결하기 위해 인도 영어와 힌디어 위키피디아, CCNet [28]으로 구성된 텍스트 데이터(총칭하여 WEBDOC)가 사용되었습니다. 이 코퍼스는 29억 개의 단어 토큰을 생성하는 1억 7천만 개의 문장으로 구성되어 있습니다. 90%의 데이터는 훈련에 사용하고 10%의 데이터는 검증에 사용합니다. 혼합 문자 체계의 모든 데이터는 en-in에 사용된 ASR 모델 훈련 데이터와 일관성을 유지하도록 라틴어로 음역되었습니다. 4.2. 대규모 언어 모델 훈련 XL 및 XXL 크기의 T5 및 MT5 모델을 실험했습니다. T5 및 MT5 모델 모두 span corruption 작업을 사용하여 1M 단계 동안 사전 훈련한 다음 C4/MC4에서 prefix LM 작업을 사용하여 100K 단계 동안 미세 조정했습니다. en-in에서 흔히 발생하는 심한 코드 전환과 MC4 코퍼스의 힌디어 데이터 부족을 해결하기 위해 WEBDOC 코퍼스에서 추가로 300k 단계에 대해 LM 작업에서 MT5를 미세 조정합니다. 세 가지 다른 크기의 PaLM 모델은 en-us 작업에 대해 [5]에 설명된 대로 훈련되었습니다. 이러한 모델을 훈련하는 데 사용된 코퍼스는 필터링된 웹 페이지, 책, 위키피디아, 뉴스 기사, 소스 코드 및 소셜 미디어 대화로 구성되었습니다. 이러한 사전 훈련된 모델을 추가 미세 조정 없이 그대로 사용합니다. 4.3. ASR 모델 HAT 인수 분해[18]를 사용하는 컨포머 아키텍처[24] 기반의 첫 번째 통과 ASR 모델을 사용합니다. 인코더는 합성곱 서브 샘플링 계층과 17개 계층의 컨포머 블록으로 구성됩니다. 컨포머 블록은 피드 포워드 모듈, 상대 위치 인코딩 모듈이 있는 멀티헤드 셀프 어텐션, 합성곱 및 최종 피드 포워드 모듈로 구성되며 함께 쌓입니다. 이 작업에서 사용된 구성은 인코더 차원이 512, 어텐션 헤드, 합성곱 커널 크기가 32, 디코더 차원이 640입니다[24]. 레이블 yu의 디코더는 이전 두 레이블 Yu-1과 Yu-2에만 조건화되며, 이들의 임베딩은 연결되고 투영됩니다[29]. 모델은 80차원 로그-멜 필터 뱅크 계수에 대해 학습되고 단어 조각 타겟(en-us의 경우 및 en-in의 경우 8192)을 예측합니다. 이러한 매개변수의 선택은 예상 모델 크기 내에서 최상의 성능을 위해 스윕을 통해 결정되었습니다. 4.4. 신경망 및 최대 엔트로피 기반 언어 모델 ASR에서 LLM의 가치를 더 잘 이해하기 위해 기존 신경망 LM과 최대 엔트로피 기반 LM이라는 두 가지 최신 LM을 학습했습니다. 기존의 신경 LM은 원래 장치 내 재점수 매기기[21]를 위해 설계된 70M 매개변수를 갖는 작고 단방향의 컨포머 LM(CLM)입니다.각각 384차원, 피드포워드 계층 차원 2048, 크기 15의 합성곱 커널을 갖는 12개의 인과 컨포머 계층으로 구성됩니다.우리는 좌측 컨텍스트 크기 31의 4-헤드 셀프 어텐션을 사용합니다.이 모델은 LLM과 동일한 데이터에서 학습되어 첫 번째 패스 ASR 모델과 동일한 단어 조각 타겟을 예측합니다.따라서 en-us의 경우 C4에서 학습했고 en-in의 경우 WEBDOC에서 학습하여 MT5의 미세 조정 코퍼스와 일치시켰습니다.최대 엔트로피 기반(MaxEnt) LM[30, 31]은 N-그램 및 스킵 그램 단어 컨텍스트를 기반으로 하는 로그 선형 모델로 매개변수 크기가 4.5B이며 T5/MTXL 모델의 크기와 비슷합니다. 또한 기존의 Neural LM과 동일한 데이터에서 학습합니다.4.5. 디코딩 및 재점수 디코딩은 각 프레임의 활성 가설 수가 빔 크기 k로 제한되는 너비 검색 확장 전략[32]을 사용하여 시간 동기 빔 검색을 통해 수행됩니다.VAD 기반 세그먼터[33]는 빔 검색 디코더와 병렬로 실행됩니다.디코더가 세그먼터로부터 세그먼트 끝 신호를 수신하면 현재 활성 가설에서 세그먼트 격자가 생성됩니다.존재하는 경우 재점수 LM이 이 세그먼트 격자에 적용되며, 이전 세그먼트의 1-최상의 가설은 선택적으로 컨텍스트로 제공됩니다.격자에서 최상의 가설만(결국 재점수 후) 다음 세그먼트의 빔 검색에서 전달됩니다.최종 발화 격자는 모든 세그먼트 격자를 연결하여 얻습니다. 영어: 무제한 레이블 컨텍스트가 있는 ASR 모델을 사용하는 경우 빔 내의 각 가설은 발언 시작부터 전체 기록을 인코딩합니다.따라서 세그먼트 격자는 빔 크기 k로 제한된 총 경로 수(예: 가설)를 갖는 트라이입니다.레이블 컨텍스트가 n[34]으로 제한된 ASR 모델을 사용하는 경우 길이가 n인 동일한 레이블 컨텍스트를 공유하는 빔 검색 가설은 세그먼트 격자의 동일한 상태에 해당합니다.이로 인해 경로 수가 상태 수에서 기하급수적으로 증가할 수 있는 적절한(비트리) 디그래프 토폴로지가 있는 격자가 생성됩니다.이로 인해 격자 품질이 크게 향상되고 격자 다양성이 향상되고 오라클 WER이 감소하는 것으로 나타났습니다[34]. 4.3절에서 설명한 ASR 모델은 n = 2인 제한된 레이블 컨텍스트를 사용했습니다. 그러나 HAT 융합 결과를 사용하여 빔 검색 중에 4.4절의 적합 LM과 이러한 모델을 결합하면 레이블 컨텍스트 한계가 극적으로 증가하여 결과적으로 결합된 모델이 사실상 무제한 레이블 컨텍스트를 갖게 됩니다. 5.1. 격자 품질 5. 결과 재점수 부여 방식의 성공은 1차 통과 빔 검색 디코더의 가설 품질에 크게 좌우됩니다. 격자 품질을 평가하기 위해 우리는 표 2에 보고된 대로 en-us 및 en-in 개발 세트에 대한 기준 시스템의 N-best oracle WER 및 세그먼트당 경로/가설의 평균 수와 같은 메트릭을 계산했습니다. dev Baseline No state merging Neural LM fusion #paths/segment Oracle WER en-us en-in 8.8.7.3 12.13.11.WER en-us en-in 12.2 17.12.2 17.11.6 15.en-us en-in 4e4e5.5.5.5.표 2: en-us 및 en-in 개발 세트의 격자 품질. 레이블 yu에서 일차 통과 모델의 사후 및 내부 LM에 대한 기여가 이전 두 레이블에만 의존하기 때문에, 베이스라인 시스템은 섹션 4.5에서 설명한 제한된 컨텍스트 모델의 상태 병합 이점을 활용할 수 있으며, 이는 상대적으로 낮은 오라클 WER과 세그먼트당 많은 경로 수에서 입증됩니다. 격자 품질은 HAT 융합을 사용하여 빔 검색 디코딩에서 신경 LM을 통합하여 일차 통과 모델링을 개선함으로써 개선할 수 있습니다. 표 2는 이것이 1-최적 WER에서 상당한 개선으로 이어진다는 것을 보여줍니다. 그러나 이것은 상태 병합 이점의 손실을 초래하고 en-us에서 오라클 WER의 증가로 이어집니다. 그러나 이것은 여전히 베이스라인 시스템에서 상태 병합을 비활성화하는 것과 비교하면 상당한 개선입니다. 5.2. LM 비교 이 섹션에서는 enus 작업에 대한 LM 통합의 영향을 고려합니다. 표 3은 큰 LM에 더 긴 컨텍스트를 제공하는 가치를 보여줍니다. 각 행에는 서로 다른 길이의 컨텍스트를 이월할 때 T5 XXL 모델로 재점수를 매긴 결과, 즉 서로 다른 수의 이전 세그먼트에서 1-최상의 가설을 이월한 결과가 포함됩니다. 이전 컨텍스트를 이월하는 것이 컨텍스트를 사용하지 않는 것보다 성능이 더 우수하다는 것을 관찰했습니다. 그러나 더 긴 컨텍스트는 추가 승리를 제공하지 않는 것으로 보입니다. 따라서 이 논문의 나머지 부분에서는 이전 세그먼트의 컨텍스트 정보만 사용합니다. WER | dev Baseline 12.+ T5 재점수 매기기, 0 세그먼트 이월 + T5 재점수 매기기, 1 세그먼트 이월 + T5 재점수 매기기, 2 세그먼트 이월 11.11.11.표 3: 이월된 컨텍스트의 길이가 다른 en-us 테스트 세트에서 WER 비교 표 4는 다양한 LM에 대한 en-us 개발 및 평가 테스트 세트에서 재점수 매기기 및 융합 결과를 보여줍니다. 먼저 작은 Neural LM이 Maxent LM의 성능을 앞지르는 것을 관찰했습니다. 게다가, NLM보다 크기가 약간 작은 T5 S 모델은 NLM보다 약간 뒤처졌지만, Tleads의 크기를 늘리면 더 나은 결과를 얻을 수 있었습니다. 또한, NLM과 T5 XXL 모델은 융합을 통해 더 나은 1-best WER을 제공할 수 있으므로 상호 보완적이라는 점도 흥미롭습니다. 게다가, 우리는 더 거대한 PALM LMS로 실험했고, 더 큰 용량과 많은 양의 학습 텍스트의 힘을 발휘하여 T5보다 더 나은 결과를 얻을 수 있었습니다. 5.3. 코드 전환 작업 이 섹션에서는 코드 전환이 많은 더 어려운 en-in 작업에서 LLM의 성능을 제시합니다. MT5는 다국어 LM이지만, 다양한 언어의 학습 데이터 양이 불균형합니다. 학습 데이터는 5.67%가 영어로 구성되어 있지만, 데바나가리 문자의 힌디어는 1.21%에 불과합니다[23]. en-in과 WER 개발 간의 불균형 기준선 12.2 16.+ MaxEnt 재채점 12.16.+ NLM 재채점 11.15.+ T5 S 재채점 11.9 15.+ T5 M 재채점 + T5 XXL 재채점 + T5 XL 재채점 + PaLM S 재채점 + PALM M 재채점 + PaLM L 재채점 11.7 15.11.6 15.11.5 15.11.15.11.3 15.11.11.11.15.15.+ NLM 융합 + NLM 융합 및 T5 XXL 재채점 표 4: T5와 다른 LM 간의 en-us WER 비교 힌디어는 en-in 테스트 세트에서 우세한 영어와 힌디어 간의 빈번한 코드 전환을 포착하지 못합니다. 이 문제를 해결하기 위해 LM 과제를 사용하여 WEBDOC 코퍼스에서 XL 및 XXL MT5 모델을 모두 미세 조정합니다. 미세 조정의 효과를 연구하기 위해 en-in 개발 세트에서 원시 MT5 모델과 이러한 미세 조정된 모델을 평가합니다. 이러한 결과는 표 5에 표로 정리되어 있습니다. en-in dev MT5 XL | MT5 XXL 17. 미세 조정 16.16.16.16.Baseline Raw 표 5: 크기가 XL 및 XXL인 원시 및 미세 조정된 MT5 모델을 사용한 en-in dev 세트의 WER 비교 미세 조정된 모델로 재채점하는 것이 원시 MT5 모델로 재채점하는 것보다 성과가 더 좋은 것을 알 수 있습니다. 이는 MC4 코퍼스에 충분한 힌디어 데이터가 부족하기 때문일 수 있으며, 이는 데이터 균형 미세 조정으로 수정할 수 있습니다. en-us와 비교할 때 en-in에서 LLM의 승리가 적습니다. 우리는 이것이 MC4에 비해 WEBDOC 코퍼스의 크기가 작은 것과 관련이 있을 수 있다고 가설을 세웠는데, 이는 LLM의 데이터 소모적 특성[35, 36]과 일맥상통합니다.5.4. 코드 전환 작업에서 LM 비교 WER Baseline + MaxEnt 재채점 Conformer LM과 MT5는 상호 보완적이며, 이 조합은 상대적으로 8%의 최상의 WER 감소를 가져옵니다.6. 오류 분석 큰 LM의 효과를 분석하기 위해 이 논문에서 연구한 두 언어의 평가 테스트 세트(주요 용어)에서 가장 높은 Term Frequency Inverse Document Frequency(TF-IDF) 값을 가진 유니그램과 바이그램을 선택합니다.일반적으로 이러한 용어는 비디오에서 제시된 주제를 포착합니다.한편으로는 인덱싱이나 정보 검색에 중요하지만, 다른 한편으로는 자주 등장하는 기능어(예: &quot;the&quot;, &quot;of&quot; 등)에 비해 인식하기 어렵습니다. 우리는 이러한 두드러진 용어에 대한 기준선과 다양한 대규모 LM의 성능을 분석하여 희귀 단어에 미치는 영향을 연구했습니다. 두드러진 용어 오류율(STER)은 표 7에 보고되어 있으며, 두드러진 용어의 삭제 및 대체 오류 수를 두드러진 용어의 총 수로 나눈 값으로 정의됩니다. 총 600,000개 단어 중 약 10%의 단어가 en-in의 두드러진 용어로 태그 지정되고 5%가 en-us의 두드러진 용어로 태그 지정됩니다. 먼저, 거의 모든 재점수 매기기 및 융합이 이러한 두드러진 용어에서 발생하는 오류를 줄일 수 있음을 관찰했습니다. 표 4에 보고된 WER에서 반영된 것처럼 en-us에서 T5는 다른 LM보다 성능이 뛰어납니다. 그러나 en-in에서 첫 번째 패스의 NLM 융합은 [37]에 보고된 것과 유사한 재점수 매기기 방법보다 두드러진 용어에 더 큰 영향을 미칩니다. MT5는 NLM과 동일한 데이터에서 미세 조정되었지만 en-in의 두드러진 용어에 미치는 영향이 그 자체로는 적다는 것을 알게 되었습니다. MT5는 신경망 LM과 동일한 데이터에서 미세 조정되었지만, 그 자체로는 중요한 용어에 미치는 영향이 적다는 것을 알게 되었습니다. 그러나 두 언어 모두 보간을 통한 이 두 LM의 조합은 가산적(표 6의 마지막 행)이어서 최상의 성능을 냅니다. [35, 36]에서 언급했듯이 점점 더 큰 데이터 세트로 확장하는 것은 데이터의 품질이 높고 더 큰 모델에 더 큰 데이터 세트가 필요한 경우에만 유익합니다. 이는 이 두 비교적 리소스가 높은 언어 간에 보이는 몇 가지 차이점을 설명할 수 있습니다. STER Baseline + MaxEnt 재채점 en-us en-in 28.20.28.17.+ NLM 재채점 27.16.+ T5/MT5 재채점 26.17.dev eval + NLM 융합 27.15.17.2 16.+ NLM 융합 &amp; T5/MT5 재채점 26.12.16.5 15.16.2 15.16.1 15.15.6 15.+ NLM 재채점 + MT5 XL 재채점 + NLM 융합 + NLM 융합 &amp; MT5 XL 재채점 15.4 14.표 6: en-in MT5와 다른 LM 간의 WER 비교 표 6은 다양한 LM의 재채점 결과를 보여줍니다. MT5 XL 모델은 평가 테스트 세트에서 WER이 7.3% 감소한 가장 성능이 좋은 모델입니다. 반면, 첫 번째 패스에서 얕은 융합에 사용된 Conformer LM은 추가 승리를 보여줍니다. Conformer LM과 동일한 교육 데이터에서 MT5를 미세 조정했기 때문에 WEBDOC의 10% 검증 부분에서 MT5와 Conformer LM의 복잡도도 보고합니다. MT5는 단어당 로그 복잡도가 4.15로 Conformer LM의 2.98과 MaxEnt의 3.69보다 약간 높습니다. 표 7: en-us와 en-in의 두드러진 용어에 대한 오류 분석. 7.
--- CONCLUSION ---
이 연구에서 우리는 LLM(최대 350B 매개변수)이 장문 ASR에 미치는 영향을 제시했습니다. 우리는 미국 영어(en-us)와 코드 전환 인도 영어(en-in) 장문 ASR 테스트 세트에서 단어 오류율(WER)이 최대 8% 감소하고 최대 엔트로피 기반 언어 모델을 사용하는 강력한 첫 번째 통과 기준선에 비해 Salient Term Error Rate(STER)가 최대 30% 감소하는 것을 보여주었습니다. 또한 방대한 양의 사용 가능한 데이터(예: C4[1])에서 학습한 LLM과 기존 신경 LM을 결합하여 얻은 성능 향상은 가산적이며 최대 엔트로피 LM을 사용한 강력한 첫 번째 통과 기준선보다 상당히 우수한 것으로 나타났습니다. 우리가 아는 한, 이것은 LLM을 장문 ASR로 확장한 최초의 연구입니다. 8. 참고문헌 [1] C. Raffel 등, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구.&quot; J. Mach. Learn. Res., vol. 21, no. 140, pp. 1-67, 2020. [2] A. Wang 및 K. Cho, &quot;Bert has a mouth, and it must speak: Bert as a markov random field language model,&quot; arXiv 사전 인쇄본 arXiv:1902.04094, 2019. [3] C. Raffel 등, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구&quot;, Journal of Machine Learning Research, vol. 21, no. 140, pp. 1-67, 2020. [4] T. Brown 외, &quot;언어 모델은 소수 학습자입니다.&quot; 신경 정보 처리 시스템의 발전, vol. 33, pp. 1877-1901, 2020. [5] A. Chowdhery 외, &quot;Palm: 경로로 언어 모델링 확장&quot;, arXiv 사전 인쇄본 arXiv:2204.02311, 2022. [6] X. Zheng, C. Zhang 및 PC Woodland, &quot;음성 인식을 위한 gpt, gpt2 및 bert 언어 모델 적용&quot;, IEEE ASRU, 2021, pp. 162–168. [7] J. Salazar, D. Liang, TQ Nguyen, 및 K. Kirchhoff, &quot;Masked language model scoring,&quot; 2020 ACL, 2020년 7월. [8] E. Variani et al., &quot;Neural oracle search on n-best hypotheses,&quot; ICASSP, 2020, pp. 7824–7828. [9] S.-H. Chiu 및 B. Chen, &quot;음성 인식을 위한 혁신적인 bert 기반 재순위 언어 모델,&quot; SLT. IEEE, 2021, pp. 266-271. [10] T. Hori, Y. Kubo, 및 A. Nakamura, &quot;음성 인식을 위한 순환 신경망 언어 모델을 사용한 실시간 원패스 디코딩,&quot; ICASSP. IEEE, 2014, pp. 6364–6368. [11] J. Chorowski 및 N. Jaitly, &quot;시퀀스 간 모델에서 더 나은 디코딩 및 언어 모델 통합을 향해&quot;, arXiv 사전 인쇄본 arXiv:1612.02695, 2016. [12] T. Hori, J. Cho 및 S. Watanabe, &quot;단어 기반 rnn 언어 모델을 사용한 종단 간 음성 인식&quot;, SLT. IEEE, 2018, 389-396쪽. [13] C. Peyser 외, &quot;대용량 텍스트 코퍼스를 사용하여 심의 e2e asr 모델의 테일 성능 개선&quot;, arXiv 사전 인쇄본 arXiv:2008.10491, 2020. [14] A. Sriram, H. Jun, S. Satheesh, A. Coates, &quot;콜드 퓨전: 언어 모델과 함께 seq2seq 모델 학습&quot;, arXiv 사전 인쇄본 arXiv:1708.06426, 2017. [15] C. Gulcehre 외, &quot;신경망 기계 번역에서 단일 언어 코퍼스 사용에 관하여&quot;, arXiv 사전 인쇄본 arXiv:1503.03535, 2015. [16] C. Shan 외, &quot;구성 요소 퓨전: 종단간 음성 인식 시스템을 위한 대체 가능 언어 모델 구성 요소 학습&quot;, ICASSP. IEEE, 2019, pp. 5361-5635. [17] A. Kannan et al., &quot;시퀀스 대 시퀀스 모델에 외부 언어 모델을 통합하는 분석&quot;, ICASSP, 2018, pp. 1-5828. [18] E. Variani, D. Rybach, C. Allauzen, and M. Riley, &quot;하이브리드 자기 회귀 변환기(hat)&quot;, ICASSP, 2020, pp. 6139-6143. [19] E. McDermott, H. Sak, and E. Variani, &quot;엔드투엔드 자동 음성 인식에서 언어 모델 융합을 위한 밀도 비율 접근법&quot;, ASRU, 2019, pp. 434-441. [20] C. Allauzen, E. Variani, M. Riley, D. Rybach 및 H. Zhang, &quot;장치 및 서버 애플리케이션을 위한 하이브리드 seq-2-seq ASR 디자인&quot;, Interspeech 2021, 2021, pp. 4044–4048. [21] TN Sainath 등, &quot;희귀 단어 모델링을 개선한 효율적인 스트리밍 비재귀 온디바이스 엔드투엔드 모델&quot;, Interspeech, 2021, pp. 1777–1781. [22] K. Hu 등, &quot;텍스트 전용 및 반지도 학습을 통한 심의 개선&quot;, arXiv 사전 인쇄본 arXiv:2206.14716, 2022. [23] L. Xue 등, &quot;mT5: 대규모 다국어 사전 학습된 텍스트-텍스트 변환기&quot;, 2021 NAACL: Human Language Technologies, 2021년 6월. [24] A. Gulati 등, &quot;Conformer: 음성 인식을 위한 합성 증강 변환기&quot;, arXiv 사전 인쇄본 arXiv:2005.08100, 2020. [25] H. Liao, E. McDermott 및 A. Senior, &quot;YouTube 비디오 필사를 위한 반지도 학습 데이터를 사용한 대규모 딥 신경망 음향 모델링&quot;, ASRU, 2013, 368-373쪽. [26] A. Narayanan 등, &quot;스트리밍 엔드투엔드 모델을 사용하여 장문 음성 인식&quot;, ASRU, 2019, 920-927쪽. [27] J. Emond, B. Ramabhadran, B. Roark, P. Moreno 및 M. Ma, &quot;코드 전환 음성 인식 성능 개선을 위한 음역 기반 접근 방식&quot;, SLT. IEEE, 2018, 448-455쪽. [28] G. Wenzek 등, &quot;Ccnet: 웹 크롤링 데이터에서 고품질 단일 언어 데이터 세트 추출,&quot; arXiv 사전 인쇄본 arXiv:1911.00359, 2019. [29] R. Botros 등, &quot;Tied &amp; reduced rnn-t decoder,&quot; arXiv 사전 인쇄본 arXiv:2109.07513, 2021. [30] F. Biadsy, K. Hall, P. Moreno, and B. Roark, &quot;최대 엔트로피 언어 모델을 위한 백오프에서 영감을 받은 기능,&quot; 2014. [31] F. Biadsy, M. Ghodsi, and D. Caseiro, &quot;비언어적 신호를 통합한 테라 스케일 최대 언어 모델 효과적으로 구축,&quot; 2017. [32] A. Tripathi, H. Lu, H. Sak, and H. Soltau, &quot;단조 순환 신경망 변환기 및 디코딩 전략,” ASRU, 2019, 944-948쪽. [33] R. Zazo, TN Sainath, G. Simko, 및 C. Parada, &quot;음성 활동 감지를 위한 원시 파형 cldnn을 사용한 기능 학습&quot;, Interspeech, 2016, 3668-3672쪽. [34] R. Prabhavalkar 외, &quot;Less is more: 제한된 레이블 컨텍스트 및 경로 병합을 사용한 개선된 rnn-t 디코딩&quot;, ICASSP, 2021, 5659-5663쪽. [35] J. Kaplan 외, &quot;신경 언어 모델을 위한 스케일링 법칙&quot;, arXiv 사전 인쇄본 arXiv:2001.08361, 2020. [36] J. Hoffmann 외, &quot;컴퓨팅 최적 대규모 언어 모델 학습&quot;, arXiv 사전 인쇄본 arXiv:2203.15556, 2022. [37] V. Ravi et al., “유니그램 얕은 융합을 통해 rnntransducer의 희귀 단어 정확도 향상”, arXiv 사전 인쇄본 arXiv:2012.00133, 2020.
