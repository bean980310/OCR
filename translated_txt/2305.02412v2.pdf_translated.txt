--- ABSTRACT ---
사전 훈련된 대규모 언어 모델(LLM)은 세상에 대한 절차적 지식을 포착합니다. 최근 연구에서는 LLM이 추상 계획을 생성하여 어려운 제어 작업을 단순화하는 기능을 활용하여 액션 스코어링 또는 액션 모델링(미세 조정)을 수행했습니다. 그러나 트랜스포머 아키텍처는 LLM이 에이전트 역할을 직접 수행하기 어렵게 만드는 몇 가지 제약 조건을 상속받습니다. 예를 들어 제한된 입력 길이, 미세 조정 비효율성, 사전 훈련으로 인한 편향, 비텍스트 환경과의 비호환성입니다. 저수준의 훈련 가능한 액터와의 호환성을 유지하기 위해 LLM의 지식을 사용하여 제어 문제를 해결하는 대신 단순화하는 것을 제안합니다. PET(Plan, Eliminate, and Track) 프레임워크를 제안합니다. Plan 모듈은 작업 설명을 상위 수준 하위 작업 목록으로 변환합니다. Eliminate 모듈은 현재 하위 작업에 대한 관찰에서 관련 없는 객체와 용기를 마스크합니다. 마지막으로 Track 모듈은 에이전트가 각 하위 작업을 완료했는지 여부를 확인합니다. AlfWorld 지침에 따른 벤치마크에서 PET 프레임워크는 인간의 목표 사양에 대한 일반화 측면에서 SOTA보다 15% 더 높은 개선을 보였습니다. 1.
--- INTRODUCTION ---
인간은 실행 없이도 일상 업무를 추상적으로 계획할 수 있다. 예를 들어, &quot;아침 식사 만들기&quot;라는 업무가 주어졌을 때, 우리는 머그잔을 집어 커피를 만든 다음 계란을 집어서 스크램블하는 것을 대략적으로 계획할 수 있다. 이 능력을 부여받은 구체화된 에이전트는 상식적 추론을 활용하여 더 효과적으로 일반화할 것이다. ¹카네기 멜론 대학교 2 아리엘 대학교 3 마이크로소프트 리서치 엔비디아 리서치. 연락처: 웨 우<ywu5@andrew.cmu.edu> . 계획 제거 사과를 데워 냉장고에 보관 사과 가져오기 사과 데우기 사과를 냉장고에 넣기 사과 가져오기 사과, 머그잔, 칼이 보이나요?액터 액션: 사과 줍기 추적 사과 가져오기를 마쳤나요?진행 상황 업데이트 그림 1. PET 프레임워크.계획 모듈은 LLM을 사용하여 상위 수준 계획을 생성합니다.제거 모듈은 QA 모델을 사용하여 관찰에서 관련 없는 객체를 마스크합니다.추적 모듈은 QA 모델을 사용하여 하위 작업의 완료를 추적합니다.최근 연구(Huang et al., 2022a;b; Ahn et al., 2022; Yao et al., 2020)에서는 구체화된 에이전트나 게임 에이전트에 대한 추상적 계획을 위해 LLM(Bommasani et al., 2021)을 사용했습니다.이들은 환경에서 실행 가능한 액션에 대한 사후 정렬을 통해 언어적 형태로 LLM에서 절차적 세계 지식을 추출하는 데 있어 초기 성공을 보였습니다. 그러나 그들은 LLM을 행위자로 취급하고 미세 조정(Micheli &amp; Fleuret, 2021)이나 제약 조건(Ahn et al., 2022)을 통해 LLM 출력을 실행 가능한 작업으로 조정하는 데 중점을 둡니다. LLM을 액터로 사용하면 상호 작용이 제한된 순수 텍스트 환경(Huang 등, 2022b; Ahn 등, 2022)(단순히 &quot;객체 선택/배치&quot;로 구성됨)에서 작동하지만 일반화를 다른 모달리티로 제한합니다. 또한 고려된 시나리오는 실제 세계에서 크게 단순화되었습니다. Ahn 등(2022)은 처음에 사용 가능한 모든 객체와 가능한 상호 작용을 제공하고 작업을 제공된 객체/상호 작용 세트로 제한합니다. Huang 등(2022b)은 환경을 단일 테이블의 객체로 제한합니다. 반면 실제 방에서 &quot;상추를 자르는&quot; 데 성공하려면 &quot;칼을 찾아야&quot; 하는데, 서랍이나 캐비닛이 여러 개 있을 수 있기 때문에 간단하지 않을 수 있습니다(Chaplot 등, 2020; Min 등, 2021; Blukis 등, 2021). 보다 현실적인 시나리오는 계획, 제거 및 추적으로 이어집니다. 다양하고 복잡한 작업 집합 또는 크고 변화하는 액션 공간. 나아가 관찰에 대한 텍스트 설명은 에이전트가 보는 용기와 객체의 수에 따라 증가합니다. 증가하는 롤아웃과 결합하면 상태가 너무 장황해져 어떤 LLM에도 맞지 않습니다. 이 작업에서 우리는 액터의 훈련 가능한 특성에 영향을 미치지 않고 LLM에 인코딩된 사전 지식을 활용하는 대체 메커니즘을 탐구합니다. 우리는 3단계 프레임워크(그림 1)를 제안합니다: 계획, 제거 및 추적(PET). 계획 모듈은 복잡한 작업을 하위 작업으로 나누어 단순화합니다. 사전 훈련된 LLM을 사용하여 Huang et al.(2022a); Ahn et al.(2022)과 유사한 훈련 세트의 예제 프롬프트를 사용하여 입력 작업 설명에 대한 하위 작업 목록을 생성합니다. 제거 모듈은 장기 관찰의 과제를 해결합니다. 현재 하위 작업과 관련이 없는 객체와 용기를 채점하고 마스크하기 위해 제로샷 QA 언어 모델을 사용합니다. Track 모듈은 제로샷 QA 언어 모델을 사용하여 현재 하위 작업이 완료되어 다음 하위 작업으로 이동하는지 확인합니다. 마지막으로 Action Attention 에이전트는 변압기 기반 아키텍처를 사용하여 긴 롤아웃과 가변 길이 작업 공간을 수용합니다. 에이전트는 마스크된 관찰을 관찰하고 현재 하위 작업에 따라 작업을 수행합니다. 우리는 AlfWorld(Shridhar et al., 2020b) 대화형 텍스트 환경 벤치마크에서 실내 가정에서의 지시 따르기에 집중합니다. 우리의 실험과 분석은 LLM이 상식적 QA를 통해 관찰에서 작업과 관련 없는 객체의 40%를 제거할 뿐만 아니라 99% 정확도로 고수준 하위 작업을 생성한다는 것을 보여줍니다. 또한 여러 LLM을 서로 조정하여 에이전트를 다양한 측면에서 지원할 수 있습니다. 우리의 기여는 다음과 같습니다. 1. PET: 구체화된 에이전트가 있는 사전 훈련된 LLM을 활용하기 위한 새로운 프레임워크. 우리의 작업에서는 P, E, T가 각각 보완적인 역할을 하며 제어 작업을 처리하기 위해 동시에 처리해야 함을 보여줍니다. 2. 텍스트 환경에 대한 변화하는 액션 공간을 처리하는 액션 어텐션 에이전트. 3. 하위 작업 계획 및 추적을 통해 인간의 목표에 대한 일반화를 위해 SOTA보다 15% 향상. 2.
--- RELATED WORK ---
언어 조건화 정책 이전 연구의 상당 부분은 모방 학습(Tellex 등, 2011; Mei 등, 2016; Nair 등, 2022; Stepputtis 등, 2020; Jang 등, 2022; Shridhar 등, 2022; Sharma 등, 2021)이나 강화 학습(Misra 등, 2017; Jiang 등, 2019; Cideron 등, 2020; Goyal 등, 2021; Nair 등, 2022; Akakzia 등, 2020) 정책을 자연어 교육이나 목표에 따라 조건화한 연구(MacMahon 등, 2006; Kollar 등, 2010)입니다. 일부 이전 연구에서는 사전 훈련된 언어 임베딩을 사용하여 새로운 지침에 대한 일반화를 개선했지만(Nair et al., 2022), LLM에서 포착되는 도메인 지식이 부족합니다. 당사의 PET 프레임워크는 LLM을 사용하여 계획, 진행 추적 및 관찰 필터링을 가능하게 하며 위의 모든 언어 조건부 정책과 호환되도록 설계되었습니다. 제어를 위한 LLM LLM은 최근 고수준 계획에서 성공을 거두었습니다. Huang et al. (2022a)은 사전 훈련된 LLM이 일상 업무에 대한 그럴듯한 계획을 생성할 수 있지만 생성된 하위 업무를 엔드투엔드 제어 환경에서 직접 실행할 수 없음을 보여줍니다. Ahn et al. (2022)은 LLM 작업 선택을 다시 평가하기 위해 작업 점수 모델을 훈련하여 실행 가능성 문제를 해결하고 로봇에서 성공을 보여줍니다. 그러나 LLM 점수는 픽/플레이스로 제한된 동작이 있는 간단한 환경에서는 작동하지만(Ahn et al., 2022), 더 많은 객체와 다양한 동작이 있는 환경에서는 작동하지 않습니다(Shridhar et al., 2020b). Song et al.(2022)은 GPT3를 사용하여 단계별 저수준 명령을 생성한 다음 해당 제어 정책에 의해 실행합니다. 이 작업은 더 많은 동작 다양성과 즉석 재계획으로 Ahn et al.(2022)을 개선합니다. 또한 위의 모든 LLM은 최대 17개 예제의 몇 가지 샷 데모가 필요하므로 AlfWorld에서 프롬프트의 길이가 실행 불가능합니다. Micheli &amp; Fleuret(2021)은 AlfWorld에서 전문가 궤적에 대한 GPT2-medium 모델을 미세 조정하여 인상적인 평가 결과를 보여주었습니다. 그러나 LM 미세 조정에는 완전히 텍스트 기반 환경, 일관된 전문가 궤적 및 완전히 텍스트 기반 동작 공간이 필요합니다. 이러한 요구 사항은 다른 도메인 및 다른 형태의 작업 사양에 대한 일반화를 크게 제한합니다.우리는 PET 프레임워크가 에이전트가 훈련되지 않은 인간의 목표 사양에 대한 더 나은 일반화를 달성한다는 것을 보여줍니다.자연어를 통한 계층적 계획자연어의 구조적 특성으로 인해 Andreas 등(2017)은 각 작업 설명을 모듈식 하위 정책에 연결하는 것을 탐구했습니다.나중의 연구는 단일 조건부 정책(Mei 등, 2016)을 사용하거나 하위 작업을 템플릿에 매칭(Oh 등, 2017)하여 위의 접근 방식을 확장합니다.최근 연구에 따르면 LLM은 숙련된 고급 계획자(Huang 등, 2022a; Ahn 등, 2022; Lin 등, 2022)이므로 진행 상황 추적을 통한 계층적 작업 계획의 아이디어를 재고하도록 동기를 부여합니다. 영어: 저희가 아는 한, PET는 제로샷 하위 작업 수준 LLM 플래너와 제로샷 LLM 진행 상황 추적기를 저수준 조건부 하위 작업 정책과 결합한 최초의 작업입니다.텍스트 게임 텍스트 기반 게임은 게임 상태와 액션 공간이 자연어로 표현된 복잡한 대화형 시뮬레이션입니다.이들은 언어 중심 머신 러닝 연구에 적합한 토양입니다.언어 이해 외에도 성공적인 플레이에는 기억과 계획, 탐색(시행착오), 상식과 같은 기술이 필요합니다.AlfWorld(Shridhar et al., 2020b) 시뮬레이터는 일반적인 텍스트 기반 게임 시뮬레이터인 TextWorld Côté et al.(2018a)를 확장하여 각 ALFRED 장면의 텍스트 기반 아날로그를 만듭니다.대규모 액션 공간을 위한 에이전트 He et al.(2015)은 두 가지 다른 모델을 사용하여 상태와 액션에 대한 표현을 학습하고 표현의 내적으로 Q 함수를 계산합니다.이는 대규모 액션 공간으로 일반화될 수 있지만 소수의 액션만 고려했습니다.Fulda et al.(2017); Ahn et al.(2022)은 어포던스 설정에서 액션 제거를 탐구합니다.Zahavy et al.(2018)은 외부 환경 신호에서 Zork의 잘못된 액션을 제거하는 모델을 학습합니다.그러나 기능은 외부 제거 신호의 존재에 따라 달라집니다.3. 계획, 제거 및 추적 이 섹션에서는 계획, 제거 및 추적(PET)의 3단계 프레임워크를 설명합니다.계획 모듈(MP)에서 사전 학습된 LLM은 훈련 세트의 샘플을 컨텍스트 내 예로 사용하여 입력 작업 설명에 대한 하위 작업 목록을 생성합니다.제거 모듈(ME)은 제로샷 QA 언어 모델을 사용하여 현재 하위 작업과 관련이 없는 객체와 용기를 채점하고 마스크합니다.추적 모듈(MT)은 제로샷 QA 언어 모델을 사용하여 현재 하위 작업이 완료되어 다음 하위 작업으로 이동하는지 확인합니다.계획은 생성 작업이고 제거 및 추적은 분류 작업입니다. 또한, 허용되는 각 행동을 평가하고 전문가에 대한 모방 학습을 통해 훈련되는 주의 기반 에이전트(행동 주의)를 구현합니다. 에이전트는 마스크된 관찰을 관찰하고 현재 하위 작업에 따라 행동을 취합니다. 문제 설정 작업 설명을 T, 시간 단계 t에서의 관찰 문자열을 Ot, 허용되는 행동 목록(실행 가능한 atat)을 At로 정의합니다. 각 관찰 문자열 Ot에 대해 다음을 정의합니다. 학습 세트의 예제 작업 쿼리 대상 출력 두 개의 스프레이 병을 변기에 놓는 데 필요한 중간 단계는 무엇입니까? 스프레이 병을 가져옵니다. 스프레이 병을 변기에 넣습니다. 스프레이 병을 가져옵니다. 스프레이 병을 변기에 넣습니다. 사과를 데워서 냉장고에 넣습니다. 계획 모듈(MP) 사과를 가져옵니다. 사과를 데웁니다. 사과를 냉장고에 넣습니다. 그림 2. 계획 모듈(하위 작업 생성). 작업 쿼리 설명과의 ROBERTa 임베딩 유사성을 기반으로 학습 세트에서 5개의 전체 예제를 선택했습니다. 그런 다음 예제를 작업 쿼리와 연결하여 프롬프트를 얻습니다. 마지막으로 LLM에 원하는 하위 작업을 생성하도록 프롬프트합니다. 관찰 내의 용기와 객체를 각각 r½ 및 o로 지정합니다. 용기와 객체 간의 분류는 환경에 의해 정의됩니다(Shridhar et al., 2020b). 작업 T에 대해 T를 푸는 하위 작업 S7 = {1, ... Sk} 목록이 있다고 가정합니다. 3.1. 계획 실제 세계의 작업은 종종 복잡하고 완료하려면 두 단계 이상이 필요합니다. 복잡한 작업이 주어진 경우 인간이 고수준 하위 작업을 계획할 수 있는 능력에 동기를 부여받아 작업 설명 T에 대한 고수준 하위 작업 목록을 생성하기 위해 계획 모듈(MP)을 설계합니다. LLM(Huang et al., 2022a)을 사용하여 계획하기 위한 상황별 프롬프트 기술에서 영감을 받아 LLM을 계획 모듈 Mp로 사용합니다. 주어진 작업 설명 T에 대해 &quot;T에 필요한 중간 단계는 무엇인가?&quot;라는 질의 질문 Q7을 구성하고 Mp가 하위 작업 ST {S1,…….Sk} 목록을 생성하도록 요구합니다. 구체적으로, 우리는 ROBERTa(Liu et al., 2019) 임베딩 유사성을 기반으로 훈련 세트에서 상위 5개 예제 작업 TE를 선택합니다. 그런 다음 예제 작업을 쿼리-답변 형식의 예제 하위 작업과 연결하여 Mp에 대한 프롬프트 PT를 빌드합니다(그림 2): PT = concat(QT, STE,……..‚ QTE, STE, QT) 프롬프트 형식의 예가 그림 2에 나와 있습니다. 여기서 T = &quot;사과를 데워 냉장고에 넣으세요&quot;, QT=&quot;두 개의 스프레이 병을 변기에 놓기 위해 필요한 중간 단계는 무엇인가요?&quot;, STE = &quot;스프레이 병을 가져가세요, 계획, 제거 및 추적 스프레이 병을 변기에 놓으세요, 스프레이 병을 가져가세요, 스프레이 병을 변기에 놓으세요&quot;. 이 작업 T를 달성하기 위한 예상 하위 작업 목록은 s₁ = &#39;사과 가져가세요&#39;, 82 = &#39;사과 데우세요&#39;, 83 = &#39;사과를 놓으세요&#39;입니다. in/on fridge&#39; 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 커피머신 1, 조리대 2, 조리대 1, 식탁 1, 서랍 5, 서랍 4, 서랍 3, 서랍 2, 서랍 1, 냉장고 1, 쓰레기통 1, 세면대 1, 전자레인지 1이 있습니다. 여러분의 과제는 사과를 데워서 냉장고에 넣는 것입니다. 어디로 가야 할까요? 모듈 제거(ME) 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 조리대 2, 조리대 1, 식탁 1, 냉장고 1, 쓰레기통1이 있습니다. 그림 3. 모듈 제거(수용기 마스킹). 사전 훈련된 QA 모델을 사용하여 각 장면의 관찰에서 무관한 용기/사물을 필터링합니다.보시다시피 원래 관찰은 너무 길고 빨간색으로 표시된 용기는 작업 완료와 관련이 없습니다.이러한 용기는 QA 모델에서 필터링하여 관찰을 더 짧게 만듭니다.3.2. 제거 일반적인 Alfworld 장면은 각각 최대 15개의 물체를 포함하는 약 15개의 용기로 시작할 수 있습니다.최악에 가까운 경우 약 30개의 열 수 있는 용기가 있을 수 있습니다(예: 여러 개의 캐비닛과 서랍이 있는 주방).사전 지식이 없는 에이전트가 원하는 물체를 찾는 데 50단계 이상이 걸립니다(각 용기를 방문하여 열고 닫는 프로세스를 반복).훈련과 평가 중에 많은 용기와 물체가 특정 작업과 관련이 없으며 작업에 대한 상식적 지식으로 쉽게 필터링할 수 있음을 관찰했습니다.예를 들어, 그림 3에서 작업은 사과를 데우는 것입니다. 커피 머신, 쓰레기통 또는 칼과 같은 무관한 용기를 제거하면 관찰 시간을 상당히 단축할 수 있습니다. 따라서 대규모 사전 훈련된 QA 모델에서 포착한 상식적 지식을 활용하여 무관한 용기와 물체를 마스크하도록 제거 모듈 ME를 설계하는 것을 제안합니다. 작업 T의 경우 용기의 경우 Pr. = &quot;귀하의 작업은 다음과 같습니다: T. 어디로 가야 합니까?&quot;, 물체의 경우 Po=&quot;귀하의 작업은 다음과 같습니다: T. 어떤 물체가 관련이 있을까요?&quot; 형식의 프롬프트를 만듭니다. 사전 훈련된 QA 모델 ME를 제로 샷 방식으로 사용하여 각 물체 oi에 대해 점수 poi ME(Po, oi)를 계산하고 각 용기 r에 대해 ME(Po, ri) Pri =를 계산합니다. 모든 단계에서 관찰합니다. μ는 상식적 QA 모델이 물체/용기가 T와 관련이 있다고 믿는지 여부에 대한 신념 점수를 나타냅니다. 그런 다음 μoi &lt; Tor이면 관찰에서 o₂를 제거하고 µri &lt; Tr이면 r¿를 제거합니다. 임계값 To, Tr은 하이퍼파라미터입니다. 환경 당신은 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 ..., 쓰레기통 1, 세면대 1, 토스터 1이 있습니다. ➤ 세면대로 이동 세면대 1에는 아무것도 없습니다. ➤ 식탁2로 이동합니다. } 식탁 1에 사과 1, 빵 3, 컵 3, 후추통 2가 있습니다. 식탁 1에서 사과 1을 꺼냅니다. 사과를 꺼냅니다. 사과를 데웁니다. 하위 작업 사과를 냉장고에 넣습니다. 맥락 식탁 1에 사과 1, 빵 3, .....가 보입니다. 식탁 1에서 사과 1을 꺼냅니다. 사과 가져오기 작업을 끝냈나요? 추적 모듈(MT) 그렇습니다! 진행 상황 추적기 업데이트 그림 4. 추적 모듈(진행 상황 추적). 모든 단계에서 롤아웃의 마지막 3단계를 맥락으로 삼고 현재 하위 작업이 완료되었는지 여부에 대한 쿼리를 추가하여 프롬프트를 가져옵니다. 사전 훈련된 QA 모델은 프롬프트에 대해 예/아니요 답변을 생성합니다. 답변이 &quot;예&quot;이면 추적기를 다음 하위 작업으로 업데이트합니다. 3.3. 추적 에이전트가 상위 수준 계획을 활용하려면 먼저 실행할 하위 작업을 알아야 합니다. 인간 배우는 일반적으로 첫 번째 항목부터 시작하여 완료될 때까지 작업을 하나씩 체크합니다. 따라서 섹션 3.2와 유사하게 사전 훈련된 QA 모델을 사용하여 추적 모듈 MT를 설계하여 제로샷 하위 작업 완료 감지를 수행합니다.¹ P 구체적으로 그림 4에서 설명한 대로 하위 작업 목록 S&amp; = {81, … … … Sk}의 경우 에이전트가 현재 작업 중인 하위 작업(Sp)을 나타내는 진행 추적기(1에서 초기화)를 추적합니다. 그런 다음 에이전트 관찰의 마지막 d 단계로 컨텍스트를 구성합니다.현재 시스템 설계에서는 완료된 하위 작업을 다시 방문할 수 없으므로 에이전트는 테스트 시간에 이전 하위 작업을 실행 취소한 경우 복구할 방법이 없습니다.현재 하위 작업과 &quot;sp 작업을 마쳤습니까?&quot;라는 질문에 대한 계획, 제거 및 추적.효율성을 위해 각 단계에서 d := min(d 1,3)을 설정합니다.진행 추적기가 업데이트될 때마다 d가 1로 재설정됩니다.따라서 템플릿 Pa concat(Ot-d, ..., Ot-1, &quot;sp 작업을 마쳤습니까?&quot;).Pa를 사전 학습된 제로샷 QA 모델 MT에 공급하고 다음과 같이 토큰 &#39;예&#39;와 &#39;아니요&#39;의 확률을 계산합니다.PMT(&quot;예&quot; |Pa) 및 PMò(&quot;아니요&quot;|Pa).PMT(&quot;예&quot; |Pa) &gt; PMT(&quot;아니요&quot; |Pa)이면 추적기 p를 증가시켜 다음 하위 작업을 추적합니다. 추적이 조기에 종료되는 경우, 즉 p len(ST)이지만 환경이 &quot;완료&quot;로 반환되지 않은 경우 T를 사용한 조건화로 돌아갑니다. 4.4절에서 정밀도와 재현율 측면에서 조기 종료 비율을 연구합니다. 3.4. 에이전트 허용되는 동작의 수는 환경에 따라 크게 달라질 수 있으므로 에이전트는 동작 공간의 임의의 차원을 처리해야 합니다. Shridhar et al. (2020b)은 토큰별로 동작을 생성하여 이 과제를 해결하지만 이러한 생성 프로세스는 학습 세트에서도 성능이 저하됩니다. 우리는 모델이 가변적인 입력 길이를 처리하도록 구축되는 텍스트 요약 분야에서 영감을 얻었습니다. See et al. (2017)은 출력을 단어별로 추출하는 어텐션과 유사한 &quot;포인팅&quot; 메커니즘을 통해 요약을 생성합니다. 마찬가지로 어텐션과 유사한 &quot;포인팅&quot; 모델을 사용하여 허용되는 동작 목록에서 동작을 선택할 수 있습니다. 동작 어텐션 우리는 허용되는 동작 중에서 최적의 동작을 출력하는 정책을 학습하는 데 관심이 있습니다. 우리는 (1) 이력에 걸쳐 평균을 내어 관찰을 표현하고, (2) 개별적으로 행동을 인코딩함으로써 장기 롤아웃/대규모 행동 공간 문제를 피합니다(그림 5). 제안하는 행동 주의 프레임워크에서, 우리는 먼저 이력에 걸쳐 모든 개별 관찰의 임베딩 평균으로 과거 관찰 Ht를 표현하고(식 1), HA를 현재 허용되는 모든 행동의 임베딩 목록으로 표현합니다(식 2). 그런 다음 식 3에서 작업 임베딩(H¹), 현재 관찰 임베딩(Ot), 행동 임베딩 목록(HA)에 대한 &quot;쿼리&quot; 헤드(MQ)가 있는 변환기를 사용하여 쿼리 Q를 계산합니다. 식에서 4 우리는 작업 임베딩(H), 현재 관찰 임베딩(Ot), 그리고 액션 임베딩(a;)에 대한 &quot;키&quot; 헤드(MK)를 갖는 동일한 변환기를 사용하여 각 액션 a¿에 대한 키 K¿를 계산합니다. 마지막으로, 우리는 정책 π에 대한 액션 점수로서 쿼리와 키의 내적을 계산합니다(식 5). Ht HA = = avgje[1,t-1] Embed(0³) [Embed(a),..., Embed(a)] Q = Mq (Embed(T), Ht, Embed(Ot), HA) K; = Mƒ (Embed(T), Ht, Embed (Ot), Embed(a)) π = softmax ([Q Kili Є 허용되는 모든 액션]) 4. 실험 및 결과 (1) (2) (3) (4) (5) 우리는 다음과 같이 실험을 제시합니다. 먼저, 우리는 우리의 실험을 위한 환경 설정과 기준선을 설명합니다. 그런 다음 PET를 환경의 다른 분할에 대한 기준선. 마지막으로, 우리는 절제 연구를 수행하고 PET 프레임워크를 부분별로 분석합니다. 우리는 PET가 효율적인 행동 복제 훈련에서 인간의 목표 사양에 더 잘 일반화됨을 보여줍니다. 4.1. 실험 세부 사항 AlfWorld 환경 ALFWorld(Shridhar et al., 2020b)는 ALFRED 구현 데이터 세트(Shridhar et al., 2020a)와 평행한 TextWorld 환경(Côté et al., 2018b) 집합입니다. ALFWorld에는 각각 여러 구성 하위 목표를 해결해야 하는 6가지 작업 유형이 포함됩니다. 3553개의 훈련 작업 인스턴스({tasktype, object, receptacle, room}), 분포 내 평가 작업 인스턴스(보이는 분할 작업 자체는 새롭지만 훈련 중에 보이는 방에서 발생) 및 134개의 분포 외 평가 작업 인스턴스(보이지 않는 분할 - 작업은 새로운 방에서 발생)가 있습니다. 작업의 예는 다음과 같습니다. &quot;전자레인지에 넣기 위해 계란을 헹굽니다.&quot; AlfWorld의 각 교육 인스턴스에는 전문가가 제공되며, 이를 통해 교육 데모를 수집했습니다. 인간 목표 사양 평가를 위한 크라우드 소싱 인간 목표 사양에는 보이지 않는 동사 66개와 보이지 않는 명사 189개가 포함되어 있습니다(Shridhar et al., 2020b). 이에 비해 템플릿 목표는 12가지 목표 사양 방식만 사용합니다. 또한 인간 목표 사양의 문장 구조는 템플릿 목표에 비해 더 다양합니다. 따라서 인간 목표 실험은 분포 외부 시나리오에 대한 모델의 일반화를 테스트하는 데 적합합니다. 사전 훈련된 LM. Plan 모듈(하위 작업 생성)의 경우 오픈 소스 GPT-Neo-2.7B(Black et al., 2021)와 530B 매개변수가 있는 산업 규모 LLM(Smith et al., 2022)을 실험했습니다. 계획, 제거 및 추적Οι α aa 당신은 방 한가운데에 있습니다. 주변을 빠르게 둘러보니 캐비닛 5, 캐비닛 4가 보입니다... 식탁으로 갑니다 전자레인지 1로 갑니다. 식탁 1에서 1. 열어보니 사과 1, 전자레인지 1이 보입니다... 캐비닛으로 갑니다 캐비닛을 살펴봅니다 사과 데웁니다*** 머그잔 3, 컵 3. 식탁 1에서 사과 1을 꺼냅니다. 임베딩 사과 데웁니다 T 임베딩 주의 주의 주의 주의 K사과 데웁니다 그림 5. 에이전트(행동 주의). 행동 주의 블록은 허용되는 각 행동에 대한 키 Ki를 계산하고 관찰에서 키와 쿼리 Q 간의 점곱으로 행동 점수를 출력하는 변환기 기반 프레임워크입니다. 템플릿 목표 사양 인간 목표 사양 모델 보이지 않음 BUTLER + DAgger* (Shridhar et al., 2020b) BUTLER + BC (Shridhar et al., 2020b) GPT (Micheli &amp; Fleuret, 2021)PET Action Attention(Ours)67.seenunseen52.표 1. 인간이 주석을 단 목표가 있는 경우와 없는 경우 평가 분할(보이는 경우와 보이지 않는 경우)당 완료율 측면에서 다양한 모델을 비교한 결과. PET는 템플릿 목표 사양에서 GPT보다 성능이 낮지만 인간의 목표 사양에 더 잘 일반화됩니다.* 완전성을 위해 DAgger가 포함된 BUTLER의 성능을 포함합니다. 다른 모든 행은 환경과의 상호 작용 없이, GPT의 경우 MLE, BUTLER+BC 및 PET의 경우 동작 복제 없이 학습합니다. Eliminate 모듈(수용기/객체 마스킹)의 경우 Macaw-11b(Tafjord &amp; Clark, 2021)를 선택했는데, 이는 GPT3(Brown et al., 2020)와 동일한 상식적 QA 성능을 보이는 반면 규모적으로 더 작은 것으로 보고되었습니다. Macaw 점수에 대해 0.4의 결정 임계값을 사용하며, 이보다 낮으면 객체가 마스킹됩니다. Track 모듈(진행 상황 추적)의 경우, Eliminate 모듈의 Yes/No 질문에 대한 답변과 동일한 Macaw-11b 모델을 사용합니다.Actor 모델 설계.Action Attention 에이전트(MQ 및 MK)는 12개 헤드와 은닉 차원 384를 갖춘 12층 변환기입니다.마지막 층은 두 개의 선형 헤드에 공급되어 K와 Q를 생성합니다.액션과 관찰을 임베딩하기 위해, 임베딩 차원 1024를 갖춘 사전 학습된 ROBERTa-large(Liu et al., 2019)를 사용합니다.하위 작업 생성의 경우, 학습을 위해 기준 진실 하위 작업을 사용하고, 평가를 위해 Plan 모듈에서 생성된 하위 작업을 사용합니다.실험 설정.원래 벤치마크(Shridhar et al., 2020b)와 달리, 동작 복제로 학습된 모델을 실험합니다.Shridhar et al. (2020b) 모델은 DAgger 훈련에서 큰 이점을 얻는다는 것을 관찰했습니다.DAgger는 모든 가능한 상태에서 잘 정의된 전문가를 가정하는데, 이는 비효율적이고 비실용적입니다.실험에서 DAgger를 사용한 훈련은 동작 복제에 비해 100배 느립니다(DAgger의 경우 몇 주, 동작 복제의 경우 6시간).또한 에이전트가 환경과 상호 작용할 수 있는 옵션이 없는 경우에도 모델이 DAgger로 훈련된 BUTLER(Shridhar et al., 2020b) 에이전트의 DAgger 훈련 성능을 능가한다는 것을 보여줍니다.기준선.첫 번째 기준선은 인코더, 애그리게이터, 디코더로 구성된 BUTLER::BRAIN(BUTLER) 에이전트(Shridhar et al., 2020b)입니다.각 시간 단계 t에서 인코더는 초기 관찰 so, 현재 관찰 st, 작업 문자열 Stask를 사용하여 표현 pt를 생성합니다. 순환 집계기는 rt와 마지막 순환 상태 ht-1을 결합하여 ht를 생성한 다음, 이를 동작을 나타내는 문자열 at으로 디코딩합니다. 또한 BUTLER 에이전트는 동작이 실패한 경우 빔 검색을 사용하여 멈춘 상태에서 빠져나옵니다. 두 번째 기준 GPT(Micheli &amp; Fleuret, 2021)는 AlfWorld 훈련 세트의 3553개 데모에 대한 미세 조정된 GPT2-medium입니다. 구체적으로, GPT는 표준 최대 우도 손실을 사용하여 규칙 기반 전문가를 모방하기 위해 각 동작 단계를 단어별로 생성하도록 미세 조정됩니다. 계획, 제거 및 추적 4.2. 템플릿 및 인간 목표에 대한 전반적인 결과 표 1에서 PET가 지원하는 액션 주의의 성능을 BUTLER(Shridhar et al., 2020b) 및 미세 조정된 GPT(Micheli &amp; Fleuret, 2021)와 비교했습니다. 인간 목표 사양의 경우 PET는 보이는 분할에서 SOTA(GPT)보다 25%, 보이지 않는 분할에서 5% 더 우수한 성과를 보였습니다. PET는 템플릿 목표 사양에서 GPT보다 성능이 떨어지지만 GPT는 완전히 텍스트 기반 전문가 궤적에서 미세 조정이 필요하므로 다양한 환경 설정에 대한 적응성이 떨어집니다. 정성적으로, 목표 사양이 분포에서 벗어난 인간 목표 사양 작업에서 GPT는 종종 잘못된 움직임을 한 번 생성한 후 동일한 동작을 반복하는 데 갇힙니다. 반면 PET의 Plan 모듈은 작업에 대해 훈련되지 않았기 때문에 섹션 4.5에 표시된 것처럼 인간 목표 사양에 대한 변형으로 일반화됩니다. 정량적으로 GPT는 템플릿에서 인간 목표 사양으로 전환할 때 상대적으로 50%의 성능 저하가 발생하는 반면 PET는 15~25%만 저하됩니다. PET에 가장 가까운 설정은 동작 복제를 사용한 BUTLER(BUTLER + BC)입니다. BUTLER + BC는 성능이 좋지 않으므로 DAgger 학습 결과도 포함합니다. 그럼에도 불구하고 PET의 도움을 받은 동작 주의는 DAgger를 사용한 BUTLER보다 2배 이상 성능이 뛰어나면서도 훨씬 더 효율적입니다. (4.1절) 4.3. Plan, Eliminate 및 Track에 대한 절제 표 3에서 학습 세트에서 샘플링한 140개 학습 궤적에서 동작 주의 에이전트에 각 구성 요소를 순차적으로 추가하여 각 PET 모듈의 기여도를 분석합니다. 효율적이고 희소한 설정을 위해 데이터 세트 크기는 보이는 검증 세트의 크기와 일치하도록 선택됩니다. Plan과 Track은 별도로 작동할 수 없으므로 이 절제에 대해 단일 모듈로 처리합니다. Plan and Track을 추가하면 완료율이 비교적 60%나 크게 향상되는데, 이는 구체화된 작업을 단계별로 해결하면 복잡성이 줄어든다는 가설에 대한 증거를 제공합니다. 하위 작업 추적 없이 Eliminate를 추가하면 절대 성능이 3% 향상되는 비교적 미미한 것으로 관찰됩니다. 반면 Plan and Track을 사용하여 하위 작업에 Eliminate를 적용하면 Plan and Track만 사용할 때보다 60% 이상의 상대적 향상이 관찰됩니다. 따라서 목표가 하위 작업에 더 집중되어 있을 때 무관한 객체를 제거하기가 더 쉽기 때문에 Plan and Track이 평가 중에 Eliminate의 성능을 높이는 것으로 추론합니다. 4.4. PET 모듈의 자동 분석 Plan 모듈 GPT2-XL(Radford et al., 2019), GPT-Neo2.7B(Black et al., 2021), 530B 매개변수 MT-NLG(Smith et al., 2022) 모델과 같은 다양한 LLM을 실험합니다. 표 2는 생성 정확도와 기준 진실 하위 작업에 대한 ROBERTa(Liu et al., 2019) 임베딩 코사인 유사도를 보고합니다. 모든 LLM이 문장 구조에 변화가 없는 템플릿 목표 사양에서 높은 정확도를 달성하는 것을 관찰했습니다. 인간 목표 사양의 경우 MT-NLG는 임베딩 유사도 측면에서 기준 진실과 유사한 하위 작업을 생성하는 반면 다른 작은 모델은 성능이 상당히 떨어집니다. 모듈 제거 AlfWorld의 세 분할에서 Macaw의 제로샷 리셉터클/객체 마스킹 성능을 평가합니다. 그림 6에서 모델이 각 작업을 완료할 때 규칙 기반 전문가가 상호 작용한 객체 대 객체에 할당하는 관련성 점수의 AUC 곡선을 보여줍니다. Macaw QA 모델은 제로샷 방식으로 쿼리되므로 보이지 않는 분할에서도 환경의 세 분할 모두에서 일관된 마스킹 성능을 보여줍니다. 또한, 섹션 4.5에 설명된 반직관적인 스폰 위치 때문에 객체 수용기 정확도가 일반적으로 객체 정확도보다 낮다는 점에 유의합니다.실험에서 결정 임계값 0.4는 재현율이 0.91이고 관찰에서 객체 수를 평균 40% 줄였습니다.추적 모듈 하위 작업 정렬 정보가 환경에서 제공되지 않으므로 완료 이벤트 감지를 위한 대체 성능 지표를 탐색합니다.이상적으로 하위 작업 추적기는 전문가가 환경을 &quot;완전히 해결&quot;한 경우에만 마지막 하위 작업을 &quot;완료&quot;로 기록해야 합니다.동의 척도로 Macaw-11B의 경우 정밀도 0.99와 재현율 0.78, Macaw-large의 경우 정밀도 0.96과 재현율 0.96을 보고합니다.더 큰 모델(Macaw-11b)은 더 정확하지만 감지를 놓치는 부분이 더 많아 이론적 성능이 78%로 제한됩니다. 더 작은 모델은 인간의 평가에 따르면 훨씬 덜 정확하지만 이론적으로 전체 모델 성능을 제한하지는 않습니다. 실험에서 두 모델 모두 비슷한 전체 결과를 생성한다는 것을 발견했는데, 이는 LLM이 정밀도와 재현율 모두에서 더 나은 성과를 거두면 전체 결과가 개선될 수 있음을 시사합니다. 4.5. 정성적 분석 계획 모듈 표 4에서 하위 작업 생성에 대한 두 가지 유형의 실패 사례를 보여줍니다. 첫 번째 유형의 오류는 기준 진실의 동의어를 생성하여 발생하고 두 번째 유형의 오류는 inaccuPlan, Eliminate 및 Track Template Goals Human Goals LLM GPT-2(Radford et al., 2019) GPT-Neo-2.7B(Black et al., 2021) MT-NLG(Smith et al., 2022) seen unseen seen unseen 94.29(0.97) 87.31(0.94) 10.07(0.62) 7.98(0.58) 99.29(1.00) 98.57(0.99) 96.27(0.98) 100(1.00) 4.70(0.82) 9.16(0.80) 40.04(0.94) 49.3(0.94) 표 2. Plan 모듈에 대한 다양한 LLM의 평가는 정확도와 ROBERTa 임베딩 코사인 유사도(괄호 안)를 기준으로 평가 분할(보이는 것과 보이지 않는 것)당 지상 진실 하위 작업에 대해, 인간이 주석한 목표가 있는 경우와 없는 경우입니다. 530B 매개변수를 사용하는 MT-NLG는 모든 데이터 세트 분할에서 전반적으로 가장 좋은 성능을 달성하며 인간이 목표를 지정한 어려운 작업에서 더 작은 모델의 성능을 크게 능가합니다. 또한 MT-NLG는 모든 작업에 대해 거의 완벽한 임베딩 유사도를 가진 하위 작업을 생성합니다. True Positive Rate True Positive Rate0.0.0.40.기차 수용구 식별을 위한 ROC 곡선0.0.60.0.기차 객체 식별을 위한 ROC 곡선 True Positive Rate 0.0.0.유효한_보이는 수용구 식별을 위한 ROC 곡선 유효한_보이지 않는 수용구 식별을 위한 ROC 곡선 True Positive Rate 0.0.0.0.AUC 0.AUC=0.AUC=0.0.0.0.0.0.0.0.0.거짓 긍정률 0.0.0.0.0.0.0.0.0.0.0.거짓 긍정률 거짓 긍정률 True Positive Rate0.0.0.0.유효한_보이는 객체 식별을 위한 ROC 곡선 True Positive Rate0.0.0.0.2유효한_보이지 않는 객체 식별을 위한 ROC 곡선 AUC=0.AUC=0.AUC=0.0.0.0.0.0.0.0.거짓 긍정률 0.0.0.0.0.0.0.0.0.0.0.거짓 양성률 거짓 양성률 그림 6. Macaw-11b 모델을 사용하여 Alfworld-Thor 환경에서 모든 작업에 대한 제로샷 관련성 식별의 AUC 점수 플롯. 기준 진실은 규칙 기반 전문가가 액세스한 리셉터클/객체로 얻습니다. 위: 리셉터클 관련성 식별. 아래: 객체 관련성 식별. QA 모델은 리셉터클의 경우 평균 AUC-ROC 점수 65점, 객체의 경우 76점을 얻습니다. 모델 삭제 보이지 않는 작업 주의 행동 주의 + 제거 행동 주의 + 계획 및 추적 행동 주의 + PET 52.27. 표 3. 평가 분할(보이고 보이지 않음)당 완료율 측면에서 훈련 세트에서 샘플링된 140개 데모 세트에서 훈련된 PET의 다른 삭제 비교. 영어: 단독 Eliminate 모듈을 적용하는 것은 Plan &amp; Track에 비해 전반적인 성능에 미치는 영향이 미미합니다.그러나 Plan &amp; Track과 함께 하위 작업에 Eliminate 모듈을 적용하면 성능이 훨씬 더 크게 향상됩니다.인간의 목표 사양에서 racies.Action Attention 프레임워크는 하위 작업에 동의어 변형에 강인한 것으로 알려진 ROBERTA(Liu et al., 2019) 임베딩을 사용합니다.Elimnate 모듈 우리는 제거 오류의 주요 원인은 모듈이 관심 있는 객체를 포함하는 용기를 잘못 마스크하여 에이전트가 해당 용기를 찾지 못할 때 발생한다는 것을 관찰했습니다.이는 종종 AI2Thor 시뮬레이터의 일부 객체가 상식에 따라 생성되지 않기 때문입니다.environment²의 설명서에서 언급했듯이 Apple이나 Egg와 같은 객체는 GarbageCan이나 TVStand와 같은 예상치 못한 용기에 생성될 가능성이 있습니다.그러나 AI2Thor에서 이러한 세대는 실제 배포에서는 발생할 가능성이 낮습니다. 따라서 Eliminate 모듈의 &quot;실수&quot;는 타당합니다. Track 모듈 실험적으로 하위 작업 계획/추적이 계산 절차가 필요한 작업에 특히 유용하다는 것을 발견했습니다. 표 ??에서 볼 수 있듯이 PET는 &quot;캐비닛에 비누 두 개 놓기&quot; 작업을 두 개의 반복되는 하위 작업 세트로 나눕니다. &quot;비누 가져가기→캐비닛에 비누 놓기&quot;. 따라서 하위 작업 계획 및 추적은 계산의 어려운 문제를 단순화합니다. 2 ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ 작업 GT Gen 작업 GT Gen 계획, 제거 및 추적 인간 목표 사양 예제 컵을 식혀서 캐비닛에 넣습니다. 머그잔을 식힌다→머그잔을 커피 머신에 넣는다 머그잔을 식힌다 머그잔을 커피 머신에 다시 넣는다 책상에서 연필을 가져와 책상 반대편에 놓는다 연필을 가져온다→연필을 선반에 놓는다 책상 위의 흰색 연필을 집어 올린다→흰색 연필을 책상의 다른 곳에 놓는다 표 4. 인간 목표 사양(작업), 기준 진실(GT) 대 생성된(Gen)에 대한 계획 모듈의 실패 사례. 첫 번째 예에서 생성된 계획은 기준 진실과 다르지만 의미는 일치합니다. 두 번째 예에서 생성된 계획은 인간 목표 사양의 실수로 &quot;선반&quot; 대신 &quot;책상의 다른 쪽&quot;으로 인해 기준 진실과 크게 다릅니다. 5. 결론, 제한 사항 및 향후 작업 이 작업에서 우리는 사전 훈련된 LLM을 사용하여 3단계로 구체화된 에이전트를 지원하는 계획, 제거 및 추적(PET) 프레임워크를 제안합니다. 우리의 PET 프레임워크는 미세 조정이 필요하지 않으며 모든 목표 조건부 구체화된 에이전트와 호환되도록 설계되었습니다. 실험에서 우리는 PET를 AlfWorld의 동적 동작을 처리하는 새로운 Action Attention 에이전트와 결합했습니다. 우리의 Action Attention 에이전트는 BUTLER 기준선보다 훨씬 더 우수한 성능을 보였습니다. 또한 PET 프레임워크는 훈련 세트 작업에 맞게 훈련되지 않았기 때문에 보이지 않는 인간의 목표 지정 작업에 대한 더 나은 일반화를 보여줍니다. 마지막으로, 우리의 절제 연구는 Plan 및 Track 모듈을 함께 사용하면 최상의 성능을 달성하기 위해 Eliminate 모듈의 성능이 향상됨을 보여줍니다. 우리의 결과는 LLM이 구체화된 에이전트에 대한 상식과 절차적 지식의 좋은 소스가 될 수 있으며, 여러 LLM을 서로 조정하여 효과를 더욱 향상시킬 수 있음을 보여줍니다. 현재 시스템 설계의 주요 한계 중 하나는 Track 모듈(진행 추적기)이 완료된 하위 작업을 다시 방문하지 않는다는 것입니다. 예를 들어 에이전트가 하위 작업(팬 집어 올리기, 팬을 조리대 위에 놓기)을 실행하고 팬을 집어 올렸지만 냉장고에 넣었다면(픽업 작업 취소). 진행 추적기는 이전 진행이 취소된 것을 고려하지 않으므로 이 상황에서 시스템이 중단될 수 있습니다. 향후 작업은 이러한 제한을 해결하기 위해 하위 작업 수준 동적 재계획을 추가하는 데 집중하거나 LLM이 정책 학습을 도울 수 있는 다른 방법(예: 환경에 대한 지침서 읽기)을 탐색할 수 있습니다. 참고문헌 Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Zhang, E., Ruano, RJ, Jeffrey, K., Jesmonth, S., Joshi, NJ, Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., 토셰프, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic assistances, 2022. URL https: //arxiv.org/abs/2204.01691. Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomous-acquired skills via goal generation. arXiv 사전 인쇄본 arXiv:2006.07185, 2020. Andreas, J., Klein, D., and Levine, S. 정책 스케치를 사용한 모듈식 멀티태스크 강화 학습. International Conference on Machine Learning, pp. 166–175. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021년 3월. URL https://doi.org/10.5281/zenodo. 5297715. 이 소프트웨어를 사용하는 경우 이 메타데이터를 사용하여 인용해 주세요. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for highlevel natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Bommasani, R., Hudson, DA, Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, MS, Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, JQ, Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., 헨더슨, P., Hewitt, J., Ho, DE, Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, PW, Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, XL, Li, X., Ma, Plan, Eliminate 및 Track T., Malik, A., Manning, CD, Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., 니블스, JC, Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, JS, Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, AW, Tramèr, F., Wang, RE, Wang, W., Wu, B., Wu, J., Wu, Y., Xie, SM, Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., 장 Y., 정, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2021. URL https://arxiv.org/abs/2108.07258. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, JD, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. 언어 모델은 few-shot learner입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. Chaplot, DS, Gandhi, D., Gupta, A., and Salakhutdinov, R. 목표 지향적 의미 탐색을 사용한 객체 목표 탐색, 2020. URL https://arxiv. org/abs/2007.00643. Cideron, G., Seurin, M., Strub, F., Pietquin, O. Higher: 경험 재생을 위한 사후 분석 생성을 통한 교육 수행 개선. 2020 IEEE 계산 지능 심포지엄 시리즈(SSCI), pp. 225–232. IEEE, 2020. Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, LE, Adada, M., et al. Textworld: 텍스트 기반 게임을 위한 학습 환경. Workshop on Computer Games, pp. 41–75. Springer, 2018a. Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, LE, Adada, M., et al. Textworld: 텍스트 기반 게임을 위한 학습 환경. Workshop on Computer Games에서, pp. 41–75. Springer, 2018b. Fulda, N., Ricks, D., Murdoch, B., Wingate, D. 바위로 무엇을 할 수 있을까? 단어 임베딩을 통한 어포던스 추출. arXiv 사전 인쇄본 arXiv:1703.03429, 2017. Goyal, P., Niekum, S., Mooney, R. Pixl2r: 픽셀을 보상에 매핑하여 자연어를 사용한 강화 학습 안내. Conference on Robot Learning에서, pp. 485–497. PMLR, 2021. He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., Ostendorf, M. 자연어 액션 공간을 사용한 심층 강화 학습. arXiv:1511.04636, 2015. arXiv 사전 인쇄본 Huang, W., Abbeel, P., Pathak, D., Mordatch, I. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트를 위한 실행 가능한 지식 추출, 2022a. URL https://arxiv.org/abs/2201.07207. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. 내면의 독백: 언어 모델을 통한 계획을 통한 체화된 추론, 2022b. URL https://arxiv.org/abs/2207.05608. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: 로봇 모방 학습을 통한 Zeroshot 작업 일반화. Conference on Robot Learning, pp. 991–1002. PMLR, 2022. Jiang, Y., Gu, SS, Murphy, KP, and Finn, C. 계층적 심층 강화 학습을 위한 추상화로서의 언어. 신경 정보 처리 시스템의 발전, 32, 2019. Kollar, T., Tellex, S., Roy, D., and Roy, N. 자연어 방향을 이해하기 위해. 제5회 ACM/IEEE 국제 휴먼로봇 상호작용(HRI) 컨퍼런스, pp. 259–266. IEEE, 2010. Lin, BY, Huang, C., Liu, Q., Gu, W., Sommerer, S., and Ren, X. 언어 모델을 사용한 구체화된 작업에 대한 근거 계획. arXiv 사전 인쇄본 arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: 강력하게 최적화된 bert 사전 학습 접근법. arXiv 사전 인쇄본 arXiv:1907.11692, 2019. MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk the talk: Connecting language, knowledge, and action in route instruction. Def, 2(6):4, 2006. Mei, H., Bansal, M., and Walter, MR Listen, attention, and walk: Neural mapping of navigational instruction to action sequences. 2016년 제30회 AAAI 인공지능 컨퍼런스에서. Micheli, V. 및 Fleuret, F. 언어 모델은 fewshot butlers입니다. arXiv 사전 인쇄본 arXiv:2104.07972, 2021. Min, SY, Chaplot, DS, Ravikumar, P., Bisk, Y. 및 Salakhutdinov, R. 필름: 모듈러를 사용하여 언어의 지시를 따르기
--- EXPERIMENT ---
s와 분석은 LLM이 상식적 QA를 통해 관찰에서 작업과 관련 없는 객체의 40%를 제거할 뿐만 아니라 99% 정확도로 고수준 하위 작업을 생성한다는 것을 보여줍니다. 또한 여러 LLM을 서로 조정하여 사용하여 에이전트를 다양한 측면에서 지원할 수 있습니다. 저희의 기여는 다음과 같습니다. 1. PET: 구체화된 에이전트가 있는 사전 훈련된 LLM을 활용하기 위한 새로운 프레임워크. 저희의 연구에 따르면 P, E, T는 각각 보완적인 역할을 하며 제어 작업을 처리하기 위해 동시에 처리해야 합니다. 2. 텍스트 환경에 대한 변화하는 작업 공간을 처리하는 작업 주의 에이전트. 3. 하위 작업 계획 및 추적을 통해 인간의 목표에 대한 일반화를 위한 SOTA보다 15% 향상. 2. 관련 연구 언어 조건 정책 기존 연구의 상당 부분은 모방 학습(Tellex 등, 2011; Mei 등, 2016; Nair 등, 2022; Stepputtis 등, 2020; Jang 등, 2022; Shridhar 등, 2022; Sharma 등, 2021)이나 강화 학습(Misra 등, 2017; Jiang 등, 2019; Cideron 등, 2020; Goyal 등, 2021; Nair 등, 2022; Akakzia 등, 2020) 정책을 자연어 지시 또는 목표에 따라 조건화한 연구(MacMahon 등, 2006; Kollar 등, 2010)입니다. 일부 이전 연구에서는 사전 훈련된 언어 임베딩을 사용하여 새로운 지침에 대한 일반화를 개선했지만(Nair et al., 2022), LLM에서 포착되는 도메인 지식이 부족합니다. 당사의 PET 프레임워크는 LLM을 사용하여 계획, 진행 추적 및 관찰 필터링을 가능하게 하며 위의 모든 언어 조건부 정책과 호환되도록 설계되었습니다. 제어를 위한 LLM LLM은 최근 고수준 계획에서 성공을 거두었습니다. Huang et al. (2022a)은 사전 훈련된 LLM이 일상 업무에 대한 그럴듯한 계획을 생성할 수 있지만 생성된 하위 업무를 엔드투엔드 제어 환경에서 직접 실행할 수 없음을 보여줍니다. Ahn et al. (2022)은 LLM 작업 선택을 다시 평가하기 위해 작업 점수 모델을 훈련하여 실행 가능성 문제를 해결하고 로봇에서 성공을 보여줍니다. 그러나 LLM 점수는 픽/플레이스로 제한된 동작이 있는 간단한 환경에서는 작동하지만(Ahn et al., 2022), 더 많은 객체와 다양한 동작이 있는 환경에서는 작동하지 않습니다(Shridhar et al., 2020b). Song et al.(2022)은 GPT3를 사용하여 단계별 저수준 명령을 생성한 다음 해당 제어 정책에 의해 실행합니다. 이 작업은 더 많은 동작 다양성과 즉석 재계획으로 Ahn et al.(2022)을 개선합니다. 또한 위의 모든 LLM은 최대 17개 예제의 몇 가지 샷 데모가 필요하므로 AlfWorld에서 프롬프트의 길이가 실행 불가능합니다. Micheli &amp; Fleuret(2021)은 AlfWorld에서 전문가 궤적에 대한 GPT2-medium 모델을 미세 조정하여 인상적인 평가 결과를 보여주었습니다. 그러나 LM 미세 조정에는 완전히 텍스트 기반 환경, 일관된 전문가 궤적 및 완전히 텍스트 기반 동작 공간이 필요합니다. 이러한 요구 사항은 다른 도메인 및 다른 형태의 작업 사양에 대한 일반화를 크게 제한합니다.우리는 PET 프레임워크가 에이전트가 훈련되지 않은 인간의 목표 사양에 대한 더 나은 일반화를 달성한다는 것을 보여줍니다.자연어를 통한 계층적 계획자연어의 구조적 특성으로 인해 Andreas 등(2017)은 각 작업 설명을 모듈식 하위 정책에 연결하는 것을 탐구했습니다.나중의 연구는 단일 조건부 정책(Mei 등, 2016)을 사용하거나 하위 작업을 템플릿에 매칭(Oh 등, 2017)하여 위의 접근 방식을 확장합니다.최근 연구에 따르면 LLM은 숙련된 고급 계획자(Huang 등, 2022a; Ahn 등, 2022; Lin 등, 2022)이므로 진행 상황 추적을 통한 계층적 작업 계획의 아이디어를 재고하도록 동기를 부여합니다. 영어: 저희가 아는 한, PET는 제로샷 하위 작업 수준 LLM 플래너와 제로샷 LLM 진행 상황 추적기를 저수준 조건부 하위 작업 정책과 결합한 최초의 작업입니다.텍스트 게임 텍스트 기반 게임은 게임 상태와 액션 공간이 자연어로 표현된 복잡한 대화형 시뮬레이션입니다.이들은 언어 중심 머신 러닝 연구에 적합한 토양입니다.언어 이해 외에도 성공적인 플레이에는 기억과 계획, 탐색(시행착오), 상식과 같은 기술이 필요합니다.AlfWorld(Shridhar et al., 2020b) 시뮬레이터는 일반적인 텍스트 기반 게임 시뮬레이터인 TextWorld Côté et al.(2018a)를 확장하여 각 ALFRED 장면의 텍스트 기반 아날로그를 만듭니다.대규모 액션 공간을 위한 에이전트 He et al.(2015)은 두 가지 다른 모델을 사용하여 상태와 액션에 대한 표현을 학습하고 표현의 내적으로 Q 함수를 계산합니다.이는 대규모 액션 공간으로 일반화될 수 있지만 소수의 액션만 고려했습니다.Fulda et al.(2017); Ahn et al.(2022)은 어포던스 설정에서 액션 제거를 탐구합니다.Zahavy et al.(2018)은 외부 환경 신호에서 Zork의 잘못된 액션을 제거하는 모델을 학습합니다.그러나 기능은 외부 제거 신호의 존재에 따라 달라집니다.3. 계획, 제거 및 추적 이 섹션에서는 계획, 제거 및 추적(PET)의 3단계 프레임워크를 설명합니다.계획 모듈(MP)에서 사전 학습된 LLM은 훈련 세트의 샘플을 컨텍스트 내 예로 사용하여 입력 작업 설명에 대한 하위 작업 목록을 생성합니다.제거 모듈(ME)은 제로샷 QA 언어 모델을 사용하여 현재 하위 작업과 관련이 없는 객체와 용기를 채점하고 마스크합니다.추적 모듈(MT)은 제로샷 QA 언어 모델을 사용하여 현재 하위 작업이 완료되어 다음 하위 작업으로 이동하는지 확인합니다.계획은 생성 작업이고 제거 및 추적은 분류 작업입니다. 또한, 허용되는 각 행동을 평가하고 전문가에 대한 모방 학습을 통해 훈련되는 주의 기반 에이전트(행동 주의)를 구현합니다. 에이전트는 마스크된 관찰을 관찰하고 현재 하위 작업에 따라 행동을 취합니다. 문제 설정 작업 설명을 T, 시간 단계 t에서의 관찰 문자열을 Ot, 허용되는 행동 목록(실행 가능한 atat)을 At로 정의합니다. 각 관찰 문자열 Ot에 대해 다음을 정의합니다. 학습 세트의 예제 작업 쿼리 대상 출력 두 개의 스프레이 병을 변기에 놓는 데 필요한 중간 단계는 무엇입니까? 스프레이 병을 가져옵니다. 스프레이 병을 변기에 넣습니다. 스프레이 병을 가져옵니다. 스프레이 병을 변기에 넣습니다. 사과를 데워서 냉장고에 넣습니다. 계획 모듈(MP) 사과를 가져옵니다. 사과를 데웁니다. 사과를 냉장고에 넣습니다. 그림 2. 계획 모듈(하위 작업 생성). 작업 쿼리 설명과의 ROBERTa 임베딩 유사성을 기반으로 학습 세트에서 5개의 전체 예제를 선택했습니다. 그런 다음 예제를 작업 쿼리와 연결하여 프롬프트를 얻습니다. 마지막으로 LLM에 원하는 하위 작업을 생성하도록 프롬프트합니다. 관찰 내의 용기와 객체를 각각 r½ 및 o로 지정합니다. 용기와 객체 간의 분류는 환경에 의해 정의됩니다(Shridhar et al., 2020b). 작업 T에 대해 T를 푸는 하위 작업 S7 = {1, ... Sk} 목록이 있다고 가정합니다. 3.1. 계획 실제 세계의 작업은 종종 복잡하고 완료하려면 두 단계 이상이 필요합니다. 복잡한 작업이 주어진 경우 인간이 고수준 하위 작업을 계획할 수 있는 능력에 동기를 부여받아 작업 설명 T에 대한 고수준 하위 작업 목록을 생성하기 위해 계획 모듈(MP)을 설계합니다. LLM(Huang et al., 2022a)을 사용하여 계획하기 위한 상황별 프롬프트 기술에서 영감을 받아 LLM을 계획 모듈 Mp로 사용합니다. 주어진 작업 설명 T에 대해 &quot;T에 필요한 중간 단계는 무엇인가?&quot;라는 질의 질문 Q7을 구성하고 Mp가 하위 작업 ST {S1,…….Sk} 목록을 생성하도록 요구합니다. 구체적으로, 우리는 ROBERTa(Liu et al., 2019) 임베딩 유사성을 기반으로 훈련 세트에서 상위 5개 예제 작업 TE를 선택합니다. 그런 다음 예제 작업을 쿼리-답변 형식의 예제 하위 작업과 연결하여 Mp에 대한 프롬프트 PT를 빌드합니다(그림 2): PT = concat(QT, STE,……..‚ QTE, STE, QT) 프롬프트 형식의 예가 그림 2에 나와 있습니다. 여기서 T = &quot;사과를 데워 냉장고에 넣으세요&quot;, QT=&quot;두 개의 스프레이 병을 변기에 놓기 위해 필요한 중간 단계는 무엇인가요?&quot;, STE = &quot;스프레이 병을 가져가세요, 계획, 제거 및 추적 스프레이 병을 변기에 놓으세요, 스프레이 병을 가져가세요, 스프레이 병을 변기에 놓으세요&quot;. 이 작업 T를 달성하기 위한 예상 하위 작업 목록은 s₁ = &#39;사과 가져가세요&#39;, 82 = &#39;사과 데우세요&#39;, 83 = &#39;사과를 놓으세요&#39;입니다. in/on fridge&#39; 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 커피머신 1, 조리대 2, 조리대 1, 식탁 1, 서랍 5, 서랍 4, 서랍 3, 서랍 2, 서랍 1, 냉장고 1, 쓰레기통 1, 세면대 1, 전자레인지 1이 있습니다. 여러분의 과제는 사과를 데워서 냉장고에 넣는 것입니다. 어디로 가야 할까요? 모듈 제거(ME) 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 조리대 2, 조리대 1, 식탁 1, 냉장고 1, 쓰레기통1이 있습니다. 그림 3. 모듈 제거(수용기 마스킹). 사전 훈련된 QA 모델을 사용하여 각 장면의 관찰에서 무관한 용기/사물을 필터링합니다.보시다시피 원래 관찰은 너무 길고 빨간색으로 표시된 용기는 작업 완료와 관련이 없습니다.이러한 용기는 QA 모델에서 필터링하여 관찰을 더 짧게 만듭니다.3.2. 제거 일반적인 Alfworld 장면은 각각 최대 15개의 물체를 포함하는 약 15개의 용기로 시작할 수 있습니다.최악에 가까운 경우 약 30개의 열 수 있는 용기가 있을 수 있습니다(예: 여러 개의 캐비닛과 서랍이 있는 주방).사전 지식이 없는 에이전트가 원하는 물체를 찾는 데 50단계 이상이 걸립니다(각 용기를 방문하여 열고 닫는 프로세스를 반복).훈련과 평가 중에 많은 용기와 물체가 특정 작업과 관련이 없으며 작업에 대한 상식적 지식으로 쉽게 필터링할 수 있음을 관찰했습니다.예를 들어, 그림 3에서 작업은 사과를 데우는 것입니다. 커피 머신, 쓰레기통 또는 칼과 같은 무관한 용기를 제거하면 관찰 시간을 상당히 단축할 수 있습니다. 따라서 대규모 사전 훈련된 QA 모델에서 포착한 상식적 지식을 활용하여 무관한 용기와 물체를 마스크하도록 제거 모듈 ME를 설계하는 것을 제안합니다. 작업 T의 경우 용기의 경우 Pr. = &quot;귀하의 작업은 다음과 같습니다: T. 어디로 가야 합니까?&quot;, 물체의 경우 Po=&quot;귀하의 작업은 다음과 같습니다: T. 어떤 물체가 관련이 있을까요?&quot; 형식의 프롬프트를 만듭니다. 사전 훈련된 QA 모델 ME를 제로 샷 방식으로 사용하여 각 물체 oi에 대해 점수 poi ME(Po, oi)를 계산하고 각 용기 r에 대해 ME(Po, ri) Pri =를 계산합니다. 모든 단계에서 관찰합니다. μ는 상식적 QA 모델이 물체/용기가 T와 관련이 있다고 믿는지 여부에 대한 신념 점수를 나타냅니다. 그런 다음 μoi &lt; Tor이면 관찰에서 o₂를 제거하고 µri &lt; Tr이면 r¿를 제거합니다. 임계값 To, Tr은 하이퍼파라미터입니다. 환경 당신은 방 한가운데에 있습니다. 주변을 잠깐 둘러보니 ..., 쓰레기통 1, 세면대 1, 토스터 1이 있습니다. ➤ 세면대로 이동 세면대 1에는 아무것도 없습니다. ➤ 식탁2로 이동합니다. } 식탁 1에 사과 1, 빵 3, 컵 3, 후추통 2가 있습니다. 식탁 1에서 사과 1을 꺼냅니다. 사과를 꺼냅니다. 사과를 데웁니다. 하위 작업 사과를 냉장고에 넣습니다. 맥락 식탁 1에 사과 1, 빵 3, .....가 보입니다. 식탁 1에서 사과 1을 꺼냅니다. 사과 가져오기 작업을 끝냈나요? 추적 모듈(MT) 그렇습니다! 진행 상황 추적기 업데이트 그림 4. 추적 모듈(진행 상황 추적). 모든 단계에서 롤아웃의 마지막 3단계를 맥락으로 삼고 현재 하위 작업이 완료되었는지 여부에 대한 쿼리를 추가하여 프롬프트를 가져옵니다. 사전 훈련된 QA 모델은 프롬프트에 대해 예/아니요 답변을 생성합니다. 답변이 &quot;예&quot;이면 추적기를 다음 하위 작업으로 업데이트합니다. 3.3. 추적 에이전트가 상위 수준 계획을 활용하려면 먼저 실행할 하위 작업을 알아야 합니다. 인간 배우는 일반적으로 첫 번째 항목부터 시작하여 완료될 때까지 작업을 하나씩 체크합니다. 따라서 섹션 3.2와 유사하게 사전 훈련된 QA 모델을 사용하여 추적 모듈 MT를 설계하여 제로샷 하위 작업 완료 감지를 수행합니다.¹ P 구체적으로 그림 4에서 설명한 대로 하위 작업 목록 S&amp; = {81, … … … Sk}의 경우 에이전트가 현재 작업 중인 하위 작업(Sp)을 나타내는 진행 추적기(1에서 초기화)를 추적합니다. 그런 다음 에이전트 관찰의 마지막 d 단계로 컨텍스트를 구성합니다.현재 시스템 설계에서는 완료된 하위 작업을 다시 방문할 수 없으므로 에이전트는 테스트 시간에 이전 하위 작업을 실행 취소한 경우 복구할 방법이 없습니다.현재 하위 작업과 &quot;sp 작업을 마쳤습니까?&quot;라는 질문에 대한 계획, 제거 및 추적.효율성을 위해 각 단계에서 d := min(d 1,3)을 설정합니다.진행 추적기가 업데이트될 때마다 d가 1로 재설정됩니다.따라서 템플릿 Pa concat(Ot-d, ..., Ot-1, &quot;sp 작업을 마쳤습니까?&quot;).Pa를 사전 학습된 제로샷 QA 모델 MT에 공급하고 다음과 같이 토큰 &#39;예&#39;와 &#39;아니요&#39;의 확률을 계산합니다.PMT(&quot;예&quot; |Pa) 및 PMò(&quot;아니요&quot;|Pa).PMT(&quot;예&quot; |Pa) &gt; PMT(&quot;아니요&quot; |Pa)이면 추적기 p를 증가시켜 다음 하위 작업을 추적합니다. 추적이 조기에 종료되는 경우, 즉 p len(ST)이지만 환경이 &quot;완료&quot;로 반환되지 않은 경우 T를 사용한 조건화로 돌아갑니다. 4.4절에서 정밀도와 재현율 측면에서 조기 종료 비율을 연구합니다. 3.4. 에이전트 허용되는 동작의 수는 환경에 따라 크게 달라질 수 있으므로 에이전트는 동작 공간의 임의의 차원을 처리해야 합니다. Shridhar et al. (2020b)은 토큰별로 동작을 생성하여 이 과제를 해결하지만 이러한 생성 프로세스는 학습 세트에서도 성능이 저하됩니다. 우리는 모델이 가변적인 입력 길이를 처리하도록 구축되는 텍스트 요약 분야에서 영감을 얻었습니다. See et al. (2017)은 출력을 단어별로 추출하는 어텐션과 유사한 &quot;포인팅&quot; 메커니즘을 통해 요약을 생성합니다. 마찬가지로 어텐션과 유사한 &quot;포인팅&quot; 모델을 사용하여 허용되는 동작 목록에서 동작을 선택할 수 있습니다. 동작 어텐션 우리는 허용되는 동작 중에서 최적의 동작을 출력하는 정책을 학습하는 데 관심이 있습니다. 우리는 (1) 이력에 걸쳐 평균을 내어 관찰을 표현하고, (2) 개별적으로 행동을 인코딩함으로써 장기 롤아웃/대규모 행동 공간 문제를 피합니다(그림 5). 제안하는 행동 주의 프레임워크에서, 우리는 먼저 이력에 걸쳐 모든 개별 관찰의 임베딩 평균으로 과거 관찰 Ht를 표현하고(식 1), HA를 현재 허용되는 모든 행동의 임베딩 목록으로 표현합니다(식 2). 그런 다음 식 3에서 작업 임베딩(H¹), 현재 관찰 임베딩(Ot), 행동 임베딩 목록(HA)에 대한 &quot;쿼리&quot; 헤드(MQ)가 있는 변환기를 사용하여 쿼리 Q를 계산합니다. 식에서 4 우리는 작업 임베딩(H), 현재 관찰 임베딩(Ot), 그리고 액션 임베딩(a;)에 대한 &quot;키&quot; 헤드(MK)를 갖는 동일한 변환기를 사용하여 각 액션 a¿에 대한 키 K¿를 계산합니다. 마지막으로, 우리는 정책 π에 대한 액션 점수로서 쿼리와 키의 내적을 계산합니다(식 5). Ht HA = = avgje[1,t-1] Embed(0³) [Embed(a),..., Embed(a)] Q = Mq (Embed(T), Ht, Embed(Ot), HA) K; = Mƒ (Embed(T), Ht, Embed (Ot), Embed(a)) π = softmax ([Q Kili Є 허용되는 모든 액션]) 4. 실험 및 결과 (1) (2) (3) (4) (5) 우리는 다음과 같이 실험을 제시합니다. 먼저, 우리는 우리의 실험을 위한 환경 설정과 기준선을 설명합니다. 그런 다음 PET를 환경의 다른 분할에 대한 기준선. 마지막으로, 우리는 절제 연구를 수행하고 PET 프레임워크를 부분별로 분석합니다. 우리는 PET가 효율적인 행동 복제 훈련에서 인간의 목표 사양에 더 잘 일반화됨을 보여줍니다. 4.1. 실험 세부 사항 AlfWorld 환경 ALFWorld(Shridhar et al., 2020b)는 ALFRED 구현 데이터 세트(Shridhar et al., 2020a)와 평행한 TextWorld 환경(Côté et al., 2018b) 집합입니다. ALFWorld에는 각각 여러 구성 하위 목표를 해결해야 하는 6가지 작업 유형이 포함됩니다. 3553개의 훈련 작업 인스턴스({tasktype, object, receptacle, room}), 분포 내 평가 작업 인스턴스(보이는 분할 작업 자체는 새롭지만 훈련 중에 보이는 방에서 발생) 및 134개의 분포 외 평가 작업 인스턴스(보이지 않는 분할 - 작업은 새로운 방에서 발생)가 있습니다. 작업의 예는 다음과 같습니다. &quot;전자레인지에 넣기 위해 계란을 헹굽니다.&quot; AlfWorld의 각 교육 인스턴스에는 전문가가 제공되며, 이를 통해 교육 데모를 수집했습니다. 인간 목표 사양 평가를 위한 크라우드 소싱 인간 목표 사양에는 보이지 않는 동사 66개와 보이지 않는 명사 189개가 포함되어 있습니다(Shridhar et al., 2020b). 이에 비해 템플릿 목표는 12가지 목표 사양 방식만 사용합니다. 또한 인간 목표 사양의 문장 구조는 템플릿 목표에 비해 더 다양합니다. 따라서 인간 목표 실험은 분포 외부 시나리오에 대한 모델의 일반화를 테스트하는 데 적합합니다. 사전 훈련된 LM. Plan 모듈(하위 작업 생성)의 경우 오픈 소스 GPT-Neo-2.7B(Black et al., 2021)와 530B 매개변수가 있는 산업 규모 LLM(Smith et al., 2022)을 실험했습니다. 계획, 제거 및 추적Οι α aa 당신은 방 한가운데에 있습니다. 주변을 빠르게 둘러보니 캐비닛 5, 캐비닛 4가 보입니다... 식탁으로 갑니다 전자레인지 1로 갑니다. 식탁 1에서 1. 열어보니 사과 1, 전자레인지 1이 보입니다... 캐비닛으로 갑니다 캐비닛을 살펴봅니다 사과 데웁니다*** 머그잔 3, 컵 3. 식탁 1에서 사과 1을 꺼냅니다. 임베딩 사과 데웁니다 T 임베딩 주의 주의 주의 주의 K사과 데웁니다 그림 5. 에이전트(행동 주의). 행동 주의 블록은 허용되는 각 행동에 대한 키 Ki를 계산하고 관찰에서 키와 쿼리 Q 간의 점곱으로 행동 점수를 출력하는 변환기 기반 프레임워크입니다. 템플릿 목표 사양 인간 목표 사양 모델 보이지 않음 BUTLER + DAgger* (Shridhar et al., 2020b) BUTLER + BC (Shridhar et al., 2020b) GPT (Micheli &amp; Fleuret, 2021)PET Action Attention(Ours)67.seenunseen52.표 1. 인간이 주석을 단 목표가 있는 경우와 없는 경우 평가 분할(보이는 경우와 보이지 않는 경우)당 완료율 측면에서 다양한 모델을 비교한 결과. PET는 템플릿 목표 사양에서 GPT보다 성능이 낮지만 인간의 목표 사양에 더 잘 일반화됩니다.* 완전성을 위해 DAgger가 포함된 BUTLER의 성능을 포함합니다. 다른 모든 행은 환경과의 상호 작용 없이, GPT의 경우 MLE, BUTLER+BC 및 PET의 경우 동작 복제 없이 학습합니다. Eliminate 모듈(수용기/객체 마스킹)의 경우 Macaw-11b(Tafjord &amp; Clark, 2021)를 선택했는데, 이는 GPT3(Brown et al., 2020)와 동일한 상식적 QA 성능을 보이는 반면 규모적으로 더 작은 것으로 보고되었습니다. Macaw 점수에 대해 0.4의 결정 임계값을 사용하며, 이보다 낮으면 객체가 마스킹됩니다. Track 모듈(진행 상황 추적)의 경우, Eliminate 모듈의 Yes/No 질문에 대한 답변과 동일한 Macaw-11b 모델을 사용합니다.Actor 모델 설계.Action Attention 에이전트(MQ 및 MK)는 12개 헤드와 은닉 차원 384를 갖춘 12층 변환기입니다.마지막 층은 두 개의 선형 헤드에 공급되어 K와 Q를 생성합니다.액션과 관찰을 임베딩하기 위해, 임베딩 차원 1024를 갖춘 사전 학습된 ROBERTa-large(Liu et al., 2019)를 사용합니다.하위 작업 생성의 경우, 학습을 위해 기준 진실 하위 작업을 사용하고, 평가를 위해 Plan 모듈에서 생성된 하위 작업을 사용합니다.실험 설정.원래 벤치마크(Shridhar et al., 2020b)와 달리, 동작 복제로 학습된 모델을 실험합니다.Shridhar et al. (2020b) 모델은 DAgger 훈련에서 큰 이점을 얻는다는 것을 관찰했습니다.DAgger는 모든 가능한 상태에서 잘 정의된 전문가를 가정하는데, 이는 비효율적이고 비실용적입니다.실험에서 DAgger를 사용한 훈련은 동작 복제에 비해 100배 느립니다(DAgger의 경우 몇 주, 동작 복제의 경우 6시간).또한 에이전트가 환경과 상호 작용할 수 있는 옵션이 없는 경우에도 모델이 DAgger로 훈련된 BUTLER(Shridhar et al., 2020b) 에이전트의 DAgger 훈련 성능을 능가한다는 것을 보여줍니다.기준선.첫 번째 기준선은 인코더, 애그리게이터, 디코더로 구성된 BUTLER::BRAIN(BUTLER) 에이전트(Shridhar et al., 2020b)입니다.각 시간 단계 t에서 인코더는 초기 관찰 so, 현재 관찰 st, 작업 문자열 Stask를 사용하여 표현 pt를 생성합니다. 순환 집계기는 rt와 마지막 순환 상태 ht-1을 결합하여 ht를 생성한 다음, 이를 동작을 나타내는 문자열 at으로 디코딩합니다. 또한 BUTLER 에이전트는 동작이 실패한 경우 빔 검색을 사용하여 멈춘 상태에서 빠져나옵니다. 두 번째 기준 GPT(Micheli &amp; Fleuret, 2021)는 AlfWorld 훈련 세트의 3553개 데모에 대한 미세 조정된 GPT2-medium입니다. 구체적으로, GPT는 표준 최대 우도 손실을 사용하여 규칙 기반 전문가를 모방하기 위해 각 동작 단계를 단어별로 생성하도록 미세 조정됩니다. 계획, 제거 및 추적 4.2. 템플릿 및 인간 목표에 대한 전반적인 결과 표 1에서 PET가 지원하는 액션 주의의 성능을 BUTLER(Shridhar et al., 2020b) 및 미세 조정된 GPT(Micheli &amp; Fleuret, 2021)와 비교했습니다. 인간 목표 사양의 경우 PET는 보이는 분할에서 SOTA(GPT)보다 25%, 보이지 않는 분할에서 5% 더 우수한 성과를 보였습니다. PET는 템플릿 목표 사양에서 GPT보다 성능이 떨어지지만 GPT는 완전히 텍스트 기반 전문가 궤적에서 미세 조정이 필요하므로 다양한 환경 설정에 대한 적응성이 떨어집니다. 정성적으로 목표 사양이 분포에서 벗어난 인간 목표 사양 작업에서 GPT는 종종 잘못된 움직임을 한 번 생성한 후 동일한 동작을 반복하는 데 갇힙니다. 반면 PET의 Plan 모듈은 해당 작업에 대해 훈련되지 않았기 때문에 섹션 4.5에 표시된 것처럼 인간 목표 사양에 대한 변형으로 일반화됩니다. 정량적으로 GPT는 템플릿에서 인간 목표 사양으로 전환할 때 상대적으로 50%의 성능 저하가 발생하는 반면 PET는 15~25%만 저하됩니다. PET에 가장 가까운 설정은 동작 복제를 사용한 BUTLER(BUTLER + BC)입니다. BUTLER + BC는 성능이 좋지 않으므로 DAgger 학습 결과도 포함합니다. 그럼에도 불구하고 PET의 도움을 받은 동작 주의는 DAgger를 사용한 BUTLER보다 2배 이상 성능이 뛰어나면서도 훨씬 더 효율적입니다. (4.1절) 4.3. Plan, Eliminate 및 Track에 대한 절제 표 3에서 학습 세트에서 샘플링한 140개 학습 궤적에서 동작 주의 에이전트에 각 구성 요소를 순차적으로 추가하여 각 PET 모듈의 기여도를 분석합니다. 효율적이고 희소한 설정을 위해 데이터 세트 크기는 보이는 검증 세트의 크기와 일치하도록 선택됩니다. Plan과 Track은 별도로 작동할 수 없으므로 이 절제에 대해 단일 모듈로 처리합니다. Plan and Track을 추가하면 완료율이 비교적 60%나 크게 향상되는데, 이는 구체화된 작업을 단계별로 해결하면 복잡성이 줄어든다는 가설에 대한 증거를 제공합니다. 하위 작업 추적 없이 Eliminate를 추가하면 절대 성능이 3% 향상되는 비교적 미미한 것으로 관찰됩니다. 반면 Plan and Track을 사용하여 하위 작업에 Eliminate를 적용하면 Plan and Track만 사용할 때보다 60% 이상의 상대적 향상이 관찰됩니다. 따라서 목표가 하위 작업에 더 집중되어 있을 때 무관한 객체를 제거하기가 더 쉽기 때문에 Plan and Track이 평가 중에 Eliminate의 성능을 높이는 것으로 추론합니다. 4.4. PET 모듈의 자동 분석 Plan 모듈 GPT2-XL(Radford et al., 2019), GPT-Neo2.7B(Black et al., 2021), 530B 매개변수 MT-NLG(Smith et al., 2022) 모델과 같은 다양한 LLM을 실험합니다. 표 2는 생성 정확도와 기준 진실 하위 작업에 대한 ROBERTa(Liu et al., 2019) 임베딩 코사인 유사도를 보고합니다. 모든 LLM이 문장 구조에 변화가 없는 템플릿 목표 사양에서 높은 정확도를 달성하는 것을 관찰했습니다. 인간 목표 사양의 경우 MT-NLG는 임베딩 유사도 측면에서 기준 진실과 유사한 하위 작업을 생성하는 반면 다른 작은 모델은 성능이 상당히 떨어집니다. 모듈 제거 AlfWorld의 세 분할에서 Macaw의 제로샷 리셉터클/객체 마스킹 성능을 평가합니다. 그림 6에서 모델이 각 작업을 완료할 때 규칙 기반 전문가가 상호 작용한 객체 대 객체에 할당하는 관련성 점수의 AUC 곡선을 보여줍니다. Macaw QA 모델은 제로샷 방식으로 쿼리되므로 보이지 않는 분할에서도 환경의 세 분할 모두에서 일관된 마스킹 성능을 보여줍니다. 또한, 섹션 4.5에 설명된 반직관적인 스폰 위치 때문에 객체 수용기 정확도가 일반적으로 객체 정확도보다 낮다는 점에 유의합니다.실험에서 결정 임계값 0.4는 재현율이 0.91이고 관찰에서 객체 수를 평균 40% 줄였습니다.추적 모듈 하위 작업 정렬 정보가 환경에서 제공되지 않으므로 완료 이벤트 감지를 위한 대체 성능 지표를 탐색합니다.이상적으로 하위 작업 추적기는 전문가가 환경을 &quot;완전히 해결&quot;한 경우에만 마지막 하위 작업을 &quot;완료&quot;로 기록해야 합니다.동의 척도로 Macaw-11B의 경우 정밀도 0.99와 재현율 0.78, Macaw-large의 경우 정밀도 0.96과 재현율 0.96을 보고합니다.더 큰 모델(Macaw-11b)은 더 정확하지만 감지를 놓치는 부분이 더 많아 이론적 성능이 78%로 제한됩니다. 더 작은 모델은 인간의 평가에 따르면 훨씬 덜 정확하지만 이론적으로 전체 모델 성능을 제한하지는 않습니다. 실험에서 두 모델 모두 비슷한 전체 결과를 생성한다는 것을 발견했는데, 이는 LLM이 정밀도와 재현율 모두에서 더 나은 성과를 거두면 전체 결과가 개선될 수 있음을 시사합니다. 4.5. 정성적 분석 계획 모듈 표 4에서 하위 작업 생성에 대한 두 가지 유형의 실패 사례를 보여줍니다. 첫 번째 유형의 오류는 기준 진실의 동의어를 생성하여 발생하고 두 번째 유형의 오류는 inaccuPlan, Eliminate 및 Track Template Goals Human Goals LLM GPT-2(Radford et al., 2019) GPT-Neo-2.7B(Black et al., 2021) MT-NLG(Smith et al., 2022) seen unseen seen unseen 94.29(0.97) 87.31(0.94) 10.07(0.62) 7.98(0.58) 99.29(1.00) 98.57(0.99) 96.27(0.98) 100(1.00) 4.70(0.82) 9.16(0.80) 40.04(0.94) 49.3(0.94) 표 2. Plan 모듈에 대한 다양한 LLM의 평가는 정확도와 ROBERTa 임베딩 코사인 유사도(괄호 안)를 기준으로 평가 분할(보이는 것과 보이지 않는 것)당 지상 진실 하위 작업에 대해, 인간이 주석한 목표가 있는 경우와 없는 경우입니다. 530B 매개변수를 사용하는 MT-NLG는 모든 데이터 세트 분할에서 전반적으로 가장 좋은 성능을 달성하며 인간이 목표를 지정한 어려운 작업에서 더 작은 모델의 성능을 크게 능가합니다. 또한 MT-NLG는 모든 작업에 대해 거의 완벽한 임베딩 유사도를 가진 하위 작업을 생성합니다. True Positive Rate True Positive Rate0.0.0.40.기차 수용구 식별을 위한 ROC 곡선0.0.60.0.기차 객체 식별을 위한 ROC 곡선 True Positive Rate 0.0.0.유효한_보이는 수용구 식별을 위한 ROC 곡선 유효한_보이지 않는 수용구 식별을 위한 ROC 곡선 True Positive Rate 0.0.0.0.AUC 0.AUC=0.AUC=0.0.0.0.0.0.0.0.0.거짓 긍정률 0.0.0.0.0.0.0.0.0.0.0.거짓 긍정률 거짓 긍정률 True Positive Rate0.0.0.0.유효한_보이는 객체 식별을 위한 ROC 곡선 True Positive Rate0.0.0.0.2유효한_보이지 않는 객체 식별을 위한 ROC 곡선 AUC=0.AUC=0.AUC=0.0.0.0.0.0.0.0.0.거짓 긍정률 0.0.0.0.0.0.0.0.0.0.0.거짓 양성률 거짓 양성률 그림 6. Macaw-11b 모델을 사용하여 Alfworld-Thor 환경에서 모든 작업에 대한 제로샷 관련성 식별의 AUC 점수 플롯. 기준 진실은 규칙 기반 전문가가 액세스한 리셉터클/객체로 얻습니다. 위: 리셉터클 관련성 식별. 아래: 객체 관련성 식별. QA 모델은 리셉터클의 경우 평균 AUC-ROC 점수 65점, 객체의 경우 76점을 얻습니다. 모델 삭제 보이지 않는 작업 주의 행동 주의 + 제거 행동 주의 + 계획 및 추적 행동 주의 + PET 52.27. 표 3. 평가 분할(보이고 보이지 않음)당 완료율 측면에서 훈련 세트에서 샘플링된 140개 데모 세트에서 훈련된 PET의 다른 삭제 비교. 영어: 단독 Eliminate 모듈을 적용하는 것은 Plan &amp; Track에 비해 전반적인 성능에 미치는 영향이 미미합니다.그러나 Plan &amp; Track과 함께 하위 작업에 Eliminate 모듈을 적용하면 성능이 훨씬 더 크게 향상됩니다.인간의 목표 사양에서 racies.Action Attention 프레임워크는 하위 작업에 동의어 변형에 강인한 것으로 알려진 ROBERTA(Liu et al., 2019) 임베딩을 사용합니다.Elimnate 모듈 우리는 제거 오류의 주요 원인은 모듈이 관심 있는 객체를 포함하는 용기를 잘못 마스크하여 에이전트가 해당 용기를 찾지 못할 때 발생한다는 것을 관찰했습니다.이는 종종 AI2Thor 시뮬레이터의 일부 객체가 상식에 따라 생성되지 않기 때문입니다.environment²의 설명서에서 언급했듯이 Apple이나 Egg와 같은 객체는 GarbageCan이나 TVStand와 같은 예상치 못한 용기에 생성될 가능성이 있습니다.그러나 AI2Thor에서 이러한 세대는 실제 배포에서는 발생할 가능성이 낮습니다. 따라서 Eliminate 모듈의 &quot;실수&quot;는 타당합니다. Track 모듈 실험적으로 하위 작업 계획/추적이 계산 절차가 필요한 작업에 특히 유용하다는 것을 발견했습니다. 표 ??에서 볼 수 있듯이 PET는 &quot;캐비닛에 비누 두 개 놓기&quot; 작업을 두 개의 반복되는 하위 작업 세트로 나눕니다. &quot;비누 가져가기→캐비닛에 비누 놓기&quot;. 따라서 하위 작업 계획 및 추적은 계산의 어려운 문제를 단순화합니다. 2 ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ 작업 GT Gen 작업 GT Gen 계획, 제거 및 추적 인간 목표 사양 예제 컵을 식혀서 캐비닛에 넣습니다. 머그잔을 식힌다→머그잔을 커피 머신에 넣는다 머그잔을 식힌다 머그잔을 커피 머신에 다시 넣는다 책상에서 연필을 가져와 책상 반대편에 놓는다 연필을 가져온다→연필을 선반에 넣는다→책상의 다른 곳에 놓는다 표 4. 인간 목표 사양(작업), 기준 진실(GT) 대 생성된(Gen) 계획 모듈의 실패 사례. 첫 번째 예에서 생성된 계획은 기준 진실과 다르지만 의미는 일치한다. 두 번째 예에서 생성된 계획은 인간 목표 사양에서 &quot;선반&quot; 대신 &quot;책상의 다른 쪽&quot;이라는 실수로 인해 기준 진실과 크게 다르다. 5.
--- CONCLUSION ---
, 한계 및 향후 작업 이 작업에서 우리는 세 단계로 구체화된 에이전트를 지원하기 위해 사전 훈련된 LLM을 사용하는 Plan, Eliminate, and Track(PET) 프레임워크를 제안합니다. 우리의 PET 프레임워크는 미세 조정이 필요 없으며 모든 목표 조건 구체화된 에이전트와 호환되도록 설계되었습니다. 우리의 실험에서 우리는 PET를 AlfWorld의 동적 동작을 처리하는 새로운 Action Attention 에이전트와 결합합니다. 우리의 Action Attention 에이전트는 BUTLER 기준선보다 훨씬 더 우수한 성능을 보입니다. 또한 PET 프레임워크는 훈련 세트 작업에 맞게 훈련되지 않았으므로 보이지 않는 인간의 목표 지정 작업에 대한 더 나은 일반화를 보여줍니다. 마지막으로, 우리의 절제 연구는 Plan 및 Track 모듈이 함께 Eliminate 모듈의 성능을 개선하여 최상의 성능을 달성한다는 것을 보여줍니다. 우리의 결과는 LLM이 구체화된 에이전트에 대한 상식과 절차적 지식의 좋은 원천이 될 수 있으며, 여러 LLM을 서로 조정하여 효과를 더욱 개선할 수 있음을 보여줍니다. 현재 시스템 설계의 주요 한계 중 하나는 추적 모듈(진행 추적기)이 완료된 하위 작업을 다시 방문하지 않는다는 것입니다. 예를 들어, 에이전트가 하위 작업(냄비 집어 올리기, 냄비를 조리대에 놓기)을 실행 중이고 냄비를 집어 올렸지만 냉장고에 넣었다면(픽업 작업 취소) 진행 추적기는 이전에 취소된 진행 상황을 고려하지 않으므로 이 상황에서 시스템이 중단될 수 있습니다. 향후 작업에서는 이러한 한계를 해결하기 위해 하위 작업 수준의 동적 재계획을 추가하거나 LLM이 정책 학습을 지원할 수 있는 다른 방법(예: 환경에 대한 지침서 읽기)을 탐색하는 데 집중할 수 있습니다. 참고문헌 Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Zhang, E., Ruano, RJ, Jeffrey, K., Jesmonth, S., Joshi, NJ, Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., 토셰프, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic assistances, 2022. URL https: //arxiv.org/abs/2204.01691. Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomous-acquired skills via goal generation. arXiv 사전 인쇄본 arXiv:2006.07185, 2020. Andreas, J., Klein, D., and Levine, S. 정책 스케치를 사용한 모듈식 멀티태스크 강화 학습. International Conference on Machine Learning, pp. 166–175. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021년 3월. URL https://doi.org/10.5281/zenodo. 5297715. 이 소프트웨어를 사용하는 경우 이 메타데이터를 사용하여 인용해 주세요. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for highlevel natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612. Bommasani, R., Hudson, DA, Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, MS, Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, JQ, Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., 헨더슨, P., Hewitt, J., Ho, DE, Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, PW, Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, XL, Li, X., Ma, Plan, Eliminate 및 Track T., Malik, A., Manning, CD, Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., 니블스, JC, Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, JS, Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, AW, Tramèr, F., Wang, RE, Wang, W., Wu, B., Wu, J., Wu, Y., Xie, SM, Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., 장 Y., 정, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2021. URL https://arxiv.org/abs/2108.07258. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, JD, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. 언어 모델은 few-shot learner입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. Chaplot, DS, Gandhi, D., Gupta, A., and Salakhutdinov, R. 목표 지향적 의미 탐색을 사용한 객체 목표 탐색, 2020. URL https://arxiv. org/abs/2007.00643. Cideron, G., Seurin, M., Strub, F., Pietquin, O. Higher: 경험 재생을 위한 사후 분석 생성을 통한 교육 수행 개선. 2020 IEEE 계산 지능 심포지엄 시리즈(SSCI), pp. 225–232. IEEE, 2020. Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, LE, Adada, M., et al. Textworld: 텍스트 기반 게임을 위한 학습 환경. Workshop on Computer Games, pp. 41–75. Springer, 2018a. Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, LE, Adada, M., et al. Textworld: 텍스트 기반 게임을 위한 학습 환경. Workshop on Computer Games에서, pp. 41–75. Springer, 2018b. Fulda, N., Ricks, D., Murdoch, B., Wingate, D. 바위로 무엇을 할 수 있을까? 단어 임베딩을 통한 어포던스 추출. arXiv 사전 인쇄본 arXiv:1703.03429, 2017. Goyal, P., Niekum, S., Mooney, R. Pixl2r: 픽셀을 보상에 매핑하여 자연어를 사용한 강화 학습 안내. Conference on Robot Learning에서, pp. 485–497. PMLR, 2021. He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., Ostendorf, M. 자연어 액션 공간을 사용한 심층 강화 학습. arXiv:1511.04636, 2015. arXiv 사전 인쇄본 Huang, W., Abbeel, P., Pathak, D., Mordatch, I. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트를 위한 실행 가능한 지식 추출, 2022a. URL https://arxiv.org/abs/2201.07207. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. 내면의 독백: 언어 모델을 통한 계획을 통한 체화된 추론, 2022b. URL https://arxiv.org/abs/2207.05608. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: 로봇 모방 학습을 통한 Zeroshot 작업 일반화. Conference on Robot Learning, pp. 991–1002. PMLR, 2022. Jiang, Y., Gu, SS, Murphy, KP, and Finn, C. 계층적 심층 강화 학습을 위한 추상화로서의 언어. 신경 정보 처리 시스템의 발전, 32, 2019. Kollar, T., Tellex, S., Roy, D., and Roy, N. 자연어 방향을 이해하기 위해. 제5회 ACM/IEEE 국제 휴먼로봇 상호작용(HRI) 컨퍼런스, pp. 259–266. IEEE, 2010. Lin, BY, Huang, C., Liu, Q., Gu, W., Sommerer, S., and Ren, X. 언어 모델을 사용한 구체화된 작업에 대한 근거 계획. arXiv 사전 인쇄본 arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: 강력하게 최적화된 bert 사전 학습 접근법. arXiv 사전 인쇄본 arXiv:1907.11692, 2019. MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk the talk: Connecting language, knowledge, and action in route instruction. Def, 2(6):4, 2006. Mei, H., Bansal, M., and Walter, MR Listen, attention, and walk: Neural mapping of navigational instruction to action sequences. 제30회 AAAI 인공지능 컨퍼런스, 2016. Micheli, V. 및 Fleuret, F. 언어 모델은 fewshot butlers입니다. arXiv 사전 인쇄본 arXiv:2104.07972, 2021. Min, SY, Chaplot, DS, Ravikumar, P., Bisk, Y. 및 Salakhutdinov, R. 필름: 모듈식 방법을 사용하여 언어의 지시를 따르기, 2021. 계획, 제거 및 추적 Misra, D., Langford, J. 및 Artzi, Y. 강화 학습을 사용하여 지시 및 시각적 관찰을 동작에 매핑합니다. arXiv 사전 인쇄본 arXiv:1704.08795, 2017. Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. 오프라인 데이터와 크라우드 소싱 주석에서 언어 조건화된 로봇 행동 학습. 로봇 학습 컨퍼런스, pp. 1303-1315. PMLR, 2022. Oh, J., Singh, S., Lee, H., Kohli, P. 멀티태스크 딥 강화 학습을 통한 제로샷 태스크 일반화. 기계 학습 국제 컨퍼런스, pp. 2661-2670. PMLR, 2017. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. 언어 모델은 비지도 멀티태스크 학습자입니다. 2019. A., Liu, PJ, Manning, CD 참조. 요점 파악: 포인터 생성기 네트워크를 통한 요약. arXiv 사전 인쇄본 arXiv:1704.04368, 2017. Sharma, P., Torralba, A., Andreas, J. 잠재 언어를 통한 기술 유도 및 계획. arXiv 사전 인쇄본 arXiv:2110.01517, 2021. Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. Alfred: 일상 업무에 대한 근거 있는 지시를 해석하기 위한 벤치마크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 10740-10749, 2020a. Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: 대화형 학습을 위한 텍스트와 체화된 환경 정렬. arXiv 사전 인쇄본 arXiv:2010.03768, 2020b. Shridhar, M., Manuelli, L., 및 Fox, D. Cliport: 로봇 조작을 위한 무엇과 어디 경로. 로봇 학습 컨퍼런스, 894-906쪽. PMLR, 2022. Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zheng, E., Child, R., Aminabadi, RY, Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., 및 Catanzaro, B. deepspeed와 megatron을 사용하여 megatron-turing NLG 530b를 훈련하기, 대규모 생성 언어 모델. CoRR, abs/2201.11990, 2022. URL https://arxiv.org/abs/2201.11990. Song, CH, Wu, J., Washington, C., Sadler, BM, Chao, W.-L., and Su, Y. Llm-planner: 대규모 언어 모델을 갖춘 체화된 에이전트를 위한 Few-shot grounded planning. arXiv 사전 인쇄본 arXiv:2212.04088, 2022. Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., and Ben Amor, H. 로봇 조작 작업을 위한 언어 조건 모방 학습. 신경 정보 처리 시스템의 발전, 33:13139-13150, 2020. P. Tafjord, O. and Clark, 마코로 질문 답변. arXiv:2109.02593, 2021. 범용 arXiv 사전 인쇄본 Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. 로봇 탐색 및 모바일 조작을 위한 자연어 명령 이해. AAAI 인공 지능 컨퍼런스 회의록, 25권, 1507-1514쪽, 2011. Yao, S., Rao, R., Hausknecht, M., and Narasimhan, K. Keep calm and explore: Language models for action generation in text-based games, 2020. URL https://arxiv.org/abs/2010.02903. Zahavy, T., Haroush, M., Merlis, N., Mankowitz, DJ, Mannor, S. 배우지 말아야 할 것을 배우세요: 심층 강화 학습을 통한 행동 제거. 신경 정보 처리 시스템의 발전, 31, 2018.
