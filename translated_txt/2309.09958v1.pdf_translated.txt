--- ABSTRACT ---
시각적 지시 튜닝은 최근 LLAVA 및 MiniGPT-4와 같은 오픈소스 대형 멀티모달 모델(LMM)에서 격려적인 진전을 보였습니다. 그러나 오픈소스 LMM에 대한 대부분의 기존 연구는 13B 이하의 매개변수를 가진 모델을 사용하여 수행됩니다. 이 논문에서는 LLAVA를 최대 33B 및 65B/70B까지 확장하는 실증적 연구를 제시하고 이미지 해상도, 데이터 혼합 및 LORA/QLORA와 같은 매개변수 효율적 학습 방법에 대한 탐색에서 얻은 결과를 공유합니다. 이러한 결과는 실제 작업을 완료할 때 멀티모달 및 언어 기능에 미치는 영향에 따라 평가됩니다. LMM을 확장하면 모델 성능이 지속적으로 향상되고 언어 기능이 개선되며 LMM의 LORA/QLORA 튜닝 성능이 전체 모델 미세 조정 성능과 비슷하다는 것을 발견했습니다. 또한 이 연구는 LMM 성능을 개선하기 위해 더 높은 이미지 해상도와 멀티모달 언어 데이터를 혼합하는 것의 중요성을 강조하며 시각적 지시 튜닝은 때때로 LMM의 순수 언어 기능을 개선할 수 있습니다. 이 연구가 더 큰 규모의 최첨단 LMM 연구를 더 쉽게 접근할 수 있게 하여 미래 연구를 위한 더 강력한 기준을 확립하는 데 도움이 되기를 바랍니다. 코드와 체크포인트는 공개됩니다.
--- METHOD ---
LORA/QLORA와 같은 s. 이들은 실제 작업을 야생에서 완료할 때 다중 모달 및 언어 기능에 미치는 영향에 따라 평가됩니다. LMM을 확장하면 모델 성능이 지속적으로 향상되고 언어 기능이 개선되며 LMM의 LORA/QLORA 튜닝 성능이 전체 모델 미세 조정 성능과 비슷하다는 것을 발견했습니다. 또한 이 연구는 LMM 성능을 개선하기 위해 더 높은 이미지 해상도와 다중 모달 언어 데이터를 혼합하는 것의 중요성을 강조하며, 시각적 지시 튜닝은 때때로 LMM의 순수 언어 기능을 개선할 수 있습니다. 이 연구가 대규모로 최첨단 LMM 연구를 더 쉽게 접근할 수 있게 하여 미래 연구를 위한 더 강력한 기준선을 확립하는 데 도움이 되기를 바랍니다. 코드와 검사점은 공개됩니다. 서론 대규모 다중 모달 모델(LMM)에 대한 최근 연구[9, 10]는 시각적 지시 튜닝 방법[12]에 초점을 맞추었습니다. 결과는 유망합니다. 예를 들어, 오픈소스 프로젝트인 Large Language and Vision Assistant(LLaVA)는 8개의 A-100 GPU에서 3시간 동안 멀티모달 명령어 추종 데이터로 7B의 대형 언어 모델(LLM)을 학습하면 실제로 강력한 시각적 이해 및 추론 기능을 갖춘 LMM이 생성됨을 보여줍니다. 독점적인 OpenAI 멀티모달 GPT-4 모델의 가장 매력적인 예를 재현한 것입니다[14]. 유사한 아이디어가 동시 진행 중인 작업인 MiniGPT-4[20]에서 탐구되고 있습니다. 이는 빠르게 중요한 연구 주제가 되어 수많은 새로운 모델, 벤치마크 및 애플리케이션의 개발을 촉진했습니다[10]. 그러나 높은 컴퓨팅 비용으로 인해 대부분의 기존 연구에서 7B 및 13B LLM을 활용하게 되었습니다. 따라서 모델 크기를 예를 들어 33B 및 65B로 크게 확장하는 영향은 여전히 탐구되지 않았습니다. 이 연구는 LMM을 위한 더 큰 크기의 언어 모델을 경험적으로 조사하고 확장에 대한 통찰력을 공유하여 이러한 격차를 메우는 것을 목표로 합니다.
--- EXPERIMENT ---
s 및 향후 연구를 위해 대규모 LLAVA를 사용하여 더 강력한 기준선을 확립합니다. 특히, 우리는 더 큰 모델 크기, 모델 튜닝 및 데이터 혼합 방법이 모델 성능에 미치는 영향을 탐구하고 연구 결과와 권장 사항을 제시합니다. 스케일링 레시피는 LLaVA-Bench[12] 및 MM-VET[19]에서 새로운 최첨단(SOTA) 성능을 이끌어냅니다. 우리의 연구 결과와 더 큰 LLaVA 체크포인트가 시각 지시 튜닝에 대한 향후 연구에 대한 참고 자료가 되기를 바랍니다. &quot;이 저자들은 이 작업에 동등하게 기여했습니다. 사전 인쇄본. 진행 중인 작업 2 실험 설정 모델 체크포인트. 다중 모달 기능에 대한 LLM 확장의 영향을 연구하기 위해 기존 LMM에 사용된 7B 및 13B 모델 외에도 언어 모델 크기를 33B 및 65B로 늘립니다.[15]• LLAVA-33B 우리는 오픈 소스 Vicuna-33B 체크포인트¹ [16]를 사용하여 2단계 학습을 수행합니다. 학습 데이터는 ShareGPT.com에서 수집한 약 125,000개의 대화입니다.• LLAVA-65B 공개 65B Vicuna 체크포인트가 부족하기 때문에 우리는 독립적으로 처리한 ShareGPT 데이터를 활용하여 Vicuna-65B 모델에 대한 자체 학습을 수행합니다. 이 데이터에는 학습 중에 사용된 159M개의 토큰이 포함되어 있습니다. 비교를 위해 Vicuna 33B 학습에 사용된 것으로 보고된 토큰 수는 370M 2입니다. 명령어 조정 LLM이 제공되면 우리는 [12]에 따라 2단계 LLaVA 번개 훈련을 수행합니다.(i) 1단계: 특징 정렬을 위한 사전 훈련. 선형 투영 계층이 훈련되고, 이는 시각적 특징(사전 훈련된 이미지 인코더의 마지막 계층 이전의 특징)을 LLM의 단어 임베딩 공간에 매핑합니다. 더 구체적으로, 투영 차원은 33B 모델의 경우 1024→6656이고 65B 모델의 경우 1024→8192입니다. 이 단계에서는 558K 샘플이 있는 LAION-CC-SBU 데이터의 conceptbalanced 하위 집합을 사용합니다.(ii) 2단계: 시각적 지침 튜닝. 미세 조정 단계에 LLaVA-80K 다중 모드 지침 데이터 세트를 사용합니다. 다양한 훈련 일정을 탐색하여 모델이 다양한 지침을 따라 실제로 작업을 완료할 수 있도록 하며, 이는 아래에 자세히 설명되어 있습니다. 튜닝 방법. 우리는 대규모 모델의 효율적이고 효과적인 시각적 지침 튜닝을 위해 훈련 가능한 모듈과 훈련 데이터 혼합을 모두 탐색합니다. • 학습 가능한 모듈. 선형 투영 계층을 조정하는 것 외에도 LLM을 조정하기 위해 두 가지 방식을 고려합니다. (i) LLM의 전체 모델 미세 조정 및 (ii) 매개변수 효율적 학습 방법. 후자의 경우 LORA[7] 및 QLORA[4]를 사용하여 제한된 컴퓨팅 리소스로 대규모 모델을 조정할 수 있습니다. 이는 학습 비용과 모델 성능 간의 상충 관계에 대한 심층적인 이해를 얻는 것을 목표로 합니다. ⚫ 데이터 혼합. 일반적으로 2단계에서는 멀티모달 명령어 데이터만 사용합니다. 언어 전용 명령어 데이터 ShareGPT를 LLaVA-80K 멀티모달 명령어 데이터와 혼합하여 모델의 언어와 멀티모달 기능 간의 상충 관계에 대한 심층적인 이해를 얻는 것을 추가로 고려합니다. 하이퍼 매개변수. 두 단계의 학습 프로세스에서 DeepSpeed 라이브러리³를 활용하고 ZeRO3 옵티마이저를 사용하지만 QLORA 실행을 제외하고는 ZeRO2를 사용합니다. 우리는 최대 2048의 시퀀스 길이를 사용합니다. 1단계에서는 가중치 감소 없이 1×10의 학습 속도로 33B 및 65B 모델을 모두 학습하고, 총 학습 단계의 3%에 대해 선형 감소 및 선형 워밍업이 있는 학습 속도를 사용합니다. 2단계에서는 전체 미세 조정에서 2×10−5의 학습 속도를 사용하여 전체 미세 조정에서 모든 모델에 대해 1에포크를 학습하고, LORA/QLORA 실행에 대해 1×10−4의 학습 속도를 사용합니다. 우리는 일련의 하이퍼파라미터 검색 및 LORA 실행을 수행했으며, 최상의 성능을 얻으려면 더 큰 LoRA 알파 또는 동등하게 더 큰 학습 속도가 중요하다는 것을 발견했습니다. 구체적으로, 우리는 LoRA 랭크의 2배인 LoRA 알파와 모든 모델에 가장 적합한 1×10−4의 학습 속도를 사용합니다. 전체 미세 조정을 위해 4개의 A100 노드에서 총 배치 크기 512를 사용하는데, 각 노드에는 8개의 A100-80G GPU가 장착되어 있습니다. LORA/QLORA 실행의 경우 33B 모델의 경우 1개의 A100 노드에서 총 배치 크기 512를 사용하고 65B 모델의 경우 2개의 노드에서 총 배치 크기를 사용합니다. 3 결과 LMM을 위해 특별히 설계된 두 가지 최근 벤치마크에서 먼저 대규모 체크포인트를 비교한 다음, LLaVA 모델을 확장하는 과정에서 발견한 사항을 보고합니다. https://huggingface.co/lmsys/vicuna-33b-v1.2 https://github.com/1m-sys/FastChat/blob/main/docs/vicuna_weights_version.md 3 https://github.com/microsoft/DeepSpeedModels 추론 대화 세부 정보 | 전체 Bard-78.83.69.77.Bing-Chat-90.59.52.71.LLAVA-13B(빔=1) 81.64.55.70.LLAVA-13B(빔=5) 84.68.59.73.LLAVA-33B(빔=1) 82.70.62.73.LLAVA-33B(빔=5) 83.72.61.74.LLAVA-65B(빔=1) 87.63.62.74.LLAVA-65B(빔=5) 88.59.65.74.표 1: LLaVA-Bench에서의 성능 비교. 1과 5에서의 빔 검색 크기가 보고됩니다. 모델 | Rec OCR 지식 생성 공간 수학 | MM-VET 논문에 보고된 다양한 오픈소스 LMM의 총 결과 [19] LLAMA-Adapter v2-7B [5] 16.8 7.2.3.16.4.13.6±0.OpenFlamingo-9B [1, 2] 24.6 14.13.12.18.15.21.8±0.MiniGPT-4-8B [20] 27.4 15.12.13.20.7.22.1±0.BLIP-2-12B [11] 27.5 11.11.7.16.5.22.4±0.LLAVA-7B [12] 28.17.16.18.21.11.23.8±0.MiniGPT-4-14B [20] 29.9 16.20.22.22.3.24.4±0.Otter-9B [8] 28.16.19.20.19.15.24.6±0.InstructBLIP-14B [3] 30.8 16.9.9.21.10.25.6±0.InstructBLIP-8B [3] 32.14.16.18.18.7.26.2±0.LLAVA-13B [12] 30.9 20.23.26.24.7.26.4±0.MM-ReAct-GPT-3.5 [18] 24.2 31.21.20.32.26.27.9±0.LLAVA-7B (LLAMA-2) [12] 32.9 20.19.20.25.5.28.1±0.LLAVA-13B (V1.3, 336px) [12] 38.1 22.25.25.31.3 11.32.5±0.LLAVA-13B (LLAMA-2) [12] 39.22.26.29.29.7.32.9±0.MM-ReAct-GPT-4 [18] 33.65.29.35.56.69.44.6±0.자체 실험 실행 결과 LLAVA-13B (LLAMA-2) 38.21.26.28.28.7.32.6±0.LLAVA-33B 38.25.26.28.29.7.32.9±0.LLAVA-33B (데이터 혼합) 37.7 27.26.28.28.11.34.1±0.LLAVA-65B 39.2 28.26.28.33.15.35.5±0.LLAVA-65B(데이터 혼합) 41.8 27.30.32.30.7.36.4±0.표 2: MM-VET에서 다양한 오픈소스 LMM의 성능.MM-ReAct는 단일 멀티모달 모델이 아니라 GPT-3.5 또는 GPT-4를 통해 시각적 도구를 체이닝하여 구축한 시스템이며, 이를 참고로 추가합니다.LLaVA-13B(LLaMA-2)에서 실행한 실험은 MM-VET 논문에 보고된 동일한 체크포인트로 매우 유사한 점수를 얻었으며, 이는 평가 파이프라인이 일관됨을 나타냅니다.3.1 벤치마크 비교 LLAVA-Bench. LLaVA-Bench(In-the-Wild)4[12]는 실내 및 실외 장면, 밈, 그림, 스케치를 포함하여 총 60개의 질문이 있는 이미지로 구성된 다양한 평가 데이터 세트입니다. 각 이미지는 수동으로 큐레이팅된 자세한 설명과 개방형 시각적 채팅 시나리오와 관련된 적절하게 선택된 질문 세트와 쌍을 이룹니다. 각 질문은 세 가지 유형의 작업 중 하나에 속합니다. 간단한 시각적 인식 및 QA 질문이 포함된 대화, 긴 문단으로 이미지를 특징짓는 자세한 설명, 이미지에서 의미를 추론하는 데 중점을 둔 복잡한 추론 작업입니다. 언어 GPT-4(gpt4-0314)는 생성된 답변에 점수를 매기는 데 사용됩니다. 모델 출력과 골드 응답 간의 상대 점수가 보고됩니다. LLAVA-Bench[12]에서 Microsoft BingChat 및 Google Bard를 포함한 상용 시각적 채팅 시스템과 LLaVA를 비교합니다. 4 https://github.com/haotian-liu/LLAVA/blob/main/docs/LLaVA_Bench.md Shttps://www.bing.com/chat &quot;https://bard.google.com/결과는 표 1에 나와 있습니다. 33B 및 65B 체크포인트는 13B LLaVA 모델과 Bing Chat보다 성능이 뛰어납니다. LLaVA-Bench가 작다는 사실(따라서 비교가 통계적으로 유의하지 않을 수 있음)에도 불구하고 결과는 고무적입니다. 대형 LMM에 비해 작은 오픈소스 LMM은 실제 애플리케이션에 배포하는 데 훨씬 비용 효율적입니다. 추론 지연 시간이 무시할 만큼 증가하므로 빔 검색 크기를 1에서 5로 늘리면 모든 모델 크기에 대한 성능을 크게 개선할 수 있습니다. 결과에 따르면 일반적으로 대형 LLaVA 모델은 복잡한 추론과 자세한 설명을 생성하는 작업에서 더 나은 성능을 보이는데, 여기에는 대형 LLM의 강력한 언어 역량이 필요합니다. 또한 대형 LLaVA 모델은 BingChat과 비슷한 결과를 얻습니다. 영어: 강력한 이미지 이해 능력이 필요한 다중 턴, 다중 모달 대화 작업.MM-VET.MM-VET[19]는 복잡한 작업을 해결하는 흥미로운 능력은 다양한 시각-언어(VL) 기능을 통합할 수 있는 일반주의 LMM에 의해 종종 달성된다는 가정을 기반으로 설계되었습니다.MM-Vet에는 200개의 이미지와 218개의 질문(샘플)이 포함되어 있으며, 핵심 VL 기능(인식, OCR, 지식, 언어 생성, 공간 인식 및 수학)과 이들의 조합을 평가하는 것을 목표로 합니다.평가를 위해 LLM 기반 평가자(gpt4-0613)를 사용하여 다양한 형태의 개방형 출력을 채점합니다.표 2에서 MMVET에 대한 결과를 보고합니다.성능은 13B에서 33B 및 65B로 지속적으로 향상되었습니다.가장 큰 LLaVA 모델은 엔드투엔드 오픈소스 LMM 중에서 SoTA 성능을 향상시킵니다. 가장 큰 개선은 지식과 생성 능력을 평가할 때 관찰되었으며, 그 다음으로 인식과 OCR이 뒤를 이었습니다. 공간 및 수학에 대한 성능은 비슷한 수준을 유지했습니다. 결과에 따르면 향상된 LLM 기능은 가중치에 더 많은 지식을 저장하고 더 강력한 언어 대응 능력으로 이어지는 데 도움이 되었습니다. 3.2 LLAVA 확장 실험은 세 가지 연구 질문에 답하기 위해 수행되었습니다. ⑪어떤 확장 요소가 중요합니까? 우리는 LLaVA의 성능 개선에 대한 세 가지 확장 요소의 상대적 기여도를 연구합니다. 결과는 표 3(a)에 요약되어 있습니다. • • 모델 크기. 모델 크기를 지속적으로 늘리면 전체 성능이 향상됩니다. 더 큰 모델을 학습하려면 더 큰 데이터 크기가 필수적이라고 추측합니다. 예를 들어, LLAVA-80K 데이터로만 학습하는 경우 모델 크기가 커질수록 이득이 줄어듭니다. 이미지 해상도. CLIP ViT 이미지 인코더를 수정하여 224×224 및 336×336 이미지 해상도를 사용하도록 사전 학습된 변형을 비교한 결과, 더 높은 해상도가 모든 4가지 LLM 크기에서 일관되게 2~3포인트 향상을 가져온다는 것을 발견했습니다.• 데이터 혼합. 더 큰 모델은 지침 데이터를 맞추는 능력이 더 높은 경향이 있습니다.언어 전용 지침 데이터(ShareGPT)를 LLaVA-80K와 혼합하면 다중 모드 지침 데이터만으로 학습한 경우에 비해 모델 성능을 2포인트 향상시킬 수 있습니다.표 3(b)에서는 6개 범주의 모델 추론 기술을 평가하는 2,974개 질문 세트가 포함된 MM-Bench[13]에 대한 결과를 제시합니다.세 가지 요인을 결합하면 [13]에 보고된 기준 LLaVA 7B 모델이 향상됩니다. ②매개변수 효율적 학습 방법은 언제 고려해야 합니까?모델 크기가 커짐에 따라 전체 모델 미세 조정보다 효율적인 조정 방법을 사용하는 것을 고려해야 합니다. LORA와 QLORA는 잘 알려진 매개변수 효율적 튜닝 방법입니다.표 4에서 볼 수 있듯이, 단위가 Azure 7에서 시간당 $13.63(ND A100 v4 시리즈)의 가격과 동일할 수 있으므로 노드당 GPU 시간을 사용하여 컴퓨팅 비용을 보고합니다.총 비용은 #시간과 #에포크를 곱하여 추정할 수 있습니다.표 4(a)에서 LLAVA-80K 명령어 튜닝 데이터 세트에서 1에포크 동안 LoRA 순위 8과 64로 33B와 65B 모델을 모두 학습합니다.33B 이상의 매개변수가 있는 모델의 경우 LORA 순위 값을 늘리면 전체 모델 튜닝이 특정 모델 크기에 대해 최대 성능에 도달할 때까지 성능과 비용이 모두 증가합니다.13B 모델의 경우 순위 64가 전체 모델 튜닝과 비슷한 성능을 제공할 수 있음을 알 수 있습니다.비용은 학습 가능한 매개변수 수보다 총 매개변수 수와 더 관련이 있습니다. 비용 증가 https://azure.microsoft.com/en-us/pricing/details/machine-learning/이미지 크기 데이터 혼합 7B 13B 33B 65B 224×336x336x63.6 67.1 69.3 70.65.9 70.1 72.0 72.73.9 74.(a) LLAVA-Bench의 성능 점수. 체크포인트 이미지 크기 데이터 혼합 LLAVA-7B 224×LLAVA-33B 336xLLAVA-65B 336x전체 LR AR RR FP-S FP-C CP 36.2 15.9 53.6 28.6 41.8 20.0 40.5 5.7 23.3 74.0 46.0 51.5 50.4 67.5 6.0 24.4 72.3 49.3 50.5 51.2 68.(b) MM-Bench의 성능 점수. 평가할 기술에는 논리적 추론(LR), 속성 추론(AR), 관계 추론(RR), 세분화된 단일 인스턴스 인식(FP-S), 세분화된 교차 인스턴스 인식(FP-C) 및 거친 인식(CP)이 포함됩니다. 표 3: 모델 크기, 이미지 해상도 및 데이터 혼합을 확장하기 위한 성능.LORA 순위 7B 전체13B 전체성능 ↑ 시간(노드당 GPU 시간)↓ 학습 가능한 매개변수 수(B) 65.9 70.1 70.1 70.1.3 2.1 2.3 4.7 0.26 13 0.33B 64-QLORA 64 전체 64 전체 71.71.8 72.0 72.4.68 4.79 5.80 9.0.49 0.33 0.65B 72.13.표 4: LLaVA-80K 데이터에 대한 다양한 모델 크기와 학습 방법 간 성능과 컴퓨팅 비용 간의 균형.&quot;전체&quot;는 전체 모델 미세 조정을 나타냅니다. &quot;시간&quot;은 1 에포크 학습을 완료하는 데 걸린 총 GPU 시간(실행 시간 × GPU 수)을 8(노드당 GPU 수)로 나눈 값으로 보고됩니다. 모든 모델은 LLaVA-80K 데이터에서 학습되었으며, 결과는 LLAVA-Bench에서 동일한 설정으로 반복된 평가 실행을 평균하여 얻습니다. 주어진 모델 크기에 대한 LORA 순위를 높이는 것은 모델 크기를 확대하여 발생하는 비용 증가보다 상당히 작습니다. 예를 들어, LORA 순위를 8에서 64로 높이는 것은 같은 순위의 65B 모델을 미세 조정하는 LORA 성능과 거의 일치하지만, 65B 모델의 학습 비용의 50%만 필요합니다. 실제로 33B 모델을 조정하면 비용과 성능 간에 좋은 균형을 이룰 수 있습니다. 다양한 LORA 변형은 성능이 비슷하며, QLORA는 LoRA보다 GPU 메모리 비용과 실행 시간 비용이 낮습니다. 영어: DeepSpeed ZeRO2 모드로 대규모 모델(예: 65B)을 학습시키는 경우 QLORA를 사용하여 GPU에 적합할 수 있지만 LORA에서는 OOM 문제가 발생합니다.실험에서 LoRA의 하이퍼파라미터가 성능에 큰 영향을 미치는 것을 발견했습니다.(i) LORA의 큰 학습률과 알파 값은 결과를 상당히 개선합니다.예를 들어, 동일한 순위=64에서 학습률=2 × 10−5 및 알파=16을 줄이면 LLaVA-Bench에서 성능이 71.8에서 65.5로 감소합니다.(ii) 동일한 설정에서 큰 순위는 거의 개선되지 않습니다.예를 들어, 순위를 64에서 128 및 512로 높이면 각각 65.5에서 66.1 및 68.1로 향상됩니다.3 언어와 멀티모달 모두에서 강력한 기능을 갖춘 LMM?우리는 평가를 두 가지 측면에서 확장했습니다.(i) MM-VET를 추가하여 LMM의 통합 멀티모달 기능을 측정합니다. (ii) LMM의 순수 언어 능력은 Vicuna-80 [16]과 MMLU [6]를 사용하여 측정하는데, 전자는 실제 언어 과제에서의 지시 따르기 능력을 평가하고, 후자는 다국어 멀티태스크 언어 능력을 평가한다. 결과는 표 5에 나타나 있으며, 모든 모델은 전체 모델에서 미세 조정되었다. LLaVA의 LLM 가중치를 초기화하는 Vicuna와 비교했을 때, 다중 모달 지시 데이터로만 훈련된 후 LLAVA가 비슷한 언어 능력을 보인다는 것은 놀라운 일이다. 언어 지시 데이터를 혼합하면 LLaVA의 다중 모달 능력은 향상되지만 언어 능력은 향상되지 않는다. 이는 부분적으로 복잡한 추론 질문과 장문 답변을 LLaVA-Instruct-158K에 포함했기 때문이며, 이는 LLaVA의 언어 기능을 유지하는 데 도움이 됩니다.다중 모드 언어 모델 데이터 믹스 LLAVA-Bench MM-VET Vicuna-80 MMLU Vicuna-13B 79.55.LLAVA-13B 70.32.79.55.Vicuna-33B 85.59.LLAVA-33B 72.32.85.56.LLAVA-33B 73.34.80.58.Vicuna-65B 83.62.LLAVA-65B 72.35.84.62.LLAVA-65B 74.36.82.62.LLAMA-2-70B-Chat 84.63.LLAVA-70B 69.35.81.65.표 5: 멀티모달 및 언어 기능 모두에 대한 성능. 또한 LLaMA-2-70B-Chat 체크포인트[15]에 따라 LLAVA-70B를 훈련하고 멀티모달 및 언어 기능에서 엇갈린 결과를 발견했습니다. 흥미롭게도 MMLU에서 LLaMA-2-70B-Chat을 2포인트 향상시켜 전체 MMLU 점수가 65.1이 되었는데, [17] 및 Chatbot Arena Leaderboard 8에 따르면 이는 70B 모델 크기에 대한 최고의 성능입니다. 저희가 아는 한, 이는 시각적 지시 튜닝이 대규모 LMM의 언어 능력을 향상시킨다는 것을 보여준 최초의 보고된 결과입니다.
--- CONCLUSION ---
s 및 한계 LMM의 언어 모델 크기를 확장하는 것에 대한 실증적 연구를 제시합니다. 주요 결과는 다음과 같습니다. (i) LMM을 지속적으로 확장하면 모델 성능이 향상되어 주로 LLM 모델 크기가 증가하기 때문에 언어 기능이 크게 향상됩니다. 비전 인코더를 확장하여 시각적 기능을 향상시키고 비전 인식 및 이해 작업에서 모델 성능을 개선하는 방법은 향후 작업에 맡깁니다. (ii) LORA/QLORA와 같은 매개변수 효율적 방법은 제한된 GPU 메모리가 있는 일부 실제 환경에서 대규모 LLM을 미세 조정하여 성능-비용 균형을 맞추는 실행 가능한 솔루션입니다. LoRA/QLORA의 성능은 전체 모델을 미세 조정하는 것과 비슷하며 모델 학습과 제공 모두에서 비용을 크게 절감하여 효과를 입증합니다. (iii) 학습 데이터 큐레이션에 대한 연구에 따르면 모델 학습을 위해 이미지 해상도를 적절히 선택하고 다중 모달 언어 데이터를 혼합하면 결과 LMM의 성능이 크게 향상될 수 있습니다. 또한 시각적 지시 조정이 LMM의 언어 기능을 향상시킬 수 있음을 처음으로 보여줍니다. 이 연구에서 사용된 학습 데이터 세트는 작습니다.따라서 우리의 결과는 아직 예비적입니다.향후 작업에서는 훨씬 더 큰 데이터 세트를 사용하여 다양한 학습 데이터 선택 및 혼합 방법이 훨씬 더 큰 LMM의 품질을 개선할 수 있는지 여부와 어떻게 개선할 수 있는지 자세히 조사할 것입니다.참고문헌 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.Flamingo: a visual language model for few-shot learning.Advances in Neural Information Processing Systems, 35:23716-23736, 2022.[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: 대규모 자기회귀 시각 언어 모델을 훈련하기 위한 오픈소스 프레임워크. arXiv 사전 인쇄본 arXiv:2308.01390, 2023.8 https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi. Instructblip: 명령어 튜닝을 통한 범용 시각 언어 모델을 향해, 2023.[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. Qlora: 양자화된 LLM의 효율적인 미세 조정. arXiv 사전 인쇄본 arXiv:2305.14314,2023.[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: 매개변수 효율적 시각 지시 모델. arXiv 사전 인쇄본 arXiv:2304.15010, 2023.[6] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 대규모 멀티태스크 언어 이해 측정. arXiv 사전 인쇄본 arXiv:2009.03300, 2020.[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄 arXiv:2106.09685, 2021.[8] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang 및 Ziwei Liu. Otter: 상황에 맞는 명령어 조정이 가능한 다중 모드 모델입니다. arXiv 사전 인쇄 arXiv:2305.03726,2023.[9] 리춘위안. 대규모 다중 모드 모델: CVPR 2023 튜토리얼에 대한 참고 사항. arXiv 사전 인쇄 arXiv:2306.14895, 2023.[10] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang 및 Jianfeng Gao. 다중 모드 기반 모델: 전문가부터 범용 보조자까지. arXiv 사전 인쇄, 2023.[11] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 고정 이미지 인코더 및 대규모 언어 모델을 사용한 부트스트래핑 언어 이미지 사전 훈련. arXiv 사전 인쇄 arXiv:2301.12597, 2023.[12] Haotian Liu, Chunyuan Li, Qingyang Wu, 그리고 이용재. 시각적 교육 튜닝, 2023. 1,2,[13] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: 귀하의 다중 모드 모델은 다재다능한 플레이어입니까? arXiv 사전 인쇄 arXiv:2307.06281, 2023.[14] 오픈AI. Gpt-4 기술 보고서, 2023.[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2,[16] Vicuna. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇. https://vicuna.lmsys.org/, 2023. 2,[17] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 낙타는 얼마나 멀리 갈 수 있을까? 공개 리소스에 대한 명령 조정 상태를 탐색합니다. arXiv 사전 인쇄 arXiv:2306.04751, 2023.[18] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 및 Lijuan Wang. Mm-react: 다중 모달 추론 및 조치를 위해 chatgpt 프롬프트, 2023.[19] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang 및 Lijuan Wang. Mm-vet: 통합 기능을 위한 대규모 다중 모드 모델을 평가합니다. arXiv 사전 인쇄본 arXiv:2308.02490, 2023. 1, 3,[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv 사전 인쇄본 arXiv:2304.10592, 2023. 1, 이 그림 &quot;lora_loss.png&quot;는 다음에서 &quot;png&quot; 형식으로 제공됩니다: http://arxiv.org/ps/2309.09958v
