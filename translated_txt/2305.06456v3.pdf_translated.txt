--- ABSTRACT ---
우리는 노이즈가 많은 입력(예: 비디오에서 추정한 포즈 또는 언어에서 생성된 포즈)과 예상치 못한 낙하 상황에서도 고충실도 모션 모방과 내결함성 동작을 달성하는 물리 기반 휴머노이드 컨트롤러를 제시합니다. 컨트롤러는 외부 안정화 힘을 사용하지 않고도 최대 1만 개의 모션 클립을 학습하고 실패 상태에서 자연스럽게 복구하는 방법을 학습합니다. 참조 모션이 주어지면 컨트롤러는 재설정 없이도 시뮬레이션된 아바타를 영구적으로 제어할 수 있습니다. 핵심적으로, 우리는 점점 더 어려운 모션 시퀀스를 학습하기 위해 새로운 네트워크 용량을 동적으로 할당하는 점진적 곱셈 제어 정책(PMCP)을 제안합니다. PMCP는 대규모 모션 데이터베이스에서 학습하고 재앙적인 망각 없이 실패 상태 복구와 같은 새로운 작업을 추가하는 데 효율적인 확장을 허용합니다. 우리는 라이브 및 실시간 다중 사람 아바타 사용 사례에서 비디오 기반 포즈 추정기와 언어 기반 모션 생성기에서 노이즈가 많은 포즈를 모방하는 데 컨트롤러를 사용하여 컨트롤러의 효과를 보여줍니다. 1.
--- INTRODUCTION ---
물리 기반 동작 모방은 현실적인 인간 동작을 만들고, 그럴듯한 환경적 상호 작용을 가능하게 하며, 미래의 가상 아바타 기술을 발전시킬 수 있는 잠재력 덕분에 시각 및 그래픽 커뮤니티의 상상력을 사로잡았습니다. 그러나 시뮬레이션에서 높은 자유도(DOF) 휴머노이드를 제어하는 것은 넘어지거나, 넘어지거나, 참조 동작에서 벗어나고 회복하는 데 어려움을 겪을 수 있으므로 상당한 과제를 안겨줍니다. 예를 들어, 노이즈가 많은 비디오 관찰에서 추정된 포즈를 사용하여 시뮬레이션된 휴머노이드를 제어하면 종종 휴머노이드가 땅에 떨어질 수 있습니다[50, 51, 22, 24]. 이러한 제한으로 인해 현재 제어 정책은 비디오나 언어와 같은 노이즈가 많은 관찰을 처리할 수 없기 때문에 물리 기반 방법의 광범위한 채택이 방해를 받습니다. 아바타에 물리적으로 시뮬레이션된 휴머노이드를 적용하기 위해 첫 번째 주요 과제는 높은 성공률로 인간과 같은 동작을 충실하게 재현할 수 있는 동작 모방기(컨트롤러)를 학습하는 것입니다. 강화 학습(RL) 기반 모방 정책은 유망한 결과를 보였지만, AMASS(1만 개의 클립, 40시간 분량의 동작)와 같은 대규모 데이터 세트에서 단일 정책으로 동작을 성공적으로 모방한 것은 아직 이루어지지 않았습니다. 더 크거나 여러 전문가 정책을 혼합하여 사용하려는 시도는 어느 정도 성공을 거두었지만[45, 47], 아직 가장 큰 데이터 세트로 확장되지는 않았습니다. 따라서 연구자들은 인간형을 안정화하는 데 도움이 되는 외부 힘을 사용했습니다. 잔류 힘 제어(RFC)[52]는 AMASS 데이터 세트의 최대 97%[22]를 모방할 수 있는 동작 모방자를 만드는 데 도움이 되었으며, 비디오에서 인간의 자세 추정[54, 23, 12]과 언어 기반 동작 생성[53]에 성공적으로 적용되었습니다. 그러나 외부 힘은 인간형을 꼭두각시로 조종하는 &quot;신의 손&quot; 역할을 하여 물리적 사실성을 손상시키고 날기 및 떠다니기와 같은 아티팩트를 생성합니다. RFC를 사용하면 모델이 휴머노이드에 비물리적 힘을 자유롭게 적용할 수 있으므로 시뮬레이션의 사실성이 손상된다고 주장할 수 있습니다.시뮬레이션된 휴머노이드를 제어하는 또 다른 중요한 측면은 노이즈가 많은 입력 및 실패 사례를 처리하는 방법입니다.이 연구에서는 비디오 또는 언어 입력에서 추정된 인간 포즈를 고려합니다.특히 비디오 입력과 관련하여 부유[53], 발 미끄러짐[57] 및 물리적으로 불가능한 포즈와 같은 아티팩트는 폐색, 어려운 관점 및 조명, 빠른 동작 등으로 인해 인기 있는 포즈 추정 방법에서 만연합니다.이러한 사례를 처리하기 위해 대부분의 물리 기반 방법은 실패 조건이 트리거될 때 휴머노이드를 재설정합니다[24, 22, 51].그러나 성공적으로 재설정하려면 고품질 참조 포즈가 필요하지만 포즈 추정의 노이즈 특성으로 인해 얻기 어렵고 신뢰할 수 없는 포즈로 떨어지고 재설정하는 악순환으로 이어집니다. 따라서 예상치 못한 낙상과 노이즈가 많은 입력을 우아하게 처리하고, 실패 상태에서 자연스럽게 복구하고, 모방을 재개할 수 있는 컨트롤러를 갖는 것이 중요합니다. 이 작업에서 우리의 목표는 인간 사용자의 비디오 관찰을 사용하여 아바타를 제어하는 실시간 가상 아바타를 제어하도록 특별히 설계된 휴머노이드 컨트롤러를 만드는 것입니다. 우리는 모션 모방에서 높은 성공률을 달성하고 실패 상태에서 자연스럽게 복구할 수 있는 단일 정책인 영구 휴머노이드 컨트롤러(PHC)를 설계합니다. 우리는 치명적인 망각 없이 전체 AMASS 데이터 세트의 모션 시퀀스에서 학습하는 점진적 곱셈 제어 정책(PMCP)을 제안합니다. 점점 더 어려운 모션 시퀀스를 다른 &quot;작업&quot;으로 처리하고 학습할 새로운 네트워크 용량을 점진적으로 할당함으로써 PMCP는 더 어려운 모션 클립을 학습할 때 더 쉬운 모션 클립을 모방하는 기능을 유지합니다. 또한 PMCP를 사용하면 컨트롤러가 모션 모방 기능을 손상시키지 않고 실패 상태 복구 작업을 학습할 수 있습니다. 또한 파이프라인 전체에 Adversarial Motion Prior(AMP)[35]를 채택하고 장애 상태 복구 중에 자연스럽고 인간과 같은 행동을 보장합니다. 나아가 대부분의 동작 모방 방법은 링크 위치와 회전 추정치를 모두 입력으로 필요로 하는 반면, 링크 위치만 필요한 컨트롤러를 설계할 수 있음을 보여줍니다. 이 입력은 비전 기반 3D 키포인트 추정치 또는 VR 컨트롤러의 3D 포즈 추정치를 통해 더 쉽게 생성할 수 있습니다. 요약하자면, 우리의 기여는 다음과 같습니다. (1) 외부 힘을 적용하지 않고도 AMASS 데이터 세트의 98.9%를 성공적으로 모방할 수 있는 영구 인간형 컨트롤러를 제안합니다. (2) 대규모 동작 데이터 세트에서 망각 없이 학습하고 장애 상태 복구와 같은 추가 기능을 잠금 해제하기 위한 점진적 곱셈 제어 정책을 제안합니다. (3) 컨트롤러는 작업에 구애받지 않으며 드롭인 솔루션으로 기성품 비디오 기반 포즈 추정치와 호환됩니다. 모션 캡처(MoCap)와 비디오에서 추정된 동작을 모두 평가하여 컨트롤러의 기능을 보여줍니다. 또한 웹캠 비디오를 입력으로 사용하여 지속적으로 시뮬레이션된 아바타를 운전하는 라이브(30fps) 데모를 보여드립니다. 2.
--- RELATED WORK ---
s 물리 기반 동작 모방. 물리 법칙에 따라, 시뮬레이션된 캐릭터[32, 31, 33, 35, 34, 7, 45, 52, 28, 13, 2, 11, 46, 12]는 자연스러운 인간 동작, 인간과 인간 간 상호작용[20, 48], 인간과 사물 간 상호작용[28, 34]을 만드는 뚜렷한 장점이 있습니다. 대부분의 최신 물리 시뮬레이터는 미분 불가능하기 때문에 이러한 시뮬레이션된 에이전트를 훈련하려면 RL이 필요하며, 이는 시간이 많이 걸리고 비용이 많이 듭니다. 결과적으로 대부분의 작업은 사용자 입력[45, 2, 35, 34]을 기반으로 하는 대화형 제어, 스포츠[48, 20, 28] 또는 기타 모듈형 작업(목표 달성[49], 드리블[35], 이동[32] 등)과 같은 소규모 사용 사례에 집중합니다. 반면, 대규모 동작 데이터 세트를 모방하는 것은 어렵지만 기본적인 작업인데, 참조 동작을 모방할 수 있는 에이전트는 동작 생성기와 쉽게 페어링되어 다양한 작업을 달성할 수 있기 때문이다. 단일 클립[31]을 모방하는 법을 배우는 것부터 데이터 세트[47, 45, 7, 44]에 이르기까지 동작 모방기는 참조 동작을 모방하는 인상적인 능력을 보여주었지만, 종종 고품질 MoCap 데이터를 모방하는 데 국한되었다. 그 중에서도 ScaDiver[47]는 전문가 정책을 혼합하여 CMU MoCap 데이터 세트로 확장하고 실패 시간으로 측정한 약 80%의 성공률을 달성한다. Unicon[45]은 모방과 전송에서 정성적인 결과를 보여주지만, 모방기가 데이터 세트에서 클립을 모방하는 능력을 정량화하지는 않는다. MoCapAct[44]는 먼저 CMU MoCap 데이터 세트에서 단일 클립 전문가를 학습하고, 이를 전문가 성능의 약 80%를 달성하는 단일 클립으로 정제한다. 우리와 가장 가까운 노력은 UHC[22]로, AMASS 데이터 세트의 97%를 성공적으로 모방합니다. 그러나 UHC는 균형을 맞추는 데 도움이 되는 휴머노이드의 뿌리에 비물리적 힘을 적용하는 잔류 힘 제어[51]를 사용합니다. 휴머노이드가 넘어지는 것을 방지하는 데 효과적이지만 RFC는 물리적 사실성을 감소시키고 특히 동작 시퀀스가 어려울 때 떠다니거나 흔들리는 것과 같은 아티팩트를 생성합니다[22, 23]. UHC와 비교할 때, 우리의 컨트롤러는 외부 힘을 사용하지 않습니다. 시뮬레이션된 캐릭터에 대한 실패 상태 복구. 시뮬레이션된 캐릭터는 균형을 잃으면 쉽게 넘어질 수 있으므로 복구를 돕기 위해 많은 접근 방식[39, 51, 34, 42, 7]이 제안되었습니다. PhysCap[39]은 균형을 맞출 필요가 없는 부유 기반 휴머노이드를 사용합니다. 이는 휴머노이드가 더 이상 적절하게 시뮬레이션되지 않기 때문에 물리적 사실성을 손상합니다. Egopose[51]는 휴머노이드가 떨어지려고 할 때 운동학적 포즈로 휴머노이드를 재설정하는 실패 안전 메커니즘을 설계하여 휴머노이드가 신뢰할 수 없는 운동학적 포즈로 계속 재설정되는 잠재적인 순간이동 동작을 초래합니다.NeruoMoCon[14]은 샘플링 기반 제어를 활용하고 휴머노이드가 떨어지면 샘플링 프로세스를 다시 실행합니다.이 방법은 효과적이기는 하지만 성공을 보장하지 않으며 실시간 사용 사례를 금지합니다.또 다른 자연스러운 방법은 휴머노이드가 참조 동작에서 벗어났을 때 추가 복구 정책[7]을 사용하는 것입니다.그러나 이러한 복구 정책은 더 이상 참조 동작에 액세스할 수 없으므로 고주파 지터와 같은 부자연스러운 동작이 발생합니다.이를 방지하기 위해 ASE[34]는 칼을 휘두르는 정책에 대해 자연스럽게 땅에서 일어날 수 있는 기능을 보여줍니다.인상적이기는 하지만 동작 모방에서 정책은 땅에서 일어날 뿐만 아니라 참조 동작을 추적하기 위해 다시 돌아가야 합니다. 이 연구에서 우리는 동작 모방에서 실패 상태 복구 문제에 대한 포괄적인 솔루션을 제안합니다. PHC는 쓰러진 상태에서 일어나 자연스럽게 참조 동작으로 돌아가 모방을 재개할 수 있습니다. 점진적 강화 학습. 다양한 패턴을 포함하는 데이터에서 학습할 때 미세 조정을 통해 멀티태스킹 또는 전이 학습을 수행하려고 하면 치명적인 망각[9, 27]이 관찰됩니다. 네트워크 가중치 정규화[18], 여러 전문가 학습[16] 또는 전문가 혼합[56, 38, 47]이나 곱셈 제어[33]를 사용하여 용량 증가와 같은 다양한 접근 방식[8, 16, 18]이 이 현상에 대처하기 위해 제안되었습니다. 전이 학습 및 도메인 적응에서 점진적 학습[6, 4] 또는 커리큘럼 학습[1]으로 패러다임이 연구되었습니다. 최근 점진적 강화 학습[3]이 여러 전문가 정책에서 기술을 추출하기 위해 제안되었습니다. 전문가의 최적 혼합을 찾는 대신 전문가의 동작 분포에 가장 잘 맞는 정책을 찾는 것을 목표로 합니다. 진행성 신경망(PNN) [36]은 이전에 학습된 하위 네트워크의 가중치를 동결하고 새로운 작업을 학습하기 위해 추가 하위 네트워크를 초기화하여 치명적인 망각을 방지하는 것을 제안합니다. 이전 하위 네트워크의 경험은 측면 연결을 통해 전달됩니다. PNN은 작업에 따라 사용할 하위 네트워크를 수동으로 선택해야 하므로 참조 동작에 작업 레이블 개념이 없기 때문에 동작 모방에 사용되지 않습니다. 3.
--- METHOD ---
s, 현재의 제어 정책은 비디오나 언어와 같은 노이즈가 많은 관찰을 처리할 수 없기 때문입니다.아바타에 물리적으로 시뮬레이션된 휴머노이드를 적용하기 위해 첫 번째 주요 과제는 높은 성공률로 인간과 같은 동작을 충실하게 재현할 수 있는 동작 모방자(컨트롤러)를 학습하는 것입니다.강화 학습(RL) 기반 모방 정책은 유망한 결과를 보였지만, AMASS(1만 개의 클립, 40시간의 동작)와 같은 대규모 데이터 세트에서 단일 정책으로 동작을 성공적으로 모방한 것은 아직 이루어지지 않았습니다.더 크거나 여러 전문가 정책을 사용하려는 시도는 어느 정도 성공을 거두었지만[45, 47], 아직 가장 큰 데이터 세트로 확장되지는 않았습니다.따라서 연구자들은 휴머노이드를 안정화하는 데 도움이 되는 외부 힘을 사용했습니다. 잔류 힘 제어(RFC) [52]는 AMASS 데이터 세트 [22]의 최대 97%를 모방할 수 있는 동작 모방자를 만드는 데 도움이 되었으며 비디오에서 인간 포즈 추정[54, 23, 12] 및 언어 기반 동작 생성[53]에서 성공적인 응용 프로그램을 보였습니다. 그러나 외부 힘은 인간형을 꼭두각시로 조종하는 &quot;신의 손&quot; 역할을 하여 물리적 사실성을 손상시켜 날기 및 떠다니기와 같은 아티팩트를 발생시킵니다. RFC를 사용하면 모델이 인간형에 비물리적 힘을 자유롭게 적용할 수 있으므로 시뮬레이션의 사실성이 손상된다고 주장할 수 있습니다. 시뮬레이션된 인간형을 제어하는 또 다른 중요한 측면은 노이즈가 많은 입력 및 실패 사례를 처리하는 방법입니다. 이 연구에서는 비디오 또는 언어 입력에서 추정된 인간 포즈를 고려합니다. 특히 비디오 입력과 관련하여 부유[53], 발 미끄러짐[57] 및 물리적으로 불가능한 포즈와 같은 아티팩트는 폐색, 어려운 관점 및 조명, 빠른 동작 등으로 인해 인기 있는 포즈 추정 방법에서 만연합니다. 이러한 경우를 처리하기 위해 대부분의 물리 기반 방법은 실패 조건이 트리거될 때 휴머노이드를 재설정하는 데 의존합니다[24, 22, 51]. 그러나 성공적으로 재설정하려면 고품질 참조 포즈가 필요한데, 이는 포즈 추정의 노이즈 특성으로 인해 얻기 어려운 경우가 많아 넘어지고 신뢰할 수 없는 포즈로 재설정하는 악순환이 발생합니다. 따라서 예상치 못한 넘어짐과 노이즈가 많은 입력을 우아하게 처리하고 실패 상태에서 자연스럽게 복구하고 모방을 재개할 수 있는 컨트롤러가 중요합니다. 이 작업에서 우리의 목표는 인간 사용자의 비디오 관찰을 사용하여 아바타를 제어하는 실시간 가상 아바타를 제어하도록 특별히 설계된 휴머노이드 컨트롤러를 만드는 것입니다. 우리는 모션 모방에서 높은 성공률을 달성하고 실패 상태에서 자연스럽게 복구할 수 있는 단일 정책인 Perpetual Humanoid Controller(PHC)를 설계합니다. 우리는 AMASS 데이터 세트 전체에서 모션 시퀀스로부터 학습하는 점진적 곱셈 제어 정책(PMCP)을 제안하여 치명적인 망각을 겪지 않습니다. 점점 더 어려운 모션 시퀀스를 다른 &quot;작업&quot;으로 취급하고 학습할 새로운 네트워크 용량을 점진적으로 할당함으로써 PMCP는 더 어려운 모션 클립을 학습할 때 더 쉬운 모션 클립을 모방하는 기능을 유지합니다. 또한 PMCP를 통해 컨트롤러는 모션 모방 기능을 손상시키지 않고도 실패 상태 복구 작업을 학습할 수 있습니다. 또한 파이프라인 전체에 Adversarial Motion Prior(AMP)[35]를 채택하고 실패 상태 복구 중에 자연스럽고 인간과 같은 행동을 보장합니다. 또한 대부분의 모션 모방 방법은 링크 위치와 회전을 모두 입력으로 추정해야 하지만 링크 위치만 필요한 컨트롤러를 설계할 수 있음을 보여줍니다. 이 입력은 비전 기반 3D 키포인트 추정기 또는 VR 컨트롤러의 3D 포즈 추정을 통해 더 쉽게 생성할 수 있습니다. 요약하자면, 우리의 기여는 다음과 같습니다.(1) 우리는 외부 힘을 적용하지 않고도 AMASS 데이터 세트의 98.9%를 성공적으로 모방할 수 있는 영구 인간형 컨트롤러를 제안합니다.(2) 우리는 대규모 동작 데이터 세트에서 재앙적 망각 없이 학습하고 실패 상태 복구와 같은 추가 기능을 잠금 해제하기 위한 점진적 곱셈 제어 정책을 제안합니다.(3) 우리의 컨트롤러는 작업에 구애받지 않으며 드롭인 솔루션으로 기성품 비디오 기반 포즈 추정기와 호환됩니다. 우리는 모션 캡처(MoCap)와 비디오에서 추정된 동작을 모두 평가하여 컨트롤러의 기능을 보여줍니다. 또한 웹캠 비디오를 입력으로 사용하여 영구적으로 시뮬레이션된 아바타를 운전하는 라이브(30fps) 데모를 보여줍니다.2. 관련 연구 물리 기반 동작 모방. 물리 법칙에 따라 제어되는 시뮬레이션된 캐릭터[32, 31, 33, 35, 34, 7, 45, 52, 28, 13, 2, 11, 46, 12]는 자연스러운 인간 동작, 인간과 인간 간 상호작용[20, 48], 인간과 사물 간 상호작용[28, 34]을 생성하는 뚜렷한 이점이 있습니다. 대부분의 최신 물리 시뮬레이터는 미분 불가능하기 때문에 이러한 시뮬레이션된 에이전트를 훈련하려면 RL이 필요하며 이는 시간이 많이 걸리고 비용이 많이 듭니다. 결과적으로 대부분의 작업은 사용자 입력[45, 2, 35, 34]을 기반으로 하는 대화형 제어, 스포츠 경기[48, 20, 28] 또는 기타 모듈형 작업(목표 달성[49], 드리블[35], 이동[32] 등)과 같은 소규모 사용 사례에 집중합니다. 반면, 대규모 동작 데이터 세트를 모방하는 것은 어렵지만 기본적인 작업인데, 참조 동작을 모방할 수 있는 에이전트는 동작 생성기와 쉽게 페어링되어 다양한 작업을 달성할 수 있기 때문이다. 단일 클립[31]을 모방하는 법을 배우는 것부터 데이터 세트[47, 45, 7, 44]에 이르기까지 동작 모방기는 참조 동작을 모방하는 인상적인 능력을 보여주었지만, 종종 고품질 MoCap 데이터를 모방하는 데 국한되었다. 그 중에서도 ScaDiver[47]는 전문가 정책을 혼합하여 CMU MoCap 데이터 세트로 확장하고 실패 시간으로 측정한 약 80%의 성공률을 달성한다. Unicon[45]은 모방과 전송에서 정성적인 결과를 보여주지만, 모방기가 데이터 세트에서 클립을 모방하는 능력을 정량화하지는 않는다. MoCapAct[44]는 먼저 CMU MoCap 데이터 세트에서 단일 클립 전문가를 학습하고, 이를 전문가 성능의 약 80%를 달성하는 단일 클립으로 정제한다. 우리와 가장 가까운 노력은 UHC[22]로, AMASS 데이터 세트의 97%를 성공적으로 모방합니다. 그러나 UHC는 균형을 맞추는 데 도움이 되는 휴머노이드의 뿌리에 비물리적 힘을 적용하는 잔류 힘 제어[51]를 사용합니다. 휴머노이드가 넘어지는 것을 방지하는 데 효과적이지만 RFC는 물리적 사실성을 감소시키고 특히 동작 시퀀스가 어려울 때 떠다니거나 흔들리는 것과 같은 아티팩트를 생성합니다[22, 23]. UHC와 비교할 때, 우리의 컨트롤러는 외부 힘을 사용하지 않습니다. 시뮬레이션된 캐릭터에 대한 실패 상태 복구. 시뮬레이션된 캐릭터는 균형을 잃으면 쉽게 넘어질 수 있으므로 복구를 돕기 위해 많은 접근 방식[39, 51, 34, 42, 7]이 제안되었습니다. PhysCap[39]은 균형을 맞출 필요가 없는 부유 기반 휴머노이드를 사용합니다. 이는 휴머노이드가 더 이상 적절하게 시뮬레이션되지 않기 때문에 물리적 사실성을 손상합니다. Egopose[51]는 휴머노이드가 떨어지려고 할 때 운동학적 포즈로 휴머노이드를 재설정하는 실패 안전 메커니즘을 설계하여 휴머노이드가 신뢰할 수 없는 운동학적 포즈로 계속 재설정되는 잠재적인 순간이동 동작을 초래합니다.NeruoMoCon[14]은 샘플링 기반 제어를 활용하고 휴머노이드가 떨어지면 샘플링 프로세스를 다시 실행합니다.이 방법은 효과적이기는 하지만 성공을 보장하지 않으며 실시간 사용 사례를 금지합니다.또 다른 자연스러운 방법은 휴머노이드가 참조 동작에서 벗어났을 때 추가 복구 정책[7]을 사용하는 것입니다.그러나 이러한 복구 정책은 더 이상 참조 동작에 액세스할 수 없으므로 고주파 지터와 같은 부자연스러운 동작이 발생합니다.이를 방지하기 위해 ASE[34]는 칼을 휘두르는 정책에 대해 자연스럽게 땅에서 일어날 수 있는 기능을 보여줍니다.인상적이기는 하지만 동작 모방에서 정책은 땅에서 일어날 뿐만 아니라 참조 동작을 추적하기 위해 다시 돌아가야 합니다. 이 연구에서 우리는 동작 모방에서 실패 상태 복구 문제에 대한 포괄적인 솔루션을 제안합니다. PHC는 쓰러진 상태에서 일어나 자연스럽게 참조 동작으로 돌아가 모방을 재개할 수 있습니다. 점진적 강화 학습. 다양한 패턴을 포함하는 데이터에서 학습할 때 미세 조정을 통해 멀티태스킹 또는 전이 학습을 수행하려고 하면 치명적인 망각[9, 27]이 관찰됩니다. 네트워크 가중치 정규화[18], 여러 전문가 학습[16] 또는 전문가 혼합[56, 38, 47]이나 곱셈 제어[33]를 사용하여 용량 증가와 같은 다양한 접근 방식[8, 16, 18]이 이 현상에 대처하기 위해 제안되었습니다. 전이 학습 및 도메인 적응에서 점진적 학습[6, 4] 또는 커리큘럼 학습[1]으로 패러다임이 연구되었습니다. 최근 점진적 강화 학습[3]이 여러 전문가 정책에서 기술을 추출하기 위해 제안되었습니다. 전문가의 최적 혼합을 찾는 대신 전문가의 동작 분포에 가장 잘 맞는 정책을 찾는 것을 목표로 합니다. 진행성 신경망(PNN)[36]은 이전에 학습한 하위 네트워크의 가중치를 동결하고 새로운 작업을 학습하기 위해 추가 하위 네트워크를 초기화하여 치명적인 망각을 방지하는 것을 제안합니다. 이전 하위 네트워크의 경험은 측면 연결을 통해 전달됩니다. PNN은 작업에 따라 사용할 하위 네트워크를 수동으로 선택해야 하므로 참조 동작에는 작업 레이블 개념이 없기 때문에 동작 모방에 사용되지 않습니다. 3. 방법 참조 포즈를 ât = (Ôt, pt)로 정의하고, 이는 3D 관절 회전 Ô₁ = RJ×6과 휴머노이드의 모든 J 링크의 위치 pt Є RJ×³로 구성됩니다(6 DoF 회전 표현[55] 사용). 참조 포즈 1:T에서 유한 차분 1:T Δ를 통한 참조 속도 91.7을 계산할 수 있습니다. 여기서 ât ± (ŵt, vt)는 각도 ŵ₁ € R³×³와 선형 속도 ût Є RJ×³로 구성됩니다. 회전 기반 및 키포인트 기반 모션 모방을 입력으로 구분합니다. 회전 기반 모방은 참조 포즈 1:T(회전 및 키포인트 모두)에 의존하는 반면 키포인트 기반 모방은 3D 키포인트 1:T만 필요합니다. 표기 규칙에 따라 포즈 추정기/키포인트 감지기의 운동량(물리 시뮬레이션 없음)을 나타내고, 모션 캡처(MoCap)의 기준 진실 양을 나타내며, 물리 시뮬레이션의 값에 악센트가 없는 일반 기호를 사용합니다. 참조 모션 &quot;모방&quot;, &quot;추적&quot; 및 &quot;모방&quot;을 서로 바꿔 사용합니다. Sec.3.1에서 먼저 주요 프레임워크의 예비 단계를 설정합니다. Sec.3.2에서는 대규모 인간 동작 데이터 세트를 모방하고 실패 상태에서 복구하는 방법을 학습하기 위한 점진적 곱셈 제어 정책을 설명합니다. 마지막으로 Sec.3.3에서는 실시간 사용 사례를 위해 작업 독립적인 컨트롤러를 기성품 비디오 포즈 추정기 및 생성기에 연결하는 방법을 간략하게 설명합니다. 3.1. 적대적 동작 사전을 사용한 목표 조건 동작 모방 = = 컨트롤러는 목표 조건 RL(그림 3)의 일반적인 프레임워크를 따릅니다. 여기서 목표 조건 정책 TPHC는 참조 동작 ĝ1:t 또는 키포인트 Î₁:T를 모방하는 작업을 받습니다. 이전 작업[22, 31]과 유사하게 작업을 상태, 동작, 전환 역학, 보상 함수 및 할인 요인의 튜플 M(S, A, T, R, y)으로 정의된 마르코프 결정 프로세스(MDP)로 공식화합니다. 물리 시뮬레이션은 상태 s₁ = S와 전환 역학 T를 결정하는 반면, 정책 TPHC는 € A에서 단계별 동작을 계산합니다. 시뮬레이션 상태 st와 기준 동작 ật를 기반으로 보상 함수 R은 정책에 대한 학습 신호로 보상 rt R(St, ât)를 계산합니다. 정책의 목표는 할인된 보상 E Σ -1 -1 rt를 최대화하는 것이며, 근위 정책 기울기(PPO) [37]를 사용하여 TPHC· State를 학습합니다. 시뮬레이션 상태 st(8,8%)는 인간형 고유 감각 so와 목표 상태 so로 구성됩니다. 고유 감각(qt, ġt, 3)에는 3D 신체 포즈 qt, 속도 qt 및(선택적으로) 신체 모양 ẞ가 포함됩니다. 다양한 신체 모양으로 학습된 경우 ß에는 각 신체 링크의 팔다리 길이에 대한 정보가 포함됩니다[24]. 회전 기반 동작 모방의 경우 목표 상태 so는 다음 시간 단계 참조 양과 시뮬레이션된 대응물 간의 차이로 정의됩니다.sg-rot± (Ôt+1¤¤¤‚ÂÎt+1¯Pt, Ût+1−Vt, ŵt 여기서 ‚ Pt+1)은 회전 차이를 계산합니다.키포인트 전용 모방의 경우 목표 상태는 ¸²-kp ± (ît+1 − Pt, Ût+St Vt, Pt+1)이 됩니다.sε 및 so의 위의 모든 양은 휴머노이드의 현재 향하는 방향과 루트 위치를 기준으로 정규화됩니다[49, 22].참조 동작 시뮬레이션된 동작 전체 데이터 세트 기본: P(1) 중간 난이도 ART. 하드 마이닝 기본형: p(2) ART ▶: 측면 연결 또는 가중치 공유 하드 마이닝: 평가에 의한 하드 부정 마이닝 실패 복구 가장 어려운 M 전체 데이터 세트 THE 하드 마이닝 기본형: p(K) 기본형: P(F) 작성자: C TM Thaal - 학습 진행 상황 그림 2: 점점 더 어려워지는 시퀀스를 학습하여 기본형 P(1), P(2), ..., P(K)를 학습하는 점진적 학습 절차. 실패 복구 P(F)는 최종적으로 간단한 이동 데이터에서 학습되고 작성자는 이러한 동결된 기본형을 결합하도록 학습됩니다. 정책: TPHC 사전 학습된 기본 동작 구성자: C p()p(2) pik) p(F) -a₁→ 가중치: w ‚1:(K+1), 게이팅 함수 동작 데이터: 상태: St 판별자 D 참조 상태 물리 시뮬레이션(Isaac Gym) 판별자 보상 및 추론 학습 목표 보상 학습(PPO) 그림 3: 적대적 동작 사전을 사용한 목표 조건 RL 프레임워크. 각 기본 동작 P(k)와 구성자 C는 동일한 절차를 사용하여 학습되며, 여기서 최종 제품 πPHC를 시각화합니다. 보상. 동작 모방 보상만 사용하는 사전 동작 추적 정책과 달리, 최근에 제안된 적대적 동작 사전 [35]을 사용하고 프레임워크 전체에 판별자 보상 항을 포함합니다. 판별자 항을 포함하면 컨트롤러가 안정적이고 자연스러운 동작을 생성하는 데 도움이 되며, 자연스러운 실패 상태 복구 동작을 학습하는 데 특히 중요합니다. 구체적으로, 보상은 작업 보상 r¾, 스타일 보상 r 추가 에너지 페널티 r 에너지 [31]의 합계로 정의됩니다: amp r = 0.5 +0.5t + rt . 에너지 amp 및 an (1) 판별기의 경우 AMP [35]와 동일한 관찰, 손실 공식화 및 그래디언트 페널티를 사용합니다. 에너지 페널티는 -0.0005 Σje 관절 | Mjwj|²로 표현됩니다. 여기서 μ; 및 w;는 각각 관절 토크와 관절 각속도에 해당합니다. 에너지 페널티[10]는 정책을 조절하고 외부 힘 없이 학습된 정책에서 나타날 수 있는 발의 고주파 지터를 방지합니다(4.1절 참조). 작업 보상은 현재 학습 목표에 따라 정의되며, 이는 동작 모방 Rimitation 및 실패 상태 복구 Recover에 대한 보상 함수를 전환하여 선택할 수 있습니다. 동작 추적의 경우 다음을 사용합니다. g-imitation р = Rimitation -10||âtqt|| + Wire (8t, âu) Wjpe + Wive -100 || pt-pt || (2) -0.1||t-vt || -0.1||ŵt-wt|| + Wjwe 여기서 우리는 휴머노이드의 모든 링크에 대한 강체의 평행이동, 회전, 선형 속도 및 각속도의 차이를 측정합니다. 실패 상태 복구의 경우, 우리는 보상을 Eq.3에서 정의합니다. rt g-recover = Action. 우리는 휴머노이드의 각 DoF에서 비례 미분(PD) 컨트롤러를 사용하고, at에서의 액션은 PD 타겟을 지정합니다. 타겟 관절을 q = at로 설정하면, 각 관절에 적용되는 토크는 ₁ = k³ (at — qt) - kdo ġt입니다. 이것은 액션이 참조 포즈에 추가되는 이전 동작 모방 방법에서 사용된 잔류 액션 표현[52, 22, 30]과 다르다는 점에 유의하세요: qt = ât at으로 학습 속도를 높이기 위해. PHC는 노이즈가 많고 잘못된 참조 동작에 대해 견고해야 하므로 동작 공간에서 참조 동작에 대한 이러한 종속성을 제거합니다.외부 힘[52]이나 메타 PD 제어[54]를 사용하지 않습니다.제어 정책 및 판별기.제어 정책 TPHC(@t]8t) N(μ(st), σ)는 고정된 대각선 공분산을 갖는 가우시안 분포를 나타냅니다.AMP 판별기 D(S 10:t)는 휴머노이드의 현재 선입견을 기반으로 실수 값과 가짜 값을 계산합니다.모든 네트워크(판별기, 원시, 가치 함수 및 판별기)는 차원이 [1024, 512]인 2층 다층 퍼셉트론(MLP)입니다.휴머노이드.휴머노이드 컨트롤러는 모든 인간 운동학 구조를 지원할 수 있으며, 기존 기술[54, 22, 23]에 따라 SMPL[21] 운동학 구조를 사용합니다. SMPL 바디는 강체로 구성되어 있으며, 그 중 23개가 작동하여 Є R23×3의 동작 공간이 발생합니다. 바디 비율은 바디 모양 매개변수 ẞ = R10에 따라 달라질 수 있습니다. 초기화 및 완화된 조기 종료. 훈련 중에 참조 상태 초기화(RSI) [31]를 사용하고 모방을 위한 동작 클립의 시작점을 무작위로 선택합니다. 조기 종료의 경우 UHC [22]를 따르고 관절이 참조 동작에서 평균적으로 전역적으로 0.5m 이상 떨어져 있을 때 에피소드를 종료합니다. UHC와 달리 종료 조건에서 발목과 발가락 관절을 제거합니다. RFC [52]에서 관찰한 바와 같이 시뮬레이션된 휴머노이드와 실제 인간 사이에 역학적 불일치가 존재합니다. 특히 실제 인간의 발은 다중 세그먼트이기 때문입니다 [29]. 따라서 시뮬레이션된 휴머노이드가 MoCap과 정확히 동일한 발 움직임을 갖는 것은 불가능하며, 참조 발 움직임을 맹목적으로 따르면 휴머노이드가 균형을 잃을 수 있습니다. 따라서 우리는 인간형의 발목과 발가락이 MoCap 동작에서 약간 벗어나 균형을 유지할 수 있도록 하는 Relaxed Early Termination(RET)을 제안합니다. 인간형은 여전히 이러한 신체 부위에 대해 모방 및 판별자 보상을 받아 이러한 관절이 인간이 아닌 방식으로 움직이는 것을 방지합니다. 이것은 작은 세부 사항이지만 좋은 동작 모방 성공률을 달성하는 데 도움이 됨을 보여줍니다. 하드 네거티브 마이닝. 대규모 동작 데이터 세트에서 학습할 때 더 많은 정보적 경험을 수집하기 위해 학습의 후반 단계에서 더 어려운 시퀀스에서 학습하는 것이 필수적입니다. 우리는 UHC[22]에서와 유사한 하드 네거티브 마이닝 절차를 사용하고 컨트롤러가 이 시퀀스를 성공적으로 모방할 수 있는지 여부에 따라 하드 시퀀스를 정의합니다. 동작 데이터 세트 Q에서 전체 데이터 세트에 대해 모델을 평가하고 정책이 모방하지 못하는 시퀀스를 선택하여 하드 시퀀스 hard ≤ Q를 찾습니다. 3.2. 점진적 곱셈 제어 정책 학습이 계속됨에 따라 새로운 시퀀스를 학습할 때 이전 시퀀스를 잊어버리기 때문에 모델의 성능이 정체되는 것을 알 수 있습니다. 하드 네거티브 마이닝은 어느 정도 문제를 완화하지만 동일한 문제로 어려움을 겪습니다. 실패 상태 복구와 같은 새로운 작업을 도입하면 재앙적 망각으로 인해 모방 성능이 더욱 저하될 수 있습니다. 이러한 효과는 부록(부록 C)에서 보다 구체적으로 분류됩니다. 따라서 더 어려운 시퀀스를 학습하기 위해 새로운 하위 네트워크(기본 P)를 할당하는 점진적 곱셈 제어 정책(PMCP)을 제안합니다. 점진적 신경망(PNN). PNN[36]은 전체 데이터 집합 Q에서 학습된 단일 기본 네트워크 P(1)로 시작합니다. P(1)이 모방 작업을 사용하여 전체 동작 데이터 집합 Q에서 수렴하도록 학습되면 Q에서 P(¹)을 평가하여 하드 동작의 하위 집합을 만듭니다. 수렴은 Q()의 성공률이 더 이상 증가하지 않는 것으로 정의합니다. P(1)이 실패한 시퀀스는 Q로 형성됩니다. 그런 다음 P(1)의 매개변수를 동결하고 P(¹)의 각 계층을 P(2)에 연결하는 측면 연결과 함께 새로운 기본 P(2)(무작위로 초기화됨)를 생성합니다. PNN에 대한 자세한 내용은 보충 자료를 참조하십시오. hard (1) hard hard 학습하는 동안 이전 단계 Q에서 실패한 시퀀스를 선택하여 각 Q(K)d를 구성하여 더 작은 (k) (k-1) hard &quot; (k+1) 및 더 작은 hard 하위 집합: Qhard Chard ). 이런 식으로, 그림 2에서 볼 수 있듯이 새로 시작된 각 기본 P(k)가 새롭고 더 어려운 동작 시퀀스 하위 집합을 학습하도록 합니다. 이는 새로운 기본 P를 초기화하여 학습하기 때문에 UHC [22]의 hard-negative mining과 다릅니다. 원래 PNN은 완전히 새로운 작업(예: 다른 Atari 게임)을 해결하도록 제안되었으므로 이후 작업에서 이전 경험을 재사용, 수정 또는 삭제할지 선택할 수 있도록 측면 연결 메커니즘이 제안되었습니다. 그러나 인간의 동작을 모방하는 것은 높은 상관 관계가 있으며, 더 어려운 시퀀스 Q( 에 맞추면 (k) 이전 모터 제어 경험에서 경험을 효과적으로 끌어낼 수 있습니다. 따라서 측면 연결이 없지만 새로운 기본이 이전 계층의 가중치에서 초기화되는 PNN 변형도 고려합니다. 이 가중치 공유 방식은 새로운 기본 P(+1)을 사용하여 더 어려운 동작 시퀀스에서 미세 조정하는 것과 유사하며 학습된 시퀀스를 모방하는 P(k)의 기능을 유지합니다.hard Algo 1: 점진적 곱셈 제어 정책 1 학습 함수 TrainPPO(π, Q(k), D, V, R): 수렴하지 않음 do 샘플링 메모리를 최소화; M이 가득 차지 않음 doP(k)1:T fort Q에서 샘플 동작; 1...T do St←(8), s); at π(at St) ; St+1T(8+1 St, at) // 시뮬레이션%;B rt R(st, ĝt+1) ; (st, at, rt, st+1)을 메모리 M에 저장; VPPO는 M에서 수집된 경험을 사용하여 업데이트; D← 판별자는 M에서 수집된 경험을 사용하여 업데이트 return; 15 입력: 기준 동작 데이터 세트 Q; 16 D, V, Q← Q (1) 하드 함수, 그리고 17 k 1... K do // 판별자, 값 데이터 집합 초기화; P(k) 초기화 측면 연결/가중치 공유%;B (k) ← TrainPPO (P(k), Q (k+1), D, V, 모방); hard (k) );(k+1) ←eval(P(k), P(k) <freeze P(k) loco D, V, Recover)22 P(F) TrainPPO (P // Fail-state Recovery%;B P(F) 23 TPHC Á {P(¹)... p(K) ‚c} ; 24 TPHC – TrainPPO(TPHC, Q, D, V, {R // Train Composer; imitation Rrecovery }) Fail-state Recovery. In addition to learning harder sequences, we also learn new tasks, such as recovering from fail-state. We define three types of fail-state: 1) fallen on the ground; 2) faraway from the reference motion (> 0.5m); 3) 이들의 조합: 넘어짐과 멀리 떨어짐. 이러한 상황에서 휴머노이드는 지면에서 일어나 자연스러운 방식으로 참조 동작에 접근하고 동작 모방을 재개해야 합니다. 이 새로운 작업의 경우 기본 스택의 끝에 기본 P(F)를 초기화합니다. P(F)는 P(1) ... P(k)와 동일한 입력 및 출력 공간을 공유하지만 참조 동작은 실패 상태 복구에 대한 유용한 정보를 제공하지 않으므로(휴머노이드는 지면에 누워 있을 때 참조 동작을 모방하려고 시도해서는 안 됨) 실패 상태 복구 중에 상태 공간을 수정하여 루트를 제외한 참조 동작에 대한 모든 정보를 제거합니다. 참조 관절 회전 ột = [ột, ỗ½, · · · ]의 경우 ¿번째 관절에 해당하며 = [0, 0, 0]을 구성합니다. 여기서 루트를 제외한 모든 관절 회전은 시뮬레이션된 값(없음)으로 대체됩니다. 이는 목표 상태를 계산할 때 루트가 아닌 조인트 목표를 항등성으로 설정하는 것과 같습니다.sg-Fail = (-0, pt-pt, ốt ớt, cất tốt, pi).따라서 88-Fail은 모방 목표에서 대상 루트의 상대적 위치와 방향만 제공되는 포인트 목표[49] 목표로 붕괴됩니다.참조 루트가 5x(pt-pt)만큼 너무 멀리 있는 경우(&gt; 5m), pt – pt를 정규화하여 ||pt-pt ||goal 위치를 고정합니다.휴머노이드가 충분히 가까워지면(예: &lt; 0.5m), 목표는 전체 동작 모방으로 다시 전환됩니다.= g-Fail -Fail || P||2 ≤0.그렇지 않으면.(3) 낙하 상태를 만들기 위해 ASE[34]를 따르고 에피소드 시작 시 무작위로 휴머노이드를 땅에 떨어뜨립니다. (a) MoCap 모션 모방 (b) 실패 상태 복구 (b) Nosiy 모션 모방(비디오: H36M 데이터 세트, HybriK에서 포즈 추정) (c) Nosiy 모션 모방(언어: &quot;사람이 달리고 돌아선다&quot;라는 프롬프트) (d) Nosiy 모션 모방(비디오: 실시간 및 라이브, MeTRAbs에서 포즈 추정) 그림 4: (a) 고품질 MoCap 모방 - 스핀 및 킥. (b) 넘어진 상태에서 복구하고 참조 모션으로 돌아갑니다(빨간색 점으로 표시). (b) 비디오에서 추정된 노이즈 모션 모방. (c) 언어에서 생성된 모션 모방. (d) 실시간 시뮬레이션 아바타에 웹캠 스트림에서 추정된 포즈 사용. 참조 모션에서 2~미터 떨어진 휴머노이드를 초기화하여 멀리 떨어진 상태를 만들 수 있습니다. 실패 상태 복구에 대한 보상은 AMP 보상 r, 포인트-골 보상 7-8-포인트, 보상 함수에서 계산한 에너지 페널티 에너지로 구성됩니다. Rrecover: amp tt 각 P(k)가 독립적인 가우시안이므로 액션 분포: Ci (St) kN Σ C₁₂ (st) Ci (St) µ³¹² (St), σ³ (St) = σ(St) (st) (Σ of (81) (6) g-recover recover πt = R (St,Ô4t) = g-point 0.5rP +0.50.1r energy &quot; = (4) g-point 포인트-목표 보상은 r (dt-1 - dt)로 공식화됩니다. 여기서 dt는 루트 참조와 시간 단계 t에서 시뮬레이션된 루트 사이의 거리입니다 [49]. P (F)를 학습하기 위해 주로 걷기와 달리기 시퀀스를 포함하는 Q loco라는 AMASS 데이터 세트의 엄선된 하위 집합을 사용합니다. Qloco만 사용하여 학습하면 판별자 D와 AMP 보상 r‍ª amp가 걷기와 달리기와 같은 간단한 이동에 편향됩니다. 우리는 기본 요소를 훈련하는 동안 새로운 값 함수와 판별자를 초기화하지 않고 기존 기본 요소를 지속적으로 미세 조정합니다. 곱셈 제어. 각 기본 요소가 학습되면 {P(1).P(K), P(F)}를 얻습니다. 각 기본 요소는 데이터 집합 Q의 하위 집합을 모방할 수 있습니다. 진행형 네트워크[36]에서 작업 전환은 수동으로 수행됩니다. 그러나 동작 모방에서는 하드 시퀀스와 이지 시퀀스의 경계가 모호합니다. 따라서 곱셈 제어 정책(MCP)[33]을 활용하여 추가 작성자 C를 훈련하여 학습된 기본 요소를 동적으로 결합합니다. 기본적으로 사전 훈련된 기본 요소를 작성자 C의 정보 검색 공간으로 사용하고 C는 모방을 위해 활성화할 기본 요소만 선택하면 됩니다. 구체적으로, 작성자 C(w1:K+1st)는 기본 요소와 동일한 입력을 사용하고 가중치 벡터 wi ЄR+1을 출력하여 기본 요소를 활성화합니다. 우리의 작곡가와 기본형을 결합하면, 우리는 PHC의 출력 분포를 얻습니다: 1:K+kTPHC(@t | 8t) (i) = 1) (a (1) | St) C (st), C(st) ≥ 0. (5) II C(St) 여기서 μ(st)는 P()의 j번째 액션 차원에 해당합니다. 한 번에 하나만 활성화하는(상위 1 MOE) 전문가 정책의 혼합과 달리, MCP는 액터의 분포를 결합하고 모든 액터를 동시에 활성화합니다(상위 무한 MOE와 유사). MCP와 달리, 우리는 우리의 기본형을 점진적으로 훈련하고 작곡가와 액터가 같은 입력 공간을 공유하도록 합니다. 기본형은 서로 다른 더 어려운 시퀀스에 대해 독립적으로 훈련되기 때문에, 우리는 복합 정책이 성능에서 상당한 향상을 보이는 것을 관찰합니다. 작곡가 훈련 동안, 우리는 실패 상태 복구 훈련을 끼어넣습니다. 훈련 과정은 알고리즘 1과 그림 2에 설명되어 있습니다. 3.3. 동작 추정기와 연결하기 PHC는 동작 추적을 위해 다음 타임스텝 참조 포즈 Ĩt 또는 키포인트 pt만 필요하므로 작업에 구애받지 않습니다.따라서 SMPL 운동 구조와 호환되는 기성품 비디오 기반 인간 포즈 추정기 또는 생성기를 사용할 수 있습니다.비디오에서 시뮬레이션된 아바타를 구동하기 위해 HybrIK[19]와 MeTRAbs[41, 40]를 사용하는데, 둘 다 측정 공간에서 추정하지만 HybrIK가 관절 회전 +를 출력하는 반면 MeTRAbs는 3D 키포인트 pt만 출력한다는 중요한 차이점이 있습니다.언어 기반 동작 생성을 위해 동작 확산 모델(MDM)[43]을 사용합니다.MDM은 프롬프트를 기반으로 분리된 동작 시퀀스를 생성하고 컨트롤러의 복구 기능을 사용하여 중간 동작을 달성합니다.4.
--- EXPERIMENT ---
s 우리는 Sec.4.1에서 비디오에서 추정된 고품질 MoCap 시퀀스와 노이즈가 있는 모션 시퀀스를 모방하는 휴머노이드 컨트롤러의 능력을 평가하고 제거합니다. Sec.4.2에서 컨트롤러가 실패 상태에서 복구하는 능력을 테스트합니다. 모션은 표 1에서 가장 좋습니다. MoCap 모션 시퀀스를 모방한 정량적 결과(*는 인간-사물 상호 작용이 포함된 시퀀스를 제거함을 나타냄). AMASS-Train*, AMASS-Test*, H36M-Motion*에는 각각 11313, 140, 140개의 고품질 MoCap 시퀀스가 포함되어 있습니다. AMASS-Train* AMASS-Test* H36M-Motion* 방법 RFC Succ↑ Eg-mpjpe↓ Empipe↓ Eacc↓ Evel↓ Succ ↑ Eg-mpipe↓ Empjpe↓ EaccEvel↓ Succ↑ Eg-mpipe Empipe↓ Eacc↓ Evel↓ UHC ✓ 97.0% 36.25.4.5.96.4% 50.31.9.7 12.1 87.0% 59.35.4.7.UHC Χ 84.5% 62.39.10.10.62.6% 58.98.Ours Ours-kp 98.9% 37.26.3.X 98.7% 40.32.3.4.9 96.4% 5.5 97.1% 47.30.6.53.39.7.22.8 21.9.10.23.6% 92.9% 95.7% 133.67.14.17.50.33.3.5.49.39.3.5.표 2: 노이즈가 있는 동작에 대한 동작 모방. HybrIK[19]를 사용하여 관절 회전 ết를 추정하고 MeTRAbs[41]를 사용하여 전역 3D 주요 포인트 pt를 추정합니다. HybrIK + MeTRAbs(루트): HybrIK의 관절 회전 +와 MeTRAbs의 루트 위치 po를 사용합니다. MeTRAbs(모든 주요 포인트): MeTRAbs의 모든 주요 포인트 pt를 사용하며 주요 포인트 기반 컨트롤러에만 적용 가능합니다. H36M-테스트-비디오* 방법 RFC UHC ✓ 포즈 추정 HybrIK + MeTRAbs(루트) Succ↑ Eg-mpjpe↓ Empipe↓ 58.1% 75.49.UHC Ours Ours-kp Ours-kp X HybrIK + MeTRAbs(루트) 18.1% 126.67.× HybrIK + MeTRAbs(루트) 88.7% 55.34.X HybrIK + MeTRAbs(루트) MeTRAbs(모든 주요 지점) 90.0% 55.41.91.9% 55.41.videos, 보충 자료에서 광범위한 정성적 결과를 제공합니다. 모든 실험은 세 번 실행하여 평균을 냅니다. 기준선. SOTA 모션 모방 UHC[22]와 비교하고 공식 구현을 사용합니다. 잔류 힘 제어가 있는 경우와 없는 경우 모두 UHC와 비교합니다. 구현 세부 정보. 모든 평가에 4개의 기본형(실패 상태 복구 포함)을 사용합니다.PHC는 단일 NVIDIA A100 GPU에서 학습할 수 있으며, 모든 기본형과 컴포저를 학습하는 데 약 일주일이 걸립니다.학습이 완료되면 복합 정책이 &gt; 30 FPS로 실행됩니다.물리 시뮬레이션은 NVIDIA의 Isaac Gym[26]에서 수행됩니다.제어 정책은 30Hz에서 실행되는 반면 시뮬레이션은 60Hz에서 실행됩니다.평가를 위해 체형 변화를 고려하지 않고 평균 SMPL 체형을 사용합니다.데이터 세트.PHC는 AMASS[25] 데이터 세트의 학습 분할에서 학습합니다.UHC[22]를 따르고 노이즈가 있거나 인간 물체의 상호 작용이 포함된 시퀀스를 제거하여 11313개의 고품질 학습 시퀀스와 140개의 테스트 시퀀스를 생성합니다.포즈 추정 방법에서 보이지 않는 MoCap 시퀀스와 노이즈가 있는 포즈 추정을 처리하는 정책의 능력을 평가하기 위해 인기 있는 H36M 데이터 세트[15]를 사용합니다. H36M에서 H36M-Motion*과 H36M-Test-Video*의 두 하위 집합을 파생합니다. H36M-Motion*에는 전체 H36M 데이터 세트에서 가져온 140개의 고품질 MoCap 시퀀스가 포함되어 있습니다. H36M-Test-Video*에는 H36M 테스트 분할의 비디오에서 추정한 노이즈가 있는 포즈의 160개 시퀀스가 포함되어 있습니다(SOTA 포즈 추정 방법은 H36M의 훈련 분할에서 훈련되었기 때문). *는 인간-의자 상호 작용이 포함된 시퀀스를 제거했음을 나타냅니다. 지표. 포즈 기반 및 물리 기반 지표 시리즈를 사용하여 동작 모방 성능을 평가합니다. UHC[22]에서와 같이 성공률(Succ)을 보고하며, 모방 중 어느 시점에서든 신체 관절이 참조 동작에서 평균 &gt; 0.5m 떨어져 있을 때 모방이 실패한 것으로 간주합니다. Succ는 휴머노이드가 균형을 잃거나 크게 뒤처지지 않고 참조 동작을 추적할 수 있는지 여부를 측정합니다. 또한 루트 상대 평균 관절당 위치 오차(MPJPE) Empipe와 전역 MPJPE Eg-mpjpe(mm)를 보고하여 모방자가 로컬(루트 상대) 및 전역적으로 참조 동작을 모방하는 능력을 측정합니다. 물리적 사실성을 보여주기 위해 시뮬레이션된 동작과 MoCap 동작 간의 가속도 Eace(mm/프레임²)와 속도 Evel(mm/프레임) 차이도 비교합니다. 모든 기준선과 방법은 물리적으로 시뮬레이션되므로 발이 미끄러지거나 관통하는 내용은 보고하지 않습니다. 4.1. 동작 모방 고품질 MoCap에서의 동작 모방. 표는 AMASS 훈련, 테스트 및 H36MMotion* 데이터 세트에서 동작 모방 결과를 보고합니다. RFC를 사용한 기준선과 비교할 때, 우리의 방법은 훈련 및 테스트 데이터 세트에서 거의 모든 지표에서 더 우수한 성과를 보입니다. 훈련 데이터 세트에서 PHC는 더 나은 성공률을 보이며 더 좋거나 비슷한 MPJPE를 달성하여 훈련 분할의 시퀀스를 더 잘 모방하는 능력을 보여줍니다. 테스트에서 PHC는 AMASS와 H36M 데이터에서 보이지 않는 MoCap 시퀀스에서 높은 성공률을 보였습니다. 보이지 않는 동작은 더 큰 관절당 오류에서 볼 수 있듯이 추가적인 과제를 제기합니다. 잔류 힘 없이 훈련된 UHC는 테스트 세트에서 성능이 좋지 않아 보이지 않는 참조 동작을 모방하는 능력이 부족함을 보여줍니다. 주목할 점은 고주파 지터를 사용하여 균형을 유지하기 때문에 가속 오류가 훨씬 더 큽니다. UHC와 비교할 때, 저희 컨트롤러는 보이지 않는 동작 시퀀스에 직면하더라도 가속 오류가 낮아 에너지 페널티와 동작 사전의 이점을 얻습니다. 놀랍게도, 저희의 키포인트 기반 컨트롤러는 회전 기반 컨트롤러와 동등하거나 때로는 더 나은 성능을 보입니다. 이는 키포인트 기반 동작 모방기가 회전 기반 컨트롤러에 대한 간단하고 강력한 대안이 될 수 있음을 입증합니다. 비디오의 노이즈가 있는 입력에 대한 동작 모방. 우리는 기성 포즈 추정기 HybrIK [19] 및 MeTRAbs [41]를 사용하여 H36M 테스트 세트의 이미지를 사용하여 관절 회전(HybrIK) 및 주요 포인트(MeTRAbs)를 추출합니다. 후처리 단계로 추출된 포즈와 주요 포인트에 가우시안 필터를 적용합니다. HyBrIK와 MeTRAbs는 모두 시간 정보를 사용하지 않는 프레임당 모델입니다. 깊이 모호성으로 인해 단안 전역 포즈 추정은 매우 노이즈가 많고[41] 심각한 깊이 방향 지터가 발생하여 동작 모방기에 상당한 어려움을 줍니다. MeTRAbs가 더 나은 전역 루트 추정 Ã를 출력한다는 것을 알았으므로 p를 HybrIK의 추정 관절 회전 ỗt(HybrIK + Metrabs(루트))와 결합했습니다. 표 2에서 이러한 노이즈가 있는 시퀀스를 모방하는 컨트롤러와 베이스라인의 성능을 보고합니다. MoCap Imitation의 결과와 유사하게 PHC는 기준선을 능가합니다. 표 3: H36M-Test-Video* 데이터에서 HybrIK + Metrabs(루트)의 노이즈 포즈 추정치를 사용하여 수행한 파이프라인 구성 요소에 대한 소거. RET: 완화된 조기 종료. MCP: 곱셈 제어 정책. PNN: 진행형 신경망. 표 4: 이러한 시나리오(인간형을 땅에 떨어뜨리고 참조 동작에서 멀리 떨어뜨림)를 생성하고 추적을 재개하는 데 걸리는 시간을 측정하여 컨트롤러가 실패 상태에서 복구할 수 있는지 측정합니다. H36M-테스트-비디오* RET MCP PNN 회전 실패-복구 Succ↑ Eg-mpipe↓ Empjpe↓ 방법 Ours Fallen-State Succ-5s ↑ 95.0% Far-State 51.2% 56.34.✓ XX 59.4% 60.37.Ours-kp 92.5% 98.8% 94.6% Succ-5s ↑ Succ-10s ↑ 83.7% 95.1% Succ-10s ↑ 99.5% 96.0% Fallen Far-State Succ-5s ↑ Succ-10s ↑ 93.4% 98.8% 79.4% 93.2% 66.2% 59.38.✓ ✓ ✓ ✓ 86.9% 53.33.✓ 88.7% 55.34.✓ 90.0% 55.41.큰 차이로 높은 성공률(~90%)을 달성합니다.이는 PHC가 노이즈가 있는 동작에 강하고 비디오에서 직접 시뮬레이션된 아바타를 구동하는 데 사용할 수 있다는 가설을 검증합니다.마찬가지로 키포인트 기반 컨트롤러(ours-kp)가 회전 기반 컨트롤러보다 성능이 우수한 것을 알 수 있는데, 이는 1) 이미지에서 직접 3D 키포인트를 추정하는 것이 관절 회전을 추정하는 것보다 쉬운 작업이므로 MeTRABS의 키포인트가 HybrIK의 관절 회전보다 품질이 더 높기 때문입니다.2) 키포인트 기반 컨트롤러는 키포인트를 일치시키기 위해 모든 관절 구성을 사용할 수 있는 자유가 있으므로 노이즈가 있는 입력에 더 강합니다.절제.표 3은 다양한 구성 요소가 비활성화된 상태로 학습된 컨트롤러를 보여줍니다.우리는 H36M-Test-Image*의 노이즈가 있는 입력에 절제를 수행하여 컨트롤러가 노이즈가 있는 데이터를 모방하는 능력을 더 잘 보여줍니다. 먼저, 실패 상태에서 복구하기 위한 훈련 전에 컨트롤러의 성능을 연구합니다. 행(R1)과 R2를 비교하면, 느슨한 조기 종료(RET)를 통해 정책이 균형을 맞추기 위해 발목과 발가락을 더 잘 사용할 수 있음을 알 수 있습니다. R2 대 R3은 점진적 훈련 프로세스 없이 MCP를 직접 사용하면 네트워크 용량이 확대되어 네트워크 성능이 향상됨을 보여줍니다. 그러나 PMCP 파이프라인을 사용하면 견고성과 모방 성능이 크게 향상됩니다(R3 대 R4). R4와 R5를 비교하면 PMCP가 동작 모방을 손상시키지 않고도 실패 상태 복구 기능을 추가하는 데 효과적임을 알 수 있습니다. 마지막으로, R5 대 R6은 키포인트 기반 모방자가 회전 기반 모방자와 동등할 수 있으며, 키포인트만 필요한 더 간단한 공식을 제공함을 보여줍니다. MOE 대 MCP에 대한 추가 절제, 기본 요소 수에 대해서는 보충 자료를 참조하세요. 실시간 시뮬레이션 아바타. 컨트롤러가 비디오에서 실시간으로 스트리밍된 포즈 추정치를 모방하는 능력을 보여줍니다. 그림 4는 사무실 환경에서 추정된 포즈를 사용한 라이브 데모에 대한 정성적 결과를 보여줍니다. 이를 달성하기 위해 키포인트 기반 컨트롤러와 MeTRAbs 추정 키포인트를 스트리밍 방식으로 사용합니다. 배우는 포즈와 점프와 같은 일련의 동작을 수행하고 컨트롤러는 안정을 유지할 수 있습니다. 그림 4는 또한 컨트롤러가 동작 언어 모델 MDM[43]에서 직접 생성된 참조 동작을 모방하는 기능을 보여줍니다. 실시간 사용 사례에 대한 보충 자료에서 광범위한 정성적 결과를 제공합니다. 4.2. 실패 상태 복구 컨트롤러의 실패 상태에서 복구하는 기능을 평가하기 위해 컨트롤러가 특정 시간 프레임 내에 참조 동작에 성공적으로 도달할 수 있는지 측정합니다. 세 가지 시나리오를 고려합니다. 1) 땅에 떨어짐, 2) 참조 동작에서 멀리 떨어짐, 3) 떨어지고 참조에서 멀리 떨어짐. 이 평가 동안 정지 참조 동작의 단일 클립을 사용합니다. 휴머노이드를 땅에 떨어뜨리고 150개 시간 단계 동안 무작위 관절 토크를 적용하여 떨어짐 상태를 생성합니다. 우리는 기준 동작에서 3미터 떨어진 휴머노이드를 초기화하여 원거리 상태를 생성합니다. 실험은 무작위로 1000번 시행합니다. 표 4에서 키포인트 기반 및 회전 기반 컨트롤러 모두 휴머노이드가 넘어지고 기준 동작에서 멀리 떨어진 어려운 시나리오에서도 높은 성공률(&gt;90%)로 낙하 상태에서 복구할 수 있음을 알 수 있습니다. 실패 상태 복구에 대한 보다 시각적인 분석은 보충 비디오를 참조하세요. 5. 토론 제한 사항. 우리의 의도된 PHC는 MoCap 및 노이즈가 많은 입력에서 인간의 동작을 충실하게 모방할 수 있지만, 훈련 세트에서 100% 성공률을 달성하지는 못합니다. 검사 결과, 높이뛰기 및 뒤로 뒤집기와 같은 매우 동적인 동작은 여전히 어렵다는 것을 알게 되었습니다. 이러한 시퀀스에서 단일 클립 컨트롤러를 과적합하도록 훈련할 수 있지만(보충 자료 참조), 전체 컨트롤러는 종종 이러한 시퀀스를 학습하지 못합니다. 우리는 이러한 고도로 동적인 클립(더 간단한 동작과 함께)을 학습하려면 더 많은 계획과 의도(예: 높이뛰기까지 달리기)가 필요하며, 이는 컨트롤러의 단일 프레임 포즈 타겟 t+1에서 전달되지 않는다고 가정합니다. 또한 점진적인 학습 절차로 인해 학습 시간도 깁니다. 더욱이 더 나은 다운스트림 작업을 달성하기 위해 현재의 분리된 프로세스(비디오 추정 포즈가 물리 시뮬레이션을 인식하지 못하는 경우)는 충분하지 않을 수 있습니다. 포즈 추정[54, 23] 및 언어 기반 동작 생성[53]과의 긴밀한 통합이 필요합니다.
--- CONCLUSION ---
및 향후 작업. 우리는 고품질 동작 모방을 달성하는 동시에 실패 상태에서 복구할 수 있는 범용 물리 기반 동작 모방기인 Perpetual Humanoid Controller를 소개합니다. 우리의 컨트롤러는 비디오에서 추정된 노이즈 동작에 강하며 재설정 없이도 실시간 아바타를 영구적으로 시뮬레이션하는 데 사용할 수 있습니다. 향후 방향에는 1) 모방 기능 개선 및 훈련 세트의 동작 시퀀스를 100% 모방하는 방법 학습, 2) 지형 및 장면 인식을 통합하여 인간-객체 상호 작용을 가능하게 함, 3) 포즈 추정 및 동작 생성 등과 같은 다운스트림 작업과의 긴밀한 통합이 포함됩니다. 감사의 말. 이 논문의 플롯을 만드는 데 도움을 준 Zihui Lin에게 감사드립니다. Zhengyi Luo는 Meta AI Mentorship(AIM) 프로그램의 지원을 받았습니다. 참고문헌 [1] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston. 커리큘럼 학습. 영어: In Proceedings of the 26th Annual International Conference on Machine Learning, ICML &#39;09, pages 41-48, New York, NY, USA, June 2009. Association for Computing Machinery. [2] Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. DReCon: 물리 기반 캐릭터의 데이터 기반 반응 제어. ACM Trans. Graph., 38(6):11, 2019. [3] Glen Berseth, Cheng Xie, Paul Cernek, and Michiel Van de Panne. 다중 기술 동작 제어를 위한 증류를 통한 점진적 강화 학습. 2018년 2월. [4] Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. 동물 포즈 추정을 위한 교차 도메인 적응. 2019년 8월. [5] Jinkun Cao, Xinshuo Weng, Rawal Khirodkar, Jiangmiao Pang, Kris Kitani. 관찰 중심 SORT: 강력한 다중 객체 추적을 위한 SORT 재고. 2022년 3월. [6] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang. 비지도 도메인 적응을 위한 점진적 특징 정렬. 2018년 11월. [7] Nuttapong Chentanez, Matthias Müller, Miles Macklin, Viktor Makoviychuk, Stefan Jeschke. 심층 강화 학습을 통한 물리 기반 모션 캡처 모방. 회의록 - MIG 2018: ACM SIGGRAPH 모션, 상호작용 및 게임 컨퍼런스, 2018. [8] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh 및 Tinne Tuytelaars. 지속적인 학습 설문 조사: 분류 작업에서 망각에 맞서기.IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022년 7월. [9] Robert M French 및 Nick Chater. 연결주의 네트워크에서 오류 표면을 계산하기 위한 노이즈 사용: 치명적 망각을 줄이는 새로운 수단. Neural Comput., 14(7):1755-1769, 2002년 7월. [10] Zipeng Fu, Xuxin Cheng 및 Deepak Pathak. 전신 심층 제어: 조작 및 이동을 위한 통합 정책 학습. 2022년 10월. [11] Levi Fussell, Kevin Bergamin 및 Daniel Holden. SuperTrack: 지도 학습을 사용하여 물리적으로 시뮬레이션된 캐릭터의 동작 추적. ACM Trans. Graph., 40(6):1–13, 2021년 12월. [12] Kehong Gong, Bingbing Li, Jianfeng Zhang, Tao Wang, Jing Huang, Michael Bi Mi, Jiashi Feng 및 Xinchao Wang. PoseTriplet: 자기 감독 하에 공진화하는 3D 인간 포즈 추정, 모방 및 환각. CVPR, 2022년 3월. [13] Leonard Hasenclever, Fabio Pardo, Raia Hadsell, Nicolas Heess 및 Josh Merel. CoMic: 재사용 가능한 기술을 위한 보완적 작업 학습 및 모방. http://proceedings.mlr.press/v119/hasenclever20a/hasenclever20a.pdf. 중단: 2023-2-13. Ac[14] Buzhen Huang, Liang Pan, Yuan Yang, Jingyi Ju 및 Yangang Wang. Neural MoCon: 물리적으로 그럴듯한 인간 모션 캡처를 위한 신경 모션 제어입니다. 2022년 3월. [15] Catalin Ionescu, Dragos Papava, Vlad Olaru, Cristian Sminchisescu. Human3.6M: 자연 환경에서 3D 인간 감지를 위한 대규모 데이터 세트 및 예측 방법입니다. IEEE 트랜스. 패턴 애널. 마하. Intel., 36(7):1325-1339, 2014. [16] Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu 및 Hao Su. 일반 전문가 학습을 통해 정책 최적화를 개선합니다. 2022년 6월. [17] Jocher, Glenn 및 Chaurasia, Ayush 및 Qiu, Jing. Yolov8. [18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran 및 Raia Hadsell. 신경망에서 치명적인 망각을 극복합니다. 진행 Natl. Acad. 과학. USA, 114(13):3521-3526, 2017. [19] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang 및 Cewu Lu. HybrIK: 3D 인간 포즈 및 모양 추정을 위한 하이브리드 분석-신경 역 운동학 솔루션. 2020년 11월. [20] Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, SM Ali Eslami, Daniel Hennes, Wojciech M Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y Siegel, Leonard Hasenclever, Luke Marris, Saran Tunyasuvunakool, H Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess. 시뮬레이션된 휴머노이드 축구에서 운동 제어부터 팀 플레이까지. 2021년 5월. [21] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black. SMPL: 스킨 처리된 다중인격 선형 모델. ACM Trans. Graph., 34(6), 2015. [22] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. 자기중심적 포즈 추정을 위한 동역학 조절 운동학 정책. NeurIPS, 34:25019-25032, 2021. [23] Zhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. 구체화된 장면 인식 인간 포즈 추정. NeurIPS, 2022년 6월. [24] Zhengyi Luo, Ye Yuan, and Kris M Kitani. 범용 휴머노이드 제어에서 자동으로 물리적으로 유효한 캐릭터 생성까지. 2022년 6월. [25] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. AMASS: 표면 모양으로서의 모션 캡처 아카이브. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 2019년 10월:5441-5450, 2019. [26] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Gavriel State. Isaac gym: 로봇 학습을 위한 고성능 GPU 기반 물리 시뮬레이션. 2021년 8월. [27] Michael McCloskey 및 Neal J Cohen. 연결주의 네트워크의 치명적 간섭: 순차적 학습 문제. Gordon H Bower 편집, 학습 및 동기 심리학, 24권, 109-165페이지. Academic Press, 1989년 1월. [28] Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, Nicolas Heess. 잡고 나르기: 비전 유도 전신 작업을 위한 재사용 가능한 신경 컨트롤러. ACM Trans. Graph., 39(4), 2020. [29] Hwangpil Park, Ri Yu, Jehee Lee. 인간 애니메이션을 위한 다중 세그먼트 발 모델링. 제11회 국제 동작, 상호작용 및 게임 연례 컨퍼런스 회의록, MIG &#39;18의 제16호, 1-10페이지, 뉴욕, 뉴욕, 미국, 2018년 11월. Association for Computing Machinery. [30] Soohwan Park, Ryu Hoseok, Seyoung Lee, Sunmin Lee, Jehee Lee. 비정형 인간 동작 데이터에서 예측 및 시뮬레이션 정책 학습. ACM Trans. Graph., 38(6):1– 11, 2019년 11월. [31] Xue Bin Peng, Pieter Abbeel, Sergey Levine, Michiel van de Panne. DeepMimic. ACM Trans. Graph., 37(4):1– 14, 2018. [32] Xue Bin Peng, Glen Berseth, Kangkang Yin, Michiel Van De Panne. DeepLoco: 계층적 심층 강화 학습을 사용한 동적 이동 기술. ACM Trans. Graph., 36(4):1-13, 2017년 7월. [33] Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, Sergey Levine. MCP: 곱셈 구성 정책을 사용하여 구성 가능한 계층적 제어 학습. 2019년 5월. [34] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine 및 Sanja Fidler. ASE: 물리적으로 시뮬레이션된 캐릭터를 위한 대규모 재사용 가능한 적대적 기술 임베딩. 2022년 5월. [35] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine 및 Angjoo Kanazawa. AMP: 양식화된 물리 기반 캐릭터 제어를 위한 적대적 동작 사전 확률. ACM Trans. Graph., (4):1-20, 2021년 4월. [36] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu 및 Raia Hadsell. 진행형 신경망. arXiv [cs.LG], 2016년 6월. [37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 근접 정책 최적화 알고리즘. 기술 보고서, 2017년. [38] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean. 엄청나게 큰 신경망: 희소 게이트 혼합 전문가 계층. arXiv [cs.LG], 2017년 1월. [39] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt. PhysCap: 실시간으로 물리적으로 타당한 단안 3D 동작 캡처. (1), 2020년 8월. [40] István Sárándi, Alexander Hermans 및 Bastian Leibe. 기하학 인식 자동 인코더를 사용하여 수십 개의 데이터 세트에서 3D 인간 포즈 추정을 학습하여 골격 형식을 연결합니다. 2022년 12월. [41] István Sárándi, Timm Linder, Kai O Arras 및 Bastian Leibe. MeTRAbs: 절대 3D 인간 포즈 추정을 위한 메트릭 규모 절단 강건 히트맵. arXiv, 1-14페이지, 2020년. [42] Tianxin Tao, Matthew Wilson, Ruiyu Gou 및 Michiel van de Panne. 일어나는 법 배우기. arXiv [cs.GR], 2022년 4월. [43] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano 및 Daniel Cohen-Or. 인간 동작 확산 모델.arXiv [cs.CV], 2022년 9월. [44] Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, Matthew Hausknecht. MoCapAct: 시뮬레이션된 휴머노이드 제어를 위한 다중 작업 데이터 세트.2022년 8월. [45] Tingwu Wang, Yunrong Guo, Maria Shugrina, Sanja Fidler. UniCon: 물리 기반 캐릭터 동작을 위한 범용 신경 컨트롤러.arXiv, 2020. [46] Alexander Winkler, Jungdam Won, Yuting Ye. QuestSim: 시뮬레이션된 아바타가 있는 희소 센서에서 인간 동작 추적.2022년 9월. [47] Jungdam Won, Deepak Gopinath, Jessica Hodgins. 물리적으로 시뮬레이션된 캐릭터의 다양한 행동을 제어하는 확장 가능한 접근 방식.ACM Trans. Graph., 39(4), 2020. [48] Jungdam Won, Deepak Gopinath, Jessica Hodgins. 2인 경쟁 스포츠를 수행하는 물리적으로 시뮬레이션된 캐릭터를 위한 제어 전략. ACM Trans. Graph., 40(4):1– 11, 2021년 7월. [49] Jungdam Won, Deepak Gopinath, Jessica Hodgins. 조건부 VAES를 사용하는 물리 기반 캐릭터 컨트롤러. ACM Trans. Graph., 41(4):1–12, 2022년 7월. [50] Ye Yuan과 Kris Kitani. 모방 학습을 통한 3D 자아 포즈 추정. Computer Vision - ECCV 2018, 11220권 LNCS, 763-778쪽. Springer International Publishing, 2018. [51] Ye Yuan과 Kris Kitani. 실시간 PD 제어로서의 자아 포즈 추정 및 예측. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 2019년 10월:10081-10091, 2019. [52] Ye Yuan 및 Kris Kitani. 민첩한 인간 행동 모방 및 확장된 동작 합성을 위한 잔류 힘 제어. (NeurIPS), 2020년 6월. [53] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat 및 Jan Kautz. PhysDiff: 물리학 기반 인간 동작 확산 모델. arXiv [cs.CV], 2022년 12월. [54] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani 및 Jason Saragih. SimPoE: 3D 인간 포즈 추정을 위한 시뮬레이션된 캐릭터 제어. CVPR, 2021년 4월. [55] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang 및 Hao Li. 영어: 신경망에서 회전 표현의 연속성에 관하여.IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2019년 6월: 5738-5746, 2019. [56] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, James Laudon.전문가 선택 라우팅을 사용한 전문가 혼합.2022년 2월. [57] Yuliang Zou, Jimei Yang, Duygu Ceylan, Jianming Zhang, Federico Perazzi, Jia-Bin Huang.지면 접촉 제약 조건을 사용하여 인체 동작 재구성에서 풋스케이트 줄이기.2020년 IEEE 컴퓨터 비전 응용(WACV) 동계 컨퍼런스.IEEE, 2020년 3월.부록 A 소개 B. 구현 세부 정보 B.1. 교육 세부 정보 .B.2. 실시간 사용 사례B.3. 진행성 신경망(PNN) 세부 정보C. 보충 결과C.1. 망각 문제 분류C.2. 추가 절제그림 5: 프레임워크는 체형과 성별 변화를 지원할 수 있습니다.여기서는 다양한 성별과 신체 비율의 휴머노이드가 서 있는 자세를 취하고 있는 모습을 보여줍니다.캡슐 기반(위)과 메시 기반(아래)의 두 가지 유형의 휴머노이드를 구성합니다.빨간색: 여성, 파란색: 남성.색상 그라데이션은 체중을 나타냅니다.D. 확장된 제한 및 논의 부록 A. 서론 이 문서에서는 페이지 제한으로 인해 논문에 포함되지 않은 추가 세부 정보 및 결과를 포함합니다.Sec.B에서는 훈련, 아바타 사용 사례 및 진행성 신경망(PNN)에 대한 추가 세부 정보를 포함합니다[36].Sec.C에서는 추가 절제 결과를 포함합니다.마지막으로 Sec.D에서는 제한 사항, 실패 사례 및 향후 작업에 대한 확장된 논의를 제공합니다.광범위한 정성적 결과는 프로젝트 페이지에 제공됩니다. 독자 여러분께서 저희 방법의 역량을 더 잘 이해하시기 위해 이 글을 꼭 읽어보시길 권장합니다. 구체적으로, 저희 방법이 고품질 MoCap 데이터(훈련 및 테스트 모두)와 비디오에서 추정된 노이즈가 있는 동작을 모방하는 능력을 보여드립니다. 또한 실시간 비디오 기반(단일 및 다중 사람)과 언어 기반 아바타(단일 및 다중 클립) 사용 사례도 보여드립니다. 마지막으로, 실패 상태 복구 능력을 보여드리겠습니다. B. 구현 세부 정보 B.1. 훈련 세부 정보 휴머노이드 구성. 저희 휴머노이드는 어떤 운동학 구조에서든 구성할 수 있으며, 다양한 신체 모양을 기본적으로 지원하고 포즈 추정 문헌에서 널리 채택된 SMPL 휴머노이드 구조를 사용합니다. 그림 5는 AMASS 데이터 세트에서 무작위로 선택한 성별과 신체 모양을 기반으로 구성된 휴머노이드를 보여줍니다. 그런 다음 시뮬레이션 결과를 내보내어 SMPL 메시로 렌더링할 수 있습니다. 캡슐 기반과 메시 기반의 두 가지 유형의 구성된 휴머노이드를 보여드립니다. 캡슐 기반 휴머노이드는 신체 부위를 간단한 기하학적 모양(구, 상자, 캡슐)으로 처리하여 구성됩니다. 메시 기반 휴머노이드는 SimPoE[54]와 유사한 절차에 따라 구성되며, 각 신체 부위는 각 뼈에 할당된 모든 정점의 볼록 껍질을 찾아 생성합니다.캡슐 휴머노이드는 시뮬레이션 및 설계가 더 쉬운 반면, 메시 휴머노이드는 더 복잡한 인간-사물 상호 작용을 시뮬레이션하기 위해 신체 모양을 더 잘 근사합니다.우리는 메시 기반 및 캡슐 기반 휴머노이드가 상당한 성능 차이가 없음을 발견했으며(Sec.C 참조) 모든 실험은 캡슐 기반 휴머노이드를 사용하여 수행합니다.기준선과의 공정한 비교를 위해 모든 평가에 중립적 성별을 가진 SMPL의 평균 신체 모양을 사용하고 모양 변화에 대한 정성적 결과를 보여줍니다.두 유형의 휴머노이드 모두 신체가 올바른 무게(평균 70kg)를 갖도록 기하학적 모양의 밀도를 조절합니다.모든 관절 간 충돌은 부모와 자식 관절 사이를 제외한 모든 관절 쌍에 대해 활성화됩니다.휴머노이드 간의 충돌은 원하는 대로 활성화 및 비활성화할 수 있습니다(다중 사용자 사용 사례의 경우).훈련 프로세스. 훈련하는 동안, 우리는 현재 훈련 세트 Q(k)에서 무작위로 동작을 샘플링하고 Ô1:T를 사용하여 순방향 운동학을 수행하여 시뮬레이션된 신체 모양에 대해 정규화합니다. UHC[22]와 유사하게, 우리는 에피소드 시작 시 각 휴머노이드의 발이 땅에 닿도록 루트 변환의 높이를 조정합니다. 우리는 모든 기본형과 작곡가를 훈련하기 위해 1536개의 휴머노이드를 병렬로 시뮬레이션합니다. 약 10억 개의 샘플을 수집하는 데 훈련에는 약 7일이 걸립니다. 다양한 신체 모양으로 훈련할 때, 우리는 AMASS 데이터 세트에서 유효한 인간 신체 모양을 무작위로 샘플링하고 이를 통해 휴머노이드를 구성합니다. 훈련 중에 사용된 하이퍼파라미터는 표.데이터 준비에서 찾을 수 있습니다. 우리는 인간 객체 상호 작용이 포함된 AMASS 시퀀스를 필터링하기 위해 UHC[22]와 유사한 절차를 따릅니다. 의자에 앉는 것, 러닝머신에서 움직이는 것, 테이블에 기대는 것, 계단을 밟는 것, 공중에 떠 있는 것 등을 모두 제거하여 11313개의 고품질 동작 시퀀스를 훈련용으로, 140개의 시퀀스를 테스트용으로 만들었습니다. 앉은 동작에 해당하는 신체 관절 구성을 식별하거나 연속적인 공중 프레임의 수를 세는 것에 기반한 휴리스틱 기반 필터링 프로세스를 사용합니다. 런타임. 훈련이 완료되면 PHC는 시뮬레이션 및 렌더링과 함께 실시간(~32FPS)으로 실행할 수 있으며 렌더링 없이 실행할 때는 약(~50FPS)으로 실행할 수 있습니다. 표 6은 사용된 기본 요소 수, 아키텍처 및 휴머노이드 유형에 대한 방법의 런타임을 보여줍니다. 모델 크기. 최종 모델 크기(4개의 기본 요소 포함)는 28.MB로 UHC의 모델 크기(30.4MB)와 비슷합니다. B.2. 비디오에서 실시간 물리 기반 가상 아바타를 만드는 실시간 사용 사례. 비디오로 구동되는 실시간 물리 기반 아바타를 구현하기 위해 먼저 Yolov8[17]을 사용하여 사람을 감지합니다. 포즈 추정을 위해 MeTRABS[41]와 HybrIK[19]를 사용하여 3D 주요 포인트 pt와 회전 t를 제공합니다. MeTRAbs는 다음을 계산하는 3D 주요 포인트 추정기입니다. 표 5: PHC에 대한 하이퍼 매개변수. σ: 정책에 대한 고정 분산. : 할인 계수. e: PPO 상태에 대한 클립 범위: St 배치 크기 학습률 기본: P(1) 기본: P(2) 기본: p(K) 기본: p(F) σ Υ Є 작성자: C h(D) h(2) h(K) h 값5 × 2-0.05 0.0.h) h(c) h MLP h(2) h(K) MLP MLP h(F) MLP 값 Wjp 0.Wjr Wjv Wjw MLP 0.0.0.3D 관절 위치 pt는 절대 글로벌 공간(상대 루트 공간이 아님)에 있습니다. HybrIK는 인간 메시 복구를 위한 최신 방법으로, SMPL 인체에 대한 관절 각도 ỗt와 루트 위치 p를 계산합니다. 순운동학을 사용하여 관절 각도 ết와 루트 위치 p에서 3D 키포인트 pt를 복구할 수 있습니다. 이 두 가지 방법은 모두 인과적이고, 시간 정보를 사용하지 않으며, 실시간으로 실행할 수 있습니다(~ 30FPS). 이미지 픽셀에서 3D 키포인트 위치를 추정하는 것은 관절 각도를 회귀하는 것보다 쉬운 작업인데, 3D 키포인트는 픽셀에서 학습한 피처와 더 잘 연관될 수 있기 때문입니다. 따라서 HybrIK와 MeTRAbs는 모두 3D 키포인트 pt를 추정하는데, HybrIK에는 관절 각도 Ot를 복구하기 위해 학습된 역 운동학을 수행하는 추가 단계가 포함됩니다. 우리는 키포인트 기반 컨트롤러와 함께 MeTRAbs를 사용하고 회전 기반 컨트롤러와 함께 HybrIK를 사용하여 이 두 가지 기성품 포즈 추정 방법을 사용하여 결과를 보여줍니다. 경험적으로, MeTRAbs는 키포인트 전용 공식화로 인해 더 안정적이고 정확한 3D 키포인트를 추정한다는 것을 발견했습니다. 또한 여러 아바타를 구동하고 인간형 간 충돌을 가능하게 하는 실시간 다중 사람 물리 기반 인간 대 인간 상호 작용 사용 사례를 제시합니다. 다중 인물 포즈 추정을 지원하기 위해 OCSort[5]를 사용하여 개별 트랙릿을 추적하고 포즈를 각 사람과 연관시킵니다. 실시간 사용 사례는 오프라인 처리보다 추가적인 과제를 제기합니다. 감지, 포즈/키포인트 추정 및 시뮬레이션은 모두 약 30FPS에서 실시간으로 실행해야 하며 프레임 속도의 작은 변동으로 인해 모방 및 시뮬레이션이 불안정해질 수 있습니다. 노이즈가 있는 깊이 추정치를 부드럽게 하기 위해 가우시안 필터를 사용하여 t-120에서 t까지의 추정치를 부드럽게 하고 경계 패딩에 &quot;미러&quot; 설정을 사용합니다. 언어의 가상 아바타. 언어 기반 동작 생성을 위해 MDM[43]을 텍스트-동작 모델로 채택합니다. 기본적으로 3D 키포인트 pt를 생성하고 키포인트 기반 모방자에 연결하는 공식 구현을 사용합니다. MDM은 고정 길이의 동작 클립을 생성하므로 생성된 동작의 여러 클립을 결합하려면 추가 블렌딩이 필요합니다. 그러나 PHC는 자연스럽게 멀리 떨어진 참조 동작으로 이동할 수 있고 동작 클립 간의 분리된 동작을 처리할 수 있으므로 MDM에서 생성된 여러 동작 클립을 순진하게 연결하고 여러 텍스트 프롬프트에서 일관되고 물리적으로 유효한 동작을 만들 수 있습니다. 이를 통해 연속적인 텍스트 프롬프트 스트림으로 구동할 수 있는 시뮬레이션된 아바타를 만들 수 있습니다. B.3. 진행형 신경망(PNN) 세부 정보 PNN[36]은 전체 데이터 집합 Q에서 학습된 단일 원시 네트워크 P(¹)로 시작합니다. 모방 작업을 사용하여 P(1)이 전체 동작 데이터 집합 Q에서 수렴하도록 학습되면 Q에서 P(¹)을 평가하여 하드 동작의 하위 집합을 생성합니다. P(1)이 실패하는 시퀀스는 (¹)를 형성합니다. 그런 다음 P(1)의 매개변수를 하드⚫로 고정합니다. 작성자: C h(C) MLP 1:(K+1) -가중치: w 게이팅 함수 동작: a₁ 상태: St 원시: P(1) 원시: P(2) 원시: p(K) 원시: p(F) h(1) h(2) h(K)h(2) h(K) h hQF) MLP MLP MLP MLP: MLP 층 -가중치: w ,1:(K+1) 게이팅 함수: 가중치 공유: 측면 연결 동작: at 그림 6: 진행형 신경망 아키텍처. 위: 측면 연결이 있는 PNN. 아래: 가중치 공유가 있는 PNN. h()는 j번째 기본형의 ¿번째 계층의 숨겨진 활성화를 나타냅니다. 그리고 P(¹)의 각 계층을 P(2)에 연결하는 측면 연결과 함께 새로운 기본형 P(2)(무작위로 초기화됨)을 생성합니다. 계층 가중치 W,(k), 활성화 함수 f, 학습 가능한 측면 연결 가중치 U(j:k)가 주어지면 k번째 기본형의 i번째 계층의 숨겨진 활성화 h)를 다음과 같이 얻습니다. (k) h(k) 1(k) h ( 1 + ΣU (jk) h (² = (P+R). Συ j <k (7) Fig.6 visualizes the PNN with the lateral connection architecture. Essentially, except for the first layer, each subsequent layer receives the activation of the previous layer processed by the learnable connection matrices U(jk). We do not use any adapter layer as in the original paper. As an alternative to lateral connection, we explore weight sharing and warm-starts the primitive with the weights from the previous one (as opposed to randomly initialized). We find both methods equally effective (see Sec.C) when trained with the same hard-negative mining procedure, as each newly learned primitive adds new sequences that PHC can imitate. The weight sharing strategy significantly decreases training time as the policy starts learning harder sequences with basic motor skills. We use weight sharing in all our main experiments. Epoch2000022500-27500300000 25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 500Motion Index Figure 7: Here we plot the motion indexes that the policy fails on over training time; we only plot the 529 sequences that the policy has failed on over these training epoches. A white pixel denotes that sequence is can be successfully imitated at the given epoch, and a black pixel denotes an unsuccessful imitation. We can see that while there are 30 sequences that the policy consistently fails on, the remaining can be learned and then forgotten as training progresses. The staircase pattern indicates that the policy fails on new sequences each time it learns new ones. C. Supplementary Results C.1. Categorizing the Forgetting Problem As mentioned in the main paper, one of the main issues in learning to mimic a large motion dataset is the forgetting problem. The policy will learn new sequences while forgetting the ones already learned. In Fig.7, we visualize the sequences that the policy fails to imitate during training. Starting from the 12.5k epoch, each evaluation shows that some sequences are learned, but the policy will fail on some already learned sequences. The staircase pattern indicates that when learning sequences failed previously, the policy forgets already learned sequences. Numerically, each evaluation has around 30% overlap of failed sequences (right end side). The 30% overlap contains the backflips, cartwheeling, and acrobatics; motions that the policy consistently fails to learn when trained together with other sequences. We hypothesize that these remaining sequences (around 40) may require additional sequence-level information for the policy to learn properly together with other sequences. Fail-state recovery Learning the fail-state recovery task can also lead to forgetting previously learned imitation skills. To verify this, we evaluate P(F) on the H36M-Test-Video dataset, which leads to a performance of Succ: 42.5%, Eg-mpjpe: 87.3, and Empipe: 55.9, which is much lower than the single primitive P(¹) performance of Succ: 59.4%, Eg-mpjpe: 60.2, and Empipe: 34.4. Thus, learning the fail-state recovery task may lead to severe forgetting of the imitation task, motivating our PMCP framework to learn separate primitives for imitation and fail-state recovery. C.2. Additional Ablations In this section, we provide additional ablations of the components of our framework. Specifically, we study the effect of MOE vs. MCP, lateral connection vs. weight sharing, and the number of primitives used. We also report the inference speed (counting network inference and simulation time). All experiments are carried out with the rotation-based imitator and incorporate the fail state recovery primitive P(F) as the last primitive. PNN Lateral Connection vs. Weight Sharing. As can be seen in Table 6, comparing Row 1 (R1) and R7, we can see that PNN with lateral connection and weight sharing produce similar perTable 6: Supplmentary ablation on components of our pipeline, performed using noisy pose estimate from HybrIK + Metrabs (root) on the H36M-Test-Video* data. MOE: top-1 mixture of experts. MCP: multiplicative control policy. PNN: progressive neural networks. Type: between Cap (capsule) and mesh-based humanoids. All models are trained with the same procedure. PNN-Lateral FPS H36M-Test-Video* PNN-Weight MOE MCP Туре # Prim SuccEg-mpjpe↓ Empipe↓ ✓ X x x Cap87.5% 55.36.2X ✓ ✓ Cap87.5% 56.34.3X ✓ X ✓ Mesh 4 86.9% 62.39.5X X x x Cap59.4% 60.37.2✓ Cap65.6% 58.37.3X ✓ x x Cap80.9% 56.36.1✓ Cap 4 88.7% 55.34.Cap 5 87.5% 57.36.X formance, both in terms of motion imitation and inference speed. This shows that in our setup, the weight sharing scheme is an effective alternative to lateral connections. This can be explained by the fact that in our case, each “task” on which the primitives are trained is similar and does not require lateral connection to choose whether to utilize prior experiences or not. MOE vs. MCP. The difference between the top-1 mixture of experts (MOE) and multiplicative control (MCP) is discussed in detail in the MCP paper [33]: top-1 MOE only activates one expert at a time, while MCP can activate all primitives at the same time. Comparing R2 and R7, as expected, we can see that top-MOE is slightly inferior to MCP. Since all of our primitives are pretrained and frozen, theoretically a perfect composer should be able to choose the best primitive based on input for both MCP and MOE. MCP, compared to MOE, can activate all primitives at once and search a large action space where multiple primitives can be combined. Thus, MCP provides better performance, while MOE is not far behind. This is also observed by CoMic[13], where they observe similar performance between mixture and product distributions when used to combine subnetworks. Note that top-inf MOE is similar to MCP where all primitives can be activated. Capsule vs. Mesh Humanoid. Comparing R3 and R7, we can see that mesh-based humanoid yield similar performance to capsulebased ones. It does slow down simulation by a small amount (FPS vs. 32 FPS), as simulating mesh is more compute-intensive than simulating simple geometries like capsules. Number of primitives. Comparing R4, R5, R6, R7, and R8, we can see that the performance increases as the number of primitives increases. Since the last primitive P (F) is for fail-state recovery and does not provide motion imitation improvement, Ris similar to the performance of models trained without PMCP (R4). As the number of primitives grows from 2 to 3, we can see that the model performance grows quickly, showing that MCP is effective in combining pretrained primitives to achieve motion imitation. Since we are using relatively small networks, the inference speed does not change significantly with the number of primitives used. We notice that as the number of primitives grows, Q(k) becomes more and more challenging. For instance, Q (4) contains mainly highly dynamic motions such as high-jumping, back flipping, and cartwheeling, which are increasingly difficult to learn together. We show that (see supplementary webpage) we can overfit these sequences by training on them only, yet it is significantly more challenging to learn them together. Motions that are highly dynamic require very specific steps to perform (such as moving while airborne to prepare for landing). Thus, the experiences collected when learning these sequences together may contradict each other: for example, a high jump may require a high speed running up, while a cartwheel may require a different setup of foot-movement. A per-frame policy that does not have sequence-level information may find it difficult to learn these sequences together. Thus, sequence-level or information about the future may be required to learn these high dynamic motions together. In general, we find that using 4 primitives is most effective in terms of training time and performance, so for our main evaluation and visualizations, we use 4-primitive models. D. Extended Limitation and Discussions Limitation and Failure Cases. As discussed in the main paper, PHC has yet to achieve 100% success rate on the AMASS training set. With a 98.9% success rate, PHC can imitate most of our daily motion without losing balance, but can still struggle to perform more dynamic motions, such as backflipping. For our real-time avatar use cases, we can see a noticeable degradation in performance from the offline counterparts. This is due to the following: • Discontinuity and noise in reference motion. The inherent ambiguity in monocular depth estimation can result in noisy and jittery 3D keypoints, particularly in the depth dimension. These small errors, though sometimes imperceptible to the human eye, may provide PHC with incorrect movement signals, leaving insufficient time for appropriate reactions. Velocity estimation is also especially challenging in real-time use cases, and PHC relies on stable velocity estimation to infer movement cues. • Mismatched framerate. Since our PHC assumes 30 FPS motion input, it is essential for pose estimates from video to match for a more stable imitation. However, few pose estimators are designed to perform real-time pose estimation (> 30 FPS), 추정 프레임 속도는 컴퓨터의 부하 균형과 같은 외부적인 이유로 인해 변동될 수 있습니다.• 여러 사람이 사용하는 경우 추적 및 ID 전환이 여전히 발생할 수 있으며, 이로 인해 휴머노이드가 자리를 바꿔야 하는 충격적인 경험이 발생할 수 있습니다.포즈 추정기와 컨트롤러 간의 보다 심층적인 통합이 실시간 사용 사례를 더욱 개선하는 데 필요합니다.카메라 포즈를 명시적으로 고려하지 않으므로 웹캠이 지면과 수평이고 피치나 롤이 없다고 가정합니다.카메라 높이는 세션 시작 시 수동으로 조정합니다.포즈 추정 단계에서 카메라의 포즈를 고려할 수 있습니다.또 다른 개선 영역은 실패 상태 복구 중의 자연스러움입니다.컨트롤러는 인간과 같은 방식으로 실패 상태에서 복구하고 다시 걸어가 모방을 재개할 수 있지만 속도와 자연스러움은 더욱 개선할 수 있습니다.실패 상태 복구 중의 걷는 걸음걸이, 속도 및 템포는 AMP[35]에서 알려진 아티팩트인 비대칭 동작과 같은 눈에 띄는 아티팩트를 보입니다. 실패 상태 복구와 동작 모방 사이의 전환 중에 휴머노이드는 갑자기 흔들리고 동작 모방으로 전환될 수 있습니다. 추가 조사(예: 지점-목표 공식보다 더 나은 보상, 궤적에 대한 추가 관찰)가 필요합니다. 논의 및 향후 작업. 우리는 영구 휴머노이드 컨트롤러를 제안합니다. 이는 높은 충실도로 방대한 동작 코퍼스를 모방할 수 있는 휴머노이드 동작 모방기입니다. 실패 상태에서 복구하고 동작 모방으로 돌아갈 수 있는 기능과 함께 PHC는 예상치 못한 이벤트 중에 더 이상 재설정이 필요하지 않은 시뮬레이션된 아바타 사용 사례에 이상적입니다. PHC를 실시간 포즈 추정기와 페어링하여 시뮬레이션된 아바타가 재설정 없이도 액터가 수행하는 동작을 영구적으로 모방하는 비디오 기반 아바타 사용 사례에서 사용할 수 있음을 보여줍니다. 이를 통해 물리적으로 사실적인 인간 대 인간 상호 작용을 가능하게 할 수 있는 미래의 가상 원격 존재 및 원격 작업을 강화할 수 있습니다. 또한 PHC를 언어 기반 동작 생성기에 연결하여 텍스트에서 생성된 동작을 모방하는 기능을 보여줍니다. PHC는 모션 인비트위닝을 수행하여 여러 클립을 모방할 수 있습니다. 이러한 기능을 갖춘 구현된 에이전트의 향후 작업은 자연어 프로세서와 페어링되어 복잡한 작업을 수행할 수 있습니다. 제안된 PMCP는 점진적 RL 및 멀티태스크 학습을 가능하게 하는 일반 프레임워크로 사용할 수 있습니다. 또한, 모방을 위한 모션 입력으로 3D 키포인트만 사용할 수 있으므로 관절 회전을 추정할 필요가 없습니다. 기본적으로 PHC를 사용하여 입력 3D 키포인트를 기반으로 역 운동학을 수행하고 물리 법칙을 활용하여 출력을 조절합니다. PHC는 구현된 에이전트 및 접지와 같은 다른 영역에서도 사용할 수 있으며, 이러한 영역에서는 고수준 추론 기능을 위한 저수준 컨트롤러 역할을 할 수 있다고 생각합니다.
