--- ABSTRACT ---
음악 생성은 심층 생성 모델의 발전으로 점점 더 많은 관심을 받고 있습니다. 그러나 텍스트 설명에 따라 음악을 생성하는 텍스트-음악 생성은 음악 구조의 복잡성과 높은 샘플링 속도 요구 사항으로 인해 여전히 어려운 과제입니다. 과제의 중요성에도 불구하고, 기존의 생성 모델은 음악 품질, 계산 효율성 및 일반화에 한계가 있습니다. 이 논문에서는 텍스트-음악 생성을 위한 범용 고충실도 모델인 JEN-1을 소개합니다. JEN-1은 자기 회귀 및 비자기 회귀 학습을 모두 통합한 확산 모델입니다. JEN-1은 맥락 내 학습을 통해 텍스트 안내 음악 생성, 음악 인페인팅 및 연속을 포함한 다양한 생성 과제를 수행합니다. 평가 결과, 계산 효율성을 유지하면서 텍스트-음악 정렬 및 음악 품질에서 최첨단 방법보다 JEN-1의 성능이 우수함을 보여줍니다. 데모는 https://www.futureverse.com/research/jen/demos/jen1에서 제공됩니다. &quot;음악은 인류의 보편적 언어입니다.&quot; - 헨리 워즈워스 롱펠로우
--- INTRODUCTION ---
음악은 하모니, 멜로디, 리듬을 포함하는 예술적 표현으로서 큰 문화적 의미를 지니고 있으며 인간에게 어필합니다. 최근 몇 년 동안 심층적 생성 모델(Liu et al., 2023; Kreuk et al., 2022; Agostinelli et al., 2023)의 등장으로 음악 생성에서 놀라운 진전이 있었습니다. 그러나 고충실도의 사실적인 음악을 생성하는 것은 여전히 다른 방식에 비해 고유한 과제를 안고 있습니다. 첫째, 음악은 전체 주파수 스펙트럼을 활용하여 복잡한 부분을 포착하기 위해 44.1KHz 스테레오와 같은 높은 샘플링 속도가 필요합니다. 이는 언어적 내용에 초점을 맞추고 낮은 샘플링 속도(예: 16kHz)를 사용하는 음성과 대조적입니다. 둘째, 여러 악기의 혼합과 멜로디와 하모니의 배열은 매우 복잡한 구조를 만들어냅니다. 인간은 음악적 불협화음에 민감하기 때문에 음악 생성은 불완전함에 대한 여지를 거의 허용하지 않습니다. 가장 중요한 것은 키, 장르, 멜로디와 같은 속성에 대한 제어 가능성이 창작자가 예술적 비전을 실현하는 데 필수적이라는 것입니다. 텍스트와 음악의 교차점인 텍스트-음악 생성은 자유형 텍스트 설명과 음악 작곡을 연결하는 귀중한 기능을 제공합니다. 그러나 기존의 텍스트-음악 모델은 여전히 주목할 만한 한계를 보입니다. 표 1에서 볼 수 있듯이 일부 모델(Liu et al., 2023; Ghosal et al., 2023)은 스펙트로그램 표현에서 작동하여 오디오 변환으로 인한 충실도 손실을 초래합니다. 다른 모델은 비효율적인 자기 회귀 생성 또는 계단식 모델을 사용합니다(Agostinelli et al., 2023; Copet et al., 2023; Huang et al., 2023a). 더 제한적으로, 그들의 훈련 목표는 단일 작업에 국한되어 음악을 자유롭게 조작할 수 있는 인간의 다재다능함이 부족합니다. 이러한 한계를 극복하기 위해 효율성, 품질 및 제어 가능성을 결합한 텍스트-음악 모델인 JEN-1을 소개합니다. 첫째, JEN-1은 마스크 자동 인코더와 확산 모델을 사용하여 고충실도 48kHz 스테레오 오디오를 직접 생성하여 스펙트로그램 변환 손실을 우회합니다.둘째, 텍스트-음악, 인페인팅 및 연속에 대한 멀티태스크 학습은 모델의 다양성을 향상시킵니다.사전 인쇄.작업 모델 데이터 표 1: 최첨단 음악 생성 모델 간 비교.기능 MusicLM MusicGen 높은 샘플 속도 ☑ 2채널 스테레오 파형 자기 회귀 비자기 회귀 ✓ 비캐스케이드 모델 단일 작업 학습 멀티태스크 학습 ☑ AudioLDM Noise2Music JEN-1(저희) ☑ X ☑ ✓ ✓ 셋째, JEN-1은 자기 회귀 및 비자기 회귀 확산을 통합하여 종속성 모델링과 생성 효율성의 균형을 맞춥니다.저희는 객관적인 지표와 인간 평가에 걸쳐 최첨단 기준선과 JEN-1을 광범위하게 평가합니다. 결과는 JEN-1이 현재 가장 좋은 방법(83.8/100)에 비해 지각적으로 더 높은 품질(85.7/100)의 음악을 생성한다는 것을 보여줍니다. 절제는 각 기술 구성 요소의 효능을 검증합니다. 더 중요한 것은 인간 심사위원이 JEN-1이 멜로디와 화성이 즐거운 방식으로 텍스트 프롬프트와 매우 일치하는 음악을 생성한다는 것을 확인했다는 것입니다. 요약하면, 이 연구의 주요 기여는 다음과 같습니다. 1. 우리는 JEN-1을 어려운 텍스트-음악 생성 과제에 대한 솔루션으로 제안합니다. JEN1은 맥락 내 학습을 사용하고 다중 작업 목표로 훈련되어 단일 모델 내에서 음악 생성, 음악 연속 및 음악 인페인팅을 가능하게 합니다. 2. JEN-1은 스펙트로그램과 관련된 변환 손실을 피하면서 파형을 직접 모델링하여 매우 효율적인 접근 방식을 활용합니다. 마스크 자동 인코더와 확산 모델을 통합하여 48kHz 샘플링 속도에서 고품질 음악을 생성합니다. 3. 저희 JEN-1 모델은 자기회귀 확산 모드와 비자기회귀 모드를 모두 통합하여 순차적 종속성을 개선하고 동시에 시퀀스 생성을 향상시킵니다. 그 결과, 텍스트 설명을 준수하면서도 높은 충실도를 유지하는 멜로디적으로 정렬된 음악이 탄생합니다. 4. 저희 논문은 텍스트-음악 생성 분야에서 상당한 진전을 보여주며, 텍스트 프롬프트와 멜로디 구조에 맞춰 고품질 음악을 생성하기 위한 강력하고 효율적이며 제어 가능한 프레임워크를 제공합니다. 저희는 객관적이고 인간의 판단을 포함하는 포괄적인 평가를 수행하여 저희 방법의 근간이 되는 중요한 디자인 선택을 철저히 평가합니다. 2
--- RELATED WORK ---
이 섹션에서는 음악 생성 분야의 기존 문헌에 대한 개요를 제공하며 세 가지 주요 영역, 즉 단일 작업 대 다중 작업 훈련, 파형 대 스펙트럼 기반에 초점을 맞춥니다.
--- METHOD ---
s는 계산 효율성을 유지하면서 텍스트-음악 정렬 및 음악 품질을 향상시킵니다. 데모는 https://www.futureverse.com/research/jen/demos/jen1에서 제공됩니다. &quot;음악은 인류의 보편적 언어입니다.&quot; - 헨리 워즈워스 롱펠로우 서론 하모니, 멜로디, 리듬을 포함하는 예술적 표현으로서 음악은 큰 문화적 의미를 지니고 있으며 인간에게 어필합니다. 최근 몇 년 동안 심층 생성 모델(Liu et al., 2023; Kreuk et al., 2022; Agostinelli et al., 2023)의 등장으로 음악 생성이 눈에 띄게 발전했습니다. 그러나 고충실도의 사실적인 음악을 생성하는 것은 여전히 다른 모드에 비해 고유한 과제를 안고 있습니다. 첫째, 음악은 전체 주파수 스펙트럼을 활용하여 복잡한 내용을 포착하기 위해 44.1KHz 스테레오와 같은 높은 샘플링 속도가 필요합니다. 이는 언어적 내용에 초점을 맞추고 낮은 샘플링 속도(예: 16kHz)를 사용하는 음성과는 대조적입니다. 둘째, 여러 악기의 혼합과 멜로디와 하모니의 배열은 매우 복잡한 구조를 초래합니다. 인간은 음악적 불협화음에 민감하기 때문에 음악 생성은 불완전함에 대한 여지를 거의 허용하지 않습니다. 가장 중요한 것은 키, 장르 및 멜로디와 같은 속성에 대한 제어 가능성이 창작자가 예술적 비전을 실현하는 데 필수적이라는 것입니다. 텍스트와 음악의 교차점인 텍스트-음악 생성은 자유형 텍스트 설명과 음악 작곡을 연결하는 귀중한 기능을 제공합니다. 그러나 기존의 텍스트-음악 모델은 여전히 주목할 만한 한계를 보입니다. 표 1에서 볼 수 있듯이 일부 모델(Liu et al., 2023; Ghosal et al., 2023)은 스펙트로그램 표현에서 작동하여 오디오 변환으로 인한 충실도 손실을 초래합니다. 다른 모델은 비효율적인 자기 회귀 생성 또는 계단식 모델을 사용합니다(Agostinelli et al., 2023; Copet et al., 2023; Huang et al., 2023a). 더 제한적으로, 그들의 훈련 목표는 단일 작업에 국한되어 음악을 자유롭게 조작할 수 있는 인간의 다재다능함이 부족합니다. 이러한 한계를 극복하기 위해 효율성, 품질 및 제어 가능성을 결합한 텍스트-음악 모델인 JEN-1을 소개합니다. 첫째, JEN-1은 마스크 자동 인코더와 확산 모델을 사용하여 고충실도 48kHz 스테레오 오디오를 직접 생성하여 스펙트로그램 변환 손실을 우회합니다. 둘째, 텍스트-음악, 인페인팅 및 연속에 대한 다중 작업 훈련은 모델의 다재다능함을 향상시킵니다. 사전 인쇄. 작업 모델 데이터 표 1: 최신 음악 생성 모델 간 비교. 기능 MusicLM MusicGen 높은 샘플 속도 ☑ 2채널 스테레오 파형 자기 회귀 비자기 회귀 ✓ 비캐스케이드 모델 단일 작업 학습 다중 작업 학습 ☑ AudioLDM Noise2Music JEN-1(저희) ☑ X ☑ ✓ ✓ 셋째, JEN-1은 자기 회귀 및 비자기 회귀 확산을 통합하여 종속성 모델링과 생성 효율성을 균형 있게 조정합니다. 저희는 객관적 지표와 인간 평가에 걸쳐 최첨단 기준선에 대해 JEN-1을 광범위하게 평가합니다. 결과에 따르면 JEN-1은 현재 가장 우수한 방법(83.8/100)에 비해 지각적으로 더 높은 품질의 음악(85.7/100)을 생성합니다. 절제는 각 기술 구성 요소의 효능을 검증합니다. 더 중요한 것은 인간 심사위원이 JEN-1이 멜로디와 화성이 즐거운 방식으로 텍스트 프롬프트와 매우 일치하는 음악을 생성한다는 것을 확인했다는 것입니다. 요약하자면, 이 연구의 주요 기여는 다음과 같습니다. 1. 우리는 JEN-1을 까다로운 텍스트-음악 생성 과제에 대한 솔루션으로 제안합니다. JEN1은 맥락 내 학습을 사용하고 다중 작업 목표로 훈련되어 단일 모델 내에서 음악 생성, 음악 연속 및 음악 인페인팅을 가능하게 합니다. 2. JEN-1은 스펙트로그램과 관련된 변환 손실을 피하면서 파형을 직접 모델링하여 매우 효율적인 접근 방식을 활용합니다. 마스크 자동 인코더와 확산 모델을 통합하여 48kHz 샘플링 속도에서 고품질 음악을 생성합니다. 3. 우리의 JEN-1 모델은 순차적 종속성을 개선하고 동시에 시퀀스 생성을 향상시키기 위해 자기 회귀 확산 모드와 비자기 회귀 모드를 모두 통합합니다. 그 결과 높은 충실도를 유지하면서 텍스트 설명을 준수하는 멜로디적으로 정렬된 음악이 생성됩니다. 4. 저희 논문은 텍스트-음악 생성 분야에서 상당한 진전을 보여주며, 텍스트 프롬프트와 멜로디 구조에 맞춰 고품질 음악을 생성하기 위한 강력하고 효율적이며 제어 가능한 프레임워크를 제공합니다. 저희는 저희 방법의 근간이 되는 중요한 설계 선택을 철저히 평가하기 위해 객관적이고 인간의 판단을 포함하는 포괄적인 평가를 수행합니다. 2 관련 연구 이 섹션에서는 음악 생성 분야의 기존 문헌에 대한 개요를 제공하며, 세 가지 주요 영역에 초점을 맞춥니다. 단일 작업 대 다중 작업 훈련, 파형 대 스펙트럼 기반 방법, 자기 회귀 대 비자기 회귀 생성 모델. 단일 작업 대 다중 작업. 조건부 신경 음악 생성은 광범위한 응용 프로그램을 포함합니다. 조절 신호의 특성에 따라 이러한 작업은 두 가지 유형으로 분류할 수 있습니다. 한 유형은 오디오 출력에 대한 엄격한 시간적 정렬을 가진 저수준 제어 신호를 사용합니다. 여기에는 가사 조건 음악 생성(Yu et al., 2021)과 MIDI 시퀀스에서 오디오 합성(Muhamed et al., 2021)이 포함됩니다. 다른 유형은 텍스트(Kreuk et al., 2022; Yang et al., 2023) 또는 이미지(Huang et al., 2023b)와 같은 고급 의미 설명을 조건 신호로 활용하는데, 여기서 조건은 엄격한 시간적 정렬보다는 전반적인 일관성과 일관성을 제공합니다. 그러나 실제 응용 프로그램에서 이러한<conditional signal, audio> 쌍은 종종 희소합니다. 따라서 모델은 일반적으로 오디오 인페인팅(Marafioti et al., 2019) 및 연속(Borsos et al., 2023)과 같은 자기 감독 기술을 사용하여 레이블이 지정되지 않은 오디오 데이터 세트에서 일반화를 강화하는 데 사용됩니다. 저희 작업에서는 텍스트-음악 생성을 위한 정렬된 쌍을 사용하여 멀티태스크 학습을 탐색합니다. Preprint. 자기 감독 인페인팅 및 연속 작업을 위한 오디오 전용 데이터와 함께. 이를 통해 노이즈 견고성이 향상되고 고수준 의미 설명과 저수준 제어 신호 모두에 기반한 음악 생성이 가능합니다. 파형 대 스펙트럼. 계산 효율성을 고려할 때 파형 신호의 복잡성이 높기 때문에 원시 오디오 파형을 모델 입력 또는 생성 대상으로 사용하는 것은 매우 어렵습니다(Gârbacea et al., 2019). 파형의 특징 추출 및 이산 표현은 음악 생성 작업에서 중요한 전처리 단계가 되며, 이는 두 가지 주요 접근 방식으로 분류할 수 있습니다. 한 가지 접근 방식은 먼저 파형을 멜 스펙트로그램으로 변환한 다음 벡터 양자화 변형 자동 인코더(VQ-VAE)(Van Den Oord et al., 2017) 또는 생성적 적대 네트워크(GANS)(Creswell et al., 2018)와 같은 방법을 사용하여 이미지 처리에 대한 컴퓨터 비전의 기술을 참조하여 처리합니다.Diffwave(Kong et al., 2020b) 및 Diffsound(Yang et al., 2023)와 같은 일반적인 기술은 먼저 텍스트 태그 또는 기타 조건부 신호를 스펙트로그램 디코더에 공급하여 멜 스펙트로그램 토큰을 생성합니다.그런 다음 이러한 토큰을 사전 훈련된 오디오 VQ-VAE에 공급하여 멜 스펙트로그램을 합성하고, 마지막으로 HiFi-GAN(Kong et al., 2020a)과 같은 보코더를 통해 오디오 파형으로 변환합니다. 다른 접근 방식은 양자화 기반 오디오 코덱을 활용하여 연속적인 파형 신호를 토큰화하여 보다 컴팩트하고 압축적이며 불연속적인 표현을 제공합니다. 예를 들어, SoundStream(Zeghidour et al., 2021)과 EnCodec(Défossez et al., 2022)은 높은 재구성 품질을 유지하면서 일반 오디오를 고도로 압축할 수 있는 범용 신경 오디오 코덱입니다. 예를 들어, MusicGen(Copet et al., 2023)은 텍스트 또는 멜로디 표현을 조건으로 EnCodec 오디오 토크나이저(Défossez et al., 2022)의 양자화된 단위 위에 변압기 기반 디코더를 배치합니다. AudioLM(Borsos et al., 2023)과 AudioPaLM(Rubenstein et al., 2023)은 텍스트를 입력으로 받아 디코더 전용 변환기를 통해 오디오 토큰으로 디코딩한 다음, 이 토큰을 SoundStream(Zeghidour et al., 2021)을 사용하여 원시 오디오로 다시 변환합니다. 자기 회귀 대 비 자기 회귀. 자연어 처리에서 문장 생성에서 영감을 얻어 오디오 토큰화 후 자기 회귀 또는 비 자기 회귀 접근 방식을 통해 음악을 생성할 수 있습니다. 구체적으로 PerceiverAR(Hawthorne et al., 2022), AudioGen(Kreuk et al., 2022), MusicLM(Agostinelli et al., 2023), Jukebox(Dhariwal et al., 2020)와 같은 방법은 변환기 기반(Vaswani et al., 2017) 디코더 전용 모델을 사용하여 음악 시퀀스에서 오디오 토큰을 자기 회귀적으로 생성합니다. 이러한 자기회귀 모델은 각 토큰 생성이 이전 컨텍스트에 따라 조건화되므로 매우 일관된 오디오를 생성할 수 있습니다. 그러나 순차적인 토큰별 생성 방식은 본질적으로 생성과 추론 모두에 속도를 희생하여 다운스트림 작업에서 이러한 기술의 적용 가능성을 제한합니다. 반면 비자기회귀 모델은 여러 토큰을 동시에 생성하여 생성 프로세스를 크게 가속화할 수 있습니다. 추론 중에 상당한 속도 이점을 누리는 비자기회귀 음악 생성 모델은 점점 최첨단이 되고 있으며 이 분야에서 두드러진 연구 방향이 되고 있습니다. 최근 확산 모델(Ho et al., 2020)을 기반으로 하는 비자기회귀 생성이 유망한 전선으로 부상했습니다. 확산 모델은 랜덤 노이즈를 점진적으로 제거하여 고충실도 오디오를 합성하는 잠재 표현을 얻습니다. Make-An-Audio(Huang 등, 2023b), Noise2Music(Huang 등, 2023a), AudioLDM(Liu 등, 2023), TANGO(Ghosal 등, 2023)와 같은 혁신은 잠재 확산 모델(LDM)(Rombach 등, 2022)을 활용하여 샘플 품질을 유지하면서 가속화된 음악 생성을 달성합니다.3 예비 3. 조건부 생성 모델 콘텐츠 합성 분야에서 조건부 생성 모델을 구현하려면 종종 자기 회귀(AR)(Agostinelli 등, 2023; Copet 등, 2023) 또는 비상동 회귀(NAR)(Liu 등, 2023; Ghosal 등, 2023) 패러다임을 적용해야 합니다. 각 단어가 별개의 토큰으로 기능하고 문장이 이러한 토큰에서 순차적으로 구성되는 언어의 고유한 구조는 AR 패러다임을 언어 모델링에 더 자연스러운 선택으로 만듭니다. 따라서 자연어 처리(NLP) 분야에서 GPT 시리즈와 같은 트랜스포머 기반 모델이 텍스트 생성 작업을 위한 주요 접근 방식으로 부상했습니다. AR 방법(Agostinelli et al., 2023; Copet et al., 2023)은 가시적인 기록 토큰을 기반으로 미래 토큰을 예측하는 데 의존합니다. ThePreprint. TaskText-guided Music Generation Random Noise +124/24/Full-Maksed Music Text Prompt 캐치한 멜로디가 있는 팝 댄스 트랙 JEN-Bidirectional Mode &amp; Unidirectional mode Generated Music TaskMusic In-painting Random Noise … Maksed Music JEN-Text Prompt 캐치한 멜로디가 있는 팝 댄스 트랙 Bidirectional Mode Inpainted Music TaskMusic Continuation Random Noise ||... Prefixed Music Text Prompt 캐치한 멜로디가 있는 팝 댄스 트랙 JEN-Unidirectional Mode Inpainted Music 그림 1: 텍스트-가이드 음악 생성 작업, 음악 인페인팅 작업, 음악 연속 작업을 포함한 JEN-1 멀티태스크 학습 전략의 그림. JEN-1은 채널별로 노이즈와 마스크된 오디오를 연결하여 컨텍스트 내 학습 작업 일반화를 달성합니다. JEN-1은 포괄적인 컨텍스트를 수집하는 양방향 모드와 순차적 종속성을 포착하는 단방향 모드를 모두 통합합니다. 우도는 다음과 같이 표현됩니다. N PAR (Y | x) = [[ P (Yi | Y1:i−1; x), i=(1) 여기서 y;는 시퀀스 y의 i번째 토큰을 나타냅니다. 반대로, 이미지에 명확한 시계열 구조가 없고 이미지가 일반적으로 연속 공간을 차지하는 컴퓨터 비전(CV) 분야에서는 NAR 접근 방식을 사용하는 것이 더 적합한 것으로 간주됩니다. 주목할 점은 안정 확산과 같은 NAR 접근 방식이 이미지 생성 작업을 처리하는 주요 방법으로 부상했다는 것입니다. NAR 접근 방식은 잠재 임베딩 간에 조건부 독립성을 가정하고 예측 중에 구별 없이 균일하게 생성합니다. 그 결과 우도는 다음과 같이 표현됩니다. N PNAR (y | x) = P(Yi | x) . i=(2) NAR의 병렬 생성 접근 방식은 눈에 띄는 속도 이점을 제공하지만 장기적 일관성을 포착하는 측면에서 부족합니다. 이 연구에서는 오디오 데이터를 하이브리드 형태의 데이터로 간주할 수 있다고 주장합니다. 이는 고품질 음악의 모델링을 가능하게 하는 연속적인 공간에 존재하기 때문에 이미지와 유사한 특성을 보입니다.또한 오디오는 시계열 데이터로서의 특성에서 텍스트와 유사합니다.결과적으로 JEN-1 설계에서 새로운 접근 방식을 제안하는데, 이는 자기 회귀 및 비자기 회귀 모드를 응집력 있는 전방향 확산 모델로 통합하는 것을 수반합니다.3.2 오디오 생성을 위한 확산 모델 확산 모델(Ho et al., 2020)은 데이터 분포 p(x)를 학습하는 목적으로 명시적으로 개발된 확률적 모델을 구성합니다.확산 모델의 전반적인 학습에는 전방 확산 프로세스와 점진적인 잡음 제거 프로세스가 포함되며, 각각은 마르코프 체인 역할을 하는 일련의 T 단계로 구성됩니다.전방 확산 프로세스에서 고정 선형 가우시안 모델을 사용하여 표준 가우시안 분포로 수렴할 때까지 초기 확률 변수 zo를 점진적으로 교란합니다. 이 과정은 다음과 같이 형식적으로 표현할 수 있습니다. q(zt | Zo; x) = N(zt; √ātzo, (1 − āt) I), αt = По αi, i=(3)Preprint. 여기서 ai는 시간 단계 t에 따라 단조적으로 감소하는 계수이고, zɩ는 시간 단계 t에서의 잠재 상태입니다. 역 과정은 표준 가우시안 노이즈에서 시작하여 생성을 위해 점진적으로 노이즈 제거 전환 po(zt−1 | zt; x)를 활용하는 것입니다. Po(Zt−1 | Zt; x) = N(zt−1; µə(Zt, t; x), Σo(zt, t; x)), (4) 여기서 평균 μe와 분산 Σe는 0으로 매개변수화된 모델에서 학습됩니다. 우리는 훈련 가능한 매개변수 없이 미리 정의된 분산을 사용합니다(Rombach et al., 2022; Liu et al., 2023). 간단히 확장하고 재매개변수화한 후, 조건부 확산 모델의 학습 목표는 L = E20,&lt;^~^N (0,1),t | || € – € (zr, t)||2}], (5)로 표시할 수 있습니다. 여기서 t는 {1, ..., T}에서 균일하게 샘플링되고, e는 샘플링된 노이즈의 기준 진실이며, εo (.)는 확산 모델에서 예측한 노이즈입니다. EA 기존 확산 모델은 비자기 회귀 모델로 특징지어지며, 이는 음악 흐름에서 순차적 종속성을 효과적으로 포착하는 데 어려움을 겪습니다. 이러한 한계를 해결하기 위해, 단방향 및 양방향 학습을 모두 활용하는 통합 프레임워크인 조인트 전방향 확산 모델 JEN-1을 제안합니다. 이러한 적응을 통해 예측을 조건화하는 데 사용되는 맥락적 정보를 정확하게 제어할 수 있어 음악 데이터에서 순차적 종속성을 포착하는 모델의 능력이 향상됩니다. 4 방법 이 연구 논문에서는 전방향 1D 확산 모델을 활용하는 JEN-1이라는 새로운 모델을 제안합니다.JEN-1은 양방향 및 단방향 모드를 결합하여 텍스트 또는 음악 표현에 따라 조건화된 범용 음악 생성을 위한 통합된 접근 방식을 제공합니다.이 모델은 마스크 오디오 자동 인코더에서 얻은 노이즈에 강한 잠재 임베딩 공간에서 작동하여 낮은 프레임 속도로 잠재 임베딩에서 고충실도 재구성이 가능합니다(§ 4.1).이산 토큰을 사용하거나 여러 직렬 단계를 포함하는 이전 세대 모델과 달리 JEN1은 단일 모델을 사용하여 연속적이고 고충실도 음악을 생성할 수 있는 고유한 모델링 프레임워크를 도입합니다.JEN-1은 순차적 종속성을 개선하기 위해 자기 회귀 학습과 시퀀스 생성을 동시에 향상시키기 위해 비자기 회귀 학습을 효과적으로 활용합니다(§ 4.2). 컨텍스트 내 학습 및 멀티태스크 학습을 채택함으로써 JEN1의 중요한 장점 중 하나는 텍스트 또는 멜로디를 기반으로 하는 조건부 생성을 지원하여 다양한 창의적 시나리오에 대한 적응성을 향상시킨다는 것입니다(§ 4.3). 이러한 유연성 덕분에 이 모델을 다양한 음악 생성 작업에 적용할 수 있어 음악 작곡 및 제작을 위한 다재다능하고 강력한 도구가 됩니다. 4.1 고충실도 잠재 표현 학습을 위한 마스크 자동 인코더 고충실도 신경 오디오 잠재 표현. 품질과 충실도를 손상시키지 않고 제한된 계산 리소스에 대한 학습을 용이하게 하기 위해, 저희의 접근 방식인 JEN-1은 고충실도 오디오 자동 인코더 &amp;를 사용하여 원본 오디오를 잠재 표현 z로 압축합니다. 형식적으로, 2채널 스테레오 오디오 x = R¹×²가 주어지면 인코더 &amp;는 x를 잠재 표현 z = ε(x)로 인코딩합니다. 여기서 z Є RL/hxc입니다. L은 주어진 음악의 시퀀스 길이이고, h는 홉 크기이며, c는 잠재 임베딩의 차원입니다. 디코더는 잠재 표현에서 오디오 0 x = D(z) = D(E(x))를 재구성합니다. 우리의 오디오 압축 모델은 이전 연구(Zeghidour et al., 2021; Défossez et al., 2022)에서 영감을 받아 수정되었으며, 이는 시간 및 주파수 영역에 걸친 재구성 손실과 다양한 해상도에서 작동하는 패치 기반 적대적 목적의 조합으로 학습된 자동 인코더로 구성됩니다. 이를 통해 로컬 리얼리즘을 적용하여 오디오 재구성이 원래 오디오 매니폴드로 제한되고 L1 또는 L2 목적이 있는 샘플 공간 손실에만 의존하여 발생하는 흐릿한 효과를 방지합니다. 양자화 계층을 사용하여 이산 코드를 생성하는 이전 시도(Zeghidour et al., 2021; Défossez et al., 2022)와 달리, 당사 모델은 양자화로 인한 품질 저하 손실 없이 연속 임베딩을 직접 추출합니다. 강력한 자동 인코더 표현을 활용하면 복잡성 감소와 고주파 세부 정보 보존 간에 거의 최적의 균형을 달성하여 음악 충실도가 크게 향상됩니다. 노이즈에 강한 마스크 자동 인코더. 디코더 D의 견고성을 더욱 향상시키기 위해 노이즈를 효과적으로 줄이고 아티팩트를 완화하여 우수한 품질의 사전 인쇄본을 제공하는 마스킹 전략을 제안합니다. 알고리즘 1 잠재 임베딩 공간 정규화 입력: 기존 잠재 임베딩 {z;}~₁ 및 축소된 차원 k 1: {z}의 μ 및 Σ 계산 2: U, A, UT = SVD(Σ) 계산 3: W (U√A-¹)[:,: k] 계산 4: Zi = (zi — µ)W 출력: 정규화된 잠재 임베딩 {i}\audio 재구성. 우리의 훈련 절차에서는 중간 잠재 임베딩의 Ρ = 5%가 디코더에 입력되기 전에 무작위로 마스크되는 특정 기술을 채택합니다. 이를 통해 디코더가 손상된 입력에 노출된 경우에도 최상의 품질의 데이터를 재구성하는 데 능숙해질 수 있습니다. 우리는 큰 배치 크기를 사용하여 48kHz 입체 오디오에서 자동 인코더를 훈련하고 지수 이동 평균을 사용하여 가중치를 집계합니다. 이러한 향상의 결과로, 우리의 오디오 자동 인코더의 성능은 표 2에 표시된 모든 평가된 재구성 메트릭에서 원래 모델의 성능을 능가합니다. 결과적으로 우리는 이 오디오 자동 인코더를 모든 후속 모델에 채택합니다.
--- EXPERIMENT ---
s. 잠재 임베딩 공간 정규화. 임의로 스케일링된 잠재 공간을 피하기 위해 (Rombach et al., 2022)는 성분별 분산을 추정하고 잠재 z를 단위 표준 편차로 다시 스케일링하여 더 나은 성능을 달성하는 것이 중요하다는 것을 발견했습니다. 성분별 분산만 추정하는 이전 접근 방식과 달리 JEN-1은 알고리즘 1에 표시된 대로 잠재 임베딩의 이방성 문제를 해결하기 위해 간단하면서도 효과적인 후처리 기술을 사용합니다. 특히, 잠재 임베딩에 대해 채널별로 평균 0 정규화를 수행한 다음 SVD(Singular Value Decomposition) 알고리즘을 통해 공분산 행렬을 단위 행렬로 변환합니다. 이러한 변환 통계를 계산하기 위해 배치 증분 동등 알고리즘을 구현합니다. 또한 차원 감소 전략을 통합하여 화이트닝 프로세스를 더욱 향상시키고 접근 방식의 전반적인 효과를 개선합니다. 4. 전방향성 잠복 확산 모델 양방향 모드 단방향 모드 패딩 합성 블록 변압기 블록 패딩 ☐ ☐ ☐ ☐ ☐ 모든 셀프 어텐션 마스크에 주의 ☐ ☐ ☐ ☐ 일부 이전 접근 방식(Liu et al., 2023; Ghosal et al., 2023)에서는 멜 스펙트로그램과 같은 시간-주파수 변환 기술이 오디오 생성을 이미지 생성 문제로 변환하는 데 사용되었습니다. 그럼에도 불구하고, 우리는 원시 오디오 데이터에서 멜 스펙트로그램으로의 이러한 변환이 필연적으로 상당한 품질 저하로 이어진다고 주장합니다. 이 문제를 해결하기 위해 JEN-1은 시간적 1D 효율적 U-Net을 직접 활용합니다. 효율적 U-Net(Saharia et al., 2022)의 이 수정된 버전을 사용하면 파형을 효과적으로 모델링하고 확산 모델에서 필요한 블록을 구현할 수 있습니다. U-Net 모델의 아키텍처는 잔여 연결을 통해 상호 연결된 캐스케이딩 다운 샘플링 및 업 샘플링 블록으로 구성됩니다. 각 다운/업 샘플링 블록은 다운/업 샘플링 계층으로 구성되며, 그 뒤에 1D 시간적 합성곱 계층과 자기/교차 주의 계층을 포함하는 블록 세트가 이어집니다. 쌓인 입력과 출력은 모두 길이 L의 잠재 시퀀스로 표현되는 반면, 확산 시간 t는 다운 및 업 샘플링 블록 내에서 앞서 언급한 결합 계층을 통해 모델과 상호 작용하는 단일 시간 임베딩 벡터로 인코딩됩니다. U-Net 모델의 맥락에서 입력은 xt로 표시된 노이즈 샘플로 구성되며, 이는 추가 패딩으로 쌓여 왼쪽 자기 주의 마스크에 참석합니다. 그림 2: 합성곱 블록과 변환기 블록의 양방향 모드와 단방향 모드의 그림. 단방향 모드에서 우리는 합성곱 블록에서 인과 패딩을 사용하고 마스크된 자기 주의 마스크를 사용하여 왼쪽 맥락에만 주의를 기울인다.사전 인쇄. 조건부 정보.결과 출력은 확산 프로세스 동안의 노이즈 예측 및에 해당한다.상황 내 학습을 통한 작업 일반화.다중 작업 학습 목표의 목표를 달성하기 위해 우리는 UNet 아키텍처를 명시적으로 변경하지 않고 새로운 전방향 잠재 확산 모델을 제안한다.JEN-1은 다양한 음악 생성 작업을 텍스트 안내 상황 내 학습 작업으로 공식화한다.이러한 상황 내 학습 작업의 공통적인 목표는 맥락 음악과 일관성이 있고 텍스트에서 설명하는 올바른 스타일을 가진 다양하고 사실적인 음악을 제작하는 것이다.음악 인페인팅 작업 및 음악 연속 작업과 같은 상황 내 학습 목표의 경우 모델이 조건화된 추가 마스크된 음악 정보를 잠재 임베딩으로 추출하여 입력에 추가 채널로 스택할 수 있다. 더 정확하게 말해서, 원래 잠재 채널 외에도 U-Net 블록에는 129개의 추가 입력 채널(인코딩된 마스크 오디오용 128개, 마스크 자체용 1개)이 있습니다. 양방향 모드에서 단방향 모드로. 음악의 고유한 순차적 특성을 설명하기 위해 JEN-1은 오른쪽의 잠재 생성이 왼쪽의 생성된 잠재 생성에 따라 달라지도록 하여 단방향 확산 모드를 통합합니다. 이 메커니즘은 합성곱 블록에서 단방향 자기 주의 마스크와 인과 패딩 모드를 사용하여 달성됩니다. 일반적으로 전방향 확산 모델의 아키텍처는 다양한 입력 경로를 지원하여 다양한 유형의 데이터를 모델에 통합하는 것을 용이하게 하며, 노이즈 예측 및 확산 모델링을 위한 다재다능하고 강력한 기능을 제공합니다. JEN-1은 학습하는 동안 모델의 아키텍처를 변경하지 않고도 단방향 모드와 양방향 모델 간에 전환할 수 있습니다. 매개변수 가중치는 다른 학습 목표에 대해 공유됩니다. 그림 2에서 볼 수 있듯이 JEN-1은 단방향(자기 회귀) 모드로 전환할 수 있습니다.즉, 출력 변수는 이전 값에만 의존합니다.모든 1D 합성곱 계층에 인과 패딩(Oord et al., 2016)을 적용하여 앞에 0을 패딩하여 프레임의 초기 시간 단계 값도 예측할 수 있습니다.또한 셀프 어텐션 블록에서 수신한 입력에서 미래 토큰을 패딩 및 마스크하여 삼각형 어텐션 마스크 추종(Vaswani et al., 2017)을 적용합니다.4.3 통합 음악 다중 작업 훈련 단일 텍스트 가이드 학습 목표에만 의존하는 기존 방법과 달리 제안하는 프레임워크인 JEN-1은 공통 매개변수를 공유하면서 여러 생성 학습 목표를 동시에 통합하여 새로운 접근 방식을 채택합니다. 그림 1에 나와 있듯이, 훈련 프로세스는 세 가지 별개의 음악 생성 작업을 포함합니다. 양방향 텍스트 가이드 음악 생성, 양방향 음악 인페인팅, 단방향 음악 연속. 멀티태스크 훈련의 활용은 우리 접근 방식의 주목할 만한 측면으로, 모든 원하는 음악 생성 작업에서 응집력 있고 통합된 훈련 절차를 허용합니다. 이 접근 방식은 작업 간에 일반화하는 모델의 능력을 향상시키는 동시에 음악 순차적 종속성 처리와 시퀀스의 동시 생성을 개선합니다. 텍스트 가이드 음악 생성 작업. 이 작업에서는 양방향 및 단방향 모드를 모두 사용합니다. 양방향 모델은 모든 잠재 임베딩이 노이즈 제거 프로세스 동안 서로 주의를 기울일 수 있도록 하여 이전 및 후속 방향 모두에서 포괄적인 맥락 정보를 인코딩할 수 있습니다. 반면, 단방향 모델은 모든 잠재 임베딩이 이전 시간 대응물에만 주의를 기울이도록 제한하여 음악 데이터의 시간적 종속성을 학습하는 데 도움이 됩니다. 또한 U-Net 스택 입력 프레임워크 내에서 작업 일관성을 유지하기 위해 모든 비어 있는 마스크 오디오 옆에 전체 크기 마스크를 추가 조건으로 연결합니다. 음악 인페인팅 작업. 오디오 편집 분야에서 인페인팅은 음악 내에서 누락된 세그먼트를 복원하는 프로세스를 나타냅니다. 이 복원 기술은 주로 과거의 손상된 오디오를 재구성하고 음악 작곡에서 노이즈 및 워터마크와 같은 원치 않는 요소를 제거하는 데 사용됩니다. 이 작업에서는 JEN-1의 양방향 모드를 채택합니다. 학습 단계에서 우리의 접근 방식은 마스크 비율이 20%에서 80%까지인 오디오 마스크를 무작위로 생성하여 음악 인페인팅 프로세스를 시뮬레이션하는 것을 포함합니다. 그런 다음 이러한 마스크를 사용하여 해당 마스크 오디오를 얻고 이는 U-Net 모델 내에서 조건부 컨텍스트 내 학습 입력으로 사용됩니다. 음악 연속 작업. 우리는 제안된 JEN-1 모델이 새로운 옴니디렉티브-Preprint를 사용하여 음악 인페인팅(보간)과 음악 연속(외삽)을 모두 용이하게 한다는 것을 보여줍니다.표 2: MusicCaps 테스트 세트에서 최첨단 텍스트-음악 생성 방법과의 비교.방법 정량적 FAD↓ KL↓ CLAP↑ 정성적 T2M-QLT 1 T2M-ALI↑ 확산 14.8 2.0.72.72.Mousai 7.5 1.0.76.71.MusicLM 4.81.82.Noise2Music 2.MusicGen 3.1.0.83.79.JEN-1(저희) 2.1.0.85.82.선형 확산 모델. 기존 확산 모델은 비자기회귀적 특성으로 인해 이전 연구에서 최적이 아닌 성능을 보였습니다(Borsos et al., 2023; Agostinelli et al., 2023). 이러한 제한으로 인해 오디오 연속 작업에 성공적으로 적용할 수 없었습니다. 이 문제를 해결하기 위해 음악 연속 작업에서 단방향 모드를 채택하여 예측된 잠재 임베딩이 대상 세그먼트 내에서 왼쪽 컨텍스트에만 주의를 기울이도록 했습니다. 마찬가지로 배타적 오른쪽 전용 마스크의 무작위 생성을 통해 음악 연속 프로세스를 시뮬레이션합니다. 이러한 마스크는 20%에서 80%까지 다양한 비율로 생성됩니다. 5 실험 5.1 설정 구현 세부 정보. 마스크된 음악 자동 인코더의 경우 320의 홉 크기를 사용하여 48kHz 음악 오디오를 인코딩하기 위한 125Hz 잠재 시퀀스를 생성했습니다. 잠재 임베딩의 차원은 128입니다. 훈련 중에 잠재 임베딩의 5%를 무작위로 마스크하여 노이즈 내성 디코더를 얻습니다. 우수한 텍스트 임베딩 추출을 제공하기 위해 명령 기반 대규모 언어 모델인 FLAN-T5(Chung et al., 2022)를 사용합니다. 전방향 확산 모델의 경우 중간 교차 어텐션 차원을 1024로 설정하여 746M 매개변수를 생성합니다. 멀티태스크 훈련 중에 배치의 1/3을 각 훈련 작업에 균등하게 할당합니다. 또한 분류기 없는 안내(Ho &amp; Salimans, 2022)를 적용하여 샘플과 텍스트 조건 간의 대응 관계를 개선했습니다. 훈련 중에 교차 어텐션 레이어는 확률 0.2로 셀프 어텐션으로 무작위로 대체됩니다. 우리는 Adam W 최적화기(Loshchilov &amp; Hutter, 2017)를 사용하여 8개의 A100 GPU에서 200k 단계로 JEN-1 모델을 훈련하고, 선형 감소 학습률은 3e-a에서 시작하고 총 배치 크기는 512개 예제, ẞ1 € 0.9, 62: 0.95, 분리된 가중치 감소는 0.1, 그래디언트 클리핑은 1.0입니다. = = 데이터 세트. 우리는 총 5k 시간 분량의 고품질 개인 음악 데이터를 사용하여 JEN-1을 훈련합니다. 모든 음악 데이터는 48kHz로 샘플링된 전체 길이의 음악으로 구성되며, 메타데이터는 풍부한 텍스트 설명과 추가 태그 정보(예: 장르, 악기, 분위기/주제 태그 등)로 구성됩니다. 제안된 방법은 각각 10초씩 지속되는 5.5K 전문가 준비 음악 샘플과 1K 샘플을 포함하는 장르 균형 하위 집합으로 구성된 MusicCaps(Agostinelli et al., 2023) 벤치마크를 사용하여 평가됩니다. 공정한 비교를 유지하기 위해 불균형 집합에 대한 객관적인 지표가 보고되는 반면, 장르 균형 집합에서 무작위로 샘플링된 예제에 대한 정성적 평가 및 제거 연구가 수행됩니다. 평가 지표. 정량적 평가의 경우 객관적 및 주관적 지표를 모두 사용하여 제안된 방법을 평가합니다. 객관적인 평가에는 세 가지 지표가 포함됩니다. Fréchet Audio Distance(FAD)(Kilgour et al., 2019), Kullback-Leibler Divergence(KL)(Van Erven &amp; Harremos, 2014), CLAP 점수(CLAP)(Elizalde et al., 2023). FAD는 생성된 오디오의 타당성을 나타냅니다. FAD 점수가 낮을수록 타당성이 높아집니다. 원래 음악과 생성된 음악의 유사성을 측정하기 위해 AudioSet(Gemmeke et al., 2017)에서 학습한 최첨단 오디오 분류기를 사용하여 레이블 확률에 대한 KL-divergence를 계산합니다. 낮은 KL 점수는 생성된 음악이 참조 음악과 유사한 개념을 공유한다는 것을 나타냅니다. 또한, 공식 사전 학습된 CLAP 모델을 활용하여 CLAP 점수를 사용하여 트랙 설명과 생성된 오디오 간의 오디오-텍스트 정렬을 정량화합니다. 정성적 평가의 경우, 무작위로 생성된 사전 인쇄본을 정성적으로 평가하기 위해 동일한 실험 설계(Copet et al., 2023)를 따릅니다.표 3: 절제 연구.기준선 구성에서 JEN 구성을 점진적으로 수정하여 각 구성 요소의 효과를 조사합니다.구성 유행↓ 정량적 KL↓ 클랩↑ 정성적 T2M-QLTT2M-ALI↑ 기준선 3.1 1.0.80.78.+ 자기 회귀 모드 2.5 1.0.82.79.+ 음악 인페인팅 작업 2.1.0.83.80.+ 음악 연속 작업 2.1.0.85.82.음악 샘플.인간 평가자는 생성된 음악의 두 가지 핵심 측면, 즉 텍스트 대 음악 품질(T2M-QLT)과 텍스트 입력에 대한 정렬(T2M-ALI)을 평가하는 데 참여했습니다. 인간 평가자는 텍스트-음악 품질 테스트에서 생성된 음악 샘플에 대한 지각적 품질 평가를 1~100점 척도로 제공하도록 요청받았습니다.또한 텍스트-음악 정렬 테스트에서 평가자는 오디오와 텍스트 간의 정렬을 1~100점 척도로 평가해야 했습니다.5. 최첨단 기술과의 비교 표 2에서 볼 수 있듯이 JEN-1의 성능을 Riffusion(Forsgren &amp; Martiros, 2022), Mousai(Schneider et al., 2023), MusicLM(Agostinelli et al., 2023), MusicGen(Copet et al., 2023), Noise2Music(Huang et al., 2023a)을 포함한 다른 최첨단 방법과 비교했습니다.이러한 경쟁적 접근 방식은 모두 대규모 음악 데이터 세트에서 학습되었으며 다양한 텍스트 프롬프트가 주어졌을 때 최첨단 음악 합성 능력을 보여주었습니다. 공정한 비교를 위해 MusicCaps 테스트 세트의 성능을 양적, 질적 측면에서 모두 평가합니다. 구현이 공개적으로 제공되지 않으므로 테스트에 MusicLM 공개 API를 활용합니다. Noise2Music의 경우 원래 논문에서 언급한 대로 FAD 점수만 보고합니다. 실험 결과에 따르면 JEN-1은 텍스트 대 음악 품질과 텍스트 대 음악 정렬 측면에서 다른 경쟁 기준선보다 성능이 뛰어납니다. 구체적으로 JEN-1은 FAD 및 CLAP 점수 측면에서 우수한 성능을 보이며 두 번째로 높은 방법인 Noise2Music 및 MusicGen보다 큰 차이로 성능이 뛰어납니다. 인간의 질적 평가와 관련하여 JEN-1은 지속적으로 최고의 T2M-QLT 및 T2M-ALI 점수를 달성합니다. 주목할 점은 JEN-1이 MusicGEN(746M 대 3.3B 매개변수)의 22.6%, Noise2Music(746M 대 1.3B 매개변수)의 57.7%로 계산 효율성이 더 높다는 것입니다.5. 성능 분석 이 섹션에서는 제안된 전방향 확산 모델 JEN-1의 다양한 측면을 조사하기 위한 포괄적인 성능 분석을 제시합니다.Ablation 연구. 전방향 확산 모델의 효과를 평가하기 위해 모델 구성의 효과와 다양한 멀티태스크 목표의 효과를 포함하여 다양한 구성을 비교합니다.모든 ablation은 보류된 평가 세트에서 무작위로 선택한 1K 장르 균형 샘플에서 수행됩니다.표 3에서 볼 수 있듯이 결과는 i) JEN1이 자기 회귀 모드를 통합하여 생성된 음악의 시간적 일관성을 크게 향상시켜 음악 품질이 향상됨을 보여줍니다. ii) 제안된 멀티태스크 학습 목표, 즉 텍스트 가이드 음악 생성, 음악 인페인팅, 음악 연속은 작업 일반화를 개선하고 지속적으로 더 나은 성과를 달성합니다. iii) 이 모든 전담 설계가 함께 추가 교육 비용을 도입하지 않고도 고충실도 음악 생성으로 이어집니다. 생성 다양성. 트랜스포머 기반 생성 방법과 비교할 때 확산 모델은 생성 다양성으로 유명합니다. JEN-1의 생성 다양성과 신뢰성을 더 자세히 조사하기 위해 일반적인 장르나 악기를 포함하는 설명과 같은 동일한 텍스트 프롬프트를 제공하여 여러 다른 샘플을 생성합니다. 데모 페이지에서 보여 주듯이 JEN-1은 지속적으로 높은 수준의 품질을 유지하면서 생성 출력에서 인상적인 다양성을 보여줍니다. 생성, 일반화 및 제어 가능성. 감독 학습 방식으로 쌍을 이룬 텍스트와 음악 샘플로 학습했음에도 불구하고, 저희 방법인 JEN-1은 주목할 만한 제로샷 생성 기능과 효과적인 제어 가능성을 보여줍니다. 관련된 과제에도 불구하고 사전 인쇄. 배포되지 않은 프롬프트에서 고품질 오디오를 생성하면서 JEN-1은 여전히 매력적인 음악 샘플을 제작하는 데 능숙함을 보여줍니다. 데모 페이지에서 창의적인 제로샷 프롬프트의 예를 제시하여 모델이 만족스러운 품질의 음악을 성공적으로 생성한 것을 보여줍니다. 또한 JEN-1이 음악 관련 의미론을 포착하고 prompt2prompt(Hertz et al., 2022)와 같은 편집 기술과 통합된 뛰어난 제어 가능성을 보여주는 생성 예를 제시합니다. 주목할 점은 데모에서 생성된 음악이 장르, 악기, 분위기, 속도 등과 같은 음악 개념을 적절히 반영한다는 것을 보여줍니다. 6
--- CONCLUSION ---
이 연구에서 우리는 생성된 샘플의 효율성과 품질 면에서 기존 방법을 능가하는 강력하고 효율적인 텍스트-음악 생성 프레임워크인 JEN-1을 제안했습니다. 멜로 스펙트로그램 대신 파형을 직접 모델링하고, 자기 회귀 및 비자기 회귀 학습과 다중 작업 학습 목표를 결합함으로써 JEN-1은 48kHz 샘플링 속도에서 고품질 음악을 생성할 수 있습니다. 확산 모델과 마스크 자동 인코더를 통합하면 JEN-1이 음악에서 복잡한 시퀀스 종속성을 포착하는 능력이 더욱 향상됩니다. 우리의 광범위한 양적 및 인적 평가는 JEN-1이 주관적 품질, 다양성 및 제어 가능성 면에서 강력한 기준선보다 우수함을 보여줍니다. JEN-1은 다중 작업 학습 체제에서 음악 완성 및 연속 작업에서도 탁월합니다. 이러한 결과는 음악 파형을 모델링하는 데 있어서 우리 기술의 효과성과 통합 프레임워크의 장점을 강조합니다. 이 연구는 텍스트-음악 생성의 경계를 넓히고 텍스트에서 의미적으로 제어 가능한 고품질 음악 합성을 위한 매력적인 솔루션을 제공합니다. 잠재적인 미래 방향으로는 제어 가능성을 향상시키기 위해 외부 지식을 통합하고 프레임워크를 다른 크로스 모달 생성 작업으로 확장하는 것이 포함됩니다. 저희는 저희의 작업이 강렬하고 사실적인 예술을 만드는 생성 모델을 개발하는 데 더 많은 노력을 불러일으키기를 바랍니다. 텍스트-음악 생성이 연구에서 실용적인 응용 프로그램으로 성숙함에 따라 인간의 창의성을 증강하고 사람들이 음악을 작곡, 공유 및 감상하는 방식을 재구성할 수 있는 큰 잠재력을 가지고 있습니다. 참고문헌 Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: 텍스트에서 음악 생성. arXiv 사전 인쇄 arXiv:2301.11325, 2023. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi 등 Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식입니다. 오디오, 음성 및 언어 처리에 관한 IEEE/ACM 거래, 2023. 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 명령어를 미세 조정한 언어 모델을 확장합니다. arXiv 사전 인쇄본 arXiv:2210.11416, 2022. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez. 간단하고 제어 가능한 음악 생성. arXiv 사전 인쇄본 arXiv:2306.05284, 2023. Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath. 생성적 적대적 네트워크: 개요. IEEE 신호 처리 잡지,(1):53–65, 2018. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi. 고충실도 신경 오디오 압축. arXiv 사전 인쇄본 arXiv:2210.13438, 2022. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A generative model for music. arXiv 사전 인쇄본 arXiv:2005.00341, 2020. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, Huaming Wang. 자연어 감독을 통한 박수 학습 오디오 개념. ICASSP 2023-2023 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), pp. 1–5. IEEE, 2023. 사전 인쇄본. Seth* Forsgren과 Hayk* Martiros. Riffusion - 실시간 음악 생성을 위한 안정적 확산, 2022. URL https://riffusion.com/about. Cristina Gârbacea, Aäron van den Oord, Yazhe Li, Felicia SC Lim, Alejandro Luebs, Oriol Vinyals, Thomas C Walters. vq-vae 및 wavenet 디코더를 사용한 저비트레이트 음성 코딩. ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 735-739쪽. IEEE, 2019. Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, Marvin Ritter. 오디오 세트: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트. 2017 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 776-780쪽. IEEE, 2017. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria. 명령어 조정 llm 및 잠재 확산 모델을 사용한 텍스트-오디오 생성. arXiv 사전 인쇄본 arXiv:2304.13731, 2023. Curtis Hawthorne, Andrew Jaegle, Cătălina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. 지각자 ar을 사용한 범용, 장기 컨텍스트 자기 회귀 모델링. 기계 학습 국제 컨퍼런스, 8535-8558쪽. PMLR, 2022. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626, 2022. Jonathan Ho 및 Tim Salimans. arXiv:2207.12598, 2022. 분류자 없는 확산 안내. arXiv 사전 인쇄본 Jonathan Ho, Ajay Jain 및 Pieter Abbeel. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전, 33:6840–6851, 2020. Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: 확산 모델을 사용한 텍스트 조건 음악 생성. arXiv 사전 인쇄본 arXiv:2302.03917, 2023a. Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao. Make-an-audio: 프롬프트 강화 확산 모델을 사용한 텍스트-오디오 생성. arXiv 사전 인쇄본 arXiv:2301.12661, 2023b. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, Matthew Sharifi. Fréchet 오디오 거리: 음악 강화 알고리즘을 평가하기 위한 참조 없는 메트릭. INTERSPEECH, pp. 2350-2354, 2019. Jungil Kong, Jaehyeon Kim, Jaekyoung Bae. Hifi-gan: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크. 신경 정보 처리 시스템의 발전, 33:17022-17033, 2020a. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: 오디오 합성을 위한 다재다능한 확산 모델. arXiv 사전 인쇄본 arXiv:2009.09761, 2020b. Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: 텍스트 기반 오디오 생성. arXiv 사전 인쇄본 arXiv:2209.15352, 2022. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: 잠재 확산 모델을 사용한 텍스트-오디오 생성. arXiv 사전 인쇄본 arXiv:2301.12503, 2023. Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. arXiv:1711.05101, 2017. arXiv 사전 인쇄본사전 인쇄본. Andrés Marafioti, Nathanaël Perraudin, Nicki Holighaus 및 Piotr Majdak. 오디오 인페인팅을 위한 컨텍스트 인코더. IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 27(12): 2362-2372, 2019. Aashiq Muhamed, Liang Li, Xingjian Shi, Suri Yaddanapudi, Wayne Chi, Dylan Jackson, Rahul Suresh, Zachary C Lipton 및 Alex J Smola. 트랜스포머 간을 사용한 상징적 음악 생성. AAAI 인공지능 컨퍼런스 회의록, 35권, 408-417쪽, 2021. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu. Wavenet: 원시 오디오를 위한 생성 모델. arXiv 사전 인쇄본 arXiv:1609.03499, 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 10684–10695, 2022. Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: 말하고 들을 수 있는 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2306.12925, 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. 플라비오 슈나이더, 지징 진, 베르나르트 쇤코프. Mo\^ usai: 긴 맥락 잠재 확산을 이용한 텍스트-음악 생성. arXiv 사전 인쇄본 arXiv:2301.11757, 2023. 아론 반 덴 오르드, 오리올 비냔알스 외. 신경 이산 표현 학습. 신경 정보 처리 시스템의 발전, 30, 2017. 팀 반 에르벤, 피터 하레모스. 레니 발산 및 쿨백-라이블러 발산. IEEE Transactions on Information Theory, 60(7):3797-3820, 2014. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, Dong Yu. Diffsound: 텍스트-사운드 생성을 위한 이산 확산 모델. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Yi Yu, Abhishek Srivastava, Simon Canales. 가사에서 멜로디를 생성하기 위한 조건부 lstm-gan. ACM Transactions on Multimedia Computing, Communications, and Applications(TOMM), 17(1):1–20, 2021. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: 엔드투엔드 신경 오디오 코덱. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495–507, 2021.
