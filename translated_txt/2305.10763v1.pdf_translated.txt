--- ABSTRACT ---
표현적인 텍스트-음성(TTS)을 달성하기 위해 텍스트 표현을 개선하는 데 많은 관심이 집중되었습니다. 그러나 기존 연구는 마스크된 토큰 재구성 작업으로 음운을 암묵적으로만 학습하여 낮은 학습 효율성과 음운 모델링의 어려움을 초래합니다. 우리는 서로 다른 맥락에서 동일한 텍스트 토큰의 음운 분산을 명시적으로 학습하는 교차 모달 대조 사전 학습 프레임워크인 CLAPSpeech를 제안합니다. 구체적으로, 1) 우리는 모델이 인코더 입력의 정교한 설계와 대조 손실을 통해 공동 멀티 모달 공간에서 텍스트 맥락을 해당 음운 패턴과 연결하도록 장려합니다. 2) 우리는 다중 수준에서 음운 패턴을 포착하기 위한 다중 스케일 사전 학습 파이프라인을 도입합니다. 우리는 더 나은 음운을 위해 CLAPSpeech를 기존 TTS 모델에 통합하는 방법을 보여줍니다. 세 가지 데이터 세트에 대한 실험은 CLAPSpeech가 기존 TTS 방법에 대한 음운 예측을 개선할 수 있을 뿐만 아니라 여러 언어와 다중 화자 TTS에 적응하는 일반화 능력을 보여줍니다. 또한 CLAPSpeech의 성능에 대한 원리를 심층적으로 분석합니다. 절제 연구는 우리 방법의 각 구성 요소의 필요성을 보여줍니다. 소스 코드와 오디오 샘플은 https://clapspeech.github.io에서 제공됩니다. 1
--- INTRODUCTION ---
영어: 딥 러닝의 개발로 현대 TTS 시스템의 오디오 품질이 개선되었지만, 음성학적 모델링은 여전히 어려운 문제입니다. 표현적 TTS에 대한 이전 연구에서는 외부 변이 예측자(예측 기반, PB)를 활용했습니다. *동등한 기여. *연락 저자. Yi Ren ren.yi@bytedance.com Bytedance Jinzheng He jinzhenghe@zju.edu.cn 저장 대학교 Zhou Zhao+ zhaozhou@zju.edu.cn 저장 대학교(Ren et al., 2021a) 및 변이 생성 모델(변이 기반, VB)(Kim et al., 2020; Liu et al., 2022)을 사용하여 음성학적 분산을 TTS 모델에 주입했습니다. 또 다른 인기 있는 방향은 음성학적 예측을 위해 더 나은 텍스트 표현을 학습하는 것입니다(Tan et al., 2021). 그러나 TTS를 위한 기존 텍스트 표현 학습 방법은 마스크 언어 모델 작업(Devlin 등, 2019; Jia 등, 2021; Chen 등, 2021)(즉, 텍스트 코퍼스에서 BERT와 유사한 대규모 언어 모델을 학습) 또는 마스크 음향 모델 작업(Chen 등, 2020; Bai 등, 2022)(즉, 입력 텍스트를 기반으로 마스크된 멜 스펙트로그램을 재구성)에 기반을 두고 있으며, 이로 인해 두 가지 단점이 있습니다. 첫째, 재구성 손실로 암묵적으로 음성학을 학습하므로 모델이 음성학 모델링을 개선하는 데 방해가 됩니다. 둘째, 발음 공간과 음성학 공간을 분리하지 않아 학습 효율성이 낮고 모델 용량이 낭비됩니다. 4.3.1절에서 사례 연구를 수행하면 TTS에서 사용된 이전 텍스트 표현은 다른 텍스트 맥락에서 음성학 분산을 포착할 수 없다는 것을 알 수 있습니다. 기술적으로, 음조는 서로 다른 조건(예: 텍스트 맥락 및 화자)에서 동일한 토큰의 피치 및 지속 시간 변화로 간주될 수 있습니다(Tan et al., 2021). 이 논문은 주로 텍스트 맥락과 관련된 음조를 연구합니다. 예를 들어, 동일한 단어 &quot;higher&quot;에 대해 &quot;higher up&quot; 또는 &quot;slightly higher&quot;라고 말하면 서로 다른 음조가 나올 수 있습니다. 영어: 텍스트-이미지 작업(Radford et al., 2021; Elizalde et al., 2022)에서 최근의 교차 모달 대조 학습 작업에서 영감을 얻어 텍스트-음성 조인트 멀티 모달 공간에서 텍스트 컨텍스트와 고수준 음성 패턴을 연결하는 대조 학습 방법을 제안합니다.즉, 대조 언어-오디오 사전 학습 선택된 토큰 &quot;더 높은&quot; 토큰 인덱싱 토큰 인코딩을 포함하는 텍스트는 컨텍스트 정보가 더 위에 있으면 약간 더 높은 텍스트 텍스트 인코딩 연령... 인코더가 점점 더 높아지는 대규모 ASR 데이터 세트의 텍스트-음성 쌍 음성 인코더 555-S₁ S₁T ST₂ S₁ T... S₁. TN Sz ST S₂ Tz Sp. T... Sp. T .. B ☐ S SL SL SL ST .. ... ... 선택된 토큰의 상위 음성 세그먼트 &quot;상위&quot; 음성 인코딩과 로컬 음성 정보 SN Sw T₁ Sw T₂ SN T ... SN - TN 그림 1: CLAPSpeech의 대조적 사전 학습 프로세스. 명확성을 위해 여기서는 단어 수준의 사전 학습만 보여줍니다. 또한 음소 수준의 사전 학습도 수행합니다. 텍스트 음성 변환(CLAPSpeech)을 위해. 구체적으로, 텍스트 컨텍스트에서 음성을 예측하는 텍스트 인코더와 선택된 토큰의 음성 세그먼트에서 기준 진실(GT) 음성을 추출하는 음성 인코더를 학습합니다. 학습하는 동안 동일한 발음 가능 토큰(예: 단어 &quot;상위&quot; 또는 음소 &quot;AEO&quot;)이 포함된 N개의 텍스트-음성 쌍을 선택합니다. 텍스트 토큰을 해당 음운론(GT 음성에서 추출)과 정렬하고 음운론 표현을 다른 텍스트 컨텍스트에서 밀어냄으로써 텍스트 인코더는 텍스트 컨텍스트에서 음운론을 추출하도록 장려됩니다. CLAPSpeech 사전 학습의 직관적인 예는 그림 1에서 찾을 수 있습니다. 또한 음운론 패턴은 여러 수준에서 표현될 수 있음을 관찰합니다. 따라서 음소 및 단어 수준에서 각각 음운론 정보를 캡처하기 위해 두 개의 CLAPSpeech 모델을 학습하는 다중 스케일 사전 학습 프레임워크를 제안합니다. 사전 학습 단계 후, CLAPSpeech는 모든 TTS 모델에 적용 가능한 플러그인 텍스트 인코더로 간주되어 세분화된 음운론 표현을 제공할 수 있습니다. 우리 접근 방식의 효과성과 일반화 가능성을 증명하기 위해, 우리는 두 개의 대규모 자동 음성 인식(ASR) 데이터 세트(영어의 경우 LibriSpeech(Panayotov 등, 2015) 및 중국어의 경우 WenetSpeech(Zhang 등, 2022))를 사용하여 CLAPSpeech 모델을 사전 학습시킵니다. 그런 다음 CLAPSpeech의 사전 학습된 텍스트 인코더를 예측/변동 기반 TTS 베이스라인에 연결하여 기존의 표현 TTS 시스템에 대한 CLAPSpeech의 개선을 보여줍니다. 그런 다음 단일 화자 영어 데이터 세트 하나, 단일 화자 중국어 코퍼스 하나, 다중 화자 영어 데이터 세트 하나를 포함하여 세 개의 TTS 데이터 세트에 대한 성능을 평가합니다. 모든 데이터 세트에 대한 실험은 CLAPSpeech가 TTS 모델의 음성학을 개선하고 이전의 표현 학습 방법보다 성능이 우수함을 보여줍니다. 요약하자면 CLAPSpeech는 세 가지 뛰어난 장점을 가지고 있습니다. 1) 대조적 목적 덕분에 훨씬 더 작은 모델 규모로 이전 표현 학습 방법보다 더 나은 음성 표현을 제공할 수 있습니다. 이 목적은 음성을 명시적으로 학습합니다. 2) CLAPSpeech의 텍스트 표현은 프런트엔드 네트워크 아키텍처를 약간만 수정하면 기존 TTS 시스템에서 편리하게 사용할 수 있습니다. 3) 또한 4.3.2절에서 세밀한 음성 전송과 같은 잠재적 응용 프로그램을 보여줍니다. 2
--- RELATED WORK ---
2. 표현력 있는 TTS 지난 몇 년 동안 현대 신경 TTS는 높은 실용성과 오디오 품질에서 상당한 진전을 이루었습니다(Ren 등, 2019; Kim 등, 2020; Elias 등, 2021; Miao 등, 2021; Kim 등, 2021; Donahue 등, 2021; Jiang 등, 2022). 그러나 일반 입력 텍스트를 고려하여 표현력 있는 음조를 모델링하는 것은 여전히 어려운 일입니다. 표현력 있는 TTS를 달성하기 위해 일반적인 관행 중 하나는 참조 인코더와 스타일 토큰을 사용하는 것입니다(Wang 등, 2018; Jia 등, 2018). 그러나 추론 중에 적절한 참조 오디오를 선택하는 것은 어렵습니다(Tan 등, 2021). 다른 연구에서는 고급 네트워크 설계를 통해 음성 모델링을 개선하고자 하며, 이는 두 가지 유형으로 분류할 수 있습니다. (1) 예측 기반(PB) TTS 시스템(Ren et al., 2021a)은 피치 컨투어, 지속 시간, 에너지와 같은 음성 속성을 예측하기 위해 여러 외부 예측자를 학습합니다. (2) 변이 기반(VB) TTS 시스템은 변이 자동 인코더(VAE)(Ren et al., 2021b) 또는 정규화 흐름(Kim et al., 2020)을 활용하여 잠재 공간에서 음성을 모델링합니다. 음성 예측을 돕기 위해 풍부한 사전 지식으로 더 나은 텍스트 표현을 제공하는 것을 탐구하는 연구도 있습니다. 예를 들어, Liu et al.(2021)과 Ye et al.(2022)은 전용 모델링을 통해 구문 정보를 통합합니다.
--- METHOD ---
s이지만, 여러 언어와 다중 화자 TTS에 적응하는 일반화 능력을 보여줍니다. 또한 CLAPSpeech 성능의 원리를 심층 분석합니다. 절제 연구는 방법에서 각 구성 요소의 필요성을 보여줍니다. 소스 코드와 오디오 샘플은 https://clapspeech.github.io에서 제공됩니다. 1 서론 딥 러닝의 발전으로 현대 TTS 시스템의 오디오 품질이 개선되었지만, 음성 모델링은 여전히 어려운 문제입니다. 표현형 TTS에 대한 이전 작업에서는 외부 변이 예측기(예측 기반, PB)를 활용했습니다. *동등한 기여. *연락 저자. Yi Ren ren.yi@bytedance.com Bytedance Jinzheng He jinzhenghe@zju.edu.cn Zhejiang University Zhou Zhao+ zhaozhou@zju.edu.cn Zhejiang University (Ren et al., 2021a) 및 변형 생성 모델(변형 기반, VB)(Kim et al., 2020; Liu et al., 2022)을 사용하여 TTS 모델에 음성적 분산을 주입합니다. 또 다른 인기 있는 방향은 음성적 예측을 위해 더 나은 텍스트 표현을 학습하는 것입니다(Tan et al., 2021). 그러나 TTS를 위한 기존 텍스트 표현 학습 방법은 마스크 언어 모델 작업(Devlin 등, 2019; Jia 등, 2021; Chen 등, 2021)(즉, 텍스트 코퍼스에서 BERT와 유사한 대규모 언어 모델을 학습) 또는 마스크 음향 모델 작업(Chen 등, 2020; Bai 등, 2022)(즉, 입력 텍스트를 기반으로 마스크된 멜 스펙트로그램을 재구성)에 기반을 두고 있으며, 이로 인해 두 가지 단점이 있습니다. 첫째, 재구성 손실로 암묵적으로 음성학을 학습하기 때문에 모델이 음성학 모델링을 개선하는 데 방해가 됩니다. 둘째, 발음 공간과 음성학 공간을 분리하지 않아 학습 효율성이 낮고 모델 용량이 낭비됩니다. 4.3.1절에서 사례 연구를 수행했는데, 이를 통해 TTS에서 사용된 이전 텍스트 표현은 다른 텍스트 맥락에서 음성학 분산을 포착할 수 없다는 것을 알 수 있습니다. 기술적으로, 음조는 서로 다른 조건(예: 텍스트 맥락 및 화자)에서 동일한 토큰의 피치 및 지속 시간 변화로 간주될 수 있습니다(Tan et al., 2021). 이 논문은 주로 텍스트 맥락과 관련된 음조를 연구합니다. 예를 들어, 동일한 단어 &quot;higher&quot;에 대해 &quot;higher up&quot; 또는 &quot;slightly higher&quot;라고 말하면 서로 다른 음조가 나올 수 있습니다. 영어: 텍스트-이미지 작업(Radford et al., 2021; Elizalde et al., 2022)에서 최근의 교차 모달 대조 학습 작업에서 영감을 얻어 텍스트-음성 조인트 멀티 모달 공간에서 텍스트 컨텍스트와 고수준 음성 패턴을 연결하는 대조 학습 방법을 제안합니다.즉, 대조 언어-오디오 사전 학습 선택된 토큰 &quot;더 높은&quot; 토큰 인덱싱 토큰 인코딩을 포함하는 텍스트는 컨텍스트 정보가 더 위에 있으면 약간 더 높은 텍스트 텍스트 인코딩 연령... 인코더가 점점 더 높아지는 대규모 ASR 데이터 세트의 텍스트-음성 쌍 음성 인코더 555-S₁ S₁T ST₂ S₁ T... S₁. TN Sz ST S₂ Tz Sp. T... Sp. T .. B ☐ S SL SL SL ST .. ... ... 선택된 토큰의 상위 음성 세그먼트 &quot;상위&quot; 음성 인코딩과 로컬 음성 정보 SN Sw T₁ Sw T₂ SN T ... SN - TN 그림 1: CLAPSpeech의 대조적 사전 학습 프로세스. 명확성을 위해 여기서는 단어 수준의 사전 학습만 보여줍니다. 또한 음소 수준의 사전 학습도 수행합니다. 텍스트 음성 변환(CLAPSpeech)을 위해. 구체적으로, 텍스트 컨텍스트에서 음성을 예측하는 텍스트 인코더와 선택된 토큰의 음성 세그먼트에서 기준 진실(GT) 음성을 추출하는 음성 인코더를 학습합니다. 학습하는 동안 동일한 발음 가능 토큰(예: 단어 &quot;상위&quot; 또는 음소 &quot;AEO&quot;)이 포함된 N개의 텍스트-음성 쌍을 선택합니다. 텍스트 토큰을 해당 음운론(GT 음성에서 추출)과 정렬하고 음운론 표현을 다른 텍스트 컨텍스트에서 밀어냄으로써 텍스트 인코더는 텍스트 컨텍스트에서 음운론을 추출하도록 장려됩니다. CLAPSpeech 사전 학습의 직관적인 예는 그림 1에서 찾을 수 있습니다. 또한 음운론 패턴은 여러 수준에서 표현될 수 있음을 관찰합니다. 따라서 음소 및 단어 수준에서 각각 음운론 정보를 캡처하기 위해 두 개의 CLAPSpeech 모델을 학습하는 다중 스케일 사전 학습 프레임워크를 제안합니다. 사전 학습 단계 후, CLAPSpeech는 모든 TTS 모델에 적용 가능한 플러그인 텍스트 인코더로 간주되어 세분화된 음운론 표현을 제공할 수 있습니다. 우리 접근 방식의 효과성과 일반화 가능성을 증명하기 위해, 우리는 두 개의 대규모 자동 음성 인식(ASR) 데이터 세트(영어의 경우 LibriSpeech(Panayotov 등, 2015) 및 중국어의 경우 WenetSpeech(Zhang 등, 2022))를 사용하여 CLAPSpeech 모델을 사전 학습시킵니다. 그런 다음 CLAPSpeech의 사전 학습된 텍스트 인코더를 예측/변동 기반 TTS 베이스라인에 연결하여 기존의 표현 TTS 시스템에 대한 CLAPSpeech의 개선을 보여줍니다. 그런 다음 하나의 단일 화자 영어 데이터 세트, 하나의 단일 화자 중국어 코퍼스, 하나의 다중 화자 영어 데이터 세트를 포함하여 세 개의 TTS 데이터 세트에 대한 성능을 평가합니다.
--- EXPERIMENT ---
세 가지 데이터 세트에 대한 s는 CLAPSpeech가 기존 TTS 방법에 대한 음성 예측을 개선할 수 있을 뿐만 아니라 여러 언어와 다중 화자 TTS에 적응하는 일반화 능력을 입증합니다. 또한 CLAPSpeech 성능의 원리를 심층적으로 분석합니다. 절제 연구는 방법에서 각 구성 요소의 필요성을 입증합니다. 소스 코드와 오디오 샘플은 https://clapspeech.github.io에서 제공됩니다. 1 서론 딥 러닝의 발전으로 현대 TTS 시스템의 오디오 품질이 개선되었지만 음성 모델링은 여전히 어려운 문제입니다. 표현형 TTS에 대한 이전 작업에서는 외부 변이 예측기(예측 기반, PB)를 활용했습니다. *동등한 기여. *연락 저자. Yi Ren ren.yi@bytedance.com Bytedance Jinzheng He jinzhenghe@zju.edu.cn Zhejiang University Zhou Zhao+ zhaozhou@zju.edu.cn Zhejiang University (Ren et al., 2021a) 및 변형 생성 모델(변형 기반, VB)(Kim et al., 2020; Liu et al., 2022)을 사용하여 TTS 모델에 음성적 분산을 주입합니다. 또 다른 인기 있는 방향은 음성적 예측을 위해 더 나은 텍스트 표현을 학습하는 것입니다(Tan et al., 2021). 그러나 TTS를 위한 기존 텍스트 표현 학습 방법은 마스크 언어 모델 작업(Devlin 등, 2019; Jia 등, 2021; Chen 등, 2021)(즉, 텍스트 코퍼스에서 BERT와 유사한 대규모 언어 모델을 학습) 또는 마스크 음향 모델 작업(Chen 등, 2020; Bai 등, 2022)(즉, 입력 텍스트를 기반으로 마스크된 멜 스펙트로그램을 재구성)에 기반을 두고 있으며, 이로 인해 두 가지 단점이 있습니다. 첫째, 재구성 손실로 암묵적으로 음성학을 학습하기 때문에 모델이 음성학 모델링을 개선하는 데 방해가 됩니다. 둘째, 발음 공간과 음성학 공간을 분리하지 않아 학습 효율성이 낮고 모델 용량이 낭비됩니다. 4.3.1절에서 사례 연구를 수행했는데, 이를 통해 TTS에서 사용된 이전 텍스트 표현은 다른 텍스트 맥락에서 음성학 분산을 포착할 수 없다는 것을 알 수 있습니다. 기술적으로, 음조는 서로 다른 조건(예: 텍스트 맥락 및 화자)에서 동일한 토큰의 피치 및 지속 시간 변화로 간주될 수 있습니다(Tan et al., 2021). 이 논문은 주로 텍스트 맥락과 관련된 음조를 연구합니다. 예를 들어, 동일한 단어 &quot;higher&quot;에 대해 &quot;higher up&quot; 또는 &quot;slightly higher&quot;라고 말하면 서로 다른 음조가 나올 수 있습니다. 영어: 텍스트-이미지 작업(Radford et al., 2021; Elizalde et al., 2022)에서 최근의 교차 모달 대조 학습 작업에서 영감을 얻어 텍스트-음성 조인트 멀티 모달 공간에서 텍스트 컨텍스트와 고수준 음성 패턴을 연결하는 대조 학습 방법을 제안합니다.즉, 대조 언어-오디오 사전 학습 선택된 토큰 &quot;더 높은&quot; 토큰 인덱싱 토큰 인코딩을 포함하는 텍스트는 컨텍스트 정보가 더 위에 있으면 약간 더 높은 텍스트 텍스트 인코딩 연령... 인코더가 점점 더 높아지는 대규모 ASR 데이터 세트의 텍스트-음성 쌍 음성 인코더 555-S₁ S₁T ST₂ S₁ T... S₁. TN Sz ST S₂ Tz Sp. T... Sp. T .. B ☐ S SL SL SL ST .. ... ... 선택된 토큰의 상위 음성 세그먼트 &quot;상위&quot; 음성 인코딩과 로컬 음성 정보 SN Sw T₁ Sw T₂ SN T ... SN - TN 그림 1: CLAPSpeech의 대조적 사전 학습 프로세스. 명확성을 위해 여기서는 단어 수준의 사전 학습만 보여줍니다. 또한 음소 수준의 사전 학습도 수행합니다. 텍스트 음성 변환(CLAPSpeech)을 위해. 구체적으로, 텍스트 컨텍스트에서 음성을 예측하는 텍스트 인코더와 선택된 토큰의 음성 세그먼트에서 기준 진실(GT) 음성을 추출하는 음성 인코더를 학습합니다. 학습하는 동안 동일한 발음 가능 토큰(예: 단어 &quot;상위&quot; 또는 음소 &quot;AEO&quot;)이 포함된 N개의 텍스트-음성 쌍을 선택합니다. 텍스트 토큰을 해당 음운론(GT 음성에서 추출)과 정렬하고 음운론 표현을 다른 텍스트 컨텍스트에서 밀어냄으로써 텍스트 인코더는 텍스트 컨텍스트에서 음운론을 추출하도록 장려됩니다. CLAPSpeech 사전 학습의 직관적인 예는 그림 1에서 찾을 수 있습니다. 또한 음운론 패턴은 여러 수준에서 표현될 수 있음을 관찰합니다. 따라서 음소 및 단어 수준에서 각각 음운론 정보를 캡처하기 위해 두 개의 CLAPSpeech 모델을 학습하는 다중 스케일 사전 학습 프레임워크를 제안합니다. 사전 학습 단계 후, CLAPSpeech는 모든 TTS 모델에 적용 가능한 플러그인 텍스트 인코더로 간주되어 세분화된 음운론 표현을 제공할 수 있습니다. 우리 접근 방식의 효과성과 일반화 가능성을 증명하기 위해, 우리는 두 개의 대규모 자동 음성 인식(ASR) 데이터 세트(영어의 경우 LibriSpeech(Panayotov 등, 2015) 및 중국어의 경우 WenetSpeech(Zhang 등, 2022))를 사용하여 CLAPSpeech 모델을 사전 학습시킵니다. 그런 다음 CLAPSpeech의 사전 학습된 텍스트 인코더를 예측/변동 기반 TTS 베이스라인에 연결하여 기존의 표현 TTS 시스템에 대한 CLAPSpeech의 개선을 보여줍니다. 그런 다음 단일 화자 영어 데이터 세트 하나, 단일 화자 중국어 코퍼스 하나, 다중 화자 영어 데이터 세트 하나를 포함하여 세 개의 TTS 데이터 세트에 대한 성능을 평가합니다. 모든 데이터 세트에 대한 실험은 CLAPSpeech가 TTS 모델의 음성학을 개선하고 이전의 표현 학습 방법보다 성능이 우수함을 보여줍니다. 요약하자면 CLAPSpeech에는 세 가지 뛰어난 장점이 있습니다. 1) 대조적 목적 덕분에 프로소디를 명시적으로 학습하여 훨씬 더 작은 모델 규모로 이전의 표현 학습 방법보다 더 나은 프로소디 표현을 제공할 수 있습니다. 2) CLAPSpeech의 텍스트 표현은 프런트엔드 네트워크 아키텍처를 약간 수정하기만 하면 기존 TTS 시스템에서 편리하게 사용할 수 있습니다. 3) 또한 섹션 4.3.2에서 세밀한 프로소디 전송과 같은 잠재적 응용 프로그램을 보여줍니다. 2 관련 연구 2. 표현적 TTS 지난 몇 년 동안 현대 신경 TTS는 높은 실용성과 오디오 품질 면에서 상당한 진전을 이루었습니다(Ren et al., 2019; Kim et al., 2020; Elias et al., 2021; Miao et al., 2021; Kim et al., 2021; Donahue et al., 2021; Jiang et al., 2022). 그러나 일반 입력 텍스트를 기반으로 표현적 음성학을 모델링하는 것은 여전히 어려운 일입니다.표현적 TTS를 구현하기 위해 일반적인 관행 중 하나는 참조 인코더와 스타일 토큰을 사용하는 것입니다(Wang et al., 2018; Jia et al., 2018).그러나 추론 중에 적절한 참조 오디오를 선택하는 것은 어렵습니다(Tan et al., 2021).다른 연구에서는 고급 네트워크 설계를 사용하여 음성학 모델링을 개선하고자 하며, 이는 두 가지 유형으로 분류할 수 있습니다.(1) 예측 기반(PB) TTS 시스템(Ren et al., 2021a)은 피치 윤곽, 지속 시간 및 에너지와 같은 음성학 속성을 예측하기 위해 여러 외부 예측 변수를 학습합니다.(2) 변이 기반(VB) TTS 시스템은 변이 자동 인코더(VAE)(Ren et al., 2021b) 또는 정규화 흐름(Kim et al., 2020)을 활용하여 잠재 공간에서 음성학을 모델링합니다. 또한, 음성 예측을 돕기 위해 풍부한 사전 지식으로 더 나은 텍스트 표현을 제공하는 것을 탐구하는 몇몇 연구도 있습니다.예를 들어, Liu et al.(2021)과 Ye et al.(2022)은 그래프 네트워크와 같은 전담 모델링 방법을 통해 구문 정보를 통합합니다.텍스트 사전 학습 및 음성 사전 학습을 위한 표현 학습 방법도 TTS의 음성에서 개선을 보여줍니다.다음 섹션에서 TTS를 위한 표현 학습 작업에 대해 논의합니다.2.2 TTS를 위한 표현 학습 TTS에서는 자체 감독 사전 학습 방법이 텍스트 처리 또는 음성 생성 기능을 향상시키기 위해 활용되었습니다(Chung et al., 2019; Zhang et al., 2019).일부 초기 연구(Wang et al., 2015)는 사전 학습된 단어 임베딩을 사용하여 TTS 시스템의 견고성을 개선합니다. 최근, 일부 연구에서는 웹스케일 텍스트 코퍼스에서 학습한 풍부한 의미 정보를 활용하기 위해 사전 훈련된 대규모 마스크 언어 모델(MLM)을 통합하는 방법을 모색하고 있습니다(Devlin 등, 2019; Chen 등, 2021; Jia 등, 2021). 그러나 위에서 언급한 연구는 텍스트 공간에만 초점을 맞추고 있어, 모델이 음성 공간에서 가변성이 높은 음성 패턴을 인식하지 못한다는 점을 고려할 때 표현적 음성을 모델링하는 데 어려움이 있습니다. ASR에는 여러 가지 영감을 주는 음성 표현 학습 방법이 있습니다. Baevski 등(2020)과 Hsu 등(2021)은 마스크된 연속 음성 특징을 활용하여 미리 정해진 클러스터 할당을 예측합니다. TTS의 경우, ProsoSpeech(Ren 등, 2022)는 음성에서 이산 음성 표현을 추출하기 위해 단어 수준 벡터 양자화 병목 현상을 설계합니다. 마스크 음향 모델(MAM)(Chen et al., 2020)은 연속적인 음성(음성) 표현을 생성하는 음성 인코더를 학습하는 것을 제안합니다. 구체적으로, 학습하는 동안 음성 스펙트로그램의 범위를 마스크 토큰으로 대체하고 텍스트 조건 없이 마스크된 스펙트로그램을 복구하는 방법을 학습합니다. A³T(Bai et al., 2022)는 또한 MAM이 마스크된 멜 스펙트로그램을 재구성하기 위한 보조 정보로 텍스트 인코더를 학습합니다. TTS에서 CLAPSpeech와 이전 표현 작업의 차이점은 분명합니다. 이전 작업이 마스크된 토큰 재구성 작업으로 음성 정보를 암묵적으로 학습하는 반면, CLAPSpeech는 교차 모달 대조 학습을 사용하여 컨텍스트와 관련된 음성을 명시적으로 학습하는 최초의 작업으로, 더 나은 음성 예측과 모델 용량의 더 효율적인 사용으로 이어집니다. 3 CLAPSpeech 우리는 TTS에서 음성 예측을 위한 더 나은 텍스트 표현을 제공하기 위한 교차 모달 대조 학습 접근 방식인 CLAPSpeech를 제안합니다. 그림 1에서 볼 수 있듯이 CLAPSpeech는 텍스트 인코더와 프로소디 인코더로 구성되어 있으며, 그 훈련 목적은 공동 프로소디 공간에서 텍스트 토큰과 음성 세그먼트를 연결하는 것입니다. 이 섹션에서는 먼저 이 두 인코더의 네트워크 구조와 입력 기능을 설계합니다. 이러한 정교한 설계를 통해 텍스트 인코더는 텍스트 컨텍스트를 효과적으로 처리하고 프로소디 인코더가 음색과 같은 다른 변수를 제거하면서 음성 세그먼트에서 고수준 프로소디 패턴을 추출하는 데 집중할 수 있습니다. 그런 다음 CLAPSpeech가 음소와 단어 수준 모두에서 프로소디를 포착할 수 있도록 하는 다중 스케일 대조 사전 훈련 프레임워크를 소개합니다. 마지막으로 CLAPSpeech의 사전 훈련된 텍스트 인코더를 최신 TTS 시스템에 편리하게 플러그인하여 프로소디 예측을 개선하는 방법을 보여줍니다. 다음 하위 섹션에서 이러한 설계를 자세히 설명하고 부록 A에서 더 자세한 기술적 세부 정보를 제공합니다. 3.1 텍스트 인코더와 프로소디 인코더 동일한 발음 가능 토큰¹의 프로소디는 텍스트 컨텍스트에 따라 다릅니다. CLAPSpeech는 텍스트 컨텍스트와 상위 수준 음성 패턴 간의 상관 관계를 모델링하는 것을 목표로 합니다. 이를 위해 텍스트 인코더와 음성 인코더를 설계하여 텍스트-음성 다중 모달 음성 임베딩 공간을 구성합니다. 그림 2(a)에서 볼 수 있듯이 텍스트 인코더는 입력 텍스트의 음소 및 바이트 쌍 인코딩(BPE)(Shibata et al., 1999)을 입력으로 사용합니다. 음소 및 BPE 시퀀스는 모델이 음소 &quot;AEO&quot; 또는 단어 &quot;higher&quot;와 같은 ¹ 것을 추출하는 데 도움이 됩니다. 토큰 인코딩 ↑ [N, C] 토큰 인덱싱 [N, T, C] (WP) 단어 수준 텍스트 인코딩 FFT 음성 인코딩 [N, C] 주의 풀링 1D [N, T, C] Conv1D+ x 3 x 음소 수준 LN + ReLU WordPool (단어 수준 시퀀스)에는 WP가 없습니다 (음소 수준 시퀀스) HH AE1 Z❘N EH1 V ERO (Word2Ph) + Word2Ph FFT ↑ FFT 음소 임베더 BPE 임베더 음소 BPE (a) 텍스트 인코더 Conv1D+LN + ReLU × 음성 세그먼트 (b) 음성 인코더 그림 2: CLAPSpeech의 텍스트/음성 인코더. 하위 그림 (a)에서 &quot;WP&quot;와 &quot;Word2Ph&quot;는 단어 풀링과 Word2Ph 확장 연산을 나타내며, 이는 그림 3에 설명되어 있습니다. 음운적 습관(예: 영어의 연결 현상)과 의미 정보(다른 감정적 의미를 의미할 수 있음)와 관련된 음운 패턴입니다. 텍스트 인코더의 네트워크 구조는 여러 개의 피드 포워드 변환기(FFT)(Vaswani et al., 2017)로 구성되어 있으며, 이는 TTS 모델에서 긴 텍스트 시퀀스를 처리하는 데 있어 견고함을 입증했습니다. 구체적으로, 우리는 각각 음소와 BPE 시퀀스를 처리하기 위해 두 개의 독립적인 FFT 블록을 학습합니다. 이런 방식으로 음소 FFT 블록은 음성 공간에서 음운적 습관을 모델링할 수 있고, BPE FFT 블록은 의미 정보를 추출할 수 있습니다. 한 가지 어려움은 길이가 일치하지 않는 음소와 BPE 시퀀스를 융합하는 것입니다. 시간 축에서 이 두 시퀀스를 연결하는 대신, Ren et al.의 단어 수준 풀링(WP)을 사용합니다. (2021b) BPE 인코딩을 단어 수준까지 처리한 다음 음소 수준까지 확장합니다(즉, word2ph 연산). 구체적으로, 그림 3(a)에 표시된 대로 WP 연산은 단어 경계에 따라 각 단어 내부의 음소 숨김 상태를 평균화하고 word2ph 연산은 그림 3(b)에 설명된 대로 단어 경계 내부의 각 음소에 대해 단어 숨김 상태를 반복합니다. 음소 시퀀스와 BPE 시퀀스가 융합되면 추가 FFT 블록을 사용하여 정렬된 음소와 BPE 인코딩을 융합하여 최종 음소 수준 텍스트 인코딩을 얻습니다. 사전 학습 단계에서는 선택된 토큰이 하나만 분석되므로 음소 수준 텍스트에서 enPhoneme/BPE 인코더 HH AE1 Z❘N EH1 V ERO(음소/BPE 수준 시퀀스)를 인덱싱합니다. (a) 단어 풀링은 (단어 수준 시퀀스) (b) Word2Ph 확장 그림 3: 단어 풀링 및 word2ph 확장 연산. 코딩을 통해 선택된 토큰의 인코딩(즉, 그림 2(a)의 토큰 인코딩)을 얻은 다음 이를 다중 모달 임베딩 공간에 선형적으로 투영합니다.TTS 단계에서 텍스트 인코더의 음소 수준 출력은 TTS 시스템의 보조 기능으로 편리하게 활용할 수 있으며, 이에 대해서는 섹션 3.3에서 설명합니다.음성 인코더는 선택된 토큰의 GT 음성 세그먼트에서 음성 패턴을 추출하는 것을 목표로 합니다.따라서 단어 경계²를 입력 음성 기능으로 사용하여 멜 스펙트로그램을 클리핑합니다.그런 다음 음성 인코더는 입력 멜 스펙트로그램을 토큰 인코딩과 연결할 글로벌 인코딩으로 처리합니다.클리핑된 음성 세그먼트에는 문맥 정보가 누출되지 않고 선택된 토큰에 대한 로컬 음성 정보만 포함됩니다.대조 학습 설정 덕분에 추출된 글로벌 음성 인코딩은 음성 및 화자 공간에서 분리됩니다.1) 양성 샘플과 음성 샘플이 동일한 발음 가능 토큰에 속하므로 음성 정보가 제거됩니다. 2) 화자 정보가 텍스트 인코더³에 제공되지 않으므로, 프로소디 인코더는 학습 중에 출력 피처에서 프로소디 정보를 최대화하기 위해 화자 정보를 필터링합니다. 이런 식으로, 문맥 인식 텍스트 인코딩을 문맥 인식하지 못하는 멜 인코딩과 연결함으로써, 한편으로는 프로소디 인코더가 음성 세그먼트에서 고수준 프로소디 정보를 추출하는 법을 배우고, 다른 한편으로는 텍스트 인코더가 텍스트 문맥을 활용하여 2강제 정렬 도구로 단어 경계를 추출합니다.3 데이터 세트에서 텍스트와 화자는 서로 독립적(서로 상관 관계가 없음)이라고 가정합니다.프로소디 인코더가 추출한 프로소디.그림 2(b)에서 볼 수 있듯이, 견고성 때문에 프로소디 인코더의 백본으로 ResNet-50(He et al., 2016)을 사용합니다.원래 버전에 여러 가지 수정을 가합니다.1) 멜스펙트로그램을 더 잘 처리하기 위해 레이어 정규화를 사용한 1D 합성곱을 사용하여 기본 잔차 블록을 구축합니다. 2) 동적 길이의 음성 세그먼트를 처리하기 위해 Radford et al.(2021)의 주의 풀링 계층을 사용하여 ResNet의 출력 피처 맵을 집계합니다.3. 다중 스케일 대조 사전 학습 CLAPSpeech의 핵심 아이디어는 서로 다른 맥락에서 동일한 텍스트 토큰의 음성 분산을 모델링하는 것입니다.따라서 대조 사전 학습을 위한 미니 배치를 구성하기 위해 텍스트 토큰을 무작위로 선택한 다음 선택한 토큰을 포함하는 N개의 텍스트-음성 쌍의 배치를 샘플링합니다(직관적인 샘플 중 하나는 그림 1에 나와 있으며, 여기서 &quot;higher&quot;라는 단어가 포함된 텍스트-음성 쌍을 샘플링합니다).음소 및 단어 수준에서 음성 분산을 더 잘 추출하기 위해 다중 스케일 대조 학습 프레임워크를 도입합니다.구체적으로 음소 수준 및 단어 수준 텍스트 토큰에 대해 각각 두 개의 CLAPSpeech 모델을 학습합니다.명확성을 위해 먼저 음소 수준 CLAPSpeech의 학습 프로세스를 설명합니다. 선택된 음소 토큰(예: &quot;AEO&quot;)이 포함된 텍스트 컨텍스트를 Xtext로 표현합니다. 음소 토큰의 처리된 음성 세그먼트를 X speech st X speech ERFXT로 표현합니다. 여기서 F는 Mel 빈의 수이고 T는 시간 빈의 수입니다. 단순화를 위해 Xtext와 X speech를 사용하여 N개의 텍스트-음성 쌍의 배치를 표현합니다. 텍스트와 음성은 각각 텍스트 인코더 ftext(•)와 음성 인코더 fspeech(·)를 통해 전달됩니다. 그림 2(a)에서 볼 수 있듯이 텍스트 인코더 ftext(Xtext)의 출력은 입력 텍스트의 음소 수준 인코딩이므로 이를 인덱싱하여 음소 토큰 ftext(Xtext)iph의 인코딩을 얻습니다. 여기서 iph는 음소 수준 텍스트 시퀀스에서 음소 토큰의 인덱스를 나타냅니다. 그림 2(b)에서 볼 수 있듯이 출력 음성 인코딩 f speech(X speech)는 입력 음성 세그먼트의 전역 표현입니다. 출력 표현은 정규화된 다음 다중 모달 임베딩 공간으로 선형적으로 투영됩니다.Tph = Ltext (LN(ftext (Xtext)iph)) S = Lspeech (LN(f speech (Xspeech))), (1) 여기서 Tph = RNC는 음소 토큰 표현이고 SЄ RNC는 채널 크기 C의 음성 표현입니다.LN은 계층 정규화를 의미하고 Ltext 및 Lspeech는 선형 투영입니다.이제 텍스트 및 음성 임베딩이 비교 가능하므로 CLAPSpeech는 배치에서 N × N개의 가능한 텍스트-음성 쌍 중 실제로 발생한 것을 예측하도록 학습됩니다.특히, 텍스트 인코더와 음성 인코더는 배치의 N개 실수 쌍의 텍스트 및 음성 인코딩의 코사인 유사성을 최대화하는 동시에 N² – N개의 잘못된 쌍의 임베딩의 코사인 유사성을 최소화하도록 권장됩니다.Radford et al.에 따라 (2021), 우리는 이러한 유사도 점수에 대한 대칭적 교차 엔트로피 손실을 최적화합니다: = 0.5× (ltext (T.Cph)+1 speech (T.Cph)) (2) Lph 여기서 Cph ЄRNXN은 Cph Tph ST로 측정한 음소 토큰 인코딩 Tph와 음성 인코딩 S 간의 코사인 유사도 행렬입니다. T는 로짓 범위를 조정하는 학습 가능한 온도 매개변수입니다. lk 1 NA log diag(softmax(C))는 C의 텍스트 및 음성 축을 따라 교차 엔트로피 함수입니다. i== 단어 수준 CLAPSpeech도 비슷하게 학습할 수 있습니다. 그림 2(a)에서 볼 수 있듯이 단어 수준 CLAPSpeech의 경우 단어 풀링을 사용하여 음소 수준 텍스트 인코딩을 단어 수준으로 처리한 다음 이를 인덱싱하여 단어 토큰 인코딩 Tword를 얻습니다. 방정식 2와 유사하게, 단어 수준 CLAPSpeech의 학습 손실은 다음과 같이 공식화됩니다. Lword = 0.5×(ltext (T.Cword)+1 speech (T· Cword)) (3) 여기서 Cword는 단어 토큰 인코딩 Tword와 음성 인코딩 S 간의 코사인 유사성 행렬입니다. 3.3 TTS 시스템에 플러그인된 CLAPSpeech CLAPSpeech의 텍스트 인코더는 TTS 작업을 위해 풍부한 음성 정보가 포함된 텍스트 표현을 제공할 수 있습니다. 생성된 텍스트 표현은 음소 수준에 있으며, 이는 텍스트 입력으로 음소 시퀀스를 활용하는 대부분의 현재 TTS 모델과 일치하므로 CLAPSpeech는 음성 예측을 개선하기 위한 TTS 시스템의 편리한 플러그인 단위가 될 수 있습니다. 구체적으로, 최첨단 변형 기반 TTS 시스템인 PortaSpeech를 예로 들어 보겠습니다. 그림 4에 표시된 대로, CLAPSpeech의 사전 학습된 텍스트 인코더(빨간색 점선 사각형으로 표시)는 K, V 다중 길이 판별기 변이 생성기 단어-음소 주의 지속 시간 예측기 음소 수준 결합 인코딩 단어 수준 지속 시간 LR) 단어 인코더 WP) 인코더 확장 중지 기울기 음성 인코더 단어 수준 CLAPSpeech 음소 수준 CLAPSpeech 음소 BPE 그림 4: CLAPSpeech가 플러그인된 PortaSpeech. PortaSpeech의 원래 음성 인코더에 대한 보조 인코더. 음성 인코더와 CLAPSpeech 텍스트 인코더의 음소 수준 출력은 다음 인코더에서 융합되어 처리됩니다. 과적합을 방지하기 위해 TTS 시스템을 학습하는 동안 CLAPSpeech 텍스트 인코더의 매개변수를 고정한다는 점에 유의하세요. CLAPSpeech는 비슷한 방식으로 다른 TTS 시스템에 쉽게 플러그인할 수 있습니다. 보편성을 보여주기 위해 부록 A.1에서 CLAPSpeech를 널리 사용되는 예측 기반 TTS 시스템인 FastSpeech 2와 결합하는 방법을 설명합니다. 또한 오디오 품질을 개선하기 위해 TTS 모델에서 다중 길이의 적대적 훈련을 채택합니다. 적대적 훈련에 대한 자세한 내용은 부록 A.2에서 확인할 수 있습니다. 4 실험 4.1 실험 설정 데이터 세트 및 기준선 두 가지 ASR 데이터 세트에서 CLAPSpeech를 사전 훈련합니다. 1) 2,484명의 화자의 982시간 분량의 음성을 포함하는 영어 데이터베이스인 LibriSpeech(Panayotov et al., 2015); 2) 10,000시간 분량의 음성으로 구성된 중국어 음성 코퍼스인 WenetSpeech(Zhang et al., 2022). 그런 다음 세 가지 TTS 데이터 세트에서 사전 훈련된 CLAPSpeech를 평가합니다. 1) 단일 화자 데이터베이스인 LJSpeech(Ito and Johnson, 2017)는 총 약 24시간 분량의 음성이 포함된 13,100개의 영어 오디오 클립을 포함합니다. *정확도 신뢰 수준이 0.95 이상인 샘플을 필터링하여 최종적으로 1000시간 분량의 하위 세트를 얻습니다. 2) 중국어 화자의 10,000개 문장(약 12시간)으로 구성된 중국어 음성 코퍼스인 Biaobei5 3) 1,151명의 화자의 149,736개 오디오 클립(약 245시간)이 포함된 영어 데이터 세트인 LibriTTS(Zen et al., 2019)(train clean360만 사용하고 clean100도 훈련합니다). 원시 텍스트는 오픈 소스 도구를 사용하여 음소와 BPE 시퀀스로 변환됩니다. GT 멜 스펙트로그램은 프레임 크기와 홉 크기가 256인 원시 파형에서 생성됩니다. 예측 기반(PB) TTS 모델인 FastSpeech 2와 변형 기반(VB) TTS 모델인 PortaSpeech에서 CLAPSpeech를 두 개의 사전 학습 베이스라인(BERT(Devlin et al., 2019) 및 A³T(Bai et al., 2022))과 비교합니다. 모델 구성 CLAPSpeech는 텍스트 인코더와 프로소디 인코더로 구성되며, 구조는 그림 2에 표시되어 있으며 섹션 3.2에서 설명합니다. PB 및 VB TTS 모델의 경우 원본 논문의 동일한 구조를 사용하지만 오디오 품질을 개선하기 위해 다중 길이 판별기를 추가로 사용합니다. 다중 길이 판별기는 배치 정규화가 적용된 여러 개의 쌓인 합성곱 계층으로 구성되며 입력 스펙트로그램을 이미지로 처리합니다. 부록 B.1에 더 자세한 모델 구성을 제공합니다. 훈련 및 평가 우리의 접근 방식은 Pytorch로 구현됩니다.우리는 1,024개의 텍스트-음성 쌍(GPU당 256개의 쌍)의 배치 크기로 4개의 Nvidia 3090Ti GPU에서 CLAPSpeech를 사전 훈련합니다.우리는 초기 학습 속도가 0.0005인 Adam 옵티마이저를 사용합니다.우리는 640,000번의 반복(약 1주일 소요) 동안 CLAPSpeech 모델을 훈련하고 CLIP의 코사인 학습 속도 일정을 따릅니다.그런 다음 우리는 Vaswani et al.(2017)의 학습 속도 일정을 따라 64개의 문장의 배치 크기로 1개의 Nvidia 2080Ti GPU에서 TTS 모델을 훈련합니다.우리는 보코더로 HiFi-GAN(Kong et al., 2020)을 사용합니다.우리는 음성과 오디오 품질을 측정하기 위해 평균 의견 점수(MOS) 및 비교 평균 의견 점수(CMOS) 평가를 수행합니다. 주관적 평가에 대한 자세한 내용은 부록 B.2에서 확인할 수 있습니다. 객관적인 평가의 경우 Ren et al. (2021b)에 따라 피치와 지속 시간 측면에서 음성을 평가합니다. 1) GT 음성과 합성 음성의 피치 윤곽선 사이의 평균 동적 시간 워핑(DTW)(Muller, 2007) 거리를 계산하여 피치 정확도를 측정합니다. 2) 마이크로초 단위로 평균 절대 지속 시간 오류(DE)를 계산하여 지속 시간 정확도를 측정합니다. 4.2 성능 PB/VB TTS 모델에서 CLAPSpeech의 성능을 BERT 및 A³T와 비교합니다. GT(실제 오디오)와 GT(voc.)(GT 멜라 스펙트로그램을 사용하여 보코더에서 생성한 오디오 파형)도 실험에 포함됩니다. 섹션 4.1에서 언급한 대로 세 가지 데이터 세트에서 TTS 실험을 수행합니다. 결과는 표 1에 나와 있습니다. CLAPSpeech가 MOS, 피치 정확도, 지속 시간 정확도 측면에서 PB 및 VB TTS 기준선에서 다른 표현 학습 방법보다 성능이 뛰어난 것을 볼 수 있습니다. 이는 CLAPSpeech가 현재 표현형 TTS 모델(예측 기반이든 변형 기반이든)에서 음성 예측을 효과적으로 개선할 수 있음을 증명합니다. 게다가 CLAPSpeech가 훨씬 적은 모델 매개변수로 BERT 및 A³T보다 더 나은 성능을 달성하는 것을 관찰합니다. 이는 MLM 기반 방법(즉, BERT)이 의미 정보를 저장하기 위해 큰 모델 용량이 필요하고 MAM 기반 방법(즉, A³T)이 마스크된 멜 스펙트로그램을 재구성하기 위해 음성 정보를 함께 학습해야 하기 때문이라고 생각합니다. 이와 대조적으로 CLAPSpeech는 음성 공간을 제거하고 사전 학습 중에 음성 공간에만 초점을 맞추므로 매개변수 효율적입니다. 그런 다음 그림 5에서 다양한 방법으로 생성된 멜 스펙트로그램을 시각화합니다. CLAPSpeech가 더욱 사실적인 피치 윤곽을 갖는 결과를 생성할 수 있으며, 이는 표현적인 프로소디를 생성합니다.
--- CONCLUSION ---
, 우리의 실험은 CLAPSpeech가 TTS 시스템이 보다 표현력 있고 음성적인 오디오를 합성하는 데 도움이 될 수 있음을 보여줍니다.4.3 심층 분석 4.3.1 토큰 표현 자기 유사성 TTS를 위한 기존 표현 학습 방법에 비해 CLAPSPeech의 성능 우수성을 더 잘 이해하기 위해 CLAPSpeech 및 기타 방법으로 학습한 토큰 표현을 분석합니다.Su et al. (2021)에 따라 다른 con&#39;에서 선택된 토큰의 평균 유사도를 정의합니다.PB/VB TTS 기준선에서 지속 시간은 각각 음소/단어 수준에서 예측됩니다.텍스트 T = [T1, ..., TN]은 다음과 같습니다.N Ns(T) N(N − 1) ΣΣ cosine(Τι, Τ;) i=1 j=1,j‡i (4) 여기서 T¿와 T¿는 모델이 다른 텍스트 컨텍스트에서 추출한 선택된 토큰의 인코딩입니다. 직관적으로, 낮은 s(T)는 선택된 토큰 자체가 표현을 생성하는 데 더 작은 역할을 한다는 것을 나타내며, 이는 모델이 입력 텍스트 시퀀스에서 더 많은 컨텍스트 관련 정보를 포착하여 더 나은 음조를 예측한다는 것을 의미합니다. 양적 평가 ASR 검증 데이터 세트에서 10,개의 배치(각 배치는 동일한 선택된 토큰을 포함하는 256개의 문장으로 구성됨)를 샘플링하고 평균 자기 유사성을 계산합니다. 결과는 표 2에 나와 있습니다. 우리는 대조 목적(방정식 2)으로 학습한 CLAPSpeech가 유사도 행렬의 비대각선 항목에서 가장 낮은 유사도를 달성하는 것을 관찰합니다. 이는 모델이 텍스트 컨텍스트를 사용하여 동일한 토큰의 음성 분산을 포착하고 표 1에서 가장 좋은 음성 성능을 달성했음을 나타냅니다. 또한 BERT도 비교적 낮은 비대각선 유사도를 달성하는 것을 볼 수 있는데, 이는 사전 학습 중 MLM 작업으로 인해 모델이 마스크된 토큰을 예측하기 위해 컨텍스트에서 의미 정보를 추출해야 하기 때문입니다. 반면 vanilla TTS 텍스트 인코더와 A³T는 낮은 비대각선 유사도를 달성하지 못하는데, 이는 두 모델 모두 서로 다른 컨텍스트에서 차별적인 정보를 추출할 수 없음을 의미합니다. 우리는 A³T의 실패가 MAM 목적이 모델이 입력된 마스크되지 않은 텍스트 시퀀스를 기반으로 마스크된 멜-스펙트로그램 패치를 예측하도록 장려하기 때문이라고 의심합니다.이로 인해 선택된 토큰의 음성 정보에 대한 모델의 수요가 증가합니다.정성적 평가 LibriSpeech에서 &quot;higher&quot;라는 단어가 포함된 8개 문장을 샘플링하고 CLAPSpeech와 vanilla TTS 텍스트 인코더에서 생성된 자기 유사성 행렬 M(여기서 Mi,j = cosine(Ti, T;))을 시각화합니다.결과는 그림 6에 나와 있으며, 어두운 색상일수록 자기 유사성 점수가 더 높습니다.또한 부록 C의 그림에서 BERT와 A³T의 자기 유사성 행렬을 제공합니다.CLAPSpeech의 자기 유사성이 대각선이 아닌 항목에서 훨씬 낮다는 것을 알 수 있습니다.이러한 문장을 부록 C의 표 5에 나열합니다.표 1: 다양한 방법의 성능 비교. PB와 VB는 각각 예측 기반 및 변동 기반 TTS 기준선을 나타냅니다. DTW는 Mel-spectrogram에서 피치 윤곽의 동적 시간 워핑 거리를 나타냅니다. DE는 마이크로초 단위의 평균 절대 지속 시간 오류를 의미합니다. LJSpeech Biaobei LibriTTS 방법 #Params MOS↑ DTW↓ DE↓ MOS↑ DTW↓ DE↓ MOS↑ DTW↓ DE↓ GT 4.4.4.GT(voc.) 4.4.4.PB 3.29.09 25.3.18.PB + BERT 4.27.43 24.97 3.16.PB + + A³T 3.28.18 25.63 3.17.PB+ CLAPSpeech 4.27.24.19 3.28.79 3.28.06 3.28.16.04 27.14.26 27.11.99M 13.82 26.109.48M 3.13.67 27.48.25M 3.13.26.30.51M VB 3.27.53.3.14.22 40.3.VB + BERT 4.26.52.3.VB + A³T 4.26.52.4.VB+CLAPSpeech 4.25.51.4.13.13.13.63 38.41 3.39.15 3.37.07 4.11.96 52.11.51 51.27 132.69M 11.71 51.98 59.73M 10.93 50.89 41.54M 23.02M (a) GT (b) PB (c) PB+BERT E (d) PB+A³T (e) PB+CLAPSpeech 그림 5: 시각화 다양한 TTS 시스템에서 생성된 멜 스펙트로그램. 표 2: 다양한 방법의 자기 유사성 점수. TTS는 vanilla TTS 베이스라인의 텍스트 인코더를 나타냅니다. s8-0.57 0.57 0.84 0.46 0.49 0.52 0.44 1.s8-1.00 1.00 1.00 1.00 0.99 1.00 0.96 1.$7-0.45 0.48 0.52 0.41 0.1.00 0.s7 0.96 0.96 0.96 0.96 0.98 0.96 1.00 0.s6 0.58 0.45 0.50 0.51 0.40 1.0.28 0.s6 - 0.1.00 1.00 1.00 1.00 1.00 0.96 1.$5-0.66 0.50 0.31 0.46 1.00 0.0.37 0.$5 -1.TTS BERT의 텍스트 인코더 АЗТ CLAPSPeech 자기 유사성 0.9854 0.5517 0.s4 -0.61 0.67 0.50 1.00 0.46 0.51 0.41 0.0.50 0.31 0.50 0.52 0.1.00 1.00 0.99 1.00 1.00 0.98 0.s4 1.00 1.00 0.98 1.00 0.99 1.00 0.96 1.s3 1.00 1.00 1.00 0.98 1.00 1.00 0.96 1.0.$3-0.59 0.67 1.$2 0.60 1.00 0.67 0.67 0.50 0.45 0.48 0.s2 1.00 1.00 1.00 1.00 1.00 1.00 0.96 1.s1 1.00 0.60 0.59 0.0.66 0.58 0.45 0.$1 1.00 1.00 1.00 1.00 1.00 0.99 0.96 1.T $2 $3 $$sss1.4.3.2 세분화된 음성학 전송 우리는 CLAPSpeech의 텍스트-음성 공동 다중 모달 공간이 고수준 음성학 패턴을 나타낸다는 것을 더욱 검증하기 위해 음성학 전송에 대한 직관적인 사례 연구를 수행합니다. (즉, 피치 윤곽선과 지속 시간 정보). 우리는 표에서 s7/8을 참조/소스 오디오로 취하고 단어 &quot;higher&quot;의 음조 패턴을 s7에서 s8로 전송할 것으로 예상합니다. 구체적으로, 우리는 CLAPSpeech의 텍스트 인코더를 사용하여 s8의 텍스트 음조 인코딩을 추출한 다음, s8의 &quot;higher&quot;의 텍스트 토큰 인코딩을 s7의 텍스트 토큰 인코딩으로 바꿉니다. 그림 7에서 볼 수 있듯이, 그림 7(a)의 s88의 &quot;higher&quot;의 음조 패턴이 그림 7(c)의 s7로 성공적으로 전송되었습니다. 또한 데모 페이지에서 이 사례 연구의 오디오 샘플을 제공합니다. 로컬 음조의 조작은 우리의 CLAPSpeech ex8 참조의 피치 윤곽이 초기에는 평평하게 유지되다가 후반에는 상승한다는 것을 증명합니다. (a) CLAPSpeech ss$3 $$s6 s7 s0.(b) TTS 그림 6: 예: CLAPSpeech의 자기 유사성 행렬 시각화와 vanilla의 텍스트 인코더 TTS 모델. s;는 i번째 문장을 나타냅니다. 트랙 프로소디 표현은 TTS 시스템의 프로소디 예측에 효과적으로 영향을 미칩니다. 4.4 Ablation 연구는 BPE를 보조 특징으로 사용합니다. 먼저 텍스트 컨텍스트에서 프로소디 정보를 추출하는 데 도움이 되는 보조 특징으로서 BPE의 효과를 분석합니다. CLAPSpeech의 사전 학습 단계에서 텍스트 인코더에서 BPE를 제거하면 검증 CLIP 손실이 0.3692에서 0.6764로 크게 저하되는 것을 발견했습니다. 그런 다음 TTS 단계에서 표 3의 3번째 줄에서 볼 수 있듯이 bhigher higher higher (a) 참조 (s7) (b) 소스 (s8) (c) 전송된 (s8) 그림 7: 프로소디 전송에서 생성된 멜로 스펙트로그램의 시각화. BPE 없이 사전 학습된 텍스트 인코더를 사용하여 변환된 모델은 CMOS, DTW 및 DE 측면에서 성능 저하를 초래합니다. 이는 BPE가 저수준 음소 시퀀스보다 의미 정보를 더 잘 표현할 수 있기 때문일 수 있습니다.다중 스케일 사전 학습 표 3의 4/5행에서 볼 수 있듯이 다중 스케일 사전 학습의 효과를 보여주기 위해 음소 수준 또는 단어 수준의 CLAPSpeech를 모델에서 제거하려고 시도했는데, 이로 인해 음성 성능이 저하되었습니다.또한 사전 학습 프로세스의 필요성을 증명하기 위해 훈련되지 않은 CLAPSpeech를 사용하려고 시도했으며, 이 절제된 모델(6행)이 TTS 기준선(3행)보다 약간 더 나쁜 성능을 달성하는 것을 발견했습니다.표 3: 절제 연구에 대한 성능 비교.TTS +CLAPSpeech 설정 TTS 기준선 |CMOS |DTW DE 27.16 24.-1.53 29.09 25.CLAPSpeech 및 각 구성 요소의 필요성을 증명하기 위해 절제 연구를 수행했습니다. 6 한계점 주로 두 가지 한계점이 있습니다. 첫째, 이 작업에서는 현재 문장 텍스트 맥락 관련 음성학만 고려합니다. 향후 작업에서는 장문 텍스트에 대한 일관되고 표현력 있는 TTS를 달성하기 위해 문장 간 음성학을 개선하는 데 중점을 둘 것입니다. 둘째, 대조 사전 훈련 중에 다른 변수는 고려되지 않습니다. 음성학을 화자, 감정 등과 같은 다른 조건에 연결하는 유사한 접근 방식을 탐색할 수 있습니다. 7 윤리 성명 CLAPSpeech는 합성된 음성의 음성학을 개선하여 관련 직업을 가진 사람들의 실업을 초래할 수 있습니다. 게다가 가짜 연설을 생성하면 음성 보안 문제가 발생할 수 있습니다. 음성 보안을 개선하기 위해 자동 화자 검증에 대한 추가 노력이 이루어져야 합니다. 8 감사의 글 본 연구는 중국 국가중점연구개발계획(Grant No.2022ZD0162000), 중국 국가자연과학기금(Grant No.61836002, Grant No.62072397), Yiwise의 지원을 받아 수행되었습니다. BPE 없음 -1.28.21 24. pH 수준 없음 -1.27.68 25. 단어 수준 없음 -0. 학습되지 않음 -1.27.55 24.29.45 25.5 결론 본 논문에서는 TTS를 위한 풍부한 음성 정보와 함께 더 나은 텍스트 표현을 제공하는 교차 모달 대조 사전 학습 프레임워크인 CLAPSpeech를 제안합니다. 텍스트 인코더와 음성 인코더를 설계하여 CLAPSpeech는 텍스트 컨텍스트와 음성의 해당 음성 패턴을 연결하는 방법을 학습합니다. 또한 다중 레벨에서 음성 패턴을 추출하기 위해 다중 스케일 사전 학습을 도입했습니다. 세 가지 TTS 데이터 세트(각각 영어, 중국어 및 다중 화자)에서 CLAPSpeech의 성능과 일반화 능력을 입증했습니다. 또한 개선의 원리를 심층 분석했습니다. 참고문헌 Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed 및 Michael Auli. 2020. wav2vec 2.0: 음성 표현의 자기 감독 학습을 위한 프레임워크. NIPS에서. He Bai, Renjie Zheng, Junkun Chen, Mingbo Ma, Xintong Li 및 Liang Huang. 2022. A3t: 음성 합성 및 편집을 위한 정렬 인식 음향 및 텍스트 사전 학습. ICML에서. Junkun Chen, Mingbo Ma, Renjie Zheng 및 Liang Huang. 2020. Mam: 종단 간 음성-텍스트 번역을 위한 마스크 음향 모델링. 사전 인쇄본 arXiv:2010.11445. arXiv Liping Chen, Yan Deng, Xi Wang, Frank K Soong, Lei He. 2021. 음성 bert 임베딩을 통한 신경 tts의 음성 개선. ICASSP에 게재. Yu-An Chung, Yuxuan Wang, Wei-Ning Hsu, Yu Zhang, RJ Skerry-Ryan. 2019. 반지도 학습을 통한 엔드투엔드 음성 합성의 데이터 효율성 개선. ICASSP에 게재. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. Bert: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. NAACL-HLT에 게재. Jeff Donahue, Sander Dieleman, Mikołaj Bińkowski, Erich Elsen, Karen Simonyan. 2021. 엔드투엔드 적대적 텍스트-음성 변환. ICLR에 게재. Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron J Weiss, and Yonghui Wu. 2021. Parallel tacotron: Non-autoregressive and controllable tts. ICASSP에서. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. 2022. Clap: 자연어 감독을 통한 오디오 개념 학습. arXiv 사전 인쇄본 arXiv:2206.04769. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. 이미지 인식을 위한 심층적 잔여 학습. CVPR에서. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: 숨겨진 유닛의 마스크된 예측을 통한 자체 감독 음성 표현 학습. 오디오, 음성 및 언어 처리에 관한 IEEE/ACM 거래, 29:3451-3460. 키스 이토와 린다 존슨. 2017. lj 음성 데이터세트. https://keithito.com/LJ-Speech-Dataset/. Ye Jia, Heiga Zen, Jonathan Shen, Yu Zhang, Yonghui Wu. 2021. Png bert: 신경 tts에 대한 음소 및 문자소에 대한 증강된 bert입니다. Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu 등 2018. 화자 검증에서 다중 화자 텍스트 음성 합성으로 학습을 전환합니다. NIPS. Ziyue Jiang, Su Zhe, Zhou Zhao, Qian Yang, Yi Ren, Jinglin Liu 및 Zhenhui Ye. 2022. Dicttts: 사전 사전 지식을 바탕으로 텍스트-음성을 위한 발음 학습. arXiv 사전 인쇄본 arXiv:2206.02147. Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. 2020. Glow-tts: 단조 정렬 검색을 통한 텍스트-음성을 위한 생성 흐름. NIPS에 게재. Jaehyeon Kim, Jungil Kong, and Juhee Son. 2021. 엔드투엔드 텍스트-음성을 위한 적대적 학습을 갖춘 조건부 변이 자동 인코더. ICML에 게재. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: 효율적이고 충실도가 높은 음성 합성을 위한 생성적 적대적 네트워크. NIPS에 게재. Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. 2022. Diffsinger: 얕은 확산 메커니즘을 통한 노래 음성 합성. AAAI에서. Rui Liu, Berrak Sisman, Haizhou Li. 2021. Graphspeech: 신경 음성 합성을 위한 구문 인식 그래프 어텐션 네트워크. ICASSP에서. Chenfeng Miao, Liang Shuang, Zhengchen Liu, Chen Minchuan, Jun Ma, Shaojun Wang, Jing Xiao. 2021. Efficienttts: 효율적이고 고품질의 텍스트 음성 변환 아키텍처. ICML에서. Meinard Muller. 2007. 동적 시간 워핑. 음악 및 모션에 대한 정보 검색, 69-84쪽. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur. 2015. Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스. ICASSP에서. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독을 통한 전이 가능한 시각적 모델 학습. ICML에서. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2021a. Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트-음성 변환. ICLR에서. Yi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang, Qian Chen, Zhijie Yan, and Zhou Zhao. 2022. Prosospeech: 텍스트-음성 변환에서 양자화된 벡터 사전 학습을 통한 음성 향상. ICASSP에서. Yi Ren, Jinglin Liu, and Zhou Zhao. 2021b. Portaspeech: 휴대 가능하고 고품질의 생성 텍스트-음성 변환. NIPS에서. Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu. 2019. Fastspeech: 빠르고 강력하며 제어 가능한 텍스트-음성 변환. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, Setsuo Arikawa. 1999. 바이트 쌍 인코딩: 패턴 매칭을 가속화하는 텍스트 압축 체계. Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, Nigel Collier. 2021. Tacl: 토큰 인식 대조 학습을 통한 bert 사전 학습 개선. arXiv 사전 인쇄본 arXiv:2111.04198. Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu. 2021. 신경 음성 합성에 관한 조사. arXiv 사전 인쇄 arXiv:2106.15561. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. NIPS에서. Peilu Wang, Yao Qian, Frank K Soong, Lei He 및 Hai Zhao. 2015. 순환 신경망 기반 TTS 합성을 위한 단어 임베딩. ICASSP에서. 다중 길이 판별기 Mel-Spec 디코더 지속 시간 및 피치 예측기 결합 인코더 정지 기울기 음소 수준 인코딩 음성 인코더 음소 확장 단어 수준 CLAPSpeech 음소 수준 CLAPSpeech BPE 그림 8: CLAPSpeech가 플러그인된 FastSpeech 2. Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, Rif A Saurous. 2018. 스타일 토큰: 엔드투엔드 음성 합성에서의 비지도 스타일 모델링, 제어 및 전송. ICML에서. Zhenhui Ye, Zhou Zhao, Yi Ren, Fei Wu. 2022. Syntaspeech: 구문 인식 생성적 적대적 텍스트-음성. arXiv 사전 인쇄본 arXiv:2204.11792. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen 및 Yonghui Wu. 2019. Libritts: 텍스트 음성 변환을 위한 librispeech에서 파생된 코퍼스입니다. arXiv 사전 인쇄 arXiv:1904.02882. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng 등 2022. Wenetspeech: 음성 인식을 위한 10000시간 이상의 다중 도메인 중국어 말뭉치. ICASSP에서. Mingyang Zhang, Xin Wang, Fuming Fang, Haizhou Li, Junichi Yamagishi. 2019. 다중 소스 타코트론 및 웨이브넷을 사용한 텍스트 음성 변환 및 음성 변환을 위한 공동 훈련 프레임워크. 인터스피치에서. 모델 세부 정보 A.1 FastSpeech에 플러그인된 CLAPSpeech CLAPSpeech를 인기 있는 예측 기반 TTS 시스템인 FastSpeech 2에 통합하는 방법을 보여드립니다.그림 8에서 볼 수 있듯이 CLAPSpeech의 사전 학습된 텍스트 인코더(빨간색 점선 사각형으로 표시)는 FastSpeech 2의 원래 음성 인코더에 대한 보조 인코더 역할을 합니다.음성 인코더와 CLAPSpeech 텍스트 인코더의 음소 수준 출력은 다음 인코더에서 융합되어 처리됩니다.과적합을 방지하기 위해 TTS 시스템을 학습하는 동안 CLAPSpeech 텍스트 인코더의 매개변수를 고정합니다.A.2 다중 길이 적대적 학습 테스트된 TTS 베이스라인의 경우 추가 다중 길이 판별기를 채택하여 최소 제곱 GAN 손실을 제공하여 오디오 품질을 개선합니다.다중 길이 판별기는 서로 다른 길이의 랜덤 창을 기반으로 멜 스펙트로그램을 평가하는 여러 CNN 기반 판별기의 앙상블입니다. 자세한 내용은 Ye et al. (2022)을 참조할 수 있습니다.B 자세한 실험 설정 B.1 모델 구성 표 4에 CLAPSpeech의 하이퍼 매개변수와 테스트된 TTS 기준선을 나열했습니다.B.2 주관적 평가 테스트된 각 데이터 세트에 대해 테스트 세트에서 무작위로 텍스트를 선택하고 TTS 시스템을 사용하여 오디오 샘플을 생성합니다.각 오디오는 크라우드소싱 플랫폼인 Zhengshu Technology에서 모집한 최소 20명의 모국어 청취자가 들었습니다.청취자에게 &quot;음조(예: 피치, 에너지, 지속 시간)와 오디오 품질(잡음, 음색, 사운드 선명도, 고주파 세부 정보)의 자연스러움을 조사하는 데 집중하세요&quot;라고 말합니다.MOS의 경우 각 테스터는 1~5 리커트 척도로 문장의 주관적 자연스러움을 평가하도록 요청받습니다. CMOS의 경우 청취자에게 시스템 A와 B에서 생성된 오디오 쌍을 비교하고 두 오디오 중 어느 것을 선호하는지 표시하고 다음 점수 중 하나를 선택하도록 요청합니다.0은 차이 없음, 1은 작은 차이, 2는 큰 차이, 3은 매우 큰 차이를 나타냅니다.C 분석에 대한 자세한 내용 C.예시 문장 표 5에 8개의 예문 문장을 나열했습니다.이 문장들은 섹션 4.3에서 예로 사용됩니다.C.2 다른 기준선의 자기 유사성 A³T와 BERT의 자기 유사성 시각화는 그림 9에서 찾을 수 있습니다.4.3.1절에서 결과에 대해 논의합니다.표 4: 자세한 모델 구성 하이퍼 매개변수 음소/BPE 임베딩 숨김 크기 음소/BPE 인코더 FFT 블록 텍스트 인코더 숨김 크기 Conv1D 커널 음성 인코더 Conv1D 필터 크기 잔여 블록 블록당 conv 레이어 수 숨김 크기 입력 mel-spectrogram 길이 풀링 레이어의 숨김 크기 #풀링 레이어의 어텐션 헤드 클랩 음성 매개변수 수 2+258 +222 +18.517M 21.801M 인코더 레이어 디코더 레이어 예측 기반 TTS 기준선 11.993M 인코더/디코더 Conv1D 커널 인코더/디코더 Conv1D 채널 크기 인코더 레이어 디코더 레이어 변형 기반 TTS 기준선 이전 흐름 레이어 인코더/디코더 Conv1D 커널 인코더/디코더 Conv1D 채널 크기 잠재 크기 23.020M 이전 흐름 Conv1D 커널 이전 흐름 Conv1D 채널 크기다중 길이 판별기 CNN 기반 판별기 수 창 크기 Conv2D 레이어 숨김 크기32,64,0.927Msl s$s$· 엄격한 판사의 평판은 자비로운 판사의 평판보다 높지 않습니다... 계속 나아가면서 절벽이 더 높아지고 튀어나온 것처럼 보였습니다. 수로가 좁아졌습니다... 더 좋아지고, 더 좋아지고, 더 좋아졌습니다! 그녀의 목소리는 더 좋아질 때마다 더 높아졌고 마침내 삐걱거리는 소리가 났습니다. 그리고 우리 고등 교육 기관의 토박이 졸업생들은 그들의 힘을 보여주기 시작했습니다... 무고함은 미덕보다 더 높습니다. s6 삶에 더 깊은 의미와 더 높은 가치를 부여하기에 더 어울리지 않는 것은 없습니다. $7 더 높은 곳에서 중국인들이 보였지만, 그들이 낚시를 했는지 세탁을 했는지 알 수 없었습니다. s8 그들이 회복하고 극복하는 사람이 되어 스스로 더 높은 몸을 만들기를 바랍니다! 표 5: 직관적 예에서 사용된 텍스트 문장, 선택된 단어 토큰 &quot;higher&quot;는 굵게 표시되어 있습니다. s8 0.0.98 0.0.99 1.00 1.00 0.90 1.s8 0.0.72 0.62 0.71 0.71 0.76 0.53 1.s7 0.94 0.91 0.94 0.92 0.90 0.90 1.00 0.s7 0.42 0.52 0.53 0.51 0.53 0.50 1.00 0.s6 0.0.98 0.98 0.96 1.00 1.00 0.90 1.s6 0.66 0.62 0.53 0.67 0.70 1.00 0.50 0.$5 0.98 0.0.97 0.97 1.1.00 0.1.$5 0.73 0.58 0.60 0.66 1.00 0.70 0.53 0.s4 0.98 0.0.97 1.00 0.97 0.96 0.0.$4 0.60 0.64 0.59 1.00 0.66 0.67 0.51 0.$30.99 0.99 1.00 0.97 0.97 0.98 0.0.s3 - 0.57 0.72 1.00 0.59 0.60 0.53 0.53 0.$2 - 0.99 1.00 0.99 0.99 0.99 0.98 0.0.s2-0.61 1.00 0.72 0.64 0.58 0.62 0.52 0.s1 - 1.00 0.99 0.99 0.98 0.98 0.96 0.94 0.s1 1.00 0.61 0.57 0.60 0.73 0.66 0.42 0.s$$4 $5 s6 ssss(a) A³T $4 $5 ss(b) BERT 0.그림 9: A³T 및 BERT의 자기 유사성 행렬 시각화.
