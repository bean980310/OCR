--- ABSTRACT ---
최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰어난 성능과 일반화 기능으로 인해 연구 커뮤니티에서 상당한 주목을 받았습니다. 이 논문에서는 LLM을 통합한 음성 인식 모델을 문맥화하는 새로운 방법을 소개합니다. 저희의 접근 방식은 음성 인식을 사전 훈련된 LLM을 기반으로 하는 혼합 모달 언어 모델링 작업으로 간주합니다. 저희는 컨텍스트에 대한 선택적 텍스트 토큰과 함께 오디오 기능을 제공하여 시스템이 디코더 전용 방식으로 필사본을 완료하도록 훈련합니다. 결과적으로 시스템은 훈련 중에 비정형적인 컨텍스트 정보를 활용하는 방법을 학습하도록 암묵적으로 인센티브를 받습니다. 저희의 경험적 결과는 추가 텍스트 컨텍스트가 제공될 때 6%의 WER 감소로 성능이 크게 향상되었음을 보여줍니다. 게다가 저희의 방법은 경쟁력 있는 성능을 보이며 전체적으로 7.5%의 WER, 25배 이상 큰 음성 데이터 세트에서 훈련된 기준 컨텍스트화된 RNN-T 시스템에 비해 희귀 단어에 대한 WER이 17% 향상됨을 발견했습니다. 전반적으로, 어댑터를 통해 소수의 훈련 가능한 매개변수만 추가함으로써 사전 훈련된 LLM에 대한 문맥화된 음성 인식 기능을 잠금 해제하는 동시에 동일한 텍스트 전용 입력 기능을 유지할 수 있음을 보여줍니다. 색인 용어 문맥적 편향, 대규모 언어 모델, 음성 인식 1.
--- INTRODUCTION ---
최근 몇 년 동안, 질문 답변 및 요약과 같은 작업에서 성과를 향상시키는 데 탁월한 효능이 있어 대규모 언어 모델(LLM)에 대한 관심이 커지고 있으며, 이는 특수 모델을 능가합니다[1, 2]. LLM은 방대한 양의 텍스트 데이터로 학습되므로 네트워크 내에 풍부한 세계 지식을 캡슐화합니다. 이러한 축적된 지식과 맥락적 이해는 특히 발화 주변의 추가 맥락이 오디오 그 자체를 넘어 사용 가능할 때 자동 음성 인식(ASR) 분야에서 특히 유익한 것으로 입증되었습니다. 예를 들어, 비디오 제목과 설명은 비디오 주제에 대한 통찰력을 제공하거나 언급될 수 있는 명명된 엔터티에 대한 단서를 제공할 수 있습니다[3, 4]. 이러한 맥락적 inWork Meta AI에서 인턴십하는 동안 수행되었습니다. LLM 디코더 텍스트-텍스트 LoRa 어텐션 어댑터<bos> 오디오 토큰 문맥적 텍스트 토큰(비디오 제목, 설명) 오디오 인코더 인식된 음성 텍스트 그림 1. 사전 학습된 LLM 백본을 기반으로 하는 오디오 및 선택적 텍스트 토큰으로 구성된 혼합 모달 문맥을 갖춘 음성 인식 모델. 음성 인코더와 LLM 디코더는 모두 처음에 사전 학습됩니다. LLM 가중치는 고정되고(주황색 블록), 오디오 인코더와 LoRa 어댑터는 학습 중에 미세 조정됩니다(파란색 블록). 형성은 특정 단어, 도메인별 용어 또는 명명된 엔터티를 종종 문맥에서만 유추할 수 있으므로 어려운 발음을 모호하지 않게 하는 데 도움이 될 수 있습니다. ASR 문맥화에 대한 기존 접근 방식[4, 3, 5, 6]은 토큰 또는 구문 수준에서 작동하며 가중 유한 상태 변환기(WFST)를 사용한 바이어싱이나 특수한 주의 네트워크를 사용하는 것과 같은 기술을 사용합니다. 이러한 기술은 일반적으로 디코딩 단계에서 통합되거나 별도의 구성 요소로 학습됩니다. 결과적으로 문맥화는 명명된 엔터티 또는 특수한 도메인 내 용어를 인식하는 ASR 시스템의 기능을 크게 향상시킵니다. 그러나 이러한 접근 방식에는 몇 가지 제한이 있습니다. 편향은 외부 정보 전체를 기반으로 맥락화하는 것과는 달리 개별 구문이나 단어로 제한됩니다(예: 주제 기반 편향). 편향 강도는 일반적으로 하이퍼 매개변수를 통해 제어되거나 시스템이 과도하게 편향되지 않도록 하기 위해 특수한 아키텍처 변경 및 교육 절차가 필요합니다. 일부 맥락화 방법은 인코더와 직접 상호 작용하지 않고 디코더 상태에만 영향을 미칩니다. 이 연구에서는 음성 인식에 맞게 조정된 LLM의 최근 개발에서 영감을 받은 디코더 전용 아키텍처인 Speech LLAMA를 제안합니다. 추가 하이퍼 매개변수 없이 맥락 정보를 종단 간에 사용하도록 훈련됩니다. 구체적으로, 1) 오디오 토큰과 함께 사용 가능한 전체 텍스트 맥락을 ASR 시스템에 프롬프트로 추가합니다. 따라서 Speech LLAMA는 다음 말한 단어를 디코딩할 때 맥락 텍스트 토큰과 음향 토큰을 되돌아보고 교차 상관 관계를 지정할 수 있는 완전한 유연성을 갖추고 있습니다. 2) 우리는 Speech LLAMA를 위한 사전 학습된 디코더로 공개적으로 이용 가능한 7B LLAMA LLM[1]을 사용합니다. 이것은 음성 인식을 다음 토큰 예측이 있는 혼합 모달 언어 모델로 간주할 수 있으므로 문맥적 ASR의 전반적인 설계를 단순화합니다. 이것의 배후에 있는 우리의 직관은 사전 학습된 LLM이 이미 언어 정보를 추출하여 발화를 감안할 때 문맥의 어느 부분이 관련이 있는지 추론할 때 특히 유용해야 한다는 것입니다. 경쟁 벤치마크에 대한 우리의 결과는 이 모델링 접근 방식의 실행 가능성을 시사합니다. 2.
--- RELATED WORK ---
심층 및 얕은 바이어싱을 포함한 음성 인식 모델 맥락화에 대한 여러 연구가 있었습니다[8, 4]. Le et al.[4]은 디코딩 중에 동적으로 첨부되는 바이어싱 문자열로 구성된 가중 유한 상태 변환기(WFST)를 도입했으며 RNN-T 시스템의 점수와 바이어싱 WFST가 보간됩니다. 이러한 접근 방식의 장점은 훈련이 완료된 후 모든 시스템에 첨부할 수 있다는 것입니다. 또 다른 연구 분야는 심층 바이어싱입니다.
--- METHOD ---
LLM을 통합한 음성 인식 모델을 문맥화하기 위해. 저희의 접근 방식은 음성 인식을 사전 훈련된 LLM을 기반으로 하는 혼합 모달 언어 모델링 작업으로 간주합니다. 저희는 컨텍스트에 대한 선택적 텍스트 토큰과 함께 오디오 기능을 제공하여 시스템이 디코더 전용 방식으로 필사본을 완료하도록 훈련합니다. 결과적으로 시스템은 훈련 중에 구조화되지 않은 컨텍스트 정보를 활용하는 방법을 학습하도록 암묵적으로 인센티브를 받습니다. 저희의 경험적 결과는 추가 텍스트 컨텍스트가 제공될 때 6%의 WER 감소와 함께 성능이 크게 향상되었음을 보여줍니다. 게다가 저희의 방법은 경쟁력 있는 성능을 보이며 25배 이상 큰 음성 데이터 세트에서 훈련된 기준 컨텍스트화된 RNN-T 시스템에 비해 전체적으로 7.5%의 WER, 희귀 단어에 대한 WER은 17% 향상됩니다. 전반적으로 어댑터를 통해 소수의 훈련 가능한 매개변수만 추가하면 사전 훈련된 LLM에 대한 컨텍스트화된 음성 인식 기능을 잠금 해제하는 동시에 동일한 텍스트 전용 입력 기능을 유지할 수 있음을 보여줍니다. 색인 용어 문맥적 편향, 대규모 언어 모델, 음성 인식 1. 서론 최근 몇 년 동안, 질문 답변 및 요약과 같은 작업에서 성과를 향상시키는 데 탁월한 효능이 있어 대규모 언어 모델(LLM)에 대한 관심이 커지고 있으며, 이는 특수 모델을 능가합니다[1, 2]. LLM은 방대한 양의 텍스트 데이터로 학습되므로 네트워크 내에 풍부한 세계 지식을 캡슐화합니다. 이러한 축적된 지식과 문맥적 이해는 특히 오디오만 넘어 발화를 둘러싼 추가 맥락을 사용할 수 있는 경우 자동 음성 인식(ASR) 분야에서 특히 유익한 것으로 입증되었습니다. 예를 들어, 비디오 제목과 설명은 비디오 주제에 대한 통찰력을 제공하거나 언급될 수 있는 명명된 엔터티에 대한 단서를 제공할 수 있습니다[3, 4]. 이러한 문맥적 inWork Meta AI에서 인턴십하는 동안 수행되었습니다. LLM 디코더 텍스트-텍스트 LoRa 어텐션 어댑터<bos> 오디오 토큰 문맥적 텍스트 토큰(비디오 제목, 설명) 오디오 인코더 인식된 음성 텍스트 그림 1. 사전 학습된 LLM 백본을 기반으로 하는 오디오 및 선택적 텍스트 토큰으로 구성된 혼합 모달 문맥을 갖춘 음성 인식 모델. 음성 인코더와 LLM 디코더는 모두 처음에 사전 학습됩니다. LLM 가중치는 고정되고(주황색 블록), 오디오 인코더와 LoRa 어댑터는 학습 중에 미세 조정됩니다(파란색 블록). 형성은 특정 단어, 도메인별 용어 또는 명명된 엔터티를 종종 문맥에서만 유추할 수 있으므로 어려운 발음을 모호하지 않게 하는 데 도움이 될 수 있습니다. ASR 문맥화에 대한 기존 접근 방식[4, 3, 5, 6]은 토큰 또는 구문 수준에서 작동하며 가중 유한 상태 변환기(WFST)를 사용한 바이어싱이나 특수한 주의 네트워크를 사용하는 것과 같은 기술을 사용합니다. 이러한 기술은 일반적으로 디코딩 단계에서 통합되거나 별도의 구성 요소로 학습됩니다. 결과적으로 문맥화는 명명된 엔터티 또는 특수한 도메인 내 용어를 인식하는 ASR 시스템의 기능을 크게 향상시킵니다. 그러나 이러한 접근 방식에는 몇 가지 제한이 있습니다. 편향은 외부 정보 전체를 기반으로 맥락화하는 것과는 달리 개별 구문이나 단어로 제한됩니다(예: 주제 기반 편향). 편향 강도는 일반적으로 하이퍼 매개변수를 통해 제어되거나 시스템이 과도하게 편향되지 않도록 하기 위해 특수한 아키텍처 변경 및 교육 절차가 필요합니다. 일부 맥락화 방법은 인코더와 직접 상호 작용하지 않고 디코더 상태에만 영향을 미칩니다. 이 연구에서는 음성 인식에 맞게 조정된 LLM의 최근 개발에서 영감을 받은 디코더 전용 아키텍처인 Speech LLAMA를 제안합니다. 추가 하이퍼 매개변수 없이 맥락 정보를 종단 간에 사용하도록 훈련됩니다. 구체적으로, 1) 오디오 토큰과 함께 사용 가능한 전체 텍스트 맥락을 ASR 시스템에 프롬프트로 추가합니다. 따라서 Speech LLAMA는 다음 말한 단어를 디코딩할 때 맥락 텍스트 토큰과 음향 토큰을 되돌아보고 교차 상관 관계를 지정할 수 있는 완전한 유연성을 갖추고 있습니다. 2) 우리는 Speech LLAMA를 위한 사전 학습된 디코더로 공개적으로 이용 가능한 7B LLAMA LLM[1]을 사용합니다. 이것은 음성 인식을 다음 토큰 예측이 있는 혼합 모달 언어 모델로 간주할 수 있으므로 문맥적 ASR의 전반적인 설계를 단순화합니다. 이것의 배후에 있는 우리의 직관은 사전 학습된 LLM이 이미 언어 정보를 추출하여 발화를 고려할 때 문맥의 어느 부분이 관련이 있는지 추론할 때 특히 유용해야 한다는 것입니다. 경쟁 벤치마크에 대한 우리의 결과는 이 모델링 접근 방식의 타당성을 시사합니다. 2. 관련 연구 심층 및 얕은 바이어싱[8, 4]을 포함하여 음성 인식 모델 문맥화에 대한 여러 연구가 있었습니다. Le et al.[4]은 디코딩 중에 동적으로 첨부된 바이어싱 문자열로 구성된 가중 유한 상태 변환기(WFST)를 도입했으며 RNN-T 시스템의 점수와 바이어싱 WFST가 보간됩니다. 이러한 접근 방식의 장점은 훈련이 완료된 후 모든 시스템에 첨부할 수 있다는 것입니다. 또 다른 연구 분야는 모델 학습 중에 맥락화를 종단 간에 통합하는 심층 바이어싱 방법입니다[9, 3, 6, 10, 11, 5]. 이러한 접근 방식의 일반적인 한계는 사용 가능한 전체 맥락을 제공하는 것이 아니라 구문 수준에서 바이어싱이 발생한다는 것입니다. 또한 이러한 접근 방식은 기본 ASR 아키텍처에 특수 바이어싱 모듈을 추가해야 합니다. 이 연구와 병행하여 음성 관련 작업을 위한 LLM을 통합하는 여러 접근 방식이 제시되었습니다. Wu 등[12]은 텍스트 프롬프트(&quot;오디오를 언어 X로 번역&quot;)와 오디오 표현을 연결하여 음성 번역을 위한 LLaMA LLM을 통합했습니다. AudioPalm[13] 모델은 음성-텍스트 및 음성-음성 작업을 위해 오디오 및 텍스트 토큰을 혼합하여 제안되었습니다. Fathullah 등[14]은 다국어 데이터에서 LLAMA 모델에 대한 음성 인식 기능을 활성화하는 결과를 발표했습니다. 최근 Whisper 모델[15]은 바이어싱 접근 방식을 통합하여 이전 세그먼트의 전사본을 장문 음성 인식을 위한 프롬프트에 추가했습니다. 그들의 작업과 다른 점은, 우리는 비디오 제목과 설명이 음성의 맥락과 항상 일치하지는 않기 때문에 구조화되지 않고 때로는 관련성이 없는 텍스트 맥락에 따라 시스템을 편향시킨다는 것입니다.
--- EXPERIMENT ---
AL SETUP 모델: 그림 1은 제안된 모델의 개요를 보여줍니다. 이 음성 LLM 아키텍처는 오디오 인코더와 텍스트 디코더라는 두 가지 주요 블록으로 구성됩니다. 오디오 인코더는 먼저 4개의 다운샘플링 블록을 적용하여 오디오 표현의 시간을 16배 단축합니다. 그 후 회전 위치 임베딩[17]이 있는 Conformer[16] 블록[17] 스택이 512의 은닉 차원과 9의 커널 크기로 적용됩니다. 마지막에 추가 다운샘플링 블록을 추가합니다. 그 결과 디코더는 4,096의 차원으로 320ms마다 샘플링된 오디오 토큰을 관찰합니다. 동일한 교육 데이터에서 300k 교육 단계에 대해 Connectionist Temporal Classification[18] 기준으로 오디오 인코더를 사전 교육했습니다. 사전 교육된 7B LLaMA(v1)[1]를 디코더로 사용했습니다. 텍스트 전용 LLAMA를 음성 인식 작업에 적용하기 위해 모든 디코더 계층의 셀프 어텐션 계층에서 쿼리, 키, 값 및 출력 투영 행렬에 Low-Rank Adapter[19]를 추가하면서 나머지 LLM 매개변수는 학습 내내 고정했습니다.다음과 같은 LoRa 매개변수를 사용했습니다.크기 32의 순위, 5%의 드롭아웃 비율 및 0.05 스케일링 매개변수.전체 LoRa 매개변수는 LLM 디코더에 3천만 개의 학습 가능한 매개변수를 추가하고 나머지 60억 개는 고정된 상태로 유지합니다.25ms의 윈도우로 10ms마다 계산된 80차원 log Mel 피처를 사용했습니다.폭 27의 두 주파수 마스크와 발화 길이의 최대 폭 4%의 10개 시간 마스크가 있는 SpecAugment[20]. 우리는 혼합 정밀도로 200,000개의 업데이트에 대해 모델을 훈련했으며, 처음 20개의 업데이트에서 학습률을 5e-4로 선형적으로 증가시키고 나머지 업데이트에서 지수적으로 1e-5로 감소시켰습니다. 우리는 매개변수 ẞ1 = 0.9,0.98, 가중치 감소 1e-5, 그래디언트 노름을 1로 클리핑하는 Adam을 사용합니다. 우리 모델은 Fairseq 라이브러리[21]를 사용하여 3일 동안 128개의 A100 GPU로 훈련되었습니다. = = 데이터: 모델은 공개 Facebook 및 Instagram 비디오에서 파생된 개인 식별 정보(PII)가 없는 익명화된 사내 데이터 세트에서 훈련되었습니다. 데이터는 속도 섭동[22] 및 무작위 샘플링된 가산 배경 잡음의 두 가지 왜곡 방법으로 추가로 보강되었습니다. 평가를 위해 우리는 최소 100자 길이의 컨텍스트를 갖고 컨텍스트에서 적어도 하나의 비빈도 단어가 전사에 나타나는 약 34시간 분량의 음성으로 구성된 3,200개의 비디오를 샘플링했습니다. 메트릭: 모델을 평가하기 위해 전체 단어 오류율(WER)과 희귀 단어만 고려하는 희귀 WER을 모두 보고합니다. 단어는 훈련 데이터에서 추정된 가장 빈번한 단어의 90% 백분위수에 포함되지 않으면 희귀한 것으로 간주됩니다. 텍스트 컨텍스트: Xiao et al. [7]과 유사하게 비디오 제목과 비디오 설명을 외부 컨텍스트로 통합합니다. 유니코드 문자 정규화 및 모든 비 ASCII 기호 제거와 같은 기본 텍스트 후처리를 수행합니다. 전반적으로 감독 비디오 데이터 세트의 비디오 중 약 25%가 비어 있지 않은 텍스트 컨텍스트를 갖습니다. 비디오 제목이나 설명이 있는 경우 먼저 연결한 다음 LLAMA 토크나이저로 토큰화합니다. 그런 다음<bos> 토큰을 텍스트 토큰과 비교합니다. 비디오 제목과 설명이 모두 누락된 경우 입력은 문맥 정보가 없는 기존 ASR 설정에 해당합니다. 교차 엔트로피 표 1. 영어 음성 데이터에 대한 대규모 RNN-T 기준과 비교한 Speech LLaMA의 평가 결과. 전체 WER과 Rare WER을 보고합니다. Rare WER은 특히 데이터 세트에서 희귀 단어를 인식하는 정확도에 초점을 맞춥니다. 모델 음성 데이터(h) 학습 가능한 매개변수(M) 컨텍스트 존재 학습 WER(%) SUB INS DEL 드물게 나타나는 WER(%) 평가 1B RNN-T [7] 4M12.6.53 3.21 2.30.1B RNN-T [7] 4M12.6.23 3.2.28.Speech LLaMa 150k11.6.09 3.20 2.27.Speech LLaMa 150k11.Speech LLaMa 150k11.6.28 3.07 2.5.76 3.14 2.28.23.loss는 컨텍스트 토큰에 대해 마스크 처리되고 음성 토큰에 대해서만 계산됩니다. 이 실험에서는 계산상의 이유로 텍스트 콘텐츠를 최대 50개 토큰으로 제한합니다. 컨텍스트가 임계값보다 길면 훈련 중에 크기 50의 임의 자르기를 수행하고 추론 중에 선행 토큰을 자릅니다.기준선: 기준으로 400만 시간의 지도 및 반지도 음성 데이터로 훈련된 10억 개의 매개변수가 있는 변압기 기반 RNN-T 시스템[7]을 사용했습니다.RNN-T 시스템 아키텍처는 인코더의 변압기 계층과 디코더의 3개 LSTM 계층으로 구성됩니다.문맥화를 위해 신경 언어 모델링 얕은 융합[4]을 사용하는 WFST 바이어싱 방법을 사용하는데, 여기서 바이어싱 FST는 비디오 제목과 설명으로 구성됩니다.RNN-T 기준선과 Speech LLAMA에 대해 디코딩하는 동안 정확히 동일한 문맥 정보를 사용합니다.4. 결과 표 1은 평가 세트에 대한 디코딩 결과의 요약을 보여줍니다.디코딩 중에 문맥화 정보를 제시하는 경우와 제시하지 않는 경우의 두 가지 시나리오를 고려하여 Speech LLAMA를 오프라인 RNN-T 1b 모델과 비교합니다. RNN-T를 사용하여 이러한 시나리오에 대해 얻은 WER 점수는 각각 12.34%와 12.13%입니다. 문맥적 편향은 약 1.7%의 상대적 WER 감소로 이어집니다. 훈련 및 평가 중에 문맥 정보를 사용하지 않더라도 Speech LLAMA는 11.70%의 WER을 달성하여 훨씬 더 많은 데이터로 훈련된 RNN-T 시스템보다 상대적으로 5.2% 감소했습니다. 훈련 및 평가 중에 문맥을 통합함으로써 상당한 개선을 이루어 전체 WER이 11.22%가 되고 Rare WER에서 17%의 상대적 개선이 이루어져 문맥화를 사용한 RNN-T 모델의 성능을 능가했습니다. 문맥을 사용하여 훈련했지만 추론 중에 문맥을 제공하지 않은 Speech LLAMA를 평가할 때 WER이 11.98%라는 점에 주목할 가치가 있습니다. 이는 문맥 없이 훈련된 모델과 비교했을 때 약간의 WER 격차에 해당합니다. 우리는 이 사소한 성능 차이를 향후 작업으로 미루고, 맥락에 특정 지터를 추가하면 맥락의 존재에 대한 모델의 일반화가 개선될 수 있습니다.4.1. 소거 연구 4.1.1. 맥락 민감도 모델이 맥락을 사용하는 방법을 더 잘 이해하기 위해, 우리는 모델이 맥락 교란에 얼마나 수용적인지 연구했습니다.이를 위해 우리는 프롬프트를 수정하고 디코딩에 미치는 영향을 측정하는 몇 가지 방법을 시도했습니다.특히, 우리는 다음을 실험했습니다.1. 실제 맥락을 훈련 데이터에서 무작위로 샘플링한 단어로 대체합니다.2. 맥락을 기준 진실 단어로 대체합니다.우리는 모델이 단어를 필사하는 데 큰 문제가 없을 것이라고 가정하기 때문에 이 실험에서 빈번한 단어를 걸러냅니다.모델이 맥락에서 단어를 복사하여 붙여넣을 수 있다면 WER이 상당히 감소할 것으로 예상합니다.3. 맥락적 단어를 필사본에 나타나는 단어의 음성적 철자로 대체합니다. 우리의 직감은 이러한 대체가 모델에 특히 도전적인 일이며 무작위 대체에 비해 더 큰 WER 변화를 예상해야 한다는 것입니다.재철자를 생성하기 위해 G2G[23] 모델을 사용했습니다.기본 진실의 모든 희귀 단어에 대해 G2G 모델에서 대체 철자를 샘플링하여 컨텍스트에 추가합니다.예를 들어, 단어 ball이 컨텍스트와 기본 진실에 존재하는 경우 bawl로 대체하여 원래 토큰 대신 컨텍스트로 사용합니다.4. 이전 섭동에 더하여 컨텍스트에 비슷하게 들리는 단어를 추가하여 조사합니다(예: ball과 bawl 토큰이 모두 컨텍스트에 존재함).이는 컨텍스트에서 경쟁 단어가 주어졌을 때 ASR 시스템이 실제 말한 단어의 모호성을 해소하는 능력을 테스트합니다.표 2. 디코딩 단계 동안 다양한 컨텍스트 섭동 하의 WER.표 4. 디코더 전용 Speech LLM과 교차 주의 Speech LLM의 성능 비교. 문맥 노이즈 WER(%) 희귀 WER(%) (원래 문맥) 11.23.(모든 문맥 제거) 11.28.무작위 12.28.재철자 11.28.재철자(추가) 기준 진실 11.10.25.19.표 2에 결과를 제시합니다. 전체 문맥을 훈련 데이터에서 샘플링한 무작위 단어로 대체하면 모든 외부 문맥을 제거하는 것과 비교했을 때 WER에서 미미한 차이만 발생합니다(11.98% 대 12.07%). 이는 모델이 일부 문맥 노이즈에 강하고 관련성 있는 문맥과 관련성 없는 문맥을 구별할 수 있음을 나타냅니다. 문맥과 기준 진실 모두에 일치하는 희귀 단어를 G2G 재철자로 대체하면 WER이 상당히 감소합니다(11.22% → 11.89%). 문맥을 전혀 사용하지 않는 것과 거의 일치합니다. 이는 관찰된 대부분의 이득이 모델이 문맥에서 특정 단어를 복사할 수 있기 때문이라는 것을 암시합니다. 대조적으로, 일치하는 문맥 단어를 대체하는 대신 경쟁하는 유사한 발음의 단어를 추가했을 때, 우리는 더 작은 WER 하락(11.22% → 11.46%)을 관찰했습니다. 이는 모델이 의미가 다르지만 발음이 비슷한 단어에 반드시 혼동을 받지 않는다는 것을 나타냅니다. 나아가, 기준 진실에서 희귀 단어를 문맥에 적용하면 WER이 10.50%(상대적 변화 6%)로 개선되고 희귀 WER은 상대적으로 18% 개선됩니다. 이는 모델이 희귀 엔터티를 더 잘 인식하기 위해 문맥 정보가 있을 때 이를 활용할 수 있는 능력을 더욱 증명합니다. 표 3. WER에 대한 문맥 마스킹 구조의 영향. 마스킹 인과적 전체 마스크 WER(%) 11.11.4.1.2. 인과적 대 전체 마스킹 전통적으로 인과적 마스킹은 디코더 전용 언어 모델의 모든 셀프 어텐션 계층에서 사용되어 향후 정보 유출을 방지합니다. 그러나 오프라인 음성 인식의 경우 디코딩 시점에 전체 오디오 및 텍스트 컨텍스트가 관찰되고 인과적으로 마스크해야 하는 것은 전사 토큰뿐입니다.이 섹션에서는 모든 입력 토큰에 인과적 마스킹을 적용하는 것의 영향을 실험하고 텍스트 및 오디오 컨텍스트에 전체 마스크를 적용한 다음 전사 토큰에 인과적 마스킹을 적용하는 것과 대조합니다.오디오 표현은 이미 완전히 맥락화되었지만 텍스트 토큰이 전체 마스킹의 이점을 얻을 수 있다고 가정합니다.표 3에 결과를 제시합니다.전체 마스크는 인과적 마스킹보다 약간 더 나은 WER만 보여줍니다(11.22%에서 11.15%로 향상).효율적인 셀프 어텐션 구현이 현재 인과적 마스킹(Flash-Attention v2)에 맞춰져 있고 사용자 지정 마스킹을 사용하면 학습 속도가 10% 느려지기 때문에 비용이 발생합니다.4.1.3. 디코더 전용 대 교차 어텐션 나아가, 우리는 Speech LLM 아키텍처를 Listen-Attend-Spell 아키텍처로 변환하여 디코더 전용 접근 방식을 기존의 인코더-디코더 모델과 비교했습니다[24]. 이를 달성하기 위해 오디오와 텍스트 토큰을 연결하는 대신 별도로 처리했습니다. 우리는 모든 LLM 디코더 계층에 훈련 가능한 교차 어텐션 행렬을 추가했습니다. 표는 이 연구 결과를 보여줍니다. 우리는 두 접근 방식이 비슷한 성과를 보였지만, 인코더-디코더 아키텍처의 경우 약간의 개선만 있었습니다(11.22% 11.18%). 이는 디코더 전용 접근 방식이 맥락화가 있거나 없이 ASR을 수행하는 실행 가능하고 간단한 방법임을 나타냅니다. 그러나 디코더 전용 접근 방식의 한 가지 한계는 전체 시퀀스 길이에 제한을 가할 수 있는 이차 어텐션 복잡도입니다. 이 한계는 맥락이 커질수록 커집니다. 이 문제를 해결하기 위해 낮은 정밀도 훈련(8개 궤도) 및 선형 어텐션 근사 방법과 같은 기술을 사용할 수 있습니다[25, 26]. 5.
--- CONCLUSION ---
S 및 향후 작업 이 작업에서 우리는 사전 훈련된 LLM을 사용하여 문맥 정보를 활용하여 음성 인식을 개선하는 것에 대한 최초의 결과를 제시했습니다. 우리는 간단한 디코더 전용 아키텍처로 구조화되지 않은 텍스트에 대한 ASR 출력을 조건화할 수 있음을 보여주었습니다. 우리의 접근 방식은 강력한 기준선에 비해 우수한 성능을 보여 제안된 방법의 규모에 대한 실행 가능성을 증명합니다. LLM을 사용한 텍스트 프롬핑을 통한 종단 간 문맥화는 강력한 RNNT 기반 기준선에 비해 더 나은 문맥 활용도를 보여줍니다. 또한, 우리의 절제 연구는 시스템이 소음 교란에 강하고 음성적 모호성 해소를 수행하는 능력을 보여준다는 것을 보여줍니다. 향후 작업의 일환으로 우리는 긴 문맥 및 기타 모달리티로 방법을 확장할 계획입니다. 6. 참고문헌 [1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al., &quot;Llama: Open and efficient foundation language models,&quot; 2023. [2] OpenAI, &quot;Gpt-4 technical report,&quot; 2023. [3] Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey Zweig, Florian Metze, Yatharth Saraf, &quot;Contextual RNN-T for open domain ASR,&quot; INTERSPEECH. 2020, pp. 11-15, ISCA. [4] Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim 등, &quot;Trie 기반 딥 바이어싱 및 얕은 퓨전을 사용한 문맥화된 스트리밍 엔드투엔드 음성 인식&quot;, INTERSPEECH, 2021, pp. 1772-1776. [5] Kanthashree Mysore Sathyendra, Thejaswi Muniyappa, Feng-Ju Chang 등, &quot;신경 변환기에서 개인화된 음성 인식을 위한 문맥 어댑터&quot;, ICASSP. 2022, pp. 8537-8541, IEEE. [6] Golan Pundak, Tara N. Sainath 등, &quot;딥 컨텍스트: 엔드투엔드 문맥적 음성 인식&quot;, IEEE Spoken Language Technology Workshop. 2018, pp. 418-425, IEEE. [7] Alex Xiao, Weiyi Zheng, Gil Keren 등, 영어: &quot;Scaling asr improvements zero and few shot learning,&quot; in INTERSPEECH, 2021. [8] Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, et al., &quot;Shallow-fusion end-to-end contextual biasing,&quot; in INTERSPEECH. 2019, pp. 1418–1422, ISCA. [9] Duc Le, Gil Keren, Julian Chan, et al., &quot;Deep shallow fusion for rnn-t personalization,&quot; in 2021 IEEE Spoken Language Technology Workshop(SLT), 2021, pp. 251– 257. [10] Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur Gandhe, et al., &quot;Robust acoustic and semantic contextual biasing in neural transducers for speech awareness,&quot; CORR, vol. abs/2305.05271, 2023. [11] Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al., &quot;트랜스듀서 기반 스트리밍 음성 인식을 위한 적응적 맥락적 편향,&quot; 2023. [12] Jian Wu, Yashesh Gaur, et al., &quot;음성-텍스트 및 대규모 언어 모델 통합을 위한 디코더 전용 아키텍처에 관하여,&quot; 2023. [13] Paul K. Rubenstein et al., &quot;Audiopalm: 말하고 들을 수 있는 대규모 언어 모델,&quot; 2023. [14] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al., &quot;음성 인식 기능을 갖춘 대규모 언어 모델 프롬프트,&quot; 2023. [15] Alec Radford, Jong Wook Kim, et al., &quot;대규모 약한 감독을 통한 견고한 음성 인식,&quot; ICML. 7월 23-29일 2023, vol. 202 of Machine Learning Research, pp. 28492–28518, PMLR. [16] Anmol Gulati, James Qin, et al., &quot;Conformer: 음성 인식을 위한 합성곱 증강 변환기,&quot; INTERSPEECH에 게재됨. 2020, pp. 5036-5040, ISCA. [17] Jianlin Su, Yu Lu, et al., &quot;Roformer: 회전 위치 임베딩이 있는 향상된 변환기,&quot; CoRR, vol. abs/2104.09864, 2021. [18] Alex Graves 외, &quot;연결주의 시간 분류: 순환 신경망을 사용한 분할되지 않은 시퀀스 데이터 레이블링&quot;, ICML. 2006, ACM 국제 회의록 시리즈 vol. 148, pp. 369– 376, ACM. [19] Edward J. Hu, Yelong Shen 외, &quot;Lora: 대규모 언어 모델의 저순위 적응&quot;, ICLR. 2022, OpenReview.net. [20] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le, &quot;Specaugment: 자동 음성 인식을 위한 간단한 데이터 증강 방법,&quot; INTERSPEECH, 2019년 9월. [21] Myle Ott, Sergey Edunov, Alexei Baevski, et al., &quot;fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 툴킷,&quot; ACL(Demonstrations), 미네소타주 미니애폴리스, 2019년 6월, 48-53쪽, 계산언어학 협회. [22] Tom Ko, Vijayaditya Peddinti 외, &quot;음성 인식을 위한 오디오 증강,&quot; INTERSPEECH. 2015, 3586-3589쪽, ISCA. [23] Duc Le, Thilo Koehler, Christian Fuegen, Michael L. Seltzer, &quot;G2g: 문자적 하이브리드 ASR을 위한 Tts 기반 발음 학습,&quot; ICASSP, 2020, 6869-6873쪽. [24] William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, &quot;듣고, 주의하고, 철자하세요,&quot; CORR, vol. abs/1508.01211, 2015. [25] Tim Dettmers 및 Luke Zettlemoyer, &quot;4비트 정밀도에 대한 사례: k비트 추론 스케일링 법칙,&quot; ICML. 2023, vol. 202, pp. 7750–7774, PMLR. [26] Jiayu Ding, Shuming Ma, et al., &quot;Longnet: 1,000,000,000개 토큰으로 변환기 스케일링,&quot; 2023.
