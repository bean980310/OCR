--- ABSTRACT ---
대규모 언어 모델(LLM)은 음성 도메인에 적용되었으며, 음성과 언어 표현 간의 정렬이 맞지 않아 성능이 저하되는 경우가 많았습니다. 이러한 격차를 메우기 위해, 음성 정보 손실 없이 음성을 텍스트 토큰 임베딩 공간에 매핑하는 Speech2Text 어댑터를 사용하는 공동 음성 및 언어 모델(SLM)을 제안합니다. 또한 CTC 기반 블랭크 필터링을 사용하여 음성 시퀀스 길이를 텍스트 길이로 줄일 수 있습니다. 음성 MultiWoz 데이터 세트(DSTC11 챌린지)에서 SLM은 대화 상태 추적(DST) 성능을 크게 개선합니다(정확도 24.7%~28.4%). 드문 엔터티의 오류를 해결하기 위해, 음성을 사용하여 관련 엔터티를 검색한 다음 원래 SLM 입력에 접두사로 추가하는 Speech2Entity 검색기로 SLM을 증강합니다. 이 검색 증강 SLM(ReSLM)을 사용하면 DST 성능이 34.6% 정확도로 뛰어오릅니다. 또한, 대화 이해 작업으로 ASR 작업을 증강하면 ASR 성능이 WER 9.4%에서 8.5%로 향상됩니다. 1.
--- INTRODUCTION ---
대규모 언어 모델(LLM)의 기능을 텍스트에서 음성을 포함한 다른 모달리티로 확장하는 데 상당한 관심이 있었습니다. 한 연구 스레드는 음성과 텍스트를 동일한 잠재 표현에 매핑하려고 시도합니다[1, 2, 3]. 음성과 텍스트 모두에 공유 인코더가 사용되며, 한 경우에는 동일한 임베딩 공간을 촉진하는 명시적 손실 용어가 있고[3] 다른 경우에는 명시적 용어가 없습니다[1]. 대부분의 실용적인 구어 시스템에서 음성 입력은 자동 음성 인식(ASR)을 사용하여 인식되고 인식된 필사본은 백엔드 NLU 시스템에 공급됩니다. 여기서 백엔드는 LLM으로 구동됩니다[4]. 이 계단식 접근 방식은 잠재적인 ASR 오인식을 수정할 기회를 제공하지 않습니다. 게다가 LLM과 ASR 시스템 모두 훈련 데이터에서 제대로 표현되지 않은 엔터티를 처리하는 데 공통적인 약점이 있습니다. 이 논문에서는 DSTC-11 대화 추적 작업[5]을 사용하는 음성 이해 시스템의 맥락에서 이러한 과제를 살펴봅니다. 이 작업은 기차 및 호텔 예약과 같은 여러 도메인과 주제에 걸쳐 완전히 레이블이 지정된 인간-인간 대화의 컬렉션인 인기 있는 MultiWoz를 기반으로 합니다[6]. 이 특정 과제에서 작성된 사용자 응답은 크라우드 소싱 작업자로부터 수집된 음성 버전으로 대체되었습니다. 이 모델은 현재 사용자 발화와 주어진 대화 맥락에 해당하는 대화 상태를 추론할 것으로 예상됩니다. 맥락은 이전에 추론된 상태와 함께 대화 기록의 음향 또는 인식된 버전이 될 수 있습니다. 이 작업은 레스토랑, 관광 명소, 도시 및 기차역과 같은 희귀한 엔터티가 많이 나타나기 때문에 특히 흥미롭습니다. UtteranceSLM / ReSLM UtterancePrediction [ASR] 오전 1시 42분 [DST] 이후에 리플리에서 출발하는 기차를 찾고 있습니다. train-departure-ripley train-leaveat-1:42 am 목적지는 어디이고 어느 요일에 가고 싶으신가요? SLM / ReSLM [ASR] i&#39;m looking to travel to Agent Prediction [DST] train-leaveat-1:42 am train-destination-mcdou mcdougal, and i want to leave on gal train-day-thursday thursday. train-departure-ripley 그림 1: 다중 턴 대화에서 직접 음성에서 대화 상태로의 예측. 현재 사용자 턴 i의 음성과 대화 기록의 텍스트 사본이 주어지면 SLM / ReSLM 모델은 해당 사본 [ASR]과 대화 상태 [DST]에 대한 단일 출력 시퀀스를 생성합니다. 턴 i에서 예측된 ASR 사본은 자기 회귀 방식으로 기록 턴 i + 1로 사용됩니다. 이 논문의 주요 기여는 다음과 같습니다. 1. 우리는 정보 손실을 최소화하면서 음성 인코딩을 텍스트 토큰 임베딩으로 매핑하는 Speech2Text 어댑터를 제안합니다. 2. 우리는 입력에 음성과 텍스트 모달리티가 모두 있는 공동 음성 및 언어 모델(SLM)을 제안합니다. 어댑터를 사용하면 SLM은 LLM(우리의 경우 사전 학습된 T5 XXL)에 인코딩된 풍부한 지식을 음성 작업에 직접 활용할 수 있습니다. 3. 음성을 사용하여 엔터티를 검색하는 드문 작업별 엔터티를 처리하는 Speech2Entity 검색기를 소개합니다. 4. 검색기로 SLM을 증강하는 새로운 검색 증강 음성 언어 모델(ReSLM)을 제안합니다. ReSLM이 ASR 및 음성 이해 작업에서 강력한 성능을 달성한다는 것을 보여줍니다. 캐스케이드 시스템과 달리 SLM과 ReSLM은 모두 음성 입력에서 직접 작동하므로 첫 번째 단계 ASR의 잘못 인식된 단어에 갇히지 않습니다. DSTCchallenge 작업을 사용하여 모델의 다양한 구성 요소의 이점을 보여줍니다. 이 작업에서는 대화 추적 작업에 대한 결과를 보고하지만 모델은 음성 이해 작업에 더 광범위하게 적용할 수 있습니다. 2.
--- RELATED WORK ---
밀접하게 관련된 작업 라인은 텍스트 입력을 음성 모델[7, 1, 3, 8, 2]에 주입하고 텍스트와 음성의 학습된 표현을 동일한 공간에 정렬합니다. 이는 TTS 또는 최근에 업샘플링된 텍스트를 통해 수행되고 정렬된 음성 및 텍스트 프레임의 L2 손실을 최소화합니다. 이는 오디오 시퀀스의 프레임 속도를 줄여 텍스트에 더 가깝게 만드는 반대 작업을 수행하는 작업과 대조적입니다. 이는 예측을 사용하여 빈 프레임을 필터링하는 CTC 모델[9]을 통해 수행됩니다. 이를 통해 의미 정보를 보존하고 다운스트림 NLU 작업을 훨씬 더 쉽게 만드는 고도로 압축된 음성 인코딩이 생성됩니다. CTC 빈 프레임을 필터링하는 다른 사용 사례도 있습니다. 예를 들어, [10]의 작업에서는 RNN-T[11] 모델의 학습 속도를 높이는 데 사용했습니다. 음성 및 텍스트 모달리티를 결합할 때 음성 신호를 압축하는 것은 비전 및 텍스트 모달리티가 결합된 작업에서 유사점이 있습니다. 예를 들어, Perceiver [12] 아키텍처는 텍스트 토큰 [13]과 인터리브되기 전에 이미지를 압축하는 데 사용됩니다. 그러나 Perceiver 출력과 동결된 LM 계층 간의 교차 주의는 모델을 표준 LM과 크게 다르게 만들고 따라서 해당 모델은 우리의 작업과 달리 제공 시점에 표준 LM을 공유할 수 없습니다. 대안적인 접근 방식에서 음성 입력은 토큰화된 다음 LLM에 공급됩니다 [14, 15]. 이러한 접근 방식은 LLM이 토큰화의 오류를 수정하기 위해 음향 인코딩을 활용할 수 없는 계단식 시스템과 동일한 문제가 있습니다. 검색 증강 언어 모델은 다양한 자연어 작업 [16, 17, 18], 특히 지식 집약적 작업 [19, 20, 21]에서 우수한 성능을 보였습니다. 특히 검색 증강은 도메인별 지식 인코딩을 훈련 매개변수에서 풀어서 도메인 스키마를 통합하는 것이 중요한 요구 사항인 작업 지향 대화에 바람직한 속성이 됩니다 [22, 23]. 또한 대화 이해에서 보이지 않는 도메인과 작업은 새로운 스키마 집합에 대한 적응을 요구할 수 있습니다[24]. 이러한 과제를 해결하기 위해 이전 연구에서는 유사한 교육 예제[25, 26]를 검색하거나 대화 상태 추적을 위한 해당 의도와 슬롯을 검색하는 것을 제안합니다[27]. 이러한 작업에서 영감을 얻어 검색 증강
--- METHOD ---
s에서 음성 이해로. 앞서 언급했듯이, 서면 도메인과 달리 음성 이해는 드문 개체를 쉽게 인식할 수 없다는 추가적인 과제를 안겨줍니다[5]. 따라서 이러한 어려움을 완화하고 종단 간 음성 대화 이해에서 더 나은 성능을 달성하기 위해 오디오 검색 방법을 소개합니다. 3. 모델 3.1. 공동 음성 및 언어 모델(SLM) 음성 이해 작업에는 음성 입력을 동시에 인식하고 해당 언어의 의미적 의미를 이해할 수 있는 모델이 필요합니다. 이전 연구에서 BERT, T5, GPT와 같은 대규모 사전 학습된 언어 모델은 NLU 작업에서 의미를 이해하는 데 인상적인 역량을 보여주었습니다[28, 29, 30]. 이 역량을 활용하여 그림 2와 같이 이 음성 이해 작업을 위해 음성 인코더와 T5 모델을 결합합니다. 음성 인코더는 별도로 학습된 CTC 모델을 기반으로 하며, 이에 대해서는 섹션 4.2에서 자세히 설명합니다. CTC 모델의 비어 있지 않은 예측 프레임만 활용합니다. 이 CTC 기반 블랭크 필터링 접근 방식에는 두 가지 장점이 있습니다.첫째, 오디오 신호의 의미적으로 관련된 정보만 다운스트림 작업으로 &#39;전달&#39;되므로 NLU에 대한 미세 조정이 훨씬 쉬워집니다.둘째, 인코딩된 음성 시퀀스의 효과적인 시퀀스 길이가 약 4배 줄어듭니다.이는 음성 및 텍스트 시퀀스의 공동 모델링에 도움이 됩니다.그렇지 않으면 오디오 시퀀스가 텍스트 시퀀스보다 훨씬 크고 처리가 훨씬 어려워집니다.이는 텍스트를 음향 프레임 속도[7]와 일치하도록 업샘플링하여 사전 훈련된 LLM을 활용할 수 없는 다른 작업에서 사용된 반대 접근 방식과 대조됩니다.3.2. Speech2Text 어댑터 Speech2Text 어댑터는 CTC 필터링된 음성 인코딩을 사전 훈련된 LLM의 텍스트 토큰 임베딩에 매핑하기 위한 몇 개의 셀프 어텐션 계층으로 구성됩니다.결과 출력은 텍스트 임베딩과 연결된 다음 사전 훈련된 LLM에 입력됩니다. 어댑터가 효과적이려면 텍스트 임베딩 공간에 성공적으로 매핑되도록 사전 학습을 거치는 것이 중요합니다.이는 입력이 음성이고 예측이 해당 필사본인 ASR 작업으로 SLM을 간단히 학습하여 수행할 수 있습니다.SLM의 텍스트 입력 부분은 어댑터를 학습하는 동안 사용되지 않습니다.이 사전 학습 프로세스 동안 음성 및 언어 모델 가중치가 모두 고정된다는 점에 유의해야 합니다.따라서 Speech2Text 어댑터는 두 가지 의미 겹을 참조합니다.1) 음성 인코더와 언어 모델 사이의 몇 가지 셀프 어텐션 계층;2) 음성 및 언어 모델이 모두 고정된 사전 학습.3.3. Speech2Entity 검색기 검색기의 주요 작업은 현재 음성 입력과 관련된 주어진 작업별 엔터티 목록에서 엔터티 하위 집합을 추출하는 것입니다.우리는 음성 입력의 음향 인코딩이 키이고 입력에서 언급된 엔터티가 값인 검색기의 이중 인코더 아키텍처를 채택합니다[31]. 모델은 입력 음성의 참조 필사본에 언급된 엔티티를 사용하여 학습됩니다. 키와 값(후보 엔티티)은 별도로 인코딩되고 해당 표현 간의 코사인 거리를 사용하여 유사성을 측정합니다. 배치 내 부정은 대조 손실을 최적화하기 위한 부정 대상으로 사용됩니다. 우리의 경우 오디오와 텍스트를 모두 인코딩할 수 있기 때문에 멀티모달 SLM 인코더를 사용합니다. 추론하는 동안 SCAM 라이브러리를 사용하여 코사인 거리를 사용하여 가장 가까운 이웃을 효율적으로 계산하고 상위 K 후보를 검색합니다[32]. 3.4. 검색 증강 SLM 모델(ReSLM) 검색기 기반 SLM에서 오디오 검색기의 상위 K 후보를 이전에 설명한 SLM에 통합합니다. 구체적으로 현재 음성 입력의 음향 인코딩을 쿼리로 사용하여 작업별 엔티티의 대규모 풀에서 상위 K 엔티티를 검색합니다. 검색된 엔티티는 인코더에 공급되기 전에 원래 텍스트 입력에 사전 사전 처리됩니다. 4.
--- EXPERIMENT ---
s 및 결과 4.1. 평가 과제 DSTC11 챌린지 과제는 MutliWoz 2.1을 기반으로 하며 대화와 차례 측면에서 동일한 분포를 갖습니다[5]. 주요 차이점은 작성된 사용자 응답이 음성 버전으로 대체된다는 것입니다. 응답은 훈련 세트에서는 TTS를 사용하여 생성되었고 테스트 세트에서는 크라우드 소싱 작업자의 인간 음성으로 생성되었습니다. 또한 이전에 연구자들은 훈련 및 테스트 세트의 슬롯 값이 상당히 겹치므로 오해의 소지가 있고 지나치게 optiText [ASR] 출력으로 가는 기차를 예약하고 싶습니다 Mcdougal [DST] train-arrival=Mcdougal 텍스트 모델(T5) 디코더 인코더 토큰 임베딩 텍스트 입력 상위 k 엔터티 오디오 임베딩 연결 오늘 무엇을 도와드릴까요? 데이터베이스 Speech2Entity 검색기 Speech2Text 어댑터(사전 학습됨) 빈 프레임 제거 음성 인코더 필터링된 인코딩 음성 인코더(고정됨) 오디오 입력 Mcdougal행 기차를 예약하고 싶습니다 그림 2: ReSLM 및 SLM(Speech2Entity 검색기 구성 요소 없음)의 모델 아키텍처 SLM 및 ReSLM 모델은 음성과 텍스트를 모두 입력으로 사용합니다. 음성 프레임 시퀀스는 CTC 기반 빈 필터링으로 단축되고 Speech2Text 어댑터로 변환된 다음 텍스트 임베딩과 연결된 후 T5 인코더-디코더 모델에 입력됩니다. ReSLM에서 Speech2Entity 검색기의 음성 입력을 사용하여 몇 개의 엔터티를 선택하고 텍스트 입력 앞에 추가합니다. mistic 성능 보고서. 이 문제를 완화하기 위해 챌린지 주최측은 슬롯 값(도시, 식당 이름, 시간 등)을 대체하여 테스트 세트를 수정했습니다. 따라서 DSTC11 테스트 세트의 시스템 성능은 작성된 버전보다 낮을 것으로 예상됩니다. 작업의 주요 초점은 Joint Goal Accuracy(JGA)와 Slot Error Rate(SER)를 사용하여 성능을 측정하는 대화 상태 추적이었습니다. 자세한 내용은 [5]를 참조하십시오. 또한 인식된 입력 음성의 단어 오류율(WER)을 측정하여 오인식의 영향을 분석하는 절제 실험을 실시했습니다. 4.2. 실험 설정 음성 인코더는 약 32,000시간 분량의 음성 데이터로 구성된 PeopleSpeech 공개 코퍼스[33]에서 학습된 CTC[9] 모델에서 파생되었습니다. 인코더는 총 220m 매개변수 모델인 16개의 Transformer 레이어로 구성되어 있습니다. 모델의 입력 프레임 속도는 25ms이고 Transformer 레이어 사이에 있는 다운샘플링 레이어를 통해 얻은 75ms마다 출력을 생성합니다. 마지막 Transformer 레이어의 활성화(1024-dim)를 음성 인코딩으로 사용합니다. 또한 빈 프레임(예: 가장 높은 점수를 받은 토큰이 비어 있는 프레임)을 제거합니다. 이 모델은 평균 305ms마다 비어 있지 않은 프레임을 내보내고 각 단어는 평균 1.48프레임으로 인코딩됩니다.CTC 공백 프레임을 필터링하면 음성 신호가 매우 강력하게 압축되고 의미 정보를 보존하는 동시에 다운스트림 NLU 작업이 상당히 쉬워집니다.이전에 훈련된 단일 모드 체크포인트, 즉 텍스트 인코더-디코더의 T5 XXL 체크포인트와 음성 인코더의 CTC 음성 인코더 체크포인트를 재사용했습니다.훈련 과정 전반에 걸쳐 모든 실험에서 음성 인코더를 동결 상태로 유지하고 Speech2Text 적응 계층과 함께 텍스트 인코더-디코더를 독점적으로 훈련했습니다.또한 T5 모델을 부분적으로만 미세 조정하는 절제 연구도 보여줍니다.4.3. 자기 회귀 추론 대화에는 여러 차례의 턴이 있으며 대화 상태 값은 차례대로 자기 회귀적으로 추론됩니다.대화 상태 추적 작업에는 현재 턴 i까지의 모든 대화 상태를 예측해야 하므로 전체 대화 기록이 입력으로 필요합니다. 그림 1에서 보듯이 i 턴의 음성을 음성 입력으로, 1 턴에서 i−1 턴까지의 대화 내역을 텍스트 입력으로 입력합니다. 대화 내역은 길 수 있으며 음성이 아닌 텍스트 형태로 표현하는 것이 가장 좋습니다. 이러한 이유로 SLM 모델을 훈련하여 i 턴에서 말한 단어와 대화 상태를 하나의 출력 문자열로 동시에 인식하도록 했습니다. 각 턴의 대본을 점진적으로 정리하여 후속 턴의 대화 내역을 만듭니다. 훈련 과정에서 입력은 현재 턴의 음성과 이전에 설명한 CTC 모델의 ASR 대본을 기반으로 한 대화 내역으로 구성됩니다. 손실은 현재 턴의 참조 대본과 연관된 참조 대화 상태 값으로 구성된 대상으로 계산합니다. 4.4. Speech2Entity 검색기 결과 3.3절에 설명된 Speech2Entity 검색기는 호텔 이름, 레스토랑 이름, 도시 이름의 세 가지 엔터티 범주에 대해 훈련되었습니다[5]. 검색기는 2.5k 엔터티 풀에 대해 훈련되었고 별도의 14k 엔터티 풀이 평가에 사용되었습니다. 원칙적으로 2타워 리트리버 모델은 모든 음성 및 텍스트 인코더를 활용할 수 있습니다. 실험에서 우리는 쿼리와 후보 모두에 SLM 음성 및 텍스트 인코더를 사용합니다. 이전에 훈련된 SLM의 체크포인트는 리트리버를 훈련하기 전에 2타워 인코더를 초기화하는 데 사용되었습니다. Speech2Entity 리트리버의 성능은 표 1에 나와 있습니다. 검색된 엔터티의 하위 집합은 거리 임계값 -0.78을 사용하여 선택되었으며, 그 결과 발화당 상위 10개 엔터티가 생성되었습니다. 이 임계값은 리콜과 정밀도의 균형을 맞추기 위해 선택되었으며, 결과 ReSLM 모델이 가능한 가장 높은 범위의 엔터티에 액세스할 수 있도록 리콜을 최적화하는 데 중점을 두었습니다. 리콜에 대한 이러한 강조의 결과로 정밀도가 희생되었습니다. 그러나 우리는 ReSLM 모델이 잘못 검색된 엔터티를 버리는 법을 배울 것으로 예상했습니다. 분명히 리트리버는 더욱 개선될 수 있으며, 이는 주로 개념 증명의 시연이며, 정밀도가 낮음에도 불구하고 나중에 설명할 것처럼 ReSLM에서 상당한 이득을 얻습니다. 영어: 재현율(%) 정밀도(%) R@1 40.P@13.R@3 51.P@6.R@57.P@5.R@62.P@3.R@66.P@2.R@70.P@ 100 2.표 1: Speech2Entity 검색기의 성능. -0.78 유사도 임계값으로 필터링한 상위 k 재현율 및 정밀도. 4.5. 대화 상태 추적 결과 대화 상태 추적에 대한 결과는 표에 보고되어 있습니다. 왼쪽 절반은 SLM 모델에 해당하고 오른쪽 절반은 ReSLM에 해당합니다. 표의 위쪽 절반에서 어댑터 계층은 처음부터 학습되었고 표의 아래쪽 절반에서 Speech2Text 어댑터는 ASR 작업으로 학습되었습니다. 다른 행은 임베딩 계층, T5 모델의 인코더 및 디코더를 포함한 다양한 매개변수 그룹을 학습한 영향을 보여줍니다. 결과에 따르면 Speech2Text 어댑터는 SLM과 ReSLM 모두에서 성능이 향상되었으며 이득 범위는 3-5 JGA입니다. 흥미롭게도 Speech2Text 어댑터를 사용할 때 인코더를 학습하거나 임베딩하는 것만으로 최상의 결과(29.8% 및 35.1% JGA)를 얻을 수 있는데, 이는 어댑터가 효과적이라는 것을 시사하는 것으로 음성 모달리티를 텍스트 모달리티에 가깝게 만듭니다. Speech2Text 어댑터의 이점 외에도 Speech2Text 리트리버는 모든 조건에서 JGA를 5% 더 향상시킵니다. % WER↓ RNN-T [5] 13. RNN-T 도메인 내 미세 조정 [5] 10. 어댑터만 10. 학습 가능한 매개변수 SLM ReSLM Speech2Text 어댑터 없음 % JGA ↑ WER↓↓ JGA ↑ WER↓ 전체 TCascaded [5] 31.13.T5 인코더+emb T5 인코더만 학습 가능한 매개변수 SLM ReSLM Speech2Text 어댑터 있음 Speech2Text 어댑터 없음 전체 T전체 T24.11.31.9.T5 인코더+emb 27.10.32.8.T5 인코더+emb T5 인코더만 12.11.11.11.11.10.9.9.9.9.9.9.T5 인코더만 27.11.31.9.Speech2Text 어댑터 있음 전체 T28.9.34.8.T5 인코더+emb 29.9.35.8.T5 인코더만 29.9.34.8.표 2: 공동 목표 정확도(JGA)를 사용하여 평가한 대화 상태 추적 성능. 사전 학습된(섹션 3.2 참조) Speech2Text 어댑터가 있는 경우와 없는 경우, 검색된 엔터티가 있는 경우와 없는 경우의 모델 성능을 비교합니다. SLM/ReSLM 모델은 동일한 출력 시퀀스에서 음성 인식된 필사본과 대화 상태를 모두 예측합니다. 따라서 여기서 단어 오류율(WER)도 보고할 수 있습니다. 모든 숫자는 테스트 세트에 있습니다. 슬롯 오류율(SER)을 사용하여 대화 상태 변수 범주의 개선 사항을 확대하면 다음 범주의 대화 상태 변수가 Speech2Text 검색기에서 이점을 얻었음을 알 수 있습니다. 호텔 이름(26% 증가), 기차 목적지 역(35% 증가), 레스토랑 이름(14% 증가). 이는 Speech2Text 검색기의 정확도가 낮음에도 불구하고(아래 섹션 참조) 놀라운 향상입니다. 4.6. ASR 결과 다중 작업 목표를 사용하여 ASR을 포함하도록 모델을 학습했으므로 인식 작업에서 모델의 성능을 평가할 수 있습니다. 두 가지 명확한 추세가 있습니다. Speech2Text 어댑터는 SLM의 모든 조건에서 ASR 성능을 개선합니다. 또한 범용 기준 RNN-T ASR 모델(13.0% WER)과 유리하게 비교됩니다[5]. Speech2Entity 검색기도 사용하면 모든 경우에서 이득이 더욱 향상되어 대화 상태 추적의 결과가 반영됩니다. 유용한 절제 연구 중 하나는 DST 손실 없이 ASR 이득을 이해하는 것입니다. 음성 입력을 공급하고 ASR 작업만을 위한 모델을 학습하여 이를 테스트하고 결과를 표 4에 보고합니다. ASR 성능은 도메인 내 ASR 시스템과 일치합니다(10.4% 대 10.7%). LLM을 고정한 상태에서도 이 성능을 달성할 수 있었기 때문에 Speech2Text 어댑터가 음향 인코딩 공간에서 텍스트 인코딩 공간으로 매핑할 수 있다고 가정합니다. 흥미로운 점은 Speech2Text 검색기가 ASR 작업만으로 학습하면 추가 이득을 가져오지 않지만 DST 작업으로 학습하면 이득을 가져온다는 것입니다(표 2). 이는 DST 손실이 엔터티 인식과 해당 컨텍스트의 의미론을 개선하는 데 더욱 중점을 두기 때문일 수 있습니다.
--- CONCLUSION ---
s 우리는 음성과 텍스트 입력을 모두 사용하는 공동 음성 및 언어 모델(SLM)을 제안했습니다. 음성 입력은 별도로 훈련된 CTC 인코더를 사용하여 인코딩되며 여기서 입력은 downTable 3: 음성 인식 성능. 사전 훈련된(섹션 3.2 참조) Speech2Text 어댑터가 있는 경우와 없는 경우, 검색된 엔터티가 있는 경우와 없는 경우의 모델 성능을 비교합니다. 여기서 WER 값은 ASR 전용 설정(표 2의 공동 DST-ASR 설정과 다름)에서 계산되었습니다. % WER↓↓ RNN-T [5] 13.RNN-T 도메인 내 미세 조정 [5] 어댑터 전용 10.10.Trainable params Speech2Text 어댑터 없는 SLM 전체 T12.T5 인코더+emb T5 인코더 전용 11.11.Speech2Text 어댑터 포함 전체 T9.T5 인코더+emb T5 인코더 전용 9.9.표 4: 음성 인식 성능. 사전 훈련된(섹션 3.2 참조) Speech2Text 어댑터가 있는 경우와 없는 경우, 검색된 엔터티가 있는 경우와 없는 경우의 모델 성능을 비교합니다. 여기의 WER 값은 ASR 전용 설정(표 2의 공동 DST-ASR 설정과 다름)에서 계산되었습니다. CTC 디코더에서 공백 심볼을 필터링하여 샘플링했습니다. CTC 기반 공백 필터링은 이전 작업에서 텍스트를 음성으로 업샘플링하는 것과 달리 음성 프레임 수를 줄여 텍스트 단위와 대략 일치시킵니다[7]. seq-to-seq ASR 작업으로 학습한 Speech2Text 어댑터는 공백 필터링된 음성 인코딩을 텍스트 인코딩 공간으로 변환합니다. 이를 통해 음성과 텍스트 입력의 내용을 이해하기 위해 사전 학습된 대규모 언어 모델을 쉽게 사용할 수 있습니다. 이 모델은 인식과 다운스트림 음성 이해 작업을 동시에 수행하도록 학습할 수 있습니다. 다양한 매개변수 그룹을 학습하여 비교한 DST 및 ASR 작업 모두에 대한 결과는 LLM 디코더를 학습하지 않아도 Speech2Text 어댑터에서 대부분의 성능 이득을 얻을 수 있음을 보여줍니다. 이는 어댑터가 음성을 텍스트 인코더 공간으로 가져오는 데 효과적임을 시사합니다. 또한, SLM 인코더가 있는 2타워 모델을 사용하여 음성 입력과 관련된 엔터티를 선택하는 Speech2Entity 검색기를 소개합니다. 검색 기반 SLM(ReSLM)에서 검색된 엔터티를 텍스트 입력에 미리 추가하여 작업별 엔터티와 관련된 대화 상태를 추론하는 성능이 향상될 수 있음을 보여줍니다. 이는 또한 다운스트림 음성 이해 작업, 즉 대화 상태 예측(34.5% JGA)에서 상당한 향상으로 이어집니다. 따라서 Speech2Text 어댑터와 Speech2Text 검색기를 결합한 시스템은 DST가 오류가 발생하기 쉬운 ASR 필사본에서 학습된 강력한 캐스케이드 기준선 시스템(31.8% JGA)보다 성능이 뛰어납니다. 마찬가지로 어댑터와 검색기를 사용한 ReSLM 모델(8.6% WER)은 강력한 도메인 내 ASR 기준선(10.4% WER)보다 성능이 뛰어납니다. 실험은 DST 작업에서 수행되지만, 이 모델은 더 광범위하게 적용 가능하며 더 나은 검색기를 사용하면 성능을 더욱 향상시킬 수 있습니다. 6. 감사의 말 귀중한 도움을 주신 Jeffrey Zhao, Abhinav Rastogi, Aramys Miranda에게 감사드립니다. 7. 참고문헌 [1] A. Bapna, Y. Chung, N. Wu, A. Gulati, Y. Jia, JH Clark, M. Johnson, J. Riesa, A. Conneau, Y. Zhang, &quot;SLAM: 음성-텍스트 공동 사전 학습을 통한 음성 및 언어 모델링을 위한 통합 인코더&quot;, CORR, vol. abs/2110.10329, 2021. [온라인]. 사용 가능: https://arxiv.org/abs/2110.[2] S. Thomas, B. Kingsbury, G. Saon, 및 H.-KJ Kuo, &quot;rnn 변환기 asr 모델 훈련 및 적응을 위한 텍스트 입력 통합&quot;, Proc. ICASSP, 2022. [3] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, PJ Moreno, A. Bapna, 및 H. Zen, &quot;MAESTRO: 모달리티 매칭을 통한 매칭된 음성 텍스트 표현&quot;, Proc. Interspeech, 2022. [4] J. Zhao, R. Gupta, Y. Cao, D. Yu, M. Wang, H. Lee, A. Rastogi, I. Shafran, 및 Y. Wu, &quot;설명 기반 작업 지향 대화 모델링&quot;, arXiv 사전 인쇄본 arXiv:2201.08904, 2022. [5] H. Soltau, I. Shafran, M. Wang, A. Rastogi, J. Zhao, Y. Jia, W. Han, Y. Cao, and A. Miranda, &quot;음성 인식 대화 시스템 기술 과제(dstc11)&quot;, arXiv 사전 인쇄본 arXiv:2212.08704, 2022. [6] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao, and D. Hakkani-Tür, &quot;Multiwoz 2.1: 다중 도메인 대화 상태 수정 및 상태 추적 기준선&quot;, CORR, vol. abs/1907.01669, 2019. [온라인]. 사용 가능: http://arxiv.org/abs/ 1907.[7] A. Rosenberg, Y. Zhang, B. Ramabhadran, Y. Jia, PJ Moreno, Y. Wu, and Z. Wu, &quot;증강 합성 음성을 통한 음성 인식&quot;, CORR, vol. abs/1909.11699, 2019. [온라인]. 제공: http://arxiv.org/abs/1909.[8] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, G. Wang, &quot;Tts4pretrain 2.0: 일관성 및 대조적 손실을 통해 ASR 사전 학습에서 텍스트 및 음성 사용 향상&quot;, Proc. ICASSP, 2022. [9] A. Graves, S. Fernández, F. Gomez, J. Schmidhuber, &quot;연결주의 시간 분류: 순환 신경망을 사용하여 분할되지 않은 시퀀스 데이터 레이블링&quot;, Proc. ICML. Association for Computing Machinery, 2006. [10] Y. Wang, Z. Chen, C. Zheng, Y. Zhang, W. Han, and P. Haghani, &quot;CTC 가이드를 사용한 rnn-t 훈련 및 추론 가속화&quot;, 2022. [온라인]. 사용 가능: https://arxiv.org/abs/2210.[11] A. Graves, &quot;순환 신경망을 사용한 서열 변환&quot; CORR, 2012. [12] A. Jaegle, S. Borgeaud, J. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, OJ Hénaff, MM Botvinick, A. Zisserman, O. Vinyals, 및 J. Carreira, &quot;Perceiver io: 구조화된 입력 및 출력을 위한 일반 아키텍처.&quot; arXiv, 2021. [13] J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. 공, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman 및 K. Simonyan, &quot;Flamingo: 소수 학습을 위한 시각적 언어 모델&quot; CORR, vol. abs/2204.14198, 2022. [14] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, O. Teboul, D. Grangier, M. Tagliasacchi 및 N. Zeghidour, &quot;Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식&quot;, arXiv 사전 인쇄 arXiv:2209.03143, 2022. [15] F. Wu, K. Kim, S. Watanabe, K. Han, R. McDonald, KQ Weinberger, 및 Y. Artzi, &quot;Wav2seq: 가상 언어를 사용한 음성-텍스트 인코더-디코더 모델 사전 학습,&quot; 2022. [온라인]. 사용 가능: https://arxiv.org/abs/2205.[16] U. Khandelwal, A. Fan, D. Jurafsky, 및 LZ와 M. Lewis, &quot;최근접 이웃 기계 번역,&quot; Proc. ICLR, 2021. [17] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. vd Driessche, J.-B. Lespiau, B. Damoc, A. Clark, D. d. L. Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, JW Rae, E. Elsen, L. Sifre, &quot;수조 개의 토큰에서 검색하여 언어 모델 개선&quot;, CORR, vol. abs/2112.04426, 2021. [18] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, &quot;Atlas: 검색 증강 언어 모델을 사용한 Few-shot learning&quot;, 2022. [온라인]. 이용 가능: 한국어: https://arxiv.org/abs/2208.[19] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, &quot;검색 증강 언어 모델 사전 학습&quot;, Proc. ICML, vol. 119. PMLR, 2020, pp. 3929–3938. [20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, &quot;지식 집약적 nlp 작업을 위한 검색 증강 생성&quot;, Proc. NIPS, vol. 33, 2020, pp. 9459–9474. [21] G. Izacard 및 E. Grave, &quot;질문에 답하기 위해 독자로부터 검색자에게 지식 전달&quot;, Proc. ICLR, 2021. [온라인]. 사용 가능: https://openreview.net/forum?id=NTEz-6wysdb [22] C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong, R. Socher, and P. Fung, &quot;과제 지향 대화 시스템을 위한 이전 가능한 다중 도메인 상태 생성기&quot;, Proc. ACL, 2019년 7월, 808-819쪽. [23] L. Zhou and K. Small, &quot;동적 지식 그래프로 향상된 질의 응답을 위한 다중 도메인 대화 상태 추적&quot;, CORR, vol. abs/1911.06192, 2019. [온라인]. 사용 가능: http://arxiv.org/abs/1911.[24] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, &quot;확장 가능한 다중 도메인 대화 에이전트를 향하여: 스키마 가이드 대화 데이터 세트&quot;, Proc. AAAI 인공지능 컨퍼런스, 2020. [25] P. Pasupat, Y. Zhang 및 K. Guu, &quot;검색 증강을 통한 제어 가능한 의미 구문 분석&quot;, Proc. EMNLP, 2021년 11월, 7683-7698쪽. [26] R. Gupta, H. Lee, J. Zhao, Y. Cao, A. Rastogi 및 Y. Wu, &quot;말하지 말고 보여주세요. 스키마 기반 작업 지향 대화의 경우 설명보다 데모가 더 효과적입니다.&quot;, Proc. NACCL. ACL, 2022년 7월. [27] D. Yu, M. Wang, Y. Cao, L. El Shafey, I. Shafran, H. Soltau, &quot;지식 기반 대화 상태 추적&quot;, Proc. EMNLP, 2022년. [28] J. Devlin, M. Chang, K.Lee, K. Toutanova, &quot;Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습&quot;, arXiv 사전 인쇄본 arXiv:1810.04805, 2018. [29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, PJ Liu, &quot;통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계 탐색&quot;, The Journal of Machine Learning Research, 2020. [30] A. Radford, K. Narasimhan, T. Salimans, 및 I. Sutskever, &quot;생성적 사전 학습을 통한 언어 이해 향상&quot;, 2018. [31] J. Ni, C. Qu, J. Lu, Z. Dai, GH Ábrego, J. Ma, VY Zhao, Y. Luan, KB Hall, M.-W. Chang, 및 Y. Yang, &quot;대형 듀얼 인코더는 일반화 가능한 검색기&quot;, arXiv 사전 인쇄본 arXiv:2112.07899, 2021. [32] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, 및 S. Kumar, &quot;이방성 벡터 양자화를 통한 대규모 추론 가속화&quot;, 국제 기계 학습 컨퍼런스. PMLR, 2020, 3887-3896쪽. [33] D. Galvez, G. Diamos, J. Ciro, JF Cerón, K. Achorn, A. Gopi, D. Kanter, M. Lam, M. Mazumder 및 VJ Reddi, &quot;대중의 연설: 상업적 사용을 위한 대규모 다양한 영어 음성 인식 데이터 세트&quot;, CoRR, vol. abs/2111.09344, 2021.
