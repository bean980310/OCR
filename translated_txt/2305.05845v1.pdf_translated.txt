--- ABSTRACT ---
비디오 콘텐츠의 확산은 새로운 비디오 콘텐츠를 생성하기 위한 효율적이고 유연한 신경망 기반 접근 방식을 요구합니다. 이 논문에서는 이러한 모델의 출력을 개선하기 위해 제로 샷 텍스트-비디오 생성과 ControlNet [2]을 결합하는 새로운 접근 방식을 제안합니다. 저희의 방법은 여러 스케치 프레임을 입력으로 사용하여 이러한 프레임의 흐름과 일치하는 비디오 출력을 생성하며, Text-to-Video Zero 아키텍처 [1]를 기반으로 하고 ControlNet을 통합하여 추가 입력 조건을 활성화합니다. 먼저 입력된 스케치 사이의 프레임을 보간한 다음 제어 기술로 새로운 보간 프레임 비디오를 사용하여 Text-to-Video Zero를 실행하여 제로 샷 텍스트-비디오 생성과 ControlNet이 제공하는 강력한 제어의 이점을 모두 활용합니다. 실험 결과 저희의 방법은 비디오 내 피사체에 대한 사용자의 의도된 동작과 보다 정확하게 일치하는 고품질의 놀라울 정도로 일관된 비디오 콘텐츠를 생성하는 데 탁월합니다. 저희는 데모 비디오, 프로젝트 웹사이트, 오픈 소스 GitHub 저장소, Colab 플레이그라운드를 포함한 포괄적인 리소스 패키지를 제공하여 제안된 방법에 대한 추가 연구와 응용을 촉진합니다.
--- INTRODUCTION ---
인터넷에서 비디오 콘텐츠의 급속한 성장으로 인해 새로운 비디오 콘텐츠를 생성하기 위한 신경망 기반 접근 방식에 대한 관심이 급증했습니다. 그러나 레이블이 지정된 비디오 데이터의 오픈 데이터 세트가 부족하여 텍스트-비디오 모델을 학습하는 것이 어렵습니다. 더욱이 프롬프트의 특성으로 인해 기존 텍스트-비디오 모델에서 비디오를 생성하는 것이 어렵습니다. 이러한 과제를 해결하기 위해, 우리는 제로샷 텍스트-비디오 생성의 이점과 ControlNet이 제공하는 강력한 제어를 결합한 새로운 접근 방식을 제안합니다 [2]. 우리 모델은 안정적 확산 [3]과 같은 기존 텍스트-이미지 합성 방법의 힘을 활용하여 저비용 비디오 생성을 허용하는 텍스트-비디오 제로 아키텍처 [1]를 기반으로 합니다. 우리가 도입한 주요 수정 사항에는 동작 역학으로 생성된 프레임의 잠재 코드를 풍부하게 하고 새로운 프레임 간 주의 메커니즘을 사용하여 프레임 수준의 자기 주의를 재프로그래밍하는 것이 포함됩니다. 이러한 수정 사항은 전역 장면 및 배경 시간 일관성을 보장하고 전경 개체의 컨텍스트, 모양 및 정체성을 보존합니다. 생성된 비디오 콘텐츠에 대한 제어를 더욱 강화하기 위해 ControlNet 구조를 통합합니다. ControlNet은 에지 맵, 분할 맵, 키포인트와 같은 추가 입력 조건을 사용할 수 있으며, 엔드투엔드 방식으로 작은 데이터 세트에서 학습할 수 있습니다. Textto-Video Zero를 ControlNet과 결합함으로써 최소한의 리소스를 사용하여 비디오 콘텐츠를 생성하고 제어하기 위한 강력하고 유연한 프레임워크를 만듭니다. 저희 모델은 여러 스케치 프레임을 입력으로 사용하여 이러한 프레임의 흐름과 일치하는 비디오 출력을 생성합니다. 먼저 입력된 스케치 사이에 프레임을 보간한 다음 제어 기술로 새로운 보간 프레임 비디오를 사용하여 Text-to-Video Zero를 실행합니다. 실험 결과 저희 방법은 추가 비디오 데이터에서 학습하지 않았음에도 불구하고 낮은 오버헤드로 고품질의 놀라울 정도로 일관된 비디오 콘텐츠를 생성할 수 있음을 보여줍니다. 저희의 접근 방식은 텍스트-비디오 합성에 국한되지 않고 조건부 및 콘텐츠 전문 비디오 생성, 비디오 Instruct-Pix2Pix, 즉 지침 기반 비디오 편집과 같은 다른 작업에도 적용할 수 있습니다. 요약하자면, 우리는 Textto-Video Zero [1]와 ControlNet [2]의 장점을 결합한 새로운 접근 방식을 제시하여 최소한의 리소스로 비디오 콘텐츠를 생성하고 제어하기 위한 강력하고 유연한 프레임워크를 제공합니다. 이 작업은 다양한 애플리케이션 도메인에 맞춰 효율적이고 효과적인 비디오 생성을 위한 새로운 가능성을 열어줍니다. Φ 텍스트 프롬프트: &quot;일몰을 지나 떠다니는 열기구&quot;그림 1: STF의 기능 개요: [입력] 위쪽에 기본 텍스트 프롬프트, [입력] 두 번째 행에 스케치, 세 번째 행에 보간, [출력] 아래쪽 행에 생성된 비디오의 결과 프레임 2
--- RELATED WORK ---
이 섹션에서는 우리의 작업에 상당한 영향을 미친 두 가지 주요 논문, 즉 Text-to-Video Zero 논문과 ControlNet 논문에 대해 논의합니다. 2.1 Text-to-Video Zero Text-to-Video Zero 논문[1]은 계산적으로 무거운 훈련과 대규모 비디오 데이터 세트에 의존하지 않고 텍스트 프롬프트에서 비디오를 생성하는 과제를 다룹니다. 저자는 기존 텍스트-이미지 합성을 활용하는 제로샷 텍스트-비디오 생성 접근 방식을 소개합니다.
--- METHOD ---
여러 스케치 프레임을 입력으로 받고 이러한 프레임의 흐름과 일치하는 비디오 출력을 생성하며, Text-to-Video Zero 아키텍처[1]를 기반으로 하고 ControlNet을 통합하여 추가 입력 조건을 활성화합니다. 먼저 입력된 스케치 사이의 프레임을 보간한 다음 새로운 보간 프레임 비디오를 제어 기술로 사용하여 Text-to-Video Zero를 실행하여 제로 샷 텍스트-비디오 생성과 ControlNet이 제공하는 강력한 제어의 이점을 모두 활용합니다.
--- EXPERIMENT ---
s는 우리 방법이 비디오 내 피사체에 대한 사용자의 의도된 동작과 보다 정확하게 일치하는 고품질의 놀라울 정도로 일관된 비디오 콘텐츠를 생성하는 데 탁월함을 보여줍니다. 우리는 데모 비디오, 프로젝트 웹사이트, 오픈소스 GitHub 저장소, Colab 플레이그라운드를 포함한 포괄적인 리소스 패키지를 제공하여 제안된 방법에 대한 추가 연구와 응용을 촉진합니다. 서론 인터넷에서 비디오 콘텐츠의 급속한 성장으로 인해 새로운 비디오 콘텐츠를 생성하기 위한 신경망 기반 접근 방식에 대한 관심이 급증했습니다. 그러나 레이블이 지정된 비디오 데이터의 오픈 데이터 세트가 부족하여 텍스트-비디오 모델을 학습하는 것이 어렵습니다. 또한 프롬프트의 특성으로 인해 기존 텍스트-비디오 모델에서 비디오를 생성하는 것이 어렵습니다. 이러한 과제를 해결하기 위해 ControlNet [2]에서 제공하는 강력한 제어와 제로 샷 텍스트-비디오 생성의 이점을 결합하는 새로운 접근 방식을 제안합니다. 우리 모델은 안정적 확산 [3]과 같은 기존 텍스트-이미지 합성 방법의 힘을 활용하여 저비용 비디오 생성을 허용하는 텍스트-비디오 제로 아키텍처 [1]를 기반으로 구축됩니다. 우리가 도입하는 주요 수정 사항에는 생성된 프레임의 잠재 코드를 동작 역학으로 풍부하게 하고 새로운 프레임 간 주의 메커니즘을 사용하여 프레임 수준의 자기 주의를 재프로그래밍하는 것이 포함됩니다. 이러한 수정 사항은 전역 장면 및 배경 시간 일관성을 보장하고 전경 객체의 컨텍스트, 모양 및 정체성을 보존합니다. 생성된 비디오 콘텐츠에 대한 제어를 더욱 강화하기 위해 ControlNet 구조를 통합합니다. ControlNet은 에지 맵, 분할 맵 및 키포인트와 같은 추가 입력 조건을 활성화하며 종단 간 방식으로 작은 데이터 세트에서 학습할 수 있습니다. Textto-Video Zero를 ControlNet과 결합하여 최소한의 리소스를 사용하여 비디오 콘텐츠를 생성하고 제어하기 위한 강력하고 유연한 프레임워크를 만듭니다. 우리 모델은 여러 스케치 프레임을 입력으로 사용하여 이러한 프레임의 흐름과 일치하는 비디오 출력을 생성합니다. 먼저 입력된 스케치 사이에 프레임을 보간한 다음 새로운 보간 프레임 비디오를 제어 기술로 사용하여 Text-to-Video Zero를 실행합니다. 실험 결과, 추가 비디오 데이터에서 학습하지 않았음에도 불구하고 우리의 방법은 낮은 오버헤드로 고품질의 놀라울 정도로 일관된 비디오 콘텐츠를 생성할 수 있습니다. 우리의 접근 방식은 텍스트-비디오 합성에 국한되지 않고 조건부 및 콘텐츠 전문 비디오 생성, 비디오 Instruct-Pix2Pix, 즉 지시 기반 비디오 편집과 같은 다른 작업에도 적용할 수 있습니다. 요약하자면, 우리는 Textto-Video Zero [1]와 ControlNet [2]의 장점을 결합한 새로운 접근 방식을 제시하여 최소한의 리소스를 사용하여 비디오 콘텐츠를 생성하고 제어하기 위한 강력하고 유연한 프레임워크를 제공합니다. 이 작업은 다양한 응용 분야를 충족시키는 효율적이고 효과적인 비디오 생성을 위한 새로운 가능성을 열어줍니다. Φ 텍스트 프롬프트: &quot;일몰을 지나 떠다니는 열기구&quot;그림 1: STF의 기능 개요: [입력] 위쪽의 기본 텍스트 프롬프트, [입력] 두 번째 행의 스케치, 세 번째 행의 보간, [출력] 아래쪽 행의 생성된 비디오의 결과 프레임 2 관련 작업 이 섹션에서는 작업에 상당한 영향을 미친 두 가지 주요 논문, 즉 Text-to-Video Zero 논문과 ControlNet 논문에 대해 설명합니다. 2.1 Text-to-Video Zero Text-to-Video Zero 논문[1]은 계산량이 많은 학습과 대규모 비디오 데이터 세트에 의존하지 않고 텍스트 프롬프트에서 비디오를 생성하는 과제를 다룹니다. 저자는 Stable Diffusion[3]과 같은 기존 텍스트-이미지 합성 방법을 활용하여 비디오 도메인에 맞게 조정하는 제로샷 텍스트-비디오 생성 방식을 소개합니다. 이 방법에는 두 가지 주요 수정 사항이 포함됩니다. (i) 전역 장면과 배경의 일관성을 유지하기 위해 생성된 프레임의 잠재 코드를 동작 역학으로 풍부하게 하고 (ii) 영어: 새로운 크로스 프레임 어텐션 메커니즘으로 프레임 수준 셀프 어텐션을 재프로그래밍하여 컨텍스트, 모양 및 객체 정체성을 보존하기 위해 첫 번째 프레임에 초점을 맞춥니다.실험 결과에 따르면 이 접근 방식은 오버헤드가 낮은 고품질의 일관된 비디오를 생성할 수 있으며 조건부 및 콘텐츠 전문 비디오 생성, 지침 기반 비디오 편집을 포함한 다양한 작업에 적용할 수 있습니다.2.2 ControlNet ControlNet 논문[2]은 추가 입력 조건을 지원하여 사전 학습된 대규모 확산 모델을 제어하도록 설계된 신경망 구조를 제시합니다.ControlNet은 엔드투엔드 방식으로 작업별 조건을 학습하며 소규모 학습 데이터 세트(&lt;50k)에서도 강력한 학습을 보입니다.ControlNet을 학습하는 것은 확산 모델을 미세 조정하는 것만큼 빠르므로 개인 기기에서 학습하거나 강력한 계산 클러스터에서 대규모 데이터 세트로 확장할 수 있습니다. 저자는 안정적 확산[3]과 같은 대규모 확산 모델이 ControlNets로 증강되어 에지 맵, 분할 맵, 키포인트와 같은 조건부 입력을 활성화할 수 있으며, 이를 통해 대규모 확산 모델을 제어하고 관련 애플리케이션을 용이하게 하는 방법을 강화할 수 있음을 보여줍니다. 저희의 작업은 ControlNet을 통합하여 이러한 모델의 출력을 개선함으로써 Text-to-Video Zero 아키텍처를 기반으로 합니다. 이러한 기술을 결합하여 텍스트 프롬프트와 여러 스케치 프레임에서 비디오 콘텐츠를 생성하기 위한 보다 효율적이고 유연한 접근 방식을 만드는 것을 목표로 합니다.3 방법 제안하는 방법은 조건부 제어 기술을 통합하여 Text-to-Video 모델의 출력 품질을 개선하는 것을 목표로 합니다. 구체적으로, 저희 모델은 여러 프레임과 시간 쌍을 입력으로 사용하여 해당 프레임과 시간 쌍과 &quot;일치하는&quot; 비디오 출력을 생성합니다. 직관적으로, 저희의 아이디어는 텍스트 프롬프트에서 비디오를 생성하는 것이 비현실적이거나 잘못된 출력을 종종 초래하는 어려운 작업이기 때문에 합리적입니다. 사용자가 프레임과 시간 쌍의 형태로 입력을 제공할 수 있도록 함으로써, 우리 모델은 원하는 결과를 더 잘 이해하고 더 정확하고 바람직한 비디오 콘텐츠를 생성할 수 있습니다.우리가 제안하는 접근 방식은 텍스트-이미지 확산 모델 분야의 이전 작업을 기반으로 합니다.이러한 모델은 안정적인 확산 프로세스를 사용하여 텍스트 프롬프트에서 이미지를 생성합니다.우리는 유사한 접근 방식을 사용하여 안정적 확산(SD) 모델과 이미지-스크리블 쌍에서 비디오를 생성하도록 훈련할 수 있는 잠긴 SD 모델을 만듭니다.이는 ControlNet 논문 [2]에 설명되어 있습니다.텍스트 프롬프트: &quot;유화 스타일의 사슴 도약.&quot;제어 가능한 Text2Video-Zero 프레임 보간 네트워크 ControlNet 그림 2: ControlNet의 아키텍처 개요 또한, 우리의 작업은 Text2VideoZero가 선보인 제로 샷 비디오 생성 접근 방식 [1]에서 많은 영감을 받았습니다.Text2Video-Zero는 프레임 간에 전경과 배경을 일관되게 유지하는 기술을 적용하여 텍스트에서 비디오를 생성하는 안정적 확산을 적용합니다. ~N(0, 1) =DDIM Backward(a, At, SD) =W₁(x) =DDPM Forward(, At)for k 2,3,...,m 텍스트 프롬프트 &quot;유화 스타일로 뛰어오르는 사슴.&quot; 합성 돌출 객체 감지기 선형 K 투영 선형 투영-선형 -Q&#39; 프레임 간 주의 변환기 블록 xร M² 배경 평활화 LE = W₁(z) M+ k1-3)(a)+(1-0) DDIM 단계 for k 1,2,...,m, xT 그림 3: 제어 가능한 Text2 비디오의 아키텍처 우리는 무작위로 잠재 코드를 샘플링한 다음 안정 확산을 사용하여 수정된 잠재 코드를 생성합니다. 그런 다음 비디오의 동작을 시뮬레이션하기 위해 동작 역학으로 잠재 코드를 향상시킵니다. 그런 다음 잠재 코드는 전경 객체가 일관성을 유지하도록 하기 위해 프레임 간 주의가 추가된 ControlNet 모델을 통해 파이프라인됩니다. 크로스 프레임 어텐션을 더 확장하기 위해 각 셀프 어텐션 레이어는 피처 맵을 쿼리, 키 및 피처로 결합합니다.Text2Video-Zero 논문에서 수행한 것처럼 첫 번째 프레임에 대해 각 프레임에 크로스 프레임 어텐션 레이어를 추가합니다.이렇게 하면 전경 및 배경 객체의 동일성이 유지됩니다.4가지 실험 텍스트 프롬프트: &quot;유화 스타일의 사슴이 뛰어오릅니다.&quot; ~N(0,1) z=DDIM Backward(), At, SD) -DDPM Forward(‡, At)-for-2,3,...m 텍스트 프롬프트 &quot;유화 스타일의 사슴이 뛰어오릅니다.&quot; 프레임 보간 ControlNet 네트워크 그림 4: STF 배경 평활화 포크 1,2,m, xT의 아키텍처 개요 실험에서 제안한 Sketching the Future(STF) 접근 방식에서 제공하는 제어를 사용하고 사용하지 않을 때 Text2 Video-Zero의 성능을 정성적으로 비교합니다. 우리의 목표는 원래 Text2Video-Zero 모델이 의도한 동작을 포착하지 못하더라도 생성된 비디오 출력을 원하는 비전과 일치하도록 안내하는 STF의 효과를 보여주는 것입니다. 비디오 모델을 사용한 우리의 목표는 프레임의 왼쪽에서 오른쪽으로 걷는 남자의 비디오를 생성하는 것입니다. 우리는 텍스트 프롬프트만 있는 Text2Video-Zero의 효과성을 살펴보고, 그런 다음 텍스트 프롬프트와 함께 제어를 위한 스케치가 있는 STF를 살펴볼 것입니다. 4. 기준선: 제어 없는 Text2 Video-Zero 먼저 원하는 비디오를 설명하는 프롬프트에서 Text2Video-Zero 모델을 테스트합니다. &quot;바다 앞 해변을 걷는 남자.&quot; 결과 비디오는 카메라를 향해 해변을 걷는 남자의 다리를 보여줍니다. 해변과 바다의 맥락이 포착되었지만, 비디오는 프레임을 가로질러 왼쪽에서 오른쪽으로 걷는 남자에 대한 우리의 시각을 전달하지 못합니다.텍스트 프롬프트: &quot;바다 앞 해변을 걷는 남자&quot; 그림 5: 기본 프롬프트가 있는 Text2Video-Zero와 하단 행에 생성된 비디오의 결과 프레임 원하는 동작을 더 잘 전달하기 위해 프롬프트를 다음과 같이 확장합니다. &quot;바다 앞 해변을 걷는 남자 [왼쪽에서 오른쪽으로].&quot; 그러나 생성된 비디오는 여전히 우리의 기대에 부응하지 못합니다. 남자의 발을 클로즈업으로만 보여주고, &quot;왼쪽에서 오른쪽&quot; 측면을 나타내지 않고 프레임에 고정된 채로 있기 때문입니다. 이는 텍스트 프롬프트만으로 특정 동작 세부 정보를 포착하는 데 있어 Text2Video-Zero 모델의 한계를 강조합니다. 텍스트 프롬프트: &quot;바다 앞 해변을 왼쪽에서 오른쪽으로 걷는 남자&quot; 그림 6: 프롬프트에 모션 세부 정보가 있는 Text2Video-Zero와 아래 행에 생성된 비디오의 결과 프레임 4.2 제안된 접근 방식: 미래 스케치 Text2Video-Zero의 한계를 해결하기 위해 추가 입력으로 세 개의 스틱 피규어 남자 스케치를 통합하는 STF 접근 방식을 사용합니다. 첫 번째 스케치는 남자를 프레임의 왼쪽에, 두 번째 스케치는 가운데에, 세 번째 스케치는 오른쪽에 배치합니다. 이러한 스케치는 원하는 동작을 더 명확하게 전달하는 데 도움이 되고 모델에 의도한 비디오를 생성하는 데 필요한 추가 시각적 단서를 제공합니다. 텍스트 프롬프트: &quot;바다 앞 해변을 걷는 남자&quot; DESi 그림 7: 기본 텍스트 프롬프트가 있는 STF, 두 번째 행에 스케치, 세 번째 행에 보간, 아래 행에 생성된 비디오의 결과 프레임 STF를 Text2Video-Zero 모델에 적용하면 결과 비디오가 우리의 비전과 완벽하게 일치하여 남자가 해변의 왼쪽에서 오른쪽으로 걷는 모습이 나타납니다. 남자의 움직임이 명확하게 포착되었고, 배경은 비디오 전체에서 일관되게 유지됩니다. 이는 비디오 생성 프로세스를 안내하고 출력에 대한 더 많은 제어를 제공하는 데 있어서 저희 접근 방식의 효과를 보여주는데, 이는 Text2 Video-Zero만으로는 달성할 수 없었습니다. 저희 STF 접근 방식의 성공은 텍스트 프롬프트와 시각적 입력을 결합하여 보다 정확하고 제어된 비디오 생성 작업을 수행할 수 있는 잠재력을 강조합니다. 5
--- CONCLUSION ---
s 및 토론 이 작업에서 우리는 이러한 모델의 출력을 개선하기 위해 제로 샷 텍스트-비디오 생성과 ControlNet을 결합한 새로운 접근 방식인 STF(Sketching the Future)를 제시했습니다. 우리의 방법은 여러 개의 스케치된 프레임을 입력으로 사용하여 이러한 프레임의 흐름과 일치하는 비디오 출력을 생성하여 순수한 프롬핑이 있는 Text2Video-Zero에 비해 원하는 동작을 더 정확하게 표현합니다. 우리의 실험은 스케치된 프레임을 포함하면 STF가 원하는 동작 특성에 더 부합하는 비디오 콘텐츠를 생성할 수 있음을 보여주었습니다. 우리는 제어가 없는 Text2VideoZero, 추가 프롬핑이 있는 Text2 Video-Zero 및 제안된 STF 접근 방식의 성능을 비교했습니다. 결과에 따르면 STF는 원하는 비디오 콘텐츠를 생성할 수 있었지만 다른 방법은 의도한 동작을 완전히 포착하지 못했습니다. 실험은 본질적으로 정성적이기는 했지만 우리 접근 방식의 역량에 대한 귀중한 통찰력을 제공했습니다. 세 가지 별개의 사례에 초점을 맞춤으로써 우리는 기존 방법에 비해 STF의 장점을 보여줄 수 있었습니다. 첫 번째 사례에서 Text2Video-Zero는 남자의 다리가 왼쪽에서 오른쪽으로가 아니라 카메라를 향해 걷는 영상을 생성했지만 프롬프트와 부분적으로만 일치하는 것을 관찰했습니다. 두 번째 사례에서 프롬프트에 확장을 추가하면 남자의 발이 나오는 영상이 생성되었지만 프롬프트의 &quot;왼쪽에서 오른쪽&quot; 측면을 고려하지 못했습니다. 마지막으로 STF를 사용하면 생성된 영상은 원하는 대로 해변을 가로질러 왼쪽에서 오른쪽으로 걷는 남자를 정확하게 묘사했습니다. 이러한 정성적 결과는 주어진 프롬프트와 스케치를 기반으로 보다 정확한 영상 콘텐츠를 생성하는 데 있어 우리 접근 방식의 효과를 강조했습니다. 우리 방법은 종종 입력 프롬프트의 전체 의미를 포착하는 데 어려움을 겪는 기존 텍스트-비디오 생성 접근 방식의 한계를 극복했습니다. 실험에서 보여진 것처럼 STF의 성공은 영상 생성 프로세스를 보다 효과적으로 안내하기 위해 스케치된 프레임과 같은 추가 입력 조건을 통합하는 것의 중요성을 강조합니다. 이 작업은 향후 연구를 위한 여러 길을 열어줍니다. 한 가지 잠재적인 방향은 오디오나 텍스트와 같은 추가 유형의 입력 모달리티를 통합하여 생성된 비디오 콘텐츠에 대한 제어를 더욱 강화하는 것입니다. 나아가 텍스트 설명이나 다른 형태의 입력에서 스케치된 프레임을 자동으로 생성하는 방법을 조사하여 비디오 생성 프로세스를 간소화하는 것도 흥미로울 것입니다. 또한 스케치된 프레임 간의 보간 프로세스를 개선하고 제어 메커니즘을 최적화하여 생성된 비디오의 품질을 개선하는 방법을 탐색하면 더 나은 결과를 얻을 수 있습니다. 또 다른 잠재적인 연구 방향은 보다 복잡하고 역동적인 장면을 포함하여 더 광범위한 작업과 시나리오에서 STF의 성능을 평가하여 다재다능함과 적용 가능성을 더욱 입증하는 것입니다. 이를 통해 이 방법의 일반화 가능성과 다양한 실제 응용 프로그램에서 채택할 수 있는 잠재력을 확립하는 데 도움이 될 것입니다. 결론적으로 STF는 추가 비디오 데이터로 훈련되지 않았음에도 불구하고 낮은 오버헤드로 고품질의 놀라울 정도로 일관된 비디오 콘텐츠를 생성하는 유망한 접근 방식을 제공합니다. 여러 스케치 프레임을 입력으로 활용하고 ControlNet과 같은 강력한 기술과 결합하는 기능은 조건부 및 콘텐츠 전문 비디오 생성, 지침 기반 비디오 편집을 포함한 다양한 애플리케이션에 귀중한 도구가 됩니다. 기존 텍스트-비디오 생성 방법의 한계를 해결함으로써 STF는 미래에 보다 효과적이고 효율적인 비디오 콘텐츠 생성을 위한 길을 열어줍니다. 오픈 리소스 이 섹션에서는 STF(Sketching the Future) 프로젝트와 관련된 리소스 목록을 제공합니다. 이러한 리소스에는 데모 비디오, 프로젝트 웹사이트, 오픈 소스 GitHub 저장소, 사용자가 STF를 대화형으로 시도할 수 있는 Colab 놀이터가 포함됩니다. 1. 데모 비디오: STF 접근 방식의 기능과 생성된 비디오 콘텐츠를 보여주는 비디오 데모는 다음 링크에서 찾을 수 있습니다. https://www.youtube. com/watch?v=5QH3T7mhomw. 2. 프로젝트 웹사이트: 최신 업데이트, 뉴스, 관련 게시물을 포함한 STF 프로젝트에 대한 자세한 내용은 프로젝트 웹사이트 https://sketchingthefuture.github.io/에서 확인하세요. 3. GitHub 저장소: STF의 소스 코드는 GitHub 저장소에서 공개적으로 제공됩니다. 사용자는 다음 링크에서 코드에 액세스하고, 문제를 보고하고, 프로젝트에 기여할 수 있습니다. https://github.com/rohandkn/skribble2vid. 원하는 프레임을 스케치하고 텍스트 프롬프트를 입력하여 비디오 결과를 생성합니다. 미래 스케치 텍스트 프롬프트 입력 일몰을 지나 떠다니는 열기구 지우기 제출 0* OX 플래그 그림 8: Colab 오픈 플레이그라운드 4. Colab 플레이그라운드: 사용자가 STF 접근 방식을 대화형으로 시도하고 다양한 스케치된 프레임과 텍스트 프롬프트로 실험할 수 있도록 Colab 플레이그라운드를 만들었습니다. 사용자는 다음 링크에서 플레이그라운드에 액세스할 수 있습니다. https://colab.research. google.com/drive/1xc2P3x-10oqsMxA-2SuDlwoE41vCS0m6?usp=sharing.사용자가 이러한 리소스를 탐색하고 피드백을 제공하도록 권장합니다.이는 STF 프로젝트를 개선하고 다양한 산업과 응용 프로그램에 미치는 잠재적 영향을 더 잘 이해하는 데 도움이 됩니다.더 광범위한 영향 STF(Sketching the Future)의 개발은 광범위한 산업과 응용 프로그램에 중요한 의미를 갖습니다.제로 샷 텍스트-비디오 생성과 ControlNet을 결합한 새로운 접근 방식인 STF는 비디오 콘텐츠를 생성하고 소비하는 방식에 큰 영향을 미칠 수 있는 잠재력이 있습니다.이 섹션에서는 채택으로 인해 발생할 수 있는 긍정적, 부정적 모두의 작업의 광범위한 영향에 대해 설명합니다.6.1 긍정적 영향 창의 산업: STF는 영화 제작자, 애니메이터, 그래픽 디자이너와 같은 창의 전문가에게 귀중한 도구가 될 수 있습니다.스케치된 프레임과 텍스트 프롬프트에서 비디오 콘텐츠를 생성할 수 있도록 함으로써, 당사의 접근 방식은 창의적 프로세스를 간소화하고 고품질 비디오 콘텐츠를 만드는 데 필요한 시간과 노력을 줄이는 데 도움이 될 수 있습니다. 광고 및 마케팅: 맞춤형 비디오 콘텐츠를 빠르고 효율적으로 생성하는 기능은 광고 및 마케팅 캠페인에 유익할 수 있습니다. STF는 회사가 매력적이고 타겟팅된 홍보 자료를 만들어 의도한 대상 고객에게 더 잘 다가가 공감을 얻을 수 있도록 도울 수 있습니다. 교육 및 훈련: STF는 특정 학습 목표 또는 훈련 요구 사항에 맞는 교육 자료를 개발하는 데 사용할 수 있습니다. 원하는 학습 성과에 맞는 비디오 콘텐츠를 생성함으로써 당사의 접근 방식은 보다 효과적이고 매력적인 교육 경험에 기여할 수 있습니다. 접근성: STF는 장애가 있는 개인이 비디오 콘텐츠를 더 쉽게 이용할 수 있도록 만들 수 있는 잠재력이 있습니다. 자막이나 기타 시각적 보조 자료를 포함하는 비디오 콘텐츠를 생성함으로써 당사의 접근 방식은 정보와 엔터테인먼트를 보다 포괄적이고 광범위한 대상 고객에게 접근 가능하게 만드는 데 도움이 될 수 있습니다. 6.2 부정적 영향 잘못된 정보 및 딥페이크: 텍스트 프롬프트와 스케치된 프레임에서 사실적인 비디오 콘텐츠를 생성할 수 있는 기능은 잘못된 정보 및 딥페이크 비디오의 가능성에 대한 우려를 제기합니다. 악의적인 행위자는 STF를 사용하여 설득력 있지만 거짓된 비디오 콘텐츠를 만들 수 있으며, 이는 허위 정보를 퍼뜨리거나 여론을 조작하는 데 사용될 수 있습니다. 개인정보 보호 문제: 감시 또는 모니터링 애플리케이션에서 STF를 사용하면 잠재적으로 개인의 개인정보를 침해할 수 있습니다. 인식 가능한 사람이나 장소가 포함된 비디오 콘텐츠를 생성하는 데 당사의 접근 방식을 사용하는 경우 동의 및 데이터 보호와 관련된 윤리적, 법적 문제가 발생할 수 있습니다. 일자리 대체: 수동 비디오 콘텐츠 생성에 의존하는 산업에서 STF를 널리 채택하면 일부 전문가의 일자리 대체가 발생할 수 있습니다. 당사의 접근 방식은 비디오 제작 프로세스를 간소화할 수 있지만 애니메이터 및 비디오 편집자와 같은 창의 산업의 특정 역할에 대한 수요를 줄일 수도 있습니다. 6.3 부정적 영향 완화 이러한 잠재적 부정적 영향을 해결하기 위해 STF 및 유사 기술의 윤리적 사용에 대한 지침과 규정을 개발하는 것이 중요합니다. 연구자, 개발자 및 정책 입안자는 개인정보 보호, 동의 및 비디오 생성 기술의 책임 있는 사용을 우선시하는 모범 사례와 표준을 수립하기 위해 협력해야 합니다. 또한, 대중에게 STF 및 기타 비디오 생성 방법의 기능과 한계에 대해 교육하여 접하는 콘텐츠를 비판적으로 평가하고 해석할 수 있도록 해야 합니다.감사의 말 Visual Learning and Recognition의 Pathak 교수와 과정 직원의 지원에 감사드리며, 컴퓨팅 리소스를 제공해 주신 Mrinal Verghese에게도 감사드립니다.또한 이 논문의 작성과 구성을 도와준 ChatGPT에도 감사드립니다.참고문헌 [1] Khachatryan, Levon, et al. Text2 Video-Zero: Text-to-Image Diffusion Models Are Zero-Shot Video Generators. arXiv, 2023년 3월 23일. arXiv.org, https://doi.org/10.48550/arXiv.2303.13439. [2] Zhang, Lvmin, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. arXiv, 2023년 2월 10일. arXiv.org, https://doi.org/10.48550/arXiv.2302.05543. [3] Rombach, Robin 등. 잠재 확산 모델을 사용한 고해상도 이미지 합성. arXiv, 2022년 4월 13일. arXiv.org, https://doi.org/10.48550/arxiv.2112.10752. [4] Singer, Uriel 등. Make-A-Video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv, 2022년 9월 29일. arXiv.org, https://doi.org/10.48550/arXiv.2209.14792. [5] Ho, Jonathan 등. 비디오 확산 모델. arXiv, 2022년 6월 22일. arXiv.org, https://doi.org/10.48550/arXiv.2204.03458.
