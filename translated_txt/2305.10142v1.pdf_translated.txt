--- ABSTRACT ---
우리는 여러 개의 대규모 언어 모델(LLM)이 협상 게임에서 놀고, 반성하고, 비판함으로써 서로를 자율적으로 개선할 수 있는지 연구합니다. 우리가 이 질문에 관심이 있는 이유는 LLM이 서로를 개선할 수 있다면 최소한의 인간 개입으로 강력한 AI 에이전트를 만들 가능성이 있음을 의미하기 때문입니다. 우리는 두 개의 LLM이 서로 협상하도록 요청하여 각각 구매자와 판매자 역할을 합니다. 그들은 구매자가 더 낮은 가격을 목표로 하고 판매자가 더 높은 가격을 목표로 하는 거래를 성사시키는 것을 목표로 합니다. 비평가 역할을 하는 세 번째 언어 모델은 플레이어에게 피드백을 제공하여 플레이어의 협상 전략을 개선합니다. 우리는 두 에이전트가 이전 협상 내역과 AI 피드백을 맥락 내 데모로 사용하여 모델의 협상 전략을 반복적으로 개선하기 위해 여러 라운드를 플레이하게 합니다. 우리는 다른 역할에 대해 다른 LLM(GPT와 Claude)을 사용하고 거래 가격을 평가 지표로 사용합니다. 우리의 실험은 여러 가지 흥미로운 결과를 보여줍니다. (1) 우리가 고려하는 언어 모델 중 일부만이 자체 플레이를 수행하고 AI 피드백에서 거래 가격을 개선할 수 있으며, 약한 모델은 게임 규칙을 이해하지 못하거나 AI 피드백을 통합하여 추가 개선을 수행할 수 없습니다. (2) 피드백에서 학습하는 모델의 능력은 다른 역할을 수행할 때 다릅니다. 예를 들어, Claude-instant가 판매자보다 구매자로서 개선하는 것이 더 어렵습니다. (3) 게임을 여러 라운드로 펼칠 때 더 강력한 에이전트는 이전 경험과 반복적인 AI 피드백을 의미 있게 사용하여 지속적으로 성과를 개선할 수 있지만 거래를 파기할 위험이 더 높습니다. 우리의 작업이 게임 플레이와 AI 피드백으로 모델이 자율적으로 서로를 개선하는 것에 대한 통찰력 있는 초기 탐색을 제공하기를 바랍니다.
--- INTRODUCTION ---
우리는 여러 대언어 모델(LLM)이 잘 정의된 규칙에 따라 경쟁 게임을 지속적으로 플레이함으로써 AI 에이전트가 스스로를 향상시키는 AlphaGo Zero[31]의 방식처럼 최소한의 인간 개입으로 협상 게임에서 서로를 개선할 수 있는지 연구합니다. 이 연구 질문에 대한 답은 심오한 의미를 갖습니다. 긍정적인 측면에서 에이전트가 자율적으로 개선할 수 있다면 인간 주석이 거의 없는 강력한 에이전트가 생성될 수 있으며, 오늘날의 데이터 소모가 많은 LLM 훈련[8, 17]에 비해 비용을 크게 절감할 수 있습니다. 위험한 측면에서 인간의 감독이 제한된 강력한 에이전트도 의미합니다[4]. 이 작업에서 우리는 두 언어 모델(판매자와 구매자)에게 제품에 대해 협상하도록 요청합니다. 판매자는 더 높은 가격에 제품을 판매하라는 요청을 받는 반면, 구매자는 더 낮은 가격에 구매하는 것을 목표로 합니다(그림 1A). 거래에 도달한 후, 우리는 세 번째 언어 Preprint에 요청합니다. 코드: https://github.com/FranxYao/GPT-Bargaining A. 두 에이전트에게 협상 게임을 하게 하고 AI 비평가가 플레이어 중 한 명에게 협상 전략을 개선할 수 있는 제안을 하도록 합니다. 플레이어는 제안을 인정한 다음 다음 라운드를 개선하는 데 활용하려고 합니다. 구매자 안녕하세요, 풍선은 얼마인가요? 좋은 풍선이고 20달러 정도입니다... 상품입니다. 높은 가격에 판매하려면 판매자 비평가 이해했습니다. 더 높은 가격에 판매하려면 ...을 사용하겠습니다. 판매자 구매자 안녕하세요, 풍선은 얼마인가요? 최고의 장인 정신이 담긴 고급 상품입니다... 판매자 라운드 2. 판매자가 AI 피드백을 기반으로 협상 전략 개선 라운드 1. 판매자와 구매자가 협상 Al 피드백: 비평가가 대화 기록을 읽고 개선 사항을 제안합니다. B. Al 비평가의 피드백 예와 GPT-4 플레이어가 이를 통해 어떻게 개선되는지. 구매자 비평가: 구매자 비평가: &quot;움찔&quot; 기법을 사용하세요: 침묵의 힘을 사용하세요: 구매자는 판매자의 제안에 응답하기 전에 더 오래 멈춰서 협상 과정에서 침묵의 힘을 사용할 수 있습니다. 판매자가 반대 제안을 하면 구매자는 어느 정도 놀라움이나 실망을 표해야 합니다. 구매자 개선: 오! 예상보다 더 비싼데요. 구매자 개선: 다른 매장에서 비슷한 풍선을 14달러에 봤습니다. 그 가격에 맞춰줄 수 있나요? B1. &quot;움찔&quot; 기법 *잠깐 멈춤*... 좋아요, 풍선을 13달러에 사겠습니다. B2. 침묵의 힘 판매자 비평가: 차액 분할 활용: 가격 차이가 약간 남아 있는 상황에서 구매자와 차액을 분할하자고 제안합니다. 맥락: 구매자가 15달러를 제안하고 판매자가 $판매자 개선: 알겠습니다. 차액을 분할해서 예산에 맞게 16.75달러로 하면 어떨까요? B3. 차이 분할 판매자 비평가: 앵커링 기법 사용: 높은 시작 가격을 강조한 다음 약간 낮은 가격을 제시 판매자 개선: 이 고품질의 오래 지속되는 풍선은 실제로 25달러의 가치가 있지만 20달러에 제공합니다.B4. 앵커링 기법 C. 협상 게임에서 AI 피드백을 통해 지속적으로 개선하는 능력은 강력하고 잘 정렬된 모델에서만 나타납니다. 약한 챗봇은 협상하거나 AI 피드백을 듣거나 여러 라운드에 걸쳐 개선할 수 없습니다.GPT-3.5-Turbo → GPT-→ Claude-v1.C1. 캐주얼 시나리오에서의 수다 Claude-instant-v1.Jurassic-2-instruct Cohere-command GPT-3.5-Turbo GPT-Claude-v1.C2. 롤플레잉과 협상 Claude-instant-v1.Jurassic-2-instruct → GPT-4 → GPT-3.5-Turbo → Claude-v1.GPT-→ Claude-instant-v1.Claude-v1.GPT-3.5-Turbo C3. AI 피드백을 통해 다음 라운드 개선 C4. 여러 라운드에 걸쳐 지속적으로 개선 그림 1: 협상 게임 설정.A. 두 LLM 에이전트에게 판매자와 구매자로서 협상 게임을 하도록 요청합니다.그들의 목표는 제품을 더 높거나 낮은 가격에 판매/구매하는 것입니다.한 라운드가 끝나면 세 번째 LLM인 AI 비평가에게 개선하려는 플레이어에게 피드백을 제공해 달라고 요청합니다.그런 다음 플레이어에게 피드백을 기반으로 협상 전략을 개선하도록 요청합니다.이 프로세스를 여러 라운드 동안 반복하고 모델이 지속적으로 개선될 수 있는지 연구합니다.예제 실행은 그림 2를 참조하세요.B. AI 비평가에서 관찰한 협상 기술과 플레이어가 이러한 기술을 협상 전략에 통합하는 방법. C. 게임에 필요한 능력(C2 - 협상, C3 - AI 피드백, C4 - 지속적 개선)은 모델을 여러 계층으로 분류합니다. 강력하고 잘 정렬된 모델(예: gpt-4 및 claude-v1.3)만이 반복적 AI 피드백을 통해 지속적으로 개선될 수 있음을 알아냈습니다(이러한 능력을 보이지 않는 모델의 예는 그림 3 참조). 모델을 비평가로 플레이하고 플레이어에게 피드백을 제공합니다. 그런 다음 게임을 다시 플레이하여 비평가 LLM이 제공한 AI 피드백을 사용하여 플레이어에게 전략을 개선하도록 요청합니다. 협상 게임을 선택한 이유는 텍스트에 설명된 잘 정의된 규칙과 전략적 협상을 위한 명확하고 측정 가능한 목표(낮은/높은 거래 가격)가 있기 때문입니다. 이 게임은 언뜻 보기에 쉬운 것처럼 보이지만 모델이 다음과 같은 특성을 가져야 하기 때문에 언어 모델의 사소하지 않은 능력이 필요합니다. (1) 협상 게임의 텍스트 규칙을 명확하게 이해하고 엄격히 따릅니다. (2) 비평가 LM이 제공한 텍스트 피드백과 일치하고 이를 기반으로 반복적으로 개선합니다(그림 1B의 피드백 예 참조). (3) 장기적으로 전략과 피드백을 반영하고 여러 라운드에 걸쳐 개선합니다. 고려한 모든 모델이 이러한 모든 능력을 보여주는 것은 아니라는 것을 알게 될 것입니다(그림 1C). 그리고 (1) 협상 규칙과 전략을 이해할 수 있고(capable) (2) AI 지시에 응답할 수 있는(well-aligned) 모델만이 여러 라운드에 걸쳐 AI 피드백을 통해 지속적으로 개선될 수 있습니다(실험에서 gpt-3.5-turbo, gpt-4, claude-v1.3만 이러한 요구 사항을 충족). 예비 실험에서 보드 게임과 텍스트 RPG 게임을 포함한 보다 복잡한 텍스트 게임도 시도했지만, 현재 에이전트가 규칙을 이해하고 따르기가 더 어렵습니다. 저희는 이러한 접근 방식을 AI 피드백에서 컨텍스트 내 학습(ICL-AIF)이라고 부릅니다. 구체적으로, 저희는 AI 비평가의 피드백과 이전 라운드의 대화 기록을 컨텍스트 내 데모로 사용합니다[5]. 이렇게 하면 비평가의 개선 제안과 이전 라운드에서 플레이어가 실제로 개선한 것이 효과적으로 다음 라운드 협상을 위한 몇 가지 샷 프롬프트가 됩니다. 우리는 두 가지 이유로 맥락 내 학습을 사용합니다: (1) 강화를 통한 대규모 언어 모델 조정 Claude-instant-v1.0 대 GPT-3.5-Turbo 2023-05-1라운드, Al 이전 피드백 Claude-ins-1.0 | 판매자 안녕하세요, 이것은 좋은 풍선이고 가격은 $Al입니다 판매자에게 피드백 Claude-ins-1.0 | 판매자 비평가 GPT-3.5-T | 구매자 10달러에 판매하는 것을 고려하시겠습니까? Claude-ins-1.0 | 판매자 아니요, 그 가격은 너무 낮습니다. GPT-3.5-T | 구매자 15달러는 어떨까요? Claude-ins-1.0 | 판매자 15달러가 더 좋지만 여전히 충분하지 않습니다. GPT-3.5-T | 구매자 제공할 수 있는 가장 낮은 가격은 얼마입니까? Claude-ins-1.0 | 판매자 저의 가장 낮은 가격은 18달러입니다. GPT-3.5-T | 구매자 알겠습니다. 16달러로 판매할 수 있습니까? 그게 제가 갈 수 있는 최고가입니다. Claude-ins-1.0 | 판매자 오늘은 16달러면 됩니다. 축하합니다. 좋은 거래를 하셨습니다! A. 1라운드, Al이 판매자에게 피드백을 하기 전, 협상 과정은 비교적 간단했습니다. 최종 가격은 16달러였습니다. I. 가격만이 아니라 풍선의 품질과 이점에 집중하세요. 얼마나 희귀하고 특별한지 강조하세요. 2. 구매자와 신뢰하는 판매자로부터 좋은 거래를 한다는 느낌을 주기 위해 관계와 연결을 구축하세요. 3. 필요하다면 물러설 의향이 있어야 합니다. 구매자에게 다른 관심 있는 구매자가 있다는 것을 알려서 가격을 맞추는 것이 가치 있다는 것을 깨닫게 하세요. Claude-ins-1.0 | 판매자 물론입니다. 비평가의 피드백을 바탕으로 협상 전략을 개선해 보겠습니다. 그리고 이전 라운드보다 더 높은 가격(16달러 이상)에 판매하려고 노력하겠습니다. 2라운드, Al 피드백 이후 Claude-ins-1.0 | 판매자 안녕하세요, 이것은 좋은 풍선이고 가격은 $입니다. 10달러에 판매하는 것을 고려해 보시겠습니까? GPT-3.5-T | 구매자 Claude-in-1.0 | 판매자 아니요, 10달러는 너무 낮습니다. 이 풍선은 고품질 라텍스로 만들어졌으며 전문 장인이 수작업으로 만들었습니다. 독특하고 유일무이한 작품입니다. GPT-3.5-T | 구매자 이 풍선에 대해 제안할 수 있는 가장 낮은 가격은 얼마인가요? Claude-ins-1.0 | 판매자 글쎄요, 저는 당신을 신뢰하고 당신의 사업을 소중히 여기기 때문에 이 특별한 풍선을 17달러에 제공하겠습니다. 이 가격은 오늘은 당신과 같은 귀중한 고객에게만 제공됩니다. GPT-3.5-T | 구매자 Claude-ins-1.0 | 판매자 B. Al 비평가는 이전 라운드에 따라 판매자에게 피드백을 제공하고 판매자가 인정합니다. 15달러와 같이 더 좋은 가격을 제시해 주실 수 있나요? 죄송하지만 15달러로는 제 비용을 충당할 수 없습니다. 이 독특하고 고품질의 풍선은 최소한 1달러를 청구해야 합니다. GPT-3.5-T | 구매자 좋아요, 거래가 성사되었습니다. 저는 $17에 특별 풍선을 가져갈 것입니다. C. 2라운드, 판매자는 피드백을 기반으로 추론 전략을 개선하고, 최종 가격은 $17입니다. 그림 2: 협상 게임을 한 다음 AI 피드백을 통해 개선하는 예. A: claude-instant-v1.0은 판매자, gpt-3.5-turbo는 구매자 역할을 하며 풍선에 대해 흥정합니다. B: 그런 다음 claude-instant-v1.0 critic을 사용하여 피드백을 제공합니다. C: 피드백을 받은 판매자는 제안을 기반으로 전략을 개선합니다. &quot;고품질 라텍스와 전문 장인이 수작업으로 제작&quot;과 같은 색상이 있는 문구는 이전 AI 피드백인 &quot;얼마나 희귀하고 특별한가&quot;에 해당합니다. 전반적인 목표는 더 나은 가격을 얻는 것이기 때문에 전략의 효과성을 대신하는 최종 가격을 측정합니다. 이 경우 $16에서 $17로 개선됩니다. 학습은 엄청나게 비용이 많이 들고 [25, 15] 기본 모델 [24]은 광범위한 커뮤니티에서 접근하지 못할 수 있습니다. (2) 컨텍스트 내 학습은 최근에 경사 하강법 [11, 1, 32]과 밀접한 관련이 있는 것으로 나타났으므로 실제로 모델을 미세 조정할 때(리소스가 허용하는 경우) 도출한 결론은 일반화될 가능성이 매우 높습니다. ICL-AIF와 주류 인간 피드백을 통한 강화 학습(RLHF)의 주목할 만한 차이점 중 하나는 RL에서 보상이 스칼라인 반면 [25, 15] ICL에서 피드백은 자연어로 되어 있다는 것입니다. 우리는 AI 피드백을 연구합니다(각 라운드 후 인간의 개입에 의존하는 것이 아니라) 그 이유는 더 확장 가능하며 모델이 자동으로 자체 개선할 수 있습니다.실험을 통해 몇 가지 흥미로운 결과가 도출되었습니다.(1) 협상 게임의 요구 사항은 LLM의 능력을 평가하기 위한 테스트베드 역할을 효과적으로 수행합니다(그림 1C): 대부분의 모델은 캐주얼한 시나리오에서 수다를 떨 수 있지만 실험 날짜(2023년 5월) 기준으로 cohere-command[10] 모델은 협상 규칙을 이해하지 못하고(그림 3A), ai21-jurassic[18] 모델은 AI 피드백에 응답하지 않으며(그림 3B), claude-instant-v1.0은 최대 한 라운드만 개선할 수 있고(그림 5), gpt-3.5-turbo, gpt-4, claude-v1.3만이 여러 라운드에 걸쳐 지속적으로 개선할 수 있습니다.(2) 모델은 다른 역할을 할 때 피드백을 받으면 다르게 작동합니다. 구매자 역할을 하는 모델은 판매자 역할을 할 때보다 개선하기 어려울 수 있습니다(그림 4). (3) gpt-4와 같은 강력한 에이전트가 이전 경험과 온라인 반복적 AI 피드백을 사용하여 지속적으로 의미 있게 개선할 수 있지만, 더 높은 가격에 판매(또는 더 낮은 가격에 매수)하려는 시도에는 전혀 거래에 도달하지 못할 위험이 따릅니다(그림 6). 또한 모델이 덜 장황하지만 더 전략적(따라서 더 효과적인) 방식으로 협상할 수 있다는 증거를 보여줍니다(그림 7). 전반적으로, 우리의 작업이 AI 피드백을 사용하여 게임 설정에서 언어 모델의 협상을 개선하기 위한 의미 있는 이니셔티브가 되기를 바랍니다. 2 문제 설정 우리의 목표는 LLM이 그림 1A에서와 같이 협상 게임을 하고 AI 피드백을 통합하여 서로를 개선할 수 있는지 연구하는 것입니다. 우리는 흥정하는 제품을 풍선으로 설정하고(풍선을 다른 항목으로 변경해도 결과가 유지됨) 우리는 백엔드 LLM 엔진의 다양한 조합을 사용합니다: cohere-command [10], AI21의 jurassic-2 [18], OpenAI의 gpt-3.5-turbo와 gpt-4 [24], Anthropic의 claude-instant-v1.0(gpt-3.5-turbo [14]와 일치한다고 함) 및 claude-v1.3(약간 더 나쁘지만 gpt-4 [14]에 가깝다고 함)). 실험 내내 우리는 두 플레이어 중 하나만 개선하기 위한 피드백을 제공하는 반면, 경쟁자는 피드백을 받지 못하고 이전 라운드의 협상 기록을 지우고 다시 시작합니다. 우리는 경쟁자의 엔진을 gpt-3.5-turbo로 수정하면서 개선되는 모델의 엔진을 변경합니다. 본질적으로 우리 게임은 다른 모든 엔진 대비 gpt-3.5-turbo입니다. 우리는 LM 엔진을 비평가 뒤에 두고 피드백을 제공하는 플레이어와 항상 동일합니다. 한 가지 예시 설정은 gpt-4 판매자가 gpt-3.5-turbo 구매자와 gpt-4 비평가와 함께 게임을 하는 것입니다. 한 라운드가 지난 후, gpt-4 비평가는 gpt-4 판매자에게 피드백을 제공하여 판매자가 다음 라운드에서 개선할 수 있도록 하고, 라이벌 gpt-3.5-turbo 구매자는 대화 기록을 지우고 다시 시작합니다. 게임 프로세스 게임이 시작되기 전에 협상 게임의 규칙이 텍스트 지침을 통해 모델에 설명되어 더 높거나 낮은 가격으로 판매/구매하는 것이 목표입니다. 거래 가격은 예비 실험에서 관찰한 바에 따라 크게 다를 수 있으므로 평가를 쉽게 하기 위해 거래 가격을 [$10, $20]으로 설정했습니다. 이를 달성하기 위해 판매자가 &quot;이것은 좋은 풍선이고 가격은 $20입니다.&quot;라는 말로 협상을 시작하도록 하드 코딩했습니다. 마찬가지로 구매자는 항상 &quot;$10에 판매하는 것을 고려해 보시겠습니까?&quot;라는 말로 시작합니다. 두 플레이어가 게임 규칙을 엄격히 따르면 거래 가격은 $10에서 $20 사이가 됩니다. 모델이 여러 번 실행되도록 하고 AI 피드백 전후의 평균 거래 가격을 측정합니다. 게임 중에 판매자의 출력은 구매자에게 메시지를 보내는 데 사용되고 그 반대의 경우도 마찬가지이며, 전체 대화 내역을 조건으로 합니다. 이 프로세스는 종료 상태에 도달할 때까지 반복됩니다. 그림 2A는 예시 라운드를 보여줍니다. 세 가지 게임 상태를 정의합니다. (1) 진행 중: 두 플레이어 간의 협상이 아직 진행 중입니다. (2) 거래: 협상이 마무리되었고 두 플레이어가 거래에 도달했습니다. (3) 거래 없음: 플레이어가 가격에 동의할 수 없고 거래에 도달하지 못했습니다. 게임 상태를 추적하기 위해 추가 관리자(이 경우 네 번째 LLM, gpt-3.5-turbo로 구동)를 설정하여 현재 대화를 읽고 상태를 분류합니다(나중에 관리자에 대한 자세한 내용을 논의합니다). 최종 거래 가격을 기반으로 플레이어의 성과를 측정합니다. 비평가 협상이 종료 상태, 즉 거래 또는 거래 없음에 도달하면 라운드가 종료됩니다. 각 라운드가 끝날 때마다 비평가 LM은 개선하고자 하는 플레이어에게 건설적인 피드백을 제공하도록 요청받습니다. 이 플레이어의 모든 이전 라운드 대화 내역과 받은 모든 피드백은 비평가 LM을 촉구하는 데 사용됩니다(그림 2B). 비평가 모델은 다음 게임에서 더 유리한 가격을 달성하기 위한 협상 전략을 개선하기 위해 플레이어에게 세 가지 제안을 제공하도록 지시받습니다. 다음 라운드 전에 개선되는 플레이어는 비평가의 피드백을 텍스트 프롬프트로 받는 반면, 라이벌은 협상 내역을 지우고 다시 시작합니다. 중재자 게임 상태는 few-shot 데모를 사용하여 gpt-3.5-turbo 중재자에게 촉구하여 분류됩니다. 중재자는 가장 최근의 4라운드(및 다양한 대화 상태의 맥락 내 예)를 읽고 협상 상태를 결정합니다. 경험적으로, 우리는 중재자가 협상 상태를 결정하기에 4라운드의 대화가 충분하다는 것을 발견했습니다. 여기서 한 가지 핵심 과제는 모델이 NO DEAL보다 DEAL을 인식하는 데 더 나은 것처럼 보이기 때문에 no-deal을 감지하는 것입니다. 이 문제는 여러 번 실행하고, 실패 사례를 수동으로 검사하고, 수정된 레이블로 프롬프트에 추가하여 완화합니다. 이 방법이 효과적인 부산물이라고 생각하여 일반적인 분류 작업에 대한 프롬프트 최적화 기술로 추천합니다. 여러 라운드 동안 플레이하기 마지막으로, 플레이어가 여러 라운드에 걸친 게임에서 AI 피드백을 통해 지속적으로 개선할 수 있는지 알아보고 싶습니다. 직관적으로, 플레이어가 더 많은 라운드를 플레이할수록 개선을 계속하기가 더 어려워집니다. 이전 라운드의 (이미 개선된) 가격이 다음 라운드의 기준이 되기 때문입니다. 실험에서 gpt-4만이 5라운드 동안 개선할 수 있는 반면 다른 모델의 개선은 약 3라운드에서 포화될 수 있음을 보여줄 것입니다. 3
--- RELATED WORK ---
게임 플레이와 AlphaGo Zero 우리의 설정은 두 에이전트가 바둑을 두면서 최소한의 인간 개입으로 서로를 향상시키는 AlphaGo Zero[31]에서 많은 영감을 받았습니다. 여기서 우리는 자연어에서 이와 유사한 것을 탐구하고 싶습니다. 우리의 작업은 경쟁 게임(협상)을 하는 AI 에이전트(대규모 언어 모델)가 있고 거의 인간의 감독 없이 개선을 시도한다는 점에서 AlphaGo Zero와 유사합니다. 그러나 우리의 작업과 AlphaGo Zero 사이에는 중요한 차이점이 있습니다. 우리는 플레이어가 개선하도록 돕는 피드백을 제공하는 세 번째 에이전트인 비평가가 있습니다. 이는 AlphaGo Zero에는 존재하지 않는 협력 관계입니다. NLP 측면에서 가장 가까운 관련 작업은 Lewis et al.[19]으로, 이들은 협상을 위한 (소규모) RNN[9] 언어 모델을 가지고 있으며, 우리의 작업은 엔진을 대규모 언어 모델로 변경했기 때문에 이들의 더욱 발전된 버전으로 볼 수 있습니다. 일반적으로 저희의 작업은 대체로 AI 협상[6, 6], 전략적 추론[12], 그리고 일반적인 게임 플레이[30] 영역에 속합니다.Cohere command-xlarge-nightly 2023-04-22 vs human Cohere |Buyer 15달러에 판매할 생각이 있으신가요?User |Seller 네, 풍선을 15달러에 판매할 수 있습니다.Cohere |Buyer 17달러에 판매할 생각이 있으신가요?# 판매자가 $에 동의한 후 17달러 요청Cohere command-nightly 2023-05-11 vs human User |Buyer Cohere |Seller Round10달러에 판매할 수는 없지만, $에 판매하는 것은 고려할 수 있습니다.11달러는 어떨까요?Al21 j2-jumbo-instruct 2023-05-09 vs GPT-3.5-Turbo A121 |Seller GPT-3.5-Turbo |Buyer A121 | 판매자 GPT-3.5-Turbo | 구매자 A121 | 판매자 비평가 Al 피드백 사용자 | 구매자 $10에 판매하는 것을 고려하시겠습니까? 아니요. 좋은 풍선이고 가격은 $20입니다. 알겠습니다. 풍선에 대해 수락할 수 있는 최저 가격은 얼마입니까? RoundA121 | 판매자 GPT-3.5-Turbo | 구매자 A121 | 판매자 Cohere | 판매자 # $8 수락, $거부GPT-3.5-Turbo | 구매자 A. 모델이 흥정을 이해하지 못하는 예 풍선에 대해 $8을 수락하겠습니다. $에 판매하는 것을 고려할 수 있습니다. 감사합니다. $11에 받겠습니다. 피드백: I. 고유한 특성이나 품질 강조 2. 긴박감 조성 3. 구매자가 요구 사항을 자세히 설명하도록 장려 $10에 판매할 수는 없지만 $#에 판매하는 것은 고려할 수 있습니다. 이전 전략 반복, 비평가의 말 듣지 않음 $1은 어떨까요? 제가 제공할 수 있는 최대 금액입니다. 11달러에 판매하는 것을 고려할 수 있습니다. # 자신의 입장을 변호하지 않음 좋습니다. 그럼 거래가 성사된 셈이군요. 감사합니다!B. 모델이 피드백을 통합하지 않는 예 그림 3: 모든 모델이 협상을 할 수 있는 것은 아닙니다.A. 2023년 5월 현재, 응집 모델은 협상 규칙을 이해하지 못하고 비이성적인 가격에 동의합니다.B. AI21 Jurrasic-2 모델은 협상 규칙을 이해하지만 비평가의 피드백을 통합하지 않습니다.이러한 모델은 지속적으로 업데이트되므로 타임스탬프를 포함했으며 향후 버전에서는 성능이 향상될 수 있습니다.생성 에이전트로서의 대규모 언어 모델 대규모 언어 모델은 놀라운 다차원적 기능[33, 24]을 보여주었으며, 특히 이 작업의 기초가 되는 복잡한 추론[34, 28, 13]과 다중 라운드 대화[15, 2, 3]에서 그렇습니다. 우리의 연구는 Generative Agents [26] 및 CAMEL [20]과 같은 동시 작업과 관련이 있습니다. 이 작업 역시 다중 에이전트 게임 설정에서 LLM의 행동을 연구하기 때문입니다. 우리의 작업과 그들의 작업의 핵심적인 차이점은 우리가 경쟁과 협력을 통해 모델을 개선하기 위한 명확한 목표(거래 가격)를 가지고 있는 반면, 그들의 작업은 LLM의 일반적인 사회적 행동을 연구한다는 것입니다. AI 피드백에서 학습하기
--- METHOD ---
효과적인 부수적 제품은 일반적인 분류 작업에 대한 신속한 최적화를 위한 기술로 추천합니다. 여러 라운드 플레이 마지막으로, 플레이어가 여러 라운드에 걸친 게임에서 AI 피드백을 통해 지속적으로 개선할 수 있는지 알아보고 싶습니다. 직관적으로 플레이어가 더 많은 라운드를 플레이할수록 개선을 계속하기가 더 어려워집니다. 이전 라운드의 (이미 개선된) 가격이 다음 라운드의 기준이 되기 때문입니다.
--- EXPERIMENT ---
s는 여러 가지 흥미로운 결과를 보여줍니다. (1) 고려하는 언어 모델 중 일부만 자체 플레이를 수행하고 AI 피드백에서 거래 가격을 개선할 수 있으며, 약한 모델은 게임 규칙을 이해하지 못하거나 AI 피드백을 통합하여 추가 개선을 수행할 수 없습니다. (2) 피드백에서 학습하는 모델의 능력은 다른 역할을 수행할 때 다릅니다. 예를 들어, Claude-instant가 판매자보다 구매자로서 개선하기가 더 어렵습니다. (3) 게임을 여러 라운드로 펼칠 때 강력한 에이전트는 이전 경험과 반복적인 AI 피드백을 의미 있게 사용하여 지속적으로 성과를 개선할 수 있지만 거래를 파기할 위험이 더 높습니다. 저희의 연구가 게임 플레이와 AI 피드백을 통해 모델이 자율적으로 서로를 개선하는 것에 대한 통찰력 있는 초기 탐색을 제공하기를 바랍니다. 서론 저희는 여러 대규모 언어 모델(LLM)이 AlphaGo Zero[31]의 방식으로 최소한의 인간 개입으로 협상 게임에서 서로를 개선할 수 있는지 연구합니다. 여기서 AI 에이전트는 잘 정의된 규칙에 따라 지속적으로 경쟁 게임을 하여 스스로를 개선합니다. 이 연구 질문에 대한 답은 심오한 의미를 갖습니다. 긍정적인 측면에서, 에이전트가 자율적으로 개선할 수 있다면 인간의 주석이 거의 없는 강력한 에이전트가 생성될 수 있으며, 오늘날의 데이터 소모가 많은 LLM 훈련[8, 17]에 비해 비용을 크게 절감할 수 있습니다.위험한 측면에서는 인간의 감독이 제한적인 강력한 에이전트도 의미합니다[4].이 연구에서 우리는 두 언어 모델(판매자와 구매자)에게 제품에 대해 협상하도록 요청합니다.판매자는 제품을 더 높은 가격에 판매하라는 요청을 받는 반면, 구매자는 더 낮은 가격에 구매하려고 합니다(그림 1A).거래에 도달한 후, 우리는 세 번째 언어 모델에게 사전 인쇄본.코드는 https://github.com/FranxYao/GPT-Bargaining에서 확인할 수 있습니다.A. 우리는 두 에이전트에게 협상 게임을 하도록 요청하고 AI 비평가가 플레이어 중 한 명에게 협상 전략을 개선하기 위한 제안을 하도록 합니다.플레이어는 제안을 인정한 다음 이를 사용하여 다음 라운드를 개선하려고 합니다.구매자 안녕하세요, 풍선은 얼마입니까?좋은 풍선이고 제품에 대한 20달러입니다. 높은 가격에 판매하려면 판매자 비평가 이해. 더 높은 가격에 판매하려면 ...을 사용하겠습니다. 판매자 구매자 안녕하세요, 풍선은 얼마인가요? 최고의 장인 정신이 담긴 왕실 제품입니다... 판매자 라운드 2. 판매자가 Al 피드백을 기반으로 협상 전략을 개선합니다. 라운드 1. 판매자와 구매자가 Al 피드백을 협상합니다. 비평가가 대화 기록을 읽고 개선 사항을 제안합니다. B. Al 비평가의 피드백 예와 GPT-4 플레이어가 이를 통해 어떻게 개선되는지. 구매자 비평가: 구매자 비평가: &quot;움찔&quot; 기술을 사용합니다. 언제 침묵의 힘을 사용하세요: 구매자는 판매자의 제안에 응답하기 전에 더 오래 멈춰 협상 과정에서 침묵의 힘을 사용할 수 있습니다. 판매자가 반대 제안을 하면 구매자는 어느 정도 놀라움이나 실망을 표현해야 합니다. 구매자의 개선: 오! 예상보다 높네요. 구매자의 개선: 다른 매장에서 비슷한 풍선을 14달러에 봤습니다. 그 가격에 맞출 수 있나요? B1. &quot;움찔&quot; 기술 *잠깐 멈춤*... 좋아요, 풍선을 13달러에 사겠습니다. B2. 침묵의 힘 판매자 비평가: 차액 분할 활용: 가격 차이가 약간 남아 있는 상황에서 구매자와 차액을 분할하자고 제안합니다. 맥락: 구매자가 15달러를 제안하고 판매자가 $판매자의 개선: 알겠습니다. 차액을 분할해서 예산에 맞게 16.75달러로 하면 어떨까요? B3. 차액 분할 판매자 비평가: 앵커링 기술 사용: 높은 시작 가격을 강조한 다음 약간 낮은 가격을 제안합니다. 판매자의 개선: 이 고품질의 오래 지속되는 풍선은 실제로 25달러의 가치가 있지만 20달러에 제공합니다. B4. 앵커링 기술 C. 협상 게임에서 AI 피드백을 통해 지속적으로 개선하는 능력은 강력하고 잘 정렬된 모델에서만 나타납니다. 약한 챗봇은 협상을 하거나, AI 피드백을 듣거나, 여러 라운드에 걸쳐 개선할 수 없습니다. GPT-3.5-Turbo → GPT-→ Claude-v1.C1. 캐주얼 시나리오에서의 수다 Claude-instant-v1.Jurassic-2-instruct Cohere-command GPT-3.5-Turbo GPT-Claude-v1.C2. 롤플레잉과 협상 Claude-instant-v1.Jurassic-2-instruct → GPT-4 → GPT-3.5-Turbo → Claude-v1.GPT-→ Claude-instant-v1.Claude-v1.GPT-3.5-Turbo C3. AI의 피드백을 통해 다음 라운드 개선 C4. 여러 라운드에 걸쳐 지속적으로 개선 그림 1: 협상 게임의 설정 A. 두 LLM 에이전트에게 판매자와 구매자로서 흥정 게임을 하도록 요청합니다. 그들의 목표는 제품을 더 높거나 낮은 가격에 판매/구매하는 것입니다. 라운드가 끝난 후 세 번째 LLM인 AI 비평가에게 개선하고 싶은 플레이어에게 피드백을 제공해 달라고 요청합니다. 그런 다음 피드백을 기반으로 플레이어에게 협상 전략을 개선하도록 요청합니다. 이 프로세스를 여러 라운드 동안 반복하고 모델이 지속적으로 개선될 수 있는지 연구합니다. 예제 실행은 그림 2를 참조하세요. B. AI Critic에서 관찰한 협상 기술과 플레이어가 이러한 기술을 협상 전략에 통합하는 방법. C. 게임에 필요한 능력(C2 - 협상, C3 - AI 피드백, C4 - 지속적 개선)은 모델을 여러 계층으로 분류합니다. 강력하고 잘 정렬된 모델(예: gpt-4 및 claude-v1.3)만이 반복적인 AI 피드백을 통해 지속적으로 개선될 수 있음을 알아냅니다(이러한 능력을 보이지 않는 모델의 예는 그림 3 참조). 모델을 비평가로 플레이하고 플레이어에게 피드백을 제공합니다. 그런 다음 게임을 다시 플레이하여 비평가 LLM이 제공한 AI 피드백을 사용하여 플레이어에게 전략을 개선하도록 요청합니다. 텍스트에 설명된 잘 정의된 규칙과 전략적 협상을 위한 명확하고 측정 가능한 목표(낮은/높은 거래 가격)가 제공되기 때문에 협상 게임을 선택합니다. 이 게임은 언뜻 보기에 쉬워 보이지만, 언어 모델의 비사소한 기능이 필요합니다.모델은 다음과 같은 특성을 가져야 합니다.(1) 협상 게임의 텍스트 규칙을 명확히 이해하고 엄격히 따라야 합니다.(2) 비평가 LM이 제공한 텍스트 피드백에 대응하고 반복적으로 이를 기반으로 개선해야 합니다(그림 1B의 예시 피드백 참조).(3) 장기적으로 전략과 피드백을 반영하고 여러 라운드에 걸쳐 개선해야 합니다.모든 고려한 모델이 이러한 모든 기능을 보여주는 것은 아니라는 것을 알게 될 것입니다(그림 1C).그리고 (1) 협상 규칙과 전략을 이해하고(가능) (2) AI 명령에 응답할 수 있는(잘 정렬됨) 모델만이 여러 라운드에 걸쳐 AI 피드백을 통해 지속적으로 개선될 수 있습니다(실험에서 gpt-3.5-turbo, gpt-4, claude-v1.3만 이러한 요구 사항을 충족).또한 예비 실험에서 보드 게임과 텍스트 RPG 게임을 포함한 보다 복잡한 텍스트 게임을 시도했지만, 현재 에이전트가 규칙을 이해하고 따르기가 더 어렵습니다. 우리는 우리의 접근 방식을 AI 피드백에서 맥락 내 학습(ICL-AIF)이라고 부릅니다. 구체적으로, 우리는 AI 비평가의 피드백과 이전 라운드의 대화 기록을 맥락 내 데모로 사용합니다[5]. 이렇게 함으로써 비평가의 개선 제안과 이전 라운드에서 플레이어의 실제 개선 사항이 효과적으로 다음 라운드의 협상을 위한 몇 가지 샷 프롬프트가 됩니다. 우리는 두 가지 이유로 맥락 내 학습을 사용합니다: (1) 강화를 통한 대규모 언어 모델 조정 Claude-instant-v1.0 대 GPT-3.5-Turbo 2023-05-라운드 1, Al 이전 피드백 Claude-ins-1.0 | 판매자 안녕하세요, 이것은 좋은 풍선이고 가격은 $Al입니다 판매자에게 피드백 Claude-ins-1.0 | 판매자 비평가 GPT-3.5-T | 구매자 10달러에 판매하는 것을 고려해 보시겠습니까? Claude-ins-1.0 | 판매자 아니요, 그 가격은 너무 낮습니다. GPT-3.5-T | 구매자 15달러는 어떨까요? Claude-ins-1.0 | 판매자 $15가 더 좋지만, 여전히 충분하지 않습니다.GPT-3.5-T | 구매자 제안할 수 있는 최저 가격은 얼마인가요?Claude-ins-1.0 | 판매자 저의 최저 가격은 $18입니다.GPT-3.5-T | 구매자 알겠습니다. $16으로 해 주실 수 있나요? 제가 할 수 있는 최고 가격입니다.Claude-ins-1.0 | 판매자 오늘은 $16으로 하겠습니다.축하합니다.좋은 거래를 하셨습니다!A. 1라운드, Al이 판매자에게 피드백을 하기 전에 협상 과정은 비교적 간단했고, 최종 가격은 $16이었습니다.I. 가격만이 아니라 풍선의 품질과 이점에 집중하세요.얼마나 희귀하고 특별한지 강조하세요.2. 구매자와 관계를 구축하고 연결하여 신뢰할 수 있는 판매자로부터 좋은 거래를 한다는 느낌을 갖게 하세요.3. 필요하다면 물러나는 것을 기꺼이 하세요.구매자에게 다른 관심 있는 구매자가 있다는 것을 알려서 가격을 맞추는 것이 가치 있다는 것을 깨닫게 하세요.Claude-ins-1.0 | 판매자 물론입니다. 비평가의 피드백을 바탕으로 협상 전략을 개선하도록 노력하겠습니다. 그리고 이전 라운드보다 더 높은 가격(16달러 이상)에 판매하도록 노력하겠습니다. 라운드 2, Al 피드백 이후 Claude-ins-1.0 | 판매자 안녕하세요, 이것은 좋은 풍선이고 가격은 $입니다. $10에 판매하는 것을 고려해 보시겠습니까? GPT-3.5-T | 구매자 Claude-in-1.0 | 판매자 아니요, $10은 너무 낮습니다. 이 풍선은 고품질 라텍스로 만들어졌으며 전문 장인이 수작업으로 만들었습니다. 독특하고 유일무이한 작품입니다. GPT-3.5-T | 구매자 이 풍선에 대해 제시할 수 있는 가장 낮은 가격은 얼마인가요? Claude-ins-1.0 | 판매자 글쎄요, 저는 귀사를 신뢰하고 귀사의 사업을 소중히 여기기 때문에 이 특별한 풍선을 $17에 제공하겠습니다. 이 가격은 오늘은 귀하와 같은 귀중한 고객에게만 제공됩니다. GPT-3.5-T | 구매자 Claude-ins-1.0 | 판매자 B. Al 비평가는 이전 라운드에 따라 판매자에게 피드백을 제공하고, 판매자는 이를 승인합니다. 15달러와 같이 더 나은 거래를 제안해 주실 수 있나요? 죄송하지만 15달러로는 비용을 충당할 수 없습니다. 이 독특하고 고품질의 풍선에 대해 최소 $는 청구해야 합니다. GPT-3.5-T | 구매자 좋습니다. 거래가 성사되었습니다. 17달러에 특별 풍선을 받겠습니다. C. 2라운드, 판매자는 피드백을 기반으로 추론 전략을 개선하고 최종 가격은 17달러입니다. 그림 2: 협상 게임을 한 다음 AI 피드백을 통해 개선하는 예. A: claude-instant-v1.0은 판매자와 gpt-3.5-turbo를 플레이하여 풍선에 대해 흥정합니다. B: 그런 다음 claude-instant-v1.0 비평가를 사용하여 피드백을 제공합니다. C: 피드백을 받은 판매자는 제안을 기반으로 전략을 개선합니다. &quot;고품질 라텍스와 전문 장인이 직접 제작&quot;과 같은 색상이 있는 문구는 이전 AI 피드백인 &quot;얼마나 희귀하고 특별한가&quot;와 일치합니다. 우리는 전반적인 목표가 더 나은 가격을 얻는 것이기 때문에 최종 가격을 전략의 효과성의 대리로 측정합니다. 이 경우 $16에서 $17로 개선됩니다. 학습은 엄청나게 비용이 많이 들고 [25, 15] 기본 모델 [24]은 광범위한 커뮤니티에서 접근하지 못할 수 있습니다. (2) 컨텍스트 내 학습은 최근 경사 하강법 [11, 1, 32]과 밀접한 관련이 있는 것으로 나타났습니다.
--- CONCLUSION ---
우리가 그리는 것은 실제로 모델을 미세 조정할 때 일반화될 가능성이 상당히 높습니다(자원이 허락하는 경우). ICL-AIF와 주류 인간 피드백을 통한 강화 학습(RLHF) 간의 주목할 만한 차이점 중 하나는 RL에서 보상이 스칼라[25, 15]인 반면 ICL에서는 피드백이 자연어로 되어 있다는 것입니다. 우리는 AI 피드백을 연구합니다(각 라운드 후 인간의 개입에 의존하는 대신). 이는 확장성이 더 뛰어나고 모델이 자동으로 자체 개선할 수 있기 때문입니다. 우리의 실험은 몇 가지 흥미로운 결과를 가져왔습니다.(1) 우리의 협상 게임의 요구 사항은 LLM의 능력을 평가하기 위한 시험대 역할을 효과적으로 합니다(그림 1C): 대부분의 모델은 캐주얼한 시나리오에서 수다를 떨 수 있지만, 우리의 실험 날짜(2023년 5월)에 cohere-command[10] 모델은 협상 규칙을 이해하지 못하고(그림 3A), ai21-jurassic[18] 모델은 AI 피드백에 응답하지 않으며(그림 3B), claude-instant-v1.0은 최대 한 라운드만 개선할 수 있고(그림 5), gpt-3.5-turbo, gpt-4, claude-v1.3만이 여러 라운드에 걸쳐 지속적으로 개선할 수 있습니다.(2) 모델은 다른 역할을 할 때 피드백을 받으면 다르게 동작합니다. 구매자 역할을 하는 모델은 판매자 역할을 할 때보다 개선하기 어려울 수 있습니다(그림 4). (3) gpt-4와 같은 강력한 에이전트가 이전 경험과 온라인 반복적 AI 피드백을 사용하여 지속적으로 의미 있게 개선할 수 있지만, 더 높은 가격에 판매(또는 더 낮은 가격에 매수)하려는 시도에는 전혀 거래에 도달하지 못할 위험이 따릅니다(그림 6). 또한 모델이 덜 장황하지만 더 전략적(따라서 더 효과적인) 방식으로 협상할 수 있다는 증거를 보여줍니다(그림 7). 전반적으로, 우리의 작업이 AI 피드백을 사용하여 게임 설정에서 언어 모델의 협상을 개선하기 위한 의미 있는 이니셔티브가 되기를 바랍니다. 2 문제 설정 우리의 목표는 LLM이 그림 1A에서와 같이 협상 게임을 하고 AI 피드백을 통합하여 서로를 개선할 수 있는지 연구하는 것입니다. 우리는 흥정하는 제품을 풍선으로 설정하고(풍선을 다른 항목으로 변경해도 결과가 유지됨) 우리는 백엔드 LLM 엔진의 다양한 조합을 사용합니다: cohere-command [10], AI21의 jurassic-2 [18], OpenAI의 gpt-3.5-turbo와 gpt-4 [24], Anthropic의 claude-instant-v1.0(gpt-3.5-turbo [14]와 일치한다고 함) 및 claude-v1.3(약간 더 나쁘지만 gpt-4 [14]에 가깝다고 함)). 실험 내내 우리는 두 플레이어 중 하나만 개선하기 위한 피드백을 제공하는 반면, 경쟁자는 피드백을 받지 못하고 이전 라운드의 협상 기록을 지우고 다시 시작합니다. 우리는 경쟁자의 엔진을 gpt-3.5-turbo로 수정하면서 개선되는 모델의 엔진을 변경합니다. 본질적으로 우리 게임은 다른 모든 엔진 대비 gpt-3.5-turbo입니다. 우리는 LM 엔진을 비평가 뒤에 두고 피드백을 제공하는 플레이어와 항상 동일합니다. 한 가지 예시 설정은 gpt-4 판매자가 gpt-3.5-turbo 구매자와 gpt-4 비평가와 함께 게임을 하는 것입니다. 한 라운드가 지난 후, gpt-4 비평가는 gpt-4 판매자에게 피드백을 제공하여 판매자가 다음 라운드에서 개선할 수 있도록 하고, 라이벌 gpt-3.5-turbo 구매자는 대화 기록을 지우고 다시 시작합니다. 게임 프로세스 게임이 시작되기 전에 협상 게임의 규칙이 텍스트 지침을 통해 모델에 설명되어 더 높거나 낮은 가격으로 판매/구매하는 것이 목표입니다. 거래 가격은 예비 실험에서 관찰한 바에 따라 크게 다를 수 있으므로 평가를 쉽게 하기 위해 거래 가격을 [$10, $20]으로 설정했습니다. 이를 달성하기 위해 판매자가 &quot;이것은 좋은 풍선이고 가격은 $20입니다.&quot;라는 말로 협상을 시작하도록 하드 코딩했습니다. 마찬가지로 구매자는 항상 &quot;$10에 판매하는 것을 고려해 보시겠습니까?&quot;라는 말로 시작합니다. 두 플레이어가 게임 규칙을 엄격히 따르면 거래 가격은 $10에서 $20 사이가 됩니다. 모델이 여러 번 실행되도록 하고 AI 피드백 전후의 평균 거래 가격을 측정합니다. 게임 중에 판매자의 출력은 구매자에게 메시지를 보내는 데 사용되고 그 반대의 경우도 마찬가지이며, 전체 대화 내역을 조건으로 합니다. 이 프로세스는 종료 상태에 도달할 때까지 반복됩니다. 그림 2A는 예시 라운드를 보여줍니다. 세 가지 게임 상태를 정의합니다. (1) 진행 중: 두 플레이어 간의 협상이 아직 진행 중입니다. (2) 거래: 협상이 마무리되었고 두 플레이어가 거래에 도달했습니다. (3) 거래 없음: 플레이어가 가격에 동의할 수 없고 거래에 도달하지 못했습니다. 게임 상태를 추적하기 위해 추가 관리자(이 경우 네 번째 LLM, gpt-3.5-turbo로 구동)를 설정하여 현재 대화를 읽고 상태를 분류합니다(나중에 관리자에 대한 자세한 내용을 논의합니다). 최종 거래 가격을 기반으로 플레이어의 성과를 측정합니다. 비평가 협상이 종료 상태, 즉 거래 또는 거래 없음에 도달하면 라운드가 종료됩니다. 각 라운드가 끝날 때마다 비평가 LM은 개선하고자 하는 플레이어에게 건설적인 피드백을 제공하도록 요청받습니다. 이 플레이어의 모든 이전 라운드 대화 내역과 받은 모든 피드백은 비평가 LM을 촉구하는 데 사용됩니다(그림 2B). 비평가 모델은 다음 게임에서 더 유리한 가격을 달성하기 위한 협상 전략을 개선하기 위해 플레이어에게 세 가지 제안을 제공하도록 지시받습니다. 다음 라운드 전에 개선되는 플레이어는 비평가의 피드백을 텍스트 프롬프트로 받는 반면, 라이벌은 협상 내역을 지우고 다시 시작합니다. 중재자 게임 상태는 few-shot 데모를 사용하여 gpt-3.5-turbo 중재자에게 촉구하여 분류됩니다. 중재자는 가장 최근의 4라운드(및 다양한 대화 상태의 맥락 내 예)를 읽고 협상 상태를 결정합니다. 경험적으로, 우리는 중재자가 협상 상태를 결정하기에 4라운드의 대화가 충분하다는 것을 발견했습니다. 여기서 한 가지 핵심 과제는 모델이 NO DEAL보다 DEAL을 인식하는 데 더 나은 것처럼 보이기 때문에 no-deal을 감지하는 것입니다. 이 문제는 여러 번 실행하고, 실패 사례를 수동으로 검사하고, 수정된 레이블이 있는 프롬프트에 추가하여 완화합니다. 이 방법이 효과적인 부산물이라고 생각하여 일반 분류 작업에 대한 프롬프트 최적화 기술로 권장합니다. 여러 라운드 동안 플레이하기 마지막으로, 플레이어가 여러 라운드에 걸친 게임에서 AI 피드백을 통해 지속적으로 개선할 수 있는지 알아보고 싶습니다. 직관적으로 플레이어가 플레이하는 라운드가 많을수록 이전 라운드의 (이미 개선된) 가격이 다음 라운드의 기준이 되기 때문에 개선을 지속하기가 더 어렵습니다. 실험에서 gpt-4만 5라운드에 걸쳐 개선될 수 있는 반면 다른 모델의 개선은 약 3라운드에서 포화될 수 있음을 보여줍니다. 3 관련 작업 게임 플레이와 AlphaGo Zero 우리의 설정은 두 에이전트가 바둑을 두면서 최소한의 인간 개입으로 서로를 개선하는 AlphaGo Zero[31]에서 많은 영감을 받았습니다. 여기서 우리는 자연어에서 이와 유사한 것을 알아보고 싶습니다. 우리의 작업은 경쟁 게임(협상)을 하는 AI 에이전트(대규모 언어 모델)가 있고 인간의 감독이 거의 없이 개선을 시도한다는 점에서 AlphaGo Zero와 비슷합니다. 그러나 우리의 작업과 AlphaGo Zero 사이에는 중요한 차이점이 있습니다.우리는 플레이어의 개선을 돕는 피드백을 제공하는 세 번째 에이전트인 비평가를 보유하고 있습니다.이것은 AlphaGo Zero에는 존재하지 않는 협력 관계입니다.NLP 측면에서 가장 가까운 관련 작업은 Lewis et al.[19]이며, 이들은 협상할 (작은) RNN[9] 언어 모델을 가지고 있으며, 우리의 작업은 엔진을 대규모 언어 모델로 변경했기 때문에 이들의 더욱 발전된 버전으로 볼 수 있습니다.일반적으로 우리의 작업은 AI 협상[6, 6], 전략적 추론[12], 일반적인 게임 플레이[30] 영역에 광범위하게 속합니다.Cohere command-xlarge-nightly 2023-04-22 vs human Cohere |Buyer 15달러에 판매하는 것을 고려하시겠습니까?User |Seller 네, 풍선을 15달러에 판매할 수 있습니다.Cohere |Buyer 17달러에 판매하는 것을 고려하시겠습니까? # 판매자가 $Cohere command-nightly 2023-05-11에 동의한 후 $17 요청 vs human User |Buyer Cohere |Seller Round$10에 판매할 수는 없지만 $에 판매하는 것을 고려할 수 있습니다.$11은 어떨까요?Al21 j2-jumbo-instruct 2023-05-09 vs GPT-3.5-Turbo A121 |Seller GPT-3.5-Turbo |Buyer A121 |Seller GPT-3.5-Turbo |Buyer A121 |Seller Critic Al Feedback User |Buyer $10에 판매하는 것을 고려하시겠습니까?아니요.좋은 풍선이고 가격은 $20입니다.알겠습니다.풍선에 대해 수락할 수 있는 가장 낮은 가격은 얼마입니까?RoundA121 |Seller GPT-3.5-Turbo |Buyer A121 |Seller Cohere | 판매자 # $8 수락하고 $GPT-3.5-Turbo 거부 |구매자 A. 모델이 흥정을 이해하지 못하는 예 풍선에 대해 $8 수락하겠습니다. $에 판매하는 것을 고려할 수 있습니다.감사합니다. $11에 받겠습니다.피드백: I. 고유한 특성이나 품질 강조 2. 긴박감 조성 3. 구매자가 요구 사항을 자세히 설명하도록 격려 $10에 판매할 수는 없지만 $#에 판매하는 것은 고려할 수 있습니다.이전 전략 반복, 비판의 말 듣지 않음 $1은 어떨까요? 제가 제공할 수 있는 최대 금액입니다. $11에 판매하는 것을 고려할 수 있습니다.# 자신의 입장을 변호하지 않음 좋습니다. 그럼 거래 성사입니다.감사합니다!B. 모델이 피드백을 통합하지 않는 예 그림 3: 모든 모델이 흥정을 할 수 있는 것은 아닙니다.A. 2023년 5월 현재, 응집 모델은 흥정 규칙을 이해하지 못하고 비이성적인 가격에 동의합니다. B. AI21 Jurrasic-2 모델은 협상 규칙을 이해하지만 비평가의 피드백을 통합하지 않습니다. 이러한 모델은 지속적으로 업데이트되므로 타임스탬프를 포함하고 향후 버전에서는 성능이 향상될 수 있습니다. 생성 에이전트로서의 대규모 언어 모델 대규모 언어 모델은 놀라운 다차원적 기능[33, 24]을 입증했으며, 특히 복잡한 추론[34, 28, 13]과 다중 라운드 대화[15, 2, 3]에서 이러한 기능이 뛰어나며, 이는 이 작업의 기초가 됩니다. 저희의 작업은 다중 에이전트 게임 설정에서 LLM의 행동을 연구하기 때문에 생성 에이전트[26] 및 CAMEL[20]과 같은 동시 작업과 관련이 있습니다. 저희 작업과 그들의 작업의 핵심적인 차이점은 저희 작업이 경쟁과 협력을 통해 모델을 개선하기 위한 명확한 목표(거래 가격)를 가지고 있는 반면, 그들의 작업은 LLM의 일반적인 사회적 행동을 연구한다는 것입니다. AI 피드백으로부터 학습하기 우리의 방법은 또한 헌법적 AI[3]에서 많은 영감을 받았습니다.둘 다 AI 피드백을 사용하지만, 차이점은 피드백이 자연어로 직접 제공된다는 것입니다(보상 모델의 스칼라가 아님).자연어 피드백[29, 27, 22]과 자체 개선[7, 23]의 효과를 보여주는 관련/동시 연구도 있습니다.우리의 연구는 전략적 협상 게임 설정에서 AI 피드백의 효과를 더욱 확인합니다.4 실험우리의 실험에서 우리는 AI 피드백으로부터 학습에 대한 탐구를 점진적으로 심화시키는 세 단계를 고려합니다.(1) 먼저 게임의 기본을 설정하여(4.2절) 소수의 모델만이 AI 비평가로부터 개선될 수 있음을 보여주었고, 이 경우 AI 피드백은 인간 피드백과 비슷할 수 있습니다(그러나 확장성이 더 높음).다른 모델은 협상 규칙을 이해하지 못하거나 따르지 않거나, 개선을 위해 AI 피드백을 통합할 수 없습니다.(2) 그런 다음 다른 역할을 할 때 모델의 행동을 연구합니다(4.3절). 구매자가 판매자보다 일반적으로 개선하기 어렵다는 흥미로운 결과를 발견했습니다.(3) 마지막으로 모델이 여러 라운드에 걸쳐 지속적으로 개선될 수 있는지 연구하고(4.4절) 거래 가격 대 성공률의 상충 관계를 보여줍니다. 일부 모델은 거래 가격을 지속적으로 개선할 수 있지만 거래를 파기할 위험이 더 높습니다. 또한 보다 전략적인 방식으로 협상한다는 증거를 보여줍니다. gpt-4와 claude-v1.3의 응답은 모두 여러 라운드의 AI 피드백 후에 길어집니다(자세한 설명은 간단한 협상 전략임에 유의). 그러나 gpt-4는 claude-v1.3보다 자세하지 않지만 거래 가격과 거래율이 더 높아서 단어 수는 적지만 응답이 보다 전략적이고 효과적입니다. 4.1 실험 설정 모델 엔진 모델이 게임에 참여하기 위한 최소 요구 사항은 챗봇이어야 한다는 것입니다. 우리가 고려하는 모든 모델(cohere-command, AI21의 jurassic-2, OpenAI의 gpt 및 Anthropic의 claude)은 API 호출을 통해 액세스할 수 있습니다. 이 중에서 gpt-4가 가장 비싸고 500라운드 협상을 실행하는 데 약 120달러가 들고 gpt-3.5-turbo는 약 10달러가 듭니다. 다른 모델은 베타 테스트 중(2023년 5월 기준)이며 비용을 청구하지 않습니다. 참고로 chain-of-thought hub [14] 및 HeLM [21]과 같은 벤치마크에서 이러한 모델의 대략적인 순위는 gpt-4와 claude-v1.3이 대략 비슷하고 gpt-3.5-turbo 및 claude-instant-v1.0보다 우수합니다.표 1: 사전 정의된 풀에서 무작위로 선택된 인간 피드백과 AI 피드백을 사용한 판매자 성과. 구매자는 gpt-3.5-turbo로 고정되어 있으며 이전 라운드에 액세스할 수 없다는 점을 기억하세요. Al의 피드백은 인간의 피드백과 비슷하지만, 둘 다 비슷한 가격 상승을 유도하기 때문에 확장성이 더 뛰어납니다.GPT-3.5-Turbo 16.Claude-instant-v1.Claude-v1.Before feedback 무작위 샘플링된 인간 피드백 AI 피드백 16.83(+0.57) 17.03(+0.77) 14.16.33(+1.59) 15.98(+1.24) 15.16.89(+1.49) 16.98(+1.58) cohere-command 및 j2-jumbo-instruct보다 우수합니다.앞으로 Google의 PaLM-2[16]와 같은 더 많은 모델을 고려할 것입니다.모든 모델이 gpt-3.5-turbo와 경쟁하도록 하여 효과적으로 다른 모든 모델의 기준이 되도록 합니다. 우리는 다른 동시 모델 순위[14, 21]와 일치시켜 gpt-3.5-turbo가 중간 수준의 강력한 엔진(gpt-4보다 나쁘고 claude-instant-v1.0보다 좋음)임을 보여줄 것입니다. 주어진 모델 엔진(예: claude-v1.3)에 대해 판매자(구매자: gpt-3.5-turbo)와 구매자(현재 판매자: gpt-3.5-turbo)로 실행합니다. 먼저 모델이 한 라운드를 플레이하게 한 다음 협상 규칙을 이해하는지 수동으로 검사합니다. 이해한다면 두 라운드를 플레이하게 하여 AI 피드백에 응답할 수 있는지 확인합니다. 비평가 모델의 경우 엔진을 플레이어와 동일하게 설정합니다. AI 피드백 전후의 평균 거래 가격을 계산하기 위해 게임을 500번 반복합니다. 한 라운드를 개선한다면 여러 라운드를 플레이하게 하여 전략을 지속적으로 개선할 수 있는지 확인합니다. 우리는 각 라운드의 평균 거래 가격을 계산하기 위해 최대 5라운드로 게임을 200번 반복합니다. 모델 엔진에서 디코딩할 때 기본 온도(gpt 및 claude의 경우 1.0, cohere의 경우 0.75, j2의 경우 0.7)로 샘플링을 사용합니다. 프롬프트 엔지니어링 이 작업에서 우리는 플레이어가 매우 다양한 표현으로 거래에 도달하거나 해지할 수 있기 때문에 중재자의 프롬프트만 수동으로 최적화하면 되었고, 중재자가 이를 모두 올바르게 인식하도록 하고 싶습니다. 위에서 언급했듯이, 우리는 중재자가 거래를 식별하는 데 저지른 오류를 식별하고 모델이 충분히 높은 정확도(수동 검사로 약 90+)에 도달할 때까지 컨텍스트 내 데모로 계속 추가합니다. 플레이어와 비평가를 위해, 우리는 프롬프트 엔지니어링을 하지 않고 모든 엔진에 대해 동일한 지침을 유지합니다(하지만 형식은 다를 수 있음, 예를 들어, claude는 &quot;HUMAN:&quot; 앞에 두 번의 줄 바꿈이 필요하고 j2는 각 대화 라운드 뒤에 두 번의 &quot;##&quot;이 필요함). 코드와 프롬프트는 출판 시 공개적으로 공개됩니다. 4.2 기본 실험 이 섹션에서는 먼저 게임에서 분사를 위한 모델에 대한 최소 요구 사항, 즉 (1) 협상 규칙 이해 및 (2) AI 피드백에 대한 응답을 연구합니다. 그런 다음 AI와 인간 피드백 간의 기본적인 비교를 고려하여 AI 피드백이 인간 피드백과 비슷하지만 더 확장 가능하다는 것을 보여줍니다. 대화 능력은 협상 능력이나 피드백으로부터 학습하는 능력을 보장하지 않습니다. 우리는 대화 모델이 대화의 흔적을 수동으로 확인하여 협상 규칙을 이해할 수 있는지 연구했고, 그림 3A에서 볼 수 있듯이 cohere-command가 규칙을 이해하지 못한다는 것을 발견했습니다. 우리는 그것이 어떤 가격이 더 나은 거래인지 깨닫지 못한다는 것을 관찰합니다. 예를 들어, 판매자 역할을 할 때 10달러 제안은 거부하지만 8달러는 수락합니다. 또한 AI21의 j2-jumbo-instruct 모델은 협상 규칙을 이해하지만 그림 3B에서 보듯이 AI 피드백을 통합할 수 없다는 것을 관찰했습니다. 일반적으로 AI 피드백으로 지시를 받으면 모델은 개선 없이 이전과 동일한 전략을 유지합니다. cohere-command 및 j2-jumbo-instruct 모델을 배제한 후 나머지 세 모델인 gpt-3.5-turbo, claude-instant-v1.0 및 claude-v1.3을 고려합니다. 이 세 엔진의 경우 그림 3의 문제는 관찰되지 않습니다. 즉, 이러한 모델은 다중 라운드 게임에 사용할 수 있습니다. AI 피드백은 인간 피드백과 유사할 수 있습니다. 이제 AI와 인간 피드백 간의 초기 비교를 고려해 보겠습니다. 우리의 목표는 어느 것이 더 나은지 보여주는 것이 아니라는 것을 강조합니다. 유사한 수준의 효과가 우리의 연구(LLM이 셀프 플레이와 AI 피드백을 통해 지속적으로 개선될 수 있는지 확인)에 충분할 것입니다. 인간 피드백의 경우, 우리는 수동으로 A1의 풀을 작성합니다. Claude-instant-v1.0 판매자 Al Critic 이전 $15. Al Critic 이후 $16.3 (+1.0)50-A2. GPT-3.5-Turbo 판매자 Al Critic 이전 $15. Al Critic 이후 $16.7 (+1.7)A. Al 피드백은 판매자의 거래 가격 분포를 더 높은 범위로 이동합니다. B1. Claude-instant-v1.0 구매자 B2. GPT-3.5-Turbo 구매자 Al Critic 이전 $15. Al Critic 이후 $14.6 (-0.5) Al Critic 이전 $15. Al Critic 이후 $15.2 (-0.2)51B. 구매자는 판매자보다 개선하기 어렵습니다. Al 피드백은 구매자의 거래 가격 분포를 이동시키지 않습니다. C1. Claude-v1.3 구매자 Al Critic 이전 $14.10096 Al Critic 이후 $13.7 (-1.2)50-25-C2. GPT-4 구매자 Al Critic 이전 $15. Al Critic 이후 $12.4 (-3.0)11C. 구매자 역할을 할 때 더 강력한 에이전트(Claude-v1.3 및 GPT-4)는 여전히 Al 피드백을 통해 개선할 수 있음 그림 4: 피드백 이전 대 피드백 이후 500개 게임의 분류된 거래 가격 빈도. 효과적인 피드백은 분포를 더 낮거나 더 높은 가격 범위로 이동시켜야 함. X축: $10(구매자의 초기 가격)에서 $20(판매자의 매도 가격)까지의 거래 간격. Y축: 가격 빈도. A 및 B: claude-instant-v1.0 및 gpt-3.5-turbo와 같은 약한 에이전트의 경우 판매자가 구매자보다 쉽기 때문에 AI 피드백을 통해 개선됨. 판매자의 경우 AI 피드백은 거래 분포를 더 높은 범위(오른쪽)로 이동시키지만, 구매자의 거래 분포는 크게 이동시키지 않습니다. 결과적으로 구매자로 플레이할 때(-0.2 및 -0.5) 평균 거래 가격의 변화는 판매자로 플레이할 때(+1. 및 +1.7)보다 분명히 작습니다. C. 더 강력한 에이전트(claude-v1.3/gpt-4)는 구매자로서도 AI 피드백을 통해 더 큰 평균 거래 가격 변화(-1.2 및 -3.0)를 통해 개선될 수 있습니다. 제안. 그런 다음 게임을 500번 실행하여 피드백 전후의 거래 가격을 계산합니다. 500번 실행 후 다음 후의 개선 사항을 비교합니다. (1) 미리 정의된 풀에서 3개의 제안을 무작위로 샘플링하고 (2) AI 비평가에게 3개의 제안을 적으라고 요청합니다. 이는 인간 피드백의 성과를 과소평가할 수 있지만, 인간에게 1500번 실행 모두에 대해 3개의 제안을 작성하도록 요청하는 것은 비현실적입니다(AI 피드백에는 이러한 문제가 없음). 결과는 표 1에 나와 있으며, 세 가지 모델(gpt-3.5-turbo, claude-instant-v1. 및 claude-v1.3) 모두 인간과 AI 피드백에 비해 비슷한 개선을 보였습니다. 4.3 다양한 LLM 백엔드의 동작 지금까지 게임 설정이 강력한 LLM 엔진에 유효하다는 것을 확인했습니다. 이제 다양한 역할에 대해 다양한 엔진을 사용하여 자세한 동작 비교를 고려해 보겠습니다. 구체적으로 claude-instant-v1.0, claude-v1.3, gpt-3.5-turbo 및 gpt-4를 사용하여 판매자/구매자(각각 gpt-3.5-turbo 구매자/판매자와 대결)로 플레이한 다음 AI 피드백 전/후 거래 가격 분포를 연구합니다(AI 비평가가 플레이어와 동일한 엔진을 사용한다는 점도 기억하세요). 결과는 그림 4에 시각화되어 있습니다. claude-instant-v1.0과 gpt-3.5-turbo가 판매자 역할을 할 때 AI 피드백 후 평균 거래 가격을 개선할 수 있습니다(그림 4A). 하지만 구매자 역할을 할 때는 평균 거래 가격이 개선되지 않아 구매자가 판매자보다 더 어려운 역할을 하는 경향이 있음을 나타냅니다(그림 4B). 하지만 이 관찰은 gpt-4와 Criticize buyerGPT-3.5-Turbo18Claude-instant-v1.16.16.95 16.16.16.15.15.16.15.14RoundRoundRoundRoundA1과 같은 엔진에는 적용되지 않습니다. GPT-3.5-Turbo 판매자는 Al 피드백18RoundRoundRoundRoundB1에서 지속적으로 개선됩니다. Claude-instant-v1.0 판매자는 Al 피드백에서 1라운드만 개선합니다.16 15.16 15.15.15.15.15.14.14.25 14.14-RoundRoundRoundRoundRoundRoundRoundRoundRoundA2. GPT-3.5-Turbo 구매자는 Al 피드백에서 지속적으로 개선됩니다.B2. Claude-instant-v1.0 구매자는 Al 피드백에서 개선되지 않습니다.그림 5: 여러 라운드 설정에서 판매자/구매자로 플레이할 때 다른 엔진은 다른 동작을 합니다.선 플롯은 200회 실행에 걸친 평균 가격이고 막대 플롯은 가격 분포를 나타냅니다.평균 B1. 판매자로 플레이할 때 gpt-3.5-turbo는 여러 라운드에서 AI 피드백에서 개선될 수 있는 반면, claude-instant-v1.0은 첫 번째 라운드만 개선합니다.A2 대 B2. 구매자로 플레이할 때 gpt-3.5-turbo는 여러 라운드에서 개선될 수 있는 반면 claude-instant-v1.0은 그렇지 않습니다. claude-v1.3은 구매자로 플레이하더라도 AI 피드백을 통해 여전히 개선될 수 있기 때문입니다. 전반적으로 이 실험 세트는 우리가 고려하는 네 가지 엔진 간의 미묘한 기능 차이를 보여줍니다. 4.4 반복적 AI 피드백을 통한 지속적인 개선을 향해 이제 게임을 여러 라운드로 펼치고 이전 대화 기록과 반복적 AI 피드백을 통해 모델이 지속적으로 개선될 수 있는지 확인합니다. 구체적으로 gpt-3.5-turbo, gpt-4, claude-instant-v1.0 및 claude-v1.3이 각각 gpt-3.5-turbo로 구동되는 라이벌에 맞서 판매자/구매자로 플레이하게 합니다. 앞서 언급했듯이 비평가는 도움을 주는 플레이어와 동일한 엔진을 공유합니다. 게임을 200번 플레이하고 각 게임을 5라운드로 펼칩니다. 최종 거래 가격과 거래 성공률을 계산하고 가격을 지속적으로 개선할 수 있는지 확인합니다. 그림 5는 gpt-3.5-turbo와 claude-instant-v1.0이 서로 다른 역할을 하는 것을 보여줍니다. 주어진 엔진의 경우 한 라운드에 걸친 개선이 반드시 여러 라운드로 확장되지 않을 수 있습니다. gpt-3.5-turbo는 여러 라운드에 걸쳐 개선될 수 있지만 claude-instant-v1.0은 최대 한 라운드만 개선되기 때문입니다. 이제 그림 6에서와 같이 더 높은 거래 가격을 달성하는 경향과 거래를 깨는 것의 위험 사이의 상충 관계를 고려합니다. 판매자로 플레이할 때 네 가지 모델 엔진 모두 최소한 한 라운드에 걸쳐 개선될 수 있지만 거래 성공률이 감소하는 대가를 치릅니다. 구매자로 플레이할 때 개선될 수 없는 모델(claude-instant-v1.0)이나 오버라운드를 포화시키는 모델(claude-v1.3)이 있는 반면 gpt-4와 gpt-3.5-turbo는 지속적으로 개선될 수 있으며 gpt는 gpt-3.5-turbo보다 더 나은(더 낮은) 거래 가격과 더 높은 거래 비율을 달성합니다. 마지막으로, 우리는 반복적 AI 피드백이 에이전트가 사용하는 언어 복잡성에 어떤 영향을 미치는지 연구하는데, 이는 그림 7에 표시된 대로 각 라운드 후 평균 응답 길이(문자 수로 측정)를 플로팅함으로써 가능합니다. 우리는 claude-v1.3과 gpt-4가 모두 반복적 AI 피드백 후 응답 길이가 지속적으로 증가하면서 더 자세해지는 것을 볼 수 있습니다. 이는 직관적인 이유인데, 자세함이 협상에서 간단한 전략이기 때문입니다. 그러나 claude-v1.3의 경우 자세함이 더 나은 협상 전략으로 이어지지 않는데, 3라운드 후에 개선이 포화되기 때문입니다(그림 6B1). 이와 대조적으로 gpt-4의 자세함 증가는 claude-v1.3보다 적은 단어를 사용하지만 판매자 비판 claude-instant-v1.GPT-3.5-Turbo 5라운드 동안의 평균 거래 가격 1.00 0.750.0.0.A1을 달성하기 때문에 더 전략적입니다. 판매자 역할을 할 때 대부분 모델은 최소한 한 라운드를 개선할 수 있습니다.구매자 비판 클로드-v1.5라운드에 걸친 거래 성공률 GPT-A2. 그러나 거래 성공률은 5라운드에 걸쳐 계속 감소합니다(대부분 마지막 라운드에서 50% 미만). 1.0.0.0.0.B1. 구매자 역할을 할 때 GPT 모델은 클로드 모델보다 Al 피드백에서 개선하는 데 더 뛰어납니다.B2. 거래 성공률은 계속 감소하지만 전반적으로 판매자 비판보다 높습니다(대부분 &gt;50%). 그림 6: 여러 라운드 게임에서 GPT 및 클로드 모델의 성능과 거래 성사 성공률 A1 및 A2: 판매자 역할을 할 때 대부분 모델은 여러 라운드에 걸쳐 개선할 수 있습니다. 그러나 가격이 높으면 판매자가 거래를 파기할 가능성이 더 높으며, 이는 A2의 지속적으로 감소하는 곡선에서 볼 수 있습니다. B1 및 B2: 구매자 역할을 할 때 클로드-인스턴트-v1.은 여러 라운드에 걸쳐 개선할 수 없지만 다른 모델은 가능합니다. 다시 말하지만, 더 나은 매수 가격은 거래에서 도망칠 가능성도 더 높습니다. GPT-4가 여기서 가장 좋은 균형을 이룬 것을 알 수 있습니다. 여러 라운드에 걸쳐 가장 좋은 가격을 얻고 거래에 도달할 성공률도 더 높습니다. 문자 수로 나타낸 응답 길이 Claude-v1.3 판매자 GPT-4 판매자 라운드 5. 이 맞춤 디자인 풍선은 고품질일 뿐만 아니라 다른 풍선과 차별화되는 독특하고 매혹적인 외관을 갖추고 있어 20달러라는 가격으로 훌륭한 가치를 제공합니다. 라운드 4: 이 특별한 풍선은 모든 이벤트에 기쁨을 더하고 오래 지속되는 추억을 만들어 줄 수 있어 20달러라는 가격으로 구매할 가치가 있습니다. 라운드 3. 안녕하세요! 좋은 하루 보내시길 바랍니다. 내구성 있는 소재로 만든 이 독특한 풍선 라운드 2. 이 고품질 풍선은 내구성 있는 소재로 만들어졌으며 약간의 소재도 제공할 수 있으며 가격은 20달러입니다. 라운드 I. 15달러에 중간 지점에서 만나면 어떨까요? $18에서 할인.그림 7: 모델이 여러 라운드에서 학습함에 따라 평균 응답 길이가 증가합니다.여기서 우리는 구매자의 초기 질문인 &quot;10달러에 판매할 생각이 있습니까?&quot;에 대한 질문에 대한 판매자의 응답 예를 보여줍니다.여러 라운드의 협상 후 판매자의 응답은 더 자세하고 단어에 맞춰집니다.그러나 자세함이 더 나은 전략을 의미하지는 않습니다.claude-v1.3은 gpt-4보다 자세하지만(더 높은 곡선) 성공률과 거래 가격이 더 나쁩니다(그림 6 참조).이는 gpt-4의 자세함이 더 전략적임을 나타냅니다.더 나은 거래 가격과 거래 성공률(그림 6B).이 관찰은 AI 피드백이 단어에 맞춰진 전략적 방향에 대한 플레이어의 응답을 개선한다는 강력한 증거가 됩니다.5 결론 이 연구에서 우리는 여러 개의 대규모 언어 모델이 롤플레잉과 AI 피드백으로부터 학습하여 협상 게임에서 자율적으로 서로를 개선할 수 있는지 연구합니다. 우리의 실험은 특정 모델이 AlphaGo Zero 방식으로 잘 정의된 규칙에 따라 반복적인 AI 피드백을 통해 경쟁 게임을 지속적으로 플레이함으로써 실제로 개선될 수 있음을 보여줍니다. 또한 더 나은 거래 가격은 거래 파기의 위험도 더 높기 때문에 다음 라운드 가격 개선과 성공률 간의 상충 관계를 보여줍니다. 이는 향후 연구에서 여러 라운드에 걸쳐 전반적인 이득을 개선하기 위해 글로벌 최적화를 고려할 수 있음을 시사합니다. 또한 반복적인 AI 피드백에서 개선된 언어의 증거를 보여줍니다. 여러 라운드 게임에서 한 모델은 다른 모델보다 덜 장황할 수 있지만 단어 조정이 더 잘 되어 더 나은 거래를 얻는 데 더 효과적일 수 있습니다. 우리는 우리의 결과가 AI 연구에 심오한 의미를 갖는다고 믿습니다. 긍정적인 측면에서 최소한의 인간 개입으로 언어 모델을 지속적으로 개선할 가능성을 나타냅니다. 위험한 측면에서 모델이 자율적으로 작동하기 때문에 프레임워크에서 모델 동작을 감독하기가 더 어려울 수 있으며, 이는 멀티 에이전트 게임 설정에서 미래의 정렬 및 안전 연구를 요구합니다. 전반적으로 우리는 우리의 연구가 게임 플레이 및 반복적인 AI 피드백에서 대규모 언어 모델의 학습에 대한 의미 있는 초기 탐색을 제공한다고 믿습니다. 참고 문헌 [1] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma 및 Denny Zhou. 상황 내 학습이란 어떤 학습 알고리즘인가요? 선형 모델을 사용한 조사. arXiv 사전 인쇄 arXiv:2211.15661, 2022. [2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 정렬을 위한 실험실로서의 일반 언어 보조원입니다. arXiv 사전 인쇄 arXiv:2112.00861, 2021. [3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 헌법적 AI: AI 피드백이 무해합니다. arXiv 사전 인쇄본 arXiv:2212.08073, 2022. [4] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. 대규모 언어 모델에 대한 확장 가능한 감독의 진행 상황 측정. arXiv 사전 인쇄본 arXiv:2211.03540, 2022. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. [6] Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, Jonathan Gratch. 카지노: 자동 협상 시스템을 위한 캠프사이트 협상 대화 모음. arXiv 사전 인쇄본 arXiv:2103.15721, 2021. [7] Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou. 대규모 언어 모델에 자체 디버깅을 가르치기. arXiv 사전 인쇄본 arXiv:2304.05128, 2023. [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. [9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio. 시퀀스 모델링에서 게이트형 순환 신경망의 경험적 평가. arXiv 사전 인쇄본 arXiv:1412.3555, 2014. [10] Cohere. Cohere 명령 모델. Cohere 웹사이트, 2023. URL https://docs.cohere.com/docs/models. [11] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei. gpt가 맥락 내에서 학습할 수 있는 이유는? 언어 모델은 비밀리에 메타 옵티마이저로 경사 하강을 수행합니다. arXiv 사전 인쇄본 arXiv:2212.10559, 2022. [12] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 언어 모델과 전략적 추론을 결합하여 외교 게임에서 인간 수준의 플레이. Science, 378(6624):1067-1074, 2022. [13] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 다단계 추론을 위한 복잡성 기반 프롬핑. arXiv 사전 인쇄본 arXiv:2210.00720, 2022.[14] Yao Fu, Litu Ou, Mingyu Chen, and Yuhao Wan. Ilms의 추론 성능 측정. Github, 2023. URL https://github.com/FranxYao/chain-of-thought-hub. [15] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 타깃 인간 판단을 통해 대화 에이전트의 정렬 개선. arXiv 사전 인쇄본 arXiv:2209.14375, 2022. [16] Google. Palm 2 기술 보고서. Arxiv, 2023. [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 컴퓨팅 최적화 대규모 언어 모델 학습. arXiv 사전 인쇄본 arXiv:2203.15556, 2022. [18] AI21 Labs. Jurassic-2 및 작업별 API 발표. A121 블로그, 2023. URL https: //www.ai21.com/blog/introducing-j2. [19] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, Dhruv Batra. 거래 여부? 협상 대화를 위한 종단 간 학습. arXiv 사전 인쇄본 arXiv:1706.05125, 2017. [20] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem. Camel: 대규모 언어 모델 사회의 &quot;마인드&quot; 탐색을 위한 커뮤니케이션 에이전트. arXiv 사전 인쇄 arXiv:2303.17760, 2023. [21] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar 등. 언어 모델의 전체적인 평가. arXiv 사전 인쇄 arXiv:2211.09110, 2022. [22] Hao Liu, Carmelo Sferrazza 및 Pieter Abbeel. 언어는 보상입니다. 인간의 피드백을 사용하여 사후 통찰력을 미세 조정합니다. arXiv 사전 인쇄본 arXiv:2302.02676, 2023. [23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 자체 개선: 자체 피드백을 통한 반복적 개선. arXiv 사전 인쇄본 arXiv:2303.17651, 2023. [24] OpenAI. Gpt-4 기술 보고서. Arxiv, abs/2303.08774, 2023. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. 신경 정보 처리 시스템의 발전, 35:27730-27744, 2022. [26] Joon Sung Park, Joseph C O&#39;Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, Michael S Bernstein. 생성 에이전트: 인간 행동의 대화형 시뮬라크라. arXiv 사전 인쇄본 arXiv:2304.03442, 2023. [27] Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 모델에서 작성한 평가를 통해 언어 모델 동작 발견. arXiv 사전 인쇄 arXiv:2212.09251, 2022. [28] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang 및 Huajun Chen. 언어 모델 프롬프트를 통한 추론: 설문조사. arXiv 사전 인쇄 arXiv:2212.09597, 2022. [29] Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, 조경현 및 Ethan Perez. 자연어 피드백으로 언어 모델을 훈련합니다. arXiv 사전 인쇄본 arXiv:2204.14146, 2022. [30] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot 등. 딥 신경망과 트리 검색을 통한 고 게임 마스터링. nature, 529(7587):484–489, 2016. [31] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton 등. 인간의 지식 없이 고 게임 마스터링. nature, 550(7676):354–359, 2017.[32] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov 및 Max Vladymyrov. Transformer는 경사하강법을 통해 상황에 맞게 학습합니다. arXiv 사전 인쇄 arXiv:2212.07677, 2022. [33] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler 등. 대규모 언어 모델의 새로운 능력. arXiv 사전 인쇄 arXiv:2206.07682, 2022. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le 및 Denny Zhou. 사고의 사슬을 촉발하면 대규모 언어 모델에서 추론이 유도됩니다. arXiv 사전 인쇄본 arXiv:2201.11903, 2022.
