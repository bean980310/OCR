--- ABSTRACT ---
코드 커버리지는 명령문이나 분기와 같은 프로그램 요소가 테스트 중에 실행되는 정도를 정량화하는 데 널리 사용되는 지표입니다. 코드 커버리지를 계산하는 것은 리소스를 많이 소모하여 코드 빌드와 실행이 필요하고 계측에 대한 추가 오버헤드가 필요합니다. 게다가 코드 조각의 커버리지를 계산하려면 전체 프로그램 컨텍스트가 필요합니다. 머신 러닝을 사용하여 이 값비싼 프로세스를 상각하면 소스 코드 컨텍스트만 필요하므로 코드 커버리지 비용을 낮출 수 있으며, 코드 커버리지 예측 작업은 모델의 코드 이해 능력을 판단하는 새로운 벤치마크가 될 수 있습니다. 우리는 대규모 언어 모델(LLM)에 대한 코드 커버리지 예측이라는 새로운 벤치마크 작업을 제안합니다. 우리는 이 작업을 공식화하여 주어진 테스트 케이스와 입력에 의해 메서드의 어떤 줄이 실행되는지 확인하여 코드 실행을 이해하는 LLM의 능력을 평가합니다. 우리는 HumanEval 데이터 세트에서 테스트와 코드를 실행하고 코드 커버리지 정보를 수집하여 COVERAGEEVAL이라는 데이터 세트를 큐레이션하고 릴리스합니다. 우리는 OpenAI의 GPT-4와 GPT-3.5-Turbo, Google의 BARD, Anthropic의 Claude를 포함하여 코드 관련 작업에 사용되는 4가지 최첨단 LLM의 성능을 코드 커버리지 예측 작업에 보고합니다. 마지막으로, 우리는 코드 커버리지가 메트릭 및 사전 학습 데이터 소스로서 소프트웨어 엔지니어링 작업에서 전반적인 LLM 성능에 중요하다고 주장합니다. 1
--- METHOD ---
주어진 테스트 케이스와 입력에 의해 실행됩니다. HumanEval 데이터 세트에서 테스트와 코드를 실행하고 코드 커버리지 정보를 수집하여 COVERAGEEVAL이라는 데이터 세트를 큐레이션하여 릴리스합니다. OpenAI의 GPT-4 및 GPT-3.5-Turbo, Google의 BARD, Anthropic의 Claude를 포함하여 코드 관련 작업에 사용되는 최첨단 LLM 4개의 성능을 코드 커버리지 예측 작업에 보고합니다. 마지막으로 코드 커버리지를 메트릭 및 사전 학습 데이터 소스로 사용하면 소프트웨어 엔지니어링 작업에서 전반적인 LLM 성능에 가치가 있다고 주장합니다. 1 서론 소프트웨어 테스트는 새 버전을 출시하기 전에 프로그램의 버그를 감지하는 것을 목표로 하는 소프트웨어 수명 주기의 필수적인 부분입니다. 코드 커버리지는 테스트의 품질을 추정하는 널리 사용되는 메트릭으로, 시스템이 지정된 요구 사항에 따라 작동할 것이라는 확신을 제공합니다. 여러 표준은 배포가 허용되기 전에 소프트웨어 시스템에 대한 특정 수준의 코드 커버리지를 요구합니다. 초점 방법 {m} public String foo(int x) { if(x == 0) { return &quot;zero&quot;; } else if(x &gt; 0) { return &quot;positive&quot;; } else { } return &quot;negative&quot;; return &quot;impossible&quot;;} public void testFoo() { &gt; 테스트 케이스 {t} String res = foo(2); Assert.isEqual(&quot;positive&quot;, res);} 커버리지 주석이 달린 방법 {cov(m,t)} public String foo(int x) { if (x == 0) { return &quot;zero&quot;; } else if(x &gt; 0) { return &quot;positive&quot;; } else { } return &quot;negative&quot;; return &quot;impossible&quot;; } 그림 1: 테스트 대상 메서드인 초점 메서드 m과 해당 메서드를 포함하는 테스트 케이스 t가 주어졌을 때, t가 m에서 얻은 코드 커버리지는 커버리지 주석이 달린 메서드 cov(m,t)로 표현할 수 있습니다. 여기서 &gt;는 실행된 명령문을 나타내고, !는 실행되지 않은 명령문을 나타내며, -는 도달할 수 없는 코드를 나타냅니다. 예를 들어, 커버리지는 DO-178B(Johnson, 1998) 및 DO-178C(Rierson, 2017)에 기록된 대로 연방 항공청(FAA)에서 항공 장비의 안전 인증을 위해 고려하는 지표 중 하나입니다. 테스트 커버리지는 자동차 안전 표준 ISO 26262 Road Vehicles Functional Safety(Palin et al., 2011)의 요구 사항이기도 합니다. 테스트 케이스 t에서 직접 실행하는 초점 메서드 m이 주어졌을 때, 코드 커버리지는 테스트 t에서 실행된(즉, 커버된) 명령문의 수를 측정합니다. 그림 1은 t에서 테스트한 초점 메서드 m(테스트 대상 메서드)의 예를 보여줍니다. t에서 m으로 얻은 적용 범위는 적용 범위 주석이 달린 메서드 cov(m, t)로 표현되며, 여기서 실행된 명령문은 while로 표시되고(즉, 적용되지 않은 명령문) 도달할 수 없는 코드(즉, 죽은 코드)는 while로 표시됩니다. 이러한 표현을 통해 기능, 명령문, 분기 및 경로 적용 범위와 같은 여러 정량적 적용 범위 메트릭을 계산할 수 있습니다. 코드 적용 범위는 코드를 계측하고 코드 실행을 모니터링하는 동안 테스트 모음을 실행하여 계산합니다. 이 프로세스는 특히 대규모 소프트웨어 프로젝트의 경우 또는 코드 적용 범위가 여러 번 계산되는 경우 코드를 빌드하고 실행해야 하므로 비용이 많이 듭니다. 또한 주어진 스니펫이 포함된 전체 프로그램을 사용할 수 없으면 코드 스니펫에 대한 코드 적용 범위를 측정할 수 없습니다. 이러한 상황은 예를 들어 커밋 로그/차이점 내에서 일부 코드만 사용할 수 있거나 보안 및/또는 네트워킹 이유로 일부 코드만 서버로 전송될 때 발생합니다. 대규모 언어 모델(LLM)은 코드 관련 작업에서 두각을 나타내고 코드 생성 및 테스트 생성과 같은 분야에서 인상적인 결과를 보여주었지만 이러한 모델이 코드 실행을 실제로 어느 정도 이해하는지는 여전히 불분명합니다(Liu et al., 2023). 주어진 테스트 사례와 해당 입력에 따라 메서드의 어떤 줄이 실행되는지 정확하게 판단하는 작업에는 기본 코드 실행 역학에 대한 심층적인 이해가 필요합니다. 이는 코드 커버리지 예측이라고 하는 전담 작업의 필요성을 유발하는데, 이는 LLM이 코드 실행을 이해하는 능력을 구체적으로 평가합니다. 또한 이 작업을 수행할 수 있는 모델은 비용이 많이 드는 코드 커버리지 계산 프로세스를 상각하거나 정상적인 코드 커버리지를 계산할 수 없는 경우에 기능할 수 있으므로 독립적으로 유용합니다. 이 논문에서는 코드 커버리지 예측 작업을 공식화하여 주어진 테스트 사례에 따라 메서드의 어떤 줄이 실행되는지 정확하게 판단하여 코드 실행을 이해하는 LLM의 능력을 평가하는 것을 주요 목표로 합니다. 평가를 용이하게 하기 위해, 우리는 커버리지 주석이 달린 방법으로 구성된 COVERAGEEVAL이라는 포괄적인 데이터 세트를 큐레이션했습니다. 이 데이터 세트는 HumanEval 데이터 세트에서 테스트와 코드를 실행하여 생성되며, 이를 통해 귀중한 코드 커버리지 정보를 수집할 수 있습니다. 우리는 이 큐레이션된 데이터 세트를 정리하여 GitHub에 공개하여 연구자들이 코드 커버리지 예측 기술과 LLM 코드 이해를 탐구하고 발전시킬 수 있도록 했습니다. 우리는 코드 관련 작업에 널리 사용되는 4가지 최신 LLM, 즉 OpenAI의 GPT-4와 GPT-3.5, Google의 BARD, Anthropic의 Claude의 성능을 평가합니다. 우리의 궁극적인 목표는 다양한 시나리오에서 실행 기반 커버리지 측정에 대한 유망한 대안을 제공하여 코드 커버리지를 예측하는 LLM의 기능에 대한 통찰력을 얻는 것입니다. 이 접근 방식은 프로그램 빌드 및 실행과 관련된 비용이 엄청나거나, 코드 커버리지를 여러 번 호출해야 하거나, 코드 스니펫만 사용할 수 있는 경우(예: 서버 측 시나리오) 또는 프로젝트의 오류로 인해 빌드를 완료할 수 없는 경우에 유리합니다. 또한 이 작업은 코드 이해도를 평가하기 위한 새로운 지표를 도입하고 귀중한 (사전) 학습 목표로 사용됩니다. 이 작업에서 뛰어난 성과를 내도록 모델을 학습시킴으로써 코드 관련 작업에서 전반적인 성능을 향상시킬 수 있다고 믿습니다. 이 논문은 다음과 같은 기여를 합니다. • 코드 커버리지 예측 작업: 주어진 테스트 사례와 입력을 기반으로 메서드의 실행된 줄을 정확하게 예측하여 LLM이 코드 실행을 이해하는 능력을 평가하는 새로운 작업을 제안합니다. • • 최첨단 LLM 평가: 코드 커버리지 예측 작업에서 4가지 뛰어난 LLM(GPT-4, GPT-3.5, BARD 및 Claude)을 평가하여 코드 실행에 대한 성능과 이해도에 대한 통찰력을 제공합니다. 큐레이팅된 데이터 세트: HumanEval 데이터 세트에서 파생된 커버리지 주석이 달린 메서드와 테스트 사례의 포괄적인 데이터 세트(COVERAGEEVAL)를 큐레이팅합니다. 이 데이터 세트는 GitHub¹에서 공개적으로 사용할 수 있으므로(Microsoft, 2023) 코드 커버리지 예측 기술에 대한 추가 연구와 발전이 가능합니다. 2 배경 코드 커버리지는 테스트 모음이 소프트웨어 시스템을 실행하는 정도를 측정하는 것입니다(Ivanković et al., 2019). 코드 커버리지는 일반적으로 계측을 통해 계산됩니다. 이 기술은 https://github.com/microsoft/coverage-eval 테스트에서 프로그램의 코드나 바이너리 내의 다양한 위치에 계측 코드를 삽입하여 실행을 모니터링합니다. 이 삽입된 코드는 테스트 모음에서 실행된 프로그램의 함수나 명령문을 기록하는 카운터를 제공합니다. 원래 코드 내에 이러한 추가 명령문을 삽입하면 실행 오버헤드가 발생하며, 이는 특히 대규모 소프트웨어 프로그램의 경우 상당할 수 있습니다(Tikir and Hollingsworth, 2002). 가장 일반적인 커버리지 메트릭은 명령문 수준에서 계산되며, 여기서 명령문은 코드의 구문적 단위(예: 할당, 호출, 어설션)를 나타내며 종종 단일 코드 줄과 일치합니다. 커버리지는 명령문이 실행되었는지 여부를 나타내며, 집계된 메트릭은 테스트 모음에서 다루는 명령문의 양을 측정하기 위해 함수/프로그램 수준에서 계산될 수 있습니다. 그림 1의 예에서 테스트 케이스 t는 m에서 4개의 명령문을 실행하는데, 이는 메서드 m에 대해 ~44%의 명령문 커버리지를 구성합니다. 명령문 커버리지 정보가 주어지면 정적 분석을 통해 다른 커버리지 기준과 메트릭을 얻을 수 있습니다. 제어 구조(예: if-else 및 case 명령문)에 대한 명령문 커버리지 정보를 사용하여 분기 커버리지를 계산할 수 있으며, 이는 프로그램에서 실행된 논리적 분기의 수를 측정합니다. 그림 1의 예에서는 하나의 분기만 실행되고(즉, else if (x &gt; 0)), 다른 두 분기는 테스트 케이스 t에서 놓칩니다. 이 논문의 나머지 부분에서는 다른 커버리지 기준을 얻을 수 있는 명령문 커버리지에 중점을 둘 것입니다. 3 코드 커버리지 예측 작업 == 테스트 대상 메서드(초점 메서드) m이 주어지고, n개의 명령문으로 구성됨 Sm = S1, S2, . . ., Sn, 그리고 방법 m을 실행하는 테스트 케이스 t가 있고, 커버리지 주석이 달린 초점 방법 cov(m, t)는 n개의 문장 St S1, S2,..., 8의 시퀀스로 구성되며, 여기서 각 문장은 m의 si의 커버리지 주석이 달린 문장을 나타낸다. 구체적으로, s는 세 가지 가능한 커버리지 기호 c = {&gt;,!,-} 중 하나로 표시되며, 여기서 기호 &gt;는 t가 실행한 문장을 식별하고, 기호 !는 t가 놓친 문장을 식별하며, 기호는 도달할 수 없는 문장을 식별한다. 이는 n개의 커버리지 기호 = C1, C2, ..., Cn의 시퀀스를 정의하는데, 여기서 c₂ = {&gt;,!, −}이다. 코드 커버리지 예측 작업은 초점 방법 m과 테스트 케이스 t가 주어졌을 때 커버리지 주석이 달린 I&#39;m m 문장 St 시퀀스를 예측하는 문제로 정의한다. 형식적으로 이 문제는 입력과 예상 출력의 관점에서 정의할 수 있습니다.입력 • 초점 방법: m • 테스트 케이스: t 출력 • • S = 81, 82 또는 m C=C1, C2, ., Cn mm&#39; 구체적으로 출력은 커버리지 주석이 붙은 문장 St의 시퀀스 또는 커버리지 심볼 Ctn의 시퀀스가 될 수 있으며, 이는 원래 문장 Sm = S1, S2, . . ., Sn 시퀀스와 결합하여 커버리지 주석이 붙은 문장 St s†, s½, … …. … ‚ s½를 얻어 커버리지 cov(m, t)를 구성합니다. 이 마지막 단계는 두 시퀀스를 정렬하고 s* = ci + Si를 얻어서 수행합니다.여기서 + 연산은 문자열 연결을 나타냅니다.= 그림 1에서 초점 방법 m과 테스트 케이스 t를 예로 들어 보겠습니다.이 모델은 커버리지 주석이 붙은 문장 St 시퀀스 또는 커버리지 심볼 시퀀스를 예측할 것으로 예상됩니다.&gt; &gt; &gt; &gt; ! ! ! -. 3. 사전 학습을 위한 커버리지 예측 본 논문에서 소개한 코드 커버리지 예측 과제는 코드 생성에 초점을 맞춘 LLM을 위한 귀중한 사전 학습 과제로 활용될 수 있다고 제안합니다. Masked Language Modeling(MLM)과 같은 현재의 사전 학습 과제는 코드를 나타내는 방대한 양의 원시 텍스트를 분석하여 모델이 코드 구문과 의미를 이해하도록 돕는 반면, 제안하는 과제는 모델이 소스 코드 텍스트만으로는 기술적으로 발견할 수 없는 코드 실행에 대해 학습할 수 있도록 합니다. 이 사전 학습을 달성하기 위해 CI/CD(Continuous Integration/Continuous Deployment) 파이프라인에서 얻은 광범위한 커버리지 로그로 학습 데이터를 보강하는 것을 제안합니다. 이러한 로그에는 풀 요청 또는 커밋 중에 실행된 회귀 테스트의 코드 커버리지에 대한 귀중한 정보가 포함되어 있습니다. 사전 학습 중에 모델을 이러한 커버리지 로그에 노출시키면 실행되는 특정 코드 줄과 테스트 사례 및 입력을 연관시키는 방법을 학습할 수 있습니다. 이 사전 학습 방식은 다양한 테스트 시나리오에서 코드의 여러 부분이 어떻게 실행되는지에 대한 모델의 이해를 향상시킵니다. 따라서 모델은 입력, 테스트 및 코드 실행 간의 관계에 대한 보다 깊은 이해를 얻을 수 있어 코드 생성 기능이 향상됩니다. 사전 학습 작업으로 커버리지 예측을 통합하면 모델이 실제 테스트 시나리오에서 학습하여 실제 환경에서 코드 실행의 뉘앙스를 포착할 수 있습니다. 이러한 실제 노출은 실제 테스트 관행과 일치하는 코드를 생성하는 모델의 능력을 향상시킵니다. 또한 사전 학습 작업으로 커버리지 예측을 통합하면 전이 학습의 가능성이 열립니다. 커버리지 예측에 대해 사전 학습된 모델은 코드 실행을 이해하는 것이 중요한 버그 감지 또는 테스트 사례 생성과 같은 다운스트림 작업에서 미세 조정할 수 있습니다. 모델의 기존 코드 커버리지 지식은 이러한 관련 작업에 대한 견고한 기반을 제공하여 전반적인 성능을 잠재적으로 개선할 수 있습니다. 4 COVERAGEEVAL 데이터 세트 이 논문에서는 코드 커버리지 예측 작업을 제안하는 것 외에도 이 작업에서 LLM을 평가하도록 특별히 설계된 데이터 세트인 COVERAGEEVAL도 소개합니다. 이 섹션에서는 HumanEval 데이터 세트(Chen et al., 2021)로 시작하는 이 데이터 세트를 큐레이팅하는 프로세스를 설명합니다. HumanEval 데이터 세트에서 테스트 사례를 실행하여 코드 커버리지 정보를 수집합니다. COVERAGEEVAL을 만들려면 테스트 사례 실행 중에 생성된 코드 커버리지 로그를 구문 분석합니다. 이 구문 분석 단계를 통해 관련 커버리지 주석을 추출할 수 있습니다. 그런 다음 연구자와 실무자 모두가 사용하고 평가할 수 있는 형식으로 데이터 세트를 신중하게 구성하고 내보냅니다. 이 데이터 세트를 큐레이팅하여 코드 커버리지 예측 작업에서 LLM을 평가하기 위한 표준화된 벤치마크를 제공하는 것을 목표로 합니다. COVERAGEEVAL을 사용하면 연구자가 코드 이해를 탐색하고 발전시켜 혁신을 촉진하고 보다 효과적인 모델을 개발할 수 있습니다. 4.1 HumanEval HumanEval 데이터 세트는 164개의 손으로 쓴 문제와 해당 코드 솔루션으로 구성되어 있으며, 각 문제는 언어 이해, 추론, 알고리즘 및/또는 간단한 수학을 포함하는 프로그래밍 작업입니다(Chen et al., 2021). 데이터 세트의 각 코드 솔루션에는 함수 시그니처, 문제 설명을 포함하는 docstring, 함수 본문 및 여러 단위 테스트가 포함됩니다. 함수 본문과 해당 단위 테스트를 사용하여 계산된 적용 범위를 포함하도록 HumanEval 데이터 세트를 확장합니다. 4.2 적용 범위 분석 이 섹션에서는 HumanEval 데이터 세트의 코드 적용 범위를 분석하고 COVERAGEEVAL 데이터 세트를 만드는 데 수행한 단계를 설명합니다. HumanEval 데이터 세트의 각 코드 솔루션에는 단일 테스트 사례가 함께 제공되며, 여기에는 주어진 문제의 기능적 요구 사항에 따라 코드 솔루션의 정확성을 테스트하도록 설계된 여러 어설션이 포함됩니다. 이러한 어설션은 다양한 입력, 시나리오 및 코드 문장/분기를 포함합니다. 데이터 세트를 향상시키고 각 데이터 포인트의 복잡성을 높이기 위해 단일 테스트 사례를 여러 테스트 사례로 분할했으며 각각에 단일 어설션이 포함됩니다. 이 분할 프로세스를 통해 추가 메서드-테스트 쌍을 생성할 수 있을 뿐만 아니라 각 데이터 포인트를 더 어렵게 만들 수 있습니다. 원래 테스트 사례는 메서드의 대부분 줄과 분기를 포함할 수 있지만 각 개별 어설션은 그 중 일부만 포함합니다. 이 분할을 수행하여 데이터 세트 내에서 더 다양한 메서드-테스트 쌍 세트를 만듭니다. 각 개별 테스트 사례는 초점 메서드를 한 번 호출하고 메서드 내의 명령문과 분기의 하위 집합을 다룹니다. 이를 통해 메서드의 전체 커버리지를 넘어서 LLM이 코드 커버리지를 보다 세부적으로 예측하는 능력을 평가할 수 있습니다. 또한 각 어설션에 대한 커버리지를 예측하려면 코드와 잠재적 실행 경로에 대한 더 깊은 이해가 필요하므로 작업에 복잡성이 추가됩니다. 그런 다음 pytest를 사용하여 추출된 테스트 사례를 개별적으로 실행합니다. 실행하는 동안 coverage.py를 사용하여 커버리지 계산도 활성화합니다. 이를 위해 다음 명령을 실행합니다. coverage run -m pytest<test_name> 어디<test_name> 는 데이터 세트의 각 개별 테스트입니다. 그런 다음 각 테스트 사례 t에 대해 테스트 실행으로 얻은 해당 커버리지 보고서를 분석하여 주석이 달린 커버리지 cov(m, t)를 추출합니다. 커버리지 보고서는 파일의 각 소스 코드 줄을 커버리지 정보로 표시하여 명령문이 실행되었는지 여부를 지정합니다. 이 보고서를 자동으로 구문 분석하고 해당 주석이 달린 커버리지 cov(m, t)를 추출합니다. 이 프로세스가 끝나면 각 데이터 포인트가 삼중항 d {m, t, cov(m, t)}로 구성된 데이터 세트를 얻었습니다. 4.3 데이터 형식 = COVERAGEEVAL 데이터 세트는 각 테스트에 대한 커버리지 정보를 추가하여 HumanEval 데이터 세트의 구조를 유지합니다. 각 레코드는 고유한 문제에 해당하며 다음 필드를 포함합니다. • 문제 ID: 문제의 고유 ID • 문제: 문제를 해결하기 위해 작성된 메서드의 이름 • 메서드: 함수 서명, 문제 세부 정보가 있는 docstring 및 함수 본문을 포함한 메서드 콘텐츠. • 테스트: 문제에 대한 단위 테스트 목록입니다. 목록의 각 항목에는 테스트의 고유 ID와 테스트 코드가 포함됩니다. 또한 다음 두 가지 형태로 각 테스트에 대한 커버리지 정보를 추가했습니다. 1. 커버리지: 메서드의 코드로, 각 줄에는 주어진 테스트에서 실행되거나, 놓치거나, 도달할 수 없는 코드에 대해 &gt;, ! 또는 -로 주석이 달려 있습니다. 2. 커버리지 시퀀스: 메서드의 줄 수와 길이가 같은 목록으로, 목록의 각 값은 메서드의 해당 코드 줄 상태에 따라 또는 입니다. 그림 3(부록)은 COVERAGEEVAL 데이터 세트의 샘플 레코드를 보여줍니다. COVERAGEEVAL은 GitHub(Microsoft, 2023)을 통해 대중에게 제공됩니다. 표 1은 문제 수, 코드 솔루션, 테스트 및 커버리지 심볼 측면에서 COVERAGEEVAL 데이터 세트의 통계를 보고합니다. 문제 수와 솔루션 간의 불일치는 일부 문제에 여러 솔루션이 있다는 사실로 설명됩니다. 또한 현재 데이터 세트에 도달할 수 없는 코드(-)가 포함되어 있지 않지만 작업을 설계하는 동안 도달할 수 없는 코드의 잠재적 존재를 적극적으로 고려했다는 점도 주목할 만합니다.문제 해결책 테스트커버리지 심볼 실행됨(&gt;) 누락됨(!) 도달할 수 없음(-)표 1: COVERAGEEVAL 통계.5 LLM 평가 이 섹션에서는 제안된 코드 커버리지 예측 작업에 대한 최첨단 언어 모델(LLM)에 대한 평가를 제시합니다.코드 생성에 인기가 있을 뿐만 아니라 다른 자연어(NL) 작업에도 널리 사용되는 4개의 높은 평가를 받는 LLM을 선택했습니다.이 평가에 사용한 LLMS는 OpenAI의 GPT4 및 GPT-3.5, Google의 BARD, Anthropic의 Claude입니다. GPT-3.5(Brown et al., 2020)와 GPT-4(OpenAI, 2023)는 OpenAI에서 개발한 대규모 언어 모델로, Transformer 스타일 모델(Vaswani et al., 2017)로 문서의 다음 토큰을 예측하도록 사전 학습되었습니다. 두 모델 모두 인간 피드백을 통한 강화 학습(RLHF)(Christiano et al., 2017)을 사용하여 미세 조정되었습니다. GPT-4는 이미지와 텍스트를 모두 입력으로 받아들이고(멀티모달 모델) 텍스트를 출력으로 생성하여 이전 모델보다 개선되었습니다. BARD는 Google에서 개발한 대화형 AI로, 대화에서 학습된 Transformer 기반 언어 모델(Adiwardana et al., 2020)인 LaMDA(Thoppilan et al., 2022)를 기반으로 합니다. Anthropic Claude는 Anthropic에서 개발한 520억 개의 매개변수를 가진 LLM입니다. 클로드는 방대한 텍스트 코퍼스에서 사전 학습되었고 &quot;RL from AI Feedback&quot;(RLAIF)으로 미세 조정되었습니다. 여기서 AI 피드백은 인간이 정의한 &quot;체질&quot;(Bai et al., 2022)에서 도출한 소수의 원칙에 따라 조정됩니다. 5.1
--- EXPERIMENT ---
al 디자인 코드 커버리지 예측 과제에서 LLM을 평가할 때, 점진적으로 더 많은 정보와 예를 제공하면서 사소하지 않은 커버리지 시퀀스에서 성능을 평가하도록 실험을 설계했습니다.먼저, 커버리지 시퀀스가 오로지 기호로만 구성된 사소하고 데이터 포인트 d{m, t, cov(m, t)}를 필터링했습니다.이러한 사례는 분기가 없는 방법을 나타내거나 테스트 사례가 초점 방법의 모든 명령문을 포함하는 경우를 나타냅니다.이러한 데이터 포인트는 COVERAGEEVAL 데이터 세트에 포함되어 있지만 이 특정 평가에서는 제외했습니다.사소한 기호만 포함된 데이터 포인트의 하위 집합은 온라인 부록에 보고되어 있습니다.데이터 세트의 어떤 데이터 포인트도 오로지 또는 - 기호로만 구성된 커버리지 시퀀스가 없다는 점에 유의하는 것이 중요합니다.이 필터링 단계 후, LLM을 평가한 478개의 데이터 포인트가 남았습니다.LLM을 평가하는 데 사용된 프롬프트는 다음 섹션을 포함하도록 설계되었습니다.• • 시스템 NL 프롬프트: LLM에 작업을 전달하는 것을 목표로 하는 작업에 대한 자연어 설명을 제공하는 프롬프트입니다. 예: 작업의 0, 1 또는 여러 예.시스템 NL 프롬프트 당신은 터미널입니다.지침: 사용자가 다음을 실행할 때: coverage run -m pytest code.py 그런 다음 각 줄이 아래 두 기호 중 하나로 시작하는 code.py 파일을 cat합니다.&gt; 줄이 실행되면 !줄이 실행되지 않습니다.예제 출력: &gt; linel !line&gt; line&gt; linen 당신의 작업은 다른 테스트 사례가 주어졌을 때 어떤 줄이 실행될지 알아내는 것입니다.예제 • 초점 방법 m 및 테스트 사례 t.시스템 NL 프롬프트의 경우 평가에는 다양한 프롬프트와 설명을 실험하는 것이 포함되었습니다.터미널 환경(예: python 터미널)을 에뮬레이트하는 시스템 프롬프트를 활용하여 가장 유리한 결과를 얻었습니다.이 프롬프트 내에서 LLM에 주어진 테스트 사례 및 방법을 기반으로 코드 커버리지 출력을 생성하도록 지시했습니다.OpenAI 모델의 경우 이 프롬프트를 특정 시스템 프롬프트 섹션에 포함했지만 BARD 및 Claude의 경우 프롬프트의 초기 부분으로 통합했습니다. LLM의 성과를 종합적으로 평가하기 위해 코드 커버리지 예측 작업에 대해 다양한 수의 예제를 사용하여 평가를 수행했습니다. 구체적으로, 우리는 제로샷, 원샷 및 멀티샷 프롬핑 접근 방식을 사용했습니다. 이를 통해 예제 가용성이 모델의 성과에 미치는 영향과 다양한 방법에 걸쳐 작업을 일반화하는 능력을 조사할 수 있었습니다. 특정 방법 mi에 대한 커버리지를 평가하기 위한 예제를 선택할 때, 우리는 데이터 누출을 방지하고 LLM이 다른 방법으로 예측을 일반화하도록 장려했습니다. 이를 달성하기 위해, 예제를 제공할 때 m; ‡ m¿인 데이터 포인트 {m;,t, cov(m,t)}를 무작위로 샘플링했습니다. 마지막으로, 프롬프트는 모델이 코드 커버리지를 예측할 것으로 예상하는 초점 방법 m과 해당 테스트 케이스 t를 제공합니다. 그림 2는 우리가 설계한 프롬프트의 예를 보여줍니다. 추론은 temperature와 topp을 0으로 설정하고 하나의 샘플을 생성하는 모든 LLM에서 수행됩니다. (anaconda3-2020.11) cat code.py def split_words (txt): (anaconda3-2020.11) cat test.py def test(): assert split_words (&quot;안녕하세요, 세상!&quot;) == [&quot;안녕하세요&quot;, &quot;세상!&quot;] assert True (anaconda3-2020.11) coverage run -m &gt; def split_words (txt): &gt; &gt; if &quot;&quot;in txt: return txt.split() elif &quot;,&quot; in txt: else: pytest test.py return txt.replace(&#39;,&#39;,&#39; &#39;).split() 초점 방법 m + 테스트 케이스 t (anaconda3-2020.11) cat code.py def<focal method> (anaconda3-2020.11) cat test.py def test(): (anaconda3-2020.11) coverage run -m pytest test.py 그림 2: 코드 커버리지 예측 작업 프롬프트: (i) 시스템 NL 프롬프트는 LLM이 터미널 환경에서처럼 작동하도록 지시합니다. (ii) 커버리지 예측 작업의 0개, 1개 또는 여러 개의 예를 표시할 수 있습니다. (iii) 현재 초점 방법 m과 테스트 케이스 t가 제공됩니다. 5.2 평가 지표 이 섹션에서는 평가 지표를 설명합니다. 방법 m, 테스트 케이스 t, 커버리지 심볼 C = C1, C2,..., Cn의 시퀀스(여기서 ci Є {&gt;,!,-})가 주어지면 모델은 예측된 커버리지 심볼 C Ĉ1, 2,..., ĉn 시퀀스를 생성합니다. 제안하는 접근 방식의 성능을 평가하기 위해 다음 지표를 고려합니다. 제로샷 원샷 멀티샷 모델 매치 OpenAI GPT-4(gpt-4) OpenAI GPT-3.5(gpt-3.5-turbo) Google BARD(text-bison-001) Anthropic Claude(claude-1.3) 3.25.75 84.47 20.39.87 8.81.27 17.84.47 20.Stmt 분기 매치 22.85 90.8.17 76.1.87 86.4.83 83.Stmt 분기 매치 Stmt 분기 22.65 30.04 90.5 22.17.17 11.03 82.29 17.19.63 21.56 85.66 20.19.16 6.88 55.12.Table 2: 코드 커버리지 예측 과제에서 LLM의 성과. 이 표는 기준 진실과 일치하는 예측된 커버리지 시퀀스의 백분율(Match), 명령문에 대한 올바른 커버리지 심볼의 백분율(Stmt), 특히 분기에 대한 백분율(Branch)을 보고합니다. 제로샷, 원샷 및 멀티샷에 대해 수행된 평가. 5.2.1 완벽한 시퀀스 일치 m 완벽한 시퀀스 일치 메트릭은 예측된 시퀀스 Ct가 대상 커버리지 시퀀스 C와 정확히 일치하는 횟수(심볼별)를 계산합니다. 이는 모델이 모든 명령문과 분기에 대해 완벽한 정확도로 커버리지를 예측하는 경우를 나타냅니다. 5.2. 명령문 정확성 명령문 정확성 메트릭은 실행 예측이 올바른 명령문의 백분율을 측정합니다. 이는 대상 시퀀스와 일치하는 예측된 시퀀스의 심볼 백분율과 동일합니다. 5.2. 분기 정확성 분기 정확성 메트릭은 실행 예측이 올바른 분기별 명령문의 백분율을 측정합니다. 분기 정확성은 분기 명령문과 관련된 심볼만 고려합니다. 예측된 시퀀스(분기와 연관됨)에서 대상 시퀀스의 기호와 일치하는 기호의 백분율을 측정합니다.6 결과 표 2는 코드 커버리지 예측 작업에서 다양한 LLM의 성능을 보여줍니다.이 표는 실제 진실과 일치하는 예측된 커버리지 시퀀스의 백분율(Match), 모든 명령문에 대한 올바른 커버리지 기호의 백분율(Stmt), 분기 명령문만 고려할 때 올바른 커버리지 기호의 백분율(Branch)을 보여줍니다.평가 성능은 제로샷, 원샷 및 멀티샷 프롬핑을 사용하여 계산됩니다.OpenAI GPT-4는 이 작업에서 가장 높은 성능을 보이며, 제로샷 프롬핑으로 24.75%의 정확한 일치를 달성하고 멀티샷 프롬핑으로 30%로 향상되며 프롬프트에 최대 6개의 예가 제공됩니다.특히 다른 LLM은 제로샷 프롬핑으로 낮은 정확한 일치(0~4%)를 달성하여 이러한 기초 모델이 훈련 중에 커버리지 로그에 노출되지 않았거나 그렇지 않았을 수 있음을 시사합니다. 두 번째로 성능이 좋은 모델은 Google BARD로, 멀티샷 프롬프팅을 통해 정확한 시퀀스 일치율이 21.5%에 달했습니다. 올바른 커버리지 문장의 비율(Stmt 참조)과 관련하여 대부분의 모델은 프롬프트에 더 많은 예가 포함됨에 따라 개선을 보였습니다. OpenAI GPT-4는 문장 정확성에서 84%에서 90% 사이의 전반적인 최고 점수를 얻었습니다. 분기에 포함된 문장(예: if-else, while)만 고려할 때 올바른 예측이 상당히 떨어지는 것이 분명해졌습니다. 사실, 가장 성능이 좋은 모델인 OpeNAI GPT-4는 프롬프팅에 원샷 및 멀티샷을 사용할 때 이러한 기호의 22%를 정확하게 예측합니다. 분기와 복잡하게 연결된 이 문장 하위 집합은 LLM이 어떤 분기가 커버되는지 결정하는 부울 조건에 대해 추론해야 하기 때문에 평가에 더 큰 과제를 제공한다는 점에 유의하는 것이 중요합니다. 결과적으로 이 컨텍스트에서 커버리지 기호를 정확하게 예측하려면 모델이 프로그램 실행을 안내하는 조건 논리에 대한 심층적인 이해를 가져야 합니다. OpeNAI GPT-4가 코드 커버리지 예측 작업에서 놀라울 정도로 강력한 결과를 냈음에도 불구하고, 이 모델은 여전히 COVERAGEEVAL 데이터 세트의 메서드-테스트 쌍의 70% 이상에 대해 올바른 커버리지를 생성하지 못한다는 점에 유의해야 합니다. 이는 LLM이 코드 실행에 대한 심층적인 이해를 개발하는 데 갈 길이 멀다는 것을 강조합니다. 저희는 코드 생성 결과를 향상시키기 위해 이러한 LLM이 다양한 입력 및 테스트 사례에서 코드 실행에 대한 포괄적인 이해를 얻어야 한다고 믿습니다. 따라서 저희는 데이터 세트와 제안된 작업이 이 목표를 향한 LLM의 발전에 기여할 수 있다고 주장합니다. 7 논의 및 응용 프로그램 코드 커버리지 예측 작업에서 뛰어난 성과를 거두도록 훈련된 LLM은 다양한 시나리오에서 기존 실행 기반 코드 커버리지 측정에 대한 유망한 대안을 제공할 수 있습니다. 이 섹션에서는 이 접근 방식이 귀중하고 유익할 수 있는 여러 사용 사례 시나리오에 대해 설명합니다. 7.1 비용이 많이 드는 빌드 및 실행 수백만 줄의 코드와 수많은 종속성이 있는 대규모 소프트웨어 프로젝트의 경우 빌드 및 실행 프로세스는 시간이 많이 걸리고 비용이 많이 들 수 있습니다. 이러한 경우 개발자는 긴 빌드 단계를 기다리지 않고 새로 작성된 테스트에서 얻은 코드 커버리지를 분석하고자 할 수 있습니다.코드 커버리지 예측 작업에 대해 훈련된 LLM을 활용함으로써 개발자는 전체 프로젝트를 빌드하거나 테스트를 실행할 필요 없이 기존 메서드에서 새 테스트에서 얻은 커버리지를 예측할 수 있습니다.이를 통해 개발자는 메서드에서 놓친 줄이나 분기를 커버하기 위해 추가 테스트가 필요한지 여부를 빠르게 평가하여 귀중한 시간과 리소스를 절약할 수 있습니다.7.2 제한된 코드 가용성 기존 코드 커버리지 계산에는 코드베이스의 전체 소스 코드를 계측 및 실행에 사용할 수 있어야 합니다.그러나 코드의 일부만 액세스할 수 있는 시나리오가 있어 기존 방법을 사용하여 코드 커버리지 계산이 불가능합니다.제한된 코드 가용성이 문제가 되는 경우 코드 커버리지 예측 접근 방식을 사용할 수 있습니다.예를 들어 IDE에서 AI 코드 생성 서비스를 활용할 때 개발자는 코드의 일부만 AI 모델이 있는 서버로 전송할 수 있습니다.이 시나리오에서 서버는 제안된 접근 방식을 사용하여 주어진 메서드에서 AI가 생성한 테스트 사례의 코드 커버리지를 예측할 수 있습니다. 이를 통해 전체 코드베이스가 필요 없이 코드 커버리지를 추정하여 개인 정보 보호 문제와 네트워크 제한 사항을 해결할 수 있습니다. 그런 다음 예측된 코드 커버리지를 사용하여 커버리지가 부족한 경우 추가 테스트를 생성하거나 커버리지가 만족스러운 경우 생성된 테스트를 사용자에게 전송하는 것과 같이 정보에 입각한 결정을 내릴 수 있습니다. 7.3 라이브 커버리지 다양한 IDE에 통합된 라이브 단위 테스트를 통해 개발자는 기존 테스트에 대한 코드 변경의 영향에 대한 실시간 피드백을 받고 새로 추가되거나 수정된 코드가 기존 테스트에서 적용되는지 여부를 식별할 수 있습니다. 이 시나리오에서 코드 커버리지 예측 방식은 테스트 사례의 실제 실행을 AI 추론 호출로 대체하여 수정되거나 새로 추가된 메서드의 커버리지를 예측하여 적용할 수 있습니다. 이를 통해 개발자는 전체 테스트 모음을 실행할 필요 없이 코드 커버리지에 대한 즉각적인 피드백을 받을 수 있습니다. 코드 커버리지 예측을 위해 LLM 기반 모델을 활용함으로써 개발자는 테스트 프로세스를 간소화하고 코드 변경의 커버리지에 대한 적시 통찰력을 얻을 수 있습니다. 8
--- CONCLUSION ---
이 논문에서 우리는 주어진 테스트 케이스에 따라 실행되는 코드 줄을 정확하게 예측하여 코드 실행을 이해하는 데 있어 대규모 언어 모델(LLM)의 역량을 평가하는 것을 목표로 하는 새로운 과제인 코드 커버리지 예측을 소개했습니다. 우리는 HumanEval 데이터 세트에서 파생된 커버리지 주석이 달린 방법으로 구성된 COVERAGEEVAL이라는 포괄적인 데이터 세트를 큐레이션했습니다. 이 데이터 세트를 통해 연구자는 코드 커버리지 예측 기술과 LLM 코드 이해를 탐구하고 발전시킬 수 있습니다. 우리는 코드 커버리지 예측 과제에서 OpenAI의 GPT-4와 GPT3.5, Google의 BARD, Anthropic의 Claude라는 4가지 최첨단 LLM의 성능을 평가했습니다. 결과에 따르면 GPT-4가 가장 높은 성능을 달성했으며, 제로 샷 프롬핑으로 10.46%의 정확한 일치율을 보였고 멀티 샷 프롬핑으로 24.48%의 정확한 일치율을 보였습니다. 그러나 GPT-4를 포함한 어떤 모델도 코드 커버리지 예측에서 높은 정확도를 달성하지 못하여 LLM이 코드 실행에 대한 심층적인 이해를 개발하는 데 아직 갈 길이 멀다는 것을 나타냅니다.코드 커버리지 예측 작업은 코드 이해를 평가하는 데 귀중한 지표 역할을 하며 잠재적으로 코드 관련 작업에서 LLM의 전반적인 성과를 향상시키는 데 기여할 수 있습니다.이 작업에서 뛰어난 성과를 내도록 모델을 훈련함으로써 코드 생성 및 테스트 생성과 같은 작업에 필수적인 코드 실행 역학을 이해하는 능력을 향상시킬 수 있습니다.참고문헌 Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv 사전 인쇄본 arXiv:2001.09977. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 헨리크 폰데 드 올리베이라 핀토, 자렛 카플란, 하리 에드워즈, 유리 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필립 틸렛, 펠리페 페트로스키 수치, 데이브 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 헵겐 거스, 알렉스 니콜, 알렉스 파이노, 니콜라스 테작, 지에 탕, 이고르 바부슈킨, 수치르 발라지, 샨타누 자인, 윌리엄 손더스, 크리스토퍼 헤세, 앤드류 N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 2021. 코드에서 학습된 대규모 언어 모델 평가. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei. 2017. 인간의 선호도에서 심층 강화 학습. 신경 정보 처리 시스템의 발전, 30. Marko Ivanković, Goran Petrović, René Just, Gordon Fraser. 2019. Google에서 코드 커버리지. 2019년 27회 ACM 유럽 소프트웨어 엔지니어링 컨퍼런스 및 소프트웨어 엔지니어링 기초 심포지엄 회의록, 955-963쪽. Leslie A Johnson. 1998. Do-178b. 항공 시스템 및 장비 인증의 소프트웨어 고려 사항, Crosstalk Magazine. Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, Nan Duan. 2023. 사전 학습된 언어 모델을 사용한 코드 실행. arXiv 사전 인쇄본 arXiv:2305.05383. Microsoft. 2023. Coverage-eval. https://github. com/microsoft/coverage-eval. OpenAI. 2023. Gpt-4 기술 보고서. Rob Palin, David Ward, Ibrahim Habli, Roger Rivett. 2011. Iso 26262 안전 사례: 규정 준수 및 보증. Leanna Rierson. 2017. 안전이 중요한 소프트웨어 개발: 항공 소프트웨어 및 DO-178C 규정 준수를 위한 실용 가이드. CRC 프레스. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du 등 2022. Lamda: 대화 상자 애플리케이션을 위한 언어 모델. arXiv 사전 인쇄 arXiv:2201.08239. 무스타파 M 티키르(Mustafa M Tikir)와 제프리 K 홀링스워스(Jeffrey K Hollingsworth). 2002. 코드 범위 테스트를 위한 효율적인 계측. ACM SIGSOFT 소프트웨어 엔지니어링 노트, 27(4):86-96. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 5998-6008페이지.커버리지 평가 예제 문제: rounded_avg def rounded_avg(n, m): ID =&quot;&quot;&quot;SYNTH 두 개의 양의 정수 n과 m이 주어지고, n에서 m까지의 정수(n과 m 포함)의 평균을 계산하는 것이 과제입니다. 답을 가장 가까운 정수로 반올림하고 이를 이진수로 변환합니다. n이 m보다 크면 -1을 반환합니다. 예제: rounded_avg(1, 5) =&gt; &quot;0b11&quot; rounded_avg(7, 5) =&gt; -rounded_avg(10, 20) =&gt; &quot;0b1111&quot; rounded_avg(20, 33) =&gt; &quot;0b11010&quot; if m <n: return -summation =for i in range (n, m+1): summation += i return bin(round (summation/(m - n + 1))) def test_658(): Test Cases assert rounded_avg(185,546) == "0b101101110" assert True def test_659(): assert rounded_avg(362,496) == "0b110101101" assert True def test_660(): assert rounded_avg(560,851) == "0b1011000010" assert True Coverage-Annotated Method {test_id = 660} def rounded_avg(n, m): if mn: return -summation =for i in range(n, m+1): summation += i return bin(round (summation/(m (n + 1))) Coverage Sequence {test_id = 660} Figure 3: Example record from the COVERAGEEVAL dataset. This record is for the rounded_avg problem. We have shown 3 of the unit tests, as well as sample coverage annotation data from one unit test. where > 실행된 문장, 누락된 문장, 도달할 수 없는 코드를 나타냅니다.B 배포된 시스템 이 논문에서 설명한 일부 사용 사례를 포함하는 두 가지 시스템에 접근 방식을 배포합니다.B.1 시스템 A - 라이브 커버리지 그림 4는 개발자에게 IDE에 직접 라이브 커버리지 예측을 제공하는 시스템 A의 배포를 보여줍니다.시스템 A는 개발자가 코드베이스에서 주어진 메서드(예: 피보나치(n))에 대한 테스트를 작성하는 시나리오를 지원합니다.시스템 A는 라이브 커버리지 정보(그림 4 하단)를 제공하며, 테스트에서 다룬 줄은 &gt;로 표시되고 녹색으로 강조 표시되고, 누락된 줄은 빨간색으로 표시되고 강조 표시됩니다.시스템 A가 제공하는 이점은 다음과 같습니다.(i) 전체 코드베이스를 빌드할 필요가 없습니다.(ii) 테스트를 실행할 필요가 없습니다.(iii) 라이브 및 가벼운 커버리지 예측입니다.B.2 시스템 B - 커버리지를 포함한 테스트 생성 그림 5는 테스트 모음에 커버리지 보장을 제공하는 시스템 B의 배포를 보여줍니다. 시스템 B는 개발자가 주어진 메서드에 대한 테스트 케이스를 요청하고 테스트 중인 메서드에 대한 특정 수준의 커버리지를 얻고자 하는 시나리오를 지원합니다. 메서드가 테스트 생성 서비스로 전송되면 테스트 생성 모델(즉, AI 기반 테스트 생성 도구 또는 기타 도구)은 첫 번째 테스트 케이스 후보 배치를 출력합니다. 커버리지 예측 모델은 이러한 테스트와 테스트 중인 메서드를 분석하고 이러한 테스트가 메서드에서 달성하는 커버리지를 예측합니다. 커버리지가 만족스러운 경우(주어진 기준 및 임계값과 관련하여) 테스트는 IDE로 전송되어 개발자에게 표시됩니다. 테스트가 커버리지 측면에서 기준을 충족하지 못하는 경우 테스트 생성 서비스는 테스트 생성 모델에 추가 테스트를 요청합니다(선택적으로 여전히 커버해야 하는 특정 라인/브랜치 제공). 시스템 B가 제공하는 이점은 다음과 같습니다. (i) 커버리지 보장이 있는 자동화된 테스트 생성, (ii) 사용자 측에서 빌드 및 테스트 실행이 필요 없는 가벼운 생성. 사용자 공간 IDE 메서드 def Fibonacci(n): if n &lt; 0: print(&quot;잘못된 입력&quot;) elif n == 0: returnelif n = 1 or n == 2:returnelse: 테스트 IDE return Fibonacci(n-1) + Fibonacci(n-2) def test_Fibonacci_1(self): self.assertEqual (Fibonacci(1), 1) def test_Fibonacci_2(self): self.assertEqual(Fibonacci(2), 1) def test_Fibonacci_3(self): self.assertEqual (Fibonacci(3), 2) 메서드 def Fibonacci(n): if n &lt; 0: print(&quot;잘못된 입력&quot;) elif n == 0: returnelif n == 1 or n == 2: returnelse: return Fibonacci(n-1) + Fibonacci(n-2) -MethodTests 서버 공간 커버리지 예측 서비스 그림 4: 시스템 A - 라이브 커버리지 커버리지 예측 모델 커버리지 주석이 달린 방법 사용자 공간 IDE 방법 IDEdef Fibonacci(n): if n &lt; 0: print(&quot;잘못된 입력&quot;) elif n == 0: returnelif n = 1 or n == 2: returnelse: return Fibonacci(n-1) + Fibonacci(n-2) 생성된 테스트 def test_Fibonacci_neg(self): self.assertEqual(Fibonacci(-1), None) def test_Fibonacci_(self): self.assertEqual(Fibonacci(0), 0) def test_Fibonacci_1(self): 테스트할 방법 테스트 생성 서비스 서버 공간 테스트 생성 모델 생성된 테스트 def test_Fibonacci_1(self): self.assertEqual(Fibonacci(1), 1) def test_Fibonacci_2(self): self.assertEqual(Fibonacci(2), 1) def test_Fibonacci_3(self): self.assertEqual(Fibonacci(3), 2) 추가 테스트 요청 범위 예측 모델 self.assertEqual (Fibonacci(1), 1) 생성된 테스트 def test_Fibonacci_2(self): YES ΝΟ self.assertEqual (Fibonacci(2), 1) 만족스러운 범위? def test_Fibonacci_3(self): self.assertEqual (Fibonacci(3), 2) 그림 5: 시스템 B - 범위가 있는 테스트 생성
