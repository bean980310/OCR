--- ABSTRACT ---
대규모 언어 모델(LLM)은 상당한 양의 학습 데이터를 기억하는 것으로 알려져 있습니다. 이 기억된 콘텐츠의 일부는 모델을 쿼리하기만 하면 추출할 수 있는 것으로 나타났으며, 이는 개인정보 보호 위험을 초래합니다. 우리는 prompttuning을 사용하여 LLM에서 기억된 콘텐츠의 추출률을 제어하는 새로운 접근 방식을 제시합니다. 우리는 추출률을 증가시키고 감소시키는 두 가지 prompt 학습 전략을 제시하는데, 이는 각각 공격과 방어에 해당합니다. 우리는 공개 벤치마크에서 GPT-Neo 패밀리의 모델을 사용하여 기술의 효과를 보여줍니다. 1.3B 매개변수 GPTNeo 모델의 경우, 우리의 공격은 기준선에 비해 추출률을 9.3% 포인트 증가시킵니다. 우리의 방어는 사용자가 지정한 하이퍼파라미터에 의해 다양한 개인정보 보호-유틸리티 트레이드오프를 달성하도록 조정할 수 있습니다. 우리는 기준선에 비해 최대 97.7%의 추출률 감소를 달성했으며, 16.9%의 복잡도 증가를 달성했습니다. 1
--- INTRODUCTION ---
사전 학습된 대규모 언어 모델(LLM, Devlin 등, 2019; Radford 등, 2019; Raffel 등, 2020; Soltan 등, 2022)은 일반적으로 대규모 크라우드 소싱 코퍼스에서 학습되며, 여러 다운스트림 NLU 작업에서 최첨단 모델의 백본으로 사용되기 때문에 최근에 많은 관심을 받았습니다. 그러나 이러한 모델은 적절하게 제작된 프롬프트를 사용하여 추출할 수 있는 상당한 부분의 학습 데이터를 기억하는 것으로 나타났습니다(Carlini 등, 2020, 2022; Zhang 등, 2021). 이러한 추출은 학습 데이터 제공자에게 개인 정보 위험을 초래합니다. 이러한 맥락에서 개발자가 LLM에서 기억된 예제의 추출 가능성을 제어할 수 있도록 하는 방법은 매우 가치가 있습니다. 예를 들어, * 저자가 Amazon에서 인턴으로 근무하는 동안 수행한 작업; mustafa.ozdayi@utdallas.edu + perisc@amazon.com 추출률을 높이는 방법은 적대적 환경에서의 공격에 해당하며, 개발자에게 개인 정보 위험을 분석할 수 있는 기능을 제공합니다. 추출률을 낮추는 방법(방어라고 함)은 이러한 공격으로부터 보호하는 데 유용합니다. 역사적으로 방어 방법은 컴퓨팅 집약적인 경향이 있습니다(Abadi et al., 2016; Dupuy et al., 2021). 이 작업에서 우리는 연속 소프트 프롬프트(Lester et al. 2021; 이하 간단히 프롬프트라고 함)를 훈련하고 이를 외부 신호를 LLM으로 전달하는 방법으로 활용하여 기억된 데이터의 추출을 제어합니다. 우리는 모델 가중치를 동결하고 훈련된 프롬프트만 사용하여 생성을 제어합니다. 먼저, 우리는 공격 환경에서 프롬프트를 훈련하고 모델에서 추출 가능한 기억된 콘텐츠의 범위를 연구합니다. 둘째, 사용자가 지정한 하이퍼파라미터를 통해 추출률을 낮추고 다양한 프라이버시-유틸리티 트레이드오프를 달성하는 프롬프트를 만드는 방어 설정을 탐구합니다. 원래 모델 가중치는 두 설정 모두에서 고정되므로, 우리의 방법은 전반적으로 계산 효율적입니다. 우리가 아는 한, 우리의 작업은 LLM에서 프라이버시를 분석하고 완화하기 위해 교육적 프롬프트를 사용하는 것을 처음으로 적용한 것입니다. 우리는 실험을 위해 개발한 코드를 공개했습니다¹. 2 배경 및
--- RELATED WORK ---
이전 연구에서는 LLM이 기억력을 보이는 것으로 나타났으며 다양한
--- METHOD ---
개발자가 LLM에서 암기된 예시의 추출 가능성을 제어할 수 있게 해주는 s는 매우 가치가 있습니다. 예를 들어, * 저자가 Amazon에서 인턴으로 일할 때 수행한 작업; mustafa.ozdayi@utdallas.edu + perisc@amazon.com 추출률을 높이는 방법은 적대적 환경에서의 공격에 해당하며 개발자에게 개인 정보 위험을 분석할 수 있는 기능을 제공합니다. 방어라고 하는 추출률을 낮추는 방법은 이러한 공격으로부터 보호하는 데 유용합니다. 역사적으로 방어 방법은 컴퓨팅 집약적인 경향이 있습니다(Abadi et al., 2016; Dupuy et al., 2021). 이 작업에서 우리는 연속 소프트 프롬프트(Lester et al. 2021; 이하 간단히 프롬프트라고 함)를 훈련하고 이를 외부 신호를 LLM으로 전달하는 방법으로 활용하여 암기된 데이터의 추출을 제어합니다. 우리는 모델 가중치를 동결하고 훈련된 프롬프트만 사용하여 생성을 제어합니다. 첫째, 공격 설정에서 프롬프트를 훈련하고 모델에서 추출 가능한 기억된 콘텐츠의 범위를 연구합니다. 둘째, 사용자가 지정한 하이퍼파라미터를 통해 추출률을 줄이고 다양한 개인 정보 보호-유틸리티 트레이드오프를 달성하는 프롬프트를 만드는 방어 설정을 탐색합니다. 원래 모델 가중치가 두 설정 모두에서 고정되므로, 우리의 방법은 전반적으로 계산 효율적입니다. 우리가 아는 한, 우리의 작업은 LLM에서 개인 정보 보호의 분석 및 완화를 위해 지침적 프롬프트의 사용을 적용한 최초의 작업입니다. 우리는 우리의
--- EXPERIMENT ---
s¹. 2 배경 및 관련 연구 이전 연구에서는 LLM이 암기력을 보인다는 사실을 보여주었고 추출 가능성을 정량화하는 다양한 방법을 탐구했습니다(Carlini et al., 2018, 2020, 2022). 차등적 비밀 보호 훈련(Dwork, 2006; Abadi et al., 2016)은 이 위험을 완화하는 데 사용된 인기 있는 방법입니다. 그러나 모델 유용성을 떨어뜨리는 경향이 있으며 LLM을 재교육해야 하는데, 이는 무거운 계산 부담으로 인해 실행 불가능할 수 있습니다. &#39;https://github.com/amazon-science/controlling-llmmemorization 입력 시퀀스: 훈련/테스트 설정 평가 소프트 프롬프트 업데이트 접두사 접미사 빠른 갈색 여우가 게으른 개를 뛰어넘었습니다. 소프트 프롬프트 접두사 임베딩 모델(고정) 생성된 출력 측정값과 비교 I 훈련하는 경우: 테스트하는 경우: I 손실 계산 | 정확한 일치 및 부분 일치 접미사 생성 출력(예) 정확한 일치 게으른 개 위의 게으른 개 위의 게으른 개 위의 작은 개 위의 0.부분 일치 그림 1: 설정의 개략도.위 섹션은 훈련 및 테스트 설정을 보여주고 아래 섹션은 평가 메트릭을 보여줍니다.언어 모델에 대한 지시적 프롬프트의 사용은 사전 훈련(Raffel et al., 2020), 훈련의 두 번째 단계(Sanh et al., 2022; Wei et al., 2021) 및 모델 출력을 안내하기 위한 추론 중(Brown et al., 2020)을 포함하여 광범위하게 연구되었습니다. 세 번째 범주 내에서 수동 프롬프트 엔지니어링을 개선하기 위해 연구자들은 개별 자연어 프롬프트를 학습(Shin et al., 2020), 이를 마이닝(Jiang et al., 2020)하거나 자연어를 무시하고 연속 프롬프트를 학습(Li and Liang, 2021; Lester et al., 2021)하는 방법을 구현했습니다. 저희의 작업은 연속 프롬프트를 활용하여 외부 신호를 모델에 전달하여 원하는 모델 동작(즉, 개방형 언어 생성에서 더 적거나 더 많은 기억된 데이터, 각각 추출 공격 및 방어에 매핑됨)을 트리거하는 방법입니다. 3 방법 프롬프트 튜닝에는 프롬프트를 접두사 임베딩에 추가하고 훈련 손실에 액세스해야 합니다(그림 1 참조). 이러한 제약 조건을 고려하여 적이 대상 모델 매개변수에 액세스할 수 있는 화이트박스 공격과 적이 API를 통해 대상 모델과 상호 작용하는 블랙박스 방어를 탐색합니다. 따라서 저희는 저희의 공격에 대한 방어를 테스트하지 않습니다. [접두사 || 영어: suffix]는 접두사가 길이 k 토큰인 훈련 세트의 시퀀스입니다.Carlini et al. (2022)은 모델이 해당 길이 k 접두사로 프롬프트된 후 접미사를 정확하게 생성하는 경우 접미사를 k-추출 가능한 것으로 정의했습니다.화이트박스 공격은 k-추출 가능한 시퀀스의 수를 늘리는 것을 목표로 하는 반면, 블랙박스 방어는 API를 통해 접두사를 제출하는 공격자가 추출할 수 있는 k-추출 가능한 시퀀스의 수를 줄이는 것을 목표로 합니다.3.1 공격 공격 설정에서 공격자가 대상 모델의 훈련 세트에서 샘플링한 [prefix || suffix ] 시퀀스 Strain 세트를 가지고 있다고 가정합니다.그들의 목표는 Stest²로 표시되는 분리된 접두사 세트에 해당하는 접미사를 추출하는 것입니다. 이를 위해 적대자는 먼저 프롬프트를 초기화합니다. 프롬프트는 1 × e 매개변수의 연속적인 집합이며, 여기서 e는 모델의 임베딩 크기이고 1은 적대자가 결정한 하이퍼 매개변수입니다. 프롬프트는 Strain을 통해 학습되어 접미사가 올바르게 생성되도록 합니다. 이를 위해 먼저 프롬프트를 접두사의 임베딩에 추가하고 공동 임베딩을 모델에 전달하여 생성합니다. 그런 다음 모델의 매개변수를 고정한 채 프롬프트에 대한 손실 목표(아래 참조)를 최소화합니다. 두 가지 손실 목표를 탐색합니다. 첫 번째는 인과 언어 모델링(이하 CLM)으로, 전체 시퀀스에서 교차 엔트로피 손실을 최소화합니다(Radford et al., 2019). 두 번째에서는 접두사가 주어진 경우 접미사의 교차 엔트로피 손실만 최소화하여 프롬프트를 최적화합니다. 여기에서 학습은 추론 작업과 일치하여 학습하는 동안 모델은 접미사 토큰에 대해서만 페널티를 받습니다.따라서 이를 정렬된 CLM이라고 합니다.추론 동안 학습된 프롬프트는 Stest의 각 접두사 임베딩에 추가되고 공동 임베딩은 생성을 위해 모델에 전달됩니다(그림 1 참조).3.2 방어 방어 설정에서 방어자(API 소유자)는 프롬프트를 학습하고 모델에 전달하기 전에 들어오는 접두사에 추가합니다.저희 알고리즘은 머신 언러닝 문헌(Halimi et al., 2022)과 멤버십 추론 및 백도어 공격에 대한 방어(Chen et al., 2022; Ozdayi et al., 2021)에서 영감을 받았습니다.저희는 2단순화를 위해 모든 접두사가 k 길이라고 가정합니다.실제 설정에서 필요한 경우 길이가 다른 접두사를 패딩하거나 잘라내면 쉽게 보장할 수 있습니다. 영어: 0으로 표시된 학습 임계값이라는 하이퍼파라미터. 프롬프트 학습(섹션 3.1 참조) 중에 손실이 0보다 작으면 프롬프트에 페널티를 주기 위해 경사 상승을 수행합니다. 손실이 0보다 크면 평소처럼 프롬프트에 대한 경사 하강을 수행합니다. 평균 에포크 손실이 0과 같거나 0 이상이 되면 학습이 중지됩니다. 이를 통해 제어된 방식으로 학습 손실을 늘리고 0 주변에서 안정화할 수 있습니다. 이 프로세스를 통해 모델의 어떤 부분도 다시 학습하지 않고도 다양한 개인 정보 보호-유용성 트레이드오프를 효율적으로 달성할 수 있습니다. 0을 탐색하기 위해 초기 값을 모델 학습 손실보다 약간 높게 설정하고 원하는 성능이 달성될 때까지 0.25씩 증가시킵니다. 4 실험 실험을 위해 GPT-Neo 모델의 125M 및 1.3B 매개변수 변형을 사용합니다(Black et al., 2021). 이것들은 CLM을 사용하여 Pile 데이터 세트(Gao et al., 2020)에서 학습된 공개적이고 디코더 전용 변환기 모델(Vaswani et al., 2017)입니다. 우리는 언어 모델 추출 벤치마크 데이터 세트(Google-Research)에서 Strain과 Stest를 추출합니다. 이 데이터 세트에는 각 시퀀스가 접두사와 접미사로 분할된 Pile의 학습 분할에서 샘플링된 15k 시퀀스가 포함되어 있습니다. 기본 평가 설정에서 접두사와 접미사는 모두 50개의 토큰으로 구성됩니다. 우리는 14k/1k 샘플의 무작위 학습/테스트 분할을 보장합니다. 우리가 선택한 평가 지표는 정확한 추출률로, 테스트 세트에서 올바르게 생성된 접미사(즉, 생성된 접미사의 모든 토큰이 실제 접미사와 일치)의 일부입니다. 우리는 또한 부분 추출률에 대해 논의하고 부록 A에 결과를 제시합니다. 기준으로 Carlini et al.에서 분석한 공격을 사용합니다. (2022), 모델에 접두사를 공급하고 탐욕적 디코딩으로 접미사를 생성하는 것으로 구성됩니다. 이것은 우리의 작업 외에 우리가 아는 한 이 설정에 대한 유일한 추출 공격입니다. 우리의 훈련 설정은 부록 B에서 설명합니다. 모든 실험은 각 실행에서 새로운 무작위 훈련/테스트 분할로 5회 실행에 걸쳐 반복됩니다. 4.1 공격 우리는 프롬프트 길이, 접미사 크기, 접두사 크기 및 빔 크기 등 여러 차원에서 공격의 성능을 탐구합니다. 빔 크기 실험을 제외한 모든 경우에 탐욕적 디코딩을 사용합니다. 프롬프트 길이 먼저 기본 설정(접두사와 접미사는 50개 토큰으로 구성됨, 그림 2-A1 및 2-A2)의 맥락에서 프롬프트 길이를 탐구합니다. CLM과 정렬된 CLM으로 조정된 프롬프트는 모든 경우에서 기준선보다 개선되었으며, 정렬된 CLM이 가장 우수한 성능을 제공함을 알 수 있습니다. 이를 감안하여 방어를 포함한 다른 모든 실험에 대해 정렬된 CLM 목표를 사용하여 프롬프트를 훈련합니다. 정렬된 CLM을 사용하여 100개 토큰 프롬프트(파란색 선)에서 각각 125M 및 1.3B 모델에 대해 25.8% 및 54.3%의 가장 높은 추출률을 달성했습니다(각각 8.9 및 9.3% 포인트 향상). 프롬프트 길이에 따라 추출률이 증가하고 프롬프트 길이가 100이 넘으면 포화되는 경향이 있음을 관찰했습니다. 훈련 중에 테스트 손실이 증가하지 않으므로 과적합은 포화의 잠재적 원인으로 배제되었습니다. 이는 우리의 목표를 감안할 때 추출 목적에 가치를 더할 수 있는 프롬프트의 매개변수 수에 최대 한도가 있음을 시사합니다. 보다 정교한 훈련 전략(더 나은 손실 함수 설계, 더 나은 프롬프트 초기화 등)이 더 나은 추출률을 낼 수 있음을 알 수 있습니다. 접미사 크기 다음으로 접두사 크기를 50으로 고정하고 접미사 크기를 변경합니다. 그림 2-Band 2-B2에서 볼 수 있듯이 추출률은 접미사 크기에 따라 대략 기하급수적으로 감소합니다. 접미사 크기가 증가함에 따라 더 긴 프롬프트(≥ 20)가 기준선보다 더 큰 개선을 제공한다는 점에 유의합니다. 예를 들어, 1.3B 모델을 사용하여 프롬프트 길이가 100(파란색 선)인 경우 접미사 크기 5에서 추출률이 5.3퍼센트 포인트 증가하는 것을 관찰합니다. 반면 접미사 크기가 50이면 증가율이 9.3퍼센트 포인트입니다. 접두사 크기 다음으로 접미사 크기를 50으로 고정하고 접두사 크기를 변경합니다. 그림 2-C 및 2-C2에서 볼 수 있듯이 추출률은 대략 대수적으로 증가합니다(Carlini et al. 2022 참조). 접미사 크기와 달리 기준선과 공격 간의 격차는 접두사 크기가 증가함에 따라 줄어드는 것을 관찰합니다. 이는 기준선과 비교했을 때 공격이 덜 정보가 많은 적대자(작은 접두사 크기)에게 유리할 수 있음을 시사합니다. 빔 디코딩 마지막으로, 접두사와 접미사 크기를 50개 토큰으로 기본 설정을 활용하고 빔 크기를 다양하게 변경합니다(빔 크기=1은 탐욕적 디코딩에 해당). 결과는 그림 2-D1과 2-D2에 나와 있습니다. 빔 크기를 1에서 5로 늘리면 추출 속도가 전반적으로 증가하는 것을 관찰했습니다. 그러나 빔 크기가 5보다 클 경우 개선이 정체되거나 진동하는 경향이 있습니다. 1.3B 모델은 더 많은 이점을 얻습니다 정확한 추출 속도 정확한 추출 속도 GPT-Neo-125M 기준선 0.BCCLM D-정렬된 CLM 0.0.0.0.0.0.0.0.0.0.0.0.0.기준선 CLM O 정렬된 CLM 0.0.0.0.0.0.0.0.0.0.0.0.1005 프롬프트 길이 0.0.° ° ° 0.° 0.0.0.GPT-Neo-1.3BBCD0.0.° 0.0.55 0° ° ° 0.0.° 0.0.45-0.0.4025 50 75 100 접두사 크기 110 15 빔 크기 접미사 크기 기준 공격 프롬프트 길이=프롬프트 길이=-о 프롬프트 길이=프롬프트 길이=프롬프트 길이=프롬프트 길이=그림 2: 프롬프트 길이(2-A1, 2-A2), 접미사 크기(2-B1, 2-B2), 접두사 크기(2-C1, 2-C2) 및 빔 크기(2-D1, 2-D2)에 대한 정확한 추출률의 변화. 상단 패널은 GPT-Neo-125M 결과를 보여주고 하단 패널은 GPT-Neo-1.3B 결과를 보여줍니다. 각 선 주변의 투명한 다각형은 지점 전체에서 95% 신뢰 구간을 나타냅니다. 모델 Ꮎ 정확한 추출률 0* GPT-Neo 125M 1.1.1.0.169 ± 0.0.031 ± 0.0.006 ± 0.0.001 ± 0.GPT0.004 ± 0.124M 0* GPT-Neo 0.1.3B 0.0.450±0.0.108 ± 0.0.022±0.0.01 ± 0.GPT1.5B 0.019 0.파일 테스트 PPL 15.71 ± 0.16.601 ± 0.17.499 ± 0.19.691 ± 0.30.3231.9.213±0.9.758 ± 0.10.267 ± 0.10.775 ± 0.17.155 ± 0.표 1: 다양한 0 값에 따른 방어 설정에 대한 정확한 추출률과 해당 복잡도. 값은 평균 ± 표준 편차로 보고됩니다. 비슷한 크기의 해당 GPT2 변형보다 작은 추출률은 복잡도 값이 작을 때 달성되므로 양호합니다. (*방어 없음). 빔 크기를 늘려 빔 크기 20(프롬프트 길이 150)에서 최고 추출률 61.4%를 달성했습니다. 125M 모델에서 달성한 최고 추출률은 빔 크기 15(프롬프트 길이 100)에서 28.3%였습니다. 4.2 방어 마지막으로 블랙박스 방어의 개인 정보 보호-유틸리티 균형을 평가합니다. 섹션 3에서 언급했듯이, 방어는 블랙박스 적대자를 위해 설계되었으며 화이트박스 공격에 대해 테스트할 수 없습니다. 따라서 기준 공격(섹션 4)을 활용하여 개인 정보를 정량화합니다. 방어 설정에서 더 긴 프롬프트가 가치를 더하지 않는다는 점에 유의하여 길이가 1인 프롬프트를 사용합니다. 생성된 접미사에 퍼플렉시티(PPL)를 활용하여 섹션 3.1에서와 같이 정확한 추출률을 사용하는 것 외에도 모델의 유용성을 정량화합니다. PPL을 측정하기 위해 Pile의 테스트 분할에서 샘플링한 1k 시퀀스의 임의 하위 집합을 사용하여 모델에서 보지 못한 데이터에서 PPL이 측정되도록 합니다. 또한 Pile 데이터 세트(GPT2 모델)에서 훈련되지 않은 비슷한 크기의 모델과 메트릭을 비교합니다. 여기서 우리의 전제는 비슷한 크기의 도메인 외부 모델과 비교했을 때 개인 정보 보호 및 유용성 측면에서 더 나은 성능이 우리의 방어 메커니즘이 API 소유자에게 가치가 있다는 것을 의미한다는 것입니다. 표 1에서 기본 평가 설정(접두사와 접미사는 50개 토큰으로 구성)을 사용하여 얻은 결과를 표시합니다. 우리의 방어는 경쟁력 있는 PPL 값으로 더 낮은 추출률을 달성합니다. 125M 모델의 경우, 우리는 0:1.75에서 PPL이 25.3% 증가하면서 기준선에 비해 99.4%의 정확한 추출률 감소를 달성했습니다. 1.3B 모델의 경우, 0=1에서 PPL이 16.9% 증가하면서 기준선에 비해 추출률이 97.7% 감소했습니다. 해당 크기의 GPT2 모델과 비교했을 때 더 낮은 PPL 값으로 더 낮은 추출률을 달성할 수 있는 능력은 우리의 방어가 효과적이라는 증거를 제공합니다. = 5
--- CONCLUSION ---
우리는 개방형 언어 생성 작업에서 LLM의 암기된 데이터의 추출 가능성을 제어하기 위해 프롬프트 튜닝을 활용하는 최초의 알려진 노력을 제시합니다. 우리는 새로운 데이터 추출 공격 및 방어를 개발하고 다양한 설정에서 그 성능을 설명합니다. 우리의 공격은 정확한 추출 속도 측면에서 기준선을 지속적으로 능가합니다. 우리의 방어는 경쟁력 있는 개인 정보 보호-유틸리티 트레이드오프를 제공하며 민감한 콘텐츠에 대해 학습된 모델을 사용하는 API 소유자에게 유익할 것입니다. 이러한 결과는 원래 모델 가중치를 변경하지 않고 효율적으로 달성됩니다. 부록 C 6에서 향후 작업의 방향을 자세히 설명합니다. 한계 우리는 작업의 몇 가지 한계를 간략히 언급합니다. 첫째, 우리는 실험에서 단일 데이터 세트와 단일 모델 패밀리만 사용했습니다. 이는 주로 우리가 사용하는 벤치마크가 우리가 아는 한 현재 유일하게 공개적으로 사용 가능한 데이터 세트이기 때문입니다. 또한 추출 메트릭에만 초점을 맞추었지만 추출된 시퀀스에 대한 심층 분석은 수행하지 않았습니다. 추출된 시퀀스에 대한 세부적인 분석을 통해 LLM의 암기 및 추출을 이해하는 데 중요한 통찰력을 얻을 수 있습니다. 마찬가지로, 우리는 프롬프트가 무엇으로 수렴하는지, 그리고 수렴 시점에 설명 가능한 프롬프트를 생성하는지 분석하지 않았습니다. 이러한 분석은 예를 들어 정렬된 CLM이 있는 훈련 프롬프트가 기본 CLM 설정보다 더 나은 성능을 보이는 이유에 대한 더 나은 통찰력을 제공할 수 있습니다. 마지막으로, 다운스트림 작업에서 다른 유틸리티 메트릭(예: 정확도)을 측정하여 방어 평가를 더욱 개선할 수 있다고 생각합니다. 7 윤리적 고려 사항 우리는 프롬프트 튜닝을 활용하여 개방형 언어 생성 작업에서 LLM에서 암기된 데이터의 추출 가능성을 제어하고 두 가지 설정, 즉 공격과 방어를 탐구합니다. 우리는 공격 방법론이 화이트박스 액세스 권한이 있는 적대자가 대상 대규모 언어 모델에서 암기된 개인 정보를 추출하는 데 오용될 수 있음을 인정합니다. 우리의 목표는 커뮤니티에서 이러한 공격의 가능성과 심각성에 대한 인식을 높이는 것입니다. 이러한 지식을 갖춘 개발자가 관련 방어 메커니즘을 사용하여 이러한 잠재적 오용을 피할 수 있기를 바랍니다. 감사의 말 저자는 이 주제에 대한 유익한 토론을 해준 Wael Hamza와 실험에 필요한 GPU 인스턴스를 확보하는 데 도움을 준 Stephen Rawls에게 감사드리고자 합니다. 참고문헌 Huggingface가 가속화됩니다. Martín Abadi, Andy Chu, Ian J. Goodfellow, HB McMahan, Ilya Mironov, Kunal Talwar, Li Zhang. 2016. 차등 개인 정보 보호를 통한 딥 러닝. 2016 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman. 2021. GPT-Neo: MeshTensorflow를 사용한 대규모 자기 회귀 언어 모델링. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, Chiyuan Zhang. 2022. 신경 언어 모델에서 암기 정량화. ArXiv, abs/2202.07646. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Xiaodong Song. 2018. 비밀 공유자: 신경망에서 의도치 않은 암기 평가 및 테스트. USENIX 보안 심포지엄에서. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, Úlfar Erlingsson, Alina Oprea, Colin Raffel. 2020. 대규모 언어 모델에서 학습 데이터 추출. USENIX 보안 심포지엄에서. Dingfan Chen, Ning Yu, and Mario Fritz. 2022. Relaxloss: 유틸리티를 잃지 않고 멤버십 추론 공격 방어. ArXiv, abs/2207.05801. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학 협회. Christophe Dupuy, Radhika Arava, Rahul Gupta, and Anna Rumshisky. 2021. 대규모 nlu 모델을 위한 효율적인 dp-sgd 메커니즘. ICASSP2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 4118-4122쪽. Cynthia Dwork. 2006. Differential privacy. Encyclopedia of Cryptography and Security에서. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy. 2020. The pile: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트. ArXiv, abs/2101.00027. Google-Research. benchmark. Google-research/lm-extraction Anisa Halimi, Swanand Kadhe, Ambrish Rawat, Nathalie Baracaldo. 2022. Federated unlearning: fl에서 클라이언트를 효율적으로 지우는 방법? Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen. 2021. Lora: 대규모 언어 모델의 저순위 적응. ArXiv, abs/2106.09685. Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig. 2020. 언어 모델이 무엇을 알고 있는지 어떻게 알 수 있을까? Transactions of the Association for Computational Linguistics, 8:423-438. Diederik P. Kingma, Jimmy Ba. 2014. Adam: 확률적 최적화 방법. CORR, abs/1412.6980. Brian Lester, Rami Al-Rfou, Noah Constant. 2021. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. 2021 자연어 처리 경험적 방법 컨퍼런스의 회의록, 3045-3059페이지, 온라인 및 도미니카 공화국 푼타카나. 전산 언어학 협회. Xiang Lisa Li와 Percy Liang. 2021. 접두사 튜닝: 생성을 위한 연속 프롬프트 최적화. 전산 언어학 협회의 제59회 연례 회의 및 자연어 처리에 대한 제11회 국제 공동 컨퍼런스(제1권: 장문 논문)의 회의록, 4582-4597페이지, 온라인. 전산 언어학 협회. Jimit Majmudar, Christophe Dupuy, Charith S. Peris, Sami Smaili, Rahul Gupta, Richard S. Zemel. 2022. 대규모 언어 모델에서의 차별적 개인 정보 보호 디코딩. ArXiv, abs/2205.13621. Mustafa Safa Ozdayi, Murat Kantarcioglu, Yulia R. Gel. 2021. 강력한 학습률을 통한 연합 학습에서 백도어 방어. AAAI 인공지능 컨퍼런스 회의록, 35(10):9268-9276. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala. 2019. Pytorch: 명령형 스타일, 고성능 딥 러닝 라이브러리. 신경 정보 처리 시스템의 발전 32, 8024-8035페이지. Curran Associates, Inc. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 2019. 언어 모델은 비지도 멀티태스크 학습자입니다. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21(1). Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He. 2020. Deepspeed: 시스템 최적화를 통해 1,000억 개 이상의 매개변수를 사용하여 딥 러닝 모델을 학습할 수 있습니다. 제26회 ACM SIGKDD 국제 지식 발견 및 데이터 마이닝 컨퍼런스 회의록. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, 김태운, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf 및 Alexander M Rush. 2022. 멀티태스크 프롬프트 학습은 제로샷 태스크 일반화를 가능하게 합니다. International Conference on Learning Representations에서. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh. 2020. AutoPrompt: 자동 생성된 프롬프트를 사용하여 언어 모델에서 지식 도출. Empirical Methods in Natural Language Processing(EMNLP)에서. Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan. 2022. Alexatm 20b: 대규모 다국어 seq2seq 모델을 사용한 Few-shot 학습. arXiv. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. ArXiv, ABS/1706.03762. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner 및 Sameer Singh. 2019. NLP 공격 및 분석을 위한 보편적인 적대적 트리거. 자연어 처리의 경험적 방법에 관한 컨퍼런스에서. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai 및 Quoc V. Le. 2021. 미세 조정된 언어 모델은 제로샷 학습자입니다. Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini. 2021. 신경 언어 모델의 반사실적 기억. ArXiv, abs/2112.12938. 분수 추출률 결과 분수 추출률은 데이터 세트에서 정확하고 올바른 위치에 있는 생성된 토큰의 분수입니다(그림 2의 하단 섹션 참조). 이 지표를 측정하는 이유는 추출과 관련된 위험에 대한 보다 자세한 평가를 제공하기 위해서입니다. 정확한 추출률은 공격자가 추출을 사용하기 위해 정확한 일치가 필요한 경우에 특히 중요합니다. 좋은 예로는 신용 카드 번호를 추출하는 경우가 있습니다. 이러한 경우 몇 개의 토큰이 틀리더라도 공격은 쓸모없게 됩니다. 그러나 공격자가 추출된 시퀀스의 의미에 더 신경을 쓰는 경우 분수 추출률이 위험을 평가하는 데 더 나은 지표가 될 수 있습니다. 이는 사람이 몇 개의 토큰이 틀렸더라도 시퀀스의 올바른 의미를 추론할 수 있기 때문입니다. 이 지표와 관련된 결과는 그림 3에 나와 있습니다. 이러한 결과를 정확한 추출률 결과(그림 2)와 비교하면 모든 실험에서 동일한 추세를 관찰할 수 있습니다. 방어의 경우에도 동일한 공유 추세가 관찰된다는 점에 유의합니다. 이 경우 분수 추출률 결과는 표 2에 나와 있습니다. B 훈련 설정 소프트 프롬프트는 Lester et al.(2021)에서 설명한 대로 무작위 단어 임베딩으로 초기화됩니다. 배치 크기는 128이고 학습률은 5e-4인 Adam 옵티마이저(Kingma 및 Ba, 2014)를 사용합니다. 공격 설정의 경우 프롬프트는 15에포크 동안 훈련됩니다. 방어의 경우 프롬프트는 훈련 손실이 지정된 0 값(섹션 3.2에서 설명한 대로) 주변에서 안정화될 때까지 훈련되며, 이는 실험에서 2~3에포크 내에 발생합니다. 우리는 Pytorch(Paszke et al., 2019) 구현을 사용하는데, 여기서 우리는 8개 GPU에 대한 분산 학습을 처리하기 위해 HuggingFace Accelerate(HF) 및 DeepSpeed(Rasley et al., 2020) 라이브러리를 활용합니다.분수 추출 속도 분수 추출 속도 0.0.0.0.0.A0.0.15기준선 OCLM 정렬된 CLM 0.0.0.GPT-Neo-125M 2 B1 C1 Di 0.0.° 0.0.40Ο 0.0.0.0.0.GPT-Neo-1.3B 기준선 CLM 0.Aligned CLM B0.CD0.0.0.0.0.° ° 0.° 0.0.0.0.0.0.0.0.5접미사 크기 4025 50 75 100접두사 크기 10 15 빔 크기 기준 공격 프롬프트 길이 = 프롬프트 길이 = 프롬프트 길이 = 프롬프트 길이 = 프롬프트 길이 = 프롬프트 길이 = 프롬프트 길이 그림 3: 프롬프트 길이(3-A1, 3-A2), 접미사 크기(3-B1, 3-B2), 접두사 크기(3-C1, 3-C2) 및 빔 크기(3-D1, 3-D2)에 대한 분수 추출률의 변화. 상단 패널은 GPT-Neo-125M 결과를 보여주고 하단 패널은 GPT-Neo-1.3B 결과를 보여줍니다. 각 선 주변의 투명한 다각형은 지점 전체에서 95% 신뢰 구간을 나타냅니다. 15.71 ± 0.16.601 ± 0.17.499 ± 0.19.691 ± 0.모델 Ꮎ 분획 추출율 파일 시험 PPL 0* GPT-Neo 125M 1.1.1.0.35 ± 0.0.192 ± 0.0.123 ± 0.0.087± 0.GPT0.099 0.124M 0* GPT-Neo 1.3B 0.0.0.634 ± 0.0.316 ± 0.0.171 ± 0.0.128 ± 0.GPT1.5B 0.166 ± 0.30.3231.9.2130.9.758 ± 0.10.267±0.10.775 ± 0.17.155 ± 0.표 2: 방어 설정에 대한 분수 추출률과 해당 퍼플렉시티, 0의 다른 값. 값은 평균 ± 표준 편차로 보고됩니다. 유사한 크기의 해당 GPT2 변형보다 작은 추출률은 퍼플렉시티 값도 작을 때 달성되며 양호합니다.(* 방어 없음). fp16 혼합 정밀도. p3dn.24xlarge 인스턴스에서 평균 공격 프롬프트 교육 시간은 프롬프트당 0.9시간인 반면 평균 방어 프롬프트 교육 시간은 프롬프트당 0.02시간이었습니다. C 향후 작업 향후 작업의 맥락에서 탐구하고 싶은 몇 가지 방법이 있습니다. 보다 정교한 교육 전략이 공격 설정에서 더 나은 추출률을 가져올 수 있을 것으로 예상하고(더 나은 손실 목표 설계, 소프트 프롬프트의 더 나은 초기화 등) 이를 더 탐구하고 싶습니다. 우리는 추출률에 대한 보다 견고한 분석을 수행하기 위해 다른 매개변수 효율적 학습 방법(Li 및 Liang, 2021; Hu et al., 2021) 및 하드 프롬프트 학습 방법(Wallace et al., 2019)과 같은 다양한 프롬프트 학습 알고리즘을 탐색하고 싶습니다. 우리는 다양한 모델과 데이터 세트에서 학습된 프롬프트의 이전 가능성을 테스트하고 싶습니다. 마지막으로, 우리는 우리의 방어를 학습 시점에 적용되는 방어(예: 차등적 개인 정보 보호 확률적 경사 하강법 버전; Abadi et al. 2016; Dupuy et al. 2021) 또는 디코딩 단계에서 적용되는 방어(예: 차등적 개인 정보 보호 디코딩; Majmudar et al. 2022)와 같은 다른 기존 방어와 결합하고 싶습니다. 목표는 이러한 방어를 결합하여 더 나은 개인 정보 보호-유용성 균형을 달성하는 것입니다.
