--- ABSTRACT ---
StarCoder와 같은 Code Large Language Models(Code LLM)는 코드 관련 작업에서 뛰어난 성능을 보였습니다. 그러나 대부분의 기존 모델은 명령어 미세 조정 없이 방대한 원시 코드 데이터로만 사전 학습되었습니다. 이 논문에서는 Evol-Instruct 방법을 코드 도메인에 적용하여 Code LLM에 복잡한 명령어 미세 조정 기능을 제공하는 WizardCoder를 소개합니다. HumanEval, HumanEval+, MBPP, DS1000이라는 네 가지 주요 코드 생성 벤치마크에 대한 포괄적인 실험을 통해 모델의 뛰어난 기능을 공개합니다. 다른 모든 오픈 소스 Code LLM을 상당한 차이로 능가합니다. 게다가, 우리 모델은 HumanEval과 HumanEval+에서 가장 큰 폐쇄형 LLM인 Anthropic의 Claude와 Google의 Bard보다 성능이 뛰어납니다. 코드, 모델 가중치, 데이터는 https://github.com/nlpxucan/WizardLM에서 공개됩니다. 1
--- INTRODUCTION ---
최근 대규모 언어 모델(LLM)[1-9]이 상당한 주목을 받고 인상적인 성공을 거두었습니다. 특히 OpenAI의 ChatGPT가 탁월한 사례로 돋보입니다. 방대한 양의 인터넷 데이터에 대한 광범위한 사전 학습과 자세한 지침 데이터[10]로 추가 미세 조정을 활용하여 이러한 모델은 다양한 작업에서 최첨단(SOTA) 제로샷 성능을 달성했습니다. 이러한 추세는 코드 이해 및 생성 영역에서도 관찰됩니다. 수많은 코드 LLMS[11-18]가 코드 관련 작업과 관련된 과제를 해결하기 위해 제안되었습니다. 이러한 코드 LLM은 상당한 양의 코드 데이터를 사용하여 사전 학습을 거쳐 다양한 코드 관련 작업에서 탁월한 성과를 보이며 인상적인 성능을 보여줍니다. 주로 사전 학습 프로세스를 강조하는 대부분의 이전 코드 LLM과 달리 코드 영역에서 세분화된 지침 튜닝에 대한 탐색은 제한적이었습니다. 지침 튜닝의 도입은 원래 다양한 작업에서 LM의 일반화 기능을 향상시키는 것을 목표로 했습니다[19-25]. 예를 들어 OpenAI의 InstructGPT[10]는 인간 주석자에게 사용자의 의도와 일치하도록 명확한 지침을 제공하도록 요청하는 것을 포함했습니다. 마찬가지로 Alpaca[26]와 같은 최근 작업은 ChatGPT가 지침 데이터를 생성하는 self-instruct[27] 방법을 채택했습니다. Vicuna[28]는 ShareGPT.com에서 수집한 사용자 공유 대화를 활용했습니다. WizardLM[29]은 기존 지침 데이터를 진화시켜 더 복잡하고 다양한 데이터 세트를 생성하는 Evol-Instruct 방법을 도입했습니다. 그러나 이러한 모든 접근 방식은 주로 일반 도메인에 초점을 맞추었고 코드 도메인에 대한 구체적인 설계 고려 사항이 부족했다는 점에 주목할 가치가 있습니다. † 동등한 기여. Microsoft에서 인턴십 중에 수행한 작업. 연락 저자. 사전 인쇄본. 검토 중. Evol-Instruct 방법에서 동기를 얻은 이 연구는 코드별 Evol-Instruct를 통해 복잡한 코드 지침 데이터를 생성하여 SOTA 오픈소스 Code LLM인 StarCoder[11]의 기능을 향상시키는 것을 목표로 합니다. 이를 달성하기 위해, 우리는 코드 관련 작업에 특별히 맞춰 진화적 프롬프트 프로세스에 여러 가지 조정을 가했습니다. 이러한 수정에는 진화적 지침 개선, 진화적 프롬프트 형태 단순화, 코드 디버깅 및 시간-공간 복잡도 제약 통합이 포함됩니다. 처음에, 우리의 방법은 기본 코드 지침 데이터인 Code Alpaca[30]를 진화시키는 데 적용됩니다. 그런 다음, 우리는 새로 만든 코드 지침-추종 훈련 세트를 사용하여 StarCoder의 미세 조정을 수행하고 WizardCoder를 얻습니다. HumanEval[31], HumanEval+[32], MBPP[33], DS-100[34]의 네 가지 코드 생성 벤치마크에서 얻은 실험 결과는 우리의 WizardCoder가 다른 모든 오픈 소스 Code LLM보다 성능이 뛰어나 최신(SOTA) 성능을 달성함을 보여줍니다. 구체적으로, 우리는 pass@1 점수에서 상당한 개선을 관찰했으며, HumanEval에서 +22.3(57.3 대 35.0) 및 MBPP에서 +8.2(51.8 대 43.6) 증가했습니다. 놀랍게도, 훨씬 작은 크기에도 불구하고, 우리의 WizardCoder는 HumanEval과 HumanEval+의 통과율 측면에서 Anthropic의 Claude와 Google의 Bard를 능가합니다. 이 작업의 기여는 다음과 같이 요약할 수 있습니다. • 우리는 Code Evol-Instruct를 적용하여 오픈 소스 Code LLM인 StarCoder의 성능을 향상시키는 WizardCoder를 소개합니다. • WizardCoder는 StarCoder, CodeGen, CodeGee, CodeT5+, InstructCodeT5+, StarCoder-GPTeacher, Instruct-Codegen-16B를 포함하여 코드 생성 측면에서 다른 모든 오픈 소스 Code LLM을 상당한 마진으로 능가합니다. • WizardCoder는 크기가 상당히 작음에도 불구하고 Claude, Bard, PaLM, PaLM-2 및 LaMDA와 같은 가장 큰 폐쇄형 소스 LLM과 비교하여 코드 생성에서 뛰어난 결과를 달성합니다. 2
--- RELATED WORK ---
대규모 언어 모델. 최근 LLM은 광범위한 작업에서 놀라운 성과를 보였습니다. 저명한 기술 회사는 매우 유능한 LLM을 개발하는 데 상당한 진전을 이루었습니다. 여기에는 OpenAI의 GPT3&amp;4[1, 2], Google의 PaLM[3, 4], Bard³, DeepMind의 Chinchilla[5], Gopher[6], 그리고 Anthropic의 Claude4가 포함됩니다. 그러나 이러한 모델은 폐쇄 소스이며 특정 API를 통해서만 액세스할 수 있거나 전혀 액세스할 수 없을 수도 있다는 점에 유의하는 것이 중요합니다. AI 커뮤니티는 모델 가중치가 공개적으로 제공되는 여러 오픈 소스 LLM이 출시되는 것을 목격했습니다. EleutherAI는 GPT-NeoX-20B[35]와 GPT-J-6B[36]에 기여했습니다. Google은 UL2-20B[37]를 출시했습니다. Tsinghua University는 GLM-130B[7]를 도입했습니다. Meta는 OPT[9]와 LLAMA[8]를 출시했습니다. 이러한 오픈소스 모델이 귀중한 기여를 했지만 일반적으로 폐쇄소스 대응 모델과 동일한 수준의 성능을 보이지 않는다는 점에 주목할 가치가 있습니다.코드를 위한 대규모 언어 모델. 최근 연구에서는 코드 이해 및 생성의 과제를 해결하기 위해 코드 관련 작업을 위한 상당수의 LLM이 도입되었습니다. OpenAI는 Codex[16]와 Code-Davinci[38]를 공개했습니다. Google은 PaLM-Coder[3]를 제안했습니다. 이들은 HumanEval[31] 및 MBPP[33]와 같은 인기 있는 코드 완성 벤치마크에서 뛰어난 성능을 보입니다. 그러나 이러한 모델은 폐쇄소스입니다. 반면에 사용 가능한 오픈소스 Code LLM도 여러 개 있습니다. Salesforce는 CodeGen[13], CodeT5[17], CodeT5+[18]를 출시했습니다. Tsinghua University는 CodeGeeX[14]에 기여했고 BigCode Project는 StarCoder[11]를 개발했습니다. 이러한 모델은 코드 관련 작업에서 눈에 띄는 진전을 보였습니다. 그러나 SOTA 폐쇄 소스 모델과 비교했을 때 여전히 상당히 뒤처져 있습니다. 지시 미세 조정이 없는 앞서 언급한 모델과 달리, 저희의 작업은 Code Evol-Instruct로 Code LLM을 추가로 훈련하면 성능을 상당히 향상시킬 수 있음을 보여줍니다. 3 https://bard.google.com/ https://www.anthropic.com/index/introducing-claudeInstruction 미세 조정. 초기 단계에서 지시 미세 조정의 주요 목적은 LM의 교차 작업 일반화 기능을 향상시키는 것이었습니다. 이는 상당한 양의 공개 NLP 작업 코퍼스로 LM을 미세 조정하여 달성되었습니다. T5[19]는 이 접근 방식을 탐구한 최초의 모델 중 하나로, 다양한 지도식 텍스트 대 텍스트 작업에 대해 훈련했습니다. FLAN[20], ExT5[22], TO[23], UnifiedQA[25]와 같은 후속 연구는 LM의 전반적인 일반화 능력을 강화하기 위해 작업 범위를 더욱 확장했습니다. 특히 ZeroPrompt[24]와 FLAN-T5[21]는 수천 개의 작업을 학습 파이프라인에 통합하여 한계를 뛰어넘었습니다. 이러한 연구에서 일관된 결과가 나타났습니다. 다양한 NLP 작업 지침으로 LM을 미세 조정하면 새로운 작업에 적용할 때 성능이 크게 향상됩니다. 다양한 NLP 작업으로 LM을 미세 조정하는 것은 유망한 결과를 보였지만 실제 사용자의 의도와 일치하지 않는 경우가 많습니다. OpenAI는 다양한 형태와 광범위한 작업 유형을 포함하는 방대한 인간 지침 코퍼스를 제공하도록 인간 주석자에게 요청하여 다른 접근 방식을 추구했습니다. 이 데이터 세트를 기반으로 OpenAI는 GPT3[1] 모델을 학습하여 사용자 입력과 더 잘 일치하는 InstructGPT[10]를 생성했습니다. 이 개발 라인은 ChatGPT로 알려진 인상적인 작업으로 이어졌습니다. 그러나 이러한 발전과 관련된 데이터 세트와 모델 가중치는 공개적으로 사용할 수 없다는 점에 유의하는 것이 중요합니다. Alpaca[26]는 자체 지침을 채택하여 다른 경로를 취합니다.
--- METHOD ---
코드의 도메인으로. 포괄적인
--- EXPERIMENT ---
영어: HumanEval, HumanEval+, MBPP, DS1000이라는 네 가지 주요 코드 생성 벤치마크에서 우리 모델의 뛰어난 기능을 공개합니다.다른 모든 오픈소스 코드 LLM을 상당한 차이로 능가합니다.게다가 우리 모델은 HumanEval과 HumanEval+에서 가장 큰 폐쇄형 LLM인 Anthropic의 Claude와 Google의 Bard보다 성능이 뛰어납니다.코드, 모델 가중치, 데이터는 https://github.com/nlpxucan/WizardLM에서 공개됩니다.1 서론 최근 대규모 언어 모델(LLM)[1-9]이 상당한 주목을 받고 인상적인 성공을 거두었습니다.특히 OpenAI의 ChatGPT가 두드러진 사례로 돋보입니다.엄청난 양의 인터넷 데이터에 대한 광범위한 사전 학습과 자세한 지침 데이터[10]로 추가 미세 조정을 활용하여 이러한 모델은 다양한 작업에서 최첨단(SOTA) 제로샷 성능을 달성했습니다.이러한 추세는 코드 이해 및 생성 영역에서도 관찰됩니다. 수많은 Code LLMS[11-18]가 코드 관련 작업과 관련된 과제를 해결하기 위해 제안되었습니다. 이러한 Code LLM은 상당한 양의 코드 데이터를 사용하여 사전 학습을 거쳐 다양한 코드 관련 작업에서 탁월한 성과를 거두고 인상적인 성능을 보여줍니다. 주로 사전 학습 프로세스를 강조하는 대부분의 이전 Code LLM과 달리 Code 도메인에서 세분화된 명령어 튜닝에 대한 탐색은 제한적이었습니다. 명령어 튜닝의 도입은 처음에는 다양한 작업에서 LM의 일반화 기능을 향상시키는 것을 목표로 했습니다[19-25]. 예를 들어 OpenAI의 InstructGPT[10]는 인간 주석자에게 사용자의 의도와 일치하도록 명확한 지침을 제공하도록 요청하는 것을 포함했습니다. 마찬가지로 Alpaca[26]와 같은 최근 작업에서는 ChatGPT가 명령어 데이터를 생성하는 self-instruct[27] 방법을 사용했습니다. Vicuna[28]는 ShareGPT.com에서 수집한 사용자 공유 대화를 활용했습니다. WizardLM[29]은 기존 명령어 데이터를 진화시켜 더 복잡하고 다양한 데이터 세트를 생성하는 Evol-Instruct 방법을 도입했습니다. 그러나 이러한 모든 접근 방식은 주로 일반 도메인에 초점을 맞추었고 코드 도메인에 대한 구체적인 설계 고려 사항이 부족하다는 점에 주목할 가치가 있습니다. † 동등한 기여. Microsoft에서 인턴십 기간 동안 수행한 작업. 연락 저자. 사전 인쇄본. 검토 중. Evol-Instruct 방법에서 영감을 받은 이 연구는 코드별 Evol-Instruct를 통해 복잡한 코드 명령어 데이터를 생성하여 SOTA 오픈소스 Code LLM인 StarCoder[11]의 기능을 향상시키는 것을 목표로 합니다. 이를 달성하기 위해 코드 관련 작업에 맞게 특별히 조정된 진화적 프롬프트 프로세스에 여러 가지 조정을 가했습니다. 이러한 수정 사항에는 진화적 명령어 개선, 진화적 프롬프트 형식 단순화, 코드 디버깅 및 시간-공간 복잡도 제약 통합이 포함됩니다. 처음에는 기본 코드 명령어 데이터인 Code Alpaca[30]를 진화시키는 데 방법을 적용합니다. 그 후, 새로 만든 코드 명령어 추종 훈련 세트를 사용하여 StarCoder의 미세 조정을 수행하고 WizardCoder를 얻습니다.HumanEval[31], HumanEval+[32], MBPP[33], DS-100[34]의 네 가지 코드 생성 벤치마크에서 얻은 실험 결과는 WizardCoder가 다른 모든 오픈 소스 Code LLM보다 성능이 뛰어나 최신(SOTA) 성능을 달성한다는 것을 보여줍니다.특히, HumanEval에서 +22.3(57.3 대 35.0) 증가, MBPP에서 +8.2(51.8 대 43.6) 증가하여 pass@1 점수에서 상당한 개선을 관찰합니다.놀랍게도, 훨씬 작은 크기에도 불구하고 WizardCoder는 HumanEval과 HumanEval+에서 통과율 측면에서 Anthropic의 Claude와 Google의 Bard를 능가합니다. 이 작업의 기여는 다음과 같이 요약할 수 있습니다.• 오픈소스 Code LLM인 StarCoder의 성능을 Code Evol-Instruct의 적용을 통해 향상시키는 WizardCoder를 소개합니다.• WizardCoder는 StarCoder, CodeGen, CodeGee, CodeT5+, InstructCodeT5+, StarCoder-GPTeacher, Instruct-Codegen-16B를 포함하여 코드 생성 측면에서 다른 모든 오픈소스 Code LLM을 상당한 마진으로 능가합니다.• WizardCoder는 크기가 상당히 작음에도 불구하고 Claude, Bard, PaLM, PaLM-2, LaMDA와 같은 가장 큰 폐쇄소스 LLM과 비교하여 코드 생성에서 더 뛰어난 결과를 달성합니다.2 관련 작업 대규모 언어 모델.최근 LLM은 광범위한 작업에서 놀라운 성과를 보였습니다.유명 기술 회사는 고도로 숙련된 LLM을 개발하는 데 큰 진전을 이루었습니다. 여기에는 OpenAI의 GPT3&amp;4[1, 2], Google의 PaLM[3, 4] 및 Bard³, DeepMind의 Chinchilla[5] 및 Gopher[6], 그리고 Anthropic의 Claude4가 포함됩니다. 그러나 이러한 모델은 폐쇄형 소스이며 특정 API를 통해서만 액세스할 수 있거나 전혀 액세스할 수 없을 수도 있다는 점에 유의하는 것이 중요합니다. AI 커뮤니티는 모델 가중치가 공개적으로 제공되는 여러 오픈 소스 LLM의 출시를 목격했습니다. EleutherAI는 GPT-NeoX-20B[35] 및 GPT-J-6B[36]에 기여했습니다. Google은 UL2-20B[37]를 출시했습니다. Tsinghua University는 GLM-130B[7]를 도입했습니다. Meta는 OPT[9] 및 LLAMA[8]를 출시했습니다. 이러한 오픈 소스 모델이 귀중한 기여를 했지만 일반적으로 폐쇄형 소스 모델과 동일한 수준의 성능을 보이지 않는다는 점에 주목할 가치가 있습니다. 코드를 위한 대규모 언어 모델. 최근 연구에서는 코드 이해 및 생성의 과제를 해결하기 위해 코드 관련 작업을 위한 상당수의 LLM이 도입되었습니다. OpenAI는 Codex[16]와 Code-Davinci[38]를 공개했습니다. Google은 PaLM-Coder[3]를 제안했습니다. 이들은 HumanEval[31]과 MBPP[33]와 같은 인기 있는 코드 완성 벤치마크에서 뛰어난 성능을 보입니다. 그러나 이러한 모델은 폐쇄 소스입니다. 반면에 사용 가능한 오픈 소스 코드 LLM도 여러 개 있습니다. Salesforce는 CodeGen[13], CodeT5[17], CodeT5+[18]를 도입했습니다. Tsinghua University는 CodeGeeX[14]에 기여했고 BigCode Project는 StarCoder[11]를 개발했습니다. 이러한 모델은 코드 관련 작업에서 주목할 만한 진전을 보였습니다. 그러나 SOTA 폐쇄 소스 모델과 비교했을 때 여전히 상당히 뒤처져 있습니다. 영어: 앞서 언급한 지침 미세 조정이 없는 모델과 대조적으로, 우리의 작업은 Code Evol-Instruct를 사용하여 Code LLM을 추가로 훈련하면 성능을 상당히 향상시킬 수 있음을 보여줍니다.3 https://bard.google.com/ https://www.anthropic.com/index/introducing-claudeInstruction 미세 조정.초기 단계에서 지침 미세 조정의 주요 목적은 LM의 교차 작업 일반화 기능을 향상시키는 것이었습니다.이는 상당한 양의 공개 NLP 작업 코퍼스로 LM을 미세 조정하여 달성되었습니다.T5[19]는 이 접근 방식을 탐구한 최초의 모델 중 하나로, 다양한 지도식 텍스트-텍스트 작업에 대해 훈련했습니다.FLAN[20], ExT5[22], TO[23], UnifiedQA[25]와 같은 후속 작업은 LM의 전반적인 일반화 능력을 강화하기 위해 작업 범위를 더욱 확장했습니다. 특히 ZeroPrompt[24]와 FLAN-T5[21]는 수천 개의 작업을 훈련 파이프라인에 통합하여 한계를 뛰어넘었습니다. 이러한 연구에서 일관된 결과가 나타났습니다. 다양한 NLP 작업 지침으로 LM을 미세 조정하면 새로운 작업에 적용할 때 성능이 크게 향상됩니다. 다양한 NLP 작업으로 LM을 미세 조정하는 것은 유망한 결과를 보였지만 실제 사용자의 의도와 일치하지 않는 경우가 많습니다. OpenAI는 다양한 형태와 광범위한 작업 유형을 포함하는 방대한 인간 지침 코퍼스를 제공하도록 인간 주석자에게 요청하여 다른 접근 방식을 추구했습니다. 이 데이터 세트를 기반으로 OpenAI는 GPT3[1] 모델을 훈련하여 사용자 입력과 더 잘 일치하는 InstructGPT[10]를 생성했습니다. 이 개발 라인은 ChatGPT로 알려진 인상적인 작업으로 이어졌습니다. 그러나 이러한 발전과 관련된 데이터 세트 및 모델 가중치는 공개적으로 사용할 수 없다는 점에 유의하는 것이 중요합니다. Alpaca[26]는 자체 지시 방법[27]을 채택하여 다른 경로를 취하고 ChatGPT를 활용하여 학습을 위한 데이터를 생성합니다.Vicuna[28]는 ShareGPT.com에서 수집한 사용자 공유 대화를 활용하여 모델을 학습합니다.WizardLM[29]은 기존 지시 데이터를 진화시켜 더 복잡하고 다양한 데이터 세트를 생성하는 Evol-Instruct 방법을 도입합니다.이러한 일반적인 지시 미세 조정 접근 방식과 달리 WizardCoder는 특히 Code LLM 도메인에 Evol-Instruct 방법을 성공적으로 적용합니다.접근 방식 이 섹션에서는 WizardCoder의 방법론적 세부 사항에 대해 설명합니다.WizardLM에 따라 자체 지시를 사용하여 생성된 Code Alpaca를 진화시키고 진화된 데이터로 사전 학습된 Code LLM StarCoder를 미세 조정하는 데 Evol-Instruct 방법을 적용합니다. 3.1 코드에 대한 Evol-Instruct 프롬프트 WizardLM에서 제안한 Evol-Instruct [29] 방법에서 영감을 얻은 이 작업도 코드 사전 학습된 대형 모델의 미세 조정 효과를 높이기 위해 코드 지침을 더 복잡하게 만들려고 시도합니다.Evol-Instruct를 코드 영역에 적용하기 위해 진화적 프롬프트에 다음과 같은 수정을 가했습니다.1. 심화, 복잡한 입력 및 In-Breadth Evolving을 제거하여 진화적 지침을 간소화했습니다.2. 진화적 프롬프트 템플릿을 통합하여 진화적 프롬프트의 형태를 단순화했습니다.3. 코드 도메인의 특정 특성을 해결하기 위해 코드 디버깅 및 코드 시간-공간 복잡도 제약이라는 두 가지 진화적 지침을 추가했습니다.통합된 코드 진화적 프롬프트 템플릿은 다음과 같습니다.주어진 프로그래밍 테스트 문제의 난이도를 조금 높여주세요. 다음 방법을 사용하되 이에 국한되지 않고 난이도를 높일 수 있습니다.{method} {question} 여기서 {question}은 진화를 기다리는 현재 코드 명령을 나타내고 {method}는 진화 유형입니다.사용한 5가지 유형은 다음과 같습니다.원래 문제에 새로운 제약 조건과 요구 사항을 추가하여 약 10개의 단어를 추가합니다.프로그래밍 작업에서 일반적으로 사용되는 요구 사항을 덜 일반적이고 더 구체적인 요구 사항으로 바꿉니다.원래 문제가 몇 가지 논리적 단계만으로 해결될 수 있는 경우 추론 단계를 더 추가하세요.오류 지시를 증가시키기 위해 참조로 잘못된 코드를 제공합니다.더 높은 시간 또는 공간 복잡도 요구 사항을 제안하지만 자주 제안하지 마세요.3.2 WizardCoder 학습 다음 절차를 사용하여 WizardCoder를 학습합니다.처음에는 StarCoder 15B[11]를 기반으로 사용하고 Evol-Instruct를 통해 진화된 코드 명령 따르기 학습 세트를 사용하여 미세 조정을 진행합니다. 미세 조정을 위한 프롬프트 형식은 다음과 같습니다. 아래는 추가 컨텍스트를 제공하는 입력과 함께 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: {지침} ##23% 응답: 학습 데이터 세트를 구성하기 위해 Code Alpacas라는 20K 지침 따르기 데이터 세트로 초기화했습니다. 20,개 샘플로 구성된 이 데이터 세트에 Evol-Instruct 기술을 반복적으로 사용하여 진화된 데이터를 생성합니다. 각 라운드의 데이터 진화 후에 모든 이전 라운드의 진화된 데이터를 원래 데이터 세트와 병합하여 StarCoder를 미세 조정하고 HumanEval[31]에서 pass@1 메트릭을 평가합니다. pass@1 메트릭이 감소하는 것을 관찰하면 Evol-Instruct 사용을 중단하고 최고의 pass@1을 갖는 모델을 최종 모델로 선택합니다. 4 실험 이 섹션은 실험에서 기준 모델에 대한 포괄적인 개요를 제공하는 것으로 시작합니다. 그 후, 우리는 HumanEval[31], HumanEval+[32], MBPP[33], DS-1000[34]의 4가지 코드 생성 벤치마크에서 우리 모델의 성능을 제시합니다.4.1 기준선 폐쇄형 소스 모델.여러 기술 회사가 공개적으로 릴리스하지 않기로 선택하면서도 매우 유능한 LLM을 성공적으로 개발했습니다.이러한 모델을 폐쇄형 소스 모델이라고 합니다.우리의 연구를 위해 우리는 상당수의 이러한 모델을 기준선으로 통합했습니다.특히, 우리의 기준선은 다음을 포함합니다.(i) OpenAI의 GPT3.5&amp;4[2], Code-Davinci002[38], Code-Cushman-001[38], Codex[16];(ii) Google의 Bard, PaLM 2[4], PaLM[3], LaMDA[40];(iii) Google DeepMind의 AlphaCode[12];(iv) Anthropic의 Claude. 오픈소스 모델. 여러 오픈소스 LLM이 AI 커뮤니티에 제공되었지만, 그 성능은 일반적으로 폐쇄형 소스 모델보다 훨씬 뒤떨어집니다. 저희 연구의 일환으로, 저희는 이러한 오픈소스 모델 중 상당수를 저희의 기준선으로 통합합니다. 우리의 기준선은 다음 모델을 포함합니다: StarCoder[11], LLaMa[8], CodeGen[13], Shttps://github.com/sahil280114/codealpacaGPT85.78.68.60.59.52.53.47.40.39.GPT3.WizardCoder Claude-Plus Claude Bard 통과한 시험 비율 Human Eval Human Eval+ 44.36.34.130.5 32.327.23.819.23.221.18.915.StarCoder-GPTeacher Instruct-Codegen-16B StarCoder-Plus Guanaco-65B 그림 1: 한 번의 시도로 HumanEval(164개 문제)에서 통과율 백분율. 모든 기준선 점수는 LLM-Humaneval-Benchmarks[39]에서 검색됩니다. WizardCoder는 탐욕적 디코딩으로 답을 생성합니다.Falcon-40B-Instruct CodeGeeX [14], CodeT5+[18], InCoder[15].또한 StarCoder-GPTeacher,6 Instruct-Codegen-16B, Guanaco-65B, Falcon-40B-Instruct를 포함하여 명령어 미세 조정이 있는 여러 모델도 포함합니다.4.2 구현 세부 정보 StarCoder [11]는 기본 기반 모델 역할을 합니다.진화된 데이터 세트는 약 78k 샘플로 구성됩니다.기본 모델을 미세 조정하기 위해 512의 배치 크기, 2048의 시퀀스 길이, 200개의 미세 조정 단계, 30개의 워밍업 단계, 2e-5의 학습 속도, 코사인 학습 속도 스케줄러, fp16 혼합 정밀도를 포함한 특정 구성을 사용합니다. 4.3 HumanEval, HumanEval+ 및 MBPP 평가 HumanEval[31], HumanEval+[32] 및 MBPP[33]는 Code LLM 분야에서 광범위하게 사용되는 벤치마크입니다. 이러한 벤치마크는 광범위한 Python 프로그래밍 문제 컬렉션을 포함하며, 테스트 케이스를 사용하여 Code LLM에서 생성된 코드를 검증합니다. HumanEval은 164개의 원래 프로그래밍 문제로 구성되어 있으며, 각 문제에 평균 9.6개의 테스트 케이스가 할당됩니다. LLM 합성 코드의 기능적 정확성을 철저히 평가하기 위해 HumanEval+는 테스트 케이스 수를 크게 확장하여 문제당 평균 774.8개의 테스트 케이스를 제공합니다. 반면 MBPP는 문제당 3개의 자동화된 테스트 케이스와 함께 500개의 테스트 프로그래밍 문제를 제공합니다. 이러한 작업에 대한 프롬프트 형식은 다음과 같습니다. 아래는 추가 컨텍스트를 제공하는 입력과 함께 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: 이 문제에 대한 Python 스크립트를 만듭니다. {질문} ### 응답: &quot;https://huggingface.co/GeorgiaTechResearch Institute/starcoder-gpteacher-code-instruct 7https://huggingface.co/sahil2801/instruct-codegen-16B https://huggingface.co/TheBloke/guanaco-65B-HF https://huggingface.co/tiiuae/falcon-40b-instruct표 1: HumanEval 및 MBPP에서 통과 @1(%)의 결과. 대부분의 점수는 StarCoder [11] 및 CodeT5+ [18]의 논문에서 가져왔습니다. 이전 연구 [31]에 따라 동일한 하이퍼 매개변수 집합인 temperate=0.2 및 top_p=0.95로 통과 @1 점수를 추정하기 위해 n개의 샘플을 생성합니다. *: 이 모델을 직접 평가합니다. 모델 매개변수 HumanEval MBPP 폐쇄형 소스 모델 LaMDA [40] 137B 14.AlphaCode [12] 1.1B 17.PALM [3] 540B 26.36.PALM-Coder [3] 540B 36.47.PALM 2-S [4] 37.50.Codex [16] 2.5B 21.Codex [16] 12B 28.Code-Cushman-001 [38] 33.45.Code-Davinci-002 [38] 47.58.GPT-3.5 [2] 48.GPT-4 [2] 67.오픈소스 모델 LLaMa [8] 33B 21.30.LLaMa [8] 65B 23.37.CodeGen-Multi [13] 16B 18.20.CodeGen-Mono [13] 16B 29.35.CodeGeeX [14] 13B 22.24.StarCoder [11] 15B 33.43.6* CodeT5+ [18] 16B 30.InstructCodeT5+ [18] 16B 35.WizardCoder 15B 57.3 (+22.3) 51.8 (+8.2) 폐쇄형 소스 모델과 비교. GPT4, Claude, Bard와 같은 코드 생성을 위한 SOTA LLM은 주로 폐쇄형 소스입니다. 이러한 모델의 API에 액세스하는 것은 어려운 일입니다. 이 연구에서는 LLM-Humaneval-Benchmarks [39]에서 HumanEval 및 HumanEval+에 대한 점수를 검색하여 대체 접근 방식을 채택했습니다. 특히, 언급된 모든 모델은 단일 시도를 활용하여 각 문제에 대한 코드 솔루션을 생성하고, 그 결과 통과율 백분율이 보고됩니다. 일관성을 유지하기 위해 탐욕적 디코딩을 사용하여 답을 생성하고 제공된 평가 코드를 사용하여 WizardCoder를 평가하여 동일한 실험 설정을 사용합니다. 이러한 표준화된 절차를 준수함으로써 기존 벤치마크에 대해 모델에 대한 공정하고 비교 가능한 평가를 보장하는 것을 목표로 합니다. 그림 1에서 볼 수 있듯이 WizardCoder는 이 벤치마크에서 3위를 차지하여 Claude-Plus(59.8 대 53.0)와 Bard(59.8 대 44.5)를 앞지릅니다. 특히, 이 모델은 이러한 모델에 비해 상당히 작은 크기를 보입니다. 더욱이 WizardCoder는 지침 미세 조정을 거치는 다른 오픈소스 LLM보다 현저히 우수하여 상당한 성능 마진을 보여줍니다. 오픈소스 모델과 비교. 표 1에서 HumanEval 및 MBPP 벤치마크에서 WizardCoder와 다른 오픈소스 모델을 포괄적으로 비교합니다. 그림 1에 제시된 결과와 대조적으로, 우리는 각 문제에 대해 n개의 샘플을 생성하여 1점의 합격점을 추정함으로써 이전 연구[31]에 설명된 접근 방식을 고수합니다. 표 1에 제시된 결과는 우리의 WizardCoder가 모든 오픈 소스 모델에 비해 상당한 성능 이점을 보인다는 것을 분명히 보여줍니다. 그림 1과 표 1의 실험 결과에서 우리는 다음을 얻습니다.
--- CONCLUSION ---
s: 1. WizardCoder는 Claude, Bard, PaLM, PALM-2, LaMDA를 포함한 가장 큰 폐쇄형 소스 LLM보다 성능이 뛰어나지만, 상당히 작습니다. 표 2: DS-1000에서 WizardCoder와 기준 모델의 성능. 모든 모델은 동일한 하이퍼 매개변수 집합으로 평가됩니다: temperature=0.2, top_p=0.5, max_length=1024. 점수는 40개 샘플에 대한 평균 pass@1 정확도입니다. Matplotlib(plt) 작업에는 적절한 컨텍스트가 없으므로 삽입 및 완료 점수가 동일합니다. 형식 완료 완료 완료 모델 문제 수: InCoder-6B CodeGen-mono 완료 Code-Cushman-StarCoder plt np pd 155 22028.3 4.31.7 10.40.7 21.8 7.ру scp sk tf All115 45 1,3.4.2.2.3.7.3.7.9.10.15.11.12.4 11.3 18.12.18.51.7 29.11.4 21.20.2 29.5 24.26.Completion 삽입 삽입 WizardCoder 55.2 33.16.7 26.24.24.26.29.InCoder-6B 28.3 4.2.9 4.StarCoder 51.7 30. 삽입 WizardCoder 55.2 35.10.20.21.30.2.8 3.1 7.8 7.20.2 27.4 20.0 25.28.9 32.3 37.8 32. Evol Round 4352. Human Eval에서 Pass@1 54.57.56. 그림 2: 데이터 진화 라운드 수에 대한 Ablation 연구. 2. WizardCoder는 StarCoder, CodeGen, CodeGee, CodeT5+를 포함하여 모든 오픈소스 Code LLM을 큰 차이로(HumanEval에서 +22.3) 능가합니다. 3. WizardCoder는 InstructCodeT5+, StarCoder-GPTeacher, Instruct-Codegen-16B를 포함하여 명령어 미세 조정이 있는 모든 오픈소스 Code LLM을 상당히 능가합니다. 4.4 DS 평가-DS-1000 벤치마크[34]는 7개 라이브러리에 걸쳐 1,000개의 고유한 데이터 과학 워크플로로 구성됩니다. 테스트 케이스에 대한 코드 생성의 성능을 평가하고 완료 및 삽입의 두 가지 평가 모드를 지원합니다. 실험에서 지원하는 모델에 대한 삽입 점수만 보고합니다. DS-1000 벤치마크는 Matplotlib(plt), NumPy(np), Pandas(pd), SciPy(scp), Scikit-Learn(sk), PyTorch(py), TensorFlow(tf)를 포함하여 사용된 라이브러리를 기반으로 문제를 추가로 분류합니다. StarCoder와 동일한 프롬프트 형식을 따릅니다. 표 2에서는 각 라이브러리에 대한 pass@1(n=40) 결과와 전체 점수를 제시합니다. 이러한 결과를 바탕으로 WizardCoder가 DS-1000 벤치마크에서 데이터 과학 문제를 해결할 때 다른 모든 모델보다 상당히 우수하다는 결론을 내렸습니다. 이러한 관찰은 거의 모든 데이터 과학 라이브러리에서 사실입니다. 4. 절제 연구 그림 2는 데이터 진화 라운드 수의 영향을 조사하는 절제 연구를 나타냅니다. 진화된 데이터의 첫 번째 라운드에는 38k 샘플이 포함됩니다. 두 번째 라운드에는 58k가 포함됩니다. 세 번째 라운드에는 78k가 포함됩니다. 네 번째 라운드에는 98k가 포함됩니다. 일관성을 위해 모든 모델은 200단계로 미세 조정을 거칩니다. 결과에 따르면 humaneval에서 가장 높은 pass@1 점수는 3라운드의 데이터 진화 후에 달성되었습니다. 이 관찰을 바탕으로 세 번째 라운드 동안 진화한 데이터를 최종 데이터 세트로 선택했습니다. 4.6 예제 표 3은 WizardCoder와의 상호 작용 예를 보여줍니다. 이 예제는 모델이 명확한 설명과 함께 지속적으로 정확한 응답을 생성한다는 것을 보여줍니다. 5 결론 및 향후 작업 이 논문에서는 Code Evol-Instruct 미세 조정된 Code LLM인 WizardCoder를 소개합니다. 실험 결과에 따르면 WizardCoder는 널리 알려진 4가지 코드 생성 벤치마크인 HumanEval, HumanEval+, MBPP, DS-1000에서 기존의 모든 오픈소스 Code LLM을 능가하는 SOTA 성능을 달성합니다. 나아가 WizardCoder는 Anthropic의 Claude와 Google의 Bard를 포함한 가장 큰 폐쇄형 LLM과 비교해도 우수한 성능을 보입니다. 향후 연구. 그림 1에서 볼 수 있듯이 WizardCoder는 인상적인 코딩 성능을 보이지만, 저희 모델은 여전히 SOTA LLM인 GPT4보다 상당히 뒤처집니다. 따라서 향후 연구에서는 Code Evol-Instruct 방법을 개선하여 모델의 성능을 더욱 높이는 데 주력할 것입니다. 더 광범위한 영향. 다른 LLM과 마찬가지로 WizardCoder도 비윤리적, 유해하거나 오해의 소지가 있는 정보를 생성할 수 있습니다. 따라서 윤리적, 사회적 영향을 다루기 위한 향후 연구가 필요합니다. 지침 오늘 날짜를 알려주는 Python 함수를 작성하세요. Python 함수를 수치적으로 적분하는 방법을 모르겠습니다. 코드를 작성해 주세요. 표 3: WizardCoder와의 상호작용 예. 응답 오늘 날짜를 알려주는 Python 함수는 다음과 같습니다. import datetime def get_date(): now datetime.datetime.now() return now.strftime(&quot;%A, %B %d, %Y&quot;) print (get_date()) 이 함수는 datetime 모듈을 사용하여 현재 날짜와 시간을 가져옵니다. 그런 다음 strftime() 메서드를 사용하여 날짜를 &quot;Day, Month Date, Year&quot; 형식의 문자열로 포맷합니다. 마지막으로 포맷된 문자열을 반환합니다. Python 코드에서 이 함수를 호출하여 현재 날짜와 시간을 가져올 수 있습니다. 물론, 여기 사다리꼴 법칙을 사용하여 Python 함수를 수치적으로 적분하는 예제 코드가 있습니다. def integrated (f, a, b, n): h = (ba) / ns = (f(a) + f (b)) /for i in range (1, n): s = f(a + i *h) return h* s 이 코드는 함수 f, 하한 a, 상한 b, 그리고 부분 구간의 개수 n을 받습니다. 그런 다음 각 부분 구간 h의 너비를 계산하고 사다리꼴 법칙을 사용하여 a와 b 사이의 f의 적분을 근사합니다. 결과는 float로 반환됩니다. 이 함수를 사용하려면 적분하려는 함수, 하한, 상한, 그리고 사용하려는 부분 구간의 개수와 함께 함수를 호출합니다. 예를 들어: def f(x): return x**result = integrated (f, 0, 1, 1000) print (result) 이렇게 하면 subintervals를 사용하여 0과 1 사이에서 함수 f(x) = x**2를 적분하고 결과를 출력합니다. 20과 30 사이의 소수를 나열하는 Java 코드는 다음과 같습니다. public class Prime Numbers { + args) public static void main(String () args) { for (int i = 20%; i &lt;= 30%; i++) { boolean is Prime = true; for (int j = 2; j &lt; i; j++) { if (i % j == 0) { } } isPrime = false%3B break; if (isPrime) { } } } System.out.print(i + &quot; &quot;); 소수 나열 출력: Java로 20과 30 사이의 숫자. 설명: - 중첩된 for 루프를 사용하여 ~ 30 사이의 모든 숫자를 반복합니다. - 각 숫자에 대해 또 다른 for 루프를 사용하여 숫자가 소수인지 확인합니다. - 숫자가 1과 자기 자신 이외의 다른 숫자로 나누어 떨어지면 그 숫자는 소수가 아닙니다. - 숫자가 1과 자기 자신 이외의 다른 숫자로 나누어 떨어지지 않으면 그 숫자는 소수입니다. - 우리는 콘솔에 소수를 출력합니다.참고문헌 [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습기입니다. Hugo Larochelle, Marc&#39;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan 및 Hsuan-Tien Lin 편집자, 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상, 2020년. [2] OpenAI. GPT-4 기술 보고서. CORR, abs/2303.08774, 2023. [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevinson, Liam 페더스, 데니 저우, 다프네 Ippolito, David Luan, 임현택, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. Palm: 경로를 통한 언어 모델링 확장. CORR, abs/2204.02311, 2022. [4] 로한 아닐, 앤드류 M. 다이, 오르한 피라트, 멜빈 존슨, 드미트리 레피킨, 알렉상드르 파소스, 시아막 샤케리, 에마누엘 타로파, 페이지 베일리, 지펭 첸, 에릭 추, 조나단 H. 클라크, 로랑 엘 샤페이, 황얀핑, 캐시 마이어-헬스턴, 가우라브 미슈라, 에리카 모레이라, 마크 오머닉, 케빈 로빈슨, 세바스찬 루더, 이 타이, 케판 샤오, 위안중 쉬, 위징 장, 구스타보 에르난데스 아브레고, 준완 안, 제이콥 오스틴, 폴 바람, 얀 A. 보타, 제임스 브래드버리, 시다르타 브라흐마, 케빈 브룩스, 미셸 카타스타, 용 청, 콜린 체리, 크리스토퍼 A. 초켓추, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez 등 Palm 2 기술 보고서. CORR, abs/2305.10403, 2023. [5] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals 및 Laurent Sifre. 컴퓨팅 최적 대규모 언어 모델 교육. CoRR, abs/2203.15556, 2022. [6] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela 파가니니, 로랑 Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d&#39;Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, 카림 아유브, 제프 스탠웨이, 로레인 베넷, 데미스 하사비스, 코레이 카부크쿠오글루, 제프리 어빙. 언어 모델 확장: 고퍼 훈련을 통한 방법, 분석 및 통찰력. CoRR, abs/2112.11446, 2021. [7] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong 및 Jie 탕. GLM-130B: 사전 훈련된 개방형 이중 언어 모델입니다. CoRR, abs/2210.02414, 2022. [8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, ArmandJoulin, Edouard Grave 및 Guillaume 램플. Llama: 개방적이고 효율적인 기초 언어 모델입니다. CORR, abs/2302.13971, 2023. [9] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang 및 Luke Zettlemoyer. OPT: 사전 학습된 개방형 변환기 언어 모델. CoRR, abs/2205.01068, 2022. [10] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, Ryan Lowe. 인간의 피드백을 통해 지침을 따르도록 언어 모델 훈련. NeurIPS, 2022. [11] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: 소스가 당신과 함께 하기를! arXiv 사전 인쇄본 arXiv:2305.06161, 2023. [12] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d&#39;Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals. alphacode를 사용한 경쟁 수준 코드 생성. CORR, abs/2203.07814, 2022. [13] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. Codegen: 다중 턴 프로그램 합성을 사용한 코드용 개방형 대규모 언어 모델. 제11회 학습 표현 국제 컨퍼런스, 2023. [14] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang. Codegeex: humaneval-x에서 다국어 평가를 사용한 코드 생성을 위한 사전 학습된 모델. CORR, abs/2303.17568, 2023. [15] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer 및 Mike Lewis. Incoder: 코드 채우기 및 합성을 위한 생성 모델. CORR, abs/2204.05999, 2022. [16] 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 엔리케 폰데 드 올리베이라 핀토, 자렛 카플란, 해리슨 에드워즈, 유리 부르다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필립 틸레, 펠리페 페트로스키 수치, 데이브 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트보스, 윌리엄 헵겐 거스, 알렉스 니콜, 알렉스 파이노, 니콜라스 테작, 지에 탕, 이고르 Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 코드로 학습된 대규모 언어 모델 평가. CoRR, abs/2107.03374, 2021. [17] Yue Wang, Weishi Wang, Shafiq R. Joty, Steven CH Hoi. Codet5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델. Marie-Francine Moens, Xuanjing Huang, Lucia Specia 및 Scott Wen-tau Yih 편집자, 2021 자연어 처리 경험적 방법에 대한 컨퍼런스 논문집, EMNLP 2021, 가상 이벤트/푼타카나, 도미니카 공화국, 2021년 11월 7-11일, 8696-8708쪽. 계산언어학 협회, 2021. [18] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li 및 Steven CH Hoi. Codet5+: 코드 이해 및 생성을 위한 오픈 코드 대규모 언어 모델. CoRR, abs/2305.07922, 2023. [19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21:140:1-140:67, 2020. [20] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le. 미세 조정된 언어 모델은 제로샷 학습자입니다. 제10회 학습 표현 국제 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net, 2022.[21] 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le 및 Jason Wei. 명령어를 미세 조정한 언어 모델을 확장합니다. CORR, abs/2210.11416, 2022. [22] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder 및 Donald Metzler. Ext5: 전이 학습을 위한 극단적인 멀티태스크 확장에 관하여. 학습 표현에 관한 제10차 국제 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net, 2022. [23] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, 김태운, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, 아비쉬트 샤르마, 안드레아 Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush. 멀티태스크 프롬프트 학습은 제로샷 작업 일반화를 가능하게 합니다. 제10회 학습 표현 국제 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net, 2022. [24] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang. Zeroprompt: 프롬프트 기반 사전 학습을 1,000개 작업으로 확장하면 제로샷 일반화가 개선됩니다. Yoav Goldberg, Zornitsa Kozareva 및 Yue Zhang 편집자, 연구 결과: EMNLP 2022, 아랍에미리트 아부다비, 2022년 12월 7-11일, 4235-4252쪽. Association for Computational Linguistics, 2022. [25] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark 및 Hannaneh Hajishirzi. Unifiedqa: 단일 QA 시스템으로 포맷 경계를 넘나들기. Trevor Cohn, Yulan He, Yang Liu 편집자, Findings the Association for Computational Linguistics: EMNLP 2020, 온라인 이벤트, 2020년 11월 16-20일, Findings of ACL의 EMNLP 2020권, 1896-1907쪽. Association for Computational Linguistics, 2020. [26] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. Stanford 알파카: 지시를 따르는 라마 모델. https://github. com/tatsu-lab/stanford_alpaca, 2023. [27] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi 및 Hannaneh Hajishirzi. 자가 지시: 언어 모델을 자가 생성 지시 사항과 일치시킵니다. arXiv 사전 인쇄 arXiv:2212.10560, 2022. [28] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica 및 Eric P. Xing. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇, 2023년 3월. [29] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang. Wizardlm: 복잡한 지침을 따르도록 대규모 언어 모델 강화. arXiv 사전 인쇄본 arXiv:2304.12244, 2023. [30] Sahil Chaudhary. Code alpaca: 코드 생성을 위한 지침 따르기 라마 모델. https://github.com/sahil280114/codealpaca, 2023. [31] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 코드에서 학습된 대규모 언어 모델 평가. CORR, abs/2107.03374, 2021. [32] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang. chatgpt에서 생성한 코드가 정말 정확한가요? 코드 생성을 위한 대규모 언어 모델의 엄격한 평가. CORR, abs/2305.01210, 2023.[33] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, Charles Sutton. 대규모 언어 모델을 사용한 프로그램 합성. CORR, abs/2108.07732, 2021. [34] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida I. Wang, Tao Yu. DS-1000: 데이터 과학 코드 생성을 위한 자연스럽고 신뢰할 수 있는 벤치마크. CoRR, abs/2211.11501, 2022. [35] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang 및 Samuel Weinbach. Gpt-neox-20b: 오픈 소스 자기 회귀 언어 모델. CORR, abs/2204.06745, 2022. [36] Ben Wang 및 Aran Komatsuzaki. GPT-J-6B: 60억 매개변수 자기 회귀 언어 모델. https://github.com/kingoflolz/mesh-transformer-jax, 2021년 5월. [37] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler. 언어 학습 패러다임 통합. CoRR, abs/2205.05131, 2022. [38] Microsoft. Azure OpenAI 서비스 모델. https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models, 2023. [39] Llm humaneval 벤치마크. 11m-humaneval-benchmarks, 2023. https://github.com/my-other-github-account/ [40] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee 도시, 레넬리토 델로스 산토스, 토주 듀크, 조니 소라커, 벤 제벤베르겐, 비노드쿠마르 프라브하카란, 마크 디아즈, 벤 허친슨, 크리스틴 올슨, 알레한드라 몰리나, 에린 호프만-존, 조쉬 리, 로라 아로요, 라비 라자쿠마르, 알레나 부트리나, 매튜 램, 빅토리아 쿠즈미나, 조 펜턴, 아론 코헨, 레이첼 번스타인, 레이 커즈와일, 블레이즈 Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi 및 Quoc Le. Lamda: 대화 상자 응용 프로그램을 위한 언어 모델입니다. CoRR, ABS/2201.08239, 2022.
