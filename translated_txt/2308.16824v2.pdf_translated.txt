--- ABSTRACT ---
인간 프로그래머가 프로그래밍 언어를 완벽하게 터득하면 새로운 프로그래밍 언어를 배우는 것이 더 쉬울 것입니다. 이 보고서에서는 프로그래밍 언어가 코드 대규모 언어 모델의 명령어 미세 조정 단계에서 서로를 향상시킬 수 있는지 알아보는 데 중점을 둡니다. StarCoder에서 8가지 인기 있는 프로그래밍 언어(Python, JavaScript, TypeScript, C, C++, Java, Go, HTML)에 대한 광범위한 실험을 수행합니다. 결과는 프로그래밍 언어가 서로를 상당히 향상시킬 수 있음을 보여줍니다. 예를 들어, Python에서 학습한 CODEM-Python 15B는 HumanEval-X에서 Java를 17.95% pass@1로 증가시킬 수 있습니다. 더욱 놀랍게도 HTML 코퍼스에서 학습한 CODEM-HTML 7B는 Java를 15.24% pass@1로 향상시킬 수 있음을 발견했습니다. 당사의 학습 데이터는 https://github.com/NL2Code/CodeM에서 공개됩니다. 키워드 대규모 언어 모델 코드 생성 · 프로그래밍 언어 명령어 조정 . 1
--- INTRODUCTION ---
코드 대규모 언어 모델(코드 LLM)이 최근에 꽃을 피우고 있습니다[Zan et al., 2023]. Codex[Chen et al., 2021], AlphaCode[Li et al., 2022], PaLM-Coder[Chowdhery et al., 2022], CodeGen[Nijkamp et al., 2023], CodeGeeX[Zheng et al., 2023], StarCoder[Li et al., 2023], Code Llama[Rozière et al., 2023] 등 많은 코드 LLM이 연달아 출시되고 있습니다. 놀라운 코드 생성 성능 덕분에 코드 LLM은 학계와 산업계 모두에서 상당한 주목을 받고 있습니다. 최근 연구[Ouyang et al., 2022]에서는 LLM에 명령어를 따르는 방법을 가르칠 수 있는 명령어 튜닝 기술이 등장했습니다. 코드 생성 영역에서 WizardCoder [Luo et al., 2023]와 PanGu-Coder2 [Shen et al., 2023]도 이 기술을 채택하여 코드 생성 역량을 이끌어냅니다. CodeGen-Multi Nijkamp et al. [2023] 및 StarCoder-base Li et al. [2023]와 같은 일부 코드 LLM은 여러 프로그래밍 언어에 걸친 코퍼스에서 학습되지만 이러한 언어 간의 상호 작용은 여전히 탐구되지 않았습니다. 프로그래밍 실무에서 인간 프로그래머가 프로그래밍 언어를 마스터하면 프로그래밍 언어 간의 동질성으로 인해 새로운 언어를 배우는 것이 더 쉬울 것입니다. 이에 동기를 부여받아 코드 LLM의 지침 미세 조정 중에 서로 다른 프로그래밍 언어가 서로를 향상시킬 수 있는지 알아보고자 합니다. 이 아이디어를 탐구하기 위해 인기 있는 8가지 프로그래밍 언어(Python, JavaScript, TypeScript, C, C++, Java, Go, HTML)에 대한 교육 코퍼스를 작성했으며 각 언어에는 약 9,000개의 프로그래밍 연습이 포함되어 있습니다. 우리는 각 프로그래밍 언어 코퍼스에서 별도로 명령어 튜닝 기법을 사용하여 StarCoder 7B를 훈련하고, 모든 프로그래밍 언어에서 미세 조정된 각 모델의 성능을 테스트합니다. 우리의 연구 결과에 따르면 프로그래밍 언어는 서로를 상당히 향상시킬 수 있습니다. 한편, 우리는 서로 다른 프로그래밍 언어의 개선 마진이 언어 간 유사성과 관련이 있음을 발견했습니다. 예를 들어, JavaScript 데이터로 훈련된 CODEM-JavaScript 7B는 TypeScript에서 절대 11.80%의 pass@1 개선을 가져올 수 있습니다. 더욱 흥미로운 점은, *처음 두 저자는 이 작업에 동등하게 기여했습니다. 기술 보고서 # 명령어 로드 시 메시지를 표시하는 웹 페이지를 디자인합니다. 메시지는 &quot;Hello, World&quot;여야 합니다. HTML 코드를 사용하여 이를 달성합니다. # 응답<!DOCTYPE html><html><head><title> 안녕하세요 세상</title></head><body><h1> 안녕하세요, 세상</h1></body></html> 그림 1: 우리가 만든 지시-답변 쌍의 HTML 훈련 사례. 마크업 언어 HTML로 훈련된 CODEM-HTML 7B는 Java에서 절대 15.24%의 pass@1 개선을 달성할 수도 있습니다. 간단히 말해서, 우리의 기여는 다음과 같습니다. (1) 우리의 연구 결과에 따르면 프로그래밍 언어는 코드 LLM의 지시 미세 조정 단계에서 서로를 크게 향상시킬 수 있습니다. (2) 여러 프로그래밍 언어 간의 상관 관계에 대한 귀중한 통찰력을 얻어 코드 생성에 대한 미래 연구의 길을 열었습니다. (3) 훈련 데이터를 공개적으로 제공할 것입니다. 방법론 2.1 8가지 프로그래밍 언어로 된 훈련 코퍼스 제작 인기 있는 8가지 프로그래밍 언어를 선택하여 훈련 데이터를 별도로 구성합니다. 선택한 언어에는 Python, JavaScript, TypeScript, C, C++, Java, Go 및 HTML이 포함되어 절차 지향, 객체 지향, 스크립트 및 심지어 마크업 언어와 같은 다양한 유형을 포함합니다. 각 프로그래밍 언어에 대해 약 9K 데이터 쌍이 포함된 훈련 데이터를 구성합니다. 각 쌍에는 프로그래밍 문제를 설명하는 명령어와 해당 응답이 모두 포함됩니다.HTML의 한 가지 실제 예가 그림 1에 나와 있습니다.선택된 언어를 기반으로 일련의 단일 언어 데이터 세트를 구성합니다.CodeAlpaca 20K²의 데이터 세트에서 시작하여 Python 관련 데이터를 추출하여 시드 명령어 세트를 형성합니다.그런 다음 선택한 각 프로그래밍 언어에 대해 OpenAI의 GPT-3.5³를 프롬프트하여 시드 명령어 세트의 기존 명령어를 진화시켜 해당하는 새 명령어를 얻습니다.HTML을 제외한 모든 선택한 언어에 대해 GPT-3.5에 시드 명령어(Python)를 대상 언어(Python, JavaScript, TypeScript, C, C++, Java 또는 Go)와 관련된 더 복잡한 버전으로 다시 작성하도록 요청하여 심층적 진화[Xu et al., 2023]를 채택합니다.그러나 HTML의 경우 HTML(마크업 언어)이 다른 언어(비 마크업 언어)와 너무 다르기 때문에 폭 넓은 진화를 채택하여 완전히 새로운 HTML 관련 명령어를 생성합니다. 2.2 명령어 튜닝 Codex [Chen et al., 2021] 및 StarCoder [Li et al., 2023]와 같은 코드 사전 학습된 모델은 풍부한 코드 지식을 저장합니다.그러나 이러한 모델은 일반 코드 조각에 대해서만 학습되므로 컨텍스트를 기반으로 왼쪽에서 오른쪽으로 코드 생성만 지원합니다.최근에는 명령어 튜닝 기술 [Ouyang et al., 2022, Luo et al., 2023, Shen et al., 2023]이 제안되었으며, 이는 채팅 기능을 활성화하기 위해 명령어를 따르는 모델의 기능을 향상시킬 수 있습니다.명령어 튜닝 동안 그림 2의 프롬프트를 사용하여 StarCoder를 학습하여 CODEM을 얻습니다.DeepSpeed를 사용하여 fp16이 활성화된 CODEM의 학습을 가속화합니다. 또한, 배치 크기를 GPU당 2로, 학습률을 코사인 어닐링 일정을 사용하여 2e-5로, 그래디언트 축적 단계를 4로, 워밍업 단계를 30으로 설정했습니다. 명령어 튜닝 후, 그림 3의 프롬프트를 사용하여 다양한 프로그래밍 언어에서 다운스트림 작업에 대한 추론을 수행합니다. 추론을 위해 샘플링을 위한 탐욕적 디코딩 전략을 채택합니다. CODEM이 2https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k 3https://platform.openai.com/docs/models/gpt-3-TECHNICAL REPORT라는 점을 감안하면, 아래는 추가 컨텍스트를 제공하는 입력과 함께 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 명령어: {문제} ### 응답: {응답} 그림 2: 명령어 튜닝의 프롬프트 형식. {문제}와 {응답}은 섹션 2.1에서 얻은 명령어와 답을 참조합니다. 아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: 이 문제에 대한 {언어} 코드를 완성하세요: {문제} ### 응답: {서명} 그림 3: 추론의 프롬프트 형식. {언어}, {문제}, {서명}은 각각 다운스트림 프로그래밍 언어, 주어진 프로그래밍 문제, 함수 헤더를 나타냅니다. 채팅 스타일 모델의 경우 생성하는 응답에는 코드 그 이상의 요소가 포함되는 경우가 많아 일반적으로 실행할 수 없습니다. 따라서 생성된 응답에서 코드 조각을 추출하여 코드 생성의 성능을 평가합니다. 3 실험 3.3.1. 평가 설정 벤치마크 및 기준선 HumanEval-X [Zheng et al., 2023]를 사용하여 Python, JavaScript, C++, Java, Go에서 모델의 다국어 능력을 평가합니다. HumanEval-X는 HumanEval [Chen et al., 2021] (Python)을 다른 프로그래밍 언어에 적용하여 제작되었습니다. HumanEval-X와 동일한 접근 방식을 따라 HumanEvalC와 HumanEval-TypeScript라는 두 가지 새로운 버전의 HumanEval도 만들었습니다. HumanEval은 HTML과 같은 마크업 언어에 직접 적용할 수 없으므로 다운스트림 평가 언어에는 HTML이 포함되지 않습니다. 모든 언어 버전의 CODEM에 대한 기본 기준선은 기본 모델인 StarCoder입니다. 언어 A에서 학습한 CODEM이 언어 B를 개선할 수 있는지 분석합니다. 이 경우 기준선은 언어 B에서 직접 CODEM에서 학습한 것입니다. 3.1. 지표 모든 모델을 평가하기 위해 pass@1을 지표로 채택했습니다. 각 모델은 각 프로그래밍 작업에 대해 탐욕적 디코딩 전략을 사용하여 하나의 답을 생성하고, 답은 주어진 테스트 사례에서 실행됩니다. 모든 테스트 사례가 통과된 경우에만 생성된 코드로 프로그래밍 작업이 해결된 것으로 간주할 수 있습니다. 이 설정에서 pass@1은 |Pc|로 공식화할 수 있습니다. 여기서 |P| HumanEval에서 프로그래밍 작업의 총 수를 나타내고 |Pc|는 해결된 작업의 수를 나타냅니다. 본질적으로 우리가 사용하는 pass@1 메트릭은 정확도로 간주될 수 있습니다. 3.2 결과 3.2.|P|&quot; 주요 결과 표 1은 각각 8개 언어의 단일 언어 데이터 세트에서 학습된 일련의 모델인 CODEM의 성능을 보여줍니다. 이는 HumanEval의 다양한 언어 버전에서 수행되었습니다. 보시다시피, 모든 CODEM 모델이 다음을 능가합니다. 기술 보고서 표 1: StarCoder 7B의 Pass@1(정확도) 및 다양한 프로그래밍 언어로 학습된 CODEM. 빨간색 숫자는 StarCoder 7B와 비교했을 때 절대적인 증가를 나타냅니다. 모델 Python StarCoder 7B 26.JavaScript 24.HumanEval-Multilingual TypeScript C C++ Java Go CODEM-Python 28.38.411.24.34.7610.3725.23.CODEM-JavaScript 33.544.24.37.10.40.15.29.014.32 34.158.CODEM-TypeScript 33.546.40.3711.27.783.37.8013.32.7.37.2014.34.761 27.443.11.CODEM-C 37.278.26.1.39.6312.30.255.37.2012.30.494.CODEM-C++ 32.303.28.054.25.611.1.34.577.32.107.35.10.CODEM-Java 32.303.35.378.34.579.33.549.35.379.39.13.CODEM-GO 32.303.38.4115.37.28.664.14.35.989.9.29.634.28.053.31.105.5.33.549.CODEM-HTML CODEM-혼합 31.683.37.8014.31.714.43.2916.33.549.15 32.303.25.931.24 28.37.2012.81 37.899.32 32.107.41 37.8012.30.255.27.443.34.158.35.9812.32.327.3.38.4115.28.053.39.6316.29.274.표 2: StarCoder 15B와 CODEM-Python의 Pass@1. 빨간색 숫자는 StarCoder 15B와 비교했을 때 절대적인 개선을 나타냅니다. 모델 HumanEval-Multilingual StarCoder 15B CODEM-Python Python 32.64.6331.JavaScript TypeScript C C++ Java Go 30.47.5616.32.39.757.26.35.199.31.43.8012.30.17.48.1717.34.7617.모든 프로그래밍 언어에서 기본 모델 StarCoder 7B가 큰 차이로 앞서는 것을 발견했습니다.또한 프로그래밍 언어가 서로를 상당히 향상시킬 수 있음을 발견했습니다.예를 들어, Python 코퍼스로만 학습한 CODEM-Python은 HumanEval-Java를 절대 14.03% pass@1로 향상시킬 수 있습니다.이러한 결과는 다양한 프로그래밍 언어 간의 고유한 공통점을 보여줍니다.더욱 놀랍게도 CODEM-HTML은 HumanEval-Java를 절대 15.24% pass@1로 향상시켜 CODEM-Java를 능가하기도 합니다. 마찬가지로 CODEM-C++는 HumanEval-C에서 CODEM-C를 능가하고 CODEMJavaScript는 HumanEval-Typescript에서 CODEM-TypeScript를 능가합니다. 이러한 관찰 결과를 바탕으로 다국어 코드 생성 성능의 향상은 단순히 새로운 지식을 통합하는 것이 아니라 명령어 튜닝을 통해 자연어 또는 프로그래밍 언어 이해 및 명령어 따르기 능력과 같은 모델의 내재적 잠재력을 끌어내기 때문이라고 추측합니다. 단일 언어 학습 코퍼스에서 CODEM을 학습하는 것 외에도 8개 프로그래밍 언어를 포함하는 9K 다국어 학습 세트를 추가로 구성합니다. 각 언어는 소량(~1.2K)의 학습 인스턴스로 구성되지만 실험 결과에 따르면 CODEM-Mixed는 모든 언어에서 뛰어나며 HumanEval-Python에서 CODEM-Python을 능가하고 HumanEval-Java에서 CODEM-Java를 능가합니다. 이는 명령어 튜닝에서 다국어 데이터를 활용하여 모델의 일반화를 해치지 않고도 더 뛰어난 코드 생성 성능을 낼 수 있음을 시사합니다. 또한 CODEM의 효과를 검증하기 위해 StarCoder 15B에 대한 실험을 수행합니다. 구체적으로 WizardCoder [Luo et al., 2023]에 따라 108K Python 학습 데이터를 얻고 StarCoder 15B를 미세 조정하여 CODEM-Python을 얻습니다. 결과는 표 2에 나와 있습니다. CODEM-Python은 동일한 규모의 다른 모델과 비교하여 HumanEval-Python에서 64.63% 통과@1로 최첨단 성능을 달성합니다. CODEM-Python은 또한 다른 프로그래밍 언어의 생성에서도 엄청난 개선을 얻습니다. 예를 들어 Java와 JavaScript를 각각 절대적으로 17.95%와 16.77% 통과@1로 개선합니다. 3.2.2 더 자세한 분석 우리는 다양한 프로그래밍 언어 간의 상관 관계를 분석합니다. 그림 4(a)에서 볼 수 있듯이 코드 생성 성능의 개선은 다양한 프로그래밍 언어의 학습 코퍼스에 민감합니다. 게다가, 우리는 C와 C++가 서로를 더 크게 향상시킬 수 있다는 것을 발견했는데, 이는 JavaScript와 TypeScript의 경우도 마찬가지입니다. 이는 이러한 언어가 언어 설계에서 서로 상관관계가 있고, 공통된 구문과 문법을 공유하기 때문에 타당합니다. 그림 4(b)는 각 프로그래밍 언어에 대한 학습이 다른 모든 언어의 코드 생성 성능을 향상시킬 수 있음을 보여줍니다. 그림 4(b)의 상관관계 값이 대부분 모두 양수인 것을 볼 수 있는데, 이는 단일 언어 학습 코퍼스 하나가 가져온 다양한 언어의 향상 추세가 비교적 유사함을 의미합니다.PythonS 0.LS 0.0.U 0.Java C++ 0.-0.0.0.-0.0.0.-0.5 -0.63 -0.0.1.-0.- 0.-0.-0.-0.- -0.-0.-0.8- 0.1 -0.-0.0.0.28 0.Python JS TS C C++ Java Go-1.(a) JS PythonS-0.-0.42 0.0.94 0.62 0.HTML Go Java C++ 0.62 0.35 0.18 0.0.92 0.66 0.29 0.0.0.84 0.-0.05 0.82 0.0.기술 보고서 1.- 0.- 0.-0.-0.0.73 0.0.22 0.65 0.0.88 0.Python JS TS T -0.C C++ Java Go HTML (b) 그림 4: 다양한 프로그래밍 언어 간의 상관 관계. 표 1의 데이터를 행렬로 간주하고 Pandas 라이브러리의 &quot;df.corr()&quot;을 사용하여 다양한 프로그래밍 언어 간의 상관 관계를 계산합니다. &quot;df.T&quot; 이전과 이후의 상관 관계 결과는 각각 (a)와 (b)에 표시됩니다. 4
--- RELATED WORK ---
120억 개의 매개변수를 가진 Codex [Chen et al., 2021]는 파이썬 프로그래밍 문제를 자동으로 해결할 수 있습니다. 이 놀라운 성공은 학계와 산업계 모두에서 큰 화제를 불러일으켰습니다. Codex에 이어 AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022], CodeGen [Nijkamp et al., 2023], InCoder [Fried et al., 2023], CodeGeeX [Zheng et al., 2023], replit, CodeT5 [Wang et al., 2021, 2023], PyCodeGPT [Zan et al., 2022], SantaCoder [Allal et al., 2023], StarCoder [Li et al., 2023], Code Llama [Rozière et al., 2023], phi-1 [Gunasekar et al., 2023]을 포함한 수많은 코드 LLM이 제안되었습니다. 위의 모델은 대규모 코드 코퍼스에서 학습되었으며 인상적인 코드 생성 성능을 달성합니다. 사전 학습 중에 일부 모델은 다국어 프로그래밍 언어의 데이터 세트로 학습한 다음 단일 언어 데이터 세트로 미세 조정하여 더 강력한 전문가 버전을 생성합니다.명령어 미세 조정 단계의 경우 WizardCoder [Luo et al., 2023], PanGuCoder2 [Shen et al., 2023] 및 Phind-CodeLlama³가 명령을 따르는 기능을 강화하고 코드 생성 기능을 더욱 높이기 위해 제안되었습니다.그러나 앞서 언급한 모델 중 어느 것도 서로 다른 프로그래밍 언어 간의 복잡한 상호 작용을 탐구하지 않습니다.따라서 이 보고서에서는 단일 언어 데이터에서 코드 LLM을 학습하면 다른 프로그래밍 언어의 성능을 강화할 수 있는지 조사하고자 합니다.5 결론 우리의 연구 결과에 따르면 단일 언어 학습 코퍼스는 명령 조정을 통해 코드 LLM의 다국어 코드 생성 기능을 향상시킬 수 있습니다.이는 여러 프로그래밍 언어 간의 본질적인 공통성과 상호 연결성을 강조합니다.향후 작업에서는 여러 언어가 서로를 향상시킬 수 있는 이유를 탐구할 계획입니다. 또한, 우리는 인기 있는 언어의 데이터를 사용하여 학습함으로써 이러한 모호하거나 덜 사용되는 프로그래밍 언어의 코드 생성 기능을 높이기 위해 연구 결과를 활용하는 방법을 살펴볼 것입니다. 감사의 말 귀중한 피드백과 통찰력을 제공해 주신 동료들에게 감사드립니다. 이 연구 전반에 걸쳐 건설적인 도움을 주신 An Fu(Huawei), Jingyang Zhao(Huawei), Yuenan Guo(Huawei)에게 특별히 감사드립니다. 참고문헌 Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou. 대규모 언어 모델이 NL2Code를 충족함: 설문 조사. https://huggingface.co/replit/replit-code-v1-3b Shttps://huggingface.co/Phind/Phind-CodeLlama-34B-v기술 보고서 계산 언어학 협회(제1권: 장문 논문), 7443-7464쪽, 캐나다 토론토, 2023년 7월. 계산 언어학 협회. URL https://aclanthology.org/2023. acl-long. 411. 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 헨리크 폰데, 자렛 카플란, 해리슨 에드워즈, 유라 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필리프 틸렛, 펠리페 페트로스키 수치, 데이비드 W. 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 H. 거스, 알렉스 니콜, 이고르 바부슈킨, S. 아룬 발라지, 샨타누 자인, 앤드류 카, 얀 레이케, 조슈아 아키암, 베단트 미스라, 에반 모리카와, 알렉 래드포드, 매튜 M. 나이트, 마일즈 브런디지, 미라 무라티, 케이티 메이어, 피터 웰린더, 밥 맥그루, 다리오 아모데이, 샘 맥캔들리시, 일리아 수츠케버, 보이치에흐 자렘바. 코드로 훈련된 대규모 언어 모델 평가. Arxiv, abs/2107.03374, 2021. Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d&#39;Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, Oriol Vinyals. alphacode를 사용한 경쟁 수준 코드 생성. Science, 378:1092 – 1097, 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin 로빈슨, 리암 페더스, 데니 Zhou, Daphne Ippolito, David Luan, 임현택, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. PaLM: 경로를 통한 언어 모델링 확장. ArXiv, abs/2204.02311, 2022. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese 및 Caiming Xiong. CodeGen: 다중 턴 프로그램 합성 기능을 갖춘 코드용 개방형 대규모 언어 모델입니다. 학습 표현에 관한 제11차 국제 회의에서, 2023. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang 및 Jie Tang. CodeGeeX: humaneval-x에 대한 다국어 평가를 통해 코드 생성을 위한 사전 훈련된 모델입니다. ArXiv, abs/2303.17568, 2023. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco 조카, 마난 데이, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, 덴마크 계약자, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra 및 Harm de Vries. StarCoder: 소스가 함께하길!, 2023. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom 및 Gabriel Synnaeve. Code Llama: 코드를 위한 오픈 파운데이션 모델, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J. Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. ArXiv, abs/2203.02155, 2022. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang. WizardCoder: evol-instruct로 대규모 언어 모델 코드 강화. arXiv 사전 인쇄 arXiv:2306.08568, 2023.기술 보고서 Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo 및 Qianxiang Wang. PanGu-Coder2: 순위 피드백을 통해 코드용 대규모 언어 모델 강화, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao 및 Daxin Jiang. WizardLM: 복잡한 지침을 따르도록 대규모 언어 모델 지원, 2023. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer 및 Mike Lewis. InCoder: 코드 채우기 및 합성을 위한 생성 모델. 2023년 제11회 국제 학습 표현 컨퍼런스에서. Yue Wang, Weishi Wang, Shafiq Joty, Steven CH Hoi. CodeT5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델. 2021년 경험적 학습 컨퍼런스 회의록에서.
--- METHOD ---
ology 2.1 8개 프로그래밍 언어의 훈련 코퍼스 제작 우리는 8개의 인기 있는 프로그래밍 언어를 선택하여 훈련 데이터를 별도로 구성합니다. 우리가 선택한 언어에는 Python, JavaScript, TypeScript, C, C++, Java, Go 및 HTML이 있으며, 절차 지향, 객체 지향, 스크립트 및 마크업 언어와 같은 다양한 유형을 포함합니다. 각 프로그래밍 언어에 대해 약 9K 데이터 쌍을 포함하는 훈련 데이터를 구성합니다. 각 쌍에는 프로그래밍 문제를 설명하는 명령어와 해당 응답이 모두 포함됩니다. HTML의 한 가지 실제 예가 그림 1에 나와 있습니다. 이러한 선택된 언어를 기반으로 일련의 단일 언어 데이터 세트를 구성합니다. CodeAlpaca 20K²의 데이터 세트에서 시작하여 Python 관련 데이터를 추출하여 시드 명령어 세트를 형성합니다. 그런 다음 선택한 각 프로그래밍 언어에 대해 OpenAI의 GPT-3.5³를 프롬프트하여 시드 명령어 세트의 기존 명령어를 진화시켜 해당하는 새 명령어를 얻습니다. HTML을 제외한 모든 선택된 언어에 대해 GPT-3.5에 시드 명령어(Python)를 대상 언어(Python, JavaScript, TypeScript, C, C++, Java 또는 Go)에 적합한 더 복잡한 버전으로 다시 작성하도록 요청하여 심층적 진화[Xu et al., 2023]를 채택했습니다.그러나 HTML의 경우 HTML(마크업 언어)이 다른 언어(비 마크업 언어)와 너무 다르기 때문에 폭 넓은 진화를 채택하여 완전히 새로운 HTML 관련 명령어를 생성합니다.2.2 명령어 튜닝 Codex[Chen et al., 2021] 및 StarCoder[Li et al., 2023]와 같은 코드 사전 학습된 모델은 풍부한 코드 지식을 저장합니다.그러나 이러한 모델은 일반 코드 조각에 대해서만 학습되므로 컨텍스트를 기반으로 왼쪽에서 오른쪽으로의 코드 생성만 지원합니다. 최근에는 명령어 튜닝 기법[Ouyang et al., 2022, Luo et al., 2023, Shen et al., 2023]이 제안되었는데, 이는 명령어를 따르는 모델의 기능을 향상시켜 채팅 기능을 활성화할 수 있습니다. 명령어 튜닝 동안 그림 2의 프롬프트를 사용하여 StarCoder를 훈련하여 CODEM을 얻습니다. DeepSpeed를 사용하여 fp16이 활성화된 CODEM의 훈련을 가속화합니다. 또한 배치 크기를 GPU당 2로, 학습률을 코사인 어닐링 일정을 사용하여 2e-5로, 그래디언트 축적 단계를 4로, 워밍업 단계를 30으로 설정합니다. 명령어 튜닝 후 그림 3의 프롬프트를 사용하여 다양한 프로그래밍 언어에서 다운스트림 작업에 대한 추론을 수행합니다. 추론을 위해 샘플링을 위한 탐욕적 디코딩 전략을 채택합니다. CODEM이 2https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k 3https://platform.openai.com/docs/models/gpt-3-TECHNICAL REPORT인 경우 아래는 작업을 설명하는 지침이며, 추가 컨텍스트를 제공하는 입력과 함께 제공됩니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: {문제} ### 응답: {응답} 그림 2: 지침 튜닝의 프롬프트 형식. {문제} 및 {응답}은 섹션 2.1에서 얻은 지침과 답변을 참조합니다. 아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: 이 문제에 대한 {언어} 코드를 완성하세요. {문제} ### 응답: {서명} 그림 3: 추론의 프롬프트 형식. {language}, {problem}, {signature}는 각각 다운스트림 프로그래밍 언어, 주어진 프로그래밍 문제, 함수 헤더를 나타냅니다. chat 스타일 모델에서 생성된 응답에는 코드 그 이상의 요소가 포함되는 경우가 많으며, 이는 일반적으로 실행할 수 없게 만듭니다. 따라서 생성된 응답에서 코드 조각을 추출하여 코드 생성의 성능을 평가합니다. 3
--- EXPERIMENT ---
영어: StarCoder에서 인기 있는 8가지 프로그래밍 언어(Python, JavaScript, TypeScript, C, C++, Java, Go, HTML)의 s. 결과는 프로그래밍 언어가 서로를 상당히 개선할 수 있음을 보여줍니다. 예를 들어, Python에서 학습한 CODEM-Python 15B는 HumanEval-X에서 Java를 절대 17.95% pass@1만큼 향상시킬 수 있습니다. 더 놀랍게도 HTML 코퍼스에서 학습한 CODEM-HTML 7B는 Java를 절대 15.24% pass@1만큼 향상시킬 수 있음을 발견했습니다. 당사의 학습 데이터는 https://github.com/NL2Code/CodeM에서 공개됩니다. . 키워드 대규모 언어 모델 코드 생성 · 프로그래밍 언어 명령어 튜닝 . 1 서론 코드 대규모 언어 모델(코드 LLM)이 최근에 꽃을 피우고 있습니다[Zan et al., 2023]. 많은 코드 LLM이 연이어 출시되고 있습니다.예를 들어, Codex [Chen et al., 2021], AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022], CodeGen [Nijkamp et al., 2023], CodeGeeX [Zheng et al., 2023], StarCoder [Li et al., 2023], Code Llama [Rozière et al., 2023].놀라운 코드 생성 성능 덕분에 코드 LLM은 학계와 산업계 모두에서 상당한 주목을 받았습니다.최근 연구 [Ouyang et al., 2022]에서는 LLM에게 지침을 따르는 방법을 가르칠 수 있는 지침 튜닝 기술이 소개되었습니다. 코드 생성 영역에서 WizardCoder [Luo et al., 2023]와 PanGu-Coder2 [Shen et al., 2023]도 이 기술을 채택하여 코드 생성 역량을 이끌어냅니다. CodeGen-Multi Nijkamp et al. [2023] 및 StarCoder-base Li et al. [2023]와 같은 일부 코드 LLM은 여러 프로그래밍 언어에 걸친 코퍼스에서 학습되지만 이러한 언어 간의 상호 작용은 여전히 탐구되지 않았습니다. 프로그래밍 실무에서 인간 프로그래머가 프로그래밍 언어를 마스터하면 프로그래밍 언어 간의 동질성으로 인해 새로운 언어를 배우는 것이 더 쉬울 것입니다. 이에 동기를 부여받아 코드 LLM의 지침 미세 조정 중에 서로 다른 프로그래밍 언어가 서로를 향상시킬 수 있는지 알아보고자 합니다. 이 아이디어를 탐구하기 위해 인기 있는 8가지 프로그래밍 언어(Python, JavaScript, TypeScript, C, C++, Java, Go, HTML)에 대한 교육 코퍼스를 작성했으며 각 언어에는 약 9,000개의 프로그래밍 연습이 포함되어 있습니다. 우리는 각 프로그래밍 언어 코퍼스에서 별도로 명령어 튜닝 기법을 사용하여 StarCoder 7B를 훈련하고, 모든 프로그래밍 언어에서 미세 조정된 각 모델의 성능을 테스트합니다. 우리의 연구 결과에 따르면 프로그래밍 언어는 서로를 상당히 향상시킬 수 있습니다. 한편, 우리는 서로 다른 프로그래밍 언어의 개선 마진이 언어 간 유사성과 관련이 있음을 발견했습니다. 예를 들어, JavaScript 데이터로 훈련된 CODEM-JavaScript 7B는 TypeScript에서 절대 11.80%의 pass@1 개선을 가져올 수 있습니다. 더욱 흥미로운 점은, *처음 두 저자는 이 작업에 동등하게 기여했습니다. 기술 보고서 # 명령어 로드 시 메시지를 표시하는 웹 페이지를 디자인합니다. 메시지는 &quot;Hello, World&quot;여야 합니다. HTML 코드를 사용하여 이를 달성합니다. # 응답<!DOCTYPE html><html><head><title> 안녕하세요 세상</title></head><body><h1> 안녕하세요, 세상</h1></body></html> 그림 1: 우리가 만든 지시-답변 쌍의 HTML 훈련 사례. 마크업 언어 HTML로 훈련된 CODEM-HTML 7B는 Java에서 절대 15.24%의 pass@1 개선을 달성할 수도 있습니다. 간단히 말해서, 우리의 기여는 다음과 같습니다. (1) 우리의 연구 결과에 따르면 프로그래밍 언어는 코드 LLM의 지시 미세 조정 단계에서 서로를 크게 향상시킬 수 있습니다. (2) 여러 프로그래밍 언어 간의 상관 관계에 대한 귀중한 통찰력을 얻어 코드 생성에 대한 미래 연구의 길을 열었습니다. (3) 훈련 데이터를 공개적으로 제공할 것입니다. 방법론 2.1 8가지 프로그래밍 언어로 된 훈련 코퍼스 제작 인기 있는 8가지 프로그래밍 언어를 선택하여 훈련 데이터를 별도로 구성합니다. 선택한 언어에는 Python, JavaScript, TypeScript, C, C++, Java, Go 및 HTML이 포함되어 절차 지향, 객체 지향, 스크립트 및 심지어 마크업 언어와 같은 다양한 유형을 포함합니다. 각 프로그래밍 언어에 대해 약 9K 데이터 쌍이 포함된 훈련 데이터를 구성합니다. 각 쌍에는 프로그래밍 문제를 설명하는 명령어와 해당 응답이 모두 포함됩니다.HTML의 한 가지 실제 예가 그림 1에 나와 있습니다.선택된 언어를 기반으로 일련의 단일 언어 데이터 세트를 구성합니다.CodeAlpaca 20K²의 데이터 세트에서 시작하여 Python 관련 데이터를 추출하여 시드 명령어 세트를 형성합니다.그런 다음 선택한 각 프로그래밍 언어에 대해 OpenAI의 GPT-3.5³를 프롬프트하여 시드 명령어 세트의 기존 명령어를 진화시켜 해당하는 새 명령어를 얻습니다.HTML을 제외한 모든 선택한 언어에 대해 GPT-3.5에 시드 명령어(Python)를 대상 언어(Python, JavaScript, TypeScript, C, C++, Java 또는 Go)와 관련된 더 복잡한 버전으로 다시 작성하도록 요청하여 심층적 진화[Xu et al., 2023]를 채택합니다.그러나 HTML의 경우 HTML(마크업 언어)이 다른 언어(비 마크업 언어)와 너무 다르기 때문에 폭 넓은 진화를 채택하여 완전히 새로운 HTML 관련 명령어를 생성합니다. 2.2 명령어 튜닝 Codex [Chen et al., 2021] 및 StarCoder [Li et al., 2023]와 같은 코드 사전 학습된 모델은 풍부한 코드 지식을 저장합니다.그러나 이러한 모델은 일반 코드 조각에 대해서만 학습되므로 컨텍스트를 기반으로 왼쪽에서 오른쪽으로 코드 생성만 지원합니다.최근에는 명령어 튜닝 기술 [Ouyang et al., 2022, Luo et al., 2023, Shen et al., 2023]이 제안되었으며, 이는 채팅 기능을 활성화하기 위해 명령어를 따르는 모델의 기능을 향상시킬 수 있습니다.명령어 튜닝 동안 그림 2의 프롬프트를 사용하여 StarCoder를 학습하여 CODEM을 얻습니다.DeepSpeed를 사용하여 fp16이 활성화된 CODEM의 학습을 가속화합니다. 또한, 배치 크기를 GPU당 2로, 학습률을 코사인 어닐링 일정을 사용하여 2e-5로, 그래디언트 축적 단계를 4로, 워밍업 단계를 30으로 설정했습니다. 명령어 튜닝 후, 그림 3의 프롬프트를 사용하여 다양한 프로그래밍 언어에서 다운스트림 작업에 대한 추론을 수행합니다. 추론을 위해 샘플링을 위한 탐욕적 디코딩 전략을 채택합니다. CODEM이 2https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k 3https://platform.openai.com/docs/models/gpt-3-TECHNICAL REPORT라는 점을 감안하면, 아래는 추가 컨텍스트를 제공하는 입력과 함께 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 명령어: {문제} ### 응답: {응답} 그림 2: 명령어 튜닝의 프롬프트 형식. {문제}와 {응답}은 섹션 2.1에서 얻은 명령어와 답을 참조합니다. 아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### 지침: 이 문제에 대한 {언어} 코드를 완성하세요: {문제} ### 응답: {서명} 그림 3: 추론의 프롬프트 형식. {언어}, {문제}, {서명}은 각각 다운스트림 프로그래밍 언어, 주어진 프로그래밍 문제, 함수 헤더를 나타냅니다. 채팅 스타일 모델의 경우 생성하는 응답에는 코드 그 이상의 요소가 포함되는 경우가 많아 일반적으로 실행할 수 없습니다. 따라서 생성된 응답에서 코드 조각을 추출하여 코드 생성의 성능을 평가합니다. 3 실험 3.3.1. 평가 설정 벤치마크 및 기준선 HumanEval-X [Zheng et al., 2023]를 사용하여 Python, JavaScript, C++, Java, Go에서 모델의 다국어 능력을 평가합니다. HumanEval-X는 HumanEval [Chen et al., 2021] (Python)을 다른 프로그래밍 언어에 적용하여 제작되었습니다. HumanEval-X와 동일한 접근 방식을 따라 HumanEvalC와 HumanEval-TypeScript라는 두 가지 새로운 버전의 HumanEval도 만들었습니다. HumanEval은 HTML과 같은 마크업 언어에 직접 적용할 수 없으므로 다운스트림 평가 언어에는 HTML이 포함되지 않습니다. 모든 언어 버전의 CODEM에 대한 기본 기준선은 기본 모델인 StarCoder입니다. 언어 A에서 학습한 CODEM이 언어 B를 개선할 수 있는지 분석합니다. 이 경우 기준선은 언어 B에서 직접 CODEM에서 학습한 것입니다. 3.1. 지표 모든 모델을 평가하기 위해 pass@1을 지표로 채택했습니다. 각 모델은 각 프로그래밍 작업에 대해 탐욕적 디코딩 전략을 사용하여 하나의 답을 생성하고, 답은 주어진 테스트 사례에서 실행됩니다. 모든 테스트 사례가 통과된 경우에만 생성된 코드로 프로그래밍 작업이 해결된 것으로 간주할 수 있습니다. 이 설정에서 pass@1은 |Pc|로 공식화할 수 있습니다. 여기서 |P| HumanEval에서 프로그래밍 작업의 총 수를 나타내고 |Pc|는 해결된 작업의 수를 나타냅니다. 본질적으로 우리가 사용하는 pass@1 메트릭은 정확도로 간주될 수 있습니다. 3.2 결과 3.2.|P|&quot; 주요 결과 표 1은 각각 8개 언어의 단일 언어 데이터 세트에서 학습된 일련의 모델인 CODEM의 성능을 보여줍니다. 이는 HumanEval의 다양한 언어 버전에서 수행되었습니다. 보시다시피, 모든 CODEM 모델이 다음을 능가합니다. 기술 보고서 표 1: StarCoder 7B의 Pass@1(정확도) 및 다양한 프로그래밍 언어로 학습된 CODEM. 빨간색 숫자는 StarCoder 7B와 비교했을 때 절대적인 증가를 나타냅니다. 모델 Python StarCoder 7B 26.JavaScript 24.HumanEval-Multilingual TypeScript C C++ Java Go CODEM-Python 28.38.411.24.34.7610.3725.23.CODEM-JavaScript 33.544.24.37.10.40.15.29.014.32 34.158.CODEM-TypeScript 33.546.40.3711.27.783.37.8013.32.7.37.2014.34.761 27.443.11.CODEM-C 37.278.26.1.39.6312.30.255.37.2012.30.494.CODEM-C++ 32.303.28.054.25.611.1.34.577.32.107.35.10.CODEM-Java 32.303.35.378.34.579.33.549.35.379.39.13.CODEM-GO 32.303.38.4115.37.28.664.14.35.989.9.29.634.28.053.31.105.5.33.549.CODEM-HTML CODEM-혼합 31.683.37.8014.31.714.43.2916.33.549.15 32.303.25.931.24 28.37.2012.81 37.899.32 32.107.41 37.8012.30.255.27.443.34.158.35.9812.32.327.3.38.4115.28.053.39.6316.29.274.표 2: StarCoder 15B와 CODEM-Python의 Pass@1. 빨간색 숫자는 StarCoder 15B와 비교했을 때 절대적인 개선을 나타냅니다. 모델 HumanEval-Multilingual StarCoder 15B CODEM-Python Python 32.64.6331.JavaScript TypeScript C C++ Java Go 30.47.5616.32.39.757.26.35.199.31.43.8012.30.17.48.1717.34.7617.모든 프로그래밍 언어에서 기본 모델 StarCoder 7B가 큰 차이로 앞서는 것을 발견했습니다.또한 프로그래밍 언어가 서로를 상당히 향상시킬 수 있음을 발견했습니다.예를 들어, Python 코퍼스로만 학습한 CODEM-Python은 HumanEval-Java를 절대 14.03% pass@1로 향상시킬 수 있습니다.이러한 결과는 다양한 프로그래밍 언어 간의 고유한 공통점을 보여줍니다.더욱 놀랍게도 CODEM-HTML은 HumanEval-Java를 절대 15.24% pass@1로 향상시켜 CODEM-Java를 능가하기도 합니다. 마찬가지로 CODEM-C++는 HumanEval-C에서 CODEM-C를 능가하고 CODEMJavaScript는 HumanEval-Typescript에서 CODEM-TypeScript를 능가합니다. 이러한 관찰 결과를 바탕으로 다국어 코드 생성 성능의 향상은 단순히 새로운 지식을 통합하는 것이 아니라 명령어 튜닝을 통해 자연어 또는 프로그래밍 언어 이해 및 명령어 따르기 능력과 같은 모델의 내재적 잠재력을 끌어내기 때문이라고 추측합니다. 단일 언어 학습 코퍼스에서 CODEM을 학습하는 것 외에도 8개 프로그래밍 언어를 포함하는 9K 다국어 학습 세트를 추가로 구성합니다. 각 언어는 소량(~1.2K)의 학습 인스턴스로 구성되지만 실험 결과에 따르면 CODEM-Mixed는 모든 언어에서 뛰어나며 HumanEval-Python에서 CODEM-Python을 능가하고 HumanEval-Java에서 CODEM-Java를 능가합니다. 이는 명령어 튜닝에서 다국어 데이터를 활용하여 모델의 일반화를 해치지 않고도 더 뛰어난 코드 생성 성능을 낼 수 있음을 시사합니다. 또한 CODEM의 효과를 검증하기 위해 StarCoder 15B에 대한 실험을 수행합니다. 구체적으로 WizardCoder [Luo et al., 2023]에 따라 108K Python 학습 데이터를 얻고 StarCoder 15B를 미세 조정하여 CODEM-Python을 얻습니다. 결과는 표 2에 나와 있습니다. CODEM-Python은 동일한 규모의 다른 모델과 비교하여 HumanEval-Python에서 64.63% 통과@1로 최첨단 성능을 달성합니다. CODEM-Python은 또한 다른 프로그래밍 언어의 생성에서도 엄청난 개선을 얻습니다. 예를 들어 Java와 JavaScript를 각각 절대적으로 17.95%와 16.77% 통과@1로 개선합니다. 3.2.2 더 자세한 분석 우리는 다양한 프로그래밍 언어 간의 상관 관계를 분석합니다. 그림 4(a)에서 볼 수 있듯이 코드 생성 성능의 개선은 다양한 프로그래밍 언어의 학습 코퍼스에 민감합니다. 게다가, 우리는 C와 C++가 서로를 더 크게 향상시킬 수 있다는 것을 발견했는데, 이는 JavaScript와 TypeScript의 경우도 마찬가지입니다. 이는 이러한 언어가 언어 설계에서 서로 상관관계가 있고, 공통된 구문과 문법을 공유하기 때문에 타당합니다. 그림 4(b)는 각 프로그래밍 언어에 대한 학습이 다른 모든 언어의 코드 생성 성능을 향상시킬 수 있음을 보여줍니다. 그림 4(b)의 상관관계 값이 대부분 모두 양수인 것을 볼 수 있는데, 이는 단일 언어 학습 코퍼스 하나가 가져온 다양한 언어의 향상 추세가 비교적 유사함을 의미합니다.PythonS 0.LS 0.0.U 0.Java C++ 0.-0.0.0.-0.0.0.-0.5 -0.63 -0.0.1.-0.- 0.-0.-0.-0.- -0.-0.-0.8- 0.1 -0.-0.0.0.28 0.Python JS TS C C++ Java Go-1.(a) JS PythonS-0.-0.42 0.0.94 0.62 0.HTML Go Java C++ 0.62 0.35 0.18 0.0.92 0.66 0.29 0.0.0.84 0.-0.05 0.82 0.0.기술 보고서 1.- 0.- 0.-0.-0.0.73 0.0.22 0.65 0.0.88 0.Python JS TS T -0.C C++ Java Go HTML (b) 그림 4: 다양한 프로그래밍 언어 간의 상관 관계.표 1의 데이터를 행렬로 간주하고 Pandas 라이브러리의 &quot;df.corr()&quot;을 사용하여 다양한 프로그래밍 언어 간의 상관 관계를 계산합니다.&quot;df.T&quot; 전후의 상관 관계 결과는 각각 (a)와 (b)에 나와 있습니다.4 관련 작업 120억 개의 매개변수를 가진 Codex[Chen et al., 2021]는 Python 프로그래밍 문제를 자동으로 풀 수 있습니다.이 놀라운 성공은 학계와 산업계 모두에서 큰 화제를 불러일으켰습니다. Codex에 이어 AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022], CodeGen [Nijkamp et al., 2023], InCoder [Fried et al., 2023], CodeGeeX [Zheng et al., 2023], replit, CodeT5 [Wang et al., 2021, 2023], PyCodeGPT [Zan et al., 2022], SantaCoder [Allal et al., 2023], StarCoder [Li et al., 2023], Code Llama [Rozière et al., 2023], phi-1 [Gunasekar et al., 2023]을 포함한 수많은 코드 LLM이 제안되었습니다. 위의 모델은 대규모 코드 코퍼스에서 학습되었으며 인상적인 코드 생성 성능을 달성합니다. 사전 학습 중에 일부 모델은 다국어 프로그래밍 언어의 데이터 세트로 학습한 다음 단일 언어 데이터 세트로 미세 조정하여 더 강력한 전문가 버전을 생성합니다. 지침 미세 조정 단계의 경우 WizardCoder [Luo et al., 2023], PanGuCoder2 [Shen et al., 2023], Phind-CodeLlama³가 지침을 따르는 기능을 강화하고 코드 생성 기능을 더욱 높이기 위해 제안되었습니다. 그러나 앞서 언급한 모델 중 어느 것도 서로 다른 프로그래밍 언어 간의 복잡한 상호 작용을 탐구하지 않습니다. 따라서 이 보고서에서는 단일 언어 데이터에서 코드 LLM을 학습하면 다른 프로그래밍 언어에서 성능을 강화할 수 있는지 조사하고자 합니다. 5
--- CONCLUSION ---
우리의 연구 결과는 단일 언어 학습 코퍼스가 명령어 튜닝을 통해 코드 LLM의 다국어 코드 생성 기능을 향상시킬 수 있음을 보여줍니다. 이는 여러 프로그래밍 언어 간의 본질적인 공통성과 상호 연결성을 강조합니다. 향후 작업에서는 여러 언어가 서로를 향상시킬 수 있는 이유를 탐구할 계획입니다. 또한 인기 있는 언어의 데이터를 사용하여 학습함으로써 이러한 모호하거나 덜 사용되는 프로그래밍 언어의 코드 생성 기능을 향상시키는 데 연구 결과를 활용하는 방법을 살펴볼 것입니다. 감사의 말 귀중한 피드백과 통찰력을 제공해 주신 동료들에게 감사드립니다. 이 연구 전반에 걸쳐 건설적인 도움을 주신 An Fu(Huawei), Jingyang Zhao(Huawei), Yuenan Guo(Huawei)에게 특별히 감사드립니다. 참고문헌 Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou. 대규모 언어 모델이 NL2Code를 충족함: 설문 조사. https://huggingface.co/replit/replit-code-v1-3b Shttps://huggingface.co/Phind/Phind-CodeLlama-34B-v기술 보고서 계산 언어학 협회(제1권: 장문 논문), 7443-7464쪽, 캐나다 토론토, 2023년 7월. 계산 언어학 협회. URL https://aclanthology.org/2023. acl-long. 411. 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 헨리크 폰데, 자렛 카플란, 해리슨 에드워즈, 유라 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필리프 틸렛, 펠리페 페트로스키 수치, 데이비드 W. 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 H. 거스, 알렉스 니콜, 이고르 바부슈킨, S. 아룬 발라지, 샨타누 자인, 앤드류 카, 얀 레이케, 조슈아 아키암, 베단트 미스라, 에반 모리카와, 알렉 래드포드, 매튜 M. 나이트, 마일즈 브런디지, 미라 무라티, 케이티 메이어, 피터 웰린더, 밥 맥그루, 다리오 아모데이, 샘 맥캔들리시, 일리아 수츠케버, 보이치에흐 자렘바. 코드로 훈련된 대규모 언어 모델 평가. Arxiv, abs/2107.03374, 2021. Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d&#39;Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, Oriol Vinyals. alphacode를 사용한 경쟁 수준 코드 생성. Science, 378:1092 – 1097, 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin 로빈슨, 리암 페더스, 데니 Zhou, Daphne Ippolito, David Luan, 임현택, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. PaLM: 경로를 통한 언어 모델링 확장. ArXiv, abs/2204.02311, 2022. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese 및 Caiming Xiong. CodeGen: 다중 턴 프로그램 합성 기능을 갖춘 코드용 개방형 대규모 언어 모델입니다. 학습 표현에 관한 제11차 국제 회의에서, 2023. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang 및 Jie Tang. CodeGeeX: humaneval-x에 대한 다국어 평가를 통해 코드 생성을 위한 사전 훈련된 모델입니다. ArXiv, abs/2303.17568, 2023. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco 조카, 마난 데이, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, 덴마크 계약자, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra 및 Harm de Vries. StarCoder: 소스가 함께하길!, 2023. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom 및 Gabriel Synnaeve. Code Llama: 코드를 위한 오픈 파운데이션 모델, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J. Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. ArXiv, abs/2203.02155, 2022. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang. WizardCoder: evol-instruct로 대규모 언어 모델 코드 강화. arXiv 사전 인쇄 arXiv:2306.08568, 2023.기술 보고서 Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo 및 Qianxiang Wang. PanGu-Coder2: 순위 피드백을 통해 코드용 대규모 언어 모델 강화, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao 및 Daxin Jiang. WizardLM: 복잡한 지침을 따르도록 대규모 언어 모델 지원, 2023. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer 및 Mike Lewis. InCoder: 코드 채우기 및 합성을 위한 생성 모델. 제11회 국제 학습 표현 컨퍼런스, 2023. Yue Wang, Weishi Wang, Shafiq Joty, Steven CH Hoi. CodeT5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델. 2021 자연어 처리 경험적 방법 컨퍼런스 회의록, 8696-8708쪽, 2021. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, Steven CH Hoi. Codet5+: 코드 이해 및 생성을 위한 오픈 코드 대규모 언어 모델, 2023. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, JianGuang Lou. CERT: 라이브러리 지향 코드 생성을 위한 스케치에 대한 지속적인 사전 교육. 인공 지능에 관한 국제 공동 회의에서 2022년. Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Franz Lappert, Francesco De Toni, Bernardo Garc&#39;ia del R&#39;io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, 덴마크 계약자, Luisa Villa, Jia Li, 드미트리 바다나우, Yacine Jernite, Sean Christopher Hughes, Daniel Fried, Arjun Guha, Harm de Vries 및 Leandro von Werra. SantaCoder: 별에 손을 뻗지 마세요! ArXiv, abs/2301.03988, 2023. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li. 교과서만 있으면 됩니다, 2023.
