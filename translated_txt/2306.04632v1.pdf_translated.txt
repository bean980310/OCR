--- ABSTRACT ---
StableDiffusion은 이미지 생성 및 편집 분야에서 큰 반향을 일으키고 있는 혁신적인 텍스트-이미지 생성기입니다. 픽셀 공간에서 확산 모델을 학습하는 기존 방식과 달리 StableDiffusion은 VQGAN을 통해 잠재 공간에서 확산 모델을 학습하여 효율성과 품질을 모두 보장합니다. 이미지 생성 작업을 지원할 뿐만 아니라 이미지 인페인팅 및 로컬 편집과 같은 실제 이미지에 대한 이미지 편집도 가능합니다. 그러나 StableDiffusion에 사용된 vanilla VQGAN은 상당한 정보 손실을 초래하여 편집되지 않은 이미지 영역에서도 왜곡 아티팩트를 유발하는 것으로 나타났습니다. 이를 위해 두 가지 간단한 설계를 갖춘 새로운 비대칭 VQGAN을 제안합니다. 첫째, 인코더의 입력 외에도 디코더에는 인페인팅에서 마스크되지 않은 이미지 영역과 같은 작업별 사전 정보에서 조건 분기가 포함됩니다. 둘째, 디코더는 인코더보다 훨씬 무거워서 총 추론 비용을 약간만 증가시키면서 더 자세한 복구가 가능합니다. 비대칭 VQGAN의 학습 비용은 저렴하며, 바닐라 VQGAN 인코더와 StableDiffusion을 변경하지 않고 새로운 비대칭 디코더만 다시 학습하면 됩니다. 비대칭 VQGAN은 StableDiffusion 기반 인페인팅 및 로컬 편집 방법에 널리 사용할 수 있습니다. 광범위한 실험을 통해 원래의 텍스트-이미지 기능을 유지하면서 인페인팅 및 편집 성능을 크게 향상시킬 수 있음을 보여줍니다. 코드는 https://github.com/buxiangzhiren/Asymmetric_VQGAN에서 제공됩니다. 1.
--- INTRODUCTION ---
확산 모델은 가장 인기 있는 생성 모델로 등장하여 이미지 합성에서 놀라운 결과를 달성했습니다. 초기 확산 모델은 RGB 이미지의 고차원 픽셀 공간에서 확산 프로세스를 수행했기 때문에 상당한 계산 리소스가 필요했습니다. 생성 품질을 유지하면서 학습 비용을 줄이기 위해 잠재 확산 모델(LDM)[31]은 VQGAN[10]을 사용하여 확산 단계를 저차원 잠재 공간으로 옮깁니다. 이후 개발에서 StableDiffusion은 더 큰 모델과 데이터 규모로 LDM을 더욱 확장하여 매우 강력한 일반 텍스트-이미지 생성기를 만들었습니다. 공개된 이후로 생성 AI 분야에서 상당한 주목을 받았습니다. StableDiffusion은 텍스트-이미지 생성 기능을 갖추고 있을 뿐만 아니라 인페인팅[31] 및 로컬 편집 작업[5, 24, 40]과 같은 다양한 편집 관련 작업도 지원합니다. 이러한 편집 작업의 경우 StableDiffusion은 사용자가 제공한 입력 조건에 따라 선택한 영역에 대한 새 콘텐츠를 생성하는 동시에 다른 영역을 보존하는 것을 목표로 할 수 있습니다. 그러나 모든 기존 방법[31, 40]에서 StableDiffusion 기반 편집의 결과는 특히 세분화된 구조(예: 텍스트)가 있는 영역의 경우 편집되지 않은 이미지 영역에서 왜곡 아티팩트가 발생하는 것을 관찰했습니다. 예를 들어, 그림 1에서 볼 수 있듯이 검은색 마스크 영역만 인페인팅하거나 참조 이미지에서 제공된 객체를 마스크 영역에 합성하려는 의도에도 불구하고 마스크가 아닌/편집되지 않은 영역에서 심각한 왜곡이 관찰되었습니다. 광범위한 분석 후 이러한 문제는 StableDiffusion에서 사용하는 기본 VQGAN에 존재하는 양자화 오류로 인해 발생한다는 것을 발견했습니다. 구체적으로 VQGAN은 인코더를 사용하여 이미지를 여러 번 다운샘플링하여 잠재 공간으로 보낸 다음 다운샘플링된 이미지 벡터를 코드북을 기반으로 정량화합니다. 결과적으로, 기본 VQGAN이 기본적으로 작동하기 때문에 인코더의 출력만 디코더에 공급하는 경우 편집되지 않은 영역에서도 양자화 오류가 불가피합니다.또한 추론 프로세스 동안 VQGAN의 인코더에서 사용되는 합성곱 계층은 마스크된 영역으로 인해 마스크되지 않은 영역의 피처 벡터에 영향을 미칩니다.이를 위해 디코더 부분에서 두 가지 간단하면서도 효과적인 디자인을 갖춘 새로운 비대칭 VQGAN을 제안합니다.첫째, 로컬 편집 작업을 더 잘 지원하기 위해 VQGAN 디코더를 조건부 디코더로 재구성합니다.이는 작업별 사전 정보의 정보를 통합할 수 있는 추가 분기를 통합하여 달성됩니다.예를 들어, 인페인팅 또는 로컬 편집 작업의 경우 편집되지 않은 영역을 이 분기에 공급하여 디코더가 인코더의 출력과 원래의 편집되지 않은 영역을 모두 입력으로 사용할 수 있도록 합니다.반대로 기본 VQGAN 디코더는 VQGAN 인코더의 출력만 입력으로 사용합니다. 둘째, 인코더와 유사한 복잡성 대신 더 깊거나 더 넓은 디코더를 사용하여 디코더의 기능을 향상시킵니다. 이 강력한 디코더는 편집되지 않은 영역을 더 잘 보존하고 인코더의 양자화된 출력에서 더 많은 세부 정보를 복구할 수 있습니다. StableDiffusion 추론에서 가장 시간이 많이 걸리는 부분이 반복적 확산 프로세스라는 점을 고려할 때, 이 더 큰 디코더는 총 추론 비용을 약간만 증가시킵니다. 추론 비용 외에도 비대칭 VQGAN의 학습 비용은 여전히 매우 저렴합니다. StableDiffusion과 원래 바닐라 VQGAN 인코더를 변경하지 않고 새로운 비대칭 디코더만 다시 학습하면 됩니다. 또한 작업별 사전 확률을 디코더에 번갈아가며 공급하거나 공급하지 않음으로써 비대칭 VQGAN은 작업별 사전 확률을 필요로 하는 편집 작업과 이러한 사전 입력이 필요하지 않은 텍스트-이미지 생성과 같은 순수 생성 작업을 모두 자연스럽게 지원할 수 있습니다. 비대칭 VQGAN의 효과를 보여주기 위해 세 가지 다른 작업에 대한 실험을 수행했습니다. 마스크를 사용한 인페인팅 및 로컬 편집 작업(paint-byexample [40])에서 비대칭 VQGAN은 최첨단 성능(Place 데이터 세트에서 1.03 FID [44] 및 COCOEE 데이터 세트에서 86.35% CLIP 점수 [40])을 달성했습니다. 순수한 텍스트-이미지 작업에서 우리 모델은 원래의 StableDiffusion과 비교했을 때 동등하거나 더 나은 결과를 달성할 수도 있습니다. 저희의 기여는 아래 세 가지 측면으로 요약할 수 있습니다. • 저희가 아는 한, StableDiffusion 기반 편집 방법의 왜곡 문제를 명시적으로 지적하고 조사한 것은 저희 최초의 사례입니다. • 저희는 두 가지 간단하면서도 효과적인 디자인으로 위의 왜곡 문제를 해결하기 위해 새로운 비대칭 VQGAN을 설계합니다. 일반적인 대칭 VQGAN 디자인에 비해 이 새로운 디자인은 낮은 학습 및 추론 비용을 유지하면서도 편집되지 않은 영역을 더 잘 보존하고 세부 정보를 복구할 수 있습니다. • 우리의 비대칭 VQGAN은 두 가지 대표적인 작업, 즉 장소 데이터세트[44]에 대한 내부 페인팅 작업과 COCOEE 데이터세트에 대한 로컬 편집 작업(예시로 페인팅[40])에서 최첨단 성능을 달성합니다. 2.
--- RELATED WORK ---
2.1. 확산 모델 확산 모델은 최근 진화하고 다양한 작업에서 인상적인 성능으로 인해 상당한 주목을 받고 있는 강력한 생성 모델 계열입니다.최근 연구[8, 35]에 따르면 확산 모델은 고충실도 이미지 생성에서 놀라운 결과를 얻을 수 있으며 생성적 적대 신경망보다 성능이 우수합니다.확산 모델은 복잡하고 다양한 데이터에서 모델을 학습하는 데 자연스럽게 이상적이며 최근에 많은 변형이 제안되었습니다.예를 들어, 잡음 제거 확산 확률 모델(DDPM)[13]은 잠재 변수의 마르코프 체인에서 확산 프로세스를 수행하는 방법을 학습하는 가장 인기 있는 확산 모델입니다.그리고 잡음 제거 확산 암묵적 모델(DDIM)[35]은 잡음 제거 프로세스를 가속화하기 위해 추가로 제안되었습니다.높은 생성 품질을 유지하면서 효율성을 개선하기 위해 잠재 확산 모델(LDM)[31]은 픽셀 공간이 아닌 잠재 공간 내에서 확산 모델을 학습하는 것을 제안합니다. 확산 모델은 이미지 생성[29, 26, 14, 4], 이미지 간 변환[6, 37, 38], 초고해상도[31], 이미지 편집[26, 1]을 포함한 다양한 응용 프로그램에서 매우 효과적인 것으로 입증되었습니다. 특히, 확산 모델[34]의 최근 발전으로 최첨단 이미지 합성[8, 10, 9, 26, 32, 11, 2, 31]과 텍스트[19], 오디오[18], 비디오[15]와 같은 다른 모달리티에 대한 생성 모델이 탄생했습니다. 이 논문에서는 이미지 생성 및 편집 작업을 위해 잠재 공간에서 작동하는 StableDiffusion과 같은 확산 모델의 성능을 개선할 수 있는 새로운 VQGAN 아키텍처를 설계하는 데 중점을 둡니다. 2.2. VQGAN 벡터 양자화 변분 자동 인코더(VQVAE)[27, 30]는 널리 사용되는
--- METHOD ---
픽셀 공간에서 확산 모델을 학습하는 s와 달리 StableDiffusion은 VQGAN을 통해 잠재 공간에서 확산 모델을 학습하여 효율성과 품질을 모두 보장합니다.이미지 생성 작업을 지원할 뿐만 아니라 이미지 인페인팅 및 로컬 편집과 같은 실제 이미지에 대한 이미지 편집도 가능합니다.그러나 StableDiffusion에 사용된 vanilla VQGAN은 상당한 정보 손실을 초래하여 편집되지 않은 이미지 영역에서도 왜곡 아티팩트를 유발하는 것으로 나타났습니다.이를 위해 두 가지 간단한 설계를 갖춘 새로운 비대칭 VQGAN을 제안합니다.첫째, 인코더의 입력 외에도 디코더에는 인페인팅에서 마스크되지 않은 이미지 영역과 같은 작업별 사전 정보에서 조건 분기가 포함됩니다.둘째, 디코더는 인코더보다 훨씬 무거워서 총 추론 비용을 약간만 증가시키면서 더 자세한 복구가 가능합니다. 비대칭 VQGAN의 훈련 비용은 저렴하며, 바닐라 VQGAN 인코더와 StableDiffusion을 변경하지 않고 새로운 비대칭 디코더만 다시 훈련하면 됩니다. 비대칭 VQGAN은 StableDiffusion 기반 인페인팅 및 로컬 편집 방법에 널리 사용될 수 있습니다. 광범위한
--- EXPERIMENT ---
s는 원래의 텍스트-이미지 기능을 유지하면서 인페인팅 및 편집 성능을 크게 개선할 수 있음을 보여줍니다. 코드는 https://github.com/buxiangzhiren/Asymmetric_VQGAN에서 사용할 수 있습니다. 1. 서론 확산 모델은 가장 인기 있는 생성 모델로 등장하여 이미지 합성에서 놀라운 결과를 달성했습니다. 초기 확산 모델은 RGB 이미지의 고차원 픽셀 공간에서 확산 프로세스를 수행했기 때문에 상당한 계산 리소스가 필요했습니다. 생성 품질을 유지하면서 학습 비용을 줄이기 위해 잠재 확산 모델(LDM)[31]은 VQGAN[10]을 사용하여 확산 단계를 저차원 잠재 공간으로 이동합니다. 후속 개발에서 StableDiffusion은 더 큰 모델과 데이터 규모로 LDM을 더욱 확장하여 매우 강력한 일반 텍스트-이미지 생성기를 만들었습니다. 공개된 이후 생성 AI 분야에서 상당한 주목을 받았습니다. StableDiffusion은 텍스트-이미지 생성 기능을 가지고 있을 뿐만 아니라 인페인팅[31] 및 로컬 편집 작업[5, 24, 40]과 같은 다양한 편집 관련 작업도 지원합니다. 이러한 편집 작업의 경우 StableDiffusion은 다른 영역을 보존하는 것을 목표로 하면서 사용자가 제공한 입력 조건에 따라 선택한 영역에 대한 새 콘텐츠를 생성할 수 있습니다. 그러나 모든 기존 방법[31, 40]에서 StableDiffusion 기반 편집의 결과는 특히 세분화된 구조(예: 텍스트)가 있는 영역의 경우 편집되지 않은 이미지 영역에서 왜곡 아티팩트가 발생하는 것을 관찰했습니다. 예를 들어, 그림 1에서 볼 수 있듯이 검은색 마스크 영역만 인페인팅하거나 참조 이미지에서 제공된 객체를 마스크 영역에 합성하려는 의도에도 불구하고 마스크가 아닌/편집되지 않은 영역에서 심각한 왜곡을 관찰했습니다. 광범위한 분석 후 이러한 문제는 StableDiffusion에서 사용하는 기본 VQGAN에 존재하는 양자화 오류로 인해 발생한다는 것을 발견했습니다. 구체적으로, VQGAN은 인코더를 사용하여 이미지를 여러 번 다운샘플링하여 잠재 공간으로 만든 다음, 다운샘플링된 이미지 벡터를 코드북을 기반으로 정량화합니다. 결과적으로, 기본 VQGAN이 기본적으로 작동하기 때문에 인코더의 출력만 디코더에 공급하는 경우 편집되지 않은 영역에서도 양자화 오류가 불가피합니다. 또한 추론 프로세스 동안 VQGAN의 인코더에서 사용되는 합성곱 계층은 마스크된 영역으로 인해 마스크되지 않은 영역의 특징 벡터에 영향을 미칩니다. 이를 위해 디코더 부분에 두 가지 간단하면서도 효과적인 설계를 갖춘 새로운 비대칭 VQGAN을 제안합니다. 첫째, VQGAN 디코더를 조건부 디코더로 재구성하여 로컬 편집 작업을 더 잘 지원합니다. 이는 작업별 사전 정보의 정보를 통합할 수 있는 추가 분기를 통합하여 달성됩니다. 예를 들어, 인페인팅 또는 로컬 편집 작업의 경우 편집되지 않은 영역을 이 분기에 공급하여 디코더가 인코더의 출력과 원래 편집되지 않은 영역을 모두 입력으로 사용할 수 있도록 합니다. 이와 대조적으로 vanilla VQGAN 디코더는 VQGAN 인코더의 출력만 입력으로 받습니다. 둘째, 인코더와 유사한 복잡성 대신 더 깊거나 더 넓은 디코더를 사용하여 디코더의 기능을 향상시킵니다. 이 강력한 디코더는 편집되지 않은 영역을 더 잘 보존하고 인코더의 양자화된 출력에서 더 많은 세부 정보를 복구할 수 있습니다. StableDiffusion 추론에서 가장 시간이 많이 걸리는 부분이 반복적 확산 프로세스라는 점을 고려할 때, 이 더 큰 디코더는 총 추론 비용을 약간만 증가시킵니다. 추론 비용 외에도 비대칭 VQGAN의 학습 비용은 여전히 매우 저렴합니다. StableDiffusion과 원래 vanilla VQGAN 인코더를 변경하지 않고 새로운 비대칭 디코더만 다시 학습하면 됩니다. 또한 작업별 사전 확률을 디코더에 번갈아가며 공급하거나 공급하지 않음으로써 비대칭 VQGAN은 작업별 사전 확률을 필요로 하는 편집 작업과 이러한 사전 입력이 필요하지 않은 텍스트-이미지 생성과 같은 순수 생성 작업을 자연스럽게 모두 지원할 수 있습니다. 비대칭 VQGAN의 효과를 입증하기 위해 세 가지 다른 작업에 대한 실험을 수행했습니다. 마스크를 사용한 인페인팅 및 로컬 편집 작업(paint-byexample [40])에서 비대칭 VQGAN은 최첨단 성능(Place 데이터 세트에서 1.03 FID [44] 및 COCOEE 데이터 세트에서 86.35% CLIP 점수 [40])을 달성했습니다. 순수한 텍스트-이미지 작업에서 우리 모델은 원래 StableDiffusion과 비교했을 때 비슷하거나 더 나은 결과를 얻을 수도 있습니다. 우리의 기여는 아래 세 가지 측면으로 요약할 수 있습니다. • 우리가 아는 한, StableDiffusion 기반 편집 방법의 왜곡 문제를 명시적으로 지적하고 조사한 것은 우리 최초의 사례입니다. • 우리는 두 가지 간단하면서도 효과적인 설계로 위의 왜곡 문제를 해결하기 위해 새로운 비대칭 VQGAN을 설계합니다. 일반적인 대칭 VQGAN 설계와 비교했을 때 이 새로운 설계는 낮은 학습 및 추론 비용을 유지하면서도 편집되지 않은 영역을 더 잘 보존하고 세부 정보를 복구할 수 있습니다. • 비대칭 VQGAN은 두 가지 대표적인 작업에서 최첨단 성능을 달성합니다.Place 데이터 세트[44]의 인페인팅 작업과 COCOEE 데이터 세트의 로컬 편집 작업(예시로 페인트[40])입니다.2. 관련 연구 2.1. 확산 모델 확산 모델은 다양한 작업에서 인상적인 성능을 보여 최근 진화하고 주목을 받고 있는 강력한 생성 모델 계열입니다.최근 연구[8, 35]에 따르면 확산 모델은 고충실도 이미지 생성에서 놀라운 결과를 얻을 수 있으며 생성적 적대 신경망보다 성능이 우수합니다.확산 모델은 복잡하고 다양한 데이터에서 모델을 학습하는 데 자연스럽게 이상적이며 최근 많은 변형이 제안되었습니다.예를 들어, Denoising Diffusion Probabilistic Models(DDPMs)[13]는 잠재 변수의 마르코프 체인에서 확산 프로세스를 수행하는 방법을 학습하는 가장 인기 있는 확산 모델입니다. 또한 Denoising Diffusion Implicit Models(DDIMs)[35]가 잡음 제거 프로세스를 가속화하기 위해 추가로 제안되었습니다. 높은 생성 품질을 유지하면서 효율성을 개선하기 위해 Latent Diffusion Models(LDM)[31]은 픽셀 공간이 아닌 잠재 공간 내에서 확산 모델을 학습하는 것을 제안합니다. 확산 모델은 이미지 생성[29, 26, 14, 4], 이미지 간 변환[6, 37, 38], 초고해상도[31], 이미지 편집[26, 1]을 포함한 다양한 응용 프로그램에서 매우 효과적인 것으로 입증되었습니다. 특히, 확산 모델[34]의 최근 발전으로 최첨단 이미지 합성[8, 10, 9, 26, 32, 11, 2, 31]과 텍스트[19], 오디오[18], 비디오[15]와 같은 다른 모달리티에 대한 생성 모델이 탄생했습니다. 이 논문에서는 이미지 생성 및 편집 작업을 위해 잠재 공간에서 작동하는 StableDiffusion과 같은 확산 모델의 성능을 개선할 수 있는 새로운 VQGAN 아키텍처를 설계하는 데 중점을 둡니다.2.2. VQGAN 벡터 양자화 변분 자동 인코더(VQVAE)[27, 30]는 이산 이미지 표현을 학습하는 데 널리 사용되는 방법입니다.VQ 기반 기술은 피처 도메인에서 학습된 코드북을 활용하여 이미지 생성 및 완성에 성공적으로 적용되었습니다.[27]은 이 접근 방식을 확장하여 학습된 표현의 계층을 사용했지만 이러한 방법은 여전히 합성곱 연산의 밀도 추정에 의존하여 고해상도 이미지에서 장거리 상호 작용을 캡처하는 데 어려움을 겪습니다.위의 문제를 해결하기 위해 벡터 양자화 생성적 적대 네트워크(VQGAN)[10]가 제안되었으며, 이는 강력한 잠재 공간 자동 인코더가 다음 생성 단계에 대한 이미지 세부 정보를 캡처하는 데 중요하다는 것을 식별했습니다. 따라서 VQGAN은 VQVAE의 양자화 절차를 사용하고 VQVAE의 학습된 코드북의 풍부함을 개선합니다. VQGAN은 VQVAE의 양자화 절차를 사용하고 학습된 코드북의 풍부함을 개선합니다. 구체적으로, 적대적 손실과 지각적 손실을 사용하여 첫 번째 단계에서 더 나은 자동 인코더를 학습하여 더 많은 이미지 세부 정보를 합성합니다. VQ 기반 생성 모델은 텍스트[19], 오디오[18], 이미지 인페인팅[22] 및 비디오[15] 생성을 포함한 많은 작업에 적용되었습니다. VQGAN에는 수많은 이점이 있지만, 이로 인해 발생하는 양자화 오류로 인해 이미지 세부 정보가 손실되고 심각한 왜곡이 발생할 수 있습니다. 이러한 관찰에 의해 동기를 부여받아 기존의 대칭 VQGAN 디자인과 달리 더 많은 세부 정보를 유지하기 위해 새로운 비대칭 VQGAN을 탐색하여 이미지 생성 및 편집 작업 모두에 도움이 될 수 있습니다. 마스크된 이미지 마스크된 이미지 조건 분기 2351 7 95 15 823 49 87 인코더 더 큰 디코더 Zm 양자화된 ZT 안정된 확산 Zo 인코더 23 1 251 7 95 15 8 디코더 23 49 87 Zm 양자화된 ZT 안정된 확산 Zo 출력 출력 그림 2: 위: 대칭 VQGAN의 추론 프로세스. 아래: 바닐라 VQGAN의 추론 프로세스. 3. 방법 VQGAN은 원래의 고차원 픽셀 공간을 저차원 잠재 공간으로 매핑하는 StableDiffusion에서 중요한 역할을 합니다. 그러나 이 매핑 프로세스는 이미지 조건부 작업에서 정보 손실을 초래하여 생성된 결과의 품질을 저하시키는 세부 정보 부족을 일으킬 수 있습니다. 이 섹션에서는 먼저 VQGAN의 정보 손실 문제에 대해 논의한 다음 이 과제를 해결하는 비대칭 VQGAN이라는 솔루션을 소개합니다. 3.1. VQGAN의 정보 손실 VQGAN은 픽셀 공간을 이산 잠재 공간으로 압축하는 것을 목표로 합니다. X Є RH×W×3이 입력 이미지라고 가정합니다. VQGAN은 먼저 CNN 기반 인코더를 사용하여 피처 변수 z Є Rhxwxnz를 구합니다. 여기서 h× w는 공간 해상도이고 nz는 잠재 벡터의 채널입니다. 그런 다음 VQGAN은 이를 이산 코드북 {z}1로 표현하는 것을 목표로 합니다. 여기서 각 공간 코드 zij는 코드북에서 가장 가까운 코드북 항목 Zk를 찾습니다. 이 프로세스는 다음과 같이 표시할 수 있습니다. k=1&#39; Zij = q(ij) = arg min ||Zij – Zk|| kЄ1,2,.., K (1) 여기서 q는 벡터를 코드북 인덱스에 매핑하는 양자화 인코더입니다. 양자화된 코드워드 z를 기반으로 VQGAN은 디코더를 채택하여 입력 이미지 x를 재구성합니다. 재구성된 결과가 = Dec(q(Enc(x))라고 가정합니다. 그러면 모델과 코드북은 손실 함수를 통해 종단 간에 학습할 수 있습니다. LvQ(Enc, Dec, {zk}k=1) = Lpixel + percep + ||sg[Enc(x)] − z ||22 (2) + ß ||sg [z] – Enc(X) || 여기서 Lpixel = ||x-✩||²는 픽셀 수준 손실이고 Lpercep 지각 손실은 VGG16[33] 네트워크로 계산됩니다. sg[]는 입력 마스크 m = Encoder51 7 95 15 823 49 87 또는 Conv MGB m Conv Conv m MGB &gt; m 8 ա Conv Conv Output MGB 또는 요소별 곱셈 요소별 덧셈 마스크 가이드 블렌드 랜덤 마스크 전체 마스크 비대칭 디코더 마스크된 이미지 그림 3: 대칭 디코더의 학습 과정 VQGAN. 랜덤 마스크와 풀 마스크의 두 가지 종류의 마스크를 생성합니다. 인코더의 양자화된 벡터는 비대칭 디코더에 공급됩니다. 동시에 마스크된 이미지는 조건 분기의 입력으로 디코더에 전송됩니다. 조건 분기와 주 분기가 혼합된 후 디코더는 최종 결과를 출력합니다. 정지 그래디언트 연산자, ẞ는 손실 가중치에 대한 하이퍼 매개변수입니다. 생성된 샘플의 품질을 더욱 개선하기 위해 판별기를 사용하여 인코더와 디코더로 적대적 학습 프로세스를 수행합니다. 연속 픽셀 공간이 제한된 이산 공간에 매핑되므로 이 프로세스 중에 정보 손실 현상이 발생합니다. 또한 일부 버전의 StableDiffusion에서 픽셀 공간에서 잠재 공간으로 압축하는 것이 KL-reg에 의해 학습된다는 점에 주목합니다. KL-reg는 모두 임의로 높은 분산 잠재 공간을 피하려고 하기 때문에 VQGAN과 유사한 목적을 공유하며, 둘 다 픽셀 공간에서 잠재 공간으로의 정보 손실이라는 유사한 문제를 공유합니다. 3.2. 비대칭 VQGAN StableDiffusion이 text2image 생성에서 인상적인 성능을 보여 다양한 조건부 이미지 생성 작업에 널리 적용되었습니다. 이러한 조건 중 가장 중요하고 전형적인 것은 이미지 입력을 사용하는 것입니다. 그러나 분석에 따르면 이러한 이미지 조건은 StableDiffusion의 확산 프로세스를 충족시키기 위해 잠재 공간에 매핑되어야 합니다. 결과적으로 이러한 조건부 이미지는 조작 중에 원래 정보 중 일부를 잃을 수 있습니다. 이 논문의 초점은 StableDiffusion의 사전 훈련된 가중치를 변경하지 않고 조건부 이미지 입력의 정보를 보존하는 것입니다. 이를 위해 조건부 이미지 입력의 정보를 보존하기 위해 비대칭 VQGAN을 제안합니다. 비대칭 VQGAN은 그림 2에 표시된 것처럼 원래 VQGAN과 비교하여 두 가지 핵심 설계를 포함합니다. 첫째, 이미지 조작 작업에 대한 조건부 입력을 처리하기 위한 VQGAN의 디코더에 조건부 분기를 도입합니다. 둘째, 양자화된 코드의 손실된 세부 정보를 더 잘 복구하기 위해 VQGAN에 더 큰 디코더를 설계합니다. 다음 섹션에서는 비대칭 VQGAN의 자세한 구조와 학습 전략을 소개합니다.조건부 디코더.조건부 입력의 세부 정보를 보존하는 것을 목표로 하는 조건부 디코더를 설계합니다.그림 3에서 설명한 것처럼 조건부 이미지가 마스크 m이 있는 마스크된 입력 Y라고 가정합니다.조건부 이미지 입력을 단일 레이어 피처로 압축하는 대신 다중 레벨 피처 맵으로 표현하는 것을 제안합니다.구체적으로 조건부 입력 Y를 가벼운 인코더 E에 공급하고 조건부 입력 표현으로 다른 레이어에서 피처 맵을 추출합니다.더 공식적으로는 (3) fE(Y) = {f}(Y),fE(Y),…fE(Y)}로 정의할 수 있습니다.여기서 f½ (Y)는 인코더 E의 k번째 레벨 피처 맵을 나타내고 n은 피처 레벨의 수입니다.그런 다음 이러한 피처는 마스크 가이드 블렌딩(MGB) 모듈을 통해 디코더에 통합됩니다. MGB는 인코더 E의 기능을 최대한 활용하면서 잠재 코드를 디코딩하는 디코더의 기능을 보존하는 것을 목표로 합니다. 이는 마스크를 사용하여 디코더 기능의 마스크된 영역을 직접 복사하는 동시에 인코더 E의 마스크되지 않은 영역 기능을 결합합니다. 구체적으로, 디코더의 k번째 레벨의 기능이 f/Dec(z)라고 가정합니다. 따라서 블렌딩 프로세스는 다음과 같이 공식화할 수 있습니다. fec(z) = fec(z) &gt; m + f / ( &gt;m, (4) 여기서 는 요소별 곱셈이고 m = 1 − m입니다. 이 설계된 마스크 가이드 블렌딩 모듈을 사용하면 디코더를 수정할 필요가 없고 구조는 변경하지 않은 채 디코더 네트워크에 여러 MGB 모듈을 삽입하기만 하면 됩니다. 더 큰 디코더. 주어진 잠재 코드에서 세부 정보를 복구하기 위한 디코더의 기능을 더욱 향상시키기 위해 원래 VQGAN의 디코더 모델 크기를 확대합니다. VQGAN의 모델 크기를 늘리는 것은 효율적입니다. 조건부 이미지 입력 작업을 위한 StableDiffusion의 추론 단계에서 디코더는 40-50% 마스크된 하나만 전달하면 되기 때문입니다. 방법 FID↓ LPIPS↓↓ FID↓ 모든 샘플 LPIPS↓ LaMa [36] 12.0.2.0.CoModGAN [43] 10.0.1.0.RegionWise [23] 21.0.4.0.DeepFill v2 [41] 22.0.5.0.EdgeConnect [25] 30.0.8.0.StableDiffusion* [31] 8.0.2.Ours 6.80 0.0.1.03 0.표 1: 장소의 테스트 이미지에서 512 x 512 크기의 30k 크롭에 대한 인페인팅 성능 비교 [44]. 장소의 고해상도 이미지를 사용할 수 없으므로 256 x 256 이미지를 512 x 512로 크기를 조정합니다. 40-50% 열은 이미지 영역의 40-50%를 인페인팅해야 하는 어려운 예제에 대해 계산된 메트릭을 보고합니다. *는 결과가 당사에서 재생성되었음을 나타냅니다. StableDiffusion 모델은 상당히 더 큰 모델 크기로 여러 번 전달되어야 하는 반면 시간. 훈련 전략. 학습하는 동안 비대칭 VQGAN에서 원래 VQGAN의 동일한 가중치와 코드북을 사용하고 새로운 디코더만 학습합니다. 디코더가 조건부 마스크 입력에서만 정보를 복구하는 간단한 솔루션을 개발하는 것을 방지하기 위해 두 가지 시나리오를 고려합니다. 하나는 마스크가 무작위로 생성되는 경우이고 다른 하나는 마스크가 완전히 1로 채워져 디코더가 이미지를 복구하기 위해 잠재 코드에 의존해야 하는 경우입니다. 이 두 시나리오를 학습 프로세스의 50%에 번갈아 사용합니다. 학습 목표의 경우 섹션 3.1에서 설명한 대로 픽셀 수준 손실, 지각 손실 및 적대적 손실을 사용하여 디코더의 가중치만 업데이트합니다. 비대칭 VQGAN은 가볍고 유연하며 인코딩 및 잠재 공간 확산 프로세스를 변경하지 않으므로 인코더와 StableDiffusion의 모든 사전 학습된 가중치를 활용할 수 있습니다. 다양한 작업에 StableDiffusion의 강력한 기능을 활용하면서 디코더 부분만 변경하면 됩니다. 4. 실험 범위 모델의 뛰어난 적용 가능성을 보여주기 위해 세 가지 다른 작업에서 기본 모델과 대규모 모델을 기반으로 충분한 실험을 수행했습니다.구현 세부 정보.StableDiffusion [31]의 VQGAN [10]에 사용된 교육 설정에 따라 ImageNet [7] 데이터 세트에서 비대칭 VQGAN을 교육했습니다.교육하는 동안 이미지 해상도를 256 x 256으로 사전 처리하고 기본 모델을 12에포크 동안 교육했습니다.이 프로세스는 8개의 NVIDIA VGPU에서 약 5일이 걸렸고, GPU당 배치 크기는 10이고 학습률은 처음 5,000번의 반복에서 3.6e-4까지 워밍업했습니다.학습률은 그런 다음 코사인 스케줄러로 감소했습니다.대규모 모델의 경우 64개의 NVIDIA V100 GPU를 사용하고, GPU당 배치 크기는 5이고 학습률은 처음 5,000번의 반복에서 7.2e-4까지 워밍업했습니다. 대형 모델에 대한 학습도 약 5일이 걸렸고, 학습률은 코사인 스케줄러를 사용하여 감소했습니다.평가 벤치마크.인페인팅 작업의 경우, 이미지 마스크를 생성하기 위해 최근 인페인팅 모델인 LaMa[36]와 동일한 프로토콜을 사용하여 모델을 평가했습니다.실험은 두 가지 인기 있는 데이터 세트인 Places[44]와 ImageNet[7]에서 수행되었습니다.Places 데이터 세트에는 고해상도 이미지가 없으므로 256×256 이미지의 크기를 512×512로 조정했습니다.ImageNet 데이터 세트의 경우 ImageNet 검증 데이터 세트의 각 클래스에서 무작위로 3개의 이미지를 선택하려고 합니다.그러나 일부 범주에는 사용 가능한 이미지가 3개 없기 때문에 실험을 위해 총 2,968개의 이미지를 선택하게 됩니다.페인트 바이 예제 작업의 경우 COCOEE[40] 데이터 세트를 사용하여 모델을 평가합니다. COCOEE는 MSCOCO[20] 검증 세트의 3,500개 소스 이미지를 포함하는 COCO 예시 기반 이미지 편집 벤치마크입니다. 각 이미지에는 경계 상자가 하나만 포함되고 마스크 영역은 전체 이미지의 절반을 넘지 않습니다. 해당 참조 이미지 패치는 MSCOCO 학습 세트에서 선택합니다. 텍스트-이미지 작업에서 MSCOCO[20] 검증 세트를 사용하여 모델을 평가했습니다. 널리 사용되는 &quot;Karpathy&quot; 분할[17]에 따라 5,000개 이미지를 검증에 사용했으며 각 이미지에 약 5개의 캡션이 있었습니다. 따라서 캡션에 따라 총 25,000개 이미지를 생성했습니다. 모든 이미지는 512×512로 크기가 조정되었습니다. 평가 메트릭. 인페인팅 작업에서 FID[12] 및 LPIPS[42]를 메트릭으로 사용하여 모델 예측의 품질을 평가합니다. 또한, 이미지의 편집되지 않은 영역을 보존하는 모델의 능력을 보여주기 위해, 이러한 편집되지 않은 영역에 대한 예측과 실제 결과 사이의 평균 제곱 오차(MSE)를 보고합니다.페인트 바이 예제 작업에서, 편집되지 않은 이미지 영역의 평균 제곱 오차(MSE)와 CLIP 점수[28]를 사용하여 모델 출력의 품질을 측정합니다.CLIP 점수는 편집된 영역과 참조 이미지 간의 유사성을 평가합니다.특히, 두 이미지의 크기를 224×224로 조정하고, CLIP 이미지 인코더를 통해 해당 특징을 추출하고, 코사인 유사도를 계산합니다.CLIP 점수가 높을수록 편집된 영역이 참조 이미지와 더 유사함을 나타냅니다.텍스트 대 이미지 작업에서, FID와 IS[3]를 메트릭으로 사용하여 모델의 성능을 평가합니다.4.1. 인페인팅 작업 평가 최신 방법과의 비교.표는 다른 최신 방법과의 인페인팅 접근 방식을 비교한 것입니다. 우리의 결과는 비대칭 VQGAN을 StableDiffusion에 적용하면 imMethod FID↓ LPIPS↓ Pre_error↓ StableDiffusion [31] 9.0.1082.8e-Ours(기본) 마스크 포함 7.604 0.5.7e-Ours(기본) 마스크 없음 9.0.1078.6e-Ours(대형) 마스크 포함 Ours(대형) 마스크 없음 7.0.2.6e-9.0.1047.7e-표 2: 인페인팅 작업에서 다양한 모듈의 결과. StableDiffusion이 기준선입니다. 모든 결과는 ImageNet 검증 데이터 세트에 보고됩니다. 입력 Naïve Blend 기준선 Ours 그림 4: 인페인팅 작업에서 비대칭 VQGAN의 조화. &quot;Naive Blend&quot;는 입력의 편집되지 않은 이미지 영역과 결과의 편집된 영역을 더하는 것을 나타냅니다. StableDiffusion [31]이 기준선입니다. 모든 결과는 ImageNet 검증 데이터 세트에 보고됩니다. FID를 1.24로 증명합니다. 또한, 우리의 접근 방식은 이미지 영역의 40-50%를 다시 그려야 하는 어려운 예제에서 다른 방법보다 성능이 뛰어나 조건부 분기와 더 큰 디코더의 효과를 더욱 강조합니다. 모듈의 효과. 이 절제 연구는 조건부 분기와 더 큰 디코더의 효과를 뒷받침하는 것을 목표로 합니다. 표 2는 StableDiffusion[31]에서 우리의 조건부 디코더를 원래 디코더와 비교하는 다시 그리는 결과를 보여줍니다. 구체적으로, 우리는 원래 디코더를 우리의 조건부 디코더로 대체하여 StableDiffusion에서 얻은 양자화된 벡터를 디코딩합니다. 확산 프로세스가 다양한 샘플 결과를 생성하기 때문에 다른 디코더로 전송되는 샘플 결과는 동일하다는 점에 주목할 가치가 있습니다. 우리의 기본 모델과 더 큰 모델 모두 FID에서 약 2.00, LPIPS에서 50%의 개선을 보여줍니다. 더욱이, 편집되지 않은 이미지 영역의 오류가 상당히 줄어듭니다. 이러한 결과는 조건 분기의 도움으로 VQGAN이 세부 정보를 보존하는 능력이 크게 향상되었음을 나타냅니다. 조건 분기를 사용하지 않더라도 기본 모델은 원래 디코더와 동일한 기능을 수행할 수 있습니다. 그리고 더 큰 모델은 원래 디코더보다 성능이 더 우수할 수 있습니다. 이러한 결과는 마스크가 있든 없든 모델이 다양한 애플리케이션과 호환됨을 보여줍니다. 방법 FID↓ LPIPS Pre_error_↓ Ours(기본) add. 7.0.5.7e-Ours(기본) concat. 7.0.5.3e-7.0.2.6e-4.4e-Ours(큰) add. Ours(큰) concat. 7.56 0.표 3: 다양한 블렌딩 방식에 대한 절제 연구. &quot;add.&quot;는 추가를 나타내고 &quot;concat.&quot;는 연결을 나타냅니다. 모든 결과는 ImageNet 검증 데이터 세트에 보고됩니다. 방법 혼합 확산 [1] DCCF [39] CLIP 점수 Pre_error_↓ 80.82.StableDiffusion [31] 75.Paint-by-Example [40] Paint-by-Example* [40] 84.85.우리의 86.588.0.표 4: 크기가 512 x 512인 3500개 이미지에 대한 paint-by-example [40] 성능 비교.*는 결과가 우리가 재현한다는 것을 나타냅니다.모든 결과는 COCOEE에 보고됩니다.덧셈 또는 연결.우리 모델은 마스크 유도 덧셈과 마스크 유도 연결의 두 가지 블렌딩 방법을 제공합니다.이 절제 연구는 이것들이 어떻게 VQGAN을 개선하는지 탐구하는 것을 목표로 합니다.마스크가 하드 마스크이므로 값이 0 또는 255이므로 조건부 분기는 블렌딩 방법이 마스크 유도 덧셈인 경우 부분 합성곱 계층 [21]을 사용하지 않습니다. 즉, 메인 브랜치는 편집된 영역을 담당하는 반면, 조건 브랜치는 편집되지 않은 이미지 영역을 처리합니다. 결과적으로 편집된 영역의 정보를 추론하는 부분 합성 계층은 조건 브랜치에 기여할 수 없습니다. 자세한 결과는 표 3에 나와 있습니다. 연결의 전반적인 성능은 덧셈과 비슷합니다. 마지막으로 블렌딩 방법으로 덧셈을 선택합니다. 시각적 비교. 편집되지 않은 이미지 영역의 세부 정보를 보존하기 위해 일반적인 접근 방식은 입력의 편집되지 않은 영역을 결과의 편집된 영역에 추가하여 후처리하는 것입니다. 그러나 이 순진한 솔루션GUINNESS GUINNESS ARAC INNEGUINNESS DRAG JINNE FOREIGN EXTRA GUINNESS BAGO DRAG INNE FOREIGN EXTRATO กล Toilets Toilets Cra The Cool Frappes Cool Frappes MINSEED AUS MUSEUM AND MUSEUM AND Masked Image Reference Painting by Example Ours Source Image Figure 5: Paint-by-example [40] 작업에서 비대칭 VQGAN의 보존 능력. 모든 결과는 COCOEE 데이터 세트에 보고됩니다. CLIP 점수 Pre_error_↓ 방법 Paint-by-Example* 85.588.Ours(기본) 86.1.Ours(기본) w/o mask 86.478.Ours(대형) 86.0.Ours(대형) w/o mask 86.451.방법 StableDiffusion [31] Ours(기본) w/o mask Ours(대형) w/o mask FID↓↓ IS↑ 19.88 37.19.92 37.19.75 37.표 5: paint-byexample [40] 작업에서 다양한 모듈의 결과.Paint-by-Example이 기준입니다.모든 결과는 COCOEE 데이터 세트에 보고됩니다.조건에 따라 그림 4의 세 번째 열에 표시된 대로 조화되지 않은 문제가 발생할 수 있습니다. 대조적으로, 우리의 비대칭 VQGAN은 베이스라인(원래 StableDiffusion)과 동일한 수준의 조화로 이미지를 생성하는 반면, 편집되지 않은 영역의 많은 세부 사항을 보존합니다. 이는 우리의 방법이 이미지 조화에 해를 끼치지 않고 세부 사항을 효과적으로 보존한다는 것을 나타냅니다. 전반적으로, 우리의 결과는 비대칭 디코더와 마스크 유도 추가 블렌딩 방식을 사용한 우리 모델이 조화를 유지하고 편집되지 않은 이미지 영역의 세부 사항을 보존하면서 인페인팅 작업의 성능을 크게 개선할 수 있음을 보여줍니다. 표 6: 텍스트-이미지 작업의 결과. StableDiffusion이 우리의 베이스라인입니다. 마스크가 없더라도, 우리의 비대칭 VQGAN은 베이스라인인 StableDiffusion과 동등하거나 더 우수합니다. 모든 결과는 MSCOCO에 보고됩니다. 4.2. 예제로 페인트 예제로 페인트[40]는 샘플 이미지를 기반으로 이미지 콘텐츠를 의미적으로 변경하는 새로운 이미지 편집 시나리오입니다. 그들의 접근 방식은 강력한 사전 확률로 StableDiffusion에 의존하므로 우리 모델을 그들의 방법에 쉽게 적용할 수 있습니다. 최신 방법과의 비교. 비교 결과는 표 4에 제시되어 있으며, 여기서 &quot;CLIP 점수&quot;는 편집된 이미지의 편집된 영역과 참조 이미지 간의 유사성을 나타내는 반면, &quot;Pre_error&quot;는 편집된 이미지의 편집되지 않은 이미지 영역과 소스 이미지 간의 MSE를 나타냅니다. 저희 모델은 마스크된 이미지 영역과 편집되지 않은 이미지 영역 모두에서 최상의 성능을 달성할 수 있습니다. 저희 모델이 최상의 Input Naïve Blend Baseline Ours 그림 6: paint-by-example [40] 작업에서 비대칭 VQGAN의 조화. 입력은 마스크된 소스 이미지와 참조 이미지입니다. &quot;Naive Blend&quot;는 소스 이미지의 편집되지 않은 이미지 영역과 결과의 편집된 영역을 더하는 것을 나타냅니다. Paint-by-example [40]은 저희의 기준입니다. 모든 결과는 COCOEE 데이터 세트에 보고됩니다. &quot;길을 달리는 빨간색과 흰색 버스.&quot; &quot;테니스 코트에 라켓을 든 남자가 서 있습니다.&quot; &quot;박물관의 작은 기관차가 모두 Input을 빛냈습니다. 안정적 확산 우리의 그림 7: 텍스트-이미지 작업에서 StableDiffusion과 우리의 것을 비교한 것. 조건 분기의 도움 없이도 비슷한 결과를 얻을 수 있습니다. 마스크된 이미지 영역과 편집되지 않은 이미지 영역 모두에서 성능. 모듈의 효과. 이 절제 연구의 목적은 조건 분기와 더 큰 디코더의 효과를 보여주는 것입니다. 결과는 표 5에 나와 있습니다. 인페인팅 작업과 달리 페인트 바이 예제 작업은 두 개의 다른 이미지를 결합하는 것을 포함하므로 편집되지 않은 이미지 영역에서 세부 정보가 더 심하게 손실됩니다. 이미지의 편집되지 않은 영역은 다른 이미지의 영향을 받아 작업이 더 복잡해집니다. 기본 모델은 보존 오류를 588에서 1.37로 줄이는 반면, 더 큰 모델은 0.76으로 더 줄입니다. 놀랍게도 조건 분기는 편집되지 않은 이미지 영역의 보존을 개선할 뿐만 아니라 &quot;CLIP 점수&quot; 시각적 결과로 측정한 대로 편집된 영역의 생성 품질도 향상시킵니다. 그림 5와 6에서 우리는 비대칭 VQGAN의 보존 능력과 조화를 보여주기 위해 상당수의 시각화 결과를 제공했습니다. 이러한 시각화 결과는 우리 모델이 지속적으로 성능을 향상시키는 동시에 다양한 데이터 세트의 다양한 작업에 손쉽게 적용될 수 있음을 보여줍니다. 우리는 우리 모델이 광범위한 응용 분야에서 엄청난 잠재력을 가지고 있다고 굳게 믿습니다. 4.3. 텍스트-이미지 이러한 실험은 우리의 비대칭 VQGAN이 마스크가 있는 작업(작업별 사전 확률) 외에도 마스크나 작업별 사전 확률 없이 작업을 처리할 수 있음을 보여주는 것을 목표로 합니다. 결과는 표 6에 나와 있습니다. 기본 모델이 조건 분기를 사용하지 않을 때 성능이 기준선과 비슷한 것을 볼 수 있는데, 이는 일부 마스크를 전체 마스크로 대체하는 학습 전략이 성공적이었음을 시사합니다. 더욱이 우리의 대규모 모델이 조건 분기를 사용하지 않을 때 성능이 기준선과 비슷한 것을 볼 수 있는데, 이는 더 큰 디코더가 조건 분기의 도움 없이도 더 많은 세부 정보를 복원할 수 있음을 나타냅니다. 시각적 결과. 조건부 분기 없이도 비대칭 VQGAN이 좋은 성능을 보일 수 있다는 주장을 뒷받침하기 위해 그림 7에 시각화 결과를 제시합니다.첫 번째 행에서 우리 모델은 마스크 없이도 효과적으로 작동할 수 있으며 열등한 결과를 생성하지 않는다는 것이 분명합니다.두 번째와 세 번째 행에서 더 큰 디코더가 어느 정도 더 복잡한 세부 정보를 복구할 수 있음을 관찰합니다.모듈의 효과.이 절제 연구의 목적은 조건부 분기와 더 큰 디코더의 효과를 입증하는 것입니다.결과는 표 5에 제시되어 있습니다.인페인팅 작업과 달리 페인트 바이 예제 작업은 두 개의 다른 이미지를 결합하는 것을 포함하므로 편집되지 않은 이미지 영역에서 세부 정보가 더 심각하게 손실됩니다.이미지의 편집되지 않은 영역은 다른 이미지의 영향을 받아 작업이 더욱 복잡해질 수 있습니다.기본 모델은 보존 오류를 588에서 1.37로 줄이는 반면 더 큰 모델은 이를 0.76으로 더 줄입니다. 놀랍게도, 조건부 분기는 편집되지 않은 이미지 영역의 보존을 개선할 뿐만 아니라 &quot;CLIP 점수&quot;로 측정한 대로 편집된 영역의 생성 품질도 향상시킨다는 것을 발견했습니다. 시각적 결과. 우리는 비대칭 VQGAN의 보존 능력과 조화를 보여주기 위해 그림 5와 6에 상당수의 시각화 결과를 제공했습니다. 이러한 시각화 결과는 우리 모델이 지속적으로 성능을 향상시키는 동시에 다양한 데이터 세트의 다양한 작업에 손쉽게 적용될 수 있음을 보여줍니다. 우리는 우리 모델이 광범위한 응용 분야에서 엄청난 잠재력을 가지고 있다고 굳게 믿습니다. 범위 4.4. 텍스트-이미지 이러한 실험은 우리의 비대칭 VQGAN이 마스크가 있는 작업(작업별 사전 확률) 외에도 마스크나 작업별 사전 확률 없이 작업을 처리할 수 있음을 보여주는 것을 목표로 합니다. 결과는 표 6에 나와 있습니다. 기본 모델이 조건부 분기를 사용하지 않을 때 성능이 기준선과 비슷하다는 것을 관찰할 수 있는데, 이는 일부 마스크를 전체 마스크로 대체하는 학습 전략이 성공적이었음을 시사합니다. 게다가, 우리의 대형 모델이 조건 분기를 사용하지 않을 때, 그 성능은 기준선과 비슷하여, 더 큰 디코더가 조건 분기의 도움 없이도 더 많은 세부 정보를 복원할 수 있음을 나타냅니다. 시각적 결과. 우리의 비대칭 VQGAN이 조건 분기 없이도 좋은 성능을 낼 수 있다는 주장을 뒷받침하기 위해, 그림 7에 시각화 결과를 제시합니다. 첫 번째 행에서, 우리의 모델은 마스크 없이도 효과적으로 작동할 수 있고, 열등한 결과를 생성하지 않는다는 것이 분명합니다. 두 번째와 세 번째 행에서, 우리의 더 큰 디코더가 어느 정도 더 복잡한 세부 정보를 복구할 수 있음을 관찰합니다. 5.
--- CONCLUSION ---
이 논문에서는 두 가지 새로운 디자인 기능을 갖춘 StableDiffusion을 위한 새로운 비대칭 VQGAN을 제시합니다. 첫째, 디코더는 추가 조건 분기를 통합하여 VQGAN 인코더의 출력과 작업별 사전 확률을 모두 입력으로 받을 수 있습니다. 둘째, 디코더는 인코더보다 더 복잡하도록(예: 더 깊고 넓음) 설계되어 편집되지 않은 영역의 로컬 세부 정보를 더 잘 보존하고 인코더의 양자화된 출력에서 세부 정보를 복구할 수 있습니다. 비대칭 VQGAN 아키텍처는 학습과 추론 모두에 매우 효율적이며 로컬 편집 작업과 순수한 텍스트-이미지 생성 작업에 사용할 수 있습니다. 두 가지 대표적인 작업에 대한 광범위한 실험을 통해 비대칭 VQGAN 디자인의 효과를 보여줍니다. 앞으로 디코더를 확장하면 결과의 품질을 더욱 향상시킬 수 있는지 알아볼 계획입니다. 참고문헌 [1] Omri Avrahami, Dani Lischinski, Ohad Fried. 자연스러운 이미지의 텍스트 기반 편집을 위한 혼합 확산. CVPR, 페이지 18208-18218, 2022. [2] Yogesh Balaji, 승준 나, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: 전문 노이즈 제거 장치의 앙상블을 갖춘 텍스트-이미지 확산 모델입니다. arXiv 사전 인쇄 arXiv:2211.01324, 2022. [3] Shane Barratt 및 Rishi Sharma. 개시 점수에 대한 참고 사항입니다. arXiv 사전 인쇄 arXiv:1801.01973, 2018. [4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb 및 Christian Etmann. 조건부 이미지 생성 사전 인쇄본 점수 기반 확산 모델을 사용한 Xiv.arXiv:2111.13606, 2021. [5] Tim Brooks, Aleksander Holynski 및 Alexei A Efros. Instructpix2pix: 이미지 편집 지침을 따르는 법 배우기.arXiv 사전 인쇄본 arXiv:2211.09800, 2022. [6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon 및 Sungroh Yoon. Ilvr: 확산 확률적 모델의 노이즈 제거를 위한 조건화 방법.arXiv 사전 인쇄본 arXiv:2108.02938, 2021. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li 및 Li Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스.CVPR에서, 248-255쪽. Ieee, 2009. [8] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델은 이미지 합성에서 gans를 이겼습니다. 신경 정보 처리 시스템의 발전, 34:8780-8794, 2021. [9] Patrick Esser, Robin Rombach, Andreas Blattmann 및 Bjorn Ommer. Imagebart: 자기 회귀 이미지 합성을 위한 다항 확산을 사용한 양방향 컨텍스트. 신경 정보 처리 시스템의 발전, 34:3518– 3532, 2021. [10] Patrick Esser, Robin Rombach 및 Bjorn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. CVPR, 12873-12883페이지, 2021. [11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan 및 Baining Guo. 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. CVPR, 10696-10706페이지, 2022년. [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter. 2시간 규모 업데이트 규칙으로 학습된 Gans는 로컬 내쉬 균형으로 수렴합니다. 신경 정보 처리 시스템의 발전, 30, 2017년. [13] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020년. [14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, Tim Salimans. 고충실도 이미지 생성을 위한 계단식 확산 모델. J. Mach. 학습. Res., 23(47):1-33, 2022. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet. 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2204.03458, 2022. [16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros. 조건부 적대 네트워크를 사용한 이미지 간 변환. CVPR에서, 2017. [17] Andrej Karpathy 및 Li Fei-Fei. 이미지 설명을 생성하기 위한 심층적 시각적 의미 정렬. IEEE 패턴 분석 및 머신 인텔리전스 저널, 39:664–676, 2017. [18] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro. Diffwave: 오디오 합성을 위한 다재다능한 확산 모델. arXiv 사전 인쇄본 arXiv:2009.09761, 2020. [19] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B Hashimoto. Diffusion-lm은 제어 가능한 텍스트 생성을 개선합니다. arXiv 사전 인쇄본 arXiv:2205.14217, 2022. [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 맥락 속의 공통 객체. ECCV, 740-755페이지, 2014. [21] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro. 부분 합성을 사용하여 불규칙한 구멍에 대한 이미지 인페인팅. ECCV, 85-100페이지, 2018. [22] Qiankun Liu, Zhentao Tan, Dongdong Chen, Qi Chu, Xiyang Dai, Yinpeng Chen, Mengchen Liu, Lu Yuan, Nenghai Yu. 복수적 이미지 인페인팅을 위한 변압기에서 정보 손실 감소. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 11347-11357페이지, 2022. [23] Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Aishan Liu, Dacheng Tao, Edwin R Hancock. 대규모 누락 영역에 대한 지역별 생성적 적대적 이미지 인페인팅. IEEE 사이버네틱스 저널, 2022. [24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 유도 확산 모델을 사용하여 실제 이미지를 편집하기 위한 널 텍스트 반전. arXiv 사전 인쇄본 arXiv:2211.09794, 2022. [25] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi, Mehran Ebrahimi. Edgeconnect: 적대적 에지 학습을 통한 생성적 이미지 인페인팅. arXiv 사전 인쇄본 arXiv:1901.00212, 2019. [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. Glide: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. arXiv 사전 인쇄본 arXiv:2112.10741, 2021. [27] Jialun Peng, Dong Liu, Songcen Xu, 및 Houqiang Li. 계층적 vq-vae를 사용한 이미지 인페인팅을 위한 다양한 구조 생성. CVPR, 10775-10784페이지, 2021. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각 모델 학습. ICML, 8748-8763페이지, 2021. [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, 및 Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [30] Ali Razavi, Aaron Van den Oord, Oriol Vinyals. vq-vae-2를 사용하여 다양한 고화질 이미지 생성. 신경 정보 처리 시스템의 발전, 32, 2019. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR, 1068410695페이지, 2022. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes 등. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487, 2022. [33] Karen Simonyan 및 Andrew Zisserman. 대규모 이미지 인식을 위한 매우 깊은 합성 신경망. arXiv 사전 인쇄본 arXiv:1409.1556, 2014. [34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan 및 Surya Ganguli. 비평형 열역학을 사용한 심층적 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015. [35] Jiaming Song, Chenlin Meng 및 Stefano Ermon. 확산 암시적 모델의 노이즈 제거. arXiv 사전 인쇄본 arXiv:2010.02502, 2020. [36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky. 푸리에 합성을 사용한 해상도에 강력한 대형 마스크 인페인팅. WACV, 2149-2159페이지, 2022. [37] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li. 확산 모델을 통한 의미적 이미지 합성. arXiv 사전 인쇄 arXiv:2207.00050, 2022. [38] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan 및 Houqiang Li. Sindiffusion: 단일 자연 이미지에서 확산 모델을 학습합니다. arXiv 사전 인쇄 arXiv:2211.12445, 2022. [39] Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao 및 Xing Tang. Dccf: 고해상도 이미지 조화를 위한 깊이 있고 이해하기 쉬운 컬러 필터 학습 프레임워크입니다. ECCV, 페이지 300-316, 2022. [40] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen 및 Fang Wen. 예시로 그리기: 확산 모델을 사용한 예시 기반 이미지 편집. arXiv 사전 인쇄본 arXiv:2211.13227, 2022. [41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang. 게이트 합성곱을 사용한 자유형 이미지 인페인팅. ICCV, 4471-4480페이지, 2019. [42] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 지각적 지표로서 딥 피처의 비합리적인 효과. CVPR, 586-595페이지, 2018. [43] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu. 공동 변조 생성적 적대 네트워크를 통한 대규모 이미지 완성. arXiv 사전 인쇄본 arXiv:2103.10428, 2021. [44] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva 및 Antonio Torralba. 장소: 장면 인식을 위한 1,000만 개 이미지 데이터베이스. IEEE 패턴 분석 및 머신 인텔리전스 거래, 40(6):1452–1464, 2017. 부록 다음 섹션에서는 먼저 비대칭 VQGAN에 대한 손실 목표를 소개한 다음 VQGAN과 유사한 정신을 공유하는 이미지 공간을 잠재 공간 KLreg로 압축하는 다른 기술을 제시합니다. 마지막으로 모델의 아키텍처를 소개합니다. A. 훈련 목표 비대칭 VQGAN의 훈련을 위해 인코더와 코드북의 가중치를 고정하고 재구성 및 적대적 손실을 사용하여 디코더를 훈련합니다. 재구성 손실은 픽셀 손실과 지각적 손실의 합계입니다. 픽셀 수준 손실은 양자화된 벡터 z의 출력 이미지의 각 픽셀과 입력 이미지 x 사이에 사용된 MAE 손실로, Spixel 3HW |× - ✰로 표시할 수 있습니다.여기서 x = Dec(z)는 출력 이미지를 나타내고 H와 w는 각각 이미지의 높이와 너비를 나타냅니다.또한, 지각적 손실 percep은 VGG16[33] 네트워크로 계산되며 다음과 같이 공식화할 수 있습니다.HkWk percep(x, x) = - fk (||øk (x) — øk (ŵ)||²),Lpercep = Σ percep (x, ✰) = k== (5) 여기서 k {0, 1, 2, 3, 4}, Hk 및 Wk는 각각 k번째 계층의 이미지 특징의 높이와 너비를 나타냅니다. f는 채널을 1로 줄이기 위한 1 × 1 커널을 사용한 합성곱 연산을 의미합니다. 그리고 ok(.)는 사전 학습된 VGG16 네트워크의 k번째 계층입니다. 요약하자면, 재구성 손실은 다음과 같이 일반화할 수 있습니다. = LREC(Dec) Lpixel + Lpercep (6) 생성된 결과의 품질을 개선하기 위한 또 다른 중요한 손실은 GAN 손실이며, 방정식은 다음과 같이 정의됩니다. LGAN(Dec, D): = min max Ex [log(D(x))] Dec D + Ex[log(1 − D(✩)], (7) 여기서 D는 패치 기반 판별기[16]를 나타내고, Dec는 비대칭 디코더인 생성기를 나타냅니다. 따라서 디코더 모델을 학습하기 위한 전반적인 목적은 L = LREC(Dec) + ALGAN(Dec, D)로 읽힙니다. 여기서 적응적 가중치 &gt;는 다음에 따라 계산합니다. (8) VGL]은 디코더의 마지막 계층 L에 대한 입력의 기울기를 나타내고 10-4는 수치적 안정성에 사용됩니다. B. VAEGAN 학습을 위한 KL-reg VQGAN 외에도 일부 버전의 StableDiffusion에서는 픽셀 공간에서 잠재 공간으로의 학습은 VAEGAN으로 간주할 수 있는 KL-reg에 의해 이루어진다. VAEGAN은 모두 임의로 높은 분산 잠재 공간을 피하려고 하기 때문에 VQGAN과 유사한 목적을 공유한다. VAEGAN의 경우 인코더 네트워크는 잠재 벡터의 평균과 공분산, 즉 μ와 €를 출력한다. Lkl은 사전 분포와 제안 분포 간의 갭을 줄이는 데 사용된다. Ски = (μμ+sum(exp(€) — € − 1)) . 그리고 VAEGAN 손실은 다음과 같이 공식화할 수 있다. LVAEGAN ({Enc, Dec}) = Lpixel + Lpercep + Lkl. (10) (11) 또한 품질을 개선하기 위해 GAN 손실(Eqn. 9)을 포함한다. 비대칭 VAEGAN의 학습을 위해 인코더 부분도 고정되고, 디코더를 업데이트하기 위해 Lpixel + Lpercep + LGAN(Dec, D) 손실 함수만 적용한다. Lk는 학습 과정에서 생략됩니다.C. 모델의 아키텍처 표 7에서 기본 모델의 아키텍처에 대한 세부 정보를 추가로 제시하고, 대형 모델의 아키텍처는 표 8에 나와 있습니다.D. 더 큰 디코더 인코더와 디코더 간의 기존 균형 잡힌 크기와 대조적으로, 우리는 디코더를 인코더보다 더 무겁게 설계할 것을 제안합니다(불균형 설계).이는 StableDiffusion[31]의 주요 계산 병목 현상이 VQGAN이 아니라 확산 과정에 있다는 중요한 관찰에 근거합니다.이 설계는 로컬 편집을 위한 마스크된 영역과 마스크되지 않은 영역 모두의 품질을 개선할 수 있을 뿐만 아니라 순수한 텍스트-이미지 생성 성능에도 도움이 됩니다.표 9와 표 10에서 볼 수 있듯이, 우리의 방법은 로컬 편집에서 마스크된 영역 생성 품질(전체 편집된 이미지에 대한 FID 및 LPIPS 개선)에 도움이 될 뿐만 아니라 StableDiffusion에서 원래 텍스트-이미지 생성 작업 품질도 개선하는 반면, 모든 블렌딩 방법은 이 목표를 달성할 수 없습니다. VGL [픽셀] 입력 = V GL [LGAN] +8° (9) 분기 l-thLayer/커널 PConv2d(3, 3) 출력 크기 192 x 512 xPConv2d(3, 3) 384 × 512 ×ConditionPConv2d(4, 4) 768 x 256 ×PConv2d(4, 4) 768 × 128 ×PConv2d(4, 4) 출력 Concat Conv2d(1, 1) GroupNorm (1, 1) Conv2d/(3, 3) Concat Conv2d(1, 1) ResBlock xConcat 768 x 64 x385 × 512 x192 x 512 x192 × 512 ×3 x 512 x769 × 512 x384 × 512 xBranch l-th 레이어/커널 출력 크기 조건 PConv2d / (3, 3) PConv2d(3, 3) PConv2d(4, 4) PConv2d(4, 4) PConv2d(4, 4) 128 × 512 × 256 × 512 × 256 × 512 × 128 × Conv2d(1, 1) ResBlock x 192 x 512 x 1537 x 256 × 768 x 256 × 384 × 256 × Upsample Out MainConcat Conv2d(1, 1) GroupNorm (1, 1) Conv2d(3, 3) Concat Conv2d(1, 1) ResBlock xConcat Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Conv2d/(3,3) 512 × 64 × 257 × 512 × 128 x 512 x 128 x 512 x 3 x 512 x 513 x 512 x 256 × 512 x 메인 Concat 384 x 512 x 1537 × 128 × 128 x 512 x 1025 x 256 × Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Conv2d / (3, 3) 768 x 128 × 768 x 128 × 768 x 256 × 1537 x 64 x 768 × 64 ×768 × 64 ×768 x 128 ×512 × 256 xResBlock 256 x 256 ×768 x 64 xAttnBlock 256 × 512 xResBlock 1025 × 128 ×512 × 128 ×512 × 128 ×512 × 256 ×1025 × 64 ×512 × 64 ×512 × 64 ×512 × 128 x표 8: 대형 모델의 아키텍처. 마스크 유도 블렌딩 방식은 연결입니다. &quot;PConv&quot;는 부분 합성 계층[21]을 나타냅니다. 방법 Base decoderResBlock AttnBlock ResBlock 512 x 64 xLargel decoder Large 2 decoder FID↓ LPIPS↓ 7.60 0.7.55 0.7.49 0.Pre_error↓ 5.7e-2.6e-2.1e-표 7: 기본 모델의 아키텍처. 마스크 유도 블렌딩 방식은 연결입니다. &quot;PConv&quot;는 Partial Convolutional Layer[21]를 나타냅니다. 회색 글꼴은 vanilla decoder를 나타냅니다. 표 9: 인페인팅 작업에서 더 큰 디코더의 효과성. &quot;Large decoder&quot;는 디코더의 너비와 깊이를 1.5배 늘린다는 것을 나타냅니다. &quot;Large×2 decoder&quot;는 디코더의 너비를 2배, 깊이를 2.5배 늘린다는 것을 나타냅니다. 방법 마스크 없는 기본 디코더 마스크 없는 대형 디코더 FID↓↓ IS↑ 19.92 37.19.75 37. 마스크 없는 대형x2 디코더 19.68 37. 표 10: 텍스트-이미지 작업에 대한 대형 디코더의 효과성.
