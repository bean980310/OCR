--- ABSTRACT ---
장면 동작에 대한 이미지 공간 사전 모델을 모델링하는 방법을 제시합니다. 사전 모델은 실제 비디오에서 추출한 동작 궤적 모음에서 학습합니다. 나무, 꽃, 양초, 바람에 흔들리는 옷과 같은 물체의 자연스러운 진동 역학을 묘사하는 시퀀스입니다. 푸리에 도메인에서 밀도가 높고 장기적인 동작을 스펙트럼 볼륨으로 모델링하는데, 이는 확산 모델을 사용한 예측에 적합하다는 것을 알게 되었습니다. 단일 이미지가 주어지면, 훈련된 모델은 주파수 조정 확산 샘플링 프로세스를 사용하여 스펙트럼 볼륨을 예측하는데, 이는 전체 비디오에 걸친 동작 텍스처로 변환할 수 있습니다. 이미지 기반 렌더링 모듈과 함께 예측된 동작 표현은 정지 이미지를 매끄럽게 반복되는 비디오로 바꾸거나, 사용자가 실제 이미지의 객체와 상호 작용하여 사실적인 시뮬레이션 역학을 생성하는 등 여러 다운스트림 애플리케이션에 사용할 수 있습니다(스펙트럼 볼륨을 이미지 공간 모달 기반으로 해석). 더 많은 결과는 프로젝트 페이지에서 확인하세요: generative-dynamics.github.io. 1.
--- INTRODUCTION ---
자연 세계는 항상 움직이고 있으며, 겉보기에 정적인 장면조차도 바람, 물의 흐름, 호흡 또는 기타 자연적 리듬으로 인한 미묘한 진동을 포함합니다. 이러한 동작을 에뮬레이션하는 것은 시각적 콘텐츠 합성에서 중요합니다. 동작에 대한 인간의 민감성으로 인해 동작이 없는(또는 약간 비현실적인 동작이 있는) 이미지가 기이하거나 비현실적으로 보일 수 있습니다. 인간이 장면에서 동작을 해석하거나 상상하는 것은 쉽지만, 현실적인 장면 동작을 학습하거나 생성하도록 모델을 훈련하는 것은 결코 사소한 일이 아닙니다. 우리가 세상에서 관찰하는 동작은 장면의 기본 물리적 역학, 즉 고유한 물리적 속성(질량, 탄성 등)에 따라 반응하는 물체에 적용되는 힘의 결과입니다. 이러한 양은 측정하고 규모에 맞게 포착하기 어렵습니다. 다행히도 특정 응용 프로그램에서는 이를 측정할 필요가 없습니다. 예를 들어, 관찰된 2D 동작을 분석하기만 하면 장면에서 그럴듯한 역학을 시뮬레이션할 수 있습니다[23]. 동일한 관찰된 동작은 장면 간 역학을 학습하는 데 감독 신호 역할을 할 수도 있습니다.관찰된 동작은 다중 모드이고 복잡한 물리적 효과에 기반을 두고 있지만 그럼에도 불구하고 종종 예측 가능하기 때문입니다.양초는 특정 방식으로 깜빡거리고, 나무는 흔들리고, 잎은 바스락거립니다.인간의 경우 이러한 예측 가능성은 우리의 지각 체계에 각인되어 있습니다.정지된 이미지를 보면 그럴듯한 동작을 상상할 수 있습니다.또는 그러한 동작이 많이 있을 수 있으므로 해당 이미지에 따라 자연스러운 동작의 분포를 상상할 수 있습니다.인간이 이러한 분포를 모델링할 수 있는 용이성을 감안할 때 자연스러운 연구 문제는 이를 계산적으로 모델링하는 것입니다.특히 조건부 확산 모델[44, 85, 87]에서 생성 모델의 최근 발전으로 인해 텍스트에 따라 실제 이미지의 분포를 포함하여 풍부한 분포를 모델링할 수 있게 되었습니다[73–75].이 기능을 통해 다양하고 사실적인 이미지 콘텐츠의 텍스트 조건 생성과 같은 여러 가지 새로운 응용 프로그램이 가능해졌습니다. 이러한 이미지 모델의 성공에 따라 최근 작업에서는 이러한 모델을 비디오[7, 43] 및 3D 기하학[77, 100, 101, 103]과 같은 다른 도메인으로 확장했습니다. 이 논문에서 우리는 이미지 공간 장면 동작, 즉 단일 이미지의 모든 픽셀의 동작에 대한 생성 사전을 모델링합니다. 이 모델은 대규모 실제 비디오 시퀀스 컬렉션에서 자동으로 추출된 동작 궤적에 대해 학습합니다. 특히, 각 학습 비디오에서 우리는 스펙트럼 볼륨[22, 23]의 형태로 동작을 계산합니다. 스펙트럼 볼륨은 밀도가 높고 장거리 픽셀 궤적의 주파수 영역 표현입니다. 스펙트럼 볼륨은 바람에 움직이는 나무와 꽃과 같이 진동 역학을 보이는 장면에 적합합니다. 우리는 이 표현이 장면 동작을 모델링하기 위한 확산 모델의 출력으로도 매우 효과적이라는 것을 발견했습니다. 우리는 단일 이미지를 조건으로 하여 학습된 분포에서 스펙트럼 볼륨을 샘플링할 수 있는 생성 모델을 학습합니다. 예측된 스펙트럼 볼륨은 모션 텍스처(픽셀당 장거리 모션 궤적 세트)로 직접 변환할 수 있으며, 이를 사용하여 이미지를 애니메이션화할 수 있습니다. 스펙트럼 볼륨은 또한 대화형 역학을 시뮬레이션하는 데 사용할 이미지 공간 모달 기반으로 해석할 수 있습니다[22]. 우리는 한 번에 한 주파수씩 계수를 생성하지만 공유된 주의 모듈을 통해 주파수 대역에서 이러한 예측을 조정하는 확산 모델을 사용하여 입력 이미지에서 스펙트럼 볼륨을 예측합니다. 예측된 모션은 미래 프레임을 합성하는 데 사용할 수 있습니다(이미지 기반 렌더링 모델을 통해). 그림 1에서 볼 수 있듯이 정지 이미지를 사실적인 애니메이션으로 전환합니다. 원시 RGB 픽셀에 대한 사전 확률과 비교할 때 모션에 대한 사전 확률은 픽셀 값의 장거리 변화를 효율적으로 설명하는 더 기본적이고 저차원적인 구조를 캡처합니다. 따라서 중간 모션을 생성하면 더 일관된 장기 생성과 애니메이션에 대한 더 세밀한 제어가 가능합니다. 우리는 원활한 루핑 비디오 생성, 생성된 동작 편집, 이미지 공간 모달 기반을 통한 대화형 동적 이미지 활성화(즉, 사용자가 적용한 힘에 대한 객체 역학의 응답 시뮬레이션[22])와 같은 여러 다운스트림 애플리케이션에서 훈련된 모델의 사용을 보여줍니다.
--- RELATED WORK ---
생성 합성. 생성 모델의 최근 발전으로 텍스트 프롬프트에 따라 조건화된 이미지의 사실적인 합성이 가능해졌습니다[16, 17, 24, 73-75]. 이러한 텍스트-이미지 모델은 생성된 이미지 텐서를 시간 차원을 따라 확장하여 비디오 시퀀스를 합성하도록 확장될 수 있습니다[7, 9, 43, 62, 84, 106, 106, 111]. 이러한
--- METHOD ---
영어: 스펙트럼 볼륨[23]을 생성합니다. 스펙트럼 볼륨은 푸리에 영역에서 밀도가 높고 장기적인 픽셀 궤적을 모델링하는 동작 표현입니다. 학습된 동작 사전 확률은 단일 사진을 원활하게 반복되는 비디오로 바꾸거나, 드래그 앤 릴링 포인트와 같은 사용자 입력에 응답하는 역학의 대화형 시뮬레이션으로 바꾸는 데 사용할 수 있습니다. 오른쪽에서 출력 비디오를 공간-시간 Xt 슬라이스(왼쪽에 표시된 입력 스캔라인을 따라)로 시각화합니다. 초록 장면 동작에 대한 이미지 공간 사전 확률을 모델링하는 방법을 제시합니다. 사전 확률은 실제 비디오에서 추출한 동작 궤적 모음에서 학습합니다. 나무, 꽃, 양초, 바람에 흔들리는 옷과 같은 물체의 자연스러운 진동 역학을 묘사하는 시퀀스입니다. 푸리에 영역에서 밀도가 높고 장기적인 동작을 스펙트럼 볼륨으로 모델링하는데, 이는 확산 모델을 사용한 예측에 적합하다는 것을 알게 되었습니다. 단일 이미지가 주어지면, 훈련된 모델은 주파수 조정 확산 샘플링 프로세스를 사용하여 스펙트럼 볼륨을 예측하고, 이를 전체 비디오에 걸친 모션 텍스처로 변환할 수 있습니다. 이미지 기반 렌더링 모듈과 함께 예측된 모션 표현은 정지 이미지를 원활하게 반복되는 비디오로 바꾸거나 사용자가 실제 이미지의 객체와 상호 작용하여 사실적인 시뮬레이션 역학을 생성하는 등 여러 다운스트림 애플리케이션에 사용할 수 있습니다(스펙트럼 볼륨을 이미지 공간 모달 기반으로 해석). 더 많은 결과는 프로젝트 페이지 generative-dynamics.github.io에서 확인하세요. 1. 서론 자연 세계는 항상 움직이고 있으며, 겉보기에 정적인 장면에도 바람, 물의 흐름, 호흡 또는 기타 자연적 리듬으로 인한 미묘한 진동이 포함됩니다. 이러한 모션을 에뮬레이션하는 것은 시각적 콘텐츠 합성에서 중요합니다. 모션에 대한 인간의 민감성으로 인해 모션이 없는(또는 약간 비현실적인 모션이 있는) 이미지가 기이하거나 비현실적으로 보일 수 있습니다. 사람이 장면에서 모션을 해석하거나 상상하는 것은 쉽지만, 모델을 훈련하여 사실적인 장면 모션을 학습하거나 생성하는 것은 결코 쉬운 일이 아닙니다. 세상에서 우리가 관찰하는 움직임은 장면의 근본적인 물리적 역학, 즉 고유한 물리적 속성(질량, 탄성 등)에 따라 반응하는 물체에 적용되는 힘의 결과입니다. 이러한 양은 측정하고 규모에 맞게 포착하기 어렵습니다. 다행히도 특정 응용 프로그램에서는 측정이 필요하지 않습니다. 예를 들어, 관찰된 2D 움직임을 분석하기만 하면 장면에서 그럴듯한 역학을 시뮬레이션할 수 있습니다[23]. 이 동일한 관찰된 움직임은 장면 전체에서 역학을 학습하는 데 있어 감독 신호 역할을 할 수도 있습니다. 관찰된 움직임은 다중 모드이고 복잡한 물리적 효과에 기반을 두고 있지만 그럼에도 불구하고 종종 예측 가능하기 때문입니다. 촛불은 특정 방식으로 깜빡거리고, 나무는 흔들리고, 잎사귀는 바스락거립니다. 인간의 경우 이러한 예측 가능성은 우리의 지각 체계에 각인되어 있습니다. 정지 이미지를 보면 그럴듯한 움직임을 상상할 수 있습니다. 또는 그러한 움직임이 많이 있을 수 있으므로 해당 이미지에 따라 자연스러운 움직임의 분포를 상상할 수 있습니다. 인간이 이러한 분포를 모델링할 수 있는 능력을 감안할 때, 자연스러운 연구 문제는 이를 계산적으로 모델링하는 것입니다. 생성 모델, 특히 조건부 확산 모델[44, 85, 87]의 최근 발전으로 인해 텍스트에 따라 조건화된 실제 이미지 분포[73–75]를 포함한 풍부한 분포를 모델링할 수 있게 되었습니다. 이 기능을 통해 다양하고 사실적인 이미지 콘텐츠의 텍스트 조건 생성과 같은 여러 가지 새로운 응용 프로그램이 가능해졌습니다. 이러한 이미지 모델의 성공에 따라 최근 작업에서는 이러한 모델을 비디오[7, 43] 및 3D 기하학[77, 100, 101, 103]과 같은 다른 도메인으로 확장했습니다. 이 논문에서는 이미지 공간 장면 동작, 즉 단일 이미지의 모든 픽셀 동작에 대한 생성 사전을 모델링합니다. 이 모델은 대규모 실제 비디오 시퀀스 컬렉션에서 자동으로 추출된 동작 궤적에 대해 학습합니다. 특히, 각 훈련 비디오에서 스펙트럼 볼륨[22, 23]의 형태로 동작을 계산합니다. 스펙트럼 볼륨은 밀도가 높고 장거리 픽셀 궤적의 주파수 영역 표현입니다. 스펙트럼 볼륨은 진동 역학을 보이는 장면, 예를 들어 바람에 움직이는 나무와 꽃에 적합합니다. 이 표현은 장면 동작을 모델링하기 위한 확산 모델의 출력으로도 매우 효과적이라는 것을 발견했습니다. 단일 이미지를 조건으로 하여 학습된 분포에서 스펙트럼 볼륨을 샘플링할 수 있는 생성 모델을 훈련합니다. 예측된 스펙트럼 볼륨은 모션 텍스처(픽셀당 장거리 모션 궤적 세트)로 직접 변환할 수 있으며, 이를 사용하여 이미지를 애니메이션화할 수 있습니다. 스펙트럼 볼륨은 또한 대화형 역학을 시뮬레이션하는 데 사용할 이미지 공간 모달 기반으로 해석할 수 있습니다[22]. 한 번에 한 주파수씩 계수를 생성하지만 공유된 주의 모듈을 통해 주파수 대역 전체에서 이러한 예측을 조정하는 확산 모델을 사용하여 입력 이미지에서 스펙트럼 볼륨을 예측합니다. 예측된 동작은 미래 프레임을 합성하는 데 사용할 수 있습니다(이미지 기반 렌더링 모델을 통해).그림 1에서 볼 수 있듯이 정지 이미지를 사실적인 애니메이션으로 전환합니다.원시 RGB 픽셀에 대한 사전 확률과 비교할 때 동작에 대한 사전 확률은 더 기본적이고 저차원 구조를 캡처하여 픽셀 값의 장거리 변화를 효율적으로 설명합니다.따라서 중간 동작을 생성하면 보다 일관된 장기 생성과 애니메이션에 대한 보다 세밀한 제어가 가능합니다.우리는 원활한 루핑 비디오 생성, 생성된 동작 편집, 이미지 공간 모달 기반을 통한 대화형 동적 이미지 활성화(즉, 사용자가 적용한 힘에 대한 객체 역학의 응답을 시뮬레이션)와 같은 여러 다운스트림 애플리케이션에서 훈련된 모델을 사용하는 방법을 보여줍니다[22].2. 관련 연구 생성 합성.생성 모델의 최근 발전으로 텍스트 프롬프트를 조건으로 한 이미지의 사실적인 합성이 가능해졌습니다[16, 17, 24, 73-75]. 이러한 텍스트-이미지 모델은 생성된 이미지 텐서를 시간 차원을 따라 확장하여 비디오 시퀀스를 합성하도록 확장될 수 있습니다[7, 9, 43, 62, 84, 106, 106, 111]. 이러한 방법은 실제 영상의 시공간적 통계를 포착하는 비디오 시퀀스를 생성할 수 있지만 이러한 비디오는 종종 비일관적인 동작, 텍스처의 비현실적인 시간적 변화, 질량 보존과 같은 물리적 제약 위반과 같은 아티팩트가 발생합니다. 이미지 애니메이션. 다른 기술은 전적으로 텍스트에서 비디오를 생성하는 대신 정지 사진을 입력으로 사용하여 애니메이션을 적용합니다. 최근의 많은 딥 러닝 방법은 3DUnet 아키텍처를 채택하여 비디오 볼륨을 직접 생성합니다[27, 36, 40, 47, 53, 93]. 이러한 모델은 효과적으로 동일한 비디오 생성 모델(그러나 텍스트 대신 이미지 정보를 조건으로 함)이며 위에서 언급한 모델과 유사한 아티팩트를 나타냅니다. 이러한 한계를 극복하는 한 가지 방법은 비디오 콘텐츠 자체를 직접 생성하지 않고 이미지 기반 렌더링을 통해 입력 소스 이미지에 애니메이션을 적용하는 것입니다.즉, 주행 비디오[51, 80-82, 99], 동작 또는 3D 기하 사전 확률[8, 29, 46, 63-65, 67, 90, 97, 101, 102, 104, 109] 또는 사용자 주석[6, 18, 20, 33, 38, 98, 105, 108]과 같은 외부 소스에서 파생된 동작에 따라 이미지 콘텐츠를 이동합니다.동작 필드에 따라 이미지를 애니메이션화하면 시간적 일관성과 사실성이 더 높아지지만 이러한 사전 방법은 추가 안내 신호 또는 사용자 입력이 필요하거나 제한된 동작 표현을 활용합니다.동작 모델 및 동작 사전 확률. 컴퓨터 그래픽에서 자연스러운 진동 3D 모션(예: 물결치는 물이나 바람에 흔들리는 나무)은 푸리에 영역에서 형성된 노이즈로 모델링한 다음 시간 영역 모션 필드로 변환할 수 있습니다[79, 88]. 이러한 방법 중 일부는 시뮬레이션되는 시스템의 기본 역학에 대한 모달 분석에 의존합니다[22, 25, 89]. 이러한 스펙트럼 기술은 Chuang et al.[20]에 의해 사용자 주석이 제공된 단일 2D 사진에서 식물, 물, 구름을 애니메이션화하는 데 적용되었습니다. 저희의 작업은 특히 Davis[23]에서 영감을 받았는데, 그는 장면의 모달 분석을 해당 장면의 비디오에서 관찰된 모션과 연결하고 이 분석을 사용하여 비디오에서 대화형 역학을 시뮬레이션했습니다. 저희는 Davis et al.의 주파수 공간 스펙트럼 볼륨 모션 표현을 채택하고, 이 표현을 많은 수의 교육 비디오에서 추출하고, 스펙트럼 볼륨이 확산 모델을 사용하여 단일 이미지에서 모션을 예측하는 데 적합함을 보여줍니다. 다른 방법에서는 예측 작업에서 다양한 동작 표현을 사용했는데, 여기서 이미지나 비디오를 사용하여 결정론적 미래 동작 추정[34, 71]이나 가능한 동작의 보다 풍부한 분포[94, 96, 104]를 알려줍니다. 그러나 이러한 방법 중 다수는 전체 동작 궤적이 아닌 광학 흐름 동작 추정(즉, 각 픽셀의 순간 동작)을 예측합니다. 게다가 이러한 이전 작업의 대부분은 합성 작업이 아닌 활동 인식과 같은 작업에 초점을 맞춥니다. 최근 작업에서는 인간과 동물과 같은 여러 폐쇄 도메인 설정에서 생성 모델을 사용하여 동작을 모델링하고 예측하는 이점이 있음을 보여주었습니다[2, 19, 28, 72, 91, 107]. 텍스처로서의 비디오. 특정 움직이는 장면은 비디오를 확률 과정의 시공간 샘플로 모델링하는 일종의 텍스처 용어 동적 텍스처[26]로 생각할 수 있습니다. 동적 텍스처는 파도, 불꽃 또는 움직이는 나무와 같은 부드럽고 자연스러운 움직임을 표현할 수 있으며, 비디오 분류, 분할 또는 인코딩에 널리 사용되었습니다[12–15, 76]. 비디오 텍스처라고 하는 관련된 종류의 텍스처는 움직이는 장면을 프레임 쌍 간의 전환 확률과 함께 입력 비디오 프레임 세트로 표현합니다[66, 78]. 여러 방법이 장면 동작과 픽셀 통계를 분석하여 동적 또는 비디오 텍스처를 추정하여 원활하게 반복되거나 무한히 변하는 출력 비디오를 생성하는 것을 목표로 합니다[1, 21, 32, 58, 59, 78]. 이 작업의 대부분과 달리, 우리의 방법은 사전에 사전 지식을 학습하여 단일 이미지에 적용할 수 있습니다. 3. 개요 단일 사진 Io가 주어지면, 우리의 목표는 나무, 꽃 또는 바람에 흔들리는 촛불의 불꽃과 같은 진동하는 동작을 특징으로 하는 비디오 {Î₁, Î2., ..., ÎÃ}를 생성하는 것입니다. 저희 시스템은 두 개의 모듈로 구성되어 있습니다. 동작 예측 모듈과 이미지 기반 렌더링 모듈입니다. 저희 파이프라인은 잠재 확산 모델(LDM)을 사용하여 입력 I에 대한 스펙트럼 볼륨 S = (Sfo, Sf₁,, SfK-1)을 예측하는 것으로 시작합니다. 예측된 스펙트럼 볼륨은 역 이산 푸리에 변환을 통해 동작 텍스처 F = (F1, F2, FT)로 변환됩니다. 이 동작은 모든 미래 시간 단계에서 각 입력 픽셀의 위치를 결정합니다. 예측된 동작 텍스처가 주어지면 신경 이미지 기반 렌더링 기법(5절)을 사용하여 입력 RGB 이미지에 애니메이션을 적용합니다. 6절에서 이 방법의 응용 프로그램, 즉 원활한 루핑 애니메이션 생성 및 대화형 역학 시뮬레이션을 살펴봅니다. 4. 동작 예측 4.1. 동작 표현 형식적으로 동작 텍스처는 시간에 따라 변하는 2차원 변위 맵 F = {Ft|t = 1, ..., T}의 시퀀스로, 입력 이미지 I의 각 픽셀 좌표 p에서 2차원 변위 벡터 Ft(p)는 미래 시간 t에서 해당 픽셀의 위치를 정의합니다[20]. 시간 t에서 미래 프레임을 생성하려면 해당 변위를 사용하여 I에서 픽셀을 분산시킬 수 있습니다 진폭1.X축 해상도 포함 스케일링 Y축 0.적응형 정규화 주파수 0.0.0.0.0.0.1.1.3.0Hz에서의 푸리에 계수 진폭 0.0 2.5 5.0 7.5 10.0 12.5 15.주파수(Hz) 그림 2. 왼쪽: 실제 비디오에서 추출한 X 및 Y 동작 구성 요소의 평균 전력 스펙트럼을 파란색과 녹색 곡선으로 시각화합니다. 자연스러운 진동 동작은 주로 저주파 성분으로 구성되므로 빨간색 점으로 표시된 첫 번째 K = 16 항을 사용합니다.오른쪽: (1) 이미지 너비와 높이(파란색)로 진폭을 확장하거나 (2) 주파수 적응 정규화(빨간색)한 후 3.0Hz에서 푸리에 항의 진폭 히스토그램을 보여줍니다.적응 정규화는 계수가 극단적인 값에 집중되는 것을 방지합니다.Dt를 매핑하여 전방 워핑된 이미지 I{를 생성합니다.I(p+ Ft(p)) = Io(p).(1) 목표가 모션 텍스처를 통해 비디오를 생성하는 것이라면 한 가지 선택은 입력 이미지에서 직접 시간 영역 모션 텍스처를 예측하는 것입니다.그러나 모션 텍스처의 크기는 비디오 길이에 따라 조정되어야 합니다.T개의 출력 프레임을 생성하면 T개의 변위 필드를 예측해야 합니다. 긴 비디오에 대해 이렇게 큰 출력 표현을 예측하는 것을 피하기 위해 많은 기존 애니메이션 방법은 비디오 프레임을 자기 회귀적으로 생성하거나[7, 29, 57, 60, 93], 추가 시간 임베딩을 통해 각 미래 출력 프레임을 독립적으로 예측합니다[4]. 그러나 어느 전략도 생성된 비디오의 장기적인 시간적 일관성을 보장하지 않습니다. 다행히도 많은 자연스러운 동작은 서로 다른 주파수, 진폭 및 위상으로 표현된 소수의 고조파 발진기의 중첩으로 설명할 수 있습니다[20, 23, 25, 50, 69]. 이러한 기본 동작은 준주기적이므로 주파수 영역에서 모델링하는 것이 자연스럽습니다. 따라서 Davis et al.[23]에서 그림 3에 시각화된 스펙트럼 볼륨이라고 하는 비디오의 동작에 대한 효율적인 주파수 공간 표현을 채택했습니다. 스펙트럼 볼륨은 비디오에서 추출된 픽셀당 궤적의 시간 푸리에 변환입니다. 이러한 동작 표현이 주어지면, 동작 예측 문제를 다중 모달 이미지-이미지 변환 작업으로 공식화합니다.입력 이미지에서 출력 동작 스펙트럼 볼륨으로.잠재 확산 모델(LDM)을 채택하여 4K채널 2D 동작 스펙트럼 맵으로 구성된 스펙트럼 볼륨을 생성합니다.여기서 K &lt;&lt; T는 모델링된 주파수 수이고, 각 주파수에서 x 및 y 차원의 복소 푸리에 계수를 나타내기 위해 4개의 스칼라가 필요합니다.미래 시간 단계 F(p) = {Ft(p)|t = 1, 2, ...T}에서 픽셀의 동작 궤적과 스펙트럼 볼륨 S(p) = {Sƒk (p)|k = 0, 1, .. — 1}로 표현된 것은 고속 푸리에 변환(FFT)으로 다음과 같이 관련됩니다.S(p) = FFT(F(p)).Train €Sfo ED Sfi Inference BN~N(0, 1); ⠀ 반복적 노이즈 제거 SfKS n 노이즈가 있는 잠재 fЄRBKXCxHxW° 공간 계층 재구성 f&#39;Є RBKC&#39;H&#39;×W&#39; 주파수 주의 Д 재구성 f&quot; ERBKXC&#39;xH&#39;×W&#39; 그림 3. 동작 예측 모듈. 주파수 조정된 노이즈 제거 모델을 통해 스펙트럼 볼륨 S를 예측합니다. 확산 네트워크의 각 블록은 2D 공간 계층을 주의 계층(빨간색 상자, 오른쪽)과 끼워 넣고 반복적으로 잠재 특성 zn을 노이즈 제거합니다. 노이즈가 제거된 특성은 디코더 D에 공급되어 S를 생성합니다. 학습하는 동안 다운샘플링된 입력 Io를 인코더 E를 통해 실제 동작 텍스처에서 인코딩된 노이즈가 있는 잠재 특성과 연결하고 추론 중에 노이즈 특성을 가우시안 노이즈 z로 바꿉니다(왼쪽). K 출력 주파수를 어떻게 선택해야 할까요? 실시간 애니메이션에서의 이전 작업에서는 대부분의 자연스러운 진동 동작이 주로 저주파 성분으로 구성되어 있다는 것을 관찰했습니다. [25, 69]. 이 관찰 결과를 검증하기 위해, 무작위로 샘플링한 1,000개의 5초 실제 비디오 클립에서 추출한 동작의 평균 전력 스펙트럼을 계산했습니다. 그림 2의 왼쪽 플롯에서 볼 수 있듯이, 동작의 전력 스펙트럼은 주파수가 증가함에 따라 기하급수적으로 감소합니다. 이는 대부분의 자연스러운 진동 동작이 실제로 저주파 항으로 잘 표현될 수 있음을 시사합니다. 실제로, 우리는 첫 번째 K 16 푸리에 계수가 다양한 실제 비디오와 장면에서 원래의 자연스러운 동작을 사실적으로 재현하기에 충분하다는 것을 발견했습니다. = 4.2. 확산 모델을 사용한 동작 예측 우리는 합성 품질을 유지하면서도 픽셀 공간 확산 모델보다 계산 효율성이 높은 잠재 확산 모델(LDM)[74]을 동작 예측 모듈의 백본으로 선택했습니다. 표준 LDM은 두 가지 주요 모듈로 구성됩니다. (1) 인코더 z = E(I)를 통해 입력 이미지를 잠재 공간으로 압축한 다음 디코더 I = D(z)를 통해 잠재 피처에서 입력을 재구성하는 변형 자동 인코더(VAE), (2) 가우시안 노이즈에서 시작하여 피처의 노이즈를 반복적으로 제거하는 방법을 학습하는 U-Net 기반 확산 모델입니다. 저희의 훈련은 이 프로세스를 RGB 이미지가 아닌 실제 비디오 시퀀스의 스펙트럼 볼륨에 적용합니다. 이 볼륨은 인코딩된 다음 사전 정의된 분산 일정에 따라 n단계 동안 확산되어 노이즈가 있는 잠재 객체 zn을 생성합니다. 2D U-Net은 각 단계 n = (1, 2, ..., N)에서 잠재 객체를 업데이트하는 데 사용되는 노이즈 ε0(zn; n, c)를 반복적으로 추정하여 노이즈가 있는 잠재 객체의 노이즈를 제거하도록 훈련됩니다. LDM의 훈련 손실은 CLDM = EnЄu[1,N],&lt;&quot;EN(0,1) [||e&quot; - €0 (z&quot;; n, c)||²] (3)로 작성됩니다. 여기서 c는 텍스트와 같은 조건부 신호의 임베딩이거나, 우리의 경우 훈련 비디오 시퀀스의 첫 번째 프레임인 Io입니다. 깨끗한 잠재 특징 20은 디코더를 통과하여 스펙트럼 볼륨을 복구합니다. 우리가 해결한 한 가지 문제는 동작 텍스처가 주파수에 따라 특정 분포 특성을 갖는다는 것입니다. 그림 2의 왼쪽 플롯에서 시각화한 것처럼 스펙트럼 볼륨의 진폭은 0~100의 범위를 아우르며 주파수가 증가함에 따라 대략 기하급수적으로 감소합니다. 확산 모델은 안정적인 훈련과 잡음 제거를 위해 출력의 절대값이 -1과 1 사이여야 하므로 [44], 훈련에 사용하기 전에 실제 비디오에서 추출한 S의 계수를 정규화해야 합니다. 크기를 조정하는 경우 영어: 이러한 계수를 이전 작업 [29, 77]에서와 같이 이미지 차원을 기준으로 [0,1]로 조정하면, 그림 2의 오른쪽 플롯에 표시된 것처럼 고주파수의 거의 모든 계수가 0에 가까워집니다. 이러한 데이터로 학습된 모델은 추론 중에 작은 예측 오류조차도 비정규화 후에 큰 상대 오류를 일으킬 수 있으므로 부정확한 동작을 생성할 수 있습니다. 이 문제를 해결하기 위해 간단하지만 효과적인 주파수 적응 정규화 방법을 사용합니다. 먼저, 학습 세트에서 계산된 통계를 기반으로 각 주파수에서 푸리에 계수를 독립적으로 정규화합니다. 즉, 각 개별 주파수 fj에 대해 모든 입력 샘플에서 푸리에 계수 크기의 95번째 백분위수를 계산하고 해당 값을 주파수별 스케일링 인수 sf;로 사용합니다. 그런 다음 각 스케일링된 푸리에 계수에 거듭제곱 변환을 적용하여 극단값에서 벗어나게 합니다. 실제로 제곱근은 로그 또는 역수와 같은 다른 비선형 변환보다 성능이 더 우수하다는 것을 관찰했습니다. 요약하면, 주파수 f에서 스펙트럼 볼륨 S(p)의 최종 계수 값은 (LDM을 훈련하는 데 사용됨)은 다음과 같이 계산됩니다. = sign(Sfi Sf; (p) Sfj Io (4) 그림 2의 오른쪽 플롯에서 볼 수 있듯이 주파수 적응 정규화를 적용한 후 스펙트럼 볼륨 계수가 더 균등하게 분포됩니다. 주파수 조정된 노이즈 제거. K 주파수 대역의 스펙트럼 볼륨 S를 예측하는 간단한 방법은 단일 확산 U-Net에서 4K 채널의 텐서를 출력하는 것입니다. 그러나 이전 연구 [7]에서와 같이 많은 수의 채널을 생성하도록 모델을 훈련하면 지나치게 평활화되고 정확하지 않은 출력이 생성될 수 있음을 관찰했습니다. 대안은 LDM에 추가 주파수 임베딩을 주입하여 각 개별 주파수 슬라이스를 독립적으로 예측하는 것입니다 [4]. 그러나 이러한 설계 선택은 주파수 영역에서 상관되지 않은 예측을 초래하여 비현실적인 동작으로 이어집니다. 따라서 최근의 비디오 확산 연구 [7]에서 영감을 얻어 그림 3에 표시된 주파수 조정된 노이즈 제거 전략을 제안합니다. 특히 입력 이미지가 주어지면 Io에서, 우리는 먼저 LDM ε를 훈련시켜 스펙트럼 볼륨 Sƒ,의 단일 4채널 주파수 슬라이스를 예측합니다. 여기서 우리는 시간 단계 임베딩과 함께 LDM에 추가 주파수 임베딩을 주입합니다. 그런 다음 이 LDM €0의 매개변수를 동결하고 K 주파수 대역에 걸쳐 Є의 2D 공간 레이어와 함께 삽입된 어텐션 레이어를 도입하고 미세 조정합니다. 구체적으로, 배치 크기 B의 경우, 2D 공간 레이어는 채널 크기 C&#39;의 해당 B. K 노이즈 잠재 특징을 R(E (B ·K)×C×HXW 모양의 독립 샘플로 처리합니다. 그런 다음 어텐션 레이어는 이를 주파수 축에 걸쳐 있는 연속적인 특징으로 해석하고, 어텐션 레이어에 공급하기 전에 이전 2D 공간 레이어의 잠재 특징을 RB×K×C×H×W로 재구성합니다. 다시 말해, 주파수 어텐션 레이어는 모든 주파수 슬라이스를 조정하여 일관된 스펙트럼 볼륨을 생성하도록 미세 조정됩니다. 우리의
--- EXPERIMENT ---
s에서 단일 2D U-Net에서 주파수 조정된 노이즈 제거 모듈로 전환하면 평균 VAE 재구성 오류가 0.024에서 0.018로 개선되는 것을 볼 수 있으며, 이는 LDM 예측 정확도의 상한이 개선되었음을 시사합니다. 7.3절에서는 이 설계 선택이 비디오 생성 품질을 개선한다는 사실도 보여줍니다. 5. 이미지 기반 렌더링 = 이제 주어진 입력 이미지 I에 대해 예측된 스펙트럼 볼륨 S를 취하고 시간 t에서 미래 프레임 Ît를 렌더링하는 방법을 설명합니다. 먼저 각 픽셀 F(p) FFT¹(S(p))에 적용된 역 시간 FFT를 사용하여 시간 영역에서 모션 텍스처를 도출합니다. 미래의 프레임 Ît를 생성하기 위해, 우리는 딥 이미지 기반 렌더링 기법을 채택하고 예측된 모션 필드 Ft로 스플래팅을 수행하여 그림 4와 같이 인코딩된 Io를 전방 워핑합니다. 전방 워핑은 홀을 초래할 수 있고, 여러 소스 픽셀이 동일한 출력에 매핑될 수 있기 때문에 피처 추출기 소프트맥스 스플래팅(W에 따라) ↑ 합성 네트워크 그림 4. 렌더링 모듈. 우리는 누락된 내용을 채우고 딥 이미지 기반 렌더링 모듈을 사용하여 워핑된 입력 이미지를 정제합니다. 여기서 다중 스케일 피처는 입력 이미지 Io에서 추출됩니다. 그런 다음 소프트맥스 스플래팅은 시간 0에서 t까지의 모션 필드 Ft(가중치 W에 따라)를 사용하여 피처에 적용됩니다. 워핑된 피처는 이미지 합성 네트워크에 공급되어 렌더링된 이미지 Ît를 생성합니다. 2D 위치에서, 우리는 프레임 보간에 대한 이전 작업[68]에서 제안된 피처 피라미드 소프트맥스 스플래팅 전략을 채택합니다. 구체적으로, 우리는 피처 추출기 네트워크를 통해 I를 인코딩하여 다중 스케일 피처 맵을 생성합니다. 스케일 j에서 각 개별 피처 맵의 경우, 우리는 해상도에 따라 예측된 2D 모션 필드 Ft의 크기를 조절하고 스케일링합니다. Davis et al. [22]에서와 같이, 우리는 예측된 흐름 크기를 깊이의 프록시로 사용하여 목적지 위치에 매핑된 각 소스 픽셀의 기여 가중치를 결정합니다. 특히, 우리는 예측된 모션 텍스처의 평균 크기로 픽셀당 가중치 W(p) = ½ Σt ||Ft (p)||를 계산합니다. 다시 말해, 우리는 큰 모션이 움직이는 전경 객체에 해당하고 작거나 0인 모션이 배경에 해당한다고 가정합니다. 우리는 학습 가능한 가중치 대신 모션에서 파생된 가중치를 사용합니다. 왜냐하면 단일 뷰의 경우 학습 가능한 가중치가 분리 모호성을 해결하는 데 효과적이지 않기 때문입니다. 모션 필드 Ft와 가중치 W를 사용하여 소프트맥스 스플래팅을 적용하여 각 스케일에서 피처 맵을 워핑하여 워핑된 피처를 생성합니다. 그런 다음 워핑된 피처가 이미지 합성 디코더의 해당 블록에 주입되어 최종 렌더링된 이미지 Ît를 생성합니다. 우리는 실제 비디오에서 무작위로 샘플링된 시작 및 대상 프레임(Io, It)을 사용하여 특징 추출기 및 합성 네트워크를 공동으로 훈련하고, I에서 It로 추정된 흐름 필드를 사용하여 Io에서 인코딩된 특징을 워핑하고, VGG 지각 손실로 It에 대한 예측 It을 감독합니다[49].6. 응용 프로그램 이미지-비디오.우리의 시스템은 먼저 입력 이미지에서 동작 스펙트럼 볼륨을 예측하고 스펙트럼 볼륨에서 변환된 동작 텍스처에 이미지 기반 렌더링 모듈을 적용하여 애니메이션을 생성하여 단일 정지 사진의 애니메이션을 가능하게 합니다.우리는 장면 동작을 명시적으로 모델링하므로 동작 텍스처를 선형 보간하여 슬로우모션 비디오를 생성하거나 예측된 스펙트럼 볼륨 계수의 진폭을 조정하여 애니메이션 동작을 확대(또는 축소)할 수 있습니다.매끄러운 루핑.많은 응용 프로그램에서 비디오의 시작과 끝 사이에 불연속성이 없는 원활하게 루프되는 비디오가 필요합니다.안타깝게도 훈련을 위해 원활하게 루프되는 비디오의 대규모 컬렉션을 찾기가 어렵습니다. 대신, 우리는 반복되지 않는 일반적인 비디오 클립에서 훈련된 모션 확산 모델을 사용하여 매끄러운 반복 비디오를 생성하는 방법을 고안합니다. 이미지 편집을 위한 지침에 대한 최근 작업[3, 30]에서 영감을 얻은 우리의 방법은 명시적 반복 제약을 사용하여 모션 노이즈 제거 샘플링 처리를 안내하는 모션 자체 안내 기술입니다. 특히 추론 중 각 반복적 잡음 제거 단계에서 표준 분류기 없는 안내[45]와 함께 추가적인 동작 안내 신호를 통합하여 시작 및 종료 프레임에서 각 픽셀의 위치와 속도가 가능한 한 유사하도록 적용합니다. ên = (1 + w)eo(z&quot;; n, c) – weo (zn; n, Ø) + uo^ ▼ ₂n L™ L” = ||F™ – Fr ||1 + ||VF™ – VFÏ ||1 (5) 여기서 Fr은 시간 t 및 잡음 제거 단계 n에서 예측된 2D 변위장입니다. w는 분류기 없는 안내 가중치이고 u는 동작 자체 안내 가중치입니다. 보충 비디오에서 기준 모양 기반 루핑 알고리즘[58]을 적용하여 루핑되지 않는 출력에서 루핑 비디오를 생성하고 동작 자체 안내 기술이 왜곡과 아티팩트가 적은 원활한 루핑 비디오를 생성함을 보여줍니다. 단일 이미지의 대화형 동역학. Davis et al. [22]는 특정 공진 주파수에서 평가된 스펙트럼 볼륨이 기본 장면의 진동 모드의 투영인 이미지 공간 모달 기반을 근사할 수 있고(또는 보다 일반적으로 진동 동역학에서 공간적 및 시간적 상관 관계를 포착함) 사용자 정의 힘에 대한 개체의 응답을 시뮬레이션하는 데 사용할 수 있음을 보여줍니다. 이 모달 분석 방법[22, 70]을 채택하여 개체의 물리적 응답에 대한 이미지 공간 2D 모션 변위 필드를 복소 모달 좌표 qƒ; (t)의 상태에 의해 변조된 모션 스펙트럼 계수 Sf의 가중 합으로 작성할 수 있습니다. 각 시뮬레이션된 시간 단계 t에서: Ft(p) = Σ Sƒ; (p)qƒ,(t) fj (6) 모달 공간에 표현된 분리된 질량-스프링-댐퍼 시스템에 대한 운동 방정식에 적용된 명시적 오일러 방법을 통해 모달 좌표 qf; (t)의 상태를 시뮬레이션합니다[22, 23, 70]. 전체 유도를 위해 독자들에게 보충 자료와 원본 작업을 참조하십시오.우리의 방법은 단일 사진에서 대화형 장면을 생성하는 반면, 이러한 이전 방법은 입력으로 비디오를 요구한다는 점에 유의하십시오.방법 TATS [35] 이미지 합성 FID KID 비디오 합성 FVD FVD32 DTFVD DTFVDStochastic 12V [27] 68.MCVD [93] LFDM [67] 65.8 1.3.265.6 419.6 22.40.253.5 320.9 16.41.63.2.208.6 270.4 19.53.47.1.187.5 254.3 13.45.DMVFN [48] 37.1.206.5 316.11.54.Endo et al. [29] 10.Holynski et al. [46] 11. 우리 4.0.166.0 231.5.65.0.0.179.0 253.47.1 62.7.46.2.6.표 1. 테스트 세트에 대한 양적 비교. 우리는 이미지 합성과 비디오 합성 품질을 모두 보고합니다. 여기서 KID는 100으로 조정됩니다. 모든 오류에 대해 낮을수록 좋습니다. 기준선과 오류 메트릭에 대한 설명은 Sec. 7.1을 참조하십시오. 7. 실험 구현 세부 정보. 우리는 스펙트럼 볼륨을 예측하기 위한 백본으로 LDM[74]을 사용하며, 이를 위해 차원 4의 연속 잠복 공간을 갖는 VAE를 사용합니다. 우리는 각각 1, 0.2, 10-6의 가중치를 갖는 L₁ 재구성 손실, 다중 스케일 그래디언트 일관성 손실[54-56], KL-발산 손실로 VAE를 훈련합니다. 우리는 원래 LDM 작업에서 사용된 것과 동일한 2D U-Net을 훈련하여 간단한 MSE 손실[44]로 반복적 노이즈 제거를 수행하고, 주파수 조정 노이즈 제거를 위해 [41]의 어텐션 계층을 채택합니다.정량적 평가를 위해, 우리는 공정한 비교를 위해 256×160 크기의 이미지에서 VAE와 LDM을 모두 처음부터 훈련하며, 16개의 Nvidia AGPU를 사용하여 수렴하는 데 약 6일이 걸립니다.주요 정량적 및 정성적 결과를 위해, 우리는 DDIM[86] forsteps로 동작 확산 모델을 실행합니다.또한 데이터 세트에서 사전 훈련된 이미지 인페인팅 LDM 모델[74]을 미세 조정하여 생성된 최대 512×288 해상도의 생성된 비디오를 보여줍니다.우리는 IBR 모듈의 특징 추출기에 ResNet-34[39]를 채택합니다.우리의 이미지 합성 네트워크는 조건부 이미지 인페인팅[57, 110]을 위한 아키텍처를 기반으로 합니다. 렌더링 모듈은 추론 중 Nvidia V100 GPU에서 25FPS로 실시간으로 실행됩니다. 우리는 원활한 루핑 비디오를 생성하기 위해 보편적인 지침[3]을 채택하여 가중치 w = 1.75, u = 200을 설정하고 2개의 자체 재귀 반복과 함께 500개의 DDIM 단계를 사용합니다. 데이터. 우리는 온라인 소스에서 진동 운동을 보이는 자연 장면의 비디오 3,015개를 수집하여 처리합니다. 우리는 비디오의 10%를 테스트를 위해 보류하고 나머지는 훈련에 사용합니다. 기준 진실 동작 궤적을 추출하기 위해 선택한 각 시작 이미지와 비디오의 모든 미래 프레임 사이에 대략-미세 흐름 방법[10, 61]을 적용합니다. 훈련 데이터로서 우리는 10번째 비디오 프레임을 입력 이미지로 사용하고 다음 149개 프레임에서 계산된 동작 궤적을 사용하여 해당 기준 진실 스펙트럼 볼륨을 도출합니다. 총 150,000개 이상의 이미지-동작 쌍으로 구성된 데이터입니다. 입력 이미지 참조 확률론적-12V [27] Endo 등 [29] Holynski 등 [46] 우리의 MCVD [93] 그림 5. 다양한 접근 방식으로 생성된 비디오의 Xt 슬라이스. 왼쪽에서 오른쪽으로: 기준 비디오의 입력 이미지와 해당 Xt 비디오 슬라이스, 세 개의 기준선 [27, 29, 46, 93]에서 생성된 비디오, 마지막으로 우리의 접근 방식으로 생성된 비디오. 슬링 윈도우 FID 슬링 윈도우 DT-FVDDMVFNMCVDStochastic 12V LFDMEndo 등.Holynski 등 문Ours DT-FVDMethod Repeat Io Image Synthesis FID KID Video Synthesis FVD FVD32 DTFVD DTFVD237.5 316.7 5.45.K =3.K =K =0.3.95 0.4.60.3 78.3.8.52.1 68.2.7.0.48.2 65.2.6.Frame index Frame index w/o adaptive norm. 4.0.62.7 80.3.8.그림 6. 슬라이딩 윈도우 FID와 DTFVD. 서로 다른 방법으로 생성된 비디오에 대해 윈도우 크기가 30프레임인 슬라이딩 윈도우 FID와 크기가 16프레임인 DTFVD를 보여줍니다. Independent pred. 4.Volume pred. 영어: Baseline splat [46] 4.0.Full (K = 16) 4.03 0.0.52.5 71.2.7.4.0.53.7 71.2.7.49.5 66.2.7.47.1 62.9 2.6.Baselines. 우리는 우리의 접근 방식을 최근의 단일 이미지 애니메이션 및 비디오 예측 방법과 비교합니다.Endo et al. [29] 및 DMVFN [48]은 즉각적인 2D 동작 필드를 예측하고 미래 프레임을 자동 회귀적으로 렌더링합니다.Holynski et al. [46]는 대신 단일 정적 Eulerian 동작 설명을 통해 동작을 시뮬레이션합니다.Stochastic Image-to-Video(Stochastic-I2V) [27], TATS [35] 및 MCVD [93]와 같은 다른 최근 작업은 VAE, 변환기 또는 확산 모델을 채택하여 원시 비디오 프레임을 직접 예측합니다. LFDM[67]은 확산 모델에서 흐름 볼륨을 예측하고 잠복을 워핑하여 미래 프레임을 생성합니다.우리는 각각의 오픈 소스 구현을 사용하여 위의 모든 방법을 데이터에 대해 학습합니다.¹ 우리는 우리의 접근 방식과 이전 기준선에 의해 생성된 비디오의 품질을 두 가지 방법으로 평가합니다.첫째, 우리는 이미지 합성 작업을 위해 설계된 메트릭을 사용하여 개별 합성 프레임의 품질을 평가합니다.우리는 생성된 프레임의 분포와 기준 진실 프레임 간의 평균 거리를 측정하기 위해 Fréchet Inception Distance(FID)[42]와 Kernel Inception Distance(KID)[5]를 채택합니다.둘째, 품질과 시간적 코히어런스를 평가합니다.¹우리는 Fan et al.의 [46]의 오픈 소스 구현을 사용합니다.[83].표 2. 절제 연구.7.3절에서는 각 구성을 설명합니다. 합성된 비디오의 경우, Human Kinetics 데이터 세트[52]에서 학습된 I3D 모델[11]을 기반으로 한 창 크기 16(FVD) 및 32(FVD32)의 Fréchet Video Distance[92]를 채택합니다. 생성하려는 자연스러운 진동 동작에 대한 합성 품질을 보다 충실하게 반영하기 위해, 자연스러운 동작 텍스처로 주로 구성된 데이터 세트인 Dynamic Textures Database[37]에서 학습된 I3D 모델을 사용하여 창 크기 16(DTFVD) 및 크기 32(DTFVD32)의 비디오로부터의 거리를 측정하는 Dynamic Texture Frechet Video Distance[27]도 채택합니다. 또한 [57, 60]에서와 같이 창 크기 프레임의 슬라이딩 창 FID와 창 크기가 16인 슬라이딩 창 DTFVD를 사용하여 생성된 비디오 품질이 시간이 지남에 따라 어떻게 저하되는지 측정합니다. 모든 방법에서 중앙 자르기를 통해 256×128 해상도에서 메트릭을 평가합니다. 입력 AnimateDiff ModelScope GEN-그림 7. 최근의 3개 대형 비디오 확산 모델에서 생성된 미래 프레임을 보여줍니다[31, 36, 98].7.1. 정량적 결과 표 1은 테스트 세트에서 우리의 접근 방식과 기준선 간의 정량적 비교를 보여줍니다.우리의 접근 방식은 이미지와 비디오 합성 품질 측면에서 이전의 단일 이미지 애니메이션 기준선보다 상당히 우수한 성능을 보입니다.특히, 훨씬 낮은 FVD와 DT-FVD 거리는 우리의 접근 방식으로 생성된 비디오가 더 사실적이고 시간적으로 더 일관성이 있음을 시사합니다.또한 그림 6은 다양한 방법으로 생성된 비디오의 슬라이딩 윈도우 FID와 슬라이딩 윈도우 DT-FVD 거리를 보여줍니다.전역 스펙트럼 볼륨 표현 덕분에 우리의 접근 방식으로 생성된 비디오는 시간이 지남에 따라 저하되지 않습니다.7.2. 정성적 결과 우리는 비디오 간의 정성적 비교를 생성된 비디오의 시공간적 Xt 슬라이스로 시각화합니다.이는 비디오에서 작은 동작을 시각화하는 표준적인 방법입니다[95]. 그림 5에서 보듯이, 다른 방법에 비해 생성된 비디오 역학은 해당 실제 참조 비디오(두 번째 열)에서 관찰된 동작 패턴과 더 강하게 유사합니다.Stochastic I2V[27] 및 MCVD[93]와 같은 기준선은 시간에 따른 모양과 동작을 모두 사실적으로 모델링하지 못합니다.Endo 등[29] 및 Holynski 등[46]은 아티팩트가 적은 비디오 프레임을 생성하지만 시간에 따라 지나치게 부드럽거나 진동하지 않는 동작을 보입니다.독자에게는 생성된 비디오 프레임의 품질과 다양한 방법에 따른 추정 동작을 평가하기 위한 보충 자료를 참조하시기 바랍니다.7.3. 절제 연구 우리는 동작 예측 및 렌더링 모듈에서 주요 설계 선택을 검증하기 위해 절제 연구를 수행하여 전체 구성을 다양한 변형과 비교합니다.특히, 우리는 다양한 수의 주파수 대역 K = 4, 8, 16, 24를 사용하여 결과를 평가합니다.우리는 주파수 대역의 수를 늘리면 비디오 예측 품질이 향상되지만 16개 이상의 주파수에서는 향상이 미미하다는 것을 관찰했습니다. 다음으로, 기준 진실 스펙트럼 볼륨에서 적응 주파수 정규화를 제거하고 대신 입력 이미지 너비와 높이(적응적 규범 없음)를 기준으로 크기를 조정합니다. 또한, 주파수 조정-노이즈 제거 모듈(Independent pred.)을 제거하거나 단일 2D U-net 확산 모델을 통해 4K 채널 스펙트럼 볼륨의 텐서 볼륨을 공동으로 예측하는 더 간단한 DM으로 대체합니다(Volume pred.). 마지막으로, 그림 8에서와 같이 기준 렌더링 방법을 사용하여 결과를 비교합니다. 제한 사항. 렌더링된 미래 프레임(짝수)과 입력 및 렌더링된 이미지의 오버레이(홀수)의 예를 보여줍니다. 이 방법은 얇은 물체나 큰 동작의 영역과 많은 양의 새 콘텐츠를 채워야 하는 영역에 아티팩트를 생성할 수 있습니다. [46]에서 사용하는 학습 가능한 가중치가 적용되는 단일 스케일 피처에 소프트맥스 스플래팅을 적용합니다(Baseline splat). 또한 생성된 비디오가 입력 이미지를 N번 반복하여 볼륨이 되는 기준선을 추가합니다(Repeat Io). 표 2에서 우리는 모든 더 간단하거나 대체적인 구성이 전체 모델과 비교했을 때 더 나쁜 성능을 가져온다는 것을 관찰했습니다.7.4. 대형 비디오 모델과 비교 우리는 또한 사용자 연구를 수행하고 생성된 애니메이션을 최근의 대형 비디오 확산 모델인 AnimateDiff[36], ModelScope[98] 및 Gen-2[31]의 애니메이션과 비교했습니다.이들은 비디오 볼륨을 직접 예측합니다.테스트 세트에서 무작위로 선택한 30개 비디오에서 사용자에게 &quot;어떤 비디오가 더 사실적입니까?&quot;라고 묻습니다.사용자는 다른 접근 방식보다 우리의 접근 방식을 80.9% 선호한다고 보고합니다.또한 그림 7에서 볼 수 있듯이 이러한 기준선에서 생성된 비디오는 입력 이미지 콘텐츠를 고수하지 못하거나 시간이 지남에 따라 점진적인 색상 드리프트 및 왜곡을 보입니다.전체 비교를 위해 독자는 보충 자료를 참조하세요.8. 토론 및
--- CONCLUSION ---
제한 사항. 저희의 접근 방식은 스펙트럼 볼륨의 낮은 주파수만 예측하기 때문에 비진동 모션이나 고주파 진동을 모델링하지 못할 수 있습니다. 이는 학습된 모션 기반을 사용하여 해결할 수 있습니다. 더욱이 생성된 비디오의 품질은 기본 모션 궤적의 품질에 따라 달라지는데, 이는 얇은 움직이는 물체나 큰 변위가 있는 물체가 있는 장면에서 저하될 수 있습니다. 정확하더라도, 보이지 않는 새로운 콘텐츠를 대량으로 생성해야 하는 모션도 저하를 일으킬 수 있습니다(그림 8). 결론. 저희는 단일 정지 사진에서 자연스러운 진동 역학을 모델링하기 위한 새로운 접근 방식을 제시합니다. 저희의 이미지 공간 모션 사전은 스펙트럼 볼륨으로 표현되는데, 이는 픽셀당 모션 궤적의 주파수 표현으로, 확산 모델을 사용한 예측에 효율적이고 효과적이라고 생각되며, 실제 세계 비디오 모음에서 학습합니다. 스펙트럼 볼륨은 주파수 조정된 잠재 확산 모델을 사용하여 예측되며 이미지 기반 렌더링 모듈을 통해 미래 비디오 프레임을 애니메이션화하는 데 사용됩니다. 우리는 우리의 접근 방식이 단일 사진에서 사실적인 애니메이션을 생성하고 이전 기준선보다 상당히 우수하며, 원활하게 반복되거나 대화형 이미지 역학을 만드는 것과 같은 여러 다운스트림 애플리케이션을 지원할 수 있음을 보여줍니다.감사의 말.유익한 토론과 도움이 되는 의견을 주신 Abe Davis, Rick Szeliski, Andrew Liu, Boyang Deng, Qianqian Wang, Xuan Luo, Lucy Chai에게 감사드립니다.참고문헌 [1] Aseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh Agrawala, Michael Cohen, Brian Curless, David Salesin, Richard Szeliski.파노라마 비디오 텍스처.ACM Trans. Graphics(SIGGRAPH), 821-827쪽.2005. [2] Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee.3D 동작 예측에 확산 확률적 모델을 사용할 수 있을까요? arXiv 사전 인쇄본 arXiv:2302.14503, 2023. [3] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein. 확산 모델을 위한 범용 지침. Proc. Computer Vision and Pattern Recognition(CVPR), 843-852페이지, 2023. [4] Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, ChunHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio Escalera, Duygu Ceylan. 바람에 날리다: 정지 이미지에서 인간 시네마그래프를 위한 Cyclenet. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 459-468쪽, 2023. [5] Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, Arthur Gretton. MMD GAN의 신비 해제. arXiv 사전 인쇄본 arXiv:1801.01401, 2018. [6] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer. ipoke: 제어된 확률적 비디오 합성을 위한 정지 이미지 찌르기. Proc. Int. Conf. on Computer Vision(ICCV), 14707-14717쪽, 2021. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis. 잠재체 정렬: 잠재체 확산 모델을 사용한 고해상도 비디오 합성. Proc. Computer Vision and Pattern Recognition(CVPR), 22563-22575페이지, 2023. [8] Richard Strong Bowen, Richard Tucker, Ramin Zabih, Noah Snavely. 움직임의 차원: 흐름 부분 공간을 통한 단안 예측. International Conference on 3D Vision(3DV), 454-464페이지, 2022. [9] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, Tero Karras. 동적 장면의 긴 비디오 생성. Neural Information Processing Systems, 35:3176931781, 2022. [10] Thomas Brox, Andrés Bruhn, Nils Papenberg, Joachim Weickert. 워핑 이론을 기반으로 한 고정밀 광학 흐름 추정. Proc. European Conf. on Computer Vision(ECCV), 25-36페이지, 2004년. [11] Joao Carreira 및 Andrew Zisserman. Quo vadis, 동작 인식? 새로운 모델 및 동역학 데이터 세트. Proc. Computer Vision and Pattern Recognition(CVPR), 6299-6308페이지, 2017년. [12] Dan Casas, Marco Volino, John Collomosse 및 Adrian Hilton. 대화형 캐릭터 모양을 위한 4D 비디오 텍스처. Computer Graphics Forum, 33권, 371-380페이지. Wiley Online Library, 2014년. [13] Antoni B Chan 및 Nuno Vasconcelos. 동적 텍스처의 혼합. Proc. Int. Conf. on Computer Vision(ICCV), 641-647페이지, 2005년. [14] Antoni B Chan 및 Nuno Vasconcelos. 영어: 커널 동적 텍스처를 사용한 비디오 분류.Proc. Computer Vision and Pattern Recognition (CVPR), 2007에서. [15] Antoni B Chan 및 Nuno Vasconcelos. 동적 텍스처의 혼합을 사용한 비디오 모델링, 클러스터링 및 분할.Trans. Pattern Analysis and Machine Intelligence, 30(5):909– 926, 2008에서. [16] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: 마스크된 생성 변환기를 통한 텍스트-이미지 생성.arXiv 사전 인쇄본 arXiv:2301.00704, 2023에서. [17] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu 및 William T Freeman. Maskgit: 마스크된 생성 이미지 변환기.Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 11315-11325페이지, 2022. [18] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, Ming-Hsuan Yang. 제어 가능한 비디오 합성을 위한 동작 조건 확산 모델. arXiv 사전 인쇄본 arXiv:2304.14404, 2023. [19] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Gang Yu. 잠재 공간에서 동작 확산을 통해 명령 실행. Proc. Computer Vision and Pattern Recognition(CVPR), 18000-18010페이지, 2023. [20] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, Richard Szeliski. 확률적 모션 텍스처로 그림 애니메이션화. ACM Trans. Graphics(SIGGRAPH), 853-860페이지, 2005년. [21] Vincent C Couture, Michael S Langer 및 Sebastien Roy. 고스트 없는 옴니스테레오 비디오 텍스처. International Conference on 3D Vision, 64-70페이지. IEEE, 2013년. [22] Abe Davis, Justin G Chen 및 Frédo Durand. 비디오에서 객체를 그럴듯하게 조작하기 위한 이미지 공간 모달 기반. ACM Trans. Graphics(SIGGRAPH), 34(6):1–7, 2015년. [23] Myers Abraham Davis. 시각적 진동 분석. 매사추세츠 공과대학 박사 학위 논문, 2016년. [24] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 간을 이김. 영어: Neural Information Processing Systems, 34:8780-8794, 2021. [25] Julien Diener, Mathieu Rodriguez, Lionel Baboud 및 Lionel Reveret. 나무의 실시간 애니메이션을 위한 풍향 투영 기준. Computer graphics forum, 28권, 533-540페이지. Wiley Online Library, 2009. [26] Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu 및 Stefano Soatto. 동적 텍스처. 51:91-109, 2003. [27] Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis 및 Bjorn Ommer. cinns를 사용한 확률적 이미지-비디오 합성. Proc. Computer Vision and Pattern Recognition (CVPR), 3742-3753페이지, 2021년 6월. [28] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, 및 Artsiom Sanakoyeu. 아바타가 다리를 키운다: 확산 모델을 사용하여 희소 추적 입력에서 부드러운 인간 동작 생성. Proc. Computer Vision and Pattern Recognition (CVPR), 481-490페이지, 2023년. [29] Yuki Endo, Yoshihiro Kanamori, 및 Shigeru Kuriyama. 풍경 애니메이션: 단일 이미지 비디오 합성을 위한 분리된 동작 및 모양의 자체 감독 학습. ACM Trans. Graphics (SIGGRAPH Asia), 38(6):175:1175:19, 2019년. [30] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, 및 Aleksander Holynski. 제어 가능한 이미지 생성을 위한 확산 자체 안내. arXiv 사전 인쇄본 arXiv:2306.00986, 2023. [31] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog 및 Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 안내 비디오 합성. Proc. Int. Conf. on Computer Vision(ICCV), 73467356페이지, 2023. [32] Matthew Flagg, Atsushi Nakazawa, Qiushuang Zhang, Sing Bing Kang, Young Kee Ryu, Irfan Essa 및 James M Rehg. 인간 비디오 텍스처. In Proceedings of thesymposium on Interactive 3D graphics and games, pages 199-206, 2009. [33] Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. In International Conference on Machine Learning, pages 3233-3246. PMLR, 2020. [34] Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2Flow: 동작 인식을 위한 정적 이미지에서의 동작 환각. In Proc. Computer Vision and Pattern Recognition (CVPR), 2018. [35] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. 시간 독립적 vqgan과 시간 민감형 변압기를 사용한 긴 비디오 생성. arXiv 사전 인쇄본 arXiv:2204.03638, 2022. [36] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. Animatediff: 특정 튜닝 없이 개인화된 텍스트-이미지 확산 모델을 애니메이션화합니다. arXiv 사전 인쇄본 arXiv:2307.04725, 2023. [37] Isma Hadji 및 Richard P Wildes. 컨브넷 이해에 응용된 새로운 대규모 동적 텍스처 데이터 세트. Proc. European Conf. on Computer Vision(ECCV), 320-335페이지, 2018. [38] Zekun Hao, Xun Huang, Serge Belongie. 희소 궤적을 사용한 제어 가능한 비디오 생성. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 7854–7863, 2018. [39] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Jian Sun. 이미지 인식을 위한 심층 잔여 학습. Proc에서 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 770778, 2016. [40] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. 스토리 애니메이션: 검색 증강 비디오 생성을 통한 스토리텔링. arXiv 사전 인쇄 arXiv:2307.06940, 2023. [41] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan 및 Qifeng Chen. 임의 길이의 고화질 비디오 생성을 위한 잠재 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13221, 2022. [42] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler 및 Sepp Hochreiter. 2개 시간 척도 업데이트 규칙으로 학습된 Gans는 로컬 내쉬 균형으로 수렴합니다. 신경 정보 처리 시스템, 30, 2017. [43] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. [44] Jonathan Ho, Ajay Jain 및 Pieter Abbeel. 노이즈 제거 확산 확률 모델. 신경 정보 처리 시스템, 33:6840-6851, 2020. [45] Jonathan Ho 및 Tim Salimans. 분류기 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022. [46] Aleksander Holynski, Brian L Curless, Steven M Seitz 및 Richard Szeliski. 오일러 운동장을 사용하여 그림 애니메이션. Proc. Computer Vision and Pattern Recognition(CVPR), 5810-5819페이지, 2021. [47] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen 및 Andrea Dittadi. 비디오 예측 및 채우기를 위한 확산 모델. Trans. Mach. Learn. Res., 2022, 2022. [48] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, Shuchang Zhou. 비디오 예측을 위한 동적 다중 스케일 폭셀 흐름 네트워크. ArXiv, abs/2303.09875, 2023. [49] Justin Johnson, Alexandre Alahi, Li Fei-Fei. 실시간 스타일 전송 및 초고해상도를 위한 지각 손실. Proc. European Conf. on Computer Vision(ECCV), 694-711페이지, 2016. [50] Hitoshi Kanda와 Jun Ohya. 3D 식물 나무의 동적 동작을 애니메이션화하기 위한 효율적이고 사실적인 방법. International Conference on Multimedia and Expo, 2권, II-89페이지. IEEE, 2003. [51] Johanna Karras, Aleksander Holynski, Ting-Chun Wang 및 Ira Kemelmacher-Shlizerman. Dreampose: 안정된 확산을 통한 패션 이미지-비디오 합성. arXiv 사전 인쇄본 arXiv:2304.06025, 2023. [52] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 운동학 인간 행동 비디오 데이터 세트. arXiv 사전 인쇄본 arXiv:1705.06950, 2017. [53] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn 및 Sergey Levine. 확률적 적대적 비디오 예측. arXiv 사전 인쇄본 arXiv:1804.01523, 2018. [54] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T Freeman. 얼어붙은 사람을 관찰하여 움직이는 사람의 깊이를 배우다.Proc. Computer Vision and Pattern Recognition(CVPR), 4521-4530페이지, 2019. [55] Zhengqi Li와 Noah Snavely. Megadepth: 인터넷 사진에서 단일 시점 깊이 예측 학습.Proc. Computer Vision and Pattern Recognition(CVPR), 2041-2050페이지, 2018. [56] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, Noah Snavely. Dynibar: 신경 동적 이미지 기반 렌더링.Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 4273-4284페이지, 2023. [57] Zhengqi Li, Qianqian Wang, Noah Snavely 및 Angjoo Kanazawa. Infinitenature-zero: 단일 이미지에서 자연스러운 장면의 영구적 뷰 생성 학습. Proc. European Conf. on Computer Vision(ECCV), 515-534페이지. Springer, 2022. [58] Jing Liao, Mark Finch 및 Hugues Hoppe. 원활한 비디오 루프의 빠른 계산. ACM Trans. Graphics(SIGGRAPH), 34(6):1-10, 2015. [59] Zicheng Liao, Neel Joshi 및 Hugues Hoppe. 점진적 역동성을 갖춘 자동화된 비디오 루핑. ACM Transactions on Graphics(TOG), 32(4):1-10, 2013. [60] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, Angjoo Kanazawa. 무한한 자연: 단일 이미지에서 자연스러운 장면의 영구적 뷰 생성. Proc. Int. Conf. on Computer Vision(ICCV)에서, 14458-14467페이지, 2021. [61] Ce Liu. 픽셀 너머: 동작 분석을 위한 새로운 표현과 응용 프로그램 탐색. 매사추세츠 공과대학 박사 학위 논문, 2009. [62] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan. 비디오 퓨전: 고품질 비디오 생성을 위한 분해된 확산 모델. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 10209-10218페이지, 2023. [63] Aniruddha Mahapatra 및 Kuldeep Kulkarni. 정지 이미지에서 유체 요소의 제어 가능한 애니메이션. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 2022. [64] Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov 및 Jun-Yan Zhu. 오일러리안 시네마그래프의 텍스트 가이드 합성. 2023. [65] Arun Mallya, Ting-Chun Wang 및 Ming-Yu Liu. 이미지 세트를 사용한 애니메이션을 위한 암묵적 워핑. 영어: Neural Information Processing Systems, 35:22438-22450, 2022. [66] Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A Efros, Trevor Darrell. 비트에 맞춰 스트러밍: 오디오 조절된 대조 비디오 텍스처.Proc. Winter Conference on Applications of Computer Vision, 3761-3770페이지, 2022. [67] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, Martin Renqiang Min. 잠재 흐름 확산 모델을 사용한 조건부 이미지-비디오 생성.Proc. Computer Vision and Pattern Recognition (CVPR), 18444-18455페이지, 2023. [68] Simon Niklaus와 Feng Liu. 비디오 프레임 보간을 위한 Softmax 스플래팅.Proc. Computer Vision and Pattern Recognition (CVPR), 5437-5446페이지, 2020. [69] Shin Ota, Machiko Tamura, Kunihiko Fujita, T Fujimoto, K Muraoka, Norishige Chiba. 바람장에서 흔들리는 나무의 1/f/sup/spl beta//노이즈 기반 실시간 애니메이션. Proceedings Computer Graphics International, 52-59페이지. IEEE, 2003. [70] Automne Petitjean, Yohan Poirier-Ginter, Ayush Tewari, Guillaume Cordonnier, George Drettakis. Modalnerf: 동적으로 진동하는 장면에서 자유 시점 탐색을 위한 신경 모달 분석 및 합성. Computer Graphics Forum, 42권, 2023. [71] Silvia L. Pintea, Jan C. van Gemert, Arnold WM Smeulders. 영어: Déjà vu: 정적 이미지에서의 동작 예측. Proc. European Conf. on Computer Vision(ECCV), 2014에서. [72] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, Daniel Cohen-Or. 단일 동작 확산. arXiv 사전 인쇄본 arXiv:2302.05905, 2023. [73] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 1(2):3, 2022. [74] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 10684-10695쪽, 2022. [75] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템, 35:36479-36494, 2022. [76] Payam Saisan, Gianfranco Doretto, Ying Nian Wu, Stefano Soatto. 동적 텍스처 인식. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 2001. [77] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun 및 David J. Fleet. 광학 흐름 및 단안 깊이 추정을 위한 확산 모델의 놀라운 효과, 2023. [78] Arno Schödl, Richard Szeliski, David H Salesin 및 Irfan Essa. 비디오 텍스처. ACM 트랜스에서. 그래픽(SIGGRAPH), 페이지 489-498, 2000. [79] Mikio Shinya 및 Alain Fournier. 바람의 영향을 받는 확률론적 모션모션. 컴퓨터 그래픽 포럼, 11(3), 1992. [80] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci 및 Nicu Sebe. 심층 모션 전송을 통한 임의의 객체 애니메이션.Proc. Computer Vision and Pattern Recognition(CVPR), 2377-2386페이지, 2019. [81] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe. 이미지 애니메이션을 위한 1차 모션 모델.Neural Information Processing Systems, 32, 2019. [82] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, Sergey Tulyakov. 관절 애니메이션을 위한 모션 표현.Proc. Computer Vision and Pattern Recognition(CVPR), 13653-13662페이지, 2021. [83] Chen Qian Kwan-Yee Lin Hongsheng Li Siming Fan, Jingtan Piao. 실제 정지 이미지에서 유체 시뮬레이션. arXiv 사전 인쇄본, arXiv:2204.11335, 2022. [84] Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny. Stylegan-v: stylegan2의 가격, 이미지 품질 및 특전을 갖춘 연속 비디오 생성기. Proc. Computer Vision and Pattern Recognition(CVPR), 36263636페이지, 2022. [85] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 비평형 열역학을 사용한 심층 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015. [86] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델의 노이즈 제거. arXiv:2010.02502, 2020년 10월. [87] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. arXiv 사전 인쇄본 arXiv:2011.13456, 2020. [88] Jos Stam. 복잡한 자연 현상의 다중 규모 확률적 모델링. 박사 학위 논문, 1995. [89] Jos Stam. 확률적 동역학: 유연한 구조물에 대한 난류 효과 시뮬레이션. 컴퓨터 그래픽 포럼, 16(3), 1997. [90] Ryusuke Sugimoto, Mingming He, Jing Liao, Pedro V Sander. 정지 사진으로부터의 물 시뮬레이션 및 렌더링. ACM Trans. Graphics(SIGGRAPH Asia), 1-9페이지, 2022. [91] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H Bermano. 인간 동작 확산 모델. arXiv 사전 인쇄본 arXiv:2209.14916, 2022. [92] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, Sylvain Gelly. 비디오의 정확한 생성 모델을 향하여: 새로운 지표 및 과제. arXiv 사전 인쇄본 arXiv: 1812.01717, 2018. [93] Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal. Mcvd: 예측, 생성 및 보간을 위한 마스크 조건부 비디오 확산. 영어: Neural Information Processing Systems, 2022. [94] Carl Vondrick, Hamed Pirsiavash 및 Antonio Torralba. 장면 역학을 사용하여 비디오 생성. Neural Information Processing Systems, 2016. [95] Neal Wadhwa, Michael Rubinstein, Frédo Durand 및 William T Freeman. 위상 기반 비디오 동작 처리. ACM Trans. Graphics(SIGGRAPH), 32(4):1–10, 2013. [96] Jacob Walker, Carl Doersch, Abhinav Gupta 및 Martial Hebert. 불확실한 미래: 변분 자동 인코더를 사용하여 정적 이미지에서 예측. Proc. European Conf. on Computer Vision(ECCV)에서, 2016. [97] Jacob Walker, Abhinav Gupta 및 Martial Hebert. 정적 이미지에서 고밀도 광학 흐름 예측. Proc. Int. Conf.에서. 컴퓨터 비전(ICCV), 2443-2451페이지, 2015. [98] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao 및 Jingren Zhou. Videocomposer: 동작 제어가 가능한 구성적 비디오 합성. arXiv 사전 인쇄본 arXiv:2306.02018, 2023. [99] Yaohui Wang, Di Yang, Francois Bremond 및 Antitza Dantcheva. 잠상 이미지 애니메이터: 잠상 공간 탐색을 통해 이미지 애니메이션화 학습. arXiv 사전 인쇄본 arXiv:2203.09043, 2022. [100] Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski 및 Angjoo Kanazawa. Nerfbusters: 캐주얼하게 캡처한 nerf에서 유령 유물 제거. arXiv 사전 인쇄본 arXiv:2304.10532, 2023. [101] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi. 확산 모델을 사용한 새로운 뷰 합성. arXiv 사전 인쇄본 arXiv:2210.04628, 2022. [102] Chung-Yi Weng, Brian Curless, Ira KemelmacherShlizerman. 사진 웨이크업: 단일 사진에서 3D 캐릭터 애니메이션. Proc. Computer Vision and Pattern Recognition(CVPR), 5908-5917페이지, 2019. [103] Jamie Wynn과 Daniyar Turmukhambetov. DiffusioneRF: 노이즈 제거 확산 모델을 사용한 신경 광도장 정규화. Proc. Computer Vision and Pattern Recognition (CVPR), 2023. [104] Tianfan Xue, Jiajun Wu, Katherine L Bouman 및 William T Freeman. 시각적 역학: 계층화된 교차 합성곱 네트워크를 통한 확률적 미래 생성. 패턴 분석 및 머신 인텔리전스, 41(9):2236-2250, 2019. [105] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming 및 Nan Duan. Dragnuwa: 텍스트, 이미지 및 궤적을 통합하여 비디오 생성에서 세분화된 제어. arXiv 사전 인쇄본 arXiv:2308.08089, 2023. [106] Sihyun Yu, Kihyuk Sohn, Subin Kim 및 Jinwoo Shin. 투사된 잠재 공간의 비디오 확률적 확산 모델. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 18456-18466, 2023. [107] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang 및 Ziwei Liu. Remodiffuse: 검색 증강 모션 확산 모델. arXiv 사전 인쇄 arXiv:2304.01116, 2023. [108] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo 및 Qi Tian. Controlvideo: 교육 없이 제어 가능한 텍스트-비디오 생성. arXiv 사전 인쇄 arXiv:2305.13077, 2023. [109] Jian Zhao 및 Hui Zhang. 이미지 애니메이션을 위한 박판 스플라인 모션 모델. Proc에서 컴퓨터 비전 및 패턴 인식(CVPR), 3657-3666페이지, 2022. [110] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu. 공동 변조 생성적 적대 네트워크를 통한 대규모 이미지 완성. 국제 학습 표현 컨퍼런스(ICLR), 2021. [111] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng. Magicvideo: 잠재 확산 모델을 사용한 효율적인 비디오 생성. arXiv 사전 인쇄본 arXiv:2211.11018, 2022.
