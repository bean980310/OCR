--- INTRODUCTION ---
모션 시퀀스의 장기 생성은 컴퓨터 애니메이션, 모션 제어, 인간-컴퓨터 상호작용 등에서 무수히 많은 응용 분야가 있는 캐릭터 애니메이션에서 어렵고 오랜 문제입니다. 장기 모션을 생성하려면 퇴화된 출력(즉, 동결된 모션)을 피하는 사실적이고 반복적이지 않은 시퀀스를 생성해야 합니다. 고품질 모션을 생성하는 유망한 방법은 이미지 합성에서 전례 없는 품질을 생성한 Denoising Diffusion Probabilistic Models(DDPM)를 통한 것입니다[Ho et al. 2020]. ¹프로젝트 페이지: https://threedle.github.io/TEDi/는 최근 모션 합성에 적용되었습니다[Tevet et al. 2022; Zhang et al. 2022; Kim et al. 2022]. DDPM을 모션 합성에 일반적으로 적용하면 무작위로 샘플링된 가우시안 노이즈에서 고정 길이의 모션 시퀀스(즉, &quot;모션 이미지&quot;)가 생성됩니다. 고정 길이의 출력은 몇 가지 이유로 장기 모션 합성의 맥락에서 제한적입니다. 첫째, 짧은 시퀀스 출력에서 긴 시퀀스를 만드는 만족스러운 접근 방식은 없습니다. 단순히 동작을 연결하고 혼합하면 스티칭 아티팩트가 생성될 수 있습니다. 둘째, 일반적인 확산 프로세스는 상호 작용 제어가 제한적입니다. 확산에는 깨끗한 동작의 짧은 시퀀스를 생성하기 전에 수백 번의 노이즈 제거 반복이 필요합니다. 우리는 확산 프로세스의 시간 종속적 특성에서 영감을 얻었는데, 여기서 샘플은 확산 시간 축을 따라 작은 시간 간격으로 점진적으로 순수 노이즈에서 합성됩니다. 이 연구에서 우리는 확산을 동작의 시간 축에 맞게 조정하는 것을 제안합니다. TEDi(Temporally-Entangled Diffusion)라고 하는 우리의 방법은 고정되고 시간적으로 불변하는 분산을 갖는 가우시안 노이즈 대신 확산 프로세스의 각 단계에서 시간적으로 변하는 노이즈 레벨을 주입할 수 있도록 DDPM 프레임워크를 확장합니다. 모션 시퀀스의 시간 축을 확산 프로세스의 시간 축과 얽히게 함으로써, 확산 프로세스의 각 단계에서 깨끗한 모션 프레임의 연속적인 스트림을 생성할 수 있습니다. 프레임워크의 핵심에는 다양한 노이즈 레벨로 노이즈가 있는 미래 모션 프레임을 인코딩하는 모션 버퍼가 있습니다. 학습 단계에서는 각 프레임에 랜덤 레벨이 있도록 깨끗한 모션 시퀀스에 시간적으로 다양한 노이즈를 추가합니다. 그러나 Zihan Zhang, Richard Liu, Kfir Aberman 및 Rana Hanocka 추론 중에 모션 버퍼는 모션 프라이머로 초기화됩니다. 모션 프라이머는 노이즈 레벨이 증가하면서 노이즈가 처리되는 깨끗한 모션 프레임의 시퀀스로, 인접한 프레임에 연속적인 노이즈 레벨이 포함됩니다. TEDi는 점점 더 노이즈가 많은 미래 프레임의 노이즈를 재귀적으로 제거합니다. 각 노이즈 제거 단계에서 점진적으로 노이즈가 처리되는 모션 버퍼 구조를 지속적으로 유지하기 위해 모션 버퍼의 끝에 노이즈가 있는 프레임을 삽입하고 시작 부분에서 단일 깨끗한 프레임을 제거합니다. 이 재귀적 메커니즘은 동작 시퀀스 프레임을 지속적으로 생성할 수 있게 하며, 현재 동작 확산 모델에서 발생하는 스티칭 문제를 방지합니다(4.5.1 참조). 추론하는 동안 프로세스에 개입하고 지속적으로 깨끗한 프레임(가이드 동작이라고 함)을 주입하여 특정 동작으로 생성을 안내할 수 있습니다. 이 주입을 통해 현재 생성된 프레임 세트를 제어하고 영향을 미쳐 다가올 동작 가이드를 준비하고 계획할 수 있습니다. 이 전략은 현재 프레임과 미래의 가이드 동작 간에 계획되고 계산된 전환을 발생시킵니다. 네트워크는 동작 시퀀스의 미래 궤적에 대한 모호한 정보를 포함하는 끊임없이 진화하는 동작 버퍼의 노이즈를 계속 제거합니다. 이 공식은 동작 버퍼를 조작하여 생성된 동작을 보다 직접적으로 제어하고 더 나은 계획을 세울 수 있는 문을 엽니다. 프레임워크가 다양한 유형의 긴 동작 시퀀스를 생성할 수 있으며, 무작위적 특성으로 인해 동일한 초기화에 대해서도 다양한 결과를 제공할 수 있음을 보여줍니다. 또한 다른 장기 생성 모델과 비교하여 모델을 평가합니다. 우리의 실험 결과는 TEDi가 장기적인 동작 시퀀스를 생성하는 데 적합한 자연스러운 프레임워크라는 것을 보여줍니다.
--- RELATED WORK ---
확산 모델 잡음 제거 확산 확률 모델(DDPM) 및 그 변형[Ho et al. 2020; Dhariwal 및 Nichol 2021; Ho et al. 2022]은 조건부 및 무조건부 이미지 생성에서 전례 없는 품질을 달성했으며 일반적으로 GAN 기반[Dhariwal 및 Nichol 2021]을 능가합니다.
--- METHOD ---
TEDi(Temporally-Entangled Diffusion)라고도 하는 이 프레임워크는 고정된 시간 불변 분산을 갖는 가우시안 노이즈 대신 확산 과정의 각 단계에서 시간적으로 변하는 노이즈 레벨을 주입할 수 있도록 하여 DDPM 프레임워크를 확장합니다. 모션 시퀀스의 시간 축을 확산 과정의 시간 축과 얽히게 함으로써 확산 과정의 각 단계에서 깨끗한 모션 프레임의 연속 스트림을 생성할 수 있습니다. 프레임워크의 핵심에는 다양한 노이즈 레벨을 갖는 노이즈가 있는 미래 모션 프레임을 인코딩하는 모션 버퍼가 있습니다. 학습 단계에서는 각 프레임에 랜덤 레벨이 있도록 깨끗한 모션 시퀀스에 시간적으로 변하는 노이즈를 추가합니다. 그러나 Zihan Zhang, Richard Liu, Kfir Aberman 및 Rana Hanocka 추론 중에 모션 버퍼는 모션 프라이머로 초기화됩니다. 모션 프라이머는 노이즈 레벨이 증가하면서 노이즈가 발생하는 깨끗한 모션 프레임의 시퀀스로, 인접한 프레임에 연속적인 노이즈 레벨이 포함됩니다. TEDi는 점점 더 노이즈가 많은 미래 프레임의 노이즈를 재귀적으로 제거합니다. 각 노이즈 제거 단계 동안 점점 노이즈가 많은 모션 버퍼 구조를 지속적으로 유지하기 위해 모션 버퍼의 끝에 노이즈가 많은 프레임을 삽입하고 시작 부분에서 깨끗한 프레임 하나를 제거합니다. 이 재귀적 메커니즘을 통해 모션 시퀀스 프레임을 지속적으로 생성할 수 있으며, 현재 모션 확산 모델에서 발생하는 스티칭 문제를 피할 수 있습니다(4.5.1 참조). 추론하는 동안 프로세스에 개입하고 가이드 모션이라고 하는 깨끗한 프레임을 지속적으로 주입하여 특정 모션으로 생성을 안내할 수 있습니다. 이 주입을 통해 현재 생성된 프레임 세트를 제어하고 영향을 미쳐 다가올 모션 가이드를 준비하고 계획할 수 있습니다. 이 전략은 현재 프레임과 미래 가이드 모션 간에 미리 계획되고 계산된 전환을 일으킵니다. 네트워크는 모션 시퀀스의 미래 궤적에 대한 모호한 정보를 포함하는 끊임없이 진화하는 모션 버퍼의 노이즈를 계속 제거합니다. 이 공식은 모션 버퍼를 조작하여 생성된 모션을 보다 직접적으로 제어하고 더 잘 계획할 수 있는 문을 엽니다. 우리는 우리의 프레임워크가 다양한 유형의 긴 동작 시퀀스를 생성할 수 있으며, 무작위적 특성으로 인해 동일한 초기화에 대해서도 다양한 결과를 제공할 수 있음을 보여줍니다. 또한, 우리는 다른 장기 생성 모델과 비교하여 모델을 평가합니다. 우리의
--- EXPERIMENT ---
s는 TEDi가 장기 동작 시퀀스를 생성하는 데 자연스러운 프레임워크임을 보여줍니다.2. 관련 작업 확산 모델 잡음 제거 확산 확률 모델(DDPM)과 그 변형[Ho et al. 2020; Dhariwal and Nichol 2021; Ho et al. 2022]은 조건부 및 무조건부 이미지 생성에서 전례 없는 품질을 달성했으며, 일반적으로 시각적 품질과 샘플링 다양성 측면에서 GAN 기반[Dhariwal and Nichol 2021] 방법을 능가했습니다. 특히, 확산 모델은 대규모 모델이 텍스트와 이미지 쌍으로 학습될 때 텍스트-이미지 합성 및 편집 작업에 대해 놀라운 충실도와 의미 제어를 보여주었습니다[Ramesh et al. 2022; Saharia et al. 2022b; Rombach et al. 2021; Ruiz et al. 2022; Hertz et al. 2022]. 또한, 텍스트-비디오 및 이미지-이미지 변환과 같은 인접 도메인에 확산이 성공적으로 적용되었습니다[Saharia et al. 2022a]. 더욱이, 확산 모델은 3D 데이터를 사용하는 생성 작업에서 점점 더 많이 사용되고 있습니다. 최근 일부 연구에서는 3D 데이터를 2D 작업으로 축소하여 3D 데이터 생성을 가능하게 하는 반면, 다른 연구에서는 3D 데이터에서 전체 확산 파이프라인을 직접 학습합니다. 더 최근에는 애니메이션 도메인에서 Zhang et al.[Zhang et al. 2022], Kim et al.[Kim et al. 2022], Tevet et al.[Tevet et al. 2022], Shafir et al.[Shafir et al. 2023]은 확산 프레임워크를 직접 적용하여 전체 동작을 이미지로 처리하고 모든 프레임을 병렬로 노이즈를 제거하여 동작 생성을 위한 확산 모델을 적용할 것을 제안했습니다. 이러한 적응은 고정 길이의 동작 시퀀스만 생성할 수 있으므로 장기 생성 및 상호 작용 제어가 불가능합니다.반대로, 우리의 프레임워크는 확산 프레임워크와 자기 회귀 생성 방식을 결합하여 설계에 따라 임의 길이의 시퀀스를 생성할 수 있습니다.2.2 딥 모션 합성 현대 딥 러닝 아키텍처가 등장하기 전에 초기 작업에서는 제한된 볼츠만 머신 Taylor와 Hinton [2009]과 같은 기술을 사용하여 동작과 동작 스타일을 모델링하려고 시도했습니다.나중에 Holden et al. [2015; 2016]의 획기적인 작업 세트는 동작 데이터에 합성곱 신경망(CNN)을 적용하고 동작 매니폴드를 학습한 다음, 예를 들어 동작 매니폴드에 대한 투영을 통해 동작 편집을 수행하는 데 사용할 수 있습니다.동시에 Fragkiadaki et al. [2015]는 동작 모델링에 순환 신경망(RNN)을 사용하기로 선택했습니다.RNN 기반 작업은 단기 동작 예측에도 성공했습니다.[Fragkiadaki et al. 2015; Pavllo et al. 2018], 대화형 동작 생성[Lee et al. 2018], 음악 구동 동작 합성[Aristidou et al. 2021]. Holden et al. [2017]은 운동 생성을 위한 위상 함수 신경망(PFNN)을 제안하고 동작 합성을 위한 신경망에 위상을 도입했습니다. Zhang et al. [2018]은 사족보행 동작 생성에 유사한 아이디어를 사용했습니다. Starke et al. [2020]은 더 복잡한 동작 생성에 대처하기 위해 위상을 국소 관절로 확장했습니다. Henter et al. [2020]은 흐름 정규화를 기반으로 하는 또 다른 동작 생성 모델을 제안했습니다. 또한 딥 신경망은 동작 리타게팅[Villegas et al. 2018; Aberman et al. 2020a, 2019], 동작 스타일 전송[Aberman et al. 2020b; Mason et al. 2022], 키 프레임 기반 동작 생성 [Harvey et al. 2020], 동작 매칭 [Holden et al. 2020], 애니메이션 레이어링 [Starke et al. 2021] 및 단일 시퀀스로부터의 동작 합성 [Li et al. 2022]. 2. 장기 동작 합성 장기 동작 합성을 위한 딥 러닝 모델은 대부분 RNN을 기반으로 하는데, 이는 RNN이 자연스럽게 자기 회귀 생성을 가능하게 하고 애니메이션 프레임 간의 시간적 종속성을 포착하기 때문입니다. 일반적으로 RNN은 텍스트 생성 [Sutskever et al. 2011], 손으로 쓴 문자 [Gregor et al. 2015], 심지어 이미지 캡션 [Vinyals et al. 2015]을 위한 자연어 처리(NLP)에서 많은 성공을 거두었습니다. 또한 시공간적 예측을 위해 제안되었는데 [Ranzato et al. 2014; Srivastava et al. 2015]는 표준 LSTM의 반복 상태 전환에 2D 합성곱을 통합하고 통합된 반복 단위에서 공간적 상관관계와 시간적 역학을 모델링할 수 있는 합성곱 LSTM 네트워크를 제안했습니다.Wang 등은 쌍별 메모리 셀이 있는 합성곱 LSTM을 확장하여 장기 종속성과 단기적 변동을 모두 포착하여 예측 품질을 개선했습니다[Wang 등, 2017].Zhou 등은 학습 중에 RNN의 입력으로 네트워크의 출력과 실제 결과를 번갈아 가며 사용하여 장기 난수 생성에서 오류가 누적되는 문제를 해결했습니다[Zhou 등, 2018].acRNN이라고 하는 이 방법은 학습 세트와 유사한 길고 안정적인 동작을 생성할 수 있습니다.그러나 수정된 학습 절차에도 불구하고 acRNN은 여전히 매우 긴 동작을 생성하지 못합니다.한 가지 추측은 acRNN과 일반적으로 RNN이 시간에 따라 침식되는 메모리 구성 요소에 의존한다는 것입니다. 이와 대조적으로, 우리의 프레임워크는 우리의 וייוו 시간 가변 노이즈 클린 훈련 시퀀스 TEDI: 장기 모션 합성을 위한 시간적으로 얽힌 확산 וייווו 시간적으로 노이즈가 적용된 훈련 입력 노이즈 제거 출력 시퀀스 그림 2. TEDi 훈련 내의 프레임을 명시적으로 활용합니다.우리는 훈련 중에 깨끗한 시퀀스에 적용되는 시간적으로 가변적인 노이즈를 제거하기 위해 확산 기반 모델을 훈련합니다.각 반복에서 우리는 데이터 세트에서 K 프레임 [ƒ1, ƒ2, … … …, ƒK]의 모션 시퀀스를 가져오고, 노이즈 레벨 일정 [ßt₁› ßt₂› · · · ‚ þt¸ ]에 따라 노이즈를 적용하고, (1)에 설명된 대로 감독 방식으로 깨끗한 모션 시퀀스를 예측하도록 네트워크를 훈련합니다. 영어: 컨텍스트 창은 시간적으로 확산 시간 축과 동일한 크기만 되면 되며, 확산 과정의 성공적인 메커니즘을 준수하는 작은 증분으로 자기 회귀적으로 동작을 생성합니다.방법 시퀀스 우리는 확산 모델을 사용하여 긴 동작을 합성하는 새로운 접근 방식을 제안합니다.우리의 접근 방식은 확산 과정 중에 시간적으로 변하는 노이즈 레벨을 주입하는 것을 지원하기 위해 고전적인 DDPM 프레임워크를 확장합니다.이 확장을 통해 동작 시퀀스의 시간 축을 확산 과정의 시간 축과 얽힐 수 있습니다.시퀀스의 첫 번째 프레임이 가장 낮은 노이즈 레벨에 매핑되고 마지막 프레임이 가장 높은 레벨에 매핑되고 매핑 함수가 선형인 특정한 경우 동작 버퍼와 유사하게 추론 중에 임의로 많은 프레임을 지속적으로 합성할 수 있습니다.각 확산 단계에서 시퀀스의 시작 부분에서 깨끗한 프레임을 얻고, 깨끗한 프레임을 팝하여 스택의 프레임을 이동하고, 시퀀스의 끝에 새로운 노이즈 프레임(가우스 분포에서 추출)을 추가합니다. 추론 중 이 프로세스를 반복하면 장기 동작 합성을 위한 새로운 메커니즘이 생성됩니다. 아래에서 동작 표현(3.1), 새로운 확산 프레임워크(3.2), 훈련(3.3) 및 추론 절차(3.4)를 설명합니다. 3.1 동작 표현 xz 평면 [b]Oxz = Rx²에 대한 루트 관절 변위, 루트 관절 높이 Oy € RK 및 관절 회전 R € RKXJQ로 구성된 K 포즈의 시간적 집합으로 동작 시퀀스를 표현합니다. 여기서 J는 관절 수이고 Q는 회전 피처 수입니다. 회전은 운동학 체인에서 부모의 좌표 프레임에서 정의되고 Zhou 등이 제안한 6D 회전 피처(Q = 6)로 표현됩니다. [2019]. 발 미끄러짐 아티팩트를 완화하기 위해 발 접촉 레이블을 C · K 이진 값 L = {0, 1} KXC로 통합합니다. 이는 발 관절의 접촉 레이블에 해당합니다. 우리의 작업에서, 관절이 왼쪽(오른쪽) 발꿈치와 발가락인 C = 4로 두었습니다. 모든 특징은 채널 축을 따라 연결되고 전체 표현을 M = [Oxz, O, R, L] = RKx(JQ+C+3)으로 표시합니다. 3. 확산 모델 확산 노이즈 제거 확률적 모델(DDPM) [Sohl-Dickstein et al. 2015; Ho et al. 2020]은 물리학의 확산 과정에서 영감을 받은 쉽고 직관적인 샘플링 메커니즘으로 주어진 데이터 분포 q(mo)를 근사하는 것을 목표로 하는 생성 모델입니다. 동작 합성의 특정한 경우, 데이터는 고정 길이의 동작 시퀀스로 구성됩니다. 학습하는 동안 프로세스는 데이터 세트에서 깨끗한 동작 시퀀스 mo를 샘플링하는 것으로 시작한 다음, IID 가우시안 노이즈를 점진적으로 추가하여 프로세스 {m1,..., mT}의 잠재 변수를 구성하는 노이즈가 있는 동작 시퀀스를 형성합니다. 잠재 시퀀스는 q(m1, . . ., mt | mo) = [I¦±1 q(mi | mi-1)을 따르며, 여기서 순방향 프로세스(깨끗한 데이터에서 잡음으로)의 샘플링 단계는 스케줄 Bo, BT Є (0, 1)에 의해 매개변수화된 가우시안 전이 q(mt | mt−1) = N(√1 – ßtmt−1, ßtl)로 정의됩니다. 총 확산 시간 단계 T가 충분히 클 때 마지막 잡음 벡터 mÃ는 거의 등방성 가우시안 분포를 따릅니다. 분포 q(mo)에서 샘플링하기 위해 사후 q(mt-1 | mt)를 샘플링하여 등방성 가우시안 잡음 my에서 데이터로 이중 &quot;역방향 프로세스&quot; p(mt-1 | mt)를 정의합니다. 난해한 역과정 q(mt-1 | mt)는 알려지지 않은 데이터 분포 q(mo)에 따라 달라지므로 매개변수화된 가우시안 전이 네트워크 po(mt−1 | mt) := N(mt−1 | µµ(mt, t), Σø (mt, t))로 근사합니다. [Tevet et al. 2022]에서 제안한 대로 [Ho et al. 2020]에서 공식화한 대로 노이즈를 예측하는 대신 [Ramesh et al. 2022]를 따르고 네트워크는 다음 최적화 문제를 풀면서 신호 자체를 예측합니다. min L(0) := min Emo~q(mo), w~N(0,1),t ||mo – µg (mt, t)||₁₂, (1) Ꮎ Ꮎ는 변분 하한을 최대화합니다. 또한, 우리는 모든 시간 단계에 대해 Σ₁ = ßțI를 설정하는 역 프로세스에서 분산 일정을 고정하는 것이 가장 좋다는 것을 알아냈습니다.그러면 우리 모델은 깨끗한 동작을 예측하는 것만 학습하면 됩니다.DDPMS에 대한 자세한 내용은 [Sohl-Dickstein et al. 2015; Ho et al. 2020]을 참조하세요.3.3 시간적으로 얽힌 확산 다음으로, 확산 프로세스 동안 시간에 따라 변하는 노이즈 레벨을 주입하는 것을 지원하기 위해 DDPM 프레임워크를 확장합니다.노이즈 레벨은 프레임 인덱스의 함수가 되고 학습하는 동안 확산 시간 축의 개념은 버립니다.실제로, 우리는 T = K로 설정하고 확산 시간 축과 동작 시간 축을 식별합니다.• Zihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka. 우리는 노이즈 주입을 위한 두 가지 계획을 제안합니다. 1) 랜덤 스케줄, 2) 단조 스케줄(우리는 일반적으로 분산 스케줄의 유형을 나타내는 데 사용되는 선형 스케줄이라는 용어를 피합니다 [Nichol 및 Dhariwal 2021]). 이것들이 분산 스케줄이 아니라는 점에 유의하십시오. 구체적으로, 고정된 분산 스케줄 ßti Є (0, 1), t¡ € {0, 1, . . ., T}가 주어지면 각 학습 단계에서 랜덤 스케줄은 [Bt₁, Bt2, 반면에 단조 스케줄은 ‚ßtk], ti ~ U(0,T)로 주어집니다. [ßt₁, Bt₂, ‚ẞtk], ti = i. 확산 시간축 모션 시간축 모션 버퍼 (2) (3) 전자는 시간적으로 변하는 노이즈 레벨을 제공하는 반면 후자는 단조롭게 증가하는 노이즈 레벨을 제공합니다. 실제로, 우리는 훈련 중에 이 두 가지 노이즈 주입 방식을 혼합하여 사용하므로, 모델은 프레임 전체에 걸쳐 노이즈 레벨이 다른 동작 시퀀스를 완전히 노이즈 제거하는 방법을 학습합니다. 이를 통해 확산 과정의 시간 축과 동작의 시간 축 사이에 명시적인 얽힘을 생성할 수 있습니다. 이는 추론 중에 활용될 고유한 속성입니다. 훈련 중 각 반복에서 데이터 세트에서 길이가 K인 동작 시퀀스, [f₁, f2, ..., fk]를 샘플링합니다. 모델은 입력으로 노이즈가 주입된 모션 [ƒ₁, ƒ2, ……, ƒk]를 받습니다. 여기서 ... fi ~ N(√ä(ti) fi, (1 – ā(ti)I), _ | j=for ā (t¡) = ПÏ³½¾¹₁(1 − þt;), 그리고 깨끗한 모션 [fi, f2, fk]를 직접 예측하는 작업을 맡습니다. 네트워크에 두 가지 유형의 노이즈 주입을 혼합하기 위해 고정 확률 p와 1 - p를 갖는 랜덤 일정 또는 단조 일정을 사용하여 [ßt를 할당합니다. 실제로는 p = ½로 설정합니다. 특히 랜덤 일정을 사용한 학습 목표는 포즈 지향 확산 모델의 학습 목표와 유사하며, 여기서 전체 모션 시퀀스를 배치 크기 K의 포즈 배치로 봅니다. 그리고 각 프레임 인덱스에서 모델은 사후 q* (ft−1 | ft)를 학습하려고 시도합니다. 여기서 상위 첨자는 확산 시간 축의 시간을 나타내고 q* (fo)는 개별 포즈의 데이터 분포. 그런 다음, 단조적 노이즈 일정이 있는 목적은 추론 중에 프레임 간 원활한 전환을 보장하기 위한 추가 감독을 제공합니다. 3.3.1 손실 함수. 이전에 언급했듯이, 깨끗한 동작을 직접 예측하는 이점은 완화된 분포에 대해 정의가 잘 안 될 정규화에 액세스할 수 있다는 것입니다. 예를 들어, 관절 속도는 노이즈가 있는 동작의 손실 항으로 적절하게 정규화할 수 없습니다. 인간 모델의 계층적 특성으로 인해 오류는 운동학적 체인을 따라 축적되므로 관절 회전의 오류는 계층에서의 위치에 대해 적절하게 가중치를 두어야 합니다. 따라서 다음과 같이 정의된 위치 손실 손실을 추가합니다.KLpos = KJ t=Σ ||FKs (Ŕt, Ôt) - FKs (Rt, Ot)||2, (4) 여기서 FKS: RJQxR³ → R³는 고정된 골격 S에 대한 순방향 운동학 연산자이고, Ŕ, Ô는 모델에서 예측된 관절 회전 및 변위이고 R, O는 해당 기준 진실입니다.↑ 2에서. 클린 프레임 팝 모션 버퍼 모션 버퍼 4. 1-높이 x 노이즈 레벨 단계를 반복합니다.1. 노이즈 제거 3. 노이즈 푸시 그림 3. TEDi 재귀적 생성.TEDi는 임의로 긴 모션 시퀀스를 생성할 수 있습니다.먼저 점점 더 노이즈가 많은 모션 프레임 세트로 모션 버퍼를 초기화합니다. 그런 다음(1단계) 전체 모션 버퍼의 노이즈를 제거하고,(2단계) 모션 버퍼의 시작 부분에 새롭고 깨끗한 프레임을 팝한 다음(3단계) 모션 버퍼의 끝에 노이즈를 푸시합니다.이 프로세스는 재귀적으로 반복됩니다.또한 발 접촉은 자연스러운 동작을 생성하는 데 필수적이며 사후 처리로 역 운동학을 사용할 수 있으므로 발 관절에서 누적된 오류에 다음과 같은 발 접촉 손실을 추가로 페널티합니다.Lcontact 여기서 = KC KK||FKs Σ Σ ||FKs (Rt+1, Ot+1); – FKs (Rt, Ot);|| ·s (Ltj), jt = S = 1+e-12(x-0.5) (5) 이렇게 하면 실제 접촉 레이블을 갖는 동시에 높은 발 속도에 대한 페널티가 발생하여 생성된 동작의 자체 일관성이 보장됩니다.3.3.2 훈련. 요약하자면, 전체 훈련 손실은 L= diffLdiff +Apos Lpos + λcontact Lcontact입니다. 여기서 diff는 방정식(1)에서 지정한 확산 손실에 해당하고 매개변수는 손실의 가중치를 결정합니다. 확산 네트워크는 2D 이미지 확산 도메인에서 사용되는 일반적인 U-Net 모델에서 영감을 받았습니다[Rombach et al. 2022]. 네트워크가 1D 신호를 처리하기 위해 시간 축을 따라 1D 합성곱을 사용합니다. 또한 1D 어텐션 블록과 스킵 연결을 사용하여 장기 프레임 상관 관계가 동작 데이터 내에서 캡처됩니다. TEDI: 장기 동작 합성을 위한 시간 얽힘 확산 3.4 추론 추론 중에 모델이 훈련된 단조로운 노이즈 일정을 활용합니다. 그림 3에 나와 있는 것처럼 타자기와 같은 시스템을 사용합니다. 우리 모델은 단조롭게 증가하는 노이즈가 있는 프레임의 버퍼를 유지하는데, 버퍼의 첫 번째 프레임은 가장 낮은 노이즈 레벨에 매핑되고 마지막 프레임은 가장 높은 노이즈 레벨에 매핑됩니다(3). 이 모델은 모션을 자기 회귀적으로 생성하도록 설계되었습니다. 처음에 버퍼는 분산이 증가하는 노이즈가 있는 주어진 모션 시퀀스로 초기화됩니다. 그런 다음 각 반복에서 모델은 모션 시퀀스의 모든 프레임을 병렬로 처리하고 점진적으로 노이즈가 제거된 시퀀스를 생성합니다. 이 시점에서 시퀀스의 첫 번째 프레임은 완전히 깨끗하며 버퍼에서 팝할 수 있습니다. 표준 가우시안 분포에서 새 프레임을 샘플링하여 버퍼 끝에 있는 모션 시퀀스에 푸시합니다. 그런 다음 모델은 이 노이즈 제거 메커니즘을 반복적으로 수행할 수 있습니다. 이 생성 모드는 원하는 대로 무한정 계속될 수 있으며, 결과 모션 프레임은 모델 출력에서 프레임별로 수집됩니다. 구체적으로, 저를 모델로 하고 I = [ƒ₁, ……, ƒk]를 초기화(분산이 증가하면서 노이즈가 제거된 깨끗한 동작)라고 하며 Fout을 (처음에는 비어 있는) 출력 프레임 집합이라고 합니다.시간 단계 t, t = {1, 2, ... }에서 업데이트 Fout = [Fout, Me (I)1], fi−1 = M₁(I)i, i = {2, ……. K}, = X ~ N(0, 1), 1 = [ƒ‚…‚ ƒk]가 있는데, 여기서 Me (I);는 모델 출력의 i번째 프레임을 나타냅니다.전체 동작 길이를 사용하여 가우시안 노이즈를 샘플링하고 전체 동작을 반복적으로 노이즈 제거하는 표준 확산 프로세스의 일반적인 추론 패스와의 차이점을 강조합니다.이러한 생성 방식의 경우 모든 프레임이 모델을 T번 통과해야 합니다.여기서 추론 방식은 모델의 순방향 패스를 한 번만 거친 후 새로운 깨끗한 프레임을 출력할 수 있습니다. 동시에, 모션 버퍼에 푸시된 새로 샘플링된 프레임(순수 노이즈)은 출력에 추가되기 전에 모든 확산 시간 단계를 거치면서 T번의 반복 동안 모션 버퍼에 머물러 있습니다.간단히 말해, 우리의 추론 방법은 더 빠른 자기 회귀 생성을 가능하게 하지만 각 모션 프레임이 전체 확산 과정을 거치도록 보장합니다.실험 이 섹션에서는 여러 장기 생성 작업에서 TEDi의 효과를 보여줍니다.안내 생성을 사용하여 다가올 모션을 계획하는 기능을 포함하여 우리 방법의 여러 고유한 응용 프로그램을 보여줍니다.또한 다양한 비교 및 절제를 통해 우리 방법을 평가합니다.추가적인 정성적 결과는 보충 비디오를 참조하십시오.4.구현 세부 정보 우리의 TEDi 프레임워크는 PyTorch로 구현되었으며, 학습 및 추론은 NVIDIA A40 GPU에서 수행됩니다.우리는 Adam을 최적화 도구로 사용합니다.학습 데이터의 경우 CMU 모션 데이터 세트의 모션을 사용하고 120fps에서 30fps로 다운샘플링합니다.그림 4. 장기 생성. 우리의 방법은 임의로 긴 동작 시퀀스를 합성합니다.위의 그림에서 우리는 100프레임(~3초)마다 포즈를 시각화하여 33초의 동작을 요약합니다.우리의 모델은 전체 동작 시퀀스에서 그럴듯한 동작을 생성할 수 있습니다.그런 다음 100프레임의 보폭으로 500프레임의 창을 샘플링합니다.CMU 데이터 세트에는 다운샘플링 후 500프레임보다 짧은 프레임이 포함되어 있으며, 이는 학습에 사용되지 않습니다.학습은 500k 반복의 경우 약 3일이 걸립니다.4.장기 생성 우리의 TEDi 프레임워크는 초기 동작 버퍼를 채우는 데 사용되는 깨끗한 프라이머 동작에 따라 장기 동작을 생성할 수 있습니다.모델은 단조로운 노이즈 일정으로 점진적으로 노이즈가 적용된 K프레임 {ƒ₁, ƒ2, … … …, ƒk}의 프라이머를 입력으로 받습니다.그러면 반복적 추론 전략으로 임의로 긴 새 프레임 시퀀스를 생성할 수 있습니다. 그림 4에서 우리 방법으로 생성된 장기 시퀀스의 일부 프레임을 강조 표시합니다. 장기 생성을 유지하는 핵심은 각 반복에서 새로 샘플링된 노이즈 프레임이 &quot;버퍼&quot;가 가까운 미래에 새로운 잠재적 동작을 탐색할 수 있도록 하고 반복적인 노이즈 제거 프로세스가 동작 전체에서 프레임별 일관성을 보장한다는 것입니다. 또한 그림 5와 6에서 우리 방법이 다양한 동작 시퀀스를 생성할 수 있음을 보여줍니다. 전체 비디오 결과는 보충 비디오에서 확인할 수 있습니다. 4. 가이드 생성 움직이는 캐릭터의 경우 캐릭터가 미래의 특정 지점과 시간에 발생할 미리 정의된 동작 세트를 수행하는 것이 종종 바람직합니다. 이러한 프레임을 동작 가이드라고 합니다. 우리 프레임워크는 미래에 수행될 동작에 대한 정보가 포함된 동작 버퍼를 유지합니다. 현재 생성된 프레임 세트에 영향을 미치기 위해 동작 가이드를 사용하여 동작 버퍼를 직접 수정합니다. 구체적으로 현재 프레임 세트를 제거하고 동작 가이드의 노이즈가 적용된 버전으로 바꿉니다. 그런 다음 예측된 노이즈 제거 프레임을 버리고 적절한 확산 시간에 노이즈가 적용된 버전의 모션 가이드로 대체합니다.K개의 프레임 I = [ƒ‚…‚ƒk]의 모션 버퍼와 길이가 각각 l₁, l2, ...인 모션 가이드 세트 Q1, Q2, ...가 있다고 가정합니다.프레임 번호 n₁, n2, ...,• Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka 그림 5. 다양한 모션.우리의 방법은 다양한 긴 모션 시퀀스를 생성할 수 있습니다.왼쪽에서 오른쪽으로: 복싱, 셔플링, 손짓.그림 6. 모션 변형.확산 모델의 확률적 특성으로 인해 우리의 방법은 동일한 모션 프라이머를 입력으로 사용하여 변형을 생성할 수 있습니다.왼쪽에서 오른쪽으로 단일 프라이머에서 생성된 4개의 모션을 보여줍니다.시간이 지남에 따라 모션이 상당히 달라지기 시작하는 것을 볼 수 있습니다.그림 7. 가이드 생성. 동작 가이드 Qi(노란색으로 표시)가 주어지면 원하는 지점에서 순서대로 수행하면서 대화형으로 생성된 프레임(파란색)에서 그럴듯한 동작을 생성할 수 있습니다.좌측 상단에서 우측 하단까지, 이 방법은 원하는 동작 가이드와 대화형으로 합성된 동작을 포함하는 전체 동작 시퀀스를 생성합니다.대화형으로 생성된 동작은 다가올 동작 가이드를 위해 &quot;준비하고 계획&quot;합니다.보충 영상을 참조하세요.그림 8. 궤적 제어.가이드 생성과 유사하게, 원하는 궤적 정보 P(노란색으로 표시)가 주어지면 이 방법은 주어진 궤적을 따르는 자연스러운 동작을 생성할 수 있습니다.0.0300.0250.0200.0150.0.005단조 schdl.단조 + 무작위 schdl.TEDI: 장기 동작 합성을 위한 시간적 얽힘 확산그림 9. 소거: 여기서 무작위 일정이 있고 없는 이 방법에 대한 평균 동작 분산 오버프레임을 보여줍니다. 무작위 일정이 동작 붕괴를 방지하는 데 도움이 되는 것을 볼 수 있습니다. .... n; ≥ K. 프레임 번호 n = 1(예: 현재 동작 버퍼의 끝이 프레임 번호 K)로 시작한다고 가정하면, 각 사전 정의된 동작 Qi에 대해, je {1, 2, … … …, l; }이고 n¡ ≤ Qi; ≤ n¡ +l¡인 프레임 Qi, 중 하나가 n+5 ≤ n¡ + j ≤ n+K인 경우, 이를 동작 버퍼에 재귀적으로 대체합니다. 동작 버퍼 시작 바로 앞에 재귀적으로 대체하지 않는 프레임이 5개 있다는 점에 유의하세요. 이를 통해 네트워크는 생성된 프레임과 동작 가이드 간의 전환을 매끄럽게 할 수 있습니다. 알고리즘 1에서 재귀적 대체를 통한 가이드 생성 절차를 자세히 설명했습니다. 그림 7과 보충 자료에서 가이드 생성을 시연합니다. 알고리즘 1 가이드 생성 요구 사항: Me: 노이즈 제거 모델 I = [fi,..., fk]: 동작 버퍼 {Q1, Q2,...}: 동작 가이드 {1, 2,...}: 동작 가이드 길이 {n₁, n2, … . .}: 안내를 위한 시작 프레임 번호 Fout 0: 1, 2, ...에서 n에 대한 출력 프레임 do 모든 프레임 Qi에 대해 Me(I)를 평가하고, do n + 5 ≤ ni + j ≤ n + K이면 Mo(I)n;+jn← Qij end if end for Fout ← - [Fout, Mo(I)1] ← fi-1 Mo(I); Vi Є {2,…..K} fKX~N(0, 1) I← end for 4.4 궤적 제어 우리의 작업은 추가 학습 없이 추론 중에 궤적 제어를 수행하는 데 적용될 수 있습니다. 이전 섹션의 가이드 생성 메커니즘과 유사하게, 궤적 제어도 동작 버퍼를 수정하여 인페인팅 전략을 활용합니다. 구체적으로, I = [ƒi, ………, ƒk]를 K 프레임의 동작 버퍼라고 하고, P € R³×N을 궤적 정보(xz 평면과 루트 높이에 대한 루트 변위)라고 합니다. 여기서 N은 생성할 프레임의 원하는 수입니다. 추론하는 동안 동작 버퍼의 궤적 정보를 P의 프레임으로 재귀적으로 덮어씁니다. 자세한 절차는 알고리즘 1에 제시된 것과 비슷합니다. 그림 8에서 궤적 제어 생성을 보여줍니다. 4.5 비교 및 절제 다음으로 대체 기준선에 대해 접근 방식을 평가하고 절제 연구를 통해 프레임워크를 평가합니다. 결과를 정성적으로 평가하기 위해 이 작업에 첨부된 보충 비디오를 독자에게 참조하세요. 정량적 평가를 위해 생성된 모든 프레임에서 분산을 측정하여 동작 시퀀스의 붕괴를 방지하는 능력을 평가합니다. 비정상적으로 생성된 동작이 얼마나 되는지 측정하고 동작이 붕괴되는 시점을 감지하기 위해 로컬 윈도우에서 포즈의 평균 분산을 측정합니다.4.5.1 비교.이 실험에서는 장기 생성 작업에 대한 다른 작업과 프레임워크를 비교하는 데 중점을 둡니다.저희는 저희 방법을 ACRNN[Zhou et al. 2018] 및 Human Motion Diffusion Model(MDM)[Tevet et al. 2022]과 비교합니다.특히, ACRNN 작업[Zhou et al. 2018]은 추론 설정을 모방하고 동작 붕괴를 완화하기 위해 학습 중에 모델의 출력 프레임 일부를 수신하는 RNN 기반 작업입니다.MDM은 동작 생성을 위한 클래식 DDPM 네트워크를 변형한 것입니다.ACRNN은 CMU 데이터 세트의 샘플 하위 집합에서 학습되도록 설계되었고 추론을 위한 기본값으로 장기 생성을 갖는 반면, MDM에는 장기 생성을 위한 기본 구현이 없습니다. 따라서 MDM에 사전 학습된 체크포인트를 사용하고 MDM에 대한 장기 생성을 가능하게 하는 인페인팅 기반 방식을 구현합니다. 이 구현은 2D 이미지 생성에 사용되는 인기 있는 &quot;아웃페인팅&quot; 기술과 동일하며, 여기서 생성된 동작의 후반부를 가져와 다음 반복에서 생성된 동작의 전반부에 인페인팅합니다. 그림 10에서와 같이 ACRNN은 크고 다양한 데이터 세트에서 좋은 성능을 발휘하지 못하고 초기화 후 빠르게 붕괴되는 동작을 생성합니다. 반면에 TEDi는 붕괴에 견고한 무한히 긴 시퀀스를 생성할 수 있습니다. 반면에 MDM은 인페인팅 경계를 따라 상당한 스티칭 아티팩트를 생성합니다. 자세한 내용은 보충 비디오를 참조하십시오. 4.6 지각 연구 생성된 동작의 지각된 다양성과 품질을 평가하기 위해 지각 연구를 수행합니다. MDM과 ACRNN 외에도 Motion VAE[Ling et al. 2020], VAE를 사용한 최근 자기회귀 동작 생성 모델을 기준 비교로 사용했습니다. DALLE-2 [Ramesh et al. 2022]의 설정에 따라, 우리는 사용자에게 우리 모델, MDM, 동작 VAE, ACRNN에서 무작위로 샘플링한 동작의 3x3 그리드를 보여주고 1) 가장 다양한 동작이 있는 세트와 2) 가장 높은 품질의 동작이 있는 세트(우리의 MDM 또는 ACRNN에서만)를 선택하도록 요청했습니다. 지각 연구에서 생성된 동작의 시각적 예는 부록 A에 나와 있습니다. 우리 연구에는 55명의 응답자가 있었고, 표 1에 결과를 보고합니다. 지각 연구를 통해 우리 방법이 다양성 측면에서 상당히 우수한 성과를 보이면서 MDM과 비교했을 때 동등하거나 더 나은 품질의 동작을 생성한다는 결론을 내렸습니다. 다양성 품질 우리의 MDM ACRNNMVAEN/A 표 1. 우리 방법과 기준선에 대한 지각 연구 결과. 4.6.1 절제. 그림 9에서 우리는 시간적으로 불변하는 노이즈 수준으로 우리 모델의 버전을 훈련함으로써 우리 훈련 계획의 이점을 보여줍니다. 시간적으로 변하는 노이즈가 없으면 네트워크는 장거리 동작 생성의 다양성과 안정성이 모두 감소합니다.
--- CONCLUSION ---
이 논문에서 우리는 모션 합성을 위한 확산 모델의 적용인 TEDi를 제안했는데, 이는 모션 시간 축과 확산 시간 축을 얽히게 합니다. 이 메커니즘은 U-Net 아키텍처를 사용하여 임의로 긴 모션 시퀀스를 자기 회귀 방식으로 합성할 수 있게 합니다. 우리 작업의 고유한 측면은 고정 모션 버퍼의 개념입니다. 우리 프레임워크는 실제로 확산 시간을 증가시키지 않고도 깨끗한 프레임(즉, 확산 시간 축을 따라 진행)을 계속 생성합니다. 파이프라인이 확산 축을 따라 모션을 지속적으로 생성하는 기능은 우리 프레임워크가 견고하고 지속적으로 새로운 프레임을 생성할 수 있게 합니다. 흥미롭게도, 이러한 자기 회귀 방식으로 자연스럽게 확산을 사용하는 기능은 오디오 및 비디오와 같은 모션을 넘어선 다른 유형의 순차적 데이터나 이미지의 패치별 순서와 같이 순차적 순서를 정의할 수 있는 모달리티에 영향을 미칠 수 있습니다. 우리 시스템은 부분적으로 깨끗한 프레임을 모션 버퍼 스택에서 즉시(또는 거의 즉시) 팝오프할 수 있게 합니다. 그러나 현재 시스템의 한계는 순수 노이즈에서 깨끗한 원본을 계산하려면 노이즈 제거 확산 체인을 거쳐야 한다는 것입니다. 앞으로는 DDIM [Song et al. 2020]의 아이디어를 활용하여 노이즈 제거 프로세스 중에 건너뛰어 더 낮은 지연 시간을 달성하는 데 관심이 있습니다. 또한, 저희 프레임워크는 장기 텍스트 조건 동작 생성에 대한 미래 연구를 가능하게 합니다. 저희는 장기 생성 작업을 위해 고수준 제어를 저수준 사용자 지침과 결합하는 방법을 탐구하는 데 관심이 있습니다. 감사의 말 3DL 연구실에 귀중한 피드백과 지원에 감사드립니다. 이 작업은 부분적으로 Uchicago의 AI 클러스터 리소스, 서비스 및 직원 전문 지식을 통해 지원되었습니다. 이 작업은 또한 Grant No. 2241303에 따라 NSF에서 부분적으로 지원되었으며 Google Research의 기부금도 받았습니다. 참고문헌 Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or, Baoquan Chen. 2020a. 딥 모션 리타게팅을 위한 골격 인식 네트워크. ACM Transactions on Graphics(TOG) 39, 4(2020), 62-1. Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen. 2020b. 비디오에서 애니메이션으로의 비페어 모션 스타일 전송. ACM Transactions on Graphics(TOG) 39, 4(2020), 64-1. Kfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or. 2019. 2D에서 모션 리타게팅을 위한 캐릭터 독립적 모션 학습. ACM Trans. Graph. 38, 4 (2019), 75. Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman, Daniel Cohen-Or, Ariel Shamir, Yiorgos Chrysanthou. 2021. 리듬은 댄서: 글로벌 구조를 갖춘 음악 기반 모션 합성. arXiv 사전 인쇄본 arXiv:2111.12159 (2021). Prafulla Dhariwal, Alexander Nichol. 2021. 확산 모델은 이미지 합성에서 간을 이긴다. 신경 정보 처리 시스템의 발전 34 (2021), 8780-8794. Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik. 2015. 인간 역학을 위한 순환 네트워크 모델. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 4346-4354. Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, Daan Wierstra. 2015. Draw: 이미지 생성을 위한 순환 신경망. 기계 학습 국제 컨퍼런스에서. PMLR, 1462-1471. Félix G Harvey, Mike Yurick, Derek Nowrouzezahrai, Christopher Pal. 2020. 견고한 모션 인비트위닝. ACM Transactions on Graphics(TOG) 39, 4(2020), 60-1. Gustav Eje Henter, Simon Alexanderson, Jonas Beskow. 2020. Moglow: 정규화 흐름을 사용한 확률적이고 제어 가능한 모션 합성. ACM Transactions on Graphics(TOG) 39, 6(2020), 1-14. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 2022. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626(2022). Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전 33(2020), 6840-6851. Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, Tim Salimans. 2022. 고충실도 이미지 생성을 위한 계단식 확산 모델. J. Mach. Learn. Res. 23(2022), 47-1. Daniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. 학습된 동작 매칭. ACM Transactions on Graphics(TOG) 39, 4(2020), 53-1. Daniel Holden, Taku Komura, and Jun Saito. 2017. 캐릭터 제어를 위한 위상 함수 신경망. ACM Transactions on Graphics(TOG) 36, 4(2017), 1-13. Daniel Holden, Jun Saito, and Taku Komura. 2016. 캐릭터 동작 합성 및 편집을 위한 딥 러닝 프레임워크. ACM Transactions on Graphics(TOG) 35,(2016), 1-11. Daniel Holden, Jun Saito, Taku Komura, and Thomas Joyce. 2015. 합성곱 자동 인코더를 사용한 동작 매니폴드 학습. SIGGRAPH Asia 2015 Technical Briefs에서. 1-4. Jihoon Kim, Jiseob Kim, and Sungjoon Choi. 2022. Flame: 자유형 언어 기반 모션 합성 및 편집. arXiv 사전 인쇄본 arXiv:2209.00349 (2022). Kyungho Lee, Seyoung Lee, and Jehee Lee. 2018. 대화형 캐릭터 애니메이션 by 학습 다목적 제어. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1-10. Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. 2022. Ganimator: 단일 시퀀스에서 신경 모션 합성. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1-12. Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. 2020. 모션 VAES를 사용하는 캐릭터 컨트롤러. ACM Transactions on Graphics(2020). Ian Mason, Sebastian Starke, and Taku Komura. 2022. 특징별 변환 및 국소 동작 단계를 통한 인간 운동의 실시간 스타일 모델링. arXiv 사전 인쇄본 arXiv:2201.04439(2022). Alex Nichol and Prafulla Dhariwal. 2021. 개선된 노이즈 제거 확산 확률적 모델. (2021). arXiv:cs.LG/2102.Dario Pavllo, David Grangier, and Michael Auli. 2018. Quaternet: 인간 동작을 위한 사원수 기반 순환 모델. arXiv 사전 인쇄본 arXiv:1805.06485(2018). Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. 클립 잠재성을 이용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125(2022). Marc Aurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra. 2014. 비디오(언어) 모델링: 자연스러운 비디오 생성 모델을 위한 기준선. arXiv 사전 인쇄본 arXiv:1412.6604(2014). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2021. 잠재 확산 모델을 이용한 고해상도 이미지 합성. arXiv:cs.CV/2112. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 10684-10695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2022. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242(2022). TEDi: 장기 모션 합성을 위한 시간적 얽힘 확산 Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi. 2022a. Palette: 이미지-이미지 확산 모델. ACM SIGGRAPH 2022 컨퍼런스 회의록. 1-10. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Tim Salimans, Jonathan Ho, David J Fleet 및 Mohammad Norouzi. 2022b. 깊은 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄 arXiv:2205.11487 (2022). Yonatan Shafir, Guy Tevet, Roy Kapon 및 Amit H. Bermano. 2023. 생성적 우선순위로서의 인간 모션 확산. arXiv:cs.CV/2303.Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan 및 Surya Ganguli. 2015. 비평형 열역학을 사용한 심층적 비지도 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 2256-2265. Jiaming Song, Chenlin Meng, Stefano Ermon. 2020. 확산 암시적 모델의 잡음 제거. 표현 학습 국제 컨퍼런스에서. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov. 2015. Istms를 사용한 비디오 표현의 비지도 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 843-852. Sebastian Starke, Yiwei Zhao, Taku Komura, Kazi Zaman. 2020. 다중 접촉 캐릭터 움직임을 학습하기 위한 로컬 모션 단계. ACM Transactions on Graphics(TOG) 39, 4(2020), 54-1. Sebastian Starke, Yiwei Zhao, Fabio Zinno, Taku Komura. 2021. 무술 동작을 합성하기 위한 신경 애니메이션 레이어링. ACM Transactions on Graphics(TOG) 40, 4(2021), 1-16. Ilya Sutskever, James Martens, Geoffrey E Hinton. 2011. 순환 신경망을 사용한 텍스트 생성. ICML에서. Graham W Taylor와 Geoffrey E Hinton. 2009. 동작 스타일을 모델링하기 위한 인수분해 조건부 제한 볼츠만 머신. 제26회 기계 학습 국제 컨퍼런스 회의록에서. 1025-1032. Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H Bermano. 2022. 인간 동작 확산 모델. arXiv 사전 인쇄본 arXiv:2209.(2022). Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee. 2018. 비지도 모션 리타게팅을 위한 신경 운동학 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 8639-8648. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan. 2015. Show and tell: 신경 이미지 캡션 생성기. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 3156-3164. Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S Yu. 2017. Predrnn: 시공간적 Istms를 사용한 예측 학습을 위한 순환 신경망. 신경 정보 처리 시스템의 발전 30(2017). He Zhang, Sebastian Starke, Taku Komura, Jun Saito. 2018. 사족보행 모션 제어를 위한 모드 적응 신경망. ACM Transactions on Graphics(TOG) 37,(2018), 1-11. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. 2022. Motiondiffuse: 확산 모델을 사용한 텍스트 기반 인간 동작 생성. arXiv 사전 인쇄본 arXiv:2208.15001(2022). Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019. 신경망에서 회전 표현의 연속성에 관하여. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 5745-5753. Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. 2018. 확장된 복잡한 인간 동작 합성을 위한 자동 조건부 순환 네트워크. 국제 학습 표현 컨퍼런스에서.• Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka MDM ACRNN 그림 10. 장기 동작 합성 기준선 비교. 위: MDM을 사용한 인페인팅 구현을 통해 생성된 두 쌍의 연속 프레임을 보여줍니다 [Tevet et al. 2022]. 클래식 인페인팅은 인페인팅 경계를 따라 발생하는 눈에 띄는 불연속성을 보여줍니다. 아래: ACRNN [Zhou et al. 2018]은 대규모 데이터 세트에서 학습했을 때 발 부상 및 침투 아티팩트에서 볼 수 있듯이 안정적이지 않습니다. 지각 연구 여기서는 응답자에게 표시된 대로 그림 11과 그림 12에서 지각 연구의 스크린샷을 제공합니다. 아래의 세트 A, B, C, D에서 가장 다양한 동작 세트를 표시하는 세트를 선택하십시오. (다양성은 몇 가지 특정 패턴을 반복하는 것이 아니라 다양한 유효한 동작 패턴을 의미합니다). 세트 A 세트 B 세트 C 세트 D 아래의 세트 A, B, C 중에서 가장 높은 품질의 동작 세트를 표시하는 세트를 선택하십시오. (품질은 비디오의 해상도가 아닌 동작의 사실성을 의미합니다). 세트 B 세트 C 세트 A 그림 11. 지각 연구의 질문. * 세트 A 세트 B 세트 C 세트 D그림 12. 지각 연구의 예시 동작. 위에서 아래로: 우리, ACRNN, MDM, Motion VAE.
