--- ABSTRACT ---
대규모 언어 모델(LLM)의 엄청난 성공은 LLM-증강 자율 에이전트(LAA)에 대한 새로운 탐구를 촉진합니다. LAA는 핵심 LLM으로 동작을 생성하고 환경과 상호 작용할 수 있으며, 이는 관찰 및 동작과 같은 과거 상호 작용을 조건으로 하여 복잡한 작업을 해결하는 능력을 용이하게 합니다. LAA에 대한 조사가 아직 매우 최근이기 때문에 제한된 탐구가 가능합니다. 따라서 에이전트 아키텍처와 LLM 백본 측면에서 LAA를 포괄적으로 비교합니다. 또한 각 노동 LAA가 한 가지 유형의 동작, 즉 컨트롤러가 여러 에이전트 간의 통신을 관리하는 BOLAA에 집중하도록 여러 LAA를 조율하는 새로운 전략을 제안합니다. 의사 결정 및 다단계 추론 환경 모두에서 시뮬레이션을 수행하여 LAA의 용량을 포괄적으로 정당화합니다. 당사의 성과 결과는 LAA 아키텍처 설계와 LLM의 최적 선택, 그리고 둘의 호환성에 대한 정량적 제안을 제공합니다. 우리는 https://github.com/salesforce/BOLAA에서 LAAS 구현 코드를 대중에게 공개합니다. 1
--- INTRODUCTION ---
최근 대규모 언어 모델(LLM)의 폭발적인 성공(OpenAI, 2023; Touvron et al., 2023)은 다양한 복잡한 작업을 처리하기 위해 LLM을 사용하는 새로운 탐색을 촉진합니다(Zhang et al., 2023). 그 중에서도 LLM 증강 자율 에이전트(LAA)(Shinn et al., 2023; Madaan et al., 2023b; Huang et al., 2022; Kim et al., 2023; Paul et al., 2023; Yao et al., 2023a)가 가장 주목을 받고 있습니다. LAA는 LLM의 지능을 순차적 작업 실행으로 확장하여 환경과 상호 작용하고 관찰을 수집하여 복잡한 작업을 해결하는 데 우수성을 보여줍니다. 몇 가지를 예로 들면, BabyAGI¹는 OpenAI LLM²을 활용하여 작업을 만들고, 우선순위를 지정하고, 실행하는 AI 기반 작업 관리 시스템을 제안합니다. AutoGPT³는 LLM의 API 호출 기능을 활성화하는 또 다른 인기 있는 오픈소스 LAA 프레임워크입니다. ReAct(Yao et al., 2023a)는 최근 제안된 LAA 방법으로 환경과 상호 작용한 다음 연속적으로 다음 동작을 생성합니다. Langchain은 LAA를 개발하기 위한 최근 출시된 오픈소스 프레임워크입니다. 초기 조사로 인해 LAA는 다소 탐구되지 않았습니다. 첫째, 최적의 에이전트 아키텍처가 결정되지 않았습니다. ReAct(Yao et al., 2023a)는 LLM이 컨텍스트 내 학습을 통해 다음 동작을 생성하는 방법을 학습할 수 있도록 사전 정의된 예제로 에이전트에게 프롬프트합니다. 게다가 ReAct는 에이전트가 동작 실행 전에 중간 추론 단계를 거쳐야 한다고 주장합니다. ReWOO(Xu et al., 2023)는 LAA에 대한 추가 계획 단계를 도입합니다. Langchain은 zhiweiliu@salesforce.com을 사용하여 ReAct 에이전트를 일반화합니다. https://github.com/yoheinakajima/babyagi 2 https://platform.openai.com/docs/api-reference 3 https://github.com/Significant-Gravitas/Auto-GPT A https://github.com/langchain-ai/langchainPREPRINT 제로샷 도구 사용 능력. 본질적으로 에이전트의 최적 아키텍처는 작업과 관련 LLM 백본에 맞춰야 하며, 이는 기존 작업에서 덜 탐구되었습니다. 둘째, LAA에서 기존 LLM의 효능을 이해하는 것은 포괄적이지 않습니다. 기존의 예비 작업은 몇 가지 LLM 백본의 성능만 비교합니다. ReAct는 PaLM(Chowdhery et al., 2022)을 백본 LLM으로 채택합니다. ReWOO는 에이전트 계획을 위한 Alpaca 모델(Taori et al., 2023)에 대한 명령어 튜닝을 위해 OpenAI text-davinci-003 모델을 사용합니다. MIND2Web(Deng et al., 2023)은 일반 웹 에이전트를 위해 Flan-T5와 OpenAI GPT3.5/4를 비교합니다. 그럼에도 불구하고 현재 작업 중 다양한 사전 훈련된 LLM과 관련하여 LAA의 성능을 종합적으로 비교하는 작업은 거의 없습니다. 아주 최근의 작업(Liu et al., 2023)은 LLM을 에이전트로 평가하기 위한 벤치마크를 출시합니다. 그럼에도 불구하고 그들은 LLM 백본과 함께 에이전트 아키텍처를 공동으로 고려하지 못합니다. 효능과 효율성 관점에서 최적의 LLM을 선택하면 LAA에 대한 현재 탐색이 진전됩니다. 셋째, 작업의 복잡성이 증가함에 따라 여러 에이전트의 오케스트레이션이 필요할 수 있습니다. ReWOO는 최근 추론을 관찰에서 분리하면 LAA의 효율성이 향상된다는 것을 확인했습니다. 이 논문에서 우리는 작업 복잡성이 증가함에 따라, 특히 오픈 도메인 환경에서, 하나의 작업을 완료하기 위해 여러 에이전트를 조정하는 것이 더 낫다고 주장합니다. 예를 들어, 웹 탐색 작업과 관련하여, 우리는 클릭 가능한 버튼과 상호 작용하고 다른 검색 에이전트에게 추가 리소스를 검색하도록 요청하기 위해 하나의 클릭 에이전트를 사용할 수 있습니다. 그럼에도 불구하고, 여러 에이전트를 조정하는 방법과 조정의 영향을 조사하는 방법에 대해 논의하는 연구는 거의 없습니다. 이러한 연구 격차를 해소하기 위해 이 논문은 LAA의 성능을 종합적으로 비교하는 것을 제안합니다. 우리는 LAAS와 LLM 백본의 에이전트 아키텍처를 심층적으로 살펴봅니다. 구체적으로, 우리는 다양한 LLM 백본을 기반으로 구축된 다양한 에이전트 아키텍처의 성능을 평가하기 위해 기존 환경에서 에이전트 벤치마크를 구성합니다. 우리의 에이전트 벤치마크의 작업은 서로 다른 작업 복잡성 수준과 연관되어 있어 작업 복잡성과 관련된 에이전트 성능 분석이 가능합니다. 이러한 에이전트 아키텍처는 기존 설계 선택을 광범위하게 검증하도록 설계되었습니다. 여러 LAA의 오케스트레이션과 관련하여, 여러 협업 에이전트 위에 컨트롤러 모듈이 있는 새로운 LAA 아키텍처 BOLAA5를 제안하여 여러 노동 LAA 간의 선택과 통신을 가능하게 합니다. 이 논문의 기여는 다음과 같습니다. • 6개의 서로 다른 LAA 에이전트 아키텍처를 개발합니다. 이를 다양한 백본 LLM과 결합하여 프롬프트, 자기 사고 및 계획에서 LAA의 설계 직관을 정당화합니다. 또한 솔로 에이전트의 액션 상호 작용 능력을 향상시키는 다중 에이전트 전략을 오케스트레이션하기 위한 BOLAA를 개발합니다. • 의사 결정 웹 탐색 환경과 지식 추론 작업 환경 모두에서 광범위한 실험을 수행합니다. 최종 희소 보상과 중간 리콜 측면에서 성과를 보고하여 LAAS와 호환되는 LLM의 최적 선택에 대한 정성적 지표를 제공합니다. • WebShop 환경에서의 BOLAA는 다른 LAA 아키텍처와 비교하여 지속적으로 최고의 성능을 제공합니다. 우리의 결과는 복잡한 과제를 해결하기 위해 협력하는 전문가 에이전트를 설계하는 것의 중요성을 보여주는데, 이는 높은 일반화 능력을 가진 대규모 LLM을 훈련하는 것만큼 중요해야 합니다. 2
--- RELATED WORK ---
2. 증강 언어 에이전트 아키텍처 복잡한 작업을 완료하려면 일반적으로 여러 단계가 필요합니다. 에이전트는 이러한 단계를 이해하고 그에 따라 계획해야 합니다. CoT(Wei et al., 2022)라고도 알려진 Chain-of-Thoughts는 에이전트가 어려운 추론 작업을 더 작고 관리하기 쉬운 단계로 분해하도록 하는 획기적인 작업입니다. 반면, ReAct(Yao et al., 2023a)는 언어 및 학습 모델(LLM) 내에서 추론 및 행동에 대한 이러한 적성을 활용하여 관찰을 자연어로 추론 및 행동 추적 또는 API 호출 생성에 매핑하여 Wikipedia 검색 API를 활용하는 것과 같이 환경과의 상호 작용적 참여를 촉진하는 것을 제안합니다. &quot;기억하기 쉽도록 의도적으로 논문 제목과 같은 이름을 붙였습니다. 사전 인쇄 이 에이전트 아키텍처는 HuggingGPT(Shen et al., 2023), Generative Agents(Park et al., 2023), WebGPT(Nakano et al., 2021), AutoGPT(Gravitas, 2023), BabyAGI(Nakajima, 2023), Langchain(Chase, 2023)을 포함한 다양한 응용 프로그램을 탄생시켰습니다. 그러나 이러한 접근 방식은 에이전트의 행동을 향상시키기 위해 환경 보상과 같은 귀중한 피드백을 통합하지 못하여 사전 학습된 언어 및 학습 모델(LLM)의 품질에만 의존하는 성능을 초래합니다. 자체 정제(Madaan et al., 2023a)는 단일 LLM을 생성기, 정제기 및 피드백 제공자로 사용하여 출력의 반복적 정제를 가능하게 함으로써 이러한 제한을 해결합니다. 그러나 실제 작업 기반에 특별히 맞춤화되지 않았습니다. 환경과의 상호작용. 반면, REX(Murthy et al., 2023)와 RAP(Hao et al., 2023)는 LLM을 재활용하여 포괄적인 세계 모델과 추론 에이전트로 기능합니다. 이들은 환경 보상이 있는 광대한 추론 영역 내에서 전략적 탐색을 위해 몬테카를로 트리 탐색을 통합합니다. 이 접근 방식은 복잡한 도메인에서 효과적인 탐색과 의사 결정을 용이하게 합니다. Shinn et al.(2023)은 에이전트에게 동적 메모리와 자기 반성 기능을 제공하여 추론 기술을 향상시키는 프레임워크인 Reflexion을 제시합니다. 자기 반성은 중요한 역할을 하며, 자율적인 에이전트가 과거 행동을 반복적으로 수정하고 개선하며 반복적인 오류를 방지할 수 있도록 합니다. 최근 Yao et al.(2023b)은 플러그인 회고적 언어 모델을 학습하여 정책 그래디언트 최적화를 활용하여 에이전트의 행동을 환경별 보상과 일치시키는 프레임워크인 Retroformer를 제안합니다. 2. 웹 에이전트 웹 탐색은 인간이 정보를 수집하고 소통하는 기반입니다. LLM이 붐을 일으키기 전에 이전 노력(Liu et al., 2018; Shi et al., 2017)은 이미 웹 시뮬레이션 환경에서 웹 에이전트를 훈련하는 방법을 탐구했습니다. 아주 최근에 일련의 작업이 복잡한 웹 탐색 작업을 처리하기 위해 LAA를 개발하는 데 전념했습니다. 웹 탐색의 작업 공간은 온라인에서 사용 가능한 요소가 많기 때문에 거의 무한하지만 이러한 작업은 클릭, 입력 및 선택과 같은 몇 가지 작업 유형으로 나눌 수 있습니다. MIND2Web(Deng et al., 2023)은 웹 브라우저 데이터를 수집하여 LLM을 미세 조정하여 실행 가능한 작업을 생성하며, 이는 웹 LAA 역할을 합니다. WebAgent(Gur et al., 2023)는 작업 지침을 하위 작업으로 분해하여 웹 탐색을 위한 실행 가능한 Python 프로그램을 직접 생성할 수 있습니다. WebArena(Zhou et al., 2023)는 웹 LAA를 설계하기 위한 현실적인 작업 시뮬레이션을 지원합니다. Langchain과 ChatGPT는 모두 LLM이 웹 LAA처럼 동작하도록 편리한 웹 플러그인을 제공합니다. 우리는 웹 탐색이 LAA가 우월성을 빛내기 위한 다음 기본 과제라고 믿습니다. 2. 도구 에이전트 LLM의 진화와 다양한 도구와의 상호 작용은 최근 연구의 초점이었습니다. &quot;도구 에이전트&quot;라는 개념은 LLM이 외부 도구를 활용하여 기능을 향상하고 복잡한 작업을 해결한다는 아이디어를 요약합니다. 이 분야의 선구적인 작업 중 하나는 &quot;Gorilla&quot;(Patil et al., 2023)의 도입입니다. 이 모델은 API 호출을 작성하는 데 능숙하며 테스트 시간 문서 변경 사항을 적응시키는 기능을 보여줍니다. 또 다른 주목할 만한 작업은 &quot;ToolLLM&quot; 프레임워크(Qin et al., 2023)입니다. 이 오픈 소스 프레임워크는 LLM을 통합하여 복잡한 작업을 실행하기 위해 수많은 도구, 특히 API와 효율적으로 상호 작용합니다. 프레임워크는 도구 활용에 맞춰 조정된 명령어 튜닝 데이터 세트인 ToolBench를 포함합니다.최근에 LLM에게 새로운 도구를 사용하도록 가르치는 패러다임 전환이 (Hsieh et al., 2023)에서 논의되었으며, 이는 도구 설명서 사용을 옹호합니다.저자는 도구 설명서가 도구 사용에 대한 자세한 설명을 제공하며, 이는 보다 효과적이고 확장 가능한 접근 방식이라는 것을 시사하는 경험적 증거를 제시합니다.특히, 그들의 연구에 따르면 도구 설명서에만 기반한 제로샷 프롬프트가 퓨샷 프롬프트의 성능과 맞먹을 수 있습니다.에이전트 아키텍처 이 섹션에서는 다양한 LAA 아키텍처를 비교합니다.먼저 기존 작업의 직관에 따라 다양한 솔로 LAA를 설계하는 방법을 제시합니다. 그런 다음 다중 LAA, 즉 BOLAA의 오케스트레이션 디자인을 제시합니다.PREPRINT 환경 동작 동작 파서 관찰 환경 동작 동작 파서 관찰 환경 동작 관찰 동작 파서 작업 지시 LLM Zeroshot 프롬프트 (a) Zeroshot LAA 메모리 작업 지시 LLM 메모리 생각 생각 Zeroshot 프롬프트 (b) ZeroshotThink LAA 작업 지시 LLM Fewshot 프롬프트 메모리 생각 생각 (c) ReAct LAA 그림 1: Zeroshot-LAA(ZS-LAA), ZeroshotThink LAA(ZST-LAA) 및 ReAct LAA의 LAA 아키텍처.ZS-LAA는 zeroshot 프롬프트가 있는 LLM에서 동작을 생성합니다.ZST-LAA는 self-think가 있는 ZS-LAA를 확장합니다.ReAct LAA는 fewshot 프롬프트가 있는 ZST-LAA를 발전시킵니다.모두 동작을 통해 환경과 상호 작용하여 관찰을 수집하여 주어진 작업을 해결합니다.색상으로 더 잘 볼 수 있습니다.3.1 SOLO 에이전트 이하에서 5가지 다른 LAA를 제시합니다. 각 유형의 LAA는 자체 상호 작용 전략으로 환경과 상호 작용할 수 있습니다.Zeroshot LAA(ZS-LAA)는 LLM을 직접 확장하여 작업 실행자가 됩니다.특히, LLM이 작업 실행자로 기능하도록 하는 프롬프트는 해당 작업에 대한 자세한 설명으로 구성됩니다.예를 들어, &quot;클릭: 이 작업을 사용하여 [버튼]을 클릭하면 관찰된 []에 클릭 가능한 버튼이 있습니다.&quot;라는 클릭 작업을 이해하도록 LAA에 프롬프트하는 경우 웹 탐색 에이전트로 작동할 수 있습니다.그림 1(a)에서 ZS-LAA의 아키텍처를 제시합니다.작업 흐름은 다음과 같습니다.• 초기 단계: 먼저 ZS-LAA가 작업 지침을 수신하고 zeroshot 프롬프트를 구성합니다.그런 다음 LLM 계층이 가능한 응답을 생성하고 이를 구문 분석하여 실행 가능한 작업을 출력합니다.그런 다음 환경의 관찰 내용이 에이전트 메모리에 추가됩니다.• 작업 단계: 에이전트가 작업이 완료되었는지 확인합니다. 그렇지 않은 경우, ZS-LAA는 메모리에서 이전 작업과 관찰을 검색하고 LLM이 다음 실행 가능한 작업을 생성하도록 프롬프트를 구성합니다.ZS-LAA는 최대 단계에 도달하거나 작업을 완료할 때까지 작업 단계를 계속합니다.ZS-LAA는 최소 LAA 아키텍처입니다.이는 새로운 환경으로 일반화하기 쉽고 예제가 필요하지 않은 zeroshot 프롬프트 계층을 통해 LLM의 작업 생성 기능을 활성화합니다.ZeroshotThink LAA(ZST-LAA)는 ZS-LAA의 확장 버전입니다.ZS-LAA와 달리 ZSTLAA에는 추가 자기 생각 흐름이 있습니다.ZST-LAA의 아키텍처는 그림 1(b)에 나와 있으며, 여기서 자기 생각 흐름을 분홍색 화살표 선으로 표시합니다.자기 생각은 작업 생성 흐름의 중간 단계에서 실행되어 사슬 사고(CoT) 추론 능력을 활성화합니다.• 자기 생각 단계: 다음 작업을 생성하기 전에 ZST-LAA는 관찰과 이전 작업을 수집하여 생각 프롬프트를 구성합니다.그런 다음 생각이 메모리에 저장됩니다. 자기 사고 단계는 일반적으로 추론 과제가 주어졌을 때 유용합니다. 참고: 생각 프롬프트는 &quot;생각: 이 액션을 사용하여 액션과 추론을 계획합니다&quot;와 같이 제로샷 형식입니다. ReAct LAA는 또한 프롬프트 계층에서 ZST-LAA를 발전시키며, 여기서 fewshot 예제가 제공됩니다. ReAct LAA의 아키텍처는 그림 1(c)에 나와 있습니다. ReAct LAA는 성공적인 실행 예제를 활용하여 LLM의 액션 생성 능력을 개선하고 LAA의 환경 상호 작용을 향상시킬 수 있습니다. 이러한 fewshot 예제는 LLM의 컨텍스트 내 학습 능력을 부여하기 때문입니다. 그러나 ReAct LAA의 단점은 컨텍스트 길이가 제한되어 프롬프트에서 fewshot 예제를 차지한 후 사용할 수 있는 토큰 공간이 적다는 것입니다. PlanAct LAA는 LAA의 계획 능력을 용이하게 하도록 설계되었습니다. PlanAct LAA는 ZSLAA와 두 가지 부분이 다릅니다. 1) 계획 흐름과 2) fewshot 프롬프트입니다. 아키텍처는 다음과 같습니다. 사전 인쇄 환경 액션 관찰 계획 작업 지침 액션 파서 프롬프트 계획 계획 메모리 환경 액션 관찰 액션 파서 프롬프트 계획 계획 작업 지시 LLM 사고 계획 메모리 Fewshot 프롬프트(a) PlanAct LAA Fewshot 프롬프트 생각(a) PlanReAct LAA 그림 2: PlanAct LAA 및 PlanReAct LAA의 LAA 아키텍처.LLM • 그림 2에서.플랜닝 흐름은 초기 작업 생성 단계 전에 실행되며, 여기에는 핵심 LLM에 대한 입력을 구성하기 위한 추가 계획 프롬프트가 있습니다.• 계획 단계: PlanAct LAA는 환경과 상호 작용하기 전에 주어진 작업에 대한 계획을 생성합니다.계획은 기억되고 프롬프트를 구성하기 위해 검색됩니다.이 논문의 계획 프롬프트는 LAA가 이전의 성공적인 계획을 기반으로 계획을 생성할 수 있도록 하는 fewshot 방식이라는 점에 주목할 가치가 있습니다.PlanReAct LAA는 CoT 기능도 활성화하는 추가 자체 생각 흐름으로 PlanAct LAA를 확장합니다. PlanReAct LAA의 아키텍처는 그림 2에 나와 있습니다. 직관적으로, LAA가 환경을 관찰하기 전에 Planning 흐름이 실행되므로, self-think 흐름은 잘못된 계획으로 인해 발생하는 환각을 완화합니다. 다음으로, 다중 에이전트 오케스트레이션 아키텍처, 즉 BOLAA를 소개합니다. 3.2 BOLAA: 다중 에이전트 오케스트레이션. 환경 관찰 노동 에이전트 풀 LAALAA m LAAAction 파서 메모리 LLM LLM LLM 에이전트 선택 통신 에이전트 에이전트 에이전트 에이전트 메시지 프롬프트 프롬프트 프롬프트 컨트롤러 작업 작업 지시 그림 3: 여러 LAAS를 오케스트레이션하기 위해 컨트롤러를 사용하는 BOLAA 아키텍처. 기존 LLM이 다양한 언어 이해 작업을 완료하는 데 성공했지만, 컨텍스트 길이 제약, 컨텍스트 내 학습 및 일반화 능력 등과 같은 많은 문제가 여전히 탐구되지 않았습니다. 따라서 모든 작업을 완료하기 위해 솔로 LAA를 사용하는 것은 어려운 일이며, 특히 작업의 복잡성이 높은 경우에는 더욱 그렇습니다. 따라서 그림 3에 나와 있는 여러 LAA를 조율하기 위한 새로운 에이전트 아키텍처를 제안합니다. BOLAA에는 노동 에이전트 풀과 컨트롤러라는 두 가지 주요 모듈이 있습니다. 노동 에이전트 풀은 여러 LAA를 관리합니다. 각 LAA는 한 가지 유형의 작업을 생성하는 데만 집중할 수 있습니다. 예를 들어, 웹 탐색 환경에서 클릭 LAA를 설정하고 LAA를 검색할 수 있습니다. 이런 식으로 전자는 클릭할 다음 버튼만 생성하는 반면 후자는 검색 쿼리만 출력하여 복잡한 작업을 실행 가능한 작업으로 나눕니다. 컨트롤러는 에이전트 풀에서 LAA를 선택적으로 호출하도록 고안되었습니다. 컨트롤러에는 호출할 가장 관련성 있는 LAA를 선택하기 위한 에이전트 선택PREPRINT 계층이 있습니다. 그런 다음 컨트롤러는 선택된 LAA에 대한 메시지를 구성하고 통신을 구축합니다. 노동 LAA에서 응답을 얻은 후 컨트롤러는 이를 실행 가능한 작업으로 구문 분석한 다음 환경과 상호 작용합니다. 이러한 노동 LAA를 생각/계획 에이전트로 설계할 수도 있습니다. 이런 식으로 자체 생각 및 계획 작업 흐름도 유지됩니다. 4 실험 4. 환경 벤치마크 우리는 두 가지 환경, 즉 WebShop(Yao et al., 사전 인쇄본)과 Wikipedia API 사용(Yao et al., 2023a)을 가진 HotPotQA(Yang et al., 2018)에서 평가 벤치마크를 구성합니다. WebShop은 118만 개의 실제 제품과 인간의 지시가 있는 최근 제안된 온라인 쇼핑 웹사이트 환경입니다. 각 지시는 하나의 기준 진실 제품과 연관되며 속성 요구 사항을 포함합니다. 예를 들어, 퀵 릴리스가 가능하고 휴대하기 쉬우며 가격이 130.00달러 미만인 여행용 모노포드 카메라 삼각대를 찾고 있습니다. 이 지시에는 &quot;퀵 릴리스&quot;, &quot;카메라 삼각대&quot; 및 &quot;휴대하기 쉬운&quot; 속성의 3가지 속성 요구 사항이 포함됩니다. 우리는 속성 요구 사항의 수를 사용하여 지시의 복잡도를 정의합니다. 따라서 위의 지시 예는 복잡도 3입니다. 우리는 각 복잡도 수준에 대해 150개의 지시를 동일하게 샘플링합니다. 6보다 큰 복잡도에 대한 명령어가 150개 미만이므로 복잡도에서 명령어만 {1,2,.6}에 포함시키고, 이는 WebShop 환경에서 벤치마크 평가를 위한 900개 작업을 합산합니다. WebShop 환경에서 에이전트는 LAA의 대화형 의사 결정 능력을 평가하기 위해 환경과 상호 작용하기 위해 SEARCH[QUERY] 또는 CLICK[ELEMENT] 작업을 수행합니다. WebShop에서 관찰한 바에 따르면 클릭 가능한 버튼과 관련 페이지 콘텐츠가 포함된 간소화된 웹 브라우저입니다. LAA는 웹 탐색 에이전트로서 WebShop 환경과 상호 작용합니다. ... Wikipedia API가 포함된 HotPotQA는 이 논문에서 고려하는 또 다른 환경으로, 두 개 이상의 Wikipedia 구절에 대한 추론이 필요한 작업에 답하는 다중 홉 질문이 포함되어 있습니다. 이 시뮬레이션 환경은 AI 모델의 다단계 계획 및 이해 능력과 정보 검색 기술을 평가하는 강력한 도구 역할을 하며, 방대한 온라인 리소스에서 신뢰할 수 있는 정보를 소싱하는 데 능숙하도록 보장합니다. 실제 인터넷 브라우징 시나리오와 텍스트 분석을 독특하게 조합한 HotpotQA는 증강된 대규모 언어 에이전트 시스템의 발전을 위한 귀중한 자산입니다.HotPotQA 환경에서 에이전트는 HotPotQA 환경과 상호 작용하기 위해 SEARCH[ENTITY], LOOKUP[STRING] 및 FINISH[ANSWER]의 세 가지 유형의 동작을 합니다.HotPotQA 환경은 LAA의 지식 추론 능력을 평가하는 것을 목표로 합니다.쉬움, 보통, 어려움 수준에서 100개의 질문을 무작위로 샘플링하여 LAAS를 평가하기 위한 최종 300개의 벤치마크 질문을 구성합니다.4.평가 지표 주로 각 환경에서 보상 점수를 사용하여 LAA의 성과를 평가합니다.WebShop 환경에서 보상은 구매한 품목과 기준 진실 품목 간의 속성 중복 비율로 정의됩니다.HotPotQA 환경에서 보상은 에이전트 답변과 기준 진실 답변 간의 F1 점수 등급으로 정의됩니다. 또한, 우리는 WebShop 환경에 대한 리콜 성능을 개발하는데, 이는 기준 진실 항목이 검색되면 1로 정의되고, 한 작업 세션 동안 검색되지 않으면 0으로 정의됩니다. 리콜은 WebShop 환경의 모든 작업에 대한 평균 리콜 점수로 보고됩니다. 4.3 LLM 활용 LAA의 핵심 구성 요소는 LLM 백본입니다. 우리는 다양한 모델 크기와 컨텍스트 길이를 선택하여 서로 다른 LLM을 비교합니다. 우리는 fastchat-3b, vicuna-3b/13b/33b(Zheng et al., 2023), Llama-2-7b/13b/70b6(Touvron et al., 2023), MPT-7b/30b(Team, 2023), xgen-8k-7b, longchat-16k-7b/13b 및 text-davinci-003, gpt-3.5-turbo 및 gpt-3.5-turbo-16k를 포함한 OpenAI API LLM과 같은 개방형 LLM 모델에 대한 결과를 보고했습니다. 6모든 Llama-2 모델은 -chat-hf 버전입니다.사전 인쇄 표 1: WebShop 환경에서의 평균 보상.Len은 최대 컨텍스트 길이를 나타냅니다.굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄 친 결과는 한 열에서 가장 좋은 성능을 나타냅니다. 즉, 한 LAA 아키텍처에 대한 가장 좋은 LLM입니다. LAA 아키텍처 LLM Len. ZS ZST fastchat-t5-3b 2k 0.0.0.ReAct PlanAct PlanReAct BOLAA 0.0.0.vicuna-7b 2k 0.0.0.0.0.0.vicuna-13b 2k 0.0.0.0.0.0.vicuna-33b 2k 0.1356 0.2049 0.0.0.0.llama-2-7b 4k 0.0042 0.0.0.0.0.llama-2-13b 4k 0.0662 0.0.0.0.0.llama-2-70b 4k 0.0122 0.0080 0.0.0.0.mpt-7b-instruct 8k 0.0.0001 0.0.0.0.mpt-30b-지시 8k 0.0.0.0.0.0.xgen-8k-7b-지시 8k 0.0001 0.롱챗-7b-16k 16k 0.0165 0.롱챗-13b-16k 16k 0.0007 0.0007 0.텍스트-다빈치-0.0.0.0.0.069 0.0.0.0.0.0.0.4k 0.5292 0.gpt-3.5-터보 4k 0.5061 0.5057 0.5383 0.gpt-3.5-터보-16k 16k 0.5657 0.5642 0.4898 0.0.0.0.0.0.0.0.0.4.의사결정 시뮬레이션 이 섹션에서는 WebShop 환경에서 LAA의 의사 결정 성능을 제시하고 비교합니다.평균 보상에 대한 성능은 표 1에 보고되어 있습니다.에이전트 프롬프트는 다양한 LLM 모델의 최대 컨텍스트 길이를 기반으로 구성됩니다.BOLAA와 관련하여 검색 쿼리와 클릭 요소를 각각 생성하기 위해 하나의 검색 LAA와 하나의 클릭 LAA를 고안합니다.다음과 같은 관찰 결과가 있습니다.• BOLAA는 다른 LAA 아키텍처와 비교하여 가장 좋은 성능을 발휘하며, 특히 성능이 높은 LLM에 구축된 경우 더욱 그렇습니다.BOLAA는 적절한 LAA를 적극적으로 선택하고 질적 커뮤니케이션을 생성할 수 있어 작업 생성이 안정화됩니다.BOLAA는 3b fastchat-t5 LLM과 페어링될 때 더 강력한 LLM을 갖춘 다른 LAA 아키텍처와 비슷한 성능을 발휘하는 것을 관찰했습니다. BOLAA의 우수성은 컴퓨팅 리소스가 제한되어 있는 경우 여러 개의 작은 LAA를 조율하는 것이 더 나은 선택임을 나타냅니다. 이는 하나의 큰 일반화된 LAA를 미세 조정하는 것보다 여러 개의 작은 특수화된 LAA를 미세 조정할 수 있는 잠재력을 더욱 잘 보여줍니다. • • LLM을 최적의 LAA 아키텍처와 페어링하는 것이 중요합니다. 예를 들어, Llama-2-13b는 PlanAct LAA 아키텍처에서 가장 좋은 성능을 발휘하는 반면, Llama-2-70b는 BOLAA 아키텍처에서 가장 좋은 성능을 발휘합니다. 또한 Longchat-13b-16K는 PlanAct와 PlanReAct를 사용할 때 가장 좋은 성능을 발휘하는데, 이는 longchat-13b-16k 모델의 뛰어난 계획 능력을 나타낼 수 있습니다. 컨텍스트 길이만 늘리는 것이 반드시 LAA 성능을 개선하는 것은 아닙니다. 예를 들어, longchat-13b-16k와 llama-2-13b 모델을 비교할 때, 컨텍스트 길이가 짧더라도 후자가 더 나은 성능을 발휘합니다. LAA의 실행 로그를 확인하여 LAA가 더 많은 단계를 실행할 때 환각 생성이 더 많이 발생하는 것을 관찰했으며, 이는 결국 긴 컨텍스트의 이점을 저하시킵니다.• 강력한 LLM은 zeroshot LAA 아키텍처에서 일반화할 수 있습니다.OpenAI API 기반 모델의 최고 성능은 실제로 ZS 및 ZST 아키텍처에서 나타납니다.이는 강력한 LLM을 사용하여 일반 LAA를 개발할 수 있는 큰 잠재력을 나타냅니다.사실, 이것이 현재 오픈소스 프로젝트가 OpenAI API를 직접 호출하고 대신 zeroshot 에이전트 프롬프트를 조정하는 것을 목표로 하고 있습니다.벤치마크 결과는 ZS LAA만 사용하여 추가 Plan 또는 Self-think 흐름이 있는 LAA 아키텍처와 동등하거나 더 나은 성능을 이미 달성할 수 있음을 정량적으로 정당화합니다.그러나 덜 강력한 다른 LLM의 경우 LAA에 fewshot 프롬프트가 필요합니다.• Plan 흐름은 일반적으로 에이전트가 오픈소스 LLM에 구축될 때 성능을 개선합니다.ReAct, PlanAct 및 PlanReAct의 성능을 비교하면 성능 향상을 관찰할 수 있습니다.PREPRINT 표 2: WebShop 환경에서의 평균 리콜. Len은 최대 컨텍스트 길이를 나타냅니다. 굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄로 된 결과는 한 열의 최상의 성능, 즉 하나의 LAA 아키텍처에 대한 최상의 LLM을 나타냅니다. LAA 아키텍처 LLM Len. ZS fastchat-t5-3b 2k 0.vicuna-7b 2k 0.ZST ReAct PlanAct PlanReAct BOLAA 0.3122 0.3800 0.3700 0.0.0500 0.3600 0.0.0.0.vicuna-13b 2k 0.0.0644 0.3622 0.0.0.vicuna-33b 2k 0.0.3411 0.0.0.0.llama-2-7b 4k 0.0.0311 0.3744 0.0.0.llama-2-13b 4k 0.2856 0.0.0.0.0.llama-2-70b 4k 0.3344 0.0.3789 0.0.0.8k 0.0144 0.0.0.0.0.8k 0.0.3372 0.0.0.0.8k 0.0.1400 0.0.0.0.16k 0.0.1856 0.0.0.0.16k 0.0.0867 0.0.0.0.4k 0.3800 0.3856 0.4k 0.3889 0.3756 0.3933 0.0.3856 0.3833 0.4011 0.0.0.0.0.0.0.0.mpt-7b-지시 mpt-30b-지시 xgen-8k-7b-지시 롱챗-7b-16k 롱챗-13b-16k text-davinci-gpt-3.5-turbo gpt-3.5-turbo-16k-0613 16k 대부분 LLM 사례에서 계획 흐름을 사용할 때. 그러나 계획과 사고에는 LLM이 단계별로 추론할 수 있어야 하며, 이는 소규모 LLM의 경우 어려울 수 있습니다. 예를 들어, fastchatt5-3b는 ZS LAA 아키텍처에서 평균 이상의 성능을 발휘합니다. 그러나 PlanReAct 아키텍처에서는 성능이 크게 저하됩니다. 또한 표 2에 나와 있는 모든 LAA에 대한 중간 회수 성능을 보고합니다. 회수는 주로 검색 작업과 관련이 있습니다. 높은 회수 성능은 LAA가 정확한 검색 쿼리를 생성할 수 있음을 나타냅니다. 높은 회수는 일반적으로 더 나은 보상으로 이어집니다. 그러나 이들은 밀접하게 관련되어 있지 않습니다. 예를 들어, Llama-2-70b는 ZS LAA에서 거의 0.3344의 회수 성능을 보이며, 이는 최상의 LAA와 비슷합니다. 그러나 ZS LAA Llama-2-70b의 표 1에서 보상 성능은 0.0122에 불과합니다. 그 이유는 검색 쿼리를 생성하는 데는 올바른 클릭 동작을 생성하는 데 필요한 LLM 능력과 다른 LLM 능력이 필요하기 때문이며, 후자는 더 어렵기 때문입니다. 또 다른 관찰 결과는 제안한 BOLAA가 일반적으로 모든 LLM에서 가장 좋은 성능을 보인다는 것입니다. 즉, 검색 에이전트를 클릭 에이전트에서 분리하면 검색 동작의 정확도가 향상되어 더 높은 회수 값이 발생한다는 것을 나타냅니다. 복잡성에 대한 LAA 성능. 이러한 LAA와 LLM의 전체 성능을 비교한 후 작업 복잡성에 대한 성능에 대한 자세한 조사를 수행합니다. 공간 제한으로 인해 text-davinci-003과 llama-2-70b의 성능만 보고합니다. 보상 성능은 그림 4에 나와 있습니다. BOLAA 모델은 모든 복잡성 수준에서 일관되게 더 나은 성능을 보입니다. 또한 작업 복잡성이 증가하면 성능이 저하되는 것을 관찰했는데, 이는 직감에 따른 것입니다. 놀랍게도, 4보다 큰 작업의 복잡성을 더 증가시켜도 성능이 더 저하되지 않는다는 것을 알아냈습니다. 그 이유는 작업의 복잡성이 높을수록 리콜 성능이 증가하기 때문이며, 이는 그림 5에서 보여주었습니다. 이는 고복잡도 작업 지시가 LAA에 대한 추가 컨텍스트 정보를 더 많이 제공하기 때문입니다. 따라서 검색 작업은 높은 복잡성 수준에서 더 구체적이고 정확할 수 있습니다. 4. 지식 추론 시뮬레이션 HotPotQA 환경을 벤치마킹하여 LAAS의 다단계 추론 능력을 평가합니다. 이 환경에서 사용 가능한 검색, 조회 및 완료 작업은 모두 지식 추론과 관련이 있고 분리하기 어렵기 때문에 BOLAA 아치는 향후 작업으로 남겨두고 다른 에이전트 아치의 성능만 비교합니다. 결과는 표 3에 나와 있습니다. 일반적으로 ReAct 에이전트 아치가 가장 좋은 성능을 달성하는데, 이는 두 가지로 해석할 수 있습니다. 첫째, LAA에 대한 액션 생성 및 추론 능력을 활성화하려면 fewshot 프롬프트가 필요하며, 특히 PREPRINT Reward 0.0.ZS ZST ReAct 0.0.PlanAct 0.PlanReAct BOLAA 0.40.0.0.0.0.0.작업 지침의 복잡성(a) text-davinci-Reward € 0.0.20.1ZS ZST ReAct PlanAct PlanReAct BOLAA 0.작업 지침의 복잡성(b) Llama-2-70b 그림 4: WebShop의 작업 복잡성에 대한 보상. 각 막대는 하나의 LAA를 나타냅니다. 0.ZS ZST ZS 0.ZST ReAct ReAct PlanAct PlanAct 0.4PlanReAct 0.PlanReAct BOLAA BOLAA 0.0.30.0.0.작업 지침의 복잡성 (a) text-davinci-0.0.10.작업 지침의 복잡성 (b) Llama-2-70b 그림 5: WebShop에서 작업 복잡성에 대한 회상. 각 막대는 하나의 LAA를 나타냅니다. 이러한 소규모 언어 모델을 실험합니다. 둘째, ReAct, PlanAct 및 PlanReAct를 비교하면 LAA의 흐름 계획이 지식 추론 환경 및 작업에서 성과를 방해한다는 결론을 내릴 수 있습니다. 그 이유는 지식 추론 작업은 추론을 수행하기 위해 맥락화된 정보가 필요한 반면 흐름 계획은 상호 작용에 앞서 실행되기 때문입니다. 따라서 생성된 계획은 LAA에 대한 환각을 더 많이 유발하는 경향이 있습니다. 셋째, 이 지식 추론 작업과 관련하여 모델 크기는 맥락 길이보다 훨씬 더 중요합니다. 대형 모델은 추론 능력이 더 뛰어나 더 나은 성능을 보입니다. 또한 OpenAI gpt-3.5 모델의 뛰어난 추론 능력이 다시 한번 검증되었습니다. 또한 모든 오픈소스 LLM에서 Llama2-70b의 최고 성능을 관찰했는데, 이는 잠재적인 향후 미세 조정이 Llama-2 모델에 적용될 수 있음을 시사합니다. 복잡성에 대한 LAA 성능. 쉬운, 중간, 높은 수준의 작업이 있으므로 그림 6에서와 같이 Llama-2-70b의 성능을 다양한 수준의 복잡성과 관련하여 비교합니다. 작업의 복잡성을 높이면 성능이 저하되는 것을 관찰했습니다. HotPotQA 작업에서 난이도는 질문 답변 홉으로 정의됩니다. 따라서 어려운 질문은 LAA의 더 많은 맥락 이해 및 추론 능력이 필요합니다. OpenAI text-davinci-003 모델은 모든 수준의 복잡성에서 Llama-2-70b보다 지속적으로 우수한 성능을 보이지만 어려운 질문에서는 차이가 더 작습니다. 어려운 질문은 더 많은 추론 노력이 필요하므로 Llama2-70b는 text-davinci-003와 비슷한 추론 능력을 가지고 있다고 결론 내릴 수 있습니다.PREPRINT 표 3: HotPotQA 환경에서의 평균 보상. Len은 최대 컨텍스트 길이를 나타냅니다. 굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄로 된 결과는 한 열의 최상의 성과, 즉 하나의 LAA 아키텍처에 대한 최상의 LLM을 나타냅니다. 0.0.4LAA 아키텍처 LLM Len. ZS fastchat-t5-3b vicuna-7b 2k vicuna-13b 2k ZST ReAct PlanAct 2k 0.0252 0.0067 0.0692 0.0.1339 0.0797 0.0318 0.0.1541 0.0910 0.PlanReAct 0.0.0.0.vicuna-33b 2k 0.0.2223 0.0.0.llama-2-7b 4k 0.0.0207 0.0.0.llama-2-13b 4k 0.1731 0.2313 0.0.0.llama-2-70b 4k 0.2809 0.3207 0.0.0.mpt-7b-8k 지시 mpt-30b-인스트럭트 8k 0.0982 0.0483 0.0.1562 0.0.0.0.0.0.xgen-8k-7b-인스트럭트 8k 0.1502 0.1244 0.0.0.롱챗-7b-16k 16k 0.0791 0.0672 0.0.0.롱챗-13b-16k 16k 텍스트-다빈치-4k 0.0.3430 0.3304 0.0.0562 0.0.0.0.0.gpt-3.5-터보 4k 0.3340 0.3254 0.0.0.gpt-3.5-터보-16k-0613 16k 0.3027 0.2264 0.0.0.0.0.350.0.25ZS ZST ZS 0.ZST ReAct 0.PlanAct 0.PlanReAct 0.0.0.0.ReAct PlanAct PlanReAct 0.0.easy 중간 작업 지침의 복잡성 어려움 쉬움 중간 작업 지침의 복잡성 어려움 (a) text-davinci-(b) Llama-2-70b 그림 6: HotPotQA의 복잡성 수준에 대한 보상. 각 막대는 하나의 LAA를 나타냅니다. 5 결론 및 향후 작업 이 논문에서 우리는 다양한 LLM 백본과 페어링된 다양한 LAA 아키텍처의 성능을 체계적으로 조사합니다. 또한 우리는 하나의 새로운 오케스트레이션을 제공합니다.
--- METHOD ---
환경과 상호 작용한 다음 연속적으로 다음 동작을 생성합니다. Langchain은 LAA를 개발하기 위한 최근 출시된 오픈 소스 프레임워크입니다. 초기 조사로 인해 LAA는 다소 탐색이 부족합니다. 첫째, 최적의 에이전트 아키텍처가 결정되지 않았습니다. ReAct(Yao et al., 2023a)는 LLM이 컨텍스트 내 학습을 통해 다음 동작을 생성하는 방법을 학습할 수 있도록 사전 정의된 예제로 에이전트에게 프롬프트합니다. 게다가 ReAct는 에이전트가 동작 실행 전에 중간 추론 단계를 거쳐야 한다고 주장합니다. ReWOO(Xu et al., 2023)는 LAA에 대한 추가 계획 단계를 도입합니다. Langchain은 zhiweiliu@salesforce.com을 사용하여 ReAct 에이전트를 일반화합니다. https://github.com/yoheinakajima/babyagi 2 https://platform.openai.com/docs/api-reference 3 https://github.com/Significant-Gravitas/Auto-GPT A https://github.com/langchain-ai/langchainPREPRINT 제로샷 도구 사용 능력. 본질적으로 에이전트의 최적 아키텍처는 작업과 관련 LLM 백본에 맞춰야 하며, 이는 기존 작업에서 덜 탐구되었습니다. 둘째, LAA에서 기존 LLM의 효능을 이해하는 것은 포괄적이지 않습니다. 기존의 예비 작업은 몇 가지 LLM 백본의 성능만 비교합니다. ReAct는 PaLM(Chowdhery et al., 2022)을 백본 LLM으로 채택합니다. ReWOO는 에이전트 계획을 위한 Alpaca 모델(Taori et al., 2023)에 대한 명령어 튜닝을 위해 OpenAI text-davinci-003 모델을 사용합니다. MIND2Web(Deng et al., 2023)은 일반 웹 에이전트를 위해 Flan-T5와 OpenAI GPT3.5/4를 비교합니다. 그럼에도 불구하고 현재 작업 중 다양한 사전 훈련된 LLM과 관련하여 LAA의 성능을 종합적으로 비교하는 작업은 거의 없습니다. 아주 최근의 작업(Liu et al., 2023)은 LLM을 에이전트로 평가하기 위한 벤치마크를 출시합니다. 그럼에도 불구하고 그들은 LLM 백본과 함께 에이전트 아키텍처를 공동으로 고려하지 못합니다. 효능과 효율성 관점에서 최적의 LLM을 선택하면 LAA에 대한 현재 탐색이 진전됩니다. 셋째, 작업의 복잡성이 증가함에 따라 여러 에이전트의 오케스트레이션이 필요할 수 있습니다. ReWOO는 최근 추론을 관찰에서 분리하면 LAA의 효율성이 향상된다는 것을 확인했습니다. 이 논문에서 우리는 작업 복잡성이 증가함에 따라, 특히 오픈 도메인 환경에서, 하나의 작업을 완료하기 위해 여러 에이전트를 조정하는 것이 더 낫다고 주장합니다. 예를 들어, 웹 탐색 작업과 관련하여, 우리는 클릭 가능한 버튼과 상호 작용하고 다른 검색 에이전트에게 추가 리소스를 검색하도록 요청하기 위해 하나의 클릭 에이전트를 사용할 수 있습니다. 그럼에도 불구하고, 여러 에이전트를 조정하는 방법과 조정의 영향을 조사하는 방법에 대해 논의하는 연구는 거의 없습니다. 이러한 연구 격차를 해소하기 위해 이 논문은 LAA의 성능을 종합적으로 비교하는 것을 제안합니다. 우리는 LAAS와 LLM 백본의 에이전트 아키텍처를 심층적으로 살펴봅니다. 구체적으로, 우리는 다양한 LLM 백본을 기반으로 구축된 다양한 에이전트 아키텍처의 성능을 평가하기 위해 기존 환경에서 에이전트 벤치마크를 구성합니다. 우리의 에이전트 벤치마크의 작업은 서로 다른 작업 복잡성 수준과 연관되어 있어 작업 복잡성과 관련된 에이전트 성능 분석이 가능합니다. 이러한 에이전트 아키텍처는 기존 설계 선택을 광범위하게 검증하도록 설계되었습니다. 여러 LAA의 오케스트레이션과 관련하여, 우리는 여러 협업 에이전트 위에 컨트롤러 모듈이 있는 새로운 LAA 아키텍처 BOLAA5를 제안하여 여러 노동 LAA 간의 선택과 통신을 가능하게 합니다. 이 논문의 기여는 다음과 같습니다. • 우리는 6개의 다른 LAA 에이전트 아키텍처를 개발합니다. 우리는 이를 다양한 백본 LLM과 결합하여 LAA의 설계 직관을 촉구, 자기 사고 및 계획에서 정당화합니다. 또한 우리는 솔로 에이전트의 액션 상호 작용 능력을 향상시키는 다중 에이전트 전략을 오케스트레이션하기 위한 BOLAA를 개발합니다. • 우리는 광범위한
--- EXPERIMENT ---
의사 결정 웹 탐색 환경과 지식 추론 작업 환경 모두에 대한 s. 우리는 LAAS와 호환되는 LLM의 최적 선택에 대한 정성적 지표를 제공하는 최종 희소 보상과 중간 회상 측면에서 성과를 보고합니다. • WebShop 환경에서 BOLAA는 다른 LAA 아키텍처와 비교하여 지속적으로 가장 우수한 성과를 보입니다. 우리의 결과는 복잡한 작업을 해결하기 위해 협업할 전문 에이전트를 설계하는 것의 중요성을 보여주는데, 이는 높은 일반화 능력을 가진 대규모 LLM을 훈련하는 것만큼 중요해야 합니다. 2 관련 연구 2. 증강 언어 에이전트 아키텍처 복잡한 작업을 완료하려면 일반적으로 여러 단계가 필요합니다. 에이전트는 이러한 단계를 이해하고 그에 따라 계획해야 합니다. CoT(Wei et al., 2022)라고도 알려진 Chain-of-Thoughts는 에이전트가 어려운 추론 작업을 더 작고 관리하기 쉬운 단계로 분해하도록 하는 획기적인 작업입니다. 반면, ReAct(Yao et al., 2023a)는 언어 및 학습 모델(LLM) 내에서 추론 및 행동에 대한 적성을 활용하여 환경과의 상호 작용적 참여를 촉진하는 것을 제안합니다. 예를 들어, 자연어로 추론 및 행동 추적 또는 API 호출을 생성하기 위해 관찰을 매핑하는 위키피디아 검색 API를 활용하는 것입니다. &quot;기억하기 쉽도록 의도적으로 논문 제목과 같은 이름을 붙였습니다. 사전 인쇄 이 에이전트 아키텍처는 HuggingGPT(Shen et al., 2023), Generative Agents(Park et al., 2023), WebGPT(Nakano et al., 2021), AutoGPT(Gravitas, 2023), BabyAGI(Nakajima, 2023), Langchain(Chase, 2023)을 포함한 다양한 응용 프로그램을 탄생시켰습니다. 그러나 이러한 접근 방식은 에이전트의 행동을 향상시키기 위해 환경 보상과 같은 귀중한 피드백을 통합하지 못하여 사전 학습된 언어 및 학습 모델(LLM)의 품질에만 의존하는 성능을 초래합니다. 자체 정제(Madaan et al., 2023a)는 단일 LLM을 생성기, 정제기 및 피드백 제공자로 사용하여 출력의 반복적 정제를 가능하게 함으로써 이러한 제한을 해결합니다. 그러나 실제 작업 기반에 특별히 맞춤화되지 않았습니다. 환경과의 상호작용. 반면, REX(Murthy et al., 2023)와 RAP(Hao et al., 2023)는 LLM을 재활용하여 포괄적인 세계 모델과 추론 에이전트로 기능합니다. 이들은 환경 보상이 있는 광대한 추론 영역 내에서 전략적 탐색을 위해 몬테카를로 트리 탐색을 통합합니다. 이 접근 방식은 복잡한 도메인에서 효과적인 탐색과 의사 결정을 용이하게 합니다. Shinn et al.(2023)은 에이전트에게 동적 메모리와 자기 반성 기능을 제공하여 추론 기술을 향상시키는 프레임워크인 Reflexion을 제시합니다. 자기 반성은 중요한 역할을 하며, 자율적인 에이전트가 과거 행동을 반복적으로 수정하고 개선하며 반복적인 오류를 방지할 수 있도록 합니다. 최근 Yao et al.(2023b)은 플러그인 회고적 언어 모델을 학습하여 정책 그래디언트 최적화를 활용하여 에이전트의 행동을 환경별 보상과 일치시키는 프레임워크인 Retroformer를 제안합니다. 2. 웹 에이전트 웹 탐색은 인간이 정보를 수집하고 소통하는 기반입니다. LLM이 붐을 일으키기 전에 이전 노력(Liu et al., 2018; Shi et al., 2017)은 이미 웹 시뮬레이션 환경에서 웹 에이전트를 훈련하는 방법을 탐구했습니다. 아주 최근에 일련의 작업이 복잡한 웹 탐색 작업을 처리하기 위해 LAA를 개발하는 데 전념했습니다. 웹 탐색의 작업 공간은 온라인에서 사용 가능한 요소가 많기 때문에 거의 무한하지만 이러한 작업은 클릭, 입력 및 선택과 같은 몇 가지 작업 유형으로 나눌 수 있습니다. MIND2Web(Deng et al., 2023)은 웹 브라우저 데이터를 수집하여 LLM을 미세 조정하여 실행 가능한 작업을 생성하며, 이는 웹 LAA 역할을 합니다. WebAgent(Gur et al., 2023)는 작업 지침을 하위 작업으로 분해하여 웹 탐색을 위한 실행 가능한 Python 프로그램을 직접 생성할 수 있습니다. WebArena(Zhou et al., 2023)는 웹 LAA를 설계하기 위한 현실적인 작업 시뮬레이션을 지원합니다. Langchain과 ChatGPT는 모두 LLM이 웹 LAA처럼 동작하도록 편리한 웹 플러그인을 제공합니다. 우리는 웹 탐색이 LAA가 우월성을 빛내기 위한 다음 기본 과제라고 믿습니다. 2. 도구 에이전트 LLM의 진화와 다양한 도구와의 상호 작용은 최근 연구의 초점이었습니다. &quot;도구 에이전트&quot;라는 개념은 LLM이 외부 도구를 활용하여 기능을 향상하고 복잡한 작업을 해결한다는 아이디어를 요약합니다. 이 분야의 선구적인 작업 중 하나는 &quot;Gorilla&quot;(Patil et al., 2023)의 도입입니다. 이 모델은 API 호출을 작성하는 데 능숙하며 테스트 시간 문서 변경 사항을 적응시키는 기능을 보여줍니다. 또 다른 주목할 만한 작업은 &quot;ToolLLM&quot; 프레임워크(Qin et al., 2023)입니다. 이 오픈 소스 프레임워크는 LLM을 통합하여 복잡한 작업을 실행하기 위해 수많은 도구, 특히 API와 효율적으로 상호 작용합니다. 프레임워크는 도구 활용에 맞춰 조정된 명령어 튜닝 데이터 세트인 ToolBench를 포함합니다.최근에 LLM에게 새로운 도구를 사용하도록 가르치는 패러다임 전환이 (Hsieh et al., 2023)에서 논의되었으며, 이는 도구 설명서 사용을 옹호합니다.저자는 도구 설명서가 도구 사용에 대한 자세한 설명을 제공하며, 이는 보다 효과적이고 확장 가능한 접근 방식이라는 것을 시사하는 경험적 증거를 제시합니다.특히, 그들의 연구에 따르면 도구 설명서에만 기반한 제로샷 프롬프트가 퓨샷 프롬프트의 성능과 맞먹을 수 있습니다.에이전트 아키텍처 이 섹션에서는 다양한 LAA 아키텍처를 비교합니다.먼저 기존 작업의 직관에 따라 다양한 솔로 LAA를 설계하는 방법을 제시합니다. 그런 다음 다중 LAA, 즉 BOLAA의 오케스트레이션 디자인을 제시합니다.PREPRINT 환경 동작 동작 파서 관찰 환경 동작 동작 파서 관찰 환경 동작 관찰 동작 파서 작업 지시 LLM Zeroshot 프롬프트 (a) Zeroshot LAA 메모리 작업 지시 LLM 메모리 생각 생각 Zeroshot 프롬프트 (b) ZeroshotThink LAA 작업 지시 LLM Fewshot 프롬프트 메모리 생각 생각 (c) ReAct LAA 그림 1: Zeroshot-LAA(ZS-LAA), ZeroshotThink LAA(ZST-LAA) 및 ReAct LAA의 LAA 아키텍처.ZS-LAA는 zeroshot 프롬프트가 있는 LLM에서 동작을 생성합니다.ZST-LAA는 self-think가 있는 ZS-LAA를 확장합니다.ReAct LAA는 fewshot 프롬프트가 있는 ZST-LAA를 발전시킵니다.모두 동작을 통해 환경과 상호 작용하여 관찰을 수집하여 주어진 작업을 해결합니다.색상으로 더 잘 볼 수 있습니다.3.1 SOLO 에이전트 이하에서 5가지 다른 LAA를 제시합니다. 각 유형의 LAA는 자체 상호 작용 전략으로 환경과 상호 작용할 수 있습니다.Zeroshot LAA(ZS-LAA)는 LLM을 직접 확장하여 작업 실행자가 됩니다.특히, LLM이 작업 실행자로 기능하도록 하는 프롬프트는 해당 작업에 대한 자세한 설명으로 구성됩니다.예를 들어, &quot;클릭: 이 작업을 사용하여 [버튼]을 클릭하면 관찰된 []에 클릭 가능한 버튼이 있습니다.&quot;라는 클릭 작업을 이해하도록 LAA에 프롬프트하는 경우 웹 탐색 에이전트로 작동할 수 있습니다.그림 1(a)에서 ZS-LAA의 아키텍처를 제시합니다.작업 흐름은 다음과 같습니다.• 초기 단계: 먼저 ZS-LAA가 작업 지침을 수신하고 zeroshot 프롬프트를 구성합니다.그런 다음 LLM 계층이 가능한 응답을 생성하고 이를 구문 분석하여 실행 가능한 작업을 출력합니다.그런 다음 환경의 관찰 내용이 에이전트 메모리에 추가됩니다.• 작업 단계: 에이전트가 작업이 완료되었는지 확인합니다. 그렇지 않은 경우, ZS-LAA는 메모리에서 이전 작업과 관찰을 검색하고 LLM이 다음 실행 가능한 작업을 생성하도록 프롬프트를 구성합니다.ZS-LAA는 최대 단계에 도달하거나 작업을 완료할 때까지 작업 단계를 계속합니다.ZS-LAA는 최소 LAA 아키텍처입니다.이는 새로운 환경으로 일반화하기 쉽고 예제가 필요하지 않은 zeroshot 프롬프트 계층을 통해 LLM의 작업 생성 기능을 활성화합니다.ZeroshotThink LAA(ZST-LAA)는 ZS-LAA의 확장 버전입니다.ZS-LAA와 달리 ZSTLAA에는 추가 자기 생각 흐름이 있습니다.ZST-LAA의 아키텍처는 그림 1(b)에 나와 있으며, 여기서 자기 생각 흐름을 분홍색 화살표 선으로 표시합니다.자기 생각은 작업 생성 흐름의 중간 단계에서 실행되어 사슬 사고(CoT) 추론 능력을 활성화합니다.• 자기 생각 단계: 다음 작업을 생성하기 전에 ZST-LAA는 관찰과 이전 작업을 수집하여 생각 프롬프트를 구성합니다.그런 다음 생각이 메모리에 저장됩니다. 자기 사고 단계는 일반적으로 추론 과제가 주어졌을 때 유용합니다. 참고: 생각 프롬프트는 &quot;생각: 이 액션을 사용하여 액션과 추론을 계획합니다&quot;와 같이 제로샷 형식입니다. ReAct LAA는 또한 프롬프트 계층에서 ZST-LAA를 발전시키며, 여기서 fewshot 예제가 제공됩니다. ReAct LAA의 아키텍처는 그림 1(c)에 나와 있습니다. ReAct LAA는 성공적인 실행 예제를 활용하여 LLM의 액션 생성 능력을 개선하고 LAA의 환경 상호 작용을 향상시킬 수 있습니다. 이러한 fewshot 예제는 LLM의 컨텍스트 내 학습 능력을 부여하기 때문입니다. 그러나 ReAct LAA의 단점은 컨텍스트 길이가 제한되어 프롬프트에서 fewshot 예제를 차지한 후 사용할 수 있는 토큰 공간이 적다는 것입니다. PlanAct LAA는 LAA의 계획 능력을 용이하게 하도록 설계되었습니다. PlanAct LAA는 ZSLAA와 두 가지 부분이 다릅니다. 1) 계획 흐름과 2) fewshot 프롬프트입니다. 아키텍처는 다음과 같습니다. 사전 인쇄 환경 액션 관찰 계획 작업 지침 액션 파서 프롬프트 계획 계획 메모리 환경 액션 관찰 액션 파서 프롬프트 계획 계획 작업 지시 LLM 사고 계획 메모리 Fewshot 프롬프트(a) PlanAct LAA Fewshot 프롬프트 생각(a) PlanReAct LAA 그림 2: PlanAct LAA 및 PlanReAct LAA의 LAA 아키텍처.LLM • 그림 2에서.플랜닝 흐름은 초기 작업 생성 단계 전에 실행되며, 여기에는 핵심 LLM에 대한 입력을 구성하기 위한 추가 계획 프롬프트가 있습니다.• 계획 단계: PlanAct LAA는 환경과 상호 작용하기 전에 주어진 작업에 대한 계획을 생성합니다.계획은 기억되고 프롬프트를 구성하기 위해 검색됩니다.이 논문의 계획 프롬프트는 LAA가 이전의 성공적인 계획을 기반으로 계획을 생성할 수 있도록 하는 fewshot 방식이라는 점에 주목할 가치가 있습니다.PlanReAct LAA는 CoT 기능도 활성화하는 추가 자체 생각 흐름으로 PlanAct LAA를 확장합니다. PlanReAct LAA의 아키텍처는 그림 2에 나와 있습니다. 직관적으로, LAA가 환경을 관찰하기 전에 Planning 흐름이 실행되므로, self-think 흐름은 잘못된 계획으로 인해 발생하는 환각을 완화합니다. 다음으로, 다중 에이전트 오케스트레이션 아키텍처, 즉 BOLAA를 소개합니다. 3.2 BOLAA: 다중 에이전트 오케스트레이션. 환경 관찰 노동 에이전트 풀 LAALAA m LAAAction 파서 메모리 LLM LLM LLM 에이전트 선택 통신 에이전트 에이전트 에이전트 에이전트 메시지 프롬프트 프롬프트 프롬프트 컨트롤러 작업 작업 지시 그림 3: 여러 LAAS를 오케스트레이션하기 위해 컨트롤러를 사용하는 BOLAA 아키텍처. 기존 LLM이 다양한 언어 이해 작업을 완료하는 데 성공했지만, 컨텍스트 길이 제약, 컨텍스트 내 학습 및 일반화 능력 등과 같은 많은 문제가 여전히 탐구되지 않았습니다. 따라서 모든 작업을 완료하기 위해 솔로 LAA를 사용하는 것은 어려운 일이며, 특히 작업의 복잡성이 높은 경우에는 더욱 그렇습니다. 따라서 그림 3에 나와 있는 여러 LAA를 조율하기 위한 새로운 에이전트 아키텍처를 제안합니다. BOLAA에는 노동 에이전트 풀과 컨트롤러라는 두 가지 주요 모듈이 있습니다. 노동 에이전트 풀은 여러 LAA를 관리합니다. 각 LAA는 한 가지 유형의 작업을 생성하는 데만 집중할 수 있습니다. 예를 들어, 웹 탐색 환경에서 클릭 LAA를 설정하고 LAA를 검색할 수 있습니다. 이런 식으로 전자는 클릭할 다음 버튼만 생성하는 반면 후자는 검색 쿼리만 출력하여 복잡한 작업을 실행 가능한 작업으로 나눕니다. 컨트롤러는 에이전트 풀에서 LAA를 선택적으로 호출하도록 고안되었습니다. 컨트롤러에는 호출할 가장 관련성 있는 LAA를 선택하기 위한 에이전트 선택PREPRINT 계층이 있습니다. 그런 다음 컨트롤러는 선택된 LAA에 대한 메시지를 구성하고 통신을 구축합니다. 노동 LAA에서 응답을 얻은 후 컨트롤러는 이를 실행 가능한 작업으로 구문 분석한 다음 환경과 상호 작용합니다. 이러한 노동 LAA를 생각/계획 에이전트로 설계할 수도 있습니다. 이런 식으로 자체 생각 및 계획 작업 흐름도 유지됩니다. 4 실험 4. 환경 벤치마크 우리는 두 가지 환경, 즉 WebShop(Yao et al., 사전 인쇄본)과 Wikipedia API 사용(Yao et al., 2023a)을 가진 HotPotQA(Yang et al., 2018)에서 평가 벤치마크를 구성합니다. WebShop은 118만 개의 실제 제품과 인간의 지시가 있는 최근 제안된 온라인 쇼핑 웹사이트 환경입니다. 각 지시는 하나의 기준 진실 제품과 연관되며 속성 요구 사항을 포함합니다. 예를 들어, 퀵 릴리스가 가능하고 휴대하기 쉬우며 가격이 130.00달러 미만인 여행용 모노포드 카메라 삼각대를 찾고 있습니다. 이 지시에는 &quot;퀵 릴리스&quot;, &quot;카메라 삼각대&quot; 및 &quot;휴대하기 쉬운&quot; 속성의 3가지 속성 요구 사항이 포함됩니다. 우리는 속성 요구 사항의 수를 사용하여 지시의 복잡도를 정의합니다. 따라서 위의 지시 예는 복잡도 3입니다. 우리는 각 복잡도 수준에 대해 150개의 지시를 동일하게 샘플링합니다. 6보다 큰 복잡도에 대한 명령어가 150개 미만이므로 복잡도에서 명령어만 {1,2,.6}에 포함시키고, 이는 WebShop 환경에서 벤치마크 평가를 위한 900개 작업을 합산합니다. WebShop 환경에서 에이전트는 LAA의 대화형 의사 결정 능력을 평가하기 위해 환경과 상호 작용하기 위해 SEARCH[QUERY] 또는 CLICK[ELEMENT] 작업을 수행합니다. WebShop에서 관찰한 바에 따르면 클릭 가능한 버튼과 관련 페이지 콘텐츠가 포함된 간소화된 웹 브라우저입니다. LAA는 웹 탐색 에이전트로서 WebShop 환경과 상호 작용합니다. ... Wikipedia API가 포함된 HotPotQA는 이 논문에서 고려하는 또 다른 환경으로, 두 개 이상의 Wikipedia 구절에 대한 추론이 필요한 작업에 답하는 다중 홉 질문이 포함되어 있습니다. 이 시뮬레이션 환경은 AI 모델의 다단계 계획 및 이해 능력과 정보 검색 기술을 평가하는 강력한 도구 역할을 하며, 방대한 온라인 리소스에서 신뢰할 수 있는 정보를 소싱하는 데 능숙하도록 보장합니다. 실제 인터넷 브라우징 시나리오와 텍스트 분석을 독특하게 조합한 HotpotQA는 증강된 대규모 언어 에이전트 시스템의 발전을 위한 귀중한 자산입니다.HotPotQA 환경에서 에이전트는 HotPotQA 환경과 상호 작용하기 위해 SEARCH[ENTITY], LOOKUP[STRING] 및 FINISH[ANSWER]의 세 가지 유형의 동작을 합니다.HotPotQA 환경은 LAA의 지식 추론 능력을 평가하는 것을 목표로 합니다.쉬움, 보통, 어려움 수준에서 100개의 질문을 무작위로 샘플링하여 LAAS를 평가하기 위한 최종 300개의 벤치마크 질문을 구성합니다.4.평가 지표 주로 각 환경에서 보상 점수를 사용하여 LAA의 성과를 평가합니다.WebShop 환경에서 보상은 구매한 품목과 기준 진실 품목 간의 속성 중복 비율로 정의됩니다.HotPotQA 환경에서 보상은 에이전트 답변과 기준 진실 답변 간의 F1 점수 등급으로 정의됩니다. 또한, 우리는 WebShop 환경에 대한 리콜 성능을 개발하는데, 이는 기준 진실 항목이 검색되면 1로 정의되고, 한 작업 세션 동안 검색되지 않으면 0으로 정의됩니다. 리콜은 WebShop 환경의 모든 작업에 대한 평균 리콜 점수로 보고됩니다. 4.3 LLM 활용 LAA의 핵심 구성 요소는 LLM 백본입니다. 우리는 다양한 모델 크기와 컨텍스트 길이를 선택하여 서로 다른 LLM을 비교합니다. 우리는 fastchat-3b, vicuna-3b/13b/33b(Zheng et al., 2023), Llama-2-7b/13b/70b6(Touvron et al., 2023), MPT-7b/30b(Team, 2023), xgen-8k-7b, longchat-16k-7b/13b 및 text-davinci-003, gpt-3.5-turbo 및 gpt-3.5-turbo-16k를 포함한 OpenAI API LLM과 같은 개방형 LLM 모델에 대한 결과를 보고했습니다. 6모든 Llama-2 모델은 -chat-hf 버전입니다.사전 인쇄 표 1: 웹숍 환경에서의 평균 보상.Len은 최대 컨텍스트 길이를 나타냅니다.굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄 친 결과는 한 열에서 가장 좋은 성능을 나타냅니다. 즉, 한 LAA 아키텍처에 대한 가장 좋은 LLM입니다. LAA 아키텍처 LLM Len. ZS ZST fastchat-t5-3b 2k 0.0.0.ReAct PlanAct PlanReAct BOLAA 0.0.0.vicuna-7b 2k 0.0.0.0.0.0.vicuna-13b 2k 0.0.0.0.0.0.vicuna-33b 2k 0.1356 0.2049 0.0.0.0.llama-2-7b 4k 0.0042 0.0.0.0.0.llama-2-13b 4k 0.0662 0.0.0.0.0.llama-2-70b 4k 0.0122 0.0080 0.0.0.0.mpt-7b-instruct 8k 0.0.0001 0.0.0.0.mpt-30b-지시 8k 0.0.0.0.0.0.xgen-8k-7b-지시 8k 0.0001 0.롱챗-7b-16k 16k 0.0165 0.롱챗-13b-16k 16k 0.0007 0.0007 0.텍스트-다빈치-0.0.0.0.0.069 0.0.0.0.0.0.0.4k 0.5292 0.gpt-3.5-터보 4k 0.5061 0.5057 0.5383 0.gpt-3.5-터보-16k 16k 0.5657 0.5642 0.4898 0.0.0.0.0.0.0.0.0.4.의사결정 시뮬레이션 이 섹션에서는 WebShop 환경에서 LAA의 의사 결정 성능을 제시하고 비교합니다.평균 보상에 대한 성능은 표 1에 보고되어 있습니다.에이전트 프롬프트는 다양한 LLM 모델의 최대 컨텍스트 길이를 기반으로 구성됩니다.BOLAA와 관련하여 검색 LAA 하나와 클릭 LAA 하나를 고안하여 각각 검색 쿼리와 클릭 요소를 생성합니다.다음과 같은 관찰 결과가 있습니다.• BOLAA는 다른 LAA 아키텍처와 비교하여 가장 좋은 성능을 발휘하며, 특히 성능이 높은 LLM에 구축된 경우 더욱 그렇습니다.BOLAA는 적절한 LAA를 적극적으로 선택하고 질적 커뮤니케이션을 생성할 수 있어 작업 생성이 안정화됩니다.BOLAA는 3b fastchat-t5 LLM과 페어링될 때 더 강력한 LLM을 갖춘 다른 LAA 아키텍처와 비슷한 성능을 발휘하는 것을 관찰했습니다. BOLAA의 우수성은 컴퓨팅 리소스가 제한되어 있는 경우 여러 개의 작은 LAA를 조율하는 것이 더 나은 선택임을 나타냅니다. 이는 하나의 큰 일반화된 LAA를 미세 조정하는 것보다 여러 개의 작은 특수화된 LAA를 미세 조정할 수 있는 잠재력을 더욱 잘 보여줍니다. • • LLM을 최적의 LAA 아키텍처와 페어링하는 것이 중요합니다. 예를 들어, Llama-2-13b는 PlanAct LAA 아키텍처에서 가장 좋은 성능을 발휘하는 반면, Llama-2-70b는 BOLAA 아키텍처에서 가장 좋은 성능을 발휘합니다. 또한 Longchat-13b-16K는 PlanAct와 PlanReAct를 사용할 때 가장 좋은 성능을 발휘하는데, 이는 longchat-13b-16k 모델의 뛰어난 계획 능력을 나타낼 수 있습니다. 컨텍스트 길이만 늘리는 것이 반드시 LAA 성능을 개선하는 것은 아닙니다. 예를 들어, longchat-13b-16k와 llama-2-13b 모델을 비교할 때, 컨텍스트 길이가 짧더라도 후자가 더 나은 성능을 발휘합니다. LAA의 실행 로그를 확인하여 LAA가 더 많은 단계를 실행할 때 환각 생성이 더 많이 발생하는 것을 관찰했으며, 이는 결국 긴 컨텍스트의 이점을 저하시킵니다.• 강력한 LLM은 zeroshot LAA 아키텍처에서 일반화할 수 있습니다.OpenAI API 기반 모델의 최고 성능은 실제로 ZS 및 ZST 아키텍처에서 나타납니다.이는 강력한 LLM을 사용하여 일반 LAA를 개발할 수 있는 큰 잠재력을 나타냅니다.사실, 이것이 현재 오픈소스 프로젝트가 OpenAI API를 직접 호출하고 대신 zeroshot 에이전트 프롬프트를 조정하는 것을 목표로 하고 있습니다.벤치마크 결과는 ZS LAA만 사용하여 추가 Plan 또는 Self-think 흐름이 있는 LAA 아키텍처와 동등하거나 더 나은 성능을 이미 달성할 수 있음을 정량적으로 정당화합니다.그러나 덜 강력한 다른 LLM의 경우 LAA에 fewshot 프롬프트가 필요합니다.• Plan 흐름은 일반적으로 에이전트가 오픈소스 LLM에 구축될 때 성능을 개선합니다.ReAct, PlanAct 및 PlanReAct의 성능을 비교하면 성능 향상을 관찰할 수 있습니다.PREPRINT 표 2: WebShop 환경에서의 평균 리콜. Len은 최대 컨텍스트 길이를 나타냅니다. 굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄로 된 결과는 한 열의 최상의 성능, 즉 하나의 LAA 아키텍처에 대한 최상의 LLM을 나타냅니다. LAA 아키텍처 LLM Len. ZS fastchat-t5-3b 2k 0.vicuna-7b 2k 0.ZST ReAct PlanAct PlanReAct BOLAA 0.3122 0.3800 0.3700 0.0.0500 0.3600 0.0.0.0.vicuna-13b 2k 0.0.0644 0.3622 0.0.0.vicuna-33b 2k 0.0.3411 0.0.0.0.llama-2-7b 4k 0.0.0311 0.3744 0.0.0.llama-2-13b 4k 0.2856 0.0.0.0.0.llama-2-70b 4k 0.3344 0.0.3789 0.0.0.8k 0.0144 0.0.0.0.0.8k 0.0.3372 0.0.0.0.8k 0.0.1400 0.0.0.0.16k 0.0.1856 0.0.0.0.16k 0.0.0867 0.0.0.0.4k 0.3800 0.3856 0.4k 0.3889 0.3756 0.3933 0.0.3856 0.3833 0.4011 0.0.0.0.0.0.0.0.mpt-7b-지시 mpt-30b-지시 xgen-8k-7b-지시 롱챗-7b-16k 롱챗-13b-16k text-davinci-gpt-3.5-turbo gpt-3.5-turbo-16k-0613 16k 대부분 LLM 사례에서 계획 흐름을 사용할 때. 그러나 계획과 사고에는 LLM이 단계별로 추론할 수 있어야 하며, 이는 소규모 LLM의 경우 어려울 수 있습니다. 예를 들어, fastchatt5-3b는 ZS LAA 아키텍처에서 평균 이상의 성능을 발휘합니다. 그러나 PlanReAct 아키텍처에서는 성능이 크게 저하됩니다. 또한 표 2에 나와 있는 모든 LAA에 대한 중간 회수 성능을 보고합니다. 회수는 주로 검색 작업과 관련이 있습니다. 높은 회수 성능은 LAA가 정확한 검색 쿼리를 생성할 수 있음을 나타냅니다. 높은 회수는 일반적으로 더 나은 보상으로 이어집니다. 그러나 이들은 밀접하게 관련되어 있지 않습니다. 예를 들어, Llama-2-70b는 ZS LAA에서 거의 0.3344의 회수 성능을 보이며, 이는 최상의 LAA와 비슷합니다. 그러나 ZS LAA Llama-2-70b의 표 1에서 보상 성능은 0.0122에 불과합니다. 그 이유는 검색 쿼리를 생성하는 데는 올바른 클릭 동작을 생성하는 데 필요한 LLM 능력과 다른 LLM 능력이 필요하기 때문이며, 후자는 더 어렵기 때문입니다. 또 다른 관찰 결과는 제안한 BOLAA가 일반적으로 모든 LLM에서 가장 좋은 성능을 보인다는 것입니다. 즉, 검색 에이전트를 클릭 에이전트에서 분리하면 검색 동작의 정확도가 향상되어 더 높은 회수 값이 발생한다는 것을 나타냅니다. 복잡성에 대한 LAA 성능. 이러한 LAA와 LLM의 전체 성능을 비교한 후 작업 복잡성에 대한 성능에 대한 자세한 조사를 수행합니다. 공간 제한으로 인해 text-davinci-003과 llama-2-70b의 성능만 보고합니다. 보상 성능은 그림 4에 나와 있습니다. BOLAA 모델은 모든 복잡성 수준에서 일관되게 더 나은 성능을 보입니다. 또한 작업 복잡성이 증가하면 성능이 저하되는 것을 관찰했는데, 이는 직감에 따른 것입니다. 놀랍게도, 4보다 큰 작업의 복잡성을 더 증가시켜도 성능이 더 저하되지 않는다는 것을 알아냈습니다. 그 이유는 작업의 복잡성이 높을수록 리콜 성능이 증가하기 때문이며, 이는 그림 5에서 보여주었습니다. 이는 고복잡도 작업 지시가 LAA에 대한 추가 컨텍스트 정보를 더 많이 제공하기 때문입니다. 따라서 검색 작업은 높은 복잡성 수준에서 더 구체적이고 정확할 수 있습니다. 4. 지식 추론 시뮬레이션 HotPotQA 환경을 벤치마킹하여 LAAS의 다단계 추론 능력을 평가합니다. 이 환경에서 사용 가능한 검색, 조회 및 완료 작업은 모두 지식 추론과 관련이 있고 분리하기 어렵기 때문에 BOLAA 아치는 향후 작업으로 남겨두고 다른 에이전트 아치의 성능만 비교합니다. 결과는 표 3에 나와 있습니다. 일반적으로 ReAct 에이전트 아치가 가장 좋은 성능을 달성하는데, 이는 두 가지로 해석할 수 있습니다. 첫째, LAA에 대한 액션 생성 및 추론 능력을 활성화하려면 fewshot 프롬프트가 필요하며, 특히 PREPRINT Reward 0.0.ZS ZST ReAct 0.0.PlanAct 0.PlanReAct BOLAA 0.40.0.0.0.0.0.작업 지침의 복잡성(a) text-davinci-Reward € 0.0.20.1ZS ZST ReAct PlanAct PlanReAct BOLAA 0.작업 지침의 복잡성(b) Llama-2-70b 그림 4: WebShop의 작업 복잡성에 대한 보상. 각 막대는 하나의 LAA를 나타냅니다. 0.ZS ZST ZS 0.ZST ReAct ReAct PlanAct PlanAct 0.4PlanReAct 0.PlanReAct BOLAA BOLAA 0.0.30.0.0.작업 지침의 복잡성 (a) text-davinci-0.0.10.작업 지침의 복잡성 (b) Llama-2-70b 그림 5: WebShop에서 작업 복잡성에 대한 회상. 각 막대는 하나의 LAA를 나타냅니다. 이러한 소규모 언어 모델을 실험합니다. 둘째, ReAct, PlanAct 및 PlanReAct를 비교하면 LAA의 흐름 계획이 지식 추론 환경 및 작업에서 성과를 방해한다는 결론을 내릴 수 있습니다. 그 이유는 지식 추론 작업은 추론을 수행하기 위해 맥락화된 정보가 필요한 반면 흐름 계획은 상호 작용에 앞서 실행되기 때문입니다. 따라서 생성된 계획은 LAA에 대한 환각을 더 많이 유발하는 경향이 있습니다. 셋째, 이 지식 추론 작업과 관련하여 모델 크기는 맥락 길이보다 훨씬 더 중요합니다. 대형 모델은 추론 능력이 더 뛰어나 더 나은 성능을 보입니다. 또한 OpenAI gpt-3.5 모델의 뛰어난 추론 능력이 다시 한번 검증되었습니다. 또한 모든 오픈소스 LLM에서 Llama2-70b의 최고 성능을 관찰했는데, 이는 잠재적인 향후 미세 조정이 Llama-2 모델에 적용될 수 있음을 시사합니다. 복잡성에 대한 LAA 성능. 쉬운, 중간, 높은 수준의 작업이 있으므로 그림 6에서와 같이 Llama-2-70b의 성능을 다양한 수준의 복잡성과 관련하여 비교합니다. 작업의 복잡성을 높이면 성능이 저하되는 것을 관찰했습니다. HotPotQA 작업에서 난이도는 질문 답변 홉으로 정의됩니다. 따라서 어려운 질문은 LAA의 더 많은 맥락 이해 및 추론 능력이 필요합니다. OpenAI text-davinci-003 모델은 모든 수준의 복잡성에서 Llama-2-70b보다 지속적으로 우수한 성능을 보이지만 어려운 질문에서는 차이가 더 작습니다. 어려운 질문은 더 많은 추론 노력이 필요하므로 Llama2-70b는 text-davinci-003와 비슷한 추론 능력을 가지고 있다고 결론 내릴 수 있습니다.PREPRINT 표 3: HotPotQA 환경에서의 평균 보상. Len은 최대 컨텍스트 길이를 나타냅니다. 굵은 글씨로 된 결과는 한 행의 최상의 결과, 즉 하나의 LLM에 대한 최상의 LAA 아키텍처를 나타냅니다. 밑줄로 된 결과는 한 열의 최상의 성과, 즉 하나의 LAA 아키텍처에 대한 최상의 LLM을 나타냅니다. 0.0.4LAA 아키텍처 LLM Len. ZS fastchat-t5-3b vicuna-7b 2k vicuna-13b 2k ZST ReAct PlanAct 2k 0.0252 0.0067 0.0692 0.0.1339 0.0797 0.0318 0.0.1541 0.0910 0.PlanReAct 0.0.0.0.vicuna-33b 2k 0.0.2223 0.0.0.llama-2-7b 4k 0.0.0207 0.0.0.llama-2-13b 4k 0.1731 0.2313 0.0.0.llama-2-70b 4k 0.2809 0.3207 0.0.0.mpt-7b-8k 지시 mpt-30b-인스트럭트 8k 0.0982 0.0483 0.0.1562 0.0.0.0.0.0.xgen-8k-7b-인스트럭트 8k 0.1502 0.1244 0.0.0.롱챗-7b-16k 16k 0.0791 0.0672 0.0.0.롱챗-13b-16k 16k 텍스트-다빈치-4k 0.0.3430 0.3304 0.0.0562 0.0.0.0.0.gpt-3.5-터보 4k 0.3340 0.3254 0.0.0.gpt-3.5-터보-16k-0613 16k 0.3027 0.2264 0.0.0.0.0.350.0.25ZS ZST ZS 0.ZST ReAct 0.PlanAct 0.PlanReAct 0.0.0.0.ReAct PlanAct PlanReAct 0.0.easy 중간 작업 지시의 복잡성 어려움 쉬움 중간 작업 지시의 복잡성 어려움 (a) text-davinci-(b) Llama-2-70b 그림 6: HotPotQA의 복잡성 수준에 대한 보상. 각 막대는 하나의 LAA를 나타냅니다. 5
--- CONCLUSION ---
및 향후 작업 이 논문에서 우리는 다양한 LLM 백본과 페어링된 다양한 LAA 아키텍처의 성능을 체계적으로 조사합니다. 또한 여러 에이전트, 즉 BOLAA에 대한 하나의 새로운 오케스트레이션 방법을 제공합니다. 벤치마킹 결과는 LAA 조사에 대한 실험적 정당성을 제공하고 BOLAA 아키텍처의 잠재적 이점을 검증합니다. 조사 중에 우리는 또한 복합 동작이 있는 환경에 대한 BOLAA 아키텍처를 설계하는 과제를 식별합니다. 앞으로 우리는 컨트롤러에서 LLM을 활용하여 노동 에이전트와의 선택 및 통신도 완전히 자율적으로 수행할 수 있는지 알아볼 것입니다. 우리는 더 많은 LAA 아키텍처를 계속 개발하고 평가를 위한 더 많은 LLM과 환경을 포함할 것입니다.사전 인쇄 참고문헌 Harrison Chase. Langchain. https://github.com/hwchase17/langchain, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su. Mind2web: 웹을 위한 일반 에이전트를 향해. arXiv 사전 인쇄본 arXiv:2306.06070, 2023. Significant Gravitas. Auto-GPT, 2023. Autogpt. https://github.com/Significant-Gravitas/ Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust. 계획, 긴 컨텍스트 이해 및 프로그램 합성을 갖춘 실제 세계 웹 에이전트. arXiv 사전 인쇄본 arXiv:2307.12856, 2023. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu. 언어 모델을 통한 추론은 세계 모델을 통한 계획입니다. arXiv 사전 인쇄본 arXiv:2305.14992, 2023. Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister. 도구 설명서는 대규모 언어 모델에서 제로샷 도구 사용을 가능하게 합니다. arXiv 사전 인쇄본 arXiv:2308.00675, 2023. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트를 위한 실행 가능한 지식 추출. International Conference on Machine Learning, pp. 9118–9147. PMLR, 2022. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 언어 모델은 컴퓨터 작업을 해결할 수 있습니다. arXiv 사전 인쇄본 arXiv:2303.17491, 2023. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 워크플로우 기반 탐색을 사용한 웹 인터페이스에서의 강화 학습. arXiv 사전 인쇄 arXiv:1802.08802, 2018. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang. Agentbench: llms를 에이전트로 평가, 2023년. Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig 및 Amir Yazdanbakhsh. 학습 성능 향상 코드 편집. arXiv 사전 인쇄 arXiv:2302.07867, 2023a. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 등. 자체 개선: 자체 피드백을 통한 반복적 개선. arXiv 사전 인쇄본 arXiv:2303.17651, 2023b. Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese. Rex: AI 에이전트를 위한 신속한 탐색 및 활용, 2023. Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: 인간 피드백을 통한 브라우저 지원 질문 답변. arXiv 사전 인쇄본 arXiv:2112.09332, 2021. OpenAI. Gpt-4 기술 보고서. Arxiv, 2023. 사전 인쇄본 Joon Sung Park, Joseph C O&#39;Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 생성 에이전트: 인간 행동의 상호 작용적 시뮬라크라. arXiv 사전 인쇄본 arXiv:2304.03442, 2023. Shishir G Patil, Tianjun Zhang, Xin Wang, Joseph E Gonzalez. Gorilla: 대규모 API와 연결된 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2305.15334, 2023. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings. Refiner: 중간 표현에 대한 추론 피드백. arXiv 사전 인쇄본 arXiv:2304.01904, 2023. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: 16,000개 이상의 실제 API를 마스터하기 위한 대규모 언어 모델 활용. arXiv 사전 인쇄본 arXiv:2307.16789, 2023. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. Hugginggpt: huggingface에서 chatgpt와 그 친구들을 사용하여 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580, 2023. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang. World of bits: 웹 기반 에이전트를 위한 오픈 도메인 플랫폼. International Conference on Machine Learning에서, pp. 3135-3144. PMLR, 2017. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao. 반사: 언어 강화 학습을 갖춘 언어 에이전트. arXiv 사전 인쇄 arXiv:2303.11366, 2023. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang 및 Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델입니다. https://github.com/tatsu-lab/stanford_alpaca, 2023. 모자이크 ML NLP 팀. mpt-7b 소개: 상업적으로 사용 가능한 오픈 소스 Ilms의 새로운 표준, 2023. URL www.mosaicml.com/blog/mpt-7b. 접속일: 2023-05-05. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning 마오, 자비에 마르티네, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 안젤라 팬(Angela Fan), 멜라니 캄바두르(Melanie Kambadur), 샤란 나랑(Sharan Narang), 아우렐리앙 로드리게스(Aurelien Rodriguez), 로버트 스토이닉(Robert Stojnic), 세르게이 에두노프(Sergey Edunov), 토마스 시알롬(Thomas Scialom). Llama 2: 개방형 기반 및 미세 조정된 채팅 모델, 2023년. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le 및 Denny Zhou. 생각의 연쇄 유도는 대규모 언어 모델에서 추론을 이끌어냅니다. arXiv 사전 인쇄본 arXiv:2201.11903, 2022. Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, Dongkuan Xu. Rewoo: 효율적인 증강 언어 모델을 위한 추론과 관찰 분리. arXiv 사전 인쇄본 arXiv:2305.18323, 2023. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning. HotpotQA: 다양하고 설명 가능한 멀티홉 질문 답변을 위한 데이터 세트. 자연어 처리(EMNLP)의 경험적 방법에 대한 컨퍼런스에서, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao. ReAct: 언어 모델에서 추론과 행동의 시너지 효과. International Conference on Learning Representations(ICLR), 2023a. 사전 인쇄 Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan. 웹숍: 기반 언어 에이전트를 통한 확장 가능한 실제 세계 웹 상호 작용을 향해. ArXiv에서 사전 인쇄. Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese. Retroformer: 정책 그래디언트 최적화를 통한 회고적 대규모 언어 에이전트, 2023b. Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Huan Wang, Silvio Savarese 및 Caiming Xiong. Dialogstudio: 대화형 AI를 위한 가장 풍부하고 다양한 통합 데이터세트 컬렉션을 향해, 2023년. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. mt-bench 및 챗봇 경기장을 통해 Ilm-as-a-judge 심사, 2023. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon 등. Webarena: 자율 에이전트를 구축하기 위한 현실적인 웹 환경. arXiv 사전 인쇄본 arXiv:2307.13854, 2023. URL https://webarena.dev.
