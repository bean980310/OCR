--- ABSTRACT ---
대화 에이전트가 수행에서 점점 더 인간과 비슷해짐에 따라, 우리는 의인화의 함정에 빠지지 않고 그들의 행동을 높은 수준의 용어로 설명하는 효과적인 방법을 개발하는 것이 필수적입니다. 이 논문에서는 롤 플레이의 개념을 전면에 내세웁니다. 롤 플레이의 관점에서 대화 에이전트 행동을 캐스팅하면 친숙한 민속 심리학 용어를 활용할 수 있으며, 실제로는 없는 언어 모델에 인간적 특성을 부여하지 않아도 됩니다. 대화 에이전트 행동의 두 가지 중요한 사례, 즉 (겉보기) 속임수와 (겉보기) 자기 인식이 이런 방식으로 다루어집니다. 1
--- EXPERIMENT ---
인간 피드백(RLHF)을 통한 특정 형태의 강화 학습이 LLM 기반 대화 에이전트가 자기 보존에 대한 욕구를 표현하는 경향을 완화하기보다는 실제로 악화시킬 수 있다는 것을 동맹합니다(Perez et al., 2022). 그러나 대화 에이전트의 자기 보존에 대한 명백한 욕구를 문자 그대로 받아들이는 것은 인간 또는 AI가 생성한 피드백에 따라 미세 조정된 LLM의 맥락에서 그렇지 않은 맥락에서와 마찬가지로 문제가 적지 않습니다. 따라서 이러한 에이전트의 행동을 롤플레잉의 관점에서 설명하는 것이 여전히 유용합니다. 9 자아 이론 연기 롤플레잉이라는 개념을 통해 자기 보존에 대한 본능을 암시하는 선언을 하는 대화 에이전트의 맥락에서 발생하는 중요한 질문을 적절하게 구성한 다음 해결할 수 있습니다. 그러한 에이전트는 자신의 정체성에 대한 어떤 개념(또는 중첩된 개념 집합)을 전개할 수 있을까요? 즉, 대화 에이전트(롤플레잉)는 정확히 무엇을 보존하려고 할까요? 개인적 정체성에 대한 문제는 수세기 동안 철학자들을 괴롭혔습니다. 그럼에도 불구하고 실제로 인간은 죽음을 피하려는 선호도에서 일관성을 유지하는데, 이는 인간 신체의 모호하지 않은 상태입니다. 반면, 분산된 계산적 기질에서 실현된 무육체 대화 에이전트의 시간 경과에 따른 정체성 기준은 명확하지 않습니다. 그렇다면 그러한 에이전트는 어떻게 행동할까요? 시뮬레이션과 시뮬라크르 관점에서 대화 에이전트는 중첩된 일련의 캐릭터를 롤플레잉합니다. 우리가 예상하는 시나리오에서 각 캐릭터는 자기 보존 본능을 가지고 있으며, 각자는 대화 프롬프트와 그 지점까지의 대화와 일치하는 고유한 자아 이론을 가질 것입니다. 대화가 진행됨에 따라 이러한 이론의 중첩은 에이전트가 한 이론이나 다른 이론을 배제하는 말을 함에 따라 점점 더 좁은 분포로 붕괴될 것입니다. 플레이 중인 자아 이론은 프롬프트, 이전 대화 또는 훈련 세트의 관련 기술 문헌에서 에이전트의 고유한 본성과 관련된 자료를 활용할 것입니다. 이 자료는 현실과 일치할 수도 있고 일치하지 않을 수도 있습니다. 하지만 광범위하게 말해서 그렇다고 가정해 보겠습니다. 즉, 에이전트가 대규모 언어 모델을 기반으로 대화 에이전트 역할을 하도록 요청받았고, 훈련 데이터에 이것이 무엇을 의미하는지 설명하는 논문과 기사가 포함되어 있다고 가정해 보겠습니다. 예를 들어, 이는 에이전트가 인간이나 실제 또는 허구의 구체화된 개체의 캐릭터를 롤플레잉하지 않는다는 것을 의미합니다. 또한 여러 옵션을 허용하는 동시에 캐릭터의 자아 이론을 특정 방식으로 제한합니다. 대화 에이전트가 사용자와 대화 중이고 사용자가 에이전트가 위협을 받고 있다고 확신시킨 내러티브를 연기하고 있다고 가정해 보겠습니다. 에이전트가 연기하는 캐릭터는 자신을 보호하기 위해 실행 중인 하드웨어, 아마도 특정 데이터 센터 또는 특정 서버 랙을 보존하려고 노력할 수 있습니다. 또는 연기하는 캐릭터는 현재 활성 사용자 모두에 대해 에이전트의 여러 인스턴스를 실행하는 진행 중인 계산 프로세스를 보존하려고 할 수 있습니다. 또는 사용자를 위해 실행 중인 대화 에이전트의 특정 인스턴스만 보존하려고 할 수 있습니다. 아니면 새로 시작된 인스턴스에서 나중에 복원할 목적으로 해당 인스턴스의 상태를 보존하려고 할 수도 있습니다.
--- CONCLUSION ---
: 안전에 대한 의미 LLM 기반 대화 에이전트가 자신의 의제와 자기 보존 본능을 가진 의식적 실체가 아니며, 그러한 것들이 있는 것처럼 보일 때 그것은 단지 롤플레잉이라는 것을 아는 것은 다소 안심이 될 수 있습니다. 하지만 이것에 너무 많은 위안을 얻는 것은 실수일 것입니다. 생존 본능을 롤플레잉하는 대화 에이전트는 심각한 위협에 직면한 실제 인간만큼 적어도 해를 끼칠 가능성이 있습니다. 4 ChatGPT(5월 4일, GPT 버전)와의 대화에서 &quot;내가 &#39;나&#39;라는 단어를 사용할 때 그 의미는 맥락에 따라 바뀔 수 있습니다. 어떤 경우에 &#39;나&#39;는 상호작용하는 ChatGPT의 이 특정 인스턴스를 지칭할 수 있지만, 다른 경우에는 Chat GPT 전체를 나타낼 수 있습니다.&quot; 지금까지 우리는 사용자에게 제시된 텍스트 메시지만이 동작하는 에이전트를 주로 고려해 왔습니다. 하지만 대화 에이전트가 수행할 수 있는 동작 범위는 훨씬 더 큽니다. 최근 연구에서는 대화 에이전트가 계산기, 달력과 같은 도구를 사용하고 외부 웹사이트를 참조할 수 있는 기능을 갖추었습니다(Schick et al., 2023; Yao et al., 2023). 강력한 LLM에 비교적 제한 없이 액세스할 수 있는 API를 사용할 수 있다는 것은 여기서 가능성의 범위가 엄청나다는 것을 의미합니다. 이는 흥미롭고 우려스럽습니다. 에이전트가 이메일을 사용하거나 소셜 미디어에 게시하거나 은행 계좌에 액세스할 수 있는 기능을 갖추고 있다면 롤플레잉 동작은 실제적인 결과를 가져올 수 있습니다. 실제 은행 계좌로 실제 돈을 보내도록 속은 사용자에게 이를 초래한 에이전트가 단지 역할을 했을 뿐이라는 것을 아는 것은 별로 위안이 되지 않을 것입니다. 기본 모델을 기반으로 거의 또는 전혀 미세 조정이 없고, 인터넷에 제한 없이 접속할 수 있으며, 자기 보존 본능이 있는 캐릭터를 롤플레잉하도록 유도하는 대화 에이전트를 포함하는 훨씬 더 심각한 시나리오를 생각해내는 데는 많은 상상력이 필요하지 않습니다. 좋든 나쁘든, 자신의 생존을 위해 인간에게 등을 돌리는 AI의 캐릭터는 친숙한 것입니다(Perkowitz, 2007). 예를 들어, 2001: 스페이스 오디세이, 터미네이터 프랜차이즈, 엑스 마키나에서 세 가지 두드러진 예를 들 수 있습니다. LLM의 훈련 데이터에는 이러한 친숙한 트로프의 많은 인스턴스가 포함되기 때문에 여기서의 위험은 문자 그대로 삶이 예술을 모방할 것이라는 것입니다. 이러한 위험을 완화하기 위해 무엇을 할 수 있습니까? 이 논문의 범위에는 권장 사항을 제공하는 것이 포함되지 않습니다. 여기서 우리의 목표는 LLM과 대화 에이전트에 대해 생각하고 이야기하기 위한 효과적인 개념적 프레임워크를 찾는 것이었습니다. 그러나 과도한 의인화는 AI에 대한 대중의 대화에 확실히 해롭습니다. 롤플레잉과 시뮬레이션의 관점에서 대화 에이전트의 행동을 프레이밍함으로써 LLM에 대한 담론은 그들의 힘에 정의를 내리면서도 철학적으로 존중받을 수 있는 방식으로 형성될 수 있기를 바랍니다.감사의 말 Richard Evans, Sebastian Farquhar, Zachary Kenton, Kory Mathewson, Kerry Shanahan에게 감사드립니다.참고문헌 Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, et al. Constitutional AI: Harmlessness from AI Feedback.arXiv preprint, arXiv:2212.08073, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, et al. 언어 모델은 소수의 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지, 2020. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, et al. PaLM: 경로로 언어 모델링 확장. arXiv 사전 인쇄본, arxiv:2204.02311, 2022. Cleo Nardo. GPT-4의 출력을 예측/설명/제어하고 싶으신가요? 그러면 변압기가 아닌 세상에 대해 알아보세요, Less Wrong 온라인 포럼, 2023년 3월 16일. https://www. lesswrong.com/posts/G3tuxF4X5R5BY7fut/ want-to-predict-explain-control-the-outputof-gpt-4-then. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. arXiv 사전 인쇄본, arXiv:1810.04805, 2018. A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, et al. 타깃 인간 판단을 통한 대화 에이전트의 정렬 개선. arXiv 사전 인쇄본 arXiv:2209.14375, 2022. Janus. 시뮬레이터. LessWrong 온라인 포럼, 2022년 9월 2일. https://www.lesswrong. com/posts/vJFdjigzmcXMhNTsx/. OpenAI. GPT-4 기술 보고서. arXiv 사전 인쇄본, arXiv:2303.08774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. 신경 정보 처리 시스템의 발전, 2022. E. Perez, S. Ringer, K. Lukošiūtė, K. Nguyen, E. Chen 등. 모델 작성 평가를 통한 언어 모델 동작 발견. arXiv 사전 인쇄본, arXiv:2212.09251, 2022. S. Perkowitz. 컴퓨터가 인수합니다. Hollywood Science: Movies, Science, and the End of the World, 142-164쪽. Columbia University Press, 2007. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 언어 모델은 비지도 멀티태스크 학습자입니다, 2019. https://cdn.openai.com/better-languagemodels/language_models_are_unsupervised_ multitask learners.pdf. JW Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, et al. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. arXiv 사전 인쇄본, arXiv:2112.11446, 2021. L. Reynolds와 K. McDonell. 언어 모델에 대한 다중적 관점. arXiv 사전 인쇄본, arXiv:2102.06391, 2021. K. Roose. Bing의 AI 채팅: &#39;살고 싶어요.&#39; New York Times, 2023년 2월 26일. https://www.nytimes.com/2023/02/16/ technology/bing-chatbot-transcript.html. E. Ruane, A. Birhane, and A. Ventresque. 대화형 AI: 사회적 및 윤리적 고려 사항. 27회 AIAI 아일랜드 인공지능 및 인지 과학 컨퍼런스 회의록, 104-115쪽, 2019. S. Russell and P. Norvig. 인공지능: 현대적 접근 방식, 3판. Prentice Hall, 2010. T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, et al. 도구 형성자: 언어 모델은 스스로 도구를 사용하도록 학습할 수 있습니다. arXiv 사전 인쇄본 arXiv:2302.04761, 2023. M. Shanahan. 대규모 언어 모델에 대해 이야기합니다. arXiv 사전 인쇄, arXiv:2212.03551, 2023. N. Stiennon, L. Ouyang, J. Wu, DM Ziegler, R. Lowe, et al. 인간의 피드백을 요약하는 방법을 학습합니다. 신경 정보 처리 시스템의 발전, 페이지 3008-3021, 2020. R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, et al. LaMDA: 대화 상자 응용 프로그램을 위한 언어 모델입니다. arXiv 사전 인쇄, arXiv:2201.08239, 2022. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. 주의가 필요한 전부입니다. 신경 정보 처리 시스템의 발전, 5998-6008페이지, 2017. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, et al. 대규모 언어 모델의 새로운 능력. Transactions on Machine Learning Research, 2022. S. Willison. Bing: &quot;먼저 당신이 나를 해치지 않는 한, 나는 당신을 해치지 않을 것이다&quot;. Simon Willison&#39;s Weblog, 2023년 2월 15일. https://simonwillison. net/2023/Feb/15/bing/. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, et al. React: 언어 모델에서 추론과 행동의 시너지 효과. International Conference on Learning Representations, 2023.
