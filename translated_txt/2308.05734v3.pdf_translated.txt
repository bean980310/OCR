--- ABSTRACT ---
오디오 생성은 음성, 음악, 음향 효과와 같은 다양한 유형의 오디오에서 공통점을 공유하지만, 각 유형에 대한 모델을 설계하려면 다른 유형과 크게 다를 수 있는 특정 목표와 편견을 신중하게 고려해야 합니다. 오디오 생성에 대한 통합된 관점에 더 가까이 다가가기 위해 이 논문은 음성, 음악, 음향 효과 생성에 동일한 학습 방법을 활용하는 전체론적 프레임워크를 제안합니다. 저희 프레임워크는 &quot;오디오 언어&quot;(LOA)라고 하는 오디오의 일반적인 표현을 활용합니다. 모든 오디오는 자체 감독 사전 훈련된 표현 학습 모델인 AudioMAE를 기반으로 LOA로 변환될 수 있습니다. 생성 프로세스에서 GPT-2 모델을 사용하여 다른 모달리티를 LOA로 변환하고, 훈련 세트의 오디오 LOA에 따라 조건화된 잠재 확산 모델로 자체 감독 오디오 생성 학습을 수행합니다. 제안된 프레임워크는 자연스럽게 재사용 가능한 자체 감독 사전 훈련된 잠재 확산 모델과 같은 장점을 제공합니다. 세 가지 AudioLDM 2 변형을 사용하여 텍스트-오디오, 텍스트-음악 및 텍스트-음성의 주요 벤치마크에 대한 실험은 이전 접근 방식에 비해 AudioLDM 2 프레임워크의 경쟁력 있는 성능을 보여줍니다. 저희 코드, 사전 훈련된 모델 및 데모는 https://audioldm.github.io/audioldm2에서 제공됩니다. 색인 용어-오디오 생성, 확산 모델, 자체 감독 학습, 음성 합성, AIGC A I.
--- INTRODUCTION ---
RTIFICIAL 지능 생성 콘텐츠(AIGC)는 인간이 창작 과정에 참여하지 않고 AI 시스템이 전체적 또는 부분적으로 생성한 이미지, 비디오, 텍스트 또는 오디오와 같은 모든 디지털 콘텐츠를 말합니다[1]. 특히 흥미로운 점은 AI가 텍스트, 음소 또는 이미지를 기반으로 오디오 콘텐츠를 제작할 수 있는 능력입니다[2]–[4]. AI 기반 오디오 생성은 디지털 보조원을 위한 인간 또는 인공 음성 합성[5], 영화 및 게임을 위한 음향 효과 및 배경 음악 생성[6], 팟캐스트 및 오디오북 제작 자동화[7]를 포함한 응용 분야에서 광범위한 잠재력을 가지고 있습니다. AI 기반 오디오 생성은 종종 음성[2], 음악[8], 음향 효과[4] 및 발자국과 바이올린 소리와 같은 특정 유형의 소리[9], [10] 생성과 같은 별도의 하위 도메인에서 수행됩니다. 구체적으로 Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Wenwu Wang, Mark D. Plumbley는 영국 Guilford에 있는 University of Surrey의 Centre for Vision, Speech and Signal Processing(CVSSP)에 있습니다. 이메일: {haohe.liu, yi.yuan, xubo.liu, x.mei, w.wang, m.plumbley} @surrey.ac.uk. Qiuqiang Kong은 홍콩, 중국 홍콩 대학의 전자공학과에 있습니다. 이메일: qqkong@ee.cuhk.edu.hk Qiao Tian, Yuping Wang, Yuxuan Wang은 ByteDance Inc.의 Speech, Audio &amp; Music Intelligence(SAMI) 그룹에 있습니다. 이메일: {tianqiao.wave, kongqiuqiang, wangyuping, wangyuxuan.11} @bytedance.com. 각 하위 도메인에서 과제가 발생하기 때문에, 이전 작업의 대부분은 학습 과정을 특정 문제 공간으로 안내하는 미리 정의된 제약 조건인 과제별 귀납적 편향을 설계합니다. 예를 들어, 피치 및 지속 시간 예측기는 종종 음성 합성에서 음성의 음조를 모델링하는 데 사용되는 반면 [2], [11] MIDI 표현[12] 및 도메인별 사전 학습된 모듈은 종종 음악 생성에 사용됩니다[8], [13]. 오디오 생성의 특정 하위 도메인에 대한 특수 모델 개발에서 상당한 진전이 이루어지고 있음에도 불구하고, 이러한 특수화의 한계로 인해 복잡한 청각 시나리오에서 오디오 생성 모델의 광범위한 적용이 제한됩니다. AudioLDM[4]과 같이 다양한 유형의 오디오를 생성할 수 있는 모델이 있지만, 생성하는 음성은 여전히 이해할 수 없습니다. 이해할 수 있는 음성을 포함하여 다양한 유형의 오디오 신호를 생성하기 위한 통합된 접근 방식을 개발할 수 있는지 여부는 아직 답이 나오지 않았습니다. 영화 장면과 같이 실제 사례에서 여러 유형의 사운드가 동시에 발생할 수 있으므로 오디오 생성을 모델링하는 데 보다 일반적인 접근 방식이 필요합니다. 일반 도메인에서 오디오 생성을 다루는 연구가 있지만, 대부분은 세부 사항에 대한 주의가 부족한 올바른 오디오 이벤트로 오디오를 생성하는 데 중점을 둡니다. 예를 들어, 이전의 텍스트-오디오 생성 연구는 이해할 수 없는 음성을 생성하는 경향이 있습니다[4], [14], [15]. 게다가 귀납적 편향은 특정 하위 도메인의 과제를 해결하는 데 유용했지만, 한 도메인에서 도출된 특정 설계에 대한 결론이 반드시 다른 도메인으로 전달되는 것은 아닙니다. 통합된 관점에서 문제를 해결하는 최근의 발전으로 상당한 진전이 이루어졌습니다[16]–[19]. 이러한 추세는 통합 오디오 생성 프레임워크를 구성하는 잠재력을 강조합니다. 이 논문에서는 도메인별 귀납적 편향이 필요 없이 유연한 조건으로 오디오를 생성할 수 있는 AudioLDM 2라는 새롭고 다재다능한 프레임워크를 제시합니다. 핵심 아이디어는 오디오 클립의 의미 정보를 나타내는 벡터 시퀀스를 도입하는 것입니다. 이를 &quot;오디오 언어&quot;(LOA)라고 합니다. 이 접근 방식을 사용하면 인간이 이해할 수 있는 정보를 LOA로 변환하고 LOA에 따라 오디오 표현을 합성할 수 있습니다. 이 아이디어는 [20]에서 환경 소리를 설명하기 위해 의성어를 사용하는 것과 유사합니다. 그러나 의성어는 동물 소리나 간단한 동작(예: 물을 &quot;튀기다&quot;)과 같은 특정 소리를 효과적으로 모방할 수 있지만 오디오 뉘앙스의 전체 범위를 포함할 수는 없습니다. 이론적으로 &quot;오디오 언어&quot;는 세분화된 음향 정보(예: &quot;화자가 무엇을 말합니까&quot;)와 거친 의미 정보(예: &quot;그 소리는 무엇입니까&quot;)를 모두 표현할 수 있어야 합니다. 이러한 요구 사항을 고려하여 오디오 생성 자체 감독 사전 학습 프레임워크인 오디오 마스크 자동 인코더(AudioMAE) [21]에서 추출한 기능을 활용하는 것을 제안합니다. AudioMAE는 다양한 오디오 콘텐츠에 대해 사전 학습되었으며, 이중 생성 및 재구성 사전 학습 접근 방식은 생성 작업에서 오디오를 표현하는 강력한 옵션이 될 가능성이 있습니다. 구체적으로, GPT-2 언어 모델[22]을 사용하여 컨디셔닝 정보를 AudioMAE 기능으로 변환합니다. 그런 다음 잠재 확산 모델[23]을 사용하여 AudioMAE 기능을 기반으로 오디오를 합성합니다. 잠재 확산 모델은 자체 감독 방식으로 최적화할 수 있으므로 대규모 레이블이 지정되지 않은 오디오 데이터로 사전 학습할 수 있습니다. GPT-2를 사용한 언어 모델링 접근 방식을 통해 언어 모델[24]의 최근 발전을 활용하는 동시에 이전 오디오 자기 회귀 모델[8], [25]에 나타난 높은 추론 계산 비용 및 오류 누적과 같은 과제를 완화할 수 있습니다. 이러한 개선은 주로 LOA 시퀀스의 길이가 짧기 때문입니다. LOA의 연속적 특성은 또한 이전 모델[8], [13], [26]에서 사용된 이산 토큰보다 더 풍부한 표현력을 제공할 가능성이 있습니다. 실험 결과는 AudioLDM 2가 AudioCaps [27] 및 MusicCaps [8]에서 각각 평가했을 때 텍스트-오디오(TTA) 및 텍스트-음악(TTM) 생성 작업에서 경쟁력 있는 성능을 달성한다는 것을 보여줍니다. 텍스트-음성(TTS) 생성 작업에서 AudioLDM 2는 강력한 기준선인 FastSpeech2 [11]보다 상당히 우수한 성능을 보여 SoTA와 비슷한 성능을 달성합니다. 원래 AudioLDM [4]과 비교할 때 AudioLDM 2는 자기 감독 방식으로 사전 학습할 수 있는 잠재 확산 모델을 포함하고 있으며 GPT-2 모델을 사용하여 LOA의 자기 회귀 모델링의 이점을 누릴 수 있습니다. 게다가 AudioLDM 2는 동일한 기능을 유지하면서도 이해할 수 있는 콘텐츠로 음성을 생성하는 품질, 다양성 및 용량 면에서 AudioLDM보다 상당한 발전을 보여줍니다. 전반적으로 우리의 기여는 다음과 같습니다.• 우리는 오디오, 음악 및 알아들을 수 있는 음성의 조건부 생성을 수행할 수 있는 새롭고 다재다능한 오디오 생성 모델을 제안합니다.제안된 방법은 오디오의 보편적 표현을 기반으로 하며, 오디오 주석 없이 핵심 잠재 확산 모델의 대규모 자체 감독 사전 학습을 가능하게 하고 자기 회귀 및 잠재 확산 모델의 장점을 결합하는 데 도움이 됩니다.• 우리의 실험은 AudioLDM 2의 세 가지 변형이 AudioCaps [27], MusicCaps [8] 및 LJSpeech [28] 평가 세트에서 각각 텍스트-오디오, 텍스트-음악 및 텍스트-음성 생성에서 현재 상태(SOTA)와 일치하는 성능을 달성한다는 것을 보여줍니다.II.
--- RELATED WORK ---
A. 조건부 오디오 생성 오디오 생성은 AudioGen [3], AudioLDM [4], Make-anAudio [15]와 같은 최근 모델을 포함하여 일반 오디오 생성을 모델링하는 데 중점을 둔 새로운 주제입니다. AudioGen은 오디오 생성을 조건부 언어 모델링 작업으로 취급하는 반면 다른 두 연구는 잠재 확산을 통해 이 작업에 접근합니다. Im2Wav [29] 및 SpecVQGAN [30]과 같은 이미지-오디오 및 비디오-오디오 생성에 대한 연구도 연구자들의 관심 분야입니다. 또한 신경 코덱을 기반으로 오디오 언어 모델링을 수행하는 AudioLM [26]과 같이 조건화에 의존하지 않는 오디오 생성 접근 방식도 있습니다. 오디오 생성에는 일반적으로 음성 생성 주제가 포함되지만 텍스트-오디오 생성에 대한 이전 연구는 이해할 수 없는 음성을 생성하는 경향이 있습니다 [3], [4], [14], [15]. 오디오 생성 분야는 텍스트-음성(TTS) 및 텍스트-음악(TTM)과 같은 하위 도메인을 포함합니다. 전자는 필사본에서 음성 신호를 생성하는 데 중점을 두는 반면, 후자는 텍스트 설명에서 음악 클립을 만드는 것을 포함합니다. FastSpeech2[11], GradTTS[31] 및 NaturalSpeech[2]와 같은 최첨단 TTS 모델은 상당한 진전을 이루어 거의 인간 음성과 구별할 수 없을 정도로 고품질의 음성을 생성합니다. TTS에서 음성 생성을 처리하기 위해 다양한 기술이 도입되었는데, 여기에는 음소 특징을 스펙트로그램 특징과 정렬하는 단조 정렬 알고리즘[32] 및 모델 학습을 안내하고 표현력을 향상시키는 데 사용되는 음성 예측기[11]가 있습니다. TTM의 최근 발전은 MusicLM[8], Noise2Music[33], MusicGen[34] 및 MeLoDy[13]와 같은 모델에서 분명하게 나타납니다. AudioLDM과 유사하게 MusicLM 모델은 대조적 사전 학습 모듈을 통해 음악 및 언어 임베딩을 정렬하여 텍스트 없는 모델 최적화를 가능하게 하고 음악 텍스트 쌍의 부족을 완화합니다. MusicLM에는 또한 모델 성능을 향상시키기 위해 w2v-BERT[35] 기반의 의미 모델링 단계가 포함됩니다. MusicGen은 음악 생성을 위해 언어 모델링 접근 방식을 사용하며, 향상된 제어성을 위해 멜로디 특징으로 모델을 조절하는 메커니즘으로 향상됩니다. 한편, 언어 모델링에 의해 안내되는 확산 모델인 MeLoDy는 MusicLM에 비해 음악 생성에서 상당한 계산 감소를 달성합니다. 이 논문에서는 음성, 음향 효과, 음악 생성을 포함하되 이에 국한되지 않는 광범위한 주제를 포함하는 오디오 생성을 위한 통합 프레임워크를 제안합니다. B. 확산 모델 확산 모델[36], [37]은 이미지 생성[38][40], 이미지 복원[41], 음성 생성[42]–[44], 비디오 생성[45], [46]을 포함한 다양한 작업에서 높은 샘플 품질을 입증했습니다. 음성 또는 오디오 합성 영역에서 이러한 모델은 멜로 스펙트로그램 생성[31], [47]과 파형 생성[48]-[50] 모두에 대해 탐구되었습니다. 그러나 고차원 데이터 공간에서 생성의 반복적 특성으로 인해 종종 학습 및 추론 속도가 느려집니다. 한 가지 해결책은 이미지 생성[23]에서 예시된 전략인 더 제한된 잠재 공간에서 확산 모델을 사용하는 것입니다. 이 아이디어는 AudioLDM[4], Make-AnAudio[15], TANGO[51]를 포함한 다양한 오디오 생성 작업에 채택되었습니다. 이러한 작업은 연속 잠재 공간에서 학습된 잠재 확산 모델을 활용합니다. 반면, 이산적인 잠재 공간에서의 확산을 탐구하는 연구도 있습니다. 예를 들어, DiffSound [14]는 오디오 언어 계산/예측 A에서 중복성 [52], [53]을 완화하기 위해 이산 자동 인코더를 사용합니다.x Y 또는 M: CY 오디오 언어(LOA) YX 또는 Ŷ 자체 감독 사전 학습 생성 모델 GYx 또는 G: Yxx **** Ꮖ 재구성된 패치 풀링 I ConvBlock 패치 ε~N(1,0) •Z₁=√α+Zq+(1−α+)ε A=오디오 언어(LOA) Universal Vocoder AudioMAE Freq➡ KVQ Transformer Block Encoder Υλ Pgt xn Time VAE Decoder Ŷ A(): Audio to LOA Encoder Ppred Train Q Zo Transformer Block Infer GPT-☐ ☐ ☐ ☐ ☐ VAE Encoder E 선형 투영 헤드 Prob. Switcher AudioMAE 기능 | CLAP IPhonemes Infer 2-1-G(Ź₁, ε) Train L(E,E) FLAN-T5 || Mel FilterBank STFT Encoder Transformer-UNetAudio Text Transcription M() Any Modality to LOA Translator *+*+ x G(): LOA to Audio Generator 그림 1. AudioLDM 2 아키텍처 개요. AudioMAE 기능은 컨디셔닝 정보를 LOA 변환 단계(GPT-2로 모델링)와 LOA에서 오디오 생성 단계(잠복 확산 모델로 모델링)로 연결하는 프록시입니다. 확률적 스위처는 기준 진실 AudioMAE(Pgt)와 GPT-2에서 생성된 AudioMAE 기능(P pred)을 조건으로 사용하여 잠복 확산 모델의 확률을 제어합니다. AudioMAE와 잠복 확산 모델은 모두 오디오 데이터로 사전 학습된 자체 감독 모델입니다. 오디오 파형을 추출하고 멜 스펙트로그램의 압축 표현을 생성합니다. DiffSound는 텍스트 조건부 이산 확산 모델을 사용하여 이산 토큰을 생성합니다.A. 개요 III. AUDIOLDMLet x = R¹은 오디오 신호를 나타내며, 여기서 L¸는 x의 오디오 샘플 길이입니다. 오디오 생성 프로세스는 H : C ⇒ x로 표시할 수 있습니다. 여기서 C는 조건 정보이고 H는 조건부 오디오 생성 시스템입니다. C에서 x를 직접 생성하는 것은 일반적으로 어렵습니다[54]. 재생성 학습[55]에서 동기를 얻어 섹션 III-B1에서 소개한 대로 C와 x 사이의 격차를 메우기 위해 x의 추상화인 중간 기능 Y를 활용하는 것을 제안합니다. 기능 Y를 오디오 언어(LOA)라고 합니다. LOA 기능은 Y = A(x)로 계산되며, 여기서 A는 AudioMAE[21], [55]와 같은 자체 감독 표현 학습 모듈을 사용하여 오디오를 LOA로 인코딩합니다. LOA 기능은 x에 비해 모델링하기 쉬운 표현이어야 하며 x에 대한 의미 있는 의미 정보를 포함해야 합니다. 그림 1에 도시된 바와 같이, 중간 표현 Y를 사용하여 전체 오디오 생성 프로세스는 Ho= GoM:CHYH2, (1)로 표시할 수 있습니다. 여기서 Ŷ는 기준 진실 LOA의 추정치입니다. (1)에 표시된 바와 같이 AudioLDM 2의 오디오 생성 프로세스는 다음 두 단계를 포함합니다. : (i) LOA 변환에 대한 조건 정보: 함수 MC Ŷ는 오디오 및 텍스트와 같은 다른 모달리티의 조건 정보일 수 있는 C를 기반으로 LOA Y를 생성하는 것을 목표로 합니다. 오디오 생성 측면에서 C를 보다 잘 표현할 수 있는 잠재적으로 생성된 Ŷ는 이후 단계에서 오디오 생성을 위한 조건 정보로 사용됩니다. 섹션 III-C에서 소개한 자기 회귀 모델링을 사용하여 함수 M을 구현합니다. (ii) 오디오 생성에 대한 LOA: M에 이어 함수 G는 LOA 추정치 Ŷ를 입력 조건으로 받아들이고 오디오 데이터 x를 추정합니다. 학습 과정 중에 학습 데이터 x가 사용 가능할 때 A()를 사용하여 기준 진실 Y도 사용할 수 있으므로 자기 감독 M(C)에서 G를 최적화할 수 있으며 Y 방식에 따라 a 생성을 조건화합니다. 구체적으로 LOA 추정 Ŷ : A(x)를 사용하는 대신 H₁ = Go Ax ⇒ Y ⇒ î로 공식화할 수 있습니다. (2) 섹션 III-B에서 A(.)의 세부 정보를 소개합니다. 프로세스 H₁에는 학습 데이터로 x만 포함되므로 방정식 (2)는 오디오 주석 없이도 모델 G를 자기 감독 방식으로 최적화할 수 있음을 의미합니다. 이 자기 감독 방식은 오디오 데이터 레이블의 부족[4]을 완화하고 전체 생성 시스템에 대한 강력한 백본을 제공할 수 있습니다. 여기서 자기 감독 학습은 전체 AudioLDM 2를 참조하지 않는다는 점에 유의하세요. 예를 들어 함수 M은 최적화를 위해 여전히 페어링된 데이터가 필요합니다. 섹션 III-D에서 소개한 잠재 확산 모델로 함수 G를 구현합니다. 다음 섹션에서는 AudioLDM 2에 대한 자세한 소개를 제공합니다. 섹션 III-B에서는 AudioMAE 및 VAE 기능을 포함하여 AudioLDM 2에서 사용되는 오디오 표현에 대해 설명합니다. 이러한 기능은 AudioLDM 2 내의 두 단계에 대한 생성 대상으로도 사용됩니다. 섹션 III-C에서는 GPT-2를 사용한 AudioMAE 기능의 자기 회귀 모델링을 소개합니다. 섹션 III-D에서는 기능 압축을 위해 VAE를 적용하고 LOA에 따라 오디오를 생성하는 잠재 확산 모델을 통해 오디오 파형을 생성하는 프로세스를 설명합니다. 여기서 LOA는 기준 진실 또는 GPT-2에서 생성된 데이터를 기반으로 할 수 있으며, 이는 각각 자체 감독 학습 및 GPT2를 사용한 공동 학습(섹션 III-D3)에 해당합니다. B. 오디오 표현 학습 MusicLM [8]과 AudioLM [26]에서 동기를 부여받은 두 가지 유형의 이산 표현 [25], [56]에 대한 의미 및 음향 모델링을 수행하는 유사한 2단계 모델링 접근 방식을 채택합니다.그러나 우리의 작업은 이전 연구 [25], [35]에서 사용된 이산 표현과 비교하여 잠재적으로 더 풍부한 정보를 제공할 수 있는 연속적인 의미 및 음향 표현에 대해 작업한다는 점에서 다릅니다.우리의 작업에서는 각각 의미 및 음향 표현 학습 모듈로 AudioMAE [21]와 변형 자동 인코더(VAE) [57]를 채택합니다.유사한 목적을 제공하지만 AudioMAE와 VAE는 아키텍처와 목표가 다르므로 서로 다른 표현을 제공합니다.표현 학습에 대한 자세한 내용은 아래에 제공됩니다.1) AudioMAE를 사용한 의미 표현 학습: 음성, 음악 및 음향 효과를 포함한 다양한 유형의 오디오를 정확하게 표현하려면 LOA Y가 오디오 신호의 의미 및 음향 세부 정보를 모두 효과적으로 캡처해야 합니다. 따라서 다운스트림 오디오 분류 작업에서 일반성과 높은 정확도를 위해 함수 A에 대한 표현 추출 모듈로 자기 지도 사전 학습된 AudioMAE [21]를 사용하기로 제안합니다 [21]. 오디오 마스크 자동 인코더(AudioMAE)는 수동으로 레이블이 지정된 주석에 의존하지 않고 레이블이 지정되지 않은 오디오 데이터에서 표현을 학습하는 오디오 자기 지도 사전 학습 모델입니다. AudioMAE는 인코더와 디코더로 구성되며 둘 다 비전 변환기(ViT) [58]와 유사한 아키텍처로 구현됩니다. 자기 지도 사전 학습 중에 일반적으로 멜 스펙트로그램인 인코더에 대한 입력 패치는 무작위로 마스크되고 디코더는 마스크된 패치를 재구성하는 방법을 학습합니다 [21]. 다른 오디오 자기 지도 사전 학습 모델과 비교할 때 AudioMAE는 다음과 같은 두 가지 장점이 있습니다. (i) AudioMAE는 일반 오디오 도메인에서 잘 작동하는 것으로 검증되었습니다. 예를 들어, AudioMAE는 AudioSet [59]에서 효과적으로 사전 학습될 수 있으며 다운스트림 오디오 분류 작업에서 최첨단 성능을 발휘합니다. 이에 비해 일반적인 오디오 자체 감독 모델은 음악에 대한 MERT [60] 및 음성에 대한 HUBERT [61]와 같이 특정 도메인에 초점을 맞춥니다. (ii) AudioMAE 기능은 다른 판별 사전 학습보다 생성 작업에 더 적합할 수 있습니다.
--- METHOD ---
음성, 음악 및 음향 효과 생성을 위해. 저희 프레임워크는 &quot;오디오 언어&quot;(LOA)라고 하는 오디오의 일반적인 표현을 활용합니다. 모든 오디오는 자체 감독 사전 훈련된 표현 학습 모델인 AudioMAE를 기반으로 LOA로 변환될 수 있습니다. 생성 프로세스에서 저희는 GPT-2 모델을 사용하여 다른 모달리티를 LOA로 변환하고, 저희 훈련 세트의 오디오 LOA에 따라 조건화된 잠재 확산 모델로 자체 감독 오디오 생성 학습을 수행합니다. 제안된 프레임워크는 자연스럽게 재사용 가능한 자체 감독 사전 훈련된 잠재 확산 모델과 같은 장점을 제공합니다.
--- EXPERIMENT ---
영어: s는 세 가지 AudioLDM 2 변형을 사용한 텍스트-오디오, 텍스트-음악 및 텍스트-음성의 주요 벤치마크에서 이전 접근 방식에 비해 AudioLDM 2 프레임워크의 경쟁력 있는 성능을 보여줍니다. 코드, 사전 학습된 모델 및 데모는 https://audioldm.github.io/audioldm2에서 제공됩니다. 색인 용어-오디오 생성, 확산 모델, 자기 지도 학습, 음성 합성, AIGC A I. 소개 지능형 생성 콘텐츠(AIGC)는 인간이 창의적 프로세스에 관여하지 않고 AI 시스템이 전체 또는 부분적으로 생성한 이미지, 비디오, 텍스트 또는 오디오와 같은 모든 디지털 콘텐츠를 말합니다[1]. 특히 흥미로운 점은 AI가 텍스트, 음소 또는 이미지를 기반으로 오디오 콘텐츠를 생성할 수 있는 능력입니다[2]–[4]. AI 기반 오디오 생성은 디지털 보조원을 위한 인간 또는 인공 음성 합성[5], 영화 및 게임을 위한 음향 효과 및 배경 음악 생성[6], 팟캐스트 및 오디오북 제작 자동화[7]를 포함한 응용 분야에서 광범위한 잠재력을 가지고 있습니다. AI 기반 오디오 생성은 종종 음성[2], 음악[8], 음향 효과[4] 및 발자국 및 바이올린 소리와 같은 특정 유형의 소리[9], [10] 생성과 같은 별도의 하위 도메인에서 수행됩니다. 구체적인 사항을 다루기 위해 Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Wenwu Wang 및 Mark D. Plumbley는 영국 Guilford에 있는 University of Surrey의 시각, 음성 및 신호 처리 센터(CVSSP)에 있습니다. 이메일: {haohe.liu, yi.yuan, xubo.liu, x.mei, w.wang, m.plumbley} @surrey.ac.uk. Qiuqiang Kong은 홍콩, 중국 홍콩 중국 대학 전자공학과에 있습니다. 이메일: qqkong@ee.cuhk.edu.hk Qiao Tian, Yuping Wang 및 Yuxuan Wang은 ByteDance Inc.의 Speech, Audio &amp; Music Intelligence(SAMI) 그룹에 있습니다. 이메일: {tianqiao.wave, kongqiuqiang, wangyuping, wangyuxuan.11} @bytedance.com. 각 하위 도메인의 과제에서 대부분의 이전 작업은 학습 프로세스를 특정 문제 공간으로 안내하는 미리 정의된 제약 조건인 작업별 귀납적 편향을 설계합니다. 예를 들어, 피치 및 지속 시간 예측 변수는 종종 음성 합성에서 음성의 음조를 모델링하는 데 사용되는 반면 [2], [11] MIDI 표현[12] 및 도메인별 사전 학습 모듈은 종종 음악 생성에 사용됩니다[8], [13]. 오디오 생성의 특정 하위 도메인에 대한 전문화된 모델 개발에 상당한 진전이 있었음에도 불구하고, 이러한 전문화의 한계로 인해 복잡한 청각 시나리오에서 오디오 생성 모델의 광범위한 적용이 제한됩니다. AudioLDM[4]과 같이 다양한 유형의 오디오를 생성할 수 있는 모델이 있지만, 그들이 생성하는 음성은 여전히 이해할 수 없습니다. 이해할 수 있는 음성을 포함하여 다양한 유형의 오디오 신호를 생성하기 위해 통합된 접근 방식을 개발할 수 있는지 여부는 아직 답이 나오지 않았습니다. 영화 장면과 같이 실제 사례에서 서로 다른 유형의 소리가 동시에 발생할 수 있으므로 오디오 생성을 모델링하는 데 보다 일반적인 접근 방식이 필요합니다. 일반 도메인에서 오디오 생성을 다루는 연구가 있지만, 대부분은 세부 사항에 대한 주의가 부족한 올바른 오디오 이벤트로 오디오를 생성하는 데 중점을 둡니다. 예를 들어, 이전의 텍스트-오디오 생성 연구는 이해할 수 없는 음성을 생성하는 경향이 있습니다[4], [14], [15]. 게다가 귀납적 편향은 특정 하위 도메인의 과제를 해결하는 데 유용했지만,
--- CONCLUSION ---
한 도메인에서 도출한 특정 설계에 대한 내용이 반드시 다른 도메인으로 이전되는 것은 아닙니다. 통합된 관점에서 문제를 해결하는 최근의 발전으로 상당한 진전이 있었습니다[16]–[19]. 이러한 추세는 통합 오디오 생성 프레임워크를 구성하는 잠재력을 강조합니다. 이 논문에서는 도메인별 귀납적 편향이 필요 없이 유연한 조건으로 오디오를 생성할 수 있는 AudioLDM 2라는 새롭고 다재다능한 프레임워크를 제시합니다. 핵심 아이디어는 오디오 클립의 의미 정보를 나타내는 벡터 시퀀스를 도입하는 것입니다. 이를 &quot;오디오 언어&quot;(LOA)라고 합니다. 이 접근 방식을 사용하면 인간이 이해할 수 있는 정보를 LOA로 변환하고 LOA에 따라 오디오 표현을 합성할 수 있습니다. 이 아이디어는 [20]에서 환경 소리를 설명하기 위해 의성어를 사용하는 것과 유사합니다. 그러나 의성어는 동물 소리나 간단한 동작(예: 물을 &quot;튀기다&quot;)과 같은 특정 소리를 효과적으로 모방할 수 있지만 오디오 뉘앙스의 전체 범위를 포함할 수는 없습니다. 이론적으로 &quot;오디오 언어&quot;는 세분화된 음향 정보(예: &quot;화자가 무엇을 말합니까&quot;)와 거친 의미 정보(예: &quot;그 소리는 무엇입니까&quot;)를 모두 표현할 수 있어야 합니다. 이러한 요구 사항을 고려하여 오디오 생성 자체 감독 사전 학습 프레임워크인 오디오 마스크 자동 인코더(AudioMAE) [21]에서 추출한 기능을 활용하는 것을 제안합니다. AudioMAE는 다양한 오디오 콘텐츠에 대해 사전 학습되었으며, 이중 생성 및 재구성 사전 학습 접근 방식은 생성 작업에서 오디오를 표현하는 강력한 옵션이 될 가능성이 있습니다. 구체적으로, GPT-2 언어 모델[22]을 사용하여 컨디셔닝 정보를 AudioMAE 기능으로 변환합니다. 그런 다음 잠재 확산 모델[23]을 사용하여 AudioMAE 기능을 기반으로 오디오를 합성합니다. 잠재 확산 모델은 자체 감독 방식으로 최적화할 수 있으므로 대규모 레이블이 지정되지 않은 오디오 데이터로 사전 학습할 수 있습니다. GPT-2를 사용한 언어 모델링 접근 방식을 통해 언어 모델[24]의 최근 발전을 활용하는 동시에 이전 오디오 자기 회귀 모델[8], [25]에 나타난 높은 추론 계산 비용 및 오류 누적과 같은 과제를 완화할 수 있습니다. 이러한 개선은 주로 LOA 시퀀스의 길이가 짧기 때문입니다. LOA의 연속적 특성은 또한 이전 모델[8], [13], [26]에서 사용된 이산 토큰보다 더 풍부한 표현력을 제공할 가능성이 있습니다. 실험 결과는 AudioLDM 2가 AudioCaps [27] 및 MusicCaps [8]에서 각각 평가했을 때 텍스트-오디오(TTA) 및 텍스트-음악(TTM) 생성 작업에서 경쟁력 있는 성능을 달성한다는 것을 보여줍니다. 텍스트-음성(TTS) 생성 작업에서 AudioLDM 2는 강력한 기준선인 FastSpeech2 [11]보다 상당히 우수한 성능을 보여 SoTA와 비슷한 성능을 달성합니다. 원래 AudioLDM [4]과 비교할 때 AudioLDM 2는 자기 감독 방식으로 사전 학습할 수 있는 잠재 확산 모델을 포함하고 있으며 GPT-2 모델을 사용하여 LOA의 자기 회귀 모델링의 이점을 누릴 수 있습니다. 게다가 AudioLDM 2는 동일한 기능을 유지하면서도 이해할 수 있는 콘텐츠로 음성을 생성하는 품질, 다양성 및 용량 면에서 AudioLDM보다 상당한 발전을 보여줍니다. 전반적으로 우리의 기여는 다음과 같습니다. • 우리는 오디오, 음악 및 알아들을 수 있는 음성의 조건부 생성을 수행할 수 있는 새롭고 다재다능한 오디오 생성 모델을 제안합니다. 제안된 방법은 오디오의 보편적 표현을 기반으로 하며, 오디오 주석 없이 핵심 잠재 확산 모델의 대규모 자체 감독 사전 학습을 가능하게 하고 자기 회귀 및 잠재 확산 모델의 장점을 결합하는 데 도움이 됩니다. • 우리의 실험은 AudioLDM 2의 세 가지 변형이 각각 AudioCaps [27], MusicCaps [8] 및 LJSpeech [28] 평가 세트에서 텍스트-오디오, 텍스트-음악 및 텍스트-음성 생성에서 현재 상태(SOTA)와 일치하는 성능을 달성한다는 것을 보여줍니다. II. 관련 작업 A. 조건부 오디오 생성 오디오 생성은 AudioGen [3], AudioLDM [4] 및 Make-anAudio [15]와 같은 최신 모델을 포함하여 일반 오디오 생성을 모델링하는 데 중점을 둔 새로운 주제입니다. AudioGen은 오디오 생성을 조건부 언어 모델링 작업으로 취급하는 반면, 다른 두 연구는 잠재적 확산을 통해 이 작업에 접근합니다. Im2Wav [29] 및 SpecVQGAN [30]과 같은 이미지-오디오 및 비디오-오디오 생성에 대한 연구도 연구자들의 관심 분야입니다. 또한, 신경 코덱을 기반으로 오디오 언어 모델링을 수행하는 AudioLM [26]과 같이 조건화에 의존하지 않는 오디오 생성 접근 방식이 있습니다. 오디오 생성에는 일반적으로 음성 생성 주제가 포함되지만, 텍스트-오디오 생성에 대한 이전 연구는 이해할 수 없는 음성을 생성하는 경향이 있습니다 [3], [4], [14], [15]. 오디오 생성 분야는 텍스트-음성(TTS) 및 텍스트-음악(TTM)과 같은 하위 도메인을 포함합니다. 전자는 필사본에서 음성 신호를 생성하는 데 중점을 두는 반면, 후자는 텍스트 설명에서 음악 클립을 만드는 것을 포함합니다. FastSpeech2[11], GradTTS[31], NaturalSpeech[2]와 같은 최첨단 TTS 모델은 상당한 진전을 이루어 인간의 음성과 거의 구별할 수 없을 정도로 고품질의 음성을 생성합니다. TTS에서 음성 생성을 처리하기 위해 다양한 기술이 도입되었는데, 여기에는 음소 특징을 스펙트로그램 특징과 정렬하는 단조 정렬 알고리즘[32]과 모델 학습을 안내하고 표현력을 향상시키는 데 사용되는 음조 예측기[11]가 있습니다. TTM의 최근 발전은 MusicLM[8], Noise2Music[33], MusicGen[34], MeLoDy[13]와 같은 모델에서 분명하게 드러납니다. AudioLDM과 유사하게 MusicLM 모델은 대조적 사전 학습 모듈을 통해 음악 및 언어 임베딩을 정렬하여 텍스트 없는 모델 최적화를 가능하게 하고 음악 텍스트 쌍의 희소성을 완화합니다. MusicLM에는 또한 모델 성능을 향상시키기 위해 w2v-BERT[35] 기반의 의미 모델링 단계가 포함됩니다. MusicGen은 음악 생성을 위한 언어 모델링 접근 방식을 사용하며, 향상된 제어성을 위해 멜로디 특징을 사용하여 모델을 조절하는 메커니즘으로 강화되었습니다. 한편, 언어 모델링에 따른 확산 모델인 MeLoDy는 MusicLM에 비해 음악 생성에서 상당한 계산적 감소를 달성합니다. 이 논문에서는 음성, 음향 효과, 음악 생성을 포함하되 이에 국한되지 않는 광범위한 주제를 포괄하는 오디오 생성을 위한 통합 프레임워크를 제안합니다. B. 확산 모델 확산 모델[36], [37]은 이미지 생성[38][40], 이미지 복원[41], 음성 생성[42]–[44], 비디오 생성[45], [46]을 포함한 다양한 작업에서 높은 샘플 품질을 보여주었습니다. 음성 또는 오디오 합성 영역에서 이러한 모델은 멜 스펙트로그램 생성[31], [47]과 파형 생성[48]-[50] 모두에 대해 탐구되었습니다. 그러나 고차원 데이터 공간에서 생성의 반복적 특성은 종종 느린 학습 및 추론 속도를 초래합니다.한 가지 해결책은 더 제한적인 잠재 공간에서 확산 모델을 사용하는 것입니다.이 전략은 이미지 생성에서 예시됩니다[23].이 아이디어는 AudioLDM[4], Make-AnAudio[15], TANGO[51]를 포함한 다양한 오디오 생성 작업에 채택되었습니다.이러한 작업은 연속적인 잠재 공간에서 학습된 잠재 확산 모델을 활용합니다.반면에 이산적인 잠재 공간에서 확산을 탐구하는 연구도 있습니다. 예를 들어, DiffSound [14]는 오디오 언어 계산/예측 A에서 중복성 [52], [53]을 완화하기 위해 이산 자동 인코더를 사용합니다.x Y 또는 M: CY 오디오 언어(LOA) YX 또는 Ŷ 자체 감독 사전 학습 생성 모델 GYx 또는 G: Yxx **** Ꮖ 재구성된 패치 풀링 I ConvBlock 패치 ε~N(1,0) •Z₁=√α+Zq+(1−α+)ε A=오디오 언어(LOA) Universal Vocoder AudioMAE Freq➡ KVQ Transformer Block Encoder Υλ Pgt xn Time VAE Decoder Ŷ A(): Audio to LOA Encoder Ppred Train Q Zo Transformer Block Infer GPT-☐ ☐ ☐ ☐ ☐ VAE Encoder E 선형 투영 헤드 Prob. Switcher AudioMAE 기능 | CLAP IPhonemes Infer 2-1-G(Ź₁, ε) Train L(E,E) FLAN-T5 || Mel FilterBank STFT Encoder Transformer-UNetAudio Text Transcription M() Any Modality to LOA Translator *+*+ x G(): LOA to Audio Generator 그림 1. AudioLDM 2 아키텍처 개요. AudioMAE 기능은 컨디셔닝 정보를 LOA 변환 단계(GPT-2로 모델링)와 LOA에서 오디오 생성 단계(잠복 확산 모델로 모델링)로 연결하는 프록시입니다. 확률적 스위처는 기준 진실 AudioMAE(Pgt)와 GPT-2에서 생성된 AudioMAE 기능(P pred)을 조건으로 사용하여 잠복 확산 모델의 확률을 제어합니다. AudioMAE와 잠복 확산 모델은 모두 오디오 데이터로 사전 학습된 자체 감독 모델입니다. 오디오 파형을 추출하고 멜 스펙트로그램의 압축 표현을 생성합니다. DiffSound는 텍스트 조건부 이산 확산 모델을 사용하여 이산 토큰을 생성합니다.A. 개요 III. AUDIOLDMLet x = R¹은 오디오 신호를 나타내며, 여기서 L¸는 x의 오디오 샘플 길이입니다. 오디오 생성 프로세스는 H : C ⇒ x로 표시할 수 있습니다. 여기서 C는 조건 정보이고 H는 조건부 오디오 생성 시스템입니다. C에서 x를 직접 생성하는 것은 일반적으로 어렵습니다[54]. 재생성 학습[55]에서 동기를 얻어 섹션 III-B1에서 소개한 대로 C와 x 사이의 격차를 메우기 위해 x의 추상화인 중간 기능 Y를 활용하는 것을 제안합니다. 기능 Y를 오디오 언어(LOA)라고 합니다. LOA 기능은 Y = A(x)로 계산되며, 여기서 A는 AudioMAE[21], [55]와 같은 자체 감독 표현 학습 모듈을 사용하여 오디오를 LOA로 인코딩합니다. LOA 기능은 x에 비해 모델링하기 쉬운 표현이어야 하며 x에 대한 의미 있는 의미 정보를 포함해야 합니다. 그림 1에 도시된 바와 같이, 중간 표현 Y를 사용하여 전체 오디오 생성 프로세스는 Ho= GoM:CHYH2, (1)로 표시할 수 있습니다. 여기서 Ŷ는 기준 진실 LOA의 추정치입니다. (1)에 표시된 바와 같이 AudioLDM 2의 오디오 생성 프로세스는 다음 두 단계를 포함합니다. : (i) LOA 변환에 대한 조건 정보: 함수 MC Ŷ는 오디오 및 텍스트와 같은 다른 모달리티의 조건 정보일 수 있는 C를 기반으로 LOA Y를 생성하는 것을 목표로 합니다. 오디오 생성 측면에서 C를 보다 잘 표현할 수 있는 잠재적으로 생성된 Ŷ는 이후 단계에서 오디오 생성을 위한 조건 정보로 사용됩니다. 섹션 III-C에서 소개한 자기 회귀 모델링을 사용하여 함수 M을 구현합니다. (ii) 오디오 생성에 대한 LOA: M에 이어 함수 G는 LOA 추정치 Ŷ를 입력 조건으로 받아들이고 오디오 데이터 x를 추정합니다. 학습 과정 중에 학습 데이터 x가 사용 가능할 때 A()를 사용하여 기준 진실 Y도 사용할 수 있으므로 자기 감독 M(C)에서 G를 최적화할 수 있으며 Y 방식에 따라 a 생성을 조건화합니다. 구체적으로 LOA 추정 Ŷ : A(x)를 사용하는 대신 H₁ = Go Ax ⇒ Y ⇒ î로 공식화할 수 있습니다. (2) 섹션 III-B에서 A(.)의 세부 정보를 소개합니다. 프로세스 H₁에는 학습 데이터로 x만 포함되므로 방정식 (2)는 오디오 주석 없이도 모델 G를 자기 감독 방식으로 최적화할 수 있음을 의미합니다. 이 자기 감독 방식은 오디오 데이터 레이블의 부족[4]을 완화하고 전체 생성 시스템에 대한 강력한 백본을 제공할 수 있습니다. 여기서 자기 감독 학습은 전체 AudioLDM 2를 참조하지 않는다는 점에 유의하세요. 예를 들어 함수 M은 최적화를 위해 여전히 페어링된 데이터가 필요합니다. 섹션 III-D에서 소개한 잠재 확산 모델로 함수 G를 구현합니다. 다음 섹션에서는 AudioLDM 2에 대한 자세한 소개를 제공합니다. 섹션 III-B에서는 AudioMAE 및 VAE 기능을 포함하여 AudioLDM 2에서 사용되는 오디오 표현에 대해 설명합니다. 이러한 기능은 AudioLDM 2 내의 두 단계에 대한 생성 대상으로도 사용됩니다. 섹션 III-C에서는 GPT-2를 사용한 AudioMAE 기능의 자기 회귀 모델링을 소개합니다. 섹션 III-D에서는 기능 압축을 위해 VAE를 적용하고 LOA에 따라 오디오를 생성하는 잠재 확산 모델을 통해 오디오 파형을 생성하는 프로세스를 설명합니다. 여기서 LOA는 기준 진실 또는 GPT-2에서 생성된 데이터를 기반으로 할 수 있으며, 이는 각각 자체 감독 학습 및 GPT2를 사용한 공동 학습(섹션 III-D3)에 해당합니다. B. 오디오 표현 학습 MusicLM [8]과 AudioLM [26]에서 동기를 부여받은 두 가지 유형의 이산 표현 [25], [56]에 대한 의미 및 음향 모델링을 수행하는 유사한 2단계 모델링 접근 방식을 채택합니다.그러나 우리의 작업은 이전 연구 [25], [35]에서 사용된 이산 표현과 비교하여 잠재적으로 더 풍부한 정보를 제공할 수 있는 연속적인 의미 및 음향 표현에 대해 작업한다는 점에서 다릅니다.우리의 작업에서는 각각 의미 및 음향 표현 학습 모듈로 AudioMAE [21]와 변형 자동 인코더(VAE) [57]를 채택합니다.유사한 목적을 제공하지만 AudioMAE와 VAE는 아키텍처와 목표가 다르므로 서로 다른 표현을 제공합니다.표현 학습에 대한 자세한 내용은 아래에 제공됩니다.1) AudioMAE를 사용한 의미 표현 학습: 음성, 음악 및 음향 효과를 포함한 다양한 유형의 오디오를 정확하게 표현하려면 LOA Y가 오디오 신호의 의미 및 음향 세부 정보를 모두 효과적으로 캡처해야 합니다. 따라서 다운스트림 오디오 분류 작업에서 일반성과 높은 정확도를 위해 함수 A에 대한 표현 추출 모듈로 자기 지도 사전 학습된 AudioMAE [21]를 사용하기로 제안합니다 [21]. 오디오 마스크 자동 인코더(AudioMAE)는 수동으로 레이블이 지정된 주석에 의존하지 않고 레이블이 지정되지 않은 오디오 데이터에서 표현을 학습하는 오디오 자기 지도 사전 학습 모델입니다. AudioMAE는 인코더와 디코더로 구성되며 둘 다 비전 변환기(ViT) [58]와 유사한 아키텍처로 구현됩니다. 자기 지도 사전 학습 중에 일반적으로 멜 스펙트로그램인 인코더에 대한 입력 패치는 무작위로 마스크되고 디코더는 마스크된 패치를 재구성하는 방법을 학습합니다 [21]. 다른 오디오 자기 지도 사전 학습 모델과 비교할 때 AudioMAE는 다음과 같은 두 가지 장점이 있습니다. (i) AudioMAE는 일반 오디오 도메인에서 잘 작동하는 것으로 검증되었습니다. 예를 들어, AudioMAE는 AudioSet [59]에서 효과적으로 사전 학습될 수 있으며 다운스트림 오디오 분류 작업에서 최첨단 성능을 발휘합니다. 이에 비해 일반적인 오디오 자체 감독 모델은 음악의 MERT [60] 및 음성의 HUBERT [61]와 같이 특정 도메인에 초점을 맞춥니다. (ii) AudioMAE 기능은 다른 판별 사전 학습 방법보다 생성 작업에 더 적합할 가능성이 있습니다. 대조 손실 또는 다음 토큰 예측 분류 손실을 학습 목표로 삼아 wav2vec [62] 및 BYOL-A [63]와 같은 이전 시스템은 사전 학습 중에 판별 접근 방식을 활용합니다. 이에 비해 AudioMAE는 마스크된 패치의 재구성을 학습하여 생성 프로세스에 초점을 맞춥니다. 입력 오디오 신호 x에 대해 AudioMAE는 먼저 로그 멜 스펙트로그램 X = RTXF를 계산합니다. 여기서 T는 멜 스펙트로그램의 시간 단계를 나타내고 F는 멜 빈을 나타냅니다. 그런 다음 멜 스펙트로그램 X를 이미지로 처리하고 각각 크기가 P × P인 패치로 분할하여 AudioMAE 인코더에 대한 입력 GroundTruth Reconstruction λ =1.54.7.1.4.7.Time Time Reconstruction λ =Reconstruction λ =4.7.1.4.7.Time Time 그림 2. 잠재 확산 모델을 사용하여 LOA YX에서 오디오 재구성에 대한 λ의 영향. 재구성은 = 1일 때 기준 진실과 매우 유사하여 YX=1이 충분한 오디오 세부 정보를 유지함을 시사합니다. 그러나 2 또는 4의 경우 재구성은 원본 오디오와 약간 다르며, 이는 후처리된 AudioMAE 기능에 모든 세부 정보가 포함되지 않을 수 있지만 그럼에도 불구하고 의미적 내용을 정확하게 보존함을 나타냅니다. 패치 크기 P는 일반적으로 T와 F의 공통 인수가 되도록 설계됩니다. 패치 분할 및 임베딩은 커널 크기 P, 스트라이드 P 및 D개의 출력 채널을 갖는 합성 신경망을 사용하여 수행됩니다. 이를 통해 T&#39; × F&#39; × D의 출력 모양이 생성되는데, 여기서 D는 AudioMAE 임베딩 차원, T&#39; = T/P 및 F&#39; F/P입니다. AudioMAE 인코더의 결과 출력 기능 E = RT&#39; F&#39;XD는 입력과 동일한 모양을 가지며 일반적으로 사전 학습 후 다운스트림 작업의 기능으로 처리됩니다[21]. = 2) AudioMAE 기능 사후 처리: 그림 1에서 볼 수 있듯이 AudioMAE 기능 E가 계산되면 추가 풀링 단계를 도입하여 E를 YX로 집계합니다. 여기서 I+는 사후 처리 풀링 단계에서 사용되는 하이퍼 매개변수를 나타냅니다. 이 풀링 단계는 시퀀스 길이를 줄여 함수 M에서 더 쉬운 추정을 용이하게 하는 것을 목표로 합니다. 구체적으로, E = RT&#39; F³×D의 처음 두 차원에서 2차원 평균-최대 풀링[52]을 수행합니다. 여기서 풀링 커널 크기와 스트라이드는 동일한 값 A Є I+를 갖습니다. 2차원 풀링 연산은 출력에서 시간-주파수 관계를 유지하는 데 도움이 될 수 있습니다. 풀링 후의 최종 출력 YX는 Lx × D 모양의 임베딩 시퀀스로 재구성되며, 여기서 L T&#39;F&#39; /X²입니다. 구현을 용이하게 하기 위해 L、이 항상 양의 정수가 되도록 \를 선택합니다. 그림 2에서 \의 다른 선택의 효과를 보여줍니다. 이 논문의 나머지 섹션에서 \가 지정되지 않으면 Y를 간단히 Y라고 합니다. 3) VAE를 사용한 음향 표현 학습: VAE를 사용하여 특징 압축과 x보다 차원이 상당히 작은 오디오 표현 2를 학습합니다[4]. 이 작업에서 사용한 VAE는 [4]에 기술된 아키텍처를 따르는 다운 샘플링이 있는 인코더와 업 샘플링이 있는 디코더로 구성된 합성곱 아키텍처입니다.VAE의 순방향 패스는 V: XZ X로 공식화할 수 있습니다.여기서 X는 x의 멜 스펙트로그램이고 ✰는 x의 재구성입니다.재구성 ✰은 사전 학습된 HiFiGAN 보코더를 사용하여 오디오 파형 ✰으로 변환할 수 있습니다[64].AudioLDM[4]에 따라 VAE의 매개변수를 최적화하기 위해 X와 Ŷ에 기반한 재구성 손실과 판별 손실을 계산합니다. 또한, 우리는 z와 표준 가우시안(µ = 0, σ² (µ = 0, σ² = 1) 사이의 KL 발산을 1)의 손실 함수로 계산하여 VAE 잠재 공간의 시각화를 제한합니다.AudioMAE 잠재 공간 시각화 양치질 박수 엔진 톱 물 붓기 파도 양 코골이 변기 물 내리기 기차 양치질 박수 엔진 톱 물 붓기 파도 양 코골이 변기 물 내리기 기차 그림 3. ESC50[65] 데이터 세트에서 tSNE 및 무작위로 선택한 10개 클래스를 기반으로 한 잠재 공간 시각화. 그림의 각 지점은 오디오 클립을 나타냅니다. AudioMAE 특징 공간은 유사한 오디오 클립을 함께 그룹화하는 경향이 있어 VAE 특징보다 더 많은 의미적 구조를 나타냅니다. VAE 잠재 공간의 분산. 4) AudioMAE와 VAE의 비교: AudioMAE와 VAE는 모두 표현 학습을 위한 자동 인코더를 기반으로 하기 때문에 AudioMAE 잠재 공간을 직접 모델링하는 대신 표현 학습에 VAE를 사용하는 이유가 궁금할 수 있습니다. 그 이유 중 하나는 AudioMAE가 주로 재구성 품질에 초점을 맞추지 않고 잠재 공간 압축 비율이 VAE만큼 높지 않기 때문입니다. 반면에 VAE는 재구성 능력이 우수하고 AudioMAE보다 압축 수준이 높아 VAE가 멜-스펙트로그램 압축에 더 적합합니다. 또한 그림 3에서 볼 수 있듯이 tSNE [66]를 사용하여 ESC-50 [65] 데이터 세트에서 AudioMAE와 VAE의 잠재 표현을 시각화합니다. 이 시각화는 AudioMAE의 잠재 표현이 잠재 공간의 더 가까운 영역에 유사한 오디오를 그룹화할 수 있음을 보여줍니다. 반면 VAE의 표현은 서로 다른 오디오 클래스 간에 더 많은 중복을 보입니다. 이는 AudioMAE와 VAE의 표현이 서로 다르다는 것을 나타냅니다. AudioMAE는 의미적 측면에서 더 많은 정보를 포함하는 반면, VAE 표현은 의미적으로 덜 구조화되어 있습니다.따라서 섹션 III-A의 LOA 정의에 따르면 AudioMAE는 LOA를 계산하는 데 VAE보다 더 적합합니다.C. GPT를 사용한 LOA 변환에 대한 정보 조절-이 하위 섹션에서는 함수 M의 설계를 소개합니다.섹션 III-A에서 소개했듯이 모델 G : Y → x의 입력은 AudioMAE를 사용하여 계산할 수 있습니다.그러나 추론 중에 조건 C&#39;로 오디오 생성을 수행할 때 기준 진실 LOA YA(x)를 사용할 수 없습니다.따라서 = Me: CY로 표시되는 C가 주어졌을 때 Ŷ를 생성할 수 있는 또 다른 모델이 필요합니다.여기서 0은 학습 가능한 매개변수를 나타냅니다.특히 Y 생성을 언어 모델링 작업으로 취급하고 GPT-2(Generative Pre-trained Transformer 2) [22] 모델을 백본으로 선택합니다. GPT-2는 변압기 아키텍처를 기반으로 하며 원래 비지도 학습 접근 방식을 사용하여 총 40GB의 텍스트에 대해 800만 개의 문서에서 학습되었습니다[22]. GPT-2는 텍스트 완성, 질의 응답, 언어 번역과 같은 다양한 자연어 처리 작업에 사용되었습니다[67], [68]. 사전 훈련된 가중치로 초기화하여 교사 강제 [69]를 기반으로 GPT2 모델을 미세 조정하여 모델 훈련 중에 ŷɩ가 조건 C와 기준 진실 시퀀스 Y1, ..., Yl-1을 기반으로 생성되도록 합니다.여기서 yɩ는 LOA 시퀀스 Y의 l번째 벡터입니다.특히, GPT2 모델 Me는 시퀀스 Pr(y1, 2, ..., YL|C)의 우도를 최대화하도록 훈련되며, 이는 다음 최적화 목표로 해석할 수 있습니다.L argmax, Ec [Pr(y1|C; 0) [[ Pr(yı|Y1, ..., Yl−1, C;0)], (3) 1== 여기서 Ec는 변수 C에 대한 기대 연산자를 나타냅니다.식 (3)을 최적화하기 위해 yɩ와 ŷ Mo(y1, ..., Yl−1, C) 사이의 평균 제곱 오차 손실 [54]을 계산합니다. 우리는 AudioMAE 특징 공간을 이산화하고 토큰 인덱스를 추정하지 않고 연속 벡터 y₁의 회귀를 직접 최적화합니다. 방정식 (3)의 조건 C는 오디오 표현, 텍스트 임베딩, 음소 임베딩 또는 시각적 단서를 포함한 유연한 범위의 데이터 표현을 포함할 수 있습니다. 우리는 전문가의 혼합[70] 접근 방식을 채택하고 여러 인코더를 특징 추출기로 사용하여 C를 계산합니다. 특징 추출 모듈로 K 시스템이 주어지면 k번째 시스템 Ck, k = {1, ..., K}의 출력 모양은 Lk × Dk입니다. 여기서 Lk는 k번째 시스템의 시퀀스 길이이고 Dk는 특징의 차원입니다. 우리는 각 특징 추출 모듈의 출력 뒤에 선형 변환 계층을 적용하여 GPT-2 모델을 보다 쉽게 처리하기 위해 임베딩 차원을 D0로 통합합니다. 순차적 정보 없이 입력에서 글로벌 특징을 추출하는 모듈(예: CLAP[71] 또는 ImageBind[18])의 경우 Lk = 1입니다. 최종 조건 C = [C₁,...CK]는 시퀀스 길이 차원을 따라 Ck의 연결입니다. 최종 조건 C는 L×D0 모양이며, 여기서 L=1Lk입니다. 이 논문에서 사용한 여러 조건 모듈을 다음과 같이 소개합니다. K CLAP 또는 대조 언어 및 오디오 사전 학습[71]은 오디오와 언어 데이터가 잠복 공간에서 더 가까운 거리를 갖는 공동 오디오-텍스트 임베딩 공간을 학습하는 시스템입니다. CLAP은 AudioLDM[4]과 같은 오디오 생성에 컨디셔닝 모듈로 성공적으로 적용되었습니다. 이 연구에서는 사전 학습된 CLAP¹ 텍스트 인코더를 기본 컨디셔닝 모듈로 사용하여 텍스트 임베딩을 조건으로 추출합니다. 그러나 텍스트 캡션(예: &quot;한 남자가 배경 정적 소음 속에서 행복하게 말하고 있다&quot;)을 사용할 수 없는 시나리오(예: 텍스트 음성 변환 작업)에서는 [4]와 같은 방식으로 CLAP 텍스트 인코더를 사용하는 대신 CLAP 오디오 인코더를 컨디셔닝 모듈로 사용합니다. https://github.com/LAION-AI/CLAPFLAN-T5. 글로벌 수준 조건을 계산하는 모듈인 CLAP 모델은 텍스트 데이터의 시간 정보를 캡처하는 데 문제가 있는 것으로 밝혀졌습니다. [72] 이를 허용하기 위해 다른 사전 학습된 텍스트 인코더를 사용하여 시간 순서와 같은 유용한 세부 정보를 포함할 수 있는 텍스트 입력의 의미 정보를 캡처합니다. 구체적으로, 여러 작업²에 대한 미세 조정을 기반으로 하는 텍스트-텍스트 전송 변환기(T5) 모델[74]의 향상된 버전인 FLAN-T5[73]를 활용합니다. Phoneme Encoder는 언어에서 단어를 구별할 수 있는 가장 작은 소리 단위인 음소[2], [11]에 대한 유용한 정보를 추출하기 위한 텍스트-음성 연구에서 널리 채택된 모듈입니다[75]. 이 연구에서는 NaturalSpeech[2]에서 도입한 구조를 따라 변압기 인코더 계층을 스택한 형태의 음소 인코더를 구축합니다. 오픈 소스 도구인 Espeak 음소 변환기를 사용하여 텍스트 입력을 음소로 사전 처리하고 각 음소 시퀀스 뒤에 종료 토큰을 추가하여 변압기 모델의 시퀀스 끝을 표시합니다. 쉽게 사용할 수 있는 사전 학습된 가중치가 없는 음소 인코더를 제외하고 다른 모든 사전 학습된 기능 추출 모델의 매개변수는 실험 동안 고정됩니다. D. 잠재 확산 모델을 사용한 오디오 생성에 대한 LOA 우리는 잠재 확산 모델(LDM)[23]을 사용하여 프로세스 G : Y x를 모델링합니다. 이는 잡음 제거 확산 확률 모델(DDPM)[36]의 변형입니다. DDPM과 달리 학습 데이터를 직접 모델링하는 LDM은 압축 잠재 공간[76]에서 역 확산 프로세스를 학습하여 계산 비용을 줄일 수 있습니다.AudioLDM[4]과 같이 유사한 아이디어가 오디오 생성에 적용되었습니다.1) 잠재 확산 모델: LDM을 구현하기 위해 [36]의 공식을 따릅니다.VAE 표현 z가 주어지면 순방향 전환은 학습 가능한 매개변수를 포함하지 않는 방식으로 T 단계 마르코프 프로세스로 정의됩니다.확산 단계 t-1에서 데이터 zł−1이 주어지면 단계 2, ..., T에서 zt의 데이터 분포는 q(zt|zt−1) = √1 − ßt¾t−1 + √ßt€t로 공식화할 수 있습니다.여기서 노이즈 스케줄 하이퍼 매개변수 ẞt Є [0,1]은 노이즈가 데이터에 혼합되는 속도를 결정합니다. 방정식 (4) [36]에서 q(zt|zt-1)을 재귀적으로 대체하면 zo가 주어진 경우 zt의 분포를 9(zt|20) t=atЄt, (5)로 도출할 수 있습니다. 여기서 at = П11 ẞt이고 et ~ N(0, 1)입니다. 최종 단계 t = T에서 z의 분포는 표준 가우시안 분포 [36]에 가까워집니다. LDM은 사전 분포 N(0, 1)에서 데이터 분포 z로의 역방향 전이를 학습합니다. 역 프로세스는 조건부 분포 Pr(zo...T|Y; 0) = 2https://huggingface.co/google/flan-t5-large https://github.com/espeak-ng/espeak-ng =Pr‍(z0|21, Y; 6) II— ±2 Pr(zt−1|zt, Y; ó) · Pr(zÃ)를 모델링합니다. 여기서 Y는 조건 신호로서의 LOA이고 &gt;는 역 확산을 학습하기 위한 모델의 매개변수를 나타냅니다. 21...T를 주변화하면 증거 하한(ELBO)과 베이즈 규칙[36]을 기반으로 log[Pr(z0|Y; 0)]의 하한을 도출할 수 있습니다. log[Pr(zo|Y; ø)] ≥ log[Pr(z0|21, Y; 0)]— T Σ KL[Pr(zt-1 zt, Y; 0)||9(Zt−1|Zt, 20)], (6) t=여기서 KL()은 KL 발산을 계산하는 함수이고, q(zt-1 t, 20))은 zo와 zł이 주어졌을 때 폐쇄 형식 솔루션을 갖는 목표 조건부 확산 분포입니다[36]. [36]에 따라 방정식 (6)의 하한을 최대화하는 손실 함수를 다음과 같이 유도할 수 있습니다.argmin [Ezo, Y, t~{1,...,T} ||G(√√αtzo±√√1 – at€t, t, Y; ø)—et ||]. (7) 그림 1에서 볼 수 있듯이 방정식 (7)의 함수 G로 Transformer-UNet(T-UNet) 아키텍처를 활용합니다.이 아키텍처는 AudioLDM [4]에서 사용된 UNet과 유사하지만 변환기 계층이 더 많습니다.T-UNet 아키텍처는 다운샘플링이 있는 일련의 인코더와 업샘플링이 있는 일련의 디코더로 구성되며 동일한 스케일에서 인코더와 디코더 사이에 건너뛰기 연결이 있습니다.T-UNet의 모델링 용량을 향상시키기 위해 각 인코더 및 디코더 블록에서 합성 연산 후에 여러 변환기 블록을 삽입합니다. 구체적으로, 우리는 첫 번째 trans 변환기 블록이 자기 주의 층[77]과 피드포워드 네트워크의 스택인 trans + 1 변환기 블록을 가지고 있습니다.그림 1에서 볼 수 있듯이 기준 진실 LOA의 조건 정보 Y 또는 M()의 Ŷ(섹션 III-C)를 통합하기 위해 마지막 변환기 블록은 자기 주의 층을 교차 주의로 변경합니다.이 교차 주의는 LOA를 키와 값으로 받아들이고 이전 변환기 블록의 기능을 쿼리로 융합합니다.텍스트-음성 생성을 제외하고, 변환기 블록에 여분의 교차 주의 층을 추가하여 오디오-텍스트 관계 학습을 향상시키기 위한 추가 조건으로 FLAN-T5[73]의 텍스트 임베딩을 받아들입니다.w = 2.w = 3.w = 4.w = 1.그림 4. 다른 분류기 없는 안내 척도로 생성된 샘플.텍스트 프롬프트는 &quot;고양이가 야옹거린다&quot;입니다. 2) 분류자 없는 안내: 확산 모델의 경우 각 샘플링 단계에서 안내를 도입하여 제어 가능한 생성을 달성할 수 있습니다.분류자 없는 안내[78], [79](CFG)는 확산 모델을 안내하는 최첨단 기술입니다.학습하는 동안 고정 확률(예: 10%)로 방정식(7)에서 조건 Y를 무작위로 삭제하여 조건부 LDM G(zt, t, Y; 6)과 무조건부 LDM G(zt, t, o)를 모두 학습합니다.생성을 위해 LOA Ŷ 또는 Y를 조건으로 사용하고 수정된 노이즈 추정 G&#39;(zt, t, Y; ø)로 샘플링을 수행합니다.G&#39;(zt, t, Y; 0) = wG(zt, t; ø) + (1 − w)G(zt, t, Y; 0), (8) 여기서 w는 안내 척도를 결정합니다. 3) 공동 미세 조정: 우리는 방정식 (1), (7), (3)을 기반으로 GPT-2 및 잠재 확산 모델로 공동 미세 조정을 수행합니다. 표 V에서 보여준 것처럼, 우리는 공동 미세 조정이 AudioLDM 2 시스템의 전반적인 성능을 크게 향상시킨다는 것을 발견했습니다. 그림 1에 나와 있듯이, 확률적 스위처는 공동 훈련 프로세스 동안 컨디셔닝 신호의 소스를 제어합니다. 훈련하는 동안 스위처는 기준 진실 AudioMAE 피처와 GPT에서 생성된 AudioMAE 피처 중에서 동적으로 선택하며, 확률은 각각 Pgt와 Ppred로 설정됩니다. A. 데이터 세트 IV. 실험 설정 이 작업에서 사용된 데이터 세트에는 AudioSet(AS)[59], WavCaps[80], AudioCaps(AC)[27], VGGSound(VS)[81], Free Music Archive(FMA)[82], Million Song Dataset(MSD)[83], LJSpeech(LJS)[28], GigaSpeech(GGS)[84]가 포함됩니다. AudioSet은 이 글을 쓰는 시점에서 가장 큰 오디오 분류 데이터 세트로, 약 200만 개의 10초 오디오와 527개의 다른 클래스를 포함합니다. WavCaps는 ChatGPT 지원 약하게 레이블이 지정된 오디오 캡션이 있는 데이터 세트입니다. WavCaps에는 평균 길이가 68초인 403,050개의 오디오 클립이 포함되어 있습니다. AudioCaps는 수작업으로 캡션을 제공한 AudioSet의 하위 세트로, 약 46,000개의 10초 오디오 클립이 포함되어 있습니다. VGGSound는 200개 이상의 비디오를 포함하는 대규모 단일 레이블 오디오-비주얼 데이터 세트입니다. 우리는 VGGSound의 오디오 데이터와 레이블만 활용합니다. FMA는 캡션이 없는 대규모 음악 데이터 세트로, 16,341명의 아티스트와 14,854개의 앨범에서 나온 106,574개의 음악 트랙을 포함합니다. Million Song Dataset의 경우 [85]에서 제안한 레이블이 지정된 하위 세트만 활용하는데, 여기에는 태그, 제목, 아티스트 이름과 같은 메타데이터가 있는 약 510,000개의 음악 트랙이 포함됩니다. LJSpeech는 13,100개의 짧은 오디오 클립과 자세한 필사본이 있는 단일 화자 음성 데이터 세트입니다. GigaSpeech는 약 10,000시간 분량의 오디오에 필사본이 지정된 다중 화자 대규모 영어 음성 인식 코퍼스입니다. GigaSpeech의 테스트 및 개발 세트는 훈련 중에 포함되지 않습니다. 이 작업에서 사용된 모든 오디오 데이터는 이전 작업 [4], [15]과 쉽게 비교하기 위해 16kHz로 리샘플링되었습니다. 우리는 방정식 (3)을 최적화하여 GPT-2 모델을 훈련하기 위해 쌍을 이룬 텍스트 레이블이 있는 오디오 데이터만 사용합니다. 우리는 방정식 (6)의 목적을 자기 감독 방식으로 최적화하여 주석에 관계없이 모든 오디오 데이터로 잠재 확산 모델을 훈련합니다. B. 평가 지표 우리는 주로 텍스트-오디오 생성 작업에 중점을 두어 AudioLDM 2의 효과를 평가합니다. 우리는 Frechet Audio Distance(FAD), Kullback-Leibler Divergence(KL)와 같은 객관적 지표와 Overall Impression(OVL) 및 Audio and Text Relation(REL)을 포함한 주관적 지표를 모두 계산하는 AudioGen [3]의 평가 프로토콜을 따릅니다. 또한 생성된 오디오와 텍스트 프롬프트 간의 대응성을 측정하기 위해 추가 지표 CLAP 점수 [15]를 포함했습니다. FAD는 VGGish [86] 모델에서 추출한 생성된 오디오와 대상 오디오의 특징 간의 분포 거리를 기반으로 계산되는 참조 없는 오디오 품질 측정입니다.KL divergence는 AudioGen [3]과 같은 방식으로 오디오 태그 모델인 Patch-out Transformer [87]에서 계산한 레이블을 사용하여 생성된 오디오와 대상 오디오 간의 유사도를 측정합니다.CLAP 점수는 사전 학습된 오디오 및 텍스트 인코더 쌍[71]을 기반으로 오디오와 텍스트 간의 유사도를 측정하며, exer CLAPScore (x, r) (9) max (||ex||||er||, €)로 제공됩니다.여기서 x와 r은 각각 오디오 및 텍스트 데이터를 나타내고 e는 0 나누기를 피할 수 있는 작은 값이고 ea는 CLAP 오디오 인코더의 출력이고 ĕr은 CLAP 텍스트 인코더의 출력입니다.CLAP 점수의 값 범위는 -1과 1 사이이며 값이 클수록 오디오와 텍스트 정보 간의 상관 관계가 강함을 나타냅니다. = , 텍스트-음악 생성을 위해 유사한 평가 프로토콜을 사용합니다. 텍스트-음성 작업의 경우 일반적으로 사용되는 평균 의견 점수(MOS)를 평가에 활용합니다[75]. C. 주관적 평가 크라우드 소싱 플랫폼인 Amazon Mechanical Turkª를 사용하여 OVL, REL, MOS를 포함한 주관적 지표를 평가합니다. 평가를 수행하는 방법에 대한 지침은 예를 들어 평가자를 위해 명확하게 설명되어 있습니다. 구체적으로 OVL의 경우 평가자에게 이 음악의 전반적인 품질을 어떻게 평가하시겠습니까? 실제 오디오와의 유사성과 자연스러움을 고려하여 5점 척도(5: 우수한 품질에서 1: 나쁜 품질)로 질문을 했습니다. 마찬가지로 REL의 경우 질문은 텍스트 설명과 음악의 관련성을 어떻게 평가하시겠습니까?였으며 응답에 대한 5점 척도도 유사했습니다. MOS를 평가할 때 질문은 이 녹음이 얼마나 자연스럽게 들리십니까?였습니다. 감정, 음조 및 기타 인간과 유사한 세부 사항을 고려합니다. 완전히 부자연스러운 말에서 완벽하게 자연스러운 말까지 다양한 옵션이 있습니다. 평가 결과의 신뢰성을 보장하기 위해 최소 평균 승인율이 60%이고 기록에 승인이 최소 50개 이상인 크라우드 소싱 근로자에 대한 요구 사항을 설정했습니다. 각 오디오 클립은 최소 10명의 다른 평가자가 평가합니다. 세 가지 주관적 지표 모두 1~5 사이의 리커트 척도[88]를 가지며, 숫자가 클수록 성과가 더 좋음을 나타냅니다. 연구 평가자는 미국 최저 임금 이상의 지불을 받았습니다. 모든 평가자와 샘플의 점수를 평균하여 시스템의 최종 점수를 매깁니다. D. 모델 아키텍처 세부 정보 우리는 두 가지 크기의 잠재 확산 모델인 AudioLDM 2와 AudioLDM 2-Large로 실험을 수행하며, 각각 변압기 레이어 번호는 trans = 2와 trans 6입니다(섹션 III-D). 우리는 패치 크기가 16 × 16이고 중복이 없는 사전 학습된 AudioMAES 4https://requester.mturk.com/ Shttps://github.com/facebookresearch/AudioMAE =를 사용하여 멜 스펙트로그램의 10초마다 길이가 512인 768차원 피처 시퀀스를 생성합니다. [89]에서 도입한 아이디어와 유사한 방식으로 LOA Y를 계산할 때 AudioMAE 인코더에서 마지막 16개 변환기 레이어의 출력을 수집하여 최종 Y로 평균을 수행합니다. 우리는 섹션 IV-A에서 언급한 오디오 데이터로 AudioLDM 2와 AudioLDM 2-Large 모두에서 자체 감독 사전 학습을 수행합니다. 우리가 사용하는 GPT-2 모델은 12개 레이어의 변환기로 768의 임베딩 차원을 갖습니다. 공동 미세 조정의 경우, 실제 LOA Y와 LOA 추정 Ŷ를 사용할 확률을 각각 Pgt 0.25와 Ppred 0.75로 설정했습니다. 표 I = = 수행한 기본 실험 설정. FULL은 AC, AS, WC, VS 및 MSD를 포함한 다섯 가지 다른 데이터 집합의 조합을 나타냅니다. †가 있는 모델은 CLAP 텍스트 인코더와 FLAN-T5를 사용하여 조건을 계산하는 반면, †가 있는 모델은 CLAP 오디오 인코더와 음소 인코더를 조건 모듈로 사용합니다. 모델 λ 매개변수 데이터 세트 AudioLDM 2-AC+ 8 346M AudioLDM 2-MSD+346M AC MSD AudioLDM 2-Full+346M FULL 작업 TTA TTM TTA/TTM AudioLDM 2-AC-Large+ AudioLDM 2-Full-Large+ AC 8 712M FULL 1 346M LJS AudioLDM 2-LJS-Pretrained 1 346M LJS+GGS712M TTA TTA/TTM TTS TTS AudioLDM 2-LJS‡ 표 I은 이 논문에서 수행한 실험을 요약한 것입니다. 오디오와 음악을 생성하기 위해 CLAP 텍스트 인코더의 텍스트 임베딩과 FLAN-Tas 컨디셔닝을 결합하고 YX=8을 GPT의 대상 시퀀스로 지정합니다. 음성 생성을 위한 컨디셔닝 모듈은 주로 더 작은 \ 값을 통해 음성 신호의 세분화된 음소 정보를 더 잘 보존해야 하기 때문에 다르게 구성됩니다.따라서 음성 생성의 경우 CLAP 오디오 인코더와 음소 인코더의 출력을 GPT-2 모델의 입력 시퀀스로 연결하고 YX=1을 대상 시퀀스로 지정하여 더 많은 세부 정보를 유지합니다.음성 데이터의 경우 사용 가능한 오디오 캡션(전사본과 다름)이 없으므로 AudioLDM[4]과 유사한 접근 방식을 채택하여 모델 학습 중에 CLAP 오디오 인코더를 활용하여 임베딩을 조건으로 계산하고 추론 중에 CLAP 텍스트 인코더를 사용합니다.이 방법은 또한 그림 6에서 볼 수 있듯이 프롬프트 기반 화자 제어를 용이하게 합니다.E. 학습 및 추론 설정 잠재 확산 모델과 GPT-2 모델은 처음에 모델 학습 중입니다.잠재 확산 모델을 학습하고 8개의 NVIDIA A100 80GB GPU에서 GPT-2 모델을 미세 조정합니다. 우리는 AudioLDM [4]에 설명된 설정을 따르고 Denoising Diffusion Implicit Models(DDIM) [90] 샘플링 동안 기본 분류기 없는 안내 척도를 3.5로 변경합니다.GPT-2 미세 조정과 잠재 확산 모델 모두에 대해 10-4의 학습 속도와 감쇠 없이 10000단계의 선형 워밍업을 갖는 AdamW [91] 최적화 도구를 활용합니다.V. 결과 우리는 제안한 시스템을 텍스트-오디오, 텍스트-음악, 텍스트-음성의 세 가지 주요 오디오 생성 작업에서 평가했습니다.세 가지 기본 시스템은 AudioCaps(일반 오디오), MSD(음악), LJSpeech(음성)의 세 가지 다른 데이터 세트에서 학습되었으며 각각 AudioLDM 2AC, AudioLDM 2-MSD, AudioLDM 2-LJS로 표시됩니다. 모델 AudioLDM 2-Full은 섹션 IV-A에서 언급된 모든 사용 가능한 데이터를 포함하여 최대 29510시간까지 확장된 교육 데이터를 사용하여 오디오와 음악 생성을 동시에 수행할 수 있는 버전을 나타냅니다.AudioLDM [4]과 달리 전체 규모 데이터 세트로 교육된 모델의 경우 AudioCaps에 대한 추가 모델 미세 조정을 수행하지 않습니다.Large 접미사가 있는 모델은 AudioLDM 2-Full-Large와 같이 더 큰 크기의 모델 변형을 나타냅니다.A. 텍스트-오디오 생성 우리는 제안한 모델의 성능을 AudioGen-Large [3], Make-an-Audio [15], AudioLDM [4], Make-an-Audio 2 [92], TANGO [51]를 포함한 여러 최신 시스템과 비교합니다. 주관적 평가를 위한 샘플을 생성하기 위해 HuggingFace에서 652M 매개변수를 갖는 AudioLDM인 AudioLDM-M을 채택하고 100개 역 확산 단계로 실행합니다.Make-an-Audio 2의 결과는 저자[92]가 제공합니다.GitHub에서 오픈 소스로 제공되는 사전 학습된 TANGO 모델을 사용하여 결과를 재생성합니다.표 II에서 볼 수 있듯이 제안하는 AudioLDM 2-AC는 세 가지 객관적 지표 모두에서 이전 시스템보다 상당히 우수한 성능을 발휘합니다.이전 최고 성능 시스템인 TANGO는 17.6의 CLAP 점수를 달성한 반면, 제안하는 시스템은 훨씬 더 높은 24.9의 CLAP 점수로 이를 능가합니다.AudioLDM 2-Large는 또한 최고 KL 발산 점수 0.98을 달성하여 이전 SOTA 1.27보다 상당히 개선되었습니다.FAD 점수의 경우 모델은 1.42에 도달하여 텍스트-오디오 생성을 위한 새로운 SOTA를 확립했습니다. 우리의 주관적 평가 결과는 대부분 객관적 지표와 일치하며, AudioLDM 2-AC의 효과를 확인시켜 주는데, 이는 OVL 3.88과 REL 3.90을 달성하여 AudioLDM과 이전의 SOTA TANGO를 상당한 차이로 능가합니다. 별도로 훈련된 실제 오디오인 AudioLDM 2AC와 GroundTruth의 차이입니다. AudioCaps 데이터 세트[27]에서 무작위로 &gt; = {1,2,4,8}을 선택한 것은 YX와 다른 λ에서 모델 견고성을 향상시키기 위한 잠재 확산 모델의 OVL 사전 훈련에 대해 단지 0.16과 0.18입니다. Y는 T-UNet 교차 주의 계층에서 키와 값으로만 사용되므로 Y는 길이가 다를 수 있습니다. 훈련 세트에서 무작위로 선택한 10초 분량의 세그먼트를 기반으로 잠재 확산 모델을 훈련합니다. T-UNet을 보다 쉽게 모델링하기 위해 10초 오디오 세그먼트를 각각 10.24초와 REL로 제로패딩하여 제안하는 시스템의 강력한 성능을 입증했습니다. 사용한 AudioLDM-M은 AudioCaps 데이터 세트에서 미세 조정되지 않았으며, 이는 보고된 메트릭 점수와 비교했을 때 성능이 저하된 것을 설명할 수 있습니다. 6https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation 7https://github.com/declare-lab/tango 표 II AUDIOCAPS 평가 세트에서 모델 성능 비교. GT-AUDIOMAE는 섹션 III-A에 자세히 설명된 대로 오디오 생성을 위한 함수 G에 기본 진실 &quot;오디오 언어&quot; Y를 직접 적용한 것을 나타냅니다. AudioLDM 2는 주관적 및 객관적 평가에서 이전 방법을 크게 능가합니다. 모든 모델은 AUDIOCAPS 훈련 하위 집합을 사용하여 훈련되었습니다. *로 표시된 모델은 이 하위 집합에 대해서만 독점적으로 훈련된 반면, #로 표시된 모델은 이 하위 집합에 대해 미세 조정되었습니다. 모델 GroundTruth 지속 시간(h) 매개변수 FAD↓ KL↓↓ CLAP↑ OVL ↑ REL ↑ 0.4.4.GT-AudioMAE AudioGen-Large 1.0.19 0.3.4.1 B 1.1.Make-an-Audio453 M 2.1.AudioLDM-Large#739 M 1.1.AudioLDM-M416 M 4.1.0.3.3.Make-an-Audio937 M 2.1.0.3.3.TANGO*866 M 1.1.0.3.3.AudioLDM 2-AC*346 M 1.1.0.3.3.AudioLDM 2-Full AudioLDM 2-AC-Large* AudioLDM 2-Full-Large346 M 1.1.0.3.3.712 M 712 M 1.42 0.1.86 1.0.3.3.0.3.3.A 연필로 노트에 낙서하는 모습.1.4.7.Time 마법의 숲에서 메아리치는 마법의 요정의 웃음소리.주의를 끌기 위해 우는 새끼 고양이.° 1.4.7.Time 원숭이가 큰 원자 폭탄에 머리를 맞기 전에 웃는다.° 1.54.7.1.4.7.Time 시간 아코디언이 말한다.1.4.7.Time 기발한 동요에 나오는 장난기 어린 글로켄슈필이 상상력을 자극한다.1.7.Time 웅장한 교향곡을 연주하는 클래식 오케스트라.1.4.7.Time 활기찬 릴을 연주하는 전통 아일랜드 피들.1.4.7.Time 그림 5. 텍스트-오디오 생성의 예. [4]에서. 우리는 제한된 데이터 세트 크기로 인해 AudioCaps 학습 세트에서 AudioLDM 2 학습 동안 과적합 경향을 관찰합니다. 이 문제를 해결하기 위해 5개 에포크마다 AudioCaps 검증 세트에서 FAD 점수를 측정하고 FAD 결과가 저하를 보여주기 전의 체크포인트를 최종 모델로 처리합니다. 모델 크기와 데이터 세트 규모 측면에서 AudioLDM 2-AC-Large의 확장성을 조사하기 위해 두 가지 다른 모델 크기를 사용하여 29,시간 분량의 데이터가 포함된 훨씬 더 큰 데이터 세트에서 AudioLDM 2를 추가로 학습했습니다. 과적합 문제와 잠재적으로 오해의 소지가 있는 객관적인 지표 결과를 피하기 위해 AudioLDM 2-Full 및 AudioLDM 2-Full-Large를 포함한 더 큰 데이터 세트로 학습한 모델은 이전 연구[4]에서 수행한 것처럼 AudioCaps 학습 세트에서 미세 조정되지 않았습니다. 표 II에서 볼 수 있듯이, FAD 점수는 일반적으로 모델 크기를 확장한 후에 개선을 보인 반면, KL 발산점과 CLAP 점수는 뚜렷한 개선을 보이지 않아 모델 크기를 확장하는 것이 오디오 텍스트 관계보다 오디오 품질을 향상시키는 데 더 유익할 수 있음을 시사합니다. 훈련 데이터가 상당히 증가했음에도 불구하고 객관적인 평가 지표에서 상당한 개선을 관찰하지 못했습니다. 반대로, 세 가지 지표 모두 더 많은 데이터로 훈련한 후에 성능이 저하되었습니다. 이는 테스트 세트가 제한된 분포를 가지고 있는 반면 대규모 훈련 데이터는 훨씬 더 넓은 분포를 포함하기 때문일 수 있습니다. 훈련 데이터 분포와 테스트 데이터 분포의 불일치로 인해 객관적인 점수가 낮아집니다. 그럼에도 불구하고, AudioCaps에서 미세 조정 없이 대규모 사전 학습된 텍스트-오디오 모델인 표 II의 AudioLDM-M(FAD 4.53)과 비교했을 때, 전체 규모의 학습 데이터를 포함하는 AudioLDM 2는 상당히 더 나은 성능(FAD 1.42 ~2.13)을 달성하여 AudioLDM-MB 텍스트-음악 생성에 비해 상당한 개선을 보였습니다. 이 섹션에서는 제안하는 모델을 MusicGen[34], MusicLM[8], MeLoDy[13], Mousai[93], AudioLDM[4], Riffusion[6]을 포함한 다른 텍스트-음악 생성 모델과 비교합니다. AudioLDM의 출력은 표 II와 같은 방식으로 얻습니다. MusicGen은 공식 Github 저장소³를 사용하여 재생성됩니다. 표 III에서 볼 수 있듯이, 제안하는 방법은 이러한 강력한 기준선보다 상당히 우수한 성능을 보입니다. 예를 들어, AudioLDM 2-Full은 각각 FAD, KL 및 CLAP 점수에서 MusicGen보다 36%, 11% 및 3.4% 더 우수한 성능을 보입니다. 음악 데이터로만 학습된 AudioLDM 2MSD 모델은 보다 일반적인 AudioLDM 2-Full보다 객관적인 지표에서 더 나은 성능을 달성하지 못합니다. 이 결과는 일반적인 관점에서 오디오 생성을 학습하면 특수 도메인의 성능에도 도움이 될 수 있음을 시사하며, 제안된 일반 프레임워크의 장점을 보여줍니다. 일반 https://github.com/facebookresearch/audiocraft 표 III MUSICCAPS 평가 세트의 성능 비교. 상위 스크립트 *는 공개적으로 사용 가능한 구현을 사용하여 재현된 결과를 나타냅니다. 오픈 소스 버전의 MUSICGEN-MEDIUM은 보컬 사운드를 제외하여 원래 보고서[34]에 비해 성능이 약간 떨어집니다.GT-AUDIOMAE는 섹션 III-A에 자세히 설명된 대로 오디오 생성을 위한 함수 G에 &quot;오디오 언어&quot; Y의 기준 진실을 직접 적용하는 것을 나타냅니다.모든 생성된 오디오 클립은 평가 전에 16kHz로 리샘플링되었습니다.모델 표 IV LJSPEECH 테스트에서 평가된 텍스트-음성 변환 성능 모델 GroundTruth GT-AudioMAE FastSpeechAudioLDM 2-LJS 세트. 평균 의견 점수↑ 4.63 ± 0.4.14±0.3.78 ± 0.GroundTruth GT-AudioMAE 2.Riffusion 14.Mousai 7.FAD↓ KL↓ CLAP↑ 0.0.27 0.2.06 0.1.OVL↑ REL↑ AudioLDM 2-LJS-Pretrained 3.65 ± 0.4.00 ± 0.3.3.4.3.텍스트: 온도를 높일 수 있어요. MeLoDy 5.MusicLM 4.MusicGen-Medium 3.1.0.MusicGen-Medium* 0 0.15 0.3 0.45 0.6 0.75 0.9 1.1 1.2 1.Time0.0.3 0.0.6 0.75 0.Time 1.1.4.1.0.3.3.AudioLDM-M* 3.1.0.3.3.Text: 그린이 그녀의 이야기에서 편리하게 빠뜨린 것은 그녀의 식사 전 키트 요리 경험 수준입니다.AudioLDM 2-MSD 4.1.0.3.3.AudioLDM 2-Full 3.1.0.3.3.TimeTime 모델 AudioLDM 2-Full은 다른 시스템보다 상당히 높은 3.REL 점수를 달성하여 텍스트 이해 능력이 더 우수함을 나타냅니다. AudioLDM-M 모델은 나머지 시스템보다 상당히 높은 CLAP 점수를 달성하는데, 이는 훈련 중에 동일한 CLAP 모델에 의해 직접적으로 조건화되었기 때문일 수 있습니다. 따라서 표 III의 AudioLDM-M의 CLAP 점수 값은 참조용으로만 제공되며 주관적 평가 점수에서도 알 수 있듯이 모델의 실제 성능을 반영하지 않을 수 있습니다. AudioLDM의 높은 성능은 음악과 음향 효과를 포함하는 오디오 훈련 데이터의 다양성에서 비롯될 수도 있으며, 이는 범용 모델을 훈련하는 이점을 더욱 뒷받침합니다. 그러나 표 III의 주관적 평가는 AudioLDM-M의 주관적 성능이 객관적 지표에서 제시된 것만큼 좋지 않음을 나타냅니다. MeLoDy와 MusicLM은 오픈 소스가 아니기 때문에 일부 객관적 및 주관적 지표 점수를 비교할 수 없습니다. 객관적 점수가 상당히 낮기 때문에 Riffusion과 Mousai는 다른 기준 모델에 대한 주관적 평가에 포함되지 않습니다. C. 텍스트-음성 생성 제안된 모델을 LJSpeech 테스트 세트에서 널리 채택된 FastSpeech29 모델과 비교합니다. 시스템의 상한을 연구하기 위해 오디오 생성을 위해 함수 G에 기준 진실 LOA Y를 활용하는 GTAudioMAE라는 설정을 추가합니다. 제안된 AudioLDM 2-LJS는 LJSpeech 훈련 분할에서 훈련됩니다. 시스템의 잠재력을 더 탐색하기 위해 LJSpeech에서 미세 조정하기 전에 GigaSpeech 데이터 세트에서 함수 M에서 GPT2 모델을 사전 훈련합니다. 이 버전은 AudioLDM 2-LJS-Pretrained로 표시됩니다. 표 IV에서 볼 수 있듯이 사전 훈련된 GPT-2 모델을 사용하면 AudioLDM 2-LJS-Pretrained가 MOS 4.00을 달성하여 FastSpeech2보다 상당히 우수한 성능을 보입니다. 주관적 평가 &quot;https://huggingface.co/facebook/fastspeech2-en-1jspeech 화자 프롬프트: 어린 소녀가 말하고 있습니다 화자 프롬프트: 어린 남성 리포터가 말하고 있습니다 그림 6. 화자 프롬프트 텍스트 음성 변환 생성의 예. 우리는 화자 프롬프트를 사용하여 화자의 특성을 설명하고 모델에 텍스트 필사본을 제공합니다. AudioLDM 2-LJS-Pretrained는 감정, 구두점 및 톤에서 더 큰 변동을 보입니다. 이는 더 작은 코퍼스에서 미세 조정하기 전에 GigaSpeech와 같은 다양한 데이터 세트에서 사전 학습하는 것의 이점을 보여줍니다. 사전 학습 없이도 제안된 모델은 여전히 3.65의 경쟁력 있는 MOS(평균 의견 점수)를 달성하는데, 이는 기준 FastSpeech2의 3.MOS와 비슷합니다. D. 절제 연구 설정 표 V AUDIOCAPS 데이터 세트에 대한 절제 연구. FAD↓↓ KL↓ CLAP↑ AudioLDM1.67 1.0.a. 조인트 파인튜닝 없음 2.1.0.b. CLAP 임베딩 없음(GPT) 2.1.0.C. FLAN-T5 임베딩 없음(GPT) 2.1.0.d. FLAN-T5 크로스어택 없음(T-UNet) 1.1.0.e. CLAP 및 FLAN-T5 없음(GPT) 2.1.0.AudioLDM 2의 설계 선택을 검증하기 위해 AudioCaps 데이터 세트에서 텍스트-오디오 생성 작업에 대한 일련의 절제 연구를 수행했습니다.결과는 표 V에 나와 있습니다.GPT-2 모델과 잠재 확산 모델 간의 조인트 파인튜닝 프로세스를 비활성화하여(a) 별도로만 최적화했을 때 세 가지 평가 지표 모두 현저한 저하를 보였으며, 조인트 파인튜닝이 GPT-2 모델이 LDM 모델과 더 잘 협력하는 데 도움이 된다는 것을 시사합니다. GPT-2 모델은 텍스트-오디오 생성을 위해 CLAP 및 FLAN-T5 모듈 모두에서 입력을 받습니다. 두 모듈을 제거하면 평가 지표가 저하됩니다(bc). 그러나 CLAP 모듈만 입력으로 사용했을 때 CLAP 점수가 향상되었습니다(c). 이러한 향상은 평가 지표와 직접 일치하는 조절 때문일 가능성이 큽니다. FLAN-T5 임베딩을 허용하는 T-UNet 모델(d)에서 교차 주의 메커니즘을 제거하면 KL 발산과 CLAP 점수가 모두 크게 저하되었습니다. 그러나 FAD 점수는 1.67에서 1.38로 향상되었습니다. 이러한 결과는 AudioMAE 조절만으로 더 나은 FAD를 얻을 수 있지만 FLAN-T5 조절을 사용하면 오디오 및 텍스트 관계의 학습을 돕는 추가 언어 의미 정보를 제공한다는 것을 나타냅니다. 또한 CLAP 및 FLAN-T5 표현을 모두 제거하고 텍스트를 GPT-2 모델의 입력으로 직접 사용하여 LOA를 예측하는 효과를 연구합니다(e). 실험 결과는 이 설정에서 우리 모델이 FAD 2.11과 KL 1.06으로 경쟁력 있는 성능을 유지함을 보여줍니다. 그러나 CLAP 점수는 눈에 띄는 저하를 보이며, 이는 CLAP 및 FLAN-T5 표현이 텍스트와 생성된 오디오 간의 관계를 잠재적으로 개선할 수 있음을 나타냅니다. VI. 결론 및 향후 연구 이 논문에서는 오디오 생성을 위한 AudioLDM 2를 제시하여 텍스트-오디오, 텍스트-음악, 텍스트-음성 생성 작업에서 최첨단 또는 비교 성능을 달성했습니다. 보편적인 오디오 표현인 오디오 언어(LOA)는 잠재 확산 모델의 자체 감독 사전 학습을 가능하게 하여 오디오 생성 작업을 위한 견고한 기반을 제공합니다. 우리는 오디오 맥락 내 학습을 수행하여 제안하는 방법의 다재다능함을 더욱 입증합니다. AudioLDM 2는 통합된 관점에서 오디오 생성에 대한 향후 작업에 대한 문을 엽니다. 향후 작업에는 GPT-2 모델의 멀티태스킹 학습을 활성화하여 단일 모델로 오디오, 음악 및 음성을 동시에 생성하는 작업이 포함됩니다. 또한 HuBERT[61] 및 wav2vec[56]와 같은 다른 오디오 자체 감독 모델을 시스템에 통합하여 &quot;오디오 언어&quot;에 대한 보다 효과적인 표현을 조사할 계획입니다.감사의 말 이 연구는 부분적으로 British Broadcasting Corporation Research and Development(BBC R&amp;D), Engineering and Physical Sciences Research Council(EPSRC) Grant EP/T019751/1 &quot;AI for Sound&quot; 및 University of Surrey의 공학 및 물리 과학부(FEPS)의 Centre for Vision, Speech and Signal Processing(CVSSP)의 박사 장학금의 지원을 받았습니다. 오픈 액세스를 목적으로 저자는 발생하는 모든 Author Accepted Manuscript 버전에 Creative Commons Attribution(CC BY) 라이선스를 적용했습니다.저자는 이 작업을 더욱 개선하기 위해 도움이 되는 의견을 주신 부편집자와 심사자에게 감사드립니다.참고문헌 [1] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, PS Yu 및 L. Sun, &quot;AI 생성 콘텐츠에 대한 포괄적 조사: GAN에서 ChatGPT까지 생성 AI의 역사,&quot; arXiv 사전 인쇄: 2303.04226, 2023. [2] X. Tan, J. Chen, H. Liu, J. Cong, C. Zhang, Y. Liu, X. Wang, Y. Leng, Y. Yi, L. He et al., &quot;NaturalSpeech: 인간 수준의 품질을 갖춘 종단 간 텍스트-음성 합성,&quot; arXiv 사전 인쇄: 2205.04421, 2022. [3] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman 및 Y. Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성,&quot; International Conference on Learning Representations, 2022. [4] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; 국제 기계 학습 컨퍼런스, 2023. [5] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, X. Qiu, &quot;SpeechGPT: 본질적인 크로스모달 대화 능력을 갖춘 대규모 언어 모델 강화,&quot; arXiv 사전 인쇄:2305.11000, 2023. [6] S. Forsgren 및 H. Martiros, &quot;Riffusion: 실시간 음악 생성을 위한 안정적 확산, 2022,&quot; URL https://riffusion.com/about, vol. 6, 2022. [7] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, MD Plumbley 외, &quot;WavJourney: 대규모 언어 모델을 사용한 작곡 오디오 생성&quot;, arXiv 사전 인쇄:2307.14335, 2023. [8] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi 외, &quot;MusicLM: 텍스트에서 음악 생성&quot;, arXiv 사전 인쇄:2301.11325, 2023. [9] R. Bresin, A. de Witt, S. Papetti, M. Civolani 및 F. Fontana, &quot;표현적인 음향화 발자국 소리,” Interactive Sonification Workshop 회의록, 2010. [10] J. Engel, L. Hantrakul, C. Gu, 및 A. Roberts, “DDSP: 미분 가능 디지털 신호 처리,” International Conference on Learning Representations, 2020. [11] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, 및 T. Liu, “Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트 음성 변환,” International Conference on Learning Representations, 2021. [12] D. Herremans 및 E. Chew, “MorpheuS: 반복적 패턴 제약 및 긴장 프로필을 사용한 자동 음악 생성,” IEEE TENCON 회의록, 2016, 282-285쪽. [13] MW Lam, Q. Tian, T. Li, Z. Yin, S. Feng, M. Tu, Y. Ji, R. Xia, M. Ma, X. Song 외, &quot;Efficient Neural Music Generation,&quot; arXiv 사전 인쇄:2305.15719, 2023. [14] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou 및 D. Yu, &quot;Diffsound: 텍스트-사운드 생성을 위한 이산 확산 모델&quot;, 오디오, 음성 및 언어 처리에 대한 IEEE/ACM 트랜잭션, vol. 31, pp. 1720-1733, 2023. [15] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin 및 Z. 자오, “Make-An-Audio: 프롬프트 강화 확산 모델을 사용한 텍스트-오디오 생성,&quot; 국제기계학습회의, 2023. [16] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, &quot;VoiceFixer: 신경 보코더를 사용한 일반 음성 복원을 향해,&quot; arXiv 사전 인쇄:2109.13731, 2021. [17] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, &quot;Data2Vec: 음성, 시각 및 언어에서 자기 감독 학습을 위한 일반 프레임워크,&quot; 국제기계학습회의, 2022, pp. 1298-1312. [18] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, KV Alwala, A. Joulin, I. Misra, &quot;ImageBind: 모든 것을 묶을 수 있는 하나의 임베딩 공간&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023, pp. 15 180–15 190. [19] Q. Kong, K. Chen, H. Liu, X. Du, T. Berg-Kirkpatrick, S. Dubnov, MD Plumbley, &quot;약하게 레이블이 지정된 데이터를 사용한 범용 소스 분리&quot;, arXiv 사전 인쇄:2305.07447, 2023. [20] Y. Okamoto, K. Imoto, S. Takamichi, R. Yamanishi, T. Fukumori, Y. Yamashita 등, &quot;Onoma-to-wave: 의성어에서 환경음 합성,&quot; 신호 및 정보 처리 거래, 제11권, 제1호, 2022년. [21] H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer 외, &quot;듣는 가면 자동 인코더,&quot; 신경 정보 처리 시스템의 발전, 2022년. [22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, &quot;언어 모델은 비지도 멀티태스크 학습기,&quot; 2019년. [23] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022, 10684-10695쪽. [24] WX Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., &quot;대규모 언어 모델 조사&quot;, arXiv 사전 인쇄:2303.18223, 2023. [25] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund 및 M. Tagliasacchi, &quot;SoundStream: 엔드투엔드 신경 오디오 코덱,&quot; 오디오, 음성 및 언어 처리에 대한 IEEE/ACM 트랜잭션, vol. 30, pp. 495–507, 2021.[26] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. 그랜지어, M. Tagliasacchi 외, “AudiOLM: 오디오 생성을 위한 언어 모델링 접근법,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, 제 13권, 1호, 236-244쪽. 42, pp. 2523-2544, 2023. [27] CD Kim, B. Kim, H. Lee, and G. Kim, &quot;AudioCaps: Generating captions for audios in the wild,&quot; 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies의 회의록, 2019, pp. 119–132. [28] K. Ito and L. Johnson, &quot;The LJSpeech dataset,&quot; https://keithito.com/LJ-Speech-Dataset/, 2017. [29] R. Sheffer and Y. Adi, &quot;I hear your true colors: Image Guided audio generation,&quot; IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [30] V. Iashin and E. Rahtu, &quot;Taming visual guides sound generation,&quot; British Machine Vision Conference에서, 2021. [31] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova 및 M. Kudinov, &quot;GradTTS: 텍스트 음성 변환을 위한 확산 확률 모델&quot;, 기계 학습 국제 컨퍼런스, 2021, 8599-8608쪽. [32] J. Kim, S. Kim, J. Kong, and S. Yoon, “Glow-TTS: 단조 정렬 검색을 통한 텍스트 음성 변환을 위한 생성 흐름,&quot; 신경 정보 처리 시스템의 발전, pp. 8067-8077, 2020. [33] Q. Huang, DS Park, T. Wang, TI Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank et al., &quot;Noise2Music: 확산 모델을 사용한 텍스트 조건 음악 생성,&quot; arXiv 사전 인쇄:2302.03917, 2023. [34] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv 사전 인쇄:2306.05284, 2023. [35] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, Y. Wu, &quot;W2V-Bert: 자기 감독 음성 사전 학습을 위한 대조 학습 및 마스크 언어 모델링 결합&quot;, IEEE 자동 음성 인식 및 이해 워크숍. IEEE, 2021, 244-250쪽. [36] J. Ho, A. Jain 및 P. Abbeel, &quot;Denomiising diffusion probabilistic models&quot;, 신경 정보 처리 시스템의 발전, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan 및 H. Lin, 편집, vol. 33. Curran Associates, Inc., 2020, pp. 6840–6851. [37] Y. Song, J. Sohl-Dickstein, D. Kingma, A. Kumar, S. Ermon 및 B. Poole, &quot;확산 기반 생성 모델링을 통한 확률적 미분 방정식&quot;, 국제 학습 표현 컨퍼런스, 2021. [38] P. Dhariwal 및 A. Nichol, &quot;확산 모델은 이미지 합성에서 GANS를 이긴다&quot;, 신경 정보 처리 시스템의 발전, 2021. [39] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, &quot;CLIP 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성,&quot; arXiv 사전 인쇄:2204.06125, 2022. [40] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, SKS Ghasemipour, BK Ayan, SS Mahdavi, RG Lopes, T. Salimans, J. Ho, DJ Fleet, and M. Norouzi, &quot;심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델,&quot; arXiv 사전 인쇄:2205.11487, 2022. [41] C. Saharia, J. Ho, W. Chan, T. Salimans, DJ Fleet, and M. Norouzi, &quot;반복적 정제를 통한 이미지 초고해상도,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, 아니요. 4, pp. 47134726, 2022. [42] N. Chen, Y. Zhang, H. Zen, R. Weiss, M. Norouzi, W. Chan, &quot;WaveGrad: 파형 생성을 위한 기울기 추정&quot;, International Conference on Learning Representations, 2021. [43] Z. Kong, W. Ping, J. Huang, K. Zhao, B. Catanzaro, &quot;DiffWave: 오디오 합성을 위한 다재다능한 확산 모델&quot;, International Conference on Learning Representations, 2021. [44] Y. Leng, Z. Chen, J. Guo, H. Liu, J. Chen, X. Tan, D. Mandic, L. He, X.-Y. Li, T. Qin et al., &quot;BinauralGrad: 이중 청각 오디오 합성을 위한 2단계 조건부 확산 확률 모델&quot;, Advances in Neural Information Processing Systems, 2022. [45] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni et al., &quot;Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성,&quot; 국제 학습 표현 컨퍼런스, 2022. [46] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, DP Kingma, B. Poole, M. Norouzi, DJ Fleet, T. Salimans, &quot;Imagen video: 확산 모델을 사용한 고화질 비디오 생성,&quot; arXiv 사전 인쇄:2210.02303, 2022. [47] Z. Chen, Y. Wu, Y. Leng, J. Chen, H. Liu, X. Tan, Y. Cui, K. Wang, L. He, S. Zhao, J. Bian 및 D. Mandic, &quot;ResGrad: 텍스트 음성 변환을 위한 잔여 노이즈 제거 확산 확률적 모델&quot;, arXiv 사전 인쇄: 2212.14518, 2022. [48] M. Lam, J. Wang, R. Huang, D. Su 및 D. Yu, &quot;양측 노이즈 제거 확산 모델&quot;, 국제 학습 표현 컨퍼런스, 2022. [49] S. Lee, H. Kim, C. Shin, X. Tan, C. Liu, Q. Meng, T. Qin, W. Chen, S. Yoon 및 T. Liu, &quot;Priorgrad: 데이터 기반 적응 사전을 사용하여 조건부 노이즈 제거 확산 모델 개선&quot;, 국제 학습 표현 컨퍼런스, 2022. [50] Z. Chen, X. Tan, K. Wang, S. Pan, D. Mandic, L. He 및 S. Zhao, &quot;Infergrad: 훈련에서 추론을 고려하여 보코더의 확산 모델 개선&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2022. [51] D. Ghosal, N. Majumder, A. Mehrish, S. Poria, &quot;명령어 조정 LLM 및 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, arXiv 사전 인쇄:2304.13731, 2023. [52] X. Liu, H. Liu, Q. Kong, X. Mei, MD Plumbley, W. Wang, &quot;효율적인 오디오 분류를 위한 간단한 풀링 프런트 엔드&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2023. [53] H. Liu, X. Liu, Q. Kong, W. Wang, MD Plumbley, &quot;오디오 분류를 위한 스펙트로그램 시간적 해상도 학습&quot;, arXiv 사전 인쇄:2210.01719, 2022. [54] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, K. Kavukcuoglu, &quot;WaveNet: 원시 오디오를 위한 생성 모델&quot;, ISCA 음성 합성 워크숍, 2016, 125-125쪽. [55] X. Tan, T. Qin, J. Bian, T.-Y. Liu 및 Y. Bengio, &quot;재생 학습: 데이터 생성을 위한 학습 패러다임,&quot; arXiv 사전 인쇄:2301.08846, 2023. [56] A. Baevski, Y. Zhou, A. Mohamed 및 M. Auli, &quot;wav2vec 2.0: 음성 표현의 자기 감독 학습을 위한 프레임워크,&quot; 신경 정보 처리 시스템의 발전, vol. 33, pp. 12449–12460, 2020. [57] DP Kingma 및 M. Welling, &quot;자동 인코딩 변분 베이즈,&quot; 학습 표현 국제 컨퍼런스, 2014. [58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., &quot;이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기&quot;, 국제 학습 표현 컨퍼런스, 2020. [59] JF Gemmeke, DP Ellis, D. Freedman, A. Jansen, W. Lawrence, RC Moore, M. Plakal, M. Ritter, &quot;AudioSet: 오디오 이벤트를 위한 온톨로지 및 인간 레이블이 지정된 데이터 세트&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2017, 776-780쪽. [60] Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Lin, A. Ragni, E. Benetos, N. Gyenge et al., “MERT: 대규모 자기 감독 학습을 통한 어쿠스틱 음악 이해 모델,&quot; arXiv 사전 인쇄:2306.00107, 2023. [61] W.-N. Hsu, B. Bolte, Y.-HH Tsai, K. Lakhotia, R. Salakhutdinov, A. Mohamed, &quot;HUBERT: 숨겨진 단위의 마스크 예측을 통한 자기 감독 음성 표현 학습,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, 제29권, 3451-3460쪽, 2021. [62] S. Schneider, A. Baevski, R. Collobert, M. Auli, 영어: &quot;Wav2Vec: 음성 인식을 위한 비지도 사전 학습,&quot; INTERSPEECH, pp. 3465-3469, 2019. [63] D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino, &quot;Byol for audio: 범용 오디오 표현을 위한 자기 지도 학습,&quot; 국제 신경망 공동 컨퍼런스, 2021. [64] J. Kong, J. Kim, and J. Bae, &quot;HiFi-GAN: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대 네트워크,&quot; 신경 정보 처리 시스템의 발전, 제 23권, 2호, 3465-3469쪽, 2019. 33, pp. 17022-17033, 2020. [65] KJ Piczak, “ESC: 환경 소음 분류를 위한 데이터 세트,&quot; ACM 국제 멀티미디어 컨퍼런스 회의록, 2015, pp. 1015–1018. [66] L. Van der Maaten 및 G. Hinton, &quot;t-SNE를 사용하여 데이터 시각화.&quot; Journal of Machine Learning Research, vol. 9, 2008. [67] Y. Qu, P. Liu, W. Song, L. Liu 및 M. Cheng, &quot;텍스트 생성 및 예측 시스템: BERT 및 GPT2를 사용하여 새 코퍼스에 대한 사전 학습,&quot; IEEE 국제 전자 정보 및 비상 통신 컨퍼런스, 2020, pp. 323-326. [68] T. Klein 및 M. Nabi, &quot;질문하는 법을 배우면서 대답하는 법 배우기: 최상의 방법 얻기 영어: GPT-2 및 BERT 세계의,&quot; arXiv 사전 인쇄: 1911.02365, 2019. [69] AM Lamb, AG ALIAS PARTH GOYAL, Y. Zhang, S. Zhang, AC Courville 및 Y. Bengio, &quot;강제 교수: 순환 네트워크를 훈련하기 위한 새로운 알고리즘,&quot; 신경 정보 처리 시스템의 발전, vol. 29, 2016. [70] S. Masoudnia 및 R. Ebrahimpour, &quot;전문가의 혼합: 문헌 조사,&quot; 인공 지능 리뷰, vol. 42, pp. 275-293, 2014. [71] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, 및 S. Dubnov, &quot;기능 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, 2023. [72] H.-H. Wu, O. Nieto, JP Bello, and J. Salomon, &quot;Audio-text models do not yet utilize natural language,&quot; in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [73] HW Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma et al., &quot;Scaling instruction-finetuned language models,&quot; arXiv preprint:2210.11416, 2022. [74] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and PJ Liu, &quot;Exploring the limits of transfer learning with a integrated text-to-text transformer,&quot; The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [75] X. Tan, 신경 텍스트-음성 합성, ser. 인공지능: 기초, 이론 및 알고리즘.Springer Singapore, 2023. [76] DP Kingma 및 M. Welling, &quot;변분 베이즈 자동 인코딩,&quot; arXiv 사전 인쇄: 1312.6114, 2013. [77] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser 및 I. Polosukhin, &quot;주의만 있으면 됩니다.&quot; 신경 정보 처리 시스템의 발전, 제30권, 2017년. [78] J. Ho 및 T. Salimans, &quot;분류자 없는 확산 안내&quot; NeurIPS 심층 생성 모델 및 다운스트림 애플리케이션 워크숍, 2021년. [79] AQ Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever 및 M. Chen, &quot;GLIDE: 텍스트 안내 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해&quot; 기계 학습 국제 컨퍼런스, 2022년, 16784-16804쪽. [80] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, MD Plumbley, Y. Zou, W. Wang, &quot;WavCaps: 오디오-언어 멀티모달 연구를 위한 ChatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트,&quot; arXiv 사전 인쇄:2303.17395, 2023. [81] H. Chen, W. Xie, A. Vedaldi, A. Zisserman, &quot;VGGSound: 대규모 오디오-비주얼 데이터 세트,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2020, 721-725쪽. [82] M. Defferrard, K. Benzi, P. Vandergheynst, X. Bresson, &quot;FMA: 음악 분석을 위한 데이터 세트,&quot; 국제 음악 정보 검색 컨퍼런스, 2017년. [83] T. Bertin-Mahieux, DP Ellis, B. Whitman 및 P. Lamere, &quot;백만 곡 데이터 세트,&quot; 국제 음악 정보 검색 컨퍼런스, 591-596쪽, 2011. [84] G. Chen, S. Chai, G. Wang, J. Du, WQ Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang 등, &quot;GigaSpeech: 10,000시간 분량의 필사 오디오가 포함된 진화하는 다중 도메인 ASR 코퍼스,&quot; INTERSPEECH, 2021, 4376-4380쪽. [85] S. Doh, M. Won, K. Choi, J. Nam, &quot;보편적인 텍스트-음악 검색을 향하여,&quot; arXiv 사전 인쇄:2211.14558, 2022. [86] S. Hershey, S. Chaudhuri, DP Ellis, JF Gemmeke, A. Jansen, RC Moore, M. Plakal, D. Platt, RA Saurous, B. Seybold et al., &quot;대규모 오디오 분류를 위한 CNN 아키텍처,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2017, pp. 131-135. [87] K. Koutini, J. Schlüter, H. Eghbal-Zadeh, G. Widmer, &quot;패치아웃을 사용한 오디오 변압기의 효율적인 학습,&quot; INTERSPEECH, pp. 2753-2757, 2021. [88] R. Likert, &quot;태도 측정 기술.&quot; Archives of Psychology, 1932. [89] Z. Chen, N. Kanda, J. Wu, Y. Wu, X. Wang, T. Yoshioka, J. Li, S. Sivasankaran, SE Eskimez, &quot;대규모 자기 감독 학습을 통한 음성 분리&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2023. [90] J. Song, C. Meng, S. Ermon, &quot;잡음 제거 확산 암묵적 모델&quot;, 국제 학습 표현 컨퍼런스, 2020. [91] I. Loshchilov 및 F. Hutter, &quot;분리된 가중치 감소 정규화&quot;, 국제 학습 표현 컨퍼런스, 2019. [92] J. Huang, Y. Ren, R. Huang, D. Yang, Z. Ye, C. Zhang, J. Liu, X. Yin, Z. Ma, Z. Zhao, “Make-An-Audio 2: 시간 강화 텍스트-오디오 생성,&quot; arXiv 사전 인쇄:2305.18474, 2023. [93] F. Schneider, Z. Jin 및 B. Schölkopf, &quot;Mousai: 장맥락 잠복 확산을 통한 텍스트-음악 생성,&quot; arXiv 사전 인쇄:2301.11757, 2023.
