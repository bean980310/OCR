--- ABSTRACT ---
이 연구 논문은 주어진 텍스트 프롬프트에서 이미지와 깊이 맵 데이터를 모두 생성하는 3D용 잠재 확산 모델(LDM3D)을 제안하여 사용자가 텍스트 프롬프트에서 RGBD 이미지를 생성할 수 있도록 합니다. LDM3D 모델은 RGB 이미지, 깊이 맵 및 캡션을 포함하는 튜플 데이터 세트에서 미세 조정되고 광범위한 실험을 통해 검증되었습니다. 또한 TouchDesigner를 사용하여 생성된 RGB 이미지와 깊이 맵을 사용하여 몰입적이고 대화형 360도 뷰 경험을 만드는 DepthFusion이라는 애플리케이션을 개발합니다. 이 기술은 엔터테인먼트와 게임부터 건축과 디자인에 이르기까지 광범위한 산업을 혁신할 잠재력이 있습니다. 전반적으로 이 논문은 생성 AI와 컴퓨터 비전 분야에 상당한 기여를 하며 LDM3D와 DepthFusion이 콘텐츠 제작과 디지털 경험에 혁명을 일으킬 잠재력을 보여줍니다. 이 접근 방식을 요약한 짧은 비디오는 https://t.ly/tdi2에서 찾을 수 있습니다. 1.
--- INTRODUCTION ---
컴퓨터 비전 분야는 최근 몇 년 동안 상당한 진전을 이루었으며, 특히 생성 AI 분야에서 그러했습니다. 이미지 생성 분야에서 Stable Diffusion은 텍스트 프롬프트에서 임의의 고화질 RGB 이미지를 생성하는 오픈 소프트웨어를 제공함으로써 콘텐츠 생성에 혁명을 일으켰습니다. 이 작업은 Stable Diffusion[20] v1.4를 기반으로 하며 3D를 위한 잠재 확산 모델(LDM3D)을 제안합니다. 원래 모델과 달리 LDM3D는 그림 1에서 볼 수 있듯이 주어진 텍스트 프롬프트에서 이미지와 깊이 맵 데이터를 모두 생성할 수 있습니다. 사용자는 텍스트 프롬프트의 완전한 RGBD 표현을 생성하여 생생하고 몰입감 넘치는 360° 뷰에서 생생하게 표현할 수 있습니다. LDM3D 모델은 RGB 이미지, 깊이 맵 및 캡션이 포함된 튜플 데이터 세트에서 미세 조정되었습니다. 이 데이터 세트는 4억 개가 넘는 이미지-캡션 쌍을 포함하는 대규모 이미지-캡션 데이터 세트인 LAION-400M 데이터 세트의 하위 세트에서 구성되었습니다. 미세 조정에 사용된 깊이 맵은 DPT-Large 깊이 추정 모델[18, 19]에 의해 생성되었으며, 이는 이미지의 각 픽셀에 대해 매우 정확한 상대적 깊이 추정치를 제공합니다. 정확한 깊이 맵을 사용하는 것은 사용자가 텍스트 프롬프트를 생생한 디테일로 경험할 수 있도록 하는 사실적이고 몰입감 있는 360° 뷰를 생성하는 데 매우 중요했습니다. LDM3D의 잠재력을 보여주기 위해 TouchDesigner[1]를 사용하여 생성된 2D RGB 이미지와 깊이 맵을 사용하여 360° 투영을 계산하는 애플리케이션인 DepthFusion을 개발했습니다. TouchDesigner는 몰입적이고 대화형 멀티미디어 경험을 만들 수 있는 다재다능한 플랫폼입니다. 이 애플리케이션은 TouchDesigner의 힘을 활용하여 텍스트 프롬프트를 생생한 디테일로 생생하게 표현하는 독특하고 매력적인 360° 뷰를 만듭니다. DepthFusion은 디지털 콘텐츠를 경험하는 방식에 혁명을 일으킬 잠재력이 있습니다. 고요한 숲, 번화한 도시 풍경 또는 미래 지향적인 공상과학 &quot;책이 있는 테이블&quot;에 대한 설명이든, 고정된 텍스트 E Concat RGBD KL-E Diffusion U-Net KL-D RGBD 추론 그림 1. LDM3D 개요. 학습 파이프라인 설명: 16비트 회색조 깊이 맵은 3채널 RGB 유사 깊이 이미지로 패킹된 다음 채널 차원을 따라 RGB 이미지와 연결됩니다. 이 연결된 RGBD 입력은 수정된 KL-AE를 거쳐 잠재 공간에 매핑됩니다. 잠재 표현에 노이즈가 추가된 다음 U-Net 모델에서 반복적으로 노이즈가 제거됩니다. 텍스트 프롬프트는 고정된 CLIP-텍스트 인코더를 사용하여 인코딩되고 교차 주의를 사용하여 U-Net의 다양한 계층에 매핑됩니다. 잠재 공간의 노이즈가 제거된 출력은 KL 디코더에 공급되고 6채널 RGBD 출력으로 픽셀 공간에 다시 매핑됩니다. 마지막으로 출력은 RGB 이미지와 16비트 회색조 깊이 맵으로 분리됩니다.파란색 프레임: 텍스트-이미지 추론 파이프라인.64x64x4차원 잠재 공간에서 가우시안 분포 노이즈 샘플에서 시작합니다.텍스트 프롬프트가 주어지면 이 파이프라인은 RGB 이미지와 해당 깊이 맵을 생성합니다.세계에서 DepthFusion은 사용자가 이전에는 불가능했던 방식으로 텍스트 프롬프트를 경험할 수 있도록 하는 몰입적이고 매력적인 360도 뷰를 생성할 수 있습니다.이 기술은 엔터테인먼트와 게임부터 건축과 디자인에 이르기까지 광범위한 산업을 혁신할 잠재력이 있습니다.요약하면, 우리의 기여는 세 가지입니다.(1) 텍스트 프롬프트가 주어지면 RGBD 이미지(해당 깊이 맵이 있는 RGB 이미지)를 출력하는 새로운 확산 모델인 LDM3D를 제안합니다.(2) LDM3D로 생성된 RGBD 이미지를 기반으로 몰입형 360도 뷰 경험을 만드는 애플리케이션인 DepthFusion을 개발합니다. (3) 광범위한 실험을 통해 생성된 RGBD 이미지와 360도 시야 몰입형 비디오의 품질을 검증합니다. 2.
--- RELATED WORK ---
단안 깊이 추정은 주어진 단일 RGB 이미지의 각 픽셀에 대한 깊이 값을 추정하는 작업입니다. 최근 연구에서는 합성곱 신경망을 기반으로 한 딥 러닝 모델을 사용하여 깊이 추정에서 뛰어난 성능을 보였습니다[11, 12, 14, 22, 28, 29]. 나중에 주의 기반 Transformer 모델이 CNN의 제한된 수용 영역 문제를 극복하기 위해 채택되어 모델이 깊이 값을 예측할 때 전역 컨텍스트를 고려할 수 있게 되었습니다[3,5, 19, 30]. 가장 최근에는 확산 모델도 깊이 추정에 적용되어 이러한 혁신적인 생성 기능을 활용했습니다.
--- EXPERIMENT ---
s. 또한 생성된 RGB 이미지와 깊이 맵을 사용하여 TouchDesigner를 사용하여 몰입적이고 대화형 360도 뷰 경험을 만드는 DepthFusion이라는 애플리케이션을 개발합니다. 이 기술은 엔터테인먼트와 게임부터 건축과 디자인에 이르기까지 광범위한 산업을 혁신할 잠재력이 있습니다. 전반적으로 이 논문은 생성 AI와 컴퓨터 비전 분야에 상당한 기여를 하며 LDM3D와 DepthFusion이 콘텐츠 생성과 디지털 경험을 혁신할 잠재력을 보여줍니다. 이 접근 방식을 요약한 짧은 비디오는 https://t.ly/tdi2에서 찾을 수 있습니다. 1. 서론 컴퓨터 비전 분야는 최근 몇 년 동안 상당한 발전을 이루었으며 특히 생성 AI 분야에서 그러했습니다. 이미지 생성 분야에서 Stable Diffusion은 텍스트 프롬프트에서 임의의 고화질 RGB 이미지를 생성하는 오픈 소프트웨어를 제공함으로써 콘텐츠 생성에 혁명을 일으켰습니다. 이 작업은 Stable Diffusion[20] v1.4를 기반으로 하며 3D를 위한 잠재 확산 모델(LDM3D)을 제안합니다. 원래 모델과 달리 LDM3D는 그림 1에서 볼 수 있듯이 주어진 텍스트 프롬프트에서 이미지와 깊이 맵 데이터를 모두 생성할 수 있습니다.사용자는 텍스트 프롬프트의 완전한 RGBD 표현을 생성하여 생생하고 몰입감 넘치는 360° 뷰로 생생하게 표현할 수 있습니다.LDM3D 모델은 RGB 이미지, 깊이 맵 및 캡션이 포함된 튜플 데이터 세트에서 미세 조정되었습니다.이 데이터 세트는 4억 개가 넘는 이미지-캡션 쌍이 포함된 대규모 이미지-캡션 데이터 세트인 LAION-400M 데이터 세트의 하위 집합에서 구성되었습니다.미세 조정에 사용된 깊이 맵은 DPT-Large 깊이 추정 모델[18, 19]에 의해 생성되었으며, 이는 이미지의 각 픽셀에 대한 매우 정확한 상대적 깊이 추정치를 제공합니다.정확한 깊이 맵을 사용하면 사실적이고 몰입감 넘치는 360° 뷰를 생성하여 사용자가 텍스트 프롬프트를 생생한 디테일로 경험할 수 있도록 하는 데 중요했습니다. LDM3D의 잠재력을 보여주기 위해, 우리는 생성된 2D RGB 이미지와 깊이 맵을 사용하여 TouchDesigner[1]를 사용하여 360° 투영을 계산하는 애플리케이션인 DepthFusion을 개발했습니다.TouchDesigner는 몰입적이고 대화형 멀티미디어 경험을 만들 수 있는 다재다능한 플랫폼입니다.이 애플리케이션은 TouchDesigner의 힘을 활용하여 텍스트 프롬프트를 생생한 디테일로 구현하는 독특하고 매력적인 360° 뷰를 만듭니다.DepthFusion은 디지털 콘텐츠를 경험하는 방식을 혁신할 잠재력이 있습니다.고요한 숲, 번화한 도시 풍경 또는 미래 지향적인 공상 과학 &quot;책이 있는 테이블&quot;에 대한 설명이든 고정된 텍스트 E Concat RGBD KL-E Diffusion U-Net KL-D RGBD 추론 그림 1. LDM3D 개요.학습 파이프라인 설명: 16비트 회색조 깊이 맵은 3채널 RGB 유사 깊이 이미지로 패킹된 다음 채널 차원을 따라 RGB 이미지와 연결됩니다. 이 연결된 RGBD 입력은 수정된 KL-AE를 거쳐 잠재 공간에 매핑됩니다. 잠재 표현에 노이즈가 추가된 다음 U-Net 모델에서 반복적으로 노이즈를 제거합니다. 텍스트 프롬프트는 동결된 CLIP-텍스트 인코더를 사용하여 인코딩되고 교차 주의를 사용하여 U-Net의 다양한 계층에 매핑됩니다. 잠재 공간에서 노이즈가 제거된 출력은 KL-디코더에 공급되고 6채널 RGBD 출력으로 픽셀 공간에 다시 매핑됩니다. 마지막으로 출력은 RGB 이미지와 16비트 회색조 깊이 맵으로 분리됩니다. 파란색 프레임: 텍스트-이미지 추론 파이프라인. 64x64x4차원 잠재 공간에서 가우시안 분포 노이즈 샘플에서 시작합니다. 텍스트 프롬프트가 주어지면 이 파이프라인은 RGB 이미지와 해당 깊이 맵을 생성합니다. DepthFusion은 몰입적이고 매력적인 360° 뷰를 생성하여 사용자가 이전에는 불가능했던 방식으로 텍스트 프롬프트를 경험할 수 있도록 합니다. 이 기술은 엔터테인먼트와 게임부터 건축과 디자인에 이르기까지 광범위한 산업을 혁신할 잠재력이 있습니다. 요약하자면, 우리의 기여는 세 가지입니다. (1) 텍스트 프롬프트가 주어지면 RGBD 이미지(해당 깊이 맵이 있는 RGB 이미지)를 출력하는 새로운 확산 모델인 LDM3D를 제안합니다. (2) LDM3D로 생성된 RGBD 이미지를 기반으로 몰입형 360도 뷰 경험을 만드는 애플리케이션인 DepthFusion을 개발합니다. (3) 광범위한 실험을 통해 생성된 RGBD 이미지와 360도 뷰 몰입형 비디오의 품질을 검증합니다. 2. 관련 연구 단안 깊이 추정은 주어진 단일 RGB 이미지의 각 픽셀에 대한 깊이 값을 추정하는 작업입니다. 최근 연구에서는 합성곱 신경망[11, 12, 14, 22, 28, 29]을 기반으로 하는 딥 러닝 모델을 사용하여 깊이 추정에서 뛰어난 성능을 보였습니다. 나중에, CNN의 제한된 수용 영역 문제를 극복하기 위해 어텐션 기반 Transformer 모델이 채택되어 모델이 깊이 값을 예측할 때 전역 컨텍스트를 고려할 수 있게 되었습니다[3,5, 19, 30]. 가장 최근에는 이러한 방법의 혁신적인 생성 기능을 활용하기 위해 확산 모델도 깊이 추정에 적용되었습니다. 확산 모델은 입력 프롬프트나 조건에 따라 매우 자세한 이미지를 생성하는 데 놀라운 기능을 보여주었습니다[16,20,23]. 깊이 추정치의 사용은 이전에 깊이-이미지 생성을 수행하기 위한 추가 조건으로 확산 모델에서 사용되었습니다[31]. 나중에, [24]와 [6]은 단안 깊이 추정이 입력 조건으로 이미지를 사용하여 잡음 제거 확산 프로세스로 모델링될 수도 있음을 보여주었습니다. 이 연구에서 우리는 텍스트 프롬프트를 입력으로 주어졌을 때 RGB 이미지와 해당 깊이 맵을 동시에 생성하는 확산 모델을 제안합니다. 제안된 모델은 캐스케이드 방식의 이미지 생성 및 깊이 추정 모델과 기능적으로 비슷할 수 있지만, 제안된 결합 모델에는 여러 가지 차이점, 과제 및 이점이 있습니다. 적절한 단안 깊이 추정 모델은 방대하고 다양한 데이터를 필요로 하지만[15,19], 생성된 이미지에 대한 깊이 기준 진실이 없기 때문에 기성형 깊이 추정 모델이 확산 모델의 출력에 적응하기 어렵습니다. 공동 학습을 통해 깊이 생성이 이미지 생성 프로세스에 훨씬 더 많이 주입되어 확산 모델이 보다 자세하고 정확한 깊이 값을 생성할 수 있습니다. 제안된 모델은 또한 참조 이미지가 이제 모델에서 생성하는 새로운 이미지이기 때문에 표준 단안 깊이 추정 작업과 다릅니다. 여러 이미지를 동시에 생성하는 유사한 작업은 확산 모델을 사용하여 비디오 생성에 연결할 수 있습니다[8, 9, 26]. 비디오 확산 모델은 대부분 [9]를 기반으로 하며, 여기서는 고정된 수의 연속 프레임 이미지를 공동으로 모델링하여 이를 사용하여 비디오를 구성하는 3D U-Net을 제안했습니다. 그러나 비디오와 동일한 공간적, 시간적 종속성을 반드시 필요로 하지 않는 두 가지 출력(깊이 및 RGB)만 필요하므로 모델에서 다른 접근 방식을 활용합니다.3. 방법론 이 섹션에서는 LDM3D 모델의 방법론, 학습 프로세스 및 동시 RGB 이미지 및 깊이 맵 생성과 LDM3D 출력을 기반으로 한 몰입형 360도 뷰 생성을 용이하게 하는 고유한 특성을 설명합니다.3.1. LDM-3D 3.1. 모델 아키텍처 LDM3D는 Stable Diffusion [20]에서 약간 수정하여 16억 개의 매개변수를 갖는 KL-정규화 확산 모델로, 텍스트 프롬프트에서 이미지와 깊이 맵을 동시에 생성할 수 있습니다(그림 1 참조).모델에서 사용된 KL-자동 인코더는 [7]을 기반으로 하는 변분 자동 인코더(VAE) 아키텍처로, KL 발산 손실 항을 통합합니다. 이 모델을 우리의 특정 요구 사항에 맞게 조정하기 위해, 우리는 KL-자동 인코더의 첫 번째와 마지막 Conv2d 레이어를 수정했습니다. 이러한 조정을 통해 모델은 연결된 RGB 이미지와 깊이 맵으로 구성된 수정된 입력 형식을 수용할 수 있었습니다. 생성 확산 모델은 주로 2D 합성곱 레이어로 구성된 U-Net 백본 [21] 아키텍처를 활용합니다. 확산 모델은 [20]과 유사하게 학습된 저차원 KL-정규화된 잠재 공간에서 학습되었습니다. 픽셀 공간에서 학습된 변환기 기반 확산 모델에 비해 더 정확한 재구성과 효율적인 고해상도 합성이 가능합니다. 텍스트 컨디셔닝을 위해 동결된 CLIP-텍스트 인코더 [17]가 사용되고 인코딩된 텍스트 프롬프트는 교차 주의를 사용하여 U-Net의 다양한 레이어에 매핑됩니다. 이 접근 방식은 복잡한 자연어 텍스트 프롬프트로 효과적으로 일반화되어 단일 패스에서 고품질 이미지와 깊이 맵을 생성하며 참조 안정적 확산 모델에 비해 9,600개의 추가 매개변수만 있습니다. 3.1.2 데이터 전처리 모델은 이미지와 캡션 쌍을 포함하는 LAION400M [25] 데이터 세트의 하위 집합에서 미세 조정되었습니다. LDM3D 모델을 미세 조정하는 데 사용된 깊이 맵은 384×384의 기본 해상도에서 추론을 실행하는 DPT-Large 깊이 추정 모델에 의해 생성되었습니다. 깊이 맵은 16비트 정수 형식으로 저장되었으며 RGB 이미지에서 사전 학습된 안정된 확산 모델의 입력 요구 사항과 더 밀접하게 일치하도록 3채널 RGB와 유사한 배열로 변환되었습니다. 이 변환을 달성하기 위해 16비트 깊이 데이터는 세 개의 별도 8비트 채널로 압축 해제되었습니다. 이러한 채널 중 하나는 16비트 깊이 데이터의 경우 0이지만 이 구조는 잠재적인 24비트 깊이 맵 입력과 호환되도록 설계되었습니다. 이 재매개변수화를 통해 완전한 깊이 범위 정보를 보존하면서 RGB와 유사한 이미지 형식으로 깊이 정보를 인코딩할 수 있었습니다. 원래 RGB 이미지와 생성된 RGB 유사 깊이 맵은 [0, 1] 범위 내의 값을 갖도록 정규화되었습니다. 자동 인코더 모델 학습에 적합한 입력을 생성하기 위해 RGB 이미지와 RGB 유사 깊이 맵이 채널 차원을 따라 연결되었습니다. 이 프로세스의 결과 512x512x6 크기의 입력 이미지가 생성되었는데, 여기서 처음 세 채널은 RGB 이미지에 해당하고 나머지 세 채널은 RGB 유사 깊이 맵을 나타냅니다. 연결된 입력을 통해 LDM3D 모델은 RGB 이미지와 깊이 맵의 공동 표현을 학습하여 일관된 RGBD 출력을 생성하는 기능을 향상할 수 있었습니다. 3.1.3 미세 조정 절차. 미세 조정 프로세스는 [20]에 제시된 기술과 유사한 두 단계로 구성됩니다. 첫 번째 단계에서는 자동 인코더를 학습하여 차원이 낮고 지각적으로 동등한 데이터 표현을 생성합니다. 그런 다음 동결 자동 인코더를 사용하여 확산 모델을 미세 조정하여 학습을 간소화하고 효율성을 높입니다. 이 방법은 고차원 데이터로 효과적으로 확장하여 복잡한 재구성 및 생성 기능의 균형 없이 더 정확한 재구성 및 효율적인 고해상도 이미지 및 깊이 합성을 제공함으로써 변압기 기반 접근 방식보다 성능이 뛰어납니다.자동 인코더 미세 조정.KL-자동 인코더는 8233개 샘플로 구성된 학습 세트와 2059개 샘플을 포함하는 검증 세트에서 미세 조정되었습니다.이러한 세트의 각 샘플에는 이전에 전처리 섹션에서 설명한 대로 캡션과 해당 이미지 및 깊이 맵 쌍이 포함되었습니다.수정된 자동 인코더의 미세 조정을 위해 픽셀 공간 이미지 해상도의 8배인 다운샘플링 계수를 가진 KL-자동 인코더 아키텍처를 사용했습니다.이 다운샘플링 계수는 빠른 학습 프로세스 및 고품질 이미지 합성 측면에서 최적인 것으로 나타났습니다[20]. 미세 조정 프로세스 동안 우리는 10-5의 학습 속도와 8의 배치 크기를 갖는 Adam 최적화 도구를 사용했습니다. 우리는 83개의 에포크 동안 모델을 훈련했고, 진행 상황을 모니터링하기 위해 각 에포크 후에 출력을 샘플링했습니다. 이미지와 깊이 데이터의 손실 함수는 원래 KL-AE[7]의 사전 훈련에 사용된 지각적 손실[32]과 패치 기반 적대적 유형 손실[10]의 조합으로 구성되었습니다. 여기서 LAutoencoder min max Lrec (x, D(E(x))) E,D ป — Ladv(D(E(x))) + log D√(x) (1) + Lreg (x; E, D) D(E(x)) Lrec(x, D(E(x)))는 재구성된 이미지, 지각적 재구성 손실, Lady (D(E(x)))는 적대적 손실, D₁(x)는 패치 기반 판별자 손실, Lreg(x; E, D)는 KL-정규화 손실입니다. 확산 모델 미세 조정 자동 인코더 미세 조정에 따라 두 번째 단계로 진행했는데, 이는 확산 모델 미세 조정을 포함합니다. 이는 동결된 자동 인코더의 잠재 표현을 입력으로 사용하여 달성되었으며 잠재 입력 크기는 64x64x4입니다. 이 단계에서는 학습률이 10-5이고 배치 크기가 32인 Adam 최적화 도구를 사용했습니다.손실 함수를 사용하여 178개 에포크 동안 확산 모델을 학습합니다.· N(0, 1), t [||€ – €o(zt,t)||2] (2) = LLDM3D Eε(x), € ~ 여기서 (zt, t)는 노이즈 제거 U-Net에서 예측한 노이즈이고 t는 균일하게 샘플링됩니다.Stable Diffusion v1.4 [20] 모델의 가중치를 시작점으로 사용하여 LDM3D 미세 조정을 시작합니다.생성된 이미지와 깊이 맵을 샘플링하고, 품질을 평가하고, 모델의 수렴을 보장하여 미세 조정 전체에서 진행 상황을 모니터링합니다.컴퓨팅 인프라이 작업에서 보고된 모든 교육 실행은 Intel Xeon 프로세서와 Intel Habana Gaudi AI 가속기로 구성된 Intel AI 슈퍼컴퓨팅 클러스터에서 수행됩니다. LDM3D 모델 학습 실행은 9,600개 튜플(텍스트 캡션, RGB 이미지, 깊이 맵)의 코퍼스에서 16개 가속기(Gaudis)로 확장됩니다. LDM3D 모델에서 사용된 KL-자동 인코더는 Nvidia A6000 GPU에서 학습되었습니다. 3.1.4 평가 이전 연구와 마찬가지로 MS-COCO[13] 검증 세트를 사용하여 텍스트-이미지 생성 성능을 평가합니다. 생성된 이미지의 품질을 측정하기 위해 Fréchet Inception Distance(FID), Inception Score(IS) 및 CLIP 유사성 메트릭을 사용합니다. 자동 인코더의 성능은 재구성된 이미지의 품질을 해당 원본 입력 이미지와 비교하는 데 널리 사용되는 메트릭인 상대적 FID 점수를 사용하여 평가합니다. 평가는 LAION-400M 데이터 세트에서 512x512 크기의 27,265개 샘플에서 수행되었습니다. 3.2. 몰입형 경험 생성 AI 이미지 생성을 위한 AI 모델은 AI 아트 분야에서 두드러지게 나타났으며, 일반적으로 확산된 콘텐츠의 2D 표현을 위해 설계되었습니다. 3D 몰입형 환경에 이미지를 투사하려면 허용 가능한 결과를 얻기 위해 매핑과 해상도를 수정해야 했습니다. 올바르게 투사된 출력의 또 다른 이전 제한은 단일 시점의 단안적 관점으로 인해 지각이 손실될 때 발생합니다. 최신 시청 장치와 기술은 입체적 몰입 경험을 얻기 위해 두 시점 간의 차이가 필요합니다. 녹화 장치는 일반적으로 고정된 거리에서 두 대의 카메라에서 영상을 캡처하여 차이와 카메라 매개변수를 기반으로 3D 출력을 생성할 수 있습니다. 그러나 단일 이미지에서 동일한 결과를 얻으려면 픽셀 공간의 오프셋을 계산해야 합니다. LDM3D 모델을 사용하면 깊이 맵이 RGB 색상 공간에서 별도로 추출되고 3D에서 동일한 이미지 공간의 적절한 &quot;왼쪽&quot; 및 &quot;오른쪽&quot; 관점을 구별하는 데 사용할 수 있습니다. 먼저 초기 이미지가 생성되고 해당 깊이 맵이 저장됩니다(그림 2a 참조). TouchDesigner [1]를 사용하여 RGB 색상 이미지를 3D 공간의 직사각형 구형 극성 객체 외부로 투사합니다(그림 2b 참조). 관점은 몰입형 공간을 보는 중심인 구형 객체 내부의 원점 0,0,0으로 설정됩니다. 구의 정점은 원점에서 모든 방향으로 동일한 거리로 정의됩니다. 그런 다음 깊이 맵을 지침으로 사용하여 모노톤 색상 값을 기반으로 원점에서 해당 정점까지의 거리를 조작합니다. 값이 1.0에 가까울수록 정점이 원점에 가까워지고 값이 0.0이면 원점에서 더 먼 거리로 조정됩니다. 값이 0.5이면 정점 조작이 수행되지 않습니다. 0,0,0의 단안 시점에서는 &quot;광선&quot;이 원점에서 바깥쪽으로 선형적으로 확장되므로 이미지에 변화가 없습니다. 그러나 입체 시점의 이중 관점을 사용하면 매핑된 RGB 이미지의 픽셀이 동적으로 왜곡되어 깊이의 환상을 줍니다. 동일한 효과는 정점 거리가 초기 계산과 동일하게 확장되므로 단일 시점을 원점 0,0,0에서 멀리 옮기는 동안에도 관찰할 수 있습니다.RGB 색상 공간과 깊이 맵 픽셀이 동일한 영역을 차지하므로 인식된 기하학적 모양이 있는 객체는 TouchDesigner 내의 렌더 엔진에서 고유한 가상 기하학적 차원을 통해 대략적인 깊이가 제공됩니다.그림 2는 전체 파이프라인을 설명합니다.이 접근 방식은 TouchDesigner 플랫폼에 국한되지 않으며 파이프라인에서 RGB 및 깊이 색상 공간을 활용할 수 있는 유사한 렌더링 엔진 및 소프트웨어 제품군 내에서도 복제될 수 있습니다.init_image 파이프라인 &quot;파란색 사무실&quot; 고정된 텍스트 E LDM3D 24비트 색상 이미지 LDM3D 연결 RGB(D) E 확산 Unet 16비트 깊이 맵 D RGBD MiDaS 깊이 맵 추론 (a) 1단계: LDM3D용 Img-to-img 추론 파이프라인. DPT-Large [18, 19]를 사용하여 계산된 파노라마 이미지와 해당 깊이 맵에서 시작합니다. RGBD 입력은 LDM3D 이미지-이미지 파이프라인을 통해 처리되어 주어진 텍스트 프롬프트에 따라 변환된 이미지와 깊이 맵이 생성됩니다.LDM3D 24비트 컬러 이미지 직사각형에서 구면으로 투영 메싱 카메라 배치 메시가 있는 원점(0,0,0)에 텍스처 처리된 구 LDM3D 16비트 깊이 맵 깊이 맵에서 정점 조작으로 메시 정제(b) 2단계: LDM3D에서 생성된 이미지는 확산된 깊이 맵을 기반으로 하는 정점 조작을 사용하여 구에 투영된 다음 메싱됩니다.카메라 이동 관점.시점은 깊이 근접성을 보여줍니다.비디오 조립(c) 3단계: 다양한 관점에서 이미지를 생성하고 비디오 조립합니다.그림 2. 몰입형 경험 생성 파이프라인.프레임 조립을 통해 동영상 파일 출력으로. RGB 이미지 SDv1.LDM3D(저희) 깊이 맵 DPT-Large 그림 3. COCO 검증 데이터 세트의 512×이미지에서 안정적 확산 v1.4 [20]에 대한 이미지와 DPT-Large [18, 19]에 대한 깊이 맵의 정성적 비교. 위에서 아래로 캡션: &quot;테이블 위에 있는 피자 한 장의 클로즈업&quot;, &quot;테이블 위에 있는 레몬 사진&quot;, &quot;머리에 분홍색 리본을 한 어린 소녀가 브로콜리를 먹고 있다&quot;, &quot;길에서 말을 타고 있는 남자&quot;, &quot;포크 옆에 있는 검은색 머핀 랩에 싸인 머핀&quot;, &quot;바위 옆에서 물가에서 물을 마시는 흰 북극곰&quot;. 4. 결과 다음에서 LDM3D 모델의 생성된 이미지와 깊이 맵의 높은 품질을 보여줍니다. 또한 깊이 모달리티를 추가할 때 자동 인코더의 성능에 미치는 영향도 보여줍니다. 4.1. 정성적 평가 생성된 이미지와 깊이 맵에 대한 정성적 분석은 LDM3D 모델이 제공된 텍스트 프롬프트에 잘 부합하는 시각적으로 일관된 출력을 효과적으로 생성할 수 있음을 보여줍니다.생성된 이미지는 미세한 디테일과 복잡한 구조를 보여주는 반면, 깊이 맵은 장면의 공간 정보를 정확하게 나타냅니다(그림 3 참조).이러한 결과는 3D 장면 재구성 및 몰입형 콘텐츠 생성을 포함한 다양한 응용 분야에서 모델의 잠재력을 강조합니다(그림 2 참조).전체 파이프라인을 사용하여 생성할 수 있는 몰입형 360도 뷰의 예가 있는 비디오는 https://t.ly/TYA5A에서 찾을 수 있습니다.4.2. 정량적 이미지 평가 LDM3D 모델은 텍스트 프롬프트에서 고품질 이미지와 깊이 맵을 생성하는 데 인상적인 성능을 보여줍니다.MS-COCO 검증 세트에서 평가할 때 이 모델은 FID 및 CLIP 유사성 메트릭을 사용하여 안정적 확산 기준선에 대한 경쟁력 있는 점수를 얻습니다(표 참조). 1. IS(inception score)가 저하되는데, 이는 유사한 FID 점수에서 알 수 있듯이 특징 분포 면에서 실제 이미지에 가까운 이미지를 생성하지만 다양성이나 IS가 포착하는 일부 이미지 품질 측면이 부족할 수 있음을 나타낼 수 있습니다. 그럼에도 불구하고 IS는 클래스 내 다양성을 포착하는 데 어려움을 겪고[4] 모델 매개변수와 구현에 매우 민감한 반면, FID는 이미지 품질에 영향을 미치지 않는 네트워크 가중치의 사소한 변경에는 덜 민감하면서 실제 이미지와 생성된 이미지의 분포 간 유사성을 평가하는 데 더 뛰어나기 때문에 FID보다 덜 강력한 지표로 간주됩니다[2]. 높은 CLIP 유사도 점수는 모델이 텍스트 프롬프트와 관련하여 높은 수준의 세부 정보와 충실도를 유지함을 나타냅니다. 방법 SD v1.SD v1.FID↓ 28.IS↑ 34.17±0.CLIP↑ 26.13±2.27.34.02 0.26.132.28.79±0.LDM3D(저희) 27.26.61±2.표 1. 텍스트-이미지 합성. 50 DDIM[27] 단계를 사용하여 512 x 512 크기의 MS-COCO[13] 데이터 세트에서 텍스트 조건부 이미지 합성 평가. 저희 모델은 동일한 수의 매개변수(1.06B)를 갖는 안정 확산 모델과 동등합니다. IS 및 CLIP 유사도 점수는 MS-COCO 데이터 세트의 30,000개 캡션에 대해 평균화됩니다. 또한 주요 하이퍼 매개변수와 생성된 이미지의 품질 간의 관계를 조사합니다. 우리는 FID 및 IS 점수를 분류기 없는 확산 유도 스케일 인자(그림 4), 노이즈 제거 단계 수(그림 5) 및 훈련 단계(그림 7)에 대해 플로팅합니다. 또한, 우리는 그림 6에서 분류기 없는 확산 유도 스케일 인자에 대해 CLIP 유사도 점수를 플로팅했습니다. 그림 4는 이미지 품질과 다양성 사이에서 최상의 균형을 생성하는 최적의 분류기 없는 확산 FID 유도 스케일 인자가 약 s=5이며, 안정된 확산 v1.4(s=3)에서 보고된 것보다 높다는 것을 나타냅니다. 그림 6은 생성된 이미지와 입력 텍스트 프롬프트의 정렬이 5보다 큰 배율 인수의 경우 배율 인수가 변경되어도 거의 영향을 받지 않음을 나타냅니다.그림 5는 이미지 품질이 노이즈 제거 단계 수에 따라 향상되고 DDIM 단계를 50에서 100으로 늘릴 때 가장 큰 개선이 나타남을 나타냅니다.CLIP 유사도 FID FID - IS ||분류자 없는 확산 안내 척도 그림 4. FID/IS 대 분류자 없는 확산 안내 척도 인수.DDIM[27] 단계를 사용하여 MS-COCO[13] 데이터 세트의 512 x 512 크기의 샘플에 대한 텍스트 조건부 이미지 합성 평가.FIDIS 16.16.16.분류자 없는 확산 안내 척도 그림 6. CLIP 유사도 점수 대 분류자 없는 확산 안내 척도 인수. 영어: 2000개 샘플의 평균, MS-COCO [13] 데이터 세트 캡션에서 생성된 512 x 512 크기, 50개 DDIM [27] 단계.FIDTraining 단계.그림 7. FID 대 Training 단계.MSCOCO [13] 데이터 세트의 2000개 샘플, 512 x 512 크기에 대한 텍스트 조건부 이미지 합성 평가: 50개 DDIM [27] 단계, s=3.DDIM 단계200그림 5. FID/IS 대 DDIM 단계.MSCOCO [13] 데이터 세트의 2000개 샘플, 512 x 512 크기에 대한 텍스트 조건부 이미지 합성 평가, s=3.4.3. 양적 깊이 평가 LDM3D 모델은 이미지와 해당 깊이 맵을 함께 출력합니다. 이러한 이미지에 대한 기준 진실 깊이 참조가 없으므로 깊이 메트릭을 계산할 참조 모델을 정의합니다. 이를 위해 ZoeDepth 메트릭 깊이 추정 모델을 선택합니다. LDM3D는 DPT-Large에서 생성한 깊이 맵을 사용하여 미세 조정되었으므로 시차 공간에서 깊이를 출력합니다. 이러한 깊이 맵을 ZoeDepth에서 생성한 참조 깊이 맵에 맞춥니다. 이 정렬은 [19]의 접근 방식과 유사한 전역 최소 제곱 적합 방식으로 시차 공간에서 수행됩니다. 적합할 지점은 추정된 깊이 맵과 대상 깊이 맵의 교차된 유효 맵에 적용된 임의 샘플링을 통해 결정되며, 여기서 유효 깊이는 단순히 음수가 아닌 것으로 정의됩니다. 정렬 절차는 LDM3D 및 DPT-Large 깊이 맵에 적용되는 샘플당 스케일 및 이동 계수를 계산하여 깊이를 ZoeDepth LDM3D wrt ZoeDepth-N AbsRel RMSE [m] 0.DPT-Large 0.0.0에 맞춥니다.표 2. 참조 모델로 사용되는 ZoeDepth-N에 대한 LDM3D 및 DPT-Large를 비교하는 깊이 평가.모델 RFID Abs.Rel.사전 학습된 KL-AE, RGB 0.미세 조정된 KL-AE, RGBD 1.0.표 3. KL 자동 인코더 미세 조정 접근 방식 비교.사전 학습된 KL-AE는 31,471개 이미지에서 평가되었고 미세 조정된 KL-AE는 LAION-400M [25] 데이터 세트의 512x512 크기의 27,265개 이미지에서 평가되었습니다. RGB(LDM3D) 깊이(LDM3D) 깊이(DPT-L) 깊이(ZoeD-N) 그림 8. 표 2에 수반되는 깊이 시각화. 모든 깊이 맵은 반전되어 메트릭 깊이 공간으로 변환됩니다. 계산하는 두 가지 깊이 메트릭은 절대 상대 오차(AbsRel)와 평균 제곱근 오차(RMSE)입니다. 메트릭은 이미지 평가에 사용된 30k 세트에서 6k 이미지 하위 세트에 대해 집계됩니다. 표 2는 LDM3D가 DPT-Large와 유사한 깊이 정확도를 달성하여 미세 조정 접근 방식의 성공을 보여줍니다. 해당 시각화는 그림 8에 나와 있습니다. 4.4. 자동 인코더 성능 먼저 상대 FID 점수를 사용하여 미세 조정된 KLAE의 성능을 평가합니다(표 3 참조). 당사의 연구 결과에 따르면 사전 훈련된 KL-AE에 비해 재구성된 이미지의 품질이 약간 떨어지지만 측정 가능한 수준으로 저하되었습니다. 이는 픽셀 공간에서 RGB 이미지와 함께 깊이 정보를 통합할 때 데이터 압축률이 증가하지만 잠재 공간 차원은 변경되지 않기 때문일 수 있습니다. AE에 대한 조정은 최소한이며 사전 학습된 AE에 9,615개의 매개변수만 추가한다는 점에 유의하세요. AE에 대한 추가 수정을 통해 성능이 더욱 향상될 것으로 예상합니다. 현재 아키텍처에서 이러한 품질 감소는 확산 U-Net을 미세 조정하여 보상합니다. 결과 LDM3D 모델은 이전 섹션에서 보여준 것처럼 바닐라 안정적 확산과 동일한 성능을 보입니다. 5.
--- CONCLUSION ---
결론적으로, 이 연구 논문은 텍스트 프롬프트에서 RGBD 이미지를 생성하는 새로운 확산 모델인 LDM3D를 소개합니다. LDM3D의 잠재력을 보여주기 위해 TouchDesigner에서 생성된 RGBD 이미지를 사용하여 몰입적이고 대화형 360도 뷰 경험을 만드는 애플리케이션인 DepthFusion도 개발했습니다. 이 연구의 결과는 엔터테인먼트와 게임부터 건축과 디자인에 이르기까지 디지털 콘텐츠를 경험하는 방식에 혁명을 일으킬 잠재력이 있습니다. 이 논문의 기여는 멀티뷰 생성 AI와 컴퓨터 비전 분야에서 더 발전할 수 있는 길을 열어줍니다. 이 분야가 어떻게 계속 발전할지 기대하며, 제시된 작업이 커뮤니티에 유용하기를 바랍니다. 참고문헌 [1] Touchdesigner. https://derivative.ca. 액세스: 2022-12-03. 1,[2] Shane Barratt and Rishi Sharma. A note on the inception score, 2018.[3] Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka. Adabins: 적응형 빈을 사용한 깊이 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 4009-4018페이지, 2021년 6월.[4] Ali Borji. Gan 평가 측정의 장단점: 새로운 개발, 2021년.[5] Zeyu Cheng, Yi Zhang, Chengkai Tang. Swindepth: 단안 기반 깊이 추정을 위한 변압기 및 다중 스케일 융합 사용. IEEE Sensors Journal, 21(23):26912-26920, 2021년.[6] Yiqun Duan, Zheng Zhu, Xianda Guo. Diffusiondepth: 단안 깊이 추정을 위한 확산 잡음 제거 접근 방식, 2023년.[7] Patrick Esser, Robin Rombach, and Björn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. arXiv 사전 인쇄본 arXiv: 2012.09841, 2020. 3,[8] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022.[9] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. 비디오 확산 모델. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022.[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. 조건부 적대 네트워크를 사용한 이미지 간 변환, 2018.[11] Yevhen Kuznietsov, Jorg Stuckler, Bastian Leibe. 단안 깊이 맵 예측을 위한 반지도 딥 러닝. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2017년 7월.[12] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab. 완전 합성 잔여 네트워크를 사용한 더 깊은 깊이 예측. 2016년 제4회 3D 비전(3DV) 국제 학술 대회, 239-248쪽, 2016년.[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár. Microsoft coco: 맥락 속의 공통 객체, 2015년. 4, 6,[14] Armin Masoumian, Hatem A Rashwan, Saddam Abdulwahab, Julián Cristiano, M Salman Asif, Domenec Puig. Gcndepth: 그래프 합성 신경망을 기반으로 한 자기 감독 단안 깊이 추정. Neurocomputing, 517:81-92, 2023년.[15] Yue Ming, Xuyang Meng, Chunxiao Fan, Hui Yu. 단안 깊이 추정을 위한 딥 러닝: 리뷰. Neurocomputing, 438:14-33, 2021.[16] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen. GLIDE: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, Sivan Sabato 편집자, 제39회 국제 머신 러닝 컨퍼런스 논문집, 머신 러닝 연구 논문집 162권, 16784-16804쪽. PMLR, 2022년 7월 17일~23일.[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 자연어 감독을 통한 전이 가능한 시각 모델 학습, 2021.[18] René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. 밀도 예측을 위한 비전 변환기. ICCV, 2021. 1, 5,[19] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향하여: 제로샷 교차 데이터세트 전송을 위한 데이터세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 거래, 44(3):1623-1637, 2020. 1, 2, 5, 6,[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. arXiv 사전 인쇄본 arXiv:2112.10752, 2022. 1, 2, 3, 4,[21] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-net: 생물의학 이미지 분할을 위한 합성 네트워크, 2015.[22] Anirban Roy 및 Sinisa Todorovic. 신경 회귀 숲을 사용한 단안 깊이 추정. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 2016년 6월.[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, Mohammad Norouzi. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022.[24] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, David J Fleet. 확산 모델을 사용한 단안 깊이 추정. arXiv 사전 인쇄본 arXiv:2302.14816, 2023.[25] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114, 2021. 3,[26] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. 제11회 학습 표현 국제 컨퍼런스, 2023.[27] Jiaming Song, Chenlin Meng, Stefano Ermon. 노이즈 제거 확산 암묵적 모델. 2021년 국제 학습 표현 컨퍼런스에서. 6,[28] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe. 단안 깊이 추정을 위한 순차적 딥 네트워크로서의 다중 스케일 연속 crfs. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 2017년 7월.[29] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci. 단안 깊이 추정을 위한 구조화된 주의 유도 합성 신경 필드. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 2018년 6월.[30] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, Elisa Ricci. 연속 픽셀 단위 예측을 위한 변압기 기반 주의 네트워크. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 16269-16279페이지, 2021년 10월.[31] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023.[32] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman 및 Oliver Wang. 지각적 측정 기준으로서 딥 피처의 비합리적인 효과성, 2018년.
