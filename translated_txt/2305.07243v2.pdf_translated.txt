--- METHOD ---
성능 개선에 대한 이론은 이미지에 국한될 필요가 없습니다. 이 논문에서는 이미지 생성 도메인의 발전을 음성 합성에 적용하는 방법을 설명합니다. 그 결과, 표현력이 풍부한 다중 음성 텍스트-음성 시스템인 TorToise가 탄생했습니다. 모든 모델 코드와 훈련된 가중치는 https://github.com/neonbjb/tortoise-tts에서 오픈 소스로 공개되었습니다. 1 배경 1.1 텍스트-음성 텍스트-음성(TTS) 연구 분야는 비교적 작은 데이터 세트에서 훈련된 효율적인 모델을 개발하는 데 크게 제한되어 왔습니다. 이러한 선택은 다음에 의해 주도되었습니다. 1. 대규모로 배포할 수 있고 따라서 높은 샘플링 속도를 가져야 하는 효율적인 음성 생성 모델을 구축하려는 욕구. 2. 매우 큰 필사된 음성 데이터 세트의 부족. 3. TTS에서 전통적으로 사용되는 인코더-디코더 모델 아키텍처를 확장하는 데 따른 과제. 1.1.1 신경 MEL 인버터 대부분의 최신 텍스트-음성 시스템은 MEL 스펙트로그램으로 인코딩된 음성 데이터에서 작동합니다. 이 인코딩 공간에서 작동해야 하는 설득력 있는 이유가 많지만 신경망의 경우 가장 설득력 있는 이유는 공간적으로 매우 압축된다는 것입니다. 예를 들어 Tacotron에서 사용하는 MEL 구성은 22kHz로 샘플링된 원시 오디오 파형 데이터에 대해 256배 압축으로 작동하지만 해당 데이터에서 발견되는 대부분의 정보를 포함합니다. 이 때문에 MEL 스펙트로그램을 오디오 파형으로 다시 디코딩하는 고품질 방법을 찾기 위해 많은 연구가 진행되었습니다. 이 작업을 수행하는 합성기는 일반적으로 &quot;보코더&quot;라고 하지만, 이 논문에서는 일반적으로 &quot;MEL 인버터&quot;라고 부릅니다. 신경망을 기반으로 하는 최신 MEL 인버터는 매우 정교합니다. 인간의 귀에 녹음된 파형과 거의 구별할 수 없는 파형을 생성하며, 학습 세트 외부에서 매우 일반화할 수 있습니다. 저는 텍스트 음성 변환 시스템의 최종 단계로 Univnet(Kim, 2021) 구현을 사용하여 이 작업을 활용합니다. 1.2 이미지 생성 TTS 시스템은 대체로 대기 시간에 초점을 맞추지만, 다른 도메인에서는 그렇지 않았습니다. 예를 들어, 이미지 생성의 경우 샘플링 시간에 관계없이 고품질 결과를 생성하는 학습 모델에 더 많은 초점이 적용되었습니다. 이 논문의 목적을 위해 두 가지 연구 주제를 살펴봅니다. 1.2.1 DALL-E DALL-E(Ramesh et al., 2021)는 자기 회귀 디코더를 텍스트-이미지 생성에 적용할 수 있는 방법을 보여주었습니다. 이는 특히 매력적입니다. NLP 도메인에서 디코더 전용 모델을 확장하는 데 쏟아부은 방대한 양의 연구. DALL-E에는 두 가지 중요한 문제가 있습니다. 첫째, 전체 시퀀스 자체 주의에 의존하는데, 이는 O(N2) 컴퓨팅 및 메모리 비용을 수반합니다. 여기서 N은 시퀀스 길이입니다. 이는 순진하게 처리하면 시퀀스 길이가 긴 이미지나 오디오와 같은 모달리티를 처리할 때 특히 번거롭습니다. 둘째, 기존의 자기 회귀 접근 방식은 이산 도메인에서 작동해야 합니다. 이미지는 양자화 자동 인코더를 사용하여 이산 토큰 시퀀스로 인코딩됩니다. 그런 다음 DALL-E는 자기 회귀 사전 모델을 사용하여 이러한 토큰 시퀀스를 모델링합니다. 이는 표현력 측면에서 DALL-E의 강점이지만, 이러한 이미지 토큰을 실제로 이미지를 구성하는 픽셀 값으로 다시 변환할 수 있는 디코더가 필요하다는 단점이 있습니다. 제 생각에는 DALL-E에서 사용하는 학습된 VQVAE 디코더가 대부분 샘플에서 나타나는 흐릿한 불일치에 주로 책임이 있습니다. 1.2.2 DDPMS 생성 모델 공간은 오랫동안 평균 추구 동작(흐릿함 발생) 또는 모드 붕괴(다양성 또는 일반화 부족 발생)를 보이는 모델에 의해 괴로움을 받아 왔습니다. 잡음 제거 확산 확률 모델(DDPM(Ho et al., 2020))은 최근 선명하고 일관성 있고 다양한 이미지를 생성할 수 있는 최초의 생성 모델 유형으로 등장했습니다. 이러한 모델은 저품질 유도 신호를 사용하여 해당 유도 신호가 파생된 고차원 공간을 재구성하는 데 매우 효과적인 것으로 나타났습니다. 다시 말해, 초고해상도에 뛰어납니다. DDPMS에는 두 가지 중요한 주의 사항이 있습니다. 1. DDPM에 대한 기존 접근 방식은 샘플링이 시작되기 전에 알려진 고정된 출력 모양에 의존합니다. 이 논문과 관련된 구체적인 예로, DDPM은 텍스트와 오디오 간의 암묵적 정렬 문제를 해결할 수 없기 때문에 텍스트를 오디오 신호로 변환하는 방법을 학습할 수 없습니다. 2. DDPM은 여러 번의 반복을 통해 샘플링해야 합니다. 이 샘플링 프로세스는 많은 양의 컴퓨팅을 소모하며, DDPM에서 샘플링하면 항상 상당한 지연 비용이 발생합니다.1.2.3 재순위 지정 DALL-E는 자기 회귀 모델의 출력을 &quot;재순위 지정&quot;하는 프로세스를 도입했습니다.이 프로세스는 자기 회귀 모델에서 무작위로 샘플링하고 다운스트림 사용을 위해 k개의 출력 중 가장 높은 품질의 출력을 선택합니다.이러한 절차에는 강력한 판별자, 즉 좋은 텍스트/이미지 페어링과 나쁜 페어링을 구별할 수 있는 모델이 필요합니다.DALL-E는 대조 텍스트 및 이미지 페어링 목적으로 학습된 모델인 CLIP(Radford et al., 2021)을 사용했습니다.2 방법 2.1 자기 회귀 디코더와 DDPM 결합 위에서 도출한 결론 중 일부를 검토해 보겠습니다.1. 자기 회귀 모델은 시각, 텍스트 및 음성과 같은 정렬되지 않은 도메인 간 변환에 강력합니다.텍스트 컨디셔닝 클립(파형) 자기 회귀 변압기 컨디셔닝 MEL 코드 ! 잠재 활성화 CLVP 확산 디코더 Argmax 출력 MEL 보코더 ------------출력 파형 그림 1: Tortoise-v2 아키텍처 설계 다이어그램. 텍스트 입력과 참조 오디오 클립(스피커 복제용)은 일련의 디코딩 및 필터링 네트워크를 거쳐 고품질 음성을 생성합니다. 2. DDPM은 연속 도메인에서 작동하여 표현적 모달리티를 모델링할 수 있습니다. 두 가지 유형의 모델 모두 추가 컴퓨팅 및 데이터로 성능을 확장할 수 있는 기능을 보여주었습니다. 음성 스펙트로그램이나 이미지와 같은 연속 데이터를 생성하는 것과 같은 문제가 발생할 때 이 두 가지 접근 방식을 결합하면 몇 가지 뚜렷한 이점이 있을 수 있다는 것이 분명해집니다. 구체적으로 추론에서 자기 회귀 모델을 사용하여 텍스트 토큰 시퀀스를 출력 공간을 나타내는 토큰 시퀀스(이 경우 음성 토큰)로 변환합니다. 그런 다음 DDPM을 사용하여 이러한 토큰을 고품질 음성 표현으로 디코딩합니다. 2.2 TTS에 자기 회귀+DDPM 적용 이전에 제안된 시스템을 구축하려면 다음 신경망을 학습해야 합니다. 1. 텍스트에 따라 음성 토큰의 확률 분포를 예측하는 자기 회귀 디코더. 2. 자기 회귀 디코더의 출력을 순위 지정하는 데 사용되는 CLIP과 유사한 대조 모델. 3. 음성 토큰을 다시 음성 스펙트로그램으로 변환할 수 있는 DDPM. 이러한 모든 네트워크의 아키텍처와 학습 프로세스는 대체로 해당 문헌에서 찾을 수 있는 절차를 따릅니다. 자세한 내용은 B 2.2.1에서 확인할 수 있습니다. 입력 조절 TorToise에서 선택한 고유한 설계는 자기 회귀 생성기와 DDPM 모두에 제공되는 추가 입력으로, 제가 음성 조절 입력이라고 부릅니다. 음성 조절 입력은 대상과 동일한 화자의 하나 이상의 오디오 클립으로 시작합니다. 이러한 클립은 MEL 스펙트로그램으로 변환되어 셀프 어텐션 계층 스택으로 구성된 인코더를 통해 입력됩니다. 자기 회귀 생성기와 DDPM은 각자의 컨디셔닝 인코더를 가지고 있으며, 둘 다 각각의 네트워크와 함께 학습됩니다. 이러한 계층의 출력은 평균화되어 단일 벡터를 생성합니다. 그런 다음 모든 인코딩된 컨디셔닝 클립의 벡터는 자기 회귀 또는 컨디셔닝 네트워크에 입력으로 제공되기 전에 다시 평균화됩니다. 컨디셔닝 입력의 직관은 모델이 톤과 음조와 같은 음성 특성을 유추하여 주어진 텍스트 입력에 해당하는 가능한 음성 출력의 검색 공간이 크게 줄어드는 방법을 제공한다는 것입니다. 2.2.2 &quot;거북이 트릭&quot; 대부분의 학습 절차에서 DDPM은 이산 음성 코드를 MEL 스펙트로그램으로 변환하도록 학습됩니다. 이 프로세스가 수렴한 후 음성 코드 대신 AR 모델 출력에서 가져온 자기 회귀 잠재 공간에서 DDPM을 미세 조정합니다. 이는 B에서 자세히 설명합니다. 여기서의 논리는 AR 잠재 공간이 이산 토큰보다 의미적으로 훨씬 더 풍부하다는 것입니다. 이 잠재 공간을 미세 조정함으로써 다운스트림 확산 모델의 효율성을 개선합니다. 저는 이를 동결된 텍스트 인코더에 조건화된 디코더 모델을 학습하여 큰 효율성 이득을 얻는다는 것을 보여주는 최근 연구에 비유합니다. 이 미세 조정은 제가 다양한 모델 학습 프로세스에 적용한 모든 조정 중에서 모델 출력 품질에 가장 크게 기여하는 요소 중 하나입니다. 2.3 CLVP 앞서 언급했듯이 생성 모델에서 표현적인 출력을 수집하기 위한 좋은 전략은 정성적 판별기를 사용하여 여러 출력을 다시 순위 지정한 다음 가장 좋은 출력만 선택하는 것입니다. DALL-E는 이를 위해 CLIP을 사용합니다. CLIP에 사용된 이와 동일한 유형의 접근 방식은 음성에도 적용할 수 있습니다. 결국 대부분의 TTS 데이터 세트는 단순히 오디오 클립과 텍스트의 쌍입니다. 대조 설정에서 이러한 쌍에 대한 모델을 학습함으로써 모델은 음성에 대한 좋은 판별기가 됩니다. Tortoise의 경우 대조적 언어-음성 사전 학습 변환기 또는 CLVP를 학습합니다. CLIP과 동일한 속성이 많지만, 특히 AR 모델에서 TTS 출력을 다시 순위 지정하는 데 사용할 스코어링 모델로 사용됩니다. 추론에서 이 작업을 효율적으로 수행하기 위해 CLVP를 훈련하여 이산화된 음성 토큰과 텍스트 토큰을 페어링했습니다. 이런 식으로 CLVP는 값비싼 확산 모델을 적용하지 않고도 여러 AR 출력을 다시 순위 지정할 수 있습니다. 3 훈련 이 모델은 1년 동안 8대의 NVIDIA RTX-3090으로 구성된 소규모 클러스터에서 훈련되었습니다. 이러한 모델이 어떻게 훈련되는지에 대한 구체적인 내용은 B에서 확인할 수 있습니다. 4 추론 프로세스 프레임워크의 4개 모델이 완전히 훈련되면 추론 절차는 다음과 같습니다. 1. 조건 입력과 텍스트를 자기 회귀 모델에 입력하고 많은 수의 출력 후보를 디코딩합니다. 2. CLVP를 사용하여 각 음성 후보와 텍스트 간의 상관 관계를 생성합니다. 3. 상위 k개의 음성 후보를 선택하고 각 후보에 대해 다음을 수행합니다. 4. DDPM을 사용하여 MEL 스펙트로그램으로 디코딩합니다. 5. 기존 보코더를 사용하여 파형으로 변환합니다.6. 자기 회귀 모델을 디코딩할 때 P=.8, 반복 페널티=2, 소프트맥스 온도=.8인 핵 샘플링이 사용됩니다.DDPM에서 샘플링하는 것은 매우 연구되고 빠르게 변화하는 분야입니다.Tortoise를 설계할 당시, 품질과 추론 속도 사이에서 가장 좋은 균형을 이루는 샘플링 구성은 다음과 같다는 것을 발견했습니다.1. 알고리즘: DDIM(Song et al., 2022)2. 일정: 선형3. 샘플링 단계:4. 조건 없는 안내 상수:데이터 세트 본질적으로 대규모 언어 모델을 훈련하는 것이 목표였기 때문에 많은 데이터가 필요했습니다.LibriTTS(Zen et al., 2019)와 HiFiTTS(Bakhturina et al., 2021) 데이터 세트로 시작했는데, 두 데이터 세트를 합치면 896시간 분량의 필사된 음성이 들어 있습니다. 저는 인터넷에서 스크래핑한 오디오북과 팟캐스트에서 49,000시간 분량의 음성 오디오로 구성된 추가 &quot;확장&quot; 데이터 세트를 구축했습니다. 이 데이터 세트가 어떻게 구축되었는지에 대한 자세한 내용은 부록 I에 나와 있습니다. 공식 LibriTTS 테스트 분할은 검증 목적으로 사용되었습니다.
--- EXPERIMENT ---
s 텍스트 음성 변환 시스템은 많은 최첨단 시스템이 비교할 샘플이 거의 없는 폐쇄 소스이기 때문에 실험적으로 비교하기 어렵습니다. 이를 위해 저는 CLVP를 사용하여 실제 샘플과 생성된 샘플 간의 거리 측정치를 생성하는 나만의 평가 모음을 만들었습니다. 이는 이미지에서 사용하는 FID 점수와 유사합니다. 또한 오픈 소스 wav2vec 모델을 사용하여 음성 세그먼트의 &quot;이해성&quot;을 특성화합니다. 저는 이 작업을 여기에서 오픈 소스로 공개했습니다. 그 외에도 TorToise에서 생성된 샘플과 다른 논문에서 생성된 샘플 간의 비교는 여기에서 확인할 수 있습니다.
--- CONCLUSION ---
위에 그려진 것: 1. 자기 회귀 모델은 시각, 텍스트 및 음성과 같은 정렬되지 않은 도메인 간 변환에 강합니다.텍스트 컨디셔닝 클립(파형) 자기 회귀 변압기 컨디셔닝 MEL 코드 !잠복 활성화 CLVP 확산 디코더 Argmax 출력 MEL 보코더 ------------출력 파형 그림 1: Tortoise-v2 아키텍처 설계 다이어그램.텍스트 입력과 참조 오디오 클립(스피커 복제용)은 일련의 디코딩 및 필터링 네트워크를 거쳐 고품질 음성을 생성합니다.2. DDPM은 연속 도메인에서 작동하여 표현 양식을 모델링할 수 있습니다.두 유형의 모델 모두 추가 컴퓨팅 및 데이터로 성능을 확장할 수 있는 기능을 보여주었습니다.음성 스펙트로그램이나 이미지와 같은 연속 데이터를 생성하는 것과 같은 문제가 발생할 때 이 두 가지 접근 방식을 결합하면 몇 가지 뚜렷한 이점이 있을 수 있음이 분명해집니다. 구체적으로 추론에서 자기 회귀 모델을 사용하여 텍스트 토큰 시퀀스를 출력 공간을 나타내는 토큰 시퀀스(이 경우 음성 토큰)로 변환합니다.그런 다음 DDPM을 사용하여 이러한 토큰을 고품질 음성 표현으로 디코딩합니다.2.2 TTS에 자기 회귀+DDPM 적용 이전에 제안한 시스템을 구축하려면 다음 신경망을 학습해야 합니다.1. 텍스트를 조건으로 음성 토큰의 확률 분포를 예측하는 자기 회귀 디코더.2. 자기 회귀 디코더의 출력을 순위 지정하는 데 사용되는 CLIP과 유사한 대조 모델.3. 음성 토큰을 다시 음성 스펙트로그램으로 변환할 수 있는 DDPM.이러한 모든 네트워크의 아키텍처와 학습 프로세스는 대체로 해당 문헌에서 찾을 수 있는 절차를 따릅니다.자세한 내용은 B 2.2.1 입력 조절에서 확인할 수 있습니다.TorToise에서 선택한 고유한 설계는 자기 회귀 생성기와 DDPM 모두에 제공되는 추가 입력으로, 저는 이를 음성 조절 입력이라고 합니다. 음성 컨디셔닝 입력은 대상과 동일한 화자의 하나 이상의 오디오 클립으로 시작합니다. 이러한 클립은 MEL 스펙트로그램으로 변환되어 셀프 어텐션 레이어 스택으로 구성된 인코더를 통해 공급됩니다. 자기 회귀 생성기와 DDPM에는 자체 컨디셔닝 인코더가 있으며, 둘 다 해당 네트워크와 함께 학습됩니다. 이러한 레이어의 출력은 평균화되어 단일 벡터를 생성합니다. 인코딩된 모든 컨디셔닝 클립의 벡터는 자기 회귀 또는 컨디셔닝 네트워크에 입력으로 공급되기 전에 다시 평균화됩니다. 컨디셔닝 입력의 직관은 주어진 텍스트 입력에 해당하는 가능한 음성 출력의 검색 공간이 크게 줄어들도록 모델이 음색 및 음조와 같은 음성 특성을 유추할 수 있는 방법을 제공한다는 것입니다. 2.2.2 &quot;거북이 트릭&quot; 대부분의 학습 절차에서 DDPM은 이산 음성 코드를 MEL 스펙트로그램으로 변환하도록 학습됩니다. 이 프로세스가 수렴한 후, 음성 코드 대신 AR 모델 출력에서 가져온 자기 회귀 잠재 공간에서 DDPM을 미세 조정합니다. 이는 B에서 자세히 설명합니다. 여기서의 논리는 AR 잠재 공간이 이산 토큰보다 의미적으로 훨씬 더 풍부하다는 것입니다. 이 잠재 공간에서 미세 조정을 통해 다운스트림 확산 모델의 효율성을 개선합니다. 저는 이를 동결된 텍스트 인코더를 조건으로 디코더 모델을 학습하여 큰 효율성 이득을 얻는다는 것을 보여주는 최근 연구에 비유합니다. 이 미세 조정은 다양한 모델 학습 프로세스에 적용한 모든 조정 중에서 모델 출력 품질에 가장 크게 기여하는 요소 중 하나입니다. 2.3 CLVP 앞서 언급했듯이 생성 모델에서 표현적인 출력을 수집하기 위한 좋은 전략은 정성적 판별기를 사용하여 여러 출력을 다시 순위를 매긴 다음 가장 좋은 출력만 선택하는 것입니다. DALL-E는 이를 위해 CLIP을 사용합니다. CLIP에 사용된 이와 동일한 유형의 접근 방식은 음성에도 적용할 수 있습니다.결국 대부분의 TTS 데이터 세트는 오디오 클립과 텍스트의 단순한 쌍입니다.대조 설정에서 이러한 쌍에 대한 모델을 학습하면 모델이 음성에 대한 우수한 판별자가 됩니다.Tortoise의 경우 Contrastive Language-Voice Pretrained Transformer 또는 CLVP를 학습합니다.CLP와 동일한 속성이 많지만 특히 AR 모델의 TTS 출력을 다시 순위 지정하는 데 사용할 수 있는 스코어링 모델로 사용됩니다.이를 추론에서 효율적으로 작동시키기 위해 CLVP를 학습하여 이산화된 음성 토큰과 텍스트 토큰을 쌍으로 지정했습니다.이렇게 하면 CLVP가 값비싼 확산 모델을 적용하지 않고도 여러 AR 출력을 다시 순위 지정할 수 있습니다.3 학습 이 모델은 1년에 걸쳐 8대의 NVIDIA RTX-3090으로 구성된 소규모 클러스터에서 학습되었습니다. 이러한 모델이 훈련되는 방법에 대한 구체적인 내용은 B. 4 추론 프로세스에서 찾을 수 있습니다. 프레임워크의 네 가지 모델이 완전히 훈련되면 추론 절차는 다음과 같습니다. 1. 조건 입력과 텍스트를 자기 회귀 모델에 공급하고 많은 수의 출력 후보를 디코딩합니다. 2. CLVP를 사용하여 각 음성 후보와 텍스트 간의 상관 관계를 생성합니다. 3. 상위 k개의 음성 후보를 선택하고 각 후보에 대해 다음을 수행합니다. 4. DDPM을 사용하여 MEL 스펙트로그램으로 디코딩합니다. 5. 기존 보코더를 사용하여 파형으로 변환합니다. 6. 자기 회귀 모델을 디코딩할 때 P=.8, 반복 페널티=2, 소프트맥스 온도=.8인 핵 샘플링이 사용됩니다. DDPM의 샘플링은 매우 연구되고 빠르게 변화하는 분야입니다. Tortoise를 설계할 당시 품질과 추론 속도 간의 균형이 가장 좋은 샘플링 구성은 다음과 같습니다. 1. 알고리즘: DDIM(Song et al., 2022)2. 일정: 선형 3. 샘플링 단계: 4. 컨디셔닝 없는 안내 상수: 데이터 세트 내 목표는 본질적으로 대규모 언어 모델을 훈련하는 것이었기 때문에 많은 데이터가 필요했습니다. LibriTTS(Zen et al., 2019)와 HiFiTTS(Bakhturina et al., 2021) 데이터 세트로 시작했는데, 두 데이터 세트는 합쳐서 896시간 분량의 필사된 음성을 포함하고 있습니다. 저는 인터넷에서 스크래핑한 오디오북과 팟캐스트에서 49,000시간 분량의 음성 오디오로 구성된 추가 &quot;확장된&quot; 데이터 세트를 구축했습니다. 이 데이터 세트를 구축한 방법에 대한 자세한 내용은 부록 I에 나와 있습니다. 공식 LibriTTS 테스트 분할은 검증 목적으로 사용되었습니다. 실험 텍스트 음성 변환 시스템은 많은 최첨단 시스템이 비교할 샘플이 거의 없는 폐쇄 소스이기 때문에 실험적으로 비교하기 어렵습니다. 이를 위해 저는 CLVP를 사용하여 실제 샘플과 생성된 샘플 간의 거리 메트릭을 생성하는 나만의 평가 모음을 구축했습니다. 이는 이미지에서 사용하는 FID 점수와 유사합니다. 또한 오픈 소스 wav2vec 모델을 사용하여 음성 세그먼트의 &quot;이해성&quot;을 특성화합니다. 이 작업을 여기에서 오픈 소스로 공개했습니다. 그 외에도 TorToise에서 생성된 샘플과 다른 논문에서 생성된 샘플 간의 비교는 여기에서 확인할 수 있습니다. 결론 Tortoise는 일반 모델 아키텍처를 사용하는 최근의 최첨단 혁신의 최신 버전입니다. Tortoise의 거의 모든 부분이 오디오 처리를 위해 특별히 설계되지 않았지만 이전 모든 TTS 모델보다 사실적으로 뛰어납니다. 다음을 통해 이를 수행합니다. 변압기 계층 스택과 같은 일반 아키텍처를 채택합니다. 대규모 고품질 데이터 세트를 활용합니다. 대규모 규모와 높은 배치 크기로 학습합니다. 이 프로젝트에서 제가 얻은 가장 큰 이점은 위의 3가지 사항을 준수함으로써 얻은 결과가 얼마나 믿을 수 없을 정도로 강력한지입니다. 모든 디지털화된 모달리티는 이 프레임워크를 사용하여 생성 모델링을 받을 가능성이 있는 것 같습니다. 참고문헌 Bakhturina, E., Lavrukhin, V., Ginsburg, B., Zhang, Y. (2021). Hi-fi 다중 스피커 영어 TTS 데이터 세트. Ho, J., Jain, A., Abbeel, P. (2020). 확산 확률적 모델 잡음 제거. Kim, J. (2021). Mindslab UnivNet 구현. Nichol, A., Dhariwal, P. (2021). 개선된 확산 확률적 모델 잡음 제거. Radford, A., Kim, JW, Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). 자연어 감독에서 전이 가능한 시각적 모델 학습. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). 제로샷 텍스트-이미지 생성. Seonghyeon, K. (2019). VQVAE(rosinality). Song, J., Meng, C., and Ermon, S. (2022). 확산 암시적 모델 노이즈 제거. Wang, P. (2020). 벡터 양자화(Lucidrains). Wang, P. (2021). x-transformers (Lucidrains).Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, RJ, Jia, Y., Chen, Z., and Wu, Y. (2019). Libritts: 텍스트 음성 변환을 위한 librispeech에서 파생된 코퍼스. 확장된 데이터 세트 컬렉션 저는 웹에서 스크래핑한 오디오북과 팟캐스트로 구성된 확장된 TTS 데이터 세트를 독립적으로 구축했습니다. 이 데이터는 500ms의 침묵으로 분할되었고, 5~20초 사이의 오디오 클립은 모두 보관되었습니다. 그런 다음 배경 소음, 음악, 품질이 좋지 않은(예: 전화 통화), 동시에 말하는 여러 음성 및 리버브가 있는 모든 오디오를 제거하는 제가 훈련한 분류기 파이프라인을 통해 결과 클립을 공급했습니다. 디스크 공간 제한으로 인해 스크래핑 양을 제한해야 했습니다. 최종 결과는 49,000시간 분량의 정리된 오디오 클립이었습니다. 저는 wav2vec2-large 모델을 사용하여 이 데이터 세트를 필사했습니다. 저는 개인적으로 이 모델을 미세 조정하여 구두점을 예측했습니다. 따옴표, 쉼표, 느낌표는 음성을 생성하는 데 중요하지만 일반적으로 음성 인식 모델의 학습에는 포함되지 않기 때문입니다. 미세 조정은 LibriTTS와 HiFiTTS에서 수행되었으며 사전 학습된 모델 가중치와 필사 스크립트는 여기에서 찾을 수 있습니다. B 학습 및 아키텍처 세부 정보 B.1 VQVAE TorToise와 함께 사용되는 VQVAE는 van der Oord 등이 만든 원래 VQVAE와 가장 유사합니다. MEL 스펙트로그램에서 작동합니다. 스펙트로그램을 추가로 4배 압축하고 8192개 토큰으로 구성된 코드북을 생성하는 작은 잔차 합성 신경망으로 구성됩니다. VQVAE를 학습할 때 배치 크기가 클수록 재구성 손실이 줄어든다는 것을 발견하여 인프라에 매우 큰 배치 크기를 사용했습니다. 입력 샘플은 40960개 PCM 판독값 또는 2초 오디오로 제한되었습니다. VQVAE를 훈련하는 데 있어서 주요 병목 현상은 dataloader였습니다. 0.0.0.0.0.0.0.0.0.0.loss_reconstruction_loss -Irdvae_final-Irdvae_final Irdvae_final Irdvae_final Irdvae_final-Irdvae_final1k 10k 그림 2: VQVAE의 훈련 곡선. Y축은 log-log 스케일의 MSE 손실입니다. X축은 훈련 단계 수입니다. 모델 모양 상단 dim 하단 dim 코드북 dim 1D Conv resnet, 인코더 + 디코더 양자화 토큰 수 양자화 알고리즘 배치 크기 총 학습 손실 클러스터링 a la original VQVAE, 재시작 없음 360M 샘플 MSE 재구성 손실, 커밋먼트 손실 3e-.9.LR B1, B 가중치 감소 .EMA 가중치는 LR 감소를 비율로 대체합니다.표 1: VQVAE 모델 세부 정보 및 하이퍼파라미터 B.2 자기 회귀 사전 AR 디코더는 bog-standard GPT-2 아키텍처를 사용하고 일반적으로 DALLE-1 논문의 학습 지침을 따릅니다. DALL-E와 달리 dense self-attention만 사용됩니다. 프롬프트는 다음과 같이 조립됩니다.<SC> 비티엑스&lt;<T><T><T> .. <T><ET><BM><M<M<M> ... <EM>SC=음성 조절 인코딩 BT=텍스트 토큰 시작 T=텍스트 토큰 ET 텍스트 토큰 종료 BM MEL 토큰 시작 M-MEL 토큰 EM-MEL 토큰 종료 음성 조절 인코딩은 관련 클립(같은 사람이 말하는 다른 클립)의 MEL 스펙트로그램을 가져오고 주의 맥락의 앞에 배치되는 단일 벡터 임베딩을 생성하는 별도의 인코더에 의해 학습됩니다. 각 학습 샘플에 대해 두 개의 인코딩이 생성되어 함께 평균화됩니다. 조절 인코더에 대한 최대 입력 길이는 132,300개 샘플 또는 6초의 오디오입니다. 학습된 위치 임베딩이 사용됩니다. MEL 토큰과 텍스트 토큰은 자체 위치 매개변수를 가져옵니다. 텍스트 입력은 패딩되지 않고 MEL 토큰은 각 배치의 시퀀스 길이에 맞게 오른쪽으로 패딩됩니다. 최대 시퀀스 길이는 402개 텍스트 토큰 + 604개 MEL 토큰입니다. 효율성을 위해 학습 전반부에서 모델은 ¡6초 오디오 클립만 보았습니다. 그 후, 전체 길이(27초)까지의 오디오 클립이 보였습니다.loss_mel_ce -unified_with_bpe_large unified_with_bpe_large unified_with_bpe_large-unified_with_bpe_large unified_with_bpe_large unified_with_bpe_large1k 10k 그림 3: 로그-로그 스케일의 초기 훈련 곡선. Y축은 MEL 토큰의 교차 엔트로피 손실입니다. X축은 훈련 단계 수입니다. 곡선에 재현 불가능한 노이즈를 추가하여 온라인 변경으로 인해 긴 훈련 및 미세 조정은 포함하지 않습니다. 모델 아키텍처 계층 모델 dim 어텐션 헤드 텍스트 토큰화 배치 크기 전체 학습 텍스트, 다음 토큰 예측, 손실 가중치 MEL 토큰, 다음 토큰 예측 가중치 LR B1, B 가중치 감소 LR 워밍업 EMA 감소율 인과 마스킹이 있는 트랜스포머 스택 사용자 지정 BPE, 256개 토큰 폭.1억 1,900만 샘플.1e-.9..500단계.표 2: AR 사전 세부 정보 및 하이퍼파라미터 자기 회귀 디코더를 수렴하도록 학습한 후 LibriTTS 및 HIFITTS의 클린 오디오 데이터 세트에서 미세 조정했습니다.B.CLVP 원래 DALLE은 주어진 텍스트 프롬프트에 대한 많은 수의 이미지를 디코딩하여 작동한 다음 CLIP에 공급했습니다.CLIP이 입력 텍스트에 가장 가깝다고 간주한 이미지가 최종 출력으로 사용되었습니다.결과 섹션에서 분명해질 이유 때문에 Tortoise에 대한 이 리드를 계속 따릅니다. 저는 CLIP과 매우 유사한 간단한 모델을 구축했는데, 이를 &quot;대조적 언어-음성 사전 학습&quot; 모델 또는 CLVP라고 부릅니다. CLIP과 마찬가지로 이 모델은 텍스트/음성 쌍에 대한 거리 메트릭을 생성합니다. CLVP는 CLIP 텍스트 인코더와 유사한 아키텍처를 사용하지만, 두 개를 사용합니다. 하나는 텍스트 토큰용이고 다른 하나는 MEL 토큰용입니다. 두 인코더의 토큰은 15%의 비율로 삭제되었습니다. 고정 위치 임베딩이 사용되었습니다. 최대 텍스트 입력 길이는 350개 토큰이었습니다(실제로는 본 적이 없음). 최대 MEL 토큰 입력 길이는 293 또는 13초의 오디오였습니다. loss_clip_total -r4_medium_size r4_mel_masking r4_medium_size r4_medium_size r4_medium_size -r4_medium_size r4_medium_size r4_medium_size -r4_medium_size 0.0.0.0.0.0.30M 40M 50M 60M 70M 80M 그림 4: 로그-로그 스케일의 CLVP에 대한 후기 학습 곡선. Y축은 교차 엔트로피 손실입니다. X축은 샘플 수입니다. 초기 학습 곡선은 손실되었습니다. 모델 아키텍처 깊이 이중 변압기 스택 모델 dim 어텐션 헤드 텍스트 토큰화 사용자 지정 BPE, 256개 토큰 폭 배치 크기 총 학습 손실 80M개 샘플. 대조적 LR 3e-B1, B.9. 가중치 감소 .LR 워밍업 500단계 EMA 감소율 .표 3: CLVP 학습 세부 정보 및 하이퍼파라미터 B.4 확산 디코더 확산 모델은 잔여 합성곱과 고밀도 셀프 어텐션을 결합한 맞춤형 아키텍처를 사용합니다. DDPM에 사용되는 기존 U-Net 모델과 가장 유사하지만 업샘플링이나 다운샘플링이 없습니다. 확산 모델은 3가지 컨디셔닝 소스를 받습니다. 네트워크에서 사용하는 그룹 규범의 규모와 이동을 변조하는 타임스텝 신호. 그룹 규범의 규모와 이동을 변조하는 음성 컨디셔닝 신호. 자기 회귀 모델의 최종 활성화. 확산 모델을 훈련할 때, 이 모델을 결정하기 전에 여러 가지 다른 아키텍처와 컨디셔닝 유형을 반복했습니다. 여기에는 다음이 포함됩니다. 아키텍처: 주의가 있는 &quot;전통적인&quot; U-net이 시도되었습니다. 전체 주의 네트워크는 프레셰 거리 평가에서 상당히 더 나은 성능을 보였습니다. MEL이 아닌 PCM 데이터에서 작동합니다. 여기에는 매우 작은 컨텍스트 창이 필요했고 여전히 훈련하는 데 엄청난 시간이 걸렸습니다. MEL을 디코딩하고 보코더를 사용한 결과 품질이 상당히 향상되었습니다. 기존 확산 노이즈 일정과의 호환성을 강제하기 위해 입력 MEL을 [-1,1] 간격으로 재조정했습니다. MEL 토큰 디코딩 대 AR 활성화. AR 활성화에 대한 훈련은 각 훈련 단계에서 AR 네트워크를 통해 prop을 전달해야 하기 때문에 비용이 많이 듭니다. 그러나 AR 활성화에 대한 훈련은 확산 네트워크에 대한 모든 설계 결정 중 출력 품질에서 가장 큰 도약을 이루었습니다. 텍스트를 어텐션 컨텍스트에 두는 것과 같은 트릭을 수행하면 이러한 이점이 다소 사라질 수 있습니다. 이미지 확산 모델과 마찬가지로 분류기 없는 안내를 활용하는 것은 고품질 출력을 위해 매우 중요합니다. TorToise의 경우 음성 조절 신호와 AR 모델의 활성화에 대한 안내를 수행합니다. 훈련하는 동안 15%의 경우 두 신호가 모두 삭제되고 학습된 임베딩으로 대체됩니다. 확산 디코더를 훈련할 때 입력 오디오는 무작위로 220,500개 샘플 또는 오디오 초로 클리핑되었습니다. 조절 입력은 102,400개 샘플 또는 오디오 5초로 클리핑되었습니다. 나머지 Tortoise 스택은 22kHz의 오디오 샘플링 속도로 작동하는 반면, 확산 디코더는 24kHz 오디오에서 계산된 MEL 스펙트로그램을 출력합니다. 이러한 불일치는 모델 스택이 사용하는 사전 학습된 Univnet 보코더와의 호환성을 보장하기 위한 것일 뿐, 성능상의 이유로 수행된 것이 아닙니다.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.10k loss_diffusion_loss tts_flat_r5 tts_flat_r5 tts_flat_r5tts_flat_r1M 그림 5: 확산 모델 손실, 로그-로그 스케일. Y축: MSE 손실, X축: 학습 샘플.단계 모델 모양 교대 전체 어텐션 + 변환 레스블록 깊이 모델 dim 어텐션 헤드 배치 크기 총 학습 65M 샘플 손실 LR B1, BMSE(가중치 1) + VLB(가중치 n) le-.9, .가중치 감소 .LR 워밍업 EMA 감소율 1000단계 .C 향후 작업 표 4: 확산 디코더 세부 정보 및 하이퍼파라미터 Tortoise는 말하자면 제 급여 등급을 훨씬 넘어서는 작업의 산물입니다. 저는 독립적인 연구원으로서 실험을 수행할 GPU가 소수에 불과했고 그 과정에서 많은 실수를 했습니다. Tortoise를 기반으로 하는 향후 작업에서 수행할 아키텍처 조정에 대한 권장 사항은 다음과 같습니다. 1. VQVAE 코드북 임베딩 dim을 제한합니다. 이는 실험적으로 성능이 크게 향상되는 것으로 나타났습니다. 2. 상대 위치 인코딩. AR 모델은 고정된 위치 인코딩을 사용하는데, 이는 생성할 수 있는 총 음성 양을 제한합니다. 상대 인코딩을 사용하면 임의 길이의 시퀀스를 허용합니다. 3. 더 큰 배치 크기에서 CLVP를 훈련합니다. 대조 모델은 매우 큰 배치 크기에서 이점을 얻습니다. 4. 더 긴 오디오 시퀀스에서 CLVP를 훈련합니다. CLVP는 13초 클립만 보았기 때문에 긴 샘플에서 다시 순위를 매기는 것이 어려울 수 있습니다. 5. 확산 디코더 아키텍처. 확산 디코더는 피드포워드 블록을 생략한 주의 네트워크입니다. 회고해보면 이는 잘못된 설계 결정이었고 피드포워드 블록을 포함해야 합니다. 6. 전체 모델 스택을 24kHz에서 훈련하거나 Univnet을 22kHz 샘플링 속도로 다시 훈련합니다. 7. 더 많은 데이터에서 더 오랫동안 훈련합니다. TorToise의 훈련 곡선은 과적합과는 거리가 멀다는 것을 나타냅니다. 단순히 더 오래 훈련하면 결과가 개선되었을 것입니다. D 특별 감사 연구 커뮤니티에서 수행한 이전 작업보다 이 프로젝트는 오픈 소스 커뮤니티의 산물이었습니다. 위에 언급되지 않았지만 Tortoise를 만드는 데 도움이 되었다고 생각되는 몇몇 기여자에게 감사드리고 싶습니다.1. Wang(2021)과 Wang(2020)을 쓴 Phil Wang.2. Seonghyeon(2019)을 쓴 Kim Seonghyeon.3. 내가 사용하는 대부분의 도구를 유지 관리하고 Tortoise의 기반 기술 대부분을 오픈 소스로 공개한 FAIR.4. Nichol과 Dhariwal(2021)을 만든 Prafulla Dhariwal과 Alex Nichol이 없었다면 아직도 GAN 지옥에 있었을 것입니다.또한 이 시스템을 만드는 데 필요한 2년 동안 높은 전기 요금, 덥고 시끄러운 다용도실, 그리고 늦은 밤까지 버텨내며 나를 지원해준 아내 Kim Betker에게도 감사드리고 싶습니다.</em>
