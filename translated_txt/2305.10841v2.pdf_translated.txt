--- ABSTRACT ---
영어: 상징적 음악 생성은 제공된 소스 트랙을 기반으로 대상 악기 트랙을 생성하는 것과 같이 사용자가 음악을 작곡하는 데 도움이 될 수 있는 음표를 만드는 것을 목표로 합니다. 미리 정의된 트랙 앙상블과 다양한 작곡 요구 사항이 있는 실제 시나리오에서 다른 트랙을 기반으로 모든 대상 트랙을 생성할 수 있는 효율적이고 효과적인 생성 모델이 중요해집니다. 그러나 이전의 노력은 음악 표현과 모델의 한계로 인해 이러한 필요성을 해결하는 데 부족했습니다. 이 논문에서는 GETMusic이라는 프레임워크를 소개합니다. &quot;GET&quot;은 &quot;GENerate music Tracks&quot;의 약자입니다. 이 프레임워크는 새로운 음악 표현 &quot;GETScore&quot;와 확산 모델 &quot;GETDiff&quot;를 포함합니다. GETScore는 음표를 토큰으로 표현하고 토큰을 2D 구조로 구성하며, 트랙은 수직으로 쌓이고 시간이 지남에 따라 수평으로 진행됩니다. 훈련 단계에서 음악 작품의 각 트랙은 무작위로 대상 또는 소스로 선택됩니다. 훈련에는 두 가지 프로세스가 포함됩니다. 순방향 프로세스에서 대상 트랙은 토큰을 마스크하여 손상되고 소스 트랙은 기준 진실로 유지됩니다. 노이즈 제거 프로세스에서 GETDiff는 소스 트랙을 조건으로 마스크된 타겟 토큰을 예측하도록 훈련됩니다. 비자기 회귀 생성 모델과 결합된 제안된 표현은 GETMusic이 임의의 소스-타겟 트랙 조합으로 음악을 생성할 수 있도록 합니다. 실험 결과 다재다능한 GETMusic이 특정 작곡 작업에 대해 제안된 이전 작업보다 성능이 우수함을 보여줍니다. 1
--- METHOD ---
s [5; 18; 26]는 일반적으로 하나의 특정 소스-대상 트랙 조합 또는 트랙의 연속에 초점을 맞춥니다. 반면, 이미지 기반 연구는 음악을 2D 이미지로 표현하며, 피아노롤³이 인기 있는 선택입니다. 피아노롤은 음표를 수평선으로 표현하며, 수직 위치는 피치를 나타내고 길이는 지속 시간을 나타냅니다. 피아노롤은 트랙을 명시적으로 분리하지만 악기의 전체 피치 범위를 통합해야 하므로 크고 희소한 이미지가 생성됩니다. 희소하고 고해상도의 이미지를 생성하는 과제로 인해 대부분의 연구는 단일 소스 또는 대상 트랙[6; 25; 20]만 포함하는 조건부 구성 또는 무조건적 생성[17]에 초점을 맞췄습니다. 유연하고 다양한 소스-대상 트랙 조합에서 생성을 지원하기 위해 GETMusic(&quot;GET&quot;은 GEnerate music Tracks의 약자)이라는 통합 표현 및 확산 프레임워크를 제안합니다. 이는 GETScore라는 표현과 GETDiff라는 이산 확산 모델[1]로 구성됩니다. GETScore는 트랙이 수직으로 쌓이고 시간이 지남에 따라 수평으로 진행되는 2D 구조로 음악을 표현합니다. 각 트랙 내에서 동일한 시작점을 가진 음표를 단일 피치 토큰과 단일 지속 시간 토큰으로 효율적으로 표현하고 시작 시간을 기준으로 배치합니다. 학습 단계에서 학습 샘플의 각 트랙은 대상 또는 소스로 무작위로 선택됩니다. 학습은 두 가지 프로세스로 구성됩니다. 순방향 프로세스에서 대상 트랙은 마스킹 토큰으로 손상되고 소스 트랙은 기준 진실로 보존됩니다. 노이즈 제거 프로세스에서 GETDiff는 제공된 소스를 기반으로 마스킹된 대상 토큰을 예측하는 방법을 학습합니다. GETMusic의 공동 설계 표현 및 확산 모델은 이전 작업에 비해 여러 가지 이점을 제공합니다. • GETScore의 별도 및 시간적으로 정렬된 트랙과 비자기 회귀 생성 모델을 결합하여 GETMusic은 다양한 소스-대상 조합에서 음악을 능숙하게 작곡합니다. • GETScore는 트랙 내 및 트랙 간 동시 음표 간의 상호 의존성을 효과적으로 보존하는 동시에 컴팩트한 다중 트랙 음악 표현으로, 조화로운 음악 생성을 촉진합니다. ³https://en.wikipedia.org/wiki/Piano_roll피아노 멜로디드럼 피아노 멜로디 드럼 E (a) 악보 바로, TS4/4, 포지셔노, BPM 120, 트랙 피아노, 피치 A3, 지속시간 2, 속도 62, 피치 c4, 지속시간 2, 속도 62, 피치F4, 지속시간 2, 속도 62, 바로, TS 4/4, 포지션, BPM 120, 트랙 드럼, 피치 심벌즈 2, 속도 62, 피치 베이스 드럼, 속도 62, 바로, TS4/4, 포지션2, BPM 120, 트랙 피아노, 피치 F3, 지속시간 2, 속도 62, 바로, TS 4/4, 포지션 2, 트랙 멜로디, 피치 F3, &lt; 지속시간, 속도 62, 바0, TS 4/4, 포지션 2, 트랙 드럼, 피치 심벌 1, 벨로시티 62, Bar0, TS 4/4, 포지션 4, BPM 120, 트랙 피아노, 피치 A3, 지속 시간 2, 벨로시티 62, 피치 c4, 지속 시간 2, 벨로시티 62, 피치 F4, 지속 시간 2, 벨로시티 62, (b) 시퀀스 표현 시간 단위 피치 토큰 지속 시간 토큰 패딩 피아노 픽셀 멜로디 드럼 22NNNN N&#39;Nia N/ 00 5555000955566 -220(c) 피아노 롤 (d) GETScore 그림 2: 같은 음악에 대한 다른 표현. 그림 (a)는 악보입니다. 그림 (b)는 REMI[13] 스타일의 시퀀스 기반 표현을 보여주며, 시퀀스의 길이로 인해 그림 (a)에서 점선 상자로 둘러싸인 부분만 보여줍니다. 그림(c)는 선으로 노트를 표현하는 희소 피아노롤을 보여줍니다. 그림(d)에서 GETScore는 트랙을 분리하고 정렬하여 다양한 소스-대상 조합에서 생성을 통합하기 위한 기반을 형성합니다. 또한 동시 노트 간의 상호 의존성을 보존하여 음악 생성에서 조화를 촉진합니다. (d)의 숫자는 데모용 토큰 인덱스를 나타냅니다. • 트랙별 생성을 넘어 GETDiff의 마스크 및 노이즈 제거 메커니즘은 제로 샷 생성(즉, GETScore의 임의의 위치에서 마스크된 토큰의 노이즈 제거)을 가능하게 하여 다양성과 창의성을 더욱 향상시킵니다. 이 논문에서 우리의
--- EXPERIMENT ---
s는 다재다능한 GETMusic이 특정 작곡 작업에 대해 제안된 이전 작업보다 성능이 우수함을 보여줍니다. 1 서론 상징적 음악 생성은 음악 작곡에 도움이 될 수 있는 음표를 만드는 것을 목표로 합니다. 유연하고 다양한 음악 작곡에 대한 실질적인 요구로 인해 다른 트랙을 기반으로 임의의 트랙을 생성할 수 있는 효율적이고 통합된 접근 방식에 대한 필요성이 높습니다². 그러나 현재 연구는 표현과 모델에 의해 부과되는 고유한 한계로 인해 이러한 요구를 충족하지 못합니다. 결과적으로 이러한 접근 방식은 멜로디를 기반으로 피아노 반주를 생성하는 것과 같은 특정 소스-대상 조합으로 제한됩니다. *연락처: Xu Tan(xuta@microsoft.com) 및 Rui Yan(ruiyan@ruc.edu.cn). 2 음악은 일반적으로 여러 악기 트랙으로 구성됩니다. 이 논문에서는 미리 정의된 트랙 앙상블이 주어지면 생성할 트랙을 &quot;대상 트랙&quot;이라고 하고 조건으로 작용하는 트랙을 &quot;소스 트랙&quot;이라고 합니다. 이러한 트랙 오케스트레이션을 &quot;소스-대상 조합&quot;이라고 합니다. 사전 인쇄. 검토 중. 상징적 음악 피아노 현 멜로디 트랙 드럼 H GETScore 시간 트랙 → GETScore 마스크된 시간 마스크된 GETDiff 마스크된 M -- 마스크된 토큰의 노이즈 제거 -➤ ⇧ ↑ ⇧ 상징적 음악 그림 1: 새로운 음악 표현인 &quot;GETScore&quot;와 이산 확산 모델인 &quot;GETDiff&quot;를 포함하는 GETMusic의 개요. 미리 정의된 악기 트랙 앙상블이 주어지면 GETDiff는 GETScores를 입력으로 사용하여 모든 소스 트랙(①, ②, ③)을 조건으로 원하는 대상 트랙을 생성할 수 있습니다. 이러한 유연성은 트랙별 생성을 넘어 마스크된 모든 부분에 대해 제로 샷 생성을 수행할 수 있습니다(4). 현재 연구는 음악 표현에 따라 시퀀스 기반과 이미지 기반의 두 가지 주요 접근 방식으로 분류할 수 있습니다. 한편, 시퀀스 기반 작품[13; 27; 3]은 음악을 이산 토큰의 시퀀스로 표현하는데, 여기서 음표는 시작, 피치, 지속 시간, 악기와 같은 속성을 설명하기 위해 여러 토큰이 필요합니다. 이러한 토큰은 연대순으로 배열되어 다른 트랙의 음표가 끼어들게 되고 일반적으로 자기 회귀 모델에 의해 순차적으로 예측됩니다. 트랙의 끼어들기는 자기 회귀 모델이 대상 트랙 토큰을 출력할 시기를 암묵적으로 결정하고 다른 트랙에서 토큰을 생성하지 않기 때문에 정확한 대상 생성이라는 과제를 제기합니다. 또한 소스 및 대상 트랙의 사양을 복잡하게 만듭니다. 따라서 기존 방법[5; 18; 26]은 일반적으로 하나의 특정 소스-대상 트랙 조합이나 트랙의 연속에 초점을 맞춥니다.반면에 이미지 기반 연구는 음악을 2D 이미지로 표현하며, 피아노롤³이 인기 있는 선택입니다.피아노롤은 음표를 수평선으로 표현하며, 수직 위치는 피치를 나타내고 길이는 지속 시간을 나타냅니다.피아노롤은 트랙을 명시적으로 분리하지만 악기의 전체 피치 범위를 통합해야 하므로 크고 희소한 이미지가 생성됩니다.희소하고 고해상도의 이미지를 생성하는 과제로 인해 대부분의 연구는 단일 소스 또는 대상 트랙[6; 25; 20]만 포함하는 조건부 구성이나 무조건적 생성[17]에 초점을 맞췄습니다.유연하고 다양한 소스-대상 트랙 조합에서 생성을 지원하기 위해 GETMusic(&quot;GET&quot;은 GEnerate music Tracks의 약자)이라는 통합 표현 및 확산 프레임워크를 제안합니다.이 프레임워크는 GETScore라는 표현과 GETDiff라는 이산 확산 모델[1]로 구성됩니다. GETScore는 트랙이 수직으로 쌓이고 시간이 지남에 따라 수평으로 진행되는 2D 구조로 음악을 표현합니다. 각 트랙 내에서 동일한 시작점을 가진 음표를 단일 피치 토큰과 단일 지속 시간 토큰으로 효율적으로 표현하고 시작 시간을 기준으로 배치합니다. 학습 단계에서 학습 샘플의 각 트랙은 대상 또는 소스로 무작위로 선택됩니다. 학습은 두 가지 프로세스로 구성됩니다. 순방향 프로세스에서 대상 트랙은 마스킹 토큰으로 손상되고 소스 트랙은 기준 진실로 보존됩니다. 노이즈 제거 프로세스에서 GETDiff는 제공된 소스를 기반으로 마스킹된 대상 토큰을 예측하는 방법을 학습합니다. GETMusic의 공동 설계 표현 및 확산 모델은 이전 작업에 비해 여러 가지 이점을 제공합니다. • GETScore의 별도 및 시간적으로 정렬된 트랙과 비자기 회귀 생성 모델을 결합하여 GETMusic은 다양한 소스-대상 조합에서 음악을 능숙하게 작곡합니다. • GETScore는 트랙 내 및 트랙 간 동시 음표 간의 상호 의존성을 효과적으로 보존하는 동시에 컴팩트한 다중 트랙 음악 표현으로, 조화로운 음악 생성을 촉진합니다. ³https://en.wikipedia.org/wiki/Piano_roll피아노 멜로디드럼 피아노 멜로디 드럼 E (a) 악보 바로, TS4/4, 포지셔노, BPM 120, 트랙 피아노, 피치 A3, 지속시간 2, 속도 62, 피치 c4, 지속시간 2, 속도 62, 피치F4, 지속시간 2, 속도 62, 바로, TS 4/4, 포지션, BPM 120, 트랙 드럼, 피치 심벌즈 2, 속도 62, 피치 베이스 드럼, 속도 62, 바로, TS4/4, 포지션2, BPM 120, 트랙 피아노, 피치 F3, 지속시간 2, 속도 62, 바로, TS 4/4, 포지션 2, 트랙 멜로디, 피치 F3, &lt; 지속시간, 속도 62, 바0, TS 4/4, 포지션 2, 트랙 드럼, 피치 심벌 1, 벨로시티 62, Bar0, TS 4/4, 포지션 4, BPM 120, 트랙 피아노, 피치 A3, 지속 시간 2, 벨로시티 62, 피치 c4, 지속 시간 2, 벨로시티 62, 피치 F4, 지속 시간 2, 벨로시티 62, (b) 시퀀스 표현 시간 단위 피치 토큰 지속 시간 토큰 패딩 피아노 픽셀 멜로디 드럼 22NNNN N&#39;Nia N/ 00 5555000955566 -220(c) 피아노 롤 (d) GETScore 그림 2: 같은 음악에 대한 다른 표현. 그림 (a)는 악보입니다. 그림 (b)는 REMI[13] 스타일의 시퀀스 기반 표현을 보여주며, 시퀀스의 길이로 인해 그림 (a)에서 점선 상자로 둘러싸인 부분만 보여줍니다. 그림(c)는 선으로 노트를 표현하는 희소 피아노롤을 보여줍니다. 그림(d)에서 GETScore는 트랙을 분리하고 정렬하여 다양한 소스-대상 조합에서 생성을 통합하기 위한 기반을 형성합니다. 또한 동시 노트 간의 상호 의존성을 보존하여 음악 생성에서 조화를 촉진합니다. (d)의 숫자는 데모용 토큰 인덱스를 나타냅니다. • 트랙별 생성을 넘어 GETDiff의 마스크 및 노이즈 제거 메커니즘은 제로 샷 생성(즉, GETScore의 임의의 위치에서 마스크된 토큰의 노이즈 제거)을 가능하게 하여 다양성과 창의성을 더욱 향상시킵니다. 이 논문에서 우리의 실험은 베이스, 드럼, 기타, 피아노, 현악기, 멜로디의 여섯 가지 악기를 고려하여 665개의 소스-대상 조합을 도출합니다(부록 A 참조). 우리는 제안된 다재다능한 GETMusic이 조건부 반주 또는 멜로디 생성과 같은 특정 작업에 대해 제안된 접근 방식과 처음부터 생성하는 방식을 능가한다는 것을 보여줍니다. 2 배경 2.1 상징적 음악 생성 상징적 음악 생성은 처음부터 [17; 26] 또는 코드, 트랙 [20; 13; 6], 가사 [16; 14; 19] 또는 기타 음악적 속성 [28]과 같은 주어진 조건에 따라 음악적 음표를 생성하여 사용자가 음악을 작곡하는 데 도움을 주는 것을 목표로 합니다. 실제 음악 작곡에서 일반적인 사용자 요구 사항은 처음부터 악기 트랙을 만들거나 기존 트랙을 조건으로 만드는 것입니다. 미리 정의된 트랙 앙상블이 주어지고 실제로 유연한 작곡 요구 사항을 고려할 때 임의의 소스-대상 조합을 처리할 수 있는 생성 모델이 중요합니다. 그러나 기존 접근 방식 중 어느 것도 주로 표현 및 모델에 내재된 한계로 인해 여러 소스-대상 조합에 걸친 생성을 통합할 수 없습니다. 현재 접근 방식은 채택된 표현과 관련하여 시퀀스 기반과 이미지 기반의 두 가지 주요 범주로 광범위하게 분류할 수 있습니다. 시퀀스 기반 방법 [13; 12; 27; 18]에서 음악은 개별 토큰의 시퀀스로 표현됩니다. 토큰은 음표의 특정 속성, 예를 들어 음표의 시작 시간(음표의 시작 시간), 피치(음표 빈도), 지속 시간 및 악기에 해당하며 토큰은 일반적으로 연대순으로 배열됩니다. 결과적으로 서로 다른 트랙을 나타내는 음표는 일반적으로 그림 2(b)에서와 같이 색상으로 트랙을 구분하여 서로 엇갈리게 배치됩니다. 일반적으로 자기 회귀 모델을 적용하여 시퀀스를 처리하고 토큰을 하나씩 예측합니다. 엇갈린 트랙과 자기 회귀 생성은 모델이 원하는 대상 트랙의 토큰을 언제 출력할지 암묵적으로 결정하고 다른 트랙에 속하는 토큰을 통합하지 않도록 강제하여 원하는 트랙의 정확한 생성에 어려움을 줍니다. 순차적 표현 및 모델링은 생성된 음악의 하모니에 영향을 미치는 동시 음표 간의 상호 종속성을 명시적으로 보존하지 않습니다. 또한 모델은 긴 시퀀스를 감안할 때 장기 종속성[2]을 학습할 수 있는 능력이 매우 뛰어나야 합니다. 일부 비전통적 방법[7]은 트랙 엇갈림을 제거하기 위해 트랙 순서에 따라 토큰을 구성합니다. 그러나 장기적으로나 트랙 전체에서 종속성이 약해지므로 트레이드오프가 따릅니다.이미지 기반 방법은 주로 2D 이미지에서 음표를 수평선으로 표현하는 피아노롤 표현을 사용하며, 수직 위치는 피치를 나타내고 길이는 지속 시간을 나타냅니다.그러나 피아노롤은 악기의 전체 피치 범위를 포함해야 하므로 크고 희소한 이미지가 생성됩니다.예를 들어, 그림 2(c)는 수백 픽셀 너비에 걸쳐 있지만 그 안의 굵은 선만이 음악 정보를 전달하는 3트랙 음악 작품의 피아노롤 표현을 보여줍니다.대부분의 작품은 단일 소스/대상 트랙[6; 25; 20]만 포함하는 조건부 작곡이나 희소하고 고해상도 이미지를 생성하는 것이 어렵기 때문에 무조건 생성[17]에 중점을 둡니다.제안된 GETMusic은 공동 설계 표현과 이산 확산 모델을 통해 위의 한계를 해결하여 다양한 트랙 생성에 대한 효과적인 솔루션을 제공합니다. 2.2 확산 모델 [21]에서 처음 제안하고 이후 연구 [9; 22; 10; 4]에서 더욱 개선된 확산 모델은 복잡한 분포를 모델링하는 데 인상적인 역량을 보여주었습니다.이러한 모델은 순방향(확산) 프로세스와 역방향(잡음 제거) 프로세스라는 두 가지 핵심 프로세스로 구성됩니다.순방향 프로세스 q(x1:T|xo) q(x+|xt−1)은 원래 데이터 x에 잡음을 반복적으로 도입하여 T 단계 동안 x0와 독립적인 사전 분포 p(x)로 손상시킵니다.확산 모델의 목표는 x를 점진적으로 데이터 분포로 잡음을 제거하는 역방향 프로세스 po(xt−1|x+)를 학습하는 것입니다. 모델은 변분 하한(VLB) [9]을 최적화하여 학습합니다.Lvlb = T = Eq[− log pe(xo|x1)] + Σ DKL [q(Xt−1|Xt, X0)||Po(xt−1|xt))] + DkL[q(XT|x0)||P(XT)]].(1) t=확산 모델은 연속 및 이산 버전으로 분류할 수 있습니다.제안하는 GETScore가 음악을 이산 토큰의 2차원 배열로 나타내므로 이 방법에서 이산 확산 프레임워크를 사용합니다.[21]의 이산 확산 모델은 이진 시퀀스 학습을 위해 개발되었습니다.[11]은 이러한 모델을 확장하여 범주형 난수 변수를 처리하는 반면, [1]은 보다 구조화된 범주형 순방향 프로세스를 도입했습니다.순방향 프로세스는 전이 행렬로 정의된 마르코프 체인으로, 시간 t−1에서 토큰을 시간 t에서 확률에 따라 다른 토큰으로 전이합니다. 우리의 확산 모델 GETDiff의 경우, 우리는 기반으로서 그들의 순방향 프로세스를 채택합니다.우리는 또한 xo-매개변수화[1]로 알려진 중요한 기술을 채택합니다.이 기술에서는 시간 단계 t에서 xt-1을 직접 예측하는 대신, 모델은 노이즈가 없는 원본 데이터 xo에 맞게 학습하고 예측된 ão를 손상시켜 xt-1을 얻습니다.결과적으로, 하이퍼 매개변수 \로 스케일된 보조 항이 VLB에 추가됩니다: Lx = T Lvlb log pe(xo| xt) t=3 GETMusic 이 섹션에서는 GETMusic의 두 가지 핵심 구성 요소인 표현 GETScore와 확산 모델 GETDiff를 소개합니다.먼저 각 구성 요소에 대한 개요를 제공한 다음, 모든 트랙의 유연하고 다양한 생성을 지원하는 데 있어서 각각의 장점을 강조합니다.3.1 GETScore 우리의 목표는 다중 트랙 음악을 모델링하기 위한 효율적이고 효과적인 표현을 설계하는 것입니다.이를 통해 소스 및 타겟 트랙을 유연하게 지정하고 다양한 트랙 생성 작업의 기반을 마련할 수 있습니다. 새로운 표현인 GETScore에는 두 가지 핵심 아이디어가 포함됩니다.(1) 2D 트랙 배열 및 (2) 음표 토큰화.트랙 배열 우리는 악보에서 영감을 얻어 트랙을 수직으로 배열하고 각 트랙은 시간이 지남에 따라 수평으로 진행됩니다.수평 축은 세분화된 시간 단위로 나뉘며 각 단위는 16분음표의 길이와 같습니다.이 수준의 시간적 세부 정보는 대부분의 교육 데이터에 충분합니다.이러한 트랙 배열은 여러 가지 이점을 제공합니다.· 다른 트랙의 내용이 섞이는 것을 방지하여 소스 및 대상 트랙의 지정을 간소화하고 원하는 트랙을 정확하게 생성할 수 있습니다.· 트랙은 악보처럼 시간적으로 정렬되므로 상호 의존성이 잘 보존됩니다.음표 토큰화 음표를 표현하기 위해 우리는 구성과 직접 관련된 피치와 길이라는 두 가지 속성에 초점을 맞춥니다.속도 및 템포 변화와 같은 일부 동적 요소는 연구 범위를 벗어납니다.우리는 각각 음표의 피치와 길이를 나타내는 데 두 개의 별개의 토큰을 사용합니다. 이러한 쌍을 이룬 피치-지속 토큰은 GETScore 내의 시작 시간과 트랙에 따라 배치됩니다. GETScore 내의 일부 위치는 토큰에 의해 차지되지 않을 수 있습니다. 이러한 경우 그림 2(d)의 빈 블록에서 설명한 대로 패딩 토큰을 사용하여 채웁니다. 각 트랙에는 고유한 피치 토큰 어휘가 있지만 피치 특성은 악기에 따라 달라지는 반면 지속 시간은 모든 트랙에서 보편적인 특징이므로 공통적인 지속 시간 어휘를 공유합니다. GETScore의 적용 범위를 확대하려면 두 가지 문제를 더 해결해야 합니다. (1) 트랙 내에서 동시에 연주되는 음표 그룹을 나타내기 위해 단일 피치 및 지속 시간 토큰을 사용하는 방법은 무엇입니까? 동시 음표 그룹의 피치 토큰을 단일 복합 피치 토큰으로 병합하는 것을 제안합니다. 또한 그룹 내에서 가장 자주 발생하는 지속 시간 토큰을 최종 지속 시간 토큰으로 식별합니다. 지속 시간 표현의 이러한 단순화는 전체 교육 데이터에서 관찰한 결과에서 뒷받침되는데, 97% 이상의 그룹의 음표가 동일한 지속 시간을 공유합니다. 단 0.5%의 그룹에서만 음표 간의 최대 지속 시간 차이가 시간 단위를 초과합니다. 이러한 결과는 이러한 단순화가 GETScore의 표현적 품질에 미치는 영향이 미미함을 시사합니다. 그림 2(d)는 복합 토큰을 보여줍니다. 피아노 트랙에서 처음 세 음표 &quot;A&quot;, &quot;C&quot; 및 &quot;F&quot;를 &quot;147&quot;로 색인된 단일 토큰으로 병합합니다. (2) &quot;피치&quot; 및 &quot;지속 시간&quot;이라는 개념이 포함되지 않은 드럼과 같은 타악기를 표현하는 방법은 무엇입니까? 개별 드럼 액션(예: 킥, 스네어, 햇, 톰 및 심벌즈)을 피치 토큰으로 처리하고 특수 지속 시간 토큰과 맞춥니다. 그림 2(d)의 드럼 트랙은 우리의 접근 방식을 보여줍니다.
--- CONCLUSION ---
, 트랙 배열의 이점 외에도 GETScore는 이 노트 토큰화를 통해 이점을 얻습니다.• 각 트랙은 피치 및 지속 시간 토큰을 수용하는 데 두 행만 필요하므로 GETScore의 효율성을 크게 향상시킵니다.• 복합 토큰은 트랙 내의 상호 종속성을 보존합니다.생성될 때 해당 노트 그룹이 실제 데이터에서 파생되므로 하모니가 본질적으로 보장됩니다.3.2 GETDiff 이 섹션에서는 먼저 각각 훈련 중 GETDiff의 전방 및 잡음 제거 프로세스를 소개합니다.다음으로 추론 절차를 소개하고 트랙 생성에 대한 다양한 요구 사항을 해결하는 데 GETDiff의 이점을 설명합니다.전방 프로세스 GETMusic은 이산 토큰으로 구성된 GETScore에서 작동하므로 이산 확산 모델을 사용합니다.전방 프로세스의 흡수 상태로 특수 토큰 [MASK]을 어휘에 도입합니다. 시간 t-1에서 일반 토큰은 at의 확률로 현재 상태를 유지하고 t = 1 at의 확률로 [MASK]로 전환됩니다(즉, 노이즈로 손상됨). GETScore에는 GETMusic이 지원하는 고정된 수의 트랙이 포함되고 구성에 항상 모든 트랙이 포함되지 않으므로 관련되지 않은 트랙을 다른 특수 토큰 [EMPTY]로 채웁니다. [EMPTY]는 다른 토큰으로 전환되지 않으며 다른 토큰에서 전환될 수도 없습니다. 이 디자인은 특정 구성에서 관련되지 않은 트랙의 간섭을 방지합니다. 형식적으로, 전이 행렬 [Qt]mn q(xt = m|xt−1 = n) Є RK×K는 전이 =Xt 피아노 멜로디 NNNNNNNN 비운 드럼 Ο Σ Σ Σ ΣΟ ° (a) 임베딩 모듈 21d 。 Σ 3° LL d True + MLP False 임베딩 행렬 조건 플래그 GETDiff xt-디코딩 피아노 모듈 ↑ Roformer 레이어 멜로디 비운 드럼 임베딩 모듈 .....ozo (b) 디코딩 모듈 Gumbel-softmax MLP K입력 행렬(Lxd 모델) 출력 행렬(Lxd 모델) L 그림 3: 3트랙 GETScore를 사용하여 GETDiff를 훈련하는 개요. GETScore는 아무리 많은 트랙이라도 수용할 수 있으며, 이 예는 장난감 예제로 제공됩니다. 이 학습 단계에서 GETMusic은 멜로디 트랙을 무시하고 피아노 트랙을 소스로, 드럼 트랙을 대상으로 무작위로 선택합니다. 따라서 xt는 기준 진실 피아노 트랙, 비워진 멜로디 트랙, 손상된 드럼 트랙으로 구성됩니다. GETDiff는 출력의 토큰을 수정할 수 있는 비자기 회귀 방식으로 모든 토큰을 동시에 생성합니다. 따라서 xt-1을 얻으면 소스는 기준 진실로 복구되고 무시된 트랙은 다시 비워집니다. 시간 t 1에서 n번째 토큰부터 시간 t에서 m번째 토큰까지의 확률: 00에서 0 ... 0αι Qt 0 0LYt It It 10(3) 여기서 K는 두 개의 특수 토큰을 포함한 총 어휘 크기입니다. 행렬의 마지막 두 열은 각각 q(xt|xt−1 = [EMPTY]) 및 q(xt|xt−1 = [MASK])에 해당합니다. v(x)를 x의 범주를 나타내는 원핫 열 벡터로 표시하고 순방향 프로세스의 마르코프 특성을 고려하면 시간 t에서의 마진과 시간 t — - 1에서의 사후를 다음과 같이 표현할 수 있습니다.q(xt|xo) = = v(xt)Qtv(xo), 여기서 Qt = Qt . . . Q1.q(xt-1❘xt, xo) q(xt|xt−1,xo)q(xt−1|x0) q(x+|xo) (u (xt)Qto(2t−1)) (•T (t-1)t_1(zo)) v (xt) Qtv(x0) 추적 가능한 사후를 사용하여 Eq.2로 GETDiff를 최적화할 수 있습니다.(4) (5) 노이즈 제거 프로세스 그림 3은 GETMusic이 L 시간 단위 길이의 3트랙 학습 샘플에 대한 노이즈를 제거하는 과정을 간략하게 보여줍니다. GETDiff에는 세 가지 주요 구성 요소가 있습니다.임베딩 모듈, Roformer[23] 레이어, 디코딩 모듈입니다.Roformer는 상대적 위치 정보를 어텐션 행렬에 통합하는 변환기[24] 변형으로, 추론 중에 모델의 길이 외삽 능력을 향상시킵니다.훈련 중에 GETMusic은 21개 행의 GETScore로 표현되는 I개 트랙의 음악 작품에 대한 다양한 소스-대상 조합을 다루어야 합니다.이를 달성하기 위해 m개 트랙(GETScore에서 2m개 행 생성)이 소스로 무작위로 선택되고, n개 트랙(GETScore에서 2n개 행 생성)이 대상으로 선택됩니다.m ≥ 0, n &gt; 0, m + n≤I.무작위로 샘플링된 시간 t에서 원래 GETScore xo에서 xt를 구하기 위해 대상 트랙의 토큰은 Qt에 따라 전환되고, 소스 트랙의 토큰은 기준 진실로 유지되고, 관련되지 않은 트랙은 비워집니다. GETDiff는 그림 3에 표시된 것처럼 4단계로 xt의 노이즈를 제거합니다. (1) GETScore의 모든 토큰은 d차원 임베딩에 임베딩되어 크기가 2Idx L인 임베딩 행렬을 형성합니다. (2) 학습 가능한 조건 플래그가 행렬에 추가되어 토큰을 조건화할 수 있는 GETDiff를 안내하고, 이를 통해 추론 성능을 향상시킵니다. 조건 플래그의 효과성은 § 4.3에서 분석합니다. (3) 임베딩 행렬은 MLP를 사용하여 GETDiff의 입력 차원 dmodel에 맞게 크기가 조정된 다음 Roformer 모델에 입력됩니다. (4) 출력 행렬은 분류 헤드를 통과하여 크기 K의 어휘에 대한 토큰 분포를 얻고 gumbel-softmax 기법을 사용하여 최종 토큰을 얻습니다. 추론 추론 중에 사용자는 모든 대상 및 소스 트랙을 지정할 수 있으며, GETMusic은 소스 트랙, 마스크된 대상 트랙 및 비워진 트랙(있는 경우)의 기준 진실을 포함하는 xT로 표시된 해당 GETScore 표현을 구성합니다.그런 다음 GETMusic은 XT의 노이즈를 단계별로 제거하여 xo를 얻습니다.GETMusic은 비자기 회귀 방식으로 모든 토큰을 동시에 생성하여 출력에서 소스 토큰을 잠재적으로 수정하므로 소스 트랙의 일관된 안내를 보장해야 합니다.xt-1을 획득하면 소스 트랙의 토큰은 기준 진실 값으로 복구되는 반면 관련되지 않은 트랙의 토큰은 다시 비워집니다.표현과 확산 모델의 결합된 이점을 고려할 때 GETMusic은 다양한 작곡 요구 사항을 해결하는 데 두 가지 주요 이점을 제공합니다.● GETMusic은 통합 확산 모델을 통해 재교육 없이도 다양한 소스-대상 조합에 걸쳐 음악을 작곡할 수 있습니다. • 트랙별 생성을 넘어 GETDiff의 마스크 및 노이즈 제거 메커니즘은 GETScore에서 임의의 마스크된 위치의 제로샷 생성을 가능하게 하여 다양성과 창의성을 더욱 향상시킵니다. 이에 대한 예는 그림 1의 사례 ④에서 찾을 수 있습니다. 4 실험 4.1 실험 설정 데이터 및 전처리 Musescore 4에서 1,569,469개의 MIDI 파일을 크롤링했습니다. [18]에 따라 데이터를 전처리하여 베이스, 드럼, 기타, 피아노, 현악기, 멜로디 및 추가 코드 진행 트랙의 I = 6 악기 트랙이 포함된 음악을 만들었습니다. 엄격한 클렌징 및 필터링 후 최대 L을 512로 하여 137,812개의 GETScore(약 2,800시간)를 구성했습니다. 이 중 1,000개는 검증용으로, 100개는 테스트용으로, 나머지는 학습용으로 샘플링했습니다. 크롤링된 데이터에서 모든 베이스라인을 학습합니다. 어휘 크기 K는 11,883입니다. 데이터와 사전 처리에 대한 자세한 내용은 부록 B에 있습니다.학습 세부 정보 확산 타임스텝 T = 100과 보조 손실 스케일 λ = 0.001을 설정했습니다. 전이 행렬 Qt의 경우 √t(누적 Yt)를 0에서 1로 선형적으로 증가시키고 from에서 0으로 감소시킵니다. GETDiff에는 d = 96, dmodel = 768인 12개의 Roformer 레이어가 있으며, 여기에는 약 86M개의 학습 가능한 매개변수가 있습니다. 학습하는 동안 학습 속도가 1e – 4, ẞ1 = 0.9, 2=0.999인 AdamW 옵티마이저를 사용합니다. 학습 속도는 먼저 1000단계 워밍업한 다음 선형적으로 감소합니다. 학습은 8 × 32G Nvidia V100 GPU에서 수행되며 각 GPU의 배치 크기는 3입니다. 모델을 50개 에포크 동안 학습하고 1000단계마다 검증하는데, 총 70시간이 걸립니다. 검증 손실을 기반으로 모델 매개변수를 선택합니다. 작업 및 기준선 세 가지 상징적 음악 생성 작업을 고려합니다. (1) 멜로디 기반 반주 생성, (2) 반주 기반 멜로디 생성, (3) 처음부터 트랙 생성. 처음 두 작업의 경우 GETMusic을 PopMAG[18]와 비교합니다. PopMAG는 시퀀스 표현 MuMIDI를 처리하는 자기 회귀 변환기 인코더-디코더 모델입니다. [18]에 따라 추가 코드 진행이 더 많은 구성 지침을 제공하고 코드 진행을 GETScore의 다른 트랙으로 처리합니다(부록 B 참조). 비교를 위해 생성된 음악을 최대 128비트 길이로 제한하는데, 이는 PopMAG의 가장 긴 구성 길이입니다. 세 번째 과제에서는 GETMusic을 가장 경쟁력 있는 무조건 생성 모델 중 하나인 Museformer[26]와 비교합니다. 각 곡의 비트가 128개로 제한된 100곡의 6개 트랙을 처음부터 생성합니다. 평가 생성 품질을 정량적으로 평가하는 객관적인 지표를 도입합니다. [18]에 따라 두 가지 측면에서 모델을 평가합니다. 4 https://musescore.com/ 표 1: 반주/멜로디 생성과 처음부터 생성하는 세 가지 대표적 과제를 통해 GETMusic을 PopMAG 및 Museformer와 비교합니다. 모든 인간 평가에서 k 값은 지속적으로 0.6을 초과하여 평가자 간에 상당한 동의를 나타냅니다. 방법 | CA(%) ↑ KLPitch↓ KLDur↓ KLIOI HR ↑ 반주 생성 PopMAG GETMusic 61.10.7.6.2.65.10.4.4.3.리드 멜로디 생성 PopMAG 73.10.3.4.3.GETMusic 81.9.3.3.3.Generation from Scratch Museformer GETMusic 8.3.5.3.7.3.5.3.(1) 코드 정확도: 작업 1 및 2의 경우 생성된 대상 트랙과 기준 진실 간의 코드 정확도 CA를 측정하여 멜로디 일관성을 평가합니다. Ntracks NchordsCA = Ntracks Nchords i=Σ 1 (Ci,j = Ci,j).j=(6) 여기서 Ntracks 및 Nchords는 각각 트랙 및 코드의 수를 나타냅니다. C. 및 Cij는 각각 i번째 생성된 대상 트랙의 j번째 코드와 기준 진실을 나타냅니다. 이 메트릭은 세 번째 작업에 적합하지 않습니다. 대신, 세 번째 작업에 대한 멜로디 평가는 나중에 논의할 피치 분포와 인간 평가에 모두 의존합니다. (2) 특징 분포 발산: 처음 두 작업의 경우 생성된 트랙과 기준 진실 트랙에서 몇 가지 중요한 음악적 특징의 분포를 평가합니다. 음표 피치, 지속 시간(Dur) 및 막대 내 두 연속 음표 사이의 시간 간격을 측정하는 Inter-Onset Interval(IOI)입니다. 먼저 음표 피치, 지속 시간 및 IOI를 16개 클래스로 양자화한 다음 가우시안 커널 밀도 추정을 사용하여 히스토그램을 확률 밀도 함수(PDF)로 변환합니다. 마지막으로 생성된 대상 트랙의 PDF와 기준 진실 간의 KL-발산[15] KL{피치, Dur, IOI}를 계산합니다. 세 번째 과제에서는 생성된 대상 트랙의 PDF와 해당 교육 데이터 분포 간의 KL{피치, 지속 시간, IOI}를 계산합니다.(4) 인간 평가: 기본적인 음악 지식이 있는 10명의 평가자를 모집했습니다. 그들에게 GETMusic에서 생성한 노래와 블라인드 테스트의 기준선이 제시되었습니다. 평가자는 1(불량)에서 5(우수)까지의 척도로 인간 평가(HR)를 제공했습니다. HR 평가는 생성된 노래의 전반적인 품질과 대상 트랙과 소스 트랙 간의 일관성(해당되는 경우)을 반영합니다. 4.2 이전 작업과의 생성 결과 비교 표 1은 세 가지 작곡 과제의 결과를 보여줍니다. 처음 두 과제에서 GETMusic은 모든 지표에서 PopMAG보다 지속적으로 우수한 성과를 보이며 제공된 소스 트랙과 잘 일치하는 더욱 조화로운 멜로디와 리듬으로 음악을 만드는 능력을 보여줍니다. 처음 두 과제를 비교하면 더 많은 소스 트랙을 포함할수록 음악 품질이 향상되었음을 알 수 있습니다. 두 번째 과제에서는 모든 5개의 반주 악기가 소스 트랙으로 사용되며, 멜로디만을 소스 트랙으로 사용하는 첫 번째 과제에 비해 대부분의 지표에서 더 나은 점수를 얻습니다. 무조건 생성에서 GETMusic은 대부분의 지표에서 경쟁 기준선보다 우수한 성과를 보입니다. 주관적인 평가는 GETMusic의 효과를 더욱 확증합니다. 독자는 생성된 샘플을 보려면 데모 페이지를 방문하세요. 제로샷 생성 GETMusic은 트랙별 생성을 위해 훈련되었지만 마스크 및 노이즈 제거 메커니즘으로 인해 임의의 위치에서 마스크된 토큰을 제로샷으로 복구할 수 있습니다. 제로샷 생성은 그림 1의 사례 ④에서 예시됩니다. 이 기능은 GETMusic의 다재다능함과 창의성을 향상시킵니다. 예를 들어, 두 개의 다른 노래 중간에 마스크 토큰을 삽입하여 연결할 수 있습니다.GETMusic은 마스크된 신호의 잡음을 반복적으로 제거하여 조화로운 브리지를 생성합니다.Method PopMAG CA(%) ↑ 61.표 2: 생성 패러다임에 대한 소멸 연구: 자기 회귀 대 비자기 회귀.KLPitch↓ KLDur↓ KLIOI↓ 시간 |HR↑ 10.7.6.23.2.GETMusic (AR) 46.11.7.6.17.2.GETMusic 65.10.4.4.4.3.Method 표 3: 상태 플래그의 효과성에 대한 소멸 연구.| CA↑ KLPitch↓ KLDur↓ KLIOI↓ Loss↓ GETMusic (AG) 65.10.4.4.1.condition flags 45.10.6.5.1.GETMusic (UN) 7.3.5.1.condition flags 8.3.5.1.tokens 나머지 토큰은 변경하지 않고 그대로 유지합니다. 평가의 어려움에도 불구하고 데모 페이지의 8번째와 9번째 데모는 우리 접근 방식의 유연성과 창의성을 보여줍니다. 4.3 방법 분석 GETScore와 GETDiff의 상호 보완적 특성 이를 보여주기 위해 GETDiff를 자기 회귀 모델로 대체하는 절제 연구로 시작합니다. 처음부터 음악을 생성하는 과제를 위해 12개의 예측 헤드가 장착된 변압기 디코더를 훈련합니다. 각 디코딩 단계에서 12개의 토큰(트랙이 포함된 GETScore에서 6개의 피치 토큰과 6개의 기간 토큰)을 예측합니다. GETMusic(AR)으로 표기된 이 변형의 결과는 표 2에 자세히 나와 있으며, 반복적인 멜로디를 생성하는 경향이 특징인 최적이 아닌 결과를 보여줍니다. 또한, Nvidia A100 GPU를 사용하여 각 음악 작품을 작곡하는 데 필요한 평균 시간을 초 단위로 제시하여 비자기 회귀적 노이즈 제거 프로세스가 속도 면에서 자기 회귀 디코딩을 크게 앞지른다는 점을 강조합니다. 기존 시퀀스 표현으로 학습한 확산 모델을 평가하는 것이 더 유익할 수 있지만, 이 접근 방식은 다루기 힘듭니다. 첫째, 자기 회귀 모델에 비해 확산 모델을 학습하는 데 본질적으로 더 높은 계산 리소스 요구 사항과 기존 시퀀스 표현이 동일한 음악 작품을 표현할 때 일반적으로 GETScore보다 한 자릿수 더 길다는 사실로 인해 학습 비용이 엄청나게 높아집니다. 더욱이, 확산 모델은 사전에 세대 길이를 지정해야 합니다. 그러나 동일한 수의 막대를 나타내는 기존 시퀀스의 길이는 매우 다양하여 생성된 음악의 길이와 구조에 제어할 수 없는 변화가 발생할 수 있습니다.위의 결과와 분석을 바탕으로 제안한 GETScore와 GETDiff가 함께 다양하고 다재다능한 심볼릭 음악 생성을 위한 효율적이고 효과적인 솔루션을 제공한다고 믿습니다.조건 플래그의 효과 GETScore에서 모든 일반 토큰은 정보를 전달하기 때문에 예측된 일반 토큰의 부정확성은 추론 중에 노이즈 제거 방향의 편차로 이어질 수 있습니다.이 문제를 해결하기 위해 신뢰할 수 있는 토큰을 나타내기 위해 학습 가능한 조건 플래그를 내장된 GETScore에 통합합니다.조건 플래그의 효과를 평가하기 위해 확산 모델에서 제거합니다.결과는 표 3에 나와 있습니다.비교 가능한 손실을 감안할 때 조건 플래그를 제거하면 학습 및 수렴에 미치는 영향이 최소화되지만 반주 생성(AG)에서 생성 품질이 낮아지고 무조건 생성(UN)에는 약간의 영향을 미칩니다. 이는 특히 조건부 생성 시나리오에서 고품질 음악을 생성하기 위한 모델 안내에서 조건 플래그의 효과를 보여줍니다.5 결론 우리는 사용자의 다양한 작곡 요구를 해결할 수 있는, 처음부터 또는 사용자가 제공한 소스 트랙을 기반으로 원하는 대상 트랙을 효과적이고 효율적으로 생성하기 위한 통합된 표현 및 확산 프레임워크인 GETMusic을 제안합니다.GETMusic에는 새로운 표현 GETScore와 확산 모델 GETDiff의 두 가지 핵심 구성 요소가 있습니다.GETScore는 효율성, 간단한 소스-대상 지정, 동시 음표 상호 종속성의 명시적 보존을 포함하여 다중 트랙 음악을 표현하는 데 여러 가지 이점을 제공합니다.GETScore의 힘과 GETDiff의 비자기 회귀적 특성을 활용하여 GETMusic은 다양한 소스-대상 조합에서 음악을 작곡하고 임의의 위치에서 제로 샷 생성을 수행할 수 있습니다.향후에는 가사를 트랙으로 통합하여 가사-멜로디 생성을 가능하게 하는 것과 같이 GETMusic의 잠재력을 계속 탐색할 것입니다. 참고문헌 [1] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg. 이산 상태 공간에서의 구조화된 잡음 제거 확산 모델. A. Beygelzimer, Y. Dauphin, P. Liang, J. Wortman Vaughan 편집자, 신경 정보 처리 시스템의 발전, 2021. [2] Y. Bengio, P. Simard, P. Frasconi. 경사 하강을 사용하여 장기 종속성을 학습하는 것은 어렵습니다. IEEE 신경망 거래, 5(2):157–166, 1994. [3] Walshaw Christopher. abc 음악 표준 2.1. ABC 표기법 표준, 2011. [4] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 gans를 이겼습니다. M. Ranzato, A. Beygelzimer, Y. Dauphin, PS Liang, J. Wortman Vaughan 편집자, 신경 정보 처리 시스템의 발전, 34권, 8780-8794페이지. Curran Associates, Inc., 2021. [5] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, Taylor Berg-Kirkpatrick. 다중트랙 음악 변환기. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP) 회의록, 2023. [6] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan Yang. Musegan: 상징적 음악 생성 및 반주를 위한 다중트랙 순차적 생성적 적대 네트워크, 2017. [7] Jeffrey Ens와 Philippe Pasquier. MMM: 변압기를 사용한 조건부 다중 트랙 음악 생성 탐색.CoRR, abs/2008.06048, 2020. [8] Rui Guo, Dorien Herremans, Thor Magnusson. Midi miner - 음조 긴장 및 트랙 분류를 위한 파이썬 라이브러리.CORR, abs/1910.02049, 2019. [9] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거.H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집자, 신경 정보 처리 시스템의 발전, 33권, 6840~6851페이지.Curran Associates, Inc., 2020. [10] Jonathan Ho와 Tim Salimans. 분류자 없는 확산 안내. NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [11] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling. Argmax 흐름 및 다항 확산: 범주형 분포 학습. A. Beygelzimer, Y. Dauphin, P. Liang, J. Wortman Vaughan 편집자, Advances in Neural Information Processing Systems, 2021. [12] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, Yi-Hsuan Yang. 합성어 변환기: 동적 지향 하이퍼그래프에서 전체 노래 음악을 구성하는 방법 학습. AAAI 인공지능 컨퍼런스 회의록, 35(1):178–186, 2021년 5월. [13] Yu-Siang Huang, Yi-Hsuan Yang. 팝 음악 트랜스포머: 비트 기반 모델링 및 표현력 있는 팝 피아노 작곡 생성. 제28회 ACM 국제 멀티미디어 컨퍼런스 회의록, MM &#39;20, 1180-1188페이지, 뉴욕, 뉴욕, 미국, 2020. Association for Computing Machinery. [14] Zeqian Ju, Peiling Lu, Xu Tan, Rui Wang, Chen Zhang, Songruoyao Wu, Kejun Zhang, Xiangyang Li, Tao Qin, Tie-Yan Liu. 텔레멜로디: 템플릿 기반 2단계 방법을 사용한 가사-멜로디 생성. CORR, abs/2109.09617, 2021.[15] Solomon Kullback 및 Richard A Leibler. 정보와 충분성에 관하여. 수리통계연보, 22(1):79–86, 1951. [16] Ang Lv, Xu Tan, Tao Qin, Tie-Yan Liu, Rui Yan. 창작물의 재창조: 가사-멜로디 생성을 위한 새로운 패러다임, 2022. [17] Gautam Mittal, Jesse Engel, Curtis Hawthorne, Ian Simon. 확산 모델을 통한 상징적 음악 생성, 2021. [18] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu. Popmag: 팝 음악 반주 생성. 제28회 ACM 국제 멀티미디어 컨퍼런스 회의록, MM &#39;20, 1198–1206쪽, 뉴욕, 뉴욕, 미국, 2020. 컴퓨팅 기계 협회. [19] Zhonghao Sheng, Kaitao Song, Xu Tan, Yi Ren, Wei Ye, Shikun Zhang 및 Tao Qin. Songmass: 사전 훈련 및 정렬 제약을 사용한 자동 노래 쓰기. CORR, abs/2012.05168, 2020. [20] Li Shuyu 및 Yunsick Sung. Melodydiffusion: 변압기 기반 확산 모델을 사용한 코드 조건 멜로디 생성. Mathematics 11, no. 8: 1915., 2023. [21] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan 및 Surya Ganguli. 비평형 열역학을 사용한 심층적 비지도 학습. Francis Bach와 David Blei 편집, 제32회 국제기계학습대회 논문집, 기계학습연구 논문집 37권, 2256-2265쪽, 프랑스 릴, 2015년 7월 7-9일. PMLR. [22] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암묵적 모델의 잡음 제거. 국제학습표현대회, 2021. [23] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. Roformer: 회전위치 임베딩이 있는 향상된 변압기. arXiv 사전 인쇄본 arXiv:2104.09864, 2021. [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 5998-6008페이지, 2017. [25] Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang. Midinet: 1차원 및 2차원 조건을 사용하는 심볼릭 도메인 음악 생성을 위한 합성곱 생성적 적대 네트워크. CORR, abs/1703.10847, 2017. [26] Botao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan, Wei Ye, Shikun Zhang, Tao Qin 및 Tie-Yan Liu. Museformer: 음악 세대를 위한 세밀하고 거친 세심함을 갖춘 트랜스포머. Alice H. Oh, Alekh Agarwal, Danielle Belgrave 및 조경현 편집자, 신경 정보 처리 시스템의 발전, 2022. [27] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin 및 Tie-Yan Liu. MusicBERT: 대규모 사전 훈련을 통한 상징적 음악 이해. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 791-800페이지, 온라인, 2021년 8월. Association for Computational Linguistics. [28] Chen Zhang, Yi Ren, Kejun Zhang, Shuicheng Yan. Sdmuse: 하이브리드 표현을 통한 확률적 차분 음악 편집 및 생성, 2022.A 소스-타겟 분할의 수 주어진 k-트랙 음악 입력에 대해 GETMusic은 m개의 트랙을 소스로 선택하고 나머지 k-m개의 트랙에서 선택된 n개의 타겟 트랙을 생성할 수 있습니다. 각 트랙을 소스, 타겟 또는 비워 둘 수 있다는 점을 고려하면 3가지 가능한 조합이 있습니다. 그러나 특정 시나리오는 m개의 트랙을 소스로 선택하고 나머지 k-m개의 트랙을 비워두면 Σ=0 Cm = 2k개의 불법 조합이 발생하는 것입니다. 따라서 유효한 조합의 수는 3k~2k입니다. 우리의 설정에서는 6개의 악기 트랙이 있어 665개의 가능한 조합이 있습니다. 주목할 점은 코드 진행 트랙이 이 계산에서 7번째 트랙으로 간주되지 않는데, 이는 조건 생성의 품질을 향상시키기 위해 코드 진행을 소스 트랙으로 일관되게 활성화하기 때문입니다. B 데이터 전처리 데이터 정리 [18]에서 제안한 방법에 따라 4단계로 데이터 정리 프로세스를 수행합니다. 첫째, MIDI Miner [8]를 사용하여 멜로디 트랙을 식별합니다. 둘째, 나머지 트랙을 베이스, 드럼, 기타, 피아노, 현악기의 다섯 가지 악기 유형으로 축소합니다. 셋째, 필터링 기준을 적용하여 최소한의 음표가 포함된 데이터, 트랙 수가 적은 데이터, 여러 템포를 보이는 데이터 또는 멜로디 트랙이 없는 데이터를 제외합니다. 넷째, 모든 데이터에 대해 Magenta(https://github.com/magenta/magenta)에서 구현한 Viterbi 알고리즘을 사용하여 해당 코드 진행을 유추하여 추가 작곡 가이드 역할을 합니다. 마지막으로, 데이터를 최대 32개 막대의 조각으로 분할하고 이러한 조각을 GETScore 표현으로 변환합니다.코드 진행 코드 진행 트랙의 구성은 일반적인 악기 트랙과 다릅니다. 일반적으로 사용되는 특정 코드가 특정 악기 트랙에 나타나고 피치 토큰으로 표현되었지만 코드 진행 트랙이 각 개별 트랙에 대해 공평한 지침을 제공하도록 하기 위해 이러한 토큰을 재사용하지 않습니다. GETMusic은 12개의 코드 루트(C, C#, D, D#, E, F, F#, G, G#, A, A#, B)와 8개의 코드 품질(장조, 단조, 감소, 증가, 장조7, 단조, 우세, 반감소)을 통합합니다. 위의 클렌징 프로세스의 4단계에서 음악 작품의 막대당 하나의 코드를 식별합니다. GETScore의 코드 진행 트랙에서 첫 번째 행에 코드 루트를 할당하고 두 번째 행에 품질을 할당합니다. 코드 트랙은 패딩 없이 완전히 채워집니다. 그림 4는 코드 트랙이 포함된 GETScore의 예입니다.LeadBass******Drum......4643Guitar 11153155128Piano1...... 4 12Stringс с с с с ...... Chord maj maj maj maj maj maj maj maj maj maj ...... AAAAAAAAAA min min man min min man min many min min min min min min min min minj min min 512 단위 그림 4: 실험에 사용된 7개 트랙의 GETScore를 보여주는 예입니다. 여기서 숫자는 토큰 인덱스를 나타냅니다. 이 예는 표시용일 뿐 실제 음악 작품과 일치하지 않습니다. 어휘 위에서 언급한 정리 과정의 마지막 단계에서는 음악 조각을 GETScore로 변환하기 전에 어휘를 구성하는 것이 필수적입니다. GETMusic에서 각 트랙은 자체 피치 어휘를 가지고 있는 반면, 지속 시간 어휘는 모든 트랙에서 공유됩니다.GETMusic에서 지원하는 최대 지속 시간은 16시간 단위이며, 드럼의 특수 지속 시간 토큰인 0에서 16시간 단위까지 총 17개의 지속 시간 토큰이 생성됩니다.피치 어휘를 구성하기 위해 음악을 먼저 C장조 또는 A단조 키로 정규화하여 피치 토큰 조합의 수를 크게 줄입니다.각 트랙에서 고유한(복합) 피치 토큰을 식별하고 빈도에 따라 순위를 매깁니다.추론하는 동안 입력 음악도 먼저 C장조 또는 A단조로 정규화하고 그에 따라 토큰화합니다.GETMusic은 생성된 음악을 원래 키로 다시 정규화합니다. 최종 어휘는 17개의 지속 토큰, 20개의 코드 토큰, 패딩 토큰, [MASK] 토큰, [EMPTY] 토큰, 그리고 각 트랙에 대한 특정 피치 토큰으로 구성됩니다: 리드의 경우 128개, 베이스의 경우 853개, 드럼의 경우 4개, 피아노의 경우 1,555개, 기타의 경우 3,568개, 스트링의 경우 1,370개. 어휘는 총 11,883개의 토큰으로 구성됩니다.
