pre-trained large language models (LLMs) capture procedural knowledge about the world. recent work has leveraged LLM’s ability to generate abstract plans to simplify challenging control tasks. but transformer architecture inherits several constraints that make it difficult for the LLM o directly serve as the agent: limited inut lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nonext e. to maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify he control problem. the plan module transates a task description into a list of high-level sub-tasks. the track module determines whether the agent has accomplished each sub-t. the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications. humans can abstractly plan their everyday tasks without execution; for example, given the task “Make breakfast”, we can roughly plan to first pick up a mug and make coffee, before grabbing eggs to scramble. Eliminate Take an apple You see apple, Paug-knife — Action: Pickup Apple Update Progress Finished taking | an apple? J Figure 1. PET framework. Track module uses a QA model to track the completion of an observation. recent work has used LLMs for abstract planning for embodied or gaming agents. these have shown incipient success in extracting procedural world knowledge from LLMs in linguistic orm with posthoc alignment to executable actions. but they treat LLMs as the acor, and focus on adapting LLM outputs to executable actions either through fine-tuning (Micheli & Fleuret, 2021) or constraints actor uses LLM as the actor for pure-text environments with imited interactions (Huang et al., 2022b; Ahn et al., 2022) he provides all available objects and possible interactions at the start and imits tasks to the set of provided objects/interactions. he limits the environment to the set of provided objects/interactions. a more realistic scenario leads to a --- -- --Plan, Eliminate, and Track diverse, complicated tasks or large and changing action space. the text description of the observation increases as a function of the number o receptacles a table. in this work, we explore alternative mechanisms to leverage prior knowledge encoded in LLMs withow impacting the trainable nature of the actor. we propose a 3-step framework (Figure 1): Plan, Eliminate, an track. it simplifies complex tasks by breaking them down into sub-tasks. it uses a pretrained LLM to generate a list of sub-tasks for an inpu task description using example. the Eliminate module addresses the challenge of long observations. it uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. the Action Attention agen uses a transformer-based architecture to accommodate for long roll-outs. the agent observes the masked observation and takes an action conditioned on the current sub-task. our experiments and analysis demonstrate that LLMs remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. multiple LLMs may be used in coording and coording. our contributions are as follows: PET: A novel framework for leveraging pretrained LLMs with embodied agents. each of P, E, T serves a complementary role and should be addressed to tackle control tasks. a 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. TED WORK --- Language Conditioned Policies A considerable portion of prior work studies imitation learning. policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010; Kollar et al., 2010; Kollar et al., 2010; Kollar et al., 2010). pre-trained language embeddings lack domain knowledge that is captured in LLMs. our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs for control LLMs have recently achieved success in high-level planning. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. but LLM scores work for simple environments with actions limited to pick/place (Ahn et al., 2022) but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-plan. all the above LLMs require few-shot demonstrations of up to 17 examples. we show that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. hierarchical planning with natural language Due to the structured nature of natural language, Andreas et al. (2017) explored associating each task description to a modular subpolicy. recent works have shown that LLMs are proficient high-level planners. recent works have shown that LLMs are proficient high-level planners. PET is the first work combining a zero-shot subtask-level planner and a template. text games are complex, interactive simulations where the game state and action space are in natural lanugage. successful play requires skills like memory and planning, exploration (trial and error) and common sense. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two different models and computes the Q function as the inner product of the representations. Zahavy et al. (2018) trains a model to create text-based analogs of each ALFRED scene. eliminate invalid actions on Zork from external environment signals. but the functionality depends on the existence of external elimination signal. plan, Eliminate module (Mg) uses a zero-shot QA language model to score mask objects. the Track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete and moves to the next sub-task. we also implement an attention-based agent (Action Attention) which scores each permissible action and is trained on imitation learning on the expert. problem setting We define the task description as T, the observation string at time step t as O', and the list of permissible actions a!|a can be executed as A’. 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. receptacles and objects within the observation as rf and o; respectively. the classification between receptacles and objects is defined by the environment. for a task J, we assume there exists a list of sub-tasks Sy = s1,...s8, that solves T. 3.1. we design the plan module (Mp) to generate a list of high-level sub-tasks for a task description T. for a given task description, we compose the query question Q7 as “What are the middle steps required to JT?” and require Mp to generate a list of sub-tasks Sy = 51,... sx. ERTa embedding similarity with query task 7 embedding similarity with query task 7. we then concatenate the example tasks with example tasks in a query-answer format to build the prompt Pz for Mp (Fig. 2): Pr= concat(Qre, Srp, Ore, Spe, Ore, ore, ore, ore, ore, ore, ore, ore, or a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a mic. the expected list of sub-tasks to achieve this task T is s; =‘ a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. we use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. receptacles shown in red are not relevant for task completion. receptacles are filtered by the QA model making the observation shorter. in some close-to-worst cases, there can be around 30 open-able receptacles (e.g. a kitchen with many cabinets and drawers), and it easily takes an agent with no prior knowledge more than 50 steps for the agent to find the desired many receptacles and objects are irrelevant to specific asks during training and evaluation. by removing the irrelevant receptacles like coffeemachine, garbagecan, or objects like knife, we could significantly shorten our observation. we herefore propose to leverage commonsense knowledge captured by larg. e pre-trained QA models to design our Eliminate module Mg. for task 7, we create prompts in the format P, =“Your ask is to: T. Where should you go to?” for receptacles and P, =“Your task is to: T. Which objects will be relevant?” for objects. r the common-sense QA model believes the object /receptacle is relevant to T. we then remove 0; from observation if fio,  To, and remove 1; if jp,  7,. Threshold 7,,7, are hyperparameters. Environment You are in the middle of a room. go to sinkbasinOn the sinkbasin 1, you see nothing. subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3,..... You take apple 1 from diningtable 1. Tracking Module (Progress Tracking) at every step, we take the last 3 steps of roll-out as context and append a query (about whether the current sub-task is completed) to get the prompt. a human actor typically starts from the first item and check-off the tasks one by one until completion. a human actor typically starts from the first item and checks-off the tasks one by one until completion. for sub-task list Sr = s1,... 8x, we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection. we keep track of a progress tracker p (initialized at 1) that indicates the sub-task the agent is currently working on (sp). we then compose the context as the last d steps of the agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks. the agent has no means to recover if it undoes its previous sub-task at test time. n(d + 1,3) is reset to whenever the progress tracker updates. we feed P, to a pre-trained zeroshot QA model M+. we compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No"[Pa)- IE pate “Yes” [Pa) > pate (*No" [P we study the rate of pre-mature ends in Section 4.4 in terms of precision and recall. 3.4. Agent Since the number of permissible actions can vary a lot by the environment, the agent needs to handle arbitrary dimensions of action space. we draw inspiration from the field of text summarizaion, where mr. mr. mr. mr. mr. mr. mr. action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. we eschew the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action in our proposed action attention ramework, we first represent historical observations H' as the average of embeddings of all individual observations through history. in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H"), the current observation embedding (O*) and the list of action embeddings (H“). we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H), the current observation embedding (O') and embedding of action (a;). present our experiments as follows. first, we explain the environment setup and baselines for our experiments. then we compare PET to the baselines on different splits of the environment. 4.1. Experimental Details AlfWorld Environment ALFWorld (Shridhar et al., 2020b) is a set of TextWorld environments (Coté et al., 2018b) there are 3553 training task instances (tasktype, object, receptacle, room),in-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) 134 out-of-distribution evaluation task instances (unseen split - tasks take place in novels rooms) human goal specifications contain 66 unseen verbs and 189 unseen nouns. the template goals use only 12 ways of goal specification. the sentence structure for human goal specification is more diverse compared to the template goals. oal experiments are good for testing the generalization of models to out-of-distribution scenarios. we experimented with the open-source GPT-Neo-2.7B (black et al., 2021) and an industryscale LLM with 530B parameters (Smith et al., 2022). u see a cabinet 1 cabinet 1 applesee a cabinet 5, a mug 3, a cup 3 apple 1 from | diningtable 1. Heat the et x v T J Za J GSC K, Ks N _ Action Attention block is a transformer-based framework that computes a key K; for each permissible action and output action scores as dot-product between key and query Q from the observations. DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 9 - GPT (Micheli & Fleuret, 2021) 91 95 42PET + Action Attention (Ours) 70 67.5 52.5Table 1. o have common sense QA performance on par with GPT3. we use a decision hreshold of 0.4 for Macaw score below which the objects are masked out. for the track module, we use the same Macaw-11b model as the Eliminate module answer to Yes/No questions. we use re-trained RoBERTa-large (Liu et al., 2019) with emedding dimension 1024. for sub-task generation, we use ground-truth sub-tasks for training and generated sub-tasks from Plan module for evaluation. training is 100x slower with DAgger compared to behavior cloning. all other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b) agent consists of an encoder, an aggregator, and a decoder. each time step t, the encoder takes initial observation s°, current observation s‘, and task string Stas, and generates representation r' our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the alfworld training set. the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. overall results on Template and Human Goals. PET outperforms SOTA (GPT) by 25% on seen and 5% on unseen split. GPT requires fine-tuning on fully text-based expert trajectory and loses adaptability to different enviromental enviromental enviromental enviromental enviromental enviromental enviromental enviromental en on human goal specification tasks, GPT gets stuck repeating the same action after producing a single wrong move. on the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal specifications as shown in Section 4.5. the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC per‘orms poorly), we also include DAgger training results. however, action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. the data set size is chosen  match the size of the seen validation set. we treat Plan and Track as a single module for this ablation. more than 60% relaive improvement over just Plan and Track alone. we deduce that Plan and Track boost the performance of Eliminate during evaluation. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity against ground-truth sub-tasks. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity against ground-truth sub-tasks. we observe that all LLMs achieve high accuracy on template goal specifications, where there is no variation in sentence structures. we evaluate the zero-shot recepacle/object masking performance of Macaw on the hree splits of alfworld. in Fig 6, we illustrate the AUC curve of the relevance score that the model assigns ‘0 the objects v.s. objects that the rule-based exper interacted with whencompleting each task. object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. a decision threshol 0. (o) ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. as an agreement measure, we report a precision of 0.99 an a recall of 0.78 for Macaw-11B and a precision of 0.96 for Macaw-large. the larger mode (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. the smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. both models produce similar overall results. the first type of error is caused by generating synonyms of the ground truth. the second type of error is caused by inaccu --- -- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al. MT-NLG (Smith et al., 2022) 98.57 (0.99) | 100 (1.00) 40.04 (0.94) 49.3 (0.94) Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity against ground-truth sub-tasks. overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. oa 0608 10 00 02 oa 0608 10 00 02 oa 060810 00 02 oa 0608False Positive Rate False Positive Rate False Positive Rate False Positive Rete Figure 6. the QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention + Eliminate 25Action Attent. ion + plan & track 35Action Attention + PET 52.5 27.Table 3. applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. but applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. the main source of elimination error occurs when the module of interest fails to find such receptacles. the main source of elimination error occurs when the module of interest so the agent fails to find such receptacles. some objects in the AI2Thor simulator do not spawn according to common sense. hat subfor tasks Track Module Experimentally, we find task planning/tracking is particularly helpful that require counting procedures. we find task planning/tracking is particularly helpful that require counting procedures. ai2thor.allenai.org/ithor/documentation/objects/objecttypes. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on om the Plan module on human goal specifications (Task), ground-truth (GT) v.s. generated (Gen) in the first example, generated plan differs from the ground truth but the meaning agrees. in the second example, generated plan largely differs from the ground truth due to the mistake in human goal specification. we propose the plan, Eliminate, and track framework that uses pre-trained LLMs our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. n our experiments, we combine PET with a novel Acion Attention agent that handles the dynamic action space in AlfWorld. our ablation studies show the Plan and Track modules together imrove the performance of Eliminate module to achieve he best performance. our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents. multiple LLMs may be used in coordination with each other to further improve effectiveness. the progress tracker does not take into consideration previous progress being undone. future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk. reference: Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Ruano, R. J. arXiv preprint arXiv:2006.07185, 2020. Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language to autonomously-acquired skills via goal generation. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. if you use this software, pleasecite it using these metadata. man, R., von Arx, von Arx, von Arx, Bernstein, M. S., Bosselut, A., Brunskill, E., Buch, S., Card, D., Castellon, R., Chatterji, Chen, A., Creel, A., Davis, J. Q., Demszky, D., Doumbouya, M., Dur --- --Plan, Eliminate, and track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Niebles, J. C., Nyarko, J., Orr, L. a., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, J., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W. an, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Askell, A. et al. Advances in neural information processing systems, 33:1877-1901, 2020. Object goal navigation using goal-oriented semantic exploration, 2020. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., and al. Textworld: A learning environment for text-based games. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. arXiv preprint arXiv:1511.04636, 2015. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Zeng, A., Tompson, J., Mordatch, L., Chebotar, Y., Brown, N., Jackson, T., Luu, L., Levine, S. h planning with language models, 2022b. conference on robot learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. arXiv preprint arXiv:2209.00465, 2022. oy, D., and Roy, N. Toward understanding natural language directions. arXiv preprint arXiv:2209.00465, 2022. arXiv preprint arXiv:1907.11692, 2019. reprint arXiv:1907.11692, 2019.. mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. Micheli, V. and Fleuret, F. Language models are fewshot butlers. LLMs remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. PET: A novel framework for leveraging pretrained LLMs with embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied our work shows that each of P, E, T serves a complementary role. an Action Attention agent that handles the changing action space for text environments. a 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. al., 2020; Jang et al., 2022; Shridhar et al., 2022; Sharma et al., 2021) or reinforcement learning (Misra. et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs. LLMs for Control LLMs have recently achieved success in high-level planning. the generated sub-tasks cannot be directly executed in an end-to-end control environment. LLM scores work for simple environments with actions limited to pick/place. but fails with environments with more objects and diverse actions. the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-pll. Micheli & Fleuret fine-tuned a GPT2-medium model on expert trajectories in alfworld. LM fine-tuning requires a fully text-based environment, consistent expert trajectories, and a fully text-based action space. such requirements greatly limit the generalization to other domains, and even to other forms. Andreas et al. (2017) explored associating each task description to a modular sub-policy. recent works have shown that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. PET is the first work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games _ Text-based games are complex, interactive simulations where the game state and action spacing. e are in natural lanugage. they are fertile ground for language-focused machine learning research. successful play requires skills like memory and planning, exploration (trial and error) and common sense. th two different models and computes the Q function as the inner product of the representations. this could generalize to large action space, but only considered a small number of actions. Zahavy et al. trains a model to eliminate invalid actions on Zork from external environment signals. but the functionality depends on the existence of external elimination signal. in plan module (Mp), a pre-trained LLM generates a list of sub-tasks. the track module (Mr) uses a zeroshot QA language model to score mask objects and receptacles that are irrelevant to the current sub-task. the track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete. the agen observes the masked observation and takes an action conditioned on the current sub-task. the task description is T, the observation string at time step t as O', and the list of permissible actions a!|a can be executed. 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. receptacles and objects within the observation as rf and o; respectively, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rf and o; respectively, we define the Examples from Training set Planning Module ( Target Output Figure 2. receptacles and objects are defined by the environment. for a task J, we assume there exists a list of sub-tasks Sy = s1,...s8, that solves T. 3.1. we design the Plan module (Mp) to generate a list of high-level sub-tasks for a task description T. we select the top 5 example tasks 7” from the training set based on RoBERTa embedding similarity with the query task 7. we then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt Pz for a given task. the expected list of sub-tasks to achieve this task is s; =‘take an apple’, 82 =‘heat the apple’, and s3 =‘place the appl. a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a microwave 1. looking quickly around you, you see a a cabinet 5, a cabin 5, et 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. we use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. the original observation is too long and the receptacles shown in red are not relevant for task completion. nes can start with around 15 receptacles, each containing up to 15 objects. in some close-to-worst cases, there can be around 30 open-able receptacles. it takes an agent with no prior knowledge more than 50 steps for the agent to find the desired object. we propose to leverage commonsense knowledge captured by pre-trained QA models to design our Eliminate module Mg to mask out irrelevant receptacles and objects. for task 7, we create prompts in the format P, =“Your ask is to: T”. we compute score flo, = Mag(P.,0:) for each object 0; and pr, = Mg(Po, ri) for each receptacle r; in observation at every step.represents the belief score of whether the common-sense QA model believes the object /receptacle is relevant to T. you take apple 1 from diningtable 1. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. Tracking Module (Progress Tracking) is a pre-trained QA model that generates a Yes/No answer to the prompt. a pre-trained QA model generates a Yes/No answer to the prompt. te. a human actor typically starts from the first item and check-off the tasks one by one until completion. we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection. we then compose the context as the last d steps of thomas. e agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks. the agent has no means to recover if it undoes its previous sub-task at test time. for efficiency, we set d := min(d + 1,3) at each step. the emplate P, = concat(O'4,..., 01, “Did you finish the task of a pre-trained zeroshot QA model M+ compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No"[Pa)- IE pate “Yes” [Pa) > pate (*No) hen we increment the tracker p to track the next subask. f the tracking ends prematurely, meaning Shridhar et al. (2020b) addresses this challenge by generating actions token-by-token. such a generation process leads o degenerate performance even on the training set. et al. (2017) generates a summary through an attention-like “pointing” mechanism that extracts he output word by word. action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. we eschew the long rollout/ large action space problems by (1) representing observations y averaging over history, and (2) individually encoding actions. in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H), the current observation embedding (O*) and the list of action embeddings (H“). in the query Q, we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H); and the we compute the dot-product of the query and keys as action scores for the policy 7 (Eq. 5). first, we explain the environment setup and baselines for our experiments. then we compare PET to the baselines on different splits of the environmental splits of the environmental splits of the environmental splits of the environmental splits of the environmental splits. we conduct ablation studies and analyze the PET framework part by part. we show that PET generalizes better to human goal specification under efficient behavior cloning training. ALFWorld includes 6 task types that each require solving multiple compositional sub-goals. 53 training task instances (tasktype, object, receptacle, room) and 134 out-of-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) 134 out-of-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) crowd-sourced human goal specifications for evaluation contain 66 unseen verbs and 189 unseen nouns. human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. the open-source GPT-Neo-2.7B (Black et al., 2021) and an industryscale LLM with 530B parameters (Smith et al., 2022). You go to diningtable You go to microwave = of a room. action Attention block is a transformer-based framework that computes a key K. for each permissible action and output action scores as dot-product between key and query Q from the observations. template Goal Specification Model seen unseen seen unseen BUTLER + DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 ison of different models in terms of completion rate per evaluation split (seen and unseen) we choose Macaw-11b (Tafjord & Clark, 2021) which is reported to have common sense QA performance on par with GPT3 (Brown et al., 2020) the track module (progress racking) uses the same Macaw-11b model as the Eliminate module answer to Yes/No questions. our Action Attention agent (Mg and Mx) is a 12-layer transformer with 2 heads and hidden dimension 384. the last layer is fed into two linear heads to generate K and Q. for sub-task generation, we use ground-truth-truth-truth-truth-truth we experiment with models trained with behavior cloning. DAgger assumes an expert that is well-defined at all possible states. training is 100x slower with DAgger compared to behavior cloning (human goal spherical spherical spherical spherical spherical spherical spherical spherical spherical sp we include the performance of BUTLER with DAgger for completeness. all other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. weeks for DAgger v.s. 6 hours for behavior cloning for BUTLER+BC and PET. baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. the recurrent aggregator combines r’ with the last recurrent state h'! to produce h’, which is then decoded into a string a’ representing action. our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the alfworld training set. the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. for human goal specifications, PET outperforms SOTA (GPT) by 25% on seen and 5% on unseen split. GPT requires fine-tuning on fully text-based expert trajectory and loses adaptability to different environment settings. GPT suffers from a relative 50% performance drop transferring from template to human-goal specifications. the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC per‘orms poorly). action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. the data set size is chosen  match the size of the seen validation set, for an efficient and sparse setting. the data set size is chosen  match the size of the seen validation set, for an efficient and sparse setting. ngle module for this ablation since hey cannot work separately. Adding Plan and Track greatly improves the compleion rate by 60%. we observe a relatively insignificant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. ngle module for this ablation since hey cannot work separately. Plan and Track deduce that Plan and Track boost the performance of Eliminate during evaluation. we experiment with different LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo2.7B (Black et al., 2021) and the 530B parameter MT-NLG (Smith et al., 2022) we observe that all LLMs achieve high accuracy on template goal specifications. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity. he other smaller models perform significantly worse. e of the relevance score that the model assigns '0 the objects v.s. objects that the rule-based exper interacted with whencompleting each task. we note that object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. a sub-task tracker shoul record the last sub-task as shed” if and only if the environment is “fully solved” by the expert. the larger mode (Macaw-11b) is more precise but misses more detection. the smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. the larger mode (Macaw-11b) is more precise but misses more detection. the first type of error is caused by generating synonyms of the ground truth. the second type of error is caused by inaccu --- -- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58) GPT-Neo the MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. MT-NLG generates sub-tasks on hard tasks with human goal specification. ROC curve for train receptacle identification ROC curve for valid_unseen receptacle identification 10 1008 08206 = 06 =Bas Bos Bas & & & 02 ozoo) nuc=0.6362842168817953 oo 4 — Auc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa os 08False entification ROC curve for valid_unseen object identification 10 10o8 o8 o2 06 2 06 2 06 2Box Eon &  oz oz oo be — nuc=0.746903768505050358 oo be — wuc=0.00 02 04 0608 10 00 02 oa 0608False Positive Rate False Positive the QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention 25Action Attention + Eliminate 25Action Attention + Plan & Track 35Action Attention + PET 52.5 27.Table 3. Comparison of different ablations of PET trained on a sampled set of applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. the main source of elimination error occurs when the module of interest fails to find such receptacles. this is often because some objects in the AI2Thor simulator do not spawn according to common sense. so, such generations in AI2Thor are unlikely in real deployment; thus, the “mistakes” of our Eliminate module are reasonable. task planning/tracking is particularly helpful that require counting procedures. ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ --- --Plan, Eliminate, and track human goal specification Examples Task Chill. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal specifications (Task), ground-truth (GT) v n this work, we propose the Plan, Eliminate, and Track framework that uses pre-trained LLMs. our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. Acion Attention agent handles the dynamic action space in alfworld. our action attention agent greatly outperforms the BUTLER baseline. our ablation studies show the plan and track modules together imrove the performance of Eliminate module to achieve he best performance. the track module (progress tracker) does not re-visit finished sub-tasks. it does not re-visit finished sub-tasks. the agent is executing sub-tasks, and it picked up a pan. future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk—put the policy. reference: Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakri i can, not as i say: Grounding language in robotic affordances, 2022. i can, not as i can, not as i say: grounding language in robotic affordances, 2022. arXiv preprint arXiv:2006.07185, 2020. kzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. a persistent spatial semantic representation for highlevel natural language instruction execution, 2021. a persistent spatial semantic representation for highlevel natural language instruction execution, 2021. Davis, J. Q., Demszky, D., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Guha, N. I., Li, X. L., Li, X., Ma,. plan, elimination, and track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Narayan, A., Niebles, J. C., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, L a. W., Tramér, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zheng, L., Zhou, K. in 2020, IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225-232. IEEE, 2020. Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration, 2020. Textworld: A learning environment for text-based games. in workshop on computer games, pp. 41-75. a rock can be extracted from a rock or a rock. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. racting actionable knowledge for embodied agents, 2022a. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Chebotar, Y., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ich conference on robot learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. arXiv preprint arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Ren, X. Walk the talk: Connecting language, knowledge, and action in route instructions. arXiv preprint arXiv:2104.07972, 2021. min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y., and Salakhutdinov, R. Film: Following instructions in language with modular methods, 2021. arXiv preprint arXiv:1704.08795, 2017. Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. learning language-conditioned robot behayior from offline data and crowd-sourced annotation. arXiv preprint arXiv:1704.04368, 2017. Sharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. ing, pp. 2661-2670. PMLR, 2017. Radford, A., Wu, J., Child, R., Luan, D., Luan, D., Luan, D., and Shridhar, M., Yuan, X., Coté, M.-A., Bisk, Y., Trischler, A., and Fox, D. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. arXiv preprint arXiv:2010.03768, 2020b. Using deepspeed and megatron to train megatron-turing NLG 530b, a large-scale generative language model. coRR, abs/2201.11990, 2022. Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B. Using deepspeed and megatron to train megatron- arXiv preprint arXiv:2212.04088, 2022. stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C. and Ben Amor, H. Language-conditioned imitation learning for robot manipulation tasks. v preprint Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. Understanding natural language commands for robotic navigation and mobile manipulation. v preprint Tellex, S., Kollar, T., Dickerson, T., Dickerson, S., Walter, M., Banerjee, A action elimination with deep reinforcement learning. Advances in neural information processing systems, 31, 2018. edward edward et al., edward et al., et al., et al., et al., et al., et al., et al., et al., et al., et al.