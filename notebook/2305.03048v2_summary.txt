arXiv:2305.03048v2 [cs.CV] 4 Oct 2023 PERSONALIZE SEGMENT ANYTHING MODEL WITHONE SHOT Renrui Zhang1,2, Zhengkai Jiang3*, Ziyu Guo2*, Shilin Yan2, Junting Pan1, Xianzheng Ma2 Hao Donga, Yu Qiao2, Peng Gao2, Hongs ining, Segment Anything Model (SAM) has revolutionized the segmenta-tion field. despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored. ining introduces a training-free Personalization approach for SAM. tive lo- cation prior for the target concept in new images. aided by target visual semantics, we empower SAM for personalized object segmentation via two pro- posed techniques. in this way, we can effectively customize the general-purpose SAM for private use without any training. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. code is released at https://github.com/ZrrSkywalker/Personalize-SAM. User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F. PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat "A [V] cat on a beach" e introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. PerSAM-F can help to achieve higher-quality person- alized text-to-image generation. ap- proach can help to achieve higher-quality person- alized text-to-image generation. al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. Segment Anything (Kirillov et al., 2023) develops a bsequently trains a segmentation foundation model, known as SAM. it defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask. however, SAM inherently loses the capability to segment specific visual concepts. imagine intending to crop your lovely pet dog in a thick photo album or find the missing clock from a picture of your bedroom. perSAM is a training-free personalization approach for Segment Anything Model. perSAM allows you to segment user-designated visual concepts in a simple and efficient manner. we first obtain a location confidence map for the target object in the test image by feature similarities. two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. we propose to inject visual semantics of the target object in the decoder for segmentation. object to unleash SAM's personalized segmentation power with two techniques: • Target-guided Attention. we guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. we fuse the original prompt tokens with the embedding of the target object. perSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. the approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output. to alleviate this issue, we further propose a fine-tuning variant of our approach, e.g., a fine-tuning variant of our approach. we freeze the entire SAM to preserve its pre-trained knowledge. we only fine-tune 2 parameters within 10 seconds on a single A100 GPU. perSAM-F avoids over-fitting on the one-shot on the one-shot. our approach can also assist DreamBooth to better fine-tune diffusion models for personalized text-to-image generation. a few images containing a specific visual concept, e.g., your pet cat or backpack, can be converted into an identifier [V] in the word embedding space. however, the background informatia can simultaneously include the background informatia. we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. we first investigate how to customize a general- purpose segmentation model into psa. in perSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. in perSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. ts on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. perSAM can enhance DreamBooth for better personalized text-to-image synthesis. pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. penAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019); and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities. CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) exhibit exceptional accuracy in zero-shot visual recognition. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. there are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023d), faster inference speed (Zhao et al., we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. from another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. imagen improves generation quality by segmenting foreground target objects. segmentation requires a pixel-level comprehension of a image. segmentation-related tasks include semantic segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation and cd segmentation. instance segmentation, focusing on the identification of individual object instances. panoptic segmentation, assigning both class labels and instance identification. interactive segmentation, involving human intervention for re-evaluation. several concurrent works have proposed large-scale vision models for image segmentation. they are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) uses a data engine with model-in-the-loop anno-tation to learn a promptable segmentation framework. painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile erSeg proposes two approaches, perSAM and perSAM-F. persAM and perSAM-F customize SAM for personalized segmentation. works have focused on developing parameter-efficient methods to freeze weights of foundation models and append small-scale modules for fine-tuning. prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks. low-Rank Adaption (LORA) injects trainable rank decomposition matrices concurrently to each pre-trained weight. Adapters are designed to be inserted between layers of the original transformer. we adopt a more efficient adaption method delicately designed for SAM. the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10 seconds. this effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. PerSAM and PerSAM-F consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder. as a promptable framework, SAM takes as input an image I, denoted as Encp, Enc, and Decм. Encp encodes human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1) 4 F1 Encode Cosine Similarity FT=1 Test Image I Target Local Features T=1 FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-At ss-Attention  Aggregate Attention Matrix A Local Features T=1 Token Self-Attention Overall Confidence Map S Aggregate  a Concat( + Repeat(). we calculate a location confidence map for the target object in new test image by the appear- ance of all local parts. then, we select the loca- tion prior as the point prompt for PerS -level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). to in-ject SAM with target semantics, we explicitly guide the cross-attention layers. we propose additional prompting with high-level cues. mask tokens are responsible for generating mask output, formulated as M = Decм (FI, Concat(TM,Tp)) mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. the user provides only a single reference image, and a mask indicating the target visual concept. the given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. for evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. t al., 2022; Kumari et al., 2022; Kumari et al., 2022; Kumari et al., 2022); containing various categories of visual concepts in different poses or scenes. in this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.2 TRAINING-FREE PERSAM Location Confidence Map. the encoder can be SAM's frozen backbone or other pre-trained vision models. we formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR  R1xwxc. then, we use the reference mask MR  Rhxwx1 to crop the features of foreground pixels within the visual concept from FR. n confidence maps for each foreground pixel i by the cosine similarity between T12 and test image feature F as S2  1 = F1TT) 11. each S2 represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. PerSAM selects two points with the highest and lowest confidence values in S. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object, while the latter invenes the most likely center position of the target object. the positive and negative point prompts are fed into the prompt encoder as Tp = Encp(Ph, P1)  R2c, (7) which denotes the prompt tokens for SAM's decoder. in this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. attention operation in SAM's decoder focuses feature aggregation within target regions. the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image. based on such a property, we use S to guide the attention map in every token-to-image cross-attention layer of the decoder. mask and prompt tokens are compelled to capture more visual semantics associated with the target subject. this contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. we propose to use the visual feature of the target concept as an additional high-level semantic prompting. we first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR =  €R1xc n i=1 Then, we element-wisely add TR to all the input tokens of the test image in Equation. perSAM is prompted by low-level location points, but also high-level target visual cues. a repeat operation replicates the target visual embedding in the decoder block. a repeat operation replicates the target visual embedding in the decoder block. PerSAM-F uses two learnable weights for adaptively aggregating three-scale masks. we use PerSAM to decouple the target ob-jects from the background for improving the generation of DreamBooth. segmentation mask on the test image from SAM's decoder may include rough edges and isolated background noise. in the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. in the second step, we acquire the bounding box enclosing the mask from the first step. for the second step, we acquire the bounding box enclosing the mask from the first step 3.3 FINE-TUNING of PERSAM-F Ambiguity of Segmentation Scales. the training-free PerSAM can tackle most cases with satisfac- tory segmentation accuracy. some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. m is comprised of two parts: a lid and a body. the negative prompt (denoted by a green pentagram) does not exclude the platform in a similar color. the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, perSAM would be misled for segmentation. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. this motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning perSAM-F first follows PerSAM to obtain the location prior. we use two learnable mask weights, w1, W2, and calculate the final mask output by a weighted summation as 1/3. to learn the optimal mask weights, we conduct one-shot fine- tuning on the reference image, and regard the given mask as the groud. we freeze the entire SAM model to preserve its pre-trained knowledge. we fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. in this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts. we compare the overall mIoU, blou, and learnable parameters method mIoU bloU Param denotes works concurrent to ours. can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 63.8 92.6 94.1 94.4 93.7 97.2 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 Table 3: one-shot Semantic and Part one-shot Semantic Seg. FSS-1000 LVIS-922 One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F  AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F Method PACO-Part HSNet DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 35 photos of a specific object. it learns to generate the cat referred to by a text prompt, “a [V] cat” and calculates the loss over the entire reconstructed images. this would inject the redundant background information in the training images into the identifier [V] our PerSAM- assisted DreamBooth can synthesize the target object with beads. the target object can be systoked with beads. the target object can also be systoked with beads. the target object can also be systoked with beads. the target object can be systoked with beads. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1. we then illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. perSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. each object is associated with 57 images and masks, where we fix one image-mask pair as the user-provided one-shot dat. the raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari e the mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. enlarged data scale of PerSeg. 8 The ring on a clock 2 The teapot on a tray The backpack carried by a can woman Figure 8: Visualization of PerSAM-F's Im- provement. the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. we show more visualiza-tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP), Painter (Wang et al., 2022), SEEM (Zou et al., 2023 our perSAM-F surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score. our one-shot fine-tuning approach can outperform methods (Lin et al., the results fully illustrate our strong generalization ability for temporal video data and complex scenarios. we evaluate our approach for one-shot image segmentation respectively on four datasets, including FSS-1000 (Li et al., 2020), LVIS-922 (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan e our PerSAM-F achieves consistently better results than Painter. for models with in-domain training, our approach can achieve higher scores than HSNet. the experiments well demonstrate that our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. PerSAM-assisted DreamBooth is able to fine-tune a pre-trained image synthesis. the "jungle" and "snow" by DreamBooth are still the sofa with green and white decorations. the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. 9 User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM. the improved DreamBooth (Ruiz et al., 2022) can better pc and pc. mIoU Gain 69.1 Method PerSAM Param. mIoU Method Mask Box 0 89.32 Painter 56.4 42.0 + Post-refinement 72.5 +3.4 83.9 +11.4 Prompt Tuning 12K 76.5 VP 65.9 38.1 Adapter 196K 78.3 SEEM 87.1 64.9 + Guided Attention + Semantic Prompt 85.8 .0 89.3 +3.5 3 Mask Weights 3 92.9 + Scale Tuning 95.3 +6.0 PerSAM-F 95.3 88.1 94.9 Positive Prior + Negative Prior 4.4 ABLATION STUDY Main Components. in table 4, we introduce the high-level target semantics into SAM's decoder for attent semantics. PerSAM-F boosts the score by +6.0% via the efficient scale-aware fine-tuning. we experiment with other parameter-efficient fine-tuning methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. in table 6, we relax the input restrictions to a bounding box designating the expected object. our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. in this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. we hope our work may expand the applicability of SAM to a wider: range of scenarios. onal encoder- decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. the IEEE/CVF Conference on computer vision and pattern recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. lora for efficient stable diffusion fine-tuning. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. an image is worth one word: Personalizing text-to-image generation using textual inversion. IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019.. a dataset for large vocabulary instance segmentation. arXiv preprint arXiv:2211.10155, 2022. arXiv preprint arXiv:2211.10155, 2022. cost aggregation with 4d convolutional swin transformer for few-shot segmentation. in european conference on computer vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen. isual and vision-language representation learning with noisy text supervision. in international conference on machine learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. in European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. arXiv preprint arXiv:2304.02643, 2023. nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. the power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. ntion-guided unified network for panoptic segmentation. yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. video object segmentation with adaptive feature bank and uncertain-region refinement. arXiv preprint arXiv:2004.03829, 2020. ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. onference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. learning how to ask: Querying lms with mixture of soft prompts. odels are unsupervised multitask learners. openAI blog, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark. an, et al. Paco: Parts and attributes of common objects. in Proceedings of the IEEE Conference on computer vision and pattern recognition, pp. 7141-7151, 2023. arXiv preprints arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, and Mohit Bansal. e on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Ed arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. arXiv preprint arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. pyramid scene parsing network, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Tao Yu, Min Li yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. arXiv preprint arXiv:2304.06718, 2023. 15 115 preprint arXiv:2304.06718, 2023.