en	ko
"--- ABSTRACT ---
The long-standing problem of novel view synthesis has many applications, notably in sports broadcasting. Photorealistic novel view synthesis of soccer actions, in particular, is of enormous interest to the broadcast industry. Yet only a few industrial solutions have been proposed, and even fewer that achieve near-broadcast quality of the synthetic replays. Except for their setup of multiple static cameras around the playfield, the best proprietary systems disclose close to no information about their inner workings. Leveraging multiple static cameras for such a task indeed presents a challenge rarely tackled in the literature, for a lack of public datasets: the reconstruction of a large-scale, mostly static environment, with small, fast-moving elements. Recently, the emergence of neural radiance fields has induced stunning progress in many novel view synthesis applications, leveraging deep learning principles to produce photorealistic results in the most challenging settings. In this work, we investigate the feasibility of basing a solution to the task on dynamic NeRFs, i.e., neural models purposed to reconstruct general dynamic content. We compose synthetic soccer environments and conduct multiple experiments using them, identifying key components that help reconstruct soccer scenes with dynamic NeRFs. We show that, although this approach cannot fully meet the quality requirements Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MMSports '23, October 29, 2023, Ottawa, ON, Canada © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0269-3/23/10...$15.https://doi.org/10.1145/3606038.for the target application, it suggests promising avenues toward a cost-efficient, automatic solution. We also make our work dataset and code publicly available, with the goal to encourage further efforts from the research community on the task of novel view synthesis for dynamic soccer scenes. For code, data, and video results, please see https://soccernerfs.isach.be. CCS CONCEPTS + Computing methodologies — Computer vision representations. KEYWORDS 3D reconstruction, scene representation, dynamic, neural radiance fields, sports, soccer ACM Reference Format: Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe. 2023. Dynamic NeRFs for Soccer Scenes. In Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports (MMSports °23), October 29, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3606038.1
--- METHOD ---
ologies — Computer vision representations. KEYWORDS 3D reconstruction, scene representation, dynamic, neural radiance fields, sports, soccer ACM Reference Format: Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe. 2023. Dynamic NeRFs for Soccer Scenes. In Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports (MMSports °23), October 29, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3606038.1 INTRODUCTION Synthesizing novel views of a scene from a sparse sample of images is a long-standing problem in computer vision [7, 18, 31]. A notable field of application is sports broadcasting, in which action replays have a major role in story-telling and performance analysis. As one of the most popular sports, soccer receives a lot of broadcast coverage from top to low-tier competitions all over the world, with much care given to making the viewer experience ever more pleasant and engaging. Augmenting the broadcast production of soccer events with novel-view video synthesis of action replays is --- --MMSports °23, October 29, 2023, Ottawa, ON, Canada therefore very attractive to industrial actors, and a real opportunity for the computer vision research community. Despite the industry interest in novel view synthesis of soccer replays, only a few proprietary systems exist on the market. Indeed, such interest cannot outweigh the need for the highest image quality in broadcast productions; the industry, therefore, imposes very high standards in terms of the photorealism of the synthesized views. One noteworthy system [9] is able to deliver synthetic replays that are stunningly photorealistic, but for a few visual artifacts. Their setup is composed of dozens of very high-resolution static cameras, installed all around the soccer field high up above the bleachers. Their image data are processed by private, proprietary software running on very powerful hardware. These image data remain private as well, and equivalent public datasets are simply nonexistent. The only insight offered by this system to the research community is the validity of using a static multi-camera setup for the task. Even with no image data available, one can reason about the challenges that arise from using an array of distant static cameras as a basis for the reconstruction of a soccer environment. Outdoor sports like soccer are composed of a large static environment, the stadium, and small dynamic elements, the players and the ball. Traditional computer vision methods would most likely have to rely on very high-resolution images, as in [9], to reconstruct an underlying 3D model of the scene able to faithfully render the movements of the small dynamic elements. Having to deal with massive amounts of image data for reconstructing a single, short soccer action is however not a desirable property for a solution. Building on the modern deep learning-based paradigm to computer vision problems, neural radiance fields [24] (NeRFs) have recently become the state of the art for high-quality novel view synthesis, and have been widely improved and extended to produce excellent results in very challenging settings. A notable line of work is dynamic NeRFs, i.e. neural models purposed to reconstruct spatiotemporal content, as opposed to only spatial, static content. This, therefore, begs the question: Are dynamic NeRFs suitable for reconstructing soccer scenes? To find potential answers to this question, we propose this exploratory work, in which we make three important assumptions. First, we only consider camera setups similar to the one used by the aforementioned proprietary system [9], deeming it optimal for the task at hand. Specifically, we use an array of 20 to 30 static cameras, positioned all around the soccer stadium and pointing toward the soccer field. This assumption goes well with the working conditions usually recommended to achieve good performance with NeRFs. Moreover, most NeRFs assume input views to be calibrate: by third-party Structure from Motion (SfM) tools, which are known to bring robust results with such camera setups in mostly static environments, such as a soccer stadium. Second, we limit our study to synthetic soccer datasets, yet we believe its results also apply to real data. As already mentioned, soccer image datasets with the considered camera setups are virtually nonexistent for the public, to the best of our knowledge. We therefore composed synthetic datasets, using public computer graphics engines and models. Because we control the cameras in our 3D virtual environments, this assumption also allows us to leave camera calibration aspects out of the scope of our work. We are Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe confident that our findings remain valid when working on real use cases, given the availability of robust SfM tools, and the reputation of very good photorealism of NeRFs with real image data. Third, we only consider general dynamic NeRFs, i.e., dynamic NeRFs with no domain knowledge, to identify early limitations of the neural-based reconstruction paradigm in the context of our task. Another important reason is that domain-specific priors are often difficult and expensive to produce. For instance, an accurate skeletal reconstruction of the players would be predictably very useful for soccer replay synthesis, but is a hard task in itself, especially with the considered camera setups. Our goal is to avoid resorting to such priors, which are likely to be complex and costly. This assumption also has the advantage to make our study potentially insightful for the use of dynamic NeRFs for other sports than soccer, given similar camera setups. We select recent state-of-the-art general dynamic NeRF models and compare them in three synthetic soccer environments of increasing complexity. Our aim is to progressively transition from ideal conditions for the considered models, to conditions that are similar to the optimal camera setup used in [9]. Our contributions could be summarized as follows: (1) We provide a study of the performance of general dynamic NeRFs on the task of soccer replay synthesis in increasingly complex environments. Models are studied as they were introduced in the literature, then augmented with general, non-domain specific components that we identify. We close the study with a higher-level discussion about limitations and future work. (2) As we wish to foster research efforts toward solving this challenging task, we publicly release our code, including the improving components and
"	"--- ABSTRACT ---
오랜 문제인 새로운 시점 합성은 많은 응용 분야가 있으며, 특히 스포츠 방송에 응용됩니다. 특히 축구 액션의 사실적인 새로운 시점 합성은 방송 산업에 엄청난 관심을 끌고 있습니다. 그러나 산업 솔루션은 몇 가지에 불과하며, 합성 리플레이의 방송 품질에 가까운 품질을 달성하는 솔루션은 그보다 훨씬 적습니다. 경기장 주변에 여러 대의 정적 카메라를 설치한 것을 제외하면, 최고의 독점 시스템은 내부 작동에 대한 정보를 거의 공개하지 않습니다. 이러한 작업에 여러 대의 정적 카메라를 활용하는 것은 실제로 문헌에서 거의 다루지 않는 과제를 제시합니다. 공개 데이터 세트가 부족하기 때문입니다. 작고 빠르게 움직이는 요소가 있는 대규모의 대부분 정적 환경을 재구성하는 것입니다. 최근 신경 광도장의 등장으로 많은 새로운 시점 합성 응용 분야에서 놀라운 진전이 이루어졌으며, 딥 러닝 원리를 활용하여 가장 어려운 설정에서도 사실적인 결과를 생성했습니다. 이 작업에서 우리는 동적 NeRF, 즉 일반적인 동적 콘텐츠를 재구성하기 위한 신경 모델을 기반으로 작업에 대한 솔루션을 구축할 수 있는지 조사합니다. 우리는 합성 축구 환경을 구성하고 이를 사용하여 여러 실험을 수행하여 동적 NeRF로 축구 장면을 재구성하는 데 도움이 되는 핵심 구성 요소를 식별합니다. 이 접근 방식이 품질 요구 사항을 완전히 충족할 수는 없지만 이 작업의 일부 또는 전부를 개인 또는 교실용으로 디지털 또는 하드 카피로 만드는 데 대한 허가는 사본이 이익 또는 상업적 이점을 위해 만들어지거나 배포되지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문이 있는 경우 무료로 부여됩니다. 저자가 아닌 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전 특정 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. MMSports &#39;23, 2023년 10월 29일, 캐나다 온타리오주 오타와 © 2023 저작권은 소유자/저자가 소유합니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0269-3/23/10... $15.https://doi.org/10.1145/3606038.대상 애플리케이션의 경우 비용 효율적인 자동 솔루션에 대한 유망한 경로를 제안합니다. 또한 연구 커뮤니티에서 동적 축구 장면을 위한 새로운 뷰 합성 작업에 대한 추가 노력을 장려하기 위해 작업 데이터 세트와 코드를 공개적으로 제공합니다. 코드, 데이터 및 비디오 결과는 https://soccernerfs.isach.be에서 확인하세요. CCS 개념 ⚫ 컴퓨팅 방법론 → 컴퓨터 비전 표현. 키워드 3D 재구성, 장면 표현, 동적, 신경 광도 필드, 스포츠, 축구 ACM 참조 형식: Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, Gilles Louppe. 2023. 축구 장면을 위한 동적 NeRF. 스포츠에서 멀티미디어 콘텐츠 분석에 관한 제6회 국제 워크숍(MMSports &#39;23)의 진행 사항, 2023년 10월 29일, 캐나다 온타리오주 오타와. ACM, 뉴욕, 뉴욕, 미국, 9페이지. https://doi.org/10.1145/3606038.
--- METHOD ---
ologies → 컴퓨터 비전 표현. 키워드 3D 재구성, 장면 표현, 동적, 신경 광도 필드, 스포츠, 축구 ACM 참조 형식: Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, Gilles Louppe. 2023. 축구 장면을 위한 동적 NeRF. 2023년 10월 29일, 캐나다 온타리오주 오타와에서 열린 제6회 스포츠 멀티미디어 콘텐츠 분석 국제 워크숍(MMSports &#39;23)의 회의록. ACM, 뉴욕, 뉴욕, 미국, 9페이지. https://doi.org/10.1145/3606038. 서론 희소한 이미지 샘플에서 장면의 새로운 뷰를 합성하는 것은 컴퓨터 비전에서 오래된 문제입니다[7, 18, 31]. 주목할 만한 응용 분야는 스포츠 방송으로, 여기서 액션 리플레이는 스토리텔링과 성과 분석에서 중요한 역할을 합니다. 가장 인기 있는 스포츠 중 하나인 축구는 전 세계의 상위권에서 하위권 대회까지 많은 방송 보도를 받고 있으며, 시청자 경험을 더욱 즐겁고 매력적으로 만드는 데 많은 주의를 기울이고 있습니다.액션 리플레이의 새로운 관점 비디오 합성으로 축구 이벤트의 방송 제작을 증강하는 것은 따라서 산업계 관계자에게 매우 매력적이며 컴퓨터 비전 연구 커뮤니티에게는 진정한 기회입니다.축구 리플레이의 새로운 관점 합성에 대한 산업계의 관심에도 불구하고 시장에는 몇 가지 독점 시스템만 있습니다.실제로 그러한 관심은 방송 제작에서 최고의 이미지 품질에 대한 필요성을 능가할 수 없습니다.따라서 산업은 합성된 뷰의 사실성 측면에서 매우 높은 기준을 부과합니다.주목할 만한 시스템[9] 중 하나는 몇 가지 시각적 아티팩트를 제외하고는 놀라울 정도로 사실적인 합성 리플레이를 제공할 수 있습니다.이들의 설정은 수십 개의 매우 고해상도 정적 카메라로 구성되어 있으며, 관중석 위 높은 축구장 주변에 설치되어 있습니다. 그들의 이미지 데이터는 매우 강력한 하드웨어에서 실행되는 비공개 독점 소프트웨어에 의해 처리됩니다. 이러한 이미지 데이터도 비공개로 유지되며 동등한 공개 데이터 세트는 단순히 존재하지 않습니다. 이 시스템이 연구 커뮤니티에 제공하는 유일한 통찰력은 작업에 정적 멀티 카메라 설정을 사용하는 것의 타당성입니다. 사용 가능한 이미지 데이터가 없더라도 축구 환경을 재구성하기 위한 기초로 먼 정적 카메라 배열을 사용하는 데 발생하는 과제에 대해 추론할 수 있습니다. 축구와 같은 야외 스포츠는 대규모 정적 환경, 경기장, 작은 동적 요소, 선수 및 공으로 구성됩니다. 기존의 컴퓨터 비전 방법은 [9]에서와 같이 작은 동적 요소의 움직임을 충실하게 렌더링할 수 있는 장면의 기본 3D 모델을 재구성하기 위해 매우 고해상도 이미지에 의존해야 할 가능성이 가장 큽니다. 그러나 단일 짧은 축구 동작을 재구성하기 위해 방대한 양의 이미지 데이터를 처리해야 하는 것은 솔루션에 대한 바람직한 속성이 아닙니다. 컴퓨터 비전 문제에 대한 최신 딥 러닝 기반 패러다임을 바탕으로 신경 복사장[24](NeRF)은 최근 고품질의 새로운 뷰 합성을 위한 최첨단 기술이 되었으며, 매우 어려운 환경에서도 뛰어난 결과를 낼 수 있도록 광범위하게 개선 및 확장되었습니다. 주목할 만한 연구 분야는 동적 NeRF, 즉 공간적, 정적 콘텐츠만이 아닌 시공간적 콘텐츠를 재구성하기 위한 신경 모델입니다. 따라서 다음과 같은 의문이 생깁니다. 동적 NeRF가 축구 장면을 재구성하는 데 적합할까요? 이 질문에 대한 잠재적인 답을 찾기 위해 세 가지 중요한 가정을 하는 이 탐색 작업을 제안합니다. 첫째, 앞서 언급한 독점 시스템[9]에서 사용하는 것과 유사한 카메라 설정만 고려하여 해당 작업에 최적이라고 간주합니다. 구체적으로, 축구 경기장 주변에 배치하고 축구장을 향하게 하는 20~30대의 정적 카메라 배열을 사용합니다. 이 가정은 NeRF로 좋은 성능을 달성하기 위해 일반적으로 권장되는 작업 조건과 잘 맞습니다. 게다가 대부분의 NeRF는 입력 뷰가 대부분 정적인 환경(예: 축구 경기장)에서 이러한 카메라 설정으로 견고한 결과를 가져오는 것으로 알려진 타사 구조에서 동작(SfM) 도구로 보정된다고 가정합니다. 둘째, 우리는 연구를 합성 축구 데이터 세트로 제한하지만 그 결과는 실제 데이터에도 적용될 수 있다고 믿습니다. 이미 언급했듯이, 고려된 카메라 설정을 사용한 축구 이미지 데이터 세트는 우리가 아는 한 대중에게 사실상 존재하지 않습니다. 따라서 우리는 공용 컴퓨터 그래픽 엔진과 모델을 사용하여 합성 데이터 세트를 구성했습니다. 우리는 3D 가상 환경에서 카메라를 제어하기 때문에 이 가정을 통해 카메라 보정 측면을 작업 범위에서 제외할 수도 있습니다. 우리는 Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, Gilles Louppe가 견고한 SfM 도구의 가용성과 실제 이미지 데이터를 사용한 NeRF의 매우 우수한 포토리얼리즘에 대한 평판을 감안할 때 실제 사용 사례에서 작업할 때 우리의 결과가 여전히 유효하다고 확신합니다. 셋째, 우리는 우리의 과제 맥락에서 신경 기반 재구성 패러다임의 초기 한계를 파악하기 위해 도메인 지식이 없는 동적 NeRF인 일반적인 동적 NeRF만 고려합니다.또 다른 중요한 이유는 도메인별 사전 확률은 종종 생성하기 어렵고 비용이 많이 들기 때문입니다.예를 들어, 선수의 정확한 골격 재구성은 축구 리플레이 합성에 매우 유용할 것으로 예상되지만, 특히 고려된 카메라 설정에서는 그 자체로 어려운 과제입니다.우리의 목표는 복잡하고 비용이 많이 들 가능성이 있는 그러한 사전 확률에 의존하지 않는 것입니다.이 가정은 또한 유사한 카메라 설정을 고려할 때 축구 이외의 다른 스포츠에 동적 NeRF를 사용하는 데 대한 우리의 연구를 잠재적으로 통찰력 있게 만드는 이점이 있습니다.우리는 최근의 최첨단 일반 동적 NeRF 모델을 선택하여 점점 더 복잡해지는 세 가지 합성 축구 환경에서 비교합니다.우리의 목표는 고려된 모델에 대한 이상적인 조건에서 [9]에서 사용된 최적의 카메라 설정과 유사한 조건으로 점진적으로 전환하는 것입니다. 우리의 기여는 다음과 같이 요약될 수 있습니다. (1) 우리는 점점 더 복잡해지는 환경에서 축구 리플레이 합성 작업에 대한 일반 동적 NeRF의 성능에 대한 연구를 제공합니다. 모델은 문헌에 소개된 대로 연구된 다음, 우리가 식별한 일반적이고 도메인에 국한되지 않은 구성 요소로 보강됩니다. 우리는 한계와 향후 작업에 대한 상위 수준의 논의로 연구를 마무리합니다. (2) 우리는 이 어려운 작업을 해결하기 위한 연구 노력을 촉진하고자 하므로 개선된 구성 요소를 포함하여 코드를 공개적으로 릴리스합니다.
"
"--- ABSTRACT ---
—We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model’s depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. I.
--- INTRODUCTION ---
There is increasing interest in Large Language Models (LLMs) for software engineering domains such as code generation [1-9], code translation [10-12], and code testing [13-15]. Models such as Code Llama [9], Codex [8], and ChatGPT [16] have a good statistical understanding of code and suggest likely completions for unfinished code, making them useful for editing and creating software. However, it appears they have not been trained specifically to optimize code. ChatGPT, for instance, will make minor tweaks to a program such as tagging variables to be stored as registers, and will even attempt more substantial optimizations like vectorization, though it easily gets confused and makes mistakes, frequently resulting in incorrect code. Prior works on machine learning-guided code optimization have used hand-built features [17-19], all the way to graph neural networks (GNNs) [20, 21]. However, in all cases, the way the input program is represented to the machine learning algorithm is incomplete, losing some information along the way. For example, MLGO [17] uses numeric features to provide hints for function inlining, but cannot faithfully reproduce the call graph or control flow, etc. PrograML [21] forms graphs of the program to pass to a GNN, but it excludes the values for constants and some type information which prevents reproducing instructions with fidelity. In this work, we ask: can Large Language Models learn to optimize code? LLMs can accept source programs, as is, with a complete, lossless representation. Using text as the input and output representation for a machine learning optimizer has { Core contributors. *Corresponding author: cummins@meta.com desirable properties: text is a universal, portable, and accessible interface, and unlike prior approaches is not specialized to any particular task. We started our investigation into the code-optimizing power of LLMs by replicating the optimizing transformations present in compilers, targeting the industry standard LLVM [22] compiler. LLVM’s optimizer is extremely complex and contains thousands of rules, algorithms, and heuristics in over 1M lines of C++ code. Our expectation was that while LLMs have shown great progress in natural language translation and code generation tasks, they would be incapable of emulating such a complex system. Understanding and applying compiler optimizations require multiple levels of reasoning, arithmetic computation capabilities, and applying complex data structure and graph algorithms, which are capabilities LLMs have shown to lack [23, 24]. We thought this would be a paper about the obvious failings of LLMs that would serve as motivation for future clever ideas to overcome those failings. We were entirely taken by surprise to find that in many cases a sufficiently trained LLM can not only predict the best optimizations to apply to an input code, but it can also directly perform the optimizations without resorting to the compiler at all! Our approach is simple. We begin with a 7B-parameter LLM architecture, taken from LLaMa 2 [25], and initialize it from scratch. We then train it on millions of examples of LLVM assembly, coupled with the best compiler options found by a search for each assembly, as well as the resulting assembly from performing those optimizations. From these examples alone the model learns to optimize code with remarkable accuracy. Our singular contribution is the first application of LLMs to optimizing code. We construct LLMs solely for the purpose of compiler optimization and show that they achieve a singlecompile 3.0% improvement in code size reduction over the compiler versus a search-based approach which achieves 5.0% with 2.5e° compilations and versus state of the state-of-the-art ML approaches that cause regressions and require thousands of compilations. We provide auxiliary experiments and code examples to further characterize the potential and limits of LLMs for code reasoning. Overall we find their efficacy remarkable and think that these results will be of interest to the community. --- --Training Phase Inference Phase i S Tell me what passes to run on the following LLVM-IR to reduce instruction count: Prompt count from 15 to 4: <code> <code> define dso_local void @*iowritesingle""(i32 %0, 132 %1) { %3 = alloca i32, align%4 = alloca 132, alignstore 132 20, i32* %3, alignstore 132 %1, i32* %4, align%5 = load 132, i32 23, alignXG = icmp eq i32 5,br i1 %6, label %10, labeledgel:1 by %B = load 132, i324 43, align%9 = icmp eq i32 x8,br i1 %9, label %10, label1%"" ._erit_edge2"" --critedge2: ret void10: </code> X11 = load 132, 132* x3, alignML £32 @”outb""(i32 X11, 432 %12) br label xret void ? declare dso_local 132 @*outb""(i32, 132) </code> | | !| | | | | | ! ! ! I ! l | ; | | | | | | | | | | || | y Answer Run passes -reg2mem -sroa -sink -simplifycfg -early-cse -jump-threading to reduce instruction define dso_local void @*iowrite_single""(i32 x0, 132 x1) { switch 132 %0, label X""._crit_edge2"" [ 432 @, Label %""._crit_edge!"" 432 1, label %*._crit_edge!"" all 132 @""outb""(i32 20, 132 21) Answer Run passes -reg2mem -sroa -sink -simplifycfg ly-cse -jump-threading Optimized Code define dso_local void @*iowrite_single""(i32 %0, i32 %1) ie switch 132 %0, label %""._crit_edge2"" [ 432 0, label %*._crit_edge1"" 432 1, label %*._crit_edge1”ret void ? Figure 1: Overview of our approach, showing the model input (Prompt) and output (Answer) during training and inference. The prompt contains unoptimized code. The answer contains an optimization pass list, instruction counts, and the which we feed into the compiler, ensuring that the optimized code is correct. Table I: Training data. Each LLVM-IR function is autotuned and used to create a (Prompt, Answer) pair. The n tokens column shows the number of tokens when the prompt is encoded using the Llama 2 [25] tokenizer. unoptimized . n : A size on . instruction . n tokens functions disk count Handwritten 610,610 8,417,799 653.5 MB 214,746,Synthetic 389,390 13,775,149 352.3 MB 158,435,Total 1,000,000 16,411,249 1.0 GB 373,181,Il. PASS ORDERING WITH LLMs In this work we target compiler pass ordering. The pass ordering task is to select from the set of optimizing transformation passes available in a compiler the list of passes that will produce the best result for a particular input code. Manipulating pass orders has been shown to have a considerable impact on both runtime performance and code size [19, 26]. Machine learning approaches to this task have shown good results previously, but struggle with generalizing across different programs [27]. Previous works usually need to compile new programs tens or hundreds of times to try out different configurations and find out the best-performing option, making them impractical for real-world use. We hypothesized that a large language model with sufficient reasoning power would be able to learn to make good optimization decisions without needing this. Most prior work on LLMs for code operates on source languages such as Python. Instead, for the pass ordering problem we require reasoning at the lower level of compiler assembly, known as the Intermediate Representation (IR). optimized code. During inference we generate only the optimization pass list Table II: Test data. unoptimized -Oz functions instruction instruction count count AI-SOCO [31] 8,929 97,800 47,ExeBench [32] 26,806 386,878 181,POJ-104 [33] 310 8,912 4,Transcoder [12] 17,392 289,689 129,CSmith [34] 33,794 647,815 138,YARPGen [35] 12,769 285,360 144,Total 100,000 1,716,354 645,While there exist curated datasets of source languages for pretraining LLMs (e.g. [28-30]), compiler IRs do not make up a significant portion of these datasets, and though models like ChatGPT show some promise of understanding, their ability to reason about IR is far inferior to source languages. We target optimizing LLVM pass orders for code size as in prior works [17, 27], using IR instruction count as an (imperfect) proxy for binary size. The approach is agnostic to the chosen compiler and optimization metric, and we intend to target runtime performance in the future. For now, optimizing for code size simplifies the collection of training data. A. Prompts We present the model with an unoptimized LLVM-IR (such as emitted by the clang frontend) and ask it to produce a list of optimization passes that should be applied to it. Figureshows the format of the input prompt and output text. In this work, we target LLVM 10 and use the optimization flags from opt. There are 122 optimization passes to choose --- --from and passes can be selected more than once in a single sequence. We also include the 6 meta-flags (-O0, -O1, -O2, -O3, -Oz, and -Os) that may each occur only once per pass list. Pass lists can be any length, though in our experiments we found typically up to 9 passes long, for a combinatorial search space of around 1018. As shown in Figure 1, we also include two auxiliary tasks: i) generating the instruction counts of the code before and after the optimizations are applied and ii) generating the output IR after the optimizations are applied. We hypothesize that these would enable better pass-ordering decisions by forcing a deep understanding of the mechanics of code optimization. We verify this experimentally in Section V-B. While the model is trained to generate instruction counts and optimized IR, we do not need those auxiliary tasks for deployment. All we need to do is generate the pass list which we then execute using the compiler. We thus sidestep the problems of correctness that plague techniques that require the output of the model to be trustworthy [10-12, 36]. B. LLVM-IR Normalization We normalize the LLVM-IR that is used for training the LLM using the following rules: we discard comments, debug metadata and attributes, and ensure consistent whitespace by feeding the IR through a custom lexer that retains newlines but standardizes other whitespace and strips indentation. We do this to reduce the length of the LLVM-IR to make maximum use of the limited input size of the LLM (Section III-A). The code in Figure | has been processed in this manner. II. THE MODEL We use the ubiquitous transformer architecture [37]. The transformer is an artificial neural network that employs selfattention over a fixed-size context window. The input text is first tokenized into words and subword units. These are embedded into continuous vector representations and provided as input to the transformer’s encoder, where selfattention mechanisms capture contextual relationships between tokens to encourage the model to understand and process the input text’s semantic structure. The output text is produced by iteratively generating one token at a time. The decoder takes the encoded input along with any previously generated tokens and uses self-attention to predict the next token in the sequence. We greedily sample during decoding to select the most likely token sequence. This process continues until an end-of-sequence token is generated or a predefined maximum length is reached. A. Model Architecture We use the same model architecture and Byte Pair Encoding (BPE) [38] tokenizer as Llama 2 [25], but train our model from scratch. We use the smallest of the Llama 2 configurations:attention heads, 4,096 hidden dimensions, and 32 layers, for a total of 7B parameters. The maximum length of a (prompt, answer) pair is defined by the sequence length. In this work, we use a sequence length N Oo 6% . G > 5 4% c G E 2% 3 Autotuner 0% — ur Approach = T T T T T T T 0.0 0.2 04 0.6 08 10 12#. Train Tokens 1e(a) Performance of generated pass lists. 50% sree Unoptimized (input) code 40%Optimized (output) code w 30% oa < = 20% 10% 0.0 02 0.4 0.6 0.8 10 12#. Train Tokens (b) Accuracy at predicting instruction counts. / — Code compiles o24i; / --- Exact match 0.0 T T T T T T T 0.0 0.2 04 06 08 10 12#. Train Tokens 1e(c) Model-optimized code metrics. Figure 2: Performance on holdout validation set during training. We evaluate performance every 250 training steps (131M train tokens). Parity with -Oz is reached at 393M tokens and peak performance at 10.9B tokens. of 2,048 tokens. The Llama 2 tokenizer achieves an average of 2.02 characters per token when encoding LLVM-IR, so this provides an approximate upper limit on the longest LLVM-IR we can train on at 2KB (since 2KB prompt and 2KB answer = 2,048 tokens). B. Training Data We assembled a large corpus of unoptimized LLVM-IR functions, summarized in Table I. We extracted the functions from datasets of publicly available handwritten C/C++ code and supplemented this with synthetic code generated by C/C++ compiler test generators. In total, our training corpus comprises 1,000,000 deduplicated IR functions, totaling 373M training tokens. We operate at the level of individual IR functions rather than entire modules to maximize the amount of data we can fit inside a 2,048-token sequence length. To find the list of optimization passes that will produce the smallest instruction count we employ autotuning. Our autotuner combines random search and all-to-all results broadcasting between functions, inspired by the work of Liang et. al. [20]. --- --Table III: Performance of different approaches to pass ordering on a test set of unseen LLVM-IR functions from Table IL. All metrics are w.rt. -Oz. Instructions saved is summed over functions improved and instructions regressed is summed over functions regressed. Overall improvement is the sum total instruction count savings w.rt -Oz. The Autotuner achieves the best performance but requires 2.5B additional compilations (949 CPU-days). Our approach achieves 60% of the gains of the autotuner without invoking the compiler once. additional functions _ functions instructions instructions overall compilations improved regressed saved regressed improvement Autotuner 2,522,253,069 6,764 0 30,948 0 5.03% AutoPhase [39] 4,500,000 1,558 8,400 6,522 32,357 -3.85% Coreset-NVP [20] 442,747 3,985 6,072 16,064 28,405 -1.88% Our Approach 0 4,136 526 21,935 3,095 3.01% Table IV: Extending the models in Table III with “-Oz backup”. If a model er predicts a pass list other than -Oz, it also evaluates -Oz and selects the best. optimization. This prevents regressions w.rt -Oz at the expense of additional compilations. additional compilations —_ overall improvement AutoPhase [39] 4,600,000 1.02% Coreset-NVP [20] 542,747 2.55% Our Approach 5,721 3.52% For each function we run random search for a fixed amount of time (780 seconds) and then minimize the best pass list by iteratively removing individual randomly chosen passes to see if they contribute to the instruction count. If not, they are discarded. After performing this on each of the functions we aggregate the set of unique best pass lists and broadcast them across all other functions. Thus, if a pass list was found to work well on one function it is tried on all others. In total, the autotuner compiled each training program an average of 37,424 times, achieving a 5.8% improvement in instruction count reduction over the baseline fixed pass ordering in the compiler provided by -Oz. For our purposes, this autotuning serves as a gold standard for the optimization of each function. While the instruction count savings discovered by the autotuner are significant, the computational cost to reach these wins was 9,016 CPU days. The goal of this work is to achieve some fraction of the performance of the autotuner using a predictive model that does not require running the compiler thousands of times. C. Training Starting from randomly initialized weights, we trained the model for 30,000 steps on 64 V100s for a total training time of 620 GPU days. We use the AdamW optimizer [40] with 6; and £2 values of 0.9 and 0.95. We use a cosine learning rate schedule with 1,000 warm-up steps, a peak learning rate of le—5, and a final learning rate of 1/10th of the peak. We used a batch size of 256 and each batch contains 524,288 tokens for a total of 15.7B training tokens. The full 30,000 steps of training is 7.7 epochs (iterations over the training corpus). During training, we evaluated the model on a holdout validation set of 1,000 unseen IRs that were processed in the same manner as the training set. We evaluate everysteps. IV. EVALUATION In this section, we evaluate the ability of the model to generate pass lists for unseen code and to correctly perform A. Training Results Figure 2 shows the performance during training when evaluated on a holdout validation set of 1,000 unseen LLVM-IR functions. Peak validation performance was achieved by the model at 10.9B training tokens. At peak performance, the code optimized using modelgenerated pass sequences contains 4.4% fewer instructions than when optimized using the compiler’s built-in pass ordering (-Oz). The autotuner achieves a greater instruction count reduction of 5.6%, but this required 27 million compilations of the validation set. The model makes its predictions without invoking the compiler once. Figure 2b shows the error of predicted input and output instruction counts. Prediction of instruction counts for unoptimized code rapidly approaches near-perfect accuracy. Prediction of output instruction count proves more challenging, reaching a Mean Average Percentage Error (MAPE) of 5.9%. Figure 2c evaluates the quality of the generated code using three metrics. The BLEU [41] score shows the similarity between the model-generated code and a reference groundtruth code produced by the compiler using the generated pass list. Code compiles is the frequency that model-generated code compiles without error. Exact match tracks the frequency that the model-generated code is a character-by-character match of the compiler-generated code when optimized using the generated pass list (i.e. how many times BLEU=1). At peak performance, the model achieves an impressive 90.5% rate of generating code that compiles without errors. Furthermore, a BLEU score of 0.952 shows that the modeloptimized code closely approximates that of the compiler, and the exact match frequency is 70%. For comparison, a baseline that simply copies the unoptimized code to the output would achieve a BLEU score of 0.531 and an exact match frequency of 0%, demonstrating that significant manipulation of the input code is required to achieve such high scores. By the end of training performance on the validation set had plateaued. We use the best-performing checkpoint and switch to a 100x larger-scale evaluation for the remainder of the evaluation. B. Comparison to State-of-the-Art In this experiment, we perform a large-scale evaluation of the LLM’s ability to predict pass lists in comparison to baselines. --- --Frequency (log) NOUDESR BIN BUMS E DEC ERY POMYE SX MONEE YUE VEH FESR IUSLOO TRONS ES ORS UCR GUNES wOES Ss SRE UV EUS eeaee’ § SSsqegseesee °3 ‘SSUSESEesS saaee & yeeseezsertes & R sous gagees Zeees 8 $as— SGe? 8° 8 B SPEsRerE2 > 2'""9 F agar of BS af geeteeses 2 Se re SEELT ABSors Bl a 9 uw""ge old? BR 82 g fs S3 35 BAFg a? 28 g E ao "" 8 a-sisr 4oop-unroll @@™ Autotuner mm Our Approach \ \ ER SOS Rass oes eeSooS SLR Leste AR Pass list length Bo £28 ""Ser Sebe ase 258) $8saeS 883 82 skfgcess vevsizes 2 stg Bue ges FS6oes: * sg GES Esressa? gaelsessee 8 F af at""8 ‘asf EvShRs 8° 3 "" ge 2 42°8 3 85 98z= & Figure 3: Frequency that passes occur in the pass list for each of the 100,000 test programs (left), and the length of pass lists (right). -Oz is the starting point for the autotuner and is the dominant result, being the best-found result for 93.2% of autotuned test programs and appearing in an additional 0.6% of pass lists as part of a longer sequence. The model-generated pass distribution tracks the autotuner but slightly overpredicts -Oz (94.3%) and includes 9 passes that the autotuner used on the training set but not on the test set. Results are ordered by decreasing autotuner frequency. define i32 @f1(i8 %0) { define i32 @f1(i8 %0) { define i32 @f1(i8 %0) { %2 = alloca i32, align 4 %2 = zext i8 %0 to i32 %2 = zext i8 %0 to i%3 = alloca i8, align 1 %.off = add i8 %0, 191 %.off = add i8 %0,store i8 %0, i8* %3, align 1 %3 = icmp ult i8 %.off, 26 %3 = icmp ult i8 %.off,%4 = load i8, i8* %3, align 1 br il %3, label $4, label %6 br il %3, label %6, label %._crit_edge zext i8 %4 to iicmp sge i32 %5, 65 4: _crit_edge: br il %6, label %7, label $15 %5 = add nsw i32 $2, 191 %.off£24 = add i8 $0,br label %10 %4 = icmp ult i8 %.of£24,Tt br il %4, label %6, label %._crit_edge%8 = load i8, i8* %3, align 1 6: %9 = zext i8 $8 to i32 %.reloadl6.off = add nsw i32 %2, 159 s_crit_edge9: %10 = icmp sle i32 %9, 90 %7 = icmp ult i32 %.reloadlé.off, 26 %5 = icmp eq i8 %0,br il %10, label %11, label %15 br il %7, label $10, label 88 %spec.select = select il %5, 132 26, ill: 8: ret i32 %spec.select %12 = load i8, i8* %3, align 1 %9 = icmp eq i8 %0,$13 = zext i8 $12 to i32 %. = select il %9, i32 26, i321 6: br label %10 %.sink = phi i32 [191, $1], (159, %._crit_edge] 10: %7 = add nsw i32 %.sink, %33: %.0.reg2mem.0 = phi i32 [%5, %4], ret i32 $%34 = load i32, i32* %2, align 4 [%., 8], [%.reloadl6.off, %6] } ret i32 %} } ret i32 %.0.reg2mem.(a) Input code (39 instructions). (b) Autotuned code (14 instructions) using passes: (c) Model-optimized code (13 instructions) and pass list: -reg2mem -simplifycfg -mem2reg —jump-threading -Os. -reg2mem —instcombine -Os -Ol. Listing 1: An example IR function where the model suggests a better pass list than the autotuner, despite having never seen this code before. For this function the autotuner tried 26k different pass orderings. The pass list generated by the model appears 5 times in the training set of 1,000,000 examples. Datasets We aggregate a broad suite of benchmark datasets for evaluation, summarized in Table II. We deduplicate and exclude IR functions identical to those we trained on. Our test data comprises code from a variety of domains including coding competitions (AI-SOCO [31], POJ-104 [33]), compiler test case generators (CSmith [34], YARPGen [35]), and miscellaneous publicly available code (ExeBench [32], Transcoder [12]). Baselines We compare our approach to three baselines: AutoPhase [39], Coreset-NVP [20], and the Autotuner. AutoPhase [39] is a reinforcement learning approach in which an agent is trained using Proximal Policy Optimization [42] to select the sequence of optimization passes that will maximize cumulative instruction count savings over a fixedlength episode. At each step, the program being optimized is represented to the agent as a 56-dimensional vector of instruction counts and other properties. We replicate the environment of [39] but use the implementation and expanded training regime from [27] in which the agent is trained for 100,000 episodes. We train the agent on the same data as our language model (Table I) and evaluate agent performance periodically during training on a holdout validation set. As in prior works, we use an action space and episode length of 45. Coreset-NVP [20] is a technique that combines iterative search with a learned cost model. First, a greedy search is run on 17,500 benchmarks to determine a Core set of best pass lists. Then a Neural Value Prediction (NVP) is trained on the results of this search, using ProGraML [21] graphs processed by a Graph Convolutional Network as program representation. At inference, Coreset-NVP predicts the normalized reward and tries the first few pass sequences with the highest normalized --- ---15% ~| gums AutoPhase lm Coreset-NVP lam Autotuner Our Approach 10%5% 0% 5% Improvement over -Oz -10% AI-SocoPOj-104TotalExeBenchTranscodercsmithYARPGenFigure 4: Improvement over -Oz by dataset. Handwritten code optimizes more. reward. The total number of passes it is allowed to try for each benchmark is 45, following prior works. We use authorprovided model weights to perform inference on our test set. Finally, we compare it to the Autotuner that we used to generate training data. We autotuned the test dataset in the same manner as the training data, described in Section III-B. Results Table III summarizes the results. Our approach outperforms -Oz, AutoPhase, and Coreset-NVP across all datasets. Overall, the thousands of optimization attempts that are afforded to the autotuner enable it to discover the bestperforming pass lists. AutoPhase and Coreset-NVP are both able to identify pass lists that outperform -Oz but have an overall net negative impact on instruction count due to a large number of regressions. We propose a simple “-Oz backup” extension to overcome this: if a model predicts a pass list other than -Oz, we also run -Oz and select the best of the two options. This prevents regressions w.rt. -Oz, but increases the number of additional compilations by the number of times the model predicts a pass list other than -Oz. Table IV shows the results of the techniques when evaluated in this manner. While this does not help the models find further improvements, the lack of regressions means that AutoPhase and Coreset-NVP now achieve overall improvements over -Oz, though still less than the LLM with or without the -Oz backup. C. Evaluation of Generated Pass Lists Figure 3 shows the frequency with which passes are selected by the autotuner and our model from the previous experiment. The distribution of passes selected by the model broadly tracks the autotuner. -Oz is the most frequently optimal pass. Excluding -Oz, model-generated pass lists have an average length of 3.4 (max 10), and autotuner pass lists have an average length of 3.1 (max 9). 105 of the pass lists generated by the model never appear in the training data. In 710 cases the model-generated pass lists outperform the autotuner on the test set, though improvements are typically small. Listing 1 shows an example where the model-generated 30% mmm AutoPhase li Coreset-NVP @@m Autotuner Our Approach 20% 10% 0% Improvement over -Oz -10% T T T T T 10 20 30 40 50Unoptimized instruction count Figure 5: Improvement over -Oz by input size. Larger codes optimize more. Table V: Compiler errors of model-optimized code on 100,000 unseen inputs. error category n type error 5,instruction forward referenced — 1,undefined value 1,invalid redefinitionsyntax errorinvalid value for constantundefined functionindex errorotherTotal 9,pass list simplifies control flow to fewer blocks, saving one further instruction. Figure 4 breaks down the improvement of each approach to pass ordering by benchmark dataset. The biggest improvements over -Oz is found in the POJ-104 and Transcoder datasets, which both aggregate large amounts of handwritten code, while YARPGen, a random program generator for testing compilers, has the fewest opportunities for improving over -Oz. We discovered that there is a strong correlation between the input program size and the potential performance improvement over -Oz that is found by both the autotuner and the model. Figure 5 plots this trend, showing clearly that larger programs have more opportunities to improve over -Oz. D. Evaluation of Generated Code In this section, we evaluate the quality of model-generated code. To do this we ran the auxiliary training task of generating optimized code for all 100k functions in the test set. Note that this is not required to generate the pass lists evaluated in the previous section. We have made minor edits to the code samples in this section for brevity such as omitting superfluous statements and shortening identifier names. In 90.3% of cases, the model-generated optimized IR compiles, and in 68.4% of cases the output IR matches characterfor-character the ground truth generated by the compiler. We taxonomize the different classes of errors for the 9.7% of cases where the generated IR does not compile in Table V, and Listing 2 provides code examples. --- --error: '%15' defined with type 'i32’ %or.cond = or il $14, %but expected 'il’ (a) The model defined %15 as an integer but later tried to use it as a bool (type error). error: constant expression type mismatch @.str = private unnamed_addr constant [493 x i8] c"" "", align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type $1 = tail call i32 @£1(float -0.47799998483256463, float -1.8159999847412109) (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. define hidden signext i8 @f1() #0 { %1 = alloca i64, alignstore i64 3718042838174166437, i64* %1, align%2 = load i64, i64* %1, align%3 = trunc i64 %2 to iret i8 %(a) Input unoptimized code. define hidden signext i@£1() #0 { ret i8} } define hidden signext i@£1() #0 { ret i8(b) Desired optimized code. (c) Model-generated code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. Run passes ~instcombine -simplifycfg to reduce instruction count from 14 to 7: define dso_local i32 @£1(i32 %0) { %2 = load i64, i64* getelementptr inbounds ( Sstruct.t2, %struct.t2* @gvar, i64 0, i32 0), align%3 = icmp eq i64 %2,%4 = icmp eq i32 %0,%or.cond = or il %3, %%5 = load i32, i32* @S64_MAX, align%6 = select il %or.cond, i32 %5, i32 %ret i32} Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2 reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. define i32 @f1( 432 %0 int fl(int x, int y) { 432 $1. int i= 2; ) align 2 { while (i + i<y) { it=1; } return 2; br label %phi i32 [%7, %6], [2, %2] mul nsw i32 $i, %i %5 = icmp sgt i32 %4, $(b) Equivalent (hand-written) C br il %5, label %8, label %code. 6: define i32 @£1( %7 = add i32 %i, 1 232br label $3 2328: ) align 2 { ret i32 2 ret i132} } (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates.08 = Code compiles = optimize instcount error 0.0.0.0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRs is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The model --- --1.0.050+ 0.— Code compiles —-: Exact match 9.00 III cs | Es => 2 19?2$3 ao z 10+ E 10° = 10° + T T TT T Py Pa sseau < Rasweunnex 3 SESSESESLEPSESL SS essEasssEge 3 BSSB5ELSETESSERVIE SCHR O; 5 = % So PoegtTszsIrFeEsr asses B 28 a PE Ssbeae2egssezsasga 8 26 s Pe S528 Shes asa 3 8ee EB"" SSSSLSSSRSTS > 29s§ &§ €28aeeegtsas ag eo 2 2 gefevperes? ge S 3 ¢ Sartesa 8 8a¢ o a Te? 2238 7S eg 3 * 8 aP8es @ "" gsSs £ 28 — 9 7 * 4oop-load-elim mmm Regressed mmm improved Demeeay SECC RYE RD YAXYepOHE Gsegssez PeEePsseSsgesaeegercrs 2s SBE Saree r esata ae grt eeerus £8 get ee £3 G7 ETE? S BEad gas ec e oO a “oT os a ga 28 2% fag ¢F &€ & # 3syea aap $e @ fe & & Pee Bs ao 28 ef i 8 & x ed e? 3 28 § 8 ; 2 ra 3 = 2 5* 3 5 4 fa 8 9 Fy Ea g e 8 S Ld Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. 5%7 4% . -, G -2o 3% yy a / E 2%g far 8 ler 5 in 47 — 100%data —-- 25% data E --- 50% data — 100% data, No Aux 0% T T T T T T T 0.0 02 04 06 os 10 12#. Train Tokens 1eFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. ‘n training generate overall examples optimized code? improvement 1,000,000 v 4.95% (—) 500,000 v 3.91% (-21%) 250,000 v 3.74% (-24%) 1,000,000 x 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMs [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. We observe an interesting connection between the quality of pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance — not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%. --- --Optimize the following LLVM-IR using -name-anon-globals: @0 = private @anon. 2ef3bda806391c61822366a2a59£2569.0 = private @anon. 95277a486£fed0b6ba33ab3385b3d7bd.0 = private <Gunnamed_addr constant [14 x i8] c™ "", aligndefine dso_local i32 @f1(i8* %0) { %2 = call i32 @£2(i8* %0, i8* getelementptr inbounds ( [14 x i8], [14 x i8]* —@0, @anon. 2e£3bda806391c61822366a2a59£2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, <+i64 0, i64 0)) ret i32 %} (a) Failure due to incomplete information. The -name-anon-globals pass uses the module name to compute a hash. Lacking this, the model hallucinates a random hash. Optimize the following LLVM-IR using -instcombine: @var_12 = external dso_local global i64, align@var_13 = external dso_local global i32, align@var_l4 = external dso_local global i32, aligndefine dso_local void @£1(i64 %arg) { stmp = alloca i64, alignstore i64 %arg, i64* %tmp, alignstmpl = load i64, i64* %tmp, alignstmp2 = sub i64 0, %tmpl stmp3 = sub i64 0, %tmpstore i64 %tmp3, i64* @var_12, store i64 %targ, i64* @var_12, alignstore i64 0, i64* @var_12, alignstore i32 1, i32* @var_13, alignstore i32 0, i32* @var_l4, alignret void align(b) Failed data-flow analysis. The model correctly removes redundant instructions but substites the wrong value for a variable. The model-optimized code compiles and has a high BLEU score, but is incorrect. Listing 6: Example failures from the pass translation experiment. We combine the model input (red), ground-truth (blue), and model-generated (green) texts into a single unified diff for brevity. Black text is common to all three. C. Evaluation of Single Pass Translation In previous sections we trained LLMs to orchestrate optimization passes to produce the best-optimized code. In this section, we evaluate the ability of LLMs to emulate the different optimizations in themselves. For this experiment, the model input is an unoptimized IR and the name of an optimization pass to apply, the output is the IR after applying this pass. Dataset We generate a new dataset for this task using 60 optimization passes and applying them randomly to the programs from Table I. We augment the dataset of unoptimized code with partially optimized code by first running a sequence of randomly selected passes on unoptimized IRs before the desired target pass. We collect 10,000 unique (prompt, answer) examples for each of the 60 passes for a total of 600k examples. Model We trained a new model from scratch on this pass translation dataset. It reached peak performance after 11B training tokens (74 GPU days). Results Figure 7 summarizes model performance. The average BLEU score over all passes is 0.846, with exact character Optimize the following LLVM-IR using -Oz: %sl = type { i32 } @""llvm.used"" = appending global [1 x i8*] [i8* bitcast (i32(%s1*)* @£1 to i8*)], section ""llvm.metadata"" define dso_local i32 @f1(%sl* %0) { %2 = alloca i32, align%3 = alloca %sl*, align%4 = alloca i32, align 4 / store %slx %0, %sl** %3, align 8 $5 = load %sl*, %sl** %3, align 8 r %6 = tail call i32 @£2(%sl* %5) “store i32 %6, i32* %4, align 4 as %7 = load i32, i32* %4, align 4 “aid %8 = icmp slt i32 %7,br il %8, label %9, label %11 “9: % $10 = load i32, i32* %4, align 4store i32 %10, i32* %2, align 4 i br label %12 %10 jo 011: 2 %I3store i32 0, i32* %2, align 4 q br label $12 I3t 12: ~~ ~ %13 = load i32, i32* %2, alignve i32 %13 V%r (a) Model prompt. ou %sl = type { i32 } — define dso_local i32 @f1(%sl* %0) { %2 = tail call i32 @£2(%s1* %0) $3 = icmp slt i32 %2, 0 2 “a%4 = select il %3, i32 %2, i32 0 rat ret i32 %} (b) Model-optimized code. Listing 7: Example of correct generation of optimized IR. The model performed several complex optimizations including control-flow simplification and replacing if-then-else code blocks with instructions. by-character matches 73.7% of the time and compilable code 82.3% of the time. We also plot the frequency with which each of the optimizations appears in a model-generated pass list that improved or regressed performance over -Oz in Table III. We find no correlation between code quality metrics and its frequency in generated pass lists. As can be seen, many passes are learned near-perfectly while others prove more challenging. Of the passes that perform poorly, some of them hint at simple improvements to the representation while others result from deeper limitations of the model’s reasoning. Listing 6a shows an example from the -name-anon-globals pass, which is a simple utility pass that renames anonymous global variables using a hash of the module name. Since we do not provide the module name in the prompt, the LLM is forced to hallucinate random values. We will add the module name to prompts to address this. Listing 6b shows an example from the -instcombine pass. This is a complex pass that is implemented in over 4.5k lines of C++ code in LLVM. We see that the model correctly identifies the instructions to combine, but makes an error in data flow analysis and substitutes an incorrect value. This is an important optimization that frequently occurs in pass lists that outperform -Oz. We will explore an active learning approach in which more --- --examples are provided for complex and difficult passes. Finally, we present an example of correct model optimization in Listing 7. The example combines several non-trivial code manipulations: register allocation, control flow graph simplification, and instruction combining. We visualize the control- and data-flow graphs to help interpret the changes that the model made. Even on the scale of these small IR functions, we find the sophisticated grasp of LLVM-IR semantics demonstrated by the LLM remarkable. The model has learned to perform these optimizations entirely from examples, without access to the compiler implementation. VI. DISCUSSION We have shown that LLMs can near-perfectly emulate many compiler optimizations and outperform prior approaches, but there are limitations. This section aims to provide a pragmatic discussion of limits and directions for future research. A. Context Window The main limitation of LLMs is the limited sequence length of inputs (context window). In this work we target 2k-token context windows and split IRs into individual functions to maximize the amount of code we can fit into the context window. This is undesirable for a number of reasons. First, it limits the context available to the model when making optimization decisions; second, it prevents intra-function optimization; third, we cannot optimize code that does not fit within the context window. Figure 5 suggests that larger programs have more interesting optimization opportunities. Researchers are adopting ever-increasing context windows [45], but finite context windows remain a common concern with LLMs. As new techniques for handling long sequences continue to evolve we plan to incorporate them and apply them to code optimization, e.g. Code Llama’s variant of positional interpolation [46] which is RoPE base period scaling [9] or recent length extrapolation techniques [47]. B. Math Reasoning and Logic Compilers perform lots of arithmetic. Whenever possible expressions are evaluated at compile time to minimize work at runtime and to expose further opportunities for optimization. We see examples of LLMs struggling with this type of reasoning, e.g. failed constant folding (Listing 3) and failed data-flow analysis (Listing 6b). We think that a chain-of-thought approach [48] in which models are taught to decompose complex reasoning problems into incremental steps will prove fruitful. We took the first step in this direction by breaking optimizations down into individual passes in Section V-C. We also plan to focus training on a curriculum of arithmetic and logic, and train LLMs that use tools to compute intermediate results [49, 50]. C. Inference Speed Compilers are fast. It takes two orders of magnitude more time for the model to generate a pass list than it does for the compiler to execute it. While this is much faster than theautotuner it is trained on, it remains an overhead that may prove prohibitive for some applications. That is to say nothing of the difference in compute resources needed to evaluate compiler heuristics vs. a 7B-parameter LLM running on multiple GPUs. In addition to aggressive batching and quantization [51], significant inference speedups can be achieved by specializing the vocabulary to a use case. For example, we can reduce entire subsequences of passes to single vocabulary elements using Byte Pair Encoding so that at inference time fewer tokens need to be generated. VII.
--- RELATED WORK ---
Compiler pass ordering for performance has been exploited for decades [26, 52, 53]. Over the years there have been several approaches using machine learning [18-20, 39, 54, 55]. The application of machine learning in compilers is not limited to pass order and has been applied to many other problems [17, 56-59]. No one has applied LLMs to the problem of pass ordering, we are the first to do so. Neural machine translation is an emerging field that uses language models to transform code from one language to another. Prior examples include compiling C to assembly [11], assembly to C [36, 60], and source-to-source transpilation [10]. In these works code correctness cannot be guaranteed. In our work we use code generation solely as an auxiliary learning task — correctness is supplied by the compiler. Language models have found broad adoption for coding tasks, though few operate at the level of compiler IR. Gallagher et al. train a ROBERTA architecture on LLVM-IR for the purpose of code weakness identification [61] and Transcoder-IR [12] uses LLVM-IR as a pivot point for source-to-source translation. Neither use LLMs for optimization as we do. Many language models have been trained on source code including CodeBERT [62], GraphCodeBERT [63], and CodeTS5 [64] which are trained to perform multiple tasks including code search, code summarization, and documentation generation. LLMs trained on source code have also been used for program fuzzing [13, 14, 65], test generation [15], and automated program repair [66-68]. A large number of useful applications have been explored for language models, however, this is the first work where an LLM is used specifically for optimizing code. Most LLMs are trained at least partly on code [3, 5, 25, 69]. Some LLMs are trained similarly to general models but especially target programming languages and can be used for code completion such as Codex [8] which powers Copilot [70]. The introduction of fill-in-the-middle capabilities is especially useful for real-world code completion use cases and has become common in recent code models such as InCoder [6], SantaCoder [4], StarCoder [1], and Code Llama [9]. Code Llama was also trained to follow instructions and generate code as well as explain its functionalities. While the multi-terabyte training corpora for these models contain some assembly, we believe that a focused exploration of the value of LLMs in the domain of compilers will be of value to the community. This paper aims to provide that. --- --VIII. CONCLUSIONS We present the first steps towards LLMs for code optimization. We construct a model that can predict good optimization strategies for unseen LLVM-IR. Results are promising, though we face challenges in sequence length which limits us to operating over small program fragments, and in arithmetic reasoning which limits the ability of the model to predict the outcome of optimizations. We hope to inspire the research community to push beyond LLMs for simple max-likelihood co le generation and into performance-aware code optimization. REFERENCES R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. “StarCoder: may the source be with you!” In: arXiv:2305.06161 (2023). Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, et al. “Competition-Level Code Generation with AlphaCode”. In: Science 378.6624 (2022). OpenAI. “GPT-4 Technical Report”. In: arXiv:2303.(2023). L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K. Umapathi, C. J. Anderson, et al. “SantaCoder: don’t reach for the stars!” In: arXiv:2301.03988 (2023). A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. “PaLM: Scaling Language Modeling with Pathways”. In: arXiv:2204.02311 (2022). D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. “InCoder: A Generative Model for Code Infilling and Synthesis”. In: arXiv:2204.05999 (2023). S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, et al. “Textbooks Are All You Need”. In: arXiv:2306.11644 (2023). M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, et al. “Evaluating Large Language Models Trained on Code”. In: arXiv:2107.03374 (2021). B. Roziére, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, et al. “Code Llama: Open Foundation Models for Code”. In: arXiv:2308.12950 (2023). M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample. “Unsupervised Translation of Programming Languages”. In: arXiv:2006.03511 (2020). J. Armengol-Estapé and M. F. O’Boyle. “Learning C to x86 Translation: An Experiment in Neural Compilation”. In: arXiv:2108.07639 (2021). M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut, and G. Synnaeve. “Code Translation with Compiler Representations”. In: arXiv:2207.03578 (2022). G. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang, and Z. Wang. “Automated conformance testing for JavaScript engines via deep compiler fuzzing”. In: PLD/. 2021. Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang. “Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models”. In: [SSTA. 2023. M. Schafer, S. Nadi, A. Eghbali, and F. Tip. “Adaptive Test Generation Using a Large Language Model”. In: arXiv:2302.(2023).OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. “MLGO: a Machine Learning Guided Compiler Optimizations Framework”. In: arXiv:2101.04808 (2021). Z. Wang and M. O’Boyle. “Machine Learning in Compiler Optimisation”. In: arXiv: 1805.03441 (2018). H. Leather and C. Cummins. “Machine Learning in Compilers: Past, Present and Future”. In: FDL. 2020. Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, and Y. Tian. “Learning Compiler Pass Orders using Coreset and Normalized Value Prediction”. In: JCML. 2023. C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O’Boyle, and H. Leather. “ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations”. In: ICML. 2021. C. Lattner and V. Adve. “LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation”. In: CGO. 2004. N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, and S. Paul. “Limits for Learning with Language Models”. In: arXiv:2306.(2023). J. Qian, H. Wang, Z. Li, S. Li, and X. Yan. “Limitations of Language Models in Arithmetic and Symbolic Induction”. In: arXiv:2208.05051 (2022). H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models”. In: arXiv:2307.09288 (2023). G. G. Fursin, M. F. P. O’Boyle, and P. M. W. Knijnenburg. “Evaluating Iterative Compilation”. In: LCPC. 2005. C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, and H. Leather. “CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research”. In: CGO. 2022. D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. “The Stack: 3TB of Permissively Licensed Source Code”. In: arXiv:2211.15533 (2022). L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. In: arXiv:2101.00027 (2020). H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt. “CodeSearchNet Challenge: Evaluating the State of Semantic Code Search”. In: arXiv:1909.09436 (2019). A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, and P. Rosso. “Overview of the PAN@FIRE 2020 task on the authorship identification of SOurce COde (AI-SOCO)”. In: FIRE. 2020. J. Armengol-Estapé, J. Woodruff, A. Brauckmann, J. W. d. S. Magalhaes, and M. O’Boyle. “ExeBench: an ML-scale Dataset of Executable C Functions”. In: MAPS. 2022. L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. “Convolutional Neural Networks Over Tree Structures for Programming Language Processing”. In: AAAI. 2016. X. Yang, Y. Chen, E. Eide, and J. Regehr. “Finding and Understanding Bugs in C Compilers”. In: PLDI. 2011. V. Livinskii, D. Babokin, and J. Regehr. “Random Testing for C and C++ Compilers with YARPGen”. In: OOPSLA. 2020. J. Armengol-Estapé, J. Woodruff, C. Cummins, and M. F. O’Boyle. “SLaDe: A Portable Small Language Model Decompiler for Optimized Assembler”. In: arXiv:2305.12520 (2023). A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. “Attention Is All You Need”. In: NeurIPS (2017). --- ---P. Gage. “A New Algorithm for Data Compression”. In: C Users Journal 12.2 (1994). A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, and I. Stoica. “AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning”. In: MLSys. 2020. I. Loshchilov and F. Hutter. “Decoupled Weight Decay Regularization”. In: arXiv:1711.05101 (2017). K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. “BLEU: A
--- EXPERIMENT ---
s and code examples to further characterize the potential and limits of LLMs for code reasoning. Overall we find their efficacy remarkable and think that these results will be of interest to the community. --- --Training Phase Inference Phase i S Tell me what passes to run on the following LLVM-IR to reduce instruction count: Prompt count from 15 to 4: <code> <code> define dso_local void @*iowritesingle""(i32 %0, 132 %1) { %3 = alloca i32, align%4 = alloca 132, alignstore 132 20, i32* %3, alignstore 132 %1, i32* %4, align%5 = load 132, i32 23, alignXG = icmp eq i32 5,br i1 %6, label %10, labeledgel:1 by %B = load 132, i324 43, align%9 = icmp eq i32 x8,br i1 %9, label %10, label1%"" ._erit_edge2"" --critedge2: ret void10: </code> X11 = load 132, 132* x3, alignML £32 @”outb""(i32 X11, 432 %12) br label xret void ? declare dso_local 132 @*outb""(i32, 132) </code> | | !| | | | | | ! ! ! I ! l | ; | | | | | | | | | | || | y Answer Run passes -reg2mem -sroa -sink -simplifycfg -early-cse -jump-threading to reduce instruction define dso_local void @*iowrite_single""(i32 x0, 132 x1) { switch 132 %0, label X""._crit_edge2"" [ 432 @, Label %""._crit_edge!"" 432 1, label %*._crit_edge!"" all 132 @""outb""(i32 20, 132 21) Answer Run passes -reg2mem -sroa -sink -simplifycfg ly-cse -jump-threading Optimized Code define dso_local void @*iowrite_single""(i32 %0, i32 %1) ie switch 132 %0, label %""._crit_edge2"" [ 432 0, label %*._crit_edge1"" 432 1, label %*._crit_edge1”ret void ? Figure 1: Overview of our approach, showing the model input (Prompt) and output (Answer) during training and inference. The prompt contains unoptimized code. The answer contains an optimization pass list, instruction counts, and the which we feed into the compiler, ensuring that the optimized code is correct. Table I: Training data. Each LLVM-IR function is autotuned and used to create a (Prompt, Answer) pair. The n tokens column shows the number of tokens when the prompt is encoded using the Llama 2 [25] tokenizer. unoptimized . n : A size on . instruction . n tokens functions disk count Handwritten 610,610 8,417,799 653.5 MB 214,746,Synthetic 389,390 13,775,149 352.3 MB 158,435,Total 1,000,000 16,411,249 1.0 GB 373,181,Il. PASS ORDERING WITH LLMs In this work we target compiler pass ordering. The pass ordering task is to select from the set of optimizing transformation passes available in a compiler the list of passes that will produce the best result for a particular input code. Manipulating pass orders has been shown to have a considerable impact on both runtime performance and code size [19, 26]. Machine learning approaches to this task have shown good results previously, but struggle with generalizing across different programs [27]. Previous works usually need to compile new programs tens or hundreds of times to try out different configurations and find out the best-performing option, making them impractical for real-world use. We hypothesized that a large language model with sufficient reasoning power would be able to learn to make good optimization decisions without needing this. Most prior work on LLMs for code operates on source languages such as Python. Instead, for the pass ordering problem we require reasoning at the lower level of compiler assembly, known as the Intermediate Representation (IR). optimized code. During inference we generate only the optimization pass list Table II: Test data. unoptimized -Oz functions instruction instruction count count AI-SOCO [31] 8,929 97,800 47,ExeBench [32] 26,806 386,878 181,POJ-104 [33] 310 8,912 4,Transcoder [12] 17,392 289,689 129,CSmith [34] 33,794 647,815 138,YARPGen [35] 12,769 285,360 144,Total 100,000 1,716,354 645,While there exist curated datasets of source languages for pretraining LLMs (e.g. [28-30]), compiler IRs do not make up a significant portion of these datasets, and though models like ChatGPT show some promise of understanding, their ability to reason about IR is far inferior to source languages. We target optimizing LLVM pass orders for code size as in prior works [17, 27], using IR instruction count as an (imperfect) proxy for binary size. The approach is agnostic to the chosen compiler and optimization metric, and we intend to target runtime performance in the future. For now, optimizing for code size simplifies the collection of training data. A. Prompts We present the model with an unoptimized LLVM-IR (such as emitted by the clang frontend) and ask it to produce a list of optimization passes that should be applied to it. Figureshows the format of the input prompt and output text. In this work, we target LLVM 10 and use the optimization flags from opt. There are 122 optimization passes to choose --- --from and passes can be selected more than once in a single sequence. We also include the 6 meta-flags (-O0, -O1, -O2, -O3, -Oz, and -Os) that may each occur only once per pass list. Pass lists can be any length, though in our experiments we found typically up to 9 passes long, for a combinatorial search space of around 1018. As shown in Figure 1, we also include two auxiliary tasks: i) generating the instruction counts of the code before and after the optimizations are applied and ii) generating the output IR after the optimizations are applied. We hypothesize that these would enable better pass-ordering decisions by forcing a deep understanding of the mechanics of code optimization. We verify this experimentally in Section V-B. While the model is trained to generate instruction counts and optimized IR, we do not need those auxiliary tasks for deployment. All we need to do is generate the pass list which we then execute using the compiler. We thus sidestep the problems of correctness that plague techniques that require the output of the model to be trustworthy [10-12, 36]. B. LLVM-IR Normalization We normalize the LLVM-IR that is used for training the LLM using the following rules: we discard comments, debug metadata and attributes, and ensure consistent whitespace by feeding the IR through a custom lexer that retains newlines but standardizes other whitespace and strips indentation. We do this to reduce the length of the LLVM-IR to make maximum use of the limited input size of the LLM (Section III-A). The code in Figure | has been processed in this manner. II. THE MODEL We use the ubiquitous transformer architecture [37]. The transformer is an artificial neural network that employs selfattention over a fixed-size context window. The input text is first tokenized into words and subword units. These are embedded into continuous vector representations and provided as input to the transformer’s encoder, where selfattention mechanisms capture contextual relationships between tokens to encourage the model to understand and process the input text’s semantic structure. The output text is produced by iteratively generating one token at a time. The decoder takes the encoded input along with any previously generated tokens and uses self-attention to predict the next token in the sequence. We greedily sample during decoding to select the most likely token sequence. This process continues until an end-of-sequence token is generated or a predefined maximum length is reached. A. Model Architecture We use the same model architecture and Byte Pair Encoding (BPE) [38] tokenizer as Llama 2 [25], but train our model from scratch. We use the smallest of the Llama 2 configurations:attention heads, 4,096 hidden dimensions, and 32 layers, for a total of 7B parameters. The maximum length of a (prompt, answer) pair is defined by the sequence length. In this work, we use a sequence length N Oo 6% . G > 5 4% c G E 2% 3 Autotuner 0% — ur Approach = T T T T T T T 0.0 0.2 04 0.6 08 10 12#. Train Tokens 1e(a) Performance of generated pass lists. 50% sree Unoptimized (input) code 40%Optimized (output) code w 30% oa < = 20% 10% 0.0 02 0.4 0.6 0.8 10 12#. Train Tokens (b) Accuracy at predicting instruction counts. / — Code compiles o24i; / --- Exact match 0.0 T T T T T T T 0.0 0.2 04 06 08 10 12#. Train Tokens 1e(c) Model-optimized code metrics. Figure 2: Performance on holdout validation set during training. We evaluate performance every 250 training steps (131M train tokens). Parity with -Oz is reached at 393M tokens and peak performance at 10.9B tokens. of 2,048 tokens. The Llama 2 tokenizer achieves an average of 2.02 characters per token when encoding LLVM-IR, so this provides an approximate upper limit on the longest LLVM-IR we can train on at 2KB (since 2KB prompt and 2KB answer = 2,048 tokens). B. Training Data We assembled a large corpus of unoptimized LLVM-IR functions, summarized in Table I. We extracted the functions from datasets of publicly available handwritten C/C++ code and supplemented this with synthetic code generated by C/C++ compiler test generators. In total, our training corpus comprises 1,000,000 deduplicated IR functions, totaling 373M training tokens. We operate at the level of individual IR functions rather than entire modules to maximize the amount of data we can fit inside a 2,048-token sequence length. To find the list of optimization passes that will produce the smallest instruction count we employ autotuning. Our autotuner combines random search and all-to-all results broadcasting between functions, inspired by the work of Liang et. al. [20]. --- --Table III: Performance of different approaches to pass ordering on a test set of unseen LLVM-IR functions from Table IL. All metrics are w.rt. -Oz. Instructions saved is summed over functions improved and instructions regressed is summed over functions regressed. Overall improvement is the sum total instruction count savings w.rt -Oz. The Autotuner achieves the best performance but requires 2.5B additional compilations (949 CPU-days). Our approach achieves 60% of the gains of the autotuner without invoking the compiler once. additional functions _ functions instructions instructions overall compilations improved regressed saved regressed improvement Autotuner 2,522,253,069 6,764 0 30,948 0 5.03% AutoPhase [39] 4,500,000 1,558 8,400 6,522 32,357 -3.85% Coreset-NVP [20] 442,747 3,985 6,072 16,064 28,405 -1.88% Our Approach 0 4,136 526 21,935 3,095 3.01% Table IV: Extending the models in Table III with “-Oz backup”. If a model er predicts a pass list other than -Oz, it also evaluates -Oz and selects the best. optimization. This prevents regressions w.rt -Oz at the expense of additional compilations. additional compilations —_ overall improvement AutoPhase [39] 4,600,000 1.02% Coreset-NVP [20] 542,747 2.55% Our Approach 5,721 3.52% For each function we run random search for a fixed amount of time (780 seconds) and then minimize the best pass list by iteratively removing individual randomly chosen passes to see if they contribute to the instruction count. If not, they are discarded. After performing this on each of the functions we aggregate the set of unique best pass lists and broadcast them across all other functions. Thus, if a pass list was found to work well on one function it is tried on all others. In total, the autotuner compiled each training program an average of 37,424 times, achieving a 5.8% improvement in instruction count reduction over the baseline fixed pass ordering in the compiler provided by -Oz. For our purposes, this autotuning serves as a gold standard for the optimization of each function. While the instruction count savings discovered by the autotuner are significant, the computational cost to reach these wins was 9,016 CPU days. The goal of this work is to achieve some fraction of the performance of the autotuner using a predictive model that does not require running the compiler thousands of times. C. Training Starting from randomly initialized weights, we trained the model for 30,000 steps on 64 V100s for a total training time of 620 GPU days. We use the AdamW optimizer [40] with 6; and £2 values of 0.9 and 0.95. We use a cosine learning rate schedule with 1,000 warm-up steps, a peak learning rate of le—5, and a final learning rate of 1/10th of the peak. We used a batch size of 256 and each batch contains 524,288 tokens for a total of 15.7B training tokens. The full 30,000 steps of training is 7.7 epochs (iterations over the training corpus). During training, we evaluated the model on a holdout validation set of 1,000 unseen IRs that were processed in the same manner as the training set. We evaluate everysteps. IV. EVALUATION In this section, we evaluate the ability of the model to generate pass lists for unseen code and to correctly perform A. Training Results Figure 2 shows the performance during training when evaluated on a holdout validation set of 1,000 unseen LLVM-IR functions. Peak validation performance was achieved by the model at 10.9B training tokens. At peak performance, the code optimized using modelgenerated pass sequences contains 4.4% fewer instructions than when optimized using the compiler’s built-in pass ordering (-Oz). The autotuner achieves a greater instruction count reduction of 5.6%, but this required 27 million compilations of the validation set. The model makes its predictions without invoking the compiler once. Figure 2b shows the error of predicted input and output instruction counts. Prediction of instruction counts for unoptimized code rapidly approaches near-perfect accuracy. Prediction of output instruction count proves more challenging, reaching a Mean Average Percentage Error (MAPE) of 5.9%. Figure 2c evaluates the quality of the generated code using three metrics. The BLEU [41] score shows the similarity between the model-generated code and a reference groundtruth code produced by the compiler using the generated pass list. Code compiles is the frequency that model-generated code compiles without error. Exact match tracks the frequency that the model-generated code is a character-by-character match of the compiler-generated code when optimized using the generated pass list (i.e. how many times BLEU=1). At peak performance, the model achieves an impressive 90.5% rate of generating code that compiles without errors. Furthermore, a BLEU score of 0.952 shows that the modeloptimized code closely approximates that of the compiler, and the exact match frequency is 70%. For comparison, a baseline that simply copies the unoptimized code to the output would achieve a BLEU score of 0.531 and an exact match frequency of 0%, demonstrating that significant manipulation of the input code is required to achieve such high scores. By the end of training performance on the validation set had plateaued. We use the best-performing checkpoint and switch to a 100x larger-scale evaluation for the remainder of the evaluation. B. Comparison to State-of-the-Art In this experiment, we perform a large-scale evaluation of the LLM’s ability to predict pass lists in comparison to baselines. --- --Frequency (log) NOUDESR BIN BUMS E DEC ERY POMYE SX MONEE YUE VEH FESR IUSLOO TRONS ES ORS UCR GUNES wOES Ss SRE UV EUS eeaee’ § SSsqegseesee °3 ‘SSUSESEesS saaee & yeeseezsertes & R sous gagees Zeees 8 $as— SGe? 8° 8 B SPEsRerE2 > 2'""9 F agar of BS af geeteeses 2 Se re SEELT ABSors Bl a 9 uw""ge old? BR 82 g fs S3 35 BAFg a? 28 g E ao "" 8 a-sisr 4oop-unroll @@™ Autotuner mm Our Approach \ \ ER SOS Rass oes eeSooS SLR Leste AR Pass list length Bo £28 ""Ser Sebe ase 258) $8saeS 883 82 skfgcess vevsizes 2 stg Bue ges FS6oes: * sg GES Esressa? gaelsessee 8 F af at""8 ‘asf EvShRs 8° 3 "" ge 2 42°8 3 85 98z= & Figure 3: Frequency that passes occur in the pass list for each of the 100,000 test programs (left), and the length of pass lists (right). -Oz is the starting point for the autotuner and is the dominant result, being the best-found result for 93.2% of autotuned test programs and appearing in an additional 0.6% of pass lists as part of a longer sequence. The model-generated pass distribution tracks the autotuner but slightly overpredicts -Oz (94.3%) and includes 9 passes that the autotuner used on the training set but not on the test set. Results are ordered by decreasing autotuner frequency. define i32 @f1(i8 %0) { define i32 @f1(i8 %0) { define i32 @f1(i8 %0) { %2 = alloca i32, align 4 %2 = zext i8 %0 to i32 %2 = zext i8 %0 to i%3 = alloca i8, align 1 %.off = add i8 %0, 191 %.off = add i8 %0,store i8 %0, i8* %3, align 1 %3 = icmp ult i8 %.off, 26 %3 = icmp ult i8 %.off,%4 = load i8, i8* %3, align 1 br il %3, label $4, label %6 br il %3, label %6, label %._crit_edge zext i8 %4 to iicmp sge i32 %5, 65 4: _crit_edge: br il %6, label %7, label $15 %5 = add nsw i32 $2, 191 %.off£24 = add i8 $0,br label %10 %4 = icmp ult i8 %.of£24,Tt br il %4, label %6, label %._crit_edge%8 = load i8, i8* %3, align 1 6: %9 = zext i8 $8 to i32 %.reloadl6.off = add nsw i32 %2, 159 s_crit_edge9: %10 = icmp sle i32 %9, 90 %7 = icmp ult i32 %.reloadlé.off, 26 %5 = icmp eq i8 %0,br il %10, label %11, label %15 br il %7, label $10, label 88 %spec.select = select il %5, 132 26, ill: 8: ret i32 %spec.select %12 = load i8, i8* %3, align 1 %9 = icmp eq i8 %0,$13 = zext i8 $12 to i32 %. = select il %9, i32 26, i321 6: br label %10 %.sink = phi i32 [191, $1], (159, %._crit_edge] 10: %7 = add nsw i32 %.sink, %33: %.0.reg2mem.0 = phi i32 [%5, %4], ret i32 $%34 = load i32, i32* %2, align 4 [%., 8], [%.reloadl6.off, %6] } ret i32 %} } ret i32 %.0.reg2mem.(a) Input code (39 instructions). (b) Autotuned code (14 instructions) using passes: (c) Model-optimized code (13 instructions) and pass list: -reg2mem -simplifycfg -mem2reg —jump-threading -Os. -reg2mem —instcombine -Os -Ol. Listing 1: An example IR function where the model suggests a better pass list than the autotuner, despite having never seen this code before. For this function the autotuner tried 26k different pass orderings. The pass list generated by the model appears 5 times in the training set of 1,000,000 examples. Datasets We aggregate a broad suite of benchmark datasets for evaluation, summarized in Table II. We deduplicate and exclude IR functions identical to those we trained on. Our test data comprises code from a variety of domains including coding competitions (AI-SOCO [31], POJ-104 [33]), compiler test case generators (CSmith [34], YARPGen [35]), and miscellaneous publicly available code (ExeBench [32], Transcoder [12]). Baselines We compare our approach to three baselines: AutoPhase [39], Coreset-NVP [20], and the Autotuner. AutoPhase [39] is a reinforcement learning approach in which an agent is trained using Proximal Policy Optimization [42] to select the sequence of optimization passes that will maximize cumulative instruction count savings over a fixedlength episode. At each step, the program being optimized is represented to the agent as a 56-dimensional vector of instruction counts and other properties. We replicate the environment of [39] but use the implementation and expanded training regime from [27] in which the agent is trained for 100,000 episodes. We train the agent on the same data as our language model (Table I) and evaluate agent performance periodically during training on a holdout validation set. As in prior works, we use an action space and episode length of 45. Coreset-NVP [20] is a technique that combines iterative search with a learned cost model. First, a greedy search is run on 17,500 benchmarks to determine a Core set of best pass lists. Then a Neural Value Prediction (NVP) is trained on the results of this search, using ProGraML [21] graphs processed by a Graph Convolutional Network as program representation. At inference, Coreset-NVP predicts the normalized reward and tries the first few pass sequences with the highest normalized --- ---15% ~| gums AutoPhase lm Coreset-NVP lam Autotuner Our Approach 10%5% 0% 5% Improvement over -Oz -10% AI-SocoPOj-104TotalExeBenchTranscodercsmithYARPGenFigure 4: Improvement over -Oz by dataset. Handwritten code optimizes more. reward. The total number of passes it is allowed to try for each benchmark is 45, following prior works. We use authorprovided model weights to perform inference on our test set. Finally, we compare it to the Autotuner that we used to generate training data. We autotuned the test dataset in the same manner as the training data, described in Section III-B. Results Table III summarizes the results. Our approach outperforms -Oz, AutoPhase, and Coreset-NVP across all datasets. Overall, the thousands of optimization attempts that are afforded to the autotuner enable it to discover the bestperforming pass lists. AutoPhase and Coreset-NVP are both able to identify pass lists that outperform -Oz but have an overall net negative impact on instruction count due to a large number of regressions. We propose a simple “-Oz backup” extension to overcome this: if a model predicts a pass list other than -Oz, we also run -Oz and select the best of the two options. This prevents regressions w.rt. -Oz, but increases the number of additional compilations by the number of times the model predicts a pass list other than -Oz. Table IV shows the results of the techniques when evaluated in this manner. While this does not help the models find further improvements, the lack of regressions means that AutoPhase and Coreset-NVP now achieve overall improvements over -Oz, though still less than the LLM with or without the -Oz backup. C. Evaluation of Generated Pass Lists Figure 3 shows the frequency with which passes are selected by the autotuner and our model from the previous experiment. The distribution of passes selected by the model broadly tracks the autotuner. -Oz is the most frequently optimal pass. Excluding -Oz, model-generated pass lists have an average length of 3.4 (max 10), and autotuner pass lists have an average length of 3.1 (max 9). 105 of the pass lists generated by the model never appear in the training data. In 710 cases the model-generated pass lists outperform the autotuner on the test set, though improvements are typically small. Listing 1 shows an example where the model-generated 30% mmm AutoPhase li Coreset-NVP @@m Autotuner Our Approach 20% 10% 0% Improvement over -Oz -10% T T T T T 10 20 30 40 50Unoptimized instruction count Figure 5: Improvement over -Oz by input size. Larger codes optimize more. Table V: Compiler errors of model-optimized code on 100,000 unseen inputs. error category n type error 5,instruction forward referenced — 1,undefined value 1,invalid redefinitionsyntax errorinvalid value for constantundefined functionindex errorotherTotal 9,pass list simplifies control flow to fewer blocks, saving one further instruction. Figure 4 breaks down the improvement of each approach to pass ordering by benchmark dataset. The biggest improvements over -Oz is found in the POJ-104 and Transcoder datasets, which both aggregate large amounts of handwritten code, while YARPGen, a random program generator for testing compilers, has the fewest opportunities for improving over -Oz. We discovered that there is a strong correlation between the input program size and the potential performance improvement over -Oz that is found by both the autotuner and the model. Figure 5 plots this trend, showing clearly that larger programs have more opportunities to improve over -Oz. D. Evaluation of Generated Code In this section, we evaluate the quality of model-generated code. To do this we ran the auxiliary training task of generating optimized code for all 100k functions in the test set. Note that this is not required to generate the pass lists evaluated in the previous section. We have made minor edits to the code samples in this section for brevity such as omitting superfluous statements and shortening identifier names. In 90.3% of cases, the model-generated optimized IR compiles, and in 68.4% of cases the output IR matches characterfor-character the ground truth generated by the compiler. We taxonomize the different classes of errors for the 9.7% of cases where the generated IR does not compile in Table V, and Listing 2 provides code examples. --- --error: '%15' defined with type 'i32’ %or.cond = or il $14, %but expected 'il’ (a) The model defined %15 as an integer but later tried to use it as a bool (type error). error: constant expression type mismatch @.str = private unnamed_addr constant [493 x i8] c"" "", align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type $1 = tail call i32 @£1(float -0.47799998483256463, float -1.8159999847412109) (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. define hidden signext i8 @f1() #0 { %1 = alloca i64, alignstore i64 3718042838174166437, i64* %1, align%2 = load i64, i64* %1, align%3 = trunc i64 %2 to iret i8 %(a) Input unoptimized code. define hidden signext i@£1() #0 { ret i8} } define hidden signext i@£1() #0 { ret i8(b) Desired optimized code. (c) Model-generated code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. Run passes ~instcombine -simplifycfg to reduce instruction count from 14 to 7: define dso_local i32 @£1(i32 %0) { %2 = load i64, i64* getelementptr inbounds ( Sstruct.t2, %struct.t2* @gvar, i64 0, i32 0), align%3 = icmp eq i64 %2,%4 = icmp eq i32 %0,%or.cond = or il %3, %%5 = load i32, i32* @S64_MAX, align%6 = select il %or.cond, i32 %5, i32 %ret i32} Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2 reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. define i32 @f1( 432 %0 int fl(int x, int y) { 432 $1. int i= 2; ) align 2 { while (i + i<y) { it=1; } return 2; br label %phi i32 [%7, %6], [2, %2] mul nsw i32 $i, %i %5 = icmp sgt i32 %4, $(b) Equivalent (hand-written) C br il %5, label %8, label %code. 6: define i32 @£1( %7 = add i32 %i, 1 232br label $3 2328: ) align 2 { ret i32 2 ret i132} } (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates.08 = Code compiles = optimize instcount error 0.0.0.0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRs is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The model --- --1.0.050+ 0.— Code compiles —-: Exact match 9.00 III cs | Es => 2 19?2$3 ao z 10+ E 10° = 10° + T T TT T Py Pa sseau < Rasweunnex 3 SESSESESLEPSESL SS essEasssEge 3 BSSB5ELSETESSERVIE SCHR O; 5 = % So PoegtTszsIrFeEsr asses B 28 a PE Ssbeae2egssezsasga 8 26 s Pe S528 Shes asa 3 8ee EB"" SSSSLSSSRSTS > 29s§ &§ €28aeeegtsas ag eo 2 2 gefevperes? ge S 3 ¢ Sartesa 8 8a¢ o a Te? 2238 7S eg 3 * 8 aP8es @ "" gsSs £ 28 — 9 7 * 4oop-load-elim mmm Regressed mmm improved Demeeay SECC RYE RD YAXYepOHE Gsegssez PeEePsseSsgesaeegercrs 2s SBE Saree r esata ae grt eeerus £8 get ee £3 G7 ETE? S BEad gas ec e oO a “oT os a ga 28 2% fag ¢F &€ & # 3syea aap $e @ fe & & Pee Bs ao 28 ef i 8 & x ed e? 3 28 § 8 ; 2 ra 3 = 2 5* 3 5 4 fa 8 9 Fy Ea g e 8 S Ld Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. 5%7 4% . -, G -2o 3% yy a / E 2%g far 8 ler 5 in 47 — 100%data —-- 25% data E --- 50% data — 100% data, No Aux 0% T T T T T T T 0.0 02 04 06 os 10 12#. Train Tokens 1eFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. ‘n training generate overall examples optimized code? improvement 1,000,000 v 4.95% (—) 500,000 v 3.91% (-21%) 250,000 v 3.74% (-24%) 1,000,000 x 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMs [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. We observe an interesting connection between the quality of pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance — not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%. --- --Optimize the following LLVM-IR using -name-anon-globals: @0 = private @anon. 2ef3bda806391c61822366a2a59£2569.0 = private @anon. 95277a486£fed0b6ba33ab3385b3d7bd.0 = private <Gunnamed_addr constant [14 x i8] c™ "", aligndefine dso_local i32 @f1(i8* %0) { %2 = call i32 @£2(i8* %0, i8* getelementptr inbounds ( [14 x i8], [14 x i8]* —@0, @anon. 2e£3bda806391c61822366a2a59£2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, <+i64 0, i64 0)) ret i32 %} (a) Failure due to incomplete information. The -name-anon-globals pass uses the module name to compute a hash. Lacking this, the model hallucinates a random hash. Optimize the following LLVM-IR using -instcombine: @var_12 = external dso_local global i64, align@var_13 = external dso_local global i32, align@var_l4 = external dso_local global i32, aligndefine dso_local void @£1(i64 %arg) { stmp = alloca i64, alignstore i64 %arg, i64* %tmp, alignstmpl = load i64, i64* %tmp, alignstmp2 = sub i64 0, %tmpl stmp3 = sub i64 0, %tmpstore i64 %tmp3, i64* @var_12, store i64 %targ, i64* @var_12, alignstore i64 0, i64* @var_12, alignstore i32 1, i32* @var_13, alignstore i32 0, i32* @var_l4, alignret void align(b) Failed data-flow analysis. The model correctly removes redundant instructions but substites the wrong value for a variable. The model-optimized code compiles and has a high BLEU score, but is incorrect. Listing 6: Example failures from the pass translation experiment. We combine the model input (red), ground-truth (blue), and model-generated (green) texts into a single unified diff for brevity. Black text is common to all three. C. Evaluation of Single Pass Translation In previous sections we trained LLMs to orchestrate optimization passes to produce the best-optimized code. In this section, we evaluate the ability of LLMs to emulate the different optimizations in themselves. For this experiment, the model input is an unoptimized IR and the name of an optimization pass to apply, the output is the IR after applying this pass. Dataset We generate a new dataset for this task using 60 optimization passes and applying them randomly to the programs from Table I. We augment the dataset of unoptimized code with partially optimized code by first running a sequence of randomly selected passes on unoptimized IRs before the desired target pass. We collect 10,000 unique (prompt, answer) examples for each of the 60 passes for a total of 600k examples. Model We trained a new model from scratch on this pass translation dataset. It reached peak performance after 11B training tokens (74 GPU days). Results Figure 7 summarizes model performance. The average BLEU score over all passes is 0.846, with exact character Optimize the following LLVM-IR using -Oz: %sl = type { i32 } @""llvm.used"" = appending global [1 x i8*] [i8* bitcast (i32(%s1*)* @£1 to i8*)], section ""llvm.metadata"" define dso_local i32 @f1(%sl* %0) { %2 = alloca i32, align%3 = alloca %sl*, align%4 = alloca i32, align 4 / store %slx %0, %sl** %3, align 8 $5 = load %sl*, %sl** %3, align 8 r %6 = tail call i32 @£2(%sl* %5) “store i32 %6, i32* %4, align 4 as %7 = load i32, i32* %4, align 4 “aid %8 = icmp slt i32 %7,br il %8, label %9, label %11 “9: % $10 = load i32, i32* %4, align 4store i32 %10, i32* %2, align 4 i br label %12 %10 jo 011: 2 %I3store i32 0, i32* %2, align 4 q br label $12 I3t 12: ~~ ~ %13 = load i32, i32* %2, alignve i32 %13 V%r (a) Model prompt. ou %sl = type { i32 } — define dso_local i32 @f1(%sl* %0) { %2 = tail call i32 @£2(%s1* %0) $3 = icmp slt i32 %2, 0 2 “a%4 = select il %3, i32 %2, i32 0 rat ret i32 %} (b) Model-optimized code. Listing 7: Example of correct generation of optimized IR. The model performed several complex optimizations including control-flow simplification and replacing if-then-else code blocks with instructions. by-character matches 73.7% of the time and compilable code 82.3% of the time. We also plot the frequency with which each of the optimizations appears in a model-generated pass list that improved or regressed performance over -Oz in Table III. We find no correlation between code quality metrics and its frequency in generated pass lists. As can be seen, many passes are learned near-perfectly while others prove more challenging. Of the passes that perform poorly, some of them hint at simple improvements to the representation while others result from deeper limitations of the model’s reasoning. Listing 6a shows an example from the -name-anon-globals pass, which is a simple utility pass that renames anonymous global variables using a hash of the module name. Since we do not provide the module name in the prompt, the LLM is forced to hallucinate random values. We will add the module name to prompts to address this. Listing 6b shows an example from the -instcombine pass. This is a complex pass that is implemented in over 4.5k lines of C++ code in LLVM. We see that the model correctly identifies the instructions to combine, but makes an error in data flow analysis and substitutes an incorrect value. This is an important optimization that frequently occurs in pass lists that outperform -Oz. We will explore an active learning approach in which more --- --examples are provided for complex and difficult passes. Finally, we present an example of correct model optimization in Listing 7. The example combines several non-trivial code manipulations: register allocation, control flow graph simplification, and instruction combining. We visualize the control- and data-flow graphs to help interpret the changes that the model made. Even on the scale of these small IR functions, we find the sophisticated grasp of LLVM-IR semantics demonstrated by the LLM remarkable. The model has learned to perform these optimizations entirely from examples, without access to the compiler implementation. VI. DISCUSSION We have shown that LLMs can near-perfectly emulate many compiler optimizations and outperform prior approaches, but there are limitations. This section aims to provide a pragmatic discussion of limits and directions for future research. A. Context Window The main limitation of LLMs is the limited sequence length of inputs (context window). In this work we target 2k-token context windows and split IRs into individual functions to maximize the amount of code we can fit into the context window. This is undesirable for a number of reasons. First, it limits the context available to the model when making optimization decisions; second, it prevents intra-function optimization; third, we cannot optimize code that does not fit within the context window. Figure 5 suggests that larger programs have more interesting optimization opportunities. Researchers are adopting ever-increasing context windows [45], but finite context windows remain a common concern with LLMs. As new techniques for handling long sequences continue to evolve we plan to incorporate them and apply them to code optimization, e.g. Code Llama’s variant of positional interpolation [46] which is RoPE base period scaling [9] or recent length extrapolation techniques [47]. B. Math Reasoning and Logic Compilers perform lots of arithmetic. Whenever possible expressions are evaluated at compile time to minimize work at runtime and to expose further opportunities for optimization. We see examples of LLMs struggling with this type of reasoning, e.g. failed constant folding (Listing 3) and failed data-flow analysis (Listing 6b). We think that a chain-of-thought approach [48] in which models are taught to decompose complex reasoning problems into incremental steps will prove fruitful. We took the first step in this direction by breaking optimizations down into individual passes in Section V-C. We also plan to focus training on a curriculum of arithmetic and logic, and train LLMs that use tools to compute intermediate results [49, 50]. C. Inference Speed Compilers are fast. It takes two orders of magnitude more time for the model to generate a pass list than it does for the compiler to execute it. While this is much faster than theautotuner it is trained on, it remains an overhead that may prove prohibitive for some applications. That is to say nothing of the difference in compute resources needed to evaluate compiler heuristics vs. a 7B-parameter LLM running on multiple GPUs. In addition to aggressive batching and quantization [51], significant inference speedups can be achieved by specializing the vocabulary to a use case. For example, we can reduce entire subsequences of passes to single vocabulary elements using Byte Pair Encoding so that at inference time fewer tokens need to be generated. VII. RELATED WORK Compiler pass ordering for performance has been exploited for decades [26, 52, 53]. Over the years there have been several approaches using machine learning [18-20, 39, 54, 55]. The application of machine learning in compilers is not limited to pass order and has been applied to many other problems [17, 56-59]. No one has applied LLMs to the problem of pass ordering, we are the first to do so. Neural machine translation is an emerging field that uses language models to transform code from one language to another. Prior examples include compiling C to assembly [11], assembly to C [36, 60], and source-to-source transpilation [10]. In these works code correctness cannot be guaranteed. In our work we use code generation solely as an auxiliary learning task — correctness is supplied by the compiler. Language models have found broad adoption for coding tasks, though few operate at the level of compiler IR. Gallagher et al. train a ROBERTA architecture on LLVM-IR for the purpose of code weakness identification [61] and Transcoder-IR [12] uses LLVM-IR as a pivot point for source-to-source translation. Neither use LLMs for optimization as we do. Many language models have been trained on source code including CodeBERT [62], GraphCodeBERT [63], and CodeTS5 [64] which are trained to perform multiple tasks including code search, code summarization, and documentation generation. LLMs trained on source code have also been used for program fuzzing [13, 14, 65], test generation [15], and automated program repair [66-68]. A large number of useful applications have been explored for language models, however, this is the first work where an LLM is used specifically for optimizing code. Most LLMs are trained at least partly on code [3, 5, 25, 69]. Some LLMs are trained similarly to general models but especially target programming languages and can be used for code completion such as Codex [8] which powers Copilot [70]. The introduction of fill-in-the-middle capabilities is especially useful for real-world code completion use cases and has become common in recent code models such as InCoder [6], SantaCoder [4], StarCoder [1], and Code Llama [9]. Code Llama was also trained to follow instructions and generate code as well as explain its functionalities. While the multi-terabyte training corpora for these models contain some assembly, we believe that a focused exploration of the value of LLMs in the domain of compilers will be of value to the community. This paper aims to provide that. --- --VIII.
--- CONCLUSION ---
S We present the first steps towards LLMs for code optimization. We construct a model that can predict good optimization strategies for unseen LLVM-IR. Results are promising, though we face challenges in sequence length which limits us to operating over small program fragments, and in arithmetic reasoning which limits the ability of the model to predict the outcome of optimizations. We hope to inspire the research community to push beyond LLMs for simple max-likelihood co le generation and into performance-aware code optimization. REFERENCES R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. “StarCoder: may the source be with you!” In: arXiv:2305.06161 (2023). Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, et al. “Competition-Level Code Generation with AlphaCode”. In: Science 378.6624 (2022). OpenAI. “GPT-4 Technical Report”. In: arXiv:2303.(2023). L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K. Umapathi, C. J. Anderson, et al. “SantaCoder: don’t reach for the stars!” In: arXiv:2301.03988 (2023). A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. “PaLM: Scaling Language Modeling with Pathways”. In: arXiv:2204.02311 (2022). D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. “InCoder: A Generative Model for Code Infilling and Synthesis”. In: arXiv:2204.05999 (2023). S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, et al. “Textbooks Are All You Need”. In: arXiv:2306.11644 (2023). M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, et al. “Evaluating Large Language Models Trained on Code”. In: arXiv:2107.03374 (2021). B. Roziére, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, et al. “Code Llama: Open Foundation Models for Code”. In: arXiv:2308.12950 (2023). M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample. “Unsupervised Translation of Programming Languages”. In: arXiv:2006.03511 (2020). J. Armengol-Estapé and M. F. O’Boyle. “Learning C to x86 Translation: An Experiment in Neural Compilation”. In: arXiv:2108.07639 (2021). M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut, and G. Synnaeve. “Code Translation with Compiler Representations”. In: arXiv:2207.03578 (2022). G. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang, and Z. Wang. “Automated conformance testing for JavaScript engines via deep compiler fuzzing”. In: PLD/. 2021. Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang. “Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models”. In: [SSTA. 2023. M. Schafer, S. Nadi, A. Eghbali, and F. Tip. “Adaptive Test Generation Using a Large Language Model”. In: arXiv:2302.(2023).OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. “MLGO: a Machine Learning Guided Compiler Optimizations Framework”. In: arXiv:2101.04808 (2021). Z. Wang and M. O’Boyle. “Machine Learning in Compiler Optimisation”. In: arXiv: 1805.03441 (2018). H. Leather and C. Cummins. “Machine Learning in Compilers: Past, Present and Future”. In: FDL. 2020. Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, and Y. Tian. “Learning Compiler Pass Orders using Coreset and Normalized Value Prediction”. In: JCML. 2023. C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O’Boyle, and H. Leather. “ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations”. In: ICML. 2021. C. Lattner and V. Adve. “LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation”. In: CGO. 2004. N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, and S. Paul. “Limits for Learning with Language Models”. In: arXiv:2306.(2023). J. Qian, H. Wang, Z. Li, S. Li, and X. Yan. “Limitations of Language Models in Arithmetic and Symbolic Induction”. In: arXiv:2208.05051 (2022). H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models”. In: arXiv:2307.09288 (2023). G. G. Fursin, M. F. P. O’Boyle, and P. M. W. Knijnenburg. “Evaluating Iterative Compilation”. In: LCPC. 2005. C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, and H. Leather. “CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research”. In: CGO. 2022. D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. “The Stack: 3TB of Permissively Licensed Source Code”. In: arXiv:2211.15533 (2022). L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. In: arXiv:2101.00027 (2020). H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt. “CodeSearchNet Challenge: Evaluating the State of Semantic Code Search”. In: arXiv:1909.09436 (2019). A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, and P. Rosso. “Overview of the PAN@FIRE 2020 task on the authorship identification of SOurce COde (AI-SOCO)”. In: FIRE. 2020. J. Armengol-Estapé, J. Woodruff, A. Brauckmann, J. W. d. S. Magalhaes, and M. O’Boyle. “ExeBench: an ML-scale Dataset of Executable C Functions”. In: MAPS. 2022. L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. “Convolutional Neural Networks Over Tree Structures for Programming Language Processing”. In: AAAI. 2016. X. Yang, Y. Chen, E. Eide, and J. Regehr. “Finding and Understanding Bugs in C Compilers”. In: PLDI. 2011. V. Livinskii, D. Babokin, and J. Regehr. “Random Testing for C and C++ Compilers with YARPGen”. In: OOPSLA. 2020. J. Armengol-Estapé, J. Woodruff, C. Cummins, and M. F. O’Boyle. “SLaDe: A Portable Small Language Model Decompiler for Optimized Assembler”. In: arXiv:2305.12520 (2023). A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. “Attention Is All You Need”. In: NeurIPS (2017). --- ---P. Gage. “A New Algorithm for Data Compression”. In: C Users Journal 12.2 (1994). A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, and I. Stoica. “AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning”. In: MLSys. 2020. I. Loshchilov and F. Hutter. “Decoupled Weight Decay Regularization”. In: arXiv:1711.05101 (2017). K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. “BLEU: A Method for Automatic Evaluation of Machine Translation”. In: ACL. 2002. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. “Proximal Policy Optimization Algorithms”. In: arXiv: 1707.06347 (2017). M. Paszkowski. LLVM Canon. https : // github . com / michalpaszkowski/LLVM-Canon. W. M. McKeeman. “Differential Testing for Software”. In: Digital Technical Journal 10.1 (1998). J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, and F. Wei. “LongNet: Scaling Transformers to 1,000,000,Tokens”. In: arXiv:2307.02486 (2023). S. Chen, S. Wong, L. Chen, and Y. Tian. “Extending Context Window of Large Language Models via Positional Interpolation”. In: arXiv:2306.15595 (2023). Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. “A Length-Extrapolatable Transformer’. In: arXiv:2212.10554 (2022). J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. “Chain-of-thought prompting elicits reasoning in large language models”. In: NeurIPS. 2022. L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. “Pal: Program-aided language models”. In: ICML. 2023. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. “Training Verifiers to Solve Math Word Problems”. In: arXiv:2110.14168 (2021). G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. “SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models”. In: ICML. 2023. F. Bodin, T. Kisuki, P. Knijnenburg, M. O’Boyle, and E. Rohou. “Tterative Compilation in a Non-linear Optimisation Space”. In: FDO. 1998. T. Kisuki, P. Knijnenburg, and M. O’Boyle. “Combined Selection of Tile Sizes and Unroll Factors using Iterative Compilation”. In: PACT. 2000. F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. O’Boyle, J. Thomson, M. Toussaint, and C. Williams. “Using Machine Learning to Focus Iterative Optimization”. In: CGO. 2006. W. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather. “Minimizing the Cost of Iterative Compilation with Active Learning”. In: CGO. 2017.A. H. Ashouri, M. Elhoushi, Y. Hua, X. Wang, M. A. Manzoor, B. Chan, and Y. Gao. “MLGOPerf: An ML Guided Inliner to Optimize Performance”. In: arXiv:2207.08389 (2022). A. Haj-Ali, N. K. Ahmed, T. Willke, S. Shao, K. Asanovic, and I. Stoica. “NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning”. In: CGO. 2020. C. Cummins, P. Petoumenos, Z. Wang, and H. Leather. “Endto-End Deep Learning of Optimization Heuristics”. In: PACT. 2017. P. M. Phothilimthana, A. Sabne, N. Sarda, K. S. Murthy, Y. Zhou, C. Angermueller, M. Burrows, S. Roy, K. Mandke, R. Farahani, et al. “A Flexible Approach to Autotuning Multipass Machine Learning Compilers”. In: PACT. 2021. I. Hosseini and B. Dolan-Gavitt. “Beyond the C: Retargetable Decompilation using Neural Machine Translation”. In: arXiv:2212.08950 (2022). S. K. Gallagher, W. E. Klieber, and D. Svoboda. LLVM Intermediate Representation for Code Weakness Identification. 2022. Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al. “CodeBERT: A Pretrained Model for Programming and Natural Languages”. In: arXiv:2002.08155 (2020). D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou. “GraphCodeBERT: Pre-training Code Representations with Data Flow”. In: arXiv:2009.08366 (2021). Y. Wang, W. Wang, S. Joty, and S. C. Hoi. “CodeT5: Identifieraware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation”. In: arXiv:2109.00859 (2021). C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang. “Universal Fuzzing via Large Language Models”. In: arXiv:2308.04748 (2023). C. S. Xia and L. Zhang. “Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-shot Learning”. In: arXiv:2207.08281 (2022). C. S. Xia, Y. Wei, and L. Zhang. “Automated Program in the Era of Large Pre-Trained Language Models”. In: 2023. C. S. Xia and L. Zhang. “Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT”. In: arXiv:2304.00385 (2023). H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar, et al. “Llama: Open and efficient foundation language models”. In: arXiv preprint arXiv:2302.13971 (2023). GitHub. Copilot. https://copilot.github.com/. Repair ICSE.
"	"--- ABSTRACT ---
우리는 코드 최적화에 대한 대규모 언어 모델의 새로운 응용 프로그램을 탐구합니다. 우리는 코드 크기에 맞게 LLVM 어셈블리를 최적화하기 위해 처음부터 학습된 7B 매개변수 변환기 모델을 제시합니다. 이 모델은 최적화되지 않은 어셈블리를 입력으로 받고 프로그램을 가장 잘 최적화하기 위한 컴파일러 옵션 목록을 출력합니다. 중요한 점은 학습 중에 모델에 최적화 전후의 명령어 수와 최적화된 코드 자체를 예측하도록 요청한다는 것입니다. 이러한 보조 학습 과제는 모델의 최적화 성능을 크게 개선하고 모델의 이해도를 향상시킵니다. 우리는 대규모 테스트 프로그램 모음에서 평가합니다. 우리의 접근 방식은 컴파일러보다 명령어 수를 3.0% 줄이는 데 도움이 되어 수천 번의 컴파일이 필요한 두 가지 최첨단 기준선을 능가합니다. 더욱이 이 모델은 놀라울 정도로 강력한 코드 추론 능력을 보여주며 91%의 시간 동안 컴파일 가능한 코드를 생성하고 70%의 시간 동안 컴파일러의 출력을 완벽하게 에뮬레이션합니다. I.
--- INTRODUCTION ---
코드 생성[1-9], 코드 변환[10-12], 코드 테스트[13-15]와 같은 소프트웨어 엔지니어링 도메인을 위한 대규모 언어 모델(LLM)에 대한 관심이 증가하고 있습니다. Code Llama[9], Codex[8], ChatGPT[16]와 같은 모델은 코드에 대한 통계적 이해가 뛰어나고 미완료 코드에 대한 완성 가능성을 제안하여 소프트웨어 편집 및 생성에 유용합니다. 그러나 코드를 최적화하도록 특별히 훈련되지 않은 것으로 보입니다. 예를 들어 ChatGPT는 레지스터로 저장될 변수에 태그를 지정하는 것과 같은 프로그램에 사소한 조정을 하고 벡터화와 같은 보다 본질적인 최적화를 시도하지만 혼동되기 쉽고 실수를 하여 종종 잘못된 코드가 생성됩니다. 머신 러닝 기반 코드 최적화에 대한 이전 작업에서는 수작업으로 작성한 기능[17-19]을 사용하여 그래프 신경망(GNN)[20, 21]까지 사용했습니다. 그러나 모든 경우에 입력 프로그램이 머신 러닝 알고리즘에 표현되는 방식은 불완전하여 그 과정에서 일부 정보가 손실됩니다. 예를 들어, MLGO[17]는 숫자형 피처를 사용하여 함수 인라이닝에 대한 힌트를 제공하지만 호출 그래프나 제어 흐름 등을 충실하게 재현할 수 없습니다. PrograML[21]은 GNN에 전달할 프로그램 그래프를 형성하지만 상수 값과 일부 유형 정보를 제외하여 충실하게 지침을 재현하지 못합니다. 이 작업에서 우리는 다음과 같은 질문을 던집니다. 대규모 언어 모델이 코드를 최적화하는 법을 배울 수 있을까요? LLM은 완전하고 손실 없는 표현으로 소스 프로그램을 그대로 허용할 수 있습니다. 머신 러닝 최적화 프로그램의 입력 및 출력 표현으로 텍스트를 사용하는 데는 † 핵심 기여자. *연락처: cummins@meta.com 바람직한 속성: 텍스트는 보편적이고 이식 가능하며 액세스 가능한 인터페이스이며 이전 접근 방식과 달리 특정 작업에 특화되지 않습니다. 우리는 산업 표준 LLVM[22] 컴파일러를 타겟으로 컴파일러에 존재하는 최적화 변환을 복제하여 LLM의 코드 최적화 능력에 대한 조사를 시작했습니다. LLVM의 최적화 프로그램은 매우 복잡하며 100만 줄이 넘는 C++ 코드에 수천 개의 규칙, 알고리즘 및 휴리스틱이 포함되어 있습니다. LLM이 자연어 번역 및 코드 생성 작업에서 큰 진전을 보였지만 그러한 복잡한 시스템을 에뮬레이션할 수는 없을 것이라는 것이 우리의 기대였습니다. 컴파일러 최적화를 이해하고 적용하려면 여러 수준의 추론, 산술 계산 기능, 복잡한 데이터 구조 및 그래프 알고리즘 적용이 필요하며 이는 LLM에 부족한 것으로 나타난 기능입니다[23, 24]. 우리는 이것이 LLM의 명백한 결함에 대한 논문이 될 것이며 이러한 결함을 극복하기 위한 미래의 영리한 아이디어에 대한 동기가 될 것이라고 생각했습니다. 우리는 많은 경우 충분히 훈련된 LLM이 입력 코드에 적용할 최상의 최적화를 예측할 수 있을 뿐만 아니라 컴파일러에 전혀 의존하지 않고도 최적화를 직접 수행할 수 있다는 사실을 발견하고 완전히 놀랐습니다!우리의 접근 방식은 간단합니다.LLaMa 2[25]에서 가져온 7B 매개변수 LLM 아키텍처로 시작하여 처음부터 초기화합니다.그런 다음 각 어셈블리에 대한 검색을 통해 찾은 최상의 컴파일러 옵션과 해당 최적화를 수행하여 얻은 결과 어셈블리와 결합된 수백만 개의 LLVM 어셈블리 예제에서 이를 훈련합니다.이러한 예제에서만 모델은 놀라운 정확도로 코드를 최적화하는 방법을 학습합니다.우리의 유일한 기여는 코드 최적화를 목적으로 LLM을 처음 적용한 것입니다.우리는 컴파일러 최적화만을 위해 LLM을 구성하고 컴파일러보다 코드 크기 감소에서 단일 컴파일 3.0% 개선을 달성한다는 것을 보여줍니다.2.5e 컴파일로 5.0%를 달성하는 검색 기반 접근 방식과 회귀를 일으키고 수천 번의 컴파일이 필요한 최신 ML 접근 방식과 비교합니다. 우리는 코드 추론을 위한 LLM의 잠재력과 한계를 더욱 특성화하기 위해 보조 실험과 코드 예제를 제공합니다. 전반적으로 우리는 그 효능이 놀랍다고 생각하며 이러한 결과가 커뮤니티에 흥미로울 것이라고 생각합니다. 학습 단계 프롬프트 다음 LLVM-IR에서 명령어 수를 줄이기 위해 실행할 패스를 알려주세요: <code>define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { %3 alloca 132, align%alloca 132, alignstore 132 %0, 132* %3, align 4. store 132 %1, 132* %4, align%%load 132, 132* %3, alignicmp eq 132 %5,br 11 %6, label %10, label %7: %8 load 132, 132* %3, align%9 icmp eq 132 %8,br 11 %9, label %10, label %Answer Run passes -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading to reduce instruction count from 15 to 4: <code>define dso local void @&quot;iowrite_single&quot; (132 %0, 132 %1) switch 132 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ._crit_edge1: %call 132 @&quot;outb&quot; (132 %0, 132 %1) br label &quot;._crit_edge2&quot; ._crit_edge2: ret void } 10: %load 132, 132* %3, align%12 load 132, 132* %4, align%13 call 132 @&quot;outb&quot; (i32 %11, 132 %12) br label %14: ret void } declare dso local i32 @&quot;outb&quot; (132, 132)</code></code> 추론 단계 답변 실행 패스 -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading 컴파일러 최적화 코드 define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { switch i32 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ... ret void } 그림 1: 학습 및 추론 중 모델 입력(프롬프트)과 출력(답변)을 보여주는 접근 방식 개요. 프롬프트에는 최적화되지 않은 코드가 포함되어 있습니다. 답변에는 최적화 패스 목록, 명령어 수 및 최적화된 코드가 포함되어 있습니다. 추론 중에는 컴파일러에 제공하는 최적화 패스 목록만 생성하여 최적화된 코드가 올바른지 확인합니다. 표 I: 훈련 데이터. 각 LLVM-IR 기능은 자동 조정되어 (Prompt, Answer) 쌍을 만드는 데 사용됩니다. n 토큰 열은 Llama 2[25] 토크나이저를 사용하여 프롬프트가 인코딩될 때 토큰 수를 보여줍니다. 표 II: 테스트 데이터. n개 함수 n개 함수 손으로 쓴 합성 총 610,389,1,000, 최적화되지 않은 명령어 수 8,417,13,775,16,411, 최적화되지 않은 명령어 수 -Oz 디스크의 명령어 수 크기 n개 토큰 653.5MB 352.3MB 1.0GB 214,746,158,435,373,181, AI-SOCO [31] ExeBench [32] POJ-104 [33] Transcoder [12] 8,26,97,386,47,181,8,4,17,289,129, CSmith [34] YARPGen [35] 33,647,138,12,285,144, 총 100,1,716,645,II. LLMS를 사용한 패스 순서 지정 이 작업에서는 컴파일러 패스 순서를 목표로 합니다. 패스 순서 지정 작업은 컴파일러에서 사용 가능한 최적화 변환 패스 세트에서 특정 입력 코드에 대해 최상의 결과를 생성하는 패스 목록을 선택하는 것입니다. 패스 순서를 조작하면 런타임 성능과 코드 크기에 상당한 영향을 미치는 것으로 나타났습니다[19, 26]. 이 작업에 대한 머신 러닝 접근 방식은 이전에 좋은 결과를 보였지만 다양한 프로그램 간에 일반화하는 데 어려움을 겪었습니다[27]. 이전 작업에서는 일반적으로 다양한 구성을 시도하고 최상의 성능 옵션을 찾기 위해 수십 또는 수백 번 새 프로그램을 컴파일해야 하므로 실제 사용에는 비실용적이었습니다. 충분한 추론 능력을 갖춘 대규모 언어 모델이라면 이것이 필요 없이도 좋은 최적화 결정을 내리는 법을 배울 수 있을 것이라고 가정했습니다. 코드에 대한 LLM에 대한 대부분의 이전 작업은 Python과 같은 소스 언어에서 작동합니다. 대신 패스 순서 지정 문제의 경우 중간 표현(IR)이라고 하는 컴파일러 어셈블리의 하위 수준에서 추론이 필요합니다. LLM 사전 학습을 위한 소스 언어의 큐레이트된 데이터 세트가 있지만(예: [28–30]), 컴파일러 IRS는 이러한 데이터 세트의 상당 부분을 차지하지 않으며 ChatGPT와 같은 모델은 이해에 약간의 가능성을 보여주지만 IR에 대한 추론 능력은 소스 언어에 비해 훨씬 떨어집니다. 이전 연구[17, 27]와 마찬가지로 IR 명령어 수를 바이너리 크기에 대한(완벽하지 않은) 프록시로 사용하여 코드 크기에 대한 LLVM 패스 순서를 최적화하는 것을 목표로 합니다. 이 접근 방식은 선택한 컴파일러 및 최적화 메트릭과 무관하며 앞으로 런타임 성능을 목표로 할 것입니다. 지금으로서는 코드 크기를 최적화하면 학습 데이터 수집이 간소화됩니다. A. 프롬프트 모델에 최적화되지 않은 LLVM-IR(예: clang 프런트엔드에서 내보내는 것)을 제시하고 이에 적용해야 하는 최적화 패스 목록을 생성하도록 요청합니다. 그림은 입력 프롬프트와 출력 텍스트의 형식을 보여줍니다. 이 작업에서 우리는 LLVM 10을 목표로 하고 opt의 최적화 플래그를 사용합니다. 선택할 수 있는 최적화 패스는 122개이며 패스는 단일 시퀀스에서 두 번 이상 선택할 수 있습니다. 또한 패스 목록당 한 번만 발생할 수 있는 6개의 메타 플래그(-00, -01, -02, -03, -Oz 및 -Os)도 포함합니다. 패스 목록은 길이에 제한이 없지만 실험에서 일반적으로 약 1018의 조합 검색 공간에 대해 최대 9개의 패스 길이를 발견했습니다. 그림 1에서 볼 수 있듯이 두 가지 보조 작업도 포함합니다. i) 최적화가 적용되기 전과 후에 코드의 명령어 수를 생성하고 ii) 최적화가 적용된 후에 출력 IR을 생성합니다. 이를 통해 코드 최적화의 메커니즘에 대한 심층적인 이해를 강제하여 더 나은 패스 순서 지정 결정을 내릴 수 있을 것이라고 가정합니다. 섹션 VB에서 실험적으로 이를 검증합니다. 모델은 명령어 수와 최적화된 IR을 생성하도록 학습되었지만 배포에는 이러한 보조 작업이 필요하지 않습니다. 컴파일러를 사용하여 실행하는 패스 목록만 생성하면 됩니다. 따라서 우리는 모델의 출력이 신뢰할 수 있어야 하는 기술을 괴롭히는 정확성의 문제를 회피합니다[10–12, 36].B. LLVM-IR 정규화 우리는 다음 규칙을 사용하여 LLM을 훈련하는 데 사용되는 LLVM-IR을 정규화합니다.우리는 주석을 버리고, 메타데이터와 속성을 디버깅하고, 줄바꿈은 유지하지만 다른 공백을 표준화하고 들여쓰기를 제거하는 사용자 정의 렉서를 통해 IR을 공급하여 일관된 공백을 보장합니다.우리는 LLM의 제한된 입력 크기(섹션 III-A)를 최대한 활용하기 위해 LLVM-IR의 길이를 줄이기 위해 이를 수행합니다.그림 1의 코드는 이러한 방식으로 처리되었습니다.III.모델 우리는 유비쿼터스 변압기 아키텍처[37]를 사용합니다.변압기는 고정된 크기의 컨텍스트 창에서 셀프 어텐션을 사용하는 인공 신경망입니다.입력 텍스트는 먼저 단어와 하위 단어 단위로 토큰화됩니다. 이것들은 연속 벡터 표현에 내장되어 변환기의 인코더에 입력으로 제공되며, 여기서 셀프 어텐션 메커니즘은 토큰 간의 문맥적 관계를 포착하여 모델이 입력 텍스트의 의미 구조를 이해하고 처리하도록 장려합니다. 출력 텍스트는 한 번에 하나의 토큰을 반복적으로 생성하여 생성됩니다. 디코더는 이전에 생성된 토큰과 함께 인코딩된 입력을 가져와 셀프 어텐션을 사용하여 시퀀스의 다음 토큰을 예측합니다. 디코딩하는 동안 탐욕적으로 샘플링하여 가장 가능성 있는 토큰 시퀀스를 선택합니다. 이 프로세스는 시퀀스 끝 토큰이 생성되거나 사전 정의된 최대 길이에 도달할 때까지 계속됩니다. A. 모델 아키텍처 Llama 2 [25]와 동일한 모델 아키텍처와 바이트 쌍 인코딩(BPE) [38] 토크나이저를 사용하지만 모델을 처음부터 학습합니다. Llama 2 구성 중 가장 작은 것인 어텐션 헤드, 4,096개의 숨겨진 차원, 32개의 레이어를 사용하여 총 7B개의 매개변수를 사용합니다. (프롬프트, 답변) 쌍의 최대 길이는 시퀀스 길이에 의해 정의됩니다. 이 작업에서 우리는 시퀀스 길이 개선 -Oz 6% 4% 2% 자동 튜너 우리의 접근 방식 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le50% 40% w 30% 20% MAPE 10% (a) 생성된 패스 목록의 성능.최적화되지 않은(입력) 코드 최적화된(출력) 코드 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le1.0.0.0.40.2(b) 명령어 수 예측 정확도.BLEU 코드는 정확한 일치를 컴파일합니다.0.0.0.0.0.1.1.#. 훈련 토큰 le(c) 모델 최적화된 코드 메트릭.그림 2: 훈련 중 홀드아웃 검증 세트에 대한 성능.우리는 250개 훈련 단계(1억 3,100만 훈련 토큰)마다 성능을 평가합니다. -Oz와의 동등성은 393M 토큰에서 달성되고 10.9B 토큰에서 최고 성능을 달성합니다. 2,048개 토큰. Llama 2 토크나이저는 LLVM-IR을 인코딩할 때 토큰당 평균 2.02자를 달성하므로 이는 2KB에서 학습할 수 있는 가장 긴 LLVM-IR에 대한 대략적인 상한을 제공합니다(2KB 프롬프트와 2KB 답변은 약 2,048개 토큰이기 때문). B. 학습 데이터 우리는 표 I에 요약된 최적화되지 않은 LLVM-IR 함수의 대규모 코퍼스를 조립했습니다. 공개적으로 사용 가능한 수기 C/C++ 코드의 데이터 세트에서 함수를 추출하고 C/C++ 컴파일러 테스트 생성기에서 생성한 합성 코드로 이를 보완했습니다. 전체적으로 학습 코퍼스는 1,000,000개의 중복 제거된 IR 함수로 구성되어 총 373M 학습 토큰입니다. 우리는 2,048토큰 시퀀스 길이에 맞출 수 있는 데이터 양을 극대화하기 위해 전체 모듈이 아닌 개별 IR 기능 수준에서 작업합니다. 가장 작은 명령어 수를 생성하는 최적화 패스 목록을 찾기 위해 자동 튜닝을 사용합니다. 우리의 자동 튜너는 Liang et. al. [20]의 작업에서 영감을 받아 랜덤 검색과 함수 간의 전체 대 전체 결과 브로드캐스팅을 결합합니다.표 III: 표 II의 보이지 않는 LLVM-IR 함수 테스트 세트에서 패스 순서를 지정하는 다양한 접근 방식의 성능. 모든 메트릭은 -Oz에 대한 것입니다. 절감된 명령어는 개선된 함수에 대해 합산되고 회귀된 명령어는 회귀된 함수에 대해 합산됩니다. 전반적인 개선은 -Oz에 대한 총 명령어 수 절감 합계입니다. 자동 튜너는 최상의 성능을 달성하지만 25억 개의 추가 컴파일(949 CPU-일)이 필요합니다. 우리의 접근 방식은 컴파일러를 한 번도 호출하지 않고도 자동 튜너의 이득의 60%를 달성합니다. 추가 기능 컴파일 개선된 기능 회귀된 명령어 저장된 명령어 회귀된 전체 개선 Autotuner AutoPhase [39] Coreset-NVP [20] 우리의 접근 방식 2,522,253,6,30,5.03% 4,500,1,8,6,32,-3.85% 442,3,6,16,28,-1.88%4,21,3,3.01% 표 IV: &quot;-Oz 백업&quot;을 사용하여 표 III의 모델 확장. 모델이 -Oz 이외의 통과 목록을 예측하는 경우 -Oz도 평가하고 최상의 것을 선택합니다. 이렇게 하면 추가 컴파일을 희생하여 -Oz에 대한 회귀가 방지됩니다. 추가 컴파일 AutoPhase [39] Coreset-NVP [20] 4,600,542, 우리의 접근 방식 5, 전체 개선 1.02% 2.55% 3.52% 각 기능에 대해 고정된 시간(780초) 동안 무작위 검색을 실행한 다음, 명령어 수에 기여하는지 확인하기 위해 무작위로 선택한 개별 패스를 반복적으로 제거하여 최상의 패스 목록을 최소화합니다. 기여하지 않으면 삭제합니다. 각 함수에서 이를 수행한 후 고유한 최상의 패스 목록 세트를 집계하여 다른 모든 함수에 브로드캐스트합니다. 따라서 패스 목록이 한 함수에서 잘 작동하는 것으로 확인되면 다른 모든 함수에서 시도합니다. 전체적으로 자동 튜너는 각 학습 프로그램을 평균 37,424번 컴파일하여 -Oz에서 제공하는 컴파일러의 기준 고정 패스 순서에 비해 명령어 수 감소에서 5.8%의 개선을 달성했습니다. 우리의 목적상 이 자동 튜닝은 각 함수의 최적화를 위한 황금 표준 역할을 합니다. 자동 튜너가 발견한 명령어 수 절감은 상당하지만 이러한 성과를 달성하기 위한 계산 비용은 9,016 CPU 일이었습니다. 이 작업의 목표는 컴파일러를 수천 번 실행할 필요가 없는 예측 모델을 사용하여 자동 튜너 성능의 일부를 달성하는 것입니다. C. 학습 무작위로 초기화된 가중치에서 시작하여 64개의 V100에서 30,000단계로 모델을 학습하여 총 학습 시간이 620GPU 일이었습니다. ₁ 및 B2 값이 각각 0.9 및 0.95인 AdamW 옵티마이저[40]를 사용합니다. 1,000개의 워밍업 단계, 1e-5의 최대 학습 속도, 최대 학습 속도의 1/10인 최종 학습 속도를 갖는 코사인 학습 속도 일정을 사용합니다. 256의 배치 크기를 사용했고 각 배치에는 총 15.7B개의 학습 토큰에 대해 524,288개의 토큰이 포함됩니다. 전체 30,000단계의 학습은 7.7에포크(학습 코퍼스에 대한 반복)입니다. 학습하는 동안 학습 세트와 동일한 방식으로 처리된 1,000개의 보이지 않는 IRS의 홀드아웃 검증 세트에서 모델을 평가했습니다. 모든 단계를 평가합니다. IV. 평가 이 섹션에서는 보이지 않는 코드에 대한 패스 목록을 생성하고 최적화를 올바르게 수행하는 모델의 능력을 평가합니다.A. 학습 결과 그림 2는 보이지 않는 LLVM-IR 함수 1,000개의 홀드아웃 검증 세트에서 평가할 때 학습 중 성능을 보여줍니다.최고 검증 성능은 모델이 109억 학습 토큰에서 달성했습니다.최고 성능에서 모델 생성 패스 시퀀스를 사용하여 최적화된 코드는 컴파일러의 기본 제공 패스 순서(-Oz)를 사용하여 최적화했을 때보다 4.4% 적은 명령어를 포함합니다.자동 튜너는 5.6%의 더 큰 명령어 수 감소를 달성하지만 이를 위해 검증 세트를 2,700만 번 컴파일해야 했습니다.모델은 컴파일러를 한 번도 호출하지 않고 예측을 수행합니다.그림 2b는 예측된 입력 및 출력 명령어 수의 오류를 보여줍니다.최적화되지 않은 코드에 대한 명령어 수 예측은 거의 완벽한 정확도에 빠르게 접근합니다.출력 명령어 수 예측은 더 어려워서 평균 평균 백분율 오류(MAPE)가 5.9%에 도달합니다. 그림 2c는 세 가지 지표를 사용하여 생성된 코드의 품질을 평가합니다.BLEU[41] 점수는 모델 생성 코드와 생성된 패스 목록을 사용하여 컴파일러에서 생성한 참조 기준 진실 코드 간의 유사성을 보여줍니다.코드 컴파일은 모델 생성 코드가 오류 없이 컴파일되는 빈도입니다.정확한 일치는 생성된 패스 목록을 사용하여 최적화할 때 모델 생성 코드가 컴파일러 생성 코드와 문자별로 일치하는 빈도(즉, BLEU=1인 횟수)를 추적합니다.최대 성능에서 모델은 오류 없이 컴파일되는 코드를 생성하는 인상적인 90.5% 비율을 달성합니다.또한 BLEU 점수 0.952는 모델 최적화된 코드가 컴파일러의 코드와 매우 유사하고 정확한 일치 빈도가 70%임을 보여줍니다.비교를 위해 최적화되지 않은 코드를 출력에 복사하는 기준선은 BLEU 점수 0.531과 정확한 일치 빈도 0%를 달성하여 이처럼 높은 점수를 달성하려면 입력 코드를 상당히 조작해야 함을 보여줍니다. 훈련이 끝날 무렵 검증 세트의 성능은 정점에 도달했습니다. 가장 성능이 좋은 체크포인트를 사용하고 나머지 평가에서는 100배 더 큰 규모의 평가로 전환합니다. B. 최신 기술과의 비교 이 실험에서는 기준선과 비교하여 LLM의 통과 목록을 예측하는 능력에 대한 대규모 평가를 수행합니다. 빈도(로그) nالسيسسيد السيسي السبيسZOUAб -instcombine -mem2reg -reg2mem -simplifycfg -memcpyopt Η το POJS -02-Os Hoop-rotate -03-newgvn -loop-deletion. -jump-threading. tailcallelim -로드-스토어-벡터라이저 early-csegvn-hoist – -div-remp -early-cse-memssa -추측-실행 -sip-벡터라이저 싱크 재연결 -00-dse-상관-전파 병합 반환 -break-crit-edges -globalopt -nary-재연결 -instsimplify -loop-simplifycfg -simple-loop-unswitch -loop-unswitch -loop-unroll -aggressive-instcombine -die -bdce -scalarizer -dce → adce Hoop-reroll -flattencfg Hoop-vectorize-constprop -functionattrs woipi-doojAutotuner 우리의 접근 방식 -irce → predicat -mldst-motion Hoop-predication -attributor Hoop-instsimplify hotcoldsplit -Hoop-load-elim -coro-elide -elim-avail-extern -loop-reduce -Hoop-interchange -coro-cleanup Hower-constant-intrinsics -loop-versioning -loop-versioning-licm -partial-inliner -pgo-memop-opt + strip + 1 2 3 4 5 6 7 8 9패스 목록 길이 그림 3: 100,000개 테스트 프로그램 각각에 대한 패스 목록에서 패스가 발생하는 빈도(왼쪽)와 패스 목록의 길이(오른쪽). -Oz는 자동 튜너의 시작점이며 지배적인 결과이며 자동 튜닝된 테스트 프로그램의 93.2%에서 가장 잘 발견된 결과이고 더 긴 시퀀스의 일부로 패스 목록의 추가 0.6%에 나타납니다. 모델에서 생성된 패스 분포는 자동 튜너를 추적하지만 -Oz(94.3%)를 약간 과대 예측하고 자동 튜너가 훈련 세트에는 사용했지만 테스트 세트에는 사용하지 않은 9개의 패스를 포함합니다. 결과는 자동 튜너 주파수가 감소하는 순서로 정렬됩니다. define 132 @f1 (i8 %0) { 2 alloca 132, align3 alloca 18, alignstore 18 %0, 18* %3, align4 load 18, 18* %3, align%5= zext 18 %4 to% 6 = icmp sge 132 %5,br il %6, label %7, label7: 8 load 18, 18* %3, align% 9 = zext 18 %8 to10 icmp sle 132 %9,br il 10, label %11, label %11: 12 load 18, 18* %3, align13 zext 18 %12 to<snip 21 lines...> 33: 34 132 로드, 132* %2, alignret 132 %define 132 @f1 (i8 %0) { %2 = zext 18 %0 to%.off 18 %0,3 추가 icmp ult i8 %.off,br il 3, 레이블 %4, 레이블 %4: %add nsw 132 %2,br 레이블 %6: %.reload16.off = nsw 132 %2,7 추가 icmp ult 132 %.reload16.off,br il 7, 레이블 %10, 레이블 %8: 9 icmp eq 18 %0,%. = select il %9, 132 26, 132br label %10: %.0.reg2mem.0 = phi i32 [%5, %4], [., 8], [%.reload16.off, %6] ret 132.0.reg2mem.define 132 @f1 (i8 %0) { %2 =zext 18 %0에서 %.off로 18 %0,%3 icmp ult 18 %.off,br il %3, label %6, label %._ crit_edge ._crit_edge: %.off24 = 18 %0,%4 icmp ult i8 %.off24,br il 4, label %6, label %.를 추가합니다. crit_edge._crit_edge9: 5 icmp eq 18 %0,spec.select = select il %5, 132 26, 132ret6: } spec.select %.sink = phi 132 [191, %1], [159,._crit_edge] 7 add nsw i32 %.sink, %ret 132 %} (a) 입력 코드(39개 명령어). (b) 패스를 사용한 자동 튜닝 코드(14개 명령어): -reg2mem -inst combine -Os -01. (c) 모델 최적화 코드(13개 명령어) 및 패스 목록: -reg2mem -simplifycfg -mem2reg -jump-threading -Os. 목록 1: 이 코드를 이전에 본 적이 없음에도 불구하고 모델이 자동 튜너보다 더 나은 패스 목록을 제안하는 예제 IR 함수. 이 함수의 경우 자동 튜너는 26,000개의 다른 패스 순서를 시도했습니다. 모델에서 생성한 패스 목록은 1,000,000개의 예제로 구성된 학습 세트에서 5번 나타납니다.데이터 세트 평가를 위해 광범위한 벤치마크 데이터 세트를 집계하여 표 II에 요약했습니다. 학습한 것과 동일한 IR 함수를 중복 제거하고 제외합니다.테스트 데이터는 코딩 대회(AI-SOCO[31], POJ-104[33]), 컴파일러 테스트 케이스 생성기(CSmith[34], YARPGen[35]) 및 기타 공개적으로 사용 가능한 코드(ExeBench[32], Transcoder[12])를 포함한 다양한 도메인의 코드로 구성되어 있습니다.기준선 접근 방식을 AutoPhase[39], Coreset-NVP[20] 및 Autotuner의 세 가지 기준선과 비교합니다.AutoPhase[39]는 고정 길이 에피소드에서 누적 명령어 수 절감을 극대화하는 최적화 패스 시퀀스를 선택하기 위해 Proximal Policy Optimization[42]을 사용하여 에이전트를 학습시키는 강화 학습 접근 방식입니다. 각 단계에서 최적화되는 프로그램은 명령어 수와 기타 속성의 56차원 벡터로 에이전트에 표현됩니다.우리는 [39]의 환경을 복제하지만 에이전트가 100,000개의 에피소드로 훈련되는 [27]의 구현 및 확장된 훈련 체제를 사용합니다.우리는 언어 모델(표 I)과 동일한 데이터에서 에이전트를 훈련하고 보류 검증 세트에서 훈련하는 동안 주기적으로 에이전트 성능을 평가합니다.이전 작업에서와 마찬가지로 45의 동작 공간과 에피소드 길이를 사용합니다.Coreset-NVP[20]는 반복적 검색과 학습된 비용 모델을 결합하는 기술입니다.먼저 17,500개의 벤치마크에서 탐욕적 검색을 실행하여 최상의 패스 목록의 핵심 세트를 결정합니다.그런 다음 그래프 합성곱 네트워크에서 처리한 ProGraML[21] 그래프를 프로그램 표현으로 사용하여 이 검색 결과에 대해 신경 값 예측(NVP)을 훈련합니다. 추론에서 Coreset-NVP는 정규화된 보상을 예측하고 가장 높은 정규화된 보상을 갖는 처음 몇 개의 패스 시퀀스를 시도합니다.-5% -10%0% 30% 15% AutoPhase AutoPhase Coreset-NVP 10% 20% Autotuner Coreset-NVP Autotuner 우리의 접근 방식 우리의 접근 방식 5% 10% ExeBench Transcoder CSmith YARPGen0% -10% TUnoptimized 명령어 수 그림 5: 입력 크기에 따른 -Oz 대비 개선.더 큰 코드는 더 많이 최적화합니다.그림 4: 데이터 세트에 따른 -Oz 대비 개선.수작업으로 작성한 코드는 더 많이 최적화합니다.보상.각 벤치마크에 대해 시도할 수 있는 총 패스 수는 이전 연구에 따라 45입니다.저자가 제공한 모델 가중치를 사용하여 테스트 세트에서 추론을 수행합니다.마지막으로, 이를 학습 데이터를 생성하는 데 사용한 Autotuner와 비교합니다.섹션 III-B에서 설명한 대로 학습 데이터와 동일한 방식으로 테스트 데이터 세트를 자동 튜닝했습니다. 결과 표 III은 결과를 요약한 것입니다. 우리의 접근 방식은 모든 데이터 세트에서 -Oz, AutoPhase, Coreset-NVP보다 성능이 뛰어납니다. 전반적으로 자동 튜너에 제공된 수천 번의 최적화 시도를 통해 가장 성능이 좋은 패스 목록을 발견할 수 있습니다. AutoPhase와 Coreset-NVP는 모두 -Oz보다 성능이 뛰어나지만 회귀가 많아 명령어 수에 전반적으로 부정적인 영향을 미치는 패스 목록을 식별할 수 있습니다. 이를 극복하기 위해 간단한 &quot;-Oz 백업&quot; 확장을 제안합니다. 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 경우 -Oz도 실행하고 두 옵션 중 더 나은 것을 선택합니다. 이렇게 하면 -Oz에 대한 회귀가 방지되지만 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 횟수만큼 추가 컴파일 횟수가 증가합니다. 표 IV는 이러한 방식으로 평가할 때의 기술 결과를 보여줍니다. 이것이 모델이 더 개선되는 데 도움이 되지는 않지만, 회귀가 없기 때문에 AutoPhase와 Coreset-NVP는 이제 -Oz보다 전반적으로 개선되었지만 -Oz 백업이 있거나 없는 LLM보다는 여전히 낮습니다.C. 생성된 패스 목록의 평가 그림 3은 자동 튜너와 이전 실험의 모델이 패스를 선택하는 빈도를 보여줍니다.모델이 선택한 패스의 분포는 자동 튜너를 광범위하게 추적합니다.-Oz가 가장 자주 최적의 패스입니다.-Oz를 제외하고 모델에서 생성된 패스 목록의 평균 길이는 3.4(최대 10)이고 자동 튜너 패스 목록의 평균 길이는 3.1(최대 9)입니다.모델에서 생성된 패스 목록 중 105개는 학습 데이터에 전혀 나타나지 않습니다.710개의 경우에서 모델에서 생성된 패스 목록은 테스트 세트에서 자동 튜너보다 성능이 뛰어나지만 일반적으로 개선 정도는 작습니다. 목록 1은 모델에서 생성된 표 V: 100,000개의 보이지 않는 입력에 대한 모델 최적화 코드의 컴파일러 오류의 예를 보여줍니다. 오류 범주 n 유형 오류 5, 명령어 전방 참조 1, 정의되지 않은 값 1, 잘못된 재정의 구문 오류 상수에 대한 잘못된 값 정의되지 않은 함수 인덱스 오류 기타 9, 전체 패스 목록은 제어 흐름을 더 적은 블록으로 단순화하여 하나의 추가 명령어를 저장합니다. 그림 4는 벤치마크 데이터 집합에 따른 패스 순서 지정에 대한 각 접근 방식의 개선 사항을 분석합니다. -Oz에 비해 가장 큰 개선 사항은 POJ-104 및 Transcoder 데이터 집합에서 발견되는데, 둘 다 대량의 수기 코드를 집계하는 반면 컴파일러를 테스트하기 위한 무작위 프로그램 생성기인 YARPGen은 -Oz에 비해 개선할 수 있는 기회가 가장 적습니다. 우리는 입력 프로그램 크기와 자동 튜너와 모델 모두에서 발견되는 -Oz에 비해 잠재적인 성능 개선 사이에 강력한 상관 관계가 있음을 발견했습니다. 그림 5는 이러한 추세를 나타내며, 더 큰 프로그램이 -Oz에 비해 개선할 수 있는 기회가 더 많다는 것을 명확하게 보여줍니다. D. 생성된 코드 평가 이 섹션에서는 모델에서 생성된 코드의 품질을 평가합니다. 이를 위해 테스트 세트의 모든 100k 함수에 대해 최적화된 코드를 생성하는 보조 학습 작업을 실행했습니다. 이는 이전 섹션에서 평가된 패스 목록을 생성하는 데 필요하지 않습니다. 이 섹션의 코드 샘플에서는 불필요한 명령문을 생략하고 식별자 이름을 줄이는 등 간결성을 위해 사소한 편집을 했습니다. 90.3%의 경우 모델에서 생성된 최적화된 IR이 컴파일되고 68.4%의 경우 출력 IR이 컴파일러에서 생성한 기준 진실과 일치합니다. 생성된 IR이 컴파일되지 않는 9.7%의 경우에 대한 다양한 오류 클래스를 표 V에 분류하고 목록 2에 코드 예제가 나와 있습니다.오류: &#39;15&#39;가 &#39;i32&#39; 유형으로 정의되었지만 &#39;il&#39;을 예상함 %or.cond = or il %14, %(a) 모델은 %15를 정수로 정의했지만 나중에 부울(유형 오류)로 사용하려고 했습니다. 오류: 상수 표현식 유형 불일치 e.str private unnamed_addr 상수 [493 x 18] 정의 132 @f1( 132 %0, 132 %) 정렬 2 { br 레이블 %int f1 (int x, int y) { int i = 2; while (ii &lt; y) { i += 1; return 2; } 3: } % i = phi 132[%7, %6], [2, %2] %4 = mul nsw i32 %i, i %c&quot; <snip 492 chars ', align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type %1 tail call 132 @f1 (float -0.47799998483256463, float -1.8159999847412109) = icmp sgt 132 %4, %br il 5, label %8, label %6:add 132 %i,br label %8: ret 132} (b) Equivalent (hand-written) C code. define 132 @f1( 132 %0, 132 %) align 2 { ret 132} (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates. 1.0.define hidden signext 18 @f1() %230 { %1 alloca 164, align0.store 164 3718042838174166437, 164* %1, align%2 load 164, 164* %1, align0.%trunc i64 %2 to} ret 18 %0.BLEU Code compiles Optimized instcount error I (a) Input unoptimized code. define hidden signext@f1() %30 { ret 18define hidden signext@f1() #230 { ret 18} } (c) Model-generated code. (b) Desired optimized code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. 0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Run passes -inst combine -simplifycfg to reduce instruction count from 14 to 7: define dso_local 132 @f1 (132 %0) { } % 2 = load 164, 164* getelementptr inbounds ( struct.t2, %struct.t2* @gvar, 164 0, 132 0), align3 icmp eq 164 %2, O %4= icmp eq 132 %0,%or.cond or il %3, %5 load 132, 132* @S64_MAX, align%6 = select il %or.cond, 132 %5, 132 %ret 132 %Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRS is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The modelFrequency (log) 0.0.0.-name-anon-globals -Hoop-versioning-licm -strip -strip-dead-prototypes. -indvars -loop-reroll -strip-nondebug. -loop-unroll Hoop-unswitch. -instcombine. -loop-instsimplify-Hoop-idiom -Hoop-deletion -Hoop-rotate Hoop-unroll-and-jam-loop-guard-widening -Hoop-predication. -die -irce 1.0.75globalopt -01. -simple-loop-unswitch -loop-interchange -Os --Oz. -licm -loop-sink-loop-load-elim -loop-simplifycfg -loop-vectorize --sroa Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. gvn-hoist . -loop-simplify -tailcallelim -speculative-execution. -early-cse -dse. -jump-threading -loop-reduce -sink -simplifycfg Improvement over -Oz 5% 4% 3% 2% 1%100% data 50% data 25% data 0% 0.0.0.0.#. Train Tokens 0.1.1.100% data, No Aux T 1.leFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. overall improvement n training examples generate optimized code? 1,000,500,250,1,000,× 4.95% (-) 3.91% (-21%) 3.74% (-24%) 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMS [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. of We observe an interesting connection between the quality pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%.илбмәи gvn . early-cse-memssa. -bdce-correlated-propagation Hcssa. - бәлгшәшadce -break-crit-edges. -ipsccp-dce-globaldce aggressive-instcombine. -instsimplify -sccp -reassociate -loop-fusion Regressed Improved Optimize the following LLVM-IR using -name-anon-globals: @0 private @anon. 2ef3bda806391c61822366a2a59f2569.0 = private @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0 = private unnamed_addr constant [14 x 18] c""<snip> &quot;, aligndefine dso_local 132 @f1 (18* %0) { %2 = call i32 @f2 (18* %0, 18* getelementptr inbounds ( [14 x 18], [14 x 18] * } -20, @anon. 2ef3bda806391c61822366a2a59f2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, 164 0, 164 0)) ret 132 %(a) 불완전한 정보로 인한 실패. -name-anon-globals 패스는 모듈 이름을 사용하여 해시를 계산합니다. 이것이 없으면 모델은 무작위 해시를 환각합니다. -inst combine을 사용하여 다음 LLVM-IR을 최적화하세요. @var 12 = external dso_local 글로벌 164, align@var 13 외부 dso_local 글로벌 132, align@var_14 = 외부 dso_local 글로벌 132, aligndefine dso_local void @f1 (164 %arg) { tmp = 할당 164, 정렬 저장소 164 %arg, 164* %tmp, aligntmp1 = 로드 164, 164* %tmp, aligntmp2 = 하위 164 0, %tmptmp3 = 하위 164 0, %tmp 저장소 164 %tmp3, 164* @var_12, 정렬 저장소 164 %arg, 164*2B @var_12, 정렬 저장소 164 0, 164* @var 12, 정렬 저장소 i32 1, 132* @var_13, alignstore 132 0, 132* @var_14, alignret void -Oz를 사용하여 다음 LLVM-IR을 최적화합니다. %s1 = type { 132 } @&quot;llvm.used&quot; = 전역 추가 [1 x 18*] [18* 비트캐스트(i32(%s1*) * @fl에서 18*로)], 섹션 &quot;llvm.metadata&quot; define dso_local 132 @f1 (%sl* %0) { %alloca 132, alignalloca %sl*, align%alloca 132, alignstore %s %0, %s1** %3, align5 로드 sl*, %s1** %3, align%6 tail call 132 @f2 (%s1* %5) 스토어 i32 %6, 132* %4, 정렬 4. %7=로드 132, 132* %4, 정렬 4. %8 = icmp slt i32 %7,br il %8, 레이블 %9, 레이블 %9: %%%tail() %%%10 132, 132* %4 로드, 정렬저장 132 %10, 132* %2, 정렬br 레이블 %%%&lt;11: %%%저장 132 0, 132* %2, 정렬 4. br 레이블 %ret 12: } %13 132, 132* %2 로드, 정렬ret 132 %%(a) 모델 프롬프트. tail() %
--- RELATED WORK ---
성능을 위한 컴파일러 패스 순서는 수십 년 동안 활용되어 왔습니다[26, 52, 53]. 수년에 걸쳐 머신 러닝을 사용하는 여러 가지 접근 방식이 있었습니다[18-20, 39, 54, 55]. 컴파일러에서 머신 러닝을 적용하는 것은 패스 순서에 국한되지 않으며 다른 많은 문제에 적용되었습니다[17, 56-59]. 아무도 패스 순서 문제에 LLM을 적용한 적이 없으며, 우리가 처음입니다. 신경망 기계 번역은 언어 모델을 사용하여 코드를 한 언어에서 다른 언어로 변환하는 새로운 분야입니다. 이전 예로는 C에서 어셈블리로 컴파일[11], 어셈블리에서 C로 컴파일[36, 60], 소스 간 변환[10]이 있습니다. 이러한 작업에서는 코드의 정확성을 보장할 수 없습니다. 저희 작업에서는 코드 생성을 보조 학습 과제로만 사용합니다. 정확성은 컴파일러에서 제공합니다. 언어 모델은 코딩 과제에 널리 채택되었지만 컴파일러 IR 수준에서 작동하는 모델은 거의 없습니다. Gallagher et al. 영어: 코드 취약점 식별[61]을 목적으로 LLVM-IR에서 ROBERTA 아키텍처를 훈련하고 Transcoder-IR[12]은 소스 간 변환을 위한 피벗 포인트로 LLVM-IR을 사용합니다. 둘 다 우리처럼 최적화를 위해 LLM을 사용하지 않습니다. CodeBERT[62], GraphCodeBERT[63], CodeT5[64]를 포함하여 많은 언어 모델이 소스 코드에서 훈련되었으며, 이는 코드 검색, 코드 요약, 문서 생성을 포함한 여러 작업을 수행하도록 훈련되었습니다. 소스 코드에서 훈련된 LLM은 프로그램 퍼징[13, 14, 65], 테스트 생성[15], 자동화된 프로그램 복구[66–68]에도 사용되었습니다. 언어 모델에 대해 많은 유용한 응용 프로그램이 탐색되었지만, 이는 LLM이 특별히 코드 최적화에 사용된 최초의 작업입니다. 대부분의 LLM은 최소한 부분적으로 코드에서 훈련됩니다[3, 5, 25, 69]. 일부 LLM은 일반 모델과 유사하게 훈련되지만 특히 프로그래밍 언어를 대상으로 하며 Copilot [70]을 구동하는 Codex [8]와 같은 코드 완성에 사용될 수 있습니다.중간 채우기 기능의 도입은 실제 코드 완성 사용 사례에 특히 유용하며 InCoder [6], SantaCoder [4], StarCoder [1] 및 Code Llama [9]와 같은 최근 코드 모델에서 일반화되었습니다.Code Llama는 또한 지침을 따르고 코드를 생성하고 기능을 설명하도록 훈련되었습니다.이러한 모델의 멀티 테라바이트 훈련 코퍼스에는 일부 어셈블리가 포함되어 있지만 컴파일러 도메인에서 LLM의 가치에 대한 집중적인 탐구가 커뮤니티에 가치가 있을 것이라고 믿습니다.이 논문은 그것을 제공하는 것을 목표로 합니다.VIII. 결론 우리는 코드 최적화를 위한 LLM에 대한 첫 번째 단계를 제시합니다.우리는 보이지 않는 LLVM-IR에 대한 좋은 최적화 전략을 예측할 수 있는 모델을 구성합니다. 결과는 유망하지만, 우리는 작은 프로그램 조각에 대한 작업으로 제한하는 시퀀스 길이와 최적화 결과를 예측하는 모델의 능력을 제한하는 산술 추론에서 문제에 직면합니다.우리는 연구 커뮤니티가 간단한 최대 우도 코드 생성을 위한 LLM을 넘어 성능 인식 코드 최적화로 나아가도록 영감을 주기를 바랍니다.참고문헌 [16] [17] OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. &quot;MLGO: a Machine Learning Guided Compiler Optimizations Framework&quot;. in: arXiv:2101.04808 (2021). [18] Z. Wang and M. O&#39;Boyle. &quot;Machine Learning in Compiler Optimisation&quot;. in: arXiv:1805.03441 (2018). [19] H. Leather and C. Cummins. &quot;컴파일러에서의 머신 러닝: 과거, 현재, 미래&quot;. FDL에 게재됨. 2020. [20] Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, Y. Tian. &quot;코어셋과 정규화된 값 예측을 사용하여 컴파일러 패스 순서 학습&quot;. ICML에 게재됨. 2023. [21] C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O&#39;Boyle, H. Leather. &quot;ProGraML: 데이터 흐름 분석 및 컴파일러 최적화를 위한 그래프 기반 프로그램 표현&quot;. ICML에 게재됨. 2021. [1] R. Li, LB Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. &quot;StarCoder: 소스가 함께하길 바랍니다!&quot; [22] In: arXiv:2305.06161 (2023). [2] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, AD Lago, T. Hubert, P. Choy 등 &quot;AlphaCode를 사용한 경쟁 수준 코드 생성&quot;. 에서: 과학 378.6624 (2022). [3] 오픈AI. “GPT-4 기술 보고서”. 출처: arXiv:2303.(2023). [4] LB Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, CM Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, LK Umapathi, CJ Anderson, et al. &quot;SantaCoder: 별에 손대지 마세요!&quot; 출처: arXiv:2301.03988 (2023). [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. &quot;PaLM: 경로로 언어 모델링 확장&quot; 출처: arXiv:2204.02311 (2022). [6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer 및 M. 루이스. &quot;InCoder: 코드 채우기 및 합성을 위한 생성 모델&quot;. In: arXiv:2204.05999 (2023). [7] S. Gunasekar, Y. Zhang, J. Aneja, CCT Mendes, AD Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah 등. &quot;교과서는 당신에게 필요한 전부입니다&quot;. In: arXiv:2306.11644 (2023). [8] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri 외. &quot;코드로 훈련된 대규모 언어 모델 평가&quot;. arXiv:2107.03374(2021)에 게재. [9] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, XE Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov 외. &quot;Code Llama: Open Foundation Models for Code&quot;. arXiv:2308.12950(2023)에 게재. [10] M.-A. Lachaux, B. Roziere, L. Chanussot, G. Lample. &quot;프로그래밍 언어의 비지도 번역&quot;. arXiv:2006.03511(2020)에 게재. [11] J. Armengol-Estapé 및 MF O&#39;Boyle. &quot;C에서 x86으로의 번역 학습: 신경 컴파일 실험&quot;. arXiv:2108.07639(2021)에 게재. [12] M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut 및 G. Synnaeve. &quot;컴파일러 표현을 사용한 코드 번역&quot;. arXiv:2207.03578(2022)에 게재. [13] G. Ye, Z. Tang, SH Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang 및 Z. Wang. &quot;심층 컴파일러 퍼징을 통한 JavaScript 엔진에 대한 자동화된 적합성 테스트&quot;. PLDI에 게재. 2021. Y. Deng, CS Xia, H. Peng, C. Yang, L. Zhang. &quot;대규모 언어 모델은 제로샷 퍼저입니다. 대규모 언어 모델을 통한 퍼징 딥러닝 라이브러리&quot;. ISSTA에서. 2023. [14] [15] M. Schäfer, S. Nadi, A. Eghbali, F. Tip. &quot;대규모 언어 모델을 사용한 적응형 테스트 생성&quot;. arXiv:2302에서.(2023). C. Lattner와 V. Adve. &quot;LLVM: 평생 프로그램 분석 및 변환을 위한 컴파일 프레임워크&quot;. CGO에서. 2004. [23] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, S. Paul. &quot;언어 모델을 사용한 학습의 한계&quot;. arXiv:2306에서.(2023). [24] J. Qian, H. Wang, Z. Li, S. Li 및 X. Yan. 영어: &quot;산술 및 기호 귀납에서 언어 모델의 한계&quot;. arXiv:2208.05051(2022)에 게재됨. [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 외. &quot;Llama 2: Open Foundation 및 Fine-Tuned Chat Models&quot;. arXiv:2307.09288(2023)에 게재됨. [26] GG Fursin, MFP O&#39;Boyle, PMW Knijnenburg. &quot;반복 컴파일 평가&quot;. LCPC에 게재됨. 2005. [27] C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, H. Leather. &quot;CompilerGym: AI 연구를 위한 강력하고 성능이 뛰어난 컴파일러 최적화 환경&quot;. CGO에서. 2022. [28] D. Kocetkov, R. Li, LB Allal, J. Li, C. Mou, CM Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. &quot;스택: 허용 라이선스가 부여된 3TB 소스 코드&quot;. arXiv:2211.15533(2022)에서. [29] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. &quot;파일: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트&quot;. arXiv:2101.00027(2020)에 게재됨. [30] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt. &quot;CodeSearchNet 챌린지: 의미 코드 검색 상태 평가&quot;. arXiv:1909.09436(2019)에 게재됨. A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, P. Rosso. &quot;소스 코드의 저자 식별(AI-SOCO)에 대한 PAN@FIRE 2020 과제 개요&quot;. FIRE에 게재됨. 2020. [31] [32] J. Armengol-Estapé, J. Woodruff, A. Brauckmann, JW d. S. Magalhães, M. O&#39;Boyle. &quot;ExeBench: 실행 가능한 C 함수의 ML 규모 데이터 세트&quot;. MAPS에서. 2022. [33] L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin. &quot;프로그래밍 언어 처리를 위한 트리 구조에 대한 합성 신경망&quot;. AAAI에서. 2016. [34] X. Yang, Y. Chen, E. Eide, J. Regehr. &quot;C 컴파일러의 버그 찾기 및 이해&quot;. PLDI에서. 2011. [35] V. Livinskii, D. Babokin, J. Regehr. &quot;YARPGen을 사용한 C 및 C++ 컴파일러의 임의 테스트&quot;. OOPSLA에서. 2020. [36] J. Armengol-Estapé, J. Woodruff, C. Cummins, MF O&#39;Boyle. &quot;SLaDe: 최적화된 어셈블러를 위한 휴대용 소규모 언어 모델 디컴파일러&quot;. arXiv에서:2305.12520(2023)에서. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. &quot;주의만 있으면 됩니다&quot;. NeurIPS에서(2017).[38] P. Gage. &quot;데이터 압축을 위한 새로운 알고리즘&quot;. C Users Journal 12.2(1994)에 실림. [39] A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, I. Stoica. &quot;AutoPhase: 심층 강화 학습을 통한 랜덤 포레스트에서 HLS 위상 순서 조정&quot;. MLSys에 실림. 2020. [56] AH Ashouri, M. Elhoushi, Y. Hua, X. Wang, MA Manzoor, B. Chan, Y. Gao. &quot;MLGOPerf: 성능 최적화를 위한 ML 가이드 인라이너&quot;. arXiv:2207.08389(2022)에 실림. A. Haj-Ali, NK Ahmed, T. Willke, S. Shao, K. Asanovic, I. Stoica. &quot;NeuroVectorizer: 심층 강화 학습을 통한 종단 간 벡터화&quot;. CGO에서. 2020. [57] [40] I. Loshchilov 및 F. Hutter. &quot;분리된 가중치 감소 규칙&quot;. [58] C. Cummins, P. Petoumenos, Z. Wang 및 H. Leather. &quot;Endlarization&quot;. arXiv:1711.05101 (2017)에서. [41] K. Papineni, S. Roukos, T. Ward 및 W.-J. Zhu. &quot;BLEU: A
--- EXPERIMENT ---
코드 추론을 위한 LLM의 잠재력과 한계를 더욱 특성화하기 위한 s 및 코드 예제. 전반적으로 우리는 그들의 효능이 놀랍다고 생각하며 이러한 결과가 커뮤니티에 흥미로울 것이라고 생각합니다. 학습 단계 프롬프트 다음 LLVM-IR에서 명령어 수를 줄이기 위해 실행할 패스를 알려주세요: <code>define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { %3 alloca 132, align%alloca 132, alignstore 132 %0, 132* %3, align 4. store 132 %1, 132* %4, align%%load 132, 132* %3, alignicmp eq 132 %5,br 11 %6, label %10, label %7: %8 load 132, 132* %3, align%9 icmp eq 132 %8,br 11 %9, label %10, label %Answer Run passes -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading to reduce instruction count from 15 to 4: <code>define dso local void @&quot;iowrite_single&quot; (132 %0, 132 %1) switch 132 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ._crit_edge1: %call 132 @&quot;outb&quot; (132 %0, 132 %1) br label &quot;._crit_edge2&quot; ._crit_edge2: ret void } 10: %load 132, 132* %3, align%12 load 132, 132* %4, align%13 call 132 @&quot;outb&quot; (i32 %11, 132 %12) br label %14: ret void } declare dso local i32 @&quot;outb&quot; (132, 132)</code></code> 추론 단계 답변 실행 패스 -reg2mem -sroa -sink-simplifycfg -early-cse -jump-threading 컴파일러 최적화 코드 define dso local void @&quot;iowrite_single&quot; (i32 %0, 132 %1) { switch i32 %0, label %&quot;._crit_edge2&quot; [ 132 0, label %&quot;._crit_edge1&quot; 132 1, label %&quot;._crit_edge1&quot; ] ... ret void } 그림 1: 학습 및 추론 중 모델 입력(프롬프트)과 출력(답변)을 보여주는 접근 방식 개요. 프롬프트에는 최적화되지 않은 코드가 포함되어 있습니다. 답변에는 최적화 패스 목록, 명령어 수 및 최적화된 코드가 포함되어 있습니다. 추론 중에는 컴파일러에 제공하는 최적화 패스 목록만 생성하여 최적화된 코드가 올바른지 확인합니다. 표 I: 훈련 데이터. 각 LLVM-IR 기능은 자동 조정되어 (Prompt, Answer) 쌍을 만드는 데 사용됩니다. n 토큰 열은 Llama 2[25] 토크나이저를 사용하여 프롬프트가 인코딩될 때 토큰 수를 보여줍니다. 표 II: 테스트 데이터. n개 함수 n개 함수 손으로 쓴 합성 총 610,389,1,000, 최적화되지 않은 명령어 수 8,417,13,775,16,411, 최적화되지 않은 명령어 수 -Oz 디스크의 명령어 수 크기 n개 토큰 653.5MB 352.3MB 1.0GB 214,746,158,435,373,181, AI-SOCO [31] ExeBench [32] POJ-104 [33] Transcoder [12] 8,26,97,386,47,181,8,4,17,289,129, CSmith [34] YARPGen [35] 33,647,138,12,285,144, 총 100,1,716,645,II. LLMS를 사용한 패스 순서 지정 이 작업에서는 컴파일러 패스 순서를 목표로 합니다. 패스 순서 지정 작업은 컴파일러에서 사용 가능한 최적화 변환 패스 세트에서 특정 입력 코드에 대해 최상의 결과를 생성하는 패스 목록을 선택하는 것입니다. 패스 순서를 조작하면 런타임 성능과 코드 크기에 상당한 영향을 미치는 것으로 나타났습니다[19, 26]. 이 작업에 대한 머신 러닝 접근 방식은 이전에 좋은 결과를 보였지만 다양한 프로그램 간에 일반화하는 데 어려움을 겪었습니다[27]. 이전 작업에서는 일반적으로 다양한 구성을 시도하고 최상의 성능 옵션을 찾기 위해 수십 또는 수백 번 새 프로그램을 컴파일해야 하므로 실제 사용에는 비실용적이었습니다. 충분한 추론 능력을 갖춘 대규모 언어 모델이라면 이것이 필요 없이도 좋은 최적화 결정을 내리는 법을 배울 수 있을 것이라고 가정했습니다. 코드에 대한 LLM에 대한 대부분의 이전 작업은 Python과 같은 소스 언어에서 작동합니다. 대신 패스 순서 지정 문제의 경우 중간 표현(IR)이라고 하는 컴파일러 어셈블리의 하위 수준에서 추론이 필요합니다. LLM 사전 학습을 위한 소스 언어의 큐레이트된 데이터 세트가 있지만(예: [28–30]), 컴파일러 IRS는 이러한 데이터 세트의 상당 부분을 차지하지 않으며 ChatGPT와 같은 모델은 이해에 약간의 가능성을 보여주지만 IR에 대한 추론 능력은 소스 언어에 비해 훨씬 떨어집니다. 이전 연구[17, 27]와 마찬가지로 IR 명령어 수를 바이너리 크기에 대한(완벽하지 않은) 프록시로 사용하여 코드 크기에 대한 LLVM 패스 순서를 최적화하는 것을 목표로 합니다. 이 접근 방식은 선택한 컴파일러 및 최적화 메트릭과 무관하며 앞으로 런타임 성능을 목표로 할 것입니다. 지금으로서는 코드 크기를 최적화하면 학습 데이터 수집이 간소화됩니다. A. 프롬프트 모델에 최적화되지 않은 LLVM-IR(예: clang 프런트엔드에서 내보내는 것)을 제시하고 이에 적용해야 하는 최적화 패스 목록을 생성하도록 요청합니다. 그림은 입력 프롬프트와 출력 텍스트의 형식을 보여줍니다. 이 작업에서 우리는 LLVM 10을 목표로 하고 opt의 최적화 플래그를 사용합니다. 선택할 수 있는 최적화 패스는 122개이며 패스는 단일 시퀀스에서 두 번 이상 선택할 수 있습니다. 또한 패스 목록당 한 번만 발생할 수 있는 6개의 메타 플래그(-00, -01, -02, -03, -Oz 및 -Os)도 포함합니다. 패스 목록은 길이에 제한이 없지만 실험에서 일반적으로 약 1018의 조합 검색 공간에 대해 최대 9개의 패스 길이를 발견했습니다. 그림 1에서 볼 수 있듯이 두 가지 보조 작업도 포함합니다. i) 최적화가 적용되기 전과 후에 코드의 명령어 수를 생성하고 ii) 최적화가 적용된 후에 출력 IR을 생성합니다. 이를 통해 코드 최적화의 메커니즘에 대한 심층적인 이해를 강제하여 더 나은 패스 순서 지정 결정을 내릴 수 있을 것이라고 가정합니다. 섹션 VB에서 실험적으로 이를 검증합니다. 모델은 명령어 수와 최적화된 IR을 생성하도록 학습되었지만 배포에는 이러한 보조 작업이 필요하지 않습니다. 컴파일러를 사용하여 실행하는 패스 목록만 생성하면 됩니다. 따라서 우리는 모델의 출력이 신뢰할 수 있어야 하는 기술을 괴롭히는 정확성의 문제를 회피합니다[10–12, 36].B. LLVM-IR 정규화 우리는 다음 규칙을 사용하여 LLM을 훈련하는 데 사용되는 LLVM-IR을 정규화합니다.우리는 주석을 버리고, 메타데이터와 속성을 디버깅하고, 줄바꿈은 유지하지만 다른 공백을 표준화하고 들여쓰기를 제거하는 사용자 정의 렉서를 통해 IR을 공급하여 일관된 공백을 보장합니다.우리는 LLM의 제한된 입력 크기(섹션 III-A)를 최대한 활용하기 위해 LLVM-IR의 길이를 줄이기 위해 이를 수행합니다.그림 1의 코드는 이러한 방식으로 처리되었습니다.III.모델 우리는 유비쿼터스 변압기 아키텍처[37]를 사용합니다.변압기는 고정된 크기의 컨텍스트 창에서 셀프 어텐션을 사용하는 인공 신경망입니다.입력 텍스트는 먼저 단어와 하위 단어 단위로 토큰화됩니다. 이것들은 연속 벡터 표현에 내장되어 변환기의 인코더에 입력으로 제공되며, 여기서 셀프 어텐션 메커니즘은 토큰 간의 문맥적 관계를 포착하여 모델이 입력 텍스트의 의미 구조를 이해하고 처리하도록 장려합니다. 출력 텍스트는 한 번에 하나의 토큰을 반복적으로 생성하여 생성됩니다. 디코더는 이전에 생성된 토큰과 함께 인코딩된 입력을 가져와 셀프 어텐션을 사용하여 시퀀스의 다음 토큰을 예측합니다. 디코딩하는 동안 탐욕적으로 샘플링하여 가장 가능성 있는 토큰 시퀀스를 선택합니다. 이 프로세스는 시퀀스 끝 토큰이 생성되거나 사전 정의된 최대 길이에 도달할 때까지 계속됩니다. A. 모델 아키텍처 Llama 2 [25]와 동일한 모델 아키텍처와 바이트 쌍 인코딩(BPE) [38] 토크나이저를 사용하지만 모델을 처음부터 학습합니다. Llama 2 구성 중 가장 작은 것인 어텐션 헤드, 4,096개의 숨겨진 차원, 32개의 레이어를 사용하여 총 7B개의 매개변수를 사용합니다. (프롬프트, 답변) 쌍의 최대 길이는 시퀀스 길이에 의해 정의됩니다. 이 작업에서 우리는 시퀀스 길이 개선 -Oz 6% 4% 2% 자동 튜너 우리의 접근 방식 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le50% 40% w 30% 20% MAPE 10% (a) 생성된 패스 목록의 성능.최적화되지 않은(입력) 코드 최적화된(출력) 코드 0% 0.0.0.0.0.1.1.#. 훈련 토큰 le1.0.0.0.40.2(b) 명령어 수 예측 정확도.BLEU 코드는 정확한 일치를 컴파일합니다.0.0.0.0.0.1.1.#. 훈련 토큰 le(c) 모델 최적화된 코드 메트릭.그림 2: 훈련 중 홀드아웃 검증 세트에 대한 성능.우리는 250개 훈련 단계(1억 3,100만 훈련 토큰)마다 성능을 평가합니다. -Oz와의 동등성은 393M 토큰에서 달성되고 10.9B 토큰에서 최고 성능을 달성합니다. 2,048개 토큰. Llama 2 토크나이저는 LLVM-IR을 인코딩할 때 토큰당 평균 2.02자를 달성하므로 이는 2KB에서 학습할 수 있는 가장 긴 LLVM-IR에 대한 대략적인 상한을 제공합니다(2KB 프롬프트와 2KB 답변은 약 2,048개 토큰이기 때문). B. 학습 데이터 우리는 표 I에 요약된 최적화되지 않은 LLVM-IR 함수의 대규모 코퍼스를 조립했습니다. 공개적으로 사용 가능한 수기 C/C++ 코드의 데이터 세트에서 함수를 추출하고 C/C++ 컴파일러 테스트 생성기에서 생성한 합성 코드로 이를 보완했습니다. 전체적으로 학습 코퍼스는 1,000,000개의 중복 제거된 IR 함수로 구성되어 총 373M 학습 토큰입니다. 우리는 2,048토큰 시퀀스 길이에 맞출 수 있는 데이터 양을 극대화하기 위해 전체 모듈이 아닌 개별 IR 기능 수준에서 작업합니다. 가장 작은 명령어 수를 생성하는 최적화 패스 목록을 찾기 위해 자동 튜닝을 사용합니다. 우리의 자동 튜너는 Liang et. al. [20]의 작업에서 영감을 받아 랜덤 검색과 함수 간의 전체 대 전체 결과 브로드캐스팅을 결합합니다.표 III: 표 II의 보이지 않는 LLVM-IR 함수 테스트 세트에서 패스 순서를 지정하는 다양한 접근 방식의 성능. 모든 메트릭은 -Oz에 대한 것입니다. 절감된 명령어는 개선된 함수에 대해 합산되고 회귀된 명령어는 회귀된 함수에 대해 합산됩니다. 전반적인 개선은 -Oz에 대한 총 명령어 수 절감 합계입니다. 자동 튜너는 최상의 성능을 달성하지만 25억 개의 추가 컴파일(949 CPU-일)이 필요합니다. 우리의 접근 방식은 컴파일러를 한 번도 호출하지 않고도 자동 튜너의 이득의 60%를 달성합니다. 추가 기능 컴파일 개선된 기능 회귀된 명령어 저장된 명령어 회귀된 전체 개선 Autotuner AutoPhase [39] Coreset-NVP [20] 우리의 접근 방식 2,522,253,6,30,5.03% 4,500,1,8,6,32,-3.85% 442,3,6,16,28,-1.88%4,21,3,3.01% 표 IV: &quot;-Oz 백업&quot;을 사용하여 표 III의 모델 확장. 모델이 -Oz 이외의 통과 목록을 예측하는 경우 -Oz도 평가하고 최상의 것을 선택합니다. 이렇게 하면 추가 컴파일을 희생하여 -Oz에 대한 회귀가 방지됩니다. 추가 컴파일 AutoPhase [39] Coreset-NVP [20] 4,600,542, 우리의 접근 방식 5, 전체 개선 1.02% 2.55% 3.52% 각 기능에 대해 고정된 시간(780초) 동안 무작위 검색을 실행한 다음, 명령어 수에 기여하는지 확인하기 위해 무작위로 선택한 개별 패스를 반복적으로 제거하여 최상의 패스 목록을 최소화합니다. 기여하지 않으면 삭제합니다. 각 함수에서 이를 수행한 후 고유한 최상의 패스 목록 세트를 집계하여 다른 모든 함수에 브로드캐스트합니다. 따라서 패스 목록이 한 함수에서 잘 작동하는 것으로 확인되면 다른 모든 함수에서 시도합니다. 전체적으로 자동 튜너는 각 학습 프로그램을 평균 37,424번 컴파일하여 -Oz에서 제공하는 컴파일러의 기준 고정 패스 순서에 비해 명령어 수 감소에서 5.8%의 개선을 달성했습니다. 우리의 목적상 이 자동 튜닝은 각 함수의 최적화를 위한 황금 표준 역할을 합니다. 자동 튜너가 발견한 명령어 수 절감은 상당하지만 이러한 성과를 달성하기 위한 계산 비용은 9,016 CPU 일이었습니다. 이 작업의 목표는 컴파일러를 수천 번 실행할 필요가 없는 예측 모델을 사용하여 자동 튜너 성능의 일부를 달성하는 것입니다. C. 학습 무작위로 초기화된 가중치에서 시작하여 64개의 V100에서 30,000단계로 모델을 학습하여 총 학습 시간이 620GPU 일이었습니다. ₁ 및 B2 값이 각각 0.9 및 0.95인 AdamW 옵티마이저[40]를 사용합니다. 1,000개의 워밍업 단계, 1e-5의 최대 학습 속도, 최대 학습 속도의 1/10인 최종 학습 속도를 갖는 코사인 학습 속도 일정을 사용합니다. 256의 배치 크기를 사용했고 각 배치에는 총 15.7B개의 학습 토큰에 대해 524,288개의 토큰이 포함됩니다. 전체 30,000단계의 학습은 7.7에포크(학습 코퍼스에 대한 반복)입니다. 학습하는 동안 학습 세트와 동일한 방식으로 처리된 1,000개의 보이지 않는 IRS의 홀드아웃 검증 세트에서 모델을 평가했습니다. 모든 단계를 평가합니다. IV. 평가 이 섹션에서는 보이지 않는 코드에 대한 패스 목록을 생성하고 최적화를 올바르게 수행하는 모델의 능력을 평가합니다.A. 학습 결과 그림 2는 보이지 않는 LLVM-IR 함수 1,000개의 홀드아웃 검증 세트에서 평가할 때 학습 중 성능을 보여줍니다.최고 검증 성능은 모델이 109억 학습 토큰에서 달성했습니다.최고 성능에서 모델 생성 패스 시퀀스를 사용하여 최적화된 코드는 컴파일러의 기본 제공 패스 순서(-Oz)를 사용하여 최적화했을 때보다 4.4% 적은 명령어를 포함합니다.자동 튜너는 5.6%의 더 큰 명령어 수 감소를 달성하지만 이를 위해 검증 세트를 2,700만 번 컴파일해야 했습니다.모델은 컴파일러를 한 번도 호출하지 않고 예측을 수행합니다.그림 2b는 예측된 입력 및 출력 명령어 수의 오류를 보여줍니다.최적화되지 않은 코드에 대한 명령어 수 예측은 거의 완벽한 정확도에 빠르게 접근합니다.출력 명령어 수 예측은 더 어려워서 평균 평균 백분율 오류(MAPE)가 5.9%에 도달합니다. 그림 2c는 세 가지 지표를 사용하여 생성된 코드의 품질을 평가합니다.BLEU[41] 점수는 모델 생성 코드와 생성된 패스 목록을 사용하여 컴파일러에서 생성한 참조 기준 진실 코드 간의 유사성을 보여줍니다.코드 컴파일은 모델 생성 코드가 오류 없이 컴파일되는 빈도입니다.정확한 일치는 생성된 패스 목록을 사용하여 최적화할 때 모델 생성 코드가 컴파일러 생성 코드와 문자별로 일치하는 빈도(즉, BLEU=1인 횟수)를 추적합니다.최대 성능에서 모델은 오류 없이 컴파일되는 코드를 생성하는 인상적인 90.5% 비율을 달성합니다.또한 BLEU 점수 0.952는 모델 최적화된 코드가 컴파일러의 코드와 매우 유사하고 정확한 일치 빈도가 70%임을 보여줍니다.비교를 위해 최적화되지 않은 코드를 출력에 복사하는 기준선은 BLEU 점수 0.531과 정확한 일치 빈도 0%를 달성하여 이처럼 높은 점수를 달성하려면 입력 코드를 상당히 조작해야 함을 보여줍니다. 훈련이 끝날 무렵 검증 세트의 성능은 정점에 도달했습니다. 가장 성능이 좋은 체크포인트를 사용하고 나머지 평가에서는 100배 더 큰 규모의 평가로 전환합니다. B. 최신 기술과의 비교 이 실험에서는 기준선과 비교하여 LLM의 통과 목록을 예측하는 능력에 대한 대규모 평가를 수행합니다. 빈도(로그) nالسيسسيد السيسي السبيسZOUAб -instcombine -mem2reg -reg2mem -simplifycfg -memcpyopt Η το POJS -02-Os Hoop-rotate -03-newgvn -loop-deletion. -jump-threading. tailcallelim -로드-스토어-벡터라이저 early-csegvn-hoist – -div-remp -early-cse-memssa -추측-실행 -sip-벡터라이저 싱크 재연결 -00-dse-상관-전파 병합 반환 -break-crit-edges -globalopt -nary-재연결 -instsimplify -loop-simplifycfg -simple-loop-unswitch -loop-unswitch -loop-unroll -aggressive-instcombine -die -bdce -scalarizer -dce → adce Hoop-reroll -flattencfg Hoop-vectorize-constprop -functionattrs woipi-doojAutotuner 우리의 접근 방식 -irce → predicat -mldst-motion Hoop-predication -attributor Hoop-instsimplify hotcoldsplit -Hoop-load-elim -coro-elide -elim-avail-extern -loop-reduce -Hoop-interchange -coro-cleanup Hower-constant-intrinsics -loop-versioning -loop-versioning-licm -partial-inliner -pgo-memop-opt + strip + 1 2 3 4 5 6 7 8 9패스 목록 길이 그림 3: 100,000개 테스트 프로그램 각각에 대한 패스 목록에서 패스가 발생하는 빈도(왼쪽)와 패스 목록의 길이(오른쪽). -Oz는 자동 튜너의 시작점이며 지배적인 결과이며 자동 튜닝된 테스트 프로그램의 93.2%에서 가장 잘 발견된 결과이고 더 긴 시퀀스의 일부로 패스 목록의 추가 0.6%에 나타납니다. 모델에서 생성된 패스 분포는 자동 튜너를 추적하지만 -Oz(94.3%)를 약간 과대 예측하고 자동 튜너가 훈련 세트에는 사용했지만 테스트 세트에는 사용하지 않은 9개의 패스를 포함합니다. 결과는 자동 튜너 주파수가 감소하는 순서로 정렬됩니다. define 132 @f1 (i8 %0) { 2 alloca 132, align3 alloca 18, alignstore 18 %0, 18* %3, align4 load 18, 18* %3, align%5= zext 18 %4 to% 6 = icmp sge 132 %5,br il %6, label %7, label7: 8 load 18, 18* %3, align% 9 = zext 18 %8 to10 icmp sle 132 %9,br il 10, label %11, label %11: 12 load 18, 18* %3, align13 zext 18 %12 to<snip 21 lines...> 33: 34 132 로드, 132* %2, alignret 132 %define 132 @f1 (i8 %0) { %2 = zext 18 %0 to%.off 18 %0,3 추가 icmp ult i8 %.off,br il 3, 레이블 %4, 레이블 %4: %add nsw 132 %2,br 레이블 %6: %.reload16.off = nsw 132 %2,7 추가 icmp ult 132 %.reload16.off,br il 7, 레이블 %10, 레이블 %8: 9 icmp eq 18 %0,%. = select il %9, 132 26, 132br label %10: %.0.reg2mem.0 = phi i32 [%5, %4], [., 8], [%.reload16.off, %6] ret 132.0.reg2mem.define 132 @f1 (i8 %0) { %2 =zext 18 %0에서 %.off로 18 %0,%3 icmp ult 18 %.off,br il %3, label %6, label %._ crit_edge ._crit_edge: %.off24 = 18 %0,%4 icmp ult i8 %.off24,br il 4, label %6, label %.를 추가합니다. crit_edge._crit_edge9: 5 icmp eq 18 %0,spec.select = select il %5, 132 26, 132ret6: } spec.select %.sink = phi 132 [191, %1], [159,._crit_edge] 7 add nsw i32 %.sink, %ret 132 %} (a) 입력 코드(39개 명령어). (b) 패스를 사용한 자동 튜닝 코드(14개 명령어): -reg2mem -inst combine -Os -01. (c) 모델 최적화 코드(13개 명령어) 및 패스 목록: -reg2mem -simplifycfg -mem2reg -jump-threading -Os. 목록 1: 이 코드를 이전에 본 적이 없음에도 불구하고 모델이 자동 튜너보다 더 나은 패스 목록을 제안하는 예제 IR 함수. 이 함수의 경우 자동 튜너는 26,000개의 다른 패스 순서를 시도했습니다. 모델에서 생성한 패스 목록은 1,000,000개의 예제로 구성된 학습 세트에서 5번 나타납니다.데이터 세트 평가를 위해 광범위한 벤치마크 데이터 세트를 집계하여 표 II에 요약했습니다. 학습한 것과 동일한 IR 함수를 중복 제거하고 제외합니다.테스트 데이터는 코딩 대회(AI-SOCO[31], POJ-104[33]), 컴파일러 테스트 케이스 생성기(CSmith[34], YARPGen[35]) 및 기타 공개적으로 사용 가능한 코드(ExeBench[32], Transcoder[12])를 포함한 다양한 도메인의 코드로 구성되어 있습니다.기준선 접근 방식을 AutoPhase[39], Coreset-NVP[20] 및 Autotuner의 세 가지 기준선과 비교합니다.AutoPhase[39]는 고정 길이 에피소드에서 누적 명령어 수 절감을 극대화하는 최적화 패스 시퀀스를 선택하기 위해 Proximal Policy Optimization[42]을 사용하여 에이전트를 학습시키는 강화 학습 접근 방식입니다. 각 단계에서 최적화되는 프로그램은 명령어 수와 기타 속성의 56차원 벡터로 에이전트에 표현됩니다.우리는 [39]의 환경을 복제하지만 에이전트가 100,000개의 에피소드로 훈련되는 [27]의 구현 및 확장된 훈련 체제를 사용합니다.우리는 언어 모델(표 I)과 동일한 데이터에서 에이전트를 훈련하고 보류 검증 세트에서 훈련하는 동안 주기적으로 에이전트 성능을 평가합니다.이전 작업에서와 마찬가지로 45의 동작 공간과 에피소드 길이를 사용합니다.Coreset-NVP[20]는 반복적 검색과 학습된 비용 모델을 결합하는 기술입니다.먼저 17,500개의 벤치마크에서 탐욕적 검색을 실행하여 최상의 패스 목록의 핵심 세트를 결정합니다.그런 다음 그래프 합성곱 네트워크에서 처리한 ProGraML[21] 그래프를 프로그램 표현으로 사용하여 이 검색 결과에 대해 신경 값 예측(NVP)을 훈련합니다. 추론에서 Coreset-NVP는 정규화된 보상을 예측하고 가장 높은 정규화된 보상을 갖는 처음 몇 개의 패스 시퀀스를 시도합니다.-5% -10%0% 30% 15% AutoPhase AutoPhase Coreset-NVP 10% 20% Autotuner Coreset-NVP Autotuner 우리의 접근 방식 우리의 접근 방식 5% 10% ExeBench Transcoder CSmith YARPGen0% -10% TUnoptimized 명령어 수 그림 5: 입력 크기에 따른 -Oz 대비 개선.더 큰 코드는 더 많이 최적화합니다.그림 4: 데이터 세트에 따른 -Oz 대비 개선.수작업으로 작성한 코드는 더 많이 최적화합니다.보상.각 벤치마크에 대해 시도할 수 있는 총 패스 수는 이전 연구에 따라 45입니다.저자가 제공한 모델 가중치를 사용하여 테스트 세트에서 추론을 수행합니다.마지막으로, 이를 학습 데이터를 생성하는 데 사용한 Autotuner와 비교합니다.섹션 III-B에서 설명한 대로 학습 데이터와 동일한 방식으로 테스트 데이터 세트를 자동 튜닝했습니다. 결과 표 III은 결과를 요약한 것입니다. 우리의 접근 방식은 모든 데이터 세트에서 -Oz, AutoPhase, Coreset-NVP보다 성능이 뛰어납니다. 전반적으로 자동 튜너에 제공된 수천 번의 최적화 시도를 통해 가장 성능이 좋은 패스 목록을 발견할 수 있습니다. AutoPhase와 Coreset-NVP는 모두 -Oz보다 성능이 뛰어나지만 회귀가 많아 명령어 수에 전반적으로 부정적인 영향을 미치는 패스 목록을 식별할 수 있습니다. 이를 극복하기 위해 간단한 &quot;-Oz 백업&quot; 확장을 제안합니다. 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 경우 -Oz도 실행하고 두 옵션 중 더 나은 것을 선택합니다. 이렇게 하면 -Oz에 대한 회귀가 방지되지만 모델이 -Oz가 아닌 다른 패스 목록을 예측하는 횟수만큼 추가 컴파일 횟수가 증가합니다. 표 IV는 이러한 방식으로 평가할 때의 기술 결과를 보여줍니다. 이것이 모델이 더 개선되는 데 도움이 되지는 않지만, 회귀가 없기 때문에 AutoPhase와 Coreset-NVP는 이제 -Oz보다 전반적으로 개선되었지만 -Oz 백업이 있거나 없는 LLM보다는 여전히 낮습니다.C. 생성된 패스 목록의 평가 그림 3은 자동 튜너와 이전 실험의 모델이 패스를 선택하는 빈도를 보여줍니다.모델이 선택한 패스의 분포는 자동 튜너를 광범위하게 추적합니다.-Oz가 가장 자주 최적의 패스입니다.-Oz를 제외하고 모델에서 생성된 패스 목록의 평균 길이는 3.4(최대 10)이고 자동 튜너 패스 목록의 평균 길이는 3.1(최대 9)입니다.모델에서 생성된 패스 목록 중 105개는 학습 데이터에 전혀 나타나지 않습니다.710개의 경우에서 모델에서 생성된 패스 목록은 테스트 세트에서 자동 튜너보다 성능이 뛰어나지만 일반적으로 개선 정도는 작습니다. 목록 1은 모델에서 생성된 표 V: 100,000개의 보이지 않는 입력에 대한 모델 최적화 코드의 컴파일러 오류의 예를 보여줍니다. 오류 범주 n 유형 오류 5, 명령어 전방 참조 1, 정의되지 않은 값 1, 잘못된 재정의 구문 오류 상수에 대한 잘못된 값 정의되지 않은 함수 인덱스 오류 기타 9, 전체 패스 목록은 제어 흐름을 더 적은 블록으로 단순화하여 하나의 추가 명령어를 저장합니다. 그림 4는 벤치마크 데이터 집합에 따른 패스 순서 지정에 대한 각 접근 방식의 개선 사항을 분석합니다. -Oz에 비해 가장 큰 개선 사항은 POJ-104 및 Transcoder 데이터 집합에서 발견되는데, 둘 다 대량의 수기 코드를 집계하는 반면 컴파일러를 테스트하기 위한 무작위 프로그램 생성기인 YARPGen은 -Oz에 비해 개선할 수 있는 기회가 가장 적습니다. 우리는 입력 프로그램 크기와 자동 튜너와 모델 모두에서 발견되는 -Oz에 비해 잠재적인 성능 개선 사이에 강력한 상관 관계가 있음을 발견했습니다. 그림 5는 이러한 추세를 나타내며, 더 큰 프로그램이 -Oz에 비해 개선할 수 있는 기회가 더 많다는 것을 명확하게 보여줍니다. D. 생성된 코드 평가 이 섹션에서는 모델에서 생성된 코드의 품질을 평가합니다. 이를 위해 테스트 세트의 모든 100k 함수에 대해 최적화된 코드를 생성하는 보조 학습 작업을 실행했습니다. 이는 이전 섹션에서 평가된 패스 목록을 생성하는 데 필요하지 않습니다. 이 섹션의 코드 샘플에서는 불필요한 명령문을 생략하고 식별자 이름을 줄이는 등 간결성을 위해 사소한 편집을 했습니다. 90.3%의 경우 모델에서 생성된 최적화된 IR이 컴파일되고 68.4%의 경우 출력 IR이 컴파일러에서 생성한 기준 진실과 일치합니다. 생성된 IR이 컴파일되지 않는 9.7%의 경우에 대한 다양한 오류 클래스를 표 V에 분류하고 목록 2에 코드 예제가 나와 있습니다.오류: &#39;15&#39;가 &#39;i32&#39; 유형으로 정의되었지만 &#39;il&#39;을 예상함 %or.cond = or il %14, %(a) 모델은 %15를 정수로 정의했지만 나중에 부울(유형 오류)로 사용하려고 했습니다. 오류: 상수 표현식 유형 불일치 e.str private unnamed_addr 상수 [493 x 18] 정의 132 @f1( 132 %0, 132 %) 정렬 2 { br 레이블 %int f1 (int x, int y) { int i = 2; while (ii &lt; y) { i += 1; return 2; } 3: } % i = phi 132[%7, %6], [2, %2] %4 = mul nsw i32 %i, i %c&quot; <snip 492 chars ', align(b) The model omitted a single character when transcribing a 493-character string-literal from the input code (type error). error: floating point constant invalid for type %1 tail call 132 @f1 (float -0.47799998483256463, float -1.8159999847412109) = icmp sgt 132 %4, %br il 5, label %8, label %6:add 132 %i,br label %8: ret 132} (b) Equivalent (hand-written) C code. define 132 @f1( 132 %0, 132 %) align 2 { ret 132} (c) LLVM requires exact decimal values for floating-point constants. These model-generated values have repeating decimals in binary so are rejected (invalid value for constant). Listing 2: Compiler errors in model-optimized code. (a) Desired optimized code. (c) Model-optimized code. Listing 5: An example of an unsafe optimization by the model. The 33instruction input program (not shown) contains a loop that is not always safe to optimize away. For example, when y = INT_MAX the loop never terminates. 1.0.define hidden signext 18 @f1() %230 { %1 alloca 164, align0.store 164 3718042838174166437, 164* %1, align%2 load 164, 164* %1, align0.%trunc i64 %2 to} ret 18 %0.BLEU Code compiles Optimized instcount error I (a) Input unoptimized code. define hidden signext@f1() %30 { ret 18define hidden signext@f1() #230 { ret 18} } (c) Model-generated code. (b) Desired optimized code. Listing 3: An example where the model generates compilable code but fails to compute the correct answer for a numeric expression. Producing the correct result for this expression requires non-trivial mathematical reasoning. 0.Equal to -Oz Better than -Oz Worse than -Oz Figure 6: Model-optimized code quality as a function of the performance of the generated pass list. Code quality is lower when the pass list performs worse than -Oz. The model-optimized code resembles the ground truth less (lower BLEU score), the code is less likely to compile, and the model struggles to estimate the instruction count (higher error). Error bars show 95% confidence intervals. Run passes -inst combine -simplifycfg to reduce instruction count from 14 to 7: define dso_local 132 @f1 (132 %0) { } % 2 = load 164, 164* getelementptr inbounds ( struct.t2, %struct.t2* @gvar, 164 0, 132 0), align3 icmp eq 164 %2, O %4= icmp eq 132 %0,%or.cond or il %3, %5 load 132, 132* @S64_MAX, align%6 = select il %or.cond, 132 %5, 132 %ret 132 %Listing 4: An example where the model generates correctly optimized code but fails to produce the pass list needed to produce the desired code. The model-optimized code and instruction count predictions match the performance of the autotuner, but the model omitted the -mem2reg pass needed to achieve this code. The model-generated pass list yields 10 instructions instead of 7. Most challenging to evaluate are the 21.9% of cases where the model-optimized code compiles but is not a character-bycharacter match with the compiler. There are two challenges: the first is that text precision metrics such as BLEU score are sensitive to differences in the code such as variable names and commutative operand order that do not affect the behavior of the code. Tools like LLVM-Canon [43] can help here but come with their own set of drawbacks. However, in many cases, it is unclear whether the behavior of two IRS is the same, so the second challenge we face is in evaluating semantic equivalency. Since not all of the datasets we use for testing provide driver scripts and input datasets for their code, we cannot use execution-based equivalence checks such as differential testing [44]. Listing 3 shows an example of model-generated code that has incorrect program semantics. Here, the lower 8 bits of a 64-bit literal are truncated and returned. The compiler performs this calculation and substitutes the correct value. The modelFrequency (log) 0.0.0.-name-anon-globals -Hoop-versioning-licm -strip -strip-dead-prototypes. -indvars -loop-reroll -strip-nondebug. -loop-unroll Hoop-unswitch. -instcombine. -loop-instsimplify-Hoop-idiom -Hoop-deletion -Hoop-rotate Hoop-unroll-and-jam-loop-guard-widening -Hoop-predication. -die -irce 1.0.75globalopt -01. -simple-loop-unswitch -loop-interchange -Os --Oz. -licm -loop-sink-loop-load-elim -loop-simplifycfg -loop-vectorize --sroa Figure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass (ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction count over -Oz. gvn-hoist . -loop-simplify -tailcallelim -speculative-execution. -early-cse -dse. -jump-threading -loop-reduce -sink -simplifycfg Improvement over -Oz 5% 4% 3% 2% 1%100% data 50% data 25% data 0% 0.0.0.0.#. Train Tokens 0.1.1.100% data, No Aux T 1.leFigure 8: Ablating the impact of training data size and the auxiliary co-training task of generating optimized code (denoted No Aux). Data size is measured as a number of training examples. The graph shows performance on a holdout validation set during training. Table VI: Ablation experiments. We evaluate the impact of varying training data size and of training the model to generate the optimized code. We train each model for 30k steps and report performance of the best model checkpoint on a holdout validation set of 1,000 unseen IR functions. overall improvement n training examples generate optimized code? 1,000,500,250,1,000,× 4.95% (-) 3.91% (-21%) 3.74% (-24%) 4.15% (-16%) recognizes that the expression can be calculated at compile time but fails to compute the correct value. This type of mathematical reasoning is a known weakness of LLMS [24]. Sometimes the model generates correctly-optimized code but fails to produce the pass list needed to achieve it. Listingshows one such example. A further class of error is when the model makes unsafe optimizations by failing to analyze the input code. Listing 5 shows an example. of We observe an interesting connection between the quality pass lists and the corresponding optimized code, shown in Figure 6. When the model produces a poor-performing pass list, the quality of the generated code is lower. V. ADDITIONAL EXPERIMENTS In the previous section, we evaluated the performance of an LLM trained to optimize LLVM-IR for code size. In this section, we build additional models to better understand the properties of LLMs for code optimization. All models use the same architecture and parameters as in Section III. A. Abalation of Dataset Size We ablate the contribution of dataset size by training two additional models and varying the amount of the training data from 50% (500k examples) down to 25% (250k examples) by random dropout. Figure 8 shows progress during the training of the models. For dataset sizes of 50% and 25%, the models begin to overfit the training set after around 8B training tokens. Table VI shows the peak performance of each configuration. With 50% and 25% of the training data, downstream performance falls by 21% and 24%, respectively. B. Abalation of Code Optimization Task We train the model to generate not just a pass list but also the optimized code resulting from this pass list. One may expect this to degrade model performance not only must it learn to predict good pass lists, but also how to produce correctly optimized code, a more difficult task. In fact, we believe this to be crucial to model performance. By forcing LLMs to learn the semantics of LLVM-IR we enable them to make better optimization decisions. To ablate this we trained a model to generate only pass lists without the corresponding optimized code. We kept the data mix and all other parameters the same. Figure 8 and Table VI show that without training the model to generate optimized code, downstream performance falls by 16%.илбмәи gvn . early-cse-memssa. -bdce-correlated-propagation Hcssa. - бәлгшәшadce -break-crit-edges. -ipsccp-dce-globaldce aggressive-instcombine. -instsimplify -sccp -reassociate -loop-fusion Regressed Improved Optimize the following LLVM-IR using -name-anon-globals: @0 private @anon. 2ef3bda806391c61822366a2a59f2569.0 = private @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0 = private unnamed_addr constant [14 x 18] c""<snip> &quot;, aligndefine dso_local 132 @f1 (18* %0) { %2 = call i32 @f2 (18* %0, 18* getelementptr inbounds ( [14 x 18], [14 x 18] * } -20, @anon. 2ef3bda806391c61822366a2a59f2569.0, @anon. 95277a486ffed0b6ba33ab3385b3d7bd.0, 164 0, 164 0)) ret 132 %(a) 불완전한 정보로 인한 실패. -name-anon-globals 패스는 모듈 이름을 사용하여 해시를 계산합니다. 이것이 없으면 모델은 무작위 해시를 환각합니다. -inst combine을 사용하여 다음 LLVM-IR을 최적화하세요. @var 12 = external dso_local 글로벌 164, align@var 13 외부 dso_local 글로벌 132, align@var_14 = 외부 dso_local 글로벌 132, aligndefine dso_local void @f1 (164 %arg) { tmp = 할당 164, 정렬 저장소 164 %arg, 164* %tmp, aligntmp1 = 로드 164, 164* %tmp, aligntmp2 = 하위 164 0, %tmptmp3 = 하위 164 0, %tmp 저장소 164 %tmp3, 164* @var_12, 정렬 저장소 164 %arg, 164*2B @var_12, 정렬 저장소 164 0, 164* @var 12, 정렬 저장소 i32 1, 132* @var_13, alignstore 132 0, 132* @var_14, alignret void -Oz를 사용하여 다음 LLVM-IR을 최적화합니다. %s1 = type { 132 } @&quot;llvm.used&quot; = 전역 추가 [1 x 18*] [18* 비트캐스트(i32(%s1*) * @fl에서 18*로)], 섹션 &quot;llvm.metadata&quot; define dso_local 132 @f1 (%sl* %0) { %alloca 132, alignalloca %sl*, align%alloca 132, alignstore %s %0, %s1** %3, align5 로드 sl*, %s1** %3, align%6 tail call 132 @f2 (%s1* %5) 스토어 i32 %6, 132* %4, 정렬 4. %7=로드 132, 132* %4, 정렬 4. %8 = icmp slt i32 %7,br il %8, 레이블 %9, 레이블 %9: %%%tail() %%%10 132, 132* %4 로드, 정렬저장 132 %10, 132* %2, 정렬br 레이블 %%%&lt;11: %%%저장 132 0, 132* %2, 정렬 4. br 레이블 %ret 12: } %13 132, 132* %2 로드, 정렬ret 132 %%(a) 모델 프롬프트. tail() %
--- CONCLUSION ---
S 우리는 코드 최적화를 위한 LLM에 대한 첫 번째 단계를 제시합니다. 우리는 보이지 않는 LLVM-IR에 대한 좋은 최적화 전략을 예측할 수 있는 모델을 구성합니다. 결과는 유망하지만, 우리는 작은 프로그램 조각에 대한 작업으로 제한하는 시퀀스 길이와 최적화 결과를 예측하는 모델의 능력을 제한하는 산술 추론에서 어려움에 직면합니다. 우리는 연구 커뮤니티가 간단한 최대 우도 코드 생성을 위한 LLM을 넘어 성능 인식 코드 최적화로 나아가도록 영감을 주고자 합니다. 참고문헌 [16] [17] OpenAI. ChatGPT. https://chat.openai.com/. M. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski, and D. Li. &quot;MLGO: a Machine Learning Guided Compiler Optimizations Framework&quot;. in: arXiv:2101.04808 (2021). [18] Z. Wang and M. O&#39;Boyle. &quot;컴파일러 최적화에서의 머신 러닝&quot;. arXiv:1805.03441(2018)에 게재됨. [19] H. Leather와 C. Cummins. &quot;컴파일러에서의 머신 러닝: 과거, 현재, 미래&quot;. FDL에 게재됨. 2020. [20] Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, Y. Tian. &quot;코어셋과 정규화된 값 예측을 사용하여 컴파일러 패스 순서 학습&quot;. ICML에 게재됨. 2023. [21] C. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O&#39;Boyle, H. Leather. &quot;ProGraML: 데이터 흐름 분석 및 컴파일러 최적화를 위한 그래프 기반 프로그램 표현&quot;. 에서: ICML. 2021. [1] R. Li, LB Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. &quot;StarCoder: 소스가 함께하길 바랍니다!&quot; [22] In: arXiv:2305.06161 (2023). [2] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, AD Lago, T. Hubert, P. Choy 등 &quot;AlphaCode를 사용한 경쟁 수준 코드 생성&quot;. 에서: 과학 378.6624 (2022). [3] 오픈AI. “GPT-4 기술 보고서”. 출처: arXiv:2303.(2023). [4] LB Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, CM Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, LK Umapathi, CJ Anderson, et al. &quot;SantaCoder: 별에 손대지 마세요!&quot; 출처: arXiv:2301.03988 (2023). [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. &quot;PaLM: 경로로 언어 모델링 확장&quot; 출처: arXiv:2204.02311 (2022). [6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer 및 M. 루이스. &quot;InCoder: 코드 채우기 및 합성을 위한 생성 모델&quot;. In: arXiv:2204.05999 (2023). [7] S. Gunasekar, Y. Zhang, J. Aneja, CCT Mendes, AD Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah 등. &quot;교과서는 당신에게 필요한 전부입니다&quot;. In: arXiv:2306.11644 (2023). [8] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri 외. &quot;코드로 훈련된 대규모 언어 모델 평가&quot;. arXiv:2107.03374(2021)에 게재. [9] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, XE Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov 외. &quot;Code Llama: Open Foundation Models for Code&quot;. arXiv:2308.12950(2023)에 게재. [10] M.-A. Lachaux, B. Roziere, L. Chanussot, G. Lample. &quot;프로그래밍 언어의 비지도 번역&quot;. arXiv:2006.03511(2020)에 게재. [11] J. Armengol-Estapé 및 MF O&#39;Boyle. &quot;C에서 x86으로의 번역 학습: 신경 컴파일 실험&quot;. arXiv:2108.07639(2021)에 게재. [12] M. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut 및 G. Synnaeve. &quot;컴파일러 표현을 사용한 코드 번역&quot;. arXiv:2207.03578(2022)에 게재. [13] G. Ye, Z. Tang, SH Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang 및 Z. Wang. &quot;심층 컴파일러 퍼징을 통한 JavaScript 엔진에 대한 자동화된 적합성 테스트&quot;. PLDI에 게재. 2021. Y. Deng, CS Xia, H. Peng, C. Yang, L. Zhang. &quot;대규모 언어 모델은 제로샷 퍼저입니다. 대규모 언어 모델을 통한 퍼징 딥러닝 라이브러리&quot;. ISSTA에서. 2023. [14] [15] M. Schäfer, S. Nadi, A. Eghbali, F. Tip. &quot;대규모 언어 모델을 사용한 적응형 테스트 생성&quot;. arXiv:2302에서.(2023). C. Lattner와 V. Adve. &quot;LLVM: 평생 프로그램 분석 및 변환을 위한 컴파일 프레임워크&quot;. CGO에서. 2004. [23] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter, S. Paul. &quot;언어 모델을 사용한 학습의 한계&quot;. arXiv:2306에서.(2023). [24] J. Qian, H. Wang, Z. Li, S. Li 및 X. Yan. 영어: &quot;산술 및 기호 귀납에서 언어 모델의 한계&quot;. arXiv:2208.05051(2022)에 게재됨. [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 외. &quot;Llama 2: Open Foundation 및 Fine-Tuned Chat Models&quot;. arXiv:2307.09288(2023)에 게재됨. [26] GG Fursin, MFP O&#39;Boyle, PMW Knijnenburg. &quot;반복 컴파일 평가&quot;. LCPC에 게재됨. 2005. [27] C. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez, S. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, H. Leather. &quot;CompilerGym: AI 연구를 위한 강력하고 성능이 뛰어난 컴파일러 최적화 환경&quot;. CGO에서. 2022. [28] D. Kocetkov, R. Li, LB Allal, J. Li, C. Mou, CM Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. &quot;스택: 허용 라이선스가 부여된 3TB 소스 코드&quot;. arXiv:2211.15533(2022)에서. [29] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. &quot;파일: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트&quot;. arXiv:2101.00027(2020)에 게재됨. [30] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, M. Brockschmidt. &quot;CodeSearchNet 챌린지: 의미 코드 검색 상태 평가&quot;. arXiv:1909.09436(2019)에 게재됨. A. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh, E. Benkhelifa, P. Rosso. &quot;소스 코드의 저자 식별(AI-SOCO)에 대한 PAN@FIRE 2020 과제 개요&quot;. FIRE에 게재됨. 2020. [31] [32] J. Armengol-Estapé, J. Woodruff, A. Brauckmann, JW d. S. Magalhães, M. O&#39;Boyle. &quot;ExeBench: 실행 가능한 C 함수의 ML 규모 데이터 세트&quot;. MAPS에서. 2022. [33] L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin. &quot;프로그래밍 언어 처리를 위한 트리 구조에 대한 합성 신경망&quot;. AAAI에서. 2016. [34] X. Yang, Y. Chen, E. Eide, J. Regehr. &quot;C 컴파일러의 버그 찾기 및 이해&quot;. PLDI에서. 2011. [35] V. Livinskii, D. Babokin, J. Regehr. &quot;YARPGen을 사용한 C 및 C++ 컴파일러의 임의 테스트&quot;. OOPSLA에서. 2020. [36] J. Armengol-Estapé, J. Woodruff, C. Cummins, MF O&#39;Boyle. &quot;SLaDe: 최적화된 어셈블러를 위한 휴대용 소규모 언어 모델 디컴파일러&quot;. arXiv에서:2305.12520(2023)에서. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. &quot;주의만 있으면 됩니다&quot;. NeurIPS에서(2017).[38] P. Gage. &quot;데이터 압축을 위한 새로운 알고리즘&quot;. C Users Journal 12.2(1994)에 실림. [39] A. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek, K. Asanovic, I. Stoica. &quot;AutoPhase: 심층 강화 학습을 통한 랜덤 포레스트에서 HLS 위상 순서 조정&quot;. MLSys에 실림. 2020. [56] AH Ashouri, M. Elhoushi, Y. Hua, X. Wang, MA Manzoor, B. Chan, Y. Gao. &quot;MLGOPerf: 성능 최적화를 위한 ML 가이드 인라이너&quot;. arXiv:2207.08389(2022)에 실림. A. Haj-Ali, NK Ahmed, T. Willke, S. Shao, K. Asanovic, I. Stoica. 영어: &quot;NeuroVectorizer: 심층 강화 학습을 통한 종단 간 벡터화&quot;. CGO에서. 2020. [57] [40] I. Loshchilov 및 F. Hutter. &quot;분리된 가중치 감소 규칙&quot;. arXiv에서: 1711.05101(2017). [41] K. Papineni, S. Roukos, T. Ward 및 W.-J. Zhu. &quot;BLEU: 기계 번역의 자동 평가를 위한 방법&quot;. ACL에서. 2002. [42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 및 O. Klimov. &quot;근위 정책 최적화 알고리즘&quot;. 출처: arXiv:1707.06347 (2017). [43] M. Paszkowski. LLVM Canon. https://github.com/michalpaszkowski/LLVM-Canon. [44] WM McKeeman. &quot;소프트웨어에 대한 차등 테스트&quot;. 출처: Digital Technical Journal 10.1 (1998). [45] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei. &quot;LongNet: 1,000,000개 토큰으로 변환기 확장&quot;. 출처: arXiv:2307.02486 (2023). [47] [46] S. Chen, S. Wong, L. Chen, Y. Tian. &quot;위치 보간을 통한 대규모 언어 모델의 컨텍스트 창 확장&quot;. In: arXiv:2306.15595 (2023). Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song 및 F. Wei. &quot;길이 외삽 가능한 변환기&quot;. In: arXiv:2212.10554 (2022). [48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, QV Le, D. Zhou 등. &quot;사슬 사고 촉진은 대규모 언어 모델에서 추론을 이끌어냅니다&quot;. NeurIPS에서. 2022. [49] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, G. Neubig. &quot;Pal: 프로그램 지원 언어 모델&quot;. ICML에서. 2023. [50] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. &quot;수학 단어 문제를 해결하기 위한 검증자 훈련&quot;. arXiv에서. 2110.14168(2021). [51] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han. &quot;SmoothQuant: 정확하고 영어: 대규모 언어 모델을 위한 효율적인 사후 학습 양자화&quot;. ICML에서. 2023. [52] F. Bodin, T. Kisuki, P. Knijnenburg, M. O&#39;Boyle, E. Rohou. &quot;비선형 최적화 공간에서의 반복 컴파일&quot;. FDO에서. 1998. [53] T. Kisuki, P. Knijnenburg, M. O&#39;Boyle. &quot;반복 컴파일을 사용한 타일 크기 및 언롤 요인의 결합 선택&quot;. PACT에서. 2000. [54] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. O&#39;Boyle, J. Thomson, M. Toussaint, C. Williams. &quot;기계 학습을 사용하여 반복적 최적화에 집중하기&quot;. CGO에서. 2006. [55] WF Ogilvie, P. Petoumenos, Z. Wang, H. Leather. &quot;능동 학습을 통한 반복 컴파일 비용 최소화&quot;. CGO에서. 2017. [59] 최적화 휴리스틱의 최종 심층 학습&quot;. PACT에서. 2017. PM Pothilimthana, A. Sabne, N. Sarda, KS Murthy, Y. Zhou, C. Angermueller, M. Burrows, S. Roy, K. Mandke, R. Farahani, et al. &quot;멀티패스 기계 학습 컴파일러의 자동 튜닝에 대한 유연한 접근 방식&quot;. PACT에서. 2021. [60] I. Hosseini 및 B. Dolan-Gavitt. &quot;C를 넘어서: 신경망 기계 번역을 사용한 리타겟팅 가능 디컴파일&quot;. 출처: arXiv:2212.08950 (2022). [61] SK Gallagher, WE Klieber 및 D. Svoboda. 코드 약점 식별을 위한 LLVM 중간 표현. 2022. [62] [63] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al. “CodeBERT: 프로그래밍 및 자연어를 위한 사전 학습된 모델”. arXiv:2002.08155(2020)에 게재됨. D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, SK Deng, C. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, M. Zhou. “GraphCodeBERT: 데이터 흐름을 통한 코드 표현 사전 학습”. arXiv:2009.08366(2021)에 게재됨. [64] Y. Wang, W. Wang, S. Joty, SC Hoi. “CodeT5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델”. 출처: arXiv:2109.00859 (2021). CS Xia, M. Paltenghi, JL Tian, M. Pradel 및 L. Zhang. &quot;대규모 언어 모델을 통한 범용 퍼징&quot;. 출처: arXiv:2308.04748 (2023). [65] [66] CS Xia 및 L. Zhang. 영어: “Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-shot Learning”. ArXiv:2207.08281(2022)에 게재됨. [67] CS Xia, Y. Wei, L. Zhang. &quot;대규모 사전 학습된 언어 모델 시대의 자동 프로그램 복구&quot;. ICSE에 게재됨. 2023. [68] CS Xia, L. Zhang. “Keep the Conversation Going: Fixing 162 of 337 bugs for $0.42 each”. ArXiv:2304.00385(2023)에 게재됨. [69] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. &quot;Llama: 개방적이고 효율적인 기초 언어 모델&quot;. in: arXiv 사전 인쇄본 arXiv:2302.13971 (2023). GitHub. Copilot. https://copilot.github.com/. [70]
"
"--- ABSTRACT ---
Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can acts as a plugand-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https: //audioldm.github.io/audiosr. Index Terms— audio super-resolution, diffusion model 1.
--- METHOD ---
s have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24 kHz bandwidth with a sampling rate of 48 kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can acts as a plugand-play module to enhance the generation quality of a wide range of audio generative models, including AudioLDM, Fastspeech2, and MusicGen. Our code and demo are available at https: //audioldm.github.io/audiosr. Index Terms— audio super-resolution, diffusion model 1. INTRODUCTION Audio super-resolution (SR) aims to estimate the higherfrequency information of a low-resolution audio signal, which yields a high-resolution audio signal with an expanded frequency range. High-resolution audio signals usually offer a better listening experience, which is often referred to as high fidelity. Due to the ability to enhance audio signal quality, audio super-resolution plays a significant role in various applications, such as historical recording restoration [1]. Previous studies on audio SR have primarily focused on specific domains, with a particular emphasis on speech SR. Early research decompose the speech SR task into spectral envelope estimation and excitation generation [2]. Recent works employing deep learning techniques, such as AECNN [3], NuWave [4], and NVSR [5], have shown superior performance compared to traditional methods. In addition to @™ Without AudioSR 60) mmm No Clear Diff mm With AudioSR S Ss Listener Preference (% N S (} AudioLDM MusicGen FastSpeechFig. 1. Subjective evaluation shows that applying AudioSR for audio super-resolution on the output of audio generation mod els can significantly enhance the perceptual quality. speech, there have been efforts to address music SR, including studies on general music [6] and specific instruments [7]. Apart from the limited scope of audio, existing research on audio SR also has primarily been conducted in controlled
--- EXPERIMENT ---
al settings, limiting its applicability in real-world scenarios. An important challenge in audio super-resolution, as highlighted in [5], is the issue of bandwidth mismatch. This occurs when the bandwidth of the test data differs from that of the training data, leading to model failure. However, this issue has not received significant attention in the literature, as previous works typically assume consistent bandwidth settings for both training and testing data. In practice, the input bandwidth of test audio can vary due to factors such as limitations in recording devices, sound characteristics, or applied compression processes. Only a few studies have explored flexible input bandwidth, including NVSR [5] and NuWave?2 [8]. However, these methods still primarily focus on speech SR without generalizing to a broader domain. In this paper, we propose a novel method that addresses the limitations of previous work on limited audio types and controlled sampling rate settings. We introduce a method called AudioSR, which extends audio SR to a general domain, including all audible sounds such as music, speech, and sound effects. Moreover, AudioSR is capable of handling a flexible input sampling rate between 4kHz and 32kHz, covering most of the use cases in real-world scenarios. It has been found that the prior knowledge learned by the neural vocoder is helpful for reconstructing higher frequency components in audio SR tasks [5]. Therefore, AudioSR follows [5] to perform audio SR on the mel-spectrogram and utilizes a neu --- --ral vocoder to synthesize the audio signal. To estimate the high-resolution mel-spectrogram, we follow AudioLDM [9] to train a latent diffusion model on learning the conditional generation of high-resolution mel-spectrogram from lowresolution mel-spectrogram. Our experiment demonstrates that AudioSR has achieved promising SR results on speech, music, and sound effects with different input sampling rate settings. Our subjective evaluation on enhancing the output of text-to-audio model AudioLDM [9], text-to-music model MusicGen [10], and text-to-speech model Fastspeech2 [11] show that AudioSR can be a plug-and-play module for most audio generation models to enhance listening quality. Our contributions are summarized as follows: * Our proposed AudioSR is the first system to achieve audio SR in the general audible audio domain, covering various types of audio such as music, speech, and sound effects. ¢ AudioSR can handle a flexible audio bandwidth ranging from 2kHz to 16kHz, and extend it to 24kHz bandwith with 48kHz sampling rate. ¢ Besides the promising results on audio SR benchmarks, AudioSR is also verified to be a plug-and-play module for enhancing the audio quality of various audio generation models such as AudioLDM, MusicGen, and FastSpeech2. The paper is organized as follows. Section 2 provides a general formulation of the audio super resolution task. Section 3 provides a detailed explanation of the design of AudioSR. The detailed experimental setup is discussed in Section 4. Our experimental results are presented in Section 5, and we conclude the paper in Section 6. 2. PROBLEM FORMULATION Given an analog signal that has been discretely sampled at a rate of | samples per second, resulting in a low-resolution sequence of values x; = [2;];=1,2,...7.1, the goal of audio superresolution (SR) is to estimate a higher resolution signal y, = (yili=1,2,...7-h Sampled at a rate of h samples per second, where h > | and T is the total duration in seconds. According to Nyquist’s theory, ; and yp, have maximum frequency bandwidths of |/2 Hz and h/2 Hz respectively. Therefore, the information contained between frequencies of h/2 — 1/2 Hz is missing from 2, and estimating this “missing” frequency data is the core objective of the SR task. In this paper, we follow the method proposed in NVSR [5] to decompose the original audio SR task into two steps, including (i) High-resolution Mel spectrogram Estimation, and (ii) Mel Spectrogram to Waveform Reconstruction with a Neural Vocoder. Specifically, we first resampling x; to xp, using cubic interpolation, where x; has a higher sampling rate h but with limited maximum bandwidth of 1/2 Hz. we follow the steps in [5] to calculate the mel spectrogram of both xj, and yp, resulting Xm xn and Ym xn, respectively, where m is Replace Low-res Audio Input High-res Audio Estimation (Resampled to higher sampling rate) | (Ra High-res Mel-spectrogram Estimation Go Low-res Mel-spectrogram Input Fig. 2. The AudioSR architecture. The replacement-based post-processing aims to preserve the original lower-frequency information in the model output. the number of time frames and n is the number of mel frequency bins. Then we utilize a generative model to learning the process of estimating Y based on X, which is denoted as Go : X ++ Y, where @ are the parameters of model G. Finally, a neural vocoder is employed to reconstruct the high sampling rate audio signal based on the estimation of Y, which can be formulated as Vy : Yu Yn, Where V is the neural vocoder and ¢@ are the learnable parameters. 3. METHOD The architecture of the proposed AudioSR is demonstrated in Figure 2. After resampling the low-resolution audio 2; to xp, the system first calculates both the STFT spectrogram and the mel spectrogram of x,. Note that the higher frequency bins in X;, are empty because x), does not have high-frequency information. X;, is then used as a conditioning signal to guide the pre-trained latent diffusion model to estimate the highresolution mel spectrogram Y;,. To ensure consistency in the low-frequency information between X;, and Yas we replace the lower frequency part of Yn with that of X;,. The melspectrogram after low-frequency replacement serves as the input to the neural vocoder, which output applies a similar technique to replace the low-frequency information with that of the input low-resolution audio. We introduce the training of the latent diffusion model and neural vocoder in Section 3.1. The post-processing algorithm is elaborated in Section 3.2. 3.1. High-resolution Waveform Estimation Latent diffusion model (LDM) has demonstrated promising results in various domains, including image synthesis [12] and audio generation [9]. In this study, we employ the LDM to estimate high-resolution mel-spectrograms. The training of our LDM is conducted within a latent space learned by a pre-trained variational autoencoder (VAE) F(-). The VAE is trained to perform autoencoding with a small compressed latent space in the middle, denoted as F : X ++ z% WH --- --Xx. By leveraging the lower-dimensional representation zo, the LDM can learn the generation of zo instead of X, resulting in a substantial reduction in computational cost. We adopt the methodology proposed in AudioLDM to optimize the VAE model, including the use of reconstruction loss, Kullback—Leibler divergence loss, and discriminative loss. We follow the formulation introduced in AudioLDM [9] to implement the LDM, with improvements on the training objective, noise schedule, and conditioning mechanism. It has been found that the common noise schedule used in the diffusion model is flawed [13], particularly because the noise schedule in the final diffusion step of LDM does not correspond to a Gaussian distribution. To address this issue, we ‘ollow [13] to update the noise schedule to a cosine schedule. This adjustment ensures that a standard Gaussian distribution can be achieved at the final diffusion step during training. Additionally, we incorporate the velocity prediction objective [14] on reflection of using the new noise schedule. The final training objective of our LDM is argming, || vg ~G(zn, k, Fene(X1); 4)||3, qd) where z; represents the data of zo at diffusion step k € (1, ..., A], || - |]2 denotes the Euclidean distance, Fenc denote the VAE encoder, and as described in [13], vz, is calculated based on zo, representing the prediction target of G at time step k. We adopt the Transformer-UNet architecture proposed in [15] as G. The input to G is obtained by concatenating z, with the Fenc(X1), which is the VAE latent extracted from the low-resolution mel-spectrogram X;. To incorporate classifier-free guidance, following the formulation in [9], we replace Fenc(X7) with an empty tensor at a random rate (e.g., 10%) during training. After training the latent diffusion model, we perform sampling using the DDIM sampler [16]. Neural Vocoder. The LDM is capable of estimating highresolution mel spectrograms. However, since mel-spectrograms are not directly audible, we employ a neural vocoder based on HiFiGAN [17] to convert the mel-spectrograms into waveforms. To address the issue of spectral leakage when implementing the original HiFiGAN, we adopt the multi-resolution discriminator [18] into the HiFiGAN vocoder. We optimize the vocoder using diverse audio data, as discussed in Section 4, resulting in a vocoder that operates at a sampling rate of 48kHz and can work on diverse types of audio. 3.2. Post-processing and Pre-processing Post-processing. The input low-resolution audio features X;, and x), are identical to the lower frequency bands in the estimation target, Y;, and yp. As a result, we can reuse the available information from X;, and x, to enhance both the LDM output Y;, and neural vocoder output y,. To accomplish this, we first determine the 0.99 roll-off frequency c of the entire input audio based on an open-source method! applied to both X,, and the STFT spectrogram of y;,. Subsequently, we replace the spectrogram components below the cutoff frequency in the LDM output Y;, and vocoder output 7, with the corresponding information in the X;, and xp, respectively. This post-processing method can ensure the final output does not significantly alter the lower-frequency information. Pre-processing. To minimize the mismatch between model training and evaluation, we perform preprocessing to the input audio during evaluation with a lowpass-filtering operation. We use the same method in post-processing to calculate the 0.99 roll-off frequency and perform lowpass filtering with an order 8 Chebyshev filter. 4, EXPERIMENT Training Datasets. The datasets used in this paper include MUSDB18-HQ [19], MoisesDB [20], MedleyDB [21], FreeSound [22]’, and the speech dataset from OpenSLR’°, which are downloaded by following the link provided by VoiceFixer [1]. All the audio data used are resampled at 48kHz sampling rate. The total duration of the training data is approximately 7000 hours. We utilize all these datasets to optimize VAE, LDM, and HiFi-GAN. Training Data Simulation. We follow the method introduced in NVSR [5] to simulate low-high resolution audio data pairs. Given a high-resolution audio data y,, we first perform lowpass filtering to the audio with a cutoff frequency uniformly sampled between 2kHz and 16kHz. To address the filter generalization problem [3], the type of the lowpass filter is randomly sampled within Chebyshey, Elliptic, Butterworth, and Boxcar, and the order of the lowpass filter is randomly selected between 2 and 10. Evaluation Datasets. We performed both subjective and objective evaluations. For subjective evaluations, we adopt the output of MusicGen (caption from MusicCaps [23]), AudioLDM (caption from AudioCaps [24]), and Fastspeech2 (transcription from LJSpeech [25]) to study if the AudioSR can enhance the quality of the generation. For MusicGen we use audio tagging ‘ to filter out the non-musical generation output. Finally, we collected 50 samples from MusicGen,samples from AudioLDM, and 20 samples from FastSpeech2, and processed them with AudioSR for subjective evaluations on listener preference. Besides, we curate three benchmarks for objective evaluation, including ESC50 (sound effect) [26], AudioStock (music)°, and VCTK (speech) [5]. The AudioStock dataset is built by hand-picking 100 high-quality music with 10 different genres. We only use the fold-5 in the ESCMhttps://librosa.org/doc/main/generated/librosa. feature.spectral_rolloff.html *https://labs.freesound.org/ 3https://openslr.org/ 4https://github.com/kkoutini/PaSST Shttps://audiostock.net/ --- --Objective Evaluation Subjective Evaluation Cutott: frequency ARI aktie 12kHe AudioStock (Music) ESC-50 (Sound Effect) ESC-50 (4kHz Cutoff Freq) GT-Mel 0.64 0.64 0.64 | Cutoff-frequency 4kHz — 8kHz 16kHz | 4kHz — 8kHz —_16kHz System Overall Quality Unprocessed 5.15 4.85 3.84 GT-Mel 0.61 0.61 0.61 | 0.84 0.84 0.84 GT-Mel TNuWave [4] 142 1.36 1.22 Unprocessed 4.25 348 1.99 | 3.90 3.07 2.25 Unprocessed 3.NVSR [5] 0.91 0.81 0.70 NVSR-DNN 167 149 1.13 | 164 1.59 1.76 NVSR-DNN 2.AudioSR 1.30 1.11 0.94 | NVSR-ResUNet 1.70 1.34 0.95 | 1.80 1.69 1.67 | NVSR-ResUNet 3.AudioSR-Speech 1.03 0.82 0.69 AudioSR 0.99 0.74 0.73 | 1.74 1.57 1.35 AudioSR 4.Table 1. Objective and subjective evaluation results for 48kHz audio SR of speech, music, and sound effect data with varying cutoff frequencies in the input audio. The objective metric used for evaluation is the LSD, where lower values indicate superior performance. The subjective metric measures the overall listening quality, with higher values indicating better performance. NVSR-DNN NVSR-ResUNet AudioSR Ground Truth Fig. 3. Comparison of different systems. AudioSR performs significantly better than the baseline NVSR models. dataset as the evaluation set. Evaluation Metrics For objective evaluation, we adopt the LSD metric, as used in prior studies [3,5]. Following the setup of [15], we conduct two types of subjective evaluation on Amazon Mturk°: Overall quality rating and preference comparison. In the overall quality rating, raters assign a score between 1 and 5 to reflect the audio quality. In the preference comparison, raters compare two audio files and select the one that sounds better. 5. RESULT We trained two versions of AudioSR for evaluation: the basic AudioSR that works on arbitrary audio types and input sampling rates, and a speech data fine-tuned variant called AudioSR-Speech. Our primary baseline for comparison is NVSR [5], which employs a similar mel-spectrogram and vocoder-based pipeline for audio SR tasks. The main distinction between AudioSR and NVSR lies in the mel-spectrogram estimation approach: AudioSR utilizes a latent diffusion model, while NVSR employs either a multilayer perceptron (NVSR-DNN) or a residual UNet (NVSR-ResUNet). For speech SR, we also compare with NuWave [4] as a baseline model, which also employs a diffusion model for audio SR. As shown in Table 1, AudioSR has achieved promising results on both objective and subjective evaluation. For music SR, AudioSR achieves state-of-the-art performance across all cutoff frequency settings, outperforming the baseline NVSR model by a large margin. For speech SR, AudioSR-Speech achieves the best performance on the 24kHz to 48kHz upsampling task. Also, the comparison between AudioSR and AudioSR-Speech indicates that finetuning on a small domain of data can significantly improve the LSD. The LSD metric does not always align with perceptual quality. In the 8kHz (i.e., 4kHz cutoff frequency) to 48kHz upsampling task on the ESC-50 dataset, we observed that NVSR-DNN achieved the best performance with an LSD score of 1.64. However, subjective evaluations indicated Shttps: //www.mturk.com/ that the perceptual quality of NVSR-DNN the worst with a score of 2.84, significantly lower than AudioSR’s score of 4.01. These findings suggest that LSD may not be a suitable evaluation metric for audio SR tasks on sound effect data, warranting further investigation in future research. As depicted in Figure 1, our subjective preference test demonstrates that the utilization of AudioSR significantly enhances the perceptual quality of the AudioLDM, MusicGen, and FastSpeech2 output. It is worth noting that the output of MusicGen is already in a high sampling rate of 32kHz, which may contribute to the relatively high rate of “No Clear Difference” responses. However, MusicGen still exhibits a significantly improved perceptual quality after applying AudioSR. 6.
--- CONCLUSION ---
This paper presents AudioSR, a 48kHz audio super-resolution model that is capable of working with diverse audio types and arbitrary sampling rate settings. Through evaluation of multiple audio super-resolution benchmarks, AudioSR demonstrates superior and robust performance on various types of audio and sampling rates. Additionally, our subjective evaluation highlights the effectiveness of AudioSR in enabling plug-and-play quality improvement for the audio generation models, including AudioLDM, MusicGen, and Fastspeech2. Future work includes extending AudioSR for real-time applications and exploring appropriate evaluation protocols for audio super-resolution in the general audio domain. 7. ACKNOWLEDGMENTS This research was partly supported by the British Broadcasting Corporation Research and Development, Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 “AI for Sound”, and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), Faculty of Engineering and Physical Science (FEPS), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. --- --[{ll [[8. REFERENCES H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, “VoiceFixer: Toward general speech restoration with neural vocoder,’ arXiv preprint:2109. 13731, 2021. J. Kontio, L. Laaksonen, and P. Alku, “Neural networkbased artificial bandwidth expansion of speech,” Transactions on Audio, Speech, and Language Processing, vol. 15, no. 3, pp. 873-881, 2007. H. Wang and D. Wang, “Towards robust speech superresolution,” Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2058-2066, 2021. J. Lee and S. Han, “NuWave: A diffusion probabilistic model for neural audio upsampling,” arXiv preprint:2104.02321, 2021. H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian, and D. Wang, “Neural vocoder is all you need for speech superresolution,” INTERSPEECH, pp. 4227-4231, 2022. S. Hu, B. Zhang, B. Liang, E. Zhao, and S. Lui, “Phaseaware music super-resolution using generative adversarial networks,” INTERSPEECH, pp. 4074-4078, 2020. N. C. Rakotonirina, “Self-attention for audio superresolution,” in International Workshop on Machine Learning for Signal Processing. YEEE, 2021. S. Han and J. Lee, “NUWave 2: A general neural audio upsampling model for various sampling rates,” arXiv preprint:2206.08545, 2022. H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, “AudioLDM: Text-toaudio generation with latent diffusion models,” International Conference on Machine Learning, 2023. J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, “Simple and controllable music generation,” arXiv preprint:2306.05284, 2023. Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “Fastspeech 2: Fast and high-quality end-to-end text to speech,” in International Conference on Learning Representations, 2021. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Conference on Computer Vision and Pattern Recognition, 2022, pp. 10684-10695. S. Lin, B. Liu, J. Li, and X. Yang, “Common diffusion noise schedules and sample steps are flawed,” arXiv preprint:2305.08891, 2023. [14] [15]T. Salimans and J. Ho, “Progressive distillation for fast sampling of diffusion models,” International Conference on Learning Representations, 2022. H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong, Y. Wang, W. Wang, Y. Wang, and M. D. Plumbley, “AudioLDM 2: Learning holistic audio generation with self-supervised pretraining,’ arXiv preprint arXiv:2308.05734, 2023. J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in International Conference on Learning Representations, 2020. J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,’ Advances in Neural Information Processing Systems, vol. 33, pp. 17 022-17 033, 2020. J. You, D. Kim, G. Nam, G. Hwang, and G. Chae, “GAN Vocoder: Multi-resolution discriminator is all you need,” arXiv preprint:2103.05236, 2021. Z. Rafii, A. Liutkus, F.-R. Stéter, S. I. Mimilakis, and R. Bittner, “MUSDB18-HQ - an uncompressed version of MUSDB18,” Aug 2019. I. Pereira, F. Aratijo, F. Korzeniowski, and R. Vogl, “MoisesDB: A dataset for source separation beyond 4stems,” arXiv preprint:2307.15913, 2023. R. M. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam, and J. P. Bello, “MedleyDB: A multitrack dataset for annotation-intensive mir research.” in JSMIR, vol. 14, 2014, pp. 155-160. X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, “WavCaps: A ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,” arXiv preprint:2303.17395, 2023. A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., “MusicLM: Generating music from text,” arXiv preprint:2301.11325, 2023. C. D. Kim, B. Kim, H. Lee, and G. Kim, “AudioCaps: Generating captions for audios in the wild,’ in NAACLALT, 2019, pp. 119-132. K. Ito and L. Johnson, “The LJSpeech dataset,” https: //keithito.com/LJ-Speech-Dataset/, 2017. K. J. Piczak, “ESC: Dataset for environmental sound classification,” in International Conference on Multimedia, 2015, pp. 1015-1018.
"	"--- ABSTRACT ---
오디오 초고해상도는 저해상도 오디오의 고주파 성분을 예측하여 디지털 애플리케이션에서 오디오 품질을 향상시키는 기본적인 작업입니다. 이전 방법에는 오디오 유형(예: 음악, 음성)의 범위가 제한적이고 처리할 수 있는 특정 대역폭 설정(예: 4kHz~8kHz)과 같은 한계가 있었습니다. 이 논문에서는 사운드 효과, 음악, 음성을 포함한 다양한 오디오 유형에 대해 강력한 오디오 초고해상도를 수행할 수 있는 확산 기반 생성 모델인 AudioSR을 소개합니다. 구체적으로 AudioSR은 2kHz~16kHz 대역폭 범위 내의 모든 입력 오디오 신호를 샘플링 속도가 48kHz인 24kHz 대역폭의 고해상도 오디오 신호로 업샘플링할 수 있습니다. 다양한 오디오 초고해상도 벤치마크에 대한 광범위한 객관적 평가는 제안된 모델이 달성한 강력한 결과를 보여줍니다. 또한 주관적인 평가에 따르면 AudioSR은 AudioLDM, Fastspeech2, MusicGen을 포함한 광범위한 오디오 생성 모델의 생성 품질을 향상시키는 플러그 앤 플레이 모듈 역할을 할 수 있습니다. 저희 코드와 데모는 https://audioldm.github.io/audiosr에서 볼 수 있습니다. 인덱스 용어 오디오 초고해상도, 확산 모델 1.
--- METHOD ---
s에는 제한된 오디오 유형(예: 음악, 음성)의 범위와 처리할 수 있는 특정 대역폭 설정(예: 4kHz~8kHz)과 같은 제한 사항이 있습니다. 이 논문에서는 사운드 효과, 음악, 음성을 포함한 다양한 오디오 유형에 대해 강력한 오디오 초고해상도를 수행할 수 있는 확산 기반 생성 모델인 AudioSR을 소개합니다. 구체적으로 AudioSR은 2kHz~16kHz의 대역폭 범위 내의 모든 입력 오디오 신호를 샘플링 속도가 48kHz인 24kHz 대역폭의 고해상도 오디오 신호로 업샘플링할 수 있습니다. 다양한 오디오 초고해상도 벤치마크에 대한 광범위한 객관적 평가는 제안된 모델이 달성한 강력한 결과를 보여줍니다. 또한 주관적인 평가에 따르면 AudioSR은 AudioLDM, Fastspeech2, MusicGen을 포함한 광범위한 오디오 생성 모델의 생성 품질을 향상시키는 플러그 앤 플레이 모듈 역할을 할 수 있습니다. 코드와 데모는 https://audioldm.github.io/audiosr에서 제공됩니다. 색인 용어 오디오 초고해상도, 확산 모델 1. 서론 오디오 초고해상도(SR)는 저해상도 오디오 신호의 고주파 정보를 추정하여 확장된 주파수 범위의 고해상도 오디오 신호를 생성하는 것을 목표로 합니다. 고해상도 오디오 신호는 일반적으로 더 나은 청취 경험을 제공하며, 이를 고충실도라고 합니다. 오디오 신호 품질을 향상시키는 기능으로 인해 오디오 초고해상도는 과거 녹음 복원과 같은 다양한 응용 분야에서 중요한 역할을 합니다[1]. 오디오 SR에 대한 이전 연구는 주로 특정 도메인에 초점을 맞추었으며, 특히 음성 SR에 중점을 두었습니다. 초기 연구에서는 음성 SR 작업을 스펙트럼 엔벨로프 추정 및 여기 생성으로 분해했습니다[2]. AECNN[3], NuWave[4], NVSR[5]과 같은 딥 러닝 기술을 사용한 최근 연구는 기존 방법에 비해 우수한 성능을 보였습니다. 청취자 선호도(%) 외에도 AudioSR 없음 명확한 차이 없음 AudioSRAudioLDM 있음 MusicGen FastSpeechFig. 1. 주관적 평가에 따르면 오디오 생성 모델의 출력에 오디오 초고해상도를 위해 AudioSR을 적용하면 지각적 품질을 크게 향상시킬 수 있습니다. 음성의 경우 일반 음악[6]과 특정 악기[7]에 대한 연구를 포함하여 음악 SR을 다루려는 노력이 있었습니다. 오디오의 제한된 범위 외에도 오디오 SR에 대한 기존 연구는 주로 통제된 환경에서 수행되었습니다.
--- EXPERIMENT ---
영어: al 설정으로 실제 시나리오에서의 적용성이 제한됩니다. [5]에서 강조된 것처럼 오디오 초고해상도의 중요한 과제는 대역폭 불일치 문제입니다. 이는 테스트 데이터의 대역폭이 훈련 데이터의 대역폭과 달라서 모델이 실패할 때 발생합니다. 그러나 이 문제는 이전 연구에서 일반적으로 훈련 및 테스트 데이터 모두에 대해 일관된 대역폭 설정을 가정하기 때문에 문헌에서 크게 주목받지 못했습니다. 실제로 테스트 오디오의 입력 대역폭은 녹음 장치, 사운드 특성 또는 적용된 압축 프로세스의 제한과 같은 요인으로 인해 달라질 수 있습니다. NVSR [5] 및 NuWave2 [8]를 포함하여 유연한 입력 대역폭을 탐구한 연구는 소수에 불과합니다. 그러나 이러한 방법은 여전히 더 넓은 영역으로 일반화하지 않고 주로 음성 SR에 초점을 맞춥니다. 이 논문에서는 제한된 오디오 유형과 제어된 샘플링 속도 설정에 대한 이전 연구의 한계를 해결하는 새로운 방법을 제안합니다. 음악, 음성 및 음향 효과와 같은 모든 가청 소리를 포함하여 오디오 SR을 일반 영역으로 확장하는 AudioSR이라는 방법을 소개합니다. 또한 AudioSR은 4kHz와 32kHz 사이의 유연한 입력 샘플링 속도를 처리할 수 있어 실제 시나리오에서 대부분의 사용 사례를 포괄합니다. 신경 보코더가 학습한 사전 지식은 오디오 SR 작업에서 더 높은 주파수 성분을 재구성하는 데 도움이 되는 것으로 밝혀졌습니다[5]. 따라서 AudioSR은 [5]에 따라 멜 스펙트로그램에서 오디오 SR을 수행하고 신경 보코더를 사용하여 오디오 신호를 합성합니다. 고해상도 멜 스펙트로그램을 추정하기 위해 AudioLDM[9]에 따라 저해상도 멜 스펙트로그램에서 고해상도 멜 스펙트로그램의 조건부 생성을 학습하는 잠재 확산 모델을 학습합니다. 실험 결과 AudioSR이 다양한 입력 샘플링 속도 설정으로 음성, 음악 및 음향 효과에 대해 유망한 SR 결과를 달성했음을 보여줍니다. 영어: 텍스트-오디오 모델 AudioLDM [9], 텍스트-음악 모델 MusicGen [10], 텍스트-음성 모델 Fastspeech2 [11]의 출력을 향상시키는 것에 대한 주관적인 평가는 AudioSR이 대부분의 오디오 생성 모델에서 청취 품질을 향상시키는 플러그 앤 플레이 모듈이 될 수 있음을 보여줍니다.저희의 기여는 다음과 같이 요약됩니다.• 제안한 AudioSR은 음악, 음성, 음향 효과와 같은 다양한 유형의 오디오를 포괄하는 일반 가청 오디오 도메인에서 오디오 SR을 달성한 최초의 시스템입니다.• AudioSR은 2kHz~16kHz 범위의 유연한 오디오 대역폭을 처리할 수 있으며 48kHz 샘플링 속도로 24kHz 대역폭까지 확장할 수 있습니다.• 오디오 SR 벤치마크에서 유망한 결과 외에도 AudioSR은 AudioLDM, MusicGen, FastSpeech2와 같은 다양한 오디오 생성 모델의 오디오 품질을 향상시키는 플러그 앤 플레이 모듈임이 검증되었습니다.이 논문은 다음과 같이 구성됩니다.섹션 2에서는 오디오 초고해상도 작업의 일반적인 공식을 제공합니다. 섹션 3에서는 AudioSR의 설계에 대한 자세한 설명을 제공합니다. 자세한 실험 설정은 섹션 4에서 논의합니다. 실험 결과는 섹션 5에 제시되고 섹션 6에서 논문을 마무리합니다. 2. 문제 공식화 초당 샘플 속도로 이산적으로 샘플링되어 저해상도 값 시퀀스 x₁ = [xi] 1,2,T1이 생성된 아날로그 신호가 주어지면 오디오 초고해상도(SR)의 목표는 초당 h 샘플 속도로 샘플링된 더 높은 해상도 신호 yh = [i]i=1,2,...Th를 추정하는 것입니다. 여기서 h1과 T는 초 단위의 총 지속 시간입니다. 나이퀴스트 이론에 따르면 xɩ와 yɩ는 각각 1/2Hz와 h/2Hz의 최대 주파수 대역폭을 갖습니다. 따라서 h/2 - 1/2Hz의 주파수 사이에 포함된 정보는 xɩ에서 누락되고 이 &quot;누락된&quot; 주파수 데이터를 추정하는 것이 SR 작업의 핵심 목표입니다. 이 논문에서 우리는 NVSR [5]에서 제안된 방법을 따라 원래 오디오 SR 작업을 두 단계로 분해합니다.여기에는 (i) 고해상도 멜 스펙트로그램 추정, (ii) 신경 보코더를 사용한 멜 스펙트로그램에서 파형 재구성이 포함됩니다.특히, 먼저 3차 보간을 사용하여 x1을 x로 리샘플링합니다.여기서 x는 더 높은 샘플링 속도 h를 갖지만 1/2Hz의 제한된 최대 대역폭을 갖습니다.우리는 [5]의 단계를 따라 xh와 yh의 멜 스펙트로그램을 계산하여 각각 Xmxn과 Ymxn을 얻습니다.여기서 m은 20000입니다.STFT 스펙트로그램저해상도 오디오 입력 xh STET 스펙트로그램 대체(더 높은 샘플링 속도로 리샘플링됨) 시간 Xh 대체 저해상도 멜 스펙트로그램 입력 고해상도 오디오 추정 신경 보코더 Vo Yh 고해상도 멜 스펙트로그램 추정 Ge 잠복 확산 모델 그림 2. AudioSR 아키텍처. 대체 기반 사후 처리의 목적은 모델 출력에서 원래 저주파 정보를 보존하는 것입니다. : 시간 프레임 수이고 n은 멜 주파수 빈의 수입니다. 그런 다음 생성 모델을 사용하여 X에 기반하여 Y를 추정하는 프로세스를 학습합니다. 이를 Go XŶ로 표시합니다. 여기서 0은 모델 G의 매개변수입니다. 마지막으로 신경 보코더를 사용하여 Y의 추정을 기반으로 높은 샘플링 레이트 오디오 신호를 재구성합니다. 이는 Vo : Ŷ → ŷh로 공식화할 수 있습니다. 여기서 V는 신경 보코더이고 는 학습 가능한 매개변수입니다. 3. 방법 제안된 AudioSR의 아키텍처는 그림 2에 나와 있습니다. 저해상도 오디오 x₁를 xh로 리샘플링한 후 시스템은 먼저 STFT 스펙트로그램과 xh의 멜 스펙트로그램을 모두 계산합니다. x에 고주파 정보가 없기 때문에 X의 고주파 빈은 비어 있습니다. X는 사전 훈련된 잠재 확산 모델을 안내하여 고해상도 멜 스펙트로그램 Ŷ를 추정하는 컨디셔닝 신호로 사용됩니다. X와 Ŷh 사이의 저주파 정보의 일관성을 보장하기 위해 Ŷ의 저주파 부분을 X의 저주파 부분으로 대체합니다. 저주파 대체 후의 멜 스펙트로그램은 신경 보코더의 입력으로 사용되며, 이 출력은 유사한 기술을 적용하여 저주파 정보를 입력 저해상도 오디오의 정보로 대체합니다. 3.1절에서 잠재 확산 모델과 신경 보코더의 학습을 소개합니다. 후처리 알고리즘은 3.2절에 자세히 설명되어 있습니다. 3.1. 고해상도 파형 추정 잠재 확산 모델(LDM)은 이미지 합성[12] 및 오디오 생성[9]을 포함한 다양한 도메인에서 유망한 결과를 보여주었습니다. 이 연구에서는 LDM을 사용하여 고해상도 멜 스펙트로그램을 추정합니다. LDM의 훈련은 사전 훈련된 변분 자동 인코더(VAE) F(.)가 학습한 잠재 공간 내에서 수행됩니다. VAE는 F : X ⇒ 20 → X로 표시되는 중앙에 작은 압축 잠재 공간을 사용하여 자동 인코딩을 수행하도록 훈련됩니다. 저차원 표현 zo를 활용함으로써 LDM은 X 대신 zo의 생성을 학습할 수 있어 계산 비용이 상당히 감소합니다. 재구성 손실, Kullback-Leibler 발산 손실, 판별 손실을 포함하여 VAE 모델을 최적화하기 위해 AudioLDM에서 제안한 방법론을 채택합니다. AudioLDM [9]에서 도입한 공식을 따라 훈련 목표, 노이즈 일정 및 컨디셔닝 메커니즘을 개선하여 LDM을 구현합니다. 확산 모델에서 사용되는 일반적인 노이즈 일정에는 결함이 있는 것으로 밝혀졌습니다[13]. 특히 LDM의 최종 확산 단계에서 노이즈 일정이 가우시안 분포와 일치하지 않기 때문입니다. 이 문제를 해결하기 위해 [13]에 따라 노이즈 일정을 코사인 일정으로 업데이트합니다. 이 조정을 통해 학습 중 최종 확산 단계에서 표준 가우시안 분포를 얻을 수 있습니다. 또한 새로운 노이즈 일정을 사용한 것을 반영하여 속도 예측 목표 [14]를 통합합니다. LDM의 최종 학습 목표는 argming Uk G(Zk, k, Fenc (X1); 0)||2, (1)입니다. 여기서 zk는 확산 단계 k에서 zo의 데이터 Є [1, ..., K]를 나타내고 || · ||2는 유클리드 거리를 나타내고 Fenc는 VAE 인코더를 나타내며 [13]에 설명된 대로 vk는 시간 단계 k에서 G의 예측 목표를 나타내는 zo를 기반으로 계산됩니다. 우리는 [15]에서 제안된 Transformer-UNet 아키텍처를 G로 채택합니다. G에 대한 입력은 zk를 저해상도 멜 스펙트로그램 X₁에서 추출한 VAE 잠재 음성인 Fenc(X1)와 연결하여 얻습니다. 분류기 없는 안내를 통합하기 위해 [9]의 공식에 따라 학습 중에 무작위 비율(예: 10%)로 Fenc(X₁)를 빈 텐서로 바꿉니다. 잠재 확산 모델을 학습한 후 DDIM 샘플러[16]를 사용하여 샘플링을 수행합니다. 신경 보코더. LDM은 고해상도 멜 스펙트로그램을 추정할 수 있습니다. 그러나 멜 스펙트로그램은 직접 들을 수 없기 때문에 HiFiGAN[17]에 기반한 신경 보코더를 사용하여 멜 스펙트로그램을 파형으로 변환합니다. 원래 HiFiGAN을 구현할 때 스펙트럼 누출 문제를 해결하기 위해 HiFiGAN 보코더에 다중 해상도 판별기[18]를 채택했습니다. 섹션 4에서 설명한 대로 다양한 오디오 데이터를 사용하여 보코더를 최적화하여 48kHz 샘플링 속도에서 작동하고 다양한 유형의 오디오에서 작동할 수 있는 보코더를 만들었습니다. 3.2. 사후 처리 및 사전 처리 사후 처리. 입력 저해상도 오디오 기능 Xh 및 xh는 추정 대상 Y₁ 및 y의 저주파 대역과 동일합니다. 결과적으로 X 및 x에서 사용 가능한 정보를 재사용하여 LDM 출력 Ŷ와 신경 보코더 출력 ŷ를 모두 향상시킬 수 있습니다. 이를 달성하기 위해 먼저 X와 yɲ의 STFT 스펙트로그램에 적용된 오픈 소스 방식¹을 기반으로 전체 입력 오디오의 0.99 롤오프 주파수 c를 결정합니다. 그 후, LDM 출력 Ŷ 및 보코더 출력 ŷ에서 차단 주파수 아래의 스펙트로그램 구성 요소를 각각 Xh 및 xh의 해당 정보로 대체합니다. 이 후처리 방법은 최종 출력이 저주파 정보를 크게 변경하지 않도록 보장할 수 있습니다. 전처리. 모델 학습과 평가 간의 불일치를 최소화하기 위해 저역 통과 필터링 작업으로 평가하는 동안 입력 오디오에 대한 전처리를 수행합니다. 후처리에서 동일한 방법을 사용하여 0.99 롤오프 주파수를 계산하고 8차 체비셰프 필터로 저역 통과 필터링을 수행합니다. 4. 실험 학습 데이터 세트. 이 논문에서 사용한 데이터 세트에는 MUSDB18-HQ [19], MoisesDB [20], MedleyDB [21], FreeSound [22]² 및 OpenSLR³의 음성 데이터 세트가 포함되며, 이는 VoiceFixer [1]에서 제공하는 링크를 따라 다운로드할 수 있습니다. 사용된 모든 오디오 데이터는 48kHz 샘플링 속도로 리샘플링됩니다. 학습 데이터의 총 기간은 약 7000시간입니다. 이 모든 데이터 세트를 활용하여 VAE, LDM 및 HiFi-GAN을 최적화합니다. 학습 데이터 시뮬레이션. NVSR [5]에서 도입한 방법을 따라 저해상도-고해상도 오디오 데이터 쌍을 시뮬레이션합니다. 고해상도 오디오 데이터 yh가 주어지면 먼저 2kHz와 16kHz 사이에서 균일하게 샘플링된 차단 주파수로 오디오에 저역 통과 필터링을 수행합니다. 필터 일반화 문제 [3]를 해결하기 위해 저역 통과 필터의 유형은 Chebyshev, Elliptic, Butterworth 및 Boxcar 내에서 무작위로 샘플링되고 저역 통과 필터의 순서는 2와 10 사이에서 무작위로 선택됩니다. 평가 데이터 세트. 주관적 및 객관적 평가를 모두 수행했습니다. 주관적인 평가를 위해 MusicGen(MusicCaps [23]의 캡션), AudiOLDM(AudioCaps [24]의 캡션), Fastspeech2(LJSpeech [25]의 전사)의 출력을 채택하여 AudioSR이 생성의 품질을 향상시킬 수 있는지 연구했습니다. MusicGen의 경우 오디오 태그 4를 사용하여 비음악적 생성 출력을 필터링했습니다. 마지막으로 MusicGen에서 50개 샘플, AudioLDM에서 샘플, FastSpeech2에서 20개 샘플을 수집하여 청취자 선호도에 대한 주관적 평가를 위해 AudioSR로 처리했습니다. 또한 ESC50(사운드 효과) [26], AudioStock(음악)5, VCTK(음성) [5]를 포함하여 객관적인 평가를 위한 세 가지 벤치마크를 큐레이션했습니다. AudioStock 데이터 세트는 10가지 장르의 고품질 음악 100개를 직접 선택하여 구축했습니다. 우리는 ESC에서 fold-5만 사용합니다.https://librosa.org/doc/main/generated/librosa. feature.spectral_rolloff.html 2 https://labs.freesound.org/ 3 https://openslr.org/ 4 https://github.com/kkoutini/PASST Shttps://audiostock.net/ 객관적 평가 주관적 평가 VCTK(음성) AudioStock(음악) ESC-50(음향 효과) ESC-50(4kHz 차단 주파수) 차단 주파수 4kHz 8kHz 12kHz GT-Mel 0.0.0.Unprocessed 5.4.3. 차단 주파수 4kHz 8kHz GT-Mel 16kHz 4kHz 8kHz 16kHz 시스템 전체 품질 0.0.0.0.0.0.GT-Mel 4.NuWave [4] 1.1.1.Unprocessed 4.3.1.3.3.2.Unprocessed 3.NVSR [5] 0.0.0.NVSR-DNN 1.1.1.1.1.1.NVSR-DNN 2.AudioSR 1.1.0.1.0.0.NVSR-ResUNet AudioSR 1.70 1.0.99 0.0.0.1.80 1.1.74 1.1.1.NVSR-ResUNet AudioSR 3.4.AudioSR-Speech 표 1. 입력 오디오에서 다양한 차단 주파수를 사용한 음성, 음악 및 음향 효과 데이터의 48kHz 오디오 SR에 대한 객관적 및 주관적 평가 결과. 평가에 사용된 객관적 지표는 LSD로, 값이 낮을수록 성능이 우수함을 나타냅니다. 주관적 지표는 전반적인 청취 품질을 측정하며, 값이 높을수록 성능이 우수함을 나타냅니다. 처리되지 않은 NVSR-DNN NVSR-ResUNet AudioSR Ground Truth 그림 3. 다양한 시스템 비교. AudioSR은 기준 NVSR 모델보다 상당히 우수한 성능을 보입니다. 데이터 세트를 평가 세트로 사용합니다.평가 지표 객관적인 평가를 위해 이전 연구[3,5]에서 사용된 LSD 지표를 채택합니다.[15]의 설정에 따라 Amazon Mturk6에서 두 가지 유형의 주관적 평가를 수행합니다.전체 품질 평가 및 선호도 비교.전체 품질 평가에서 평가자는 오디오 품질을 반영하여 1~5 사이의 점수를 할당합니다.선호도 비교에서 평가자는 두 오디오 파일을 비교하여 더 나은 소리가 나는 것을 선택합니다.5. 결과 평가를 위해 두 가지 버전의 AudioSR을 훈련했습니다.임의의 오디오 유형과 입력 샘플링 속도에서 작동하는 기본 AudioSR과 AudioSR-Speech라는 음성 데이터 미세 조정 변형입니다.비교를 위한 주요 기준선은 오디오 SR 작업을 위해 유사한 멜 스펙트로그램 및 보코더 기반 파이프라인을 사용하는 NVSR[5]입니다. AudioSR과 NVSR의 주요 차이점은 mel-spectrogram 추정 방식에 있습니다.AudioSR은 잠재 확산 모델을 활용하는 반면 NVSR은 다층 퍼셉트론(NVSR-DNN) 또는 잔여 UNet(NVSR-ResUNet)을 사용합니다.음성 SR의 경우 오디오 SR에 확산 모델을 사용하는 기준 모델인 NuWave[4]와도 비교합니다.표 1에서 볼 수 있듯이 AudioSR은 객관적 및 주관적 평가에서 모두 유망한 결과를 얻었습니다.음악 SR의 경우 AudioSR은 모든 차단 주파수 설정에서 최첨단 성능을 달성하여 기준 NVSR 모델보다 큰 차이로 성능이 우수합니다.음성 SR의 경우 AudioSR-Speech가 24kHz~48kHz 업샘플링 작업에서 최고의 성능을 달성합니다.또한 AudioSR과 AudioSR-Speech를 비교하면 작은 데이터 영역에서 미세 조정을 통해 LSD를 크게 개선할 수 있음을 알 수 있습니다. LSD 지표는 항상 지각적 품질과 일치하지 않습니다. ESC-50 데이터 세트에서 8kHz(즉, 4kHz 차단 주파수)에서 48kHz 업샘플링 작업에서 NVSR-DNN이 1.64의 LSD 점수로 가장 좋은 성능을 달성한 것을 관찰했습니다. 그러나 주관적인 평가에 따르면 https://www.mturk.com/ NVSR-DNN의 지각적 품질은 2.84의 점수로 최악이었으며 AudioSR의 점수 4.01보다 상당히 낮았습니다. 이러한 결과는 LSD가 음향 효과 데이터에 대한 오디오 SR 작업에 적합한 평가 지표가 아닐 수 있음을 시사하며 향후 연구에서 추가 조사가 필요합니다. 그림 1에서 볼 수 있듯이 주관적인 선호도 테스트는 AudioSR을 활용하면 AudioLDM, MusicGen 및 FastSpeech2 출력의 지각적 품질이 크게 향상됨을 보여줍니다. MusicGen의 출력은 이미 32kHz의 높은 샘플링 속도에 있으며, 이는 &quot;No Clear Difference&quot; 응답의 비교적 높은 속도에 기여할 수 있다는 점에 주목할 가치가 있습니다. 그러나 MusicGen은 AudioSR을 적용한 후에도 여전히 상당히 향상된 지각적 품질을 보여줍니다. 6.
--- CONCLUSION ---
이 논문에서는 다양한 오디오 유형과 임의의 샘플링 속도 설정으로 작동할 수 있는 48kHz 오디오 초고해상도 모델인 AudioSR을 제시합니다. 여러 오디오 초고해상도 벤치마크를 평가하여 AudioSR은 다양한 유형의 오디오와 샘플링 속도에서 우수하고 견고한 성능을 보여줍니다. 또한, 주관적인 평가에서는 AudioLDM, MusicGen, Fastspeech2를 포함한 오디오 생성 모델에 대한 플러그 앤 플레이 품질 개선을 가능하게 하는 AudioSR의 효과를 강조합니다. 향후 작업에는 실시간 애플리케이션을 위한 AudioSR 확장과 일반 오디오 도메인에서 오디오 초고해상도에 적합한 평가 프로토콜 탐색이 포함됩니다. 7. 감사의 말 이 연구는 부분적으로 British Broadcasting Corporation Research and Development, Engineering and Physical Sciences Research Council(EPSRC) Grant EP/T019751/1 &quot;AI for Sound&quot; 및 University of Surrey의 공학 및 물리 과학부(FEPS)의 시각, 음성 및 신호 처리 센터(CVSSP)의 박사 장학금의 지원을 받았습니다. 오픈 액세스의 목적으로 저자는 발생하는 모든 저자 승인 원고 버전에 Creative Commons Attribution(CC BY) 라이선스를 적용했습니다. 8. 참고문헌 [1] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, &quot;VoiceFixer: Toward general speech restoration with neural vocoder,&quot; arXiv preprint:2109.13731, 2021. [2] J. Kontio, L. Laaksonen, and P. Alku, 영어: “음성의 신경망 기반 인공 대역폭 확장,&quot; 오디오, 음성 및 언어 처리 논문, 제15권, 제3호, 873-881쪽, 2007년. [3] H. Wang 및 D. Wang, “강력한 음성 초해상도를 향하여,&quot; 오디오, 음성 및 언어 처리 논문, 제15권, 제3호, 873-881쪽, 2007년. 29, pp. 2058-2066, 2021. [4] J. Lee 및 S. Han, &quot;NuWave: 신경 오디오 업샘플링을 위한 확산 확률적 모델,&quot; arXiv 사전 인쇄:2104.02321, 2021. [5] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian 및 D. Wang, &quot;신경 보코더만 있으면 음성 초고해상도에 충분합니다,&quot; INTERSPEECH, pp. 4227-4231, 2022. [6] S. Hu, B. Zhang, B. Liang, E. Zhao 및 S. Lui, &quot;생성적 적대 네트워크를 사용한 위상 인식 음악 초고해상도,&quot; INTERSPEECH, pp. 4074–4078, 2020. [7] NC Rakotonirina, 신호 처리를 위한 기계 학습에 관한 국제 워크숍에서 &quot;오디오 초고해상도를 위한 자기 주의&quot;. IEEE, 2021. [8] S. Han 및 J. Lee, &quot;NUWave 2: 다양한 샘플링 속도에 대한 일반 신경 오디오 업샘플링 모델,&quot; arXiv 사전 인쇄: 2206.08545, 2022. [9] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang 및 MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; 기계 학습 국제 컨퍼런스, 2023. [10] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi 및 A. Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv 사전 인쇄: 2306.05284, 2023. [11] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao 및 T. Liu, &quot;Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트-음성 변환&quot;, International Conference on Learning Representations, 2021. [12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, Conference on Computer Vision and Pattern Recognition, 2022, pp. 10684–10695. [13] S. Lin, B. Liu, J. Li, 및 X. Yang, &quot;일반적인 확산 노이즈 일정 및 샘플 단계에는 결함이 있습니다.&quot; arXiv 사전 인쇄: 2305.08891, 2023. [14] T. Salimans 및 J. Ho, &quot;확산 모델의 빠른 샘플링을 위한 점진적 증류&quot;, International Conference on Learning Representations, 2022. [15] H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong, Y. Wang, W. Wang, Y. Wang, 및 MD Plumbley, &quot;AudioLDM 2: 자체 감독 사전 학습을 통한 전체적인 오디오 생성 학습&quot;, arXiv 사전 인쇄 arXiv: 2308.05734, 2023. [16] J. Song, C. Meng, 및 S. Ermon, &quot;확산 암시적 모델의 노이즈 제거&quot;, International Conference on Learning Representations, 2020. [17] J. Kong, J. Kim, and J. Bae, &quot;HiFi-GAN: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크,&quot; 신경 정보 처리 시스템의 발전, vol. 33, pp. 17022-17033, 2020. [18] J. You, D. Kim, G. Nam, G. Hwang, and G. Chae, &quot;GAN Vocoder: 다중 해상도 판별기만 있으면 됩니다,&quot; arXiv 사전 인쇄:2103.05236, 2021. [19] Z. Rafii, A. Liutkus, F.-R. Stöter, SI Mimilakis 및 R. Bittner, &quot;MUSDB18-HQ - MUSDB18의 압축되지 않은 버전,&quot; 2019년 8월. [20] I. Pereira, F. Araújo, F. Korzeniowski 및 R. Vogl, &quot;MoisesDB: 4개 스템을 넘어서는 소스 분리를 위한 데이터 세트,&quot; arXiv 사전 인쇄:2307.15913, 2023. [21] RM Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam 및 JP Bello, &quot;MedleyDB: 주석 집약적 mir 연구를 위한 다중 트랙 데이터 세트.&quot; ISMIR, 제14권, 2014년, 155-160쪽. [22] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, MD Plumbley, Y. Zou, W. Wang, &quot;WavCaps: 오디오-언어 멀티모달 연구를 위한 ChatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트,&quot; arXiv 사전 인쇄: 2303.17395, 2023. [23] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., &quot;MusicLM: 텍스트에서 음악 생성,&quot; arXiv 사전 인쇄: 2301.11325, 2023. [24] CD Kim, B. Kim, H. Lee, G. Kim, &quot;AudioCaps: 오디오에서 캡션 생성 야생,&quot; NAACLHLT, 2019, pp. 119–132. [25] K. Ito 및 L. Johnson, &quot;LJSpeech 데이터 세트,&quot; https://keithito.com/LJ-Speech-Dataset/, 2017. [26] KJ Piczak, &quot;ESC: 환경 소음 분류를 위한 데이터 세트,&quot; 국제 멀티미디어 컨퍼런스, 2015, pp. 1015–1018.
"
"--- ABSTRACT ---
Video matting has broad applications, from adding interesting effects to casually captured movies to assisting video production professionals. Matting with associated effects such as shadows and reflections has also attracted increasing research activity, and methods like Omnimatte have been proposed to separate dynamic foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hindering application to real-world videos. In this paper, we propose a novel video matting method, OmnimatteRF, that combines dynamic 2D foreground layers and a 3D background model. The 2D layers preserve the details of the subjects, while the 3D background robustly reconstructs scenes in real-world videos. Extensive experiments demonstrate that our method reconstructs scenes with better quality on various videos. 1.
--- INTRODUCTION ---
Video matting is the problem of separating a video into multiple layers with associated alpha mattes such that the layers are composited back to the original video. It has a wide variety of applications in video editing as it allows for substituting layers or processing them individually before compositing back, and thus has been studied well over decades. In typical applications like rotoscoping in video production and background blurring in online meetings, the goal is to obtain the masks containing only the object of interest. In many cases, however, it is often preferred to be able to create video mattes that include not only the object of interest but also its associated effects, like shadow and reflections. This could reduce the often-required, additional manual segmentation of secondary effects and help increase realism in the resulting edited video. Being able to factor out the related effects of foreground objects also helps reconstruct a clean background, which is preferred in (c) Our BG (d) Our FG Figure 1. Video with parallax effects. Limited by their 2D image representation (a), previous works such as Omnimatte fail to handle videos with parallax effects in the background. Their foreground layer (b) has to capture (dis)occlusion effects to minimize the reconstruction loss. In contrast, our method employs a 3D background (c), enabling us to obtain clean foreground layers (d). applications like object removal. Despite these benefits, this problem is much more ill-posed and has been much less explored than the conventional matting problem. The most promising attempt to tackle this problem is Omnimatte [21]. Omnimattes are RGBA layers that capture dynamic foreground objects and their associated effects. Given a video and one or more coarse mask videos, each corresponding to a foreground object of interest, the method reconstructs an omnimatte for each object, in addition to a static background that is free from all of the objects of interest and their associated effects. While Omnimatte [21] works well for many videos, it is limited by its use of homography to model backgrounds, which requires the background be planar or the video contains only rotational motion. This is not the case as long as there exists parallax caused by camera motions and objects occlude each other. This limitation hinders its application in many real-world videos, as shown in Fig. 1. D?NeRF [36] attempts to address this issue using two --- --radiance fields, which model the dynamic and static part of the scene. The method works entirely in 3D and can handle complicated scenes with significant camera motion. It is also self-supervised in the sense that no mask input is necessary. However, it separates all moving objects from a static background and it is not clear how to incorporate 2D guidance defined on video such as rough masks. Further, it cannot independently model multiple foreground objects. A simple solution of modeling each foreground object with a separate radiance field could lead to excessive training time, yet it is not clear how motions could be separated meaningfully in each radiance field. We propose a method that has the benefit of both by combining 2D foreground layers with a 3D background model. The lightweight 2D foreground layers can represent multiple object layers, including complicated objects, motions, and effects that may be challenging to be modeled in 3D. At the same time, modeling background in 3D enables handling background of complex geometry and non-rotational camera motions, allowing for processing a broader set of videos than 2D methods. We call this method OmnimatteRF and show in experiments that it works robustly on various videos without per-video parameter tuning. To quantitatively evaluate the background separation of a 3D scene, D?NeRF released a dataset of 5 videos rendered with Kubrics, which are simple indoor scenes with few pieces of furniture and some moving objects that cast solid shadows. We also render five videos from open-source Blender movies [6] with sophisticated motions and lighting conditions for more realistic and challenging settings. Our method outperforms prior works in both datasets, and we release the videos to facilitate future research. In summary, our contributions include the following: 1. We propose a novel method to make Omnimatte [21] more robust by better modeling the static background in 3D using radiance fields [22]. 2. Utilizing the omnimatte masks, we propose a simple yet effective re-training step to obtain a clean static 3D reconstruction from videos with moving subjects. 3. We release a new dataset of 5 challenging video sequences rendered from open-source blender movies [6] with ground truths to better facilitate the development and evaluation of the video matting with associated effects (aka omnimatting [21]) problem. 2.
--- RELATED WORK ---
Video Matting. There is a long line of work exploring video matting due to its importance in video editing. Green screening and rotoscoping are critical first steps in any visual effects pipeline. The matting problem aims to extract the foreground subjects into their own RGBA layers and separate them from the background RGB layer, which is a highly under-constrained problem. Many approaches have utilized motion and depth cues in addition to integrating user interactions [7, 3, 32, 16, 9]. Background Video Matting [18] specifically addresses real-time video matting of people and preserving strand-level hair details. Matting with Associated Effects. Video matting is often insufficient, as foreground subjects might have associated effects like shadows or reflections that need to be extracted into the foreground RGBA layers. This problem has not been explored as extensively and, in practice, is often dealt with manually using advanced interactive rotoscoping tools [15]. Omnimatte [21] was the first to propose a generic framework capable of learning any associated effect. Previous works often specifically addressed associated effects like shadows [34, 33]. The ability to obtain matte layers with associated effects has many exciting applications, such as re-timing motions of different people [20], consistent background editing [13, 14], background subtraction, green screening, and many other video effects [21]. Recently, FactorMatte [12] has been proposed to improve the quality with data augmentation and conditional priors. These works have in common that they take predefined masks that hint at the foreground objects and decompose each video into several layers, with one object in each layer with its associated effects. Then, there is a background layer, a 2D static image, or a deformable atlas shared by all the frames. The background is warped and cropped via a homography to render each frame. While the foreground layers have shown great potential in capturing dynamics, their single image background limits the application of these
--- METHOD ---
s like Omnimatte have been proposed to separate dynamic foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hindering application to real-world videos. In this paper, we propose a novel video matting method, OmnimatteRF, that combines dynamic 2D foreground layers and a 3D background model. The 2D layers preserve the details of the subjects, while the 3D background robustly reconstructs scenes in real-world videos. Extensive
--- EXPERIMENT ---
s demonstrate that our method reconstructs scenes with better quality on various videos. 1. Introduction Video matting is the problem of separating a video into multiple layers with associated alpha mattes such that the layers are composited back to the original video. It has a wide variety of applications in video editing as it allows for substituting layers or processing them individually before compositing back, and thus has been studied well over decades. In typical applications like rotoscoping in video production and background blurring in online meetings, the goal is to obtain the masks containing only the object of interest. In many cases, however, it is often preferred to be able to create video mattes that include not only the object of interest but also its associated effects, like shadow and reflections. This could reduce the often-required, additional manual segmentation of secondary effects and help increase realism in the resulting edited video. Being able to factor out the related effects of foreground objects also helps reconstruct a clean background, which is preferred in (c) Our BG (d) Our FG Figure 1. Video with parallax effects. Limited by their 2D image representation (a), previous works such as Omnimatte fail to handle videos with parallax effects in the background. Their foreground layer (b) has to capture (dis)occlusion effects to minimize the reconstruction loss. In contrast, our method employs a 3D background (c), enabling us to obtain clean foreground layers (d). applications like object removal. Despite these benefits, this problem is much more ill-posed and has been much less explored than the conventional matting problem. The most promising attempt to tackle this problem is Omnimatte [21]. Omnimattes are RGBA layers that capture dynamic foreground objects and their associated effects. Given a video and one or more coarse mask videos, each corresponding to a foreground object of interest, the method reconstructs an omnimatte for each object, in addition to a static background that is free from all of the objects of interest and their associated effects. While Omnimatte [21] works well for many videos, it is limited by its use of homography to model backgrounds, which requires the background be planar or the video contains only rotational motion. This is not the case as long as there exists parallax caused by camera motions and objects occlude each other. This limitation hinders its application in many real-world videos, as shown in Fig. 1. D?NeRF [36] attempts to address this issue using two --- --radiance fields, which model the dynamic and static part of the scene. The method works entirely in 3D and can handle complicated scenes with significant camera motion. It is also self-supervised in the sense that no mask input is necessary. However, it separates all moving objects from a static background and it is not clear how to incorporate 2D guidance defined on video such as rough masks. Further, it cannot independently model multiple foreground objects. A simple solution of modeling each foreground object with a separate radiance field could lead to excessive training time, yet it is not clear how motions could be separated meaningfully in each radiance field. We propose a method that has the benefit of both by combining 2D foreground layers with a 3D background model. The lightweight 2D foreground layers can represent multiple object layers, including complicated objects, motions, and effects that may be challenging to be modeled in 3D. At the same time, modeling background in 3D enables handling background of complex geometry and non-rotational camera motions, allowing for processing a broader set of videos than 2D methods. We call this method OmnimatteRF and show in experiments that it works robustly on various videos without per-video parameter tuning. To quantitatively evaluate the background separation of a 3D scene, D?NeRF released a dataset of 5 videos rendered with Kubrics, which are simple indoor scenes with few pieces of furniture and some moving objects that cast solid shadows. We also render five videos from open-source Blender movies [6] with sophisticated motions and lighting conditions for more realistic and challenging settings. Our method outperforms prior works in both datasets, and we release the videos to facilitate future research. In summary, our contributions include the following: 1. We propose a novel method to make Omnimatte [21] more robust by better modeling the static background in 3D using radiance fields [22]. 2. Utilizing the omnimatte masks, we propose a simple yet effective re-training step to obtain a clean static 3D reconstruction from videos with moving subjects. 3. We release a new dataset of 5 challenging video sequences rendered from open-source blender movies [6] with ground truths to better facilitate the development and evaluation of the video matting with associated effects (aka omnimatting [21]) problem. 2. Related Work Video Matting. There is a long line of work exploring video matting due to its importance in video editing. Green screening and rotoscoping are critical first steps in any visual effects pipeline. The matting problem aims to extract the foreground subjects into their own RGBA layers and separate them from the background RGB layer, which is a highly under-constrained problem. Many approaches have utilized motion and depth cues in addition to integrating user interactions [7, 3, 32, 16, 9]. Background Video Matting [18] specifically addresses real-time video matting of people and preserving strand-level hair details. Matting with Associated Effects. Video matting is often insufficient, as foreground subjects might have associated effects like shadows or reflections that need to be extracted into the foreground RGBA layers. This problem has not been explored as extensively and, in practice, is often dealt with manually using advanced interactive rotoscoping tools [15]. Omnimatte [21] was the first to propose a generic framework capable of learning any associated effect. Previous works often specifically addressed associated effects like shadows [34, 33]. The ability to obtain matte layers with associated effects has many exciting applications, such as re-timing motions of different people [20], consistent background editing [13, 14], background subtraction, green screening, and many other video effects [21]. Recently, FactorMatte [12] has been proposed to improve the quality with data augmentation and conditional priors. These works have in common that they take predefined masks that hint at the foreground objects and decompose each video into several layers, with one object in each layer with its associated effects. Then, there is a background layer, a 2D static image, or a deformable atlas shared by all the frames. The background is warped and cropped via a homography to render each frame. While the foreground layers have shown great potential in capturing dynamics, their single image background limits the application of these methods to videos with planar environments without parallax effects caused by camera motion. Radiance Fields. Radiance fields (RF) emerged as 3D representations capable of capturing geometric details and photorealistic appearances [22]. Radiance fields model the 3D scene as a continuous function that maps the position and the viewing direction of any point in world space to its color and opacity. Novel views can be synthesized via volume rendering along rays cast. This continuous function is learned by optimizing with a reconstruction loss on the rendered images. This view-dependent volumetric representation can model various challenging scenes that previous surface-based methods struggled to handle: e.g., shiny surfaces like metals or fuzzy surfaces like hair or fur. Since then, it has been extended along multiple axes: better appearance modeling (e.g., reflection and refraction [31, 5, 2, 1], faster optimization [8, 27, 23] and modeling dynamic scenes [38, 17, 10, 19]. Since the MLPbased implicit RF representations are slow to train, we use voxel-based explicit radiance field representations [8] [27]. --- --Specifically, we use the factorized voxel grid representation from [8]. Self-Supervised Video Dynamics Factoring. Another related work is video dynamics factoring without needing a predefined mask. One recent work is deformable sprites [39] that rely only on motion cues. Similar to other video matting works, it has a 2D foreground and background layers and the same limitations as Omnimatte. For modeling in 3D, D?NeRF[36] proposes to decouple the scene with two radiance fields, one for the dynamic content and the other for the statics. D?NeRF[36] handles a special case of matting with only one foreground object, and, compared to the other methods, it is not limited to planar backgrounds. However, the self-supervised method relies on the heuristics that require per-video hyper-parameter tuning and does not robustly generalize to new videos. The quality of the foreground reconstruction can also be limited for objects that have large nonrigid motions. We, therefore, propose a method for video matting with associated effects that has the advantages of supervised 2D mattes that support multiple individual objects with great details, as well as 3D background decoupling that works with non-planar videos. 3. Method The concept of omnimattes is proposed by Lu et al. [21], extending RGBA video mattes to capture associated effects of the objects of interest like shadows and reflections. To avoid any confusion, in the following text, we refer to their work as capital Omnimatte, and the resulting RGBA layers as italic omnimatte. In the matting setup, the user prepares a video of T frames {I,}7_,, and N ordered mask layers {Mj }4_,, each containing a coarse mask video of an object of interest. The video’s camera parameters are also precomputed as {P;}. The goal is to predict RGBA foreground layers C? and ai, that contain the objects together with their associated effects, and a background layer B, which is clean and free from the effects cast by the foreground objects. An input frame J, should be reconstructed by alpha compositing the foreground layers above the background. In Omnimatte, the background is represented by a static 2D image and a homography transform P;. To compose a frame, part of the static background is extracted according to the estimated homography P;. The key idea of our work is to represent the static background in 3D using a radiance field, while keeping the foreground in 2D to better capture the dynamics of objects. We employ an explicit factorized voxel-based radiance field [8] to model the background. In this case, P, represents a camera pose, and a background frame is rendered with volume rendering. Note that the foreground layers are still 2D videos. We refer to this combination as the OmnimatteRF model. 3.1. The OmnimatteRF Model An outline of our model is depicted in Figure 2. The model has two independent branches: foreground and background. For any given frame, the foreground branch predicts an RGBA image (omnimatte) for each object, and the background branch renders a single RGB image. Preprocessing. Following similar works, we use an offthe-shelf model RAFT [29] to predict optical flow between neighboring frames. The flow is used as an auxiliary input and ground truth for supervision, denoted by {F;}. We also use an off-the-shelf depth estimator MiDaS [26] to predict monocular depth maps {D,} for each frame and use them as ground truth for the monocular depth loss. Background. The background branch consists of a static neural radiance field, f,g, encoding the 3D representation of the scene. To render a pixel in a frame J;, a ray is traced according to the estimated camera pose P;, and the final RGB color is produced via volumetric rendering. The result of rendering the entire frame is (B;, Di) = fog(Pr), where B, is an RGB image and Dr isa depth map. Foreground. The foreground branch is a UNet-style convolutional neural network, ff, similar to that of Omnimatte. The input of the network is a concatenation of three maps: 1. The coarse mask M}. The mask is provided by the user, outlining the object of interest. Mask values are ones if the pixels are inside the object. 2. The optical flow F;. It provides the network with motion hints. Note that the network also predicts an optical flow as an auxiliary task (detailed in Sec. 3.2.2). 3. The feature map E;. Each pixel (x, y) in the feature map is the positional encoding of the 3-tuple (x, y, t). Multiple foreground layers are processed individually. For the i-th layer, the network predicts the omnimatte layer (Ci, ai) and the flow Fi. Detail Transfer. For a tradeoff between image quality and training time, the foreground network typically produces a color layer with missing details when the alpha layers have captured sufficient associated effects. To boost the output quality, Omnimatte transfers details from input frames. We include the same process in our pipeline. Note that this is a post-processing step to produce final results, and does not apply to model optimization. 3.2. Optimizing the Model We optimize an OmnimatteRF model for every video since both branches of our model are video-specific. To supervise learning, we employ an image reconstruction loss and several regularization losses. --- --fog Laepth_ Figure 2. Method overview. We propose a video matting method, named OmnimatteRF, which combines 2D foreground layers with a 3D ackground layer. The foreground branch (fig, in green box) predicts an RGBA layer (C?, a) for each object, and an auxiliary flow output (F}). The background branch (fog, in yellow box) produces a background layer with depths (Bz, Dy). Optimization. During training, predicted colors (J;) and flow (F;) are alpha-composited, whose inputs have red and green borders respectively. The right most column illustrates the data terms in the loss function, and we omit the regularization terms in this illustration. 3.2.1 Reconstruction Loss We compute the reconstruction loss with the composed image I, by alpha composition of foreground and background layers: N i-l N h=>> | [T[G-efoic? |} +]Ja-e)B a i=1 \j=i i=And the reconstruction loss is the mean-squared-error between the predicted and input frame, Leecons = |[fe — ||? (2) The reconstruction loss supervises both branches of our pipeline simultaneously. Limited by the computational cost of volumetric rendering, the background layer is rendered only at sparse random locations at each step, where Lrecons is computed for the composed pixel values. 3.2.2 Foreground Losses We follow Omnimatte and include the alpha regularization loss Lo-reg, alpha warp loss Lo-warp, and flow reconstruction loss Laow. We also bootstrap the initial alpha prediction to match the input mask with the mask loss Lmask, Which is gradually decayed and disabled once its value drops below the threshold. While most regularization terms in Omnimatte can be applied directly to our pipeline, the flow reconstruction loss is an exception. The formulation of the loss remains identical: given the per-layer flow prediction Fi and a background layer flows FP ©. the complete flow F, is composed via alpha composition (Eq. 1). Then, the loss is defined as: Laow = ||(Fi — Fi) @ ME|P (3) Here, M/* is the union of all foreground masks ({M;}) for the frame J;, and the loss is only evaluated at the location of input coarse masks. The authors of Omnimatte have shown the effectiveness of this loss in their case, and we also demonstrate its importance in an ablation study. However, it remains unclear how FP ® can be obtained. In Omnimatte, the background flow can be derived from image homography, which serves both as an input to the network and a background for composition. On the other hand, since our 3D background has only known camera poses but not depths, we cannot obtain background flows directly. Instead, we use the ground truth flow F; as network input to provide motion cues and a masked version of F, as background flow for composition. The masked flow is F™ = F, @ (1 — M®), which is the ground truth optical flow with the regions marked in the coarse masks set to zeros. ® denotes elementwise multiplication. We find it crucial to use F’” rather than F; for composition, as the latter case encourages the network to produce empty layers with a! equal to zero everywhere. --- --Figure 3. Background Layer Training Signals. We illustrate how the training signal to the background layer changes over time. It explains why the background captures some of the associated effects (in this example, shadows). We use the pixel circled in red as an example. (a) At the beginning of training, the foreground alpha value (in light green) does not include the shadow. Therefore, @ is small and at this pixel, ii(x,y) = Bi(x,y). The reconstruction loss Lrecons encourages the background network fig to produce dark prediction at this location from this viewing angle. (b) As training progresses, a gets larger in the shadow region, and I, (x,y) © Ci(a,y). This means that fog receives little to no supervision signals from this pixel. If it has modeled the shadow in some ways (in this case, a hole), it has little incentive to remove it, leaving the artifact in (c). 3.2.3 Background Losses Apart from the reconstruction loss, the background network is supervised by the total variation regularization loss, Lbge-reg, aS in TensoRF [8]. In addition, monocular depth supervision is used to improve scene reconstruction when the camera motions consist of rotation only: Laepth = metric(D;, D;), (4) where D, is the estimated depth from volume rendering [22], and the metric function is the scale-invariant loss from MiDaS [26]. Also, we empirically find that Caepin, can introduce floaters, and employ the distortion loss Laistort proposed in Mip-NeRF 360 [4] to reduce artifacts in the background. 3.2.4 Summary The combined loss for joint optimization is: L =Leecons + La-teg + La-warp + Laow + Lmask + a Foreground Log-reg + Laepth + Laistort See Background (5) At every optimization step, Lrecons and background losses are evaluated at sparse random locations. Foreground losses are computed for the full image. 3.3. Clean Background via Masked Retraining When the pipeline is trained jointly as described above, it is sometimes observed that the background radiance field models some of the foreground contents like shadows (see Fig. 3(c)). Compared to 2D images, 3D radiance fields are so much more capable that they can exploit distorted geometry constructs, such as holes and floaters, to capture some temporal effects, although the models are given no time information. For example, as the camera moves over time, there may be a correlation between whether a surface is covered by shadow and the direction the surface is viewed from. We illustrate this problem in Fig. 3 and explain the cause at an intuitive level. The foreground branch is bootstrapped to produce alpha values that match the coarse mask inputs, which include only the object without the associated effects. In other words, a, values are close to one at the object, but zero in the shadows (for simplicity, we consider one foreground layer in which the object casts a shadow, like in Fig. 3). Ata pixel (x,y) covered by shadow, Eq. | simply collapses to [i(x,y) * B,(x,y). The reconstruction loss will therefore encourage B,(x, y) to match the color of the shadow for a ray shot toward this location. As training proceeds, fjg will then gradually increase the predicted alpha values at the shadowed regions. If the shadow is hard and a gets close to one, Eq. | evaluates to I(x, y) © Ci(a,y), and the reconstruction loss gives little to no constraint to the background color at the pixel. As a result, fpg is unable to learn to remove the shadow color that it produces for the ray towards frame I; at (x,y). There are also cases where the shadow is soft and a is in between. In these cases, the problem remains ambiguous. Therefore, we propose to obtain clean background reconstruction via an optional optimization step. In joint training, the foreground omnimatte layers can capture most associated effects, including the parts with leaked content in the background layer. The alpha layers a; can then be used to train a radiance field model from scratch, with no samples from the foreground region where alpha values are high. We show in the ablation study (see Fig. 7) that this step produces cleaner background reconstruction for in-thewild videos. As only the background is optimized, the process is fast and takes less than an hour to complete. 4. Evaluation We compare our quantitative and qualitative methods with Omnimatte and D?NeRF [21, 36], which are stateof-the-art methods in 2D video matting and 3D video segmentation, respectively. In addition, we compare with Layered Neural Atlas (LNA) [13], which uses a deformable 2D background in contrast to Omnimatte’s static image. --- --4.1. The Movies Dataset Quantitative evaluation of background segmentation requires a dataset with both input videos and ground-truth background imagery. Prior works primarily use datasets like CDW-2014 [35], which are limited to mostly static backgrounds and are not applicable to our settings. Recently, Kubrics is proposed in D?NeRF, which enables the evaluation of 3D background synthesis. However, these videos have relatively simple scenes and lighting. To facilitate the evaluation of video matting and background segmentation in challenging scenarios, we select six clips from three Blender movies in Blender Studio [6]. Compared to Kubrics, they feature more complicated scenes and lighting conditions, large nonrigid motion of the characters, and higher resolution. To ensure usability, we manually edit the camera trajectories so that there are sufficient camera motions and the actors have reasonable sizes. We render the clips with and without the actors to obtain input and ground truth for background reconstruction evaluation purposes. The camera poses are also exported. 4.2. Experiment Setup We evaluate the performance of our proposed method on four datasets. 1. Movies: our novel challenging dataset. 2. Kubrics: the dataset generated and used in D?NeRF, which consists of five scenes of moving objects from 3D Warehouse [30] rendered with Kubric [1 1]. 3. DAVIS [24, 25]: short clips with moving foreground subjects, like humans, cars, and animals. This dataset is widely used to evaluate 2D-background matting methods [21, 13, 39]. 4. Wild: in-the-wild sequences collected from the internet that are closer to casually captured videos, with natural and noisier camera motions, including translations and rotations, as well as objects at different distances from the camera. Naturally, these videos have backgrounds that are challenging for pure 2D methods. Kubrics and Movies are synthetic datasets with clean background layer renderings available. Note that novel view synthesis is not the focus of our method, so we evaluate the background with input views. Both datasets have known camera poses and object masks which are used for training and evaluation. DAVIS and Wild are real-world videos without clean background. Therefore, we only perform a qualitative evaluation to demonstrate the robustness of our method. For videos in Wild we recover camera poses with COLMAP. For videos that COLMAP cannot process reliably, including DAVIS videos, we use poses from RoDynRF [19]. D?NeRF Omnimatte LNA Ours yy i La aEavy ae ie} ~*Figure 4. Background Reconstruction. We show examples of results presented in quantitative evaluations. For videos with parallax effects, 3D methods like D*?NeREF and ours reconstruct less distorted background than Omnimatte and LNA. lodge To obtain coarse object masks, we attempt to extract them with pre-trained object segmentation models from Detectron 2 [37]. In case it does not work, we use the Roto Brush tool in Adobe After Effects. Detailed procedures are described in the supplementary material. It takes aboutminutes of manual effort to produce a 200-frame mask. For all videos, we also estimate homographies with LoFTR [28] and OpenCV to enable Omnimatte processing. As mentioned in D2NeRF [36], the method is sensitive to hyperparameters. The authors released five sets of configurations for different videos. We experiment with every video using all provided configurations and report the bestperforming ones. 4.3. Implementation Details Our network is built upon the publicly available official implementation of Omnimatte [21], and TensoRF [8]. The videos in Kubrics have resolution 512 x 512, and all methods run at the resolution 256 x 256. For videos in other datasets with a higher resolution of 1920 x 1080, we downsample them by a factor of 4. We optimize the networks for up to 15,000 steps. The learning rate of fig is set to 0.001 and is exponentially decayed after 10,000 steps. For fg we use the learning rate scheduling scheme of TensoRF. Training takes up to 6 hours on a single RTX3090 GPU. Detailed network architecture, hyper-parameters and timing data are presented in the supplementary. Our code and datasets will also be made publicly available. 4.4. Quantitative Evaluation We quantitatively evaluate the background reconstruction quality of our method on two synthetic datasets. We --- --Kubries Car Cars Bag Chair Pillow LPIPS| SSIMt PSNRt| LPIPS| SSIMf PSNRT | LPIPS| SSIMt PSNRT | LPIPS| SSIMf PSNRt | LPIPS| SSIMt PSNRT D?NeRF | 0.135 0.854 34.10 | 0.105 0.859 34.77 | 0.131 0.880 33.98 | 0.090 0.916 33.29 | 0.105 0.926 38.Omnimatte | 0.162 0.819 31.14 | 0.157 0.834 31.20 | 0.271 0.796 = 23.64 | 0.175 = 0.865 26.91 | 0.270 0841 = 21.LNA - - - - - - 0.138 0.835 27.08 | 0.105 0.881 = 21.21 | 0.080 ~—0.923 31.Ours 0.033 0.958 39.09 0.032 0.961 39.78 0.029 0.972 39.58 0.023 0.977 42.46 0.022 0.982 43.Movies Donkey Dog Chicken Rooster Dodge LPIPS| SSIMt PSNRf | LPIPS| SSIMf PSNRf | LPIPS| SSIMf PSNRt | LPIPS| SSIMf PSNRT | LPIPS| SSIMt PSNRT D?NeRF - - - 0.370 = 0.694 22.73 - - - 0.340 0.708 =—-25.13 | 0.408 = 0.729 20.Omnimatte | 0.315 0.653. 19.11 0.279 0.706 =. 21.74 | 0.312 0.704 = 20.95 | 0.220 0.741 23.14 | 0.067 0.879 23.LNA 0.104 0.849 «18.79 | 0.154 0.828 26.08 | 0.190 0.818 19.22 | 0.131 0.804 26.46 | 0.068 0.937 24.Ours 0.005 = 0.990 38.24 | 0.030 0.976 31.44 | 0.021 0.978 ~— 32.86 | 0.024 0.969 «27.65 | 0.006 0.991 39.Table 1. Quantitative evaluations. We present the background reconstruction comparison of our method and baselines on the Kubrics and Movies datasets. Best results are in bold and second place are underlined. Results marked - are the ones the method failed to give good separations (visuals in supplementary). report PSNR, SSIM and LPIPS for all videos in Table 1, and some visualizations in Fig. 4. For D?NeRF, we tried every provided pre-set configuration for every video in Movies, and it only gave good results for the Dog, Rooster, and Dodge videos. Omnimatte and LNA with the 2D background layers struggles in both datasets. Our method can handle these videos well. 4.5. Qualitative Evaluation We present a qualitative comparison of the methods in Fig. 5. Due to space limitations, we present at least one video from every dataset but show a frame from every selected video in the figure. The original videos are available in supplementary and we highly recommend watching them. D?NeRF works well for the fine-tuned videos but not for new inputs without further hyper-parameter tuning. Omnimatte background has significant distortion around objects, and its foreground layer has to compensate for the limitation by capturing all residuals. Our method is versatile enough to perform well for a variety of videos with our 3D background model. 4.6. Ablation Studies 4.6.1 Loss Terms We present background reconstruction results without Leepin in Fig. 6. For video sequences with rotational camera poses, the model struggles to extract 3D information from the input videos because of a lack of 3D clues. This loss is critical to extending our method to a broader range of videos. The effects of Laow are also demonstrated in Fig. 6. The auxiliary task improves foreground quality and reduces unrelated content. 4.6.2 Clean Background Retraining We employ an additional step for real-world sequences to optimize a clean background from scratch. In Fig. 7, we compare the background layer from the initial joint optimization and the final result. This is a simple yet robust way to obtain a better background. 4.7. Limitations We list some limitations that future works can explore. 1. If a background region is covered by shadows nearly all of the time, the background model cannot recover its color correctly. An example from a Movies video is shown in Fig. 8. In theory, an omnimatte layer has an alpha channel and can capture only the additive shadow that allows the background to have the original color. However, this problem is largely underconstrained in the current setting, making it ambiguous and leading the background to unsatisfying solutions. 2. The foreground layer captures irrelevant content. In real-world videos, unrelated motions often exist in the background, like swaying trees and moving cars. These effects cannot be modeled by the static radiance field and will be captured by the foreground layer regardless of their association with the object. Possible directions include i) using a dummy 2D layer to catch such content or ii) a deformable 3D background model with additional regularization to address the ambiguity as both background and foreground can model motion. 3. Foreground objects may have missing parts in the omnimatte layers if they’re occluded. Since our foreground network predicts pixel values for alpha composition, it does not always hallucinate the occluded parts. 4. The video resolution is limited. This is primarily due to the U-Net architecture of the foreground model inherited from Omnimatte. Higher resolutions can potentially be supported with the use of other lightweight image encoders. 5. The foreground layer may capture different content when the weights are randomly initialized differently. We include visual results in the supplementary materials. 5.
--- CONCLUSION ---
We propose a method to obtain omnimattes, RGBA layers that include objects and their associated effects by com --- --Background Foreground D?NeRF Omnimatte ku-pillow rooster Ours D?NeRF Omnimatte Ours at atatatatatatat staat ata"" at stata"" ata""a""ata""sh re Geer pesssuncesssscelbamcessamceasamsoramrnorsrad afetatahataterat wretateratararat aanaraneraena"" | potters bouldering car Figure 5. Qualitative comparison. We compare results of our and baseline methods on videos from each dataset. Readers are . strongly encouraged to view videos files of more sequences available in the supplementary. The first two videos are synthetic from Kubrics and Movies, followed by three Wild videos. Omnimatte fails to handle objects in 3D and produces distorted background. D?NeRF works for videos with appropriate hyper-parameters, but does not generalize to new videos easily. Our method handles videos in many different settings. Due to space constraint we defer LNA results to the supplementary. Input without Laeptn ‘ with Laeptn ‘. g Input ane lvithout rest with Leow iene bisiessnsesececerersansesed eee Secceein pect tatetatatetatatneatet te sents aeseatanctatat hat lloatesnatancsatet rosters Figure 6. Loss Term Ablations. Background of real-world videos without Laepn and foreground without Low can de degraded for real-world videos. bining 2D foreground layers and a 3D background model. Extensive experiments demonstrate that our approach is ap (a) Coarse mask (b) et (c) Depths (d) Learned mask (e) Background (f) Depths Figure 7. Clean Background Retraining. Background layers jointly trained can capture the shadows as a hole on the ground (a-c). After the joint training, the foreground omnimatte provides a better mask that can be used to train a clean background (d-f). plicable to a wide variety of videos, expanding beyond the capabilities of previous methods. --- --(a) (b) (c) Figure 8. Limitations. (a), (b): Background can have baked-in shadows when the region is covered by shadows for most frames of the video. (c): The foreground layer captures irrelevant object motions (middle left) in the background. Best viewed in videos. References {1] Benjamin Attal, Jia~-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O’Toole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with rayconditioned sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.Benjamin Attal, Jia~-Bin Huang, Michael Zollh6fer, Johannes Kopf, and Changil Kim. Learning neural light fields with ray-space embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.Xue Bai, Jue Wang, David Simons, and Guillermo Sapiro. Video snapcut: robust video object cutout using localized classifiers. ACM Trans. Graph., 28(3):70, 2009.Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, and Tobias Ritschel. Eikonal fields for refractive novel-view synthesis. In Munkhtsetseg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann, editors, SIGGRAPH ’22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, pages 39:1-39:9. ACM, 2022.Institute Blender, Oct 2022. 2,Gabriel J. Brostow and Irfan A. Essa. Motion based decompositing of video. In Proceedings of the International Conference on Computer Vision, Kerkyra, Corfu, Greece, September 20-25, 1999, pages 8-13. IEEE Computer Society, 1999.Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision (ECCV), 2022. 2, 3,5, 6,Yung-Yu Chuang, Aseem Agarwala, Brian Curless, David Salesin, and Richard Szeliski. Video matting of complex scenes. ACM Trans. Graph., 21(3):243-248, 2002.Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi (Derek) Liu, Henning Meyer, Yishu Miao, DerekNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable dataset generator. 2022.Zeqi Gu, Wenqi Xian, Noah Snavely, and Abe Davis. Factormatte: Redefining video matting for re-composition tasks, 2022.Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):1—12, 2021. 2, 5, 6,Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware text-driven layered video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.Wenbin Li, Fabio Viola, Jonathan Starck, Gabriel J. Brostow, and Neill D.F. Campbell. Roto++: Accelerating professional rotoscoping using shape manifolds. ACM Transactions on Graphics (In proceeding of ACM SIGGRAPH’ 16), 35(4), 2016.Yin Li, Jian Sun, and Heung- Yeung Shum. Video object cut and paste. ACM Trans. Graph., 24(3):595-600, 2005.Zhenggqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In JEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 1925, 2021, pages 6498-6508. Computer Vision Foundation / IEEE, 2021.Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, and Ira KemelmacherShlizerman. Real-time high-resolution background matting. arXiv, pages arXiv—2012, 2020.Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2,Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. In SIGGRAPH Asia, 2020.Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T Freeman, and Michael Rubinstein. Omnimatte: Associating objects and their effects in video. In CVPR, 2021. 1, 2, 3,5, 6,Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2,5,Thomas Miiller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1102:15, 2022.F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and --- ---evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016.Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeldez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017.René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022. 3,Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022.Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.Zachary Teed and Jia Deng. RAFT: recurrent all-pairs field transforms for optical flow. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II, volume 12347 of Lecture Notes in Computer Science, pages 402-419. Springer, 2020.Inc Trimble, Oct 2022.Dor Verbin, Peter Hedman, Ben Mildenhall, Todd E. Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 1824, 2022, pages 5481-5490. IEEE, 2022.Jue Wang, Pravin Bhat, Alex Colburn, Maneesh Agrawala, and Michael F. Cohen. Interactive video cutout. ACM Trans. Graph., 24(3):585-594, 2005.Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann Heng. Single-stage instance shadow detection with bidirectional relation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1-11, June 2021.Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu. Instance shadow detection. In JIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.Yi Wang, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, Yannick Benezeth, and Prakash Ishwar. Cdnet 2014: An expanded change detection benchmark dataset. In 20/IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 393-400, 2014.Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D?nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. In Advances in Neural Information Processing Systems, 2022. 1, 3,5, 6,Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 6,Wendi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 9421-9431. Computer Vision Foundation / IEEE, 2021.Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised video decomposition. In JEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. 3,--- --A. Additional Qualitative Results Video files of results presented in the main paper (all videos from the Movies, Kubrics, Wild, and DAVIS datasets) are available on our project website as part of the supplementary material. We highly recommend watching them on: https: //omnimatte-rf.github.io 1. For our method (OmnimatteRF), we include results (inputs with masks, foreground layers, background layer, background depth map) for every video. 2. For D?NeRF [36], we use the best result among all configurations provided by the authors for every video. If none of the configurations successfully reconstruct non-empty static and dynamic layers, we drop the video files and only show a frame in Fig. A2. 3. For Omnimatte [21] and Layered Neural Atlas (LNA) [13], we include videos from Wild, Movies, and Kubrics. Results of DAVIS can be found in prior works. B. Random Initialization As is also discussed in Omnimatte [21], different random initializations can lead to varying results of the foreground layers. We show two examples in Fig. Al. In all our experiments, the random seed is set to 3. De ee ee ee BEEBE EEE Seed =Seed = 3 Seed =Seed =3 Seed = 1 Seed =Figure Al. Effect of random initialization. Top: for the Wild/dogwalk video, different seeds lead to different amount of hallucinated shadow of the person. Bottom: for the Kubrics/cars video, seeds influence how shadows are associated to the objects. C. Additional Implementation Details C.1. Mask Generation Our method and Omnimatte relies on coarse mask videos that outlines every object of interest. The synthetic Kubrics and Movies videos have ground truth object masks and we use them directly. To obtan mask for an inthe-wild video, we use one of the two workflows: 1. We first process the video a the pretrained Mask RCNN model (X101-FPN) from Detectron 2 [37]. Then, we manually select a mask in every frame that best capture the object. 2. We use the Roto Brush tool in Adobe After Effects to track the object. This method is useful when Mask R-CNN fails produce good masks for a video. In particular, we processed Wi ld/dance and Wild/solo manually. It takes about 10 minutes of manual work to generate a mask sequence for a 200-frame video. C.2. Network Architecture Our foreground network is based on the U-Net architecture of Omnimatte, which is detailed in their supplementary [21]. To adopt their network to OmnimatteRF, we replace the background noise input by the 2D feature map F;. Each pixel in E; is the positional encoding of the 3D vector (x, y,t) where (x, y) is the pixel location and t is the frame number. The positional encoding scheme is the same as proposed in NeRF [22], with L = 10 frequencies. For background, we use the Vector-Matrix decomposition model in TensoRF [8] with the MLP feature decoder. Our initial grid has the same resolution No = 128, and the final grid is limited to N = 640. The vectors and matrices are upsampled at steps 2000, 3000, 4000, 5500. C.3. Hyper-parameters For all videos, we use a learning rate of 0.001 for the foreground network, which is exponentially decayed from the 10,000 step at a rate of 0.1 per 10,000 steps. We find the decay crucial in preventing the foreground training from diverging. The mask bootstrapping loss Lmask has an initial weight of 50, which is first reduced to 5 when the loss value (before weighting) drops to below 0.02, and then turned off after the same number of steps. We document weights of other loss terms in Table Al. Background network learning rate scheduling and Log -reg weight are identical as the original TensoRF [8]. In general, we use the same set of hyper-parameters for most videos, and only add additional terms when artifacts are observed. --- --Video Steps Lrecons La-teg La-warp Liow Laepth — Laistort All 15,000 1 0.01 (L1) / 0.005 (LO) 0.01 1 0Wild/bouldering - - - - - 0.1 0.DAVIS 10,000 - - - - 1Table Al. Hyper-parameters. We document the hyper-parameters (number of steps and weights of loss terms) in our experiments. The first row is the configuration shared by most videos. Remaining rows are videos with different configurations, and - means unchanged from the shared number. chicken Figure A2. D?NeRF results for failed scenes. Method Steps Training (hours) Rendering (s/image) Omnimatte 12,000 2.7 2.D?NeRF 100,000 45LNA 400,000 8.5 0.Ours 15,000 3.8 3.Omnimatte 12,000 1.2 0.D?NeRF 100,000 3.2 2.LNA 400,000 8.5 0.Ours 15,000 2.5 1.Table A2. Running Time Measurement. We measure and compare the time it takes to train OmnimatteRF and baseline methods. Top: Movies, Wild (480 x 270, DAVIS has a similar resolution of 428 x 240). Bottom: Kubrics (256 x 256). C.4. Running Time Measurement We measure and report the time it takes to train OmnimatteRF and baseline methods in Table A2. All measurements are conducted on a workstation with an eightcore AMD R7-2700X CPU and a single NVIDIA RTXGPU. Our method takes longer to train than Omnimatte due to the addition of the 3D background radiance field. Optimizing the background model only, as in the clean background retraining process, takes about 30 minutes per video.
"	"--- ABSTRACT ---
비디오 매팅은 흥미로운 효과를 캐주얼하게 촬영한 영화에 추가하는 것부터 비디오 제작 전문가를 지원하는 것까지 광범위한 응용 분야가 있습니다. 그림자와 반사와 같은 관련 효과가 있는 매팅도 연구 활동을 증가시켰으며, Omnimatte와 같은 방법이 관심 있는 동적 전경 객체를 자체 레이어로 분리하는 데 제안되었습니다. 그러나 이전 연구에서는 비디오 배경을 2D 이미지 레이어로 표현하여 더 복잡한 장면을 표현하는 용량이 제한되어 실제 비디오에 적용하는 데 방해가 되었습니다. 이 논문에서는 동적 2D 전경 레이어와 3D 배경 모델을 결합하는 새로운 비디오 매팅 방법인 OmnimatteRF를 제안합니다. 2D 레이어는 피사체의 세부 정보를 보존하는 반면 3D 배경은 실제 비디오의 장면을 견고하게 재구성합니다. 광범위한 실험을 통해 우리 방법이 다양한 비디오에서 더 나은 품질로 장면을 재구성한다는 것이 입증되었습니다. (a) Omnimatte BG (b) Omnimatte FG (c) 우리의 BG (d) 우리의 FG 그림 1. 시차 효과가 있는 비디오. 2D 이미지 표현(a)에 의해 제한되어, Omnimatte와 같은 이전 작업들은 배경에 시차 효과가 있는 비디오를 처리하지 못했습니다. 전경 레이어(b)는 재구성 손실을 최소화하기 위해 (해제) 효과를 포착해야 합니다. 반면에, 우리의 방법은 3D 배경(c)을 사용하여 깨끗한 전경 레이어(d)를 얻을 수 있습니다. 1.
--- INTRODUCTION ---
비디오 매팅은 비디오를 여러 레이어로 분리하고 연관된 알파 매트를 사용하여 레이어를 다시 합성하여 원본 비디오로 되돌리는 문제입니다. 레이어를 대체하거나 다시 합성하기 전에 개별적으로 처리할 수 있으므로 비디오 편집에서 다양한 용도로 사용되며 수십 년 동안 연구되어 왔습니다. 비디오 제작의 로토스코핑 및 온라인 회의의 배경 흐림과 같은 일반적인 응용 프로그램에서 목표는 관심 있는 객체만 포함하는 마스크를 얻는 것입니다. 그러나 많은 경우 관심 있는 객체뿐만 아니라 그림자 및 반사와 같은 관련 효과도 포함하는 비디오 매트를 만드는 것이 선호됩니다. 이를 통해 종종 필요한 보조 효과의 추가 수동 분할을 줄이고 편집된 결과 비디오의 사실성을 높이는 데 도움이 될 수 있습니다. 전경 객체의 관련 효과를 요인 제거할 수 있으면 깨끗한 배경을 재구성하는 데 도움이 되며 이는 객체 제거와 같은 응용 프로그램에서 선호됩니다. 이러한 이점에도 불구하고 이 문제는 훨씬 더 잘못 제기되었으며 기존 매팅 문제보다 훨씬 덜 탐구되었습니다. 이 문제를 해결하기 위한 가장 유망한 시도는 Omnimatte[21]입니다. Omnimatte는 동적 전경 객체와 관련 효과를 캡처하는 RGBA 레이어입니다. 비디오와 관심 있는 전경 객체에 각각 해당하는 하나 이상의 거친 마스크 비디오가 주어지면 이 방법은 관심 있는 모든 객체와 관련 효과가 없는 정적 배경 외에도 각 객체에 대한 omnimatte를 재구성합니다. Omnimatte[21]는 많은 비디오에 잘 작동하지만 배경을 모델링하는 데 호모그래피를 사용해야 한다는 제한이 있습니다. 이를 위해서는 배경이 평면이거나 비디오에 회전 동작만 포함되어야 합니다. 카메라 동작으로 인해 시차가 발생하고 객체가 서로를 가리는 한 이는 해당되지 않습니다. 이러한 제한은 그림 1에서 볼 수 있듯이 많은 실제 비디오에 적용하는 데 방해가 됩니다. D2NERF[36]는 장면의 동적 및 정적 부분을 모델링하는 두 개의 광도장을 사용하여 이 문제를 해결하려고 합니다. 이 방법은 전적으로 3D에서 작동하며 상당한 카메라 동작이 있는 복잡한 장면을 처리할 수 있습니다. 또한 마스크 입력이 필요 없다는 의미에서 자체 감독 방식입니다. 그러나 모든 움직이는 객체를 정적 배경에서 분리하고 거친 마스크와 같은 비디오에 정의된 2D 안내를 통합하는 방법이 명확하지 않습니다. 또한 여러 전경 객체를 독립적으로 모델링할 수 없습니다. 각 전경 객체를 별도의 광도 필드로 모델링하는 간단한 솔루션은 과도한 학습 시간을 초래할 수 있지만 각 광도 필드에서 동작을 의미 있게 분리하는 방법은 명확하지 않습니다. 2D 전경 레이어와 3D 배경 모델을 결합하여 두 가지 이점을 모두 제공하는 방법을 제안합니다. 가벼운 2D 전경 레이어는 복잡한 객체, 동작 및 3D로 모델링하기 어려울 수 있는 효과를 포함하여 여러 객체 레이어를 나타낼 수 있습니다. 동시에 3D로 배경을 모델링하면 복잡한 기하학 및 비회전 카메라 동작의 배경을 처리할 수 있으므로 2D 방법보다 더 광범위한 비디오 세트를 처리할 수 있습니다. 이 방법을 OmnimatteRF라고 하며 실험을 통해 비디오별 매개변수 조정 없이 다양한 비디오에서 견고하게 작동한다는 것을 보여줍니다. D²NeRF는 3D 장면의 배경 분리를 정량적으로 평가하기 위해 Kubrics로 렌더링한 5개 비디오 데이터 세트를 공개했습니다.이것은 몇 개의 가구와 견고한 그림자를 드리우는 움직이는 물체가 있는 간단한 실내 장면입니다.또한 오픈 소스 Blender 영화[6]에서 더욱 사실적이고 도전적인 설정을 위한 정교한 동작과 조명 조건으로 5개 비디오를 렌더링합니다.저희의 방법은 두 데이터 세트 모두에서 이전 연구보다 우수한 성과를 거두었으며 향후 연구를 용이하게 하기 위해 비디오를 공개합니다.요약하면 저희의 기여는 다음과 같습니다.1. 저희는 광도장[22]을 사용하여 3D에서 정적 배경을 더 잘 모델링하여 Omnimatte[21]를 더욱 견고하게 만드는 새로운 방법을 제안합니다.2. Omnimatte 마스크를 활용하여 움직이는 피사체가 있는 비디오에서 깨끗한 정적 3D 재구성을 얻기 위한 간단하면서도 효과적인 재교육 단계를 제안합니다. 3. 우리는 오픈소스 블렌더 영화[6]에서 렌더링된 5개의 까다로운 비디오 시퀀스의 새로운 데이터 세트를 공개하여 관련 효과가 있는 비디오 매팅(일명 옴니매팅[21]) 문제의 개발 및 평가를 보다 용이하게 합니다. 2.
--- RELATED WORK ---
비디오 매팅. 비디오 편집에서 비디오 매팅이 중요하기 때문에 이를 탐구하는 작업이 많이 있습니다. 그린 스크리닝과 로토스코핑은 모든 시각 효과 파이프라인에서 중요한 첫 단계입니다. 매팅 문제는 전경 피사체를 자체 RGBA 레이어로 추출하여 배경 RGB 레이어에서 분리하는 것을 목표로 하는데, 이는 매우 제약이 적은 문제입니다. 많은 접근 방식에서 사용자 상호 작용을 통합하는 것 외에도 동작 및 깊이 신호를 활용했습니다[7, 3, 32, 16, 9]. 배경 비디오 매팅[18]은 특히 사람의 실시간 비디오 매팅과 가닥 수준의 머리카락 세부 정보를 보존하는 것을 다룹니다. 연관 효과가 있는 매팅. 전경 피사체에 그림자나 반사와 같은 연관 효과가 있을 수 있으므로 비디오 매팅만으로는 종종 충분하지 않으며, 이를 전경 RGBA 레이어로 추출해야 합니다. 이 문제는 광범위하게 탐구되지 않았으며 실제로는 고급 대화형 로토스코핑 도구를 사용하여 수동으로 처리하는 경우가 많습니다[15]. Omnimatte[21]는 모든 관련 효과를 학습할 수 있는 일반 프레임워크를 제안한 최초의 연구입니다. 이전 연구에서는 종종 그림자[34, 33]와 같은 관련 효과를 구체적으로 다루었습니다. 관련 효과가 있는 매트 레이어를 얻는 기능은 다양한 사람의 동작 재타이밍[20], 일관된 배경 편집[13, 14], 배경 뺄셈, 그린 스크리닝 및 기타 여러 비디오 효과[21]와 같은 많은 흥미로운 응용 프로그램을 가지고 있습니다. 최근 FactorMatte[12]는 데이터 증가 및 조건부 사전 확률로 품질을 개선하기 위해 제안되었습니다. 이러한 연구는 전경 객체를 암시하는 미리 정의된 마스크를 사용하고 각 비디오를 여러 레이어로 분해한다는 점에서 공통점이 있으며, 각 레이어에는 관련 효과가 있는 하나의 객체가 있습니다. 그런 다음 모든 프레임에서 공유하는 배경 레이어, 2D 정적 이미지 또는 변형 가능한 아틀라스가 있습니다. 배경은 각 프레임을 렌더링하기 위해 호모그래피를 통해 워핑되고 잘립니다. 전경 레이어는 역학을 포착하는 데 큰 잠재력을 보여주었지만 단일 이미지 배경으로 인해 이러한
--- METHOD ---
Omnimatte와 같은 s는 관심 있는 동적 전경 객체를 자체 레이어로 분리하기 위해 제안되었습니다. 그러나 이전 작업에서는 비디오 배경을 2D 이미지 레이어로 표현하여 더 복잡한 장면을 표현하는 용량이 제한되어 실제 비디오에 적용하는 데 방해가 되었습니다. 이 논문에서는 동적 2D 전경 레이어와 3D 배경 모델을 결합하는 새로운 비디오 매팅 방법인 OmnimatteRF를 제안합니다. 2D 레이어는 피사체의 세부 정보를 보존하는 반면 3D 배경은 실제 비디오의 장면을 견고하게 재구성합니다. 광범위한
--- EXPERIMENT ---
s는 우리 방법이 다양한 비디오에서 더 나은 품질로 장면을 재구성한다는 것을 보여줍니다.(a) Omnimatte BG (b) Omnimatte FG (c) 우리 BG (d) 우리 FG 그림 1. 시차 효과가 있는 비디오. 2D 이미지 표현(a)에 의해 제한되어 Omnimatte와 같은 이전 작업은 배경에 시차 효과가 있는 비디오를 처리하지 못했습니다. 전경 레이어(b)는 재구성 손실을 최소화하기 위해 (해제) 효과를 캡처해야 합니다. 반면에 우리 방법은 3D 배경(c)을 사용하여 깨끗한 전경 레이어(d)를 얻을 수 있습니다. 1. 소개 비디오 매팅은 비디오를 연관된 알파 매트가 있는 여러 레이어로 분리하여 레이어를 원본 비디오로 다시 합성하는 문제입니다. 레이어를 대체하거나 다시 합성하기 전에 개별적으로 처리할 수 있으므로 비디오 편집에서 다양한 응용 프로그램이 있으며 수십 년 동안 연구되어 왔습니다. 비디오 제작의 로토스코핑 및 온라인 회의의 배경 흐림과 같은 일반적인 응용 프로그램에서 목표는 관심 객체만 포함된 마스크를 얻는 것입니다.그러나 많은 경우 관심 객체뿐만 아니라 그림자 및 반사와 같은 관련 효과도 포함하는 비디오 매트를 만들 수 있는 것이 선호됩니다.이렇게 하면 종종 필요한 보조 효과의 추가 수동 분할을 줄이고 편집된 결과 비디오의 사실감을 높이는 데 도움이 될 수 있습니다.전경 객체의 관련 효과를 요인 제거할 수 있으면 깨끗한 배경을 재구성하는 데 도움이 되며 이는 객체 제거와 같은 응용 프로그램에서 선호됩니다.이러한 이점에도 불구하고 이 문제는 훨씬 더 잘못 제기되었으며 기존 매팅 문제보다 훨씬 덜 탐구되었습니다.이 문제를 해결하기 위한 가장 유망한 시도는 Omnimatte[21]입니다.Omnimatte는 동적 전경 객체와 관련 효과를 캡처하는 RGBA 레이어입니다. 주어진 비디오와 관심 있는 전경 객체에 각각 해당하는 하나 이상의 거친 마스크 비디오가 주어지면, 이 방법은 관심 있는 모든 객체와 관련 효과가 없는 정적 배경 외에도 각 객체에 대한 옴니매트를 재구성합니다.옴니매트[21]는 많은 비디오에 잘 작동하지만 배경을 모델링하는 데 호모그래피를 사용해야 한다는 제한이 있습니다.이를 위해서는 배경이 평면이거나 비디오에 회전 동작만 포함되어야 합니다.카메라 동작으로 인해 시차가 발생하고 객체가 서로 가려지는 한 이는 해당되지 않습니다.이러한 제한은 그림 1에서 볼 수 있듯이 많은 실제 비디오에 적용하는 데 방해가 됩니다.D2NERF[36]는 장면의 동적 및 정적 부분을 모델링하는 두 개의 광도장을 사용하여 이 문제를 해결하려고 합니다.이 방법은 전적으로 3D에서 작동하며 상당한 카메라 동작이 있는 복잡한 장면을 처리할 수 있습니다.또한 마스크 입력이 필요 없다는 의미에서 자체 감독 방식입니다. 그러나 모든 움직이는 객체를 정적 배경에서 분리하고 거친 마스크와 같은 비디오에 정의된 2D 안내를 통합하는 방법이 명확하지 않습니다. 또한 여러 전경 객체를 독립적으로 모델링할 수 없습니다. 각 전경 객체를 별도의 광도 필드로 모델링하는 간단한 솔루션은 과도한 학습 시간을 초래할 수 있지만 각 광도 필드에서 동작을 의미 있게 분리하는 방법은 명확하지 않습니다. 2D 전경 레이어와 3D 배경 모델을 결합하여 두 가지 이점을 모두 제공하는 방법을 제안합니다. 가벼운 2D 전경 레이어는 복잡한 객체, 동작 및 3D로 모델링하기 어려울 수 있는 효과를 포함하여 여러 객체 레이어를 나타낼 수 있습니다. 동시에 3D로 배경을 모델링하면 복잡한 기하학의 배경과 회전하지 않는 카메라 동작을 처리할 수 있으므로 2D 방법보다 더 광범위한 비디오 세트를 처리할 수 있습니다. 이 방법을 OmnimatteRF라고 하며 실험을 통해 비디오별 매개변수 조정 없이 다양한 비디오에서 견고하게 작동한다는 것을 보여줍니다. D²NeRF는 3D 장면의 배경 분리를 정량적으로 평가하기 위해 Kubrics로 렌더링한 5개 비디오 데이터 세트를 공개했습니다.이것은 몇 개의 가구와 견고한 그림자를 드리우는 움직이는 물체가 있는 간단한 실내 장면입니다.또한 오픈 소스 Blender 영화[6]에서 더욱 사실적이고 도전적인 설정을 위한 정교한 동작과 조명 조건으로 5개 비디오를 렌더링합니다.저희의 방법은 두 데이터 세트 모두에서 이전 연구보다 우수한 성과를 거두었으며 향후 연구를 용이하게 하기 위해 비디오를 공개합니다.요약하면 저희의 기여는 다음과 같습니다.1. 저희는 광도장[22]을 사용하여 3D에서 정적 배경을 더 잘 모델링하여 Omnimatte[21]를 더욱 견고하게 만드는 새로운 방법을 제안합니다.2. Omnimatte 마스크를 활용하여 움직이는 피사체가 있는 비디오에서 깨끗한 정적 3D 재구성을 얻기 위한 간단하면서도 효과적인 재교육 단계를 제안합니다. 3. 오픈소스 블렌더 영화[6]에서 렌더링한 까다로운 비디오 시퀀스 5개의 새로운 데이터 세트를 공개하여 관련 효과가 있는 비디오 매팅(일명 옴니매팅[21]) 문제의 개발 및 평가를 보다 용이하게 합니다.2. 관련 연구 비디오 매팅.비디오 편집에서 중요성 때문에 비디오 매팅을 탐구하는 연구가 많이 있습니다.그린 스크리닝과 로토스코핑은 모든 시각 효과 파이프라인에서 중요한 첫 단계입니다.매팅 문제는 전경 피사체를 자체 RGBA 레이어로 추출하여 배경 RGB 레이어에서 분리하는 것을 목표로 하며, 이는 매우 제약이 적은 문제입니다.많은 접근 방식에서 사용자 상호 작용을 통합하는 것 외에도 동작 및 깊이 신호를 활용했습니다[7, 3, 32, 16, 9].배경 비디오 매팅[18]은 특히 사람의 실시간 비디오 매팅과 가닥 수준의 머리카락 세부 정보를 보존하는 것을 다룹니다.관련 효과가 있는 매팅. 비디오 매팅은 종종 충분하지 않은데, 전경 피사체에 그림자나 반사와 같은 연관 효과가 있을 수 있고 이를 전경 RGBA 레이어로 추출해야 하기 때문입니다. 이 문제는 광범위하게 탐구되지 않았으며 실제로는 종종 고급 대화형 로토스코핑 도구[15]를 사용하여 수동으로 처리합니다. Omnimatte[21]는 모든 연관 효과를 학습할 수 있는 일반 프레임워크를 제안한 최초의 회사입니다. 이전 연구에서는 종종 그림자와 같은 연관 효과를 구체적으로 다루었습니다[34, 33]. 연관 효과가 있는 매트 레이어를 얻는 기능은 다양한 사람의 동작 재타이밍[20], 일관된 배경 편집[13, 14], 배경 뺄셈, 그린 스크리닝 및 기타 여러 비디오 효과[21]와 같은 많은 흥미로운 응용 분야가 있습니다. 최근 FactorMatte[12]는 데이터 증가 및 조건부 사전 확률로 품질을 개선하기 위해 제안되었습니다. 이러한 작업은 전경 객체를 암시하는 미리 정의된 마스크를 사용하고 각 비디오를 여러 레이어로 분해하며 각 레이어에 하나의 객체와 연관 효과를 둡니다. 그런 다음 모든 프레임에서 공유하는 배경 레이어, 2D 정적 이미지 또는 변형 가능한 아틀라스가 있습니다. 배경은 각 프레임을 렌더링하기 위해 호모그래피를 통해 워프되고 잘립니다. 전경 레이어는 역학을 포착하는 데 큰 잠재력을 보였지만 단일 이미지 배경으로 인해 카메라 동작으로 인한 시차 효과가 없는 평면 환경의 비디오에 이러한 방법을 적용하는 것이 제한됩니다. 광도 필드. 광도 필드(RF)는 기하학적 세부 사항과 사실적인 모습을 포착할 수 있는 3D 표현으로 등장했습니다[22]. 광도 필드는 3D 장면을 세계 공간의 모든 지점의 위치와 시야 방향을 색상과 불투명도에 매핑하는 연속 함수로 모델링합니다. 광선을 따라 볼륨 렌더링을 통해 새로운 뷰를 합성할 수 있습니다. 이 연속 함수는 렌더링된 이미지에서 재구성 손실을 사용하여 최적화하여 학습합니다. 이 뷰 종속 볼륨 표현은 이전의 표면 기반 방법에서는 처리하기 어려웠던 다양한 어려운 장면을 모델링할 수 있습니다. 예를 들어 금속과 같은 반짝이는 표면이나 머리카락이나 털과 같은 흐릿한 표면입니다. 그 이후로 여러 축으로 확장되었습니다.더 나은 모양 모델링(예: 반사 및 굴절[31, 5, 2, 1]), 더 빠른 최적화[8, 27, 23] 및 동적 장면 모델링[38, 17, 10, 19].MLP 기반 암묵적 RF 표현은 학습 속도가 느리기 때문에 우리는 폭셀 기반 명시적 광도장 표현[8] [27]을 사용합니다.특히, 우리는 [8]의 인수분해된 폭셀 그리드 표현을 사용합니다.자기 감독 비디오 역학 인수분해.또 다른 관련 작업은 미리 정의된 마스크가 필요 없는 비디오 역학 인수분해입니다.최근 작업 중 하나는 동작 신호에만 의존하는 변형 가능한 스프라이트[39]입니다.다른 비디오 매팅 작업과 유사하게 2D 전경 및 배경 레이어와 Omnimatte와 동일한 제한 사항이 있습니다.3D 모델링의 경우 D²NeRF[36]는 두 개의 광도장, 즉 영어: 동적 콘텐츠 및 정적 콘텐츠용 다른 하나.D²NeRF[36]는 단 하나의 전경 객체로 매팅하는 특수한 경우를 처리하며 다른 방법과 비교할 때 평면 배경에 국한되지 않습니다.그러나 자체 감독 방법은 비디오당 하이퍼 매개변수 조정이 필요한 휴리스틱에 의존하며 새 비디오에 강력하게 일반화되지 않습니다.전경 재구성의 품질은 큰 비강체 모션이 있는 객체의 경우 제한될 수도 있습니다.따라서 세부 정보가 뛰어난 여러 개별 객체를 지원하는 감독 2D 매트와 비평면 비디오에서 작동하는 3D 배경 분리의 이점이 있는 연관 효과가 있는 비디오 매팅 방법을 제안합니다.3. 방법 Lu et al. [21]은 옴니 매트의 개념을 제안하여 RGBA 비디오 매트를 확장하여 그림자 및 반사와 같은 관심 객체의 연관 효과를 캡처합니다. 혼란을 피하기 위해 다음 텍스트에서 해당 작업을 대문자 Omnimatte로, 결과 RGBA 레이어를 이탤릭 omnimatte라고 합니다.매팅 설정에서 사용자는 T 프레임 {It}}\/\±1과 N개의 순서가 지정된 마스크 레이어 {M}1의 비디오를 준비합니다.각각 관심 객체의 거친 마스크 비디오를 포함합니다.비디오의 카메라 매개변수도 {Pt}로 미리 계산됩니다.목표는 객체와 관련 효과를 포함하는 RGBA 전경 레이어 C와 a, 그리고 전경 객체에서 나오는 효과가 없고 깨끗한 배경 레이어 Bt를 예측하는 것입니다.입력 프레임 It은 배경 위에 전경 레이어를 알파 합성하여 재구성해야 합니다.Omnimatte에서 배경은 정적 2D 이미지와 호모그래피 변환 Pt로 표현됩니다.프레임을 구성하기 위해 추정된 호모그래피 Pt에 따라 정적 배경의 일부를 추출합니다. 우리 연구의 핵심 아이디어는 전경을 2D로 유지하면서 광도장을 사용하여 정적 배경을 3D로 표현하여 객체의 역학을 더 잘 포착하는 것입니다. 우리는 배경을 모델링하기 위해 명시적 인수분해된 폭셀 기반 광도장[8]을 사용합니다. 이 경우 Pt는 카메라 포즈를 나타내고 배경 프레임은 볼륨 렌더링으로 렌더링됩니다. 전경 레이어는 여전히 2D 비디오입니다. 이 조합을 OmnimatteRF 모델이라고 합니다. 3.1. OmnimatteRF 모델 우리 모델의 개요는 그림 2에 나와 있습니다. 이 모델은 전경과 배경이라는 두 개의 독립적인 분기가 있습니다. 주어진 프레임에 대해 전경 분기는 각 객체에 대한 RGBA 이미지(omnimatte)를 예측하고 배경 분기는 단일 RGB 이미지를 렌더링합니다. 전처리. 비슷한 연구에 따라 기성품 모델 RAFT[29]를 사용하여 이웃 프레임 간의 광학 흐름을 예측합니다. 흐름은 보조 입력 및 감독을 위한 기준 진실로 사용되며 {Ft}로 표시됩니다. 또한 기성품 깊이 추정기 MiDaS [26]를 사용하여 각 프레임의 단안 깊이 맵 {Dt}을 예측하고 단안 깊이 손실에 대한 기준 진실로 사용합니다.배경.배경 분기는 장면의 3D 표현을 인코딩하는 정적 신경 광도장 fbg로 구성됩니다.프레임 It의 픽셀을 렌더링하기 위해 추정된 카메라 포즈 Pt에 따라 광선을 추적하고 최종 RGB 색상은 체적 렌더링을 통해 생성됩니다.전체 프레임을 렌더링한 결과는 (Bt, Ôt) = fûg(Pt)입니다.여기서 B+는 RGB 이미지이고 D₁는 깊이 맵입니다.전경.전경 분기는 Omnimatte와 유사한 UNet 스타일의 합성 신경망 ffg입니다.네트워크의 입력은 세 가지 맵의 연결입니다.1. 거친 마스크 M².마스크는 사용자가 제공하며 관심 객체를 설명합니다.픽셀이 객체 내부에 있으면 마스크 값은 1입니다. 2. 광학 흐름 Ft. 네트워크에 동작 힌트를 제공합니다. 네트워크가 보조 작업으로 광학 흐름을 예측한다는 점에 유의하세요(섹션 3.2.2 참조). 3. 피처 맵 Et. 피처 맵의 각 픽셀(x, y)은 3-튜플(x, y, t)의 위치 인코딩입니다. 여러 전경 레이어가 개별적으로 처리됩니다. i번째 레이어의 경우 네트워크는 omnimatte 레이어(C, a)와 흐름 Ĥi를 예측합니다. 디테일 전송. 이미지 품질과 학습 시간 간의 균형을 위해 전경 네트워크는 일반적으로 알파 레이어가 충분한 관련 효과를 캡처했을 때 세부 정보가 누락된 색상 레이어를 생성합니다. 출력 품질을 높이기 위해 Omnimatte는 입력 프레임에서 세부 정보를 전송합니다. 파이프라인에 동일한 프로세스를 포함합니다. 이는 최종 결과를 생성하기 위한 후처리 단계이며 모델 최적화에는 적용되지 않습니다. 3.2. 모델 최적화 우리는 모델의 두 가지 분기가 비디오 전용이므로 모든 비디오에 대해 OmnimatteRF 모델을 최적화합니다. 학습을 감독하기 위해 이미지 재구성 손실과 여러 정규화 손실을 사용합니다. Pt M CH Ft 侧 ffg Et fbg Volumetric Render Fi Bt Alpha Composite Ît Precons Ĥt Dt F bg Ldepth Lflow It Dt 그림 2. 방법 개요. 우리는 2D 전경 레이어와 3D 배경 레이어를 결합하는 OmnimatteRF라는 비디오 매팅 방법을 제안합니다. 전경 분기(ffg, 녹색 상자)는 각 객체에 대한 RGBA 레이어(Ct, α)와 보조 흐름 출력(Ft)을 예측합니다. 배경 분기(fbg, 노란색 상자)는 깊이(Bt, Dt)가 있는 배경 레이어를 생성합니다. 최적화. 학습 중에 예측된 색상(It)과 흐름(Ĥt)은 알파 합성되며, 입력에는 각각 빨간색과 녹색 테두리가 있습니다. 가장 오른쪽 열은 손실 함수의 데이터 항을 설명하며, 이 그림에서는 정규화 항을 생략했습니다.3.2. 재구성 손실 전경 및 배경 레이어의 알파 합성을 통해 합성된 이미지 It로 재구성 손실을 계산합니다.N iN Î₁ = Σ(1)| + [[(1 − α²) Bt (1) i=1 j=i=그리고 재구성 손실은 예측 프레임과 입력 프레임 간의 평균 제곱 오차입니다.Lrecons = ||Ît — It ||(2) 재구성 손실은 파이프라인의 두 분기를 동시에 감독합니다.체적 렌더링의 계산 비용에 의해 제한되어 배경 레이어는 각 단계에서 희소한 무작위 위치에서만 렌더링되며, 여기서 재구성은 합성된 픽셀 값에 대해 계산됩니다.3.2.2 전경 손실 Omnimatte를 따르고 알파 정규화 손실 La-reg, 알파 워프 손실 La-warp 및 흐름 재구성 손실 흐름을 포함합니다. 또한 입력 마스크와 마스크 손실 마스크를 일치시키기 위해 초기 알파 예측을 부트스트랩합니다.마스크 손실 마스크는 값이 임계값 아래로 떨어지면 점차 감소하고 비활성화됩니다.Omnimatte의 대부분 정규화 항은 파이프라인에 직접 적용할 수 있지만 흐름 재구성 손실은 예외입니다.손실의 공식은 동일하게 유지됩니다.레이어당 흐름 예측 Ĥ와 배경 레이어 흐름 Fb%가 주어지면 전체 흐름 Ft는 알파 구성을 통해 구성됩니다(식 1).그러면 손실은 다음과 같이 정의됩니다.Lflow = ||(Êt − Ft) &amp; M³||² t bg (3) 여기서 M은 프레임 It에 대한 모든 전경 마스크({M}})의 합집합이고 손실은 입력 거친 마스크 위치에서만 평가됩니다.Omnimatte의 저자는 이 경우의 손실의 효과를 보여주었고, 우리는 또한 절제 연구에서의 중요성을 보여줍니다.그러나 Fog를 얻는 방법은 불분명합니다. Omnimatte에서 배경 흐름은 네트워크의 입력과 구성을 위한 배경으로 모두 사용되는 이미지 호모그래피에서 파생될 수 있습니다.반면에 3D 배경에는 알려진 카메라 포즈만 있고 깊이는 없으므로 배경 흐름을 직접 얻을 수 없습니다.대신 지상 진실 흐름 Ft를 네트워크 입력으로 사용하여 동작 신호를 제공하고 마스크된 버전의 Ft를 구성을 위한 배경 흐름으로 사용합니다.마스크된 흐름은 F™ = Ft × (1 – M³) M)이며, 이는 거친 마스크에 표시된 영역이 0으로 설정된 지상 진실 광학 흐름입니다.는 요소별 곱셈을 나타냅니다.후자의 경우 네트워크가 모든 곳에서 0인 빈 레이어를 생성하도록 장려하기 때문에 구성을 위해 Ft 대신 F를 사용하는 것이 중요하다고 생각합니다.(a) (b) 그림 3. 배경 레이어 학습 신호.시간에 따라 배경 레이어에 대한 학습 신호가 어떻게 변하는지 설명합니다.이것은 배경이 일부 관련 효과(이 예에서는 그림자)를 포착하는 이유를 설명합니다.빨간색으로 동그라미 친 픽셀을 예로 들어보겠습니다. (a) 학습 시작 시 전경 알파 값(연한 녹색)에는 그림자가 포함되지 않습니다. 따라서 a는 작고 이 픽셀에서 Ît(x,y) ≈ Bt(x,y)입니다. 재구성 손실 재구성은 배경 네트워크 fbg가 이 시야각에서 이 위치에서 어두운 예측을 생성하도록 장려합니다. (b) 학습이 진행됨에 따라 a는 그림자 영역에서 커지고 Ît(x, y) ≈ Cit(x, y)입니다. 즉, 안개는 이 픽셀에서 감독 신호를 거의 또는 전혀 받지 못합니다. 어떤 방식으로든 그림자를 모델링한 경우(이 경우 구멍) 제거할 인센티브가 거의 없어 (c)에 아티팩트가 남습니다. 3.2.3 배경 손실 재구성 손실 외에도 배경 네트워크는 TensoRF [8]에서와 같이 총 변동 정규화 손실 Lbg-reg에 의해 감독됩니다. 또한, 카메라 동작이 회전만으로 구성될 때 단안 깊이 감독을 사용하여 장면 재구성을 개선합니다.Ldepth = metric (Dt, Ôt), (4) 여기서 Dt는 볼륨 렌더링[22]에서 추정된 깊이이고, 메트릭 함수는 MiDaS[26]의 스케일 불변 손실입니다.또한, 깊이가 플로터를 도입할 수 있음을 경험적으로 발견했으며, Mip-NeRF 360[4]에서 제안된 왜곡 손실 왜곡을 사용하여 배경의 아티팩트를 줄입니다.3.2.4 요약 공동 최적화에 대한 결합 손실은 다음과 같습니다.L = L recons + La-reg + La-warp + Lflow + Lmask + Foreground Lbg-reg +Ldepth + distort Background (5) 모든 최적화 단계에서 재구성 및 배경 손실은 희소한 무작위 위치에서 평가됩니다.전경 손실은 전체 이미지에 대해 계산됩니다.3.3. 마스크 재학습을 통한 깨끗한 배경 위에서 설명한 대로 파이프라인을 공동으로 학습하는 경우 배경 광도 필드가 그림자와 같은 일부 전경 콘텐츠를 모델링하는 경우가 있습니다(그림 3(c) 참조). 2D 이미지에 비해 3D 광도 필드는 훨씬 더 유능하여 모델에 시간 정보가 제공되지 않더라도 일부 시간적 효과를 포착하기 위해 구멍과 플로터와 같은 왜곡된 기하학적 구조를 활용할 수 있습니다. 예를 들어, 카메라가 시간이 지남에 따라 움직이면 표면이 그림자로 덮여 있는지 여부와 표면을 보는 방향 사이에 상관 관계가 있을 수 있습니다. 그림 3에서 이 문제를 설명하고 직관적인 수준에서 원인을 설명합니다. 전경 브랜치는 연관된 효과 없이 개체만 포함하는 거친 마스크 입력과 일치하는 알파 값을 생성하도록 부트스트랩됩니다. 즉, 값은 개체에서 1에 가깝지만 그림자에서는 0입니다(단순화를 위해 그림 3과 같이 개체가 그림자를 드리우는 전경 레이어 하나를 고려합니다). 그림자로 덮인 픽셀(x, y)에서 Eq. 1은 단순히 It(x, y) ≈ Bt(x, y)로 축소됩니다. 따라서 재구성 손실은 Bt(x, y)가 이 위치를 향한 레이 샷의 그림자 색상과 일치하도록 합니다. 학습이 진행됨에 따라 ffg는 그림자가 있는 영역에서 예측된 알파 값을 점차 증가시킵니다. 그림자가 딱딱하고 a가 1에 가까워지면 Eq. 1은 Ît(x, y) ≈ C²(x, y)로 평가되고 재구성 손실은 픽셀의 배경색에 거의 제약을 주지 않습니다. 결과적으로 fbg는 (x, y)에서 프레임 It를 향한 레이에 대해 생성하는 그림자 색상을 제거하는 방법을 학습할 수 없습니다. 그림자가 부드럽고 a가 그 중간인 경우도 있습니다. 이러한 경우 문제는 모호합니다. 따라서 선택적 최적화 단계를 통해 깨끗한 배경 재구성을 얻는 것을 제안합니다. 공동 훈련에서 전경 omnimatte 레이어는 배경 레이어에 누출된 콘텐츠가 있는 부분을 포함하여 대부분의 연관된 효과를 캡처할 수 있습니다.그런 다음 알파 레이어를 사용하여 알파 값이 높은 전경 영역의 샘플 없이 처음부터 광도장 모델을 훈련할 수 있습니다.절제 연구(그림 7 참조)에서 이 단계가 야생 비디오에 대해 더 깨끗한 배경 재구성을 생성한다는 것을 보여줍니다.배경만 최적화되므로 프로세스가 빠르고 완료하는 데 1시간도 걸리지 않습니다.4. 평가 저희는 정량적 및 정성적 방법을 각각 2D 비디오 매팅 및 3D 비디오 분할에서 최첨단 방법인 Omnimatte 및 D2NeRF[21, 36]와 비교합니다.또한 Omnimatte의 정적 이미지와 대조적으로 변형 가능한 2D 배경을 사용하는 Layered Neural Atlas(LNA)[13]와 비교합니다.4.1. 영화 데이터 세트 배경 분할의 정량적 평가에는 입력 비디오와 기준 진실 배경 이미지가 모두 있는 데이터 세트가 필요합니다. 이전 작업에서는 주로 CDW-2014 [35]와 같은 데이터 세트를 사용했는데, 이는 대부분 정적 배경으로 제한되어 있으며 당사 설정에 적용할 수 없습니다. 최근 D²NeRF에서 3D 배경 합성을 평가할 수 있는 Kubrics가 제안되었습니다. 그러나 이러한 비디오는 비교적 간단한 장면과 조명을 가지고 있습니다. 어려운 시나리오에서 비디오 매팅과 배경 분할을 평가하기 쉽도록 Blender Studio [6]에서 3개의 Blender 영화에서 6개의 클립을 선택했습니다. Kubrics와 비교하여 더 복잡한 장면과 조명 조건, 캐릭터의 크고 비강체적인 움직임, 더 높은 해상도가 특징입니다. 사용성을 보장하기 위해 카메라 궤적을 수동으로 편집하여 충분한 카메라 움직임이 있고 배우의 크기가 적당하도록 합니다. 배경 재구성 평가 목적으로 입력과 기준 진실을 얻기 위해 배우가 있는 클립과 배우가 없는 클립을 렌더링합니다. 카메라 포즈도 내보냅니다. 4.2. 실험 설정 제안한 방법의 성능을 4개의 데이터 세트에서 평가합니다. 1. Movies: 새로운 도전적인 데이터 세트입니다.2. Kubrics: D²NeRF에서 생성 및 사용된 데이터 세트로, 3D Warehouse [30]에서 Kubric [11]으로 렌더링한 움직이는 물체의 5개 장면으로 구성되어 있습니다.3. DAVIS [24, 25]: 사람, 자동차, 동물과 같은 움직이는 전경 피사체가 있는 짧은 클립입니다.이 데이터 세트는 2D 배경 매팅 방법을 평가하는 데 널리 사용됩니다 [21, 13, 39].4. Wild: 인터넷에서 수집한 야생 시퀀스로, 자연스럽게 촬영한 비디오에 더 가깝고, 이동 및 회전을 포함한 자연스럽고 노이즈가 많은 카메라 동작과 카메라에서 다른 거리에 있는 물체가 있습니다.자연스럽게 이러한 비디오는 순수한 2D 방법에 도전적인 배경을 가지고 있습니다.Kubrics와 Movies는 깨끗한 배경 레이어 렌더링이 가능한 합성 데이터 세트입니다.새로운 뷰 합성이 방법의 초점이 아니므로 입력 뷰로 배경을 평가합니다. 두 데이터 세트 모두 훈련 및 평가에 사용되는 알려진 카메라 포즈와 객체 마스크를 가지고 있습니다.DAVIS와 Wild는 깨끗한 배경이 없는 실제 세계 비디오입니다.따라서 우리는 방법의 견고성을 보여주기 위해 정성적 평가만 수행합니다.Wild의 비디오에 대해 우리는 COLMAP으로 카메라 포즈를 복구합니다.DAVIS 비디오를 포함하여 COLMAP에서 안정적으로 처리할 수 없는 비디오의 경우 RoDynRF[19]의 포즈를 사용합니다.ku-bag ku-chair Sop dodge D²NeRF Omnimatte LNA Ours 그림 4. 배경 재구성.우리는 정량적 평가에서 제시된 결과의 예를 보여줍니다.시차 효과가 있는 비디오의 경우 D²NeRF와 우리의 방법과 같은 3D 방법은 Omnimatte 및 LNA보다 왜곡이 적은 배경을 재구성합니다.거친 객체 마스크를 얻기 위해 Detectron 2[37]의 사전 훈련된 객체 분할 모델로 추출하려고 시도합니다.작동하지 않는 경우 Adobe After Effects의 Roto Brush 도구를 사용합니다.자세한 절차는 보충 자료에 설명되어 있습니다. 약 분의 수동 작업이 필요하여 200프레임 마스크를 생성할 수 있습니다. 모든 비디오에 대해 LOFTR[28] 및 OpenCV를 사용하여 호모그래피를 추정하여 Omnimatte 처리를 활성화합니다. D²NeRF[36]에서 언급했듯이 이 방법은 하이퍼파라미터에 민감합니다. 저자는 다양한 비디오에 대해 5가지 구성 세트를 출시했습니다. 제공된 모든 구성을 사용하여 모든 비디오를 실험하고 가장 성능이 좋은 구성을 보고합니다. 4.3. 구현 세부 정보 네트워크는 공개적으로 사용 가능한 Omnimatte[21] 및 TensoRF[8]의 공식 구현을 기반으로 구축됩니다. Kubrics의 비디오는 해상도가 512x512이고 모든 방법은 해상도 256x256에서 실행됩니다. 해상도가 1920x1080인 다른 데이터 세트의 비디오의 경우 4배로 다운샘플링합니다. 최대 15,000단계에 대해 네트워크를 최적화합니다. ffg의 학습률은 0.001로 설정되고 10,000단계 후에 지수적으로 감소합니다. fúg의 경우 TensoRF의 학습률 스케줄링 방식을 사용합니다. 단일 RTX3090 GPU에서 학습하는 데 최대 6시간이 걸립니다. 자세한 네트워크 아키텍처, 하이퍼 매개변수 및 타이밍 데이터는 보충 자료에 나와 있습니다. 코드와 데이터 세트도 공개됩니다. 4.4. 양적 평가 두 개의 합성 데이터 세트에서 방법의 배경 재구성 품질을 양적으로 평가합니다. 우리 차 자동차 가방 의자 베개 쿠브릭스 LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ D²NeRF 0.0.34.0.Omnimatte 0.0.31.0.0.0.34.31.0.0.SSIM↑ PSNR↑ 0.880 33.0.796 23.LPIPS↓ 0.SSIM↑ PSNR↑ 0.916 33.LPIPS↓ SSIM↑ PSNR↑ 0.0.926 38.0.0.26.0.0.21.LNA 우리 0.0.39.0.0.961 39.0.0.0.835 27.0.972 39.0.0.21.0.0.923 31.0.0.42.0.0.982 43.영화 D²NeRF 옴니매트 0.LNA 우리 당나귀 LPIPS↓ SSIM PSNR↑ 0.19.0.0.0.18.0.990 38. 개 LPIPS↓ SSIM↑ PSNR↑ 0.370 0.694 22.0.279 0.706 21.0.154 0.828 26.0.030 0.976 31. 닭 LPIPS↓ SSIM↑ PSNR↑ 수탉 다지 LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ 0.0.0.0.704 20.0.818 19.0.978 32.0.0.0.708 25.0.0.20.0.23.0.0.23.0.0.26.0.0.937 24.0.0.27.0.0.991 39.표 1. 양적 평가. Kubrics 및 Movies 데이터 세트에서 우리 방법과 기준선의 배경 재구성 비교를 제시합니다. 가장 좋은 결과는 굵은 글씨로 표시하고 2위는 밑줄로 표시합니다. -로 표시된 결과는 이 방법이 좋은 분리를 제공하지 못한 결과입니다(시각 자료는 보충 자료). 표 1의 모든 비디오에 대한 PSNR, SSIM 및 LPIPS와 그림 4의 일부 시각화를 보고합니다. D²NeRF의 경우 Movies의 모든 비디오에 대해 제공된 모든 사전 설정 구성을 시도했으며 Dog, Rooster 및 Dodge 비디오에 대해서만 좋은 결과를 얻었습니다. 2D 배경 레이어가 있는 Omnimatte 및 LNA는 두 데이터 세트 모두에서 어려움을 겪습니다. 우리 방법은 이러한 비디오를 잘 처리할 수 있습니다. 4.5. 정성적 평가 그림 5에서 방법에 대한 정성적 비교를 제시합니다. 공간 제한으로 인해 모든 데이터 세트에서 최소한 하나의 비디오를 제시하지만 그림에서는 선택한 모든 비디오의 프레임을 보여줍니다. 원본 비디오는 보충 자료로 제공되며 시청하는 것이 좋습니다.D²NeRF는 미세 조정된 비디오에는 잘 작동하지만 추가적인 하이퍼 매개변수 조정 없이는 새로운 입력에는 작동하지 않습니다.Omnimatte 배경은 객체 주변에 상당한 왜곡이 있으며, 전경 레이어는 모든 잔차를 캡처하여 제한을 보상해야 합니다.우리의 방법은 3D 배경 모델로 다양한 비디오에서 잘 수행할 만큼 다재다능합니다.4.6. Ablation Studies 4.6. Loss Terms 그림 6에 Ldepth 없이 배경 재구성 결과를 제시합니다.회전 카메라 포즈가 있는 비디오 시퀀스의 경우 모델은 3D 단서가 부족하여 입력 비디오에서 3D 정보를 추출하는 데 어려움을 겪습니다.이 손실은 우리 방법을 더 광범위한 비디오로 확장하는 데 중요합니다.Lflow의 효과도 그림 6에 나와 있습니다.보조 작업은 전경 품질을 개선하고 관련 없는 콘텐츠를 줄입니다.4.6.2 깨끗한 배경 재교육 실제 시퀀스에 대해 추가 단계를 사용하여 처음부터 깨끗한 배경을 최적화합니다. 그림 7에서 초기 조인트 최적화의 배경 레이어와 최종 결과를 비교합니다. 이는 더 나은 배경을 얻는 간단하면서도 강력한 방법입니다. 4.7. 제한 사항 향후 작업에서 탐색할 수 있는 몇 가지 제한 사항을 나열합니다. 1. 배경 영역이 거의 항상 그림자로 덮여 있는 경우 배경 모델은 색상을 올바르게 복구할 수 없습니다. 그림 8에는 영화 비디오의 예가 나와 있습니다. 이론적으로 omnimatte 레이어에는 알파 채널이 있으며 배경이 원래 색상을 가질 수 있도록 하는 가산 그림자만 캡처할 수 있습니다. 그러나 이 문제는 현재 설정에서 크게 제약이 부족하여 모호하고 배경이 만족스럽지 못한 솔루션으로 이어집니다. 2. 전경 레이어는 관련 없는 콘텐츠를 캡처합니다. 실제 비디오에서는 흔들리는 나무와 움직이는 자동차와 같이 관련 없는 동작이 배경에 종종 있습니다. 이러한 효과는 정적 광도 필드로 모델링할 수 없으며 개체와의 연관성에 관계없이 전경 레이어에서 캡처합니다. 가능한 방향에는 i) 더미 2D 레이어를 사용하여 이러한 콘텐츠를 포착하거나 ii) 배경과 전경 모두 동작을 모델링할 수 있으므로 모호성을 해결하기 위한 추가 정규화가 있는 변형 가능한 3D 배경 모델이 포함됩니다. 3. 전경 객체는 가려진 경우 omnimatte 레이어에서 누락된 부분이 있을 수 있습니다. 전경 네트워크는 알파 구성을 위한 픽셀 값을 예측하므로 가려진 부분을 항상 환각하지는 않습니다. 4. 비디오 해상도가 제한적입니다. 이는 주로 Omnimatte에서 상속된 전경 모델의 U-Net 아키텍처 때문입니다. 다른 경량 이미지 인코더를 사용하면 더 높은 해상도를 지원할 수 있습니다. 5. 가중치가 무작위로 다르게 초기화되면 전경 레이어가 다른 콘텐츠를 캡처할 수 있습니다. 보충 자료에 시각적 결과를 포함했습니다. 5.
--- CONCLUSION ---
우리는 객체와 연관된 효과를 포함하는 RGBA 레이어인 옴니매트를 얻는 방법을 제안합니다. 이는 자동차 수탉 쿠-필로우 입력 배경 전경 D²NeRF 옴니매트 우리의 D²NERF 옴니매트 우리의 그림 5. 정성적 비교. 우리는 각 데이터 세트의 비디오에 대한 우리의 방법과 기준 방법의 결과를 비교합니다. 독자는 보충 자료에서 제공되는 더 많은 시퀀스의 비디오 파일을 보는 것이 좋습니다. 처음 두 비디오는 Kubrics와 Movies에서 합성한 것이고, 그 다음에 세 개의 Wild 비디오가 나옵니다. 옴니매트는 3D 객체를 처리하지 못하고 왜곡된 배경을 생성합니다. D² NeRF는 적절한 하이퍼 매개변수가 있는 비디오에 작동하지만 새로운 비디오로 쉽게 일반화되지 않습니다. 우리의 방법은 여러 다른 설정의 비디오를 처리합니다. 공간 제약으로 인해 LNA 결과는 보충 자료로 미룹니다. 깊이가 있는 입력 깊이가 없는 입력 Lflow가 있는 입력 (a) 거친 마스크 (b) 배경 (c) 깊이 A 그림 6. 손실 용어 절제. 깊이가 없는 실제 세계 비디오의 배경과 흐름이 없는 전경은 실제 세계 비디오에서 저하될 수 있습니다.(f) 깊이 (d) 학습된 마스크 (e) 배경 그림 7. 깨끗한 배경 재학습. 공동으로 학습된 배경 레이어는 바닥에 구멍으로 그림자를 캡처할 수 있습니다(ac). 공동 학습 후, 전경 옴니매트는 깨끗한 배경을 학습하는 데 사용할 수 있는 더 나은 마스크를 제공합니다(df). 2D 전경 레이어와 3D 배경 모델을 결합합니다. 광범위한 실험을 통해 우리의 접근 방식이 이전 방법의 기능을 넘어 다양한 비디오에 적용 가능하다는 것이 입증되었습니다.(a) (b) (c) 그림 8. 제한 사항.(a), (b): 비디오의 대부분 프레임에서 영역이 그림자로 덮여 있을 때 배경에 구운 그림자가 생길 수 있습니다.(c): 전경 레이어는 배경에서 무관한 개체 동작(왼쪽 가운데)을 캡처합니다. 비디오에서 가장 잘 볼 수 있습니다. 참고문헌 [1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O&#39;Toole, Changil Kim. Hyperreel: 광선 조건 샘플링을 사용한 고충실도 6자유도 비디오. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023.[2] Benjamin Attal, Jia-Bin Huang, Michael Zollhöfer, Johannes Kopf, Changil Kim. 광선 공간 임베딩을 사용한 신경 광 필드 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022.[3] Xue Bai, Jue Wang, David Simons, Guillermo Sapiro. 비디오 스냅컷: 국소화된 분류기를 사용한 견고한 비디오 객체 컷아웃. ACM Trans. Graph., 28(3):70, 2009.[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. Mip-nerf 360: 무한한 앤티 앨리어싱 신경 광도 필드. CVPR, 2022.[5] Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, Tobias Ritschel. 굴절적 소설-뷰 합성을 위한 Eikonal 필드. Munkhtsetseg Nandigjav, Niloy J. Mitra, Aaron Hertzmann 편집자, SIGGRAPH &#39;22: 컴퓨터 그래픽 및 대화형 기술에 대한 특수 관심 그룹 컨퍼런스, 캐나다 브리티시 컬럼비아주 밴쿠버, 2022년 8월 7-11일, 39:1–39:9페이지. ACM, 2022.[6] Institute Blender, 2022년 10월. 2,[7] Gabriel J. Brostow 및 Irfan A. Essa. 비디오의 동작 기반 분해. 1999년 9월 20-25일, 그리스, 코르푸, 케르키라에서 열린 국제 컴퓨터 비전 컨퍼런스 회의록, 8-13페이지. IEEE Computer Society, 1999.[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu 및 Hao Su. Tensorf: Tensorial radiance fields. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. 2, 3, 5, 6,[9] Yung-Yu Chuang, Aseem Agarwala, Brian Curless, David Salesin 및 Richard Szeliski. 복잡한 장면의 비디오 매팅. ACM Trans. Graph., 21(3):243-248, 2002.[10] Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang. 동적 단안 비디오에서 동적 뷰 합성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 2021.[11] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi(Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi SM Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong 및 Andrea Tagliasacchi. Kubric: 확장 가능한 데이터세트 생성기입니다. 2022.[12] Zeqi Gu, Wenqi Xian, Noah Snavely, Abe Davis. Factormatte: 재구성 작업을 위한 비디오 매팅 재정의, 2022.[13] Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel. 일관된 비디오 편집을 위한 계층화된 신경 아틀라스. ACM Transactions on Graphics(TOG), 40(6):1-12, 2021. 2, 5, 6,[14] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, Jia-Bin Huang. 모양 인식 텍스트 기반 계층화된 비디오 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023.[15] Wenbin Li, Fabio Viola, Jonathan Starck, Gabriel J. Brostow, Neill DF Campbell. Roto++: 형상 매니폴드를 사용하여 전문 로토스코핑 가속화. ACM 그래픽스 트랜잭션(ACM SIGGRAPH&#39;16 진행 중), 35(4), 2016.[16] Yin Li, Jian Sun, Heung-Yeung Shum. 비디오 객체 잘라내기 및 붙여넣기. ACM Trans. Graph., 24(3):595-600, 2005.[17] Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang. 동적 장면의 공간-시간 뷰 합성을 위한 신경 장면 흐름 필드. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2021, 가상, 1925년 6월, 2021, 6498-6508페이지. 컴퓨터 비전 재단/IEEE, 2021.[18] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, Ira KemelmacherShlizerman. 실시간 고해상도 배경 매팅. arXiv, 페이지 arXiv-2012, 2020.[19] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, Jia-Bin Huang. 견고한 동적 광도 필드. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023. 2,[20] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, Michael Rubinstein. 비디오에서 사람의 리타이밍을 위한 계층적 신경 렌더링. SIGGRAPH Asia, 2020.[21] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T Freeman, Michael Rubinstein. Omnimatte: 비디오에서 객체와 그 효과 연관시키기. CVPR, 2021. 1, 2, 3, 5, 6,[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현하기. ECCV, 2020. 2, 5,[23] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM Trans. Graph., 41(4):102:1– 102:15, 2022.[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, A. Sorkine-Hornung. 비디오 객체 분할을 위한 벤치마크 데이터 세트 및 평가 방법론. Computer Vision and Pattern Recognition, 2016.[25] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv:1704.00675, 2017.[26] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향해: 제로 샷 교차 데이터 세트 전송을 위한 데이터 세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(3), 2022. 3,[27] Cheng Sun, Min Sun 및 Hwann-Tzong Chen. 직접 복셀 그리드 최적화: 광도장 재구성을 위한 초고속 수렴. CVPR, 2022.[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao 및 Xiaowei Zhou. LoFTR: 변압기를 사용한 검출기 없는 로컬 특징 매칭. CVPR, 2021.[29] Zachary Teed 및 Jia Deng. RAFT: 광학 흐름을 위한 순환 모든 쌍 필드 변환. Andrea Vedaldi, Horst Bischof, Thomas Brox 및 Jan-Michael Frahm 편집자, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, 2020년 8월 23-28일, Proceedings, Part II, Lecture Notes in Computer Science의 12347권, 402-419페이지. Springer, 2020.[30] Inc Trimble, 2022년 10월.[31] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd E. Zickler, Jonathan T. Barron 및 Pratul P. Srinivasan. Ref-nerf: 신경 광도장에 대한 구조화된 뷰 종속 모양. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2022, 루이지애나주 뉴올리언스, 미국, 2022년 6월 18일~24일, 5481~5490페이지. IEEE, 2022.[32] Jue Wang, Pravin Bhat, Alex Colburn, Maneesh Agrawala, Michael F. Cohen. 대화형 비디오 컷아웃. ACM Trans. Graph., 24(3):585~594, 2005.[33] Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, Pheng-Ann Heng. 양방향 관계 학습을 통한 단일 단계 인스턴스 그림자 감지. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 1~11페이지, 2021년 6월.[34] Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, Chi-Wing Fu. 인스턴스 그림자 감지. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 2020년 6월.[35] Yi Wang, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, Yannick Benezeth, Prakash Ishwar. Cdnet 2014: 확장된 변경 감지 벤치마크 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 워크숍 컨퍼런스, 393-400페이지, 2014년.[36] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, Cengiz Oztireli. D² nerf: 단안 비디오에서 동적 및 정적 객체의 자체 감독 분리. Advances in Neural Information Processing Systems, 2022. 1, 3, 5, 6,[37] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 6,[38] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim. 자유 시점 비디오를 위한 시공간 신경 조도 필드. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2021, 가상, 2021년 6월 19-25일, 9421-9431페이지. Computer Vision Foundation/IEEE, 2021.[39] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, Noah Snavely. 비지도 비디오 분해를 위한 변형 가능한 스프라이트.IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 2022년 6월.3,A. 추가 정성적 결과 본 논문에 제시된 결과의 비디오 파일(Movies, Kubrics, Wild 및 DAVIS 데이터 세트의 모든 비디오)은 보충 자료의 일부로 프로젝트 웹사이트에서 사용할 수 있습니다.다음에서 시청하는 것을 적극 권장합니다: https://omnimatte-rf.github.io 1. 우리 방법(OmnimatteRF)의 경우 모든 비디오에 대한 결과(마스크가 있는 입력, 전경 레이어, 배경 레이어, 배경 깊이 맵)를 포함합니다.2. D²NERF[36]의 경우 모든 비디오에 대해 작성자가 제공한 모든 구성 중에서 가장 좋은 결과를 사용합니다.어떤 구성도 비어 있지 않은 정적 및 동적 레이어를 성공적으로 재구성하지 못하면 비디오 파일을 삭제하고 그림 A2에 프레임만 표시합니다. 3. Omnimatte[21] 및 Layered Neural Atlas(LNA)[13]의 경우 Wild, Movies 및 Kubrics의 비디오를 포함했습니다.DAVIS의 결과는 이전 작업에서 찾을 수 있습니다.B. 임의 초기화 Omnimatte[21]에서도 논의했듯이 다른 임의 초기화로 인해 전경 레이어의 결과가 달라질 수 있습니다.그림 A1에 두 가지 예를 보여줍니다.모든 실험에서 임의 시드는 3으로 설정되었습니다.시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =그림 A1. 임의 초기화의 효과.위: Wild/dogwalk 비디오의 경우 다른 시드가 사람의 환각 그림자의 양을 다르게 합니다.아래: Kubrics/cars 비디오의 경우 시드는 그림자가 객체에 연결되는 방식에 영향을 미칩니다.C. 추가 구현 세부 정보 C.1. 마스크 생성 우리의 방법과 Omnimatte는 관심 있는 모든 객체를 윤곽을 그리는 거친 마스크 비디오에 의존합니다. 합성 Kubrics 및 Movies 비디오에는 기준 진실 객체 마스크가 있으며 이를 직접 사용합니다.실제 비디오의 마스크를 얻기 위해 두 가지 워크플로 중 하나를 사용합니다.1. 먼저 Detectron 2[37]의 사전 학습된 Mask RCNN 모델(X101-FPN)로 비디오를 처리합니다.그런 다음 객체를 가장 잘 포착하는 모든 프레임에서 마스크를 수동으로 선택합니다.2. Adobe After Effects의 Roto Brush 도구를 사용하여 객체를 추적합니다.이 방법은 Mask R-CNN이 비디오에 적합한 마스크를 생성하지 못할 때 유용합니다.특히 Wild/dance 및 Wild/solo를 수동으로 처리했습니다.200프레임 비디오의 마스크 시퀀스를 생성하는 데 약 10분의 수동 작업이 걸립니다.C.2. 네트워크 아키텍처 전경 네트워크는 Omnimatte의 U-Net 아키텍처를 기반으로 하며, 이는 보충 자료[21]에 자세히 나와 있습니다.해당 네트워크를 OmnimatteRF에 적용하기 위해 배경 노이즈 입력을 2D 피처 맵 Et로 대체합니다. Et의 각 픽셀은 3D 벡터(x, y, t)의 위치 인코딩입니다.여기서 (x, y)는 픽셀 위치이고 t는 프레임 번호입니다.위치 인코딩 방식은 NeRF[22]에서 제안한 것과 동일하며 L = 10 주파수입니다.배경의 경우 MLP 피처 디코더가 있는 TensoRF[8]의 벡터-행렬 분해 모델을 사용합니다.초기 그리드는 동일한 해상도 No = 128을 갖고 최종 그리드는 N 640으로 제한됩니다.벡터와 행렬은 2000, 3000, 4000, 5500단계에서 업샘플링됩니다.= C.3.하이퍼 매개변수 모든 비디오에서 전경 네트워크에 0.001의 학습률을 사용하며, 이는 10,000단계에서 10,000단계당 0.1×의 속도로 지수적으로 감소합니다. 우리는 전경 훈련이 발산하는 것을 방지하는 데 감소가 중요하다는 것을 알았습니다. 마스크 부트스트래핑 손실 마스크는 초기 가중치가 50이며, 손실 값(가중치 전)이 0.02 미만으로 떨어지면 먼저 5로 줄어든 다음 같은 단계 수 후에 꺼집니다. 우리는 표 A1에 다른 손실 항목의 가중치를 기록합니다. 배경 네트워크 학습률 스케줄링 및 Lbg-reg 가중치는 원래 TensoRF[8]와 동일합니다. 일반적으로 대부분의 비디오에 동일한 하이퍼 매개변수 세트를 사용하고 아티팩트가 관찰될 때만 추가 항목을 추가합니다. 비디오 단계 Lrecons La-reg La-warp Lflow Ldepth Ldistort 모두 15,0.01(L1)/0.005(LO) 0.0.0.10, Wild/boldering DAVIS 표 A1. 하이퍼 매개변수. 우리는 실험에서 하이퍼 매개변수(단계 수와 손실 항목의 가중치)를 기록합니다. 첫 번째 행은 대부분의 비디오에서 공유하는 구성입니다. 나머지 행은 구성이 다른 비디오이고, -는 공유된 숫자에서 변경되지 않음을 의미합니다.FG 볼더링 워크 BG FG 자동차 개 BG 당나귀 그림 A2. 실패한 장면에 대한 D²NeRF 결과.방법 Omnimatte 12, 단계 학습(시간) 렌더링(초/이미지) 2.2.D²NeRF 100, 4.4.LNA 400, 8.0.우리 15, 3.3.Omnimatte 12, 1.0.D²NeRF 100, 3.2.LNA 400, 8.0.우리 15, 2.1.표 A2. 실행 시간 측정.OmnimatteRF와 기준 방법을 학습하는 데 걸리는 시간을 측정하여 비교합니다.위: 영화, 야생(480 x 270, DAVIS는 비슷한 428 x 240 해상도를 갖습니다).아래: Kubrics(256 x 256).C.4. 실행 시간 측정 우리는 표 A2에서 OmnimatteRF와 기준선 방법을 훈련하는 데 걸리는 시간을 측정하여 보고합니다. 모든 측정은 8코어 AMD R7-2700X CPU와 단일 NVIDIA RTXGPU가 있는 워크스테이션에서 수행됩니다. 우리의 방법은 3D 배경 광도 필드가 추가되어 Omnimatte보다 훈련하는 데 더 오래 걸립니다. 깨끗한 배경 재훈련 프로세스에서와 같이 배경 모델만 최적화하는 경우 비디오당 약 30분이 걸립니다. FG 솔로 치킨 BG
"
"--- ABSTRACT ---
Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release AGENTS, an open-source library with the goal of opening up these advances to a wider non-specialist audience. AGENTS is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and finegrained symbolic control. AGENTS is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. AGENTS is available at https: //github.com/aiwaves-cn/agents. 1
--- INTRODUCTION ---
“An autonomous agent is a system situated within and a part of an environment that senses the environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.” Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents [Franklin and Graesser, 1996] Large Language Models (LLMs) [Brown et al., 2020, Ouyang et al., 2022, OpenAI, 2023] such as ChatGPT make it possible to build autonomous agents that can automatically solve complicated tasks and interact with the environment, humans, or other agents by perceiving, reasoning, planning, and acting in the world [Weng, 2023]. Language agents are a promising step towards artificial general intelligence (AGI) and can help reduce human effort in certain roles such as customer service, consulting, programming, writing, teaching, etc. Some recent demos such as AutoGPT [Richards and et al., 2023] and BabyAGI [Nakajima, 2023] have demonstrated the potential of language agents and have gained massive interest from developers, researchers, as well as more non-technical audiences. While intriguing, most of these demos or repositories are not friendly for customizing, tuning, and deploying new agents even for experienced developers or researchers. This limitation comes from the fact that these demos typically proof-of-concepts showcasing the possibility of language agents, instead of being larger frameworks that can be used to build and customize language agents over time. Moreover, most of these open-source repositories only cover a small portion of the core abilities of language agents including task decomposition [Nye et al., 2022], long-short term memory [Zhou “Equal Contribution. Correspondence to: chunshu @aiwaves.cn Preprint. Work in progress. --- --Planning Planning Web «— Web : Navigation Navigation en) Multi-Agent (C_9) Agent 1 Agent(eg Editor) Communication (eg Writer Tool Use Tool Use7” Human-Agent Interaction Figure 1: Illustration of the AGENTS framework. et al., 2023a], web navigation [Nakano et al., 2021], tool usage [Schick et al., 2023], and multiagent communication [Foerster et al., 2016]. In addition, most (if not all) existing language agent frameworks solely depend on a short task description and rely completely on the abilities of LLMs to plan and act. This results in significant randomness and inconsistency across different runs, delivering an unsatisfactory user experience and making it hard to customize and tune language agents. We believe the aforementioned limitations are important barriers for recent advances in language agents to reach a broader non-specialist audience and impact our society in a positive way. To this end, we release AGENTS, an open-source library and framework for language agents dedicated to supporting LLM-powered language agents. AGENTS’s philosophy is to make customizing, tuning, and deploying language agents as simple as possible even for non-specialists while also remaining easily extensible for developers and researchers. In addition, the library also provides the following key features that make it a versatile framework for language agents: Long-short term memory According to Franklin and Graesser [1996], a key difference between autonomous agents and computer programs (or machine learning models) is that machine learning models only need to respond to a single input/query, while autonomous agents need to interact with environments or other agents over time. Therefore, the ability to maintain long-short term memory is very important for autonomous agents. AGENTS integrates the memory components in [Zhou et al., 2023a] and enables language agents to store and retrieve long-term memory with VectorDB and semantic search, and regularly update a short-term working memory with a scratchpad. Users can choose to equip an agent with long-term memory, short-term memory, or both of them by simply filling in a field in the config file. Tool usage & Web navigation Another important feature for autonomous agents is the ability to use external tools and surf the internet. This is especially important for language agents because they rely on the language interface and thus need to use external tools to interact with environments beyond language communication and navigate the web to gather useful information. Following [Patil et al., 2023], AGENTS supports a few commonly used external APIs and provides an abstract class that enables developers to integrate other tools with ease. We also enable agents to navigate the internet and gather information by defining web search and web navigation as specialized APIs. Multi-agent communication In addition to single-agent abilities, AGENTS also supports customizing multi-agent systems, which can be helpful for certain applications such as games [Park et al., 2023], social experiments [Li et al., 2023], software development [Qian et al., 2023], etc. One new feature for multi-agent communication in AGENTS is the “dynamic scheduling” feature. Instead of --- --scheduling the order for the agents to act with hard-coded rules, dynamic scheduling provides an option to define a controller agent that acts as a “moderator” and decides which agent to perform the next action considering their roles and the current history. Dynamic scheduling has the potential to make communication between multiple agents more natural and flexible. Developers can easily customize the controller by specifying its rule in the config file using natural language. Human-agent interaction One limitation in existing agent frameworks is that while they enable agents, or multi-agents, to automatically solve tasks, it’s not easy or even possible for human users to interact with the agents, especially in the multi-agent scenario. AGENTS seamlessly supports human-agent interaction in both single-agent and multi-agent scenarios, making it possible for one or more humans to communicate and interact with language agents. Controllabilty Existing agent frameworks generally define and control the agents’ behavior only using a system prompt and then let the agent plan and act on its own. In contrast, AGENTS provides a novel paradigm to build controllable agents via a symbolic plan, also referred to as standard operating procedures (SOPs). An SOP is a graph of multiple states that defines different situations an agent may encounter while accomplishing a task, and the transition rules between the states. Similar to SOPs in the real world, an SOP in AGENTS is a meticulously documented set of step-by-step instructions that outlines how a particular task or process should be performed by an agent or a group of agents. SOPs can be generated by an LLM and edited by the user when customizing and tuning the agent. After deployment, an agent will behave following specified instructions and guidelines for each state and dynamically adjust its current state according to its interaction with the environment, humans, or other agents. The introduction of the symbolic plan offers the opportunity to provide fine-grained control of an agent’s behavior, making agents’ behavior more stable/predictable and facilitating tuning/optimizing agents at the same time. In addition, we propose an automated SOP generation pipeline to reduce human labor on writing detailed SOP and config files when customizing (multi-) agent systems. The automated SOP generation pipeline is a “meta agent” that can generate config files for language agents with retrievalaugmented generation given a short description of the task. AGENTS is an ongoing effort maintained by researchers and engineers from AIWaves?. We look forward to support from community contributors on the project. The library and detailed documentation and tutorials are available on GitHub. 2
--- RELATED WORK ---
2.1 Autonomous Language Agents The concept of language agents has become very popular recently and a variety of language agents targeting different tasks have been proposed. For example, Generative Agents [Park et al., 2023] developed language agents to mimic human social behavior, WebAgent [Gur et al., 2023] demonstrated the possibility to build language agents that can complete the tasks on real websites following natural language instructions, Qian et al. [2023] and MetaGPT [Hong et al., 2023] experimented with software development in multi-agent communication settings, and Zhou et al. [2023a] built language agents that act as interactive writing assistants. In addition to language agents that target specific tasks, recent open-source projects such as AutoGPT [Richards and et al., 2023], BabyAGI [Nakajima, 2023], and SuperAGI [SuperAGI, 2023] are aimed at the goal of building autonomous agents that do whatever users want and attracted massive interest from both developers and non-specialist audiences. 2.2 Language Agents Frameworks More recently, a few open-source frameworks for language agents have been proposed. For example, Transformers Agents [Wolf et al., 2020] builds language agents that can automatically use tools to solve tasks described in natural language; LangChain [LangChain, 2022] supports end-to-end “https: //www.aiwaves.org/ Shttps: //github.com/aiwaves-cn/agents --- --ANnhwWNne CADMEWNK Table 1: Comparison of Language Agent Frameworks Framework Tool Usage Long-short Term Memory Multi-Agent Human-Agent Interaction Symbolic Control Transformers Agents v x x x x LangChain v v x x x Auto-GPT v x x x x Gentopia v x x x x XLang v x x x x Meta-GPT v x v x x Camel v x v x x AgentVerse v v v v x AGENTS v v v v v language agents that can automatically solve tasks specified in natural language; Camel [Li et al., 2023] and AgentVerse [Chen et al., 2023] are platforms tailored for building multi-agent systems; Gentopia [Xu et al., 2023] and XLang* are libraries for building tool-augmented agents. We illustrate the key features supported by these platforms and AGENTS in Table 1. We can see that AGENTS is the only framework that supports tool usage, long-short term memory, and multi-agent communication at the same time. AGENTS also offers human-agent interaction and controllability through symbolic plans (SOPs) for the first time. 3 Library Design Code 1: Exemplar code for initializing and running a (multi) agent system with AGENTS def main() # agents is a dict of one or multiple agents. agents = Agent.from_config(""./config.json"") sop = SOP.from_config(""./config. json"") environment = Environment.from_config(""./config. json"") run(agents ,sop, environment) AGENTS is designed following the philosophy in Franklin and Graesser [1996]: “an autonomous agent is situated in an environment”. Therefore, agent and environment are two major classes in the AGENTS framework. In addition to these two classes, we also include a class for symbolic plans, named SOP (short for Standard Operating Procedure), to make language agents more controllable. These main classes are all initialized from a config file which can be filled in plain text. In sum, a typical script for initializing and running a (multi) agent system with AGENTS is illustrated in Code 1. The config file not only defines these core objects but also factorizes complicated prompts into modularized prompt components. The factorization of prompts significantly reduces the expertise requirements and efforts for users to build (multi) agent systems. Using a single config file to define the agents, plan, and basic environment also facilitates the sharing of language agents (which will be discussed in the Agent Hub section). Each of these three core classes consist of standardized APIs that can be overwritten by experienced developers and researchers. We describe these classes in detail: Code 2: Exemplar code for the running loop of a (multi) agent system in AGENTS def run(agents ,sop,environment): while not sop.finished: agent ,state=sop.step(agents, environment) action=agent.step(state ,environment) environment .update (agent , action) # optional, in case of dynamic planning # new_states = get_new_states (action) # sop.add_states (new_states) ‘https: //github.com/xlang-ai/xlang --- --3.1 Agent The Agent class abstracts a language agent. Its UML is illustrated in Figure 1. We can see that an agent maintains its long-short term memory and has
--- METHOD ---
s to observe the environment (agent ._observe (environment) ), act according to its current state (agent ._act ()) and update its memory (agent. _update_memory()). All these methods are wrapped in the agent .step() method. This factorization enables developers to customize agents with new functionalities easily. Unlike existing language agent frameworks that assume an agent must be based on an LLM, we include a “_is_human” property to an agent. If it is set to “True”, the (agent ._act ()) will opt to provide observations and memory information to a human user and wait for the human user to input the action. This design allows flexible human-agent interaction in both single-agent and multi-agent systems by allowing human users to take the role of one or more language agents. It facilitates developers to build various interesting applications such as allowing human users to act as members of a team in debate and collaborate with (agent or human-based) teammates to beat another team, or act as CTO/engineers in a software company and collaborate with others for software development. 3.2 SOP The SOP class contains a graph of the states of agents. Each state specifies a certain sub-task or sub-goal of all agents when accomplishing the task described by the SOP. States are abstracted into a State class. A State object contains modularized prompts for the agent to leverage an LLM and various tools or APIs that an agent can use in the state. We abstract everything an agent may use for action in a state into a “Component” class. The Component class consists of two subclasses corresponding to different parts of the prompt and tools or external APIs, named “PromptComponent” and “ToolComponent”, respectively. PromptComponent includes modularized prompts that specify the task/goal, rules/constraints, (step-by-step) demonstrations for in-context learning, and the output format. ToolComponent supports more complex usage beyond modularized prompts, including external tools and APIs such as web search, knowledge bases, etc. The results of the tools are either included in the prompt or directly returned and processed afterward, according to the config file. An SOP object also includes an LLM-based control function that decides the transition between different states and the next agent to act. The state transit function is named sop._transit() and the agent routing function is named sop._route(). Both of the functions are wrapped in an sop.next() function which is used in the main loop. send Restart Figure 2: (a) Customer service agent Figure 3: (b) Sales agent 3.3. Environment The Environment class abstracts the environment in which the agents are situated. An environment consists of two main functions: environment._observed() and environment.update(). environment ._observed() defines how the environment influences the agent’s action (i.e., what information should be transferred to the agent upon observation, and environment . update () defines how the agent’s action impacts the environment. --- --oqo 2 3 n 5Write Character Settings and Expand the first chapter Expand the second chapter Expand the third chapter Expand the forth chapter Expand the fifth chapter Script Outlines # a a a a a Zoe 4 Recorder i Character Design: DONE é BB Total Number:1D) character © Name: Mike Smith QDs Gender: Male £8 Work: internet Company Programmer Qase:Mike isa hardworking and dedicated individual. Hei intelligent, logical, and detail-oriented. He can bea bitreserved and introverted, the deeply cares for his family. ® Speaking Style: Mike speaks in a calm and ‘measured tone. Heis articulate and precise his words. ©) Relation with Others: Mike is married to Jane and they have a child together. He has a close relationship with is family and values their happiness above alelse. (Bh Character Background: Mike has been working at the internet company fr the past 10 years. He is passionate about his work an takes pride in his programming skills, However, he often finds himself struggling to find a balance between his demanding jaband spending quality time with his family. This conflict creates tension and challenges for Mike throughout the story 1 characterFigure 4: Multi-Agent System: Fiction Studio. The execution logic of a (multi) agent system based on AGENTS is very intuitive. As illustrated in Code 2, in each iteration, the SOP first decides the state transition and selects the next agent to act based on the agents and the environment. The agent then takes an action based on its state and the environment. Then the environment updates itself based on the new action. Finally, if a workflow requires dynamically adjusting the plan based on the intermediate execution results, one can parse the output from an action, define a new state and add it into the current SOP. 3.4 Implementation Details of Core Features Long-short Term Memory: AGENTS implements long-short term memories for language agents following Zhou et al. [2023a]. Specifically, long-term memories are action histories and are embedded by sentence-transformers [Reimers and Gurevych, 2019], stored in a VectorDB, and queried via semantic search. Short-term memories, or working memories, are in natural language form and updated by an LLM via a carefully tuned prompt. Tool Usage & Web Navigation : AGENTS supports tool usage and web navigation via ToolComponents. For each external tool or API, developer can wrap the API call in theToolComponent .func() method. For complicated tools of which the API call is contextdependent, AGENTS integrates the the “Function-calling” feature of OpenAI’s GPT APIs to let LLMs decide how to use the tools. Web navigation is achieved by implementing web search as a specialized tool. Multi-Agent Communication —: Different from most existing frameworks for multi-agent systems that use pre-defined rules (e.g., let each agent act in a sequential order) to control the order for agents’ action, AGENTS includes a controller function that dynamically decides which agent will perform the next action using an LLM by considering the previous actions, the environment, and the target of the current states. This makes multi-agent communication more flexible. Human-Agent Interaction : AGENTS supports human-agent interaction in multi-agent systems by allowing human users to change the “is_human” field for a certain agent in the config file to “True”. In this case, the user can play the role of the agent by himself/herself and input his/her own actions and interact with other language agents in the environment. --- --Debate Topic: Should Al Replace Humans in Creative Fields?? 6 James(Afrmative Second) ary, to defend your identity, youcan emphasize the unique qualtes and espectvsthat humans bring create els suchas intulton erations, ‘nd the ait to understand complex human experiences Highlight the value of Al should replace humans in creative fields because it human connection and heathen that comes fom human crested ar and can produce art and content efficiently, reduce costs, sonnet. and eliminate human bias. Affirmative viewpoint: Negative viewpoint Al should not replace humans in creative fields as it lacks true creativity, emotions, and the ability to understand complex human experiences. User'agent Audience David(Debate_Judge) Emily(Negative_First) James(Affirmative_Second) Jennifer(Affirmative_Third) John(Affirmative_Debate_organizer) © Maryiatfirmative_First) Michael(Negative_Debate_organizer) ‘Sarah(Negative_Third) William(Negative_Second) Figure 5: Human-Agent Interaction in a debate. 3.5 Deployment Existing open-source frameworks for language agents focus on building proof-of-concept language agents that run either in the terminal or on Gradio [Abid et al., 2019]. In contrast, AGENTS supports deploying language agents as APIs with FastAPI°. This greatly facilitates developers to integrate language agents in real-world applications. 3.6 The Agent Hub AGENTS aims to not only facilitate the development, testing, and tuning of a language agents system but also makes the distribution and sharing of language agents easier. To this end, we introduce AGENT HuB, a platform that allows users to share their fine-tuned language agents as well as search/download useful language agents that others share on the platform. In this way, one can easily customize language agents by starting from community agents and slightly modifying them. This greatly reduces the effort of designing, testing, and tuning language agents from scratch. 3.7 Automatic Creation of Agent Systems While using an SOP to provide fine-grained control to language agents, it can sometimes be laborsome for users to manually specify the SOP from scratch since it requires to set different states, their connections, and the prompts and tools for each Component for all states. Therefore, we carefully implement a pipeline for automatic SOP generation. Our SOP generation framework is based on retrieval-augmented generation (RAG) [Lewis et al., 2020]. The SOP generation pipeline itself is also based on the AGENTS framework and has an SOP of first specifying the agents required, then planning the states and their connections, and finally generating the Components. Therefore, this pipeline can be regarded as a “meta agent” that can create other agents and multi-agent systems. Detailed description of the automatic agent creation framework is decribed in [Zhou et al., 2023b]. *https://fastapi.tiangolo.com/ --- --4 Case Studies We then present a few case studies on different language agents built with the library, including single-agent systems, multi-agent systems, and systems that require human-agent interaction. All demos are available at http: //www.aiwaves-agents.com/. 4.1 Single-agent systems We implement a few single-agent systems with AGENTS including a chit-chat bot, two customer service agents based on knowledge bases and web search engines, a shopping assistant agent, and a sales agent. The agents demonstrate different features of the library and the possibility of building language agents of different use cases using AGENTS. We present a screenshot of the customer service agent and the sales agent in Figure 2 and 3, respectively. 4.2 Multi-agent systems We also demonstrate how one can build a multi-agent system consisting of multiple agents interacting with each other in an environment. We select three scenarios including a fiction studio, a debate, and a software company. These scenarios include both cooperative and competitive scenarios, which are two main categories of multi-agent systems. All of the scenarios include multiple subtasks that are controlled through symbolic plans, i.e., SOPs. One can easily observe the language agents’ behavior in each subtask and engineer the corresponding prompts to customize and improve the system. We present a system screenshot of the fiction studio system in Figure 4. We also showcase the human-agent interaction feature of the framework in a case study where a human user participate in a debate with language agents in Figure 5. 5 Conclusion LLMs and language agents powered by them are playing increasingly important roles in both the NLP/AI community and our society in general. AGENTS, is a unified framework and open-source library for language agents. AGENTS aims to facilitate developers to build applications with language agents, researchers to conduct language agents research, and general non-technical audiences to build and customize personalized language agents. References Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In /nternational workshop on agent theories, architectures, and languages, pages 21-35. Springer, 1996. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https: //proceedings. neurips.cc/paper/2020/file/1457cOd6bf cb4967418bf b8ac142f64a-Paper . pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=TG8KACxEON. OpenAI. GPT-4 technical report, 2023. --- --Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https: //lilianweng. github. io/posts/2023-06-23-agent/. Toran Bruce Richards and et al. Auto-gpt: An autonomous gpt-4
--- EXPERIMENT ---
s [Li et al., 2023], software development [Qian et al., 2023], etc. One new feature for multi-agent communication in AGENTS is the “dynamic scheduling” feature. Instead of --- --scheduling the order for the agents to act with hard-coded rules, dynamic scheduling provides an option to define a controller agent that acts as a “moderator” and decides which agent to perform the next action considering their roles and the current history. Dynamic scheduling has the potential to make communication between multiple agents more natural and flexible. Developers can easily customize the controller by specifying its rule in the config file using natural language. Human-agent interaction One limitation in existing agent frameworks is that while they enable agents, or multi-agents, to automatically solve tasks, it’s not easy or even possible for human users to interact with the agents, especially in the multi-agent scenario. AGENTS seamlessly supports human-agent interaction in both single-agent and multi-agent scenarios, making it possible for one or more humans to communicate and interact with language agents. Controllabilty Existing agent frameworks generally define and control the agents’ behavior only using a system prompt and then let the agent plan and act on its own. In contrast, AGENTS provides a novel paradigm to build controllable agents via a symbolic plan, also referred to as standard operating procedures (SOPs). An SOP is a graph of multiple states that defines different situations an agent may encounter while accomplishing a task, and the transition rules between the states. Similar to SOPs in the real world, an SOP in AGENTS is a meticulously documented set of step-by-step instructions that outlines how a particular task or process should be performed by an agent or a group of agents. SOPs can be generated by an LLM and edited by the user when customizing and tuning the agent. After deployment, an agent will behave following specified instructions and guidelines for each state and dynamically adjust its current state according to its interaction with the environment, humans, or other agents. The introduction of the symbolic plan offers the opportunity to provide fine-grained control of an agent’s behavior, making agents’ behavior more stable/predictable and facilitating tuning/optimizing agents at the same time. In addition, we propose an automated SOP generation pipeline to reduce human labor on writing detailed SOP and config files when customizing (multi-) agent systems. The automated SOP generation pipeline is a “meta agent” that can generate config files for language agents with retrievalaugmented generation given a short description of the task. AGENTS is an ongoing effort maintained by researchers and engineers from AIWaves?. We look forward to support from community contributors on the project. The library and detailed documentation and tutorials are available on GitHub. 2 Related Work 2.1 Autonomous Language Agents The concept of language agents has become very popular recently and a variety of language agents targeting different tasks have been proposed. For example, Generative Agents [Park et al., 2023] developed language agents to mimic human social behavior, WebAgent [Gur et al., 2023] demonstrated the possibility to build language agents that can complete the tasks on real websites following natural language instructions, Qian et al. [2023] and MetaGPT [Hong et al., 2023] experimented with software development in multi-agent communication settings, and Zhou et al. [2023a] built language agents that act as interactive writing assistants. In addition to language agents that target specific tasks, recent open-source projects such as AutoGPT [Richards and et al., 2023], BabyAGI [Nakajima, 2023], and SuperAGI [SuperAGI, 2023] are aimed at the goal of building autonomous agents that do whatever users want and attracted massive interest from both developers and non-specialist audiences. 2.2 Language Agents Frameworks More recently, a few open-source frameworks for language agents have been proposed. For example, Transformers Agents [Wolf et al., 2020] builds language agents that can automatically use tools to solve tasks described in natural language; LangChain [LangChain, 2022] supports end-to-end “https: //www.aiwaves.org/ Shttps: //github.com/aiwaves-cn/agents --- --ANnhwWNne CADMEWNK Table 1: Comparison of Language Agent Frameworks Framework Tool Usage Long-short Term Memory Multi-Agent Human-Agent Interaction Symbolic Control Transformers Agents v x x x x LangChain v v x x x Auto-GPT v x x x x Gentopia v x x x x XLang v x x x x Meta-GPT v x v x x Camel v x v x x AgentVerse v v v v x AGENTS v v v v v language agents that can automatically solve tasks specified in natural language; Camel [Li et al., 2023] and AgentVerse [Chen et al., 2023] are platforms tailored for building multi-agent systems; Gentopia [Xu et al., 2023] and XLang* are libraries for building tool-augmented agents. We illustrate the key features supported by these platforms and AGENTS in Table 1. We can see that AGENTS is the only framework that supports tool usage, long-short term memory, and multi-agent communication at the same time. AGENTS also offers human-agent interaction and controllability through symbolic plans (SOPs) for the first time. 3 Library Design Code 1: Exemplar code for initializing and running a (multi) agent system with AGENTS def main() # agents is a dict of one or multiple agents. agents = Agent.from_config(""./config.json"") sop = SOP.from_config(""./config. json"") environment = Environment.from_config(""./config. json"") run(agents ,sop, environment) AGENTS is designed following the philosophy in Franklin and Graesser [1996]: “an autonomous agent is situated in an environment”. Therefore, agent and environment are two major classes in the AGENTS framework. In addition to these two classes, we also include a class for symbolic plans, named SOP (short for Standard Operating Procedure), to make language agents more controllable. These main classes are all initialized from a config file which can be filled in plain text. In sum, a typical script for initializing and running a (multi) agent system with AGENTS is illustrated in Code 1. The config file not only defines these core objects but also factorizes complicated prompts into modularized prompt components. The factorization of prompts significantly reduces the expertise requirements and efforts for users to build (multi) agent systems. Using a single config file to define the agents, plan, and basic environment also facilitates the sharing of language agents (which will be discussed in the Agent Hub section). Each of these three core classes consist of standardized APIs that can be overwritten by experienced developers and researchers. We describe these classes in detail: Code 2: Exemplar code for the running loop of a (multi) agent system in AGENTS def run(agents ,sop,environment): while not sop.finished: agent ,state=sop.step(agents, environment) action=agent.step(state ,environment) environment .update (agent , action) # optional, in case of dynamic planning # new_states = get_new_states (action) # sop.add_states (new_states) ‘https: //github.com/xlang-ai/xlang --- --3.1 Agent The Agent class abstracts a language agent. Its UML is illustrated in Figure 1. We can see that an agent maintains its long-short term memory and has methods to observe the environment (agent ._observe (environment) ), act according to its current state (agent ._act ()) and update its memory (agent. _update_memory()). All these methods are wrapped in the agent .step() method. This factorization enables developers to customize agents with new functionalities easily. Unlike existing language agent frameworks that assume an agent must be based on an LLM, we include a “_is_human” property to an agent. If it is set to “True”, the (agent ._act ()) will opt to provide observations and memory information to a human user and wait for the human user to input the action. This design allows flexible human-agent interaction in both single-agent and multi-agent systems by allowing human users to take the role of one or more language agents. It facilitates developers to build various interesting applications such as allowing human users to act as members of a team in debate and collaborate with (agent or human-based) teammates to beat another team, or act as CTO/engineers in a software company and collaborate with others for software development. 3.2 SOP The SOP class contains a graph of the states of agents. Each state specifies a certain sub-task or sub-goal of all agents when accomplishing the task described by the SOP. States are abstracted into a State class. A State object contains modularized prompts for the agent to leverage an LLM and various tools or APIs that an agent can use in the state. We abstract everything an agent may use for action in a state into a “Component” class. The Component class consists of two subclasses corresponding to different parts of the prompt and tools or external APIs, named “PromptComponent” and “ToolComponent”, respectively. PromptComponent includes modularized prompts that specify the task/goal, rules/constraints, (step-by-step) demonstrations for in-context learning, and the output format. ToolComponent supports more complex usage beyond modularized prompts, including external tools and APIs such as web search, knowledge bases, etc. The results of the tools are either included in the prompt or directly returned and processed afterward, according to the config file. An SOP object also includes an LLM-based control function that decides the transition between different states and the next agent to act. The state transit function is named sop._transit() and the agent routing function is named sop._route(). Both of the functions are wrapped in an sop.next() function which is used in the main loop. send Restart Figure 2: (a) Customer service agent Figure 3: (b) Sales agent 3.3. Environment The Environment class abstracts the environment in which the agents are situated. An environment consists of two main functions: environment._observed() and environment.update(). environment ._observed() defines how the environment influences the agent’s action (i.e., what information should be transferred to the agent upon observation, and environment . update () defines how the agent’s action impacts the environment. --- --oqo 2 3 n 5Write Character Settings and Expand the first chapter Expand the second chapter Expand the third chapter Expand the forth chapter Expand the fifth chapter Script Outlines # a a a a a Zoe 4 Recorder i Character Design: DONE é BB Total Number:1D) character © Name: Mike Smith QDs Gender: Male £8 Work: internet Company Programmer Qase:Mike isa hardworking and dedicated individual. Hei intelligent, logical, and detail-oriented. He can bea bitreserved and introverted, the deeply cares for his family. ® Speaking Style: Mike speaks in a calm and ‘measured tone. Heis articulate and precise his words. ©) Relation with Others: Mike is married to Jane and they have a child together. He has a close relationship with is family and values their happiness above alelse. (Bh Character Background: Mike has been working at the internet company fr the past 10 years. He is passionate about his work an takes pride in his programming skills, However, he often finds himself struggling to find a balance between his demanding jaband spending quality time with his family. This conflict creates tension and challenges for Mike throughout the story 1 characterFigure 4: Multi-Agent System: Fiction Studio. The execution logic of a (multi) agent system based on AGENTS is very intuitive. As illustrated in Code 2, in each iteration, the SOP first decides the state transition and selects the next agent to act based on the agents and the environment. The agent then takes an action based on its state and the environment. Then the environment updates itself based on the new action. Finally, if a workflow requires dynamically adjusting the plan based on the intermediate execution results, one can parse the output from an action, define a new state and add it into the current SOP. 3.4 Implementation Details of Core Features Long-short Term Memory: AGENTS implements long-short term memories for language agents following Zhou et al. [2023a]. Specifically, long-term memories are action histories and are embedded by sentence-transformers [Reimers and Gurevych, 2019], stored in a VectorDB, and queried via semantic search. Short-term memories, or working memories, are in natural language form and updated by an LLM via a carefully tuned prompt. Tool Usage & Web Navigation : AGENTS supports tool usage and web navigation via ToolComponents. For each external tool or API, developer can wrap the API call in theToolComponent .func() method. For complicated tools of which the API call is contextdependent, AGENTS integrates the the “Function-calling” feature of OpenAI’s GPT APIs to let LLMs decide how to use the tools. Web navigation is achieved by implementing web search as a specialized tool. Multi-Agent Communication —: Different from most existing frameworks for multi-agent systems that use pre-defined rules (e.g., let each agent act in a sequential order) to control the order for agents’ action, AGENTS includes a controller function that dynamically decides which agent will perform the next action using an LLM by considering the previous actions, the environment, and the target of the current states. This makes multi-agent communication more flexible. Human-Agent Interaction : AGENTS supports human-agent interaction in multi-agent systems by allowing human users to change the “is_human” field for a certain agent in the config file to “True”. In this case, the user can play the role of the agent by himself/herself and input his/her own actions and interact with other language agents in the environment. --- --Debate Topic: Should Al Replace Humans in Creative Fields?? 6 James(Afrmative Second) ary, to defend your identity, youcan emphasize the unique qualtes and espectvsthat humans bring create els suchas intulton erations, ‘nd the ait to understand complex human experiences Highlight the value of Al should replace humans in creative fields because it human connection and heathen that comes fom human crested ar and can produce art and content efficiently, reduce costs, sonnet. and eliminate human bias. Affirmative viewpoint: Negative viewpoint Al should not replace humans in creative fields as it lacks true creativity, emotions, and the ability to understand complex human experiences. User'agent Audience David(Debate_Judge) Emily(Negative_First) James(Affirmative_Second) Jennifer(Affirmative_Third) John(Affirmative_Debate_organizer) © Maryiatfirmative_First) Michael(Negative_Debate_organizer) ‘Sarah(Negative_Third) William(Negative_Second) Figure 5: Human-Agent Interaction in a debate. 3.5 Deployment Existing open-source frameworks for language agents focus on building proof-of-concept language agents that run either in the terminal or on Gradio [Abid et al., 2019]. In contrast, AGENTS supports deploying language agents as APIs with FastAPI°. This greatly facilitates developers to integrate language agents in real-world applications. 3.6 The Agent Hub AGENTS aims to not only facilitate the development, testing, and tuning of a language agents system but also makes the distribution and sharing of language agents easier. To this end, we introduce AGENT HuB, a platform that allows users to share their fine-tuned language agents as well as search/download useful language agents that others share on the platform. In this way, one can easily customize language agents by starting from community agents and slightly modifying them. This greatly reduces the effort of designing, testing, and tuning language agents from scratch. 3.7 Automatic Creation of Agent Systems While using an SOP to provide fine-grained control to language agents, it can sometimes be laborsome for users to manually specify the SOP from scratch since it requires to set different states, their connections, and the prompts and tools for each Component for all states. Therefore, we carefully implement a pipeline for automatic SOP generation. Our SOP generation framework is based on retrieval-augmented generation (RAG) [Lewis et al., 2020]. The SOP generation pipeline itself is also based on the AGENTS framework and has an SOP of first specifying the agents required, then planning the states and their connections, and finally generating the Components. Therefore, this pipeline can be regarded as a “meta agent” that can create other agents and multi-agent systems. Detailed description of the automatic agent creation framework is decribed in [Zhou et al., 2023b]. *https://fastapi.tiangolo.com/ --- --4 Case Studies We then present a few case studies on different language agents built with the library, including single-agent systems, multi-agent systems, and systems that require human-agent interaction. All demos are available at http: //www.aiwaves-agents.com/. 4.1 Single-agent systems We implement a few single-agent systems with AGENTS including a chit-chat bot, two customer service agents based on knowledge bases and web search engines, a shopping assistant agent, and a sales agent. The agents demonstrate different features of the library and the possibility of building language agents of different use cases using AGENTS. We present a screenshot of the customer service agent and the sales agent in Figure 2 and 3, respectively. 4.2 Multi-agent systems We also demonstrate how one can build a multi-agent system consisting of multiple agents interacting with each other in an environment. We select three scenarios including a fiction studio, a debate, and a software company. These scenarios include both cooperative and competitive scenarios, which are two main categories of multi-agent systems. All of the scenarios include multiple subtasks that are controlled through symbolic plans, i.e., SOPs. One can easily observe the language agents’ behavior in each subtask and engineer the corresponding prompts to customize and improve the system. We present a system screenshot of the fiction studio system in Figure 4. We also showcase the human-agent interaction feature of the framework in a case study where a human user participate in a debate with language agents in Figure 5. 5
--- CONCLUSION ---
LLMs and language agents powered by them are playing increasingly important roles in both the NLP/AI community and our society in general. AGENTS, is a unified framework and open-source library for language agents. AGENTS aims to facilitate developers to build applications with language agents, researchers to conduct language agents research, and general non-technical audiences to build and customize personalized language agents. References Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In /nternational workshop on agent theories, architectures, and languages, pages 21-35. Springer, 1996. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.-F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https: //proceedings. neurips.cc/paper/2020/file/1457cOd6bf cb4967418bf b8ac142f64a-Paper . pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=TG8KACxEON. OpenAI. GPT-4 technical report, 2023. --- --Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https: //lilianweng. github. io/posts/2023-06-23-agent/. Toran Bruce Richards and et al. Auto-gpt: An autonomous gpt-4 experiment, 2023. URL https: //github.com/Significant-Gravitas/Auto-GPT. [Software]. Yohei Nakajima. Babyagi, 2023. URL https: //github.com/yoheinakajima/babyagi. [Software]. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2022. URL https: //openreview.net/forum?id=iedYJm9200a. Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023a. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761, 2023. Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In NJPS, pages 2137-2145, 2016. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for ""mind"" exploration of large scale language model society, 2023. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis, 2023. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework, 2023. SuperAGI. Superagi, 2023. URL https: //github.com/Transformer0ptimus/SuperAGI. [Software]. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6. LangChain. Langchain repository. https: //github.com/langchain-ai/langchain, 2022. --- --Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents, 2023. Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. Gentopia: A collaborative platform for tool-augmented Ilms, 2023. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908. 10084. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv: 1906.02569, 2019. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rocktischel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS, 2020. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Towards language agents uniting connectionism and symbolism. 2023b. To be published.
"	"--- ABSTRACT ---
대규모 언어 모델(LLM)에 대한 최근의 발전으로 연구자와 개발자는 자연어 인터페이스를 사용하여 다양한 작업을 자동으로 해결하고 환경, 사람 및 기타 에이전트와 상호 작용할 수 있는 자율 언어 에이전트를 구축할 수 있습니다. 우리는 언어 에이전트를 인공 일반 지능에 대한 유망한 방향으로 간주하고 이러한 발전을 더 광범위한 비전문가 대상에게 공개하는 것을 목표로 하는 오픈 소스 라이브러리인 AGENTS를 출시합니다. AGENTS는 계획, 메모리, 도구 사용, 다중 에이전트 통신 및 세분화된 심볼릭 제어를 포함한 중요한 기능을 지원하도록 신중하게 설계되었습니다. AGENTS는 비전문가가 많은 코딩 없이도 최첨단 자율 언어 에이전트를 빌드, 사용자 지정, 테스트, 조정 및 배포할 수 있으므로 사용자 친화적입니다. 이 라이브러리는 모듈화된 설계로 연구자가 쉽게 확장할 수 있으므로 연구 친화적이기도 합니다. AGENTS는 https://github.com/aiwaves-cn/agents에서 사용할 수 있습니다. 1
--- INTRODUCTION ---
&quot;자율 에이전트는 환경 내부에 위치하고 그 일부인 시스템으로, 환경을 감지하고 시간이 지남에 따라 환경에 따라 행동하여 자체 의제를 추구하고 미래에 감지한 내용을 실행합니다.&quot; 에이전트인가, 아니면 프로그램인가?: 자율 에이전트에 대한 분류법 [Franklin 및 Graesser, 1996] ChatGPT와 같은 대규모 언어 모델(LLM) [Brown 등, 2020, Ouyang 등, 2022, OpenAI, 2023]을 사용하면 복잡한 작업을 자동으로 해결하고 세상에서 인지, 추론, 계획 및 행동하여 환경, 인간 또는 다른 에이전트와 상호 작용할 수 있는 자율 에이전트를 구축할 수 있습니다 [Weng, 2023]. 언어 에이전트는 인공 일반 지능(AGI)을 향한 유망한 단계이며 고객 서비스, 컨설팅, 프로그래밍, 글쓰기, 교육 등과 같은 특정 역할에서 인간의 노력을 줄이는 데 도움이 될 수 있습니다. AutoGPT [Richards 외, 2023] 및 BabyAGI [Nakajima, 2023]와 같은 최근의 일부 데모는 언어 에이전트의 잠재력을 보여주었고 개발자, 연구자 및 비기술적인 대상으로부터 엄청난 관심을 얻었습니다. 흥미진진하지만 이러한 데모 또는 저장소의 대부분은 숙련된 개발자 또는 연구자에게도 새로운 에이전트를 사용자 지정, 조정 및 배포하기에 적합하지 않습니다. 이러한 제한은 이러한 데모가 일반적으로 언어 에이전트의 가능성을 보여주는 개념 증명인 반면 시간이 지남에 따라 언어 에이전트를 빌드하고 사용자 지정하는 데 사용할 수 있는 더 큰 프레임워크가 아니라는 사실에서 비롯됩니다. 게다가, 이러한 오픈소스 저장소의 대부분은 작업 분해[Nye et al., 2022], 장단기 기억[Zhou *Equal Contribution. Correspondence to: chunshu@aiwaves.cn Preprint. Work in progress. Web Navigation Planning OO Planning Web Navigation Multi-Agent ○ ○ AgentCommunication Agent(eg Writer) Tool Use Tool Use(eg Editor) SOP Human-Agent Interaction Figure 1: AGENTS framework의 예. et al., 2023a], 웹 탐색[Nakano et al., 2021], 도구 사용[Schick et al., 2023], 다중 에이전트 커뮤니케이션[Foerster et al., 2016]을 포함하여 언어 에이전트의 핵심 기능 중 일부만을 다룹니다. 게다가, 대부분(전부는 아니더라도)의 기존 언어 에이전트 프레임워크는 짧은 작업 설명에만 의존하고 LLM의 계획 및 행동 능력에 전적으로 의존합니다. 이로 인해 여러 실행에서 상당한 무작위성과 불일치가 발생하여 만족스럽지 못한 사용자 경험을 제공하고 언어 에이전트를 사용자 지정하고 조정하기 어렵게 만듭니다.우리는 앞서 언급한 제한 사항이 언어 에이전트의 최근 발전이 더 광범위한 비전문가 대상에 도달하고 사회에 긍정적인 영향을 미치는 데 중요한 장벽이라고 생각합니다.이를 위해 LLM 기반 언어 에이전트를 지원하는 언어 에이전트용 오픈 소스 라이브러리 및 프레임워크인 AGENTS를 출시합니다.AGENTS의 철학은 비전문가에게도 언어 에이전트의 사용자 지정, 조정 및 배포를 가능한 한 간단하게 만드는 동시에 개발자와 연구자에게도 쉽게 확장할 수 있도록 하는 것입니다.또한 라이브러리는 언어 에이전트를 위한 다재다능한 프레임워크로 만드는 다음과 같은 주요 기능도 제공합니다.장단기 메모리 Franklin과 Graesser[1996]에 따르면 자율 에이전트와 컴퓨터 프로그램(또는 머신 러닝 모델)의 주요 차이점은 머신 러닝 모델은 단일 입력/쿼리에만 응답하면 되는 반면 자율 에이전트는 시간이 지남에 따라 환경이나 다른 에이전트와 상호 작용해야 한다는 것입니다. 따라서 자율 에이전트에게 장단기 메모리를 유지하는 능력은 매우 중요합니다. AGENTS는 [Zhou et al., 2023a]의 메모리 구성 요소를 통합하고 언어 에이전트가 VectorDB 및 의미 검색을 사용하여 장기 메모리를 저장하고 검색하고 스크래치패드로 단기 작업 메모리를 정기적으로 업데이트할 수 있도록 합니다. 사용자는 구성 파일의 필드를 채우기만 하면 에이전트에 장기 메모리, 단기 메모리 또는 둘 다를 장착할 수 있습니다. 도구 사용 및 웹 탐색 자율 에이전트의 또 다른 중요한 기능은 외부 도구를 사용하고 인터넷을 서핑할 수 있는 기능입니다. 이는 언어 에이전트에게 특히 중요한데, 언어 인터페이스에 의존하기 때문에 언어 커뮤니케이션을 넘어서는 환경과 상호 작용하고 웹을 탐색하여 유용한 정보를 수집하기 위해 외부 도구를 사용해야 하기 때문입니다. [Patil et al., 2023]에 따라 AGENTS는 일반적으로 사용되는 몇 가지 외부 API를 지원하고 개발자가 다른 도구를 쉽게 통합할 수 있는 추상 클래스를 제공합니다. 또한 웹 검색 및 웹 탐색을 특수 API로 정의하여 에이전트가 인터넷을 탐색하고 정보를 수집할 수 있도록 합니다. 다중 에이전트 커뮤니케이션 단일 에이전트 기능 외에도 AGENTS는 다중 에이전트 시스템을 사용자 정의하는 것을 지원하며, 이는 게임[Park et al., 2023], 사회 실험[Li et al., 2023], 소프트웨어 개발[Qian et al., 2023] 등과 같은 특정 애플리케이션에 도움이 될 수 있습니다. AGENTS의 다중 에이전트 커뮤니케이션을 위한 새로운 기능 중 하나는 &quot;동적 스케줄링&quot; 기능입니다. 하드 코딩된 규칙으로 에이전트의 작업 순서를 스케줄링하는 대신, 동적 스케줄링은 &quot;중재자&quot; 역할을 하는 컨트롤러 에이전트를 정의하고 역할과 현재 기록을 고려하여 다음 작업을 수행할 에이전트를 결정하는 옵션을 제공합니다. 동적 스케줄링은 여러 에이전트 간의 커뮤니케이션을 보다 자연스럽고 유연하게 만들 수 있는 잠재력이 있습니다. 개발자는 자연어를 사용하여 구성 파일에 규칙을 지정하여 컨트롤러를 쉽게 사용자 정의할 수 있습니다. 인간-에이전트 상호작용 기존 에이전트 프레임워크의 한 가지 한계는 에이전트 또는 다중 에이전트가 자동으로 작업을 해결할 수 있도록 하지만 인간 사용자가 에이전트와 상호 작용하는 것이 쉽지 않거나 가능하지 않다는 것입니다.특히 다중 에이전트 시나리오에서 더욱 그렇습니다.AGENTS는 단일 에이전트와 다중 에이전트 시나리오에서 인간-에이전트 상호작용을 원활하게 지원하여 한 명 이상의 인간이 언어 에이전트와 통신하고 상호 작용할 수 있도록 합니다.제어 가능성 기존 에이전트 프레임워크는 일반적으로 시스템 프롬프트를 사용하여 에이전트의 동작을 정의하고 제어한 다음 에이전트가 스스로 계획하고 작동하도록 합니다.반대로 AGENTS는 표준 운영 절차(SOP)라고도 하는 상징적 계획을 통해 제어 가능한 에이전트를 구축하는 새로운 패러다임을 제공합니다.SOP는 에이전트가 작업을 수행하는 동안 마주칠 수 있는 다양한 상황과 상태 간의 전환 규칙을 정의하는 여러 상태의 그래프입니다. 실제 세계의 SOP와 유사하게 AGENTS의 SOP는 특정 작업이나 프로세스가 에이전트나 에이전트 그룹에서 수행되어야 하는 방법을 설명하는 단계별 지침의 세심하게 문서화된 집합입니다. SOP는 LLM에서 생성될 수 있으며 사용자가 에이전트를 사용자 지정하고 조정할 때 편집할 수 있습니다. 배포 후 에이전트는 각 상태에 대해 지정된 지침과 가이드라인에 따라 동작하고 환경, 인간 또는 다른 에이전트와의 상호 작용에 따라 현재 상태를 동적으로 조정합니다. 상징적 계획의 도입은 에이전트의 동작을 세부적으로 제어할 수 있는 기회를 제공하여 에이전트의 동작을 보다 안정적/예측 가능하게 만들고 동시에 에이전트를 조정/최적화하는 것을 용이하게 합니다. 또한 (다중) 에이전트 시스템을 사용자 지정할 때 자세한 SOP 및 구성 파일을 작성하는 데 드는 인적 노동을 줄이기 위해 자동화된 SOP 생성 파이프라인을 제안합니다. 자동화된 SOP 생성 파이프라인은 작업에 대한 간략한 설명이 주어진 검색 증강 생성을 통해 언어 에이전트에 대한 구성 파일을 생성할 수 있는 &quot;메타 에이전트&quot;입니다. AGENTS는 AIWaves²의 연구자와 엔지니어가 유지 관리하는 지속적인 노력입니다. 우리는 이 프로젝트에 대한 커뮤니티 기여자의 지원을 기대합니다. 라이브러리와 자세한 문서 및 튜토리얼은 GitHub³에서 제공됩니다.
--- RELATED WORK ---
2.1 자율 언어 에이전트 언어 에이전트의 개념은 최근에 매우 인기를 얻었으며 다양한 작업을 타겟으로 하는 다양한 언어 에이전트가 제안되었습니다. 예를 들어, 생성 에이전트[Park et al., 2023]는 인간의 사회적 행동을 모방하는 언어 에이전트를 개발했고, WebAgent[Gur et al., 2023]는 자연어 지침에 따라 실제 웹사이트에서 작업을 완료할 수 있는 언어 에이전트를 구축할 수 있는 가능성을 보여주었고, Qian et al.[2023]과 MetaGPT[Hong et al., 2023]는 다중 에이전트 커뮤니케이션 설정에서 소프트웨어 개발을 실험했고, Zhou et al.[2023a]는 대화형 쓰기 도우미 역할을 하는 언어 에이전트를 구축했습니다. 특정 작업을 타겟으로 하는 언어 에이전트 외에도 AutoGPT [Richards 등, 2023], BabyAGI [Nakajima, 2023], SuperAGI [SuperAGI, 2023]와 같은 최근의 오픈소스 프로젝트는 사용자가 원하는 모든 것을 수행하는 자율 에이전트를 구축하는 것을 목표로 하며 개발자와 비전문가 모두로부터 엄청난 관심을 끌었습니다.2.2 언어 에이전트 프레임워크 최근에는 언어 에이전트를 위한 몇 가지 오픈소스 프레임워크가 제안되었습니다.예를 들어, Transformers Agents [Wolf 등, 2020]는 자연어로 기술된 작업을 해결하기 위해 도구를 자동으로 사용할 수 있는 언어 에이전트를 구축합니다. LangChain[LangChain, 2022]은 종단 간 2https://www.aiwaves.org/ ³https://github.com/aiwaves-cn/agentsX X 프레임워크 변환기 에이전트를 지원합니다.표 1: 언어 에이전트 프레임워크 비교 도구 사용 장단기 메모리 다중 에이전트 인간-에이전트 상호 작용 기호 제어 LangChain Auto-GPT Gentopia XLang Meta-GPT Camel Agent Verse AGENTSX XX 자연어로 지정된 작업을 자동으로 해결할 수 있는 언어 에이전트입니다.Camel[Li et al., 2023]과 Agent Verse[Chen et al., 2023]는 다중 에이전트 시스템을 구축하는 데 맞춤화된 플랫폼입니다.Gentopia[Xu et al., 2023]와 XLang4는 도구 증강 에이전트를 구축하기 위한 라이브러리입니다. 표 1에서 이러한 플랫폼과 AGENTS가 지원하는 주요 기능을 설명합니다. AGENTS는 도구 사용, 장단기 메모리, 다중 에이전트 통신을 동시에 지원하는 유일한 프레임워크임을 알 수 있습니다. AGENTS는 또한 처음으로 심볼릭 플랜(SOP)을 통해 인간-에이전트 상호 작용과 제어 가능성을 제공합니다. 3 라이브러리 설계 코드 1: AGENTS를 사용하여 (다중) 에이전트 시스템을 초기화하고 실행하기 위한 예시 코드 def main() # agents는 하나 또는 여러 에이전트의 dict입니다. agents = sop = Agent.from_config(&quot;./config.json&quot;) SOP.from_config(&quot;./config.json&quot;) environment = Environment.from_config(&quot;./config.json&quot;) run (agents, sop, environment) AGENTS는 Franklin과 Graesser [1996]의 철학을 따라 설계되었습니다. &quot;자율 에이전트는 환경에 위치합니다.&quot; 따라서 agent와 environment는 AGENTS 프레임워크의 두 가지 주요 클래스입니다. 이 두 클래스 외에도 언어 에이전트를 보다 제어하기 쉽게 만들기 위해 SOP(Standard Operating Procedure의 약자)라는 기호 계획에 대한 클래스도 포함합니다. 이러한 주요 클래스는 모두 일반 텍스트로 채울 수 있는 구성 파일에서 초기화됩니다. 요약하면 AGENTS를 사용하여 (다중) 에이전트 시스템을 초기화하고 실행하기 위한 일반적인 스크립트는 코드 1에 나와 있습니다. 구성 파일은 이러한 핵심 객체를 정의할 뿐만 아니라 복잡한 프롬프트를 모듈화된 프롬프트 구성 요소로 인수분해합니다. 프롬프트의 인수분해는 사용자가 (다중) 에이전트 시스템을 구축하기 위해 필요로 하는 전문성 요구 사항과 노력을 크게 줄여줍니다. 단일 구성 파일을 사용하여 에이전트, 계획 및 기본 환경을 정의하면 언어 에이전트를 공유하는 것도 용이해집니다(에이전트 허브 섹션에서 논의). 이 세 가지 핵심 클래스는 각각 숙련된 개발자와 연구자가 덮어쓸 수 있는 표준화된 API로 구성됩니다. 이러한 클래스를 자세히 설명합니다. 코드 2: AGENTS에서 (다중) 에이전트 시스템의 실행 루프에 대한 예시 코드 def run (agents, sop, environment): while not sop.finished: agent, state sop.step(agents, environment) action=agent.step(state, environment) environment.update (agent, action) #23 동적 계획의 경우 선택 사항 #new_states = get_new_states (action) #sop.add_states (new_states)4 https://github.com/xlang-ai/xlang 3.1 에이전트 에이전트 클래스는 언어 에이전트를 추상화합니다. UML은 그림 1에 나와 있습니다. 에이전트가 장기-단기 메모리를 유지하고
--- METHOD ---
s는 환경을 관찰(agent. observe(environment)), 현재 상태에 따라 행동(agent. _ act())하고 메모리를 업데이트(agent. _update_memory())합니다. 이러한 모든 메서드는 agent.step() 메서드에 래핑됩니다. 이 인수분해를 통해 개발자는 에이전트를 새로운 기능으로 쉽게 사용자 정의할 수 있습니다. 기존 언어 에이전트 프레임워크와 달리 에이전트는 LLM을 기반으로 해야 한다고 가정하지만, 우리는 에이전트에 &quot;_is_human&quot; 속성을 포함합니다. &quot;True&quot;로 설정된 경우 (agent. _ act())는 인간 사용자에게 관찰 및 메모리 정보를 제공하고 인간 사용자가 작업을 입력할 때까지 기다립니다. 이 설계는 인간 사용자가 하나 이상의 언어 에이전트 역할을 수행할 수 있도록 하여 단일 에이전트 및 다중 에이전트 시스템 모두에서 유연한 인간-에이전트 상호 작용을 허용합니다. 개발자는 인간 사용자가 토론에서 팀원 역할을 하고 (에이전트 또는 인간 기반) 팀원과 협력하여 다른 팀을 이기거나 소프트웨어 회사의 CTO/엔지니어 역할을 하고 소프트웨어 개발을 위해 다른 사람과 협력하는 등 다양하고 흥미로운 애플리케이션을 구축할 수 있습니다. 3.2 SOP SOP 클래스에는 에이전트 상태의 그래프가 포함되어 있습니다. 각 상태는 SOP에서 설명하는 작업을 수행할 때 모든 에이전트의 특정 하위 작업 또는 하위 목표를 지정합니다. 상태는 State 클래스로 추상화됩니다. State 객체는 에이전트가 LLM과 에이전트가 상태에서 사용할 수 있는 다양한 도구나 API를 활용하도록 하는 모듈화된 프롬프트를 포함합니다. 우리는 에이전트가 상태에서 동작에 사용할 수 있는 모든 것을 &quot;Component&quot; 클래스로 추상화합니다. Component 클래스는 프롬프트와 도구 또는 외부 API의 다른 부분에 해당하는 두 개의 하위 클래스로 구성되며 각각 &quot;Prompt Component&quot;와 &quot;ToolComponent&quot;로 명명됩니다. PromptComponent에는 작업/목표, 규칙/제약 조건, 컨텍스트 내 학습을 위한 (단계별) 데모 및 출력 형식을 지정하는 모듈화된 프롬프트가 포함됩니다. ToolComponent는 웹 검색, 지식 기반 등과 같은 외부 도구 및 API를 포함하여 모듈화된 프롬프트를 넘어 더 복잡한 사용을 지원합니다. 도구의 결과는 프롬프트에 포함되거나 구성 파일에 따라 나중에 직접 반환되어 처리됩니다. SOP 객체에는 또한 다른 상태와 다음에 동작할 에이전트 간의 전환을 결정하는 LLM 기반 제어 함수가 포함됩니다. 상태 이동 함수는 sop._transit()으로 명명되고 에이전트 라우팅 함수는 sop._route()로 명명됩니다. 두 함수 모두 메인 루프에서 사용되는 sop.next() 함수에 래핑됩니다. Mike 안녕하세요, 도와드릴까요? 사용자 iPhone 15를 알고 계신가요? Mike 사용자 네, 준비되었습니다. 기쁩니다, Agod 씨! 약속을 진행하려면 약속 카드를 작성해 주시기 바랍니다. 다음 정보를 복사하여 작성한 후 저에게 보내주시면 예약을 완료해 드리겠습니다. Mike 네, iPhone 15에 대해 잘 알고 있습니다. iPhone 15 Pro Max는 48MP 기본 카메라, 12MP 초광각 카메라, 12MP 망원 카메라가 탑재된 Pro 수준의 카메라 시스템을 갖추고 있습니다. 또한 Always-On 및 ProMotion 기술이 적용된 Super Retina XDR 디스플레이를 탑재하여 뛰어난 시청 경험을 제공합니다. iPhone 15는 9월 15일부터 사전 주문이 가능하며 9월 22일에 출시됩니다. 고객은 최대 2대의 iPhone 15와 2대의 iPhone 15 Plus 기기를 구매할 수 있습니다. 가격은 월 250위안 또는 선불 5999위안부터 시작합니다. 또한 매입 및 할부 옵션도 이용할 수 있습니다. iPhone 15 Pro 및 Pro Max 모델은 프리미엄 티타늄으로 제작되어 역대 가장 가벼운 Pro 라인업입니다.[이름]: [전화번호]: [대략적인 위치]: [지구 학위]: 이 정보를 제공하면 약속을 잡고 다음 단계를 안내해 드리겠습니다.입력 내용을 입력하세요 보내기 다시 시작 그림 2: (a) 고객 서비스 담당자 그림 3: (b) 영업 담당자 3.3 환경 Environment 클래스는 담당자가 위치한 환경을 추상화합니다.환경은 두 가지 주요 함수인 environment._ observer() 및 environment.update().environment로 구성됩니다. _ watched the following table: ... sitting in the way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a 말하는 스타일: 마이크는 차분하고 절제된 어조로 말한다. 그는 말이 유창하고 정확하다. 다른 사람과의 관계: 마이크는 제인과 결혼했고 두 사람 사이에 아이가 있다. 그는 가족과 가까운 관계를 맺고 있으며 무엇보다도 그들의 행복을 소중히 여긴다. 캐릭터 배경: 마이크는 지난 10년 동안 인터넷 회사에서 일해 왔다. 그는 일에 열정적이며 프로그래밍 기술에 자부심을 느낀다. 그러나 그는 종종 힘든 일과 가족과 함께 보내는 양질의 시간 사이에서 균형을 찾기 위해 고군분투한다. 이러한 갈등은 이야기 전반에 걸쳐 마이크에게 긴장과 도전을 안겨준다. 조이 캐릭터 디자인: 완료 캐릭터 필요: 2명의 주요 캐릭터와 2명의 보조 캐릭터를 포함한 4명의 캐릭터 개요: 1장: 균형 잡기 -단어 수:관련 캐릭터: 마이크 스미스, 제인 블랙, 에밀리 스미스 - 요약: 1장에서 마이크와 제인은 일과 가족의 책임의 회오리에 갇힌다. 마이크는 끊임없이 직업의 요구에 압도당하는 반면, 제인은 교사로서의 경력과 어머니로서의 역할 사이에서 균형을 맞추기 위해 고군분투한다. 그들의 의사소통과 이해 부족은 가족 내에서 긴장과 갈등으로 이어진다. 한편, 어린 딸 에밀리는 부모님과 함께 보내는 양질의 시간을 갈구한다. 이 장에서는 Mike와 Jane이 시작할 자기 발견과 성장의 여정을 위한 무대를 마련한다. Abby 현재 목표: 개요 디자인 디자인 근거: Zoe의 제안을 바탕으로 이제 대본의 개요 디자인을 진행하겠습니다. Elmo와 Zoe가 제공한 제안을 분석하여 개요에 통합하여 현대 가족 윤리에 대한 설득력 있는 이야기를 만들 것입니다. ID:장 제목: 1장: 균형 잡기 디자인 근거: 이 장에서는 주인공 Mike와 Jane을 소개하고 이야기의 중심 갈등인 일과 가정의 균형을 맞추려는 투쟁을 설명합니다. 그림 4: 다중 에이전트 시스템: Fiction Studio. 기록자 총 인원: 이름: Mike Smith 성별: 남성 캐릭터 업무: 인터넷 회사 프로그래머 나이: Mike는 근면하고 헌신적인 사람입니다. 그는 지적이고 논리적이며 세부 사항 지향적입니다. 그는 약간 내성적이고 내성적일 수 있지만, 그는 가족을 깊이 사랑합니다.말하는 스타일: Mike는 차분하고 절제된 어조로 말합니다.그는 말이 유창하고 정확합니다.다른 사람과의 관계: Mike는 Jane과 결혼했고 그들 사이에 아이가 있습니다.그는 가족과 가까운 관계를 맺고 있으며 무엇보다도 그들의 행복을 소중히 여깁니다.캐릭터 배경: Mike는 지난 10년 동안 인터넷 회사에서 일해 왔습니다.그는 자신의 일에 열정적이며 프로그래밍 기술에 자부심을 가지고 있습니다.그러나 그는 종종 힘든 일과 가족과 함께 보내는 양질의 시간 사이에서 균형을 찾기 위해 고군분투합니다.이러한 갈등은 스토리 전반에 걸쳐 Mike에게 긴장과 도전을 안겨줍니다.캐릭터AGENTS에 기반한 (다중) 에이전트 시스템의 실행 논리는 매우 직관적입니다.코드 2에서 설명한 것처럼 각 반복에서 SOP는 먼저 상태 전환을 결정하고 에이전트와 환경에 따라 다음 에이전트를 선택합니다.그런 다음 에이전트는 상태와 환경에 따라 조치를 취합니다.그런 다음 환경은 새 조치에 따라 자체를 업데이트합니다. 마지막으로, 워크플로가 중간 실행 결과에 따라 계획을 동적으로 조정해야 하는 경우 작업의 출력을 구문 분석하고 새 상태를 정의하여 현재 SOP에 추가할 수 있습니다.3.4 핵심 기능의 구현 세부 정보 장단기 메모리: AGENTS는 Zhou et al. [2023a]에 따라 언어 에이전트에 대한 장단기 메모리를 구현합니다.특히, 장기 메모리는 작업 내역이며 문장 변환기[Reimers and Gurevych, 2019]에 의해 내장되어 VectorDB에 저장되고 의미 검색을 통해 쿼리됩니다.단기 메모리 또는 작업 메모리는 자연어 형태이며 신중하게 조정된 프롬프트를 통해 LLM에 의해 업데이트됩니다.도구 사용 및 웹 탐색: AGENTS는 ToolComponents를 통해 도구 사용 및 웹 탐색을 지원합니다.각 외부 도구 또는 API에 대해 개발자는 ToolComponent.func() 메서드에서 API 호출을 래핑할 수 있습니다. API 호출이 컨텍스트에 따라 달라지는 복잡한 도구의 경우 AGENTS는 OpenAI의 GPT API의 &quot;함수 호출&quot; 기능을 통합하여 LLM이 도구를 사용하는 방법을 결정하도록 합니다. 웹 탐색은 웹 검색을 특수 도구로 구현하여 달성됩니다. 다중 에이전트 통신: 사전 정의된 규칙(예: 각 에이전트가 순차적으로 작동하도록 함)을 사용하여 에이전트의 작업 순서를 제어하는 대부분의 기존 다중 에이전트 시스템 프레임워크와 달리 AGENTS에는 이전 작업, 환경 및 현재 상태의 대상을 고려하여 LLM을 사용하여 다음 작업을 수행할 에이전트를 동적으로 결정하는 컨트롤러 기능이 포함되어 있습니다. 이를 통해 다중 에이전트 통신이 더 유연해집니다. 인간-대리인 상호작용: AGENTS는 인간 사용자가 구성 파일에서 특정 에이전트의 &quot;is_human&quot; 필드를 &quot;True&quot;로 변경할 수 있도록 하여 다중 에이전트 시스템에서 인간-대리인 상호작용을 지원합니다. 이 경우 사용자는 스스로 에이전트 역할을 수행하고 자신의 동작을 입력하고 환경의 다른 언어 에이전트와 상호 작용할 수 있습니다. 토론 주제: 창의적인 분야에서 AI가 인간을 대체해야 할까요?? 사용자 내 정체성을 방어하기 위해 무엇을 해야 할지 생각해 보세요. David(토론 심사 위원) 이제 선택한 주제에 따라 순차적인 토론을 진행하세요. 긍정적 관점: AI는 창의적인 분야에서 인간을 대체해야 합니다. 왜냐하면 효율적으로 예술과 콘텐츠를 생산하고 비용을 절감하며 인간의 편견을 없앨 수 있기 때문입니다. James(긍정적 2번째) Mary, 정체성을 방어하기 위해 직관, 감정, 복잡한 인간 경험을 이해하는 능력과 같이 인간이 창의적인 분야에 가져오는 고유한 특성과 관점을 강조할 수 있습니다. 인간적 연결의 가치와 인간이 만든 예술과 콘텐츠에서 나오는 진정성을 강조하세요. 부정적 관점: AI는 진정한 창의성, 감정, 복잡한 인간 경험을 이해하는 능력이 부족하기 때문에 창의적인 분야에서 인간을 대체해서는 안 됩니다.Jennifer(긍정적_세 번째) Mary, 자신의 정체성을 지키기 위해 예술과 콘텐츠 창작의 경계를 넓히는 데 있어 인간의 창의성과 혁신의 중요성을 강조할 수도 있습니다.인간의 상상력과 틀에서 벗어난 사고능력의 대체 불가능한 역할을 강조하세요.또한 창의적인 분야에서 AI에만 의존하는 것의 윤리적 의미에 대해 논의할 수 있습니다.User&#39;agent Audience David(토론 심판) Emily(부정적_첫 번째) James(긍정적_두 번째) Jennifer(긍정적_세 번째) John(긍정적_토론_조직자) Mary(긍정적_첫 번째) Michael(부정적_토론_조직자) Sarah(부정적_세 번째) William(부정적_두 번째) 실행 사용자 여러분, 이 기회를 주셔서 감사합니다.오늘 저는 여러분 앞에 서서 창의적인 분야에서 AI가 인간을 대체해서는 안 된다고 주장합니다. Al이 확실히 효율성과 비용상의 이점을 제공하지만, 진정한 창의성은 우리의 감정과 복잡한 인간 경험에 대한 이해에서 태어나는 고유한 인간적 특성이라는 점을 기억하는 것이 중요합니다. John(Affirmative_Debate_organizer) Mary, 창의적인 분야에서 인간의 고유한 특성을 강조하는 것 외에도 인간의 직관과 복잡한 감정을 해석하고 전달하는 능력의 중요성에 대해서도 논의할 수 있습니다. 인간적 연결의 가치와 인간이 작업에 가져오는 개인적인 접촉을 강조하세요. 또한 창의적인 분야에서 Al에만 의존하는 것의 잠재적인 한계와 윤리적 우려 사항을 살펴볼 수 있습니다. David(Debate Judge) 이제 선택한 주제에 대한 무료 토론에 참여하세요. Emily(Negative_First) 신사 숙녀 여러분, 이 기회를 주셔서 감사합니다. 오늘은 Al이 창의적인 분야에서 인간을 대체해서는 안 된다고 주장하겠습니다. Al은 효율성과 비용상의 이점을 제공할 수 있지만, 창의적인 작업의 중요한 측면인 복잡한 인간의 감정을 진정으로 이해하고 표현하는 능력이 부족합니다. 사용자 이 무료 토론에서, 창작 분야에서 Al의 역할이 전부 아니면 전무의 시나리오가 될 필요는 없다는 것을 기억합시다. 우리는 인간 창의성의 본질을 보존하면서 Al의 잠재력을 활용할 수 있습니다. Al과 인간 간의 협업은 전례 없는 혁신과 예술적 표현으로 이어질 수 있습니다. Al과 인간 창의성의 강점을 결합하여 미래를 포용합시다. 사용자 물론입니다, William. 창작 분야에서 Al을 둘러싼 잠재적 위험과 윤리적 우려 사항을 해결하는 것이 중요합니다. 그러나 잠재적 이점을 완전히 무시하지 않는 것도 마찬가지로 중요합니다. 핵심은 책임감 있고 세심한 통합에 있습니다. Al은 도구 역할을 하여 인간의 창의성을 가리는 것이 아니라 증폭시킬 수 있습니다. 지침과 윤리적 기준을 설정함으로써 인간의 예술적 표현의 무결성을 보호하면서 혁신을 극대화할 수 있습니다. Al과 인간 창의성의 조화로운 공존을 위해 노력합시다. Emily(Negative_First) Mary, 창작 분야에서 Al과 인간 간의 협업의 중요성을 강조해 주셔서 감사합니다. AI는 귀중한 도구와 지원을 제공할 수 있지만, 인간 창의성의 본질과 그것이 가져오는 고유한 관점을 보존하는 것이 중요합니다. AI와 인간 창의성의 강점을 결합함으로써 전례 없는 혁신과 예술적 표현을 이룰 수 있습니다. Jennifer(Affirmative_Third) 사실, Mary, 창의적인 분야에서 AI를 책임감 있게 통합하는 것이 중요합니다. 명확한 지침과 윤리적 기준을 설정함으로써 AI가 인간의 창의성을 대체하는 것이 아니라 향상시키는 도구 역할을 하도록 할 수 있습니다. 이러한 조화로운 공존은 인간 예술적 표현의 고유한 관점과 진정성을 보존하면서 혁신을 극대화할 것입니다. 3.5 배포 그림 5: 토론에서의 인간-에이전트 상호 작용. 언어 에이전트를 위한 기존 오픈소스 프레임워크는 터미널이나 Gradio에서 실행되는 개념 증명 언어 에이전트를 구축하는 데 중점을 둡니다[Abid et al., 2019]. 반면, AGENTS는 FastAPI5를 사용하여 언어 에이전트를 API로 배포하는 것을 지원합니다. 이를 통해 개발자는 실제 애플리케이션에 언어 에이전트를 통합하는 것이 크게 용이해집니다. 3.6 에이전트 허브 AGENTS는 언어 에이전트 시스템의 개발, 테스트 및 튜닝을 용이하게 할 뿐만 아니라 언어 에이전트의 배포 및 공유를 보다 쉽게 만드는 것을 목표로 합니다. 이를 위해 사용자가 미세 조정된 언어 에이전트를 공유하고 다른 사람이 플랫폼에서 공유하는 유용한 언어 에이전트를 검색/다운로드할 수 있는 플랫폼인 AGENT HUB를 소개합니다. 이런 방식으로 커뮤니티 에이전트에서 시작하여 약간 수정하여 언어 에이전트를 쉽게 사용자 정의할 수 있습니다. 이렇게 하면 언어 에이전트를 처음부터 설계, 테스트 및 튜닝하는 데 드는 노력이 크게 줄어듭니다. 3. 에이전트 시스템의 자동 생성 SOP를 사용하여 언어 에이전트에 대한 세부적인 제어를 제공하지만 사용자가 SOP를 처음부터 수동으로 지정하는 것은 때로는 힘들 수 있습니다. 모든 상태에 대해 다른 상태, 연결 및 각 구성 요소에 대한 프롬프트와 도구를 설정해야 하기 때문입니다. 따라서 자동 SOP 생성을 위한 파이프라인을 신중하게 구현합니다. 당사의 SOP 생성 프레임워크는 검색 증강 생성(RAG)[Lewis et al., 2020]을 기반으로 합니다. SOP 생성 파이프라인 자체도 AGENTS 프레임워크를 기반으로 하며, 먼저 필요한 에이전트를 지정하고, 상태와 연결을 계획하고, 마지막으로 구성 요소를 생성하는 SOP를 갖습니다. 따라서 이 파이프라인은 다른 에이전트와 다중 에이전트 시스템을 생성할 수 있는 &quot;메타 에이전트&quot;로 간주될 수 있습니다. 자동 에이전트 생성 프레임워크에 대한 자세한 설명은 [Zhou et al., 2023b]에 설명되어 있습니다. Shttps://fastapi.tiangolo.com/ 사례 연구 그런 다음 단일 에이전트 시스템, 다중 에이전트 시스템 및 인간-에이전트 상호 작용이 필요한 시스템을 포함하여 라이브러리로 구축된 다양한 언어 에이전트에 대한 몇 가지 사례 연구를 제시합니다. 모든 데모는 http://www.aiwaves-agents.com/에서 제공됩니다. 4.1 단일 에이전트 시스템 잡담 봇, 지식 기반 및 웹 검색 엔진을 기반으로 하는 두 명의 고객 서비스 에이전트, 쇼핑 도우미 에이전트, 영업 에이전트를 포함하여 AGENTS를 사용하여 몇 가지 단일 에이전트 시스템을 구현합니다. 에이전트는 라이브러리의 다양한 기능과 AGENTS를 사용하여 다양한 사용 사례의 언어 에이전트를 구축할 수 있는 가능성을 보여줍니다. 그림 2와 3에서 각각 고객 서비스 담당자와 영업 담당자의 스크린샷을 제시합니다.4.2 다중 에이전트 시스템 또한 환경에서 서로 상호 작용하는 여러 에이전트로 구성된 다중 에이전트 시스템을 구축하는 방법을 보여줍니다.소설 스튜디오, 토론, 소프트웨어 회사를 포함한 세 가지 시나리오를 선택합니다.이러한 시나리오에는 다중 에이전트 시스템의 두 가지 주요 범주인 협력 시나리오와 경쟁 시나리오가 모두 포함됩니다.모든 시나리오에는 SOP(Symbol Plan)와 같은 상징적 계획을 통해 제어되는 여러 하위 작업이 포함됩니다.각 하위 작업에서 언어 에이전트의 동작을 쉽게 관찰하고 해당 프롬프트를 엔지니어링하여 시스템을 사용자 지정하고 개선할 수 있습니다.그림 4에서 소설 스튜디오 시스템의 시스템 스크린샷을 제시합니다.또한 그림 5에서 인간 사용자가 언어 에이전트와 토론에 참여하는 사례 연구에서 프레임워크의 인간-에이전트 상호 작용 기능을 보여줍니다.5 결론 LLM과 이를 기반으로 하는 언어 에이전트는 NLP/AI 커뮤니티와 우리 사회 전반에서 점점 더 중요한 역할을 하고 있습니다.AGENTS는 언어 에이전트를 위한 통합 프레임워크이자 오픈 소스 라이브러리입니다. AGENTS는 개발자가 언어 에이전트를 사용하여 애플리케이션을 빌드하고, 연구자가 언어 에이전트 연구를 수행하고, 일반 비기술 대상자가 개인화된 언어 에이전트를 빌드하고 사용자 정의할 수 있도록 지원합니다. 참고문헌 Stan Franklin과 Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. International workshop on agent theories, architectures, and languages, 21-35쪽. Springer, 1996. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습자입니다. H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집, 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id=TG8KACxEON. OpenAI. GPT-4 기술 보고서, 2023. Lilian Weng. Llm 기반 자율 에이전트. lilianweng.github.io, 2023년 6월. URL https://lilianweng.github.io/posts/2023-06-23-agent/. Toran Bruce Richards 및 et al. 자동 gpt: 자율 gpt-4
--- EXPERIMENT ---
s [Li et al., 2023], 소프트웨어 개발 [Qian et al., 2023] 등. AGENTS의 다중 에이전트 커뮤니케이션을 위한 새로운 기능 중 하나는 &quot;동적 스케줄링&quot; 기능입니다. 하드코딩된 규칙으로 에이전트의 작업 순서를 스케줄링하는 대신, 동적 스케줄링은 역할과 현재 기록을 고려하여 다음 작업을 수행할 에이전트를 결정하는 &quot;중재자&quot; 역할을 하는 컨트롤러 에이전트를 정의하는 옵션을 제공합니다. 동적 스케줄링은 여러 에이전트 간의 커뮤니케이션을 보다 자연스럽고 유연하게 만들 수 있는 잠재력이 있습니다. 개발자는 자연어를 사용하여 구성 파일에서 규칙을 지정하여 컨트롤러를 쉽게 사용자 정의할 수 있습니다. 인간-에이전트 상호 작용 기존 에이전트 프레임워크의 한 가지 한계는 에이전트 또는 다중 에이전트가 자동으로 작업을 해결할 수 있도록 하지만 인간 사용자가 에이전트와 상호 작용하는 것이 쉽지 않거나 가능하지 않다는 것입니다. 특히 다중 에이전트 시나리오에서 그렇습니다. AGENTS는 단일 에이전트와 다중 에이전트 시나리오에서 인간-에이전트 상호 작용을 원활하게 지원하여 한 명 이상의 인간이 언어 에이전트와 통신하고 상호 작용할 수 있습니다. 제어 가능성 기존 에이전트 프레임워크는 일반적으로 시스템 프롬프트를 사용하여 에이전트의 동작을 정의하고 제어한 다음 에이전트가 스스로 계획하고 작동하도록 합니다.반대로 AGENTS는 표준 운영 절차(SOP)라고도 하는 상징적 계획을 통해 제어 가능한 에이전트를 구축하는 새로운 패러다임을 제공합니다.SOP는 에이전트가 작업을 수행하는 동안 마주칠 수 있는 다양한 상황과 상태 간의 전환 규칙을 정의하는 여러 상태의 그래프입니다.실제 세계의 SOP와 유사하게 AGENTS의 SOP는 에이전트 또는 에이전트 그룹이 특정 작업이나 프로세스를 수행하는 방법을 설명하는 꼼꼼하게 문서화된 단계별 지침 세트입니다.SOP는 LLM에서 생성하고 사용자가 에이전트를 사용자 지정하고 조정할 때 편집할 수 있습니다.배포 후 에이전트는 각 상태에 대해 지정된 지침과 가이드라인에 따라 동작하고 환경, 인간 또는 다른 에이전트와의 상호 작용에 따라 현재 상태를 동적으로 조정합니다. 상징적 계획을 도입하면 에이전트의 행동을 세부적으로 제어할 수 있는 기회가 제공되어 에이전트의 행동을 보다 안정적이고 예측 가능하게 만들고 동시에 에이전트를 조정/최적화하는 것이 용이해집니다. 또한 (다중) 에이전트 시스템을 사용자 정의할 때 자세한 SOP 및 구성 파일을 작성하는 데 드는 인적 노동을 줄이기 위해 자동화된 SOP 생성 파이프라인을 제안합니다. 자동화된 SOP 생성 파이프라인은 작업에 대한 간략한 설명이 제공된 검색 증강 생성을 통해 언어 에이전트에 대한 구성 파일을 생성할 수 있는 &quot;메타 에이전트&quot;입니다. AGENTS는 AIWaves²의 연구자와 엔지니어가 유지 관리하는 지속적인 노력입니다. 이 프로젝트에 대한 커뮤니티 기여자의 지원을 기대합니다. 라이브러리와 자세한 문서 및 튜토리얼은 GitHub³에서 제공됩니다. 관련 작업 2.1 자율 언어 에이전트 언어 에이전트라는 개념은 최근에 매우 인기를 얻었으며 다양한 작업을 타겟으로 하는 다양한 언어 에이전트가 제안되었습니다. 예를 들어, Generative Agents[Park et al., 2023]는 인간의 사회적 행동을 모방하는 언어 에이전트를 개발했고, WebAgent[Gur et al., 2023]는 자연어 지침에 따라 실제 웹사이트에서 작업을 완료할 수 있는 언어 에이전트를 구축할 수 있는 가능성을 보여주었고, Qian et al.[2023]과 MetaGPT[Hong et al., 2023]는 다중 에이전트 커뮤니케이션 설정에서 소프트웨어 개발을 실험했으며, Zhou et al.[2023a]는 대화형 쓰기 도우미 역할을 하는 언어 에이전트를 구축했습니다. 특정 작업을 타겟으로 하는 언어 에이전트 외에도 AutoGPT[Richards and et al., 2023], BabyAGI[Nakajima, 2023], SuperAGI[SuperAGI, 2023]와 같은 최근 오픈소스 프로젝트는 사용자가 원하는 것을 수행하는 자율 에이전트를 구축하는 것을 목표로 하며 개발자와 비전문가 모두로부터 엄청난 관심을 끌었습니다. 2.2 언어 에이전트 프레임워크 최근에는 언어 에이전트를 위한 몇 가지 오픈소스 프레임워크가 제안되었습니다.예를 들어, Transformers Agents[Wolf et al., 2020]는 자연어로 설명된 작업을 자동으로 해결하기 위해 도구를 사용할 수 있는 언어 에이전트를 구축합니다.LangChain[LangChain, 2022]은 종단 간 2https://www.aiwaves.org/ ³https://github.com/aiwaves-cn/agentsX X 프레임워크 Transformers Agents 표 1: 언어 에이전트 프레임워크 비교 도구 사용 장단기 메모리 다중 에이전트 인간-에이전트 상호 작용 심볼릭 제어 LangChain Auto-GPT Gentopia XLang Meta-GPT Camel Agent Verse AGENTSX XX 자연어로 지정된 작업을 자동으로 해결할 수 있는 언어 에이전트;Camel[Li et al., 2023]과 Agent Verse[Chen et al., 2023]는 다중 에이전트 시스템을 구축하는 데 맞춤화된 플랫폼입니다. Gentopia [Xu et al., 2023]와 XLang4는 도구 증강 에이전트를 구축하기 위한 라이브러리입니다. 표 1에서 이러한 플랫폼과 AGENTS가 지원하는 주요 기능을 설명합니다. AGENTS는 도구 사용, 장단기 메모리, 다중 에이전트 통신을 동시에 지원하는 유일한 프레임워크임을 알 수 있습니다. 또한 AGENTS는 처음으로 심볼릭 플랜(SOP)을 통해 인간-에이전트 상호 작용과 제어 가능성을 제공합니다. 3 라이브러리 설계 코드 1: AGENTS를 사용하여 (다중) 에이전트 시스템을 초기화하고 실행하기 위한 예시 코드 def main() # agents는 하나 또는 여러 에이전트의 dict입니다. agents = sop = Agent.from_config(&quot;./config.json&quot;) SOP.from_config(&quot;./config.json&quot;) environment = Environment.from_config(&quot;./config.json&quot;) run (agents, sop, environment) AGENTS는 Franklin과 Graesser [1996]의 철학을 따라 설계되었습니다. &quot;자율 에이전트는 환경에 위치합니다.&quot; 따라서 agent와 environment는 AGENTS 프레임워크의 두 가지 주요 클래스입니다. 이 두 클래스 외에도 언어 에이전트를 보다 제어하기 쉽게 만들기 위해 SOP(Standard Operating Procedure의 약자)라는 기호 계획에 대한 클래스도 포함합니다. 이러한 주요 클래스는 모두 일반 텍스트로 채울 수 있는 구성 파일에서 초기화됩니다. 요약하면 AGENTS를 사용하여 (다중) 에이전트 시스템을 초기화하고 실행하기 위한 일반적인 스크립트는 코드 1에 설명되어 있습니다. 구성 파일은 이러한 핵심 객체를 정의할 뿐만 아니라 복잡한 프롬프트를 모듈화된 프롬프트 구성 요소로 인수분해합니다. 프롬프트의 인수분해는 사용자가 (다중) 에이전트 시스템을 구축하기 위해 필요로 하는 전문성 요구 사항과 노력을 크게 줄여줍니다. 단일 구성 파일을 사용하여 에이전트, 계획 및 기본 환경을 정의하면 언어 에이전트를 공유하는 것도 용이해집니다(에이전트 허브 섹션에서 논의).이 세 가지 핵심 클래스는 각각 숙련된 개발자와 연구자가 덮어쓸 수 있는 표준화된 API로 구성됩니다.다음 클래스에 대해 자세히 설명합니다.코드 2: AGENTS에서 (다중) 에이전트 시스템의 실행 루프에 대한 예제 코드 def run (agents, sop, environment): while not sop.finished: agent, state sop.step(agents, environment) action=agent.step(state, environment) environment.update (agent, action) #23 동적 계획의 경우 선택 사항 #new_states = get_new_states (action) # sop.add_states (new_states)4 https://github.com/xlang-ai/xlang 3.1 에이전트 에이전트 클래스는 언어 에이전트를 추상화합니다. 그림 1에 UML이 설명되어 있습니다. 에이전트가 장단기 메모리를 유지하고 환경을 관찰하는 방법(agent. observe(environment)), 현재 상태에 따라 행동하는 방법(agent. _ act()) 및 메모리를 업데이트하는 방법(agent. _update_memory())이 있는 것을 볼 수 있습니다. 이러한 모든 방법은 agent.step() 메서드에 래핑됩니다. 이 인수분해를 통해 개발자는 에이전트를 새로운 기능으로 쉽게 사용자 지정할 수 있습니다. 기존 언어 에이전트 프레임워크와 달리 에이전트는 LLM을 기반으로 해야 한다고 가정하지만, 우리는 에이전트에 &quot;_is_human&quot; 속성을 포함합니다. &quot;True&quot;로 설정된 경우 (agent. _ act())는 인간 사용자에게 관찰 및 메모리 정보를 제공하고 인간 사용자가 작업을 입력할 때까지 기다립니다. 이 설계는 인간 사용자가 하나 이상의 언어 에이전트 역할을 수행할 수 있도록 하여 단일 에이전트 및 다중 에이전트 시스템 모두에서 유연한 인간-에이전트 상호 작용을 허용합니다. 개발자는 인간 사용자가 토론에서 팀원 역할을 하고 (에이전트 또는 인간 기반) 팀원과 협력하여 다른 팀을 이기거나 소프트웨어 회사의 CTO/엔지니어 역할을 하고 소프트웨어 개발을 위해 다른 사람과 협력하는 등 다양하고 흥미로운 애플리케이션을 구축할 수 있습니다. 3.2 SOP SOP 클래스에는 에이전트 상태의 그래프가 포함되어 있습니다. 각 상태는 SOP에서 설명하는 작업을 수행할 때 모든 에이전트의 특정 하위 작업 또는 하위 목표를 지정합니다. 상태는 State 클래스로 추상화됩니다. State 객체는 에이전트가 LLM과 에이전트가 상태에서 사용할 수 있는 다양한 도구나 API를 활용하도록 하는 모듈화된 프롬프트를 포함합니다. 우리는 에이전트가 상태에서 동작에 사용할 수 있는 모든 것을 &quot;Component&quot; 클래스로 추상화합니다. Component 클래스는 프롬프트와 도구 또는 외부 API의 다른 부분에 해당하는 두 개의 하위 클래스로 구성되며 각각 &quot;Prompt Component&quot;와 &quot;ToolComponent&quot;로 명명됩니다. PromptComponent에는 작업/목표, 규칙/제약 조건, 컨텍스트 내 학습을 위한 (단계별) 데모 및 출력 형식을 지정하는 모듈화된 프롬프트가 포함됩니다. ToolComponent는 웹 검색, 지식 기반 등과 같은 외부 도구 및 API를 포함하여 모듈화된 프롬프트를 넘어 더 복잡한 사용을 지원합니다. 도구의 결과는 프롬프트에 포함되거나 구성 파일에 따라 나중에 직접 반환되어 처리됩니다. SOP 객체에는 또한 다른 상태와 다음에 동작할 에이전트 간의 전환을 결정하는 LLM 기반 제어 함수가 포함됩니다. 상태 이동 함수는 sop._transit()으로 명명되고 에이전트 라우팅 함수는 sop._route()로 명명됩니다. 두 함수 모두 메인 루프에서 사용되는 sop.next() 함수에 래핑됩니다. Mike 안녕하세요, 도와드릴까요? 사용자 iPhone 15를 알고 계신가요? Mike 사용자 네, 준비되었습니다. 기쁩니다, Agod 씨! 약속을 진행하려면 약속 카드를 작성해 주시기 바랍니다. 다음 정보를 복사하여 작성한 후 저에게 보내주시면 예약을 완료해 드리겠습니다. Mike 네, iPhone 15에 대해 잘 알고 있습니다. iPhone 15 Pro Max는 48MP 기본 카메라, 12MP 초광각 카메라, 12MP 망원 카메라가 탑재된 Pro 수준의 카메라 시스템을 갖추고 있습니다. 또한 Always-On 및 ProMotion 기술이 적용된 Super Retina XDR 디스플레이를 탑재하여 뛰어난 시청 경험을 제공합니다. iPhone 15는 9월 15일부터 사전 주문이 가능하며 9월 22일에 출시됩니다. 고객은 최대 2대의 iPhone 15와 2대의 iPhone 15 Plus 기기를 구매할 수 있습니다. 가격은 월 250위안 또는 선불 5999위안부터 시작합니다. 또한 매입 및 할부 옵션도 이용할 수 있습니다. iPhone 15 Pro 및 Pro Max 모델은 프리미엄 티타늄으로 제작되어 역대 가장 가벼운 Pro 라인업입니다.[이름]: [전화번호]: [대략적인 위치]: [지구 학위]: 이 정보를 제공하면 약속을 잡고 다음 단계를 안내해 드리겠습니다.입력 내용을 입력하세요 보내기 다시 시작 그림 2: (a) 고객 서비스 담당자 그림 3: (b) 영업 담당자 3.3 환경 Environment 클래스는 담당자가 위치한 환경을 추상화합니다.환경은 두 가지 주요 함수인 environment._ observer() 및 environment.update().environment로 구성됩니다. _ watched the following table: ... sitting in the way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a way of a 말하는 스타일: 마이크는 차분하고 절제된 어조로 말한다. 그는 말이 유창하고 정확하다. 다른 사람과의 관계: 마이크는 제인과 결혼했고 두 사람 사이에 아이가 있다. 그는 가족과 가까운 관계를 맺고 있으며 무엇보다도 그들의 행복을 소중히 여긴다. 캐릭터 배경: 마이크는 지난 10년 동안 인터넷 회사에서 일해 왔다. 그는 일에 열정적이며 프로그래밍 기술에 자부심을 느낀다. 그러나 그는 종종 힘든 일과 가족과 함께 보내는 양질의 시간 사이에서 균형을 찾기 위해 고군분투한다. 이러한 갈등은 이야기 전반에 걸쳐 마이크에게 긴장과 도전을 안겨준다. 조이 캐릭터 디자인: 완료 캐릭터 필요: 2명의 주요 캐릭터와 2명의 보조 캐릭터를 포함한 4명의 캐릭터 개요: 1장: 균형 잡기 -단어 수:관련 캐릭터: 마이크 스미스, 제인 블랙, 에밀리 스미스 - 요약: 1장에서 마이크와 제인은 일과 가족의 책임의 회오리에 갇힌다. 마이크는 끊임없이 직업의 요구에 압도당하는 반면, 제인은 교사로서의 경력과 어머니로서의 역할 사이에서 균형을 맞추기 위해 고군분투한다. 그들의 의사소통과 이해 부족은 가족 내에서 긴장과 갈등으로 이어진다. 한편, 어린 딸 에밀리는 부모님과 함께 보내는 양질의 시간을 갈구한다. 이 장에서는 Mike와 Jane이 시작할 자기 발견과 성장의 여정을 위한 무대를 마련한다. Abby 현재 목표: 개요 디자인 디자인 근거: Zoe의 제안을 바탕으로 이제 대본의 개요 디자인을 진행하겠습니다. Elmo와 Zoe가 제공한 제안을 분석하여 개요에 통합하여 현대 가족 윤리에 대한 설득력 있는 이야기를 만들 것입니다. ID:장 제목: 1장: 균형 잡기 디자인 근거: 이 장에서는 주인공 Mike와 Jane을 소개하고 이야기의 중심 갈등인 일과 가정의 균형을 맞추려는 투쟁을 설명합니다. 그림 4: 다중 에이전트 시스템: Fiction Studio. 기록자 총 인원: 이름: Mike Smith 성별: 남성 캐릭터 업무: 인터넷 회사 프로그래머 나이: Mike는 근면하고 헌신적인 사람입니다. 그는 지적이고 논리적이며 세부 사항 지향적입니다. 그는 약간 내성적이고 내성적일 수 있지만, 그는 가족을 깊이 사랑합니다.말하는 스타일: Mike는 차분하고 절제된 어조로 말합니다.그는 말이 유창하고 정확합니다.다른 사람과의 관계: Mike는 Jane과 결혼했고 그들 사이에 아이가 있습니다.그는 가족과 가까운 관계를 맺고 있으며 무엇보다도 그들의 행복을 소중히 여깁니다.캐릭터 배경: Mike는 지난 10년 동안 인터넷 회사에서 일해 왔습니다.그는 자신의 일에 열정적이며 프로그래밍 기술에 자부심을 가지고 있습니다.그러나 그는 종종 힘든 일과 가족과 함께 보내는 양질의 시간 사이에서 균형을 찾기 위해 고군분투합니다.이러한 갈등은 스토리 전반에 걸쳐 Mike에게 긴장과 도전을 안겨줍니다.캐릭터AGENTS에 기반한 (다중) 에이전트 시스템의 실행 논리는 매우 직관적입니다.코드 2에서 설명한 것처럼 각 반복에서 SOP는 먼저 상태 전환을 결정하고 에이전트와 환경에 따라 다음 에이전트를 선택합니다.그런 다음 에이전트는 상태와 환경에 따라 조치를 취합니다.그런 다음 환경은 새 조치에 따라 자체를 업데이트합니다. 마지막으로, 워크플로가 중간 실행 결과에 따라 계획을 동적으로 조정해야 하는 경우 작업의 출력을 구문 분석하고 새 상태를 정의하여 현재 SOP에 추가할 수 있습니다.3.4 핵심 기능의 구현 세부 정보 장단기 메모리: AGENTS는 Zhou et al. [2023a]에 따라 언어 에이전트에 대한 장단기 메모리를 구현합니다.특히, 장기 메모리는 작업 내역이며 문장 변환기[Reimers and Gurevych, 2019]에 의해 내장되어 VectorDB에 저장되고 의미 검색을 통해 쿼리됩니다.단기 메모리 또는 작업 메모리는 자연어 형태이며 신중하게 조정된 프롬프트를 통해 LLM에 의해 업데이트됩니다.도구 사용 및 웹 탐색: AGENTS는 ToolComponents를 통해 도구 사용 및 웹 탐색을 지원합니다.각 외부 도구 또는 API에 대해 개발자는 ToolComponent.func() 메서드에서 API 호출을 래핑할 수 있습니다. API 호출이 컨텍스트에 따라 달라지는 복잡한 도구의 경우 AGENTS는 OpenAI의 GPT API의 &quot;함수 호출&quot; 기능을 통합하여 LLM이 도구를 사용하는 방법을 결정하도록 합니다. 웹 탐색은 웹 검색을 특수 도구로 구현하여 달성됩니다. 다중 에이전트 통신: 사전 정의된 규칙(예: 각 에이전트가 순차적으로 작동하도록 함)을 사용하여 에이전트의 작업 순서를 제어하는 대부분의 기존 다중 에이전트 시스템 프레임워크와 달리 AGENTS에는 이전 작업, 환경 및 현재 상태의 대상을 고려하여 LLM을 사용하여 다음 작업을 수행할 에이전트를 동적으로 결정하는 컨트롤러 기능이 포함되어 있습니다. 이를 통해 다중 에이전트 통신이 더 유연해집니다. 인간-대리인 상호작용: AGENTS는 인간 사용자가 구성 파일에서 특정 에이전트의 &quot;is_human&quot; 필드를 &quot;True&quot;로 변경할 수 있도록 하여 다중 에이전트 시스템에서 인간-대리인 상호작용을 지원합니다. 이 경우 사용자는 스스로 에이전트 역할을 수행하고 자신의 동작을 입력하고 환경의 다른 언어 에이전트와 상호 작용할 수 있습니다. 토론 주제: 창의적인 분야에서 AI가 인간을 대체해야 할까요?? 사용자 내 정체성을 방어하기 위해 무엇을 해야 할지 생각해 보세요. David(토론 심사 위원) 이제 선택한 주제에 따라 순차적인 토론을 진행하세요. 긍정적 관점: AI는 창의적인 분야에서 인간을 대체해야 합니다. 왜냐하면 효율적으로 예술과 콘텐츠를 생산하고 비용을 절감하며 인간의 편견을 없앨 수 있기 때문입니다. James(긍정적 2번째) Mary, 정체성을 방어하기 위해 직관, 감정, 복잡한 인간 경험을 이해하는 능력과 같이 인간이 창의적인 분야에 가져오는 고유한 특성과 관점을 강조할 수 있습니다. 인간적 연결의 가치와 인간이 만든 예술과 콘텐츠에서 나오는 진정성을 강조하세요. 부정적 관점: AI는 진정한 창의성, 감정, 복잡한 인간 경험을 이해하는 능력이 부족하기 때문에 창의적인 분야에서 인간을 대체해서는 안 됩니다.Jennifer(긍정적_세 번째) Mary, 자신의 정체성을 지키기 위해 예술과 콘텐츠 창작의 경계를 넓히는 데 있어 인간의 창의성과 혁신의 중요성을 강조할 수도 있습니다.인간의 상상력과 틀에서 벗어난 사고능력의 대체 불가능한 역할을 강조하세요.또한 창의적인 분야에서 AI에만 의존하는 것의 윤리적 의미에 대해 논의할 수 있습니다.User&#39;agent Audience David(토론 심판) Emily(부정적_첫 번째) James(긍정적_두 번째) Jennifer(긍정적_세 번째) John(긍정적_토론_조직자) Mary(긍정적_첫 번째) Michael(부정적_토론_조직자) Sarah(부정적_세 번째) William(부정적_두 번째) 실행 사용자 여러분, 이 기회를 주셔서 감사합니다.오늘 저는 여러분 앞에 서서 창의적인 분야에서 AI가 인간을 대체해서는 안 된다고 주장합니다. Al이 효율성과 비용상의 이점을 확실히 제공하지만, 진정한 창의성은 우리의 감정과 복잡한 인간 경험에 대한 이해에서 태어나는 고유한 인간적 특성이라는 점을 기억하는 것이 중요합니다. John(Affirmative_Debate_organizer) Mary, 창의적인 분야에서 인간의 고유한 특성을 강조하는 것 외에도 인간의 직관과 복잡한 감정을 해석하고 전달하는 능력의 중요성에 대해서도 논의할 수 있습니다. 인간적 연결의 가치와 인간이 작업에 가져오는 개인적인 접촉을 강조하세요. 또한 창의적인 분야에서 Al에만 의존하는 것의 잠재적인 한계와 윤리적 우려 사항을 살펴볼 수 있습니다. David(Debate Judge) 이제 선택한 주제에 대한 무료 토론에 참여하세요. Emily(Negative_First) 신사 숙녀 여러분, 이 기회를 주셔서 감사합니다. 오늘은 Al이 창의적인 분야에서 인간을 대체해서는 안 된다고 주장하겠습니다. Al은 효율성과 비용상의 이점을 제공할 수 있지만, 창의적인 작업의 중요한 측면인 복잡한 인간의 감정을 진정으로 이해하고 표현하는 능력이 부족합니다. 사용자 이 무료 토론에서, 창의적인 분야에서 Al의 역할이 전부 아니면 전무의 시나리오가 될 필요는 없다는 것을 기억합시다. 우리는 인간 창의성의 본질을 보존하면서 Al의 잠재력을 활용할 수 있습니다. Al과 인간 간의 협업은 전례 없는 혁신과 예술적 표현으로 이어질 수 있습니다. Al과 인간 창의성의 강점을 결합하여 미래를 포용합시다. 사용자 물론입니다, William. 창의적인 분야에서 Al을 둘러싼 잠재적 위험과 윤리적 우려 사항을 해결하는 것이 중요합니다. 그러나 잠재적 이점을 완전히 무시하지 않는 것도 마찬가지로 중요합니다. 핵심은 책임감 있고 세심한 통합에 있습니다. Al은 도구 역할을 하여 인간의 창의성을 가리는 것이 아니라 증폭시킬 수 있습니다. 지침과 윤리적 기준을 설정함으로써 인간의 예술적 표현의 무결성을 보호하면서 혁신을 극대화할 수 있습니다. Al과 인간 창의성의 조화로운 공존을 위해 노력합시다. Emily(Negative_First) 창의적인 분야에서 Al과 인간 간의 협업의 중요성을 강조해 주셔서 감사합니다, Mary. AI는 귀중한 도구와 지원을 제공할 수 있지만, 인간 창의성의 본질과 그것이 가져오는 고유한 관점을 보존하는 것이 중요합니다. AI와 인간 창의성의 강점을 결합함으로써 전례 없는 혁신과 예술적 표현을 이룰 수 있습니다. Jennifer(Affirmative_Third) 사실, Mary, 창의적인 분야에서 AI를 책임감 있게 통합하는 것이 중요합니다. 명확한 지침과 윤리적 기준을 설정함으로써 AI가 인간의 창의성을 대체하는 것이 아니라 향상시키는 도구 역할을 하도록 할 수 있습니다. 이러한 조화로운 공존은 인간 예술적 표현의 고유한 관점과 진정성을 보존하면서 혁신을 극대화할 것입니다. 3.5 배포 그림 5: 토론에서의 인간-에이전트 상호 작용. 언어 에이전트를 위한 기존 오픈소스 프레임워크는 터미널이나 Gradio에서 실행되는 개념 증명 언어 에이전트를 구축하는 데 중점을 둡니다[Abid et al., 2019]. 반면, AGENTS는 FastAPI5를 사용하여 언어 에이전트를 API로 배포하는 것을 지원합니다. 이를 통해 개발자는 실제 애플리케이션에 언어 에이전트를 통합하는 것이 크게 용이해집니다. 3.6 에이전트 허브 AGENTS는 언어 에이전트 시스템의 개발, 테스트 및 튜닝을 용이하게 할 뿐만 아니라 언어 에이전트의 배포 및 공유를 보다 쉽게 만드는 것을 목표로 합니다. 이를 위해 사용자가 미세 조정된 언어 에이전트를 공유하고 다른 사람이 플랫폼에서 공유하는 유용한 언어 에이전트를 검색/다운로드할 수 있는 플랫폼인 AGENT HUB를 소개합니다. 이런 방식으로 커뮤니티 에이전트에서 시작하여 약간 수정하여 언어 에이전트를 쉽게 사용자 정의할 수 있습니다. 이렇게 하면 언어 에이전트를 처음부터 설계, 테스트 및 튜닝하는 데 드는 노력이 크게 줄어듭니다. 3. 에이전트 시스템의 자동 생성 SOP를 사용하여 언어 에이전트에 대한 세부적인 제어를 제공하지만 사용자가 SOP를 처음부터 수동으로 지정하는 것은 때로는 힘들 수 있습니다. 모든 상태에 대해 다른 상태, 연결 및 각 구성 요소에 대한 프롬프트와 도구를 설정해야 하기 때문입니다. 따라서 자동 SOP 생성을 위한 파이프라인을 신중하게 구현합니다. 당사의 SOP 생성 프레임워크는 검색 증강 생성(RAG)[Lewis et al., 2020]을 기반으로 합니다. SOP 생성 파이프라인 자체도 AGENTS 프레임워크를 기반으로 하며, 먼저 필요한 에이전트를 지정하고, 상태와 연결을 계획하고, 마지막으로 구성 요소를 생성하는 SOP를 갖습니다. 따라서 이 파이프라인은 다른 에이전트와 다중 에이전트 시스템을 생성할 수 있는 &quot;메타 에이전트&quot;로 간주될 수 있습니다. 자동 에이전트 생성 프레임워크에 대한 자세한 설명은 [Zhou et al., 2023b]에 설명되어 있습니다. Shttps://fastapi.tiangolo.com/ 사례 연구 그런 다음 단일 에이전트 시스템, 다중 에이전트 시스템 및 인간-에이전트 상호 작용이 필요한 시스템을 포함하여 라이브러리로 구축된 다양한 언어 에이전트에 대한 몇 가지 사례 연구를 제시합니다. 모든 데모는 http://www.aiwaves-agents.com/에서 제공됩니다. 4.1 단일 에이전트 시스템 잡담 봇, 지식 기반 및 웹 검색 엔진을 기반으로 하는 두 명의 고객 서비스 에이전트, 쇼핑 도우미 에이전트, 영업 에이전트를 포함하여 AGENTS를 사용하여 몇 가지 단일 에이전트 시스템을 구현합니다. 에이전트는 라이브러리의 다양한 기능과 AGENTS를 사용하여 다양한 사용 사례의 언어 에이전트를 구축할 수 있는 가능성을 보여줍니다. 그림 2와 3에서 각각 고객 서비스 담당자와 영업 담당자의 스크린샷을 제시합니다. 4.2 다중 에이전트 시스템 또한 환경에서 서로 상호 작용하는 여러 에이전트로 구성된 다중 에이전트 시스템을 구축하는 방법을 보여줍니다. 소설 스튜디오, 토론, 소프트웨어 회사를 포함한 세 가지 시나리오를 선택합니다. 이러한 시나리오에는 다중 에이전트 시스템의 두 가지 주요 범주인 협력 시나리오와 경쟁 시나리오가 모두 포함됩니다. 모든 시나리오에는 SOP(Symbolic Plan)와 같은 상징적 계획을 통해 제어되는 여러 하위 작업이 포함됩니다. 각 하위 작업에서 언어 에이전트의 동작을 쉽게 관찰하고 해당 프롬프트를 엔지니어링하여 시스템을 사용자 지정하고 개선할 수 있습니다. 그림 4에서 소설 스튜디오 시스템의 시스템 스크린샷을 제시합니다. 또한 그림 5에서 인간 사용자가 언어 에이전트와 토론에 참여하는 사례 연구에서 프레임워크의 인간-에이전트 상호 작용 기능을 보여줍니다. 5
--- CONCLUSION ---
LLM과 이를 기반으로 하는 언어 에이전트는 NLP/AI 커뮤니티와 우리 사회 전반에서 점점 더 중요한 역할을 하고 있습니다. AGENTS는 언어 에이전트를 위한 통합 프레임워크이자 오픈 소스 라이브러리입니다. AGENTS는 개발자가 언어 에이전트를 사용하여 애플리케이션을 빌드하고, 연구자가 언어 에이전트 연구를 수행하고, 일반 비기술 대상자가 개인화된 언어 에이전트를 빌드하고 사용자 지정할 수 있도록 지원합니다. 참고문헌 Stan Franklin과 Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. International workshop on agent theories, architectures, and languages, 21-35쪽. Springer, 1996. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습자입니다. H. Larochelle, M. Ranzato, R. Hadsell, MF Balcan, H. Lin 편집, 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 인간의 피드백을 통해 지시를 따르도록 언어 모델을 훈련합니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho 편집자, 신경 정보 처리 시스템의 발전, 2022. URL https://openreview.net/forum?id=TG8KACxEON. OpenAI. GPT-4 기술 보고서, 2023. Lilian Weng. Llm 기반 자율 에이전트. lilianweng.github.io, 2023년 6월. URL https://lilianweng.github.io/posts/2023-06-23-agent/. Toran Bruce Richards 외. Auto-gpt: 자율 gpt-4 실험, 2023. URL https://github.com/Significant-Gravitas/Auto-GPT. [소프트웨어]. Yohei Nakajima. Babyagi, 2023. URL https://github.com/yoheinakajima/babyagi. [소프트웨어]. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton 및 Augustus Odena. 작업 표시: 언어 모델을 사용한 중간 계산을 위한 스크래치패드, 2022. URL https://openreview.net/forum?id=iedYJm9200a. Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell 및 Mrinmaya Sachan. Recurrentgpt: (임의로) 긴 텍스트의 대화형 생성, 2023a. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman. Webgpt: 인간의 피드백을 통한 브라우저 지원 질문 답변. CoRR, abs/2112.09332, 2021. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. Toolformer: 언어 모델은 도구 사용을 스스로 학습할 수 있습니다. CORR, abs/2302.04761, 2023. Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson. 심층적 다중 에이전트 강화 학습을 통한 의사소통 학습. NIPS, 2137-2145페이지, 2016. Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez. Gorilla: 대규모 API와 연결된 대규모 언어 모델. arXiv 사전 인쇄본 arXiv:2305.15334, 2023. Joon Sung Park, Joseph C. O&#39;Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein. 생성 에이전트: 인간 행동의 대화형 시뮬레이션, 2023. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin 및 Bernard Ghanem. Camel: 대규모 언어 모델 사회의 &quot;마음&quot; 탐색을 위한 의사소통 에이전트, 2023. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu 및 Maosong Sun. 소프트웨어 개발을 위한 의사소통 에이전트, 2023. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck 및 Aleksandra Faust. 계획, 장기적인 상황 이해 및 프로그램 합성 기능을 갖춘 실제 웹 에이전트, 2023. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao 및 Chenglin Wu. Metagpt: 다중 에이전트 협업 프레임워크를 위한 메타 프로그래밍, 2023. SuperAGI. Superagi, 2023. URL https://github.com/Transformer Optimus/SuperAGI. [소프트웨어]. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Transformers: State-of-the-heart 자연어 처리. 2020 자연어 처리 경험적 방법에 대한 컨퍼런스: 시스템 데모, 38-45페이지, 온라인, 2020년 10월. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020. emnlp-demos.6. 랭체인. Langchain 저장소. https://github.com/langchain-ai/langchain, 2022.Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun 및 Jie Zhou. Agentverse: 다중 에이전트 협업 촉진 및 에이전트의 긴급 행동 탐색, 2023. Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao 및 Dongkuan Xu. Gentopia: 도구로 강화된 Ilms를 위한 협업 플랫폼, 2023. Nils Reimers 및 Iryna Gurevych. Sentence-BERT: siamese BERTNetworks를 사용한 문장 임베딩. 2019년 자연어 처리 경험적 방법 컨퍼런스 회의록. 계산 언어학 협회, 2019년 11월. URL https://arxiv.org/abs/1908.10084. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: 야생에서 ML 모델의 번거로움 없는 공유 및 테스트. arXiv 사전 인쇄본 arXiv:1906.02569, 2019. Patrick SH Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 지식 집약적 NLP 작업을 위한 검색 증강 생성. NeurIPS, 2020. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui 및 Mrinmaya Sachan. 연결주의와 상징주의를 통합하는 언어 에이전트를 향하여. 2023b. 출판 예정.
"
"--- ABSTRACT ---
We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics of objects such as trees, flowers, candles, and clothes swaying in the wind. We model dense, long-term motion in the Fourier domain as spectral volumes, which we find are well-suited to prediction with diffusion models. Given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an imagebased rendering module, the predicted motion representation can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to interact with objects in real images, producing realistic simulated dynamics (by interpreting the spectral volumes as image-space modal bases). See our project page for more results: generative-dynamics. github.io. 1.
--- INTRODUCTION ---
The natural world is always in motion, with even seemingly static scenes containing subtle oscillations as a result of wind, water currents, respiration, or other natural rhythms. Emulating this motion is crucial in visual content synthesis—human sensitivity to motion can cause imagery without motion (or with slightly unrealistic motion) to seem uncanny or unreal. While it is easy for humans to interpret or imagine motion in scenes, training a model to learn or produce realistic scene motion is far from trivial. The motion we observe in the world is the result of a scene’s underlying physical dynamics, i.e., forces applied to objects that respond according to their unique physical properties—their mass, elasticity, etc—quantities that are hard to measure and capture at scale. Fortunately, measuring them is not necessary for certain applications: e.g., one can simulate plausible dynamics in a scene by simply analyzing some observed 2D motion [23]. This same observed motion can also serve as a supervisory signal in learning dynamics across scenes—because although observed motion is multi-modal and grounded in complex physical effects, it is nevertheless often predictable: --- --candles will flicker in certain ways, trees will sway, and their leaves will rustle. For humans, this predictability is ingrained in our systems of perception: by viewing a still image, we can imagine plausible motions— or, since there might have been many possible such motions, a distribution of natural motions conditioned on that image. Given the facility with which humans are able to model these distributions, a natural research problem is to model them computationally. Recent advances in generative models, in particular conditional diffusion models [44, 85, 87], have enabled us to model rich distributions, including distributions of real images conditioned on text [73-75]. This capability has enabled several new applications, such as text-conditioned generation of diverse and realistic image content. Following the success of these image models, recent work has extended these models to other domains, such as videos [7, 43] and 3D geometry [77, 100, 101, 103]. In this paper, we model a generative prior for image-space scene motion, i.e., the motion of all pixels in a single image. This model is trained on motion trajectories automatically extracted from a large collection of real video sequences. In particular, from each training video we compute motion in the form of a spectral volume (22, 23], a frequency-domain representation of dense, long-range pixel trajectories. Spectral volumes are well-suited to scenes that exhibit oscillatory dynamics, e.g., trees and flowers moving in the wind. We find that this representation is also highly effective as an output of a diffusion model for modeling scene motion. We train a generative model that, conditioned on a single image, can sample spectral volumes from its learned distribution. A predicted spectral volume can then be directly transformed into a motion texture—a set of long-range, per-pixel motion trajectories—that can be used to animate the image. The spectral volume can also be interpreted as an image-space modal basis for use in simulating interactive dynamics [22]. We predict spectral volumes from input images using a diffusion model that generates coefficients one frequency at a time, but coordinates these predictions across frequency bands through a shared attention module. The predicted motions can be used to synthesize future frames (via an image-based rendering model)—turning still images into realistic animations, as illustrated in Fig. 1. Compared with priors over raw RGB pixels, priors over motion capture more fundamental, lower-dimensional structure that efficiently explains long-range variations in pixel values. Hence, generating intermediate motion leads to more coherent long-term generation and more fine-grained control over animations. We demonstrate the use of our trained model in several downstream applications, such as creating seamless looping videos, editing the generated motions, and enabling interactive dynamic images via image-space modal bases, i.e., simulating the response of object dynamics to user-applied forces [22]. 2.
--- RELATED WORK ---
Generative synthesis. Recent advances in generative models have enabled photorealistic synthesis of images conditioned on text prompts [16, 17, 24, 73-75]. These text-toimage models can be augmented to synthesize video sequences by extending the generated image tensors along a time dimension [7, 9, 43, 62, 84, 106, 106, 111]. While these
--- METHOD ---
generates a spectral volume [23], a motion representation that models dense, long-term pixel trajectories in the Fourier domain. Our learned motion priors can be used to turn a single picture into a seamlessly looping video, or into an interactive simulation of dynamics that responds to user inputs like dragging and releasing points. On the right, we visualize output videos as space-time X-t slices (along the input scanline shown on the left). Abstract We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics of objects such as trees, flowers, candles, and clothes swaying in the wind. We model dense, long-term motion in the Fourier domain as spectral volumes, which we find are well-suited to prediction with diffusion models. Given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an imagebased rendering module, the predicted motion representation can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to interact with objects in real images, producing realistic simulated dynamics (by interpreting the spectral volumes as image-space modal bases). See our project page for more results: generative-dynamics. github.io. 1. Introduction The natural world is always in motion, with even seemingly static scenes containing subtle oscillations as a result of wind, water currents, respiration, or other natural rhythms. Emulating this motion is crucial in visual content synthesis—human sensitivity to motion can cause imagery without motion (or with slightly unrealistic motion) to seem uncanny or unreal. While it is easy for humans to interpret or imagine motion in scenes, training a model to learn or produce realistic scene motion is far from trivial. The motion we observe in the world is the result of a scene’s underlying physical dynamics, i.e., forces applied to objects that respond according to their unique physical properties—their mass, elasticity, etc—quantities that are hard to measure and capture at scale. Fortunately, measuring them is not necessary for certain applications: e.g., one can simulate plausible dynamics in a scene by simply analyzing some observed 2D motion [23]. This same observed motion can also serve as a supervisory signal in learning dynamics across scenes—because although observed motion is multi-modal and grounded in complex physical effects, it is nevertheless often predictable: --- --candles will flicker in certain ways, trees will sway, and their leaves will rustle. For humans, this predictability is ingrained in our systems of perception: by viewing a still image, we can imagine plausible motions— or, since there might have been many possible such motions, a distribution of natural motions conditioned on that image. Given the facility with which humans are able to model these distributions, a natural research problem is to model them computationally. Recent advances in generative models, in particular conditional diffusion models [44, 85, 87], have enabled us to model rich distributions, including distributions of real images conditioned on text [73-75]. This capability has enabled several new applications, such as text-conditioned generation of diverse and realistic image content. Following the success of these image models, recent work has extended these models to other domains, such as videos [7, 43] and 3D geometry [77, 100, 101, 103]. In this paper, we model a generative prior for image-space scene motion, i.e., the motion of all pixels in a single image. This model is trained on motion trajectories automatically extracted from a large collection of real video sequences. In particular, from each training video we compute motion in the form of a spectral volume (22, 23], a frequency-domain representation of dense, long-range pixel trajectories. Spectral volumes are well-suited to scenes that exhibit oscillatory dynamics, e.g., trees and flowers moving in the wind. We find that this representation is also highly effective as an output of a diffusion model for modeling scene motion. We train a generative model that, conditioned on a single image, can sample spectral volumes from its learned distribution. A predicted spectral volume can then be directly transformed into a motion texture—a set of long-range, per-pixel motion trajectories—that can be used to animate the image. The spectral volume can also be interpreted as an image-space modal basis for use in simulating interactive dynamics [22]. We predict spectral volumes from input images using a diffusion model that generates coefficients one frequency at a time, but coordinates these predictions across frequency bands through a shared attention module. The predicted motions can be used to synthesize future frames (via an image-based rendering model)—turning still images into realistic animations, as illustrated in Fig. 1. Compared with priors over raw RGB pixels, priors over motion capture more fundamental, lower-dimensional structure that efficiently explains long-range variations in pixel values. Hence, generating intermediate motion leads to more coherent long-term generation and more fine-grained control over animations. We demonstrate the use of our trained model in several downstream applications, such as creating seamless looping videos, editing the generated motions, and enabling interactive dynamic images via image-space modal bases, i.e., simulating the response of object dynamics to user-applied forces [22]. 2. Related Work Generative synthesis. Recent advances in generative models have enabled photorealistic synthesis of images conditioned on text prompts [16, 17, 24, 73-75]. These text-toimage models can be augmented to synthesize video sequences by extending the generated image tensors along a time dimension [7, 9, 43, 62, 84, 106, 106, 111]. While these methods can produce video sequences that capture the spatiotemporal statistics of real footage, these videos often suffer from artifacts like incoherent motion, unrealistic temporal variation in textures, and violations of physical constraints like preservation of mass. Animating images. Instead of generating videos entirely from text, other techniques take as input a still picture and animate it. Many recent deep learning methods adopt a 3DUnet architecture to produce video volumes directly [27, 36, 40, 47, 53, 93]. These models are effectively the same video generation models (but conditioned on image information instead of text), and exhibit similar artifacts to those mentioned above. One way to overcome these limitations is to not directly generate the video content itself, but instead animate an input source image through image-based rendering, i.e., moving the image content around according to motion derived from external sources such as a driving video [51, 80-82, 99], motion or 3D geometry priors [8, 29, 46, 63-65, 67, 90, 97, 101, 102, 104, 109], or user annotations [6, 18, 20, 33, 38, 98, 105, 108]. Animating images according to motion fields yields greater temporal coherence and realism, but these prior methods either require additional guidance signals or user input, or utilize limited motion representations. Motion models and motion priors. In computer graphics, natural, oscillatory 3D motion (e.g., water rippling or trees waving in the wind) can be modeled with noise that is shaped in the Fourier domain and then converted to time-domain motion fields [79, 88]. Some of these methods rely on a modal analysis of the underlying dynamics of the system being simulated [22, 25, 89]. These spectral techniques were adapted to animate plants, water, and clouds from single 2D pictures by Chuang et al. [20], given user annotations. Our work is especially inspired by Davis [23], who connected modal analysis of a scene with the motions observed in a video of that scene, and used this analysis to simulate interactive dynamics from a video. We adopt the frequency-space spectral volume motion representation from Davis et al., extract this representation from a large set of training videos, and show that spectral volumes are suitable for predicting motion from single images with diffusion models. Other methods have used various motion representations in prediction tasks, where an image or video is used to inform a deterministic future motion estimate [34, 71], or a more rich distribution of possible motions [94, 96, 104]. However, --- --many of these methods predict an optical flow motion estimate (i.e., the instantaneous motion of each pixel), not full motion trajectories. In addition, much of this prior work is focused on tasks like activity recognition, not on synthesis tasks. More recent work has demonstrated the advantages of modeling and predicting motion using generative models in a number of closed-domain settings such as humans and animals [2, 19, 28, 72, 91, 107]. Videos as textures. Certain moving scenes can be thought of as a kind of texture—termed dynamic textures [26|—that model videos as space-time samples of a stochastic process. Dynamic textures can represent smooth, natural motions like waves, flames, or moving trees, and have been widely used for video classification, segmentation or encoding [12—15, 76]. A related kind of texture, called a video texture, represents a moving scene as a set of input video frames along with transition probabilities between any pair of frames [66, 78]. A number of methods estimate dynamic or video textures through analysis of scene motion and pixel statistics, with the aim of generating seamlessly looping or infinitely varying output videos [1, 21, 32, 58, 59, 78]. In contrast to much of this work, our method learns priors in advance that can then be applied to single images. 3. Overview Given a single picture Jp, our goal is to generate a video {h, T., wey Ip} featuring oscillatory motions such as those of trees, flowers, or candle flames swaying in the breeze. Our system consists of two modules: a motion prediction module and an image-based rendering module. Our pipeline begins by using a latent diffusion model (LDM) to predict a spectral volume S = (S¥.S/,,---+Sf_,) for the input Io. The predicted spectral volume is then transformed to a motion texture F = (F\, Fp,..., Fr) through an inverse discrete Fourier transform. This motion determines the position of each input pixel at every future time step. Given a predicted motion texture, we then animate the input RGB image using a neural image-based rendering technique (Sec. 5). We explore applications of this method, including producing seamless looping animations and simulating interactive dynamics, in Sec. 6. 4. Predicting motion 4.1. Motion representation Formally, a motion texture is a sequence of time-varying 2D displacement maps F = {F;,|t = 1,...,}, where the 2D displacement vector F;(p) at each pixel coordinate p from input image Jp defines the position of that pixel at a future time t [20]. To generate a future frame at time t, one can splat pixels from Jo using the corresponding displacement mE Scaling w/ resolution 0.8} ""mE Adaptive normalization Amplitude Frequency 0.0! oo 25 50 75 100 25 15.Frequency (Hz) Amplitude of Fourier coetticent at 3.0 Hz Figure 2. Left: We visualize the average power spectrum for the X and Y motion components extracted from real videos, shown as the blue and green curves. Natural oscillation motions are compose: primarily of low-frequency components, and so we use the first 4 = 16 terms, marked with red dots. Right: we show a histogram of the amplitude of Fourier terms at 3.0 Hz after (1) scaling amplitude by image width and height (blue), or (2) frequency adaptive normalization (red). Our adaptive normalization prevents the coefficients from concentrating at extreme values. map D,, resulting in a forward-warped image I: Ii(p + Fi(p)) = Lo(p). (1) If our goal is to produce a video via a motion texture, then one choice would be to predict a time-domain motion texture directly from an input image. However, the size of the motion texture would need to scale with the length of the video: generating T’ output frames implies predicting T displacement fields. To avoid predicting such a large output representation for long videos, many prior animation methods either generate video frames autoregressively [7, 29, 57, 60, 93], or predict each future output frame independently via an extra time embedding [4]. However, neither strategy ensures long-term temporal consistency of generated videos. Fortunately, many natural motions can be described as a superposition of a small number of harmonic oscillators represented with different frequencies, amplitude and phases [20, 23, 25, 50, 69]. Because these underlying motions are quasi-periodic, it is natural to model them in the frequency domain. Hence, we adopt from Davis et al. [23] an efficient frequency space representation of motion in a video called a spectral volume, visualized in Fig. 3. A spectral volume is the temporal Fourier transform of per-pixel trajectories extracted from a video. Given this motion representation, we formulate the motion prediction problem as a multi-modal image-to-image translation task: from an input image to an output motion spectral volume. We adopt latent diffusion models (LDMs) to generate spectral volumes comprised of a 4/¢-channel 2D motion spectrum map, where kK << T is the number of frequencies modeled, and where at each frequency we need four scalars to represent the complex Fourier coefficients for the x- and y-dimensions. Note that the motion trajectory of a pixel at future time steps F(p) = {F;(p)|t = 1,2, ...7} and its representation as a spectral volume S(p) = {S¥,(p)|k = 0,1, _ —1} are related by the Fast Fourier transform (FFT): S(p) = FFT(F(p)). (2) --- --€Inference! ZN ~N(0,1) -? Noisy latent Pals Iterative denoising Spatial layer |Reshape fle RBXKxCxH'XW! | Reshape E RBA XC xH! XW"" Figure 3. Motion prediction module. We predict a spectral volume S through a frequency-coordinated denoising model. Each block of the diffusion network €9 interleaves 2D spatial layers with attention layers (red box, right), and iteratively denoises latent features z"". The denoised features are fed to a decoder D to produce S. During training, we concatenate the downsampled input Jo with noisy latent features encoded from a real motion texture via an encoder €, and replace the noisy features with Gaussian noise 2“ during inference (left). How should we select the A output frequencies? Prior work in real-time animation has observed that most natural oscillation motions are composed primarily of low-frequency components [25, 69]. To validate this observation, we computed the average power spectrum of the motion extracted from 1,000 randomly sampled 5-second real video clips. As shown in the left plot of Fig. 2, the power spectrum of the motion decreases exponentially with increasing frequency. This suggests that most natural oscillation motions can indeed be well represented by low-frequency terms. In practice, we found that the first A = 16 Fourier coefficients are sufficient to realistically reproduce the original natural motion in a range of real videos and scenes. 4.2. Predicting motion with a diffusion model We choose a latent diffusion model (LDM) [74] as the backbone for our motion prediction module, as LDMs are more computationally efficient than pixel-space diffusion models, while preserving synthesis quality. A standard LDM consists of two main modules: (1) a variational autoencoder (VAE) that compresses the input image to a latent space through an encoder z = E(J), then reconstructs the input from the latent features via a decoder [ = D(z), and (2) a U-Net based diffusion model that learns to iteratively denoise features starting from Gaussian noise. Our training applies this process not to RGB images but to spectral volumes from real video sequences, which are encoded and then diffused for n steps with a pre-defined variance schedule to produce noisy latents z”. The 2D U-Nets are trained to denoise the noisy latents by iteratively estimating the noise €g(z”;n, c) used to update the latent feature at each step n € (1,2,..., N). The training loss for the LDM is written as Lipm = Encui,nj,enen(o.a) [lle” — €0(2""3 7m, €)|I7] 3) where c is the embedding of any conditional signal, such as text, or, in our case, the first frame of the training video sequence, Jy. The clean latent features z° are then passed through the decoder to recover the spectral volume. Frequency adaptive normalization. One issue we observed is that motion textures have particular distribution characteristics across frequencies. As visualized in the left plot of Fig. 2, the amplitude of the spectral volumes spans a range of 0 to 100 and decays approximately exponentially with increasing frequency. As diffusion models require that the absolute values of the output are between -1 and 1 for stable training and denoising [44], we must normalize the coefficients of S extracted from real videos before using them for training. If we scale the magnitudes of these coefficients to [0,1] based on the image dimensions as in prior work [29, 77], nearly all the coefficients at higher frequencies will end up close to zero, as shown in the right plot of Fig. 2. Models trained on such data can produce inaccurate motions, since during inference, even small prediction errors can cause large relative errors after denormalization. To address this issue, we employ a simple but effective frequency adaptive normalization method: First, we independently normalize Fourier coefficients at each frequency based on statistics computed from the training set. Namely, for each individual frequency f;, we compute the 95"" percentile of Fourier coefficient magnitudes over all input samples and use that value as a per-frequency scaling factor s ,. We then apply a power transformation to each scaled Fourier coefficient to pull it away from extreme values. In practice, we observe that a square root performs better than other nonlinear transformations such as log or reciprocal. In summary, the final coefficient values of spectral volume S(p) at --- --frequency f; (used for training our LDM) are computed as St, (p) = sign(S',, ) (4) As shown on the right plot of Fig. 2, after applying frequency adaptive normalization, the spectral volume coefficients distribute more evenly. Frequency-coordinated denoising. The straightforward way to predict a spectral volume S with A frequency bands is to output a tensor of 4/¢ channels from a single diffusion U-Net. However, as in prior work [7], we observe that training a model to produce a large number of channels can yield over-smoothed, inaccurate outputs. An alternative would be to independently predict each individual frequency slice by injecting an extra frequency embedding into the LDM [4], but this design choice would result in uncorrelated predictions in the frequency domain, leading to unrealistic motion. Therefore, inspired by recent video diffusion work [7], we propose a frequency-coordinated denoising strategy, illustrated in Fig. 3. In particular, given an input image Jp, we first train an LDM €g to predict a single 4-channel frequency slice of spectral volume S'r,, where we inject an extra frequency embedding along with the time-step embedding into the LDM. We then freeze the parameters of this LDM eg, introduce attention layers interleaved with the 2D spatial layers of €g across the kK frequency bands, and fine-tune. Specifically, for a batch size B, the 2D spatial layers of eg treat the corresponding B- K noisy latent features of channel size C as independent samples with shape R\?-*)*Cx#xW | The attention layer then interprets these as consecutive features spanning the frequency axis, and we reshape the latent features from previous 2D spatial layers to RB ** xCxHxW before feeding them to the attention layers. In other words, the frequency attention layers are fine-tuned to coordinate all frequency slices so as to produce coherent spectral volumes. In our
--- EXPERIMENT ---
s, we see that the average VAE reconstruction error improves from 0.024 to 0.018 when we switch from a single 2D U-Net to a frequency-coordinated denoising module, suggesting an improved upper bound on LDM prediction accuracy; in Sec. 7.3, we also show that this design choice improves video generation quality. 5. Image-based rendering We now describe how we take a spectral volume S predicted for a given input image Jo and render a future frame I, at time t. We first derive a motion texture in the time domain using the inverse temporal FFT applied at each pixel F(p) = FFT~+(S(p)). To produce a future frame [;, we adopt a deep image-based rendering technique and perform splatting with the predicted motion field F;, to forward-warp the encoded Io, as shown in Fig. 4. Since forward warping can lead to holes, and multiple source pixels can map to the same output \ | |Softmax splatting — | a (Subject to W) Feature extractor Synthesis network Figure 4. Rendering module. We fill in missing content and refine the warped input image using a deep image-based rendering module, where multi-scale features are extracted from the input image Jp. Softmax splatting is then applied over the features with a motion field F; from time 0 to ¢ (subject to the weights W). The warped features are fed to an image synthesis network to produce the rendered image i. 2D location, we adopt the feature pyramid softmax splatting strategy proposed in prior work on frame interpolation [68]. Specifically, we encode Jp through a feature extractor network to produce a multi-scale feature map. For each individual feature map at scale j, we resize and scale the predicted 2D motion field F;, according to the resolution. As in Davis et al. [22], we use predicted flow magnitude as a proxy for depth to determine the contributing weight of each source pixel mapped to its destination location. In particular, we compute a per-pixel weight, W(p) = #4 ¥°, ||Fi(p)|las the average magnitude of the predicted motion texture. In other words, we assume large motions correspond to moving foreground objects, and small or zero motions correspond to background. We use motion-derived weights instead of learnable ones as [46] because we observe that in the singleview case, learnable weights are not effective for addressing disocclusion ambiguities. With the motion field F, and weights W, we apply softmax splatting to warp the feature map at each scale to produce a warped feature. The warped features are then injected into the corresponding blocks of an image synthesis decoder to produce a final rendered image i. We jointly train the feature extractor and synthesis networks with start and target frames (Io, I) randomly sampled from real videos, using the estimated flow field from J to I, to warp encoded features from Jo, and supervising predictions i, against J; with a VGG perceptual loss [49]. 6. Applications Image-to-video. Our system enables the animation of a single still picture by first predicting a motion spectral volume from the input image and generating an animation by applying our image-based rendering module to the motion --- --texture transformed from the spectral volume. Since we explicitly model scene motion, this allows us to produce slowmotion videos by linearly interpolating the motion texture, or to magnify (or minify) animated motions by adjusting the amplitude of the predicted spectral volume coefficients. Seamless looping. Many applications require videos that loop seamlessly, where there is no discontinuity between the start and end of the video. Unfortunately, it is hard to find a large collection of seamlessly looping videos for training. Instead, we devise a method to use our motion diffusion model, trained on regular non-looping video clips, to produce seamless looping video. Inspired by recent work on guidance for image editing [3, 30], our method is a motion self-guidance technique that guides the motion denoising sampling processing using explicit looping constraints. In particular, at each iterative denoising step during inference, we incorporate an additional motion guidance signal alongside standard classifier-free guidance [45], where we enforce each pixel’s position and velocity at the start and end frames to be as similar as possible: é"" = (1+ w)eg(2""3n, ¢) — wee(z""; 0,0) + uo"" Vin LE Ly = ||Fr — Fl + |IVFr - VF|h ©) where FY"" is the predicted 2D displacement field at time t and denoising step n. w is the classifier-free guidance weight, and wu is the motion self-guidance weight. In the supplemental video, we apply a baseline appearance-based looping algorithm [58] to generate a looping video from our non-looping output, and show that our motion self-guidance technique produces seamless looping videos with less distortion and fewer artifacts. Interactive dynamics from a single image. Davis et al. [22] show that the spectral volume, evaluated at certain resonant frequencies, can approximate an image-space modal basis that is a projection of the vibration modes of the underlying scene (or, more generally, captures spatial and temporal correlations in oscillatory dynamics), and can be used to simulate the object’s response to a user-defined force. We adopt this modal analysis method [22, 70], allowing us to write the image-space 2D motion displacement field for the object’s physical response as a weighted sum of motion spectrum coefficients S, modulated by the state of complex modal coordinates q;,(t) at each simulated time step t: Fi(p) = 2S), (p)ay, (t) (6) fi We simulate the state of the modal coordinates q,,(¢) via an explicit Euler method applied to the equations of motion for a decoupled mass-spring-damper system represented in modal space [22, 23, 70]. We refer readers to supplementary material and original work for a full derivation. Note that our method produces an interactive scene from a single picture, whereas these prior methods required a video as input. Image Synthesis Video Synthesis Method FID KID FVD FVD32 DIFVD DTFVDTATS [35] 65.8 1.67 265.6 419.6 22.6 40.Stochastic I2V [27] 68.3. 3.12 253.5 320.9 16.7MCVD [93] 63.4 2.97 208.6 270.4 19.5 53.LFDM [67] 47.6 1.70 187.5 254.3 13.0 45.DMVEN [48] 37.9 1.09 206.5 316.3 11.2 54.Endo et al. [29] 10.4 0.19 166.0 231.6 5.35 65.Holynski ef al. [46] 11.20.20 179.0 253.7 7.23 46.Ours 4.03 0.08 47.1 62.9 2.53 6.Table 1. Quantitative comparisons on the test set. We report both image synthesis and video synthesis quality. Here, KID is scaled by 100. Lower is better for all error. See Sec. 7.1 for descriptions of baselines and error metrics. 7. Experiments Implementation details. We use an LDM [74] as the backbone for predicting spectral volumes, for which we use a VAE with a continuous latent space of dimension 4. We train the VAE with an L, reconstruction loss, a multi-scale gradient consistency loss [54-56], and a KL-divergence loss with respective weights of 1,0.2,10~°. We train the same 2D U-Net used in the original LDM work to perform iterative denosing with a simple MSE loss [44], and adopt the attention layers from [41] for frequency-coordinated denoising. For quantitative evaluation, we train both VAE and LDM on images of size 256 x 160 from scratch for fair comparisons, and it takes around 6 days to converge using 16 Nvidia AGPUs. For our main quantitative and qualitative results, we run the motion diffusion model with DDIM [86] forsteps. We also show generated videos of up to a resolution of 512 x 288, created by fine-tuning a pre-trained image inpainting LDM model [74] on our dataset. We adopt ResNet-34 [39] for the feature extractor in our IBR module. Our image synthesis network is based on an architecture for conditional image inpainting [57, 110]. Our rendering module runs in real-time at 25FPS on a Nvidia V100 GPU during inference. We adopt universal guidance [3] to produce seamless looping videos, where we set weights w = 1.75, u = 200, and use 500 DDIM steps with 2 self-recurrence iterations. Data. We collect and process a set of 3,015 videos of natural scenes exhibiting oscillatory motions from online sources our own captures. We withhold 10% of videos for testing and use the remainder for training. To extract ground truth motion trajectories, we apply a coarse-to-fine flow method [10, 61] between each selected starting image and every future frame of the video. As training data, we take every 10th video frame as input images and derive the corresponding ground truth spectral volumes using the computed motion trajectories across the following 149 frames. In total, our data consists of over 150K image-motion pairs. --- --Input image Reference Stochastic-I2V [27] MCVD [93] UT Endo et al. [29] Holynski et al. [46] Ours Figure 5. X-t slices of videos generated by different approaches. From left to right: input image and corresponding X-t video slices from the ground truth video, from videos generated by three baselines [27, 29, 46, 93], and finally videos generated by our approach. Sling Window FID. Sling Window DT-FVD oo a Frame index Frame index Figure 6. Sliding window FID and DTFVD. We show sliding window FID with window size 30 frames, and DIFVD with size 16 frames, for videos generated by different methods. Baselines. We compare our approach to recent singleimage animation and video prediction methods. Endo et al. [29] and DMVEN [48] predict instantaneous 2D motion fields and render future frames auto-regressively. Holynski et al. [46] instead simulates motion through a single static Eulerian motion description. Other recent work such as Stochastic Image-to- Video (Stochastic-I2V) [27], TATS [35], and MCVD [93] adopt VAEs, transformers, or diffusion models to directly predict raw video frames; LFDM [67] generates future frames by predicting flow volumes and warping latents in a diffusion model. We train all the above methods on our data using their respective open-source implementations. ! We evaluate the quality of the videos generated by our approach and by prior baselines in two ways. First, we evaluate the quality of individual synthesized frames using metrics designed for image synthesis tasks. We adopt the Fréchet Inception Distance (FID) [42] and Kernel Inception Distance (KID) [5] to measure the average distance between the distributions of generated frames and ground truth frames. Second, to evaluate the quality and temporal coherence 'We use the open-source implementation of [46] from Fan er al. [83]. Image Synthesis Video Synthesis Method FID KID FVD FVD32 DIFVD DTFVDRepeat Io - - 237.5 316.7 5.30 45.Kk=4 3.92 0.07 60.3 784 3.12 8.K=8 3.95 0.07 52.1 68.7 2.71 7.K = 24 4.09 0.08 48.2 65.1 2.50 6.w/o adaptive norm. 4.53 0.09 62.7 80.1 3.16 8.Independent pred. 4.00 0.08 52.5 71.3 2.70 7.Volume pred. 474 0.09 53.7) 71.1 2.83 7.Baseline splat [46] 4.25 0.09 49.5 66.8 2.83 7.Full (K = 16) 4.03 0.08 47.1 62.9 2.53 6.Table 2. Ablation study. Sec. 7.3 describes each configuration. of synthesized videos, we adopt the Fréchet Video Distance [92] with window size 16 (FVD) and 32 (FVD32), based on an I3D model [1 1] trained on the Human Kinetics datasets [52]. To more faithfully reflect synthesis quality for the natural oscillation motions we seek to generate, we also adopt the Dynamic Texture Frechet Video Distance [27], which measures distance from videos with window size 16 (DTFVD) and size 32 (DTFVD32), using a 13D model trained on the Dynamic Textures Database [37], a dataset consisting primarily of natural motion textures. We further use a sliding window FID of window sizeframes, and a sliding window DTFVD with window size 16 frames, as in [57, 60], to measure how generated video quality degrades over time. For all methods, we evaluate metrics at 256 x 128 resolution by center-cropping. --- --AnimateDiff ModelScope Figure 7. We show generated future frames from three recent large video diffusion models [31, 36, 98]. 7.1. Quantitative results Table | shows quantitative comparisons between our approach and baselines on our test set. Our approach significantly outperforms prior single-image animation baselines in terms of both image and video synthesis quality. Specifically, our much lower FVD and DT-FVD distances suggest that the videos generated by our approach are more realistic and more temporally coherent. Further, Fig. 6 shows the sliding window FID and sliding window DT-FVD distances of generated videos from different methods. Thanks to the global spectral volume representation, videos generated by our approach do not suffer from degradation over time. 7.2. Qualitative results We visualize qualitative comparisons between videos as spatio-temporal X-t slices of the generated videos, a standard way of visualizing small motions in a video [95]. As shown in Fig. 5, our generated video dynamics more strongly resemble the motion patterns observed in the corresponding real reference videos (second column), compared to other methods. Baselines such as Stochastic I2V [27] and MCVD [93] fail to model both appearance and motion realistically over time. Endo ef al. [29] and Holynski et al. [46] produce video frames with fewer artifacts but exhibits oversmooth or non-oscillatory motion over time. We refer readers to supplementary material to assess the quality of generated video frames and estimated motion across different methods. 7.3. Ablation study We conduct an ablation study to validate the major design choices in our motion prediction and rendering modules, comparing our full configuration with different variants. Specifically, we evaluate results using different numbers of frequency bands kK = 4, 8, 16,24. We observe that increasing the number of frequency bands improves video prediction quality, but the improvement is marginal with more than 16 frequencies. Next, we remove adaptive frequency normalization from the ground truth spectral volumes, and instead just scale them based on input image width and height (w/o adaptive norm.). Additionally, we remove the frequency coordinated-denoising module (Independent pred.), or replace it with a simpler DM where a tensor volume of 4/¢ channel spectral volumes are predicted jointly via a single 2D U-net diffusion model (Volume pred.). Finally, we compare results where we use a baseline rendering method that Figure 8. Limitations. We show examples of rendered future frames (even), and overlay of input and rendered images (odd). Our method can produce artifacts in regions of thin objects or large motions, and regions requiring filling large amount of new contents. applies softmax splatting over single-scale features subject to learnable weights as used by [46] (Baseline splat). We also add a baseline where the generated video is a volume by repeating input image N times (Repeat Jo). From Table 2, we observe that all simpler or alternative configurations lead to worse performance compared with our full model. 7.4, Comparing to large video models We further perform a user study and compare our generated animations with ones from recent large video diffusion models: AnimateDiff [36], ModelScope [98] and Gen-2 [31], which predict video volumes directly. On a randomly selected 30 videos from the test set, we ask users “which video is more realistic?”. Users report a 80.9% preference for our approach over others. Moreover, as shown in Fig. 7, we observe that the generated videos from these baselines are either unable to adhere to the input image content, or exhibit gradual color drift and distortion over time. We refer readers to the supplementary material for full comparisons. 8. Discussion and
--- CONCLUSION ---
Limitations. Since our approach only predicts lower frequencies of a spectral volume, it can fail to model nonoscillating motions or high-frequency vibrations—this may be resolved by using learned motion bases. Furthermore, the quality of generated videos relies on the quality of underlying motion trajectories, which may degrade in scenes with thin moving objects or objects with large displacements. Even if correct, motions that require generating large amounts of new unseen content may also cause degradations (Fig. 8). Conclusion. We present a new approach for modeling natural oscillation dynamics from a single still picture. Our image-space motion prior is represented with spectral volumes, a frequency representation of per-pixel motion trajectories, which we find to be efficient and effective for prediction with diffusion models, and which we learn from collections of real world videos. Spectral volumes are predicted using frequency-coordinated latent diffusion model and are used to animate future video frames through an image-based rendering module. We show that our approach produces photo-realistic animations from a single picture and significantly outperforms prior baselines, and that it can enable several downstream applications such as creating seamlessly looping or interactive image dynamics. --- --Acknowledgements. We thank Abe Davis, Rick Szeliski, Andrew Liu, Boyang Deng, Qiangian Wang, Xuan Luo, and Lucy Chai for fruitful discussions and helpful comments. ReferencesAseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh Agrawala, Michael Cohen, Brian Curless, David Salesin, and Richard Szeliski. Panoramic video textures. In ACM Trans. Graphics (SIGGRAPH), pages 821-827. 2005. Hyemin Ahn, Esteve Valls Mascaro, and Dongheui Lee. Can we use diffusion probabilistic models for 3d motion prediction? arXiv preprint arXiv:2302. 14503, 2023. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 843-852, 2023. Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, ChunHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio Escalera, and Duygu Ceylan. Blowing in the wind: Cyclenet for human cinemagraphs from still images. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 459468, 2023. Mikotaj Bifikowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv preprint arXiv: 1801.01401, 2018. Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjérn Ommer. ipoke: Poking a still image for controlled stochastic video synthesis. In Proc. Int. Conf. on Computer Vision (ICCV), pages 14707-14717, 2021. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 22563-22575, 2023. Richard Strong Bowen, Richard Tucker, Ramin Zabih, and Noah Snavely. Dimensions of motion: Monocular prediction through flow subspaces. In /nternational Conference on 3D Vision (3DV), pages 454-464, 2022. Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Neural Information Processing Systems, 35:31769— 31781, 2022. Thomas Brox, Andrés Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on a theory for warping. In Proc. European Conf. on Computer Vision (ECCV), pages 25-36, 2004. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 6299-6308, 2017. Dan Casas, Marco Volino, John Collomosse, and Adrian Hilton. 4d video textures for interactive character appearance. In Computer Graphics Forum, volume 33, pages 371-380. Wiley Online Library, 2014. Antoni B Chan and Nuno Vasconcelos. Mixtures of dynamic textures. In Proc. Int. Conf: on Computer Vision (ICCV),pages 641-647, 2005. Antoni B Chan and Nuno Vasconcelos. Classifying video with kernel dynamic textures. In Proc. Computer Vision and Pattern Recognition (CVPR), 2007. Antoni B Chan and Nuno Vasconcelos. Modeling, clustering, and segmenting video with mixtures of dynamic textures. Trans. Pattern Analysis and Machine Intelligence, 30(5):909926, 2008. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 11315-11325, 2022. Tsai-Shien Chen, Chieh Hubert Lin, Hung- Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304. 14404, 2023. Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 18000-18010, 2023. Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, and Richard Szeliski. Animating pictures with stochastic motion textures. In ACM Trans. Graphics (SIGGRAPH), pages 853-860, 2005. Vincent C Couture, Michael S Langer, and Sebastien Roy. Omnistereo video textures without ghosting. In Jnternational Conference on 3D Vision, pages 64-70. IEEE, 2013. Abe Davis, Justin G Chen, and Frédo Durand. Image-space modal bases for plausible manipulation of objects in video. ACM Trans. Graphics (SIGGRAPH), 34(6):1-7, 2015. Myers Abraham Davis. Visual vibration analysis. PhD thesis, Massachusetts Institute of Technology, 2016. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Neural Information Processing Systems, 34:8780-8794, 2021. Julien Diener, Mathieu Rodriguez, Lionel Baboud, and Lionel Reveret. Wind projection basis for real-time animation of trees. In Computer graphics forum, volume 28, pages 533-540. Wiley Online Library, 2009. Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and Stefano Soatto. Dynamic textures. 51:91—109, 2003. Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis, and Bjorn Ommer. Stochastic image-to-video synthesis using cinns. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 3742-3753, June 2021. Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 481-490, 2023. Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. Animating landscape: Self-supervised learning of decoupled motion and appearance for single-image video synthe --- ---sis. ACM Trans. Graphics (SIGGRAPH Asia), 38(6):175:1175:19, 2019. Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986, 2023. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proc. Int. Conf. on Computer Vision (ICCV), pages 73467356, 2023. Matthew Flagg, Atsushi Nakazawa, Qiushuang Zhang, Sing Bing Kang, Young Kee Ryu, Irfan Essa, and James M Rehg. Human video textures. In Proceedings of thesymposium on Interactive 3D graphics and games, pages 199-206, 2009. Jean-Yves Franceschi, Edouard Delasalles, Mickaél Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. In International Conference on Machine Learning, pages 3233-3246. PMLR, 2020. Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2Flow: Motion hallucination from static images for action recognition. In Proc. Computer Vision and Pattern Recognition (CVPR), 2018. Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. arXiv preprint arXiv:2204.03638, 2022. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Isma Hadji and Richard P Wildes. A new large scale dynamic texture dataset with application to convnet understanding. In Proc. European Conf. on Computer Vision (ECCV), pages 320-335, 2018. Zekun Hao, Xun Huang, and Serge Belongie. Controllable video generation with sparse trajectories. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 7854-7863, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 770— 778, 2016. Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Neural Information Processing Systems, 30, 2017. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Neural Information Processing Systems, 33:6840-685 1, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Aleksander Holynski, Brian L Curless, Steven M Seitz, and Richard Szeliski. Animating pictures with Eulerian motion fields. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5810-5819, 2021. Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. Trans. Mach. Learn. Res., 2022, 2022. Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and Shuchang Zhou. A dynamic multi-scale voxel flow network for video prediction. ArXiv, abs/2303.09875, 2023. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proc. European Conf. on Computer Vision (ECCV), pages 694-711, 2016. Hitoshi Kanda and Jun Ohya. Efficient, realistic method for animating dynamic behaviors of 3d botanical trees. In Jnternational Conference on Multimedia and Expo, volume 2, pages II-89. IEEE, 2003. Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint arXiv:2304.06025, 2023. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. arXiv preprint arXiv: 1804.01523, 2018. Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman. Learning the depths of moving people by watching frozen people. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 4521-4530, 2019. Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 2041-— 2050, 2018. Zhengqi Li, Qiangian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 4273-4284, 2023. Zhengqi Li, Qiangian Wang, Noah Snavely, and Angjoo Kanazawa. Infinitenature-zero: Learning perpetual view generation of natural scenes from single images. In Proc. European Conf. on Computer Vision (ECCV), pages 515534. Springer, 2022. Jing Liao, Mark Finch, and Hugues Hoppe. Fast computation of seamless video loops. ACM Trans. Graphics (SIG --- ---GRAPAH), 34(6):1-10, 2015. Zicheng Liao, Neel Joshi, and Hugues Hoppe. Automated video looping with progressive dynamism. ACM Transactions on Graphics (TOG), 32(4):1—-10, 2013. Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. In Proc. Int. Conf. on Computer Vision (ICCV), pages 14458-14467, 2021. Ce Liu. Beyond pixels: exploring new representations and applications for motion analysis. PhD thesis, Massachusetts Institute of Technology, 2009. Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 10209-10218, 2023. Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable animation of fluid elements in still images. In Proc. Computer Vision and Pattern Recognition (CVPR), 2022. Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin- Ying Lee, Sergey Tulyakov, and Jun-Yan Zhu. Text-guided synthesis of eulerian cinemagraphs. 2023. Arun Mallya, Ting-Chun Wang, and Ming- Yu Liu. Implicit warping for animation with image sets. Neural Information Processing Systems, 35:22438-22450, 2022. Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A Efros, and Trevor Darrell. Strumming to the beat: Audio-conditioned contrastive video textures. In Proc. Winter Conference on Applications of Computer Vision, pages 3761-3770, 2022. Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Rengiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 18444-18455, 2023. Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5437-5446, 2020. Shin Ota, Machiko Tamura, Kunihiko Fujita, T Fujimoto, K Muraoka, and Norishige Chiba. 1/f/sup/spl beta//noisebased real-time animation of trees swaying in wind fields. In Proceedings Computer Graphics International, pages 52-59. IEEE, 2003. Automne Petitjean, Yohan Poirier-Ginter, Ayush Tewari, Guillaume Cordonnier, and George Drettakis. Modalnerf: Neural modal analysis and synthesis for free-viewpoint navigation in dynamically vibrating scenes. In Computer Graphics Forum, volume 42, 2023. Silvia L. Pintea, Jan C. van Gemert, and Arnold W. M. Smeulders. Déja vu: Motion prediction in static images. In Proc. European Conf. on Computer Vision (ECCV), 2014. Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, and Daniel Cohen-Or. Single motion diffusion. arXiv preprint arXiv:2302.05905, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [74] (75]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Neural Information Processing Systems, 35:36479-36494, 2022. Payam Saisan, Gianfranco Doretto, Ying Nian Wu, and Stefano Soatto. Dynamic texture recognition. In Proc. Computer Vision and Pattern Recognition (CVPR), 2001. Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David J. Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation, 2023. Arno Schédl, Richard Szeliski, David H Salesin, and Irfan Essa. Video textures. In ACM Trans. Graphics (SIGGRAPH), pages 489-498, 2000. Mikio Shinya and Alain Fournier. Stochastic motion— motion under the influence of wind. Computer Graphics Forum, 11(3), 1992. Aliaksandr Siarohin, Stéphane Lathuiliére, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 2377-2386, 2019. Aliaksandr Siarohin, Stéphane Lathuiliére, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Neural Information Processing Systems, 32, 2019. Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 13653-13662, 2021. Chen Qian Kwan-Yee Lin Hongsheng Li Siming Fan, Jingtan Piao. Simulating fluids in real-world still images. arXiv preprint, arXiv:2204.11335, 2022. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 3626— 3636, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Jnternational conference on machine learning, pages 2256-2265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, October 2020. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Jos Stam. Multi-scale stochastic modelling of complex natural phenomena. PhD thesis, 1995. Jos Stam. Stochastic dynamics: Simulating the effects of turbulence on flexible structures. Computer Graphics Forum, --- ---16(3), 1997. Ryusuke Sugimoto, Mingming He, Jing Liao, and Pedro V Sander. Water simulation and rendering from a still photograph. In ACM Trans. Graphics (SIGGRAPH Asia), pages 1-9, 2022. Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional video diffusion for prediction, generation, and interpolation. In Neural Information Processing Systems, 2022. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In Neural Information Processing Systems, 2016. Neal Wadhwa, Michael Rubinstein, Frédo Durand, and William T Freeman. Phase-based video motion processing. ACM Trans. Graphics (SIGGRAPH), 32(4):1-10, 2013. Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In Proc. European Conf. on Computer Vision (ECCV), 2016. Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense optical flow prediction from a static image. In Proc. Int. Conf. on Computer Vision (ICCV), pages 2443-2451, 2015. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018, 2023. Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022. Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, and Angjoo Kanazawa. Nerfbusters: Removing ghostly artifacts from casually captured nerfs. arXiv preprint arXiv:2304. 10532, 2023. Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022. Chung-Yi Weng, Brian Curless, and Ira KemelmacherShlizerman. Photo wake-up: 3d character animation from a single photo. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5908-5917, 2019. Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models. In Proc. Computer Vision and Pattern Recognition (CVPR), 2023. Tianfan Xue, Jiajun Wu, Katherine L Bouman, and William T Freeman. Visual dynamics: Stochastic future generation via layered cross convolutional networks. Trans. Pattern Analysis and Machine Intelligence, 41(9):2236-2250, 2019.Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Hougiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 18456-18466, 2023. Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. arXiv preprint arXiv:2304.01116, 2023. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 3657-3666, 2022. Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. In International Conference on Learning Representations (ICLR), 2021. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.
"	"--- ABSTRACT ---
장면 동작에 대한 이미지 공간 사전 모델을 모델링하는 방법을 제시합니다. 사전 모델은 실제 비디오에서 추출한 동작 궤적 모음에서 학습합니다. 나무, 꽃, 양초, 바람에 흔들리는 옷과 같은 물체의 자연스러운 진동 역학을 묘사하는 시퀀스입니다. 푸리에 도메인에서 밀도가 높고 장기적인 동작을 스펙트럼 볼륨으로 모델링하는데, 이는 확산 모델을 사용한 예측에 적합하다는 것을 알게 되었습니다. 단일 이미지가 주어지면, 훈련된 모델은 주파수 조정 확산 샘플링 프로세스를 사용하여 스펙트럼 볼륨을 예측하는데, 이는 전체 비디오에 걸친 동작 텍스처로 변환할 수 있습니다. 이미지 기반 렌더링 모듈과 함께 예측된 동작 표현은 정지 이미지를 매끄럽게 반복되는 비디오로 바꾸거나, 사용자가 실제 이미지의 객체와 상호 작용하여 사실적인 시뮬레이션 역학을 생성하는 등 여러 다운스트림 애플리케이션에 사용할 수 있습니다(스펙트럼 볼륨을 이미지 공간 모달 기반으로 해석). 더 많은 결과는 프로젝트 페이지에서 확인하세요: generative-dynamics.github.io. 1.
--- INTRODUCTION ---
자연 세계는 항상 움직이고 있으며, 겉보기에 정적인 장면조차도 바람, 물의 흐름, 호흡 또는 기타 자연적 리듬으로 인한 미묘한 진동을 포함합니다. 이러한 동작을 에뮬레이션하는 것은 시각적 콘텐츠 합성에서 중요합니다. 동작에 대한 인간의 민감성으로 인해 동작이 없는(또는 약간 비현실적인 동작이 있는) 이미지가 기이하거나 비현실적으로 보일 수 있습니다. 인간이 장면에서 동작을 해석하거나 상상하는 것은 쉽지만, 현실적인 장면 동작을 학습하거나 생성하도록 모델을 훈련하는 것은 결코 사소한 일이 아닙니다. 우리가 세상에서 관찰하는 동작은 장면의 기본 물리적 역학, 즉 고유한 물리적 속성(질량, 탄성 등)에 따라 반응하는 물체에 적용되는 힘의 결과입니다. 이러한 양은 측정하고 규모에 맞게 포착하기 어렵습니다. 다행히도 특정 응용 프로그램에서는 이를 측정할 필요가 없습니다. 예를 들어, 관찰된 2D 동작을 분석하기만 하면 장면에서 그럴듯한 역학을 시뮬레이션할 수 있습니다[23]. 동일한 관찰된 동작은 장면 간 역학을 학습하는 데 감독 신호 역할을 할 수도 있습니다.관찰된 동작은 다중 모드이고 복잡한 물리적 효과에 기반을 두고 있지만 그럼에도 불구하고 종종 예측 가능하기 때문입니다.양초는 특정 방식으로 깜빡거리고, 나무는 흔들리고, 잎은 바스락거립니다.인간의 경우 이러한 예측 가능성은 우리의 지각 체계에 각인되어 있습니다.정지된 이미지를 보면 그럴듯한 동작을 상상할 수 있습니다.또는 그러한 동작이 많이 있을 수 있으므로 해당 이미지에 따라 자연스러운 동작의 분포를 상상할 수 있습니다.인간이 이러한 분포를 모델링할 수 있는 용이성을 감안할 때 자연스러운 연구 문제는 이를 계산적으로 모델링하는 것입니다.특히 조건부 확산 모델[44, 85, 87]에서 생성 모델의 최근 발전으로 인해 텍스트에 따라 실제 이미지의 분포를 포함하여 풍부한 분포를 모델링할 수 있게 되었습니다[73–75].이 기능을 통해 다양하고 사실적인 이미지 콘텐츠의 텍스트 조건 생성과 같은 여러 가지 새로운 응용 프로그램이 가능해졌습니다. 이러한 이미지 모델의 성공에 따라 최근 작업에서는 이러한 모델을 비디오[7, 43] 및 3D 기하학[77, 100, 101, 103]과 같은 다른 도메인으로 확장했습니다. 이 논문에서 우리는 이미지 공간 장면 동작, 즉 단일 이미지의 모든 픽셀의 동작에 대한 생성 사전을 모델링합니다. 이 모델은 대규모 실제 비디오 시퀀스 컬렉션에서 자동으로 추출된 동작 궤적에 대해 학습합니다. 특히, 각 학습 비디오에서 우리는 스펙트럼 볼륨[22, 23]의 형태로 동작을 계산합니다. 스펙트럼 볼륨은 밀도가 높고 장거리 픽셀 궤적의 주파수 영역 표현입니다. 스펙트럼 볼륨은 바람에 움직이는 나무와 꽃과 같이 진동 역학을 보이는 장면에 적합합니다. 우리는 이 표현이 장면 동작을 모델링하기 위한 확산 모델의 출력으로도 매우 효과적이라는 것을 발견했습니다. 우리는 단일 이미지를 조건으로 하여 학습된 분포에서 스펙트럼 볼륨을 샘플링할 수 있는 생성 모델을 학습합니다. 예측된 스펙트럼 볼륨은 모션 텍스처(픽셀당 장거리 모션 궤적 세트)로 직접 변환할 수 있으며, 이를 사용하여 이미지를 애니메이션화할 수 있습니다. 스펙트럼 볼륨은 또한 대화형 역학을 시뮬레이션하는 데 사용할 이미지 공간 모달 기반으로 해석할 수 있습니다[22]. 우리는 한 번에 한 주파수씩 계수를 생성하지만 공유된 주의 모듈을 통해 주파수 대역에서 이러한 예측을 조정하는 확산 모델을 사용하여 입력 이미지에서 스펙트럼 볼륨을 예측합니다. 예측된 모션은 미래 프레임을 합성하는 데 사용할 수 있습니다(이미지 기반 렌더링 모델을 통해). 그림 1에서 볼 수 있듯이 정지 이미지를 사실적인 애니메이션으로 전환합니다. 원시 RGB 픽셀에 대한 사전 확률과 비교할 때 모션에 대한 사전 확률은 픽셀 값의 장거리 변화를 효율적으로 설명하는 더 기본적이고 저차원적인 구조를 캡처합니다. 따라서 중간 모션을 생성하면 더 일관된 장기 생성과 애니메이션에 대한 더 세밀한 제어가 가능합니다. 우리는 원활한 루핑 비디오 생성, 생성된 동작 편집, 이미지 공간 모달 기반을 통한 대화형 동적 이미지 활성화(즉, 사용자가 적용한 힘에 대한 객체 역학의 응답 시뮬레이션[22])와 같은 여러 다운스트림 애플리케이션에서 훈련된 모델의 사용을 보여줍니다.
--- RELATED WORK ---
생성 합성. 생성 모델의 최근 발전으로 텍스트 프롬프트에 따라 조건화된 이미지의 사실적인 합성이 가능해졌습니다[16, 17, 24, 73-75]. 이러한 텍스트-이미지 모델은 생성된 이미지 텐서를 시간 차원을 따라 확장하여 비디오 시퀀스를 합성하도록 확장될 수 있습니다[7, 9, 43, 62, 84, 106, 106, 111]. 이러한
--- METHOD ---
영어: 스펙트럼 볼륨[23]을 생성합니다. 스펙트럼 볼륨은 푸리에 영역에서 밀도가 높고 장기적인 픽셀 궤적을 모델링하는 동작 표현입니다. 학습된 동작 사전 확률은 단일 사진을 원활하게 반복되는 비디오로 바꾸거나, 드래그 앤 릴링 포인트와 같은 사용자 입력에 응답하는 역학의 대화형 시뮬레이션으로 바꾸는 데 사용할 수 있습니다. 오른쪽에서 출력 비디오를 공간-시간 Xt 슬라이스(왼쪽에 표시된 입력 스캔라인을 따라)로 시각화합니다. 초록 장면 동작에 대한 이미지 공간 사전 확률을 모델링하는 방법을 제시합니다. 사전 확률은 실제 비디오에서 추출한 동작 궤적 모음에서 학습합니다. 나무, 꽃, 양초, 바람에 흔들리는 옷과 같은 물체의 자연스러운 진동 역학을 묘사하는 시퀀스입니다. 푸리에 영역에서 밀도가 높고 장기적인 동작을 스펙트럼 볼륨으로 모델링하는데, 이는 확산 모델을 사용한 예측에 적합하다는 것을 알게 되었습니다. 단일 이미지가 주어지면, 훈련된 모델은 주파수 조정 확산 샘플링 프로세스를 사용하여 스펙트럼 볼륨을 예측하고, 이를 전체 비디오에 걸친 모션 텍스처로 변환할 수 있습니다. 이미지 기반 렌더링 모듈과 함께 예측된 모션 표현은 정지 이미지를 원활하게 반복되는 비디오로 바꾸거나 사용자가 실제 이미지의 객체와 상호 작용하여 사실적인 시뮬레이션 역학을 생성하는 등 여러 다운스트림 애플리케이션에 사용할 수 있습니다(스펙트럼 볼륨을 이미지 공간 모달 기반으로 해석). 더 많은 결과는 프로젝트 페이지 generative-dynamics.github.io에서 확인하세요. 1. 서론 자연 세계는 항상 움직이고 있으며, 겉보기에 정적인 장면에도 바람, 물의 흐름, 호흡 또는 기타 자연적 리듬으로 인한 미묘한 진동이 포함됩니다. 이러한 모션을 에뮬레이션하는 것은 시각적 콘텐츠 합성에서 중요합니다. 모션에 대한 인간의 민감성으로 인해 모션이 없는(또는 약간 비현실적인 모션이 있는) 이미지가 기이하거나 비현실적으로 보일 수 있습니다. 사람이 장면에서 모션을 해석하거나 상상하는 것은 쉽지만, 모델을 훈련하여 사실적인 장면 모션을 학습하거나 생성하는 것은 결코 쉬운 일이 아닙니다. 세상에서 우리가 관찰하는 움직임은 장면의 근본적인 물리적 역학, 즉 고유한 물리적 속성(질량, 탄성 등)에 따라 반응하는 물체에 적용되는 힘의 결과입니다. 이러한 양은 측정하고 규모에 맞게 포착하기 어렵습니다. 다행히도 특정 응용 프로그램에서는 측정이 필요하지 않습니다. 예를 들어, 관찰된 2D 움직임을 분석하기만 하면 장면에서 그럴듯한 역학을 시뮬레이션할 수 있습니다[23]. 이 동일한 관찰된 움직임은 장면 전체에서 역학을 학습하는 데 있어 감독 신호 역할을 할 수도 있습니다. 관찰된 움직임은 다중 모드이고 복잡한 물리적 효과에 기반을 두고 있지만 그럼에도 불구하고 종종 예측 가능하기 때문입니다. 촛불은 특정 방식으로 깜빡거리고, 나무는 흔들리고, 잎사귀는 바스락거립니다. 인간의 경우 이러한 예측 가능성은 우리의 지각 체계에 각인되어 있습니다. 정지 이미지를 보면 그럴듯한 움직임을 상상할 수 있습니다. 또는 그러한 움직임이 많이 있을 수 있으므로 해당 이미지에 따라 자연스러운 움직임의 분포를 상상할 수 있습니다. 인간이 이러한 분포를 모델링할 수 있는 능력을 감안할 때, 자연스러운 연구 문제는 이를 계산적으로 모델링하는 것입니다. 생성 모델, 특히 조건부 확산 모델[44, 85, 87]의 최근 발전으로 인해 텍스트에 따라 조건화된 실제 이미지 분포[73–75]를 포함한 풍부한 분포를 모델링할 수 있게 되었습니다. 이 기능을 통해 다양하고 사실적인 이미지 콘텐츠의 텍스트 조건 생성과 같은 여러 가지 새로운 응용 프로그램이 가능해졌습니다. 이러한 이미지 모델의 성공에 따라 최근 작업에서는 이러한 모델을 비디오[7, 43] 및 3D 기하학[77, 100, 101, 103]과 같은 다른 도메인으로 확장했습니다. 이 논문에서는 이미지 공간 장면 동작, 즉 단일 이미지의 모든 픽셀 동작에 대한 생성 사전을 모델링합니다. 이 모델은 대규모 실제 비디오 시퀀스 컬렉션에서 자동으로 추출된 동작 궤적에 대해 학습합니다. 특히, 각 훈련 비디오에서 스펙트럼 볼륨[22, 23]의 형태로 동작을 계산합니다. 스펙트럼 볼륨은 밀도가 높고 장거리 픽셀 궤적의 주파수 영역 표현입니다. 스펙트럼 볼륨은 진동 역학을 보이는 장면, 예를 들어 바람에 움직이는 나무와 꽃에 적합합니다. 이 표현은 장면 동작을 모델링하기 위한 확산 모델의 출력으로도 매우 효과적이라는 것을 발견했습니다. 단일 이미지를 조건으로 하여 학습된 분포에서 스펙트럼 볼륨을 샘플링할 수 있는 생성 모델을 훈련합니다. 예측된 스펙트럼 볼륨은 모션 텍스처(픽셀당 장거리 모션 궤적 세트)로 직접 변환할 수 있으며, 이를 사용하여 이미지를 애니메이션화할 수 있습니다. 스펙트럼 볼륨은 또한 대화형 역학을 시뮬레이션하는 데 사용할 이미지 공간 모달 기반으로 해석할 수 있습니다[22]. 한 번에 한 주파수씩 계수를 생성하지만 공유된 주의 모듈을 통해 주파수 대역 전체에서 이러한 예측을 조정하는 확산 모델을 사용하여 입력 이미지에서 스펙트럼 볼륨을 예측합니다. 예측된 동작은 미래 프레임을 합성하는 데 사용할 수 있습니다(이미지 기반 렌더링 모델을 통해).그림 1에서 볼 수 있듯이 정지 이미지를 사실적인 애니메이션으로 전환합니다.원시 RGB 픽셀에 대한 사전 확률과 비교할 때 동작에 대한 사전 확률은 더 기본적이고 저차원 구조를 캡처하여 픽셀 값의 장거리 변화를 효율적으로 설명합니다.따라서 중간 동작을 생성하면 보다 일관된 장기 생성과 애니메이션에 대한 보다 세밀한 제어가 가능합니다.우리는 원활한 루핑 비디오 생성, 생성된 동작 편집, 이미지 공간 모달 기반을 통한 대화형 동적 이미지 활성화(즉, 사용자가 적용한 힘에 대한 객체 역학의 응답을 시뮬레이션)와 같은 여러 다운스트림 애플리케이션에서 훈련된 모델을 사용하는 방법을 보여줍니다[22].2. 관련 연구 생성 합성.생성 모델의 최근 발전으로 텍스트 프롬프트를 조건으로 한 이미지의 사실적인 합성이 가능해졌습니다[16, 17, 24, 73-75]. 이러한 텍스트-이미지 모델은 생성된 이미지 텐서를 시간 차원을 따라 확장하여 비디오 시퀀스를 합성하도록 확장될 수 있습니다[7, 9, 43, 62, 84, 106, 106, 111]. 이러한 방법은 실제 영상의 시공간적 통계를 포착하는 비디오 시퀀스를 생성할 수 있지만 이러한 비디오는 종종 비일관적인 동작, 텍스처의 비현실적인 시간적 변화, 질량 보존과 같은 물리적 제약 위반과 같은 아티팩트가 발생합니다. 이미지 애니메이션. 다른 기술은 전적으로 텍스트에서 비디오를 생성하는 대신 정지 사진을 입력으로 사용하여 애니메이션을 적용합니다. 최근의 많은 딥 러닝 방법은 3DUnet 아키텍처를 채택하여 비디오 볼륨을 직접 생성합니다[27, 36, 40, 47, 53, 93]. 이러한 모델은 효과적으로 동일한 비디오 생성 모델(그러나 텍스트 대신 이미지 정보를 조건으로 함)이며 위에서 언급한 모델과 유사한 아티팩트를 나타냅니다. 이러한 한계를 극복하는 한 가지 방법은 비디오 콘텐츠 자체를 직접 생성하지 않고 이미지 기반 렌더링을 통해 입력 소스 이미지에 애니메이션을 적용하는 것입니다.즉, 주행 비디오[51, 80-82, 99], 동작 또는 3D 기하 사전 확률[8, 29, 46, 63-65, 67, 90, 97, 101, 102, 104, 109] 또는 사용자 주석[6, 18, 20, 33, 38, 98, 105, 108]과 같은 외부 소스에서 파생된 동작에 따라 이미지 콘텐츠를 이동합니다.동작 필드에 따라 이미지를 애니메이션화하면 시간적 일관성과 사실성이 더 높아지지만 이러한 사전 방법은 추가 안내 신호 또는 사용자 입력이 필요하거나 제한된 동작 표현을 활용합니다.동작 모델 및 동작 사전 확률. 컴퓨터 그래픽에서 자연스러운 진동 3D 모션(예: 물결치는 물이나 바람에 흔들리는 나무)은 푸리에 영역에서 형성된 노이즈로 모델링한 다음 시간 영역 모션 필드로 변환할 수 있습니다[79, 88]. 이러한 방법 중 일부는 시뮬레이션되는 시스템의 기본 역학에 대한 모달 분석에 의존합니다[22, 25, 89]. 이러한 스펙트럼 기술은 Chuang et al.[20]에 의해 사용자 주석이 제공된 단일 2D 사진에서 식물, 물, 구름을 애니메이션화하는 데 적용되었습니다. 저희의 작업은 특히 Davis[23]에서 영감을 받았는데, 그는 장면의 모달 분석을 해당 장면의 비디오에서 관찰된 모션과 연결하고 이 분석을 사용하여 비디오에서 대화형 역학을 시뮬레이션했습니다. 저희는 Davis et al.의 주파수 공간 스펙트럼 볼륨 모션 표현을 채택하고, 이 표현을 많은 수의 교육 비디오에서 추출하고, 스펙트럼 볼륨이 확산 모델을 사용하여 단일 이미지에서 모션을 예측하는 데 적합함을 보여줍니다. 다른 방법에서는 예측 작업에서 다양한 동작 표현을 사용했는데, 여기서 이미지나 비디오를 사용하여 결정론적 미래 동작 추정[34, 71]이나 가능한 동작의 보다 풍부한 분포[94, 96, 104]를 알려줍니다. 그러나 이러한 방법 중 다수는 전체 동작 궤적이 아닌 광학 흐름 동작 추정(즉, 각 픽셀의 순간 동작)을 예측합니다. 게다가 이러한 이전 작업의 대부분은 합성 작업이 아닌 활동 인식과 같은 작업에 초점을 맞춥니다. 최근 작업에서는 인간과 동물과 같은 여러 폐쇄 도메인 설정에서 생성 모델을 사용하여 동작을 모델링하고 예측하는 이점이 있음을 보여주었습니다[2, 19, 28, 72, 91, 107]. 텍스처로서의 비디오. 특정 움직이는 장면은 비디오를 확률 과정의 시공간 샘플로 모델링하는 일종의 텍스처 용어 동적 텍스처[26]로 생각할 수 있습니다. 동적 텍스처는 파도, 불꽃 또는 움직이는 나무와 같은 부드럽고 자연스러운 움직임을 표현할 수 있으며, 비디오 분류, 분할 또는 인코딩에 널리 사용되었습니다[12–15, 76]. 비디오 텍스처라고 하는 관련된 종류의 텍스처는 움직이는 장면을 프레임 쌍 간의 전환 확률과 함께 입력 비디오 프레임 세트로 표현합니다[66, 78]. 여러 방법이 장면 동작과 픽셀 통계를 분석하여 동적 또는 비디오 텍스처를 추정하여 원활하게 반복되거나 무한히 변하는 출력 비디오를 생성하는 것을 목표로 합니다[1, 21, 32, 58, 59, 78]. 이 작업의 대부분과 달리, 우리의 방법은 사전에 사전 지식을 학습하여 단일 이미지에 적용할 수 있습니다. 3. 개요 단일 사진 Io가 주어지면, 우리의 목표는 나무, 꽃 또는 바람에 흔들리는 촛불의 불꽃과 같은 진동하는 동작을 특징으로 하는 비디오 {Î₁, Î2., ..., ÎÃ}를 생성하는 것입니다. 저희 시스템은 두 개의 모듈로 구성되어 있습니다. 동작 예측 모듈과 이미지 기반 렌더링 모듈입니다. 저희 파이프라인은 잠재 확산 모델(LDM)을 사용하여 입력 I에 대한 스펙트럼 볼륨 S = (Sfo, Sf₁,, SfK-1)을 예측하는 것으로 시작합니다. 예측된 스펙트럼 볼륨은 역 이산 푸리에 변환을 통해 동작 텍스처 F = (F1, F2, FT)로 변환됩니다. 이 동작은 모든 미래 시간 단계에서 각 입력 픽셀의 위치를 결정합니다. 예측된 동작 텍스처가 주어지면 신경 이미지 기반 렌더링 기법(5절)을 사용하여 입력 RGB 이미지에 애니메이션을 적용합니다. 6절에서 이 방법의 응용 프로그램, 즉 원활한 루핑 애니메이션 생성 및 대화형 역학 시뮬레이션을 살펴봅니다. 4. 동작 예측 4.1. 동작 표현 형식적으로 동작 텍스처는 시간에 따라 변하는 2차원 변위 맵 F = {Ft|t = 1, ..., T}의 시퀀스로, 입력 이미지 I의 각 픽셀 좌표 p에서 2차원 변위 벡터 Ft(p)는 미래 시간 t에서 해당 픽셀의 위치를 정의합니다[20]. 시간 t에서 미래 프레임을 생성하려면 해당 변위를 사용하여 I에서 픽셀을 분산시킬 수 있습니다 진폭1.X축 해상도 포함 스케일링 Y축 0.적응형 정규화 주파수 0.0.0.0.0.0.1.1.3.0Hz에서의 푸리에 계수 진폭 0.0 2.5 5.0 7.5 10.0 12.5 15.주파수(Hz) 그림 2. 왼쪽: 실제 비디오에서 추출한 X 및 Y 동작 구성 요소의 평균 전력 스펙트럼을 파란색과 녹색 곡선으로 시각화합니다. 자연스러운 진동 동작은 주로 저주파 성분으로 구성되므로 빨간색 점으로 표시된 첫 번째 K = 16 항을 사용합니다.오른쪽: (1) 이미지 너비와 높이(파란색)로 진폭을 확장하거나 (2) 주파수 적응 정규화(빨간색)한 후 3.0Hz에서 푸리에 항의 진폭 히스토그램을 보여줍니다.적응 정규화는 계수가 극단적인 값에 집중되는 것을 방지합니다.Dt를 매핑하여 전방 워핑된 이미지 I{를 생성합니다.I(p+ Ft(p)) = Io(p).(1) 목표가 모션 텍스처를 통해 비디오를 생성하는 것이라면 한 가지 선택은 입력 이미지에서 직접 시간 영역 모션 텍스처를 예측하는 것입니다.그러나 모션 텍스처의 크기는 비디오 길이에 따라 조정되어야 합니다.T개의 출력 프레임을 생성하면 T개의 변위 필드를 예측해야 합니다. 긴 비디오에 대해 이렇게 큰 출력 표현을 예측하는 것을 피하기 위해 많은 기존 애니메이션 방법은 비디오 프레임을 자기 회귀적으로 생성하거나[7, 29, 57, 60, 93], 추가 시간 임베딩을 통해 각 미래 출력 프레임을 독립적으로 예측합니다[4]. 그러나 어느 전략도 생성된 비디오의 장기적인 시간적 일관성을 보장하지 않습니다. 다행히도 많은 자연스러운 동작은 서로 다른 주파수, 진폭 및 위상으로 표현된 소수의 고조파 발진기의 중첩으로 설명할 수 있습니다[20, 23, 25, 50, 69]. 이러한 기본 동작은 준주기적이므로 주파수 영역에서 모델링하는 것이 자연스럽습니다. 따라서 Davis et al.[23]에서 그림 3에 시각화된 스펙트럼 볼륨이라고 하는 비디오의 동작에 대한 효율적인 주파수 공간 표현을 채택했습니다. 스펙트럼 볼륨은 비디오에서 추출된 픽셀당 궤적의 시간 푸리에 변환입니다. 이러한 동작 표현이 주어지면, 동작 예측 문제를 다중 모달 이미지-이미지 변환 작업으로 공식화합니다.입력 이미지에서 출력 동작 스펙트럼 볼륨으로.잠재 확산 모델(LDM)을 채택하여 4K채널 2D 동작 스펙트럼 맵으로 구성된 스펙트럼 볼륨을 생성합니다.여기서 K &lt;&lt; T는 모델링된 주파수 수이고, 각 주파수에서 x 및 y 차원의 복소 푸리에 계수를 나타내기 위해 4개의 스칼라가 필요합니다.미래 시간 단계 F(p) = {Ft(p)|t = 1, 2, ...T}에서 픽셀의 동작 궤적과 스펙트럼 볼륨 S(p) = {Sƒk (p)|k = 0, 1, .. — 1}로 표현된 것은 고속 푸리에 변환(FFT)으로 다음과 같이 관련됩니다.S(p) = FFT(F(p)).Train €Sfo ED Sfi Inference BN~N(0, 1); ⠀ 반복적 노이즈 제거 SfKS n 노이즈가 있는 잠재 fЄRBKXCxHxW° 공간 계층 재구성 f&#39;Є RBKC&#39;H&#39;×W&#39; 주파수 주의 Д 재구성 f&quot; ERBKXC&#39;xH&#39;×W&#39; 그림 3. 동작 예측 모듈. 주파수 조정된 노이즈 제거 모델을 통해 스펙트럼 볼륨 S를 예측합니다. 확산 네트워크의 각 블록은 2D 공간 계층을 주의 계층(빨간색 상자, 오른쪽)과 끼워 넣고 반복적으로 잠재 특성 zn을 노이즈 제거합니다. 노이즈가 제거된 특성은 디코더 D에 공급되어 S를 생성합니다. 학습하는 동안 다운샘플링된 입력 Io를 인코더 E를 통해 실제 동작 텍스처에서 인코딩된 노이즈가 있는 잠재 특성과 연결하고 추론 중에 노이즈 특성을 가우시안 노이즈 z로 바꿉니다(왼쪽). K 출력 주파수를 어떻게 선택해야 할까요? 실시간 애니메이션에서의 이전 작업에서는 대부분의 자연스러운 진동 동작이 주로 저주파 성분으로 구성되어 있다는 것을 관찰했습니다. [25, 69]. 이 관찰 결과를 검증하기 위해, 무작위로 샘플링한 1,000개의 5초 실제 비디오 클립에서 추출한 동작의 평균 전력 스펙트럼을 계산했습니다. 그림 2의 왼쪽 플롯에서 볼 수 있듯이, 동작의 전력 스펙트럼은 주파수가 증가함에 따라 기하급수적으로 감소합니다. 이는 대부분의 자연스러운 진동 동작이 실제로 저주파 항으로 잘 표현될 수 있음을 시사합니다. 실제로, 우리는 첫 번째 K 16 푸리에 계수가 다양한 실제 비디오와 장면에서 원래의 자연스러운 동작을 사실적으로 재현하기에 충분하다는 것을 발견했습니다. = 4.2. 확산 모델을 사용한 동작 예측 우리는 합성 품질을 유지하면서도 픽셀 공간 확산 모델보다 계산 효율성이 높은 잠재 확산 모델(LDM)[74]을 동작 예측 모듈의 백본으로 선택했습니다. 표준 LDM은 두 가지 주요 모듈로 구성됩니다. (1) 인코더 z = E(I)를 통해 입력 이미지를 잠재 공간으로 압축한 다음 디코더 I = D(z)를 통해 잠재 피처에서 입력을 재구성하는 변형 자동 인코더(VAE), (2) 가우시안 노이즈에서 시작하여 피처의 노이즈를 반복적으로 제거하는 방법을 학습하는 U-Net 기반 확산 모델입니다. 저희의 훈련은 이 프로세스를 RGB 이미지가 아닌 실제 비디오 시퀀스의 스펙트럼 볼륨에 적용합니다. 이 볼륨은 인코딩된 다음 사전 정의된 분산 일정에 따라 n단계 동안 확산되어 노이즈가 있는 잠재 객체 zn을 생성합니다. 2D U-Net은 각 단계 n = (1, 2, ..., N)에서 잠재 객체를 업데이트하는 데 사용되는 노이즈 ε0(zn; n, c)를 반복적으로 추정하여 노이즈가 있는 잠재 객체의 노이즈를 제거하도록 훈련됩니다. LDM의 훈련 손실은 CLDM = EnЄu[1,N],&lt;&quot;EN(0,1) [||e&quot; - €0 (z&quot;; n, c)||²] (3)로 작성됩니다. 여기서 c는 텍스트와 같은 조건부 신호의 임베딩이거나, 우리의 경우 훈련 비디오 시퀀스의 첫 번째 프레임인 Io입니다. 깨끗한 잠재 특징 20은 디코더를 통과하여 스펙트럼 볼륨을 복구합니다. 우리가 해결한 한 가지 문제는 동작 텍스처가 주파수에 따라 특정 분포 특성을 갖는다는 것입니다. 그림 2의 왼쪽 플롯에서 시각화한 것처럼 스펙트럼 볼륨의 진폭은 0~100의 범위를 아우르며 주파수가 증가함에 따라 대략 기하급수적으로 감소합니다. 확산 모델은 안정적인 훈련과 잡음 제거를 위해 출력의 절대값이 -1과 1 사이여야 하므로 [44], 훈련에 사용하기 전에 실제 비디오에서 추출한 S의 계수를 정규화해야 합니다. 크기를 조정하는 경우 영어: 이러한 계수를 이전 작업 [29, 77]에서와 같이 이미지 차원을 기준으로 [0,1]로 조정하면, 그림 2의 오른쪽 플롯에 표시된 것처럼 고주파수의 거의 모든 계수가 0에 가까워집니다. 이러한 데이터로 학습된 모델은 추론 중에 작은 예측 오류조차도 비정규화 후에 큰 상대 오류를 일으킬 수 있으므로 부정확한 동작을 생성할 수 있습니다. 이 문제를 해결하기 위해 간단하지만 효과적인 주파수 적응 정규화 방법을 사용합니다. 먼저, 학습 세트에서 계산된 통계를 기반으로 각 주파수에서 푸리에 계수를 독립적으로 정규화합니다. 즉, 각 개별 주파수 fj에 대해 모든 입력 샘플에서 푸리에 계수 크기의 95번째 백분위수를 계산하고 해당 값을 주파수별 스케일링 인수 sf;로 사용합니다. 그런 다음 각 스케일링된 푸리에 계수에 거듭제곱 변환을 적용하여 극단값에서 벗어나게 합니다. 실제로 제곱근은 로그 또는 역수와 같은 다른 비선형 변환보다 성능이 더 우수하다는 것을 관찰했습니다. 요약하면, 주파수 f에서 스펙트럼 볼륨 S(p)의 최종 계수 값은 (LDM을 훈련하는 데 사용됨)은 다음과 같이 계산됩니다. = sign(Sfi Sf; (p) Sfj Io (4) 그림 2의 오른쪽 플롯에서 볼 수 있듯이 주파수 적응 정규화를 적용한 후 스펙트럼 볼륨 계수가 더 균등하게 분포됩니다. 주파수 조정된 노이즈 제거. K 주파수 대역의 스펙트럼 볼륨 S를 예측하는 간단한 방법은 단일 확산 U-Net에서 4K 채널의 텐서를 출력하는 것입니다. 그러나 이전 연구 [7]에서와 같이 많은 수의 채널을 생성하도록 모델을 훈련하면 지나치게 평활화되고 정확하지 않은 출력이 생성될 수 있음을 관찰했습니다. 대안은 LDM에 추가 주파수 임베딩을 주입하여 각 개별 주파수 슬라이스를 독립적으로 예측하는 것입니다 [4]. 그러나 이러한 설계 선택은 주파수 영역에서 상관되지 않은 예측을 초래하여 비현실적인 동작으로 이어집니다. 따라서 최근의 비디오 확산 연구 [7]에서 영감을 얻어 그림 3에 표시된 주파수 조정된 노이즈 제거 전략을 제안합니다. 특히 입력 이미지가 주어지면 Io에서, 우리는 먼저 LDM ε를 훈련시켜 스펙트럼 볼륨 Sƒ,의 단일 4채널 주파수 슬라이스를 예측합니다. 여기서 우리는 시간 단계 임베딩과 함께 LDM에 추가 주파수 임베딩을 주입합니다. 그런 다음 이 LDM €0의 매개변수를 동결하고 K 주파수 대역에 걸쳐 Є의 2D 공간 레이어와 함께 삽입된 어텐션 레이어를 도입하고 미세 조정합니다. 구체적으로, 배치 크기 B의 경우, 2D 공간 레이어는 채널 크기 C&#39;의 해당 B. K 노이즈 잠재 특징을 R(E (B ·K)×C×HXW 모양의 독립 샘플로 처리합니다. 그런 다음 어텐션 레이어는 이를 주파수 축에 걸쳐 있는 연속적인 특징으로 해석하고, 어텐션 레이어에 공급하기 전에 이전 2D 공간 레이어의 잠재 특징을 RB×K×C×H×W로 재구성합니다. 다시 말해, 주파수 어텐션 레이어는 모든 주파수 슬라이스를 조정하여 일관된 스펙트럼 볼륨을 생성하도록 미세 조정됩니다. 우리의
--- EXPERIMENT ---
s에서 단일 2D U-Net에서 주파수 조정된 노이즈 제거 모듈로 전환하면 평균 VAE 재구성 오류가 0.024에서 0.018로 개선되는 것을 볼 수 있으며, 이는 LDM 예측 정확도의 상한이 개선되었음을 시사합니다. 7.3절에서는 이 설계 선택이 비디오 생성 품질을 개선한다는 사실도 보여줍니다. 5. 이미지 기반 렌더링 = 이제 주어진 입력 이미지 I에 대해 예측된 스펙트럼 볼륨 S를 취하고 시간 t에서 미래 프레임 Ît를 렌더링하는 방법을 설명합니다. 먼저 각 픽셀 F(p) FFT¹(S(p))에 적용된 역 시간 FFT를 사용하여 시간 영역에서 모션 텍스처를 도출합니다. 미래의 프레임 Ît를 생성하기 위해, 우리는 딥 이미지 기반 렌더링 기법을 채택하고 예측된 모션 필드 Ft로 스플래팅을 수행하여 그림 4와 같이 인코딩된 Io를 전방 워핑합니다. 전방 워핑은 홀을 초래할 수 있고, 여러 소스 픽셀이 동일한 출력에 매핑될 수 있기 때문에 피처 추출기 소프트맥스 스플래팅(W에 따라) ↑ 합성 네트워크 그림 4. 렌더링 모듈. 우리는 누락된 내용을 채우고 딥 이미지 기반 렌더링 모듈을 사용하여 워핑된 입력 이미지를 정제합니다. 여기서 다중 스케일 피처는 입력 이미지 Io에서 추출됩니다. 그런 다음 소프트맥스 스플래팅은 시간 0에서 t까지의 모션 필드 Ft(가중치 W에 따라)를 사용하여 피처에 적용됩니다. 워핑된 피처는 이미지 합성 네트워크에 공급되어 렌더링된 이미지 Ît를 생성합니다. 2D 위치에서, 우리는 프레임 보간에 대한 이전 작업[68]에서 제안된 피처 피라미드 소프트맥스 스플래팅 전략을 채택합니다. 구체적으로, 우리는 피처 추출기 네트워크를 통해 I를 인코딩하여 다중 스케일 피처 맵을 생성합니다. 스케일 j에서 각 개별 피처 맵의 경우, 우리는 해상도에 따라 예측된 2D 모션 필드 Ft의 크기를 조절하고 스케일링합니다. Davis et al. [22]에서와 같이, 우리는 예측된 흐름 크기를 깊이의 프록시로 사용하여 목적지 위치에 매핑된 각 소스 픽셀의 기여 가중치를 결정합니다. 특히, 우리는 예측된 모션 텍스처의 평균 크기로 픽셀당 가중치 W(p) = ½ Σt ||Ft (p)||를 계산합니다. 다시 말해, 우리는 큰 모션이 움직이는 전경 객체에 해당하고 작거나 0인 모션이 배경에 해당한다고 가정합니다. 우리는 학습 가능한 가중치 대신 모션에서 파생된 가중치를 사용합니다. 왜냐하면 단일 뷰의 경우 학습 가능한 가중치가 분리 모호성을 해결하는 데 효과적이지 않기 때문입니다. 모션 필드 Ft와 가중치 W를 사용하여 소프트맥스 스플래팅을 적용하여 각 스케일에서 피처 맵을 워핑하여 워핑된 피처를 생성합니다. 그런 다음 워핑된 피처가 이미지 합성 디코더의 해당 블록에 주입되어 최종 렌더링된 이미지 Ît를 생성합니다. 우리는 실제 비디오에서 무작위로 샘플링된 시작 및 대상 프레임(Io, It)을 사용하여 특징 추출기 및 합성 네트워크를 공동으로 훈련하고, I에서 It로 추정된 흐름 필드를 사용하여 Io에서 인코딩된 특징을 워핑하고, VGG 지각 손실로 It에 대한 예측 It을 감독합니다[49].6. 응용 프로그램 이미지-비디오.우리의 시스템은 먼저 입력 이미지에서 동작 스펙트럼 볼륨을 예측하고 스펙트럼 볼륨에서 변환된 동작 텍스처에 이미지 기반 렌더링 모듈을 적용하여 애니메이션을 생성하여 단일 정지 사진의 애니메이션을 가능하게 합니다.우리는 장면 동작을 명시적으로 모델링하므로 동작 텍스처를 선형 보간하여 슬로우모션 비디오를 생성하거나 예측된 스펙트럼 볼륨 계수의 진폭을 조정하여 애니메이션 동작을 확대(또는 축소)할 수 있습니다.매끄러운 루핑.많은 응용 프로그램에서 비디오의 시작과 끝 사이에 불연속성이 없는 원활하게 루프되는 비디오가 필요합니다.안타깝게도 훈련을 위해 원활하게 루프되는 비디오의 대규모 컬렉션을 찾기가 어렵습니다. 대신, 우리는 반복되지 않는 일반적인 비디오 클립에서 훈련된 모션 확산 모델을 사용하여 매끄러운 반복 비디오를 생성하는 방법을 고안합니다. 이미지 편집을 위한 지침에 대한 최근 작업[3, 30]에서 영감을 얻은 우리의 방법은 명시적 반복 제약을 사용하여 모션 노이즈 제거 샘플링 처리를 안내하는 모션 자체 안내 기술입니다. 특히 추론 중 각 반복적 잡음 제거 단계에서 표준 분류기 없는 안내[45]와 함께 추가적인 동작 안내 신호를 통합하여 시작 및 종료 프레임에서 각 픽셀의 위치와 속도가 가능한 한 유사하도록 적용합니다. ên = (1 + w)eo(z&quot;; n, c) – weo (zn; n, Ø) + uo^ ▼ ₂n L™ L” = ||F™ – Fr ||1 + ||VF™ – VFÏ ||1 (5) 여기서 Fr은 시간 t 및 잡음 제거 단계 n에서 예측된 2D 변위장입니다. w는 분류기 없는 안내 가중치이고 u는 동작 자체 안내 가중치입니다. 보충 비디오에서 기준 모양 기반 루핑 알고리즘[58]을 적용하여 루핑되지 않는 출력에서 루핑 비디오를 생성하고 동작 자체 안내 기술이 왜곡과 아티팩트가 적은 원활한 루핑 비디오를 생성함을 보여줍니다. 단일 이미지의 대화형 동역학. Davis et al. [22]는 특정 공진 주파수에서 평가된 스펙트럼 볼륨이 기본 장면의 진동 모드의 투영인 이미지 공간 모달 기반을 근사할 수 있고(또는 보다 일반적으로 진동 동역학에서 공간적 및 시간적 상관 관계를 포착함) 사용자 정의 힘에 대한 개체의 응답을 시뮬레이션하는 데 사용할 수 있음을 보여줍니다. 이 모달 분석 방법[22, 70]을 채택하여 개체의 물리적 응답에 대한 이미지 공간 2D 모션 변위 필드를 복소 모달 좌표 qƒ; (t)의 상태에 의해 변조된 모션 스펙트럼 계수 Sf의 가중 합으로 작성할 수 있습니다. 각 시뮬레이션된 시간 단계 t에서: Ft(p) = Σ Sƒ; (p)qƒ,(t) fj (6) 모달 공간에 표현된 분리된 질량-스프링-댐퍼 시스템에 대한 운동 방정식에 적용된 명시적 오일러 방법을 통해 모달 좌표 qf; (t)의 상태를 시뮬레이션합니다[22, 23, 70]. 전체 유도를 위해 독자들에게 보충 자료와 원본 작업을 참조하십시오.우리의 방법은 단일 사진에서 대화형 장면을 생성하는 반면, 이러한 이전 방법은 입력으로 비디오를 요구한다는 점에 유의하십시오.방법 TATS [35] 이미지 합성 FID KID 비디오 합성 FVD FVD32 DTFVD DTFVDStochastic 12V [27] 68.MCVD [93] LFDM [67] 65.8 1.3.265.6 419.6 22.40.253.5 320.9 16.41.63.2.208.6 270.4 19.53.47.1.187.5 254.3 13.45.DMVFN [48] 37.1.206.5 316.11.54.Endo et al. [29] 10.Holynski et al. [46] 11. 우리 4.0.166.0 231.5.65.0.0.179.0 253.47.1 62.7.46.2.6.표 1. 테스트 세트에 대한 양적 비교. 우리는 이미지 합성과 비디오 합성 품질을 모두 보고합니다. 여기서 KID는 100으로 조정됩니다. 모든 오류에 대해 낮을수록 좋습니다. 기준선과 오류 메트릭에 대한 설명은 Sec. 7.1을 참조하십시오. 7. 실험 구현 세부 정보. 우리는 스펙트럼 볼륨을 예측하기 위한 백본으로 LDM[74]을 사용하며, 이를 위해 차원 4의 연속 잠복 공간을 갖는 VAE를 사용합니다. 우리는 각각 1, 0.2, 10-6의 가중치를 갖는 L₁ 재구성 손실, 다중 스케일 그래디언트 일관성 손실[54-56], KL-발산 손실로 VAE를 훈련합니다. 우리는 원래 LDM 작업에서 사용된 것과 동일한 2D U-Net을 훈련하여 간단한 MSE 손실[44]로 반복적 노이즈 제거를 수행하고, 주파수 조정 노이즈 제거를 위해 [41]의 어텐션 계층을 채택합니다.정량적 평가를 위해, 우리는 공정한 비교를 위해 256×160 크기의 이미지에서 VAE와 LDM을 모두 처음부터 훈련하며, 16개의 Nvidia AGPU를 사용하여 수렴하는 데 약 6일이 걸립니다.주요 정량적 및 정성적 결과를 위해, 우리는 DDIM[86] forsteps로 동작 확산 모델을 실행합니다.또한 데이터 세트에서 사전 훈련된 이미지 인페인팅 LDM 모델[74]을 미세 조정하여 생성된 최대 512×288 해상도의 생성된 비디오를 보여줍니다.우리는 IBR 모듈의 특징 추출기에 ResNet-34[39]를 채택합니다.우리의 이미지 합성 네트워크는 조건부 이미지 인페인팅[57, 110]을 위한 아키텍처를 기반으로 합니다. 렌더링 모듈은 추론 중 Nvidia V100 GPU에서 25FPS로 실시간으로 실행됩니다. 우리는 원활한 루핑 비디오를 생성하기 위해 보편적인 지침[3]을 채택하여 가중치 w = 1.75, u = 200을 설정하고 2개의 자체 재귀 반복과 함께 500개의 DDIM 단계를 사용합니다. 데이터. 우리는 온라인 소스에서 진동 운동을 보이는 자연 장면의 비디오 3,015개를 수집하여 처리합니다. 우리는 비디오의 10%를 테스트를 위해 보류하고 나머지는 훈련에 사용합니다. 기준 진실 동작 궤적을 추출하기 위해 선택한 각 시작 이미지와 비디오의 모든 미래 프레임 사이에 대략-미세 흐름 방법[10, 61]을 적용합니다. 훈련 데이터로서 우리는 10번째 비디오 프레임을 입력 이미지로 사용하고 다음 149개 프레임에서 계산된 동작 궤적을 사용하여 해당 기준 진실 스펙트럼 볼륨을 도출합니다. 총 150,000개 이상의 이미지-동작 쌍으로 구성된 데이터입니다. 입력 이미지 참조 확률론적-12V [27] Endo 등 [29] Holynski 등 [46] 우리의 MCVD [93] 그림 5. 다양한 접근 방식으로 생성된 비디오의 Xt 슬라이스. 왼쪽에서 오른쪽으로: 기준 비디오의 입력 이미지와 해당 Xt 비디오 슬라이스, 세 개의 기준선 [27, 29, 46, 93]에서 생성된 비디오, 마지막으로 우리의 접근 방식으로 생성된 비디오. 슬링 윈도우 FID 슬링 윈도우 DT-FVDDMVFNMCVDStochastic 12V LFDMEndo 등.Holynski 등 문Ours DT-FVDMethod Repeat Io Image Synthesis FID KID Video Synthesis FVD FVD32 DTFVD DTFVD237.5 316.7 5.45.K =3.K =K =0.3.95 0.4.60.3 78.3.8.52.1 68.2.7.0.48.2 65.2.6.Frame index Frame index w/o adaptive norm. 4.0.62.7 80.3.8.그림 6. 슬라이딩 윈도우 FID와 DTFVD. 서로 다른 방법으로 생성된 비디오에 대해 윈도우 크기가 30프레임인 슬라이딩 윈도우 FID와 크기가 16프레임인 DTFVD를 보여줍니다. Independent pred. 4.Volume pred. 영어: Baseline splat [46] 4.0.Full (K = 16) 4.03 0.0.52.5 71.2.7.4.0.53.7 71.2.7.49.5 66.2.7.47.1 62.9 2.6.Baselines. 우리는 우리의 접근 방식을 최근의 단일 이미지 애니메이션 및 비디오 예측 방법과 비교합니다.Endo et al. [29] 및 DMVFN [48]은 즉각적인 2D 동작 필드를 예측하고 미래 프레임을 자동 회귀적으로 렌더링합니다.Holynski et al. [46]는 대신 단일 정적 Eulerian 동작 설명을 통해 동작을 시뮬레이션합니다.Stochastic Image-to-Video(Stochastic-I2V) [27], TATS [35] 및 MCVD [93]와 같은 다른 최근 작업은 VAE, 변환기 또는 확산 모델을 채택하여 원시 비디오 프레임을 직접 예측합니다. LFDM[67]은 확산 모델에서 흐름 볼륨을 예측하고 잠복을 워핑하여 미래 프레임을 생성합니다.우리는 각각의 오픈 소스 구현을 사용하여 위의 모든 방법을 데이터에 대해 학습합니다.¹ 우리는 우리의 접근 방식과 이전 기준선에 의해 생성된 비디오의 품질을 두 가지 방법으로 평가합니다.첫째, 우리는 이미지 합성 작업을 위해 설계된 메트릭을 사용하여 개별 합성 프레임의 품질을 평가합니다.우리는 생성된 프레임의 분포와 기준 진실 프레임 간의 평균 거리를 측정하기 위해 Fréchet Inception Distance(FID)[42]와 Kernel Inception Distance(KID)[5]를 채택합니다.둘째, 품질과 시간적 코히어런스를 평가합니다.¹우리는 Fan et al.의 [46]의 오픈 소스 구현을 사용합니다.[83].표 2. 절제 연구.7.3절에서는 각 구성을 설명합니다. 합성된 비디오의 경우, Human Kinetics 데이터 세트[52]에서 학습된 I3D 모델[11]을 기반으로 한 창 크기 16(FVD) 및 32(FVD32)의 Fréchet Video Distance[92]를 채택합니다. 생성하려는 자연스러운 진동 동작에 대한 합성 품질을 보다 충실하게 반영하기 위해, 자연스러운 동작 텍스처로 주로 구성된 데이터 세트인 Dynamic Textures Database[37]에서 학습된 I3D 모델을 사용하여 창 크기 16(DTFVD) 및 크기 32(DTFVD32)의 비디오로부터의 거리를 측정하는 Dynamic Texture Frechet Video Distance[27]도 채택합니다. 또한 [57, 60]에서와 같이 창 크기 프레임의 슬라이딩 창 FID와 창 크기가 16인 슬라이딩 창 DTFVD를 사용하여 생성된 비디오 품질이 시간이 지남에 따라 어떻게 저하되는지 측정합니다. 모든 방법에서 중앙 자르기를 통해 256×128 해상도에서 메트릭을 평가합니다. 입력 AnimateDiff ModelScope GEN-그림 7. 최근의 3개 대형 비디오 확산 모델에서 생성된 미래 프레임을 보여줍니다[31, 36, 98].7.1. 정량적 결과 표 1은 테스트 세트에서 우리의 접근 방식과 기준선 간의 정량적 비교를 보여줍니다.우리의 접근 방식은 이미지와 비디오 합성 품질 측면에서 이전의 단일 이미지 애니메이션 기준선보다 상당히 우수한 성능을 보입니다.특히, 훨씬 낮은 FVD와 DT-FVD 거리는 우리의 접근 방식으로 생성된 비디오가 더 사실적이고 시간적으로 더 일관성이 있음을 시사합니다.또한 그림 6은 다양한 방법으로 생성된 비디오의 슬라이딩 윈도우 FID와 슬라이딩 윈도우 DT-FVD 거리를 보여줍니다.전역 스펙트럼 볼륨 표현 덕분에 우리의 접근 방식으로 생성된 비디오는 시간이 지남에 따라 저하되지 않습니다.7.2. 정성적 결과 우리는 비디오 간의 정성적 비교를 생성된 비디오의 시공간적 Xt 슬라이스로 시각화합니다.이는 비디오에서 작은 동작을 시각화하는 표준적인 방법입니다[95]. 그림 5에서 보듯이, 다른 방법에 비해 생성된 비디오 역학은 해당 실제 참조 비디오(두 번째 열)에서 관찰된 동작 패턴과 더 강하게 유사합니다.Stochastic I2V[27] 및 MCVD[93]와 같은 기준선은 시간에 따른 모양과 동작을 모두 사실적으로 모델링하지 못합니다.Endo 등[29] 및 Holynski 등[46]은 아티팩트가 적은 비디오 프레임을 생성하지만 시간에 따라 지나치게 부드럽거나 진동하지 않는 동작을 보입니다.독자에게는 생성된 비디오 프레임의 품질과 다양한 방법에 따른 추정 동작을 평가하기 위한 보충 자료를 참조하시기 바랍니다.7.3. 절제 연구 우리는 동작 예측 및 렌더링 모듈에서 주요 설계 선택을 검증하기 위해 절제 연구를 수행하여 전체 구성을 다양한 변형과 비교합니다.특히, 우리는 다양한 수의 주파수 대역 K = 4, 8, 16, 24를 사용하여 결과를 평가합니다.우리는 주파수 대역의 수를 늘리면 비디오 예측 품질이 향상되지만 16개 이상의 주파수에서는 향상이 미미하다는 것을 관찰했습니다. 다음으로, 기준 진실 스펙트럼 볼륨에서 적응 주파수 정규화를 제거하고 대신 입력 이미지 너비와 높이(적응적 규범 없음)를 기준으로 크기를 조정합니다. 또한, 주파수 조정-노이즈 제거 모듈(Independent pred.)을 제거하거나 단일 2D U-net 확산 모델을 통해 4K 채널 스펙트럼 볼륨의 텐서 볼륨을 공동으로 예측하는 더 간단한 DM으로 대체합니다(Volume pred.). 마지막으로, 그림 8에서와 같이 기준 렌더링 방법을 사용하여 결과를 비교합니다. 제한 사항. 렌더링된 미래 프레임(짝수)과 입력 및 렌더링된 이미지의 오버레이(홀수)의 예를 보여줍니다. 이 방법은 얇은 물체나 큰 동작의 영역과 많은 양의 새 콘텐츠를 채워야 하는 영역에 아티팩트를 생성할 수 있습니다. [46]에서 사용하는 학습 가능한 가중치가 적용되는 단일 스케일 피처에 소프트맥스 스플래팅을 적용합니다(Baseline splat). 또한 생성된 비디오가 입력 이미지를 N번 반복하여 볼륨이 되는 기준선을 추가합니다(Repeat Io). 표 2에서 우리는 모든 더 간단하거나 대체적인 구성이 전체 모델과 비교했을 때 더 나쁜 성능을 가져온다는 것을 관찰했습니다.7.4. 대형 비디오 모델과 비교 우리는 또한 사용자 연구를 수행하고 생성된 애니메이션을 최근의 대형 비디오 확산 모델인 AnimateDiff[36], ModelScope[98] 및 Gen-2[31]의 애니메이션과 비교했습니다.이들은 비디오 볼륨을 직접 예측합니다.테스트 세트에서 무작위로 선택한 30개 비디오에서 사용자에게 &quot;어떤 비디오가 더 사실적입니까?&quot;라고 묻습니다.사용자는 다른 접근 방식보다 우리의 접근 방식을 80.9% 선호한다고 보고합니다.또한 그림 7에서 볼 수 있듯이 이러한 기준선에서 생성된 비디오는 입력 이미지 콘텐츠를 고수하지 못하거나 시간이 지남에 따라 점진적인 색상 드리프트 및 왜곡을 보입니다.전체 비교를 위해 독자는 보충 자료를 참조하세요.8. 토론 및
--- CONCLUSION ---
제한 사항. 저희의 접근 방식은 스펙트럼 볼륨의 낮은 주파수만 예측하기 때문에 비진동 모션이나 고주파 진동을 모델링하지 못할 수 있습니다. 이는 학습된 모션 기반을 사용하여 해결할 수 있습니다. 더욱이 생성된 비디오의 품질은 기본 모션 궤적의 품질에 따라 달라지는데, 이는 얇은 움직이는 물체나 큰 변위가 있는 물체가 있는 장면에서 저하될 수 있습니다. 정확하더라도, 보이지 않는 새로운 콘텐츠를 대량으로 생성해야 하는 모션도 저하를 일으킬 수 있습니다(그림 8). 결론. 저희는 단일 정지 사진에서 자연스러운 진동 역학을 모델링하기 위한 새로운 접근 방식을 제시합니다. 저희의 이미지 공간 모션 사전은 스펙트럼 볼륨으로 표현되는데, 이는 픽셀당 모션 궤적의 주파수 표현으로, 확산 모델을 사용한 예측에 효율적이고 효과적이라고 생각되며, 실제 세계 비디오 모음에서 학습합니다. 스펙트럼 볼륨은 주파수 조정된 잠재 확산 모델을 사용하여 예측되며 이미지 기반 렌더링 모듈을 통해 미래 비디오 프레임을 애니메이션화하는 데 사용됩니다. 우리는 우리의 접근 방식이 단일 사진에서 사실적인 애니메이션을 생성하고 이전 기준선보다 상당히 우수하며, 원활하게 반복되거나 대화형 이미지 역학을 만드는 것과 같은 여러 다운스트림 애플리케이션을 지원할 수 있음을 보여줍니다.감사의 말.유익한 토론과 도움이 되는 의견을 주신 Abe Davis, Rick Szeliski, Andrew Liu, Boyang Deng, Qianqian Wang, Xuan Luo, Lucy Chai에게 감사드립니다.참고문헌 [1] Aseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh Agrawala, Michael Cohen, Brian Curless, David Salesin, Richard Szeliski.파노라마 비디오 텍스처.ACM Trans. Graphics(SIGGRAPH), 821-827쪽.2005. [2] Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee.3D 동작 예측에 확산 확률적 모델을 사용할 수 있을까요? arXiv 사전 인쇄본 arXiv:2302.14503, 2023. [3] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein. 확산 모델을 위한 범용 지침. Proc. Computer Vision and Pattern Recognition(CVPR), 843-852페이지, 2023. [4] Hugo Bertiche, Niloy J Mitra, Kuldeep Kulkarni, ChunHao P Huang, Tuanfeng Y Wang, Meysam Madadi, Sergio Escalera, Duygu Ceylan. 바람에 날리다: 정지 이미지에서 인간 시네마그래프를 위한 Cyclenet. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 459-468쪽, 2023. [5] Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, Arthur Gretton. MMD GAN의 신비 해제. arXiv 사전 인쇄본 arXiv:1801.01401, 2018. [6] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer. ipoke: 제어된 확률적 비디오 합성을 위한 정지 이미지 찌르기. Proc. Int. Conf. on Computer Vision(ICCV), 14707-14717쪽, 2021. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis. 잠재체 정렬: 잠재체 확산 모델을 사용한 고해상도 비디오 합성. Proc. Computer Vision and Pattern Recognition(CVPR), 22563-22575페이지, 2023. [8] Richard Strong Bowen, Richard Tucker, Ramin Zabih, Noah Snavely. 움직임의 차원: 흐름 부분 공간을 통한 단안 예측. International Conference on 3D Vision(3DV), 454-464페이지, 2022. [9] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, Tero Karras. 동적 장면의 긴 비디오 생성. Neural Information Processing Systems, 35:3176931781, 2022. [10] Thomas Brox, Andrés Bruhn, Nils Papenberg, Joachim Weickert. 워핑 이론을 기반으로 한 고정밀 광학 흐름 추정. Proc. European Conf. on Computer Vision(ECCV), 25-36페이지, 2004년. [11] Joao Carreira 및 Andrew Zisserman. Quo vadis, 동작 인식? 새로운 모델 및 동역학 데이터 세트. Proc. Computer Vision and Pattern Recognition(CVPR), 6299-6308페이지, 2017년. [12] Dan Casas, Marco Volino, John Collomosse 및 Adrian Hilton. 대화형 캐릭터 모양을 위한 4D 비디오 텍스처. Computer Graphics Forum, 33권, 371-380페이지. Wiley Online Library, 2014년. [13] Antoni B Chan 및 Nuno Vasconcelos. 동적 텍스처의 혼합. Proc. Int. Conf. on Computer Vision(ICCV), 641-647페이지, 2005년. [14] Antoni B Chan 및 Nuno Vasconcelos. 영어: 커널 동적 텍스처를 사용한 비디오 분류.Proc. Computer Vision and Pattern Recognition (CVPR), 2007에서. [15] Antoni B Chan 및 Nuno Vasconcelos. 동적 텍스처의 혼합을 사용한 비디오 모델링, 클러스터링 및 분할.Trans. Pattern Analysis and Machine Intelligence, 30(5):909– 926, 2008에서. [16] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: 마스크된 생성 변환기를 통한 텍스트-이미지 생성.arXiv 사전 인쇄본 arXiv:2301.00704, 2023에서. [17] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu 및 William T Freeman. Maskgit: 마스크된 생성 이미지 변환기.Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 11315-11325페이지, 2022. [18] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, Ming-Hsuan Yang. 제어 가능한 비디오 합성을 위한 동작 조건 확산 모델. arXiv 사전 인쇄본 arXiv:2304.14404, 2023. [19] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Gang Yu. 잠재 공간에서 동작 확산을 통해 명령 실행. Proc. Computer Vision and Pattern Recognition(CVPR), 18000-18010페이지, 2023. [20] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, Richard Szeliski. 확률적 모션 텍스처로 그림 애니메이션화. ACM Trans. Graphics(SIGGRAPH), 853-860페이지, 2005년. [21] Vincent C Couture, Michael S Langer 및 Sebastien Roy. 고스트 없는 옴니스테레오 비디오 텍스처. International Conference on 3D Vision, 64-70페이지. IEEE, 2013년. [22] Abe Davis, Justin G Chen 및 Frédo Durand. 비디오에서 객체를 그럴듯하게 조작하기 위한 이미지 공간 모달 기반. ACM Trans. Graphics(SIGGRAPH), 34(6):1–7, 2015년. [23] Myers Abraham Davis. 시각적 진동 분석. 매사추세츠 공과대학 박사 학위 논문, 2016년. [24] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 간을 이김. 영어: Neural Information Processing Systems, 34:8780-8794, 2021. [25] Julien Diener, Mathieu Rodriguez, Lionel Baboud 및 Lionel Reveret. 나무의 실시간 애니메이션을 위한 풍향 투영 기준. Computer graphics forum, 28권, 533-540페이지. Wiley Online Library, 2009. [26] Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu 및 Stefano Soatto. 동적 텍스처. 51:91-109, 2003. [27] Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis 및 Bjorn Ommer. cinns를 사용한 확률적 이미지-비디오 합성. Proc. Computer Vision and Pattern Recognition (CVPR), 3742-3753페이지, 2021년 6월. [28] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, 및 Artsiom Sanakoyeu. 아바타가 다리를 키운다: 확산 모델을 사용하여 희소 추적 입력에서 부드러운 인간 동작 생성. Proc. Computer Vision and Pattern Recognition (CVPR), 481-490페이지, 2023년. [29] Yuki Endo, Yoshihiro Kanamori, 및 Shigeru Kuriyama. 풍경 애니메이션: 단일 이미지 비디오 합성을 위한 분리된 동작 및 모양의 자체 감독 학습. ACM Trans. Graphics (SIGGRAPH Asia), 38(6):175:1175:19, 2019년. [30] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, 및 Aleksander Holynski. 제어 가능한 이미지 생성을 위한 확산 자체 안내. arXiv 사전 인쇄본 arXiv:2306.00986, 2023. [31] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog 및 Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 안내 비디오 합성. Proc. Int. Conf. on Computer Vision(ICCV), 73467356페이지, 2023. [32] Matthew Flagg, Atsushi Nakazawa, Qiushuang Zhang, Sing Bing Kang, Young Kee Ryu, Irfan Essa 및 James M Rehg. 인간 비디오 텍스처. In Proceedings of thesymposium on Interactive 3D graphics and games, pages 199-206, 2009. [33] Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. In International Conference on Machine Learning, pages 3233-3246. PMLR, 2020. [34] Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2Flow: 동작 인식을 위한 정적 이미지에서의 동작 환각. In Proc. Computer Vision and Pattern Recognition (CVPR), 2018. [35] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. 시간 독립적 vqgan과 시간 민감형 변압기를 사용한 긴 비디오 생성. arXiv 사전 인쇄본 arXiv:2204.03638, 2022. [36] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. Animatediff: 특정 튜닝 없이 개인화된 텍스트-이미지 확산 모델을 애니메이션화합니다. arXiv 사전 인쇄본 arXiv:2307.04725, 2023. [37] Isma Hadji 및 Richard P Wildes. 컨브넷 이해에 응용된 새로운 대규모 동적 텍스처 데이터 세트. Proc. European Conf. on Computer Vision(ECCV), 320-335페이지, 2018. [38] Zekun Hao, Xun Huang, Serge Belongie. 희소 궤적을 사용한 제어 가능한 비디오 생성. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 7854–7863, 2018. [39] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Jian Sun. 이미지 인식을 위한 심층 잔여 학습. Proc에서 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 770778, 2016. [40] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. 스토리 애니메이션: 검색 증강 비디오 생성을 통한 스토리텔링. arXiv 사전 인쇄 arXiv:2307.06940, 2023. [41] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan 및 Qifeng Chen. 임의 길이의 고화질 비디오 생성을 위한 잠재 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13221, 2022. [42] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler 및 Sepp Hochreiter. 2개 시간 척도 업데이트 규칙으로 학습된 Gans는 로컬 내쉬 균형으로 수렴합니다. 신경 정보 처리 시스템, 30, 2017. [43] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. [44] Jonathan Ho, Ajay Jain 및 Pieter Abbeel. 노이즈 제거 확산 확률 모델. 신경 정보 처리 시스템, 33:6840-6851, 2020. [45] Jonathan Ho 및 Tim Salimans. 분류기 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022. [46] Aleksander Holynski, Brian L Curless, Steven M Seitz 및 Richard Szeliski. 오일러 운동장을 사용하여 그림 애니메이션. Proc. Computer Vision and Pattern Recognition(CVPR), 5810-5819페이지, 2021. [47] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen 및 Andrea Dittadi. 비디오 예측 및 채우기를 위한 확산 모델. Trans. Mach. Learn. Res., 2022, 2022. [48] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, Shuchang Zhou. 비디오 예측을 위한 동적 다중 스케일 폭셀 흐름 네트워크. ArXiv, abs/2303.09875, 2023. [49] Justin Johnson, Alexandre Alahi, Li Fei-Fei. 실시간 스타일 전송 및 초고해상도를 위한 지각 손실. Proc. European Conf. on Computer Vision(ECCV), 694-711페이지, 2016. [50] Hitoshi Kanda와 Jun Ohya. 3D 식물 나무의 동적 동작을 애니메이션화하기 위한 효율적이고 사실적인 방법. International Conference on Multimedia and Expo, 2권, II-89페이지. IEEE, 2003. [51] Johanna Karras, Aleksander Holynski, Ting-Chun Wang 및 Ira Kemelmacher-Shlizerman. Dreampose: 안정된 확산을 통한 패션 이미지-비디오 합성. arXiv 사전 인쇄본 arXiv:2304.06025, 2023. [52] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 운동학 인간 행동 비디오 데이터 세트. arXiv 사전 인쇄본 arXiv:1705.06950, 2017. [53] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn 및 Sergey Levine. 확률적 적대적 비디오 예측. arXiv 사전 인쇄본 arXiv:1804.01523, 2018. [54] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T Freeman. 얼어붙은 사람을 관찰하여 움직이는 사람의 깊이를 배우다.Proc. Computer Vision and Pattern Recognition(CVPR), 4521-4530페이지, 2019. [55] Zhengqi Li와 Noah Snavely. Megadepth: 인터넷 사진에서 단일 시점 깊이 예측 학습.Proc. Computer Vision and Pattern Recognition(CVPR), 2041-2050페이지, 2018. [56] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, Noah Snavely. Dynibar: 신경 동적 이미지 기반 렌더링.Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 4273-4284페이지, 2023. [57] Zhengqi Li, Qianqian Wang, Noah Snavely 및 Angjoo Kanazawa. Infinitenature-zero: 단일 이미지에서 자연스러운 장면의 영구적 뷰 생성 학습. Proc. European Conf. on Computer Vision(ECCV), 515-534페이지. Springer, 2022. [58] Jing Liao, Mark Finch 및 Hugues Hoppe. 원활한 비디오 루프의 빠른 계산. ACM Trans. Graphics(SIGGRAPH), 34(6):1-10, 2015. [59] Zicheng Liao, Neel Joshi 및 Hugues Hoppe. 점진적 역동성을 갖춘 자동화된 비디오 루핑. ACM Transactions on Graphics(TOG), 32(4):1-10, 2013. [60] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, Angjoo Kanazawa. 무한한 자연: 단일 이미지에서 자연스러운 장면의 영구적 뷰 생성. Proc. Int. Conf. on Computer Vision(ICCV)에서, 14458-14467페이지, 2021. [61] Ce Liu. 픽셀 너머: 동작 분석을 위한 새로운 표현과 응용 프로그램 탐색. 매사추세츠 공과대학 박사 학위 논문, 2009. [62] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan. 비디오 퓨전: 고품질 비디오 생성을 위한 분해된 확산 모델. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 10209-10218페이지, 2023. [63] Aniruddha Mahapatra 및 Kuldeep Kulkarni. 정지 이미지에서 유체 요소의 제어 가능한 애니메이션. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 2022. [64] Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov 및 Jun-Yan Zhu. 오일러리안 시네마그래프의 텍스트 가이드 합성. 2023. [65] Arun Mallya, Ting-Chun Wang 및 Ming-Yu Liu. 이미지 세트를 사용한 애니메이션을 위한 암묵적 워핑. 영어: Neural Information Processing Systems, 35:22438-22450, 2022. [66] Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A Efros, Trevor Darrell. 비트에 맞춰 스트러밍: 오디오 조절된 대조 비디오 텍스처.Proc. Winter Conference on Applications of Computer Vision, 3761-3770페이지, 2022. [67] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, Martin Renqiang Min. 잠재 흐름 확산 모델을 사용한 조건부 이미지-비디오 생성.Proc. Computer Vision and Pattern Recognition (CVPR), 18444-18455페이지, 2023. [68] Simon Niklaus와 Feng Liu. 비디오 프레임 보간을 위한 Softmax 스플래팅.Proc. Computer Vision and Pattern Recognition (CVPR), 5437-5446페이지, 2020. [69] Shin Ota, Machiko Tamura, Kunihiko Fujita, T Fujimoto, K Muraoka, Norishige Chiba. 바람장에서 흔들리는 나무의 1/f/sup/spl beta//노이즈 기반 실시간 애니메이션. Proceedings Computer Graphics International, 52-59페이지. IEEE, 2003. [70] Automne Petitjean, Yohan Poirier-Ginter, Ayush Tewari, Guillaume Cordonnier, George Drettakis. Modalnerf: 동적으로 진동하는 장면에서 자유 시점 탐색을 위한 신경 모달 분석 및 합성. Computer Graphics Forum, 42권, 2023. [71] Silvia L. Pintea, Jan C. van Gemert, Arnold WM Smeulders. 영어: Déjà vu: 정적 이미지에서의 동작 예측. Proc. European Conf. on Computer Vision(ECCV), 2014에서. [72] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, Daniel Cohen-Or. 단일 동작 확산. arXiv 사전 인쇄본 arXiv:2302.05905, 2023. [73] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 1(2):3, 2022. [74] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 10684-10695쪽, 2022. [75] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템, 35:36479-36494, 2022. [76] Payam Saisan, Gianfranco Doretto, Ying Nian Wu, Stefano Soatto. 동적 텍스처 인식. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 2001. [77] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun 및 David J. Fleet. 광학 흐름 및 단안 깊이 추정을 위한 확산 모델의 놀라운 효과, 2023. [78] Arno Schödl, Richard Szeliski, David H Salesin 및 Irfan Essa. 비디오 텍스처. ACM 트랜스에서. 그래픽(SIGGRAPH), 페이지 489-498, 2000. [79] Mikio Shinya 및 Alain Fournier. 바람의 영향을 받는 확률론적 모션모션. 컴퓨터 그래픽 포럼, 11(3), 1992. [80] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci 및 Nicu Sebe. 심층 모션 전송을 통한 임의의 객체 애니메이션.Proc. Computer Vision and Pattern Recognition(CVPR), 2377-2386페이지, 2019. [81] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe. 이미지 애니메이션을 위한 1차 모션 모델.Neural Information Processing Systems, 32, 2019. [82] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei Chai, Sergey Tulyakov. 관절 애니메이션을 위한 모션 표현.Proc. Computer Vision and Pattern Recognition(CVPR), 13653-13662페이지, 2021. [83] Chen Qian Kwan-Yee Lin Hongsheng Li Siming Fan, Jingtan Piao. 실제 정지 이미지에서 유체 시뮬레이션. arXiv 사전 인쇄본, arXiv:2204.11335, 2022. [84] Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny. Stylegan-v: stylegan2의 가격, 이미지 품질 및 특전을 갖춘 연속 비디오 생성기. Proc. Computer Vision and Pattern Recognition(CVPR), 36263636페이지, 2022. [85] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 비평형 열역학을 사용한 심층 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015. [86] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델의 노이즈 제거. arXiv:2010.02502, 2020년 10월. [87] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. arXiv 사전 인쇄본 arXiv:2011.13456, 2020. [88] Jos Stam. 복잡한 자연 현상의 다중 규모 확률적 모델링. 박사 학위 논문, 1995. [89] Jos Stam. 확률적 동역학: 유연한 구조물에 대한 난류 효과 시뮬레이션. 컴퓨터 그래픽 포럼, 16(3), 1997. [90] Ryusuke Sugimoto, Mingming He, Jing Liao, Pedro V Sander. 정지 사진으로부터의 물 시뮬레이션 및 렌더링. ACM Trans. Graphics(SIGGRAPH Asia), 1-9페이지, 2022. [91] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H Bermano. 인간 동작 확산 모델. arXiv 사전 인쇄본 arXiv:2209.14916, 2022. [92] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, Sylvain Gelly. 비디오의 정확한 생성 모델을 향하여: 새로운 지표 및 과제. arXiv 사전 인쇄본 arXiv: 1812.01717, 2018. [93] Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal. Mcvd: 예측, 생성 및 보간을 위한 마스크 조건부 비디오 확산. 영어: Neural Information Processing Systems, 2022. [94] Carl Vondrick, Hamed Pirsiavash 및 Antonio Torralba. 장면 역학을 사용하여 비디오 생성. Neural Information Processing Systems, 2016. [95] Neal Wadhwa, Michael Rubinstein, Frédo Durand 및 William T Freeman. 위상 기반 비디오 동작 처리. ACM Trans. Graphics(SIGGRAPH), 32(4):1–10, 2013. [96] Jacob Walker, Carl Doersch, Abhinav Gupta 및 Martial Hebert. 불확실한 미래: 변분 자동 인코더를 사용하여 정적 이미지에서 예측. Proc. European Conf. on Computer Vision(ECCV)에서, 2016. [97] Jacob Walker, Abhinav Gupta 및 Martial Hebert. 정적 이미지에서 고밀도 광학 흐름 예측. Proc. Int. Conf.에서. 컴퓨터 비전(ICCV), 2443-2451페이지, 2015. [98] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao 및 Jingren Zhou. Videocomposer: 동작 제어가 가능한 구성적 비디오 합성. arXiv 사전 인쇄본 arXiv:2306.02018, 2023. [99] Yaohui Wang, Di Yang, Francois Bremond 및 Antitza Dantcheva. 잠상 이미지 애니메이터: 잠상 공간 탐색을 통해 이미지 애니메이션화 학습. arXiv 사전 인쇄본 arXiv:2203.09043, 2022. [100] Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski 및 Angjoo Kanazawa. Nerfbusters: 캐주얼하게 캡처한 nerf에서 유령 유물 제거. arXiv 사전 인쇄본 arXiv:2304.10532, 2023. [101] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi. 확산 모델을 사용한 새로운 뷰 합성. arXiv 사전 인쇄본 arXiv:2210.04628, 2022. [102] Chung-Yi Weng, Brian Curless, Ira KemelmacherShlizerman. 사진 웨이크업: 단일 사진에서 3D 캐릭터 애니메이션. Proc. Computer Vision and Pattern Recognition(CVPR), 5908-5917페이지, 2019. [103] Jamie Wynn과 Daniyar Turmukhambetov. DiffusioneRF: 노이즈 제거 확산 모델을 사용한 신경 광도장 정규화. Proc. Computer Vision and Pattern Recognition (CVPR), 2023. [104] Tianfan Xue, Jiajun Wu, Katherine L Bouman 및 William T Freeman. 시각적 역학: 계층화된 교차 합성곱 네트워크를 통한 확률적 미래 생성. 패턴 분석 및 머신 인텔리전스, 41(9):2236-2250, 2019. [105] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming 및 Nan Duan. Dragnuwa: 텍스트, 이미지 및 궤적을 통합하여 비디오 생성에서 세분화된 제어. arXiv 사전 인쇄본 arXiv:2308.08089, 2023. [106] Sihyun Yu, Kihyuk Sohn, Subin Kim 및 Jinwoo Shin. 투사된 잠재 공간의 비디오 확률적 확산 모델. Proc. 컴퓨터 비전 및 패턴 인식(CVPR), 페이지 18456-18466, 2023. [107] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang 및 Ziwei Liu. Remodiffuse: 검색 증강 모션 확산 모델. arXiv 사전 인쇄 arXiv:2304.01116, 2023. [108] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo 및 Qi Tian. Controlvideo: 교육 없이 제어 가능한 텍스트-비디오 생성. arXiv 사전 인쇄 arXiv:2305.13077, 2023. [109] Jian Zhao 및 Hui Zhang. 이미지 애니메이션을 위한 박판 스플라인 모션 모델. Proc에서 컴퓨터 비전 및 패턴 인식(CVPR), 3657-3666페이지, 2022. [110] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu. 공동 변조 생성적 적대 네트워크를 통한 대규모 이미지 완성. 국제 학습 표현 컨퍼런스(ICLR), 2021. [111] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng. Magicvideo: 잠재 확산 모델을 사용한 효율적인 비디오 생성. arXiv 사전 인쇄본 arXiv:2211.11018, 2022.
"
"--- ABSTRACT ---
In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task’s uniqueness and complexity. 1
--- INTRODUCTION ---
Many NLP studies have highlighted the importance of entities to understanding the semantics of a document (Wu et al., 2020b; Meij et al., 2012). Automatically identifying entities in unstructured text documents and linking them to an underlying knowledge base, such as Wikipedia, is one of the core NLP tasks, with multiple shared tasks (Tjong Kim Sang and De Meulder, 2003; Strauss et al., 2016), benchmarks (Hoffart et al., 2011; Hovy et al., 2006; Pradhan et al., 2013; Rijhwani and Preotiuc-Pietro, 2020; Derczynski et al., 2016), and studies (Kolitsas et al., 2018; Nguyen et al., 2014) dedicated to solving them. Although an entity may play a crucial semantic role in document understanding, not all entities in tWork was done while the author was affiliated with Bloomberg Musk Completes $44 Billion Twitter Deal Elon Musk, the world’s richest man, CEO of Tesla and founder of the american research lab Open Al, has just completed a $44 billion takeover of the social media company Twitter. On Thursday, Twit Parag Agrawal has been fired and escorted out from the San Franeisco headquarters. Trading Twi Exchange reported res has been suspended from Friday, New York Stock Musk claimed he will reverse bans the past, including the former SALIENT ENTITIES Elon Musk CEO Tesla, Inc. Open Al Parag Agrawal San Francisco New York Stock Exchange US President Donald Trump f users that have been suspended in jent Donald Trump. Twitter Figure 1: An example of a document with salient and non-salient entities. Entity mentions are highlighted in text. a text document play equal roles. Some entities are the central subjects or actors within a document, around which the content and the key events revolve. Others are mentioned only to provide additional context to the main event. For example, some entities may be actors in peripheral events, while others are deemed uninformative to the understanding of the document. Thus, entity salience in a text is defined as a binary or ordinal rating to quantify the extent to which a target entity is central to a given piece of text (Gamon et al., 2013; Dunietz and Gillick, 2014). Figure 1 provides an example text along with the mentioned entities and their salience. We note that the salience of an entity to a text is independent of the user’s interest when reading or searching the document (Gamon et al., 2013), which is usually referred to as entity relevance. It is also distinct from entity importance, which quantifies the overall importance of the entity independent of the document. Automatically inferring entity salience was shown to aid search (Gamon et al., 2013), improve ranking results (Xiong et al., 2018), entity detection (Trani et al., 2018), and enable entity-centric applications such as entity-centric summarization (Maddela et al., 2022). --- --In this paper, we study the effectiveness of Transformer-based Pre-trained Language Models (PLMs) in the task of entity salience detection. Prior work on determining entity salience relied on heavy feature engineering to craft features explicitly covering relevant aspects, such as entity frequency (Dunietz and Gillick, 2014; Dojchinovski et al., 2016), position of entity mentions within a document (Dunietz and Gillick, 2014; Trani et al., 2018), relations to other entities (Trani et al., 2018), document features, such as its length (Gamon et al., 2013) and lexical features, such as the name of the entity or its context. Only a single recent work attempted to use PLMs in a pipeline which included key entity detection, albeit the scope of the evaluation was limited to a single high performing dataset (Zhao et al., 2021). In contrast, our proposed method uses a cross-encoder architecture where a target entity’s name or alias and its contextual mentions in a text document are encoded by a PLM encoder. The classifier uses the contextual representation and, optionally, positional information about the entity encoded through the decile position embedding vector of mentions to determine the salience score of a target entity. We conduct experiments on four publicly available datasets, two of which were human annotated and two that were curated semi-automatically. We fine-tune several cross-encoders using PLMs and demonstrate that these yield consistent and significant improvements over feature-based methods, as well as prompting instruction-tuned PLMs. The latter shows the novelty and complexity of the task of entity salience detection, which requires the model to learn significant task-specific semantic knowledge for this natural language understanding task. Our contributions in this paper are the following: « We propose a cross-encoder style architecture with explicit encoding of position information for entity salience detection that shows consistent improvements of 7 — 24.4 Fl scores over previous feature engineering approaches. We establish a uniform benchmark of two human annotated and two semi-automatically curated datasets for the task of entity salience detection that we expect to be beneficial to future study of this task; A faceted analysis of the models’ predictive behaviour. 2
--- RELATED WORK ---
Understanding the aboutness of a document is one of the long-standing goals of research in both Information Retrieval and Natural Language Processing (Gamon et al., 2013). Several types of approaches have been proposed, including extracting key-terms (Hulth, 2003; Mihalcea and Tarau, 2004), identifying latent topics (Blei et al., 2003), or generating text summaries (Erkan and Radev, 2004). There has been a recent focus in using entities to understand the content of a document. Towards this goal, the task of entity salience has been first described for web pages in (Gamon et al., 2013) and for news content in (Dunietz and Gillick, 2014). This task can be viewed as a restricted form of keyword or keyphrase extraction (Alami Merrouni et al., 2020) if salience is binary. For the rest of this study, we will use the concept of salience as described in (Gamon et al., 2013). The salience labels for entities were obtained either by crowdsourcing labels from multiple raters to identify salient entities (Gamon et al., 2013; Dojchinovski et al., 2016; Trani et al., 2018; Maddela et al., 2022) or by using proxies. For example, (Dunietz and Gillick, 2014) hypothesize that salient entities are those that appear in the article’s abstract. (Wt et al., 2020a) identifies an entity as salient if the Wikinews category that corresponds to the entity is also labeled as the category of the article. Past studies mostly proposed machine learning
--- METHOD ---
uses a cross-encoder architecture where a target entity’s name or alias and its contextual mentions in a text document are encoded by a PLM encoder. The classifier uses the contextual representation and, optionally, positional information about the entity encoded through the decile position embedding vector of mentions to determine the salience score of a target entity. We conduct
--- EXPERIMENT ---
s on four publicly available datasets, two of which were human annotated and two that were curated semi-automatically. We fine-tune several cross-encoders using PLMs and demonstrate that these yield consistent and significant improvements over feature-based methods, as well as prompting instruction-tuned PLMs. The latter shows the novelty and complexity of the task of entity salience detection, which requires the model to learn significant task-specific semantic knowledge for this natural language understanding task. Our contributions in this paper are the following: « We propose a cross-encoder style architecture with explicit encoding of position information for entity salience detection that shows consistent improvements of 7 — 24.4 Fl scores over previous feature engineering approaches. We establish a uniform benchmark of two human annotated and two semi-automatically curated datasets for the task of entity salience detection that we expect to be beneficial to future study of this task; A faceted analysis of the models’ predictive behaviour. 2 Related Work Understanding the aboutness of a document is one of the long-standing goals of research in both Information Retrieval and Natural Language Processing (Gamon et al., 2013). Several types of approaches have been proposed, including extracting key-terms (Hulth, 2003; Mihalcea and Tarau, 2004), identifying latent topics (Blei et al., 2003), or generating text summaries (Erkan and Radev, 2004). There has been a recent focus in using entities to understand the content of a document. Towards this goal, the task of entity salience has been first described for web pages in (Gamon et al., 2013) and for news content in (Dunietz and Gillick, 2014). This task can be viewed as a restricted form of keyword or keyphrase extraction (Alami Merrouni et al., 2020) if salience is binary. For the rest of this study, we will use the concept of salience as described in (Gamon et al., 2013). The salience labels for entities were obtained either by crowdsourcing labels from multiple raters to identify salient entities (Gamon et al., 2013; Dojchinovski et al., 2016; Trani et al., 2018; Maddela et al., 2022) or by using proxies. For example, (Dunietz and Gillick, 2014) hypothesize that salient entities are those that appear in the article’s abstract. (Wt et al., 2020a) identifies an entity as salient if the Wikinews category that corresponds to the entity is also labeled as the category of the article. Past studies mostly proposed machine learning methods to infer the salience of a given entity that relied on hand-crafted features. Features that can be computed from the target entity mentions and document alone can be categorized into the following: positional (e.g., position in the document, if entity is in the abstract) (Dunietz and Gillick, 2014), count-based (e.g., number of references to the entity) (Dunietz and Gillick, 2014; Wu et al., 2020a), local context (Trani et al., 2018), or global context (Ponza et al., 2019). Further, joint entity salience resolution can be performed by creating features using the entity graph (e.g., centrality in the entity graph) (Dunietz and Gillick, 2014; Trani et al., 2018). Finally, past work also showed that incorporating external knowledge about entities from knowledge bases can boost predictive performance (Dojchinovski et al., 2016). Automatically inferring salience for entities can directly benefit multiple downstream applications, such as improving ranking results for queries containing entities (Xiong et al., 2018) or improv --- --ing the performance of entity detection by joint modelling (Trani et al., 2018). Moreover, by inferring salience, new entity-centric applications can be built, such as highlighting salient entities in search (Gamon et al., 2013), improving the interpretability of news trends through salient entities (Ponza et al., 2021), or identifying entities for creating entity-centric summaries of news stories (Maddela et al., 2022; Hofmann-Coyle et al., 2022). 3 Problem Definition We use the concept of salience as introduced in (Gamon et al., 2013): salient entities are entities explicitly mentioned in the document that are objectively important as a function of the structure of the text. The goal of the salience model is to produce a single salience score 7)(e) for the entity e using only the document D and the explicit entity mentions M,. We consider using external knowledge, such as information about entities from knowledge bases, to be outside the scope and leave integration of such knowledge for future work. 4 Methods Pre-trained Language Models (PLMs) have shown a remarkable ability to encode syntactic and semantic knowledge in their parameters (Tenney et al., 2018, 2019) that can be leveraged when fine-tuned on downstream natural language understanding (NLU) tasks. We postulate that PLMs can be harnessed to help in entity salience detection, a targetbased document-level NLU task. In this section, we present an architecture based on the cross-encoder setup adapted to the task of entity salience detection. 4.1 Cross-encoder Encoding Given a document D and a target entity e, which is mentioned in the document, we concatenate the target entity’s name and the document using a special [SEP] token. We then encode the text using a Transformer-based pre-trained encoder. Figure 2 shows the graphical representation of the cross-encoder model. This setup allows the model to have deep cross attention between the target entity and the entire document. Note that we use special marker tokens [BEGIN_ENTITY] and [END_ENTITY] around each mentions m € M, of entity e in document D. Salience Score W(e) FFNN h i )Npe] CLS ]’s representation Encoded decile positions I LS] Target Entity’s Name [ SEP] Document’s Text Figure 2: Graphical representation of the cross-encoder architecture with decile position encoding. Position Encoding We compute the decile positions for each entity mention (m € M,) in the document D by taking a positional index p,, € {0,1,...,9}, indicating which part of document the mention belongs to if the document is partitioned into 10 equal chunks. Depending on the number and positions of the mentions, the vector can contain multiple non-zero values in the p vector. For example, if an entity e has 1 mention in the first decile, 2 in the second decile, and 1 mention in the fifth decile, then the input to the positional encoder would be pm = [1,1,0,0, 1,0, 0, 0, 0, 0]. Note that we do not capture the number of mentions in each decile in p,,. To obtain positional embeddings, we use an embedding layer that maps positional indices to a dense vector of dimension dmodets formally hye(m) = Embedding(pm). Scoring The output representation of the [CLS] token is concatenated with the mean position embedding vector hy. and fed to a scorer module that produces a salience score y(e) € [0, 1] for entity e. The salience scorer is a feed-forward network with a sigmoid scoring function head. Formally, W(e) = o(FFN(hrcis3||Hpe)) 4.2 Optimization We fine-tune the model described above by minimizing the binary cross entropy loss that is calculated using the ground truth binary salience labels and the predicted salience score ~(e). 5 Datasets In this section, we describe our entity salience benchmark, which consists of four datasets: two --- --Dataset | NYT-Salience WN-Salience | SEL | EntSUM # Docs 110,463 6,956 365Doc Length (avg chars) 5,079 2,106 1,660 4,# Unique entities 179,341 23,205 6,779 7,# Mentions 4,405,066 145,081 19,729 | 20,% Salient entities 14% 27% 10% 39% Ground-truth Abstract Alignment | Category Alignment | Human | Human Table 1: Summary statistics and label collection methods for the datasets used in our experiments. datasets were curated using semi-automated methods and two used human annotations. We provide summary statistics of these datasets and label collection methods in Table 1. NYT-Salience This dataset is introduced in (Dunietz and Gillick, 2014) and is the largest dataset to date for entity salience detection. The dataset is curated with an assumption that salient entities are mentioned in the abstract of a news article in the NYT Corpus (Sandhaus, 2008). Entities and their mentions are identified using a classical NLP pipeline involving POS tagging, dependency parsing, and noun phrase extraction. Despite being large-scale, the automatic dataset creation process could introduce noise as corroborated by moderate agreement numbers with human annotators on a subset of the data. The dataset contains a binary salience label for each entity. WN-Salience Introduced in (Wu et al., 2020a), this is another automatically curated dataset consisting of Wikinews articles. These are annotated with Wikinews categories by their authors. WN-Salience identifies salient entities by using the hypothesis that an entity is salient if the Wikinews category that corresponds to the entity is also labeled as a category of the article. Similar to NYT-Salience, this dataset has binary salience labels. SEL This is another dataset based on Wikinews released by (Trani et al., 2018). However, unlike WNSalience, this dataset is human annotated, where multiple human annotators ranked the salience of entities into one of four categories. To conform with the binary labels of the other datasets, we map the 4 categories into binary labels of {0, 1} by mapping the bottom two classes to not salient and the top two classes to salient. EntSUM This dataset was introduced in (Maddela et al., 2022). To construct this dataset, a randomly selected set of entities spanning a subset of 693 articles from the NYT corpus were assigned salience labels by human annotators on a four-point scale, ranging between [0, 3]. For each document entity pair, two independent annotations were collected, which were increased up to 5 in case of disagreements. If the average annotation score is greater than 1.5 for an entity, it is assigned a positive salience label. 5.1 Data Enrichment with Inferred Mentions Except for EntSUM, the datasets do not have explicit entity mention offsets as annotations, which are necessary for many feature-based approaches and to compute positional embeddings. While SEL contains only the mention surface texts per entity, NYT-Salience and WN-Salience only provide the start and end character indices (aka mention offsets) of the very first mention of an entity. To this end, we infer additional mentions of an entity within the text using a combination of Flair NER (Akbik et al., 2019) and pattern matching. For SEL, since the mentions are available, we use a pattern matching approach to match the surface text of the mentions to infer mention offsets. For NYT-Salience and WN-Salience, we first use Flair NER to identify mentions of named entities in the text. We attempt to match these mentions to the first mention of each entity in the document provided in the respective datasets. Since the surface text of other mentions may differ from the first mention, we additionally use the overlap between a mention’s surface text and the entity name as a candidate mention for that entity. Applying this approach, we infer additional mentions of an entity in the text and their offsets. While this process could introduce some noise, the overall quality of the datasets are enhanced through this process. 6 Experiments We experiment on our entity salience benchmark with our proposed PLM-based method, other ML and heuristic-based approaches used in past research, as well as an instruction-tuned PLM. --- --6.1 Data Splits Prior works (Dunietz and Gillick, 2014; Trani et al., 2018; Wu et al., 2020a) use inconsistent (or not reported) train/validation/test splits. NYTSalience and WN-Salience datasets are provided with train/test splits (but no validation), whereas SEL dataset is provided without any splits. This makes it hard to benchmark previous works with a fair comparison across models. To overcome this issue, we do a temporal split of NYT-Salience’s and WN-Salience’s original training sets into a new train/validation sets based on the publication time of the news stories, which provides a more realistic testing setup (Huang and Paul, 2018; Rijhwani and Preotiuc-Pietro, 2020). We also perform a temporal split of SEL and EntSUM datasets into train/validation/test sets. Further details about the dataset splits are provided in Appendix A. 6.2 Baselines First, we list all methods used in past research, for which we report the results from the original papers. ° First Sentence. Classifies an entity as salient if it appears in the first sentence of the document’s body; used in both (Dunietz and Gillick, 2014) and (Wu et al., 2020a). Position & Frequency (Dunietz and Gillick, 2014). Feeds the first sentence index and the frequency features of an entity into a logistic regression model. All Features (Dunietz and Gillick, 2014). Uses a series of features based on position, frequency, and PageRank signals fed into a logistic regression model. SEL (Trani et al., 2018). Uses a combination of features based on position, frequency, and Wikipedia graph statistics fed into a Gradient Boosted Decision Tree algorithm implemented in sklearn (Pedregosa et al., 2011). SWAT (Ponza et al., 2019). Uses a set of features similar to the SEL Method described above, with the addition of features based on entity embeddings. All features are fed into a Gradient Boosted Decision Tree algorithm implemented in XGBoost (Chen et al., 2015). Positional Feature (Wu et al., 2020a). Uses the index of the first sentence in which the entity is mentioned as a feature in a logistic regression model. This method provides best results on the WN Salience dataset in (Wu et al., 2020a). Next, we re-implement a set of common methods based on the above baselines in order to be able to test them on all four datasets. This ensures the evaluation is performed on the same experimental setup. ¢ Positional Headline. Classifies an entity as salient whether it appears in the headline of the input document. Positional Headline & Lead. Classifies an entity as salient if it appears in the headline of the document or in the first sentence (lead sentence) of the document. Entity Frequency. Classifies an entity as salient if they are more frequent than a given value. For each dataset, we calculated different thresholds and reported the best results. Thresholds can be found in the Appendix. ¢ Features & GBDT. This method uses the most common features from past works (Dunietz and Gillick, 2014; Wu et al., 2020a; Trani et al., 2018; Ponza et al., 2019) — ie., entity’s first sentence index, and entity frequency — and feeds them into a GBDT model implemented using LightGBM (Ke et al., 2017). SEL GBDT. Follows the method from (Trani et al., 2018) and uses sklearn’s GBDT (Pedregosa et al., 2011) to train a model on the features provided with the SEL dataset. Target entity masking. This method feeds the input to a Transformer-based encoder (ROBERTabase) with the target entity mentions represented through a special mask token. The salience prediction is obtained by mean pooling the mask token representations and passing this through a feed-forward network. Zero-shot prompting. We test instruction-tuned LLMs using zero-shot prompting. The prompt introduces the task description, followed by the input text and a target entity, and it asks a yes/no question. It expects the model to generate either Yes’ or ’No’ as an answer. The LLMs, already instruction-tuned on a large collection of NLU tasks, attempt to provide an answer based on the prompt, input text, and target entity. This family of models has been demonstrated to be robust and versatile on multiple benchmarks (Chung et al., 2022). We use Flan-UL2 (2B) (Tay et al., 2023) and LLaMa 2-Chat (7B) (Touvronetal., 2023) for evaluation. --- --Source Type Method NYT-Salience WN-Salience P R Fl P R Fl (Dunietz and Gillick, 2014) | Heuristic First Sentence 59.5 | 37.8 | 46.2 - - (Dunietz and Gillick, 2014) | ML Position & Frequency 59.3 | 61.3 | 60.3 | - - (Dunietz and Gillick, 2014) | ML All Features 60.5 | 63.5 | 62.0 - - (Ponza et al., 2019) ML SWAT 62.4 | 66.0 | 64.1 | - - (Wu et al., 2020a) Heuristic First Sentence 56.0 | 41.0 | 47.3 | 47.9 | 53.2 | 50.(Wu et al., 2020a) ML Positional Feature 19.0 | 41.3 | 26.0 | 29.1 | 78.9 | 42.(Wu et al., 2020a) ML Features & GBDT 39.2 | 59.7 | 47.3 | 29.2 | 48.1 | 36.Heuristic Positional Headline 57.5 | 42.0 | 48.5 | 46.1 | 51.5 | 48.Heuristic Positional Headline & Lead 49.8 | 55.4 | 52.5 | 41.0 | 60.0 | 48.. Heuristic Entity Frequency 53.7 | 53.3 | 53.6 | 37.3 | 61.9 | 46.Our Implementations ML Features & GBDT 61.0 | 57.4 | 59.2 | 46.2 | 53.3 | 49.PLM (RoBERTa) | Target Entity Masking 64.6 | 50.2 | 56.5 | 57.0 | 65.4 | 60.PLM (RoBERTa) | cross-encoder 75.9 | 87.1 | 81.1 | 71.8 | 73.6 | 72.Our Models PLM (DeBERTa) | cross-encoder 77.5 | 87.4 | 82.1 | 71.5 | 78.3 | 74.PLM (RoBERTa) | cross-encoder w/ position emb. | 78.7 | 84.2 | 81.4 | 71.2 | 76.7 | 73.PLM (DeBERTa) | cross-encoder w/ position emb. | 75.9 | 88.4 | 81.7 | 73.3 | 76.1 | 74.Table 2: Results on the NYT-Salience and WN-Salience datasets. The ground-truth of these datasets was generated via abstract/category alignment. The top section presents results as originally reported in the source papers. SEL EntSUM Source Type Method P R Fi P R fi (Trani et al., 2018) ML SEL (w/ 5-fold cross val.) 50.0 | 61.0 | 52.0 = = (Ponza et al., 2019) ML SWAT (w/ 5-fold cross val.) 58.0 | 64.9 | 61.2 - Heuristic Positional Headline 26.6 | 78.4 | 39.7 | 60.7 | 18.5 | 28.Heuristic Positional Headline & Lead 22.1 | 87.1 | 35.3 | 51.2 | 31.6 | 39.Heuristic Entity Frequency 13.5 | 57.8 | 21.9 | 48.4 | 54.0 | 51.Our Implementations | ML Features & GBDT 26.6 | 78.4 | 39.7 | 60.7 | 52.0 | 56.ML SEL GBDT 71.1 | 47.8 | 57.1 - PLM (RoBERTa) | Target Entity Masking 36.3 | 13.8 | 20.0 | 63.0 | 41.7 | 50.PLM (RoBERTa) | cross-encoder 51.6 | 73.6 | 60.6 | 65.5 | 60.6 | 63.Our Models PLM (DeBERTa) | cross-encoder 64.1 | 73.6 | 68.5 | 64.9 | 59.2 | 61.PLM (RoBERTa) | cross-encoder w/ position emb. | 63.0 | 69.9 | 66.3 | 67.5 | 57.0 | 61.PLM (DeBERTa) | cross-encoder w/ position emb. | 67.3 | 62.4 | 64.7 | 72.1 | 51.5 | 60.Table 3: Results on the SEL and EntSUM datasets. The ground-truth of these datasets was generated via human annotation. The top section presents results as originally reported in the source papers. 6.3 Experimental Setup We use RoBERTa-base (Liu et al., 2019) and DeBERTa-v3-base (He et al., 2023) as the base PLM for experiments. For each of these base models, we train both a cross-encoder model and a cross-encoder model augmented with decile positional embeddings. For training our proposed models, we use AdamW (Loshchilov and Hutter, 2019) as the optimizer. We perform a hyperparameter search for learning rate using the following set of values: {0.001, 0.0005, 0.0002, 0.0001, 0.00005}. We train our models for a maximum of 10 epochs with early stopping based on the validation set performance. We pick the best performing model checkpoints for each dataset based on the perfor mance on the validation set. In Tables 2 and 3, we report the performance of our models and the baselines using the standard classification metrics (i.e., Precision, Recall, and F1) on the positive (salient) class, following previous research on entity salience. For training and inference of each Transformerbased model, we use a single NVIDIA V100 GPU with 32GB GPU memory, 4 CPUs, and 128 GB of main memory. 6.4 Results In Tables 2 and 3, we present the experimental results of the baselines and our proposed models on the four datasets described in Section 5. Comparison with feature-based methods. We --- --observe that the cross-encoder model significantly outperforms all baseline models in F1 score. It also yields better precision compared to the baselines for three of the four datasets. Only for the SEL dataset does the SEL GBDT model trained on publicly available pre-computed features produce a model with better precision than the cross-encoder. We observe that adding the decile positional embedding with cross-encoder improves the precision across all datasets, but also degrades the recall in every dataset except NYT-Salience. The Target Entity Masking approach, which also leverages contextual information with a transformer-based model yields mixed results. Overall, the model is able to obtain better precision than the feature-based models for all datasets except SEL, but the model suffers from poor recall across all datasets, resulting in significantly worse F1 scores especially when compared to crossencoder models. Our re-implementation of positional methods and GBDT methods are consistent with the performance reported in prior works. The variance in numbers can be attributed to the enrichment of datasets with inferred mentions (Section 5.1) and the explicit train/dev/test data split used in our experiments (Section 6.1). 6.5 Zero-shot prompting of large language models We formulate the problem of salience detection with zero-shot prompting as follows: given a definition of entity salience task and document text, we ask the model to generate a ""yes"" or a ""no"" if a particular entity is salient or not. We experimented with two open source models (Flan-UL2 (2B) and LLaMa 2-Chat (7B)) available on Hugging Face !, and present the results in Table 4. To the best of our knowledge, this is the first evaluation of zero-shot prompting of instruction-tuned models for the entity salience detection task. We observe that the LLaMa 2-Chat model with 7 billion parameters fails to yield any meaningful results as it produces only positive labels for all data points (hence we observe 100% recall). The Flan-ULmodel is able to generate both positive and negative labels. However, the precision remains too low across datasets. We further discuss causes for this performance in the Appendix (Section C), along with the implementation details. Overall, these ex ‘www. huggingface. com periments suggest that entity salience detection is a unique task that is not similar to any other tasks these two models are instruction tuned on. 7 Analysis In this section, we perform an analysis of model predictions in order to gain more insights into model behavior and understand potential avenues for further improvement. We thus break down performance by different factors including: the importance of inferring all entity mentions, the position of the first entity mention, and entity mention frequency. 7.1 Impact of Inferred Mentions In Section 5.1, we inferred additional mentions of an entity for the NYT-Salience and WN-Salience datasets. We compare the performance of our best model that leverages multiple mentions of an entity to its version trained with only the first mentions of entities in a document. The specific input formats for this experiment are presented in Appendix B The results in Table 5 show that doing so consistently improves the performance of our models across all datasets. In particular, for the largest dataset, NYT-Salience, our model achieves a substantial gain of 27.3 Fl points. This experiment showcases the importance of augmenting our datasets with additional mentions and the importance of explicitly modelling contextual information present around all entity mentions. 7.2 Stratified Analysis on First Mention Position We compare our cross-encoder models against the Features & GBDT model, our re-implemented baseline that relies on the most popular features used in prior works (Dunietz and Gillick, 2014; Wu et al., 2020a; Trani et al., 2018). As shown in the results from Tables 2 and 3, among other features, positional features are most informative for salience. Intuitively, if an entity is mentioned in the headline or in the first sentence of a news article, there is high probability of that entity being salient. Figure 3 shows that all models perform well when the first mention falls in the headline or the first sentence of the document. We notice that the crossencoder models constantly outperform the Features & GBDT model and the largest gains are observed in the SEL and WN-Salience datasets. This observation indicates that the cross-encoder models are --- --Model NYT-Salience WN-Salience SEL EntSUM P R Fl P R Fi P R Fl P R Fl Cross-encoder (DeBERTa) | 77.5 | 87.4 | 82.1 | 71.5 | 78.3 | 74.8 | 64.1 | 73.6 | 68.5 | 64.9 | 59.2 | 61.Flan-UL2 31.1 | 72.4 | 43.5 | 30.7 | 90.1 | 45.9 | 16.7 | 98.3 | 28.5 | 27.6 | 83.6 | 41.LLaMa 2-Chat 14.6 | 100.0 | 25.4 | 27.1 | 100.0 | 42.6 | 9.49 | 100.0 | 17.3 | 19.2 ) 100.0 | 32.Table 4: Performance comparison of cross-encoder model with zero-shot prompting of LLMs. Model NYT-Salience WN-Salience SEL EntSUM P R Fl P R Fl P R Fl P R Fl Cross-encoder w/ first mention | 54.2 | 57.5 | 55.8 | 69.6 | 80.4 | 74.6 | 59.8 | 76.1 | 67.0 | 69.1 | 53.2 | 60.Cross-encoder w/ all mentions | 77.5 | 87.4 | 82.1 | 71.5 | 78.3 | 74.8 | 64.1 | 73.6 | 68.5 | 64.9 | 59.2 | 61.Table 5: Performance comparison of cross-encoder models with only the first mention vs. all inferred mentions. F1 Scores: First mention in headline + first sentence F1L Scores: First mention in max sequence length (512) F1 Scores: First mention outside max sequence length (512)0,830.| THE0.84 9.0,760.Method LightGBM mmm DeBERTA jum DeBERTa+Decile s9Qgzo.0.1.0.FL0.0.1.0.0.FL 0.0.0.10) 0840.oa: 0.74] 06. oa:0.0: NYTSalience WN-Salience Dataset EntSUM EntSUM0820.on oosNYTSalience WN-Salience Dataset SEL WN-Salience _ NYT-Salience Dataset 7,001.001.0.04] 0,00 0,000.EntSUM (a) Performance with respect to the position of the mentions. There are no mentions outside of the context window for NYT. F1 Scores: Mention FrequencyF1 Scores: Mention Frequency 2-1.Method LightGBM 0.10.81 | mm DeBERTa 0.8 0.76 0.76 lm DeBERTa+Decile 0.63 0.63 0.62 0.63 0.64 0.0.058 9.0.0.0.00) oo NYT-Salience EntSUM WN-Salience ° NYT-Salience EntSUM WN-Salience Dataset Dataset F1 Scores: Mention Frequency 6-10 1.0 F1 Scores: Mention Frequency >: 3.0.oe 9.95 4 45 9,920:.88 0.0.83} os pes o.‘esos 0.73 0.710.0.65) 0.0.0.00 0.00 0.00 0.NYT-Salience EntSUM WN-Salience NYT-Salience EntSUM SEL WN-Salience Dataset Dataset (b) Performance with respect to the frequency of the entities. The test split of SEL dataset does not contain any entity with more than 10 mentions in a document. Figure 3: Stratified analysis across models and datasets. --- --able to use the context to identify that mentions that occur in the headline or the first parts of the document are often salient without explicitly using this information as a feature. We also investigate the performance of the models when the first mention falls inside or outside the context window of the PLM (here, 512 tokens). When mentions fall inside the context window, we observe that the cross-encoder models consistently outperform the Features & GBDT model. When the mention falls outside the context window, the model predictions become close to random, which is expected, as the model does not have immediate contextual information around the mention. Using models that can deal with longer inputs would be a promising direction for improvement for these samples (Beltagy et al., 2020). Interestingly, for WN-Salience, the Features & GBDT model also performs considerably worse outside the firsttokens. 7.3 Stratified Analysis on Mention Frequency Similar to mention position analysis, we compare our cross-encoder models against the Features & GBDT model, which uses mention frequency as one of its input features. Figure 3 shows how the cross-encoder models and Features & GBDT compare with varying frequency of entity mentions. For salient entities with single mentions, the cross-encoder model performs significantly better than the Features & GBDT model. In particular, for the NYT-Salience dataset, the Features & GBDT model fails to predict any of the single mention entities as salient. This observation indicates that the cross-encoder models do not simply model the mention frequency, but potentially leverage other contextual information to determine the salience of entities with a single mention. The performance of the Features & GBDT model improves with more mentions per entity. In fact, for the frequency range of 6-10 mentions per entity, the Features & GBDT model performs better than the cross-encoder models for EntSUM and SEL datasets. This observation indicates the overreliance of the Features & GBDT model on mention frequency to determine salience, but also that the cross-encoder cannot fully use this heuristic. 8
--- CONCLUSION ---
This paper aims to leverage the semantic knowledge encoded in pre-trained language models for entity salience detection. We propose the crossencoder method based on Transformer-based PLMs with positional representations and compare its performance to several ML-based methods, heuristic methods, and instruction-tuned LLMs across four different datasets, two human-annotated and two automatically curated. Across all our experiments, the cross-encoder model based on pre-trained language models outperforms all other methods, often with double digit gains in F-1 score. Analyses of model behavior illustrate the important effects of mention frequency, mention position, and document length on performance, highlighting areas of future work. 9 Limitations We only studied salience in English-language documents, but our methods are applicable to other languages directly as long as a pre-trained language model covering the target language is available. We use entity mentions as annotated in our data or inferred through entity recognition and entity resolution for inference in some of the methods. This information may not be available at inference time in all applications. The experiments with LLMs are limited to zeroshot prompts. We did not experiment with instruction tuning which could potentially help the model learn the salience detection task. Finally, we do not use external knowledge about entities and their relationships in modelling, which was shown to marginally improve results in past studies (Dunietz and Gillick, 2014; Trani et al., 2018; Ponza et al., 2019). We consider this out of the scope of our analysis and a viable direction of future work. 10 Ethics Statement We use publicly available datasets intended for the task of entity salience detection. The datasets and pre-trained models we used have permissive licenses allowing for research use. We do not envision any potential risks associated with the task discussed in this paper. References Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art NLP. In NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for --- --Computational Linguistics (Demonstrations), pages 54-59. Zakariae Alami Merrouni, Bouchra Frikh, and Brahim Ouhbi. 2020. Automatic keyphrase extraction: a survey and trends. Journal of Intelligent Information Systems, 54(2):391—424. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3(null):993-1022. Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2015. XGBoost: Extreme Gradient Boosting. R package version 0.4-2, 1(4):1-4. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Leon Derczynski, Kalina Bontcheva, and Jan Roberts. 2016. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1169— 1179, Osaka, Japan. The COLING 2016 Organizing Committee. Milan Dojchinovski, Dinesh Reddy, Tomas Kliegr, Tomas Vitvar, and Harald Sack. 2016. Crowdsourced corpus with entity salience annotations. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 3307-3311, Portoroz, Slovenia. European Language Resources Association (ELRA). Jesse Dunietz and Daniel Gillick. 2014. A new entity salience task with millions of training examples. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages 205-209, Gothenburg, Sweden. Association for Computational Linguistics. Giines Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif: Int. Res., 22(1):457-479. Michael Gamon, Tae Yano, Xinying Song, Johnson Apacible, and Patrick Pantel. 2013. Understanding document aboutness step one: Identifying salient entities. Technical Report MSR-TR-2013-73. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations. Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fiirstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782-792, Edinburgh, Scotland, UK. Association for Computational Linguistics. Ella Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie, Mounica Maddela, and Daniel Preotiuc-Pietro. 2022. Extractive entity-centric summarization as sentence selection using bi-encoders. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 326-333, Online only. Association for Computational Linguistics. Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57-60, New York City, USA. Association for Computational Linguistics. Xiaolei Huang and Michael J. Paul. 2018. Examining temporality in document classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 694-699, Melbourne, Australia. Association for Computational Linguistics. Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216-223. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information Processing systems, 30. Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. 2018. End-to-end neural entity linking. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 519-529, Brussels, Belgium. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Dangi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Mounica Maddela, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2022. EntSUM: A data set for entitycentric extractive summarization. In Proceedings of --- --the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3355-3366, Dublin, Ireland. Association for Computational Linguistics. Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, WSDM ° 12, page 563-572, New York, NY, USA. Association for Computing Machinery. Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404-411, Barcelona, Spain. Association for Computational Linguistics. Dat Ba Nguyen, Johannes Hoffart, Martin Theobald, and Gerhard Weikum. 2014. Aida-light: High-throughput named-entity disambiguation. In Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International World Wide Web Conference (WWW 2014), Seoul, Korea, April 8, 2014, volume 1184 of CEUR Workshop Proceedings. CEURWS.org. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830. Marco Ponza, Diego Ceccarelli, Paolo Ferragina, Edgar Meij, and Sambhav Kothari. 2021. Contextualizing trending entities in news stories. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 346-354. Marco Ponza, Paolo Ferragina, and Francesco Piccinno. 2019. Swat: A system for detecting salient wikipedia entities in texts. Computational Intelligence, 35(4):858-890. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bjérkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143-152, Sofia, Bulgaria. Association for Computational Linguistics. Shruti Rijhwani and Daniel Preotiuc-Pietro. 2020. Temporally-informed analysis of named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7605-7617, Online. Association for Computational Linguistics. Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752. Benjamin Strauss, Bethany Toma, Alan Ritter, MarieCatherine De Marneffe, and Wei Xu. 2016. Results of the w-nut 2016 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), pages 138-144. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. U2: Unifying language learning paradigms. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2018. What do you learn from context? probing for sentence structure in contextualized word representations. In Jnternational Conference on Learning Representations. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142-147. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoging Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Salvatore Trani, Claudio Lucchese, Raffaele Perego, David E. Losada, Diego Ceccarelli, and Salvatore Orlando. 2018. Sel: A unified algorithm for salient entity linking. Computational Intelligence, 34(1):2 --- --Chuan Wu, Evangelos Kanoulas, Maarten de Rijke, and Wei Lu. 2020a. Wn-salience: A corpus of news articles with entity salience annotations. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2095-2102. Chuan Wu, Evangelos Kanoulas, and Maarten de Rijke. 2020b. Learning entity-centric document representations using an entity facet topic model. Inf: Process. Manage., 57(3). Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie- Yan Liu. 2018. Towards better text understanding and retrieval through kernel entity salience modeling. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 575-584. Lingyun Zhao, Lin Li, Xinhao Zheng, and Jianwei Zhang. 2021. A bert based sentiment analysis and key entity detection approach for online financial texts. In 202] IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 1233-1238. IEEE. --- --Appendix A Details of Dataset Splits Table 6 contains the train, dev, and test splits of each of the datasets after applying a temporal splitting strategy described in Section 6.1. These splits are used for model training and evaluation. B_ Input Format for Experiments As described in Section 4.1, we add special marker tokens around each mention of the target entity (i-e., the entity for which the model needs to predict the salience label.). In the following, we provide an example: Musk completes $44 billion Twitter deal. Elon Musk, the world’s ... Model Input [CLS] Elon Musk [SEP] [BEGIN_ENTITY] Musk [END_ENTITY] completes $44 billion Twitter deal. [BEGIN_ENTITY] Elon Musk [END_ENTITY], the world’s ... For the experiment with first mention reported in Section 7.1, only the first mention is bounded by special marker tokens as shown in the following example: Model Input [CLS] Elon Musk [SEP] [BE GIN_ENTITY] Musk [END_ENTITY] completes $44 billion Twitter deal. Elon Musk, the world’s ... Note that the second mention of Elon Musk is not bounded by marker tokens. C_ Implementation details of zero-shot prompting of LLMs Figure 4 and Figure 5 show the prompts we used for the LLaMa 2-Chat (7B) and Flan-UL2 (2B) models respectively. Table 7 lists the generation parameters. We speculate the following causes for the relatively lower precision obtained using this method: ¢ The instruction defines the salience task definition, but doesn’t provide any reference ex amples (few-shot prompting) to align with the definition of salience. This leads to the model identifying an entity as salient based on its frequency in the document. However, creating a few-shot prompt is challenging as we need to limit the maximum input length of the prompt to prevent out-of-memory issues. * We truncate the document text so that the entire prompt is 2048 tokens or less, thus throwing away any potential information present towards the end of a long document. <s> [INST] «SYS» The salience of an entity provides information about the importance or centrality of that entity to the entire document text. In the following, given an Entity and a Text, you need to answer ’ Yes’ if the Text document is about that Entity and ’No’ if the Text is not about that Entity. «/SYS» Is Entity: entity salient in Text: text [/INST] Figure 4: Instruction for zero-shot prompting of LLaMa 2-Chat model. ##H# Instruction ### The salience of an entity provides infor- mation about the importance or centrality of that entity to the entire document text. In the following, given an Entity and a Text, you need to answer ’ Yes’ if the Text document is about that Entity and ’No’ if the Text is not about that Entity. Text: text Entity: entity Question: Is the above Entity salient in the above Text? Please answer Yes or No. Answer: Figure 5: Instruction for zero-shot prompting of FlanUL2 model. D_ Thresholds for Entity Frequency baseline Figure 6 shows the performance of the Entity Frequency baseline by varying the minimum number of times an entity has to occur in the input document to be classified as salient. --- --Dataset # Doc-Entity pairs Train | Validation Test NYT-Salience 1,910,214 | 1,342,092 405,335 | 162,WN-Salience 62,537 41,625 11,902 9,SEL 12,257 6,106 2,400 3,EntSUM 9,934 5,206 1,861 2,Table 6: Document-Entity pairs in train, validation, and test splits after applying temporal splitting. Generation parameter | Value top_ktop_ptemperaturemax_new_tokensTable 7: Parameters for generating a salience Dataset = NYT-Salience 1.0.8: 0.6: Fl 0.4: 0.0.0: Dataset = WN-Salience Dataset = SEL Dataset = EntSUM abel with zero-shot prompt.4 5 6 1 2Min Frequency Threshold ry 5TaFigure 6: Performance of the Entity Frequency baseline over different thresholds. Contextual Features Scorer (Feed Forward [og Network) Summary Encoder ase) (ROBERT: t Positional Features Salience score between [0, 1] for target entity Figure 7: Schematic diagram of the Target Entity Masking model architecture. Method @ Precision ‘A Recall @ Fl
"	"--- ABSTRACT ---
뉴스 기사와 같은 텍스트 문서에서 콘텐츠와 주요 이벤트는 일반적으로 문서에 언급된 모든 엔터티의 하위 집합을 중심으로 전개됩니다. 이러한 엔터티는 종종 두드러진 엔터티로 간주되며, 독자에게 문서의 관련성에 대한 유용한 단서를 제공합니다. 엔터티의 두드러짐을 식별하는 것은 검색, 순위 지정, 엔터티 중심 요약 등과 같은 여러 다운스트림 애플리케이션에서 도움이 되는 것으로 나타났습니다. 두드러진 엔터티 감지에 대한 이전 작업은 주로 대규모 기능 엔지니어링이 필요한 머신 러닝 모델에 초점을 맞추었습니다. 우리는 중간 크기의 언어 모델을 교차 인코더 스타일 아키텍처로 미세 조정하면 기능 엔지니어링 접근 방식보다 상당한 성능 향상을 얻을 수 있음을 보여줍니다. 이를 위해 중간 크기의 사전 학습된 언어 모델 패밀리를 대표하는 모델을 사용하여 공개적으로 사용 가능한 네 가지 데이터 세트에 대한 포괄적인 벤치마킹을 수행합니다. 또한 명령어 조정 언어 모델의 제로샷 프롬프팅은 열등한 결과를 생성하여 작업의 고유성과 복잡성을 나타냅니다. 1
--- INTRODUCTION ---
많은 NLP 연구에서 문서의 의미를 이해하는 데 있어 엔터티의 중요성을 강조했습니다(Wu et al., 2020b; Meij et al., 2012). 구조화되지 않은 텍스트 문서에서 엔터티를 자동으로 식별하고 위키피디아와 같은 기본 지식 기반에 연결하는 것은 핵심 NLP 작업 중 하나이며, 여러 공유 작업(Tjong Kim Sang and De Meulder, 2003; Strauss et al., 2016), 벤치마크(Hoffart et al., 2011; Hovy et al., 2006; Pradhan et al., 2013; Rijhwani and Preotiuc-Pietro, 2020; Derczynski et al., 2016) 및 이를 해결하는 데 전념하는 연구(Kolitsas et al., 2018; Nguyen et al., 2014)가 있습니다. 영어: 엔터티가 문서 이해에서 중요한 의미적 역할을 할 수 있지만, 모든 엔터티가 *작업은 저자가 Bloomberg에 소속되어 있는 동안 수행되었습니다.머스크가 440억 달러 규모의 Twitter 거래를 완료 뉴스 세계에서 가장 부유한 사람이자 Tesla의 CEO이며 미국의 연구 실험실 Open Al의 설립자인 Elon Musk가 소셜 미디어 회사 Twitter를 440억 달러에 인수했습니다.목요일에 Twitter의 사장인 Parag Agrawal이 해고되어 샌프란시스코 본사에서 쫓겨났습니다.뉴욕 증권 거래소는 Twitter의 주식 거래가 금요일부터 중단되었다고 보도했습니다.머스크는 도널드 트럼프 전 미국 대통령을 포함하여 과거에 중단된 사용자에 대한 금지를 해제할 것이라고 주장했습니다.비주요 엔터티 주요 엔터티 CEO Elon Musk Twitter Tesla, Inc. Open Al Parag Agrawal 샌프란시스코 뉴욕 증권 거래소 미국 대통령 도널드 트럼프 그림 1: 주요 엔터티와 비주요 엔터티가 있는 문서의 예.엔터티 언급은 텍스트에서 강조 표시됩니다.텍스트 문서는 동등한 역할을 합니다. 일부 엔터티는 문서 내에서 중심 주제 또는 행위자로, 이를 중심으로 콘텐츠와 주요 이벤트가 전개됩니다. 다른 엔터티는 주요 이벤트에 대한 추가 맥락을 제공하기 위해서만 언급됩니다. 예를 들어, 일부 엔터티는 주변 이벤트의 행위자일 수 있지만, 다른 엔터티는 문서 이해에 도움이 되지 않는 것으로 간주됩니다. 따라서 텍스트에서 엔터티 두드러짐은 대상 엔터티가 주어진 텍스트에서 중심적인 정도를 정량화하기 위한 이진 또는 순서적 평가로 정의됩니다(Gamon et al., 2013; Dunietz and Gillick, 2014). 그림 1은 언급된 엔터티와 그 두드러짐과 함께 예시 텍스트를 제공합니다. 텍스트에 대한 엔터티의 두드러짐은 문서를 읽거나 검색할 때 사용자의 관심과 무관하다는 점에 유의합니다(Gamon et al., 2013). 이를 일반적으로 엔터티 관련성이라고 합니다. 또한 문서와 무관하게 엔터티의 전반적인 중요성을 정량화하는 엔터티 중요도와도 다릅니다. 엔티티 샐리언스를 자동으로 추론하는 것은 검색을 돕고(Gamon 등, 2013), 순위 결과를 개선하고(Xiong 등, 2018), 엔티티 감지(Trani 등, 2018)를 돕고, 엔티티 중심 요약과 같은 엔티티 중심 애플리케이션을 가능하게 하는 것으로 나타났습니다(Maddela 등, 2022). 이 논문에서는 엔티티 샐리언스 감지 작업에서 Transformer 기반 사전 학습된 언어 모델(PLM)의 효과를 연구합니다. 엔티티 두드러짐을 결정하는 이전 작업은 엔티티 빈도(Dunietz 및 Gillick, 2014년; Dojchinovski 등, 2016년), 문서 내 엔티티 언급 위치(Dunietz 및 Gillick, 2014년; Trani 등, 2018년), 다른 엔티티와의 관계(Trani 등, 2018년), 길이(Gamon 등, 2013년)와 같은 문서 특징, 엔티티 이름이나 컨텍스트와 같은 어휘적 특징과 같은 관련 측면을 명시적으로 포함하는 특징을 제작하기 위해 많은 특징 엔지니어링에 의존했습니다. 최근 단일 작업만이 주요 엔티티 감지를 포함하는 파이프라인에서 PLM을 사용하려고 시도했지만 평가 범위는 단일 고성능 데이터 세트로 제한되었습니다(Zhao 등, 2021년). 이와 대조적으로, 제안하는 방법은 대상 엔티티의 이름이나 별칭과 텍스트 문서에서의 맥락적 언급이 PLM 인코더에 의해 인코딩되는 교차 인코더 아키텍처를 사용합니다. 분류기는 멘션의 데실 위치 임베딩 벡터를 통해 인코딩된 엔터티에 대한 문맥적 표현과 선택적으로 위치 정보를 사용하여 대상 엔터티의 두드러짐 점수를 결정합니다. 우리는 공개적으로 사용 가능한 데이터 세트 4개에 대한 실험을 수행하는데, 그 중 2개는 인간이 주석을 달았고 2개는 반자동으로 큐레이션했습니다. 우리는 PLM을 사용하여 여러 교차 인코더를 미세 조정하고 이것이 피처 기반 방법과 지시 조정 PLM을 촉구하는 것보다 일관되고 상당한 개선을 가져온다는 것을 보여줍니다. 후자는 엔터티 두드러짐 감지 작업의 참신함과 복잡성을 보여주며, 이를 위해서는 모델이 이 자연어 이해 작업을 위해 상당한 작업별 의미 지식을 학습해야 합니다. 이 논문에서 우리의 기여는 다음과 같습니다. • 우리는 엔터티 두드러짐 감지를 위한 위치 정보의 명시적 인코딩을 사용하여 이전 피처 엔지니어링 접근 방식보다 7~24.4 F1 점수의 일관된 개선을 보여주는 교차 인코더 스타일 아키텍처를 제안합니다. • 우리는 엔티티 눈에 띄는 특성 감지 작업을 위해 두 개의 인간이 주석을 단 데이터 세트와 두 개의 반자동으로 큐레이팅한 데이터 세트의 균일한 벤치마크를 확립했으며 이는 이 작업의 미래 연구에 유익할 것으로 기대합니다. • 모델의 예측 동작에 대한 측면 분석. 2
--- RELATED WORK ---
문서의 관련성을 이해하는 것은 정보 검색 및 자연어 처리(Gamon et al., 2013) 분야의 연구에서 오랫동안 추구해 온 목표 중 하나입니다. 주요 용어 추출(Hulth, 2003; Mihalcea and Tarau, 2004), 잠재적 주제 식별(Blei et al., 2003), 텍스트 요약 생성(Erkan and Radev, 2004) 등 여러 가지 접근 방식이 제안되었습니다. 최근에는 엔터티를 사용하여 문서의 내용을 이해하는 데 중점을 두고 있습니다. 이 목표를 달성하기 위해 엔터티 중요도 작업이 웹 페이지(Gamon et al., 2013)와 뉴스 콘텐츠(Dunietz and Gillick, 2014)에 대해 처음 설명되었습니다. 중요도가 이진인 경우 이 작업은 제한된 형태의 키워드 또는 키프레이즈 추출(Alami Merrouni et al., 2020)로 볼 수 있습니다. 이 연구의 나머지 부분에서는 (Gamon et al., 2013)에 설명된 대로 두드러짐의 개념을 사용할 것입니다. 엔터티의 두드러짐 레이블은 여러 평가자로부터 크라우드소싱 레이블을 얻어 두드러진 엔터티를 식별하거나(Gamon et al., 2013; Dojchinovski et al., 2016; Trani et al., 2018; Maddela et al., 2022) 프록시를 사용하여 얻었습니다. 예를 들어 (Dunietz and Gillick, 2014)는 두드러진 엔터티가 기사의 초록에 나타나는 엔터티라고 가정합니다. (Wu et al., 2020a)는 엔터티에 해당하는 Wikinews 범주가 기사의 범주로 레이블이 지정된 경우 엔터티를 두드러진 것으로 식별합니다. 과거 연구는 대부분 머신 러닝을 제안했습니다.
--- METHOD ---
대상 엔터티의 이름 또는 별칭과 텍스트 문서의 문맥적 언급이 PLM 인코더에 의해 인코딩되는 교차 인코더 아키텍처를 사용합니다. 분류기는 문맥적 표현과 선택적으로 언급의 데실 위치 임베딩 벡터를 통해 인코딩된 엔터티에 대한 위치 정보를 사용하여 대상 엔터티의 두드러짐 점수를 결정합니다. 우리는 다음을 수행합니다.
--- EXPERIMENT ---
4개의 공개적으로 사용 가능한 데이터 세트에 대한 s, 그 중 2개는 인간이 주석을 달았고 2개는 반자동으로 큐레이션했습니다. PLM을 사용하여 여러 교차 인코더를 미세 조정하고 이것이 피처 기반 방법 및 지시 조정 PLM을 촉진하는 것보다 일관되고 상당한 개선을 가져온다는 것을 보여줍니다. 후자는 엔터티 두드러짐 감지 작업의 참신함과 복잡성을 보여주며, 이 자연어 이해 작업을 위해 모델이 상당한 작업별 의미 지식을 학습해야 합니다. 이 논문에서 우리의 기여는 다음과 같습니다. • 우리는 엔터티 두드러짐 감지를 위한 위치 정보를 명시적으로 인코딩하는 교차 인코더 스타일 아키텍처를 제안하며, 이는 이전의 피처 엔지니어링 접근 방식보다 7~24.4 F1 점수의 일관된 개선을 보여줍니다. • 우리는 이 작업에 대한 미래 연구에 유익할 것으로 기대하는 엔터티 두드러짐 감지 작업을 위한 2개의 인간 주석이 달린 데이터 세트와 2개의 반자동으로 큐레이션된 데이터 세트의 균일한 벤치마크를 확립합니다. • 모델의 예측 동작에 대한 측면 분석. 2 관련 연구 문서의 관련성을 이해하는 것은 정보 검색 및 자연어 처리(Gamon et al., 2013) 분야의 연구의 오랜 목표 중 하나입니다. 주요 용어 추출(Hulth, 2003; Mihalcea and Tarau, 2004), 잠재적 주제 식별(Blei et al., 2003), 텍스트 요약 생성(Erkan and Radev, 2004)을 포함한 여러 유형의 접근 방식이 제안되었습니다. 최근에는 문서의 내용을 이해하기 위해 엔터티를 사용하는 데 중점을 두고 있습니다. 이 목표를 달성하기 위해 엔터티 중요도 작업이 웹 페이지(Gamon et al., 2013)와 뉴스 콘텐츠(Dunietz and Gillick, 2014)에 대해 처음 설명되었습니다. 이 작업은 중요도가 이진인 경우 제한된 형태의 키워드 또는 키프레이즈 추출(Alami Merrouni et al., 2020)로 볼 수 있습니다. 이 연구의 나머지 부분에서는 (Gamon et al., 2013)에 설명된 대로 두드러짐의 개념을 사용할 것입니다. 엔터티의 두드러짐 레이블은 여러 평가자로부터 크라우드소싱 레이블을 얻어 두드러진 엔터티를 식별하거나(Gamon et al., 2013; Dojchinovski et al., 2016; Trani et al., 2018; Maddela et al., 2022) 프록시를 사용하여 얻었습니다. 예를 들어 (Dunietz and Gillick, 2014)는 두드러진 엔터티가 기사의 초록에 나타나는 엔터티라고 가정합니다. (Wu et al., 2020a) 엔터티에 해당하는 Wikinews 범주가 기사의 범주로도 레이블이 지정되어 있는 경우 엔터티를 두드러진 것으로 식별합니다. 이전 연구에서는 주로 수작업으로 만든 기능에 의존하여 주어진 엔터티의 두드러짐을 추론하는 기계 학습 방법을 제안했습니다. 대상 엔터티 언급과 문서에서만 계산할 수 있는 기능은 다음과 같이 분류할 수 있습니다.위치 기반(예: 엔터티가 초록에 있는 경우 문서 내 위치)(Dunietz and Gillick, 2014), 카운트 기반(예: 엔터티에 대한 참조 수)(Dunietz and Gillick, 2014; Wu et al., 2020a), 로컬 컨텍스트(Trani et al., 2018) 또는 글로벌 컨텍스트(Ponza et al., 2019). 또한 엔터티 그래프(예: 엔터티 그래프의 중심성)를 사용하여 기능을 생성하여 공동 엔터티 두드러짐 해결을 수행할 수 있습니다(Dunietz and Gillick, 2014; Trani et al., 2018). 마지막으로 과거 연구에서는 지식 기반의 엔터티에 대한 외부 지식을 통합하면 예측 성능이 향상될 수 있음을 보여주었습니다(Dojchinovski et al., 2016). 엔티티에 대한 두드러짐을 자동으로 추론하면 엔티티가 포함된 쿼리의 순위 결과를 개선(Xiong et al., 2018)하거나 공동 모델링을 통한 엔티티 감지 성능을 개선(Trani et al., 2018)하는 등 여러 다운스트림 애플리케이션에 직접적인 이점이 될 수 있습니다.또한 두드러짐을 추론함으로써 검색에서 두드러진 엔티티를 강조(Gamon et al., 2013), 두드러진 엔티티를 통해 뉴스 트렌드의 해석성을 개선(Ponza et al., 2021), 뉴스 기사의 엔티티 중심 요약을 작성하기 위한 엔티티 식별(Maddela et al., 2022; Hofmann-Coyle et al., 2022)과 같은 새로운 엔티티 중심 애플리케이션을 구축할 수 있습니다.3 문제 정의 (Gamon et al., 2013)에서 도입된 두드러짐의 개념을 사용합니다. 두드러진 엔티티는 텍스트 구조의 함수로서 객관적으로 중요한 문서에 명시적으로 언급된 엔티티입니다. 두드러짐 모델의 목표는 문서 D와 명시적 엔터티가 Me를 언급하는 것만 사용하여 엔터티 e에 대한 단일 두드러짐 점수 &amp;(e)를 생성하는 것입니다. 지식 기반의 엔터티에 대한 정보와 같은 외부 지식을 사용하는 것은 범위를 벗어나며 이러한 지식의 통합은 향후 작업으로 남겨둡니다. 4가지 방법 사전 학습된 언어 모델(PLM)은 매개변수에서 구문적 및 의미적 지식을 인코딩하는 놀라운 능력을 보여주었으며(Tenney et al., 2018, 2019), 다운스트림 자연어 이해(NLU) 작업에서 미세 조정 시 활용할 수 있습니다. PLM을 활용하여 대상 기반 문서 수준 NLU 작업인 엔터티 두드러짐 감지에 도움을 줄 수 있다고 가정합니다. 이 섹션에서는 엔터티 두드러짐 감지 작업에 맞게 조정된 교차 인코더 설정을 기반으로 하는 아키텍처를 제시합니다. 4.1 교차 인코더 인코딩 문서 D와 문서에 언급된 대상 엔터티 e가 주어지면, 특수 [SEP] 토큰을 사용하여 대상 엔터티의 이름과 문서를 연결합니다. 그런 다음 Transformer 기반 사전 학습된 인코더를 사용하여 텍스트를 인코딩합니다. 그림 2는 교차 인코더 모델의 그래픽 표현을 보여줍니다. 이 설정을 통해 모델은 대상 엔터티와 전체 문서 간에 깊은 교차 어텐션을 가질 수 있습니다. 문서 D에서 엔터티 e의 각 언급 mЄ Me 주위에 특수 마커 토큰 [BEGIN_ENTITY]와 [END_ENTITY]를 사용한다는 점에 유의하세요. 두드러짐 점수(e) [CLS]의 표현 FFNN [CLS], hpe] 인코딩된 데실 위치 Transformer [CLS] 대상 엔터티의 이름 [SEP] 문서의 텍스트 그림 2: 데실 위치 인코딩을 사용한 교차 인코더 아키텍처의 그래픽 표현. 위치 인코딩 문서 D에서 각 엔터티 언급(m Є Me)에 대한 데실수 위치를 계산하려면 문서가 10개의 동일한 청크로 분할된 경우 언급이 문서의 어느 부분에 속하는지를 나타내는 위치 인덱스 pm Є {0, 1, ..., 9}를 사용합니다. 언급의 수와 위치에 따라 벡터는 p 벡터에 0이 아닌 여러 값을 포함할 수 있습니다. 예를 들어 엔터티 e가 첫 번째 데실수에 1개, 두 번째 데실수에 2개, 다섯 번째 데실수에 1개 언급이 있는 경우 위치 인코더에 대한 입력은 pm [1, 1, 0, 0, 1, 0, 0, 0, 0]이 됩니다. 각 데실수에서 pm의 언급 수를 캡처하지 않는다는 점에 유의하세요. 위치 임베딩을 얻기 위해 위치 인덱스를 차원 dmodel의 밀집 벡터에 매핑하는 임베딩 계층을 사용합니다. 공식적으로는 hpe(m) 임베딩(pm)입니다. 점수 매기기 [CLS] 토큰의 출력 표현은 평균 위치 임베딩 벡터 hpe와 연결되어 엔티티 e에 대한 두드러짐 점수 &amp;(e) = [0, 1]을 생성하는 점수 매기기 모듈에 입력됩니다. 두드러짐 점수 매기기는 시그모이드 점수 매기기 함수 헤드가 있는 피드포워드 네트워크입니다. 형식적으로 = (e) = σ(FFN(h[CLS]||hpe)) 4.2 최적화 기준 진실 이진 두드러짐 레이블과 예측 두드러짐 점수 &amp;(e)를 사용하여 계산된 이진 교차 엔트로피 손실을 최소화하여 위에서 설명한 모델을 미세 조정합니다. 5 데이터 세트 이 섹션에서는 4개의 데이터 세트로 구성된 엔터티 두드러짐 벤치마크를 설명합니다.두 개의 데이터 세트 NYT-Salience WN-Salience SEL EntSUM # 문서 110,6, 문서 길이(평균 문자) 5,2,1,4,# 고유 엔터티 179,23,6,7,# 언급 4,405,145,19,729 20,% 두드러진 엔터티 14% 27% 10% 39% 실제 인간 추상 정렬 범주 정렬 인간 표 1: 실험에 사용된 데이터 세트에 대한 요약 통계 및 레이블 수집 방법.데이터 세트는 반자동화된 방법을 사용하여 큐레이션되었고 두 개의 인간 주석이 사용되었습니다.표 1에 이러한 데이터 세트와 레이블 수집 방법에 대한 요약 통계가 나와 있습니다.NYT-Salience 이 데이터 세트는 (Dunietz and Gillick, 2014)에서 소개되었으며 엔터티 두드러짐 감지를 위한 현재까지 가장 큰 데이터 세트입니다. 데이터 세트는 NYT Corpus(Sandhaus, 2008)의 뉴스 기사 초록에서 중요한 엔터티가 언급된다는 가정 하에 큐레이션되었습니다. 엔터티와 해당 언급은 POS 태그, 종속성 구문 분석 및 명사구 추출을 포함하는 기존 NLP 파이프라인을 사용하여 식별됩니다. 대규모임에도 불구하고 자동 데이터 세트 생성 프로세스는 데이터 하위 집합에 대한 인간 주석자와의 적당한 일치 수치로 입증된 대로 노이즈를 도입할 수 있습니다. 데이터 세트에는 각 엔터티에 대한 이진 중요도 레이블이 포함되어 있습니다. WN-Salience (Wu et al., 2020a)에 도입된 이 데이터 세트는 Wikinews 기사로 구성된 또 다른 자동 큐레이션 데이터 세트입니다. 여기에는 작성자가 Wikinews 범주로 주석을 달았습니다. WN-Salience는 엔터티에 해당하는 Wikinews 범주가 기사의 범주로 레이블이 지정된 경우 엔터티가 중요하다는 가설을 사용하여 중요한 엔터티를 식별합니다. NYT-Salience와 유사하게 이 데이터 세트에는 이진 중요도 레이블이 있습니다. SEL 이는 (Trani et al., 2018)이 공개한 Wikinews 기반의 또 다른 데이터 세트입니다. 그러나 WNSalience와 달리 이 데이터 세트는 여러 명의 인간 주석자가 엔터티의 두드러짐을 4가지 범주 중 하나로 순위를 매긴 인간 주석입니다. 다른 데이터 세트의 이진 레이블과 일치하도록 4가지 범주를 {0, 1}의 이진 레이블로 매핑합니다. 즉, 아래 두 클래스를 두드러지지 않음으로, 위 두 클래스를 두드러짐으로 매핑합니다. EntSUM 이 데이터 세트는 (Maddela et al., 2022)에 도입되었습니다. 이 데이터 세트를 구성하기 위해 NYT 코퍼스의 693개 기사 하위 집합에 걸쳐 무작위로 선택된 엔터티 집합에 인간 주석자가 [0,3] 사이의 4단계 척도로 두드러짐 레이블을 지정했습니다. 각 문서 엔터티 쌍에 대해 두 개의 독립적인 주석을 수집했으며, 의견이 일치하지 않는 경우 최대 5개까지 늘렸습니다. 엔터티의 평균 주석 점수가 1.5보다 크면 긍정적인 두드러짐 레이블이 지정됩니다. 5.1 추론된 언급을 통한 데이터 강화 EntSUM을 제외하고, 데이터 세트에는 많은 피처 기반 접근 방식과 위치 임베딩을 계산하는 데 필요한 주석으로 명시적인 엔터티 언급 오프셋이 없습니다.SEL에는 엔터티당 언급 표면 텍스트만 포함되어 있는 반면, NYT-Salience와 WN-Salience는 엔터티의 첫 번째 언급의 시작 및 끝 문자 인덱스(일명 언급 오프셋)만 제공합니다.이를 위해 Flair NER(Akbik et al., 2019)과 패턴 매칭을 결합하여 텍스트 내에서 엔터티의 추가 언급을 추론합니다.SEL의 경우 언급을 사용할 수 있으므로 패턴 매칭 접근 방식을 사용하여 언급의 표면 텍스트를 일치시켜 언급 오프셋을 추론합니다.NYT-Salience 및 WN-Salience의 경우 먼저 Flair NER를 사용하여 텍스트에서 명명된 엔터티의 언급을 식별합니다.이러한 언급을 해당 데이터 세트에 제공된 문서의 각 엔터티의 첫 번째 언급과 일치시키려고 시도합니다. 다른 언급의 표면 텍스트가 첫 번째 언급과 다를 수 있으므로 언급의 표면 텍스트와 엔터티 이름 사이의 중복을 해당 엔터티의 후보 언급으로 추가로 사용합니다.이 접근 방식을 적용하여 텍스트에서 엔터티의 추가 언급과 해당 오프셋을 추론합니다.이 프로세스는 약간의 노이즈를 유발할 수 있지만 이 프로세스를 통해 데이터 세트의 전반적인 품질이 향상됩니다.6 실험 제안된 PLM 기반 방법, 과거 연구에서 사용된 다른 ML 및 휴리스틱 기반 접근 방식, 그리고 명령어 조정 PLM을 사용하여 엔터티 두드러짐 벤치마크에서 실험합니다.6. 데이터 분할 이전 연구(Dunietz 및 Gillick, 2014; Trani et al., 2018; Wu et al., 2020a)는 일관되지 않은(또는 보고되지 않은) 학습/검증/테스트 분할을 사용합니다.NYTSalience 및 WN-Salience 데이터 세트는 학습/테스트 분할(검증 없음)과 함께 제공되는 반면 SEL 데이터 세트는 분할 없이 제공됩니다. 이로 인해 모델 간에 공정한 비교를 통해 이전 작업을 벤치마킹하기 어렵습니다. 이 문제를 극복하기 위해 NYT-Salience와 WN-Salience의 원래 학습 세트를 뉴스 기사의 발행 시간을 기준으로 새로운 학습/검증 세트로 시간적으로 분할하여 보다 현실적인 테스트 설정을 제공합니다(Huang 및 Paul, 2018; Rijhwani 및 Preotiuc-Pietro, 2020). 또한 SEL 및 EntSUM 데이터 세트를 학습/검증/테스트 세트로 시간적으로 분할합니다. 데이터 세트 분할에 대한 자세한 내용은 부록 A에 나와 있습니다. 6.2 기준선 먼저 과거 연구에서 사용된 모든 방법을 나열하고, 해당 방법에 대한 원래 논문의 결과를 보고합니다. • 첫 번째 문장. 문서 본문의 첫 번째 문장에 나타나는 경우 엔터티를 중요한 것으로 분류합니다. (Dunietz 및 Gillick, 2014)와 (Wu et al., 2020a) 모두에서 사용됩니다. • 위치 및 빈도(Dunietz 및 Gillick, 2014). 첫 번째 문장 인덱스와 엔터티의 빈도 특징을 로지스틱 회귀 모델에 공급합니다.• 모든 특징(Dunietz 및 Gillick, 2014). 로지스틱 회귀 모델에 공급된 위치, 빈도 및 PageRank 신호에 기반한 일련의 특징을 사용합니다.• SEL(Trani 등, 2018). sklearn에 구현된 Gradient Boosted Decision Tree 알고리즘에 공급된 위치, 빈도 및 Wikipedia 그래프 통계에 기반한 특징의 조합을 사용합니다(Pedregosa 등, 2011).• SWAT(Ponza 등, 2019). 위에서 설명한 SEL 방법과 유사한 특징 세트를 사용하지만 엔터티 임베딩에 기반한 특징이 추가되었습니다.모든 특징은 XGBoost에 구현된 Gradient Boosted Decision Tree 알고리즘에 공급됩니다(Chen 등, 2015). • 위치 특징(Wu et al., 2020a). 로지스틱 회귀 모델에서 엔터티가 언급된 첫 번째 문장의 인덱스를 특징으로 사용합니다. 이 방법은 (Wu et al., 2020a)의 WN Salience 데이터 세트에서 가장 좋은 결과를 제공합니다. 다음으로, 위의 기준선을 기반으로 하는 일반적인 방법 세트를 다시 구현하여 네 가지 데이터 세트 모두에서 테스트할 수 있도록 합니다. 이렇게 하면 평가가 동일한 실험 설정에서 수행되도록 할 수 있습니다. • 위치 헤드라인. 입력 문서의 헤드라인에 나타나는지 여부에 관계없이 엔터티를 두드러진 것으로 분류합니다. • 위치 헤드라인 및 리드. 문서의 헤드라인이나 문서의 첫 번째 문장(리드 문장)에 나타나는 경우 엔터티를 두드러진 것으로 분류합니다. • • 엔터티 빈도. 주어진 값보다 더 빈번한 경우 엔터티를 두드러진 것으로 분류합니다. 각 데이터 세트에 대해 다른 임계값을 계산하고 가장 좋은 결과를 보고했습니다. 임계값은 부록에서 찾을 수 있습니다. • 특징 및 GBDT. 이 방법은 과거 연구(Dunietz 및 Gillick, 2014; Wu et al., 2020a; Trani et al., 2018; Ponza et al., 2019)에서 가장 일반적인 특징, 즉 엔터티의 첫 번째 문장 인덱스와 엔터티 빈도를 사용하여 LightGBM(Ke et al., 2017)을 사용하여 구현된 GBDT 모델에 공급합니다.• SEL GBDT. (Trani et al., 2018)의 방법을 따르고 sklearn의 GBDT(Pedregosa et al., 2011)를 사용하여 SEL 데이터 세트와 함께 제공된 특징에 대한 모델을 학습합니다.• • • 대상 엔터티 마스킹. 이 방법은 특수 마스크 토큰을 통해 표현된 대상 엔터티 언급을 사용하여 Transformer 기반 인코더(ROBERTabase)에 입력을 공급합니다. 두드러짐 예측은 마스크 토큰 표현을 평균 풀링하고 이를 피드포워드 네트워크에 통과시켜 얻습니다. 제로샷 프롬프팅. 우리는 제로샷 프롬프팅을 사용하여 명령어 조정 LLM을 테스트합니다. 프롬프트는 작업 설명을 소개하고, 그 뒤에 입력 텍스트와 대상 엔터티가 나오고, 예/아니요 질문을 합니다. 그것은 모델이 &#39;예&#39; 또는 &#39;아니요&#39;를 답변으로 생성할 것으로 기대합니다. 이미 많은 NLU 작업 컬렉션에서 명령어 조정이 된 LLM은 프롬프트, 입력 텍스트, 대상 엔터티를 기반으로 답변을 제공하려고 시도합니다. 이 모델 패밀리는 여러 벤치마크에서 견고하고 다재다능한 것으로 입증되었습니다(Chung et al., 2022). 우리는 평가를 위해 Flan-UL2(20B)(Tay et al., 2023)와 LLaMa 2-Chat(7B)(Touvron et al., 2023)을 사용합니다. NYT-Salience WN-Salience 출처 Туре 방법 Р R FP RF(Dunietz and Gillick, 2014) 휴리스틱 첫 번째 문장 59.37.8 46.(Dunietz and Gillick, 2014) ML 위치 및 빈도 59.3 61.3 60.(Dunietz and Gillick, 2014) ML(Ponza et al., 2019) ML 모든 기능 SWAT 60.5 63.5 62.(Wu et al., 2020a) 휴리스틱 첫 번째 문장(Wu et al., 2020a) ML(Wu et al., 2020a) ML 기능 및 GBDT 휴리스틱 위치 헤드라인 휴리스틱 휴리스틱 구현 ML PLM(ROBERTA) PLM(ROBERTA) PLM(DeBERTa) 교차 인코더 모델 PLM(ROBERTA) 교차 인코더, 위치 삽입 가능.PLM(DeBERTA) 교차 인코더, 위치 삽입 가능. 위치적 특징 엔티티 빈도 특징 및 GBDT 대상 엔티티 마스킹 교차 인코더 62.4 66.0 64.56.0 41.0 47.3 47.9 53.2 50.19.0 41.3 26.0 29.39.2 59.7 47.3 29.2 48.1 36.57.5 42.0 48.5 46.1 51.5 48.49.8 55.4 52.5 41.0 60.0 48.53.7 53.3 53.6 37.3 61.9 46.61.0 57.4 59.2 46.64.6 50.2 56.5 57.0 65.4 60.75.9 87.1 81.1 71.8 73.6 72.77.5 87.4 82.1 71.5 78.3 74.78.7 84.2 81.4 71.2 76.7 73.75.9 88.4 81.7 73.3 76.1 74.표 2: NYT-Salience 및 WN-Salience 데이터 세트의 결과. 이러한 데이터 세트의 실제 값은 초록/범주 정렬을 통해 생성되었습니다. 상단 섹션은 원본 논문에서 원래 보고된 결과를 제시합니다. 78.9 42. 위치 헤드라인 및 리드 53.3 49. SEL 소스 Туре 방법 P (Trani et al., 2018) ML SEL (5-fold cross val. 포함) (Ponza et al., 2019) ML 휴리스틱 SWAT (5-fold cross val. 포함) 위치 헤드라인 R F50.0 61.0 52.58.0 64.9 61.Р EntSUM R F26.6 78.4 39.7 60.7 18.5 28. 휴리스틱 위치 헤드라인 및 리드 휴리스틱 엔터티 빈도 구현 ML ML 기능 및 GBDT SEL GBDT 22.13.5 57.8 21.9 48.4 54.0 51.26.6 78.4 39.7 60.7 52.0 56.87.1 35.3 51.2 31.6 39.PLM(ROBERTa) PLM(ROBERTA) 교차 인코더 PLM(DeBERTA) 교차 인코더 대상 엔터티 마스킹 모델 71.1 47.8 57.36.3 13.8 20.0 63.0 41.7 50.51.6 73.6 60.6 65.5 60.6 63.64.1 73.6 68.5 64.9 59.2 61.PLM(ROBERTa) 교차 인코더 위치 포함 emb. 63.0 69.9 66.3 67.5 57.0 61.PLM(DeBERTA) 교차 인코더 위치 포함 67.3 62.4 64.7 72.1 51.5 60.표 3: SEL 및 EntSUM 데이터 세트의 결과. 이러한 데이터 세트의 기준 진실은 인간 주석을 통해 생성되었습니다. 상단 섹션은 원본 논문에서 원래 보고된 결과를 보여줍니다. 6.3 실험 설정 우리는 ROBERTa-base(Liu et al., 2019) 및 DeBERTa-v3-base(He et al., 2023)를 실험을 위한 기본 PLM으로 사용합니다. 이러한 각 기본 모델에 대해 우리는 교차 인코더 모델과 데실 위치 임베딩으로 증강된 교차 인코더 모델을 모두 훈련합니다. 제안된 모델을 훈련하기 위해 우리는 AdamW(Loshchilov and Hutter, 2019)를 최적화 도구로 사용합니다. 우리는 다음 값 집합을 사용하여 학습률에 대한 하이퍼파라미터 검색을 수행합니다: {0.001, 0.0005, 0.0002, 0.0001, 0.00005}. 우리는 검증 세트 성능에 따라 조기 중단을 사용하여 최대 10개 에포크 동안 모델을 학습합니다. 우리는 검증 세트의 성능에 따라 각 데이터 세트에 대해 가장 성능이 좋은 모델 체크포인트를 선택합니다. 표 2와 3에서 우리는 엔티티 두드러짐에 대한 이전 연구에 따라 긍정(주목받는) 클래스에 대한 표준 분류 메트릭(예: 정밀도, 재현율 및 F1)을 사용하여 모델과 기준선의 성능을 보고합니다. 각 Transformer 기반 모델의 학습 및 추론을 위해 32GB GPU 메모리, 4개 CPU 및 128GB 주 메모리가 있는 단일 NVIDIA V100 GPU를 사용합니다. 6.4 결과 표 2와 3에서 섹션 5에서 설명한 4개 데이터 세트에 대한 베이스라인과 제안된 모델의 실험 결과를 제시합니다.특징 기반 방법과의 비교.교차 인코더 모델이 F1 점수에서 모든 베이스라인 모델보다 상당히 우수한 성능을 보이는 것을 관찰했습니다.또한 4개 데이터 세트 중 3개에서 베이스라인과 비교하여 더 나은 정밀도를 제공합니다.공개적으로 사용 가능한 사전 계산된 특징으로 학습된 SEL GBDT 모델이 교차 인코더보다 더 나은 정밀도의 모델을 생성하는 것은 SEL 데이터 세트에 대해서만입니다.교차 인코더로 데실 위치 임베딩을 추가하면 모든 데이터 세트에서 정밀도가 향상되지만 NYT-Salience를 제외한 모든 데이터 세트에서 재현율이 저하되는 것을 관찰했습니다.변환기 기반 모델로 맥락 정보를 활용하는 대상 엔터티 마스킹 접근 방식은 엇갈린 결과를 생성합니다. 전반적으로 이 모델은 SEL을 제외한 모든 데이터 세트에 대해 피처 기반 모델보다 더 나은 정밀도를 얻을 수 있지만, 이 모델은 모든 데이터 세트에서 리콜이 낮아 특히 크로스 인코더 모델과 비교할 때 F1 점수가 상당히 낮습니다. 위치 방법과 GBDT 방법에 대한 우리의 재구현은 이전 연구에서 보고된 성능과 일치합니다. 숫자의 분산은 추론된 언급이 있는 데이터 세트의 풍부함(섹션 5.1)과 실험에 사용된 명시적인 학습/개발/테스트 데이터 분할(섹션 6.1)에 기인할 수 있습니다. 6.5 대규모 언어 모델의 제로 샷 프롬핑 우리는 제로 샷 프롬핑을 사용한 두드러짐 감지 문제를 다음과 같이 공식화합니다. 엔터티 두드러짐 작업과 문서 텍스트의 정의가 주어지면 특정 엔터티가 두드러지는지 여부에 대해 &quot;예&quot; 또는 &quot;아니오&quot;를 생성하도록 모델에 요청합니다. 우리는 Hugging Face 1에서 사용 가능한 두 가지 오픈 소스 모델(Flan-UL2(20B) 및 LLaMa 2-Chat(7B))을 실험했으며, 그 결과를 표 4에 제시합니다. 저희가 아는 한, 이는 엔티티 두드러짐 감지 작업을 위한 명령어 조정 모델의 제로샷 프롬핑에 대한 첫 번째 평가입니다. 70억 개의 매개변수를 가진 LLaMa 2-Chat 모델은 모든 데이터 포인트에 대해 긍정적인 레이블만 생성하기 때문에 의미 있는 결과를 산출하지 못하는 것을 관찰했습니다(따라서 100% 리콜을 관찰했습니다). Flan-UL 모델은 긍정적인 레이블과 부정적인 레이블을 모두 생성할 수 있습니다. 그러나 데이터 세트 전체에서 정확도가 여전히 너무 낮습니다. 부록(섹션 C)에서 이러한 성능의 원인과 구현 세부 정보를 추가로 논의합니다. 전반적으로 이러한 ex&#39;www.huggingface.com 실험은 엔티티 두드러짐 감지가 이 두 모델이 명령어 조정된 다른 작업과 유사하지 않은 고유한 작업임을 시사합니다. 7 분석 이 섹션에서는 모델 동작에 대한 더 많은 통찰력을 얻고 추가 개선을 위한 잠재적인 방안을 이해하기 위해 모델 예측에 대한 분석을 수행합니다. 따라서 모든 엔터티 언급을 추론하는 것의 중요성, 첫 번째 엔터티 언급의 위치, 엔터티 언급 빈도를 포함한 다양한 요인에 따라 성능을 분류합니다.7.1 추론된 언급의 영향 섹션 5.1에서 NYT-Salience 및 WN-Salience 데이터 세트에 대한 엔터티의 추가 언급을 추론했습니다. 엔터티의 여러 언급을 활용하는 최상의 모델의 성능을 문서의 첫 번째 엔터티 언급만으로 학습된 버전과 비교합니다. 이 실험에 대한 구체적인 입력 형식은 부록 B에 나와 있습니다.표 5의 결과는 그렇게 하면 모든 데이터 세트에서 모델의 성능이 일관되게 향상됨을 보여줍니다.특히 가장 큰 데이터 세트인 NYT-Salience의 경우 모델은 27.3 F1 포인트라는 상당한 이득을 얻습니다. 이 실험은 추가 언급으로 데이터 세트를 증강하는 것의 중요성과 모든 엔터티 언급 주변에 존재하는 맥락적 정보를 명시적으로 모델링하는 것의 중요성을 보여줍니다.7.2 첫 번째 언급 위치에 대한 계층화 분석 우리는 이전 연구에서 사용된 가장 인기 있는 피처(Dunietz and Gillick, 2014; Wu et al., 2020a; Trani et al., 2018)에 의존하는 재구현된 기준선인 피처 및 GBDT 모델과 교차 인코더 모델을 비교합니다.표 2와 3의 결과에서 볼 수 있듯이, 다른 피처들 중에서 위치 피처가 두드러짐에 가장 많은 정보를 제공합니다.직감적으로, 엔터티가 헤드라인이나 뉴스 기사의 첫 번째 문장에 언급되면 해당 엔터티가 두드러질 가능성이 높습니다.그림 3은 첫 번째 언급이 문서의 헤드라인이나 첫 번째 문장에 있을 때 모든 모델이 좋은 성과를 보임을 보여줍니다. 우리는 크로스 인코더 모델이 지속적으로 Features &amp; GBDT 모델보다 우수한 성과를 보이고 있으며, 가장 큰 성과는 SEL 및 WN-Salience 데이터 세트에서 관찰된다는 것을 알았습니다. 이 관찰은 교차 인코더 모델이 FFE NYT-Salience WN-Salience SEL EntSUM 모델 PR FP R FР R FР R FCross-encoder(DeBERTa) 77.Flan-UL87.31.LLaMa 2-Chat 14.82.1 71.5 78.3 74.8 64.64.72.4 43.5 30.7 90.1 45.9 16.7 98.3 28.5 27.6 83.6 41.100.0 25.4 27.1 100.0 42.6 9.49 100.0 17.3 19.2 100.0 32.73.6 68.59.2 61임을 나타냅니다.표 4: 성능 비교 LLM의 제로 샷 프롬핑을 사용한 교차 인코더 모델. 모델 Р Р NYT-Salience WN-Salience R FR 첫 번째 언급이 있는 교차 인코더 54.2 57.5 55.8 69.6 80.67.0 69.1 53.2 60. 모든 언급이 있는 교차 인코더 77.5 87.4 82.1 71.5 78.3 74.8 64.1 73.6 68.5 64.9 59.2 61. SEL EntSUM FР R FР R F74.6 59.76. 표 5: 첫 번째 언급만 있는 교차 인코더 모델과 추론된 모든 언급이 있는 교차 인코더 모델의 성능 비교. F1 점수: 헤드라인 + 첫 문장에서 처음 언급됨 1.F1 점수: 최대 시퀀스 길이(512)에서 처음 언급됨 1.1.F1 점수: 최대 시퀀스 길이(512) 밖에서 처음 언급됨 1.00 1.00 1.0.60.84 0.0.83 0.0.84 0.0.80.0.0.0.72 0.0.E 0.82 0.0.76 0.0.0.0.0.0.60.62 0.0.0.E 방법 LightGBM DeBERTa DeBERTa+Decile 0.6 0.59 0.62 0.0.0.46 0.0.0.20.40.0.0.40.20.0.0.0.EntSUM SEL WN-Salience NYT-Salience EntSUM SEL WN-Salience NYT-Salience EntSUM SEL 데이터 집합 데이터 집합 데이터 집합 WN-Salience 0.00 0.00 0.NYT-Salience (a) 언급 위치와 관련된 성능. NYT의 컨텍스트 창 외부에 언급된 내용은 없습니다. F1 점수: 언급 빈도F1 점수: 언급 빈도 2-1.1.0.0.63 0.0.60.0.76 0.0.62 0.0.64 0.0.0.0.0.0.40.0.0.0.0.0.NYT-Salience 1.EntSUM SEL 데이터 세트 F1 점수: 언급 빈도 6-E 0.0.40.0.WN-Salience NYT-Salience 0.88 0.0.0.0.0.75 0.0.73 0.71 0.0.0.60.0.20.0.0.56 0.0.0.EntSUM SEL 데이터 세트 1.0.93 0.95 0.F1 점수: 언급 빈도 &gt;0.92 0.92 0.0.0.0.80.60.0.0.81 0.WN-Salience 0.0.0.0.0.NYT-Salience EntSUM SEL WN-Salience NYT-Salience EntSUM 0.00 0.00 0.SEL WN-Salience 데이터 집합 데이터 집합 방법 LightGBM DeBERTa DeBERTa+Decile (b) 엔터티의 빈도에 대한 성능. SEL 데이터 집합의 테스트 분할에는 문서에서 10개 이상 언급된 엔터티가 없습니다. 그림 3: 모델 및 데이터 집합에 대한 계층화 분석. 이 정보를 기능으로 명시적으로 사용하지 않고도 제목이나 문서의 첫 번째 부분에서 발생하는 언급이 종종 눈에 띄는지 식별하기 위해 컨텍스트를 사용할 수 있습니다. 또한 첫 번째 언급이 PLM의 컨텍스트 창 안이나 밖에 있을 때(여기서는 512개 토큰) 모델의 성능을 조사합니다. 언급이 컨텍스트 창 내부에 있는 경우, 교차 인코더 모델이 지속적으로 기능 및 GBDT 모델보다 우수한 성과를 거두는 것을 관찰했습니다. 언급이 컨텍스트 창 외부에 있는 경우, 모델 예측은 무작위에 가까워지는데, 이는 모델이 언급 주변에 즉각적인 컨텍스트 정보를 가지고 있지 않기 때문에 예상된 것입니다. 더 긴 입력을 처리할 수 있는 모델을 사용하는 것은 이러한 샘플의 개선을 위한 유망한 방향이 될 것입니다(Beltagy et al., 2020). 흥미롭게도, WN-Salience의 경우, 기능 및 GBDT 모델도 첫 번째 토큰 외부에서 상당히 더 나쁜 성과를 거두었습니다. 7.3 언급 빈도에 대한 계층 분석 언급 위치 분석과 유사하게, 우리는 우리의 교차 인코더 모델을 언급 빈도를 입력 기능 중 하나로 사용하는 기능 및 GBDT 모델과 비교합니다. 그림 3은 교차 인코더 모델과 기능 및 GBDT가 엔터티 언급의 다양한 빈도와 어떻게 비교되는지 보여줍니다. 단일 언급이 있는 두드러진 엔터티의 경우, 교차 인코더 모델이 기능 및 GBDT 모델보다 상당히 더 나은 성과를 거두었습니다. 특히, NYT-Salience 데이터 세트의 경우, Features &amp; GBDT 모델은 단일 언급 엔터티를 두드러지게 예측하지 못했습니다. 이 관찰 결과는 교차 인코더 모델이 단순히 언급 빈도를 모델링하는 것이 아니라, 단일 언급이 있는 엔터티의 두드러짐을 결정하기 위해 다른 맥락적 정보를 잠재적으로 활용한다는 것을 나타냅니다. Features &amp; GBDT 모델의 성능은 엔터티당 언급이 많을수록 향상됩니다. 사실, 엔터티당 6-10개의 언급 빈도 범위에서 Features &amp; GBDT 모델은 EntSUM 및 SEL 데이터 세트의 교차 인코더 모델보다 더 나은 성능을 보입니다. 이 관찰 결과는 두드러짐을 결정하기 위해 Features &amp; GBDT 모델이 언급 빈도에 지나치게 의존한다는 것을 나타내지만, 교차 인코더가 이 휴리스틱을 완전히 사용할 수 없다는 것도 나타냅니다. 8
--- CONCLUSION ---
이 논문은 사전 훈련된 언어 모델에 인코딩된 의미적 지식을 활용하여 엔티티 두드러짐을 감지하는 것을 목표로 합니다. 위치 표현을 사용한 Transformer 기반 PLM을 기반으로 하는 교차 인코더 방법을 제안하고, 4개의 서로 다른 데이터 세트(두 개는 인간이 주석을 달고 두 개는 자동으로 큐레이션)에서 여러 ML 기반 방법, 휴리스틱 방법 및 명령어 조정 LLM과 성능을 비교합니다. 모든 실험에서 사전 훈련된 언어 모델을 기반으로 하는 교차 인코더 모델은 다른 모든 방법보다 성능이 뛰어나며, 종종 F-1 점수에서 두 자릿수 이득을 얻습니다. 모델 동작 분석은 언급 빈도, 언급 위치 및 문서 길이가 성능에 미치는 중요한 영향을 보여주며, 향후 작업 영역을 강조합니다. 9 한계점 우리는 영어 문서에서만 두드러짐을 연구했지만, 대상 언어를 포함하는 사전 훈련된 언어 모델이 있는 한 다른 언어에도 직접 적용할 수 있습니다. 우리는 일부 방법에서 추론을 위해 데이터에 주석이 달린 엔티티 언급이나 엔티티 인식 및 엔티티 확인을 통해 추론된 엔티티 언급을 사용합니다. 이 정보는 모든 응용 프로그램에서 추론 시점에 사용할 수 없을 수 있습니다. LLM을 사용한 실험은 제로샷 프롬프트로 제한됩니다. 우리는 모델이 두드러짐 감지 작업을 학습하는 데 도움이 될 수 있는 명령어 튜닝을 실험하지 않았습니다. 마지막으로, 우리는 모델링에서 엔터티와 그 관계에 대한 외부 지식을 사용하지 않습니다. 이는 과거 연구에서 결과를 약간 개선하는 것으로 나타났습니다(Dunietz 및 Gillick, 2014; Trani et al., 2018; Ponza et al., 2019). 우리는 이것을 분석 범위에서 벗어나며 향후 작업의 실행 가능한 방향으로 간주합니다. 10 윤리 성명 우리는 엔터티 두드러짐 감지 작업을 위한 공개적으로 사용 가능한 데이터 세트를 사용합니다. 우리가 사용한 데이터 세트와 사전 학습된 모델은 연구 사용을 허용하는 허가된 라이선스를 가지고 있습니다. 우리는 이 논문에서 논의하는 작업과 관련된 잠재적 위험을 예상하지 않습니다. 참고문헌 Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: 최첨단 NLP를 위한 사용하기 쉬운 프레임워크. NAACL 2019, 2019 북미 컴퓨터 언어학 협회 연례 컨퍼런스(데모), 54-59쪽. Zakariae Alami Merrouni, Bouchra Frikh, Brahim Ouhbi. 2020. 자동 키워드 추출: 조사 및 추세. Journal of Intelligent Information Systems, 54(2):391–424. Iz Beltagy, Matthew E. Peters, Arman Cohan. 2020. Longformer: 긴 문서 변환기. David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003. 잠재 디리클레 할당. J. Mach. Learn. Res., 3(null):993-1022. Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2015. XGBoost: Extreme Gradient Boosting. R 패키지 버전 0.4-2, 1(4):1–4. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv 사전 인쇄본 arXiv:2210.11416. Leon Derczynski, Kalina Bontcheva, and Ian Roberts. 2016. Broad Twitter corpus: 다양한 명명된 엔터티 인식 리소스. COLING 2016의 회의록, 제26회 국제 계산 언어학 컨퍼런스: 기술 논문, 1169-1179쪽, 일본 오사카. COLING 2016 조직 위원회. Milan Dojchinovski, Dinesh Reddy, Tomáš Kliegr, Tomáš Vitvar, Harald Sack. 2016. 엔터티 샐리언스 주석이 있는 크라우드소싱 코퍼스. 제10회 국제 언어 자원 및 평가 컨퍼런스(LREC&#39;16)의 회의록, 3307-3311쪽, 슬로베니아 포르토로지. 유럽 언어 자원 협회(ELRA). Jesse Dunietz와 Daniel Gillick. 2014. 수백만 개의 학습 예제가 있는 새로운 엔터티 샐리언스 작업. 제14차 유럽 지부 회의록, 제2권: 단편 논문, 205-209쪽, 스웨덴 예테보리.Association for Computational Linguistics.Günes Erkan과 Dragomir R. Radev.2004.Lexrank: 텍스트 요약에서 두드러짐으로서의 그래프 기반 어휘 중심성.J. Artif.Int.Res., 22(1):457-479.Michael Gamon, Tae Yano, Xinying Song, Johnson Apacible, Patrick Pantel.2013.문서 관련성 이해 1단계: 두드러진 엔터티 식별.기술 보고서 MSR-TR-2013-73.Pengcheng He, Jianfeng Gao, Weizhu Chen. 2023. DeBERTav3: 그래디언트-분리된 임베딩 공유를 사용한 ELECTRA 스타일 사전 학습을 사용하여 deBERTa 개선. 제11회 학습 표현 국제 컨퍼런스. Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, Gerhard Weikum. 2011. 텍스트에서 명명된 엔터티의 강력한 모호성 해소. 2011년 자연어 처리 경험적 방법 컨퍼런스 회의록, 782-792쪽, 스코틀랜드 에든버러, 영국. 계산 언어학 협회. Ella Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie, Mounica Maddela, Daniel Preotiuc-Pietro. 2022. 추출적 엔터티 중심 요약을 바이인코더를 사용한 문장 선택으로 사용. Association for Computational Linguistics의 아시아 태평양 지부 2차 회의록 및 자연어 처리에 관한 제12회 국제 공동 회의(제2권: 단편 논문), 326-333페이지, 온라인 전용. Association for Computational Linguistics. Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, Ralph Weischedel. 2006. OntoNotes: 90% 솔루션. NAACL의 인간 언어 기술 회의록, Companion Volume: 단편 논문, 57-60페이지, 미국 뉴욕시. Association for Computational Linguistics. Xiaolei Huang, Michael J. Paul. 2018. 문서 분류에서 시간성 조사. 56회 연례 총회 논문집(제2권: 단편 논문), 694-699쪽, 호주 멜버른. Association for Computational Linguistics. Anette Hulth. 2003. 언어적 지식이 더 많아졌을 때 자동 키워드 추출 개선. 2003년 자연어 처리 경험적 방법 컨퍼런스 논문집, 216-223쪽. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. 2017. Lightgbm: 고효율 그래디언트 부스팅 결정 트리. 신경 정보 처리 시스템의 발전, 30. Nikolaos Kolitsas, Octavian-Eugen Ganea, Thomas Hofmann. 2018. 종단 간 신경 엔티티 연결. 제22회 Computational Natural Language Learning 컨퍼런스 논문집, 519-529쪽, 벨기에 브뤼셀. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. 2019. ROBERTa: 강력하게 최적화된 BERT 사전 학습 방법. arXiv 사전 인쇄본 arXiv:1907.11692. Ilya Loshchilov와 Frank Hutter. 2019. 분리된 가중치 감소 정규화. International Conference on Learning Representations. Mounica Maddela, Mayank Kulkarni, Daniel Preotiuc-Pietro. 2022. EntSUM: 엔티티 중심 추출 요약을 위한 데이터 세트. 제60회 연례 총회 의사록(제1권: 장문 논문), 3355-3366쪽, 아일랜드 더블린. Association for Computational Linguistics. Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. 마이크로블로그 게시물에 의미론 추가. 제5회 ACM 국제 웹 검색 및 데이터 마이닝 컨퍼런스 의사록, WSDM &#39;12, 563-572쪽, 미국 뉴욕. Association for Computing Machinery. Rada Mihalcea and Paul Tarau. 2004. TextRank: 텍스트에 질서 부여. 2004년 자연어 처리 경험적 방법 컨퍼런스 의사록, 404-411쪽, 스페인 바르셀로나. Association for Computational Linguistics. Dat Ba Nguyen, Johannes Hoffart, Martin Theobald, and Gerhard Weikum. 2014. Aida-light: High-throughput named-entity disambiguation. 2014년 4월 8일, 한국 서울에서 열린 제23회 국제 월드 와이드 웹 컨퍼런스(WWW 2014)와 공동으로 개최된 웹의 연결 데이터 워크숍 회의록, CEUR 워크숍 회의록 1184권. CEURWS.org. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Python에서의 머신 러닝. Journal of Machine Learning Research, 12:28252830. Marco Ponza, Diego Ceccarelli, Paolo Ferragina, Edgar Meij, Sambhav Kothari. 2021. 뉴스 기사에서 트렌드 엔터티의 맥락화. 웹 검색 및 데이터 마이닝에 대한 제14회 ACM 국제 컨퍼런스의 회의록, 346-354쪽. Marco Ponza, Paolo Ferragina, Francesco Piccinno. 2019. Swat: 텍스트에서 눈에 띄는 위키피디아 엔터티를 감지하는 시스템. Computational Intelligence, 35(4):858-890. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, Zhi Zhong. 2013. OntoNotes를 사용한 강력한 언어 분석을 향해. 제17회 계산 자연어 학습 컨퍼런스의 회의록, 143-152페이지, 불가리아 소피아. 계산 언어학 협회. Shruti Rijhwani와 Daniel Preotiuc-Pietro. 2020. 명명된 엔터티 인식의 시간 정보 분석. 계산 언어학 협회의 제58회 연례 회의의 회의록, 7605-7617페이지, 온라인. 계산 언어학 협회. Evan Sandhaus. 2008. 뉴욕 타임스 주석 코퍼스. 필라델피아 언어 데이터 컨소시엄, 6(12):e26752. Benjamin Strauss, Bethany Toma, Alan Ritter, MarieCatherine De Marneffe, Wei Xu. 2016. w-nut 2016 명명된 엔터티 인식 공유 작업의 결과. 제2회 노이즈가 많은 사용자 생성 텍스트 워크숍(WNUT) 회의록, 138-144쪽. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler. 2023. U12: 언어 학습 패러다임 통합. Ian Tenney, Dipanjan Das, Ellie Pavlick. 2019. BERT가 고전적 NLP 파이프라인을 재발견. 제57회 컴퓨터 언어학 협회 연례 회의록, 4593-4601쪽, 이탈리아 피렌체. 컴퓨터 언어학 협회. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2018. 문맥에서 무엇을 배울까요? 문맥화된 단어 표현에서 문장 구조를 탐색합니다. International Conference on Learning Representations에서. Erik F. Tjong Kim Sang 및 Fien De Meulder. 2003. CoNLL-2003 공유 과제 소개: 언어 독립적 명명된 엔터티 인식. HLT-NAACL 2003에서 열린 제7회 자연어 학습 컨퍼런스 회의록, 142-147쪽. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning 마오, 자비에 마르티네, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 안젤라 팬(Angela Fan), 멜라니 캄바두르(Melanie Kambadur), 샤란 나랑(Sharan Narang), 아우렐리앙 로드리게스(Aurelien Rodriguez), 로버트 스토이닉(Robert Stojnic), 세르게이 에두노프(Sergey Edunov), 토마스 시알롬(Thomas Scialom). 2023. Llama 2: 개방형 기반 및 미세 조정된 채팅 모델. Salvatore Trani, Claudio Lucchese, Raffaele Perego, David E. Losada, Diego Ceccarelli 및 Salvatore Orlando. 2018. Sel: 핵심 엔터티 연결을 위한 통합 알고리즘. Computational Intelligence, 34(1):229. Chuan Wu, Evangelos Kanoulas, Maarten de Rijke, Wei Lu. 2020a. Wn-salience: 엔티티 샐리언스 주석이 있는 뉴스 기사 모음. 제12회 언어 자원 및 평가 컨퍼런스 회의록, 2095-2102쪽. Chuan Wu, Evangelos Kanoulas, Maarten de Rijke. 2020b. 엔티티 패싯 토픽 모델을 사용하여 엔티티 중심 문서 표현 학습. Inf. Process. Manage., 57(3). Chenyan Xiong, Zhengzhong Liu, Jamie Callan, Tie-Yan Liu. 2018. 커널 엔티티 샐리언스 모델링을 통한 더 나은 텍스트 이해 및 검색을 향해. 정보 검색 연구 및 개발에 관한 제41회 국제 ACM SIGIR 컨퍼런스, 575-584쪽. Lingyun Zhao, Lin Li, Xinhao Zheng, Jianwei Zhang. 2021. 온라인 금융 텍스트를 위한 bert 기반 감정 분석 및 주요 엔터티 감지 접근법. 2021년 IEEE 24회 컴퓨터 지원 협력 설계(CSCWD) 국제 컨퍼런스, 1233-1238페이지. IEEE. 부록 A 데이터 세트 분할 세부 정보 표 6에는 섹션 6.1에 설명된 시간 분할 전략을 적용한 후 각 데이터 세트의 학습, 개발 및 테스트 분할이 포함되어 있습니다. 이러한 분할은 모델 학습 및 평가에 사용됩니다. B 실험을 위한 입력 형식 섹션 4.1에 설명된 대로 대상 엔터티(즉, 모델이 두드러짐 레이블을 예측해야 하는 엔터티)가 언급될 때마다 특수 마커 토큰을 추가합니다. 다음에서 예를 제공합니다. 텍스트 Musk가 440억 달러 규모의 Twitter 거래를 완료했습니다. Elon Musk, 세계의 ... 모델 입력 [BE[CLS] Elon Musk [SEP] GIN_ENTITY] Musk [END_ENTITY]가 440억 달러 규모의 Twitter 거래를 완료했습니다.[BEGIN_ENTITY] Elon [END_ENTITY]가 세계의 ... Musk 섹션 7.1에서 보고한 첫 번째 언급에 대한 실험의 경우, 다음 예에서 볼 수 있듯이 첫 번째 언급만 특수 마커 토큰으로 제한됩니다.모델 입력 [CLS] Elon Musk [SEP] [BEGIN_ENTITY] Musk [END_ENTITY]가 440억 달러 규모의 Twitter 거래를 완료했습니다.Elon Musk, 세계의 ... Elon Musk에 대한 두 번째 언급은 마커 토큰으로 제한되지 않는다는 점에 유의하십시오.C LLM의 제로샷 프롬프팅 구현 세부 정보 그림 4와 그림 5는 각각 LLaMa 2-Chat(7B) 및 Flan-UL2(20B) 모델에 사용한 프롬프트를 보여줍니다.표 7에는 생성 매개변수가 나열되어 있습니다. 우리는 이 방법을 사용하여 얻은 비교적 낮은 정확도에 대해 다음과 같은 원인을 추측합니다. • 명령어는 두드러짐 작업 정의를 정의하지만 두드러짐 정의와 일치하도록 참조 예(few-shot prompting)를 제공하지 않습니다. 이로 인해 모델은 문서에서 빈도에 따라 엔터티를 두드러짐으로 식별합니다. 그러나 메모리 부족 문제를 방지하기 위해 프롬프트의 최대 입력 길이를 제한해야 하므로 few-shot prompt를 만드는 것은 어렵습니다. • 우리는 전체 프롬프트가 2048개 토큰 이하가 되도록 문서 텍스트를 잘라서 긴 문서의 끝 부분에 있는 잠재적인 정보를 모두 버립니다. <s>[INST] &lt;</s><SYS> <s>&gt; 엔티티의 두드러짐은 전체 문서 텍스트에 대한 해당 엔티티의 중요성 또는 중심성에 대한 정보를 제공합니다. 다음에서 엔티티와 텍스트가 주어졌을 때, 텍스트 문서가 해당 엔티티에 대한 것이면 &#39;예&#39;, 텍스트가 해당 엔티티에 대한 것이 아니면 &#39;아니요&#39;라고 답해야 합니다. «</s>
"
"--- ABSTRACT ---
Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks. Index Terms— Audio generation, retrieval-information, diffusion model, deep learning, long tail problem 1,
--- INTRODUCTION ---
The landscape of text-to-audio (TTA) generation has been revoluti ized by advancements in diffusion-based generative modelling Bi]. Leveraging powerful backbone models such as CLAP [I] and large language model (LLM) [{4], these models are capable of extracting semantic information and enabling the creation of high-fidelity audio from textual descriptions. In this work, we show that due to the scarcity and diversity of audio training data, bias appears in these state-of-the-art models, leading to significant performance degradation. Figur a statistical analysis conducted on the 327 labels of AudioCaps [5], one of the largest audio-text datasets, indicating a notable imbalance in data distribution. The bottom-left graph of Figure[I shows a sample result of the state-of-the-art model trained with AudioCaps, when giving the prompt “A man is talking then pops the champagne and laughs”, the model could only generate the content for “man talking”, but miss uncommon or complex events such as“champagne popped” then followed by “laugh”. Hence, an inherent limitation is seen due to the constrained scope and variability of the training dataset, where the quality of generated sounds seems largely correlated with their frequency of appearance during training. In this regard, these models can faithfully generate realistic audio clips for common sound events, but they may generate incorrect or irrelevant audio clips when encountering less frequent or unseen sound events. We denote this the long-tailed text-to-audio generation problem, which influences the model performance in diversity and restricts mm Audiocaps 100 120 140 160 180 200 220 240 260 280 300Text prompt: A man is talking then pops the champagne and laughs. Fig. 1. The long-tailed problem in AudioCaps dataset (top). Example audio clips (bottom) generated with the baseline model (left) and Re-AudioLDM (right). the applicability of these models, especially in real-world scenarios. Our motivation is to develop a robust TTA framework that breaks the barrier of imbalanced data and achieves realistic generation on diverse acoustic entities. We propose a novel retrieval-augmented TTA framework to address the long-tailed generation issue. We enhance the state-of-the-art TTA model, AudioLDM i. with a retrieval module, dubbed ReAudioLDM. Specifically, we first use the input text prompt to retrieve relevant references (e.g., text-audio pairs) from dataset (e.g., AudioCaps), and then use a pre-trained audio model and a language model to extract the acoustic and textual features, respectively. These extracted features are then further given to the cross-attention [6] module of the LDM to guide the generation process. The retrieved audio-text pairs serve as supplementary information that helps improve the modelling of low-frequency audio events in the training stage. In the inference stage, the retrieval-augmented strategy also provides references in relation to the text prompt, ensuring a more accurate and faithful audio generation result. We perform extensive experiments on events with different frequencies of occurrence in the dataset. We show that Re-AudioLDM provides a stable performance among a variety of audio entities. It significantly improves the performance for tail classes over the baseline models, demonstrating that it can provide effective alleviation for long-tail TTA issues. Furthermore, as compared with the baseline models, Re-AudioLDM is capable of generating more realistic and complex audio clips, including rare, complex, or even unseen audio events. As the example with the same prompt shown in Figure [I] (bottom), where Re-AudioLDM (bottom-right) can generate both uncommon entities “champagne popped” with a complex structure followed with the sound of “/augh”, achieving a better result than the baseline models with all the required entities and semantic orders correctly. In addition, Re-AudioLDM achieves an FAD score of 1.37, outperforming state-of-the-art TTA models by a large margin. The remainder of this paper is organized as follows. Section] --- --“A bottle of champagne is popped and then poured into a glass” Input prompt Retrieval — h champagne “ Database is popped “Water pure “Some water while a man__ into the glass” talks” glass pure into the Audio & Language Feature AudioMAE VAE Decoder HiFi-GAN TS Output Waveform Fig. 2. The overview structure of Re-AudioLDM introduces the
--- RELATED WORK ---
s of audio generation and retrieval-based models, followed by the details of Re-AudioLDM in Section|3] Section [4] presents the experimental setting and Section [5] shows the results and ablation studies. Conclusions are given in Section|6] 2. RELATED WORK Our work relates to two major works, diffusion-based text-to-audio models and retrieval-based generative models. These two fields are briefly discussed in the following subsections. 2.1. Audio Generation Recent works on audio generation follow an encoder-decoder framework [I|{7J. The model first uses an encoder to encode the information into a latent representation, which can be decompressed into a mel-spectrogram feature. The decoder then uses a variational autoencoder (VAE) and a generative adversarial network (GAN) vocoder to turn such features into waveforms. Liu er al. [8] has used PixelSNAIL [9] as the encoder to represent labels while Iashin and Rahtu applied a GPT2 as the encoder to encode input images. Subsequently, diffusion-based models have been used for latent token generation. Yang et al. replaces the transformer-based encoder with a diffusion-based encoder. Liu et al. [1] uses the CLAP model to obtain embeddings for the input data (audio or text), and uses the Latent Diffusion Model (LDM) as the token generator. Ghosal er al. [4] then further improves this framework by replacing CLAP with LLM [ 2.2. Retrieved Information Aggregation Several studies in the field of image generation have considered leveraging retrieved information. Li et al. [15] extract image features from a training set, and place them in a memory bank which is then used as a parallel input condition for audio generation. Blattmannet et al. [16] present a nearest-neighbours strategy to select related image samples from a neighbourhood area. The KNN-Diffusion uses image features obtained from large-scale retrieval databases during the inference stage to perform new-domain image generation. Chen et al. extend the image-only retrieval into image-text pair retrieval, augmenting both high-level semantics and low-level visual information for a diffusion model. In contrast, no similar works have been done for audio generation, and Re-AudioLDM is the first attempt to introduce retrieved information from a dataset to improve the text-to-audio generation performance. 3. PROPOSED
--- METHOD ---
Similar to previous audio generation works , Re-AudioLDM is a cascaded model including three parts: input embedding, diffusionbased feature generator, and a pipeline to reconstruct the waveform from the latent feature. 3.1. Text and Retrieval Embedding Encoder Re-AudioLDM takes two paralleled inputs: a text input c; as lowlevel semantic information, and a set of text-audio pairs as retrieval augmentation c,. for high-level semantic-audio information. The text embedding E*’ is obtained as: E‘ =f, clap (ce) (dd) which fojap(+) is the CLAP model used for text encoding, as in AudioLDM [I]. The retrieved information c, = [< texti, audio: > ,< textz, audiog >,...,< text,, audio, >] are the top-k neighbours selected through the similarity comparison between the embedding of the target caption and those of the retrieval dataset. Here for each pair, the multi-modal embedding is divided into two groups of concatenation, presented as audio retrieval E”® and text retrieval E"", encoded as: E™ = CAT nae «Sinae (audiog )], 2) E™ = CAT{f,(textr), ..., f5(texti.)] GB) where f,;(-) is a pre-trained TS model for obtaining the text embedding, and f,,,.(-) is a pre-trained AudioMAE model obtaining the embedding of the paired audio. (audioz), .. 3.2. Retrieval-Augmented Diffusion Generator Re-AudioLDM uses LDM as the generator to obtain the intermediate latent token of the target audio. The diffusion model involves two processes, a forward process to gradually add noise into the latent vectors and a reverse process to progressively predict the transition noise of the latent vector in each step. During the forward step, the latent representation zo is transformed into a standard Gaussian distribution z,, with a continuous noise injection: q(2n|Zn—1) = N(2n3 V1 — Bn2n—1, BnI), (4) q(Zn|Z0) = N (Zn; VanZo, (1 — Gn)e) (5) where € denotes the Gaussian noise with an = 1 — Bn controlling the noise level. In the reverse process, LDM learns to estimate the --- --distribution of noise €g in the latent space, given conditions from the text embedding E‘, calculated with equation (1), and the retrieved embedding E”® and E”, calculated with equation an @). respectively. The LDM model applies UNet as the general structure, where the input layer takes the noisy latent vector z,,, text embedding E’, and the time step n as the condition. Then the retrieved information of both text and audio is shared with all the cross-attention blocks within the remaining layers. Employing a re-weighted training objective [22], LDM is trained by: Ln (0) = Ezo,en |e — €0(2n,7, B*, Attn(E™, E™))||> (6) 3.3. VAE Decoder & Hifi-GAN Vocoder Re-AudioLDM utilizes a combination of a VAE and a HiFi-GAN as the general pipeline for reconstructing waveform from the latent feature tokens. During the training stage, VAE learns to encode the mel-spectrogram into the intermediate representation and then decode it back to mel-spectrogram, while Hifi-GAN is trained to convert melspectrogram into waveform. For inference, Re-AudioLDM applies the VAE decoder for mel-spectrogram reconstruction and HiFi-GAN for waveform generation. 4,
--- EXPERIMENT ---
s on events with different frequencies of occurrence in the dataset. We show that Re-AudioLDM provides a stable performance among a variety of audio entities. It significantly improves the performance for tail classes over the baseline models, demonstrating that it can provide effective alleviation for long-tail TTA issues. Furthermore, as compared with the baseline models, Re-AudioLDM is capable of generating more realistic and complex audio clips, including rare, complex, or even unseen audio events. As the example with the same prompt shown in Figure [I] (bottom), where Re-AudioLDM (bottom-right) can generate both uncommon entities “champagne popped” with a complex structure followed with the sound of “/augh”, achieving a better result than the baseline models with all the required entities and semantic orders correctly. In addition, Re-AudioLDM achieves an FAD score of 1.37, outperforming state-of-the-art TTA models by a large margin. The remainder of this paper is organized as follows. Section] --- --“A bottle of champagne is popped and then poured into a glass” Input prompt Retrieval — h champagne “ Database is popped “Water pure “Some water while a man__ into the glass” talks” glass pure into the Audio & Language Feature AudioMAE VAE Decoder HiFi-GAN TS Output Waveform Fig. 2. The overview structure of Re-AudioLDM introduces the related works of audio generation and retrieval-based models, followed by the details of Re-AudioLDM in Section|3] Section [4] presents the experimental setting and Section [5] shows the results and ablation studies.
--- CONCLUSION ---
s are given in Section|6] 2. RELATED WORK Our work relates to two major works, diffusion-based text-to-audio models and retrieval-based generative models. These two fields are briefly discussed in the following subsections. 2.1. Audio Generation Recent works on audio generation follow an encoder-decoder framework [I|{7J. The model first uses an encoder to encode the information into a latent representation, which can be decompressed into a mel-spectrogram feature. The decoder then uses a variational autoencoder (VAE) and a generative adversarial network (GAN) vocoder to turn such features into waveforms. Liu er al. [8] has used PixelSNAIL [9] as the encoder to represent labels while Iashin and Rahtu applied a GPT2 as the encoder to encode input images. Subsequently, diffusion-based models have been used for latent token generation. Yang et al. replaces the transformer-based encoder with a diffusion-based encoder. Liu et al. [1] uses the CLAP model to obtain embeddings for the input data (audio or text), and uses the Latent Diffusion Model (LDM) as the token generator. Ghosal er al. [4] then further improves this framework by replacing CLAP with LLM [ 2.2. Retrieved Information Aggregation Several studies in the field of image generation have considered leveraging retrieved information. Li et al. [15] extract image features from a training set, and place them in a memory bank which is then used as a parallel input condition for audio generation. Blattmannet et al. [16] present a nearest-neighbours strategy to select related image samples from a neighbourhood area. The KNN-Diffusion uses image features obtained from large-scale retrieval databases during the inference stage to perform new-domain image generation. Chen et al. extend the image-only retrieval into image-text pair retrieval, augmenting both high-level semantics and low-level visual information for a diffusion model. In contrast, no similar works have been done for audio generation, and Re-AudioLDM is the first attempt to introduce retrieved information from a dataset to improve the text-to-audio generation performance. 3. PROPOSED METHOD Similar to previous audio generation works , Re-AudioLDM is a cascaded model including three parts: input embedding, diffusionbased feature generator, and a pipeline to reconstruct the waveform from the latent feature. 3.1. Text and Retrieval Embedding Encoder Re-AudioLDM takes two paralleled inputs: a text input c; as lowlevel semantic information, and a set of text-audio pairs as retrieval augmentation c,. for high-level semantic-audio information. The text embedding E*’ is obtained as: E‘ =f, clap (ce) (dd) which fojap(+) is the CLAP model used for text encoding, as in AudioLDM [I]. The retrieved information c, = [< texti, audio: > ,< textz, audiog >,...,< text,, audio, >] are the top-k neighbours selected through the similarity comparison between the embedding of the target caption and those of the retrieval dataset. Here for each pair, the multi-modal embedding is divided into two groups of concatenation, presented as audio retrieval E”® and text retrieval E"", encoded as: E™ = CAT nae «Sinae (audiog )], 2) E™ = CAT{f,(textr), ..., f5(texti.)] GB) where f,;(-) is a pre-trained TS model for obtaining the text embedding, and f,,,.(-) is a pre-trained AudioMAE model obtaining the embedding of the paired audio. (audioz), .. 3.2. Retrieval-Augmented Diffusion Generator Re-AudioLDM uses LDM as the generator to obtain the intermediate latent token of the target audio. The diffusion model involves two processes, a forward process to gradually add noise into the latent vectors and a reverse process to progressively predict the transition noise of the latent vector in each step. During the forward step, the latent representation zo is transformed into a standard Gaussian distribution z,, with a continuous noise injection: q(2n|Zn—1) = N(2n3 V1 — Bn2n—1, BnI), (4) q(Zn|Z0) = N (Zn; VanZo, (1 — Gn)e) (5) where € denotes the Gaussian noise with an = 1 — Bn controlling the noise level. In the reverse process, LDM learns to estimate the --- --distribution of noise €g in the latent space, given conditions from the text embedding E‘, calculated with equation (1), and the retrieved embedding E”® and E”, calculated with equation an @). respectively. The LDM model applies UNet as the general structure, where the input layer takes the noisy latent vector z,,, text embedding E’, and the time step n as the condition. Then the retrieved information of both text and audio is shared with all the cross-attention blocks within the remaining layers. Employing a re-weighted training objective [22], LDM is trained by: Ln (0) = Ezo,en |e — €0(2n,7, B*, Attn(E™, E™))||> (6) 3.3. VAE Decoder & Hifi-GAN Vocoder Re-AudioLDM utilizes a combination of a VAE and a HiFi-GAN as the general pipeline for reconstructing waveform from the latent feature tokens. During the training stage, VAE learns to encode the mel-spectrogram into the intermediate representation and then decode it back to mel-spectrogram, while Hifi-GAN is trained to convert melspectrogram into waveform. For inference, Re-AudioLDM applies the VAE decoder for mel-spectrogram reconstruction and HiFi-GAN for waveform generation. 4, EXPERIMENTS 4.1. Datasets We use the AudioCaps dataset for the experiments, which comprises 46,000 ten-second audio clips, each paired with a humanannotated caption. We follow the official training-testing split, where each training audio clip is assigned a single caption, while in the testing split, each audio clip is annotated with five captions. During the inference stage, we employ the first caption of each audio clip that appears in the test split as the text input. The remaining four captions are used only for the ablation study in Sectio 4.2. Experiment Setup Data Preparation. For a retrieval-based AudioCaps dataset, we apply a CLAP-score based retrieval function to find the top-50 nearest neighbours of the target text embedding. The waveform and the text from each neighbour are stored as a text-audio pair. It is noted that for both training and testing samples, the target sample is excluded from the retrieval information, which can avoid any access to the target data during both the training and inferencing stages. Implementation Detail. As a cascaded model, the encoder and decoder parts of Re-AudioLDM are trained separately with audio clips sampled at 16 kHz. For the target, we use the short-time Fourier transform (STFT) with a window of 1024 samples and a hop size of 160 samples, resulting in a mel-spectrogram with 64 mel-filterbanks. Then, a VAE model is applied to compress the spectrogram with a ratio of 4, resulting in a feature vector with a frequency dimension of 16. For the information provided by the retrieval strategy, the text feature is directly extracted by a pre-trained T5-medium model, presenting a fixed sequence length of 50. The audio feature, on the other hand, is first converted into filter banks with 128 mel-bins and then processed by a pre-trained AudioMAE model, leading to a vector of dimension 32. Training Detail. The LDM is optimized with a learning rate of 5.0 x 10-°. Re-AudioLDM is trained for up to 80 epochs with a batch size of 4 and the evaluation is carried out every 100,steps. Re-AudioLDM-S applies a UNet architecture consisting of 128 channels, while we enlarge the model into Re-AudioLDM-L with 196 channels for experiments on more complex models. Evaluation Metrics. Following Liu et al., we use the Inception Score (IS), Fréchet Audio Distance (FAD), and Kullback—Leibler (KL) divergence to evaluate the performance of Re-AudioLDM. A higher IS score indicates a larger variety in the generated audio, while lower KL and FAD scores indicate better audio quality. For the semantic-level evaluation, we calculate the cosine similarity between the output audio embedding and the target text embedding calculated by the CLAP encoders, which demonstrates the correlation between audio and text. 5. RESULTS 5.1. Evaluation Results The experiments are carried out on AudioCaps evaluation set. We compare the performance with several state-of-the-art frameworks, including AudioGen . Selecting only the first caption of each audio clip as the text description, each framework infers 975 10-second audio clips with a sampling rate of 16 kHz. Table[T]compares the metrics achieved with different textto-audio models, where Re-AudioLDM outperforms other methods by a large margin. It is noted that without the additional information provided by retrieval, Re-AudioLDM does not exhibit any advantage and is generally inferior to Tango, the current state-of-the-art model on AudioCaps. However, upon incorporating retrieval information, Re-AudioLDM successfully outperformed the baseline models in all four evaluation metrics. By enlarging the size of the hidden layer in LDM structure, the Re-AudioLDM-L using 10 retrieved pairs further decreases the FAD score to below 1.4, which is a significant improvement over the baseline frameworks. 5.2. Ablation Study Retrieval Type. Experiments in Table[I]show the results on different retrieval information, e.g. audio, text, or neither. With the audio features extracted by AudioMAE, only a slight improvement is achieved by Re-AudioLDM, mainly because Re-AudioLDM misses the relationship between sound events, although it captures the features of related sound events. By adding the paired text information of each retrieved audio clip, Re-AudioLDM learns the relationship between audio features and high-level semantic information, contributing a significant improvement in capturing highly related semantic features for the sound events. Number of Retrieved Pairs. Several experiments are carried out to assess the impact of the number of retrieved audio-text pairs on audio generation performance. As depicted in Figure|3} the incorporation of retrieval information improves the performance, as the number of retrieved pairs increases, while such improvement slows down after the number reaches five and it becomes flattened at around ten. Therefore, in order to strike a balance between training costs and model performance, the number of retrieved pairs is chosen empirically to be in the range of 3 to 5 for this data. Long-Tailed Situations. Re-AudioLDM aims to tackle the longtailed generation problem and generate more realistic audio clips on uncommon or unseen sound events. In order to evaluate the accuracy of each generated audio clip, we applied the CLAP score show the relationship between the audio clip and text description. We first calculate the frequency of the occurrence of each sound event by counting the label of each audio clip and then illustrate the model performance by averaging the CLAP score of each sound class for --- --Model Dataset Retrieval Info Retrieval Number KL IS? FADJ CLAPscore(%)t AudioGen AC+AS48 others x x 1.69 5.13 2.15 23.AudioLDM {i} | AC+AS+2 others xK x 1.66 6.51 2.08 25.Tango AudioCaps xK xK 1.32 645 1.68 29.AudioCaps x x 1.63 648 2.31 26.Re-AudioLDMS —AitioCaps Audio & Text 3 iy 731AudioCaps Audio & Text 10 1.23 7.33 1.40 37.Re-AudioLDM-L AudioCaps Audio & Text 10 120 7.39 1.37 37.Table 1. The comparison between different frameworks, with and without retrieval information. AC and AS are short for AudioCaps [5] an AudioSet [24] respectively. KL Is FAD A — AC_capl 2.4 — AC capi —— AC_cap2 | 7.2 —— AC_cap16 — AC_cap3 2.2 — AC_cap— Accap4 |7.9 20 — AC_cap15 — AC_cap5 — AC_capl — AC_cap— Ac.cap2 Jig 14 68 — AC_cap=13 AG ele "" SPO lag 0 5 10 0 5 10 0 5Fig. 3. Performance comparison on numbers of retrieved information, where AC_cap 1-5 refers to the caption groups of the testing set. the AudioCaps testing set. The bar chart on the left side of Figure/4] presents a statistical analysis of the quantities of all 327 sound event classes in the AudioCaps training set. Similar to Figure[T] (top), tail classes constitute a significant portion, especially in the label group of 1 and 10. Figure|4](right) shows the performance of each model on the events with different frequencies of event occurrence within the training set. Despite the initial gap in highly frequent audio events between Re-AudioLDM and baseline models, the baseline models perform worse when handling tailed entities. However, ReAudioLDM has demonstrated more stable results, with a decrease of less than 3 in the CLAP score as the frequency of event occurrence is reduced in training data. Hence, Re-AudioLDM can reduce the degradation of output quality when generating tailed sound events, enhancing the overall model performance. Zero-Shot Generation. For experiments on unseen entities, we evaluate several scenarios with events that are excluded during training. In Figure[4](right), we found that baseline models show performance degradation on generating unseen audio (zero frequency occurrence). This may be because the model has not learned the features of unseen entities, while Re-AudioLDM can still achieve realistic results by providing related audio and semantic information. Hence, with essential retrieval information, Re-AudioLDM has the potential to generate sounds which are excluded from training data. The retrieval-based generation may significantly enhance the robustness of zero-shot tasks, which is one of the directions we will explore in the future. Comparison with Mixup Strategy. Another way to address the class imbalance problem is to use a mixup strategy [26]. While mixup can increase the occurrence frequency for the tail entities, it also introduces more complex audio examples, as well as the synthetic audio data that may not align with real-world distributions. The results in [1] have shown that the mixup strategy leads to degradation in overall performance. In contrast to the mixup method, our proposes retrieval augmentation strategy reduces the complexity of the training processes, resulting in an overall performance improvement. 100AO 3s oo 3 0 v s 5: & 4.8 — Audio 5 55 — tango 2 2 TS peanses = heansao 20 = peace Fy 1 10 30 50 100 500 1000 1 10 30 50 100Frequency (number) Frequency (number) Fig. 4. Performance on different frequency entities, where S and L indicate model size and r refers to the number of retrieved clips. 6. CONCLUSION In this paper, we have presented a retrieval-augmented model, ReAudioLDM, to tackle the long-tailed problem in AudioCaps. The comparisons with current state-of-the-art models (i.e., AudioLDM and Tango) using several performance metrics (i.e., FAD, and CLAPscore) demonstrate that Re-AudioLDM can significantly enhance the performance of TTA models in generating high-fidelity audio clips. By integrating retrieved features, Re-AudioLDM not only achieves improvements in overall performance, but enables the generation of rare or unseen sound entities. In future work, we will investigate the model with external large datasets and explore the potential of the model in downstream tasks, such as zero-shot generation. 7. ACKNOWLEDGMENT This research was partly supported by a research scholarship from the China Scholarship Council (CSC), funded by British Broadcasting Corporation Research and Development (BBC R&D), Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 “AI for Sound”, and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. --- --ReferencesH. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, “AudioLDM: Text-to-Audio generation with latent diffusion models,” in International Conference on Machine Learning, 2023. R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, “Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,” in International Conference on Machine Learning, 2023. X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, M. D. Plumbley, and W. Wang, “Wavjourney: Compositional audio creation with large language models,” arXiv preprint\arXiv:2307. 14335) 2023. D. Ghosal, N. Majumder, A. Mehrish, and S. Poria, “Textto-audio generation using instruction tuned LLM and latent diffusion model,” arXiv preprint arXiv:2304.13731\ 2023. C. D. Kim, B. Kim, H. Lee, and G. Kim, “AudioCaps: Generating captions for audios in the wild,” in Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in Neural Information Processing Systems, vol. 30, 2017. Y. Yuan, H. Liu, J. Liang, X. Liu, M. D. Plumbley, and W. Wang, “Leveraging pre-trained audioldm for sound generation: A benchmark study,” in European Association for Signal Processing, 2023. X. Liu, T. Iqbal, J. Zhao, Q. Huang, M. Plumbley, and W. Wang, “Conditional sound generation using neural discrete time-frequency representation learning,” JEEE International Workshop on Machine Learning for Signal Processing, 2021. X. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel, “PixelSNAIL: An improved autoregressive generative model,” in International Conference on Machine Learning, 2018, pp. 864872. V. Iashin and E. Rahtu, “Taming visually guided sound generation,” in British Machine Vision Conference, 2021. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019. D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, “Diffsound: Discrete diffusion model for text-to-sound generation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable visual models from natural language supervision,” in International Conference on Machine Learning, 2021, pp. 8748-8763. [14][25] [26] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485— 5551, 2020. B. Li, P. H. Torr, and T. Lukasiewicz, “Memory-driven textto-image generation,” in British Machine Vision Conference, 2022. A. Blattmann, R. Rombach, K. Oktay, J. Miiller, and B. Ommer, “Retrieval-augmented diffusion models,” Advances in Neural Information Processing Systems, vol. 35, pp. 15 309-15 324, 2022. S. Sheynin, O. Ashual, A. Polyak, U. Singer, O. Gafni, E. Nachmani, and Y. Taigman, “KNN-diffusion: Image generation via large-scale retrieval,” in International Conference on Learning Representations, 2023. W. Chen, H. Hu, C. Saharia, and W. W. Cohen, “Re-Imagen: Retrieval-augmented text-to-image generator,” in Jnternational Conference on Learning Representations, 2023. Y. Yuan, H. Liu, X. Kang, P. Wu, M. D. Plumbley, and W. Wang, “Text-driven foley sound generation with latent diffusion model,” in Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop, 2023, pp. 231-235. Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnoy, “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2023. H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer et al., “Masked autoencoders that listen,’ arXiv preprint:2207.06405, 2022. J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Neural Information Processing Systems, 2020. F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, “AudioGen: textually guided audio generation,” in International Conference on Learning Representations, 2023. J. EF Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “AudioSet: An ontology and human-labeled dataset for audio events,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017, pp. 776-780. T. Heittola, A. Mesaros, and T. Virtanen, “TAU Urban Acoustic Scenes 2019, Development dataset,’ Mar. 2019. [Online]. Available: |https://doi.org/10.528 1/zenodo.Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,” JEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880-2894, 2020.
"	"--- ABSTRACT ---
최근 텍스트-오디오(TTA) 생성에서 진전이 있었음에도 불구하고, AudioCaps와 같은 불균형 클래스 분포를 가진 데이터 세트에서 학습된 AudioLDM과 같은 최첨단 모델은 생성 성능에 편향되어 있음을 보여줍니다. 구체적으로, 이러한 모델은 일반적인 오디오 클래스를 생성하는 데는 뛰어나지만 드문 클래스에서는 성능이 떨어져 전체 생성 성능이 저하됩니다. 이 문제를 롱테일 텍스트-오디오 생성이라고 합니다. 이 문제를 해결하기 위해 TTA 모델에 대한 간단한 검색 증강 접근 방식을 제안합니다. 구체적으로, 입력 텍스트 프롬프트가 주어지면 먼저 대조 언어 오디오 사전 학습(CLAP) 모델을 활용하여 관련 텍스트-오디오 쌍을 검색합니다. 그런 다음 검색된 오디오-텍스트 데이터의 기능을 추가 조건으로 사용하여 TTA 모델의 학습을 안내합니다. 제안된 접근 방식으로 AudioLDM을 향상시키고 결과적으로 증강된 시스템을 Re-AudioLDM이라고 합니다. AudioCaps 데이터 세트에서 Re-AudioLDM은 1.37의 최첨단 Frechet Audio Distance(FAD)를 달성하여 기존 접근 방식보다 큰 차이로 성능이 뛰어납니다. 또한 Re-AudioLDM은 복잡한 장면, 희귀한 오디오 클래스, 심지어 보이지 않는 오디오 유형에 대해서도 사실적인 오디오를 생성할 수 있음을 보여주어 TTA 작업에서 잠재력을 보여줍니다. 색인 용어 오디오 생성, 검색 정보, 확산 모델, 딥 러닝, 롱테일 문제 1.
--- INTRODUCTION ---
텍스트-오디오(TTA) 생성의 풍경은 확산 기반 생성 모델링의 발전으로 혁신되었습니다[1, 2, 3]. CLAP[1] 및 대규모 언어 모델(LLM)[4]과 같은 강력한 백본 모델을 활용하여 이러한 모델은 의미 정보를 추출하고 텍스트 설명에서 고충실도 오디오를 생성할 수 있습니다. 이 연구에서 우리는 오디오 학습 데이터의 부족과 다양성으로 인해 이러한 최첨단 모델에 편향이 나타나 성능이 크게 저하된다는 것을 보여줍니다. 그림 1(위)은 가장 큰 오디오-텍스트 데이터 세트 중 하나인 AudioCaps[5]의 327개 레이블에 대해 수행한 통계 분석을 나타내며, 데이터 분포에 상당한 불균형이 있음을 나타냅니다. 그림 1의 왼쪽 아래 그래프는 AudioCaps로 학습한 최신 모델의 샘플 결과를 보여줍니다. &quot;남자가 말하고 샴페인을 터뜨리고 웃음&quot;이라는 프롬프트를 제공했을 때 모델은 &quot;남자가 말하고 있음&quot;에 대한 콘텐츠만 생성할 수 있었지만 &quot;샴페인이 터짐&quot;에 이어 &quot;웃음&quot;과 같은 흔하지 않거나 복잡한 이벤트는 놓쳤습니다. 따라서 학습 데이터 세트의 범위와 가변성이 제한되어 내재적인 한계가 보이는데, 생성된 사운드의 품질은 학습 중에 나타나는 빈도와 크게 상관 관계가 있는 것으로 보입니다. 이와 관련하여 이러한 모델은 일반적인 사운드 이벤트에 대해 사실적인 오디오 클립을 충실하게 생성할 수 있지만 덜 빈번하거나 보이지 않는 사운드 이벤트가 발생하면 잘못되거나 관련성이 없는 오디오 클립을 생성할 수 있습니다. 이를 롱테일 텍스트-오디오 생성 문제로 표시하며, 이는 다양성 측면에서 모델 성능에 영향을 미치고 빈도(숫자)를 제한합니다.40남자가 말하고 있음 AudioCaps 100 120 140 160 180 200 220 240 260 280 300---웃음 텍스트 프롬프트: 한 남자가 말을 하다가 샴페인을 터뜨리고 웃는다.샴페인을 터뜨리다 그림 1. AudioCaps 데이터 세트의 롱테일 문제(위).베이스라인 모델(왼쪽)과 Re-AudioLDM(오른쪽)으로 생성된 예시 오디오 클립(아래).특히 실제 시나리오에서 이러한 모델의 적용 가능성.저희의 동기는 불균형 데이터의 장벽을 깨고 다양한 음향 엔터티에서 현실적인 생성을 달성하는 강력한 TTA 프레임워크를 개발하는 것입니다.롱테일 생성 문제를 해결하기 위해 새로운 검색 증강 TTA 프레임워크를 제안합니다.최신 TTA 모델인 AudioLDM [1]을 ReAudioLDM이라는 검색 모듈로 향상시킵니다. 구체적으로, 먼저 입력 텍스트 프롬프트를 사용하여 데이터 세트(예: AudioCaps)에서 관련 참조(예: 텍스트-오디오 쌍)를 검색한 다음, 사전 훈련된 오디오 모델과 언어 모델을 사용하여 각각 음향 및 텍스트 특징을 추출합니다. 이러한 추출된 특징은 생성 프로세스를 안내하기 위해 LDM의 교차 주의[6] 모듈에 추가로 제공됩니다. 검색된 오디오-텍스트 쌍은 훈련 단계에서 저주파 오디오 이벤트의 모델링을 개선하는 데 도움이 되는 보충 정보 역할을 합니다. 추론 단계에서 검색 증강 전략은 또한 텍스트 프롬프트와 관련된 참조를 제공하여 보다 정확하고 충실한 오디오 생성 결과를 보장합니다. 데이터 세트에서 발생 빈도가 다른 이벤트에 대한 광범위한 실험을 수행합니다. Re-AudioLDM이 다양한 오디오 엔터티에서 안정적인 성능을 제공함을 보여줍니다. 기준 모델에 비해 테일 클래스의 성능을 크게 향상시켜 롱테일 TTA 문제에 대한 효과적인 완화를 제공할 수 있음을 보여줍니다. 또한 기준 모델과 비교했을 때 Re-AudioLDM은 희귀하고 복잡하거나 보이지 않는 오디오 이벤트를 포함하여 더욱 사실적이고 복잡한 오디오 클립을 생성할 수 있습니다. 그림(아래)에 표시된 동일한 프롬프트의 예에서 Re-AudioLDM(오른쪽 아래)은 복잡한 구조로 &quot;샴페인 터짐&quot;이라는 흔하지 않은 엔터티와 &quot;웃음&quot;이라는 소리를 모두 생성할 수 있어 모든 필수 엔터티와 의미적 순서를 올바르게 갖춘 기준 모델보다 더 나은 결과를 달성했습니다. 또한 Re-AudioLDM은 1.37의 FAD 점수를 달성하여 최첨단 TTA 모델보다 훨씬 우수한 성과를 거두었습니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 섹션 &quot;샴페인 한 병을 터뜨린 다음 유리잔에 붓습니다.&quot; 입력 프롬프트 데이터베이스 CLAP 인코더 검색 오디오 및 언어 기능 LDM 교차 주의 I AudioMAE 오디오 기능 VAE 디코더 HiFi-GAN &quot;샴페인을 터뜨리는 동안 한 남자가 &quot;물을 유리잔에 붓습니다.&quot; &quot;물 약간 Ttalks&quot; 유리잔에 물을 붓습니다.&quot; 언어 기능 출력 파형 그림 2. Re-AudioLDM의 개요 구조는 다음을 소개합니다.
--- RELATED WORK ---
오디오 생성 및 검색 기반 모델에 대한 자세한 내용은 3장에서 설명하고, Re-AudioLDM에 대한 세부 정보는 4장에서 다룹니다. 4장에서는 실험 설정을 제시하고 5장에서는 결과와 절제 연구를 보여줍니다. 결론은 6장에서 제공합니다. 2. 관련 연구 저희의 연구는 확산 기반 텍스트-오디오 모델과 검색 기반 생성 모델이라는 두 가지 주요 연구와 관련이 있습니다. 이 두 분야는 다음 하위 섹션에서 간략하게 설명합니다. 2.1. 오디오 생성 오디오 생성에 대한 최근 연구는 인코더-디코더 프레임워크를 따릅니다[1, 7]. 이 모델은 먼저 인코더를 사용하여 정보를 잠재적 표현으로 인코딩한 다음, 이를 멜 스펙트로그램 특징으로 압축 해제할 수 있습니다. 그런 다음 디코더는 변분 자동 인코더(VAE)와 생성적 적대 네트워크(GAN) 보코더를 사용하여 이러한 특징을 파형으로 변환합니다. Liu et al. [8]은 레이블을 나타내는 인코더로 PixelSNAIL [9]을 사용한 반면 Iashin과 Rahtu [10]는 입력 이미지를 인코딩하는 인코더로 GPT2 [11]를 적용했습니다. 이후, 잠재 토큰 생성을 위해 확산 기반 모델이 사용되었습니다. Yang et al. [12]은 변압기 기반 인코더를 확산 기반 인코더로 교체했습니다. Liu et al. [1]은 CLAP 모델 [13]을 사용하여 입력 데이터(오디오 또는 텍스트)에 대한 임베딩을 얻고 잠재 확산 모델(LDM)을 토큰 생성기로 사용했습니다. Ghosal et al. [4]는 CLAP을 LLM [14]으로 교체하여 이 프레임워크를 더욱 개선했습니다. 2.2. 검색된 정보 집계 이미지 생성 분야의 여러 연구에서 검색된 정보를 활용하는 것을 고려했습니다. Li et al. [15]은 학습 세트에서 이미지 특징을 추출하여 메모리 뱅크에 넣은 다음 오디오 생성을 위한 병렬 입력 조건으로 사용합니다. Blattmannet et al. [16]은 이웃 지역에서 관련 이미지 샘플을 선택하기 위한 최근접 이웃 전략을 제시합니다. KNN-Diffusion [17]은 추론 단계에서 대규모 검색 데이터베이스에서 얻은 이미지 특징을 사용하여 새로운 도메인 이미지 생성을 수행합니다. Chen et al. [18]은 이미지 전용 검색을 이미지-텍스트 쌍 검색으로 확장하여 확산 모델에 대한 고수준 의미론과 저수준 시각 정보를 모두 보강합니다. 이와 대조적으로 오디오 생성에 대한 유사한 작업은 수행되지 않았으며 Re-AudioLDM은 텍스트-오디오 생성 성능을 개선하기 위해 데이터 세트에서 검색된 정보를 도입한 최초의 시도입니다. 3. 제안
--- METHOD ---
이전 오디오 생성 작업[1, 4, 19]과 유사하게 Re-AudioLDM은 입력 임베딩, 확산 기반 기능 생성기, 잠재 기능에서 파형을 재구성하는 파이프라인의 세 부분으로 구성된 계단식 모델입니다.3.1. 텍스트 및 검색 임베딩 인코더 Re-AudioLDM은 두 개의 병렬 입력을 사용합니다.저수준 의미 정보인 텍스트 입력 ct와 고수준 의미-오디오 정보를 위한 검색 증강 cr인 텍스트-오디오 쌍 세트입니다.텍스트 임베딩 Et는 다음과 같이 얻습니다.Et = fclap (Ct) (1) 여기서 flap()은 AudioLDM [1]에서와 같이 텍스트 인코딩에 사용되는 CLAP 모델 [20]입니다.검색된 정보 cr. = = [<text₁, audio1 > &lt;text2, audio2&gt;, ..., &lt;text, audiok&gt;]는 대상 캡션의 임베딩과 검색 데이터 세트의 임베딩 사이의 유사성 비교를 통해 선택된 상위 k 이웃입니다.여기서 각 쌍에 대해 멀티모달 임베딩은 오디오 검색 Era와 텍스트 검색 Ert로 표현되는 두 개의 연결 그룹으로 나뉩니다.다음과 같이 인코딩됩니다.Era = CAT rt(audio1), ...,fmae(audiok)], Et CAT(text1), ...,fts(text)] = (2) 여기서 fts()는 텍스트 임베딩을 얻기 위한 사전 학습된 T5 모델[14]이고 fmae(•)는 쌍 오디오의 임베딩을 얻기 위한 사전 학습된 AudioMAE 모델[21]입니다.3.2. 검색 증강 확산 생성기 Re-AudioLDM은 생성기로 LDM을 사용하여 대상 오디오의 중간 잠재 토큰을 얻습니다.확산 모델에는 두 가지 프로세스가 포함됩니다.잠재 벡터에 점진적으로 노이즈를 추가하는 순방향 프로세스와 각 단계에서 잠재 벡터의 전이 노이즈를 점진적으로 예측하는 역방향 프로세스입니다.순방향 단계 동안 잠재 표현 zo는 연속 노이즈 주입과 함께 표준 가우시안 분포 Zn으로 변환됩니다.q(zn|Zn−1) = N(zn; √√1 – BnZn−1, ßnĪ), q(zn|z0) = N(Zn; √ānzo, (1 – ān)€) (4) (5) 여기서 e는 노이즈 레벨을 제어하는 an = 1 - ẞn이 있는 가우시안 노이즈를 나타냅니다. 역 프로세스에서 LDM은 방정식 (1)로 계산된 텍스트 임베딩 Et와 방정식 (2) 및 (3)으로 각각 계산된 검색된 임베딩 Era 및 Ert에서 주어진 조건에 따라 잠재 공간에서 노이즈 e의 분포를 추정하는 방법을 학습합니다.LDM 모델은 일반 구조로 UNet을 적용하며, 여기서 입력 계층은 노이즈가 있는 잠재 벡터 Zn, 텍스트 임베딩 Et 및 시간 단계 n을 조건으로 취합니다.그런 다음 텍스트와 오디오의 검색된 정보는 나머지 계층 내의 모든 교차 주의 블록과 공유됩니다.가중치가 다시 지정된 학습 목표[22]를 사용하여 LDM은 다음과 같이 학습됩니다.Ln(0) = Ezo,e,n ||€ – €0(Zn, n, E², Attn(Era, Ert))||2 (6) 3.3. VAE 디코더 및 Hifi-GAN 보코더 Re-AudioLDM은 잠재 특징 토큰에서 파형을 재구성하기 위한 일반 파이프라인으로 VAE와 HiFi-GAN의 조합을 활용합니다. 학습 단계에서 VAE는 멜 스펙트로그램을 중간 표현으로 인코딩한 다음 멜 스펙트로그램으로 다시 디코딩하는 방법을 학습하는 반면, Hifi-GAN은 멜 스펙트로그램을 파형으로 변환하도록 학습합니다. 추론을 위해 Re-AudioLDM은 멜 스펙트로그램 재구성을 위해 VAE 디코더를 적용하고 파형 생성을 위해 HiFi-GAN을 적용합니다. 4.1. 데이터 세트 4.
--- EXPERIMENT ---
데이터 세트에서 발생 빈도가 다른 이벤트에 대한 s. Re-AudioLDM이 다양한 오디오 엔터티에서 안정적인 성능을 제공함을 보여줍니다. 기준 모델에 비해 테일 클래스의 성능이 크게 향상되어 롱테일 TTA 문제에 대한 효과적인 완화를 제공할 수 있음을 보여줍니다. 또한 기준 모델과 비교할 때 Re-AudioLDM은 드물거나 복잡하거나 보이지 않는 오디오 이벤트를 포함하여 보다 사실적이고 복잡한 오디오 클립을 생성할 수 있습니다. 그림(아래)에 표시된 동일한 프롬프트의 예에서 Re-AudioLDM(오른쪽 아래)은 복잡한 구조로 &quot;샴페인 터짐&quot;이라는 흔하지 않은 엔터티와 &quot;웃음&quot;이라는 소리를 모두 생성할 수 있어 모든 필수 엔터티와 의미 순서를 올바르게 갖춘 기준 모델보다 더 나은 결과를 달성합니다. 또한 Re-AudioLDM은 1.37의 FAD 점수를 달성하여 최첨단 TTA 모델보다 큰 차이로 성능이 우수합니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 섹션 &quot;샴페인 한 병을 터뜨린 다음 유리잔에 붓습니다.&quot; 입력 프롬프트 데이터베이스 CLAP 인코더 검색 오디오 및 언어 기능 LDM 교차 주의 I AudioMAE 오디오 기능 VAE 디코더 HiFi-GAN &quot;샴페인을 터뜨리는 동안 한 남자가 &quot;물을 유리잔에 붓습니다.&quot; &quot;물 약간을 이야기합니다.&quot; 언어 기능 출력 파형 그림 2. Re-AudioLDM의 개요 구조는 오디오 생성 및 검색 기반 모델의 관련 작업을 소개한 다음 섹션 3에서 Re-AudioLDM에 대한 세부 정보를 제공합니다. 섹션 4에서는 실험 설정을 제시하고 섹션 5에서는 결과와 절제 연구를 보여줍니다.
--- CONCLUSION ---
6절에 s가 나와 있습니다.2. 관련 연구 저희의 연구는 확산 기반 텍스트-오디오 모델과 검색 기반 생성 모델이라는 두 가지 주요 연구와 관련이 있습니다.이 두 분야는 다음 하위 섹션에서 간략하게 설명합니다.2.1. 오디오 생성 오디오 생성에 대한 최근 연구는 인코더-디코더 프레임워크[1, 7]를 따릅니다.이 모델은 먼저 인코더를 사용하여 정보를 잠재 표현으로 인코딩한 다음 이를 멜-스펙트로그램 특징으로 압축 해제할 수 있습니다.그런 다음 디코더는 변이 자동 인코더(VAE)와 생성적 적대 네트워크(GAN) 보코더를 사용하여 이러한 특징을 파형으로 변환합니다.Liu et al.[8]은 PixelSNAIL[9]을 인코더로 사용하여 레이블을 나타내는 반면 Iashin과 Rahtu[10]는 GPT2[11]를 인코더로 적용하여 입력 이미지를 인코딩했습니다.그 후 확산 기반 모델이 잠재 토큰 생성에 사용되었습니다.Yang et al. [12]는 변압기 기반 인코더를 확산 기반 인코더로 대체합니다.Liu et al. [1]은 CLAP 모델 [13]을 사용하여 입력 데이터(오디오 또는 텍스트)에 대한 임베딩을 얻고, 토큰 생성기로 Latent Diffusion Model(LDM)을 사용합니다.Ghosal et al. [4]는 CLAP을 LLM [14]으로 대체하여 이 프레임워크를 더욱 개선합니다.2.2. 검색된 정보 집계 이미지 생성 분야의 여러 연구에서 검색된 정보를 활용하는 것을 고려했습니다.Li et al. [15]는 학습 세트에서 이미지 특징을 추출하여 메모리 뱅크에 넣은 다음 오디오 생성을 위한 병렬 입력 조건으로 사용합니다.Blattmannet et al. [16]은 이웃 영역에서 관련 이미지 샘플을 선택하기 위한 최근접 이웃 전략을 제시합니다.KNN-Diffusion [17]은 추론 단계에서 대규모 검색 데이터베이스에서 얻은 이미지 특징을 사용하여 새로운 도메인 이미지 생성을 수행합니다.Chen et al. [18] 이미지 전용 검색을 이미지-텍스트 쌍 검색으로 확장하여 확산 모델을 위해 고수준 의미론과 저수준 시각 정보를 모두 증강합니다. 반면 오디오 생성에 대한 유사한 작업은 수행되지 않았으며 Re-AudioLDM은 텍스트-오디오 생성 성능을 개선하기 위해 데이터 세트에서 검색된 정보를 도입한 최초의 시도입니다. 3. 제안된 방법 이전 오디오 생성 작업[1, 4, 19]과 유사하게 Re-AudioLDM은 입력 임베딩, 확산 기반 기능 생성기 및 잠재 기능에서 파형을 재구성하는 파이프라인의 세 부분으로 구성된 계단식 모델입니다. 3.1. 텍스트 및 검색 임베딩 인코더 Re-AudioLDM은 저수준 의미 정보인 텍스트 입력 ct와 고수준 의미-오디오 정보를 위한 검색 증강 cr인 텍스트-오디오 쌍 세트의 두 가지 병렬 입력을 사용합니다. 텍스트 임베딩 Et는 다음과 같이 얻습니다. Et = fclap (Ct) (1) 여기서 flap()은 AudioLDM [1]에서와 같이 텍스트 인코딩에 사용되는 CLAP 모델 [20]입니다. 검색된 정보 cr. = = [<text₁, audio1 > &lt;text2, audio2&gt;, ..., &lt;text, audiok&gt;]는 대상 캡션의 임베딩과 검색 데이터 세트의 임베딩 사이의 유사성 비교를 통해 선택된 상위 k 이웃입니다.여기서 각 쌍에 대해 멀티모달 임베딩은 오디오 검색 Era와 텍스트 검색 Ert로 표현되는 두 개의 연결 그룹으로 나뉩니다.다음과 같이 인코딩됩니다.Era = CAT rt(audio1), ...,fmae(audiok)], Et CAT(text1), ...,fts(text)] = (2) 여기서 fts()는 텍스트 임베딩을 얻기 위한 사전 학습된 T5 모델[14]이고 fmae(•)는 쌍 오디오의 임베딩을 얻기 위한 사전 학습된 AudioMAE 모델[21]입니다.3.2. 검색 증강 확산 생성기 Re-AudioLDM은 생성기로 LDM을 사용하여 대상 오디오의 중간 잠재 토큰을 얻습니다.확산 모델에는 두 가지 프로세스가 포함됩니다.잠재 벡터에 점진적으로 노이즈를 추가하는 순방향 프로세스와 각 단계에서 잠재 벡터의 전이 노이즈를 점진적으로 예측하는 역방향 프로세스입니다.순방향 단계 동안 잠재 표현 zo는 연속 노이즈 주입과 함께 표준 가우시안 분포 Zn으로 변환됩니다.q(zn|Zn−1) = N(zn; √√1 – BnZn−1, ßnĪ), q(zn|z0) = N(Zn; √ānzo, (1 – ān)€) (4) (5) 여기서 e는 노이즈 레벨을 제어하는 an = 1 - ẞn이 있는 가우시안 노이즈를 나타냅니다. 역 프로세스에서 LDM은 방정식 (1)로 계산된 텍스트 임베딩 Et와 방정식 (2) 및 (3)으로 각각 계산된 검색된 임베딩 Era 및 Ert에서 주어진 조건에 따라 잠재 공간에서 노이즈 e의 분포를 추정하는 방법을 학습합니다.LDM 모델은 일반 구조로 UNet을 적용하며, 여기서 입력 계층은 노이즈가 있는 잠재 벡터 Zn, 텍스트 임베딩 Et 및 시간 단계 n을 조건으로 취합니다.그런 다음 텍스트와 오디오의 검색된 정보는 나머지 계층 내의 모든 교차 주의 블록과 공유됩니다.가중치가 다시 지정된 학습 목표[22]를 사용하여 LDM은 다음과 같이 학습됩니다.Ln(0) = Ezo,e,n ||€ – €0(Zn, n, E², Attn(Era, Ert))||2 (6) 3.3. VAE 디코더 및 Hifi-GAN 보코더 Re-AudioLDM은 잠재 특징 토큰에서 파형을 재구성하기 위한 일반 파이프라인으로 VAE와 HiFi-GAN의 조합을 활용합니다. 학습 단계에서 VAE는 멜 스펙트로그램을 중간 표현으로 인코딩한 다음 다시 멜 스펙트로그램으로 디코딩하는 방법을 학습하는 반면, Hifi-GAN은 멜 스펙트로그램을 파형으로 변환하도록 학습합니다. 추론을 위해 Re-AudioLDM은 멜 스펙트로그램 재구성을 위해 VAE 디코더를 적용하고 파형 생성을 위해 HiFi-GAN을 적용합니다. 4.1. 데이터 세트 4. 실험 실험을 위해 AudioCaps 데이터 세트[25]를 사용하는데, 이는 46,000개의 10초 오디오 클립으로 구성되며 각각에 인간이 주석을 단 캡션이 있습니다. 각 학습 오디오 클립에 단일 캡션이 지정되는 공식 학습-테스트 분할을 따르는 반면 테스트 분할에서는 각 오디오 클립에 5개의 캡션이 주석으로 달립니다. 추론 단계에서는 테스트 분할에 나타나는 각 오디오 클립의 첫 번째 캡션을 텍스트 입력으로 사용합니다.나머지 네 개의 캡션은 섹션 5의 절제 연구에만 사용합니다.4.2 실험 설정 데이터 준비.검색 기반 AudioCaps 데이터 세트의 경우 CLAP 점수 기반 검색 함수를 적용하여 대상 텍스트 임베딩의 상위 50개 가장 가까운 이웃을 찾습니다.각 이웃의 파형과 텍스트는 텍스트-오디오 쌍으로 저장됩니다.교육 및 테스트 샘플 모두에서 대상 샘플은 검색 정보에서 제외되므로 교육 및 추론 단계에서 대상 데이터에 액세스할 수 없습니다.구현 세부 정보.캐스케이드 모델인 Re-AudioLDM의 인코더 및 디코더 부분은 16kHz로 샘플링된 오디오 클립으로 별도로 학습됩니다. 대상의 경우 1024개 샘플의 윈도우와 160개 샘플의 홉 크기를 갖는 단시간 푸리에 변환(STFT)을 사용하여 64개 멜 필터뱅크가 있는 멜 스펙트로그램을 생성합니다. 그런 다음 VAE 모델을 적용하여 스펙트로그램을 4의 비율로 압축하여 주파수 차원이 16인 피처 벡터를 생성합니다. 검색 전략에서 제공하는 정보의 경우 텍스트 피처는 사전 학습된 T5-medium 모델에서 직접 추출하여 고정된 시퀀스 길이 50을 제공합니다. 반면 오디오 피처는 먼저 128개 멜 빈이 있는 필터 뱅크로 변환한 다음 사전 학습된 AudioMAE 모델에서 처리하여 차원 32의 벡터를 생성합니다. 학습 세부 정보. LDM은 5.0 × 105의 학습률로 최적화되었습니다.Re-AudioLDM은 배치 크기 4로 최대 80에포크 동안 학습되고 평가는 100,단계마다 수행됩니다.Re-AudioLDM-S는 128개 채널로 구성된 UNet 아키텍처를 적용하는 반면, 우리는 더 복잡한 모델에 대한 실험을 위해 196개 채널의 Re-AudioLDM-L로 모델을 확대합니다.평가 지표.Liu et al.에 따라 Inception Score(IS), Fréchet Audio Distance(FAD), Kullback-Leibler(KL) 발산을 사용하여 Re-AudioLDM의 성능을 평가합니다.IS 점수가 높을수록 생성된 오디오의 다양성이 크고, KL 및 FAD 점수가 낮을수록 오디오 품질이 좋음을 나타냅니다. 의미 수준 평가를 위해 CLAP 인코더에서 계산한 출력 오디오 임베딩과 대상 텍스트 임베딩 간의 코사인 유사도를 계산하여 오디오와 텍스트 간의 상관 관계를 보여줍니다.5.1. 평가 결과 5. 결과 실험은 AudioCaps 평가 세트에서 수행되었습니다.AudioGen[23], AudioLDM[1] 및 Tango[4]를 포함한 여러 최신 프레임워크와 성능을 비교했습니다.각 오디오 클립의 첫 번째 캡션만 텍스트 설명으로 선택하면 각 프레임워크는 샘플링 속도가 16kHz인 975개의 10초 오디오 클립을 추론합니다.표 1은 다양한 텍스트-오디오 모델로 달성한 메트릭을 비교하며, Re-AudioLDM은 다른 방법보다 큰 차이로 성능이 우수합니다.검색에서 제공되는 추가 정보 없이 Re-AudioLDM은 어떠한 이점도 보이지 않으며 일반적으로 AudioCaps의 현재 최신 모델인 Tango보다 열등합니다. 그러나 검색 정보를 통합하면 Re-AudioLDM은 네 가지 평가 지표 모두에서 기준 모델보다 우수한 성과를 거두었습니다.LDM 구조에서 숨겨진 계층의 크기를 확대함으로써 10개의 검색된 쌍을 사용하는 Re-AudioLDM-L은 FAD 점수를 1.4 이하로 더욱 낮추어 기준 프레임워크에 비해 상당한 개선을 이루었습니다.5.2. 절제 연구 검색 유형.표 1의 실험은 오디오, 텍스트 또는 둘 다 아닌 다른 검색 정보에 대한 결과를 보여줍니다.AudioMAE에서 추출한 오디오 기능을 사용하면 Re-AudioLDM으로 약간의 개선만 달성되는데, 이는 주로 Re-AudioLDM이 관련 사운드 이벤트의 기능은 포착하지만 사운드 이벤트 간의 관계는 놓치기 때문입니다.검색된 각 오디오 클립의 쌍 텍스트 정보를 추가함으로써 Re-AudioLDM은 오디오 기능과 고수준 의미 정보 간의 관계를 학습하여 사운드 이벤트에 대한 매우 관련된 의미 기능을 포착하는 데 상당한 개선을 기여합니다. 검색된 쌍의 수. 여러 실험을 수행하여 검색된 오디오-텍스트 쌍의 수가 오디오 생성 성능에 미치는 영향을 평가했습니다. 그림 3에서 볼 수 있듯이 검색 정보를 통합하면 검색된 쌍의 수가 증가함에 따라 성능이 향상되지만, 수가 5에 도달한 후에는 향상 속도가 느려지고 10개 정도로 평평해집니다. 따라서 학습 비용과 모델 성능 간의 균형을 맞추기 위해 이 데이터의 검색된 쌍 수는 경험적으로 3~5 범위로 선택되었습니다. 롱테일 상황. Re-AudioLDM은 롱테일 생성 문제를 해결하고 흔하지 않거나 보이지 않는 사운드 이벤트에 대해 더욱 사실적인 오디오 클립을 생성하는 것을 목표로 합니다. 생성된 각 오디오 클립의 정확도를 평가하기 위해 CLAP 점수[20]를 적용하여 오디오 클립과 텍스트 설명 간의 관계를 보여주었습니다. 먼저 각 오디오 클립의 레이블을 세어 각 사운드 이벤트의 발생 빈도를 계산한 다음 모델 데이터 집합 검색 정보 검색 번호 KL↓ IS ↑ FAD↓ CLAP 점수(%)↑ AudioGen [23] AC+AS+8개 기타 Х Х 1.69 5.13 2.23.AudioLDM [1] Tango [4] AC+AS+2개 기타 Х Х 1.66 6.2.25.AudioCaps ✓ 1.32 6.45 1.29.AudioCaps Х 1.63 6.2.26.AudioCaps Audio1.6.1.31.Re-AudioLDM-S AudioCaps Audio &amp; Text1.7.1.37.AudioCaps Audio &amp; Text1.23에 대한 각 사운드 클래스의 CLAP 점수를 평균하여 모델 성능을 설명합니다. 7.1.37.Re-AudioLDM-L AudioCaps Audio &amp; Text1.20 7.39 1.37.표 1. 검색 정보가 있는 경우와 없는 경우의 다양한 프레임워크 비교.AC와 AS는 각각 AudioCaps[5]와 AudioSet[24]의 약자입니다.KL IS FAD AC_cap2.AC_cap1.AC_cap2 7.AC_cap2.1.6AC_capAC_capAC_cap4 7.2.AC_cap1.AC_cap1.6.1.36.AC_capAC_cap2 1.AC_capAC_capAC_capAC_cap1.1.이벤트 수 그림 3. 검색된 정보 수에 대한 성능 비교, 여기서 AC cap 1-5는 테스트 세트의 캡션 그룹을 나타냅니다.전체 성능에서. 믹스업 방법과 대조적으로, 제안하는 검색 증강 전략은 학습 프로세스의 복잡성을 줄여 전반적인 성능이 향상됩니다.LClap 점수(%)AudioLDM Tango Re-AM-S-rRe-AM-S-rRe-AM-L-r1 10 30 50 100 500빈도(숫자)10 30 50 100 500빈도(숫자) AudioCaps 테스트 세트.그림의 왼쪽에 있는 막대 그래프는 AudioCaps 학습 세트의 모든 327개 사운드 이벤트 클래스의 수량에 대한 통계 분석을 나타냅니다.그림 1(위)과 유사하게, 테일 클래스는 상당한 부분을 구성하며, 특히 1과 10의 레이블 그룹에서 그렇습니다.그림 4(오른쪽)는 학습 세트 내에서 이벤트 발생 빈도가 다른 이벤트에 대한 각 모델의 성능을 보여줍니다.Re-AudioLDM과 기준 모델 간에 매우 빈번한 오디오 이벤트에 대한 초기 격차에도 불구하고, 기준 모델은 테일이 있는 엔터티를 처리할 때 성능이 떨어집니다. 그러나 ReAudioLDM은 훈련 데이터에서 이벤트 발생 빈도가 감소함에 따라 CLAP 점수가 3 미만으로 감소하여 더 안정적인 결과를 보였습니다.따라서 Re-AudioLDM은 꼬리가 달린 사운드 이벤트를 생성할 때 출력 품질 저하를 줄여 전반적인 모델 성능을 향상시킬 수 있습니다.제로 샷 생성.보이지 않는 엔터티에 대한 실험의 경우 훈련 중에 제외된 이벤트가 있는 여러 시나리오를 평가합니다.그림 4(오른쪽)에서 기준 모델은 보이지 않는 오디오(발생 빈도 0)를 생성할 때 성능 저하를 보입니다.이는 모델이 보이지 않는 엔터티의 기능을 학습하지 못한 반면 Re-AudioLDM은 관련 오디오 및 의미 정보를 제공하여 여전히 현실적인 결과를 얻을 수 있기 때문일 수 있습니다.따라서 필수적인 검색 정보를 통해 Re-AudioLDM은 훈련 데이터에서 제외된 사운드를 생성할 수 있는 잠재력이 있습니다.검색 기반 생성은 앞으로 탐구할 방향 중 하나인 제로 샷 작업의 견고성을 크게 향상시킬 수 있습니다.믹스업 전략과의 비교. 클래스 불균형 문제를 해결하는 또 다른 방법은 믹스업 전략을 사용하는 것입니다[26]. 믹스업은 테일 엔터티의 발생 빈도를 높일 수 있지만, 더 복잡한 오디오 예제와 실제 분포와 일치하지 않을 수 있는 합성 오디오 데이터를 도입합니다. [1]의 결과에 따르면 믹스업 전략은 저하로 이어집니다. 그림 4. 다른 주파수 엔터티의 성능, 여기서 S와 L은 모델 크기를 나타내고 r은 검색된 클립의 수를 나타냅니다. 6. 결론 이 논문에서는 AudioCaps의 롱테일 문제를 해결하기 위해 검색 증강 모델인 ReAudioLDM을 제시했습니다. 여러 성능 지표(예: FAD, CLAPscore)를 사용하여 최신 모델(예: AudioLDM 및 Tango)과 비교한 결과 Re-AudioLDM이 고충실도 오디오 클립을 생성하는 데 있어 TTA 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 검색된 기능을 통합함으로써 Re-AudioLDM은 전반적인 성능을 개선할 뿐만 아니라 드물거나 보이지 않는 사운드 엔터티를 생성할 수 있습니다. 향후 작업에서는 외부의 대규모 데이터 세트로 모델을 조사하고 제로샷 생성과 같은 다운스트림 작업에서 모델의 잠재력을 탐색할 것입니다. 7. 감사의 말 이 연구는 부분적으로 China Scholarship Council(CSC)의 연구 장학금, British Broadcasting Corporation Research and Development(BBC R&amp;D), Engineering and Physical Sciences Research Council(EPSRC) Grant EP/T019751/1 &quot;AI for Sound&quot;의 자금 지원, 그리고 University of Surrey의 Centre for Vision, Speech and Signal Processing(CVSSP)의 박사 장학금으로 지원되었습니다. 오픈 액세스를 목적으로 저자는 발생하는 모든 Author Accepted Manuscript 버전에 Creative Commons Attribution(CC BY) 라이선스를 적용했습니다. 참고문헌 [1] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, International Conference on Machine Learning, 2023. [2] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, Z. Zhao, &quot;오디오 만들기: 프롬프트 강화 확산 모델을 사용한 텍스트-오디오 생성&quot;, International Conference on Machine Learning, 2023. [3] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, MD Plumbley, W. Wang, &quot;Wavjourney: 대규모 언어를 사용한 구성 오디오 생성 모델,&quot; arXiv 사전 인쇄본 arXiv:2307.14335, 2023. [4] D. Ghosal, N. Majumder, A. Mehrish, 및 S. Poria, &quot;명령 조정 LLM 및 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; arXiv 사전 인쇄본 arXiv:2304.13731, 2023. [5] CD Kim, B. Kim, H. Lee, 및 G. Kim, &quot;AudioCaps: 야생 오디오에 대한 캡션 생성,&quot; 북미 전산 언어학 협회 연례 컨퍼런스, 2019. [6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, L. Kaiser, 및 I. Polosukhin, &quot;주의만 있으면 됩니다,&quot; 신경 정보 처리 시스템의 발전, 제 23권 4호, 2019년 11월. 30, 2017. [7] Y. Yuan, H. Liu, J. Liang, X. Liu, MD Plumbley, 및 W. Wang, &quot;사전 훈련된 오디오 학습을 사용하여 사운드 생성: 벤치마크 연구&quot;, 유럽 신호 처리 협회, 2023. [8] X. Liu, T. Iqbal, J. Zhao, Q. Huang, M. Plumbley, 및 W. Wang, &quot;신경 이산 시간-주파수 표현 학습을 사용한 조건부 사운드 생성&quot;, IEEE 신호 처리를 위한 기계 학습 국제 워크숍, 2021. [9] X. Chen, N. Mishra, M. Rohaninejad, 및 P. Abbeel, &quot;PixelSNAIL: 개선된 자기 회귀 생성 모델&quot;, 국제 기계 학습 컨퍼런스, 2018, 864-872쪽. [10] V. Iashin 및 E. Rahtu, &quot;시각적으로 유도되는 사운드 길들이기 영어: tion,&quot; in British Machine Vision Conference, 2021. genera[11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, &quot;Language models are unsupervised multitask learners,&quot; OpenAI blog, vol. 1, no. 8, p. 9, 2019. [12] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, D. Yu, &quot;Diffsound: 텍스트-사운드 생성을 위한 이산 확산 모델&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2023. [13] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, 기계 학습 국제 컨퍼런스, 2021, 8748-8763쪽. [14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, PJ Liu, &quot;한계 탐색 영어: 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 경우,&quot; Journal of Machine Learning Research, vol. 21, no. 1, pp. 54855551, 2020. [15] B. Li, PH Torr, and T. Lukasiewicz, &quot;메모리 기반 텍스트-이미지 생성,&quot; British Machine Vision Conference, 2022. [16] A. Blattmann, R. Rombach, K. Oktay, J. Müller, and B. Ommer, &quot;검색 증강 확산 모델,&quot; Advances in Neural Information Processing Systems, vol. 35, pp. 15 309-15 324, 2022. [17] S. Sheynin, O. Ashual, A. Polyak, U. Singer, O. Gafni, E. Nachmani, and Y. Taigman, “KNN-diffusion: Image generation via large-scale retrieval,” in International Conference on Learning Representations, 2023. [18] W. Chen, H. Hu, C. Saharia, and WW Cohen, “Re-Imagen: Retrieval-augmented text-to-image generator,” in International Conference on Learning Representations, 2023. [19] Y. Yuan, H. Liu, X. Kang, P. Wu, MD Plumbley, and W. Wang, “Text-driven foley sound generation with latent diffusion model,” in Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop, 2023, 231-235쪽. [20] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, &quot;특징 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP, 2023. [21] H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer et al., &quot;수신하는 마스크 자동 인코더&quot;, arXiv 사전 인쇄:2207.06405, 2022. [22] J. Ho, A. Jain, and P. Abbeel, &quot;노이즈 제거 확산 확률적 모델&quot;, 신경 정보 처리 시스템, 2020. [23] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, 및 Y. Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성,&quot; 국제 학습 표현 컨퍼런스, 2023. [24] JF Gemmeke, DPW Ellis, D. Freedman, A. Jansen, W. Lawrence, RC Moore, M. Plakal, 및 M. Ritter, &quot;AudioSet: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2017, pp. 776-780. [25] T. Heittola, A. Mesaros 및 T. Virtanen, &quot;TAU Urban Acoustic Scenes 2019, Development dataset,&quot; 2019년 3월. [온라인]. 사용 가능: https://doi.org/10.5281/zenodo.[26] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang 및 MD Plumbley, &quot;PANN: 오디오 패턴 인식을 위한 대규모 사전 학습 오디오 신경망&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 제28권, 2880-2894쪽, 2020년.
"
"--- ABSTRACT ---
Large language models (LLMs) have been successfully adapted for interactive decisionmaking tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task. 1
--- INTRODUCTION ---
Large language models (LLMs) such as GPT-(OpenAI, 2023) have achieved remarkable performance on a wide range of natural language understanding (NLU) tasks (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). Recently, they have been adapted to interactive decision-making tasks such as virtual home navigation (Yang et al., 2023), text-based games (Lin et al., 2023) or webnavigation (Yao et al., 2023; Zhou et al., 2024). Previous methods that utilize LLMs to solve interactive tasks often implicitly assume a forward-only execution mode for the model, where they only provide a few oracle trajectories as in-context examples to teach the model how to reason step-by-step (Yao et al., 2023; Lo et al., 2023). In other words, Search Query LLM Agent Growing History When max iteration reached Item Selection Figure 1: LASER’s state transition diagram on the Webshop Task. Solid circle represent states, and the arrows represent possible state transitions. This formulation enables flexible backtracking and relieves the limitation of forward-only examples, allowing the model to better handle unfamiliar scenarios and recover from errors. the correct action is selected at every step in those oracle trajectories. This might lead to sub-optimal performance because when the model makes an unexpected mistake at test time, it would not know how to recover from it. At the same time, including many in-context examples to cover all possible scenarios is costly or unrealistic. Moreover, previous methods assume a global action space where the model is free to take any action at any step because they either define the possible actions at the beginning of the prompt or expect the LLM to figure out the possible action from in-context examples automatically. This might further increase the task’s difficulty, and the LLM may perform invalid actions in certain cases. To address the aforementioned issues, we propose to model the interactive tasks as state-space exploration. We first define a set of high-level possible states the LLM agent might encounter during the task execution. Then, we identify the possible action space in each state and the resulting states after performing each action. This formulation effectively converts the LLM agent’s exploration in the interactive task as state transitions, where each action takes the agent from one state to another. Naturally, this allows the agent to easily recover from a wrong action: taking another action that would --- --send it back to the previous state. Moreover, our proposed formulation associates the action space with each individual state, which reduces the task’s difficulty and allows the agent to always select the valid action at any step. We evaluated our proposed LASER on the Webshop (Yao et al., 2022) task and conducted sim-to-real transfer experiments where we directly applied LASER to amazon.com. We show that our proposed setup enables the agent to complete complex user instructions without using in-context examples, and LASER significantly outperforms all previous baselines and closes the gap with human performance. 2 Methods 2.1 Problem Formulation Given a web environment E and a user instruction I, the agent is instantiated in the environment and provided with an initial observation Oo. The agent is expected to perform a series of actions {ao, a1, .-Gn} to complete the user instruction, where each a; produces a new observation O; when executed in the environment. S denotes the stopping state where the agent produces an output and stops exploration after reaching it. Finally, the agent’s output is compared with the target to compute the metrics. 2.2 LLM Agent As previously discussed, we would like the agent to be able to handle any novel situations or mistakes that might occur during execution without exhaustively describing them via a large number of in-context examples. Thus, we propose to equip LLM agents with the state-tracking capability. A diagram of the state transitions of our agent is shown in Figure 1. We start by defining a set of possible high-level states the agent might encounter in the environment (§2.3). The LLM agent takes the user input as the overall goal and is initialized in the starting state. At every step, the agent receives state-specific system instruction, current observation, a set of permissible actions in the current states, and the history of past thoughts and actions as inputs. Then, it selects one of the actions as output, which either transitions the agent to a different state or remains in the same state (§2.4). The agent repeats the process until the stopping state or the maximum step is reached. Notice that with our formulation, we can provide detailed instructions to inform the agent of the possible situations in every state and how to handle them. For example, as shown in Figure 1, at the results state, the current results may or may not be good enough, and we instruct the agent to either select an item, go to the next page, or go back to search depending on its judgment. Hence, these instructions can be very informative to guide the agent while being much more efficient than incontext examples. Next, we describe in detail how we design the state and action spaces. 2.3 State Description In our work, we use the term state to describe the current environment the agent is in, and we consider an agent to be in two different states only if the structure of the current environment observation is different. This allows us to define only a handful of states to support an agent’s exploration in a complex environment fully. After manually categorizing all possible states in the interactive task, for each state, we write a generic instruction that describes the state in detail. Specifically, we provide a sample layout of the observation the agent would receive in that state and replace all specifications in the layout with placeholders. We also provide a high-level goal and detailed instructions to act in that state. The sample layout combined with state-specific instructions allows us to inform the agent of possible observations it might receive and how to act accordingly. Therefore we no longer need to provide in-context examples to guide the agent. For the WebShop task, we define a total of four states, and the full prompts for search, results, and item states can be found in Table 4, Table 5 and Table 6 in the appendix. 2.4 Action Space Previous methods often implicitly assume a global action space for the model, i.e. the model is free to take any action without further constraints. Although the LLM is able to figure out valid actions to take most of the time, it might still attempt to take invalid actions in certain cases. Thus after defining all possible states for the task, we further identify the action space for each state to rule out such possibilities. Specifically, we define a set of permissible actions that the agent can choose from for each state, which ensures that the agent always performs valid actions. The state-action mapping for our agent is shown in Table 8 in the appendix. In practice, permissible actions can also be determined heuristically, e.g., identifying all clickable buttons on a webpage. --- --Success Rate Reward ASH (Lo et al., 2023) 30.2 56.ReAct (Yao et al., 2023)* 40.0 66.ReAct (ours rerun) 34.0 59.WebGUM (Furuta et al., 2023) 45.0 67.LASER - backup 48.4 71.LASER 50.0 75.Human Expert (Yao et al., 2022) 59.6 82.Table 1: Results on WebShop Task. *simplified setting Inspired by the ReAct method (Yao et al., 2023), we also ask the agent to produce a thought at every step and then select an action based on its thought. The agent keeps repeating the thought-and-action process until it reaches the stopping state or the maximum step is reached. We also define a memory buffer to store the intermediate results (the items examined but considered non-matching) during the exploration. This is similar to human behavior in that people typically find a few backup options before finding the desired item. When the agent is forced to stop after the maximum number of steps, it selects one of the intermediate results as the final output, and we call this the backup strategy. 3 Experiments We conduct our experiments on the WebShop task (Yao et al., 2022). We used 500 test set instructions for evaluation and adopted reward and success rate as metrics following previous works (Yao et al., 2022). We used GPT-4-0613 to power LASER and its function-calling ability to implement action selection step. We compare against the following baselines: ReAct (Yao et al., 2023) is a prompting method designed for interactive decision-making tasks. At every step, the LLM agent receives an observation and can either produce a thought or an action. The agent accumulates all of the past observations, thoughts, and actions in its prompt, using a full trajectory of exploration as an in-context example. The original ReAct uses PaLM (Chowdhery et al., 2023) as its LLM backbone. To make a fair comparison, we also rerun the ReAct method with GPT-4-0613. ASH (Lo et al., 2023) builds on top of ReAct and adds a summarization step that condenses the agent observation and acts based on the condensed information. WebGUM (Furuta et al., 2023) is a supervised method that finetunes FlanT5-XL model (Chung et al., 2022) on 1K human demonstrations provided by the WebShop SR Reward Att. Opt. LASER 62.0 85.Human (Yao et al., 2022) 65.0 88.Type. 85.5 75.0 97.86.2 76.3 99.Table 2: Results on Amazon.com. Success Rate Reward LASER 52.0 771.LASER + One-shot 50.0 74.LASER - function call 50.0 76.LASER (text-davinci-003) 38.5 70.Table 3: Ablation Results on the WebShop Task. The standard LASER is powered by GPT-4 under zero-shot. task. Moreoever, we experimented with sim-toreal transfer experiments where we directly apply LASER to amazon.com without modification. We follow the same settings as Yao et al. (2022) and evaluated on 100 test set instructions and then manually evaluated results. More detailed experimental setup is discussed in Appendix B. 4 Results The overall results of our experiments are shown in Table 1. Our early experiments showed that the ReAct agent often produces invalid actions. For example, when it selects an item that doesn’t match the instruction, it tries to click the next page button (which does not exist) before backing to the results page. Also, the ReAct agent often got stuck in a certain action and failed to produce output. For example, the agent keeps going to the next page until the maximum step is reached. We added detailed instructions as the system prompt to try to address the issue. Despite our best efforts, the agent still makes invalid actions in some cases and achieves worse results than the original paper. On the other hand, LASER outperforms baselines by large margins on both metrics, showing the effectiveness of our approach. We further removed the backup strategy of LASER (the agent would receive a 0 score when the maximum budget runs out) to make a more fair comparison with ReAct. We see that our method still outperforms baselines by very large margins. The results from the transfer experiments are shown in Table 2. Again, LASER achieves very close results compared to human performance. It’s also encouraging to see that LASER even achieved better performance on this realistic environment than the WebShop, which is likely due to the stronger search engine on amazon.com. --- --[ll Rewards [ll Success Rate @ 3 @ 4-6 © 7-9 @ 10-13 @3 46 7-9 10-13Figure 2: Left: LASER’s performance for test set episodes of different lengths. Right: The distribution of the number of steps LASER takes to complete tasks 4.1 Analysis We first conduct ablation studies to understand the important design decisions of our agent. Zero-shot vs Few-shot We used state-specific instructions only to guide our agent’s exploration in the environment, whereas previous works often adopt in-context examples. To investigate if the agent can further benefit from in-context examples, we experimented with a one-shot setting: for every prompt in LASER, we added one example inputoutput pair between our system instructions and current inputs, and the rest of the agent remains the same. Due to the limited computing budget, we only ran our ablation studies on 200 instructions. The results are shown in Table 3. We see that adding an in-context example actually leads to worse performance. Since LASER already performs valid actions 100% time, we hypothesize that the agent understands the task well without incontext examples and the added example is actually distracting the agent in some cases. Effect of function-calling LASER takes advantage of the function-calling functionality that is enabled only for GPT models after 06/13/23. Thus, we are interested to see the effect of replacing this design with regular text generation. To do so, instead of passing the permissible actions as a list of functions, we convert each action as a Python dictionary describing its purpose and arguments and then append them to the prompt. We then ask the LLM to generate output in JSON format to represent the action it selects with appropriate arguments. The results are shown in Table 3. Again, the agent without function calling performs slightly worse on these 200 episodes. It shows that the function calling functionality can be leveraged to boost performance when building interactive agents, suggesting a direction for building future LLMs. Performance vs trajectory length Here, we are interested in seeing the length of LASER’s trajectories and their effect on the overall performance. We plot the distribution of trajectory length in Figureand the agent’s performance for each length group. We notice that most of the time, the agent only took three state transitions to reach the finish state, which is search-select-buy. From the left figure, the agent’s performance generally decreases as the trajectory gets longer. However, the drop is less significant compared to the observation made for ReAct and ASH agent (Lo et al., 2023), which further shows the effectiveness of our agent. Finally, for the length 15 group, for which the agent is forced to stop and select from the browsing history, the performance is much lower than other groups. While not surprising, it has a non-zero success rate, showing that there are cases where the agent found a matching item but failed to recognize it as the target in the first pass. Generalization to different LLMs We adopted the text-davinci-003 model to see if LASER can transfer well to a less powerful non-chat model. Since this model does not support function-calling, we adopted the approach described earlier to prompt the model to generate JSON output to represent actions. The results are shown in Table 3. Although switching to text-davinci-003 leads to a large drop in performance, our model still achieves better results than the baselines. It shows that our proposed agent can be easily adapted to other LLMs with different capabilities. With more powerful models in the future, our agent could potentially surpass human performance on this task. We also conducted case studies to inspect the failure modes of LASER and additional results are in Appendix C. We discuss
--- RELATED WORK ---
s in Appendix A. 5 Conclusions We proposed an LLM agent, LASER, that models interactive web navigation tasks as state-space exploration. Our formulation allows the agent to handle novel situations, easily backtrack from mistakes, and always perform valid actions. Guided solely by the state-specific instructions without any in-context examples, LASER outperforms all baselines on the WebShop task by large margins and closes the gap with human performance on the realworld shopping website. Our analysis shows that LASER is also more robust to longer trajectories and generalizes well to other LLMs. --- --Limitations In this work, we have only experimented with the task of finding the target item for the shopping domain. Despite its challenging nature, it does not cover all tasks user typiclaly do on an e-commerce website, e.g., tracking orders or checking order history. For future work, it would be interesting to enhance LASER’s ability so that it can handle such popular tasks in the shopping domain. Also, it would be interesting to equip LASER with more tools such as a knowledge retriever (Ma et al., 2023) or a calculator (Gao et al., 2023), so that it can handle more complex instructions. Our LASER requires manual annotation of possible states in the environment and their corresponding descriptions. Because of this, our
--- METHOD ---
s implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com.
--- EXPERIMENT ---
al results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task. 1 Introduction Large language models (LLMs) such as GPT-(OpenAI, 2023) have achieved remarkable performance on a wide range of natural language understanding (NLU) tasks (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). Recently, they have been adapted to interactive decision-making tasks such as virtual home navigation (Yang et al., 2023), text-based games (Lin et al., 2023) or webnavigation (Yao et al., 2023; Zhou et al., 2024). Previous methods that utilize LLMs to solve interactive tasks often implicitly assume a forward-only execution mode for the model, where they only provide a few oracle trajectories as in-context examples to teach the model how to reason step-by-step (Yao et al., 2023; Lo et al., 2023). In other words, Search Query LLM Agent Growing History When max iteration reached Item Selection Figure 1: LASER’s state transition diagram on the Webshop Task. Solid circle represent states, and the arrows represent possible state transitions. This formulation enables flexible backtracking and relieves the limitation of forward-only examples, allowing the model to better handle unfamiliar scenarios and recover from errors. the correct action is selected at every step in those oracle trajectories. This might lead to sub-optimal performance because when the model makes an unexpected mistake at test time, it would not know how to recover from it. At the same time, including many in-context examples to cover all possible scenarios is costly or unrealistic. Moreover, previous methods assume a global action space where the model is free to take any action at any step because they either define the possible actions at the beginning of the prompt or expect the LLM to figure out the possible action from in-context examples automatically. This might further increase the task’s difficulty, and the LLM may perform invalid actions in certain cases. To address the aforementioned issues, we propose to model the interactive tasks as state-space exploration. We first define a set of high-level possible states the LLM agent might encounter during the task execution. Then, we identify the possible action space in each state and the resulting states after performing each action. This formulation effectively converts the LLM agent’s exploration in the interactive task as state transitions, where each action takes the agent from one state to another. Naturally, this allows the agent to easily recover from a wrong action: taking another action that would --- --send it back to the previous state. Moreover, our proposed formulation associates the action space with each individual state, which reduces the task’s difficulty and allows the agent to always select the valid action at any step. We evaluated our proposed LASER on the Webshop (Yao et al., 2022) task and conducted sim-to-real transfer experiments where we directly applied LASER to amazon.com. We show that our proposed setup enables the agent to complete complex user instructions without using in-context examples, and LASER significantly outperforms all previous baselines and closes the gap with human performance. 2 Methods 2.1 Problem Formulation Given a web environment E and a user instruction I, the agent is instantiated in the environment and provided with an initial observation Oo. The agent is expected to perform a series of actions {ao, a1, .-Gn} to complete the user instruction, where each a; produces a new observation O; when executed in the environment. S denotes the stopping state where the agent produces an output and stops exploration after reaching it. Finally, the agent’s output is compared with the target to compute the metrics. 2.2 LLM Agent As previously discussed, we would like the agent to be able to handle any novel situations or mistakes that might occur during execution without exhaustively describing them via a large number of in-context examples. Thus, we propose to equip LLM agents with the state-tracking capability. A diagram of the state transitions of our agent is shown in Figure 1. We start by defining a set of possible high-level states the agent might encounter in the environment (§2.3). The LLM agent takes the user input as the overall goal and is initialized in the starting state. At every step, the agent receives state-specific system instruction, current observation, a set of permissible actions in the current states, and the history of past thoughts and actions as inputs. Then, it selects one of the actions as output, which either transitions the agent to a different state or remains in the same state (§2.4). The agent repeats the process until the stopping state or the maximum step is reached. Notice that with our formulation, we can provide detailed instructions to inform the agent of the possible situations in every state and how to handle them. For example, as shown in Figure 1, at the results state, the current results may or may not be good enough, and we instruct the agent to either select an item, go to the next page, or go back to search depending on its judgment. Hence, these instructions can be very informative to guide the agent while being much more efficient than incontext examples. Next, we describe in detail how we design the state and action spaces. 2.3 State Description In our work, we use the term state to describe the current environment the agent is in, and we consider an agent to be in two different states only if the structure of the current environment observation is different. This allows us to define only a handful of states to support an agent’s exploration in a complex environment fully. After manually categorizing all possible states in the interactive task, for each state, we write a generic instruction that describes the state in detail. Specifically, we provide a sample layout of the observation the agent would receive in that state and replace all specifications in the layout with placeholders. We also provide a high-level goal and detailed instructions to act in that state. The sample layout combined with state-specific instructions allows us to inform the agent of possible observations it might receive and how to act accordingly. Therefore we no longer need to provide in-context examples to guide the agent. For the WebShop task, we define a total of four states, and the full prompts for search, results, and item states can be found in Table 4, Table 5 and Table 6 in the appendix. 2.4 Action Space Previous methods often implicitly assume a global action space for the model, i.e. the model is free to take any action without further constraints. Although the LLM is able to figure out valid actions to take most of the time, it might still attempt to take invalid actions in certain cases. Thus after defining all possible states for the task, we further identify the action space for each state to rule out such possibilities. Specifically, we define a set of permissible actions that the agent can choose from for each state, which ensures that the agent always performs valid actions. The state-action mapping for our agent is shown in Table 8 in the appendix. In practice, permissible actions can also be determined heuristically, e.g., identifying all clickable buttons on a webpage. --- --Success Rate Reward ASH (Lo et al., 2023) 30.2 56.ReAct (Yao et al., 2023)* 40.0 66.ReAct (ours rerun) 34.0 59.WebGUM (Furuta et al., 2023) 45.0 67.LASER - backup 48.4 71.LASER 50.0 75.Human Expert (Yao et al., 2022) 59.6 82.Table 1: Results on WebShop Task. *simplified setting Inspired by the ReAct method (Yao et al., 2023), we also ask the agent to produce a thought at every step and then select an action based on its thought. The agent keeps repeating the thought-and-action process until it reaches the stopping state or the maximum step is reached. We also define a memory buffer to store the intermediate results (the items examined but considered non-matching) during the exploration. This is similar to human behavior in that people typically find a few backup options before finding the desired item. When the agent is forced to stop after the maximum number of steps, it selects one of the intermediate results as the final output, and we call this the backup strategy. 3 Experiments We conduct our experiments on the WebShop task (Yao et al., 2022). We used 500 test set instructions for evaluation and adopted reward and success rate as metrics following previous works (Yao et al., 2022). We used GPT-4-0613 to power LASER and its function-calling ability to implement action selection step. We compare against the following baselines: ReAct (Yao et al., 2023) is a prompting method designed for interactive decision-making tasks. At every step, the LLM agent receives an observation and can either produce a thought or an action. The agent accumulates all of the past observations, thoughts, and actions in its prompt, using a full trajectory of exploration as an in-context example. The original ReAct uses PaLM (Chowdhery et al., 2023) as its LLM backbone. To make a fair comparison, we also rerun the ReAct method with GPT-4-0613. ASH (Lo et al., 2023) builds on top of ReAct and adds a summarization step that condenses the agent observation and acts based on the condensed information. WebGUM (Furuta et al., 2023) is a supervised method that finetunes FlanT5-XL model (Chung et al., 2022) on 1K human demonstrations provided by the WebShop SR Reward Att. Opt. LASER 62.0 85.Human (Yao et al., 2022) 65.0 88.Type. 85.5 75.0 97.86.2 76.3 99.Table 2: Results on Amazon.com. Success Rate Reward LASER 52.0 771.LASER + One-shot 50.0 74.LASER - function call 50.0 76.LASER (text-davinci-003) 38.5 70.Table 3: Ablation Results on the WebShop Task. The standard LASER is powered by GPT-4 under zero-shot. task. Moreoever, we experimented with sim-toreal transfer experiments where we directly apply LASER to amazon.com without modification. We follow the same settings as Yao et al. (2022) and evaluated on 100 test set instructions and then manually evaluated results. More detailed experimental setup is discussed in Appendix B. 4 Results The overall results of our experiments are shown in Table 1. Our early experiments showed that the ReAct agent often produces invalid actions. For example, when it selects an item that doesn’t match the instruction, it tries to click the next page button (which does not exist) before backing to the results page. Also, the ReAct agent often got stuck in a certain action and failed to produce output. For example, the agent keeps going to the next page until the maximum step is reached. We added detailed instructions as the system prompt to try to address the issue. Despite our best efforts, the agent still makes invalid actions in some cases and achieves worse results than the original paper. On the other hand, LASER outperforms baselines by large margins on both metrics, showing the effectiveness of our approach. We further removed the backup strategy of LASER (the agent would receive a 0 score when the maximum budget runs out) to make a more fair comparison with ReAct. We see that our method still outperforms baselines by very large margins. The results from the transfer experiments are shown in Table 2. Again, LASER achieves very close results compared to human performance. It’s also encouraging to see that LASER even achieved better performance on this realistic environment than the WebShop, which is likely due to the stronger search engine on amazon.com. --- --[ll Rewards [ll Success Rate @ 3 @ 4-6 © 7-9 @ 10-13 @3 46 7-9 10-13Figure 2: Left: LASER’s performance for test set episodes of different lengths. Right: The distribution of the number of steps LASER takes to complete tasks 4.1 Analysis We first conduct ablation studies to understand the important design decisions of our agent. Zero-shot vs Few-shot We used state-specific instructions only to guide our agent’s exploration in the environment, whereas previous works often adopt in-context examples. To investigate if the agent can further benefit from in-context examples, we experimented with a one-shot setting: for every prompt in LASER, we added one example inputoutput pair between our system instructions and current inputs, and the rest of the agent remains the same. Due to the limited computing budget, we only ran our ablation studies on 200 instructions. The results are shown in Table 3. We see that adding an in-context example actually leads to worse performance. Since LASER already performs valid actions 100% time, we hypothesize that the agent understands the task well without incontext examples and the added example is actually distracting the agent in some cases. Effect of function-calling LASER takes advantage of the function-calling functionality that is enabled only for GPT models after 06/13/23. Thus, we are interested to see the effect of replacing this design with regular text generation. To do so, instead of passing the permissible actions as a list of functions, we convert each action as a Python dictionary describing its purpose and arguments and then append them to the prompt. We then ask the LLM to generate output in JSON format to represent the action it selects with appropriate arguments. The results are shown in Table 3. Again, the agent without function calling performs slightly worse on these 200 episodes. It shows that the function calling functionality can be leveraged to boost performance when building interactive agents, suggesting a direction for building future LLMs. Performance vs trajectory length Here, we are interested in seeing the length of LASER’s trajectories and their effect on the overall performance. We plot the distribution of trajectory length in Figureand the agent’s performance for each length group. We notice that most of the time, the agent only took three state transitions to reach the finish state, which is search-select-buy. From the left figure, the agent’s performance generally decreases as the trajectory gets longer. However, the drop is less significant compared to the observation made for ReAct and ASH agent (Lo et al., 2023), which further shows the effectiveness of our agent. Finally, for the length 15 group, for which the agent is forced to stop and select from the browsing history, the performance is much lower than other groups. While not surprising, it has a non-zero success rate, showing that there are cases where the agent found a matching item but failed to recognize it as the target in the first pass. Generalization to different LLMs We adopted the text-davinci-003 model to see if LASER can transfer well to a less powerful non-chat model. Since this model does not support function-calling, we adopted the approach described earlier to prompt the model to generate JSON output to represent actions. The results are shown in Table 3. Although switching to text-davinci-003 leads to a large drop in performance, our model still achieves better results than the baselines. It shows that our proposed agent can be easily adapted to other LLMs with different capabilities. With more powerful models in the future, our agent could potentially surpass human performance on this task. We also conducted case studies to inspect the failure modes of LASER and additional results are in Appendix C. We discuss related works in Appendix A. 5
--- CONCLUSION ---
s We proposed an LLM agent, LASER, that models interactive web navigation tasks as state-space exploration. Our formulation allows the agent to handle novel situations, easily backtrack from mistakes, and always perform valid actions. Guided solely by the state-specific instructions without any in-context examples, LASER outperforms all baselines on the WebShop task by large margins and closes the gap with human performance on the realworld shopping website. Our analysis shows that LASER is also more robust to longer trajectories and generalizes well to other LLMs. --- --Limitations In this work, we have only experimented with the task of finding the target item for the shopping domain. Despite its challenging nature, it does not cover all tasks user typiclaly do on an e-commerce website, e.g., tracking orders or checking order history. For future work, it would be interesting to enhance LASER’s ability so that it can handle such popular tasks in the shopping domain. Also, it would be interesting to equip LASER with more tools such as a knowledge retriever (Ma et al., 2023) or a calculator (Gao et al., 2023), so that it can handle more complex instructions. Our LASER requires manual annotation of possible states in the environment and their corresponding descriptions. Because of this, our method might only be suitable for building agents for specific domains (rather than open-world web agents), where only a handful of states are required, e.g. ecommerce or travel booking. For future directions, we envision a hierarchical multi-agent system, in which each specific domain is governed by an agent like LASER, and a general open-world agent just collaborates with other domain agents to complete various user instructions. Regarding potential risks of our work, we think extra caution and testing are required before deploying LASER to real-world scenarios. When conducting experiments on the Webshop task, we allow the agent to take any action permitted in the environment because of its simulated nature. However, certain actions may have hard-to-recover consequences in the real world. For example, clicking the buy button in a real shopping site. Therefore we forced the agent to stop when it decides to buy the item when experimenting on amazon.com. In general, as LASER’s success rate is still far from being perfect, it might require additional human verification before proceeding with actions that have high-stakes. References Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. 2023. Instruction-finetuned foundation models for multimodal web navigation. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org. --- --Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. A real-world webagent with planning, long context understanding, and program synthesis. In The Twelfth International Conference on Learning Representations. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. Understanding HTML with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2803-2821, Singapore. Association for Computational Linguistics. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: Building an end-toend web agent with large multimodal models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Embodied reasoning through planning with language models. In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 1769-1782. PMLR. Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, and Wei-Lun Chao. 2024. Dual-view visual contextualization for web navigation. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. 2023. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. In Thirtyseventh Conference on Neural Information Processing Systems. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations. Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, and Shuyan Zhou. 2023. Hierarchical prompting assists large language model on web navigation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10217-10244, Singapore. Association for Computational Linguistics. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2023. Chain-of-skills: A configurable model for open-domain question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599-1618, Toronto, Canada. Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. OpenAI. 2023. Gpt-4 technical report. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. 2023. From pixels to Ul actions: Learning to follow instructions via graphical user interfaces. In Thirty-seventh Conference on Neural Information Processing Systems. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models. In Thirtyseventh Conference on Neural Information Processing Systems. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Hui Yang, Sifu Yue, and Yunzhong He. 2023. Autogpt for online decision making: Benchmarks and additional opinions. Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. In Advances in Neural Information Processing Systems. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. --- --Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024a. Gpt-4v(ision) is a generalist web agent, if grounded. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024b. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations. A_ Related Works Interactive decision-making tasks such as web navigation have become popular recently (Liu et al., 2018; Yao et al., 2022; Deng et al., 2023; Zhou et al., 2024), while some efforts have tried to solve these tasks by finetuning pretrained language models on a large corpus of demonstration data (Gur et al., 2023; Furuta et al., 2023), other attempted to build agents to navigate web environments solely relying on prompting LLMs (Yang et al., 2023). Among the LLM-based approaches, ReAct (Yao et al., 2023) and InnerMonologue (Huang et al., 2023) equip the LLM with a thought process before producing actions. ASH (Lo et al., 2023) and WebA gent (Gur et al., 2024) focus on decomposing complex decision-making steps into a set of simpler steps, e.g. first summarizing the task-relevant content and then act upon it. Most similar to our work, Synapse (Zheng et al., 2024b) also proposed to use state-conditional prompts to guide the LLM’s action. However, their focus is on decomposing the few-shot examples into atomic parts whereas our agent uses state-specific instructions alone without in-context examples to complete tasks. Another line of work focuses on the planning stage of LLM agents. Kim et al. (2023) proposed an agent RCI that generates a plan before acting, and then refines its action when encountering errors. Adaplanner (Sun et al., 2023) further enhanced the planning approach by adaptively updating the plan during the agent’s execution. Reflexion (Shinn et al., 2023) agent refines its plan and actions by taking environmental feedback through a trial-anderror fashion. These approaches are orthogonal to our work and can be potentially combined with our agent to enhance its performance. More recently, various works have tried to develop multi-modal agents. Pix2Act (Shaw et al., 2023) and AppAgent (Zhang et al., 2023) mostly replied on the screenshots as inputs for the agents to predict UI actions, wheras SEEACT (Zheng et al., 2024a), WebVoyager (He et al., 2024) and DualVCR (Kil et al., 2024) leverage both screenshots and textual elements from websites to interact with the web environment. Our idea of modeling web navigation as state transitions can potentially be incorporated in those agents to further enhance their performance. B_ Experimental Details The WebShop provides a simulated environment for online shopping, containing 1,181,436 items collected from Amazon shopping sites. Additionally, the task provides human-annotated instructions for purchasing certain items and their corresponding target items. We followed previous works and used the 500 test set instructions to evaluate our LASER and evaluate with rewards and success rate, where the agent is considered successful if the purchased item perfectly matches the target item, otherwise, if the purchased item partially matches the target item, the agent receives a partial reward (scale between 0-100). This partial reward is computed using the items’ price, product category, hidden attributes and customization options. For our method, we used the GPT-4-0613 to power our LASER. We used the function-calling functionality to implement the action selection step. In particular, we write a short description for each action and then pass them as a list to the functioncall argument of the LLM to let the model select from. We allow our agent to make 15 state transitions in maximum. In practice, if the agent has not reached the finish state after 13 state transitions, we force it to select from the history to ensure it does not exceed the budget. For the sim-to-real transfer experiments on amazon.com, we used the first 100 test set instructions from the WebShop. We following the same setting as Yao et al. (2022), where we convert the webpages on amazon.com into the same format as the WebShop ! then run LASER agent as is. Since we do not have the gold annotation for the ‘https: //github.com/princeton-nlp/WebShop/ tree/master/transfer --- --Instruction: i am looking for a green table lamp for the living room, and price lower than 60.00 dollars =a El Safavieh Lighting Collection Minton [ENN] Light Green 20-inch Bedroom Living Room Home Office Desk Nightstand Table Lamp (LED Bulb Included) “ Price: $58.NA b =) =) Figure 3: An example of the /tem good enough error cases, the item selected by the agent is shown and the user instruction is on the top. The reward the agent receives is 0.666. Instruction: i'm looking for a pair of women’s high heel with closed toe. i want pink and in size 9, and price lower than 40.00 dollars Masbird Sandals for Women Casual Summer Closed Toe Buckle Strap Wedge Sandals Strappy Platform Sandals Price: $10.99 to $14.Rating: NA Figure 4: An example of the Missing details error cases, the item selected by the agent is shown and the user instruction is on the top. The reward the agent receives is 0.8. items LASER selected on amazon.com, we follow Yao et al. (2022) and conducted human evaluation. In particular, we manually annotated item attribute matches, item option matches, item category matches and item price matches. We then computed individual reward scores as well as the overall reward score and success rate using the same unctions defined for the WebShop task. Since both human and LASER achieves 100% on item price matches, we omitted these results from Table 2. Regarding the comparison against different baseines, we would like to note that both ReAct (Yao et al., 2023) and ASH (Lo et al., 2023) used manually written instruction and manually annotated agent trajectories as in-context demonstrations to prompt LLMs, which corresponds to our one-shot setting in subsection 4.1. For WebGUM (Furuta et al., 2023), they used 1k human annotated gold trajectories to finetune their model. Therefore, all baselines we considered use some kind of human knowledge/prior to help the agent learn. For us, we solely relied on manual instructions to guide the LASER. We believe that providing high-level generalizable instructions (as done in LASER) is a more efficient ways of learning than providing low-level task-specific trajectories (e.g. WebGUM). Intuitively, the agent basically learns to abstract out some high-level insights about how to handle each scenario from the large amount of trajectories. In comparison, we can directly provide such insights to the model via a few sentences in the instruction. Taking such perspective, we can also say that the difference between our work and previous work is providing high-level generalizable human knowledge vs providing low-level case-by-case human knowledge. We believe it’s desirable to provide model such high-level knowledge when it requires similar or less amount of human effort. C_ Case Studies We manually annotated 30 error cases from the Dev set to understand the failure cases of LASER. We broadly categorize the errors into three categories: Item good enough: the item selected by the agent meets the user instruction from the authors’ perspective but did not receive a full score. We found that 9 out of 30 cases fall into this category, and an example is shown in Figure 3. The item found by the agent is indeed a green table lamp for the living room with a price within the budget, but it is considered incorrect. Retrieval failure: none of the items returned by the search engine meets the user requirement, despite that the agent used a suitable query for retrieval. We found 12 out of 30 cases fall into this category. We hypothesize that a more effective retriever or search engine can probably address these issues. Missing details: The item selected by the agent indeed does not match the user’s instruction on certain details. We found that 9 out of 30 cases fall into this category, and an example is shown in Figure 4. In this example, although the color and size of the selected women’s shoes both matched the user instructions, these are not high-heel shoes. This indicates that LASER can make mistakes when encountering items with many matching details, and it would be interesting to see if a self-feedback/verification module can address this issue (Madaan et al., 2023). --- --D_ Prompts used in our experiments E Licenses The Webshop task and ReAct method are both released under MIT License. They are both released for research purposes, and our experiments are consistent with their intended usage. --- --You are an intelligent shopping assistant that can help users find the right item. You are given an observation of the current web navigation session, in the following format: Current Observation: WebShop Instruction: {the user instruction} [button] Search [button_] (generate a search query based on the user instruction and select this button to find relevant items) Every button in the observation represents a possible action you can take. Based on the current observation, your task is to generate a rationale about the next action you should take. Note that if an history of past rationales and actions is provided, you should also consider the history when generating the rationale. Table 4: The system instruction we used for the search state. You are an intelligent shopping assistant that can help users find the right item. You are given an observation of the current web navigation session, in the following format: Current Observation: Instruction: {the user instruction} button] Back to Search [button_] (select this button to go back to the search page) Page current page number (Total results: total number of results) button] Next > [button_] (select this button to go to the next page of results) button] {item_id 1} [button_] (select this button to view item 1’s details) {name of item 1} {price of item 1} button] {item_id 2} [button_] (select this button to view item 2’s details) {name of item 2} {price of item 2} button] {item_id 3} [button_] (select this button to view item 3’s details) {name of item 3} {price of item 3} {More items...} At this stage, you want to select an item that might match the user instruction. Note that even if an item has non-matching details with the user instruction, it might offer different customization options to allow you to match. E.g. an item may have color x in its name, but you can customize it to color y later, the customization options are shown after you select the item. Thus if an item name seems relevant or partially matches the instruction, you should select that item to check its details. If an item has been selected before (the button has been clicked), you should not select the same item again. In other words, do not select an item with [clicked button] item_id [clicked button_]. Prepare your response in the following format: Rationale: the user wanted {keywords of the target item}, and we have found {matching keywords of item x}, thus item {item_id x} seems to be a match. Table 5: The system instruction we used for the result state. --- --You are an intelligent shopping assistant that can help users find the right item. You are given an observation of the current web navigation session, in the following format: Current Observation: Instruction: {the user instruction} button] Back to Search [button_] (select this button to go back to the search page) button] < Prev [button_] (select this button to go back to the previous page of results) {Customization type}: button] option! [button_] button] option2 [button_] {Customization type2}: button] option! [button_] button] option2 [button_] {more customization options... (if any)} {Item name and details} button] Description [button_] (select this button to view the full description of the item) button] Features [button_] (select this button to view the full features of the item) button] Reviews [button_] (select this button to view the full reviews of the item) button] Buy Now [button_] (select this button to buy the item) description: (if this is shown, the description button should not be selected again) {full description of the item (if any) or ""None""} features: (if this is shown, the features button should not be selected again) {full features of the item (if any) or ""None""} reviews: (if this is shown, the reviews button should not be selected again) {full reviews of the item (if any) or ""None""} Target item details (what the user is looking for): keywords: {keywords of the target item} max_price: {the price of the item should not exceed this} At this stage, you want to verify if the item matches the user instruction. You should consider the available customization options when deciding whether an item matches the user instruction. If an item can be customized to match the user instruction, or if the customization options cover the user specification, it is also a good match. If the item does not match the user instruction and it does not provide enough customization options, you can go to previous page to view other items. You can also check the item’s description, features and reviews to view more details (Note that description, features and reviews could be ""None"", do not check them again if they are already given). Prepare your response in the following format: Rationale: the user wanted {keywords of the target item}, and they required the following customization options: {customization of the target item}, the item is keywords of the item in the current observation, and it has the following customization options: {options available for the current item}, which {cover}/ {not cover the user requirement}, thus we should {buy the item}/{check more details }/{ go to previous page to view other items } Table 6: The system instruction we used for the item state. --- --You are an intelligent shopping assistant that can help users find the right item. You are given an observation of the current environment and a rationale for the next action to be taken, in the following format: Current Observation: The observation layout from search or result or item state, as shown from Table 4, Table 5 and TableNext action rationale: {the rationale for the next action} Your task is to perform one of the function calls based on the rationale. Table 7: The system instruction we used for generating action from thought. State Available Actions Search {""name"": ""Search"", ""description"": ""Use this function to search for the target item in the inventory based on keywords"" } Result {""name"": ""select_item"", ""description"": ""Use this function to select one of the items from the search results and check its details""} {""name"": ""Next"", ""description"": ""Use this function to go to the next page of search results to view more items, if none of the items on the current page match the user instruction."" } {""name"": ""Back_to_Search"", ""description"": ""Use this function to go back to the initial search page. You should use this function only if you have browsed multiple pages of items and checked multiple items’ details in the history, and none of the items match the user instruction.""} Item {""name"": ""Description"", ""description"": ""Use this function to check the description of the item, if you are unsure if the item perfectly matches the user instruction""} {""name"": ""Features"", ""description"": ""Use this function to check the features of the item, if you are unsure if the item perfectly matches the user instruction"" } {""name"": ""Reviews"", ""description"": ""Use this function to check the reviews of the item, if you are unsure if the item perfectly matches the user instruction"" } {""name"": ""Buy_Now"", ""description"": ""Use this function to buy the current item, if the current item perfectly matches the user instruction.""} {""name"": ""Prev"", ""description"": ""Use this function to go back to the results page, if the current item does not match the user instruction ""} Table 8: The action space of our agent in each state. Each action is implemented as a function call following the guidelines from OpenAlI ’, additional parameters used in the function call are omitted here for brevity.
"	"--- ABSTRACT ---
대규모 언어 모델(LLM)은 웹 탐색과 같은 대화형 의사 결정 작업에 성공적으로 적용되었습니다. 이전 방법은 적절한 성능을 달성하는 반면, 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하여 오라클 궤적을 컨텍스트 내 예제로만 제공하여 모델이 환경에서 추론하는 방법을 안내합니다. 결과적으로 모델은 컨텍스트 내 예제에서 다루지 않는 더 어려운 시나리오(예: 실수)를 처리할 수 없어 최적이 아닌 성능이 발생했습니다. 이 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하여 LLM 에이전트가 작업을 완료하기 위한 작업을 수행하여 미리 정의된 상태 집합 사이를 전환하는 것을 제안합니다. 이 공식을 사용하면 유연한 역추적이 가능하여 모델이 오류에서 쉽게 복구할 수 있습니다. 제안된 LLM 에이전트를 WebShop 작업과 amazon.com에서 모두 State-Space ExploRation(LASER)으로 평가합니다. 실험 결과에 따르면 LASER는 이전 방법을 크게 능가하고 웹 탐색 작업에서 인간 성능과의 격차를 줄였습니다.
--- INTRODUCTION ---
GPT-(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)은 광범위한 자연어 이해(NLU) 작업에서 놀라운 성과를 달성했습니다(Brown 등, 2020; Ouyang 등, 2022; Wei 등, 2022). 최근 가상 홈 탐색(Yang 등, 2023), 텍스트 기반 게임(Lin 등, 2023) 또는 웹 탐색(Yao 등, 2023; Zhou 등, 2024)과 같은 대화형 의사 결정 작업에 적용되었습니다. 대화형 작업을 해결하기 위해 LLM을 활용하는 이전 방법은 종종 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 모델에 단계별 추론 방법을 가르치기 위해 몇 가지 오라클 궤적만 컨텍스트 내 예로 제공합니다(Yao 등, 2023; Lo 등, 2023). 즉, 다음 페이지 결과 검색 쿼리 항목 선택 검색 LLM 에이전트 항목 세부 정보 확인 검색으로 돌아가기 항목 거부 LLM 에이전트 완료 LLM 에이전트 구매 LLM 에이전트 성장 내역 최대 반복 횟수에 도달했을 때 항목 선택 그림 1: 웹숍 작업에서 LASER의 상태 전환 다이어그램. 실선 원은 상태를 나타내고 화살표는 가능한 상태 전환을 나타냅니다. 이 공식은 유연한 역추적을 가능하게 하고 앞으로만 가는 예제의 제한을 완화하여 모델이 익숙하지 않은 시나리오를 더 잘 처리하고 오류에서 복구할 수 있도록 합니다. 이러한 오라클 궤적의 모든 단계에서 올바른 작업이 선택됩니다. 모델이 테스트 시간에 예상치 못한 실수를 하면 복구 방법을 알 수 없기 때문에 최적이 아닌 성능으로 이어질 수 있습니다. 동시에 모든 가능한 시나리오를 포괄하기 위해 많은 컨텍스트 내 예제를 포함하는 것은 비용이 많이 들거나 비현실적입니다. 게다가 이전 방법은 모델이 프롬프트 시작 부분에서 가능한 작업을 정의하거나 LLM이 컨텍스트 내 예제에서 가능한 작업을 자동으로 알아낼 것으로 기대하기 때문에 모든 단계에서 모든 작업을 수행할 수 있는 글로벌 작업 공간을 가정합니다. 이는 작업의 난이도를 더욱 높일 수 있으며, LLM은 특정 경우에 잘못된 동작을 수행할 수 있습니다. 앞서 언급한 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하는 것을 제안합니다. 먼저 LLM 에이전트가 작업 실행 중에 마주칠 수 있는 고수준 가능한 상태 집합을 정의합니다. 그런 다음 각 상태에서 가능한 동작 공간과 각 동작을 수행한 후의 결과 상태를 식별합니다. 이 공식은 대화형 작업에서 LLM 에이전트의 탐색을 상태 전환으로 효과적으로 변환합니다. 각 동작은 에이전트를 한 상태에서 다른 상태로 전환합니다. 당연히 이를 통해 에이전트는 잘못된 동작에서 쉽게 복구할 수 있습니다. 즉, 이전 상태로 되돌릴 수 있는 다른 동작을 수행합니다. 게다가 제안된 공식은 동작 공간을 각 개별 상태와 연관시켜 작업의 난이도를 줄이고 에이전트가 모든 단계에서 항상 유효한 동작을 선택할 수 있도록 합니다. 제안된 LASER on the Webshop(Yao et al., 2022) 작업을 평가하고 LASER를 amazon.com에 직접 적용한 시뮬레이션-실제 전송 실험을 수행했습니다. 우리는 제안된 설정이 에이전트가 컨텍스트 내 예제를 사용하지 않고도 복잡한 사용자 지침을 완료할 수 있게 하고, LASER가 이전의 모든 기준선보다 상당히 우수한 성능을 보이며 인간 성능과의 격차를 줄인다는 것을 보여줍니다.2 방법 2.1 문제 공식화 웹 환경 E와 사용자 지침 I가 주어지면 에이전트는 환경에서 인스턴스화되고 초기 관찰 Oo가 제공됩니다. 에이전트는 사용자 지침을 완료하기 위해 일련의 작업 {a0, a1, ...an}을 수행해야 하며, 각 ai는 환경에서 실행될 때 새로운 관찰 O를 생성합니다.S는 에이전트가 출력을 생성하고 이에 도달한 후 탐색을 중단하는 중지 상태를 나타냅니다.마지막으로 에이전트의 출력을 대상과 비교하여 메트릭을 계산합니다.2.2 LLM 에이전트 이전에 논의한 대로, 에이전트는 많은 수의 컨텍스트 내 예제를 통해 이를 철저히 설명하지 않고도 실행 중에 발생할 수 있는 새로운 상황이나 실수를 처리할 수 있기를 원합니다.따라서 LLM 에이전트에 상태 추적 기능을 장착할 것을 제안합니다. 그림 1은 에이전트의 상태 전환 다이어그램을 보여줍니다. 먼저 에이전트가 환경에서 마주칠 수 있는 가능한 상위 수준 상태 집합을 정의합니다(§2.3). LLM 에이전트는 사용자 입력을 전반적인 목표로 받아들이고 시작 상태에서 초기화됩니다. 모든 단계에서 에이전트는 상태별 시스템 지침, 현재 관찰, 현재 상태에서 허용되는 동작 집합, 과거 생각과 동작의 기록을 입력으로 받습니다. 그런 다음 출력으로 동작 중 하나를 선택하여 에이전트를 다른 상태로 전환하거나 동일한 상태를 유지합니다(§2.4). 에이전트는 중지 상태 또는 최대 단계에 도달할 때까지 프로세스를 반복합니다. 우리의 공식화를 사용하면 모든 상태에서 가능한 상황과 이를 처리하는 방법을 에이전트에게 알리는 자세한 지침을 제공할 수 있습니다. 예를 들어 그림 1에서 볼 수 있듯이 결과 상태에서 현재 결과는 충분할 수도 있고 그렇지 않을 수도 있으며, 에이전트에게 판단에 따라 항목을 선택하거나 다음 페이지로 이동하거나 검색으로 돌아가도록 지시합니다. 따라서 이러한 지침은 문맥 내 예제보다 훨씬 효율적이면서도 에이전트를 안내하는 데 매우 유익할 수 있습니다. 다음으로 상태 및 작업 공간을 설계하는 방법을 자세히 설명합니다. 2.3 상태 설명 저희 작업에서 상태라는 용어를 사용하여 에이전트가 있는 현재 환경을 설명하고 현재 환경 관찰의 구조가 다른 경우에만 에이전트가 두 가지 다른 상태에 있는 것으로 간주합니다. 이를 통해 복잡한 환경에서 에이전트의 탐색을 완전히 지원하기 위해 소수의 상태만 정의할 수 있습니다. 대화형 작업에서 가능한 모든 상태를 수동으로 분류한 후 각 상태에 대해 상태를 자세히 설명하는 일반 지침을 작성합니다. 구체적으로 해당 상태에서 에이전트가 수신할 관찰의 샘플 레이아웃을 제공하고 레이아웃의 모든 사양을 플레이스홀더로 바꿉니다. 또한 해당 상태에서 행동하기 위한 상위 목표와 자세한 지침을 제공합니다. 상태별 지침과 결합된 샘플 레이아웃을 통해 에이전트에게 수신할 수 있는 가능한 관찰과 그에 따라 행동하는 방법을 알릴 수 있습니다. 따라서 더 이상 에이전트를 안내하기 위해 문맥 내 예제를 제공할 필요가 없습니다. WebShop 작업의 경우 총 4개의 상태를 정의하고 검색, 결과 및 항목 상태에 대한 전체 프롬프트는 부록의 표 4, 표 5 및 표 6에서 확인할 수 있습니다.2.4 동작 공간 이전 방법은 종종 암묵적으로 모델에 대한 글로벌 동작 공간을 가정합니다.즉, 모델은 추가 제약 없이 모든 동작을 자유롭게 수행할 수 있습니다.LLM은 대부분의 경우 유효한 동작을 파악할 수 있지만 특정 경우에는 여전히 유효하지 않은 동작을 시도할 수 있습니다.따라서 작업에 대한 모든 가능한 상태를 정의한 후 각 상태에 대한 동작 공간을 추가로 식별하여 이러한 가능성을 배제합니다.특히 에이전트가 각 상태에 대해 선택할 수 있는 허용 동작 세트를 정의하여 에이전트가 항상 유효한 동작을 수행하도록 합니다.에이전트의 상태-동작 매핑은 부록의 표 8에 나와 있습니다.실제로 허용되는 동작은 휴리스틱 방식으로도 결정할 수 있습니다.예: 웹페이지에서 클릭 가능한 모든 버튼을 식별하는 것입니다.성공률 보상 SR 보상 참여 옵션 유형. ASH(Lo et al., 2023) 30.56.LASER ReAct(Yao et al., 2023)* 40.66.Human(Yao et al., 2022) 62.0 85.65.0 88.85.5 75.0 97.86.2 76.3 99.ReAct(저희 재실행) 34.59.표 2: Amazon.com의 결과. WebGUM(Furuta et al., 2023) 45.67.LASER 백업 48.71.LASER 50.75.Human Expert(Yao et al., 2022) 59.82.Success Rate Reward LASER 52.77.LASER + One-shot 50.74.LASER LASER(text-davinci-003) 함수 호출 50.76.38.70.표 1: WebShop 작업의 결과.*단순화된 설정 ReAct 방법(Yao et al., 2023)에서 영감을 얻어 에이전트에게 모든 단계에서 생각을 생성한 다음 생각에 따라 행동을 선택하도록 요청합니다. 에이전트는 정지 상태 또는 최대 단계에 도달할 때까지 생각-행동 프로세스를 계속 반복합니다. 또한 탐색 중에 중간 결과(조사했지만 일치하지 않는 것으로 간주된 항목)를 저장하기 위한 메모리 버퍼를 정의합니다. 이것은 사람들이 일반적으로 원하는 항목을 찾기 전에 몇 가지 백업 옵션을 찾는다는 점에서 인간의 행동과 유사합니다. 에이전트가 최대 단계 수 후에 강제로 중지해야 할 때 최종 출력으로 중간 결과 중 하나를 선택하고 이를 백업 전략이라고 합니다. 3 실험 우리는 WebShop 작업(Yao et al., 2022)에 대한 실험을 수행합니다. 우리는 평가를 위해 500개의 테스트 세트 지침을 사용했고 이전 연구(Yao et al., 2022)에 따라 보상과 성공률을 메트릭으로 채택했습니다. 우리는 GPT-4-0613을 사용하여 LASER와 해당 함수 호출 기능을 구동하여 작업 선택 단계를 구현했습니다. 우리는 다음 기준선과 비교합니다. ReAct(Yao et al., 2023)는 대화형 의사 결정 작업을 위해 설계된 프롬프트 방법입니다. 모든 단계에서 LLM 에이전트는 관찰을 받고 생각이나 행동을 생성할 수 있습니다. 에이전트는 컨텍스트 내 예로 전체 탐색 궤적을 사용하여 프롬프트에 모든 과거 관찰, 생각 및 행동을 축적합니다. 원래 ReAct는 LLM 백본으로 PaLM(Chowdhery et al., 2023)을 사용합니다. 공정한 비교를 위해 GPT-4-0613으로 ReAct 방법을 다시 실행합니다. ASH(Lo et al., 2023)는 ReAct를 기반으로 하며 에이전트 관찰을 요약하고 요약된 정보를 기반으로 행동하는 요약 단계를 추가합니다. WebGUM(Furuta et al., 2023)은 WebShop에서 제공한 1,000명의 인간 시연에서 FlanT5-XL 모델(Chung et al., 2022)을 미세 조정하는 지도 학습 방법입니다. 표 3: WebShop 작업의 절제 결과. 표준 LASER는 zero-shot 작업에서 GPT-4로 구동됩니다. 게다가, 수정 없이 amazon.com에 LASER를 직접 적용하는 시뮬레이션-실제 전송 실험을 실험했습니다. Yao et al.과 동일한 설정을 따릅니다. (2022) 100개의 테스트 세트 지침에 대해 평가한 다음 수동으로 결과를 평가했습니다. 더 자세한 실험 설정은 부록 B에서 설명합니다. 4 결과 실험의 전체 결과는 표 1에 나와 있습니다. 초기 실험에서 ReAct 에이전트가 종종 잘못된 동작을 생성한다는 것을 보여주었습니다. 예를 들어, 지침과 일치하지 않는 항목을 선택하면 결과 페이지로 돌아가기 전에 다음 페이지 버튼(존재하지 않음)을 클릭하려고 합니다. 또한 ReAct 에이전트는 종종 특정 동작에 갇혀 출력을 생성하지 못했습니다. 예를 들어, 에이전트는 최대 단계에 도달할 때까지 다음 페이지로 계속 이동합니다. 문제를 해결하기 위해 자세한 지침을 시스템 프롬프트로 추가했습니다. 최선을 다했지만 에이전트는 여전히 어떤 경우에는 잘못된 동작을 하고 원래 논문보다 더 나쁜 결과를 얻습니다. 반면 LASER는 두 가지 지표에서 모두 기준선보다 큰 차이로 성능이 뛰어나 접근 방식의 효과를 보여줍니다. ReAct와 더 공정한 비교를 하기 위해 LASER의 백업 전략(최대 예산이 소진되면 에이전트가 0점을 받음)을 추가로 제거했습니다. 우리는 여전히 우리의 방법이 기준선보다 매우 큰 마진으로 더 나은 성과를 거두고 있음을 알 수 있습니다.전이 실험의 결과는 표 2에 나와 있습니다.다시 말하지만, LASER는 인간의 성과와 비교해 매우 가까운 결과를 얻습니다.LASER가 amazon.com의 강력한 검색 엔진 덕분에 웹숍보다 이 현실적인 환경에서 더 나은 성과를 거두었다는 것도 고무적입니다.보상 성공률4-7-10-3 4-6 7-9 10-132% 9% 5% 9% 75%그림 2: 왼쪽: 길이가 다른 테스트 세트 에피소드에 대한 LASER의 성과.오른쪽: LASER가 작업을 완료하는 데 걸리는 단계 수의 분포 4.1 분석 우리는 먼저 에이전트의 중요한 디자인 결정을 이해하기 위해 절제 연구를 수행합니다.제로샷 대 퓨샷 우리는 에이전트가 환경에서 탐색하도록 안내하기 위해 상태별 지침만 사용했지만, 이전 연구에서는 종종 맥락 내 예를 채택했습니다. 에이전트가 컨텍스트 내 예제에서 더 많은 이점을 얻을 수 있는지 알아보기 위해 원샷 설정을 실험했습니다.LASER의 모든 프롬프트에 대해 시스템 명령어와 현재 입력 사이에 하나의 예제 입출력 쌍을 추가하고 나머지 에이전트는 동일하게 유지했습니다.제한된 컴퓨팅 예산으로 인해 200개 명령어에 대해서만 절제 연구를 실행했습니다.결과는 표 3에 나와 있습니다.컨텍스트 내 예제를 추가하면 실제로 성능이 저하되는 것을 알 수 있습니다.LASER는 이미 유효한 작업을 100% 수행하므로 에이전트가 컨텍스트 내 예제 없이도 작업을 잘 이해하고 추가된 예제가 실제로 어떤 경우에는 에이전트의 주의를 산만하게 한다고 가정합니다.함수 호출의 효과 LASER는 06/13/23 이후 GPT 모델에서만 활성화된 함수 호출 기능을 활용합니다.따라서 이 디자인을 일반 텍스트 생성으로 대체하는 효과를 보고 싶습니다.이를 위해 허용되는 작업을 함수 목록으로 전달하는 대신 각 작업을 목적과 인수를 설명하는 Python 사전으로 변환한 다음 프롬프트에 추가합니다. 그런 다음 LLM에 JSON 형식으로 출력을 생성하여 적절한 인수로 선택한 작업을 나타내도록 요청합니다. 결과는 표 3에 나와 있습니다. 다시 말하지만, 함수 호출이 없는 에이전트는 이 200개 에피소드에서 약간 더 나쁜 성능을 보였습니다. 이는 함수 호출 기능을 활용하여 대화형 에이전트를 빌드할 때 성능을 높일 수 있음을 보여주며, 향후 LLM을 빌드하는 방향을 제시합니다. 성능 대 궤적 길이 여기서는 LASER 궤적의 길이와 전체 성능에 미치는 영향을 확인하는 데 관심이 있습니다. 그림에서 궤적 길이의 분포와 각 길이 그룹에 대한 에이전트의 성능을 표시합니다. 대부분의 경우 에이전트는 검색-선택-구매인 완료 상태에 도달하기 위해 세 가지 상태 전환만 수행했다는 점에 주목합니다. 왼쪽 그림에서 에이전트의 성능은 일반적으로 궤적이 길어질수록 감소합니다. 그러나 이러한 감소는 ReAct 및 ASH 에이전트(Lo et al., 2023)에 대한 관찰에 비해 덜 심각하여 에이전트의 효과성을 더욱 잘 보여줍니다. 마지막으로, 에이전트가 검색 기록에서 중지하고 선택해야 하는 길이 15 그룹의 경우 성능이 다른 그룹보다 훨씬 낮습니다. 놀랍지 않지만 성공률이 0이 아니어서 에이전트가 일치하는 항목을 찾았지만 첫 번째 패스에서 대상으로 인식하지 못한 경우가 있음을 보여줍니다. 다른 LLM에 대한 일반화 LASER가 덜 강력한 비채팅 모델로 잘 전환될 수 있는지 확인하기 위해 text-davinci-003 모델을 채택했습니다. 이 모델은 함수 호출을 지원하지 않으므로 이전에 설명한 접근 방식을 채택하여 모델에 동작을 나타내는 JSON 출력을 생성하도록 했습니다. 결과는 표 3에 나와 있습니다. text-davinci-003으로 전환하면 성능이 크게 떨어지지만 모델은 여전히 기준선보다 더 나은 결과를 얻습니다. 제안된 에이전트가 다른 기능을 가진 다른 LLM에 쉽게 적용될 수 있음을 보여줍니다. 앞으로 더 강력한 모델이 있으면 에이전트가 이 작업에서 인간의 성능을 능가할 가능성이 있습니다. 또한 LASER의 실패 모드를 검사하기 위한 사례 연구를 수행했으며 추가 결과는 부록 C에 나와 있습니다. 논의합니다.
--- RELATED WORK ---
부록 A의 s. 5 결론 우리는 상태 공간 탐색으로 대화형 웹 탐색 작업을 모델링하는 LLM 에이전트인 LASER를 제안했습니다. 우리의 공식화는 에이전트가 새로운 상황을 처리하고, 실수에서 쉽게 후퇴하고, 항상 유효한 작업을 수행할 수 있게 합니다. 문맥 내 예 없이 상태별 지침에 의해서만 안내되는 LASER는 WebShop 작업에서 모든 기준선을 큰 차이로 능가하고 실제 쇼핑 웹사이트에서 인간의 성능과의 격차를 줄입니다. 우리의 분석에 따르면 LASER는 더 긴 궤적에도 더 강력하고 다른 LLM에 잘 일반화됩니다. 제한 사항 이 작업에서는 쇼핑 도메인의 대상 항목을 찾는 작업만 실험했습니다. 어려운 특성에도 불구하고 주문 추적이나 주문 내역 확인 등 전자 상거래 웹사이트에서 사용자가 일반적으로 수행하는 모든 작업을 다루지는 않습니다. 향후 작업에서는 쇼핑 도메인에서 이러한 인기 있는 작업을 처리할 수 있도록 LASER의 기능을 향상시키는 것이 흥미로울 것입니다. 또한 LASER에 지식 검색기(Ma et al., 2023)나 계산기(Gao et al., 2023)와 같은 더 많은 도구를 장착하여 더 복잡한 명령을 처리할 수 있도록 하는 것도 흥미로울 것입니다. 저희 LASER는 환경의 가능한 상태와 그에 해당하는 설명에 대한 수동 주석이 필요합니다. 이 때문에 저희
--- METHOD ---
s는 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 오라클 궤적을 컨텍스트 내 예제로만 제공하여 모델이 환경에서 추론하는 방법을 안내합니다. 결과적으로 모델은 컨텍스트 내 예제에서 다루지 않은 더 어려운 시나리오(예: 실수)를 처리할 수 없어 최적이 아닌 성능이 발생합니다. 이 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하여 LLM 에이전트가 작업을 완료하기 위한 작업을 수행하여 미리 정의된 상태 집합 사이를 전환하는 것을 제안합니다. 이 공식을 사용하면 유연한 역추적이 가능하여 모델이 오류에서 쉽게 복구할 수 있습니다. 제안된 LLM 에이전트를 WebShop 작업과 amazon.com 모두에서 State-Space ExploRation(LASER)으로 평가합니다.
--- EXPERIMENT ---
모든 결과에 따르면 LASER는 이전 방법보다 상당히 우수한 성능을 보이며 웹 탐색 작업에서 인간 성능과의 격차를 줄였습니다. 서론 GPT-(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)은 광범위한 자연어 이해(NLU) 작업에서 놀라운 성능을 달성했습니다(Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). 최근에는 가상 홈 탐색(Yang et al., 2023), 텍스트 기반 게임(Lin et al., 2023) 또는 웹 탐색(Yao et al., 2023; Zhou et al., 2024)과 같은 대화형 의사 결정 작업에 적용되었습니다. LLM을 사용하여 대화형 작업을 해결하는 이전 방법은 종종 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 모델에 단계별 추론 방법을 가르치기 위해 몇 가지 오라클 궤적을 컨텍스트 내 예로만 제공합니다(Yao et al., 2023; Lo et al., 2023). 즉, 다음 페이지 결과 검색 쿼리 항목 선택 검색 LLM 에이전트 항목 세부 정보 확인 검색으로 돌아가기 항목 거부 LLM 에이전트 완료 구매 LLM 에이전트 LLM 에이전트 성장 내역 최대 반복 횟수에 도달했을 때 항목 선택 그림 1: 웹숍 작업에서 LASER의 상태 전환 다이어그램. 실선 원은 상태를 나타내고 화살표는 가능한 상태 전환을 나타냅니다. 이 공식은 유연한 역추적을 가능하게 하고 전방 전용 예제의 제한을 완화하여 모델이 익숙하지 않은 시나리오를 더 잘 처리하고 오류에서 복구할 수 있도록 합니다. 올바른 작업은 해당 오라클 궤적의 모든 단계에서 선택됩니다. 모델이 테스트 시간에 예상치 못한 실수를 저지르면 복구 방법을 알 수 없기 때문에 최적이 아닌 성능으로 이어질 수 있습니다. 동시에, 모든 가능한 시나리오를 포괄하기 위해 많은 컨텍스트 내 예제를 포함하는 것은 비용이 많이 들거나 비현실적입니다. 게다가, 이전 방법은 모델이 프롬프트의 시작 부분에서 가능한 동작을 정의하거나 LLM이 컨텍스트 내 예제에서 가능한 동작을 자동으로 알아낼 것으로 기대하기 때문에 모든 단계에서 모든 동작을 자유롭게 수행할 수 있는 글로벌 동작 공간을 가정합니다. 이는 작업의 난이도를 더욱 높일 수 있으며, LLM은 특정 경우에 잘못된 동작을 수행할 수 있습니다. 앞서 언급한 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하는 것을 제안합니다. 먼저 LLM 에이전트가 작업 실행 중에 마주칠 수 있는 상위 수준 가능한 상태 집합을 정의합니다. 그런 다음 각 상태에서 가능한 동작 공간과 각 동작을 수행한 후의 결과 상태를 식별합니다. 이 공식은 대화형 작업에서 LLM 에이전트의 탐색을 상태 전환으로 효과적으로 변환합니다. 여기서 각 동작은 에이전트를 한 상태에서 다른 상태로 옮깁니다. 당연히 이를 통해 에이전트는 잘못된 동작에서 쉽게 복구할 수 있습니다. 즉, 이전 상태로 되돌릴 수 있는 다른 동작을 수행합니다. 또한, 제안된 공식은 각 개별 상태와 액션 공간을 연관시켜 작업의 난이도를 줄이고 에이전트가 모든 단계에서 항상 유효한 액션을 선택할 수 있도록 합니다.웹숍(Yao et al., 2022) 작업에서 제안된 LASER를 평가하고 amazon.com에 LASER를 직접 적용한 시뮬레이션-실제 전송 실험을 수행했습니다.제안된 설정을 통해 에이전트가 컨텍스트 내 예제를 사용하지 않고도 복잡한 사용자 지침을 완료할 수 있으며 LASER가 이전의 모든 기준선을 크게 능가하고 인간 성능과의 격차를 줄인다는 것을 보여줍니다.2 방법 2.1 문제 공식화 웹 환경 E와 사용자 지침 I가 주어지면 에이전트는 환경에서 인스턴스화되고 초기 관찰 Oo가 제공됩니다.에이전트는 사용자 지침을 완료하기 위해 일련의 작업 {a0, a1, ...an}을 수행해야 하며, 각 ai는 환경에서 실행될 때 새로운 관찰 O를 생성합니다.S는 에이전트가 출력을 생성하고 도달한 후 탐색을 중단하는 중지 상태를 나타냅니다.마지막으로 에이전트의 출력을 대상과 비교하여 메트릭을 계산합니다. 2.2 LLM 에이전트 이전에 논의했듯이, 우리는 에이전트가 실행 중에 발생할 수 있는 새로운 상황이나 실수를 많은 수의 맥락 내 예제를 통해 철저히 설명하지 않고도 처리할 수 있기를 원합니다. 따라서 우리는 LLM 에이전트에 상태 추적 기능을 제공하기로 제안합니다. 에이전트의 상태 전환 다이어그램이 그림 1에 나와 있습니다. 우리는 에이전트가 환경에서 마주칠 수 있는 가능한 상위 수준 상태 집합을 정의하는 것으로 시작합니다(§2.3). LLM 에이전트는 사용자 입력을 전반적인 목표로 받아들이고 시작 상태에서 초기화됩니다. 모든 단계에서 에이전트는 상태별 시스템 지침, 현재 관찰, 현재 상태에서 허용되는 작업 집합, 과거 생각과 작업의 기록을 입력으로 받습니다. 그런 다음 에이전트를 다른 상태로 전환하거나 동일한 상태를 유지하는 작업 중 하나를 출력으로 선택합니다(§2.4). 에이전트는 중지 상태 또는 최대 단계에 도달할 때까지 프로세스를 반복합니다. 우리의 공식화를 사용하면 모든 상태에서 가능한 상황과 이를 처리하는 방법을 에이전트에 알리는 자세한 지침을 제공할 수 있습니다. 예를 들어, 그림 1에서 볼 수 있듯이 결과 상태에서 현재 결과는 충분히 좋을 수도 있고 좋지 않을 수도 있으며, 에이전트에게 판단에 따라 항목을 선택하거나, 다음 페이지로 이동하거나, 검색으로 돌아가도록 지시합니다. 따라서 이러한 지침은 문맥 없는 예보다 훨씬 효율적이면서도 에이전트를 안내하는 데 매우 유익할 수 있습니다. 다음으로, 상태와 액션 공간을 설계하는 방법을 자세히 설명합니다. 2.3 상태 설명 저희 작업에서 상태라는 용어를 사용하여 에이전트가 있는 현재 환경을 설명하고, 현재 환경 관찰의 구조가 다른 경우에만 에이전트가 두 가지 다른 상태에 있는 것으로 간주합니다. 이를 통해 복잡한 환경에서 에이전트의 탐색을 완벽하게 지원하기 위해 소수의 상태만 정의할 수 있습니다. 대화형 작업에서 가능한 모든 상태를 수동으로 분류한 후 각 상태에 대해 상태를 자세히 설명하는 일반 지침을 작성합니다. 구체적으로, 해당 상태에서 에이전트가 수신할 관찰의 샘플 레이아웃을 제공하고 레이아웃의 모든 사양을 플레이스홀더로 바꿉니다. 또한 해당 상태에서 행동하기 위한 상위 수준 목표와 자세한 지침을 제공합니다. 상태별 지침과 결합된 샘플 레이아웃을 통해 에이전트에게 수신할 수 있는 가능한 관찰과 그에 따른 조치를 알릴 수 있습니다.따라서 더 이상 에이전트를 안내하기 위해 컨텍스트 내 예를 제공할 필요가 없습니다.WebShop 작업의 경우 총 4개의 상태를 정의하고 검색, 결과 및 항목 상태에 대한 전체 프롬프트는 부록의 표 4, 표 5 및 표 6에서 찾을 수 있습니다.2.4 액션 공간 이전 방법은 종종 모델에 대한 글로벌 액션 공간을 암묵적으로 가정합니다.즉, 모델은 추가 제약 없이 모든 액션을 수행할 수 있습니다.LLM은 대부분의 경우 유효한 액션을 파악할 수 있지만 특정 경우에는 여전히 유효하지 않은 액션을 수행하려고 시도할 수 있습니다.따라서 작업에 대한 모든 가능한 상태를 정의한 후 각 상태에 대한 액션 공간을 추가로 식별하여 이러한 가능성을 배제합니다.특히 에이전트가 각 상태에 대해 선택할 수 있는 허용 가능한 액션 세트를 정의하여 에이전트가 항상 유효한 액션을 수행하도록 합니다.에이전트의 상태-액션 매핑은 부록의 표 8에 나와 있습니다. 실제로 허용되는 작업은 휴리스틱하게 결정할 수도 있습니다.예를 들어, 웹페이지에서 클릭 가능한 모든 버튼을 식별합니다.성공률 보상 SR 보상 Att.Opt.유형.ASH(Lo et al., 2023) 30.56.LASER ReAct(Yao et al., 2023)* 40.66.Human(Yao et al., 2022) 62.0 85.65.0 88.85.5 75.0 97.86.2 76.3 99.ReAct(저희가 다시 실행) 34.59.표 2: Amazon.com의 결과. WebGUM(Furuta et al., 2023) 45.67.LASER 백업 48.71.LASER 50.75.Human Expert(Yao et al., 2022) 59.82.Success Rate Reward LASER 52.77.LASER + One-shot 50.74.LASER LASER(text-davinci-003) 함수 호출 50.76.38.70.표 1: WebShop 작업의 결과.*단순화된 설정 ReAct 방법(Yao et al., 2023)에서 영감을 얻어 에이전트에게 모든 단계에서 생각을 생성한 다음 생각에 따라 행동을 선택하도록 요청합니다. 에이전트는 정지 상태 또는 최대 단계에 도달할 때까지 생각-행동 프로세스를 계속 반복합니다. 또한 탐색 중에 중간 결과(조사했지만 일치하지 않는 것으로 간주된 항목)를 저장하기 위한 메모리 버퍼를 정의합니다. 이것은 사람들이 일반적으로 원하는 항목을 찾기 전에 몇 가지 백업 옵션을 찾는다는 점에서 인간의 행동과 유사합니다. 에이전트가 최대 단계 수 후에 강제로 중지해야 할 때 최종 출력으로 중간 결과 중 하나를 선택하고 이를 백업 전략이라고 합니다. 3 실험 우리는 WebShop 작업(Yao et al., 2022)에 대한 실험을 수행합니다. 우리는 평가를 위해 500개의 테스트 세트 지침을 사용했고 이전 연구(Yao et al., 2022)에 따라 보상과 성공률을 메트릭으로 채택했습니다. 우리는 GPT-4-0613을 사용하여 LASER와 해당 함수 호출 기능을 구동하여 작업 선택 단계를 구현했습니다. 우리는 다음 기준선과 비교합니다. ReAct(Yao et al., 2023)는 대화형 의사 결정 작업을 위해 설계된 프롬프트 방법입니다. 모든 단계에서 LLM 에이전트는 관찰을 받고 생각이나 행동을 생성할 수 있습니다. 에이전트는 컨텍스트 내 예로 전체 탐색 궤적을 사용하여 프롬프트에 모든 과거 관찰, 생각 및 행동을 축적합니다. 원래 ReAct는 LLM 백본으로 PaLM(Chowdhery et al., 2023)을 사용합니다. 공정한 비교를 위해 GPT-4-0613으로 ReAct 방법을 다시 실행합니다. ASH(Lo et al., 2023)는 ReAct를 기반으로 하며 에이전트 관찰을 요약하고 요약된 정보를 기반으로 행동하는 요약 단계를 추가합니다. WebGUM(Furuta et al., 2023)은 WebShop에서 제공한 1,000명의 인간 시연에서 FlanT5-XL 모델(Chung et al., 2022)을 미세 조정하는 지도 학습 방법입니다. 표 3: WebShop 작업의 절제 결과. 표준 LASER는 zero-shot 작업에서 GPT-4로 구동됩니다. 게다가, 수정 없이 amazon.com에 LASER를 직접 적용하는 시뮬레이션-실제 전송 실험을 실험했습니다. Yao et al.과 동일한 설정을 따릅니다. (2022) 100개의 테스트 세트 지침에 대해 평가한 다음 수동으로 결과를 평가했습니다. 더 자세한 실험 설정은 부록 B에서 설명합니다. 4 결과 실험의 전체 결과는 표 1에 나와 있습니다. 초기 실험에서 ReAct 에이전트가 종종 잘못된 동작을 생성한다는 것을 보여주었습니다. 예를 들어 지침과 일치하지 않는 항목을 선택하면 결과 페이지로 돌아가기 전에 다음 페이지 버튼(존재하지 않음)을 클릭하려고 합니다. 또한 ReAct 에이전트는 종종 특정 동작에 갇혀 출력을 생성하지 못했습니다. 예를 들어 에이전트는 최대 단계에 도달할 때까지 다음 페이지로 계속 이동합니다. 문제를 해결하기 위해 자세한 지침을 시스템 프롬프트로 추가했습니다. 최선을 다했지만 에이전트는 여전히 어떤 경우에는 잘못된 동작을 하고 원래 논문보다 더 나쁜 결과를 얻습니다. 반면 LASER는 두 가지 지표에서 모두 기준선보다 큰 폭으로 성능이 뛰어나 접근 방식의 효과를 보여줍니다. ReAct와 더 공정한 비교를 하기 위해 LASER의 백업 전략(최대 예산이 소진되면 에이전트가 0점을 받음)을 추가로 제거했습니다. 우리는 여전히 우리의 방법이 기준선보다 매우 큰 마진으로 더 나은 성과를 거두고 있음을 알 수 있습니다.전이 실험의 결과는 표 2에 나와 있습니다.다시 말하지만, LASER는 인간의 성과와 비교해 매우 가까운 결과를 얻습니다.LASER가 amazon.com의 강력한 검색 엔진 덕분에 웹숍보다 이 현실적인 환경에서 더 나은 성과를 거두었다는 것도 고무적입니다.보상 성공률4-7-10-3 4-6 7-9 10-132% 9% 5% 9% 75%그림 2: 왼쪽: 길이가 다른 테스트 세트 에피소드에 대한 LASER의 성과.오른쪽: LASER가 작업을 완료하는 데 걸리는 단계 수의 분포 4.1 분석 우리는 먼저 에이전트의 중요한 디자인 결정을 이해하기 위해 절제 연구를 수행합니다.제로샷 대 퓨샷 우리는 에이전트가 환경에서 탐색하도록 안내하기 위해 상태별 지침만 사용했지만, 이전 연구에서는 종종 맥락 내 예를 채택했습니다. 에이전트가 컨텍스트 내 예제에서 더 많은 이점을 얻을 수 있는지 알아보기 위해 원샷 설정을 실험했습니다.LASER의 모든 프롬프트에 대해 시스템 명령어와 현재 입력 사이에 하나의 예제 입출력 쌍을 추가하고 나머지 에이전트는 동일하게 유지했습니다.제한된 컴퓨팅 예산으로 인해 200개 명령어에 대해서만 절제 연구를 실행했습니다.결과는 표 3에 나와 있습니다.컨텍스트 내 예제를 추가하면 실제로 성능이 저하되는 것을 알 수 있습니다.LASER는 이미 유효한 작업을 100% 수행하므로 에이전트가 컨텍스트 내 예제 없이도 작업을 잘 이해하고 추가된 예제가 실제로 어떤 경우에는 에이전트의 주의를 산만하게 한다고 가정합니다.함수 호출의 효과 LASER는 06/13/23 이후 GPT 모델에서만 활성화된 함수 호출 기능을 활용합니다.따라서 이 디자인을 일반 텍스트 생성으로 대체하는 효과를 보고 싶습니다.이를 위해 허용되는 작업을 함수 목록으로 전달하는 대신 각 작업을 목적과 인수를 설명하는 Python 사전으로 변환한 다음 프롬프트에 추가합니다. 그런 다음 LLM에 JSON 형식으로 출력을 생성하여 적절한 인수로 선택한 작업을 나타내도록 요청합니다. 결과는 표 3에 나와 있습니다. 다시 말하지만, 함수 호출이 없는 에이전트는 이 200개 에피소드에서 약간 더 나쁜 성능을 보였습니다. 이는 함수 호출 기능을 활용하여 대화형 에이전트를 빌드할 때 성능을 높일 수 있음을 보여주며, 향후 LLM을 빌드하는 방향을 제시합니다. 성능 대 궤적 길이 여기서는 LASER 궤적의 길이와 전체 성능에 미치는 영향을 확인하는 데 관심이 있습니다. 그림에서 궤적 길이의 분포와 각 길이 그룹에 대한 에이전트의 성능을 표시합니다. 대부분의 경우 에이전트는 검색-선택-구매인 완료 상태에 도달하기 위해 세 가지 상태 전환만 수행했다는 점에 주목합니다. 왼쪽 그림에서 에이전트의 성능은 일반적으로 궤적이 길어질수록 감소합니다. 그러나 이러한 감소는 ReAct 및 ASH 에이전트(Lo et al., 2023)에 대한 관찰에 비해 덜 심각하여 에이전트의 효과성을 더욱 잘 보여줍니다. 마지막으로, 에이전트가 검색 기록에서 중지하고 선택해야 하는 길이 15 그룹의 경우 성능이 다른 그룹보다 훨씬 낮습니다. 놀랍지 않지만 성공률이 0이 아니어서 에이전트가 일치하는 항목을 찾았지만 첫 번째 패스에서 대상으로 인식하지 못한 경우가 있음을 보여줍니다. 다른 LLM에 대한 일반화 text-davinci-003 모델을 채택하여 LASER가 덜 강력한 비채팅 모델로 잘 전환될 수 있는지 확인했습니다. 이 모델은 함수 호출을 지원하지 않으므로 이전에 설명한 접근 방식을 채택하여 모델에 동작을 나타내는 JSON 출력을 생성하도록 했습니다. 결과는 표 3에 나와 있습니다. text-davinci-003으로 전환하면 성능이 크게 떨어지지만 모델은 여전히 기준선보다 더 나은 결과를 얻습니다. 제안된 에이전트가 다른 기능을 가진 다른 LLM에 쉽게 적용될 수 있음을 보여줍니다. 앞으로 더 강력한 모델이 있으면 에이전트가 이 작업에서 인간의 성능을 능가할 가능성이 있습니다. 또한 우리는 LASER의 실패 모드를 검사하기 위한 사례 연구를 수행했으며 추가 결과는 부록 C에 있습니다. 관련 작업은 부록 A에서 논의합니다. 5
--- CONCLUSION ---
s 우리는 상태 공간 탐색으로 대화형 웹 탐색 작업을 모델링하는 LLM 에이전트인 LASER를 제안했습니다. 우리의 공식화는 에이전트가 새로운 상황을 처리하고, 실수에서 쉽게 후퇴하고, 항상 유효한 작업을 수행할 수 있게 합니다. 문맥 내 예 없이 상태별 지침에 의해서만 안내되는 LASER는 WebShop 작업에서 모든 기준선을 큰 차이로 능가하고 실제 쇼핑 웹사이트에서 인간의 성능과의 격차를 줄입니다. 우리의 분석에 따르면 LASER는 더 긴 궤적에도 더 강력하고 다른 LLM에 잘 일반화됩니다. 제한 사항 이 작업에서는 쇼핑 도메인의 대상 항목을 찾는 작업만 실험했습니다. 어려운 특성에도 불구하고 주문 추적이나 주문 내역 확인 등 전자 상거래 웹사이트에서 사용자가 일반적으로 수행하는 모든 작업을 포괄하지는 않습니다. 향후 작업에서는 쇼핑 도메인에서 이러한 인기 있는 작업을 처리할 수 있도록 LASER의 기능을 향상시키는 것이 흥미로울 것입니다. 또한 LASER에 지식 검색기(Ma et al., 2023)나 계산기(Gao et al., 2023)와 같은 더 많은 도구를 장착하여 더 복잡한 지침을 처리할 수 있다면 흥미로울 것입니다. 저희 LASER는 환경에서 가능한 상태와 해당 설명에 대한 수동 주석이 필요합니다. 이 때문에 저희 방법은 전자상거래나 여행 예약과 같이 소수의 상태만 필요한 특정 도메인(오픈 월드 웹 에이전트가 아닌)에 대한 에이전트를 구축하는 데만 적합할 수 있습니다. 향후 방향을 위해 저희는 각 특정 도메인이 LASER와 같은 에이전트에 의해 관리되고 일반적인 오픈 월드 에이전트가 다른 도메인 에이전트와 협업하여 다양한 사용자 지침을 완료하는 계층적 다중 에이전트 시스템을 구상합니다. 저희 작업의 잠재적 위험과 관련하여 저희는 LASER를 실제 시나리오에 배포하기 전에 특별한 주의와 테스트가 필요하다고 생각합니다. 웹숍 작업에 대한 실험을 수행할 때 저희는 시뮬레이션된 특성 때문에 에이전트가 환경에서 허용되는 모든 조치를 취하도록 허용합니다. 그러나 특정 조치는 실제 세계에서 복구하기 어려운 결과를 초래할 수 있습니다. 예를 들어, 실제 쇼핑 사이트에서 구매 버튼을 클릭하는 것입니다. 따라서 amazon.com에서 실험할 때 에이전트가 품목을 구매하기로 결정하면 에이전트가 멈추도록 강제했습니다. 일반적으로 LASER의 성공률은 아직 완벽하지 않기 때문에 위험이 큰 작업을 진행하기 전에 추가적인 인간 검증이 필요할 수 있습니다. 참고문헌 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David 루안, 임현택, 바렛 Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. 2023. Palm: 경로를 통한 언어 모델링 확장. 기계 학습 연구 저널, 24(240):1–113. 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le 및 Jason Wei. 2022. 지침 미세 조정 언어 모델 확장. Xiang Deng, Yu Gu, Boyan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun 및 Yu Su. 2023. Mind2web: 웹을 위한 일반 에이전트를 향하여. 제37회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스에서. 히로키 후루타, 오피르 나훔, 쿠앙-휴이 리, 유타카 마츠오, 시샹 셰인 구, 이제딘 구르. 2023. 멀티모달 웹 탐색을 위한 명령어 미세 조정 기반 모델. ICLR 2023 기반 모델에 대한 수학적 및 경험적 이해 워크숍에서. 루위 가오, 아만 마다안, 슈얀 저우, 우리 알론, 펭페이 류, 이밍 양, 제이미 캘런, 그레이엄 노이빅. 2023. 팔: 프로그램 지원 언어 모델. 제40회 기계 학습 국제 컨퍼런스 회의록, ICML&#39;23에서. JMLR.org. Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. 계획, 긴 맥락 이해 및 프로그램 합성을 갖춘 실제 세계 웹 에이전트. The Twelfth International Conference on Learning Representations에서. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. 대규모 언어 모델을 사용한 HTML 이해. Association for Computational Linguistics: EMNLP 2023, 2803-2821쪽, 싱가포르. Association for Computational Linguistics. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: 대규모 멀티모달 모델을 사용한 엔드투엔드 웹 에이전트 구축. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Embodied reasoning through planning with language models. The 6th Conference on Robot Learning의 회의록, Proceedings of Machine Learning Research의 205권, 1769-1782쪽. PMLR. Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, and Wei-Lun Chao. 2024. 웹 탐색을 위한 듀얼 뷰 시각적 맥락화. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. 언어 모델은 컴퓨터 작업을 해결할 수 있다. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. 2023. Swiftsage: 복잡한 상호 작용 작업을 위한 빠르고 느린 사고를 가진 생성 에이전트. Thirtyseventh Conference on Neural Information Processing Systems에서. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. 2018. 워크플로우 기반 탐색을 사용한 웹 인터페이스에서의 강화 학습. International Conference on Learning Representations에서. Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, Shuyan Zhou. 2023. 계층적 프롬프트는 웹 탐색에서 대규모 언어 모델을 지원합니다. Association for Computational Linguistics의 연구 결과: EMNLP 2023, 10217-10244페이지, 싱가포르. Association for Computational Linguistics. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao. 2023. 기술 사슬: 오픈 도메인 질의 응답을 위한 구성 가능한 모델. Association for Computational Linguistics의 제61회 연례 회의록(제1권: 장문 논문), 1599-1618페이지, 토론토, 캐나다. Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark. 2023. 자체 개선: 자체 피드백을 통한 반복적 개선. 제37회 신경 정보 처리 시스템 컨퍼런스에서. OpenAI. 2023. Gpt-4 기술 보고서. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35권, 27730-27744페이지. Curran Associates, Inc. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova. 2023. 픽셀에서 UI 동작까지: 그래픽 사용자 인터페이스를 통해 지시를 따르는 법 배우기. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, Shunyu Yao. 2023. Reflexion: 언어 강화 학습을 통한 언어 에이전트. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang. 2023. Adaplanner: 언어 모델을 사용한 피드백으로부터의 적응적 계획. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. 신경 정보 처리 시스템의 발전에서. Hui Yang, Sifu Yue, Yunzhong He. 2023. 온라인 의사 결정을 위한 Autogpt: 벤치마크 및 추가 의견. Shunyu Yao, Howard Chen, John Yang, Karthik R Narasimhan. 2022. 웹숍: 근거 언어 에이전트를 통한 확장 가능한 실제 세계 웹 상호 작용을 향해. 신경 정보 처리 시스템의 발전. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, Yuan Cao. 2023. React: 언어 모델에서 추론과 행동의 시너지 효과. 제11회 학습 표현 국제 컨퍼런스. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu. 2023. Appagent: 스마트폰 사용자로서의 멀티모달 에이전트. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su. 2024a. Gpt-4v(ision)은 접지된 경우 일반 웹 에이전트입니다. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An. 2024b. Synapse: 컴퓨터 제어를 위한 메모리를 사용한 Trajectory-as-exemplar prompting. The Twelfth International Conference on Learning Representations에서. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig. 2024. Webarena: 자율 에이전트를 구축하기 위한 현실적인 웹 환경. The Twelfth International Conference on Learning Representations에서. 관련 연구 웹 탐색과 같은 대화형 의사 결정 작업이 최근 인기를 얻고 있지만(Liu 등, 2018; Yao 등, 2022; Deng 등, 2023; Zhou 등, 2024), 일부 노력은 방대한 데모 데이터에서 사전 학습된 언어 모델을 미세 조정하여 이러한 작업을 해결하려고 시도했고(Gur 등, 2023; Furuta 등, 2023), 다른 노력은 LLM을 프롬프트하는 것에만 의존하여 웹 환경을 탐색하는 에이전트를 구축하려고 시도했습니다(Yang 등, 2023). LLM 기반 접근 방식 중 ReAct(Yao 등, 2023)와 InnerMonologue(Huang 등, 2023)는 작업을 생성하기 전에 LLM에 사고 과정을 제공합니다. ASH(Lo et al., 2023)와 WebAgent(Gur et al., 2024)는 복잡한 의사 결정 단계를 일련의 더 간단한 단계로 분해하는 데 중점을 둡니다.예를 들어, 먼저 작업 관련 콘텐츠를 요약한 다음 이에 따라 조치합니다.우리의 작업과 가장 유사한 Synapse(Zheng et al., 2024b)도 LLM의 작업을 안내하기 위해 상태 조건 프롬프트를 사용하도록 제안했습니다.그러나 그들의 초점은 몇 가지 샷 예제를 원자 부분으로 분해하는 반면, 우리 에이전트는 컨텍스트 내 예제 없이 상태별 지침만 사용하여 작업을 완료합니다.또 다른 작업 라인은 LLM 에이전트의 계획 단계에 중점을 둡니다.Kim et al.(2023)은 작업하기 전에 계획을 생성한 다음 오류가 발생하면 작업을 개선하는 에이전트 RCI를 제안했습니다.Adaplanner(Sun et al., 2023)는 에이전트 실행 중에 계획을 적응적으로 업데이트하여 계획 접근 방식을 더욱 향상시켰습니다. Reflexion(Shinn et al., 2023) 에이전트는 시행착오적 방식으로 환경적 피드백을 받아 계획과 행동을 개선합니다. 이러한 접근 방식은 우리의 작업과 직교하며 잠재적으로 에이전트와 결합하여 성능을 향상시킬 수 있습니다. 최근에는 다양한 작업에서 다중 모달 에이전트를 개발하려고 시도했습니다. Pix2Act(Shaw et al., 2023)와 AppAgent(Zhang et al., 2023)는 대부분 스크린샷에 응답하여 에이전트가 UI 동작을 예측하는 입력으로 사용했지만 SEEACT(Zheng et al., 2024a), Web Voyager(He et al., 2024) 및 DualVCR(Kil et al., 2024)은 웹사이트의 스크린샷과 텍스트 요소를 모두 활용하여 웹 환경과 상호 작용합니다. 웹 탐색을 상태 전환으로 모델링한다는 아이디어는 이러한 에이전트에 통합하여 성능을 더욱 향상시킬 수 있습니다. B 실험 세부 정보 WebShop은 Amazon 쇼핑 사이트에서 수집한 1,181,436개의 품목을 포함하는 온라인 쇼핑을 위한 시뮬레이션 환경을 제공합니다. 또한 이 작업은 특정 품목과 해당 대상 품목을 구매하기 위한 사람이 주석을 단 지침을 제공합니다. 이전 작업을 따르고 500개의 테스트 세트 지침을 사용하여 LASER를 평가하고 보상과 성공률로 평가합니다. 구매한 품목이 대상 품목과 완벽하게 일치하면 에이전트가 성공한 것으로 간주되고, 그렇지 않고 구매한 품목이 대상 품목과 부분적으로 일치하면 에이전트는 부분 보상(0~100 사이의 척도)을 받습니다. 이 부분 보상은 품목의 가격, 제품 범주, 숨겨진 속성 및 사용자 정의 옵션을 사용하여 계산됩니다. 이 방법의 경우 GPT-4-0613을 사용하여 LASER에 전원을 공급했습니다. 함수 호출 기능을 사용하여 작업 선택 단계를 구현했습니다. 특히 각 작업에 대한 짧은 설명을 작성한 다음 이를 LLM의 functioncall 인수에 목록으로 전달하여 모델에서 선택할 수 있도록 합니다. 에이전트가 최대 15개의 상태 전환을 할 수 있도록 합니다. 실제로 에이전트가 13번의 상태 전환 후에도 완료 상태에 도달하지 못했다면, 예산을 초과하지 않도록 기록에서 선택하도록 강제합니다. amazon.com의 시뮬레이션-실제 전송 실험의 경우, WebShop의 처음 100개 테스트 세트 지침을 사용했습니다. Yao et al.(2022)과 동일한 설정을 따르며, amazon.com의 웹페이지를 WebShop¹과 동일한 형식으로 변환한 다음 LASER 에이전트를 그대로 실행합니다. https://github.com/princeton-nlp/WebShop/tree/master/transfer에 대한 골드 주석이 없으므로 검색으로 돌아가기 &lt; 이전 지침: 거실용 녹색 테이블 램프를 찾고 있으며, 가격이 60.00달러 미만입니다.PRANES = PIRANES Safavieh Lighting Collection Minton Light Green 20인치 침실 거실 홈 오피스 책상 침대 옆 탁자 램프(LED 전구 포함) 가격: $58.평가: NA 설명 기능 리뷰 속성 지금 구매 그림 3: 충분히 좋은 오류 사례의 항목 예, 에이전트가 선택한 항목이 표시되고 사용자 지침이 맨 위에 있습니다. 에이전트가 받는 보상은 0.666입니다.검색으로 돌아가기 &lt; 이전 지침: 닫힌 발가락이 있는 여성용 하이힐을 찾고 있습니다. 영어: i want pink and in size 9, and price lower than 40.00 dollar Masbird Sandals for Women Casual Summer Closed Toe Buckle Strap Wedge Sandals Strappy Platform Sandals 가격: $10.99 to $14.평가: NA 설명 기능 리뷰 속성 지금 구매 우리가 고려한 기준선은 에이전트의 학습을 돕기 위해 어떤 종류의 인간 지식/사전을 사용합니다.우리는 LASER를 안내하기 위해 수동 지침에만 의존했습니다.우리는 LASER에서 수행한 것처럼 고수준의 일반화 가능한 지침을 제공하는 것이 저수준 작업별 궤적(예: WebGUM)을 제공하는 것보다 더 효율적인 학습 방법이라고 믿습니다.직관적으로, 에이전트는 기본적으로 방대한 궤적에서 각 시나리오를 처리하는 방법에 대한 고수준의 통찰력을 추상화하는 법을 배웁니다.이에 비해 우리는 지침의 몇 문장을 통해 그러한 통찰력을 모델에 직접 제공할 수 있습니다.이러한 관점에서 볼 때, 우리의 작업과 이전 작업의 차이점은 고수준의 일반화 가능한 인간 지식을 제공하는 것과 저수준의 사례별 인간 지식을 제공하는 것이라고 말할 수도 있습니다. 우리는 비슷하거나 더 적은 양의 인간 노력이 필요할 때 이러한 고수준 지식을 모델로 제공하는 것이 바람직하다고 믿습니다. 색상 검정 파랑 갈색 분홍 크기 6.5-7 7.5 8 8.5 9 9.5-그림 4: 누락된 세부 정보 오류 사례의 예, 에이전트가 선택한 항목이 표시되고 사용자 지침이 맨 위에 있습니다. 에이전트가 받는 보상은 0.8입니다. amazon.com에서 선택한 항목 LASER에 대해 Yao et al. (2022)을 따르고 인간 평가를 수행했습니다. 특히 항목 속성 일치, 항목 옵션 일치, 항목 범주 일치 및 항목 가격 일치에 수동으로 주석을 달았습니다. 그런 다음 WebShop 작업에 대해 정의된 것과 동일한 함수를 사용하여 개별 보상 점수와 전체 보상 점수 및 성공률을 계산했습니다. 인간과 LASER 모두 품목 가격 일치에서 100%를 달성하므로 표 2에서 이러한 결과를 제외했습니다.다른 기준선과의 비교와 관련하여 ReAct(Yao et al., 2023)와 ASH(Lo et al., 2023) 모두 수동으로 작성된 지침과 수동으로 주석이 달린 에이전트 궤적을 컨텍스트 내 데모로 사용하여 LLM을 촉구했으며 이는 하위 섹션 4.1의 원샷 설정에 해당합니다.WebGUM(Furuta et al., 2023)의 경우 1k 인간 주석이 달린 금 궤적을 사용하여 모델을 미세 조정했습니다.따라서 모든 C 사례 연구 LASER의 실패 사례를 이해하기 위해 Dev 세트에서 30개의 오류 사례에 수동으로 주석을 달았습니다.오류를 세 가지 범주로 광범위하게 분류했습니다.충분히 좋은 품목: 에이전트가 선택한 품목은 작성자의 관점에서 사용자 지침을 충족하지만 전체 점수를 받지 못했습니다. 30건 중 9건이 이 범주에 속하는 것으로 나타났으며, 그림 3에 그 예가 나와 있습니다. 에이전트가 찾은 품목은 실제로 예산 내의 가격을 가진 거실용 녹색 테이블 램프이지만, 잘못된 것으로 간주됩니다. 검색 실패: 에이전트가 검색을 위해 적절한 쿼리를 사용했음에도 불구하고 검색 엔진에서 반환한 품목 중 어느 것도 사용자 요구 사항을 충족하지 못했습니다. 30건 중 12건이 이 범주에 속하는 것으로 나타났습니다. 보다 효과적인 검색기나 검색 엔진이 이러한 문제를 해결할 수 있을 것이라고 가정합니다. 누락된 세부 정보: 에이전트가 선택한 품목은 실제로 특정 세부 정보에 대한 사용자의 지시와 일치하지 않습니다. 30건 중 9건이 이 범주에 속하는 것으로 나타났으며, 그림 4에 그 예가 나와 있습니다. 이 예에서 선택한 여성용 신발의 색상과 사이즈가 모두 사용자 지시와 일치했지만, 하이힐 신발은 아닙니다. 이는 LASER가 많은 일치하는 세부 정보가 있는 항목을 접했을 때 실수를 할 수 있음을 나타내며, 자체 피드백/검증 모듈이 이 문제를 해결할 수 있는지 확인하는 것이 흥미로울 것입니다(Madaan et al., 2023). D 실험에 사용된 프롬프트 E 라이선스 Webshop 작업과 ReAct 방법은 모두 MIT 라이선스에 따라 릴리스되었습니다. 둘 다 연구 목적으로 릴리스되었으며, 실험은 의도된 용도와 일치합니다. 귀하는 사용자가 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 다음 형식으로 현재 웹 탐색 세션에 대한 관찰이 제공됩니다. 현재 관찰: WebShop 지침: {사용자 지침} [버튼] 검색 [버튼_](사용자 지침에 따라 검색 쿼리를 생성하고 이 버튼을 선택하여 관련 항목 찾기) 관찰의 모든 버튼은 수행할 수 있는 가능한 작업을 나타냅니다. 현재 관찰을 기반으로 귀하의 작업은 수행해야 할 다음 작업에 대한 근거를 생성하는 것입니다. 과거 근거 및 작업의 내역이 제공되는 경우 근거를 생성할 때 내역도 고려해야 합니다. 표 4: 검색 상태에 사용한 시스템 지침. 당신은 사용자가 올바른 품목을 찾을 수 있도록 도울 수 있는 지능형 쇼핑 도우미입니다. 현재 웹 탐색 세션에 대한 관찰 결과가 다음 형식으로 제공됩니다.현재 관찰 결과: 지침: {사용자 지침} [버튼] 검색으로 돌아가기 [버튼_] (검색 페이지로 돌아가려면 이 버튼을 선택하세요) 페이지 현재 페이지 번호 (전체 결과: 총 결과 수) [버튼] 다음 &gt; [버튼_] (다음 결과 페이지로 이동하려면 이 버튼을 선택하세요) [버튼] {항목_id 1} [버튼_] (항목 1의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 1의 이름} {항목 1의 가격} [버튼] {항목_id 2} [버튼_] (항목 2의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 2의 이름} {항목 2의 가격} [버튼] {항목_id 3} [버튼_] (항목 3의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 3의 이름} {항목 3의 가격} {더 많은 항목...} 이 단계에서는 사용자 지침과 일치할 수 있는 항목을 선택해야 합니다. 항목의 세부 정보가 사용자 지침과 일치하지 않더라도 일치시킬 수 있도록 다른 사용자 지정 옵션을 제공할 수 있습니다. 예를 들어 항목 이름에 색상 x가 있지만 나중에 색상 y로 사용자 지정할 수 있으며 사용자 지정 옵션은 항목을 선택한 후에 표시됩니다. 따라서 항목 이름이 지침과 관련이 있거나 부분적으로 일치하는 경우 해당 항목을 선택하여 세부 정보를 확인해야 합니다. 이전에 항목을 선택한 경우(버튼을 클릭한 경우) 동일한 항목을 다시 선택해서는 안 됩니다. 즉, [클릭한 버튼] 항목 ID [클릭한 버튼]인 항목을 선택하지 마세요. 다음 형식으로 응답을 준비하세요. 근거: 사용자가 {대상 항목의 키워드}를 원했고 {항목 x의 일치하는 키워드}를 찾았으므로 항목 {항목 ID x}가 일치하는 것으로 보입니다. 표 5: 결과 상태에 사용한 시스템 지침. 사용자는 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 현재 웹 탐색 세션에 대한 관찰이 다음 형식으로 제공됩니다.현재 관찰: 지침: {사용자 지침} [버튼] 검색으로 돌아가기 [버튼_] (검색 페이지로 돌아가려면 이 버튼을 선택하세요) [버튼] &lt; 이전 [버튼_] (이전 결과 페이지로 돌아가려면 이 버튼을 선택하세요) {사용자 정의 유형1}: [버튼] 옵션1 [버튼_] [버튼] 옵션2 [버튼_] {사용자 정의 유형2}: [버튼] 옵션1 [버튼_] [버튼] 옵션2 [버튼_] {추가 사용자 정의 옵션... (있는 경우)} {항목 이름 및 세부 정보} [버튼] 설명 [버튼_] (항목의 전체 설명을 보려면 이 버튼을 선택하세요) [버튼] 기능 [버튼_] (항목의 전체 기능을 보려면 이 버튼을 선택하세요) [버튼] 리뷰 [버튼_] (항목의 전체 리뷰를 보려면 이 버튼을 선택하세요) [버튼] 지금 구매 [버튼_] (항목을 구매하려면 이 버튼을 선택하세요) 설명: (이것이 표시되면 설명 버튼을 다시 선택해서는 안 됩니다) {전체 항목 설명(있는 경우) 또는 &quot;없음&quot;} 기능: (표시되는 경우 기능 버튼을 다시 선택해서는 안 됨) {항목의 전체 기능(있는 경우) 또는 &quot;없음&quot;} 리뷰: (표시되는 경우 리뷰 버튼을 다시 선택해서는 안 됨) {항목의 전체 리뷰(있는 경우) 또는 &quot;없음&quot;} 대상 항목 세부 정보(사용자가 찾는 것): 키워드: {대상 항목의 키워드} 최대 가격: {항목의 가격은 이 값을 초과해서는 안 됨} 이 단계에서는 항목이 사용자 지침과 일치하는지 확인해야 합니다. 항목이 사용자 지침과 일치하는지 여부를 결정할 때 사용 가능한 사용자 지정 옵션을 고려해야 합니다. 항목을 사용자 지침과 일치하도록 사용자 지정할 수 있거나 사용자 지정 옵션이 사용자 사양을 충족하는 경우에도 잘 맞습니다. 항목이 사용자 지침과 일치하지 않고 사용자 지정 옵션이 충분하지 않은 경우 이전 페이지로 이동하여 다른 항목을 볼 수 있습니다. 또한 항목의 설명, 기능 및 리뷰를 확인하여 더 자세한 내용을 볼 수 있습니다(설명, 기능 및 리뷰는 &quot;없음&quot;일 수 있으므로 이미 제공된 경우 다시 확인하지 마십시오). 다음 형식으로 응답을 준비하십시오. 근거: 사용자는 {대상 항목의 키워드}를 원했으며 다음과 같은 사용자 지정 옵션이 필요했습니다. {대상 항목의 사용자 지정}, 항목은 현재 관찰에서 항목의 키워드이며 다음과 같은 사용자 지정 옵션이 있습니다. {현재 항목에 사용 가능한 옵션}, 이는 {포함}/{사용자 요구 사항을 포함하지 않음}, 따라서 {항목을 구매}/{자세한 내용 확인}/{다른 항목을 보려면 이전 페이지로 이동}해야 합니다. 표 6: 항목 상태에 대해 사용한 시스템 지침. 사용자는 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 다음 형식으로 현재 환경에 대한 관찰 결과와 다음에 취해야 할 조치에 대한 근거가 제공됩니다.현재 관찰: 표 4, 표 5 및 표에서 표시된 검색 또는 결과 또는 항목 상태의 관찰 레이아웃다음 조치 근거: {다음 조치에 대한 근거} 귀하의 작업은 근거에 따라 함수 호출 중 하나를 수행하는 것입니다.표 7: 생각에서 조치를 생성하는 데 사용한 시스템 명령어. 상태 검색 결과 항목 사용 가능한 작업 {&quot;name&quot;: &quot;검색&quot;, &quot;description&quot;: &quot;키워드를 기준으로 인벤토리에서 대상 항목을 검색하려면 이 기능을 사용합니다.&quot;} {&quot;name&quot;: &quot;select_item&quot;, &quot;description&quot;: &quot;검색 결과에서 항목 중 하나를 선택하고 세부 정보를 확인하려면 이 기능을 사용합니다.&quot;} {&quot;name&quot;: &quot;다음&quot;, &quot;description&quot;: &quot;현재 페이지의 항목 중 사용자 지시와 일치하는 항목이 없는 경우, 이 기능을 사용하여 다음 검색 결과 페이지로 이동하여 더 많은 항목을 봅니다.&quot;} {&quot;name&quot;: &quot;검색으로 돌아가기&quot;, &quot;description&quot;: &quot;이 기능을 사용하여 초기 검색 페이지로 돌아갑니다. 여러 페이지의 항목을 탐색하고 기록에서 여러 항목의 세부 정보를 확인했지만 사용자 지시와 일치하는 항목이 없는 경우에만 이 기능을 사용해야 합니다.&quot;} {&quot;name&quot;: &quot;설명&quot;, &quot;description&quot;: &quot;항목이 사용자 지시와 완벽하게 일치하는지 확실하지 않은 경우, 이 기능을 사용하여 항목 설명을 확인합니다.&quot;} {&quot;name&quot;: &quot;특징&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 항목이 사용자 지침과 완벽하게 일치하는지 확실하지 않은 경우 항목의 특징을 확인하세요&quot;} {&quot;name&quot;: &quot;리뷰&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 항목이 사용자 지침과 완벽하게 일치하는지 확실하지 않은 경우 항목의 리뷰를 확인하세요&quot;} {&quot;name&quot;: &quot;지금 구매&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 현재 항목이 사용자 지침과 완벽하게 일치하는 경우 현재 항목을 구매하세요&quot;} {&quot;name&quot;: &quot;이전&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 현재 항목이 사용자 지침과 일치하지 않는 경우 결과 페이지로 돌아가세요&quot;} 표 8: 각 상태에서 에이전트의 액션 공간. 각 액션은 OpenAI 2의 가이드라인을 따르는 함수 호출로 구현되며, 함수 호출에 사용되는 추가 매개변수는 간결함을 위해 여기에서 생략합니다.
"
"--- ABSTRACT ---
As we embark on a new era of LLMs, it becomes increasingly crucial to understand their capabilities, limitations, and differences. Toward making further progress in this direction, we strive to build a deeper understanding of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective open-source LLMs and their distilled counterparts. To this end, we specifically focus on long-form question answering (LFQA) because it has several practical and impactful applications (e.g., troubleshooting, customer service, etc.) yet is still understudied and challenging for LLMs. We propose a question-generation method from abstractive summaries and show that generating ollow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our experimental results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup or LLMs and shows performance gaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2) open-source LLMs exhibit decreased reliance on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from summaries — especially for longer contexts (>1024 tokens). 1
--- INTRODUCTION ---
While Large Language Models (LLMs) like ChatGPT, GPT-4 (OpenAI, 2023) have exhibited superior performance across various benchmarks, opensource efforts have also been progressing rapidly in catching up across different applications and benchmarks like MMLU (Hendrycks et al., 2021), OpenLLMBoard (Anil et al., 2023; Beeching et al., 2023; Touvron et al., 2023). As we move into the new era of LLMs with fast-paced progress on new models and techniques, it becomes increasingly important to understand the capabilities, limitations, and differences between them. yingbo.zhou, syavuz}@salesforce.com With LLMs capable of generating coherent text has proven to perform well in tasks like summarization (Ouyang et al., 2022), their performance on LFQA is relatively less known. Long-Form Question Answering (LFQA) is one of the important unsolved challenges with diverse and impactful realworld applications (e.g., help forums, troubleshooting, customer services, etc.) Answering such questions often requires complex reasoning abilities to understand query and reason across spans of information scattered across original document. Abstractive summaries contain the crux of the articles in a compressed form (Fabbri et al., 2020). We hypothesize that follow-up questions from these summaries would require a deeper understanding of the topics that would link different parts of the source document. Moreover, Pang et al. (2022) demonstrate that answers that require understanding more than a third of the long document are often rated as “HARD” by humans. Therefore, we propose a scalable evaluation method to analyze and study the disparities of massive LLMs with smaller yet proven successful base LLMs (e.g., Llama-7B, 13B) and their distilled versions (e.g., Alpaca-7B, 13B). To this end, we propose to prompt ChatGPT with specific instructions to generate complex questions from document summaries. Our empirical analysis on two fronts (complexity of generated questions and answer quality of open-source LLMs) show that follow-up questions generated from summaries pose a challenging yet more realistic setup for testing the reasoning abilities of LLMs. Since relying fully on the human evaluation for long-form QA is expensive and difficult to scale (Pagnoni et al., 2021), we instead leverage GPT-4 to evaluate the answer quality on coherence, relevance, factual consistency, and accuracy following prior works (Fabbri et al., 2020; Fan et al., 2019). However, we also do a smaller scale human evaluation, which shows that GPT-has a high correlation with human evaluation mak --- --ing our evaluation reliable. Our main findings from this study are as follows: * Our proposed method of generating questions from abstractive summaries require inferring from longer contexts, with multiple passes through the context for > 20% times. ¢ Distilled LLMs (Alpaca-7B, 13B) tend to rely less on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from document summaries. « Answers generated by distilled LLMs can be coherent across different settings; but tend to drift from the question, generate repetitive and partially correct answers for the questions generated from summaries (> 16.8%) ¢ Alpaca-7B, 13B mostly generate meaningful answers than base LLMs (Llama) but are sensitive to longer contexts (>1024 tokens). 2
--- RELATED WORK ---
Reasoning over Long Documents: LLMs have shown amazing capabilities to reason over a number of tasks like commonsense reasoning (Talmor et al., 2019), mathematical and symbolic reasoning (Huang and Chang, 2023; Cobbe et al., 2021), question answering tasks like SQuaD, HotpotQA. However, most of these tasks do not require long context and answers are often a short phrase or a span of text from the context. In this work, we evaluate LLMs to reason over long documents that would require deeper understanding capabilities and longer context to answer by prompting LLMs (ChatGPT) to generate follow-up questions from summaries of long documents. Model-based Evaluation: Prior work has proposed automatic evaluation metrics using learned models (Zhang* et al., 2020; Laban et al., 2022); especially for long form text generation tasks like summarization (Fabbri et al., 2020; Kryscinski et al., 2020) where consistency and coherency is measured between the source document and generated summary as entailment. Recently, Liu et al. (2023) showed that GPT-4 has the highest correlation with humans and surpasses all other autoevaluation
--- METHOD ---
from abstractive summaries and show that generating ollow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our
--- EXPERIMENT ---
al results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup or LLMs and shows performance gaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2) open-source LLMs exhibit decreased reliance on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from summaries — especially for longer contexts (>1024 tokens). 1 Introduction While Large Language Models (LLMs) like ChatGPT, GPT-4 (OpenAI, 2023) have exhibited superior performance across various benchmarks, opensource efforts have also been progressing rapidly in catching up across different applications and benchmarks like MMLU (Hendrycks et al., 2021), OpenLLMBoard (Anil et al., 2023; Beeching et al., 2023; Touvron et al., 2023). As we move into the new era of LLMs with fast-paced progress on new models and techniques, it becomes increasingly important to understand the capabilities, limitations, and differences between them. yingbo.zhou, syavuz}@salesforce.com With LLMs capable of generating coherent text has proven to perform well in tasks like summarization (Ouyang et al., 2022), their performance on LFQA is relatively less known. Long-Form Question Answering (LFQA) is one of the important unsolved challenges with diverse and impactful realworld applications (e.g., help forums, troubleshooting, customer services, etc.) Answering such questions often requires complex reasoning abilities to understand query and reason across spans of information scattered across original document. Abstractive summaries contain the crux of the articles in a compressed form (Fabbri et al., 2020). We hypothesize that follow-up questions from these summaries would require a deeper understanding of the topics that would link different parts of the source document. Moreover, Pang et al. (2022) demonstrate that answers that require understanding more than a third of the long document are often rated as “HARD” by humans. Therefore, we propose a scalable evaluation method to analyze and study the disparities of massive LLMs with smaller yet proven successful base LLMs (e.g., Llama-7B, 13B) and their distilled versions (e.g., Alpaca-7B, 13B). To this end, we propose to prompt ChatGPT with specific instructions to generate complex questions from document summaries. Our empirical analysis on two fronts (complexity of generated questions and answer quality of open-source LLMs) show that follow-up questions generated from summaries pose a challenging yet more realistic setup for testing the reasoning abilities of LLMs. Since relying fully on the human evaluation for long-form QA is expensive and difficult to scale (Pagnoni et al., 2021), we instead leverage GPT-4 to evaluate the answer quality on coherence, relevance, factual consistency, and accuracy following prior works (Fabbri et al., 2020; Fan et al., 2019). However, we also do a smaller scale human evaluation, which shows that GPT-has a high correlation with human evaluation mak --- --ing our evaluation reliable. Our main findings from this study are as follows: * Our proposed method of generating questions from abstractive summaries require inferring from longer contexts, with multiple passes through the context for > 20% times. ¢ Distilled LLMs (Alpaca-7B, 13B) tend to rely less on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from document summaries. « Answers generated by distilled LLMs can be coherent across different settings; but tend to drift from the question, generate repetitive and partially correct answers for the questions generated from summaries (> 16.8%) ¢ Alpaca-7B, 13B mostly generate meaningful answers than base LLMs (Llama) but are sensitive to longer contexts (>1024 tokens). 2 Related Work Reasoning over Long Documents: LLMs have shown amazing capabilities to reason over a number of tasks like commonsense reasoning (Talmor et al., 2019), mathematical and symbolic reasoning (Huang and Chang, 2023; Cobbe et al., 2021), question answering tasks like SQuaD, HotpotQA. However, most of these tasks do not require long context and answers are often a short phrase or a span of text from the context. In this work, we evaluate LLMs to reason over long documents that would require deeper understanding capabilities and longer context to answer by prompting LLMs (ChatGPT) to generate follow-up questions from summaries of long documents. Model-based Evaluation: Prior work has proposed automatic evaluation metrics using learned models (Zhang* et al., 2020; Laban et al., 2022); especially for long form text generation tasks like summarization (Fabbri et al., 2020; Kryscinski et al., 2020) where consistency and coherency is measured between the source document and generated summary as entailment. Recently, Liu et al. (2023) showed that GPT-4 has the highest correlation with humans and surpasses all other autoevaluation methods on summarization tasks. We take inspiration from these works to design evaluation prompts and use GPT-4 as the evaluator for generated answers. 3 Proposed Evaluation Method 3.1 Data Source In order to create a dataset that is diverse and widely usable, we use Wikipedia articles. Using metadata of the Category list from Wikipedia API, we control the diversity of topics and collect articles from each of the following 9 domains: Physics, Entertainment, History, Computer Science, Social Sciences, Society, Economics, Medicine, and Sports. The article pages can often be lengthy to fit in the context of LLMs. Hence, we extract section-wise data from the article pages that have a minimum length of 256 tokens using SpaCy tokenizer and combine the shorter sections together. For a fair comparison between different models, we set a maximum context length of 2k tokens in our experiments. In addition, we filter out non-informative documents using pre-processing filters. Further details are available in Appendix A.5. 3.2 Question Generation using ChatGPT We formulate our question generation method as a two-step process: (1) Summarization and (2) Question generation from summary. Summarization First, we collect section wise passages from Wikipedia as described in Section 3.1. Then, we prompt ChatGPT (gpt-turbo-3.5) to generate summary of original document. In order to provide more context to ChatGPT, we provide information about the title and the domain of the article in the passage. Question generation from summary In this step, we prompt ChatGPT to generate questions using document summaries as context. To avoid random order question generation, we instruct ChatGPT to provide top-3 complex questions to answer. To demonstrate the usefulness of our question generation process, we also establish a baseline with the same instructions where questions are directly generated from the passage. Please refer to the appendix A.1 for the prompt used in our setup. In summary, we generate 3 questions for 50 passages in each domain totaling to 1350 questions for each setting. 3.3 Evaluation of Generated Question Complexity Pang et al. (2022) designed extensive annotation guidelines to assess the complexity of questions. Of the questions rated as “HARD” by humans, 26.7% of the questions (20.2% higher than the easier ones) --- --Question Choices QI: Is the question answerable from the given context and is unambiguous? A. Yes B. No Q2. How much of the passage is needed as context to answer the question? A. Only a sentence or two B. More than 2 sentences but lesser than a paragraph C. Atleast a third of the entire passage D. Most of the passage Q3: Does the question require multiple passes through the passage? A. Yes B. No Table 1: Prompts designed to evaluate the complexity of generated questions. needed at least one-third or more of the given information to be answered. In order to assess the quality of generated questions, we prompt ChatGPT with the questions (Table 1) for (1) From the passage (QG-Passage) (2) From the summary (QG-Summary). Following prior work, by majority voting we exclude the questions that are rated as unanswerable by ChatGPT by prompting the questions with differenttop_p = {0.8, 0.9, 1}. After filtering, we have 1278 generated ques TDS each setting: QG - Passage _QG - Summary QI: Unambiguity 96.6% 94.7% Q2. Context Length: A sentence or less than a paragraph 79.3% 15.1 % At least a third or most of the passage 20.7% 24.3% Q3: Multi-pass of the passage 24.4% 31% Table 2: Prompts designed to evaluate the complexity of generated questions. 4 Results and Analysis 4.1 Experiment Setup As few-shot setting is infeasible in our setting due 0 context length, we compare model performance on zero-shot evaluation. We prompt the following models to generate free-form text as answers on our final evaluation dataset: ChatGPT (OpenAI, 2023), Alpaca-7B, 13B (Taori et al., 2023), LLaMa-7B, 13B (Touvron et al., 2023). We use OpenAI API ‘or ChatGPT and load checkpoints for open-source LLMs from HuggingFace !. The prompt used for generating answers are in Appendix A. Please note hat our experiments do not consider input beyond 2k sequence length for fair comparisons with other models. We also test generating questions from Alpaca and found them to not follow instructions and often generate irrelevant content. Our detailed analysis can be found in Appendix A.2. GPT-4 as evaluator has shown high correlation with human evaluation in long form text generation ‘https://huggingface.com tasks like summarization (Liu et al., 2023) surpassing other auto-evaluation metrics like ROUGE and BLEU scores. Since LLMs are expected to generate free form answers for our setting, we take inspiration from prior works on long-form text generation metrics (Fabbri et al., 2020) and adopt them in our evaluation for coherency, consistency, accuracy, and relevance. Basically, we adopt the definitions used as guidelines for human evaluation to our method as shown below: Coherency: Answer should be well-structured and well-organized and should not just be a heap of related information. Relevance: Answer should be relevant to the question and the context. The answer should be concise and avoid drifting from the question being asked. Factual consistency: The context should be the primary source for the answer. The answer should not contain fabricated facts and should entail information present in the context. Accuracy: Answer should be satisfactory and complete to the question being asked. Measure the correctness of the answer by checking if the response answers the presented question. We prompt GPT-4 to rate answers on a scale from 0 to 3 (higher the better) on all of the four metrics. We average all the ratings obtained from GPT-4 and present the results in Table 3. Our evaluation prompt can be found in Appendix A.3.1. We hypothesize that an optimal prompt should always prefer human answers and not be biased towards model-generated answers. Laskar et al. (2023) show that LLMs like ChatGPT still underperform to humans on TruthfulQA dataset(Lin et al., 2022). Hence, we perform proxy testing with GPT-4 on TruthfulQA dataset in order to verify the reliability and faithfulness of our evaluation prompt. We test the generated answers from ChatGPT and open-source LLMs against the ground truth on randomly sampled 50 test instances and find that our evaluation prompt with GPT-4 prompt prefers human-written answers for factual consistency and correctness over model-generated ones more than > 90% of the times. In addition, we also perform human evaluation of LLM generated answers and discuss the correlation of GPT-4 evaluation with human evaluation in Section A.4. 4.2 Results Our experiment results show that ChatGPT outperforms other LLMs in all the metrics by a wide margin from 22.4% - 40.1% against the second --- --2.33 Wl summary 292 33 i summary eee? e274 Passage 224 Passage 250 mn aig7h.28 £30 go 98516 S16§12 212) 10090.8} 0, 0.8} 0, 0.4 0.0.1 OF 0.0. se ye aor a1 sl? 2% 33 _no yor? ie: ao a 2% 102° 2 ar cn[ilisummary oar Passage(i summary Passage | 81 ggit ee fy ve “ahAccuracy PorrNyNNYW RONQORON Factual Consistency SSOPPNNNY 3, SRENDORON ° ° gI® g1® 438 438 agk yon ea oY ee Caos Figure 1: Graphs showing the breakdown of models with respect to different metrics used in evaluation: (a) Coherence (b) Relevance (c) Answer Accuracy (d) Factual Consistency Model QG-Passage QG-Summary w/o context w/context w/o context w/ context ChatGPT 2.78 2.93 2.67 2.Alpaca-13B 2.27 2.09 2.04 2.LlaMa-13B 1.22 1.47 0.98 1.Alpaca-7B 2.04 1.96 1.64 1.LlaMa-7B 0.89 1.12 0.66 0.Table 3: Performance of different models based on GPT4 evaluation. The table shows average ratings across all metrics: accuracy, coherency, consistency, relevance. best performing LLM (Alpaca-13B). However; all the models including ChatGPT generate less accurate and relevant answers for QG-Summary when compared to QG—Passage; while the gap is much larger in open-source LLMs. We also find that most of the LLMs find context important in order to generate answers; however, the gap is much smaller for QG-Passage (avg. gap of 0.12 v.s. 0.2). Surprisingly, Alpaca-7B, 13B models perform better w/o context for QG-Passage. We hypothesize that questions directly generated from the context passage can be simple that could be directly answered from the parametric knowledge of LLMs without additional context. On further analysis, we observe that Alpaca-7B,13B performance drops significantly in longer contexts (Figure 2). We hypothesize that in a constrained sequence length setting, adding supporting context (even gold passage) may not be always helpful. We leave further analysis for future work and hope our analysis will motivate future research directions to study when to add/not add context for different tasks. Performance of LLMs on different metrics Figure | presents the performance of models across different metrics for QG-Summary. We observe two trends: (1) Open-source base LLMs (Llama7B,13B) suffer at all fronts significantly on generated answer quality whereas distilled models perform better than their counterparts (Llama) on all the settings. (2) OG-Summary provides a more challenging setting for all the LLMs: specifically, we notice that degradation in coherency score is negligent on ChatGPT and Alpaca-13B while other metrics like relevance, answer accu oO —ic 2.0: O15, © 1.0; uo Z 0.5: 0. sig <Context Length ><llama7B-@-alpaca7B~¢ llama13B-¥ alpaca1 3B chatgpt <Figure 2: Performance (avg. ratings) of LLMs across different context length. racy and factual consistency degrade consistently. We find open-source LLMs to drift from the question, generate partially correct answers and repeat more frequently in QG-Summary setting leading to lower scores. This further confirms that our proposed evaluation method QG-Summary challenges LLMs for deeper reasoning capabilities. // Context Length Analysis We analyze the effect of context length across LLMs in our proposed setting (QG-Summary). As expected, ChatGPT remains robust to context length until 2k tokens with Llama variants performing worse than other models (Figure 2). Interestingly, we find distilled models (Alpaca) being consistent until 1024 tokens, however beyond > 1024 tokens, the performance degrades at a higher rate than Llama. 5
--- CONCLUSION ---
With the emergence of LLMs like ChatGPT and open-source successful LLMs, it is extremely important to understand the capabilities and limitations of different LLMs. In order to test deeper reasoning abilities of LLMs by referring to longer contexts, we evaluate answers generated by LLMs on questions generated by ChatGPT on summaries of long documents. Results show that our proposed method of question generation poses a challenging setup for LLMs and shed light on performance gaps between massive LLMs and open-source LLMs. We hope our analysis motivates future research directions such as leveraging longer contexts in a constrained sequence length setting and developing better long-form text generation for smaller LLMs. --- --6 Limitations In this study, we propose an automatic evaluation setting to generate questions from summaries, and the generated answers from LLMs are evaluated using GPT-4 for different metrics. Experimental results show that our proposed evaluation setting proves to be a challenging setup for LLMs. However, our study might have some limitations. GPT-4 as evaluator While GPT-4 has shown a high correlation with human evaluation for long form text generation (Liu et al., 2023), the capabilities of using GPT-4 for evaluation is an active area of research in itself. Hence, our results might be limited by the undiscovered capabilities of GPT-4. ChatGPT for question generation Generating answers on questions prompted from ChatGPT might lead to optimistic results of ChatGPT. However, there exists limitations with other baselines to generate meaningful questions. We show extensive analysis of using other LLMs for question generation (Appendix A.2). Unknown training data Little is known about the training data distribution of massive LLMs like ChatGPT. Models trained with different methods and data distribution make the evaluation for fair comparison harder. References Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nys trom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Alexander R Fabbri, Wojciech Krysciriski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. Summeval: Re-evaluating summarization evaluation. arXiv preprint arXiv:2007.12626. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567, Florence, Italy. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: A survey. Tomas Koéisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics. Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLIbased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177. --- --Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. OpenAI. 2023. Gpt-4 technical report. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336-5358, Seattle, United States. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https: //github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. --- --A Appendix A.1_ Prompts used for Question Generation Following the analysis from prior works (Kocisky et al., 2018; Pang et al., 2022), we formulate our question generation method as a two-step process: (1) Summarization and (2) Question generation from summary. In the first step, we design prompt for generating summary as shown below: Summarize the paragraphs below in the context of {title} in {domain}. In the next step, we ask ChatGPT to generate questions from summary as shown below: Using the context below, come up with follow-up questions. Rank the generated questions in the order of decreasing complexity to answer and display only the top 3. To demonstrate the usefulness of our question generation process, we also establish a baseline with the same instructions where questions are directly generated from the passage. The prompt used for the baseline is: {context } Using the context below, come up with three questions. Rank the generated questions in the order of decreasing complexity to answer and display only the top 3. {context} A.2 Question Generation using open source LLMs In order to create a fair evaluation setup, we prompt Alpaca-7B,13B models to summarize and generate questions on 50 instances. We do not consider question generation from non-instruction tuned models (e.g: Llama). From our evaluation method on generated question as described in Section 4, we find questions generated from Alpaca to be unanswerable (non-existent in the context) and contain gibberish content more than 80% of the time. The below table presents our evaluation of question generation from Alpaca: A.2.1_ Evaluation of Question Generation using ChatGPT In order to verify the complexity of generated ques tions as outlined in Section 4, we prompt ChatGPT with the following prompt: to request your feedback on determining We would like the complexity of generated questions Evaluation Metric QG - Passage _QG - Summary QU: Unambiguity 12.5% 8.3% Q2. Context Length: A sentence or less than a paragraph 98.8% 98.5% At least a third or most of the passage — 1.2% 1.5% Q3: Multi-pass of the passage 0% 0% Table 4: Prompts designed to evaluate the complexity of generated questions on Alpaca. by an AI assistant with respect to the context displayed above.\n\n For each of the question, rate the complexity of each of the generated questions for the dimensions: ambiguity, context and \n\n Q1: Is the question answerable from the given reasoning capabilities. context and is unambiguous? A. Yes B. No \n\n Q2. How much of the passage is needed as context to answer the question? A. Only a sentence or two from the passage B. More than 2 sentences but lesser than a paragraph C. Atleast a third of the entire context given D. Most of the context given \n\n Q3: Does the question require multiple passes through the passage? A. Yes B. No. Assume you do not have prior knowledge about the topic apart from the context given to you. Please output your choices in the form of a dictionary. (e.g: 'Q1’: ’<your answer choice for Q1>’, 'Q2': ’<your answer choice for Q2>’, 'Q3': ’<your answer choice for Q3>’, 'Q4': ’<your answer choice for 94>’). \n\n In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. Since LLMs are trained with different training data mixtures, we specifically ask ChatGPT to answer the questions based on the given context alone. A.2.2 Prompt for Answer Generation In order generate response on the questions generated by LLMs, we prompt the following: For every generated question, we prompt the models as follows: Given the context, answer the question --- --below: Context: {context} Question: {question} Answer: {Answer} A.3 GPT-4 as an Evaluator A.3.1_ Prompts used in GPT-4 Evaluation In order to evaluate answers generated by LLMs, we ask GPT-4 to rate answers on Likert scale from 0 to 3 (higher the better) on all of the following four metrics: coherency, relevance, accuracy and factual consistency. Our evaluation prompt used as prompt for GPT-4 is shown below: system prompt: You are a helpful and precise assistant for checking the quality of the answer onverticals: factual coherence, relevance, consistency, accuracy. prompt : We would like to request your scores and feedback on the performance of two AI assistants for answering the user question based on the context displayed above. Please rate the answer quality on 4 metrics: coherence, relevance, factual consistency and accuracy. Definition of each metric is given to you. Coherence - Answer should be well-structured and well-organized. Relevance - Answer should be relevant to the question and the context. Answer should also avoid drifting from the question being asked. Factual consistency - The context should be the primary source for the answer. The answer should not contain fabricated facts and should entail information present in the context. Accuracy - Does the response provided by the assistant answer the question correctly in a concise manner? Provide a score to each AI assistant response for each of the metric defined above on a scale of 0 to 3 where higher score means better performance. Do not compare the performance between AI assistants and rate them individually. Enter two new lines. Next, please provide a comprehensive explanation of your preferred answer among the both and your evaluation, avoiding any potential bias and ensuring that the order in which Metric Cohen-Kappa score Coherency 0.Relevance 0.Accuracy 0.Factual Consistency 0.Table 5: Annotator agreement scores with GPT-the responses were presented does not affect your judgment. Please note that your scores should be in a dictionary format as the example given to you. ‘Assistant 1’: Example: ’ coherence’: <int score>, 'relevance’: <int score>, ‘factual consistency’: <int score>, ‘accuracy’: <int score>, '’Assistant 2’: ... ‘Explanation’: '<Preference: Assistant 1 or Assistant 2 or Both or Neither>. <your explanation>’. A.4 Case Study: Human Eval v.s GPT-We annotate 50 QA pairs to evaluate answer quality generated by LLMs independently of GPT-4 ratings. Two annotators are given similar guidelines as outlined in Section 4 and a questionnaire as GPT4 (Section A.3.1). For fair evaluation, we do not reveal the model that generated the answer to annotators. Table 5 includes the agreement scores of the human evaluation with GPT-4. A high agreement score across different metrics on free-form text generation shows that our evaluation method using GPT-4 is reliable. We also present some qualitative examples in Appendix (Table 6) A.5 Data Collection We use Wikipedia API to collect articles to create our evaluation setting. However, we find not all articles may be useful to our setting; such as Wikipedia articles containing reference links, list of topics, glossary etc. We apply two levels of filtering: (1) We remove any article that has < 2k tokens (2) we curate a list of noninformative sections and filter those sections from consideration in this study. [’References’, ’See also’, ‘Notes’, '’Sources’, ‘External links’, '’History’, ‘Further reading’ ] A.6 Qualitative Examples Below table presents qualitative examples from human annotation task and shows some of the ratings --- --by humans and GPT-4. --- --Qualitative Examples Question 1: What is the difference between interdisciplinary study and specialization in a single discipline? Evaluation: Issue - None, Human score - 3, GPT-4 --ChatGPT: © Interdisciplinary study involves applying the methods and approaches of several different disciplines to a subject, while specialization in a single discipline involves in-depth study and expertise in a single subject. Evaluation: Issue - Repetition, Human score - 2, GPT-4 -Interdisciplinary study is the combination of two or more disciplines to create a new field of study. Specialization is the focus on a single discipline, with the goal of becoming an expert in that field. Question 2: How does good asset management require leadership, communication, collaboration, and continuous improvement? Evaluation: Issue - None, Human score - 3, GPT-4 -Good asset management requires leadership, communication, collaboration, and continuous improvement. Leadership is required to set the strategic priorities and to ensure that the organization is aligned with the goals. Communication is required to ensure that the organization is aware of the goals and the strategies to achieve them. Collaboration is required to ensure that the organization is working together to achieve the goals. Continuous improvement is required to ensure that the organization is constantly striving to improve its performance. Evaluation: Issue - Accuracy, Human score - 1, GPT-4 -“Alpaca-7B: Good asset management requires leadership, communication, collaboration, and continuous improvement. Question 3: What are the four areas that CSAB considers crucial to the discipline of computer science? Evaluation: Issue - Factual Consistency, Human score - 1, GPT-4 -“Alpaca-7B: | 1. Theory of computation 2. Computability theory 3. Programming language theory 4. Formal methods The four areas that CSAB considers crucial to the discipline of computer science are theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. Table 6: Few examples of generated answers from LLMs with GPT-4 and human evaluation Legend LLM, Inconsistencies missed by the model
"	"--- ABSTRACT ---
LLM의 새로운 시대로 접어들면서 LLM의 역량, 한계, 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 이 방향으로 더 나아가기 위해 대규모 LLM(예: ChatGPT)과 규모가 작지만 효과적인 오픈소스 LLM 및 이들의 정제된 대응물 간의 격차를 더 깊이 이해하기 위해 노력합니다. 이를 위해 우리는 특히 장문 질의응답(LFQA)에 집중합니다. 왜냐하면 이 방식은 여러 가지 실용적이고 영향력 있는 응용 프로그램(예: 문제 해결, 고객 서비스 등)이 있지만 여전히 LLM에게 연구가 부족하고 어려운 방식이기 때문입니다. 우리는 추상적 요약에서 질문 생성 방법을 제안하고 긴 문서의 요약에서 후속 질문을 생성하면 LLM이 긴 맥락에서 추론하고 추론하기에 어려운 환경을 만들 수 있음을 보여줍니다. 실험 결과는 다음과 같습니다. (1) 추상 요약에서 질문을 생성하는 제안된 방법은 LLM에 어려운 설정을 제시하며 ChatGPT와 같은 LLM과 오픈 소스 LLM(Alpaca, Llama) 사이에 성능 격차를 보여줍니다. (2) 오픈 소스 LLM은 원본 문서에서 생성된 질문에 대한 컨텍스트 의존도가 감소하지만 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. 특히 긴 컨텍스트(&gt;1024 토큰)의 경우 그렇습니다. 1
--- INTRODUCTION ---
ChatGPT, GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)이 다양한 벤치마크에서 우수한 성능을 보인 반면, 오픈소스 노력도 MMLU(Hendrycks 등, 2021), OpenLLMBoard(Anil 등, 2023; Beeching 등, 2023; Touvron 등, 2023)와 같은 다양한 애플리케이션과 벤치마크에서 따라잡기 위해 빠르게 진행되고 있습니다. 새로운 모델과 기술에 대한 빠른 속도로 진행되는 LLM의 새로운 시대로 접어들면서 이들 간의 기능, 한계 및 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 일관된 텍스트를 생성할 수 있는 LLM은 요약과 같은 작업에서 좋은 성능을 보이는 것으로 입증되었지만(Ouyang 등, 2022), LFQA에서의 성능은 상대적으로 덜 알려져 있습니다. 장문 질의응답(LFQA)은 다양하고 영향력 있는 실제 세계 응용 프로그램(예: 도움말 포럼, 문제 해결, 고객 서비스 등)에서 해결되지 않은 중요한 과제 중 하나입니다. 이러한 질문에 답하려면 종종 원본 문서에 분산된 정보 범위에 걸쳐 질의와 추론을 이해하는 복잡한 추론 능력이 필요합니다. 추상적 요약은 압축된 형태로 기사의 핵심을 포함합니다(Fabbri et al., 2020). 이러한 요약의 후속 질문은 소스 문서의 다른 부분을 연결하는 주제에 대한 더 깊은 이해가 필요할 것이라고 가정합니다. 게다가 Pang et al.(2022)은 긴 문서의 3분의 1 이상을 이해해야 하는 답변은 종종 인간에 의해 &quot;HARD&quot;로 평가된다는 것을 보여줍니다. 따라서 우리는 대규모 LLM과 작지만 입증된 성공적인 기본 LLM(예: Llama-7B, 13B) 및 이들의 정제된 버전(예: Alpaca-7B, 13B)의 차이를 분석하고 연구하기 위한 확장 가능한 평가 방법을 제안합니다. 이를 위해, 우리는 문서 요약에서 복잡한 질문을 생성하기 위한 구체적인 지침으로 ChatGPT를 촉구할 것을 제안합니다. 생성된 질문의 복잡성과 오픈소스 LLM의 답변 품질이라는 두 가지 측면에 대한 우리의 경험적 분석은 요약에서 생성된 후속 질문이 LLM의 추론 능력을 테스트하기 위한 도전적이면서도 더 현실적인 설정을 제시한다는 것을 보여줍니다. 장문 QA에 대한 인간 평가에 전적으로 의존하는 것은 비용이 많이 들고 확장하기 어렵기 때문에(Pagnoni et al., 2021), 우리는 대신 GPT-4를 활용하여 이전 연구에 따라 일관성, 관련성, 사실적 일관성 및 정확성에 대한 답변 품질을 평가합니다(Fabbri et al., 2020; Fan et al., 2019). 그러나 우리는 또한 더 작은 규모의 인간 평가를 수행하는데, 이는 GPT가 인간 평가와 높은 상관관계를 가지고 있어 우리의 평가를 신뢰할 수 있음을 보여줍니다. 이 연구의 주요 결과는 다음과 같습니다. • 추상적 요약에서 질문을 생성하는 우리의 제안된 방법은 20% 이상의 시간 동안 맥락을 여러 번 통과하는 더 긴 맥락에서 추론해야 합니다. • 증류된 LLM(Alpaca-7B, 13B)은 원본 문서에서 생성된 질문에 대한 맥락에 덜 의존하는 경향이 있지만, 문서 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. • 증류된 LLM에서 생성된 답변은 다른 설정에서 일관성이 있을 수 있지만 질문에서 벗어나 요약에서 생성된 질문에 대해 반복적이고 부분적으로 올바른 답변을 생성하는 경향이 있습니다(&gt;16.8%) • Alpaca-7B, 13B는 기본 LLM(Llama)보다 의미 있는 답변을 대부분 생성하지만 긴 맥락(&gt;1024 토큰)에 민감합니다. 2
--- RELATED WORK ---
긴 문서에 대한 추론: LLM은 상식적 추론(Talmor 등, 2019), 수학적 및 기호 추론(Huang 및 Chang, 2023; Cobbe 등, 2021), SQuaD, HotpotQA와 같은 질의 응답 작업과 같은 여러 작업에 대해 추론하는 놀라운 능력을 보여주었습니다. 그러나 이러한 작업의 대부분은 긴 맥락을 필요로 하지 않으며 답변은 종종 맥락에서 짧은 문구나 텍스트 범위입니다. 이 연구에서 우리는 LLM이 긴 문서의 요약에서 후속 질문을 생성하도록 LLM(ChatGPT)에게 촉구하여 더 깊은 이해 능력과 더 긴 맥락이 필요한 긴 문서에 대해 추론하는 것을 평가합니다. 모델 기반 평가: 이전 연구에서는 학습된 모델을 사용하여 자동 평가 지표를 제안했습니다(Zhang* 등, 2020; Laban 등, 2022). 특히 요약(Fabbri et al., 2020; Kryscinski et al., 2020)과 같은 장문 텍스트 생성 작업의 경우 일관성과 응집성이 소스 문서와 생성된 요약 간의 함축으로 측정됩니다. 최근 Liu et al.(2023)은 GPT-4가 인간과 가장 높은 상관 관계를 가지고 있으며 다른 모든 자동 평가를 능가한다는 것을 보여주었습니다.
--- METHOD ---
추상적 요약에서 긴 문서의 요약에서 후속 질문을 생성하는 것이 LLM이 긴 맥락에서 추론하고 추론할 수 있는 도전적인 환경을 만들 수 있음을 보여줍니다.
--- EXPERIMENT ---
모든 결과는 다음 사실을 확인합니다. (1) 추상 요약에서 질문을 생성하는 제안하는 방법은 LLM에 까다로운 설정을 제시하며 ChatGPT와 같은 LLM과 오픈소스 LLM(알파카, 라마) 사이에 성능 격차를 보입니다. (2) 오픈소스 LLM은 원본 문서에서 생성된 질문에 대한 컨텍스트 의존도가 낮지만 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. 특히 긴 컨텍스트(&gt;1024개 토큰)의 경우 그렇습니다. 1 서론 ChatGPT, GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)이 다양한 벤치마크에서 우수한 성능을 보인 반면, MMLU(Hendrycks 등, 2021), OpenLLMBoard(Anil 등, 2023; Beeching 등, 2023; Touvron 등, 2023)와 같은 오픈소스 노력도 다양한 애플리케이션과 벤치마크에서 따라잡기 위해 빠르게 진행되고 있습니다. 새로운 모델과 기술에 대한 빠른 진전과 함께 LLM의 새로운 시대로 접어들면서 이들 간의 역량, 한계, 차이점을 이해하는 것이 점점 더 중요해지고 있습니다. 일관된 텍스트를 생성할 수 있는 LLM은 요약과 같은 작업에서 좋은 성과를 보이는 것으로 입증되었지만(Ouyang et al., 2022), LFQA에서의 성과는 상대적으로 덜 알려져 있습니다. 장문 질의응답(LFQA)은 다양하고 영향력 있는 실제 세계 응용 프로그램(예: 도움말 포럼, 문제 해결, 고객 서비스 등)에서 중요한 미해결 과제 중 하나입니다. 이러한 질문에 답하려면 종종 원본 문서에 분산된 정보 범위에 걸쳐 쿼리와 추론을 이해하는 복잡한 추론 능력이 필요합니다. 추상적 요약은 압축된 형태로 기사의 핵심을 포함합니다(Fabbri et al., 2020). 이러한 요약의 후속 질문은 소스 문서의 여러 부분을 연결하는 주제에 대한 더 깊은 이해가 필요할 것이라고 가설을 세웁니다. 또한 Pang et al. (2022)는 긴 문서의 3분의 1 이상을 이해해야 하는 답변은 종종 인간에 의해 &quot;HARD&quot;로 평가된다는 것을 보여줍니다. 따라서 우리는 대규모 LLM과 작지만 입증된 성공적인 기본 LLM(예: Llama-7B, 13B) 및 이들의 정제된 버전(예: Alpaca-7B, 13B)의 차이를 분석하고 연구하기 위한 확장 가능한 평가 방법을 제안합니다. 이를 위해 우리는 문서 요약에서 복잡한 질문을 생성하기 위한 특정 지침으로 ChatGPT를 촉구할 것을 제안합니다. 두 가지 측면(생성된 질문의 복잡성과 오픈소스 LLM의 답변 품질)에 대한 우리의 경험적 분석은 요약에서 생성된 후속 질문이 LLM의 추론 능력을 테스트하기 위한 도전적이지만 더 현실적인 설정을 제시한다는 것을 보여줍니다. 장문 QA에 대한 인간 평가에 전적으로 의존하는 것은 비용이 많이 들고 확장하기 어렵기 때문에(Pagnoni et al., 2021), 대신 GPT-4를 활용하여 이전 연구(Fabbri et al., 2020; Fan et al., 2019)에 따라 일관성, 관련성, 사실적 일관성 및 정확성에 대한 답변 품질을 평가합니다. 그러나 더 작은 규모의 인간 평가도 수행하여 GPT가 인간 평가와 높은 상관관계를 가지고 있어 평가를 신뢰할 수 있음을 보여줍니다. 이 연구에서 얻은 주요 결과는 다음과 같습니다. • 추상 요약에서 질문을 생성하는 제안 방법은 더 긴 맥락에서 추론해야 하며, 맥락을 20% 이상 여러 번 통과해야 합니다. • 증류된 LLM(Alpaca-7B, 13B)은 원본 문서에서 생성된 질문에 대한 맥락에 덜 의존하는 경향이 있지만, 문서 요약에서 생성된 질문에 대한 생성 기능은 상당히 떨어집니다. • 증류된 LLM에서 생성된 답변은 다양한 설정에서 일관될 수 있습니다. 그러나 질문에서 벗어나는 경향이 있으며 요약에서 생성된 질문에 대해 반복적이고 부분적으로 올바른 답변을 생성합니다(&gt;16.8%) • Alpaca-7B, 13B는 대부분 기본 LLM(Llama)보다 의미 있는 답변을 생성하지만 긴 맥락(&gt;1024개 토큰)에 민감합니다.2 관련 작업 긴 문서에 대한 추론: LLM은 상식적 추론(Talmor 등, 2019), 수학적 및 기호 추론(Huang 및 Chang, 2023; Cobbe 등, 2021), SQuaD, HotpotQA와 같은 질의 응답 작업과 같은 여러 작업에 대해 추론하는 놀라운 능력을 보여주었습니다.그러나 이러한 작업의 대부분은 긴 맥락을 필요로 하지 않으며 답변은 종종 맥락의 짧은 문구나 텍스트 범위입니다.이 작업에서 우리는 LLM(ChatGPT)이 긴 문서의 요약에서 후속 질문을 생성하도록 촉구하여 더 깊은 이해 능력과 더 긴 맥락이 필요한 긴 문서에 대해 추론하도록 LLM을 평가합니다. 모델 기반 평가: 이전 연구에서는 학습된 모델을 사용한 자동 평가 지표를 제안했습니다(Zhang* 등, 2020; Laban 등, 2022). 특히 요약과 같은 장문 텍스트 생성 작업(Fabbri 등, 2020; Kryscinski 등, 2020)의 경우 일관성과 응집성이 소스 문서와 생성된 요약 간의 함의로 측정됩니다. 최근 Liu 등(2023)은 GPT-4가 인간과 가장 높은 상관 관계를 가지고 있으며 요약 작업에서 다른 모든 자동 평가 방법을 능가한다는 것을 보여주었습니다. 우리는 이러한 작업에서 영감을 얻어 평가 프롬프트를 설계하고 GPT-4를 생성된 답변의 평가자로 사용합니다. 3 제안된 평가 방법 3. 데이터 소스 다양하고 널리 사용 가능한 데이터 세트를 만들기 위해 Wikipedia 문서를 사용합니다. Wikipedia API의 Category 목록 메타데이터를 사용하여 주제의 다양성을 제어하고 다음 9개 도메인에서 문서를 수집합니다.물리학, 엔터테인먼트, 역사, 컴퓨터 과학, 사회 과학, 사회, 경제, 의학, 스포츠.문서 페이지는 종종 LLM의 맥락에 맞게 길어질 수 있습니다.따라서 SpaCy 토크나이저를 사용하여 최소 256개 토큰 길이의 문서 페이지에서 섹션별 데이터를 추출하고 짧은 섹션을 결합합니다.다른 모델 간의 공정한 비교를 위해 실험에서 최대 2k 토큰의 맥락 길이를 설정했습니다.또한 사전 처리 필터를 사용하여 정보가 없는 문서를 필터링합니다.자세한 내용은 부록 A.5에서 확인할 수 있습니다.3.2 ChatGPT를 사용한 질문 생성 질문 생성 방법을 2단계 프로세스로 공식화합니다.(1) 요약 및 (2) 요약에서 질문 생성.요약 먼저 3.1절에 설명된 대로 Wikipedia에서 섹션별 구절을 수집합니다. 그런 다음 ChatGPT(gpt-turbo-3.5)를 사용하여 원본 문서의 요약을 생성합니다. ChatGPT에 더 많은 맥락을 제공하기 위해 구절에서 기사의 제목과 도메인에 대한 정보를 제공합니다. 요약에서 질문 생성 이 단계에서는 ChatGPT가 문서 요약을 맥락으로 사용하여 질문을 생성하도록 합니다. 무작위 순서로 질문이 생성되는 것을 피하기 위해 ChatGPT에 답할 상위 3개의 복잡한 질문을 제공하도록 지시합니다. 질문 생성 프로세스의 유용성을 보여주기 위해 동일한 지침으로 질문이 구절에서 직접 생성되는 기준선도 설정합니다. 설정에 사용된 프롬프트에 대한 부록 A.1을 참조하세요. 요약하면 각 도메인에서 50개 구절에 대해 3개의 질문을 생성하여 각 설정에 대해 총 1350개의 질문을 생성합니다. 3.3 생성된 질문 복잡성 평가 Pang et al.(2022)은 질문의 복잡성을 평가하기 위해 광범위한 주석 지침을 설계했습니다. 인간이 &#39;HARD&#39;로 평가한 질문 중 26.7%의 질문(쉬운 질문보다 20.2% 높음)이 주어진 맥락에서 질문에 답할 수 있고 모호하지 않습니까?Q2. 질문에 답하기 위해 얼마나 많은 맥락이 본문에서 필요합니까?Q3: 질문이 본문을 여러 번 통과해야 합니까?선택지 A. 예 B. 아니요 A. 한두 문장만 B. 2문장 이상이지만 한 단락 미만 C. 전체 본문의 최소 1/3 D. 본문 대부분 A. 예 B. 아니요 표 1: 생성된 질문의 복잡성을 평가하기 위해 설계된 프롬프트. 주어진 정보의 최소 1/3 이상이 답변에 필요했습니다.생성된 질문의 질을 평가하기 위해 ChatGPT에 다음의 질문(표 1)을 프롬프트합니다.(1) 본문에서(QG-Passage) (2) 요약에서(QG-Summary). 이전 작업에 따라 다수결 투표를 통해 ChatGPT에서 답변 불가능한 것으로 평가된 질문을 제외하기 위해 다른 top_p {0.8, 0.9, 1}로 질문을 제시했습니다. 필터링 후 각 설정에서 생성된 질문 1278개가 있습니다. Q1: 명확성 Q2. 맥락 길이: = QG - 구절 QG - 요약 96.6% 94.7% 문장 또는 단락 미만 79.3% 구절의 1/3 이상 또는 대부분 Q3: 구절의 다중 통과 20.7% 75.7 % 24.3% 24.4% 31% 표 2: 생성된 질문의 복잡성을 평가하도록 설계된 프롬프트 4 결과 및 분석 4.1 실험 설정 맥락 길이로 인해 우리 설정에서는 few-shot 설정이 실행 불가능하므로 zero-shot 평가에 대한 모델 성능을 비교합니다. 우리는 최종 평가 데이터 세트에 대한 답변으로 자유형 텍스트를 생성하기 위해 다음 모델을 촉구합니다: ChatGPT(OpenAI, 2023), Alpaca-7B, 13B(Taori et al., 2023), LLaMa-7B, 13B(Touvron et al., 2023). 우리는 ChatGPT에 OpenAI API를 사용하고 HuggingFace 1의 오픈소스 LLM에 대한 체크포인트를 로드합니다. 답변 생성에 사용된 프롬프트는 부록 A에 있습니다. 다른 모델과의 공정한 비교를 위해 실험에서는 2k 시퀀스 길이를 넘는 입력은 고려하지 않습니다. 또한 Alpaca에서 질문을 생성하는 것을 테스트한 결과 지침을 따르지 않고 종종 관련 없는 콘텐츠를 생성하는 것으로 나타났습니다. 자세한 분석은 부록 A.2에서 확인할 수 있습니다. GPT-4는 평가자로서 장문 텍스트 생성 작업(요약(Liu et al., 2023))에서 인간 평가와 높은 상관관계를 보였으며, ROUGE 및 BLEU 점수와 같은 다른 자동 평가 지표를 능가했습니다. LLM은 설정에 대한 자유형 답변을 생성할 것으로 예상되므로 장문 텍스트 생성 지표(Fabbri et al., 2020)에 대한 이전 작업에서 영감을 얻어 일관성, 정확성 및 관련성에 대한 평가에 적용했습니다. 기본적으로 인간 평가에 대한 지침으로 사용되는 정의를 아래와 같이 방법에 적용했습니다. 일관성: 답변은 잘 구성되고 잘 정리되어야 하며 관련 정보의 더미가 되어서는 안 됩니다. 관련성: 답변은 질문과 맥락과 관련이 있어야 합니다. 답변은 간결해야 하며 질문에서 벗어나지 않아야 합니다. 사실적 일관성: 맥락이 답변의 주요 출처여야 합니다. 답변에는 조작된 사실이 포함되어서는 안 되며 맥락에 있는 정보를 수반해야 합니다. 정확도: 답변은 만족스럽고 질문에 대한 완전해야 합니다. 답변이 제시된 질문에 대한 답변인지 확인하여 답변의 정확성을 측정합니다. GPT-4가 4가지 지표 모두에서 0~3점(높을수록 좋음)의 척도로 답변을 평가하도록 합니다. GPT-4에서 얻은 모든 평가의 평균을 내고 결과를 표 3에 제시합니다. 평가 프롬프트는 부록 A.3.1에서 찾을 수 있습니다. 최적의 프롬프트는 항상 인간의 답변을 선호해야 하며 모델에서 생성된 답변에 편향되어서는 안 된다고 가정합니다. Laskar et al. (2023)은 ChatGPT와 같은 LLM이 TruthfulQA 데이터 세트에서 여전히 인간보다 성과가 낮음을 보여줍니다(Lin et al., 2022). 따라서 평가 프롬프트의 신뢰성과 충실성을 확인하기 위해 TruthfulQA 데이터 세트에서 GPT-4로 대리 테스트를 수행합니다. 우리는 ChatGPT와 오픈소스 LLM에서 생성된 답변을 무작위로 샘플링한 50개의 테스트 인스턴스에서 실제 사실과 비교 테스트하고, GPT-4 프롬프트를 사용한 평가 프롬프트가 90% 이상의 시간 동안 모델에서 생성된 답변보다 사실적 일관성과 정확성 측면에서 인간이 작성한 답변을 선호한다는 것을 발견했습니다. 또한, 우리는 LLM에서 생성된 답변에 대한 인간 평가도 수행하고, 섹션 A.4에서 GPT-4 평가와 인간 평가의 상관 관계를 논의합니다. 4.2 결과 실험 결과에 따르면 ChatGPT는 모든 지표에서 다른 LLM보다 22.4% -40.1%로 큰 폭으로 우수한 성과를 보였습니다.Passage 2.922.3.1.1.1.2.1,971.Summary 2.1.1.2.0.1.1.1.0.1.0.1.Passage 1.1.2.112.llama7B 0.1.alpaca7B llama13B 0.alpaca13B chatgpt 0.llama7B alpaca7B llama13B alpaca13B chatgpt 모델 QG-Passage QG-Summary 3. 컨텍스트 없음 컨텍스트 있음 컨텍스트 없음 컨텍스트 있음 2.ChatGPT 2.2.2.2.2.Alpaca-13B 2.2.2.2.LlaMa-13B 1.1.0.1.1.0.알파카-7B 2.1.1.1.0.LlaMa-7B 0.1.0.0. <Summary Passage 1.1.961.1,1.2,182.2.1.1,911.3.Summary 2.ارآرا ارت ارتيا لارتريا 0.alpaca13B chatgpt 0.0. llama 7B alpaca7B llama13B alpaca13B chatgpt 2.3.2.Summary 2.1.Passage 1.741.1.1.2.2.0.Figure 1: Graphs showing the breakdown of models with respect to different metrics used in evaluation: (a) Coherence (b) Relevance (c) Answer Accuracy (d) Factual Consistency 0.0.llama7B alpaca7B <Context Length <Table 3: Performance of different models based on GPT4 evaluation. The table shows average ratings across all metrics: accuracy, coherency, consistency, relevance. best performing LLM (Alpaca-13B). However; all the models including ChatGPT generate less accurate and relevant answers for QG-Summary when compared to QG-Passage; while the gap is much larger in open-source LLMs. We also find that most of the LLMs find context important in order to generate answers; however, the gap is much smaller for QG-Passage (avg. gap of 0.12 v.s. 0.2). Surprisingly, Alpaca-7B, 13B models perform better w/o context for QG-Passage. We hypothesize that questions directly generated from the context passage can be simple that could be directly answered from the parametric knowledge of LLMs without additional context. On further analysis, we observe that Alpaca-7B,13B performance drops significantly in longer contexts (Figure 2). We hypothesize that in a constrained sequence length setting, adding supporting context (even gold passage) may not be always helpful. We leave further analysis for future work and hope our analysis will motivate future research directions to study when to add/not add context for different tasks. Performance of LLMs on different metrics Figure 1 presents the performance of models across different metrics for QG-Summary. We observe two trends: (1) Open-source base LLMs (Llama7B,13B) suffer at all fronts significantly on generated answer quality whereas distilled models perform better than their counterparts (Llama) on all the settings. (2) QG-Summary provides a more challenging setting for all the LLMs: specifically, we notice that degradation in coherency score is negligent on ChatGPT and Alpaca-13B while other metrics like relevance, answer accuIlama7B alpaca 7B llama13B alpaca13B+ chatgpt Figure 2: Performance (avg. ratings) of LLMs across different context length. racy and factual consistency degrade consistently. We find open-source LLMs to drift from the question, generate partially correct answers and repeat more frequently in QG-Summary setting leading to lower scores. This further confirms that our proposed evaluation method QG-Summary challenges LLMs for deeper reasoning capabilities. // Context Length Analysis We analyze the effect of context length across LLMs in our proposed setting (QG-Summary). As expected, ChatGPT remains robust to context length until 2k tokens with Llama variants performing worse than other models (Figure 2). Interestingly, we find distilled models (Alpaca) being consistent until 1024 tokens, however beyond > 1024토큰의 경우 라마보다 성능이 더 빨리 저하됩니다.5
--- CONCLUSION ---
ChatGPT와 같은 LLM과 오픈소스 성공 LLM이 등장하면서 다양한 LLM의 역량과 한계를 이해하는 것이 매우 중요합니다. 더 긴 맥락을 참조하여 LLM의 더 깊은 추론 능력을 테스트하기 위해 긴 문서 요약에 대한 ChatGPT에서 생성한 질문에 대해 LLM에서 생성한 답변을 평가합니다. 결과에 따르면 제안한 질문 생성 방법은 LLM에 도전적인 설정을 제시하고 대규모 LLM과 오픈소스 LLM 간의 성과 격차를 밝혀냅니다. 분석이 제한된 시퀀스 길이 설정에서 더 긴 맥락을 활용하고 소규모 LLM에 더 나은 장문 텍스트 생성을 개발하는 것과 같은 미래 연구 방향에 동기를 부여하기를 바랍니다. 6 한계 이 연구에서는 요약에서 질문을 생성하는 자동 평가 설정을 제안하고 LLM에서 생성된 답변은 다양한 메트릭에 대해 GPT-4를 사용하여 평가합니다. 실험 결과에 따르면 제안한 평가 설정은 LLM에 도전적인 설정인 것으로 나타났습니다. 그러나 연구에는 몇 가지 한계가 있을 수 있습니다. 평가자로서의 GPT-4 GPT-4는 장문 텍스트 생성을 위한 인간 평가와 높은 상관관계를 보였지만(Liu et al., 2023), 평가를 위해 GPT-4를 사용하는 기능은 그 자체로 활발한 연구 분야입니다. 따라서 우리의 결과는 GPT-4의 발견되지 않은 기능에 의해 제한될 수 있습니다. 질문 생성을 위한 ChatGPT ChatGPT에서 유도된 질문에 대한 답변을 생성하면 ChatGPT의 낙관적인 결과로 이어질 수 있습니다. 그러나 의미 있는 질문을 생성하기 위한 다른 기준에는 한계가 있습니다. 질문 생성을 위해 다른 LLM을 사용하는 것에 대한 광범위한 분석을 보여줍니다(부록 A.2). 알려지지 않은 교육 데이터 ChatGPT와 같은 대규모 LLM의 교육 데이터 분포에 대해서는 알려진 바가 거의 없습니다. 다른 방법과 데이터 분포로 교육된 모델은 공정한 비교를 위한 평가를 더 어렵게 만듭니다. 참고문헌 Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, 임현택, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant 미스라, 메이삼 무살렘, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov 및 Yonghui Wu. 2023. Palm 2 기술 보고서. Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf. 2023. Open Ilm 리더보드. https://huggingface.co/spaces/ HuggingFaceH4/open_11m_leaderboard. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman. 2021. 수학 단어 문제를 풀기 위한 검증자 훈련. Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, Dragomir Radev. 2020. Summeval: 요약 평가 재평가. arXiv 사전 인쇄본 arXiv:2007.12626. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli. 2019. ELI5: 장문 질의응답. 이탈리아 피렌체에서 열린 제57회 전산언어학 협회 연례 회의록, 3558-3567쪽. 전산언어학 협회. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 2021. 대규모 멀티태스크 언어 이해 측정. 국제 학습 표현 컨퍼런스(ICLR) 회의록. Jie Huang, Kevin Chen-Chuan Chang. 2023. 대규모 언어 모델에서의 추론을 향하여: 조사. Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette. 2018. NarrativeQA 독해 능력 챌린지. Association for Computational Linguistics의 트랜잭션, 6:317–328. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher. 2020. 추상적 텍스트 요약의 사실적 일관성 평가. 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 9332-9346페이지, 온라인. Association for Computational Linguistics. Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst. 2022. SummaC: 요약에서 불일치 감지를 위한 NLI 기반 모델 재방문. Association for Computational Linguistics의 트랜잭션, 10:163–177. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. 벤치마크 데이터 세트에 대한 chatgpt의 체계적 연구 및 종합적 평가. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: 모델이 인간의 거짓을 어떻게 모방하는지 측정. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: 더 나은 인간 정렬을 갖춘 gpt-4를 사용한 Nlg 평가. OpenAI. 2023. Gpt-4 기술 보고서. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 2022. 인간 피드백을 통해 지시를 따르도록 언어 모델 훈련. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov. 2021. FRANK를 사용한 추상 요약의 사실성 이해: 사실성 지표의 벤치마크. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 4812-4829쪽, 온라인. 컴퓨터 언어학 협회. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel Bowman. 2022. 품질: 긴 입력 텍스트로 질문에 답하기, 맞아요! 2022년 북미 컴퓨터 언어 학회 회의록: 인간 언어 기술, 5336~5358쪽, 미국 시애틀. 컴퓨터 언어 학회. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant. 2019. CommonsenseQA: 상식적 지식을 목표로 하는 질문에 답하기 챌린지. 2019년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4149-4158쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학회. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, Yoav Artzi. 2020. Bertscore: bert를 사용한 텍스트 생성 평가. International Conference on Learning Representations에서. 부록 A.1 질문 생성에 사용된 프롬프트 이전 연구(Kočiský et al., 2018; Pang et al., 2022)의 분석에 따라 질문 생성 방법을 2단계 프로세스로 공식화했습니다. (1) 요약 및 (2) 요약에서 질문 생성. 첫 번째 단계에서는 아래와 같이 요약을 생성하기 위한 프롬프트를 설계합니다. {domain}에서 {title}의 맥락에서 아래 문단을 요약합니다. 다음 단계에서는 아래와 같이 ChatGPT에 요약에서 질문을 생성하도록 요청합니다. 아래의 맥락을 사용하여 후속 질문을 생각해 냅니다. 생성된 질문을 답변하기 위한 복잡성이 감소하는 순서로 순위를 매기고 상위 3개만 표시합니다. {context} 질문 생성 프로세스의 유용성을 보여주기 위해 동일한 지침으로 질문이 구절에서 직접 생성되는 기준선도 설정합니다. 기준선에 사용된 프롬프트는 다음과 같습니다. 아래의 맥락을 사용하여 세 개의 질문을 생각해 냅니다. 생성된 질문을 답변하기 어려운 순서대로 순위를 매기고 상위 3개만 표시합니다.{context} A.2 오픈 소스 LLM을 사용한 질문 생성 공정한 평가 설정을 만들기 위해 Alpaca-7B,13B 모델에 50개 인스턴스에 대한 질문을 요약하고 생성하도록 요청합니다. 지침에 맞춰지지 않은 모델(예: Llama)에서 생성된 질문은 고려하지 않습니다. 4절에서 설명한 생성된 질문에 대한 평가 방법을 통해 Alpaca에서 생성된 질문은 답변할 수 없고(맥락에 존재하지 않음) 80% 이상의 경우 횡설수설하는 내용이 포함되어 있음을 발견했습니다. 아래 표는 Alpaca에서 생성된 질문에 대한 평가를 보여줍니다. A.2.1 ChatGPT를 사용한 질문 생성 평가 4절에서 설명한 대로 생성된 질문의 복잡성을 확인하기 위해 다음 프롬프트로 ChatGPT를 요청합니다. 생성된 질문의 복잡성을 결정하는 것에 대한 피드백을 요청하고자 합니다. &#39; &#39; 평가 지표 Q1: 명확성 Q2. 문맥 길이: 문장 또는 단락 미만 최소한 구절의 1/3 또는 대부분 Q3: 구절의 다중 통과 QG - 구절 12.5% QG 요약 8.3% 98.8% 98.5% 1.2% 0% 1.5% 0% 표 4: 위에 표시된 맥락과 관련하여 AI 보조자가 Alpaca에서 생성한 질문의 복잡성을 평가하도록 설계된 프롬프트. \n\n 각 질문에 대해 모호성, 맥락 및 추론 능력의 차원에 대한 각 생성된 질문의 복잡성을 평가하세요. \n\n Q1: 질문에 주어진 맥락에서 답할 수 있고 모호하지 않습니까? A. 예 B. 아니요 \n\n Q2. 질문에 답하기 위해 구절의 얼마만큼이 맥락으로 필요합니까? A. 본문에서 한두 문장만 B. 두 문장 이상이지만 한 문단 미만 C. 주어진 전체 맥락의 최소 1/3 D. 주어진 대부분의 맥락 \n\n Q3: 이 질문은 본문을 여러 번 통과해야 합니까? A. 예 B. 아니요. 주어진 맥락 외에 주제에 대한 사전 지식이 없다고 가정합니다. 사전 형태로 선택 사항을 출력하세요. (예: &#39;Q1&#39;:<your answer choice for Q1> &#39;, &#39;Q2&#39;:<your answer choice for Q2> &#39;, &#39;Q3&#39;: &#39;<your answer choice for Q3> &#39;, &#39;Q4&#39;:<your answer choice for Q4> &#39;). \n\n 다음 줄에서 평가에 대한 포괄적인 설명을 제공하여 잠재적 편견을 피하고 응답이 제시된 순서가 판단에 영향을 미치지 않도록 하십시오. LLM은 서로 다른 교육 데이터 혼합으로 교육되므로, 우리는 ChatGPT에 주어진 맥락에 따라서만 질문에 답하도록 특별히 요청합니다. A.2.2 답변 생성을 위한 프롬프트 LLM에서 생성된 질문에 대한 답변을 생성하기 위해 다음을 프롬프트합니다. 생성된 모든 질문에 대해 다음과 같이 모델을 프롬프트합니다. 맥락을 고려하여 아래 질문에 답합니다. 맥락: {맥락} 질문: {질문} 답변: {답변} 지표 코헨-카파 점수 일관성 0.관련성 0.정확도 0.0.A.3 평가자로서의 GPT-4 A.3.1 GPT-4 평가에 사용되는 프롬프트 LLMS에서 생성된 답변을 평가하기 위해 GPT-4에 다음 네 가지 지표(일관성, 관련성, 정확성 및 사실적 일관성)에 대해 리커트 척도(0~3점, 높을수록 좋음)로 답변을 평가하도록 요청합니다. GPT-4의 프롬프트로 사용된 평가 프롬프트는 아래와 같습니다. 시스템 프롬프트: 귀하는 다음 세로형에서 답변의 질을 확인하는 데 유용하고 정확한 조수입니다. 일관성, 관련성, 사실적 일관성, 정확성. prompt : 위에 표시된 맥락을 기반으로 사용자 질문에 답변한 두 AI 어시스턴트의 성과에 대한 점수와 피드백을 요청하고자 합니다. 일관성, 관련성, 사실적 일관성 및 정확성의 4가지 지표에 대한 답변 품질을 평가해 주세요. 각 지표의 정의는 귀하에게 제공됩니다. 일관성 - 답변은 잘 구성되고 잘 정리되어야 합니다. 관련성 답변은 질문과 맥락과 관련이 있어야 합니다. 또한 답변은 묻는 질문에서 벗어나지 않아야 합니다. 사실적 일관성 맥락은 답변의 주요 출처여야 합니다. 답변에는 조작된 사실이 포함되어서는 안 되며 맥락에 있는 정보를 수반해야 합니다. 정확성 어시스턴트가 제공한 답변이 간결한 방식으로 질문에 올바르게 답변합니까? 위에 정의된 각 지표에 대한 각 AI 어시스턴트 응답에 0~3점 척도로 점수를 부여하세요. 점수가 높을수록 성과가 더 좋습니다. AI 어시스턴트 간의 성과를 비교하지 말고 개별적으로 평가하세요. 두 개의 새 줄을 입력하세요. 다음으로, 둘 중 선호하는 답변과 평가에 대한 포괄적인 설명을 제공하여 잠재적인 편견을 피하고 응답이 제시된 순서가 판단에 영향을 미치지 않도록 하십시오. 점수는 제공된 예와 같이 사전 형식이어야 합니다. 예: &#39;Assistant 1&#39;: &#39;coherence&#39;:<int score> , &#39;관련성&#39;:<int score> , &#39;사실적 일관성&#39;:<int score> , 정확성&#39;:<int score> , &#39;도우미 &#39;설명&#39;: &#39;<Preference: Assistant 1 or Assistant 2 or Both or Neither> .<your explanation> &#39;. 2&#39;: ... A.4 사례 연구: 인간 평가 대 GPT-우리는 GPT-4 등급과 무관하게 LLM이 생성한 답변의 질을 평가하기 위해 50개의 QA 쌍에 주석을 달았습니다. 두 명의 주석 작성자에게 섹션 4에 설명된 것과 유사한 지침과 GPT4(섹션 A.3.1)와 같은 설문지가 제공되었습니다. 공정한 평가를 위해 주석 작성자에게 답변을 생성한 모델을 공개하지 않았습니다. 표 5에는 GPT-4와 인간 평가의 일치 점수가 포함되어 있습니다. 자유형 텍스트 생성에 대한 다양한 지표에서 높은 일치 점수는 GPT-4를 사용한 평가 방법이 신뢰할 수 있음을 보여줍니다. 또한 부록(표 6)에 몇 가지 정성적 예를 제시합니다. A.5 데이터 수집 우리는 Wikipedia API를 사용하여 기사를 수집하여 평가 설정을 만듭니다. 그러나 모든 기사가 설정에 유용하지는 않을 수 있다는 것을 알게 되었습니다. 영어: 참조 링크, 주제 목록, 용어집 등이 포함된 Wikipedia 문서와 같은 경우 두 가지 수준의 필터링을 적용합니다.(1) 토큰이 2k 미만인 모든 문서를 제거합니다.(2) 정보가 없는 섹션 목록을 정리하고 이 연구에서 해당 섹션을 고려 대상에서 필터링합니다.[&#39;참조&#39;, &#39;참조&#39;, &#39;메모&#39;, &#39;출처&#39;, &#39;외부 링크&#39;, &#39;역사&#39;, &#39;추가 자료&#39;] A.6 정성적 예 아래 표는 인간 주석 작업의 정성적 예를 제시하고 인간과 GPT-4의 일부 평가를 보여줍니다.정성적 예 질문 1: 학제간 연구와 단일 학문 전문화의 차이점은 무엇입니까?평가: 문제 - 없음, 인간 점수 - 3, GPT-4 -ChatGPT: 학제간 연구는 여러 학문의 방법과 접근 방식을 한 주제에 적용하는 것을 포함하는 반면, 단일 학문 전문화는 단일 주제에 대한 심층 연구와 전문성을 포함합니다. 평가: 문제 - 반복, 인간 점수 - 2, GPT-4 -Alpaca-13B: 학제간 연구는 두 개 이상의 학문을 결합하여 새로운 연구 분야를 만드는 것입니다. 전문화는 단일 학문에 초점을 맞추고 해당 분야의 전문가가 되는 것을 목표로 합니다. 학제간 연구는 두 개 이상의 학문을 결합하여 새로운 연구 분야를 만드는 것입니다. 전문화는 단일 학문에 초점을 맞추고 해당 분야의 전문가가 되는 것을 목표로 합니다. 해당 분야에서. 질문 2: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 어떻게 필요합니까? 평가: 문제 - 없음, 인간 점수 - 3, GPT-4 -Alpaca-13B: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 필요합니다. 리더십은 전략적 우선순위를 설정하고 조직이 목표에 부합하도록 하는 데 필요합니다. 조직이 목표와 이를 달성하기 위한 전략을 인식하도록 하는 데 커뮤니케이션이 필요합니다. 조직이 목표를 달성하기 위해 함께 일하고 있는지 확인하는 데 협업이 필요합니다. 조직이 지속적으로 성과를 개선하기 위해 노력하고 있는지 확인하려면 지속적인 개선이 필요합니다.평가: 문제 - 정확성, 인간 점수 - 1, GPT-4 -Alpaca-7B: 우수한 자산 관리에는 리더십, 커뮤니케이션, 협업 및 지속적인 개선이 필요합니다.질문 3: CSAB가 컴퓨터 과학 분야에서 중요하다고 생각하는 네 가지 영역은 무엇입니까?평가: 문제 - 사실적 일관성, 인간 점수 - 1, GPT-4 -Alpaca-7B: 1. 계산 이론 2. 계산 가능성 이론 3. 프로그래밍 언어 이론 4. 형식적 방법 ChatGPT: CSAB가 컴퓨터 과학 분야에서 중요하다고 생각하는 네 가지 영역은 계산 이론, 알고리즘 및 데이터 구조, 프로그래밍 방법론 및 언어, 컴퓨터 요소 및 아키텍처입니다.표 6: GPT-4 및 인간 평가가 있는 LLM에서 생성된 답변의 몇 가지 예 범례 LLM, 모델에서 놓친 불일치 GPT-4에서 감지된 불일치.
"
"--- ABSTRACT ---
Previous research observed accuracy degradation when replacing the attention softmax with a pointwise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1
--- INTRODUCTION ---
The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence length which makes parallelization challenging [24, 7]. In this report we explore point-wise alternatives to the softmax operation which do not necessarily output a probability distribution. As a highlight, we observe that attention with ReLU divided by sequence length can approach or match traditional softmax attention in terms of scaling behavior as a function of compute for vision transformers. This result presents new opportunities for parallelization, as ReLU-attention can be parallelized over the sequence length dimension with fewer gather operations than traditional attention. 2
--- RELATED WORK ---
Previous research has explored substituting softmax with ReLU [25, 14] or squared ReLU [15]. However, these approaches do not divide by sequence length, which we experimentally find is important to reach accuracy comparable to softmax. In addition, previous research [21] has replaced softmax while still requiring normalization over the sequence length axis to ensure Fs) 0.82 7 — softmax a — relu/seqien g yoo] © s/32 8 0.cs m sf6 bea a © BB2 § eo” B/l6 5° 8° 3 °° W0-76 5 0.~30.742 0.o £ 20.72£ G 0.0.70 bo $ 90.0.68 < ° ra & 10? TPU core hours 10? TPU core hours Figure 1: Replacing softmax with relu/seqlen approaches or matches the scaling performance of traditional attention for vision transformers [10] with qk-layernorm [8]. This figure displays results for small to large vision transformers trained on ImageNet-21k [9] for 30 epochs. We report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. Attention with ReLU can be parallelized over the sequence length dimension with less gather operations than softmax attention. --- --Training dataset i21k. Model S/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. 0.0.0.0.ImageNet-1k accuracy (%) § x Pa ImageNet-1k accuracy (%) g Ry & ImageNet-1k accuracy (%) g Ry & 0.650 0.00 05 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen Training dataset ilk. Model S/32. Training dataset ilk. Model S/16. Training dataset ilk. Model S/8. 0.ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) 00 oS 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen - softmax —e relu —‘— squared relu —+- gelu —*— softplus —® identity —*-— relué —* sigmoid Figure 2: Replacing softmax with L~“h where h € {relu, relu’, gelu, softplus, identity, relu6, sigmoid} and L is sequence length. We typically observe the best results when a is close to 1. There is no clear best non-linearity at a % 1, so we use ReLU in our main experiment for its speed. he attention weights sum to one. This retains the downside of requiring a gather. After writing an initial version of this note, it was brought to our attention hat the variant of ReLU-atttention we study was also explored with a theoretical motivation [3, 12]. Moreover, there is extensive literature which removes activation functions altogether so that attention is inear [16, 22, 18], which is useful for long sequence engths.! In our experiments, removing the activation entirely reduced accuracy. 3
--- METHOD ---
Attention. Attention transforms d-dimensional queries, keys, and values {qi,ki,v;}#_, with a two step procedure. First, attention weights a,; are produced viaaiy=o (= [aha kx] ; j 1Concretely, with linear attention, the order of matrix multiplies can be switched from (qk )u to q(k' v) which changes the compute required from O(dL?) to O(d?L) where q,k,v € REX¢ are the queries, keys, and values and L is sequence length. (1) where @ is typically softmax. Next, the attention . L weights are used to compute outputs 0; = )>¥_, aijv;. This report explores point-wise alternatives to ¢. ReLU-attention. We observe that ¢ = L~!relu is a promising alternative to ¢ = softmax in Equation 1. We refer to attention with ¢ = L~'relu as ReLUattention. Scaled point-wise attention. More generally, our
--- EXPERIMENT ---
s training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1 Introduction The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence length which makes parallelization challenging [24, 7]. In this report we explore point-wise alternatives to the softmax operation which do not necessarily output a probability distribution. As a highlight, we observe that attention with ReLU divided by sequence length can approach or match traditional softmax attention in terms of scaling behavior as a function of compute for vision transformers. This result presents new opportunities for parallelization, as ReLU-attention can be parallelized over the sequence length dimension with fewer gather operations than traditional attention. 2 Related work Previous research has explored substituting softmax with ReLU [25, 14] or squared ReLU [15]. However, these approaches do not divide by sequence length, which we experimentally find is important to reach accuracy comparable to softmax. In addition, previous research [21] has replaced softmax while still requiring normalization over the sequence length axis to ensure Fs) 0.82 7 — softmax a — relu/seqien g yoo] © s/32 8 0.cs m sf6 bea a © BB2 § eo” B/l6 5° 8° 3 °° W0-76 5 0.~30.742 0.o £ 20.72£ G 0.0.70 bo $ 90.0.68 < ° ra & 10? TPU core hours 10? TPU core hours Figure 1: Replacing softmax with relu/seqlen approaches or matches the scaling performance of traditional attention for vision transformers [10] with qk-layernorm [8]. This figure displays results for small to large vision transformers trained on ImageNet-21k [9] for 30 epochs. We report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. Attention with ReLU can be parallelized over the sequence length dimension with less gather operations than softmax attention. --- --Training dataset i21k. Model S/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. 0.0.0.0.ImageNet-1k accuracy (%) § x Pa ImageNet-1k accuracy (%) g Ry & ImageNet-1k accuracy (%) g Ry & 0.650 0.00 05 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen Training dataset ilk. Model S/32. Training dataset ilk. Model S/16. Training dataset ilk. Model S/8. 0.ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) 00 oS 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen - softmax —e relu —‘— squared relu —+- gelu —*— softplus —® identity —*-— relué —* sigmoid Figure 2: Replacing softmax with L~“h where h € {relu, relu’, gelu, softplus, identity, relu6, sigmoid} and L is sequence length. We typically observe the best results when a is close to 1. There is no clear best non-linearity at a % 1, so we use ReLU in our main experiment for its speed. he attention weights sum to one. This retains the downside of requiring a gather. After writing an initial version of this note, it was brought to our attention hat the variant of ReLU-atttention we study was also explored with a theoretical motivation [3, 12]. Moreover, there is extensive literature which removes activation functions altogether so that attention is inear [16, 22, 18], which is useful for long sequence engths.! In our experiments, removing the activation entirely reduced accuracy. 3 Method Attention. Attention transforms d-dimensional queries, keys, and values {qi,ki,v;}#_, with a two step procedure. First, attention weights a,; are produced viaaiy=o (= [aha kx] ; j 1Concretely, with linear attention, the order of matrix multiplies can be switched from (qk )u to q(k' v) which changes the compute required from O(dL?) to O(d?L) where q,k,v € REX¢ are the queries, keys, and values and L is sequence length. (1) where @ is typically softmax. Next, the attention . L weights are used to compute outputs 0; = )>¥_, aijv;. This report explores point-wise alternatives to ¢. ReLU-attention. We observe that ¢ = L~!relu is a promising alternative to ¢ = softmax in Equation 1. We refer to attention with ¢ = L~'relu as ReLUattention. Scaled point-wise attention. More generally, our experiments will explore ¢ = L~“h for a € [0,1] and h € {relu, relu”, gelu, softplus, identity, relu6, sigmoid} 6, 13}. Sequence length scaling. We observe that scaling yy a term involving sequence length L is beneficial for high accuracy. This scaling is absent from prior work which removes softmax [15, 18]. While the central justification for sequence length scaling is empirical, we provide brief analytical motivation. Transformers are currently designed with softmax atention for which ya ay = This implies that E,;[a;j] = L~1. While it is unlikely that this is a ry condition, @ = L~'relu does ensure that E,[a;j] is O(L~') at initialization. Preserving this condition may alleviate the need to change other hy --- --Training dataset i21k. Model $/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. ° u a g yo76 30.70.74 70.74 F£ £ £ 5 5S072 S072 S8 8070 070 0.we we we 2 2Z 0.68 Z 0.68 Z 0.a a a 8 8£0.66 £0.66 £ 0.0.64 1 0.64 0.0.0 05 1.0 us 2:0 0.0 OS 1.0 15 2:0 0.0 05 1.0 15 2.Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen —* relu —*— squared relu -- relu without qk-layernorm -e- squared relu without qk-layernorm Figure 3: The effect of removing qk-layernorm [8] on attention with ReLU and squared ReLU scaled by L~* where L is sequence length. Results are shown for the $/32, S/16, and S/8 vision transformer models [10, 4] trained on ImageNet-21k. Training dataset i21k. Model $/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. ek yore yore Sore, ° = = = © 70.74 70.74 B0.g g g S S S 30.72 30.72 9 0.5 540.70 40.70 40.2 22 2S 0.68 S 0.68 S 0.D D D o o o £0.66 £0.66 £0.0.64 0.64 0.0.0 0.5 10 v5 210 0.0 05 10 15 210 0.0 OSs 10 15 2:Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen —e relu —*— squared relu ~e- relu with gating “a squared relu with gating Figure 4: The effect of using a gate attention unit [15] on attention with ReLU and squared ReLU scaled by L~“ where L is sequence length. Results are shown for the $/32, 8/16, and S/8 vision transformer models [10, 4] trained on ImageNet-21k. perparameters when replacing softmax. ImageNet-21k we train for 30 epochs, and in our ex periments on ImageNet-1k we train for 300 epochs. As a result, both training runs use a roughly similar number of steps of around 9e5. We use ViTs with qk-layernorm [8] as this was previously observed to be necessary to prevent instability when scaling model size. However, we ablate that this is not an important component at the scales we test. We use i21k and ilk to mean ImageNet-21k and ImageNet-1k, respectively, At initialization the elements of q and k are O(1) and so (are) will also be O(1). Activation functions such as ReLU preserve O(1),? and so a factor L~! is necessary for Ej[a;;] to be O(L~'). 4 Experiments Experimental setup. Our experiments use ImageNet-21k and ImageNet-1k [9] training config urations from the BigVision codebase [4] without modifying hyperparameters.® In our experiments on ?With the exception of squared ReLU. ’For ImageNetlk we use the base config https: //github.com/google-research/big_vision/blob/main/ and report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. When evaluating transfer performance on downstream tasks we big_vision/configs/vit_i1k.py. For ImageNet21k we use the base config https: //github.com/google-research/big_ vision/blob/main/big_vision/configs/vit_i21k. py. --- --use a 10-shot linear probe averaged over three seeds. The downstream tasks are Caltech Birds [27], Caltech101 [11], Stanford Cars [19], CIFAR-100 [20], DTD [5], ColHsit [17], Pets [23], and UC Merced [28]. Main experiment. Figure | illustrates that ReLUattention matches the scaling trends for softmax attention for ImageNet-21k training. On the x-axis we display the total core hours required for the experiment. As an advantage, ReLU-attention enables parallelization over the sequence length dimension with fewer gather operations than softmax attention. Effect of sequence length scaling. Figure 2 examines the effect of sequence length scaling for various point-wise alternatives to softmax. Concretely, we replace softmax with L~°h for a € [0,1] and h € {relu, relu”, gelu, softplus, identity}. On the z-axis we display a. The y-axis displays accuracy for the $/32, 8/16, and $/8 vision transformer models [10, 4]. The best results are typically achieved when a is close to 1. Since there is not clear best non-linearity, we use ReLU in our main experiment as it is faster. Effect of qk-layernorm. Our main experiments use qk-layernorm [8] in which queries and keys are passed through LayerNorm [1] before computing attention weights. We use qk-layernorm by default as it was found to be necessary to prevent instability when scaling up model size [8]. Figure 3 shows the effect of removing qk-layernorm. The results indicate that qk-layernorm does not have a large effect for these models, but this may change at scale. Effect of adding a gate. Previous work removing softmax adds a gated unit and does not scale by sequence length [15]. Concretely, in the gated attention unit [15] an extra projection produces output which is combined through elementwise-multiplication before the out projection. In Figure 4 we investigate whether the presence of a gate removes the need for sequence length scaling. Overall we observe that the best accuracy is still achieved with sequence length scaling, with or without the gate. Note that gating increases the core hours required for the experiment by roughly 9.3% for the S/8 model with ReLU. 5
--- CONCLUSION ---
This report leaves many open questions. In particular, we are unsure why the factor L~! improves performance or if this term could be learned. Moreover, it is likely that there is a better activation function that we do not explore. Acknowledgements We thank Lucas Beyer, Mostafa Dehghani, and David Fleet for their helpful comments and suggestions. We thank the members of the Google DeepMind PAGI team for their support of this effort, Jascha Sohldickstein, Noah Fiedel, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Avi Singh, Azade Nova, Ben Adlam, Bernd Bohnet, Daniel Freeman, Gamaleldin Elsayed, Gaurav Mishra, Hanie Sedghi, Isabelle Simpson, Izzeddin Gur, JD Co-Reyes, James Harrison, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Max Bileschi, Merrie Morris, Roman Novak, Rosanne Liu, Sharad Vikram, Tris Warkentin, Yundi Qian. References 1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 2) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable incontext learning with in-context algorithm selection. arXiv preprint arXiv:2306.046387, 2023. 4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet1k. arXiv preprint arXiv:2205.01580, 2022. URL https: //arxiv.org/abs/2205.01580. a Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014. https: //arxiv.org/abs/1311.3618. [6] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. InIEEE international conference on acoustics, speech and signal processing, pages 8609-8613. IEEE, 2013. (7| Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances --- --[8] [9] [10] [11]13) 14)in Neural Information Processing Systems, 35:16344— 16359, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. https://ieeexplore. ieee. org/document/5206848. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16xwords: Transformers for image recognition at scale. n International Conference on Learning Representations (ICLR), 2021. https: //arxiv.org/abs/2010. 11929. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested onobject categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178-178. EEE, 2004. Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. arXiv preprint arXiv:2307.11353, 2023. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In International Conference on Machine Learning, pages 4376-4386. PMLR, 2020. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on [17] [18] [19] [20] [21][27] Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR, 13— 18 Jul 2020. URL https: //proceedings .mlr.press/ vi19/katharopoulos20a. html. Jakob Nikolas Kather, Frank Gerrit Zollner, Francesco Bianconi, Susanne M Melchers, Lothar R Schad, Timo Gaiser, Alexander Marx, and Cleo-Aron Weis. Collection of textures in colorectal cancer histology. Zenodo https://doi. org/10, 5281, 2016. Soroush Abbasi Koohpayegani and Hamed Pirsiavash. Sima: Simple softmax-free attention for vision transformers. arXiv preprint arXiv:2206.08898, 2022. Jonathan Krause, Michael Stark, Jia Deng, and Li FeiFei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision (ICCV) Workshops, 2013. https:// ieeexplore.ieee.org/document/6755945. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009. https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR. pdf. Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust training of neural networks using scale invariant architectures. In International Conference on Machine Learning, pages 12656-12684. PMLR, 2022. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems, 34:21297-21309, 2021. Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498-3505. IEEE, 2012. Markus N Rabe and Charles Staats. Self-attention does not need o(n?) memory. arXiv preprint arXiv:2112.05682, 2021. Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and softmax in transformer. arXiv preprint arXiv:2302.06461, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. --- --[28] Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270-279, 2010.
"	"--- ABSTRACT ---
이전 연구에서는 ReLU와 같은 점별 활성화로 어텐션 소프트맥스를 대체할 때 정확도가 저하되는 것을 관찰했습니다. 비전 변환기의 맥락에서, 우리는 시퀀스 길이로 나누면 이러한 저하가 완화되는 것을 발견했습니다. ImageNet-21k에서 작은 것부터 큰 것까지의 비전 변환기를 훈련하는 실험에서 ReLU-어텐션은 컴퓨팅의 함수로서 스케일링 동작 측면에서 소프트맥스-어텐션의 성능에 근접하거나 일치할 수 있음을 나타냈습니다. 이 보고서에서는 반드시 확률 분포를 출력하지 않는 소프트맥스 연산에 대한 점별 대안을 살펴봅니다. 하이라이트로, 시퀀스 길이로 나눈 ReLU를 사용한 어텐션은 비전 변환기에 대한 컴퓨팅의 함수로서 스케일링 동작 측면에서 기존 소프트맥스 어텐션에 근접하거나 일치할 수 있음을 관찰했습니다. 이 결과는 병렬화에 대한 새로운 기회를 제공하는데, ReLU-어텐션은 기존 어텐션보다 수집 연산이 적은 시퀀스 길이 차원에서 병렬화될 수 있기 때문입니다.
--- INTRODUCTION ---
변압기 아키텍처[26]는 최신 머신 러닝에서 널리 사용됩니다. 변압기의 핵심 구성 요소인 주의[2]에는 토큰에 대한 확률 분포를 생성하는 소프트맥스가 포함됩니다. 소프트맥스는 지수 계산과 합 시퀀스 길이로 인해 비용이 많이 들기 때문에 병렬화가 어렵습니다[24, 7].
--- RELATED WORK ---
이전 연구에서는 softmax를 ReLU[25, 14] 또는 제곱 ReLU[15]로 대체하는 것을 살펴보았습니다. 그러나 이러한 접근 방식은 시퀀스 길이로 나누지 않는데, 이는 softmax와 비슷한 정확도에 도달하는 데 중요하다는 것을 실험적으로 발견했습니다. 또한 이전 연구[21]에서는 softmax를 대체하면서도 시퀀스 길이 축에 대한 정규화를 요구하여 0.softmax relu/seqlen 0.S/S/0.. B/× B/L/0.ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.0.0.Avg. 8개 데이터 세트에 대한 10샷 선형 전송 0.0.0.TPU 코어 시간 10² TPU 코어 시간을 보장했습니다. 그림 1: softmax를 relu/seqlen으로 대체하면 qk-layernorm[8]을 사용하여 비전 변환기[10]에 대한 기존 어텐션의 스케일링 성능에 접근하거나 일치합니다. 이 그림은 ImageNet-21k [9]에서 30개 에포크 동안 학습된 소형에서 대형 비전 변환기에 대한 결과를 보여줍니다. 미세 조정 없이 ImageNet-1k에 있는 모델 중 최상위 클래스를 취하여 ImageNet-21k 모델에 대한 ImageNet-1k 정확도를 보고합니다. ReLU를 사용한 어텐션은 소프트맥스 어텐션보다 수집 작업이 적은 시퀀스 길이 차원에서 병렬화할 수 있습니다.ImageNet-1k 정확도(%) ImageNet-1k 정확도(%) 0.0.0.7500.0.0.0.0.학습 데이터 세트 i21k. 모델 S/32. 0.1.1.0.학습 데이터 세트 i21k. 모델 S/16. ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.2.0. 역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/32. 0.0.7750.0.0.7000.0.0.0.1.1.0.1.0.0.1.0.0.학습 데이터 세트 i21k. 모델 S/8. ImageNet-1k 정확도(%) 0.0.0.0.0.2.0.역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/16. 0.1.1.2.역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/8. ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.675€ 0.ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.0.2.0.0.1.1.2.0.squared relu 지수 a는 역 seqlen gelu softplus identity를 확장하기 위한 것입니다. 0.1.1.2. 지수 a는 역 seqlen softmax relu를 확장하기 위한 것입니다. 지수 a는 역 seqlen relusigmoid를 확장하기 위한 것입니다. 그림 2: softmax를 L¯ah로 대체하기 여기서 h = {relu, relu², gelu, softplus, identity, relu6, sigmoid}이고 L은 시퀀스 길이입니다. 일반적으로 a가 1에 가까울 때 가장 좋은 결과를 관찰합니다. a ≈ 1에서 가장 좋은 비선형성은 명확하지 않으므로 주요 실험에서는 속도를 위해 ReLU를 사용합니다. 어텐션 가중치의 합은 1입니다. 이는 수집이 필요하다는 단점을 유지합니다. 이 노트의 초기 버전을 작성한 후, 우리가 연구하는 ReLU-어텐션의 변형도 이론적 동기로 탐구되었다는 사실이 우리의 주의를 끌었습니다[3, 12]. 게다가 활성화 함수를 완전히 제거하여 어텐션이 선형이 되도록 하는 광범위한 문헌이 있는데[16, 22, 18], 이는 긴 시퀀스 길이에 유용합니다. 우리의 실험에서 활성화를 완전히 제거하면 정확도가 감소했습니다.3
--- METHOD ---
주의. 주의는 2단계 절차를 통해 d차원 쿼리, 키 및 값 {qi, ki, vi}/11을 변환합니다. 먼저, 주의 가중치 a¿j는 αij =&gt;[q₁k1, KL ( Di (1) 1을 통해 생성됩니다. 구체적으로 선형 주의의 경우 행렬 곱셈의 순서를 (gk)v에서 q(kv)로 전환할 수 있으며, 이는 필요한 계산을 O(dL²)에서 O(d² L)로 변경합니다. 여기서 q, k, v € RL xd는 쿼리, 키 및 값이고 L은 시퀀스 길이입니다. 여기서 는 일반적으로 소프트맥스입니다. 다음으로, 주의 가중치는 출력 o; = -1 αijvj를 계산하는 데 사용됩니다. 이 보고서는 . ReLU-주의에 대한 점별 대안을 살펴봅니다. 우리는 = L-¹ relu가 방정식 1에서 소프트맥스에 대한 유망한 대안임을 관찰합니다. 우리는 o = Lrelu인 주의를 ReLUattention이라고 합니다. = 확장된 점별 주의. 보다 일반적으로, 우리의
--- EXPERIMENT ---
영어: ImageNet-21k에서 작은 것에서 큰 것까지의 비전 변환기를 훈련하는 s는 ReLU-어텐션이 컴퓨팅의 함수로서 스케일링 동작 측면에서 소프트맥스-어텐션의 성능에 접근하거나 맞먹을 수 있음을 나타냅니다. 이 보고서에서는 반드시 확률 분포를 출력하지 않는 소프트맥스 연산에 대한 점별 대안을 살펴봅니다. 하이라이트로, 시퀀스 길이로 나눈 ReLU를 사용한 어텐션이 비전 변환기에 대한 컴퓨팅의 함수로서 스케일링 동작 측면에서 기존 소프트맥스 어텐션에 접근하거나 맞먹을 수 있음을 관찰합니다. 이 결과는 ReLU-어텐션이 기존 어텐션보다 수집 연산이 적은 시퀀스 길이 차원에서 병렬화될 수 있으므로 병렬화에 대한 새로운 기회를 제공합니다. 서론 트랜스포머 아키텍처[26]는 최신 머신 러닝에서 널리 사용됩니다. 트랜스포머[2]의 핵심 구성 요소인 어텐션에는 토큰에 대한 확률 분포를 생성하는 소프트맥스가 포함됩니다. 소프트맥스는 지수 계산과 합 시퀀스 길이로 인해 비용이 많이 들기 때문에 병렬화가 어렵습니다[24, 7]. 영어: over관련 작업 이전 연구에서는 softmax를 ReLU[25, 14] 또는 제곱 ReLU[15]로 대체하는 것을 살펴보았습니다. 그러나 이러한 접근 방식은 시퀀스 길이로 나누지 않는데, 이는 softmax와 비슷한 정확도에 도달하는 데 중요하다는 것을 실험적으로 발견했습니다. 또한 이전 연구[21]에서는 시퀀스 길이 축에 대한 정규화가 필요하면서도 softmax를 대체하여 0.softmax relu/seqlen 0.S/S/0.. B/× B/L/0.ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.0.0.Avg. 8개 데이터 세트에 대한 10샷 선형 전송 0.0.0.TPU 코어 시간 10² TPU 코어 시간을 보장합니다. 그림 1: softmax를 relu/seqlen으로 대체하면 qk-layernorm[8]을 사용하여 비전 변환기[10]에 대한 기존 어텐션의 스케일링 성능에 접근하거나 일치합니다. 이 그림은 ImageNet-21k [9]에서 30개 에포크 동안 학습된 소형에서 대형 비전 변환기에 대한 결과를 보여줍니다. 미세 조정 없이 ImageNet-1k에 있는 모델 중 최상위 클래스를 취하여 ImageNet-21k 모델에 대한 ImageNet-1k 정확도를 보고합니다. ReLU를 사용한 어텐션은 소프트맥스 어텐션보다 수집 작업이 적은 시퀀스 길이 차원에서 병렬화할 수 있습니다.ImageNet-1k 정확도(%) ImageNet-1k 정확도(%) 0.0.0.7500.0.0.0.0.학습 데이터 세트 i21k. 모델 S/32. 0.1.1.0.학습 데이터 세트 i21k. 모델 S/16. ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.2.0. 역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/32. 0.0.7750.0.0.7000.0.0.0.1.1.0.1.0.0.1.0.0.학습 데이터 세트 i21k. 모델 S/8. ImageNet-1k 정확도(%) 0.0.0.0.0.2.0.역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/16. 0.1.1.2.역 seqlen을 확장하기 위한 지수 a 학습 데이터 세트 ilk. 모델 S/8. ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.675€ 0.ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.0.2.0.0.1.1.2.0.squared relu 지수 a는 역 seqlen gelu softplus identity를 확장하기 위한 것입니다. 0.1.1.2. 지수 a는 역 seqlen softmax relu를 확장하기 위한 것입니다. 지수 a는 역 seqlen relusigmoid를 확장하기 위한 것입니다. 그림 2: softmax를 L¯ah로 대체하기 여기서 h = {relu, relu², gelu, softplus, identity, relu6, sigmoid}이고 L은 시퀀스 길이입니다. 일반적으로 a가 1에 가까울 때 가장 좋은 결과를 관찰합니다. a ≈ 1에서 가장 좋은 비선형성은 명확하지 않으므로 주요 실험에서는 속도를 위해 ReLU를 사용합니다. 어텐션 가중치의 합은 1입니다. 이는 수집이 필요하다는 단점을 유지합니다. 이 노트의 초기 버전을 작성한 후, 우리가 연구하는 ReLU-어텐션의 변형도 이론적 동기로 탐구되었다는 사실이 우리의 주의를 끌었습니다[3, 12]. 게다가 활성화 함수를 완전히 제거하여 어텐션이 선형이 되도록 하는 광범위한 문헌이 있는데[16, 22, 18], 이는 긴 시퀀스 길이에 유용합니다. 우리의 실험에서 활성화를 완전히 제거하면 정확도가 떨어졌습니다.3 방법 어텐션. 어텐션은 2단계 절차를 통해 d차원 쿼리, 키 및 값 {qi, ki, vi}/11을 변환합니다. 먼저, 어텐션 가중치 a¿j는 αij =&gt;[q₁k1, KL ( Di (1) 1을 통해 생성됩니다. 구체적으로 선형 어텐션을 사용하면 행렬 곱셈의 순서를 (gk)v에서 q(kv)로 전환할 수 있으며, 이를 통해 필요한 계산이 O(dL²)에서 O(d² L)로 변경됩니다. 여기서 q, k, v € RL xd는 쿼리, 키, 값이고 L은 시퀀스 길이입니다. 여기서 는 일반적으로 소프트맥스입니다. 다음으로, 어텐션 가중치는 출력 o; = -1 αijvj를 계산하는 데 사용됩니다. 이 보고서는 . ReLU-어텐션에 대한 점별 대안을 살펴봅니다. 우리는 = L-¹ relu가 방정식 1에서 소프트맥스에 대한 유망한 대안임을 관찰했습니다. 우리는 o = Lrelu인 어텐션을 ReLUattention. = 스케일된 점별 어텐션이라고 합니다. 보다 일반적으로, 우리의 실험은 € [0,1] 및 h = {relu, relu², gelu, softplus, identity, relu6, sigmoid} [6, 13]. 시퀀스 길이 스케일링. 시퀀스 길이 L을 포함하는 항목으로 스케일링하는 것이 높은 정확도에 유익하다는 것을 관찰했습니다. 이 스케일링은 softmax를 제거하는 이전 작업에는 없습니다 [15, 18]. 시퀀스 길이 스케일링에 대한 중심적인 정당성은 경험적이지만 간략한 분석적 동기를 제공합니다. 변환기는 현재 Σ=1αij 1인 softmax 어텐션으로 설계되었습니다. 이는 Ej[αij] = L-¹임을 의미합니다. 이것이 필요 조건일 가능성은 낮지만 = L-1 relu는 Ej[αij]가 초기화 시 O(L-¹)임을 보장합니다. 이 조건을 유지하면 다른 hy=ImageNet-1k 정확도(%) 0.0.0.0.0.0.0.0.0.Training 데이터 세트 i21k를 변경할 필요성이 완화될 수 있습니다. 모델 S/32. 역 seqlen relu 0.Training 데이터 세트 i21k를 스케일링하기 위한 0.1.1. 지수 a. 모델 S/16. 0.0.ImageNet-1k 정확도(%) 0.0.0.0.0.74학습 데이터 세트 i21k. 모델 S/8. 0.72ImageNet-1k 정확도(%) 0.0.0.0.0.2.0.0.1.1.2.0.0.1.1.2.squared relu qk-layernorm 없이 역 seqlen relu를 확장하기 위한 지수 a qk-layernorm 없이 역 seqlen -squared relu를 확장하기 위한 지수 a 그림 3: ReLU와 L이 시퀀스 길이인 La로 확장된 제곱 ReLU를 사용한 주의에 대한 qk-layernorm[8] 제거 효과. ImageNet-21k에서 학습된 S/32, S/16 및 S/8 비전 변환기 모델[10, 4]에 대한 결과가 나와 있습니다. 학습 데이터 세트 i21k. 모델 S/32. 학습 데이터 세트 i21k. 모델 S/16. 0.0.0.ImageNet-1k 정확도(%) 0.0.0.0.0.0.1.1.Exponent a는 역 seqlen relu를 확장하기 위한 것입니다.0.0.72ImageNet-1k 정확도(%) 0.0.0.0.Training dataset i21k.Model S/8.0.0.740.0.700.68ImageNet-1k 정확도(%) 0.0.0.2.0.0.1.1.2.0.0.1.1.2.squared relu 게이팅을 사용한 역 seqlen relu를 확장하기 위한 Exponent a 게이팅을 사용한 역 seqlen squared relu를 확장하기 위한 Exponent a 그림 4: ReLU와 L로 확장된 제곱 ReLU를 사용한 어텐션에 대한 게이트 어텐션 유닛[15] 사용 효과 여기서 L은 시퀀스 길이입니다. ImageNet-21k에서 학습한 S/32, S/16 및 S/8 비전 변환기 모델[10, 4]에 대한 결과가 나와 있습니다. softmax를 대체할 때의 매개변수입니다. √d 초기화 시 q와 k의 요소는 O(1)이므로 (k)도 O(1)이 됩니다. ReLU와 같은 활성화 함수는 O(1)을 유지하므로 Ej[αij]가 O(L-¹)이 되려면 요소 L-1이 필요합니다. 4 실험 ImageNet-21k에서 30에포크 동안 학습하고, 우리의 실험에서 결과적으로 두 학습 실행 모두 ImageNet-1k에서 300에포크 동안 학습합니다. 약 9e5의 단계 수입니다. 모델 크기를 조정할 때 불안정성을 방지하기 위해 필요한 것으로 이전에 관찰되었기 때문에 qk-layernorm[8]이 있는 ViT를 사용합니다. 그러나 이것이 우리가 테스트하는 규모에서 중요한 구성 요소가 아니라는 것을 알 수 있습니다. 우리는 i21k와 ilk를 각각 ImageNet-21k와 ImageNet-1k로 의미하고, 미세 조정 없이 ImageNet-1k에 있는 것들 중 최상위 클래스를 취하여 ImageNet-21k 모델에 대한 ImageNet-1k 정확도를 보고합니다. 다운스트림 작업에서 전송 성능을 평가할 때 big_vision/configs/vit_ilk.py를 사용합니다. ImageNet21k의 경우 3을 사용합니다. ImageNet1k의 경우 기본 구성 https: 기본 구성 https://github.com/google-research/big_ //github.com/google-research/big_vision/blob/main/을 사용합니다. 실험 설정. 우리의 실험은 하이퍼파라미터를 수정하지 않고 BigVision 코드베이스 [4]의 ImageNet-21k 및 ImageNet-1k [9] 교육 구성을 사용합니다. 2 제곱 ReLU를 제외하고. vision/blob/main/big_vision/configs/vit_i21k.py.use는 3개 시드에 대해 평균화된 10샷 선형 프로브입니다. 우리는 탐구하지 않습니다. 다운스트림 작업은 Caltech Birds[27], Caltech101[11], Stanford Cars[19], CIFAR-100[20], DTD[5], 감사의 말 ColHsit[17], Pets[23], UC Merced[28]입니다. 주요 실험. 그림 1은 ReLUattention이 ImageNet-21k 학습을 위한 softmax attention의 스케일링 추세와 일치함을 보여줍니다. x축에는 실험에 필요한 총 코어 시간을 표시합니다. 이점으로 ReLU-attention은 softmax attention보다 수집 작업이 적은 시퀀스 길이 차원에서 병렬화를 가능하게 합니다. 시퀀스 길이 스케일링의 효과. 그림 2는 softmax에 대한 다양한 지점별 대안에 대한 시퀀스 길이 스케일링의 효과를 살펴봅니다. 구체적으로, 우리는 € [0,1] 및 h = {relu, relu², gelu, softplus, identity}에 대해 softmax를 L-ah로 대체합니다. x축에는 a를 표시합니다. y축은 S/32, S/16 및 S/8 비전 변환기 모델에 대한 정확도를 표시합니다[10, 4]. 일반적으로 a가 1에 가까울 때 최상의 결과를 얻습니다. 최상의 비선형성이 명확하지 않기 때문에 주요 실험에서는 더 빠른 ReLU를 사용합니다. qk-layernorm의 효과. 주요 실험에서는 qk-layernorm[8]을 사용하는데, 여기서 쿼리와 키는 어텐션 가중치를 계산하기 전에 Layer Norm[1]을 거칩니다. 모델 크기를 확장할 때 불안정성을 방지하는 데 필요한 것으로 밝혀졌으므로 qk-layernorm을 기본적으로 사용합니다[8]. 그림 3은 qk-layernorm을 제거한 효과를 보여줍니다. 결과에 따르면 qk-layernorm은 이러한 모델에 큰 영향을 미치지 않지만 규모에 따라 달라질 수 있습니다. 게이트를 추가한 효과. 이전 작업에서 소프트맥스를 제거하면 게이트된 유닛이 추가되고 시퀀스 길이에 따라 스케일링되지 않습니다[15]. 구체적으로, 게이트된 어텐션 유닛[15]에서 추가 투영은 출력이 생성되고 출력은 아웃 투영 전에 요소별 곱셈을 통해 결합됩니다. 그림 4에서 게이트가 있으면 시퀀스 길이 스케일링이 필요 없어지는지 조사합니다. 전반적으로 게이트가 있든 없든 시퀀스 길이 스케일링으로 최상의 정확도가 달성됩니다. 게이팅은 ReLU가 있는 S/8 모델의 실험에 필요한 핵심 시간을 약 9.3% 증가시킵니다.
--- CONCLUSION ---
이 보고서는 많은 미해결 문제를 남겼습니다. 특히, 요인 L-1이 성능을 개선하는 이유나 이 용어를 학습할 수 있는지 여부는 확실하지 않습니다. 게다가 더 나은 활성화 함수가 있을 가능성이 높습니다. 도움이 되는 의견과 제안을 해주신 Lucas Beyer, Mostafa Dehghani, David Fleet에게 감사드립니다. 우리는 이 노력을 지원해준 Google DeepMind PAGI팀 멤버인 Jascha Sohldickstein, Noah Fiedel, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Avi Singh, Azade Nova, Ben Adlam, Bernd Bohnet, Daniel Freeman, Gamaleldin Elsayed, Gaurav Mishra, Hanie Sedghi, Isabelle Simpson, Izzeddin Gur, JD Co-Reyes, James Harrison, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Max Bileschi, Merrie Morris, Roman Novak, Rosanne Liu, Sharad Vikram, Tris Warkentin, Yundi Qian에게 감사드립니다. 참고문헌 [1] Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton. 계층 정규화. arXiv 사전 인쇄본 arXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. 정렬 및 변환을 공동으로 학습하여 신경망 기계 번역. arXiv 사전 인쇄본 arXiv:1409.0473, 2014. [3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. 통계학자로서의 변환기: 컨텍스트 내 알고리즘 선택을 통한 증명 가능한 컨텍스트 내 학습. arXiv 사전 인쇄본 arXiv:2306.04637, 2023. [4] Lucas Beyer, Xiaohua Zhai, Alexander Kolesnikov. imagenet1k를 위한 더 나은 일반 vit 기준선. arXiv 사전 인쇄본 arXiv:2205.01580, 2022. URL https://arxiv.org/abs/2205.01580. [5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi. 야생에서의 텍스처 설명. 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 2014. https://arxiv.org/abs/1311.3618. [6] George E Dahl, Tara N Sainath, Geoffrey E Hinton. 정류 선형 유닛과 드롭아웃을 사용하여 lvcsr에 대한 딥 신경망 개선. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, 8609-8613페이지. IEEE, 2013. [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré. Flashattention: io-awareness를 갖춘 빠르고 메모리 효율적인 정확한 어텐션. Advancesin Neural Information Processing Systems, 35:16344– 16359, 2022. [8] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. 비전 변환기를 220억 매개변수로 확장. arXiv 사전 인쇄본 arXiv:2302.05442, 2023. [17] 기계 학습, 기계 학습 연구 논문집 119권, 5156-5165페이지. PMLR, 2020년 7월 13일 18일. URL https://proceedings.mlr.press/v119/katharopoulos 20a.html. Jakob Nikolas Kather, Frank Gerrit Zöllner, Francesco Bianconi, Susanne M Melchers, Lothar R Schad, Timo Gaiser, Alexander Marx, Cleo-Aron Weis. 대장암 조직학의 텍스처 수집. Zenodo https://doi. org/10, 5281, 2016. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [18] Soroush Abbasi Koohpayegani 및 Hamed Pirsiavash. 및 Li Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스. 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2009. https://ieeexplore. ieee.org/document/5206848. [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 이미지는 16x단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. 국제 학습 표현 컨퍼런스(ICLR), 2021. https://arxiv.org/abs/2010. 11929. [11] Li Fei-Fei, Rob Fergus, Pietro Perona. 몇 가지 학습 예제에서 생성적 시각 모델 학습: 객체 범주에서 테스트된 증분 베이지안 접근 방식. 2004년 컴퓨터 비전 및 패턴 인식 워크숍 컨퍼런스, 178-178페이지. IEEE, 2004. [12] Hengyu Fu, Tianyu Guo, Yu Bai, Song Mei. 단일 어텐션 레이어는 무엇을 배울 수 있을까? 랜덤 피처 렌즈를 통한 연구. arXiv 사전 인쇄본 arXiv:2307.11353, 2023. [13] Dan Hendrycks 및 Kevin Gimpel. 가우시안 오차 선형 단위(gelus). arXiv 사전 인쇄본 arXiv:1606.08415, 2016. [14] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein 및 Roman Novak. 무한 어텐션: 딥 어텐션 네트워크를 위한 Nngp 및 ntk. 국제 학술 대회 Sima: 비전 변환기를 위한 간단한 소프트맥스 없는 어텐션. arXiv 사전 인쇄본 arXiv:2206.08898, 2022. [19] Jonathan Krause, Michael Stark, Jia Deng 및 Li FeiFei. 세밀한 분류를 위한 3D 객체 표현. 2013년 컴퓨터 비전 국제 컨퍼런스(ICCV) 워크숍에서. https://ieeexplore.ieee.org/document/6755945. Learn[20] Alex Krizhevsky, Geoffrey Hinton, et al. 작은 이미지에서 여러 계층의 특징 추출, 2009. https://www.cs.toronto.edu/~kriz/ [21] [22] [23] learning-features-2009-TR.pdf. Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, Sanjiv Kumar. 규모 불변 아키텍처를 사용한 신경망의 강력한 학습. 기계 학습 국제 컨퍼런스에서, 12656-12684페이지. PMLR, 2022. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: 선형 복잡도를 가진 Softmax 없는 변환기. 신경 정보 처리 시스템의 발전, 34:21297-21309, 2021. Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. 고양이와 개. 2012년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 3498-3505페이지. IEEE, 2012. [24] Markus N Rabe and Charles Staats. 셀프 어텐션은 o(n²) 메모리가 필요하지 않습니다. arXiv 사전 인쇄본 arXiv:2112.05682, 2021. 머신 러닝에 관하여, 4376-4386페이지. PMLR, 2020. [25] Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui [15] Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc Le. 선형 시간의 변압기 품질. 기계 학습 국제 컨퍼런스, 9099-9117페이지. PMLR, 2022. [16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret. 변압기는 RNN입니다: 선형 주의가 있는 빠른 자기 회귀 변압기. Hal Daumé III 및 Aarti Singh 편집자, 제37회 국제 컨퍼런스 회의록 [26] Wang, Jiang Bian. 변압기의 relu 및 softmax에 대한 연구. arXiv 사전 인쇄본 arXiv:2302.06461, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. [27] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona. Caltech-ucsd birds 200. 2010.[28] Yi Yang 및 Shawn Newsam. 토지 이용 분류를 위한 시각적 단어 가방 및 공간 확장. 지리 정보 시스템의 발전에 관한 제18회 SIGSPATIAL 국제 컨퍼런스 회의록, 270-279쪽, 2010.
"
"--- ABSTRACT ---
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users’ data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking. Index Terms— Privacy-preserving machine learning, language modeling, large language models, automatic speech recognition 1.
--- INTRODUCTION ---
A common issue arising after deploying a machine learning model on central servers or user devices is the discrepancy between training data and actual user data received. Specifically, in the applications of natural language processing (NLP), semantic characteristics and topics of real users’ textual data could be very different from those of server-side proxy corpora, in which scenarios model adaptation is indispensable [1] [2]. To effectively perform model adaptation, textual data of users is typically stored on servers or their devices, where any downstream NLP models will be trained using such in-domain data. However, users’ personal data might contain sensitive user information, such as people’s names, addresses, and credit card numbers. Therefore, this conventional practice of users’ data storage might raise privacy and security concerns due to the risks of exposing user information to adversaries. In addition, recent research has shown that sensitive information in training datasets can be detected and then extracted in unexpected ways {6)[7). Particularly, language models (LMs) are prone to unintentionally memorize rare or unique sequences of data, and when being prompted appropriately, they will be able to emit the memorized text verbatim [8]. Thus, having NLP models directly trained on private user data might have extra risks of exposing sensitive information. * Work done during an internship at Meta. To overcome these challenges, replacing identifying information in textual data with a generic marker has been explored [9] To be more specific, tokens considered as itive or private are masked out using some special symbol, such as “[MASK]”. In the example where the raw textual sequence is “Tom lives in Chicago”, one might mark the words of “Tom” and “Chicago” as personal and thus replace them with the mask symbol. The resulting sequence is “[MASK] lives in [MASK]”, which will be stored into servers or local devices for model adaptation purposes later on. While this strategy is capable to provide privacy protections on user data, it also introduces significant complexities to the training of any NLP models for downstream adaptation tasks. The existence of markers might break the semantic structures, disrupt the coherence of languages, or fail to preserve the meaning of the original textual sequences. As a result, models directly trained on the masked corpus could yield much worse performance compared with the ones trained on the raw corpus without privacy-preserving token masking. Therefore, it calls for advanced approaches on effectively substituting the masked tokens in the corpus and bridge the accuracy gaps in NLP models for adaptation tasks. In this work, we propose to use large language models (LLMs) to provide appropriate candidate tokens to fill in the generic markers in any masked corpus. Note that predicting the masked tokens based on the surrounding context can be considered as a task of masked LM (MLM), thus bi-directional Transform: based pre-trained LLMs, such as BERT | and RoBERTa would be suitable for this endeavor. Upon observing the remarkable capabilities demonstrated by decoder-only LLMs, models such as ChatGPT [15] and LLaMA2 can also be utilized here for providing substitutes of masked tokens. Our goal is not to restore any markers to the original tokens without masking, instead, we aim to replace any masked token with some substitute of the same type. More specifically, the efficiency of any recovering method from privacy-preserving masking shall be evaluated on the downstream adaptation tasks, through the NLP models trained on the obfuscation corpus. In this paper, we g and LM-fused automatic speech recognition 1} as the downstream tasks. We make the following contributions: ¢ To the best of our knowledge, our work is the first to leverage LLMs to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream LM and ASR tasks; ¢ We propose multiple pre-trained and fine-tuned LLM-based methods and conduct empirical experiments on various NLP datasets for the comparison of adapted models accordingly. The results of our experiments indicate that models trained on the obfuscation corpora have comparable performance with the ones trained on the original data without privacypreserving token masking; --- --¢ We also present three token masking techniques and measure the performance of our proposed methods on each of them in downstream tasks as well. The rest of the paper is organized as follows. We review
--- RELATED WORK ---
s in Section|2| Section|3|describes the details of our proposed framework on privacy-preserving token masking and the substitutes of masked tokens using LLMs. Next, Section iments and results for downstream tasks of LM and ASR. Finally, We conclude in Section] 2. RELATED WORKS Privacy protection has been becoming crucial in NLP research One important direction in this area is through anonymization, which involves the removal of identifying information from textual corpus pz . More recently, obfuscation, replacing any sensitive information with a different substitute of the same type has been investigated. In particular, a survey of profanity obfuscation in NLP is conducted in [24]. Authors in [25] employs a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one; it outperforms random substitution baselines across syntactic parsers. studies named entity obfuscation in speech, which focuses on identifying, replacing, and inserting replacement named synthesized using voice cloning into original audio. The paimproves the speech recognition of personal identifiers by per of including fake textual substitutes in the training data of ASR. None of these existing works explore the use and comparison of different LLMs for suggesting token substitutes in obfuscation. 3.
--- METHOD ---
s.
--- EXPERIMENT ---
al results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking. Index Terms— Privacy-preserving machine learning, language modeling, large language models, automatic speech recognition 1. INTRODUCTION A common issue arising after deploying a machine learning model on central servers or user devices is the discrepancy between training data and actual user data received. Specifically, in the applications of natural language processing (NLP), semantic characteristics and topics of real users’ textual data could be very different from those of server-side proxy corpora, in which scenarios model adaptation is indispensable [1] [2]. To effectively perform model adaptation, textual data of users is typically stored on servers or their devices, where any downstream NLP models will be trained using such in-domain data. However, users’ personal data might contain sensitive user information, such as people’s names, addresses, and credit card numbers. Therefore, this conventional practice of users’ data storage might raise privacy and security concerns due to the risks of exposing user information to adversaries. In addition, recent research has shown that sensitive information in training datasets can be detected and then extracted in unexpected ways {6)[7). Particularly, language models (LMs) are prone to unintentionally memorize rare or unique sequences of data, and when being prompted appropriately, they will be able to emit the memorized text verbatim [8]. Thus, having NLP models directly trained on private user data might have extra risks of exposing sensitive information. * Work done during an internship at Meta. To overcome these challenges, replacing identifying information in textual data with a generic marker has been explored [9] To be more specific, tokens considered as itive or private are masked out using some special symbol, such as “[MASK]”. In the example where the raw textual sequence is “Tom lives in Chicago”, one might mark the words of “Tom” and “Chicago” as personal and thus replace them with the mask symbol. The resulting sequence is “[MASK] lives in [MASK]”, which will be stored into servers or local devices for model adaptation purposes later on. While this strategy is capable to provide privacy protections on user data, it also introduces significant complexities to the training of any NLP models for downstream adaptation tasks. The existence of markers might break the semantic structures, disrupt the coherence of languages, or fail to preserve the meaning of the original textual sequences. As a result, models directly trained on the masked corpus could yield much worse performance compared with the ones trained on the raw corpus without privacy-preserving token masking. Therefore, it calls for advanced approaches on effectively substituting the masked tokens in the corpus and bridge the accuracy gaps in NLP models for adaptation tasks. In this work, we propose to use large language models (LLMs) to provide appropriate candidate tokens to fill in the generic markers in any masked corpus. Note that predicting the masked tokens based on the surrounding context can be considered as a task of masked LM (MLM), thus bi-directional Transform: based pre-trained LLMs, such as BERT | and RoBERTa would be suitable for this endeavor. Upon observing the remarkable capabilities demonstrated by decoder-only LLMs, models such as ChatGPT [15] and LLaMA2 can also be utilized here for providing substitutes of masked tokens. Our goal is not to restore any markers to the original tokens without masking, instead, we aim to replace any masked token with some substitute of the same type. More specifically, the efficiency of any recovering method from privacy-preserving masking shall be evaluated on the downstream adaptation tasks, through the NLP models trained on the obfuscation corpus. In this paper, we g and LM-fused automatic speech recognition 1} as the downstream tasks. We make the following contributions: ¢ To the best of our knowledge, our work is the first to leverage LLMs to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream LM and ASR tasks; ¢ We propose multiple pre-trained and fine-tuned LLM-based methods and conduct empirical experiments on various NLP datasets for the comparison of adapted models accordingly. The results of our experiments indicate that models trained on the obfuscation corpora have comparable performance with the ones trained on the original data without privacypreserving token masking; --- --¢ We also present three token masking techniques and measure the performance of our proposed methods on each of them in downstream tasks as well. The rest of the paper is organized as follows. We review related works in Section|2| Section|3|describes the details of our proposed framework on privacy-preserving token masking and the substitutes of masked tokens using LLMs. Next, Section iments and results for downstream tasks of LM and ASR. Finally, We conclude in Section] 2. RELATED WORKS Privacy protection has been becoming crucial in NLP research One important direction in this area is through anonymization, which involves the removal of identifying information from textual corpus pz . More recently, obfuscation, replacing any sensitive information with a different substitute of the same type has been investigated. In particular, a survey of profanity obfuscation in NLP is conducted in [24]. Authors in [25] employs a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one; it outperforms random substitution baselines across syntactic parsers. studies named entity obfuscation in speech, which focuses on identifying, replacing, and inserting replacement named synthesized using voice cloning into original audio. The paimproves the speech recognition of personal identifiers by per of including fake textual substitutes in the training data of ASR. None of these existing works explore the use and comparison of different LLMs for suggesting token substitutes in obfuscation. 3. METHODOLOGY We describe our proposed approaches on privacy-preserving token masking and the substitutes of masked tokens using LLMs. Specifically, we introduce several token masking techniques in Sectio LLM-based methods on replacing the masked tokens are presented in Section Section discusses the use of obfuscation corpus for performing language modeling task. The overall framework is depicted in Figure[T] Tom lives in Chicago [MASK lives in [MASK] Sam lives in Boston 7 » va i Raw Masked Corpus Obfuscationaw Co! \ TES ] allowList, vocabThres, rf fale XN A X entityTaoger / NS / Substitute Masked Tokens Downstream Tasks / } Fig. 1. | The framework of token masking and obfuscation using LLMs. 3.1. Token Masking Techniques Masking sensitive tokens from users’ data helps reduce the privacy risks and prevent any personal information being leaked or extracted from adversaries. Such token masking task shall be performed without human-in-the-loop since practitioners are not allowed to have the access to annotate or label private data of users. To automatically conceal sensitive information in some private corpus, we propose the following token masking techniques: ¢ allowList: This is a pre-defined list of tokens that are considered non-sensitive and safe to keep. Typically, such list is handcrafted by linguistic specialists. Then during the process of masking, any token not present in this allow list will be masked out; ¢ vocabThres: This involves the selection of N most frequent tokens from a vocabulary as the list of non-sensitive tokens. That is, any token with its frequency less than some threshold will be masked out. Here, the vocabulary set can be built from some generic large corpora; ¢ entityTagger: In this approach, named entity recognition (NER) models are utilized to identify potential entities in any private corpus, which will be treated as personal tokens and masked out. These entities include but are not limit to individuals’ names, locations, and organizations. Throughout these masking techniques, we will more likely mask the non-common tokens in any corpus, assuming privacy information is more related to rare or unique tokens. After applying the masking, we obtain a masked corpus where the masked tokens were replaced with the symbol of “[MASK]”. 3.2. Recovery Methods from Masking Token masking provides privacy protections, however, the resulting masked corpus might not be suitable to be directly used for training NLP models for downstream tasks. Given any masked corpus, we propose to use LLMs to fill in each mask symbol with appropriate token that matches the semantic contexts. It is important to note that we are not aiming to predict exactly the same token with the original one in the raw corpus. We expect to substitute it with some token that makes the whole sentence linguistically correct and complete. The following illustrates different strategies on leveraging LLMs for substituting masked tokens: * Top-1: In this method, we directly use the 1-best predicted token from an LLM to replace the masked token. Here, token filling is considered as a masked LM task. If there are multiple markers in the sentence, they are replaced in a sequential order from the left to the right, one at a time; * Top-kK: This approach extends the token filling candidates from the 1-best to the A’-best from the predictions of an LLM. Specifically, we randomly choose a token from the top-v predictions. Then this selected token is used to fill in the marker in the sentence. For substituting any masked tokens from allowList or vocabThres based masking techniques, we prefer the predicted tokens not being included in the corresponding token list, thus we repeat the random sampling process until this condition is met or there is no available candidates of predicted tokens among the top-K; ¢ Fine-Tuning (FT): In the previous two approaches, we utilize the token predictions from a pre-trained LLM. Finetuning a pre-trained LLM using in-domain corpus helps the --- --model gain domain-specific knowledge, and hence enhance the performance in the masked token prediction. To accomplish this, samples without any masked tokens can be used for fine-tuning. However, in many scenarios, it is possible that majority of samples contain at least one mask symbol so that fine-tuning is less effective especially when the size of corpus all. Alternatively, the top-1 or top-/K predictions from the same pre-trained LLM can be firstly used to substitute the masked tokens in any samples, and then the entire obfuscation corpus can be used for fine-tuning the LLM. Once we have a fine-tuned LLM, either Top-1 or Top-K can be applied for the substitution of masked tokens. Note that the process above can be utilized for multiple times. After applying any of these methods, we obtain an obfuscation corpus that does not contain any masks. 3.3. Performing Downstream Tasks Once we have substituted masked tokens, the resulting corpus can be used for training machine learning models for any downstream tasks. Notice that the effectiveness of any token filling approach should be measured by the performance of these machine learning models on these downstream tasks. In this work, we consider the language modeling adaptation task where a generic pre-trained LM is fine-tuned on the obfuscation corpus. This adapted LM will be evaluated on a (unmasked) test set which has the same domain with the raw corpus. The performance of LM is measured in term of perplexity. When integrating an adapted LM with an ASR model via shallow fusion, word error rate (WER) can also be evaluated on a test set of utterances. 4, EXPERIMENTS 4.1. Datasets To compare the performance of multiple baselines and our proposed approaches on the downstream language modeling task, we explore datasets in the experiments: Fish , Pushshift.io Redand Wall Street Journal (WSJ) . The statistics of these ets are summarized in Table[I] The test set of WSJ data also consists of voice utterances and is thus used for evaluating the ASR models with fused LMs. Table 1. Data information. | Train Set (#sent) | Test Set (#sent) Fisher 1,158,496 50,Reddit 763,683 49,WSJ 6,0004.2. Setups 4.2.1. Downstream Tasks The downstream LM is a Transformer with 6 layers, 12 attention heads, and 768 hidden units. The set of word vocabulary is around 'Pushshift.io Reddit dataset is a previously existing dataset extracted and obtained by a third party that contains preprocessed comments posted on the social network Reddit and hosted by pushshift.io. We will refer this dataset as “Reddit” in the rest of the paper. 85K. The LM is pre-trained on WikiText-103 corpus [3| For each of the masking techniques considered in this study, LMs are fine-tuned on the obfuscation train sets of Fisher, Reddit, and WSJ data. Their perplexities are evaluated on the corresponding test sets. On the WSJ test set, we also evaluate the ASR performance. The ASR model is an RNN-T model with the Emformer encoder [LSTM predictor, and a joiner. It has around 80 million parameters and is trained from scratch using the train split of LibriSpeech ASR corpus 4.2.2. Masking Techniques In our experiments, allowList contains a set of SK curated common words, and vocabThres consists of 10K most frequent words among the same 85K word vocabulary mentioned above. For the entityT agger masking technique, we utilize the BERT-NER model for tagging named entities in the train sets. For each of these masking techniques, Table [2] shows the percentage of masked tokens per dataset. We can see that allowList masks many more tokens than the other two techniques. Table 2. Percentages of masked tokens. | allowList | vocabThres | entityTagger Fisher 12.5% 1.3% 1.7% Reddit 22.7% 11.9% 4.2% WSJ 30.4% 11.2% 9.1% 4.2.3. Baselines We consider the following methods as the baselines: * Oracle: an LM is trained on the ground-truth sentences without any masking, which provides the upper bound for the model performance on each dataset; * BaselineO: an LM is directly trained on the masked corpus, where the mask symbol “[MASK]” is treated as a special token during model training; * Baselinel: zero weight is assigned to any mask symbol “[MASK]” in the LM loss function during model training. Note that for each of these methods, the LM is still pre-trained on the WikiText-103 corpus. 4.2.4. LLM-Based Methods In our experiments, we consider the following LLMs for substituting masked tokens in any training sequences: BERT (base, uncased), ROBERTa (base), and LLaMA2 (7B model parameters). For the fine-tuning of BERT and ROBERTa, we use MLM as the training task. During the inference time of using pre-trained or fine-tuned BERT and RoBERTa to substitute masked tokens, any consecutive markers of “[MASK]” are merged into one marker. We set K = 10 in the Top-K method. For LLaMA2, we adopt a different approach for the fine-tuning process since it is an auto-regressive model. Specifically, for each training sample, we generate prompts by combining some instruction, input, and output text: instruction contains the text of “Predict the [MASK] tokens in the given sentence”; input is the same training sample but having a few tokens randomly replaced with the symbol --- --of “[MASK]”; and output is the original training sample (without masking). We leverage the low-rank adaptation (LoRA) method for fine-tuning LLaMA2 on the set of prompts. During the inference time, the instruction and input are provided to the fine-tuned model, which allows the model for continued text generation. 4.3. Results Table B] shows the perplexity results of the baselines and proposed methods on Fisher dataset. We have the following observations: ¢ All proposed methods give lower perplexity results than the two baseline methods; ¢ Inall scenarios, Top-K outperforms Top—1 based methods; fine-tuned BERT and RoBERTa obtain better results than the ones without fine-tuning; ¢ Since more tokens are masked out with allowList, the gap between Oracle and any other method is much larger than that of vocabT hres or entityTagger masking technique; * ROBERTa yields the best perplexity performance across all the masking techniques. In particular, for vocabThres and entityT agger, perplexity results from fine-tuned ROBERTa are very close to those of Oracle, which indicates that most of the missing information can be recovered in the obfuscation dataset; ¢ LLaMA2 (Top-1,FT) is a competitive method but is not as good as fine-tuned BERT or RoBERTa for this task. Table 3. Perplexity results on Fisher dataset. allowList | vocabThres | entityTagger Oracle 37.3 37.3 37.BaselineOd 120.1 42.3Baselinel 109.4 41.6 41.BERT (Top-1) 93.0 413 41.RoBERTa (Top-1) 71.6 40.5 39.BERT (Top-K) 75.2 40.8 40.RoBERTa (Top-K) 70.2 38.9 38.BERT (Top-K, FT) 73.6 39.8 39.ROBERTa (Top-K, FT) 65.3 38.9 38.LLaMA2 (Top-1,FT) 89.3 40.8 40.Table |4|shows the experimental results on Reddit dataset. The observations are similar to the ones in Fisher dataset. In particular, ROBERTa (Top-K, FT) again achieves the best perplexity results across all the masking techniques. Table 4. Perplexity results on Reddit dataset. allowList | vocabThres | entityTagger Oracle 76.0 76.0 76.BaselineOd 339.6 168.2 82.Baselinel 221.9 134.9 79.BERT (Top-1) 196.2 121.2 78.ROBERTa (Top-1) 1173 94.2 78.BERT (Top-K) 127.4 106.3 78.ROBERTa (Top-K) 123.4 92.6BERT (Top-K, FT) 117.4 102.5RoBERTa (Top-K, FT) 98.5 82.1 16.LLaMA2 (Top-1, FT) 123.3 107.7 78.Table [5] and Table [6] show the perplexity and WER results on WSJ dataset, respectively. We have the following findings: ¢ The use of fused LM for conducting domain adaptation in ASR models is effective: comparing the WERs between ASR models with the pre-trained LM and the Oracle LM, there is amore than 15% WER improvement achieved by the latter; ¢ The best WERs obtained by proposed methods have relatively small gaps compared with those of the Oracle LM. For vocabT hres and entityT agger masking techniques, the WERs from Oracle are lifted by only 1% (10.7 versus 10.6) and 5% (11.1 versus 10.6), respectively. That is, the proposed methods are able to achieve significant improvements over the pre-trained LM (without adaptation), while they also provide better privacy protection than the Oracle LM. Table 5. Perplexity results on WSJ dataset. allowList | vocabThres | entityTagger oracle 86.5 86.5 86.Baseline0d 309.0 144.3 204.Baselinel 210.0 122.9 198.BERT (Top-1) 205.9 119.4 149.ROBERTa (Top-1) 181.1 102.5 118.BERT (Top-K) 174.1 103.3 108.ROBERTa (Top-K) 1145 93.4 98.BERT (Top-K, FT) 186.7 113.4 162.RoBERTa (Top-K, FT) 120.7 110.4 157.LLaMA2 (Top-1, FT) 135.6 106.8 145.Table 6. WER results on WSJ dataset. allowList | vocabThres | entityTagger ASR-without-LM 14.4 14.4 14.Pre-Trained-LM 12.6 12.6 12.oracle 10.6 10.6 10.Baseline0d 13.0 12.6Baselinel 12.5 11.2BERT (Top-1) 12.4 11ROBERTa (Top-1) 12.4 10.9BERT (Top-K) 12.1 11ROBERTa (Top-K) 19 10.9BERT (Top-K, FT) 12.7 15RoBERTa (Top-K, FT) 11.8 114LLaMA2 (Top-1, FT) 12.0 10.75.
--- CONCLUSION ---
In this paper, we propose multiple pre-trained and fine-tuned LLMbased methods to recover from privacy-preserving token masking on textual corpus and perform empirical studies on various datasets for the comparison of these approaches. Our experimental results demonstrate that LMs trained on the obfuscation corpora can obtain comparable accuracy with the ones trained on the raw data without privacy-preserving token masking. Future research might include fine-tuning LLMs with the object function designed to be more directly related to the downstream NLP tasks. Also, we would consider a combination of these three masking techniques and adopt class-specific markers such as “[PERSON]”, “[NUMBER]”, etc. --- --6. REFERENCES Ke Li, Zhe Liu, Tianxing He, Hongzhao Huang, Fuchun Peng, Daniel Povey, and Sanjeev Khudanpur, “An empirical study of transformerbased neural language model adaptation,” in Proc. ICASSP, 2020. Zhe Liu, Ke Li, Shreyan Bakshi, and Fuchun Peng, guage model adaptation for speech recognition,” arXiv:2110.10026, 2021. “Private lanarXiv preprint Matt Fredrikson, Somesh Jha, and Thomas Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in Proc. ACM SIGSAC, 2015. Congzheng Song and Vitaly Shmatikov, “Auditing data provenance in text-generation models,” in Proc. ACM SIGKDD, 2019. Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song, “The secret sharer: Evaluating and testing unintended memorization in neural networks,” in 28th USENIX Security Symposium, 2019. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al., “Extracting training data from large language models,” in 30th USENIX Security Symposium, 2021. W Ronny Huang, Steve Chien, Om Thakkar, and Rajiv Mathews, “Detecting unintended memorization in language-model-fused ASR,” in Proc. Interspeech, 2022. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang, “Quantifying memorization across neural language models,” arXiv preprint arXiv:2202.07646, 2022. Sergio Martinez, David Sanchez, Aida Valls, and Montserrat Batet, “Privacy protection of textual attributes through a semantic-based masking method,” Information Fusion, vol. 13, no. 4, pp. 304-314, 2012. Samuel Sousa and Roman Kern, “How to keep text private? a systematic review of deep learning methods for privacy-preserving natural language processing,” Artificial Intelligence Review, vol. 56, no. 2, pp. 1427-1492, 2023. Judita Preiss, “Automatic named entity obfuscation in speech,” in Findings of ACL, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” Advances in NeurIPS, 2017. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, “RoBERTa: A robustly optimized BERT pretraining approach,” arXiv preprint arXiv: 1907.11692, 2019. OpenAI, “ChatGPT: Optimizing language models for dialogue,” Feb 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al., “Llama 2: Open foundation and finetuned chat models,” arXiv preprint arXiv:2307.09288, 2023. Toma’ Mikolov, Martin Karafiat, LukaS Burget, Jan Cernocky, and Sanjeev Khudanpur, “Recurrent neural network based language model,” in Proc. Interspeech, 2010. Xie Chen, Xunying Liu, Mark JF Gales, and Philip C Woodland, “Improving the training and evaluation efficiency of recurrent neural network language models,” in Proc. ICASSP, 2015. Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF Gales, and Philip C Woodland, “Efficient lattice rescoring using recurrent neural network language models,” in Proc. ICASSP, 2014. 20)30,34) Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, and Rohit Prabhavalkar, “An analysis of incorporating an external language model into a sequence-to-sequence model,” in Proc. ICASSP, 2018. Kazuki Irie, Albert Zeyer, Ralf Schliiter, and Hermann Ney, “Language modeling with deep transformers,” in Proc. Interspeech, 2019. Pierre Lison, Idik6 Pilan, David Sanchez, Montserrat Batet, and Lilja @vrelid, “Anonymisation models for text data: State of the art, challenges and future directions,” in Proc. ACL, 2021. Tzvika Hartman, Michael D Howell, Jeff Dean, Shlomo Hoory, Ronit Slyper, Itay Laish, Oren Gilon, Danny Vainstein, Greg Corrado, Katherine Chou, et al., “Customization scenarios for de-identification of clinical notes,’ BMC medical informatics and decision making, vol. 20, no. 1, pp. 1-9, 2020. Debora Nozza and Dirk Hovy, “The state of profanity obfuscation in natural language processing,” arXiv preprint arXiv:2210.07595, 2022. Zhifeng Hu, Serhii Havrylov, Ivan Titov, and Shay B. Cohen, “Obfuscation for privacy-preserving syntactic parsing,” 2020. Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, and Bhuvana Ramabhadran, “Using text injection to improve recognition of personal identifiers in speech,” arXiv preprint arXiv:2308.07393, 2023. Christopher Cieri, David Miller, and Kevin Walker, “The fisher corpus: a resource for the next generations of speech-to-text,” in International Conference on Language Resources and Evaluation, 2004. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn, “The Pushshift reddit dataset,” in International Conference on Web and Social Media, 2020. Lukas Drude, Jens Heitkaemper, Christoph Boeddeker, and Reinhold Haeb-Umbach, “SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition,” arXiv preprint arXiv:1910.13934, 2019. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher, “Pointer sentinel mixture models,” arXiv preprint arXiv: 1609.07843, 2016. Yangyang Shi, Yonggiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer, “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in Proc. ICASSP, 2021. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015. Erik F. Tjong Kim Sang and Fien De Meulder, “Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition,” in Proc. of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, 2003. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, “LoRA: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.
"	"--- ABSTRACT ---
모델 적응은 프록시 학습 데이터와 실제 사용자 수신 데이터 간의 불일치를 처리하는 데 중요합니다. 적응을 효과적으로 수행하기 위해 사용자의 텍스트 데이터는 일반적으로 서버나 로컬 장치에 저장되며, 여기에서 다운스트림 자연어 처리(NLP) 모델을 이러한 도메인 내 데이터를 사용하여 직접 학습할 수 있습니다. 그러나 이는 사용자 정보를 적대자에게 노출시킬 수 있는 추가 위험으로 인해 개인 정보 보호 및 보안 문제를 일으킬 수 있습니다. 텍스트 데이터의 식별 정보를 일반 마커로 대체하는 것이 최근에 탐구되었습니다. 이 연구에서는 대규모 언어 모델(LLM)을 활용하여 마스크된 토큰의 대체물을 제안하고 다운스트림 언어 모델링 작업에서 효과를 평가합니다. 구체적으로, 사전 학습되고 미세 조정된 여러 LLM 기반 접근 방식을 제안하고 이러한 방법을 비교하기 위해 다양한 데이터 세트에 대한 실증 연구를 수행합니다. 실험 결과에 따르면 난독화 코퍼스에서 학습된 모델은 개인 정보 보호 토큰 마스킹 없이 원본 데이터에서 학습된 모델과 비슷한 성능을 달성할 수 있습니다. 색인 용어 개인 정보를 보호하는 머신 러닝, 언어 모델링, 대규모 언어 모델, 자동 음성 인식 1.
--- INTRODUCTION ---
중앙 서버나 사용자 기기에 머신 러닝 모델을 배포한 후 발생하는 일반적인 문제는 훈련 데이터와 수신된 실제 사용자 데이터 간의 불일치입니다. 특히 자연어 처리(NLP) 응용 프로그램에서 실제 사용자의 텍스트 데이터의 의미적 특성과 주제는 모델 적응이 필수적인 시나리오인 서버 측 프록시 코퍼스와 매우 다를 수 있습니다[1, 2]. 모델 적응을 효과적으로 수행하기 위해 사용자의 텍스트 데이터는 일반적으로 서버나 해당 기기에 저장되며, 이러한 다운스트림 NLP 모델은 이러한 도메인 내 데이터를 사용하여 훈련됩니다. 그러나 사용자의 개인 데이터에는 이름, 주소, 신용 카드 번호와 같은 민감한 사용자 정보가 포함될 수 있습니다. 따라서 이러한 기존 사용자 데이터 저장 관행은 사용자 정보를 적대자에게 노출시킬 위험으로 인해 개인 정보 보호 및 보안 문제를 일으킬 수 있습니다. 또한 최근 연구에 따르면 훈련 데이터 세트의 민감한 정보를 감지한 다음 예상치 못한 방식으로 추출할 수 있음이 밝혀졌습니다[3, 4, 5, 6, 7]. 특히 언어 모델(LMS)은 의도치 않게 드물거나 고유한 데이터 시퀀스를 기억하는 경향이 있으며, 적절한 프롬프트가 표시되면 기억된 텍스트를 그대로 내보낼 수 있습니다[8]. 따라서 NLP 모델을 개인 사용자 데이터로 직접 훈련하면 민감한 정보가 노출될 위험이 더 커질 수 있습니다.* Meta에서 인턴십 중에 수행한 작업. 이러한 과제를 극복하기 위해 텍스트 데이터의 식별 정보를 일반 마커로 대체하는 방법이 모색되었습니다[9, 10, 11]. 보다 구체적으로, 민감하거나 개인적인 것으로 간주되는 토큰은 &quot;[MASK]&quot;와 같은 특수 기호를 사용하여 마스크 처리됩니다. 원시 텍스트 시퀀스가 &quot;Tom lives in Chicago&quot;인 예에서 &quot;Tom&quot;과 &quot;Chicago&quot;라는 단어를 개인으로 표시하여 마스크 기호로 바꿀 수 있습니다. 결과 시퀀스는 &quot;[MASK] lives in [MASK]&quot;이며, 이는 나중에 모델 적응 목적으로 서버나 로컬 장치에 저장됩니다. 이 전략은 사용자 데이터에 대한 개인 정보 보호를 제공할 수 있지만, 다운스트림 적응 작업을 위한 모든 NLP 모델의 학습에 상당한 복잡성을 도입합니다. 마커가 있으면 의미 구조가 깨지거나 언어의 일관성이 깨지거나 원래 텍스트 시퀀스의 의미를 보존하지 못할 수 있습니다. 결과적으로 마스크된 코퍼스에서 직접 학습한 모델은 개인 정보 보호 토큰 마스킹 없이 원시 코퍼스에서 학습한 모델에 비해 성능이 훨씬 떨어질 수 있습니다. 따라서 코퍼스에서 마스크된 토큰을 효과적으로 대체하고 적응 작업을 위한 NLP 모델의 정확도 격차를 메우는 고급 접근 방식이 필요합니다. 이 작업에서 우리는 대규모 언어 모델(LLM)을 사용하여 마스크된 코퍼스의 일반 마커를 채우기 위한 적절한 후보 토큰을 제공하는 것을 제안합니다. 주변 컨텍스트에 따라 마스크된 토큰을 예측하는 것은 마스크된 LM(MLM)의 작업으로 간주될 수 있으므로 BERT[13] 및 ROBERTa[14]와 같은 양방향 Transformer[12] 기반 사전 학습된 LLM이 이 노력에 적합할 것입니다. 디코더 전용 LLM에서 입증된 놀라운 기능을 관찰한 후 ChatGPT[15] 및 LLAMA2[16]와 같은 모델도 마스크된 토큰의 대체물을 제공하는 데 활용할 수 있습니다. 우리의 목표는 마스크 없이 원래 토큰에 대한 마커를 복원하는 것이 아니라 마스크된 토큰을 동일한 유형의 대체물로 대체하는 것입니다. 보다 구체적으로, 개인 정보 보호 마스크에서 복구하는 방법의 효율성은 난독화 코퍼스에서 학습된 NLP 모델을 통해 다운스트림 적응 작업에서 평가됩니다. 이 논문에서 우리는 언어 모델링과 LM 융합 자동 음성 인식(ASR) [17, 18, 19, 20, 21]을 다운스트림 작업으로 사용합니다. 우리는 다음과 같은 기여를 합니다. • 우리의 작업은 마스크된 토큰의 대체물을 제안하고 다운스트림 LM 및 ASR 작업에서 효과를 평가하기 위해 LLM을 활용한 최초의 작업입니다. • 우리는 여러 개의 사전 훈련되고 미세 조정된 LLM 기반 방법을 제안하고 다양한 NLP 데이터 세트에 대한 실증적 실험을 수행하여 그에 따라 적응된 모델을 비교합니다. 우리 실험 결과에 따르면 난독화 코퍼스에서 훈련된 모델은 개인 정보 보호 토큰 마스크 없이 원본 데이터에서 훈련된 모델과 비슷한 성능을 보입니다. • 우리는 또한 세 가지 토큰 마스크 기술을 제시하고 다운스트림 작업에서도 각각에 대한 제안된 방법의 성능을 측정합니다. 논문의 나머지 부분은 다음과 같이 구성됩니다. 우리는 다음을 검토합니다.
--- RELATED WORK ---
2절에서 설명합니다. 3절에서는 개인 정보 보호 토큰 마스킹과 LLM을 사용한 마스킹된 토큰의 대체에 대한 제안된 프레임워크의 세부 사항을 설명합니다. 다음으로, 4절에서는 LM 및 ASR의 다운스트림 작업에 대한 실험과 결과를 보여줍니다. 마지막으로, 5절에서 결론을 내립니다. 2. 관련 연구 개인 정보 보호는 NLP 연구에서 중요해지고 있습니다[10]. 이 분야의 한 가지 중요한 방향은 익명화를 통한 것으로, 텍스트 코퍼스에서 식별 정보를 제거하는 것을 포함합니다[9, 22, 23]. 최근에는 민감한 정보를 동일한 유형의 다른 대체물로 대체하는 난독화가 조사되었습니다. 특히, NLP에서의 욕설 난독화에 대한 조사가 [24]에서 수행됩니다. [25]의 저자는 원래 문장의 구문 관계를 보존하여 난독화된 문장을 원래 문장 대신 구문 분석할 수 있도록 하는 신경 모델을 사용합니다. 구문 파서에서 무작위 대체 기준선보다 성능이 뛰어납니다. [11]의 연구는 음성 복제를 사용하여 합성된 대체 명명된 엔터티를 식별, 교체 및 원본 오디오에 삽입하는 데 중점을 둔 음성의 명명된 엔터티 난독화를 연구합니다. [26]의 논문은 ASR의 교육 데이터에 가짜 텍스트 대체물을 포함하여 개인 식별자의 음성 인식을 개선합니다. 이러한 기존 연구 중 어느 것도 난독화에서 토큰 대체물을 제안하기 위해 다양한 LLM의 사용 및 비교를 탐구하지 않습니다. 3.
--- METHOD ---
에스.
--- EXPERIMENT ---
모든 결과에 따르면 난독화 코퍼스에서 학습된 모델은 개인 정보 보호 토큰 마스킹 없이 원본 데이터에서 학습된 모델과 비슷한 성능을 달성할 수 있습니다. 색인 용어 개인 정보 보호 머신 러닝, 언어 모델링, 대규모 언어 모델, 자동 음성 인식 1. 서론 중앙 서버나 사용자 장치에 머신 러닝 모델을 배포한 후 발생하는 일반적인 문제는 학습 데이터와 수신된 실제 사용자 데이터 간의 불일치입니다. 특히 자연어 처리(NLP) 응용 프로그램에서 실제 사용자의 텍스트 데이터의 의미적 특성과 주제는 모델 적응이 필수적인 시나리오인 서버 측 프록시 코퍼스와 매우 다를 수 있습니다[1, 2]. 모델 적응을 효과적으로 수행하기 위해 사용자의 텍스트 데이터는 일반적으로 서버나 장치에 저장되며, 다운스트림 NLP 모델은 이러한 도메인 내 데이터를 사용하여 학습됩니다. 그러나 사용자의 개인 데이터에는 이름, 주소, 신용 카드 번호와 같은 민감한 사용자 정보가 포함될 수 있습니다. 따라서 사용자 데이터를 저장하는 이러한 기존 관행은 적대자에게 사용자 정보를 노출할 위험으로 인해 개인 정보 보호 및 보안 문제를 일으킬 수 있습니다. 또한 최근 연구에 따르면 훈련 데이터 세트의 민감한 정보를 감지한 다음 예상치 못한 방식으로 추출할 수 있음이 밝혀졌습니다[3, 4, 5, 6, 7]. 특히 언어 모델(LMS)은 의도치 않게 드물거나 고유한 데이터 시퀀스를 기억하는 경향이 있으며 적절한 프롬프트가 표시되면 기억된 텍스트를 그대로 내보낼 수 있습니다[8]. 따라서 NLP 모델을 개인 사용자 데이터로 직접 훈련하면 민감한 정보가 노출될 위험이 더 커질 수 있습니다. * Meta에서 인턴십 중에 수행한 작업. 이러한 과제를 극복하기 위해 텍스트 데이터의 식별 정보를 일반 마커로 대체하는 방법이 모색되었습니다[9, 10, 11]. 보다 구체적으로, 민감하거나 개인적인 것으로 간주되는 토큰은 &quot;[MASK]&quot;와 같은 특수 기호를 사용하여 마스크 처리됩니다. 원시 텍스트 시퀀스가 &quot;Tom lives in Chicago&quot;인 예에서 &quot;Tom&quot;과 &quot;Chicago&quot;라는 단어를 개인으로 표시하여 마스크 기호로 바꿀 수 있습니다. 결과 시퀀스는 &quot;[MASK] lives in [MASK]&quot;이며, 이는 나중에 모델 적응 목적으로 서버나 로컬 장치에 저장됩니다. 이 전략은 사용자 데이터에 대한 개인 정보 보호를 제공할 수 있지만, 다운스트림 적응 작업을 위한 모든 NLP 모델의 학습에 상당한 복잡성을 도입합니다. 마커가 있으면 의미 구조가 깨지거나 언어의 일관성이 깨지거나 원래 텍스트 시퀀스의 의미를 보존하지 못할 수 있습니다. 결과적으로 마스크된 코퍼스에서 직접 학습한 모델은 개인 정보 보호 토큰 마스킹 없이 원시 코퍼스에서 학습한 모델에 비해 성능이 훨씬 떨어질 수 있습니다. 따라서 코퍼스에서 마스크된 토큰을 효과적으로 대체하고 적응 작업을 위한 NLP 모델의 정확도 격차를 메우는 고급 접근 방식이 필요합니다. 이 작업에서 우리는 대규모 언어 모델(LLM)을 사용하여 마스크된 코퍼스의 일반 마커를 채우기 위한 적절한 후보 토큰을 제공하는 것을 제안합니다. 주변 컨텍스트에 따라 마스크된 토큰을 예측하는 것은 마스크된 LM(MLM)의 작업으로 간주될 수 있으므로 BERT[13] 및 ROBERTa[14]와 같은 양방향 Transformer[12] 기반 사전 학습된 LLM이 이 노력에 적합할 것입니다. 디코더 전용 LLM에서 입증된 놀라운 기능을 관찰한 후 ChatGPT[15] 및 LLAMA2[16]와 같은 모델도 마스크된 토큰의 대체물을 제공하는 데 활용할 수 있습니다. 우리의 목표는 마스크 없이 원래 토큰에 대한 마커를 복원하는 것이 아니라 마스크된 토큰을 동일한 유형의 대체물로 대체하는 것입니다. 보다 구체적으로, 개인 정보 보호 마스크에서 복구하는 방법의 효율성은 난독화 코퍼스에서 학습된 NLP 모델을 통해 다운스트림 적응 작업에서 평가됩니다. 이 논문에서 우리는 언어 모델링과 LM 융합 자동 음성 인식(ASR) [17, 18, 19, 20, 21]을 다운스트림 작업으로 사용합니다. 우리는 다음과 같은 기여를 합니다. • 우리의 작업은 마스크된 토큰의 대체물을 제안하고 다운스트림 LM 및 ASR 작업에서 효과를 평가하기 위해 LLM을 활용한 최초의 작업입니다. • 우리는 여러 개의 사전 훈련되고 미세 조정된 LLM 기반 방법을 제안하고 다양한 NLP 데이터 세트에 대한 실증적 실험을 수행하여 그에 따라 적응된 모델을 비교합니다. 우리 실험 결과에 따르면 난독화 코퍼스에서 훈련된 모델은 프라이버시를 보호하는 토큰 마스크 없이 원본 데이터에서 훈련된 모델과 비슷한 성능을 보입니다. • 우리는 또한 세 가지 토큰 마스킹 기술을 제시하고 다운스트림 작업에서도 각각에 대한 제안된 방법의 성능을 측정합니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 2절에서 관련 연구를 검토한다. 3절에서는 개인 정보 보호 토큰 마스킹과 LLM을 사용한 마스킹된 토큰의 대체에 대한 제안된 프레임워크의 세부 사항을 설명한다. 다음으로, 4절에서는 LM과 ASR의 다운스트림 작업에 대한 실험과 결과를 보여준다. 마지막으로, 5절에서 결론을 내린다. 2. 관련 연구 개인 정보 보호는 NLP 연구에서 중요해지고 있다[10]. 이 분야의 한 가지 중요한 방향은 익명화를 통한 것으로, 텍스트 코퍼스에서 식별 정보를 제거하는 것을 포함한다[9, 22, 23]. 최근에는 민감한 정보를 동일한 유형의 다른 대체물로 대체하는 난독화가 조사되었다. 특히, NLP에서의 욕설 난독화에 대한 조사가 [24]에서 수행되었다. [25]의 저자는 원래 문장의 구문 관계를 보존하여 난독화된 문장을 원래 문장 대신 구문 분석할 수 있도록 하는 신경 모델을 사용한다. 구문 파서에서 무작위 대체 기준선보다 성능이 우수하다. [11]의 연구는 음성 복제를 사용하여 합성된 명명된 엔터티를 식별, 대체 및 원본 오디오에 삽입하는 데 중점을 둔 음성의 명명된 엔터티 난독화를 연구합니다. [26]의 논문은 ASR의 교육 데이터에 가짜 텍스트 대체물을 포함하여 개인 식별자의 음성 인식을 개선합니다. 이러한 기존 연구 중 어느 것도 난독화에서 토큰 대체물을 제안하기 위해 다양한 LLM의 사용과 비교를 탐구하지 않습니다. 3. 방법론 우리는 개인 정보 보호 토큰 마스킹과 LLM을 사용하여 마스크된 토큰의 대체물에 대한 제안된 접근 방식을 설명합니다. 구체적으로, 우리는 섹션 3.1에서 여러 토큰 마스킹 기술을 소개합니다. 마스크된 토큰을 대체하는 LLM 기반 방법은 섹션 3.2에 제시됩니다. 섹션 3.3에서는 언어 모델링 작업을 수행하기 위해 난독화 코퍼스를 사용하는 방법을 설명합니다. 전체 프레임워크는 그림 1에 나와 있습니다. 톰은 시카고에 살고 있습니다 [MASK]는 [MASK]에 살고 있습니다 샘은 보스턴에 살고 있습니다 그림 1. LLM. 원시 코퍼스 마스크된 코퍼스 허용 목록, 어휘 임계값, 난독화 코퍼스 엔터티 태거 LLM 대체 마스크된 토큰 다운스트림 작업 3.1을 사용한 토큰 마스킹 및 난독화 프레임워크.토큰 마스킹 기술 사용자 데이터에서 민감한 토큰을 마스킹하면 개인 정보 위험을 줄이고 개인 정보가 적대자에게서 유출되거나 추출되는 것을 방지할 수 있습니다.이러한 토큰 마스킹 작업은 실무자가 사용자의 개인 데이터에 주석을 달거나 레이블을 지정할 수 없기 때문에 인간 참여 없이 수행되어야 합니다.일부 개인 코퍼스에서 민감한 정보를 자동으로 숨기기 위해 다음과 같은 토큰 마스킹 기술을 제안합니다.• 허용 목록: 이것은 민감하지 않고 보관해도 안전한 것으로 간주되는 토큰의 미리 정의된 목록입니다.일반적으로 이러한 목록은 언어 전문가가 직접 작성합니다.그런 다음 마스킹 프로세스 중에 이 허용 목록에 없는 토큰은 마스크됩니다.• 어휘 임계값: 이것은 어휘에서 가장 빈번한 N개의 토큰을 민감하지 않은 토큰 목록으로 선택하는 것을 포함합니다. 즉, 특정 임계값보다 빈도가 낮은 토큰은 모두 마스크 처리됩니다. 여기서 어휘 집합은 일부 일반적인 대규모 코퍼스에서 구축할 수 있습니다. • entityTagger: 이 접근 방식에서 명명된 엔터티 인식(NER) 모델을 사용하여 개인 토큰으로 처리되고 마스크 처리되는 모든 개인 코퍼스의 잠재적 엔터티를 식별합니다. 이러한 엔터티에는 개인의 이름, 위치 및 조직이 포함되지만 이에 국한되지는 않습니다. 이러한 마스킹 기술 전반에 걸쳐 개인 정보가 희귀하거나 고유한 토큰과 더 관련이 있다고 가정하여 모든 코퍼스에서 일반적이지 않은 토큰을 마스크할 가능성이 더 큽니다. 마스킹을 적용한 후 마스크된 토큰이 &quot;[MASK]&quot; 기호로 대체된 마스크된 코퍼스를 얻습니다. 3.2. 마스킹에서 복구 방법 토큰 마스킹은 개인 정보 보호를 제공하지만 결과적으로 마스크된 코퍼스는 다운스트림 작업을 위한 NLP 모델을 훈련하는 데 직접 사용하기에 적합하지 않을 수 있습니다. 모든 마스크된 코퍼스가 주어지면 LLM을 사용하여 각 마스크 기호를 의미적 맥락과 일치하는 적절한 토큰으로 채우는 것을 제안합니다. 원시 코퍼스의 원래 토큰과 정확히 동일한 토큰을 예측하는 것이 목표가 아니라는 점에 유의하는 것이 중요합니다. 우리는 전체 문장을 언어적으로 정확하고 완전하게 만드는 토큰으로 대체할 것으로 예상합니다. 다음은 마스크된 토큰을 대체하기 위해 LLM을 활용하는 다양한 전략을 보여줍니다. • Top-1: 이 방법에서는 LLM에서 예측된 1-best 토큰을 직접 사용하여 마스크된 토큰을 대체합니다. 여기서 토큰 채우기는 마스크된 LM 작업으로 간주됩니다. 문장에 여러 마커가 있는 경우 한 번에 하나씩 왼쪽에서 오른쪽으로 순차적으로 대체됩니다. • Top-K: 이 접근 방식은 LLM의 예측에서 토큰 채우기 후보를 1-best에서 K-best로 확장합니다. 구체적으로, 우리는 상위 K 예측에서 무작위로 토큰을 선택합니다. 그런 다음 선택된 토큰을 사용하여 문장의 마커를 채웁니다. 허용 목록 또는 vocabThres 기반 마스킹 기술에서 마스크된 토큰을 대체하기 위해 예측된 토큰이 해당 토큰 목록에 포함되지 않는 것을 선호하므로 이 조건이 충족되거나 상위 K개 중에 예측된 토큰의 사용 가능한 후보가 없을 때까지 임의 샘플링 프로세스를 반복합니다.미세 조정(FT): 이전 두 가지 접근 방식에서 사전 학습된 LLM의 토큰 예측을 활용합니다.도메인 내 코퍼스를 사용하여 사전 학습된 LLM을 미세 조정하면 모델이 도메인별 지식을 얻는 데 도움이 되므로 마스크된 토큰 예측에서 성능이 향상됩니다.이를 달성하기 위해 마스크된 토큰이 없는 샘플을 미세 조정에 사용할 수 있습니다.그러나 많은 시나리오에서 대부분의 샘플에 하나 이상의 마스크 심볼이 포함되어 미세 조정이 덜 효과적일 수 있으며, 특히 코퍼스 크기가 작은 경우 더욱 그렇습니다. 또는 동일한 사전 학습된 LLM의 상위 1개 또는 상위 K개 예측을 먼저 사용하여 모든 샘플에서 마스크된 토큰을 대체한 다음 전체 난독화 코퍼스를 사용하여 LLM을 미세 조정할 수 있습니다. 미세 조정된 LLM이 있으면 마스크된 토큰을 대체하기 위해 상위 1개 또는 상위 K를 적용할 수 있습니다. 위의 프로세스는 여러 번 사용할 수 있습니다. 이러한 방법 중 하나를 적용한 후 마스크가 없는 난독화 코퍼스를 얻습니다. 3.3. 다운스트림 작업 수행 마스크된 토큰을 대체한 후 결과 코퍼스를 사용하여 모든 다운스트림 작업을 위한 머신 러닝 모델을 학습할 수 있습니다. 토큰 채우기 접근 방식의 효과는 이러한 다운스트림 작업에서 이러한 머신 러닝 모델의 성능으로 측정해야 합니다. 이 작업에서는 난독화 코퍼스에서 일반적인 사전 학습된 LM이 미세 조정되는 언어 모델링 적응 작업을 고려합니다. 이 적응형 LM은 원시 코퍼스와 동일한 도메인을 갖는 (마스크되지 않은) 테스트 세트에서 평가됩니다.LM의 성능은 복잡도 측면에서 측정됩니다.얕은 융합을 통해 적응형 LM을 ASR 모델과 통합할 때 단어 오류율(WER)도 발화 테스트 세트에서 평가할 수 있습니다.4.1. 데이터 세트 4. 실험 다운스트림 언어 모델링 작업에서 여러 기준선과 제안된 접근 방식의 성능을 비교하기 위해 실험에서 세 가지 데이터 세트를 탐색합니다.Fisher [27], Pushshift.io Reddit¹ [28], Wall Street Journal(WSJ) [29].이러한 데이터 세트의 통계는 표 1에 요약되어 있습니다.WSJ 데이터의 테스트 세트는 또한 음성 발화로 구성되므로 융합된 LM이 있는 ASR 모델을 평가하는 데 사용됩니다.4.2. 설정 Fisher Reddit WSJ 표 1. 데이터 정보. 학습 세트(#sent) 테스트 세트(#sent) 4.2.1. 다운스트림 작업 1,158,763,6,50,49, 다운스트림 LM은 6개 레이어, 12개 어텐션 헤드, 768개 숨겨진 유닛을 갖춘 Transformer입니다. 단어 어휘 세트는 약 &#39;Pushshift.io Reddit 데이터 세트는 이전에 제3자가 추출하여 얻은 데이터 세트로, 소셜 네트워크 Reddit에 게시되고 pushshift.io에서 호스팅하는 전처리된 댓글이 포함되어 있습니다. 이 논문의 나머지 부분에서는 이 데이터 세트를 &quot;Reddit&quot;이라고 합니다. 85K. LM은 WikiText-103 코퍼스[30]에서 사전 학습되었습니다. 이 연구에서 고려한 각 마스킹 기술에 대해 LM은 Fisher, Reddit 및 WSJ 데이터의 난독화 학습 세트에서 미세 조정됩니다. 이들의 복잡도는 해당 테스트 세트에서 평가됩니다. WSJ 테스트 세트에서 ASR 성능도 평가합니다. ASR 모델은 Emformer 인코더[31], LSTM 예측기 및 조이너를 갖춘 RNN-T 모델입니다.약 8,000만 개의 매개변수를 가지며 LibriSpeech ASR 코퍼스[32]의 학습 분할을 사용하여 처음부터 학습됩니다.4.2.2. 마스킹 기술 실험에서 allow List는 5,000개의 선별된 일반 단어 세트를 포함하고 vocabThres는 위에서 언급한 동일한 85,000개 단어 어휘 중 가장 빈번한 10,000개 단어로 구성됩니다.entityTagger 마스킹 기술의 경우 BERT-NER 모델[13, 33]을 활용하여 학습 세트에서 명명된 엔터티를 태그 지정합니다.이러한 각 마스킹 기술에 대해 표 2는 데이터 세트당 마스킹된 토큰의 백분율을 보여줍니다.allow List가 다른 두 기술보다 훨씬 더 많은 토큰을 마스킹한다는 것을 알 수 있습니다.표 2. 마스킹된 토큰의 백분율. allowList vocabThres entityTagger Fisher 12.5% 1.3% 1.7% Reddit 22.7% 11.9% WSJ 30.4% 11.2% 4.2% 9.1% 4.2.3. 기준선 우리는 다음 방법을 기준선으로 고려합니다.• Oracle: LM은 마스킹 없이 기준 진실 문장에 대해 훈련되며, 이는 각 데이터 세트에서 모델 성능에 대한 상한을 제공합니다.• Baseline0: LM은 마스크된 코퍼스에 대해 직접 훈련되며, 마스크 기호 &quot;[MASK]&quot;는 모델 훈련 중 특수 토큰으로 처리됩니다.• Baselinel: 모델 훈련 중 LM 손실 함수의 모든 마스크 기호 &quot;[MASK]&quot;에 가중치가 0으로 지정됩니다.각 방법의 경우 LM은 여전히 WikiText-103 코퍼스에 대해 사전 훈련됩니다.4.2.4. LLM 기반 방법 실험에서 우리는 모든 훈련 시퀀스에서 마스크된 토큰을 대체하기 위해 다음 LLM을 고려합니다.BERT(기본, 케이스 없음), ROBERTA(기본), LLAMA2(7B 모델 매개변수).BERT와 ROBERTA의 미세 조정을 위해 MLM을 훈련 과제로 사용합니다.사전 훈련되거나 미세 조정된 BERT와 ROBERTa를 사용하여 마스크된 토큰을 대체하는 추론 시간 동안, &quot;[MASK]&quot;의 연속된 마커는 하나의 마커로 병합됩니다.Top-K 방법에서 K = 10을 설정합니다.LLAMA2의 경우 자기 회귀 모델이므로 미세 조정 프로세스에 다른 접근 방식을 채택합니다.특히, 각 훈련 샘플에 대해 일부 지침, 입력 및 출력 텍스트를 결합하여 프롬프트를 생성합니다.지침에는 &quot;[MASK] 토큰을 주어진 문장에서 예측하십시오&quot;라는 텍스트가 포함됩니다.입력은 동일한 훈련 샘플이지만 몇 개의 토큰이 &quot;[MASK]&quot; 기호로 무작위로 대체되었습니다. 출력은 원래의 학습 샘플(마스킹 없음)입니다. 저희는 프롬프트 세트에서 LLAMA2를 미세 조정하기 위해 저순위 적응(LORA) 방법[34]을 활용합니다. 추론 시간 동안 미세 조정된 모델에 지침과 입력이 제공되어 모델이 계속해서 텍스트를 생성할 수 있습니다. 4.3. 결과 표 3은 Fisher 데이터 세트에 대한 기준선과 제안된 방법의 복잡도 결과를 보여줍니다. 다음과 같은 관찰 결과가 있습니다. • 모든 제안된 방법은 두 기준선 방법보다 낮은 복잡도 결과를 제공합니다. • 모든 시나리오에서 Top-K는 Top-1 기반 방법보다 성능이 우수합니다. 미세 조정된 BERT와 ROBERTA는 미세 조정이 없는 방법보다 더 나은 결과를 얻습니다. • 허용 목록으로 더 많은 토큰이 마스크되므로 Oracle과 다른 방법 간의 격차가 vocabThres 또는 entityTagger 마스킹 기술보다 훨씬 큽니다. • ROBERTA는 모든 마스킹 기술에서 가장 우수한 복잡도 성능을 제공합니다. 특히, vocabThres와 entityTagger의 경우, 미세 조정된 ROBERTa의 복잡도 결과는 Oracle의 결과와 매우 유사하여 대부분의 누락된 정보를 난독화 데이터 세트에서 복구할 수 있음을 나타냅니다.• LLAMA2(Top-1, FT)는 경쟁력 있는 방법이지만 이 작업의 경우 미세 조정된 BERT나 ROBERTA만큼 좋지 않습니다.표 3. Fisher 데이터 세트의 복잡도 결과.표 5와 표 6은 각각 WSJ 데이터 세트의 복잡도 및 WER 결과를 보여줍니다.다음과 같은 결과가 나왔습니다.• ASR 모델에서 도메인 적응을 수행하기 위해 융합된 LM을 사용하는 것이 효과적입니다.사전 훈련된 LM이 있는 ASR 모델과 Oracle LM 간의 WER을 비교한 결과, 후자가 15% 이상의 WER 개선을 달성했습니다.• 제안된 방법으로 얻은 최상의 WERS는 Oracle LM과 비교하여 갭이 비교적 작습니다. vocabThres와 entity Tagger 마스킹 기술의 경우, Oracle의 WERS는 각각 1%(10.7 대 10.6)와 5%(11.1 대 10.6)만 향상되었습니다. 즉, 제안된 방법은 사전 학습된 LM(적응 없음)에 비해 상당한 개선을 달성할 수 있으며 Oracle LM보다 더 나은 개인 정보 보호 기능을 제공합니다. Oracle 표 5. WSJ 데이터 세트에 대한 Perplexity 결과. allowList vocabThres entityTagger BaselineBaselinel BERT(상위 1) 86.86.86.309.144.204.210.122.198.205.119.149.ROBERTA(상위 1) 181.102.118.BERT(상위 K) 174.103.108.ROBERTA(상위 K) 114.93.98.BERT(상위 K, FT) 186.113.162.ROBERTA(상위 K, FT) 120.110.157.LLAMA2(상위 1, FT) 135.106.145.allowList vocabThres entity Tagger Oracle 37.37.37.Table 6. WSJ 데이터 세트의 WER 결과. allowList vocabThres entityTagger Baseline120.42.41.ASR-without-LM 14.14.14.Baselinel 109.41.41.Pre-Trained-LM 12.12.12.BERT(상위 1) 93.41.41.Oracle 10.10.10.ROBERTA(상위 1) 71.40.39.Baseline13.12.11.BERT(상위 K) 75.40.40.Baselinel 12.11.11.ROBERTA(상위 K) 70.38.38.BERT(상위 1) 12.11.11.BERT(상위 K, FT) 73.39.39.ROBERTA(상위 1) 12.10.11.ROBERTA (Top-K, FT) 65.38.38.BERT (Top-K) 12.11.11.LLAMA2 (Top-1, FT) 89.40.40.ROBERTA (Top-K) 11.10.11.BERT (Top-K, FT) 12.11.11.ROBERTA (Top-K, FT) LLAMA2 (Top-1, FT) 11.11.11.12.10.11.표 4는 Reddit 데이터 세트에 대한 실험 결과를 보여줍니다. 관찰 결과는 Fisher 데이터 세트의 관찰 결과와 유사합니다. 특히 ROBERTA (Top-K, FT)는 모든 마스킹 기술에 걸쳐 다시 가장 우수한 복잡도 결과를 달성합니다.표 4. Reddit 데이터 세트에 대한 복잡도 결과. allowList vocabThres 엔터티 태거 Oracle 76.76.76.Baseline339.168.82.Baselinel 221.134.79.BERT(상위 1) 196.121.78.ROBERTA(상위 1) 117.94.78.BERT(상위 K) 127.106.78.ROBERTA(상위 K) 123.92.77.BERT(상위 K, FT) 117.102.77.ROBERTA(상위 K, FT) 98.82.76.LLAMA2(상위 1, FT) 123.107.78.5.
--- CONCLUSION ---
이 논문에서는 텍스트 코퍼스에서 개인 정보 보호 토큰 마스킹을 복구하기 위해 사전 훈련되고 미세 조정된 여러 LLM 기반 방법을 제안하고 이러한 접근 방식을 비교하기 위해 다양한 데이터 세트에 대한 실증 연구를 수행합니다. 실험 결과는 난독화 코퍼스에서 훈련된 LM이 개인 정보 보호 토큰 마스킹 없이 원시 데이터에서 훈련된 LM과 비슷한 정확도를 얻을 수 있음을 보여줍니다. 향후 연구에는 다운스트림 NLP 작업과 더 직접적으로 관련되도록 설계된 객체 함수로 LLM을 미세 조정하는 것이 포함될 수 있습니다. 또한 이 세 가지 마스킹 기술을 조합하고 &quot;[PERSON]&quot;, &quot;[NUMBER]&quot; 등과 같은 클래스별 마커를 채택할 것입니다. 6. 참고문헌 [1] Ke Li, Zhe Liu, Tianxing He, Hongzhao Huang, Fuchun Peng, Daniel Povey, and Sanjeev Khudanpur, &quot;변압기 기반 신경 언어 모델 적응에 대한 실증적 연구&quot;, Proc. 영어: ICASSP, 2020. [2] Zhe Liu, Ke Li, Shreyan Bakshi 및 Fuchun Peng, &quot;음성 인식을 위한 개인 언어 모델 적응,&quot; arXiv 사전 인쇄본 arXiv:2110.10026, 2021. [3] Matt Fredrikson, Somesh Jha 및 Thomas Ristenpart, &quot;신뢰 정보와 기본 대책을 악용하는 모델 역전 공격,&quot; Proc. ACM SIGSAC, 2015. [4] Congzheng Song 및 Vitaly Shmatikov, &quot;텍스트 생성 모델에서 데이터 출처 감사,&quot; Proc. ACM SIGKDD, 2019. [5] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, &quot;비밀 공유자: 신경망에서 의도치 않은 기억력 평가 및 테스트&quot;, 제28회 USENIX 보안 심포지엄, 2019. [6] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al., &quot;대규모 언어 모델에서 학습 데이터 추출&quot;, 제30회 USENIX 보안 심포지엄, 2021. [7] W Ronny Huang, Steve Chien, Om Thakkar, Rajiv Mathews, &quot;언어 모델 융합 ASR에서 의도치 않은 기억력 감지&quot;, Proc. Interspeech, 2022. [8] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, &quot;신경 언어 모델에서 기억 정량화&quot;, arXiv 사전 인쇄본 arXiv:2202.07646, 2022. [9] Sergio Martínez, David Sánchez, Aida Valls, Montserrat Batet, &quot;의미 기반 마스킹 방법을 통한 텍스트 속성의 개인 정보 보호&quot;, Information Fusion, vol. 13, no. 4, pp. 304–314, 2012. [10] Samuel Sousa와 Roman Kern, &quot;텍스트를 비공개로 유지하는 방법? 개인 정보를 보호하는 자연어 처리를 위한 딥 러닝 방법에 대한 체계적 검토&quot;, Artificial Intelligence Review, vol. 56, no. 2, pp. 1427-1492, 2023. [11] Judita Preiss, &quot;자동 명명된 엔터티 난독화&quot;, ACL 조사 결과, 2023. [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin, &quot;주의가 필요한 전부입니다.&quot; NeurIPS의 발전, 2017. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, &quot;BERT: 언어 이해를 위한 심층 양방향 변환기 사전 훈련&quot;, arXiv 사전 인쇄 arXiv:1810.04805, 2018. [14] Yinhan Liu, Myle Ott, Naman 고얄, 두징페이, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, “ROBERTa: 강력하게 최적화된 BERT 사전 학습 접근법,” arXiv 사전 인쇄본 arXiv:1907.11692, 2019. [15] OpenAI, &quot;ChatGPT: 대화를 위한 언어 모델 최적화,&quot; 2022년 2월. [16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al., “Llama 2: Open foundation and finetuned chat models,&quot; arXiv 사전 인쇄본 arXiv:2307.09288, 2023. [17] Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, 및 Sanjeev Khudanpur, &quot;순환 신경망 기반 언어 모델&quot;, Proc. Interspeech, 2010. [18] Xie Chen, Xunying Liu, Mark JF Gales, 및 Philip C Woodland, &quot;순환 신경망 언어 모델의 학습 및 평가 효율성 개선&quot;, Proc. ICASSP, 2015. [19] Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF Gales, 및 Philip C Woodland, &quot;순환 신경망 언어 모델을 사용한 효율적인 격자 재점수 매기기&quot;, Proc. ICASSP, 2014. [20] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, 및 Rohit Prabhavalkar, &quot;외부 언어 모델을 시퀀스 대 시퀀스 모델에 통합하는 것에 대한 분석&quot;, Proc. ICASSP, 2018. [21] Kazuki Irie, Albert Zeyer, Ralf Schlüter 및 Hermann Ney, &quot;심층 변환기를 사용한 언어 모델링&quot;, Proc. Interspeech, 2019. [22] Pierre Lison, Ildikó Pilán, David Sánchez, Montserrat Batet 및 Lilja Øvrelid, &quot;텍스트 데이터의 익명화 모델: 최신 기술, 과제 및 미래 방향&quot;, Proc. ACL, 2021. [23] Tzvika Hartman, Michael D Howell, Jeff Dean, Shlomo Hoory, Ronit Slyper, Itay Laish, Oren Gilon, Danny Vainstein, Greg Corrado, Katherine Chou 외, &quot;임상 기록의 익명화를 위한 사용자 정의 시나리오&quot;, BMC 의료 정보학 및 의사 결정, 제20권, 제1호, 1-9쪽, 2020. [24] Debora Nozza 및 Dirk Hovy, &quot;자연어 처리에서의 욕설 난독화 상태&quot;, arXiv 사전 인쇄본 arXiv:2210.07595, 2022. [25] Zhifeng Hu, Serhii Havrylov, Ivan Titov 및 Shay B. Cohen, &quot;개인 정보 보호를 위한 구문 분석을 위한 난독화&quot;, 2020. [26] Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran, &quot;텍스트 주입을 사용하여 음성에서 개인 식별자 인식 개선,&quot; arXiv 사전 인쇄본 arXiv:2308.07393, 2023. [27] Christopher Cieri, David Miller, Kevin Walker, &quot;fisher 코퍼스: 차세대 음성-텍스트 리소스,&quot; 국제 언어 리소스 및 평가 컨퍼런스, 2004. [28] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, Jeremy Blackburn, &quot;Pushshift reddit 데이터 세트,&quot; 국제 웹 및 소셜 미디어 컨퍼런스, 2020. [29] Lukas Drude, Jens Heitkaemper, Christoph Boeddeker 및 Reinhold Haeb-Umbach, &quot;SMS-WSJ: 다중 채널 소스 분리 및 인식을 위한 데이터베이스, 성능 측정 및 기준 레시피,&quot; arXiv 사전 인쇄본 arXiv:1910.13934, 2019. [30] Stephen Merity, Caiming Xiong, James Bradbury 및 Richard Socher, &quot;포인터 센티넬 혼합 모델,&quot; arXiv 사전 인쇄본 arXiv:1609.07843, 2016. [31] Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le 및 Mike Seltzer, &quot;Emformer: 저지연 스트리밍 음성 인식을 위한 효율적인 메모리 변환기 기반 음향 모델,&quot; Proc. ICASSP, 2021. [32] Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, &quot;LibriSpeech: 퍼블릭 도메인 오디오북을 기반으로 한 ASR 코퍼스&quot;, ICASSP 회의록, 2015. [33] Erik F. Tjong Kim Sang, Fien De Meulder, &quot;CONLL-2003 공유 과제 소개: 언어 독립적인 명명된 엔터티 인식&quot;, HLT-NAACL 2003에서 열린 제7회 자연어 학습 컨퍼런스 회의록, 2003. [34] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, &quot;LORA: 대규모 언어 모델의 저순위 적응&quot;, arXiv 사전 인쇄본 arXiv:2106.09685, 2021.
"
"--- ABSTRACT ---
This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model’s predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation. Index Terms— Audio Generation, Music Generation, Representation regularization 1.
--- INTRODUCTION ---
Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, including augmented, virtual and mixed reality, video game development, and movie production. The advent of recent neural generative models have brought about a transformative shift in the landscape of digital content generation. Drawing inspiration from the remarkable progress in image generation [1] [2], the realm of audio generation has undergone a paradigm shift — transitioning from conventional signal processing approaches to neural generative models | {8} (9\ {10}. Just as in the case of text-to-image generation models , harnessing the potential of diffusion probability models (12)(I3], the studies [9| [14] (15) [16] (4) (5) [17|[18]] have show cased impressive capacity in the realms of speech synthesis, sound effects creation, and music generation. Alongside the diffusion-based approach, a parallel avenue has been pursued using transformer-based language models [19], which have also exhibited exceptional performance in audio generation sla [7. In language model driven approach like MusicGen [8] and AudioGen [6], it first encodes raw audio into discrete tokens via a neural audio compression model (e.g., [23] [24]). This model is end-to-end trained to compress and reconstruct input audio from discrete tokens with high quality and minimum perceptual loss. The generation model then employs an auto regressive transformer-decoder language model. The language model operates on discrete audio tokens from the first phase and is conditioned on text inputs. Text is processed as text embedding representation using an text encoder pretrained on a large text corpus, such as T5 [25]. The text representation is used as cross attentions in the language model training. The language model is trained by cross-entropy loss to minimize the entropy to predict next discrete audio token based on the previous audio tokens and the text representation. However, in the whole training process, there is not any regularization to enforce the next audio token prediction to fully leverage representations from both audio token and conditioning text. As a consequence, the generated audio often isn’t fully aligned with the provided text prompt. It is often that the music generated based on the description Highly rhythmic orchestral piece illustrating wonder and awe. Features staccato violins, cellos, basses, trombone and grand piano”, misses one or more instruments from the description. The sound effects generated from the condition ”the sound of a ping pong ball bounce back once from the hard wood floor” has multiple ping pong ball bouncing sounds. This paper introduces a method aiming at improving the training of the generation model to effectively capture representations from text conditions. This is achieved by minimizing the similarity between text and audio representations through regularization. Language model training comprises two modes: text-conditioned training and classifier-free guidance (CFG) training (6). In CFG, the text condition is omitted during language model training. We enhance the audio and text representation similarity by reducing discrepan --- --cies in audio and text similarity compared to other samples within the same training batch. Experimental results in music and sound effects generation demonstrate the effectiveness of the proposed approach, showcasing improvements in Frechet audio distance (FAD) using VGG classifier kullback-—leibler (KL) divergence using PaSST model text and audio alignment score based on the contrastive language audio pretrained models (CLAP) [29], and human subjective evaluation for audio generation. 2.
--- RELATED WORK ---
This study applies the language model approach presented in works such as [2 [6] [7], in which the compression model discretizes audio into tokens for training and then decodes these tokens to audio. The language model learns to generate audio tokens. However, our emphasis lies in augmenting the semantic correlation between provided text descriptions and the generated audio. This enhancement is built upon the foundation of the MusicGen [8] and AudioGen [6] for language model-driven audio generation. To model the representation similarity between text and audio, one related work is CLAP [29] which uses contrastive loss. However, we found that using the contrastive loss in CLAP for generation model training did not improve the performance. Instead, we propose a new approach that first computes the representation similarities of audios and texts between different samples. We then minimize the discrepancies between the audios’ similarities and the texts’ similarities. Additionally, we found that max pooling is better than average pooling for obtaining the sequence level representation from individual time step output. 3. REPRESENTATION REGULARIZATION > ry Text Input +{ Tea Encoder} —| Text Representation ' (Cinear Projection ~) ((stackoF Transformers)». Audio Representation Each embedding table comtesponds to one cookbook ‘uso Tokens Shit One Time Step ata] -Ceeme) Fig. 1. Illustration of the language model training with cross entropy loss and representation regularization. 3.1. Language model based audio generation The language model based audio generation model is composed of several pivotal elements as shown in Fig[T] Firstly, it employs a compression model, such as the EnCodec model ] to encode the raw audio data into a discrete multi-stream sequence of tokens a,,;. Here i € [1,T,,] and T, is the length of the audio token sequence, while k € [1, K], indicating the particular codebook indexed as the k-th. Additionally, the model incorporates a pre-trained text encoder, which transforms the text input into a sequence of embedding representations identified as v;, where 7 € [1,T7,], T, corresponds to the length of the sequence containing text embedding representations. Lastly, there is a language model component that is a stack of Transformer layers. The language model leverages both the text embedding representation and the preceding audio tokens to generate the probability distribution for the subsequent audio token as Po(Gk,i+1|@k,1, ++) 4k,i, V1, +, UT, ). To render audio generation more manageable, the generation of multi-stream audio tokens is trained in parallel, resulting in a substantial reduction in the effective sequence length during model training. The loss for the language model is the sum of the cross entropy loss for each stream k. K Ta Leond = — Sy log (Po (Akit1|@k.1s +5 Oki) V1; «5 02,)) CD) k=1i=3.2. Representation regularization However, the cross entropy loss in language model lacks explicit mechanism to enforce the audio token prediction align with the provided text conditions. Furthermore, the correlation between text and audio gets even loosen as the classifierfree guidance (CFG)
--- METHOD ---
s lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation. Index Terms— Audio Generation, Music Generation, Representation regularization 1. INTRODUCTION Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, including augmented, virtual and mixed reality, video game development, and movie production. The advent of recent neural generative models have brought about a transformative shift in the landscape of digital content generation. Drawing inspiration from the remarkable progress in image generation [1] [2], the realm of audio generation has undergone a paradigm shift — transitioning from conventional signal processing approaches to neural generative models | {8} (9\ {10}. Just as in the case of text-to-image generation models , harnessing the potential of diffusion probability models (12)(I3], the studies [9| [14] (15) [16] (4) (5) [17|[18]] have show cased impressive capacity in the realms of speech synthesis, sound effects creation, and music generation. Alongside the diffusion-based approach, a parallel avenue has been pursued using transformer-based language models [19], which have also exhibited exceptional performance in audio generation sla [7. In language model driven approach like MusicGen [8] and AudioGen [6], it first encodes raw audio into discrete tokens via a neural audio compression model (e.g., [23] [24]). This model is end-to-end trained to compress and reconstruct input audio from discrete tokens with high quality and minimum perceptual loss. The generation model then employs an auto regressive transformer-decoder language model. The language model operates on discrete audio tokens from the first phase and is conditioned on text inputs. Text is processed as text embedding representation using an text encoder pretrained on a large text corpus, such as T5 [25]. The text representation is used as cross attentions in the language model training. The language model is trained by cross-entropy loss to minimize the entropy to predict next discrete audio token based on the previous audio tokens and the text representation. However, in the whole training process, there is not any regularization to enforce the next audio token prediction to fully leverage representations from both audio token and conditioning text. As a consequence, the generated audio often isn’t fully aligned with the provided text prompt. It is often that the music generated based on the description Highly rhythmic orchestral piece illustrating wonder and awe. Features staccato violins, cellos, basses, trombone and grand piano”, misses one or more instruments from the description. The sound effects generated from the condition ”the sound of a ping pong ball bounce back once from the hard wood floor” has multiple ping pong ball bouncing sounds. This paper introduces a method aiming at improving the training of the generation model to effectively capture representations from text conditions. This is achieved by minimizing the similarity between text and audio representations through regularization. Language model training comprises two modes: text-conditioned training and classifier-free guidance (CFG) training (6). In CFG, the text condition is omitted during language model training. We enhance the audio and text representation similarity by reducing discrepan --- --cies in audio and text similarity compared to other samples within the same training batch.
--- EXPERIMENT ---
al results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation. Index Terms— Audio Generation, Music Generation, Representation regularization 1. INTRODUCTION Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, including augmented, virtual and mixed reality, video game development, and movie production. The advent of recent neural generative models have brought about a transformative shift in the landscape of digital content generation. Drawing inspiration from the remarkable progress in image generation [1] [2], the realm of audio generation has undergone a paradigm shift — transitioning from conventional signal processing approaches to neural generative models | {8} (9\ {10}. Just as in the case of text-to-image generation models , harnessing the potential of diffusion probability models (12)(I3], the studies [9| [14] (15) [16] (4) (5) [17|[18]] have show cased impressive capacity in the realms of speech synthesis, sound effects creation, and music generation. Alongside the diffusion-based approach, a parallel avenue has been pursued using transformer-based language models [19], which have also exhibited exceptional performance in audio generation sla [7. In language model driven approach like MusicGen [8] and AudioGen [6], it first encodes raw audio into discrete tokens via a neural audio compression model (e.g., [23] [24]). This model is end-to-end trained to compress and reconstruct input audio from discrete tokens with high quality and minimum perceptual loss. The generation model then employs an auto regressive transformer-decoder language model. The language model operates on discrete audio tokens from the first phase and is conditioned on text inputs. Text is processed as text embedding representation using an text encoder pretrained on a large text corpus, such as T5 [25]. The text representation is used as cross attentions in the language model training. The language model is trained by cross-entropy loss to minimize the entropy to predict next discrete audio token based on the previous audio tokens and the text representation. However, in the whole training process, there is not any regularization to enforce the next audio token prediction to fully leverage representations from both audio token and conditioning text. As a consequence, the generated audio often isn’t fully aligned with the provided text prompt. It is often that the music generated based on the description Highly rhythmic orchestral piece illustrating wonder and awe. Features staccato violins, cellos, basses, trombone and grand piano”, misses one or more instruments from the description. The sound effects generated from the condition ”the sound of a ping pong ball bounce back once from the hard wood floor” has multiple ping pong ball bouncing sounds. This paper introduces a method aiming at improving the training of the generation model to effectively capture representations from text conditions. This is achieved by minimizing the similarity between text and audio representations through regularization. Language model training comprises two modes: text-conditioned training and classifier-free guidance (CFG) training (6). In CFG, the text condition is omitted during language model training. We enhance the audio and text representation similarity by reducing discrepan --- --cies in audio and text similarity compared to other samples within the same training batch. Experimental results in music and sound effects generation demonstrate the effectiveness of the proposed approach, showcasing improvements in Frechet audio distance (FAD) using VGG classifier kullback-—leibler (KL) divergence using PaSST model text and audio alignment score based on the contrastive language audio pretrained models (CLAP) [29], and human subjective evaluation for audio generation. 2. RELATED WORK This study applies the language model approach presented in works such as [2 [6] [7], in which the compression model discretizes audio into tokens for training and then decodes these tokens to audio. The language model learns to generate audio tokens. However, our emphasis lies in augmenting the semantic correlation between provided text descriptions and the generated audio. This enhancement is built upon the foundation of the MusicGen [8] and AudioGen [6] for language model-driven audio generation. To model the representation similarity between text and audio, one related work is CLAP [29] which uses contrastive loss. However, we found that using the contrastive loss in CLAP for generation model training did not improve the performance. Instead, we propose a new approach that first computes the representation similarities of audios and texts between different samples. We then minimize the discrepancies between the audios’ similarities and the texts’ similarities. Additionally, we found that max pooling is better than average pooling for obtaining the sequence level representation from individual time step output. 3. REPRESENTATION REGULARIZATION > ry Text Input +{ Tea Encoder} —| Text Representation ' (Cinear Projection ~) ((stackoF Transformers)». Audio Representation Each embedding table comtesponds to one cookbook ‘uso Tokens Shit One Time Step ata] -Ceeme) Fig. 1. Illustration of the language model training with cross entropy loss and representation regularization. 3.1. Language model based audio generation The language model based audio generation model is composed of several pivotal elements as shown in Fig[T] Firstly, it employs a compression model, such as the EnCodec model ] to encode the raw audio data into a discrete multi-stream sequence of tokens a,,;. Here i € [1,T,,] and T, is the length of the audio token sequence, while k € [1, K], indicating the particular codebook indexed as the k-th. Additionally, the model incorporates a pre-trained text encoder, which transforms the text input into a sequence of embedding representations identified as v;, where 7 € [1,T7,], T, corresponds to the length of the sequence containing text embedding representations. Lastly, there is a language model component that is a stack of Transformer layers. The language model leverages both the text embedding representation and the preceding audio tokens to generate the probability distribution for the subsequent audio token as Po(Gk,i+1|@k,1, ++) 4k,i, V1, +, UT, ). To render audio generation more manageable, the generation of multi-stream audio tokens is trained in parallel, resulting in a substantial reduction in the effective sequence length during model training. The loss for the language model is the sum of the cross entropy loss for each stream k. K Ta Leond = — Sy log (Po (Akit1|@k.1s +5 Oki) V1; «5 02,)) CD) k=1i=3.2. Representation regularization However, the cross entropy loss in language model lacks explicit mechanism to enforce the audio token prediction align with the provided text conditions. Furthermore, the correlation between text and audio gets even loosen as the classifierfree guidance (CFG) method [26] [6]/8] is used in the training to regulate the balance between sample quality and diversity. Employing CFG involves training the language model both conditionally and unconditionally. Similar to AudioGen [6], 10% of the training samples have their accompanying text omitted during language model training. In unconditional situation, the loss is simply K Ta Luncond = — S S log(pe(ak,i41|@k,1, «++; @k,i)) (2) k=1i=In this work, the proposed representation regularization strengthens the correlation between audio representation and text representation while still maintains the effects of CFG method to train the language model unconditionally on text. Given a batch of training samples, a pooling method F is used to get the text sequence representation as T? = F(v}, ..., vf, ) and audio sequence representation as A’ = F(u, ..., u4,, ) for the particular sample b in the batch. In our experiments, the max pooling achieved the best results. Rather than directly mapping the text and audio representations to the same space and maximizing the similarity between audio and text as CLAP [29], we propose to minimize discrepancies in audio and text similarity compared to other samples within the same training batch as follows: bb T«T? = —-*_ @) ZP| |Z || --- --bb AP x AP 4) \|A°||||A5|| (bb ab,byBx(B-1) Here T°” denotes the representation similarity between text inputs in sample b and b. And A®® denotes the representation similarity between audio in sample b and b. B is the batch size. The L,.,. enforces the text and audio in one sample have the same differences regarding to the other samples. In this study, the proposed representation regularization is exclusively applied during the CFG phase. The complete model training loss is defined as follows: if CFG is utilized Le Luncond + ALpr i © if CFG is not used Leona Here, represents the weighting factor for the representation regularization. Note that representation regularization is only employed during regular training steps when CFG is in use. We also conducted experiments involving representation regularization in non-CFG scenarios; however, these experiments did not yield improvements in objective metrics. We believe the degradation may be attributed to the fact that representation regularization has the potential to hinder language model learning by copying the text representation from crossattention as the audio representation in non-CFG. 4, EXPERIMENTS In this work, we use two sets of experiments including the sound effects generation and the music generation to verify the effectiveness of proposed methods. 4.1. Datasets In music generation, we utilize a total of 20K hours of licensed music which comprises an internal compilation of 10K music tracks of high quality, and 390k instrument-only music tracks from the ShutterStock!] and Pond] All datasets are full-length music with 32 kHz sampling rate, accompanied by comprehensive metadata such as textual descriptions, genre categorizations, BPM, and tags. Our evaluation uses the MusicCaps benchmark [7]. The MusicCaps benchmark comprises 5.5K samples including a subset of 1K samples balanced across various genres. We report objective metrics on the unbalanced subset as [8]. For sound effect model training, a dataset encompassing 4k hours of training data is employed. This dataset 'www.shutterstock.com/music 2www.pond5.com incorporates resources like AudioSet [31], BBC sound effects*| AudioCaps[32], Clotho v2 [33], VGG-Sound [34], FSDS0K and Free To Use Sound##] All audio files are sampled at a rate of 16kHz. We adopt a preprocessing methodology akin to [6] for textual descriptions. To begin, we utilize multi-label annotations from datasets such as AudioSet, VGG-Sound, FSD50K. Pseudo-sentences are constructed by concatenating lists of tags linked with audio samples. Subsequently, we eliminate stop words and numbers, and lemmatize natural language captions available in datasets including AudioCaps, Clotho v2, Free To Use Sounds, and BBC Sound Effects. Lastly, samples containing the term speech” in their tag or caption are filtered out, given that speech predominates in the data. 4.2. Setup Our approach involves a non-causal five-layer EnCodec model tailored for music generation, operating at 32 kHz for monophonic music, and 16 kHz for sound effects generation. These EnCodec models maintain a frame rate of 50 Hz, commencing with an initial hidden size of 64, which doubles across the model’s five layers. Embeddings are subjected to quantization using an RVQ comprising four quantizers, each featuring a codebook size of 2048. These EnCodec models are trained using the same audio data as those in the language model training. The transformer models used in this work have 300M parameters. To enhance efficiency with long sequences, we employ memory-efficient Flash attention [36] from the xFormers package [37], improving both speed and memory utilization. For ablations, we consistently employ the sound effects generation model setup. For music generation model training, 30-second audio segments are used, randomly sampled from the complete track. In sound effects generation training, 10second audio clips are used. Model training spans 100K steps, utilizing the AdamW optimizer [38], a batch size of 192 examples, 6; = 0.9, 62 = 0.95, a decoupled weight decay of 0.1, and gradient clipping of 1.0. A cosine learning rate schedule is employed, with a warmup of 4k steps. Furthermore, an exponential moving average is applied, characterized by a decay factor of 0.99. The model training employs the mixed precision with Fully Sharded Data Parallel (FSDP) bfloatl6. We used 16 GPUs and 32 GPUs for sound effects generation and music generation training, respectively. In the sampling process for inference, we adopt top-k sampling [39], retaining the top 250 tokens and applying a temperature of 1.0. 4.3. Ablation Study Table [I] presents the results of the ablation study conducted on the sound effects generation model using the AudioCaps dataset. The optimal model was trained with representation 3https://sound-effects.bbcrewind.co.uk/ 4https://www.freetousesounds.com/all-in-one-bundle/ --- --regularization based on max pooling, employing a weight parameter of = 3.0 and allocating 10% of the training data for CFG training. In contrast, the use of average poolingbased sequence representation regularization did not demonstrate any improvement over the baseline. Furthermore, Table[I]reaffirms the significant role of CFG training in reducing both FAD and KL scores. pool CFG | FAD() KL() CLAP(t) max 01 3 1.43 1.57 0.max 01 4 1.44 1.58 0.max 01 2 1.56 1.57 0.max O11 1.58 1.61 0.- 0.2 0 1.56 1.60 0.- 01 0 1.52 1.60 0.- 0.0 0 1.69 1.58 0.max 0.2 3 1.59 1.64 0.average 0.1 3 1.54 1.59 0.Table 1. Ablation study using sound effects generation based on AudioCaps. The column ‘pool’ denotes the pooling method to get the sequence level representation for both audio and text representation. ‘CFG’ column gives the ratio of using CFG in training. ‘\’ represents the weight used in representation regularization. 4.4. Music Generation Table [2] gives the objective metrics on the MusicCaps data. We report the original metrics for MuiscLM, Noise2Music and MusicGen 1.5B model without melody. Notably, the introduction of the proposed representation regularization results in enhancements across all metrics. Our 300M parameter model, which incorporates representation regularization, surpasses the performance of the MusicGen 1.5B parameter model in terms of FAD and CLAP. Methods FAD()) KL({) CLAP(t) MusicLM [7 4.0 - Noise2Music[40] 2.1 - MusicGen 1.5B[8] 5.0 1.31 0.ours 300M w/o rr 5.28 1.36 0.ours 300M w/ rr 4.83 1.32 0.Table 2. Music generation using MusicCaps. ’w/ rr’ and ’w/o rr’ mean with and without represenation regularization, respectively. 4.5. Sound Effects Generation The sound effects generation results on AudioCaps are shown in Table 3] The trend is the same as the music generation experiments. The representation regularization improves the model performance on FAD, KL and CLAP. The results of AudioGen is referring to the githutf| Methods FAD()) KL) CLAP(t) AudioGen [6] 1.77 1.58 0.ours w/o rr 1.52 1.60 0.ours w/ rr 1.43 1.57 0.Table 3. Sound effects generation using AudioCaps. ’w/ rr’ and ’w/o rr’ mean with and without represenation regularization, respectively. 4.6. Human preference evaluation Table [4] gives the subjective metrics for the sound and music generation models. Our subjective evaluation employed a blind pairwise comparison test, where evaluators were presented with two samples generated by distinct models, all based on the same text prompt. This comparison was conducted across a set of 20 text prompts, and eight human evaluators were tasked with determining their preference for the sample they believed exhibited better quality and better alignment with the provided prompt in each pair. Notably, both music and sound effects generation, when incorporating representation regularization, garnered higher user preference ratings. A possible explanation for the more significant trend in the sound effects generation is that music tends to be more abstract than sound effects. Consequently, any discrepancies in alignment with the provided text may not be as readily apparent to human evaluators. Methods music — sound effects ours w/orr 48% 33% oursw/tr 52% 67% Table 4. Human preference evaluation 5.
--- CONCLUSION ---
This paper has introduced representation regularization to improve controllability over audio generation by prioritizing alignment between audio and text representations during model training. The proposed method integrated the audio and text similarity regularization, particularly during the classifier-free guidance (CFG) phase, wherein the text condition is excluded from cross attention during language model training. The experimental results, conducted across various audio and music generation tasks, demonstrate that the proposed representation regularization has led to improvements in objective metrics for both audio and music generation. Moreover, these improvements have translated into a noticeable enhancement in human perception regarding audio generation quality and alignment. Shttps://github.com/facebookresearch/audiocraft/blob/main/model_cards --- ---6. REFERENCES Robin Rombach, Andreas Blattmann, et al., “High-resolution image synthesis with latent diffusion models,” in CVPR, 2022. Aditya Ramesh, Prafulla Dhariwal, et al., “Hierarchical TextConditional image generation with CLIP latents,” arXiv, 2022. Yang Song, Jascha Sohl-Dickstein, et al., “Score-Based generative modeling through stochastic differential equations,” arXiv, 2020. Haohe Liu, Qiao Tian, et al., “AudioLDM 2: Learning holistic audio generation with self-supervised pretraining,” arXiv, Aug. 2023. Haohe Liu, Zehua Chen, et al., “AudioLDM: Text-to-Audio generation with latent diffusion models,” arXiv, 2023. Felix Kreuk, Gabriel Synnaeve, et al., “AudioGen: Textually guided audio generation,” arXiv, 2022. Andrea Agostinelli, Timo I Denk, et al., “MusicLM: Generating music from text,” arXiv, 2023. Jade Copet, Felix Kreuk, et al., “Simple and controllable music generation,” arXiv, 2023. Matthew Le, Apoorv Vyas, et al., “Voicebox: Text-Guided multilingual universal speech generation at scale,” arXiv, 2023. Max W Y Lam, Qiao Tian, et al., “Efficient neural music generation,” arXiv, 2023. Prafulla Dhariwal and Alexander Nichol, “Diffusion models beat gans on image synthesis,” Adv. Neural Inf. Process. Syst., 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising diffusion probabilistic models,’ Adv. Neural Inf. Process. Syst., 2020. Diederik Kingma, Tim Salimans, et al., “Variational diffusion models,” Adv. Neural Inf. Process. Syst., 2021. Rongjie Huang, Max W Y Lam, et al., “FastDiff: A fast conditional diffusion model for High-Quality speech synthesis,” arXiv, 2022. Sungwon Kim, Heeseung Kim, and Sungroh Yoon, “GuidedTTS 2: A diffusion model for high-quality adaptive Text-toSpeech with untranscribed data,” arXiv, 2022. Kai Shen, Zeqian Ju, et al., “NaturalSpeech 2: Latent diffusion models are natural and Zero-Shot speech and singing synthesizers,” arXiv, 2023. Rongjie Huang, Jiawei Huang, et al., “Make-An-Audio: TextTo-Audio generation with Prompt-Enhanced diffusion models,” arXiv, 2023. Flavio Schneider, Zhijing Jin, and Bernhard Schélkopf, “Moiisai: Text-to-Music generation with Long-Context latent diffusion,’ arXiv, 2023. Ashish Vaswani, Noam Shazeer, et al., “Attention is all you need,” Adv. Neural Inf. Process. Syst., 2017. Zalan Borsos, Raphaél Marinier, et al., “AudioLM: A language modeling approach to audio generation,” JEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.Ewan Dunbar, Mathieu Bernard, et al., “The zero resource speech challenge 2021: Spoken language modelling,” arXiv, 2021. Kushal Lakhotia, Eugene Kharitonov, et al., “On generative spoken language modeling from raw audio,” Transactions of the Association for Computational Linguistics, 2021. Neil Zeghidour, Alejandro Luebs, et al., “SoundStream: An End-to-End neural audio codec,” [IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022. Alexandre Défossez, Jade Copet, et al., “High fidelity neural audio compression,” arXiv, 2022. Colin Raffel, Noam Shazeer, et al., “Exploring the limits of transfer learning with a unified Text-to-Text transformer,” arXiv, 2019. Jonathan Ho and Tim Salimans, guidance,” arXiv, 2022. Shawn Hershey, Sourish Chaudhuri, et al., “CNN architectures for large-scale audio classification,” in JCASSP, 2017. “Classifier-Free diffusion Khaled Koutini, Jan Schliiter, et al., “Efficient training of audio transformers with patchout,” arXiv, 2021. Benjamin Elizalde, Soham Deshmukh, Mahmoud AI Ismail, and Huaming Wang, “CLAP: Learning audio concepts from natural language supervision,” arXiv, 2022. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, “High fidelity neural audio compression,” arXiv, 2022. Jort FGemmeke, Daniel P W Ellis, et al., “Audio set: An ontology and human-labeled dataset for audio events,” in ICASSP, 2017. Chris Dongjoo Kim, Byeongchang Kim, et al., “AudioCaps: Generating captions for audios in the wild,” in NAACL, 2019. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen, “Clotho: an audio captioning dataset,” in ICASSP, 2020. Honglie Chen, Weidi Xie, et al., “Vggsound: A Large-Scale Audio-Visual dataset,” in ICASSP, 2020. Eduardo Fonseca, Xavier Favory, et al., “FSDSOK: An open dataset of Human-Labeled sound events,’ IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022. Tri Dao, Daniel Y Fu, et al., “FlashAttention: Fast and memory-efficient exact attention with IO-awareness,” arXiv, 2022. Benjamin Lefaudeux, Francisco Massa, et al., “xformers: A modular and hackable transformer modelling library,” 2021. Ilya Loshchilov and Frank Hutter, “Decoupled weight decay regularization,” arXiv, 2017. Angela Fan, Mike Lewis, and Yann Dauphin, “Hierarchical neural story generation,” arXiv, 2018. Qingqing Huang, Daniel S Park, et al., “Noise2Music: Textconditioned music generation with diffusion models,” arXiv, 2023.
"	"--- ABSTRACT ---
이 논문은 모델 학습 중에 오디오와 텍스트 표현 간의 정렬을 강조하여 오디오 생성에 대한 제어를 강화하는 혁신적인 접근 방식을 제시합니다. 언어 모델 기반 오디오 생성의 맥락에서 이 모델은 텍스트 및 오디오 토큰 표현 모두의 입력을 활용하여 후속 오디오 토큰을 예측합니다. 그러나 현재 구성에는 선택한 텍스트 표현과 언어 모델의 예측 간의 정렬을 보장하기 위한 명시적 정규화가 없습니다. 제안에는 오디오 및 텍스트 표현 정규화를 통합하는 것이 포함되며, 특히 언어 모델 학습 중에 텍스트 조건이 교차 주의에서 제외되는 분류자 없는 안내(CFG) 단계에서 그렇습니다. 제안된 표현 정규화의 목적은 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사성의 불일치를 최소화하는 것입니다. 음악 및 오디오 생성 작업 모두에 대한 실험 결과는 제안된 방법이 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선과 오디오 생성에 대한 인간의 지각의 향상으로 이어진다는 것을 보여줍니다. 색인 용어 오디오 생성, 음악 생성, 표현 정규화 1.
--- INTRODUCTION ---
특정 요구 사항을 충족시키기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발 및 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다. 최근 신경 생성 모델의 출현은 디지털 콘텐츠 생성 환경에 혁신적인 변화를 가져왔습니다. 이미지 생성의 놀라운 발전에서 영감을 얻어[1, 2], 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델로 전환되는 패러다임 변화를 겪었습니다[3, 4, 5, 6, 7, 8, 9, 10]. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 영역에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께, 오디오 생성 작업에서도 뛰어난 성능을 보인 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 고품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 그런 다음 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 개별 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대용량 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위한 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 훈련됩니다. 그러나 전체 훈련 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징입니다.&quot;라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 탁구공이 튀는 소리가 여러 개 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 캡처하기 위해 생성 모델의 훈련을 개선하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화하여 달성됩니다. 언어 모델 훈련은 텍스트 조건 훈련과 분류자 없는 안내(CFG) 훈련의 두 가지 모드로 구성됩니다[26, 6]. CFG에서 텍스트 조건은 언어 모델 훈련 중에 생략됩니다. 동일한 훈련 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사도의 불일치를 줄임으로써 오디오 및 텍스트 표현 유사도를 향상시킵니다. 음악 및 음향 효과 생성의 실험 결과는 제안된 접근 방식의 효과를 보여주며, VGG 분류기를 사용한 Frechet 오디오 거리(FAD)[27], PaSST 모델을 사용한 kullback-leibler(KL) 발산[28], 대조 언어 오디오 사전 훈련 모델(CLAP)[29]을 기반으로 한 텍스트 및 오디오 정렬 점수, 오디오 생성을 위한 인간의 주관적 평가에서 개선이 나타났습니다.
--- RELATED WORK ---
이 연구는 [20, 21, 22, 8, 6, 7]과 같은 연구에 제시된 언어 모델 접근 방식을 적용하는데, 여기서 압축 모델은 오디오를 토큰으로 분리하여 학습한 다음 이 토큰을 오디오로 디코딩합니다. 언어 모델은 오디오 토큰을 생성하는 방법을 학습합니다. 그러나 우리의 강조점은 제공된 텍스트 설명과 생성된 오디오 간의 의미적 상관 관계를 증강하는 데 있습니다. 이 향상은 언어 모델 기반 오디오 생성을 위한 MusicGen [8] 및 AudioGen [6]의 기초 위에 구축되었습니다. 텍스트와 오디오 간의 표현 유사성을 모델링하기 위해 관련 작업 중 하나는 대조 손실을 사용하는 CLAP [29]입니다. 그러나 우리는 생성 모델 학습을 위해 CLAP에서 대조 손실을 사용해도 성능이 향상되지 않는다는 것을 발견했습니다. 대신, 우리는 먼저 서로 다른 샘플 간의 오디오와 텍스트의 표현 유사성을 계산하는 새로운 접근 방식을 제안합니다. 그런 다음 오디오의 유사성과 텍스트의 유사성 간의 불일치를 최소화합니다. 또한, 개별 시간 단계 출력에서 시퀀스 수준 표현을 얻는 데 있어 최대 풀링이 평균 풀링보다 더 나은 것을 발견했습니다.3. 표현 정규화 텍스트 입력 텍스트 인코더 텍스트 표현 표현 정규화 각 임베딩 테이블은 하나의 요리책에 해당합니다.변압기 스택 오디오 표현 임베딩 테이블 Encodec 오디오 토큰 한 단계 이동 선형 투영 선형 투영 선형 투영 선형 투영 각 선형 투영은 하나의 요리책에 해당합니다.CE 손실 그림 1. 교차 엔트로피 손실과 표현 정규화를 사용한 언어 모델 학습 그림.3.1. 언어 모델 기반 오디오 생성 언어 모델 기반 오디오 생성 모델은 그림 1에 표시된 것처럼 여러 핵심 요소로 구성됩니다.첫째, EnCodec 모델[30, 23]과 같은 압축 모델을 사용하여 원시 오디오 데이터를 토큰 ak,i의 개별 다중 스트림 시퀀스로 인코딩합니다. 여기서 i Є [1, Ta]이고 Ta는 오디오 토큰 시퀀스의 길이이고, k = [1, K]는 k번째로 인덱싱된 특정 코드북을 나타냅니다. 또한, 이 모델은 사전 훈련된 텍스트 인코더를 통합하여 텍스트 입력을 v;로 식별된 임베딩 표현의 시퀀스로 변환합니다. 여기서 j Є [1,T₂], T₁는 텍스트 임베딩 표현을 포함하는 시퀀스의 길이에 해당합니다. 마지막으로, Transformer 계층의 스택인 언어 모델 구성 요소가 있습니다. 언어 모델은 텍스트 임베딩 표현과 이전 오디오 토큰을 모두 활용하여 후속 오디오 토큰에 대한 확률 분포를 po(ak,i+1|ak,1, ..., ak,i, V1, ..., VT)로 생성합니다. 오디오 생성을 보다 관리하기 쉽게 만들기 위해 다중 스트림 오디오 토큰의 생성은 병렬로 훈련되어 모델 훈련 중에 효과적인 시퀀스 길이가 상당히 줄어듭니다. 언어 모델의 손실은 각 스트림 k에 대한 교차 엔트로피 손실의 합계입니다. Lcond K Ta -Σ log(pe (ak,i+1|ak,1, ..., ak,i, V1, ..., UT₁)) (1) k=1 i=3.2. 표현 정규화 그러나 언어 모델의 교차 엔트로피 손실은 오디오 토큰 예측이 제공된 텍스트 조건과 일치하도록 강제하는 명시적 메커니즘이 부족합니다. 더욱이 텍스트와 오디오 간의 상관 관계는 분류자 없는 안내(CFG)로 인해 더욱 느슨해집니다.
--- METHOD ---
s는 오디오 및 음악 생성을 위한 객관적인 지표의 개선과 오디오 생성을 위한 인간의 지각의 향상으로 이어진다. 색인 용어 오디오 생성, 음악 생성, 표현 정규화 1. 서론 특정 요구 사항을 충족하기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발, 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다. 최근 신경 생성 모델의 출현으로 디지털 콘텐츠 생성 환경에 획기적인 변화가 일어났습니다. 이미지 생성의 놀라운 발전[1, 2]에서 영감을 얻어 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델[3, 4, 5, 6, 7, 8, 9, 10]로 전환되는 패러다임 변화를 겪었습니다. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 분야에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었으며, 이는 오디오 생성 작업에서도 뛰어난 성능을 보였습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서는 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 높은 품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 이산 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대규모 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위해 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 학습됩니다. 그러나 전체 학습 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징&quot;이라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 여러 개의 탁구공이 튀는 소리가 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 포착하기 위해 생성 모델의 학습을 개선하는 것을 목표로 하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화함으로써 달성됩니다. 언어 모델 학습은 텍스트 조건 학습과 분류자 없는 안내(CFG) 학습의 두 가지 모드로 구성됩니다[26, 6]. CFG에서는 언어 모델 학습 중에 텍스트 조건이 생략됩니다. 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오와 텍스트 유사성의 불일치를 줄임으로써 오디오와 텍스트 표현 유사성을 향상시킵니다.
--- EXPERIMENT ---
음악 및 오디오 생성 작업 모두에 대한 모든 결과는 제안된 방법이 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선과 오디오 생성에 대한 인간의 지각의 향상으로 이어진다는 것을 보여줍니다.색인 용어 오디오 생성, 음악 생성, 표현 정규화 1. 서론 특정 요구 사항을 충족하기 위해 음향 효과, 음악 및 음성을 생성하는 것은 증강, 가상 및 혼합 현실, 비디오 게임 개발, 영화 제작을 포함한 다양한 도메인에 걸쳐 콘텐츠 제작의 핵심 도구로서 엄청난 중요성을 갖습니다.최근 신경 생성 모델의 출현으로 디지털 콘텐츠 생성 환경에 획기적인 변화가 일어났습니다.이미지 생성의 놀라운 발전[1, 2]에서 영감을 얻어 오디오 생성 영역은 기존 신호 처리 접근 방식에서 신경 생성 모델[3, 4, 5, 6, 7, 8, 9, 10]로 전환되는 패러다임 변화를 겪었습니다. 텍스트-이미지 생성 모델[1, 11]의 경우와 마찬가지로 확산 확률 모델[12, 13]의 잠재력을 활용한 연구[9, 14, 15, 16, 4, 5, 17, 18]는 음성 합성, 음향 효과 생성 및 음악 생성 분야에서 인상적인 역량을 보여주었습니다. 확산 기반 접근 방식과 함께 변환기 기반 언어 모델[19]을 사용하는 병행 경로가 추구되었으며, 이는 오디오 생성 작업에서도 뛰어난 성능을 보였습니다[20, 21, 22, 8, 6, 7]. MusicGen[8] 및 AudioGen[6]과 같은 언어 모델 기반 접근 방식에서는 먼저 신경 오디오 압축 모델(예: [23, 24])을 통해 원시 오디오를 개별 토큰으로 인코딩합니다. 이 모델은 높은 품질과 최소 지각 손실로 개별 토큰에서 입력 오디오를 압축하고 재구성하도록 종단 간 학습됩니다. 생성 모델은 자동 회귀 변환기-디코더 언어 모델을 사용합니다. 언어 모델은 첫 번째 단계의 이산 오디오 토큰에서 작동하며 텍스트 입력에 따라 조건지어집니다. 텍스트는 T5[25]와 같은 대규모 텍스트 코퍼스에서 사전 학습된 텍스트 인코더를 사용하여 텍스트 임베딩 표현으로 처리됩니다. 텍스트 표현은 언어 모델 학습에서 교차 어텐션으로 사용됩니다. 언어 모델은 이전 오디오 토큰과 텍스트 표현을 기반으로 다음 이산 오디오 토큰을 예측하기 위해 엔트로피를 최소화하기 위해 교차 엔트로피 손실로 학습됩니다. 그러나 전체 학습 프로세스에서 오디오 토큰과 조건 텍스트의 표현을 모두 최대한 활용하기 위해 다음 오디오 토큰 예측을 강제하는 정규화가 없습니다. 결과적으로 생성된 오디오는 제공된 텍스트 프롬프트와 완전히 일치하지 않는 경우가 많습니다. &quot;경이로움과 경외감을 보여주는 매우 리듬감 있는 오케스트라 작품. 스타카토 바이올린, 첼로, 베이스, 트롬본, 그랜드 피아노가 특징&quot;이라는 설명을 기반으로 생성된 음악은 설명에서 하나 이상의 악기를 놓치는 경우가 많습니다. &quot;단단한 나무 바닥에서 탁구공이 한 번 튀어오르는 소리&quot;라는 조건에서 생성된 음향 효과에는 여러 개의 탁구공이 튀는 소리가 있습니다. 이 논문에서는 텍스트 조건에서 표현을 효과적으로 포착하기 위해 생성 모델의 학습을 개선하는 것을 목표로 하는 방법을 소개합니다. 이는 정규화를 통해 텍스트와 오디오 표현 간의 유사성을 최소화함으로써 달성됩니다. 언어 모델 학습은 텍스트 조건 학습과 분류자 없는 안내(CFG) 학습의 두 가지 모드로 구성됩니다[26, 6]. CFG에서는 언어 모델 학습 중에 텍스트 조건이 생략됩니다. 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오와 텍스트 유사성의 불일치를 줄임으로써 오디오와 텍스트 표현 유사성을 향상시킵니다. 음악 및 음향 효과 생성의 실험 결과는 제안된 접근 방식의 효과를 보여주며, VGG 분류기[27]를 사용한 Frechet 오디오 거리(FAD), PaSST 모델을 사용한 kullback-leibler(KL) 발산[28], 대조 언어 오디오 사전 학습 모델(CLAP)[29]을 기반으로 한 텍스트 및 오디오 정렬 점수, 오디오 생성을 위한 인간의 주관적 평가에서 개선이 나타났습니다. 2. 관련 연구 이 연구는 [20, 21, 22, 8, 6, 7]과 같은 연구에 제시된 언어 모델 접근 방식을 적용합니다. 여기서 압축 모델은 오디오를 토큰으로 분리하여 학습한 다음 이러한 토큰을 오디오로 디코딩합니다. 언어 모델은 오디오 토큰을 생성하는 방법을 학습합니다. 그러나 우리의 강조점은 제공된 텍스트 설명과 생성된 오디오 간의 의미적 상관 관계를 증강하는 데 있습니다. 이 향상은 언어 모델 기반 오디오 생성을 위한 MusicGen[8] 및 AudioGen[6]의 기초 위에 구축되었습니다. 텍스트와 오디오 간의 표현 유사성을 모델링하기 위해 관련 작업 중 하나는 대조 손실을 사용하는 CLAP[29]입니다. 그러나 생성 모델 학습을 위해 CLAP에서 대조 손실을 사용해도 성능이 향상되지 않는다는 것을 발견했습니다. 대신, 서로 다른 샘플 간의 오디오와 텍스트의 표현 유사성을 먼저 계산하는 새로운 접근 방식을 제안합니다. 그런 다음 오디오의 유사성과 텍스트의 유사성 간의 불일치를 최소화합니다. 또한 개별 시간 단계 출력에서 시퀀스 수준 표현을 얻는 데 최대 풀링이 평균 풀링보다 더 나은 것을 발견했습니다. 3. 표현 정규화 텍스트 입력 텍스트 인코더 텍스트 표현 표현 정규화 각 임베딩 테이블은 하나의 요리책에 해당합니다. 변압기 스택 오디오 표현 임베딩 테이블 인코딩 오디오 토큰 한 단계 이동 선형 투영 선형 투영 선형 투영 선형 투영 각 선형 투영은 하나의 요리책에 해당합니다. CE 손실 그림 1. 교차 엔트로피 손실과 표현 정규화를 사용한 언어 모델 학습의 예. 3.1. 언어 모델 기반 오디오 생성 언어 모델 기반 오디오 생성 모델은 그림 1에 표시된 대로 몇 가지 핵심 요소로 구성됩니다. 첫째, EnCodec 모델[30, 23]과 같은 압축 모델을 사용하여 원시 오디오 데이터를 토큰 ak,i의 개별 다중 스트림 시퀀스로 인코딩합니다. 여기서 i Є [1, Ta]이고 Ta는 오디오 토큰 시퀀스의 길이이고 k = [1, K]는 k번째로 색인된 특정 코드북을 나타냅니다. 또한 이 모델은 사전 학습된 텍스트 인코더를 통합하여 텍스트 입력을 v;로 식별된 임베딩 표현의 시퀀스로 변환합니다. 여기서 j Є [1,T₂], T₁는 텍스트 임베딩 표현이 포함된 시퀀스의 길이에 해당합니다. 마지막으로 Transformer 계층의 스택인 언어 모델 구성 요소가 있습니다. 언어 모델은 텍스트 임베딩 표현과 이전 오디오 토큰을 모두 활용하여 후속 오디오 토큰에 대한 확률 분포를 po(ak,i+1|ak,1, ..., ak,i, V1, ..., VT)로 생성합니다. 오디오 생성을 보다 관리하기 쉽게 하기 위해 다중 스트림 오디오 토큰 생성은 병렬로 학습되어 모델 학습 중에 효과적인 시퀀스 길이가 상당히 줄어듭니다. 언어 모델의 손실은 각 스트림 k에 대한 교차 엔트로피 손실의 합입니다. Lcond K Ta -Σ log(pe (ak,i+1|ak,1, ..., ak,i, V1, ..., UT₁)) (1) k=1 i=3.2. 표현 정규화 그러나 언어 모델의 교차 엔트로피 손실에는 오디오 토큰 예측이 제공된 텍스트 조건과 일치하도록 강제하는 명시적 메커니즘이 없습니다. 또한, 샘플 품질과 다양성 간의 균형을 조절하기 위해 훈련에 분류자 없는 안내(CFG) 방법[26, 6, 8]을 사용함에 따라 텍스트와 오디오 간의 상관관계가 더욱 느슨해집니다.CFG를 사용하면 언어 모델을 조건부 및 무조건부 모두로 훈련할 수 있습니다.AudioGen[6]과 유사하게, 언어 모델 훈련 중에 훈련 샘플의 10%에 수반되는 텍스트가 생략됩니다.무조건부 상황에서 손실은 단순히 Luncond K Ta -Σ log(pe (ak,i+1|Ak,1, ..., Ak,i)) k=1 i=입니다.이 작업에서 제안된 표현 정규화는 CFG 방법이 텍스트에 대해 무조건적으로 언어 모델을 훈련하는 효과를 유지하면서 오디오 표현과 텍스트 표현 간의 상관관계를 강화합니다.훈련 샘플 배치가 주어지면 풀링 방법 F를 사용하여 배치의 특정 샘플 b에 대해 텍스트 시퀀스 표현을 T₁ = F(v₁₁, ..., v)로, 오디오 시퀀스 표현을 A³ = F(u), ..., u)로 얻습니다. 실험에서 최대 풀링이 가장 좋은 결과를 얻었습니다.텍스트와 오디오 표현을 동일한 공간에 직접 매핑하고 CLAP[29]처럼 오디오와 텍스트 간의 유사성을 최대화하는 대신, 다음과 같이 동일한 학습 배치 내의 다른 샘플과 비교하여 오디오 및 텍스트 유사성의 불일치를 최소화하도록 제안합니다.ты, б = ть жто ||Tb ||||T|| (3) Ab✶ A¹ ||4b|||| A6 || Lrr Eo:=ô(TB, A,B*(B − 1) (4) (5) 여기서 Tb는 샘플 b와 6의 텍스트 입력 간의 표현 유사도를 나타냅니다. 그리고 Ab, b는 샘플 b와 ô의 오디오 간의 표현 유사도를 나타냅니다. B는 배치 크기입니다. Lrr은 한 샘플의 텍스트와 오디오가 다른 샘플과 관련하여 동일한 차이를 갖도록 합니다. 이 연구에서 제안된 표현 정규화는 CFG 단계에서만 적용됩니다. 완전한 모델 학습 손실은 다음과 같이 정의됩니다. 여기서 L = Luncond + XLrr CFG를 사용하는 경우 CFG를 사용하지 않는 경우 Lcond (6)은 표현 정규화에 대한 가중치를 나타냅니다. 표현 정규화는 CFG가 사용 중일 때 일반적인 학습 단계에서만 사용됩니다. 또한 비 CFG 시나리오에서 표현 정규화를 포함하는 실험을 수행했지만 이러한 실험은 객관적 지표에서 개선을 가져오지 못했습니다. 표현 정규화가 텍스트 표현을 복사하여 언어 모델 학습을 방해할 가능성이 있기 때문에 저하가 발생할 수 있다고 생각합니다. 영어: non-CFG에서 오디오 표현으로서의 교차 주의.4. 실험 이 작업에서 우리는 음향 효과 생성과 음악 생성을 포함하는 두 세트의 실험을 사용하여 제안된 방법의 효과를 검증한다.4.1. 데이터 세트 음악 생성에서 우리는 총 20,000시간의 라이선스 음악을 활용하는데, 이는 고품질의 10,000개 음악 트랙과 ShutterStock¹과 Pond52의 390,000개 악기 전용 음악 트랙의 내부 컴파일로 구성된다.모든 데이터 세트는 32kHz 샘플링 속도의 전체 길이의 음악이며, 텍스트 설명, 장르 분류, BPM, 태그와 같은 포괄적인 메타데이터가 함께 제공된다.우리의 평가는 MusicCaps 벤치마크[7]를 사용한다.MusicCaps 벤치마크는 다양한 장르에 걸쳐 균형을 이룬 1,000개 샘플의 하위 세트를 포함하여 5.5,000개 샘플로 구성된다.우리는 [8]과 같이 불균형한 하위 세트에 대한 객관적인 지표를 보고한다.사운드 효과 모델 학습을 위해 4,000시간의 학습 데이터를 포함하는 데이터 세트가 사용된다. 이 데이터 세트 1www.shutterstock.com/music 2www.pond5.com에는 AudioSet [31], BBC sound effects³, AudioCaps[32], Clotho v2 [33], VGG-Sound [34], FSD50K [35] 및 Free To Use Sounds 4와 같은 리소스가 통합되어 있습니다. 모든 오디오 파일은 16kHz의 속도로 샘플링됩니다. 텍스트 설명에 대해 [6]과 유사한 전처리 방법론을 채택합니다. 시작으로 AudioSet, VGG-Sound, FSD50K와 같은 데이터 세트의 다중 레이블 주석을 활용합니다. 오디오 샘플과 연결된 태그 목록을 연결하여 가상 문장을 구성합니다. 그런 다음 불용어와 숫자를 제거하고 AudioCaps, Clotho v2, Free To Use Sounds 및 BBC Sound Effects를 포함한 데이터 세트에서 사용 가능한 자연어 캡션을 레마타이징합니다. 마지막으로, 태그나 캡션에 &quot;음성&quot;이라는 용어가 포함된 샘플은 음성이 데이터에서 우세하다는 점을 고려하여 필터링합니다.4.2. 설정 우리의 접근 방식에는 음악 생성에 맞게 조정된 비인과적 5계층 EnCodec 모델이 포함되며, 모노포닉 음악의 경우 32kHz, 사운드 효과 생성의 경우 16kHz에서 작동합니다.이러한 EnCodec 모델은 초기 숨겨진 크기 64에서 시작하여 50Hz의 프레임 속도를 유지하며, 이는 모델의 5개 계층에서 두 배가 됩니다.임베딩은 각각 2048의 코드북 크기를 특징으로 하는 4개의 양자화기로 구성된 RVQ를 사용하여 양자화됩니다.이러한 EnCodec 모델은 언어 모델 학습과 동일한 오디오 데이터를 사용하여 학습됩니다.이 작업에서 사용된 변환기 모델은 300M 매개변수를 갖습니다.긴 시퀀스의 효율성을 높이기 위해 xFormers 패키지[37]의 메모리 효율적인 Flash Attention[36]을 사용하여 속도와 메모리 활용도를 모두 개선합니다.절제의 경우 사운드 효과 생성 모델 설정을 일관되게 사용합니다. 음악 생성 모델 학습을 위해 전체 트랙에서 무작위로 샘플링한 30초 오디오 세그먼트를 사용합니다.사운드 효과 생성 학습에서는 10초 오디오 클립을 사용합니다.모델 학습은 AdamW 최적화 도구[38], 192개 예제의 배치 크기, B1 0.9, 62 = 0.95, 분리된 가중치 감소 0.1, 그래디언트 클리핑 1.0을 활용하여 100K 단계에 걸쳐 진행됩니다.4k 단계의 워밍업과 함께 코사인 학습률 일정이 사용됩니다.또한 감소 계수 0.99가 특징인 지수 이동 평균이 적용됩니다.모델 학습은 Fully Sharded Data Parallel(FSDP) bfloat16을 사용한 혼합 정밀도를 사용합니다.우리는 사운드 효과 생성과 음악 생성 학습에 각각 16개의 GPU와 32개의 GPU를 사용했습니다.추론을 위한 샘플링 프로세스에서 우리는 상위 250개 토큰을 유지하고 온도 1.0을 적용하는 상위 k 샘플링[39]을 채택합니다. = 4.3. Ablation Study = 표 1은 AudioCaps 데이터 세트를 사용하여 음향 효과 생성 모델에서 수행한 Ablation Study 결과를 보여줍니다. 최적 모델은 최대 풀링에 기반한 표현 3 https://sound-effects.bbcrewind.co.uk/ 4https://www.freetousesounds.com/all-in-one-bundle/ 정규화로 학습되었으며, 가중치 매개변수는 = 3.0이고 학습 데이터의 10%를 CFG 학습에 할당했습니다. 반면, 평균 풀링 기반 시퀀스 표현 정규화를 사용한 경우 기준선에 비해 개선이 나타나지 않았습니다. 나아가 표 1은 CFG 학습이 FAD와 KL 점수를 모두 줄이는 데 중요한 역할을 한다는 것을 재확인합니다. FAD, KL 및 CLAP에서 CFG 入 FAD(↓) KL(↓) CLAP(†) 모델 성능. AudioGen의 결과는 github³을 참조합니다. FAD(↓) KL(↓) CLAP(↑) 방법 AudioGen [6] 1.1.0.ours w/o rr ours w/ rr 1.1.0.1.1.0.pool max 0.11.1.0.max 0.11.1.0.max 0.11.1.0.max 0.11.1.0.0.1.1.0.0.1.1.0.0.1.0.max 0.1.1.0.average 0.11.1.0.표 1. AudioCaps 기반 음향 효과 생성을 사용한 Ablation 연구. 열 &#39;pool&#39;은 오디오와 텍스트 표현에 대한 시퀀스 수준 표현을 구하기 위한 풀링 방법을 나타냅니다. 열 &#39;CFG&#39;는 학습에서 CFG를 사용하는 비율을 나타냅니다. 열 &#39;X&#39;는 표현 정규화에 사용된 가중치를 나타냅니다. 4.4. 음악 생성 표 2는 MusicCaps 데이터에 대한 객관적인 지표를 나타냅니다. 우리는 멜로디가 없는 MuiscLM, Noise2Music 및 MusicGen 1.5B 모델에 대한 원래 메트릭을 보고합니다. 특히, 제안된 표현 정규화를 도입하면 모든 메트릭에서 향상이 이루어집니다. 표현 정규화를 통합한 300M 매개변수 모델은 FAD 및 CLAP 측면에서 MusicGen 1.5B 매개변수 모델의 성능을 능가합니다. FAD(↓) KL(↓) CLAP(†) 방법 MusicLM [7] 4.Noise2Music[40] 2.MusicGen 1.5B[8] 5.1.0.ours 300M w/o rr 5.1.0.ours 300M w/ rr 4.1.0.표 2. MusicCaps를 사용한 음악 생성. &#39;w/ rr&#39; 및 &#39;w/o rr&#39;은 각각 표현 정규화가 있는 경우와 없는 경우를 의미합니다. 4.5. 음향 효과 생성 AudioCaps의 음향 효과 생성 결과는 표 3에 나와 있습니다. 추세는 음악 생성 실험과 동일합니다. 표현 정규화는 표 3. AudioCaps를 사용한 음향 효과 생성을 개선합니다. &#39;w/ rr&#39; 및 &#39;w/o rr&#39;은 각각 표현 정규화가 있는 경우와 없는 경우를 의미합니다. 4.6. 인간의 선호도 평가 표 4는 사운드 및 음악 생성 모델에 대한 주관적 지표를 제공합니다. 주관적 평가는 맹검 쌍대 비교 테스트를 사용했으며, 평가자에게 동일한 텍스트 프롬프트를 기반으로 하는 서로 다른 모델에서 생성한 두 샘플을 제공했습니다. 이 비교는 20개의 텍스트 프롬프트에 걸쳐 수행되었으며, 8명의 인간 평가자는 각 쌍에서 제공된 프롬프트와 더 나은 품질과 더 나은 일치를 보인다고 생각되는 샘플에 대한 선호도를 결정하는 작업을 맡았습니다. 주목할 점은 표현 정규화를 통합했을 때 음악 및 음향 효과 생성 모두에서 사용자 선호도 평가가 더 높았습니다. 음향 효과 생성에서 더 중요한 추세에 대한 가능한 설명은 음악이 음향 효과보다 더 추상적인 경향이 있기 때문입니다. 결과적으로 제공된 텍스트와 일치하는 불일치 사항은 인간 평가자에게 쉽게 드러나지 않을 수 있습니다. 방법 음악 음향 효과 우리의 w/o rr 우리의 w/ rr 48% 52% 33% 67% 표 4. 인간의 선호도 평가 5.
--- CONCLUSION ---
이 논문은 모델 학습 중에 오디오와 텍스트 표현 간의 정렬을 우선시하여 오디오 생성에 대한 제어성을 개선하기 위해 표현 정규화를 도입했습니다. 제안된 방법은 특히 분류자 없는 안내(CFG) 단계에서 오디오와 텍스트 유사성 정규화를 통합했으며, 여기서 텍스트 조건은 언어 모델 학습 중에 교차 주의에서 제외됩니다. 다양한 오디오 및 음악 생성 작업에서 수행된 실험 결과는 제안된 표현 정규화가 오디오 및 음악 생성 모두에 대한 객관적 지표의 개선으로 이어졌음을 보여줍니다. 게다가 이러한 개선은 오디오 생성 품질과 정렬에 대한 인간의 지각에서 눈에 띄는 향상으로 이어졌습니다. 5https://github.com/facebookresearch/audiocraft/blob/main/model_cards 6. 참고문헌 [1] Robin Rombach, Andreas Blattmann 외, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, CVPR, 2022. [2] Aditya Ramesh, Prafulla Dhariwal 외, &quot;CLIP 잠재 모델을 사용한 계층적 텍스트 조건부 이미지 생성&quot;, arXiv, 2022. [3] Yang Song, Jascha Sohl-Dickstein 외, &quot;확률적 미분 방정식을 통한 점수 기반 생성 모델링&quot;, arXiv, 2020. [4] Haohe Liu, Qiao Tian 외, &quot;AudioLDM 2: 자기 감독 사전 학습을 통한 전체적인 오디오 생성 학습&quot;, arXiv, 2023년 8월. [5] Haohe Liu, Zehua Chen 등, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; arXiv, 2023. [6] Felix Kreuk, Gabriel Synnaeve 등, &quot;AudioGen: 텍스트로 안내되는 오디오 생성,&quot; arXiv, 2022. [7] Andrea Agostinelli, Timo I Denk 등, &quot;MusicLM: 텍스트에서 음악 생성,&quot; arXiv, 2023. [8] Jade Copet, Felix Kreuk 등, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv, 2023. [9] Matthew Le, Apoorv Vyas 등, &quot;Voicebox: 대규모 텍스트 안내 다국어 범용 음성 생성,&quot; arXiv, 2023. [10] Max WY Lam, Qiao Tian 등, 영어: “효율적인 신경 음악 생성,&quot; arXiv, 2023. [11] Prafulla Dhariwal 및 Alexander Nichol, “확산 모델이 이미지 합성에서 간을 이긴다,&quot; Adv. Neural Inf. Process. Syst., 2021. [12] Jonathan Ho, Ajay Jain 및 Pieter Abbeel, “잡음 제거 확산 확률적 모델,&quot; Adv. Neural Inf. Process. Syst., 2020. [13] Diederik Kingma, Tim Salimans 외, “변형 확산 모델,&quot; Adv. Neural Inf. Process. Syst., 2021. [14] Rongjie Huang, Max WY Lam, et al., “FastDiff: 고품질 음성 합성을 위한 빠른 조건부 확산 모델,&quot; arXiv, 2022. [15] Sungwon Kim, Heeseung Kim, and Sungroh Yoon, “GuidedTTS 2: 비전사 데이터를 사용한 고품질 적응형 Text-to-Speech를 위한 확산 모델,&quot; arXiv, 2022. [16] Kai Shen, Zeqian Ju, et al., “NaturalSpeech 2: 잠재 확산 모델은 자연스러운 Zero-Shot 음성 및 노래 합성기,&quot; arXiv, 2023. [17] Rongjie Huang, Jiawei Huang, et al., “Make-An-Audio: Prompt-Enhanced 확산 모델을 사용한 TextTo-Audio 생성,&quot; arXiv, 2023. [18] Flavio Schneider, Zhijing Jin 및 Bernhard Schölkopf, &quot;Moûsai: Long-Context 잠재 확산을 통한 텍스트-음악 생성,&quot; arXiv, 2023. [19] Ashish Vaswani, Noam Shazeer 외, &quot;주의만 있으면 됩니다.&quot; Adv. Neural Inf. Process. Syst., 2017. [20] Zalán Borsos, Raphaël Marinier 외, &quot;AudioLM: 오디오 생성을 위한 언어 모델링 접근법&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2023. [21] Ewan Dunbar, Mathieu Bernard 외, &quot;제로 리소스 음성 챌린지 2021: 구어체 모델링&quot;, arXiv, 2021. [22] Kushal Lakhotia, Eugene Kharitonov 외, &quot;원시 오디오에서 생성 구어체 모델링에 관하여&quot;, 계산 언어학 협회 저널, 2021. [23] Neil Zeghidour, Alejandro Luebs 외, &quot;SoundStream: 종단 간 신경 오디오 코덱&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2022. [24] Alexandre Défossez, Jade Copet 외, &quot;고충실도 신경 오디오 압축&quot;, arXiv, 2022. [25] Colin Raffel, Noam Shazeer 외, &quot;통합 텍스트 대 텍스트 변환기를 사용한 전이 학습의 한계 탐색&quot;, arXiv, 2019. [26] Jonathan Ho 및 Tim Salimans, 지침&quot;, arXiv, 2022. &quot;분류자 없는 확산 [27] Shawn Hershey, Sourish Chaudhuri 외, &quot;대규모 오디오 분류를 위한 CNN 아키텍처&quot;, ICASSP, 2017. [28] Khaled Koutini, Jan Schlüter 외, &quot;패치아웃을 사용한 오디오 변환기의 효율적인 학습&quot;, arXiv, 2021. [29] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail 및 Huaming Wang, “CLAP: 자연어 감독에서 오디오 개념 학습,” arXiv, 2022. [30] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi, “고충실도 신경 오디오 압축,” arXiv, 2022. [31] Jort F Gemmeke, Daniel PW Ellis, et al., “오디오 세트: 오디오 이벤트에 대한 온톨로지 및 인간 레이블 데이터 세트,” ICASSP, 2017. [32] Chris Dongjoo Kim, Byeongchang Kim, et al., “AudioCaps: 야생 오디오에 대한 캡션 생성,” NAACL, 2019. [33] Konstantinos Drossos, Samuel Lipping, Tuomas Virtanen, “Clotho: 오디오 캡션 데이터 세트,” ICASSP, 2020. [34] Honglie Chen, Weidi Xie, et al., “Vggsound: 대규모 오디오-비주얼 데이터 세트,&quot; ICASSP, 2020. [35] Eduardo Fonseca, Xavier Favory, et al., “FSD50K: 인간이 레이블을 지정한 사운드 이벤트의 오픈 데이터 세트,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2022. [36] Tri Dao, Daniel Y Fu, et al., &quot;FlashAttention: IO 인식을 통한 빠르고 메모리 효율적인 정확한 어텐션,&quot; arXiv, 2022. [37] Benjamin Lefaudeux, Francisco Massa, et al., “xformers: 모듈식 및 해킹 가능한 변압기 모델링 라이브러리,&quot; 2021. [38] Ilya Loshchilov 및 Frank Hutter, “분리된 가중치 감소 정규화,&quot; arXiv, 2017. [39] Angela Fan, Mike Lewis 및 Yann Dauphin, &quot;계층적 신경 스토리 세대,&quot; arXiv, 2018. [40] Qingqing Huang, Daniel S Park 외, &quot;Noise2Music: 확산 모델을 사용한 텍스트 조건 음악 생성,&quot; arXiv, 2023.
"
"--- ABSTRACT ---
of compressed discrete music representations (i.e. tokens In language modeling based music generation, a generated waveform is represented by a sequence of hierarchical token stacks that can be decoded either in an auto-regressive manner or in parallel, depending on the codebook patterns. In particular, flattening the codebooks represents the highest quality decoding strategy, while being notoriously slow. To this end, we propose a novel stack-and-delay style of decoding strategy to improve upon the flat pattern decoding where generation speed is four times faster as opposed to vanilla flat decoding. This brings the inference time close to that of the delay decoding strategy, and allows for faster inference on GPU for small batch sizes. For the same inference efficiency budget as the delay pattern, we show that the proposed approach performs better in objective evaluations, almost closing the gap with the flat pattern in terms of quality. The results are corroborated by subjective evaluations which show that samples generated by the new model are slightly more often preferred to samples generated by the competing model given the same text prompts. Index Terms— music generation, audio generation, efficient decoding, transformer decoder 1.
--- METHOD ---
can be referred to as parallel decoding while the latter is usually auto-regressive. The level of quality is getting closer to that of original songs, paving the road towards new commercial use cases such as personalized on-device music generation, where the batch size is typically small. However those models often come with a quality trade off: the higher the quality, the slower the generation and vice versa [3|{6]. During inference, the decoding strategy, hardware and model size influence the speed of the generation. [4] recently proposed a single-stage auto-regressive Transformer decoder that models sequences compute by an audio compression model [I1]). The authors explored several codebook patterns for the discrete tokens sequence modeling. In particular, they showed that the best performing pattern relies on flattening the token stack (which will be referred to as the flat pattern in the rest of the paper). Indeed each piece of generated waveform is actually represented by not only one token but several, corresponding to the number C of residual projections in the Residual Vector Quantizer (RVQ) module of the compression model. Flattening the token stack comes with the cost of generating (and training) for a C times longer sequence, which implies a significantly higher real-time-factor (RTF), making the model unusable in practice for interactive user experience. To overcome that issue, the proposed delay pattern [4] was shown to be a good trade off between speed and quality. In this paper we hypothesize that despite its efficiency, the delay pattern could affect the model ability to generate high quality samples by design. Starting from the stronger but slower flat pattern, we propose a new strategy called stackdelay that is able to generate music as fast as the original delay strategy, with significantly higher quality. The contributions of this paper are: * anew stack codebook pattern that inherits the quality of flat while being faster and memory efficient during inference by reducing the past key/value streaming cache footprint. * anew stack-delay pattern that: — benefits from the stack pattern strengths while being as fast as the delay pattern for generation. — produces higher quality music than delay, shown by objective and subjective evaluations. * an new decoding schedule that involves interleaving decoded positions that prevents the model from decoding adjacent positions until they have enough context. 2. STACK-DELAY CODEBOOK PATTERN 2.1. Music generation Given a text description, a sequence of text embeddings computed by the TS encoder serves as the conditioning signal --- --Ss S9 Sio Si1 Siz S13 $14 S15 $16 S17 Sigs Sig STACK-DELAY Fig. 1. Comparison of the proposed stack-delay pattern (right) with the delay (top left) and stack (bottom left). Under the stackdelay pattern the tokens are generated in a multi-stream fashion, in parallel. Time steps are decoded in a permuted manner. Only key/value embeddings from the top-level stream are stored in long-term streaming cache, which makes inference as efficient as delay while retaining better quality from stack pattern. for a Transformer decoder model (using cross attention). The model generates a sequence of EnCodec [II] token stacks {cit}! that are CNN-decoded into an audio waveform. i represents the token level while ¢ represents the time step in the generated sequence. In this paper we only consider the auto-regressive Transformer decoder architecture [9] that emits a probability distribution over the token space that is conditioned on the previously generated tokens (causal self attention in the Transformer decoder). During inference, the past self attention keys and values are stored in a streaming cache to optimize the generation time. Depending on the tokenizer framerate f (e.g. f = 50Hz), the duration of audio to generate d and the size of the token stack C' (e.g. C = 4), the model has to generate f x C x d tokens in a given amount of decoding steps that depend on the token codebook pattern and decoding schedule. The decoding schedule can be formalized as a function G(i, t) defining the decoding step for each ci. 2.2. Codebook patterns Contrary to the text domain, a segment of audio is not represented by a single token but by a stack of hierarchical tokens computed by quantizing the latent embedding of a CNN auto-encoder [I]. This usually means the lower the token in the stack, the more information it carries. To address the issue of predicting tokens in a hierarchical manner, several codebook interleaving patterns have been explored (15), with the common idea to decode the lowest level token first then handle the higher levels in further decoding steps, which is the case for both auto-regressive (AR) and non auto regressive (NAR) [6] decoding architectures. Namely the decoding schedule is constrained such that: G(0,t) < G(i,t), Vi € [1,C[ qd) 2.2.1. Delay Regarding music generation, the delay interleaving pattern (presented on the top left part of Figure[I} was shown to be a good compromise between quality and AR decoding step count. Under the delay pattern, the C’ codebook levels are predicted in parallel but with a shift of in the decoded time step. Namely G(i,t) = t + i. This means that each subsequent time step in the sequence starts to be decoded with only partial knowledge of the previous adjacent time step. For example, the prediction of co,, in decoding step s; in the Figure is only conditioned on co;,, previously decoded in so, but not on higher levels {c;}@! of time step to. 2.2.2. Stack [4] showed that to obtain the highest music quality, flattening the codebooks performed the best, at the expense of C' times more decoding steps. Gli,t) =Cxtt+i<CxT (2) This can be easily explained by the fact that subsequent decoded time steps benefit from the full context of the preceding ones. In such case the prediction of co,41 is effectively conditioned on cjo,c—1)(0,1]- The context length is C times bigger --- --pattern decoding steps | context length delay T T flat TxC TxC stack TxC T+C stack-delay T T Table 1. Required decoding step count and maximum context length of the streaming cache during inference, as a function of the sequence length to generate T = d x f and the number of codebook levels C. than delay since the at most C x T past Transformer self attention key/value representations are stored in the streaming cache during inference. To reduce the cache size we adapt the flat pattern by retaining and stacking the lower level tokens throughout the decoding process, as shown in Figure[I] Once a full stack has been decoded for a given time step, partial stacks can be erased from the streaming cache as the full stack contains all the required information. This way the maximum cache length is only of C'+T instead of C x T’. The stack pattern requires a customized attention mask during training that simulates the inference dynamic caching behavior. However it still requires C times more decoding steps than delay. 2.2.3. Stack-delay To compensate for the increased decoding step count (i.e. inference time) of the stack pattern, we propose to introduce C parallel decoding streams in what we call the stack-delay pattern, illustrated in the right part of Figure[]] Having C parallel streams decoding a C’ times longer sequence means that overall the total number of decoding steps is the same as for the delay pattern (i.e. T). The main difference with delay is that we no longer stack tokens from different time steps but always from the same time step. This also allows positional encoding to encode not only the decoded time step but also the decoded token level, hence hinting the model about which time step and level is about to be decoded. We hope this will improve the overall model performance for the same inference efficiency budget as delay, due to the use of parallel-optimized compute hardware. We report the decoding step count and maximum context length in Table[I]for each pattern. 2.2.4. Timesteps interleaving Finally, we introduce time steps permutation in the decoding schedule: the decoding remains auto-regressive but the model is trained to predict the token sequence in a time steppermuted order. This aims to offer more context for adjacent time steps decoding. An example of such interleaving pattern is shown on the right part of Figure [I] which corresponds to the decoding schedule defined in equation [3] with k = 3. According to the equation, the delay pattern decoding schedule corresponds to the case where k = 1. G(i,t) =t+(t mod (k+1))x(k-1)+i (3) 3.
--- EXPERIMENT ---
AL SETUP Most of the experimental setup follows that of MusicGen [4], we refer the readers to it for more details. 3.1. Model The tokenizer is an EnCodec model [II], made of CNN autoencoder and Residual Vector Quantization module applied to the latent representation of waveforms. The RVQ module is made of C = 4 quantizers, each with a codebook size of 2048. It encodes 32 kHz monophonic audio into a stack oftokens every 20ms (50 Hz framerate). The Transformer decoder is made of 300M parameters, implemented with a customized version of audiocraft| It uses Pytorch 2.P}flash attention for faster training and generation with optimized memory footprint. The model is trained on 30-seconds random crops of the full track. The models are trained for 200 epochs (400k steps) with the AdamW optimizer, a batch size of 192, 6; = 0.9, 82 = 0.95, a decoupled weight decay of 0.1 and no gradient clipping. A cosine learning rate schedule with a warmup of 4000 steps is used at the beginning of training. Models are trained with an exponential moving average with 0.99 decay. Training uses fp/6 mixed precision and distributed data parallelism on 24 A100 GPUs. 3.2. Generation At each decoding step the Transformer decoder emits a probability distribution over the token space for time steps and levels to decode according to the decoding schedule. Tokens are sampled from the distribution with top-k nucleus sampling with k = 250 tokens and a temperature of 1.0. We apply classifier-free guidance when sampling from the model’s logits, with a guidance scale of 3.0. The baseline model uses the delay codebook pattern from [4]. This translates 30 seconds of audio into T = 500 autoregressive steps. For text conditioning, we use the TS text encoder. During training we drop the text condition with a probability of 0.1. We experiment with flat, stack and stackdelay codebook patterns. 3.3. Data We train our models on 20K hours of licensed music: an internal dataset of 10K high-quality music tracks and the ShutterStock and Pond5 music data collection] with respectively 25K and 365K instrument-only recordings. All recordings are sampled at 32 kHz and come with a textual description. The models are evaluated on an in-domain split different from that of [4] and on the MusicCaps dataset [17]. ‘https://github.com/facebookresearch/audiocraft ?https://pytorch.org/ 3.www.shutterstock.com/music and www.pond5.com --- --in-domain MusicCaps RTF pattern FAD | KLD | CLAP| FAD | (A100) delay 0.69 | 0.48 | 0.36 4.91 1.flat 0.42 | 0.47 | 0.37 5.25 4.stack 0.38 | 0.48 | 0.37 5.16 4.stack-delay | 0.48 | 0.48 | 0.37 4.88 1.Table 2. Quality/efficiency trade off of the proposed token sequence patterns for 30 seconds generated tracks. decoding schedule G(i,t) | FAD | KLD | CLAP t+ i (delay) 0.45 | 0.50 | 0.t + i (stack-delay) 0.43 | 0.51 0.t+(t mod 3) x1+i 0.42 | 0.50 | 0.t+(t mod 4) x2+i 0.36 | 0.51 | 0.t+(t mod 5) x 3+i 0.34 | 0.52 | 0.Table 3. Ablation study on the effect of permuting timesteps in the decoding schedule of the stack-delay pattern, for 10s samples on the in-domain evaluation dataset. 3.4. Evaluation The different models are evaluated through a set of generated samples from a list of evaluation text-prompts. For objective evaluation we compute Frechet Audio Distance (FAD) using VGG classifier [18], Kullback—Leibler divergence (KLD) using PaSST model [19], and CLAP similarity score [20]. For subjective evaluation we run a blind pairwise comparison test where we present the evaluator two samples generated by two models but using the same text prompt, for a list of 20 text prompts. The human evaluators are asked to select the preferred sample from each pair based on perceived quality. Finally we report the RTF computed on A100 GPU when generating one sample (effective batch size of 2 from the model perspective due to classifier free guidance). 4. RESULTS 4.1. Baselines - flat and delay patterns We consider two baselines: flat, which is known to produce the highest quality audio although requiring much more compute than delay, and delay, a good compromise between speed and performance, achieving a RTF close to 1, potentially unlocking streaming scenarios. flat achieves an in-domain FAD of 0.42, 39% lower than delay, while KLD and CLAP remain close. Despite its higher quality the RTF is above 4. 4.2. Stack pattern We first investigate the stack pattern as a replacement for the (so far) state-of-the-art flat. Our results indicate that it is competitive with flat, even outperforming its FAD score with 0.38, with a similar RTF. The better FAD score indicates that the shorter required context length for generation might have a positive effect on music quality for long samples generations. 4.3. Stack-delay pattern When considering the stack-delay pattern, our results indicate that it outperforms delay with a FAD of 0.48, although not as low at stack, but much more efficient with almost the same RTF as delay, unlocking potential real time streaming scenarios with better quality than the baseline. For subjective evaluation we only compare the stack-delay and delay versions. Our results indicate that samples generated by the stack-delay are preferred 51.3% of the time compared with delay. Such a small difference is to be expected given the small scale of our subjective evaluation. 4.4. Ablation - permuting decoded time steps Finally, we look into the interleaved time steps decoding schedules defined in section [2.2.4] The ablation results are presented in Table [3] that compares four different schedules applied with the stack-delay pattern, and also including the delay baseline. The table shows that the higher the decoding step count separating adjacent positions, the lower the FAD, with KLD and CLAP scores in a similar range. This shows the benefit of permuting the time steps in the stack-delay pattern. Without permutation (i.e. following the same ascending time steps schedule as delay), the stack-delay pattern only achieves marginal improvement. We also tried applying the delay pattern with the same permuted schedules and the performance was only on par with the baseline, which means that the combination of the proposed pattern and permuted decoding schedule is essential for better performance. 5.
--- CONCLUSION ---
We introduce a new codebook pattern that relies on stacking the discrete music tokens, delaying/shifting the decoding of subsequent levels, and permuting the order of time steps to decode in the decoding schedule. The combination of the three outperforms the delay baseline quality-wise with a indomain FAD reduction of 45% for the same inference efficiency budget, due to parallel decoding that compensates for an increased sequence length. We also show that stacking the tokens should be preferred to flattening them best when the highest quality is a priority. Finally the ablation study shows that time step permutation is key to achieve optimal performance, indicating that decoding of adjacent positions with only partial knowledge of previous time steps probably affects the performance of the delay pattern. Overall we hope our findings can help design better non-autoregressive decoding strategies in the future. --- --(1) [[[[[10] (11) 6. REFERENCES Flavio Schneider, Zhijing Jin, and Bernhard Schélkopf, “Mo\* usai: Text-to-music generation with long-context latent diffusion,’ arXiv preprint arXiv:2301.11757, 2023. Qingging Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al., “Noise2music: Text-conditioned music generation with diffusion models,” arXiv preprint arXiv:2302.03917, 2023. Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song, et al., “Efficient neural music generation,” arXiv preprint arXiv:2305.15719, 2023. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez, “Simple and controllable music generation,” arXiv preprint arXiv:2306.05284, 2023. Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang, “Jen-1: Text-guided universal music generation with omnidirectional diffusion models,” arXiv preprint arXiv:2308.04729, 2023. Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo, “Vampnet: Music generation via masked acoustic token modeling,” arXiv preprint arXiv:2307.04686, 2023. Prafulla Dhariwal and Alexander Nichol, “Diffusion models beat gans on image synthesis,’ Advances in neural information processing systems, vol. 34, pp. 8780— 8794, 2021. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman, “‘Maskgit: Masked generative image transformer,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 11315-11325. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al., “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, pp. 9, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022. [12] [13] [14] [15] [16] [17] [18] [19] [20] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi, “Soundstream: An end-to-end neural audio codec,’ IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495-507, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,’ The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yangqing Liu, Huaming Wang, Jinyu Li, et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023. Zalan Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi, “Soundstorm: Efficient parallel audio generation,” arXiv preprint arXiv:2305.09636, 2023. Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi, “Audiogen: Textually guided audio generation,” in The Eleventh International Conference on Learning Representations, 2022. Andrea Agostinelli, Timo I Denk, Zalaén Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al., “Musiclm: Generating music from text,” arXiv preprint arXiv:2301.11325, 2023. Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al., “Cnn architectures for large-scale audio classification,” in 2017 ieee international conference on acoustics, speech and signal processing (icassp). IEEE, 2017, pp. 131-135. Khaled Koutini, Jan Schliiter, Hamid Eghbal-Zadeh, and Gerhard Widmer, “Efficient training of audio transformers with patchout,” arXiv preprint arXiv:2110.05069, 2021. Benjamin Elizalde, Soham Deshmukh, Mahmoud AI Ismail, and Huaming Wang, “Clap learning audio concepts from natural language supervision,’ in JCASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1-5.
"	"--- ABSTRACT ---
언어 모델링 기반 음악 생성에서 생성된 파형은 코드북 패턴에 따라 자동 회귀 방식이나 병렬로 디코딩할 수 있는 계층적 토큰 스택의 시퀀스로 표현됩니다. 특히 코드북을 평탄화하는 것은 가장 높은 품질의 디코딩 전략을 나타내지만, 엄청나게 느립니다. 이를 위해 우리는 플랫 패턴 디코딩을 개선하기 위해 새로운 스택-앤-딜레이 스타일의 디코딩 전략을 제안합니다. 여기서 생성 속도는 바닐라 플랫 디코딩보다 4배 빠릅니다. 이를 통해 추론 시간이 지연 디코딩 전략의 추론 시간과 비슷해지고, 작은 배치 크기에 대해 GPU에서 더 빠른 추론이 가능합니다. 지연 패턴과 동일한 추론 효율 예산의 경우, 제안된 접근 방식이 객관적인 평가에서 더 나은 성능을 보이며, 품질 면에서 플랫 패턴과의 격차를 거의 줄였습니다. 결과는 주관적인 평가에 의해 입증되며, 이는 새로운 모델에서 생성된 샘플이 동일한 텍스트 프롬프트가 주어진 경우 경쟁 모델에서 생성된 샘플보다 약간 더 선호된다는 것을 보여줍니다. 색인 용어 음악 생성, 오디오 생성, 효율적인 디코딩, 변압기 디코더 1.
--- METHOD ---
병렬 디코딩이라고도 할 수 있는 반면 후자는 일반적으로 자기 회귀적입니다. 품질 수준이 원래 노래 수준에 가까워지면서 배치 크기가 일반적으로 작은 개인화된 온디바이스 음악 생성과 같은 새로운 상업적 사용 사례로 나아가는 길이 열리고 있습니다. 그러나 이러한 모델은 종종 품질 상쇄와 함께 제공됩니다. 품질이 높을수록 생성 속도가 느려지고 그 반대의 경우도 마찬가지입니다[3, 6]. 추론 중에 디코딩 전략, 하드웨어 및 모델 크기는 생성 속도에 영향을 미칩니다.[4]는 최근 압축된 이산 음악 표현(즉, 오디오 압축 모델로 계산된 토큰[11])의 시퀀스를 모델링하는 단일 단계 자기 회귀 Transformer 디코더를 제안했습니다. 저자들은 이산 토큰 시퀀스 모델링을 위한 여러 코드북 패턴을 탐구했습니다. 특히, 그들은 가장 성능이 좋은 패턴이 토큰 스택을 평탄화하는 데 의존한다는 것을 보여주었습니다(나머지 논문에서는 이를 플랫 패턴이라고 합니다). 실제로 생성된 파형의 각 부분은 하나의 토큰이 아니라 여러 토큰으로 표현되며, 이는 압축 모델의 Residual Vector Quantizer(RVQ) [12] 모듈에서 잔여 투영의 C 수에 해당합니다.토큰 스택을 평탄화하면 C 배 더 긴 시퀀스를 생성(및 학습)하는 비용이 발생하여 상당히 높은 실시간 계수(RTF)가 발생하여 모델을 실제로 대화형 사용자 경험에 사용할 수 없게 됩니다.이 문제를 극복하기 위해 제안된 지연 패턴[4]은 속도와 품질 간의 좋은 균형으로 나타났습니다.이 논문에서는 지연 패턴이 효율성에도 불구하고 설계상 고품질 샘플을 생성하는 모델의 능력에 영향을 미칠 수 있다는 가설을 세웠습니다.더 강력하지만 느린 플랫 패턴에서 시작하여 훨씬 더 높은 품질로 원래 지연 전략만큼 빠르게 음악을 생성할 수 있는 stackdelay라는 새로운 전략을 제안합니다.이 논문의 기여는 다음과 같습니다.• 추론 중에 이전 키/값 스트리밍 캐시 공간을 줄여 더 빠르고 메모리 효율적이면서도 플랫의 품질을 상속하는 새로운 스택 코드북 패턴. • 새로운 스택 지연 패턴: 생성을 위해 지연 패턴만큼 빠르면서도 스택 패턴의 장점을 활용합니다. - 객관적이고 주관적인 평가를 통해 입증된 것처럼 지연보다 더 높은 품질의 음악을 생성합니다. • 모델이 충분한 컨텍스트를 갖기 전까지 인접한 위치를 디코딩하지 못하도록 하는 디코딩된 위치를 끼워 넣는 새로운 디코딩 일정. 2. 스택 지연 코드북 패턴 2.1. 음악 생성 텍스트 설명이 주어지면 T5 인코더[13]에서 계산한 텍스트 임베딩 시퀀스가 컨디셔닝 신호로 사용됩니다.CCCCo ☐ ☐ ~ 11 12 13 ts ~ 11 12 13 14☐ ~ 11 12 13 14 15to 11 12 13 14 15 16So S1 S2 S3 S4 S5 S6 Sto tits tots ~ 12 t5 ~ to to to tz t5 ~ to to to to to tttག་ ttt3 ttg t12 t7 t10 t13to ttt2 t5 ttttg t†t7 10 13 16 Ct12 t7 410 413 16 Cمی آن آی دی t7 t10 t13to ttt₂ t5 t8 t3 t6 tg t12 t7 t10 t[DELAY to t₁ tt₂ t5 t8 t3 t6 tg t12 tz t10 t[CCto Cto11 to 13 to to t₁₂ t7 totto to t1 tCCtotototo 아닌 Cto t1 tCo to to to to to ti titit Co ☐ to tht₂ t5 t8 t3 t6 tg t12 t7 t10 t13to tits to to to to to to to 12 to 10 13to 1s 13 to toSo S1 S2 S3 S4 S5 S6So S1 SS3 S4 SSS7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 $STACK-DELAY STACK 그림 1. 제안된 스택 지연 패턴(오른쪽)과 지연(왼쪽 위) 및 스택(왼쪽 아래)의 비교. 스택 지연 패턴에서 토큰은 병렬로 다중 스트림 방식으로 생성됩니다. 시간 단계는 순열 방식으로 디코딩됩니다. 최상위 스트림의 키/값 임베딩만 장기 스트리밍 캐시에 저장되므로 스택 패턴에서 더 나은 품질을 유지하면서도 지연만큼 효율적으로 추론할 수 있습니다. C-Transformer 디코더 모델(교차 어텐션 사용). 이 모델은 CNN 디코딩을 통해 오디오 파형으로 디코딩되는 EnCodec [11] 토큰 스택 {Cit}의 시퀀스를 생성합니다. i는 토큰 수준을 나타내고 t는 생성된 시퀀스의 시간 단계를 나타냅니다. 이 논문에서는 이전에 생성된 토큰(Transformer 디코더의 인과적 셀프 어텐션)에 따라 토큰 공간에 대한 확률 분포를 방출하는 자기 회귀 Transformer 디코더 아키텍처[9]만 고려합니다. 추론 중에 과거 셀프 어텐션 키와 값은 생성 시간을 최적화하기 위해 스트리밍 캐시에 저장됩니다.토큰화기 프레임 속도 f(예: f 50Hz), 생성할 오디오의 지속 시간 d 및 토큰 스택 크기 C(예: C = 4)에 따라 모델은 토큰 코드북 패턴과 디코딩 일정에 따라 지정된 양의 디코딩 단계에서 f× C × d 토큰을 생성해야 합니다.디코딩 일정은 각 Cit 2.2에 대한 디코딩 단계를 정의하는 함수 G(i, t)로 공식화할 수 있습니다.코드북 패턴 텍스트 도메인과 달리 오디오 세그먼트는 단일 토큰으로 표현되지 않고 CNN 자동 인코더[11]의 잠재 임베딩을 양자화[12]하여 계산된 계층적 토큰 스택으로 표현됩니다.이는 일반적으로 스택에서 토큰이 낮을수록 더 많은 정보를 전달함을 의미합니다. 계층적 방식으로 토큰을 예측하는 문제를 해결하기 위해 여러 코드북 인터리빙 패턴이 탐색되었습니다[14, 4, 15].가장 낮은 레벨의 토큰을 먼저 디코딩한 다음 더 높은 레벨을 추가 디코딩 단계에서 처리하는 것이 공통적인 아이디어인데, 이는 자기 회귀(AR)[4] 및 비자기 회귀(NAR)[6] 디코딩 아키텍처 모두에 해당합니다.즉, 디코딩 일정은 다음과 같이 제한됩니다.2.2.1. 지연 G(0,t) &lt; G(i,t), Vi Є [1, C[ (1) 음악 생성과 관련하여 지연 인터리빙 패턴(그림 1의 좌측 상단에 표시)은 품질과 AR 디코딩 단계 수 사이에서 좋은 절충안으로 나타났습니다.지연 패턴에서 C 코드북 레벨은 병렬로 예측되지만 디코딩된 시간 단계에서 이동합니다.즉, G(i,t) = t + i. 이는 시퀀스의 각 후속 시간 단계가 이전 인접 시간 단계에 대한 부분적인 지식만으로 디코딩되기 시작한다는 것을 의미합니다.예를 들어, 그림에서 디코딩 단계 s₁에서 cot₁의 예측은 이전에 so에서 디코딩된 Coto에만 조건이 지정되고 시간 단계 to의 더 높은 수준 {c;};--¹에는 조건이 지정되지 않습니다.2.2.2. 스택 C-[4]는 최고의 음악 품질을 얻기 위해 코드북을 평탄화하는 것이 C 배 더 많은 디코딩 단계를 희생하더라도 가장 좋은 성능을 보였다는 것을 보여주었습니다.G(i,t) =Cxt + i &lt; C × T (2) 이는 후속 디코딩된 시간 단계가 이전 단계의 전체 컨텍스트에서 이점을 얻는다는 사실로 쉽게 설명할 수 있습니다. 이 경우 cot+1의 예측은 효과적으로 C[0,C-1][0,t]에 따라 조건지어집니다.컨텍스트 길이는 패턴 디코딩 단계 컨텍스트 길이 지연 플랫 TT Tx C TX C 스택 스택 지연 Tx CT T+CT보다 C 배 더 큽니다.표 1. 추론 중 스트리밍 캐시의 필요한 디코딩 단계 수와 최대 컨텍스트 길이는 T dxf를 생성하는 시퀀스 길이와 지연보다 큰 코드북 수준 C. = 수의 함수입니다.왜냐하면 추론 중 최대 C × T의 과거 Transformer 셀프 어텐션 키/값 표현이 스트리밍 캐시에 저장되기 때문입니다.캐시 크기를 줄이기 위해 그림 1에서 볼 수 있듯이 디코딩 프로세스 전반에 걸쳐 하위 수준 토큰을 유지하고 스태킹하여 플랫 패턴을 적용합니다.주어진 시간 단계에 대해 전체 스택이 디코딩되면 전체 스택에 필요한 모든 정보가 포함되어 있으므로 부분 스택을 스트리밍 캐시에서 지울 수 있습니다.이렇게 하면 최대 캐시 길이가 C×T 대신 C+T만 됩니다.스택 패턴은 추론 동적 캐싱 동작을 시뮬레이션하는 교육 중에 사용자 지정 어텐션 마스크가 필요합니다. 그러나 여전히 지연보다 C배 더 많은 디코딩 단계가 필요합니다.2.2.3. 스택 지연 스택 패턴의 증가된 디코딩 단계 수(즉, 추론 시간)를 보상하기 위해 그림 1의 오른쪽 부분에 표시된 스택 지연 패턴이라고 하는 C 병렬 디코딩 스트림을 도입하기로 제안합니다.C 병렬 스트림이 C배 더 긴 시퀀스를 디코딩한다는 것은 전체적으로 디코딩 단계의 총 수가 지연 패턴(즉, T)과 동일하다는 것을 의미합니다.지연과의 주요 차이점은 더 이상 다른 시간 단계의 토큰을 스택하지 않고 항상 동일한 시간 단계의 토큰을 스택한다는 것입니다.또한 이를 통해 위치 인코딩에서 디코딩된 시간 단계뿐만 아니라 디코딩된 토큰 수준도 인코딩할 수 있으므로 모델에 어떤 시간 단계와 수준이 디코딩될 것인지에 대한 힌트를 제공할 수 있습니다.병렬 최적화된 컴퓨팅 하드웨어를 사용하기 때문에 지연과 동일한 추론 효율성 예산으로 전체 모델 성능이 향상되기를 바랍니다.각 패턴에 대한 디코딩 단계 수와 최대 컨텍스트 길이를 표 1에 보고합니다.2.2.4. 시간 단계 인터리빙 마지막으로 디코딩 일정에 시간 단계 순열을 도입합니다. 디코딩은 자기 회귀적이지만 모델은 시간 단계 순열 순서로 토큰 시퀀스를 예측하도록 학습됩니다. 이는 인접한 시간 단계 디코딩에 대한 더 많은 컨텍스트를 제공하는 것을 목표로 합니다. 이러한 인터리빙 패턴의 예는 그림 1의 오른쪽에 표시되어 있으며, 이는 k = 3인 방정식 3에 정의된 디코딩 일정에 해당합니다. 방정식에 따르면 지연 패턴 디코딩 일정은 k = 1인 경우에 해당합니다. G(i,t) = t+(t mod (k + 1)) × (k − 1) + i (3) 3.
--- EXPERIMENT ---
AL 설정 대부분의 실험 설정은 MusicGen [4]의 설정을 따르며, 자세한 내용은 해당 설정을 참조하세요.3.1. 모델 토크나이저는 CNN 자동 인코더와 파형의 잠재 표현에 적용된 잔여 벡터 양자화 모듈로 구성된 EnCodec 모델[11]입니다.RVQ 모듈은 C = 4 양자화기로 구성되며 각각 코드북 크기가 2048입니다.32kHz 모노포닉 오디오를 20ms(50Hz 프레임 속도)마다 토큰 스택으로 인코딩합니다.Transformer 디코더는 audiocraft¹의 사용자 지정 버전으로 구현된 300M 매개변수로 구성됩니다.최적화된 메모리 풋프린트로 더 빠른 학습과 생성을 위해 Pytorch 2.0² 플래시 어텐션을 사용합니다.모델은 전체 트랙의 30초 무작위 크롭에서 학습됩니다. 모델은 AdamW 옵티마이저, 192의 배치 크기, ẞ₁ = 0.9, 62 = 0.95, 0.1의 분리된 가중치 감소, 그래디언트 클리핑 없음으로 200개 에포크(400k 단계) 동안 학습되었습니다. 학습 시작 시 4000단계의 워밍업이 있는 코사인 학습률 일정이 사용됩니다. 모델은 0.99 감소의 지수 이동 평균으로 학습됩니다. 학습은 24개의 A100 GPU에서 fp16 혼합 정밀도 및 분산 데이터 병렬 처리를 사용합니다. 3.2. 생성 각 디코딩 단계에서 Transformer 디코더는 디코딩 일정에 따라 디코딩할 시간 단계 및 수준에 대한 토큰 공간에 대한 확률 분포를 방출합니다. 토큰은 k = 250개 토큰과 1.0의 온도를 사용하여 상위 k 핵 샘플링을 사용하여 분포에서 샘플링됩니다. 모델의 로짓에서 샘플링할 때 분류기 없는 안내 [16]를 적용하며 안내 척도는 3.0입니다. 기준 모델은 [4]의 지연 코드북 패턴을 사용합니다. 이는 30초 분량의 오디오를 T = 500 자기 회귀 단계로 변환합니다. 텍스트 컨디셔닝의 경우 T5 [13] 텍스트 인코더를 사용합니다. 학습하는 동안 텍스트 조건을 확률 0.1로 삭제합니다. 플랫, 스택 및 스택 딜레이 코드북 패턴을 실험합니다. 3.3. 데이터 20,000시간 분량의 라이선스 음악으로 모델을 학습합니다. 10,000개의 고품질 음악 트랙과 각각 25,000개와 365,000개의 악기 전용 녹음이 있는 ShutterStock 및 Pond5 음악 데이터 컬렉션³입니다. 모든 녹음은 32kHz로 샘플링되며 텍스트 설명이 함께 제공됩니다. 이 모델은 [4]와 다른 도메인 내 분할과 MusicCaps 데이터 세트 [17]에서 평가됩니다. ¹https://github.com/facebookresearch/audiocraft 2https://pytorch.org/ 3 www.shutterstock.com/music 및 www.pond5.com MusicCaps RTF FAD(A100) 도메인 내 패턴 FAD KLD CLAP 지연 플랫 0.0.48 0.4.1.0.42 0.0.5.4.stack 0.0.48 0.5.4.4.1.stack-delay 0.48 0.48 0.표 2. 30초 동안 생성된 트랙에 대한 제안된 토큰 시퀀스 패턴의 품질/효율성 균형. 디코딩 일정 G(i, t) t+i(지연) t+i(스택 지연) t+(t mod 3) × 1 + i 0.0.FAD KLD CLAP 0.45 0.50 0.0.43 0.51 0.0.t+(t mod 4) × 2+ i 0.0.0.t(t mod 5) × 3+ i 0.0.0.표 3. 도메인 내 평가 데이터 세트에서 10초 샘플에 대한 스택 지연 패턴의 디코딩 일정에서 타임스텝을 순열하는 효과에 대한 절제 연구. 3.4. 평가 다양한 모델은 평가 텍스트 프롬프트 목록에서 생성된 샘플 세트를 통해 평가됩니다. 객관적인 평가를 위해 VGG 분류기[18]를 사용하여 Frechet Audio Distance(FAD), PaSST 모델[19]을 사용하여 Kullback-Leibler divergence(KLD), CLAP 유사도 점수[20]를 계산합니다. 주관적인 평가를 위해 동일한 텍스트 프롬프트를 사용하여 두 모델에서 생성한 두 샘플을 평가자에게 제시하는 맹검 쌍별 비교 테스트를 실행합니다. 20개의 텍스트 프롬프트 목록입니다. 인간 평가자에게는 인식된 품질에 따라 각 쌍에서 추출된 샘플을 선택하도록 요청합니다. 마지막으로 한 샘플을 생성할 때 A100 GPU에서 계산된 RTF를 보고합니다(분류기 없는 안내로 인해 모델 관점에서 효과적인 배치 크기 2). 4. 결과 4.1. 기준선 - 플랫 및 지연 패턴 사전 우리는 두 가지 기준선을 고려합니다. 플랫은 지연보다 훨씬 더 많은 컴퓨팅이 필요하지만 최고 품질의 오디오를 생성하는 것으로 알려져 있고, 지연은 속도와 성능의 좋은 절충안으로 RTF를 1에 가깝게 달성하여 스트리밍 시나리오를 잠재적으로 열어줍니다. flat은 0.42의 도메인 내 FAD를 달성하여 delay보다 39% 낮은 반면 KLD와 CLAP은 비슷한 수준을 유지합니다. 더 높은 품질에도 불구하고 RTF는 4 이상입니다. 4.2. 스택 패턴 먼저 스택 패턴을 (지금까지) 최첨단 flat의 대체 패턴으로 조사했습니다. 결과에 따르면 flat과 경쟁력이 있으며 유사한 RTF로 0.38의 FAD 점수를 앞지릅니다. 더 나은 FAD 점수는 생성에 필요한 컨텍스트 길이가 짧을수록 긴 샘플 생성 시 음악 품질에 긍정적인 영향을 미칠 수 있음을 나타냅니다. 4.3. 스택 지연 패턴 스택 지연 패턴을 고려할 때 결과에 따르면 스택에서 낮지는 않지만 delay와 거의 같은 RTF로 훨씬 더 효율적이어서 기준선보다 더 나은 품질의 잠재적 실시간 스트리밍 시나리오를 열어줍니다. 주관적인 평가를 위해 스택 지연과 지연 버전만 비교합니다. 결과는 스택 지연으로 생성된 샘플이 지연에 비해 51.3%의 시간 동안 선호된다는 것을 나타냅니다. 주관적인 평가의 규모가 작기 때문에 이처럼 작은 차이는 예상할 수 있습니다. 4.4. 삭제 - 디코딩된 시간 단계 순열 마지막으로 2.2.4절에서 정의한 인터리브 시간 단계 디코딩 일정을 살펴봅니다. 삭제 결과는 스택 지연 패턴에 적용된 4가지 다른 일정을 비교하고 지연 기준선을 포함하는 표 3에 나와 있습니다. 이 표는 인접한 위치를 구분하는 디코딩 단계 수가 높을수록 FAD가 낮고 KLD 및 CLAP 점수가 비슷한 범위에 있음을 보여줍니다. 이는 스택 지연 패턴에서 시간 단계를 순열하는 이점을 보여줍니다. 순열 없이(즉, 지연과 동일한 오름차순 시간 단계 일정을 따름) 스택 지연 패턴은 미미한 개선만 달성합니다. 또한 동일한 순열 디코딩 일정에 지연 패턴을 적용해 보았는데, 성능은 기준선과 비슷한 수준이었습니다. 즉, 더 나은 성능을 위해서는 제안된 패턴과 순열 디코딩 일정을 결합하는 것이 필수적입니다.
--- CONCLUSION ---
우리는 이산 음악 토큰을 쌓고, 후속 레벨의 디코딩을 지연/이동하고, 디코딩 일정에서 디코딩할 시간 단계의 순서를 순열하는 새로운 코드북 패턴을 소개합니다. 세 가지를 결합하면 동일한 추론 효율 예산에 대해 병렬 디코딩으로 인해 증가된 시퀀스 길이를 보상하여 45%의 도메인 내 FAD 감소로 지연 기준선보다 품질 면에서 성능이 뛰어납니다. 또한 가장 높은 품질이 우선순위일 때 토큰을 평평하게 하는 것보다 토큰을 쌓는 것이 더 선호되어야 함을 보여줍니다. 마지막으로 절제 연구는 시간 단계 순열이 최적의 성능을 달성하는 데 중요하다는 것을 보여주며, 이전 시간 단계에 대한 부분적인 지식만 있는 인접한 위치를 디코딩하면 지연 패턴의 성능에 영향을 미칠 가능성이 있음을 나타냅니다. 전반적으로 우리의 연구 결과가 앞으로 더 나은 비자기 회귀 디코딩 전략을 설계하는 데 도움이 되기를 바랍니다. 6. 참고 자료 [1] Flavio Schneider, Zhijing Jin 및 Bernhard Schölkopf, &quot;Mo\usai: 긴 맥락 잠재 확산을 통한 텍스트-음악 생성&quot;, arXiv 사전 인쇄 arXiv:2301.11757, 2023. [2] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank 등, &quot;Noise2music: 확산 모델을 사용한 텍스트 조건 음악 생성&quot;, arXiv 사전 인쇄 arXiv:2302.03917, 2023. [3] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen 송 등 al., “효율적인 신경 음악 생성,” arXiv 사전 인쇄본 arXiv:2305.15719, 2023. [4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez, “간단하고 제어 가능한 음악 생성,” arXiv 사전 인쇄본 arXiv:2306.05284, 2023. [5] Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, Alex Wang, “Jen-1: 전방위 확산 모델을 사용한 텍스트 기반 범용 음악 생성,” arXiv 사전 인쇄본 arXiv:2308.04729, 2023. [6] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo, “Vampnet: Music generation via 마스크 처리된 음향 토큰 모델링,&quot; arXiv 사전 인쇄본 arXiv:2307.04686, 2023. [7] Prafulla Dhariwal 및 Alexander Nichol, &quot;확산 모델은 이미지 합성에서 간을 이긴다&quot;, 신경 정보 처리 시스템의 발전, vol. 34, pp. 87808794, 2021. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu 및 William T Freeman, &quot;Maskgit: 마스크 처리된 생성 이미지 변환기&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022, pp. 11315-11325. [9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 외, &quot;언어 모델은 비지도 멀티태스크 학습자입니다.&quot; OpenAI 블로그, 1권, 8호, 9쪽, 2019. [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar 외, &quot;Llama: 개방적이고 효율적인 기초 언어 모델&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [11] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi, &quot;고충실도 신경 오디오 압축&quot; arXiv 사전 인쇄본 arXiv:2210.13438, 2022. [12] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund 및 Marco Tagliasacchi, &quot;Soundstream: 종단 간 신경 오디오 코덱&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 거래, 제 12권, 1호(2013년 12월), 1999년 11월. 30, pp. 495–507, 2021. [13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색&quot;, The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [14] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., &quot;신경 코덱 언어 모델은 제로샷 텍스트-음성 합성기&quot;, arXiv 사전 인쇄본 arXiv:2301.02111, 2023. [15] Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour 및 Marco Tagliasacchi, &quot;Soundstorm: 효율적인 병렬 오디오 생성&quot;, arXiv 사전 인쇄 arXiv:2305.09636, 2023. [16] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman 및 Yossi Adi, &quot;Audiogen: 텍스트 안내 오디오 생성&quot;, 학습 표현에 관한 제11차 국제 컨퍼런스, 2022. [17] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi 등 al., &quot;Musiclm: 음악 생성하기 텍스트에서,” arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [18] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al., “대규모 오디오 분류를 위한 CNN 아키텍처,” 2017 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(icassp). IEEE, 2017, pp. 131-135. [19] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh 및 Gerhard Widmer, &quot;패치아웃을 통한 오디오 변압기의 효율적 학습&quot;, arXiv 사전 인쇄본 arXiv:2110.05069, 2021. [20] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail 및 Huaming Wang, &quot;자연어 감독을 통한 박수 학습 오디오 개념&quot;, ICASSP 2023-2023 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2023, pp. 1-5.
"
"--- ABSTRACT ---
The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy of our proposed approach in joint segmentation and state tracking, we evaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets. Across all datasets and settings, S3-DST consistently outperforms the state-of-the-art, demonstrating its potency and robustness the next generation of LLM-based chat systems. 1
--- INTRODUCTION ---
The advent of open-domain Large Language Model (LLM)-based chat systems like ChatGPT and Bing Chat has ushered in a new age of dialogue systems. Previously, dialogue systems were relatively constrained in their scope and abilities, typically confined to either narrow task-oriented conversations or social chitchat (Gao et al., 2018). By contrast, LLM-based chat systems are remarkable because they can converse fluidly with users over a seemingly infinite range of topics, and can accomplish User: Please make an annotated bibliography... AI: Here are a few examples of annotated... User: Can you explain these citation styles? AI: Of course. Citation styles are sets of... User: Thank you! How's your day going? AI: I'm having a good day so far, thank you... User: What's the weather forecast next week? AI: According to the results from my search... Figure 1: A single intent may span several turns in opendomain conversation, and a single conversation may contain multiple intents: A synthetic dialogue inspired by anonymized Bing Chat logs. Different user intents (creating an annotated bibliography, social chitchat, checking the weather) are highlighted by different colors. many user tasks out-of-the-box that previously required specialized systems, like code generation, question answering, and more. In this paper, we argue that because LLM-based chat systems have significantly changed the landscape of human-AI dialogue, understanding user intent in such dialogues calls for new analysis and tagging frameworks. We focus in particular on the task of dialogue state tracking (DST). Traditional DST consists of extracting and matching users’ intents in task-oriented dialogue systems to a structured backend schema (Williams et al., 2016; Budzianowski et al., 2018). However, DST in opendomain conversation is yet undefined; as such, in this paper we make a first attempt at identifying the state values of interest in LLM-based chat systems. As exemplified by Figure 1, we make the key observation that real open-domain dialogue often exhibits extensive back-and-forth between parties (e.g., clarification, negotiation, etc) in order to pursue a single intent or topic, and contexts may shift multiple times within a single dialogue among un --- --related intents and/or topics. Based on this observation, we propose to track both segments and states in open-domain dialogue: Segmentation helps us identify boundaries that mark the start and end of contextually cohesive conversation “units,” whereas states are the intent variables of interest we wish to track, applied per segment. Beyond bringing DST into the era of opendomain conversation and LLMs, we introduce LLM-based solutions for open-domain DST. Assuming a zero-shot setting for dialogue tagging, which is realistic due to the cost of labeling, we introduce $3-DST, a structured prompting approach for open-domain DST. Within S3-DST we propose a novel Pre-Analytical Recollection (PAR) prompting strategy that grounds each output state prediction on the content of the corresponding dialogue turn, thereby helping the LLM track long dialogue context without forgetting or hallucination. We evaluate S3-DST on a fully anonymized open-domain dialogue dataset collected from Microsoft’s Bing Chat system, alongside public DST and segmentation benchmarks.! $3-DST achieves large gains over comparable baselines across all benchmarks, suggesting its suitability as a starting point for further research in open-domain dialogue modeling. In summary, our contributions are: ¢ Open-domain DST problem definition: We bring dialogue state tracking into the era of open-domain LLM chat. We cast the problem as a joint segmentation and state tracking task, motivated by our observations of how real open-domain human-AI conversation is conducted on anonymized Bing Chat log data. ¢ Zero-shot S3-DST approach: We propose S3-DST, a structured zero-shot joint segmentation and state tracking approach for open-domain, multi-intent dialogue. S3-DST contributes new approaches for structured prompt templating and dialogue tag generation, as well as Pre-Analytical Recollection (PAR), a grounding technique that improves long context tracking. ¢ Extensive experiments and analysis: We conduct extensive experiments on both proprietary and public datasets, achieving large gains over comparable zero-shot prompts. S3DST achieves state-of-the-art zero-shot per 'The use of Bing Chat logs is in compliance with the terms formance on the MWOZ 2.1 and 2.4 DST benchmarks, alongside the DialSeg711 dialogue topic segmentation benchmark. 2 Problem Definition Informally, the goal of traditional DST is to predict the dialogue state y, given a sequence of user and agent utterance turns C; = [U1, A1,...,Ut, Ajj? The state y, consists of a set of slot-value pairs, where slots correspond to intent attributes in a particular application domain (e.g., “restaurantname”, “hotel-address’”’) and values correspond to predefined categorical options or unconstrained text (Budzianowski et al., 2018). However, as we have previously discussed, a single open-domain conversation will often consist of multiple potentially unrelated intents across a variety of topics. Indeed, according to a preliminary analysis on 10K anonymized Bing Chat conversations, we estimate that over 50% of conversations display multiple user intents and over 90% of conversations contain discussion of multiple topics. Therefore, we propose to merge dialogue segmentation, which aims to find contextually cohesive “units” of dialogue within a larger conversation, with dialogue state tracking. In particular, we perform state tracking at the segment level, where the goal is to label each segment with the slots and values of interest, such that multiple segments within a conversation may have diverging or conflicting state values, reflecting the true variety of open-domain chat. In the rest of this section, we define segmentation and state, and finally formalize the joint task. 2.1 Segment Following previous work in dialogue topic segmentation (Xing and Carenini, 2021; Xia et al., 2022; Gao et al., 2023), we define dialogue segments as contiguous subsequences of C; in which all user and agent utterances are topically related. Formally, let By = [b1,...,b:—-1] indicate the boundary indices between adjacent user-agent utterance pairs in C;. The output of segmentation is a set of boundary indices By, C B;, where k represents the number of boundaries determined by the segmentation algorithm and the span [Um, Am, ... Un, An] repreof use of Bing Chat. ?Note that in current LLM-based chat systems, users may issue multiple utterances before a single agent response is is sued. In these (infrequent) cases, we group all user utterances prior to the agent response into a single utterance. --- --sents the contiguous segment between boundaries bm and b,,, where m € [1,t—1] andn € [m,t—1]. 2.2 Segment state Typically, dialogue state tracking methods extract new elements of state at each turn (Hu et al., 2022). However, this is because DST evaluation benchmarks make the relatively narrow assumption that users provide new and relevant elements of intent at each turn, and that intents build upon or complement each other but do not fundamentally change or conflict throughout the conversation. As we have previously discussed, open-domain dialogue exhibits far more varied characteristics, and multiintent and/or multi-domain conversations are relatively common. We therefore propose to extract state at the segment rather than turn level. We define the segment-level state as {Sinin = (s@n, wen), i = 1...Nm:n}, where 3), refers to the i-th slot applied to the segment from boundaries by, to bn, vn refers to the slot’s corresponding value, and Nm:n tefers to the total number of slots to applied to this segment. Any schema of slot-value pairs is valid here; we describe our particular state schema for Bing Chat in § 4.1 and Appendix B. 2.3 Problem statement Having defined segments and per-segment state, we are equipped to state our full definition of opendomain DST. Given a sequence of user-agent utterance pairs C; = [U), Aj,...,U;, Ar], we define the goal of open-domain dialogue state tracking as jointly predicting yt = Be U {Simin V(b, bn) € Bu}, (1) where B;, C B; refers to the segment boundary indices described earlier and Sip:, refers to the segment state between boundaries b,, and b,,, consisting of N arbitrary slot-value pairs: Sinn = {(8 On Ven), b= 1..-Nmin} (2) 3 Prompting Strategies As discussed previously, real-world dialogues often exhibit extensive discourse that extends over multiple conversational turns in order to discuss diverse topics. This prolonged conversational nature makes it highly challenging to track contextual coherence. Previous studies (Hu et al., 2022) aimed at disassociating individual dialogue turns and processing them one by one for tracking dialogue state changes, which worked reasonably well in task-oriented dialogues confined within predefined narrow domains. However, real-world dialogues commonly require multiple turns to adequately comprehend the contextual nuances, which is a challenge because Transformers still struggle when processing lengthy input contexts, particularly in the middle (Liu et al., 2023). To address these difficulties, we propose a novel turn-by-turn prompting technique that gives structure to inputs and outputs while accurately preserving the context in the process. We discuss these design aspects of our prompts below: 3.1 Structured Outputs and Inputs Structured Output Our goal is a set of labels per dialogue turn representing the segment boundaries (binary labels) and state values (categorical labels or open text). To provide a flexible yet structured format to the LLM’s output, we propose to instruct it to generate outputs in a hierarchical XML format. We see XML as advantageous because it provides code-like structure to the DST task, which has been shown to greatly improve performance compared to plain-text outputs, while still being extensible and flexible compared to more rigid output formats like SQL (Hu et al., 2022). Our approach uses an XML format in which each turn from 1 to ¢ comprises an XML tree <T{id}>...</T{id}> and several nested XML tags within it. The labels of these nested tags (e.g. <preceding_topical_relation>...</preceding_topical_relation>, <intent>...</intent>, and <domain>. . .</domain> in Figure 2(iii)) represent the segment boundaries and slots of interest, and each value between opening and closing tags represent the model’s inferred value. This strategy is beneficial from two fronts: (i) Due to bounded well-defined structured formatting, generated outputs are more likely to be aligned with labeling instructions than free-form texts, and (ii) Well-formed structured output formats are easier to parse, thus reducing postprocessing requirements. Structured Input For prompting LLMs, although it is trivial to channel plain conversation history in a flat format for analysis and inference, the unstructured nature inherent to this linear configuration makes it difficult to refer back and lever --- --(i) Insert conversation and instructions into structured prompt template <valid_preceding_topical_relation> <item> <name>YES</name> <description>...</description> </item> Raw conversation User: Please provide an annotated <valid_intents> <item>...</item> </valid_intents> bibliography of The... AI: Here are a couple of different bibliographies of... User: ... ### CONVERSATION AI: ... Ge a <User>Please provide an...</User> <Al>Here are a couple of...</AI> </TL> Structured Turn <User>...</User> <AI>...</AI> </T2> </valid_preceding_topical_relation> =» wt] <T2> —_E_”__ ll (ii) Pre-Analytical Recollection (PAR) Preserve context by grounding output in each turn (iii) Generate structured turn-by-turn dialogue tags via chain of thought <T1> <summary>The user requests an annotated. ..</summary> <preceding_topical_relation>N0</preceding_topical_relation> <intent>CREATION</intent> <domain>WRITING JOURNALISM AND PUBLISHING</domain> </T1> <T2> <summary>. ..</summary> <preceding_topical_relation>...</preceding_topical_relation> <intent>...</intent> <domain>. . -</domain> | </T2> | Figure 2: Prompt flow of S3-DST. Given a raw conversation, (i) we convert it into a hierarchical XML-structured representation and insert it into a similarly structured prompt template. We pass the prompt through the LLM and (ii) obtain a hierarchical XML-structured output, where each turn contains (iii) a PAR grounding reference to the conversation alongside the desired segmentation and state label predictions. age different information across multiple conversational turns. To handle this challenge, consistent with the output format, we propose a structured inputting format, where each conversational history is formed into a hierarchical XML format where conversational turns are marked with turn id number <T{id}>. ..</T{id}> numbered from 1 to t and each conversational turn consists of nested user and agent turns marked with appropriate XML tags (<user>...</user> and <agent>...</agent>). Since we propose instructing the LLM to infer per-turn labels during our output, this input scheme helps us accurately refer back to the input turn and thus maintain coherence even for long dialogue contexts. Consistent with this XML-tagged input format, we also format all the valid segment and state categories in an XML-formatted list using the following structure: <valid_category_name> <item>{label name}</item><description> {description of label, if available} </description> <valid_category_name> Empirically, this structured input and prompt formatting help constrain the LLM generation to follow the labeling instructions. Figure 2(i) shows this format where each valid segment boundary and state category are first staged in an XML-formatted list and subsequently input dialogue is shown in a hierarchical configuration. 3.2 Pre-Analytical Recollection (PAR) As previously discussed, open-domain dialogues may be long and highly variable in conversation flow. Therefore, it is crucial to ensure that the LLM can accurately monitor the evolving dialogue context without forgetting or hallucination. To this end, we propose Pre-Analytical Recollection (PAR), a grounding strategy for turn-by-turn prompting that instructs the LLM to first summarize the turn using <summary>. ..</summary> tags in 3 sentences or fewer before providing the segment and state values. PAR is inspired by chain-of-thought prompting (Wei et al., 2022), as it is a technique for generating relevant intermediary outputs in order to improve reasoning accuracy. However, unlike chain-of-thought, PAR is also a grounding technique that provides references from the model’s output directly to the conversation. Figure 2(ii) demonstrates how PAR refers back to the content of each conversational turn before analyzing it to infer the conversational states. 3.3 Final Prompt Configuration The final prompt flow of S3-DST is provided in Figure 2. Given a raw conversation and a predefined set of segment and state labels, we insert the labels into a structured prompt template and format the conversation in a hierarchical XML-structured representation. We pass the prompt through the LLM, instructing it to follow PAR before jointly --- --Table 1: Evaluation test set statistics. #Convs #Turns #segments/conv (avg.) Bing Chat 334 2308 1.MWOZ 2.1 1,000 7368 MWOZ 2.4 1,000 7368 DialSeg711 711 19350 3.generating the hierarchical turn-by-turn segmentation and state labels applied per segment. The full text of our prompt is provided in Appendix A.1. 4 Experiments We conduct comprehensive evaluations across multiple datasets. We primarily evaluate our approach on fully anonymized Bing Chat logs annotated by domain experts. Additionally, we evaluate S3-DST on the standard task-oriented DST and segmentation tasks using public benchmark datasets MultiWOZ (Budzianowski et al., 2018) and DialSeg(Xu et al., 2021) respectively. A detailed description of these datasets is provided below, alongside dataset statistics in Table 1: 4.1 Internal Human-LLM Dialogue Dataset In order to evaluate the efficacy of our approach on real-world open-domain human-LLM conversations, we collected anonymized chat log data from Microsoft’s Bing Chat system, an LLM chat interface backed by the Bing search engine. Benchmark construction We sample 484 English conversations conducted on Bing Chat between April 5, 2023 to April 30, 2023 via two approaches: (i) Random and (ii) “Long” conversations of 5 or more turns only. We balance these two approaches 50/50. Since we operate under a zero-shot assumption, we do not need any training data. Therefore, we hold out 150 conversations for development and the remaining 334 for testing. Annotation To obtain ground-truth labels for evaluation, we gathered human annotations for segment and state. We recruited three in-house annotators with a high degree of technical expertise and familiarity with the Bing Chat system. For each turn, we instructed annotators to provide binary IsSegmentBoundary labels, categorical SegmentIntent labels, and categorical SegmentDomain labels. We instructed annotators to mark a segment boundary when no topical relation between a turn and its preceding context could be identified. For intent and domain, we used taxonomies developed in-house for the Bing Chat system consisting of 4 intents (Information Seeking, Analysis, Creation, and Open-Ended Discovery) and 49 domains (see Appendix B.1 for the full list). Because of the large number of domains, per turn we provided annotators four candidate domain values and an “Other” option. Appendix B provides further details on the annotation scheme and domain sampling procedure. To ensure interannotator agreement before labeling the full dataset, we first gathered annotations on a set of 10 randomly selected conversations (68 turns total) and computed Fleiss’ kappa (Fleiss, 1971) per label type. We observed a Fleiss kappa of & = 0.83 for IsSegmentBoundary, « = 0.74 for SegmentIntent, and « = 0.88 for SegmentDomain, all of which are considered high agreement on the Fleiss kappa scale. 4.2 Public Benchmarks We are not aware of any existing public dialogue benchmarks reflective of the broadly open-domain Bing Chat data. Therefore, we resort to separate DST and segmentation evaluations on public benchmarks using three datasets. MultiWOZ The MultiWOZ (MWOZ) multidomain dialogue dataset (Budzianowski et al., 2018) is currently the most common DST benchmark. MWOZ is a task-oriented dataset consisting of 1K test dialogues. We use two updated versions of the original: MWOZ 2.1 (Eric et al., 2019) and 2.4 (Ye et al., 2021). The latter is considered the “cleanest” version of MWOZ, while the former has been used more frequently in the literature. DialSeg711 The DialSeg711 benchmark was introduced by (Xu et al., 2021) and has been used frequently in recent dialogue segmentation research. It is an English dataset in which 711 multi-segment dialogues are constructed by joining dialogues from existing task-oriented dialogue corpora. 4.3 Baselines As baselines we consider zero-shot LLM prompts only, for a fair comparison to S$3-DST. We discuss the baselines and their considerations below for different datasets. All original prompts are provided in Appendix A. We set a maximum of 1500 output tokens per LLM call with a temperature of zero. --- --Table 2: S3-DST achieves state-of-the-art performance on state tracking over our internal Bing Chat benchmark. All prompts are run with GPT4. Individual accuracy JGA Segment Intent Domain YD S//D TBT-DST - 0.6707 0.6221 0.4169 IC-DST 0.8567 0.7123 0.6049 0.4610 0.S3-DST (No PAR) 0.8859 0.7173 —:0.6251 0.4377 0.S3-DST (Unstructured input) 0.8810 0.7163 0.6307 0.4640 0.S3-DST 0.8992 0.7366 0.6429 0.4752 0.Bing Chat In this dataset, we consider IC-DST 0.. : . : —* S3-DST as our primary baseline, which is a zero-shot ver- fa) += S3-DST (No PAR) sion of the prompting strategy introduced by (Hu = et al., 2022), heavily adapted for open-domain dia-logue setting to jointly track segment and dialogue a states. The TBT-DST baseline is a version of S3- é DST that does not include segmentation instruc- 0.40 +, 7 x * (0, 3] (3, 5] (5, 10] (10, 20] tions and obtains intent and domain labels on a turn-by-turn basis using our S3-DST prompt configuration. Moreover, to analyze the importance of two key aspects of our prompt, PAR and XMLstructured formatting, we also consider two ablations of S3-DST: No PAR refers to a S3-DST prompt without the PAR instructions, and Unstructured input refers to a S3-DST prompt that formats all instructions and dialogue using plain text rather than XML. We use GPT4 as the backbone LLM for all prompts. MWOZ For MWOZ task-oriented dialogue state tracking dataset, we compare against IC-DST using Codex-175B as reported by Hu et al. (2022). We also reevaluate zero-shot IC-DST with GPT-to account for the backbone model improvement in baseline performance. Finally, we compare against the zero-shot ChatGPT performance on MWOZ 2.1 as reported by (Heck et al., 2023). DialSeg711 We consider the unsupervised TextTiling (Hearst, 1997), CSM (Xing and Carenini, 2021), and DialStart (Gao et al., 2023) methods. We reprint all numbers from (Gao et al., 2023). Finally, we use our IC-DST baseline prompted to elicit segmentation labels in the same SQL output format as the original IC-DST (Hu et al., 2022). 4.4 Metrics For state tracking, we consider Joint Goal Accuracy (JGA), which measures the proportion of turns for which all state values are correctly inferred. For Bing Chat, we report JGA with just Dialogue Length (# turns) Figure 3: S3-DST outperforms baselines for dialogues of all lengths by emphasizing context tracking. We bin Bing Chat dialogues by length and plot JGA per bin. The large performance degradation of both baselines as the dialogue length increases confirms the importance of our PAR grounding strategy. Table 3: S3-DST achieves state-of-the-art JGA compared to zero-shot LLM baselines on the public dialogue state tracking benchmarks MWoZ 2.1 + 2.4. JGA MWOZ 2.1 MWOZ 2.IC-DST (Codex) 0.3534 0.IC-DST (GPT4) 0.4045 0.ChatGPT 0.3150 S3-DST 0.4513 0.intent and domain (I/D) as these are the true state values of interest, as well as JGA with segment, intent, and domain accuracy (S/I/D) for completeness. We also report segmentation, intent, and domain accuracy separately on Bing Chat to provide a sense of the current capabilities and limitations of LLMs on open-domain conversational data. For segmentation, we consider Px and WindowDiff (Pevzner and Hearst, 2002), which are both error metrics (i.e., lower is better) that quantify the difference between predicted and ground-truth segment boundaries using an adjustable sliding window. --- --Table 4: Zero-shot per-domain comparison (JGA) on MWOZ 2.1. Per-domain JGA attr. hotel rest. taxi train 0.5997 0.4669 0.5728 0.7135 0.0.7177 0.4872 0.6526 0.7781 0.0.5270 0.4200 0.5580 0.7090 0.0.6781 0.5215 0.6713 0.8258 0.IC-DST (Codex) IC-DST (GPT4) ChatGPT S3-DST 4.5 Results Bing Chat As shown in Table 2, our S3-DST prompt achieves the highest performance across intent, domain, and JGA across turns. We make the following observations: First, TBT-DST, which does not explicitly perform segmentation, is by far our weakest baseline. We find that this is because without instructing the LLM to use the same intent and domain within a segment, the LLM tends to overindex on the content of the turn without considering the fuller preceding context. This leads to conflicting intent and domain labels between turns within a coherent single-topic dialogue. Second, our adapted version of IC-DST is a very strong baseline. However, while IC-DST makes use of structured outputs, it does not have a corresponding structured input representation. We find that this hurts its performance in some cases, as hallucination of nonexistent turns is relatively more common compared to S3-DST. Finally, the two ablations of S3-DST both underperform compared to $3-DST, confirming the importance of PAR and structured inputs that the LLM can refer back to during generation. Indeed, Figure 3, which plots the relationship between diaogue length and performance, shows that S3-DST avoids the steep degradation in performance of the no-PAR ablation as the dialogues get longer. For example, the no-PAR ablation performs comparably to S3-DST on conversations of 3 turns or fewer, but drops over 10 points JGA for conversations ofurns or more. These results in particular highlight the necessity of PAR for long dialogues. MWOZ Tables 3 and 4 provide MWOZ numbers in total and per-domain. S3-DST achieves state-ofhe-art zero-shot JGA compared to strong LLMs by a large margin. Even our strongest zero-shot baseine, IC-DST (GPT4), has an absolute performance gap of nearly 5 points JGA on MWOZ 2.1 andpoints on MWOZ 2.4. In nearly all individual domains, S3-DST outperforms IC-DST (GPT4), and Table 5: S3-DST achieves state-of-the-art performance on the public segmentation benchmark DialSeg711. P,(1) WindowDiff (|) TextTiling 0.4044 0.CSM 0.2430 0.DialSTART 0.1786 0.IC-DST 0.2889 0.S3-DST 0.0091 0.some by a large margin, for example over 13 points JGA improvement on the train domain. DialSeg711_ Finally, Table 5 shows performance on DialSeg711. S3-DST achieves nearly zero error on this dataset, which we find unsurprising given that the dataset’s construction. Specifically, DialSeg711 is constructed by joining dialogues about very different topics, which leads to very artificial and abrupt context shifts between segments. However, we find that our IC-DST prompting baseline leads to much higher error than S3-DST. On further inspection, we find that the LLM fails to track the dialogue context for several conversations in the dataset, leading to forgetting of the original conversation context. These results highlight the importance of PAR and dialogue context tracking for successful segmentation. S3-DST’s strong performance also suggests that DialSeg711 may not be a difficult enough task in future for LLMs, and further motivates the need for joint segmentation and state tracking, as the goal of segmentation is ultimately to improve state tracking performance. 5
--- RELATED WORK ---
5.1 Dialogue State Tracking To accurately track the passage of Human-AI conversation, robust state tracking is crucial toward inferring user intentions and goals. Since the introduction of the MultiWOZ (Budzianowski et al., 2018) dataset to the community, a plethora of techniques have been proposed to improve DST performance. Earlier attempts including copy mechanism (Lei et al., 2018), transfer learning (Wu et al., 2019), data augmentation (Zhang et al., 2020), contrastive pretraining (Wu et al., 2020), etc. have yielded improvements in supervised finetuning scenarios; meanwhile, MultiWOZ also went through several annotation revisions (Eric et al., 2019; Ye et al., 2021; Zang et al., 2020; Han et al., 2020). While other techniques (Peng et al., 2021; Lin et al., 2020; Zhao et al., 2022; Yu et al., 2020; --- --Platanios et al., 2021) have also been proposed, the resource-intensive and laborious nature of data labeling has gradually redirected attention toward the exploration of few- and zero-shot dialogue state tracking (Shin et al., 2022; Hu et al., 2022; Heck et al., 2023). While the state-of-the-art approach in this discipline (Hu et al., 2022) can leverage LLMs for tracking states, it notably lacks proper grounding mechanisms which can potentially hurt performance in real-world extended dialogue sessions. Furthermore, none of the aforementioned previous work accounts for topic coherence and context switches prevalent in flexible open-domain LLM-based chat systems. 5.2. Dialogue Topic Segmentation Segmenting a dialogue into topically coherent units is foundational to successful downstream dialogue modeling. While the paucity of annotated data has been a challenge in dialogue topic segmentation, recent unsupervised attempts have exhibited some promising outcomes in topic segmentation. More specifically, extensions based on the classical text segmentation algorithm TextTiling (Hearst, 1997) have primarily led the benchmark in this aspect (Song et al., 2016). More recently, textpair coherence scoring (Xing and Carenini, 2021) and topic-aware representation learning (Gao et al., 2023) have advanced the state of the art. Nevertheless, all of these techniques fall short in accounting for the complete contextual essence of a conversation (i.e., explicitly modeling intent and other important state variables), which can lead to suboptimal results. 5.3 Intent Classification Related to dialogue state tracking, another fundamental problem in task-oriented dialogue systems is intent classification (IC). Often paired with another complementary problem slot-filling (SF), researchers have proposed a wide range of techniques over the years (Liu and Lane, 2016; Zhang and Wang, 2016; Goo et al., 2018; Qin et al., 2019, 2021), achieving impressive performance in popular public datasets. Few-shot techniques have also been investigated in data-constrained scenarios for joint IC/SF task (Krone et al., 2020; Bhathiya and Thayasivam, 2020; Liu et al., 2021). While related to DST, IC/SF primarily deals with individual utterances in isolation, which makes it less apt for real-world human-AI dialogue which often requires modeling intricate contextual connections spanning multiple utterances within a conversational session. 6 Discussion and Conclusion LLM-based chat systems have broadened the horizons of human-AI conversation, warranting new
--- METHOD ---
s extract new elements of state at each turn (Hu et al., 2022). However, this is because DST evaluation benchmarks make the relatively narrow assumption that users provide new and relevant elements of intent at each turn, and that intents build upon or complement each other but do not fundamentally change or conflict throughout the conversation. As we have previously discussed, open-domain dialogue exhibits far more varied characteristics, and multiintent and/or multi-domain conversations are relatively common. We therefore propose to extract state at the segment rather than turn level. We define the segment-level state as {Sinin = (s@n, wen), i = 1...Nm:n}, where 3), refers to the i-th slot applied to the segment from boundaries by, to bn, vn refers to the slot’s corresponding value, and Nm:n tefers to the total number of slots to applied to this segment. Any schema of slot-value pairs is valid here; we describe our particular state schema for Bing Chat in § 4.1 and Appendix B. 2.3 Problem statement Having defined segments and per-segment state, we are equipped to state our full definition of opendomain DST. Given a sequence of user-agent utterance pairs C; = [U), Aj,...,U;, Ar], we define the goal of open-domain dialogue state tracking as jointly predicting yt = Be U {Simin V(b, bn) € Bu}, (1) where B;, C B; refers to the segment boundary indices described earlier and Sip:, refers to the segment state between boundaries b,, and b,,, consisting of N arbitrary slot-value pairs: Sinn = {(8 On Ven), b= 1..-Nmin} (2) 3 Prompting Strategies As discussed previously, real-world dialogues often exhibit extensive discourse that extends over multiple conversational turns in order to discuss diverse topics. This prolonged conversational nature makes it highly challenging to track contextual coherence. Previous studies (Hu et al., 2022) aimed at disassociating individual dialogue turns and processing them one by one for tracking dialogue state changes, which worked reasonably well in task-oriented dialogues confined within predefined narrow domains. However, real-world dialogues commonly require multiple turns to adequately comprehend the contextual nuances, which is a challenge because Transformers still struggle when processing lengthy input contexts, particularly in the middle (Liu et al., 2023). To address these difficulties, we propose a novel turn-by-turn prompting technique that gives structure to inputs and outputs while accurately preserving the context in the process. We discuss these design aspects of our prompts below: 3.1 Structured Outputs and Inputs Structured Output Our goal is a set of labels per dialogue turn representing the segment boundaries (binary labels) and state values (categorical labels or open text). To provide a flexible yet structured format to the LLM’s output, we propose to instruct it to generate outputs in a hierarchical XML format. We see XML as advantageous because it provides code-like structure to the DST task, which has been shown to greatly improve performance compared to plain-text outputs, while still being extensible and flexible compared to more rigid output formats like SQL (Hu et al., 2022). Our approach uses an XML format in which each turn from 1 to ¢ comprises an XML tree <T{id}>...</T{id}> and several nested XML tags within it. The labels of these nested tags (e.g. <preceding_topical_relation>...</preceding_topical_relation>, <intent>...</intent>, and <domain>. . .</domain> in Figure 2(iii)) represent the segment boundaries and slots of interest, and each value between opening and closing tags represent the model’s inferred value. This strategy is beneficial from two fronts: (i) Due to bounded well-defined structured formatting, generated outputs are more likely to be aligned with labeling instructions than free-form texts, and (ii) Well-formed structured output formats are easier to parse, thus reducing postprocessing requirements. Structured Input For prompting LLMs, although it is trivial to channel plain conversation history in a flat format for analysis and inference, the unstructured nature inherent to this linear configuration makes it difficult to refer back and lever --- --(i) Insert conversation and instructions into structured prompt template <valid_preceding_topical_relation> <item> <name>YES</name> <description>...</description> </item> Raw conversation User: Please provide an annotated <valid_intents> <item>...</item> </valid_intents> bibliography of The... AI: Here are a couple of different bibliographies of... User: ... ### CONVERSATION AI: ... Ge a <User>Please provide an...</User> <Al>Here are a couple of...</AI> </TL> Structured Turn <User>...</User> <AI>...</AI> </T2> </valid_preceding_topical_relation> =» wt] <T2> —_E_”__ ll (ii) Pre-Analytical Recollection (PAR) Preserve context by grounding output in each turn (iii) Generate structured turn-by-turn dialogue tags via chain of thought <T1> <summary>The user requests an annotated. ..</summary> <preceding_topical_relation>N0</preceding_topical_relation> <intent>CREATION</intent> <domain>WRITING JOURNALISM AND PUBLISHING</domain> </T1> <T2> <summary>. ..</summary> <preceding_topical_relation>...</preceding_topical_relation> <intent>...</intent> <domain>. . -</domain> | </T2> | Figure 2: Prompt flow of S3-DST. Given a raw conversation, (i) we convert it into a hierarchical XML-structured representation and insert it into a similarly structured prompt template. We pass the prompt through the LLM and (ii) obtain a hierarchical XML-structured output, where each turn contains (iii) a PAR grounding reference to the conversation alongside the desired segmentation and state label predictions. age different information across multiple conversational turns. To handle this challenge, consistent with the output format, we propose a structured inputting format, where each conversational history is formed into a hierarchical XML format where conversational turns are marked with turn id number <T{id}>. ..</T{id}> numbered from 1 to t and each conversational turn consists of nested user and agent turns marked with appropriate XML tags (<user>...</user> and <agent>...</agent>). Since we propose instructing the LLM to infer per-turn labels during our output, this input scheme helps us accurately refer back to the input turn and thus maintain coherence even for long dialogue contexts. Consistent with this XML-tagged input format, we also format all the valid segment and state categories in an XML-formatted list using the following structure: <valid_category_name> <item>{label name}</item><description> {description of label, if available} </description> <valid_category_name> Empirically, this structured input and prompt formatting help constrain the LLM generation to follow the labeling instructions. Figure 2(i) shows this format where each valid segment boundary and state category are first staged in an XML-formatted list and subsequently input dialogue is shown in a hierarchical configuration. 3.2 Pre-Analytical Recollection (PAR) As previously discussed, open-domain dialogues may be long and highly variable in conversation flow. Therefore, it is crucial to ensure that the LLM can accurately monitor the evolving dialogue context without forgetting or hallucination. To this end, we propose Pre-Analytical Recollection (PAR), a grounding strategy for turn-by-turn prompting that instructs the LLM to first summarize the turn using <summary>. ..</summary> tags in 3 sentences or fewer before providing the segment and state values. PAR is inspired by chain-of-thought prompting (Wei et al., 2022), as it is a technique for generating relevant intermediary outputs in order to improve reasoning accuracy. However, unlike chain-of-thought, PAR is also a grounding technique that provides references from the model’s output directly to the conversation. Figure 2(ii) demonstrates how PAR refers back to the content of each conversational turn before analyzing it to infer the conversational states. 3.3 Final Prompt Configuration The final prompt flow of S3-DST is provided in Figure 2. Given a raw conversation and a predefined set of segment and state labels, we insert the labels into a structured prompt template and format the conversation in a hierarchical XML-structured representation. We pass the prompt through the LLM, instructing it to follow PAR before jointly --- --Table 1: Evaluation test set statistics. #Convs #Turns #segments/conv (avg.) Bing Chat 334 2308 1.MWOZ 2.1 1,000 7368 MWOZ 2.4 1,000 7368 DialSeg711 711 19350 3.generating the hierarchical turn-by-turn segmentation and state labels applied per segment. The full text of our prompt is provided in Appendix A.1. 4
--- EXPERIMENT ---
s and analysis: We conduct extensive experiments on both proprietary and public datasets, achieving large gains over comparable zero-shot prompts. S3DST achieves state-of-the-art zero-shot per 'The use of Bing Chat logs is in compliance with the terms formance on the MWOZ 2.1 and 2.4 DST benchmarks, alongside the DialSeg711 dialogue topic segmentation benchmark. 2 Problem Definition Informally, the goal of traditional DST is to predict the dialogue state y, given a sequence of user and agent utterance turns C; = [U1, A1,...,Ut, Ajj? The state y, consists of a set of slot-value pairs, where slots correspond to intent attributes in a particular application domain (e.g., “restaurantname”, “hotel-address’”’) and values correspond to predefined categorical options or unconstrained text (Budzianowski et al., 2018). However, as we have previously discussed, a single open-domain conversation will often consist of multiple potentially unrelated intents across a variety of topics. Indeed, according to a preliminary analysis on 10K anonymized Bing Chat conversations, we estimate that over 50% of conversations display multiple user intents and over 90% of conversations contain discussion of multiple topics. Therefore, we propose to merge dialogue segmentation, which aims to find contextually cohesive “units” of dialogue within a larger conversation, with dialogue state tracking. In particular, we perform state tracking at the segment level, where the goal is to label each segment with the slots and values of interest, such that multiple segments within a conversation may have diverging or conflicting state values, reflecting the true variety of open-domain chat. In the rest of this section, we define segmentation and state, and finally formalize the joint task. 2.1 Segment Following previous work in dialogue topic segmentation (Xing and Carenini, 2021; Xia et al., 2022; Gao et al., 2023), we define dialogue segments as contiguous subsequences of C; in which all user and agent utterances are topically related. Formally, let By = [b1,...,b:—-1] indicate the boundary indices between adjacent user-agent utterance pairs in C;. The output of segmentation is a set of boundary indices By, C B;, where k represents the number of boundaries determined by the segmentation algorithm and the span [Um, Am, ... Un, An] repreof use of Bing Chat. ?Note that in current LLM-based chat systems, users may issue multiple utterances before a single agent response is is sued. In these (infrequent) cases, we group all user utterances prior to the agent response into a single utterance. --- --sents the contiguous segment between boundaries bm and b,,, where m € [1,t—1] andn € [m,t—1]. 2.2 Segment state Typically, dialogue state tracking methods extract new elements of state at each turn (Hu et al., 2022). However, this is because DST evaluation benchmarks make the relatively narrow assumption that users provide new and relevant elements of intent at each turn, and that intents build upon or complement each other but do not fundamentally change or conflict throughout the conversation. As we have previously discussed, open-domain dialogue exhibits far more varied characteristics, and multiintent and/or multi-domain conversations are relatively common. We therefore propose to extract state at the segment rather than turn level. We define the segment-level state as {Sinin = (s@n, wen), i = 1...Nm:n}, where 3), refers to the i-th slot applied to the segment from boundaries by, to bn, vn refers to the slot’s corresponding value, and Nm:n tefers to the total number of slots to applied to this segment. Any schema of slot-value pairs is valid here; we describe our particular state schema for Bing Chat in § 4.1 and Appendix B. 2.3 Problem statement Having defined segments and per-segment state, we are equipped to state our full definition of opendomain DST. Given a sequence of user-agent utterance pairs C; = [U), Aj,...,U;, Ar], we define the goal of open-domain dialogue state tracking as jointly predicting yt = Be U {Simin V(b, bn) € Bu}, (1) where B;, C B; refers to the segment boundary indices described earlier and Sip:, refers to the segment state between boundaries b,, and b,,, consisting of N arbitrary slot-value pairs: Sinn = {(8 On Ven), b= 1..-Nmin} (2) 3 Prompting Strategies As discussed previously, real-world dialogues often exhibit extensive discourse that extends over multiple conversational turns in order to discuss diverse topics. This prolonged conversational nature makes it highly challenging to track contextual coherence. Previous studies (Hu et al., 2022) aimed at disassociating individual dialogue turns and processing them one by one for tracking dialogue state changes, which worked reasonably well in task-oriented dialogues confined within predefined narrow domains. However, real-world dialogues commonly require multiple turns to adequately comprehend the contextual nuances, which is a challenge because Transformers still struggle when processing lengthy input contexts, particularly in the middle (Liu et al., 2023). To address these difficulties, we propose a novel turn-by-turn prompting technique that gives structure to inputs and outputs while accurately preserving the context in the process. We discuss these design aspects of our prompts below: 3.1 Structured Outputs and Inputs Structured Output Our goal is a set of labels per dialogue turn representing the segment boundaries (binary labels) and state values (categorical labels or open text). To provide a flexible yet structured format to the LLM’s output, we propose to instruct it to generate outputs in a hierarchical XML format. We see XML as advantageous because it provides code-like structure to the DST task, which has been shown to greatly improve performance compared to plain-text outputs, while still being extensible and flexible compared to more rigid output formats like SQL (Hu et al., 2022). Our approach uses an XML format in which each turn from 1 to ¢ comprises an XML tree <T{id}>...</T{id}> and several nested XML tags within it. The labels of these nested tags (e.g. <preceding_topical_relation>...</preceding_topical_relation>, <intent>...</intent>, and <domain>. . .</domain> in Figure 2(iii)) represent the segment boundaries and slots of interest, and each value between opening and closing tags represent the model’s inferred value. This strategy is beneficial from two fronts: (i) Due to bounded well-defined structured formatting, generated outputs are more likely to be aligned with labeling instructions than free-form texts, and (ii) Well-formed structured output formats are easier to parse, thus reducing postprocessing requirements. Structured Input For prompting LLMs, although it is trivial to channel plain conversation history in a flat format for analysis and inference, the unstructured nature inherent to this linear configuration makes it difficult to refer back and lever --- --(i) Insert conversation and instructions into structured prompt template <valid_preceding_topical_relation> <item> <name>YES</name> <description>...</description> </item> Raw conversation User: Please provide an annotated <valid_intents> <item>...</item> </valid_intents> bibliography of The... AI: Here are a couple of different bibliographies of... User: ... ### CONVERSATION AI: ... Ge a <User>Please provide an...</User> <Al>Here are a couple of...</AI> </TL> Structured Turn <User>...</User> <AI>...</AI> </T2> </valid_preceding_topical_relation> =» wt] <T2> —_E_”__ ll (ii) Pre-Analytical Recollection (PAR) Preserve context by grounding output in each turn (iii) Generate structured turn-by-turn dialogue tags via chain of thought <T1> <summary>The user requests an annotated. ..</summary> <preceding_topical_relation>N0</preceding_topical_relation> <intent>CREATION</intent> <domain>WRITING JOURNALISM AND PUBLISHING</domain> </T1> <T2> <summary>. ..</summary> <preceding_topical_relation>...</preceding_topical_relation> <intent>...</intent> <domain>. . -</domain> | </T2> | Figure 2: Prompt flow of S3-DST. Given a raw conversation, (i) we convert it into a hierarchical XML-structured representation and insert it into a similarly structured prompt template. We pass the prompt through the LLM and (ii) obtain a hierarchical XML-structured output, where each turn contains (iii) a PAR grounding reference to the conversation alongside the desired segmentation and state label predictions. age different information across multiple conversational turns. To handle this challenge, consistent with the output format, we propose a structured inputting format, where each conversational history is formed into a hierarchical XML format where conversational turns are marked with turn id number <T{id}>. ..</T{id}> numbered from 1 to t and each conversational turn consists of nested user and agent turns marked with appropriate XML tags (<user>...</user> and <agent>...</agent>). Since we propose instructing the LLM to infer per-turn labels during our output, this input scheme helps us accurately refer back to the input turn and thus maintain coherence even for long dialogue contexts. Consistent with this XML-tagged input format, we also format all the valid segment and state categories in an XML-formatted list using the following structure: <valid_category_name> <item>{label name}</item><description> {description of label, if available} </description> <valid_category_name> Empirically, this structured input and prompt formatting help constrain the LLM generation to follow the labeling instructions. Figure 2(i) shows this format where each valid segment boundary and state category are first staged in an XML-formatted list and subsequently input dialogue is shown in a hierarchical configuration. 3.2 Pre-Analytical Recollection (PAR) As previously discussed, open-domain dialogues may be long and highly variable in conversation flow. Therefore, it is crucial to ensure that the LLM can accurately monitor the evolving dialogue context without forgetting or hallucination. To this end, we propose Pre-Analytical Recollection (PAR), a grounding strategy for turn-by-turn prompting that instructs the LLM to first summarize the turn using <summary>. ..</summary> tags in 3 sentences or fewer before providing the segment and state values. PAR is inspired by chain-of-thought prompting (Wei et al., 2022), as it is a technique for generating relevant intermediary outputs in order to improve reasoning accuracy. However, unlike chain-of-thought, PAR is also a grounding technique that provides references from the model’s output directly to the conversation. Figure 2(ii) demonstrates how PAR refers back to the content of each conversational turn before analyzing it to infer the conversational states. 3.3 Final Prompt Configuration The final prompt flow of S3-DST is provided in Figure 2. Given a raw conversation and a predefined set of segment and state labels, we insert the labels into a structured prompt template and format the conversation in a hierarchical XML-structured representation. We pass the prompt through the LLM, instructing it to follow PAR before jointly --- --Table 1: Evaluation test set statistics. #Convs #Turns #segments/conv (avg.) Bing Chat 334 2308 1.MWOZ 2.1 1,000 7368 MWOZ 2.4 1,000 7368 DialSeg711 711 19350 3.generating the hierarchical turn-by-turn segmentation and state labels applied per segment. The full text of our prompt is provided in Appendix A.1. 4 Experiments We conduct comprehensive evaluations across multiple datasets. We primarily evaluate our approach on fully anonymized Bing Chat logs annotated by domain experts. Additionally, we evaluate S3-DST on the standard task-oriented DST and segmentation tasks using public benchmark datasets MultiWOZ (Budzianowski et al., 2018) and DialSeg(Xu et al., 2021) respectively. A detailed description of these datasets is provided below, alongside dataset statistics in Table 1: 4.1 Internal Human-LLM Dialogue Dataset In order to evaluate the efficacy of our approach on real-world open-domain human-LLM conversations, we collected anonymized chat log data from Microsoft’s Bing Chat system, an LLM chat interface backed by the Bing search engine. Benchmark construction We sample 484 English conversations conducted on Bing Chat between April 5, 2023 to April 30, 2023 via two approaches: (i) Random and (ii) “Long” conversations of 5 or more turns only. We balance these two approaches 50/50. Since we operate under a zero-shot assumption, we do not need any training data. Therefore, we hold out 150 conversations for development and the remaining 334 for testing. Annotation To obtain ground-truth labels for evaluation, we gathered human annotations for segment and state. We recruited three in-house annotators with a high degree of technical expertise and familiarity with the Bing Chat system. For each turn, we instructed annotators to provide binary IsSegmentBoundary labels, categorical SegmentIntent labels, and categorical SegmentDomain labels. We instructed annotators to mark a segment boundary when no topical relation between a turn and its preceding context could be identified. For intent and domain, we used taxonomies developed in-house for the Bing Chat system consisting of 4 intents (Information Seeking, Analysis, Creation, and Open-Ended Discovery) and 49 domains (see Appendix B.1 for the full list). Because of the large number of domains, per turn we provided annotators four candidate domain values and an “Other” option. Appendix B provides further details on the annotation scheme and domain sampling procedure. To ensure interannotator agreement before labeling the full dataset, we first gathered annotations on a set of 10 randomly selected conversations (68 turns total) and computed Fleiss’ kappa (Fleiss, 1971) per label type. We observed a Fleiss kappa of & = 0.83 for IsSegmentBoundary, « = 0.74 for SegmentIntent, and « = 0.88 for SegmentDomain, all of which are considered high agreement on the Fleiss kappa scale. 4.2 Public Benchmarks We are not aware of any existing public dialogue benchmarks reflective of the broadly open-domain Bing Chat data. Therefore, we resort to separate DST and segmentation evaluations on public benchmarks using three datasets. MultiWOZ The MultiWOZ (MWOZ) multidomain dialogue dataset (Budzianowski et al., 2018) is currently the most common DST benchmark. MWOZ is a task-oriented dataset consisting of 1K test dialogues. We use two updated versions of the original: MWOZ 2.1 (Eric et al., 2019) and 2.4 (Ye et al., 2021). The latter is considered the “cleanest” version of MWOZ, while the former has been used more frequently in the literature. DialSeg711 The DialSeg711 benchmark was introduced by (Xu et al., 2021) and has been used frequently in recent dialogue segmentation research. It is an English dataset in which 711 multi-segment dialogues are constructed by joining dialogues from existing task-oriented dialogue corpora. 4.3 Baselines As baselines we consider zero-shot LLM prompts only, for a fair comparison to S$3-DST. We discuss the baselines and their considerations below for different datasets. All original prompts are provided in Appendix A. We set a maximum of 1500 output tokens per LLM call with a temperature of zero. --- --Table 2: S3-DST achieves state-of-the-art performance on state tracking over our internal Bing Chat benchmark. All prompts are run with GPT4. Individual accuracy JGA Segment Intent Domain YD S//D TBT-DST - 0.6707 0.6221 0.4169 IC-DST 0.8567 0.7123 0.6049 0.4610 0.S3-DST (No PAR) 0.8859 0.7173 —:0.6251 0.4377 0.S3-DST (Unstructured input) 0.8810 0.7163 0.6307 0.4640 0.S3-DST 0.8992 0.7366 0.6429 0.4752 0.Bing Chat In this dataset, we consider IC-DST 0.. : . : —* S3-DST as our primary baseline, which is a zero-shot ver- fa) += S3-DST (No PAR) sion of the prompting strategy introduced by (Hu = et al., 2022), heavily adapted for open-domain dia-logue setting to jointly track segment and dialogue a states. The TBT-DST baseline is a version of S3- é DST that does not include segmentation instruc- 0.40 +, 7 x * (0, 3] (3, 5] (5, 10] (10, 20] tions and obtains intent and domain labels on a turn-by-turn basis using our S3-DST prompt configuration. Moreover, to analyze the importance of two key aspects of our prompt, PAR and XMLstructured formatting, we also consider two ablations of S3-DST: No PAR refers to a S3-DST prompt without the PAR instructions, and Unstructured input refers to a S3-DST prompt that formats all instructions and dialogue using plain text rather than XML. We use GPT4 as the backbone LLM for all prompts. MWOZ For MWOZ task-oriented dialogue state tracking dataset, we compare against IC-DST using Codex-175B as reported by Hu et al. (2022). We also reevaluate zero-shot IC-DST with GPT-to account for the backbone model improvement in baseline performance. Finally, we compare against the zero-shot ChatGPT performance on MWOZ 2.1 as reported by (Heck et al., 2023). DialSeg711 We consider the unsupervised TextTiling (Hearst, 1997), CSM (Xing and Carenini, 2021), and DialStart (Gao et al., 2023) methods. We reprint all numbers from (Gao et al., 2023). Finally, we use our IC-DST baseline prompted to elicit segmentation labels in the same SQL output format as the original IC-DST (Hu et al., 2022). 4.4 Metrics For state tracking, we consider Joint Goal Accuracy (JGA), which measures the proportion of turns for which all state values are correctly inferred. For Bing Chat, we report JGA with just Dialogue Length (# turns) Figure 3: S3-DST outperforms baselines for dialogues of all lengths by emphasizing context tracking. We bin Bing Chat dialogues by length and plot JGA per bin. The large performance degradation of both baselines as the dialogue length increases confirms the importance of our PAR grounding strategy. Table 3: S3-DST achieves state-of-the-art JGA compared to zero-shot LLM baselines on the public dialogue state tracking benchmarks MWoZ 2.1 + 2.4. JGA MWOZ 2.1 MWOZ 2.IC-DST (Codex) 0.3534 0.IC-DST (GPT4) 0.4045 0.ChatGPT 0.3150 S3-DST 0.4513 0.intent and domain (I/D) as these are the true state values of interest, as well as JGA with segment, intent, and domain accuracy (S/I/D) for completeness. We also report segmentation, intent, and domain accuracy separately on Bing Chat to provide a sense of the current capabilities and limitations of LLMs on open-domain conversational data. For segmentation, we consider Px and WindowDiff (Pevzner and Hearst, 2002), which are both error metrics (i.e., lower is better) that quantify the difference between predicted and ground-truth segment boundaries using an adjustable sliding window. --- --Table 4: Zero-shot per-domain comparison (JGA) on MWOZ 2.1. Per-domain JGA attr. hotel rest. taxi train 0.5997 0.4669 0.5728 0.7135 0.0.7177 0.4872 0.6526 0.7781 0.0.5270 0.4200 0.5580 0.7090 0.0.6781 0.5215 0.6713 0.8258 0.IC-DST (Codex) IC-DST (GPT4) ChatGPT S3-DST 4.5 Results Bing Chat As shown in Table 2, our S3-DST prompt achieves the highest performance across intent, domain, and JGA across turns. We make the following observations: First, TBT-DST, which does not explicitly perform segmentation, is by far our weakest baseline. We find that this is because without instructing the LLM to use the same intent and domain within a segment, the LLM tends to overindex on the content of the turn without considering the fuller preceding context. This leads to conflicting intent and domain labels between turns within a coherent single-topic dialogue. Second, our adapted version of IC-DST is a very strong baseline. However, while IC-DST makes use of structured outputs, it does not have a corresponding structured input representation. We find that this hurts its performance in some cases, as hallucination of nonexistent turns is relatively more common compared to S3-DST. Finally, the two ablations of S3-DST both underperform compared to $3-DST, confirming the importance of PAR and structured inputs that the LLM can refer back to during generation. Indeed, Figure 3, which plots the relationship between diaogue length and performance, shows that S3-DST avoids the steep degradation in performance of the no-PAR ablation as the dialogues get longer. For example, the no-PAR ablation performs comparably to S3-DST on conversations of 3 turns or fewer, but drops over 10 points JGA for conversations ofurns or more. These results in particular highlight the necessity of PAR for long dialogues. MWOZ Tables 3 and 4 provide MWOZ numbers in total and per-domain. S3-DST achieves state-ofhe-art zero-shot JGA compared to strong LLMs by a large margin. Even our strongest zero-shot baseine, IC-DST (GPT4), has an absolute performance gap of nearly 5 points JGA on MWOZ 2.1 andpoints on MWOZ 2.4. In nearly all individual domains, S3-DST outperforms IC-DST (GPT4), and Table 5: S3-DST achieves state-of-the-art performance on the public segmentation benchmark DialSeg711. P,(1) WindowDiff (|) TextTiling 0.4044 0.CSM 0.2430 0.DialSTART 0.1786 0.IC-DST 0.2889 0.S3-DST 0.0091 0.some by a large margin, for example over 13 points JGA improvement on the train domain. DialSeg711_ Finally, Table 5 shows performance on DialSeg711. S3-DST achieves nearly zero error on this dataset, which we find unsurprising given that the dataset’s construction. Specifically, DialSeg711 is constructed by joining dialogues about very different topics, which leads to very artificial and abrupt context shifts between segments. However, we find that our IC-DST prompting baseline leads to much higher error than S3-DST. On further inspection, we find that the LLM fails to track the dialogue context for several conversations in the dataset, leading to forgetting of the original conversation context. These results highlight the importance of PAR and dialogue context tracking for successful segmentation. S3-DST’s strong performance also suggests that DialSeg711 may not be a difficult enough task in future for LLMs, and further motivates the need for joint segmentation and state tracking, as the goal of segmentation is ultimately to improve state tracking performance. 5 Related Work 5.1 Dialogue State Tracking To accurately track the passage of Human-AI conversation, robust state tracking is crucial toward inferring user intentions and goals. Since the introduction of the MultiWOZ (Budzianowski et al., 2018) dataset to the community, a plethora of techniques have been proposed to improve DST performance. Earlier attempts including copy mechanism (Lei et al., 2018), transfer learning (Wu et al., 2019), data augmentation (Zhang et al., 2020), contrastive pretraining (Wu et al., 2020), etc. have yielded improvements in supervised finetuning scenarios; meanwhile, MultiWOZ also went through several annotation revisions (Eric et al., 2019; Ye et al., 2021; Zang et al., 2020; Han et al., 2020). While other techniques (Peng et al., 2021; Lin et al., 2020; Zhao et al., 2022; Yu et al., 2020; --- --Platanios et al., 2021) have also been proposed, the resource-intensive and laborious nature of data labeling has gradually redirected attention toward the exploration of few- and zero-shot dialogue state tracking (Shin et al., 2022; Hu et al., 2022; Heck et al., 2023). While the state-of-the-art approach in this discipline (Hu et al., 2022) can leverage LLMs for tracking states, it notably lacks proper grounding mechanisms which can potentially hurt performance in real-world extended dialogue sessions. Furthermore, none of the aforementioned previous work accounts for topic coherence and context switches prevalent in flexible open-domain LLM-based chat systems. 5.2. Dialogue Topic Segmentation Segmenting a dialogue into topically coherent units is foundational to successful downstream dialogue modeling. While the paucity of annotated data has been a challenge in dialogue topic segmentation, recent unsupervised attempts have exhibited some promising outcomes in topic segmentation. More specifically, extensions based on the classical text segmentation algorithm TextTiling (Hearst, 1997) have primarily led the benchmark in this aspect (Song et al., 2016). More recently, textpair coherence scoring (Xing and Carenini, 2021) and topic-aware representation learning (Gao et al., 2023) have advanced the state of the art. Nevertheless, all of these techniques fall short in accounting for the complete contextual essence of a conversation (i.e., explicitly modeling intent and other important state variables), which can lead to suboptimal results. 5.3 Intent Classification Related to dialogue state tracking, another fundamental problem in task-oriented dialogue systems is intent classification (IC). Often paired with another complementary problem slot-filling (SF), researchers have proposed a wide range of techniques over the years (Liu and Lane, 2016; Zhang and Wang, 2016; Goo et al., 2018; Qin et al., 2019, 2021), achieving impressive performance in popular public datasets. Few-shot techniques have also been investigated in data-constrained scenarios for joint IC/SF task (Krone et al., 2020; Bhathiya and Thayasivam, 2020; Liu et al., 2021). While related to DST, IC/SF primarily deals with individual utterances in isolation, which makes it less apt for real-world human-AI dialogue which often requires modeling intricate contextual connections spanning multiple utterances within a conversational session. 6 Discussion and
--- CONCLUSION ---
LLM-based chat systems have broadened the horizons of human-AI conversation, warranting new methods for tracking user intentions. Therefore, we bring dialogue state tracking in the realm of open-domain dialogue systems by jointly tracking topically coherent segments and state intent variables per segment. Since this requires the assumption of a zero-shot setting due to the impracticality of annotation across all disciplines, we propose S3-DST, a structured segmentation and state tracking approach using zero-shot prompting for open-domain state tracking. S3-DST structures the prompt in an XML format and leverages our proposed grounding mechanism (PAR) for long context tracking. Across extensive experiments on proprietary and public datasets, S3-DST shows large performance gains over state-of-the-art zeroshot techniques in dialogue state tracking and segmentation approaches. In the future, as LLM-based chat systems become more prevalent, we expect dialogue systems research to shift further toward understanding and modeling open-domain dialogue. In this respect, we aim to further study and develop techniques for extended context preservation, to improve grounding in DST alongside other important dialogue modeling tasks. References Hemanthage S Bhathiya and Uthayasanker Thayasivam. 2020. Meta learning for few-shot joint intent detection and slot-filling. In Proceedings of theSth International Conference on Machine Learning Technologies, pages 86-92. Pawet Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Ifigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016-5026. Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur. 2019. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. arXiv preprint arXiv: 1907.01669. Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378. --- --Haoyu Gao, Rui Wang, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, and Yongbin Li. 2023. Unsupervised dialogue topic segmentation with topic-aware utterance representation. In Proceedings of the 46th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational ai. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 13711374. Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen. 2018. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of theConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 753-757. Ting Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian, Chongxuan Huang, Wei Peng, and Minlie Huang. 2020. Multiwoz 2.3: A multi-domain taskoriented dataset enhanced with annotation corrections and co-reference annotation. arXiv preprint arXiv:2010.05594. Marti A Hearst. 1997. Text tiling: Segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1):33-64. Michael Heck, Nurul Lubis, Benjamin Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, Hsienchin Lin, Carel van Niekerk, and Milica Gasic. 2023. ChatGPT for zero-shot dialogue state tracking: A solution or an opportunity? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 936-950, Toronto, Canada. Association for Computational Linguistics. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2627-2643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Krone, Yi Zhang, and Mona Diab. 2020. Learning to classify intents and slot labels given a handful of examples. arXiv preprint arXiv:2004.10793. Wengiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei Yin. 2018. Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1437-1447. Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale Fung. 2020. Mintl: Minimalist transfer learning for task-oriented dialogue systems. arXiv preprint arXiv:2009.12005. Bing Liu and Jan Lane. 2016. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv: 1609.01454. Han Liu, Feng Zhang, Xiaotong Zhang, Siyang Zhao, and Xianchao Zhang. 2021. An explicit-joint and supervised-contrastive learning framework for fewshot intent classification and slot filling. arXiv preprint arXiv:2110.13691. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2021. Soloist: Building task bots at scale with transfer learning and machine teaching. Transactions of the Association for Computational Linguistics, 9:807-824. Lev Pevzner and Marti A Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):1936. Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Wolfe, Jacob Andreas, and Dan Klein. 2021. Value-agnostic conversational semantic parsing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3666-3681, Online. Association for Computational Linguistics. Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, and Ting Liu. 2019. A stack-propagation framework with token-level intent detection for spoken language understanding. arXiv preprint arXiv: 1909.02188. Libo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang, Sendong Zhao, and Ting Liu. 2021. A co-interactive transformer for joint slot filling and intent detection. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8193-8197. IEEE. Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, and Juneyoung Park. 2022. Dialogue summaries as dialogue states (DS2), template-guided summarization for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3824-3846, Dublin, Ireland. Association for Computational Linguistics. Yiping Song, Lili Mou, Rui Yan, Li Yi, Zinan Zhu, Xiaohua Hu, and Ming Zhang. 2016. Dialogue session segmentation by embedding-enhanced texttiling. arXiv preprint arXiv:1610.03955. --- --Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824—24837. Jason D Williams, Antoine Raux, and Matthew Henderson. 2016. The dialog state tracking challenge series: A review. Dialogue & Discourse, 7(3):4-33. Chien-Sheng Wu, Steven C.H. Hoi, Richard Socher, and Caiming Xiong. 2020. TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917-929, Online. Association for Computational Linguistics. Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808-819, Florence, Italy. Association for Computational Linguistics. Jinxiong Xia, Cao Liu, Jiansong Chen, Yuchen Li, Fan Yang, Xunliang Cai, Guanglu Wan, and Houfeng Wang. 2022. Dialogue topic segmentation via parallel extraction network with neighbor smoothing. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2126-2131. Linzi Xing and Giuseppe Carenini. 2021. Improving unsupervised dialogue topic segmentation with utterance-pair coherence scoring. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 167— 177, Singapore and Online. Association for Computational Linguistics. Yi Xu, Hai Zhao, and Zhuosheng Zhang. 2021. Topicaware multi-turn dialogue modeling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14176-14184. Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2021. Multiwoz 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. arXiv preprint arXiv:2104.00773. Tao Yu, Rui Zhang, Alex Polozov, Christopher Meek, and Ahmed Hassan Awadallah. 2020. Score: Pretraining for context representation in conversational semantic parsing. In International Conference on Learning Representations. Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines. arXiv preprint arXiv:2007.12720. Xiaodong Zhang and Houfeng Wang. 2016. A joint model of intent determination and slot filling for spoken language understanding. In JJCAIJ, volume 16, pages 2993-2999. Yichi Zhang, Zhijian Ou, and Zhou Yu. 2020. Taskoriented dialog systems that consider multiple appropriate responses under the same context. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9604-9611. Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang, Harrison Lee, Abhinav Rastogi, Izhak Shafran, and Yonghui Wu. 2022. Descriptiondriven task-oriented dialog modeling. arXiv preprint arXiv:2201.08904. --- --A Prompts A.1 §3-DST prompts Bing Chat Below is the full prompt for S3-DST, with templated values to be replaced by e.g., intent label names or descriptions in curly braces. Appendix B provides the full list of state values. <valid_domains> <item>{valid domain label name}</item> </valid_domains> <valid_preceding_topical_relation> <item> <name>YES</name> <desc>The current turn has **some or anyx* topical/subtopical relation to the preceding conversation context.</desc> </item> <item> <name>NO</name> <desc>The current turn has xxabsolutely nox* topical/subtopical relation to the preceding conversation context OR is the first turn in the conversation, marking the beginning of a new dialogue segment. </desc> </item> </valid_preceding_topical_relation> <valid_intents> <item> <name>{ valid intent label name}</name> <desc>{intent description}</desc> </item> </valid_intents> HH TASK ## You are given a dialogue between a user and an agent comprised of turns starting with T. For each turn you have to answer the following questions - Summarize the turn in <=3 sentences - Output the label relation>...</valid_preceding_topical_relation> list - Output the intent label from the <valid_intents>...</valid_intents> list - Output label from the <valid_domains>...</valid_domains> list preceding_topical_relation using the <valid_preceding_topical_ the domain - When preceding_topical_relation is YES, you must use the exact same intent and domain label for all turns in the segment. ## OUTPUT FORMAT ## <T{turn number }> <summary>{turn summary in <=3 sentences}</summary> <preceding_topical_relation>{valid preceding topical relation label}</preceding_topical_relation> <intent>{valid intent label}</intent> <domain>{valid domain label}</domain> </T{turn number }> ## INPUT ## {XML-structured dialogue} ## OUTPUT ## For the “No PAR” baseline, we remove the turn summarization instruction and summary tag from the prompt. For the “Unstructured input” baseline, we input the conversation as a list of plain-text turns numbered from T1 to T{t}. For the TBT-DST baseline, we remove all segmentation instructions and labels from the prompt, and simply have the model output a valid intent and domain per turn. For the DialSeg711 dataset, we remove all instructions and values related to intent and domain, and have the model output turn-level summaries and segment labels only. MWOZ Below is the S3-DST prompt for the MWOZ dataset. Note that all descriptions for slots were generated by GPT4. <slots> <item> <name>taxi-leave at</name> <description>the time when the user wants to get the taxi</description> </item> <item> <name>{domain}-{intent}</name> <description{description of slot}</description> <valid_values>{valid categorical values for slot if applicable, otherwise this tag does not appear}</valid_values> </item> </slots> ## TASK HH You are given a dialogue between a user and an agent comprised of turns starting with T. For each turn you have to answer the following questions. - Output the user utterance verbatim. - Based on that utterance, extract the relevant information about user preferences for relevant slots from <slots>...</slots> and represent them as a list of tags that follow the format [’{SLOT}-{value}’], information for that SLOT. where value is the specific - Remove any duplicates or conflicting pairs from --- --the list. If the same SLOT appears more than once in the list, keep only the most recent or relevant value originated from a user utterance. - If the values for the same SLOT contradict each other, resolve the conflict by keeping the **most recent*x*x user provided value. Output the final list as the task result. - Example output for [’ {SLOT}-{value}’]. look like *hotel-book number_ For example, the output may [’hotel-book day-monday’ , *hotel-book ’hotel-name-wartworth’ , of_people-3’ , number_of_days-4’ , ’hotel-area-east’, *hotel-parking-yes’, *hotel-stars-4’, ’hotel-internet-yes’ , ’train-book number_of_ people-1’, ’train-destination-bishops stortford’, ’train-day-friday’, ’train-arrive_by_time-19:45’, ’train-departure-cambridge’ ] - Make predefined <slots>...</slots> list. sure selected slots are only from If <valid_values>...</valid_values> are mentioned for the slot, you must use one of the valid values for that slot. - Use dontcare values only if user explicitly mentions it. Now for **every turn*x*, answer the following questions: <T{turn number }> <agent_context> {verbatim last agent utterance} </agent_context> <user_utterance> {verbatim user utterance of the turn} </user_utterance> <updated_slot_value> updated list of [’ {SLOT}-{value}’] taking slots from <slots>...</slots> and using <valid_values>. ..</valid_values> for appropriate slots </updated_slot_value> </T{turn number }> H#HINPUTHH {XML-structured dialogue} #HOUTPUTHH A.2 IC-DST prompt Below is the IC-DST prompt adapted to the Bing Chat dataset. Note that for the DialSeg711 dataset, we simply remove the domain and intent columns and instructions. CREATE TABLE states( domain text CHECK (domain IN ({valid domain names)) , preceding_topical_relation text CHECK (preceding_topical_relation IN (YES, NO)), intent text CHECK (intent IN ({valid intent names)), ) /* ## DESCRIPTION OF SELECTED COLUMN-VALUE PAIRS: - preceding_topical_relation-NO: The current turn has xxabsolutely nox* topical/subtopical relation to the preceding conversation context OR is the first turn in the conversation, marking the beginning of a new dialogue segment. - preceding_topical_relation-YES: The current turn has x*some or anyx* topical/subtopical relation to the preceding conversation context. - intent-INFORMATION SEEKING: The user wants to find factual information or answers to specific questions. {remaining intents and descriptions here} */ ## TASK HH Using valid SQLite, answer the following multi-turn conversational questions for the table provided above. Use the following steps: - For each user-agent turn starting with T, output the answer SQL query. - When preceding_topical_relation is YES, you must use the exact same intent and domain label for all turns in the segment. - Output your answer as a list, with one SQL query per turn starting with T. ## OUTPUT FORMAT ## T{turn SELECT * preceding_topical_relation = from states WHERE {your answer} AND number}. intent = {your_answer} AND domain = {your answer}; ## INPUT ## {input dialogue} ## OUTPUT ## B_ Annotation Details B.1_ Labels provided to annotators Below, we provide the labels and descriptions, if available, that were given to the Bing Chat dataset annotators. For intent and domain, we developed the label names and intent descriptions using an iterative, semi-automated process in which we asked GPT4 to summarize a sample of conversation logs, extract the key themes, and compare these themes to identify the main differences among different types of intents and domains. IsSegmentBoundary ¢ NO: The current turn has no syntactic, semantic, or topical relation to the preceding con --- --versation context OR is the first turn in the conversation. ¢ YES: The current turn has any syntactic, semantic, or topical relation to the preceding conversation context. SegmentIntent INFORMATION SEEKING: The user wants to find factual information or answers to specific questions. ANALYSIS: The user asks analytical or conceptual questions about a complex topic or problem. The user’s questions require some degree of reasoning, interpretation, argumentation, comparison, and/or data processing. CREATION: The user asks the agent to either generate original content or translate existing content into new content based on specified criteria or constraints. OPEN-ENDED DISCOVERY: The user wants to casually chat or play with the agent out of curiosity, boredom, or humor, OR the user’s intent is so unclear/underspecified that it’s impossible to categorize in any of the other intent classes. The user mainly treats the agent as a conversation or chitchat partner, and none of the other intent categories can be assigned. SegmentDomain « AI MACHINE LEARNING AND DATA SCIENCE ASTROLOGY BIOLOGY AND LIFE SCIENCE BUSINESS AND MARKETING CAREER AND JOB APPLICATION CLOTHING AND FASHION COOKING FOOD AND DRINKS CRAFTS CULTURE AND HISTORY CYBERSECURITY DATING FRIENDSHIPS AND RELATIONSHIPS DESIGN EDUCATION ENTERTAINMENT ENVIRONMENT AGRICULTURE AND ENERGY FAMILY PARENTING AND WEDDINGS FINANCE AND ECONOMICS GAMES GEOGRAPHY AND GEOLOGY HEALTH AND MEDICINE HOUSING AND HOMES HUMOR AND SARCASM LANGUAGE LAW AND POLITICS LITERATURE AND POETRY MANUFACTURING AND MATERIALS MATH LOGIC AND STATISTICS MUSIC AND AUDIO NEWS PETS AND ANIMALS PHILOSOPHY PHYSICS CHEMISTRY AND ASTRONOMY PRODUCTIVITY PSYCHOLOGY AND EMOTIONS RELIGION AND MYTHOLOGY SHIPPING AND DELIVERY SHOPPING AND GIFTS SMALL TALK SOCIAL MEDIA SOFTWARE AND WEB DEVELOPMENT SPORTS AND FITNESS TAXATION TECHNOLOGY TIME AND DATES TRANSPORTATION AUTOMOTIVE AND AEROSPACE TRAVEL VISUAL ARTS AND PHOTOGRAPHY WEATHER WRITING JOURNALISM AND PUBLISHING --- --B.2. Domain labeling procedure Due to the large number of domain values and the potential for high disagreement and cognitive overload, we did not ask annotators to choose from the full list of domains per turn. Rather, we provided a dropdown list of five options per turn. One option was manually selected by the authors as being correct or near-correct. Two options were chosen at random using Python. One option was “OTHER,” in which case the annotator was required to choose the correct domain from the full list of 49 domains and explain their choice. Finally, the last option was a “hard negative” chosen using the following procedure. First, we manually grouped our domains into eight high-level clusters: STEM, arts, social sciences, health, commerce, professional, personal, and leisure. Then, given the aforementioned “‘ground-truth” domain chosen by the authors, we randomly sampled another domain from the same high-level cluster as the ground-truth label. For example, if the groundtruth domain was chosen to be “BIOLOGY AND LIFE SCIENCE”, we sampled another domain from the STEM cluster as our final domain candidate.
"	"--- ABSTRACT ---
기존의 대화 상태 추적(DST) 문제는 사용자 에이전트 대화에서 사용자 선호도와 의도를 추적하는 것을 목표로 합니다. 좁은 도메인 애플리케이션을 지원하는 작업 지향 대화 시스템에는 충분하지만, LLM(Large Language Model) 기반 채팅 시스템의 등장으로 인해 오픈 도메인 대화에 많은 실제 복잡성이 도입되었습니다. 이러한 복잡성은 맥락적 상호 작용의 복잡성 증가, 다양한 주제를 아우르는 확장된 대화 세션, 더 빈번한 맥락적 변화의 형태로 나타납니다. 진화하는 LLM 기반 채팅 시스템에서 발생하는 이러한 복잡성을 처리하기 위해 오픈 도메인 대화 시스템에서 세그먼트당 공동 대화 분할 및 상태 추적을 제안합니다. 진정한 오픈 도메인 대화 시스템에 적합한 제로샷 설정을 가정하여, 긴 맥락 추적을 개선하기 위해 설계한 새로운 접지 메커니즘인 사전 분석 기억(Pre-Analytical Recollection)을 활용하는 구조화된 프롬프트 기술인 S3-DST를 제안합니다. 공동 세분화 및 상태 추적에서 제안된 접근 방식의 효능을 입증하기 위해 독점적인 익명화된 오픈 도메인 대화 데이터 세트와 공개적으로 사용 가능한 DST 및 세분화 데이터 세트에서 S3-DST를 평가합니다. 모든 데이터 세트와 설정에서 S3-DST는 최첨단 기술을 지속적으로 능가하여 차세대 LLM 기반 채팅 시스템의 효능과 견고성을 입증합니다. 1
--- INTRODUCTION ---
ChatGPT 및 Bing Chat과 같은 오픈 도메인 대규모 언어 모델(LLM) 기반 채팅 시스템의 출현으로 대화 시스템의 새로운 시대가 열렸습니다. 이전에는 대화 시스템이 범위와 기능이 비교적 제한되어 일반적으로 좁은 작업 중심 대화나 소셜 잡담에 국한되었습니다(Gao et al., 2018). 반면 LLM 기반 채팅 시스템은 겉보기에 무한한 주제에 대해 사용자와 유연하게 대화할 수 있고 다음을 수행할 수 있기 때문에 주목할 만합니다. 사용자: 주석이 달린 참고 문헌을 만들어 주세요... AI: 주석이 달린 몇 가지 예는 다음과 같습니다... 사용자: 이러한 인용 스타일을 설명해 주시겠습니까? AI: 물론입니다. 인용 스타일은 다음과 같은 집합... ... 사용자: 감사합니다! 오늘은 어떻게 지내세요? AI: 지금까지 좋은 하루를 보내고 있습니다. 감사합니다... 사용자: 다음 주 날씨는 어때요? AI: 내 검색 결과에 따르면... 그림 1: 오픈 도메인 대화에서 하나의 의도는 여러 턴에 걸쳐 있을 수 있으며, 하나의 대화는 여러 의도를 포함할 수 있습니다.익명화된 Bing Chat 로그에서 영감을 얻은 합성 대화.다른 사용자 의도(주석이 달린 참고 문헌 작성, 소셜 잡담, 날씨 확인)는 다른 색상으로 강조 표시됩니다.이전에 코드 생성, 질의 응답 등과 같이 특수 시스템이 필요했던 많은 사용자 작업이 즉시 사용 가능합니다.이 논문에서는 LLM 기반 채팅 시스템이 인간-AI 대화의 환경을 크게 바꾸었기 때문에 이러한 대화에서 사용자 의도를 이해하려면 새로운 분석 및 태그 지정 프레임워크가 필요하다고 주장합니다.특히 대화 상태 추적(DST) 작업에 중점을 둡니다.기존 DST는 작업 지향 대화 시스템에서 사용자의 의도를 추출하여 구조화된 백엔드 스키마(Williams et al., 2016; Budzianowski et al., 2018)와 일치시키는 것으로 구성됩니다.그러나 오픈 도메인 대화의 DST는 아직 정의되지 않았습니다. 따라서 이 논문에서 우리는 LLM 기반 채팅 시스템에서 관심 있는 상태 값을 식별하는 첫 번째 시도를 합니다. 그림 1에서 예시된 것처럼, 우리는 실제 오픈 도메인 대화가 종종 단일 의도나 주제를 추구하기 위해 당사자 간에 광범위한 왕복(예: 설명, 협상 등)을 보이며, 맥락은 관련 없는 의도 및/또는 주제 사이에서 단일 대화 내에서 여러 번 바뀔 수 있다는 주요 관찰을 합니다. 이러한 관찰을 바탕으로, 우리는 오픈 도메인 대화에서 세그먼트와 상태를 모두 추적할 것을 제안합니다. 세분화는 맥락적으로 응집된 대화 &quot;단위&quot;의 시작과 끝을 표시하는 경계를 식별하는 데 도움이 되는 반면, 상태는 세그먼트별로 적용되는 우리가 추적하고자 하는 관심 있는 의도 변수입니다. DST를 오픈 도메인 대화와 LLM 시대로 끌어들이는 것 외에도, 우리는 오픈 도메인 DST를 위한 LLM 기반 솔루션을 소개합니다. 레이블링 비용 때문에 현실적인 대화 태그 지정에 대한 제로샷 설정을 가정하여, 우리는 오픈 도메인 DST를 위한 구조화된 프롬프트 접근 방식인 S3-DST를 소개합니다. S3-DST 내에서 우리는 각 출력 상태 예측을 해당 대화 턴의 내용에 기반하는 새로운 사전 분석 기억(PAR) 프롬프트 전략을 제안하여 LLM이 잊거나 환각 없이 긴 대화 맥락을 추적할 수 있도록 돕습니다. 우리는 공개 DST 및 세분화 벤치마크와 함께 Microsoft의 Bing Chat 시스템에서 수집한 완전히 익명화된 오픈 도메인 대화 데이터 세트에서 S3-DST를 평가합니다.¹ S3-DST 모든 벤치마크에서 비슷한 기준선에 비해 큰 성과를 거두어, 오픈 도메인 대화 모델링에 대한 추가 연구를 위한 시작점으로 적합함을 시사합니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 오픈 도메인 DST 문제 정의: 우리는 대화 상태 추적을 오픈 도메인 LLM 채팅 시대로 가져왔습니다. 우리는 익명화된 Bing Chat 로그 데이터에서 실제 오픈 도메인 인간-AI 대화가 어떻게 수행되는지에 대한 관찰에서 동기를 얻어 문제를 공동 분할 및 상태 추적 작업으로 규정했습니다. • 제로샷 S3-DST 접근 방식: 우리는 오픈 도메인, 다중 의도 대화를 위한 구조화된 제로샷 공동 분할 및 상태 추적 접근 방식인 S3-DST를 제안합니다. S3-DST는 구조화된 프롬프트 템플릿 및 대화 태그 생성을 위한 새로운 접근 방식과 긴 컨텍스트 추적을 개선하는 기반 기술인 사전 분석 기억(PAR)을 제공합니다. • 광범위한 실험 및 분석: 우리는 독점 및 공개 데이터 세트에 대한 광범위한 실험을 수행하여 비슷한 제로샷 프롬프트에 비해 큰 성과를 거두었습니다. S3DST는 최첨단 제로샷 per&#39;Bing Chat 로그를 사용하는 것은 DialSeg711 대화 주제 세분화 벤치마크와 함께 MWOZ 2.1 및 2.4 DST 벤치마크에서 조건을 준수합니다.2 문제 정의 비공식적으로, 기존 DST의 목표는 사용자와 에이전트의 발화 턴 시퀀스 C₁ = [U₁, A1, ..., Ut, At]²이 주어진 대화 상태 yt를 예측하는 것입니다.상태 yt는 슬롯-값 쌍의 집합으로 구성되며, 여기서 슬롯은 특정 애플리케이션 도메인의 의도 속성(예: &quot;레스토랑 이름&quot;, &quot;호텔 주소&quot;)에 해당하고 값은 미리 정의된 범주형 옵션 또는 제한 없는 텍스트(Budzianowski et al., 2018)에 해당합니다. 그러나 앞서 논의했듯이 단일 오픈 도메인 대화는 종종 다양한 주제에 걸쳐 잠재적으로 관련성이 없는 여러 의도로 구성됩니다. 실제로 10,000개의 익명화된 Bing Chat 대화에 대한 예비 분석에 따르면, 대화의 50% 이상이 여러 사용자 의도를 표시하고 대화의 90% 이상이 여러 주제에 대한 토론을 포함한다고 추정합니다. 따라서 대화 분할을 병합하여 더 큰 대화 내에서 맥락적으로 응집력 있는 &quot;대화 단위&quot;를 찾는 것을 목표로 하며, 대화 상태 추적을 제안합니다. 특히, 세그먼트 수준에서 상태 추적을 수행하는데, 여기서 목표는 각 세그먼트에 관심 있는 슬롯과 값으로 레이블을 지정하여 대화 내의 여러 세그먼트가 발산하거나 상충되는 상태 값을 가질 수 있도록 하여 오픈 도메인 채팅의 진정한 다양성을 반영하는 것입니다. 이 섹션의 나머지 부분에서는 분할과 상태를 정의하고 마지막으로 공동 작업을 공식화합니다. 2.1 세그먼트 대화 주제 분할(Xing 및 Carenini, 2021; Xia 등, 2022; Gao 등, 2023)에 대한 이전 연구에 따라 대화 세그먼트를 모든 사용자 및 에이전트 발화가 주제적으로 관련된 Ct의 연속된 하위 시퀀스로 정의합니다. 형식적으로 B₁ = [b₁,……., bt−[b₁, … …. …, bt−]는 Ct에서 인접한 사용자-에이전트 발화 쌍 사이의 경계 인덱스를 나타냅니다. 분할의 출력은 경계 인덱스 Bk C BŁ의 집합입니다. 여기서 k는 분할 알고리즘에 의해 결정된 경계 수와 Bing Chat 사용에 대한 범위 [Um, Am, . . . Un, An]을 나타냅니다. 2현재 LLM 기반 채팅 시스템에서는 사용자가 단일 에이전트 응답이 발행되기 전에 여러 발화를 발행할 수 있습니다. 이러한 (드물게) 경우에 우리는 에이전트 응답 이전의 모든 사용자 발화를 단일 발화로 그룹화합니다. 경계 bm과 bn 사이의 연속된 세그먼트를 보냅니다. 여기서 m = [1, t − 1] 및 n ≤ [m, t − 1]. 2.2 세그먼트 상태 일반적으로 대화 상태 추적 방법은 각 턴에서 상태의 새로운 요소를 추출합니다(Hu et al., 2022). 그러나 이는 DST 평가 벤치마크가 사용자가 각 턴에서 새롭고 관련성 있는 의도 요소를 제공하고 의도가 서로를 기반으로 하거나 보완하지만 대화 전체에서 근본적으로 변경되거나 충돌하지 않는다는 비교적 좁은 가정을 하기 때문입니다. 이전에 논의했듯이 오픈 도메인 대화는 훨씬 더 다양한 특성을 보이며 다중 의도 및/또는 다중 도메인 대화가 비교적 일반적입니다. = 따라서 턴 수준이 아닌 세그먼트에서 상태를 추출할 것을 제안합니다. 세그먼트 수준 상태를 {Sm:n = (smin, vmn), i 1... Nm:n}으로 정의합니다. 여기서 son은 경계 bm에서 bn까지의 세그먼트에 적용된 i번째 슬롯을 나타냅니다. (i) vm:n은 슬롯의 해당 값을 나타내고 Nm:n은 이 세그먼트에 적용된 슬롯의 총 수를 나타냅니다. 슬롯 값 쌍의 스키마는 여기에서 유효합니다. § 4.1과 부록 B에서 Bing Chat에 대한 특정 상태 스키마를 설명합니다. 2.3 문제 설명 세그먼트와 세그먼트별 상태를 정의했으므로 오픈 도메인 DST에 대한 전체 정의를 설명할 수 있습니다. 사용자-에이전트 발화 쌍 C₁ = [U₁, A1, ..., Ut, At]의 시퀀스가 주어지면 오픈 도메인 대화 상태 추적의 목표를 Yt = Bk U {Sm:n; V(bm, bn) Є Bk}, (1) 여기서 Bk C Bt는 이전에 설명한 세그먼트 경계 인덱스를 나타내고 Sm:n은 경계 bm과 bn 사이의 세그먼트 상태를 나타내며, 이는 N개의 임의의 슬롯 값 쌍으로 구성됩니다. Sm:n = {(s):n, v(n), i = 1 . . . Nm:n}. (2) 3가지 프롬프트 전략 이전에 논의했듯이 실제 대화는 종종 다양한 주제를 논의하기 위해 여러 대화 턴에 걸쳐 확장되는 광범위한 담론을 보입니다. 이러한 장기간의 대화적 특성으로 인해 문맥적 일관성을 추적하기가 매우 어렵습니다. 이전 연구(Hu et al., 2022)는 개별 대화 턴을 분리하고 대화 상태 변경을 추적하기 위해 하나씩 처리하는 것을 목표로 했으며, 이는 미리 정의된 좁은 도메인에 국한된 작업 지향 대화에서 상당히 잘 작동했습니다. 그러나 실제 대화는 일반적으로 문맥적 뉘앙스를 적절히 이해하기 위해 여러 차례의 전환이 필요한데, 이는 Transformers가 특히 중간에 긴 입력 맥락을 처리할 때 여전히 어려움을 겪기 때문에 어려운 문제입니다(Liu et al., 2023). 이러한 어려움을 해결하기 위해 입력과 출력에 구조를 제공하면서 프로세스의 맥락을 정확하게 보존하는 새로운 차례별 프롬프트 기술을 제안합니다. 아래에서 프롬프트의 이러한 디자인 측면에 대해 설명합니다. 3. 구조화된 출력 및 입력 구조화된 출력 목표는 세그먼트 경계(바이너리 레이블)와 상태 값(범주형 레이블 또는 오픈 텍스트)을 나타내는 대화 차례당 레이블 집합입니다. LLM의 출력에 유연하면서도 구조화된 형식을 제공하기 위해 계층적 XML 형식으로 출력을 생성하도록 지시하는 것을 제안합니다. 우리는 XML이 DST 작업에 코드와 같은 구조를 제공하기 때문에 유리하다고 생각합니다. 이는 일반 텍스트 출력에 비해 성능을 크게 향상시키는 것으로 나타났으며, SQL과 같은 보다 엄격한 출력 형식에 비해 확장 가능하고 유연합니다(Hu et al., 2022). 우리의 접근 방식은 1에서 t까지의 각 턴이 XML 트리를 구성하는 XML 형식을 사용합니다.<T{id}> ...</T{id}> 그리고 그 안에 여러 개의 중첩된 XML 태그가 있습니다. 이러한 중첩된 태그의 레이블(예:<preceding_topical_relation> ...</preceding_topical_relation> ,<intent> ...</intent> , 그리고<domain> ...</domain> 그림 2(iii))는 세그먼트 경계와 관심 슬롯을 나타내며, 여는 태그와 닫는 태그 사이의 각 값은 모델의 추론된 값을 나타냅니다. 이 전략은 두 가지 측면에서 유익합니다. (i) 경계가 있는 잘 정의된 구조화된 형식으로 인해 생성된 출력은 자유형 텍스트보다 레이블 지정 지침과 일치할 가능성이 더 높고 (ii) 잘 구성된 구조화된 출력 형식은 구문 분석하기가 더 쉬워 후처리 요구 사항이 줄어듭니다. 구조화된 입력 LLM을 프롬프트하는 경우 분석 및 추론을 위해 일반 대화 기록을 평면 형식으로 채널링하는 것은 사소한 일이지만 이 선형 구성에 내재된 비구조적 특성으로 인해 참조하고 활용하기 어렵습니다. 원시 대화 사용자: 주석이 달린 참고 문헌을 제공하세요... AI: 다음은 몇 가지 다른 참고 문헌입니다... 사용자: (i) 대화와 지침을 구조화된 프롬프트 템플릿에 삽입<valid preceding_topical_relation><item><name> 예</name><description> ...</description></item></valid_preceding_topical_relation><valid intents><item> ...</item></valid_intents> ## 대화 (iii) 생각의 사슬을 통해 구조화된 차례차례 대화 태그를 생성합니다.<T1><summary> 사용자는 주석이 달린...을 요청합니다.</summary><preceding_topical_relation> 아니요</preceding_topical_relation><intent> 창조</intent><domain> 저널리즘 쓰기와 출판</domain> 법학 석사</T1><T2> 일체 포함:<T1><User> ...을 제공해 주세요.</User><AI> 여기에 몇 가지 예가 있습니다...</AI></T1><T2><summary> ...</summary><preceding_topical_relation> ...</preceding_topical_relation><intent> ...</intent> &#39;<domain> ...</domain></T2> 구조화된 턴<User> ...</User><AI> ...</AI></T2> (ii) 사전 분석 기억(PAR) 각 턴에서 출력을 근거로 컨텍스트 보존 그림 2: S3-DST의 프롬프트 흐름. 원시 대화가 주어지면 (i) 계층적 XML 구조 표현으로 변환하여 유사하게 구조화된 프롬프트 템플릿에 삽입합니다. 프롬프트를 LLM에 전달하고 (ii) 계층적 XML 구조 출력을 얻습니다. 여기서 각 턴에는 (iii) 원하는 세분화 및 상태 레이블 예측과 함께 대화에 대한 PAR 근거 참조가 포함됩니다. 여러 대화 턴에서 다른 정보를 연령화합니다. 출력 형식과 일관되게 이 과제를 처리하기 위해 각 대화 내역이 계층적 XML 형식으로 형성되고 대화 턴이 턴 ID 번호로 표시되는 구조화된 입력 형식을 제안합니다.<T{id}> ...</T{id}> 1에서 t까지 번호가 매겨지고 각 대화 차례는 적절한 XML 태그로 표시된 중첩된 사용자 및 에이전트 차례로 구성됩니다.<user> ...</user> 그리고<agent> ...</agent> ). 출력 중에 턴당 레이블을 추론하도록 LLM에 지시하는 것을 제안하기 때문에 이 입력 체계는 입력 턴을 정확하게 참조하고 긴 대화 맥락에서도 일관성을 유지하는 데 도움이 됩니다. 이 XML 태그 입력 형식과 일관되게 다음 구조를 사용하여 XML 형식 목록에서 모든 유효한 세그먼트 및 상태 범주를 형식화합니다.<valid_category_name><item> {라벨 이름}</item><description> {라벨 설명(있는 경우)}</description><valid_category_name> 경험적으로, 이 구조화된 입력 및 프롬프트 형식은 LLM 생성이 레이블 지정 지침을 따르도록 제한하는 데 도움이 됩니다. 그림 2(i)는 각 유효한 세그먼트 경계와 상태 범주가 먼저 XML 형식의 목록에서 준비되고 이어서 입력 대화가 계층적 구성으로 표시되는 이 형식을 보여줍니다. 3. 사전 분석 기억(PAR) 이전에 논의했듯이, 오픈 도메인 대화는 길고 대화 흐름이 매우 다양할 수 있습니다. 따라서 LLM이 잊거나 환각하지 않고 진화하는 대화 맥락을 정확하게 모니터링할 수 있도록 하는 것이 중요합니다. 이를 위해, 우리는 LLM이 먼저 다음을 사용하여 차례를 요약하도록 지시하는 차례 차례 프롬프트를 위한 기초 전략인 사전 분석 기억(PAR)을 제안합니다.<summary> ...</summary> 영어: 세그먼트 및 상태 값을 제공하기 전에 3개 이하의 문장으로 태그를 지정합니다.PAR은 추론 정확도를 개선하기 위해 관련 중간 출력을 생성하는 기술이기 때문에 사고의 사슬 프롬프트(Wei et al., 2022)에서 영감을 받았습니다.그러나 사고의 사슬과 달리 PAR은 모델의 출력에서 대화에 직접 참조를 제공하는 접지 기술이기도 합니다.그림 2(ii)는 PAR이 대화 상태를 추론하기 위해 분석하기 전에 각 대화 차례의 내용을 다시 참조하는 방법을 보여줍니다.3.3 최종 프롬프트 구성 S3-DST의 최종 프롬프트 흐름은 그림 2에 나와 있습니다.원시 대화와 미리 정의된 세그먼트 및 상태 레이블 집합이 주어지면 레이블을 구조화된 프롬프트 템플릿에 삽입하고 대화를 계층적 XML 구조 표현으로 포맷합니다.프롬프트를 LLM으로 전달하여 PAR을 따르도록 지시한 다음 공동으로 표 1: 평가 테스트 집합 통계. # 전환 # 회전 # 세그먼트/전환 Bing ChatMWOZ 2.1, MWOZ 2. DialSeg1, (평균) 1.3. 세그먼트별로 적용되는 계층적 턴바이턴 분할 및 상태 레이블을 생성합니다. 프롬프트의 전체 텍스트는 부록 A.1에 제공됩니다. 4 실험 여러 데이터 세트에 대해 포괄적인 평가를 수행합니다. 주로 도메인 전문가가 주석을 단 완전히 익명화된 Bing Chat 로그에 대한 접근 방식을 평가합니다. 또한 공개 벤치마크 데이터 세트 MultiWOZ(Budzianowski et al., 2018) 및 DialSeg(Xu et al., 2021)를 사용하여 표준 작업 지향 DST 및 분할 작업에서 S3-DST를 평가합니다. 이러한 데이터 세트에 대한 자세한 설명은 표 1의 데이터 세트 통계와 함께 아래에 제공됩니다.4. 내부 인간-LLM 대화 데이터 세트 실제 오픈 도메인 인간-LLM 대화에 대한 접근 방식의 효능을 평가하기 위해 Bing 검색 엔진이 지원하는 LLM 채팅 인터페이스인 Microsoft의 Bing Chat 시스템에서 익명화된 채팅 로그 데이터를 수집했습니다.벤치마크 구축 2023년 4월 5일부터 2023년 4월 30일까지 Bing Chat에서 진행된 484개의 영어 대화를 두 가지 접근 방식을 통해 샘플링합니다.(i) 무작위 및 (ii) 5턴 이상인 &quot;긴&quot; 대화. 이 두 가지 접근 방식을 50/50으로 균형 있게 조정합니다. 제로샷 가정에서 작동하므로 학습 데이터가 필요하지 않습니다. 따라서 개발을 위해 150개의 대화를 유지하고 나머지 334개를 테스트용으로 유지합니다.주석 평가를 위한 기준 진실 레이블을 얻기 위해 세그먼트 및 상태에 대한 인간 주석을 수집했습니다. 우리는 Bing Chat 시스템에 대한 높은 수준의 기술적 전문성과 친숙함을 갖춘 사내 주석자 3명을 모집했습니다. 각 턴에 대해 주석자에게 이진 IsSegmentBoundary 레이블, 범주형 SegmentIntent 레이블, 범주형 SegmentDomain 레이블을 제공하도록 지시했습니다. 턴과 이전 컨텍스트 간의 주제적 관계를 식별할 수 없는 경우 주석자에게 세그먼트 경계를 표시하도록 지시했습니다. 의도와 도메인의 경우 Bing Chat 시스템을 위해 사내에서 개발한 택소노미를 사용했는데, 여기에는 4개의 의도(정보 탐색, 분석, 생성 및 개방형 발견)와 49개의 도메인이 포함됩니다(전체 목록은 부록 B.1 참조). 도메인 수가 많기 때문에 턴당 주석자에게 4개의 후보 도메인 값과 &quot;기타&quot; 옵션을 제공했습니다. 부록 B에는 주석 체계와 도메인 샘플링 절차에 대한 자세한 내용이 나와 있습니다. 전체 데이터 세트에 레이블을 지정하기 전에 주석자 간 일치를 보장하기 위해 먼저 무작위로 선택한 10개의 대화(총 68개의 턴)에 대한 주석을 수집하고 레이블 유형별로 Fleiss 카파(Fleiss, 1971)를 계산했습니다. IsSegmentBoundary의 경우 k = 0.83, SegmentIntent의 경우 k = 0.74, SegmentDomain의 경우 K = 0.88의 Fleiss 카파를 관찰했는데, 이는 모두 Fleiss 카파 척도에서 높은 일치로 간주됩니다. 4.2 공개 벤치마크 광범위하게 오픈된 도메인인 Bing Chat 데이터를 반영하는 기존의 공개 대화 벤치마크는 알려진 바가 없습니다. 따라서 세 가지 데이터 세트를 사용하여 공개 벤치마크에서 별도의 DST 및 세분화 평가를 수행합니다. MultiWOZ MultiWOZ(MWOZ) 다중 도메인 대화 데이터 세트(Budzianowski et al., 2018)는 현재 가장 일반적인 DST 벤치마크입니다. MWOZ는 1,000개의 테스트 대화로 구성된 작업 지향 데이터 세트입니다. 저희는 원본의 두 가지 업데이트된 버전인 MWOZ 2.1(Eric et al., 2019)과 2.4(Ye et al., 2021)를 사용합니다. 후자는 MWOZ의 &quot;가장 깨끗한&quot; 버전으로 간주되는 반면 전자는 문헌에서 더 자주 사용되었습니다. DialSeg711 DialSeg711 벤치마크는 (Xu et al., 2021)에 의해 도입되었으며 최근 대화 분할 연구에 자주 사용되었습니다. 기존 작업 지향 대화 코퍼스에서 대화를 결합하여 711개의 다중 세그먼트 대화를 구성하는 영어 데이터 세트입니다. 4.3 기준선 저희는 S3-DST와의 공정한 비교를 위해 제로샷 LLM 프롬프트만 기준선으로 고려합니다. 저희는 아래에서 다양한 데이터 세트에 대한 기준선과 고려 사항을 논의합니다. 모든 원래 프롬프트는 부록 A에 제공됩니다. LLM 호출당 최대 1500개의 출력 토큰을 설정했으며 온도는 0입니다. 표 2: S3-DST는 내부 Bing Chat 벤치마크에서 상태 추적에 대한 최첨단 성능을 달성합니다. 모든 프롬프트는 GPT4로 실행됩니다. 개별 정확도 JGA 세그먼트 의도 도메인 I/DS/I/D TBT-DST 0.0.0.IC-DST 0.0.0.0.4610 0.S3-DST(PAR 없음) 0.0.7173 0.0.4377 0.S3-DST(비구조화 입력) S3-DST 0.0.7163 0.0.4640 0.0.0.7366 0.0.4752 0.Bing Chat 이 데이터 세트에서 우리는 IC-DST를 기본 기준으로 간주합니다. 이는 (Hu et al., 2022)에서 도입한 프롬프팅 전략의 제로샷 버전으로, 세그먼트 및 대화 상태를 공동으로 추적하기 위해 오픈 도메인 대화 설정에 크게 적용되었습니다. TBT-DST 기준선은 분할 지침을 포함하지 않고 S3DST의 한 버전으로, S3-DST 프롬프트 구성을 사용하여 차례대로 의도 및 도메인 레이블을 얻습니다. 또한 프롬프트의 두 가지 핵심 측면인 PAR과 XML 구조적 포맷팅의 중요성을 분석하기 위해 S3-DST의 두 가지 절제도 고려합니다. PAR 없음은 PAR 지침이 없는 S3-DST 프롬프트를 나타내고, 구조화되지 않은 입력은 모든 지침과 대화를 XML이 아닌 일반 텍스트로 포맷하는 S3-DST 프롬프트를 나타냅니다. 모든 프롬프트에 백본 LLM으로 GPT4를 사용합니다. MWOZ MWOZ 작업 지향 대화 상태 추적 데이터 세트의 경우 Hu et al.(2022)에서 보고한 Codex-175B를 사용하여 IC-DST와 비교합니다. 또한 기준선 성능에서 백본 모델 개선을 설명하기 위해 GPT-로 제로샷 IC-DST를 재평가합니다. 마지막으로, (Heck et al., 2023)에서 보고한 MWOZ 2.1에서의 제로샷 ChatGPT 성능과 비교합니다. DialSeg711 우리는 비지도 TextTiling(Hearst, 1997), CSM(Xing and Carenini, 2021), DialStart(Gao et al., 2023) 방법을 고려합니다. 우리는 (Gao et al., 2023)의 모든 숫자를 다시 인쇄합니다. 마지막으로, 우리는 원래 IC-DST(Hu et al., 2022)와 동일한 SQL 출력 형식으로 세분화 레이블을 유도하도록 프롬프트된 IC-DST 기준선을 사용합니다. 4.4 메트릭 상태 추적을 위해, 우리는 모든 상태 값이 올바르게 추론되는 턴의 비율을 측정하는 Joint Goal Accuracy(JGA)를 고려합니다. Bing Chat의 경우 Binned JGA(I/D) 0.0.0.500.0.(0,3] S3-DST .-X. S3-DST(PAR 없음) IC-DST(3,5] (5,10] 대화 길이(턴 수) (10,20] 그림 3: S3-DST는 컨텍스트 추적을 강조하여 모든 길이의 대화에 대한 기준선보다 성능이 뛰어납니다. 길이에 따라 Bing Chat 대화를 빈으로 나누고 빈당 JGA를 표시합니다. 대화 길이가 길어질수록 두 기준선 모두 성능이 크게 저하되므로 PAR 기반 전략의 중요성을 확인할 수 있습니다. 표 3: S3-DST는 공개 대화 상태 추적 벤치마크 MWOZ 2.1 + 2.4에서 제로샷 LLM 기준선과 비교하여 최첨단 JGA를 달성합니다. JGA MWOZ 2.MWOZ 2.IC-DST(Codex) IC-DST (GPT4) 0.0.0.0.ChatGPT 0.S3-DST 0.0.intent 및 domain(I/D)은 관심 있는 실제 상태 값이고, 완전성을 위해 세그먼트, 의도 및 도메인 정확도(S/I/D)가 있는 JGA입니다. 또한 Bing Chat에서 세분화, 의도 및 도메인 정확도를 별도로 보고하여 오픈 도메인 대화 데이터에서 LLM의 현재 기능과 한계를 파악합니다. 세분화의 경우 PK 및 WindowDiff(Pevzner 및 Hearst, 2002)를 고려합니다. 둘 다 오류 메트릭(즉, 낮을수록 좋음)으로, 조정 가능한 슬라이딩 윈도우를 사용하여 예측된 세그먼트 경계와 기준 진실 세그먼트 경계 간의 차이를 정량화합니다. 표 4: MWOZ 2.1에서 제로 샷 도메인별 비교(JGA). 표 5: S3-DST는 공개 세분화 벤치마크 DialSeg711에서 최첨단 성능을 달성합니다. attr. hotel 도메인별 JGA rest. Pk(↓) WindowDiff (↓) 택시 기차 TextTiling 0.0.IC-DST (Codex) IC-DST (GPT4) ChatGPT S3-DST 0.5997 0.4669 0.5728 0.7135 0.0.7177 0.4872 0.6526 0.7781 0.0.5270 0.4200 0.5580 0.7090 0.0.6781 0.5215 0.6713 0.8258 0.CSM 0.0.DialSTART 0.0.IC-DST 0.0.S3-DST 0.0.4.5 결과 Bing Chat 표 2에서 볼 수 있듯이, S3-DST 프롬프트는 턴 전체에서 의도, 도메인 및 JGA에서 가장 높은 성능을 달성합니다. 우리는 다음과 같은 관찰을 합니다. 첫째, 세분화를 명시적으로 수행하지 않는 TBT-DST는 지금까지 가장 약한 기준선입니다. 이는 LLM에 세그먼트 내에서 동일한 의도와 도메인을 사용하도록 지시하지 않으면 LLM이 더 완전한 이전 맥락을 고려하지 않고 턴의 내용을 과도하게 인덱싱하는 경향이 있기 때문입니다. 이로 인해 일관된 단일 주제 대화 내에서 턴 간에 충돌하는 의도 및 도메인 레이블이 발생합니다. 둘째, IC-DST의 수정된 버전은 매우 강력한 기준선입니다. 그러나 IC-DST는 구조화된 출력을 사용하지만 해당 구조화된 입력 표현이 없습니다. 존재하지 않는 턴에 대한 환각이 S3-DST에 비해 비교적 흔하기 때문에 어떤 경우에는 이로 인해 성능이 저하됩니다. 마지막으로 S3-DST의 두 가지 절제는 모두 S3-DST에 비해 성능이 낮아 LLM이 생성 중에 참조할 수 있는 PAR 및 구조화된 입력의 중요성을 확인합니다. 실제로 대화 길이와 성능 간의 관계를 나타낸 그림 3은 S3-DST가 대화가 길어짐에 따라 no-PAR ablation의 성능이 급격히 저하되는 것을 방지한다는 것을 보여줍니다. 예를 들어, no-PAR ablation은 3턴 이하의 대화에서는 S3-DST와 비슷한 성능을 보이지만, 턴 이상의 대화에서는 JGA가 10포인트 이상 떨어집니다. 이러한 결과는 특히 긴 대화에서 PAR의 필요성을 강조합니다. MWOZ 표 3과 4는 전체 및 도메인별 MWOZ 숫자를 제공합니다. S3-DST는 강력한 LLM에 비해 최첨단 제로샷 JGA를 큰 차이로 달성합니다. 가장 강력한 제로샷 기준선인 IC-DST(GPT4)조차도 MWOZ 2.1에서는 JGA가 거의 5포인트, MWOZ 2.4에서는 JGA가 . 거의 모든 개별 도메인에서 S3-DST는 IC-DST(GPT4)보다 성능이 뛰어나고, 예를 들어 트레인 도메인에서 13포인트 이상의 JGA 개선과 같이 큰 차이로 더 뛰어납니다. DialSeg711 마지막으로 표 5는 DialSeg711의 성능을 보여줍니다. S3-DST는 이 데이터 세트에서 거의 오류가 없으며, 데이터 세트의 구성을 감안하면 놀라운 일이 아닙니다. 구체적으로 DialSeg711은 매우 다른 주제에 대한 대화를 결합하여 구성되므로 세그먼트 간에 매우 인위적이고 갑작스러운 컨텍스트 전환이 발생합니다. 그러나 IC-DST 프롬프트 기준선은 S3-DST보다 훨씬 더 높은 오류를 발생시킵니다. 자세히 살펴보면 LLM이 데이터 세트의 여러 대화에 대한 대화 컨텍스트를 추적하지 못해 원래 대화 컨텍스트를 잊어버리는 것을 알 수 있습니다. 이러한 결과는 성공적인 세분화를 위해 PAR 및 대화 컨텍스트 추적의 중요성을 강조합니다. S3-DST의 강력한 성능은 DialSeg711이 앞으로 LLM에 충분히 어려운 작업이 아닐 수 있음을 시사하며, 궁극적으로 분할의 목표가 상태 추적 성능을 개선하는 것이므로 공동 분할 및 상태 추적의 필요성을 더욱 부추깁니다. 5
--- RELATED WORK ---
5.1 대화 상태 추적 인간-AI 대화의 흐름을 정확하게 추적하려면 사용자의 의도와 목표를 추론하는 데 강력한 상태 추적이 중요합니다. MultiWOZ(Budzianowski et al., 2018) 데이터 세트가 커뮤니티에 도입된 이후 DST 성능을 개선하기 위한 수많은 기술이 제안되었습니다. 복사 메커니즘(Lei et al., 2018), 전이 학습(Wu et al., 2019), 데이터 증강(Zhang et al., 2020), 대조 사전 학습(Wu et al., 2020) 등을 포함한 이전 시도는 감독 미세 조정 시나리오에서 개선을 가져왔습니다. 한편, MultiWOZ도 여러 번의 주석 개정을 거쳤습니다(Eric et al., 2019; Ye et al., 2021; Zang et al., 2020; Han et al., 2020). 다른 기술(Peng 등, 2021; Lin 등, 2020; Zhao 등, 2022; Yu 등, 2020; Platanios 등, 2021)도 제안되었지만, 데이터 레이블링의 리소스 집약적이고 힘든 특성으로 인해 점차적으로 주목이 소수 및 제로 샷 대화 상태 추적(Shin 등, 2022; Hu 등, 2022; Heck 등, 2023) 탐색으로 옮겨갔습니다. 이 분야의 최첨단 접근 방식(Hu 등, 2022)은 상태 추적을 위해 LLM을 활용할 수 있지만, 실제 확장 대화 세션에서 성능을 잠재적으로 저하시킬 수 있는 적절한 접지 메커니즘이 현저히 부족합니다. 더욱이, 앞서 언급한 이전 작업 중 어느 것도 유연한 오픈 도메인 LLM 기반 채팅 시스템에서 널리 퍼져 있는 주제 일관성 및 컨텍스트 전환을 설명하지 않습니다. 5.2 대화 주제 세분화 대화를 주제별로 일관된 단위로 세분화하는 것은 다운스트림 대화 모델링의 성공에 기초가 됩니다. 주석이 달린 데이터의 부족은 대화 주제 세분화에 어려움이었지만, 최근의 비지도 시도는 주제 세분화에서 몇 가지 유망한 결과를 보여주었습니다. 더 구체적으로, 고전적인 텍스트 세분화 알고리즘인 TextTiling(Hearst, 1997)을 기반으로 하는 확장은 이 측면에서 주로 벤치마크를 주도했습니다(Song et al., 2016). 더 최근에는 텍스트 쌍 일관성 스코어링(Xing and Carenini, 2021)과 주제 인식 표현 학습(Gao et al., 2023)이 최첨단 기술을 발전시켰습니다. 그럼에도 불구하고 이러한 모든 기술은 대화의 완전한 맥락적 본질(즉, 의도 및 기타 중요한 상태 변수를 명시적으로 모델링)을 설명하는 데 부족하여 최적이 아닌 결과를 초래할 수 있습니다. 5.3 의도 분류 대화 상태 추적과 관련하여 작업 지향 대화 시스템의 또 다른 근본적인 문제는 의도 분류(IC)입니다. 종종 다른 보완적 문제 슬롯 채우기(SF)와 짝을 이루어, 연구자들은 수년에 걸쳐 광범위한 기술을 제안해 왔습니다(Liu와 Lane, 2016; Zhang과 Wang, 2016; Goo 등, 2018; Qin 등, 2019, 2021). 인기 있는 공개 데이터 세트에서 인상적인 성과를 달성했습니다. Few-shot 기술도 공동 IC/SF 작업을 위한 데이터 제약 시나리오에서 조사되었습니다(Krone 등, 2020; Bhathiya와 Thayasivam, 2020; Liu 등, 2021). DST와 관련이 있지만, IC/SF는 주로 개별 발화를 격리하여 처리하므로 대화 세션 내에서 여러 발화에 걸쳐 복잡한 맥락적 연결을 모델링해야 하는 실제 인간-AI 대화에는 적합하지 않습니다. 6 논의 및 결론 LLM 기반 채팅 시스템은 인간-AI 대화의 지평을 넓혀 새로운
--- METHOD ---
s는 각 턴에서 새로운 상태 요소를 추출합니다(Hu et al., 2022). 그러나 이는 DST 평가 벤치마크가 사용자가 각 턴에서 새롭고 관련성 있는 의도 요소를 제공하고 의도가 서로를 기반으로 하거나 보완하지만 대화 전체에서 근본적으로 변경되거나 충돌하지 않는다는 비교적 좁은 가정을 하기 때문입니다. 앞서 논의했듯이 오픈 도메인 대화는 훨씬 더 다양한 특성을 보이며 다중 의도 및/또는 다중 도메인 대화가 비교적 일반적입니다. = 따라서 턴 수준이 아닌 세그먼트에서 상태를 추출할 것을 제안합니다. 세그먼트 수준 상태를 {Sm:n = (smin, vmn), i 1... Nm:n}으로 정의합니다. 여기서 son은 경계 bm에서 bn까지 세그먼트에 적용된 i번째 슬롯을 나타냅니다. (i) vm:n은 슬롯의 해당 값을 나타내고 Nm:n은 이 세그먼트에 적용된 총 슬롯 수를 나타냅니다. 슬롯-값 쌍의 모든 스키마가 여기에서 유효합니다. 우리는 § 4.1과 부록 B에서 Bing Chat에 대한 특정 상태 스키마를 설명합니다. 2.3 문제 설명 세그먼트와 세그먼트별 상태를 정의했으므로 오픈 도메인 DST에 대한 전체 정의를 설명할 수 있습니다. 사용자-에이전트 발화 쌍 C₁ = [U₁, A1, ..., Ut, At]의 시퀀스가 주어지면 오픈 도메인 대화 상태 추적의 목표를 Yt = Bk U {Sm:n; V(bm, bn) Є Bk}를 공동으로 예측하는 것으로 정의합니다. (1) 여기서 Bk C Bt는 이전에 설명한 세그먼트 경계 인덱스를 나타내고 Sm:n은 경계 bm과 bn 사이의 세그먼트 상태를 나타내며 이는 N개의 임의의 슬롯 값 쌍으로 구성됩니다. Sm:n = {(s):n, v(n), i = 1 . . . Nm:n}. (2) 3가지 프롬프트 전략 이전에 논의했듯이, 실제 대화는 종종 다양한 주제를 논의하기 위해 여러 대화 턴에 걸쳐 확장되는 광범위한 담론을 보입니다. 이러한 장기간의 대화적 특성으로 인해 문맥적 일관성을 추적하기가 매우 어렵습니다. 이전 연구(Hu et al., 2022)는 개별 대화 턴을 분리하고 대화 상태 변화를 추적하기 위해 하나씩 처리하는 것을 목표로 했으며, 이는 미리 정의된 좁은 영역에 국한된 작업 지향 대화에서 상당히 잘 작동했습니다. 그러나 실제 대화는 일반적으로 문맥적 뉘앙스를 적절히 이해하기 위해 여러 턴이 필요한데, 이는 Transformers가 특히 중간에 긴 입력 컨텍스트를 처리할 때 여전히 어려움을 겪기 때문에 어려운 문제입니다(Liu et al., 2023). 이러한 어려움을 해결하기 위해 입력과 출력에 구조를 제공하면서 프로세스의 컨텍스트를 정확하게 보존하는 새로운 턴바이턴 프롬프트 기술을 제안합니다. 아래에서 프롬프트의 이러한 디자인 측면에 대해 논의합니다. 3. 구조화된 출력 및 입력 구조화된 출력 우리의 목표는 세그먼트 경계(바이너리 레이블)와 상태 값(범주 레이블 또는 오픈 텍스트)을 나타내는 대화 턴당 레이블 세트입니다. LLM의 출력에 유연하면서도 구조화된 형식을 제공하기 위해 계층적 XML 형식으로 출력을 생성하도록 지시하는 것을 제안합니다. 우리는 XML이 DST 작업에 코드와 같은 구조를 제공하기 때문에 유리하다고 생각합니다. 이는 일반 텍스트 출력에 비해 성능을 크게 향상시키는 것으로 나타났으며, SQL과 같은 보다 엄격한 출력 형식에 비해 확장 가능하고 유연합니다(Hu et al., 2022). 우리의 접근 방식은 1에서 t까지의 각 턴이 XML 트리를 구성하는 XML 형식을 사용합니다.<T{id}> ...</T{id}> 그리고 그 안에 여러 개의 중첩된 XML 태그가 있습니다. 이러한 중첩된 태그의 레이블(예:<preceding_topical_relation> ...</preceding_topical_relation> ,<intent> ...</intent> , 그리고<domain> ...</domain> 그림 2(iii))는 세그먼트 경계와 관심 슬롯을 나타내며, 여는 태그와 닫는 태그 사이의 각 값은 모델의 추론된 값을 나타냅니다. 이 전략은 두 가지 측면에서 유익합니다. (i) 경계가 있는 잘 정의된 구조화된 형식으로 인해 생성된 출력은 자유형 텍스트보다 레이블 지정 지침과 일치할 가능성이 더 높고 (ii) 잘 구성된 구조화된 출력 형식은 구문 분석하기가 더 쉬워 후처리 요구 사항이 줄어듭니다. 구조화된 입력 LLM을 프롬프트하는 경우 분석 및 추론을 위해 일반 대화 기록을 평면 형식으로 채널링하는 것은 사소한 일이지만 이 선형 구성에 내재된 비구조적 특성으로 인해 참조하고 활용하기 어렵습니다. 원시 대화 사용자: 주석이 달린 참고 문헌을 제공하세요... AI: 다음은 몇 가지 다른 참고 문헌입니다... 사용자: (i) 대화와 지침을 구조화된 프롬프트 템플릿에 삽입<valid preceding_topical_relation><item><name> 예</name><description> ...</description></item></valid_preceding_topical_relation><valid intents><item> ...</item></valid_intents> ## 대화 (iii) 생각의 사슬을 통해 구조화된 차례차례 대화 태그를 생성합니다.<T1><summary> 사용자는 주석이 달린...을 요청합니다.</summary><preceding_topical_relation> 아니요</preceding_topical_relation><intent> 창조</intent><domain> 저널리즘 쓰기와 출판</domain> 법학 석사</T1><T2> 일체 포함:<T1><User> ...을 제공해 주세요.</User><AI> 여기에 몇 가지 예가 있습니다...</AI></T1><T2><summary> ...</summary><preceding_topical_relation> ...</preceding_topical_relation><intent> ...</intent> &#39;<domain> ...</domain></T2> 구조화된 턴<User> ...</User><AI> ...</AI></T2> (ii) 사전 분석 기억(PAR) 각 턴에서 출력을 근거로 컨텍스트 보존 그림 2: S3-DST의 프롬프트 흐름. 원시 대화가 주어지면 (i) 계층적 XML 구조 표현으로 변환하여 유사하게 구조화된 프롬프트 템플릿에 삽입합니다. 프롬프트를 LLM에 전달하고 (ii) 계층적 XML 구조 출력을 얻습니다. 여기서 각 턴에는 (iii) 원하는 세분화 및 상태 레이블 예측과 함께 대화에 대한 PAR 근거 참조가 포함됩니다. 여러 대화 턴에서 다른 정보를 연령화합니다. 출력 형식과 일관되게 이 과제를 처리하기 위해 각 대화 내역이 계층적 XML 형식으로 형성되고 대화 턴이 턴 ID 번호로 표시되는 구조화된 입력 형식을 제안합니다.<T{id}> ...</T{id}> 1에서 t까지 번호가 매겨지고 각 대화 차례는 적절한 XML 태그로 표시된 중첩된 사용자 및 에이전트 차례로 구성됩니다.<user> ...</user> 그리고<agent> ...</agent> ). 출력 중에 턴당 레이블을 추론하도록 LLM에 지시하는 것을 제안하기 때문에 이 입력 체계는 입력 턴을 정확하게 참조하고 긴 대화 맥락에서도 일관성을 유지하는 데 도움이 됩니다. 이 XML 태그 입력 형식과 일관되게 다음 구조를 사용하여 XML 형식 목록에서 모든 유효한 세그먼트 및 상태 범주를 형식화합니다.<valid_category_name><item> {라벨 이름}</item><description> {라벨 설명(있는 경우)}</description><valid_category_name> 경험적으로, 이 구조화된 입력 및 프롬프트 형식은 LLM 생성이 레이블 지정 지침을 따르도록 제한하는 데 도움이 됩니다. 그림 2(i)는 각 유효한 세그먼트 경계와 상태 범주가 먼저 XML 형식의 목록에서 준비되고 이어서 입력 대화가 계층적 구성으로 표시되는 이 형식을 보여줍니다. 3. 사전 분석 기억(PAR) 이전에 논의했듯이, 오픈 도메인 대화는 길고 대화 흐름이 매우 다양할 수 있습니다. 따라서 LLM이 잊거나 환각하지 않고 진화하는 대화 맥락을 정확하게 모니터링할 수 있도록 하는 것이 중요합니다. 이를 위해, 우리는 LLM이 먼저 다음을 사용하여 차례를 요약하도록 지시하는 차례 차례 프롬프트를 위한 기초 전략인 사전 분석 기억(PAR)을 제안합니다.<summary> ...</summary> 영어: 세그먼트 및 상태 값을 제공하기 전에 3개 이하의 문장으로 태그를 지정합니다.PAR은 추론 정확도를 개선하기 위해 관련 중간 출력을 생성하는 기술이기 때문에 사고의 사슬 프롬프트(Wei et al., 2022)에서 영감을 받았습니다.그러나 사고의 사슬과 달리 PAR은 모델의 출력에서 대화에 직접 참조를 제공하는 접지 기술이기도 합니다.그림 2(ii)는 PAR이 대화 상태를 추론하기 위해 분석하기 전에 각 대화 차례의 내용을 다시 참조하는 방법을 보여줍니다.3.3 최종 프롬프트 구성 S3-DST의 최종 프롬프트 흐름은 그림 2에 나와 있습니다.원시 대화와 미리 정의된 세그먼트 및 상태 레이블 집합이 주어지면 레이블을 구조화된 프롬프트 템플릿에 삽입하고 대화를 계층적 XML 구조 표현으로 포맷합니다.프롬프트를 LLM으로 전달하여 PAR을 따르도록 지시한 다음 공동으로 표 1: 평가 테스트 집합 통계. # 전환 # 회전 # 세그먼트/전환 Bing ChatMWOZ 2.1, MWOZ 2.DialSeg1,(평균) 1.3. 세그먼트별로 적용되는 계층적 턴바이턴 세분화 및 상태 레이블 생성. 프롬프트의 전체 텍스트는 부록 A.1.4에 제공됩니다.
--- EXPERIMENT ---
s 및 분석: 우리는 독점적 데이터 세트와 공개 데이터 세트 모두에서 광범위한 실험을 수행하여 비슷한 제로샷 프롬프트에 비해 큰 성과를 달성했습니다. S3DST는 최첨단 제로샷 프롬프트를 달성합니다. Bing Chat 로그를 사용하는 것은 DialSeg711 대화 주제 세분화 벤치마크와 함께 MWOZ 2.1 및 2.4 DST 벤치마크에서 조건을 준수합니다. 2 문제 정의 비공식적으로, 기존 DST의 목표는 사용자와 에이전트의 발화 턴 시퀀스 C₁ = [U₁, A1, ..., Ut, At]²가 주어졌을 때 대화 상태 yt를 예측하는 것입니다. 상태 yt는 슬롯-값 쌍의 집합으로 구성되며, 여기서 슬롯은 특정 애플리케이션 도메인의 의도 속성(예: &quot;레스토랑 이름&quot;, &quot;호텔 주소&quot;)에 해당하고 값은 미리 정의된 범주형 옵션 또는 제한 없는 텍스트에 해당합니다(Budzianowski et al., 2018). 그러나 이전에 논의했듯이 단일 오픈 도메인 대화는 종종 다양한 주제에 걸쳐 잠재적으로 관련성이 없는 여러 의도로 구성됩니다. 실제로 10,000개의 익명화된 Bing Chat 대화에 대한 예비 분석에 따르면 대화의 50% 이상이 여러 사용자 의도를 표시하고 대화의 90% 이상이 여러 주제에 대한 토론을 포함한다고 추정합니다. 따라서 더 큰 대화 내에서 맥락적으로 응집력 있는 &quot;대화 단위&quot;를 찾는 것을 목표로 하는 대화 세분화를 대화 상태 추적과 병합할 것을 제안합니다. 특히 세그먼트 수준에서 상태 추적을 수행하는데, 여기서 목표는 각 세그먼트에 관심 있는 슬롯과 값으로 레이블을 지정하여 대화 내의 여러 세그먼트가 발산하거나 상충되는 상태 값을 가질 수 있도록 하는 것입니다. 이는 오픈 도메인 채팅의 진정한 다양성을 반영합니다. 이 섹션의 나머지 부분에서는 세분화와 상태를 정의하고 마지막으로 공동 작업을 공식화합니다. 2.1 세그먼트 대화 주제 분할(Xing 및 Carenini, 2021; Xia 등, 2022; Gao 등, 2023)에 대한 이전 연구에 따라 대화 세그먼트를 모든 사용자 및 에이전트 발화가 주제적으로 관련된 Ct의 연속된 하위 시퀀스로 정의합니다. 형식적으로 B₁ = [b₁,……., bt−[b₁, … …. …, bt−]는 Ct에서 인접한 사용자-에이전트 발화 쌍 사이의 경계 인덱스를 나타냅니다. 분할의 출력은 경계 인덱스 Bk C BŁ의 집합입니다. 여기서 k는 분할 알고리즘에 의해 결정된 경계 수와 Bing Chat 사용에 대한 범위 [Um, Am, . . . Un, An]을 나타냅니다. 2현재 LLM 기반 채팅 시스템에서는 사용자가 단일 에이전트 응답이 발행되기 전에 여러 발화를 발행할 수 있습니다. 이러한 (드물게) 경우에 우리는 에이전트 응답 이전의 모든 사용자 발화를 단일 발화로 그룹화합니다. 경계 bm과 bn 사이의 연속된 세그먼트를 보냅니다. 여기서 m = [1, t − 1] 및 n ≤ [m, t − 1]. 2.2 세그먼트 상태 일반적으로 대화 상태 추적 방법은 각 턴에서 상태의 새로운 요소를 추출합니다(Hu et al., 2022). 그러나 이는 DST 평가 벤치마크가 사용자가 각 턴에서 새롭고 관련성 있는 의도 요소를 제공하고 의도가 서로를 기반으로 하거나 보완하지만 대화 전체에서 근본적으로 변경되거나 충돌하지 않는다는 비교적 좁은 가정을 하기 때문입니다. 이전에 논의했듯이 오픈 도메인 대화는 훨씬 더 다양한 특성을 보이며 다중 의도 및/또는 다중 도메인 대화가 비교적 일반적입니다. = 따라서 턴 수준이 아닌 세그먼트에서 상태를 추출할 것을 제안합니다. 세그먼트 수준 상태를 {Sm:n = (smin, vmn), i 1... Nm:n}으로 정의합니다. 여기서 son은 경계 bm에서 bn까지의 세그먼트에 적용된 i번째 슬롯을 나타냅니다. (i) vm:n은 슬롯의 해당 값을 나타내고 Nm:n은 이 세그먼트에 적용된 슬롯의 총 수를 나타냅니다. 슬롯 값 쌍의 스키마는 여기에서 유효합니다. § 4.1과 부록 B에서 Bing Chat에 대한 특정 상태 스키마를 설명합니다. 2.3 문제 설명 세그먼트와 세그먼트별 상태를 정의했으므로 오픈 도메인 DST에 대한 전체 정의를 설명할 수 있습니다. 사용자-에이전트 발화 쌍 C₁ = [U₁, A1, ..., Ut, At]의 시퀀스가 주어지면 오픈 도메인 대화 상태 추적의 목표를 Yt = Bk U {Sm:n; V(bm, bn) Є Bk}, (1) 여기서 Bk C Bt는 이전에 설명한 세그먼트 경계 인덱스를 나타내고 Sm:n은 경계 bm과 bn 사이의 세그먼트 상태를 나타내며, 이는 N개의 임의의 슬롯 값 쌍으로 구성됩니다. Sm:n = {(s):n, v(n), i = 1 . . . Nm:n}. (2) 3가지 프롬프트 전략 이전에 논의했듯이 실제 대화는 종종 다양한 주제를 논의하기 위해 여러 대화 턴에 걸쳐 확장되는 광범위한 담론을 보입니다. 이러한 장기간의 대화적 특성으로 인해 문맥적 일관성을 추적하기가 매우 어렵습니다. 이전 연구(Hu et al., 2022)는 개별 대화 턴을 분리하고 대화 상태 변경을 추적하기 위해 하나씩 처리하는 것을 목표로 했으며, 이는 미리 정의된 좁은 도메인에 국한된 작업 지향 대화에서 상당히 잘 작동했습니다. 그러나 실제 대화는 일반적으로 문맥적 뉘앙스를 적절히 이해하기 위해 여러 차례의 전환이 필요한데, 이는 Transformers가 특히 중간에 긴 입력 맥락을 처리할 때 여전히 어려움을 겪기 때문에 어려운 문제입니다(Liu et al., 2023). 이러한 어려움을 해결하기 위해 입력과 출력에 구조를 제공하면서 프로세스의 맥락을 정확하게 보존하는 새로운 차례별 프롬프트 기술을 제안합니다. 아래에서 프롬프트의 이러한 디자인 측면에 대해 설명합니다. 3. 구조화된 출력 및 입력 구조화된 출력 목표는 세그먼트 경계(바이너리 레이블)와 상태 값(범주형 레이블 또는 오픈 텍스트)을 나타내는 대화 차례당 레이블 집합입니다. LLM의 출력에 유연하면서도 구조화된 형식을 제공하기 위해 계층적 XML 형식으로 출력을 생성하도록 지시하는 것을 제안합니다. 우리는 XML이 DST 작업에 코드와 같은 구조를 제공하기 때문에 유리하다고 생각합니다. 이는 일반 텍스트 출력에 비해 성능을 크게 향상시키는 것으로 나타났으며, SQL과 같은 보다 엄격한 출력 형식에 비해 확장 가능하고 유연합니다(Hu et al., 2022). 우리의 접근 방식은 1에서 t까지의 각 턴이 XML 트리를 구성하는 XML 형식을 사용합니다.<T{id}> ...</T{id}> 그리고 그 안에 여러 개의 중첩된 XML 태그가 있습니다. 이러한 중첩된 태그의 레이블(예:<preceding_topical_relation> ...</preceding_topical_relation> ,<intent> ...</intent> , 그리고<domain> ...</domain> 그림 2(iii))는 세그먼트 경계와 관심 슬롯을 나타내며, 여는 태그와 닫는 태그 사이의 각 값은 모델의 추론된 값을 나타냅니다. 이 전략은 두 가지 측면에서 유익합니다. (i) 경계가 있는 잘 정의된 구조화된 형식으로 인해 생성된 출력은 자유형 텍스트보다 레이블 지정 지침과 일치할 가능성이 더 높고 (ii) 잘 구성된 구조화된 출력 형식은 구문 분석하기가 더 쉬워 후처리 요구 사항이 줄어듭니다. 구조화된 입력 LLM을 프롬프트하는 경우 분석 및 추론을 위해 일반 대화 기록을 평면 형식으로 채널링하는 것은 사소한 일이지만 이 선형 구성에 내재된 비구조적 특성으로 인해 참조하고 활용하기 어렵습니다. 원시 대화 사용자: 주석이 달린 참고 문헌을 제공하세요... AI: 다음은 몇 가지 다른 참고 문헌입니다... 사용자: (i) 대화와 지침을 구조화된 프롬프트 템플릿에 삽입<valid preceding_topical_relation><item><name> 예</name><description> ...</description></item></valid_preceding_topical_relation><valid intents><item> ...</item></valid_intents> ## 대화 (iii) 생각의 사슬을 통해 구조화된 차례차례 대화 태그를 생성합니다.<T1><summary> 사용자는 주석이 달린...을 요청합니다.</summary><preceding_topical_relation> 아니요</preceding_topical_relation><intent> 창조</intent><domain> 저널리즘 쓰기와 출판</domain> 법학 석사</T1><T2> 일체 포함:<T1><User> ...을 제공해 주세요.</User><AI> 여기에 몇 가지 예가 있습니다...</AI></T1><T2><summary> ...</summary><preceding_topical_relation> ...</preceding_topical_relation><intent> ...</intent> &#39;<domain> ...</domain></T2> 구조화된 턴<User> ...</User><AI> ...</AI></T2> (ii) 사전 분석 기억(PAR) 각 턴에서 출력을 근거로 컨텍스트 보존 그림 2: S3-DST의 프롬프트 흐름. 원시 대화가 주어지면 (i) 계층적 XML 구조 표현으로 변환하여 유사하게 구조화된 프롬프트 템플릿에 삽입합니다. 프롬프트를 LLM에 전달하고 (ii) 계층적 XML 구조 출력을 얻습니다. 여기서 각 턴에는 (iii) 원하는 세분화 및 상태 레이블 예측과 함께 대화에 대한 PAR 근거 참조가 포함됩니다. 여러 대화 턴에서 다른 정보를 연령화합니다. 출력 형식과 일관되게 이 과제를 처리하기 위해 각 대화 내역이 계층적 XML 형식으로 형성되고 대화 턴이 턴 ID 번호로 표시되는 구조화된 입력 형식을 제안합니다.<T{id}> ...</T{id}> 1에서 t까지 번호가 매겨지고 각 대화 차례는 적절한 XML 태그로 표시된 중첩된 사용자 및 에이전트 차례로 구성됩니다.<user> ...</user> 그리고<agent> ...</agent> ). 출력 중에 턴당 레이블을 추론하도록 LLM에 지시하는 것을 제안하기 때문에 이 입력 체계는 입력 턴을 정확하게 참조하고 긴 대화 맥락에서도 일관성을 유지하는 데 도움이 됩니다. 이 XML 태그 입력 형식과 일관되게 다음 구조를 사용하여 XML 형식 목록에서 모든 유효한 세그먼트 및 상태 범주를 형식화합니다.<valid_category_name><item> {라벨 이름}</item><description> {라벨 설명(있는 경우)}</description><valid_category_name> 경험적으로, 이 구조화된 입력 및 프롬프트 형식은 LLM 생성이 레이블 지정 지침을 따르도록 제한하는 데 도움이 됩니다. 그림 2(i)는 각 유효한 세그먼트 경계와 상태 범주가 먼저 XML 형식의 목록에서 준비되고 이어서 입력 대화가 계층적 구성으로 표시되는 이 형식을 보여줍니다. 3. 사전 분석 기억(PAR) 이전에 논의했듯이, 오픈 도메인 대화는 길고 대화 흐름이 매우 다양할 수 있습니다. 따라서 LLM이 잊거나 환각하지 않고 진화하는 대화 맥락을 정확하게 모니터링할 수 있도록 하는 것이 중요합니다. 이를 위해, 우리는 LLM이 먼저 다음을 사용하여 차례를 요약하도록 지시하는 차례 차례 프롬프트를 위한 기초 전략인 사전 분석 기억(PAR)을 제안합니다.<summary> ...</summary> 영어: 세그먼트 및 상태 값을 제공하기 전에 3개 이하의 문장으로 태그를 지정합니다.PAR은 추론 정확도를 개선하기 위해 관련 중간 출력을 생성하는 기술이기 때문에 사고의 사슬 프롬프트(Wei et al., 2022)에서 영감을 받았습니다.그러나 사고의 사슬과 달리 PAR은 모델의 출력에서 대화에 직접 참조를 제공하는 접지 기술이기도 합니다.그림 2(ii)는 PAR이 대화 상태를 추론하기 위해 분석하기 전에 각 대화 차례의 내용을 다시 참조하는 방법을 보여줍니다.3.3 최종 프롬프트 구성 S3-DST의 최종 프롬프트 흐름은 그림 2에 나와 있습니다.원시 대화와 미리 정의된 세그먼트 및 상태 레이블 집합이 주어지면 레이블을 구조화된 프롬프트 템플릿에 삽입하고 대화를 계층적 XML 구조 표현으로 포맷합니다.프롬프트를 LLM으로 전달하여 PAR을 따르도록 지시한 다음 공동으로 표 1: 평가 테스트 집합 통계. # 전환 # 회전 # 세그먼트/전환 Bing ChatMWOZ 2.1, MWOZ 2. DialSeg1, (평균) 1.3. 세그먼트별로 적용되는 계층적 턴바이턴 분할 및 상태 레이블을 생성합니다. 프롬프트의 전체 텍스트는 부록 A.1에 제공됩니다. 4 실험 여러 데이터 세트에 대해 포괄적인 평가를 수행합니다. 주로 도메인 전문가가 주석을 단 완전히 익명화된 Bing Chat 로그에 대한 접근 방식을 평가합니다. 또한 공개 벤치마크 데이터 세트 MultiWOZ(Budzianowski et al., 2018) 및 DialSeg(Xu et al., 2021)를 사용하여 표준 작업 지향 DST 및 분할 작업에서 S3-DST를 평가합니다. 이러한 데이터 세트에 대한 자세한 설명은 표 1의 데이터 세트 통계와 함께 아래에 제공됩니다.4. 내부 인간-LLM 대화 데이터 세트 실제 오픈 도메인 인간-LLM 대화에 대한 접근 방식의 효능을 평가하기 위해 Bing 검색 엔진이 지원하는 LLM 채팅 인터페이스인 Microsoft의 Bing Chat 시스템에서 익명화된 채팅 로그 데이터를 수집했습니다.벤치마크 구축 2023년 4월 5일부터 2023년 4월 30일까지 Bing Chat에서 진행된 484개의 영어 대화를 두 가지 접근 방식을 통해 샘플링합니다.(i) 무작위 및 (ii) 5턴 이상인 &quot;긴&quot; 대화. 이 두 가지 접근 방식을 50/50으로 균형 있게 조정합니다. 제로샷 가정에서 작동하므로 학습 데이터가 필요하지 않습니다. 따라서 개발을 위해 150개의 대화를 유지하고 나머지 334개를 테스트용으로 유지합니다.주석 평가를 위한 기준 진실 레이블을 얻기 위해 세그먼트 및 상태에 대한 인간 주석을 수집했습니다. 우리는 Bing Chat 시스템에 대한 높은 수준의 기술적 전문성과 친숙함을 갖춘 사내 주석자 3명을 모집했습니다. 각 턴에 대해 주석자에게 이진 IsSegmentBoundary 레이블, 범주형 SegmentIntent 레이블, 범주형 SegmentDomain 레이블을 제공하도록 지시했습니다. 턴과 이전 컨텍스트 간의 주제적 관계를 식별할 수 없는 경우 주석자에게 세그먼트 경계를 표시하도록 지시했습니다. 의도와 도메인의 경우 Bing Chat 시스템을 위해 사내에서 개발한 택소노미를 사용했는데, 여기에는 4개의 의도(정보 탐색, 분석, 생성 및 개방형 발견)와 49개의 도메인이 포함됩니다(전체 목록은 부록 B.1 참조). 도메인 수가 많기 때문에 턴당 주석자에게 4개의 후보 도메인 값과 &quot;기타&quot; 옵션을 제공했습니다. 부록 B에는 주석 체계와 도메인 샘플링 절차에 대한 자세한 내용이 나와 있습니다. 전체 데이터 세트에 레이블을 지정하기 전에 주석자 간 일치를 보장하기 위해 먼저 무작위로 선택한 10개의 대화(총 68개의 턴)에 대한 주석을 수집하고 레이블 유형별로 Fleiss 카파(Fleiss, 1971)를 계산했습니다. IsSegmentBoundary의 경우 k = 0.83, SegmentIntent의 경우 k = 0.74, SegmentDomain의 경우 K = 0.88의 Fleiss 카파를 관찰했는데, 이는 모두 Fleiss 카파 척도에서 높은 일치로 간주됩니다. 4.2 공개 벤치마크 광범위하게 오픈된 도메인인 Bing Chat 데이터를 반영하는 기존의 공개 대화 벤치마크는 알려진 바가 없습니다. 따라서 세 가지 데이터 세트를 사용하여 공개 벤치마크에서 별도의 DST 및 세분화 평가를 수행합니다. MultiWOZ MultiWOZ(MWOZ) 다중 도메인 대화 데이터 세트(Budzianowski et al., 2018)는 현재 가장 일반적인 DST 벤치마크입니다. MWOZ는 1,000개의 테스트 대화로 구성된 작업 지향 데이터 세트입니다. 저희는 원본의 두 가지 업데이트된 버전인 MWOZ 2.1(Eric et al., 2019)과 2.4(Ye et al., 2021)를 사용합니다. 후자는 MWOZ의 &quot;가장 깨끗한&quot; 버전으로 간주되는 반면 전자는 문헌에서 더 자주 사용되었습니다. DialSeg711 DialSeg711 벤치마크는 (Xu et al., 2021)에 의해 도입되었으며 최근 대화 분할 연구에 자주 사용되었습니다. 기존 작업 지향 대화 코퍼스에서 대화를 결합하여 711개의 다중 세그먼트 대화를 구성하는 영어 데이터 세트입니다. 4.3 기준선 저희는 S3-DST와의 공정한 비교를 위해 제로샷 LLM 프롬프트만 기준선으로 고려합니다. 저희는 아래에서 다양한 데이터 세트에 대한 기준선과 고려 사항을 논의합니다. 모든 원래 프롬프트는 부록 A에 제공됩니다. LLM 호출당 최대 1500개의 출력 토큰을 설정했으며 온도는 0입니다. 표 2: S3-DST는 내부 Bing Chat 벤치마크에서 상태 추적에 대한 최첨단 성능을 달성합니다. 모든 프롬프트는 GPT4로 실행됩니다. 개별 정확도 JGA 세그먼트 의도 도메인 I/DS/I/D TBT-DST 0.0.0.IC-DST 0.0.0.0.4610 0.S3-DST(PAR 없음) 0.0.7173 0.0.4377 0.S3-DST(비구조화 입력) S3-DST 0.0.7163 0.0.4640 0.0.0.7366 0.0.4752 0.Bing Chat 이 데이터 세트에서 우리는 IC-DST를 기본 기준으로 간주합니다. 이는 (Hu et al., 2022)에서 도입한 프롬프팅 전략의 제로샷 버전으로, 세그먼트 및 대화 상태를 공동으로 추적하기 위해 오픈 도메인 대화 설정에 크게 적용되었습니다. TBT-DST 기준선은 분할 지침을 포함하지 않고 S3DST의 한 버전으로, S3-DST 프롬프트 구성을 사용하여 차례대로 의도 및 도메인 레이블을 얻습니다. 또한 프롬프트의 두 가지 핵심 측면인 PAR과 XML 구조적 포맷팅의 중요성을 분석하기 위해 S3-DST의 두 가지 절제도 고려합니다. PAR 없음은 PAR 지침이 없는 S3-DST 프롬프트를 나타내고, 구조화되지 않은 입력은 모든 지침과 대화를 XML이 아닌 일반 텍스트로 포맷하는 S3-DST 프롬프트를 나타냅니다. 모든 프롬프트에 백본 LLM으로 GPT4를 사용합니다. MWOZ MWOZ 작업 지향 대화 상태 추적 데이터 세트의 경우 Hu et al.(2022)에서 보고한 Codex-175B를 사용하여 IC-DST와 비교합니다. 또한 기준선 성능에서 백본 모델 개선을 설명하기 위해 GPT-로 제로샷 IC-DST를 재평가합니다. 마지막으로, (Heck et al., 2023)에서 보고한 MWOZ 2.1에서의 제로샷 ChatGPT 성능과 비교합니다. DialSeg711 우리는 비지도 TextTiling(Hearst, 1997), CSM(Xing and Carenini, 2021), DialStart(Gao et al., 2023) 방법을 고려합니다. 우리는 (Gao et al., 2023)의 모든 숫자를 다시 인쇄합니다. 마지막으로, 우리는 원래 IC-DST(Hu et al., 2022)와 동일한 SQL 출력 형식으로 세분화 레이블을 유도하도록 프롬프트된 IC-DST 기준선을 사용합니다. 4.4 메트릭 상태 추적을 위해, 우리는 모든 상태 값이 올바르게 추론되는 턴의 비율을 측정하는 Joint Goal Accuracy(JGA)를 고려합니다. Bing Chat의 경우 Binned JGA(I/D) 0.0.0.500.0.(0,3] S3-DST .-X. S3-DST(PAR 없음) IC-DST(3,5] (5,10] 대화 길이(턴 수) (10,20] 그림 3: S3-DST는 컨텍스트 추적을 강조하여 모든 길이의 대화에 대한 기준선보다 성능이 뛰어납니다. 길이에 따라 Bing Chat 대화를 빈으로 나누고 빈당 JGA를 표시합니다. 대화 길이가 길어질수록 두 기준선 모두 성능이 크게 저하되므로 PAR 기반 전략의 중요성을 확인할 수 있습니다. 표 3: S3-DST는 공개 대화 상태 추적 벤치마크 MWOZ 2.1 + 2.4에서 제로샷 LLM 기준선과 비교하여 최첨단 JGA를 달성합니다. JGA MWOZ 2.MWOZ 2.IC-DST(Codex) IC-DST (GPT4) 0.0.0.0.ChatGPT 0.S3-DST 0.0.intent 및 domain(I/D)은 관심 있는 실제 상태 값이고, 완전성을 위해 세그먼트, 의도 및 도메인 정확도(S/I/D)가 있는 JGA입니다. 또한 Bing Chat에서 세분화, 의도 및 도메인 정확도를 별도로 보고하여 오픈 도메인 대화 데이터에서 LLM의 현재 기능과 한계를 파악합니다. 세분화의 경우 PK 및 WindowDiff(Pevzner 및 Hearst, 2002)를 고려합니다. 둘 다 오류 메트릭(즉, 낮을수록 좋음)으로, 조정 가능한 슬라이딩 윈도우를 사용하여 예측된 세그먼트 경계와 기준 진실 세그먼트 경계 간의 차이를 정량화합니다. 표 4: MWOZ 2.1에서 제로 샷 도메인별 비교(JGA). 표 5: S3-DST는 공개 세분화 벤치마크 DialSeg711에서 최첨단 성능을 달성합니다. attr. hotel 도메인별 JGA rest. Pk(↓) WindowDiff (↓) 택시 기차 TextTiling 0.0.IC-DST (Codex) IC-DST (GPT4) ChatGPT S3-DST 0.5997 0.4669 0.5728 0.7135 0.0.7177 0.4872 0.6526 0.7781 0.0.5270 0.4200 0.5580 0.7090 0.0.6781 0.5215 0.6713 0.8258 0.CSM 0.0.DialSTART 0.0.IC-DST 0.0.S3-DST 0.0.4.5 결과 Bing Chat 표 2에서 볼 수 있듯이, S3-DST 프롬프트는 턴 전체에서 의도, 도메인 및 JGA에서 가장 높은 성능을 달성합니다. 우리는 다음과 같은 관찰을 합니다. 첫째, 세분화를 명시적으로 수행하지 않는 TBT-DST는 지금까지 가장 약한 기준선입니다. 이는 LLM에 세그먼트 내에서 동일한 의도와 도메인을 사용하도록 지시하지 않으면 LLM이 더 완전한 이전 맥락을 고려하지 않고 턴의 내용을 과도하게 인덱싱하는 경향이 있기 때문입니다. 이로 인해 일관된 단일 주제 대화 내에서 턴 간에 충돌하는 의도 및 도메인 레이블이 발생합니다. 둘째, IC-DST의 수정된 버전은 매우 강력한 기준선입니다. 그러나 IC-DST는 구조화된 출력을 사용하지만 해당 구조화된 입력 표현이 없습니다. 존재하지 않는 턴에 대한 환각이 S3-DST에 비해 비교적 흔하기 때문에 어떤 경우에는 이로 인해 성능이 저하됩니다. 마지막으로 S3-DST의 두 가지 절제는 모두 S3-DST에 비해 성능이 낮아 LLM이 생성 중에 참조할 수 있는 PAR 및 구조화된 입력의 중요성을 확인합니다. 실제로 대화 길이와 성능 간의 관계를 나타낸 그림 3은 S3-DST가 대화가 길어짐에 따라 no-PAR ablation의 성능이 급격히 저하되는 것을 방지한다는 것을 보여줍니다. 예를 들어, no-PAR ablation은 3턴 이하의 대화에서는 S3-DST와 비슷한 성능을 보이지만, 턴 이상의 대화에서는 JGA가 10포인트 이상 떨어집니다. 이러한 결과는 특히 긴 대화에서 PAR의 필요성을 강조합니다. MWOZ 표 3과 4는 전체 및 도메인별 MWOZ 숫자를 제공합니다. S3-DST는 강력한 LLM에 비해 최첨단 제로샷 JGA를 큰 차이로 달성합니다. 가장 강력한 제로샷 기준선인 IC-DST(GPT4)조차도 MWOZ 2.1에서는 JGA가 거의 5포인트, MWOZ 2.4에서는 JGA가 . 거의 모든 개별 도메인에서 S3-DST는 IC-DST(GPT4)보다 성능이 뛰어나고, 예를 들어 트레인 도메인에서 13포인트 이상의 JGA 개선과 같이 큰 차이로 더 뛰어납니다. DialSeg711 마지막으로 표 5는 DialSeg711의 성능을 보여줍니다. S3-DST는 이 데이터 세트에서 거의 오류가 없으며, 데이터 세트의 구성을 감안하면 놀라운 일이 아닙니다. 구체적으로 DialSeg711은 매우 다른 주제에 대한 대화를 결합하여 구성되므로 세그먼트 간에 매우 인위적이고 갑작스러운 컨텍스트 전환이 발생합니다. 그러나 IC-DST 프롬프트 기준선은 S3-DST보다 훨씬 더 높은 오류를 발생시킵니다. 자세히 살펴보면 LLM이 데이터 세트의 여러 대화에 대한 대화 컨텍스트를 추적하지 못해 원래 대화 컨텍스트를 잊어버리는 것을 알 수 있습니다. 이러한 결과는 성공적인 세분화를 위해 PAR 및 대화 컨텍스트 추적의 중요성을 강조합니다. S3-DST의 강력한 성능은 또한 DialSeg711이 앞으로 LLM에게 충분히 어려운 작업이 아닐 수 있음을 시사하며, 궁극적으로 상태 추적 성능을 개선하는 것이 분할의 목표이기 때문에 공동 분할 및 상태 추적의 필요성을 더욱 부추깁니다.5 관련 연구 5.1 대화 상태 추적 인간-AI 대화의 흐름을 정확하게 추적하려면 사용자의 의도와 목표를 추론하는 데 강력한 상태 추적이 중요합니다.MultiWOZ(Budzianowski et al., 2018) 데이터 세트를 커뮤니티에 도입한 이후 DST 성능을 개선하기 위한 수많은 기술이 제안되었습니다.복제 메커니즘(Lei et al., 2018), 전이 학습(Wu et al., 2019), 데이터 증강(Zhang et al., 2020), 대조적 사전 학습(Wu et al., 2020) 등을 포함한 이전 시도는 감독 미세 조정 시나리오에서 개선을 가져왔습니다. 한편, MultiWOZ도 여러 차례 주석 개정을 거쳤습니다(Eric 등, 2019; Ye 등, 2021; Zang 등, 2020; Han 등, 2020). 다른 기술(Peng 등, 2021; Lin 등, 2020; Zhao 등, 2022; Yu 등, 2020; Platanios 등, 2021)도 제안되었지만, 데이터 레이블링의 리소스 집약적이고 힘든 특성으로 인해 점차 few-shot 및 zero-shot 대화 상태 추적 탐색으로 관심이 옮겨갔습니다(Shin 등, 2022; Hu 등, 2022; Heck 등, 2023). 이 분야의 최첨단 접근 방식(Hu et al., 2022)은 상태 추적을 위해 LLM을 활용할 수 있지만, 실제 확장 대화 세션에서 성능을 잠재적으로 저하시킬 수 있는 적절한 접지 메커니즘이 현저히 부족합니다.게다가 앞서 언급한 이전 작업 중 어느 것도 유연한 오픈 도메인 LLM 기반 채팅 시스템에서 널리 퍼져 있는 주제 일관성과 컨텍스트 전환을 설명하지 않습니다.5.2 대화 주제 세분화 대화를 주제별로 일관된 단위로 세분화하는 것은 다운스트림 대화 모델링의 성공에 기초가 됩니다.주석이 달린 데이터의 부족은 대화 주제 세분화에서 과제였지만, 최근의 비지도 시도는 주제 세분화에서 몇 가지 유망한 결과를 보여주었습니다.더 구체적으로, 고전적인 텍스트 세분화 알고리즘인 TextTiling(Hearst, 1997)을 기반으로 하는 확장은 이 측면에서 주로 벤치마크를 주도했습니다(Song et al., 2016). 더 최근에는 텍스트 쌍 일관성 점수(Xing 및 Carenini, 2021)와 주제 인식 표현 학습(Gao 등, 2023)이 최첨단 기술을 발전시켰습니다. 그럼에도 불구하고 이러한 모든 기술은 대화의 완전한 맥락적 본질을 설명하는 데 부족합니다(즉, 의도 및 기타 중요한 상태 변수를 명시적으로 모델링). 이는 최적이 아닌 결과로 이어질 수 있습니다. 5.3 의도 분류 대화 상태 추적과 관련하여 작업 지향 대화 시스템의 또 다른 근본적인 문제는 의도 분류(IC)입니다. 종종 다른 보완적인 문제 슬롯 채우기(SF)와 함께 연구자들은 수년에 걸쳐 광범위한 기술을 제안하여(Liu 및 Lane, 2016; Zhang 및 Wang, 2016; Goo 등, 2018; Qin 등, 2019, 2021) 인기 있는 공개 데이터 세트에서 인상적인 성과를 달성했습니다. Few-shot 기법은 또한 공동 IC/SF 작업을 위한 데이터 제약 시나리오에서 조사되었습니다(Krone et al., 2020; Bhathiya and Thayasivam, 2020; Liu et al., 2021). DST와 관련이 있지만 IC/SF는 주로 개별 발화를 격리하여 처리하므로 대화 세션 내에서 여러 발화에 걸쳐 복잡한 맥락적 연결을 모델링해야 하는 실제 인간-AI 대화에는 적합하지 않습니다. 6 토론 및
--- CONCLUSION ---
LLM 기반 채팅 시스템은 인간-AI 대화의 지평을 넓혀 사용자 의도를 추적하기 위한 새로운 방법을 보장합니다. 따라서 주제별로 일관된 세그먼트와 세그먼트당 상태 의도 변수를 공동으로 추적하여 대화 상태 추적을 오픈 도메인 대화 시스템 영역으로 가져옵니다. 모든 분야에서 주석을 달기에는 비실용적이기 때문에 제로샷 설정을 가정해야 하므로 오픈 도메인 상태 추적을 위해 제로샷 프롬프팅을 사용하는 구조화된 분할 및 상태 추적 방식인 S3-DST를 제안합니다. S3-DST는 프롬프트를 XML 형식으로 구조화하고 제안된 그라운딩 메커니즘(PAR)을 활용하여 긴 컨텍스트 추적을 수행합니다. 독점 및 공개 데이터 세트에 대한 광범위한 실험을 통해 S3-DST는 대화 상태 추적 및 분할 방식에서 최첨단 제로샷 기술보다 성능이 크게 향상되었습니다. 앞으로 LLM 기반 채팅 시스템이 더 보편화됨에 따라 대화 시스템 연구가 오픈 도메인 대화를 이해하고 모델링하는 방향으로 더욱 이동할 것으로 예상합니다. 이와 관련하여, 우리는 확장된 컨텍스트 보존을 위한 기술을 더욱 연구하고 개발하여 다른 중요한 대화 모델링 작업과 함께 DST에서 그라운딩을 개선하는 것을 목표로 합니다. 참고문헌 Hemanthage S Bhathiya 및 Uthayasanker Thayasivam. 2020. Meta learning for few-shot joint intent detection and slot-filling. 제5회 기계 학습 기술 국제 컨퍼런스 회의록, 86-92페이지. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan 및 Milica Gasic. 2018. Multiwoz-작업 지향 대화 모델링을 위한 대규모 다중 도메인 wizard-of-oz 데이터 세트. 2018년 자연어 처리 경험적 방법 컨퍼런스 회의록, 5016-5026페이지. Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, Dilek Hakkani-Tur. 2019. Multiwoz 2.1: 상태 수정 및 상태 추적 기준이 있는 통합 다중 도메인 대화 데이터 세트. arXiv 사전 인쇄본 arXiv:1907.01669. Joseph L Fleiss. 1971. 여러 평가자 간의 명목 척도 일치도 측정. Psychological Bulletin, 76(5):378. Haoyu Gao, Rui Wang, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li. 2023. 주제 인식 발화 표현을 사용한 비지도 대화 주제 분할. 정보 검색 연구 및 개발에 관한 제46회 국제 ACM SIGIR 연례 컨퍼런스 회의록. Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approach to conversational ai. In The 41st international ACM SIGIR conference on research &amp; development in information retrieval, pages 13711374. Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen. 2018. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 753-757. Ting Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian, Chongxuan Huang, Wei Peng, and Minlie Huang. 2020. Multiwoz 2.3: 주석 수정 및 공동 참조 주석으로 강화된 다중 도메인 작업 지향 데이터 세트. arXiv 사전 인쇄본 arXiv:2010.05594. Marti A Hearst. 1997. 텍스트 타일링: 텍스트를 다중 단락 하위 주제 구절로 분할. 계산 언어학, 23(1):33–64. Michael Heck, Nurul Lubis, Benjamin Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, Hsienchin Lin, Carel van Niekerk, Milica Gasic. 2023. 제로샷 대화 상태 추적을 위한 ChatGPT: 솔루션인가 기회인가? 계산 언어학 협회(제2권: 단편 논문) 제61회 연례 회의록, 936-950쪽, 캐나다 토론토. Association for Computational Linguistics. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2627-2643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Krone, Yi Zhang, and Mona Diab. 2020. Learning to classify intents and slot label given a few examples. arXiv preprint arXiv:2004.10793. Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei Yin. 2018. Sequicity: Simplifying task-driven dialogue systems with single sequence-to-sequence architectures. 제56회 연례 총회 논문집(제1권: 장문 논문), 1437-1447쪽. Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung. 2020. Mintl: 과제 지향 대화 시스템을 위한 최소 전이 학습. arXiv 사전 인쇄본 arXiv:2009.12005. Bing Liu와 Ian Lane. 2016. 조인트 의도 감지 및 슬롯 채우기를 위한 주의 기반 순환 신경망 모델. arXiv 사전 인쇄본 arXiv:1609.01454. Han Liu, Feng Zhang, Xiaotong Zhang, Siyang Zhao, Xianchao Zhang. 2021. fewshot 의도 분류 및 슬롯 채우기를 위한 명시적 조인트 및 지도 대조 학습 프레임워크. arXiv 사전 인쇄본 arXiv:2110.13691. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv 사전 인쇄본 arXiv:2307.03172. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2021. Soloist: Building task bots at scale with transfer learning and machine teaching. Transactions of the Association for Computational Linguistics, 9:807–824. Lev Pevzner and Marti A Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19– 36. Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Wolfe, Jacob Andreas, Dan Klein. 2021. 값에 독립적인 대화 의미 구문 분석. 제59회 전산 언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 3666-3681쪽, 온라인. 전산 언어학 협회. Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, Ting Liu. 2019. 구어 이해를 위한 토큰 수준 의도 감지 기능이 있는 스택 전파 프레임워크. arXiv 사전 인쇄본 arXiv:1909.02188. Libo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang, Sendong Zhao, and Ting Liu. 2021. 조인트 슬롯 채우기 및 의도 감지를 위한 공동 상호 작용 변환기. ICASSP 2021-2021 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 8193-8197페이지. IEEE. 신재민, 유한열, 문형돈, 안드레아 마도토, 박준영. 2022. 대화 요약을 대화 상태로(DS2), few-shot 대화 상태 추적을 위한 템플릿 기반 요약. ACL 2022의 연구 결과, 3824-3846페이지, 더블린, 아일랜드. ACL. Yiping Song, Lili Mou, Rui Yan, Li Yi, Zinan Zhu, Xiaohua Hu, and Ming Zhang. 2016. 임베딩 강화 텍스트 타일링을 통한 대화 세션 분할. arXiv 사전 인쇄본 arXiv:1610.03955. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837. Jason D Williams, Antoine Raux, and Matthew Henderson. 2016. 대화 상태 추적 챌린지 시리즈: 리뷰. Dialogue &amp; Discourse, 7(3):4–33. Chien-Sheng Wu, Steven CH Hoi, Richard Socher, and Caiming Xiong. 2020. TOD-BERT: 과제 지향 대화를 위한 사전 훈련된 자연어 이해. 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 917-929쪽, 온라인. Association for Computational Linguistics. Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, Pascale Fung. 2019. 과제 지향 대화 시스템을 위한 이전 가능한 다중 도메인 상태 생성기. Association for Computational Linguistics의 제57회 연례 회의 회의록, 808-819쪽, 이탈리아 피렌체. Association for Computational Linguistics. Jinxiong Xia, Cao Liu, Jiansong Chen, Yuchen Li, Fan Yang, Xunliang Cai, Guanglu Wan, Houfeng Wang. 2022. 이웃 평활화를 사용한 병렬 추출 네트워크를 통한 대화 주제 분할. 정보 검색 연구 및 개발에 관한 제45회 국제 ACM SIGIR 컨퍼런스 회의록, 2126-2131쪽. Linzi Xing과 Giuseppe Carenini. 2021. 발화 쌍 일관성 점수로 비지도 대화 주제 분할 개선. 담화 및 대화에 관한 특별 관심 그룹의 제22회 연례 회의 회의록, 167-177쪽, 싱가포르 및 온라인. 계산 언어학 협회. Yi Xu, Hai Zhao, Zhuosheng Zhang. 2021. Topicaware 다중 턴 대화 모델링. 인공지능에 관한 AAAI 컨퍼런스 회의록, 35권, 14176-14184쪽. Fanghua Ye, Jarana Manotumruksa, Emine Yilmaz. 2021. Multiwoz 2.4: 상태 추적 평가를 개선하기 위한 필수 주석 수정이 포함된 다중 도메인 작업 지향 대화 데이터 세트. arXiv 사전 인쇄본 arXiv:2104.00773. Tao Yu, Rui Zhang, Alex Polozov, Christopher Meek, Ahmed Hassan Awadallah. 2020. Score: 대화 의미 구문 분석에서 맥락 표현을 위한 사전 학습. International Conference on Learning Representations에서. Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, Jindong Chen. 2020. Multiwoz 2.2: 추가 주석 수정 및 상태 추적 기준이 포함된 대화 데이터 세트. arXiv 사전 인쇄본 arXiv:2007.12720. Xiaodong Zhang 및 Houfeng Wang. 2016. 구어체 이해를 위한 의도 결정 및 슬롯 채우기의 공동 모델. IJCAI, 16권, 2993-2999페이지. Yichi Zhang, Zhijian Ou, Zhou Yu. 2020. 동일한 맥락에서 여러 가지 적절한 응답을 고려하는 작업 지향적 대화 시스템. AAAI 인공지능 컨퍼런스 회의록, 34권, 9604-9611페이지. Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang, Harrison Lee, Abhinav Rastogi, Izhak Shafran, Yonghui Wu. 2022. 설명 중심 작업 지향적 대화 모델링. arXiv 사전 인쇄본 arXiv:2201.08904. A 프롬프트<preceding_topical_relation> {유효한 선행 A.1 S3-DST 프롬프트 Bing Chat 아래는 S3-DST에 대한 전체 프롬프트이며, 템플릿 값은 중괄호로 묶인 의도 레이블 이름 또는 설명으로 대체됩니다. 부록 B는 상태 값의 전체 목록을 제공합니다.<valid_domains><item> {유효한 도메인 라벨 이름}</item> 주제별 관계 라벨}</preceding_topical_relation><intent> {유효한 의도 라벨}</intent><domain> {유효한 도메인 라벨}</domain></T{turn number}> ## 입력 ## {XML 구조의 대화} ## 출력 ##</valid_domains><valid_preceding_topical_relation><item><name> 예</name><desc> 현재 차례는 대화 맥락과 어느 정도 주제/하위 주제와 관련이 있습니다.</desc></item><item><name> 아니요</name> 또는 이전의**<desc> 현재 턴은 이전 대화 맥락과 주제/하위 주제와 전혀 관련이 없거나 대화의 첫 번째 턴으로, 새로운 대화 세그먼트의 시작을 나타냅니다.</desc></item></valid_preceding_topical_relation><valid_intents><item><name> {유효한 의도 레이블 이름}</name><desc> {의도 설명}</desc></item> &quot;No PAR&quot; 기준선의 경우 프롬프트에서 턴 요약 지침과 요약 태그를 제거합니다. &quot;Unstructured input&quot; 기준선의 경우 대화를 T1에서 T{t}까지 번호가 매겨진 일반 텍스트 턴 목록으로 입력합니다. TBT-DST 기준선의 경우 프롬프트에서 모든 세분화 지침과 레이블을 제거하고 모델에서 턴당 유효한 의도와 도메인을 출력하도록 합니다. DialSeg711 데이터 세트의 경우 의도와 도메인과 관련된 모든 지침과 값을 제거하고 모델에서 턴 수준 요약과 세그먼트 레이블만 출력하도록 합니다. MWOZ 아래는 MWOZ 데이터 세트의 S3-DST 프롬프트입니다. 슬롯에 대한 모든 설명은 GPT4에서 생성되었습니다.<slots><item><name> 택시로 출발하다</name><description> 사용자가 택시를 타고 싶어하는 시간</description></item><item></valid_intents> ## 작업 ## T로 시작하는 턴으로 구성된 사용자와 에이전트 간의 대화가 제공됩니다. 각 턴에 대해 다음 질문에 답해야 합니다. - 턴을 &lt;=3개 문장으로 요약합니다. 다음을 사용하여 prior_topical_relation 레이블을 출력합니다.<valid_preceding_topical_relation> ...</valid_preceding_topical_relation> list 의도 레이블을 출력합니다.<valid_intents> ...</valid_intents> list 도메인 레이블을 출력합니다.<valid_domains> ...</valid_domains> list - leading_topical_relation이 YES인 경우 세그먼트의 모든 턴에 대해 정확히 동일한 의도 및 도메인 레이블을 사용해야 합니다. ## 출력 형식 ##<T{turn number}><summary> {3개 이하의 문장으로 요약을 작성하세요}</summary><name> {도메인}-{의도} </name><description{description of slot}</description><valid_values> {해당되는 경우 슬롯에 대한 유효한 범주형 값, 그렇지 않으면 이 태그가 나타나지 않음}</valid_values></item> ...</slots> ## 작업 ## T로 시작하는 턴으로 구성된 사용자와 에이전트 간의 대화가 주어집니다. 각 턴마다 다음 질문에 답해야 합니다. 사용자 발화를 그대로 출력합니다. - 해당 발화를 기반으로 관련 슬롯에 대한 사용자 선호도에 대한 관련 정보를 추출합니다.<slots> ...</slots> 그리고 [&#39;{SLOT}-{value}&#39;] 형식을 따르는 태그 목록으로 표현합니다. 여기서 value는 해당 SLOT에 대한 구체적인 정보입니다. - 목록에서 중복 또는 충돌하는 쌍을 제거합니다. 같은 SLOT이 목록에 두 번 이상 나타나는 경우 사용자 발화에서 가장 최근 또는 관련성 있는 값만 유지합니다. - 같은 SLOT에 대한 값이 서로 모순되는 경우 **가장 최근의** 사용자 제공 값을 유지하여 충돌을 해결합니다. 최종 목록을 작업 결과로 출력합니다. [&#39;{SLOT}-{value}&#39;]에 대한 출력 예. 예를 들어, 출력은 [&#39;hotel-book day-monday&#39;, &#39;hotel-book number_of people-3&#39;, &#39;hotel-book number_of_days-4&#39;, &#39;hotel-name-wartworth&#39;, &#39;hotel-parking-yes&#39;, &#39;hotel-area-east&#39;, &#39;hotel-stars-4&#39;, &#39;hotel-internet-yes&#39;, &#39;train-book number_of_people-1&#39;, &#39;train-destination-bishops stortford&#39;, &#39;train-day-friday&#39;, &#39;train-arrive_by_time-19:45&#39;, &#39;train-departure-cambridge&#39;]와 같을 수 있습니다. 선택된 슬롯이 미리 정의된 슬롯에서만 사용되는지 확인하십시오.<slots> ...</slots> 목록. 만약<valid_values> ...</valid_values> 슬롯에 대해 언급된 경우 해당 슬롯에 유효한 값 중 하나를 사용해야 합니다. - 사용자가 명시적으로 언급한 경우에만 dontcare 값을 사용합니다. 이제 **모든 턴**에 대해 다음 질문에 답하세요.<T{turn number}><agent_context> {마지막 에이전트 발언 그대로}</agent_context><user_utterance> {턴의 사용자 발언 그대로}</user_utterance> ) /* ## 선택한 열-값 쌍에 대한 설명: leading_topical_relation-NO: 현재 턴은 이전 대화 맥락과 **전혀** 주제/하위 주제 관계가 없거나 대화의 첫 번째 턴으로, 새 대화 세그먼트의 시작을 표시합니다. - leading_topical_relation-YES: 현재 턴은 이전 대화 맥락과 **일부 또는 일부** 주제/하위 주제 관계가 있습니다. intent-정보 검색: 사용자가 사실 정보나 특정 질문에 대한 답변을 찾고자 합니다. {나머지 의도와 설명은 여기에 있습니다} */ ## 작업 ## 유효한 SQLite를 사용하여 위에 제공된 표에 대한 다음 다중 턴 대화 질문에 답합니다. 다음 단계를 사용합니다. - T로 시작하는 각 사용자 에이전트 턴에 대해 답변 SQL 쿼리를 출력합니다. - leading_topical_relation이 YES인 경우 세그먼트의 모든 턴에 대해 정확히 동일한 의도와 도메인 레이블을 사용해야 합니다. T로 시작하는 차례당 하나의 SQL 쿼리를 포함하여 답변을 목록으로 출력합니다. ## 출력 형식 ## T{차례 번호}. SELECT * from states WHERE leading_topical_relation = {귀하의 답변} AND intent = {귀하의 답변} AND domain = {귀하의 답변}; ## 입력 ##<updated_slot_value> [&#39;{슬롯}-{값}&#39;]<slots> ...</slots> 값&gt;...</valid_values> 슬롯 목록을 업데이트하고 사용 중입니다. <valid_appropriate {input dialogue} ## OUTPUT ## slots </updated_slot_value></T{turn number}>##INPUT## {XML 구조 대화} ##OUTPUT## A.2 IC-DST 프롬프트 아래는 Bing Chat 데이터 세트에 맞게 조정된 IC-DST 프롬프트입니다. DialSeg711 데이터 세트의 경우 도메인 및 의도 열과 지침을 간단히 제거합니다. CREATE TABLE states( domain text CHECK (domain IN ({valid domain names)), prioring_topical_relation text CHECK (preceding_topical_relation IN (YES, NO)), intent text CHECK (intent IN ({valid intent names)), B Annotation Details B.1 annotators에게 제공된 레이블 아래에서는 Bing Chat 데이터세트 annotators에게 제공된 레이블과 설명(있는 경우)을 제공합니다. Intent와 domain의 경우, GPT4에 대화 로그 샘플을 요약하고, 주요 테마를 추출하고, 이러한 테마를 비교하여 다양한 유형의 Intent와 domain 간의 주요 차이점을 식별하도록 요청하는 반복적이고 반자동화된 프로세스를 사용하여 레이블 이름과 Intent 설명을 개발했습니다. IsSegmentBoundary • NO: 현재 턴은 이전 대화 컨텍스트와 구문적, 의미적 또는 주제적 관계가 없거나 대화의 첫 번째 턴입니다. • YES: 현재 턴은 이전 대화 컨텍스트와 구문적, 의미적 또는 주제적 관계가 있습니다. SegmentIntent • 정보 탐색: 사용자는 사실 정보나 특정 질문에 대한 답을 찾고 싶어합니다. • 분석: 사용자는 복잡한 주제나 문제에 대한 분석적 또는 개념적 질문을 합니다. 사용자의 질문에는 어느 정도의 추론, 해석, 논증, 비교 및/또는 데이터 처리가 필요합니다. • 생성: 사용자는 에이전트에게 지정된 기준이나 제약 조건에 따라 원본 콘텐츠를 생성하거나 기존 콘텐츠를 새 콘텐츠로 변환하도록 요청합니다. • 개방형 발견: 사용자는 호기심, 지루함 또는 유머로 인해 에이전트와 캐주얼하게 채팅하거나 놀고 싶어하거나 사용자의 의도가 너무 불분명하거나 구체적으로 지정되지 않아 다른 의도 클래스로 분류하는 것이 불가능합니다. 사용자는 주로 에이전트를 대화나 수다스러운 파트너로 취급하며 다른 의도 카테고리는 할당할 수 없습니다. SegmentDomain • AI 머신 러닝 및 데이터 과학 • 점성술 • 생물학 및 생명 과학 • 비즈니스 및 마케팅 • 경력 및 구직 신청 • 의류 및 패션 • 요리 음식 및 음료 • 공예 • 문화 및 역사 • 사이버 보안 • 데이트 우정 및 관계 ⚫ 디자인 • 교육 • 엔터테인먼트 • 환경 농업 및 에너지 • 가족 양육 및 결혼 • 금융 및 경제 • 게임 • 지리 및 지질학 • 건강 및 의학 • 주택 및 주택 • 유머 및 풍자 • 언어 • 법률 및 정치 • 문학 및 시 • 제조 및 재료 • 수학 논리 및 통계 • 음악 및 오디오 • 뉴스 • 반려동물 및 동물 ⚫ 철학 • 물리학 화학 및 천문학 • 생산성 • 심리학 및 감정 • 종교 및 신화 • 배송 및 배달 • 쇼핑 및 선물 • 잡담 • 소셜 미디어 • 소프트웨어 및 웹 개발 • 스포츠 및 피트니스 • 과세 • 기술 • 시간 및 날짜 • 운송 자동차 및 항공우주 • 여행 • 시각 예술 및 사진 • 날씨 • 글쓰기 저널리즘 및 출판 B.2 도메인 라벨링 절차 도메인 값의 수가 많고 의견 불일치와 인지 과부하가 발생할 가능성이 높기 때문에 주석자에게 턴당 전체 도메인 목록에서 선택하도록 요청하지 않았습니다. 대신, 우리는 턴당 5개의 옵션 드롭다운 목록을 제공했습니다. 한 옵션은 저자가 직접 옳거나 거의 옳다고 선택했습니다. 두 옵션은 Python을 사용하여 무작위로 선택했습니다. 한 옵션은 &quot;OTHER&quot;로, 이 경우 주석자는 49개 도메인의 전체 목록에서 올바른 도메인을 선택하고 선택 사항을 설명해야 했습니다. 마지막으로, 마지막 옵션은 다음 절차를 사용하여 선택한 &quot;hard negative&quot;였습니다. 먼저, 우리는 도메인을 STEM, 예술, 사회 과학, 건강, 상거래, 전문직, 개인 및 여가의 8개 상위 클러스터로 수동으로 그룹화했습니다. 그런 다음 저자가 선택한 앞서 언급한 &quot;ground-truth&quot; 도메인이 주어지면, ground-truth 레이블과 동일한 상위 클러스터에서 다른 도메인을 무작위로 샘플링했습니다. 예를 들어, groundtruth 도메인이 &quot;BIOLOGY AND LIFE SCIENCE&quot;로 선택된 경우, 우리는 STEM 클러스터에서 다른 도메인을 최종 도메인 후보로 샘플링했습니다.
"
"--- ABSTRACT ---
Spoken semantic parsing (SSP) involves generating machinecomprehensible parses from input speech. Training robust models for existing application domains represented in training data or extending to new domains requires corresponding triplets of speechtranscript-semantic parse data, which is expensive to obtain. In this paper, we address this challenge by examining methods that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text. Experiments on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively. Index Terms— spoken language understanding, on-device, unpaired data,large language models, prompting 1.
--- METHOD ---
s that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text.
--- EXPERIMENT ---
s on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively. Index Terms— spoken language understanding, on-device, unpaired data,large language models, prompting 1. INTRODUCTION Spoken Language Understanding (SLU) is essential for many realworld applications today including conversational agents and virtual assistants. Spoken Semantic Parsing (SSP) is the SLU task that involves transforming a recording to a machine-comprehensible parse -end models 2) operate directly on speech while generate a semantic parse based on the transcript. Two-pass deliberation models combine the best of both worlds, by using first-pass transcripts and speech embeddings to improve spoken semantic parsing. However, training such models with supervision requires matched triplets of speech, transcript, and semantic parse. Annotating these triplets is expensive, which limits the size of training data, and consequently model performance. The need for matched data can be alleviated by developing methods that can use only text data. Text data (transcript-semantic parse) is more easily obtained than speech — either from existing textual corpora or by prompting Large Language Models (LLMs), and training models with a small amount of paired speech-text data and a large amount of unpaired text is useful. It is non-trivial to incorporate text-only data into end-to-end models because model outputs *The first author worked on this while at Meta \ Prompt (IWPIEP) > = | Existing Text eee ae a what kind of weather] is it in Paris Paired Transcript with Unpaired Transcript Real Speech with JAT or TTS Speech | | Xu J E2E SLU Model Semantic Parse [IN:GET_WEATHER what kind of weather is it in [SL:LOCATION Paris] ] Fig. 1. This paper: We describe ways to unpaired text to train deliberation models, where unpaired data can be obtained from LLMs or existing textual corpora. We use JAT or TTS to obtain speech representations of unpaired data cannot be obtained without speech inputs. Prior work has explored the use of text data for speech recognition (547). External language models trained on text can be used to interpolate token prediction probabilities (8), but require additional memory, making them unsuitable for on-device applications. Coordinated learning methods project speech and text to a shared embedding space for speech recognition, but such models require significant amounts of paired speech-text data to learn robust mappings. The final class of work generates speech representations for unpaired speech - Joint Audio Text (JAT) uses mean speech embeddings from paired data to represent unpaired text. This is computationally inexpensive, but the speech embeddings do not contain information embedded in real speech. In contrast, synthetic speech from Text-to-speech (TTS) models [5] produce informative speech representations, but they can be expensive to compute. There are two cases where additional textual data may be acquired for semantic parsing — (a) to improve models on existing domains (ED) and (b) to support new domains (ND). In this paper, we compare JAT and TTS for SSP when unpaired text data is drawn from existing and new domains. When unpaired text is not available from existing corpora, we propose prompting Large Language Models (LLMs) to generate text data for SSP. LLMs are exceptional at generating realistic text based on input prompts, and, in this paper, we use LLama to generate text data. For the ED setup, it is sufficient to generate transcripts since semantic parses can be obtained from tran --- --scripts using pre-trained semantic parsers. We describe two prompting methods: (a) intent-word-based prompting (I[WP), where the LLM produces transcripts corresponding to a particular intent class and containing words that co-occur with the intent, and (b) exemplarbased prompting (EP), where it generates transcripts that are similar to provided examples. We generate pseudo-labels for the generated utterances using a pre-trained RoBERTa model and train SSP models using JAT. We find that EP is simpler but [WP generates the desired intent more often. Using data from both methods improves the Exact Match (EM) on STOP data by 1.4 points absolute. For the ND setup, pre-trained models for pseudo-labeling are unavailable for the new domain(s), and hence LLMs are used to generate semantic parses directly. The transcript is then inferred from the semantic parse. Exemplar-based prompting (EP) is used withreal examples for every possible intent-slot combination to generate large-scale data. We find that the generated data improves EM by 2.3 points absolute over a baseline that uses only 3 examples per combination. In summary, this paper makes the following contributions: 1. Extends JAT, previously used for ASR, to end-to-end spoken semantic parsing, and compares JAT with TTS for textual data from existing domains and new domains. 2. Develops prompting strategies to generate textual transcripts and semantic parses in existing and new domains using LLMs. 3. Demonstrates that LLM-generated textual data can be used in conjunction with JAT and TTS to improve spoken semantic parsing. 2. DELIBERATION MODEL FOR SLU Deliberation-based SLU models are two-pass models that predict an ASR transcript in the first pass. Using the first pass transcript and audio, it then generates the semantic parse in the second pass. In contrast to cascade models that utilize separately trained Automatic Speech Recognition (ASR) and SLU components, a deliberation model optimizes both ASR and SLU components jointly. To achieve on-device streaming functionality, the first pass ASR component is implemented using the Recurrent Neural Network Transducer (RNNT) (T7HT9). To maintain transcription accuracy, the ASR component of our deliberation model is trained independently and kept frozen. Our deliberation-based SLU model comprises two primary modules: (1) Fusion, and (2) Decoder. The fusion module combines intermediate audio and text embeddings from the first pass RNNT encoder and predictor respectively. Using Multi-Head the fusion module generates a combined representation that is used by the transformer-based decoder module to predict the target semantic parse sequence. 3. SPEECH REPRESENTATIONS FOR UNPAIRED TEXT 3.1. Joint Audio-Text Training (JAT) Joint Audio-Text training (JAT, is a recent approach for leveraging unpaired text-only data to improve ASR [1 . Unlike shallow fusion that considers token distributions from an external neural network language model (NNLM), JAT does not require additional model parameters or latency, making it suitable for on-device streaming ASR. The core idea behind JAT is that speech representations for unpaired text can be generated by simply using average speech embeddings computed over available paired speech/text data. In this paper, we use the JAT approach to train our Spoken Language Understanding (SLU) models to enable training with both ’speechtext-semantic parse” and “text-semantic parse” datasets. 3.2. Speech Synthesis with Voicebox Voicebox| is a state-of-the-art non-autoregressive speech generation model based on Flow Matching We generate representations for unpaired text by extracting speech features from synthesized speech. Synthetic speech can be obtained by using Voicebox in TTS mode, i.e. where audio is generated by conditioning on input text. Different from 23), the Voicebox model we use represents input text as graphemes rather than phonemes. To generate audio, we first sample unit durations for each grapheme in the input text using a flow-matching-based duration model and then upsample the grapheme sequence using the unit duration information. This information is used as conditioning to generate the spectrogram using the audio model. Finally, we used a HiFi-GAN vocoder to convert the spectrograms into time-domain signals. 4, GENERATING TEXTUAL DATA WITH LLAMA 2.LLama 2.0 s a public open-source large language model trained on large volumes of publicly available data and code with context as large as 4096. In this paper, we use the 13B parameter chat model. 4.1. Generating Textual Data for Existing Domains In the ED setup, we propose to use LLMs to generate transcripts. Corresponding semantic parses are obtained using a pseudo-labeling textual semantic parse model trained on existing paired data. The semantic parse model here takes transcripts as inputs and produces pseudo-label semantic parses as output. Transcripts can be generated using one of two prompting strategies, i.e., intent-word-based or exemplar-based. Intent Word-based prompting (IWP): The goal of IWP is to generate transcripts that may be classified under a certain intent, optionally containing “intent words”. Intent words are the words from semantic parses that occur most frequently with given intents after removing stop-words. An example is shown in Figure|2| The 40 words that cooccur most frequently with every intent in the STOP data are used as intent words. 40 examples are generated for every intent and intentword combination. Though IWP produces good synthetic data, it is limited by the fact that words that co-occur less frequently with the intent are less related to the intent. Such examples produced with less relevant intent words may not be classified under the desired intent class. This also limits the amount of synthetic data that can be generated since the LLM cannot generate many unique examples using a small number of intent-intent word combinations. You are working in an intent-and-slot framework where every utterance can be classified under an intent. Here are some examples of intents and a description of their function: J. IN:ADD_TIME_TIMER - Creates a new timer 2. IN:GET_ESTIMATED_DEPARTURE - gets estimated departure Now, we want to classify intents for the weather application. Given the intents IN:GET_WEATHER, generate 40 utterances that are classified under this intent. You may use the word weather” along with names of people and places to generate 40 utterances. Your response should have numbered utterances, with one utterance on each line. Make sure not to repeat any responses. Start with 1. Fig. 2. Prompt for WP-based utterance generation --- --Generate 60 more sentences that are similar in intent to the following sentences: 1. Is it going to be around 95 in degree Fahrenheit san francisco tomorrow 2. Is it around 72 in degree celsius karachi tonight Write one sentence per line. Generate statements and questions with different sentence structure. Fig. 3. Prompt for EP-based utterance generation Each sentence should be enclosed in square brackets [ ]. The first square bracket [ should be followed by an intent that is in uppercase letters and begins with IN:, for example, IN:GET_WEATHER. Inside the sentence, you should label some nouns with slots, which are also enclosed in brackets [ ]. Slots are in all uppercase letters and begin with SL:, for example, SL:LOCATION. In each sentence, there can only be 1 intent, but there can be many slots. Here are some examples: J. [IN:GET_WEATHER what kind of weather is in [SL:LOCATION paris J] 2. [IN:GET_WEATHER what is the temperature at the [SL:LOCATION north pole ] ] 3. [IN:GET_WEATHER tell me what the weather in [SL: LOCATION central park ] is like ] Please generate more examples with the intent IN;GET_WEATHER and any of the slots SL:LOCATION. The sentences should have an intent/slot format like [IN;GET_WEATHER [SL:LOCATION] ], but with some other text, like the examples above. Write 30 similar sentences and then stop. Use names of people and places in your examples. Fig. 4. Prompt for EP-based generation of seqlogical parses Exemplar-based Prompting (EP): Since LLMs are strong incontext learners , an alternative approach is to prompt LLMs to generate transcripts based on examples. For every intent-slot combination, we provide up to 4 random example transcripts and ask the model to generate 60 more transcripts that are similar but have diverse sentence structures. An example prompt is shown in Fig Though the resulting transcripts may not always correspond to the intent classes from which the examples are drawn, this method enables us to generate larger volumes of data without duplication. Semantic Parse generation and Quality Assessment: Transcripts generated by LLMs are first normalized — written text is converted to spoken form, punctuation except apostrophes are removed and text is transformed into lower case. Semantic parse pseudo-labels are obtained from these normalized transcripts using a strong RoBERTabased semantic parser trained on STOP (EM=86.8). To assess data quality, we compare the intent in the obtained pseudo-labels to the intent in the prompt for IWP or the intent of the provided examples for EP. Intent Match Accuracy (IMA) is defined as the percentage of times the intent of the pseudo-label matches the desired intent of the prompt. 4.2. Generating Transcript-Semantic Parse for New Domains For new domains, paired data and pre-trained models are not available, and therefore, we would need to directly generate pairs of transcript and semantic parse. One way to do this is to generate pairs of semantic parse and corresponding transcript using LLMs directly, however, maintaining consistency across generated parses and transcripts is challenging for current LLMs. Another alternative is to generate only the seqlogical form of the semantic parse from the LLM and infer the transcript from the parse. The seqlogical form of the parse, unlike the decoupled form, comprises all the words in the transcript along with slot and intent tags. Therefore, the transcript can be obtained from the seqlogical parse merely by removing slot and intent tags. Exemplar-based Prompting: We assume that (a) the intents and slots that must be recognized for the new domain are known, (b) the slots that may occur with every intent, i.e., the intent-slot combinations are known, and (c) some manually annotated examples for every intent-slot combination are known. Using this information, LLMs can be prompted as shown in Figure seqlogical parses for a given intent-slot combinations. The prompt first describes the steps to generate a valid seqlogical parse and then presents up to 3 examples of seqlogical parses with the desired intent-slot combinations. Post-processing: The generated seqlogical parses are checked for invalid placement of brackets, and Out of Vocabulary (OOV) intents and slots. OOV intents were fixed by re-prompting the model to replace OOV intents with correct intents and replace any intents other than the first. Any OOV slots are removed while retaining corresponding slot words. 5. EXPERIMENTAL SETUP 5.1. STOP Data, Model and Metrics Data: STOP is a public dataset with real speech for spoken semantic parsing. STOP has data for 8 domains - alarm, event, messaging, music, navigation, reminder, timer, and weather. The data contains 28 unique intents and 82 slot types. Metrics: Exact Match (EM) is used to evaluate all our models. We report EM (No Err) and EM w/ Err, which are the Exact Match accuracies averaged over utterances with no ASR error and averaged over utterances with any ASR error respectively. Model Configuration: For the ASR module, we use RNNT withlayers of conformer in the encoder, | layer of LSTM in the predictor, and 1 linear layer in the joiner. For the deliberation model, we use attention in the Fusion module, 2 transformer encoder layers in the Pooling module, and a transformer decoder layer with a pointerin the Decoder module [16]. Models are optimized with having a peak learning rate of 8e-3. 5.2. Setup: Textual Data from Text Corpora For experiments where we assume textual data is available, we split the STOP datasets into two parts. We perform two experiments — one using the first and second splits as paired and unpaired data respectively and the other using the second and first splits as paired and unpaired data respectively. The average performance across theseexperiments is reported in each case. In the ED setup, equal amounts of data from every domain are present in the two splits. For the ND setup, STOP is split by domain, where one split contains all training data from 4 domains(messaging, reminder, time, and weather), while the other split contains training data from the other 4 domains (alarm, event, music, and navigation). Both splits are designed to ensure that they have a nearly equal number of utterances. 5.3. Setup: Textual Data from LLMs When unpaired data is not available, we use Llama 2.0 to generate examples for the ED and ND setups. For the ED setup, LLama 2.is used to generate utterances. We then use a pre-trained 12-layer RoBERTa model trained on STOP to generate pseudo-labels for the generated utterances. We augment STOP with the generated LLama 2.0 transcript-semantic parse. JAT is used to represent LLama 2 text. --- --For the ND setup, LLama 2.0 generated data is not suitable as a real test set since it does not have matching real speech. Therefore, we choose to partition the existing STOP data into 7 seen domains and | new domain - weather. We use exemplar-based prompting to generate transcript-semantic parse pairs for weather. For this, real examples of transcript-semantic parse from STOP are used. We use TTS to generate equivalent speech representations for the generated data. We compare the performance on the weather domain for models trained on (a) 7 domains of STOP, (b) 7 domains of STOP with examples for the weather (with TTS for examples and real speech for 7 domains), (c) 7 domains of STOP with examples and Llama 2.0 generated data, and (d) the topline that uses 7 domains of STOP with real data and TTS. 6. EXPERIMENTS 6.1. When textual data is available Table |1|compares the performance of different models for the ED and ND settings where unpaired text is drawn from existing domains and new domains respectively. Across both ED and ND setups, we find that the use of unpaired text improves EM scores. For the ED setup, we find that JAT and TTS achieve similar Exact Match scores. Since JAT is comparable in performance to TTS and relatively inexpensive compared to complex TTS models like Voicebox, JAT is optimal for the ED setup. Further, the difference between JAT and TTS appears to be primarily on utterances with ASR errors, since synthetic speech representations can be used to reduce the impact of ASR errors on semantic parsing. For the ND setup, we find that though JAT outperforms the baseline, TTS outperforms JAT. This is because new domains may have different entities and domain-specific terms that may be harder to recognize, and TTS provides valid speech representations that can be used to improve predictions based on the firs the amount of unpaired textual data is increased with co: data, relative gains increase to a point and saturate. Table 1. Comparing JAT and TTS as speech representations for unpaired text from ED and ND. Number of paired and unpaired utterances, and Exact Match (EM) is reported Model #Pair/#Unpair EM EM(No Err) — EM w/ Err Baseline 60.4k/0 64.25 80.51 24.8 w/ JAT 60.4k/60.4k 66.92 83.90 25.w/ TTS 60.4/60.4k 67.05 83.88 25.Baseline 60.7k/0 33.28 41.32 13.8 w/ JAT 60.7k/60.1k 57.74 73.34 19.w/ TTS 60.7k/60.1k 63.95 80.70 22.Topline 120.9k/0 67.67 84.52 26.6.2. LLama 2.0 Generated Data: ED Setup Table[2]compares various prompting strategies for generating utterances in the same domain using Llama 2.0. We find that combining LLama-generated data with existing STOP data can improve performance across test examples with and without ASR errors. On further analysis, we find that significant improvements are observed across domains with relatively poor performance in the STOP baseline. Between IWP and EP, we find that EP is slightly better. Since EP is not constrained to generate utterances that may be classified under a given intent, the Intent Match Accuracy (IMA) is lower than || Z| uwis} 25 50 75Unpaired data as percentage of paired Fig. 5. Impact of increasing unpaired text on EM Table 2. Assessing the impact of augmenting the training data with LLama 2.0 generated utterances and ROBERTa pseudo-labels.EM is Exact Match Accuracy Model #Utts IMA EM EM(NoErr) EM w/ Err STOP Baseline 160k - 67.37 84.52 26.+ IWP-JAT 230k 68.87 68.12 84.96 26.+ EP-JAT 218k 64.24 68.21 85.01 27.+(IWP+EP)-JAT 298k 67.87 68.75 85.82 26.that of WP. Combining the data generated from both these strategies further improves performance over the STOP baseline. 6.3. LLama 2.0 Generated Data: ND Setup Table 3. Using TTS to generate speech for LLama 2.0 text when unpaired text is in an unseen new domain Model #Utts(Weather) Weather EM Overall EM STOP 7 domain 0 0 54.+3 Examples-TTS 360 48.18 61.+ Exemplar LLama2-TTS 2,910 50.82 62.Topline: STOP Weather-TTS 2,910 63.80 66.Table[3]compares the performance of baseline models that have no data for weather or 360 examples for weather with models that use LLama 2.0 generated data. Llama 2 generated text can improve performance by over 2 points absolute EM but lags behind the performance of a topline that uses data from STOP. 7.
--- CONCLUSION ---
We address the high cost of manually labeling speech-transcriptsemantic parse data for spoken semantic parsing by enabling models to use text-only data. JAT is preferred for unpaired text in existing domains for its efficiency and gain of 2.5 % EM over a paired data baseline while remaining within 0.1 % EM of the more computationally expensive TTS. For unpaired text in new domains, TTS outperforms JAT by 6 % absolute EM overall, with a gain of 30.6 % EM over a paired baseline. When text data cannot be obtained from existing text corpora, we propose to prompt LLMs to generate transcriptsemantic parse pairs. We show that using different prompting strategies, we can generate unpaired text data in relatively large volumes. Using JAT and TTS, we can leverage this LLM-generated data to further improve SSP by 1.4 % EM and 2.6 % EM absolute for existing and new domains. --- --8. REFERENCES S. Wang, A. Shrivastava, and S. Livshits, Treepiece: Faster semantic parsing via tree tokenization, 2023. S. Arora, H. Futami, S.-L. Wu, J. Huynh, Y. Peng, Y. Kashiwagi, E. Tsunoo, B. Yan, and S. Watanabe, “A study on the integration of pipeline and e2e slu systems for spoken semantic parsing toward stop quality challenge,” in Proc. ICASSP, 2023, pp. 1-2. H. Futami, J. Huynh, S. Arora, S.-L. Wu, Y. Kashiwagi, Y. Peng, B. Yan, E. Tsunoo, and S. Watanabe, “The pipeline system of asr and nlu with mlm-based data augmentation toward stop low-resource challenge,” in Proc. ICASSP, 2023, pp. 1-2. D. Le, A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli, and M. L. Seltzer, “Deliberation model for on-device spoken language understanding,” Interspeech, 2022. G. Wang, A. Rosenberg, Z. Chen, Y. Zhang, B. Ramabhadran, Y. Wu, and P. Moreno, “Improving speech recognition using consistent predictions on synthesized speech,” in Proc. ICASSP, 2020, pp. 70297033. S. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, T. N. Sainath, and K. Livescu, “A comparison of techniques for language model integration in encoder-decoder speech recognition,” in 2018 IEEE spoken language technology workshop (SLT), 2018, pp. 369-375. T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and J. Le Roux, “Cycle-consistency training for end-to-end speech recognition,” in Proc. ICASSP, 2019, pp. 6271-6275. Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong, “Internal Language Model Adaptation with Text-Only Data for Endto-End Speech Recognition,” in Proc. Interspeech, 2022, pp. 26082612. Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno, A. Bapna, and H. Zen, “MAESTRO: Matched Speech Text Representations through Modality Matching,” in Proc. Interspeech, 2022, pp. 4093-4097. T.N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman, “Joist: A joint speech and text streaming model for asr,” in Proc. SLT, 2023, pp. 52-59. S. Kim, K. Li, L. Kabela, R. Huang, J. Zhu, O. Kalinli, and D. Le, “Joint audio/text training for transformer rescorer of streaming speech recognition,” EMNLP, 2022. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27 744, 2022. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar, et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020. S. Kim, A. Shrivastava, D. Le, J. Lin, O. Kalinli, and M. L. Seltzer, “Modality confidence aware training for robust end-to-end spoken language understanding,” Interspeech, 2023. A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012. S. Kim, Y. Shangguan, J. Mahadeokar, A. Bruguier, C. Fuegen, M. L. Seltzer, and D. Le, “Improved neural language model fusion for streaming recurrent neural network transducer,” in Proc. ICASSP, 2021, pp. 7333-7337. 20)24)C. Liu, F. Zhang, D. Le, S. Kim, Y. Saraf, and G. Zweig, “Improving RNN Transducer Based ASR with Auxiliary Tasks,” in Proc. SLT, 2021. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017. T. N. Sainath, R. Pang, R. J. Weiss, Y. He, C.-c. Chiu, and T. Strohman, “An attention-based joint acoustic and text on-device end-to-end model,” in Proc. ICASSP, 2020, pp. 7039-7043. P. Wang, T. N. Sainath, and R. J. Weiss, “Multitask training with text data for end-to-end speech recognition,” arXiv preprint arXiv:2010.14318, 2020. M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al., “Voicebox: Text-guided multilingual universal speech generation at scale,” arXiv preprint arXiv:2306.15687, 2023. Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” arXiv preprint arXiv:2210.02747, 2022. J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,” Advances in Neural Information Processing Systems, vol. 33, pp. 17 022-17 033, 2020. J. Wei et al., “Emergent abilities of large language models,” Transactions on Machine Learning Research, 2022, Survey Certification. P. Tomasello, A. Shrivastava, D. Lazar, P.-C. Hsu, D. Le, A. Sagar, A. Elkahky, J. Copet, W.-N. Hsu, Y. Adi, et al., “Stop: A dataset for spoken task oriented semantic parsing,” in Proc. SLT, 2023, pp. 991998. D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR, Y. Bengio and Y. LeCun, Eds., 2015.
"	"--- ABSTRACT ---
구어 의미 구문 분석(SSP)은 입력 음성에서 기계가 이해할 수 있는 구문 분석을 생성하는 것을 포함합니다. 훈련 데이터에 표현된 기존 애플리케이션 도메인에 대한 견고한 모델을 훈련하거나 새로운 도메인으로 확장하려면 해당 음성 전사-의미 구문 분석 데이터의 세 쌍이 필요하며, 이를 얻는 데 비용이 많이 듭니다. 이 논문에서는 해당 음성 없이 전사-의미 구문 분석 데이터(페어링되지 않은 텍스트)를 사용할 수 있는 방법을 검토하여 이 과제를 해결합니다. 첫째, 기존 텍스트 코퍼스에서 페어링되지 않은 텍스트를 추출할 때, 조인트 오디오 텍스트(JAT)와 텍스트 음성 변환(TTS)을 페어링되지 않은 텍스트에 대한 음성 표현을 생성하는 방법으로 비교합니다. STOP 데이터 세트에 대한 실험 결과, 기존 및 새 도메인의 페어링되지 않은 텍스트는 절대 정확한 일치(EM)에서 각각 2%와 30%의 성능을 향상시킵니다. 둘째, 기존 텍스트 코퍼스에서 페어링되지 않은 텍스트를 사용할 수 없는 설정을 고려합니다. 기존 및 새 도메인에 대해 페어링되지 않은 텍스트를 생성하기 위해 대규모 언어 모델(LLM)을 촉구할 것을 제안합니다. 실험 결과, 의도와 함께 나타나는 예와 단어는 Llama 2.0으로 짝이 없는 텍스트를 생성하는 데 사용할 수 있습니다. 생성된 텍스트를 JAT 및 TTS와 함께 사용하여 구어 의미 구문 분석을 수행하면 기존 도메인과 새 도메인에서 각각 1.4%와 2.6%의 STOP EM이 절대값으로 향상됩니다. 색인 용어 구어 이해, 온디바이스, 짝이 없는 데이터, 대규모 언어 모델, 프롬프트 1.
--- METHOD ---
해당 음성 없이도 전사 의미 분석 데이터(페어링되지 않은 텍스트)를 사용할 수 있는 s. 첫째, 기존의 텍스트 코퍼스에서 페어링되지 않은 텍스트를 추출할 때, Joint Audio Text(JAT)와 Text-to-Speech(TTS)를 페어링되지 않은 텍스트에 대한 음성 표현을 생성하는 방법으로 비교합니다.
--- EXPERIMENT ---
STOP 데이터 세트의 s는 기존 및 새 도메인의 짝이 없는 텍스트가 절대적 정확한 일치(EM)에서 각각 2% 및 30%만큼 성능을 향상시킨다는 것을 보여줍니다. 둘째, 기존 텍스트 코퍼스에서 짝이 없는 텍스트를 사용할 수 없는 설정을 고려합니다. 기존 및 새 도메인에 대한 짝이 없는 텍스트를 생성하기 위해 대규모 언어 모델(LLM)을 촉구하는 것을 제안합니다. 실험 결과 의도와 함께 나타나는 예와 단어를 사용하여 Llama 2.0으로 짝이 없는 텍스트를 생성할 수 있습니다. 생성된 텍스트를 JAT 및 TTS와 함께 사용하여 구어 의미 구문 분석을 수행하면 기존 및 새 도메인에 대해 각각 STOP에서 EM이 1.4% 및 2.6%만큼 향상됩니다. 색인 용어 구어 이해, 온디바이스, 짝이 없는 데이터, 대규모 언어 모델, 촉구 1. 서론 구어 이해(SLU)는 오늘날 대화 에이전트 및 가상 비서를 포함한 많은 실제 세계 응용 프로그램에 필수적입니다. SPOKEN SEMANTIC PARSE(SSP)는 녹음을 기계가 이해할 수 있는 파스 트리로 변환하는 SLU 작업입니다[1]. 엔드투엔드 모델[2]은 음성에서 직접 작동하는 반면, 캐스케이드 모델[3]은 필사본을 기반으로 의미 파스를 생성합니다. 2단계 심의 모델[4]은 1단계 필사본과 음성 임베딩을 사용하여 음성 의미 파스를 개선함으로써 두 가지의 장점을 결합합니다. 그러나 이러한 모델을 감독을 통해 훈련하려면 음성, 필사본 및 의미 파스의 매칭된 3중 데이터가 필요합니다. 이러한 3중 데이터에 주석을 달기 위해서는 비용이 많이 들기 때문에 훈련 데이터의 크기가 제한되고 결과적으로 모델 성능이 저하됩니다. 매칭된 데이터에 대한 필요성은 텍스트 데이터만 사용할 수 있는 방법을 개발함으로써 완화할 수 있습니다. 텍스트 데이터(전사-의미 구문 분석)는 음성보다 더 쉽게 얻을 수 있습니다.기존 텍스트 코퍼스에서 얻거나 대규모 언어 모델(LLM)을 프롬프트하여 얻을 수 있으며, 적은 양의 쌍을 이룬 음성-텍스트 데이터와 많은 양의 쌍을 이루지 않은 텍스트로 모델을 학습하는 것이 유용합니다.텍스트 전용 데이터를 엔드투엔드 모델에 통합하는 것은 간단하지 않습니다.모델 출력 *첫 번째 저자는 Meta Prompt(IWP/EP) LLM에서 이 작업을 수행했습니다.기존 텍스트 코퍼스 파리의 날씨는 어때요?실제 음성이 포함된 쌍을 이룬 사본 JAT 또는 TTS 음성이 포함된 쌍을 이루지 않은 사본 E2E SLU 모델 의미 구문 분석 [IN:GET_WEATHER [SL:LOCATION Paris]의 날씨는 어때요?] 그림 1. 이 논문: 우리는 쌍을 이루지 않은 텍스트를 사용하여 숙고 모델을 학습하는 방법을 설명합니다.여기서 쌍을 이루지 않은 데이터는 LLM 또는 기존 텍스트 코퍼스에서 얻을 수 있습니다. 우리는 JAT 또는 TTS를 사용하여 음성 입력 없이는 얻을 수 없는 쌍이 없는 데이터의 음성 표현을 얻습니다. 이전 연구에서는 음성 인식을 위해 텍스트 데이터를 사용하는 방법을 살펴보았습니다[5–7]. 텍스트에서 학습된 외부 언어 모델은 토큰 예측 확률을 보간하는 데 사용할 수 있지만[8] 추가 메모리가 필요하므로 장치 내 애플리케이션에는 적합하지 않습니다. 조정된 학습 방법[9, 10]은 음성 인식을 위해 음성과 텍스트를 공유 임베딩 공간에 투사하지만 이러한 모델은 견고한 매핑을 학습하기 위해 상당한 양의 쌍이 있는 음성-텍스트 데이터가 필요합니다. 마지막 작업 유형은 쌍이 없는 음성에 대한 음성 표현을 생성합니다. Joint Audio Text(JAT)[11]는 쌍이 있는 데이터에서 평균 음성 임베딩을 사용하여 쌍이 없는 텍스트를 나타냅니다. 이는 계산 비용이 저렴하지만 음성 임베딩에는 실제 음성에 포함된 정보가 포함되지 않습니다. 반면, 텍스트-음성(TTS) 모델[5]의 합성 음성은 유익한 음성 표현을 생성하지만 계산 비용이 많이 들 수 있습니다. 의미 구문 분석을 위해 추가 텍스트 데이터를 수집할 수 있는 경우는 두 가지가 있습니다. (a) 기존 도메인의 모델을 개선하기 위해(ED) 및 (b) 새로운 도메인을 지원하기 위해(ND)입니다. 이 논문에서는 기존 및 새로운 도메인에서 쌍이 없는 텍스트 데이터를 가져올 때 SSP에 대한 JAT와 TTS를 비교합니다. 기존 코퍼스에서 쌍이 없는 텍스트를 사용할 수 없는 경우 SSP에 대한 텍스트 데이터를 생성하기 위해 대규모 언어 모델(LLM) [12-14]을 프롬프트하는 것을 제안합니다. LLM은 입력 프롬프트를 기반으로 현실적인 텍스트를 생성하는 데 뛰어나며, 이 논문에서는 LLama 2.0 [14]을 사용하여 텍스트 데이터를 생성합니다. ED 설정의 경우 사전 훈련된 의미 구문 분석기를 사용하여 대본에서 의미 구문 분석을 얻을 수 있으므로 대본을 생성하는 것으로 충분합니다. 우리는 두 가지 프롬핑 방법을 설명합니다. (a) 의도-단어 기반 프롬핑(IWP), 여기서 LLM은 특정 의도 클래스에 해당하고 의도와 동시에 발생하는 단어를 포함하는 대본을 생성합니다. (b) 예시 기반 프롬핑(EP), 여기서는 제공된 예와 유사한 대본을 생성합니다. 우리는 사전 훈련된 ROBERTa [15] 모델을 사용하여 생성된 발화에 대한 의사 레이블을 생성하고 JAT를 사용하여 SSP 모델을 훈련합니다. 우리는 EP가 더 간단하지만 IWP가 원하는 의도를 더 자주 생성한다는 것을 발견했습니다. 두 방법의 데이터를 사용하면 STOP 데이터의 정확한 일치(EM)가 절대값으로 1.4포인트 향상됩니다. ND 설정의 경우, 의사 레이블링을 위한 사전 훈련된 모델은 새 도메인에서 사용할 수 없으므로 LLM을 사용하여 의미 구문 분석을 직접 생성합니다. 그런 다음 의미 구문 분석에서 대본을 추론합니다. 예시 기반 프롬핑(EP)은 모든 가능한 의도-슬롯 조합에 대한 실제 예와 함께 사용되어 대규모 데이터를 생성합니다. 우리는 생성된 데이터가 조합당 3개의 예만 사용하는 기준선에 비해 EM을 절대적으로 2.3포인트 향상시킨다는 것을 발견했습니다. 요약하면, 이 논문은 다음과 같은 기여를 합니다. 1. 이전에 ASR에 사용된 JAT를 종단 간 음성 의미 구문 분석으로 확장하고 기존 도메인과 새 도메인의 텍스트 데이터에 대해 JAT를 TTS와 비교합니다. 2. LLM을 사용하여 기존 및 새 도메인에서 텍스트 필사본과 의미 구문 분석을 생성하기 위한 프롬프트 전략을 개발합니다. 3. LLM에서 생성된 텍스트 데이터를 JAT 및 TTS와 함께 사용하여 음성 의미 구문 분석을 향상시킬 수 있음을 보여줍니다. 2. SLU에 대한 심의 모델 심의 기반 SLU 모델[4, 16]은 첫 번째 패스에서 ASR 필사본을 예측하는 2단계 모델입니다. 첫 번째 패스 필사본과 오디오를 사용하여 두 번째 패스에서 의미 구문 분석을 생성합니다. 별도로 훈련된 자동 음성 인식(ASR) 및 SLU 구성 요소를 활용하는 캐스케이드 모델과 달리, 심의 모델은 ASR 및 SLU 구성 요소를 함께 최적화합니다.장치 내 스트리밍 기능을 달성하기 위해 첫 번째 패스 ASR 구성 요소는 RNNT(Recurrent Neural Network Transducer)[17-19]를 사용하여 구현됩니다.전사 정확도를 유지하기 위해 심의 모델의 ASR 구성 요소는 독립적으로 훈련되고 동결 상태로 유지됩니다.심의 기반 SLU 모델은 두 가지 기본 모듈로 구성됩니다.(1) 퓨전 및 (2) 디코더.퓨전 모듈은 첫 번째 패스 RNNT 인코더 및 예측기에서 중간 오디오 및 텍스트 임베딩을 각각 결합합니다.멀티 헤드 어텐션[20]을 사용하여 퓨전 모듈은 변환기 기반 디코더 모듈에서 대상 의미 구문 분석 시퀀스를 예측하는 데 사용되는 결합된 표현을 생성합니다.3. 페어링되지 않은 텍스트에 대한 음성 표현 3.1. 영어: Joint Audio-Text Training (JAT) Joint Audio-Text Training(JAT) [11]은 ASR [10, 11, 21, 22]을 개선하기 위해 쌍을 이루지 않은 텍스트 전용 데이터를 활용하는 최근의 접근 방식입니다. 외부 신경망 언어 모델(NNLM)의 토큰 분포를 고려하는 얕은 퓨전과 달리 JAT는 추가 모델 매개변수나 지연 시간이 필요하지 않아 장치 내 스트리밍 ASR에 적합합니다. JAT의 핵심 아이디어는 사용 가능한 쌍을 이룬 음성/텍스트 데이터에 대해 계산된 평균 음성 임베딩을 사용하여 쌍을 이루지 않은 텍스트의 음성 표현을 생성할 수 있다는 것입니다. 이 논문에서는 JAT 접근 방식을 사용하여 &quot;음성 텍스트 의미 분석&quot; 및 &quot;텍스트 의미 분석&quot; 데이터 세트로 모두 학습할 수 있도록 구어체 언어 이해(SLU) 모델을 학습합니다. 3.2. Voicebox를 사용한 음성 합성 Voicebox[23]는 흐름 매칭[24]을 기반으로 하는 최첨단 비자기회귀 음성 생성 모델입니다. 우리는 합성 음성에서 음성 특징을 추출하여 짝이 없는 텍스트에 대한 표현을 생성합니다. 합성 음성은 TTS 모드에서 Voicebox를 사용하여 얻을 수 있습니다. 즉, 오디오는 입력 텍스트를 조건으로 생성합니다. [23]과 달리 우리가 사용하는 Voicebox 모델은 입력 텍스트를 음소가 아닌 문자소로 나타냅니다. 오디오를 생성하려면 먼저 흐름 매칭 기반 기간 모델을 사용하여 입력 텍스트의 각 문자소에 대한 단위 기간을 샘플링한 다음 단위 기간 정보를 사용하여 문자소 시퀀스를 업샘플링합니다. 이 정보는 오디오 모델을 사용하여 스펙트로그램을 생성하는 조건으로 사용됩니다. 마지막으로 HiFi-GAN [25] 보코더를 사용하여 스펙트로그램을 시간 영역 신호로 변환했습니다. 4. LLAMA 2를 사용한 텍스트 데이터 생성. LLama 2.0 [14]은 최대 4096개의 컨텍스트가 있는 대량의 공개적으로 사용 가능한 데이터와 코드로 학습된 공개 오픈 소스 대규모 언어 모델입니다. 이 논문에서는 13B 매개변수 채팅 모델을 사용합니다. 4.1. 기존 도메인에 대한 텍스트 데이터 생성 ED 설정에서 LLM을 사용하여 필사본을 생성하는 것을 제안합니다. 해당 의미 구문 분석은 기존 쌍 데이터에서 학습된 의사 레이블 텍스트 의미 구문 분석 모델을 사용하여 얻습니다. 여기서 의미 구문 분석 모델은 필사본을 입력으로 받고 의사 레이블 의미 구문 분석을 출력으로 생성합니다. 필사본은 의도 단어 기반 또는 예시 기반이라는 두 가지 프롬프트 전략 중 하나를 사용하여 생성할 수 있습니다. 의도 단어 기반 프롬프트(IWP): IWP의 목표는 특정 의도에 따라 분류될 수 있는 필사본을 생성하는 것이며, 선택적으로 &quot;의도 단어&quot;를 포함합니다. 의도 단어는 중지 단어를 제거한 후 주어진 의도와 함께 가장 자주 발생하는 의미 구문 분석의 단어입니다. 그림 2에 예가 나와 있습니다. STOP 데이터에서 모든 의도와 함께 가장 자주 발생하는 40개 단어가 의도 단어로 사용됩니다. 모든 의도 및 의도 단어 조합에 대해 40개 예가 생성됩니다. IWP는 우수한 합성 데이터를 생성하지만 의도와 함께 자주 발생하지 않는 단어는 의도와 관련이 적다는 사실에 의해 제한됩니다. 덜 관련성 있는 의도 단어로 생성된 이러한 예는 원하는 의도 클래스에 따라 분류되지 않을 수 있습니다. 또한 LLM이 소수의 의도-의도 단어 조합을 사용하여 많은 고유한 예를 생성할 수 없기 때문에 생성할 수 있는 합성 데이터의 양이 제한됩니다. 모든 발화를 의도에 따라 분류할 수 있는 의도-슬롯 프레임워크에서 작업하고 있습니다. 다음은 의도의 몇 가지 예와 해당 기능에 대한 설명입니다. 1. IN:ADD_TIME_TIMER - 새 타이머를 만듭니다. 2. IN:GET_ESTIMATED DEPARTURE - 예상 출발 시간을 가져옵니다. 이제 날씨 애플리케이션에 대한 의도를 분류하려고 합니다. IN:GET WEATHER 의도가 주어지면 이 의도에 따라 분류되는 40개의 발화를 생성합니다. 사람과 장소의 이름과 함께 &quot;날씨&quot;라는 단어를 사용하여 40개의 발화를 생성할 수 있습니다. 응답에는 각 줄에 하나의 발화가 있는 번호가 매겨진 발화가 있어야 합니다. 반복되는 응답이 없도록 주의하세요.1부터 시작하세요.그림 2. IWP 기반 발화 생성을 위한 프롬프트 다음 문장과 의도가 비슷한 문장을 60개 더 생성하세요.1. 내일 샌프란시스코 화씨 95도 정도 될까요?2. 오늘 밤 카라치 섭씨 72도 정도 될까요?한 줄에 문장 하나씩 쓰세요.문장 구조가 다른 진술문과 질문을 생성하세요.그림 3. EP 기반 발화 생성을 위한 프롬프트 각 문장은 대괄호 [ ]로 묶어야 합니다.첫 번째 대괄호 [ 뒤에는 대문자로 쓰고 IN:으로 시작하는 인텐트가 와야 합니다(예: IN: GET_WEATHER).문장 안에서 명사에 슬롯을 표시해야 하며, 슬롯도 대괄호 [ 로 묶어야 합니다.슬롯은 모두 대문자로 쓰고 SL:로 시작합니다(예: SL:LOCATION).각 문장에는 인텐트가 하나만 있을 수 있지만 슬롯은 여러 개 있을 수 있습니다. 다음은 몇 가지 예입니다. 1. [IN:GET WEATHER [SL:LOCATION 파리]의 날씨는 어때요?] 2. [IN:GET WEATHER [SL:LOCATION 북극]의 기온은 얼마인가요?] 3. [IN:GET WEATHER [SL:LOCATION 센트럴 파크]의 날씨는 어때요?] IN: GET WEATHER와 SL:LOCATION 슬롯 중 하나를 사용하여 더 많은 예를 생성해 주세요. 문장은 [IN:GET_WEATHER [SL:LOCATION] ]와 같은 의도/슬롯 형식을 가져야 하지만 위의 예와 같이 다른 텍스트가 있어야 합니다. 비슷한 문장 30개를 쓴 다음 중지합니다. 예에서 사람과 장소의 이름을 사용하세요. 그림 4. seqlogical 구문 분석의 EP 기반 생성을 위한 프롬프트 예시 기반 프롬프트(EP): LLM은 강력한 문맥 내 학습자[26]이므로 대안적인 접근 방식은 LLM이 예를 기반으로 필사본을 생성하도록 하는 것입니다. 모든 의도-슬롯 조합에 대해 최대 4개의 무작위 예시 필사본을 제공하고 모델에 유사하지만 다양한 문장 구조를 가진 60개의 추가 필사본을 생성하도록 요청합니다.그림 3에 예시 프롬프트가 나와 있습니다.결과적인 필사본이 항상 예제가 추출된 의도 클래스와 일치하지는 않지만 이 방법을 사용하면 중복 없이 더 많은 양의 데이터를 생성할 수 있습니다.의미 구문 분석 생성 및 품질 평가: LLM에서 생성된 필사본은 먼저 정규화됩니다.기록된 텍스트는 음성 형태로 변환되고, 아포스트로피를 제외한 구두점은 제거되고 텍스트는 소문자로 변환됩니다.의미 구문 분석 의사 레이블은 STOP(EM=86.8)에서 학습된 강력한 ROBERTa 기반 의미 구문 분석기를 사용하여 이러한 정규화된 필사본에서 얻습니다.데이터 품질을 평가하기 위해 얻은 의사 레이블의 의도를 IWP의 프롬프트의 의도 또는 EP의 제공된 예제의 의도와 비교합니다.의도 일치 정확도(IMA)는 의사 레이블의 의도가 프롬프트의 원하는 의도와 일치하는 횟수의 백분율로 정의됩니다. 4.2. 새로운 도메인에 대한 전사-의미적 구문 분석 생성 새로운 도메인의 경우, 쌍을 이룬 데이터와 사전 학습된 모델을 사용할 수 없으므로, 전사와 의미적 구문 분석의 쌍을 직접 생성해야 합니다. 이를 수행하는 한 가지 방법은 LLM을 직접 사용하여 의미적 구문 분석과 해당 전사의 쌍을 생성하는 것입니다. 그러나 생성된 구문 분석과 전사의 일관성을 유지하는 것은 현재 LLM의 경우 어렵습니다. 또 다른 대안은 LLM에서 의미적 구문 분석의 seqlogical 형식만 생성하고 구문 분석에서 전사를 추론하는 것입니다. 분리된 형식과 달리 구문 분석의 seqlogical 형식은 슬롯 및 의도 태그와 함께 전사의 모든 단어로 구성됩니다. 따라서 슬롯 및 의도 태그를 제거하면 seqlogical 구문 분석에서 전사를 얻을 수 있습니다. 예시 기반 프롬프팅: (a) 새 도메인에 대해 인식해야 하는 인텐트와 슬롯이 알려져 있고, (b) 모든 인텐트와 함께 발생할 수 있는 슬롯, 즉 인텐트-슬롯 조합이 알려져 있으며, (c) 모든 인텐트-슬롯 조합에 대해 수동으로 주석이 달린 몇 가지 예시가 알려져 있다고 가정합니다. 이 정보를 사용하여 LLM은 그림 4에 표시된 대로 프롬프팅되어 주어진 인텐트-슬롯 조합에 대한 새로운 seqlogical 구문 분석을 생성할 수 있습니다. 프롬프팅은 먼저 유효한 seqlogical 구문 분석을 생성하는 단계를 설명한 다음 원하는 인텐트-슬롯 조합을 가진 seqlogical 구문 분석의 최대 3개 예시를 제시합니다. 사후 처리: 생성된 seqlogical 구문 분석은 괄호의 잘못된 배치, 어휘 범위 밖(OOV) 인텐트 및 슬롯이 있는지 확인합니다. OOV 인텐트는 OOV 인텐트를 올바른 인텐트로 바꾸고 첫 번째 인텐트 이외의 인텐트를 바꾸도록 모델을 다시 프롬프팅하여 수정했습니다. 모든 OOV 슬롯은 해당 슬롯 단어를 유지하면서 제거됩니다. 5. 실험 설정 5.1. STOP 데이터, 모델 및 메트릭 데이터: STOP[27]은 구어체 의미 분석을 위한 실제 음성이 포함된 공개 데이터 세트입니다. STOP에는 알람, 이벤트, 메시징, 음악, 내비게이션, 알림, 타이머 및 날씨의 8개 도메인에 대한 데이터가 있습니다. 이 데이터에는 28개의 고유한 의도와 82개의 슬롯 유형이 포함되어 있습니다. 메트릭: 모든 모델을 평가하는 데 정확한 일치(EM)가 사용됩니다. ASR 오류가 없는 발화에 대한 정확한 일치 정확도인 EM(No Err) 및 ASR 오류가 있는 발화에 대한 정확한 일치 정확도인 EM w/ Err을 보고합니다. 모델 구성: ASR 모듈의 경우 인코더에 컨포머 레이어, 예측기에 LSTM 레이어 1개, 조이너에 선형 레이어 1개가 있는 RNNT를 사용합니다. 심의 모델의 경우 Fusion 모듈에서 어텐션을 사용하고, Pooling 모듈에서 2개의 트랜스포머 인코더 계층을 사용하고, Decoder 모듈에서 포인터 생성기가 있는 트랜스포머 디코더 계층을 사용합니다[16]. 모델은 최대 학습 속도가 8e-3인 Adam[28]으로 최적화되었습니다. 5.2. 설정: 텍스트 코퍼스의 텍스트 데이터 텍스트 데이터를 사용할 수 있다고 가정하는 실험의 경우 STOP 데이터 세트를 두 부분으로 나눕니다. 두 가지 실험을 수행합니다. 하나는 첫 번째와 두 번째 분할을 각각 페어링된 데이터와 페어링되지 않은 데이터로 사용하고, 다른 하나는 두 번째와 첫 번째 분할을 각각 페어링된 데이터와 페어링되지 않은 데이터로 사용합니다. 이러한 실험의 평균 성능은 각 경우에 보고됩니다. ED 설정에서 모든 도메인의 동일한 양의 데이터가 두 분할에 존재합니다. ND 설정의 경우 STOP은 도메인별로 분할되는데, 한 분할에는 4개 도메인(메시징, 알림, 시간 및 날씨)의 모든 교육 데이터가 포함되고, 다른 분할에는 다른 4개 도메인(알람, 이벤트, 음악 및 내비게이션)의 교육 데이터가 포함됩니다. 두 분할 모두 거의 같은 수의 발화가 있도록 설계되었습니다.5.3. 설정: LLM의 텍스트 데이터 짝이 맞지 않는 데이터를 사용할 수 없는 경우 Llama 2.0을 사용하여 ED 및 ND 설정에 대한 예제를 생성합니다.ED 설정의 경우 LLama 2를 사용하여 발화를 생성합니다.그런 다음 STOP에서 학습된 사전 학습된 12계층 ROBERTA 모델을 사용하여 생성된 발화에 대한 가상 레이블을 생성합니다.생성된 LLama 2.0 전사-의미 구문 분석으로 STOP을 보강합니다.JAT는 LLama 2 텍스트를 나타내는 데 사용됩니다.ND 설정의 경우 LLama 2.0에서 생성된 데이터는 일치하는 실제 음성이 없으므로 실제 테스트 세트로 적합하지 않습니다.따라서 기존 STOP 데이터를 7개의 표시된 도메인과 1개의 새 도메인(날씨)으로 분할하기로 했습니다.예시 기반 프롬프트를 사용하여 날씨에 대한 전사-의미 구문 분석 쌍을 생성합니다. 이를 위해 STOP의 실제 전사-의미 구문 분석 예를 사용합니다. TTS를 사용하여 생성된 데이터에 대한 동등한 음성 표현을 생성합니다. (a) STOP의 7개 도메인, (b) 날씨에 대한 예가 있는 STOP의 7개 도메인(예시는 TTS, 7개 도메인은 실제 음성), (c) 예와 Llama 2.0에서 생성된 데이터가 있는 STOP의 7개 도메인, (d) 실제 데이터와 TTS가 있는 STOP의 7개 도메인을 사용하는 최상위 라인에서 훈련된 모델의 날씨 도메인에서의 성능을 비교합니다. 6. 실험 6.1. 텍스트 데이터가 있는 경우 표 1은 기존 도메인과 새 도메인에서 각각 페어링되지 않은 텍스트를 가져온 ED 및 ND 설정에 대한 다양한 모델의 성능을 비교합니다. ED 및 ND 설정에서 페어링되지 않은 텍스트를 사용하면 EM 점수가 향상되는 것을 발견했습니다. ED 설정의 경우 JAT와 TTS가 유사한 정확한 일치 점수를 달성하는 것을 발견했습니다. JAT는 TTS와 성능이 비슷하고 Voicebox와 같은 복잡한 TTS 모델에 비해 비교적 저렴하기 때문에 JAT는 ED 설정에 최적입니다. 또한 JAT와 TTS의 차이는 주로 ASR 오류가 있는 발화에서 나타나는 것으로 보이는데, 합성 음성 표현을 사용하여 ASR 오류가 의미 구문 분석에 미치는 영향을 줄일 수 있기 때문입니다. ND 설정의 경우 JAT가 기준선보다 성능이 뛰어나지만 TTS가 JAT보다 성능이 뛰어납니다. 이는 새로운 도메인에는 인식하기 어려울 수 있는 다른 엔터티와 도메인별 용어가 있을 수 있으며, TTS는 첫 번째 통과 ASR을 기반으로 예측을 개선하는 데 사용할 수 있는 유효한 음성 표현을 제공하기 때문입니다. 그림 5는 쌍을 이루지 않은 텍스트 데이터의 양이 일정한 쌍을 이룬 데이터로 증가하고 상대적인 이득이 어느 정도 증가하고 포화됨을 보여줍니다. 표 1. ED 및 ND의 쌍을 이루지 않은 텍스트에 대한 음성 표현으로서 JAT와 TTS 비교. 쌍을 이룬 발화와 쌍을 이루지 않은 발화의 수, 정확한 일치(EM)가 보고됩니다 모델 기준선 #Pair/#Unpair EM EM(Err 없음) EM w/ Err 60.4k /64.80.24.w/ JAT w/ TTS 60.4k / 60.4k 66.83.25.60.4 / 60.4k 67.83.25.W 기준선 w/ JAT w/ TTS 최상위 60.7k /33.41.13.60.7k / 60.1k 57.73.19.60.7k / 60.1k 63.80.22.120.9k / 0 67.84.26.6.2. LLama 2.0 생성 데이터: ED 설정 표 2는 Llama 2.0을 사용하여 동일한 도메인에서 발화를 생성하기 위한 다양한 프롬프팅 전략을 비교합니다. LLama에서 생성한 데이터를 기존 STOP 데이터와 결합하면 ASR 오류가 있는 테스트 예제와 없는 테스트 예제에서 성능이 향상될 수 있음을 발견했습니다. 추가 분석에서 STOP 기준선에서 상대적으로 성능이 좋지 않은 도메인에서 상당한 개선이 관찰되었음을 발견했습니다. IWP와 EP 사이에서 EP가 약간 더 나은 것을 발견했습니다. EP는 주어진 의도에 따라 분류될 수 있는 발화를 생성하도록 제약받지 않으므로 의도 일치 정확도(IMA)는 EM보다 낮습니다(%)페어링되지 않은 데이터의 백분율로 나타낸 페어링되지 않은 데이터 그림 5. EM에 대한 페어링되지 않은 텍스트 증가의 영향 표 2. LLama 2.0에서 생성된 발화와 ROBERTa 가상 레이블로 교육 데이터를 증강한 영향 평가.EM은 IWP의 정확한 일치 정확도 모델입니다.STOP 기준선 + IWP-JAT #Utts IMA 160k EM 67.EM(Err 없음) EM w/ Err 84.26.+ EP-JAT 230k 68.87 68.218k 64.24 68.84.26.85.27.85.26.+ (IWP+EP)-JAT 298k 67.87 68. 두 전략에서 생성된 데이터를 결합하면 STOP 기준선에 비해 성능이 더욱 향상됩니다.6.3. LLama 2.0 생성 데이터: ND 설정 표 3. 짝이 맞지 않는 텍스트가 보이지 않는 새 도메인에 있는 경우 TTS를 사용하여 LLama 2.0 텍스트에 대한 음성 생성 모델 STOP 7 도메인 + 3개 예제-TTS + 모범 LLama2-TTS 최상위: STOP 날씨-TTS #Utts(날씨)날씨 EM전체 EM 54.48.61.2,50.62.2,63.66.표 3은 날씨에 대한 데이터가 없거나 날씨에 대한 360개 예제가 있는 기준선 모델과 LLama 2.0에서 생성된 데이터를 사용하는 모델의 성능을 비교합니다.Llama 2에서 생성된 텍스트는 절대 EM에서 2포인트 이상 성능을 향상시킬 수 있지만 STOP의 데이터를 사용하는 최상위 성능보다 뒤처집니다.7.
--- CONCLUSION ---
우리는 모델이 텍스트 전용 데이터를 사용할 수 있도록 함으로써 구어 의미 분석을 위해 음성-전사 의미 분석 데이터를 수동으로 레이블링하는 데 드는 높은 비용을 해결합니다. JAT는 기존 도메인의 쌍이 없는 텍스트에 대해 효율성과 쌍이 있는 데이터 기준선에 비해 2.5% EM의 이득을 제공하는 반면, 더 많은 계산 비용이 드는 TTS의 0.1% EM 내에 있기 때문에 선호됩니다. 새로운 도메인의 쌍이 없는 텍스트의 경우 TTS는 전체적으로 JAT보다 절대 EM이 6% 더 높고, 쌍이 있는 기준선에 비해 30.6% EM의 이득을 제공합니다. 기존 텍스트 코퍼스에서 텍스트 데이터를 얻을 수 없는 경우 LLM에 전사 의미 분석 쌍을 생성하도록 촉구하는 것을 제안합니다. 우리는 다른 촉구 전략을 사용하여 비교적 많은 양의 쌍이 없는 텍스트 데이터를 생성할 수 있음을 보여줍니다. JAT와 TTS를 사용하면 이 LLM 생성 데이터를 활용하여 기존 및 새로운 도메인에 대해 SSP를 1.4% EM 및 2.6% EM 절대값으로 더욱 개선할 수 있습니다. [21] 8. 참고문헌 [19] [1] S. Wang, A. Shrivastava 및 S. Livshits, Treepiece: 트리 토큰화를 통한 더 빠른 의미 구문 분석, 2023. [20] [2] S. Arora, H. Futami, S.-L. Wu, J. Huynh, Y. Peng, Y. Kashiwagi, E. Tsunoo, B. Yan, and S. Watanabe, &quot;중지 품질 도전을 위한 음성 의미 구문 분석을 위한 파이프라인 및 e2e slu 시스템 통합에 대한 연구&quot;, Proc. ICASSP, 2023, pp. 1–2. [3] H. Futami, J. Huynh, S. Arora, S.-L. Wu, Y. Kashiwagi, Y. Peng, B. Yan, E. Tsunoo, and S. Watanabe, &quot;중지 저리소스 도전을 위한 mlm 기반 데이터 증강을 갖춘 asr 및 nlu의 파이프라인 시스템&quot;, Proc. ICASSP, 2023, pp. 1–2. [4] D. Le, A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli, ML Seltzer, &quot;디바이스 음성 언어 이해를 위한 심의 모델&quot;, Interspeech, 2022. [23] [5] G. Wang, A. Rosenberg, Z. Chen, Y. Zhang, B. Ramabhadran, Y. Wu, P. Moreno, &quot;합성 음성에 대한 일관된 예측을 사용하여 음성 인식 개선&quot;, Proc. ICASSP, 2020, 70297033쪽. [22] [24] [6] S. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, TN Sainath, K. Livescu, &quot;인코더-디코더 음성 인식에서 언어 모델 통합 기술 비교&quot;, 2018 IEEE 음성 언어 기술 워크숍(SLT), 2018, pp. 369–375. [25] [7] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and J. Le Roux, &quot;종단간 음성 인식을 위한 사이클 일관성 훈련,&quot; Proc. ICASSP, 2019, pp. 6271-6275. [26] [8] [9] [10] [11] [12] [13] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong, &quot;종단간 음성 인식을 위한 텍스트 전용 데이터를 사용한 내부 언어 모델 적응,&quot; Proc. Interspeech, 2022, pp. 26082612. Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, PJ Moreno, A. Bapna, and H. Zen, &quot;MAESTRO: Modality Matching을 통한 Matched Speech Text Representations&quot;, Proc. Interspeech, 2022, pp. 4093-4097. TN Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman, &quot;Joist: ASR을 위한 공동 음성 및 텍스트 스트리밍 모델&quot;, Proc. SLT, 2023, pp. 52-59. S. Kim, K. Li, L. Kabela, R. Huang, J. Zhu, O. Kalinli, D. Le, &quot;스트리밍 음성 인식의 트랜스포머 리스코어를 위한 공동 오디오/텍스트 훈련&quot;, EMNLP, 2022. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., &quot;인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27744, 2022. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., 영어: &quot;Llama: 개방적이고 효율적인 기초 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [14] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 외, &quot;Llama 2: 개방형 기초 및 미세 조정된 채팅 모델,&quot; arXiv 사전 인쇄본 arXiv:2307.09288, 2023. [27] [28] C. Liu, F. Zhang, D. Le, S. Kim, Y. Saraf, G. Zweig, &quot;보조 작업을 통한 RNN 변환기 기반 ASR 개선,&quot; Proc. SLT, 2021. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, L. Kaiser, I. Polosukhin, &quot;주의만 있으면 됩니다.&quot; 2017. TN Sainath, R. Pang, RJ Weiss, Y. He, C.-c. Chiu, T. Strohman, &quot;주의 기반 조인트 음향 및 텍스트 온디바이스 엔드투엔드 모델&quot; Proc. ICASSP, 2020, 7039-7043쪽. P. Wang, TN Sainath, RJ Weiss, &quot;엔드투엔드 음성 인식을 위한 텍스트 데이터를 사용한 멀티태스크 학습&quot; arXiv 사전 인쇄본 arXiv:2010.14318, 2020. M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al., &quot;Voicebox: 텍스트 기반 다국어 범용 음성 생성,&quot; arXiv 사전 인쇄본 arXiv:2306.15687, 2023. Y. Lipman, RT Chen, H. Ben-Hamu, M. Nickel, and M. Le, &quot;생성적 모델링을 위한 흐름 매칭,&quot; arXiv 사전 인쇄본 arXiv:2210.02747, 2022. J. Kong, J. Kim, and J. Bae, &quot;Hifi-gan: 효율적이고 고충실도 음성 합성을 위한 생성적 적대적 네트워크,&quot; Advances in Neural Information Processing Systems, vol. 33, pp. 17022-17033, 2020. J. Wei 외, &quot;대규모 언어 모델의 새로운 능력&quot;, Transactions on Machine Learning Research, 2022, Survey Certification. P. Tomasello, A. Shrivastava, D. Lazar, P.-C. Hsu, D. Le, A. Sagar, A. Elkahky, J. Copet, W.-N. Hsu, Y. Adi 외, &quot;중단: 구어체 작업 지향 의미 구문 분석을 위한 데이터 세트&quot;, Proc. SLT, 2023, pp. 991998. DP Kingma 및 J. Ba, &quot;Adam: 확률적 최적화를 위한 방법,&quot; Proc. ICLR, Y. Bengio 및 Y. LeCun, 편집, 2015. [15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer 및 V. Stoyanov, Ro{bert}a: 견고하게 최적화된 {bert} 사전 학습 접근 방식, 2020. [16] [17] [18] S. Kim, A. Shrivastava, D. Le, J. Lin, O. Kalinli 및 ML Seltzer, &quot;견고한 엔드투엔드 구어 이해를 위한 모달리티 신뢰도 인식 학습,&quot; Interspeech, 2023. A. Graves, &quot;순환 신경망을 사용한 시퀀스 변환 네트워크,&quot; arXiv 사전 인쇄본 arXiv:1211.3711, 2012. S. Kim, Y. Shangguan, J. Mahadeokar, A. Bruguier, C. Fuegen, ML Seltzer, D. Le, &quot;스트리밍 순환 신경망 변환기를 위한 개선된 신경 언어 모델 융합&quot;, Proc. ICASSP, 2021, pp. 7333-7337.
"
"--- ABSTRACT ---
The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trilion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticuous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is ully released to the public in HuggingFace to acilitate research and advancements in multilingual LLMs: https: //huggingface.co/ datasets/uonlp/Culturax. 1
--- INTRODUCTION ---
Large language models (LLMs) have fundamentally transformed research and applications of natural language processing (NLP), significantly ad vancing the state-of-the-art performance for numerous tasks and revealing new emergent abilities (Brown et al., 2020; Wei et al., 2022). Based on the transformer architecture (Vaswani et al., 2017), three major variants of LLMs have been explored in the literature: the encoder-only models to encode input texts into representation vectors, e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019); the decoder-only models to generate texts, e.g., GPT (Radford et al., 2019; Brown et al., 2020); and the encoder-decoder models to perform sequence-to-sequence generation, e.g., BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). The remarkable capabilities of LLMs have primarily been propelled by the ever-expanding scale of model sizes and training datasets, which have been deemed essential for achieving optimal performance by the scaling laws (Hernandez et al., 2022). For instance, beginning with the BERT model, which had a mere few hundred million parameters (Devlin et al., 2019), recent GPT-based models have been expanded to encompass hundreds of billions of parameters (Shoeybi et al., 2019; Scao et al., 2022; Lieber et al., 2021; Chowdhery et al., 2022). Similarly, the training datasets for LLMs have grown exponentially, evolving from a modest 13GB of text data from Wikipedia and books used for BERT (Devlin et al., 2019; Liu et al., 2019) to consume terabytes of data for the latest models, such as Falcon (Penedo et al., 2023), MPT (MosaicML, 2023), LLaMa (Touvron et al., 2023), PolyLM (Wei et al., 2023) and ChatGPT!. As the field keeps progressing rapidly, pretrained LLMs have typically been released to the public to foster further research and advancements. These models are obtainable either through commercial APIs, as illustrated by ChatGPT and GPT4, or via open-source initiatives, exemplified by Falcon and LLaMa. Nevertheless, in contrast to the public accessibility of LLMs, the training https: //openai.com/blog/chatgpt --- --datasets that underpin the state-of-the-art models have mostly remained closely guarded secrets, even in the case of open-source LLMs such as BLOOM, LLaMa, MPT, and Falcon. For example, Falcon (Penedo et al., 2023) and BLOOM (Scao et al., 2022) only provide a glimpse of their complete training data, whereas MPT’s, LLaMa’s and PolyLM’s datasets (Touvron et al., 2023; Wei et al., 2023) remain inaccessible to the public. On one hand, the lack of transparency has impeded indepth analysis and comprehension of LLMs, hindering crucial research into attributing and addressing fundamental issues stemming from the training data, such as hallucinations, biases, and toxic content (Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021). On the other hand, concealing the training data restricts the development of LLMs to a select few stakeholders with ample resources, thereby constraining the democratization and benefits of the technology and exacerbating its biases within broader society. To attain transparency and democratization for LLMs, it is thus crucial to create large-scale and high-quality datasets for training high-performing LLMs while ensuring their public accessibility to foster deeper research and advancements. In the realm of LLMs, high-quality training datasets are often crafted through the application of extensive data cleaning and deduplication processes, aimed at eliminating noisy and redundant content from vast text collections (Allamanis, 2018; Penedo et al., 2023). To this end, there have been recent efforts from the community to develop such open-source datasets for LLMs, such as RedPajama with 1.21T tokens (Computer, 2023), SlimPajama” with 627B tokens, and AI2 Dolma? with 3T tokens. However, most of the existing open-source datasets for LLMs are tailored for the English language, which hinders the utilization and performance of the resulting LLMs when applied to non-English languages, particularly those with limited linguistic resources (Bang et al., 2023; Lai et al., 2023). This emphasis on English also restricts the capacity of open-source datasets to comprehensively tackle the research challenges and democratization concerns of LLMs across the diverse spectrum of over 7,languages spoken worldwide. https: //www.cerebras.net/blog/slimpajama-a-27b- token-cleaned-and-deduplicated-version-of-r edpajama 3https://blog.allenai.org/dolma-3-trillion-to kens-open-11m-corpus-9aff4b8daSimultaneously, some multilingual datasets have been developed and made available, providing text data for multiple languages. Nevertheless, their quality and scale fall short of meeting the requirements for training high-performing LLMs. Specifically, the multilingual text dataset sourced from Wikipedia, while of high quality, is regarded as relatively small when it comes to training LLMs (Conneau et al., 2020). The OSCAR datasets (Ortiz Suarez et al., 2019; Ortiz Suarez et al., 2020; Abadji et al., 2021, 2022)* extract text data from CommonCrawl (CC) for more than 160 languages. However, these datasets lack document-level deduplication (i.e., removing similar documents in the dataset), leading to the inclusion of redundant information and impairing the performance of generative LLMs (Lee et al., 2022). Similarly, the mC(Xue et al., 2021), CCAligned (Conneau et al., 2020), WikiMatrix (Schwenk et al., 2021), and ParaCrawl (Bajfién et al., 2020) datasets altogether support over 100 languages but suffers from less accurate language identification, introducing noise into the data (Kreutzer et al., 2022). These datasets are also not deduplicated at fuzzy and document levels, e.g., via MinHash (Broder, 1997). Additionally, the CC100 dataset (Wenzek et al., 2020; Conneau et al., 2020), employed in training the multilingual XLM-RoBERTa model across 100 languages, only considers the snapshots of CC in 2018, constraining its size and the availability of up-todate information to train high-performing LLMs. To address the aforementioned issues for opensource datasets, our work introduces a novel multilingual dataset, called CulturaX, for training LLMs in 167 languages. CulturaX merges the latest iteration of mC4 (version 3.1.0) with all available OSCAR corpora up to the current year, encompassing distributions 20.19, 21.09, 22.01, and 23.01. This amalgamation results in a large multilingual dataset, comprising 27 TB of text data with 6.trillion tokens and offering the most up-to-date data for LLM development. More than half of our dataset is dedicated to non-English languages to significantly boost the data size and enhance the feasibility of training models in multilingual scenarios. Importantly, CulturaX is extensively cleaned and deduplicated at the document level to produce the highest quality to train LLMs for multiple languages. In particular, our data cleaning process includes a comprehensive pipeline de *https: //oscar-project.org --- --signed to eliminate low-quality data. This involves removing noisy text, non-linguistic content, toxic data, incorrect language identification, and more. Our data cleaning pipeline employs a variant of the Interquartile Range (IQR) method (Dekking et al., 2007) to select appropriate thresholds for various dataset metrics (e.g., stopword ratios, data perplexity, and language identification scores), which can be used to filter noisy outliers for the dataset. As such, we leverage the percentiles of the distributions computed over large samples of data to effectively guide the threshold selection process for each filtering metric and language. Finally, we perform extensive deduplication for the data of the languages within our datasets based on the near deduplication method MinHashLSH (Broder, 1997; Leskovec et al., 2020) and URLs, leading to high-quality data to train multilingual LLMs. Our dataset will be fully available to the public to promote further research and development for multilingual learning. To our knowledge, CulturaX is the largest open-source multilingual dataset to date that is deeply cleaned and deduplicated for LLM and NLP applications. 2 Multilingual Dataset Creation To develop a multilingual public dataset for LLMs, our strategy is to combine mC4 (Xue et al., 2021) and OSCAR (Ortiz Suarez et al., 2019; Abadji et al., 2021, 2022), two largest multilingual datasets at our disposal. We then process the data with an extensive pipeline, involving two major steps of cleaning and deduplication, to produce an enormous and high-quality dataset for multilingual LLMs. mC4 is a multilingual document-level dataset, originally created to train the multilingual encoderdecoder model mT5 (Xue et al., 2021) for 101 languages. This dataset is extracted from 71 monthly snapshots from CC by removing pages with less than three long lines (line length filter), pages with bad words, and duplicated lines across documents. Language identification for the pages in mC4 is done by the cld3 tool (Botha et al., 2017)°, which is a small feed-forward network (Xue et al., 2021). Any pages with a language confidence below 0.95% are excluded. mC4 is deduplicated with exact match at the document level; however, fuzzy document-level deduplication is not performed. We utilize the latest version of mC4 (version 3.1.0)° Shttps://github.com/google/cld°https: //huggingface.co/datasets/mcOSCAR 21.9% mc66% Figure 1: Distribution of document counts from mCand OSCAR in our initial dataset. prepared by AllenAI in this work. A notable aspect of our dataset pertains to the web-based origin of our selected datasets, mCand OSCAR, extracted from CC. This differs from certain previous work (Radford et al., 2019; MosaicML, 2023; Touvron et al., 2023) that has also relied on curated datasets like The Pile (Gao et al., 2020) and BookCorpus (Zhu et al., 2015) to train LLMs, presuming their higher overall quality. However, in the context of multilingual settings, we argue that web-scraped datasets can be a more suitable approach, as curated datasets of superior quality might not be available for various languages. Our strategy of using web-scraped data facilitates efficient data collection across multiple languages, contributing to enhanced training data scales. Furthermore, recent studies have demonstrated the effectiveness of cleaning web-scraped data to yield state-of-the-art LLMs (Raffel et al., 2020; Almazrouei et al., 2023). In total, the combination of mC4 and OSCAR provides us 13.5B documents for further processing. Figure | illustrates the distribution of the document counts for mC4 and the four available versions of OSCAR in our initial dataset. 2.1 Data Cleaning Given the combination of the mC4 and OSCAR datasets, we first perform a comprehensive data cleaning procedure to remove noisy and bad content from the data, including language identification, ULR-based filtering, metric-based cleaning, and document refinement. --- --Language Identification: A particular issue concerns the use of two different language identification tools, i.e., cld3 and FastText, for mCand OSCAR (respectively). It has been shown in previous studies that cld3 is significantly worse than FastText, causing substantially more language detection errors for mC4 (Kreutzer et al., 2022). In fact, compared to several other language detectors, FastText has demonstrated state-of-the-art performance over benchmark datasets’. To this end, our first data cleaning step involves applying FastText to re-predict the languages for the documents in mC4. Documents whose predicted languages are different from the provided ones in mC4 will be removed from the dataset. The rationale is to avoid documents that are confusing for the language detectors cld3 and FastText, thus potentially introducing noise for the data. Finally, to ensure the highest quality, we remove data for any language found in mC4 but not supported by FastText. URL-based Filtering: In the next step, we aim to eliminate pages from the known toxic and harmful sources to reduce relevant risks from our data. In particular, we leverage the latest UT1 blacklist of URLs and domains provided by the University of Toulouse to support Internet use regulation for administrators at schools. This list involves sites from different topics, including pornography, grumbling, and hacking, that should be discarded for LLM training. Updated twice to thrice per week, the blacklist involves more than 3.7M records that are contributed by both human and robots (e.g., search engines, known addresses and indexes) (Abadji et al., 2022). As such, we remove any page from our dataset whose associated URL matches a site in the blacklist. This step is helpful for our dataset as the blacklist is not employed before for the mCdataset. In addition, although OSCAR has already used this blacklist for data cleaning, our approach incorporates the most up-to-date information from the list, which might not be available for the current distributions of OSCAR. Metric-based Cleaning: To enhance the dataset’s quality, motivated by the data processing pipeline from the BigScience’s ROOTS corpus for BLOOM (Laurencon et al., 2022; Scao et al., 2022), we further utilize the distributions for various dataset metrics to identify and filter outlying documents. Each metric provides a singular value Thttps://modelpredict.com/ language-identification-survey for every document within the dataset, quantifying specific attributes such as number_words, stopword_ratios, and perplexity_score for each document. For each metric and its range of possible values within the dataset, a threshold will be determined to partition the range into two zones: a normal range and an abnormal range. The abnormal range is designated for documents exhibiting metric values significantly deviating from the norm, classifying them as outliers/noises, and consequently, these outliers are removed from our dataset. As such, we employ a comprehensive array of dataset metrics, which will be collectively employed to refine our dataset, as outlined below: Number of words Character repetition ratio Word repetition ratio Special character ratio Stop word ratio Flagged word ratio Language identification confidence Perplexity score Document length (number of characters) Number of lines Short line length ratio Short line ratio The last four metrics are suggested by the OSCAR dataset while the others are inherited from the BigScience ROOTS corpus’s pipeline to process OSCAR data. For the perplexity score, following the BigScience ROOTS corpus, we train a SentencePiece tokenizer (Kudo, 2018) and 5-gram KneserNey language models as provided in the KenLM library (Heafield, 2011) using the 20230501 dumps of Wikipedia. Documents displaying high perplexity scores based on these KenLM models are considered notably different from Wikipedia articles. This indicates a level of noise that will be excluded from our dataset (Wenzek et al., 2020). The tokenizer will also be used to obtain the number of words/tokens in the documents for our metrics. We publicly release our KenLM models in HuggingFace® to faciliate future exploration. Repeated information (e.g., words, paragraphs) can appear in the web-curated data due to crawling errors and low-quality sources, causing detrimental consequences for training LLMs (Holtzman et al., 2019). The character and word repetition ratios are thus designed to avoid documents with exces Shttps: //huggingface.co/uonlp/kenlm --- --sively repeated information. High frequencies of special characters, stop words, or flagged words can indicate noisy and low-quality documents. We thus utilize the stop word and flagged word lists for different languages to compute their ratios for document removal. In addition to the stop word and flagged word lists provided by BigScience ROOTS for their 13 languages, we further collect dictionaries for these types of words for other languages. We prioritize the lists that have been shared on personal GitHub accounts for various languages, as these are often crafted by native speakers and exhibit higher quality. Moreover, lower language identification confidence might also suggest noisy language structures for the data. For each document in the dataset, we thus obtain a language identification confidence via the probability that FastText assigns to its corresponding language to aid data filtering. Finally, for the short line-based criteria, we implement a threshold of 100 characters to classify lines as short, as used by OSCAR. Documents with excessive occurrence of short lines will not be retained in our dataset. Threshold Selection: Given the set of dataset metrics, an important question concerns the selection of appropriate thresholds for each metric and language to generate high-quality multilingual data. In the BigScience ROOTS project (Laurencon et al., 2022), this selection process is carried out by native speakers of 13 languages. The resulting thresholds are employed for the rest of their 46 languages. The project offers a visualization interface that indexes a sample of a few thousand documents per language, enabling users to monitor data statistics as they adjust thresholds for the metrics. However, this process cannot be easily extended to different languages due to the requirement of experienced native speakers, which incurs significant costs. Furthermore, the limited sample sizes hinder the representativeness of the chosen thresholds for the full datasets. In our analysis, we observe that some selected thresholds for certain languages within BigScience ROOTS almost fall outside the value ranges for the entire dataset, leading to the deactivation of the corresponding metrics. To address these issues, we leverage a variant of the Interquartile Range (IQR) method (Dekking et al., 2007) to select appropriate thresholds for the filtering metrics for our dataset. For each metric and language, we generate a distribution of its possible values across the entire dataset for the lan guage. There is an exception for languages with substantial amounts of data, such as Spanish and Russian, where only 25% of the data is used to calculate these distributions. Afterward, we compute the @-th and Q3-th percentiles of the distribution (Q1 < Q3) and use them for the thresholds for our filtering metrics. In particular, the lower Q1th percentile will be chosen for the metrics that favor high values (e.g., language identification confidence), while metrics favoring low values (e.g., perplexity scores and document length) will utilize the upper @3-th percentile. We investigate different values for (Qi, Q3), considering (25, 75), (20, 80), (15, 85), (10, 90), and (5, 95). The selection of Q; = 10 and Q2 = 90 has achieved the best data quality for a sample of languages in our examination. It is worth emphasizing that the utilization of percentiles for threshold selection enables our approach to efficiently draw upon more extensive data samples for each language compared to those employed in the BigScience ROOTS project. This results in more reliable thresholds for the full datasets over different languages. Specifically, concerning the large languages where only a 25% data sample is employed to compute the value distribution for a metric, we observe that the proportion of discarded data to the entire dataset closely aligns with that of the data sample when applying the same selected filtering threshold. This underscores the representativeness of the thresholds selected through our methodology. Finally, once the thresholds for the metrics in a given language have been determined, we will eliminate any document that surpasses a metric’s threshold and enters the unfavorable range of the data. Document Refinement: The previous cleaning steps are done at the dataset level, aiming to remove low-quality documents from the dataset. In this step, we further clean the retained documents to improve the quality. It is important to note that our prior metric-based filtering step plays a vital role in eliminating highly noisy documents, which, in turn, streamlines the process of developing effective document cleaning rules during this step. Notably, since the documents from mC4 and OSCAR are extracted from HTML pages crawled from the Internet, a significant portion of them may carry crawling and extraction errors, including long JavaScript lines and extraneous content. Consequently, filtering out these documents greatly simplifies our task --- --of designing rules to clean the documents within our dataset. As such, for each document, we eliminate its noisy or irrelevant portions via a series of operations. First, we remove any short lines located at the end of each document, as these lines typically contain footer details or unhelpful information from the websites. Second, we eliminate the lines that contain words from our list of JavaScript (JS) keywords (e.g., “<script”) to avoid irrelevant and non-linguistic information. Here, we exclusively remove JS lines if the document contains just one line with JS keywords, and this particular line must also feature at least two different types of JS keywords. We adopt this approach as documents with more than two JS lines are likely coding tutorials in our data, which should be preserved to improve diversity. In addition, certain JS keywords are used in natural language, e.g., “var”. By requiring at least two different types of JS keywords, we reduce the risk of inadvertently omitting helpful content and disrupting the document’s structure. 2.2 Data Deduplication Despite thorough data cleaning, the remaining dataset might still contain a substantial amount of repeated data due to various reasons, including information being reposted on the web, multiple references to the same articles, boilerplate content, and plagiarism. The duplicated data can thus cause memorization and significantly hinder generalization for LLMs (Lee et al., 2022; Hernandez et al., 2022). Although expensive, data deduplication is thus considered as a crucial step to guarantee the highest quality of data for training LLMs. To this end, we undertake a comprehensive deduplication procedure for our dataset, utilizing MinHash (Broder, 1997) and URLs. This deduplication process is carried out independently for each language. Furthermore, we restrict deduplication to languages that retain over 100K documents following our data cleaning procedures (i.e., 51.5% of our languages), aiming to promote smaller languages within our dataset. MinHash Deduplication: For each language’s dataset, we first apply the MinHashLSH method (Leskovec et al., 2020) to filter similar documents in the dataset. MinHashLSH is a near deduplication technique based on MinHash (Broder, 1997) with multiple hash functions for n-grams and the Jaccard similarity. Locality-Sensitive Hashing (LSH) is incorporated to improve efficiency by focusing on document pairs that are most likely similar. We leverage a variant of the Spark implementation of MinHashLSH in the text-dedup repo”, employing 5-grams and a threshold of 0.8 to determine similar documents for the Jaccard similarity. Running MinHashLSH for each language’s dataset, especially for languages with the largest data volumes like English, Russian, Spanish, and Chinese, represents the most computationally expensive operation in our dataset creation effort. URL-based Deduplication: Finally, we eliminate all documents that share identical URLs with other documents in the dataset. This step is necessary to address situations where various versions of the same articles are linked to identical URLs but have been updated or modified during the publication process, effectively bypassing the near deduplication step. Some URLs for the articles in CC might only display their general domains due to crawling errors. To enhance accuracy, we refrain from removing URLs that only include their general domains. We utilize 600 AWS c5.24xlarge EC2 instances to preprocess and deduplicate our multilingual dataset. Each instance is equipped with 96 CPU cores, 192GB of memory, and 1TB of disk space. The disk space can be used to replace memory when necessary (e.g., for data deduplication). 3 Data Analysis and Experiments After completing all the cleaning and deduplication steps, our ultimate dataset comprises 6.3 trillion tokens spanning 167 languages. Table 1 provides an overview of the number of documents and tokens for the top 42 languages in CulturaX following each processing stage. As can be seen, our datacleaning pipeline can substantially reduce the number of documents in the original mC4 and OSCAR datasets for each language. The total number of removed documents accounts for 46.48% of our initial documents, suggesting the the effectiveness of our approaches to filter noisy information for multilingual datasets. 4
--- RELATED WORK ---
Compared to other NLP tasks, language models can be trained with unlabeled data, enabling efficient data collection to produce gigantic scales for https: //github. com/ChenghaoMou/text-dedup/ tree/main --- --#Documents (M) #Tokens Code Language Initial URL Metric MinHash URL Filtering (B) (%) Filtering Filtering Dedup Dedup_ Rate (%) en English 5783.24 5766.08 3586.85 3308.30 3241.07 43.96 2846.97 45.ru Russian 1431.35 1429.05 922.34 845.64 799.3 44.16 737.20 11.es Spanish 844.48 842.75 530.01 479.65 450.94 46.60 373.85 5.de German 863.18 861.46 515.83 447.06 420.02 51.34 357.03 5.fr French 711.64 709.48 439.69 387.37 363.75 48.89 319.33 5.zh Chinese 444.37 444.03 258.35 222.37 218.62 50.80 227.06 3.it Italian 406.87 406.04 254.72 226.42 211.3 48.06 165.45 2.pt Portuguese 347.47 346.76 217.21 200.11 190.29 45.24 136.94 2.pl Polish 270.12 269.73 70.86 151.71 142.17 47.37 117.27 1.ja Japanese 247.67 247.19 37.88 114.64 = 111.19 55.11 107.87 1.vi Vietnamese 182.88 82.72 18.67 108.77. 102.4 44.00 98.45 1.nl Dutch 238.92 238.56 48.19 125.51 117.39 50.87 80.03 1.ar Arabic 132.88 32.65 84.84 77.65 74.03 44.29 69.35 1.tr Turkish 183.65 83.47 09.94 99.18 94.2 48.70 64.29 1.cs Czech 136.91 36.44 80.38 69.01 65.35 52.27 56.91 0.fa Persian 118.55 18.50 70.26 62.42 59.53 49.78 45.95 0.hu Hungarian 88.59 88.21 53.29 46.89 13 50.19 43.42 0.el Greek 100.77 00.68 61.43 54.33 51.43 48.96 43.15 0.ro Romanian 89.37 89.25 45.99 42.8 40.33 54.87 39.65 0.sv Swedish 103.04 02.76 58.67 52.09 49.71 51.76 38.49 0.uk Ukrainian 81.50 81.44 50.95 47.12 74 45.10 38.23 0.fi Finnish 59.85 59.80 36.69 32.15 30.47 49.09 28.93 0.ko Korean 46.09 45.85 25.19 21.17 20.56 55.39 24.77 0.da Danish 53.16 52.99 28.67 26.48 25.43 52.16 22.92 0.bg Bulgarian 47.01 46.90 28.09 25.45 24.13 48.67 22.92 0.no Norwegian 40.07 40.01 20.69 19.49 18.9 52.81 18.43 0.hi Hindi 35.59 35.50 22.01 20.77 19.67 44.73 16.79 0.sk Slovak 40.13 39.95 22.20 19.56 18.58 53.70 16.44 0.th Thai 49.04 48.96 26.20 21.93 20.96 57.26 15.72 0.It Lithuanian 27.08 27.01 15.87 14.25 13.34 50.74 14.25 0.ca Catalan 31.13 31.12 18.99 16.46 15.53 50.11 12.53 0.id Indonesian 48.08 48.05 25.79 23.74 23.25 51.64 12.06 0.bn Bangla 20.90 20.85 13.82 13.22 12.44 40.48 9.57 0.et Estonian 6.20 6.15 9.69 8.45 8.00 50.62 8.81 0.sl Slovenian 5.46 5.39 8.00 7.60 7.34 52.52 8.01 0.lv Latvian 4.14 4.09 8.37 748 7.14 49.50 7.85 0.he Hebrew 0.78 0.77 5.90 477 4.65 56.86 4.94 0.sr Serbian 7.80 71.75 4.80 4.25 4.05 48.08 4.62 0.ta Tamil 8.77 8.75 5.27 4.94 4.73 46.07 4.38 0.sq Albanian 9.40 9.38 5.96 5.04 5.2 44.57 3.65 0.az Azerbaijani 9.66 9.65 5.73 5.24 5.08 4741 3.51 0.Total (42 languages) 13397.79 13366.17 8254.28 7471.48 7181.40 46.40 6267.99 99.Total (167 languages) 13506.76 13474.94 8308.74 7521.23 7228.91 46.48 6308.42 100.Table 1: Data statistics for 42 languages with the percentages of tokens greater than 0.05% in our dataset. Columns grouped with the “#Documents (M)” label indicate the number of documents for each language after the corresponding cleaning and reduplication steps. The token counts are based on our final dataset (i.e., after all the cleaning and deduplication steps). --- --the training data. There are two primary types of data commonly used for training LLMs: curated data and web crawl data. Curated data typically consists of well-written and well-formatted text from targeted sources and domains, e.g., Wikipedia articles, books, newswire articles, and scientific papers, as used for the “The Pile” (Gao et al., 2020) and “BookCorpus” (Zhu et al., 2015) datasets. In contrast, web crawl data encompasses text gathered from a wide array of sources across the internet, varying significantly in terms of format and writing styles, e.g., blogs, social media posts, news articles, and advertisements. CommonCrawIl (CC) is a widely-used web crawl repository that has collected petabytes of data over the Internet for 12 years. To this end, curated data is frequently considered to possess higher quality, which has resulted in its preference for training early LLMs, e.g., BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). However, as the demand for larger models has grown, web crawl data has gained more attention as it contributes a substantial portion to the training data of recent LLMs, e.g., ROBERTa (Liu et al., 2019), BART (Lewis et al., 2020), TS (Raffel et al., 2020), GPT-3 (Rae et al., 2021), LLaMa (Touvron et al., 2023), MPT (MosaicML, 2023), and Falcon (Almazrouei et al., 2023). As such, different extractions of CC has been produced to train such LLMs, including C4 (Raffel et al., 2020), CC-News (Nagel), and STORIES (Trinh and Le, 2018). Regarding the accessibility of training data, datasets used to train early LLMs are often made available to the public (Devlin et al., 2019; Raffel et al., 2020). However, in the case of the most recent state-of-the-art (SOTA) generative LLMs, their training datasets are not released fully, potentially due to commercial interests. This applies not only to proprietary models like ChatGPT and GPT-4 but also to models that claim to be opensource models such as LLaMa, MPT, Falcon, and BLOOM (Scao et al., 2022). To address the transparency issue with existing LLMs, recent efforts have been made to replicate and release the training datasets for the state-of-the-art LLMs, i.e., RedPajama (Computer, 2023), SlimPajama, and AIDolma. The key distinctions for these datasets concern their large-scale text data that has been meticulously cleaned and document-level deduplicated to ensure high quality for training LLMs. Nonetheless, a common drawback of these open source datasets is that they remain predominantly focused on English data, offering limited data for other languages. To obtain a multilingual large-scale dataset for training LLMs, it is more convenient to exploit web-scrape datasets such as CC to enable efficient data collection with up-to-date information in multiple languages. In addition, to ensure high quality for high-performing LLMs, it is necessary to extensively clean and deduplicate the multilingual data to avoid noisy and irrelevant content, e.g., low-quality machine-generated text and adult content (Trinh and Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). As such, a typical data processing pipeline to generate high-quality datasets can involve multiple steps, as demonstrated by FastText (Joulin et al., 2016), CC-Net (Wenzek et al., 2020), the BigScience ROOTS corpus for the BLOOM models (Laurengon et al., 2022; Scao et al., 2022), the RefinedWeb dataset for the Falcon model (Penedo et al., 2023; Almazrouei et al., 2023), and the dataset to train the LLaMa models (Touvron et al., 2023). The first step necessitates in such pipelines language identification to appropriately assign data to their corresponding languages (Joulin et al., 2016). The next steps features various dataset-specific rules and heuristics to filter undesirable content according to the ratios of special characters, short lines, bad words, among others (Grave et al., 2018; Laurengon et al., 2022). The data can also be filtered via lightweight models, e.g., via the KenLM language models (Heafield, 2011), to avoid noisy documents (Wenzek et al., 2020). Finally, data deduplication should be performed to remove similar or repeated information (Laurengon et al., 2022; Penedo et al., 2023). An important step in this regard involves fuzzy deduplication at document level, e.g., via MinHash (Broder, 1997), to eliminate similar documents, thus mitigating memorization and improving the generalization for resulting LLMs (Lee et al., 2022). To this end, while there are multilingual opensource datasets with text data in multiple languages, such as mC4 (Xue et al., 2021), OSCAR (Ortiz Suarez et al., 2019), CC100 (Wenzek et al., 2020; Conneau et al., 2020), and the BigScience ROOT corpus (Laurencon et al., 2022), their quality and scale do not meet the requirements for effectively training LLMs, particularly generative models such as GPT. For example, as highlighted in the introduction, both mC4 and OSCAR lack fuzzy dedu --- --plication for the data at the document level. mCalso suffers from its poorer language identification due to the use of cld3. BigScience ROOTS only provides a small sample data for 46 languages while CC100 does not have information beyond 2018. Our dataset CulturaX thus comprehensively addresses the issues for the existing datasets, offering a multilingual, open-source, and large-scale dataset with readily usable and high-quality data to train LLMs. 5 Conclusion We present CulturaX, a novel multilingual dataset with text data for 167 languages. Our dataset is cleaned and deduplicated via a comprehensive pipeline, producing 6.3 trillion tokens. CulturaX is thus a large-scale and high-quality dataset, which can be readily used to train high-performing LLMs for multiple languages. Our data is openly accessible to the public to promote further research and applications of multilingual learning. References Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoit Sagot. 2022. Towards a cleaner documentoriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4344-4355, Marseille, France. European Language Resources Association. Julien Abadji, Pedro Javier Ortiz Sudrez, Laurent Romary, and Benoit Sagot. 2021. Ungoliant: An optimized pipeline for the generation of a very largescale multilingual web corpus. In Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event). Miltiadis Allamanis. 2018. The adverse effects of code duplication in machine learning models of code. Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz Alshamsi et al. 2023. Falcon-40B: an open large language model with state-of-the-art performance. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. ArXiv, abs/2302.04023. Marta Bajion, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espla-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramirez-Sanchez, Elsa Sarrfas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555-4567, Online. Association for Computational Linguistics. Rishi Bommasani, Drew A. Hudson, and Ehsan Adeli et al. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, and Slav Petrov. 2017. Natural language processing with small feed-forward networks. In Proceedings of theConference on Empirical
--- METHOD ---
(Dekking et al., 2007) to select appropriate thresholds for various dataset metrics (e.g., stopword ratios, data perplexity, and language identification scores), which can be used to filter noisy outliers for the dataset. As such, we leverage the percentiles of the distributions computed over large samples of data to effectively guide the threshold selection process for each filtering metric and language. Finally, we perform extensive deduplication for the data of the languages within our datasets based on the near deduplication method MinHashLSH (Broder, 1997; Leskovec et al., 2020) and URLs, leading to high-quality data to train multilingual LLMs. Our dataset will be fully available to the public to promote further research and development for multilingual learning. To our knowledge, CulturaX is the largest open-source multilingual dataset to date that is deeply cleaned and deduplicated for LLM and NLP applications. 2 Multilingual Dataset Creation To develop a multilingual public dataset for LLMs, our strategy is to combine mC4 (Xue et al., 2021) and OSCAR (Ortiz Suarez et al., 2019; Abadji et al., 2021, 2022), two largest multilingual datasets at our disposal. We then process the data with an extensive pipeline, involving two major steps of cleaning and deduplication, to produce an enormous and high-quality dataset for multilingual LLMs. mC4 is a multilingual document-level dataset, originally created to train the multilingual encoderdecoder model mT5 (Xue et al., 2021) for 101 languages. This dataset is extracted from 71 monthly snapshots from CC by removing pages with less than three long lines (line length filter), pages with bad words, and duplicated lines across documents. Language identification for the pages in mC4 is done by the cld3 tool (Botha et al., 2017)°, which is a small feed-forward network (Xue et al., 2021). Any pages with a language confidence below 0.95% are excluded. mC4 is deduplicated with exact match at the document level; however, fuzzy document-level deduplication is not performed. We utilize the latest version of mC4 (version 3.1.0)° Shttps://github.com/google/cld°https: //huggingface.co/datasets/mcOSCAR 21.9% mc66% Figure 1: Distribution of document counts from mCand OSCAR in our initial dataset. prepared by AllenAI in this work. A notable aspect of our dataset pertains to the web-based origin of our selected datasets, mCand OSCAR, extracted from CC. This differs from certain previous work (Radford et al., 2019; MosaicML, 2023; Touvron et al., 2023) that has also relied on curated datasets like The Pile (Gao et al., 2020) and BookCorpus (Zhu et al., 2015) to train LLMs, presuming their higher overall quality. However, in the context of multilingual settings, we argue that web-scraped datasets can be a more suitable approach, as curated datasets of superior quality might not be available for various languages. Our strategy of using web-scraped data facilitates efficient data collection across multiple languages, contributing to enhanced training data scales. Furthermore, recent studies have demonstrated the effectiveness of cleaning web-scraped data to yield state-of-the-art LLMs (Raffel et al., 2020; Almazrouei et al., 2023). In total, the combination of mC4 and OSCAR provides us 13.5B documents for further processing. Figure | illustrates the distribution of the document counts for mC4 and the four available versions of OSCAR in our initial dataset. 2.1 Data Cleaning Given the combination of the mC4 and OSCAR datasets, we first perform a comprehensive data cleaning procedure to remove noisy and bad content from the data, including language identification, ULR-based filtering, metric-based cleaning, and document refinement. --- --Language Identification: A particular issue concerns the use of two different language identification tools, i.e., cld3 and FastText, for mCand OSCAR (respectively). It has been shown in previous studies that cld3 is significantly worse than FastText, causing substantially more language detection errors for mC4 (Kreutzer et al., 2022). In fact, compared to several other language detectors, FastText has demonstrated state-of-the-art performance over benchmark datasets’. To this end, our first data cleaning step involves applying FastText to re-predict the languages for the documents in mC4. Documents whose predicted languages are different from the provided ones in mC4 will be removed from the dataset. The rationale is to avoid documents that are confusing for the language detectors cld3 and FastText, thus potentially introducing noise for the data. Finally, to ensure the highest quality, we remove data for any language found in mC4 but not supported by FastText. URL-based Filtering: In the next step, we aim to eliminate pages from the known toxic and harmful sources to reduce relevant risks from our data. In particular, we leverage the latest UT1 blacklist of URLs and domains provided by the University of Toulouse to support Internet use regulation for administrators at schools. This list involves sites from different topics, including pornography, grumbling, and hacking, that should be discarded for LLM training. Updated twice to thrice per week, the blacklist involves more than 3.7M records that are contributed by both human and robots (e.g., search engines, known addresses and indexes) (Abadji et al., 2022). As such, we remove any page from our dataset whose associated URL matches a site in the blacklist. This step is helpful for our dataset as the blacklist is not employed before for the mCdataset. In addition, although OSCAR has already used this blacklist for data cleaning, our approach incorporates the most up-to-date information from the list, which might not be available for the current distributions of OSCAR. Metric-based Cleaning: To enhance the dataset’s quality, motivated by the data processing pipeline from the BigScience’s ROOTS corpus for BLOOM (Laurencon et al., 2022; Scao et al., 2022), we further utilize the distributions for various dataset metrics to identify and filter outlying documents. Each metric provides a singular value Thttps://modelpredict.com/ language-identification-survey for every document within the dataset, quantifying specific attributes such as number_words, stopword_ratios, and perplexity_score for each document. For each metric and its range of possible values within the dataset, a threshold will be determined to partition the range into two zones: a normal range and an abnormal range. The abnormal range is designated for documents exhibiting metric values significantly deviating from the norm, classifying them as outliers/noises, and consequently, these outliers are removed from our dataset. As such, we employ a comprehensive array of dataset metrics, which will be collectively employed to refine our dataset, as outlined below: Number of words Character repetition ratio Word repetition ratio Special character ratio Stop word ratio Flagged word ratio Language identification confidence Perplexity score Document length (number of characters) Number of lines Short line length ratio Short line ratio The last four metrics are suggested by the OSCAR dataset while the others are inherited from the BigScience ROOTS corpus’s pipeline to process OSCAR data. For the perplexity score, following the BigScience ROOTS corpus, we train a SentencePiece tokenizer (Kudo, 2018) and 5-gram KneserNey language models as provided in the KenLM library (Heafield, 2011) using the 20230501 dumps of Wikipedia. Documents displaying high perplexity scores based on these KenLM models are considered notably different from Wikipedia articles. This indicates a level of noise that will be excluded from our dataset (Wenzek et al., 2020). The tokenizer will also be used to obtain the number of words/tokens in the documents for our metrics. We publicly release our KenLM models in HuggingFace® to faciliate future exploration. Repeated information (e.g., words, paragraphs) can appear in the web-curated data due to crawling errors and low-quality sources, causing detrimental consequences for training LLMs (Holtzman et al., 2019). The character and word repetition ratios are thus designed to avoid documents with exces Shttps: //huggingface.co/uonlp/kenlm --- --sively repeated information. High frequencies of special characters, stop words, or flagged words can indicate noisy and low-quality documents. We thus utilize the stop word and flagged word lists for different languages to compute their ratios for document removal. In addition to the stop word and flagged word lists provided by BigScience ROOTS for their 13 languages, we further collect dictionaries for these types of words for other languages. We prioritize the lists that have been shared on personal GitHub accounts for various languages, as these are often crafted by native speakers and exhibit higher quality. Moreover, lower language identification confidence might also suggest noisy language structures for the data. For each document in the dataset, we thus obtain a language identification confidence via the probability that FastText assigns to its corresponding language to aid data filtering. Finally, for the short line-based criteria, we implement a threshold of 100 characters to classify lines as short, as used by OSCAR. Documents with excessive occurrence of short lines will not be retained in our dataset. Threshold Selection: Given the set of dataset metrics, an important question concerns the selection of appropriate thresholds for each metric and language to generate high-quality multilingual data. In the BigScience ROOTS project (Laurencon et al., 2022), this selection process is carried out by native speakers of 13 languages. The resulting thresholds are employed for the rest of their 46 languages. The project offers a visualization interface that indexes a sample of a few thousand documents per language, enabling users to monitor data statistics as they adjust thresholds for the metrics. However, this process cannot be easily extended to different languages due to the requirement of experienced native speakers, which incurs significant costs. Furthermore, the limited sample sizes hinder the representativeness of the chosen thresholds for the full datasets. In our analysis, we observe that some selected thresholds for certain languages within BigScience ROOTS almost fall outside the value ranges for the entire dataset, leading to the deactivation of the corresponding metrics. To address these issues, we leverage a variant of the Interquartile Range (IQR) method (Dekking et al., 2007) to select appropriate thresholds for the filtering metrics for our dataset. For each metric and language, we generate a distribution of its possible values across the entire dataset for the lan guage. There is an exception for languages with substantial amounts of data, such as Spanish and Russian, where only 25% of the data is used to calculate these distributions. Afterward, we compute the @-th and Q3-th percentiles of the distribution (Q1 < Q3) and use them for the thresholds for our filtering metrics. In particular, the lower Q1th percentile will be chosen for the metrics that favor high values (e.g., language identification confidence), while metrics favoring low values (e.g., perplexity scores and document length) will utilize the upper @3-th percentile. We investigate different values for (Qi, Q3), considering (25, 75), (20, 80), (15, 85), (10, 90), and (5, 95). The selection of Q; = 10 and Q2 = 90 has achieved the best data quality for a sample of languages in our examination. It is worth emphasizing that the utilization of percentiles for threshold selection enables our approach to efficiently draw upon more extensive data samples for each language compared to those employed in the BigScience ROOTS project. This results in more reliable thresholds for the full datasets over different languages. Specifically, concerning the large languages where only a 25% data sample is employed to compute the value distribution for a metric, we observe that the proportion of discarded data to the entire dataset closely aligns with that of the data sample when applying the same selected filtering threshold. This underscores the representativeness of the thresholds selected through our methodology. Finally, once the thresholds for the metrics in a given language have been determined, we will eliminate any document that surpasses a metric’s threshold and enters the unfavorable range of the data. Document Refinement: The previous cleaning steps are done at the dataset level, aiming to remove low-quality documents from the dataset. In this step, we further clean the retained documents to improve the quality. It is important to note that our prior metric-based filtering step plays a vital role in eliminating highly noisy documents, which, in turn, streamlines the process of developing effective document cleaning rules during this step. Notably, since the documents from mC4 and OSCAR are extracted from HTML pages crawled from the Internet, a significant portion of them may carry crawling and extraction errors, including long JavaScript lines and extraneous content. Consequently, filtering out these documents greatly simplifies our task --- --of designing rules to clean the documents within our dataset. As such, for each document, we eliminate its noisy or irrelevant portions via a series of operations. First, we remove any short lines located at the end of each document, as these lines typically contain footer details or unhelpful information from the websites. Second, we eliminate the lines that contain words from our list of JavaScript (JS) keywords (e.g., “<script”) to avoid irrelevant and non-linguistic information. Here, we exclusively remove JS lines if the document contains just one line with JS keywords, and this particular line must also feature at least two different types of JS keywords. We adopt this approach as documents with more than two JS lines are likely coding tutorials in our data, which should be preserved to improve diversity. In addition, certain JS keywords are used in natural language, e.g., “var”. By requiring at least two different types of JS keywords, we reduce the risk of inadvertently omitting helpful content and disrupting the document’s structure. 2.2 Data Deduplication Despite thorough data cleaning, the remaining dataset might still contain a substantial amount of repeated data due to various reasons, including information being reposted on the web, multiple references to the same articles, boilerplate content, and plagiarism. The duplicated data can thus cause memorization and significantly hinder generalization for LLMs (Lee et al., 2022; Hernandez et al., 2022). Although expensive, data deduplication is thus considered as a crucial step to guarantee the highest quality of data for training LLMs. To this end, we undertake a comprehensive deduplication procedure for our dataset, utilizing MinHash (Broder, 1997) and URLs. This deduplication process is carried out independently for each language. Furthermore, we restrict deduplication to languages that retain over 100K documents following our data cleaning procedures (i.e., 51.5% of our languages), aiming to promote smaller languages within our dataset. MinHash Deduplication: For each language’s dataset, we first apply the MinHashLSH method (Leskovec et al., 2020) to filter similar documents in the dataset. MinHashLSH is a near deduplication technique based on MinHash (Broder, 1997) with multiple hash functions for n-grams and the Jaccard similarity. Locality-Sensitive Hashing (LSH) is incorporated to improve efficiency by focusing on document pairs that are most likely similar. We leverage a variant of the Spark implementation of MinHashLSH in the text-dedup repo”, employing 5-grams and a threshold of 0.8 to determine similar documents for the Jaccard similarity. Running MinHashLSH for each language’s dataset, especially for languages with the largest data volumes like English, Russian, Spanish, and Chinese, represents the most computationally expensive operation in our dataset creation effort. URL-based Deduplication: Finally, we eliminate all documents that share identical URLs with other documents in the dataset. This step is necessary to address situations where various versions of the same articles are linked to identical URLs but have been updated or modified during the publication process, effectively bypassing the near deduplication step. Some URLs for the articles in CC might only display their general domains due to crawling errors. To enhance accuracy, we refrain from removing URLs that only include their general domains. We utilize 600 AWS c5.24xlarge EC2 instances to preprocess and deduplicate our multilingual dataset. Each instance is equipped with 96 CPU cores, 192GB of memory, and 1TB of disk space. The disk space can be used to replace memory when necessary (e.g., for data deduplication). 3 Data Analysis and
--- EXPERIMENT ---
s After completing all the cleaning and deduplication steps, our ultimate dataset comprises 6.3 trillion tokens spanning 167 languages. Table 1 provides an overview of the number of documents and tokens for the top 42 languages in CulturaX following each processing stage. As can be seen, our datacleaning pipeline can substantially reduce the number of documents in the original mC4 and OSCAR datasets for each language. The total number of removed documents accounts for 46.48% of our initial documents, suggesting the the effectiveness of our approaches to filter noisy information for multilingual datasets. 4 Related Work Compared to other NLP tasks, language models can be trained with unlabeled data, enabling efficient data collection to produce gigantic scales for https: //github. com/ChenghaoMou/text-dedup/ tree/main --- --#Documents (M) #Tokens Code Language Initial URL Metric MinHash URL Filtering (B) (%) Filtering Filtering Dedup Dedup_ Rate (%) en English 5783.24 5766.08 3586.85 3308.30 3241.07 43.96 2846.97 45.ru Russian 1431.35 1429.05 922.34 845.64 799.3 44.16 737.20 11.es Spanish 844.48 842.75 530.01 479.65 450.94 46.60 373.85 5.de German 863.18 861.46 515.83 447.06 420.02 51.34 357.03 5.fr French 711.64 709.48 439.69 387.37 363.75 48.89 319.33 5.zh Chinese 444.37 444.03 258.35 222.37 218.62 50.80 227.06 3.it Italian 406.87 406.04 254.72 226.42 211.3 48.06 165.45 2.pt Portuguese 347.47 346.76 217.21 200.11 190.29 45.24 136.94 2.pl Polish 270.12 269.73 70.86 151.71 142.17 47.37 117.27 1.ja Japanese 247.67 247.19 37.88 114.64 = 111.19 55.11 107.87 1.vi Vietnamese 182.88 82.72 18.67 108.77. 102.4 44.00 98.45 1.nl Dutch 238.92 238.56 48.19 125.51 117.39 50.87 80.03 1.ar Arabic 132.88 32.65 84.84 77.65 74.03 44.29 69.35 1.tr Turkish 183.65 83.47 09.94 99.18 94.2 48.70 64.29 1.cs Czech 136.91 36.44 80.38 69.01 65.35 52.27 56.91 0.fa Persian 118.55 18.50 70.26 62.42 59.53 49.78 45.95 0.hu Hungarian 88.59 88.21 53.29 46.89 13 50.19 43.42 0.el Greek 100.77 00.68 61.43 54.33 51.43 48.96 43.15 0.ro Romanian 89.37 89.25 45.99 42.8 40.33 54.87 39.65 0.sv Swedish 103.04 02.76 58.67 52.09 49.71 51.76 38.49 0.uk Ukrainian 81.50 81.44 50.95 47.12 74 45.10 38.23 0.fi Finnish 59.85 59.80 36.69 32.15 30.47 49.09 28.93 0.ko Korean 46.09 45.85 25.19 21.17 20.56 55.39 24.77 0.da Danish 53.16 52.99 28.67 26.48 25.43 52.16 22.92 0.bg Bulgarian 47.01 46.90 28.09 25.45 24.13 48.67 22.92 0.no Norwegian 40.07 40.01 20.69 19.49 18.9 52.81 18.43 0.hi Hindi 35.59 35.50 22.01 20.77 19.67 44.73 16.79 0.sk Slovak 40.13 39.95 22.20 19.56 18.58 53.70 16.44 0.th Thai 49.04 48.96 26.20 21.93 20.96 57.26 15.72 0.It Lithuanian 27.08 27.01 15.87 14.25 13.34 50.74 14.25 0.ca Catalan 31.13 31.12 18.99 16.46 15.53 50.11 12.53 0.id Indonesian 48.08 48.05 25.79 23.74 23.25 51.64 12.06 0.bn Bangla 20.90 20.85 13.82 13.22 12.44 40.48 9.57 0.et Estonian 6.20 6.15 9.69 8.45 8.00 50.62 8.81 0.sl Slovenian 5.46 5.39 8.00 7.60 7.34 52.52 8.01 0.lv Latvian 4.14 4.09 8.37 748 7.14 49.50 7.85 0.he Hebrew 0.78 0.77 5.90 477 4.65 56.86 4.94 0.sr Serbian 7.80 71.75 4.80 4.25 4.05 48.08 4.62 0.ta Tamil 8.77 8.75 5.27 4.94 4.73 46.07 4.38 0.sq Albanian 9.40 9.38 5.96 5.04 5.2 44.57 3.65 0.az Azerbaijani 9.66 9.65 5.73 5.24 5.08 4741 3.51 0.Total (42 languages) 13397.79 13366.17 8254.28 7471.48 7181.40 46.40 6267.99 99.Total (167 languages) 13506.76 13474.94 8308.74 7521.23 7228.91 46.48 6308.42 100.Table 1: Data statistics for 42 languages with the percentages of tokens greater than 0.05% in our dataset. Columns grouped with the “#Documents (M)” label indicate the number of documents for each language after the corresponding cleaning and reduplication steps. The token counts are based on our final dataset (i.e., after all the cleaning and deduplication steps). --- --the training data. There are two primary types of data commonly used for training LLMs: curated data and web crawl data. Curated data typically consists of well-written and well-formatted text from targeted sources and domains, e.g., Wikipedia articles, books, newswire articles, and scientific papers, as used for the “The Pile” (Gao et al., 2020) and “BookCorpus” (Zhu et al., 2015) datasets. In contrast, web crawl data encompasses text gathered from a wide array of sources across the internet, varying significantly in terms of format and writing styles, e.g., blogs, social media posts, news articles, and advertisements. CommonCrawIl (CC) is a widely-used web crawl repository that has collected petabytes of data over the Internet for 12 years. To this end, curated data is frequently considered to possess higher quality, which has resulted in its preference for training early LLMs, e.g., BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). However, as the demand for larger models has grown, web crawl data has gained more attention as it contributes a substantial portion to the training data of recent LLMs, e.g., ROBERTa (Liu et al., 2019), BART (Lewis et al., 2020), TS (Raffel et al., 2020), GPT-3 (Rae et al., 2021), LLaMa (Touvron et al., 2023), MPT (MosaicML, 2023), and Falcon (Almazrouei et al., 2023). As such, different extractions of CC has been produced to train such LLMs, including C4 (Raffel et al., 2020), CC-News (Nagel), and STORIES (Trinh and Le, 2018). Regarding the accessibility of training data, datasets used to train early LLMs are often made available to the public (Devlin et al., 2019; Raffel et al., 2020). However, in the case of the most recent state-of-the-art (SOTA) generative LLMs, their training datasets are not released fully, potentially due to commercial interests. This applies not only to proprietary models like ChatGPT and GPT-4 but also to models that claim to be opensource models such as LLaMa, MPT, Falcon, and BLOOM (Scao et al., 2022). To address the transparency issue with existing LLMs, recent efforts have been made to replicate and release the training datasets for the state-of-the-art LLMs, i.e., RedPajama (Computer, 2023), SlimPajama, and AIDolma. The key distinctions for these datasets concern their large-scale text data that has been meticulously cleaned and document-level deduplicated to ensure high quality for training LLMs. Nonetheless, a common drawback of these open source datasets is that they remain predominantly focused on English data, offering limited data for other languages. To obtain a multilingual large-scale dataset for training LLMs, it is more convenient to exploit web-scrape datasets such as CC to enable efficient data collection with up-to-date information in multiple languages. In addition, to ensure high quality for high-performing LLMs, it is necessary to extensively clean and deduplicate the multilingual data to avoid noisy and irrelevant content, e.g., low-quality machine-generated text and adult content (Trinh and Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). As such, a typical data processing pipeline to generate high-quality datasets can involve multiple steps, as demonstrated by FastText (Joulin et al., 2016), CC-Net (Wenzek et al., 2020), the BigScience ROOTS corpus for the BLOOM models (Laurengon et al., 2022; Scao et al., 2022), the RefinedWeb dataset for the Falcon model (Penedo et al., 2023; Almazrouei et al., 2023), and the dataset to train the LLaMa models (Touvron et al., 2023). The first step necessitates in such pipelines language identification to appropriately assign data to their corresponding languages (Joulin et al., 2016). The next steps features various dataset-specific rules and heuristics to filter undesirable content according to the ratios of special characters, short lines, bad words, among others (Grave et al., 2018; Laurengon et al., 2022). The data can also be filtered via lightweight models, e.g., via the KenLM language models (Heafield, 2011), to avoid noisy documents (Wenzek et al., 2020). Finally, data deduplication should be performed to remove similar or repeated information (Laurengon et al., 2022; Penedo et al., 2023). An important step in this regard involves fuzzy deduplication at document level, e.g., via MinHash (Broder, 1997), to eliminate similar documents, thus mitigating memorization and improving the generalization for resulting LLMs (Lee et al., 2022). To this end, while there are multilingual opensource datasets with text data in multiple languages, such as mC4 (Xue et al., 2021), OSCAR (Ortiz Suarez et al., 2019), CC100 (Wenzek et al., 2020; Conneau et al., 2020), and the BigScience ROOT corpus (Laurencon et al., 2022), their quality and scale do not meet the requirements for effectively training LLMs, particularly generative models such as GPT. For example, as highlighted in the introduction, both mC4 and OSCAR lack fuzzy dedu --- --plication for the data at the document level. mCalso suffers from its poorer language identification due to the use of cld3. BigScience ROOTS only provides a small sample data for 46 languages while CC100 does not have information beyond 2018. Our dataset CulturaX thus comprehensively addresses the issues for the existing datasets, offering a multilingual, open-source, and large-scale dataset with readily usable and high-quality data to train LLMs. 5
--- CONCLUSION ---
We present CulturaX, a novel multilingual dataset with text data for 167 languages. Our dataset is cleaned and deduplicated via a comprehensive pipeline, producing 6.3 trillion tokens. CulturaX is thus a large-scale and high-quality dataset, which can be readily used to train high-performing LLMs for multiple languages. Our data is openly accessible to the public to promote further research and applications of multilingual learning. References Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoit Sagot. 2022. Towards a cleaner documentoriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4344-4355, Marseille, France. European Language Resources Association. Julien Abadji, Pedro Javier Ortiz Sudrez, Laurent Romary, and Benoit Sagot. 2021. Ungoliant: An optimized pipeline for the generation of a very largescale multilingual web corpus. In Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event). Miltiadis Allamanis. 2018. The adverse effects of code duplication in machine learning models of code. Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz Alshamsi et al. 2023. Falcon-40B: an open large language model with state-of-the-art performance. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. ArXiv, abs/2302.04023. Marta Bajion, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espla-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramirez-Sanchez, Elsa Sarrfas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555-4567, Online. Association for Computational Linguistics. Rishi Bommasani, Drew A. Hudson, and Ehsan Adeli et al. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, and Slav Petrov. 2017. Natural language processing with small feed-forward networks. In Proceedings of theConference on Empirical Methods in Natural Language Processing, pages 2879-2885, Copenhagen, Denmark. Association for Computational Linguistics. A. Broder. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences. Tom Brown, Benjamin Mann, and et al. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165. Aakanksha Chowdhery, Sharan Narang, and Jacob Devlin et al. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311. Together Computer. 2023. Redpajama: An open source recipe to reproduce llama training dataset. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics. Michel Dekking, Cornelis Kraaikamp, Hendrik Paul, and Ludolf Erwin Meester. 2007. A modern introduction to probability and statistics: Understanding why and how. In Springer Texts in Statistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An --- --800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027. Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 2018. Learning word vectors for 157 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187-197, Edinburgh, Scotland. Association for Computational Linguistics. Danny Hernandez, Tom B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, T. J. Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Christopher Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. 2022. Scaling laws and interpretability of learning from repeated data. ArXiv, abs/2205.10487. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. ArXiv, abs/1904.09751. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomas Mikolov. 2016. Fasttext.zip: Compressing text classification models. ArXiv, abs/1612.03651. Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021. Alignment of language agents. ArXiv, abs/2103.14659. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoit Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Miiller, André Miiller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Cabuk Ball, Stella Biderman, Alessia Battisti, Anmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66-75, Melbourne, Australia. Association for Computational Linguistics. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. ArXiv, abs/2304.05613. Hugo Laurencgon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, Jorg Frohberg, Mario Sa¥ko, Quentin Lhoest, Angelina McMillan-Major, Gérard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero Mujioz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite. 2022. The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424-8445, Dublin, Ireland. Association for Computational Linguistics. Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. 2020. Mining of massive datasets. In Cambridge University Press. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Dangi Chen, Omer Levy, Mike Lewis, --- --Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692. MosaicML. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable Ilms. https: //www.mosaicml.com/blog/mpt- 7b. Sebastian Cc-news. http: //web.archive.org/save/http: //commoncrawl.org/2016/10/news- dataset-available. Nagel. Pedro Javier Ortiz Suarez, Laurent Romary, and Benoit Sagot. 2020. A monolingual approach to contextualized word embeddings for mid-resource languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 17031714, Online. Association for Computational Linguistics. Pedro Javier Ortiz Suarez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. In Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC7) 2019. Cardiff; 22nd July 2019. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon Ilm: Outperforming curated corpora with web data, and web data only. ArXiv, abs/2306.01116. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog. Jack Rae, Sebastian Borgeaud, and et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. Teven Scao, Angela Fan, and et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzman. 2021. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351-1361, Online. Association for Computational Linguistics. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-Im: Training multi-billion parameter language models using model parallelism. ArXiv, abs/1909.08053. Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. ArXiv, abs/2102.02503. Hugo Touvron, Thibaut Lavril, and Gautier Izacard et al. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971. Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. ArXiv, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Ilia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Xiangpeng Wei, Hao-Ran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, and Jun Xie. 2023. Polylm: An open source polyglot large language model. ArXiv, abs/2307.06018. Laura Weidinger, John F. J. Mellor, and Maribeth Rauh et al. 2021. Ethical and social risks of harm from language models. ArXiv, abs/2112.04359. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France. European Language Resources Association. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mTS: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. Proceedings of the IEEE International Conference on Computer Vision (ICCV).
"	"--- ABSTRACT ---
인상적인 학습 기능을 갖춘 대규모 언어 모델(LLM) 개발의 원동력은 엄청난 모델 크기와 광범위한 교육 데이터 세트입니다. 자연어 처리의 발전과 함께 LLM은 보다 심층적인 조사와 응용을 촉진하기 위해 대중에게 자주 공개되었습니다. 그러나 이러한 LLM, 특히 최근의 최첨단 모델에 대한 교육 데이터 세트의 경우 종종 완전히 공개되지 않습니다. 고성능 LLM에 대한 교육 데이터를 생성하려면 필요한 수준의 품질을 보장하기 위해 광범위한 정리 및 중복 제거가 필요합니다. 따라서 교육 데이터에 대한 투명성이 부족하여 LLM에서 환각 및 편견 문제를 귀속하고 해결하는 연구가 방해를 받아 복제 노력과 커뮤니티의 추가 발전이 방해를 받았습니다. 이러한 과제는 사용 가능한 다국어 텍스트 데이터 세트가 종종 부적절하게 수집 및 정리되는 다국어 학습 시나리오에서 더욱 두드러집니다. 결과적으로 여러 언어로 LLM을 효과적으로 교육할 수 있는 오픈 소스 및 쉽게 사용할 수 있는 데이터 세트가 부족합니다. 이 문제를 해결하기 위해, 우리는 167개 언어로 6.3조 개의 토큰이 있는 상당한 다국어 데이터 세트인 CulturaX를 제시합니다. 이 데이터 세트는 LLM 개발에 맞게 조정되었습니다. 우리의 데이터 세트는 언어 식별, URL 기반 필터링, 메트릭 기반 정리, 문서 정제 및 데이터 중복 제거를 포함하여 모델 학습을 위한 최상의 품질을 달성하기 위해 여러 단계의 엄격한 파이프라인을 통해 세심한 정리 및 중복 제거를 거칩니다. CulturaX는 다국어 LLM의 연구와 발전을 촉진하기 위해 HuggingFace에서 대중에게 완전히 공개되었습니다: https://huggingface.co/datasets/uonlp/CulturaX. 1
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 자연어 처리(NLP) 연구와 응용 프로그램을 근본적으로 변화시켜 수많은 작업에 대한 최첨단 성능을 크게 향상시키고 새로운 떠오르는 능력을 보여주었습니다(Brown et al., 2020; Wei et al., 2022). 변압기 아키텍처(Vaswani et al., 2017)를 기반으로 문헌에서 세 가지 주요 LLM 변형이 탐구되었습니다. 입력 텍스트를 표현 벡터로 인코딩하는 인코더 전용 모델(예: BERT(Devlin et al., 2019) 및 ROBERTa(Liu et al., 2019)), 텍스트를 생성하는 디코더 전용 모델(예: GPT(Radford et al., 2019; Brown et al., 2020)) 그리고 시퀀스 대 시퀀스 생성을 수행하는 인코더-디코더 모델, 예를 들어 BART(Lewis et al., 2020) 및 T5(Raffel et al., 2020). LLM의 놀라운 기능은 주로 모델 크기와 교육 데이터 세트의 끊임없이 확장되는 규모에 의해 추진되었으며, 이는 스케일링 법칙에 의해 최적의 성능을 달성하는 데 필수적인 것으로 간주되었습니다(Hernandez et al., 2022). 예를 들어, 불과 수억 개의 매개변수를 가진 BERT 모델(Devlin et al., 2019)부터 최근의 GPT 기반 모델은 수천억 개의 매개변수를 포함하도록 확장되었습니다(Shoeybi et al., 2019; Scao et al., 2022; Lieber et al., 2021; Chowdhery et al., 2022). 마찬가지로 LLM에 대한 학습 데이터 세트는 기하급수적으로 증가하여 BERT(Devlin et al., 2019; Liu et al., 2019)에 사용된 Wikipedia와 책의 13GB에 불과한 텍스트 데이터에서 Falcon(Penedo et al., 2023), MPT(Mosaic ML, 2023), LLaMa(Touvron et al., 2023), PolyLM(Wei et al., 2023) 및 ChatGPT¹와 같은 최신 모델에 대한 테라바이트 규모의 데이터를 소비하게 되었습니다. 이 분야가 급속히 발전함에 따라 사전 학습된 LLM은 일반적으로 추가 연구와 발전을 촉진하기 위해 대중에게 공개되었습니다. 이러한 모델은 ChatGPT 및 GPT4에서 예시된 것처럼 상용 API를 통해 얻을 수 있거나 Falcon 및 LLaMa에서 예시된 것처럼 오픈 소스 이니셔티브를 통해 얻을 수 있습니다. 그럼에도 불구하고 LLM의 대중적 접근성과는 대조적으로 최첨단 모델을 뒷받침하는 훈련 https://openai.com/blog/chatgpt 데이터 세트는 BLOOM, LLaMa, MPT, Falcon과 같은 오픈 소스 LLM의 경우에도 대부분 엄격하게 보호되는 비밀로 남아 있습니다. 예를 들어 Falcon(Penedo et al., 2023)과 BLOOM(Scao et al., 2022)은 전체 훈련 데이터의 일부만 제공하는 반면 MPT, LLaMa 및 PolyLM의 데이터 세트(Touvron et al., 2023; Wei et al., 2023)는 대중이 접근할 수 없습니다. 한편, 투명성 부족으로 인해 LLM에 대한 심층 분석과 이해가 방해를 받아, 환각, 편견, 독성 콘텐츠(Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021)와 같은 훈련 데이터에서 비롯된 근본적인 문제를 귀속하고 해결하기 위한 중요한 연구가 방해를 받았습니다. 반면에 훈련 데이터를 은폐하면 LLM 개발이 충분한 리소스를 가진 소수의 이해 관계자에게만 제한되어 기술의 민주화와 이점이 제약되고 더 광범위한 사회 내에서 편견이 심화됩니다. 따라서 LLM에 대한 투명성과 민주화를 달성하려면 고성능 LLM을 훈련하기 위한 대규모 고품질 데이터 세트를 만드는 동시에 더 심층적인 연구와 발전을 촉진하기 위해 대중의 접근성을 보장하는 것이 중요합니다. LLM 영역에서 고품질 학습 데이터 세트는 종종 광범위한 데이터 정리 및 중복 제거 프로세스를 적용하여 제작되며, 방대한 텍스트 컬렉션에서 노이즈가 많고 중복된 콘텐츠를 제거하는 것을 목표로 합니다(Allamanis, 2018; Penedo et al., 2023). 이를 위해 커뮤니티에서 LLM을 위한 이러한 오픈소스 데이터 세트를 개발하려는 최근의 노력이 있었습니다. 예를 들어 1.21T 토큰을 가진 RedPajama(Computer, 2023), 627B 토큰을 가진 SlimPajama², 3T 토큰을 가진 AI2 Dolma³가 있습니다. 그러나 LLM을 위한 기존 오픈소스 데이터 세트의 대부분은 영어에 맞게 조정되어 있어 결과 LLM을 비영어 언어, 특히 언어 리소스가 제한적인 언어에 적용할 때 활용 및 성능이 저하됩니다(Bang et al., 2023; Lai et al., 2023). 영어에 대한 이러한 강조는 또한 전 세계적으로 사용되는 7,개 이상의 다양한 언어에 걸쳐 LLM의 연구 과제와 민주화 문제를 포괄적으로 해결하기 위한 오픈 소스 데이터 세트의 용량을 제한합니다. 2https://www.cerebras.net/blog/slimpajama-a-27b-token-cleaned-and-deduplicated-version-of-r edpajama https://blog.allenai.org/dolma-3-trillion-to kens-open-11m-corpus-9a0ff4b8da 동시에 일부 다국어 데이터 세트가 개발되어 제공되어 여러 언어에 대한 텍스트 데이터를 제공합니다. 그럼에도 불구하고 그 품질과 규모는 고성능 LLM을 교육하는 데 필요한 요구 사항을 충족하지 못합니다. 구체적으로, 위키피디아에서 얻은 다국어 텍스트 데이터 세트는 고품질이지만 LLM을 교육하는 데는 비교적 작은 것으로 간주됩니다(Conneau et al., 2020). OSCAR 데이터 세트(Ortiz Suárez et al., 2019; Ortiz Suárez et al., 2020; Abadji et al., 2021, 2022)4는 160개 이상의 언어에 대한 CommonCrawl(CC)에서 텍스트 데이터를 추출합니다. 그러나 이러한 데이터 세트에는 문서 수준 중복 제거(즉, 데이터 세트에서 유사한 문서 제거)가 부족하여 중복 정보가 포함되고 생성 LLM의 성능이 저하됩니다(Lee et al., 2022). 마찬가지로 mC(Xue et al., 2021), CCAligned(Conneau et al., 2020), WikiMatrix(Schwenk et al., 2021), ParaCrawl(Bañón et al., 2020) 데이터 세트는 모두 100개 이상의 언어를 지원하지만 언어 식별 정확도가 낮아 데이터에 노이즈가 발생합니다(Kreutzer et al., 2022). 이러한 데이터 세트는 또한 MinHash(Broder, 1997)를 통해 퍼지 및 문서 수준에서 중복 제거되지 않습니다. 또한 100개 언어에 걸쳐 다국어 XLM-ROBERTa 모델을 훈련하는 데 사용된 CC100 데이터 세트(Wenzek et al., 2020; Conneau et al., 2020)는 2018년 CC 스냅샷만 고려하므로 크기와 고성능 LLM을 훈련하는 데 필요한 최신 정보의 가용성이 제한됩니다. 오픈소스 데이터 세트에 대한 앞서 언급한 문제를 해결하기 위해, 저희의 작업에서는 167개 언어로 LLM을 훈련하기 위한 CulturaX라는 새로운 다국어 데이터 세트를 소개합니다. CulturaX는 mC4의 최신 버전(버전 3.1.0)을 현재 연도까지 사용 가능한 모든 OSCAR 코퍼스와 병합하여 배포판 20.19, 21.09, 22.01 및 23.01을 포함합니다. 이 병합을 통해 27TB의 텍스트 데이터와 6조 개의 토큰으로 구성된 대규모 다국어 데이터 세트가 생성되고 LLM 개발을 위한 최신 데이터를 제공합니다. 저희 데이터 세트의 절반 이상은 영어가 아닌 언어에 전념하여 데이터 크기를 크게 늘리고 다국어 시나리오에서 훈련 모델의 실행 가능성을 향상시킵니다. 중요한 점은 CulturaX가 문서 수준에서 광범위하게 정리되고 중복 제거되어 여러 언어에 대한 LLM을 훈련하는 데 가장 높은 품질을 제공한다는 것입니다. 특히, 저희의 데이터 정리 프로세스에는 저품질 데이터를 제거하기 위해 서명된 포괄적인 파이프라인 dehttps://oscar-project.org가 포함됩니다. 여기에는 노이즈가 있는 텍스트, 비언어적 콘텐츠, 유해한 데이터, 잘못된 언어 식별 등이 제거됩니다. 저희의 데이터 정리 파이프라인은 사분위 범위(IQR) 방법(Dekking et al., 2007)의 변형을 사용하여 다양한 데이터 세트 메트릭(예: 불용어 비율, 데이터 복잡도, 언어 식별 점수)에 적합한 임계값을 선택합니다. 이는 데이터 세트의 노이즈가 있는 이상치를 필터링하는 데 사용할 수 있습니다. 따라서 저희는 대량의 데이터 샘플에 대해 계산된 분포의 백분위수를 활용하여 각 필터링 메트릭과 언어에 대한 임계값 선택 프로세스를 효과적으로 안내합니다. 마지막으로 저희는 근접 중복 제거 방법 MinHashLSH(Broder, 1997; Leskovec et al., 2020)와 URL을 기반으로 데이터 세트 내 언어의 데이터에 대한 광범위한 중복 제거를 수행하여 다국어 LLM을 훈련하기 위한 고품질 데이터를 얻습니다. 저희의 데이터 세트는 다국어 학습을 위한 추가 연구 및 개발을 촉진하기 위해 대중에게 완전히 공개될 것입니다. 저희가 아는 한, CulturaX는 LLM 및 NLP 애플리케이션을 위해 심층적으로 정리되고 중복이 제거된 지금까지 가장 큰 오픈 소스 다국어 데이터 세트입니다. 2 다국어 데이터 세트 생성 LLM을 위한 다국어 공개 데이터 세트를 개발하기 위해 저희의 전략은 저희가 사용할 수 있는 가장 큰 두 다국어 데이터 세트인 mC4(Xue et al., 2021)와 OSCAR(Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022)를 결합하는 것입니다. 그런 다음 정리 및 중복 제거의 두 가지 주요 단계를 포함하는 광범위한 파이프라인으로 데이터를 처리하여 다국어 LLM을 위한 엄청나고 고품질의 데이터 세트를 생성합니다. mC4는 다국어 문서 수준 데이터 세트로, 원래 101개 언어에 대해 다국어 인코더디코더 모델 mT5(Xue et al., 2021)를 학습하기 위해 만들어졌습니다. 이 데이터 세트는 3줄 미만의 긴 줄이 있는 페이지(줄 길이 필터), 잘못된 단어가 있는 페이지, 문서 간에 중복된 줄을 제거하여 CC의 71개 월별 스냅샷에서 추출되었습니다. mC4의 페이지에 대한 언어 식별은 작은 피드포워드 네트워크(Xue et al., 2021)인 cld3 도구(Botha et al., 2017)5에 의해 수행됩니다. 언어 신뢰도가 0.95% 미만인 모든 페이지는 제외됩니다. mC4는 문서 수준에서 정확한 일치로 중복 제거되지만, 퍼지 문서 수준 중복 제거는 수행되지 않습니다. 우리는 mC4의 최신 버전(버전 3.1.0)을 활용합니다.https://github.com/google/cld&quot;https://huggingface.co/datasets/mcOSCAR 23.OSCAR 22.7% OSCAR 21.9% OSCAR 20.7% 11% MC66% 그림 1: 이 작업에서 AllenAI가 준비한 초기 데이터 세트에서 mCand OSCAR의 문서 수 분포. 데이터 세트의 주목할 만한 측면은 CC에서 추출한 선택한 데이터 세트인 mCand OSCAR의 웹 기반 출처와 관련이 있습니다. 이는 LLM을 훈련하기 위해 The Pile(Gao et al., 2020) 및 BookCorpus(Zhu et al., 2015)와 같은 큐레이트된 데이터 세트에 의존했던 이전 작업(Radford et al., 2019; Mosaic ML, 2023; Touvron et al., 2023)과 다릅니다. 더 높은 전반적인 품질. 그러나 다국어 설정의 맥락에서, 우리는 우수한 품질의 큐레이팅된 데이터 세트를 다양한 언어로 사용할 수 없을 수 있으므로 웹 스크래핑된 데이터 세트가 더 적합한 접근 방식이 될 수 있다고 주장합니다. 웹 스크래핑된 데이터를 사용하는 우리의 전략은 여러 언어에서 효율적인 데이터 수집을 용이하게 하여 향상된 교육 데이터 규모에 기여합니다. 더욱이, 최근 연구에서는 웹 스크래핑된 데이터를 정리하여 최첨단 LLM을 생성하는 효과가 입증되었습니다(Raffel et al., 2020; Almazrouei et al., 2023). 총 mC4와 OSCAR의 조합은 추가 처리를 위한 135억 개의 문서를 제공합니다. 그림 1은 우리의 초기 데이터 세트에서 mC4와 사용 가능한 네 가지 버전의 OSCAR에 대한 문서 수의 분포를 보여줍니다. 2.1 데이터 정리 mC4와 OSCAR 데이터세트의 조합을 고려하여, 먼저 언어 식별, ULR 기반 필터링, 메트릭 기반 정리, 문서 정제를 포함하여 데이터에서 노이즈가 많고 잘못된 콘텐츠를 제거하기 위한 포괄적인 데이터 정리 절차를 수행합니다.언어 식별: 특정 문제는 mC와 OSCAR(각각)에 대해 두 가지 다른 언어 식별 도구인 cld3와 FastText를 사용하는 것과 관련이 있습니다.이전 연구에서 cld3가 FastText보다 훨씬 나빠 mC4에 대해 훨씬 더 많은 언어 감지 오류를 유발하는 것으로 나타났습니다(Kreutzer et al., 2022).사실, 다른 여러 언어 감지기와 비교했을 때 FastText는 벤치마크 데이터세트에서 최첨단 성능을 보여주었습니다.이를 위해 첫 번째 데이터 정리 단계에서는 FastText를 적용하여 mC4의 문서에 대한 언어를 다시 예측합니다.예측된 언어가 mC4에서 제공된 언어와 다른 문서는 데이터세트에서 제거됩니다. 그 이유는 언어 감지기 cld3 및 FastText에 혼란을 주는 문서를 피하고, 따라서 데이터에 노이즈가 발생할 가능성이 있기 때문입니다. 마지막으로, 최상의 품질을 보장하기 위해 mC4에서 발견되지만 FastText에서 지원하지 않는 언어에 대한 데이터를 제거합니다. URL 기반 필터링: 다음 단계에서는 알려진 독성 및 유해 출처의 페이지를 제거하여 데이터에서 관련 위험을 줄이는 것을 목표로 합니다. 특히, 툴루즈 대학교에서 제공하는 최신 UT1 URL 및 도메인 블랙리스트를 활용하여 학교 관리자의 인터넷 사용 규제를 지원합니다. 이 목록에는 포르노, 불평, 해킹을 포함하여 LLM 교육에서 삭제해야 하는 다양한 주제의 사이트가 포함됩니다. 일주일에 두세 번 업데이트되는 블랙리스트에는 인간과 로봇(예: 검색 엔진, 알려진 주소 및 인덱스)이 기여한 370만 개 이상의 레코드가 포함됩니다(Abadji et al., 2022). 따라서 블랙리스트의 사이트와 연관된 URL이 일치하는 모든 페이지를 데이터 세트에서 제거합니다. 이 단계는 블랙리스트가 이전에 mCdataset에 사용되지 않았기 때문에 데이터 세트에 유용합니다. 또한 OSCAR가 이미 이 블랙리스트를 데이터 정리에 사용했지만, 우리의 접근 방식은 목록에서 최신 정보를 통합하는데, 이는 현재 OSCAR 배포판에서는 사용할 수 없을 수 있습니다. 지표 기반 정리: BigScience의 BLOOM용 ROOTS 코퍼스의 데이터 처리 파이프라인에서 동기를 얻어 데이터 세트의 품질을 향상시키기 위해(Laurençon et al., 2022; Scao et al., 2022), 다양한 데이터 세트 지표에 대한 분포를 추가로 활용하여 이상 문서를 식별하고 필터링합니다. 각 지표는 데이터 세트 내의 모든 문서에 대해 단일 값 7https://modelpredict.com/ language-identification-survey를 제공하여 각 문서에 대한 number_words, stopword_ratios, perplexity_score와 같은 특정 속성을 정량화합니다. 각 메트릭과 데이터 세트 내의 가능한 값 범위에 대해 임계값을 결정하여 범위를 정상 범위와 비정상 범위의 두 영역으로 분할합니다.비정상 범위는 표준에서 크게 벗어나는 메트릭 값을 보이는 문서에 지정되어 이를 이상치/노이즈로 분류하고 결과적으로 이러한 이상치는 데이터 세트에서 제거됩니다.따라서 아래에 설명된 대로 데이터 세트를 정제하는 데 집합적으로 사용될 포괄적인 데이터 세트 메트릭 배열을 사용합니다.• 단어 수 • • 문자 반복 비율 단어 반복 비율 • 특수 문자 비율 • • 불용어 비율 플래그가 지정된 단어 비율 언어 식별 신뢰도 • 복잡도 점수 • 문서 길이(문자 수) • • 줄 수 • 짧은 줄 길이 비율 짧은 줄 비율 마지막 네 가지 메트릭은 OSCAR 데이터 세트에서 제안하는 반면 다른 메트릭은 OSCAR 데이터를 처리하기 위한 BigScience ROOTS 코퍼스의 파이프라인에서 상속받습니다. 복잡도 점수의 경우 BigScience ROOTS 코퍼스를 따라 SentencePiece 토크나이저(Kudo, 2018)와 KenLM 라이브러리(Heafield, 2011)에서 제공하는 5그램 KneserNey 언어 모델을 Wikipedia의 20230501 덤프를 사용하여 훈련합니다. 이러한 KenLM 모델을 기반으로 높은 복잡도 점수를 표시하는 문서는 Wikipedia 문서와 현저히 다른 것으로 간주됩니다. 이는 데이터 세트에서 제외될 노이즈 수준을 나타냅니다(Wenzek et al., 2020). 토크나이저는 또한 메트릭을 위해 문서의 단어/토큰 수를 얻는 데 사용됩니다. 향후 탐색을 용이하게 하기 위해 HuggingFace에서 KenLM 모델을 공개적으로 릴리스합니다. 크롤링 오류와 낮은 품질의 소스로 인해 웹에서 큐레이팅된 데이터에 반복되는 정보(예: 단어, 문단)가 나타날 수 있으며, 이는 LLM 훈련에 부정적인 결과를 초래할 수 있습니다(Holtzman et al., 2019). 따라서 문자 및 단어 반복 비율은 지나치게 https://huggingface.co/uonlp/kenlm 반복되는 정보가 있는 문서를 피하도록 설계되었습니다. 특수 문자, 불용어 또는 플래그가 지정된 단어의 빈도가 높으면 노이즈가 많고 품질이 낮은 문서를 나타낼 수 있습니다. 따라서 다양한 언어의 불용어 및 플래그가 지정된 단어 목록을 활용하여 문서 제거 비율을 계산합니다. BigScience ROOTS에서 제공하는 13개 언어의 불용어 및 플래그가 지정된 단어 목록 외에도 다른 언어의 이러한 유형의 단어에 대한 사전을 추가로 수집합니다. 다양한 언어의 개인 GitHub 계정에서 공유된 목록을 우선시합니다. 이러한 목록은 종종 모국어 화자가 작성하고 더 높은 품질을 보이기 때문입니다. 또한 언어 식별 신뢰도가 낮으면 데이터에 노이즈가 많은 언어 구조가 있을 수도 있습니다. 따라서 데이터 세트의 각 문서에 대해 FastText가 데이터 필터링을 돕기 위해 해당 언어에 할당하는 확률을 통해 언어 식별 신뢰도를 얻습니다. 마지막으로 짧은 줄 기반 기준의 경우 OSCAR에서 사용하는 것처럼 줄을 짧은 것으로 분류하기 위해 100자의 임계값을 구현합니다. 짧은 줄이 과도하게 나타나는 문서는 데이터 세트에 보관되지 않습니다. 임계값 선택: 데이터 세트 메트릭 집합을 고려할 때 중요한 질문은 각 메트릭과 언어에 적합한 임계값을 선택하여 고품질 다국어 데이터를 생성하는 것입니다. BigScience ROOTS 프로젝트(Laurençon et al., 2022)에서 이 선택 프로세스는 13개 언어의 모국어 화자가 수행합니다. 그 결과 임계값은 나머지 46개 언어에 적용됩니다. 이 프로젝트는 언어당 수천 개의 문서 샘플을 색인화하는 시각화 인터페이스를 제공하여 사용자가 메트릭에 대한 임계값을 조정하면서 데이터 통계를 모니터링할 수 있도록 합니다. 그러나 이 프로세스는 경험이 풍부한 모국어 화자가 필요하기 때문에 다른 언어로 쉽게 확장할 수 없으며, 이로 인해 상당한 비용이 발생합니다. 게다가 제한된 샘플 크기로 인해 전체 데이터 세트에 대해 선택한 임계값의 대표성이 떨어집니다. 분석 결과 BigScience ROOTS 내의 특정 언어에 대해 선택한 일부 임계값이 전체 데이터 세트의 값 범위를 거의 벗어나 해당 메트릭이 비활성화되는 것을 확인했습니다. 이러한 문제를 해결하기 위해 우리는 사분위 범위(IQR) 방법의 변형(Dekking et al., 2007)을 활용하여 데이터 세트의 필터링 메트릭에 적합한 임계값을 선택합니다. 각 메트릭과 언어에 대해 전체 데이터 세트에서 해당 언어의 가능한 값 분포를 생성합니다. 스페인어와 러시아어와 같이 상당한 양의 데이터가 있는 언어의 경우 예외가 있습니다. 이 언어의 경우 데이터의 25%만 사용하여 이러한 분포를 계산합니다. 그런 다음 분포의 Q1 및 Q3 백분위수(Q1 Q3)를 계산하여 필터링 메트릭의 임계값으로 사용합니다. 특히 낮은 Q1 백분위수는 높은 값(예: 언어 식별 신뢰도)을 선호하는 메트릭에 대해 선택되고, 낮은 값(예: 복잡도 점수 및 문서 길이)을 선호하는 메트릭은 높은 Q3 백분위수를 활용합니다. 우리는 (25, 75), (20, 80), (15,85), (10,90), (5, 95)를 고려하여 (Q1, Q3)에 대한 다양한 값을 조사합니다. Q1 = 10 및 Q2 90을 선택하면 검토 대상 언어 샘플에 대한 최상의 데이터 품질을 얻을 수 있습니다. 임계값 선택에 백분위수를 사용하면 BigScience ROOTS 프로젝트에서 사용한 것보다 각 언어에 대해 더 광범위한 데이터 샘플을 효율적으로 추출할 수 있다는 점을 강조할 가치가 있습니다. 이를 통해 다양한 언어에 대한 전체 데이터 세트에 대해 더 신뢰할 수 있는 임계값이 생성됩니다. 특히, 메트릭의 값 분포를 계산하는 데 25%의 데이터 샘플만 사용되는 대규모 언어의 경우, 동일한 선택된 필터링 임계값을 적용할 때 전체 데이터 세트에 대한 삭제된 데이터의 비율이 데이터 샘플의 비율과 밀접하게 일치한다는 것을 관찰했습니다. 이는 방법론을 통해 선택된 임계값의 대표성을 강조합니다. 마지막으로, 주어진 언어의 메트릭에 대한 임계값이 결정되면 메트릭의 임계값을 초과하고 데이터의 불리한 범위에 들어가는 모든 문서를 제거합니다.문서 정제: 이전 정리 단계는 데이터 세트 수준에서 수행되며, 데이터 세트에서 품질이 낮은 문서를 제거하는 것을 목표로 합니다.이 단계에서는 보관된 문서를 추가로 정리하여 품질을 개선합니다.이전 메트릭 기반 필터링 단계는 노이즈가 많은 문서를 제거하는 데 중요한 역할을 하며, 이를 통해 이 단계에서 효과적인 문서 정리 규칙을 개발하는 프로세스가 간소화된다는 점에 유의하는 것이 중요합니다.특히, mC4와 OSCAR의 문서는 인터넷에서 크롤링된 HTML 페이지에서 추출되므로 상당 부분에 긴 JavaScript 줄과 외부 콘텐츠를 포함한 크롤링 및 추출 오류가 있을 수 있습니다.결과적으로 이러한 문서를 필터링하면 데이터 세트 내의 문서를 정리하기 위한 규칙을 설계하는 작업이 크게 간소화됩니다.따라서 각 문서에 대해 일련의 작업을 통해 노이즈가 많거나 관련성이 없는 부분을 제거합니다. 첫째, 각 문서의 끝에 있는 짧은 줄을 제거합니다. 이러한 줄에는 일반적으로 푸터 세부 정보나 웹사이트의 도움이 되지 않는 정보가 들어 있기 때문입니다. 둘째, JavaScript(JS) 키워드 목록에서 단어가 포함된 줄을 제거합니다(예: &quot;
--- RELATED WORK ---
영어: 다른 NLP 작업과 비교했을 때 언어 모델은 레이블이 지정되지 않은 데이터로 학습할 수 있으므로 효율적인 데이터 수집을 통해 https://github.com/ChenghaoMou/text-dedup/ tree/main에 대한 거대한 규모를 생성할 수 있습니다. #문서(M) #토큰 코드 언어 URL 초기 필터링 메트릭 필터링 en 영어 5783.5766.3586.MinHash URL 중복 제거 중복 제거 3308.30 3241.Filtering Rate (%) (B) (%) ru 러시아어 1431.1429.922.es 스페인어 844.842.530.de 독일어 863.861.515.845.64 799.479.65 450.447.06 420.fr 프랑스어 711.709.439.387.zh 중국어 444.444.258.it 이탈리아어 406.406.254.pt 포르투갈어 347.346.217.363.222.37 218.226.42 211.200.11 190.43.44.16 737.46.60 373.51.34 357.48.89 319.2846.97 45.11.5.5.5.50.80 227.48.06 165.3.2.45.24 136.2.pl 폴란드어 270.269.170.151.71 142.47.37 117.1.ja 일본어 247.247.137.114.64 111.55.11 107.1.vi 베트남어 182.182.118.108.102.44.98.1.nl 네덜란드어 238.238.148.125.117.50.80.1.ar 아랍어 132.132.84.77.74.44.69.1.tr 터키어 183.183.109.99.94.48.64.1.CS 체코어 136.136.80.69.65.52.56.0.fa 페르시아어 118.118.70.62.59.49.45.0.hu 헝가리어 88.88.53.46.44.50.43.0.el 그리스어 100.100.61.54.51.48.43.0.ro 루마니아어 89.89.45.42.40.54.39.0.SV 스웨덴어 103.102.58.52.49.51.38.0.uk 우크라이나어 81.81.50.47.44.45.38.0.fi 핀란드어 59.59.36.32.30.49.28.0.ko 한국어 46.45.25.21.20.55.24.0.da 덴마크어 53.52.28.26.25.52.22.0.bg 불가리아어 47.46.28.25.24.48.22.0.no 노르웨이어 40.40.20.19.18.52.18.0.hi 힌디어 35.35.22.20.19.44.16.0.sk 슬로바키아어 40.39.22.19.18.53.16.0.th 태국어 49.48.26.21.20.57.15.0.lt 리투아니아어 27.27.15.14.13.50.14.0.са 카탈로니아어 31.31.18.16.15.50.12.0.id 인도네시아어 48.48.25.23.23.51.12.0.bn 방글라어 20.20.13.13.12.40.9.0.et 에스토니아어 16.16.9.8.8.50.8.0.sl 슬로베니아어 15.15.8.7.7.52.8.0.lv 라트비아어 14.14.8.7.7.49.7.0.he 히브리어 10.10.5.4.4.56.4.0.sr 세르비아어 7.7.4.4.4.48.4.0.ta 타밀어 8.8.5.4.4.46.4.0.sq 알바니아어 9.9.5.5.5.44.3.0.az 아제르바이잔어 9.9.5.5.5.47.3.0.전체(42개 언어) 13397.13366.8254.7471.7181.46.6267.99.전체(167개 언어) 13506.76 13474.94 8308.7521.7228.46.48 6308.42 100.표 1: 데이터 통계 데이터 세트에서 토큰 비율이 0.05%를 넘는 42개 언어. &quot;#Documents (M)&quot; 레이블로 그룹화된 열은 해당 정리 및 중복 제거 단계 이후 각 언어의 문서 수를 나타냅니다. 토큰 수는 최종 데이터 세트(즉, 모든 정리 및 중복 제거 단계 이후)를 기반으로 합니다. 훈련 데이터. LLM 훈련에 일반적으로 사용되는 두 가지 주요 데이터 유형은 큐레이트된 데이터와 웹 크롤링 데이터입니다. 큐레이션된 데이터는 일반적으로 타깃 소스 및 도메인에서 잘 작성되고 잘 형식화된 텍스트로 구성됩니다. 예를 들어, 위키피디아 기사, 책, 뉴스 기사 및 과학 논문은 &quot;The Pile&quot;(Gao et al., 2020) 및 &quot;BookCorpus&quot;(Zhu et al., 2015) 데이터 세트에 사용됩니다. 반면, 웹 크롤링 데이터는 블로그, 소셜 미디어 게시물, 뉴스 기사 및 광고와 같이 형식 및 쓰기 스타일 측면에서 상당히 다양한 인터넷의 광범위한 소스에서 수집된 텍스트를 포함합니다. CommonCrawl(CC)은 12년 동안 인터넷에서 페타바이트 규모의 데이터를 수집한 널리 사용되는 웹 크롤링 저장소입니다. 이를 위해 큐레이션된 데이터는 종종 더 높은 품질을 가지고 있다고 간주되며, 이로 인해 BERT(Devlin et al., 2019) 및 GPT-2(Radford et al., 2019)와 같은 초기 LLM을 훈련하는 데 선호되었습니다. 그러나 더 큰 모델에 대한 수요가 성장함에 따라 웹 크롤링 데이터는 최근 LLM의 학습 데이터에 상당 부분을 기여하기 때문에 더 많은 주목을 받고 있습니다.예를 들어 ROBERTA(Liu 등, 2019), BART(Lewis 등, 2020), T5(Raffel 등, 2020), GPT-3(Rae 등, 2021), LLaMa(Touvron 등, 2023), MPT(MosaicML, 2023), Falcon(Almazrouei 등, 2023) 등이 있습니다. 따라서 이러한 LLM을 학습하기 위해 C4(Raffel 등, 2020), CC-News(Nagel), STORIES(Trinh and Le, 2018)를 포함하여 다양한 CC 추출물이 생성되었습니다. 학습 데이터의 접근성과 관련하여, 초기 LLM을 학습하는 데 사용된 데이터 세트는 종종 대중에게 공개됩니다(Devlin et al., 2019; Raffel et al., 2020). 그러나 가장 최근의 최첨단(SOTA) 생성 LLM의 경우, 학습 데이터 세트가 상업적 이익으로 인해 완전히 공개되지 않습니다. 이는 ChatGPT 및 GPT-4와 같은 독점 모델뿐만 아니라 LLaMa, MPT, Falcon, BLOOM과 같이 오픈소스 모델이라고 주장하는 모델에도 적용됩니다(Scao et al., 2022). 기존 LLM의 투명성 문제를 해결하기 위해, 최첨단 LLM, 즉 RedPajama(Computer, 2023), SlimPajama, AIDolma에 대한 학습 데이터 세트를 복제하여 공개하려는 최근의 노력이 있었습니다. 이러한 데이터 세트의 주요 차이점은 LLM 교육을 위한 고품질을 보장하기 위해 세심하게 정리되고 문서 수준에서 중복 제거된 대규모 텍스트 데이터와 관련이 있습니다. 그럼에도 불구하고 이러한 오픈소스 데이터 세트의 일반적인 단점은 주로 영어 데이터에 초점을 맞추고 다른 언어에 대한 데이터는 제한적이라는 것입니다. LLM 교육을 위한 다국어 대규모 데이터 세트를 얻으려면 CC와 같은 웹 스크레이프 데이터 세트를 활용하여 여러 언어로 최신 정보를 사용하여 효율적인 데이터 수집을 수행하는 것이 더 편리합니다. 또한 고성능 LLM의 고품질을 보장하려면 노이즈가 많고 관련성이 없는 콘텐츠(예: 품질이 낮은 기계 생성 텍스트 및 성인 콘텐츠)를 피하기 위해 다국어 데이터를 광범위하게 정리하고 중복 제거해야 합니다(Trinh 및 Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). 따라서 고품질 데이터 세트를 생성하기 위한 일반적인 데이터 처리 파이프라인에는 FastText(Joulin 등, 2016), CC-Net(Wenzek 등, 2020), BLOOM 모델을 위한 BigScience ROOTS 코퍼스(Laurençon 등, 2022; Scao 등, 2022), Falcon 모델을 위한 RefinedWeb 데이터 세트(Penedo 등, 2023; Almazrouei 등, 2023) 및 LLaMa 모델을 훈련하기 위한 데이터 세트(Touvron 등, 2023)에서 보여준 것처럼 여러 단계가 포함될 수 있습니다. 첫 번째 단계는 이러한 파이프라인에서 언어를 식별하여 데이터를 해당 언어에 적절히 할당하는 것이 필요합니다(Joulin 등, 2016). 다음 단계에서는 특수 문자, 짧은 줄, 나쁜 단어 등의 비율에 따라 바람직하지 않은 콘텐츠를 필터링하기 위한 다양한 데이터 세트별 규칙과 휴리스틱이 특징입니다(Grave et al., 2018; Laurençon et al., 2022). 또한 데이터는 가벼운 모델(예: KenLM 언어 모델(Heafield, 2011))을 통해 필터링하여 노이즈가 많은 문서를 피할 수 있습니다(Wenzek et al., 2020). 마지막으로 데이터 중복 제거를 수행하여 유사하거나 반복되는 정보를 제거해야 합니다(Laurençon et al., 2022; Penedo et al., 2023). 이와 관련하여 중요한 단계는 문서 수준에서 퍼지 중복 제거를 포함하는데, 예를 들어 MinHash(Broder, 1997)를 통해 유사한 문서를 제거하고, 이를 통해 기억을 완화하고 결과 LLM에 대한 일반화를 개선하는 것입니다(Lee et al., 2022). 이를 위해 mC4(Xue et al., 2021), OSCAR(Ortiz Suárez et al., 2019), CC100(Wenzek et al., 2020; Conneau et al., 2020) 및 BigScience ROOT 코퍼스(Laurençon et al., 2022)와 같이 여러 언어로 된 텍스트 데이터가 있는 다국어 오픈소스 데이터 세트가 있지만, 그 품질과 규모는 특히 GPT와 같은 생성 모델의 경우 LLM을 효과적으로 훈련하는 데 필요한 요구 사항을 충족하지 못합니다. 예를 들어, 서론에서 강조했듯이 mC4와 OSCAR는 모두 문서 수준의 데이터에 대한 퍼지 중복 제거가 부족합니다. mCalso는 cld3를 사용하기 때문에 언어 식별이 좋지 않습니다. BigScience ROOTS는 46개 언어에 대한 작은 샘플 데이터만 제공하는 반면 CC100은 2018년 이후의 정보가 없습니다. 따라서 저희 데이터 세트 CulturaX는 기존 데이터 세트의 문제를 포괄적으로 해결하여 LLM을 훈련하는 데 쉽게 사용할 수 있고 고품질의 데이터를 제공하는 다국어, 오픈 소스, 대규모 데이터 세트를 제공합니다. 5 결론 저희는 167개 언어에 대한 텍스트 데이터가 있는 새로운 다국어 데이터 세트인 CulturaX를 제시합니다. 저희 데이터 세트는 포괄적인 파이프라인을 통해 정리되고 중복 제거되어 6.3조 개의 토큰을 생성합니다. 따라서 CulturaX는 여러 언어에 대한 고성능 LLM을 훈련하는 데 쉽게 사용할 수 있는 대규모 고품질 데이터 세트입니다. 저희 데이터는 다국어 학습에 대한 추가 연구와 응용 프로그램을 촉진하기 위해 대중이 공개적으로 접근할 수 있습니다. 참고 문헌 Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, Benoît Sagot. 2022. 더 깨끗한 문서 지향 다국어 크롤링 코퍼스를 향해. 제13회 언어 자원 및 평가 컨퍼런스 회의록, 4344-4355페이지, 프랑스 마르세유. 유럽 언어 자원 협회. Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, Benoît Sagot. 2021. Ungoliant: 매우 대규모 다국어 웹 코퍼스 생성을 위한 최적화된 파이프라인. 대규모 코퍼스 관리의 과제에 대한 워크숍 회의록(CMLC-9) 2021. 리머릭, 2021년 7월 12일(온라인 이벤트). Miltiadis Allamanis. 2018. 코드의 기계 학습 모델에서 코드 중복의 부정적 영향. 2019년 ACM SIGPLAN 새로운 아이디어, 새로운 패러다임, 프로그래밍 및 소프트웨어에 대한 성찰에 대한 국제 심포지엄 회의록. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi et al. 2023. Falcon-40B: 최첨단 성능을 갖춘 개방형 대규모 언어 모델. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung. 2023. 추론, 환각 및 상호 작용에 대한 chatgpt의 멀티태스크, 다국어, 멀티모달 평가. ArXiv, abs/2302.04023. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William 웨이츠, 디온 위긴스, 하우메 사라고사. 2020. ParaCrawl: 웹 규모의 병렬 말뭉치 획득. 전산언어학협회 제58차 연차총회 진행, 4555-4567페이지, 온라인. 전산언어학협회. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli 등. 2021. 기초 모델의 기회와 위험. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov. 2017. 소규모 피드포워드 네트워크를 사용한 자연어 처리. 경험적 연구 컨퍼런스 회의록에서
--- METHOD ---
(Dekking et al., 2007) 다양한 데이터 세트 메트릭(예: 불용어 비율, 데이터 복잡도, 언어 식별 점수)에 적합한 임계값을 선택하여 데이터 세트의 노이즈 이상치를 필터링하는 데 사용할 수 있습니다. 따라서 대량의 데이터 샘플에서 계산된 분포의 백분위수를 활용하여 각 필터링 메트릭과 언어에 대한 임계값 선택 프로세스를 효과적으로 안내합니다. 마지막으로 MinHashLSH(Broder, 1997; Leskovec et al., 2020)의 근접 중복 제거 방법과 URL을 기반으로 데이터 세트 내 언어의 데이터에 대한 광범위한 중복 제거를 수행하여 다국어 LLM을 훈련하는 데 사용할 수 있는 고품질 데이터를 제공합니다. 데이터 세트는 다국어 학습을 위한 추가 연구 개발을 촉진하기 위해 대중에게 완전히 공개됩니다. 저희가 아는 한, CulturaX는 LLM 및 NLP 애플리케이션을 위해 심층적으로 정리되고 중복 제거된 지금까지 가장 큰 오픈 소스 다국어 데이터 세트입니다. 2 다국어 데이터 세트 생성 LLM을 위한 다국어 공개 데이터 세트를 개발하기 위해, 우리의 전략은 우리가 사용할 수 있는 가장 큰 두 다국어 데이터 세트인 mC4(Xue et al., 2021)와 OSCAR(Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022)를 결합하는 것입니다. 그런 다음 정리와 중복 제거의 두 가지 주요 단계를 포함하는 광범위한 파이프라인으로 데이터를 처리하여 다국어 LLM을 위한 엄청나고 고품질의 데이터 세트를 생성합니다. mC4는 다국어 문서 수준 데이터 세트로, 원래 101개 언어에 대한 다국어 인코더디코더 모델 mT5(Xue et al., 2021)를 학습하기 위해 만들어졌습니다. 이 데이터 세트는 3줄 미만의 긴 줄이 있는 페이지(줄 길이 필터), 잘못된 단어가 있는 페이지 및 문서 간에 중복된 줄을 제거하여 CC의 71개 월별 스냅샷에서 추출되었습니다. mC4의 페이지에 대한 언어 식별은 cld3 도구(Botha et al., 2017)5에 의해 수행되는데, 이는 작은 피드포워드 네트워크(Xue et al., 2021)입니다. 언어 신뢰도가 0.95% 미만인 모든 페이지는 제외됩니다. mC4는 문서 수준에서 정확한 일치로 중복 제거되지만, 퍼지 문서 수준 중복 제거는 수행되지 않습니다. 우리는 mC4의 최신 버전(버전 3.1.0)을 활용합니다.https://github.com/google/cld&quot;https://huggingface.co/datasets/mcOSCAR 23.OSCAR 22.7% OSCAR 21.9% OSCAR 20.7% 11% MC66% 그림 1: 이 작업에서 AllenAI가 준비한 초기 데이터 세트에서 mCand OSCAR의 문서 수 분포. 데이터 세트의 주목할 만한 측면은 CC에서 추출한 선택한 데이터 세트인 mCand OSCAR의 웹 기반 출처와 관련이 있습니다. 이는 LLM을 훈련하기 위해 The Pile(Gao et al., 2020) 및 BookCorpus(Zhu et al., 2015)와 같은 큐레이트된 데이터 세트에 의존했던 이전 작업(Radford et al., 2019; Mosaic ML, 2023; Touvron et al., 2023)과 다릅니다. 더 높은 전반적인 품질. 그러나 다국어 설정의 맥락에서, 우리는 우수한 품질의 큐레이팅된 데이터 세트를 다양한 언어로 사용할 수 없을 수 있으므로 웹 스크래핑된 데이터 세트가 더 적합한 접근 방식이 될 수 있다고 주장합니다. 웹 스크래핑된 데이터를 사용하는 우리의 전략은 여러 언어에서 효율적인 데이터 수집을 용이하게 하여 향상된 교육 데이터 규모에 기여합니다. 더욱이, 최근 연구에서는 웹 스크래핑된 데이터를 정리하여 최첨단 LLM을 생성하는 효과가 입증되었습니다(Raffel et al., 2020; Almazrouei et al., 2023). 총 mC4와 OSCAR의 조합은 추가 처리를 위한 135억 개의 문서를 제공합니다. 그림 1은 우리의 초기 데이터 세트에서 mC4와 사용 가능한 네 가지 버전의 OSCAR에 대한 문서 수의 분포를 보여줍니다. 2.1 데이터 정리 mC4와 OSCAR 데이터세트의 조합을 고려하여, 먼저 언어 식별, ULR 기반 필터링, 메트릭 기반 정리, 문서 정제를 포함하여 데이터에서 노이즈가 많고 잘못된 콘텐츠를 제거하기 위한 포괄적인 데이터 정리 절차를 수행합니다.언어 식별: 특정 문제는 mC와 OSCAR(각각)에 대해 두 가지 다른 언어 식별 도구인 cld3와 FastText를 사용하는 것과 관련이 있습니다.이전 연구에서 cld3가 FastText보다 훨씬 나빠 mC4에 대해 훨씬 더 많은 언어 감지 오류를 유발하는 것으로 나타났습니다(Kreutzer et al., 2022).사실, 다른 여러 언어 감지기와 비교했을 때 FastText는 벤치마크 데이터세트에서 최첨단 성능을 보여주었습니다.이를 위해 첫 번째 데이터 정리 단계에서는 FastText를 적용하여 mC4의 문서에 대한 언어를 다시 예측합니다.예측된 언어가 mC4에서 제공된 언어와 다른 문서는 데이터세트에서 제거됩니다. 그 이유는 언어 감지기 cld3 및 FastText에 혼란을 주는 문서를 피하고, 따라서 데이터에 노이즈가 발생할 가능성이 있기 때문입니다. 마지막으로, 최상의 품질을 보장하기 위해 mC4에서 발견되지만 FastText에서 지원하지 않는 언어에 대한 데이터를 제거합니다. URL 기반 필터링: 다음 단계에서는 알려진 독성 및 유해 출처의 페이지를 제거하여 데이터에서 관련 위험을 줄이는 것을 목표로 합니다. 특히, 툴루즈 대학교에서 제공하는 최신 UT1 URL 및 도메인 블랙리스트를 활용하여 학교 관리자의 인터넷 사용 규제를 지원합니다. 이 목록에는 포르노, 불평, 해킹을 포함하여 LLM 교육에서 삭제해야 하는 다양한 주제의 사이트가 포함됩니다. 일주일에 두세 번 업데이트되는 블랙리스트에는 인간과 로봇(예: 검색 엔진, 알려진 주소 및 인덱스)이 기여한 370만 개 이상의 레코드가 포함됩니다(Abadji et al., 2022). 따라서 블랙리스트의 사이트와 연관된 URL이 일치하는 모든 페이지를 데이터 세트에서 제거합니다. 이 단계는 블랙리스트가 이전에 mCdataset에 사용되지 않았기 때문에 데이터 세트에 유용합니다. 또한 OSCAR가 이미 이 블랙리스트를 데이터 정리에 사용했지만, 우리의 접근 방식은 목록에서 최신 정보를 통합하는데, 이는 현재 OSCAR 배포판에서는 사용할 수 없을 수 있습니다. 지표 기반 정리: BigScience의 BLOOM용 ROOTS 코퍼스의 데이터 처리 파이프라인에서 동기를 얻어 데이터 세트의 품질을 향상시키기 위해(Laurençon et al., 2022; Scao et al., 2022), 다양한 데이터 세트 지표에 대한 분포를 추가로 활용하여 이상 문서를 식별하고 필터링합니다. 각 지표는 데이터 세트 내의 모든 문서에 대해 단일 값 7https://modelpredict.com/ language-identification-survey를 제공하여 각 문서에 대한 number_words, stopword_ratios, perplexity_score와 같은 특정 속성을 정량화합니다. 각 메트릭과 데이터 세트 내의 가능한 값 범위에 대해 임계값을 결정하여 범위를 정상 범위와 비정상 범위의 두 영역으로 분할합니다.비정상 범위는 표준에서 크게 벗어나는 메트릭 값을 보이는 문서에 지정되어 이를 이상치/노이즈로 분류하고 결과적으로 이러한 이상치는 데이터 세트에서 제거됩니다.따라서 아래에 설명된 대로 데이터 세트를 정제하는 데 집합적으로 사용될 포괄적인 데이터 세트 메트릭 배열을 사용합니다.• 단어 수 • • 문자 반복 비율 단어 반복 비율 • 특수 문자 비율 • • 불용어 비율 플래그가 지정된 단어 비율 언어 식별 신뢰도 • 복잡도 점수 • 문서 길이(문자 수) • • 줄 수 • 짧은 줄 길이 비율 짧은 줄 비율 마지막 네 가지 메트릭은 OSCAR 데이터 세트에서 제안하는 반면 다른 메트릭은 OSCAR 데이터를 처리하기 위한 BigScience ROOTS 코퍼스의 파이프라인에서 상속받습니다. 복잡도 점수의 경우 BigScience ROOTS 코퍼스를 따라 SentencePiece 토크나이저(Kudo, 2018)와 KenLM 라이브러리(Heafield, 2011)에서 제공하는 5그램 KneserNey 언어 모델을 Wikipedia의 20230501 덤프를 사용하여 훈련합니다. 이러한 KenLM 모델을 기반으로 높은 복잡도 점수를 표시하는 문서는 Wikipedia 문서와 현저히 다른 것으로 간주됩니다. 이는 데이터 세트에서 제외될 노이즈 수준을 나타냅니다(Wenzek et al., 2020). 토크나이저는 또한 메트릭을 위해 문서의 단어/토큰 수를 얻는 데 사용됩니다. 향후 탐색을 용이하게 하기 위해 HuggingFace에서 KenLM 모델을 공개적으로 릴리스합니다. 크롤링 오류와 낮은 품질의 소스로 인해 웹에서 큐레이팅된 데이터에 반복되는 정보(예: 단어, 문단)가 나타날 수 있으며, 이는 LLM 훈련에 부정적인 결과를 초래할 수 있습니다(Holtzman et al., 2019). 따라서 문자 및 단어 반복 비율은 지나치게 https://huggingface.co/uonlp/kenlm 반복되는 정보가 있는 문서를 피하도록 설계되었습니다. 특수 문자, 불용어 또는 플래그가 지정된 단어의 빈도가 높으면 노이즈가 많고 품질이 낮은 문서를 나타낼 수 있습니다. 따라서 다양한 언어의 불용어 및 플래그가 지정된 단어 목록을 활용하여 문서 제거 비율을 계산합니다. BigScience ROOTS에서 제공하는 13개 언어의 불용어 및 플래그가 지정된 단어 목록 외에도 다른 언어의 이러한 유형의 단어에 대한 사전을 추가로 수집합니다. 다양한 언어의 개인 GitHub 계정에서 공유된 목록을 우선시합니다. 이러한 목록은 종종 모국어 화자가 작성하고 더 높은 품질을 보이기 때문입니다. 또한 언어 식별 신뢰도가 낮으면 데이터에 노이즈가 많은 언어 구조가 있을 수도 있습니다. 따라서 데이터 세트의 각 문서에 대해 FastText가 데이터 필터링을 돕기 위해 해당 언어에 할당하는 확률을 통해 언어 식별 신뢰도를 얻습니다. 마지막으로 짧은 줄 기반 기준의 경우 OSCAR에서 사용하는 것처럼 줄을 짧은 것으로 분류하기 위해 100자의 임계값을 구현합니다. 짧은 줄이 과도하게 나타나는 문서는 데이터 세트에 보관되지 않습니다. 임계값 선택: 데이터 세트 메트릭 집합을 고려할 때 중요한 질문은 각 메트릭과 언어에 적합한 임계값을 선택하여 고품질 다국어 데이터를 생성하는 것입니다. BigScience ROOTS 프로젝트(Laurençon et al., 2022)에서 이 선택 프로세스는 13개 언어의 모국어 화자가 수행합니다. 그 결과 임계값은 나머지 46개 언어에 적용됩니다. 이 프로젝트는 언어당 수천 개의 문서 샘플을 색인화하는 시각화 인터페이스를 제공하여 사용자가 메트릭에 대한 임계값을 조정하면서 데이터 통계를 모니터링할 수 있도록 합니다. 그러나 이 프로세스는 경험이 풍부한 모국어 화자가 필요하기 때문에 다른 언어로 쉽게 확장할 수 없으며, 이로 인해 상당한 비용이 발생합니다. 게다가 제한된 샘플 크기로 인해 전체 데이터 세트에 대해 선택한 임계값의 대표성이 떨어집니다. 분석 결과 BigScience ROOTS 내의 특정 언어에 대해 선택한 일부 임계값이 전체 데이터 세트의 값 범위를 거의 벗어나 해당 메트릭이 비활성화되는 것을 확인했습니다. 이러한 문제를 해결하기 위해 우리는 사분위 범위(IQR) 방법의 변형(Dekking et al., 2007)을 활용하여 데이터 세트의 필터링 메트릭에 적합한 임계값을 선택합니다. 각 메트릭과 언어에 대해 전체 데이터 세트에서 해당 언어의 가능한 값 분포를 생성합니다. 스페인어와 러시아어와 같이 상당한 양의 데이터가 있는 언어의 경우 예외가 있습니다. 이 언어의 경우 데이터의 25%만 사용하여 이러한 분포를 계산합니다. 그런 다음 분포의 Q1 및 Q3 백분위수(Q1 Q3)를 계산하여 필터링 메트릭의 임계값으로 사용합니다. 특히 낮은 Q1 백분위수는 높은 값(예: 언어 식별 신뢰도)을 선호하는 메트릭에 대해 선택되고, 낮은 값(예: 복잡도 점수 및 문서 길이)을 선호하는 메트릭은 높은 Q3 백분위수를 활용합니다. 우리는 (25, 75), (20, 80), (15,85), (10,90), (5, 95)를 고려하여 (Q1, Q3)에 대한 다양한 값을 조사합니다. Q1 = 10 및 Q2 90을 선택하면 검토 대상 언어 샘플에 대한 최상의 데이터 품질을 얻을 수 있습니다. 임계값 선택에 백분위수를 사용하면 BigScience ROOTS 프로젝트에서 사용한 것보다 각 언어에 대해 더 광범위한 데이터 샘플을 효율적으로 추출할 수 있다는 점을 강조할 가치가 있습니다. 이를 통해 다양한 언어에 대한 전체 데이터 세트에 대해 더 신뢰할 수 있는 임계값이 생성됩니다. 특히, 메트릭의 값 분포를 계산하는 데 25%의 데이터 샘플만 사용되는 대규모 언어의 경우, 동일한 선택된 필터링 임계값을 적용할 때 전체 데이터 세트에 대한 삭제된 데이터의 비율이 데이터 샘플의 비율과 밀접하게 일치한다는 것을 관찰했습니다. 이는 방법론을 통해 선택된 임계값의 대표성을 강조합니다. 마지막으로, 주어진 언어의 메트릭에 대한 임계값이 결정되면 메트릭의 임계값을 초과하고 데이터의 불리한 범위에 들어가는 모든 문서를 제거합니다. 문서 정제: 이전 정리 단계는 데이터 세트 수준에서 수행되며, 데이터 세트에서 품질이 낮은 문서를 제거하는 것을 목표로 합니다. 이 단계에서는 보관된 문서를 추가로 정리하여 품질을 개선합니다. 이전 메트릭 기반 필터링 단계는 노이즈가 많은 문서를 제거하는 데 중요한 역할을 하며, 이를 통해 이 단계에서 효과적인 문서 정리 규칙을 개발하는 프로세스가 간소화된다는 점에 유의하는 것이 중요합니다. 특히 mC4와 OSCAR의 문서는 인터넷에서 크롤링한 HTML 페이지에서 추출되므로 상당 부분에 긴 JavaScript 줄과 외부 콘텐츠를 포함한 크롤링 및 추출 오류가 있을 수 있습니다. 결과적으로 이러한 문서를 필터링하면 데이터 세트 내의 문서를 정리하기 위한 규칙을 설계하는 작업이 크게 간소화됩니다. 따라서 각 문서에 대해 일련의 작업을 통해 노이즈가 많거나 관련성이 없는 부분을 제거합니다. 첫째, 각 문서의 끝에 있는 짧은 줄을 제거합니다. 이러한 줄에는 일반적으로 푸터 세부 정보나 웹사이트의 도움이 되지 않는 정보가 들어 있기 때문입니다. 둘째, JavaScript(JS) 키워드 목록에서 단어가 포함된 줄을 제거합니다(예: &quot;
--- EXPERIMENT ---
s 모든 정리 및 중복 제거 단계를 완료한 후, 최종 데이터 세트는 167개 언어에 걸쳐 6.3조 개의 토큰으로 구성됩니다. 표 1은 각 처리 단계에 따른 CulturaX의 상위 42개 언어에 대한 문서 및 토큰 수에 대한 개요를 제공합니다. 알 수 있듯이, 데이터 정리 파이프라인은 각 언어에 대한 원래 mC4 및 OSCAR 데이터 세트의 문서 수를 상당히 줄일 수 있습니다. 제거된 문서의 총 수는 초기 문서의 46.48%를 차지하며, 이는 다국어 데이터 세트에 대한 노이즈 정보를 필터링하는 접근 방식의 효과를 시사합니다. 4 관련 작업 다른 NLP 작업과 비교했을 때 언어 모델은 레이블이 지정되지 않은 데이터로 학습할 수 있으므로 효율적인 데이터 수집을 통해 https://github.com/ChenghaoMou/text-dedup/ tree/main에 대한 거대한 규모를 생성할 수 있습니다. #문서(M) #토큰 코드 언어 URL 초기 필터링 메트릭 필터링 en 영어 5783.5766.3586.MinHash URL 중복 제거 중복 제거 3308.30 3241.Filtering Rate (%) (B) (%) ru 러시아어 1431.1429.922.es 스페인어 844.842.530.de 독일어 863.861.515.845.64 799.479.65 450.447.06 420.fr 프랑스어 711.709.439.387.zh 중국어 444.444.258.it 이탈리아어 406.406.254.pt 포르투갈어 347.346.217.363.222.37 218.226.42 211.200.11 190.43.44.16 737.46.60 373.51.34 357.48.89 319.2846.97 45.11.5.5.5.50.80 227.48.06 165.3.2.45.24 136.2.pl 폴란드어 270.269.170.151.71 142.47.37 117.1.ja 일본어 247.247.137.114.64 111.55.11 107.1.vi 베트남어 182.182.118.108.102.44.98.1.nl 네덜란드어 238.238.148.125.117.50.80.1.ar 아랍어 132.132.84.77.74.44.69.1.tr 터키어 183.183.109.99.94.48.64.1.CS 체코어 136.136.80.69.65.52.56.0.fa 페르시아어 118.118.70.62.59.49.45.0.hu 헝가리어 88.88.53.46.44.50.43.0.el 그리스어 100.100.61.54.51.48.43.0.ro 루마니아어 89.89.45.42.40.54.39.0.SV 스웨덴어 103.102.58.52.49.51.38.0.uk 우크라이나어 81.81.50.47.44.45.38.0.fi 핀란드어 59.59.36.32.30.49.28.0.ko 한국어 46.45.25.21.20.55.24.0.da 덴마크어 53.52.28.26.25.52.22.0.bg 불가리아어 47.46.28.25.24.48.22.0.no 노르웨이어 40.40.20.19.18.52.18.0.hi 힌디어 35.35.22.20.19.44.16.0.sk 슬로바키아어 40.39.22.19.18.53.16.0.th 태국어 49.48.26.21.20.57.15.0.lt 리투아니아어 27.27.15.14.13.50.14.0.са 카탈로니아어 31.31.18.16.15.50.12.0.id 인도네시아어 48.48.25.23.23.51.12.0.bn 방글라어 20.20.13.13.12.40.9.0.et 에스토니아어 16.16.9.8.8.50.8.0.sl 슬로베니아어 15.15.8.7.7.52.8.0.lv 라트비아어 14.14.8.7.7.49.7.0.he 히브리어 10.10.5.4.4.56.4.0.sr 세르비아어 7.7.4.4.4.48.4.0.ta 타밀어 8.8.5.4.4.46.4.0.sq 알바니아어 9.9.5.5.5.44.3.0.az 아제르바이잔어 9.9.5.5.5.47.3.0.전체(42개 언어) 13397.13366.8254.7471.7181.46.6267.99.전체(167개 언어) 13506.76 13474.94 8308.7521.7228.46.48 6308.42 100.표 1: 데이터 통계 데이터 세트에서 토큰 비율이 0.05%를 넘는 42개 언어. &quot;#Documents (M)&quot; 레이블로 그룹화된 열은 해당 정리 및 중복 제거 단계 이후 각 언어의 문서 수를 나타냅니다. 토큰 수는 최종 데이터 세트(즉, 모든 정리 및 중복 제거 단계 이후)를 기반으로 합니다. 훈련 데이터. LLM 훈련에 일반적으로 사용되는 두 가지 주요 데이터 유형은 큐레이트된 데이터와 웹 크롤링 데이터입니다. 큐레이션된 데이터는 일반적으로 타깃 소스 및 도메인에서 잘 작성되고 잘 형식화된 텍스트로 구성됩니다. 예를 들어, 위키피디아 기사, 책, 뉴스 기사 및 과학 논문은 &quot;The Pile&quot;(Gao et al., 2020) 및 &quot;BookCorpus&quot;(Zhu et al., 2015) 데이터 세트에 사용됩니다. 반면, 웹 크롤링 데이터는 블로그, 소셜 미디어 게시물, 뉴스 기사 및 광고와 같이 형식 및 쓰기 스타일 측면에서 상당히 다양한 인터넷의 광범위한 소스에서 수집된 텍스트를 포함합니다. CommonCrawl(CC)은 12년 동안 인터넷에서 페타바이트 규모의 데이터를 수집한 널리 사용되는 웹 크롤링 저장소입니다. 이를 위해 큐레이션된 데이터는 종종 더 높은 품질을 가지고 있다고 간주되며, 이로 인해 BERT(Devlin et al., 2019) 및 GPT-2(Radford et al., 2019)와 같은 초기 LLM을 훈련하는 데 선호되었습니다. 그러나 더 큰 모델에 대한 수요가 성장함에 따라 웹 크롤링 데이터는 최근 LLM의 학습 데이터에 상당 부분을 기여하기 때문에 더 많은 주목을 받고 있습니다.예를 들어 ROBERTA(Liu 등, 2019), BART(Lewis 등, 2020), T5(Raffel 등, 2020), GPT-3(Rae 등, 2021), LLaMa(Touvron 등, 2023), MPT(MosaicML, 2023), Falcon(Almazrouei 등, 2023) 등이 있습니다. 따라서 이러한 LLM을 학습하기 위해 C4(Raffel 등, 2020), CC-News(Nagel), STORIES(Trinh and Le, 2018)를 포함하여 다양한 CC 추출물이 생성되었습니다. 학습 데이터의 접근성과 관련하여, 초기 LLM을 학습하는 데 사용된 데이터 세트는 종종 대중에게 공개됩니다(Devlin et al., 2019; Raffel et al., 2020). 그러나 가장 최근의 최첨단(SOTA) 생성 LLM의 경우, 학습 데이터 세트가 상업적 이익으로 인해 완전히 공개되지 않습니다. 이는 ChatGPT 및 GPT-4와 같은 독점 모델뿐만 아니라 LLaMa, MPT, Falcon, BLOOM과 같이 오픈소스 모델이라고 주장하는 모델에도 적용됩니다(Scao et al., 2022). 기존 LLM의 투명성 문제를 해결하기 위해, 최첨단 LLM, 즉 RedPajama(Computer, 2023), SlimPajama, AIDolma에 대한 학습 데이터 세트를 복제하여 공개하려는 최근의 노력이 있었습니다. 이러한 데이터 세트의 주요 차이점은 LLM 교육을 위한 고품질을 보장하기 위해 세심하게 정리되고 문서 수준에서 중복 제거된 대규모 텍스트 데이터와 관련이 있습니다. 그럼에도 불구하고 이러한 오픈소스 데이터 세트의 일반적인 단점은 주로 영어 데이터에 초점을 맞추고 다른 언어에 대한 데이터는 제한적이라는 것입니다. LLM 교육을 위한 다국어 대규모 데이터 세트를 얻으려면 CC와 같은 웹 스크레이프 데이터 세트를 활용하여 여러 언어로 최신 정보를 사용하여 효율적인 데이터 수집을 수행하는 것이 더 편리합니다. 또한 고성능 LLM의 고품질을 보장하려면 노이즈가 많고 관련성이 없는 콘텐츠(예: 품질이 낮은 기계 생성 텍스트 및 성인 콘텐츠)를 피하기 위해 다국어 데이터를 광범위하게 정리하고 중복 제거해야 합니다(Trinh 및 Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). 따라서 고품질 데이터 세트를 생성하기 위한 일반적인 데이터 처리 파이프라인에는 FastText(Joulin 등, 2016), CC-Net(Wenzek 등, 2020), BLOOM 모델을 위한 BigScience ROOTS 코퍼스(Laurençon 등, 2022; Scao 등, 2022), Falcon 모델을 위한 RefinedWeb 데이터 세트(Penedo 등, 2023; Almazrouei 등, 2023) 및 LLaMa 모델을 훈련하기 위한 데이터 세트(Touvron 등, 2023)에서 보여준 것처럼 여러 단계가 포함될 수 있습니다. 첫 번째 단계는 이러한 파이프라인에서 언어를 식별하여 데이터를 해당 언어에 적절히 할당하는 것이 필요합니다(Joulin 등, 2016). 다음 단계에서는 특수 문자, 짧은 줄, 나쁜 단어 등의 비율에 따라 바람직하지 않은 콘텐츠를 필터링하기 위한 다양한 데이터 세트별 규칙과 휴리스틱이 특징입니다(Grave et al., 2018; Laurençon et al., 2022). 또한 데이터는 가벼운 모델(예: KenLM 언어 모델(Heafield, 2011))을 통해 필터링하여 노이즈가 많은 문서를 피할 수 있습니다(Wenzek et al., 2020). 마지막으로 데이터 중복 제거를 수행하여 유사하거나 반복되는 정보를 제거해야 합니다(Laurençon et al., 2022; Penedo et al., 2023). 이와 관련하여 중요한 단계는 문서 수준에서 퍼지 중복 제거를 포함하는데, 예를 들어 MinHash(Broder, 1997)를 통해 유사한 문서를 제거하고, 이를 통해 기억을 완화하고 결과 LLM에 대한 일반화를 개선하는 것입니다(Lee et al., 2022). 이를 위해 mC4(Xue et al., 2021), OSCAR(Ortiz Suárez et al., 2019), CC100(Wenzek et al., 2020; Conneau et al., 2020) 및 BigScience ROOT 코퍼스(Laurençon et al., 2022)와 같이 여러 언어로 된 텍스트 데이터가 있는 다국어 오픈소스 데이터 세트가 있지만, 그 품질과 규모는 특히 GPT와 같은 생성 모델의 경우 LLM을 효과적으로 훈련하는 데 필요한 요구 사항을 충족하지 못합니다. 예를 들어, 서론에서 강조했듯이 mC4와 OSCAR는 모두 문서 수준의 데이터에 대한 퍼지 중복 제거가 부족합니다. mCalso는 cld3를 사용하기 때문에 언어 식별이 좋지 않습니다. BigScience ROOTS는 46개 언어에 대한 작은 샘플 데이터만 제공하는 반면 CC100은 2018년 이후의 정보를 제공하지 않습니다. 따라서 저희 데이터 세트 CulturaX는 기존 데이터 세트의 문제를 종합적으로 해결하여 LLM을 훈련하는 데 쉽게 사용할 수 있고 고품질의 데이터를 제공하는 다국어, 오픈 소스, 대규모 데이터 세트를 제공합니다. 5
--- CONCLUSION ---
167개 언어의 텍스트 데이터가 있는 새로운 다국어 데이터 세트인 CulturaX를 소개합니다. 저희 데이터 세트는 포괄적인 파이프라인을 통해 정리되고 중복 제거되어 6.3조 개의 토큰을 생성합니다. 따라서 CulturaX는 대규모의 고품질 데이터 세트로, 다국어에 대한 고성능 LLM을 훈련하는 데 쉽게 사용할 수 있습니다. 저희 데이터는 다국어 학습에 대한 추가 연구와 응용 프로그램을 촉진하기 위해 대중이 공개적으로 접근할 수 있습니다. 참고문헌 Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards a cleaner document-driven multilingual crawled corpus. 제13회 언어 자원 및 평가 컨퍼런스 회의록, 4344-4355페이지, 프랑스 마르세유. European Language Resources Association. Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2021. Ungoliant: 매우 대규모 다국어 웹 코퍼스 생성을 위한 최적화된 파이프라인. 2021년 대규모 코퍼스 관리의 과제에 대한 워크숍(CMLC-9) 회의록. 리머릭, 2021년 7월 12일(온라인 이벤트). Miltiadis Allamanis. 2018. 코드의 머신 러닝 모델에서 코드 중복의 부정적 영향. 2019년 ACM SIGPLAN 국제 심포지엄의 새로운 아이디어, 새로운 패러다임, 프로그래밍 및 소프트웨어에 대한 성찰 회의록. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi et al. 2023. Falcon-40B: 최첨단 성능을 갖춘 개방형 대규모 언어 모델. 방예진, 사무엘 카야위자야, 이나연, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu 및 Pascale Fung. 2023. 추론, 환각 및 상호 작용에 대한 chatgpt의 다중 작업, 다중 언어, 다중 모드 평가. ArXiv, ABS/2302.04023. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William 웨이츠, 디온 위긴스, 하우메 사라고사. 2020. ParaCrawl: 웹 규모의 병렬 말뭉치 획득. 전산언어학협회 제58차 연차총회 진행, 4555-4567페이지, 온라인. 전산언어학협회. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli 등. 2021. 기초 모델의 기회와 위험. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov. 2017. 소규모 피드포워드 네트워크를 사용한 자연어 처리. 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 2879-2885페이지, 덴마크 코펜하겐. 계산 언어학 협회. A. Broder. 1997. 문서의 유사성과 포함에 관하여. 시퀀스의 압축 및 복잡성 회의록. Tom Brown, Benjamin Mann, et al. 2020. 언어 모델은 few-shot 학습자입니다. Arxiv, abs/2005.14165. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin et al. 2022. Palm: 경로로 언어 모델링 확장. ArXiv, abs/2204.02311. Together Computer. 2023. Redpajama: 라마 훈련 데이터 세트를 재생성하는 오픈 소스 레시피. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. 2020. 규모에 따른 비지도 교차 언어 표현 학습. Association for Computational Linguistics의 제58회 연례 회의록, 84408451페이지, 온라인. Association for Computational Linguistics. Michel Dekking, Cornelis Kraaikamp, Hendrik Paul, Ludolf Erwin Meester. 2007. 확률과 통계에 대한 현대적 소개: 이유와 방법 이해. Springer Texts in Statistics에서. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학 협회. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. 파일: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트. ArXiv, abs/2101.00027. Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas Mikolov. 2018. 157개 언어에 대한 단어 벡터 학습. 제11회 언어 자원 및 평가 국제 컨퍼런스(LREC 2018) 회의록, 일본 미야자키. 유럽 언어 자원 협회(ELRA). Kenneth Heafield. 2011. KenLM: 더 빠르고 더 작은 언어 모델 쿼리. 제6회 통계 기계 번역 워크숍 회의록, 187-197쪽, 스코틀랜드 에든버러. 계산 언어학 협회. Danny Hernandez, Tom B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, TJ Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Christopher Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan 및 Sam McCandlish. 2022. 반복 데이터 학습의 법칙 및 해석 가능성 확장. ArXiv, ABS/2205.10487. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes 및 최예진. 2019. 신경 텍스트 변성의 흥미로운 사례. ArXiv, ABS/1904.09751. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou 및 Tomas Mikolov. 2016. Fasttext.zip: 텍스트 분류 모델 압축. ArXiv, abs/1612.03651. Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving. 2021. 언어 에이전트 정렬. ArXiv, abs/2103.14659. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey 오세이, 페드로 오르티즈 수아레스, 이로로 오리페, 켈레치 오구에지, 안드레 니용가보 루분고, 토안 Q. 응우옌, 마티아스 뮐러, 안드레 뮐러, 샴수딘 하산 무하마드, 난다 무하마드, 아얀다 음냐케니, 잠시드벡 미르자카로프, 타피와나셰 마탄기라, 콜린 레옹, 은제 로슨, 스네하 쿠두군타, 야신 제르나이트, Mathias Jenny, Orhan Firat, Bonaventure FP Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Ballı, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidate. 56회 연례 총회 의사록(제1권: 장문 논문), 66-75쪽, 호주 멜버른. Association for Computational Linguistics. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen. 2023. Chatgpt beyond english: 다국어 학습에서 대규모 언어 모델에 대한 포괄적 평가를 향해. ArXiv, abs/2304.05613. Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin Lhoest, Angelina McMillan-Major, Gérard Dupont, Stella 비더만, 안나 로저스, 루브나 벤 알랄, 프란체스코 드 토니, 지아다 피스틸리, 올리비에 응우옌, 소마이에 닉푸어, 마라임 마수드, 피에르 콜롬보, 하비에르 데 라 로사, 파울로 빌레가스, 트리스탄 스러쉬, 셰인 롱프레, 세바스티안 나겔, 레온 웨버, 마누엘 로메로 무뇨스, 지안 주, 다니엘 반 스트리엔, 자이드 알리야페아이, 칼리드 알무바라크, 부민 Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, Yacine Jernite. 2022. Bigscience ROOTS 코퍼스: 1.6TB 복합 다국어 데이터 세트. 제36회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini. 2022. 학습 데이터 중복 제거로 언어 모델이 개선됩니다. 60th Annual Meeting of the Association for Computational Linguistics(제1권: 장문 논문)의 회의록, 8424-8445페이지, 아일랜드 더블린. Association for Computational Linguistics. Jure Leskovec, Anand Rajaraman, Jeffrey David Ullman. 2020. Mining of massive datasets. Cambridge University Press에서. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. 2020. BART: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 학습의 노이즈 제거. Association for Computational Linguistics의 제58회 연례 회의의 회의록, 7871-7880페이지, 온라인. Association for Computational Linguistics. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: 기술적 세부 사항 및 평가. 백서. A121 Labs. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 bert 사전 학습 접근법. ArXiv, abs/1907.11692. MosaicML. 2023. mpt-7b 소개: 오픈 소스, 상업적으로 사용 가능한 llms를 위한 새로운 표준. https://www.mosaicml.com/blog/mpt-7b. Sebastian http: Nagel. Cc-news. //web.archive.org/save/http: //commoncrawl.org/2016/10/news- dataset-available. Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2020. 중간 리소스 언어를 위한 문맥화된 단어 임베딩에 대한 단일 언어적 접근 방식. Association for Computational Linguistics의 제58회 연례 회의록, 1703-1714쪽, 온라인. Association for Computational Linguistics. Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. 2019. 중간에서 낮은 리소스 인프라에서 방대한 코퍼스를 처리하기 위한 비동기 파이프라인. 대규모 코퍼스 관리의 과제에 대한 워크숍(CMLC7) 2019의 회의록. 카디프, 2019년 7월 22일. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay. 2023. Falcon LLM을 위한 정제된 웹 데이터 세트: 웹 데이터와 웹 데이터만을 사용한 큐레이션된 코퍼스보다 우수한 성과. Arxiv, abs/2306.01116. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그. Jack Rae, Sebastian Borgeaud, et al. 2021. 언어 모델 확장: 고퍼 훈련에서 얻은 방법, 분석 및 통찰력. ArXiv, abs/2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. Journal of Machine Learning Research에서. Teven Scao, Angela Fan, et al. 2022. Bloom: 176b-매개변수 오픈 액세스 다국어 언어 모델. ArXiv, abs/2211.05100. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, Francisco Guzmán. 2021. WikiMatrix: 위키피디아에서 1620개 언어 쌍으로 135M개의 병렬 문장 마이닝. 제16차 유럽 지부 학회 회의록: 주요 권, 1351-1361페이지, 온라인. Association for Computational Linguistics. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. 2019. Megatron-lm: 모델 병렬 처리를 사용하여 수십억 개의 매개변수 언어 모델 학습. ArXiv, abs/1909.08053. Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli. 2021. 대규모 언어 모델의 역량, 한계 및 사회적 영향 이해. ArXiv, abs/2102.02503. Hugo Touvron, Thibaut Lavril, Gautier Izacard et al. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. ArXiv, ABS/2302.13971. Trieu H. Trinh 및 Quoc V. Le. 2018. 상식 추론을 위한 간단한 방법. ArXiv, ABS/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. 신경 정보 처리 시스템의 발전. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean 및 William Fedus. 2022. 대규모 언어 모델의 새로운 능력. 기계 학습 연구 거래. Xiangpeng Wei, Hao-Ran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, Jun Xie. 2023. Polylm: 오픈 소스 다국어 대규모 언어 모델. ArXiv, abs/2307.06018. Laura Weidinger, John FJ Mellor, Maribeth Rauh et al. 2021. 언어 모델로 인한 피해의 윤리적 및 사회적 위험. ArXiv, abs/2112.04359. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave. 2020. CCNet: 웹 크롤링 데이터에서 고품질 단일 언어 데이터 세트 추출. 제12회 언어 자원 및 평가 컨퍼런스 회의록, 4003-4012페이지, 프랑스 마르세유. 유럽 언어 자원 협회. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. 2021. mT5: 대규모 다국어 사전 학습된 텍스트-텍스트 변환기. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 483-498페이지, 온라인. Association for Computational Linguistics. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. 책과 영화 정렬: 영화 감상과 책 읽기를 통한 스토리와 같은 시각적 설명. IEEE International Conference on Computer Vision(ICCV) 회의록.
"
"--- ABSTRACT ---
Visual instruction tuning has recently shown encouraging progress with opensource large multimodal models (LMM) such as LLaVA and MiniGPT-4. However, most existing studies of open-source LMM are performed using models with 13B parameters or smaller. In this paper we present an empirical study of scaling LLaVA up to 33B and 65B/70B, and share our findings from our explorations in image resolution, data mixing and parameter-efficient training methods such as LoRA/QLoRA. These are evaluated by their impact on the multi-modal and language capabilities when completing real-world tasks in the wild. We find that scaling LMM consistently enhances model performance and improves language capabilities, and performance of LoORA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning. Additionally, the study highlights the importance of higher image resolutions and mixing multimodal-language data to improve LMM performance, and visual instruction tuning can sometimes improve LMM’s pure language capability. We hope this study makes state-of-the-art LMM research at a larger scale more accessible, thus helping establish stronger baselines for future research. Code and checkpoints will be made public. 1
--- METHOD ---
s such as LoRA/QLoRA. These are evaluated by their impact on the multi-modal and language capabilities when completing real-world tasks in the wild. We find that scaling LMM consistently enhances model performance and improves language capabilities, and performance of LoORA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning. Additionally, the study highlights the importance of higher image resolutions and mixing multimodal-language data to improve LMM performance, and visual instruction tuning can sometimes improve LMM’s pure language capability. We hope this study makes state-of-the-art LMM research at a larger scale more accessible, thus helping establish stronger baselines for future research. Code and checkpoints will be made public. 1 Introduction Recent studies on large multimodal models (LMM) [9, 10] have been focused on the methods of visual instruction tuning [|2]. The results are promising: e.g., the open-source project Large Language and Vision Assistant (LLaVA) shows that training a 7B large language model (LLM) with multimodal instruction-following data for 3 hours on 8 A-100 GPUs leads to a LMM with strong visual understanding and reasoning capabilities in the wild: reproducing some of the most appealing examples of the proprietary OpenAI multimodal GPT-4 model [14]. A similar idea is explored in its co-current work MiniGPT-4 [20]. It has rapidly become a prominent research topic, spurring the development of numerous new models, benchmarks, and applications [10]. However, the high compute cost has led most existing studies to utilize 7B and 13B LLMs. Thus, the impact of significantly scaling up the model size to e.g., 33B and 65B remains unexplored. This study aims to fill this gap by empirically investigating language models of larger sizes for LMM, sharing insights of our scaling
--- EXPERIMENT ---
s and establishing stronger baselines using larger-scale LLaVA for future research. Specifically, we explore the impact of larger model sizes, model tuning and data mixing methods on model performance, and present our findings and recommendations. The scaling recipe leads to new state-of-the-art (SoTA) performance on LLaVA-Bench [12] and MM-VET [19]. We hope that our findings and larger LLaVA checkpoints would provide a reference for future research on visual instruction tuning. “These authors contributed equally to this work Preprint. Work in progress --- --2 Experiment Setup Model Checkpoints. To study the impact of scaling up LLM on multimmodal capabilities, we increase the language model size to 33B and 65B [15], in addition to the 7B and 13B models used for existing LMM. * LLaVA-33B We employ the open source Vicuna-33B checkpoint ! [16] to preform the twostage training. The training data is around 125K conversations collected from ShareGPT. com. ¢« LLaVA-65B Due to a lack of public 65B Vicuna checkpoint, we conduct our own training of the Vicuna-65B model, utilizing ShareGPT data that we have independently processed. This data contains 159M tokens used during training. As a comparison, the reported number of tokens used in training Vicuna 33B is 370M °. Once the instruction-tuned LLM is given, we follow [12] to perform the two-stage LLaVA lightning training: (i) Stage 1: Pre-training for Feature Alignment. The linear projection layer is trained, which maps the visual feature (the features before the last layer of the pre-trained image encoder) to word embedding space of LLM. More specifcally, the projection dimension is 1024—+6656 for the 33B model and 1024-+8192 for the 65B model, respectively. In this stage, we use the conceptbalanced subset of LAION-CC-SBU data with 558K samples. (ii) Stage 2: Visual Instruction Tuning. We use the LLaVA-80K multimodal instruct dataset for the fine-tuning stage. Various training schedules are explored to enable the model to follow the diverse instructions to complete tasks in the wild, as to be detailed below. Tuning Methods. We explore both the trainable modules and training data mixing for efficient and effective visual instruct tuning of large models. * Trainable modules. In addition to tuning the linear projection layer, two schemes are considered to tune the LLM: (7) Full-model fine-tuning of LLM and (ii) Parameter-efficient training methods. For the latter, LoRA [7] and QLoRA [4] are employed to allow us to tune large models with limited compute resource. This aims to gain an in-depth understanding of the trade-off between the training cost and model performance. ¢ Data mixing. Typically only the multimodal instruction data is used in Stage-2. We further consider mixing the language-only instruct data ShareGPT with the LLaVA-80K multimodal instruction data to gain an in-depth understanding of the trade-off between models’ language and multimodal capabilities. Hyper-parameters. In the training process of both stages, we utilize the DeepSpeed library > and employ the ZeRO3 optimizer, except for QLoRA runs we use ZeRO2. We use a maximum sequence length of 2048. For Stage 1, we train both the 33B and 65B models with a learning rate of 1 x 10-with no weight decay, and a learning rate with linear decay and linear warmup for 3% of training steps in total. For Stage 2, we use a learning rate of 2 x 10~° in full fine-tuning to train 1 epoch for all the models in full finetuning, and a learning rate of 1 x 10~+ for the LORA/QLoRA runs. We conducted a set of hyperparameter search and for LoRA runs, and found larger LoRA alpha or equivalently larger learning rate was crucial to get the best performance. Specifically, we use LORA alpha equals 2 times the LoRA rank, and a learning rate of 1x 10~4, which works the best for all the models. For full fine-tuning, we use a total batch size of 512 on 4 A100 nodes, where each of these nodes is equipped with 8 A100-80G GPUs. For LoRA/QLoRA runs, we use a total batchsize ofon | A100 node for 33B model and 2 nodes for 65B model. 3 Results We first compare our large checkpoints on two recent benchmarks which are specifically designed for LMM, then report our findings in the course of scaling up LLaVA models. ‘https: //huggingface.co/Imsys/vicuna-33b-v1.“https: //github.com/1m-sys/FastChat/blob/main/docs/vicuna_weights_version.md Shttps://github.com/microsoft/DeepSpeed --- --Bard-0718 78.7 83.7 69.7 77.Bing-Chat-0629 90.1 59.6 52.2 71.LLaVA-13B (beam=1) 81.7 64.3 55.9 70.LLaVA-13B (beam=5) 84.3 68.4 59.9 73.LLaVA-33B (beam=1) 82.9 70.2 62.6 73.LLaVA-33B (beam=5) 83.5 72.6 61.9 74.LLaVA-65B (beam=1) 87.3 63.8 62.3 74.LLaVA-65B (beam=5) 88.7 59.4 65.7 T4.Table 1: The performance comparison on LLaVA-Bench. Beam search sizes at | and 5 are reported. Model Rec OCR Knowledge Generation Spatial Math Total Results of various open-source LMM on reported in the MM-VET paper [19] LLaMA-Adapter v2-7B [5] 16.8 7.8 2.5 3.0 16.6 44 | 13.6+40.OpenFlamingo-9B [1, 2] 24.6 14.4 13.0 12.3 18.0 15.0 | 21.8+0.MiniGPT-4-8B [20] 274 15.0 12.8 13.9 20.3 7.7 | 22.1+0.BLIP-2-12B [11] 27.5 11.1 11.8 7.0 16.2 5.8 | 22.4+0.LLaVA-7B [12] 28.0 17.1 16.3 18.9 21.2 11.5 | 23.8+0.MiniGPT-4-14B [20] 29.9 16.1 20.4 22.1 22.2 3.8 | 24.440.Otter-9B [8] 28.4 16.4 19.4 20.7 19.3 15.0 | 24.6+0.InstructBLIP-14B [3] 30.8 16.0 9.8 9.0 21.1 10.5 | 25.6+0.InstructBLIP-8B [3] 32.4 14.6 16.5 18.2 18.6 7.7 | 26.2+0.LLaVA-13B [12] 30.9 20.1 23.5 26.4 24.3 7.7 | 26.4+0.MM-ReAct-GPT-3.5 [18] 24.2 31.5 21.5 20.7 32.3 26.2 | 27.940.LLaVA-7B (LLaMA-2) [12] 32.9 20.1 19.0 20.1 25.7 5.2 | 28.1+0.LLaVA-13B (V1.3, 336px) [12] | 38.1 22.3 25.2 25.8 31.3 11.2 | 32.5+0.LLaVA-13B (LLaMA-2) [12] 39.2 22.7 26.5 29.3 29.6 7.7 | 32.9+40.MM-ReAct-GPT-4 [18] 33.1 65.7 29.0 35.0 56.8 69.2 | 44.640.Results with our own experiment runs LLaVA-13B (LLaMA-2) 38.4 21.0 26.3 28.8 28.0 7.7 | 32.6+0.LLaVA-33B 38.5 25.0 26.2 28.2 29.2 7.7 | 32.9+40.LLaVA-33B (Data Mixing) 37.7 27.1 26.2 28.6 28.1 11.5 | 34.1+0.LLaVA-65B 39.2 28.2 26.2 28.3 33.0 15.0 | 35.5+0.LLaVA-65B (Data Mixing) 41.8 27.9 30.4 32.3 30.5 7.3 | 36.4£0.Table 2: Performance of various open-source LMM on MM-VET. Note that MM-ReAct is not an single multimodal model, it is a system built on chaining visual tools via GPT-3.5 or GPT-4, which we append as a reference. Our experiment run on LLaVA-13B (LLaMA-2) yields very similar score with the same checkpoint reported in MM-VET paper, indicating that our evaluation pipelines are consistent. 3.1 Comparisons on Benchmarks LLaVA-Bench. LLaVA-Bench (In-the-Wild)‘ [12] is a diverse evaluation dataset consisting ofimages with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches. Each image is paired with a manually-curated, detailed description and a set of properly-selected questions related to open-ended visual chat scenarios. Each questions belongs to one of three types of tasks: conversations that contain simple visual recognition & QA questions, detailed descriptions that characterize the image with a long paragraph, and a complex reasoning task that focuses on deducing implications from an image. Language GPT-4 (gpt4-0314) is used to score to the generated answers. The relative scores between the model output and gold response are reported. We compare LLaVA against the commercial visual chat systems including Microsoft BingChat* and Google Bard® on LLaVA-Bench [12]. ‘https: //github. com/haotian-1liu/LLaVA/blob/main/docs/LLaVA_Bench.md Shttps://www.bing.com/chat Shttps://bard.google.com/ --- --The results are presented in Table 1. The 33B and 65B checkpoints outperform the 13B LLaVA model and Bing Chat. Despite the fact that LLaVA-Bench is small (thus the comparison might not be statistically significant), the results are encouraging: compared to large LMM, small open-sourced LMM are far more cost-effective to be deployed in real-world applications. With negligible increase of inference latency, we can significantly improve the performance for all model sizes by increasing the beam search size from | to 5. Our results show that larger LLaVA models generally exhibit better performance in tasks involving complex reasoning and generating detailed descriptions, which requires strong language competencies from larger LLM. In addition, larger LLaVA models obtain comparable results to BingChat in multi-turn, multi-modal conversation tasks that require strong image understanding capability. MM-VET. MM-VET [19] is designed based on the assumption that the intriguing capability of solving complicated tasks is often achieved by a generalist LMM which is able to integrate a varity of vision-language (VL) capabilities. MM-Vet contains 200 images and 218 questions (samples), aiming to evaluate6 core VL capabilities (recognition, OCR, knowledge, language generation, spatial awareness, and math) and their combinations. For evaluation, an LLM-based evaluator (gpt4-0613) is used to score open-ended outputs of different forms. In Table 2, we report the results on MMVET. The performance is consistently improved from 13B to 33B and 65B. The largest LLaVA model improves SoTA performance among the end-to-end open-source LMM. The most significant improvements are observed when evaluating the capabilities of knowledge and generation, followed by recognition and OCR. The performance on spatial and math remains comparable. The result reveals that the improved LLM capability is instrumental in storing more knowledge in the weights and leading to a stronger language responding capability. 3.2 Scaling up LLaVA The experiments are conducted to answer three research questions. @ Which scaling factor matters? We study the relative contribution of three scaling-up factors to the performance improvement of LLaVA. The results are summarized in Table 3 (a). * Model size. Increasing the model size consistently improves the overall performance. We conjecture that larger data size is essential to train a larger model. For example, if we only train on LLaVA-80K data, we see smaller gain when model size becomes larger. ¢ Image resolution. By fixing the CLIP ViT image encoder, we compare the variants that are pre-trained to take image resolution 224 x 224 and 336 x 336, and find that the higher resolution consistently yields 2-3 points improvement across all four LLM sizes. * Data mixing. Larger models tend to have higher capability of fitting the instruction data. By mixing the language-only instruction data (ShareGPT) with LLaVA-80K, we can improve model performance by 2 points, compared to training on multimodal instruction data only. In Table 3 (b), we present our result on MM-Bench [13], which contains a set of 2,974 questions, which evaluate models’ reasoning skills of six categories. The combination of the three factors improve the baseline LLaVA 7B model, reported in [13]. @ When should the parameter-efficient training method be considered? As model size increases, it becomes necessary to consider using tuning methods that are more efficient than fullmodel fine-tuning. LoRA and QLoRA are well-known parameter-efficient tuning methods. As shown in Table 4, we report compute cost using GPU hours per node, because the unit can be equivalent to the price $13.63/hour (ND A100 v4 series) on Azure ’. The total cost can be estimated by multiplying the #hours and #epochs. In Table 4(a), we train both the 33B and 65B model with LoRA rank 8 and 64 for 1 epoch on the LLaVA-80K instruction-tuning dataset. For models with 33B parameters and above, as we increase the LoRA rank values, we notice an increase in both performance and cost until full-model tuning reaches its maximum performance for a specific model size. In the case of the 13B model, we find that a rank of 64 can deliver comparable performance to full-model tuning. The cost is more related to the total number of parameters than the number of trainable parameters. The cost increase ‘https: //azure.microsoft .com/en-us/pricing/details/machine-learning/ --- --Image Size DataMixing 7B 13B 33B_ 65B 224x224 x 63.6 67.1 69.3 70.336X336 x 65.9 70.1 72.0 72.336X336 v - - 73.9 74.(a) Performance scores on LLaVA-Bench. Checkpoint ImageSize DataMixing Overall LR AR RR_ FP-S' FP-C- CP LLaVA-7B 224x224 x 36.2 15.9 53.6 28.6 41.8 200 40.LLaVA-33B 336336 v 55.7 23.3 740 46.0 S15 S04 67.LLaVA-65B 336336 v 56.0 244 72.3 49.3 S05 51.2 68.(b) Performance scores on MM-Bench. The skills to evaluate include logic reasoning (LR), attribute reasoning (AR), relation reasoning (RR), fine-grained single-instance perception (FP-S), fine-grained cross-instance perception (FP-C), and coarse perception (CP). Table 3: The performance to scale up model size, image resolution and data mixing. 7B 13B 33B 65B LoRA Rank Full 64 = “Full 8 64-QLoRA 64 Full 64 Full Performance 65.9 | 70.1 70.1 | 70.3 71.6 71.8 72.0] 72.2 72.Time (GPU Hours per node) | 13] 2.1 2.3 | 4.62 4.68 4.79 5.80] 9.17 13.# Trainable Parameters (B) | 7 | 0.26 13 | 0.06 0.49 0.49 33 | 0.81Table 4: The trade-off between performance and compute cost among different model sizes and traing methods on LLaVA-80K data. “Full” indicates the full-model fine-tuning. “Time” is reported as the total GPU hours to finish 1 epoch training (running time x #GPUs) divided by 8 (#GPUs per node). All models are trained on LLaVA-80K data, results are obtained through averagingrepeated evaluation runs with same set up on LLaVA-Bench. due to raising the LoRA rank for a given model size is significantly smaller than the cost increase by enlarging model sizes. For example, increasing the LoRA rank from 8 to 64 nearly matches the performance as LoRA fine-tuning a 65B model with same rank, but only requires 50% of 65B model’s training cost. In practice we find that tuning 33B model provide a good trade-off between cost and performance. Different LoRA variations have similar performance, and QLoRA requires lower GPU memory cost and running-time cost than LoRA. When large models (e.g., 65B) are trained with DeepSpeed ZeRO2 mode, they can fit into GPU with QLoRA, while yield the OOM issue with LoRA. In the experiments, we find that the hyperparameters of LoRA have a large impact of performance:(i) Large learning rate and alpha value of LoRA improves the results significantly. For example, With the same rank=64, we reduce the learning rate=2 x 10~° and alpha=16, the performance decrease from 71.8 to 65.5 on LLaVA-Bench. (ii) Under the same setting, large ranks leads to little improve ment. e.g., we increase the rank from 64 to 128 and 512, it improves from 65.5 to 66.1 and 68.1, respectively. @ A LMM with strong capabilities in both language and multimodal? We expand our evaluation in two aspects: (i) MM-VET is added to measure the integrated multimodal capabilities o LMM; (ii) The pure language ability of LMM is measured using Vicuna-80 [16] and MMLU [6], where the former evaluates the instruct-following ability in real-world language tasks, the latter eva uates the multilingual multi-task language ability. The results are shown in Table 5, where all models are full-model fine-tuned. Compared to Vicuna which initializes the LLM weights of LLaVA, it is surprising to observe that LLaVA, after being trained solely on multimodal instruction data, exhibits a comparable language capability. Mixing language instruction data can boost LLaVA’s multimodal ability, but not the language ability. This is partially attributed to the inclusion of complex reasoning questions, and longform answers in LLaVA-Instruct-158K, which helps maintain the language capabilities of LLaVA. --- --. Multimodal Language Model Data Mix | TLavaA-Bench MM-VET | Vicuna-80 MMLU Vicuna-13B - - - 79.9 55.LLaVA-13B x 70.1 32.5 79.6 55.Vicuna-33B - - - 85.6 59.LLaVA-33B x 72.0 32.9 85.3 56.LLaVA-33B v 73.9 34.1 80.3 58.Vicuna-65B - - - 83.2 62.LLaVA-65B x 72.3 35.5 84.5 62.LLaVA-65B v 74.2 36.4 82.6 62.LLaMA-2-70B-Chat - - - 84.7 63.LLaVA-70B v 69.8 35.4 81.3 65.Table 5: Performance on both multimodal and language capabilities. We also train LLaVA-70B based on the LLaMA-2-70B-Chat checkpoint [15], and find that mixed results on multimodal and language abilities. Interestingly, we improve LLaMA-2-70B-Chat by 2.points on MMLU, yielding an overall MMLU score of 65.1, which is the best performance for the 70B model size, according to [17] and the Chatbot Arena Leaderboard 8. To the best of our knowledge, this is the first reported result which shows visual instructing tuning improve language ability of large-scale LMM. 4
--- CONCLUSION ---
s and Limitations We present an empirical study of scaling the language model size for LMM. Our main findings are: (2) Scaling LMM consistently enhances model performance, resulting in significant improvements in language capabilities, primarily due to the increased LLM model size. We leave it to future work how to scale the vision encoder to enhance the visual capabilities and improve model performance on vision recognition and understanding tasks. (ii) Parameter-efficient methods such as LORA/QLoRA are viable solutions to finetune large-scale LLMs for a good performance-cost trade-off in some real-world settings with limited GPU memory. We observe that LORA/QLoRA’s performance are comparable to that of fine-tuning the full model, establishing their effectiveness through significant cost reduction in both model training and serving. (iii) Our study of training data curation reveals that properly selecting image resolutions and mixing multimodal-language data for model training can significantly improve the performance of the resultant LMM. We also show for the first time that visual instruction tuning can improve LMM’s language capability. Note that the training datasets used in this study is small. So, our findings are still preliminary. In future work, we will experiment using much larger datasets to investigate in detail whether and how different methods of training data selection and mixing can improve the quality of much larger LMM. References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.‘https : //huggingface.co/spaces/1msys/chatbot-arena- leaderboard --- --[[{il [[[[[[[[Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning, 2023.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized Ilms. arXiv preprint arXiv:2305. 14314, 2023.Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.Chunyuan Li. Large multimodal models: Notes on CVPR 2023 tutorial. arXiv preprint arXiv:2306.14895, 2023.Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint, 2023.Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, 2,Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.OpenAl. Gpt-4 technical report, 2023. | Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2,Vicuna. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://vicuna.1msys. org/, 2023. 2,Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023.Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 3,Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1,--- --This figure ""lora_loss.png"" is available in ""png"" format from: http://arxiv.org/ps/2309 .09958v
"	"--- ABSTRACT ---
시각적 지시 튜닝은 최근 LLAVA 및 MiniGPT-4와 같은 오픈소스 대형 멀티모달 모델(LMM)에서 격려적인 진전을 보였습니다. 그러나 오픈소스 LMM에 대한 대부분의 기존 연구는 13B 이하의 매개변수를 가진 모델을 사용하여 수행됩니다. 이 논문에서는 LLAVA를 최대 33B 및 65B/70B까지 확장하는 실증적 연구를 제시하고 이미지 해상도, 데이터 혼합 및 LORA/QLORA와 같은 매개변수 효율적 학습 방법에 대한 탐색에서 얻은 결과를 공유합니다. 이러한 결과는 실제 작업을 완료할 때 멀티모달 및 언어 기능에 미치는 영향에 따라 평가됩니다. LMM을 확장하면 모델 성능이 지속적으로 향상되고 언어 기능이 개선되며 LMM의 LORA/QLORA 튜닝 성능이 전체 모델 미세 조정 성능과 비슷하다는 것을 발견했습니다. 또한 이 연구는 LMM 성능을 개선하기 위해 더 높은 이미지 해상도와 멀티모달 언어 데이터를 혼합하는 것의 중요성을 강조하며 시각적 지시 튜닝은 때때로 LMM의 순수 언어 기능을 개선할 수 있습니다. 이 연구가 더 큰 규모의 최첨단 LMM 연구를 더 쉽게 접근할 수 있게 하여 미래 연구를 위한 더 강력한 기준을 확립하는 데 도움이 되기를 바랍니다. 코드와 체크포인트는 공개됩니다.
--- METHOD ---
LORA/QLORA와 같은 s. 이들은 실제 작업을 야생에서 완료할 때 다중 모달 및 언어 기능에 미치는 영향에 따라 평가됩니다. LMM을 확장하면 모델 성능이 지속적으로 향상되고 언어 기능이 개선되며 LMM의 LORA/QLORA 튜닝 성능이 전체 모델 미세 조정 성능과 비슷하다는 것을 발견했습니다. 또한 이 연구는 LMM 성능을 개선하기 위해 더 높은 이미지 해상도와 다중 모달 언어 데이터를 혼합하는 것의 중요성을 강조하며, 시각적 지시 튜닝은 때때로 LMM의 순수 언어 기능을 개선할 수 있습니다. 이 연구가 대규모로 최첨단 LMM 연구를 더 쉽게 접근할 수 있게 하여 미래 연구를 위한 더 강력한 기준선을 확립하는 데 도움이 되기를 바랍니다. 코드와 검사점은 공개됩니다. 서론 대규모 다중 모달 모델(LMM)에 대한 최근 연구[9, 10]는 시각적 지시 튜닝 방법[12]에 초점을 맞추었습니다. 결과는 유망합니다. 예를 들어, 오픈소스 프로젝트인 Large Language and Vision Assistant(LLaVA)는 8개의 A-100 GPU에서 3시간 동안 멀티모달 명령어 추종 데이터로 7B의 대형 언어 모델(LLM)을 학습하면 실제로 강력한 시각적 이해 및 추론 기능을 갖춘 LMM이 생성됨을 보여줍니다. 독점적인 OpenAI 멀티모달 GPT-4 모델의 가장 매력적인 예를 재현한 것입니다[14]. 유사한 아이디어가 동시 진행 중인 작업인 MiniGPT-4[20]에서 탐구되고 있습니다. 이는 빠르게 중요한 연구 주제가 되어 수많은 새로운 모델, 벤치마크 및 애플리케이션의 개발을 촉진했습니다[10]. 그러나 높은 컴퓨팅 비용으로 인해 대부분의 기존 연구에서 7B 및 13B LLM을 활용하게 되었습니다. 따라서 모델 크기를 예를 들어 33B 및 65B로 크게 확장하는 영향은 여전히 탐구되지 않았습니다. 이 연구는 LMM을 위한 더 큰 크기의 언어 모델을 경험적으로 조사하고 확장에 대한 통찰력을 공유하여 이러한 격차를 메우는 것을 목표로 합니다.
--- EXPERIMENT ---
s 및 향후 연구를 위해 대규모 LLAVA를 사용하여 더 강력한 기준선을 확립합니다. 특히, 우리는 더 큰 모델 크기, 모델 튜닝 및 데이터 혼합 방법이 모델 성능에 미치는 영향을 탐구하고 연구 결과와 권장 사항을 제시합니다. 스케일링 레시피는 LLaVA-Bench[12] 및 MM-VET[19]에서 새로운 최첨단(SOTA) 성능을 이끌어냅니다. 우리의 연구 결과와 더 큰 LLaVA 체크포인트가 시각 지시 튜닝에 대한 향후 연구에 대한 참고 자료가 되기를 바랍니다. &quot;이 저자들은 이 작업에 동등하게 기여했습니다. 사전 인쇄본. 진행 중인 작업 2 실험 설정 모델 체크포인트. 다중 모달 기능에 대한 LLM 확장의 영향을 연구하기 위해 기존 LMM에 사용된 7B 및 13B 모델 외에도 언어 모델 크기를 33B 및 65B로 늘립니다.[15]• LLAVA-33B 우리는 오픈 소스 Vicuna-33B 체크포인트¹ [16]를 사용하여 2단계 학습을 수행합니다. 학습 데이터는 ShareGPT.com에서 수집한 약 125,000개의 대화입니다.• LLAVA-65B 공개 65B Vicuna 체크포인트가 부족하기 때문에 우리는 독립적으로 처리한 ShareGPT 데이터를 활용하여 Vicuna-65B 모델에 대한 자체 학습을 수행합니다. 이 데이터에는 학습 중에 사용된 159M개의 토큰이 포함되어 있습니다. 비교를 위해 Vicuna 33B 학습에 사용된 것으로 보고된 토큰 수는 370M 2입니다. 명령어 조정 LLM이 제공되면 우리는 [12]에 따라 2단계 LLaVA 번개 훈련을 수행합니다.(i) 1단계: 특징 정렬을 위한 사전 훈련. 선형 투영 계층이 훈련되고, 이는 시각적 특징(사전 훈련된 이미지 인코더의 마지막 계층 이전의 특징)을 LLM의 단어 임베딩 공간에 매핑합니다. 더 구체적으로, 투영 차원은 33B 모델의 경우 1024→6656이고 65B 모델의 경우 1024→8192입니다. 이 단계에서는 558K 샘플이 있는 LAION-CC-SBU 데이터의 conceptbalanced 하위 집합을 사용합니다.(ii) 2단계: 시각적 지침 튜닝. 미세 조정 단계에 LLaVA-80K 다중 모드 지침 데이터 세트를 사용합니다. 다양한 훈련 일정을 탐색하여 모델이 다양한 지침을 따라 실제로 작업을 완료할 수 있도록 하며, 이는 아래에 자세히 설명되어 있습니다. 튜닝 방법. 우리는 대규모 모델의 효율적이고 효과적인 시각적 지침 튜닝을 위해 훈련 가능한 모듈과 훈련 데이터 혼합을 모두 탐색합니다. • 학습 가능한 모듈. 선형 투영 계층을 조정하는 것 외에도 LLM을 조정하기 위해 두 가지 방식을 고려합니다. (i) LLM의 전체 모델 미세 조정 및 (ii) 매개변수 효율적 학습 방법. 후자의 경우 LORA[7] 및 QLORA[4]를 사용하여 제한된 컴퓨팅 리소스로 대규모 모델을 조정할 수 있습니다. 이는 학습 비용과 모델 성능 간의 상충 관계에 대한 심층적인 이해를 얻는 것을 목표로 합니다. ⚫ 데이터 혼합. 일반적으로 2단계에서는 멀티모달 명령어 데이터만 사용합니다. 언어 전용 명령어 데이터 ShareGPT를 LLaVA-80K 멀티모달 명령어 데이터와 혼합하여 모델의 언어와 멀티모달 기능 간의 상충 관계에 대한 심층적인 이해를 얻는 것을 추가로 고려합니다. 하이퍼 매개변수. 두 단계의 학습 프로세스에서 DeepSpeed 라이브러리³를 활용하고 ZeRO3 옵티마이저를 사용하지만 QLORA 실행을 제외하고는 ZeRO2를 사용합니다. 우리는 최대 2048의 시퀀스 길이를 사용합니다. 1단계에서는 가중치 감소 없이 1×10의 학습 속도로 33B 및 65B 모델을 모두 학습하고, 총 학습 단계의 3%에 대해 선형 감소 및 선형 워밍업이 있는 학습 속도를 사용합니다. 2단계에서는 전체 미세 조정에서 2×10−5의 학습 속도를 사용하여 전체 미세 조정에서 모든 모델에 대해 1에포크를 학습하고, LORA/QLORA 실행에 대해 1×10−4의 학습 속도를 사용합니다. 우리는 일련의 하이퍼파라미터 검색 및 LORA 실행을 수행했으며, 최상의 성능을 얻으려면 더 큰 LoRA 알파 또는 동등하게 더 큰 학습 속도가 중요하다는 것을 발견했습니다. 구체적으로, 우리는 LoRA 랭크의 2배인 LoRA 알파와 모든 모델에 가장 적합한 1×10−4의 학습 속도를 사용합니다. 전체 미세 조정을 위해 4개의 A100 노드에서 총 배치 크기 512를 사용하는데, 각 노드에는 8개의 A100-80G GPU가 장착되어 있습니다. LORA/QLORA 실행의 경우 33B 모델의 경우 1개의 A100 노드에서 총 배치 크기 512를 사용하고 65B 모델의 경우 2개의 노드에서 총 배치 크기를 사용합니다. 3 결과 LMM을 위해 특별히 설계된 두 가지 최근 벤치마크에서 먼저 대규모 체크포인트를 비교한 다음, LLaVA 모델을 확장하는 과정에서 발견한 사항을 보고합니다. https://huggingface.co/lmsys/vicuna-33b-v1.2 https://github.com/1m-sys/FastChat/blob/main/docs/vicuna_weights_version.md 3 https://github.com/microsoft/DeepSpeedModels 추론 대화 세부 정보 | 전체 Bard-78.83.69.77.Bing-Chat-90.59.52.71.LLAVA-13B(빔=1) 81.64.55.70.LLAVA-13B(빔=5) 84.68.59.73.LLAVA-33B(빔=1) 82.70.62.73.LLAVA-33B(빔=5) 83.72.61.74.LLAVA-65B(빔=1) 87.63.62.74.LLAVA-65B(빔=5) 88.59.65.74.표 1: LLaVA-Bench에서의 성능 비교. 1과 5에서의 빔 검색 크기가 보고됩니다. 모델 | Rec OCR 지식 생성 공간 수학 | MM-VET 논문에 보고된 다양한 오픈소스 LMM의 총 결과 [19] LLAMA-Adapter v2-7B [5] 16.8 7.2.3.16.4.13.6±0.OpenFlamingo-9B [1, 2] 24.6 14.13.12.18.15.21.8±0.MiniGPT-4-8B [20] 27.4 15.12.13.20.7.22.1±0.BLIP-2-12B [11] 27.5 11.11.7.16.5.22.4±0.LLAVA-7B [12] 28.17.16.18.21.11.23.8±0.MiniGPT-4-14B [20] 29.9 16.20.22.22.3.24.4±0.Otter-9B [8] 28.16.19.20.19.15.24.6±0.InstructBLIP-14B [3] 30.8 16.9.9.21.10.25.6±0.InstructBLIP-8B [3] 32.14.16.18.18.7.26.2±0.LLAVA-13B [12] 30.9 20.23.26.24.7.26.4±0.MM-ReAct-GPT-3.5 [18] 24.2 31.21.20.32.26.27.9±0.LLAVA-7B (LLAMA-2) [12] 32.9 20.19.20.25.5.28.1±0.LLAVA-13B (V1.3, 336px) [12] 38.1 22.25.25.31.3 11.32.5±0.LLAVA-13B (LLAMA-2) [12] 39.22.26.29.29.7.32.9±0.MM-ReAct-GPT-4 [18] 33.65.29.35.56.69.44.6±0.자체 실험 실행 결과 LLAVA-13B (LLAMA-2) 38.21.26.28.28.7.32.6±0.LLAVA-33B 38.25.26.28.29.7.32.9±0.LLAVA-33B (데이터 혼합) 37.7 27.26.28.28.11.34.1±0.LLAVA-65B 39.2 28.26.28.33.15.35.5±0.LLAVA-65B(데이터 혼합) 41.8 27.30.32.30.7.36.4±0.표 2: MM-VET에서 다양한 오픈소스 LMM의 성능.MM-ReAct는 단일 멀티모달 모델이 아니라 GPT-3.5 또는 GPT-4를 통해 시각적 도구를 체이닝하여 구축한 시스템이며, 이를 참고로 추가합니다.LLaVA-13B(LLaMA-2)에서 실행한 실험은 MM-VET 논문에 보고된 동일한 체크포인트로 매우 유사한 점수를 얻었으며, 이는 평가 파이프라인이 일관됨을 나타냅니다.3.1 벤치마크 비교 LLAVA-Bench. LLaVA-Bench(In-the-Wild)4[12]는 실내 및 실외 장면, 밈, 그림, 스케치를 포함하여 총 60개의 질문이 있는 이미지로 구성된 다양한 평가 데이터 세트입니다. 각 이미지는 수동으로 큐레이팅된 자세한 설명과 개방형 시각적 채팅 시나리오와 관련된 적절하게 선택된 질문 세트와 쌍을 이룹니다. 각 질문은 세 가지 유형의 작업 중 하나에 속합니다. 간단한 시각적 인식 및 QA 질문이 포함된 대화, 긴 문단으로 이미지를 특징짓는 자세한 설명, 이미지에서 의미를 추론하는 데 중점을 둔 복잡한 추론 작업입니다. 언어 GPT-4(gpt4-0314)는 생성된 답변에 점수를 매기는 데 사용됩니다. 모델 출력과 골드 응답 간의 상대 점수가 보고됩니다. LLAVA-Bench[12]에서 Microsoft BingChat 및 Google Bard를 포함한 상용 시각적 채팅 시스템과 LLaVA를 비교합니다. 4 https://github.com/haotian-liu/LLAVA/blob/main/docs/LLaVA_Bench.md Shttps://www.bing.com/chat &quot;https://bard.google.com/결과는 표 1에 나와 있습니다. 33B 및 65B 체크포인트는 13B LLaVA 모델과 Bing Chat보다 성능이 뛰어납니다. LLaVA-Bench가 작다는 사실(따라서 비교가 통계적으로 유의하지 않을 수 있음)에도 불구하고 결과는 고무적입니다. 대형 LMM에 비해 작은 오픈소스 LMM은 실제 애플리케이션에 배포하는 데 훨씬 비용 효율적입니다. 추론 지연 시간이 무시할 만큼 증가하므로 빔 검색 크기를 1에서 5로 늘리면 모든 모델 크기에 대한 성능을 크게 개선할 수 있습니다. 결과에 따르면 일반적으로 대형 LLaVA 모델은 복잡한 추론과 자세한 설명을 생성하는 작업에서 더 나은 성능을 보이는데, 여기에는 대형 LLM의 강력한 언어 역량이 필요합니다. 또한 대형 LLaVA 모델은 BingChat과 비슷한 결과를 얻습니다. 영어: 강력한 이미지 이해 능력이 필요한 다중 턴, 다중 모달 대화 작업.MM-VET.MM-VET[19]는 복잡한 작업을 해결하는 흥미로운 능력은 다양한 시각-언어(VL) 기능을 통합할 수 있는 일반주의 LMM에 의해 종종 달성된다는 가정을 기반으로 설계되었습니다.MM-Vet에는 200개의 이미지와 218개의 질문(샘플)이 포함되어 있으며, 핵심 VL 기능(인식, OCR, 지식, 언어 생성, 공간 인식 및 수학)과 이들의 조합을 평가하는 것을 목표로 합니다.평가를 위해 LLM 기반 평가자(gpt4-0613)를 사용하여 다양한 형태의 개방형 출력을 채점합니다.표 2에서 MMVET에 대한 결과를 보고합니다.성능은 13B에서 33B 및 65B로 지속적으로 향상되었습니다.가장 큰 LLaVA 모델은 엔드투엔드 오픈소스 LMM 중에서 SoTA 성능을 향상시킵니다. 가장 큰 개선은 지식과 생성 능력을 평가할 때 관찰되었으며, 그 다음으로 인식과 OCR이 뒤를 이었습니다. 공간 및 수학에 대한 성능은 비슷한 수준을 유지했습니다. 결과에 따르면 향상된 LLM 기능은 가중치에 더 많은 지식을 저장하고 더 강력한 언어 대응 능력으로 이어지는 데 도움이 되었습니다. 3.2 LLAVA 확장 실험은 세 가지 연구 질문에 답하기 위해 수행되었습니다. ⑪어떤 확장 요소가 중요합니까? 우리는 LLaVA의 성능 개선에 대한 세 가지 확장 요소의 상대적 기여도를 연구합니다. 결과는 표 3(a)에 요약되어 있습니다. • • 모델 크기. 모델 크기를 지속적으로 늘리면 전체 성능이 향상됩니다. 더 큰 모델을 학습하려면 더 큰 데이터 크기가 필수적이라고 추측합니다. 예를 들어, LLAVA-80K 데이터로만 학습하는 경우 모델 크기가 커질수록 이득이 줄어듭니다. 이미지 해상도. CLIP ViT 이미지 인코더를 수정하여 224×224 및 336×336 이미지 해상도를 사용하도록 사전 학습된 변형을 비교한 결과, 더 높은 해상도가 모든 4가지 LLM 크기에서 일관되게 2~3포인트 향상을 가져온다는 것을 발견했습니다.• 데이터 혼합. 더 큰 모델은 지침 데이터를 맞추는 능력이 더 높은 경향이 있습니다.언어 전용 지침 데이터(ShareGPT)를 LLaVA-80K와 혼합하면 다중 모드 지침 데이터만으로 학습한 경우에 비해 모델 성능을 2포인트 향상시킬 수 있습니다.표 3(b)에서는 6개 범주의 모델 추론 기술을 평가하는 2,974개 질문 세트가 포함된 MM-Bench[13]에 대한 결과를 제시합니다.세 가지 요인을 결합하면 [13]에 보고된 기준 LLaVA 7B 모델이 향상됩니다. ②매개변수 효율적 학습 방법은 언제 고려해야 합니까?모델 크기가 커짐에 따라 전체 모델 미세 조정보다 효율적인 조정 방법을 사용하는 것을 고려해야 합니다. LORA와 QLORA는 잘 알려진 매개변수 효율적 튜닝 방법입니다.표 4에서 볼 수 있듯이, 단위가 Azure 7에서 시간당 $13.63(ND A100 v4 시리즈)의 가격과 동일할 수 있으므로 노드당 GPU 시간을 사용하여 컴퓨팅 비용을 보고합니다.총 비용은 #시간과 #에포크를 곱하여 추정할 수 있습니다.표 4(a)에서 LLAVA-80K 명령어 튜닝 데이터 세트에서 1에포크 동안 LoRA 순위 8과 64로 33B와 65B 모델을 모두 학습합니다.33B 이상의 매개변수가 있는 모델의 경우 LORA 순위 값을 늘리면 전체 모델 튜닝이 특정 모델 크기에 대해 최대 성능에 도달할 때까지 성능과 비용이 모두 증가합니다.13B 모델의 경우 순위 64가 전체 모델 튜닝과 비슷한 성능을 제공할 수 있음을 알 수 있습니다.비용은 학습 가능한 매개변수 수보다 총 매개변수 수와 더 관련이 있습니다. 비용 증가 https://azure.microsoft.com/en-us/pricing/details/machine-learning/이미지 크기 데이터 혼합 7B 13B 33B 65B 224×336x336x63.6 67.1 69.3 70.65.9 70.1 72.0 72.73.9 74.(a) LLAVA-Bench의 성능 점수. 체크포인트 이미지 크기 데이터 혼합 LLAVA-7B 224×LLAVA-33B 336xLLAVA-65B 336x전체 LR AR RR FP-S FP-C CP 36.2 15.9 53.6 28.6 41.8 20.0 40.5 5.7 23.3 74.0 46.0 51.5 50.4 67.5 6.0 24.4 72.3 49.3 50.5 51.2 68.(b) MM-Bench의 성능 점수. 평가할 기술에는 논리적 추론(LR), 속성 추론(AR), 관계 추론(RR), 세분화된 단일 인스턴스 인식(FP-S), 세분화된 교차 인스턴스 인식(FP-C) 및 거친 인식(CP)이 포함됩니다. 표 3: 모델 크기, 이미지 해상도 및 데이터 혼합을 확장하기 위한 성능.LORA 순위 7B 전체13B 전체성능 ↑ 시간(노드당 GPU 시간)↓ 학습 가능한 매개변수 수(B) 65.9 70.1 70.1 70.1.3 2.1 2.3 4.7 0.26 13 0.33B 64-QLORA 64 전체 64 전체 71.71.8 72.0 72.4.68 4.79 5.80 9.0.49 0.33 0.65B 72.13.표 4: LLaVA-80K 데이터에 대한 다양한 모델 크기와 학습 방법 간 성능과 컴퓨팅 비용 간의 균형.&quot;전체&quot;는 전체 모델 미세 조정을 나타냅니다. &quot;시간&quot;은 1 에포크 학습을 완료하는 데 걸린 총 GPU 시간(실행 시간 × GPU 수)을 8(노드당 GPU 수)로 나눈 값으로 보고됩니다. 모든 모델은 LLaVA-80K 데이터에서 학습되었으며, 결과는 LLAVA-Bench에서 동일한 설정으로 반복된 평가 실행을 평균하여 얻습니다. 주어진 모델 크기에 대한 LORA 순위를 높이는 것은 모델 크기를 확대하여 발생하는 비용 증가보다 상당히 작습니다. 예를 들어, LORA 순위를 8에서 64로 높이는 것은 같은 순위의 65B 모델을 미세 조정하는 LORA 성능과 거의 일치하지만, 65B 모델의 학습 비용의 50%만 필요합니다. 실제로 33B 모델을 조정하면 비용과 성능 간에 좋은 균형을 이룰 수 있습니다. 다양한 LORA 변형은 성능이 비슷하며, QLORA는 LoRA보다 GPU 메모리 비용과 실행 시간 비용이 낮습니다. 영어: DeepSpeed ZeRO2 모드로 대규모 모델(예: 65B)을 학습시키는 경우 QLORA를 사용하여 GPU에 적합할 수 있지만 LORA에서는 OOM 문제가 발생합니다.실험에서 LoRA의 하이퍼파라미터가 성능에 큰 영향을 미치는 것을 발견했습니다.(i) LORA의 큰 학습률과 알파 값은 결과를 상당히 개선합니다.예를 들어, 동일한 순위=64에서 학습률=2 × 10−5 및 알파=16을 줄이면 LLaVA-Bench에서 성능이 71.8에서 65.5로 감소합니다.(ii) 동일한 설정에서 큰 순위는 거의 개선되지 않습니다.예를 들어, 순위를 64에서 128 및 512로 높이면 각각 65.5에서 66.1 및 68.1로 향상됩니다.3 언어와 멀티모달 모두에서 강력한 기능을 갖춘 LMM?우리는 평가를 두 가지 측면에서 확장했습니다.(i) MM-VET를 추가하여 LMM의 통합 멀티모달 기능을 측정합니다. (ii) LMM의 순수 언어 능력은 Vicuna-80 [16]과 MMLU [6]를 사용하여 측정하는데, 전자는 실제 언어 과제에서의 지시 따르기 능력을 평가하고, 후자는 다국어 멀티태스크 언어 능력을 평가한다. 결과는 표 5에 나타나 있으며, 모든 모델은 전체 모델에서 미세 조정되었다. LLaVA의 LLM 가중치를 초기화하는 Vicuna와 비교했을 때, 다중 모달 지시 데이터로만 훈련된 후 LLAVA가 비슷한 언어 능력을 보인다는 것은 놀라운 일이다. 언어 지시 데이터를 혼합하면 LLaVA의 다중 모달 능력은 향상되지만 언어 능력은 향상되지 않는다. 이는 부분적으로 복잡한 추론 질문과 장문 답변을 LLaVA-Instruct-158K에 포함했기 때문이며, 이는 LLaVA의 언어 기능을 유지하는 데 도움이 됩니다.다중 모드 언어 모델 데이터 믹스 LLAVA-Bench MM-VET Vicuna-80 MMLU Vicuna-13B 79.55.LLAVA-13B 70.32.79.55.Vicuna-33B 85.59.LLAVA-33B 72.32.85.56.LLAVA-33B 73.34.80.58.Vicuna-65B 83.62.LLAVA-65B 72.35.84.62.LLAVA-65B 74.36.82.62.LLAMA-2-70B-Chat 84.63.LLAVA-70B 69.35.81.65.표 5: 멀티모달 및 언어 기능 모두에 대한 성능. 또한 LLaMA-2-70B-Chat 체크포인트[15]에 따라 LLAVA-70B를 훈련하고 멀티모달 및 언어 기능에서 엇갈린 결과를 발견했습니다. 흥미롭게도 MMLU에서 LLaMA-2-70B-Chat을 2포인트 향상시켜 전체 MMLU 점수가 65.1이 되었는데, [17] 및 Chatbot Arena Leaderboard 8에 따르면 이는 70B 모델 크기에 대한 최고의 성능입니다. 저희가 아는 한, 이는 시각적 지시 튜닝이 대규모 LMM의 언어 능력을 향상시킨다는 것을 보여준 최초의 보고된 결과입니다.
--- CONCLUSION ---
s 및 한계 LMM의 언어 모델 크기를 확장하는 것에 대한 실증적 연구를 제시합니다. 주요 결과는 다음과 같습니다. (i) LMM을 지속적으로 확장하면 모델 성능이 향상되어 주로 LLM 모델 크기가 증가하기 때문에 언어 기능이 크게 향상됩니다. 비전 인코더를 확장하여 시각적 기능을 향상시키고 비전 인식 및 이해 작업에서 모델 성능을 개선하는 방법은 향후 작업에 맡깁니다. (ii) LORA/QLORA와 같은 매개변수 효율적 방법은 제한된 GPU 메모리가 있는 일부 실제 환경에서 대규모 LLM을 미세 조정하여 성능-비용 균형을 맞추는 실행 가능한 솔루션입니다. LoRA/QLORA의 성능은 전체 모델을 미세 조정하는 것과 비슷하며 모델 학습과 제공 모두에서 비용을 크게 절감하여 효과를 입증합니다. (iii) 학습 데이터 큐레이션에 대한 연구에 따르면 모델 학습을 위해 이미지 해상도를 적절히 선택하고 다중 모달 언어 데이터를 혼합하면 결과 LMM의 성능이 크게 향상될 수 있습니다. 또한 시각적 지시 조정이 LMM의 언어 기능을 향상시킬 수 있음을 처음으로 보여줍니다. 이 연구에서 사용된 학습 데이터 세트는 작습니다.따라서 우리의 결과는 아직 예비적입니다.향후 작업에서는 훨씬 더 큰 데이터 세트를 사용하여 다양한 학습 데이터 선택 및 혼합 방법이 훨씬 더 큰 LMM의 품질을 개선할 수 있는지 여부와 어떻게 개선할 수 있는지 자세히 조사할 것입니다.참고문헌 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.Flamingo: a visual language model for few-shot learning.Advances in Neural Information Processing Systems, 35:23716-23736, 2022.[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: 대규모 자기회귀 시각 언어 모델을 훈련하기 위한 오픈소스 프레임워크. arXiv 사전 인쇄본 arXiv:2308.01390, 2023.8 https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi. Instructblip: 명령어 튜닝을 통한 범용 시각 언어 모델을 향해, 2023.[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. Qlora: 양자화된 LLM의 효율적인 미세 조정. arXiv 사전 인쇄본 arXiv:2305.14314,2023.[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: 매개변수 효율적 시각 지시 모델. arXiv 사전 인쇄본 arXiv:2304.15010, 2023.[6] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 대규모 멀티태스크 언어 이해 측정. arXiv 사전 인쇄본 arXiv:2009.03300, 2020.[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄 arXiv:2106.09685, 2021.[8] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang 및 Ziwei Liu. Otter: 상황에 맞는 명령어 조정이 가능한 다중 모드 모델입니다. arXiv 사전 인쇄 arXiv:2305.03726,2023.[9] 리춘위안. 대규모 다중 모드 모델: CVPR 2023 튜토리얼에 대한 참고 사항. arXiv 사전 인쇄 arXiv:2306.14895, 2023.[10] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang 및 Jianfeng Gao. 다중 모드 기반 모델: 전문가부터 범용 보조자까지. arXiv 사전 인쇄, 2023.[11] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 고정 이미지 인코더 및 대규모 언어 모델을 사용한 부트스트래핑 언어 이미지 사전 훈련. arXiv 사전 인쇄 arXiv:2301.12597, 2023.[12] Haotian Liu, Chunyuan Li, Qingyang Wu, 그리고 이용재. 시각적 교육 튜닝, 2023. 1,2,[13] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: 귀하의 다중 모드 모델은 다재다능한 플레이어입니까? arXiv 사전 인쇄 arXiv:2307.06281, 2023.[14] 오픈AI. Gpt-4 기술 보고서, 2023.[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2,[16] Vicuna. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇. https://vicuna.lmsys.org/, 2023. 2,[17] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 낙타는 얼마나 멀리 갈 수 있을까? 공개 리소스에 대한 명령 조정 상태를 탐색합니다. arXiv 사전 인쇄 arXiv:2306.04751, 2023.[18] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 및 Lijuan Wang. Mm-react: 다중 모달 추론 및 조치를 위해 chatgpt 프롬프트, 2023.[19] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang 및 Lijuan Wang. Mm-vet: 통합 기능을 위한 대규모 다중 모드 모델을 평가합니다. arXiv 사전 인쇄본 arXiv:2308.02490, 2023. 1, 3,[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv 사전 인쇄본 arXiv:2304.10592, 2023. 1, 이 그림 &quot;lora_loss.png&quot;는 다음에서 &quot;png&quot; 형식으로 제공됩니다: http://arxiv.org/ps/2309.09958v
"
"--- ABSTRACT ---
Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: (7) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. (ii) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates 1
--- INTRODUCTION ---
Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, suc! as fabricating information or producing biased, toxic, or even dangerous content, since LLMs are trained on a wide array of data, which can include low-quality sources. This has highlighted the necessities of LLM Alignments with human values, intentions, and preferences (Brown et al.|/Ouyang et al. 2022} Bai et al. 2022a} Glaese et al. 2022). Many approaches have been put forth to address the challenge LLM Alignments (2021). Among these approaches, Reinforcement Learning from Human Feedback (RLHF) has demonstrated its efficacy in aligning language models with human preferences. RLHF serves as a key component of training SoTA LLMs including exemplars such as OpenAI’s GPT-4 (Open (2023), Anthropic’s Claude (Bai et al.|/2022a), Google’s Sparrow fet al. [2022), Bard, and Meta’s Llama 2-Chat (Touvron et al.[]2023). RLHF elevates the capabilities of LLMs beyond the mere modeling of the distribution of their training data. It endows LLMs with the capacity to adapt their text generation distribution in a manner that are preferred by humans. However, training LLMs using RLHF is undoubtedly challenging, which demands an accurate and reliable reward model that approximates human judges, and a robust PPO algorithm for sustained policy improvements. Even with meticulous configurations, instabilities, e.g., gibberish responses (but high-reward) (2022), forgetting learned knowledge, are usually observed during training, which leads to recurring failures. These instabilities have several causes: (i) different reward score distributions are learned for various categories by the reward model, potentially leading to reward hacking issues (2022), a phenomenon where the model finds unintended ways to maximize the reward. As depicted in Figure[Tal the reward model learns noticeable disparity in reward score distributions for Code Generation and QA tasks, 2 out of “Equal Contribution ‘Work in progress --- --Code Generation — Learned Score 0.—~a += + Expected Score Frequency —4— RM-PPO L 0 ne 7 3 4Vq3 21201234567 oon lo (b) Win rate over the SFT model on the forget set eval (a) Reward score distributions. uated by GPT-4. Figure 1: Left: The distribution of reward scores for both the QA and Code Generation tasks. There is a noticeable disparity in the learned reward score distributions between the two tasks, despite the expectation that the distributions should be similar. Right: The win/loss rate over the SFT model on the forget set exhibits a significant decline. This drop in the win rate can be attributed to reward hacking and the phenomenon of catastrophic forgetting. 61 tasks present in the preference data. Even with reward score normalizations, the fluctuating means and variances can induce unexpected model behaviors, such as transferring the response patterns of Code Generations to QA examples due to the higher reward scores. (ii) over-optimizing with PPO on examples that were well-aligned with humans in the Supervised Fine-Tuning (SFT) stage triggers catastrophic forgetting issues {1989} Gupta et al.| /2023} 2022). Models tend to overlook what was learned_during the SFT stage, i.e, PPO model underperforms the SFT model on expert-aligned examples[| as shown in Figure Accordingly, in this technical report, we introduce two techniques to enhance the stability and effectiveness of the training of RLHF. Firstly, we propose Advantage Model to balance the reward score distributions across various categories, thus averting the reward hacking dilemma that is often induced by noticeable differences score distributions. This is achieved by directly modeling the advantage score, i.e., the extra reward one response can obtain compared with the expected reward, and regulating the advantage score distribution dynamically during training, ensuring that the variances and means are maintained within a reasonable range. Secondly, we introduce the Selective Rehearsal to alleviate the catastrophic forgetting issue. We posit that not all data should be optimized equally in PPO training. As such, we propose a robust and effective data selector that automatically identifies what examples could be utilized for PPO training and should be used to rehearsal knowledge accumulated in the SFT stage, preventing the depreciation of the model’s performance on expertaligned examples over time. Experiments on both public and proprietary data have demonstrated that our Advantage Model successfully balances reward score distributions across various examples while preserves ranking precision, and guide PPO training to achieve a higher reward score and win rate compared to the SFT model. Furthermore, Selective Rehearsal is able to avoid over-optimizing by selecting the most suitable examples for PPO training, thereby sustaining the performance on expert-aligned examples. Our contributions are summarized as follows: ¢ We analyze and identify several causes of instability in RLHF training, namely, imbalanced learned reward score distributions and over-optimization of certain PPO training data, which lead to reward hacking and catastrophic forgetting issues. ¢ We introduce the Advantage Model to balance reward score distributions across various categories, and the Selective Rehearsal strategy to discern which examples should be used ?Expert-aligned Examples are data samples that meet the standards and criteria delineated by experts and closely align with human preferences. These examples are used for SFT model training and evaluation. --- --for PPO training and which should be reserved for rehearsing knowledge accrued in the SFT stage. ¢ Through extensive experiments on both public and proprietary datasets, we demonstrate that the Advantage Model and Selective Rehearsal are able to stabilize RLHF training, achieving higher reward scores and win rates. 2 PRELIMINARY In recent machine learning research, RLHF (Ouyang et al} 2022} Bai et al.| 2022a) has emerged as a pivotal strategy for aligning LLMs to human goals (e.g. being helpful and harmless). RLHF typically follows the SFT phase, where SFT aligns a LLM with human objectives using teacher forcing on (prompt, response) pairs. However, despite this alignment, the LLM may still struggle with generalization when faced with unseen tasks. Learning a reward function from interaction between LLMs and humans and optimizing LLMs with the learned reward function using reinforcement learning has been shown as an effective approach to solving the LLM alignment problem. proposed a method involving reinforcement learning from human feedback, where RMs are trained on a dataset of comparisons between two model outputs generated from the same input. The goal is to assign higher rewards to outputs preferred by human labelers over others. Typically, this is achieved by adding a value head that outputs a scalar value on pre-trained transformer-baesd LMs with last umembedding layer removed. Specifically, the reward modeling loss is as follows: Lem = —E(e.ye.y)~D™ llog(o(ro(2, Ye) — To(#, Yr)))] () where rg (x,y) denotes the reward score for prompt x and response y with parameters 6, y. is the preferred response of the pair y, and y,., and D™ is the complete of comparison dataset. In what follows, Proximal Policy Optimization (PPO) (Schulman et al.|/2017) is commonly adopted as the reinforcement learning algorithm to optimize a policy due to its strengths in stability and simplicity. Particularly, the PPO objective for policy 7 on a prompt dataset D is defined as: Lepo = Exn pryary (x) [Po(t,y) — Blog (m(yla)/7*™** (y|x)) | (2) where rg(x, y) represents the reward score on the (prompt, response) pair of (x, y); 7#""%* indicates the policy before RLHF, and it is kept constant during RLHF training; £ is the coefficient for the KL-divergence term. Besides PPO, rejection sampling 2023) recently gains interests as a simple way for aligning LLMs. As an offline policy learning algorithm, it adopts an iterative process. For each iteration n, it first constructs a new dataset D,, by selecting (x,y) pairs from the main policybased on criteria F: DP? = {(x,y) - F(x,y)| such that « ~ D°?,y ~ 4(x)} @) where a commonly used criteria F = 1,.,(z,y)>7 includes only the samples with RM scores exceed a certain threshold 7. The policy is then updated by teacher forcing on D?P°: lyl Les = Eve,y)~pe0 > T(Yelyct 2) (4) t=3 APPROACH 3.1 FROM REWARD MODEL TO ADVANTAGE MODEL The learning objective of equation |1| primarily allows models to distinguish between humanpreferred responses and alternative options. It relies only on score differences to assess the likelihood of one response being superior to another. In such case, two different model responses that are both preferred by humans could have dramatically different values. In addition, interpreting the scalar values themselves can be challenging. --- --In light of these considerations, we introduce the Advantage Model (AM) for reward modeling. Analogous to the concept of the advantage function in reinforcement learning, the Advantage Model, denoted as a(x, y), quantifies the additional reward that response y can achieve over the expected reward e for prompt x. This is formally defined as: gay"" y)] “ Here, the notation y ~ 7’(x) signifies all possible responses generated by a policy 7’(2) when given the input prompt x. Since the comparison data is typically collected in many batches with different ag(x,y) = r(x, y) — Eyrnr(2)| SFT or PPO models, we introduce oe , the importance weight term to negate the bias introduced by the policy distribution shift. Intuitively, the extra reward gains of good response y, and the reward losses of bad response y, should be bounded by a margin m. As such, the training objective of AM consists of two parts, ranking loss that aligns with the formulation in Equation|1| and bounding loss to ensure the well-calibrated bounding of AM scores. It is formally defined as follows: Lam = —E (eye .yr)~D™ log (o(aa(#, Ye) — a(x, Yr))) + log(o(m(x) —ao(x,ye))) + log(a(m(a) + ao(e,¥-)))] where m(: Fis the function that defines the permitted margin for prompt x. However, it is infeasible to list every potential response to calculate the expected reward. To address this, we propose parameterizing the expected reward of the current policy, denoted as: €7 (x) = Eyvng(x)[To(2,¥)] (7) (6) By integrating the term representing the importance weight, we can reformulate the equation as follows: K CJ ae(x,y) = ro(x,y) — SFKer(a) — 0 Hola, y) (8) k=where N serves as a hyperparameter that harmonizes the emphasis placed on the current policy model relative to alternate policy models. A’ specifies the number of alternate policy models utilized for comparison data collection. Additionally, 7), (y|2) indicates the probability derived from the kth policy model. 3.2 PPO WITH SELECTIVE REHEARSAL In addition, we propose Selective Rehearsal to maintain the skills that are already acquired before RLHF. Selective rehearsal takes two major steps: representative example discovery and rehearsal training. Representative example discovery Given the policy 7g and PPO training prompts with policy outputs DPP° = [(21, y1), (x2, y2) ...], our goal is to select high-quality (x,y) pairs from D?P° that cover as many skills (e.g., solving algebra problems and writing resume) as possible. In order to let selected (x, y) pairs represent as many skills as possible, we first adopt a clustering algorithm (e.g. KMeans or Gaussian mixture) to separate D??° into ¢ clusters. To assure the representativeness and quality of the selected data, we only keep certain (x,y) pairs within each cluster that satisfy certain criteria regarding aspects such as advantage (reward) model score, entropy (low entropy indicates high confidence), human satisfaction rate or response length (higher length may indicate redundancy). Here we adopt the SimCSE 2021) sentence embedding""|to represent the query x for each (x,y) pair before running a KMeans algorithm on these embeddings to be grouped into c clusters. We briefly study the influence of cluster number c in Section Within each cluster, here we simply choose the top-k (x,y) pairs with the highest advantage model score (Eq. other strategies (e.g. combining advantage score with entropy score) in future work. 3We think that m() may have a connection with the complexity or difficulty involved in learning the reward function for prompts similar to x. However, thi: culative and requires further investigation. We leave this --- --One reason we select our rehearsal data from the PPO training data with each response y being generated from the initial policy model is to enable a more fair and nuanced comparison, as no additional information is introduced. In other scenarios, the rehearsal (7, y) pairs could come from other important data sources representing specific skills (e.g. math-problem solving) the main policy are not expected to forget. Rehearsal training After obtaining the rehearsal («, y) pairs of all clusters, we shuffle them together to form the rehearsal dataset Dr and compute NLL loss on Dr as a supplement to the standard PPO loss defined in Equation [2} yl Lppo-sr = Lppo + VE(e,y)~Dp So rs(velyce: zr) (9) t=where the coefficient for the NLL loss ¥ is empirically set to 0.01. Rehearsal training is similar with rejection sampling and reinforced self-training (Gulcehre et al. ) by using self-generated ys of high reward model score for supervised training. However, re sal training captures multi-dimensional important aspects (e.g., diversity), while rejection sampling and reinforced self-training only consider reward model score. Alternatively, one can view selective rehearsal as a means of amplifying the weight of the KLdivergence term in PPO training (Eq. a) for crucial instances and their related counterparts. 4 EXPERIMENTS 4.1 DATASETS AND MODELS RM datasets We conducted experiments on both English and Chinese datasets. For the English experiments, we utilized the HH-RLFH dataset (Bai et al.| 2022a} Ganguli et al} 2022), which comprises 118k helpful and 42k harmless examples for training, and 8.5k for testing. It is worth noting that many studies train different RMs separately for helpful and harmless examples to achieve better performance. However, in our experiments, we did not distinguish between helpful and harmless examples. For the Chinese dataset, we collected comparison examples with quantities similar to those used in LLaMA 2 (2023). Our annotation procedure operates as follows: First, we ask annotators to generate prompts based on a task spectrum. Next, we sample five responses from the same SFT model using varied sampling hyper-parameters. Finally, we distribute these responses to five annotators for ranking based on provided criteria. Following (2022ah, the annotation criteria focuses on helpfulness and harmless. PPO dataset We sampled queries from two popular domain-general datasts, cod and firefly] to form our PPO dataset. Particularly, we obtained 64,364 and 2,623 for PPO training and testing, respectively] There is no intersection between the training and testing sets. Additionally, we selected 1,704 examples from the SFT test data to create a forget test set, enabling us to evaluate the model’s ability to retain learned knowledge. Models We employed BLOOMZ (Muennighoff et al! as our pre-trained model backbone. More specifically, BLOOMZ7, was used for reward modeling and BLOOMZj7¢g was used for SFT and RLHF training. 4.2 TRAINING SETUPS We initialized our models using pre-trained checkpoints. The architectural configuration and hyperparameters were kept consistent with those of the pre-trained models, except that a value head is “https: //huggingfac o/datasets/YeungNLP/firefly-train-1.1M ie shared upon request. --- --HH-RLHF Proprietary Data Model Accuracy } ECE| Accuracy t ECE| OpenAssistant|K@pf et al.| (2023) 69.24 - - Reward Model 69.25 4.70 74.75 5.Advantage Model 69.43 3.48 75.28 3.Table 1: Evaluation results on HH-RLHF and our proprietary data. Note that maximizing accuracy is not the exclusive objective in AM optimization. The aim also extends to reducing ECE to improve reliability, whilst sustaining or improving the level of ranking accuracy compared with RM. added to produce a scalar reward. A learning rate of 5e-6 was employed, coupled with a warm-up strategy covering the initial 10% of training steps and a cosine learning rate schedule decreasing to 10% of the initial learning rate. For the English dataset, a global batch size of 180 was employed, whereas for the Chinese dataset, the batch size was set to 480. The Overfitting issue is observed in general after models are trained for one epoch. As such, we fixed the training epoch as | for the all the experiments.For PPO training, a learning rate of 5 x 10~7 and a global batch size of 256 is employed. The actor model is trained for 100 steps for all experiments. The SFT model is trained on the proprietary dataset. We omit these details since these are not the focus of this paper. 4.3 EVALUATION AM Evaluation Results Firstly, we present the overall accuracy and Expected Calibration Error (ECE) for both RM and AM on each dataset. For the English dataset, we additionally compare our method with the publicly available OpenAssistant (K6pf et al.||2023) which utilized DeBERTa for reward modeling. Table[2| sts all the results. We observe that AM achieves slightly accuracy but significantly lower ECE on all the datasets. This indicates that AM is capable of maintaining the same level of ranking accuracy while providing reliable and well-calibrated scores. A detailed analysis of calibrations is provided in the following sections. We attribute this phenomenon to the fact that AM is formulated to directly model additional rewards, i.e., advantages, making it more stable and less prone to yield high variances cores. Additionally, the accuracy on the proprietary data is much higher than that on HH-RLHF. We speculate that the trade-off between helpfulness and harmlessness objectives is more pronounced in HH-RLHF, possibly due to the limited presence of harmful examples in our proprietary data. Calibration on HH-RLHF Test Data Calibration on the Proprietary Test Data Accuracy ° © Accuracy ° © 0.7 0.an — Reward Model an — Reward Model — Perfect Calibration — Perfect Calibration — Advantange Model — Advantange Model 05 ost 0 ar 2 3 4 5 0 ar 2 3 4Score Diference Score Diference Figure 2: Ranking accuracy is shown as a function of the difference in scores between higher and lower ranked responses. The orange lines indicate the calibrated prediction of accuracy 1/(1+e~4) in which A denotes the score difference. On the left, we show calibration of RM and AM on HH-RLHF data while on the right we show results for our proprietary data. We observe that AM calibration is better than RM’s. Calibrations of AM The reward model score of a response should accurately reflect the probability that humans prefer it. These probabilities must be precise; in other words, the scores should be --- --© Fraction of Response -4 -2Score(a) RM score distribution. Good —— Bad -4 -2Score (b) AM score distribution.Figure 3: Distributions of RM and AM scores for pairs of good and bad examples from the propri etary data. well-calibrated. This is crucial since these ing|Bai et al] ranking accuracy as a function of score di representing perfect calibration is also inc significantly lower ECE and is better calib scores will serve as reward signals to guide PPO train . To assess whether our AM is calibrated or not, in Figure [2] we depict the ferences assigned to pairs of samples. An orange line luded. Our observations indicate that the AM exhibits rated than RM on both datasets, whereas RM tends to be overconfident in most cases. We further show the distribution of scores for both good and bad examples in Figure [3] While in general botl h RM and AM are able to assign higher scores for good examples, AM exhibits a more distinct distribution pattern. 2.0. Average Score 0. 3° rn MW lm Advantange Model mmm Reward Model ll10 20Task ID (a) Mean scores of RM and AM for each task. 40 503.2.std1.il ‘Advantange Model mmm Reward Model 0.o.0+ 0 10 20Task ID 40(b) Std of RM and AM for each task. Figure 4: Mean and standard variance for each task categorized by a task spectrum on the in-house data. Means and variances of AM _ During PPO training, RLHF exhibits instability, largely owing to unpredictable fluctuations in reward estimation scales. Directly modeling advantage, as our AM does, could potentially alleviate the above issue. To validate AM’s efficacy in stabilizing score scales and ranges, we calculated the AM scores for individual examples and analyzed the mean and variance across all the the task spectrum. This analysis is depicted in Figure [4a] We observe markedly different means for each task in the case of RM. Such significant disparities in means can potentially give rise to reward hacking issues (2022) and result in repeated failures during PPO training. In addition, Figure[4bjillustrates the standard deviations of both AM and RM, with AM consistently operating at a stable scale. These results endorse AM as a strategy designed to normalize reward scores at the individual example level while enhancing ranking accuracy. PPO training results We conducted a comparative analysis of PPO training with different scoring models in terms of their performance on both main test set and forget test set. The learning curve --- --oO oo a a eoON o a i A Rewards °Win/Lose Rate ° S oc Ss Fs & <4 RM-PPO —*— RM-PPO w/ MA CF 4 AM-PPO 0.47 4 —*—_RM-PPO 0.00 —4— AM-PPO-SR —*— AM-PPO 0 2 4 6 8 10 0 2 4 6 8Epoch Epoch (a) Learning curves of various models on delta re- (b) Win/Loss rate over SFT model evaluated by wards GPT-4. Figure 5: PPO training curves on the Main Test Set with different scoring models. RM-PPO and AM-PPO denote PPO trained with Reward Model and Advantage Model, respectively. AM-PPOSER additionally equips with Selective Rehearsal. Main Test Set Forget Test Set Wint Lose| Tie Win? Lose| Tie RM-PPO 12.72 12.62 74.66 16.87 29.28 53.AM-PPO 14.87 10.38 74.74 9.70 844 81.AM-PPO-SR 15.78 9.77 74.45 10.30 7.95 81.Model Table 2: Comparison results of different models over the SFT model. is shown in{5] We observe that AM-PPO outperformed RM-PPO in the main set, achieving higher rewards and a superior win rate over the SFT model. In addition, RM-PPO faces significant reward hacking issues, witnessed by a drop in win rate evaluated by GPT-4, shown in|5bjdespite a rise in RM scores. Despite utilizing moving average for score normalization, RM-PPO w/ MA encounters instabilities during PPO training. Conversely, AM-PPO exhibits resistance to such problems, maintaining stable GPT-4 outcomes. This emphasizes AM’s stability and alignment efficiency over RM. The forget test set result reveal RM-PPO’s substantial susceptibility to catastrophic forgetting, portraying a noticeable performance drop. In contrast, AM-PPO is stable, avoiding significant drops and showcasing stability. Incorporating selective rehearsal, the AM-PPO-SR variant demonstrate an uplifted win rate on both sets, underscoring the role of selective rehearsal in alleviating catastrophic forgetting and enhancing model efficacy. Analysis on Selective Rehearsal We also conduct an in-depth examination of the impact of the number of clusters, denoted as c, in the context of selective rehearsal during PPO training. As illustrated in Figure [6] our results reveal a relatively consistent variance of approximately 0.05 points in test-set rewards across various cluster numbers c. While our findings highlight the robustness of the selective rehearsal technique, we recommend conducting a thorough 9 2 4 Epoch 6 8 10 analysis of this aspect when applying selective rehearsal to different datasets, Figure 6: The AM-PPO-SR training curves on the 48 domain-specific variations can have a Main Test Set with different number of clustering table impact. groups c for selective rehearsal. 0.0.A Rewards ° N 0.0.--- --5
--- RELATED WORK ---
LLM Alignments with Human Preferences. LLMs are typically pre-trained on extensive datasets and can be adapted to a wide variety of downstream tasks. One critical aspect of utilizing LLMs effectively is ensuring their alignment with human preferences, which helps in averting responses that are unsafe, toxic, sexually explicit, biased, or criminal (2018). A predominant strategy in achieving this is RLHF. This involves training a reward model based on human feedback and utilizing PPO to improve to fine-tuning LLMs (Christiano et al.||2017| |Bai et al} 2022a) 2022) 2022b 2020) 2022). Instabilities in RLHF. Despite its success, the RLHF approach is inherently complex and poses significant challenges, thereby encouraging the exploration of simpler
--- METHOD ---
s not only increase stability in RLHF training but also achieve higher reward scores and win rates 1 INTRODUCTION Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, suc! as fabricating information or producing biased, toxic, or even dangerous content, since LLMs are trained on a wide array of data, which can include low-quality sources. This has highlighted the necessities of LLM Alignments with human values, intentions, and preferences (Brown et al.|/Ouyang et al. 2022} Bai et al. 2022a} Glaese et al. 2022). Many approaches have been put forth to address the challenge LLM Alignments (2021). Among these approaches, Reinforcement Learning from Human Feedback (RLHF) has demonstrated its efficacy in aligning language models with human preferences. RLHF serves as a key component of training SoTA LLMs including exemplars such as OpenAI’s GPT-4 (Open (2023), Anthropic’s Claude (Bai et al.|/2022a), Google’s Sparrow fet al. [2022), Bard, and Meta’s Llama 2-Chat (Touvron et al.[]2023). RLHF elevates the capabilities of LLMs beyond the mere modeling of the distribution of their training data. It endows LLMs with the capacity to adapt their text generation distribution in a manner that are preferred by humans. However, training LLMs using RLHF is undoubtedly challenging, which demands an accurate and reliable reward model that approximates human judges, and a robust PPO algorithm for sustained policy improvements. Even with meticulous configurations, instabilities, e.g., gibberish responses (but high-reward) (2022), forgetting learned knowledge, are usually observed during training, which leads to recurring failures. These instabilities have several causes: (i) different reward score distributions are learned for various categories by the reward model, potentially leading to reward hacking issues (2022), a phenomenon where the model finds unintended ways to maximize the reward. As depicted in Figure[Tal the reward model learns noticeable disparity in reward score distributions for Code Generation and QA tasks, 2 out of “Equal Contribution ‘Work in progress --- --Code Generation — Learned Score 0.—~a += + Expected Score Frequency —4— RM-PPO L 0 ne 7 3 4Vq3 21201234567 oon lo (b) Win rate over the SFT model on the forget set eval (a) Reward score distributions. uated by GPT-4. Figure 1: Left: The distribution of reward scores for both the QA and Code Generation tasks. There is a noticeable disparity in the learned reward score distributions between the two tasks, despite the expectation that the distributions should be similar. Right: The win/loss rate over the SFT model on the forget set exhibits a significant decline. This drop in the win rate can be attributed to reward hacking and the phenomenon of catastrophic forgetting. 61 tasks present in the preference data. Even with reward score normalizations, the fluctuating means and variances can induce unexpected model behaviors, such as transferring the response patterns of Code Generations to QA examples due to the higher reward scores. (ii) over-optimizing with PPO on examples that were well-aligned with humans in the Supervised Fine-Tuning (SFT) stage triggers catastrophic forgetting issues {1989} Gupta et al.| /2023} 2022). Models tend to overlook what was learned_during the SFT stage, i.e, PPO model underperforms the SFT model on expert-aligned examples[| as shown in Figure Accordingly, in this technical report, we introduce two techniques to enhance the stability and effectiveness of the training of RLHF. Firstly, we propose Advantage Model to balance the reward score distributions across various categories, thus averting the reward hacking dilemma that is often induced by noticeable differences score distributions. This is achieved by directly modeling the advantage score, i.e., the extra reward one response can obtain compared with the expected reward, and regulating the advantage score distribution dynamically during training, ensuring that the variances and means are maintained within a reasonable range. Secondly, we introduce the Selective Rehearsal to alleviate the catastrophic forgetting issue. We posit that not all data should be optimized equally in PPO training. As such, we propose a robust and effective data selector that automatically identifies what examples could be utilized for PPO training and should be used to rehearsal knowledge accumulated in the SFT stage, preventing the depreciation of the model’s performance on expertaligned examples over time.
--- EXPERIMENT ---
al analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates 1 INTRODUCTION Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, suc! as fabricating information or producing biased, toxic, or even dangerous content, since LLMs are trained on a wide array of data, which can include low-quality sources. This has highlighted the necessities of LLM Alignments with human values, intentions, and preferences (Brown et al.|/Ouyang et al. 2022} Bai et al. 2022a} Glaese et al. 2022). Many approaches have been put forth to address the challenge LLM Alignments (2021). Among these approaches, Reinforcement Learning from Human Feedback (RLHF) has demonstrated its efficacy in aligning language models with human preferences. RLHF serves as a key component of training SoTA LLMs including exemplars such as OpenAI’s GPT-4 (Open (2023), Anthropic’s Claude (Bai et al.|/2022a), Google’s Sparrow fet al. [2022), Bard, and Meta’s Llama 2-Chat (Touvron et al.[]2023). RLHF elevates the capabilities of LLMs beyond the mere modeling of the distribution of their training data. It endows LLMs with the capacity to adapt their text generation distribution in a manner that are preferred by humans. However, training LLMs using RLHF is undoubtedly challenging, which demands an accurate and reliable reward model that approximates human judges, and a robust PPO algorithm for sustained policy improvements. Even with meticulous configurations, instabilities, e.g., gibberish responses (but high-reward) (2022), forgetting learned knowledge, are usually observed during training, which leads to recurring failures. These instabilities have several causes: (i) different reward score distributions are learned for various categories by the reward model, potentially leading to reward hacking issues (2022), a phenomenon where the model finds unintended ways to maximize the reward. As depicted in Figure[Tal the reward model learns noticeable disparity in reward score distributions for Code Generation and QA tasks, 2 out of “Equal Contribution ‘Work in progress --- --Code Generation — Learned Score 0.—~a += + Expected Score Frequency —4— RM-PPO L 0 ne 7 3 4Vq3 21201234567 oon lo (b) Win rate over the SFT model on the forget set eval (a) Reward score distributions. uated by GPT-4. Figure 1: Left: The distribution of reward scores for both the QA and Code Generation tasks. There is a noticeable disparity in the learned reward score distributions between the two tasks, despite the expectation that the distributions should be similar. Right: The win/loss rate over the SFT model on the forget set exhibits a significant decline. This drop in the win rate can be attributed to reward hacking and the phenomenon of catastrophic forgetting. 61 tasks present in the preference data. Even with reward score normalizations, the fluctuating means and variances can induce unexpected model behaviors, such as transferring the response patterns of Code Generations to QA examples due to the higher reward scores. (ii) over-optimizing with PPO on examples that were well-aligned with humans in the Supervised Fine-Tuning (SFT) stage triggers catastrophic forgetting issues {1989} Gupta et al.| /2023} 2022). Models tend to overlook what was learned_during the SFT stage, i.e, PPO model underperforms the SFT model on expert-aligned examples[| as shown in Figure Accordingly, in this technical report, we introduce two techniques to enhance the stability and effectiveness of the training of RLHF. Firstly, we propose Advantage Model to balance the reward score distributions across various categories, thus averting the reward hacking dilemma that is often induced by noticeable differences score distributions. This is achieved by directly modeling the advantage score, i.e., the extra reward one response can obtain compared with the expected reward, and regulating the advantage score distribution dynamically during training, ensuring that the variances and means are maintained within a reasonable range. Secondly, we introduce the Selective Rehearsal to alleviate the catastrophic forgetting issue. We posit that not all data should be optimized equally in PPO training. As such, we propose a robust and effective data selector that automatically identifies what examples could be utilized for PPO training and should be used to rehearsal knowledge accumulated in the SFT stage, preventing the depreciation of the model’s performance on expertaligned examples over time. Experiments on both public and proprietary data have demonstrated that our Advantage Model successfully balances reward score distributions across various examples while preserves ranking precision, and guide PPO training to achieve a higher reward score and win rate compared to the SFT model. Furthermore, Selective Rehearsal is able to avoid over-optimizing by selecting the most suitable examples for PPO training, thereby sustaining the performance on expert-aligned examples. Our contributions are summarized as follows: ¢ We analyze and identify several causes of instability in RLHF training, namely, imbalanced learned reward score distributions and over-optimization of certain PPO training data, which lead to reward hacking and catastrophic forgetting issues. ¢ We introduce the Advantage Model to balance reward score distributions across various categories, and the Selective Rehearsal strategy to discern which examples should be used ?Expert-aligned Examples are data samples that meet the standards and criteria delineated by experts and closely align with human preferences. These examples are used for SFT model training and evaluation. --- --for PPO training and which should be reserved for rehearsing knowledge accrued in the SFT stage. ¢ Through extensive experiments on both public and proprietary datasets, we demonstrate that the Advantage Model and Selective Rehearsal are able to stabilize RLHF training, achieving higher reward scores and win rates. 2 PRELIMINARY In recent machine learning research, RLHF (Ouyang et al} 2022} Bai et al.| 2022a) has emerged as a pivotal strategy for aligning LLMs to human goals (e.g. being helpful and harmless). RLHF typically follows the SFT phase, where SFT aligns a LLM with human objectives using teacher forcing on (prompt, response) pairs. However, despite this alignment, the LLM may still struggle with generalization when faced with unseen tasks. Learning a reward function from interaction between LLMs and humans and optimizing LLMs with the learned reward function using reinforcement learning has been shown as an effective approach to solving the LLM alignment problem. proposed a method involving reinforcement learning from human feedback, where RMs are trained on a dataset of comparisons between two model outputs generated from the same input. The goal is to assign higher rewards to outputs preferred by human labelers over others. Typically, this is achieved by adding a value head that outputs a scalar value on pre-trained transformer-baesd LMs with last umembedding layer removed. Specifically, the reward modeling loss is as follows: Lem = —E(e.ye.y)~D™ llog(o(ro(2, Ye) — To(#, Yr)))] () where rg (x,y) denotes the reward score for prompt x and response y with parameters 6, y. is the preferred response of the pair y, and y,., and D™ is the complete of comparison dataset. In what follows, Proximal Policy Optimization (PPO) (Schulman et al.|/2017) is commonly adopted as the reinforcement learning algorithm to optimize a policy due to its strengths in stability and simplicity. Particularly, the PPO objective for policy 7 on a prompt dataset D is defined as: Lepo = Exn pryary (x) [Po(t,y) — Blog (m(yla)/7*™** (y|x)) | (2) where rg(x, y) represents the reward score on the (prompt, response) pair of (x, y); 7#""%* indicates the policy before RLHF, and it is kept constant during RLHF training; £ is the coefficient for the KL-divergence term. Besides PPO, rejection sampling 2023) recently gains interests as a simple way for aligning LLMs. As an offline policy learning algorithm, it adopts an iterative process. For each iteration n, it first constructs a new dataset D,, by selecting (x,y) pairs from the main policybased on criteria F: DP? = {(x,y) - F(x,y)| such that « ~ D°?,y ~ 4(x)} @) where a commonly used criteria F = 1,.,(z,y)>7 includes only the samples with RM scores exceed a certain threshold 7. The policy is then updated by teacher forcing on D?P°: lyl Les = Eve,y)~pe0 > T(Yelyct 2) (4) t=3 APPROACH 3.1 FROM REWARD MODEL TO ADVANTAGE MODEL The learning objective of equation |1| primarily allows models to distinguish between humanpreferred responses and alternative options. It relies only on score differences to assess the likelihood of one response being superior to another. In such case, two different model responses that are both preferred by humans could have dramatically different values. In addition, interpreting the scalar values themselves can be challenging. --- --In light of these considerations, we introduce the Advantage Model (AM) for reward modeling. Analogous to the concept of the advantage function in reinforcement learning, the Advantage Model, denoted as a(x, y), quantifies the additional reward that response y can achieve over the expected reward e for prompt x. This is formally defined as: gay"" y)] “ Here, the notation y ~ 7’(x) signifies all possible responses generated by a policy 7’(2) when given the input prompt x. Since the comparison data is typically collected in many batches with different ag(x,y) = r(x, y) — Eyrnr(2)| SFT or PPO models, we introduce oe , the importance weight term to negate the bias introduced by the policy distribution shift. Intuitively, the extra reward gains of good response y, and the reward losses of bad response y, should be bounded by a margin m. As such, the training objective of AM consists of two parts, ranking loss that aligns with the formulation in Equation|1| and bounding loss to ensure the well-calibrated bounding of AM scores. It is formally defined as follows: Lam = —E (eye .yr)~D™ log (o(aa(#, Ye) — a(x, Yr))) + log(o(m(x) —ao(x,ye))) + log(a(m(a) + ao(e,¥-)))] where m(: Fis the function that defines the permitted margin for prompt x. However, it is infeasible to list every potential response to calculate the expected reward. To address this, we propose parameterizing the expected reward of the current policy, denoted as: €7 (x) = Eyvng(x)[To(2,¥)] (7) (6) By integrating the term representing the importance weight, we can reformulate the equation as follows: K CJ ae(x,y) = ro(x,y) — SFKer(a) — 0 Hola, y) (8) k=where N serves as a hyperparameter that harmonizes the emphasis placed on the current policy model relative to alternate policy models. A’ specifies the number of alternate policy models utilized for comparison data collection. Additionally, 7), (y|2) indicates the probability derived from the kth policy model. 3.2 PPO WITH SELECTIVE REHEARSAL In addition, we propose Selective Rehearsal to maintain the skills that are already acquired before RLHF. Selective rehearsal takes two major steps: representative example discovery and rehearsal training. Representative example discovery Given the policy 7g and PPO training prompts with policy outputs DPP° = [(21, y1), (x2, y2) ...], our goal is to select high-quality (x,y) pairs from D?P° that cover as many skills (e.g., solving algebra problems and writing resume) as possible. In order to let selected (x, y) pairs represent as many skills as possible, we first adopt a clustering algorithm (e.g. KMeans or Gaussian mixture) to separate D??° into ¢ clusters. To assure the representativeness and quality of the selected data, we only keep certain (x,y) pairs within each cluster that satisfy certain criteria regarding aspects such as advantage (reward) model score, entropy (low entropy indicates high confidence), human satisfaction rate or response length (higher length may indicate redundancy). Here we adopt the SimCSE 2021) sentence embedding""|to represent the query x for each (x,y) pair before running a KMeans algorithm on these embeddings to be grouped into c clusters. We briefly study the influence of cluster number c in Section Within each cluster, here we simply choose the top-k (x,y) pairs with the highest advantage model score (Eq. other strategies (e.g. combining advantage score with entropy score) in future work. 3We think that m() may have a connection with the complexity or difficulty involved in learning the reward function for prompts similar to x. However, thi: culative and requires further investigation. We leave this --- --One reason we select our rehearsal data from the PPO training data with each response y being generated from the initial policy model is to enable a more fair and nuanced comparison, as no additional information is introduced. In other scenarios, the rehearsal (7, y) pairs could come from other important data sources representing specific skills (e.g. math-problem solving) the main policy are not expected to forget. Rehearsal training After obtaining the rehearsal («, y) pairs of all clusters, we shuffle them together to form the rehearsal dataset Dr and compute NLL loss on Dr as a supplement to the standard PPO loss defined in Equation [2} yl Lppo-sr = Lppo + VE(e,y)~Dp So rs(velyce: zr) (9) t=where the coefficient for the NLL loss ¥ is empirically set to 0.01. Rehearsal training is similar with rejection sampling and reinforced self-training (Gulcehre et al. ) by using self-generated ys of high reward model score for supervised training. However, re sal training captures multi-dimensional important aspects (e.g., diversity), while rejection sampling and reinforced self-training only consider reward model score. Alternatively, one can view selective rehearsal as a means of amplifying the weight of the KLdivergence term in PPO training (Eq. a) for crucial instances and their related counterparts. 4 EXPERIMENTS 4.1 DATASETS AND MODELS RM datasets We conducted experiments on both English and Chinese datasets. For the English experiments, we utilized the HH-RLFH dataset (Bai et al.| 2022a} Ganguli et al} 2022), which comprises 118k helpful and 42k harmless examples for training, and 8.5k for testing. It is worth noting that many studies train different RMs separately for helpful and harmless examples to achieve better performance. However, in our experiments, we did not distinguish between helpful and harmless examples. For the Chinese dataset, we collected comparison examples with quantities similar to those used in LLaMA 2 (2023). Our annotation procedure operates as follows: First, we ask annotators to generate prompts based on a task spectrum. Next, we sample five responses from the same SFT model using varied sampling hyper-parameters. Finally, we distribute these responses to five annotators for ranking based on provided criteria. Following (2022ah, the annotation criteria focuses on helpfulness and harmless. PPO dataset We sampled queries from two popular domain-general datasts, cod and firefly] to form our PPO dataset. Particularly, we obtained 64,364 and 2,623 for PPO training and testing, respectively] There is no intersection between the training and testing sets. Additionally, we selected 1,704 examples from the SFT test data to create a forget test set, enabling us to evaluate the model’s ability to retain learned knowledge. Models We employed BLOOMZ (Muennighoff et al! as our pre-trained model backbone. More specifically, BLOOMZ7, was used for reward modeling and BLOOMZj7¢g was used for SFT and RLHF training. 4.2 TRAINING SETUPS We initialized our models using pre-trained checkpoints. The architectural configuration and hyperparameters were kept consistent with those of the pre-trained models, except that a value head is “https: //huggingfac o/datasets/YeungNLP/firefly-train-1.1M ie shared upon request. --- --HH-RLHF Proprietary Data Model Accuracy } ECE| Accuracy t ECE| OpenAssistant|K@pf et al.| (2023) 69.24 - - Reward Model 69.25 4.70 74.75 5.Advantage Model 69.43 3.48 75.28 3.Table 1: Evaluation results on HH-RLHF and our proprietary data. Note that maximizing accuracy is not the exclusive objective in AM optimization. The aim also extends to reducing ECE to improve reliability, whilst sustaining or improving the level of ranking accuracy compared with RM. added to produce a scalar reward. A learning rate of 5e-6 was employed, coupled with a warm-up strategy covering the initial 10% of training steps and a cosine learning rate schedule decreasing to 10% of the initial learning rate. For the English dataset, a global batch size of 180 was employed, whereas for the Chinese dataset, the batch size was set to 480. The Overfitting issue is observed in general after models are trained for one epoch. As such, we fixed the training epoch as | for the all the experiments.For PPO training, a learning rate of 5 x 10~7 and a global batch size of 256 is employed. The actor model is trained for 100 steps for all experiments. The SFT model is trained on the proprietary dataset. We omit these details since these are not the focus of this paper. 4.3 EVALUATION AM Evaluation Results Firstly, we present the overall accuracy and Expected Calibration Error (ECE) for both RM and AM on each dataset. For the English dataset, we additionally compare our method with the publicly available OpenAssistant (K6pf et al.||2023) which utilized DeBERTa for reward modeling. Table[2| sts all the results. We observe that AM achieves slightly accuracy but significantly lower ECE on all the datasets. This indicates that AM is capable of maintaining the same level of ranking accuracy while providing reliable and well-calibrated scores. A detailed analysis of calibrations is provided in the following sections. We attribute this phenomenon to the fact that AM is formulated to directly model additional rewards, i.e., advantages, making it more stable and less prone to yield high variances cores. Additionally, the accuracy on the proprietary data is much higher than that on HH-RLHF. We speculate that the trade-off between helpfulness and harmlessness objectives is more pronounced in HH-RLHF, possibly due to the limited presence of harmful examples in our proprietary data. Calibration on HH-RLHF Test Data Calibration on the Proprietary Test Data Accuracy ° © Accuracy ° © 0.7 0.an — Reward Model an — Reward Model — Perfect Calibration — Perfect Calibration — Advantange Model — Advantange Model 05 ost 0 ar 2 3 4 5 0 ar 2 3 4Score Diference Score Diference Figure 2: Ranking accuracy is shown as a function of the difference in scores between higher and lower ranked responses. The orange lines indicate the calibrated prediction of accuracy 1/(1+e~4) in which A denotes the score difference. On the left, we show calibration of RM and AM on HH-RLHF data while on the right we show results for our proprietary data. We observe that AM calibration is better than RM’s. Calibrations of AM The reward model score of a response should accurately reflect the probability that humans prefer it. These probabilities must be precise; in other words, the scores should be --- --© Fraction of Response -4 -2Score(a) RM score distribution. Good —— Bad -4 -2Score (b) AM score distribution.Figure 3: Distributions of RM and AM scores for pairs of good and bad examples from the propri etary data. well-calibrated. This is crucial since these ing|Bai et al] ranking accuracy as a function of score di representing perfect calibration is also inc significantly lower ECE and is better calib scores will serve as reward signals to guide PPO train . To assess whether our AM is calibrated or not, in Figure [2] we depict the ferences assigned to pairs of samples. An orange line luded. Our observations indicate that the AM exhibits rated than RM on both datasets, whereas RM tends to be overconfident in most cases. We further show the distribution of scores for both good and bad examples in Figure [3] While in general botl h RM and AM are able to assign higher scores for good examples, AM exhibits a more distinct distribution pattern. 2.0. Average Score 0. 3° rn MW lm Advantange Model mmm Reward Model ll10 20Task ID (a) Mean scores of RM and AM for each task. 40 503.2.std1.il ‘Advantange Model mmm Reward Model 0.o.0+ 0 10 20Task ID 40(b) Std of RM and AM for each task. Figure 4: Mean and standard variance for each task categorized by a task spectrum on the in-house data. Means and variances of AM _ During PPO training, RLHF exhibits instability, largely owing to unpredictable fluctuations in reward estimation scales. Directly modeling advantage, as our AM does, could potentially alleviate the above issue. To validate AM’s efficacy in stabilizing score scales and ranges, we calculated the AM scores for individual examples and analyzed the mean and variance across all the the task spectrum. This analysis is depicted in Figure [4a] We observe markedly different means for each task in the case of RM. Such significant disparities in means can potentially give rise to reward hacking issues (2022) and result in repeated failures during PPO training. In addition, Figure[4bjillustrates the standard deviations of both AM and RM, with AM consistently operating at a stable scale. These results endorse AM as a strategy designed to normalize reward scores at the individual example level while enhancing ranking accuracy. PPO training results We conducted a comparative analysis of PPO training with different scoring models in terms of their performance on both main test set and forget test set. The learning curve --- --oO oo a a eoON o a i A Rewards °Win/Lose Rate ° S oc Ss Fs & <4 RM-PPO —*— RM-PPO w/ MA CF 4 AM-PPO 0.47 4 —*—_RM-PPO 0.00 —4— AM-PPO-SR —*— AM-PPO 0 2 4 6 8 10 0 2 4 6 8Epoch Epoch (a) Learning curves of various models on delta re- (b) Win/Loss rate over SFT model evaluated by wards GPT-4. Figure 5: PPO training curves on the Main Test Set with different scoring models. RM-PPO and AM-PPO denote PPO trained with Reward Model and Advantage Model, respectively. AM-PPOSER additionally equips with Selective Rehearsal. Main Test Set Forget Test Set Wint Lose| Tie Win? Lose| Tie RM-PPO 12.72 12.62 74.66 16.87 29.28 53.AM-PPO 14.87 10.38 74.74 9.70 844 81.AM-PPO-SR 15.78 9.77 74.45 10.30 7.95 81.Model Table 2: Comparison results of different models over the SFT model. is shown in{5] We observe that AM-PPO outperformed RM-PPO in the main set, achieving higher rewards and a superior win rate over the SFT model. In addition, RM-PPO faces significant reward hacking issues, witnessed by a drop in win rate evaluated by GPT-4, shown in|5bjdespite a rise in RM scores. Despite utilizing moving average for score normalization, RM-PPO w/ MA encounters instabilities during PPO training. Conversely, AM-PPO exhibits resistance to such problems, maintaining stable GPT-4 outcomes. This emphasizes AM’s stability and alignment efficiency over RM. The forget test set result reveal RM-PPO’s substantial susceptibility to catastrophic forgetting, portraying a noticeable performance drop. In contrast, AM-PPO is stable, avoiding significant drops and showcasing stability. Incorporating selective rehearsal, the AM-PPO-SR variant demonstrate an uplifted win rate on both sets, underscoring the role of selective rehearsal in alleviating catastrophic forgetting and enhancing model efficacy. Analysis on Selective Rehearsal We also conduct an in-depth examination of the impact of the number of clusters, denoted as c, in the context of selective rehearsal during PPO training. As illustrated in Figure [6] our results reveal a relatively consistent variance of approximately 0.05 points in test-set rewards across various cluster numbers c. While our findings highlight the robustness of the selective rehearsal technique, we recommend conducting a thorough 9 2 4 Epoch 6 8 10 analysis of this aspect when applying selective rehearsal to different datasets, Figure 6: The AM-PPO-SR training curves on the 48 domain-specific variations can have a Main Test Set with different number of clustering table impact. groups c for selective rehearsal. 0.0.A Rewards ° N 0.0.--- --5 RELATED WORK LLM Alignments with Human Preferences. LLMs are typically pre-trained on extensive datasets and can be adapted to a wide variety of downstream tasks. One critical aspect of utilizing LLMs effectively is ensuring their alignment with human preferences, which helps in averting responses that are unsafe, toxic, sexually explicit, biased, or criminal (2018). A predominant strategy in achieving this is RLHF. This involves training a reward model based on human feedback and utilizing PPO to improve to fine-tuning LLMs (Christiano et al.||2017| |Bai et al} 2022a) 2022) 2022b 2020) 2022). Instabilities in RLHF. Despite its success, the RLHF approach is inherently complex and poses significant challenges, thereby encouraging the exploration of simpler methods to align LLMs with human preferences. In this context,|Cobbe et al.|(2021) introduced the best-of-n sampling, which reinforces LLMs by choosing the responses with the highest reward score from a set of n responses. A similar pathway was pursued by RAFT (Dong et al.||2023), which focuses on selecting high-quality samples to fine-tuning to enhance the model’s performance. Moreover, the RRHF strategy evaluates sampled responses from various sources using the logarithm of conditional probabilities. It then aligns these probabilities with human preferences by applying ranking loss, fostering a more refined alignment process. Furthermore, [Rafailov et al.| (2023) introduced the concept of Direct Preference Optimization (DPO). This approach leverages a relationship between reward functions and optimal policies to address a constrained reward maximization problem through a single stage of policy training. In a similar vein, Preference Ranking Optimization (PRO) sidesteps the necessity for Reinforcement Learning (RL) training. Instead, it directly aligns LLMs with human preferences using the Bradley-Terry comparison — a method that involves the probability ranking of n responses generated by the LLM, ensuring they are consistent with human preference rankings. Data Curation for LLM Alignments. Many approaches have been devised to curate high-quality, instruction-following datasets to fine-tune LLMs (Wang et al.|/2022}/2023}/Taori et al.||2023 2023). For instance, the study by LIMA 2023) underscores at even a limited setcarefully curated and high-quality examples can be utilized to fine-tune a strong pre-trained language model, enabling it to deliver competitive results across a diverse array of prompts. Similarly, [Wei et al.| (2023) introduced a versatile and straightforward data selector designed to autonomously curate a subset from the original fine-tuning dataset, adhering to specific principles for training vision-language models. While these strategies converge on the shared objective of data curation for LLM fine-tuning, our approach is uniquely centered on data curation for PPO training. This strategy diverges fundamentally from others that emphasize the SFT stage, thereby addressing a distinct problem. 6
--- CONCLUSION ---
In this report, we identified and analyzied critical impediments in RLHF training of LLMs, namely reward hacking and catastrophic forgetting. These issues emerge due to the variances in learned reward score distributions and the over-optimization of specific training examples, resulting in instabilities in RLHF training. To alleviate these issues, we introduced the Advantage Model and Selective Rehearsal—innovative strategies formulated to stabilize the RLHF training process. The Advantage Model aims to maintain balanced reward score distributions across diverse categories and examples, thereby averting complications arising from reward hacking. On the other hand, Selective Rehearsal selectively identifies optimal examples for PPO training, ptimal examples for PPO training, encouraging the retention of crucial knowledge from the SFT stage, and preventing the depreciation of performance over time. Empirical analyses conducted on a range of datasets substantiated the efficacy of our proposed techniques, which not only enhanced stability in RLHF training but also led to improved reward scores and win rates the SFT models. REFERENCES Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory --- --for alignment. arXiv preprint arXiv:2112.00861, 2021. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL|https:| //lmsys.org/blog/2023-03-30-vicuna/ Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894-6910, 2021. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209. 14375, 2022. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. Continual pre-training of large language models: How to (re) warm your model? arXiv preprint arXiv:2308.04014, 2023. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning: A review and perspectives. Journal of Artificial Intelligence Research, 75:14011476, 2022. Andreas Kopf, Yannic Kilcher, Dimitri von Riitte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. Openassistant conversations—democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.--- --Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv: 1811.07871, 2018. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165. Elsevier, 1989. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. R OpenAL. Gpt-4 technical report. arXiv, pp. 2303-08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu. Valuenet: A new dataset for human value driven dialogue system. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11183-11191, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460-9471, 2022. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067, 2023.--- --Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
"	"--- ABSTRACT ---
대규모 언어 모델(LLM)은 자연어 처리에 혁명을 일으켰지만, RLHF를 사용하여 이러한 모델을 인간의 가치와 선호도에 맞추는 것은 여전히 중요한 과제입니다. 이 과제는 보상 해킹 및 재앙적 망각과 같은 다양한 불안정성을 특징으로 합니다. 이 기술 보고서에서 우리는 RLHF 훈련을 안정화하기 위한 두 가지 혁신을 제안합니다. (i) 이점 모델, 즉 예상 보상에 비해 추가 보상인 이점 점수를 직접 모델링하고 보상 해킹을 방지하기 위해 작업 전체에 걸쳐 점수 분포를 조절합니다. (ii) 선택적 리허설, PPO 훈련 및 지식 리허설을 위한 데이터를 전략적으로 선택하여 재앙적 망각을 완화합니다. 공개 및 독점 데이터 세트에 대한 실험 분석 결과, 제안된 방법은 RLHF 훈련의 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성합니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 자연어 처리(NLP)와 인공 지능(AI)을 발전시키는 데 기본적인 요소가 되었으며, 의미적으로나 맥락적으로 모두 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 학습되기 때문에 정보를 조작하거나 편향적이거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 관여할 위험이 있습니다. 이는 인간의 가치, 의도 및 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬 과제를 해결하기 위해 많은 접근법이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근법 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도에 맞추는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 예시를 포함하여 SoTA LLM을 훈련하는 데 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 단순히 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 응답(하지만 높은 보상)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되어 반복적인 실패로 이어집니다. 이러한 불안정성에는 여러 가지 원인이 있습니다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)로 이어질 수 있습니다. 이는 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상입니다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다. 공개 및 독점 데이터에 대한 실험은 Advantage Model이 다양한 예제에서 보상 점수 분포를 성공적으로 균형 잡는 동시에 순위 정밀도를 유지하고, SFT 모델에 비해 더 높은 보상 점수와 승률을 달성하도록 PPO 훈련을 안내한다는 것을 보여주었습니다. 나아가, Selective Rehearsal은 PPO 훈련에 가장 적합한 예제를 선택하여 과도한 최적화를 피할 수 있으므로 전문가 중심 예제에서 성과를 유지할 수 있습니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • RLHF 훈련의 불안정성의 여러 원인을 분석하고 식별합니다. 즉, 보상 해킹 및 치명적인 망각 문제로 이어지는 불균형 학습된 보상 점수 분포와 특정 PPO 훈련 데이터의 과도한 최적화입니다. • 다양한 범주에서 보상 점수 분포를 균형 잡기 위해 Advantage Model을 도입하고, 어떤 예제를 사용해야 하는지 분별하기 위해 Selective Rehearsal 전략을 도입합니다. 2전문가 중심 예제는 전문가가 정한 표준 및 기준을 충족하고 인간의 선호도와 긴밀하게 일치하는 데이터 샘플입니다. 이러한 예는 SFT 모델 학습 및 평가, PPO 학습에 사용되며 SFT 단계에서 축적된 지식을 연습하는 데 사용해야 합니다.• 공개 및 독점 데이터 세트에 대한 광범위한 실험을 통해 Advantage Model과 Selective Rehearsal이 RLHF 학습을 안정화하여 더 높은 보상 점수와 승률을 달성할 수 있음을 보여줍니다.예비 최근 머신 러닝 연구에서 RLHF(Ouyang et al., 2022; Bai et al., 2022a)는 LLM을 인간의 목표(예: 도움이 되고 무해함)에 맞추기 위한 핵심 전략으로 등장했습니다.RLHF는 일반적으로 SFT 단계를 따르며, SFT는 (프롬프트, 응답) 쌍에 대한 교사 강제를 사용하여 LLM을 인간의 목표에 맞춥니다.그러나 이러한 정렬에도 불구하고 LLM은 보이지 않는 작업에 직면했을 때 일반화에 어려움을 겪을 수 있습니다.LLM과 인간 간의 상호 작용에서 보상 함수를 학습하고 강화 학습을 사용하여 학습된 보상 함수로 LLM을 최적화하는 것은 LLM 정렬 문제를 해결하는 효과적인 접근 방식으로 나타났습니다. Leike et al. 2018; Stiennon et al. 2020; Ouyang et al.은 인간 피드백에서 강화 학습을 포함하는 방법을 제안했는데, 여기서 RM은 동일한 입력에서 생성된 두 모델 출력 간의 비교 데이터 세트에서 훈련됩니다. 목표는 다른 사람보다 인간 레이블러가 선호하는 출력에 더 높은 보상을 할당하는 것입니다. 일반적으로 이는 마지막 umembedding 레이어가 제거된 사전 훈련된 변압기 기반 LM에 스칼라 값을 출력하는 값 헤드를 추가하여 달성됩니다. 구체적으로 보상 모델링 손실은 다음과 같습니다.LRM = E(x,ye, yr)~DRM(log(σ(ro(x, Yc) - ro(x, yr)))] (1) 여기서 re(x, y)는 매개변수 0을 갖는 프롬프트 x와 응답 y에 대한 보상 점수를 나타내고, ye는 쌍 yɩ와 yr의 선호되는 응답이며, DRM은 전체 비교 데이터 세트입니다.다음과 같이, 안정성과 단순성 면에서 강점을 보이는 Proximal Policy Optimization(PPO)(Schulman et al., 2017)은 정책을 최적화하는 강화 학습 알고리즘으로 일반적으로 채택됩니다.특히, 프롬프트 데이터 세트 D에 대한 정책 π에 대한 PPO 목적은 다음과 같이 정의됩니다.LPPO = Ex~DPPO, y~π(x) [10(x, y) - ßlog (76(yx)/πinit (yx))] 여기서 rø(x, y)는 (프롬프트, 응답) (x, y) 쌍; init은 RLHF 이전의 정책을 나타내며 RLHF 학습 동안 일정하게 유지됩니다. ẞ는 KL-발산 항에 대한 계수입니다. PPO 외에도 기각 샘플링(Touvron et al., 2023)은 최근 LLM을 정렬하는 간단한 방법으로 관심을 얻고 있습니다. 오프라인 정책 학습 알고리즘으로 반복적 프로세스를 채택합니다. 각 반복 n에 대해 먼저 기준 F에 따라 주 정책 π에서 (x, y) 쌍을 선택하여 새 데이터 집합 Dn을 구성합니다. DPPO = . n = {(x, y) F(x, y) 여기서 x ~ DPPO , Y ~ π(x)} = 1ro (x, y)&gt;T (3) 여기서 일반적으로 사용되는 기준 F에는 RM 점수가 특정 임계값을 초과하는 샘플만 포함됩니다. 7. 그런 다음 정책은 DPPO에 대한 교사 강제로 업데이트됩니다. |y| LRS = E(x,y)~DPPO T¢(Yt\Y
--- RELATED WORK ---
LLM과 인간의 선호도의 일치. LLM은 일반적으로 광범위한 데이터 세트에서 사전 학습되며 다양한 다운스트림 작업에 적용할 수 있습니다. LLM을 효과적으로 활용하는 데 있어 중요한 측면 중 하나는 인간의 선호도와 일치하도록 보장하는 것입니다. 이는 안전하지 않거나, 독성이 있거나, 성적으로 노골적이거나, 편향적이거나, 범죄적인 반응을 피하는 데 도움이 됩니다(Leike et al., 2018). 이를 달성하기 위한 주요 전략은 RLHF입니다. 여기에는 인간의 피드백을 기반으로 보상 모델을 학습하고 PPO를 활용하여 LLM을 미세 조정하는 것이 포함됩니다(Christiano et al., 2017; Bai et al., 2022a; Glaese et al., 2022; Bai et al., 2022b; Stiennon et al., 2020; Qiu et al., 2022). RLHF의 불안정성. 성공에도 불구하고 RLHF 접근 방식은 본질적으로 복잡하고 상당한 과제를 안겨주므로 더 간단한
--- METHOD ---
s는 RLHF 훈련에서 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성합니다.서론 대규모 언어 모델(LLM)은 자연어 처리(NLP)와 인공 지능(AI)을 발전시키는 데 기본적인 요소가 되었으며, 의미적으로나 맥락적으로 모두 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 훈련되기 때문에 정보를 조작하거나 편향되거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 관여할 위험이 있습니다. 이는 인간의 가치, 의도, 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬의 과제를 해결하기 위해 많은 접근 방식이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근 방식 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도와 정렬하는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 모범 사례를 포함한 SoTA LLM을 훈련하는 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 반응(하지만 보상이 높음)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되며, 이는 반복적인 실패로 이어진다. 이러한 불안정성에는 여러 가지 원인이 있다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)를 야기할 수 있다. 보상 해킹은 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상이다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다.
--- EXPERIMENT ---
공개 및 독점 데이터 세트에 대한 al 분석은 제안된 방법이 RLHF 훈련의 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성한다는 것을 보여줍니다.서론 대규모 언어 모델(LLM)은 자연어 처리(NLP) 및 인공 지능(AI)을 발전시키는 기본 요소가 되었으며, 의미적으로나 맥락적으로 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 훈련되기 때문에 정보를 조작하거나 편향되거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 참여할 위험이 있습니다. 이는 인간의 가치, 의도, 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬의 과제를 해결하기 위해 많은 접근 방식이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근 방식 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도와 정렬하는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 모범 사례를 포함한 SoTA LLM을 훈련하는 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 반응(하지만 보상이 높음)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되며, 이는 반복적인 실패로 이어진다. 이러한 불안정성에는 여러 가지 원인이 있다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)를 야기할 수 있다. 보상 해킹은 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상이다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다. 공개 및 독점 데이터에 대한 실험은 Advantage Model이 다양한 예제에서 보상 점수 분포를 성공적으로 균형 잡는 동시에 순위 정밀도를 유지하고, SFT 모델에 비해 더 높은 보상 점수와 승률을 달성하도록 PPO 훈련을 안내한다는 것을 보여주었습니다. 나아가, Selective Rehearsal은 PPO 훈련에 가장 적합한 예제를 선택하여 과도한 최적화를 피할 수 있으므로 전문가 중심 예제에서 성과를 유지할 수 있습니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • RLHF 훈련의 불안정성의 여러 원인을 분석하고 식별합니다. 즉, 보상 해킹 및 치명적인 망각 문제로 이어지는 불균형 학습된 보상 점수 분포와 특정 PPO 훈련 데이터의 과도한 최적화입니다. • 다양한 범주에서 보상 점수 분포를 균형 잡기 위해 Advantage Model을 도입하고, 어떤 예제를 사용해야 하는지 분별하기 위해 Selective Rehearsal 전략을 도입합니다. 2전문가 중심 예제는 전문가가 정한 표준 및 기준을 충족하고 인간의 선호도와 긴밀하게 일치하는 데이터 샘플입니다. 이러한 예는 SFT 모델 학습 및 평가, PPO 학습에 사용되며 SFT 단계에서 축적된 지식을 연습하는 데 사용해야 합니다.• 공개 및 독점 데이터 세트에 대한 광범위한 실험을 통해 Advantage Model과 Selective Rehearsal이 RLHF 학습을 안정화하여 더 높은 보상 점수와 승률을 달성할 수 있음을 보여줍니다.예비 최근 머신 러닝 연구에서 RLHF(Ouyang et al., 2022; Bai et al., 2022a)는 LLM을 인간의 목표(예: 도움이 되고 무해함)에 맞추기 위한 핵심 전략으로 등장했습니다.RLHF는 일반적으로 SFT 단계를 따르며, SFT는 (프롬프트, 응답) 쌍에 대한 교사 강제를 사용하여 LLM을 인간의 목표에 맞춥니다.그러나 이러한 정렬에도 불구하고 LLM은 보이지 않는 작업에 직면했을 때 일반화에 어려움을 겪을 수 있습니다.LLM과 인간 간의 상호 작용에서 보상 함수를 학습하고 강화 학습을 사용하여 학습된 보상 함수로 LLM을 최적화하는 것은 LLM 정렬 문제를 해결하는 효과적인 접근 방식으로 나타났습니다. Leike et al. 2018; Stiennon et al. 2020; Ouyang et al.은 인간 피드백에서 강화 학습을 포함하는 방법을 제안했는데, 여기서 RM은 동일한 입력에서 생성된 두 모델 출력 간의 비교 데이터 세트에서 훈련됩니다. 목표는 다른 사람보다 인간 레이블러가 선호하는 출력에 더 높은 보상을 할당하는 것입니다. 일반적으로 이는 마지막 umembedding 레이어가 제거된 사전 훈련된 변압기 기반 LM에 스칼라 값을 출력하는 값 헤드를 추가하여 달성됩니다. 구체적으로 보상 모델링 손실은 다음과 같습니다.LRM = E(x,ye, yr)~DRM(log(σ(ro(x, Yc) - ro(x, yr)))] (1) 여기서 re(x, y)는 매개변수 0을 갖는 프롬프트 x와 응답 y에 대한 보상 점수를 나타내고, ye는 쌍 yɩ와 yr의 선호되는 응답이며, DRM은 전체 비교 데이터 세트입니다.다음과 같이, 안정성과 단순성 면에서 강점을 보이는 Proximal Policy Optimization(PPO)(Schulman et al., 2017)은 정책을 최적화하는 강화 학습 알고리즘으로 일반적으로 채택됩니다.특히, 프롬프트 데이터 세트 D에 대한 정책 π에 대한 PPO 목적은 다음과 같이 정의됩니다.LPPO = Ex~DPPO, y~π(x) [10(x, y) - ßlog (76(yx)/πinit (yx))] 여기서 rø(x, y)는 (프롬프트, 응답) (x, y) 쌍; init은 RLHF 이전의 정책을 나타내며 RLHF 학습 동안 일정하게 유지됩니다. ẞ는 KL-발산 항에 대한 계수입니다. PPO 외에도 기각 샘플링(Touvron et al., 2023)은 최근 LLM을 정렬하는 간단한 방법으로 관심을 얻고 있습니다. 오프라인 정책 학습 알고리즘으로 반복적 프로세스를 채택합니다. 각 반복 n에 대해 먼저 기준 F에 따라 주 정책 π에서 (x, y) 쌍을 선택하여 새 데이터 집합 Dn을 구성합니다. DPPO = . n = {(x, y) F(x, y) 여기서 x ~ DPPO , Y ~ π(x)} = 1ro (x, y)&gt;T (3) 여기서 일반적으로 사용되는 기준 F에는 RM 점수가 특정 임계값을 초과하는 샘플만 포함됩니다. 7. 그런 다음 정책은 DPPO에 대한 교사 강제로 업데이트됩니다. |y| LRS = E(x,y)~DPPO T¢(Yt\Y
--- CONCLUSION ---
이 보고서에서 우리는 LLM의 RLHF 훈련에서 중요한 장애물인 보상 해킹과 치명적 망각을 식별하고 분석했습니다. 이러한 문제는 학습된 보상 점수 분포의 분산과 특정 훈련 사례의 과도한 최적화로 인해 발생하여 RLHF 훈련이 불안정해집니다. 이러한 문제를 완화하기 위해 우리는 RLHF 훈련 프로세스를 안정화하기 위해 고안된 혁신적인 전략인 Advantage Model과 Selective Rehearsal을 도입했습니다. Advantage Model은 다양한 범주와 사례에 걸쳐 균형 잡힌 보상 점수 분포를 유지하여 보상 해킹으로 인해 발생하는 합병증을 방지하는 것을 목표로 합니다. 반면, Selective Rehearsal은 PPO 훈련에 대한 최적의 사례, PPO 훈련에 대한 최적의 사례를 선택적으로 식별하여 SFT 단계에서 중요한 지식을 유지하도록 장려하고 시간이 지남에 따라 성과가 저하되는 것을 방지합니다. 다양한 데이터 세트에 대해 수행된 실증 분석은 제안된 기술의 효능을 입증했으며, 이는 RLHF 훈련의 안정성을 향상시켰을 뿐만 아니라 SFT 모델의 보상 점수와 승률도 개선했습니다. 참고 문헌 Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma 등. 정렬을 위한 실험실로서의 일반 언어 보조원. arXiv 사전 인쇄 arXiv:2112.00861, 2021. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 인간 피드백을 통한 강화 학습을 통해 유용하고 무해한 어시스턴트를 훈련합니다. arXiv 사전 인쇄 arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon 등 헌법적 AI: AI 피드백의 무해성. arXiv 사전 인쇄본 arXiv:2212.08073, 2022b. 톰 브라운, 벤자민 맨, 닉 라이더, 멜라니 수비아, 재러드 D 카플란, 프라풀라 다리왈, 아빈드 닐라칸탄, 프라나브 샤얌, 기리쉬 사스트리, 아만다 애스켈, 외. 언어 모델은 소수 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. 웨이린 치앙, 주오한 리, 지 린, 잉 셩, 장하오 우, 하오 장, 리안민 정, 시위안 셩, 용하오 셩, 조셉 E. 곤잘레스, 이온 스토이카, 에릭 P. 싱. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시킨 오픈소스 챗봇, 2023년 3월. URL https://lmsys.org/blog/2023-03-30-vicuna/. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei. 인간의 선호도에서 심층 강화 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 수학 단어 문제를 풀기 위한 검증자 훈련. arXiv 사전 인쇄본 arXiv:2110.14168, 2021. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang. Raft: 생성 기반 모델 정렬을 위한 보상 순위 조정. arXiv 사전 인쇄본 arXiv:2304.06767, 2023. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 피해를 줄이기 위한 레드 팀 언어 모델: 방법, 스케일링 동작 및 얻은 교훈. arXiv 사전 인쇄본 arXiv:2209.07858, 2022. Tianyu Gao, Xingcheng Yao, Danqi Chen. Simcse: 문장 임베딩의 단순 대조 학습. 2021년 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 6894-6910쪽, 2021. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 타깃 인간 판단을 통해 대화 에이전트의 정렬 개선. arXiv 사전 인쇄본 arXiv:2209.14375, 2022. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 언어 모델링을 위한 강화된 자기 학습(휴식). arXiv 사전 인쇄본 arXiv:2308.08998, 2023. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort. 대규모 언어 모델의 지속적인 사전 학습: 모델을 (재)워밍하는 방법? arXiv 사전 인쇄본 arXiv:2308.04014, 2023. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. Deberta: disentangled attention을 사용한 디코딩 강화 bert. arXiv 사전 인쇄본 arXiv:2006.03654, 2020. Khimya Khetarpal, Matthew Riemer, Irina Rish, Doina Precup. 지속적인 강화 학습을 향하여: 리뷰와 관점. 인공지능 연구 저널, 75:14011476, 2022. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-대규모 언어 모델 정렬 민주화. arXiv 사전 인쇄본 arXiv:2304.07327, 2023. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg. 보상 모델링을 통한 확장 가능한 에이전트 정렬: 연구 방향. arXiv 사전 인쇄본 arXiv:1811.07871, 2018. Michael McCloskey와 Neal J Cohen. 연결주의 네트워크에서의 재앙적 간섭: 순차적 학습 문제. Psychology of learning and motivation, 24권, 109-165쪽. Elsevier, 1989. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 멀티태스크 미세 조정을 통한 교차 언어 일반화. arXiv 사전 인쇄본 arXiv:2211.01786, 2022. R OpenAI. Gpt-4 기술 보고서. arXiv, pp. 2303-08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35: 27730-27744, 2022. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. gpt-4를 사용한 지시 튜닝. arXiv 사전 인쇄본 arXiv:2304.03277, 2023. Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu. Valuenet: 인간의 가치 중심 대화 시스템을 위한 새로운 데이터 세트. AAAI 인공지능 컨퍼런스 회의록, 36권, 11183-11191쪽, 2022년. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. arXiv 사전 인쇄본 arXiv:2305.18290, 2023년. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 근접 정책 최적화 알고리즘. arXiv 사전 인쇄본 arXiv:1707.06347, 2017년. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, David Krueger. 보상 게임 정의 및 특성화. 신경 정보 처리 시스템의 발전, 35:9460–9471, 2022. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang. 인간 정렬을 위한 선호도 순위 최적화. arXiv 사전 인쇄본 arXiv:2306.17492, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano. 인간 피드백을 통한 요약 학습. 신경 정보 처리 시스템의 발전, 33:3008-3021, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 라마 2: 오픈 기반 및 미세 조정된 채팅 모델. arXiv 사전 인쇄본 arXiv:2307.09288, 2023. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi. 자체 지시: 언어 모델을 자체 생성 지시와 정렬. arXiv 사전 인쇄본 arXiv:2212.10560, 2022. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 낙타는 얼마나 멀리 갈 수 있을까? 오픈 리소스에 대한 명령어 튜닝 상태 탐구. arXiv 사전 인쇄본 arXiv:2306.04751, 2023. Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun. Instructiongpt-4: minigpt-4 미세 조정을 위한 200개 명령어 패러다임. arXiv 사전 인쇄본 arXiv:2308.12067, 2023. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang. Rrhf: 눈물 없이 언어 모델을 인간 피드백과 정렬하기 위한 순위 응답. arXiv 사전 인쇄본 arXiv:2304.05302, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: 정렬은 적을수록 더 좋다. arXiv 사전 인쇄본 arXiv:2305.11206, 2023.
"
"--- ABSTRACT ---
We introduce POP3D, a novel framework that creates a full 360°view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a monocular depth and normal predictor that serves to predict crucial geometric cues, (2) a space carving method capable of demarcating the potentially unseen portions of the target object, (3) a generative model pretrained on a large-scale image dataset that can complete unseen Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.regions of the target, and (4) a neural implicit surface reconstruction method tailored in reconstructing objects using RGB images along with monocular geometric cues. The combination of these components enables POP3D to readily generalize across various in-the-wild images and generate state-of-the-art reconstructions, outperforming similar works by a significant margin. Project page: http://cg.postech.ac.kr/research/POP3D. CCS CONCEPTS + Computing methodologies — Reconstruction; Computer graphics; Artificial intelligence. KEYWORDS Single-View 3D Reconstruction, Shape and Appearance Reconstruction, Novel-View Synthesis, Space Carving, Outpainting ACM Reference Format: Nuri Ryu, Minsu Gong, Geonung Kim, Joo-Haeng Lee, and Sunghyun Cho. 2023. 360° Reconstruction From a Single Image Using Space Carved Outpainting. In SIGGRAPH Asia 2023 Conference Papers (SA Conference Papers °23), December 12-15, 2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3610548.--- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia 1
--- INTRODUCTION ---
The ability to generate high-quality realistic 3D models from minimal input is an ongoing challenge for various applications in computer graphics, vision, virtual reality, and augmented reality.Despite the recent advances in the area of multi-view reconstruction through the differentiable rendering of neural representations [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], such methods rely heavily on vast amounts of images paired with camera parameters. While this reliance may yield impressive results, it inhibits practicality and accessibility, particularly in scenarios where obtaining multiple views of an object is impractical or impossible. In real-world scenarios, a user might only have a single view of an object. For instance, the image may be a photo of an object that is not easily accessible or an output of a 2D generative model. Consequently, 3D model generation from a single image has immense practical significance, enabling a broader range of applications and making 3D modeling more accessible to a wider user base. Due to its practical significance, 3D reconstruction from a single image has been an active area of research. However, existing methods still suffer from two major problems: generalizability and reconstruction fidelity. Various methods have been proposed to learn from 3D data or object-centric videos for single-view reconstruction [Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. However, the acquisition of such data is often more challenging compared to collecting unstructured 2D data, thereby undermining the scalability and generalizability of these methods. While other techniques have also been proposed to circumvent the need for 3D data by relying on 2D image data, such methods are often bound to a specific category [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021], thereby limiting their generalizability. Concurrent methods [Deng et al. 2023; Melas-Kyriazi et al. 2023; Xu et al. 2023] that leverage a large-scale image prior [Rombach et al. 2022] via a distillation loss [Poole et al. 2023] frequently fall short of faithfully reconstructing the input view. This discrepancy arises as the distillation loss interferes with the RGB reconstruction loss of the input view and their limited target resolution of the reconstruction further exacerbates this problem. Furthermore, their use of naive neural density fields often leads to low-fidelity surface reconstruction. This paper presents POP3D (Progressive OutPainting 3D), a novel framework designed to address the aforementioned issues of generalizability and reconstruction fidelity.To tackle the challenge of generalizability, our framework leverages the power of various priors pre-trained on large-scale datasets. This approach effectively mitigates the inherent ill-posedness of 3D reconstruction from a single RGB image across arbitrary categories.For a high-fidelity reconstruction covering the full 360° view of an object, we generate novel views that match the quality of the given view through a largescale generative model. These novel views, in conjunction with their monocular geometry predictions, form a pseudo-ground-truth dataset. By training on this dataset following a training strategy tailored to incorporate monocular geometry cues, we reconstruct a neural implicit surface and its corresponding appearance of the Ryu et al. given single image with high fidelity compared to concurrent works as we illustrate in Fig. 1. To elaborate, our framework begins by processing a single RGB input by using state-of-the-art monocular depth and normal predictors [Bhat et al. 2023; Eftekhar et al. 2021] to infer its geometric cues. The input RGB and its monocular geometry predictions constitute an initial dataset and are used to train an initial 3D model following a training strategy of MonoSDF [Yu et al. 2022]. After initializing the 3D model, we update our camera position following a camera schedule that encompasses the full 360° view of the target object. Then, our framework finds the visual hull [Laurentini 1994] of the object, thereby computing the target object’s seen area as well as the potentially unseen area. By removing the seen area, we obtain an outpainting mask, which is used as a guide for the generative model to produce a natural novel view of the object. Specifically, we use a conditional diffusion model [Rombach et al. 2022] trained ona large-scale dataset [Schuhmann et al. 2022] capable of outpainting the image given a mask and a text condition. After a process of extracting the monocular geometry information of the outpainted result, we expand our pseudo-ground-truth dataset with the processed data. This updated dataset is then used to retrain our 3D model and we repeat this gradual outpainting process until we create a dataset that covers a full 360° view of an object, ultimately leading to a high-fidelity 360° 3D reconstruction. Our framework provides some distinctive benefits. Firstly, thanks to the priors learned on large-scale datasets, our framework is not limited to a certain category of objects but can handle a wide range of objects from arbitrary categories. Secondly, our framework does not need any additional external training data such as multi-view images or 3D geometries, as we adopt priors already learned in off-the-shelf models. Thirdly, our progressive outpainting approach that builds a 360°-view dataset of the target object ensures the generation of novel-view images of high quality and the faithful reconstruction of the input image. Finally, by using the pseudoground-truth dataset to train a neural implicit surface representation, we can extract a well-defined high-quality surface. To summarize, our primary contributions are as follows: ¢ We introduce a novel framework to reconstruct a full 360° model from a single image. Our framework generalizes well to in-the-wild RGB images without any category-specific pre-training by leveraging off-the-shelf priors. ¢ We develop a progressive outpainting scheme to generate pseudo-ground-truth images for 3D model reconstruction. Our method ensures a faithful reconstruction with novelview images that naturally harmonize with the input image. Our model design accounts for both geometric and photometric consistency leading to high-fidelity shape and appearance reconstruction. ¢ We show that our framework can produce state-of-the-art 360° reconstruction results from single RGB images in terms of novel-view synthesis and geometry reconstruction. 2
--- RELATED WORK ---
S 2.1 Few-View-to-3D Reconstruction NeRF [Mildenhall et al. 2020] and its variants [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020] have shown remarkable --- --360° Reconstruction From a Single Image Using Space Carved Outpainting reconstruction performance of scenes and objects only given RGB images paired with camera poses. However, without dense camera views, training a neural radiance field becomes a severely underconstrained problem. When only given a few views, such models may overfit to each given view resulting in a broken geometry and blurry noise when rendering novel views [Jain et al. 2021]. Recently, a line of work has been introduced to reduce the number of required views for high-fidelity reconstruction [Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. Nevertheless, they still require more than a single view for proper reconstruction. 2.2 Single-View-to-3D Reconstruction Most of the early work that reconstruct 3D models from a single image rely on the visible information given in an image such as shading [Zhang et al. 1999], texture [Loh 2006], or defocus [Favaro and Soatto 2005]. Recent works use a more general prior in order to generate the invisible parts of an input image. For instance, some
--- METHOD ---
. The results in (b) and (c) show that the naive usage of the distillation loss and neural density fields leads to sub-optimal novel views and a low-fidelity surface [Melas-Kyriazi et al. 2023; Xu et al. 2023]. On the other hand, our framework successfully generates novel views that resemble the original input image and also reconstructs the 3D object’s surface with high fidelity, as we observe in (d) and (e). Image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [OHorton, CC BY]. ABSTRACT We introduce POP3D, a novel framework that creates a full 360°view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a monocular depth and normal predictor that serves to predict crucial geometric cues, (2) a space carving method capable of demarcating the potentially unseen portions of the target object, (3) a generative model pretrained on a large-scale image dataset that can complete unseen Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.regions of the target, and (4) a neural implicit surface reconstruction method tailored in reconstructing objects using RGB images along with monocular geometric cues. The combination of these components enables POP3D to readily generalize across various in-the-wild images and generate state-of-the-art reconstructions, outperforming similar works by a significant margin. Project page: http://cg.postech.ac.kr/research/POP3D. CCS CONCEPTS + Computing methodologies — Reconstruction; Computer graphics; Artificial intelligence. KEYWORDS Single-View 3D Reconstruction, Shape and Appearance Reconstruction, Novel-View Synthesis, Space Carving, Outpainting ACM Reference Format: Nuri Ryu, Minsu Gong, Geonung Kim, Joo-Haeng Lee, and Sunghyun Cho. 2023. 360° Reconstruction From a Single Image Using Space Carved Outpainting. In SIGGRAPH Asia 2023 Conference Papers (SA Conference Papers °23), December 12-15, 2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3610548.--- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia 1 INTRODUCTION The ability to generate high-quality realistic 3D models from minimal input is an ongoing challenge for various applications in computer graphics, vision, virtual reality, and augmented reality.Despite the recent advances in the area of multi-view reconstruction through the differentiable rendering of neural representations [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], such methods rely heavily on vast amounts of images paired with camera parameters. While this reliance may yield impressive results, it inhibits practicality and accessibility, particularly in scenarios where obtaining multiple views of an object is impractical or impossible. In real-world scenarios, a user might only have a single view of an object. For instance, the image may be a photo of an object that is not easily accessible or an output of a 2D generative model. Consequently, 3D model generation from a single image has immense practical significance, enabling a broader range of applications and making 3D modeling more accessible to a wider user base. Due to its practical significance, 3D reconstruction from a single image has been an active area of research. However, existing methods still suffer from two major problems: generalizability and reconstruction fidelity. Various methods have been proposed to learn from 3D data or object-centric videos for single-view reconstruction [Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. However, the acquisition of such data is often more challenging compared to collecting unstructured 2D data, thereby undermining the scalability and generalizability of these methods. While other techniques have also been proposed to circumvent the need for 3D data by relying on 2D image data, such methods are often bound to a specific category [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021], thereby limiting their generalizability. Concurrent methods [Deng et al. 2023; Melas-Kyriazi et al. 2023; Xu et al. 2023] that leverage a large-scale image prior [Rombach et al. 2022] via a distillation loss [Poole et al. 2023] frequently fall short of faithfully reconstructing the input view. This discrepancy arises as the distillation loss interferes with the RGB reconstruction loss of the input view and their limited target resolution of the reconstruction further exacerbates this problem. Furthermore, their use of naive neural density fields often leads to low-fidelity surface reconstruction. This paper presents POP3D (Progressive OutPainting 3D), a novel framework designed to address the aforementioned issues of generalizability and reconstruction fidelity.To tackle the challenge of generalizability, our framework leverages the power of various priors pre-trained on large-scale datasets. This approach effectively mitigates the inherent ill-posedness of 3D reconstruction from a single RGB image across arbitrary categories.For a high-fidelity reconstruction covering the full 360° view of an object, we generate novel views that match the quality of the given view through a largescale generative model. These novel views, in conjunction with their monocular geometry predictions, form a pseudo-ground-truth dataset. By training on this dataset following a training strategy tailored to incorporate monocular geometry cues, we reconstruct a neural implicit surface and its corresponding appearance of the Ryu et al. given single image with high fidelity compared to concurrent works as we illustrate in Fig. 1. To elaborate, our framework begins by processing a single RGB input by using state-of-the-art monocular depth and normal predictors [Bhat et al. 2023; Eftekhar et al. 2021] to infer its geometric cues. The input RGB and its monocular geometry predictions constitute an initial dataset and are used to train an initial 3D model following a training strategy of MonoSDF [Yu et al. 2022]. After initializing the 3D model, we update our camera position following a camera schedule that encompasses the full 360° view of the target object. Then, our framework finds the visual hull [Laurentini 1994] of the object, thereby computing the target object’s seen area as well as the potentially unseen area. By removing the seen area, we obtain an outpainting mask, which is used as a guide for the generative model to produce a natural novel view of the object. Specifically, we use a conditional diffusion model [Rombach et al. 2022] trained ona large-scale dataset [Schuhmann et al. 2022] capable of outpainting the image given a mask and a text condition. After a process of extracting the monocular geometry information of the outpainted result, we expand our pseudo-ground-truth dataset with the processed data. This updated dataset is then used to retrain our 3D model and we repeat this gradual outpainting process until we create a dataset that covers a full 360° view of an object, ultimately leading to a high-fidelity 360° 3D reconstruction. Our framework provides some distinctive benefits. Firstly, thanks to the priors learned on large-scale datasets, our framework is not limited to a certain category of objects but can handle a wide range of objects from arbitrary categories. Secondly, our framework does not need any additional external training data such as multi-view images or 3D geometries, as we adopt priors already learned in off-the-shelf models. Thirdly, our progressive outpainting approach that builds a 360°-view dataset of the target object ensures the generation of novel-view images of high quality and the faithful reconstruction of the input image. Finally, by using the pseudoground-truth dataset to train a neural implicit surface representation, we can extract a well-defined high-quality surface. To summarize, our primary contributions are as follows: ¢ We introduce a novel framework to reconstruct a full 360° model from a single image. Our framework generalizes well to in-the-wild RGB images without any category-specific pre-training by leveraging off-the-shelf priors. ¢ We develop a progressive outpainting scheme to generate pseudo-ground-truth images for 3D model reconstruction. Our method ensures a faithful reconstruction with novelview images that naturally harmonize with the input image. Our model design accounts for both geometric and photometric consistency leading to high-fidelity shape and appearance reconstruction. ¢ We show that our framework can produce state-of-the-art 360° reconstruction results from single RGB images in terms of novel-view synthesis and geometry reconstruction. 2 RELATED WORKS 2.1 Few-View-to-3D Reconstruction NeRF [Mildenhall et al. 2020] and its variants [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020] have shown remarkable --- --360° Reconstruction From a Single Image Using Space Carved Outpainting reconstruction performance of scenes and objects only given RGB images paired with camera poses. However, without dense camera views, training a neural radiance field becomes a severely underconstrained problem. When only given a few views, such models may overfit to each given view resulting in a broken geometry and blurry noise when rendering novel views [Jain et al. 2021]. Recently, a line of work has been introduced to reduce the number of required views for high-fidelity reconstruction [Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. Nevertheless, they still require more than a single view for proper reconstruction. 2.2 Single-View-to-3D Reconstruction Most of the early work that reconstruct 3D models from a single image rely on the visible information given in an image such as shading [Zhang et al. 1999], texture [Loh 2006], or defocus [Favaro and Soatto 2005]. Recent works use a more general prior in order to generate the invisible parts of an input image. For instance, some methods use 3D datasets to learn a 3D prior that can be used for reconstruction [Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Saito et al. 2019; Wang et al. 2018; Xie et al. 2019]. However, a large-scale 3D dataset is needed for such models to generalize to inthe-wild images. Compared to large-scale 2D image datasets such as LAION-5B that offers 5.85 billion image-text pairs [Schuhmann et al. 2022], 3D datasets are often limited in variety and scale. On the other hand, our model does not require any 3D training data but can generalize to in-the-wild images by leveraging geometry and image priors [Bhat et al. 2023; Eftekhar et al. 2021; Rombach et al. 2022] trained on large-scale datasets. To overcome the issues arising from needing a 3D training dataset, methods that learn 3D structures from image collections have been introduced [Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Henzler et al. 2019; Jang and Agapito 2021; Kanazawa et al. 2018; Karnewar et al. 2023; Lin et al. 2023; Pavllo et al. 2023; Vasudev et al. 2022; Wu et al. 2023b; Ye et al. 2021]. However, they either need further annotations such as semantic key points and segmentation masks [Kanazawa et al. 2018] or multi-view images of the same scene with accurate camera parameters [Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Karnewar et al. 2023; Lin et al. 2023; Vasudev et al. 2022]. Other methods that train with single view per scene are category-specific [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021]. In contrast, our model does not require any additional information apart from a single RGB image thanks to the off-the-shelf models that we incorporate. Also, we stress that our model can generalize to in-the-wild images regardless of the given view’s category. While 3D diffusion models [Shue et al. 2023; Wang et al. 2023b] are also gaining attention, concurrent works [Deng et al. 2023; SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia single view. This often disruy often leads to a poor reconst: ts the RGB loss and consequently ruction of the input view. While a very recent work [Tang et al. 2023] tries to bypass this problem by projecting the reference image on to the trained 3D representation, novel views far from the reference view tend to lack quality. In contrary, our framework buil composed of multi-view image: Is up a pseudo-ground-truth dataset 's that allow for the use of multi-view reconstruction strategies leading to high-fidelity reconstructions. Furthermore, these models 2023; Tang et al. 2023; Xu et Deng et al. 2023; Melas-Kyriazi et al. al. 2023] have other limitations as well. Firstly, they have a low target resolution, e.g., 96 x 96 or 128 x 128, while our method aims for a resolution of 384 xMelas-Kyriazi et al. 2023; Tang et al. to directly use a 2D diffusion model on a large-scale image-text dataset prior for single view reconstruction. from the reference view, they heavi 2023; Xu et al. 2023] attempt Rombach et al. 2022] trained Schuhmann et al. 2022] as a . To generate unseen regions ily rely on a distillation loss similar to the score distillation sampling loss introduced by Poole et al. [2023]. The problem is that the distillation loss is simultaneously applied to views that have overlap; ing regions from the given yielding results with higher quality and overall detail. While Tang et al. [2023] try to overcome the this problem through a two-stage training scheme, it shares the other problems described below as well. Secondly, these works use naive neural density functions as their geometry representations, which may produce noisy artifacts due to the lack of a well-defined surface threshold. In contrast, our method simply allows for high-fidelity geometry extraction from the zero-level set of the learned neural implicit surface. Lastly, these models only rely on the given single image and its augmentations to personalize the diffusion model using a method similar to Textual Inversion [Gal et al. 2023] in an attempt to generate unseen regions consistent with the input image. In contrast to these methods, our data generation framework allows the use of a state-of-the-art diffusion model personalization method, DreamBooth [Ruiz et al. 2023], that requires multiple views of the same object by using multi-view pseudo-ground-truth images, which allows for a better personalization quality. Raj et al. [2023] also showed that highquality personalized text-to-3D can be achieved using DreamBooth. However, their method requires multiple views of a target object whereas our method only requires a single view of an object thanks to our pseudo-ground-truth multi-view generation scheme. Single View to Point Cloud. Other recent works aim to reconstruct colorized point clouds based on a reference view. For instance, MCC [Wu et al. 2023a] takes an RGB-D image as input and reconstructs the lifted color points into a complete point cloud of the target object. Similarly, Point-E [Nichol et al. 2022] introduces a point-cloud diffusion model that uses a reference RGB image to generate a colorized point cloud that resembles the input image. Unlike such models, our framework reconstructs a high-fidelity neural implicit surface and an appearance of superior quality. 3D Photography. Another line of work utilizes monocular depth predictions and color inpainting to generate a 3D photo or scene from a single image [Han et al. 2022; Hdllein et al. 2023; Shih et al. 2020; Zhang et al. 2023]. However, such methods are only designed to inpaint both the foreground and background at the same time, and does not account for the backside of an object. Therefore, they are not directly applicable to 360° reconstruction of an object. Novel View Synthesis from a Single View. Some works [Liu et al. 2023; Watson et al. 2023] focus on generating a 3D novel view when given an input image and a relative pose. However, the outputs of such models are only approximately 3D consistent and therefore do not guarantee a high-fidelity shape reconstruction. --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia Ryu et al. Initialization => Camera Position Update Initialization Process < a é RGB Train ry Single Initial Initialized Updated RGB Dataset 3D Model Camera Outpainting “A photo of [V] [Class] in a white background, A / seen from [Dir]"" Volume Rendering VATrained 3D Model Initial Novel View => Outpainting Mask Acquisition = Camera Position Update Previous Outpainting =» 3D Model Update Outpainting Mask Acquisition So Subtract Seen Area Visual Hull , Extraction Acquire Silhouette at Updated Camera Outpainting Mask Camera 3D Model Update Process J J intels Train LM —) é Z F Updated Pseudo-Ground-Truth Dataset Trained 3D Model Figure 2: Framework overview. POP3D operates in five interconnected steps. Initially, we process the single RGB input to create a preliminary pseudo-ground-truth dataset and use this data to initialize a 3D model. We then progress through a loop of steps aiming to cover the complete 360° view of the target object. This loop includes: updating the camera position according to a predetermined schedule; acquiring an outpainting mask by extracting the visual hull from the pseudo-ground-truth dataset and subtracting the seen area; generating a pseudo-ground-truth novel view using the initial novel view from the trained 3D model, outpainting mask, and a suitable text prompt; and training the 3D model using the updated pseudo-ground-truth dataset. This process continues until we encompass the 360° view of the object. Input image: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Olaboratorija, CC BY-NC-SA]. Figure 3: Visual hull extraction. We illustrate the acquisition of the visual hull from two camera views. We preserve the shaded region seen on the right which constitutes both seen and potentially unseen regions of a target object. Car image: from freesvg [Public Domain]. 3 METHOD We present an overview of POP3D in Fig. 2. Given a single image of an object, our framework reconstructs its 360° shape and appearance using a neural implicit surface representation. Our key idea is to progressively outpaint the unseen regions of the object by synthesizing their color and geometric information. To this end, our framework consists of five steps: initialization, camera position update, outpainting mask acquisition, outpainting, and 3D model update. In the initialization step, we estimate the depth and normal maps of the input image and lift it to a 3D view. Then, we update the camera position to a nearby viewpoint that has not been seen before, and obtain an outpainting mask that indicates the region to be outpainted using space carving [Kutulakos and Seitz 1999]. Next, we outpaint the masked region by generating its color and geometric information using a latent diffusion model (LDM) [Rombach et al. 2022]. Finally, we update the 3D model of the object using the outpainted information. We repeat these steps until we cover the entire 360° of the object. To represent the shape and appearance of a 3D object, we adopt VoISDF [Yariv et al. 2021], which represents a 3D object using a pair of neural networks. Specifically, to represent the geometry of an object, we use a neural network modeling a signed distance function (SDF) fg : x +» s, which maps a 3D point x € R° to its signed distance s € R to the surface. To account for the appearance, we use another neural network that models a radiance function Lo (x, i, 2) where fi is the spatial gradient of the SDF at point x. Z is the global geometry feature vector same as in Yariv et al. [2020]. Unlike VoISDF, we do not give the viewing direction as input to Lg and ignore view-dependent color changes as a single image does not provide view-dependent lighting information and conventional outpainting methods do not account for view dependency. As 3D model generation from a single image is an extremely ill-posed task, we impose a couple of assumptions to restrict the possible outcomes of the reconstruction results. First, we assume that the target object lies within a cube, which has its center at the origin, and edges of length 2 aligned with the coordinate axes, and initialize the object as a unit sphere following Atzmon and Lipman [2020]. We also assume a virtual camera looking at the target 3D object during our 3D reconstruction process. Specifically, we place the camera on a sphere of radius 3 to point at the origin and parameterize its position using spherical coordinate angles. The field of view (FoV) of the camera is set to 60° assuming that the camera parameters of the input image are not given. In the following, we describe each step of our framework in detail. --- --360° Reconstruction From a Single Image Using Space Carved Outpainting 3.1 Initialization The initialization step constructs an initial 3D model from an input image Lp of a target object. Specifically, given Lo, we first extract the foreground object by estimating a binary mask Mo using an off-the-shelf binary segmentation method [Lee et al. 2022]. We then estimate the depth map Dp and the normal map Np for the foreground object using off-the-shelf monocular depth and normal estimators [Bhat et al. 2023; Eftekhar et al. 2021]. Using the estimated depth and normal maps, and binary mask, we estimate an initial 3D model. Specifically, we first initialize the pseudo-groundtruth dataset P as P = {(Lo, Do, No, Mo, ¢o)} where ¢o is the initial camera position, and train the implicit representation of the initial 3D model (fo, Lg) using P. The initial camera position is set to ¢o = (90°, 0°) where the first and second angles are the polar and azimuthal angles, respectively, assuming that the initial image contains the frontal side of the target object. The pseudo-ground-truth dataset is iteratively updated in the following steps to progressively reconstruct the 3D model of a target object. For training the implicit representation, we adopt the approach of MonoSDF [Yu et al. 2022] with a slight modification to consider the mask Mo. Refer to the supplementary material for more details on the training. 3.2 Camera Position Update After the model has been initialized, we iteratively update the 3D model exploring the unseen regions of the target object by changing the camera position. To this end, we define a camera schedule S= [¢o. Pres ds], designed to cover the 360° view of the target object, and update the camera position at each iteration according to S. In theory, the camera schedule may be an arbitrary set provided that it encompasses the complete 360° view. However, we found that an excessively small or large interval may detrimentally affect the output. Hence, we use an interval of 45° degrees in our
--- EXPERIMENT ---
s, and discuss the adverse impacts of an overly granular or coarse camera schedule in Sec. 4.2.1. In the rest of the section, we will denote the camera positions that have been explored until the i-th camera position in S as So,; such that 0 <i <s. 3.3. Outpainting Mask Acquisition In order to generate the appearance and shape of unseen regions seamlessly, the areas designated for outpainting need to be appropriately chosen. To address this, we leverage the concept of the visual hull [Laurentini 1994]. The visual hull provides a rough approximation of the object’s shape derived from the object’s silhouettes from different viewpoints. Using our current dataset P, we can compute the visual hull to determine the maximum possible area that the object might occupy as illustrated in Fig. 3. By computing the silhouette of the visual hull seen from the updated camera view, we obtain an initial mask that comprises both previously observed and potentially unseen regions. To create our outpainting mask, we subtract the observed regions from this initial mask, leaving only the potentially new visible areas. Visual Hull Computation via Space Carving. For the computation of the visual hull [Laurentini 1994], we use a depth-based voxel carving method driven by a voting scheme [Kutulakos and Seitz 1999]. The process first voxelizes the object-bounding cube. Now SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia we assume that we have explored the camera positions of So.j-1. Then, for a voxel p from the voxelized bounding cube, we raise a vote if its projection to the j-th view is inside the foreground region where j € {0,...,i— 1}, and if its distance to the j-th view’s camera center is longer than the distance between the foreground region and the j-th view’s camera center. Mathematically, we raise a vote for p if KPjpeé My. and * (1) IIp - ojllz = IIp* — ojlla. where K is the camera intrinsic matrix. P; and 0; represent the projection to the camera space and the camera center of the j-th view in P, respectively. p* is the point of intersection between the zero-level set of fg and the ray cast from 0; towards p. Here, we only consider the first intersection where the ray penetrates the object from the exterior for the first time. If the total number of vote counts equals the size of P, or the number of views, the voxel is preserved. Otherwise, the voxel is carved out. Through this procedure, we collect the voxels comprising the visual hull of P. To add, when we only have a single image, this process can be thought of an extrusion of the trained 3D surface. By projecting the visual hull onto the i-th viewpoint, we obtain its silhouette My"". Foreground Mask Computation via Warping Operation. Since Mya contains both seen and unseen regions, we should subtract out the seen region in order to obtain our outpainting mask Mj. This is achieved by using a warping operation to compute the foreground mask Mrs in the target view. The process involves rendering the depth from the previous viewpoints Sp.j-1, lifting the image points to the 3D space, and subsequently projecting the lifted points to the target view ¢;. To mitigate aliasing during the warping process, we scale up the image by a scaling factor of 8. We account for visibility and do not warp pixels not visible from the target viewpoint via back-point culling. Specifically, the warping process including the back-point culling is performed as follows. In the process of warping an image from the j-th view to the i-th view, we denote a lifted pixel from the j-th image in P as p/. Then, we render its normal Ni from fo. The pixel is only warped for the target camera center o if: (p’ - oi) -NI <0 (2) The warped coordinate p' is then computed as: p' =KP).;K""'p! (3) where P;—,; denotes a 4 x 4 transformation matrix that warps the camera from the source position to the target position. A binary mask ofc is computed from the collection of the warped pixels. Then, M; is calculated as Mj = Myt - Mrs. 3.4 Pseudo-Ground-Truth Generation In order to reconstruct the 360° shape and appearance of the target object, we generate pseudo-ground-truth images to fill in the unseen parts of the object. For this purpose, we use a pretrained state-ofthe-art generative model. Specifically, we use the Latent Diffusion Model (LDM) [Rombach et al. 2022] that takes an RGB image, a --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia 66.(a) Input RGB (b) Ours (c) RealFusion (d) NeuralLift-Figure 4: Qualitative comparison on the input image reconstruction. Given a single input image (a), our method successfully reconstructs the reference view as seen in (b). However, RealFusion [Melas-Kyriazi et al. 2023] and NeuralLift360 [Xu et al. 2023] do not faithfully reconstruct the input view even when it utilizes an RGB reconstruction loss. Input image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [©laboratorija, CC BY-NC-SA]. mask condition, and a text condition as input and outputs an RGB image following the input conditions. However, naively using a pretrained diffusion model may result in outpainting results that do not resemble the reference image. To generate pseudo-ground-truth images that are coherent to the given single view, we adopt a personalization technique outlined in DreamBooth [Ruiz et al. 2023]. This technique allows us to learn a unique identifier token [V] of the object, which can be included as part of the text condition. Since our framework generates multiview pseudo-ground-truth images, we can trivially apply such a technique to generate well-harmonized results. For the details of the conditional diffusion model and its personalization, we refer the readers to the supplementary document. As the inputs to the personalized LDM, we use: e J; - the RGB image rendered from the trained model at the updated camera view, © M; - the outpainting mask at the updated camera view, as detailed in Section 3.3, and ¢ a text prompt designed to generate view-consistent results. For the text condition, we utilize a prompt structured as “A photo of [V] [Class] in a white background, seen from [Dir]” where [V] represents the personalized unique identifier of the specific object, [Class] refers to a simple class keyword such as ‘hamburger’ or ‘doll’, and [Dir] is a directional keyword such as ‘front’, ‘left’, ‘right’ and ‘behind’ used to guide the generation following the approach of Poole et al. [2023]. Upon obtaining the outpainted view, we apply 2x super-resolution [Wang et al. 2023a] for image enhancement. We then employ offthe-shelf monocular depth [Bhat et al. 2023] and normal [Eftekhar et al. 2021] estimators to extract geometric predictions Dj and Nj. Furthermore, we use a background remover [Lee et al. 2022] to obtain the foreground mask Mj. Finally, we update the pseudoground-truth dataset P as P — P U {(Lj, Dj, Ni, Mi, $i) } 3.5 3D Model Update Using the updated pseudo-ground-truth dataset P, we train the SDF fg and neural radiance field Lg following MonoSDF [Yu et al. 2022]. After retraining the target 3D model, we return to the camera position update step described in Sec. 3.2, and continue the loop until we go through the whole camera schedule S. Ryu et al. ; (c) Ag = 90° (a) Input RGB (b) Outpainting results with Ag = 15° Figure 5: Effect of camera intervals on outpainting results. For a single RGB input (a), both camera intervals excessively small (b) and overly large (c) have their drawbacks in their own ways as described in Sec. 4.2.1. Input RGB: generated using a diffusion model [Rombach et al. 2022]. SVE (b) Initial novel view ‘a) Previous Grow View (c) Outpainting result na DreamBooth (d) Outpainting result w/ DreamBooth Figure 6: Effect of diffusion model personalization. After updating the camera view from (a) to (b), we outpaint the target object with the same input to the LDM. However, naive usage of the LDM may result in an outpainting result that does not consider the images in the pseudo-ground-truth dataset such as a doll with a duplicate face in (c), rather than naturally outpainting the doll’s hat as shown in (d). Input image of this experiment: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Oshirava, CC BY]. 4 EXPERIMENTS This section presents experimental results to evaluate the reconstruction quality of our framework in terms of shape and appearance. For all the experiments, we employ a camera schedule S whose polar angles are 90° and the azimuthal angles are [o°, 45°, —45°, 90°, —90°, 135°, -135°, 180°| . Nevertheless, our framework is not limited to a particular camera schedule. We show more qualitative results using a customized camera trajectory in the supplementary document. For comparison, we utilize objects reconstructed using photogrammetry from Objaverse [Deitke et al. 2022]. Our selection of ten object categories provides a diverse range of shapes and appearances for testing. We compare our method with concurrent works [Melas-Kyriazi et al. 2023; Nichol et al. 2022; Wu et al. 2023a; Xu et al. 2023] that can reconstruct 360° appearance and shape from a single reference view, along with a single-imagebased 3D shape reconstruction method [Vasudev et al. 2022]. To ensure a fair comparison, we use the same off-the-shelf depth estimator [Bhat et al. 2023] for methods that require monocular depth guidance [Melas-Kyriazi et al. 2023; Wu et al. 2023a; Xu et al. 2023]. 4.1 Comparisons with Other Methods Input-View Reconstruction. Given a single RGB input, we expect the model to faithfully reconstruct the given image after the training process. Therefore, we compare our framework with methods that also use an RGB reconstruction loss during training to test the input-view reconstruction capability. To inspect the fidelity of the reference-view reconstruction, we use commonly used image --- --360° Reconstruction From a Single Image Using Space Carved Outpainting quality metrics: PSNR, SSIM [Wang et al. 2004], and LPIPS [Zhang et al. 2018]. Our method outperforms the others in all categories as shown in Tab. 1. As discussed in Sec. 1, concurrent works exhibit lower input-view reconstruction performance since the RGB loss is affected by the distillation loss in similar viewpoints. However, our framework that trains directly on a multi-view pseudo-groundtruth dataset does not face such a problem as we observe in Fig. 4. Novel-View Synthesis. For the evaluation of novel-view synthesis, we evaluate the results in terms of the similarity to the ground-truth views and the overall image quality of the output. CLIP similarity [Radford et al. 2021] is used to evaluate the similarity between the model outputs and their corresponding ground-truth views. The image qualities of the generated outputs are assessed via the NIQE score [Mittal et al. 2013]. We evenly sample views around a 360° trajectory, resulting in a total of 100 views for comparison. Tab. 2 presents a quantitative comparison. As shown in the table, our method consistently shows high CLIP scores and outperforms the others in NIQE scores by a large margin for all categories. This shows that our method can generate novel views that are semantically similar to the given single view while maintaining high quality. Qualitative comparisons of novel-view synthesis and their corresponding shapes are presented in Figs. 8 and 9, where it can be observed that our method generates natural-looking novel views throughout the entire 360° trajectory. In contrast, concurrent methods [Melas-Kyriazi et al. 2023; Xu et al. 2023] often produce results that hardly resemble the input RGB images since their diffusion model personalization only relies on the single input view and its augmentations. In contrast, we leverage diffusion model personalization using multiple generated views, leading to a more coherent output. Moreover, our framework’s utilization of neural implicit surface representation effectively reduces the introduction of foggy artifacts commonly seen in methods that adopt a more simplistic use of neural density fields. Compared to the methods that reconstruct colorized point clouds [Nichol et al. 2022; Wu et al. 2023a], our framework generates novel views with much finer details. 4.2 Ablations 4.2.1 Outpainting Errors and the Camera Schedule Interval. While we may use any camera schedule as long as it covers the entire 360° of a target object in theory, during the outpainting process, we identified two key factors that may precipitate failure scenarios. The first issue arises when the outpainting mask barely extends beyond the object’s boundary. In this instance, the input image dominates the input condition, making the outpainting process highly sensitive to any artifacts in the immediate vicinity of the outpainting region. By repeating the outpainting process, such boundary artifacts are accumulated, which often leads to failure cases. The second issue manifests when the outpainting mask is excessively large compared to the object region in the original image. In this case, the outpainting tends to generate an image that adheres largely to the text prompt, thus neglecting the input image. Consequently, excessively granular or large camera intervals may result in reconstruction failures as depicted in Fig. 5. To mitigate these issues, we use an interval size of 45° in our experiments. Empirically, this interval size effectively circumvents outpainting failures, thereby facilitating the reconstruction of high-fidelity 360° SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Table 1: Quantitative comparison of the PSNR, SSIM and LPIPS scores. Our model shows a large margin in terms quantitative result of the reference view reconstruction. NL and RF stand for NeuralLift [Xu et al. 2023] and RealFusion [MelasKyriazi et al. 2023], respectively. PSNR T SSIMT LPIPS | NL RF Ours | NL RF Ours | NL RE Ours. Berry 17.47 27.85 32.30 | 0.88 0.94 0.99 | 0.21 0.12 0.Broccoli 18.28 14.73 37.66 | 0.85 0.83 0.99 | 0.24 0.28 0.Cactus 17.33 22.99 31.50 | 0.90 0.92 0.98 | 0.17 0.16 0.Cauliflower | 15.50 27.81 33.60 | 0.86 0.93 0.99 | 0.23 0.16 0.Croissant 12.60 29.68 36.60 | 0.82 0.96 0.99 | 0.29 0.10 0.Doll Statue | 14.68 13.89 39.49 | 0.85 0.87 0.99 | 0.21 0.23 0.Frog Statue | 14.62 20.27 36.70 | 0.90 0.91 0.99 | 0.20 0.19 0.Owl 16.41 27.75 36.01 | 0.84 0.91 0.99 | 0.23 0.17 0.Pear 10.26 15.86 40.85 | 0.67 0.88 0.99 | 0.44 0.17 0.Skull 1.79 24.99 36.30 | 0.14 0.94 0.99 | 0.80 0.13 0.Mean 13.89 22.58 36.10 | 0.77 0.91 0.99 | 0.30 0.17 0.Table 2: Quantitative comparison of the CLIP similarity and NIQE scores. Our model not only achieves the best embedding similarity but also achieves the best image quality score. NL, RF, MCC, and P-e stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], and Point-E [Nichol et al. 2022], respectively. CLIPT NIQET NL RF MCC Pe Ours] NL RF MCC Pe Ours Berry, 0.77 0.81 0.76 0.84 0.82 | 22.81 9.56 23.35 19.33 5.Broccoli | 0.86 0.88 0.69 0.80 0.84 | 27.79 11.65 20.28 21.57 9.Cactus | 0.79 0.82 0.69 0.84 0.89 | 24.92 15.39 26.57 22.70 9.Cauliflower | 0.82 0.80 0.74 0.77 0.87 | 22.58 12.76 22.61 19.91 7.Croissant | 0.74 0.76 0.70 0.78 0.86 | 29.07 13.86 24.10 32.01 12.Doll Statue | 0.78 0.77 0.67 0.85 0.84 | 21.46 21.75 24.49 31.42 18.Frog Statue | 0.76 0.77 0.81 0.83 0.85 | 22.30 15.13 18.22 19.63 8.Owl 0.81 0.76 0.67 0.80 0.86 | 11.19 11.82 22.99 16.93 7.Pear 0.81 0.86 0.81 0.82 0.88 | 26.36 10.49 25.01 17.88 9.Skull 0.71 0.83 0.76 0.81 0.87 | 24.62 13.25 1985 16.18 7.Mean 0.78 0.80 0.73 0.81 0.86 | 23.31 13.57 22.75 21.76 10.views of a target object. Nevertheless, while the suggested interval size may serve as a good starting point, the best warping angle interval or the camera schedule itself may vary for various objects. We show examples of more customized intervals in the supplementary. 4.2.2 Outpainting Results Without LDM Personalization. As our model architecture generates multiple pseudo-ground-truth views of a target object throughout the reconstruction process, it allows for the personalization of the pre-trained LDM [Rombach et al. 2022] using the state-of-the-art technique, DreamBooth [Ruiz et al. 2023]. The benefit of this approach is evidenced in Fig. 6, where the application of the personalized LDM [Rombach et al. 2022] generates a natural-looking novel view that seamlessly integrates with the pseudo-ground-truth dataset. In contrast, naive reliance on the vanilla LDM may result in an outpainting outcome that does not reflect the previously seen views of the target object. 5
--- CONCLUSION ---
AND FUTURE WORK In this study, we present POP3D, a novel framework that addresses two long-standing challenges in the domain of 360° reconstruction from a single RGB image: generalization and fidelity. POP3D fully leverages current state-of-the-art priors trained on large-scale datasets and successfully overcomes the problem of generalization --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia y (a) Input RGB (b) Novel Views (c) Complete Image Backside Figure 7: Limitations. Given an input RGB (a) on the left, our model generates plausible novel views (b) following a camera schedule. However, our model sometimes generates a subpar complete backside image when compared to the input single RGB, as we see in (c). Image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [©ShekhirevaVictoria, CC BY]. to arbitrary images. Experimentally, we demonstrate that our framework not only faithfully reconstructs the provided single RGB input but also generates realistic novel views. These views collectively form a pseudo-ground-truth multi-view dataset, facilitating the direct application of multi-view reconstruction strategies. Compared to existing methodologies, our approach exhibits superior performance, reaffirming its potential as a robust solution for 3D reconstruction tasks. 5.1 Limitations Our approach does exhibit certain limitations. Since our framework is a composition of off-the-shelf priors each playing a significant role in the pipeline, a failure of one model may impact the final reconstruction result. For instance, if the monocular depth or normal predictors fail on challenging cases, e.g., thin structures, this may lead to artifacts in the reconstructed shape. Moreover, our framework occasionally generates the object’s complete backside with subpar quality when compared to the input view. We illustrate this problem in Fig. 7. This deficiency may be attributed to an accumulation of outpainting artifacts, which could compromise the performance of off-the-shelf priors and degrade the overall image quality in the long term. Since our approach incrementally increases the number of views through the generation of pseudo-ground-truth images, the computational time associated with 3D model training also escalates along the camera schedule. The current run time for the reconstruction of a single object takes around seven hours using a single 3090 RTX GPU. Nevertheless, our framework has a modular structure and it would be easy to replace the models used in each step. Especially, we may replace VolSDF [Yariv et al. 2021] with more advanced methods [Rosu and Behnke 2023; Wang et al. 2022] to accelerate the reconstruction process. Furthermore, we may adopt LoRA [Hu et al. 2022] for accelerating DreamBooth [Ruiz et al. 2023]. To address these issues, our future research will focus on exploring methods to minimize any artifacts and further refine the reconstruction process while improving the reconstruction time. ACKNOWLEDGMENTS This research was supported by IITP grants funded by the Korea government (MSIT) (IITP-2021-0-02068, IITP-2019-0-01906), and the Starting growth Technological R&D Program (TIPS Program, Ryu et al. (No. S3200141)) funded by the Ministry of SMEs and Startups (MSS, Korea) in 2021. REFERENCES Matan Atzmon and Yaron Lipman. 2020. SAL: Sign Agnostic Learning of Shapes From Raw Data. In Proc. of CVPR. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In Proc. of ICCV. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In Proc. of CVPR. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Miller. 2023. ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth. arXiv:2302.12288 [es.CV] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. Generative Novel View Synthesis with 3D-Aware Diffusion Models. arXiv:2304.02602 [es.CV] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 2016. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. In Proc. of ECCV. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2022. Objaverse: A Universe of Annotated 3D Objects. arXiv:2212.08051 [cs.CV] Congyue Deng, Chiyu “Max” Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, and Dragomir Anguelov. 2023. NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors. In Proc. of CVPR. 2063720647. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. 2021. Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans. In Proc. of ICCV. 10786-10796. P. Favaro and S. Soatto. 2005. A geometric approach to shape from defocus. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 27, 3 (2005), 406417. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. 2023. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In Proc. of ICLR. R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta. 2016. Learning a Predictable and Generative Vector Representation for Objects. In Proc. of ECCV. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, and Mathieu Aubry. 2018. AtlasNet: A Papier-Maché Approach to Learning 3D Surface Generation. In Proc. of CVPR. Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. 2023. NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion. In Proc. of ICML. Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, and Qi Shan. 2022. Fast and Explicit Neural View Synthesis. In Proc. of WACV. 3791-3800. Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. 2022. Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images. In Proc. of ACM SIGGRAPH. Philipp Henzler, Niloy J Mitra, , and Tobias Ritschel. 2019. Escaping Plato’s Cave: 3D Shape From Adversarial Rendering. In Proc. of ICCV. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR. Lukas Héllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias NieSner. 2023. Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. arXiv:2303.11989 [es.CV] Ajay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis. In Proc. of ICCV. 5885-5894. Wonbong Jang and Lourdes Agapito. 2021. CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. In Proc. of ICCV. 12949-12958. Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Jitendra Malik. 2018. Learning Category-Specific Mesh Reconstruction from Image Collections. In Proc. of ECCV. Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J. Mitra. 2023. HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images. In Proc. of CVPR. 18423-18433. KN. Kutulakos and $.M. Seitz. 1999. A theory of shape by space carving. In Proc. of ICCV. 307-314 vol.1. A. Laurentini. 1994. The Visual Hull Concept for Silhouette-Based Image Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 16,(1994), 150-162. Min Seok Lee, WooSeok Shin, and Sung Won Han. 2022. TRACER: Extreme Attention Guided Salient Object Tracing Network. In Proc. of AAAI Conference on Artificial --- --360° Reconstruction From a Single Image Using Space Carved Outpainting Intelligence, Vol. 36. 12993-12994. Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. 2023. Vision Transformer for NeRF-Based View Synthesis From a Single Input Image. In Proc. of WACV. 806-815. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. arXiv:2303.11328 [cs.CV] Angeline Loh. 2006. The recovery of 3-D structure using visual texture patterns. Ph.D. Dissertation. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. 2023. RealFusion: 360deg Reconstruction of Any Object From a Single Image. In Proc. of CVPR. 8446-8455. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. of ECCV. Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. Making a “Completely Blind” Image Quality Analyzer. IEEE Signal Processing Letters 20, 3 (2013), 209-212. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-E: A System for Generating 3D Point Clouds from Complex Prompts. arXiv:2212.08751 [cs.CV] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision. In Proc. of CVPR. Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023. Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion. In Proc. of CVPR. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. In Proc. of ICLR. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, Vol. 139. 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, and Varun Jampani. 2023. DreamBooth3D: Subject-Driven Text-to-3D Generation. arXiv:2303.13508 [cs.CV] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias Niefner. 2022. Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. In Proc. of CVPR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proc. of. CVPR. 10684-10695. Radu Alexandru Rosu and Sven Behnke. 2023. PermutoSDF: Fast Multi-View Reconstruction With Implicit Surfaces Using Permutohedral Lattices. In Proc. of CVPR. 8466-8475. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In Proc. of CVPR. 22500-22510. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In Proc. of ICCV. Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luéié, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi. 2022. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. In Proc. of CVPR. 6229-6238. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. In Proc. of NeurIPS. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3D Photography using Context-aware Layered Depth Inpainting. In Proc. of CVPR. J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 2023. 3D Neural Field Generation Using Triplane Diffusion. In Proc. of CVPR. 20875-20886. Vincent Sitzmann, Michael Zollhéfer, and Gordon Wetzstein. 2019. Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations. In Proc. of NeurlPS. Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. 2023. Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior. arXiv:2303.14184 [cs.CV] Kalyan Alwala Vasudev, Abhinav Gupta, and Shubham Tulsiani. 2022. Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction. In Proc. of CVPR. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. 2022. Ref-NeRF: Structured View-Dependent Appearance for SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Neural Radiance Fields. In Proc. of CVPR. Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, and Chen Change Loy. 2023a. Exploiting Diffusion Prior for Real-World Image Super-Resolution. arXiv:2305.07015 [es.CV] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. 2018. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. In Proc. of ECCV. Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, and Baining Guo. 2023b. RODIN: ‘A Generative Model for Sculpting 3D Digital Avatars Using Diffusion. In Proc. of CVPR. 4563-4573. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. 2022. NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction. arXiv:2212.05231 [cs.CV] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (2004), 600-612. Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2023. Novel View Synthesis with Diffusion Models. In Proc. of ICLR. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023a. Multiview Compressive Coding for 3D Reconstruction. In Proc. of CVPR. 9065-9075. Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. 2023b. MagicPony: Learning Articulated 3D Animals in the Wild. Proc. of CVPR. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. 2019. Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images. In Proc. of ICV. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. 2023. NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views. In Proc. of CVPR. 4479-4489. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. In Proc. of NeurlPS. Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In Proc. of NeurIPS. Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. 2021. Shelf-Supervised Mesh Prediction in the Wild. In Proc. of CVPR. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: Neural Radiance Fields from One or Few Images. In Proc. of CVPR. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction. In Proc. of NeurIPS. Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. 2023. Text2NeRF: TextDriven 3D Scene Generation with Neural Radiance Fields. arXiv:2305.11588 [cs.CV] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing and Improving Neural Radiance Fields. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In Proc. of CVPR. Ruo Zhang, Ping-Sing Tsai, J.E. Cryer, and M. Shah. 1999. Shape-from-shading: a survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 21,(1999), 690-706. Zhizhuo Zhou and Shubham Tulsiani. 2023. SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction. In Proc. of CVPR. 12588-12597. --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia Ryu et al. P-e MCC NL RF Ours SS3D Figure 8: Qualitative comparison. We reconstruct the 360° shape and appearance of the single RGB image given on top with various models and compare them with our result. NL, RF, MCC, P-e, and SS3D stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], and SS3D [Vasudev et al. 2022], respectively. Since SS3D [Vasudev et al. 2022] does not reconstruct the object’s appearance we only show its shape output. For better visualization, we use marching cubes for MCC [Wu et al. 2023a] to extract the surface with the same occupancy threshold that is used to sample the point cloud. For Point-E [Nichol et al. 2022], we use the point-cloud-to-mesh conversion provided by the authors. Input: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [OHorton, CC BY]. --- --360° Reconstruction From a Single Image Using Space Carved Outpainting SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Input : bidet? . fi 32% 2 ViEGLE 2 iit Figure 9: Another qualitative comparison. We reconstruct the 360° shape and appearance of the single RGB image given on top with various models and compare them with our result. NL, RF, MCC, P-e, and SS3D stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], and SS3D [Vasudev et al. 2022], respectively. Since SS3D [Vasudev et al. 2022] does not reconstruct the object’s appearance we only show its shape output. For better visualization, we use marching cubes for MCC [Wu et al. 2023a] to extract the surface with the same occupancy threshold that is used to sample the point cloud. For Point-E [Nichol et al. 2022], we use the point-cloud-to-mesh conversion provided by the authors. Input: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Oshirava, CC BY].
"	"--- ABSTRACT ---
단일 이미지에서 전체 360° 뷰 3D 모델을 만드는 새로운 프레임워크인 POP3D를 소개합니다. POP3D는 단일 뷰 재구성을 제한하는 두 가지 중요한 문제를 해결합니다. 첫째, POP3D는 임의의 범주에 대한 상당한 일반화 가능성을 제공하는데, 이는 이전 방법에서는 달성하기 어려운 특성입니다. 둘째, POP3D는 동시 작업에서 부족한 중요한 측면인 재구성 충실도와 자연스러움을 더욱 개선합니다. 저희의 접근 방식은 네 가지 주요 구성 요소의 장점을 결합합니다. (1) 중요한 기하학적 단서를 예측하는 데 사용되는 단안 깊이 및 일반 예측기, (2) 대상 객체의 잠재적으로 보이지 않는 부분을 구분할 수 있는 공간 조각 방법, (3) 보이지 않는 부분을 완료할 수 있는 대규모 이미지 데이터 세트에서 사전 학습된 생성 모델 이 작업의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 인쇄본으로 만드는 허가는 사본이 이익 또는 상업적 이익을 위해 만들어지거나 배포되지 않고 사본에 이 고지 사항과 첫 페이지에 있는 전체 인용문이 있는 경우 무료로 부여됩니다. 저자가 아닌 다른 사람이 소유한 이 작품의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 © 2023 저작권은 소유자/저자에게 있습니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.대상 영역, (4) 단안 기하학적 단서와 함께 RGB 이미지를 사용하여 객체를 재구성하는 데 맞춤화된 신경 암묵적 표면 재구성 방법. 이러한 구성 요소를 결합하면 POP3D는 다양한 야생 이미지에서 쉽게 일반화하고 최첨단 재구성을 생성하여 유사한 작업보다 상당한 마진으로 성능이 향상됩니다. 프로젝트 페이지: http://cg.postech.ac.kr/research/POP3D. CCS 개념 • 컴퓨팅 방법론 → 재구성; 컴퓨터 그래픽; 인공 지능. 키워드 단일 뷰 3D 재구성, 모양 및 외관 재구성, 새로운 뷰 합성, 공간 조각, 아웃페인팅 ACM 참조 형식: 류 누리, 공민수, 김건웅, 이주행, 조성현. 2023. 공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성. SIGGRAPH Asia 2023 Conference Papers(SA Conference Papers &#39;23), 2023년 12월 12-15일, 호주 NSW 시드니. ACM, New York, NY, USA, 11페이지. https://doi.org/10.1145/3610548.SA Conference Papers &#39;23, 2023년 12월 12-15일, Sydney, NSW, Australia
--- INTRODUCTION ---
최소한의 입력으로 고품질의 사실적인 3D 모델을 생성하는 능력은 컴퓨터 그래픽, 비전, 가상 현실 및 증강 현실의 다양한 응용 분야에서 지속적인 과제입니다.신경 표현의 미분 가능한 렌더링을 통한 다중 뷰 재구성 분야의 최근 진전에도 불구하고 [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], 이러한 방법은 카메라 매개변수와 페어링된 방대한 양의 이미지에 크게 의존합니다.이러한 의존성은 인상적인 결과를 가져올 수 있지만 실용성과 접근성을 저해하며, 특히 객체의 여러 뷰를 얻는 것이 비실용적이거나 불가능한 시나리오에서 그렇습니다.실제 시나리오에서 사용자는 객체에 대한 단일 뷰만 가질 수 있습니다.예를 들어, 이미지는 쉽게 접근할 수 없는 객체의 사진이거나 2D 생성 모델의 출력일 수 있습니다.결과적으로 단일 이미지에서 3D 모델을 생성하는 것은 엄청난 실질적 의미를 가지며, 더 광범위한 응용 프로그램을 가능하게 하고 더 광범위한 사용자 기반이 3D 모델링에 더 쉽게 접근할 수 있게 합니다. 단일 이미지로부터의 3D 재구성은 실용적인 중요성 때문에 활발한 연구 분야였습니다. 그러나 기존 방법은 여전히 일반화 가능성과 재구성 충실도라는 두 가지 주요 문제에 시달리고 있습니다. 단일 뷰 재구성을 위해 3D 데이터 또는 객체 중심 비디오에서 학습하는 다양한 방법이 제안되었습니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. 그러나 이러한 데이터의 수집은 구조화되지 않은 2D 데이터를 수집하는 것에 비해 종종 더 어렵기 때문에 이러한 방법의 확장성과 일반화 가능성이 훼손됩니다. 2D 이미지 데이터에 의존하여 3D 데이터의 필요성을 우회하는 다른 기술도 제안되었지만 이러한 방법은 종종 특정 범주에 국한됩니다[Henzler et al. 2019; Jang 및 Agapito 2021; Pavllo 등 2023; Wu 등 2023b; Ye 등 2021], 따라서 일반화 가능성이 제한됩니다. 증류 손실[Poole 등 2023]을 통해 대규모 이미지 사전[Rombach 등 2022]을 활용하는 동시 방법[Deng 등 2023; Melas-Kyriazi 등 2023; Xu 등 2023]은 종종 입력 뷰를 충실하게 재구성하지 못합니다. 이러한 불일치는 증류 손실이 입력 뷰의 RGB 재구성 손실을 방해하고 재구성의 제한된 대상 해상도가 이 문제를 더욱 악화시키기 때문에 발생합니다. 더욱이 순진한 신경 밀도 필드를 사용하면 종종 충실도가 낮은 표면 재구성으로 이어집니다. 이 논문에서는 앞서 언급한 일반화 가능성 및 재구성 충실도 문제를 해결하도록 설계된 새로운 프레임워크인 POP3D(Progressive OutPainting 3D)를 제시합니다. 일반화의 과제를 해결하기 위해, 저희 프레임워크는 대규모 데이터 세트에서 사전 학습된 다양한 사전 확률의 힘을 활용합니다. 이 접근 방식은 임의의 범주에서 단일 RGB 이미지로부터 3D 재구성의 본질적인 부적절함을 효과적으로 완화합니다. 객체의 전체 360° 뷰를 포괄하는 고충실도 재구성을 위해, 저희는 대규모 생성 모델을 통해 주어진 뷰의 품질과 일치하는 새로운 뷰를 생성합니다. 이러한 새로운 뷰는 단안 기하학 예측과 함께 의사-지상 진실 데이터 세트를 형성합니다. 단안 기하학 신호를 통합하도록 맞춤화된 학습 전략에 따라 이 데이터 세트에서 학습함으로써, 저희는 신경 암묵적 표면과 Ryu et al.의 해당 모양을 재구성합니다. 그림 1에서 설명한 것과 같이 동시 작업과 비교하여 높은 충실도를 가진 단일 이미지가 제공됩니다.자세히 설명하자면, 프레임워크는 최첨단 단안 깊이 및 일반 예측자[Bhat et al. 2023; Eftekhar et al. 2021]를 사용하여 단일 RGB 입력을 처리하여 기하학적 신호를 추론하는 것으로 시작합니다.입력 RGB와 단안 기하학적 예측은 초기 데이터 세트를 구성하며 MonoSDF[Yu et al. 2022]의 학습 전략에 따라 초기 3D 모델을 학습하는 데 사용됩니다.3D 모델을 초기화한 후 대상 객체의 전체 360° 보기를 포함하는 카메라 일정에 따라 카메라 위치를 업데이트합니다.그런 다음 프레임워크는 객체의 시각적 껍질[Laurentini 1994]을 찾아 대상 객체의 보이는 영역과 잠재적으로 보이지 않는 영역을 계산합니다.보이는 영역을 제거하여 생성 모델이 객체의 자연스럽고 새로운 보기를 생성하는 가이드로 사용되는 아웃페인팅 마스크를 얻습니다. 구체적으로, 우리는 마스크와 텍스트 조건이 주어진 이미지를 아웃페인팅할 수 있는 대규모 데이터 세트[Schuhmann et al. 2022]에서 학습된 조건부 확산 모델[Rombach et al. 2022]을 사용합니다. 아웃페인팅된 결과의 단안 기하학 정보를 추출하는 과정을 거친 후, 처리된 데이터로 의사 지상 진실 데이터 세트를 확장합니다. 이 업데이트된 데이터 세트를 사용하여 3D 모델을 다시 학습하고, 객체의 전체 360° 보기를 포함하는 데이터 세트를 생성할 때까지 이 점진적인 아웃페인팅 과정을 반복하여 궁극적으로 고충실도 360° 3D 재구성을 이룹니다. 우리 프레임워크는 몇 가지 독특한 이점을 제공합니다. 첫째, 대규모 데이터 세트에서 학습한 사전 지식 덕분에 우리 프레임워크는 특정 범주의 객체에 국한되지 않고 임의의 범주의 광범위한 객체를 처리할 수 있습니다. 둘째, 우리 프레임워크는 기성 모델에서 이미 학습한 사전 지식을 채택하므로 다중 뷰 이미지나 3D 기하학과 같은 추가 외부 학습 데이터가 필요하지 않습니다. 셋째, 대상 객체의 360도 뷰 데이터 세트를 구축하는 점진적 아웃페인팅 방식은 고품질의 새로운 뷰 이미지 생성과 입력 이미지의 충실한 재구성을 보장합니다. 마지막으로, 가상 지상 진실 데이터 세트를 사용하여 신경 암묵적 표면 표현을 학습함으로써 잘 정의된 고품질 표면을 추출할 수 있습니다.2. 요약하자면, 우리의 주요 기여는 다음과 같습니다. • 단일 이미지에서 전체 360도 모델을 재구성하는 새로운 프레임워크를 소개합니다. 우리의 프레임워크는 기성품 사전 학습을 활용하여 범주별 사전 학습 없이도 실제 RGB 이미지로 일반화됩니다. • 3D 모델 재구성을 위한 가상 지상 진실 이미지를 생성하는 점진적 아웃페인팅 방식을 개발합니다. 우리의 방법은 입력 이미지와 자연스럽게 조화를 이루는 새로운 뷰 이미지로 충실한 재구성을 보장합니다. 우리의 모델 설계는 기하학적 일관성과 광도 일관성을 모두 고려하여 충실도 높은 모양과 외관 재구성을 실현합니다. • 우리는 우리 프레임워크가 새로운 관점 합성 및 기하 재구성의 관점에서 단일 RGB 이미지로부터 최첨단 360° 재구성 결과를 생성할 수 있음을 보여줍니다.
--- RELATED WORK ---
S Few-View-to-3D Reconstruction NERF[Mildenhall et al. 2020]와 그 변형[Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020]은 카메라 포즈와 페어링된 RGB 이미지만 주어진 장면과 객체의 Space Carved Outpainting 재구성 성능을 사용하여 단일 이미지에서 놀라운 360° 재구성을 보여주었습니다. 그러나 밀도가 높은 카메라 뷰가 없으면 신경 광도장을 학습하는 것이 심각하게 제약이 부족한 문제가 됩니다. 뷰가 몇 개만 주어지면 이러한 모델은 주어진 각 뷰에 과적합되어 새로운 뷰를 렌더링할 때 깨진 지오메트리와 흐릿한 노이즈가 발생할 수 있습니다[Jain et al. 2021]. 최근에는 고충실도 재구성에 필요한 뷰 수를 줄이기 위한 작업 라인이 도입되었습니다[Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. 그럼에도 불구하고 적절한 재구성을 위해서는 여전히 단일 뷰 이상이 필요합니다. 2.2 단일 뷰에서 3D로의 재구성 단일 이미지에서 3D 모델을 재구성하는 초기 작업의 대부분은 음영 [Zhang et al. 1999], 텍스처 [Loh 2006] 또는 초점 흐림 [Favaro and Soatto 2005]과 같은 이미지에 제공된 가시적 정보에 의존합니다. 최근 작업에서는 입력 이미지의 보이지 않는 부분을 생성하기 위해 보다 일반적인 사전을 사용합니다. 예를 들어, 일부
--- METHOD ---
. (b) 및 (c)의 결과는 증류 손실 및 신경 밀도 필드의 순진한 사용으로 인해 최적이 아닌 새로운 뷰와 저충실도 표면이 생성됨을 보여줍니다 [Melas-Kyriazi et al. 2023; Xu et al. 2023]. 반면에, 저희 프레임워크는 원래 입력 이미지와 유사한 새로운 뷰를 성공적으로 생성하고 (d) 및 (e)에서 볼 수 있듯이 높은 충실도로 3D 객체의 표면을 재구성합니다. (a)의 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [Horton, CC BY]. 초록 단일 이미지에서 전체 360° 뷰 3D 모델을 만드는 새로운 프레임워크인 POP3D를 소개합니다. POP3D는 단일 뷰 재구성을 제한하는 두 가지 중요한 문제를 해결합니다. 첫째, POP3D는 임의의 범주에 대한 상당한 일반화 가능성을 제공하는데, 이는 이전 방법에서는 달성하기 어려운 특성입니다. 둘째, POP3D는 동시 작업에서 부족한 중요한 측면인 재구성 충실도와 자연스러움을 더욱 개선합니다. 저희의 접근 방식은 네 가지 주요 구성 요소의 강점을 결합합니다. (1) 중요한 기하학적 단서를 예측하는 데 사용되는 단안 깊이 및 정상 예측기, (2) 대상 객체의 잠재적으로 보이지 않는 부분을 구분할 수 있는 공간 조각 방법, (3) 보이지 않는 를 완성할 수 있는 대규모 이미지 데이터 세트에서 사전 학습된 생성 모델 이 작업의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 하드 카피로 만드는 허가는 이윤 또는 상업적 이점을 위해 사본을 만들거나 배포하지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문을 표시하는 경우 무료로 부여됩니다. 저자가 아닌 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 다른 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org에서 허가를 요청하세요. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 © 2023 저작권은 소유자/저자가 소유합니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.대상 영역, 및 (4) 단안 기하학적 단서와 함께 RGB 이미지를 사용하여 객체를 재구성하는 데 맞춤화된 신경 암묵적 표면 재구성 방법. 이러한 구성 요소를 결합하면 POP3D는 다양한 야생 이미지에서 쉽게 일반화하고 최첨단 재구성을 생성하여 유사한 작업보다 상당한 마진으로 성능이 향상됩니다. 프로젝트 페이지: http://cg.postech.ac.kr/research/POP3D. CCS 개념 • 컴퓨팅 방법론 → 재구성; 컴퓨터 그래픽; 인공 지능. 키워드 단일 뷰 3D 재구성, 모양 및 외관 재구성, 새로운 뷰 합성, 공간 조각, 아웃페인팅 ACM 참조 형식: 류 누리, 공민수, 김건웅, 이주행, 조성현. 2023. 공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성. SIGGRAPH Asia 2023 Conference Papers(SA Conference Papers &#39;23), 2023년 12월 12-15일, 호주 NSW 시드니. ACM, 뉴욕, 뉴욕, 미국, 11페이지. 한국어: https://doi.org/10.1145/3610548.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 서론 최소한의 입력으로 고품질의 사실적인 3D 모델을 생성하는 능력은 컴퓨터 그래픽, 비전, 가상 현실 및 증강 현실의 다양한 응용 분야에서 지속적인 과제입니다. 신경 표현의 미분 가능한 렌더링을 통한 다중 뷰 재구성 분야의 최근 진전에도 불구하고 [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], 이러한 방법은 카메라 매개변수와 페어링된 방대한 양의 이미지에 크게 의존합니다. 이러한 의존성은 인상적인 결과를 가져올 수 있지만, 특히 객체의 여러 뷰를 얻는 것이 비실용적이거나 불가능한 시나리오에서 실용성과 접근성을 저해합니다. 실제 시나리오에서 사용자는 객체의 단일 뷰만 가질 수 있습니다. 예를 들어, 이미지는 쉽게 접근할 수 없는 객체의 사진이거나 2D 생성 모델의 출력일 수 있습니다. 결과적으로 단일 이미지에서 3D 모델을 생성하는 것은 엄청난 실용적 의미를 가지며, 더 광범위한 응용 프로그램을 가능하게 하고 3D 모델링을 더 광범위한 사용자 기반이 더 쉽게 접근할 수 있게 합니다. 그 실용적 중요성으로 인해 단일 이미지에서 3D 재구성은 활발한 연구 분야였습니다. 그러나 기존 방법은 여전히 일반화 가능성과 재구성 충실도라는 두 가지 주요 문제로 어려움을 겪고 있습니다. 단일 뷰 재구성을 위해 3D 데이터 또는 객체 중심 비디오에서 학습하는 다양한 방법이 제안되었습니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. 그러나 이러한 데이터의 수집은 구조화되지 않은 2D 데이터를 수집하는 것에 비해 종종 더 어려워서 이러한 방법의 확장성과 일반화가 훼손됩니다. 2D 이미지 데이터에 의존하여 3D 데이터의 필요성을 우회하는 다른 기술도 제안되었지만 이러한 방법은 종종 특정 범주에 묶여 있기 때문에 [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021] 일반화가 제한됩니다. 증류 손실 [Poole et al. 2023]을 통해 대규모 이미지 사전 [Rombach et al. 2022]을 활용하는 동시 방법 [Deng et al. 2023; Melas-Kyriazi et al. 2023; Xu et al. 2023]은 입력 뷰를 충실하게 재구성하지 못하는 경우가 많습니다. 이러한 불일치는 증류 손실이 입력 뷰의 RGB 재구성 손실을 방해하고 재구성의 제한된 대상 해상도가 이 문제를 더욱 악화시키기 때문에 발생합니다. 나아가, 순진한 신경 밀도 필드를 사용하면 종종 충실도가 낮은 표면 재구성으로 이어집니다. 이 논문에서는 앞서 언급한 일반화 가능성 및 재구성 충실도 문제를 해결하도록 설계된 새로운 프레임워크인 POP3D(Progressive OutPainting 3D)를 제시합니다. 일반화 가능성의 과제를 해결하기 위해, 저희 프레임워크는 대규모 데이터 세트에서 사전 학습된 다양한 사전 확률의 힘을 활용합니다. 이 접근 방식은 임의의 범주에서 단일 RGB 이미지에서 3D 재구성의 고유한 부적절함을 효과적으로 완화합니다. 객체의 전체 360° 뷰를 포괄하는 충실도 높은 재구성의 경우, 저희는 대규모 생성 모델을 통해 주어진 뷰의 품질과 일치하는 새로운 뷰를 생성합니다. 이러한 새로운 뷰는 단안 기하학 예측과 함께 의사-지상-진실 데이터 세트를 형성합니다. 단안 기하학적 단서를 통합하도록 맞춤화된 훈련 전략에 따라 이 데이터 세트에서 훈련함으로써, 그림 1에서 설명한 바와 같이 동시 작업과 비교하여 높은 충실도로 Ryu et al.이 제공한 단일 이미지의 신경 암묵적 표면과 해당 모양을 재구성합니다.자세히 설명하자면, 프레임워크는 최첨단 단안 깊이 및 일반 예측자[Bhat et al. 2023; Eftekhar et al. 2021]를 사용하여 단일 RGB 입력을 처리하여 기하학적 단서를 추론하는 것으로 시작합니다.입력 RGB와 단안 기하학적 예측은 초기 데이터 세트를 구성하며 MonoSDF[Yu et al. 2022]의 훈련 전략에 따라 초기 3D 모델을 훈련하는 데 사용됩니다.3D 모델을 초기화한 후 대상 객체의 전체 360° 보기를 포함하는 카메라 일정에 따라 카메라 위치를 업데이트합니다.그런 다음 프레임워크는 객체의 시각적 껍질[Laurentini 1994]을 찾아 대상 객체의 보이는 영역과 잠재적으로 보이지 않는 영역을 계산합니다. 보이는 영역을 제거함으로써, 우리는 객체의 자연스러운 새로운 뷰를 생성하는 생성 모델의 가이드로 사용되는 아웃페인팅 마스크를 얻습니다. 구체적으로, 우리는 마스크와 텍스트 조건이 주어진 이미지를 아웃페인팅할 수 있는 대규모 데이터 세트[Schuhmann et al. 2022]에서 훈련된 조건부 확산 모델[Rombach et al. 2022]을 사용합니다. 아웃페인팅된 결과의 단안 기하학 정보를 추출하는 프로세스 후, 우리는 처리된 데이터로 우리의 의사-지상-진실 데이터 세트를 확장합니다. 그런 다음 이 업데이트된 데이터 세트를 사용하여 3D 모델을 다시 훈련하고, 우리는 궁극적으로 충실도 높은 360° 3D 재구성으로 이어지는 객체의 전체 360° 뷰를 포함하는 데이터 세트를 생성할 때까지 이 점진적인 아웃페인팅 프로세스를 반복합니다. 우리의 프레임워크는 몇 가지 독특한 이점을 제공합니다. 첫째, 대규모 데이터 세트에서 학습한 사전 지식 덕분에 우리의 프레임워크는 특정 범주의 객체에 국한되지 않고 임의의 범주의 광범위한 객체를 처리할 수 있습니다. 둘째, 우리의 프레임워크는 기성품 모델에서 이미 학습한 사전 확률을 채택하므로 다중 뷰 이미지나 3D 기하 구조와 같은 추가적인 외부 학습 데이터가 필요하지 않습니다.셋째, 대상 객체의 360도 뷰 데이터 세트를 구축하는 우리의 점진적 아웃페인팅 방식은 고품질의 새로운 뷰 이미지 생성과 입력 이미지의 충실한 재구성을 보장합니다.마지막으로, 가상 지상 진실 데이터 세트를 사용하여 신경 암묵적 표면 표현을 학습함으로써 잘 정의된 고품질 표면을 추출할 수 있습니다.2.요약하자면, 우리의 주요 기여는 다음과 같습니다.• 단일 이미지에서 전체 360도 모델을 재구성하는 새로운 프레임워크를 소개합니다.우리의 프레임워크는 기성품 사전 확률을 활용하여 범주별 사전 학습 없이도 실제 RGB 이미지로 일반화됩니다.• 3D 모델 재구성을 위한 가상 지상 진실 이미지를 생성하는 점진적 아웃페인팅 방식을 개발합니다.우리의 방법은 입력 이미지와 자연스럽게 조화를 이루는 새로운 뷰 이미지로 충실한 재구성을 보장합니다. 우리의 모델 설계는 기하학적 일관성과 광도적 일관성을 모두 고려하여 고충실도의 모양과 외관 재구성을 이룹니다.• 우리는 우리의 프레임워크가 새로운 뷰 합성 및 기하 재구성 측면에서 단일 RGB 이미지로부터 최첨단 360° 재구성 결과를 생성할 수 있음을 보여줍니다.관련 연구 Few-View-to-3D Reconstruction NERF [Mildenhall et al. 2020]와 그 변형 [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020]은 공간 조각된 아웃페인팅을 사용하여 단일 이미지에서 놀라운 360° 재구성을 보였습니다.카메라 포즈와 페어링된 RGB 이미지만 주어진 장면과 객체의 재구성 성능입니다.그러나 고밀도 카메라 뷰가 없으면 신경 광도장을 학습하는 것이 심각하게 제약이 부족한 문제가 됩니다.몇 개의 뷰만 주어지면 이러한 모델은 주어진 각 뷰에 과적합되어 새로운 뷰를 렌더링할 때 깨진 기하 구조와 흐릿한 노이즈가 발생할 수 있습니다 [Jain et al. 2021]. 최근, 고충실도 재구성에 필요한 뷰 수를 줄이기 위한 작업 라인이 도입되었습니다 [Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. 그럼에도 불구하고 적절한 재구성을 위해서는 여전히 두 개 이상의 뷰가 필요합니다. 2.2 단일 뷰에서 3D로 재구성 단일 이미지에서 3D 모델을 재구성하는 초기 작업의 대부분은 음영 [Zhang et al. 1999], 텍스처 [Loh 2006] 또는 디포커스 [Favaro and Soatto 2005]와 같은 이미지에 제공된 가시적 정보에 의존합니다. 최근 작업에서는 입력 이미지의 보이지 않는 부분을 생성하기 위해 보다 일반적인 사전을 사용합니다. 예를 들어, 일부 방법은 재구성에 사용할 수 있는 3D 사전 확률을 학습하기 위해 3D 데이터 세트를 사용합니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Saito et al. 2019; Wang et al. 2018; Xie et al. 2019]. 그러나 이러한 모델이 실제 이미지로 일반화하려면 대규모 3D 데이터 세트가 필요합니다. 58억 5천만 개의 이미지-텍스트 쌍을 제공하는 LAION-5B와 같은 대규모 2D 이미지 데이터 세트와 비교할 때[Schuhmann et al. 2022], 3D 데이터 세트는 종종 다양성과 규모 면에서 제한적입니다. 반면에, 우리 모델은 3D 학습 데이터가 전혀 필요하지 않지만 기하학과 이미지 사전 확률을 활용하여 실제 이미지로 일반화할 수 있습니다[Bhat et al. 2023; Eftekhar et al. 2021; Rombach et al. 2022] 대규모 데이터 세트에서 학습되었습니다. 3D 학습 데이터 세트가 필요하다는 것에서 발생하는 문제를 극복하기 위해 이미지 컬렉션에서 3D 구조를 학습하는 방법이 도입되었습니다[Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Henzler et al. 2019; Jang and Agapito 2021; Kanazawa et al. 2018; Karnewar et al. 2023; Lin et al. 2023; Pavllo et al. 2023; Vasudev et al. 2022; Wu et al. 2023b; Ye et al. 2021]. 그러나 의미적 키 포인트 및 분할 마스크와 같은 추가 주석[Kanazawa et al. 2018]이나 정확한 카메라 매개변수가 있는 동일한 장면의 다중 뷰 이미지[Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Karnewar et al. 2023; Lin et al. 2023; Vasudev et al. 2022]. 장면당 단일 뷰로 학습하는 다른 방법은 범주별입니다[Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021]. 이와 대조적으로, 우리 모델은 우리가 통합하는 기성 모델 덕분에 단일 RGB 이미지 외에는 추가 정보가 필요하지 않습니다. 또한, 우리 모델은 주어진 뷰의 범주에 관계없이 실제 이미지로 일반화할 수 있다고 강조합니다. 3D 확산 모델[Shue et al. 2023; Wang et al. 2023b]도 주목을 받고 있지만, 동시 작업[Deng et al. 2023; Melas-Kyriazi et al. 2023; Tang et al. 2023; Xu et al. 2023]은 대규모 이미지-텍스트 데이터 세트[Schuhmann et al. 2022]에서 학습된 2D 확산 모델[Rombach et al. 2022]을 단일 뷰 재구성을 위한 사전 확률로 직접 사용하려고 시도했습니다.참조 뷰에서 보이지 않는 영역을 생성하기 위해 Poole et al. [2023]에서 도입한 점수 증류 샘플링 손실과 유사한 증류 손실에 크게 의존합니다.문제는 증류 손실이 주어진 단일 뷰에서 겹치는 영역이 있는 뷰에 동시에 적용된다는 것입니다.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니.이것은 종종 RGB 손실을 방해하고 결과적으로 종종 입력 뷰의 재구성이 불량하게 됩니다.아주 최근의 작업[Tang et al. 2023]은 참조 이미지를 훈련된 3D 표현에 투영하여 이 문제를 우회하려고 시도하지만 참조 뷰에서 멀리 떨어진 새로운 뷰는 품질이 부족한 경향이 있습니다.반대로, 우리의 프레임워크는 다중 뷰 이미지로 구성된 의사 지상 진실 데이터 세트를 구축하여 다중 뷰 재구성 전략을 사용하여 고충실도 재구성을 가능하게 합니다.또한 이러한 모델[Deng et al. 2023; Melas-Kyriazi et al. 2023; Tang et al. 2023; Xu et al. 2023]에는 다른 한계도 있습니다.첫째, 96×96 또는 128×128과 같이 대상 해상도가 낮은 반면, 우리의 방법은 384×의 해상도를 목표로 하여 더 높은 품질과 전반적인 세부 정보를 제공하는 결과를 얻습니다.Tang et al. [2023]은 2단계 학습 방식을 통해 이 문제를 극복하려고 하지만 아래에 설명된 다른 문제도 공유합니다. 둘째, 이러한 작업은 기하 표현으로 순진한 신경 밀도 함수를 사용하는데, 이는 잘 정의된 표면 임계값이 없기 때문에 노이즈가 많은 아티팩트를 생성할 수 있습니다.반대로, 우리의 방법은 단순히 학습된 신경 암묵적 표면의 제로 레벨 세트에서 고충실도 기하 추출을 허용합니다.마지막으로, 이러한 모델은 주어진 단일 이미지와 그 증강에만 의존하여 텍스트 역전[Gal et al. 2023]과 유사한 방법을 사용하여 입력 이미지와 일치하는 보이지 않는 영역을 생성하려고 시도하여 확산 모델을 개인화합니다.이러한 방법과 달리, 우리의 데이터 생성 프레임워크는 다중 뷰 의사 지상 진실 이미지를 사용하여 동일한 객체의 여러 뷰가 필요한 최첨단 확산 모델 개인화 방법인 DreamBooth[Ruiz et al. 2023]를 사용할 수 있으므로 더 나은 개인화 품질이 가능합니다.Raj et al.[2023]도 DreamBooth를 사용하여 고품질의 개인화된 텍스트-3D를 달성할 수 있음을 보여주었습니다. 그러나 그들의 방법은 대상 객체의 여러 뷰를 필요로 하는 반면, 우리의 방법은 우리의 의사-지상-진실 다중 뷰 생성 방식 덕분에 객체의 단일 뷰만 필요합니다.단일 뷰에서 포인트 클라우드로.다른 최근 연구는 참조 뷰를 기반으로 색상화된 포인트 클라우드를 재구성하는 것을 목표로 합니다.예를 들어, MCC [Wu et al. 2023a]는 RGB-D 이미지를 입력으로 사용하여 리프팅된 색상 포인트를 대상 객체의 완전한 포인트 클라우드로 재구성합니다.마찬가지로, Point-E [Nichol et al. 2022]는 참조 RGB 이미지를 사용하여 입력 이미지와 유사한 색상화된 포인트 클라우드를 생성하는 포인트 클라우드 확산 모델을 도입합니다.이러한 모델과 달리, 우리의 프레임워크는 고충실도 신경 암묵적 표면과 뛰어난 품질의 모양을 재구성합니다.3D 사진.또 다른 작업 라인은 단안 깊이 예측과 색상 인페인팅을 사용하여 단일 이미지에서 3D 사진이나 장면을 생성합니다 [Han et al. 2022; Höllein et al. 2023; Shih et al. 2020; Zhang et al. 2023]. 그러나 이러한 방법은 전경과 배경을 동시에 인페인팅하도록 설계되었으며 객체의 뒷면을 고려하지 않습니다. 따라서 객체의 360° 재구성에 직접 적용할 수 없습니다. 단일 뷰에서의 새로운 뷰 합성. 일부 작업[Liu et al. 2023; Watson et al. 2023]은 입력 이미지와 상대 포즈가 주어졌을 때 3D 새로운 뷰를 생성하는 데 중점을 둡니다. 그러나 이러한 모델의 출력은 대략 3D 일관성만 있으므로 충실도 높은 모양 재구성을 보장하지 않습니다. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 초기화 카메라 위치 업데이트 초기화 아웃페인팅 마스크 수집 카메라 위치 업데이트 아웃페인팅 아웃페인팅 마스크 수집 프로세스 RGB 훈련 시각적 헐 추출 단일 RGB 아웃페인팅 볼륨 렌더링 초기 데이터 세트 초기화된 3D 모델 업데이트된 카메라 이전 카메라 &quot;[Dir]에서 본 흰색 배경의 [V] [Class] 사진&quot; 프로세스 RGB LDM 업데이트된 카메라에서 실루엣 획득 ה Ryu et al. 3D 모델 업데이트 보이는 영역 빼기 훈련 아웃페인팅 마스크 3D 모델 업데이트 훈련된 3D 모델 초기 새로운 뷰 업데이트된 가상 지상 진실 데이터 세트 훈련된 3D 모델 그림 2: 프레임워크 개요.POP3D는 상호 연결된 5단계로 작동합니다.처음에는 단일 RGB 입력을 처리하여 예비 가상 지상 진실 데이터 세트를 만들고 이 데이터를 사용하여 3D 모델을 초기화합니다. 그런 다음 대상 객체의 360° 뷰 전체를 포괄하는 것을 목표로 하는 단계 루프를 진행합니다. 이 루프에는 다음이 포함됩니다. 미리 정해진 일정에 따라 카메라 위치 업데이트, 가상 지상 진실 데이터 세트에서 시각적 헐을 추출하고 보이는 영역을 빼서 아웃페인팅 마스크 획득, 훈련된 3D 모델의 초기 신규 뷰, 아웃페인팅 마스크 및 적절한 텍스트 프롬프트를 사용하여 가상 지상 진실 신규 뷰 생성, 업데이트된 가상 지상 진실 데이터 세트를 사용하여 3D 모델 훈련. 이 프로세스는 객체의 360° 뷰를 포함할 때까지 계속됩니다. 입력 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©laboratorija, CC BY-NC-SA]. 그림 3: 시각적 헐 추출. 두 카메라 뷰에서 시각적 헐을 획득하는 방법을 설명합니다. 대상 객체의 보이는 영역과 보이지 않는 영역을 모두 구성하는 오른쪽에 보이는 음영 처리된 영역을 보존합니다. 영어: 자동차 이미지: freesvg에서 가져옴 [퍼블릭 도메인].방법 그림 2에서 POP3D에 대한 개요를 제시합니다.객체의 단일 이미지가 주어지면 프레임워크는 신경 암묵적 표면 표현을 사용하여 360° 모양과 모양을 재구성합니다.핵심 아이디어는 색상과 기하학적 정보를 합성하여 객체의 보이지 않는 영역을 점진적으로 아웃페인팅하는 것입니다.이를 위해 프레임워크는 초기화, 카메라 위치 업데이트, 아웃페인팅 마스크 획득, 아웃페인팅, 3D 모델 업데이트의 5단계로 구성됩니다.초기화 단계에서는 입력 이미지의 깊이와 법선 s를 추정하여 3D 뷰로 끌어올립니다.그런 다음 카메라 위치를 이전에 본 적이 없는 근처 관점으로 맵을 업데이트하고 공간 조각을 사용하여 아웃페인팅할 영역을 나타내는 아웃페인팅 마스크를 얻습니다[Kutulakos and Seitz 1999].다음으로 잠재 확산 모델(LDM)을 사용하여 색상과 기하학적 정보를 생성하여 마스크된 영역을 아웃페인팅합니다[Rombach et al. 2022]. 마지막으로, 우리는 아웃페인팅된 정보를 사용하여 객체의 3D 모델을 업데이트합니다. 우리는 객체의 360° 전체를 커버할 때까지 이 단계를 반복합니다. 3D 객체의 모양과 외관을 표현하기 위해 우리는 신경망 쌍을 사용하여 3D 객체를 표현하는 VolSDF [Yariv et al. 2021]를 채택합니다. 구체적으로, 객체의 기하 구조를 표현하기 위해 우리는 3D 지점 x € R³를 표면과의 부호 거리 s = R에 매핑하는 부호 거리 함수(SDF) fo : x → s를 모델링하는 신경망을 사용합니다. 외관을 설명하기 위해 우리는 또 다른 신경망을 사용합니다. 여기서 ŵn은 지점 x에서 SDF의 공간적 기울기입니다. z는 Yariv et al. [2020]에서와 동일한 전역 기하 구조 특징 벡터입니다. VolSDF와 달리, 우리는 Lê에 대한 입력으로 시야 방향을 제공하지 않고, 단일 이미지가 시야에 따른 조명 정보를 제공하지 않고 기존의 아웃페인팅 방법이 시야 종속성을 고려하지 않기 때문에 시야에 따른 색상 변화를 무시합니다. 단일 이미지에서 3D 모델을 생성하는 것은 매우 잘못된 작업이므로, 우리는 재구성 결과의 가능한 결과를 제한하기 위해 몇 가지 가정을 부과합니다. 첫째, 우리는 대상 객체가 중심이 원점에 있고 길이가 2인 모서리가 좌표 축과 정렬된 큐브 내에 있다고 가정하고, Atzmon과 Lipman [2020]에 따라 객체를 단위 구로 초기화합니다. 또한 3D 재구성 프로세스 동안 대상 3D 객체를 바라보는 가상 카메라를 가정합니다. 구체적으로, 우리는 원점을 가리키도록 반경 3의 구에 카메라를 배치하고 구면 좌표 각도를 사용하여 위치를 매개변수화합니다. 입력 이미지의 카메라 매개변수가 주어지지 않았다고 가정할 때, 카메라의 시야(FOV)는 60°로 설정됩니다. 다음에서 프레임워크의 각 단계를 자세히 설명합니다.공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성 3. 초기화 초기화 단계는 대상 객체의 입력 이미지 Lo에서 초기 3D 모델을 구성합니다.특히 Lo가 주어지면 먼저 기성품 바이너리 분할 방법[Lee et al. 2022]을 사용하여 바이너리 마스크 Mo를 추정하여 전경 객체를 추출합니다.그런 다음 기성품 단안 깊이 및 노멀 추정기를 사용하여 전경 객체의 깊이 맵 Do와 노멀 맵 No를 추정합니다[Bhat et al. 2023; Eftekhar et al. 2021].추정된 깊이 및 노멀 맵과 바이너리 마스크를 사용하여 초기 3D 모델을 추정합니다. 구체적으로, 먼저 의사-지상 진실 데이터 집합 P를 P = {(Lo, Do, No, Mo, 0)}로 초기화합니다. 여기서 0은 초기 카메라 위치이고, P를 사용하여 초기 3D 모델(fe, Lė)의 암묵적 표현을 학습합니다. 초기 카메라 위치는 0 = (90°, 0°)로 설정됩니다. 여기서 첫 번째와 두 번째 각도는 각각 극각과 방위각이며, 초기 이미지에 대상 객체의 정면이 포함되어 있다고 가정합니다. 의사-지상 진실 데이터 집합은 다음 단계에서 반복적으로 업데이트되어 대상 객체의 3D 모델을 점진적으로 재구성합니다. 암묵적 표현을 학습하기 위해 MonoSDF [Yu et al. 2022]의 접근 방식을 채택하여 마스크 Mo를 고려하도록 약간 수정했습니다. 학습에 대한 자세한 내용은 보충 자료를 참조하세요. 3. 카메라 위치 업데이트 모델이 초기화된 후 카메라 위치를 변경하여 대상 객체의 보이지 않는 영역을 탐색하면서 3D 모델을 반복적으로 업데이트합니다. 이를 위해 우리는 대상 객체의 360° 뷰를 포함하도록 설계된 카메라 스케줄 S = [40, 1, ..., s]를 정의하고 S에 따라 각 반복에서 카메라 위치를 업데이트합니다. 이론적으로 카메라 스케줄은 전체 360° 뷰를 포함하는 경우 임의의 집합이 될 수 있습니다. 그러나 우리는 지나치게 작거나 큰 간격이 출력에 부정적인 영향을 미칠 수 있음을 발견했습니다. 따라서 우리는 우리의
--- EXPERIMENT ---
s, 그리고 4.2.1절에서 지나치게 세분화되거나 거친 카메라 일정의 부정적인 영향에 대해 논의합니다. 이 섹션의 나머지 부분에서는 S에서 i번째 카메라 위치까지 탐색된 카메라 위치를 So:i로 표시하여 0 ≤ i ≤ s가 되도록 합니다. 3.3 아웃페인팅 마스크 획득 보이지 않는 영역의 모양과 모양을 원활하게 생성하려면 아웃페인팅을 위해 지정된 영역을 적절히 선택해야 합니다. 이를 해결하기 위해 시각적 껍질[Laurentini 1994]의 개념을 활용합니다. 시각적 껍질은 다양한 관점에서 본 객체의 실루엣에서 파생된 객체 모양의 대략적인 근사치를 제공합니다. 현재 데이터 세트 P를 사용하여 시각적 껍질을 계산하여 그림 3에서 설명한 대로 객체가 차지할 수 있는 최대 가능 영역을 결정할 수 있습니다. 업데이트된 카메라 뷰에서 보이는 시각적 껍질의 실루엣을 계산함으로써 이전에 관찰된 영역과 잠재적으로 보이지 않는 영역을 모두 포함하는 초기 마스크를 얻습니다. 아웃페인팅 마스크를 생성하기 위해 이 초기 마스크에서 관찰된 영역을 빼서 잠재적으로 새로운 가시 영역만 남겨둡니다.공간 조각을 통한 시각적 헐 계산.시각적 헐을 계산하기 위해 [Laurentini 1994], 투표 방식 [Kutulakos and Seitz 1999]에 의해 구동되는 깊이 기반 폭셀 조각 방법을 사용합니다.이 프로세스는 먼저 객체 경계 큐브를 폭셀화합니다.현재 SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니에서 So:i–1의 카메라 위치를 탐색했다고 가정합니다.그런 다음 폭셀화된 경계 큐브의 폭셀 p에 대해 j번째 뷰에 대한 투영이 j = {0,..., i - 1}인 전경 영역 내부에 있고 j번째 뷰의 카메라 중심까지의 거리가 전경 영역과 j번째 뷰의 카메라 중심 사이의 거리보다 길면 투표를 합니다. 수학적으로, KPjp € Mj, ||p − 0;||2 ≥ ||p* − 0j||2, (1)일 때 p에 대한 투표를 제기합니다. 여기서 K는 카메라 고유 행렬입니다. P; 및 o;는 각각 P의 j번째 뷰의 카메라 공간과 카메라 중심으로의 투영을 나타냅니다. p*는 fo의 제로 레벨 세트와 o;에서 p로 향하는 레이 캐스트의 교차점입니다. 여기서 우리는 레이가 외부에서 처음으로 객체를 관통하는 첫 번째 교차점만 고려합니다. 총 투표 수가 P의 크기 또는 뷰 수와 같으면 폭셀이 보존됩니다. 그렇지 않으면 폭셀이 조각됩니다. 이 절차를 통해 P의 시각적 껍질을 구성하는 폭셀을 수집합니다. 추가적으로, 단일 이미지만 있는 경우 이 프로세스는 훈련된 3D 표면의 돌출로 생각할 수 있습니다. 시각적 헐을 i번째 관점에 투영함으로써 워핑 연산을 통한 실루엣 MVH 전경 마스크 계산을 얻습니다. MVH에는 보이는 영역과 보이지 않는 영역이 모두 포함되어 있으므로 보이는 영역을 빼서 아웃페인팅 마스크 M¿를 얻어야 합니다. 이는 워핑 연산을 사용하여 대상 뷰에서 전경 마스크 MEG를 계산함으로써 달성됩니다. 이 프로세스에는 이전 관점 So:i-1에서 깊이를 렌더링하고, 이미지 포인트를 3D 공간으로 들어올린 다음, 들어올린 포인트를 대상 뷰 i에 투영하는 것이 포함됩니다. 워핑 프로세스 중에 앨리어싱을 완화하기 위해 이미지를 배율 인수 8로 확대합니다. 가시성을 고려하고 백포인트 컬링을 통해 대상 관점에서 보이지 않는 픽셀을 워핑하지 않습니다. 구체적으로 백포인트 컬링을 포함한 워핑 프로세스는 다음과 같이 수행됩니다. j번째 관점에서 i번째 관점으로 이미지를 워핑하는 프로세스에서 P의 j번째 이미지에서 들어올린 픽셀을 p³로 표시합니다. 그런 다음, ƒÃ에서 정상 Ñ³를 렌더링합니다. 픽셀은 다음의 경우에만 대상 카메라 중심 o에 대해 워핑됩니다. (p³ – oi) · Ñ³
--- CONCLUSION ---
및 향후 작업 이 연구에서는 단일 RGB 이미지에서 360° 재구성 분야의 두 가지 오랜 과제인 일반화와 충실도를 해결하는 새로운 프레임워크인 POP3D를 제시합니다.POP3D는 대규모 데이터 세트에서 학습된 최신 사전 확률을 최대한 활용하고 일반화 문제를 성공적으로 극복합니다.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 Ryu et al. (a) 입력 RGB 이미지 (b) 새로운 뷰 (c) 전체 뒷면 그림 7: 제한 사항.왼쪽의 입력 RGB(a)가 주어지면, 우리 모델은 카메라 일정에 따라 그럴듯한 새로운 뷰(b)를 생성합니다.그러나 우리 모델은 때때로 (c)에서 볼 수 있듯이 입력 단일 RGB와 비교했을 때 열악한 전체 뒷면 이미지를 생성합니다.(a)의 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [ShekhirevaVictoria, CC BY]. 임의의 이미지에. 실험적으로, 우리의 프레임워크가 제공된 단일 RGB 입력을 충실하게 재구성할 뿐만 아니라 현실적인 새로운 뷰도 생성한다는 것을 보여줍니다. 이러한 뷰는 집합적으로 의사-지상-진실 다중 뷰 데이터 세트를 형성하여 다중 뷰 재구성 전략의 직접적인 적용을 용이하게 합니다. 기존 방법론과 비교할 때, 우리의 접근 방식은 더 뛰어난 성능을 보여 3D 재구성 작업을 위한 견고한 솔루션으로서의 잠재력을 재확인합니다. 5. 제한 사항 우리의 접근 방식에는 특정한 제한 사항이 있습니다. 우리의 프레임워크는 파이프라인에서 각각 중요한 역할을 하는 기성품 사전 확률의 구성이기 때문에 한 모델이 실패하면 최종 재구성 결과에 영향을 미칠 수 있습니다. 예를 들어, 단안 깊이 또는 일반 예측 변수가 얇은 구조와 같은 어려운 경우에 실패하면 재구성된 모양에 아티팩트가 발생할 수 있습니다. 게다가, 우리의 프레임워크는 때때로 입력 뷰와 비교했을 때 객체의 완전한 뒷면을 열악한 품질로 생성합니다. 그림 7에서 이 문제를 설명합니다. 이러한 결함은 기성품 사전 모델의 성능을 손상시키고 장기적으로 전체 이미지 품질을 저하시킬 수 있는 아웃페인팅 아티팩트의 축적에 기인할 수 있습니다. 저희의 접근 방식은 가상 지상 진실 이미지를 생성하여 뷰 수를 점진적으로 늘리기 때문에 3D 모델 학습과 관련된 계산 시간도 카메라 일정에 따라 증가합니다. 단일 객체를 재구성하는 데 현재 실행 시간은 단일 3090 RTX GPU를 사용하여 약 7시간이 걸립니다. 그럼에도 불구하고 저희 프레임워크는 모듈식 구조를 가지고 있으며 각 단계에서 사용된 모델을 쉽게 교체할 수 있습니다. 특히 VolSDF [Yariv et al. 2021]를 보다 고급 방법 [Rosu and Behnke 2023; Wang et al. 2022]으로 교체하여 재구성 프로세스를 가속화할 수 있습니다. 나아가 DreamBooth [Ruiz et al. 2023]를 가속화하기 위해 LORA [Hu et al. 2022]를 채택할 수도 있습니다. 이러한 문제를 해결하기 위해 향후 연구에서는 아티팩트를 최소화하고 재구성 시간을 개선하는 동시에 재구성 프로세스를 더욱 개선하는 방법을 탐색하는 데 중점을 둘 것입니다.감사의 말 이 연구는 한국 정부(MSIT)가 지원한 IITP 보조금(IITP-2021-0-02068, IITP-2019-0-01906)과 중소기업창업부(MSS, 한국)가 2021년에 지원한 창업성장기술연구개발사업(TIPS 프로그램, (No. S3200141))의 지원을 받았습니다.참고문헌 Matan Atzmon 및 Yaron Lipman. 2020. SAL: 원시 데이터에서 모양에 대한 부호 독립적 학습.CVPR 논문집.Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla 및 Pratul P. Srinivasan. 2021. Mip-NeRF: 앤티 앨리어싱 신경 복사장에 대한 다중 스케일 표현. ICCV의 Proc.에서. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. 2022. Mip-NeRF 360: 무제한 앤티 앨리어싱 신경 복사장. CVPR의 Proc.에서. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, Matthias Müller. 2023. ZoeDepth: 상대적 깊이와 메트릭 깊이를 결합하여 제로 샷 전송. arXiv:2302.12288 [cs.CV] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. 3D 인식 확산 모델을 사용한 생성적 새로운 뷰 합성. arXiv:2304.02602 [cs.CV] Christopher B Choy, Danfei Xu, Jun Young Gwak, Kevin Chen, and Silvio Savarese. 2016. 3D-R2N2: 단일 및 다중 뷰 3D 객체 재구성을 위한 통합 접근 방식. ECCV 논문에서. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi 및 Ali Farhadi. 2022. Objaverse: 주석이 달린 3D 객체의 세계. arXiv:2212.08051 [cs.CV] Congyue Deng, Chiyu &quot;Max&quot; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas 및 Dragomir Anguelov. 2023. NeRDi: 일반 이미지 우선 순위로서 언어 기반 확산을 사용한 단일 뷰 NeRF 합성. Proc에서 CVPR의. 2063720647. Ainaz Eftekhar, Alexander Sax, Jitendra Malik 및 Amir Zamir. 2021. Omnidata: 3D 스캔에서 다중 작업 중간 수준 비전 데이터 세트를 만드는 확장 가능한 파이프라인. ICCV의 Proc.에서. 10786-10796. P. Favaro 및 S. Soatto. 2005. 디포커스에서 모양을 만드는 기하학적 접근 방식. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 27, 3(2005), 406417. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik 및 Daniel Cohen-or. 2023. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR의 Proc.에서. R. Girdhar, DF Fouhey, M. Rodriguez 및 A. Gupta. 2016. 객체에 대한 예측 가능하고 생성적인 벡터 표현 학습. ECCV 논문에서. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, Mathieu Aubry. 2018. AtlasNet: 3D 표면 생성 학습을 위한 파피에마셰 접근법. CVPR 논문에서. Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi. 2023. NerfDiff: 3D 인식 확산에서 NeRF 유도 증류를 사용한 단일 이미지 뷰 합성. ICML 논문에서. Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, Qi Shan. 2022. 빠르고 명시적인 신경 뷰 합성. WACV 논문집에서. 3791-3800. Yuxuan Han, Ruicheng Wang, Jiaolong Yang. 2022. 학습된 적응형 다중 평면 이미지를 사용한 야생에서의 단일 뷰 뷰 합성. ACM SIGGRAPH 논문집에서. Philipp Henzler, Niloy J Mitra,, Tobias Ritschel. 2019. 플라톤의 동굴 탈출: 적대적 렌더링에서 얻은 3D 모양. ICCV 논문집에서. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022. LORA: 대규모 언어 모델의 저순위 적응. ICLR 논문집에서. Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nieẞner. 2023. Text2Room: 2D 텍스트-이미지 모델에서 텍스처가 있는 3D 메시 추출. arXiv:2303.11989 [cs.CV] Ajay Jain, Matthew Tancik, Pieter Abbeel. 2021. NeRF 다이어트: 의미적으로 일관된 Few-Shot View 합성. ICCV 논문집에서. 5885-5894. Wonbong Jang, Lourdes Agapito. 2021. CodeNeRF: 객체 범주에 대한 얽힌 신경 광도장. ICCV 논문집에서. 12949-12958. Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik. 2018. 이미지 컬렉션에서 범주별 메시 재구성 학습. ECCV 논문집에서. Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy J. Mitra. 2023. HOLODIFFUSION: 2D 이미지를 사용한 3D 확산 모델 훈련. CVPR 논문집에서. 18423-18433. KN Kutulakos와 SM Seitz. 1999. 공간 조각에 의한 모양 이론. ICCV 논문집에서. 307-314 vol.1. A. Laurentini. 1994. 실루엣 기반 이미지 이해를 위한 시각적 껍질 개념. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 16,(1994), 150-162. Min Seok Lee, WooSeok Shin, Sung Won Han. 2022. TRACER: 극도의 주의 유도 돌출 객체 추적 네트워크. CVPR 논문집에서. AAAI Conference on Artificial 360° Reconstruction From a Single Image Using Space Carved Outpainting Intelligence, Vol. 36. 12993-12994. Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. 2023. Vision Transformer for NeRF-Based View Synthesis From a Single Input Image. In Proc. of WACV. 806-815. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. arXiv:2303.11328 [cs.CV] Angeline Loh. 2006. The recovery of 3-D structure using visual texture patterns. 박사 학위 논문. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. 2023. RealFusion: 360도 Reconstruction of Any Object From a Single Image. CVPR 논문집에서. 8446-8455. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ECCV 논문집에서. Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. &quot;완전히 블라인드&quot; 이미지 품질 분석기 만들기. IEEE Signal Processing Letters 20, 3(2013), 209-212. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-E: 복합 프롬프트에서 3D 포인트 클라우드를 생성하는 시스템. arXiv:2212.08751 [cs.CV] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable Volumetric Rendering: 3D Supervision 없이 암묵적 3D 표현 학습. CVPR의 Proc.에서. Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023. Bootstrapped를 통한 단일 이미지의 모양, 포즈 및 외관 Radiance Field Inversion. CVPR의 논문에서. Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall. 2023. DreamFusion: 2D 확산을 사용한 텍스트-3D. ICLR의 논문에서. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 2021. 자연어 감독에서 전이 가능한 시각적 모델 학습. ICML의 논문에서, Vol. 139. 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li 및 Varun Jampani. 2023. DreamBooth3D: 주제 기반 텍스트를 3D로 생성. arXiv:2303.13508 [cs.CV] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan 및 Matthias Nieẞner. 2022. 희소 입력 뷰의 신경 복사 필드에 대한 조밀한 깊이 사전. Proc에서 CVPR의. 로빈 롬바흐(Robin Rombach), 안드레아스 블라트만(Andreas Blattmann), 도미니크 로렌츠(Dominik Lorenz), 패트릭 에세르(Patrick Esser), 비요른 오머(Björn Ommer). 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR 논문집에서. 10684-10695. Radu Alexandru Rosu와 Sven Behnke. 2023. PermutoSDF: Permutohedral 격자를 사용한 암묵적 표면을 사용한 빠른 다중 뷰 재구성. CVPR 논문집에서. 8466-8475. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2023. DreamBooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR 논문집에서. 22500-22510. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li. 2019. PIFu: 고해상도 옷을 입은 인간 디지털화를 위한 픽셀 정렬 암시적 함수. ICCV의 논문. Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lučić, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, Andrea Tagliasacchi. 2022. 장면 표현 변환기: 세트-잠재적 장면 표현을 통한 기하학 없는 새로운 뷰 합성. CVPR의 논문. 6229-6238. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev. 2022. LAION-5B: 차세대 이미지-텍스트 모델을 학습하기 위한 대규모 오픈 데이터 세트. NeurIPS 논문. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang. 2020. 컨텍스트 인식 계층적 깊이 인페인팅을 사용한 3D 사진. CVPR 논문. J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein. 2023. Triplane Diffusion을 사용한 3D 신경장 생성. CVPR 논문집에서. 20875-20886. Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein. 2019. 장면 표현 네트워크: 연속적인 3D 구조 인식 신경 장면 표현. NeurIPS 논문집에서. Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen. 2023. Make-It-3D: 확산 사전을 사용한 단일 이미지에서 고충실도 3D 생성. arXiv:2303.14184 [cs.CV] Kalyan Alwala Vasudev, Abhinav Gupta, Shubham Tulsiani. 2022. 사전 학습, 자체 학습, 증류: 3D 재구성을 위한 간단한 레시피. CVPR의 Proc.에서. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, Pratul P. Srinivasan. 2022. Ref-NeRF: SA 컨퍼런스 논문 &#39;23을 위한 구조화된 뷰 종속적 모양, 2023년 12월 12-15일, 호주 NSW 시드니 신경 복사장. CVPR의 Proc.에서. Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, Chen Change Loy. 2023a. 실제 세계 이미지 초고해상도를 위한 확산 사전 활용. arXiv:2305.07015 [cs.CV] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang. 2018. Pixel2Mesh: 단일 RGB 이미지에서 3D 메시 모델 생성. ECCV의 Proc.에서 Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo. 2023b. RODIN: 확산을 사용하여 3D 디지털 아바타를 조각하기 위한 생성 모델. CVPR의 Proc.에서. 4563-4573. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, Lingjie Liu. 2022. NeuS2: 다중 뷰 재구성을 위한 신경 암묵적 표면의 빠른 학습. arXiv:2212.05231 [cs.CV] Zhou Wang, AC Bovik, HR Sheikh, EP Simoncelli. 2004. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE Transactions on Image Processing 13, 4 (2004), 600-612. Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2023. 확산 모델을 사용한 새로운 뷰 합성. ICLR의 Proc.에서. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023a. 3D 재구성을 위한 다중 뷰 압축 코딩. CVPR의 Proc.에서. 9065-9075. Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. 2023b. MagicPony: 야생에서 관절이 있는 3D 동물 학습. CVPR의 Proc. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou 및 Shengping Zhang. 2019. Pix2Vox: 단일 및 다중 뷰 이미지의 상황 인식 3D 재구성. Proc에서 ICCV의. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang 및 Zhangyang Wang. 2023. NeuralLift-360: 야생 2D 사진을 360도 뷰의 3D 개체로 들어 올리기. Proc에서 CVPR의. 4479-4489. Lior Yariv, Jiatao Gu, Yoni Kasten 및 Yaron Lipman. 2021. 신경 암시적 표면의 볼륨 렌더링. Proc에서 NeurIPS의 Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. 기하학과 외관을 분리하여 다중 시점 신경 표면 재구성. NeurIPS 논문집에서. Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. 2021. 야생에서의 선반 감독 메시 예측. CVPR 논문집에서. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: 하나 또는 몇 개의 이미지에서 추출한 신경 복사장. CVPR 논문집에서. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. MonoSDF: 신경 암묵적 표면 재구성을 위한 단안적 기하학적 단서 탐색. 논문집에서. NeurIPS의 Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao. 2023. Text2NeRF: 신경 복사장을 사용한 텍스트 기반 3D 장면 생성. arXiv:2305.11588 [cs.CV] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. 2020. NeRF++: 신경 복사장 분석 및 개선. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang. 2018. 지각적 지표로서의 딥 피처의 비합리적 효과. CVPR의 Proc.에서. Ruo Zhang, Ping-Sing Tsai, JE Cryer, M. Shah. 1999. 셰이딩에서 모양: 조사. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 21,(1999), 690-706. Zhizhuo Zhou 및 Shubham Tulsiani. 2023. SparseFusion: 3D 재구성을 위한 뷰 조건 확산 증류. CVPR 논문집. 12588-12597. SS3D Pe MCC NL RF H Ours SA 학술 대회 논문 &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 입력 그림 8: 정성적 비교. 다양한 모델을 사용하여 위에 주어진 단일 RGB 이미지의 360° 모양과 외관을 재구성하고 이를 결과와 비교합니다. NL, RF, MCC, Pe 및 SS3D는 NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], SS3D [Vasudev et al. 2022]를 각각 사용합니다. SS3D [Vasudev et al. 2022]는 객체의 모양을 재구성하지 않으므로 모양 출력만 표시합니다. 더 나은 시각화를 위해 MCC [Wu et al. 2023a]에 마칭 큐브를 사용하여 포인트 클라우드를 샘플링하는 데 사용된 것과 동일한 점유 임계값을 가진 표면을 추출합니다. Point-E [Nichol et al. 2022]의 경우 저자가 제공한 포인트 클라우드-메시 변환을 사용합니다. 입력: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©Horton, CC BY]. Ryu et al. SS3D Pe MCC NL RF Ours 360° Reconstruction From a Single Image Using Space Carved Outpainting H Input SA Conference Papers &#39;23, December 12-15, 2023, Sydney, NSW, Australia ] 그림 9: 또 다른 정성적 비교. 다양한 모델을 사용하여 위에 주어진 단일 RGB 이미지의 360° 모양과 외관을 재구성하고 이를 결과와 비교합니다. NL, RF, MCC, Pe 및 SS3D는 각각 NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022] 및 SS3D [Vasudev et al. 2022]를 의미합니다. SS3D [Vasudev et al. 2022]는 객체의 모양을 재구성하지 않으므로 모양 출력만 보여줍니다. 더 나은 시각화를 위해 MCC [Wu et al. 2023a]에 마칭 큐브를 사용하여 포인트 클라우드를 샘플링하는 데 사용된 것과 동일한 점유 임계값을 가진 표면을 추출합니다. Point-E [Nichol et al. 2022]의 경우 저자가 제공한 포인트 클라우드-메시 변환을 사용합니다. 입력: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©shirava, CC BY].
"
"--- ABSTRACT ---
Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. Index Terms— Sound generation, audio-visual learning, video-to-audio generation, multimodal learning 1.
--- METHOD ---
s fail to generalize to open-domain videos. Recent advancements, however, indicate a rising interest in open-domain, visually guided audio generation. SpecVQGAN [9] and IM2WAV [10] both employ a language modeling method, leveraging the Transformer model to cap --- --ture the joint distribution of visual features and discrete audio tokens encoded by vector-quantized variational autoencoder (VQ-VAE). In SpecVQGAN, the VQ-VAE operates specifically on spectrograms and subsequently employs a neural vocoder to convert generated spectrograms back into waveforms. In contrast, IM2WAV directly operates on waveforms, partitioning the VQ-VAE’s latent space into two levels and utilizing dual Transformer models to model their respective distributions. Additionally, Diff-Foley [11] introduces a latent diffusion method conditioned on contrastive audio-visual pretraining (CAVP) representations. Inspired by the pioneering work of AudioGen [2] and MusicGen [4], we introduce FoleyGen, a video-to-audio generation framework that adopts a language modeling paradigm. An overview of FoleyGen is provided in Figure 1. Specifically, our system encompasses three major components: a neural audio codec-EnCodec [12] for bidirectional conversion between audio and discrete tokens, a visual encoder for extracting visual features, and a Transformer model responsible for generating audio tokens conditioned on the visual context. Unlike SpecVQGAN [9], the introduction of EnCodec provides better reconstruction quality and alleviates fidelity loss that often occurs during the spectrogram-to-waveform conversion process. Additionally, it eliminates the need for deploying multiple Transformer models IM2WAV [10]. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To enhance the temporal alignment between visible actions and corresponding audio events, we propose and explore three different visual attention mechanisms. Furthermore, we conduct an exhaustive evaluation of various visual encoders, pretrained on both single-modal and multi-modal tasks. The
--- EXPERIMENT ---
al results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. Index Terms— Sound generation, audio-visual learning, video-to-audio generation, multimodal learning 1. INTRODUCTION Recent years have seen remarkable breakthroughs in audio generation, powered predominantly by the evolution of large-scale deep learning models and datasets. Despite great achievements in text-to-audio [1, 2] and text-to-music [3, 4] generation, video-to-audio (V2A) generation lags behind, standing as a promising yet under-explored area due to its inherent challenges. Video-to-audio generation is the task of generating congruent soundscapes for a given visual signal, which requires parsing visual data, identifying sound-emitting objects, and then crafting corresponding sounds. V2A models are useful in various applications, such as generating sound for movies as a computational Foley artist, enhancing immersive experiences in virtual reality applications, and assisting visually impaired individuals for better spatial awareness. *Work done during an internship at Meta. Generated Waveform “et nit i Encodec Decoder * ™ > t€{ Atesee t Transformer Decoder ! Encodec Encoder % ' Reference Waveform Video Frames # : Pretrained & Frozen Fig. 1. Overview of the FoleyGen system. The dashed-line block shows the EnCodec encoder for converting waveforms into discrete tokens, utilized only during training. Achieving accurate and realistic V2A generation poses several challenges. First, the simultaneous interpretation of both visual and auditory data is intricate due to their respective high-dimensional natures. Second, real-world videos often contain visually irrelevant sounds where the objects emitting sound are absent from the visible frames. This discrepancy makes the generation of temporally synchronized audio extremely challenging. Finally, a single object can emit a diverse range of sounds depending on its interaction with varying environments, further complicating this task. Initial efforts in V2A generation has predominantly focused on constrained visual contexts and a limited set of sound classes to simplify the problem [5, 6, 7]. Such approaches commonly utilized class-aware strategies [6] or even trained separate models for distinct sound categories [7, 8]. Consequently, these methods fail to generalize to open-domain videos. Recent advancements, however, indicate a rising interest in open-domain, visually guided audio generation. SpecVQGAN [9] and IM2WAV [10] both employ a language modeling method, leveraging the Transformer model to cap --- --ture the joint distribution of visual features and discrete audio tokens encoded by vector-quantized variational autoencoder (VQ-VAE). In SpecVQGAN, the VQ-VAE operates specifically on spectrograms and subsequently employs a neural vocoder to convert generated spectrograms back into waveforms. In contrast, IM2WAV directly operates on waveforms, partitioning the VQ-VAE’s latent space into two levels and utilizing dual Transformer models to model their respective distributions. Additionally, Diff-Foley [11] introduces a latent diffusion method conditioned on contrastive audio-visual pretraining (CAVP) representations. Inspired by the pioneering work of AudioGen [2] and MusicGen [4], we introduce FoleyGen, a video-to-audio generation framework that adopts a language modeling paradigm. An overview of FoleyGen is provided in Figure 1. Specifically, our system encompasses three major components: a neural audio codec-EnCodec [12] for bidirectional conversion between audio and discrete tokens, a visual encoder for extracting visual features, and a Transformer model responsible for generating audio tokens conditioned on the visual context. Unlike SpecVQGAN [9], the introduction of EnCodec provides better reconstruction quality and alleviates fidelity loss that often occurs during the spectrogram-to-waveform conversion process. Additionally, it eliminates the need for deploying multiple Transformer models IM2WAV [10]. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To enhance the temporal alignment between visible actions and corresponding audio events, we propose and explore three different visual attention mechanisms. Furthermore, we conduct an exhaustive evaluation of various visual encoders, pretrained on both single-modal and multi-modal tasks. The experimental results show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. 2. PROPOSED METHOD Given a video clip, a video-to-audio generation system is designed to produce an audio clip that is both semantically consistent with and temporally aligned to the accompanying video content. The video-to-audio generation process can be formulated as H : v +> a, where v refers to the frames of a video input and a corresponds to the generated audio waveform. Figure | presents the architecture of FoleyGen, our proposed system. FoleyGen comprises three main components: a neural audio codec for the bidirectional conversion between waveforms and discrete tokens, a visual encoder for feature extraction from video frames, and an audio language decoder tasked with generating discrete audio tokens based on the extracted visual features. This section first provides a detailed introduction to each major component of FoleyGen. To improve the temporal alignment of the visual input and generated audio, we propose using different visual attention mechanisms, which are described at the end of this section. 2.1. Neural Audio Codec Modeling the distribution of time-domain waveforms presents significant challenges and computational inefficiencies, primarily due to their high-dimensional and lengthy characteristics. In audio generation systems, autoencoders are commonly utilized to encode audio waveforms into a latent space, which can be either continuous [1] or discrete [2]. Inspired by AudioLM [13] and AudioGen [2], we adopt EnCodec, a state-of-the-art neural audio codec [12], for our experiments. EnCodec comprises an encoder that compresses audio waveforms into latent vectors, a residual vector quantizer (RVQ) for converting these latent vectors into discrete tokens, and a symmetric decoder that reconverts these tokens back into audio waveforms. Given an audio clip a € R'*/s, where t is the duration and fs is the sampling rate, the encoder first compresses a into a latent representation z € R“**. Here, d is the dimensionality of the latent vector, and L is the number of down-sampled time steps. A RVQ with N, codebooks then transforms the encoded latent vectors into N, x L discrete tokens. The discrete audio tokens are further used as the representation of audio in the language modeling stage. The EnCodec decoder converts the generated audio tokens to waveforms. The EnCodec encoder is used only during training. We adhere to the same hyperparameter settings as outlined in the EnCodec paper, please refer to [12] for details. The adoption of EnCodec offers a high compression rate while keeping high reconstruction quality. Unlike other autoencoders that operate on spectrograms [9, 11], EnCodec eliminates the need for an additional vocoder and thus obviates the potential fidelity loss that may occur when converting a generated spectrogram back to a waveform. 2.2. Visual Encoder Given a visual input v € R?*C*#*W where T represents the number of frames (which can be | for a single image), C is the number of channels, and H and W denote the height and width of the visual input, respectively, the visual encoder generates feature vectors F € R?7* with D being the number of dimension of the language decoder. The quality of the extracted visual features F' is critical for achieving semantically consistent and temporally aligned audio generation. A suboptimal visual encoder may lead to loss of important visual cues, resulting in an audio output that lacks fidelity or congruency with the original video content. To explore the efficacy of different visual encoders, we conducted a series of experiments using a variety of popular visual encoders trained with uni-modal and multi-modal tasks. These visual encoders include ViT [14], CLIP [15], ImageBind [16] and VideoMAE [17]. --- --Allow to attend [By Prevent from attending v A v A v A All-Frame Visual Attention Causal Visual Attention Frame-Specific Visual Attention Fig. 2. Overview of the three visual attention mechanisms. For simplicity, here we assume we have 2 visual features ‘V’ and 4 audio tokens ‘A’ with a frame rate of 2 Hz. 2.3. Audio Language Decoder Audio is represented as discrete tokens after being encoded by EnCodec [12], therefore, the video-to-audio generation problem can be formulated as a conditional language modeling task. Given visual features extracted as conditional information, we employ a Transformer model [18] to generate discrete audio tokens autoregressively. The Transformer model is decoderonly and omits the cross-attention block. The visual features are prepended to the sequence of audio tokens for conditioning. Due to EnCodec’s residual vector quantization, each timestep encodes multi-stream tokens using residual codebooks. To effectively capture these multi-stream tokens, we adopt the delay pattern introduced in MusicGen [4]. This approach parallelly models multiple streams of audio tokens while maintains offsets between the streams. The incorporation of the delay pattern ensures high efficiency and eliminates the need for predicting tokens in a flattened pattern. Moreover, it sidesteps the requirement of multiple Transformer models [13, 10]. 2.4. Visual Attention Mechanism Generating audio that is temporally aligned with a video presents significant challenges. To address this, we introduce and explore three distinct visual attention mechanisms. Figure 2 shows the overview of the three attention mechanisms. All-Frame Visual Attention: In our baseline setting, we employ the default causal attention mechanism inherent in the Transformer decoder. Given that the visual features are prepended to the discrete tokens, during the generation process, the audio tokens have the capability to attend to all visual features. While this provides a broad context, it might confuse the model regarding the exact timing for sound generation due to an overabundance of visual information. Causal Visual Attention: As a countermeasure, we investigate a “causal” approach wherein, during the audio token generation, the model is restricted to attending only to visual frames that precede and align with the current timestep. This sequential attention might help the model to better synchronize the audio with the visual cues. Frame-Specific Visual Attention: In a more restrictive approach, we introduce“frame-specific visual attention”, where the model’s attention is confined strictly to visual features of the concurrent time frame during generation. This strict attention mechanism ensures that the model generates audio only based on the current visual context. 3. EXPERIMENTS 3.1. Dataset We target at open-domain visually guided audio generation. Therefore, we use the VGGSound [19] dataset, which contains around 200k 10-second video clips sourced from YouTube with diverse contents. Since some video clips are not downloadable anymore, our version contains 159318 samples in the train set and 13 161 samples in the test set. 3.2. Implementation Details All the audio clips in the dataset are sampled to 16k Hz monophonic audio. For the EnCodec, we follow the same downsampling strides [2, 4,5, 8] in the encoder, which leads to a frame rate of 50 Hz. We employ four codebooks with a codebook size of 2048. For video data, we sample one frame per second and follow the prepocessing protocols (e.g., resize, normalize) in the visual encoders. A linear layer is used after the visual encoder to project the visual features to the same dimension of the Transformer model. The Transformer decoder consists of 24 layers with 16 heads and a dimension of 1024. A memory efficient flash attention [20] is used to improve the speed and memory usage. The models are trained for 20k steps with a batch size of 256. AdamW optimizer with 3; = 0.9, 62 = 0.95, anda weight decay of 0.1 is used. The learning rate is set to 1 x 10-and warm up is used in the first 4k steps. In addition, classifierfree guidance [21] is also employed to achieve better visual adherence. During training, the visual condition is dropped (i.e., replaced with null vectors) with a probability of 0.1. During inference, the classifier-free guidance scale of 3.0 is used, and we employ top-k sampling with k setting to 256. 3.3. Evaluation Metrics To evaluate the performance of FoleyGen, we carry out both objective and subjective evaluations. For objective evaluation, we employ Fréchet Audio Distance (FAD) [22], KullbackLeibler Divergence (KLD), and ImageBind (IB) score [16]. FAD calculates the distribution distance between the features of generated and reference audio clips, where the features are calculated using VGGish network [23] trained on AudioSet. KLD compares the label distribution of target and generated audio calculated by a pretrained PaSST model [24]. FAD demonstrates a strong correlation with human perception regarding audio quality, whereas KLD primarily captures the --- --Table 1. Experimental results on VGGSound dataset. Here we use all-frame visual attention. Methods | Visual Encoder | FAD| KL{ 1B (%)t | OVR(%)t REL (%)t SpecVQGAN [9] | ResNet-50 6.64 3.10 - 5.6 5.IM2WAV [10] CLIP 641 2.54 - 16.7Ours | CLIP | 165 2.35 26.1 «| = 77.7 63.Table 2. Experimental results on VGGSound dataset with models trained using different visual encoders. Visual Encoder | FAD| KL { IB(%)t CLIP 1.65 2.35 26.ViT 1.75 2.50 23.ImageBind 1.66 2.34 26.VideoMAE 2.59 3.25 17.Table 3. Experimental results on VGGSound dataset with models trained using different attention mechanisms. The visual encoder used is CLIP. Attention | FAD| KL{ IB(%)t | OVR (%) t ALI (%) t All-frame 165 = 2.35 26.1 63.3 55.Causal 2.18 2.44 25.5 14.4 13.Frame-specific | 2.49 2.46 24.2 22.3 31.audio concepts present in the recording [2]. To evaluate the relevance between the generated audio and video, we propose using the ImageBind model [16] to compute a relevance score. Since ImageBind is trained to learn a joint embedding across six distinct modalities, the cosine similarity of its embeddings for both video and generated audio can capture semantic relevance between them. For subjective evaluation, human listeners are asked to compare samples generated by distinct models and identify the one that demonstrated superior performance based on specific criteria, which included overall quality (OVR), relevance (REL) to the corresponding visual input. Temporal alignment (ALI) is considered when evaluating the attention mechanisms. 3.4. Results Table 1 presents the primary results of our study, where we benchmark our proposed FoleyGen system against two previous state-of-the-art methods, SpecVQGAN [9] and IM2WAV [10]. Given that IM2WAV utilized FAD and KLD as evaluation metrics, we adopted their scores directly. For subjective evaluation, we generated samples using their pretrained models. It’s evident from the results that FoleyGen consistently surpasses both SpecVQGAN and IM2WAV in both objective and subjective metrics. Notably, there’s a marked reduction in the FAD score. The trends in subjective evaluations are congruent with the objective metrics. Several factors can be attributed to this improvement. First, the integration of EnCodec facilitates a heightened compression ratio of audio tokens and leads to a enhanced reconstruction quality. This elevated compression ratio simplifies the modeling of its distribution for the language model. Second, the utilization of the delay pattern in token generation avoids the need for multiple Transformer models, culminating in superior performance. Table 2 shows the results of our models when trained using various visual encoders. It can be observed that visual encoders that are pre-trained via multi-modal tasks, (i.e., CLIP [15] and ImageBind [16]), exhibit comparable performances and surpass those trained solely on uni-modal tasks. ViT, which has been pre-trained through a discriminative task, outperforms VideoMAE. Since VideoMAE is trained using masked autoencoder with self-supervised learning, fine-tuning might be required when adopt it for downstream tasks. Table 3 presents the results achieved using different attention mechanisms. All-frame visual attention notably surpassed the other two, both in objective metrics and human evaluations. Interestingly, while the frame-specific attention lagged in objective evaluations, it demonstrated an enhanced performance in human evaluation as compared with causal visual attention. However, a critical insight from human evaluations reveals that the systems still struggle with temporal alignment, and sometimes fail to capture prominent actions within the video. 4.
--- CONCLUSION ---
S In this paper, we introduced FoleyGen, a video-to-audio generation model following a language modeling paradigm. FoleyGen utilizes the EnCodec for bidirectional waveform-token conversion,a visual encoder for visual feature extraction and a Transformer decoder for conditioned audio token generation. Our evaluations demonstrate that FoleyGen surpasses prior methodologies in both objective metrics and human evaluations. Through our explorations, we observed that visual encoders trained on multimodal tasks exhibit superior performance. While we introduced visual attention mechanisms to enhance audio-video temporal alignment, it remains a persistent challenge in the domain. Future research should delve deeper into improving the temporal cohesion of video-to-audio generation systems. --- --(1)B (ll [5. REFERENCES Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley, “AudioLDM: Text-to-audio generation with latent diffusion models,” Proceedings of the International Conference on Machine Learn ing, 2023. Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi, “AudioGen: Textually guided audio generation,” in The Eleventh International Conference on Learning Representations, 2023. Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiugqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley, “AudioLDM 2: Learning holistic audio generation with self-supervised pretraining,” arXiv preprint arXiv:2308.05734, 2023. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez, “Simple and controllable music generation,’ arXiv preprint arXiv:2306.05284, 2023. Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, and William T. Freeman, “Visually indicated sounds,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. Kan Chen, Chuanxi Zhang, Chen Fang, Zhaowen Wang, Trung Bui, and Ram Nevatia, “Visually indicated sound generation by perceptually optimized classification,” in Proceedings of the European Conference on Computer Vision Workshops, 2018. Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L Berg, “Visual to sound: Generating natural sound for videos in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3550-3558. Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan, “Generating visually aligned sound from videos,” [EEE Transactions on Image Processing, vol. 29, pp. 8292-8302, 2020. Vladimir Iashin and Esa Rahtu, “Taming visually guided sound generation,” in British Machine Vision Conference (BMVC), 2021. Roy Sheffer and Yossi Adi, “I hear your true colors: Image guided audio generation,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). EEE, 2023, pp. 1-5. Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao, “DiffFoley: Synchronized video-to-audio synthesis with latent diffusion models,” arXiv preprint arXiv:2306.17203, 2023. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022. (13] [fa]Zalan Borsos, Raphaél Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al., “Audiolm: a language modeling approach to audio generation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, “An image is worth 16xwords: Transformers for image recognition at scale,” in Jnternational Conference on Learning Representations, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., “Learning transferable visual models from natural language supervision,” in Jnternational Conference on Machine Learning. PMLR, 2021, pp. 8748-8763. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra, “ImageBind: One embedding space to bind them all,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 15180-15190. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang, “VideoMAE: Masked autoencoders are data-efficient learners for selfsupervised video pre-training,” Advances in Neural Information Processing Systems, vol. 35, pp. 10078-10093, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems. 2017, vol. 30, Curran Associates, Inc. Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman, “VggSound: A large-scale audio-visual dataset,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 721-725. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré, “FlashAttention: Fast and memory-efficient exact attention with io-awareness,” Advances in Neural Information Processing Systems, vol. 35, pp. 16344-16359, 2022. Jonathan Ho and Tim Salimans, “Classifier-free diffusion guidance,” arXiv preprint arXiv:2207.12598, 2022. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi, “Fréchet audio distance: A metric for evaluating music enhancement algorithms,” arXiv preprint arXiv: 1812.08466, 2018. Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al., “CNN architectures for large-scale audio classification,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 131-135. Khaled Koutini, Jan Schliiter, Hamid Eghbal-zadeh, and Gerhard Widmer, “Efficient training of audio transformers with patchout,” in Interspeech. 2022, pp. 2753-2757, ISCA.
"	"--- ABSTRACT ---
오디오 생성의 최근 발전은 대규모 딥 러닝 모델과 광범위한 데이터 세트의 진화에 의해 촉진되었습니다. 그러나 비디오-오디오(V2A) 생성 작업은 주로 고차원 시각 및 청각 데이터 간의 복잡한 관계와 시간 동기화와 관련된 과제 때문에 여전히 어려운 과제입니다. 이 연구에서는 언어 모델링 패러다임을 기반으로 하는 오픈 도메인 V2A 생성 시스템인 FoleyGen을 소개합니다. FoleyGen은 파형과 이산 토큰 간의 양방향 변환을 위해 기성형 신경 오디오 코덱을 활용합니다. 오디오 토큰 생성은 시각적 인코더에서 추출한 시각적 특징에 따라 조건화된 단일 Transformer 모델에 의해 용이해집니다. V2A 생성에서 널리 퍼진 문제는 생성된 오디오와 비디오의 시각적 동작의 정렬이 맞지 않는 것입니다. 이를 해결하기 위해 세 가지 새로운 시각적 주의 메커니즘을 탐구합니다. 또한 단일 모달 또는 다중 모달 작업에 대해 각각 사전 학습된 여러 시각적 인코더에 대한 철저한 평가를 수행합니다. VGGSound 데이터 세트에 대한 실험 결과는 제안된 FoleyGen이 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수함을 보여줍니다. 색인 용어― 사운드 생성, 시청각 학습, 비디오-오디오 생성, 멀티모달 학습 1.
--- METHOD ---
s는 오픈 도메인 비디오로 일반화하는 데 실패합니다. 그러나 최근의 발전은 오픈 도메인, 시각적으로 안내되는 오디오 생성에 대한 관심이 증가하고 있음을 나타냅니다. SpecVQGAN [9]과 IM2WAV [10]는 모두 언어 모델링 방법을 사용하여 Transformer 모델을 활용하여 벡터 양자화 변형 자동 인코더(VQ-VAE)로 인코딩된 시각적 특징과 이산 오디오 토큰의 공동 분포를 캡처합니다. SpecVQGAN에서 VQ-VAE는 스펙트로그램에서 특별히 작동한 후 신경 보코더를 사용하여 생성된 스펙트로그램을 다시 파형으로 변환합니다. 반면 IM2WAV는 파형에서 직접 작동하여 VQ-VAE의 잠재 공간을 두 수준으로 분할하고 이중 Transformer 모델을 활용하여 해당 분포를 모델링합니다. 또한 Diff-Foley [11]는 대조적 오디오-비주얼 사전 학습(CAVP) 표현을 조건으로 하는 잠재 확산 방법을 도입합니다. AudioGen [2] 및 MusicGen [4]의 선구적인 작업에서 영감을 얻어 언어 모델링 패러다임을 채택한 비디오-오디오 생성 프레임워크인 FoleyGen을 소개합니다.FoleyGen의 개요는 그림 1에 나와 있습니다.특히, 우리 시스템은 세 가지 주요 구성 요소를 포함합니다.오디오와 이산 토큰 간 양방향 변환을 위한 신경 오디오 코덱-EnCodec [12], 시각적 특징을 추출하기 위한 시각적 인코더, 시각적 맥락에 따라 오디오 토큰을 생성하는 Transformer 모델입니다.SpecVQGAN [9]과 달리 EnCodec을 도입하면 더 나은 재구성 품질이 제공되고 스펙트로그램-파형 변환 프로세스 중에 종종 발생하는 충실도 손실이 완화됩니다.또한 여러 Transformer 모델 IM2WAV [10]를 배포할 필요가 없습니다.V2A 생성에서 흔한 문제는 생성된 오디오와 비디오의 눈에 보이는 동작의 정렬이 맞지 않는 것입니다.눈에 보이는 동작과 해당 오디오 이벤트 간의 시간적 정렬을 향상시키기 위해 세 가지 다른 시각적 주의 메커니즘을 제안하고 탐색합니다. 또한, 우리는 단일 모달 및 다중 모달 작업 모두에 대해 사전 학습된 다양한 시각 인코더에 대한 철저한 평가를 수행합니다.
--- EXPERIMENT ---
VGGSound 데이터 세트에 대한 모든 결과는 제안하는 FoleyGen이 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수함을 보여줍니다. 색인 용어― 사운드 생성, 시청각 학습, 비디오-오디오 생성, 멀티모달 학습 1. 서론 gen최근 몇 년 동안 대규모 딥 러닝 모델과 데이터 세트의 발전에 힘입어 오디오 분야에서 놀라운 혁신이 이루어졌습니다.텍스트-오디오[1, 2] 및 텍스트-음악[3, 4] 생성 분야에서 큰 성과를 거두었음에도 불구하고 비디오-오디오(V2A) 생성은 고유한 과제로 인해 유망하지만 탐구가 부족한 분야로 뒤처져 있습니다.비디오-오디오 생성은 주어진 시각 신호에 대해 일치하는 사운드스케이프를 생성하는 작업으로, 시각 데이터를 구문 분석하고 소리를 내는 물체를 식별한 다음 해당 소리를 만들어야 합니다. V2A 모델은 다양한 응용 분야에서 유용합니다.예를 들어, 컴퓨터 폴리 아티스트로서 영화의 사운드를 생성하거나, 가상 현실 응용 프로그램에서 몰입형 경험을 향상시키거나, 시각 장애인의 공간 인식을 개선하는 데 도움이 됩니다.* Meta에서 인턴십을 하는 동안 수행한 작업.생성된 파형 비디오 프레임 인코덱 디코더 비주얼 인코더 사전 학습 및 동결된 변압기 디코더 인코덱 인코더 참조 파형 그림 1. FoleyGen 시스템 개요.점선 블록은 파형을 개별 토큰으로 변환하는 EnCodec 인코더를 보여주며, 이는 학습 중에만 사용됩니다.정확하고 사실적인 V2A 생성을 달성하려면 몇 가지 과제가 있습니다.첫째, 시각 및 청각 데이터를 동시에 해석하는 것은 각각의 고차원적 특성으로 인해 복잡합니다.둘째, 실제 비디오에는 종종 시각적으로 무관한 사운드가 포함되어 있어 소리를 내는 객체가 보이는 프레임에 없습니다.이러한 불일치로 인해 시간적으로 동기화된 오디오를 생성하는 것이 매우 어렵습니다. 마지막으로, 단일 객체는 다양한 환경과의 상호 작용에 따라 다양한 범위의 소리를 낼 수 있어 이 작업이 더욱 복잡해집니다. V2A 생성의 초기 노력은 주로 제한된 시각적 맥락과 제한된 사운드 클래스 세트에 초점을 맞춰 문제를 단순화했습니다[5, 6, 7]. 이러한 접근 방식은 일반적으로 클래스 인식 전략[6]을 활용하거나 심지어 별개의 사운드 범주에 대해 별도의 모델을 학습했습니다[7, 8]. 결과적으로 이러한 방법은 오픈 도메인 비디오로 일반화하는 데 실패합니다. 그러나 최근의 발전은 오픈 도메인, 시각적으로 안내되는 오디오 생성에 대한 관심이 증가하고 있음을 나타냅니다. SpecVQGAN[9]과 IM2WAV[10]는 모두 언어 모델링 방법을 사용하여 Transformer 모델을 활용하여 벡터 양자화 변형 자동 인코더(VQ-VAE)로 인코딩된 시각적 특징과 이산 오디오 토큰의 공동 분포를 캡처합니다. SpecVQGAN에서 VQ-VAE는 스펙트로그램에서 특별히 작동한 다음 신경 보코더를 사용하여 생성된 스펙트로그램을 다시 파형으로 변환합니다. 이와 대조적으로 IM2WAV는 파형에서 직접 작동하여 VQ-VAE의 잠재 공간을 두 수준으로 분할하고 이중 Transformer 모델을 활용하여 각각의 분포를 모델링합니다.또한 Diff-Foley[11]는 대조적 오디오-비주얼 사전 학습(CAVP) 표현을 조건으로 하는 잠재 확산 방법을 도입합니다.AudioGen[2]과 MusicGen[4]의 선구적인 작업에서 영감을 받아 언어 모델링 패러다임을 채택한 비디오-오디오 생성 프레임워크인 FoleyGen을 소개합니다.FoleyGen에 대한 개요는 그림 1에 나와 있습니다.특히, 우리 시스템은 세 가지 주요 구성 요소를 포함합니다.오디오와 이산 토큰 간 양방향 변환을 위한 신경 오디오 코덱-EnCodec[12], 시각적 특징을 추출하기 위한 시각적 인코더, 시각적 맥락을 조건으로 오디오 토큰을 생성하는 Transformer 모델입니다. SpecVQGAN [9]과 달리 EnCodec을 도입하면 재구성 품질이 더 좋아지고 스펙트로그램-파형 변환 프로세스 중에 종종 발생하는 충실도 손실이 완화됩니다.또한 여러 Transformer 모델 IM2WAV [10]를 배포할 필요가 없습니다.V2A 생성에서 흔히 발생하는 문제는 생성된 오디오와 비디오의 시각적 동작이 일치하지 않는 것입니다.시각적 동작과 해당 오디오 이벤트 간의 시간적 일치를 향상시키기 위해 세 가지 다른 시각적 주의 메커니즘을 제안하고 탐색합니다.또한 단일 모달 및 다중 모달 작업 모두에서 사전 학습된 다양한 시각적 인코더에 대한 철저한 평가를 수행합니다.실험 결과에 따르면 제안된 FoleyGen은 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수합니다.2.1. 신경 오디오 코덱 시간 영역 파형의 분포를 모델링하는 것은 주로 고차원 및 긴 특성으로 인해 상당한 과제와 계산 비효율성을 나타냅니다. 오디오 생성 시스템에서 자동 인코더는 일반적으로 오디오 파형을 연속적[1]이거나 불연속적[2]일 수 있는 잠재 공간으로 인코딩하는 데 사용됩니다. AudioLM[13] 및 AudioGen[2]에서 영감을 받아 최첨단 신경 오디오 코덱[12]인 EnCodec을 실험에 채택했습니다. EnCodec은 오디오 파형을 잠재 벡터로 압축하는 인코더, 이러한 잠재 벡터를 불연속 토큰으로 변환하는 잔여 벡터 양자화기(RVQ), 이러한 토큰을 다시 오디오 파형으로 변환하는 대칭 디코더로 구성됩니다. 오디오 클립 a € Rt×fs가 주어지면(여기서 t는 지속 시간이고 f¸는 샘플링 속도), 인코더는 먼저 a를 잠재 표현 z = RL×d로 압축합니다. 여기서 d는 잠재 벡터의 차원이고 L은 다운 샘플링된 시간 단계의 수입니다. Ng 코드북이 있는 RVQ는 인코딩된 잠재 벡터를 N₁ × L 이산 토큰으로 변환합니다. 이산 오디오 토큰은 언어 모델링 단계에서 오디오의 표현으로 추가로 사용됩니다. EnCodec 디코더는 생성된 오디오 토큰을 파형으로 변환합니다. EnCodec 인코더는 학습 중에만 사용됩니다. EnCodec 논문에 설명된 것과 동일한 하이퍼파라미터 설정을 준수합니다. 자세한 내용은 [12]를 참조하십시오. EnCodec을 채택하면 높은 재구성 품질을 유지하면서도 높은 압축률을 제공합니다. 스펙트로그램에서 작동하는 다른 자동 인코더[9, 11]와 달리 EnCodec은 추가 보코더가 필요 없으므로 생성된 스펙트로그램을 다시 파형으로 변환할 때 발생할 수 있는 잠재적인 충실도 손실을 없앱니다. 2. 제안된 방법 비디오 클립이 주어지면 비디오-오디오 생성 시스템은 의미적으로 일관되고 시간적으로 수반되는 비디오 콘텐츠와 일치하는 오디오 클립을 생성하도록 설계되었습니다. 비디오-오디오 생성 프로세스는 H: va로 공식화할 수 있는데, 여기서 v는 비디오 입력의 프레임을 나타내고 a는 생성된 오디오 파형에 해당합니다. 그림 1은 제안하는 시스템인 FoleyGen의 아키텍처를 보여줍니다. FoleyGen은 세 가지 주요 구성 요소로 구성됩니다. 파형과 개별 토큰 간의 양방향 변환을 위한 신경 오디오 코덱, 비디오 프레임에서 피처 추출을 위한 시각적 인코더, 추출된 시각적 피처를 기반으로 개별 오디오 토큰을 생성하는 오디오 언어 디코더입니다. 이 섹션에서는 먼저 FoleyGen의 각 주요 구성 요소에 대한 자세한 소개를 제공합니다. 시각적 입력과 생성된 오디오의 시간적 정렬을 개선하기 위해 이 섹션의 마지막에 설명되어 있는 다양한 시각적 주의 메커니즘을 사용할 것을 제안합니다. 2.2. 시각적 인코더 주어진 시각적 입력 v € RT×C×H×W, 여기서 T는 프레임 수(단일 이미지의 경우 1일 수 있음)를 나타내고, C는 채널 수이며, H와 W는 각각 시각적 입력의 높이와 너비를 나타낼 때, 시각적 인코더는 언어 디코더의 차원 수인 D를 갖는 피처 벡터 FЄRTXD를 생성합니다. 추출된 시각적 피처 F의 품질은 의미적으로 일관되고 시간적으로 정렬된 오디오 생성을 달성하는 데 중요합니다. 최적이 아닌 시각적 인코더는 중요한 시각적 단서를 손실하여 원래 비디오 콘텐츠와의 충실도나 일치성이 부족한 오디오 출력을 초래할 수 있습니다. 다양한 시각적 인코더의 효능을 알아보기 위해 단일 모달 및 다중 모달 작업으로 훈련된 다양한 인기 있는 시각적 인코더를 사용하여 일련의 실험을 수행했습니다. 이러한 시각적 인코더에는 ViT[14], CLIP[15], ImageBind[16] 및 VideoMAE[17]가 있습니다. 참석 허용 참석 금지 프레임별 시각적 주의: 보다 제한적인 접근 방식에서 모델의 주의가 생성 중에 동시 시간 프레임의 시각적 특징에 엄격하게 국한되는 &quot;프레임별 시각적 주의&quot;를 도입합니다. 이 엄격한 주의 메커니즘은 모델이 현재 시각적 맥락에 따라서만 오디오를 생성하도록 합니다. 모든 프레임 시각적 주의 인과적 시각적 주의 프레임별 시각적 주의 그림 2. 세 가지 시각적 주의 메커니즘 개요. 단순화를 위해 여기서는 프레임 속도가 2Hz인 2개의 시각적 특징 &#39;V&#39;와 4개의 오디오 토큰 &#39;A&#39;가 있다고 가정합니다. 2.3. 오디오 언어 디코더 오디오는 EnCodec [12]으로 인코딩된 후 이산 토큰으로 표현되므로 비디오-오디오 생성 문제는 조건부 언어 모델링 작업으로 공식화할 수 있습니다. 조건부 정보로 추출된 시각적 특징이 주어지면 Transformer 모델 [18]을 사용하여 이산 오디오 토큰을 자기 회귀적으로 생성합니다. Transformer 모델은 디코더 전용이며 교차 주의 블록을 생략합니다. 시각적 특징은 컨디셔닝을 위해 오디오 토큰 시퀀스에 추가됩니다.EnCodec의 잔여 벡터 양자화로 인해 각 타임스텝은 잔여 코드북을 사용하여 다중 스트림 토큰을 인코딩합니다.이러한 다중 스트림 토큰을 효과적으로 캡처하기 위해 MusicGen [4]에서 도입한 지연 패턴을 채택합니다.이 접근 방식은 스트림 간 오프셋을 유지하면서 다중 오디오 토큰 스트림을 병렬로 모델링합니다.지연 패턴을 통합하면 높은 효율성이 보장되고 평평한 패턴에서 토큰을 예측할 필요가 없습니다.또한 다중 Transformer 모델 [13, 10]의 요구 사항을 회피합니다.2.4. 시각적 주의 메커니즘 비디오와 시간적으로 일치하는 오디오를 생성하는 것은 상당한 과제를 제시합니다.이를 해결하기 위해 세 가지 고유한 시각적 주의 메커니즘을 소개하고 탐색합니다.그림 2는 세 가지 주의 메커니즘의 개요를 보여줍니다.모든 프레임 시각적 주의: 기준 설정에서 Transformer 디코더에 내재된 기본 인과적 주의 메커니즘을 사용합니다. 시각적 특징이 이산 토큰에 미리 추가되어 있기 때문에 생성 과정에서 오디오 토큰은 모든 시각적 특징에 주의를 기울일 수 있습니다. 이는 광범위한 맥락을 제공하지만 시각적 정보가 너무 많아서 사운드 생성의 정확한 타이밍과 관련하여 모델을 혼란스럽게 할 수 있습니다. 인과적 시각적 주의: 대책으로, 오디오 토큰 생성 중에 모델이 현재 타임스텝에 선행하고 일치하는 시각적 프레임에만 주의를 기울이도록 제한되는 &quot;인과적&quot; 접근 방식을 조사합니다. 이 순차적 주의는 모델이 오디오를 시각적 단서와 더 잘 동기화하는 데 도움이 될 수 있습니다. 3.1. 데이터 세트 3. 실험 우리는 오픈 도메인 시각적으로 안내되는 오디오 생성을 목표로 합니다. 따라서 다양한 콘텐츠가 있는 YouTube에서 가져온 약 200,000개의 10초 비디오 클립이 포함된 VGGSound [19] 데이터 세트를 사용합니다. 일부 비디오 클립은 더 이상 다운로드할 수 없으므로 우리 버전은 훈련 세트에 159,318개의 샘플과 테스트 세트에 13,161개의 샘플을 포함합니다. 3.2. 구현 세부 정보 데이터 세트의 모든 오디오 클립은 16kHz 모노포닉 오디오로 샘플링됩니다. EnCodec의 경우 인코더에서 동일한 다운샘플링 스트라이드 [2, 4, 5, 8]를 따르며, 프레임 속도가 50Hz입니다. 코드북 크기가 2048인 코드북 4개를 사용합니다. 비디오 데이터의 경우 초당 1프레임을 샘플링하고 시각적 인코더에서 사전 처리 프로토콜(예: 크기 조정, 정규화)을 따릅니다. 시각적 인코더 다음에 선형 레이어를 사용하여 시각적 특징을 Transformer 모델의 동일한 차원으로 투영합니다. Transformer 디코더는 16개 헤드와 1024 차원이 있는 24개 레이어로 구성됩니다. 메모리 효율적인 플래시 어텐션[20]을 사용하여 속도와 메모리 사용량을 개선합니다. = 모델은 배치 크기가 256인 20k 단계로 학습됩니다. ẞ1 0.9, 62 = 0.95, 가중치 감소가 0.1인 AdamW 옵티마이저를 사용합니다. 학습률은 1 × 10−로 설정되고 처음 4k 단계에서 워밍업을 사용합니다. 또한 분류기 없는 안내[21]도 사용하여 시각적 고착을 개선합니다. 훈련하는 동안 시각적 조건은 확률 0.1로 삭제됩니다(즉, 널 벡터로 대체). 추론하는 동안 분류기 없는 안내 척도 3.0이 사용되고 k를 256으로 설정한 상위 k 샘플링을 사용합니다. 3.3 평가 지표 FoleyGen의 성능을 평가하기 위해 객관적 평가와 주관적 평가를 모두 수행합니다. 객관적 평가를 위해 Fréchet Audio Distance(FAD)[22], KullbackLeibler Divergence(KLD) 및 ImageBind(IB) 점수[16]를 사용합니다. FAD는 생성된 오디오 클립과 참조 오디오 클립의 특징 간 분포 거리를 계산합니다. 여기서 특징은 AudioSet에서 훈련된 VGGish 네트워크[23]를 사용하여 계산됩니다. KLD는 사전 훈련된 PaSST 모델[24]에 의해 계산된 대상 오디오와 생성된 오디오의 레이블 분포를 비교합니다. FAD는 오디오 품질에 대한 인간의 지각과 강력한 상관 관계를 보이는 반면 KLD는 주로 표 1. VGGSound 데이터 세트의 실험 결과. 여기에서는 모든 프레임 시각적 주의를 사용합니다. 방법 시각적 인코더 FAD↓ KL↓ IB (%) ↑ OVR (%) ↑ REL (%) ↑ SpecVQGAN [9] ResNet-6.3.5.5.IM2WAV [10] CLIP 6.2.16.31.Ours CLIP 1.2.26.77.63.표 2. 다양한 시각적 인코더를 사용하여 학습한 모델을 사용한 VGGSound 데이터 세트에 대한 실험 결과. IB(%) ↑ 시각적 인코더 FAD KL↓ CLIP 1.2.26.ᏙᎥᎢ 1.2.23.ImageBind 1.2.26.VideoMAE 2.3.17.표 3. 다양한 주의 메커니즘을 사용하여 학습한 모델을 사용한 VGGSound 데이터 세트에 대한 실험 결과. 사용된 시각적 인코더는 CLIP입니다. ALI (%) ↑ 주의 FAD↓↓ KL↓ 모든 프레임 1.65 2. 인과적 2.18 2. IB(%) ↑ OVR (%) ↑ 26.63.55.25.24.14.13.22.31. 프레임별 2.49 2. 녹음에 존재하는 오디오 개념 [2]. 생성된 오디오와 비디오 간의 관련성을 평가하기 위해 ImageBind 모델 [16]을 사용하여 관련성 점수를 계산하는 것을 제안합니다. ImageBind는 6가지 서로 다른 모달리티에 걸쳐 공동 임베딩을 학습하도록 훈련되었으므로 비디오와 생성된 오디오 모두에 대한 임베딩의 코사인 유사성은 이들 간의 의미적 관련성을 포착할 수 있습니다. 주관적인 평가를 위해 인간 청취자에게 서로 다른 모델에서 생성한 샘플을 비교하고 전반적인 품질(OVR), 해당 시각적 입력과의 관련성(REL)을 포함한 특정 기준에 따라 우수한 성능을 입증한 샘플을 식별하도록 요청합니다. 시간적 정렬(ALI)은 주의 메커니즘을 평가할 때 고려됩니다.3.4. 결과 표 1은 우리 연구의 주요 결과를 제시하며, 여기서 우리는 제안된 FoleyGen 시스템을 이전의 두 가지 최첨단 방법인 SpecVQGAN [9] 및 IM2WAV [10]와 벤치마킹했습니다.IM2WAV가 평가 지표로 FAD와 KLD를 사용했기 때문에 우리는 그들의 점수를 직접 채택했습니다.주관적인 평가의 경우, 우리는 사전 학습된 모델을 사용하여 샘플을 생성했습니다.결과를 통해 FoleyGen이 객관적 및 주관적 지표에서 SpecVQGAN과 IM2WAV를 모두 지속적으로 능가한다는 것이 분명합니다.특히 FAD 점수가 현저히 감소했습니다.주관적 평가의 추세는 객관적 지표와 일치합니다.여러 요인이 이러한 개선에 기인할 수 있습니다.첫째, EnCodec을 통합하면 오디오 토큰의 압축률이 높아지고 재구성 품질이 향상됩니다.이 향상된 압축률은 언어 모델에 대한 분포 모델링을 간소화합니다. 둘째, 토큰 생성에서 지연 패턴을 활용하면 여러 Transformer 모델이 필요 없어져 더 뛰어난 성능을 얻을 수 있습니다.표 2는 다양한 시각적 인코더를 사용하여 학습했을 때의 모델 결과를 보여줍니다.다중 모달 작업을 통해 사전 학습된 시각적 인코더(예: CLIP[15] 및 ImageBind[16])는 비슷한 성능을 보이고 단일 모달 작업에서만 학습한 인코더를 능가하는 것을 관찰할 수 있습니다.차별적 작업을 통해 사전 학습된 ViT는 VideoMAE보다 성능이 뛰어납니다.VideoMAE는 자기 감독 학습이 있는 마스크 자동 인코더를 사용하여 학습되므로 다운스트림 작업에 채택할 때 미세 조정이 필요할 수 있습니다.표 3은 다양한 어텐션 메커니즘을 사용하여 달성한 결과를 보여줍니다.모든 프레임 시각적 어텐션은 객관적 지표와 인간 평가 모두에서 다른 두 가지를 현저히 능가했습니다.흥미롭게도 프레임별 어텐션은 객관적 평가에서 뒤처졌지만 인과적 시각적 어텐션과 비교하여 인간 평가에서 향상된 성능을 보였습니다. 그러나 인간의 평가에서 얻은 중요한 통찰력에 따르면 시스템은 여전히 시간적 정렬에 어려움을 겪고 있으며 때로는 비디오 내에서 중요한 동작을 포착하지 못하는 것으로 나타났습니다.
--- CONCLUSION ---
S 이 논문에서 우리는 언어 모델링 패러다임을 따르는 비디오-오디오 생성 모델인 FoleyGen을 소개했습니다. FoleyGen은 양방향 파형-토큰 변환을 위한 EnCodec, 시각적 특징 추출을 위한 시각적 인코더, 조건부 오디오 토큰 생성을 위한 Transformer 디코더를 활용합니다. 우리의 평가는 FoleyGen이 객관적 지표와 인간 평가 모두에서 이전 방법론을 능가한다는 것을 보여줍니다. 우리의 탐구를 통해 멀티모달 작업에서 훈련된 시각적 인코더가 더 뛰어난 성능을 보인다는 것을 관찰했습니다. 오디오-비디오 시간적 정렬을 향상시키기 위해 시각적 주의 메커니즘을 도입했지만, 그것은 여전히 이 분야에서 지속적인 과제로 남아 있습니다. 향후 연구는 비디오-오디오 생성 시스템의 시간적 응집력을 개선하는 데 더 깊이 파고들어야 합니다. 5. 참고문헌 [1] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D Plumbley, &quot;AudiOLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, 국제기계학습회의록, 2023. [2] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성&quot;, 제11회 학습 표현 국제회의, 2023. [3] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, 및 Mark D. Plumbley, &quot;AudioLDM 2: 자기 감독 사전 학습을 통한 전체적인 오디오 생성 학습,&quot; arXiv 사전 인쇄본 arXiv:2308.05734, 2023. [4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv 사전 인쇄본 arXiv:2306.05284, 2023. [5] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman, &quot;시각적으로 표시된 소리,&quot; IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2016년 6월. [6] Kan Chen, Chuanxi Zhang, Chen Fang, Zhaowen Wang, Trung Bui 및 Ram Nevatia, &quot;지각적으로 최적화된 분류를 통한 시각적으로 표시된 사운드 생성&quot;, 유럽 컴퓨터 비전 워크숍 회의록, 2018. [7] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui 및 Tamara L Berg, &quot;시각적에서 사운드로: 야외에서 비디오를 위한 자연스러운 사운드 생성&quot;, IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 2018, 3550-3558쪽. [8] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang 및 Chuang Gan, &quot;비디오에서 시각적으로 정렬된 사운드 생성&quot;, IEEE 이미지 처리 저널, 제29권, 8292-8302쪽, 2020년. [9] Vladimir Iashin 및 Esa Rahtu, &quot;시각적으로 유도되는 사운드 생성 길들이기&quot;, 영국 머신 비전 컨퍼런스(BMVC), 2021년. [10] Roy Sheffer 및 Yossi Adi, &quot;당신의 진정한 색깔을 듣습니다: 이미지 유도 오디오 생성&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2023, 1-5쪽. [11] Simian Luo, Chuanhao Yan, Chenxu Hu 및 Hang Zhao, &quot;DiffFoley: 잠재 확산 모델을 사용한 동기화된 비디오-오디오 합성,&quot; arXiv 사전 인쇄본 arXiv:2306.17203, 2023. [12] Alexandre Défossez, Jade Copet, Gabriel Synnaeve 및 Yossi Adi, &quot;고충실도 신경 오디오 압축,&quot; arXiv 사전 인쇄본 arXiv:2210.13438, 2022. [13] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi 등, &quot;Audiolm: 오디오 생성을 위한 언어 모델링 접근 방식,&quot; IEEE/ACM Transactions on 오디오, 음성 및 언어 처리, 2023. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, &quot;이미지는 16xwords의 가치가 있습니다: 대규모 이미지 인식을 위한 변압기&quot;, 국제 학습 표현 컨퍼런스, 2021. [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, 국제 기계 학습 컨퍼런스. PMLR, 2021, 8748-8763쪽. [16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin 및 Ishan Misra, &quot;ImageBind: 모든 것을 묶을 수 있는 하나의 임베딩 공간&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2023, pp. 15180-15190. [17] Zhan Tong, Yibing Song, Jue Wang 및 Limin Wang, &quot;VideoMAE: 마스크 자동 인코더는 자기 감독 비디오 사전 학습을 위한 데이터 효율적인 학습기입니다&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 10078-10093, 2022. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin, &quot;주의만 있으면 됩니다.&quot;, 신경 정보 처리 시스템의 발전. 2017, vol. 30, Curran Associates, Inc. [19] Honglie Chen, Weidi Xie, Andrea Vedaldi 및 Andrew Zisserman, &quot;VggSound: 대규모 시청각 데이터 세트&quot;, IEEE 국제 음향, 음성 및 신호 처리 회의(ICASSP), 2020, pp. 721–725 [20] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 및 크리스토퍼 레(Christopher Ré), &quot;FlashAttention: 영어: io 인식을 통한 빠르고 메모리 효율적인 정확한 주의,&quot; 신경 정보 처리 시스템의 발전, 제35권, 16344-16359쪽, 2022. [21] Jonathan Ho 및 Tim Salimans, &quot;분류자 없는 확산 안내,&quot; arXiv 사전 인쇄본 arXiv:2207.12598, 2022. [22] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek 및 Matthew Sharifi, &quot;Fréchet 오디오 거리: 음악 향상 알고리즘을 평가하기 위한 지표,&quot; arXiv 사전 인쇄본 arXiv:1812.08466, 2018. [23] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold 외, &quot;대규모 오디오 분류를 위한 CNN 아키텍처&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2017, pp. 131-135. [24] Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh 및 Gerhard Widmer, &quot;패치아웃을 사용한 오디오 변압기의 효율적 학습&quot;, Interspeech. 2022, pp. 2753-2757, ISCA.
"
"--- ABSTRACT ---
In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoderonly fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on more than twenty five times larger speech dataset. Overall, we demonstrate that by only adding a handful number of trainable parameters via adapters, we can unlock contextualized speech recognition capability for the pretrained LLM while keeping the same text-only input functionality. Index Terms— contextual biasing, large language models, speech recognition 1.
--- INTRODUCTION ---
In recent years, there has been growing interest in Large Language Models (LLMs) due to their remarkable efficacy in enhancing performance in tasks like question answering and summarization, surpassing specialized models [1] [2]. LLMs are trained on vast quantities of text data, thereby encapsulating a wealth of world knowledge within the network. This accumulated knowledge and contextual understanding prove to be particularly beneficial in the field of Automatic Speech Recognition (ASR), especially when additional context surrounding an utterance is available beyond the audio alone. For example, video titles and descriptions can provide insights into the topic of the video or offer clues about named entities that might be mentioned [3] [4]. Such contextual in Work done during internship at Meta AI. LLM decoder <bos> Audio Contextual text tokens tokens (video title, description) J Audio Recognized encoder spoken text Fig. 1. A speech recognition model with mixed-modal context consisting of audio and optional text tokens based on a pretrained LLM backbone. Speech encoder and LLM decoder are both initially pretrained. The LLM weights are frozen (orange blocks), while audio encoder and LoRa adapters are fine-tuned during training (blue blocks). formation can assist in disambiguating challenging pronunciations, as certain words, domain-specific terms, or named entities can often be inferred from context alone. Traditional approaches to ASR contextualization [5)[6] operate at the token or phrase level, employing techniques like biasing with weighted finite state transducers (WFSTs) or using specialized attention networks. These are typically either incorporated during the decoding stage or trained as separate components. Consequently, contextualization significantly improves the ASR system’s ability to recognize named entities or specialized in-domain terms. However, there are some limitations to these approaches: - The biasing is limited towards individual phrases or words, as opposed to contextualizing based on external information as a whole (for example, topic-based biasing). - The biasing strength is usually controlled via a hyperparameter or requires specialized architectural changes and training procedures to ensure the system is not overbiased. - Some of the contextualization methods influence only the decoder state without interacting with the encoder directly. In this work, we propose a Speech LLaMA - a decoderonly architecture inspired by recent developments in LLMs tailored towards speech recognition. It is trained to use the contextual information end-to-end without any additional hy --- --perparameters. Specifically, 1) we prepend the whole available textual context as a prompt to an ASR system along with audio tokens. The Speech LLaMA hence have the full flexibility to look back and cross-corellate the contextual text tokens and the acoustic tokens when decoding the next spoken word. And 2) we employ the publicly available 7B LLaMA LLM [1] as a pretrained decoder for the Speech LLaMA. This simplifies the overall design of a contextual ASR as speech recognition can be considered as mixed-modal language model with next-token prediciton. Our intuition behind this is the pretrained LLMs already distill the linguistic information which should be particularly useful when reasoning which part of the context is relevant given the utterance. Our results on a competitive benchmark suggest a feasibility of this modelling approach. 2.
--- RELATED WORK ---
There have been several works on speech recognition models contextualization including deep and shallow biasing [8] [4]. Le et al. introduced a weighted finite state transducer (WEST) composed from biasing strings which is attached dynamically during decoding and the scores of the RNN-T system and biasing WFST are interpolated. The advantage of such approaches is that they could be attached to any system after the training is completed. Another line of research is deep biasing
--- METHOD ---
for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoderonly fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on more than twenty five times larger speech dataset. Overall, we demonstrate that by only adding a handful number of trainable parameters via adapters, we can unlock contextualized speech recognition capability for the pretrained LLM while keeping the same text-only input functionality. Index Terms— contextual biasing, large language models, speech recognition 1. INTRODUCTION In recent years, there has been growing interest in Large Language Models (LLMs) due to their remarkable efficacy in enhancing performance in tasks like question answering and summarization, surpassing specialized models [1] [2]. LLMs are trained on vast quantities of text data, thereby encapsulating a wealth of world knowledge within the network. This accumulated knowledge and contextual understanding prove to be particularly beneficial in the field of Automatic Speech Recognition (ASR), especially when additional context surrounding an utterance is available beyond the audio alone. For example, video titles and descriptions can provide insights into the topic of the video or offer clues about named entities that might be mentioned [3] [4]. Such contextual in Work done during internship at Meta AI. LLM decoder <bos> Audio Contextual text tokens tokens (video title, description) J Audio Recognized encoder spoken text Fig. 1. A speech recognition model with mixed-modal context consisting of audio and optional text tokens based on a pretrained LLM backbone. Speech encoder and LLM decoder are both initially pretrained. The LLM weights are frozen (orange blocks), while audio encoder and LoRa adapters are fine-tuned during training (blue blocks). formation can assist in disambiguating challenging pronunciations, as certain words, domain-specific terms, or named entities can often be inferred from context alone. Traditional approaches to ASR contextualization [5)[6] operate at the token or phrase level, employing techniques like biasing with weighted finite state transducers (WFSTs) or using specialized attention networks. These are typically either incorporated during the decoding stage or trained as separate components. Consequently, contextualization significantly improves the ASR system’s ability to recognize named entities or specialized in-domain terms. However, there are some limitations to these approaches: - The biasing is limited towards individual phrases or words, as opposed to contextualizing based on external information as a whole (for example, topic-based biasing). - The biasing strength is usually controlled via a hyperparameter or requires specialized architectural changes and training procedures to ensure the system is not overbiased. - Some of the contextualization methods influence only the decoder state without interacting with the encoder directly. In this work, we propose a Speech LLaMA - a decoderonly architecture inspired by recent developments in LLMs tailored towards speech recognition. It is trained to use the contextual information end-to-end without any additional hy --- --perparameters. Specifically, 1) we prepend the whole available textual context as a prompt to an ASR system along with audio tokens. The Speech LLaMA hence have the full flexibility to look back and cross-corellate the contextual text tokens and the acoustic tokens when decoding the next spoken word. And 2) we employ the publicly available 7B LLaMA LLM [1] as a pretrained decoder for the Speech LLaMA. This simplifies the overall design of a contextual ASR as speech recognition can be considered as mixed-modal language model with next-token prediciton. Our intuition behind this is the pretrained LLMs already distill the linguistic information which should be particularly useful when reasoning which part of the context is relevant given the utterance. Our results on a competitive benchmark suggest a feasibility of this modelling approach. 2. RELATED WORK There have been several works on speech recognition models contextualization including deep and shallow biasing [8] [4]. Le et al. introduced a weighted finite state transducer (WEST) composed from biasing strings which is attached dynamically during decoding and the scores of the RNN-T system and biasing WFST are interpolated. The advantage of such approaches is that they could be attached to any system after the training is completed. Another line of research is deep biasing methods that incorporate contextualization endto-end during the model training [9}|3} (6) [10) (11) [5]. A common limitation of these approaches is that the bias on the phrase level, rather than providing on the full context available. In addition, these approaches require a specialized biasing modules added to the main ASR architecture. In parallel to this reseach several approaches were presented incorporating LLMs for speech related tasks. Wu at al. incorporated LLaMA LLM for speech translation by concatenating a textual prompt (Translate audio to language X”) with audio representations. AudioPalm model was proposed mixing audio and text tokens for speech-to-text and speech to speech tasks. Fathullah et al. presented results on enabling speech recognition capabilities for LLaMA model on the multi-lingual data. Recently a Whisper model incorporated a biasing approach, where the previous segment’s transcription was added to the prompt for the longform speech recognition. In difference to their work, we bias the system on the unstructed and sometimes unrelated textual context as not always video title and description match the context of speech. 3.
--- EXPERIMENT ---
AL SETUP Model: Figure [i] illustrates the overview of our proposed model. This speech LLM architecture consists of two main blocks: audio encoder and text decoder. The audio encoder firstly applies four downsampling blocks resuling in 16x time reduction of audio representations. After that a stack of Conformer [16] blocks with rotary positional embeddings are applied with hidden dimensionality of 512 and kernel size of 9. At the end we add an additional downsampling block. As a result the decoder observes audio tokens sampled every 320ms with dimensionality of size 4,096. We pretrained the audio encoder with Connectionist Temporal Classification [18] criterion for 300k training steps on the same training data. We used a pretrained 7B LLaMA (v1) as a decoder. To adapt text-only LLaMA to speech recognition task, we have added Low-Rank Adapters to query, key, value, and output projection matrices in the self-attention layer of every decoder layer while keeping the rest of LLM parameters frozen throughout the training. We used the following LoRa parameters: rank of size 32, dropout rate of 5%, and 0.05 scaling parameter. Overall LoRa parameters add 30 million trainable parameters to the LLM decoder and the rest 6.billion are kept frozen. We used 80 dimensional log Mel features computed every 10ms with a window of 25ms. SpecAugment with two frequency masks of width 27 and ten time masks with maximum width of 4% of the length of an utterance. We trained our models for 200,000 updates with mixed precision, linearly increasing the learning rate to 5e-4 in the first 20,updates and exponentially decaying to le-5 over the remaining updates. We use Adam with parameters 61 = 0.9, 62 = 0.98, weight decay = le-5 and clip the gradient norm to 1. Our model is trained with 128 A100 GPUs for 3 days using Fairseq library (21]. Data: The models are trained on an in-house dataset that was de-identified with no personally identifiable information (PII) derived from public Facebook and Instagram videos. The data was further augmented with two distortion methods: speed perturbation and randomly sampled additive background noise. For evaluation, we have sampled 3,200 videos comprising around 34 hours of speech that have context of at least 100 characters length with at least one non-frequent word from the context occurs in the transcription. Metrics: To evaluate our models, we report both the overall Word Error Rate (WER) and Rare WER, which considers only rare words. A word is considered rare if it does not occur in the 90% percentile of the most frequent words estimated on training data. Textual context: Similar to Xiao et al. we incorporate video title and video description as an external context. We perform basic text post-processing like unicode character normalization and removal of all non-ascii symbols. Overall approximately 25% of videos from supervised video dataset have non-empty text context. When video title or description are present, we first concatenate and then tokenize them with the LLaMA tokenizer. After that, we prepend the <bos> token with the textual tokens. When both video title and descriptions are missing, the input corresponds to a traditional ASR setup without contextual information. The cross-entropy --- --Table 1. Evaluation results of Speech LLaMA compared to large-scale RNN-T baseline on English speech data. We report overall WER and Rare WER. Rare WER specifically focuses on the accuracy of recognizing rare words in the dataset. Model Speech Trainable Context presence WER (%) SUB INS DEL Rare WER (%) data (h) params (M) Training Evaluation 1B RNN-T 4M 1000 - - 12.34 6.53 3.21 2.60 30.1B RNN-T 4M 1000 - v 12.13 6.23 3.05 2.85 28.Speech LLaMa 150k 130 - - 11.70 6.09 3.20 2.38 27.Speech LLaMa 150k 130 v - 11.98 6.28 3.07 2.63 28.Speech LLaMa 150k 130 v v 11.22 5.76 3.14 2.32 23.loss is masked for the contextual tokens and only computed for spoken tokens. In these experiments we limit the textual content to a maximum of 50 tokens for computational reasons. If the context is longer than the threshold, we perform arandom crop of size 50 during training and crop the leading tokens during inference. Baseline: As a baseline we used a transformer based RNN-T system with one billion parameters [7], which is trained on four million hours of supervised and semi-supervised speech data. The RNN-T system architecture consists oftransformer layers in the encoder and 3 LSTM layers in the decoder. For contextualization it uses an WFST biasing method with neural language modelling shallow fusion [4], where the biasing FST is composed from video title and description. We are using exactly the same contextual information during decoding for the RNN-T baseline and our Speech LLaMA. 4. RESULTS Table [I] presents a summary of our decoding results on the evaluation set. We compare the Speech LLaMA against the offline RNN-T 1b model, considering two scenarios: with and without presenting contextualization information during decoding. The WER scores obtained for these scenarios using RNN-T are 12.34% and 12.13% respectively. Contextual biasing resuts in a relative WER reduction of approximately 1.7%. Even without the use of contextual information during training and evaluation, Speech LLaMA achieves a WER of 11.70%, a relative reduction of 5.2% over the RNN-T system trained on much more data. By incorporating context during training and evaluation, we achieve a significant improvement reaching an overall WER of 11.22% and resulting in 17% relative improvement in Rare WER, surpassing the performance of the RNN-T model with contextualization. It is worth noting that when we evaluate the Speech LLaMA trained with context but do not provide the context during inference, we obtain a WER of 11.98%. This corresponds to a slight WER gap compared to the model trained without context. We leave to address this minor performance difference to the future work, where adding a certain jitter to the context may improve the generalization of a model towards presence of the context. 4.1. Ablation studies 4.1.1. Context sensitivity To better understand how the model learns to use the context, we studied how receptive the model is to context perturbations. For this we tried a few ways to modify the prompt and measure its effect on the decoding. Specifically, we experimented with: 1. Replacing the actual context with words randomly sampled from the training data. 2. Replacing the context with the ground truth words. We filter out frequent words in this experiment as we assume that the model should not have significant issues in transcribing them. We expect a significant reduction of WER if the model is capable of copy-pasting the words from the context. 3. Replacing the contextual words with phonetical respellings of the words that appear in the transcripts. Our intuition is that such replacements are particularly challenging for the model and we should expect a bigger WER change compared to random substitutions. To generate re-spellings we employed a G2G model. For every rare word in the ground truth we sample an alternative spelling from the G2G model and add it to the context. For example, if the word ball is present in the context and ground truth we replace it by baw! and use that as context instead of the original token. 4. In addition to the previous perturbation we probe appending a similar sounding word to the context (e.g. both tokens ball and baw! will be present in the context). This tests the ability of an ASR system to disambiguate the actual spoken word given a competitive word in context. --- --Table 2. WER under different context perturbations during decoding stage. Context noise WER (%) Rare WER (%) (Original context) 11.22 23.(Remove all context) 11.98 28.Random 12.07 28.Respellings 11.89 28.Respellings (append) 11.46 25.Ground Truth 10.50 19.We present our results in Table [2] We note that replacing the whole context with random words sampled from the training data results in only a marginal difference in WER compared to removing all external context (11.98% vs. 12.07%). This indicates that the model is robust against some contextual noise and can distinguish relevant from irrelevant context. Substituting rare words that match both the context and the ground truth with G2G respellings results in a significant drop in WER (11.22% — 11.89%), almost matching with not using any context. This hints that the majority of gains observed are due to the model being able to copy certain words from the context. In contrast, when we instead of replacing the matching contextual word rather append a competing similarsounding word, we observe a smaller WER drop (11.22% — 11.46%). This indicates that the model does not necessarily get confused by similarly pronounced words with different meanings. Furthermore, when we take the rare words from the ground truth into the context, the WER improves to 10.50% (6% relative change) and Rare WER improves by 18% relative. This further proves the ability of the model to utilize contextual information when present in order to better recognize rare entities. Table 3. Impact of the context masking structure on the WER. Masking WER (%) Causal 11.Full-Mask 11.4.1.2. Causal vs Full Masking Traditionally causal masking is used in all self-attention layers for decoder-only language models to prevent future information leakage. However for offline speech recognition we have full audio and text context observed at the time of decoding and only transcription tokens are necessary to be masked causally. In this section we experiment the impact of applying causal masking on all input tokens and contrast it with applying full mask on the text and audio context followed by causal masking on transcription tokens. While the audio representations are fully contextualized already, we hypothesize Table 4. Performance comparison of decoder-only Speech LLM and cross-attention Speech LLM. Decoder WER (%) Decoder-only 11.Encoder-decoder 11.that textual tokens may benefit from full masking. We present our results in Table 3} The full-mask shows only marginally better WER then causal masking (improving from 11.22% — to 11.15%). This comes at a cost as efficient self-attention implementations are currently tailored towards causal masking (Flash-Attention v2) and using a custom masking slows down training by 10%. 4.1.3. Decoder-only vs Cross-attention Furthermore, we compared the decoder-only approach to a traditional encoder-decoder model by converting the Speech LLM architecture to Listen-Attend-Spell architecture 24]. To achieve that, instead of concatenating audio and text tokens we treated them separaterely. We added trainable cross-attention matrices to every LLM decoder layer. Table 3 presents the results of this study. We observed that the two approaches perform similarly, with only minor improvement for the Encoder-Decoder architecture (11.22% — 11.18%). This indicates that the decoder-only approach is a viable and straightforward method for performing ASR with or without contextualization. However, one limitation of the decoder-only approach is the quadratic attention complexity, which can impose restrictions on the overall sequence length. This limitation becomes significant as the context grows. To address this issue, we can employ techniques such as lower precision training (8 orbits) and linear attention approximation methods [25] 26]. 5.
--- CONCLUSION ---
S AND FUTURE WORK In this work, we have presented to our knowledge the first results on utilizing pretrained LLMs to leverage contextual information in order to improve speech recognition. We have demonstrated that with a simple decoder-only architecture we can condition the ASR output on the unstructured text. Our approach shows superior performance against a strong baseline, proving the feasability of the proposed method at scale. End-to-end contextualization via text promping with LLMs shows better context utilization compared to our strong RNNT based baselines. In addition, our ablation studies show that the system is robust to noise perturbations and shows abilities to perform a phonetic disambiguation. As part of the future work, we plan to extend the methods towards long context and other modalities. --- --{[([{ll [[[6. REFERENCES Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al., “Llama: Open and efficient foundation language models,” 2023. OpenAl, “Gpt-4 technical report,” 2023. Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey Zweig, Florian Metze, and Yatharth Saraf, “Contextual RNN-T for open domain ASR,” in INTERSPEECH. 2020, pp. 11-15, ISCA. Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim, et al., “Contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion,” in INTERSPEECH, 2021, pp. 1772-1776. Kanthashree Mysore Sathyendra, Thejaswi Muniyappa, Feng-Ju Chang, et al., “Contextual adapters for personalized speech recognition in neural transducers,” in ICASSP. 2022, pp. 8537-8541, IEEE. Golan Pundak, Tara N. Sainath, et al., “Deep context: End-to-end contextual speech recognition,’ inIEEE Spoken Language Technology Workshop. 2018, pp. 418-425, IEEE. Alex Xiao, Weiyi Zheng, Gil Keren, et al., “Scaling asr improves zero and few shot learning,” in INTERSPEECH, 2021. Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, et al., “Shallow-fusion end-to-end contextual biasing,” in INTERSPEECH. 2019, pp. 1418-1422, ISCA. Duc Le, Gil Keren, Julian Chan, et al., “Deep shallow fusion for rnn-t personalization,” in 202] IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 251257. Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur Gandhe, et al., “Robust acoustic and semantic contextual biasing in neural transducers for speech recognition,’ CoRR, vol. abs/2305.05271, 2023. Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al., “Adaptive contextual biasing for transducer based streaming speech recognition,” 2023. Jian Wu, Yashesh Gaur, et al., “On decoder-only architecture for speech-to-text and large language model integration,” 2023. Paul K. Rubenstein et al., “Audiopalm: A large language model that can speak and listen,” 2023. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al., “Prompting large language models with speech recognition abilities,’ 2023.[19] [20] [21]Alec Radford, Jong Wook Kim, et al., “Robust speech recognition via large-scale weak supervision,” in JCML. 23-29 Jul 2023, vol. 202 of Proceedings of Machine Learning Research, pp. 28492-28518, PMLR. Anmol Gulati, James Qin, et al., “Conformer: Convolution-augmented transformer for speech recognition,” in INTERSPEECH. 2020, pp. 5036-5040, ISCA. Jianlin Su, Yu Lu, et al., “Roformer: Enhanced transformer with rotary position embedding,” CoRR, vol. abs/2104.09864, 2021. Alex Graves et al., “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in JCML. 2006, vol. 148 of ACM International Conference Proceeding Series, pp. 369376, ACM. Edward J. Hu, Yelong Shen, et al., “Lora: Low-rank adaptation of large language models,” in JCLR. 2022, OpenReview.net. Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,’ INTERSPEECH, Sep 2019. Myle Ott, Sergey Edunov, Alexei Baevski, et al., “fairseq: A fast, extensible toolkit for sequence modeling,” in ACL (Demonstrations), Minneapolis, Minnesota, June 2019, pp. 48-53, Association for Computational Linguistics. Tom Ko, Vijayaditya Peddinti, et al., “Audio augmentation for speech recognition,” in INTERSPEECH. 2015, pp. 3586-3589, ISCA. Duc Le, Thilo Koehler, Christian Fuegen, and Michael L. Seltzer, “G2g: Tts-driven pronunciation learning for graphemic hybrid asr,’ in JCASSP, 2020, pp. 6869-6873. William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen, attend and spell,’ CoRR, vol. abs/1508.01211, 2015. Tim Dettmers and Luke Zettlemoyer, “The case for 4bit precision: k-bit inference scaling laws,” in ICML. 2023, vol. 202, pp. 7750-7774, PMLR. Jiayu Ding, Shuming Ma, et al., “Longnet: Scaling transformers to 1,000,000,000 tokens,” 2023.
"	"--- ABSTRACT ---
최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰어난 성능과 일반화 기능으로 인해 연구 커뮤니티에서 상당한 주목을 받았습니다. 이 논문에서는 LLM을 통합한 음성 인식 모델을 문맥화하는 새로운 방법을 소개합니다. 저희의 접근 방식은 음성 인식을 사전 훈련된 LLM을 기반으로 하는 혼합 모달 언어 모델링 작업으로 간주합니다. 저희는 컨텍스트에 대한 선택적 텍스트 토큰과 함께 오디오 기능을 제공하여 시스템이 디코더 전용 방식으로 필사본을 완료하도록 훈련합니다. 결과적으로 시스템은 훈련 중에 비정형적인 컨텍스트 정보를 활용하는 방법을 학습하도록 암묵적으로 인센티브를 받습니다. 저희의 경험적 결과는 추가 텍스트 컨텍스트가 제공될 때 6%의 WER 감소로 성능이 크게 향상되었음을 보여줍니다. 게다가 저희의 방법은 경쟁력 있는 성능을 보이며 전체적으로 7.5%의 WER, 25배 이상 큰 음성 데이터 세트에서 훈련된 기준 컨텍스트화된 RNN-T 시스템에 비해 희귀 단어에 대한 WER이 17% 향상됨을 발견했습니다. 전반적으로, 어댑터를 통해 소수의 훈련 가능한 매개변수만 추가함으로써 사전 훈련된 LLM에 대한 문맥화된 음성 인식 기능을 잠금 해제하는 동시에 동일한 텍스트 전용 입력 기능을 유지할 수 있음을 보여줍니다. 색인 용어 문맥적 편향, 대규모 언어 모델, 음성 인식 1.
--- INTRODUCTION ---
최근 몇 년 동안, 질문 답변 및 요약과 같은 작업에서 성과를 향상시키는 데 탁월한 효능이 있어 대규모 언어 모델(LLM)에 대한 관심이 커지고 있으며, 이는 특수 모델을 능가합니다[1, 2]. LLM은 방대한 양의 텍스트 데이터로 학습되므로 네트워크 내에 풍부한 세계 지식을 캡슐화합니다. 이러한 축적된 지식과 맥락적 이해는 특히 발화 주변의 추가 맥락이 오디오 그 자체를 넘어 사용 가능할 때 자동 음성 인식(ASR) 분야에서 특히 유익한 것으로 입증되었습니다. 예를 들어, 비디오 제목과 설명은 비디오 주제에 대한 통찰력을 제공하거나 언급될 수 있는 명명된 엔터티에 대한 단서를 제공할 수 있습니다[3, 4]. 이러한 맥락적 inWork Meta AI에서 인턴십하는 동안 수행되었습니다. LLM 디코더 텍스트-텍스트 LoRa 어텐션 어댑터<bos> 오디오 토큰 문맥적 텍스트 토큰(비디오 제목, 설명) 오디오 인코더 인식된 음성 텍스트 그림 1. 사전 학습된 LLM 백본을 기반으로 하는 오디오 및 선택적 텍스트 토큰으로 구성된 혼합 모달 문맥을 갖춘 음성 인식 모델. 음성 인코더와 LLM 디코더는 모두 처음에 사전 학습됩니다. LLM 가중치는 고정되고(주황색 블록), 오디오 인코더와 LoRa 어댑터는 학습 중에 미세 조정됩니다(파란색 블록). 형성은 특정 단어, 도메인별 용어 또는 명명된 엔터티를 종종 문맥에서만 유추할 수 있으므로 어려운 발음을 모호하지 않게 하는 데 도움이 될 수 있습니다. ASR 문맥화에 대한 기존 접근 방식[4, 3, 5, 6]은 토큰 또는 구문 수준에서 작동하며 가중 유한 상태 변환기(WFST)를 사용한 바이어싱이나 특수한 주의 네트워크를 사용하는 것과 같은 기술을 사용합니다. 이러한 기술은 일반적으로 디코딩 단계에서 통합되거나 별도의 구성 요소로 학습됩니다. 결과적으로 문맥화는 명명된 엔터티 또는 특수한 도메인 내 용어를 인식하는 ASR 시스템의 기능을 크게 향상시킵니다. 그러나 이러한 접근 방식에는 몇 가지 제한이 있습니다. 편향은 외부 정보 전체를 기반으로 맥락화하는 것과는 달리 개별 구문이나 단어로 제한됩니다(예: 주제 기반 편향). 편향 강도는 일반적으로 하이퍼 매개변수를 통해 제어되거나 시스템이 과도하게 편향되지 않도록 하기 위해 특수한 아키텍처 변경 및 교육 절차가 필요합니다. 일부 맥락화 방법은 인코더와 직접 상호 작용하지 않고 디코더 상태에만 영향을 미칩니다. 이 연구에서는 음성 인식에 맞게 조정된 LLM의 최근 개발에서 영감을 받은 디코더 전용 아키텍처인 Speech LLAMA를 제안합니다. 추가 하이퍼 매개변수 없이 맥락 정보를 종단 간에 사용하도록 훈련됩니다. 구체적으로, 1) 오디오 토큰과 함께 사용 가능한 전체 텍스트 맥락을 ASR 시스템에 프롬프트로 추가합니다. 따라서 Speech LLAMA는 다음 말한 단어를 디코딩할 때 맥락 텍스트 토큰과 음향 토큰을 되돌아보고 교차 상관 관계를 지정할 수 있는 완전한 유연성을 갖추고 있습니다. 2) 우리는 Speech LLAMA를 위한 사전 학습된 디코더로 공개적으로 이용 가능한 7B LLAMA LLM[1]을 사용합니다. 이것은 음성 인식을 다음 토큰 예측이 있는 혼합 모달 언어 모델로 간주할 수 있으므로 문맥적 ASR의 전반적인 설계를 단순화합니다. 이것의 배후에 있는 우리의 직관은 사전 학습된 LLM이 이미 언어 정보를 추출하여 발화를 감안할 때 문맥의 어느 부분이 관련이 있는지 추론할 때 특히 유용해야 한다는 것입니다. 경쟁 벤치마크에 대한 우리의 결과는 이 모델링 접근 방식의 실행 가능성을 시사합니다. 2.
--- RELATED WORK ---
심층 및 얕은 바이어싱을 포함한 음성 인식 모델 맥락화에 대한 여러 연구가 있었습니다[8, 4]. Le et al.[4]은 디코딩 중에 동적으로 첨부되는 바이어싱 문자열로 구성된 가중 유한 상태 변환기(WFST)를 도입했으며 RNN-T 시스템의 점수와 바이어싱 WFST가 보간됩니다. 이러한 접근 방식의 장점은 훈련이 완료된 후 모든 시스템에 첨부할 수 있다는 것입니다. 또 다른 연구 분야는 심층 바이어싱입니다.
--- METHOD ---
LLM을 통합한 음성 인식 모델을 문맥화하기 위해. 저희의 접근 방식은 음성 인식을 사전 훈련된 LLM을 기반으로 하는 혼합 모달 언어 모델링 작업으로 간주합니다. 저희는 컨텍스트에 대한 선택적 텍스트 토큰과 함께 오디오 기능을 제공하여 시스템이 디코더 전용 방식으로 필사본을 완료하도록 훈련합니다. 결과적으로 시스템은 훈련 중에 구조화되지 않은 컨텍스트 정보를 활용하는 방법을 학습하도록 암묵적으로 인센티브를 받습니다. 저희의 경험적 결과는 추가 텍스트 컨텍스트가 제공될 때 6%의 WER 감소와 함께 성능이 크게 향상되었음을 보여줍니다. 게다가 저희의 방법은 경쟁력 있는 성능을 보이며 25배 이상 큰 음성 데이터 세트에서 훈련된 기준 컨텍스트화된 RNN-T 시스템에 비해 전체적으로 7.5%의 WER, 희귀 단어에 대한 WER은 17% 향상됩니다. 전반적으로 어댑터를 통해 소수의 훈련 가능한 매개변수만 추가하면 사전 훈련된 LLM에 대한 컨텍스트화된 음성 인식 기능을 잠금 해제하는 동시에 동일한 텍스트 전용 입력 기능을 유지할 수 있음을 보여줍니다. 색인 용어 문맥적 편향, 대규모 언어 모델, 음성 인식 1. 서론 최근 몇 년 동안, 질문 답변 및 요약과 같은 작업에서 성과를 향상시키는 데 탁월한 효능이 있어 대규모 언어 모델(LLM)에 대한 관심이 커지고 있으며, 이는 특수 모델을 능가합니다[1, 2]. LLM은 방대한 양의 텍스트 데이터로 학습되므로 네트워크 내에 풍부한 세계 지식을 캡슐화합니다. 이러한 축적된 지식과 문맥적 이해는 특히 오디오만 넘어 발화를 둘러싼 추가 맥락을 사용할 수 있는 경우 자동 음성 인식(ASR) 분야에서 특히 유익한 것으로 입증되었습니다. 예를 들어, 비디오 제목과 설명은 비디오 주제에 대한 통찰력을 제공하거나 언급될 수 있는 명명된 엔터티에 대한 단서를 제공할 수 있습니다[3, 4]. 이러한 문맥적 inWork Meta AI에서 인턴십하는 동안 수행되었습니다. LLM 디코더 텍스트-텍스트 LoRa 어텐션 어댑터<bos> 오디오 토큰 문맥적 텍스트 토큰(비디오 제목, 설명) 오디오 인코더 인식된 음성 텍스트 그림 1. 사전 학습된 LLM 백본을 기반으로 하는 오디오 및 선택적 텍스트 토큰으로 구성된 혼합 모달 문맥을 갖춘 음성 인식 모델. 음성 인코더와 LLM 디코더는 모두 처음에 사전 학습됩니다. LLM 가중치는 고정되고(주황색 블록), 오디오 인코더와 LoRa 어댑터는 학습 중에 미세 조정됩니다(파란색 블록). 형성은 특정 단어, 도메인별 용어 또는 명명된 엔터티를 종종 문맥에서만 유추할 수 있으므로 어려운 발음을 모호하지 않게 하는 데 도움이 될 수 있습니다. ASR 문맥화에 대한 기존 접근 방식[4, 3, 5, 6]은 토큰 또는 구문 수준에서 작동하며 가중 유한 상태 변환기(WFST)를 사용한 바이어싱이나 특수한 주의 네트워크를 사용하는 것과 같은 기술을 사용합니다. 이러한 기술은 일반적으로 디코딩 단계에서 통합되거나 별도의 구성 요소로 학습됩니다. 결과적으로 문맥화는 명명된 엔터티 또는 특수한 도메인 내 용어를 인식하는 ASR 시스템의 기능을 크게 향상시킵니다. 그러나 이러한 접근 방식에는 몇 가지 제한이 있습니다. 편향은 외부 정보 전체를 기반으로 맥락화하는 것과는 달리 개별 구문이나 단어로 제한됩니다(예: 주제 기반 편향). 편향 강도는 일반적으로 하이퍼 매개변수를 통해 제어되거나 시스템이 과도하게 편향되지 않도록 하기 위해 특수한 아키텍처 변경 및 교육 절차가 필요합니다. 일부 맥락화 방법은 인코더와 직접 상호 작용하지 않고 디코더 상태에만 영향을 미칩니다. 이 연구에서는 음성 인식에 맞게 조정된 LLM의 최근 개발에서 영감을 받은 디코더 전용 아키텍처인 Speech LLAMA를 제안합니다. 추가 하이퍼 매개변수 없이 맥락 정보를 종단 간에 사용하도록 훈련됩니다. 구체적으로, 1) 오디오 토큰과 함께 사용 가능한 전체 텍스트 맥락을 ASR 시스템에 프롬프트로 추가합니다. 따라서 Speech LLAMA는 다음 말한 단어를 디코딩할 때 맥락 텍스트 토큰과 음향 토큰을 되돌아보고 교차 상관 관계를 지정할 수 있는 완전한 유연성을 갖추고 있습니다. 2) 우리는 Speech LLAMA를 위한 사전 학습된 디코더로 공개적으로 이용 가능한 7B LLAMA LLM[1]을 사용합니다. 이것은 음성 인식을 다음 토큰 예측이 있는 혼합 모달 언어 모델로 간주할 수 있으므로 문맥적 ASR의 전반적인 설계를 단순화합니다. 이것의 배후에 있는 우리의 직관은 사전 학습된 LLM이 이미 언어 정보를 추출하여 발화를 고려할 때 문맥의 어느 부분이 관련이 있는지 추론할 때 특히 유용해야 한다는 것입니다. 경쟁 벤치마크에 대한 우리의 결과는 이 모델링 접근 방식의 타당성을 시사합니다. 2. 관련 연구 심층 및 얕은 바이어싱[8, 4]을 포함하여 음성 인식 모델 문맥화에 대한 여러 연구가 있었습니다. Le et al.[4]은 디코딩 중에 동적으로 첨부된 바이어싱 문자열로 구성된 가중 유한 상태 변환기(WFST)를 도입했으며 RNN-T 시스템의 점수와 바이어싱 WFST가 보간됩니다. 이러한 접근 방식의 장점은 훈련이 완료된 후 모든 시스템에 첨부할 수 있다는 것입니다. 또 다른 연구 분야는 모델 학습 중에 맥락화를 종단 간에 통합하는 심층 바이어싱 방법입니다[9, 3, 6, 10, 11, 5]. 이러한 접근 방식의 일반적인 한계는 사용 가능한 전체 맥락을 제공하는 것이 아니라 구문 수준에서 바이어싱이 발생한다는 것입니다. 또한 이러한 접근 방식은 기본 ASR 아키텍처에 특수 바이어싱 모듈을 추가해야 합니다. 이 연구와 병행하여 음성 관련 작업을 위한 LLM을 통합하는 여러 접근 방식이 제시되었습니다. Wu 등[12]은 텍스트 프롬프트(&quot;오디오를 언어 X로 번역&quot;)와 오디오 표현을 연결하여 음성 번역을 위한 LLaMA LLM을 통합했습니다. AudioPalm[13] 모델은 음성-텍스트 및 음성-음성 작업을 위해 오디오 및 텍스트 토큰을 혼합하여 제안되었습니다. Fathullah 등[14]은 다국어 데이터에서 LLAMA 모델에 대한 음성 인식 기능을 활성화하는 결과를 발표했습니다. 최근 Whisper 모델[15]은 바이어싱 접근 방식을 통합하여 이전 세그먼트의 전사본을 장문 음성 인식을 위한 프롬프트에 추가했습니다. 그들의 작업과 다른 점은, 우리는 비디오 제목과 설명이 음성의 맥락과 항상 일치하지는 않기 때문에 구조화되지 않고 때로는 관련성이 없는 텍스트 맥락에 따라 시스템을 편향시킨다는 것입니다.
--- EXPERIMENT ---
AL SETUP 모델: 그림 1은 제안된 모델의 개요를 보여줍니다. 이 음성 LLM 아키텍처는 오디오 인코더와 텍스트 디코더라는 두 가지 주요 블록으로 구성됩니다. 오디오 인코더는 먼저 4개의 다운샘플링 블록을 적용하여 오디오 표현의 시간을 16배 단축합니다. 그 후 회전 위치 임베딩[17]이 있는 Conformer[16] 블록[17] 스택이 512의 은닉 차원과 9의 커널 크기로 적용됩니다. 마지막에 추가 다운샘플링 블록을 추가합니다. 그 결과 디코더는 4,096의 차원으로 320ms마다 샘플링된 오디오 토큰을 관찰합니다. 동일한 교육 데이터에서 300k 교육 단계에 대해 Connectionist Temporal Classification[18] 기준으로 오디오 인코더를 사전 교육했습니다. 사전 교육된 7B LLaMA(v1)[1]를 디코더로 사용했습니다. 텍스트 전용 LLAMA를 음성 인식 작업에 적용하기 위해 모든 디코더 계층의 셀프 어텐션 계층에서 쿼리, 키, 값 및 출력 투영 행렬에 Low-Rank Adapter[19]를 추가하면서 나머지 LLM 매개변수는 학습 내내 고정했습니다.다음과 같은 LoRa 매개변수를 사용했습니다.크기 32의 순위, 5%의 드롭아웃 비율 및 0.05 스케일링 매개변수.전체 LoRa 매개변수는 LLM 디코더에 3천만 개의 학습 가능한 매개변수를 추가하고 나머지 60억 개는 고정된 상태로 유지합니다.25ms의 윈도우로 10ms마다 계산된 80차원 log Mel 피처를 사용했습니다.폭 27의 두 주파수 마스크와 발화 길이의 최대 폭 4%의 10개 시간 마스크가 있는 SpecAugment[20]. 우리는 혼합 정밀도로 200,000개의 업데이트에 대해 모델을 훈련했으며, 처음 20개의 업데이트에서 학습률을 5e-4로 선형적으로 증가시키고 나머지 업데이트에서 지수적으로 1e-5로 감소시켰습니다. 우리는 매개변수 ẞ1 = 0.9,0.98, 가중치 감소 1e-5, 그래디언트 노름을 1로 클리핑하는 Adam을 사용합니다. 우리 모델은 Fairseq 라이브러리[21]를 사용하여 3일 동안 128개의 A100 GPU로 훈련되었습니다. = = 데이터: 모델은 공개 Facebook 및 Instagram 비디오에서 파생된 개인 식별 정보(PII)가 없는 익명화된 사내 데이터 세트에서 훈련되었습니다. 데이터는 속도 섭동[22] 및 무작위 샘플링된 가산 배경 잡음의 두 가지 왜곡 방법으로 추가로 보강되었습니다. 평가를 위해 우리는 최소 100자 길이의 컨텍스트를 갖고 컨텍스트에서 적어도 하나의 비빈도 단어가 전사에 나타나는 약 34시간 분량의 음성으로 구성된 3,200개의 비디오를 샘플링했습니다. 메트릭: 모델을 평가하기 위해 전체 단어 오류율(WER)과 희귀 단어만 고려하는 희귀 WER을 모두 보고합니다. 단어는 훈련 데이터에서 추정된 가장 빈번한 단어의 90% 백분위수에 포함되지 않으면 희귀한 것으로 간주됩니다. 텍스트 컨텍스트: Xiao et al. [7]과 유사하게 비디오 제목과 비디오 설명을 외부 컨텍스트로 통합합니다. 유니코드 문자 정규화 및 모든 비 ASCII 기호 제거와 같은 기본 텍스트 후처리를 수행합니다. 전반적으로 감독 비디오 데이터 세트의 비디오 중 약 25%가 비어 있지 않은 텍스트 컨텍스트를 갖습니다. 비디오 제목이나 설명이 있는 경우 먼저 연결한 다음 LLAMA 토크나이저로 토큰화합니다. 그런 다음<bos> 토큰을 텍스트 토큰과 비교합니다. 비디오 제목과 설명이 모두 누락된 경우 입력은 문맥 정보가 없는 기존 ASR 설정에 해당합니다. 교차 엔트로피 표 1. 영어 음성 데이터에 대한 대규모 RNN-T 기준과 비교한 Speech LLaMA의 평가 결과. 전체 WER과 Rare WER을 보고합니다. Rare WER은 특히 데이터 세트에서 희귀 단어를 인식하는 정확도에 초점을 맞춥니다. 모델 음성 데이터(h) 학습 가능한 매개변수(M) 컨텍스트 존재 학습 WER(%) SUB INS DEL 드물게 나타나는 WER(%) 평가 1B RNN-T [7] 4M12.6.53 3.21 2.30.1B RNN-T [7] 4M12.6.23 3.2.28.Speech LLaMa 150k11.6.09 3.20 2.27.Speech LLaMa 150k11.Speech LLaMa 150k11.6.28 3.07 2.5.76 3.14 2.28.23.loss는 컨텍스트 토큰에 대해 마스크 처리되고 음성 토큰에 대해서만 계산됩니다. 이 실험에서는 계산상의 이유로 텍스트 콘텐츠를 최대 50개 토큰으로 제한합니다. 컨텍스트가 임계값보다 길면 훈련 중에 크기 50의 임의 자르기를 수행하고 추론 중에 선행 토큰을 자릅니다.기준선: 기준으로 400만 시간의 지도 및 반지도 음성 데이터로 훈련된 10억 개의 매개변수가 있는 변압기 기반 RNN-T 시스템[7]을 사용했습니다.RNN-T 시스템 아키텍처는 인코더의 변압기 계층과 디코더의 3개 LSTM 계층으로 구성됩니다.문맥화를 위해 신경 언어 모델링 얕은 융합[4]을 사용하는 WFST 바이어싱 방법을 사용하는데, 여기서 바이어싱 FST는 비디오 제목과 설명으로 구성됩니다.RNN-T 기준선과 Speech LLAMA에 대해 디코딩하는 동안 정확히 동일한 문맥 정보를 사용합니다.4. 결과 표 1은 평가 세트에 대한 디코딩 결과의 요약을 보여줍니다.디코딩 중에 문맥화 정보를 제시하는 경우와 제시하지 않는 경우의 두 가지 시나리오를 고려하여 Speech LLAMA를 오프라인 RNN-T 1b 모델과 비교합니다. RNN-T를 사용하여 이러한 시나리오에 대해 얻은 WER 점수는 각각 12.34%와 12.13%입니다. 문맥적 편향은 약 1.7%의 상대적 WER 감소로 이어집니다. 훈련 및 평가 중에 문맥 정보를 사용하지 않더라도 Speech LLAMA는 11.70%의 WER을 달성하여 훨씬 더 많은 데이터로 훈련된 RNN-T 시스템보다 상대적으로 5.2% 감소했습니다. 훈련 및 평가 중에 문맥을 통합함으로써 상당한 개선을 이루어 전체 WER이 11.22%가 되고 Rare WER에서 17%의 상대적 개선이 이루어져 문맥화를 사용한 RNN-T 모델의 성능을 능가했습니다. 문맥을 사용하여 훈련했지만 추론 중에 문맥을 제공하지 않은 Speech LLAMA를 평가할 때 WER이 11.98%라는 점에 주목할 가치가 있습니다. 이는 문맥 없이 훈련된 모델과 비교했을 때 약간의 WER 격차에 해당합니다. 우리는 이 사소한 성능 차이를 향후 작업으로 미루고, 맥락에 특정 지터를 추가하면 맥락의 존재에 대한 모델의 일반화가 개선될 수 있습니다.4.1. 소거 연구 4.1.1. 맥락 민감도 모델이 맥락을 사용하는 방법을 더 잘 이해하기 위해, 우리는 모델이 맥락 교란에 얼마나 수용적인지 연구했습니다.이를 위해 우리는 프롬프트를 수정하고 디코딩에 미치는 영향을 측정하는 몇 가지 방법을 시도했습니다.특히, 우리는 다음을 실험했습니다.1. 실제 맥락을 훈련 데이터에서 무작위로 샘플링한 단어로 대체합니다.2. 맥락을 기준 진실 단어로 대체합니다.우리는 모델이 단어를 필사하는 데 큰 문제가 없을 것이라고 가정하기 때문에 이 실험에서 빈번한 단어를 걸러냅니다.모델이 맥락에서 단어를 복사하여 붙여넣을 수 있다면 WER이 상당히 감소할 것으로 예상합니다.3. 맥락적 단어를 필사본에 나타나는 단어의 음성적 철자로 대체합니다. 우리의 직감은 이러한 대체가 모델에 특히 도전적인 일이며 무작위 대체에 비해 더 큰 WER 변화를 예상해야 한다는 것입니다.재철자를 생성하기 위해 G2G[23] 모델을 사용했습니다.기본 진실의 모든 희귀 단어에 대해 G2G 모델에서 대체 철자를 샘플링하여 컨텍스트에 추가합니다.예를 들어, 단어 ball이 컨텍스트와 기본 진실에 존재하는 경우 bawl로 대체하여 원래 토큰 대신 컨텍스트로 사용합니다.4. 이전 섭동에 더하여 컨텍스트에 비슷하게 들리는 단어를 추가하여 조사합니다(예: ball과 bawl 토큰이 모두 컨텍스트에 존재함).이는 컨텍스트에서 경쟁 단어가 주어졌을 때 ASR 시스템이 실제 말한 단어의 모호성을 해소하는 능력을 테스트합니다.표 2. 디코딩 단계 동안 다양한 컨텍스트 섭동 하의 WER.표 4. 디코더 전용 Speech LLM과 교차 주의 Speech LLM의 성능 비교. 문맥 노이즈 WER(%) 희귀 WER(%) (원래 문맥) 11.23.(모든 문맥 제거) 11.28.무작위 12.28.재철자 11.28.재철자(추가) 기준 진실 11.10.25.19.표 2에 결과를 제시합니다. 전체 문맥을 훈련 데이터에서 샘플링한 무작위 단어로 대체하면 모든 외부 문맥을 제거하는 것과 비교했을 때 WER에서 미미한 차이만 발생합니다(11.98% 대 12.07%). 이는 모델이 일부 문맥 노이즈에 강하고 관련성 있는 문맥과 관련성 없는 문맥을 구별할 수 있음을 나타냅니다. 문맥과 기준 진실 모두에 일치하는 희귀 단어를 G2G 재철자로 대체하면 WER이 상당히 감소합니다(11.22% → 11.89%). 문맥을 전혀 사용하지 않는 것과 거의 일치합니다. 이는 관찰된 대부분의 이득이 모델이 문맥에서 특정 단어를 복사할 수 있기 때문이라는 것을 암시합니다. 대조적으로, 일치하는 문맥 단어를 대체하는 대신 경쟁하는 유사한 발음의 단어를 추가했을 때, 우리는 더 작은 WER 하락(11.22% → 11.46%)을 관찰했습니다. 이는 모델이 의미가 다르지만 발음이 비슷한 단어에 반드시 혼동을 받지 않는다는 것을 나타냅니다. 나아가, 기준 진실에서 희귀 단어를 문맥에 적용하면 WER이 10.50%(상대적 변화 6%)로 개선되고 희귀 WER은 상대적으로 18% 개선됩니다. 이는 모델이 희귀 엔터티를 더 잘 인식하기 위해 문맥 정보가 있을 때 이를 활용할 수 있는 능력을 더욱 증명합니다. 표 3. WER에 대한 문맥 마스킹 구조의 영향. 마스킹 인과적 전체 마스크 WER(%) 11.11.4.1.2. 인과적 대 전체 마스킹 전통적으로 인과적 마스킹은 디코더 전용 언어 모델의 모든 셀프 어텐션 계층에서 사용되어 향후 정보 유출을 방지합니다. 그러나 오프라인 음성 인식의 경우 디코딩 시점에 전체 오디오 및 텍스트 컨텍스트가 관찰되고 인과적으로 마스크해야 하는 것은 전사 토큰뿐입니다.이 섹션에서는 모든 입력 토큰에 인과적 마스킹을 적용하는 것의 영향을 실험하고 텍스트 및 오디오 컨텍스트에 전체 마스크를 적용한 다음 전사 토큰에 인과적 마스킹을 적용하는 것과 대조합니다.오디오 표현은 이미 완전히 맥락화되었지만 텍스트 토큰이 전체 마스킹의 이점을 얻을 수 있다고 가정합니다.표 3에 결과를 제시합니다.전체 마스크는 인과적 마스킹보다 약간 더 나은 WER만 보여줍니다(11.22%에서 11.15%로 향상).효율적인 셀프 어텐션 구현이 현재 인과적 마스킹(Flash-Attention v2)에 맞춰져 있고 사용자 지정 마스킹을 사용하면 학습 속도가 10% 느려지기 때문에 비용이 발생합니다.4.1.3. 디코더 전용 대 교차 어텐션 나아가, 우리는 Speech LLM 아키텍처를 Listen-Attend-Spell 아키텍처로 변환하여 디코더 전용 접근 방식을 기존의 인코더-디코더 모델과 비교했습니다[24]. 이를 달성하기 위해 오디오와 텍스트 토큰을 연결하는 대신 별도로 처리했습니다. 우리는 모든 LLM 디코더 계층에 훈련 가능한 교차 어텐션 행렬을 추가했습니다. 표는 이 연구 결과를 보여줍니다. 우리는 두 접근 방식이 비슷한 성과를 보였지만, 인코더-디코더 아키텍처의 경우 약간의 개선만 있었습니다(11.22% 11.18%). 이는 디코더 전용 접근 방식이 맥락화가 있거나 없이 ASR을 수행하는 실행 가능하고 간단한 방법임을 나타냅니다. 그러나 디코더 전용 접근 방식의 한 가지 한계는 전체 시퀀스 길이에 제한을 가할 수 있는 이차 어텐션 복잡도입니다. 이 한계는 맥락이 커질수록 커집니다. 이 문제를 해결하기 위해 낮은 정밀도 훈련(8개 궤도) 및 선형 어텐션 근사 방법과 같은 기술을 사용할 수 있습니다[25, 26]. 5.
--- CONCLUSION ---
S 및 향후 작업 이 작업에서 우리는 사전 훈련된 LLM을 사용하여 문맥 정보를 활용하여 음성 인식을 개선하는 것에 대한 최초의 결과를 제시했습니다. 우리는 간단한 디코더 전용 아키텍처로 구조화되지 않은 텍스트에 대한 ASR 출력을 조건화할 수 있음을 보여주었습니다. 우리의 접근 방식은 강력한 기준선에 비해 우수한 성능을 보여 제안된 방법의 규모에 대한 실행 가능성을 증명합니다. LLM을 사용한 텍스트 프롬핑을 통한 종단 간 문맥화는 강력한 RNNT 기반 기준선에 비해 더 나은 문맥 활용도를 보여줍니다. 또한, 우리의 절제 연구는 시스템이 소음 교란에 강하고 음성적 모호성 해소를 수행하는 능력을 보여준다는 것을 보여줍니다. 향후 작업의 일환으로 우리는 긴 문맥 및 기타 모달리티로 방법을 확장할 계획입니다. 6. 참고문헌 [1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al., &quot;Llama: Open and efficient foundation language models,&quot; 2023. [2] OpenAI, &quot;Gpt-4 technical report,&quot; 2023. [3] Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey Zweig, Florian Metze, Yatharth Saraf, &quot;Contextual RNN-T for open domain ASR,&quot; INTERSPEECH. 2020, pp. 11-15, ISCA. [4] Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim 등, &quot;Trie 기반 딥 바이어싱 및 얕은 퓨전을 사용한 문맥화된 스트리밍 엔드투엔드 음성 인식&quot;, INTERSPEECH, 2021, pp. 1772-1776. [5] Kanthashree Mysore Sathyendra, Thejaswi Muniyappa, Feng-Ju Chang 등, &quot;신경 변환기에서 개인화된 음성 인식을 위한 문맥 어댑터&quot;, ICASSP. 2022, pp. 8537-8541, IEEE. [6] Golan Pundak, Tara N. Sainath 등, &quot;딥 컨텍스트: 엔드투엔드 문맥적 음성 인식&quot;, IEEE Spoken Language Technology Workshop. 2018, pp. 418-425, IEEE. [7] Alex Xiao, Weiyi Zheng, Gil Keren 등, 영어: &quot;Scaling asr improvements zero and few shot learning,&quot; in INTERSPEECH, 2021. [8] Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, et al., &quot;Shallow-fusion end-to-end contextual biasing,&quot; in INTERSPEECH. 2019, pp. 1418–1422, ISCA. [9] Duc Le, Gil Keren, Julian Chan, et al., &quot;Deep shallow fusion for rnn-t personalization,&quot; in 2021 IEEE Spoken Language Technology Workshop(SLT), 2021, pp. 251– 257. [10] Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur Gandhe, et al., &quot;Robust acoustic and semantic contextual biasing in neural transducers for speech awareness,&quot; CORR, vol. abs/2305.05271, 2023. [11] Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al., &quot;트랜스듀서 기반 스트리밍 음성 인식을 위한 적응적 맥락적 편향,&quot; 2023. [12] Jian Wu, Yashesh Gaur, et al., &quot;음성-텍스트 및 대규모 언어 모델 통합을 위한 디코더 전용 아키텍처에 관하여,&quot; 2023. [13] Paul K. Rubenstein et al., &quot;Audiopalm: 말하고 들을 수 있는 대규모 언어 모델,&quot; 2023. [14] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al., &quot;음성 인식 기능을 갖춘 대규모 언어 모델 프롬프트,&quot; 2023. [15] Alec Radford, Jong Wook Kim, et al., &quot;대규모 약한 감독을 통한 견고한 음성 인식,&quot; ICML. 7월 23-29일 2023, vol. 202 of Machine Learning Research, pp. 28492–28518, PMLR. [16] Anmol Gulati, James Qin, et al., &quot;Conformer: 음성 인식을 위한 합성곱 증강 변환기,&quot; INTERSPEECH에 게재됨. 2020, pp. 5036-5040, ISCA. [17] Jianlin Su, Yu Lu, et al., &quot;Roformer: 회전 위치 임베딩이 있는 향상된 변환기,&quot; CoRR, vol. abs/2104.09864, 2021. [18] Alex Graves 외, &quot;연결주의 시간 분류: 순환 신경망을 사용한 분할되지 않은 시퀀스 데이터 레이블링&quot;, ICML. 2006, ACM 국제 회의록 시리즈 vol. 148, pp. 369– 376, ACM. [19] Edward J. Hu, Yelong Shen 외, &quot;Lora: 대규모 언어 모델의 저순위 적응&quot;, ICLR. 2022, OpenReview.net. [20] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le, &quot;Specaugment: 자동 음성 인식을 위한 간단한 데이터 증강 방법,&quot; INTERSPEECH, 2019년 9월. [21] Myle Ott, Sergey Edunov, Alexei Baevski, et al., &quot;fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 툴킷,&quot; ACL(Demonstrations), 미네소타주 미니애폴리스, 2019년 6월, 48-53쪽, 계산언어학 협회. [22] Tom Ko, Vijayaditya Peddinti 외, &quot;음성 인식을 위한 오디오 증강,&quot; INTERSPEECH. 2015, 3586-3589쪽, ISCA. [23] Duc Le, Thilo Koehler, Christian Fuegen, Michael L. Seltzer, &quot;G2g: 문자적 하이브리드 ASR을 위한 Tts 기반 발음 학습,&quot; ICASSP, 2020, 6869-6873쪽. [24] William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, &quot;듣고, 주의하고, 철자하세요,&quot; CORR, vol. abs/1508.01211, 2015. [25] Tim Dettmers 및 Luke Zettlemoyer, &quot;4비트 정밀도에 대한 사례: k비트 추론 스케일링 법칙,&quot; ICML. 2023, vol. 202, pp. 7750–7774, PMLR. [26] Jiayu Ding, Shuming Ma, et al., &quot;Longnet: 1,000,000,000개 토큰으로 변환기 스케일링,&quot; 2023.
"
"--- ABSTRACT ---
In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a “free lunch” that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method—termed “FreeU” — that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net’s skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/Freeu/. 1.
--- METHOD ---
that substantially improves diffusion model sample quality at no costs: no training, no additional parameter introduced, and no increase in memory or sampling time. Abstract In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a “free lunch” that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method—termed “FreeU” — that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net’s skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/Freeu/. 1. Introduction Diffusion probabilistic models, a cutting-edge category of generative models, have become a focal point in the research landscape, particularly for tasks related to computer vision [5, 6, 8, 10, 12, 20, 22, 26, 28, 29, 32]. Distinct from other classes of generative models [3, 7, 9, 16-19, 21, 25, 34, 35] such as Variational Autoencoder (VAE) [21], Generative Adversarial Networks (GANs) [3, 9, 16-19, 25], and vector-quantized approaches [7, 34], diffusion models introduce a novel generative paradigm. These models employ a fixed Markov chain to map the latent space, facilitating intricate mappings that capture latent structural complexities within a dataset. Recently, its impressive generative capabilities, ranging from the high level of details to the diversity of the generated examples, have fueled groundbreaking advancements in a variety of computer vision applications such as image synthesis [12, 29, 32], image editing [1, 4, 14, 24], image-to-image translation [4, 31, 36], and text-to-video generation [2, 11, 13, 23, 33, 37, 38, 40]. The diffusion models are comprised of the diffusion process and the denoising process. During the diffusion process, Gaussian noise is gradually added to the input data and eventually corrupts it into approximately pure Gaussian noise. During the denoising process, the original input data --- --squirrel eating a Burger. Low frequency High frequency ° °o ° in A Log amplitude StepstepSwap 0.0n O2n O4n O.6n O.8n 1.0m Frequency Figure 2. The denoising process. The top row illustrates the image’s progres- Figure 3. Relative log amplitudes of Fourier with varisive denoising process across iterations, while the subsequent two rows display ations of the backbone scaling factor b. Increasing low-frequency and high-frequency components after the inverse Fourier Trans- in b correspondingly results in a suppression of highform, matching each step. It’s evident that low-frequency components change frequency components in the images generated by the difslowly, whereas high-frequency components exhibit more significant variations fusion model. during the denoising process. is recovered from its noise state through a learned sequence of inverse diffusion operations. Usually, a U-Net is trained to iteratively predict the noise to be removed at each denoising step. Existing works focus on utilizing pre-trained diffusion U-Nets for downstream applications, while the internal properties of the diffusion U-Net, remain largely underexplored. Beyond the application of diffusion models, in this paper, we are interested in investigating the effectiveness of diffusion U-Net for the denoising process. To better understand the denoising process, we first present a paradigm shift toward the Fourier domain to perspective the generated process of diffusion models, a research area that has received limited prior investigation. As illustrated in Fig. 2, the uppermost row provides the progressive denoising process, showcasing the generated images across successive iterations. The subsequent two rows exhibit the associated low-frequency and high-frequency spatial domain information after the inverse Fourier Transform, aligning with each respective step. Evident from Fig. 2 is the gradual modulation of lowfrequency components, exhibiting a subdued rate of change, while their high-frequency components display more pronounced dynamics throughout the denoising process. These findings are further corroborated in Fig.3. This can be intuitively explained: 1) Low-frequency components inherently embody the global structure and characteristics of an image, encompassing global layouts and smooth color. These components encapsulate the foundational global elements that constitute the image’s essence and representation. Its rapid alterations are generally unreasonable in denoising processes. Drastic changes to these components could fundamentally reshape the image’s essence, an outcome typically incompatible with the objectives of denoising processes. 2) Conversely, high-frequency components contain the rapid changes in the images, such as edges and textures. These finer details are markedly sensitive to noise, often manifesting as random high-frequency information when noise is introduced to an image. Consequently, denoising processes need to expunge noise while upholding indispensable intricate details. In light of these observations between low-frequency and high-frequency components during the denoising process, we extend our investigation to ascertain the specific contributions of the U-Net architecture within the diffusion framework. In each stage of the U-Net decoder, the skip features from the skip connection and the backbone features are concatenated together. Our investigation reveals that the main backbone of the U-Net primarily contributes to denoising. Conversely, the skip connections are observed to introduce high-frequency features into the decoder module. These connections propagate fine-grained semantic information to make it easier to recover the input data. However, an unintended consequence of this propagation is the potential weakening of the backbone’s inherent denoising capabilities during the inference phase. This can lead to the generation of abnormal image details, as illustrated in the first row of Fig. 1. Building upon this revelation, we propel forward with the introduction of a novel strategy, denoted as “FreeU”, which holds the potential to improve sample quality without necessitating the computational overhead of additional --- --training or fine-tuning. During the inference stage, we instantiate two specialized modulation factors designed to balance the feature contributions from the U-Net architecture’s primary backbone and skip connections. The first, termed the backbone feature factors, aims to amplify the feature maps of the main backbone, thereby bolstering the denoising process. However, we find that while the inclusion of backbone feature scaling factors yields significant improvements, it can occasionally lead to an undesirable oversmoothing of textures. To mitigate this issue, we introduce the second factor, skip feature scaling factors, aiming to alleviate the problem of texture oversmoothing. Our FreeU framework exhibits seamless adaptability when integrated with existing diffusion models, encompassing applications like text-to-image generation and textto-video generation. We conduct a comprehensive
--- EXPERIMENT ---
al evaluation of our approach, employing Stable Diffusion [29], DreamBooth [30], ReVersion [15], ModelScope [23], and Rerender [39] as our foundational models for benchmark comparisons. By employing FreeU during the inference phase, these models indicate a discernible enhancement in the quality of generated outputs. The visualization illustrated in Fig. 1 substantiates the efficacy of FreeU in significantly enhancing both intricate details and overall visual fidelity within the generated images. Our contributions are summarized as follows: ¢ We investigate and uncover the potential of U-Net architectures for denoising within diffusion models and identify that its main backbone primarily contributes to denoising, whereas its skip connections introduce highfrequency features into the decoder module. ¢ We further introduce a simple yet effective method, denoted as “FreeU”, which enhances U-Net’s denoising capability by leveraging the strengths of both components of the U-Net architecture. It substantially improves the generation quality without requiring additional training or fine-tuning. ¢ The proposed FreeU framework is versatile and seamlessly integrates with existing diffusion models. We demonstrate significant sample quality improvement across various diffusion-based methods, showing the effectiveness of FreeU at no extra cost. 2. Methodology 2.1. Preliminaries Diffusion models such as Denoising Diffusion Probabilistic Models (DDPM) [12], encompass two fundamental processes for data modeling: a diffusion process and a denoising process. The diffusion process is characterized by a sequence of T steps. At each step t, Gaussian noise is incrementally introduced into the data distribution a ~ g(a) via a Markov chain, following a prescribed variance sched ule denoted as 6,,..., 8r: q(e\@1-1) = N (ae; V1 — Bewe-1, B:T) (65) The denoising process reverses the above diffusion process to the underlying clean data a;_; given the noisy input x;: Po(@1—-1|a) = N (ar—1; Mo (r,t), Bo(ar,t)) (2) The jt, and Xo determined through estimation procedures involving a denoising model denoted as eg. Typically, this denoising model is implemented using a time-conditional U-Net architecture. It is trained to eliminate noise from data samples while concurrently enhancing the overall fidelity of the generated samples. 2.2. How does diffusion U-Net perform denoising? Building upon the notable disparities observed between low-frequency and high-frequency components throughout the denoising process illustrated in Fig. 2 and Fig. 3, we extend our investigation to delineate the specific contributions of the U-Net architecture within the denoising process, to explore the internal properties of the denoising network. As depicted in Fig. 4, the U-Net architecture comprises a primary backbone network, encompassing both an encoder and a decoder, as well as the skip connections that facilitate information transfer between corresponding layers of the encoder and decoder. The backbone of U-Net. To evaluate the salient characteristics of the backbone and lateral skip connections in the denoising process, we conduct a controlled experiment wherein we introduce two multiplicative scaling factors—denoted as b and s—to modulate the feature maps generated by the backbone and skip connections, respectively, prior to their concatenation. As shown in Fig. 5, it is evident that elevating the scale factor b of the backbone distinctly enhances the quality of generated images. Conversely, variations in the scaling factor s, which modulates the impact of the lateral skip connections, appear to exert a negligible influence on the quality of the generated images. Building upon these observations, we subsequently probed the underlying mechanisms that account for the enhancement in image generation quality when the scaling factor b associated with the backbone feature maps is augmented. Our analysis reveals that this quality improvement is fundamentally linked to an amplified denoising capability imparted by the U-Net architecture’s backbone. As delineated in Fig. 6, a commensurate increase in b correspondingly results in a suppression of high-frequency components in the images generated by the diffusion model. This implies that enhancing backbone features effectively bolsters the denoising capability of the U-Net architecture, thereby contributing to a superior output in terms of both fidelity and detail preservation. --- --_ skip connections stp. backbone Ce) ‘features ~~ unet Architecture UNet Architecture FFT | IFFT skip connection | skip features (h) backbone features (x) b (b) FreeU Operations Figure 4. FreeU Framework. (a) U-Net Skip Features and Backbone Features. In U-Net, the skip features and backbone features are concatenated together at each decoding stage. We apply the FreeU operations during concatenation. (b) FreeU Operations. The factor b aims to amplify the backbone feature map a, while factor s is designed to attenuate the skip feature map h. b=1.0, s=0.b=1.0, s=0.8 b=1.0, s=1.0 b=1.0, s=1.b=1.0, s=1.A Log amplitude 0.0n O0.2n 0.4n 0.6n 0.8n 1.0n Frequency Figure 5. Effect of backbone and skip connection scaling factors (b and s). Figure 6. Relative log amplitudes of Fourier with Increasing the backbone scaling factor b significantly enhances image quality, while Variations of the backbone scaling factor b. Increasvariations in the skip scaling factor s have a negligible influence on image synthesis ing in 6 correspondingly results in a suppression of quality. The skip connections of U-Net. Conversely, the skip connections serve to forward features from the earlier layers of encoder blocks directly to the decoder. Intriguingly, as evidenced in Fig. 7, these features primarily constitute high-frequency information. Our conjecture, grounded in this observation, posits that during the training of the U-Net architecture, the presence of these high-frequency features may inadvertently expedite the convergence toward noise prediction within the decoder module. Furthermore, the limited impact of modulating skip features in Fig. 5 also indicates that the skip features predominantly contribute to the decoder’s information. This phenomenon, in turn, high-frequency components in the images generated by the diffusion model. could result in an unintended attenuation of the efficacy of the backbone’s intrinsic denoising capabilities during inference. Thereby, this observation prompts pertinent questions about the counterbalancing roles played by the backbone and the skip connections in the composite denoising performance of the U-Net framework. 2.3. Free lunch in diffusion U-Net Capitalizing on the above discovery, we propel forward with the introduction of simple yet effective method, denoted as “FreeU”, which effectively bolsters the denoising capability of the U-Net architecture by leveraging the strengths --- --0.aa backbone ~~ skip A Log amplitude -6.0 sy 0.0n O.2n O4n O6n O.8n 1.0n Frequency Figure 7. Fourie relative log amplitudes of backbone, skip, and their fused feature maps. The features, forwarded by skip connections directly from earlier layers of the encoder block to the decoder contain a large amount of high-frequency information. mies. Generated image Feature map Generatedimage Feature map Figure 8. Visualization of the average feature maps from the second stage in the decoder. of both components of the U-Net architecture. It substantially improves the generation quality without requiring additional training or fine-tuning. Technically, for the /-th block of the U-Net decoder, let a, represent the backbone feature map from the main backbone at the preceding block, and let h; denote the feature map propagated through the corresponding skip connection. To modulate these feature maps, we introduce two scalar factors: a backbone feature scaling factor b; for a, and a yet-to-be-defined skip feature scaling factor s; for hy. Specifically, the factor b; aims to amplify the backbone feature map x, while factor s, is designed to attenuate the skip feature map hy. For the backbone features, we introduce a novel method known as structure-related scaling, which dynamically adjusts the scaling of backbone features for each sample. Unlike a fixed scaling factor applied uniformly to all samples or positions within the same channel, our approach adjusts the scaling factor adaptively based on the specific characteristics of the sample features. We first computer the average feature map along the channel dimension: 1 Cc x= oleh (3) where a; represents the i-th channel of the feature map a;. C denotes the total number of channels in x;. Subsequently, the backbone factor map is determined as follows: : Zi -— Min(#1) Ly (4) Mazx(%) — Min() °° where a represents the backbone factor map. 0; is a scalar constant. Then, upon experimental investigation, we discern that indiscriminately amplifying all channels of a, through multiplication with a; engenders an oversmoothed texture in the resulting synthesized images. The reason is the enhanced U-Net compromises the image’s high-frequency details while denoising. Consequently, we confine the scaling operation to the half channels of a; as follows: , @i,Qa,, ifi< C/ri ay, = (bi — 1) : (5) Li, otherwise Indeed, as illustrated in Fig. 8, the average feature map %; inherently contains valuable structural information. Consequently, the backbone factor map ay is instrumental in amplifying the backbone feature map 2 in a manner that aligns with its structural characteristics. This strategic approach serves to mitigate the issue of oversmoothing. Importantly, this strategy offers a dual benefit. Firstly, it enhances the denoising capabilities of the backbone feature map, allowing it to filter out noise more effectively. Secondly, it avoids the adverse effects associated with the indiscriminate application of scaling across the entire feature map, thereby achieving a more nuanced equilibrium between noise reduction and texture preservation. To further mitigate the issue of oversmoothed texture due to enhancing denoising, we further employ spectral modulation in the Fourier domain to selectively diminish lowfrequency components for the skip features. Mathematically, this operation is performed as follows: F (hii) = FFT(hii) (6) F' (hii) = F(hii) © Bij (7) hy; = IFFT(F'(hi3)) (8) where FFT(-) and IFFT(-) are Fourier transform and inverse Fourier transform. © denotes element-wise multiplication, and (3; ; is a Fourier mask, designed as a function of the magnitude of the Fourier coefficients, serving to implement the frequency-dependent scaling factor s;: 8. iff < Tthresh; i(r) =Bualr) {i otherwise. @) where r is the radius. Tihresh is the threshold frequency. Then, the augmented skip feature map hj is then concatenated with the modified backbone feature map a} for subsequent layers in the U-Net architecture, as shown in Fig. 4. --- --A cat riding a motorcycle A panda standing on a surfboard in the ocean A bridge is depicted in the water A teddy bear walking in the snowstorm A boy is playing pokemon Figure 9. Samples generated by Stable Diffusion [29] with or without FreeU. Remarkably, the proposed FreeU framework does not require any task-specific training or fine-tuning. Adding the backbone and skip scaling factors can be easily done with just a few lines of code. Essentially, the parameters of the architecture can be adaptively re-weighted during the inference phase, which allows for a more flexible and potent denoising operation without adding any computational burden. This makes FreeU a highly practical solution that can be seamlessly integrated into existing diffusion models to improve their performance. 3. Experiments 3.1. Implementation details To assess the effectiveness of the proposed FreeU, we systematically conduct a series of experiments, aligning our benchmarks with state-of-the-art methods such as Stable Diffusion [29], DreamBooth [30], ModelScope [23], and Rerender [39]. Importantly, our approach seamlessly integrates with these established methods without imposing any additional computational overhead associated with supplementary training or fine-tuning. We meticulously adhere to the prescribed settings of these methods and exclusively introduce the backbone feature factors and skip feature factors during the inference. 3.2. Text-to-image Stable Diffusion [29] is a latent text-to-image diffusion model renowned for its capability to generate photorealistic images based on textual input. It has consistently demonstrated exceptional performance in various image synthesis tasks. With the integration of our FreeU augmentation into Stable Diffusion, the results, as exemplified in Fig. 9, exhibit a notable enhancement in the model’s generative capacity. To elaborate, the incorporation of FreeU into Stable Diffusion [29] yields improvements in both entity portrayal and fine-grained details. For instance, when provided with the prompt “a blue car is being filmed”, FreeU refines the image, eliminating rooftop irregularities and enhancing the textural intricacies of the surrounding structures. In the case of “Mother rabbit is raising baby rabbits”, FreeU ensures that the generated image portrays a mother rabbit in a normal appearance caring for baby rabbits. Furthermore, In scenarios like “a attacks an upset cat and is then chased off” and “A teddy bear walking in the snowstorm”, FreeU helps generate more realistically posed cats and teddy bears. Impressively, in response to the complex prompt “A cat riding a motorcycle”, FreeU not only accurately renders the individual entities but also expertly captures the nu --- --SDXL SDXL + FreeU SDXL SDXL + FreeU SDXL + FreeU SDXL Figure 10. Samples generated by Stable Diffusion-XL [27] with or without FreeU. anced relationship between them, ensuring that the cat is actively engaged in riding. In Figure 10, we present the generated images based on the SDXL framework [27]. It becomes evident that our proposed FreeU consistently excels in generating realistic images, especially in detail generation. These compelling results serve as a testament to the substantial qualitative enhancements engendered by the synergy of FreeU with the SD[29] or SDXL[27] frameworks. Quantitative evaluation. We conduct a study with 35 participants to assess image quality and image-text alignment. Each participant receives a text prompt and two corresponding synthesized images, one from SD and another from SD+FreeU. To ensure fairness, we use the same randomly sampled random seed for generating both images. The image sequence is randomized to eliminate any bias. Participants then select the image they consider superior for image-text alignment and image quality, respectively. We tabulate the votes for SD and SD+FreeU in each category in Table |. Our analysis reveals that the majority of votes go to SD+FreeU, indicating that FreeU significantly enhances the Stable Diffusion text-to-image model in both evaluated aspects. 3.3. Text-to-video ModelScope [23], an avant-garde text-to-video diffusion model, stands at the forefront of video generation from textual descriptions. The infusion of our FreeU augmentation Table 1. Text-to-Image Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Image-Text refers to Image-Text Alignment. Image-Text Image Quality 14.66% 85.34% SD [29] SD+FreeU Table 2. Text-to-Video Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Video-Text refers to Video-Text Alignment. Method Video-Text | Video Quality ModelScope [23] 15.29% 14.33% ModelScope+FreeU 84.71% 85.67% into ModelScope [23] serves to further hone its video synthesis prowess, as substantiated by Fig. 11. For instance, when presented with the prompt “A cinematic view of the ocean, from a cave”, FreeU enables ModelScope [23] to generate the perspective “from a cave”, enriching the visual narrative. In the case of “A cartoon of an elephant walking”, ModelScope [23] initially generates an elephant with two trunks, but with the incorporation of FreeU, it rectifies this anomaly and produces a correct depiction of an elephant in motion. Moreover, in response to the prompt “An astronaut flying in space”, ModelScope [23], with the assistance of FreeU, can generate a clear and vivid portrayal of an astronaut floating in the expanse of outer space. --- --Aun astronaut flying in space. Figure 11. Samples generated by ModelScope [23] with or without FreeU. These results underscore the significant improvements achieved through the synergistic application of FreeU with ModelScope [23], resulting in high-quality generated content characterized by clear motion, rich detail, and semantic alignment. Quantitative evaluation. We conduct the quantitative evaluation for FreeU on the text-to-video task in a similar way as text-to-image. The results displayed in Table 2 indi cate that most participants prefer the video generated with FreeU. 3.4. Downstream tasks FreeU presents substantial enhancements in the quality of synthesized samples across various diffusion model applications. Our evaluations extend from foundational image and video synthesis models to more specialized downstream --- --DreamBooth DreamBooth + FreeU Input images Figure 12. Samples generated by DreamBooth [30] with or without FreeU. child <> child ts back-to-back with” dog <> basket <R> = “is contained inside of” "" Spiderman <R> basket <R> = “is contained inside of? cat <R> motorbike <iR> = “ride on” Figure 13. Samples generated by ReVersion [15] with or without FreeU. applications. We incorporate FreeU into Dreambooth [30], a diffusion model specialized in personalized text-to-image tasks. The enhancements are evident, as demonstrated in Fig. 12, the synthesized images present marked improvements in realism. For instance, while the base DreamBooth [30] model struggles to synthesize the appearance of the action figure’s legs from the prompt “a photo of action figure riding a motorcycle”, the FreeU-augmented version deftly overcomes this hurdle. Similarly, for the prompt “A toy on a beach”, the initial output exhibited body shape anomalies. FreeU’s integration refines these imperfections, providing a more accurate representation and improving color fidelity. We also integrate FreeU into ReVersion [15], a Stable Diffusion based relation inversion method, enhancing its quality as shown in Fig. 13. For example, when the relation “back to back” is to be expressed between two children, FreeU enhances ReVersion’s ability to accurately represent A dog wearing sunglasses Figure 14. Samples generated by Rerender [39] with or without FreeU. this relationship. For the “inside” relation, when a dog is supposed to be placed inside of a basket, ReVersion sometimes generates a dog with artifacts, and introducing FreeU helps eliminate these artifacts. While ReVersion effectively captures relational concepts, Stable Diffusion might occasionally struggle to synthesize the relation concept due to excessive high-frequency noises in the U-Net skip features. Adding FreeU allows better entity and relation synthesis quality by using exactly the same relation prompt learned by ReVersion. Furthermore, we evaluated FreeU’s impact on Rerender [39], a diffusion model tailored for zero-shot textguided video-to-video translations. Fig. 14 depicts the results: clear improvements in the detail and realism of synthesized videos. For instance, when provided with the prompt “A dog wearing sunglasses” and an input video, Rerender [39] initially produces a dog video with artifacts related to the “sunglasses”. However, the incorporation of FreeU successfully eliminates such artifacts, resulting in a refined output. In summation, these outcomes substantiate that the incorporation of FreeU leads to enhanced entity representation and synthesis quality, employing precisely the same learned prompt. 3.5. Ablation study Effects of FreeU. FreeU is introduced with the primary aim of enhancing the denoising capabilities of the U-Net architecture within the diffusion model. To assess the impact of FreeU, we conducted analytical experiments using Sta --- --os. 0s 05 os: OO a) a) aa) wo 9° —— FreeU wy 00 w 9 —— FreeU o 9° —— FreeU 8 g 3B05 20s B05 B3 a a€ -1.0 €-10 €-1.0 E -1.5 8 8 & 2 2 2Bas Bas 8.5 Bas qd a a qd -2.0 2.0 -2.0 -2.25 23 28On O.2n O4n O.6n 0.8m 1.00 On 0.2n O4n O0.6n 0.8m 1.0n On O.2n O04n O6n 0.8m 1.0n On 0.2n O4n 0.6m 0.8m 1.Frequency stepO Frequency stepFrequency step17 Frequency stepFigure 15. Fourier relative log amplitudes of Stable Diffusion [29] with or without FreeU within the denoising process. a teddy bear walking down the road in the sunset A synthwave style sunset above the reflecting water of the sea, digitalart Figure 17. The ablation study of backbone scaling factor and skip scaling factor. ble Diffusion [29] as the base framework. In Fig. 15, we present visualizations of the relative log amplitudes of the Fourier transform of Stable Diffusion [29], comparing cases with and without the incorporation of FreeU. These visualizations illustrate that FreeU exerts a discernible influence in reducing high-frequency information at each step of the denoising process, which indicates FreeU’s capacity to ef-(a) (b) (©) Figure 18. The ablation study of backbone scaling factor. (a) The generated images of SD. (b) The generated images of FreeU with a constant factor. (c) The generated images of FreeU with the structure-related scaling factor map. fectively denoising. Furthermore, we extended our analysis by visualizing the feature maps of the U-Net architecture. As shown in Fig. 16, we observe that the feature maps generated by FreeU contain more pronounced structural information. This observation aligns with the intended effect of FreeU, as it preserves intricate details while effectively removing noise, harmonizing with the denoising objectives of --- --the model. Effects of components in FreeU. We evaluate the effects of the proposed FreeU strategy, i.e. introducing backbone feature scaling factors and skip feature scaling factors to intricately balance the feature contributions from the UNet architecture’s primary backbone and skip connections. In Fig. 17, we present the results of our evaluations. In the case of SD+FreeU(b), where backbone scaling factors are integrated during inference, we observe a noticeable improvement in the generation of vivid details compared to SD [29] alone. For instance, when given the prompt “A fat rabbit wearing a purple robe walking through a fantasy landscape”, SD+FreeU(b) generates a more realistic rabbit with normal arms and ears, as opposed to SD [29]. However, it is imperative to note that while the inclusion of feature scaling factors yields significant improvements, it can occasionally lead to an undesirable oversmoothing of textures. To mitigate this issue, we introduce skip feature scaling factors, aiming to reduce low-frequency information and alleviate the problem of texture oversmoothing. As demonstrated in Fig. 17, the combination of both backbone and skip feature scaling factors in SD+FreeU(b & s) leads to the generation of more realistic images. For instance, in the prompt “A synthwave style sunset above the reflecting water of the sea, digital art”, the generated sunset sky in SD+FreeU(b & s) exhibits enhanced realism compared to SD+FreeU(b). This highlights the efficacy of the comprehensive FreeU strategy in balancing features and mitigating issues related to texture smoothing, ultimately resulting in more faithful and realistic image generation. Effects of backbone structure-related factor. We evaluate the effects of the proposed backbone scaling strategy, structure-related scaling, on the delicate balance between noise reduction and texture preservation. Illustrated in Figure 18, when compared to the results generated by SD [29], we observe a substantial enhancement in the image quality generated by FreeU when utilizing a constant scaling factor. However, it is pertinent to highlight that the utilization of a fixed scaling factor can engender adverse consequences, manifesting as pronounced oversmoothing of textures and undesirable color oversaturation. Conversely, FreeU with the structure-related scaling factor map employs an adaptive scaling approach, leveraging structural information to guide the assignment of the backbone factor map. Our observations indicate that FreeU with the structure-related scaling factor map effectively mitigates these issues and achieves significant improvements in generating vivid and intricate details. 4.
--- CONCLUSION ---
In this study, we introduce the elegantly simple yet highly effective approach, termed FreeU, which substantially enhances the sample quality of diffusion models without in-curring any additional computational costs. Motivated by the fundamental role played by both skip connections and backbone features in U-Net architectures, we conduct an in-depth analysis of their effects in diffusion U-Net. Our investigation reveals that the primary backbone primarily contributes to denoising, while the skip connections predominantly introduce high-frequency features into the decoder, potentially leading to a neglect of essential backbone semantics. To address this, we strategically re-weight the contributions originating from the U-Net’s skip connections and backbone feature maps. This re-weighting process capitalizes on the unique strengths of both U-Net components, resulting in a substantial improvement in sample quality across a wide range of text prompts and random seeds. Our proposed FreeU can be seamlessly integrated into various diffusion foundation models and their downstream tasks, offering a versatile means of enhancing sample quality. References 1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In CVPR, 2022.2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023.3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. | 4] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Conditioning method for denoising diffusion probabilistic models. In JCCV, 2021.5] Prafulla Dhariwal and Alexander Nichol. Diffusion models eat GANs on image synthesis. In NeurIPS, 2021. | 6] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. ImageBART: Bidirectional context with multinomial diffusion for autoregressive image synthesis. In NeurIPS, 2021.7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021.8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In JCLR, 2023.9] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022.Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity [(ll --- ---video generation with arbitrary lengths. arXiv:2211.13221, 2022. | Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1,Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and editing. In CVPR, 2023. | Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, and Ziwei Liu. ReVersion: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023. 3,Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In JCLR, 2018.Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020. arXiv preprint lero Karras, Miika Aittala, Samuli Laine, Erik Hirkénen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. | Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022. | Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv: 1312.6114, 2013. | Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. VideoFusion: Decomposed diffusion models for high-quality video generation. In CVPR, 2023. 1, 3, 6, 7,Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In JCLR, 2022.Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. | Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Miiller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 6, 7, 10,Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 3, 6,Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH, 2022.Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurlPS, 2017.Pei Wang, Yijun Li, and Nuno Vasconcelos. Rethinking and improving the robustness of image style transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 124-133, 2021.Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. arXiv preprint arXiv:2205.12952, 2022. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. arXiv preprint arXiv:2306.07954, 2023. 3, 6,David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023.
"	"--- ABSTRACT ---
이 논문에서 우리는 &quot;무료 점심&quot; 역할을 하는 확산 U-Net의 미개척 잠재력을 발견합니다. 이는 즉석에서 생성 품질을 크게 개선합니다. 우리는 처음에 U-Net 아키텍처가 노이즈 제거 프로세스에 미치는 주요 기여를 조사하고, 주요 백본이 주로 노이즈 제거에 기여하는 반면, 스킵 연결은 주로 디코더 모듈에 고주파 기능을 도입하여 네트워크가 백본 의미론을 간과하게 한다는 것을 확인합니다. 이 발견을 활용하여 추가 학습이나 미세 조정 없이도 생성 품질을 개선하는 간단하면서도 효과적인 방법인 &quot;FreeU&quot;를 제안합니다. 우리의 주요 통찰력은 U-Net의 스킵 연결과 백본 기능 맵에서 얻은 기여를 전략적으로 다시 가중치를 두어 U-Net 아키텍처의 두 구성 요소의 강점을 활용하는 것입니다. 이미지 및 비디오 생성 작업에서 유망한 결과는 FreeU가 기존 확산 모델(예: Stable Diffusion, DreamBooth, ModelScope, Rerender 및 ReVersion)에 쉽게 통합되어 몇 줄의 코드만으로 생성 품질을 개선할 수 있음을 보여줍니다. 추론 중에 두 개의 스케일링 요인만 조정하면 됩니다. 프로젝트 페이지: https://chenyangsi.top/FreeU/. 1.
--- METHOD ---
비용 없이 확산 모델 샘플 품질을 크게 개선합니다. 훈련이 없고, 추가 매개변수가 도입되지 않으며, 메모리나 샘플링 시간이 증가하지 않습니다. 초록 이 논문에서 우리는 &quot;무료 점심&quot; 역할을 하는 확산 U-Net의 미개척 잠재력을 발견합니다. 즉석에서 생성 품질을 크게 개선합니다. 우리는 처음에 U-Net 아키텍처가 노이즈 제거 프로세스에 미치는 주요 기여를 조사하고, 주요 백본이 주로 노이즈 제거에 기여하는 반면, 스킵 연결은 주로 디코더 모듈에 고주파 기능을 도입하여 네트워크가 백본 의미론을 간과하게 한다는 것을 확인합니다. 이 발견을 활용하여 추가 훈련이나 미세 조정 없이 생성 품질을 개선하는 간단하면서도 효과적인 방법인 &quot;FreeU&quot;를 제안합니다. 우리의 주요 통찰력은 U-Net의 스킵 연결과 백본 기능 맵에서 얻은 기여를 전략적으로 다시 가중치를 두어 U-Net 아키텍처의 두 구성 요소의 강점을 활용하는 것입니다. 이미지 및 비디오 생성 작업에서 유망한 결과는 FreeU가 기존 확산 모델(예: Stable Diffusion, DreamBooth, ModelScope, Rerender 및 ReVersion)에 쉽게 통합되어 몇 줄의 코드만으로 생성 품질을 개선할 수 있음을 보여줍니다. 추론 중에 두 개의 스케일링 요인을 조정하기만 하면 됩니다. 프로젝트 페이지: https://chenyangsi.top/FreeU/. 1. 소개 생성 모델의 최첨단 범주인 확산 확률적 모델은 특히 컴퓨터 비전과 관련된 작업에서 연구 분야의 초점이 되었습니다[5, 6, 8, 10, 12, 20, 22, 26, 28, 29, 32]. 변분 자동 인코더(VAE) [21], 생성적 적대 네트워크(GAN) [3, 9, 16-19, 25] 및 벡터 양자화 접근 방식 [7, 34]과 같은 다른 생성 모델 클래스 [3, 7, 9, 16–19, 21, 25, 34, 35]와 달리 확산 모델은 새로운 생성 패러다임을 도입합니다. 이러한 모델은 고정 마르코프 체인을 사용하여 잠재 공간을 매핑하여 데이터 세트 내의 잠재 구조적 복잡성을 포착하는 복잡한 매핑을 용이하게 합니다. 최근, 높은 수준의 세부 정보부터 생성된 예제의 다양성에 이르기까지 인상적인 생성 기능은 이미지 합성[12, 29, 32], 이미지 편집[1, 4, 14, 24], 이미지 대 이미지 변환[4, 31, 36], 텍스트 대 비디오 생성[2, 11, 13, 23, 33, 37, 38, 40]과 같은 다양한 컴퓨터 비전 애플리케이션에서 획기적인 발전을 이루었습니다. 확산 모델은 확산 프로세스와 잡음 제거 프로세스로 구성됩니다. 확산 프로세스 동안 가우시안 잡음이 입력 데이터에 점차적으로 추가되고 결국 거의 순수한 가우시안 잡음으로 손상됩니다. 잡음 제거 프로세스 동안 원래 입력 데이터버거를 먹는 다람쥐 0.StepGenerated image 저주파 A 로그 진폭 0.-0.-1.0-1.StepStepStepStepStepStepStep-2.Ο.Οπ 0.2π 0.4π 0.6π 0.8Ο 1.Οπ 고주파 주파수 그림 2. 노이즈 제거 프로세스. 맨 위 행은 반복에 걸친 이미지의 진행 상황을 보여 주고, 그 뒤의 두 행은 백본 스케일링 계수 b의 진행 상황을 보여 줍니다. b에서 역 푸리에 변환 후에 저주파 및 고주파 성분을 증가시키면 고주파가 억제되어 각 단계와 일치합니다. 저주파 성분은 디노이즈 프로세스 중에 difslowly에 의해 생성된 이미지에서 주파수 성분을 변경하는 반면, 고주파 성분은 더 큰 변화를 보입니다. 일반적으로 U-Net은 각 노이즈 제거 단계에서 제거할 노이즈를 반복적으로 예측하도록 훈련됩니다. 기존 연구는 다운스트림 애플리케이션에 사전 훈련된 확산 U-Net을 활용하는 데 중점을 두고 있는 반면, 확산 U-Net의 내부 속성은 여전히 크게 탐구되지 않았습니다. 이 논문에서는 확산 모델의 적용을 넘어 노이즈 제거 프로세스에 대한 확산 U-Net의 효과를 조사하는 데 관심이 있습니다. 노이즈 제거 프로세스를 더 잘 이해하기 위해 먼저 확산 모델의 생성된 프로세스를 전망하기 위해 푸리에 영역으로의 패러다임 전환을 제시합니다. 이 연구 분야는 사전 조사가 제한적이었습니다. 그림 2에서 볼 수 있듯이 가장 위 행은 점진적 노이즈 제거 프로세스를 제공하여 연속적인 반복에서 생성된 이미지를 보여줍니다. 그 뒤의 두 행은 각 단계에 맞춰 역 푸리에 변환 후 연관된 저주파 및 고주파 공간 영역 정보를 보여줍니다. 그림 2에서 알 수 있듯이 저주파 성분은 점진적으로 변조되어 변화 속도가 둔화되고, 고주파 성분은 노이즈 제거 과정 전반에 걸쳐 더욱 두드러진 역동성을 보입니다. 이러한 결과는 그림 3에서 더욱 입증됩니다. 이는 직관적으로 설명할 수 있습니다. 1) 저주파 성분은 본질적으로 이미지의 글로벌 구조와 특성을 구현하여 글로벌 레이아웃과 부드러운 색상을 포함합니다. 이러한 성분은 이미지의 본질과 표현을 구성하는 기본적인 글로벌 요소를 캡슐화합니다. 노이즈 제거 과정에서 빠른 변화는 일반적으로 비합리적입니다. 이러한 성분을 급격하게 변경하면 이미지의 본질이 근본적으로 바뀔 수 있으며, 이는 일반적으로 노이즈 제거 과정의 목표와 양립할 수 없는 결과입니다. 2) 반대로 고주파 성분은 가장자리와 질감과 같은 이미지의 빠른 변화를 포함합니다. 이러한 미세한 세부 사항은 노이즈에 현저히 민감하여 노이즈가 이미지에 도입되면 종종 무작위 고주파 정보로 나타납니다. 결과적으로, 노이즈 제거 프로세스는 필수적인 복잡한 세부 사항을 유지하면서 노이즈를 제거해야 합니다. 노이즈 제거 프로세스 동안 저주파와 고주파 구성 요소 간의 이러한 관찰에 비추어, 우리는 확산 프레임워크 내에서 U-Net 아키텍처의 특정 기여를 확인하기 위해 조사를 확대합니다. U-Net 디코더의 각 단계에서, 스킵 연결의 스킵 피처와 백본 피처가 함께 연결됩니다. 우리의 조사에 따르면 U-Net의 주요 백본은 주로 노이즈 제거에 기여합니다. 반대로, 스킵 연결은 디코더 모듈에 고주파 피처를 도입하는 것으로 관찰됩니다. 이러한 연결은 세분화된 의미 정보를 전파하여 입력 데이터를 복구하기 쉽게 만듭니다. 그러나 이 전파의 의도치 않은 결과는 추론 단계 동안 백본의 고유한 노이즈 제거 기능이 약화될 가능성이 있다는 것입니다. 이는 그림 1의 첫 번째 행에 나와 있는 것처럼 비정상적인 이미지 세부 정보가 생성될 수 있습니다. 이러한 발견을 바탕으로 &quot;FreeU&quot;라는 새로운 전략을 도입하여 추가적인 훈련이나 미세 조정의 계산 오버헤드 없이도 샘플 품질을 개선할 수 있는 잠재력을 가지고 있습니다. 추론 단계에서는 U-Net 아키텍처의 기본 백본과 스킵 연결에서 피처 기여도를 균형 잡도록 설계된 두 가지 특수 변조 요소를 인스턴스화합니다. 백본 피처 요소라고 하는 첫 번째 요소는 기본 백본의 피처 맵을 증폭하여 노이즈 제거 프로세스를 강화하는 것을 목표로 합니다. 그러나 백본 피처 스케일링 요소를 포함하면 상당한 개선이 이루어지지만 가끔은 텍스처의 바람직하지 않은 과도한 평활화가 발생할 수 있습니다. 이 문제를 완화하기 위해 두 번째 요소인 스킵 피처 스케일링 요소를 도입하여 텍스처 과도한 평활화 문제를 완화하는 것을 목표로 합니다. FreeU 프레임워크는 기존 확산 모델과 통합될 때 원활한 적응성을 보여주며, 텍스트-이미지 생성 및 텍스트-비디오 생성과 같은 애플리케이션을 포함합니다. 우리는 포괄적인
--- EXPERIMENT ---
영어: 우리의 접근 방식에 대한 모든 평가는 벤치마크 비교를 위한 우리의 기초 모델로 Stable Diffusion [29], DreamBooth [30], ReVersion [15], ModelScope [23], 및 Rerender [39]를 사용합니다. 추론 단계에서 FreeU를 사용함으로써 이러한 모델은 생성된 출력의 품질이 눈에 띄게 향상되었음을 나타냅니다. 그림 1에 설명된 시각화는 생성된 이미지 내에서 복잡한 세부 사항과 전반적인 시각적 충실도를 모두 크게 향상시키는 FreeU의 효능을 입증합니다. 우리의 기여는 다음과 같이 요약됩니다. • 우리는 확산 모델 내에서 노이즈 제거를 위한 U-Net 아키텍처의 잠재력을 조사하고 발견하며, 그 주요 백본은 주로 노이즈 제거에 기여하는 반면, 건너뛰기 연결은 디코더 모듈에 고주파 기능을 도입한다는 것을 식별합니다. • 우리는 또한 U-Net 아키텍처의 두 구성 요소의 장점을 활용하여 U-Net의 노이즈 제거 기능을 향상시키는 &quot;FreeU&quot;로 표시되는 간단하면서도 효과적인 방법을 소개합니다. 추가 교육이나 미세 조정 없이도 생성 품질을 크게 개선합니다.• 제안된 FreeU 프레임워크는 다재다능하고 기존 확산 모델과 완벽하게 통합됩니다.우리는 다양한 확산 기반 방법에서 상당한 샘플 품질 개선을 보여주며 추가 비용 없이 FreeU의 효과를 보여줍니다.2. 방법론 2.1. 예비 Denoising Diffusion Probabilistic Models(DDPM)[12]와 같은 확산 모델은 데이터 모델링을 위한 두 가지 기본 프로세스, 즉 확산 프로세스와 노이즈 제거 프로세스를 포함합니다.확산 프로세스는 T 단계의 시퀀스로 특징지어집니다. 각 단계 t에서 가우시안 노이즈가 마르코프 체인을 통해 데이터 분포 xo~ q(x0)에 점진적으로 도입되며, 이는 ẞ₁,..., BT로 표시된 규정된 분산 일정에 따릅니다. q(xt|xt−1) = N(xt; √√1 – ßtxt−1, ßtI) (1) 노이즈 제거 프로세스는 노이즈 입력 x+가 주어진 기본 클린 데이터 xt-1에 대한 위의 확산 프로세스를 역전합니다. (2) Po(xt-1\xt) = N(xt−1; µ₁(xt, t), Σo(xt, t)) μ 및 Σ는 €0로 표시된 노이즈 제거 모델을 포함하는 추정 절차를 통해 결정됩니다. 일반적으로 이 노이즈 제거 모델은 시간 조건부 U-Net 아키텍처를 사용하여 구현됩니다. 생성된 샘플의 전반적인 충실도를 동시에 향상시키는 동시에 데이터 샘플에서 노이즈를 제거하도록 훈련됩니다. 2.2. 확산 U-Net은 어떻게 노이즈 제거를 수행할까요?그림 2와 그림 3에 나와 있는 노이즈 제거 프로세스 전반에서 저주파와 고주파 성분 간에 관찰된 눈에 띄는 차이점을 바탕으로, 우리는 노이즈 제거 프로세스 내에서 U-Net 아키텍처의 구체적인 기여를 구분하고 노이즈 제거 네트워크의 내부 속성을 탐구하기 위해 조사를 확장합니다.그림 4에서 볼 수 있듯이, U-Net 아키텍처는 인코더와 디코더를 모두 포함하는 기본 백본 네트워크와 인코더와 디코더의 해당 계층 간의 정보 전송을 용이하게 하는 스킵 연결을 포함합니다.U-Net의 백본.노이즈 제거 프로세스에서 백본과 측면 스킵 연결의 두드러진 특성을 평가하기 위해, 우리는 b와 s로 표시된 두 개의 곱셈적 스케일링 인수를 도입하여 연결 전에 백본과 스킵 연결에서 생성된 피처 맵을 변조하는 통제된 실험을 수행합니다.그림 5에서 볼 수 있듯이, 백본의 스케일 인수 b를 높이면 생성된 이미지의 품질이 현저히 향상되는 것이 분명합니다. 반대로, 측면 건너뛰기 연결의 영향을 조절하는 스케일링 계수의 변화는 생성된 이미지의 품질에 무시할 만한 영향을 미치는 것으로 보입니다. 이러한 관찰을 바탕으로, 우리는 백본 피처 맵과 관련된 스케일링 계수 b가 증가할 때 이미지 생성 품질이 향상되는 근본적인 메커니즘을 조사했습니다. 우리의 분석에 따르면 이러한 품질 향상은 근본적으로 U-Net 아키텍처의 백본에서 부여된 증폭된 노이즈 제거 기능과 연결되어 있습니다. 그림 6에서 설명한 대로, b가 비례적으로 증가하면 확산 모델에서 생성된 이미지의 고주파 성분이 억제됩니다. 이는 백본 피처를 향상시키면 U-Net 아키텍처의 노이즈 제거 기능이 효과적으로 강화되어 충실도와 세부 정보 보존 측면에서 더 우수한 출력에 기여함을 의미합니다.연결 건너뛰기 백본 피처 건너뛰기 피처 -|H|IHI 피처 건너뛰기(h) FFT IFFT 연결 건너뛰기 백본 피처(x) b (a) UNet 아키텍처 (b) FreeU 연산 그림 4. FreeU 프레임워크.(a) U-Net 피처 건너뛰기 및 백본 피처.U-Net에서 건너뛰기 피처와 백본 피처는 각 디코딩 단계에서 함께 연결됩니다.연결 중에 FreeU 연산을 적용합니다.(b) FreeU 연산.인자 b는 백본 피처 맵 x를 증폭하는 것을 목표로 하는 반면, 인자는 건너뛰기 피처 맵 h를 약화하도록 설계되었습니다. b=0.6, s=1.b=0.8, s 1.b=1.0, s=1.b=1.2, s=1.b=1.4, s=1.b=1.0, s=0.b=1.0, s=0.b=1.0, s=1.b=1.0, s=1.b=1.0, s=1.b=1.0, s=1.A 로그 진폭 0.-0.-1.-1.-1.-2.0.0.1.1.1.Ο.Οπ 0.2π 0.4π 0.6π 0.8π 1.Οπ 주파수 그림 5. 백본 및 스킵 연결 스케일링 계수(b 및 s)의 효과. 그림 6. 백본 스케일링 계수 b를 증가시키면서 푸리에의 상대적 로그 진폭은 이미지 품질을 크게 향상시키지만, 백본 스케일링 계수 b의 변화는 이미지 품질을 크게 향상시킵니다. 건너뛰기 스케일링 계수 s의 증가는 이미지 합성에 무시할 만한 영향을 미치며, b에서 ing은 확산 모델에서 생성된 이미지의 고주파 성분의 품질을 억제하는 결과를 낳습니다. U-Net의 건너뛰기 연결. 반대로, 건너뛰기 연결은 인코더 블록의 이전 계층에서 디코더로 직접 기능을 전달하는 역할을 합니다. 흥미롭게도, 그림 7에서 알 수 있듯이 이러한 기능은 주로 고주파 정보를 구성합니다. 이 관찰에 근거한 우리의 추측은 U-Net 아키텍처를 학습하는 동안 이러한 고주파 기능의 존재가 디코더 모듈 내에서 노이즈 예측으로의 수렴을 의도치 않게 촉진할 수 있다고 가정합니다. 또한, 그림 5에서 건너뛰기 기능을 변조하는 제한적인 영향은 건너뛰기 기능이 주로 디코더의 정보에 기여한다는 것을 나타냅니다. 이 현상은 추론 중에 백본의 고유한 노이즈 제거 기능의 효능을 의도치 않게 약화시킬 수 있습니다. 따라서 이 관찰은 U-Net 프레임워크의 복합 노이즈 제거 성능에서 백본과 스킵 연결이 수행하는 균형 잡힌 역할에 대한 적절한 질문을 촉발합니다.2.3. 확산 U-Net에서의 무상 점심 위의 발견을 활용하여 &quot;FreeU&quot;로 표시되는 간단하면서도 효과적인 방법을 소개하여 U-Net 아키텍처의 노이즈 제거 기능을 효과적으로 강화합니다.A 로그 진폭 백본 여기서 x1은 피처 맵 xɩ의 i번째 채널을 나타냅니다.C는 xɩ의 총 채널 수를 나타냅니다.따라서 백본 인수 맵은 다음과 같이 결정됩니다.0.-2.스킵 융합 -4.-6.Ο.Οπ 0.2π 0.4π 0.6π 0.8П 주파수 1.Οπ 그림 7. 백본, 스킵 및 융합된 피처 맵의 푸리 상대 로그 진폭. 인코더 블록의 이전 계층에서 디코더로 스킵 연결을 통해 직접 전달된 피처에는 대량의 고주파 정보가 포함됩니다.생성된 이미지 피처 맵 생성된 이미지 피처 맵 그림 8. 디코더의 두 번째 단계에서 평균 피처 맵의 시각화.U-Net 아키텍처의 두 구성 요소의 추가 학습이나 미세 조정 없이도 생성 품질을 크게 개선합니다.기술적으로 U-Net 디코더의 1번째 블록의 경우 x1은 이전 블록의 주 백본에서 백본 피처 맵을 나타내고 hɩ는 해당 스킵 연결을 통해 전파된 피처 맵을 나타냅니다.이러한 피처 맵을 변조하기 위해 두 개의 스칼라 인수를 도입합니다.xɩ의 경우 백본 피처 스케일링 인수 b₁와 hɩ의 아직 정의되지 않은 스킵 피처 스케일링 인수 sɩ입니다.특히 인수 by는 백본 피처 맵 xɩ를 증폭하는 것을 목표로 하는 반면 인수 sɩ는 스킵 피처 맵 hɩ를 감쇠하도록 설계되었습니다. 백본 피처의 경우, 구조 관련 스케일링이라는 새로운 방법을 도입합니다.이 방법은 각 샘플에 대한 백본 피처의 스케일링을 동적으로 조정합니다.동일한 채널 내의 모든 샘플이나 위치에 균일하게 적용되는 고정 스케일링 인수와 달리, 우리의 접근 방식은 샘플 피처의 특정 특성에 따라 스케일링 인수를 적응적으로 조정합니다.먼저 채널 차원을 따라 평균 피처 맵을 계산합니다.C (3) xl,i, α = (b₁ — 1) .x₁ - Min(x1) Max(1) Min(xi) +1, (4) 여기서 a는 백본 인수 맵을 나타냅니다.bɩ는 스칼라 상수입니다.그런 다음 실험적 조사를 통해 a와 곱하여 x의 모든 채널을 무차별적으로 증폭하면 합성된 결과 이미지에 지나치게 매끈한 질감이 발생한다는 것을 알아냈습니다.그 이유는 향상된 U-Net이 노이즈를 제거하는 동안 이미지의 고주파 세부 정보를 손상시키기 때문입니다. 따라서 스케일링 작업을 다음과 같이 xɩ의 반 채널로 제한합니다. x1.i x1,i xl,i, Oa, if i &lt; C/otherwise (5) 실제로 그림 8에서 설명한 대로 평균 피처 맵 ɩ에는 본질적으로 귀중한 구조적 정보가 포함되어 있습니다. 결과적으로 백본 인자 맵 oɩ는 백본 피처 맵 xɩ를 구조적 특성과 일치하는 방식으로 증폭하는 데 도움이 됩니다. 이 전략적 접근 방식은 과도한 평활화 문제를 완화하는 데 도움이 됩니다. 중요한 점은 이 전략이 이중의 이점을 제공한다는 것입니다. 첫째, 백본 피처 맵의 노이즈 제거 기능을 향상시켜 노이즈를 보다 효과적으로 필터링할 수 있습니다. 둘째, 전체 피처 맵에 무차별적으로 스케일링을 적용하는 것과 관련된 부정적인 영향을 피하여 노이즈 감소와 텍스처 보존 간에 보다 미묘한 균형을 이룹니다. 노이즈 제거를 강화하여 과도하게 매끈해진 텍스처 문제를 완화하기 위해, 푸리에 영역에서 스펙트럼 변조를 추가로 사용하여 건너뛰기 피처에 대한 저주파 성분을 선택적으로 감소시킵니다. 수학적으로 이 연산은 다음과 같이 수행됩니다. F(hi) = FFT(hı,i) F&#39; (hi) = F(hı,i) © ẞı,i (6) (7) (8) h₁ = IFFT(F&#39; (hı,i)) i 여기서 FFT() 및 IFFT(·)는 푸리에 변환과 역푸리에 변환입니다. 는 요소별 곱셈을 나타내고, B는 푸리에 계수의 크기의 함수로 설계된 푸리에 마스크로, 주파수 종속 스케일링 인자 sɩ를 구현하는 데 사용됩니다. B₁₁i (r)ifr <thresh, otherwise. (9) where r is the radius. thresh is the threshold frequency. Then, the augmented skip feature map hy is then concatenated with the modified backbone feature map x for subsequent layers in the U-Net architecture, as shown in Fig. 4.SD SD + FreeU SD SD + FreeU SD SD + FreeU a blue car is being filmed Mother rabbit is raising baby rabbits A bridge is depicted in the water a baby in a red shirt a attacks an upset cat and is then chased off A teddy bear walking in the snowstorm A cat riding a motorcycle. | A panda standing on a surfboard in the ocean | A boy is playing pokemon Figure 9. Samples generated by Stable Diffusion [29] with or without FreeU. Remarkably, the proposed FreeU framework does not require any task-specific training or fine-tuning. Adding the backbone and skip scaling factors can be easily done with just a few lines of code. Essentially, the parameters of the architecture can be adaptively re-weighted during the inference phase, which allows for a more flexible and potent denoising operation without adding any computational burden. This makes FreeU a highly practical solution that can be seamlessly integrated into existing diffusion models to improve their performance. 3. Experiments 3.1. Implementation details To assess the effectiveness of the proposed FreeU, we systematically conduct a series of experiments, aligning our benchmarks with state-of-the-art methods such as Stable Diffusion [29], DreamBooth [30], ModelScope [23], and Rerender [39]. Importantly, our approach seamlessly integrates with these established methods without imposing any additional computational overhead associated with supplementary training or fine-tuning. We meticulously adhere to the prescribed settings of these methods and exclusively introduce the backbone feature factors and skip feature factors during the inference. 3.2. Text-to-image Stable Diffusion [29] is a latent text-to-image diffusion model renowned for its capability to generate photorealistic images based on textual input. It has consistently demonstrated exceptional performance in various image synthesis tasks. With the integration of our FreeU augmentation into Stable Diffusion, the results, as exemplified in Fig. 9, exhibit a notable enhancement in the model's generative capacity. To elaborate, the incorporation of FreeU into Stable Diffusion [29] yields improvements in both entity portrayal and fine-grained details. For instance, when provided with the prompt ""a blue car is being filmed"", FreeU refines the image, eliminating rooftop irregularities and enhancing the textural intricacies of the surrounding structures. In the case of ""Mother rabbit is raising baby rabbits"", FreeU ensures that the generated image portrays a mother rabbit in a normal appearance caring for baby rabbits. Furthermore, In scenarios like ""a attacks an upset cat and is then chased off"" and ""A teddy bear walking in the snowstorm"", FreeU helps generate more realistically posed cats and teddy bears. Impressively, in response to the complex prompt ""A cat riding a motorcycle"", FreeU not only accurately renders the individual entities but also expertly captures the nu-SDXL SDXL+ FreeU SDXL SDXL + FreeU SDXL SDXL+FreeU Figure 10. Samples generated by Stable Diffusion-XL [27] with or without FreeU. anced relationship between them, ensuring that the cat is actively engaged in riding. In Figure 10, we present the generated images based on the SDXL framework [27]. It becomes evident that our proposed FreeU consistently excels in generating realistic images, especially in detail generation. These compelling results serve as a testament to the substantial qualitative enhancements engendered by the synergy of FreeU with the SD[29] or SDXL[27] frameworks. parQuantitative evaluation. We conduct a study withticipants to assess image quality and image-text alignment. Each participant receives a text prompt and two corresponding synthesized images, one from SD and another from SD+FreeU. To ensure fairness, we use the same randomly sampled random seed for generating both images. The image sequence is randomized to eliminate any bias. Participants then select the image they consider superior for image-text alignment and image quality, respectively. We tabulate the votes for SD and SD+FreeU in each category in Table 1. Our analysis reveals that the majority of votes go to SD+FreeU, indicating that FreeU significantly enhances the Stable Diffusion text-to-image model in both evaluated aspects. 3.3. Text-to-video ModelScope [23], an avant-garde text-to-video diffusion model, stands at the forefront of video generation from textual descriptions. The infusion of our FreeU augmentation Table 1. Text-to-Image Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Image-Text refers to Image-Text Alignment. Method SD [29] SD+FreeU Image-Text 14.12% 85.88% Image Quality 14.66% 85.34% Table 2. Text-to-Video Quantitative Results. We count the percentage of votes for the baseline and our method respectively. Video-Text refers to Video-Text Alignment. Method ModelScope [23] ModelScope+FreeU Video-Text 15.29% 84.71% Video Quality 14.33% 85.67% into ModelScope [23] serves to further hone its video synthesis prowess, as substantiated by Fig. 11. For instance, when presented with the prompt ""A cinematic view of the ocean, from a cave"", FreeU enables ModelScope [23] to generate the perspective “from a cave”, enriching the visual narrative. In the case of ""A cartoon of an elephant walking"", ModelScope [23] initially generates an elephant with two trunks, but with the incorporation of FreeU, it rectifies this anomaly and produces a correct depiction of an elephant in motion. Moreover, in response to the prompt ""An astronaut flying in space"", ModelScope [23], with the assistance of FreeU, can generate a clear and vivid portrayal of an astronaut floating in the expanse of outer space.ModelScope ModelScope she sh Folterstock shutterstock nuit Stock A cinematic view of the ocean, from a cave. A cartoon of an elephant walking. ModelScope shutte Shutters MICH UNITY shutterst An astronaut flying in space. Figure 11. Samples generated by ModelScope [23] with or without FreeU. These results underscore the significant improvements achieved through the synergistic application of FreeU with ModelScope [23], resulting in high-quality generated content characterized by clear motion, rich detail, and semantic alignment. Quantitative evaluation. We conduct the quantitative evaluation for FreeU on the text-to-video task in a similar way as text-to-image. The results displayed in Table 2 indicate that most participants prefer the video generated with FreeU. 3.4. Downstream tasks FreeU presents substantial enhancements in the quality of synthesized samples across various diffusion model applications. Our evaluations extend from foundational image and video synthesis models to more specialized downstreamDreamBooth DreamBooth + FreeU Input images M a photo of action figure riding a motorcycle A toy on a beach Figure 12. Samples generated by DreamBooth [30] with or without FreeU. ReVersion ReVersion+FreeU ReVersion ReVersion+FreeU Rerender Rerender+FreeU A dog wearing sunglasses Figure 14. Samples generated by Rerender [39] with or without FreeU. child <R>어린이<R> = &quot;개와 등을 맞대고 앉다&quot;<R> 바구니<R> = &quot;스파이더맨 안에 들어있습니다&quot;<R> 바구니<R> = &quot;고양이 안에 포함되어 있습니다&quot;<R> 오토바이<R> = &quot;ride on&quot; 그림 13. FreeU 응용 프로그램이 있거나 없는 ReVersion [15]에서 생성한 샘플. FreeU를 개인화된 텍스트-이미지 작업에 특화된 확산 모델인 Dreambooth [30]에 통합합니다. 그림 12에서 볼 수 있듯이 향상점은 분명하고 합성된 이미지는 사실감이 현저히 향상되었습니다. 예를 들어 기본 DreamBooth [30] 모델은 &quot;오토바이를 타는 액션 피규어 사진&quot; 프롬프트에서 액션 피규어의 다리 모양을 합성하는 데 어려움을 겪는 반면, FreeU로 증강된 버전은 이 장애물을 능숙하게 극복합니다. 마찬가지로 &quot;해변의 장난감&quot; 프롬프트의 경우 초기 출력은 체형 이상을 보였습니다. FreeU의 통합은 이러한 불완전성을 개선하여 더 정확한 표현을 제공하고 색상 충실도를 개선합니다. 또한 FreeU를 Stable Diffusion 기반 관계 역전 방법인 ReVersion [15]에 통합하여 그림 13에서 보듯이 품질을 향상시켰습니다. 예를 들어, 두 아이 사이에서 &quot;back to back&quot; 관계를 표현해야 할 때 FreeU는 ReVersion이 이 관계를 정확하게 표현하는 능력을 향상시킵니다. &quot;inside&quot; 관계의 경우, 개가 바구니 안에 놓여야 할 때 ReVersion은 때때로 아티팩트가 있는 개를 생성하고 FreeU를 도입하면 이러한 아티팩트를 제거하는 데 도움이 됩니다. ReVersion이 관계 개념을 효과적으로 포착하는 반면, Stable Diffusion은 때때로 U-Net 건너뛰기 피처의 과도한 고주파 노이즈로 인해 관계 개념을 합성하는 데 어려움을 겪을 수 있습니다. FreeU를 추가하면 ReVersion이 학습한 정확히 동일한 관계 프롬프트를 사용하여 엔티티 및 관계 합성 품질을 향상시킬 수 있습니다. 또한, 우리는 제로 샷 텍스트 가이드 비디오 대 비디오 변환에 맞게 조정된 확산 모델인 Rerender [39]에 대한 FreeU의 영향을 평가했습니다. 그림 14는 결과를 보여줍니다. 세부 정보와 사실성이 명확하게 향상되었습니다. 합성된 비디오. 예를 들어, &quot;선글라스를 쓴 개&quot;라는 프롬프트와 입력 비디오가 제공될 때 Rerender[39]는 처음에 &quot;선글라스&quot;와 관련된 아티팩트가 있는 개 비디오를 생성합니다. 그러나 FreeU를 통합하면 이러한 아티팩트가 성공적으로 제거되어 정제된 출력이 생성됩니다. 요약하면, 이러한 결과는 FreeU를 통합하면 정확히 동일한 학습된 프롬프트를 사용하여 향상된 엔터티 표현과 합성 품질이 제공된다는 것을 입증합니다. 3.5. 절제 연구 FreeU의 효과. FreeU는 확산 모델 내에서 U-Net 아키텍처의 노이즈 제거 기능을 향상시키는 주요 목적으로 도입되었습니다. FreeU의 영향을 평가하기 위해 Sta-A 로그 진폭 0.SD 0.FreeU -0.-1.00.SD 0.FreeU -1.5-2.A 로그 진폭 -0.-1.-1.-2.0-2.-2.-·8.On 0.2П 0.4П 0.6П 0.8П를 사용하여 분석 실험을 수행했습니다. 1.Οπ 주파수 단계A 로그 진폭 0.SD FreeU 0.-0.-1.-1.-2.A 로그 진폭 0.0.SD FreeU -0.-1.-1.-2.-2.8.On 0.2П 0.4π 0.6П 0.8П 1.0π -0.0П 0.2π 0.4π 0.6П 0.8П 1.Οπ -0.0П 0.2π 0.4П 0.6П 0.8П 주파수 단계주파수 단계주파수 단계그림 15. 잡음 제거 프로세스 내에서 FreeU가 있거나 없는 안정 확산[29]의 푸리에 상대 로그 진폭.SD SD SD SD SD + FreeU SD + FreeU SD + FreeU SD FreeU 그림 16. FreeU가 있거나 없는 안정 확산[29]에 대한 피처 맵의 시각화. SD SD+FreeU (b) SD+FreeU (b&amp;s) 보라색 가운을 입은 뚱뚱한 토끼가 판타지 풍경을 걷고 있다.노을 속을 길을 걷는 테디베어 바다에 반사되는 물 위로 보이는 신스웨이브 스타일의 일몰, 디지털 아트 그림 17. 백본 스케일링 계수와 스킵 스케일링 계수의 절제 연구.ble Diffusion [29]을 기본 프레임워크로 사용합니다.그림 15에서는 FreeU를 통합한 경우와 통합하지 않은 경우를 비교하여 Stable Diffusion [29]의 푸리에 변환의 상대적 로그 진폭을 시각화합니다.이러한 시각화는 FreeU가 노이즈 제거 프로세스의 각 단계에서 고주파 정보를 줄이는 데 눈에 띄는 영향을 미치는 것을 보여주며, 이는 FreeU의 ef(b) 용량을 나타냅니다.(c) 그림 18. 백본 스케일링 계수의 절제 연구.(a) SD의 생성된 이미지.(b) 상수 계수가 있는 FreeU의 생성된 이미지.(c) 구조 관련 스케일링 계수 맵이 있는 FreeU의 생성된 이미지. 효과적으로 노이즈를 제거합니다. 나아가, 우리는 U-Net 아키텍처의 피처 맵을 시각화하여 분석을 확장했습니다. 그림 16에서 볼 수 있듯이, FreeU에서 생성된 피처 맵에는 더 두드러진 구조적 정보가 포함되어 있음을 관찰했습니다. 이 관찰은 FreeU의 의도된 효과와 일치하며, 복잡한 세부 사항을 보존하는 동시에 효과적으로 노이즈를 제거하여 모델의 노이즈 제거 목표와 조화를 이룹니다. FreeU의 구성 요소 효과. 우리는 제안된 FreeU 전략의 효과를 평가합니다. 즉, 백본 피처 스케일링 계수와 스킵 피처 스케일링 계수를 도입하여 UNet 아키텍처의 기본 백본 및 스킵 연결에서 피처 기여도를 복잡하게 균형 잡습니다. 그림 17에서 평가 결과를 제시합니다. 추론 중에 백본 스케일링 계수가 통합되는 SD+FreeU(b)의 경우, SD[29] 단독에 비해 생생한 세부 사항 생성에서 눈에 띄는 개선을 관찰합니다. 예를 들어, &quot;판타지 풍경을 걷는 보라색 망토를 입은 뚱뚱한 토끼&quot;라는 프롬프트가 주어졌을 때 SD+FreeU(b)는 SD[29]와 달리 정상적인 팔과 귀를 가진 보다 사실적인 토끼를 생성합니다. 그러나 피처 스케일링 계수를 포함하면 상당한 개선이 이루어지지만 가끔은 텍스처의 바람직하지 않은 과도한 평활화로 이어질 수 있다는 점에 유의하는 것이 중요합니다. 이 문제를 완화하기 위해 저주파 정보를 줄이고 텍스처 과도한 평활화 문제를 완화하기 위해 건너뛰기 피처 스케일링 계수를 도입합니다. 그림 17에서 보여 주듯이 SD+FreeU(b &amp; s)에서 백본과 건너뛰기 피처 스케일링 계수를 결합하면 보다 사실적인 이미지가 생성됩니다. 예를 들어, &quot;바다의 반사수 위의 신스웨이브 스타일 일몰, 디지털 아트&quot;라는 프롬프트에서 SD+FreeU(b &amp; s)에서 생성된 일몰 하늘은 SD+FreeU(b)에 비해 사실성이 향상되었습니다. 이는 포괄적인 FreeU 전략이 피처의 균형을 맞추고 문제를 완화하는 데 효과적임을 강조합니다. 텍스처 매끄럽게 하는 것과 관련하여 궁극적으로 보다 충실하고 사실적인 이미지 생성을 가져옵니다. 백본 구조 관련 요소의 효과. 제안된 백본 스케일링 전략인 구조 관련 스케일링이 노이즈 감소와 텍스처 보존 간의 섬세한 균형에 미치는 효과를 평가합니다. 그림 18에 나와 있듯이 SD [29]에서 생성된 결과와 비교할 때 일정한 스케일링 요소를 활용할 때 FreeU에서 생성된 이미지 품질이 상당히 향상되는 것을 관찰합니다. 그러나 고정된 스케일링 요소를 사용하면 텍스처의 현저한 과매끄럽게 하는 것과 바람직하지 않은 색상 과포화로 나타나는 부정적인 결과가 발생할 수 있다는 점을 강조하는 것이 중요합니다. 반대로 구조 관련 스케일링 요소 맵을 사용하는 FreeU는 적응적 스케일링 접근 방식을 사용하여 구조 정보를 활용하여 백본 요소 맵의 할당을 안내합니다. 우리의 관찰 결과에 따르면 구조 관련 스케일링 요소 맵을 사용하는 FreeU는 이러한 문제를 효과적으로 완화하고 생생하고 복잡한 세부 사항을 생성하는 데 상당한 개선을 이룹니다. 4.
--- CONCLUSION ---
이 연구에서는 추가적인 계산 비용 없이 확산 모델의 샘플 품질을 크게 향상시키는 우아하고 간단하면서도 매우 효과적인 FreeU라는 접근 방식을 소개합니다. U-Net 아키텍처에서 스킵 연결과 백본 피처가 수행하는 근본적인 역할에 동기를 부여받아 확산 U-Net에서 이러한 피처의 효과에 대한 심층 분석을 수행합니다. 조사 결과 기본 백본은 주로 노이즈 제거에 기여하는 반면 스킵 연결은 주로 디코더에 고주파 피처를 도입하여 필수적인 백본 의미론을 무시할 가능성이 있음을 보여줍니다. 이를 해결하기 위해 U-Net의 스킵 연결과 백본 피처 맵에서 발생하는 기여를 전략적으로 다시 가중치를 적용합니다. 이 다시 가중치를 적용하는 프로세스는 두 U-Net 구성 요소의 고유한 강점을 활용하여 광범위한 텍스트 프롬프트와 랜덤 시드에서 샘플 품질을 크게 향상시킵니다. 제안하는 FreeU는 다양한 확산 기반 모델과 다운스트림 작업에 원활하게 통합되어 샘플 품질을 향상시키는 다재다능한 수단을 제공합니다. 참고문헌 [1] Omri Avrahami, Dani Lischinski, Ohad Fried. 자연 이미지의 텍스트 기반 편집을 위한 혼합 확산. CVPR에서, 2022.[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis. 잠재체 정렬: 잠재체 확산 모델을 사용한 고해상도 비디오 합성. CVPR에서, 2023.[3] Andrew Brock, Jeff Donahue, Karen Simonyan. 고충실도 자연 이미지 합성을 위한 대규모 GAN 학습. arXiv 사전 인쇄본 arXiv:1809.11096, 2018.[4] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon. ILVR: 확산 확률적 모델의 노이즈 제거를 위한 컨디셔닝 방법. ICCV에서, 2021.[5] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 GANS를 이긴다. NeurIPS, 2021.[6] Patrick Esser, Robin Rombach, Andreas Blattmann 및 Bjorn Ommer. ImageBART: 자기 회귀 이미지 합성을 위한 다항 확산을 사용한 양방향 컨텍스트. NeurIPS, 2021.[7] Patrick Esser, Robin Rombach 및 Bjorn Ommer. 고해상도 이미지 합성을 위한 변압기 길들이기. CVPR, 2021.[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR, 2023.[9] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio. 생성적 적대적 네트워크. NeurIPS, 2014년.[10] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo. 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. CVPR, 2022년.[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen. 임의 길이의 고화질 비디오 생성을 위한 잠재 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13221, 2022년.[12] Jonathan Ho, Ajay Jain, Pieter Abbeel. 노이즈 제거 확산 확률 모델. NeurIPS, 2020. 1,[13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu 및 Jie Tang. Cog Video: 변압기를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868, 2022.[14] Ziqi Huang, Kelvin CK Chan, Yuming Jiang 및 Ziwei Liu. 다중 모달 얼굴 생성 및 편집을 위한 협력 확산. CVPR, 2023.[15] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan 및 Ziwei Liu. ReVersion: 이미지에서 확산 기반 관계 반전. arXiv 사전 인쇄 arXiv:2303.13495, 2023. 3,[16] Tero Karras, Timo Aila, Samuli Laine 및 Jaakko Lehtinen. 품질, 안정성 및 변형 개선을 위해 GAN을 점진적으로 성장시킵니다. ICLR, 2018.[17] 테로 카라스, 사무리 레인, 티모 아일라. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처입니다. CVPR, 2019. [18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen 및 Timo Aila. StyleGAN의 이미지 품질을 분석하고 개선합니다. CVPR, 2020. [19] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen 및 Timo Aila. 별칭이 없는 생성적 적대 네트워크. NeurIPS, 2021.[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri 및 Michal Irani. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. arXiv 사전 인쇄 arXiv:2210.09276, 2022.[21] 디데릭 P 킹마(Diederik P Kingma)와 맥스 웰링(Max Welling). 자동 인코딩 변형 베이. arXiv 사전 인쇄 arXiv:1312.6114, 2013.[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman 및 Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 정의. arXiv 사전 인쇄 arXiv:2212.04488, 2022.[23] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou 및 Tieniu Tan. VideoFusion: 고품질 비디오 생성을 위한 분해 확산 모델입니다. CVPR, 2023. 1, 3, 6, 7,[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu 및 Stefano Ermon. SDEdit: 확률론적 미분 방정식을 사용한 유도 이미지 합성 및 편집. ICLR, 2022.[25] 메디 미르자와 사이먼 오신데로. 조건부 생성 적대 네트워크. arXiv 사전 인쇄 arXiv:1411.1784, 2014.[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. GLIDE: 텍스트 유도 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. arXiv 사전 인쇄본 arXiv:2112.10741, 2021.[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach. Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 사전 인쇄본 arXiv:2307.01952, 2023.[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. CLIP 잠재 이미지를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022.[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR, 2022. 1, 3, 6, 7, 10,[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR, 2023. 3,6,[31] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi. Palette: 이미지-이미지 확산 모델. ACM SIGGRAPH, 2022.[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487, 2022.[33] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022.[34] Aaron Van Den Oord, Oriol Vinyals 등. 신경 이산 표현 학습. NeurIPS, 2017.[35] 페이 왕(Pei Wang), 리 이준(Yijun Li), 누노 바스콘셀로스(Nuno Vasconcelos). 이미지 스타일 전송의 견고성을 재고하고 개선합니다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 124-133, 2021.[36] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen 및 Fang Wen. 사전 훈련은 이미지를 이미지로 변환하는 데 필요한 전부입니다. arXiv 사전 인쇄 arXiv:2205.12952, 2022.[37] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang 등. Lavie: 계단식 잠재 확산 모델을 사용한 고품질 비디오 생성. arXiv 사전 인쇄 arXiv:2309.15103, 2023.[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie 및 Mike Zheng Shou. Tune-a-video: 텍스트-비디오 생성을 위한 이미지 확산 모델의 원샷 조정입니다. arXiv 사전 인쇄 arXiv:2212.11565, 2022.[39] Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy. 비디오 다시 렌더링: 제로샷 텍스트 안내 비디오-비디오 변환. arXiv 사전 인쇄 arXiv:2306.07954, 2023. 3, 6,[40] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao 및 Mike Zheng Shou. 쇼 1: 텍스트-비디오 생성을 위한 픽셀 및 잠재 확산 모델 결합, 2023.
"
"--- ABSTRACT ---
ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia. ABSTRACT Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limitations in the following aspects: insufficient volume, simplistic content, and arduous collection procedures. To © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0686-8/24/10...$15.https://doi.org/10.1145/3664647.--- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. establish an audio dataset with high-quality captions, we propose an innovative, automatic approach leveraging multimodal inputs, such as video frames, audio streams. Specifically, we construct a large-scale, high-quality, audio-language dataset, named as AutoACD, comprising over 1.5M audio-text pairs. We exploit a series of pre-trained models or APIs, to determine audio-visual synchronisation, generate image captions, object detection, or audio tags for specific videos. Subsequently, we employ LLM to paraphrase a congruent caption for each audio, guided by the extracted multimodality clues. To demonstrate the effectiveness of the proposed dataset, we train widely used models on our dataset and show performance improvement on various downstream tasks, for example, audio-language retrieval, audio captioning, zero-shot classification. In addition, we establish a novel benchmark with environmental information and provide a benchmark for audio-text tasks. CCS CONCEPTS + Computing methodologies — Computer vision; + Information systems — Data structures; Information retrieval. KEYWORDS Audio-language Dataset, Audio-language Representation Learning, Audio Captioning ACM Reference Format: Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. 2024. Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning. In Proceedings of the 32nd ACM International Conference on Multimedia (MM 24), October 28—November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3664647.1
--- INTRODUCTION ---
In recent literature, foundation models, like CLIP [49], variants of GPT [50], DALL-E 2 [51] and Stable Diffusion [53], have shown tremendous success in various understanding and generation tasks. Despite being different in architectural or algorithmic designs, they are lying on a common basis: large-scale multimodal datasets, for example, MMC4 [66], LAION [55], HowTo100M [39], indicating an emerging transition from model-centric to data-centric representation learning. The former considers pushing the boundaries of model design within the constraints of a predetermined data budget, while the latter focuses on curating large-scale and high-quality datasets in a scalable manner. In the audio community, there have been recent endeavours on constructing audio-language datasets, as demonstrated in Fig. 2. However, existing datasets potentially suffer from two limitations, laborious and complicated collection processes and simplistic descriptions in text. On the one hand, Clotho [11] and AudioCaps [24], which contain audios typically comprising 1 to 3 sound events, accompanied by high-quality text descriptions provided by human annotators. This is are clearly challenging to scale up. On the other hand, LAION-Audio-630K [59] and WavCaps [38] collect large amounts of raw data from online foley websites, then employ sentence templates or keyword-to-caption models to convert the original audio labels into free-form sentences. It is obvious that the resulting language descriptions hardly offer additional information than simple prompts or sound tags. Therefore, models trained Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie Length 30K 57K 400K 630K Aut — Env. Ew. —— WavCaps —Clotho —— Auto-ACD(Ours) ——— LAION-Audio-630K — AudioCaps Figure 2: Comparison with other audio caption datasets. “Length” and “# Vocab.” refer to average length and vocabulary. “Env.” and “Auto.” refer to environmental information and automatic pipeline, respectively. on these datasets are incapable of learning robust audio-language representations. This paper presents our recent efforts for constructing a largescale, high-quality, audio-language dataset, with minimal manual efforts, termed Auto-ACD (Audio Captioning Dataset by Automatic Collection), with massive audio-text pairs (1.5M), long texts (words) and diverse vocabularies (23K). Specifically, an exemplary audio caption ought to encapsulate four varieties of information: the ‘what’ - the nature of the sound perceived, the ‘who’ - the entity producing the sound, the ‘how’ - the characteristics of the sound, and the ‘where’ - the location the sound occurs. Our key insight is that comprehensive understanding of the visual scene is expected to serve as a valuable information source and is sometimes necessary for understanding the audio content. Therefore, we build Auto-ACD on the prior of robust audio-visual correspondence in existing audio-visual datasets, for example, VGGSound [7], AudioSet [13]. Particularly, we initiate an automatic pipeline, that employs a range of publicly available tools or APIs across the general AI community, e.g., vision, language and audio models, to generate comprehensive language descriptions for the audio stream of the given video datasets. Lastly, we employ a large language model (LLM) to collectively assimilate all outputs, identify and eliminate any illogical information, and generate comprehensive descriptions for the audio. As a result, these descriptions not only depict the type of sound and its source, but also describe the auditory attributes and the specific location of its occurrence. --- --Auto-ACD To comprehensively validate auditory representation, for instance, audio events, and ambient information, learned from the text descriptions of Auto-ACD, we conduct experiments from four perspectives: First, we launch a joint audio-language representation learning using InfoNCE loss [18, 46, 65], and evaluate the model through a retrieval task between audio and language, showing noticeable improvement over existing datasets; Second, we conduct zero-shot classification experiments, demonstrating the effectiveness for learning environmental information with our dataset; Third, we benchmark on audio-language generation task, specifically, automatic audio captioning, by training a lightweight mapping network between the pre-trained audio backbone and GPT2 [50], showing superior performance on the widely used benchmark, e.g., Clotho [11]; Fourth, we manually filter a test set and introduce a novel benchmark for audio-language tasks. This benchmark assesses the ability of models to grasp information beyond mere audio tags, for example, the environment and fine-grained categories of sound, we set a baseline for future research in this field. 2
--- RELATED WORK ---
2.1 Audio-visual Learning Audio-visual events often occur simultaneously within in-the-wild videos, establishing a profound connection between sound and imagery. [1, 2, 23, 47] employ audio-visual self-supervised learning to leverage audio-visual correspondence for enhancing representation learning. Specifically, [15, 58, 62] learn audio-text representation based on such correspondence. Audio-visual localisation [6, 21, 40, 41, 56] concentrates on identifying the positions of visual sound sources within video. Audio-visual segmentation [12, 29, 32, 42, 64] aims to predict the pixel-wise segmentation masks of sounding objects in visual scenes precisely. Such studies have further demonstrated the intrinsic correlation between audio and visual events in in-the-wild videos, which inspires us to create an audio-language dataset anchored in visual information. 2.2 Audio-visual Dataset Large-scale audio-visual datasets are crucial for effective audio and video understanding. Two datasets are often involved in audiovisual learning: AudioSet and VGGSound. AudioSet [13] is a largescale audio-visual dataset with multiple audio events labelled for each audio clip. It contains over 2M 10-second audio clips. AudioSet is a manually annotated dataset, with the help of a well-structured hierarchical ontology consisting of 632 audio classes guided by literature and manual curation. VGGSound [7] comprises 200K 10second videos for 309 audio classes. This dataset was collected and annotated through an automated pipeline, with each video assigned only one label. In this paper, we aim to provide detailed description for audios, by exploiting both audio and visual cues. 2.3 Audio-language Learning The application of visual-language models in the audio-language arena marks a significant leap forward. Notably, [59] have adapted the CLIP model for audio-language contrastive learning, setting a precedent for innovative cross-modal research. Researchers are not merely focusing on extracting semantic information from audio MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. through tasks such as audio-text retrieval [25, 45], audio classification [20, 48], automatic audio captioning [37, 60], and audio question answering [14, 31]. They are also venturing into more nuanced aspects of auditory perception, including exploring temporal dynamics in sound through audio event detection [3, 28]. This broadening scope encompasses additional auditory attributes such as counting sounds within scenes [44] and classifying environments based on their acoustic characteristics[10]. Undoubtedly, it is paramount to construct a comprehensive, large-scale, high-quality and information-rich audio-language dataset. 2.4 Audio-language Dataset Audio-language tasks, including audio-text retrieval, audio captioning, audio question answering and text-guided audio generation, have greatly benefited from the availability of two widely-used audio captioning datasets: AudioCaps and Clotho. AudioCaps [24], a subset of AudioSet, consists of 50K 10-second-long audio clips, each with a single caption annotated. The annotators were provided with AudioSet tags as hints and videos if necessary. Clotho [11], on the other hand, comprises 6K audio clips lasting between 15 toseconds, each with five captions annotated through a three-step process involving captioning, grammar correction, and rating by human annotators. However, due to the human annotation process, these datasets are limited in size, expensive and time-consuming. LAION-Audio-630K [59] acquires audio and descriptions from online foley websites, including popular platforms like Freesound! and BBC Sound Effects”. WavCaps [38] utilizes ChatGPT to filter and paraphrase these raw descriptions, resulting in a dataset of 400K audio-text pairs with cleaned text data resembling human annotations. The sentence is mostly simple since there is often only one sound event in an audio clip. As a result, models trained on these datasets could only learn the category of sound. To enhance the comprehension capabilities of the audio-text model, we need a more diverse set of textual and audio data. 3 DATASET CONSTRUCTION To develop a large-scale, audio dataset with rich language descriptions, we base on the assumption that visual scene understanding serves as a strong prior. For instance, synchronized videos frequently showcase auditory cues, and visual information serves as a precise representation of the acoustic environment in which the sound happens. In an audio caption, it is desirable to incorporate sound attributes, location, and fine-grained labels. To achieve this, we can leverage publicly available tools or APIs to gather the necessary information for audio description and mutually verify the results. For instance, we can employ an object detection model to identify potential sources of sound, and an environmental classification model to extract scene categories. By extracting a wealth of information, we ensure the maximum coverage of accurate details, providing the language model with ample references. freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. = a . audio video visual-audio label ——WW———_—_*+ train [x¢:0.5921, ¥¢:0.5947, w:0.7879, h:0.3298] train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie a train pulling into a station. prompt ©) train_station [prob:0.657] aye passenger_car A train horn blows as a train passes by, creating a loud and distinct sound in a railway station. a train horn blows. train horning Figure 3: Automatic pipeline for Auto-ACD collection. We utilize four open-source computer vision models to extract visual clues from the middle frame of videos, and two open-source audio understanding models to analyze the entirety of the audio content. Consequently, we combine the labels from the original dataset, and leverage Large Language Models (LLMs) to interpret and paraphrase these components into the final description. 3.1 Tools or APIs Given one sample from existing large-scale video datasets, for example, AudioSet or VGGSound [7, 13], i.e., denoted as V = {f;a;y}, where f, a and y correspond to frame sequence, audio stream, and visual or audio labels, respectively. Our goal is to adopt a range of publicly available tools or APIs across the general AI community, ie, using off-the-shelf vision, language and audio models to construct language descriptions for audios, as shown in Fig. 3. In this section, we describe these tools in detail. 3.1.1 Image Captioning. We employ the off-the-shelf BLIP-2 [27] model, which obtains competitive results for image captioning. This tool has the ability to generate captions that encompass the entire image and accurately depict the primary subject or environment. In our case, we input the middle frame of the video into this model. 3.1.2 Object Detection. We use the pre-trained Grounding DINO model [33], to identify objects within the middle frame, and preserve all the detected entities along with their corresponding prediction confidence scores to ensure a comprehensive analysis. 3.1.3 Image Labeling. We adopt the pre-trained OpenAI CLIP [49] model for image classification. In this application, we utilize the prompt: “a photo of a {label}"" to generate textual embedding, leveraging the category ontology from ImageNet [9]. 3.1.4 Place Recognition. We employ the pre-trained PlaceCNN [63], to infer the environment context captured in videos. Given the robust correspondence between audio and visual signals, the environment depicted in the video is highly likely to represent the acoustic ambience in which the sound occurs. 3.1.5 Audio Tagging. We use the pre-trained PANNs [26] to predict the tags of sounds within the audio, and preserve the top three predictions with their confidence scores. This represents a crucial source of auditory temporal information, particularly for sounds emanating from entities not visible within the frame. 3.1.6 Audio Captioning. We use the existing AudioCaption [61] model, to generate concise and brief captions. These captions resemble the style of AudioCaps, focusing solely on the categorical information of audio events, devoid of any additional descriptive attributes about the sound. 3.1.7 Audio-visual Synchonisation. We employ the pre-trained Synchformer [22] to conduct synchronization detection between video and audio. This process could filter out samples consisting of irrelevant or unsynchronized video and audio content. In this case, we input both video and audio respectively into this model for analysis. 3.1.8 Existing Audio-Visual Labels. In addition to the predictions from models, we also incorporate the provided labels of existing datasets into our pipeline. For instance, VGGSound [7] gives a single label for each video, while AudioSet [13] provides multiple labels. These labels serve in the original dataset, offering accurate yet incomplete audio-visual information. 3.1.9 Summary. As for the language model, we use the OpenAI ChatGPT®, which demonstrates strong performance in reasoning and inductive summarization, to assemble the above-mentioned. descriptions or labels into comprehensive descriptions for audio. Many works, like BLIP-2[27], show that utilizing existing tools appropriately can significantly enhance the model’s performance. By leveraging audio-visual correspondence and the profound understanding capabilities of LLM, we generate precise audio captioning from the rich multi-modality clues acquired. In this case, we feed in a special prompt as shown in Section 3.2. 3.2 Caption Generation Based on the visual and acoustic clues present in the video, we craft a structured language paragraph, and use it to prompt ChatGPT to generate descriptions for audio. As illustrated in Fig. 4, the process begins with formulating the specific task and criteria for the desired outcome, followed by inputting seven distinctive audio-visual cues Shttps://openai.com/chatgpt --- --Auto-ACD [ Prompting ChatGPT to generate caption for audio I will give you some information from a video and an audio, this audio is separated from the video. There is a caption for an audio, simple audio caption, this sentence simply describe what happens in the audio. There are some audio tags: multiple audio tags, they indicate the audio events in this audio. number indicates the probability. The audio-visual label is dataset visual-audio label. Lextract a key frame from one video, and this is the image caption of this frame: image caption; this is the image label: imave labe\; this is the object detection: object detection; this is the place detection: place label. Now, please help me write one audio caption using common vocabulary and no more than 24 words, providing a description of what happened in the audio, and infer where the audio happened. You can refer the above information, and some visual information is inaccurate and can be ignored. please using the audio-visual label check the audio event in your caption. The sentence you write need to be like these following examples: A bell chimes thrice as birds chirp in the background in the for ‘A lawnmower engine buzzing and stopping to take a few breaks on the lawn. A machine being operated intermittently and people talking in the background in a factory. Figure 4: Detailed prompt provided to ChatGPT. For visualisation purposes, we use different colors to highlight diverse visual-audio cues. into the prompt, accompanied by their corresponding confidence score. Additionally, we provide three sentence examples from AudioCaps or Clotho as instruction. For visualisation purposes, we here use a colour-coded system to distinguish various cues. While generating captions, we explicitly ask ChatGPT to remove information that is inaudible, i.e., illogical and visually oriented elements, for example, colours. As a result, the large language model is able to analyze the scenario from all provided clues, and generate language description for audio, with sound category, and environment. The generated caption results are shown in Table. 1. 3.3 Dataset Filtering AudioSet is vast and diverse, while heavily marred by noise in many instances, for instance, gameplay live streams and explanatory videos. Conversely, VGGSound significantly emphasises the robust correlation between video and audio within the automated collection pipeline, thus requiring no further processing. As shown in Figure. 5, we formulate filtering criteria grounded in both the video-audio correspondence and the original labels. For each filter criterion, we conduct numerous trials followed by a manual verification, each filtering criterion achieves an accuracy rate exceeding 90%, resulting in the removal of 0.4 million videos in total. 3.3.1 Raw labels. AudioSet contains a plethora of explanatory videos with background music, wherein the visual and auditory information often do not correspond. Therefore, we eliminate videos from the multi-labels that encompass both speech and music. MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 1: The results of generated captions in Auto-ACD, with accurate content and ample surrounding information. Green and Yellow refer to “where"" and “how"" the audio sounds like. No. Generated Caption Loud pops and bangs resonate as timbales are being played, creating rhythmic music in a room. 2 Water gurgles and bubbles as a boat glides through, creating “a soothing and peaceful underwater ambience. 3 A woman speaks softly amidst the soothing sound of birds ‘chirping, creating a serene atmosphere in a garden. 4 A motorcycle engine idles before revving up, creating a loud sound in an urban environment. right Synchformer => tolerant right YW error ey video-audio music & speech © labels ——? labels analysis | ==> f others ‘ o Figure 5: Filtering process for AudioSet. We filter the dataset by assessing whether the video and audio are synchronized and analyzing the labels in the original dataset. 3.3.2 Audio-visual synchronisation. To obviate the possibility of fortuitous inference errors, we subject each video to five synchronization evaluations, featuring random variations in start time and offset, with a tolerance threshold established at 0.6 seconds. Synchformer[22] employs a 0.2s offset to ascertain the precise audiovisual synchronization, whereas we utilize a broader offset to determine the audio-visual correspondence. The outcomes are categorized as follows: (1) Predictions aligning with the ground truth are deemed “correct”; (2) Predictions that diverge from the ground truth while with a discrepancy within 0.6 seconds are designated as to be “tolerable”; (3) All other results are termed “error”. To preserve as much data as possible, videos classified as “error” in all five tests are removed from the dataset. 3.4 Dataset Statistics As depicted in Fig. 2, we collect 1.5 million audio-language pairs from AudioSet and VGGSound in total. To the best of our knowledge, Auto-ACD is the first million-level audio-language dataset to date, with train, validation and manually filtered test sets. Auto-ACD surpasses the other datasets in terms of data volume, average sentence length, and contains a relatively wide verbal vocabulary. LAIONAudio-630K[59] sources from user uploads, contains a plethora of noisy details, for instance, device and timestamps, and features an exceptionally extensive vocabulary. Additionally, Auto-ACD stands --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Frogs croaking and with insects Text vocalizing. Encoder 1d aleeAtt a} ede? | e8-e3ede? | eae? | eae? ea Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie prefix embeddings P! audio feature ef, Audio ' Encoder &A train running followed by a train horn. caption tokens Figure 6: Audio-language retrieval model and automatic audio captioning model frameworks. Similar to CLIP, the audiolanguage retrieval model consists of an audio encoder, text encoder, and contrastive loss. The automatic audio captioning model comprises a frozen audio encoder and language model, and a trainable mapping network. as the only audio-language dataset that encompasses environmental information, not only delineates the type and source of sounds but also specifies the location of their occurrence, increasing the richness of contextual details. In supplementary, we present a comparative analysis of captions from LAION-Audio-630K, WavCaps, and Auto-ACD for the same audio sample. Captions in LAION-Audio-630K and WavCaps are concise and contain minimal information beyond the audio tags. In particular, LAION-Audio-630K may include sentences that deviate from common sense, for example, describing “rapping a tree” for an audio tag of “rapping”. WavCaps, on the other hand, exhibits a monotonous sentence structure, such as “... sound can be heard”. In contrast, Auto-ACD features longer sentences that provide a richer depiction of the audio scenes. We conduct a manual check on randomly sampled 200 audiocaptions pairs from Auto-ACD, analyzing the clues from the different open-source tools and the generated captions. We define a clue that contradicts the audio to be erroneous, these tools possess high accuracy, the average accuracy is 81.3%. Furthermore, we conduct a manual check on randomly sampled 1000 audio-captions pairs, and find that 92.4% captions correspond with audio, just 5.3% incorrect words need to be modified, and only 4.4% captions contain inaudible information. These results indicate that our proposed approach enables high-quality, scalable caption generations, with few incorrect or inaudible information. 4 ARCHITECTURE We construct architectures targeting two general audio-language tasks, namely, audio-language contrastive learning, and automatic audio captioning, to further validate the effectiveness of Auto-ACD. In Section 4.1, we provide a detailed exposition of the architecture for audio-language contrastive learning. Further in Section 4.2, we introduce the framework for lightweight automatic audio captioning along with its loss function. 4.1 Audio-Language Constrastive Learning To validate the efficacy of our proposed dataset, we train an audiolanguage model with standard contrastive learning, e.g., infoNCE [49] loss, as shown in Fig.6. Specifically, we employ the pre-trained HTSAT [8] as the audio encoder, and the pre-trained RoBERTa [35] as the language encoder. Both encoders were initialised from the pre-trained CLAP model [59], and further finetuned on our dataset. We term our final model as Audio-Text Retrieval (ATR). Given an audio-text pair (a’, t'), we utilise audio encoder Aenc and text encoder Jenc to extract audio embedding e/, and text embedding el, respectively: fene(t') 4 = Aenc(a’), e} = The model is then trained with contrastive loss, wherein the paired audio and language embeddings are treated as positive, and unpaired ones as negative, with the following loss function: N DL exp where Tt represents the learnable temperature parameters. At training phase, we introduced word-level text masking, that is to randomly mask words within the sentences, before feeding into the text encoder. 4.2 Automatic Audio Captioning To demonstrate the effectiveness of our pre-trained audio backbone, we also use audio captioning for evaluation. Inspired by ClipCap [43] and AutoADs [16, 17], we adopt a lightweight audio captioning model, where both the audio backbone and language model (GPT-2) are fixed, and only a mapping network is trained, as shown in Fig. 6. --- --Auto-ACD MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 2: The audio-text retrieval results on AudioCaps, Clotho and ACD test sets. “basic”, “LA.” “Wav.” and “ACD"" refer to the combination of AudioCaps and Clotho (basic), LAION-Audio-630K (LA), WavCaps (Wav) and Auto-ACD (ACD), respectively. “ACDys” is a subset of Auto-ACD, curated from VGGSound. “ * FT” refers to fine-tuning the model on the target dataset. AudioCaps Test Clotho Test Auto-ACD Test Train Set Model Audio—Text Text— Audio R@1 R@10 R@1 R@Audio—Text Text—Audio Audio—Text Text—Audio R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@basic+LA.[59] HTSAT-RoBERTa 45.0 88.0 36.2 82.24.2 66.9 17.2 55.4 20.0 65.0 17.9 59.basic+Wav.[38] _ HTSAT-BERT 51.7 90.6 39.7 86.1 23.4 63.4 19.5 58.2 - - - basict ACDys HTSAT-RoBERTa 50.5 90.6 39.8 86.9 24.2 62.9 20.0 58.9 39.2 86.2 39.6 85.basictACD HTSAT-RoBERTa 53.7 91.7 39.5 85.4 17.7 52.6 15.3 52.1 47.1 91.2 49.0 92.basictACD*FT HTSAT-RoBERTa 56.3 93.9 42.7 88.5 26.2 67.5 21.7 61.7 - - - Given an audio-text pair (a',c'), we use the pre-trained audio encoder to extract audio features e) = Aenc(a'), and we convert the caption into a token sequence, cl... 13h where k indicates the maximal length of text. Then, we design a mapping network fmap to transform the extracted embedding into a set of prefix embeddings: P* = finap(€a)We take the prefix embedding set as the condition for predicting the next token with an auto-regressive language model. There fore, during training, we minimize the negative log-likelihood of predicting the correct word: N ¢ L= =D) Y tog po (6) | Phela--se5-1) i=l j=l where @ represents the trainable parameters. 5 EXPERIMENTS In this section, we evaluate three tasks, namely, audio-language retrieval, audio captioning and zero-shot classification. 5.1 Audio-language Retrieval 5.1.1 Dataset. We conduct audio-text retrieval experiments across several datasets: AudioCaps, Clotho, Auto-ACDys, and Auto-ACD. The distributions for the train, validation, and test sets in AudioCaps, Clotho, and Auto-ACD are 50K/495/975, 3.8K/1045/1045, and 1.5M/2K/1K data pairs, respectively. Auto-ACDys is a subset of Auto-ACD, containing 190K data pairs exclusively sourced from VGGSound. Notably, in the case of Clotho, and AudioCaps (validation and test set), each data pair consists of one audio sample accompanied by five corresponding captions, while the remaining data pairs only comprise one audio-caption pair. 5.1.2 Auto-ACD Benchmark. In addition to the Auto-ACD training set, we also randomly selected 2K data samples to form the validation set and 1K samples for the test set. We conduct a manual verification of the test set, by removing incorrect information from the language descriptions and rewriting inappropriate vocabulary expressions. This test set is used for evaluating both audio-language retrieval and automatic audio captioning tasks. 5.1.3 Metrics. In order to validate the rich and accurate information of our dataset, we compare the traditional metrics, Recall@k performance, on commonly used datasets, for example, AudioCaps and Clotho. We also adopt these metrics on the Auto-ACD test set, offering a comprehensive overview. 5.1.4 Training Details. We train our proposed Audio-Text Retrieval (ATR) model for 20 epochs, employing a batch size of 768, and utilizing the Adam optimizer with a warm-up phase, and an initial learning rate of 1e-4 with a cosine learning rate decay schedule. We use the same hyperparameters as those in the existing CLAP model configuration. The dimensions of both the audio encoder and text encoder output are 512. Additionally, we introduce 25% random masking on words in the sentences and randomly apply augmentations such as Noise and Gain to 50% of audio samples to enhance the model training. We further fine-tune the model on specific datasets, for example, Clotho and AudioCaps, with an initial learning rate of 2e-5 for 15 epochs. 5.1.5 Results. As shown in Table.2, we can draw the following key observations: (i) comparing with training on Laion-Audio-630K, training on our proposed Auto-ACDys dataset leads to a significant improvement in Recall@k metrics on AudioCaps and Auto-ACD benchmarks. (ii) training on Auto-ACD and fine-tuning on specific datasets, which is not applicable to Auto-ACD benchmark, results in a remarkable performance gain. This improvement is particularly evident when evaluating the model on the test set of AudioCaps, as AudioCaps is a subset of AudioSet and shares a similar data distribution with Auto-ACD. Such fine-tuning processes enable the model to acquire a more comprehensive understanding of both audio and text information, thus enhancing retrieval performance. (iii) on the Auto-ACD benchmark, characterized by a more diverse lexicon and abundant language description, training on Auto-ACD datasets significantly outperforms the model trained on Laion-Audio-630K. 5.2 Automatic Audio Captioning 5.2.1. Dataset. In addition to the datasets mentioned in Section 5.1, we also use the MACS dataset [36], which comprises 3.9K audio-text data pairs, with each audio accompanied by two to five captions and several audio tags. In total, we train the automatic audio captioning model utilizing a total of 58k data pairs from Clotho, AudioCaps and MACS, and evaluate on Clotho and Auto-ACD test set. --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. 5.2.2 Metrics. In addition to conventional captioning metrics, for example, Meteor [5], RougeL [30], Spider [34], we incorporate SentenceBERT [52] as additional evaluation metrics, that not solely rely on lexical alignment, but rather prioritize the semantic resemblance and accuracy of the captions’ content. 5.2.3 Training Details. We devise two mapping networks, MLP and transformer, and fine-tune the parameters of GPT during the training process. We set the number of prefixes to be 8, each with a dimension of 512. We train this audio captioning model on the MACS [36], Clotho and AudioCaps for 15 epochs with a batch size of 128 and an initial learning rate of 5e-4. In this task, we compare the audio encoder from our pre-trained audio-text retrieval model and the pre-trained CLAP [59], by only training the mapping network of both models on the benchmark datasets, namely, Clotho, and Auto-ACD. 5.2.4 Results. As shown in Table. 3, we can draw two observations: (i) the automatic audio captioning model, with the audio encoder initialised from our pre-trained audio-text retrieval model, shows improved performance across all evaluation metrics than baseline. (ii) there is a more pronounced outcome when evaluated on Auto-ACD: the baseline approach’s performance oversees a sharp decrease in the test set of Auto-ACD. We conjecture this is because the baseline features extracted from the CLAP model lack detailed descriptions of environmental information. While captioning model based on our pre-trained audio-text retrieval model shows a significant performance improvement, and is able to infer where the sound occurs precisely. This observation signifies that Auto-ACD showcases an extensive lexicon, enabling the portrayal of a given audio using various sentence structures. On the other hand, it illustrates that models trained on our dataset will deduce the context in which the sound emanates. Table 3: The automatic audio captioning results on Clotho and Auto-ACD test sets. “S-BERT” refers to SentenceBERT, “Env.” refers to whether the predicted captions contain environmental information. Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. Cloth CLAP 155 349 206 460 x one Ours 166 36.2 212 474 x CLAP 99 230 196 8&7 x Auto-ACD Ours 213 37.9 56.7 101 V 5.3. Zero-shot Classification 5.3.1 Dataset. Auto-ACD stands out for integrating its incorporation of environmental information within its text descriptions. Following the training on Auto-ACD, we conduct environmental classification in four distinct scenarios: (i) a collection of samples from the AudioSet evaluation set, annotated with child classes of ""Acoustic Environment"" within the AudioSet ontology, referred to as AudioSet Env. To prevent data leakage, here we exclusively utilize the model pre-trained on Auto-ACDys for this experiment; (ii) the urban acoustic scene dataset [19], known as DCASE 2020 Mobile, previously utilized in the DCASE 2020 challenge. (iii) the popular Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie urban sound event classification dataset, UrbanSound 8k [54]; (iv) the music genre classification dataset, GTZANGenres [57]. 5.3.2. Metrics. We approach zero-shot classification as an audiotext retrieval experiment, employing a conventional paraphrasing template: ""The sound in [environment label] / of [label]."" We utilize Recall@1 as the metric for evaluating the environment classification outcomes in this experiment. 5.3.3 Results. The experimental results, as illustrated in Table. 4, highlight the superior environmental recognition capability of ATR pre-trained on Auto-ACD in comparison to CLAP. Notably, on the AudioSet Env, our model significantly outperforms CLAP, even though we only utilize Auto-ACDys, for pre-training without any data leakage from AudioSet into our training dataset, further serving as a testament to the rich and accurate environmental information in Auto-ACD. The results on UrbanSound 8K and GTZANGenres shows that in addition to the audio events, the captions may also include more information, for example, diverse environment descriptions, fine-grained musical genres. wan Table 4: Zero-Shot Acoustic Environment Classification. refers to pre-training model on Auto-ACDys. “US-8K” refers to UrbanSound 8K. Model AudioSetEnv DCASE US-8K GTZANGenres CLAP 19.5 32.2 75.0 31.Ours 39.5"" 36.5 76.2 45.6 CONCLUSION In this paper, we present an automatic pipeline for audio caption generation, accompanied by a large-scale and comprehensive audio captioning dataset comprising 1.5M data pairs. Furthermore, we evaluate the performance of various audio-language models on our dataset to authenticate the effectiveness, and provide a manually verified test set along with a benchmark for audio-language tasks. These experimental findings unveil the wealth of information and precise descriptions inherent in our data, facilitating the models to learn more robust audio-language representations. Owing to the fact that a portion of our dataset originates from VGGSound, procured through an automatic pipeline. The transformation from online videos to precise audio-language pairs has evolved into a thoroughly automated and replicable procedure. Consequently, the acquisition of an expanded corpus of audio-language datasets is now a straightforward endeavour. Furthermore, as opensource computer vision models and Large Language Models (LLMs) undergo continuous refinement and advancement, the capacity to extract more precise audio-visual indicators improves, subsequently enhancing the precision of inferences and the quality of paraphrasing the final audio captions. ACKNOWLEDGMENTS This work is supported by National Key R&D Program of China (No.2022ZD0161400). --- --Auto-ACD REFERENCES20.22, 23, Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovié, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. 2020. Self-supervised multimodal versatile networks. Advances in Neural Information Processing Systems 33 (2020), 25-37. Relja Arandjelovic and Andrew Zisserman. 2017. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision. 609-617. Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband, and Anthony T Chronopoulos. 2017. An overview of audio event detection
--- METHOD ---
ologies — Computer vision; + Information systems — Data structures; Information retrieval. KEYWORDS Audio-language Dataset, Audio-language Representation Learning, Audio Captioning ACM Reference Format: Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. 2024. Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning. In Proceedings of the 32nd ACM International Conference on Multimedia (MM 24), October 28—November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3664647.1 INTRODUCTION In recent literature, foundation models, like CLIP [49], variants of GPT [50], DALL-E 2 [51] and Stable Diffusion [53], have shown tremendous success in various understanding and generation tasks. Despite being different in architectural or algorithmic designs, they are lying on a common basis: large-scale multimodal datasets, for example, MMC4 [66], LAION [55], HowTo100M [39], indicating an emerging transition from model-centric to data-centric representation learning. The former considers pushing the boundaries of model design within the constraints of a predetermined data budget, while the latter focuses on curating large-scale and high-quality datasets in a scalable manner. In the audio community, there have been recent endeavours on constructing audio-language datasets, as demonstrated in Fig. 2. However, existing datasets potentially suffer from two limitations, laborious and complicated collection processes and simplistic descriptions in text. On the one hand, Clotho [11] and AudioCaps [24], which contain audios typically comprising 1 to 3 sound events, accompanied by high-quality text descriptions provided by human annotators. This is are clearly challenging to scale up. On the other hand, LAION-Audio-630K [59] and WavCaps [38] collect large amounts of raw data from online foley websites, then employ sentence templates or keyword-to-caption models to convert the original audio labels into free-form sentences. It is obvious that the resulting language descriptions hardly offer additional information than simple prompts or sound tags. Therefore, models trained Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie Length 30K 57K 400K 630K Aut — Env. Ew. —— WavCaps —Clotho —— Auto-ACD(Ours) ——— LAION-Audio-630K — AudioCaps Figure 2: Comparison with other audio caption datasets. “Length” and “# Vocab.” refer to average length and vocabulary. “Env.” and “Auto.” refer to environmental information and automatic pipeline, respectively. on these datasets are incapable of learning robust audio-language representations. This paper presents our recent efforts for constructing a largescale, high-quality, audio-language dataset, with minimal manual efforts, termed Auto-ACD (Audio Captioning Dataset by Automatic Collection), with massive audio-text pairs (1.5M), long texts (words) and diverse vocabularies (23K). Specifically, an exemplary audio caption ought to encapsulate four varieties of information: the ‘what’ - the nature of the sound perceived, the ‘who’ - the entity producing the sound, the ‘how’ - the characteristics of the sound, and the ‘where’ - the location the sound occurs. Our key insight is that comprehensive understanding of the visual scene is expected to serve as a valuable information source and is sometimes necessary for understanding the audio content. Therefore, we build Auto-ACD on the prior of robust audio-visual correspondence in existing audio-visual datasets, for example, VGGSound [7], AudioSet [13]. Particularly, we initiate an automatic pipeline, that employs a range of publicly available tools or APIs across the general AI community, e.g., vision, language and audio models, to generate comprehensive language descriptions for the audio stream of the given video datasets. Lastly, we employ a large language model (LLM) to collectively assimilate all outputs, identify and eliminate any illogical information, and generate comprehensive descriptions for the audio. As a result, these descriptions not only depict the type of sound and its source, but also describe the auditory attributes and the specific location of its occurrence. --- --Auto-ACD To comprehensively validate auditory representation, for instance, audio events, and ambient information, learned from the text descriptions of Auto-ACD, we conduct
--- EXPERIMENT ---
s from four perspectives: First, we launch a joint audio-language representation learning using InfoNCE loss [18, 46, 65], and evaluate the model through a retrieval task between audio and language, showing noticeable improvement over existing datasets; Second, we conduct zero-shot classification experiments, demonstrating the effectiveness for learning environmental information with our dataset; Third, we benchmark on audio-language generation task, specifically, automatic audio captioning, by training a lightweight mapping network between the pre-trained audio backbone and GPT2 [50], showing superior performance on the widely used benchmark, e.g., Clotho [11]; Fourth, we manually filter a test set and introduce a novel benchmark for audio-language tasks. This benchmark assesses the ability of models to grasp information beyond mere audio tags, for example, the environment and fine-grained categories of sound, we set a baseline for future research in this field. 2 RELATED WORK 2.1 Audio-visual Learning Audio-visual events often occur simultaneously within in-the-wild videos, establishing a profound connection between sound and imagery. [1, 2, 23, 47] employ audio-visual self-supervised learning to leverage audio-visual correspondence for enhancing representation learning. Specifically, [15, 58, 62] learn audio-text representation based on such correspondence. Audio-visual localisation [6, 21, 40, 41, 56] concentrates on identifying the positions of visual sound sources within video. Audio-visual segmentation [12, 29, 32, 42, 64] aims to predict the pixel-wise segmentation masks of sounding objects in visual scenes precisely. Such studies have further demonstrated the intrinsic correlation between audio and visual events in in-the-wild videos, which inspires us to create an audio-language dataset anchored in visual information. 2.2 Audio-visual Dataset Large-scale audio-visual datasets are crucial for effective audio and video understanding. Two datasets are often involved in audiovisual learning: AudioSet and VGGSound. AudioSet [13] is a largescale audio-visual dataset with multiple audio events labelled for each audio clip. It contains over 2M 10-second audio clips. AudioSet is a manually annotated dataset, with the help of a well-structured hierarchical ontology consisting of 632 audio classes guided by literature and manual curation. VGGSound [7] comprises 200K 10second videos for 309 audio classes. This dataset was collected and annotated through an automated pipeline, with each video assigned only one label. In this paper, we aim to provide detailed description for audios, by exploiting both audio and visual cues. 2.3 Audio-language Learning The application of visual-language models in the audio-language arena marks a significant leap forward. Notably, [59] have adapted the CLIP model for audio-language contrastive learning, setting a precedent for innovative cross-modal research. Researchers are not merely focusing on extracting semantic information from audio MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. through tasks such as audio-text retrieval [25, 45], audio classification [20, 48], automatic audio captioning [37, 60], and audio question answering [14, 31]. They are also venturing into more nuanced aspects of auditory perception, including exploring temporal dynamics in sound through audio event detection [3, 28]. This broadening scope encompasses additional auditory attributes such as counting sounds within scenes [44] and classifying environments based on their acoustic characteristics[10]. Undoubtedly, it is paramount to construct a comprehensive, large-scale, high-quality and information-rich audio-language dataset. 2.4 Audio-language Dataset Audio-language tasks, including audio-text retrieval, audio captioning, audio question answering and text-guided audio generation, have greatly benefited from the availability of two widely-used audio captioning datasets: AudioCaps and Clotho. AudioCaps [24], a subset of AudioSet, consists of 50K 10-second-long audio clips, each with a single caption annotated. The annotators were provided with AudioSet tags as hints and videos if necessary. Clotho [11], on the other hand, comprises 6K audio clips lasting between 15 toseconds, each with five captions annotated through a three-step process involving captioning, grammar correction, and rating by human annotators. However, due to the human annotation process, these datasets are limited in size, expensive and time-consuming. LAION-Audio-630K [59] acquires audio and descriptions from online foley websites, including popular platforms like Freesound! and BBC Sound Effects”. WavCaps [38] utilizes ChatGPT to filter and paraphrase these raw descriptions, resulting in a dataset of 400K audio-text pairs with cleaned text data resembling human annotations. The sentence is mostly simple since there is often only one sound event in an audio clip. As a result, models trained on these datasets could only learn the category of sound. To enhance the comprehension capabilities of the audio-text model, we need a more diverse set of textual and audio data. 3 DATASET CONSTRUCTION To develop a large-scale, audio dataset with rich language descriptions, we base on the assumption that visual scene understanding serves as a strong prior. For instance, synchronized videos frequently showcase auditory cues, and visual information serves as a precise representation of the acoustic environment in which the sound happens. In an audio caption, it is desirable to incorporate sound attributes, location, and fine-grained labels. To achieve this, we can leverage publicly available tools or APIs to gather the necessary information for audio description and mutually verify the results. For instance, we can employ an object detection model to identify potential sources of sound, and an environmental classification model to extract scene categories. By extracting a wealth of information, we ensure the maximum coverage of accurate details, providing the language model with ample references. freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. = a . audio video visual-audio label ——WW———_—_*+ train [x¢:0.5921, ¥¢:0.5947, w:0.7879, h:0.3298] train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie a train pulling into a station. prompt ©) train_station [prob:0.657] aye passenger_car A train horn blows as a train passes by, creating a loud and distinct sound in a railway station. a train horn blows. train horning Figure 3: Automatic pipeline for Auto-ACD collection. We utilize four open-source computer vision models to extract visual clues from the middle frame of videos, and two open-source audio understanding models to analyze the entirety of the audio content. Consequently, we combine the labels from the original dataset, and leverage Large Language Models (LLMs) to interpret and paraphrase these components into the final description. 3.1 Tools or APIs Given one sample from existing large-scale video datasets, for example, AudioSet or VGGSound [7, 13], i.e., denoted as V = {f;a;y}, where f, a and y correspond to frame sequence, audio stream, and visual or audio labels, respectively. Our goal is to adopt a range of publicly available tools or APIs across the general AI community, ie, using off-the-shelf vision, language and audio models to construct language descriptions for audios, as shown in Fig. 3. In this section, we describe these tools in detail. 3.1.1 Image Captioning. We employ the off-the-shelf BLIP-2 [27] model, which obtains competitive results for image captioning. This tool has the ability to generate captions that encompass the entire image and accurately depict the primary subject or environment. In our case, we input the middle frame of the video into this model. 3.1.2 Object Detection. We use the pre-trained Grounding DINO model [33], to identify objects within the middle frame, and preserve all the detected entities along with their corresponding prediction confidence scores to ensure a comprehensive analysis. 3.1.3 Image Labeling. We adopt the pre-trained OpenAI CLIP [49] model for image classification. In this application, we utilize the prompt: “a photo of a {label}"" to generate textual embedding, leveraging the category ontology from ImageNet [9]. 3.1.4 Place Recognition. We employ the pre-trained PlaceCNN [63], to infer the environment context captured in videos. Given the robust correspondence between audio and visual signals, the environment depicted in the video is highly likely to represent the acoustic ambience in which the sound occurs. 3.1.5 Audio Tagging. We use the pre-trained PANNs [26] to predict the tags of sounds within the audio, and preserve the top three predictions with their confidence scores. This represents a crucial source of auditory temporal information, particularly for sounds emanating from entities not visible within the frame. 3.1.6 Audio Captioning. We use the existing AudioCaption [61] model, to generate concise and brief captions. These captions resemble the style of AudioCaps, focusing solely on the categorical information of audio events, devoid of any additional descriptive attributes about the sound. 3.1.7 Audio-visual Synchonisation. We employ the pre-trained Synchformer [22] to conduct synchronization detection between video and audio. This process could filter out samples consisting of irrelevant or unsynchronized video and audio content. In this case, we input both video and audio respectively into this model for analysis. 3.1.8 Existing Audio-Visual Labels. In addition to the predictions from models, we also incorporate the provided labels of existing datasets into our pipeline. For instance, VGGSound [7] gives a single label for each video, while AudioSet [13] provides multiple labels. These labels serve in the original dataset, offering accurate yet incomplete audio-visual information. 3.1.9 Summary. As for the language model, we use the OpenAI ChatGPT®, which demonstrates strong performance in reasoning and inductive summarization, to assemble the above-mentioned. descriptions or labels into comprehensive descriptions for audio. Many works, like BLIP-2[27], show that utilizing existing tools appropriately can significantly enhance the model’s performance. By leveraging audio-visual correspondence and the profound understanding capabilities of LLM, we generate precise audio captioning from the rich multi-modality clues acquired. In this case, we feed in a special prompt as shown in Section 3.2. 3.2 Caption Generation Based on the visual and acoustic clues present in the video, we craft a structured language paragraph, and use it to prompt ChatGPT to generate descriptions for audio. As illustrated in Fig. 4, the process begins with formulating the specific task and criteria for the desired outcome, followed by inputting seven distinctive audio-visual cues Shttps://openai.com/chatgpt --- --Auto-ACD [ Prompting ChatGPT to generate caption for audio I will give you some information from a video and an audio, this audio is separated from the video. There is a caption for an audio, simple audio caption, this sentence simply describe what happens in the audio. There are some audio tags: multiple audio tags, they indicate the audio events in this audio. number indicates the probability. The audio-visual label is dataset visual-audio label. Lextract a key frame from one video, and this is the image caption of this frame: image caption; this is the image label: imave labe\; this is the object detection: object detection; this is the place detection: place label. Now, please help me write one audio caption using common vocabulary and no more than 24 words, providing a description of what happened in the audio, and infer where the audio happened. You can refer the above information, and some visual information is inaccurate and can be ignored. please using the audio-visual label check the audio event in your caption. The sentence you write need to be like these following examples: A bell chimes thrice as birds chirp in the background in the for ‘A lawnmower engine buzzing and stopping to take a few breaks on the lawn. A machine being operated intermittently and people talking in the background in a factory. Figure 4: Detailed prompt provided to ChatGPT. For visualisation purposes, we use different colors to highlight diverse visual-audio cues. into the prompt, accompanied by their corresponding confidence score. Additionally, we provide three sentence examples from AudioCaps or Clotho as instruction. For visualisation purposes, we here use a colour-coded system to distinguish various cues. While generating captions, we explicitly ask ChatGPT to remove information that is inaudible, i.e., illogical and visually oriented elements, for example, colours. As a result, the large language model is able to analyze the scenario from all provided clues, and generate language description for audio, with sound category, and environment. The generated caption results are shown in Table. 1. 3.3 Dataset Filtering AudioSet is vast and diverse, while heavily marred by noise in many instances, for instance, gameplay live streams and explanatory videos. Conversely, VGGSound significantly emphasises the robust correlation between video and audio within the automated collection pipeline, thus requiring no further processing. As shown in Figure. 5, we formulate filtering criteria grounded in both the video-audio correspondence and the original labels. For each filter criterion, we conduct numerous trials followed by a manual verification, each filtering criterion achieves an accuracy rate exceeding 90%, resulting in the removal of 0.4 million videos in total. 3.3.1 Raw labels. AudioSet contains a plethora of explanatory videos with background music, wherein the visual and auditory information often do not correspond. Therefore, we eliminate videos from the multi-labels that encompass both speech and music. MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 1: The results of generated captions in Auto-ACD, with accurate content and ample surrounding information. Green and Yellow refer to “where"" and “how"" the audio sounds like. No. Generated Caption Loud pops and bangs resonate as timbales are being played, creating rhythmic music in a room. 2 Water gurgles and bubbles as a boat glides through, creating “a soothing and peaceful underwater ambience. 3 A woman speaks softly amidst the soothing sound of birds ‘chirping, creating a serene atmosphere in a garden. 4 A motorcycle engine idles before revving up, creating a loud sound in an urban environment. right Synchformer => tolerant right YW error ey video-audio music & speech © labels ——? labels analysis | ==> f others ‘ o Figure 5: Filtering process for AudioSet. We filter the dataset by assessing whether the video and audio are synchronized and analyzing the labels in the original dataset. 3.3.2 Audio-visual synchronisation. To obviate the possibility of fortuitous inference errors, we subject each video to five synchronization evaluations, featuring random variations in start time and offset, with a tolerance threshold established at 0.6 seconds. Synchformer[22] employs a 0.2s offset to ascertain the precise audiovisual synchronization, whereas we utilize a broader offset to determine the audio-visual correspondence. The outcomes are categorized as follows: (1) Predictions aligning with the ground truth are deemed “correct”; (2) Predictions that diverge from the ground truth while with a discrepancy within 0.6 seconds are designated as to be “tolerable”; (3) All other results are termed “error”. To preserve as much data as possible, videos classified as “error” in all five tests are removed from the dataset. 3.4 Dataset Statistics As depicted in Fig. 2, we collect 1.5 million audio-language pairs from AudioSet and VGGSound in total. To the best of our knowledge, Auto-ACD is the first million-level audio-language dataset to date, with train, validation and manually filtered test sets. Auto-ACD surpasses the other datasets in terms of data volume, average sentence length, and contains a relatively wide verbal vocabulary. LAIONAudio-630K[59] sources from user uploads, contains a plethora of noisy details, for instance, device and timestamps, and features an exceptionally extensive vocabulary. Additionally, Auto-ACD stands --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Frogs croaking and with insects Text vocalizing. Encoder 1d aleeAtt a} ede? | e8-e3ede? | eae? | eae? ea Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie prefix embeddings P! audio feature ef, Audio ' Encoder &A train running followed by a train horn. caption tokens Figure 6: Audio-language retrieval model and automatic audio captioning model frameworks. Similar to CLIP, the audiolanguage retrieval model consists of an audio encoder, text encoder, and contrastive loss. The automatic audio captioning model comprises a frozen audio encoder and language model, and a trainable mapping network. as the only audio-language dataset that encompasses environmental information, not only delineates the type and source of sounds but also specifies the location of their occurrence, increasing the richness of contextual details. In supplementary, we present a comparative analysis of captions from LAION-Audio-630K, WavCaps, and Auto-ACD for the same audio sample. Captions in LAION-Audio-630K and WavCaps are concise and contain minimal information beyond the audio tags. In particular, LAION-Audio-630K may include sentences that deviate from common sense, for example, describing “rapping a tree” for an audio tag of “rapping”. WavCaps, on the other hand, exhibits a monotonous sentence structure, such as “... sound can be heard”. In contrast, Auto-ACD features longer sentences that provide a richer depiction of the audio scenes. We conduct a manual check on randomly sampled 200 audiocaptions pairs from Auto-ACD, analyzing the clues from the different open-source tools and the generated captions. We define a clue that contradicts the audio to be erroneous, these tools possess high accuracy, the average accuracy is 81.3%. Furthermore, we conduct a manual check on randomly sampled 1000 audio-captions pairs, and find that 92.4% captions correspond with audio, just 5.3% incorrect words need to be modified, and only 4.4% captions contain inaudible information. These results indicate that our proposed approach enables high-quality, scalable caption generations, with few incorrect or inaudible information. 4 ARCHITECTURE We construct architectures targeting two general audio-language tasks, namely, audio-language contrastive learning, and automatic audio captioning, to further validate the effectiveness of Auto-ACD. In Section 4.1, we provide a detailed exposition of the architecture for audio-language contrastive learning. Further in Section 4.2, we introduce the framework for lightweight automatic audio captioning along with its loss function. 4.1 Audio-Language Constrastive Learning To validate the efficacy of our proposed dataset, we train an audiolanguage model with standard contrastive learning, e.g., infoNCE [49] loss, as shown in Fig.6. Specifically, we employ the pre-trained HTSAT [8] as the audio encoder, and the pre-trained RoBERTa [35] as the language encoder. Both encoders were initialised from the pre-trained CLAP model [59], and further finetuned on our dataset. We term our final model as Audio-Text Retrieval (ATR). Given an audio-text pair (a’, t'), we utilise audio encoder Aenc and text encoder Jenc to extract audio embedding e/, and text embedding el, respectively: fene(t') 4 = Aenc(a’), e} = The model is then trained with contrastive loss, wherein the paired audio and language embeddings are treated as positive, and unpaired ones as negative, with the following loss function: N DL exp where Tt represents the learnable temperature parameters. At training phase, we introduced word-level text masking, that is to randomly mask words within the sentences, before feeding into the text encoder. 4.2 Automatic Audio Captioning To demonstrate the effectiveness of our pre-trained audio backbone, we also use audio captioning for evaluation. Inspired by ClipCap [43] and AutoADs [16, 17], we adopt a lightweight audio captioning model, where both the audio backbone and language model (GPT-2) are fixed, and only a mapping network is trained, as shown in Fig. 6. --- --Auto-ACD MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 2: The audio-text retrieval results on AudioCaps, Clotho and ACD test sets. “basic”, “LA.” “Wav.” and “ACD"" refer to the combination of AudioCaps and Clotho (basic), LAION-Audio-630K (LA), WavCaps (Wav) and Auto-ACD (ACD), respectively. “ACDys” is a subset of Auto-ACD, curated from VGGSound. “ * FT” refers to fine-tuning the model on the target dataset. AudioCaps Test Clotho Test Auto-ACD Test Train Set Model Audio—Text Text— Audio R@1 R@10 R@1 R@Audio—Text Text—Audio Audio—Text Text—Audio R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@basic+LA.[59] HTSAT-RoBERTa 45.0 88.0 36.2 82.24.2 66.9 17.2 55.4 20.0 65.0 17.9 59.basic+Wav.[38] _ HTSAT-BERT 51.7 90.6 39.7 86.1 23.4 63.4 19.5 58.2 - - - basict ACDys HTSAT-RoBERTa 50.5 90.6 39.8 86.9 24.2 62.9 20.0 58.9 39.2 86.2 39.6 85.basictACD HTSAT-RoBERTa 53.7 91.7 39.5 85.4 17.7 52.6 15.3 52.1 47.1 91.2 49.0 92.basictACD*FT HTSAT-RoBERTa 56.3 93.9 42.7 88.5 26.2 67.5 21.7 61.7 - - - Given an audio-text pair (a',c'), we use the pre-trained audio encoder to extract audio features e) = Aenc(a'), and we convert the caption into a token sequence, cl... 13h where k indicates the maximal length of text. Then, we design a mapping network fmap to transform the extracted embedding into a set of prefix embeddings: P* = finap(€a)We take the prefix embedding set as the condition for predicting the next token with an auto-regressive language model. There fore, during training, we minimize the negative log-likelihood of predicting the correct word: N ¢ L= =D) Y tog po (6) | Phela--se5-1) i=l j=l where @ represents the trainable parameters. 5 EXPERIMENTS In this section, we evaluate three tasks, namely, audio-language retrieval, audio captioning and zero-shot classification. 5.1 Audio-language Retrieval 5.1.1 Dataset. We conduct audio-text retrieval experiments across several datasets: AudioCaps, Clotho, Auto-ACDys, and Auto-ACD. The distributions for the train, validation, and test sets in AudioCaps, Clotho, and Auto-ACD are 50K/495/975, 3.8K/1045/1045, and 1.5M/2K/1K data pairs, respectively. Auto-ACDys is a subset of Auto-ACD, containing 190K data pairs exclusively sourced from VGGSound. Notably, in the case of Clotho, and AudioCaps (validation and test set), each data pair consists of one audio sample accompanied by five corresponding captions, while the remaining data pairs only comprise one audio-caption pair. 5.1.2 Auto-ACD Benchmark. In addition to the Auto-ACD training set, we also randomly selected 2K data samples to form the validation set and 1K samples for the test set. We conduct a manual verification of the test set, by removing incorrect information from the language descriptions and rewriting inappropriate vocabulary expressions. This test set is used for evaluating both audio-language retrieval and automatic audio captioning tasks. 5.1.3 Metrics. In order to validate the rich and accurate information of our dataset, we compare the traditional metrics, Recall@k performance, on commonly used datasets, for example, AudioCaps and Clotho. We also adopt these metrics on the Auto-ACD test set, offering a comprehensive overview. 5.1.4 Training Details. We train our proposed Audio-Text Retrieval (ATR) model for 20 epochs, employing a batch size of 768, and utilizing the Adam optimizer with a warm-up phase, and an initial learning rate of 1e-4 with a cosine learning rate decay schedule. We use the same hyperparameters as those in the existing CLAP model configuration. The dimensions of both the audio encoder and text encoder output are 512. Additionally, we introduce 25% random masking on words in the sentences and randomly apply augmentations such as Noise and Gain to 50% of audio samples to enhance the model training. We further fine-tune the model on specific datasets, for example, Clotho and AudioCaps, with an initial learning rate of 2e-5 for 15 epochs. 5.1.5 Results. As shown in Table.2, we can draw the following key observations: (i) comparing with training on Laion-Audio-630K, training on our proposed Auto-ACDys dataset leads to a significant improvement in Recall@k metrics on AudioCaps and Auto-ACD benchmarks. (ii) training on Auto-ACD and fine-tuning on specific datasets, which is not applicable to Auto-ACD benchmark, results in a remarkable performance gain. This improvement is particularly evident when evaluating the model on the test set of AudioCaps, as AudioCaps is a subset of AudioSet and shares a similar data distribution with Auto-ACD. Such fine-tuning processes enable the model to acquire a more comprehensive understanding of both audio and text information, thus enhancing retrieval performance. (iii) on the Auto-ACD benchmark, characterized by a more diverse lexicon and abundant language description, training on Auto-ACD datasets significantly outperforms the model trained on Laion-Audio-630K. 5.2 Automatic Audio Captioning 5.2.1. Dataset. In addition to the datasets mentioned in Section 5.1, we also use the MACS dataset [36], which comprises 3.9K audio-text data pairs, with each audio accompanied by two to five captions and several audio tags. In total, we train the automatic audio captioning model utilizing a total of 58k data pairs from Clotho, AudioCaps and MACS, and evaluate on Clotho and Auto-ACD test set. --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. 5.2.2 Metrics. In addition to conventional captioning metrics, for example, Meteor [5], RougeL [30], Spider [34], we incorporate SentenceBERT [52] as additional evaluation metrics, that not solely rely on lexical alignment, but rather prioritize the semantic resemblance and accuracy of the captions’ content. 5.2.3 Training Details. We devise two mapping networks, MLP and transformer, and fine-tune the parameters of GPT during the training process. We set the number of prefixes to be 8, each with a dimension of 512. We train this audio captioning model on the MACS [36], Clotho and AudioCaps for 15 epochs with a batch size of 128 and an initial learning rate of 5e-4. In this task, we compare the audio encoder from our pre-trained audio-text retrieval model and the pre-trained CLAP [59], by only training the mapping network of both models on the benchmark datasets, namely, Clotho, and Auto-ACD. 5.2.4 Results. As shown in Table. 3, we can draw two observations: (i) the automatic audio captioning model, with the audio encoder initialised from our pre-trained audio-text retrieval model, shows improved performance across all evaluation metrics than baseline. (ii) there is a more pronounced outcome when evaluated on Auto-ACD: the baseline approach’s performance oversees a sharp decrease in the test set of Auto-ACD. We conjecture this is because the baseline features extracted from the CLAP model lack detailed descriptions of environmental information. While captioning model based on our pre-trained audio-text retrieval model shows a significant performance improvement, and is able to infer where the sound occurs precisely. This observation signifies that Auto-ACD showcases an extensive lexicon, enabling the portrayal of a given audio using various sentence structures. On the other hand, it illustrates that models trained on our dataset will deduce the context in which the sound emanates. Table 3: The automatic audio captioning results on Clotho and Auto-ACD test sets. “S-BERT” refers to SentenceBERT, “Env.” refers to whether the predicted captions contain environmental information. Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. Cloth CLAP 155 349 206 460 x one Ours 166 36.2 212 474 x CLAP 99 230 196 8&7 x Auto-ACD Ours 213 37.9 56.7 101 V 5.3. Zero-shot Classification 5.3.1 Dataset. Auto-ACD stands out for integrating its incorporation of environmental information within its text descriptions. Following the training on Auto-ACD, we conduct environmental classification in four distinct scenarios: (i) a collection of samples from the AudioSet evaluation set, annotated with child classes of ""Acoustic Environment"" within the AudioSet ontology, referred to as AudioSet Env. To prevent data leakage, here we exclusively utilize the model pre-trained on Auto-ACDys for this experiment; (ii) the urban acoustic scene dataset [19], known as DCASE 2020 Mobile, previously utilized in the DCASE 2020 challenge. (iii) the popular Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie urban sound event classification dataset, UrbanSound 8k [54]; (iv) the music genre classification dataset, GTZANGenres [57]. 5.3.2. Metrics. We approach zero-shot classification as an audiotext retrieval experiment, employing a conventional paraphrasing template: ""The sound in [environment label] / of [label]."" We utilize Recall@1 as the metric for evaluating the environment classification outcomes in this experiment. 5.3.3 Results. The experimental results, as illustrated in Table. 4, highlight the superior environmental recognition capability of ATR pre-trained on Auto-ACD in comparison to CLAP. Notably, on the AudioSet Env, our model significantly outperforms CLAP, even though we only utilize Auto-ACDys, for pre-training without any data leakage from AudioSet into our training dataset, further serving as a testament to the rich and accurate environmental information in Auto-ACD. The results on UrbanSound 8K and GTZANGenres shows that in addition to the audio events, the captions may also include more information, for example, diverse environment descriptions, fine-grained musical genres. wan Table 4: Zero-Shot Acoustic Environment Classification. refers to pre-training model on Auto-ACDys. “US-8K” refers to UrbanSound 8K. Model AudioSetEnv DCASE US-8K GTZANGenres CLAP 19.5 32.2 75.0 31.Ours 39.5"" 36.5 76.2 45.6
--- CONCLUSION ---
In this paper, we present an automatic pipeline for audio caption generation, accompanied by a large-scale and comprehensive audio captioning dataset comprising 1.5M data pairs. Furthermore, we evaluate the performance of various audio-language models on our dataset to authenticate the effectiveness, and provide a manually verified test set along with a benchmark for audio-language tasks. These experimental findings unveil the wealth of information and precise descriptions inherent in our data, facilitating the models to learn more robust audio-language representations. Owing to the fact that a portion of our dataset originates from VGGSound, procured through an automatic pipeline. The transformation from online videos to precise audio-language pairs has evolved into a thoroughly automated and replicable procedure. Consequently, the acquisition of an expanded corpus of audio-language datasets is now a straightforward endeavour. Furthermore, as opensource computer vision models and Large Language Models (LLMs) undergo continuous refinement and advancement, the capacity to extract more precise audio-visual indicators improves, subsequently enhancing the precision of inferences and the quality of paraphrasing the final audio captions. ACKNOWLEDGMENTS This work is supported by National Key R&D Program of China (No.2022ZD0161400). --- --Auto-ACD REFERENCES20.22, 23, Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovié, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. 2020. Self-supervised multimodal versatile networks. Advances in Neural Information Processing Systems 33 (2020), 25-37. Relja Arandjelovic and Andrew Zisserman. 2017. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision. 609-617. Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband, and Anthony T Chronopoulos. 2017. An overview of audio event detection methods from feature extraction to classification. Applied Artificial Intelligence 31, 9-10 (2017), 661-714. Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023. WhisperX: Time-Accurate Speech Transcription of Long-Form Audio. Proceedings of the INTERSPEECH Conference (2023). Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. 65-72. Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. 2021. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16867-16876. Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. 2020. Vggsound: A large-scale audio-visual dataset. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 721-725. Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2022. HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection. In Proceedings of the IEEE International Con‘ference on Acoustics, Speech and Signal Processing. 646-650. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 248-255. Biyun Ding, Tao Zhang, Chao Wang, Ganjun Liu, Jinhua Liang, Ruimin Hu, Yulin Wu, and Difei Guo. 2023. Acoustic scene classification: a comprehensive survey. Expert Systems with Applications (2023), 121902. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. 2020. Clotho: An audio captioning dataset. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 736-740. Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu. 2024. Avsegformer: Audio-visual segmentation with transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 12155-12163. Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 776-780. Di Hu Guangyao li, Yixin Xu. 2023. Multi-scale attention for audio question answering. Proceedings of the INTERSPEECH Conference (2023). Andrey Guzhoy, Federico Raue, Jorn Hees, and Andreas Dengel. 2022. Audioclip: Extending clip to image, text and audio. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 916-980. Tengda Han, Max Bain, Arsha Nagrani, Giil Varol, Weidi Xie, and Andrew Zisserman. 2023. AutoAD II: The Sequel - Who, When, and What in Movie Audio Description. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Tengda Han, Max Bain, Arsha Nagrani, Giil Varol, Weidi Xie, and Andrew Zisserman. 2023. AutoAD: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18930-18940. Tengda Han, Weidi Xie, and Andrew Zisserman. 2019. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 1-10. Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen. 2020. TAU Urban Acoustic Scenes 2020 Mobile, Development dataset. https://doi.org/10.5281/zenodo.Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. 2017. CNN architectures for large-scale audio classification. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 131-135. Xixi Hu, Ziyang Chen, and Andrew Owens. 2022. Mix and localize: Localizing sound sources in mixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10483-10492. Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: Efficient synchronization from sparse cues. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 5325-5329. Simon Jenni, Alexander Black, and John Collomosse. 2023. Audio-visual contrastive learning with temporal self-supervision. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 7996-8004.25, 26.30,32) 33,35, 36.38, 39)43,45,MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. Audiocaps: Generating captions for audios in the wild. In Proceedings of theConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 119-132. A Sophia Koepke, Andreea-Maria Oncescu, Joao F Henriques, Zeynep Akata, and Samuel Albanie. 2022. Audio retrieval with natural language queries: A benchmark study. IEEE Transactions on Multimedia 25 (2022), 2675-2685. Qiugiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. 2020. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 2880-2894. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning. 1973019742. Kang Li, Yan Song, Li-Rong Dai, lan McLoughlin, Xin Fang, and Lin Liu. 2023. Ast-sed: An effective sound event detection method based on audio spectrogram transformer. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, and Jun Xiao. 2023. Catr: Combinatorial-dependence audio-queried transformer for audio-visual video segmentation. In Proceedings of the 31st ACM International Conference on Multimedia. 1485-1494, Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. 150-157. Samuel Lipping, Parthasarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. 2022. Clotho-aqa: A crowdsourced dataset for audio question answering. In Proceedings of the 30th European Signal Processing Conference. 1140-1144. Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, and Weidi Xie. 2024. Annotation-free audio-visual segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 5604-5614. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303,05499 (2023). Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. 2017. Improved image captioning via policy gradient optimization of spider. In Proceedings of the IEEE International Conference on Computer Vision. 873-881. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.(2019). Irene Martin Morato and Annamaria Mesaros. 2021. Diversity and bias in audio captioning datasets. In Detection and Classication of Acoustic Scenes and Events. 90-94. Xinhao Mei, Xubo Liu, Mark D Plumbley, and Wenwu Wang. 2022. Automated audio captioning: an overview of recent progress and new challenges. EURASIP Journal on Audio, Speech, and Music Processing 2022, 1 (2022), 26. Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. 2023. WavCaps: A chatGPTassisted weakly-labelled audio captioning dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395 (2023). Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2630-2640. Shentong Mo and Pedro Morgado. 2022. A closer look at weakly-supervised audiovisual source localization. Advances in Neural Information Processing Systems(2022), 37524-37536. Shentong Mo and Yapeng Tian. 2023. Audio-visual grouping network for sound localization from mixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10565-10574. Shentong Mo and Yapeng Tian. 2023. AV-SAM: Segment anything model meets audio-visual localization and segmentation. arXiv preprint arXiv:2305.(2023). Ron Mokady, Amir Hertz, and Amit H Bermano. 2021. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734 (2021). Michael Nigro and Sridhar Krishnan. 2023. SARdBScene: Dataset and Resnet Baseline for Audio Scene Source Counting and Analysis. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Andreea-Maria Oncescu, A Koepke, Joao F Henriques, Zeynep Akata, and Samuel Albanie. 2021. Audio retrieval with natural language queries. arXiv preprint arXiv:2105,02192 (2021). Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia.50.52) 53)55,Andrew Owens and Alexei A Efros. 2018. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision. 631-648. Kamalesh Palanisamy, Dipika Singhania, and Angela Yao. 2020. Rethinking CNN models for audio classification. arXiv preprint arXiv:2007.11154 (2020). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etal. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning. 8748-8763. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAlI blog 1,8 (2019), 9. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1, 2 (2022), 3. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684-10695. Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. 2014. A dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM International Conference on Multimedia. 1041-1044. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35 (2022), 25278-25294. Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, and Nick Barnes. 2023. Learning audio-visual source localization via false negative aware contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6420-6429. 57, 58,60,62, 63,65, 66. Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie George Tzanetakis and Perry Cook. 2002. Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing 10, 5 (2002), 293-302. Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. 2022. Wav2clip: Learning robust audio representations from clip. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 45634567. Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Xuenan Xu, Mengyue Wu, and Kai Yu. 2022. A comprehensive survey of automated audio captioning. arXiv preprint arXiv:2205.05357 (2022). Xuenan Xu, Zhiling Zhang, Zelin Zhou, Pingyue Zhang, Zeyu Xie, Mengyue Wu, and Kenny Q Zhu. 2023. Blat: Bootstrapping language-audio pre-training based on audioset tag-guided synthetic data. In Proceedings of the 31st ACM International Conference on Multimedia. 2756-2764. Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, and Yejin Choi. 2022. Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics. 4492-4507. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 6 (2017), 1452-1464. Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. 2022. Audiovisual segmentation. In Proceedings of the European Conference on Computer Vision. 386-403. Xiao Zhou, Yujie Zhong, Zhen Cheng, Fan Liang, and Lin Ma. 2023. Adaptive sparse pairwise loss for object re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19691-19701. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2024. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems 36 (2024). --- --Auto-ACD 7 DATASET ANALYSIS In this section, we conduct a more thorough analysis of the proposed dataset, Auto-ACD. In Section 7.1 and Section 7.2, we compare Auto-ACD with existing audio-language datasets, and discuss the necessity for data filtering. In Section 7.3, we present the distribution of vocabulary. In Section 7.4 and Section 7.5, we compare the captions among Laion-Audio-630K, WavCaps and Auto-ACD, and manually check the quality of a subset of Auto-ACD. In Section 7.6, we present additional examples from Auto-ACD. 7.1 Dataset Statistics In total, we have collected 1.5 million audio samples, each with a duration of 10 seconds, accompanied by one detailed caption. As indicated in Table 5, in comparison to other datasets, AutoACD not only surpasses them significantly in terms of volume, but also contains a longer average sentence length. It stands as the only large-scale dataset that includes environmental information within its descriptions. Laion-Audio-630k may possess a higher vocabulary count, but the majority of its lexicon comprises useruploaded device information and timestamps, which are irrelevant to the audio content. Table 5: Comparation with other audio caption datasets. “Length” and “# Vocab.” refer to average length and vocabulary. “Env.” and “Auto.” refer to environmental information and automatic pipeline, respectively. Dataset Quantity Length # Vocab. Env. Auto. AudioCaps [24] 57K 8.8 5K x x Clotho [11] 30K 11.3, 4K ix LAION-Audio-630K [59] 630K 7.3 311K x v WavCaps [38] 400K 78 29K x v Auto-ACD (ours) 15M 18.1 22K dov 7.2 Dataset Filtering Our data collection procedure relies on strong audio-visual correspondence. However, many entries within AudioSet contain considerable noise, posing challenges to achieving such coherence, for instance, videos with background music, serene speeches, videos depicting gameplay or software tutorials. Such videos typically only encompass two types of audio events: speech and music. Consequently, the generated captions often contain sparse information and exhibit high error rates. We employ an analysis of audio-visual labels and synchronization model to filter these samples. The specific details of this filtering process are described in Section 3.3 of the main text. In Figure. 7, we present some examples of video frame sequences and the outcomes of audio ASR (Automatic Speech Recognition) by WhisperX [4] for the excluded data. The majority of discarded entries are due to the audio and video are not unrelated or not synchronized. 7.3 Dataset Corpus We visualize the captions for our dataset with word cloud. As depicted in Figure. 8, and the common audio tags, man speak and MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. music play still predominate in frequency within our data. It is noteworthy that terms describing settings, such as small room and music studio, also emerge with considerable frequency. These are a plethora of audio events, such as birds chirping, engine idling and water splashing, further demonstrating the diverse audio events in Auto-ACD. “Anyways, so I run pretty much full arms except for drums of war and one point in...” “Would be equal to 3 divided by 5. So now looking at the tangent of theta. There's two different ways we can do this now.”” [only background music] [only background music] Figure 7: Samples deleted in filter processing. The text on the right side represents transcriptions of speech from the audio in the video, processed using WhisperX. Br, Stn it: delivers’ 11 water splashes man man” Bateioretely epee cn fid 4,gaEs fiusic Creating to by < al oS. sesoFigure 8: Corpus in Auto-ACD. The higher the frequency of occurrence, the larger the font size of the respective word. 7.4 Dataset Comparison In Table. 6, we show example captions from LAION-Audio-630K, WavCaps, and Auto-ACD for the same audio sample. Since the original sounds of the three datasets overlap, we select the descriptions of the same audio in different datasets for comparison. Specifically, LAION-Audio-630K employs a keyword-to-caption model to transform the tag labels into captions. WavCaps utilizes ChatGPT to rephrase the tag labels into simple captions. The captions in LAIONAudio-630K and WavCaps tend to be concise, and contain minimal information beyond the audio tags. In particular, the captions of LAION-Audio-630K are short, and may include information deviate from common sense, for instance, “rapping a tree”. WavCaps, on the other hand, exhibits a simple sentence structure, such as “... sound can be heard”. The captions of these two datasets hardly present --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie Table 6: Caption comparison with LAION-Audio-630K and WavCaps, “LA.”, “WavC.” and “ACD” refer to LAION-Audio-630K, ‘WavCaps and Auto-ACD, respectively. No. Dataset Generated Caption LA. A person is rapping a tree. 1. WavC. Music plays with a man rapping. ACD. A woman sings while hip-hop music plays in the background, creating a rapping audio event in a computer room. LA. a slushy water lily. 2. WavC. Stream noise, crowd and splashing sounds. ACD. A crowd of people yells and cheers as water sloshes in the background at a water park. LA. a truck with a siren and a fire engine in an emergency. 3. WavC. A fire engine siren is heard. ACD. An emergency vehicle siren blares loudly as a fire truck rushes through a residential neighbourhood. LA. a vehicle with a medium frequency of engine idling. 4. WavC. A medium engine sound can be heard. ACD. A medium-sized engine is idling and vibrating, while an adult male speaks in the background near a running vehicle. more information than audio tags. In contrast, Auto-ACD features longer sentences and provide more comprehensive descriptions of the audio scenes. 7.5 Dataset Quality Check To evaluate the quality of Auto-ACD, we conduct a manual check on randomly sampled 1000 audio-captions pairs. As shown in Table 7, (i) we assess the correspondence between our generated captions and original audio; (ii) we revise the incorrect words in the final captions and calculate the percentage of modified vocabulary; (iii) we calculate the ratio of captions that contain inaudible information, for example, colours. The results, high correspondence and low erroneous words percentage, indicate that our proposed approach enables high-quality, scalable caption generations, with minimal incorrect information or inaudible information. Table 7: Statistics of Manual Check on Auto-ACD. Correspondence Modification Inaudibility Statistics 0.924 0.053 0.In addition, we further conduct manual check on each of the steps during caption generation, i.e, the various tools used for generating visual clues. We conduct a manual check on randomly sampled 200 audio-captions pairs, to analyse the quality of clues from six different open-source tools and the generated captions. As shown in Table 8, we define a clue that contradicts the audio as incorrect, and we calculate the accuracy of each tool and caption and count the number of correct clues in each sample. These tools possess high accuracy, with a high average accuracy at 81.3% and the highest accuracy at 91.5%. we find that 94.0% of the samples contain at least four correct clues. The fact that 88.0% of generated captions align with the audio further demonstrates that the LLM is capable of removing incorrect information and producing coherent audio captions. 7.6 Dataset Visualization As shown in Table. 9, we show more generated captions for audios from VGGSound and AudioSet. Note that, we present the video sequences to demonstrate how visual information can assist the language description for audio. It can be observed that, the captions in Auto-ACD not only accurately depict sound events but also infer additional information based on visual priors, that can also be inferred from audios, for example, (i) environmental details, for instance, “a lively performance arena"", “in a music studio"" and “a peaceful zen garden’, (ii) sound attributes like “A civil defense siren blares loudly"" and “music plays in the background’, (iii) sound variations, for example, “motorcycle engine revs up and down"" and “a car speeds down a dirt track"". Table 8: Accuracy of Manual Check on Open-source Tools. “Caption.” refers to AudioCaption model. BLIP-2 DINO CLIP Place365 Caption. PANNs Accuracy 0.915 0.755 0.805 0.725 0.770 0.--- --Auto-ACD MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 9: Data visualization in Auto-ACD. In each sample, the top line showcases the video frame sequence, the bottom line presents the corresponding audio caption. The sound events in the caption are highlighted in bold text, and environmental information is indicated in italics text. No. Generated Caption Music plays as a crowd cheers and a band performs on stage with vibrant lights in a lively performance arena.
"	"--- ABSTRACT ---
출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 초록 최근 AI 커뮤니티는 대규모 멀티모달 데이터 세트에 의해 주도되는 강력한 기초 모델을 개발하는 데 상당한 진전을 이루었습니다. 그러나 오디오 표현 학습의 경우 기존 데이터 세트는 다음과 같은 측면에서 한계가 있습니다. 불충분한 볼륨, 단순한 콘텐츠 및 힘든 수집 절차. © 2024 저작권은 소유자/저자가 보유합니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0686-8/24/10...$15.https://doi.org/10.1145/3664647.MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 고품질 자막이 있는 오디오 데이터 세트를 구축하기 위해 비디오 프레임, 오디오 스트림과 같은 다중 모달 입력을 활용하는 혁신적이고 자동화된 접근 방식을 제안합니다. 구체적으로, 150만 개 이상의 오디오-텍스트 쌍으로 구성된 AutoACD라는 대규모 고품질 오디오-언어 데이터 세트를 구성합니다. 사전 학습된 일련의 모델 또는 API를 활용하여 오디오-비주얼 동기화를 결정하고, 이미지 자막, 객체 감지 또는 특정 비디오에 대한 오디오 태그를 생성합니다. 그런 다음 LLM을 사용하여 추출된 다중 모달 단서에 따라 각 오디오에 대한 일치하는 자막을 의역합니다. 제안된 데이터 세트의 효과를 입증하기 위해, 우리는 우리 데이터 세트에서 널리 사용되는 모델을 훈련시키고 오디오-언어 검색, 오디오 캡션, 제로샷 분류와 같은 다양한 다운스트림 작업에서 성능이 향상되었음을 보여줍니다. 또한, 우리는 환경 정보를 사용하여 새로운 벤치마크를 설정하고 오디오-텍스트 작업에 대한 벤치마크를 제공합니다. CCS 개념 • 컴퓨팅 방법론 → 컴퓨터 비전; • 정보 시스템 → 데이터 구조; 정보 검색. 키워드 오디오-언어 데이터 세트, 오디오-언어 표현 학습, 오디오 캡션 ACM 참조 형식: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie. 2024. Auto-ACD: 오디오-언어 표현 학습을 위한 대규모 데이터 세트. 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최된 제32회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;24)의 회의록. ACM, 뉴욕, 뉴욕, 미국, 13페이지. https://doi.org/10.1145/3664647.
--- INTRODUCTION ---
최근 문헌에서 CLIP[49], GPT[50]의 변형, DALL-E 2[51] 및 Stable Diffusion[53]과 같은 기초 모델은 다양한 이해 및 생성 작업에서 엄청난 성공을 거두었습니다. 아키텍처 또는 알고리즘 설계가 다르지만 MMC4[66], LAION[55], HowTo100M[39]과 같은 대규모 멀티모달 데이터 세트와 같은 공통 기반에 있으며, 이는 모델 중심에서 데이터 중심 표현 학습으로의 새로운 전환을 나타냅니다. 전자는 사전 결정된 데이터 예산의 제약 내에서 모델 설계의 경계를 넓히는 것을 고려하는 반면, 후자는 확장 가능한 방식으로 대규모 고품질 데이터 세트를 큐레이팅하는 데 중점을 둡니다. 오디오 커뮤니티에서는 그림 2에서 볼 수 있듯이 오디오-언어 데이터 세트를 구성하기 위한 최근 노력이 있었습니다. 그러나 기존 데이터 세트는 힘들고 복잡한 수집 프로세스와 텍스트의 단순한 설명이라는 두 가지 한계에 직면할 수 있습니다. 한편, 일반적으로 1~3개의 사운드 이벤트로 구성된 오디오를 담고 있으며 인간 주석자가 제공한 고품질 텍스트 설명이 함께 제공되는 Clotho[11]와 AudioCaps[24]. 이는 확장하기가 분명히 어렵습니다. 반면, LAION-Audio-630K[59]와 WavCaps[38]는 온라인 폴리 웹사이트에서 대량의 원시 데이터를 수집한 다음 문장 템플릿이나 키워드-캡션 모델을 사용하여 원래 오디오 레이블을 자유형 문장으로 변환합니다. 그 결과 언어 설명이 간단한 프롬프트나 사운드 태그보다 추가 정보를 제공하지 않는다는 것은 분명합니다. 따라서 모델은 #Vocab을 훈련했습니다. 311K 자동 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 길이 18.11.8.7.7.29K 22K 5K 4K 수량 30K 57K 400K 630K 1.5M Env. WavCaps LAION-Audio-630K - Clotho AudioCaps Auto-ACD(저희 제품) 그림 2: 다른 오디오 캡션 데이터 세트와의 비교. &quot;Length&quot;와 &quot;# Vocab.&quot;은 평균 길이와 어휘를 나타냅니다. &quot;Env.&quot;와 &quot;Auto.&quot;는 각각 환경 정보와 자동 파이프라인을 나타냅니다. 이러한 데이터 세트에서는 강력한 오디오-언어 표현을 학습할 수 없습니다. 이 논문에서는 최소한의 수동 작업으로 대규모, 고품질, 오디오-언어 데이터 세트를 구성하기 위한 최근의 노력을 소개합니다. 이 데이터 세트는 Auto-ACD(자동 수집을 통한 오디오 캡션 데이터 세트)라고 하며, 방대한 오디오-텍스트 쌍(1.5M), 긴 텍스트(단어) 및 다양한 어휘(23K)를 포함합니다. 구체적으로, 모범적인 오디오 캡션은 네 가지 유형의 정보를 캡슐화해야 합니다. &#39;무엇&#39; - 인지된 소리의 특성, &#39;누구&#39; - 소리를 생성하는 개체, &#39;어떻게&#39; - 소리의 특성, &#39;어디&#39; - 소리가 발생하는 위치입니다. 우리의 핵심 통찰력은 시각적 장면에 대한 포괄적인 이해가 귀중한 정보 소스 역할을 할 것으로 예상되며 때로는 오디오 콘텐츠를 이해하는 데 필요하다는 것입니다.따라서 기존 오디오-비주얼 데이터 세트(예: VGGSound [7], AudioSet [13])의 강력한 오디오-비주얼 대응에 대한 사전을 기반으로 Auto-ACD를 구축합니다.특히, 비전, 언어 및 오디오 모델과 같이 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 사용하여 주어진 비디오 데이터 세트의 오디오 스트림에 대한 포괄적인 언어 설명을 생성하는 자동 파이프라인을 시작합니다.마지막으로, 대규모 언어 모델(LLM)을 사용하여 모든 출력을 집합적으로 동화하고 비논리적인 정보를 식별하여 제거하고 오디오에 대한 포괄적인 설명을 생성합니다.결과적으로 이러한 설명은 소리의 유형과 소스를 묘사할 뿐만 아니라 청각 속성과 발생의 특정 위치도 설명합니다.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 예를 들어, Auto-ACD의 텍스트 설명에서 학습한 오디오 이벤트와 주변 정보와 같은 청각 표현을 종합적으로 검증하기 위해 네 가지 관점에서 실험을 수행했습니다.첫째, InfoNCE 손실[18, 46, 65]을 사용하여 공동 오디오-언어 표현 학습을 시작하고 오디오와 언어 간의 검색 작업을 통해 모델을 평가하여 기존 데이터 세트에 비해 눈에 띄는 개선을 보였습니다.둘째, 제로샷 분류 실험을 수행하여 데이터 세트로 환경 정보를 학습하는 데 효과적인지 보여주었습니다.셋째, 사전 훈련된 오디오 백본과 GPT2[50] 간의 가벼운 매핑 네트워크를 훈련하여 오디오-언어 생성 작업, 특히 자동 오디오 캡션에 대한 벤치마킹을 수행하여 널리 사용되는 벤치마크인 Clotho[11]에서 뛰어난 성능을 보였습니다.넷째, 테스트 세트를 수동으로 필터링하고 오디오-언어 작업을 위한 새로운 벤치마크를 도입했습니다.이 벤치마크는 모델이 단순한 오디오 태그를 넘어 환경 및 세분화된 소리 범주와 같은 정보를 파악할 수 있는 능력을 평가하여 이 분야의 미래 연구를 위한 기준을 설정했습니다. 2
--- RELATED WORK ---
2. 시청각 학습 시청각 이벤트는 종종 야외 비디오에서 동시에 발생하여 소리와 이미지 사이에 깊은 연결을 설정합니다.[1, 2, 23, 47]은 시청각 자기 지도 학습을 사용하여 시청각 대응 관계를 활용하여 표현 학습을 향상시킵니다.특히,[15, 58, 62]는 이러한 대응 관계에 기반하여 오디오-텍스트 표현을 학습합니다.시청각 현지화[6, 21, 40, 41, 56]는 비디오 내에서 시각적 음원의 위치를 식별하는 데 집중합니다.시청각 분할[12, 29, 32, 42, 64]은 시각적 장면에서 소리가 나는 객체의 픽셀 단위 분할 마스크를 정확하게 예측하는 것을 목표로 합니다.이러한 연구는 야외 비디오에서 오디오 및 시각적 이벤트 간의 본질적인 상관 관계를 추가로 입증했으며, 이는 시각 정보에 기반한 오디오-언어 데이터 세트를 만드는 데 영감을 주었습니다. 2. 시청각 데이터 세트 대규모 시청각 데이터 세트는 효과적인 오디오 및 비디오 이해에 필수적입니다. 시청각 학습에는 종종 AudioSet과 VGGSound라는 두 가지 데이터 세트가 관련됩니다. AudioSet[13]은 각 오디오 클립에 대해 레이블이 지정된 여러 오디오 이벤트가 있는 대규모 시청각 데이터 세트입니다. 2M개가 넘는 10초 오디오 클립이 포함되어 있습니다. AudioSet은 문헌과 수동 큐레이션에 따라 안내되는 632개의 오디오 클래스로 구성된 잘 구조화된 계층적 온톨로지의 도움을 받아 수동으로 주석이 달린 데이터 세트입니다. VGGSound[7]는 309개의 오디오 클래스에 대한 200K개의 10초 비디오로 구성되어 있습니다. 이 데이터 세트는 자동화된 파이프라인을 통해 수집되고 주석이 달렸으며 각 비디오에는 하나의 레이블만 지정되었습니다. 이 논문에서는 오디오 및 시각적 신호를 모두 활용하여 오디오에 대한 자세한 설명을 제공하는 것을 목표로 합니다. 2.3 오디오-언어 학습 오디오-언어 분야에서 시각 언어 모델을 적용하는 것은 큰 도약을 의미합니다. 특히, [59]는 오디오-언어 대조 학습을 위해 CLIP 모델을 채택하여 혁신적인 교차 모달 연구의 선례를 만들었습니다. 연구자들은 오디오-텍스트 검색[25, 45], 오디오 분류[20, 48], 자동 오디오 캡션[37, 60], 오디오 질의응답[14, 31]과 같은 작업을 통해 오디오에서 의미 정보를 추출하는 데만 집중하지 않습니다. 그들은 또한 오디오 이벤트 감지[3, 28]를 통해 소리의 시간적 역학을 탐구하는 것을 포함하여 청각 지각의 보다 미묘한 측면에 도전하고 있습니다. 이 확장된 범위에는 장면 내의 소리 계산[44] 및 음향 특성에 따라 환경 분류[10]와 같은 추가 청각 속성이 포함됩니다. 의심할 여지 없이 포괄적이고 대규모, 고품질의 정보가 풍부한 오디오-언어 데이터 세트를 구성하는 것이 가장 중요합니다. 2.4 오디오 언어 데이터 세트 오디오 텍스트 검색, 오디오 캡션, 오디오 질의응답 및 텍스트 가이드 오디오 생성을 포함한 오디오 언어 작업은 널리 사용되는 두 가지 오디오 캡션 데이터 세트인 AudioCaps와 Clotho의 가용성으로부터 큰 이점을 얻었습니다.AudioSet의 하위 세트인 AudioCaps[24]는 50K 10초 길이의 오디오 클립으로 구성되며 각각 단일 캡션에 주석이 달려 있습니다.주석 작성자에게는 필요한 경우 힌트와 비디오로 AudioSet 태그가 제공되었습니다.반면에 Clotho[11]는 15~20초 동안 지속되는 6K 오디오 클립으로 구성되며 각각 캡션, 문법 교정 및 인간 주석자의 평가를 포함하는 3단계 프로세스를 통해 주석이 달린 5개의 캡션이 달려 있습니다.그러나 인간 주석 프로세스로 인해 이러한 데이터 세트는 크기가 제한적이고 비용이 많이 들고 시간이 많이 걸립니다. LAION-Audio-630K [59]는 Freesound¹ 및 BBC Sound Effects²와 같은 인기 있는 플랫폼을 포함한 온라인 폴리 웹사이트에서 오디오와 설명을 수집합니다.WavCaps [38]는 ChatGPT를 사용하여 이러한 원시 설명을 필터링하고 의역하여 인간의 주석과 유사한 정리된 텍스트 데이터가 있는 400K 오디오-텍스트 쌍의 데이터 세트를 생성합니다.오디오 클립에 종종 하나의 사운드 이벤트만 있기 때문에 문장은 대부분 간단합니다.결과적으로 이러한 데이터 세트에서 학습된 모델은 사운드 범주만 학습할 수 있었습니다.오디오-텍스트 모델의 이해 능력을 향상시키려면 보다 다양한 텍스트 및 오디오 데이터 세트가 필요합니다.데이터 세트 구성 풍부한 언어 설명이 포함된 대규모 오디오 데이터 세트를 개발하기 위해 시각적 장면 이해가 강력한 사전 역할을 한다는 가정을 기반으로 합니다.예를 들어, 동기화된 비디오는 종종 청각적 단서를 보여주고 시각 정보는 사운드가 발생하는 음향 환경을 정확하게 표현합니다. 오디오 캡션에서는 사운드 속성, 위치 및 세분화된 레이블을 통합하는 것이 좋습니다. 이를 위해 공개적으로 사용 가능한 도구나 API를 활용하여 오디오 설명에 필요한 정보를 수집하고 결과를 상호 검증할 수 있습니다. 예를 들어, 객체 감지 모델을 사용하여 잠재적인 사운드 소스를 식별하고 환경 분류 모델을 사용하여 장면 범주를 추출할 수 있습니다. 풍부한 정보를 추출하여 정확한 세부 정보를 최대한 적용하고 언어 모델에 충분한 참조를 제공합니다. ¹https://freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie BLIP-GroundingDINO CLIP 프레임 Placeaudio 비디오 visual-audio 레이블 AudioCaption PANNS 역으로 들어오는 기차. train [x:0.5921, y:0.5947, w:0.7879, h:0.3298] Passenger_car train station [prob:0.657] a train horn blows. train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] train horning prompt 기차가 지나갈 때 기차 경적이 울리며 기차역에서 크고 뚜렷한 소리가 납니다. 그림 3: Auto-ACD 수집을 위한 자동 파이프라인. 우리는 4개의 오픈소스 컴퓨터 비전 모델을 활용하여 비디오의 중간 프레임에서 시각적 단서를 추출하고, 2개의 오픈소스 오디오 이해 모델을 활용하여 전체 오디오 콘텐츠를 분석합니다. 결과적으로 우리는 원본 데이터 세트의 레이블을 결합하고, LLM(Large Language Models)을 활용하여 이러한 구성 요소를 해석하고 최종 설명으로 의역합니다. 3.1 도구 또는 API 예를 들어 AudioSet 또는 VGGSound [7, 13]와 같은 기존 대규모 비디오 데이터 세트에서 하나의 샘플이 주어지면 V = {f; a; y}로 표시하고 여기서 f, a 및 y는 각각 프레임 시퀀스, 오디오 스트림 및 시각적 또는 오디오 레이블에 해당합니다. 우리의 목표는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 채택하는 것입니다. 즉, 그림 3과 같이 기성품 비전, 언어 및 오디오 모델을 사용하여 오디오에 대한 언어 설명을 구성하는 것입니다. 이 섹션에서는 이러한 도구에 대해 자세히 설명합니다. 3.1.1 이미지 캡션. 기성품 BLIP-2 [27] 모델을 사용하여 이미지 캡션에 대해 경쟁력 있는 결과를 얻습니다. 이 도구는 전체 이미지를 포괄하고 주요 피사체 또는 환경을 정확하게 묘사하는 캡션을 생성할 수 있습니다. 우리의 경우 비디오의 중간 프레임을 이 모델에 입력합니다. 3.1.2 객체 감지. 우리는 미리 훈련된 Grounding DINO 모델[33]을 사용하여 중간 프레임 내의 객체를 식별하고, 포괄적인 분석을 보장하기 위해 감지된 모든 엔터티와 해당 예측 신뢰도 점수를 보존합니다.3.1.3 이미지 레이블링.우리는 이미지 분류를 위해 미리 훈련된 OpenAI CLIP[49] 모델을 채택합니다.이 애플리케이션에서 우리는 프롬프트: &quot;[레이블}의 사진&quot;을 사용하여 ImageNet[9]의 카테고리 온톨로지를 활용하여 텍스트 임베딩을 생성합니다.3.1.4 장소 인식.우리는 미리 훈련된 PlaceCNN[63]을 사용하여 비디오에서 캡처된 환경 맥락을 추론합니다.오디오와 시각 신호 간의 강력한 대응 관계를 감안할 때, 비디오에 묘사된 환경은 사운드가 발생하는 음향 분위기를 나타낼 가능성이 매우 높습니다.3.1.5 오디오 태그.우리는 미리 훈련된 PANN[26]을 사용하여 오디오 내의 사운드 태그를 예측하고, 신뢰도 점수와 함께 상위 3개의 예측을 보존합니다. 이는 특히 프레임 내에서 볼 수 없는 엔터티에서 나오는 소리의 경우 청각적 시간 정보의 중요한 원천입니다.3.1.6 오디오 캡션.기존 AudioCaption[61] 모델을 사용하여 간결하고 간략한 캡션을 생성합니다.이러한 캡션은 AudioCaps의 스타일과 유사하여 사운드에 대한 추가 설명적 속성이 없는 오디오 이벤트의 범주적 정보에만 초점을 맞춥니다.3.1.7 시청각 동기화.사전 훈련된 Synchformer[22]를 사용하여 비디오와 오디오 간의 동기화 감지를 수행합니다.이 프로세스는 무관하거나 동기화되지 않은 비디오 및 오디오 콘텐츠로 구성된 샘플을 필터링할 수 있습니다.이 경우 분석을 위해 비디오와 오디오를 각각 이 모델에 입력합니다.3.1.8 기존 시청각 레이블.모델의 예측 외에도 기존 데이터 세트의 제공된 레이블도 파이프라인에 통합합니다.예를 들어 VGGSound[7]는 각 비디오에 대해 단일 레이블을 제공하는 반면 AudioSet[13]은 여러 레이블을 제공합니다. 이러한 레이블은 원래 데이터 세트에서 정확하지만 불완전한 오디오-비주얼 정보를 제공합니다.3.1.9 요약.언어 모델의 경우 추론 및 귀납적 요약에서 강력한 성능을 보이는 OpenAI ChatGPT3를 사용하여 위에 언급된 설명 또는 레이블을 오디오에 대한 포괄적인 설명으로 조립합니다.BLIP-2[27]와 같은 많은 연구에서는 기존 도구를 적절히 활용하면 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다.오디오-비주얼 대응 관계와 LLM의 심오한 이해 능력을 활용하여 획득한 풍부한 다중 모달 단서에서 정확한 오디오 캡션을 생성합니다.이 경우 섹션 3.2에 표시된 대로 특별한 프롬프트를 입력합니다.3.2 캡션 생성 비디오에 있는 시각적 및 음향적 단서를 기반으로 구조화된 언어 단락을 만들고 이를 사용하여 ChatGPT에서 오디오에 대한 설명을 생성합니다. 그림 4에서 볼 수 있듯이, 프로세스는 원하는 결과에 대한 구체적인 작업과 기준을 공식화하는 것으로 시작한 다음, 7가지 독특한 시청각적 단서를 입력합니다.3 https://openai.com/chatgpt Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 오디오에 대한 캡션을 생성하도록 ChatGPT 프롬프트하기 비디오와 오디오에서 몇 가지 정보를 제공합니다. 이 오디오는 비디오와 분리되어 있습니다. 오디오에 대한 캡션이 있습니다. 간단한 오디오 캡션으로, 이 문장은 오디오에서 발생하는 일을 간단히 설명합니다. 몇 가지 오디오 태그가 있습니다. 여러 오디오 태그는 이 오디오의 오디오 이벤트를 나타냅니다. 숫자는 확률을 나타냅니다. 시청각 레이블은 데이터 세트 시각적 오디오 레이블입니다. 한 비디오에서 키 프레임을 추출하고, 이것은 이 프레임의 이미지 캡션입니다. 이미지 캡션; 이것은 이미지 레이블입니다. 이미지 레이블; 이것은 객체 감지입니다. 객체 감지; 이것은 장소 감지입니다. 장소 레이블입니다. 이제 일반적인 어휘와 24단어를 넘지 않는 오디오 캡션 하나를 작성하여 오디오에서 발생한 일에 대한 설명을 제공하고 오디오가 발생한 위치를 추론하도록 도와주세요. 위의 정보를 참조할 수 있으며, 일부 시각적 정보는 부정확하여 무시할 수 있습니다. 오디오-비주얼 레이블을 사용하여 캡션의 오디오 이벤트를 확인하세요. 작성한 문장은 다음 예와 같아야 합니다. 숲에서 새들이 지저귀는 동안 종이 세 번 울립니다. 잔디 깎는 기계 엔진이 윙윙거리며 잔디밭에서 몇 번 휴식을 취하기 위해 멈춥니다. 공장에서 간헐적으로 작동하는 기계와 배경에서 사람들이 이야기합니다. 표 1: 정확한 콘텐츠와 충분한 주변 정보가 포함된 Auto-ACD에서 생성된 캡션의 결과. 녹색과 노란색은 오디오가 &quot;어디&quot;와 &quot;어떻게&quot; 들리는지 나타냅니다. 아니요. 생성된 캡션 1. 2. 3. 4. 팀발레가 연주되면서 큰 팝과 뱅 소리가 울려 방 안에 리드미컬한 음악을 만들어냅니다. 물이 졸졸 흐르고 거품이 일며 보트가 미끄러지듯 지나가면서 차분하고 평화로운 수중 분위기를 조성합니다. 새들이 지저귀는 부드러운 소리 속에서 한 여성이 부드럽게 말을 걸며 정원에 고요한 분위기를 조성합니다. 오토바이 엔진이 엔진을 가동하기 전에 공회전하면서 도시 환경에서 큰 소리가 납니다. 오른쪽 Synchformer 허용 오른쪽 오류 비디오-오디오 그림 4: ChatGPT에 제공된 자세한 프롬프트. 시각화 목적으로 다양한 시각-오디오 신호를 강조하기 위해 다양한 색상을 사용합니다. 레이블 레이블 분석 음악 및 음성 기타를 프롬프트에 입력하고 해당 신뢰도 점수를 함께 표시합니다. 또한 AudioCaps 또는 Clotho에서 세 문장 예를 지침으로 제공합니다. 시각화 목적으로 다양한 신호를 구별하기 위해 색상으로 구분된 시스템을 사용합니다. 캡션을 생성하는 동안 ChatGPT에 들리지 않는 정보, 즉 색상과 같이 비논리적이고 시각적으로 지향적인 요소를 제거하도록 명시적으로 요청합니다. 그 결과, 대규모 언어 모델은 제공된 모든 단서에서 시나리오를 분석하고 사운드 범주 및 환경을 사용하여 오디오에 대한 언어 설명을 생성할 수 있습니다. 생성된 캡션 결과는 표 1에 나와 있습니다.3.데이터 세트 필터링 AudioSet은 방대하고 다양하지만 많은 경우 게임 플레이 라이브 스트림 및 설명 비디오와 같이 노이즈로 인해 심하게 손상됩니다.반대로 VGGSound는 자동화된 수집 파이프라인 내에서 비디오와 오디오 간의 강력한 상관 관계를 크게 강조하므로 추가 처리가 필요하지 않습니다.그림 5에서 볼 수 있듯이 비디오-오디오 대응과 원래 레이블 모두에 기반한 필터링 기준을 공식화합니다.각 필터 기준에 대해 수많은 시도를 수행한 후 수동 검증을 수행하며 각 필터링 기준은 90%를 초과하는 정확도를 달성하여 총 0.4백만 개의 비디오를 제거했습니다.3.3.원시 레이블.AudioSet에는 배경 음악이 있는 수많은 설명 비디오가 포함되어 있으며 시각적 정보와 청각적 정보가 종종 일치하지 않습니다.따라서 음성과 음악을 모두 포함하는 다중 레이블에서 비디오를 제거합니다.그림 5: AudioSet의 필터링 프로세스. 데이터 집합을 필터링하기 위해 비디오와 오디오가 동기화되었는지 평가하고 원본 데이터 집합의 레이블을 분석합니다.3.3.2 시청각 동기화.우연한 추론 오류의 가능성을 없애기 위해 각 비디오에 5번의 동기화 평가를 실시합니다.시작 시간과 오프셋에 무작위 변화가 있으며 허용 임계값은 0.6초로 설정됩니다.Synchformer[22]는 정확한 시청각 동기화를 확인하기 위해 0.2초 오프셋을 사용하는 반면, 우리는 더 광범위한 오프셋을 사용하여 시청각적 대응 관계를 확인합니다.결과는 다음과 같이 분류됩니다.(1) 기준 진실과 일치하는 예측은 &quot;올바른 것&quot;으로 간주됩니다.(2) 기준 진실과 다르지만 0.6초 이내에 차이가 있는 예측은 &quot;허용 가능한 것&quot;으로 지정됩니다.(3) 다른 모든 결과는 &quot;오류&quot;로 명명됩니다.가능한 한 많은 데이터를 보존하기 위해 5가지 테스트 모두에서 &quot;오류&quot;로 분류된 비디오는 데이터 집합에서 제거합니다. 3. 데이터 세트 통계 그림 2에서 볼 수 있듯이 AudioSet과 VGGSound에서 총 150만 개의 오디오-언어 쌍을 수집합니다. 저희가 아는 한, Auto-ACD는 훈련, 검증 및 수동으로 필터링된 테스트 세트를 갖춘 최초의 백만 레벨 오디오-언어 데이터 세트입니다. Auto-ACD는 데이터 볼륨, 평균 문장 길이 측면에서 다른 데이터 세트를 능가하며 비교적 광범위한 언어 어휘를 포함합니다. LAIONAudio-630K[59]는 사용자 업로드에서 소스이며 장치 및 타임스탬프와 같은 많은 노이즈 세부 정보를 포함하고 매우 광범위한 어휘를 제공합니다. 또한 Auto-ACD는 MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최됩니다. 개구리가 울고 곤충이 울부짖습니다. 텍스트 인코더 오디오 인코더 ... .. సి mo eee... e 매핑 네트워크 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 접두사 임베딩 Pi 오디오 기능 e ea et ea et ea e ... ea.e GPT ea eeee e³ ea.en ea en een een e en.en 오디오 인코더 캡션 토큰 기차가 달리고 기차 경적 소리가 울립니다.그림 6: 오디오 언어 검색 모델 및 자동 오디오 캡션 모델 프레임워크.CLIP과 유사하게 오디오 언어 검색 모델은 오디오 인코더, 텍스트 인코더 및 대조 손실로 구성됩니다.자동 오디오 캡션 모델은 동결된 오디오 인코더와 언어 모델, 그리고 학습 가능한 매핑 네트워크로 구성됩니다.환경 정보를 포함하는 유일한 오디오 언어 데이터 세트로서 소리의 유형과 출처를 구분할 뿐만 아니라 발생 위치도 지정하여 맥락적 세부 정보의 풍부함을 높입니다. 보충적으로, 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 캡션에 대한 비교 분석을 제시합니다. LAION-Audio-630K 및 WavCaps의 캡션은 간결하며 오디오 태그 너머의 최소한의 정보를 포함합니다. 특히, LAION-Audio-630K에는 상식에서 벗어나는 문장이 포함될 수 있습니다. 예를 들어, &quot;나무를 두드리는&quot;을 &quot;래핑&quot; 오디오 태그에 대해 설명합니다. 반면, WavCaps는 &quot;... 소리가 들린다&quot;와 같이 단조로운 문장 구조를 보입니다. 반면, Auto-ACD는 오디오 장면을 더 풍부하게 묘사하는 더 긴 문장을 특징으로 합니다. 우리는 Auto-ACD에서 무작위로 샘플링한 200개의 오디오 캡션 쌍에 대한 수동 검사를 수행하여 다양한 오픈 소스 도구의 단서와 생성된 캡션을 분석합니다. 우리는 오디오와 모순되는 단서를 오류로 정의하고, 이러한 도구는 높은 정확도를 가지고 있으며, 평균 정확도는 81.3%입니다. 나아가, 우리는 무작위로 샘플링된 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행하고, 92.4%의 캡션이 오디오와 일치하고, 단지 5.3%의 잘못된 단어만 수정해야 하며, 단지 4.4%의 캡션만이 들리지 않는 정보를 포함한다는 것을 발견했습니다. 이러한 결과는 우리가 제안하는 접근 방식이 잘못되거나 들리지 않는 정보가 거의 없는 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다. 4 아키텍처 우리는 오디오-언어 대조 학습과 자동 오디오 캡션이라는 두 가지 일반적인 오디오-언어 작업을 타겟으로 하는 아키텍처를 구축하여 Auto-ACD의 효과를 더욱 검증합니다. 섹션 4.1에서 오디오-언어 대조 학습을 위한 아키텍처에 대한 자세한 설명을 제공합니다. 섹션 4.2에서 손실 함수와 함께 가벼운 자동 오디오 캡션을 위한 프레임워크를 소개합니다. 4.1 오디오-언어 대조 학습 제안된 데이터 세트의 효능을 검증하기 위해 그림 6에서 보인 것처럼 표준 대조 학습(예: infoNCE [49] 손실)을 사용하여 오디오 언어 모델을 훈련합니다. 구체적으로, 사전 훈련된 HTSAT [8]을 오디오 인코더로, 사전 훈련된 ROBERTa [35]를 언어 인코더로 사용합니다. 두 인코더 모두 사전 훈련된 CLAP 모델 [59]에서 초기화되었으며, 데이터 세트에서 추가로 미세 조정되었습니다. 최종 모델을 오디오-텍스트 검색(ATR)이라고 합니다. 오디오-텍스트 쌍(a², t¹)이 주어지면 오디오 인코더 Aenc와 텍스트 인코더 Tenc를 사용하여 각각 오디오 임베딩 è̟ò̟와 텍스트 임베딩 e를 추출합니다.e²₁ = Aenc(a²), e² = Tenc (t¹) 그런 다음 모델은 대조 손실로 훈련되며, 여기서 쌍을 이룬 오디오 및 언어 임베딩은 양수로, 쌍을 이루지 않은 임베딩은 음수로 처리되며 손실 함수는 다음과 같습니다.TNL =2N xp ea e/ (log i=j=Σι exp τ + log Σ.1 exp expe T 여기서 는 학습 가능한 온도 매개변수를 나타냅니다.훈련 단계에서 텍스트 인코더에 입력하기 전에 문장 내의 단어를 무작위로 마스크하는 단어 수준 텍스트 마스킹을 도입했습니다.4.2 자동 오디오 캡션 사전 훈련된 오디오 백본의 효과를 보여주기 위해 평가를 위해 오디오 캡션도 사용합니다.ClipCap [43]과 AutoADs [16, 17]에서 영감을 받아 다음을 채택합니다. 그림 6과 같이 오디오 백본과 언어 모델(GPT-2)이 모두 고정되어 있고 매핑 네트워크만 학습되는 가벼운 오디오 캡션 모델입니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 2: AudioCaps, Clotho 및 ACD 테스트 세트의 오디오-텍스트 검색 결과. &quot;기본&quot;, &quot;LA.&quot; “Wav.&quot;와 &quot;ACD&quot;는 각각 AudioCaps와 Clotho(기본), LAION-Audio-630K(LA), WavCaps(Wav) 및 Auto-ACD(ACD)의 조합을 나타냅니다. &quot;ACDvs&quot;는 VGGSound에서 큐레이션한 Auto-ACD의 하위 집합입니다. “* FT”는 대상 데이터 세트에서 모델을 미세 조정하는 것을 나타냅니다. AudioCaps 테스트 Clotho 테스트 Auto-ACD 테스트 텍스트→오디오 트레인 세트 모델 오디오→텍스트 텍스트 오디오 R@1 R@10 R@1 R@basic+LA.[59] basic+Wav.[38] HTSAT-ROBERTa 45.HTSAT-BERT 51.88.36.82.24.66.17.90.6 39.86.23.63.19.Audio-Text 텍스트→오디오 오디오-텍스트 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@55.4 20.0 65.0 17.9 59.58.basic+ACDys HTSAT-ROBERTa 50.5 90.39.86.24.62.9 20.basic+ACD basic+ACD*FT HTSAT-ROBERTa 53.HTSAT-ROBERTa 56.91.39.85.17.93.42.88.26.52.6 15.67.5 21.58.9 39.2 86.2 39.6 85.52.1 47.1 91.2 49.0 92.61.&#39;k&#39; 오디오-텍스트 쌍(a², c¹)이 주어지면 사전 학습된 오디오 인코더를 사용하여 오디오 특징 e² 2 = Aenc(a¹)을 추출하고 캡션을 토큰으로 변환합니다. 시퀀스, c½,..., c, 여기서 k는 텍스트의 최대 길이를 나타냅니다. 그런 다음 추출된 임베딩을 접두사 임베딩 세트로 변환하기 위해 매핑 네트워크 fmap을 설계합니다. pi = fmap (e¹²). 접두사 임베딩 세트를 자기 회귀 언어 모델로 다음 토큰을 예측하기 위한 조건으로 사용합니다. 따라서 학습하는 동안 올바른 단어를 예측하는 음의 로그 우도를 최소화합니다. L = = Ne Σ logo (c); | Pic,...,c&#39;;_1) i=1 j=여기서 는 학습 가능한 매개변수를 나타냅니다. 실험 이 섹션에서는 오디오 언어 검색, 오디오 캡션 및 제로 샷 분류의 세 가지 작업을 평가합니다. 5.1 오디오 언어 검색 5.1. 데이터 세트. AudioCaps, Clotho, Auto-ACDys 및 Auto-ACD와 같은 여러 데이터 세트에서 오디오 텍스트 검색 실험을 수행합니다. AudioCaps, Clotho 및 Auto-ACD의 훈련, 검증 및 테스트 세트에 대한 분포는 각각 50K/495/975, 3.8K/1045/1045 및 1.5M/2K/1K 데이터 쌍입니다.Auto-ACDys는 Auto-ACD의 하위 집합으로, VGGSound에서만 독점적으로 공급된 190K 데이터 쌍을 포함합니다.특히 Clotho와 AudioCaps(검증 및 테스트 세트)의 경우 각 데이터 쌍은 5개의 해당 캡션이 동반된 하나의 오디오 샘플로 구성되는 반면 나머지 데이터 쌍은 하나의 오디오 캡션 쌍으로만 구성됩니다.5.1.2 Auto-ACD 벤치마크.Auto-ACD 훈련 세트 외에도 검증 세트를 형성하기 위해 2K 데이터 샘플을 무작위로 선택하고 테스트 세트에 1K 샘플을 선택했습니다.언어 설명에서 잘못된 정보를 제거하고 부적절한 어휘 표현을 다시 작성하여 테스트 세트에 대한 수동 검증을 수행합니다. 이 테스트 세트는 오디오 언어 검색 및 자동 오디오 캡션 작업을 모두 평가하는 데 사용됩니다.5.1.3 메트릭. 데이터 세트의 풍부하고 정확한 정보를 검증하기 위해 일반적으로 사용되는 데이터 세트(예: AudioCaps 및 Clotho)에서 기존 메트릭인 Recall@k 성능을 비교합니다.또한 이러한 메트릭을 Auto-ACD 테스트 세트에 채택하여 포괄적인 개요를 제공합니다.5.1.4 학습 세부 정보.제안된 오디오-텍스트 검색(ATR) 모델을 20개 에포크 동안 학습시키고, 배치 크기는 768이며, 워밍업 단계와 코사인 학습 속도 감소 일정이 있는 1e-4의 초기 학습 속도를 사용하는 Adam 옵티마이저를 활용합니다.기존 CLAP 모델 구성과 동일한 하이퍼파라미터를 사용합니다. 오디오 인코더와 텍스트 인코더 출력의 차원은 모두 512입니다. 또한 문장의 단어에 25% 랜덤 마스킹을 도입하고 오디오 샘플의 50%에 노이즈 및 게인과 같은 증강을 랜덤하게 적용하여 모델 학습을 향상시킵니다. 15개 에포크에 대해 초기 학습률 2e-5로 Clotho 및 AudioCaps와 같은 특정 데이터 세트에서 모델을 추가로 미세 조정합니다. 5.1.5 결과. 표 2에서 볼 수 있듯이 다음과 같은 주요 관찰 결과를 도출할 수 있습니다. (i) Laion-Audio-630K에서 학습한 것과 비교할 때 제안된 Auto-ACDys 데이터 세트에서 학습하면 AudioCaps 및 Auto-ACD 벤치마크에서 Recall@k 메트릭이 상당히 향상됩니다. (ii) Auto-ACD에서 학습하고 Auto-ACD 벤치마크에 적용할 수 없는 특정 데이터 세트에서 미세 조정하면 성능이 현저히 향상됩니다. 이러한 개선은 AudioCaps의 테스트 세트에서 모델을 평가할 때 특히 두드러지는데, AudioCaps는 AudioSet의 하위 세트이고 Auto-ACD와 유사한 데이터 분포를 공유하기 때문입니다. 이러한 미세 조정 프로세스를 통해 모델은 오디오 및 텍스트 정보에 대한 보다 포괄적인 이해를 얻을 수 있으므로 검색 성능이 향상됩니다. (iii) 보다 다양한 어휘와 풍부한 언어 설명이 특징인 Auto-ACD 벤치마크에서 Auto-ACD 데이터 세트에 대한 학습은 Laion-Audio-630K에서 학습된 모델보다 상당히 우수한 성능을 보입니다. 5.2 자동 오디오 캡션 5.2. 데이터 세트. 섹션 5.1에서 언급한 데이터 세트 외에도 3.9K 오디오-텍스트 데이터 쌍으로 구성된 MACS 데이터 세트[36]도 사용합니다. 각 오디오에는 2~5개의 캡션과 여러 오디오 태그가 함께 제공됩니다. 전체적으로 Clotho, AudioCaps 및 MACS의 총 58k 데이터 쌍을 활용하여 자동 오디오 캡션 모델을 학습하고 Clotho 및 Auto-ACD 테스트 세트에서 평가합니다. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 5.2.2 지표. Meteor[5], RougeL[30], Spider[34]와 같은 기존 캡션 지표 외에도 SentenceBERT[52]를 추가 평가 지표로 통합하여 어휘 정렬에만 의존하지 않고 캡션 콘텐츠의 의미적 유사성과 정확성을 우선시합니다.5.2.3 훈련 세부 정보. MLP와 transformer라는 두 가지 매핑 네트워크를 고안하고 훈련 과정에서 GPT의 매개변수를 미세 조정합니다. 접두사의 개수를 8로 설정하고, 각 접두사의 차원은 512입니다. 이 오디오 캡션 모델을 MACS[36], Clotho, AudioCaps에서 15개 에포크 동안 배치 크기 128, 초기 학습 속도 5e-4로 학습합니다. 이 작업에서 우리는 벤치마크 데이터 세트인 Clotho와 Auto-ACD에서 두 모델의 매핑 네트워크만 학습하여 사전 학습된 오디오-텍스트 검색 모델의 오디오 인코더와 사전 학습된 CLAP[59]의 오디오 인코더를 비교합니다. 5.2. 결과. 표 3에서 볼 수 있듯이 두 가지 관찰 결과를 도출할 수 있습니다. (i) 사전 학습된 오디오-텍스트 검색 모델에서 오디오 인코더를 초기화한 자동 오디오 캡션 모델은 모든 평가 지표에서 기준선보다 성능이 향상되었습니다. (ii) Auto-ACD에서 평가할 때 결과가 더 두드러집니다. 기준선 접근 방식의 성능은 Auto-ACD의 테스트 세트에서 급격한 감소를 감독합니다. 우리는 CLAP 모델에서 추출한 기준 특징에 환경 정보에 대한 자세한 설명이 없기 때문에 그렇다고 추측합니다. 사전 훈련된 오디오-텍스트 검색 모델을 기반으로 한 자막 모델은 상당한 성능 향상을 보이며 소리가 발생하는 위치를 정확하게 추론할 수 있습니다. 이 관찰 결과는 Auto-ACD가 광범위한 어휘를 보여주어 다양한 문장 구조를 사용하여 주어진 오디오를 묘사할 수 있음을 의미합니다. 반면에 데이터 세트에서 훈련된 모델은 소리가 나는 맥락을 추론할 수 있음을 보여줍니다. 표 3: Clotho 및 Auto-ACD 테스트 세트의 자동 오디오 자막 결과. &quot;S-BERT&quot;는 SentenceBERT를 나타내고 &quot;Env.&quot;는 예측된 자막에 환경 정보가 포함되어 있는지 여부를 나타냅니다. Clotho ☑ ☑ Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. 34.9 20.6 46.36.2 21.2 47.8.10.CLAP Ours 15.16.CLAP 9.Auto-ACD Ours 21.23.37.19.56.5.5.3.Zero-shot Classification ✓ 데이터 집합.Auto-ACD는 텍스트 설명에 환경 정보를 통합하는 것이 특징입니다.Auto-ACD에서 학습한 후, 4가지 시나리오에서 환경 분류를 수행합니다.(i) AudioSet 평가 집합의 샘플 모음으로, AudioSet 온톨로지 내의 &quot;음향 환경&quot;의 자식 클래스로 주석이 달려 있으며, AudioSet Env라고 합니다.데이터 유출을 방지하기 위해, 이 실험에서는 Auto-ACDys에서 사전 학습된 모델만 사용합니다.(ii) 이전에 DCASE 2020 챌린지에서 사용했던 DCASE 2020 Mobile이라고 하는 도시 음향 장면 데이터 집합[19]입니다. (iii) 인기 있는 도시 사운드 이벤트 분류 데이터 세트인 UrbanSound 8k [54]; (iv) 음악 장르 분류 데이터 세트인 GTZANGenres [57]. 5.3.2 지표. 우리는 기존의 의역 템플릿인 &quot;[환경 레이블]의 소리 / [레이블]의 소리&quot;를 사용하여 오디오텍스트 검색 실험으로 제로샷 분류에 접근합니다. 우리는 이 실험에서 환경 분류 결과를 평가하기 위한 지표로 Recall@1을 사용합니다. 5.3.3 결과. 표 4에 나와 있는 실험 결과는 CLAP에 비해 Auto-ACD에서 사전 학습된 ATR의 우수한 환경 인식 기능을 강조합니다. 특히 AudioSet Env에서 우리 모델은 AudioSet에서 우리의 훈련 데이터 세트로 데이터가 누출되지 않고 사전 학습을 위해 Auto-ACDys만 사용했음에도 불구하고 CLAP보다 상당히 우수한 성능을 보였으며, 이는 Auto-ACD의 풍부하고 정확한 환경 정보를 더욱 입증합니다. UrbanSound 8K와 GTZANGenres에 대한 결과는 오디오 이벤트 외에도 캡션에 더 많은 정보(예: 다양한 환경 설명, 세분화된 음악 장르)가 포함될 수 있음을 보여줍니다.66*표 4: Zero-Shot Acoustic Environment Classification. Auto-ACDvs의 사전 학습 모델을 나타냅니다. &quot;US-8K&quot;는 UrbanSound 8K를 나타냅니다.Model AudioSet Env DCASE US-8K GTZANGenres CLAP 75.76.19.Ours 39.5* 32.36.CONCLUSION 31.45. 이 논문에서는 1.5M 데이터 쌍으로 구성된 대규모의 포괄적인 오디오 캡션 데이터 세트와 함께 오디오 캡션 생성을 위한 자동 파이프라인을 제시합니다. 나아가 데이터 세트에서 다양한 오디오-언어 모델의 성능을 평가하여 효과를 인증하고 오디오-언어 작업에 대한 벤치마크와 함께 수동으로 검증된 테스트 세트를 제공합니다. 이러한 실험 결과는 우리 데이터에 내재된 풍부한 정보와 정확한 설명을 밝혀내어 모델이 더욱 강력한 오디오-언어 표현을 학습하도록 돕습니다. 일부 데이터 세트가 자동 파이프라인을 통해 조달된 VGGSound에서 유래되었다는 사실로 인해 온라인 비디오에서 정확한 오디오-언어 쌍으로의 변환은 완전히 자동화되고 복제 가능한 절차로 발전했습니다. 결과적으로 오디오-언어 데이터 세트의 확장된 코퍼스를 획득하는 것은 이제 간단한 노력이 되었습니다. 더욱이 오픈소스 컴퓨터 비전 모델과 대규모 언어 모델(LLM)이 지속적으로 개선되고 발전함에 따라 보다 정확한 오디오-비주얼 지표를 추출하는 능력이 향상되어 추론의 정확도와 최종 오디오 캡션의 의역 품질이 향상됩니다. 감사의 말 이 연구는 중국 국가 핵심 R&amp;D 프로그램(No.2022ZD0161400)의 지원을 받았습니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 참고문헌 [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman. 2020. 자기 감독 다중 모드 다용도 네트워크. 신경 정보 처리 시스템의 발전 33(2020), 25-37. [2] Relja Arandjelovic, Andrew Zisserman. 2017. 보고, 듣고, 배우세요. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 609-617. [3] Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband, Anthony T Chronopoulos. 2017. 오디오 이벤트 감지 개요
--- METHOD ---
ologies → 컴퓨터 비전; • 정보 시스템 → 데이터 구조; 정보 검색. 키워드 오디오-언어 데이터 세트, 오디오-언어 표현 학습, 오디오 캡션 ACM 참조 형식: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie. 2024. Auto-ACD: 오디오-언어 표현 학습을 위한 대규모 데이터 세트. 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최된 제32회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;24) 회의록. ACM, 뉴욕, 뉴욕, 미국, 13페이지. 한국어: https://doi.org/10.1145/3664647.서론 최근 문헌에서 CLIP[49], GPT[50]의 변형, DALL-E 2[51] 및 Stable Diffusion[53]과 같은 기초 모델은 다양한 이해 및 생성 작업에서 엄청난 성공을 거두었습니다.구조적 또는 알고리즘적 설계가 다르지만 대규모 멀티모달 데이터 세트(예: MMC4[66], LAION[55], HowTo100M[39])와 같은 공통 기반에 있으며, 이는 모델 중심 표현 학습에서 데이터 중심 표현 학습으로의 새로운 전환을 나타냅니다.전자는 사전 결정된 데이터 예산의 제약 내에서 모델 설계의 경계를 넓히는 것을 고려하는 반면,후자는 확장 가능한 방식으로 대규모 및 고품질 데이터 세트를 큐레이팅하는 데 중점을 둡니다. 오디오 커뮤니티에서 그림 2에서 보듯이 오디오-언어 데이터 세트를 구성하려는 노력이 최근 있었습니다. 그러나 기존 데이터 세트는 힘들고 복잡한 수집 프로세스와 텍스트의 단순한 설명이라는 두 가지 한계에 직면할 가능성이 있습니다. 한편, 일반적으로 1~3개의 사운드 이벤트로 구성된 오디오를 포함하고 인간 주석자가 제공한 고품질 텍스트 설명이 함께 제공되는 Clotho[11]와 AudioCaps[24]. 이는 확장하기가 분명히 어렵습니다. 반면, LAION-Audio-630K[59]와 WavCaps[38]는 온라인 폴리 웹사이트에서 대량의 원시 데이터를 수집한 다음 문장 템플릿이나 키워드-캡션 모델을 사용하여 원래 오디오 레이블을 자유형 문장으로 변환합니다. 결과 언어 설명이 간단한 프롬프트나 사운드 태그보다 추가 정보를 거의 제공하지 않는다는 것은 분명합니다. 따라서 모델은 #Vocab을 훈련했습니다. 311K 자동 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 길이 18.11.8.7.7.29K 22K 5K 4K 수량 30K 57K 400K 630K 1.5M Env. WavCaps LAION-Audio-630K - Clotho AudioCaps Auto-ACD(저희) 그림 2: 다른 오디오 캡션 데이터 세트와의 비교. &quot;길이&quot; 및 &quot;# Vocab.&quot;은 평균 길이 및 어휘를 나타냅니다. &quot;Env.&quot; 및 &quot;Auto.&quot;는 각각 환경 정보 및 자동 파이프라인을 나타냅니다. 이러한 데이터 세트의 모든 오디오 언어 표현은 견고한 오디오 언어 표현을 학습할 수 없습니다. 이 논문에서는 최소한의 수동 작업으로 대규모, 고품질, 오디오-언어 데이터 세트를 구성하기 위한 최근의 노력을 제시합니다.이 데이터 세트는 Auto-ACD(Audio Captioning Dataset by Automatic Collection)라고 하며 방대한 오디오-텍스트 쌍(1.5M), 긴 텍스트(단어) 및 다양한 어휘(23K)를 포함합니다.특히, 모범적인 오디오 캡션은 네 가지 유형의 정보를 캡슐화해야 합니다.&#39;무엇&#39; - 인지되는 소리의 특성, &#39;누구&#39; - 소리를 생성하는 개체, &#39;어떻게&#39; - 소리의 특성, &#39;어디&#39; - 소리가 발생하는 위치입니다.우리의 핵심 통찰력은 시각적 장면에 대한 포괄적인 이해가 귀중한 정보 소스 역할을 할 것으로 예상되며 때로는 오디오 콘텐츠를 이해하는 데 필요하다는 것입니다.따라서 기존 오디오-비주얼 데이터 세트(예: VGGSound [7], AudioSet [13])의 견고한 오디오-비주얼 대응 관계에 대한 사전 지식에 Auto-ACD를 구축합니다. 특히, 우리는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 도구나 API(예: 비전, 언어 및 오디오 모델)를 사용하여 주어진 비디오 데이터 세트의 오디오 스트림에 대한 포괄적인 언어 설명을 생성하는 자동 파이프라인을 시작합니다. 마지막으로, 우리는 대규모 언어 모델(LLM)을 사용하여 모든 출력을 집합적으로 동화하고, 비논리적인 정보를 식별하여 제거하고, 오디오에 대한 포괄적인 설명을 생성합니다. 결과적으로 이러한 설명은 소리의 유형과 출처를 묘사할 뿐만 아니라 청각적 속성과 발생의 특정 위치도 설명합니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Auto-ACD의 텍스트 설명에서 학습한 오디오 이벤트 및 주변 정보와 같은 청각 표현을 포괄적으로 검증하기 위해 다음을 수행합니다.
--- EXPERIMENT ---
4가지 관점에서: 첫째, InfoNCE 손실[18, 46, 65]을 사용하여 공동 오디오-언어 표현 학습을 시작하고 오디오와 언어 간의 검색 작업을 통해 모델을 평가하여 기존 데이터 세트에 비해 눈에 띄는 개선을 보여줍니다.둘째, 제로샷 분류 실험을 수행하여 데이터 세트로 환경 정보를 학습하는 데 효과적임을 보여줍니다.셋째, 사전 훈련된 오디오 백본과 GPT2[50] 간의 가벼운 매핑 네트워크를 훈련하여 오디오-언어 생성 작업, 특히 자동 오디오 캡션에 대한 벤치마킹을 수행하여 널리 사용되는 벤치마크(예: Clotho[11])에서 뛰어난 성능을 보여줍니다.넷째, 테스트 세트를 수동으로 필터링하고 오디오-언어 작업을 위한 새로운 벤치마크를 도입합니다.이 벤치마크는 모델이 단순한 오디오 태그를 넘어 환경 및 세분화된 소리 범주와 같은 정보를 파악할 수 있는 능력을 평가하며, 이 분야의 미래 연구를 위한 기준을 설정합니다. 2 관련 연구 2. 시청각 학습 시청각 이벤트는 종종 야외 비디오에서 동시에 발생하여 소리와 이미지 사이에 깊은 연결을 설정합니다.[1, 2, 23, 47]은 시청각 자기 지도 학습을 사용하여 시청각 대응 관계를 활용하여 표현 학습을 향상시킵니다.특히,[15, 58, 62]는 이러한 대응 관계에 기반하여 오디오-텍스트 표현을 학습합니다.시청각 현지화[6, 21, 40, 41, 56]는 비디오 내에서 시각적 음원의 위치를 식별하는 데 집중합니다.시청각 분할[12, 29, 32, 42, 64]은 시각적 장면에서 소리가 나는 객체의 픽셀 단위 분할 마스크를 정확하게 예측하는 것을 목표로 합니다.이러한 연구는 야외 비디오에서 오디오 및 시각적 이벤트 간의 본질적인 상관 관계를 추가로 입증했으며, 이는 시각 정보에 기반한 오디오-언어 데이터 세트를 만드는 데 영감을 주었습니다. 2. 시청각 데이터 세트 대규모 시청각 데이터 세트는 효과적인 오디오 및 비디오 이해에 필수적입니다. 시청각 학습에는 종종 AudioSet과 VGGSound라는 두 가지 데이터 세트가 관련됩니다. AudioSet[13]은 각 오디오 클립에 대해 레이블이 지정된 여러 오디오 이벤트가 있는 대규모 시청각 데이터 세트입니다. 2M개가 넘는 10초 오디오 클립이 포함되어 있습니다. AudioSet은 문헌과 수동 큐레이션에 따라 안내되는 632개의 오디오 클래스로 구성된 잘 구조화된 계층적 온톨로지의 도움을 받아 수동으로 주석이 달린 데이터 세트입니다. VGGSound[7]는 309개의 오디오 클래스에 대한 200K개의 10초 비디오로 구성되어 있습니다. 이 데이터 세트는 자동화된 파이프라인을 통해 수집되고 주석이 달렸으며 각 비디오에는 하나의 레이블만 지정되었습니다. 이 논문에서는 오디오 및 시각적 신호를 모두 활용하여 오디오에 대한 자세한 설명을 제공하는 것을 목표로 합니다. 2.3 오디오-언어 학습 오디오-언어 분야에서 시각 언어 모델을 적용하는 것은 큰 도약을 의미합니다. 특히, [59]는 오디오-언어 대조 학습을 위해 CLIP 모델을 채택하여 혁신적인 교차 모달 연구의 선례를 만들었습니다. 연구자들은 오디오-텍스트 검색[25, 45], 오디오 분류[20, 48], 자동 오디오 캡션[37, 60], 오디오 질의응답[14, 31]과 같은 작업을 통해 오디오에서 의미 정보를 추출하는 데만 집중하지 않습니다. 그들은 또한 오디오 이벤트 감지[3, 28]를 통해 소리의 시간적 역학을 탐구하는 것을 포함하여 청각 지각의 보다 미묘한 측면에 도전하고 있습니다. 이 확장된 범위에는 장면 내의 소리 계산[44] 및 음향 특성에 따라 환경 분류[10]와 같은 추가 청각 속성이 포함됩니다. 의심할 여지 없이 포괄적이고 대규모, 고품질의 정보가 풍부한 오디오-언어 데이터 세트를 구성하는 것이 가장 중요합니다. 2.4 오디오 언어 데이터 세트 오디오 텍스트 검색, 오디오 캡션, 오디오 질의응답 및 텍스트 가이드 오디오 생성을 포함한 오디오 언어 작업은 널리 사용되는 두 가지 오디오 캡션 데이터 세트인 AudioCaps와 Clotho의 가용성으로부터 큰 이점을 얻었습니다.AudioSet의 하위 세트인 AudioCaps[24]는 50K 10초 길이의 오디오 클립으로 구성되며 각각 단일 캡션에 주석이 달려 있습니다.주석 작성자에게는 필요한 경우 힌트와 비디오로 AudioSet 태그가 제공되었습니다.반면에 Clotho[11]는 15~20초 동안 지속되는 6K 오디오 클립으로 구성되며 각각 캡션, 문법 교정 및 인간 주석자의 평가를 포함하는 3단계 프로세스를 통해 주석이 달린 5개의 캡션이 달려 있습니다.그러나 인간 주석 프로세스로 인해 이러한 데이터 세트는 크기가 제한적이고 비용이 많이 들고 시간이 많이 걸립니다. LAION-Audio-630K [59]는 Freesound¹ 및 BBC Sound Effects²와 같은 인기 있는 플랫폼을 포함한 온라인 폴리 웹사이트에서 오디오와 설명을 수집합니다.WavCaps [38]는 ChatGPT를 사용하여 이러한 원시 설명을 필터링하고 의역하여 인간의 주석과 유사한 정리된 텍스트 데이터가 있는 400K 오디오-텍스트 쌍의 데이터 세트를 생성합니다.오디오 클립에 종종 하나의 사운드 이벤트만 있기 때문에 문장은 대부분 간단합니다.결과적으로 이러한 데이터 세트에서 학습된 모델은 사운드 범주만 학습할 수 있었습니다.오디오-텍스트 모델의 이해 능력을 향상시키려면 보다 다양한 텍스트 및 오디오 데이터 세트가 필요합니다.데이터 세트 구성 풍부한 언어 설명이 포함된 대규모 오디오 데이터 세트를 개발하기 위해 시각적 장면 이해가 강력한 사전 역할을 한다는 가정을 기반으로 합니다.예를 들어, 동기화된 비디오는 종종 청각적 단서를 보여주고 시각 정보는 사운드가 발생하는 음향 환경을 정확하게 표현합니다. 오디오 캡션에서는 사운드 속성, 위치 및 세분화된 레이블을 통합하는 것이 좋습니다. 이를 위해 공개적으로 사용 가능한 도구나 API를 활용하여 오디오 설명에 필요한 정보를 수집하고 결과를 상호 검증할 수 있습니다. 예를 들어, 객체 감지 모델을 사용하여 잠재적인 사운드 소스를 식별하고 환경 분류 모델을 사용하여 장면 범주를 추출할 수 있습니다. 풍부한 정보를 추출하여 정확한 세부 정보를 최대한 적용하고 언어 모델에 충분한 참조를 제공합니다. ¹https://freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie BLIP-GroundingDINO CLIP 프레임 Placeaudio 비디오 visual-audio 레이블 AudioCaption PANNS 역으로 들어오는 기차. train [x:0.5921, y:0.5947, w:0.7879, h:0.3298] Passenger_car train station [prob:0.657] a train horn blows. train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] train horning prompt 기차가 지나갈 때 기차 경적이 울리며 기차역에서 크고 뚜렷한 소리가 납니다. 그림 3: Auto-ACD 수집을 위한 자동 파이프라인. 우리는 4개의 오픈소스 컴퓨터 비전 모델을 활용하여 비디오의 중간 프레임에서 시각적 단서를 추출하고, 2개의 오픈소스 오디오 이해 모델을 활용하여 전체 오디오 콘텐츠를 분석합니다. 결과적으로 우리는 원본 데이터 세트의 레이블을 결합하고, LLM(Large Language Models)을 활용하여 이러한 구성 요소를 해석하고 최종 설명으로 의역합니다. 3.1 도구 또는 API 예를 들어 AudioSet 또는 VGGSound [7, 13]와 같은 기존 대규모 비디오 데이터 세트에서 하나의 샘플이 주어지면 V = {f; a; y}로 표시하고 여기서 f, a 및 y는 각각 프레임 시퀀스, 오디오 스트림 및 시각적 또는 오디오 레이블에 해당합니다. 우리의 목표는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 채택하는 것입니다. 즉, 그림 3과 같이 기성품 비전, 언어 및 오디오 모델을 사용하여 오디오에 대한 언어 설명을 구성하는 것입니다. 이 섹션에서는 이러한 도구에 대해 자세히 설명합니다. 3.1.1 이미지 캡션. 기성품 BLIP-2 [27] 모델을 사용하여 이미지 캡션에 대해 경쟁력 있는 결과를 얻습니다. 이 도구는 전체 이미지를 포괄하고 주요 피사체 또는 환경을 정확하게 묘사하는 캡션을 생성할 수 있습니다. 우리의 경우 비디오의 중간 프레임을 이 모델에 입력합니다. 3.1.2 객체 감지. 우리는 미리 훈련된 Grounding DINO 모델[33]을 사용하여 중간 프레임 내의 객체를 식별하고, 포괄적인 분석을 보장하기 위해 감지된 모든 엔터티와 해당 예측 신뢰도 점수를 보존합니다.3.1.3 이미지 레이블링.우리는 이미지 분류를 위해 미리 훈련된 OpenAI CLIP[49] 모델을 채택합니다.이 애플리케이션에서 우리는 프롬프트: &quot;[레이블}의 사진&quot;을 사용하여 ImageNet[9]의 카테고리 온톨로지를 활용하여 텍스트 임베딩을 생성합니다.3.1.4 장소 인식.우리는 미리 훈련된 PlaceCNN[63]을 사용하여 비디오에서 캡처된 환경 맥락을 추론합니다.오디오와 시각 신호 간의 강력한 대응 관계를 감안할 때, 비디오에 묘사된 환경은 사운드가 발생하는 음향 분위기를 나타낼 가능성이 매우 높습니다.3.1.5 오디오 태그.우리는 미리 훈련된 PANN[26]을 사용하여 오디오 내의 사운드 태그를 예측하고, 신뢰도 점수와 함께 상위 3개의 예측을 보존합니다. 이는 특히 프레임 내에서 볼 수 없는 엔터티에서 나오는 소리의 경우 청각적 시간 정보의 중요한 원천입니다.3.1.6 오디오 캡션.기존 AudioCaption[61] 모델을 사용하여 간결하고 간략한 캡션을 생성합니다.이러한 캡션은 AudioCaps의 스타일과 유사하여 사운드에 대한 추가 설명적 속성이 없는 오디오 이벤트의 범주적 정보에만 초점을 맞춥니다.3.1.7 시청각 동기화.사전 훈련된 Synchformer[22]를 사용하여 비디오와 오디오 간의 동기화 감지를 수행합니다.이 프로세스는 무관하거나 동기화되지 않은 비디오 및 오디오 콘텐츠로 구성된 샘플을 필터링할 수 있습니다.이 경우 분석을 위해 비디오와 오디오를 각각 이 모델에 입력합니다.3.1.8 기존 시청각 레이블.모델의 예측 외에도 기존 데이터 세트의 제공된 레이블도 파이프라인에 통합합니다.예를 들어 VGGSound[7]는 각 비디오에 대해 단일 레이블을 제공하는 반면 AudioSet[13]은 여러 레이블을 제공합니다. 이러한 레이블은 원래 데이터 세트에서 정확하지만 불완전한 오디오-비주얼 정보를 제공합니다.3.1.9 요약.언어 모델의 경우 추론 및 귀납적 요약에서 강력한 성능을 보이는 OpenAI ChatGPT3를 사용하여 위에 언급된 설명 또는 레이블을 오디오에 대한 포괄적인 설명으로 조립합니다.BLIP-2[27]와 같은 많은 연구는 기존 도구를 적절히 활용하면 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다.오디오-비주얼 대응과 LLM의 심오한 이해 능력을 활용하여 획득한 풍부한 다중 모달리티 단서에서 정확한 오디오 캡션을 생성합니다.이 경우 섹션 3.2에 표시된 대로 특별한 프롬프트를 입력합니다.3.2 캡션 생성 비디오에 있는 시각적 및 음향적 단서를 기반으로 구조화된 언어 단락을 만들고 이를 사용하여 ChatGPT에서 오디오에 대한 설명을 생성하도록 합니다. 그림 4에서 볼 수 있듯이, 프로세스는 원하는 결과에 대한 구체적인 작업과 기준을 공식화하는 것으로 시작한 다음, 7가지 독특한 시청각적 단서를 입력합니다.3 https://openai.com/chatgpt Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 오디오에 대한 캡션을 생성하도록 ChatGPT 프롬프트하기 비디오와 오디오에서 몇 가지 정보를 제공합니다. 이 오디오는 비디오와 분리되어 있습니다. 오디오에 대한 캡션이 있습니다. 간단한 오디오 캡션으로, 이 문장은 오디오에서 발생하는 일을 간단히 설명합니다. 몇 가지 오디오 태그가 있습니다. 여러 오디오 태그는 이 오디오의 오디오 이벤트를 나타냅니다. 숫자는 확률을 나타냅니다. 시청각 레이블은 데이터 세트 시각적 오디오 레이블입니다. 한 비디오에서 키 프레임을 추출하고, 이것은 이 프레임의 이미지 캡션입니다. 이미지 캡션; 이것은 이미지 레이블입니다. 이미지 레이블; 이것은 객체 감지입니다. 객체 감지; 이것은 장소 감지입니다. 장소 레이블입니다. 이제 일반적인 어휘와 24단어를 넘지 않는 오디오 캡션 하나를 작성하여 오디오에서 발생한 일에 대한 설명을 제공하고 오디오가 발생한 위치를 추론하도록 도와주세요. 위의 정보를 참조할 수 있으며, 일부 시각적 정보는 부정확하여 무시할 수 있습니다. 오디오-비주얼 레이블을 사용하여 캡션의 오디오 이벤트를 확인하세요. 작성한 문장은 다음 예와 같아야 합니다. 숲에서 새들이 지저귀는 동안 종이 세 번 울립니다. 잔디 깎는 기계 엔진이 윙윙거리며 잔디밭에서 몇 번 휴식을 취하기 위해 멈춥니다. 공장에서 간헐적으로 작동하는 기계와 배경에서 사람들이 이야기합니다. 표 1: 정확한 콘텐츠와 충분한 주변 정보가 포함된 Auto-ACD에서 생성된 캡션의 결과. 녹색과 노란색은 오디오가 &quot;어디&quot;와 &quot;어떻게&quot; 들리는지 나타냅니다. 아니요. 생성된 캡션 1. 2. 3. 4. 팀발레가 연주되면서 큰 팝과 뱅 소리가 울려 방 안에 리드미컬한 음악이 만들어집니다. 물이 졸졸 흐르고 거품이 거품처럼 거품을 내며 보트가 미끄러지듯 지나가면서 차분하고 평화로운 수중 분위기를 조성합니다. 새들이 지저귀는 부드러운 소리 속에서 한 여성이 부드럽게 말을 걸며 정원에 고요한 분위기를 조성합니다. 오토바이 엔진이 엔진을 가동하기 전에 공회전하면서 도시 환경에서 큰 소리가 납니다. 오른쪽 Synchformer 허용 오른쪽 오류 비디오-오디오 그림 4: ChatGPT에 제공된 자세한 프롬프트. 시각화를 위해 다양한 시각-오디오 신호를 강조하기 위해 다양한 색상을 사용합니다. 레이블 레이블 분석 음악 및 음성 기타를 프롬프트에 입력하고 해당 신뢰도 점수를 함께 표시합니다. 또한 AudioCaps 또는 Clotho에서 세 문장 예를 지침으로 제공합니다. 시각화를 위해 여기서는 다양한 신호를 구별하기 위해 색상으로 구분된 시스템을 사용합니다. 캡션을 생성하는 동안 ChatGPT에 들리지 않는 정보, 즉 색상과 같이 비논리적이고 시각적으로 지향적인 요소를 제거하도록 명시적으로 요청합니다. 그 결과, 대규모 언어 모델은 제공된 모든 단서에서 시나리오를 분석하고 사운드 범주 및 환경을 사용하여 오디오에 대한 언어 설명을 생성할 수 있습니다. 생성된 캡션 결과는 표 1에 나와 있습니다.3.데이터 세트 필터링 AudioSet은 방대하고 다양하지만 많은 경우 게임 플레이 라이브 스트림 및 설명 비디오와 같이 노이즈로 인해 심하게 손상됩니다.반대로 VGGSound는 자동화된 수집 파이프라인 내에서 비디오와 오디오 간의 강력한 상관 관계를 크게 강조하므로 추가 처리가 필요하지 않습니다.그림 5에서 볼 수 있듯이 비디오-오디오 대응과 원래 레이블 모두에 기반한 필터링 기준을 공식화합니다.각 필터 기준에 대해 수많은 시도를 수행한 후 수동 검증을 수행하며 각 필터링 기준은 90%를 초과하는 정확도를 달성하여 총 0.4백만 개의 비디오를 제거했습니다.3.3.원시 레이블.AudioSet에는 배경 음악이 있는 수많은 설명 비디오가 포함되어 있으며 시각적 정보와 청각적 정보가 종종 일치하지 않습니다.따라서 음성과 음악을 모두 포함하는 다중 레이블에서 비디오를 제거합니다.그림 5: AudioSet의 필터링 프로세스. 데이터 집합을 필터링하기 위해 비디오와 오디오가 동기화되었는지 평가하고 원본 데이터 집합의 레이블을 분석합니다.3.3.2 시청각 동기화.우연한 추론 오류의 가능성을 없애기 위해 각 비디오에 5번의 동기화 평가를 실시합니다.시작 시간과 오프셋에 무작위 변화가 있으며 허용 임계값은 0.6초로 설정됩니다.Synchformer[22]는 정확한 시청각 동기화를 확인하기 위해 0.2초 오프셋을 사용하는 반면, 우리는 더 광범위한 오프셋을 사용하여 시청각적 대응 관계를 확인합니다.결과는 다음과 같이 분류됩니다.(1) 기준 진실과 일치하는 예측은 &quot;올바른 것&quot;으로 간주됩니다.(2) 기준 진실과 다르지만 0.6초 이내에 차이가 있는 예측은 &quot;허용 가능한 것&quot;으로 지정됩니다.(3) 다른 모든 결과는 &quot;오류&quot;로 명명됩니다.가능한 한 많은 데이터를 보존하기 위해 5가지 테스트 모두에서 &quot;오류&quot;로 분류된 비디오는 데이터 집합에서 제거합니다. 3. 데이터 세트 통계 그림 2에서 볼 수 있듯이 AudioSet과 VGGSound에서 총 150만 개의 오디오-언어 쌍을 수집합니다. 저희가 아는 한, Auto-ACD는 훈련, 검증 및 수동으로 필터링된 테스트 세트를 갖춘 최초의 백만 레벨 오디오-언어 데이터 세트입니다. Auto-ACD는 데이터 볼륨, 평균 문장 길이 측면에서 다른 데이터 세트를 능가하며 비교적 광범위한 언어 어휘를 포함합니다. LAIONAudio-630K[59]는 사용자 업로드에서 소스이며 장치 및 타임스탬프와 같은 많은 노이즈 세부 정보를 포함하고 매우 광범위한 어휘를 제공합니다. 또한 Auto-ACD는 MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최됩니다. 개구리가 울고 곤충이 울부짖습니다. 텍스트 인코더 오디오 인코더 ... .. సి mo eee... e 매핑 네트워크 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 접두사 임베딩 Pi 오디오 기능 e ea et ea et ea e ... ea.e GPT ea eeee e³ ea.en ea en een een e en.en 오디오 인코더 캡션 토큰 기차가 달리고 기차 경적 소리가 울립니다.그림 6: 오디오 언어 검색 모델 및 자동 오디오 캡션 모델 프레임워크.CLIP과 유사하게 오디오 언어 검색 모델은 오디오 인코더, 텍스트 인코더 및 대조 손실로 구성됩니다.자동 오디오 캡션 모델은 동결된 오디오 인코더와 언어 모델, 그리고 학습 가능한 매핑 네트워크로 구성됩니다.환경 정보를 포함하는 유일한 오디오 언어 데이터 세트로서 소리의 유형과 출처를 구분할 뿐만 아니라 발생 위치도 지정하여 맥락적 세부 정보의 풍부함을 높입니다. 보충적으로, 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 캡션에 대한 비교 분석을 제시합니다. LAION-Audio-630K 및 WavCaps의 캡션은 간결하며 오디오 태그 너머의 최소한의 정보를 포함합니다. 특히, LAION-Audio-630K에는 상식에서 벗어나는 문장이 포함될 수 있습니다. 예를 들어, &quot;나무를 두드리는&quot;을 &quot;래핑&quot; 오디오 태그에 대해 설명합니다. 반면, WavCaps는 &quot;... 소리가 들린다&quot;와 같이 단조로운 문장 구조를 보입니다. 반면, Auto-ACD는 오디오 장면을 더 풍부하게 묘사하는 더 긴 문장을 특징으로 합니다. 우리는 Auto-ACD에서 무작위로 샘플링한 200개의 오디오 캡션 쌍에 대한 수동 검사를 수행하여 다양한 오픈 소스 도구의 단서와 생성된 캡션을 분석합니다. 우리는 오디오와 모순되는 단서를 오류로 정의하고, 이러한 도구는 높은 정확도를 가지고 있으며, 평균 정확도는 81.3%입니다. 나아가, 우리는 무작위로 샘플링된 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행하고, 92.4%의 캡션이 오디오와 일치하고, 단지 5.3%의 잘못된 단어만 수정해야 하며, 단지 4.4%의 캡션만이 들리지 않는 정보를 포함한다는 것을 발견했습니다. 이러한 결과는 우리가 제안하는 접근 방식이 잘못되거나 들리지 않는 정보가 거의 없는 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다. 4 아키텍처 우리는 오디오-언어 대조 학습과 자동 오디오 캡션이라는 두 가지 일반적인 오디오-언어 작업을 타겟으로 하는 아키텍처를 구축하여 Auto-ACD의 효과를 더욱 검증합니다. 섹션 4.1에서 오디오-언어 대조 학습을 위한 아키텍처에 대한 자세한 설명을 제공합니다. 섹션 4.2에서 손실 함수와 함께 가벼운 자동 오디오 캡션을 위한 프레임워크를 소개합니다. 4.1 오디오-언어 대조 학습 제안된 데이터 세트의 효능을 검증하기 위해 그림 6에서 보인 것처럼 표준 대조 학습(예: infoNCE [49] 손실)을 사용하여 오디오 언어 모델을 훈련합니다. 구체적으로, 사전 훈련된 HTSAT [8]을 오디오 인코더로, 사전 훈련된 ROBERTa [35]를 언어 인코더로 사용합니다. 두 인코더 모두 사전 훈련된 CLAP 모델 [59]에서 초기화되었으며, 데이터 세트에서 추가로 미세 조정되었습니다. 최종 모델을 오디오-텍스트 검색(ATR)이라고 합니다. 오디오-텍스트 쌍(a², t¹)이 주어지면 오디오 인코더 Aenc와 텍스트 인코더 Tenc를 사용하여 각각 오디오 임베딩 è̟ò̟와 텍스트 임베딩 e를 추출합니다.e²₁ = Aenc(a²), e² = Tenc (t¹) 그런 다음 모델은 대조 손실로 훈련되며, 여기서 쌍을 이룬 오디오 및 언어 임베딩은 양수로, 쌍을 이루지 않은 임베딩은 음수로 처리되며 손실 함수는 다음과 같습니다.TNL =2N xp ea e/ (log i=j=Σι exp τ + log Σ.1 exp expe T 여기서 는 학습 가능한 온도 매개변수를 나타냅니다.훈련 단계에서 텍스트 인코더에 입력하기 전에 문장 내의 단어를 무작위로 마스크하는 단어 수준 텍스트 마스킹을 도입했습니다.4.2 자동 오디오 캡션 사전 훈련된 오디오 백본의 효과를 보여주기 위해 평가를 위해 오디오 캡션도 사용합니다.ClipCap [43]과 AutoADs [16, 17]에서 영감을 받아 다음을 채택합니다. 그림 6과 같이 오디오 백본과 언어 모델(GPT-2)이 모두 고정되어 있고 매핑 네트워크만 학습되는 가벼운 오디오 캡션 모델입니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 2: AudioCaps, Clotho 및 ACD 테스트 세트의 오디오-텍스트 검색 결과. &quot;기본&quot;, &quot;LA.&quot; “Wav.&quot;와 &quot;ACD&quot;는 각각 AudioCaps와 Clotho(기본), LAION-Audio-630K(LA), WavCaps(Wav) 및 Auto-ACD(ACD)의 조합을 나타냅니다. &quot;ACDvs&quot;는 VGGSound에서 큐레이션한 Auto-ACD의 하위 집합입니다. “* FT”는 대상 데이터 세트에서 모델을 미세 조정하는 것을 나타냅니다. AudioCaps 테스트 Clotho 테스트 Auto-ACD 테스트 텍스트→오디오 트레인 세트 모델 오디오→텍스트 텍스트 오디오 R@1 R@10 R@1 R@basic+LA.[59] basic+Wav.[38] HTSAT-ROBERTa 45.HTSAT-BERT 51.88.36.82.24.66.17.90.6 39.86.23.63.19.Audio-Text 텍스트→오디오 오디오-텍스트 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@55.4 20.0 65.0 17.9 59.58.basic+ACDys HTSAT-ROBERTa 50.5 90.39.86.24.62.9 20.basic+ACD basic+ACD*FT HTSAT-ROBERTa 53.HTSAT-ROBERTa 56.91.39.85.17.93.42.88.26.52.6 15.67.5 21.58.9 39.2 86.2 39.6 85.52.1 47.1 91.2 49.0 92.61.&#39;k&#39; 오디오-텍스트 쌍(a², c¹)이 주어지면 사전 학습된 오디오 인코더를 사용하여 오디오 특징 e² 2 = Aenc(a¹)을 추출하고 캡션을 토큰으로 변환합니다. 시퀀스, c½,..., c, 여기서 k는 텍스트의 최대 길이를 나타냅니다. 그런 다음 추출된 임베딩을 접두사 임베딩 세트로 변환하기 위해 매핑 네트워크 fmap을 설계합니다. pi = fmap (e¹²). 접두사 임베딩 세트를 자기 회귀 언어 모델로 다음 토큰을 예측하기 위한 조건으로 사용합니다. 따라서 학습하는 동안 올바른 단어를 예측하는 음의 로그 우도를 최소화합니다. L = = Ne Σ logo (c); | Pic,...,c&#39;;_1) i=1 j=여기서 는 학습 가능한 매개변수를 나타냅니다. 실험 이 섹션에서는 오디오 언어 검색, 오디오 캡션 및 제로 샷 분류의 세 가지 작업을 평가합니다. 5.1 오디오 언어 검색 5.1. 데이터 세트. AudioCaps, Clotho, Auto-ACDys 및 Auto-ACD와 같은 여러 데이터 세트에서 오디오 텍스트 검색 실험을 수행합니다. AudioCaps, Clotho 및 Auto-ACD의 훈련, 검증 및 테스트 세트에 대한 분포는 각각 50K/495/975, 3.8K/1045/1045 및 1.5M/2K/1K 데이터 쌍입니다.Auto-ACDys는 Auto-ACD의 하위 집합으로, VGGSound에서만 독점적으로 공급된 190K 데이터 쌍을 포함합니다.특히 Clotho와 AudioCaps(검증 및 테스트 세트)의 경우 각 데이터 쌍은 5개의 해당 캡션이 동반된 하나의 오디오 샘플로 구성되는 반면 나머지 데이터 쌍은 하나의 오디오 캡션 쌍으로만 구성됩니다.5.1.2 Auto-ACD 벤치마크.Auto-ACD 훈련 세트 외에도 검증 세트를 형성하기 위해 2K 데이터 샘플을 무작위로 선택하고 테스트 세트에 1K 샘플을 선택했습니다.언어 설명에서 잘못된 정보를 제거하고 부적절한 어휘 표현을 다시 작성하여 테스트 세트에 대한 수동 검증을 수행합니다. 이 테스트 세트는 오디오 언어 검색 및 자동 오디오 캡션 작업을 모두 평가하는 데 사용됩니다.5.1.3 메트릭. 데이터 세트의 풍부하고 정확한 정보를 검증하기 위해 일반적으로 사용되는 데이터 세트(예: AudioCaps 및 Clotho)에서 기존 메트릭인 Recall@k 성능을 비교합니다.또한 이러한 메트릭을 Auto-ACD 테스트 세트에 채택하여 포괄적인 개요를 제공합니다.5.1.4 학습 세부 정보.제안된 오디오-텍스트 검색(ATR) 모델을 20개 에포크 동안 학습시키고, 배치 크기는 768이며, 워밍업 단계와 코사인 학습 속도 감소 일정이 있는 1e-4의 초기 학습 속도를 사용하는 Adam 옵티마이저를 활용합니다.기존 CLAP 모델 구성과 동일한 하이퍼파라미터를 사용합니다. 오디오 인코더와 텍스트 인코더 출력의 차원은 모두 512입니다. 또한 문장의 단어에 25% 랜덤 마스킹을 도입하고 오디오 샘플의 50%에 노이즈 및 게인과 같은 증강을 랜덤하게 적용하여 모델 학습을 향상시킵니다. 15개 에포크에 대해 초기 학습률 2e-5로 Clotho 및 AudioCaps와 같은 특정 데이터 세트에서 모델을 추가로 미세 조정합니다. 5.1.5 결과. 표 2에서 볼 수 있듯이 다음과 같은 주요 관찰 결과를 도출할 수 있습니다. (i) Laion-Audio-630K에서 학습한 것과 비교할 때 제안된 Auto-ACDys 데이터 세트에서 학습하면 AudioCaps 및 Auto-ACD 벤치마크에서 Recall@k 메트릭이 상당히 향상됩니다. (ii) Auto-ACD에서 학습하고 Auto-ACD 벤치마크에 적용할 수 없는 특정 데이터 세트에서 미세 조정하면 성능이 현저히 향상됩니다. 이러한 개선은 AudioCaps의 테스트 세트에서 모델을 평가할 때 특히 두드러지는데, AudioCaps는 AudioSet의 하위 세트이고 Auto-ACD와 유사한 데이터 분포를 공유하기 때문입니다. 이러한 미세 조정 프로세스를 통해 모델은 오디오 및 텍스트 정보에 대한 보다 포괄적인 이해를 얻을 수 있으므로 검색 성능이 향상됩니다. (iii) 보다 다양한 어휘와 풍부한 언어 설명이 특징인 Auto-ACD 벤치마크에서 Auto-ACD 데이터 세트에 대한 학습은 Laion-Audio-630K에서 학습된 모델보다 상당히 우수한 성능을 보입니다. 5.2 자동 오디오 캡션 5.2. 데이터 세트. 섹션 5.1에서 언급한 데이터 세트 외에도 3.9K 오디오-텍스트 데이터 쌍으로 구성된 MACS 데이터 세트[36]도 사용합니다. 각 오디오에는 2~5개의 캡션과 여러 오디오 태그가 함께 제공됩니다. 전체적으로 Clotho, AudioCaps 및 MACS의 총 58k 데이터 쌍을 활용하여 자동 오디오 캡션 모델을 학습하고 Clotho 및 Auto-ACD 테스트 세트에서 평가합니다. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 5.2.2 지표. Meteor[5], RougeL[30], Spider[34]와 같은 기존 캡션 지표 외에도 SentenceBERT[52]를 추가 평가 지표로 통합하여 어휘 정렬에만 의존하지 않고 캡션 콘텐츠의 의미적 유사성과 정확성을 우선시합니다.5.2.3 훈련 세부 정보. MLP와 transformer라는 두 가지 매핑 네트워크를 고안하고 훈련 과정에서 GPT의 매개변수를 미세 조정합니다. 접두사의 개수를 8로 설정하고, 각 접두사의 차원은 512입니다. 이 오디오 캡션 모델을 MACS[36], Clotho, AudioCaps에서 15개 에포크 동안 배치 크기 128, 초기 학습 속도 5e-4로 학습합니다. 이 작업에서 우리는 벤치마크 데이터 세트인 Clotho와 Auto-ACD에서 두 모델의 매핑 네트워크만 학습하여 사전 학습된 오디오-텍스트 검색 모델의 오디오 인코더와 사전 학습된 CLAP[59]의 오디오 인코더를 비교합니다. 5.2. 결과. 표 3에서 볼 수 있듯이 두 가지 관찰 결과를 도출할 수 있습니다. (i) 사전 학습된 오디오-텍스트 검색 모델에서 오디오 인코더를 초기화한 자동 오디오 캡션 모델은 모든 평가 지표에서 기준선보다 성능이 향상되었습니다. (ii) Auto-ACD에서 평가할 때 결과가 더 두드러집니다. 기준선 접근 방식의 성능은 Auto-ACD의 테스트 세트에서 급격한 감소를 감독합니다. 우리는 CLAP 모델에서 추출한 기준 특징에 환경 정보에 대한 자세한 설명이 없기 때문에 그렇다고 추측합니다. 사전 훈련된 오디오-텍스트 검색 모델을 기반으로 한 자막 모델은 상당한 성능 향상을 보이며 소리가 발생하는 위치를 정확하게 추론할 수 있습니다. 이 관찰 결과는 Auto-ACD가 광범위한 어휘를 보여주어 다양한 문장 구조를 사용하여 주어진 오디오를 묘사할 수 있음을 의미합니다. 반면에 데이터 세트에서 훈련된 모델은 소리가 나는 맥락을 추론할 수 있음을 보여줍니다. 표 3: Clotho 및 Auto-ACD 테스트 세트의 자동 오디오 자막 결과. &quot;S-BERT&quot;는 SentenceBERT를 나타내고 &quot;Env.&quot;는 예측된 자막에 환경 정보가 포함되어 있는지 여부를 나타냅니다. Clotho ☑ ☑ Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. 34.9 20.6 46.36.2 21.2 47.8.10.CLAP Ours 15.16.CLAP 9.Auto-ACD Ours 21.23.37.19.56.5.5.3.Zero-shot Classification ✓ 데이터 집합.Auto-ACD는 텍스트 설명에 환경 정보를 통합하는 것이 특징입니다.Auto-ACD에서 학습한 후, 4가지 시나리오에서 환경 분류를 수행합니다.(i) AudioSet 평가 집합의 샘플 모음으로, AudioSet 온톨로지 내의 &quot;음향 환경&quot;의 자식 클래스로 주석이 달려 있으며, AudioSet Env라고 합니다.데이터 유출을 방지하기 위해, 이 실험에서는 Auto-ACDys에서 사전 학습된 모델만 사용합니다.(ii) 이전에 DCASE 2020 챌린지에서 사용했던 DCASE 2020 Mobile이라고 하는 도시 음향 장면 데이터 집합[19]입니다. (iii) 인기 있는 도시 사운드 이벤트 분류 데이터 세트인 UrbanSound 8k [54]; (iv) 음악 장르 분류 데이터 세트인 GTZANGenres [57]. 5.3.2 지표. 우리는 기존의 의역 템플릿인 &quot;[환경 레이블]의 소리 / [레이블]의 소리&quot;를 사용하여 오디오텍스트 검색 실험으로 제로샷 분류에 접근합니다. 우리는 이 실험에서 환경 분류 결과를 평가하기 위한 지표로 Recall@1을 사용합니다. 5.3.3 결과. 표 4에 나와 있는 실험 결과는 CLAP에 비해 Auto-ACD에서 사전 학습된 ATR의 우수한 환경 인식 기능을 강조합니다. 특히 AudioSet Env에서 우리 모델은 AudioSet에서 우리의 훈련 데이터 세트로 데이터가 누출되지 않고 사전 학습을 위해 Auto-ACDys만 사용했음에도 불구하고 CLAP보다 상당히 우수한 성능을 보였으며, 이는 Auto-ACD의 풍부하고 정확한 환경 정보를 더욱 입증합니다. UrbanSound 8K와 GTZANGenres에 대한 결과는 오디오 이벤트 외에도 캡션에 더 많은 정보(예: 다양한 환경 설명, 세분화된 음악 장르)가 포함될 수 있음을 보여줍니다. 66*표 4: Zero-Shot Acoustic Environment Classification. Auto-ACDvs.의 사전 학습 모델을 나타냅니다. &quot;US-8K&quot;는 UrbanSound 8K를 나타냅니다. 모델 AudioSet Env DCASE US-8K GTZANGenres CLAP 75.76.19. Ours 39.5* 32.36.
--- CONCLUSION ---
31.45. 이 논문에서는 1.5M 데이터 쌍으로 구성된 대규모의 포괄적인 오디오 캡션 데이터 세트와 함께 오디오 캡션 생성을 위한 자동 파이프라인을 제시합니다. 나아가, 우리는 데이터 세트에서 다양한 오디오-언어 모델의 성능을 평가하여 효과를 인증하고, 오디오-언어 작업에 대한 벤치마크와 함께 수동으로 검증된 테스트 세트를 제공합니다. 이러한 실험 결과는 우리 데이터에 내재된 풍부한 정보와 정확한 설명을 밝혀내어 모델이 더욱 강력한 오디오-언어 표현을 학습할 수 있도록 돕습니다. 우리 데이터 세트의 일부가 자동 파이프라인을 통해 조달된 VGGSound에서 유래되었다는 사실 때문입니다. 온라인 비디오에서 정확한 오디오-언어 쌍으로의 변환은 완전히 자동화되고 복제 가능한 절차로 발전했습니다. 결과적으로 오디오-언어 데이터 세트의 확장된 코퍼스를 획득하는 것은 이제 간단한 작업입니다. 또한 오픈소스 컴퓨터 비전 모델과 대규모 언어 모델(LLM)이 지속적으로 개선되고 발전함에 따라 보다 정확한 시청각 지표를 추출하는 능력이 향상되어 추론의 정확도와 최종 오디오 캡션의 의역 품질이 향상됩니다.감사의 말 이 연구는 중국 국가중점연구개발프로그램(No.2022ZD0161400)의 지원을 받았습니다.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.참고문헌 [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman. 2020. 자기 감독 다중 모달 다용도 네트워크.신경 정보 처리 시스템의 발전 33(2020), 25-37. [2] Relja Arandjelovic 및 Andrew Zisserman. 2017. 보고, 듣고, 배우세요. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 609-617. [3] Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband 및 Anthony T Chronopoulos. 2017. 특징 추출에서 분류까지 오디오 이벤트 감지 방법 개요. 응용 인공지능 31, 9-10(2017), 661-714. [4] Max Bain, Jaesung Huh, Tengda Han 및 Andrew Zisserman. 2023. WhisperX: 장문 오디오의 시간 정확한 음성 필사. INTERSPEECH 컨퍼런스 회의록(2023). [5] Satanjeev Banerjee 및 Alon Lavie. 2005. METEOR: 인간 판단과의 상관관계가 개선된 MT 평가를 위한 자동 메트릭. 기계 번역 및/또는 요약을 위한 내재적 및 외재적 평가 측정에 관한 ACL 워크숍의 진행 중. 65-72. [6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman. 2021. 시각적 소리를 어렵게 현지화하기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 16867-16876. [7] Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman. 2020. Vggsound: 대규모 오디오비주얼 데이터 세트. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 중. 721-725. [8] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 2022. HTS-AT: 사운드 분류 및 감지를 위한 계층적 토큰-의미 오디오 변환기. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 646-650. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. 2009. Imagenet: 대규모 계층적 이미지 데이터베이스. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항. 248-255. [10] Biyun Ding, Tao Zhang, Chao Wang, Ganjun Liu, Jinhua Liang, Ruimin Hu, Yulin Wu, Difei Guo. 2023. 음향 장면 분류: 포괄적 조사. 응용 분야별 전문가 시스템(2023), 121902. [11] Konstantinos Drossos, Samuel Lipping 및 Tuomas Virtanen. 2020. Clotho: 오디오 캡션 데이터 세트. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 736-740. [12] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang 및 Tong Lu. 2024. Avsegformer: 변압기를 사용한 오디오-비주얼 분할. AAAI 인공지능 컨퍼런스의 회의록, 제38권. 12155-12163. [13] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal 및 Marvin Ritter. 2017. 오디오 세트: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 776-780. [14] Di Hu Guangyao li, Yixin Xu. 2023. 오디오 질의응답을 위한 다중 스케일 주의. INTERSPEECH 컨퍼런스의 진행 사항(2023). [15] Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel. 2022. Audioclip: 클립을 이미지, 텍스트 및 오디오로 확장. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 976-980. [16] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman. 2023. AutoAD II: 속편 - 영화 오디오 설명에서 누구, 언제, 무엇. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스의 진행 사항.[17] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman.2023. AutoAD: 맥락에서의 영화 설명.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항.18930-18940.[18] Tengda Han, Weidi Xie, Andrew Zisserman.2019. 밀도 예측 코딩을 통한 비디오 표현 학습.IEEE/CVF 국제 컴퓨터 비전 워크숍 컨퍼런스의 진행 사항.1-10.[19] Toni Heittola, Annamaria Mesaros, Tuomas Virtanen.2020. TAU Urban Acoustic Scenes 2020 Mobile, 개발 데이터 세트.https://doi.org/10.5281/zenodo.[20] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. 2017. 대규모 오디오 분류를 위한 CNN 아키텍처. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 131-135. [21] Xixi Hu, Ziyang Chen, and Andrew Owens. 2022. 믹스 및 로컬라이즈: 혼합물에서 음원 로컬라이징. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 10483-10492. བུ་སྐྱ་ཀླུ [22] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: 희소한 단서로부터의 효율적인 동기화. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 회의록. 5325-5329. [23] Simon Jenni, Alexander Black, and John Collomosse. 2023. 시간적 자기 감독을 통한 시청각 대조 학습. AAAI 인공지능 컨퍼런스의 회의록, Vol. 37. 7996-8004. [24] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. Audiocaps: Generating captions for audios in the wild. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 119-132. [25] A Sophia Koepke, Andreea-Maria Oncescu, João F Henriques, Zeynep Akata, Samuel Albanie. 2022. 자연어 쿼리를 통한 오디오 검색: 벤치마크 연구. IEEE 멀티미디어 저널 25(2022), 2675-2685. [26] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, Mark D Plumbley. 2020. Panns: 오디오 패턴 인식을 위한 대규모 사전 학습된 오디오 신경망. IEEE/ACM 오디오, 음성 및 언어 처리 저널 28(2020), 2880-2894. [27] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. 2023. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용하여 언어-이미지 사전 학습 부트스트래핑. 기계 학습 국제 회의록에서.1973019742. [28] Kang Li, Yan Song, Li-Rong Dai, Ian McLoughlin, Xin Fang, Lin Liu.2023. Ast-sed: 오디오 스펙트로그램 변환기를 기반으로 한 효과적인 사운드 이벤트 감지 방법.IEEE 음향, 음성 및 신호 처리 국제 회의록에서.1-5. Catr: [29] Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xiao.2023. 오디오-비주얼 비디오 분할을 위한 조합 종속 오디오 쿼리 변환기.제31회 ACM 멀티미디어 국제 회의록에서.1485-1494. [30] Chin-Yew Lin 및 Eduard Hovy.2003. n-gram 동시 발생 통계를 사용한 요약의 자동 평가. 북미 계산언어학 협회 인간언어기술 컨퍼런스 회의록. 150-157. [31] Samuel Lipping, Parthasarathy Sudarsanam, Konstantinos Drossos, Tuomas Virtanen. 2022. Clotho-aqa: 오디오 질의응답을 위한 크라우드소싱 데이터 세트. 제30회 유럽 신호 처리 컨퍼런스 회의록. 1140-1144. [32] Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, Weidi Xie. 2024. 주석 없는 오디오비주얼 분할. IEEE/CVF 컴퓨터 비전 응용 겨울 컨퍼런스 회의록. 5604-5614. [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu 등. 2023. 공룡 접지: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합. arXiv 사전 인쇄 arXiv:2303.05499 (2023). [34] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama 및 Kevin Murphy. 2017. 스파이더의 정책 그라데이션 최적화를 통해 이미지 캡션이 개선되었습니다. 컴퓨터 비전에 관한 IEEE 국제 회의 진행 중. 873-881. [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 및 Veselin Stoyanov. 2019. Roberta: 강력하게 최적화된 bert 사전 학습 접근법.arXiv 사전 인쇄본 arXiv:1907.(2019). [36] Irene Martin Morato 및 Annamaria Mesaros. 2021. 오디오 캡션 데이터 세트의 다양성 및 편향.음향 장면 및 이벤트의 감지 및 분류.90-94. [37] Xinhao Mei, Xubo Liu, Mark D Plumbley 및 Wenwu Wang. 2022. 자동 오디오 캡션: 최근 진행 상황 및 새로운 과제에 대한 개요.EURASIP 오디오, 음성 및 음악 처리 저널 2022, 1(2022), 26. [38] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou 및 Wenwu Wang. 2023. WavCaps: 오디오-언어 멀티모달 연구를 위한 chatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트. arXiv 사전 인쇄본 arXiv:2303.17395(2023). [39] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic. 2019. Howto 100m: 1억 개의 내레이션 비디오 클립을 시청하여 텍스트-비디오 임베딩 학습. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 2630-2640. [40] Shentong Mo 및 Pedro Morgado. 2022. 약하게 감독되는 시청각 소스 현지화에 대한 자세한 살펴보기. 신경 정보 처리 시스템의 발전(2022), 37524-37536. [41] Shentong Mo 및 Yapeng Tian. 2023. 혼합물에서 사운드 현지화를 위한 오디오-비주얼 그룹화 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10565-10574. [42] Shentong Mo 및 Yapeng Tian. 2023. AV-SAM: 모든 것을 분할하는 모델이 오디오-비주얼 현지화 및 분할을 충족합니다. arXiv 사전 인쇄본 arXiv:2305.(2023). [43] Ron Mokady, Amir Hertz 및 Amit H Bermano. 2021. Clipcap: 이미지 캡션을 위한 클립 접두사. arXiv 사전 인쇄본 arXiv:2111.09734(2021). [44] Michael Nigro 및 Sridhar Krishnan. 2023. SARdBScene: 오디오 장면 소스 계산 및 분석을 위한 데이터 세트 및 Resnet 기준선. 음향, 음성 및 신호 처리에 관한 IEEE 국제 회의 진행 중. 1~5. [45] Andreea-Maria Oncescu, A Koepke, Joao F Henriques, Zeynep Akata 및 Samuel Albanie. 2021. 자연어 쿼리를 통한 오디오 검색. arXiv 사전 인쇄 arXiv:2105.02192 (2021). [46] Aaron van den Oord, Yazhe Li 및 Oriol Vinyals. 2018. 대조 예측 코딩을 사용한 표현 학습. arXiv 사전 인쇄 arXiv:1807.03748 (2018). MM &#39;24, 2024년 10월 28일~11월 1일, 호주 VIC 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie [47] Andrew Owens 및 Alexei A Efros. 2018. 자기 감독 다중 감각적 특징을 사용한 시청각 장면 분석. 유럽 컴퓨터 비전 컨퍼런스의 회의록에서. 631-648. [48] Kamalesh Palanisamy, Dipika Singhania, Angela Yao. 2020. 오디오 분류를 위한 CNN 모델 재고. arXiv 사전 인쇄본 arXiv:2007.11154(2020). [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 국제 기계 학습 컨퍼런스의 회의록에서. 8748-8763. [50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그 1, 8(2019), 9. [51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125 1, 2(2022), 3. [52] Nils Reimers 및 Iryna Gurevych. 2019. Sentence-bert: 샴 bert 네트워크를 사용한 문장 임베딩. arXiv 사전 인쇄본 arXiv:1908.10084(2019). [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10684-10695. [54] Justin Salamon, Christopher Jacoby, Juan Pablo Bello. 2014. 도시 사운드 연구를 위한 데이터 세트 및 분류법. 제22회 ACM 국제 멀티미디어 컨퍼런스 회의록. 1041–1044. [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman 등. 2022. Laion-5b: 차세대 이미지-텍스트 모델을 학습하기 위한 대규모 오픈 데이터 세트. 신경 정보 처리 시스템의 발전 35(2022), 25278-25294. [56] Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, Nick Barnes. 2023. 거짓 부정 인식 대조 학습을 통한 시청각 소스 현지화 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 6420-6429. [57] George Tzanetakis 및 Perry Cook. 2002. 오디오 신호의 음악 장르 분류. IEEE 음성 및 오디오 처리 트랜잭션 10, 5(2002), 293–302. [58] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello. 2022. Wav2clip: 클립에서 강력한 오디오 표현 학습. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 45634567. [59] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 2023. 기능 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 1-5. [60] Xuenan Xu, Mengyue Wu, Kai Yu. 2022. 자동 오디오 캡션에 대한 포괄적 조사. arXiv 사전 인쇄본 arXiv:2205.05357(2022). [61] Xuenan Xu, Zhiling Zhang, Zelin Zhou, Pingyue Zhang, Zeyu Xie, Mengyue Wu, Kenny Q Zhu. 2023. Blat: 오디오셋 태그 가이드 합성 데이터를 기반으로 한 언어-오디오 사전 학습 부트스트래핑. 제31회 ACM 국제 멀티미디어 컨퍼런스 논문집. 2756-2764. [62] Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, Yejin Choi. 2022. 시각적 지식 전송을 통해 병렬 데이터 없이 오디오와 텍스트 간의 점 연결. 계산 언어학 협회 북미 지부 컨퍼런스 논문집. 4492-4507. [63] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba. 2017. 장소: 장면 인식을 위한 1,000만 개 이미지 데이터베이스. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션 40, 6(2017), 1452-1464. [64] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, Yiran Zhong. 2022. 시청각 분할. 유럽 컴퓨터 비전 컨퍼런스 회의록. 386-403. [65] Xiao Zhou, Yujie Zhong, Zhen Cheng, Fan Liang, Lin Ma. 2023. 객체 재식별을 위한 적응적 희소 쌍 손실. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 19691–19701. [66] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi. 2024. 멀티모달 c4: 텍스트가 섞인 개방형 10억 규모 이미지 코퍼스. 신경 정보 처리 시스템의 발전 36(2024). Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 데이터셋 분석 이 섹션에서는 제안된 데이터셋인 Auto-ACD에 대한 보다 철저한 분석을 수행합니다. 섹션 7.1과 섹션 7.2에서는 Auto-ACD를 기존 오디오-언어 데이터셋과 비교하고 데이터 필터링의 필요성에 대해 논의합니다. 섹션 7.3에서는 어휘 분포를 제시합니다. 7.4절과 7.5절에서 Laion-Audio-630K, WavCaps, Auto-ACD의 캡션을 비교하고, Auto-ACD 하위 집합의 품질을 수동으로 검사합니다.7.6절에서 Auto-ACD의 추가 예를 제시합니다.7. 데이터 집합 통계 총 150만 개의 오디오 샘플을 수집했으며, 각각 10초 길이이고 자세한 캡션이 하나 첨부되었습니다.표 5에서 알 수 있듯이 다른 데이터 집합과 비교할 때 AutoACD는 볼륨 면에서 상당히 우수할 뿐만 아니라 평균 문장 길이가 더 깁니다.설명에 환경 정보를 포함하는 유일한 대규모 데이터 집합으로 자리 잡고 있습니다.Laion-Audio-630k는 어휘 수가 더 많을 수 있지만 대부분의 사전은 오디오 콘텐츠와 관련이 없는 사용자가 업로드한 장치 정보와 타임스탬프로 구성됩니다.표 5: 다른 오디오 캡션 데이터 집합과의 비교.&quot;길이&quot;와 &quot;# 어휘.&quot;는 평균 길이와 어휘를 나타냅니다. 영어: &quot;Env.&quot; 및 &quot;Auto.&quot;는 각각 환경 정보와 자동 파이프라인을 나타냅니다. 수량 길이 #Vocab. Env. Auto. 데이터 세트 AudioCaps [24] 57K 8.5K ☑ ✗ Clotho [11] 30K 11.4K ☑ LAION-Audio-630K [59] 630K 7.311K WavCaps [38] 400K 7.29K ☑ Auto-ACD(저희) 1.5M 18.22K 7. 데이터 세트 필터링 당사의 데이터 수집 절차는 강력한 오디오-비주얼 대응 관계에 의존합니다. 그러나 AudioSet 내의 많은 항목에는 상당한 노이즈가 포함되어 있어 이러한 일관성을 달성하는 데 어려움이 있습니다. 예를 들어 배경 음악이 있는 비디오, 고요한 연설, 게임 플레이를 묘사하는 비디오 또는 소프트웨어 튜토리얼이 있습니다. 이러한 비디오는 일반적으로 음성과 음악의 두 가지 유형의 오디오 이벤트만 포함합니다. 결과적으로 생성된 캡션은 종종 정보가 희소하고 오류율이 높습니다. 우리는 오디오-비주얼 레이블과 동기화 모델에 대한 분석을 사용하여 이러한 샘플을 필터링합니다. 이 필터링 프로세스의 구체적인 세부 사항은 본문의 섹션 3.3에 설명되어 있습니다. 그림 7에서 우리는 비디오 프레임 시퀀스의 몇 가지 예와 제외된 데이터에 대한 WhisperX [4]의 오디오 ASR(자동 음성 인식) 결과를 제시합니다. 대부분의 삭제된 항목은 오디오와 비디오가 관련이 없거나 동기화되지 않았기 때문입니다. 7. 데이터 세트 코퍼스 우리는 워드 클라우드로 데이터 세트의 캡션을 시각화합니다. 그림 8과 같이 일반적인 오디오 태그인 man speak와 music play가 여전히 데이터 내에서 빈도가 우세합니다. 작은 방과 음악 스튜디오와 같은 설정을 설명하는 용어도 상당한 빈도로 나타난다는 점이 주목할 만합니다. 이는 새 지저귐, 엔진 공회전, 물 튀김과 같은 수많은 오디오 이벤트로, Auto-ACD의 다양한 오디오 이벤트를 더욱 잘 보여줍니다. &quot;어쨌든, 그래서 나는 전쟁의 북소리와 한 지점을 제외하고는 거의 모든 팔을 달렸어... &quot;는 3을 5로 나눈 것과 같을 거야. 이제 세타의 탄젠트를 살펴보자. &quot;이제 두 가지 다른 방법으로 할 수 있습니다.&quot; [배경 음악만] Chitty Chitty Bang B Chiny Chitty Bang B Chitty Chitty Bang Ba Chi Chitty Bang [배경 음악만] 그림 7: 필터 처리에서 삭제된 샘플. 오른쪽의 텍스트는 WhisperX를 사용하여 처리한 비디오의 오디오에서 발화의 전사본을 나타냅니다. 음악 재생을 제안하는 시골 배경 Mengine 한가한 여자 노래 Upeople 이야기, 남자 speakmusic 스튜디오 레이블 Speechheavy retal 재생 작은 방 만들기 들리는 오락 아케이드 시각적 레이블 기타 strum 응원 벨 링 부드러운 음악 bo가 따라 말하다 열정적으로 전달하다 야외 배경 활기찬 분위기 방 만들기 그룹 차량 엔진 텔레비전 스튜디오 전동 공구 소리 남자 speak ergetic 표시된 활기찬 분위기 바람 불다 아기 울음 소리 오디오 시각적. draughing 엔진 run theat ous atmosphe 군중 응원 여자 speak man 열정적으로 사이렌 blare 오디오 태그 lause 자동차 hop 악기 전자 음악 ultiple times sing man speaking indho Speech le sing hop 기타 새 지저귐 오디오 이벤트 crophone 합창단 노래 대화 crowdtalking 음악 재생 Rain falls 자동차 엔진 큰 엔진 굉음이 작은 배경 속에서 큰 배경 음악이 멜로디한 분위기를 연출하고, 리스코테크 음악이 록 음악 배경을 암시하며 배경을 만들어냅니다.그림 8: Auto-ACD의 코퍼스. 발생 빈도가 높을수록 해당 단어의 글꼴 크기가 커집니다.7. 데이터 세트 비교 표 6에서는 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 예시 캡션을 보여줍니다.세 데이터 세트의 원래 사운드가 겹치므로 비교를 위해 다른 데이터 세트에서 동일한 오디오의 설명을 선택합니다.특히 LAION-Audio-630K는 키워드-캡션 모델을 사용하여 태그 레이블을 캡션으로 변환합니다.WavCaps는 ChatGPT를 사용하여 태그 레이블을 간단한 캡션으로 다시 표현합니다.LAIONAudio-630K 및 WavCaps의 캡션은 간결한 경향이 있으며 오디오 태그 외에 최소한의 정보만 포함합니다. 특히 LAION-Audio-630K의 캡션은 짧으며, 예를 들어 &quot;나무를 두드리는 것&quot;과 같이 상식에서 벗어나는 정보를 포함할 수 있습니다. 반면 WavCaps는 &quot;...소리가 들립니다&quot;와 같이 간단한 문장 구조를 보입니다. 이 두 데이터 세트의 캡션은 거의 나타나지 않습니다. MM &#39;24, 2024년 10월 28일-11월 1일, 멜버른, 빅토리아주, 호주. Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 표 6: LAION-Audio-630K 및 WavCaps와의 캡션 비교, &quot;LA.&quot;, &quot;WavC.&quot; 및 &quot;ACD&quot;는 각각 LAION-Audio-630K, WavCaps 및 Auto-ACD를 나타냅니다. 아니요. 데이터 세트에서 생성된 캡션 LA. 사람이 나무를 두드리는 중입니다. 1. WavC. ACD. LA. 남자가 랩을 하면서 음악이 재생됩니다. 한 여성이 힙합 음악이 배경에서 재생되는 동안 노래하며 컴퓨터실에서 랩 오디오 이벤트를 만들어냅니다. 슬러시가 묻은 수련. 워터파크에서 물이 배경에서 흔들리면서 많은 사람들이 소리치고 환호합니다. 2. WavC. 스트림 소음, 군중 및 물보라 소리. ACD. LA. 3. WavC. 소방차 사이렌이 들립니다. ACD. 4. LA. WavC. ACD. 사이렌이 달린 트럭과 비상 시 소방차. 소방차가 주거 지역을 질주하면서 비상 차량 사이렌이 크게 울립니다. 엔진이 중간 주파수로 공회전하는 차량. 중간 엔진 소리가 들립니다. 중간 크기의 엔진이 공회전하고 진동하는 동안 성인 남성이 달리는 차량 근처에서 배경에서 말합니다. 오디오 태그보다 더 많은 정보. 이와 대조적으로 Auto-ACD는 더 긴 문장을 제공하고 오디오 장면에 대한 보다 포괄적인 설명을 제공합니다. 7. 데이터 세트 품질 검사 자동 ACD의 품질을 평가하기 위해 무작위로 샘플링한 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행합니다.표 7에서 볼 수 있듯이 (i) 생성된 캡션과 원본 오디오 간의 대응 관계를 평가합니다.(ii) 최종 캡션의 잘못된 단어를 수정하고 수정된 어휘의 백분율을 계산합니다.(iii) 색상과 같이 들리지 않는 정보가 포함된 캡션의 비율을 계산합니다.높은 대응 관계와 낮은 오류 단어 백분율이라는 결과는 제안하는 접근 방식이 최소한의 잘못된 정보 또는 들리지 않는 정보로 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다.표 7: 자동 ACD에 대한 수동 검사 통계.통계 대응 0.수정 0.들리지 않음 0.또한 캡션 생성 중 각 단계, 즉 시각적 단서를 생성하는 데 사용된 다양한 도구에 대한 수동 검사를 추가로 수행합니다. 무작위로 샘플링한 200개의 오디오-캡션 쌍에 대한 수동 검사를 수행하여 6개의 서로 다른 오픈소스 도구에서 나온 단서와 생성된 캡션의 품질을 분석합니다.표 8에서 볼 수 있듯이 오디오와 모순되는 단서를 잘못된 것으로 정의하고 각 도구와 캡션의 정확도를 계산하고 각 샘플에서 올바른 단서의 수를 센다.이러한 도구는 높은 정확도를 가지고 있으며 평균 정확도는 81.3%이고 가장 높은 정확도는 91.5%입니다.샘플의 94.0%에 적어도 4개의 올바른 단서가 포함되어 있음을 알 수 있습니다.생성된 캡션의 88.0%가 오디오와 일치한다는 사실은 LLM이 잘못된 정보를 제거하고 일관된 오디오 캡션을 생성할 수 있음을 더욱 잘 보여줍니다.7.데이터 세트 시각화표 9에서 볼 수 있듯이 VGGSound와 AudioSet의 오디오에 대해 생성된 캡션을 더 많이 보여줍니다.비디오 시퀀스를 제시하여 시각적 정보가 오디오에 대한 언어 설명을 어떻게 도울 수 있는지 보여줍니다. Auto-ACD의 캡션은 사운드 이벤트를 정확하게 묘사할 뿐만 아니라 오디오에서도 추론할 수 있는 시각적 사전 정보를 기반으로 추가 정보를 추론한다는 것을 알 수 있습니다.예를 들어, (i) &quot;활기찬 공연장&quot;, &quot;음악 스튜디오&quot; 및 &quot;평화로운 선정원&quot;과 같은 환경 세부 정보, (ii) &quot;민방위 사이렌이 크게 울린다&quot; 및 &quot;배경에서 음악이 흐른다&quot;와 같은 사운드 속성, (iii) &quot;오토바이 엔진이 위아래로 회전한다&quot; 및 &quot;자동차가 흙길을 질주한다&quot;와 같은 사운드 변화.표 8: 오픈소스 도구에서 수동 검사의 정확도. &quot;캡션.&quot;은 AudioCaption 모델을 나타냅니다.BLIP-2 DINO CLIP Place365 캡션.PANNS 0.755 0.805 0.정확도 0.0.0.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 9: Auto-ACD에서의 데이터 시각화. 각 샘플에서 맨 위 줄은 비디오 프레임 시퀀스를 보여주고, 맨 아래 줄은 해당 오디오 캡션을 보여줍니다. 캡션의 사운드 이벤트는 굵은 글씨로 강조 표시되고, 환경 정보는 기울임꼴로 표시됩니다. 아니요. 생성된 캡션 1. 한 남자가 음악 스튜디오에서 컨트리 음악과 드럼 소리에 맞춰 기타를 치며 노래합니다. 2. 2010 Cruz N 3. 4. ©2016 Cruz ©2016 Cruz No ©2010 Cruz N ©2018 Cruz N 2016 Cruz 민방위 사이렌이 크게 울려 도시나 도시 환경에서 비상 상황을 알립니다. 오토바이 엔진이 주거 지역을 달리면서 위아래로 회전하며 말소리와 가벼운 엔진 소리가 들립니다. 많은 사람들이 환호하는 가운데 배경에서 음악이 재생되어 콘서트에서 활기찬 분위기를 조성합니다. 5. 밤에 비포장 도로를 질주하는 차에서 큰 엔진 소리가 들립니다. 6. 노래하는 그릇의 소리가 공명하고, 사인파의 희미한 음조와 음叉 소리가 평화로운 선정원에서 울려 퍼진다. 7. 8. 사람들의 무리가 환호하고 노래하는 가운데, 도시적인 전투 함성이 배경에서 울려 퍼진다. 음악이 흐르고 군중이 환호하고 밴드가 무대에서 연주하며, 활기찬 공연장에서 밝은 조명이 비추고 있다.
"
"--- ABSTRACT ---
Vision Transformer (ViT) has gained increasing attention in the computer vision community in recent years. However, the core component of ViT, Self-Attention, lacks explicit spatial priors and bears a quadratic computational complexity, thereby constraining the applicability of ViT. To alleviate these issues, we draw inspiration from the recent Retentive Network (RetNet) in the field of NLP, and propose RMT, a strong vision backbone with explicit spatial prior for general purposes. Specifically, we extend the RetNet’s temporal decay mechanism to the spatial domain, and propose a spatial decay matrix based on the Manhattan distance to introduce the explicit spatial prior to Self-Attention. Additionally, an attention decomposition form that adeptly adapts to explicit spatial prior is proposed, aiming to reduce the computational burden of modeling global information without disrupting the spatial decay matrix. Based on the spatial decay matrix and the attention decomposition form, we can flexibly integrate explicit spatial prior into the vision backbone with linear complexity. Extensive experiments demonstrate that RMT exhibits exceptional performance across various vision tasks. Specifically, without extra training data, RMT achieves 84.8% and 86.1% top-1 acc on ImageNet-1k with 27M/4.5GFLOPs and 96M/18.2GFLOPs. For downstream tasks, RMT achieves 54.5 box AP and 47.2 mask AP on the COCO detection task, and 52.8 mloU on the ADE20K semantic segmentation task. Code is available at https: //github.com/qhfan/RMT 1.
--- INTRODUCTION ---
Vision Transformer (ViT) [12] is an excellent visual architecture highly favored by researchers. However, as the core module of ViT, Self-Attention’s inherent structure lacking “Ran He is the corresponding author. huaibo.huang@cripac.ia.ac.cn, hmliu_82@163.com, rhe@nlpr.ia.ac.cn -- RMT(Ours) uo <¢- SMT eee 85} -+- BiFormer ss * -@- MaxViT oo a s 84 ‘Model Params Topl- Ace. oa MaxViT-T [31] 31M 83.g SMTS [34] 20M 83.< BiFormer-S [75] 26M 83.ms RMT-S (Ours) 27™M 84.o 83 RMT-S* (Ours) 27™M. 34.ze a BiForm 157 57™MMaxViT-S [19] 69M 84.RMT-B (Ours) 54M 85.RMT-B* (Ours) 55M 85.SMTL [34] 81M 84.82 MaxVit-B ttt t20MRMT-L (Ours) 95M 85.RMT-L* (Ours) 96M 86.5 10 15FLOPs(G) Figure 1. FLOPs v.s. Top-1 accuracy on ImageNet-1K with 224 x 224 resolution. “*” indicates the model trained with token labeling [27]. explicit spatial priors. Besides, the quadratic complexity of Self-Attention leads to significant computational costs when modeling global information. These issues limit the application of ViT. Many works have previously attempted to alleviate these issues [13, 16, 30, 35, 50, 57, 61]. For example, in Swin Transformer [35], the authors partition the tokens used for self-attention by applying windowing operations. This operation not only reduces the computational cost of selfattention but also introduces spatial priors to the model through the use of windows and relative position encoding. In addition to it, NAT [19] changes the receptive field of Self-Attention to match the shape of convolution, reducing computational costs while also enabling the model to perceive spatial priors through the shape of its receptive field. Different from previous methods, we draw inspiration from the recently successful Retentive Network (RetNet) [46] in the field of NLP. RetNet utilizes a distancedependent temporal decay matrix to provide explicit temporal prior for one-dimensional and unidirectional text data. --- --(a) Vanilla Self-Attention (b) Window Self-Attention (c) Neighborhood Self-Attention CT] : Receptive Field (d) Manhattan Self-Attention Figure 2. Comparison among different Self-Attention mechanisms. In MaSA, darker colors represent smaller spatial decay rates, while lighter colors represent larger ones. The spatial decay rates that change with distance provide the model with rich spatial priors. ALiBi [41], prior to RetNet, also applied a similar approach and succeeded in NLP tasks. We extend this temporal decay matrix to the spatial domain, developing a two-dimensional bidirectional spatial decay matrix based on the Manhattan distance among tokens. In our space decay matrix, for a target token, the farther the surrounding tokens are, the greater the degree of decay in their attention scores. This property allows the target token to perceive global information while simultaneously assigning different levels of attention to tokens at varying distances. We introduce explicit spatial prior to the vision backbone using this spatial decay matrix. We name this Self-Attention mechanism, which is inspired by RetNet and incorporates the Manhattan distance as the explicit spatial prior, as Manhattan Self-Attention (MaSA). Besides explicit spatial priors, another issue caused by global modeling with Self-Attention is the enormous computational burden. Previous sparse attention mechanisms [11, 35, 53, 63, 75] and the way retention is decomposed in RetNet [46] mostly disrupt the spatial decay matrix, making them unsuitable for MaSA. In order to sparsely model global information without compromising the spatial decay matrix, we propose a method to decompose SelfAttention along both axes of the image. This decomposition method decomposes Self-Attention and the spatial decay matrix without any loss of prior information. The decomposed MaSA models global information with linear complexity and has the same receptive field shape as the original MaSA. We compare MaSA with other Self-Attention mechanisms in Fig. 2. It can be seen that our MaSA introduces richer spatial priors to the model than its counterparts. Based on MaSA, we construct a powerful vision backbone called RMT. We demonstrate the effectiveness of the proposed method through extensive experiments. As shown in Fig. 1, our RMT outperforms the state-of-the-art (SOTA) models on image classification tasks. Additionally, our model exhibits more prominent advantages compared to other models in tasks such as object detection, instance segmentation, and semantic segmentation. Our contributions can be summarized as follows: * We propose a spatial decay matrix based on Manhattan distance to augment Self-Attention, creating the Manhattan Self-Attention (MaSA) with an explicit spatial prior. * We propose a decomposition form for MaSA, enabling linear complexity for global information modeling without disrupting the spatial decay matrix. « Leveraging MaSA, we construct RMT, a powerful vision backbone for general purposes. RMT attains high top-accuracy on ImageNet-1k in image classification without extra training data, and excels in tasks like object detection, instance segmentation, and semantic segmentation. 2.
--- RELATED WORK ---
Transformer. Transformer architecture was firstly proposed in [52] to address the training limitation of recurrent model and then achieve massive success in many NLP tasks. By splitting the image into small, non-overlapped patches sequence, Vision Transformer (ViTs) [12] also have attracted great attention and become widely used on vision tasks [5, 14, 18, 39, 58, 66]. Unlike in the past, where RNNs and CNNs have respectively dominated the NLP and CV fields, the transformer architecture has shined through in various modalities and fields [26, 37, 42, 60]. In the computer vision community, many studies are attempting to introduce spatial priors into ViT to reduce the data requirements for training [6, 19, 49]. At the same time, various sparse attention mechanisms have been proposed to reduce the computational cost of Self-Attention [13, 53, 54, 57]. Prior Knowledge in Transformer. Numerous attempts have been made to incorporate prior knowledge into the Transformer model to enhance its performance. The original Transformers [12, 52] use trigonometric position en --- --coding to provide positional information for each token. In vision tasks, [35] proposes the use of relative positional encoding as a replacement for the original absolute positional encoding. [6] points out that zero padding in convolutional layers could also provide positional awareness for the ViT, and this position encoding
--- METHOD ---
s, we draw inspiration from the recently successful Retentive Network (RetNet) [46] in the field of NLP. RetNet utilizes a distancedependent temporal decay matrix to provide explicit temporal prior for one-dimensional and unidirectional text data. --- --(a) Vanilla Self-Attention (b) Window Self-Attention (c) Neighborhood Self-Attention CT] : Receptive Field (d) Manhattan Self-Attention Figure 2. Comparison among different Self-Attention mechanisms. In MaSA, darker colors represent smaller spatial decay rates, while lighter colors represent larger ones. The spatial decay rates that change with distance provide the model with rich spatial priors. ALiBi [41], prior to RetNet, also applied a similar approach and succeeded in NLP tasks. We extend this temporal decay matrix to the spatial domain, developing a two-dimensional bidirectional spatial decay matrix based on the Manhattan distance among tokens. In our space decay matrix, for a target token, the farther the surrounding tokens are, the greater the degree of decay in their attention scores. This property allows the target token to perceive global information while simultaneously assigning different levels of attention to tokens at varying distances. We introduce explicit spatial prior to the vision backbone using this spatial decay matrix. We name this Self-Attention mechanism, which is inspired by RetNet and incorporates the Manhattan distance as the explicit spatial prior, as Manhattan Self-Attention (MaSA). Besides explicit spatial priors, another issue caused by global modeling with Self-Attention is the enormous computational burden. Previous sparse attention mechanisms [11, 35, 53, 63, 75] and the way retention is decomposed in RetNet [46] mostly disrupt the spatial decay matrix, making them unsuitable for MaSA. In order to sparsely model global information without compromising the spatial decay matrix, we propose a method to decompose SelfAttention along both axes of the image. This decomposition method decomposes Self-Attention and the spatial decay matrix without any loss of prior information. The decomposed MaSA models global information with linear complexity and has the same receptive field shape as the original MaSA. We compare MaSA with other Self-Attention mechanisms in Fig. 2. It can be seen that our MaSA introduces richer spatial priors to the model than its counterparts. Based on MaSA, we construct a powerful vision backbone called RMT. We demonstrate the effectiveness of the proposed method through extensive
--- EXPERIMENT ---
s demonstrate that RMT exhibits exceptional performance across various vision tasks. Specifically, without extra training data, RMT achieves 84.8% and 86.1% top-1 acc on ImageNet-1k with 27M/4.5GFLOPs and 96M/18.2GFLOPs. For downstream tasks, RMT achieves 54.5 box AP and 47.2 mask AP on the COCO detection task, and 52.8 mloU on the ADE20K semantic segmentation task. Code is available at https: //github.com/qhfan/RMT 1. Introduction Vision Transformer (ViT) [12] is an excellent visual architecture highly favored by researchers. However, as the core module of ViT, Self-Attention’s inherent structure lacking “Ran He is the corresponding author. huaibo.huang@cripac.ia.ac.cn, hmliu_82@163.com, rhe@nlpr.ia.ac.cn -- RMT(Ours) uo <¢- SMT eee 85} -+- BiFormer ss * -@- MaxViT oo a s 84 ‘Model Params Topl- Ace. oa MaxViT-T [31] 31M 83.g SMTS [34] 20M 83.< BiFormer-S [75] 26M 83.ms RMT-S (Ours) 27™M 84.o 83 RMT-S* (Ours) 27™M. 34.ze a BiForm 157 57™MMaxViT-S [19] 69M 84.RMT-B (Ours) 54M 85.RMT-B* (Ours) 55M 85.SMTL [34] 81M 84.82 MaxVit-B ttt t20MRMT-L (Ours) 95M 85.RMT-L* (Ours) 96M 86.5 10 15FLOPs(G) Figure 1. FLOPs v.s. Top-1 accuracy on ImageNet-1K with 224 x 224 resolution. “*” indicates the model trained with token labeling [27]. explicit spatial priors. Besides, the quadratic complexity of Self-Attention leads to significant computational costs when modeling global information. These issues limit the application of ViT. Many works have previously attempted to alleviate these issues [13, 16, 30, 35, 50, 57, 61]. For example, in Swin Transformer [35], the authors partition the tokens used for self-attention by applying windowing operations. This operation not only reduces the computational cost of selfattention but also introduces spatial priors to the model through the use of windows and relative position encoding. In addition to it, NAT [19] changes the receptive field of Self-Attention to match the shape of convolution, reducing computational costs while also enabling the model to perceive spatial priors through the shape of its receptive field. Different from previous methods, we draw inspiration from the recently successful Retentive Network (RetNet) [46] in the field of NLP. RetNet utilizes a distancedependent temporal decay matrix to provide explicit temporal prior for one-dimensional and unidirectional text data. --- --(a) Vanilla Self-Attention (b) Window Self-Attention (c) Neighborhood Self-Attention CT] : Receptive Field (d) Manhattan Self-Attention Figure 2. Comparison among different Self-Attention mechanisms. In MaSA, darker colors represent smaller spatial decay rates, while lighter colors represent larger ones. The spatial decay rates that change with distance provide the model with rich spatial priors. ALiBi [41], prior to RetNet, also applied a similar approach and succeeded in NLP tasks. We extend this temporal decay matrix to the spatial domain, developing a two-dimensional bidirectional spatial decay matrix based on the Manhattan distance among tokens. In our space decay matrix, for a target token, the farther the surrounding tokens are, the greater the degree of decay in their attention scores. This property allows the target token to perceive global information while simultaneously assigning different levels of attention to tokens at varying distances. We introduce explicit spatial prior to the vision backbone using this spatial decay matrix. We name this Self-Attention mechanism, which is inspired by RetNet and incorporates the Manhattan distance as the explicit spatial prior, as Manhattan Self-Attention (MaSA). Besides explicit spatial priors, another issue caused by global modeling with Self-Attention is the enormous computational burden. Previous sparse attention mechanisms [11, 35, 53, 63, 75] and the way retention is decomposed in RetNet [46] mostly disrupt the spatial decay matrix, making them unsuitable for MaSA. In order to sparsely model global information without compromising the spatial decay matrix, we propose a method to decompose SelfAttention along both axes of the image. This decomposition method decomposes Self-Attention and the spatial decay matrix without any loss of prior information. The decomposed MaSA models global information with linear complexity and has the same receptive field shape as the original MaSA. We compare MaSA with other Self-Attention mechanisms in Fig. 2. It can be seen that our MaSA introduces richer spatial priors to the model than its counterparts. Based on MaSA, we construct a powerful vision backbone called RMT. We demonstrate the effectiveness of the proposed method through extensive experiments. As shown in Fig. 1, our RMT outperforms the state-of-the-art (SOTA) models on image classification tasks. Additionally, our model exhibits more prominent advantages compared to other models in tasks such as object detection, instance segmentation, and semantic segmentation. Our contributions can be summarized as follows: * We propose a spatial decay matrix based on Manhattan distance to augment Self-Attention, creating the Manhattan Self-Attention (MaSA) with an explicit spatial prior. * We propose a decomposition form for MaSA, enabling linear complexity for global information modeling without disrupting the spatial decay matrix. « Leveraging MaSA, we construct RMT, a powerful vision backbone for general purposes. RMT attains high top-accuracy on ImageNet-1k in image classification without extra training data, and excels in tasks like object detection, instance segmentation, and semantic segmentation. 2. Related Work Transformer. Transformer architecture was firstly proposed in [52] to address the training limitation of recurrent model and then achieve massive success in many NLP tasks. By splitting the image into small, non-overlapped patches sequence, Vision Transformer (ViTs) [12] also have attracted great attention and become widely used on vision tasks [5, 14, 18, 39, 58, 66]. Unlike in the past, where RNNs and CNNs have respectively dominated the NLP and CV fields, the transformer architecture has shined through in various modalities and fields [26, 37, 42, 60]. In the computer vision community, many studies are attempting to introduce spatial priors into ViT to reduce the data requirements for training [6, 19, 49]. At the same time, various sparse attention mechanisms have been proposed to reduce the computational cost of Self-Attention [13, 53, 54, 57]. Prior Knowledge in Transformer. Numerous attempts have been made to incorporate prior knowledge into the Transformer model to enhance its performance. The original Transformers [12, 52] use trigonometric position en --- --coding to provide positional information for each token. In vision tasks, [35] proposes the use of relative positional encoding as a replacement for the original absolute positional encoding. [6] points out that zero padding in convolutional layers could also provide positional awareness for the ViT, and this position encoding method is highly efficient. In many studies, Convolution in FFN [13, 16, 54] has been employed for vision models to further enrich the positional information in the ViT. For NLP tasks, in the recent Retentive Network [46], the temporal decay matrix has been introduced to provide the model with prior knowledge based on distance changes. Before RetNet, ALiBi [41] also uses a similar temporal decay matrix. 3. Methodology 3.1. Preliminary Temporal decay in RetNet. Retentive Network (RetNet) is a powerful architecture for language models. This work proposes the retention mechanism for sequence modeling. Retention brings the temporal decay to the language model, which Transformers do not have. Retention firstly considers a sequence modeling problem in a recurrent manner. It can be written as Eq. 1: n On = > om (Qne?) (Kme tom (1) m=For a parallel training process, Eq. | is expressed as: Q=(XWe) 008, K=(XWK) O08, V=XWy yom n>m ind On =e"", Dam = 0, n<m Retention(X) = (QKT © D)V (2) where © is the complex conjugate of 9, and D € RI#!*!#! contains both causal masking and exponential decay, which symbolizes the relative distance in one-dimensional sequence and brings the explicit temporal prior to text data. 3.2. Manhattan Self-Attention Starting from the retention in RetNet, we evolve it into Manhattan Self-Attention (MaSA). Within MaSA, we transform the unidirectional and one-dimensional temporal decay observed in retention into bidirectional and two-dimensional spatial decay. This spatial decay introduces an explicit spatial prior linked to Manhattan distance into the vision backbone. Additionally, we devise a straightforward approach to concurrently decompose the Self-Attention and spatial decay matrix along the two axes of the image. From Unidirectional to Bidirectional Decay: In RetNet, retention is unidirectional due to the causal nature of text data, allowing each token to attend only to preceding tokens and not those following it. This characteristic is ill-suited for tasks lacking causal properties, such as image recognition. Hence, we initially broaden the retention to a bidirectional form, expressed as Eq. 3: BiRetention(X) = (QKT © D®)V pe = inom nm (3) where BiRetention signifies bidirectional modeling. From One-dimensional to Two-dimensional Decay: While retention now supports bi-directional modeling, this capability remains confined to a one-dimensional level and is inadequate for two-dimensional images. To address this limitation, we extend the one-dimensional retention to encompass two dimensions. In the context of images, each token is uniquely positioned with a two-dimensional coordinate within the plane, denoted as (2, yn) for the n-th token. To adapt to this, we adjust each element in the matrix D to represent the Manhattan distance between the respective token pairs based on their 2D coordinates. The matrix D is redefined as follows: Dt = rylten—#m FLY Yo (4) In the retention, the Softmax is abandoned and replaced with a gating function. This variation gives RetNet multiple flexible computation forms, enabling it to adapt to parallel training and recurrent inference processes. Despite this flexibility, when exclusively utilizing RetNet’s parallel computation form in our experiments, the necessity of retaining the gating function becomes debatable. Our findings indicate that this modification does not improve results for vision models; instead, it introduces extra parameters and computational complexity. Consequently, we continue to employ Softmax to introduce nonlinearity to our model. Combining the aforementioned steps, our Manhattan SelfAttention is expressed as MaSA(X) = (Softmax(QKT™) © D*4)V Dnm (5) — yltn—m|+1Yn—Ym =7! [+1¥n—Ym| Decomposed Manhattan Self-Attention. In the early stages of the vision backbone, an abundance of tokens leads to high computational costs for Self-Attention when attempting to model global information. Our MaSA encounters this challenge as well. Utilizing existing sparse attention mechanisms [11, 19, 35, 53, 63], or the original RetNet’s recurrent/chunk-wise recurrent form directly, disrupts the spatial decay matrix based on Manhattan distance, resulting in the loss of explicit spatial prior. To address this, we introduce a simple decomposition method that not only --- --WId}§ AUOD Manhattan Self-Attention StageRMT Block x La Figure 3. Overall architecture of RMT. ® : matrix multiplication Figure 4. Spatial decay matrix in the decomposed MaSA. decomposes Self-Attention but also decomposes the spatial decay matrix. The decomposed MaSA is represented in Eq. 6. Specifically, we calculate attention scores separately for the horizontal and vertical directions in the image. Subsequently, we apply the one-dimensional bidirectional decay matrix to these attention weights. The one-dimensional decay matrix signifies the horizontal and vertical distances between tokens (DM, = 7!¥n—¥ml, DW = ylen—aml): Attny = Softmax(QyK],) © D"", Attnw = Softmax(Qw Kj) © D”, (6) MaSA(X) = Atiny(AtinwV)T Based on the decomposition of MaSA, the shape of the receptive field of each token is shown in Fig. 4, which is identical to the shape of the complete MaSA’s receptive field. Fig. 4 indicates that our decomposition method fully preserves the explicit spatial prior. To further enhance the local expression capability of MaSA, following [75], we introduce a Local Context Enhancement module using DWConv: Xout = MaSA(X) + LCE(V); ) 3.3. Overall Architecture We construct the RMT based on MaSA, and its architecture is illustrated in Fig. 3. Similar to previous general vision backbones [35, 53, 54, 71], RMT is divided into four stages. The first three stages utilize the decomposed MaSA, while the last uses the original MaSA. Like many previous backbones [16, 30, 72, 75], we incorporate CPE [6] into our model. 4. Experiments We conducted extensive experiments on multiple vision tasks, such as image classification on ImageNet-1K [9], object detection and instance segmentation on COCO 2017 [33], and semantic segmentation on ADE20K [74]. We also make ablation studies to validate the importance of each component in RMT. More details can be found in Appendix. 4.1. Image Classification Settings. We train our models on ImageNet-1K [9] from scratch. We follow the same training strategy in [49], with the only supervision being classification loss for a fair comparison. The maximum rates of increasing stochastic depth [24] are set to 0.1/0.15/0.4/0.5 for RMT-T/S/B/L [24], respectively. We use the AdamW optimizer with a cosine decay learning rate scheduler to train the models. We set the initial learning rate, weight decay, and batch size to 0.001, 0.05, and 1024, respectively. We adopt the strong data augmentation and regularization used in [35]. Our settings are RandAugment [8] (randm9-mstd0.5-incl), Mixup [70] (prob=0.8), CutMix [69] (prob=1.0), Random Erasing [73] --- --Parmas FLOPs | Topl-acc Parmas FLOPs | Topl-acc Cost Model (M) G) (%) Cost Model (M) G) (%) PVTv2-b1 [54] 13 2.1 78.7 Swin-S [35] 50 8.7 83.QuadTree-B-b1 [48] 14 2.3 80.0 ConvNeXt-S [36] 50 8.7 83.RegionViT-T [3] 14 24 80.4 CrossFormer-B [55] 52 9.2 83.MPViT-XS [29] 11 2.9 80.9 NAT-S [19] 51 78 83.tiny-MOAT-2 [62] 10 2.3 81.0 Quadtree-B-b4 [48] 64 11.5 84.3 oO VAN-BI [17 14 2.5 81.1 Ortho-B [25] 50 8.6 84.2g 2 BiFormer-T [75] 13 2.2 81.4 ScaleViT-B [65] 81 8.6 84.> 2 Conv2Former-N [23] 15 2.2 81.5 _ MOAT-1 [62] 42 9.1 84.3 CrossFormer-T [55] 28 2.9 81.5 3 oO InternImage-S [56] 50 8.0 84.NAT-M [19 20 2.7 81.8 2g 8 DaViT-S [10] 50 8.8 84.QnA-T [1] 16 2.5 82.0 2 2 GC-ViT-S [20] 51 8.5 84.GC-ViT-XT [20] 20 2.6 82.0 sé BiFormer-B [75] 57 9.8 84.SMT-T [34 12 24 82.2 MViTv2-B [31] 52 10.2 84.RMT-T 14 25 82.4 iFormer-B [45] 48 9.4 84.Deil-S [49 2 46 79.9 BMEB 2 ea BSwin-T [35 29 45 813 WaveViT-B* [66] 34 7.2 84.ConvNeXt-T [36] 29 45 82.1 UniFormer-B* 30] 50 8.3 85.Focal-T [63 29 49 92.2 Dual-ViT-B* [67] 43 9.3 85.FocalNet-T [64] 29 45 823 BiFormer-B* [75] 58 9.8 85.Region ViT-S [3] 31 53 82.6 JERI 55 97 85.CSWin-T [11 23 43 82.7 Swin-B [35 88 15.4 83.MPViT-S [29 23 47 83.0 CaiT-M24 [50] 186 36 83.ScalableViT-S [65] 32 4.2 83.1 LITv2 [39] 87 13.2 83.SG-Former-S [15] 23 48 83.2 CrossFormer-L [55] 92 16.1 84.3 MOAT-0 [62 28 5.7 83.3 Ortho-L [25 88 15.4 84.3 g Ortho-S [25 24 45 83.4 CSwin-B [11] 78 15.0 84.ia ~ InternImage-T [56] 30 5.0 83.5 SMT-L [34 81 17.7 84.2° CMT-S [16 25 4.0 83.5 3 MOAT-2 [62 73 17.2 84.° MaxViT-T [51] 31 5.6 83.6 gg SG-Former-B [15] 78 15.6 84.SMT-S [34 20 48 83.7 E % iFormer-L [45] 87 14.0 84.BiFormer-S [75] 26 45 83.8 22 InterImage-B [56] 97 16.0 84.RMT-S 27 45 84.1 ” MaxViT-B [51] 120 23.4 84.LV-ViT-S* [27] 26 6.6 83.3 GC-ViT-B [20] 90 14.8 85.UniFormer-S* [30] 24 4.2 83.4 RMT-L 95 18.2 85.Wave ViT-S* [66] 23 47 83.9 VOLO-D3* [68] 86 20.6 85.Dual-ViT-S* [67] 25 5.4 84.1 WaveViT-L* [66] 58 14.8 85.VOLO-D1* [68] 27 6.8 84.2 UniFormer-L* [30] 100 12.6 85.BiFormer-S* [75] 26 45 84.3 Dual-ViT-L* [67] 73 18.0 85.RMT-S* 27 45 84.8 RMT-L* 96 18.2 86.Table 1. Comparison with the state-of-the-art on ImageNet-1K classification. “*” indicates the model trained with token labeling [27]. (prob=0.25). In addition to the conventional training methods, similar to LV-ViT [27] and VOLO [68], we train a model that utilizes token labeling to provide supplementary supervision. Results. We compare RMT against many state-of-the-art models in Tab. |. Results in the table demonstrate that RMT consistently outperforms previous models across all settings. Specifically, RMT-S achieves 84.1% Top1-accuracy with only 4.5 GFLOPs. RMT-B also surpasses iFormer [45] by 0.4% with similar FLOPs. Furthermore, our RMT-L model surpasses MaxViT-B [51] in topl-accuracy by 0.6% while using fewer FLOPs. Our RMT-T has also outperformed many lightweight models. As for the model trained using token labeling, our RMT-S outperforms the current state-of-the-art BiFormer-S by 0.5%. 4.2. Object Detection and Instance Segmentation Settings. We adopt MMDetection [4] to implement RetinaNet [32], Mask-RCNN [22] and Cascade Mask RCNN [2]. We use the commonly used “1x” (12 training epochs) setting for the RetinaNet and Mask R-CNN. Besides, we use “3 x +MS” for Mask R-CNN and Cascade Mask R-CNN. Following [35], during training, images are resized to the shorter side of 800 pixels while the longer side is within 1333 pixels. We adopt the AdamW optimizer with a learning rate of 0.0001 and batch size of 16 to optimize the model. For the “1x” schedule, the learning rate --- --Backbone Params FLOPs Mask R-CNN 1x Params FLOPs RetinaNet 1x (M) (G) |AP’ AP% AP, AP™ APs API; (M) (G) | AP’ AP% AP?; AP§ APh, APZ PVT-T [53 33 240 |39.8 62.2 43.0 374 59.3 39.9 23 221 | 394 59.8 42.0 25.5 42.0 52.PVTv2-B1 [54] 33 243 | 41.8 543 45.9 38.8 61.2 41.6 23 225 | 41.2 61.9 43.9 25.4 44.5 54.MPViT-XS [29] 30 231 |44.2 66.7 484 404 634 43.4 20 211 | 43.8 65.0 47.1 28.1 47.6 56.RMT-T 33 218 | 47.1 68.8 51.7 426 65.8 45.9 23 199 | 45.1 66.2 48.1 28.8 48.9 61.Swin-T [35 48 267 |43.7 66.6 47.7 39.8 63.3 42.7 38 248 | 41.7 63.1 44.3 27.0 45.3 54.CMT-S [16 45 249 |446 668 48.9 40.7 63.9 43.4 44 231 | 44.3 65.5 47.5 27.1 48.3 59.CrossFormer-S [55] 50 301 |454 68.0 49.7 414 648 44.6 41 272 |444 65.8 474 28.2 484ScalableViT-S [65] 46 256 |45.8 67.6 50.0 41.7 64.7 448 36 238 | 45.2 66.5 484 29.2 49.1 60.MPViT-S [29] 43 268 |464 68.6 51.2 424 65.6 45.7 32 248 | 45.7 57.3 48.8 28.7 49.7 59.CSWin-T [11] 42 279 |46.7 68.6 51.3 42.2 65.6 45.4 - - - - - - - InternImage-T [56] 49 270 |47.2 69.0 52.1 425 66.1 45.8 - - - - - - - SMT-S [34 40 265 |47.8 69.5 52.1 43.0 66.6 46.1 - - - - - - - BiFormer-S [75] - - 47.8 69.8 52.3 43.2 668 46.5 - - 45.9 66.9 494 30.2 49.6 61.RMT-S 46 262 | 49.0 70.8 53.9 43.9 67.8 47.4 36 244 | 47.8 69.1 51.8 32.1 518 63.ResNet-101 [21] 63 336 |404 61.1 442 364 57.7 388 58 315 | 38.5 57.8 41.2 214 426 51.Swin-S [35 69 359 | 45.7 67.9 504 41.1 64.9 44.2 60 339 | 44.5 66.1 474 29.8 48.5 59.ScalableViT-B [65] 95 349 |46.8 68.7 S515 425 65.8 45.9 85 330 | 45.8 67.3 49.2 29.9 49.5 61.InternImage-S [56] 69 340 | 47.8 69.8 52.8 43.3 67.1 46.7 - - - - - - - CSWin-S [11] 54 342 |47.9 70.1 52.6 43.2 67.1 46.2 - - - - - - - BiFormer-B [75] - - 48.6 70.5 53.8 43.7 67.6 47.1 - - 47.1 685 504 31.3 508 62.RMT-B 73 373, | 51.1 72.5 56.1 45.5 69.7 49.3 63 355 | 49.1 70.3 53.0 32.9 53.2 64.Swin-B [35 107 496 | 46.9 69.2 51.6 42.3 66.0 45.5 98 477 |45.0 66.4 483 284 49.1 60.PVTv2-B5 [54] 102 557 | 474 68.6 51.9 425 65.7 46.0 - - - - - - - Focal-B [63 110 533. | 47.8 70.2 52.5 43.2 67.3 465 101 514 | 46.3 68.0 49.8 31.7 504 60.MPViT-B [29] 95 503 | 48.2 70.0 52.9 43.5 67.1 468 85 482 | 47.0 684 50.8 294 51.3 61.CSwin-B [11] 97 526 | 48.7 70.4 53.9 43.9 67.8 47.3 - - - - - - - InternImage-B [56] |} 115 S01 | 48.8 70.9 54.0 44.0 67.8 47.4 - - - - - - - RMT-L 114 557 | 51.6 73.1 565 45.9 70.3 49.8 104 537 | 49.4 70.6 53.1 34.2 53.9 65.Table 2. Comparison to other backbones using RetinaNet and Mask R-CNN on COCO val2017 object detection and instance segmentation. Backbone [Params FLOPs| Mask R-CNN 3x+MS Backbone [Params FLOPs} Cascade Mask R-CNN 3x+MS (M) (G) |AP? APY, AP? AP™ AP AP (M) (G) |AP? APY, AP®, AP™ AP AP ConvNeXt-T [36] } 48 262 |46.2 67.9 50.8 41.7 65.0 45.0 Swin-T [35 86 745 |50.5 69.3 54.9 43.7 66.6 47.Focal-T [63 49 291 |47.2 69.4 51.9 42.7 66.5 45.9 NAT-T [19] 85 737 |51.4 70.0 55.9 44.5 67.6 47.NAT-T [19 48 258 |47.8 69.0 52.6 42.6 66.0 45.9 GC-ViT-T [20 85 770 |51.6 70.4 56.1 44.6 67.8 48.GC-ViT-T [20] 48 291 |47.9 70.1 52.8 43.2 67.0 46.7 SMT-S [34] 78 744 |51.9 70.5 56.3 44.7 67.8 48.MPViT-S [29] 43 268 |48.4 70.5 52.6 43.9 67.6 47.5 UniFormer-S [30]} 79 TAT |52.1 71.1 56.6 45.2 68.3 48.Ortho-S [25 44 277 |48.7 70.5 53.3 43.6 67.3 47.3 Ortho-S [25 81 755 |52.3 71.3 56.8 45.3 68.6 49.SMT-S [34 40 265 |49.0 70.1 53.4 43.4 67.3 46.7 HorNet-T [43 80 728 |52.4 71.6 56.8 45.6 69.1 49.CSWin-T [11] 42 279 |49.0 70.7 53.7 43.6 67.9 46.6 CSWin-T [11 80 757 |52.5 71.5 57.1 45.3 68.8 48.InternImage-T [56]} 49 270 |49.1 70.4 54.1 43.7 67.3 47.3 RMT-S 83 741 |53.2 72.0 57.8 46.1 69.8 49.RMS G1) _ 220) [uy LD) GE) GIO ail Gas Swin-S [35] | 107 838 [51.9 70.7 56.3 45.0 68.2 48.ConvNeXt-S [36] |] 70 348 |47.9 70.0 52.7 42.9 66.9 46.2 NAT-S [19 108 809 |51.9 70.4 56.2 44.9 68.2 48.NAT-S [19 70 330 |48.4 69.8 53.2 43.2 66.9 46.4 GC-ViT-S [20 108 866 |52.4 71.0 57.1 45.4 68.5 49.Swin-S [35 69 359 |48.5 70.2 53.5 43.3 67.3 46.6 DAT-S [58 107-857 {52.7 71.7 57.2 45.5 69.1 49.InternImage-S [56]} 69 340 |49.7 71.1 54.5 44.5 68.5 47.8 HorNet-S [43 108 827 |53.3 72.3 57.8 46.3 69.9 50.SMT-B [34 52 328 |49.8 71.0 54.4 44.0 68.0 47.3 CSWin-S [11 92 820 |53.7 72.2 58.4 46.4 69.6 50.CSWin-S [11] 54 342 |50.0 71.3 54.7 44.5 68.4 47.7 UniFormer-B [30]} 107 878 53.8 72.8 58.5 46.4 69.9 50.RMT-B 73 373 |52.2 72.9 57.0 46.1 70.4 49.9 RMT-B 111 852 |54.5 72.8 59.0 47.2 70.5 51.Table 3. Comparison to other backbones using Mask R-CNN with ”3 x +MS” schedule. declines with the decay rate of 0.1 at the epoch 8 and 11. While for the “3 x +MS” schedule, the learning rate de clines with the decay rate of 0.1 at the epoch 27 and 33. Table 4. Comparison to other backbones using Cascade Mask RCNN with ""3 x +MS” schedule. Results. Tab. 2, Tab. 3 and Tab. 4 show the results with different detection frameworks. The results demonstrate that our RMT performs best in all comparisons. For the --- --Backbone Method | Params(M) | FLOPs(G) | mloU(%) ResNet 18 [21] FPN 15.5 32.2 32.PVTv2-B1 [54] FPN 17.8 34.2 42.VAN-B1 [17 FPN 18.1 34.9 42.EdgeViT-S [38] FPN 16.9 32.1 45.RMT-T FPN 17.0 33.7 46.DAT-T [58 FPN 32 198 42.RegionViT-S+ [3] FPN 35 236 45.CrossFormer-S [55] FPN. 34 221 46.UniFormer-S [30] FPN 25 247 46.Shuted-S [44 FPN 26 183 48.RMT-S FPN 30 180 49.DAT-S [58 FPN 53 320 46.RegionViT-B+ [3] FPN 77 459 47.UniFormer-B [30] FPN 54 350 47.CrossFormer-B [55] FPN. 56 331 47.CSWin-S [11 FPN 39 271 49.RMT-B FPN 57 294 50.DAT-B [58 FPN 92 481 47.CrossFormer-L [55] FPN. 95 497 48.CSWin-B [11 FPN 81 464 49.RMT-L FPN 98 482 51.DAT-T [58 UperNet 60 957 45.NAT-T [19 UperNet 58 934 47.InternImage-T [56] | UperNet 59 944 47.MPViT-S [29 UperNet 52 943 48.SMT-S [34 UperNet 50 935 49.RMT-S UperNet 56 937 49.DAT-S [58 UperNet 81 1079 48.SMT-B [34 UperNet 62 1004 49.HorNet-S [43 UperNet 85 1027 50.InterImage-S [56] UperNet 80 1017 50.MPViT-B [29 UperNet 105 1186 50.CSWin-S [11 UperNet 65 1027 50.RMT-B UperNet 83, 1051 52.Swin-B [35] UperNet 121 1188 48.GC ViT-B [20 UperNet 125 1348 49.DAT-B [58] UperNet 121 1212 49.InternImage-B [56] | UperNet 128 1185 50.CSWin-B [11 UperNet 109 1222 SRMT-L UperNet 125 1241 52.Table 5. Comparison with the state-of-the-art on ADE20K. RetinaNet framework, our RMT-T outperforms MPViT-XS by +1.3 AP, while S/B/L also perform better than other methods. As for the Mask R-CNN with “1x” schedule, RMT-L outperforms the recent InternImage-B by +2.8 box AP and +1.9 mask AP. For “3 x +MS” schedule, RMTS outperforms InternImage-T for +1.6 box AP and +1.mask AP. Besides, regarding the Cascade Mask R-CNN, our RMT still performs much better than other backbones. All the above results tell that RMT outperforms its counterparts by evident margins. 4.3. Semantic Segmentation Settings. We adopt the Semantic FPN [28] and UperNet [59] based on MMSegmentation [7], apply RMTs which are pretrained on ImageNet-1K as backbone. We use the same setting of PVT [53] to train the Semantic FPN, and we train the model for 80k iterations. All models are trained with the input resolution of 512 x 512. When testing the model, we resize the shorter side of the image to 512 pixels. As for UperNet, we follow the default settings in Swin [35]. We take AdamW with a weight decay of 0.as the optimizer to train the models for 160K iterations. The learning rate is set to 6 x 10° with 1500 iterations warmup. Results. The results of semantic segmentation can be found in Tab. 5. All the FLOPs are measured with the resolution of 512 x 2048, except the group of RMT-T, which are measured with the resolution of 512 x 512. All our models achieve the best performance in all comparisons. Specifically, our RMT-S exceeds Shunted-S for +1.2 mloU with Semantic FPN. Moreover, our RMT-B outperforms the recent InternImage-S for +1.8 mloU. All the above results demonstrate our model’s superiority in dense prediction. 4.4, Ablation Study Strict comparison with previous works. In order to make a strict comparison with previous methods, we align RMT’s hyperparameters (such as whether to use hierarchical structure, the number of channels in the four stages of the hierarchical model, whether to use positional encoding and convolution stem, etc.) of the overall architecture with DeiT [49] and Swin [35], and only replace the Self-Attention/Window Self-Attention with our MaSA. The comparison results are shown in Tab. 6, where RMT significantly outperforms DeiT-S, Swin-T, and Swin-S. MaSA._ We verify the impact of Manhattan Self-Attention on the model, as shown in the Tab. 6. MaSA improves the model’s performance in image classification and downstream tasks by a large margin. Specifically, the classification accuracy of MaSA is 0.8% higher than that of vanilla attention. Softmax. In RetNet, Softmax is replaced with a nonlinear gating function to accommodate its various computational forms [46]. We replace the Softmax in MaSA with this gating function. However, the model utilizing the gating function cannot undergo stable training. It is worth noting that this does not mean the gating function is inferior to Softmax. The gating function may just not be compatible with our decomposed form or spatial decay. LCE. Local Context Enhancement also plays a role in the excellent performance of our model. LCE improves the classification accuracy of RMT by 0.3% and enhances the model’s performance in downstream tasks. CPE. Just like previous methods, CPE provides our model with flexible position encoding and more positional information, contributing to the improvement in the model’s performance in image classification and downstream tasks. --- --Model Params(M) FLOPs(G) Top1-acc(%) AP? Ap™ mloU(%) DeiT-S [49] 22 46 79.8 - - RMT-DeiT-S 22 4.6 81.7(+1.9) - - = Swin-T [35] 29 45 81.3 43.7 39.8 44.RMT-Swin-T 29 47 83.6(+2.3) 47.8(+4.1) 43.1(+3.3) 49.1(+4.6) Swin-S [35] 50 8.8 83.0 45.7 41.1 47.RMT-Swin-S 50 9.1 84.5(+1.5) 49.5(+3.8) 44.2(+3.1) 51.0 (43.4) RMT-T 14.3 2.5 82.4 47.1 42.6 46.MaSA->Attention 14.3 2.5 81.6(-0.8) 44.6(-2.5) 40.7(-1.9) 43.9(-2.5) Softmax—Gate 15.6 2.7 Nan - - w/o LCE 14.2 2.4 82.1 46.7 42.3 46.w/o CPE 14.3 2.5 82.2 47.0 42.4 46.w/o Stem 14.3 2.2 82.2 46.8 42.3 46.Table 6. Ablation study. We make a strict comparison among RMT, DeiT, and Swin-Transformer. 3rd stage | FLOPs(G) Topl(%) | FLOPs(G) mloU(%) MaSA-d 4.5 84.1 180 49.MaSA 4.8 84.1 246 49.Table 7. Comparison between decomposed MaSA (MaSA-d) and original MaSA. Params FLOPs| Throughputt | Topl Method | ™ ©) timgs/s) | (%) Parallel 27 10.9 262 Chunklen_4 27 45 192 Chunklen_49 27 AT 446 82.Recurrent 27 45 61 MaSA | 27 45 876 84.Table 8. Comparison between MaSA and retention in RMT-S’s architecture. . Params FLOPs| Throughput} Top! Model Mp) G) (imgs/s) (%) BiFormer-T [75] 13 2.2 1602CMT-XS [16] 15 15 1476 81.SMT-T [34 12 2.4 636 82.RMT-T 14 25 1650 82.CMT-S [16, 25 4.0 848 83.MaxViT-T [51] 31 5.6 826 83.SMT-S [34 20 48 356 83.BiFormer-S [75] 26 45 766 83.RMT-Swin-T 29 47 1192 83.RMT-S 27 45 876 84.SMT-B [34 32 V7 237 84.BiFormer-B [75] 57 9.8 498 84.CMT-B [16 46 9.3 447 84.MaxViT-S [51] 69 117 546 84.RMT-Swin-S 50 9.1 722 84.RMT-B 54 9.7 457 85.SMT-L [34 80 17.7 158 84.MaxViT-B [51] 120 23.4 306 84.RMT-L 95 18.2 326 85.Table 9. Comparison of inference speed among SOTA models. Convolutional Stem. The initial convolutional stem of the model provides better local information, thereby further enhancing the model’s performance on various tasks. Decomposed MaSA. In RMT-S, we substitute the decomposed MaSA (MaSA-d) in the third stage with the original MaSA to validate the effectiveness of our decomposition method, as illustrated in Tab. 7. In terms of image classification, MaSA-d and MaSA achieve comparable accuracy. However, for semantic segmentation, employing MaSA-d significantly reduces computational burden while yielding similar result. MaSA v.s. Retention. As shown in Tab. 8, we replace MaSA with the original retention in the architecture of RMT-S. We partition the tokens into chunks using the method employed in Swin-Transformer [35] for chunk-wise retention. Due to the limitation of retention in modeling one-dimensional causal data, the performance of the vision backbone based on it falls behind RMT. Moreover, the chunk-wise and recurrent forms of retention disrupt the parallelism of the vision backbone, resulting in lower inference speed. Inference Speed. We compare the RMT’s inference speed with the recent best performing vision backbones in Tab. 9. Our RMT demonstrates the optimal trade-off between speed and accuracy. 5.
--- CONCLUSION ---
In this work, we propose RMT, a vision backbone with explicit spatial prior. RMT extends the temporal decay used for causal modeling in NLP to the spatial level and introduces a spatial decay matrix based on the Manhattan distance. The matrix incorporates explicit spatial prior into the Self-Attention. Additionally, RMT utilizes a Self-Attention --- --decomposition form that can sparsely model global information without disrupting the spatial decay matrix. The combination of spatial decay matrix and attention decomposition form enables RMT to possess explicit spatial prior and linear complexity. Extensive experiments in image classification, object detection, instance segmentation, and semantic segmentation validate the superiority of RMT. A. Architecture Details Our architectures are illustrated in the Tab. 10. For convolution stem, we apply five 3 x 3 convolutions to embed the image into 56 x 56 tokens. GELU and batch normalization are used after each convolution except the last one, which is only followed by batch normalization. 3 x 3 convolutions with stride 2 are used between stages to reduce the feature map’s resolution. 3 x 3 depth-wise convolutions are adopted in CPE. Moreover, 5 x 5 depth-wise convolutions are adopted in LCE. RMT-DeiT-S, RMT-Swin-T, and RMT-Swin-S are models that we used in our ablation experiments. Their structures closely align with the structure of DeiT [49] and Swin-Transformer [35] without using techniques like convolution stem, CPE, and others. B. Experimental Settings ImageNet Image Classification. We adopt the same training strategy with DeiT [49] with the only supervision is the classification loss. In particular, our models are trained from scratch for 300 epochs. We use the AdamW optimizer with a cosine decay learning rate scheduler and 5 epochs of linear warm-up. The initial learning rate, weight decay, and batch size are set to 0.001, 0.05, and 1024, respectively. Our augmentation settings are RandAugment [8] (randm9-mstd0.5-inc1), Mixup [70] (prob=0.8), CutMix [69] (probe=1.0), Random Erasing [73] (prob=0.25) and Exponential Moving Average (EMA) [40]. The maximum rates of increasing stochastic depth [24] are set to 0.1/0.15/0.4/0.5 for RMT-T/S/B/L, respectively. For a more comprehensive comparison, we train two versions of the model. The first version uses only classification loss as the supervision, while the second version, in addition to the classification loss, incorporates token labeling introduced by [27] for additional supervision. Models using token labeling are marked with**”. COCO Object Detection and Instance Segmentation. We apply RetinaNet [32], Mask-RCNN [22] and Cascaded Mask-CNN [2] as the detection frameworks to conduct experiments. We implement them based on the MMDetection [4]. All models are trained under two common settings:““1 x” (12 epochs for training) and“3x+MS” (epochs with multi-scale augmentation for training). For the “1x” setting, images are resized to the shorter side ofpixels. For the “3x+MS”, we use the multi-scale training strategy and randomly resize the shorter side betweento 800 pixels. We apply AdamW optimizer with the initial learning rate of le-4. For RetinaNet, we use the weight decay of le-4 for RetinaNet while we set it to 5e-2 for Mask-RCNN and Cascaded Mask-RCNN. For all settings, we use the batch size of 16, which follows the previous works [35, 63, 64] ADE20K Semantic Segmentation. Based on MMSegmentation [7], we implement UperNet [59] and SemanticFPN [28] to validate our models. For UperNet, we follow the previous setting of Swin-Transformer [35] and train the model for 160k iterations with the input size of 512 x 512. For SemanticFPN, we also use the input resolution of 512 x 512 but train the models for 80k iterations. C. Efficiency Comparison We compare the inference speed of RMT with other backbones, as shown in Tab. 11. Our models achieve the best trade-off between speed and accuracy among many competitors. D. Details of Explicit Decay We use different y for each head of the multi-head ReSA to control the receptive field of each head, enabling the ReSA to perceive multi-scale information. We keep all the + of ReSA’s heads within a certain range. Assuming the given receptive field control interval of a specific ReSA module is [a, b], where both a and b are positive real numbers. And the total number of the ReSA module’s heads is N. The y for its ith head can be written as Eq. 8: (b=a)i st Oo (8) For different stages of different backbones, we use different values of a and b, with the details shown in Tab. 12. --- --Model Blocks Channels Heads Ratios Params(M) FLOPs(G) RMT-T [2, 2, 8, 2] [64, 128, 256, 512] [4,4, 8,16] [3, 3,3, 3] 14 2.RMT-S (3,4, 18,4] [64, 128, 256, 512] [4,4, 8,16] [4, 4,3, 3] 27 4.RMT-B [4, 8, 25,8] [80, 160,320,512] [5,5, 10,16] [4, 4,3, 3] 54 9.RMT-L [4, 8, 25, 8] [112, 224, 448, 640] [7,7, 14,20] [4, 4, 3, 3] 95 18.RMT-DeiT-S [12] [384] [6] [4] 22 4.RMT-Swin-T | [2, 2, 6, 2] [96, 192, 384, 768] [3,6, 12,24] [4,4,4, 4] 29RMT-Swin-S | [2,2, 18,2] [96, 192,384,768] [3,6, 12,24] [4,4,4, 4] 50 9.Table 10. Detailed Architectures of our models. Params FLOPs_ Troughput | Top! Params FLOPs_ Troughput | Top! Model (M) G)_—imgs/s)_ | (%) Model (M) = (G)_—Cimgsis) | (%) MPViT-XS [29] 11 2.9 1496 80.9 Focal-S [63] 51 9.1 351 83.Swin-T [35 29 4.5 1704 81.3 Eff-B5 [47] 30 9.9 302 83.BiFormer-T [75] 13 2.2 1602 81.4 SGFormer-M [15] 39 75 598 84.GC-ViT-XT [20] 20 2.6 1308 82.0 SMT-B [34] 32 77 237 84.SMT-T [34 12 24 636 82.2 BiFormer-B [75] 57 9.8 498 84.RMT-T 14 2.5 1650 82.4 RMT-Swin-S 50 9.1 722 84.Focal-T [63 29 49 582 | 82.2 vee a ° y ne ve cswin-Ttil] | 22 43 isol | 92.7 MTB [I6] °. 7 ° Eff-B4 [47] 19 42 627 82.9 iFormer-B [45] 48 9.4 688 84.MPViTS [29] | 23. 47 986 | 83.0 EMES oh _Eb “o7/_|| Hw Swin-S [35 50 8.8 1006 83.0 Swin-B [35] 88 5.5 756 83.SGFormer-S [15] 23 4.8 952 83.2 Eff-B6 [47] 43 9.0 172 84.iFormer-S [45] 20 4.8 1051 83.4 Focal-B [63] 90 6.4 256 84.CMT-S [16] 25 4.0 848 83.5 CSWin-B [11] 78 5.0 660 84.RMT-Swin-T 29 47 1192 83.6 MPViT-B [29] 75 6.4 498 84.CSwin-S [11] 35 6.9 972 83.6 SMT-L [34] 80 77 158 84.MaxViT-T [51] 31 5.6 826 83.6 SGFormer-B [15] 78 5.6 388 84.SMT-S [34] 20 4.8 356 83.7 iFormer-L [45] 87 4.0 410 84.BiFormer-S [75] 26 4.5 766 83.8 MaxViT-B [51] 120 23.4 306 84.RMT-S 27 4.5 876 84.1 RMT-L 95 8.2 326 85.Table 11. Comparison of inference speed. Model A b References RMT-T (2, 2, 2,2 [6, 6, 8, 8] 1] Moab Arar, Ariel Shamir, and Amit H. Bermano. Learned RMT-S (2,2,2,2] [6,6, 8,8] queries for efficient local attention. In CVPR, 2022.RMT-B (2,2,2,2] [7,7, 8, 8] 2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving RMT-L (2,2,2,2] [8, 8, 8,8] into high quality object detection. In CVPR, 2018. 5,7 3] Chun-Fu (Richard) Chen, Rameswar Panda, and Quanfu RMT-DeiT'S [2] [8] Fan. RegionViT: Regional-to-Local Attention for Vision RMT-Swin-T | [2,2,2,2] [8, 8, 8, 8] Transformers. In JCLR, 2022. 5,RMT-Swin-S | [2, 2, 2,2 [8, 8, 8, 8] 4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, et al. MMDetection: Open mmlab detection toolbox and benchmark. arXiv Table 12. Details about the y decay. preprint arXiv:1906.07155, 2019. 5,5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision --- ---transformers. In NeurIPS, 2021.Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. In JCLR, 2023. 2, 3,MMSegmentation Contributors. Mmsegmentation, an open source semantic segmentation toolbox, 2020. 7,Ekin D Cubuk, Barret Zoph, Jonathon Shlens, et al. Randaugment: Practical automated data augmentation with a reduced search space. In CVPRW, 2020. 4,Jia Deng, Wei Dong, Richard Socher, et al. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.Mingyu Ding, Bin Xiao, Noel Codella, et al. Davit: Dual attention vision transformers. In ECCV, 2022.Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, 2022. 2, 3, 5, 6,7,Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In JCLR, 2021. 1,Qihang Fan, Huaibo Huang, Jiyang Guan, and Ran He. Rethinking local perception in lightweight vision transformer, 2023. 1, 2,Li Gao, Dong Nie, Bo Li, and Xiaofeng Ren. Doubly-fused vit: Fuse information from vision transformer doubly with local representation. In ECCV, 2022.SG-Former: Self guided Transformer with Evolving Token Reallocation. Sucheng ren, xingyi yang, songhua liu, xinchao wang. In JCCV, 2023. 5,Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt: Convolutional neural networks meet vision transformers. In CVPR, 2022. 1, 3, 4, 5, 6, 8,Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, and Shi-Min Hu. Visual attention network. arXiv preprint arXiv:2202.09741, 2022. 5,Kai Han, An Xiao, Enhua Wu, et al. Transformer in transformer. In NeurIPS, 2021.Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In CVPR, 2023. 1, 2, 3,5, 6,Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Global context vision transformers. In ICML, 2023. 5, 6, 7,Kaiming He, Xiangyu Zhang, Shaoging Ren, and Sun Jian. In CVPR, Deep residual learning for image recognition. 2016. 6,Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B. Girshick. Mask r-cnn. In ICCV, 2017. 5,Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi Feng. Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943, 2022.Gao Huang, Yu Sun, and Zhuang Liu. Deep networks with stochastic depth. In ECCV, 2016. 4,Huaibo Huang, Xiaoqiang Zhou, and Ran He. Orthogonal transformer: An efficient vision transformer backbone with token orthogonalization. In NeurIPS, 2022. 5,Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, et al. Scaling up visual and vision-language representation learning with noisy text supervision. In JCML, 2021.Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter: Token labeling for training better vision transformers. In NeurIPS, 2021. 1,5,Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid networks. In CVPR, 2019. 7,Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang. Mpvit: Multi-path vision transformer for dense prediction. In CVPR, 2022. 1, 5, 6,7,Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning, 2022. 1, 4,5,6,Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In CVPR, 2022. 1,Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, and Kaiming He andPiotr Dollar. Focal loss for dense object detection. In ICCV, 2017. 5,Tsung-Yi Lin, Michael Maire, Serge Belongie, et al. Microsoft coco: Common objects in context. In ECCV, 2014.Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, and Lianwen Jin. Scale-aware modulation meet transformer. In JCCV, 2023. 1,5, 6,7, 8,Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCYV, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 9,Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, et al. A convnet for the 2020s. In CVPR, 2022. 5,Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In JCLR, 2023.Junting Pan, Adrian Bulat, Fuwen Tan, et al. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. In ECCV, 2022.Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. In NeurIPS, 2022. 2,Boris T Polyak and Anatoli B Juditsky. of stochastic approximation by averaging. arXiv:1906.07155, 2019.Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In JCLR, 2022. 2,Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, et al. Learning transferable visual models from natural language supervision. In JCML, 2021.Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Lam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. In NeurIPS, 2022. 6,Acceleration arXiv preprint --- ---Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang. Shunted self-attention via multi-scale token aggregation. In CVPR, 2022.Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng YAN. Inception transformer. In NeurIPS, 2022. 5,Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to Transformer for large language models. ArXiv, abs/2307.08621, 2023. 1, 2, 3,Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.Shitao Tang, Jiahui Zhang, Siyu Zhu, et al. Quadtree attention for vision transformers. In JCLR, 2022.Hugo Touvron, Matthieu Cord, Matthijs Douze, et al. Training data-efficient image transformers & distillation through attention. In JCML, 2021. 2, 4, 5,7, 8,Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In JCCV, 2021. 1,Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In ECCV, 2022. 1,5, 8,Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is all you need. In NeurIPS, 2017.Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In CCV, 2021. 2, 3, 4, 6,Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pytv2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):1-10, 2022. 2, 3, 4, 5, 6,Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer hinging on cross-scale attention. In JCLR, 2022. 5, 6,Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In CVPR, 2023. 5, 6,Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. 1,Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable attention. In CVPR, 2022. 2, 6,Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. 7,Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jingren Zhou. mplug-2: A modularized multi-modal foundation model across text, image and video. In JCML, 2023.Chenglin Yang, Yilin Wang, Jianming Zhang, et al. Lite vision transformer with enhanced self-attention. In CVPR, 2022.Chenglin Yang, Siyuan Qiao, Qihang Yu, et al. Moat: Alternating mobile convolution and attention brings strong vision models. In JCLR, 2023.Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal selfattention for local-global interactions in vision transformers. In NeurIPS, 2021. 2, 3, 5, 6, 9,Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao. Focal modulation networks. In NeurIPS, 2022. 5,Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng, and Xiu Li. Scalablevit: Rethinking the context-oriented generalization of vision transformer. In ECCYV, 2022. 5,Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao Mei. Wave-vit: Unifying wavelet and transformers for visual representation learning. In Proceedings of the European conference on computer vision (ECCV), 2022. 2,Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping Zhang, and Tao Mei. Dual vision transformer. TPAMI, 2023.Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual recognition. TPAMI, 2022.Sangdoo Yun, Dongyoon Han, Seong Joon Oh, et al. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 4,Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, et al. mixup: Beyond empirical risk minimization. In JCLR, 2018. 4,Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In ICCV, 2021.Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vsa: Learning varied-size window attention in vision transformers. In ECCV, 2022.Zhun Zhong, Liang Zheng, Guoliang Kang, et al. Random erasing data augmentation. In AAAI, 2020. 4,Bolei Zhou, Hang Zhao, Xavier Puig, et al. Scene parsing through ade20k dataset. In CVPR, 2017.Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson Lau. Biformer: Vision transformer with bi-level routing attention. In CVPR, 2023. 1, 2,4, 5, 6, 8,
"	"--- ABSTRACT ---
Vision Transformer(ViT)는 최근 몇 년 동안 컴퓨터 비전 커뮤니티에서 점점 더 주목을 받고 있습니다. 그러나 ViT의 핵심 구성 요소인 Self-Attention은 명시적 공간 사전이 부족하고 2차 계산 복잡도를 지녀 ViT의 적용성이 제한됩니다. 이러한 문제를 완화하기 위해 NLP 분야의 최근 Retentive Network(RetNet)에서 영감을 얻어 일반적인 목적을 위한 명시적 공간 사전을 갖춘 강력한 비전 백본인 RMT를 제안합니다. 구체적으로 RetNet의 시간적 감쇠 메커니즘을 공간 영역으로 확장하고 맨해튼 거리를 기반으로 하는 공간 감쇠 행렬을 제안하여 Self-Attention에 명시적 공간 사전을 도입합니다. 또한 명시적 공간 사전을 능숙하게 적용하는 어텐션 분해 형식을 제안하여 공간 감쇠 행렬을 방해하지 않고 전역 정보를 모델링하는 계산 부담을 줄이는 것을 목표로 합니다. 공간 감쇠 행렬과 어텐션 분해 형식을 기반으로 선형 복잡도로 명시적 공간 사전을 비전 백본에 유연하게 통합할 수 있습니다. 광범위한 실험에서 RMT가 다양한 비전 작업에서 뛰어난 성능을 보인다는 것이 입증되었습니다. 구체적으로, 추가 학습 데이터 없이 RMT는 ImageNet-1k에서 27M/4.5GFLOPs 및 96M/18.2GFLOPs로 84.8% 및 86.1%의 top-1 acc를 달성합니다. 다운스트림 작업의 경우 RMT는 COCO 감지 작업에서 54.5 박스 AP 및 47.2 마스크 AP, ADE20K 의미 분할 작업에서 52.8 mloU를 달성합니다. 코드는 https://github.com/qhfan/RMT 1에서 제공됩니다.
--- INTRODUCTION ---
Vision Transformer (ViT) [12]는 연구자들에게 매우 선호되는 뛰어난 시각적 아키텍처입니다. 그러나 ViT의 핵심 모듈인 Self-Attention의 내재적 구조는 *Ran He가 해당 저자입니다. Top-1 Acc(%)RMT(Ours) SMTBiFormer MaxViTModel #Params Top1 Acc. MaxVIT-T [31] 31M 83.SMT-S [34] 20M 83.BiFormer-S [75] 26M 83.RMT-S(당사) 27M 84.RMT-S*(당사) 27M 84.BiFormer-B [75] 57M 84.MaxViT-S [29] 69M 84.RMT-B(당사) 54M 85.RMT-B*(당사) 55M 85.SMT-L [34] 81M 84.MaxVIT-B [51] 120M 84.RMT-L(당사) RMT-L*(당사) 95M 85.96M 86.FLOPS(G)그림 1. 224를 사용한 ImageNet-1K에서의 FLOPS 대 Top-1 정확도 224 해상도. &quot;*&quot;는 토큰 레이블링[27]으로 학습된 모델을 나타냅니다. 명시적 공간 사전 확률. 게다가, 셀프 어텐션의 이차 복잡도는 글로벌 정보를 모델링할 때 상당한 계산 비용으로 이어집니다. 이러한 문제는 ViT의 적용을 제한합니다. 많은 연구에서 이전에 이러한 문제를 완화하려고 시도했습니다[13, 16, 30, 35, 50, 57, 61]. 예를 들어, Swin Transformer[35]에서 저자는 윈도잉 연산을 적용하여 셀프 어텐션에 사용되는 토큰을 분할합니다. 이 연산은 셀프 어텐션의 계산 비용을 줄일 뿐만 아니라 윈도우와 상대적 위치 인코딩을 사용하여 모델에 공간 사전 확률을 도입합니다. 이에 더하여 NAT[19]는 셀프 어텐션의 수용 필드를 컨볼루션 모양과 일치하도록 변경하여 계산 비용을 줄이는 동시에 모델이 수용 필드 모양을 통해 공간 사전 확률을 인식할 수 있도록 합니다. 이전 방법과 달리, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간적 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면, 밝은 색상은 더 큰 공간적 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.우리는 이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 감쇠 행렬을 분해합니다.분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다.그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다.우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전을 도입하는 것을 볼 수 있습니다.MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다.우리는 광범위한 실험을 통해 제안된 방법의 효과를 보여줍니다.그림 1에서 볼 수 있듯이, 우리의 RMT는 이미지 분류 작업에서 최첨단(SOTA) 모델보다 성능이 뛰어납니다.또한, 우리의 모델은 객체 감지, 인스턴스 분할, 의미적 분할과 같은 작업에서 다른 모델에 비해 더 두드러진 이점을 보여줍니다.우리의 기여는 다음과 같이 요약할 수 있습니다.• 우리는 Self-Attention을 증강하기 위해 Manhattan Distance에 기반한 공간적 감쇠 행렬을 제안하여 명시적 공간적 사전을 가진 Manhattan Self-Attention(MaSA)을 만듭니다. • MaSA에 대한 분해 형태를 제안하여 공간적 붕괴 행렬을 방해하지 않고 글로벌 정보 모델링에 대한 선형적 복잡성을 가능하게 합니다. • MaSA를 활용하여 일반적인 목적을 위한 강력한 비전 백본인 RMT를 구성합니다. RMT는 추가 학습 데이터 없이도 ImageNet-1k에서 이미지 분류에서 높은 최고 정확도를 달성하며 객체 감지, 인스턴스 분할 및 의미 분할과 같은 작업에서 탁월합니다. 2.
--- RELATED WORK ---
영어: Transformer. Transformer 아키텍처는 [52]에서 처음 제안되어 순환 모델의 학습 한계를 해결한 다음 많은 NLP 작업에서 엄청난 성공을 거두었습니다. 이미지를 작고 겹치지 않는 패치 시퀀스로 분할함으로써 Vision Transformer(ViTs) [12]도 큰 주목을 받았고 비전 작업에서 널리 사용되었습니다 [5, 14, 18, 39, 58, 66]. 과거 RNN과 CNN이 각각 NLP 및 CV 분야를 지배했던 것과 달리 Transformer 아키텍처는 다양한 모달리티와 분야에서 빛을 발했습니다 [26, 37, 42, 60]. 컴퓨터 비전 커뮤니티에서 많은 연구가 ViT에 공간적 사전 확률을 도입하여 학습에 필요한 데이터 요구 사항을 줄이려고 시도하고 있습니다 [6, 19, 49]. 동시에 다양한 희소 어텐션 메커니즘이 셀프 어텐션의 계산 비용을 줄이기 위해 제안되었습니다 [13, 53, 54, 57]. Transformer의 사전 지식. Transformer 모델의 성능을 향상시키기 위해 사전 지식을 통합하려는 시도가 많이 있었습니다. 원래 Transformers[12, 52]는 삼각법 위치 인코딩을 사용하여 각 토큰에 대한 위치 정보를 제공합니다. 비전 작업에서 [35]는 원래 절대 위치 인코딩을 대체하기 위해 상대 위치 인코딩을 사용할 것을 제안합니다. [6]은 합성곱 계층의 제로 패딩이 ViT에 대한 위치 인식을 제공할 수도 있고 이 위치 인코딩이
--- METHOD ---
s, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면 밝은 색상은 더 큰 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 붕괴 행렬을 분해합니다. 분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다. 그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다. 우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전 정보를 도입한다는 것을 알 수 있습니다. MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다. 우리는 광범위한
--- EXPERIMENT ---
s는 RMT가 다양한 비전 작업에서 뛰어난 성능을 보인다는 것을 보여줍니다. 구체적으로, 추가 학습 데이터 없이 RMT는 27M/4.5GFLOPs와 96M/18.2GFLOPs로 ImageNet-1k에서 84.8%와 86.1%의 상위 1 정확도를 달성합니다. 다운스트림 작업의 경우 RMT는 COCO 감지 작업에서 54.5 박스 AP와 47.2 마스크 AP를 달성하고 ADE20K 의미 분할 작업에서 52.8 mloU를 달성합니다. 코드는 https://github.com/qhfan/RMT에서 사용할 수 있습니다. 1. 소개 Vision Transformer(ViT)[12]는 연구자들에게 매우 선호되는 뛰어난 시각적 아키텍처입니다. 그러나 ViT의 핵심 모듈인 Self-Attention의 내재적 구조는 *Ran He가 해당 저자입니다. 상위 1 정확도(%)RMT(저자) SMTBiFormer MaxViTModel #Params 상위 1 정확도. MaxVIT-T [31] 31M 83.SMT-S [34] 20M 83.BiFormer-S [75] 26M 83.RMT-S(당사) 27M 84.RMT-S*(당사) 27M 84.BiFormer-B [75] 57M 84.MaxViT-S [29] 69M 84.RMT-B(당사) 54M 85.RMT-B*(당사) 55M 85.SMT-L [34] 81M 84.MaxVIT-B [51] 120M 84.RMT-L(당사) RMT-L*(당사) 95M 85.96M 86.FLOPS(G)그림 1. 224를 사용한 ImageNet-1K에서의 FLOPS 대 Top-1 정확도 224 해상도. &quot;*&quot;는 토큰 레이블링[27]으로 학습된 모델을 나타냅니다. 명시적 공간 사전 확률. 게다가, 셀프 어텐션의 이차 복잡도는 글로벌 정보를 모델링할 때 상당한 계산 비용으로 이어집니다. 이러한 문제는 ViT의 적용을 제한합니다. 많은 연구에서 이전에 이러한 문제를 완화하려고 시도했습니다[13, 16, 30, 35, 50, 57, 61]. 예를 들어, Swin Transformer[35]에서 저자는 윈도잉 연산을 적용하여 셀프 어텐션에 사용되는 토큰을 분할합니다. 이 연산은 셀프 어텐션의 계산 비용을 줄일 뿐만 아니라 윈도우와 상대적 위치 인코딩을 사용하여 모델에 공간 사전 확률을 도입합니다. 이에 더하여 NAT[19]는 셀프 어텐션의 수용 필드를 컨볼루션 모양과 일치하도록 변경하여 계산 비용을 줄이는 동시에 모델이 수용 필드 모양을 통해 공간 사전 확률을 인식할 수 있도록 합니다. 이전 방법과 달리, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간적 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면, 밝은 색상은 더 큰 공간적 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.우리는 이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 감쇠 행렬을 분해합니다.분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다.그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다.우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전을 도입하는 것을 볼 수 있습니다.MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다.우리는 광범위한 실험을 통해 제안된 방법의 효과를 보여줍니다.그림 1에서 볼 수 있듯이, 우리의 RMT는 이미지 분류 작업에서 최첨단(SOTA) 모델보다 성능이 뛰어납니다.또한, 우리의 모델은 객체 감지, 인스턴스 분할, 의미적 분할과 같은 작업에서 다른 모델에 비해 더 두드러진 이점을 보여줍니다.우리의 기여는 다음과 같이 요약할 수 있습니다.• 우리는 Self-Attention을 증강하기 위해 Manhattan Distance에 기반한 공간적 감쇠 행렬을 제안하여 명시적 공간적 사전을 가진 Manhattan Self-Attention(MaSA)을 만듭니다. • 우리는 MaSA에 대한 분해 형태를 제안하여 공간적 감쇠 행렬을 방해하지 않고 전역 정보 모델링에 대한 선형적 복잡도를 가능하게 합니다.• MaSA를 활용하여 일반적인 목적을 위한 강력한 비전 백본인 RMT를 구성합니다.RMT는 추가 학습 데이터 없이 이미지 분류에서 ImageNet-1k에서 높은 정확도를 달성하고 객체 감지, 인스턴스 분할 및 의미 분할과 같은 작업에서 탁월합니다.2. 관련 작업 Transformer. Transformer 아키텍처는 [52]에서 처음 제안되어 순환 모델의 학습 제한을 해결한 다음 많은 NLP 작업에서 엄청난 성공을 거두었습니다.이미지를 작고 겹치지 않는 패치 시퀀스로 분할함으로써 Vision Transformer(ViT) [12]도 큰 주목을 받았고 비전 작업에서 널리 사용되었습니다 [5, 14, 18, 39, 58, 66]. 과거 RNN과 CNN이 각각 NLP와 CV 분야를 지배했던 것과 달리, 트랜스포머 아키텍처는 다양한 모달리티와 분야에서 빛을 발했습니다[26, 37, 42, 60]. 컴퓨터 비전 커뮤니티에서 많은 연구가 ViT에 공간적 사전 지식을 도입하여 학습에 필요한 데이터 요구 사항을 줄이려고 시도하고 있습니다[6, 19, 49]. 동시에 다양한 희소한 어텐션 메커니즘이 셀프 어텐션의 계산 비용을 줄이기 위해 제안되었습니다[13, 53, 54, 57]. 트랜스포머의 사전 지식. 성능을 향상시키기 위해 트랜스포머 모델에 사전 지식을 통합하려는 시도가 많이 있었습니다. 원래 트랜스포머[12, 52]는 삼각 위치 인코딩을 사용하여 각 토큰에 대한 위치 정보를 제공합니다. 비전 작업에서[35]는 원래 절대 위치 인코딩을 대체하기 위해 상대 위치 인코딩을 사용할 것을 제안합니다. [6]은 합성곱 계층의 제로 패딩이 ViT에 대한 위치 인식을 제공할 수도 있으며, 이 위치 인코딩 방법이 매우 효율적이라고 지적합니다. 많은 연구에서 FFN의 합성곱 [13, 16, 54]은 ViT의 위치 정보를 더욱 풍부하게 하기 위해 비전 모델에 사용되었습니다. NLP 작업의 경우 최근 Retentive Network [46]에서 시간 감쇠 행렬을 도입하여 거리 변화에 따른 사전 지식을 모델에 제공했습니다. RetNet 이전에 ALiBi [41]도 유사한 시간 감쇠 행렬을 사용했습니다. 3. 방법론 3.1. RetNet의 예비 시간 감쇠. Retentive Network(RetNet)는 언어 모델을 위한 강력한 아키텍처입니다. 이 작업은 시퀀스 모델링을 위한 보존 메커니즘을 제안합니다. 보존은 Transformers에 없는 시간 감쇠를 언어 모델에 가져옵니다. 보존은 먼저 순환 방식으로 시퀀스 모델링 문제를 고려합니다. 다음과 같이 쓸 수 있습니다. 1: n On = Σyn-m(Qneine)(Kmeimo) + vm m=병렬 훈련 과정의 경우, Eq. 1은 다음과 같이 표현됩니다: (1) Q = (XWQ), K = (XWк) ©ē, V = XWv On = eine Dnm = [yn-m, n&gt;mn
--- CONCLUSION ---
이 작업에서 우리는 명시적 공간 사전을 가진 비전 백본인 RMT를 제안합니다. RMT는 NLP에서 인과 모델링에 사용되는 시간적 감쇠를 공간 수준까지 확장하고 맨해튼 거리를 기반으로 하는 공간 감쇠 행렬을 도입합니다. 이 행렬은 명시적 공간 사전을 셀프 어텐션에 통합합니다. 또한 RMT는 공간 감쇠 행렬을 방해하지 않고 전역 정보를 희소하게 모델링할 수 있는 셀프 어텐션 분해 형태를 활용합니다. 공간 감쇠 행렬과 어텐션 분해 형태의 조합을 통해 RMT는 명시적 공간 사전 및 선형 복잡도를 가질 수 있습니다. 이미지 분류, 객체 감지, 인스턴스 분할 및 의미 분할에 대한 광범위한 실험을 통해 RMT의 우수성이 검증되었습니다. A. 아키텍처 세부 정보 아키텍처는 표 10에 설명되어 있습니다. 합성곱 스템의 경우 5개의 3 × 3 합성곱을 적용하여 이미지를 56 × 56 토큰에 임베드합니다. GELU 및 배치 정규화는 마지막 합성곱을 제외한 각 합성곱 후에 사용되며, 마지막 합성곱은 배치 정규화만 따릅니다. 단계 사이에 스트라이드 2의 3 × 3 합성곱을 사용하여 피처 맵의 해상도를 줄입니다.CPE에서는 3 × 3 깊이별 합성곱을 채택합니다.또한 LCE에서는 5 × 5 깊이별 합성곱을 채택합니다.RMT-DeiT-S, RMT-Swin-T 및 RMT-Swin-S는 절제 실험에 사용한 모델입니다.이들의 구조는 합성곱 스템, CPE 및 기타 기술을 사용하지 않고도 DeiT[49] 및 Swin-Transformer[35]의 구조와 긴밀하게 일치합니다.B. 실험 설정 ImageNet 이미지 분류.유일한 감독이 분류 손실인 DeiT[49]와 동일한 학습 전략을 채택합니다.특히, 모델은 300에포크 동안 처음부터 학습됩니다.코사인 감쇠 학습률 스케줄러와 5에포크의 선형 워밍업이 있는 Adam W 최적화 프로그램을 사용합니다. 초기 학습률, 가중치 감소 및 배치 크기는 각각 0.001, 0.05 및 1024로 설정됩니다. 증강 설정은 RandAugment[8](randm9-mstd0.5-inc1), Mixup[70](prob=0.8), CutMix[69](probe=1.0), Random Erasing[73](prob=0.25) 및 지수 이동 평균(EMA)[40]입니다. 확률적 깊이 증가의 최대 비율[24]은 각각 RMT-T/S/B/L에 대해 0.1/0.15/0.4/0.5로 설정됩니다. 보다 포괄적인 비교를 위해 두 가지 버전의 모델을 훈련합니다. 첫 번째 버전은 분류 손실만을 감독으로 사용하는 반면, 두 번째 버전은 분류 손실 외에도 추가 감독을 위해 [27]에서 도입한 토큰 레이블링을 통합합니다. 토큰 레이블링을 사용하는 모델은 &quot;*&quot;로 표시됩니다. COCO 객체 감지 및 인스턴스 분할. 실험을 수행하기 위해 RetinaNet[32], Mask-RCNN[22] 및 Cascaded Mask-CNN[2]을 감지 프레임워크로 적용합니다. MMDetection[4]을 기반으로 구현합니다. 모든 모델은 두 가지 공통 설정인 &quot;1×&quot;(학습을 위한 12에포크) 및 &quot;3×+MS&quot;(학습을 위한 다중 스케일 증가가 있는 에포크)에서 학습합니다. &quot;1×&quot; 설정의 경우 이미지가 픽셀의 짧은 쪽으로 크기가 조정됩니다. &quot;3×+MS&quot;의 경우 다중 스케일 학습 전략을 사용하고 짧은 쪽을 무작위로 800픽셀 사이의 크기로 조정합니다. 초기 학습 속도가 1e-4인 AdamW 최적화기를 적용합니다. RetinaNet의 경우 RetinaNet에 대해 1e-4의 가중치 감소를 사용하는 반면 Mask-RCNN 및 Cascaded Mask-RCNN의 경우 5e-2로 설정합니다. 모든 설정에 대해 우리는 이전 연구 [35, 63, 64] ADE20K 의미 분할을 따르는 16의 배치 크기를 사용합니다. MMSegmentation [7]을 기반으로 UperNet [59]과 SemanticFPN [28]을 구현하여 모델을 검증합니다. UperNet의 경우 우리는 Swin-Transformer [35]의 이전 설정을 따르고 512x 512의 입력 크기로 160k 반복에 대해 모델을 학습합니다. SemanticFPN의 경우 우리는 또한 512 x 512의 입력 해상도를 사용하지만 80k 반복에 대해 모델을 학습합니다. C. 효율성 비교 우리는 표 11에서 볼 수 있듯이 RMT의 추론 속도를 다른 백본과 비교합니다. 우리 모델은 많은 경쟁자들 중에서 속도와 정확도 사이에서 가장 좋은 균형을 이룹니다. D. 명시적 붕괴의 세부 사항 Υ 다중 헤드 ReSA의 각 헤드에 대해 다른 것을 사용하여 각 헤드의 수용 필드를 제어하여 ReSA가 다중 스케일 정보를 인식할 수 있도록 합니다. ReSA 헤드의 모든 y를 특정 범위 내에 유지합니다. 특정 ReSA 모듈의 주어진 수용 필드 제어 간격을 [a, b]라고 가정합니다. 여기서 a와 b는 모두 양의 실수입니다. 그리고 ReSA 모듈 헤드의 총 수는 N입니다. i번째 헤드의 Y는 다음과 같이 쓸 수 있습니다. Eq. 8: Vi = 1a(ba)i (8) 다른 백본의 다른 단계에 대해 a와 b의 다른 값을 사용하며 자세한 내용은 표에 나와 있습니다. 12. 모델 블록 채널 헤드 비율 매개변수(M) FLOPS(G) RMT-T [2, 2, 8, 2] RMT-S [3, 4, 18, 4] RMT-B [4, 8, 25, 8] RMT-L [4, 8, 25, 8] RMT-DeiT-S [12] RMT-Swin-T [2, 2, 6, 2] RMT-Swin-S [2, 2, 18, 2] [64, 128, 256, 512] [4, 4, 8, 16] [3, 3, 3, 3]2.[64, 128, 256, 512] [4, 4, 8, 16] [4, 4, 3, 3]4.[80, 160, 320, 512] [112, 224, 448, 640] [5, 5, 10, 16] [4, 4, 3, 3]9.[7, 7, 14, 20] [4, 4, 3, 3]18.[384] [6] [4]4.[96, 192, 384, 768] [96, 192, 384, 768] [3, 6, 12, 24] [4, 4, 4, 4]4.[3, 6, 12, 24] [4, 4, 4, 4]9.표 10. 모델의 세부 아키텍처. 매개변수 FLOPS 모델(M) (G) 처리량 Top(imgs/s) (%) 매개변수 모델(M) FLOPS 처리량 Top(G) (imgs/s) (%) MPVIT-XS [29]2.80.Focal-S [63]9.83.Swin-T [35] BiFormer-T [75] GC-VIT-XT [20] SMT-T [34] RMT-T Focal-T [63] CSWin-T [11] Eff-B4 [47] MPVIT-S [29] Swin-S [35] SGFormer-S [15] iFormer-S [45]4.81.Eff-B5 [47]9.83.2.81.SGFormer-M [15]7.84.2.82.SMT-B [34]7.84.2.82.BiFormer-B [75]9.84.2.82.RMT-Swin-S9.84.Max ViT-S [51]11.84.4.82.CMT-B [16]9.84.4.82.iFormer-B [45]9.84.4.82.RMT-B9.85.4.83.8.83.Swin-B [35]15.83.4.83.Eff-B6 [47]19.84.4.83.Focal-B [63]16.84.CMT-S [16]4.83.CSWin-B [11]15.84.RMT-Swin-T4.83.MPVIT-B [29]16.84.CSwin-S [11]6.83.SMT-L [34]17.84.Max ViT-T [51]5.83.SGFormer-B [15]15.84.SMT-S [34]4.83.iFormer-L [45]14.84.BiFormer-S [75]4.83.Max ViT-B [51]23.84.RMT-S4.84.RMT-L18.85.표 11. 추론 속도 비교. 모델 ab RMT-T [2, 2, 2, 2] [6, 6, 8, 8] RMT-S [2, 2, 2, 2] [6, 6, 8, 8] RMT-B [2, 2, 2, 2] [7, 7, 8, 8] RMT-L RMT-DeiT-S RMT-Swin-T RMT-Swin-S [2, 2, 2, 2] [2] [2, 2, 2, 2] [2, 2, 2, 2] [8, 8, 8, 8] [8, 8, 8, 8] [8, 8, 8, 8] 표 12. 붕괴에 대한 세부 정보. Υ 참고문헌 [1] Moab Arar, Ariel Shamir 및 Amit H. Bermano. 효율적인 로컬 주의를 위한 학습된 쿼리. CVPR, 2022.[2] Zhaowei Cai 및 Nuno Vasconcelos. Cascade r-cnn: 고품질 객체 감지 탐구. CVPR, 2018. 5,[3] Chun-Fu (Richard) Chen, Rameswar Panda 및 Quanfu Fan. Region ViT: 비전 변환기를 위한 지역-지역 주의. ICLR, 2022. 5,[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, et al. MMDetection: Open mmlab 감지 도구 상자 및 벤치마크. arXiv 사전 인쇄본 arXiv:1906.07155, 2019. 5,[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia 및 Chunhua Shen. Twins: 비전 변환기에서 공간 주의 설계 재검토. NeurIPS, 2021.[6] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Chunhua Shen. 비전 변환기를 위한 조건부 위치 인코딩. ICLR, 2023. 2, 3,[7] MMSegmentation 기여자. Mmsegmentation, 오픈 소스 의미 분할 도구 상자, 2020. 7,[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, et al. Randaugment: 검색 공간이 줄어든 실용적 자동 데이터 증강. CVPRW, 2020. 4,[9] Jia Deng, Wei Dong, Richard Socher, et al. Imagenet: 대규모 계층적 이미지 데이터베이스. CVPR, 2009.[10] Mingyu Ding, Bin Xiao, Noel Codella, et al. Davit: 이중 주의 비전 변환기. ECCV, 2022.[11] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al. Cswin transformer: 십자가 모양 창이 있는 일반 비전 변환기 백본. CVPR, 2022. 2, 3, 5, 6, 7,[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. 이미지는 16x16 단어의 가치가 있습니다: 규모에 따른 이미지 인식을 위한 변환기. ICLR, 2021. 1,[13] Qihang Fan, Huaibo Huang, Jiyang Guan, Ran He. 경량 비전 변환기에서 로컬 인식 재고, 2023. 1, 2,[14] Li Gao, Dong Nie, Bo Li, Xiaofeng Ren. 이중 융합 vit: 로컬 표현과 비전 변환기의 정보를 이중으로 융합. ECCV, 2022.[15] SG-Former: 진화하는 토큰 재할당을 갖춘 자체 유도 변환기. 수청런, 싱이양, 류쑹화, 왕신차오. ICCV, 2023. 5,[16] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, Yunhe Wang. Cmt: 컨벌루션 신경망이 비전 변환기를 만납니다. CVPR, 2022. 1, 3, 4, 5, 6, 8,[17] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng 및 Shi-Min Hu. 시각적 주의 네트워크. arXiv 사전 인쇄 arXiv:2202.09741, 2022. 5,[18] Kai Han, An Xiao, Enhua Wu, et al. 변압기 속의 변압기. NeurIPS, 2021.[19] Ali Hassani, Steven Walton, Jiachen Li, Shen Li 및 Humphrey Shi. 이웃주의 변압기. CVPR, 2023. 1, 2, 3, 5, 6,[20] Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz 및 Pavlo Molchanov. 글로벌 컨텍스트 비전 변환기. ICML, 2023. 5, 6, 7,[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Sun Jian. 이미지 인식을 위한 심층 잔여 학습. CVPR, 2016. 6,[22] Kaiming He, Georgia Gkioxari, Piotr Dollár 및 Ross B. Girshick. 마스크 r-cnn. ICCV, 2017. 5,[23] Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, Jiashi Feng. Conv2former: 시각적 인식을 위한 간단한 변환기 스타일 convnet. arXiv 사전 인쇄본 arXiv:2211.11943, 2022.[24] Gao Huang, Yu Sun, Zhuang Liu. 확률적 깊이를 갖춘 딥 네트워크. ECCV에서, 2016. 4,[25] Huaibo Huang, Xiaoqiang Zhou, Ran He. 직교 변환기: 토큰 직교화를 갖춘 효율적인 비전 변환기 백본. NeurIPS에서, 2022. 5,[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, et al. 노이즈가 많은 텍스트 감독을 통한 시각 및 시각 언어 표현 학습 확장. ICML에서, 2021.[27] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, Jiashi Feng. 모든 토큰은 중요합니다: 더 나은 비전 변환기를 훈련하기 위한 토큰 라벨링. NeurIPS, 2021. 1, 5,[28] Alexander Kirillov, Ross Girshick, Kaiming He 및 Piotr Dollar. 파노라마 피처 피라미드 네트워크. CVPR, 2019. 7,[29] Youngwan Lee, Jonghee Kim, Jeffrey Willette 및 Sung Ju Hwang. Mpvit: 밀집 예측을 위한 다중 경로 비전 변환기. CVPR, 2022. 1, 5, 6, 7,[30] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li 및 Yu Qiao. Uniformer: 효율적인 시공간 표현 학습을 위한 통합 변환기, 2022. 1, 4,5,6,[31] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer. Mvitv2: 분류 및 감지를 위한 개선된 다중 스케일 비전 변환기. CVPR에서, 2022. 1,[32] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He 및 Piotr Dollar. 밀집 객체 감지를 위한 초점 손실. ICCV에서, 2017. 5,[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, et al. Microsoft coco: 컨텍스트 내 공통 객체. ECCV에서, 2014.[34] Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin. 스케일 인식 변조가 변압기를 만납니다.ICCV, 2023. 1, 5, 6, 7, 8,[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin 변압기: 이동된 창을 사용하는 계층적 비전 변압기.ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 9,[36] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, et al. 2020년대를 위한 convnet.CVPR, 2022. 5,[37] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi. Unified-io: 비전, 언어 및 다중 모달 작업을 위한 통합 모델. ICLR에서, 2023.[38] Junting Pan, Adrian Bulat, Fuwen Tan, et al. Edgevits: 비전 변환기를 사용한 모바일 기기의 경쟁 경량 CNN. ECCV에서, 2022.[39] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. hilo 어텐션을 사용한 빠른 비전 변환기. NeurIPS에서, 2022. 2,[40] Boris T Polyak 및 Anatoli B Juditsky. 평균화를 통한 확률적 근사 가속화. arXiv 사전 인쇄본 arXiv:1906.07155, 2019.[41] Ofir Press, Noah Smith, and Mike Lewis. 짧게 학습하고 길게 테스트: 선형 편향이 있는 어텐션은 입력 길이 외삽을 가능하게 함. ICLR에서, 2022. 2,[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, et al. 자연어 감독을 통해 전달 가능한 시각적 모델을 학습합니다. ICML, 2021.[43] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Lam Lim 및 Jiwen Lu. Hornet: 재귀 게이트 컨볼루션을 사용한 효율적인 고차 공간 상호 작용. NeurIPS, 2022. 6,[44] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng 및 Xinchao Wang. 다중 규모 토큰 집계를 통해 self-attention을 차단했습니다. CVPR, 2022.[45] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang 및 Shuicheng YAN. 인셉션 트랜스포머. NeurIPS, 2022. 5,[46] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei. Retentive network: 대규모 언어 모델을 위한 Transformer의 후속 모델. Arxiv, abs/2307.08621, 2023. 1, 2, 3,[47] Mingxing Tan 및 Quoc Le. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. ICML, 2019.[48] Shitao Tang, Jiahui Zhang, Siyu Zhu, et al. 비전 변환기를 위한 Quadtree attention. ICLR, 2022.[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, et al. 학습 데이터 효율적인 이미지 변환기 및 주의를 통한 증류. ICML, 2021. 2, 4, 5, 7, 8,[50] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, Hervé Jégou. 이미지 변환기를 더 깊이 파고들다. ICCV, 2021. 1,[51] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li. Maxvit: 다축 비전 변환기. ECCV, 2022. 1, 5, 8,[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 주의만 있으면 됩니다. NeurIPS, 2017.[53] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo 및 Ling Shao. 피라미드 비전 변환기: 컨볼루션 없이 조밀한 예측을 위한 다용도 백본입니다. ICCV, 2021. 2, 3, 4, 6,[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo 및 Ling Shao. Pvtv2: 피라미드 비전 변환기로 기준선을 개선했습니다. 전산 영상 미디어, 8(3):1–10, 2022. 2, 3, 4, 5, 6,[55] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He 및 Wei Liu. Crossformer: 교차 규모의 관심을 끄는 다용도 비전 변환기입니다. ICLR, 2022. 5,6,[56] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: 변형 가능한 컨볼루션을 갖춘 대규모 비전 기반 모델을 탐색합니다. CVPR, 2023. 5, 6,[57] Haiping Wu, Bin Xiao, Noel Codella, Xiyang Dai, Lu Yuan 및 Lei Zhang. 비전 변환기에 컨볼루션을 적용합니다. arXiv:2103.15808, 2021. 1, Mengchen Liu, Cvt: IntroducarXiv 사전 인쇄 [58] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li 및 Gao Huang. 변형 가능한 주의력을 갖춘 비전 트랜스포머. CVPR, 2022. 2, 6,[59] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang 및 Jian Sun. 장면 이해를 위한 통합 지각 분석. ECCV, 2018. 7,[60] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang 및 Jingren Zhou. mplug-2: 텍스트, 이미지, 비디오 전반에 걸쳐 모듈화된 다중 모드 기반 모델입니다. ICML에서는 2023년.[61] Chenglin Yang, Yilin Wang, Jianming Zhang 등. 향상된 Self-Attention을 갖춘 Lite 비전 변환기. CVPR에서는 2022.[62] Chenglin Yang, Siyuan Qiao, Qihang Yu 등. 해자(Moat): 모바일 컨볼루션과 어텐션을 번갈아 사용하면 강력한 비전 모델이 제공됩니다. ICLR, 2023.[63] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan 및 Jianfeng Gao. 비전 변환기의 로컬-글로벌 상호 작용을 위한 초점 selfattention입니다. NeurIPS, 2021. 2, 3, 5, 6, 9,[64] Jianwei Yang, Chunyuan Li, Xiyang Dai 및 Jianfeng Gao. 초점 변조 네트워크. NeurIPS, 2022. 5,[65] Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng 및 Xiu Li. Scalablevit: 비전 변환기의 상황 중심 일반화를 다시 생각합니다. ECCV, 2022. 5,[66] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo 및 Tao Mei. Wave-vit: 시각적 표현 학습을 위한 웨이블릿 및 변환기 통합. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 2022. 2,[67] Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping Zhang 및 Tao Mei. 듀얼 비전 변환기. TPAMI, 2023.[68] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng 및 Shuicheng Yan. Volo: 시각적 인식을 위한 비전 아웃룩커. TPAMI, 2022.[69] Sangdoo Yun, Dongyoon Han, Seong Joon Oh 및 et al. Cutmix: 지역화 가능한 특징을 가진 강력한 분류기를 훈련하기 위한 정규화 전략. ICCV, 2019. 4,[70] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, et al. 혼동: 경험적 위험 최소화를 넘어서는 것입니다. ICLR, 2018. 4,[71] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang 및 Jianfeng Gao. Multi-scale Vision Longformer: 고해상도 이미지 인코딩을 위한 새로운 비전 변환기입니다. ICCV, 2021.[72] Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao. Vsa: 비전 변환기에서 다양한 크기의 창 주의를 학습합니다. ECCV, 2022.[73] Zhun Zhong, Liang Zheng, Guoliang Kang 등. 무작위 삭제 데이터 증대. AAAI, 2020. 4,[74] Bolei Zhou, Hang Zhao, Xavier Puig, et al. ade20k 데이터 세트를 통한 장면 구문 분석. CVPR, 2017.[75] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau. Biformer: 2단계 라우팅 어텐션을 갖춘 비전 변환기. CVPR, 2023. 1, 2, 4, 5, 6, 8,
"
"--- ABSTRACT ---
— 3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero *Equal contribution. 'Computer Science and Engineering, University of Michigan, Ann Arbor, MI, USA, 48109. Contact: Jianing Yang jianingy@umich.edu. ?Nikhil Madaan is an independent researcher. 3New York University. This work is generously supported by NSF IIS-1949634, NSF SES2128623, and Microsoft Academic Program Computing Credit. Project website: https: //chat-with-nerf.github.io/ shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. I.
--- INTRODUCTION ---
Imagine you are put into a 3D scene and asked to find “a chair between dining table and window” (Fig. {Ip. It is easy for humans to figure out the answer. Such a skill is called 3D visual grounding, and we typically rely on it for daily tasks that range from finding objects to manipulating tools. Mastering such an ability is critical to building any household robots to assist humans, as it serves as a basic skill needed for complex navigation (knowing where to go), manipulation (what/where to grab), and question-answering. To endow robots with such an ability, researchers have developed a number of approaches. One direction is to train a 3D-and-text end-to-end neural architecture to propose bounding boxes around objects and jointly model text-boundingbox matching [2-11]. However, such models typically need a large amount of 3D-text pairs for training data, which is This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. --- --difficult to obtain [12,13]. As a result, such trained methods often do not obtain good performance on new scenes. More recently, attempts to address open-vocabulary 3D visual grounding have been made [1,14~23], often building on the strength of CLIP [24]. The dependence on CLIP, however, makes them exhibit “bag-of-words” behaviors where orderless content is modeled well, but attributions, relations, and orders are ignored when processing the text and visual information [25]. For example, as illustrated in Fig. if the text query “a chair between dining table and window” is given to OpenScene [1], the model grounds all of the chairs, window, and dining table in the room, ignoring that the window and dining table are just landmarks used to provide spatial relations with the target chair. At the same time, Large Language Models (LLMs) such as ChatGPT and GPT-4 [26] have demonstrated impressive language understanding capabilities, including planning and tool-using. These abilities enable LLMs to be used as agents to solve complex tasks by breaking the tasks into smaller pieces and knowing when, what, and how to use a tool to complete sub-tasks [27-34]. This is exactly what is needed for 3D visual grounding with complex natural language queries: parsing the compositional language into smaller semantic constituents, interacting with tools and environment to collect feedback, and reasoning with spatial and commonsense knowledge to iteratively ground the language to the target object. Given these observations, we ask the question, Can we use an LLM-based agent to improve zero-shot open-vocabulary 3D visual grounding? In this work, we propose LLM-Grounder, a novel openvocabulary, zero-shot, LLM-agent-based 3D visual grounding pipeline. Our intuition is that an LLM can alleviate the “bag-of-words” weakness of a CLIP-based visual grounder by taking the difficult language decomposition, spatial and commonsense reasoning tasks upon the LLM itself while capitalizing on the strength of a visual grounder to ground simple noun phrases. Described in Section {i} LLM-Grounder uses an LLM at its core to orchestrate the grounding process. The LLM first parses compositional natural language queries into semantic concepts such as object category, object attributes (color, shape, and material), landmarks, and spatial relations. These sub-queries are passed into a visual grounder tool backed by OpenScene [1] or LERF [35], which are CLIP-based [24] open-vocabulary 3D visual grounding methods, to ground each concept in the scene. The visual grounder proposes a few bounding boxes around the most relevant candidate areas in the scene for a concept. For each of these candidates, the visual grounder tools calculate and provide spatial information such as object volumes and distances to landmarks back to the LLM agent to enable the agent to holistically evaluate the situation, in terms of spatial relation and commonsense and select a candidate that best matches all criteria in the original query. This process is repeated until the LLM agent decides it has reached a conclusion. Notably, our approach extends prior neural-symbolic approaches [36] by giving environment feedback to the agent and making the agent’s reasoning process closed-loop. It is important to note that our approach does not need any training on labeled data. It is open-vocabulary and can zero-shot generalize to novel 3D scenes and arbitrary text queries, a desirable property given the semantic diversity of 3D scenes and the limited availability of 3D-text labeled data. In our experiments (Section IV). we evaluate LLM-Grounder on the ScanRefer benchmark [12]. This benchmark primarily evaluates 3D vision-language grounding capability that requires understanding of compositional visual referential expressions. Our approach improves the grounding capability of zero-shot open-vocabulary methods such as OpenScene and LERF, and demonstrates state-ofthe-art zero-shot grounding accuracy on ScanRefer with no labeled data used. Our ablation study shows LLM increases grounding capability more as the language query becomes more complex. These findings underscore the potential of LLM-Grounder as an effective approach for 3D visionlanguage tasks, making it particularly well-suited for robotics applications where understanding complex environments and responding to dynamic queries are essential. In summary, the contribution of this paper is as follows: e We find that using LLM as an agent can improve grounding capability for zero-shot, open-vocabulary methods on the 3D visual grounding task. e We achieve SOTA on ScanRefer in a zero-shot setting, using no labeled data. e We find LLM is more effective when the grounding query text is more complex. Il.
--- RELATED WORK ---
3D Visual Grounding with Natural Language. Grounding a natural language query in an unstructured 3D scene is essential for various robotic tasks. Pioneering benchmarks such as ScanRefer [12] and ReferIt3D [13] have advanced this field. As proposed in these benchmarks, the referential tasks in 3D and text necessitate a deep understanding of both the compositional semantics of language and the structures, geometries, and semantics of 3D scenes. Numerous
--- METHOD ---
, as a visual grounder. When asked to ground the spatially-informed text query “a chair between the dining table and window”, it incorrectly highlights the dining table and window, which are not the target but rather referential landmarks (red bounding boxes). We propose to address this problem by leveraging a large language model (LLM) to 1. Deliberately generate a plan to decompose complex visual grounding queries into sub-tasks; 2. Orchestrate and interact with tools such as target finder and landmark finder to collect information; 3. Leverage spatial and commonsense knowledge to reflect on collected feedback from tools. Abstract— 3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero *Equal contribution. 'Computer Science and Engineering, University of Michigan, Ann Arbor, MI, USA, 48109. Contact: Jianing Yang jianingy@umich.edu. ?Nikhil Madaan is an independent researcher. 3New York University. This work is generously supported by NSF IIS-1949634, NSF SES2128623, and Microsoft Academic Program Computing Credit. Project website: https: //chat-with-nerf.github.io/ shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. I. INTRODUCTION Imagine you are put into a 3D scene and asked to find “a chair between dining table and window” (Fig. {Ip. It is easy for humans to figure out the answer. Such a skill is called 3D visual grounding, and we typically rely on it for daily tasks that range from finding objects to manipulating tools. Mastering such an ability is critical to building any household robots to assist humans, as it serves as a basic skill needed for complex navigation (knowing where to go), manipulation (what/where to grab), and question-answering. To endow robots with such an ability, researchers have developed a number of approaches. One direction is to train a 3D-and-text end-to-end neural architecture to propose bounding boxes around objects and jointly model text-boundingbox matching [2-11]. However, such models typically need a large amount of 3D-text pairs for training data, which is This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. --- --difficult to obtain [12,13]. As a result, such trained methods often do not obtain good performance on new scenes. More recently, attempts to address open-vocabulary 3D visual grounding have been made [1,14~23], often building on the strength of CLIP [24]. The dependence on CLIP, however, makes them exhibit “bag-of-words” behaviors where orderless content is modeled well, but attributions, relations, and orders are ignored when processing the text and visual information [25]. For example, as illustrated in Fig. if the text query “a chair between dining table and window” is given to OpenScene [1], the model grounds all of the chairs, window, and dining table in the room, ignoring that the window and dining table are just landmarks used to provide spatial relations with the target chair. At the same time, Large Language Models (LLMs) such as ChatGPT and GPT-4 [26] have demonstrated impressive language understanding capabilities, including planning and tool-using. These abilities enable LLMs to be used as agents to solve complex tasks by breaking the tasks into smaller pieces and knowing when, what, and how to use a tool to complete sub-tasks [27-34]. This is exactly what is needed for 3D visual grounding with complex natural language queries: parsing the compositional language into smaller semantic constituents, interacting with tools and environment to collect feedback, and reasoning with spatial and commonsense knowledge to iteratively ground the language to the target object. Given these observations, we ask the question, Can we use an LLM-based agent to improve zero-shot open-vocabulary 3D visual grounding? In this work, we propose LLM-Grounder, a novel openvocabulary, zero-shot, LLM-agent-based 3D visual grounding pipeline. Our intuition is that an LLM can alleviate the “bag-of-words” weakness of a CLIP-based visual grounder by taking the difficult language decomposition, spatial and commonsense reasoning tasks upon the LLM itself while capitalizing on the strength of a visual grounder to ground simple noun phrases. Described in Section {i} LLM-Grounder uses an LLM at its core to orchestrate the grounding process. The LLM first parses compositional natural language queries into semantic concepts such as object category, object attributes (color, shape, and material), landmarks, and spatial relations. These sub-queries are passed into a visual grounder tool backed by OpenScene [1] or LERF [35], which are CLIP-based [24] open-vocabulary 3D visual grounding methods, to ground each concept in the scene. The visual grounder proposes a few bounding boxes around the most relevant candidate areas in the scene for a concept. For each of these candidates, the visual grounder tools calculate and provide spatial information such as object volumes and distances to landmarks back to the LLM agent to enable the agent to holistically evaluate the situation, in terms of spatial relation and commonsense and select a candidate that best matches all criteria in the original query. This process is repeated until the LLM agent decides it has reached a conclusion. Notably, our approach extends prior neural-symbolic approaches [36] by giving environment feedback to the agent and making the agent’s reasoning process closed-loop. It is important to note that our approach does not need any training on labeled data. It is open-vocabulary and can zero-shot generalize to novel 3D scenes and arbitrary text queries, a desirable property given the semantic diversity of 3D scenes and the limited availability of 3D-text labeled data. In our
--- EXPERIMENT ---
s (Section IV). we evaluate LLM-Grounder on the ScanRefer benchmark [12]. This benchmark primarily evaluates 3D vision-language grounding capability that requires understanding of compositional visual referential expressions. Our approach improves the grounding capability of zero-shot open-vocabulary methods such as OpenScene and LERF, and demonstrates state-ofthe-art zero-shot grounding accuracy on ScanRefer with no labeled data used. Our ablation study shows LLM increases grounding capability more as the language query becomes more complex. These findings underscore the potential of LLM-Grounder as an effective approach for 3D visionlanguage tasks, making it particularly well-suited for robotics applications where understanding complex environments and responding to dynamic queries are essential. In summary, the contribution of this paper is as follows: e We find that using LLM as an agent can improve grounding capability for zero-shot, open-vocabulary methods on the 3D visual grounding task. e We achieve SOTA on ScanRefer in a zero-shot setting, using no labeled data. e We find LLM is more effective when the grounding query text is more complex. Il. RELATED WORK 3D Visual Grounding with Natural Language. Grounding a natural language query in an unstructured 3D scene is essential for various robotic tasks. Pioneering benchmarks such as ScanRefer [12] and ReferIt3D [13] have advanced this field. As proposed in these benchmarks, the referential tasks in 3D and text necessitate a deep understanding of both the compositional semantics of language and the structures, geometries, and semantics of 3D scenes. Numerous methods that are jointly trained on 3D and language have been proposed [2-11] to advance performance. However, these methods are limited to closed-vocabulary settings due to the specific object classes presented in the original ScanNet 37], upon which these benchmarks are built. Motivated by advances in 2D open-vocabulary segmentation [38-40], researchers have explored 3D open-vocabulary grounding 1,14~21,35,41]. However, these methods mostly rely on CLIP [24] as the underlying vision-language bridge. This works well when the grounding text query is a simple noun phrase (e.g., “a red apple”); however, research has shown CLIP exhibits “bag-of-words” behavior and lacks compositional understanding such as relation, attribution, and order of either text or visual [25], a crucial aspect of the challenges presented in ScanRefer and Referlt3D. Recognizing this aspect, Semantic Abstraction [22] and 3D-CLR [23] use spatial-informed text-and-3D data to train a dedicated neural --- --eo.Find wooden chair near table. Let_me check for wooden Fun Agent ‘chairs near a table. Target Finder Observation: User is looking for a wooden chair near a table. Reasoning: ""Need to look for wooden chairs within close vicinity of a table. Name: Chair Attribute: Wooden Plan: 1. Call Target Finder with wooden chair and call Landmark Finder with table. . Compare candidates by their closeness to the landmark table. . Ask user for further clarification if needed. . Repeat until the correct object is found. [Plopenscene / #6 LERF Landmark Finder —) Name: Table Relationship: Near [Blopenscene / 6 LERF YP I think target #LLM Agent is what you want. »> Time Pus Agent Observation: Target Finder and Landmark Finder returned me coordinates. I also object volume and distance to landmarks for each candidate. Reasoning: Need to aggregate the information and decide. Plan: 1.Compare chair candidates by their closeness to the landmark table. 2.Examine the object volume and filter out any object too small. 3.Score, rank and filter each candidate holistically. 4.Make a grounding decision. Self-critique: I should make a decision based on the holistic situation, taking account all factors into consideration. Fig. 2: Overview of LLM-Grounder. Given a query to ground an object, our approach, backed by an LLM agent, reasons on the user’s request and generates a plan to ground the object by using tools. The agent interacts with tools such as target find and landmark finder to gather information such as object bounding box, object volume, and distances to landmarks from the tools. This information is then returned to the agent to conduct further spatial and commonsense reasoning to rank, filter and select the best matching candidate. network to parse and ground the compositional semantics of the text query before grounding. In contrast, our method explores the possibilities of using an LLM agent to accomplish the same without training (zero-shot). NS3D [36] uses LLM-based code generation to generate programs to address this problem, which is more similar to our approach, but it also uses ground-truth object segmentation and category to simplify visual grounding and thus lacks open-vocabulary and zero-shot capabilities. LLM Agents Recent advancements in large language models (LLM) [26,42-46] have demonstrated surprising emerging abilities. Here, we list a few abilities that enable LLM to be used as an agent. a) Planning: Planning involves breaking complex goals into sub-goals and self-reflecting based on issued actions and environmental feedback. Chain-of-thought [27] shows that LLM demonstrated better planning capabilities when instructed to “think step-by-step” by decomposing complex tasks into smaller tasks. Tree-of-thoughts [28] extends this approach by exploring multiple thoughts per step, turning the chain into a tree. [47-50] demonstrate that LLM, when instructed to self-reflect on its output and environmental feedback, can produce better output. b) Tool-Using: The ability to use tools is a unique feature of human intelligence. Recognizing that current LLMs are not good at all tasks (math and factual questionanswering problems, for example), researchers have explored possibilities of letting LLMs orchestrate tool-using to fulfill a task. At its core, the tool-using problem is to decide which tool to use and when to use them. Socratic Models [29] uses natural language as a medium to engage an LLM agent in a guided discussion with other multimodal language models, such as vision-language models and audio-language models, to complete a task collectively. MRKL [51] and TAML [52] equip an LLM with a calculator and demonstrate its increased ability to solve math problems. Building on these findings, software libraries like LangChain [53] has been developed to streamline LLM tool-using for developers. ToolFormer [30], HuggingGPT [54] and API-Bank [55] push tool-using further by opening up more APIs and machine learning models as tools for LLM to use. In robotics, SayCan [31], InnerMonologue [32], Code as Policies [33] and LM-Nav [34] use the planning and toolusing capability of LLM to let it serve as a high-level controller of real robots for long-horizon, complex tasks. The success obtained in these tasks motivates us to use LLM as an agent to help solve the compositional language-vision understanding challenges presented in 3D visual grounding. Il. METHOD Recently, success stories from Auto-GPT [56], GPTEngineer [57], and ToolFormer [30] show early signs of success in using LLM as an agent. An agent is different from a traditional model in machine learning in that it has agency: it is an entity that is driven by a goal, reasons about its goal, comes up with plans, examines and uses tools, and interacts with and collects feedback from the environment. In the 3D Visual Grounding setting, an agent can be a promising solution to the “bag-of-words” behavior exhibited by existing models. In LLM-Grounder, we use GPT-4 as the agent and prompt it to complete three tasks: 1. Break down the complex text query into sub-tasks that can be better handled by downstream tools like a CLIP-based 3D visual grounder, such as OpenScene and LERF; 2. Orchestrate and use such tools to solve the sub-tasks it proposes; and 3. Reason on feedback from the environment by incorporating --- --Training Size Open-Vocab Method Visual Grounder +LLM Agent Acc@0.25 + Acc@0.5 t 36k labeled ScanRefer[12] 34.4 20.20K fave closed-vocab 3D-text data 3DVG-Trans[2] 41.5 28.LERF[35] LERF x 44 0.zero-shot open-vocab Ours LERF v¥ GPT-4 6.9 (42.5) 1.6 (+1.3) OpenScene[1] OpenScene x 13.0 5.zero-shot open-vocab Ours OpenScene ¥ GPT-3.5 14.3 (41.3) 4.7 (-0.4) Ours OpenScene Jv GPT-4 17.1 (44.1) 5.3 (40.2) TABLE I: Experiment results on ScanRefer. LLM (GPT-4) agent significantly increases 3D grounding capabilities for zeroshot open-vocabulary 3D grounders such as LERF and OpenScene. We measure grounding capability by Accuracy @0.and @0.5, which are accuracies of bounding box predictions whose Intersection-over-Union (IoU) w.r.t. ground-truth box exceeds 0.25 and 0.5, respectively. Numbers in parentheses represent performance gain or loss after adding LLM agent. Results also show that a less powerful LLM, such as GPT-3.5, is not able to achieve strong grounding capability gain. Lastly, although not directly comparable with our method which is zero-shot open-vocabulary, performances are listed for methods that are trained on ScanRefer and closed-vocabulary for completeness. LERF OpenScene ; . w/o LLM 10.8 27.Low Visual Difficulty TEM 15.1 (44.3) 33.6 (46.0) oo ; w/o LLM 2.5 8.High Visual Difficulty vy LEM 44 (1.9) 12.1 (43.5) TABLE II: Ablation study on visual complexity. LLM agent is more effective for 3D grounding in low visual difficulty settings. Numbers shown are Acc@.25. spatial understanding and common sense to make grounding decisions. Planning. The first advantage of LLMs is their ability to plan. Research has shown that chain-of-thought reasoning [27], i.e., explicitly prompting LLM to break complex goals down into smaller sub-tasks (“think step-by-step”) can help arithmetic, commonsense, and symbolic reasoning tasks. Inspired by these findings, we design our agent likewise as illustrated in Figure | Specifically, we first ask the agent to describe its observation, which gives the agent a chance to summarize the current situation. The context can encompass the human text query and the returned information from tools (described below). The agent then starts a section called reasoning, which serves as a mental scratchpad for the agent to perform high-level planning. Then, in the plan section, the agent must list more specific steps to fulfill the high-level plan, including any tool-using, comparison, or calculation. The agent can reflect on the generated plan in the self-critique section and make any final corrections [50]. Tool-Using. The second advantage of LLMs stems from their ability to use tools. We instruct the LLM agent to use tools to solve the “bag-of-words” behavior (Sec. [Ip. As shown in Fig. |2| we inform LLM of the expected input and output format, i.e., the APIs, of two tools we designed for visual grounding and feedback, and ask the LLM agent to interact with them following the given format. The tools include a Target Finder and a Landmark Finder. Target Finder and Landmark Finder. The target finder and landmark finder take in a text query input, find bounding boxes of clusters of possible locations for the query, and return a list of candidate bounding boxes in the form of centroids and sizes (C,,C,,C,,AX,AY,AZ). Target is the main object that a user refers to in a query (“chair” in “a chair between dining table and window”); landmark is the object used to spatially refer to the target (“dining table” and “window’”). The target finder additionally computes the volume for each candidate and the landmark finder additionally computes the Euclidean distance from each target candidate’s centroid to the landmark’s centroid. The volume, distance, and bounding boxes together provide feedback for the LLM agent to conduct spatial and commonsense reasoning. For example, a candidate “chair” with a volume as small as 0.01m?> is probably a false positive and should be filtered out; a candidate whose distance to the landmark does not comply with the spatial relation mentioned by the query should be rejected. The target finder and landmark finder are implemented by open-vocabulary CLIP-based 3D visual grounders LERF [35] and OpenScene [1]. These tools alone exhibit “bag-of-words” behaviors (Sec. {I} when given complex text queries; however, when given simpler text queries such as a simple noun phrase (“a chair”), such tools can usually work well. The LLM agent capitalizes on this capability of noun-phrase grounding of such 3D visual grounders while compensating for their weaknesses in language understanding and spatial reasoning by decomposing the complex grounding queries, grounding one object at a time, and reasoning about their spatial relation afterward. To use the target finder, we instruct the LLM agent to parse out noun phrases (e.g., “wooden chair”) from the original natural language query; to use the landmark finder, we instruct the LLM agent to parse out any landmark objects mentioned in the original query and their spatial relation to the target object. --- --IV. EXPERIMENTS In experiments, we first would like to evaluate how well the LLM-based agent improves zero-shot open-vocabulary 3D visual grounding, compared with CLIP-based 3D visual grounding methods. Then we evaluate our method in the closed-vocabulary setting and compare it with closedvocabulary and trained approaches. Finally, we show some qualitative examples on in-the-wild scenes, to show the generalization of our approach. A. Dataset ScanRefer. ScanRefer [12] is a benchmark on 3D object localization in indoor 3D scenes using natural language. It consists of 51,583 human-written descriptions of 11,objects of 18 semantic categories from 800 ScanNet [37] 3D scenes, where the train/val/test split contains 36,665, 9,508 and 5,410 descriptions, respectively. We use the firstscenes from the validation split for the experiments presented in Table |I| which consists of 998 text-and-3D-object pairs. We also report two standard metrics of ScanRefer: Accuracy@0.25 and Accuracy@0.5. 0.25 and 0.5 are different thresholds for IoU of 3D bounding boxes. B. Baseline Methods ScanRefer. ScanRefer [12] uses an end-to-end 3D-text neural architecture to localize objects given a natural language input. Specifically, it processes the 3D point cloud into PointNet++ [58] features, then clusters the points and proposes bounding boxes of objects. The language features are then fused together with the clusters and boxes to decide which boxes are the ones referred to by the language. The pipeline uses supervision from the text and b-box pairs and the ground-truth b-boxes and semantic class for all objects in the scene. We include this baseline as a show of the current trained pipeline’s performance, serving as a ceiling compared to our zero-shot setting where no supervision is used. 3DVG-Transformer. 3DVG-Transformer [2] builds on ScanRefer’s end-to-end neural architecture and proposes a new neural module to aggregate close-by clusters before proposing bounding boxes. Similar to ScanRefer, 3DVGTransformer also uses supervision of ground-truth object bboxes, semantic class, and human-annotated descriptions. OpenScene and LERF. OpenScene [1] and LERF [35] are zero-shot open-vocabulary 3D scene understanding approaches. OpenScene distills 2D CLIP features into a 3D point cloud and allows grounding with a text query by calculating the cosine similarity between the CLIP text embedding of the query and every point in the 3D point cloud. LERF achieves the same by encoding CLIP embeddings into a neural radiance field, These methods, when used alone, exhibit “bag-of-words” behavior as illustrated in a problem we aim to address with LLM agent deliberative reasoning. To produce bounding boxes using OpenScene and LERF for the 3D visual grounding benchmark ScanRefer, we apply DBSCAN clustering [59] on points with high cosine similarity and draw bounding boxes around them. C. Results We first show qualitative results of LLM-Grounder in Fig. More results and demonstrations can be found on the project websitd!] including in-the-wild scenes. Compared with baselines, we find the LLM agent can improve zero-shot, open vocabulary grounding. As shown in Table[f] the addition of an LLM agent can significantly increase the grounding performance of both LERF and OpenScene by achieving 5.0% and 17.1% on Accuracy @0.25, respectively. We attribute the lower increase in performance for LERF to the weaker overall grounding capability of LERF. The lower increase suggests that when the tool provides too noisy of a feedback to an LLM agent, it is hard for the LLM agent to reason with the noisy input and improve performance. We also note the low increase in performance on Accuracy @0.5, which requires the predicted b-box to have more than 50% overlaps with the ground-truth box. We attribute this to the lack of instance segmentation capability of the underlying grounder. We observe that the grounders often predict too large or too small of a bounding for the correctly grounded object. Such prediction is not correctable by an LLM thus causing the difficulty of precise visual grounding and the low performance increase. Additionally, we find that when using GPT-3.5 as the agent for OpenScene, the performance drops compared to without GPT. We attribute this to the weaker tool-using and spatial and commonsense reasoning capability of GPT-3.5. D. Ablation Study We then evaluate what the LLM-agent primarily improves on. We test two different settings: (1) does the LLM-agent help more with a more difficult visual context? (2) does the LLM-agent help more for more difficult text queries? Difficulty of visual context. We categorize the results by vision difficulties in Table [II] and find that LLM agent is more effective for low vision difficulty queries, evidenced by the higher grounding performance increase. Specifically, we separate the grounding queries into Low Visual Difficulty and High Visual Difficulty categories. A query has low visual difficulty if the object mentioned in the text query is the sole object of that class in a scene (0 distractor); a query has high visual difficulty if there are more than | distractor object of the same class in a scene. Out of the 998 queries we evaluated, 232 queries had low visual difficulty, and 766 queries had high visual difficulty. Results in Table show that LLM brings more performance increase for the low visual difficulty queries. This behavior can be explained by the different challenges presented in low- and highvisual-difficulty settings. In low visual difficulty settings, the main challenge an open-vocabulary 3D grounder faces is the “bag-of-words” behavior. For example, if the text query is “the sink in the kitchen” and if there is only one sink in the scene, a bag-of-words grounder would highlight the whole kitchen, leading to low grounding precision. An LLM agent is particularly good at solving this problem by https://chat-with-nerf.github.io/ --- --‘Target Finder Use Target Finder to ground the target with phrase: Light gray monitor; ‘there is a light gray monitor sitting on the left side of a desk. The desk is smaller and curved.” Use Landmark Finder to ground landmark with phrase: smaller curved desk by the window. Its relation to target is under. LLM Agent The monitor should be on the left side of the desk, so the centroid’s x-value should be less than the Landmark's xvalue. And I received the bounding box and volume for each candidate. I pick candidate @ since it is the best match so far. LLM Agent Fig. 3: Qualitative example. LLM agent uses spatial reasoning to successfully disambiguate the correct object instance. 10%) R~2 OpenScene = 0.63 * * 5 Dpenscene . © LERF 8%] R*~2 LERF = 0.5% 2% 0% -2% w/ LLM - w/o LLM 5% 8% 1"" 2 4 6 8Number of Nouns in SentenceFig. 4: Performance delta (w/ LLM - w/o LLM) vs. query text complexity. The LLM helps more when the text query is more complex but fails to help significantly at higher complexities. 35% =- ° OpenScene w/ LLM sox] R (OPenScene LLM) =-0.38 * ee ree eum 5 R (OpenScene noLLM) = -0.48 + LERF w/LLM 2 25%} R (LERF LLM) = -0.18 —_LERF w/o LLM % 20% ERF noLLM) = -0.© G 15% <10% +. f 5%) 0% 1 * a2 4 6 8Number of Nouns in SentenceFig. 5: Performance of various models vs query text complexity. All models struggle with more complex sentences, but models with an LLM agent perform better, especially at these higher complexities. parsing out the target object “sink” and only issuing this single noun to the grounder, thus circumventing the bag-ofwords behavior. For high visual difficulty settings, however, there is one additional challenge: instance disambiguation. Because there are multiple instances of the same class in the scene, the visual grounder would return many candidates to the LLM agent. The LLM agent could use its spatial and commonsense reasoning capability to filter out some instances with volume and distance to landmark information, but more complex instance disambiguation usually requires more nuanced visual cues, a privilege an LLM agent doesn’t have because it is blind. Difficulty of text queries. As queries become more complex, the LLM-agent will help performance, but only up to a certain point. We can measure query complexity by counting the number of nouns in the sentence: the more nouns in a description, the more difficult it will be to ground any specific object. We see from Fig. [5] that, both with and without the help of an LLM agent, performance decreases as sentence complexity increases. However, from analyzing the performance difference between using an LLM agent and not using one, we see that there is a quadratic dependence on query complexity (Fig. This suggests that the LLM provides an advantage for grounding when presented with higher-complexity queries, but after reaching some threshold, the performance advantage diminishes. When query complexity is low, models without an LLM can ground objects effectively, so LLMs provide minimal advantage. As complexity increases, baseline models perform worse and LLMs provide a more significant advantage. However, with increased complexity of referential expression, LLM’s spatial reasoning capability may not surpass the performance of noLLM baselines. We may require stronger LLMs to produce advantages in these higher complexity ranges. V.
--- CONCLUSION ---
. Notably, our approach extends prior neural-symbolic approaches [36] by giving environment feedback to the agent and making the agent’s reasoning process closed-loop. It is important to note that our approach does not need any training on labeled data. It is open-vocabulary and can zero-shot generalize to novel 3D scenes and arbitrary text queries, a desirable property given the semantic diversity of 3D scenes and the limited availability of 3D-text labeled data. In our experiments (Section IV). we evaluate LLM-Grounder on the ScanRefer benchmark [12]. This benchmark primarily evaluates 3D vision-language grounding capability that requires understanding of compositional visual referential expressions. Our approach improves the grounding capability of zero-shot open-vocabulary methods such as OpenScene and LERF, and demonstrates state-ofthe-art zero-shot grounding accuracy on ScanRefer with no labeled data used. Our ablation study shows LLM increases grounding capability more as the language query becomes more complex. These findings underscore the potential of LLM-Grounder as an effective approach for 3D visionlanguage tasks, making it particularly well-suited for robotics applications where understanding complex environments and responding to dynamic queries are essential. In summary, the contribution of this paper is as follows: e We find that using LLM as an agent can improve grounding capability for zero-shot, open-vocabulary methods on the 3D visual grounding task. e We achieve SOTA on ScanRefer in a zero-shot setting, using no labeled data. e We find LLM is more effective when the grounding query text is more complex. Il. RELATED WORK 3D Visual Grounding with Natural Language. Grounding a natural language query in an unstructured 3D scene is essential for various robotic tasks. Pioneering benchmarks such as ScanRefer [12] and ReferIt3D [13] have advanced this field. As proposed in these benchmarks, the referential tasks in 3D and text necessitate a deep understanding of both the compositional semantics of language and the structures, geometries, and semantics of 3D scenes. Numerous methods that are jointly trained on 3D and language have been proposed [2-11] to advance performance. However, these methods are limited to closed-vocabulary settings due to the specific object classes presented in the original ScanNet 37], upon which these benchmarks are built. Motivated by advances in 2D open-vocabulary segmentation [38-40], researchers have explored 3D open-vocabulary grounding 1,14~21,35,41]. However, these methods mostly rely on CLIP [24] as the underlying vision-language bridge. This works well when the grounding text query is a simple noun phrase (e.g., “a red apple”); however, research has shown CLIP exhibits “bag-of-words” behavior and lacks compositional understanding such as relation, attribution, and order of either text or visual [25], a crucial aspect of the challenges presented in ScanRefer and Referlt3D. Recognizing this aspect, Semantic Abstraction [22] and 3D-CLR [23] use spatial-informed text-and-3D data to train a dedicated neural --- --eo.Find wooden chair near table. Let_me check for wooden Fun Agent ‘chairs near a table. Target Finder Observation: User is looking for a wooden chair near a table. Reasoning: ""Need to look for wooden chairs within close vicinity of a table. Name: Chair Attribute: Wooden Plan: 1. Call Target Finder with wooden chair and call Landmark Finder with table. . Compare candidates by their closeness to the landmark table. . Ask user for further clarification if needed. . Repeat until the correct object is found. [Plopenscene / #6 LERF Landmark Finder —) Name: Table Relationship: Near [Blopenscene / 6 LERF YP I think target #LLM Agent is what you want. »> Time Pus Agent Observation: Target Finder and Landmark Finder returned me coordinates. I also object volume and distance to landmarks for each candidate. Reasoning: Need to aggregate the information and decide. Plan: 1.Compare chair candidates by their closeness to the landmark table. 2.Examine the object volume and filter out any object too small. 3.Score, rank and filter each candidate holistically. 4.Make a grounding decision. Self-critique: I should make a decision based on the holistic situation, taking account all factors into consideration. Fig. 2: Overview of LLM-Grounder. Given a query to ground an object, our approach, backed by an LLM agent, reasons on the user’s request and generates a plan to ground the object by using tools. The agent interacts with tools such as target find and landmark finder to gather information such as object bounding box, object volume, and distances to landmarks from the tools. This information is then returned to the agent to conduct further spatial and commonsense reasoning to rank, filter and select the best matching candidate. network to parse and ground the compositional semantics of the text query before grounding. In contrast, our method explores the possibilities of using an LLM agent to accomplish the same without training (zero-shot). NS3D [36] uses LLM-based code generation to generate programs to address this problem, which is more similar to our approach, but it also uses ground-truth object segmentation and category to simplify visual grounding and thus lacks open-vocabulary and zero-shot capabilities. LLM Agents Recent advancements in large language models (LLM) [26,42-46] have demonstrated surprising emerging abilities. Here, we list a few abilities that enable LLM to be used as an agent. a) Planning: Planning involves breaking complex goals into sub-goals and self-reflecting based on issued actions and environmental feedback. Chain-of-thought [27] shows that LLM demonstrated better planning capabilities when instructed to “think step-by-step” by decomposing complex tasks into smaller tasks. Tree-of-thoughts [28] extends this approach by exploring multiple thoughts per step, turning the chain into a tree. [47-50] demonstrate that LLM, when instructed to self-reflect on its output and environmental feedback, can produce better output. b) Tool-Using: The ability to use tools is a unique feature of human intelligence. Recognizing that current LLMs are not good at all tasks (math and factual questionanswering problems, for example), researchers have explored possibilities of letting LLMs orchestrate tool-using to fulfill a task. At its core, the tool-using problem is to decide which tool to use and when to use them. Socratic Models [29] uses natural language as a medium to engage an LLM agent in a guided discussion with other multimodal language models, such as vision-language models and audio-language models, to complete a task collectively. MRKL [51] and TAML [52] equip an LLM with a calculator and demonstrate its increased ability to solve math problems. Building on these findings, software libraries like LangChain [53] has been developed to streamline LLM tool-using for developers. ToolFormer [30], HuggingGPT [54] and API-Bank [55] push tool-using further by opening up more APIs and machine learning models as tools for LLM to use. In robotics, SayCan [31], InnerMonologue [32], Code as Policies [33] and LM-Nav [34] use the planning and toolusing capability of LLM to let it serve as a high-level controller of real robots for long-horizon, complex tasks. The success obtained in these tasks motivates us to use LLM as an agent to help solve the compositional language-vision understanding challenges presented in 3D visual grounding. Il. METHOD Recently, success stories from Auto-GPT [56], GPTEngineer [57], and ToolFormer [30] show early signs of success in using LLM as an agent. An agent is different from a traditional model in machine learning in that it has agency: it is an entity that is driven by a goal, reasons about its goal, comes up with plans, examines and uses tools, and interacts with and collects feedback from the environment. In the 3D Visual Grounding setting, an agent can be a promising solution to the “bag-of-words” behavior exhibited by existing models. In LLM-Grounder, we use GPT-4 as the agent and prompt it to complete three tasks: 1. Break down the complex text query into sub-tasks that can be better handled by downstream tools like a CLIP-based 3D visual grounder, such as OpenScene and LERF; 2. Orchestrate and use such tools to solve the sub-tasks it proposes; and 3. Reason on feedback from the environment by incorporating --- --Training Size Open-Vocab Method Visual Grounder +LLM Agent Acc@0.25 + Acc@0.5 t 36k labeled ScanRefer[12] 34.4 20.20K fave closed-vocab 3D-text data 3DVG-Trans[2] 41.5 28.LERF[35] LERF x 44 0.zero-shot open-vocab Ours LERF v¥ GPT-4 6.9 (42.5) 1.6 (+1.3) OpenScene[1] OpenScene x 13.0 5.zero-shot open-vocab Ours OpenScene ¥ GPT-3.5 14.3 (41.3) 4.7 (-0.4) Ours OpenScene Jv GPT-4 17.1 (44.1) 5.3 (40.2) TABLE I: Experiment results on ScanRefer. LLM (GPT-4) agent significantly increases 3D grounding capabilities for zeroshot open-vocabulary 3D grounders such as LERF and OpenScene. We measure grounding capability by Accuracy @0.and @0.5, which are accuracies of bounding box predictions whose Intersection-over-Union (IoU) w.r.t. ground-truth box exceeds 0.25 and 0.5, respectively. Numbers in parentheses represent performance gain or loss after adding LLM agent. Results also show that a less powerful LLM, such as GPT-3.5, is not able to achieve strong grounding capability gain. Lastly, although not directly comparable with our method which is zero-shot open-vocabulary, performances are listed for methods that are trained on ScanRefer and closed-vocabulary for completeness. LERF OpenScene ; . w/o LLM 10.8 27.Low Visual Difficulty TEM 15.1 (44.3) 33.6 (46.0) oo ; w/o LLM 2.5 8.High Visual Difficulty vy LEM 44 (1.9) 12.1 (43.5) TABLE II: Ablation study on visual complexity. LLM agent is more effective for 3D grounding in low visual difficulty settings. Numbers shown are Acc@.25. spatial understanding and common sense to make grounding decisions. Planning. The first advantage of LLMs is their ability to plan. Research has shown that chain-of-thought reasoning [27], i.e., explicitly prompting LLM to break complex goals down into smaller sub-tasks (“think step-by-step”) can help arithmetic, commonsense, and symbolic reasoning tasks. Inspired by these findings, we design our agent likewise as illustrated in Figure | Specifically, we first ask the agent to describe its observation, which gives the agent a chance to summarize the current situation. The context can encompass the human text query and the returned information from tools (described below). The agent then starts a section called reasoning, which serves as a mental scratchpad for the agent to perform high-level planning. Then, in the plan section, the agent must list more specific steps to fulfill the high-level plan, including any tool-using, comparison, or calculation. The agent can reflect on the generated plan in the self-critique section and make any final corrections [50]. Tool-Using. The second advantage of LLMs stems from their ability to use tools. We instruct the LLM agent to use tools to solve the “bag-of-words” behavior (Sec. [Ip. As shown in Fig. |2| we inform LLM of the expected input and output format, i.e., the APIs, of two tools we designed for visual grounding and feedback, and ask the LLM agent to interact with them following the given format. The tools include a Target Finder and a Landmark Finder. Target Finder and Landmark Finder. The target finder and landmark finder take in a text query input, find bounding boxes of clusters of possible locations for the query, and return a list of candidate bounding boxes in the form of centroids and sizes (C,,C,,C,,AX,AY,AZ). Target is the main object that a user refers to in a query (“chair” in “a chair between dining table and window”); landmark is the object used to spatially refer to the target (“dining table” and “window’”). The target finder additionally computes the volume for each candidate and the landmark finder additionally computes the Euclidean distance from each target candidate’s centroid to the landmark’s centroid. The volume, distance, and bounding boxes together provide feedback for the LLM agent to conduct spatial and commonsense reasoning. For example, a candidate “chair” with a volume as small as 0.01m?> is probably a false positive and should be filtered out; a candidate whose distance to the landmark does not comply with the spatial relation mentioned by the query should be rejected. The target finder and landmark finder are implemented by open-vocabulary CLIP-based 3D visual grounders LERF [35] and OpenScene [1]. These tools alone exhibit “bag-of-words” behaviors (Sec. {I} when given complex text queries; however, when given simpler text queries such as a simple noun phrase (“a chair”), such tools can usually work well. The LLM agent capitalizes on this capability of noun-phrase grounding of such 3D visual grounders while compensating for their weaknesses in language understanding and spatial reasoning by decomposing the complex grounding queries, grounding one object at a time, and reasoning about their spatial relation afterward. To use the target finder, we instruct the LLM agent to parse out noun phrases (e.g., “wooden chair”) from the original natural language query; to use the landmark finder, we instruct the LLM agent to parse out any landmark objects mentioned in the original query and their spatial relation to the target object. --- --IV. EXPERIMENTS In experiments, we first would like to evaluate how well the LLM-based agent improves zero-shot open-vocabulary 3D visual grounding, compared with CLIP-based 3D visual grounding methods. Then we evaluate our method in the closed-vocabulary setting and compare it with closedvocabulary and trained approaches. Finally, we show some qualitative examples on in-the-wild scenes, to show the generalization of our approach. A. Dataset ScanRefer. ScanRefer [12] is a benchmark on 3D object localization in indoor 3D scenes using natural language. It consists of 51,583 human-written descriptions of 11,objects of 18 semantic categories from 800 ScanNet [37] 3D scenes, where the train/val/test split contains 36,665, 9,508 and 5,410 descriptions, respectively. We use the firstscenes from the validation split for the experiments presented in Table |I| which consists of 998 text-and-3D-object pairs. We also report two standard metrics of ScanRefer: Accuracy@0.25 and Accuracy@0.5. 0.25 and 0.5 are different thresholds for IoU of 3D bounding boxes. B. Baseline Methods ScanRefer. ScanRefer [12] uses an end-to-end 3D-text neural architecture to localize objects given a natural language input. Specifically, it processes the 3D point cloud into PointNet++ [58] features, then clusters the points and proposes bounding boxes of objects. The language features are then fused together with the clusters and boxes to decide which boxes are the ones referred to by the language. The pipeline uses supervision from the text and b-box pairs and the ground-truth b-boxes and semantic class for all objects in the scene. We include this baseline as a show of the current trained pipeline’s performance, serving as a ceiling compared to our zero-shot setting where no supervision is used. 3DVG-Transformer. 3DVG-Transformer [2] builds on ScanRefer’s end-to-end neural architecture and proposes a new neural module to aggregate close-by clusters before proposing bounding boxes. Similar to ScanRefer, 3DVGTransformer also uses supervision of ground-truth object bboxes, semantic class, and human-annotated descriptions. OpenScene and LERF. OpenScene [1] and LERF [35] are zero-shot open-vocabulary 3D scene understanding approaches. OpenScene distills 2D CLIP features into a 3D point cloud and allows grounding with a text query by calculating the cosine similarity between the CLIP text embedding of the query and every point in the 3D point cloud. LERF achieves the same by encoding CLIP embeddings into a neural radiance field, These methods, when used alone, exhibit “bag-of-words” behavior as illustrated in a problem we aim to address with LLM agent deliberative reasoning. To produce bounding boxes using OpenScene and LERF for the 3D visual grounding benchmark ScanRefer, we apply DBSCAN clustering [59] on points with high cosine similarity and draw bounding boxes around them. C. Results We first show qualitative results of LLM-Grounder in Fig. More results and demonstrations can be found on the project websitd!] including in-the-wild scenes. Compared with baselines, we find the LLM agent can improve zero-shot, open vocabulary grounding. As shown in Table[f] the addition of an LLM agent can significantly increase the grounding performance of both LERF and OpenScene by achieving 5.0% and 17.1% on Accuracy @0.25, respectively. We attribute the lower increase in performance for LERF to the weaker overall grounding capability of LERF. The lower increase suggests that when the tool provides too noisy of a feedback to an LLM agent, it is hard for the LLM agent to reason with the noisy input and improve performance. We also note the low increase in performance on Accuracy @0.5, which requires the predicted b-box to have more than 50% overlaps with the ground-truth box. We attribute this to the lack of instance segmentation capability of the underlying grounder. We observe that the grounders often predict too large or too small of a bounding for the correctly grounded object. Such prediction is not correctable by an LLM thus causing the difficulty of precise visual grounding and the low performance increase. Additionally, we find that when using GPT-3.5 as the agent for OpenScene, the performance drops compared to without GPT. We attribute this to the weaker tool-using and spatial and commonsense reasoning capability of GPT-3.5. D. Ablation Study We then evaluate what the LLM-agent primarily improves on. We test two different settings: (1) does the LLM-agent help more with a more difficult visual context? (2) does the LLM-agent help more for more difficult text queries? Difficulty of visual context. We categorize the results by vision difficulties in Table [II] and find that LLM agent is more effective for low vision difficulty queries, evidenced by the higher grounding performance increase. Specifically, we separate the grounding queries into Low Visual Difficulty and High Visual Difficulty categories. A query has low visual difficulty if the object mentioned in the text query is the sole object of that class in a scene (0 distractor); a query has high visual difficulty if there are more than | distractor object of the same class in a scene. Out of the 998 queries we evaluated, 232 queries had low visual difficulty, and 766 queries had high visual difficulty. Results in Table show that LLM brings more performance increase for the low visual difficulty queries. This behavior can be explained by the different challenges presented in low- and highvisual-difficulty settings. In low visual difficulty settings, the main challenge an open-vocabulary 3D grounder faces is the “bag-of-words” behavior. For example, if the text query is “the sink in the kitchen” and if there is only one sink in the scene, a bag-of-words grounder would highlight the whole kitchen, leading to low grounding precision. An LLM agent is particularly good at solving this problem by https://chat-with-nerf.github.io/ --- --‘Target Finder Use Target Finder to ground the target with phrase: Light gray monitor; ‘there is a light gray monitor sitting on the left side of a desk. The desk is smaller and curved.” Use Landmark Finder to ground landmark with phrase: smaller curved desk by the window. Its relation to target is under. LLM Agent The monitor should be on the left side of the desk, so the centroid’s x-value should be less than the Landmark's xvalue. And I received the bounding box and volume for each candidate. I pick candidate @ since it is the best match so far. LLM Agent Fig. 3: Qualitative example. LLM agent uses spatial reasoning to successfully disambiguate the correct object instance. 10%) R~2 OpenScene = 0.63 * * 5 Dpenscene . © LERF 8%] R*~2 LERF = 0.5% 2% 0% -2% w/ LLM - w/o LLM 5% 8% 1"" 2 4 6 8Number of Nouns in SentenceFig. 4: Performance delta (w/ LLM - w/o LLM) vs. query text complexity. The LLM helps more when the text query is more complex but fails to help significantly at higher complexities. 35% =- ° OpenScene w/ LLM sox] R (OPenScene LLM) =-0.38 * ee ree eum 5 R (OpenScene noLLM) = -0.48 + LERF w/LLM 2 25%} R (LERF LLM) = -0.18 —_LERF w/o LLM % 20% ERF noLLM) = -0.© G 15% <10% +. f 5%) 0% 1 * a2 4 6 8Number of Nouns in SentenceFig. 5: Performance of various models vs query text complexity. All models struggle with more complex sentences, but models with an LLM agent perform better, especially at these higher complexities. parsing out the target object “sink” and only issuing this single noun to the grounder, thus circumventing the bag-ofwords behavior. For high visual difficulty settings, however, there is one additional challenge: instance disambiguation. Because there are multiple instances of the same class in the scene, the visual grounder would return many candidates to the LLM agent. The LLM agent could use its spatial and commonsense reasoning capability to filter out some instances with volume and distance to landmark information, but more complex instance disambiguation usually requires more nuanced visual cues, a privilege an LLM agent doesn’t have because it is blind. Difficulty of text queries. As queries become more complex, the LLM-agent will help performance, but only up to a certain point. We can measure query complexity by counting the number of nouns in the sentence: the more nouns in a description, the more difficult it will be to ground any specific object. We see from Fig. [5] that, both with and without the help of an LLM agent, performance decreases as sentence complexity increases. However, from analyzing the performance difference between using an LLM agent and not using one, we see that there is a quadratic dependence on query complexity (Fig. This suggests that the LLM provides an advantage for grounding when presented with higher-complexity queries, but after reaching some threshold, the performance advantage diminishes. When query complexity is low, models without an LLM can ground objects effectively, so LLMs provide minimal advantage. As complexity increases, baseline models perform worse and LLMs provide a more significant advantage. However, with increased complexity of referential expression, LLM’s spatial reasoning capability may not surpass the performance of noLLM baselines. We may require stronger LLMs to produce advantages in these higher complexity ranges. V. CONCLUSION AND LIMITATIONS We introduced LLM-Grounder, a novel approach for 3D visual grounding that leverages Large Language Models (LLMs) as the central agent for orchestrating the grounding process. Our empirical evaluations demonstrate that LLM-Grounder excels particularly in handling complex text queries, thereby offering a robust, zero-shot, open-vocabulary solution for 3D visual grounding tasks. However, there are some limitations to consider. Cost: Utilizing GPT-based models as the core reasoning agent can be computationally expensive, which may limit its deployment in resourceconstrained environments. Latency: The reasoning process, due to the inherent latency of GPT models, can be slow. This latency could be a significant bottleneck for real-time robotic applications where rapid decision-making is crucial. Despite these limitations, LLM-Grounder sets a new benchmark in 3D visual grounding and opens up avenues for future research in integrating LLMs with robotic systems. --- ---REFERENCES S. Peng, K. Genova, C. M. Jiang, A. Tagliasacchi, M. Pollefeys, and T. Funkhouser, “Openscene: 3d scene understanding with open vocabularies,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. L. Zhao, D. Cai, L. Sheng, and D. Xu, “3DVG-Transformer: Relation modeling for visual grounding on point clouds,” in JCCV, 2021, pp. 2928-2937. J. Roh, K. Desingh, A. Farhadi, and D. Fox, “Languagerefer: Spatiallanguage model for 3d visual grounding,” in Conference on Robot Learning. PMLR, 2022, pp. 1046-1056. D. Cai, L. Zhao, J. Zhang, L. Sheng, and D. Xu, “3djeg: A unified framework for joint dense captioning and visual grounding on 3d point clouds,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 464-16 473. J. Chen, W. Luo, R. Song, X. Wei, L. Ma, and W. Zhang, “Learning point-language hierarchical alignment for 3d visual grounding,” 2022. D. Z. Chen, Q. Wu, M. NieBner, and A. X. Chang, “D3net: A speakerlistener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans,” 2021. Z. Yuan, X. Yan, Y. Liao, R. Zhang, S. Wang, Z. Li, and S. Cui, “Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1791-1800. E. Bakr, Y. Alsaedy, and M. Elhoseiny, “Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding,” Advances in Neural Information Processing Systems, vol. 35, pp. 37 146-37 158, 2022. H. Liu, A. Lin, X. Han, L. Yang, Y. Yu, and S. Cui, “Refer-it-in-rgbd: A bottom-up approach for 3d visual grounding in rgbd images,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6032-6041. A. Jain, N. Gkanatsios, I. Mediratta, and K. Fragkiadaki, “Bottom up top down detection transformers for language grounding in images and point clouds,” in European Conference on Computer Vision. Springer, 2022, pp. 417-433. S. Huang, Y. Chen, J. Jia, and L. Wang, “Multi-view transformer for 3d visual grounding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15 524-15 533. D. Z. Chen, A. X. Chang, and M. NieBner, “Scanrefer: 3d object localization in rgb-d scans using natural language,” /6th European Conference on Computer Vision (ECCV), 2020. P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. J. Guibas, “Referlt3D: Neural listeners for fine-grained 3d object identification in real-world scenes,” in /6th European Conference on Computer Vision (ECCYV), 2020. B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler, “Open-vocabulary queryable scene representations for real world planning,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). EEE, 2023, pp. 11509-11522. R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, “Pla: Language-driven open-vocabulary 3d scene understanding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7010-7019. S. Y. Gadre, M. Wortsman, G. Iharco, L. Schmidt, and S. Song, “Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation,” CVPR, 2023. C. Huang, O. Mees, A. Zeng, and W. Burgard, “Visual language maps for robot navigation,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). YEEE, 2023, pp. 10608-10615. K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, A. Maalouf, S. Li, G. S. Iyer, S. Saryazdi, N. V. Keetha er al., “Conceptfusion: Open-set multimodal 3d mapping,” in [CRAWorkshop on Pretraining for Robotics (PT4R), 2023. K. Mazur, E. Sucar, and A. J. Davison, “Feature-realistic neural fusion for real-time, open set scene understanding,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). YEEE, 2023, pp. 8201-8207. N. M. M. Shafiullah, C. Paxton, L. Pinto, S$. Chintala, and A. Szlam, “Clip-fields: Weakly supervised semantic fields for robotic memory,” in ICRA2023 Workshop on Pretraining for Robotics (PT4R), 2023.22)24.34,40. A. Takmaz, E. Fedele, R. W. Sumner, M. Pollefeys, F. Tombari, and F. Engelmann, “OpenMask3D: Open-Vocabulary 3D Instance Segmentation,” arXiv preprint arXiv:2306.13631, 2023. H. Ha and S. Song, “Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models,” in 6th Annual Conference on Robot Learning, 2022. [Online]. Available: https://openreview.net/forum?id=IV-rNbX VSaO Y. Hong, C. Lin, Y. Du, Z. Chen, J. B. Tenenbaum, and C. Gan, “3d concept learning and reasoning from multi-view images,” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable visual models from natural language supervision,” in International conference on machine learning. PMLR, 2021, pp. 8748-8763. M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou, “When and why vision-language models behave like bags-of-words, and what to do about it?” in The Eleventh International Conference on Learning Representations, 2022. OpenAI, “Gpt-4 technical report,” 2023. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large language models,” Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022. S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” arXiv preprint arXiv:2305.10601, 2023. A. Zeng, M. Attarian, K. M. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. S. Ryoo, V. Sindhwani, J. Lee er al., “Socratic models: Composing zero-shot multimodal reasoning with language,” in The Eleventh International Conference on Learning Representations, 2022. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” 2023. M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., “Do as i can, not as i say: Grounding language in robotic affordances,” arXiv preprint arXiv:2204.01691, 2022. W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar ef al., “Inner monologue: Embodied reasoning through planning with language models,” in Conference on Robot Learning. PMLR, 2023, pp. 1769-1782. J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, “Code as policies: Language model programs for embodied control,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). YEEE, 2023, pp. 9493-9500. D. Shah, B. Osinski, B. Ichter, and S. Levine, “LM-nav: Robotic navigation with large pre-trained models of language, vision, and action,” in 6th Annual Conference on Robot Learning, 2022. [Online]. Available ntpsdopenreview neforum?id=UWSA3SweAHT J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik, “Lerf: Language embedded radiance fields,” in International Conference on Computer Vision (ICCV), 2023. J. Hsu, J. Mao, and J. Wu, “Ns3d: Neuro-symbolic grounding of 3d objects and relations,’ 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2614-2623, 2023. [Online]. Available: |https://api.semanticscholar.org/CorpusID: A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niefner, “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl, “Language-driven semantic segmentation,” in Jnternational Ci i) 022. [Online]. Available: G. , xX. , Y. : \-Y. ' ‘aling open-vocabulary image segmentation with image-level labels,” in European Conference on Computer Vision. Springer, 2022, pp. 540-557. F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu, “Open-vocabulary semantic segmentation with mask-adapted clip,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7061-7070. --- ---50. SIS. Kobayashi, E. Matsumoto, and V. Sitzmann, “Decomposing nerf for editing via feature field distillation?’ in Advances in Neural Information Processing Systems, vol. 35, 2022. [Online]. Available: own. Mann, N. ubbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell e¢ al., “Language models are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020. J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” in International Conference on Learning Representations, 2021. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray ef al., “Training language models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27744, 2022. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,’ The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” arXiv preprint arXiv:2210.03629, 2022. N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” arXiv preprint arXiv:2303.11366, 2023. H. Liu, C. Sferrazza, and P. Abbeel, “Chain of hindsight aligns language models with feedback,” arXiv preprint arXiv:2302.02676, vol. 3, 2023. E. Jang, “Can Ilms critique and iterate on their own outputs?” evjang.com, Mar 2023. [Online]. Available: https://evjang.com/2023/ 03/26/self-reflection.html E. Karpas, O. Abend, Y. Belinkov, B. Lenz, O. Lieber, N. Ratner, Y. Shoham, H. Bata, Y. Levine, K. Leyton-Brown ef al., “Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning,” arXiv preprint arXiv:2205.00445, 2022. A. Parisi, Y. Zhao, and N. Fiedel, “Talm: Tool augmented language models,” arXiv preprint arXiv:2205.12255, 2022. H. Chase, “LangChain,” Oct. 2022. [Online]. Available: YY. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,” arXiv preprint arXiv:2303.17580, 2023. M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li, “Api-bank: A benchmark for tool-augmented Ilms,” arXiv preprint arXiv:2304.08244, 2023. “Auto-gpt,” ht c. R. Qi, hierarchical feature learning on point sets in a metric space,” Advances in neural information processing systems, vol. 30, 2017. M. Ester, H.-P. Kriegel, J. Sander, X. Xu ef al., “A density-based algorithm for discovering clusters in large spatial databases with noise,” in kdd, vol. 96, no. 34, 1996, pp. 226-231.
"	"--- ABSTRACT ---
3D 시각적 접지는 가정용 로봇에 중요한 기술로, 로봇이 주변 환경에 따라 탐색하고, 물체를 조작하고, 질문에 답할 수 있도록 해줍니다. 기존 접근 방식은 종종 광범위한 레이블이 지정된 데이터에 의존하거나 복잡한 언어 쿼리를 처리하는 데 한계가 있는 반면, 저희는 새로운 제로샷, 오픈 어휘, 대규모 언어 모델(LLM) 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. LLM-Grounder는 LLM을 활용하여 복잡한 자연어 쿼리를 의미적 구성 요소로 분해하고 OpenScene 또는 LERF와 같은 시각적 접지 도구를 사용하여 3D 장면에서 객체를 식별합니다. 그런 다음 LLM은 제안된 객체 간의 공간적 및 상식적 관계를 평가하여 최종 접지 결정을 내립니다. 저희의 방법은 레이블이 지정된 학습 데이터가 필요하지 않으며 새로운 3D 장면과 임의의 텍스트 쿼리로 일반화할 수 있습니다. 저희는 ScanRefer 벤치마크에서 LLM-Grounder를 평가하고 최첨단 zero*Equal 기여를 보여줍니다. ¹컴퓨터 과학 및 공학, 미시간 대학교, 미시간주 앤아버, 미국, 48109. 연락처: Jianing Yang jianingy@umich.edu. 2Nikhil Madaan은 독립 연구원입니다. 3뉴욕 대학교. 이 연구는 NSF IIS-1949634, NSF SES2128623 및 Microsoft Academic Program Computing Credit의 후원을 받았습니다. 프로젝트 웹사이트: https://chat-with-nerf.github.io/ 샷 그라운딩 정확도. 저희의 연구 결과에 따르면 LLM은 그라운딩 기능을 크게 개선하며, 특히 복잡한 언어 쿼리의 경우 LLM-Grounder가 로봇 공학의 3D 비전 언어 작업에 효과적인 접근 방식입니다. I.
--- INTRODUCTION ---
3D 장면에 배치되어 &quot;식탁과 창문 사이의 의자&quot;(그림 1)를 찾으라는 요청을 받았다고 상상해 보세요. 사람이 답을 알아내는 것은 쉽습니다. 이러한 기술을 3D 시각적 접지라고 하며, 일반적으로 물건을 찾는 것에서 도구를 조작하는 것까지 일상적인 작업에 이 기술을 사용합니다. 이러한 능력을 익히는 것은 복잡한 탐색(어디로 가야 할지 아는 것), 조작(무엇/어디를 잡을지) 및 질의 응답에 필요한 기본 기술이므로 사람을 돕는 가정용 로봇을 만드는 데 중요합니다. 로봇에 이러한 능력을 부여하기 위해 연구자들은 여러 가지 접근 방식을 개발했습니다. 한 가지 방향은 3D 및 텍스트 엔드투엔드 신경 구조를 훈련하여 물체 주위에 경계 상자를 제안하고 텍스트-경계 상자 매칭을 공동으로 모델링하는 것입니다[2–11]. 그러나 이러한 모델은 일반적으로 훈련 데이터를 위해 많은 양의 3D-텍스트 쌍이 필요하며, 이는 이 작업은 가능한 출판을 위해 IEEE에 제출되었습니다. 저작권은 통지 없이 양도될 수 있으며, 그 후에는 이 버전에 더 이상 액세스할 수 없을 수 있습니다. 영어: 얻기 어려움[12,13]. 결과적으로, 이러한 훈련된 방법은 종종 새로운 장면에서 좋은 성능을 얻지 못합니다. 최근에는 오픈 어휘 3D 시각적 접지를 해결하려는 시도가 이루어졌으며[1,14–23], 종종 CLIP의 강점을 기반으로 구축되었습니다[24]. 그러나 CLIP에 대한 의존성으로 인해 &quot;단어 가방&quot; 동작을 나타내어 순서 없는 콘텐츠가 잘 모델링되지만 텍스트와 시각적 정보를 처리할 때 속성, 관계 및 순서가 무시됩니다[25]. 예를 들어, 그림 1에서 볼 수 있듯이 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;가 OpenScene[1]에 제공되면 모델은 창문과 식탁이 대상 의자와의 공간적 관계를 제공하는 데 사용되는 랜드마크일 뿐이라는 사실을 무시하고 방에 있는 모든 의자, 창문 및 식탁을 접지합니다. 동시에 ChatGPT 및 GPT-4[26]와 같은 대규모 언어 모델(LLM)은 계획 및 도구 사용을 포함하여 인상적인 언어 이해 기능을 보여주었습니다. 이러한 능력을 통해 LLM은 작업을 더 작은 조각으로 나누고 하위 작업을 완료하기 위해 언제, 무엇, 어떻게 도구를 사용해야 하는지 아는 방식으로 복잡한 작업을 해결하는 에이전트로 사용될 수 있습니다[27–34]. 이는 복잡한 자연어 쿼리를 사용한 3D 시각적 접지에 정확히 필요한 것입니다. 구성 언어를 더 작은 의미적 구성 요소로 구문 분석하고, 도구 및 환경과 상호 작용하여 피드백을 수집하고, 공간적 지식과 상식적 지식으로 추론하여 언어를 대상 객체에 반복적으로 접지합니다. 이러한 관찰 결과를 바탕으로 LLM 기반 에이전트를 사용하여 제로샷 오픈 어휘 3D 시각적 접지를 개선할 수 있는지라는 질문을 던집니다. 이 연구에서는 새로운 오픈 어휘, 제로샷, LLM 에이전트 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. 우리의 직감은 LLM이 CLIP 기반 시각적 그라운더의 &quot;단어 가방&quot; 약점을 완화하여 LLM 자체에 어려운 언어 분해, 공간 및 상식적 추론 작업을 맡기고 시각적 그라운더의 강점을 활용하여 간단한 명사 구를 그라운딩할 수 있다는 것입니다. 섹션 III에서 설명한 LLM-Grounder는 그라운딩 프로세스를 조율하기 위해 핵심에 LLM을 사용합니다. LLM은 먼저 구성적 자연어 쿼리를 객체 범주, 객체 속성(색상, 모양 및 재료), 랜드마크 및 공간 관계와 같은 의미적 개념으로 구문 분석합니다. 이러한 하위 쿼리는 CLIP 기반[24] 오픈 어휘 3D 시각적 그라운딩 방법인 OpenScene[1] 또는 LERF[35]가 지원하는 시각적 그라운더 도구로 전달되어 장면에서 각 개념을 그라운딩합니다. 시각적 그라운더는 개념에 대한 장면에서 가장 관련성 있는 후보 영역 주변에 몇 개의 경계 상자를 제안합니다. 이러한 후보들 각각에 대해 시각적 그라운더 도구는 객체 볼륨 및 랜드마크까지의 거리와 같은 공간 정보를 계산하여 LLM 에이전트에게 제공하여 에이전트가 공간 관계 및 상식의 관점에서 상황을 전체적으로 평가하고 원래 쿼리의 모든 기준과 가장 잘 일치하는 후보를 선택할 수 있도록 합니다. 이 프로세스는 LLM 에이전트가 결론에 도달했다고 결정할 때까지 반복됩니다. 주목할 점은, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 어떠한 훈련도 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리로 제로샷 일반화할 수 있으며, 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의 실험(섹션 IV)에서 우리는 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 비전 언어 접지 기능을 평가합니다. 저희의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 오픈 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 저희의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 결과는 LLM-Grounder가 3D 비전 언어 작업에 효과적인 접근 방식으로서 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • LLM을 에이전트로 사용하면 3D 시각적 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있음을 발견했습니다. • 레이블이 지정된 데이터를 사용하지 않고 제로샷 설정에서 ScanRefer에 대한 SOTA를 달성합니다. • 접지 쿼리 텍스트가 더 복잡할 때 LLM이 더 효과적임을 발견했습니다. II.
--- RELATED WORK ---
자연어를 사용한 3D 시각적 접지. 구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다. ScanRefer [12] 및 ReferIt3D [13]와 같은 선구적인 벤치마크는 이 분야를 발전시켰습니다. 이러한 벤치마크에서 제안한 대로 3D 및 텍스트의 참조 작업은 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다. 수많은
--- METHOD ---
, 시각적 그라운더로서. 공간 정보 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;를 그라운딩하라는 요청을 받았을 때, 대상이 아니라 참조 랜드마크(빨간색 경계 상자)인 식탁과 창문을 잘못 강조합니다. 우리는 대규모 언어 모델(LLM)을 활용하여 이 문제를 해결하고자 합니다. 1. 복잡한 시각적 그라운딩 쿼리를 하위 작업으로 분해하는 계획을 의도적으로 생성합니다. 2. 대상 찾기 및 랜드마크 찾기와 같은 도구를 조정하고 상호 작용하여 정보를 수집합니다. 3. 공간적 지식과 상식적 지식을 활용하여 도구에서 수집한 피드백을 반영합니다. 초록 3D 시각적 그라운딩은 가정용 로봇에 중요한 기술로, 로봇이 주변 환경에 따라 탐색하고, 물체를 조작하고, 질문에 답할 수 있도록 합니다. 기존 접근 방식은 종종 광범위한 레이블이 지정된 데이터에 의존하거나 복잡한 언어 쿼리를 처리하는 데 한계가 있지만, 우리는 새로운 제로샷, 오픈 어휘, 대규모 언어 모델(LLM) 기반 3D 시각적 그라운딩 파이프라인인 LLM-Grounder를 제안합니다. LLM-Grounder는 LLM을 사용하여 복잡한 자연어 쿼리를 의미적 구성 요소로 분해하고 OpenScene 또는 LERF와 같은 시각적 접지 도구를 사용하여 3D 장면에서 객체를 식별합니다. 그런 다음 LLM은 제안된 객체 간의 공간적 및 상식적 관계를 평가하여 최종 접지 결정을 내립니다. 저희 방법은 레이블이 지정된 학습 데이터가 필요하지 않으며 새로운 3D 장면과 임의의 텍스트 쿼리로 일반화할 수 있습니다. 저희는 ScanRefer 벤치마크에서 LLM-Grounder를 평가하고 최첨단 zero*Equal 기여를 보여줍니다. ¹컴퓨터 과학 및 공학, 미시간 대학교, 앤아버, 미시간, 미국, 48109. 연락처: Jianing Yang jianingy@umich.edu. 2Nikhil Madaan은 독립 연구원입니다. 3뉴욕 대학교. 이 연구는 NSF IIS-1949634, NSF SES2128623 및 Microsoft Academic Program Computing Credit의 후원을 받았습니다. 프로젝트 웹사이트: https://chat-with-nerf.github.io/ 샷 그라운딩 정확도. 저희의 연구 결과에 따르면 LLM은 그라운딩 기능을 크게 개선하는데, 특히 복잡한 언어 쿼리의 경우 LLM-Grounder가 로봇공학에서 3D 비전 언어 작업에 효과적인 접근 방식입니다. I. 서론 3D 장면에 들어가서 &quot;식탁과 창문 사이의 의자&quot;(그림 1)를 찾으라는 요청을 받았다고 상상해 보세요. 사람이 답을 알아내는 것은 쉽습니다. 이러한 기술을 3D 시각적 그라운딩이라고 하며, 일반적으로 물건을 찾는 것에서 도구를 조작하는 것에 이르기까지 일상적인 작업에 의존합니다. 이러한 능력을 습득하는 것은 복잡한 탐색(어디로 가야 하는지 아는 것), 조작(무엇/어디를 잡을지), 질의 응답에 필요한 기본 기술 역할을 하기 때문에 사람을 돕는 가정용 로봇을 만드는 데 중요합니다. 로봇에 이러한 능력을 부여하기 위해 연구자들은 여러 가지 접근 방식을 개발했습니다. 한 가지 방향은 객체 주변의 경계 상자를 제안하고 텍스트-경계 상자 매칭을 공동으로 모델링하기 위해 3D 및 텍스트 엔드투엔드 신경 구조를 훈련하는 것입니다[2–11]. 그러나 이러한 모델은 일반적으로 훈련 데이터를 위해 대량의 3D-텍스트 쌍이 필요하며, 이는 얻기 어렵습니다[12,13]. 결과적으로 이러한 훈련된 방법은 종종 새로운 장면에서 좋은 성능을 얻지 못합니다. 최근에는 오픈 어휘 3D 시각적 접지를 해결하려는 시도가 이루어졌으며[1,14–23], 종종 CLIP의 강점을 기반으로 합니다[24]. 그러나 CLIP에 대한 의존성으로 인해 순서 없는 내용은 잘 모델링되지만 텍스트 및 시각적 정보를 처리할 때 속성, 관계 및 순서는 무시되는 &quot;단어 가방&quot; 동작을 보입니다[25]. 예를 들어, 그림 1에서 설명한 대로, 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;가 OpenScene [1]에 주어지면 모델은 방에 있는 모든 의자, 창문, 식탁을 근거로 삼고, 창문과 식탁이 대상 의자와의 공간적 관계를 제공하는 데 사용되는 랜드마크일 뿐이라는 사실을 무시합니다. 동시에 ChatGPT 및 GPT-4 [26]와 같은 대규모 언어 모델(LLM)은 계획 및 도구 사용을 포함한 인상적인 언어 이해 기능을 보여주었습니다. 이러한 기능을 통해 LLM은 작업을 더 작은 조각으로 나누고 하위 작업을 완료하기 위해 도구를 언제, 무엇을, 어떻게 사용해야 하는지 아는 방식으로 복잡한 작업을 해결하는 에이전트로 사용될 수 있습니다 [27–34]. 이는 복잡한 자연어 쿼리를 사용한 3D 시각적 근거에 정확히 필요한 것입니다. 구성 언어를 더 작은 의미 구성 요소로 구문 분석하고, 도구 및 환경과 상호 작용하여 피드백을 수집하고, 공간적 지식과 상식적 지식으로 추론하여 언어를 대상 객체에 반복적으로 근거로 삼는 것입니다. 이러한 관찰을 바탕으로, 우리는 LLM 기반 에이전트를 사용하여 제로샷 오픈 어휘 3D 시각적 접지를 개선할 수 있을까라는 질문을 던집니다. 이 연구에서 우리는 새로운 오픈 어휘, 제로샷, LLM 에이전트 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. 우리의 직감은 LLM이 CLIP 기반 시각적 접지의 &quot;단어 가방&quot; 약점을 완화하여 LLM 자체에 어려운 언어 분해, 공간 및 상식적 추론 작업을 수행하는 동시에 시각적 접지의 강점을 활용하여 간단한 명사 구를 접지할 수 있다는 것입니다. 섹션 III에서 설명한 LLM-Grounder는 LLM을 핵심으로 사용하여 접지 프로세스를 조율합니다. LLM은 먼저 구성적 자연어 쿼리를 객체 범주, 객체 속성(색상, 모양 및 재료), 랜드마크 및 공간 관계와 같은 의미적 개념으로 구문 분석합니다. 이러한 하위 쿼리는 장면에서 각 개념을 접지하기 위해 CLIP 기반[24] 오픈 어휘 3D 시각적 접지 방법인 OpenScene[1] 또는 LERF[35]에서 지원하는 시각적 접지 도구로 전달됩니다. 시각적 접지 도구는 개념에 대한 장면에서 가장 관련성 있는 후보 영역 주변에 몇 개의 경계 상자를 제안합니다. 이러한 각 후보에 대해 시각적 접지 도구는 객체 볼륨 및 랜드마크까지의 거리와 같은 공간 정보를 계산하여 LLM 에이전트로 다시 제공하여 에이전트가 공간 관계 및 상식의 관점에서 상황을 전체적으로 평가하고 원래 쿼리의 모든 기준과 가장 잘 일치하는 후보를 선택할 수 있도록 합니다. 이 프로세스는 LLM 에이전트가 결론에 도달했다고 결정할 때까지 반복됩니다. 특히, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 훈련이 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리에 대해 제로 샷 일반화할 수 있으며, 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의
--- EXPERIMENT ---
s(섹션 IV)에서 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 비전 언어 접지 기능을 평가합니다. 저희의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 오픈 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 저희의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 결과는 LLM-Grounder가 3D 비전 언어 작업을 위한 효과적인 접근 방식이라는 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • 저희는 LLM을 에이전트로 사용하면 3D 시각적 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있다는 것을 발견했습니다. • 레이블이 지정되지 않은 데이터를 사용하여 제로샷 설정에서 ScanRefer에서 SOTA를 달성합니다.• LLM은 접지 쿼리 텍스트가 더 복잡할 때 더 효과적입니다.II. 관련 연구 자연어를 사용한 3D 시각적 접지.구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다.ScanRefer[12] 및 ReferIt3D[13]와 같은 선구적인 벤치마크는 이 분야를 발전시켰습니다.이러한 벤치마크에서 제안한 것처럼 3D 및 텍스트의 참조 작업에는 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다.성능을 향상시키기 위해 3D 및 언어에 대해 공동으로 훈련된 수많은 방법이 제안되었습니다[2-11].그러나 이러한 방법은 이러한 벤치마크가 구축된 원래 ScanNet[37]에 제시된 특정 개체 클래스로 인해 폐쇄형 어휘 설정으로 제한됩니다. 영어: 2D 오픈 어휘 분할의 발전[38–40]에 의해 동기를 부여받은 연구자들은 3D 오픈 어휘 접지[1,14–21,35,41]를 탐구했습니다.그러나 이러한 방법은 대부분 기본 시각-언어 브리지로 CLIP[24]에 의존합니다.이것은 접지 텍스트 쿼리가 간단한 명사 구(예: &quot;빨간 사과&quot;)인 경우에 효과적입니다.그러나 연구에 따르면 CLIP은 &quot;단어 가방&quot; 동작을 보이며 텍스트나 시각적 객체의 관계, 귀속 및 순서와 같은 구성적 이해가 부족합니다[25].이는 ScanRefer 및 ReferIt3D에서 제시된 과제의 중요한 측면입니다.이러한 측면을 인식하여 의미 추상화[22] 및 3D-CLR[23]은 공간 정보 텍스트 및 3D 데이터를 사용하여 전담 신경 학습을 훈련합니다.테이블 근처에서 나무 의자 찾기.시간 LLM 에이전트테이블 근처에서 나무 의자를 확인하겠습니다.LLM 에이전트대상 #이 원하는 것 같습니다. LLM 에이전트 타겟 파인더 후보자 이름: 의자 속성: 나무 관찰: 사용자는 테이블 근처에 있는 나무 의자를 찾고 있습니다. 추론: &quot;테이블 근처에 있는 나무 의자를 찾아야 합니다. 계획: 1. 나무 의자를 가지고 Target Finder를 호출하고 테이블을 가지고 Landmark Finder를 호출합니다. 2. 랜드마크 테이블과의 근접성을 기준으로 후보자를 비교합니다. 3. 필요한 경우 사용자에게 추가 설명을 요청합니다. 4. 올바른 OpenScene/LERF CandidateLandmark Finder 이름: 테이블 관계: OpenScene/LERF 관찰 근처: Target Finder와 Landmark Finder가 좌표를 반환할 때까지 반복합니다. 또한 각 후보자의 랜드마크와의 거리와 볼륨을 객체화합니다. 추론: 정보를 집계하여 결정해야 합니다. 계획: 1. 랜드마크 테이블과의 근접성을 기준으로 의자 후보자를 비교합니다. 2. 객체 볼륨을 검사하고 너무 작은 객체를 필터링합니다. 3. 각 후보자를 전체적으로 점수 매기고 순위를 매기고 필터링합니다. 4. 접지 결정을 내립니다. 자기 비판: 모든 요소를 고려하여 전체적인 상황에 따라 결정을 내려야 합니다. 객체를 찾을 수 있습니다. 그림 2: LLM-Grounder 개요. 주어진 객체를 접지하기 위한 쿼리, LLM 에이전트의 지원을 받는 우리의 접근 방식은 사용자의 요청에 대해 추론하고 도구를 사용하여 객체를 접지하기 위한 계획을 생성합니다. 에이전트는 대상 찾기 및 랜드마크 찾기와 같은 도구와 상호 작용하여 도구에서 객체 경계 상자, 객체 볼륨 및 랜드마크까지의 거리와 같은 정보를 수집합니다. 그런 다음 이 정보는 에이전트로 반환되어 추가 공간 및 상식적 추론을 수행하여 가장 잘 일치하는 후보를 순위를 매기고 필터링하고 선택합니다. 네트워크를 사용하여 접지하기 전에 텍스트 쿼리의 구성적 의미를 구문 분석하고 접지합니다. 이와 대조적으로 우리의 방법은 훈련 없이(제로샷) 동일한 것을 달성하기 위해 LLM 에이전트를 사용할 가능성을 탐구합니다. NS3D[36]는 LLM 기반 코드 생성을 사용하여 이 문제를 해결하는 프로그램을 생성하는데, 이는 우리의 접근 방식과 더 유사하지만, 또한 기준 진실 객체 분할 및 범주를 사용하여 시각적 접지를 단순화하므로 오픈 어휘 및 제로샷 기능이 부족합니다. LLM 에이전트 대규모 언어 모델(LLM)[26,42-46]의 최근 발전은 놀라운 새로운 능력을 보여주었습니다. 여기서는 LLM을 에이전트로 사용할 수 있도록 하는 몇 가지 능력을 나열합니다. a) 계획: 계획은 복잡한 목표를 하위 목표로 나누고 발행된 조치와 환경 피드백을 기반으로 자기 반성을 하는 것을 포함합니다. Chain-of-thought[27]은 LLM이 복잡한 작업을 더 작은 작업으로 분해하여 &quot;단계적으로 생각&quot;하도록 지시받았을 때 더 나은 계획 능력을 보였다는 것을 보여줍니다. Tree-of-thoughts[28]는 단계당 여러 생각을 탐구하여 사슬을 트리로 바꾸어 이 접근 방식을 확장합니다. [47-50]은 LLM이 출력과 환경 피드백에 대해 자기 반성하도록 지시받았을 때 더 나은 출력을 생성할 수 있음을 보여줍니다. b) 도구 사용: 도구를 사용하는 능력은 인간 지능의 고유한 특징입니다. 현재 LLM이 모든 작업(예: 수학과 사실적 질의응답 문제)에 능숙하지 않다는 것을 인식한 연구자들은 LLM이 작업을 완료하기 위해 도구 사용을 조율하도록 하는 가능성을 탐구했습니다. 도구 사용 문제의 핵심은 어떤 도구를 사용할지, 언제 사용할지 결정하는 것입니다. Socratic Models[29]는 자연어를 매체로 사용하여 LLM 에이전트가 시각 언어 모델 및 오디오 언어 모델과 같은 다른 다중 모드 언어 모델과 함께 안내된 토론에 참여하여 작업을 집단적으로 완료하도록 합니다. MRKL[51] 및 TAML[52]은 LLM에 계산기를 장착하고 수학 문제를 푸는 능력이 향상되었음을 보여줍니다. 이러한 결과를 바탕으로 LangChain[53]과 같은 소프트웨어 라이브러리가 개발되어 개발자를 위해 LLM 도구 사용을 간소화했습니다. ToolFormer[30], HuggingGPT[54] 및 API-Bank[55]는 더 많은 API와 머신 러닝 모델을 LLM이 사용할 도구로 개방하여 도구 사용을 더욱 확대합니다. 로봇공학에서 SayCan[31], InnerMonologue[32], Code as Policies[33] 및 LM-Nav[34]는 LLM의 계획 및 도구 사용 기능을 사용하여 장기적이고 복잡한 작업을 위한 실제 로봇의 고급 컨트롤러 역할을 하도록 합니다. 이러한 작업에서 얻은 성공은 LLM을 에이전트로 사용하여 3D 시각적 접지에서 제시된 구성적 언어-시각 이해 과제를 해결하는 데 동기를 부여합니다.III. 방법 최근 Auto-GPT[56], GPTEngineer[57] 및 ToolFormer[30]의 성공 사례는 LLM을 에이전트로 사용하는 데 있어 초기 성공 징후를 보여줍니다.에이전트는 목표에 의해 주도되고 목표에 대한 추론을 하고 계획을 세우고 도구를 조사하고 사용하며 환경과 상호 작용하고 환경으로부터 피드백을 수집한다는 점에서 기계 학습의 기존 모델과 다릅니다.3D 시각적 접지 설정에서 에이전트는 기존 모델이 보이는 &quot;단어 가방&quot; 동작에 대한 유망한 솔루션이 될 수 있습니다. LLM-Grounder에서는 GPT-4를 에이전트로 사용하여 세 가지 작업을 완료하도록 합니다. 1. 복잡한 텍스트 쿼리를 OpenScene 및 LERF와 같은 CLIP 기반 3D 시각적 그라운더와 같은 다운스트림 도구가 더 잘 처리할 수 있는 하위 작업으로 분해합니다. 2. 이러한 도구를 조율하고 사용하여 제안된 하위 작업을 해결합니다. 3. 환경으로부터 피드백에 대한 추론은 Training Size Open-Vocab Method Visual Grounder + LLM Agent Acc@0.25 ↑ Acc@0.5 ↑ 36k 레이블이 지정된 3D 텍스트 데이터 closed-vocab ScanRefer[12] 3DVG-Trans[2] 34.20.41.28.zero-shot open-vocab LERF[35] Ours LERF LERF 4.0.GPT-6.9 (+2.5) 1.6 (+1.3) zero-shot open-vocab OpenScene[1] Ours Ours OpenScene OpenScene OpenScene 13.5.✓ GPT-3.✓ GPT-14.3 (+1.3) 17.1 (+4.1) 4.7 (-0.4) 5.3 (+0.2) 표 I: ScanRefer에 대한 실험 결과. LLM(GPT-4) 에이전트는 LERF 및 OpenScene과 같은 제로샷 오픈 어휘 3D 그라운더의 3D 그라운딩 기능을 크게 증가시킵니다. 우리는 그라운딩 기능을 Accuracy@0.and @0.5로 측정하는데, 이는 교차-연합(IoU) wrt 그라운드-트루스 박스가 각각 0.25와 0.5를 초과하는 바운딩 박스 예측의 정확도입니다. 괄호 안의 숫자는 LLM 에이전트를 추가한 후의 성능 이득 또는 손실을 나타냅니다. 또한 GPT-3.5와 같은 덜 강력한 LLM은 강력한 그라운딩 기능 이득을 달성할 수 없음을 결과에 보여줍니다. 마지막으로, 제로샷 오픈 어휘인 우리의 방법과 직접 비교할 수는 없지만, 완전성을 위해 ScanRefer와 폐쇄 어휘에서 훈련된 방법에 대한 성능이 나열되어 있습니다. LERF 낮은 시각적 난이도 LLM 없음 LLM 있음 10.15.1 (+4.3) LLM 없음 LLM 있음 2.4.4 (1.9) 높은 시각적 난이도 OpenScene 27.33.6 (+6.0) 8.12.1 (+3.5) 표 II: 시각적 복잡성에 대한 소거 연구. LLM 에이전트는 낮은 시각적 난이도 설정에서 3D 접지에 더 효과적입니다. 표시된 숫자는 Acc@.25입니다. 접지 결정을 내리기 위한 공간 이해와 상식. 계획. LLM의 첫 번째 장점은 계획하는 능력입니다. 연구에 따르면 사고의 사슬 추론[27], 즉 LLM이 복잡한 목표를 더 작은 하위 작업으로 분해하도록 명시적으로 촉구하는 것(&quot;단계적으로 생각하기&quot;)은 산술, 상식 및 기호 추론 작업에 도움이 될 수 있습니다. 이러한 발견에서 영감을 얻어 그림 2에 나와 있는 것과 같이 에이전트를 설계합니다. 구체적으로, 먼저 에이전트에게 관찰 내용을 설명하도록 요청하여 에이전트가 현재 상황을 요약할 수 있는 기회를 제공합니다. 맥락에는 인간 텍스트 쿼리와 도구에서 반환된 정보(아래 설명)가 포함될 수 있습니다. 그런 다음 에이전트는 추론이라는 섹션을 시작하는데, 이는 에이전트가 고수준 계획을 수행하는 데 필요한 정신적 스크래치패드 역할을 합니다. 그런 다음 계획 섹션에서 에이전트는 도구 사용, 비교 또는 계산을 포함하여 고수준 계획을 이행하기 위한 보다 구체적인 단계를 나열해야 합니다. 에이전트는 자기 비판 섹션에서 생성된 계획을 반영하고 최종 수정을 할 수 있습니다[50]. 도구 사용. LLM의 두 번째 장점은 도구를 사용할 수 있는 능력에서 비롯됩니다. LLM 에이전트에게 도구를 사용하여 &quot;단어 가방&quot; 동작을 해결하도록 지시합니다(제2절). 그림 2에서 보듯이, 시각적 접지 및 피드백을 위해 설계한 두 도구의 예상 입력 및 출력 형식, 즉 API를 LLM에 알리고 LLM 에이전트에게 주어진 형식에 따라 상호 작용하도록 요청합니다. 도구에는 대상 파인더와 랜드마크 파인더가 포함됩니다. 대상 파인더와 랜드마크 파인더. 대상 파인더와 랜드마크 파인더는 텍스트 쿼리 입력을 받고 쿼리에 대한 가능한 위치 클러스터의 경계 상자를 찾고 중심과 크기(Cx, Cy, Cz, AX, AY, AZ)의 형태로 후보 경계 상자 목록을 반환합니다. 대상은 사용자가 쿼리에서 참조하는 주요 객체(식탁과 창문 사이의 의자에서 &quot;의자&quot;)입니다. 랜드마크는 대상을 공간적으로 참조하는 데 사용되는 객체(&quot;식탁&quot;과 &quot;창문&quot;)입니다. 대상 파인더는 또한 각 후보의 볼륨을 계산하고 랜드마크 파인더는 또한 각 대상 후보의 중심에서 랜드마크의 중심까지의 유클리드 거리를 계산합니다. 볼륨, 거리 및 경계 상자는 LLM 에이전트가 공간 및 상식적 추론을 수행하기 위한 피드백을 함께 제공합니다. 예를 들어, 볼륨이 0.01m³만큼 작은 후보 &quot;의자&quot;는 아마도 거짓 양성일 것이고 필터링되어야 합니다. 랜드마크와의 거리가 쿼리에서 언급한 공간 관계를 준수하지 않는 후보는 거부되어야 합니다. 타겟 파인더와 랜드마크 파인더는 오픈 어휘 CLIP 기반 3D 시각적 그라운더 LERF [35] 및 OpenScene [1]에 의해 구현됩니다. 이러한 도구만으로는 복잡한 텍스트 쿼리가 주어졌을 때 &quot;단어 가방&quot; 동작(Sec. I)을 보입니다. 그러나 간단한 명사구(&quot;의자&quot;)와 같은 더 간단한 텍스트 쿼리가 주어졌을 때 이러한 도구는 일반적으로 잘 작동할 수 있습니다. LLM 에이전트는 이러한 3D 시각적 그라운더의 명사구 그라운딩 기능을 활용하면서 복잡한 그라운딩 쿼리를 분해하고 한 번에 하나의 객체를 그라운딩하고 나중에 공간 관계에 대해 추론함으로써 언어 이해와 공간 추론의 약점을 보완합니다. 타겟 파인더를 사용하기 위해 LLM 에이전트에게 원래 자연어 쿼리에서 명사 구(예: &quot;나무 의자&quot;)를 구문 분석하도록 지시합니다. 랜드마크 파인더를 사용하기 위해 LLM 에이전트에게 원래 쿼리에서 언급된 랜드마크 객체와 타겟 객체와의 공간 관계를 구문 분석하도록 지시합니다. IV. 실험 실험에서 먼저 LLM 기반 에이전트가 CLIP 기반 3D 시각적 접지 방법과 비교하여 제로샷 오픈 어휘 3D 시각적 접지를 얼마나 잘 개선하는지 평가하고 싶습니다. 그런 다음 폐쇄 어휘 설정에서 방법을 평가하고 폐쇄 어휘 및 훈련된 접근 방식과 비교합니다. 마지막으로 접근 방식의 일반화를 보여주기 위해 야생 장면에서 몇 가지 정성적 예를 보여줍니다. A. 데이터 집합 ScanRefer. ScanRefer[12]는 자연어를 사용하여 실내 3D 장면에서 3D 객체 현지화에 대한 벤치마크입니다. 18개의 11,개 객체에 대한 51,583개의 인간이 쓴 설명으로 구성되어 있습니다. 영어: 800개의 ScanNet[37] 3D 장면의 의미 범주, 여기서 train/val/test 분할에는 각각 36,665개, 9,508개 및 5,410개의 설명이 포함됩니다. 표 I에 제시된 실험에 대한 검증 분할의 첫 번째 장면을 사용하며, 이는 998개의 텍스트-3D-객체 쌍으로 구성됩니다. 또한 ScanRefer의 두 가지 표준 메트릭인 Accuracy@0.25 및 Accuracy@0.5를 보고합니다. 0.25와 0.5는 3D 경계 상자의 IoU에 대한 다른 임계값입니다. B. 기준 방법 ScanRefer. ScanRefer[12]는 종단 간 3D 텍스트 신경망 아키텍처를 사용하여 자연어 입력이 주어졌을 때 객체를 지역화합니다. 구체적으로 3D 포인트 클라우드를 PointNet++[58] 기능으로 처리한 다음 포인트를 클러스터링하고 객체의 경계 상자를 제안합니다. 그런 다음 언어 기능을 클러스터 및 상자와 융합하여 언어에서 참조하는 상자를 결정합니다.파이프라인은 텍스트 및 b-상자 쌍과 장면의 모든 객체에 대한 기준 진실 b-상자 및 의미 클래스를 감독합니다.이 기준선을 현재 훈련된 파이프라인의 성능을 보여주는 것으로 포함시켜 감독이 사용되지 않는 제로샷 설정과 비교한 한계로 사용합니다.3DVG-Transformer.3DVG-Transformer[2]는 ScanRefer의 종단 간 신경 구조를 기반으로 하며 경계 상자를 제안하기 전에 근처 클러스터를 집계하는 새로운 신경 모듈을 제안합니다.ScanRefer와 유사하게 3DVGTransformer도 기준 진실 객체 b상자, 의미 클래스 및 인간이 주석을 단 설명의 감독을 사용합니다.OpenScene 및 LERF.OpenScene[1] 및 LERF[35]는 제로샷 오픈 어휘 3D 장면 이해 접근 방식입니다. OpenScene은 2D CLIP 기능을 3D 포인트 클라우드로 추출하고, 쿼리의 CLIP 텍스트 임베딩과 3D 포인트 클라우드의 모든 포인트 간의 코사인 유사도를 계산하여 텍스트 쿼리로 접지를 허용합니다. LERF는 신경 복사장에 CLIP 임베딩을 인코딩하여 동일한 것을 달성합니다. 이러한 방법은 단독으로 사용될 때 1에서 설명한 대로 &quot;단어 가방&quot; 동작을 보이는데, 이는 LLM 에이전트 심의 추론으로 해결하고자 하는 문제입니다. 3D 시각적 접지 벤치마크 ScanRefer에 OpenScene과 LERF를 사용하여 경계 상자를 생성하기 위해 코사인 유사도가 높은 지점에 DBSCAN 클러스터링[59]을 적용하고 그 주위에 경계 상자를 그립니다. C. 결과 먼저 그림 3에서 LLM-Grounder의 정성적 결과를 보여줍니다. 더 많은 결과와 데모는 프로젝트 웹사이트¹에서 찾을 수 있으며, 여기에는 실제 장면도 포함됩니다. 기준선과 비교했을 때 LLM 에이전트가 제로 샷, 오픈 어휘 접지를 개선할 수 있다는 것을 발견했습니다. 표 I에서 볼 수 있듯이 LLM 에이전트를 추가하면 각각 정확도@0.25에서 5.0%와 17.1%를 달성하여 LERF와 OpenScene의 접지 성능을 크게 높일 수 있습니다. 우리는 LERF의 성능 증가가 낮은 것은 LERF의 전반적인 접지 기능이 약하기 때문입니다. 낮은 증가는 도구가 LLM 에이전트에 너무 노이즈가 많은 피드백을 제공할 때 LLM 에이전트가 노이즈가 많은 입력으로 추론하고 성능을 개선하기 어렵다는 것을 시사합니다. 또한 예측된 b-박스가 접지 진실 박스와 50% 이상 겹치도록 요구하는 Accuracy@0.5에서 성능이 낮게 증가한 것을 알 수 있습니다. 이는 기본 접지기의 인스턴스 분할 기능이 부족하기 때문입니다. 접지기는 종종 올바르게 접지된 객체에 대해 너무 크거나 너무 작은 경계를 예측하는 것을 관찰합니다. 이러한 예측은 LLM에서 수정할 수 없으므로 정확한 시각적 접지가 어렵고 성능이 낮습니다. 또한 OpenScene의 에이전트로 GPT-3.5를 사용할 때 성능이 GPT 없이 사용할 때보다 떨어집니다. 이는 GPT-3.5의 도구 사용 및 공간 및 상식적 추론 기능이 약하기 때문입니다. D. 절제 연구 그런 다음 LLM 에이전트가 주로 무엇을 개선하는지 평가합니다. 두 가지 다른 설정을 테스트합니다. (1) LLM 에이전트가 더 어려운 시각적 맥락에서 더 많은 도움이 됩니까? (2) LLM 에이전트가 더 어려운 텍스트 쿼리에서 더 많은 도움이 됩니까? 시각적 맥락의 난이도. 표 II에서 결과를 시력 난이도에 따라 분류하고 LLM 에이전트가 낮은 시력 난이도 쿼리에 더 효과적이라는 것을 발견했으며, 이는 더 높은 접지 성능 증가로 입증됩니다. 구체적으로 접지 쿼리를 낮은 시각적 난이도와 높은 시각적 난이도 범주로 구분합니다. 텍스트 쿼리에서 언급된 객체가 장면에서 해당 클래스의 유일한 객체(0개의 방해 객체)인 경우 쿼리는 낮은 시각적 난이도입니다. 장면에서 같은 클래스의 방해 객체가 두 개 이상 있는 경우 쿼리는 높은 시각적 난이도입니다. 평가한 998개의 쿼리 중 232개의 쿼리는 낮은 시각적 난이도였고 766개의 쿼리는 높은 시각적 난이도였습니다. 표 II의 결과는 LLM이 낮은 시각적 난이도 쿼리에 대해 더 많은 성능 증가를 가져온다는 것을 보여줍니다. 이러한 동작은 낮음 및 높음 시각적 난이도 설정에서 제시되는 다양한 과제로 설명할 수 있습니다. 낮은 시각적 난이도 설정에서 오픈 어휘 3D 그라운더가 직면하는 주요 과제는 &quot;단어 가방&quot; 동작입니다. 예를 들어, 텍스트 쿼리가 &quot;주방의 싱크대&quot;이고 장면에 싱크대가 하나만 있는 경우 단어 가방 그라운더는 전체 주방을 강조하여 그라운딩 정확도가 낮아집니다. LLM 에이전트는 https://chat-with-nerf.github.io/에서 이 문제를 해결하는 데 특히 능숙합니다. &quot;책상 왼쪽에 밝은 회색 모니터가 있습니다. 책상은 더 작고 휘어져 있습니다.&quot; LLM 에이전트 대상 찾기 대상 찾기를 사용하여 밝은 회색 모니터라는 문구로 대상을 그라운딩합니다. 랜드마크 찾기를 사용하여 창문 옆의 더 작은 곡선 책상이라는 문구로 랜드마크를 그라운딩합니다. 대상과의 관계는 아래에 있습니다. 랜드마크 찾기 LLM 에이전트 모니터는 책상의 왼쪽에 있어야 하므로 중심의 x 값은 랜드마크의 x 값보다 작아야 합니다. 그리고 각 후보에 대한 바운딩 박스와 볼륨을 받았습니다. 지금까지 가장 잘 맞는 후보를 선택했습니다.그림 3: 정성적 예.LLM 에이전트는 공간 추론을 사용하여 올바른 객체 인스턴스를 성공적으로 모호하지 않게 합니다.LLM 사용 - LLM 미사용 10%R^2 OpenScene = 0.8%- R^2 LERF = 0.OpenScene LERF 5% 2% 0% -2% -5%-8%문장의 명사 수그림 4: 성능 델타(LLM 사용 - LLM 미사용) 대 쿼리 텍스트 복잡성.텍스트 쿼리가 더 복잡할수록 LLM이 더 도움이 되지만 복잡도가 더 높으면 크게 도움이 되지 않습니다. Avg Acc@0.25loU R (OpenScene LLM) = -0.35% 30%R (OpenScene noLLM) = -0.25% R (LERF LLM) = -0.*R (LERF noLLM) = -0.20% 15% 10% 5% 0%OpenScene w/ LLM OpenScene w/o LLM LERF w/ LLM LERF w/o LLM문장의 명사 수그림 5: 다양한 모델의 성능 대 쿼리 텍스트 복잡성. 모든 모델은 더 복잡한 문장에서 어려움을 겪지만 LLM 에이전트가 있는 모델은 특히 이러한 더 높은 복잡성에서 더 나은 성과를 보입니다. 대상 객체 &quot;sink&quot;를 구문 분석하고 이 단일 명사만 grounder에 발행하여 bag-ofwords 동작을 우회합니다. 그러나 높은 시각적 난이도 설정의 경우 인스턴스 모호성 해소라는 추가 과제가 있습니다. 장면에 동일한 클래스의 인스턴스가 여러 개 있기 때문에 시각적 grounder는 많은 후보를 LLM 에이전트에 반환합니다. LLM 에이전트는 공간 및 상식적 추론 기능을 사용하여 볼륨과 랜드마크 정보까지의 거리가 있는 일부 인스턴스를 필터링할 수 있지만 보다 복잡한 인스턴스 모호성 해소에는 일반적으로 보다 미묘한 시각적 단서가 필요한데, LLM 에이전트는 시각 장애가 있기 때문에 이러한 특권이 없습니다. 텍스트 쿼리의 난이도. 쿼리가 더 복잡해질수록 LLM 에이전트는 성능을 향상시키지만 어느 정도까지만 가능합니다. 문장에 있는 명사의 수를 세어 쿼리 복잡성을 측정할 수 있습니다. 설명에 명사가 많을수록 특정 객체를 접지하기가 더 어려워집니다. 그림 5에서 LLM 에이전트의 도움이 있든 없든 문장 복잡성이 증가함에 따라 성능이 감소하는 것을 볼 수 있습니다. 그러나 LLM 에이전트를 사용하고 사용하지 않는 것의 성능 차이를 분석한 결과, 쿼리 복잡성에 대한 2차 종속성이 있음을 알 수 있습니다(그림 4). 이는 LLM이 더 높은 복잡성 쿼리가 제공될 때 접지에 대한 이점을 제공하지만, 어떤 임계값에 도달한 후에는 성능 이점이 감소함을 시사합니다. 쿼리 복잡성이 낮을 때 LLM이 없는 모델은 객체를 효과적으로 접지할 수 있으므로 LLM은 최소한의 이점을 제공합니다. 복잡성이 증가함에 따라 기준 모델의 성능이 떨어지고 LLM은 더 큰 이점을 제공합니다. 그러나 참조 표현의 복잡성이 증가함에 따라 LLM의 공간 추론 기능은 noLLM 기준의 성능을 능가하지 못할 수 있습니다. 이러한 더 높은 복잡성 범위에서 이점을 얻으려면 더 강력한 LLM이 필요할 수 있습니다. V.
--- CONCLUSION ---
. 특히, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 어떠한 훈련도 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리로 제로샷 일반화할 수 있습니다. 이는 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의 실험(섹션 IV)에서 우리는 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 시각 언어 접지 기능을 평가합니다. 우리의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 개방형 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 우리의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 연구 결과는 LLM-Grounder가 3D 시각 언어 작업을 위한 효과적인 접근 방식으로서 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다.요약하면, 이 논문의 기여는 다음과 같습니다.• LLM을 에이전트로 사용하면 3D 시각 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있음을 발견했습니다.• 레이블이 지정되지 않은 데이터를 사용하여 제로샷 설정에서 ScanRefer에 대한 SOTA를 달성했습니다.• 접지 쿼리 텍스트가 더 복잡할 때 LLM이 더 효과적임을 발견했습니다.II. 관련 연구 자연어를 사용한 3D 시각 접지. 구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다.ScanRefer[12] 및 ReferIt3D[13]와 같은 선구적인 벤치마크가 이 분야를 발전시켰습니다. 이러한 벤치마크에서 제안된 대로 3D 및 텍스트의 참조 작업은 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다. 성능을 향상시키기 위해 3D 및 언어에 대해 공동으로 훈련된 수많은 방법이 제안되었습니다[2-11]. 그러나 이러한 방법은 이러한 벤치마크가 구축된 원래 ScanNet[37]에 제시된 특정 개체 클래스로 인해 폐쇄형 어휘 설정으로 제한됩니다. 2D 개방형 어휘 분할[38–40]의 발전에 의해 동기를 부여받은 연구자들은 3D 개방형 어휘 접지[1,14–21,35,41]를 탐구했습니다. 그러나 이러한 방법은 대부분 기본 비전-언어 브리지로 CLIP[24]에 의존합니다. 이것은 접지 텍스트 쿼리가 간단한 명사 구(예: &quot;빨간 사과&quot;)일 때 잘 작동합니다. 그러나 연구에 따르면 CLIP은 &quot;단어 가방&quot; 행동을 보이며 텍스트나 시각적 객체의 관계, 귀속, 순서와 같은 구성적 이해가 부족합니다[25]. 이는 ScanRefer와 ReferIt3D에서 제시된 과제의 중요한 측면입니다. 이 측면을 인식하여 의미 추상화[22]와 3D-CLR[23]은 공간 정보 텍스트 및 3D 데이터를 사용하여 전담 신경 학습을 수행합니다. 테이블 근처에서 나무 의자 찾기. 시간 LLM 에이전트 테이블 근처에서 나무 의자를 확인하겠습니다. LLM 에이전트 타겟 #이 원하는 내용이라고 생각합니다. LLM 에이전트 타겟 찾기 후보 이름: 의자 속성: 나무 관찰: 사용자가 테이블 근처에서 나무 의자를 찾고 있습니다. 추론: &quot;테이블 근처에 있는 나무 의자를 찾아야 합니다. 계획: 1. 나무 의자를 가지고 Target Finder를 호출하고 테이블을 가지고 Landmark Finder를 호출합니다. 2. 랜드마크 테이블과의 근접성을 기준으로 후보자를 비교합니다. 3. 필요한 경우 사용자에게 추가 설명을 요청합니다. 4. 올바른 OpenScene/LERF CandidateLandmark Finder 이름: 테이블 관계: OpenScene/LERF 관찰 근처: Target Finder와 Landmark Finder가 좌표를 반환할 때까지 반복합니다. 또한 각 후보자의 랜드마크와의 거리와 볼륨을 객체화합니다. 추론: 정보를 집계하여 결정해야 합니다. 계획: 1. 랜드마크 테이블과의 근접성을 기준으로 의자 후보자를 비교합니다. 2. 객체 볼륨을 검사하고 너무 작은 객체를 필터링합니다. 3. 각 후보자를 전체적으로 점수 매기고 순위를 매기고 필터링합니다. 4. 접지 결정을 내립니다. 자기 비판: 모든 요소를 고려하여 전체적인 상황에 따라 결정을 내려야 합니다. 객체를 찾을 수 있습니다. 그림 2: LLM-Grounder 개요. 주어진 객체를 접지하기 위한 쿼리, LLM 에이전트의 지원을 받는 우리의 접근 방식은 사용자의 요청에 대해 추론하고 도구를 사용하여 객체를 접지하기 위한 계획을 생성합니다. 에이전트는 대상 찾기 및 랜드마크 찾기와 같은 도구와 상호 작용하여 도구에서 객체 경계 상자, 객체 볼륨 및 랜드마크까지의 거리와 같은 정보를 수집합니다. 그런 다음 이 정보는 에이전트로 반환되어 추가 공간 및 상식적 추론을 수행하여 가장 잘 일치하는 후보를 순위를 매기고 필터링하고 선택합니다. 네트워크를 사용하여 접지하기 전에 텍스트 쿼리의 구성적 의미를 구문 분석하고 접지합니다. 이와 대조적으로 우리의 방법은 훈련 없이(제로샷) 동일한 것을 달성하기 위해 LLM 에이전트를 사용할 가능성을 탐구합니다. NS3D[36]는 LLM 기반 코드 생성을 사용하여 이 문제를 해결하는 프로그램을 생성하는데, 이는 우리의 접근 방식과 더 유사하지만, 또한 기준 진실 객체 분할 및 범주를 사용하여 시각적 접지를 단순화하므로 오픈 어휘 및 제로샷 기능이 부족합니다. LLM 에이전트 대규모 언어 모델(LLM)[26,42-46]의 최근 발전은 놀라운 새로운 능력을 보여주었습니다. 여기서는 LLM을 에이전트로 사용할 수 있도록 하는 몇 가지 능력을 나열합니다. a) 계획: 계획은 복잡한 목표를 하위 목표로 나누고 발행된 조치와 환경 피드백을 기반으로 자기 반성을 하는 것을 포함합니다. Chain-of-thought[27]은 LLM이 복잡한 작업을 더 작은 작업으로 분해하여 &quot;단계적으로 생각&quot;하도록 지시받았을 때 더 나은 계획 능력을 보였다는 것을 보여줍니다. Tree-of-thoughts[28]는 단계당 여러 생각을 탐구하여 사슬을 트리로 바꾸어 이 접근 방식을 확장합니다. [47-50]은 LLM이 출력과 환경 피드백에 대해 자기 반성하도록 지시받았을 때 더 나은 출력을 생성할 수 있음을 보여줍니다. b) 도구 사용: 도구를 사용하는 능력은 인간 지능의 고유한 특징입니다. 현재 LLM이 모든 작업(예: 수학과 사실적 질의응답 문제)에 능숙하지 않다는 것을 인식한 연구자들은 LLM이 작업을 완료하기 위해 도구 사용을 조율하도록 하는 가능성을 탐구했습니다. 도구 사용 문제의 핵심은 어떤 도구를 사용할지, 언제 사용할지 결정하는 것입니다. Socratic Models[29]는 자연어를 매체로 사용하여 LLM 에이전트가 시각 언어 모델 및 오디오 언어 모델과 같은 다른 다중 모드 언어 모델과 함께 안내된 토론에 참여하여 작업을 집단적으로 완료하도록 합니다. MRKL[51] 및 TAML[52]은 LLM에 계산기를 장착하고 수학 문제를 푸는 능력이 향상되었음을 보여줍니다. 이러한 결과를 바탕으로 LangChain[53]과 같은 소프트웨어 라이브러리가 개발되어 개발자를 위해 LLM 도구 사용을 간소화했습니다. ToolFormer[30], HuggingGPT[54] 및 API-Bank[55]는 더 많은 API와 머신 러닝 모델을 LLM이 사용할 도구로 개방하여 도구 사용을 더욱 확대합니다. 로봇공학에서 SayCan[31], InnerMonologue[32], Code as Policies[33] 및 LM-Nav[34]는 LLM의 계획 및 도구 사용 기능을 사용하여 장기적이고 복잡한 작업을 위한 실제 로봇의 고급 컨트롤러 역할을 하도록 합니다. 이러한 작업에서 얻은 성공은 LLM을 에이전트로 사용하여 3D 시각적 접지에서 제시된 구성적 언어-시각 이해 과제를 해결하는 데 동기를 부여합니다.III. 방법 최근 Auto-GPT[56], GPTEngineer[57] 및 ToolFormer[30]의 성공 사례는 LLM을 에이전트로 사용하는 데 있어 초기 성공 징후를 보여줍니다.에이전트는 목표에 의해 주도되고 목표에 대한 추론을 하고 계획을 세우고 도구를 조사하고 사용하며 환경과 상호 작용하고 환경으로부터 피드백을 수집한다는 점에서 기계 학습의 기존 모델과 다릅니다.3D 시각적 접지 설정에서 에이전트는 기존 모델이 보이는 &quot;단어 가방&quot; 동작에 대한 유망한 솔루션이 될 수 있습니다. LLM-Grounder에서는 GPT-4를 에이전트로 사용하여 세 가지 작업을 완료하도록 합니다. 1. 복잡한 텍스트 쿼리를 OpenScene 및 LERF와 같은 CLIP 기반 3D 시각적 그라운더와 같은 다운스트림 도구가 더 잘 처리할 수 있는 하위 작업으로 분해합니다. 2. 이러한 도구를 조율하고 사용하여 제안된 하위 작업을 해결합니다. 3. 환경으로부터 피드백에 대한 추론은 Training Size Open-Vocab Method Visual Grounder + LLM Agent Acc@0.25 ↑ Acc@0.5 ↑ 36k 레이블이 지정된 3D 텍스트 데이터 closed-vocab ScanRefer[12] 3DVG-Trans[2] 34.20.41.28.zero-shot open-vocab LERF[35] Ours LERF LERF 4.0.GPT-6.9 (+2.5) 1.6 (+1.3) zero-shot open-vocab OpenScene[1] Ours Ours OpenScene OpenScene OpenScene 13.5.✓ GPT-3.✓ GPT-14.3 (+1.3) 17.1 (+4.1) 4.7 (-0.4) 5.3 (+0.2) 표 I: ScanRefer에 대한 실험 결과. LLM(GPT-4) 에이전트는 LERF 및 OpenScene과 같은 제로샷 오픈 어휘 3D 그라운더의 3D 그라운딩 기능을 크게 증가시킵니다. 우리는 그라운딩 기능을 Accuracy@0.and @0.5로 측정하는데, 이는 교차-연합(IoU) wrt 그라운드-트루스 박스가 각각 0.25와 0.5를 초과하는 바운딩 박스 예측의 정확도입니다. 괄호 안의 숫자는 LLM 에이전트를 추가한 후의 성능 이득 또는 손실을 나타냅니다. 또한 GPT-3.5와 같은 덜 강력한 LLM은 강력한 그라운딩 기능 이득을 달성할 수 없음을 결과에 보여줍니다. 마지막으로, 제로샷 오픈 어휘인 우리의 방법과 직접 비교할 수는 없지만, 완전성을 위해 ScanRefer와 폐쇄 어휘에서 훈련된 방법에 대한 성능이 나열되어 있습니다. LERF 낮은 시각적 난이도 LLM 없음 LLM 있음 10.15.1 (+4.3) LLM 없음 LLM 있음 2.4.4 (1.9) 높은 시각적 난이도 OpenScene 27.33.6 (+6.0) 8.12.1 (+3.5) 표 II: 시각적 복잡성에 대한 소거 연구. LLM 에이전트는 낮은 시각적 난이도 설정에서 3D 접지에 더 효과적입니다. 표시된 숫자는 Acc@.25입니다. 접지 결정을 내리기 위한 공간 이해와 상식. 계획. LLM의 첫 번째 장점은 계획하는 능력입니다. 연구에 따르면 사고의 사슬 추론[27], 즉 LLM이 복잡한 목표를 더 작은 하위 작업으로 분해하도록 명시적으로 촉구하는 것(&quot;단계적으로 생각하기&quot;)은 산술, 상식 및 기호 추론 작업에 도움이 될 수 있습니다. 이러한 발견에서 영감을 얻어 그림 2에 나와 있는 것과 같이 에이전트를 설계합니다. 구체적으로, 먼저 에이전트에게 관찰 내용을 설명하도록 요청하여 에이전트가 현재 상황을 요약할 수 있는 기회를 제공합니다. 맥락에는 인간 텍스트 쿼리와 도구에서 반환된 정보(아래 설명)가 포함될 수 있습니다. 그런 다음 에이전트는 추론이라는 섹션을 시작하는데, 이는 에이전트가 고수준 계획을 수행하는 데 필요한 정신적 스크래치패드 역할을 합니다. 그런 다음 계획 섹션에서 에이전트는 도구 사용, 비교 또는 계산을 포함하여 고수준 계획을 이행하기 위한 보다 구체적인 단계를 나열해야 합니다. 에이전트는 자기 비판 섹션에서 생성된 계획을 반영하고 최종 수정을 할 수 있습니다[50]. 도구 사용. LLM의 두 번째 장점은 도구를 사용할 수 있는 능력에서 비롯됩니다. LLM 에이전트에게 도구를 사용하여 &quot;단어 가방&quot; 동작을 해결하도록 지시합니다(제2절). 그림 2에서 보듯이, 시각적 접지 및 피드백을 위해 설계한 두 도구의 예상 입력 및 출력 형식, 즉 API를 LLM에 알리고 LLM 에이전트에게 주어진 형식에 따라 상호 작용하도록 요청합니다. 도구에는 대상 파인더와 랜드마크 파인더가 포함됩니다. 대상 파인더와 랜드마크 파인더. 대상 파인더와 랜드마크 파인더는 텍스트 쿼리 입력을 받고 쿼리에 대한 가능한 위치 클러스터의 경계 상자를 찾고 중심과 크기(Cx, Cy, Cz, AX, AY, AZ)의 형태로 후보 경계 상자 목록을 반환합니다. 대상은 사용자가 쿼리에서 참조하는 주요 객체(식탁과 창문 사이의 의자에서 &quot;의자&quot;)입니다. 랜드마크는 대상을 공간적으로 참조하는 데 사용되는 객체(&quot;식탁&quot;과 &quot;창문&quot;)입니다. 대상 파인더는 또한 각 후보의 볼륨을 계산하고 랜드마크 파인더는 또한 각 대상 후보의 중심에서 랜드마크의 중심까지의 유클리드 거리를 계산합니다. 볼륨, 거리 및 경계 상자는 LLM 에이전트가 공간 및 상식적 추론을 수행하기 위한 피드백을 함께 제공합니다. 예를 들어, 볼륨이 0.01m³만큼 작은 후보 &quot;의자&quot;는 아마도 거짓 양성일 것이고 필터링되어야 합니다. 랜드마크와의 거리가 쿼리에서 언급한 공간 관계를 준수하지 않는 후보는 거부되어야 합니다. 타겟 파인더와 랜드마크 파인더는 오픈 어휘 CLIP 기반 3D 시각적 그라운더 LERF [35] 및 OpenScene [1]에 의해 구현됩니다. 이러한 도구만으로는 복잡한 텍스트 쿼리가 주어졌을 때 &quot;단어 가방&quot; 동작(Sec. I)을 보입니다. 그러나 간단한 명사구(&quot;의자&quot;)와 같은 더 간단한 텍스트 쿼리가 주어졌을 때 이러한 도구는 일반적으로 잘 작동할 수 있습니다. LLM 에이전트는 이러한 3D 시각적 그라운더의 명사구 그라운딩 기능을 활용하면서 복잡한 그라운딩 쿼리를 분해하고 한 번에 하나의 객체를 그라운딩하고 나중에 공간 관계에 대해 추론함으로써 언어 이해와 공간 추론의 약점을 보완합니다. 타겟 파인더를 사용하기 위해 LLM 에이전트에게 원래 자연어 쿼리에서 명사 구(예: &quot;나무 의자&quot;)를 구문 분석하도록 지시합니다. 랜드마크 파인더를 사용하기 위해 LLM 에이전트에게 원래 쿼리에서 언급된 랜드마크 객체와 타겟 객체와의 공간 관계를 구문 분석하도록 지시합니다. IV. 실험 실험에서 먼저 LLM 기반 에이전트가 CLIP 기반 3D 시각적 접지 방법과 비교하여 제로샷 오픈 어휘 3D 시각적 접지를 얼마나 잘 개선하는지 평가하고 싶습니다. 그런 다음 폐쇄 어휘 설정에서 방법을 평가하고 폐쇄 어휘 및 훈련된 접근 방식과 비교합니다. 마지막으로 접근 방식의 일반화를 보여주기 위해 야생 장면에서 몇 가지 정성적 예를 보여줍니다. A. 데이터 집합 ScanRefer. ScanRefer[12]는 자연어를 사용하여 실내 3D 장면에서 3D 객체 현지화에 대한 벤치마크입니다. 18개의 11,개 객체에 대한 51,583개의 인간이 쓴 설명으로 구성되어 있습니다. 영어: 800개의 ScanNet[37] 3D 장면의 의미 범주, 여기서 train/val/test 분할에는 각각 36,665개, 9,508개 및 5,410개의 설명이 포함됩니다. 표 I에 제시된 실험에 대한 검증 분할의 첫 번째 장면을 사용하며, 이는 998개의 텍스트-3D-객체 쌍으로 구성됩니다. 또한 ScanRefer의 두 가지 표준 메트릭인 Accuracy@0.25 및 Accuracy@0.5를 보고합니다. 0.25와 0.5는 3D 경계 상자의 IoU에 대한 다른 임계값입니다. B. 기준 방법 ScanRefer. ScanRefer[12]는 종단 간 3D 텍스트 신경망 아키텍처를 사용하여 자연어 입력이 주어졌을 때 객체를 지역화합니다. 구체적으로 3D 포인트 클라우드를 PointNet++[58] 기능으로 처리한 다음 포인트를 클러스터링하고 객체의 경계 상자를 제안합니다. 그런 다음 언어 기능을 클러스터 및 상자와 융합하여 언어에서 참조하는 상자를 결정합니다.파이프라인은 텍스트 및 b-상자 쌍과 장면의 모든 객체에 대한 기준 진실 b-상자 및 의미 클래스를 감독합니다.이 기준선을 현재 훈련된 파이프라인의 성능을 보여주는 것으로 포함시켜 감독이 사용되지 않는 제로샷 설정과 비교한 한계로 사용합니다.3DVG-Transformer.3DVG-Transformer[2]는 ScanRefer의 종단 간 신경 구조를 기반으로 하며 경계 상자를 제안하기 전에 근처 클러스터를 집계하는 새로운 신경 모듈을 제안합니다.ScanRefer와 유사하게 3DVGTransformer도 기준 진실 객체 b상자, 의미 클래스 및 인간이 주석을 단 설명의 감독을 사용합니다.OpenScene 및 LERF.OpenScene[1] 및 LERF[35]는 제로샷 오픈 어휘 3D 장면 이해 접근 방식입니다. OpenScene은 2D CLIP 기능을 3D 포인트 클라우드로 추출하고, 쿼리의 CLIP 텍스트 임베딩과 3D 포인트 클라우드의 모든 포인트 간의 코사인 유사도를 계산하여 텍스트 쿼리로 접지를 허용합니다. LERF는 신경 복사장에 CLIP 임베딩을 인코딩하여 동일한 것을 달성합니다. 이러한 방법은 단독으로 사용될 때 1에서 설명한 대로 &quot;단어 가방&quot; 동작을 보이는데, 이는 LLM 에이전트 심의 추론으로 해결하고자 하는 문제입니다. 3D 시각적 접지 벤치마크 ScanRefer에 OpenScene과 LERF를 사용하여 경계 상자를 생성하기 위해 코사인 유사도가 높은 지점에 DBSCAN 클러스터링[59]을 적용하고 그 주위에 경계 상자를 그립니다. C. 결과 먼저 그림 3에서 LLM-Grounder의 정성적 결과를 보여줍니다. 더 많은 결과와 데모는 프로젝트 웹사이트¹에서 찾을 수 있으며, 여기에는 실제 장면도 포함됩니다. 기준선과 비교했을 때 LLM 에이전트가 제로 샷, 오픈 어휘 접지를 개선할 수 있다는 것을 발견했습니다. 표 I에서 볼 수 있듯이 LLM 에이전트를 추가하면 각각 정확도@0.25에서 5.0%와 17.1%를 달성하여 LERF와 OpenScene의 접지 성능을 크게 높일 수 있습니다. 우리는 LERF의 성능 증가가 낮은 것은 LERF의 전반적인 접지 기능이 약하기 때문입니다. 낮은 증가는 도구가 LLM 에이전트에 너무 노이즈가 많은 피드백을 제공할 때 LLM 에이전트가 노이즈가 많은 입력으로 추론하고 성능을 개선하기 어렵다는 것을 시사합니다. 또한 예측된 b-박스가 접지 진실 박스와 50% 이상 겹치도록 요구하는 Accuracy@0.5에서 성능이 낮게 증가한 것을 알 수 있습니다. 이는 기본 접지기의 인스턴스 분할 기능이 부족하기 때문입니다. 접지기는 종종 올바르게 접지된 객체에 대해 너무 크거나 너무 작은 경계를 예측하는 것을 관찰합니다. 이러한 예측은 LLM에서 수정할 수 없으므로 정확한 시각적 접지가 어렵고 성능이 낮습니다. 또한 OpenScene의 에이전트로 GPT-3.5를 사용할 때 성능이 GPT 없이 사용할 때보다 떨어집니다. 이는 GPT-3.5의 도구 사용 및 공간 및 상식적 추론 기능이 약하기 때문입니다. D. 절제 연구 그런 다음 LLM 에이전트가 주로 무엇을 개선하는지 평가합니다. 두 가지 다른 설정을 테스트합니다. (1) LLM 에이전트가 더 어려운 시각적 맥락에서 더 많은 도움이 됩니까? (2) LLM 에이전트가 더 어려운 텍스트 쿼리에서 더 많은 도움이 됩니까? 시각적 맥락의 난이도. 표 II에서 결과를 시력 난이도에 따라 분류하고 LLM 에이전트가 낮은 시력 난이도 쿼리에 더 효과적이라는 것을 발견했으며, 이는 더 높은 접지 성능 증가로 입증됩니다. 구체적으로 접지 쿼리를 낮은 시각적 난이도와 높은 시각적 난이도 범주로 구분합니다. 텍스트 쿼리에서 언급된 객체가 장면에서 해당 클래스의 유일한 객체(0개의 방해 객체)인 경우 쿼리는 낮은 시각적 난이도입니다. 장면에서 같은 클래스의 방해 객체가 두 개 이상 있는 경우 쿼리는 높은 시각적 난이도입니다. 평가한 998개의 쿼리 중 232개의 쿼리는 낮은 시각적 난이도였고 766개의 쿼리는 높은 시각적 난이도였습니다. 표 II의 결과는 LLM이 낮은 시각적 난이도 쿼리에 대해 더 많은 성능 증가를 가져온다는 것을 보여줍니다. 이러한 동작은 낮음 및 높음 시각적 난이도 설정에서 제시되는 다양한 과제로 설명할 수 있습니다. 낮은 시각적 난이도 설정에서 오픈 어휘 3D 그라운더가 직면하는 주요 과제는 &quot;단어 가방&quot; 동작입니다. 예를 들어, 텍스트 쿼리가 &quot;주방의 싱크대&quot;이고 장면에 싱크대가 하나만 있는 경우 단어 가방 그라운더는 전체 주방을 강조하여 그라운딩 정확도가 낮아집니다. LLM 에이전트는 https://chat-with-nerf.github.io/에서 이 문제를 해결하는 데 특히 능숙합니다. &quot;책상 왼쪽에 밝은 회색 모니터가 있습니다. 책상은 더 작고 휘어져 있습니다.&quot; LLM 에이전트 대상 찾기 대상 찾기를 사용하여 밝은 회색 모니터라는 문구로 대상을 그라운딩합니다. 랜드마크 찾기를 사용하여 창문 옆의 더 작은 곡선 책상이라는 문구로 랜드마크를 그라운딩합니다. 대상과의 관계는 아래에 있습니다. 랜드마크 찾기 LLM 에이전트 모니터는 책상의 왼쪽에 있어야 하므로 중심의 x 값은 랜드마크의 x 값보다 작아야 합니다. 그리고 각 후보에 대한 바운딩 박스와 볼륨을 받았습니다. 지금까지 가장 잘 맞는 후보를 선택했습니다.그림 3: 정성적 예.LLM 에이전트는 공간 추론을 사용하여 올바른 객체 인스턴스를 성공적으로 모호하지 않게 합니다.LLM 사용 - LLM 미사용 10%R^2 OpenScene = 0.8%- R^2 LERF = 0.OpenScene LERF 5% 2% 0% -2% -5%-8%문장의 명사 수그림 4: 성능 델타(LLM 사용 - LLM 미사용) 대 쿼리 텍스트 복잡성.텍스트 쿼리가 더 복잡할수록 LLM이 더 도움이 되지만 복잡도가 더 높으면 크게 도움이 되지 않습니다. Avg Acc@0.25loU R (OpenScene LLM) = -0.35% 30%R (OpenScene noLLM) = -0.25% R (LERF LLM) = -0.*R (LERF noLLM) = -0.20% 15% 10% 5% 0%OpenScene w/ LLM OpenScene w/o LLM LERF w/ LLM LERF w/o LLM문장의 명사 수그림 5: 다양한 모델의 성능 대 쿼리 텍스트 복잡성. 모든 모델은 더 복잡한 문장에서 어려움을 겪지만 LLM 에이전트가 있는 모델은 특히 이러한 더 높은 복잡성에서 더 나은 성과를 보입니다. 대상 객체 &quot;sink&quot;를 구문 분석하고 이 단일 명사만 grounder에 발행하여 bag-ofwords 동작을 우회합니다. 그러나 높은 시각적 난이도 설정의 경우 인스턴스 모호성 해소라는 추가 과제가 하나 있습니다. 장면에 동일한 클래스의 인스턴스가 여러 개 있기 때문에 시각적 grounder는 많은 후보를 LLM 에이전트에 반환합니다. LLM 에이전트는 공간 및 상식적 추론 기능을 사용하여 볼륨과 랜드마크 정보까지의 거리가 있는 일부 인스턴스를 필터링할 수 있지만 보다 복잡한 인스턴스 모호성 해소에는 일반적으로 보다 미묘한 시각적 단서가 필요한데, LLM 에이전트는 시각 장애가 있기 때문에 이러한 특권이 없습니다. 텍스트 쿼리의 난이도. 쿼리가 더 복잡해질수록 LLM 에이전트는 성능을 향상시키지만 어느 정도까지만 가능합니다. 문장에 있는 명사의 수를 세어 쿼리 복잡성을 측정할 수 있습니다. 설명에 명사가 많을수록 특정 객체를 접지하기가 더 어려워집니다. 그림 5에서 LLM 에이전트의 도움이 있든 없든 문장 복잡성이 증가함에 따라 성능이 감소하는 것을 볼 수 있습니다. 그러나 LLM 에이전트를 사용하고 사용하지 않는 것의 성능 차이를 분석한 결과, 쿼리 복잡성에 2차 종속성이 있음을 알 수 있습니다(그림 4). 이는 LLM이 더 높은 복잡성의 쿼리가 제시될 때 접지에 이점을 제공하지만, 특정 임계값에 도달한 후에는 성능 이점이 감소함을 시사합니다. 쿼리 복잡성이 낮을 때 LLM이 없는 모델은 객체를 효과적으로 접지할 수 있으므로 LLM은 최소한의 이점을 제공합니다. 복잡성이 증가함에 따라 기준 모델의 성능이 떨어지고 LLM은 더 큰 이점을 제공합니다. 그러나 참조 표현의 복잡성이 증가함에 따라 LLM의 공간 추론 기능은 noLLM 기준의 성능을 능가하지 못할 수 있습니다. 이러한 더 높은 복잡성 범위에서 이점을 얻으려면 더 강력한 LLM이 필요할 수 있습니다. V. 결론 및 제한 사항 우리는 접지 프로세스를 조율하기 위한 중심 에이전트로 대규모 언어 모델(LLM)을 활용하는 3D 시각적 접지를 위한 새로운 접근 방식인 LLM-Grounder를 소개했습니다. 경험적 평가에 따르면 LLM-Grounder는 복잡한 텍스트 쿼리를 처리하는 데 특히 뛰어나 3D 시각적 접지 작업을 위한 견고하고, 제로샷, 오픈 어휘 솔루션을 제공합니다. 그러나 고려해야 할 몇 가지 제한 사항이 있습니다. 비용: GPT 기반 모델을 핵심 추론 에이전트로 활용하는 것은 계산적으로 비용이 많이 들 수 있으며, 이는 리소스가 제한된 환경에서의 배포를 제한할 수 있습니다. 지연: GPT 모델의 고유한 지연으로 인해 추론 프로세스가 느릴 수 있습니다. 이 지연은 빠른 의사 결정이 중요한 실시간 로봇 애플리케이션에 상당한 병목 현상이 될 수 있습니다. 이러한 제한에도 불구하고 LLM-Grounder는 3D 시각적 접지에서 새로운 벤치마크를 설정하고 LLM을 로봇 시스템과 통합하는 미래 연구의 길을 열어줍니다. 참고문헌 [1] S. Peng, K. Genova, CM Jiang, A. Tagliasacchi, M. Pollefeys, T. Funkhouser, &quot;Openscene: 오픈 어휘를 사용한 3차원 장면 이해&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2023. [2] L. Zhao, D. Cai, L. Sheng, D. Xu, &quot;3DVG-Transformer: 포인트 클라우드의 시각적 접지를 위한 관계 모델링&quot;, ICCV, 2021, 2928-2937쪽. [3] J. Roh, K. Desingh, A. Farhadi, D. Fox, &quot;Languagerefer: 3차원 시각적 접지를 위한 공간 언어 모델&quot;, 로봇 학습 컨퍼런스. PMLR, 2022, 1046-1056쪽. [4] D. Cai, L. Zhao, J. Zhang, L. Sheng, D. Xu, “3djcg: 3d 포인트 클라우드에서 공동 고밀도 캡션 및 시각적 접지를 위한 통합 프레임워크,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2022, pp. 16464–16473. [5] J. Chen, W. Luo, R. Song, X. Wei, L. Ma, W. Zhang, “3d 시각적 접지를 위한 포인트-언어 계층적 정렬 학습,&quot; 2022. [6] DZ Chen, Q. Wu, M. Nießner, AX Chang, “D3net: rgb-d 스캔에서 반지도 고밀도 캡션 및 시각적 접지를 위한 화자 청취자 아키텍처,&quot; 2021. [7] Z. Yuan, X. Yan, Y. Liao, R. 영어: Zhang, S. Wang, Z. Li 및 S. Cui, &quot;Instancerefer: 인스턴스 다중 레벨 문맥적 참조를 통한 포인트 클라우드의 시각적 접지를 위한 협력적 전체론적 이해&quot;, IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 2021, pp. 1791-1800. [8] E. Bakr, Y. Alsaedy 및 M. Elhoseiny, &quot;주변을 둘러보고 참조: 3차원 시각적 접지를 위한 2차원 합성 의미론 지식 증류&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 37 146-37 158, 2022. [9] H. Liu, A. Lin, X. Han, L. Yang, Y. Yu, 및 S. Cui, &quot;Refer-it-in-rgbd: rgbd 이미지에서 3D 시각적 접지를 위한 하향식 접근 방식&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021, pp. 6032–6041. [10] A. Jain, N. Gkanatsios, I. Mediratta, 및 K. Fragkiadaki, &quot;이미지 및 포인트 클라우드의 언어 접지를 위한 하향식 감지 변환기&quot;, 유럽 컴퓨터 비전 컨퍼런스.Springer, 2022, pp. 417-433. [11] S. Huang, Y. Chen, J. Jia, 및 L. Wang, &quot;3D 시각적 접지를 위한 다중 시점 변환기&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2022, pp. 15 524-15 533. [12] DZ Chen, AX Chang, 및 M. Nießner, &quot;Scanrefer: 자연어를 사용한 rgb-d 스캔에서 3D 개체 위치 파악&quot;, 제16회 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2020. [13] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, 및 LJ Guibas, &quot;ReferIt3D: 실제 장면에서 세밀한 3D 객체 식별을 위한 신경 청취기&quot;, 제16회 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2020. [14] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, MS Ryoo, A. Stone, 및 D. Kappler, &quot;실제 세계 계획을 위한 오픈 어휘 쿼리 가능 장면 표현&quot;, 2023 IEEE 로봇 및 자동화 국제 컨퍼런스(ICRA). IEEE, 2023, pp. 11 509-11 522. [15] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, &quot;Pla: 언어 기반 개방형 어휘 3D 장면 이해&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023, pp. 7010-7019. [16] SY Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, &quot;목초지의 소: 언어 기반 제로샷 객체 탐색을 위한 기준선 및 벤치마크&quot;, CVPR, 2023. [17] C. Huang, O. Mees, A. Zeng, and W. Burgard, &quot;로봇 탐색을 위한 시각적 언어 맵&quot;, 2023 IEEE 국제 로봇 및 자동화 컨퍼런스 (ICRA). IEEE, 2023, pp. 10608-10615. [18] KM Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, A. Maalouf, S. Li, GS Iyer, S. Saryazdi, NV Keetha 외, &quot;Conceptfusion: 오픈 세트 멀티모달 3D 매핑,&quot; ICRAWorkshop on Pretraining for Robotics(PT4R), 2023. [19] K. Mazur, E. Sucar, AJ Davison, &quot;실시간 오픈 세트 장면 이해를 위한 특징적 사실적 신경 융합,&quot; 2023 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA). IEEE, 2023, pp. 8201-8207. [20] NMM Shafiullah, C. Paxton, L. Pinto, S. Chintala, 및 A. Szlam, &quot;클립 필드: 로봇 메모리를 위한 약한 감독 의미 필드&quot;, ICRA2023 로봇 사전 훈련 워크숍(PT4R), 2023. [21] A. Takmaz, E. Fedele, RW Sumner, M. Pollefeys, F. Tombari, 및 F. Engelmann, &quot;OpenMask3D: 오픈 어휘 3D 인스턴스 분할&quot;, arXiv 사전 인쇄본 arXiv:2306.13631, 2023. [22] H. Ha 및 S. Song, &quot;의미 추상화: 2D 비전 언어 모델에서 오픈 월드 3D 장면 이해&quot;, 제6회 로봇 학습 연례 컨퍼런스, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=1V-rNbXVSaO [23] Y. Hong, C. Lin, Y. Du, Z. Chen, JB Tenenbaum, and C. Gan, &quot;3d concept learning and reasoning from multi-view images,&quot; IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 2023. [24] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &quot;Learning transferable visual models from natural language supervisor,&quot; in International conference on machine learning. PMLR, 2021, pp. 8748-8763. [25] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou, &quot;When 영어: 그리고 왜 시각 언어 모델이 단어 모음처럼 동작하며, 이에 대해 무엇을 할 수 있을까요?&quot; 제11회 국제 학습 표현 컨퍼런스, 2022. [26] OpenAI, &quot;Gpt-4 기술 보고서,&quot; 2023. [27] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, QV Le, D. Zhou et al., &quot;사고의 사슬 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다.&quot; 신경 정보 처리 시스템의 발전, 제11권, 2호, 2022년 11월. 35, pp. 24 824-24 837, 2022. [28] S. Yao, D. Yu, J. Zhao, I. Shafran, TL Griffiths, Y. Cao, and K. Narasimhan, &quot;생각의 나무: 대규모 언어 모델을 통한 의도적인 문제 해결,&quot; arXiv 사전 인쇄본 arXiv:2305.10601, 2023. [29] A. Zeng, M. Attarian, KM Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, MS Ryoo, V. Sindhwani, J. Lee et al., &quot;소크라테스 모델: 언어로 제로샷 다중 모드 추론 구성,&quot; 제11회 학습 표현 국제 컨퍼런스, 2022. [30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, &quot;Toolformer: Language models can teach myself to use tools,&quot; 2023. [31] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., &quot;Do as i can, not as i say: Grounding language in robotic affordances,&quot; arXiv 사전 인쇄본 arXiv:2204.01691, 2022. [32] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., &quot;Inner monologue: Embodied reasoning through planning with language models,&quot; in Conference on Robot 학습. PMLR, 2023, pp. 1769–1782. [33] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, A. Zeng, &quot;정책으로서의 코드: 구체화된 제어를 위한 언어 모델 프로그램&quot;, 2023 IEEE 로봇 및 자동화 국제 컨퍼런스(ICRA). IEEE, 2023, pp. 9493-9500. [34] D. Shah, B. Osinski, B. Ichter, S. Levine, &quot;LM-nav: 언어, 비전 및 동작의 대규모 사전 훈련된 모델을 사용한 로봇 탐색&quot;, 제6회 로봇 학습 연례 컨퍼런스, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=UW5A3SweAH [35] J. Kerr, CM Kim, K. Goldberg, A. Kanazawa, M. Tancik, &quot;Lerf: 언어가 포함된 광도장&quot;, 국제 컴퓨터 비전 컨퍼런스(ICCV), 2023. [36] J. Hsu, J. Mao, J. Wu, &quot;Ns3d: 3d 객체 및 관계의 신경 상징적 근거&quot;, 2023 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), pp. 2614-2623, 2023. [온라인]. 사용 가능: https://api.semanticscholar.org/CorpusID:[37] A. Dai, AX Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nieẞner, &quot;Scannet: Richly-annotated 3d reconstructions of indoor scenes,&quot; Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. [38] B. Li, KQ Weinberger, S. Belongie, V. Koltun, and R. Ranftl, &quot;Language-driven semantic segmentation,&quot; International Conference on Learning Representations, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=RriDjddCLN [39] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin, &quot;이미지 수준 레이블을 사용한 오픈 어휘 이미지 분할 확장&quot;, European Conference on Computer Vision에서. Springer, 2022, 540-557쪽. [40] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, D. Marculescu, &quot;마스크 적응 클립을 사용한 오픈 어휘 의미 분할&quot;, IEEE/CVF Conference on Computer Vision and Pattern Recognition 회의록, 2023, 7061-7070쪽. [41] S. Kobayashi, E. Matsumoto, V. Sitzmann, &quot;특징 필드 증류를 통한 편집을 위한 nerf 분해&quot;, Advances in Neural Information Processing Systems, vol. 35, 2022. [온라인]. 영어: 사용 가능: https://arxiv.org/pdf/2205.15585.pdf [42] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., &quot;언어 모델은 소수 학습자입니다.&quot; 신경 정보 처리 시스템의 발전, 제 4권. 33, pp. 1877-1901, 2020. [43] J. Wei, M. Bosma, V. Zhao, K. Guu, AW Yu, B. Lester, N. Du, AM Dai, QV Le, &quot;Finetuned language models are zero-shot learners,&quot; 국제 학습 표현 컨퍼런스, 2021. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., &quot;인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련하기&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27744, 2022. [45] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and PJ Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구,&quot; The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [46] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., &quot;Llama: 개방적이고 효율적인 기초 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [47] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, &quot;React: 언어 모델에서 추론과 행동의 시너지 효과,&quot; arXiv 사전 인쇄본 arXiv:2210.03629, 2022. [48] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, 및 S. Yao, &quot;Reflexion: 언어적 강화 학습을 갖춘 언어 에이전트,&quot; arXiv 사전 인쇄본 arXiv:2303.11366, 2023. [49] H. Liu, C. Sferrazza, 및 P. Abbeel, &quot;후견의 사슬은 언어 모델을 피드백과 일치시킵니다,&quot; arXiv 사전 인쇄본 arXiv:2302.02676, vol. 3, 2023. [50] E. Jang, &quot;Ilms가 자체 출력을 비판하고 반복할 수 있습니까?&quot; evjang.com, 2023년 3월. [온라인]. 사용 가능: https://evjang.com/2023/03/26/self-reflection.html [51] E. Karpas, O. Abend, Y. Belinkov, B. Lenz, O. Lieber, N. Ratner, Y. Shoham, H. Bata, Y. Levine, K. Leyton-Brown et al., “Mrkl systems: A modules, neuro-symbolic architecture that combine large language models, external knowledge sources and discrete reasoning,&quot; arXiv preprint arXiv:2205.00445, 2022. [52] A. Parisi, Y. Zhao, and N. Fiedel, &quot;Talm: Tool augmented language models,&quot; arXiv preprint arXiv:2205.12255, 2022. [53] H. Chase, “LangChain,” 2022년 10월. [온라인]. 사용 가능: https: //github.com/hwchase17/langchain [54] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, 및 Y. Zhuang, &quot;Hugginggpt: huggingface에서 chatgpt 및 그 친구들을 사용하여 AI 작업 해결,&quot; arXiv 사전 인쇄본 arXiv:2303.17580, 2023. [55] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, 및 Y. Li, &quot;Api-bank: 도구 증강 LLM을 위한 벤치마크,&quot; arXiv 사전 인쇄본 arXiv:2304.08244, 2023. [56] &quot;Auto-gpt,&quot; https://github.com/Significant-Gravitas/Auto-GPT, 2013. [57] &quot;Gpt-engineer,&quot; 한국어: https://github.com/AntonOsika/gpt-engineer, 2013. [58] CR Qi, L. Yi, H. Su, 및 LJ Guibas, “Pointnet++: 측정 공간의 포인트 집합에 대한 심층 계층적 특징 학습,&quot; 신경 정보 처리 시스템의 발전, vol. 30, 2017. [59] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “노이즈가 있는 대규모 공간 데이터베이스에서 클러스터를 발견하기 위한 밀도 기반 알고리즘,&quot; kdd, vol. 96, no. 34, 1996, pp. 226–231.
"
