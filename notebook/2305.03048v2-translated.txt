## 단일 샷으로 세그먼트 애니씽 모델 개인화하기

**요약**

대규모 데이터 사전 훈련으로 구동되는 Segment Anything Model (SAM)은 강력한 프롬프트 가능 프레임워크로 입증되어 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 사람의 프롬프트 없이 특정 시각적 개념에 대해 SAM을 사용자 지정하는 것은 아직 탐구되지 않은 영역입니다(예: 수많은 이미지에서 애완견을 자동으로 분할). 본 논문에서는 PerSAM이라는 SAM을 위한 훈련 없는 개인화 접근 방식을 소개합니다. 단일 이미지와 참조 마스크라는 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양성-음성 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 대상 안내 주의 및 대상 의미론적 프롬프팅이라는 두 가지 제안된 기술을 통해 개인화된 객체 분할을 위해 SAM을 강화합니다. 이러한 방식으로 훈련 없이도 개인적인 용도로 범용 SAM을 효과적으로 사용자 지정할 수 있습니다. 분할 척도의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. 전체 SAM을 고정한 상태에서 다중 척도 마스크를 집계하는 척도 인식 미세 조정을 도입하여 성능 향상을 위해 10초 이내에 2개의 매개변수만 조정합니다. 효능을 입증하기 위해 개인화된 객체 분할 평가를 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다. 또한 PerSAM을 활용하여 개인화된 텍스트-이미지 합성을 위한 DreamBooth를 개선할 것을 제안합니다. 훈련 세트 배경의 방해를 완화하여 입력 텍스트 프롬프트에 대한 더 나은 대상 모양 생성과 더 높은 충실도를 보여줍니다. 코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.


**1. 소개**

비전(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), 그리고 멀티모달(Radford et al., 2021; Jia et al., 2021; Li et al., 2023) 분야의 기반 모델은 대규모 데이터 세트와 계산 리소스의 가용성 덕분에 전례 없는 보급률을 얻었습니다. 이들은 제로샷 시나리오에서 탁월한 일반화 능력을 보여주고 인간 피드백을 통합하는 다양한 상호 작용성을 보여줍니다. 이에 영감을 받아 Segment Anything (Kirillov et al., 2023)은 1,100만 개의 이미지-마스크 데이터를 수집하기 위한 정교한 데이터 엔진을 개발하고, 이후 SAM이라는 세분화 기반 모델을 훈련합니다. 수작업 프롬프트를 입력으로 받아 예상 마스크를 반환하는 새로운 프롬프트 가능 세분화 프레임워크를 정의하여 시각적 맥락에서 모든 객체를 분할할 수 있습니다.

그러나 SAM은 본질적으로 특정 시각적 개념을 분할하는 능력을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 누락된 시계를 찾는다고 상상해 보십시오. 바닐라 SAM을 사용하는 것은 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에 대해 복잡한 컨텍스트 내에서 대상 객체를 정확하게 찾은 다음 적절한 프롬프트로 SAM을 활성화하여 분할해야 합니다. 이를 고려하여 다음과 같이 질문합니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 분할하도록 SAM을 개인화할 수 있습니까?

이를 위해 Segment Anything Model을 위한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 그림 1과 같이, 이 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크라는 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 지정합니다. 구체적으로, 먼저 특징 유사성을 통해 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 얻습니다. 이는 모든 전경 픽셀의 모양을 고려합니다. 신뢰도 점수에 따라 두 점이 양성-음성 위치 사전으로 선택되고, 최종적으로 프롬프트 토큰으로 인코딩되어 분할을 위해 SAM의 디코더에 공급됩니다. 디코더 내에서 대상 객체의 시각적 의미론을 주입하여 두 가지 기술, 즉 대상 안내 주의 및 대상 의미론적 프롬프팅을 통해 SAM의 개인화된 분할 기능을 발휘합니다.

* **대상 안내 주의.** SAM 디코더의 모든 토큰-이미지 교차 주의 레이어를 위치 신뢰도 맵으로 안내합니다. 이는 프롬프트 토큰이 주로 전경 대상 영역에 집중하여 집중적인 특징 집계를 수행하도록 명시적으로 강제합니다.

* **대상 의미론적 프롬프팅.** SAM에 상위 수준 대상 의미론을 명시적으로 제공하기 위해 원본 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 하위 수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다.

위에서 언급한 설계와 계단식 사후 개선을 통해 PerSAM은 다양한 포즈 또는 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히 이 접근 방식은 여러 유사한 객체 중 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오를 잘 처리할 수 있습니다. 그럼에도 불구하고 그림 2와 같이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층적 구조(예: 테디 베어 위의 모자 또는 로봇 장난감의 머리)로 구성된 경우 실패 사례가 발생할 수 있습니다. 이러한 모호성은 로컬 부분과 글로벌 모양 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문에 PerSAM이 출력할 적절한 마스크 척도를 결정하는 데 어려움을 줍니다.

이 문제를 완화하기 위해 미세 조정 변형 접근 방식인 PerSAM-F를 추가로 제안합니다. 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 고정하고 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 설명하면, SAM이 다양한 마스크 척도의 잠재적 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 대해 최상의 척도를 적응적으로 선택하기 위해 각 마스크 척도에 대해 학습 가능한 상대적 가중치를 사용하고 가중 합계를 최종 출력으로 수행합니다. 이러한 효율적인 척도 인식 훈련을 통해 PerSAM-F는 원샷 데이터에 대한 과적합을 방지하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다.

또한 이 접근 방식은 DreamBooth(Ruiz et al., 2022)가 그림 3과 같이 개인화된 텍스트-이미지 생성을 위해 확산 모델을 더 잘 미세 조정하는 데 도움이 될 수 있음을 관찰했습니다. 특정 시각적 개념(예: 애완 고양이 또는 배낭)이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 단어 임베딩 공간에서 이러한 이미지를 식별자 [V]로 변환하는 방법을 학습하지만, 동시에 배경 정보(예: 계단 또는 숲)를 포함할 수 있습니다. 이는 새로 프롬프트된 배경을 재정의하고 대상 모양 생성을 방해합니다. 따라서 훈련 이미지 내에서 대상 객체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하기 위해 PerSAM을 활용할 것을 제안합니다.

본 논문의 기여 사항을 다음과 같이 요약합니다.

* **개인화된 객체 분할.** 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오에 맞게 사용자 지정하는 방법을 처음으로 조사합니다. 이를 위해 두 가지 효율적이고 효과적인 방법과 개인화된 객체 분할 평가를 위한 새로운 분할 데이터 세트인 PerSeg를 소개합니다.


* **PerSAM 및 PerSAM-F.** PerSAM에서는 훈련 없는 세 가지 기술을 제안하여 대상 객체의 상위 수준 의미론으로 SAM을 안내합니다. PerSAM-F에서는 10초 안에 2개의 매개변수로 척도 인식 미세 조정을 설계하여 마스크 모호성 문제를 잘 완화합니다.

* 이 접근 방식은 PerSeg 벤치마크, 원샷 부품 및 의미론적 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 달성합니다. 또한 PerSAM은 DreamBooth를 강화하여 더 나은 개인화된 텍스트-이미지 합성을 가능하게 합니다.

(중략)

**5. 결론**

본 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대해 Segment Anything Model (SAM)을 개인화하는 방법을 제안합니다. 먼저, 훈련 없는 기술을 사용하여 상위 수준 대상 의미론을 SAM에 주입하는 PerSAM을 소개합니다. 이를 바탕으로 척도 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 단 2개의 학습 가능한 매개변수를 사용하여 PerSAM-F는 마스크 척도의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 최고의 성능을 달성합니다. 또한, 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 DreamBooth를 지원하는 접근 방식의 효능을 검증합니다. 본 연구가 SAM의 적용 가능성을 더 넓은 범위의 시나리오로 확장하기를 바랍니다.