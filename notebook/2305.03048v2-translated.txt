arXiv:2305.03048v2 [cs.CV] 2023년 10월 4일 원샷으로 세그먼트 무엇이든 모델을 개인화 Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 상하이 인공지능 연구실 3 중국과학원 자동화 연구소 4페킹대학교 컴퓨터과학대학 CFCS {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk 초록 대용량 데이터 사전 학습을 통해 구동되는 세그먼트 무엇이든 모델(SAM)은 강력한 프롬프트 가능한 프레임워크는 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 인력 프롬프트 없이 특정 시각적 개념에 대한 SAM을 사용자 정의하는 것은 충분히 탐구되지 않았습니다. 예를 들어, 수많은 이미지에서 반려견을 자동으로 세분화합니다. 이 논문에서는 SAM에 대한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 참조 마스크가 있는 단일 이미지인 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양수-음수 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 두 가지 제안된 기술인 대상 유도 주의와 대상 의미 프롬프트를 통해 SAM이 개인화된 객체 세분화를 수행할 수 있도록 합니다. 이런 식으로 훈련 없이도 범용 SAM을 개인용으로 효과적으로 사용자 정의할 수 있습니다. 세분화 규모의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. SAM 전체를 동결하여 다중 스케일 마스크를 집계하기 위한 스케일 인식 미세 조정을 도입합니다.이는 10초 이내에 2개의 매개변수만 조정하여 성능을 개선합니다.효능을 입증하기 위해 개인화된 객체 분할을 평가하기 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다.또한 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 개선하기 위해 PerSAM을 활용할 것을 제안합니다.훈련 세트 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 나은 대상 모양 생성과 입력 텍스트 프롬프트에 대한 더 높은 충실도를 보여줍니다.코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.(1) 사용자가 제공합니다.(2) 원샷 학습 (3) 개인화된 분할 하나의 이미지 하나의 마스크 훈련 없는 미세 조정 PerSAM PerSAM-F ... 다양한 포즈나 장면에서 그림 1: 세그먼트 무엇이든 모델의 개인화. 영어: 우리는 특정 시각적 개념(예: 반려견)에 대해 Segment Anything Model(SAM)(Kirillov et al., 2023)을 사용자 정의합니다.단일 샷 데이터로 두 가지 효율적인 솔루션을 소개합니다.훈련이 필요 없는 PerSAM과 미세 조정 PerSAM-F입니다.* 동등한 기여.† 책임 저자.1 2 매개변수 PerSAM PerSAM-F 사용자 제공 DreamBooth PerSAM 지원 미세 조정 10초 테디베어의 모자 [V] 고양이 사진 &quot;[V] 고양이가 있는 해변&quot; ... 다양한 포즈나 장면 미세 조정 로봇 장난감의 몸체 *** 그림 2: 개인화된 세분화 예.PerSAM(왼쪽)은 유리한 성능으로 모든 맥락에서 개인 사물을 세분화할 수 있으며, PerSAM-F(오른쪽)는 규모 인식 미세 조정을 통해 모호성 문제를 더욱 완화합니다. 1 서론 [V] 백팩 사진 &quot;교실 테이블 위의 [V] 백팩&quot; 그림 3: PerSAM을 이용한 DreamBooth 개선(Ruiz et al., 2022). 훈련 중 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 높은 품질의 개인화된 텍스트-이미지 생성을 달성하는 데 도움이 될 수 있습니다. 시각(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019) 및 다중 모달리티(Radford et al., 2021; Jia et al., 2021; Li et al., 2023)의 기초 모델은 대규모 데이터 세트와 계산 리소스의 가용성에 기인하여 전례 없는 보급을 얻었습니다. 그들은 제로샷 시나리오에서 놀라운 일반화 능력을 보여주고, 인간의 피드백을 통합한 다재다능한 상호 작용을 보여줍니다. 여기에서 영감을 얻은 Segment Anything(Kirillov et al., 2023)은 1,100만 개의 이미지 마스크 데이터를 수집하기 위한 섬세한 데이터 엔진을 개발하고, 이후 SAM이라고 알려진 세분화 기반 모델을 훈련합니다. 그것은 새로운 프롬프트 가능 세분화 프레임워크를 정의합니다. 즉, 수작업 프롬프트를 입력으로 받고 예상 마스크를 반환하여 시각적 맥락에서 모든 객체를 세분화할 수 있습니다. 그러나 SAM은 본질적으로 특정 시각적 개념을 세분화하는 기능을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 사라진 시계를 찾으려고 한다고 상상해보세요. 바닐라 SAM을 사용하면 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에서 복잡한 맥락에서 대상 객체를 정확하게 찾은 다음 세분화를 위한 적절한 프롬프트로 SAM을 활성화해야 합니다. 이를 고려하여 다음과 같은 질문이 생깁니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 세분화하도록 SAM을 개인화할 수 있을까요? 이를 위해 Segment Anything Model을 위한 훈련이 필요 없는 개인화 접근 방식인 PerSAM을 소개합니다.그림 1에서 볼 수 있듯이, 저희의 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크인 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 정의합니다.특히, 저희는 먼저 모든 전경 픽셀의 모양을 고려하는 특징 유사성에 의해 테스트 이미지의 대상 객체에 대한 위치 신뢰도 맵을 얻습니다.신뢰도 점수에 따라 두 지점이 양수-음수 위치 사전으로 선택되고, 이는 최종적으로 프롬프트 토큰으로 인코딩되어 세분화를 위해 SAM의 디코더에 입력됩니다.디코더 내에서 저희는 두 가지 기술을 사용하여 대상 객체의 시각적 의미론을 주입하여 SAM의 개인화된 세분화 능력을 최대한 발휘하도록 제안합니다.• • 대상 유도 주의. 저희는 위치 신뢰도 맵에 따라 SAM의 디코더에서 모든 토큰-이미지 교차 주의 계층을 안내합니다.이는 프롬프트 토큰이 집중적인 특징 집계를 위해 주로 전경 대상 영역에 집중하도록 명시적으로 강제합니다.대상 의미적 프롬프팅. SAM에 고수준 대상 의미론을 명시적으로 제공하기 위해 원래 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 저수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다. 앞서 언급한 디자인과 계단식 사후 정제를 통해 PerSAM은 다양한 포즈나 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히, 우리의 접근 방식은 여러 유사한 객체 중에서 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오에 잘 대처할 수 있습니다. 그럼에도 불구하고 그림 2에서 볼 수 있듯이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층 구조로 구성된 경우(예: 테디베어 위의 모자 또는 로봇 장난감의 머리) 가끔 실패 사례가 있을 수 있습니다. 이러한 모호성은 PerSAM이 출력으로 적절한 마스크 크기를 결정하는 데 어려움을 줍니다. 로컬 부분과 글로벌 모양이 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문입니다. 이 문제를 완화하기 위해, 우리는 PerSAM-F라는 우리 접근 방식의 미세 조정 변형을 추가로 제안합니다. 우리는 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 동결하고, 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 말하면, 우리는 SAM이 다양한 마스크 스케일의 여러 가지 잠재적인 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 가장 적합한 스케일을 적응적으로 선택하기 위해, 우리는 각 마스크 스케일에 대해 학습 가능한 상대적 가중치를 사용하고, 최종 출력으로 가중 합산을 수행합니다. 이러한 효율적인 스케일 인식 훈련을 통해, PerSAM-F는 원샷 데이터에서 과적합을 피하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다. 또한, 우리의 접근 방식이 DreamBooth(Ruiz et al., 2022)가 그림 3에서 보듯이 개인화된 텍스트-이미지 생성을 위한 확산 모델을 보다 잘 미세 조정할 수 있도록 도울 수 있다는 것을 관찰했습니다. 애완 고양이 또는 백팩과 같은 특정 시각적 개념이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 이러한 이미지를 단어 임베딩 공간의 식별자[V]로 변환하는 방법을 학습하지만 동시에 계단이나 숲과 같은 배경 정보를 포함할 수 있습니다. 이렇게 하면 새로 프롬프트된 배경이 무시되고 대상 모양 생성이 방해받습니다. 따라서 우리는 PerSAM을 활용하여 훈련 이미지 내에서 대상 개체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하는 것을 제안합니다. 우리는 논문의 기여를 다음과 같이 요약합니다. • 개인화된 개체 분할. 우리는 먼저 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오로 사용자 지정하는 방법을 조사합니다. 이를 위해 개인화된 객체 분할을 평가하기 위한 새로운 분할 데이터 세트인 PerSeg와 함께 효율적이고 효과적인 두 가지 방법을 소개합니다.• PerSAM 및 PerSAM-F. PerSAM에서 대상 객체의 고수준 의미론에 따라 SAM을 안내하는 세 가지 무훈련 기술을 제안합니다. PerSAM-F에서 마스크 모호성 문제를 크게 완화하기 위해 10초 동안 2개의 매개변수를 사용하여 스케일 인식 미세 조정을 설계합니다.• 우리의 접근 방식은 PerSeg 벤치마크, 원샷 파트 및 의미 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 얻습니다.또한 PerSAM은 더 나은 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 향상시킬 수 있습니다.2 관련 작업 기초 모델. 강력한 일반화 용량을 통해 사전 훈련된 기초 모델을 다양한 다운스트림 시나리오에 맞게 조정하고 유망한 성능을 얻을 수 있습니다. 자연어 처리에서 BERT(Devlin 등, 2018; Lu 등, 2019), GPT 시리즈(Brown 등, 2020; OpenAI, 2023; Radford &amp; Narasimhan, 2018; Radford 등, 2019), LLAMA(Zhang 등, 2023c)는 놀라운 맥락 내 학습 능력을 보여주었으며, 도메인별 프롬프트를 통해 새로운 작업으로 전환될 수 있습니다. 마찬가지로 이미지-텍스트 쌍에 대한 대조 학습을 수행하는 CLIP(Radford 등, 2021)과 ALIGN(Jia 등, 2021)은 제로샷 시각 인식에서 뛰어난 정확도를 보여줍니다. Painter(Wang 등, 2022)는 다운스트림 미세 조정 없이 다양한 비전 작업을 수행하기 위해 네트워크 아키텍처와 맥락 내 프롬프트를 통합하는 비전 모델을 도입합니다. CaFo(Zhang et al., 2023d)는 다양한 기초 모델을 캐스케이드하고 사전 훈련된 지식을 협력하여 견고한 저데이터 이미지 분류를 수행합니다. SAM(Kirillov et al., 2023)은 10억 개의 마스크로 사전 훈련된 이미지 분할을 위한 기초 모델을 제시하고 프롬프트 기반 분할을 수행합니다. 고품질 분할(Ke et al., 2023), 더 빠른 추론 속도(Zhao et al., 2023; Zhang et al., 2023a), 모든 용도 매칭(Liu et al., 2023), 3D 재구성(Cen et al., 2023), 객체 추적(Yang et al., 2023), 의료(Ma &amp; Wang, 2023; Huang et al., 2023) 이미지 처리를 위해 SAM을 확장하는 몇 가지 동시 작업이 있습니다. 다른 관점에서, 우리는 특정 시각적 개념에 대한 세분화 기반 모델, 즉 SAM을 개인화하여 일반인을 단 한 번의 샷으로 전문가로 적응시키는 것을 제안합니다.우리의 방법은 또한 텍스트-이미지 기반 모델, 즉 Stable Diffusion(Rombach et al., 2022) 및 Imagen(Saharia et al., 2022)의 개인화를 지원할 수 있으며, 이는 전경 대상 객체를 배경 교란으로부터 세분화하여 생성 품질을 개선합니다.세그먼테이션의 3가지 대형 모델.컴퓨터 비전의 기본 작업인 세분화(Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)에는 이미지에 대한 픽셀 수준의 이해가 필요합니다. 다양한 분할 관련 작업이 탐색되었는데, 여기에는 각 픽셀을 사전 정의된 클래스 집합으로 분류하는 의미적 분할(Badrinarayanan 등, 2017; Chen 등, 2017; Zheng 등, 2021; Cheng 등, 2022; Xie 등, 2021; Song 등, 2020)이나 개별 객체 인스턴스의 식별에 초점을 맞춘 인스턴스 분할(He 등, 2017; Wang 등, 2020; Tian 등, 2020), 클래스 레이블과 인스턴스 식별을 모두 할당하는 전방위 분할(Kirillov 등, 2019; Li 등, 2019), 그리고 세분화를 위해 인간의 개입을 포함하는 대화형 분할(Hao 등, 2021; Chen 등, 2021) 등이 있습니다. 최근 언어 기반 모델(Zhang et al., 2023c; Brown et al., 2020)에서 영감을 받아 여러 동시 연구에서 이미지 분할을 위한 대규모 비전 모델이 제안되었습니다. 이 모델은 광범위한 마스크 데이터로 사전 학습되었으며 다양한 이미지 분포에 대해 강력한 일반화 기능을 보여줍니다. Segment Anything Model(SAM)(Kirillov et al., 2023)은 모델-인-더-루프 주석이 있는 데이터 엔진을 활용하여 프롬프트 가능 분할 프레임워크를 학습하는데, 이는 제로샷 방식으로 다운스트림 시나리오로 일반화됩니다. Painter(Wang et al., 2022)와 SegGPT(Wang et al., 2023)는 강력한 컨텍스트 내 학습 패러다임을 도입하고 주어진 이미지 마스크 프롬프트에 따라 모든 이미지를 분할할 수 있습니다. SEEM(Zou et al., 2023)은 언어 및 오디오와 같은 다중 모달 참조에 의해 촉발된 일반적인 분할 모델을 추가로 제시하여 다양한 의미 지식을 통합합니다. 이 연구에서는 개인화된 객체 분할이라는 새로운 작업을 도입하고 평가를 위해 새로운 데이터 세트 PerSeg에 주석을 달았습니다. 대규모 분할 모델을 개발하는 대신, 우리의 목표는 사용자가 제공한 객체를 모든 포즈나 장면에서 분할하도록 모델을 개인화하는 것입니다. 개인화된 분할을 위해 SAM을 효율적으로 사용자 지정하는 PerSAM과 PerSAM-F의 두 가지 접근 방식을 제안합니다. 매개변수 효율적인 미세 조정. 다운스트림 작업에서 전체 기초 모델을 직접 조정하는 것은 컴퓨팅 비용이 많이 들고 메모리 집약적일 수 있으며, 리소스가 제한된 애플리케이션에 어려움을 줄 수 있습니다. 이 문제를 해결하기 위해 최근 연구에서는 기초 모델의 가중치를 동결하고 미세 조정을 위한 소규모 모듈을 추가하는 매개변수 효율적 방법(Sung 등, 2022; He 등, 2022; Rebuffi 등, 2017; Qin &amp; Eisner, 2021)을 개발하는 데 중점을 두었습니다. 신속한 조정(Lester 등, 2021; Zhou 등, 2022; Jia 등, 2022; Liu 등, 2021)은 동결된 모델과 함께 학습 가능한 소프트 프롬프트를 사용하여 특정 다운스트림 작업을 수행하여 전체 모델 조정에 비해 규모와 강력한 도메인 전송으로 더 경쟁력 있는 성능을 달성하는 것을 제안합니다. 저랭크 적응(LORA)(Hu 등, 2021; Cuenca &amp; Paul, 2023; Zhang 등, 2023b; Hedegaard 등, 2022)은 학습 가능한 랭크 분해 행렬을 각 사전 학습된 가중치에 동시에 주입하여 다운스트림 작업에 필요한 학습 가능한 매개변수 수를 크게 줄입니다. 어댑터(Houlsby 등, 2019; Pfeiffer 등, 2020; Lin 등, 2020; Chen 등, 2022)는 원래 변환기의 레이어 사이에 삽입되도록 설계되어 특징 변환을 위한 경량 MLP를 도입합니다. 기존 작업과 달리 SAM을 위해 섬세하게 설계된 보다 효율적인 적응 방법, 즉 2개의 매개변수와 10초만 있는 PerSAM-F의 스케일 인식 미세 조정을 채택합니다. 이를 통해 원샷 데이터에서 과적합 문제를 효과적으로 방지하고 우수한 성능으로 세분화 규모의 모호성을 완화합니다.3 방법 3.1절에서 먼저 Segment Anything Model(SAM)(Kirillov et al., 2023)을 간략하게 다시 살펴보고 개인화된 객체 세분화를 위한 작업 정의를 소개합니다.그런 다음 각각 3.2절과 3.3절에서 PerSAM과 PerSAM-F의 방법론을 설명합니다.마지막으로 3.4절에서 DreamBooth(Ruiz et al., 2022)가 더 나은 텍스트-이미지 생성을 지원하도록 접근 방식을 활용합니다.3.1 개인화된 객체 세분화 Segment Anything 다시 살펴보기.SAM은 프롬프트 인코더, 이미지 인코더, 경량 마스크 디코더의 세 가지 구성 요소로 구성되며 각각 Encp, Enc, Decм로 표시합니다. 프롬프트 가능 프레임워크로서, SAM은 이미지 I와 프롬프트 집합 P(점, 상자 또는 거친 마스크)를 입력으로 받습니다. 구체적으로 SAM은 먼저 Enc를 사용하여 입력 이미지 특징을 얻고 Encp를 채택하여 길이가 k인 인간이 제공한 프롬프트를 FI = Enc(I), Tp = Encp(P)와 같이 프롬프트 토큰으로 인코딩합니다.(1) 4 F₁ 코사인 유사도 인코딩 {FT}=1 테스트 이미지 I 대상 로컬 특징 {T}=1 FR MR° FR PerSAM의 디코더 인코딩 대상 가이드 주의 이미지-토큰 교차 주의 대상 의미적 프롬프트 ↑ {Si)=1 로컬 신뢰 맵 토큰-이미지 교차 주의 변조 ↑ 집계 주의 행렬 A 로컬 특징 {T}=1 토큰 자체 주의 전체 신뢰 맵 S 집계 ↑ a Concat( + Repeat( ) 전체 신뢰 맵 S TM Тр × 2 TR 원샷 이미지 IR 원샷 마스크 MR 양의 사전 음의 사전 그림 4: 영어: Positive-negative Location Prior. 모든 로컬 파트의 출현에 따라 새로운 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 계산합니다. 그런 다음 PerSAM에 대한 포인트 프롬프트로 위치 사전을 선택합니다. 저수준 위치 프롬프트 고수준 의미 프롬프트 그림 5: 대상 안내 주의(왼쪽) 및 대상 의미 프롬프트(오른쪽). 대상 의미론을 SAM에 주입하기 위해 교차 주의 계층을 명시적으로 안내하고 고수준 단서를 사용하여 추가 프롬프트를 제안합니다. 여기서 F₁ = Rhxwxc 및 Tp Є Rkxc, h, w는 이미지 피처 맵의 해상도를 나타내고 c는 피처 차원을 나타냅니다. 그 후 인코딩된 이미지와 프롬프트가 주의 기반 피처 상호 작용을 위해 디코더 Decм에 입력됩니다. SAM은 프롬프트 토큰 Tp에 접두사로 여러 학습 가능한 마스크 토큰 Tм을 연결하여 디코더의 입력 토큰을 구성합니다. 이러한 마스크 토큰은 M = Decм(FI, Concat(TM,Tp))로 공식화된 마스크 출력을 생성하는 역할을 하며, 여기서 M은 SAM에서 예측한 최종 분할 마스크를 나타냅니다. (2) 작업 정의. SAM은 프롬프트를 통해 모든 객체에 대해 충분히 일반화되지만 특정 주체 인스턴스를 자동으로 분할하는 기능이 부족합니다. 이를 고려하여 개인화된 객체 분할을 위한 새로운 작업을 정의합니다. 사용자는 단일 참조 이미지와 대상 시각적 개념을 나타내는 마스크만 제공합니다. 제공된 마스크는 정확한 분할이거나 즉석에서 그린 대략적인 스케치일 수 있습니다. 우리의 목표는 추가적인 인간의 프롬프트 없이 새 이미지나 비디오 내에서 지정된 객체를 분할하도록 SAM을 사용자 지정하는 것입니다. 평가를 위해 PerSeg라는 개인화된 분할을 위한 새로운 데이터 세트에 주석을 달았습니다. 원시 이미지는 다양한 포즈나 장면에서 다양한 범주의 시각적 개념을 포함하는 주체 중심 확산 모델(Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022)에 대한 작업에서 수집되었습니다. 이 논문에서 우리는 이 과제를 위한 두 가지 효율적인 솔루션을 제안하며, 구체적으로 다음과 같이 설명합니다. 3.2 학습 없는 PERSAM 위치 신뢰도 맵. 사용자가 제공한 이미지 IR과 마스크 MR을 조건으로, PerSAM은 먼저 새로운 테스트 이미지 I에서 대상 객체의 위치를 나타내는 신뢰도 맵을 얻습니다. 그림 4에서 볼 수 있듯이, 우리는 이미지 인코더를 적용하여 IR과 I의 시각적 특징을 추출합니다. 인코더는 SAM의 동결 백본 또는 다른 사전 학습된 비전 모델이 될 수 있으며, 우리는 기본적으로 SAM의 이미지 인코더 Enc를 채택합니다. 우리는 프로세스를 FI = Enc(I), FR = EncI(IR), (3)으로 공식화합니다. 여기서 F1, FR Є R¹xwxc입니다. 그런 다음 참조 마스크 MR Є Rhxwx¹를 사용하여 FR의 시각적 개념 내에서 전경 픽셀의 특징을 잘라내어 {T}}=1 = MR ○ Fr, (4) 5와 같이 n개의 로컬 특징 세트를 생성합니다. 여기서 T½ Є R¹×ª이고 ○는 공간적 곱셈을 나타냅니다. 그런 다음 T½와 테스트 이미지 특징 FĮ 사이의 코사인 유사도에 의해 각 전경 픽셀 i에 대한 n개의 신뢰도 맵을 계산합니다. {S² } }±₁ = {F₁TT) 11, 여기서 S₁ = Rhxw. (5) FI와 T½는 픽셀별로 L2-정규화되었습니다. 각 S²는 테스트 이미지에서 개체의 다른 로컬 부분(예: 개의 머리, 몸통 또는 발)에 대한 분포 확률을 나타냅니다. 이에 더하여, 우리는 모든 n개의 로컬 맵을 집계하여 S = n n SiЄRhxw (6)와 같이 대상 객체의 전반적인 신뢰도 맵을 얻기 위해 평균 풀링을 채택합니다.모든 전경 픽셀의 신뢰도를 통합함으로써, S는 다른 객체 부분의 시각적 모양을 고려하고 비교적 포괄적인 위치 추정을 얻을 수 있습니다.양의-음의 위치 사전.PerSAM에 테스트 이미지의 위치 사전을 제공하기 위해, 우리는 S에서 가장 높고 가장 낮은 신뢰도 값을 갖는 두 지점을 선택합니다.각각 Pɲ와 Pɩ로 표시합니다.전자는 대상 객체의 가장 가능성 있는 중심 위치를 나타내고, 후자는 반대로 배경을 나타냅니다.그런 다음, 이들은 양의 및 음의 지점 프롬프트로 간주되어 Tp = Encp(Ph, P₁) Є R2×c로 프롬프트 인코더에 입력됩니다.(7) 이는 SAM 디코더의 프롬프트 토큰을 나타냅니다. 이런 식으로 SAM은 이미지에서 음의 점을 버리는 반면, 양의 점을 둘러싼 연속적인 영역을 분할하려는 경향이 있습니다.대상 유도 주의.양의-음의 점 프롬프트가 얻어졌지만, 우리는 SAM 디코더에서 교차 주의 연산에 대한 보다 명확한 의미적 지침을 제안합니다.이는 전경 대상 영역 내에서 피처 집계를 집중시킵니다.그림 5에서 볼 수 있듯이, 방정식 6의 전체 신뢰도 맵 S는 테스트 이미지에서 대상 시각적 개념의 거친 영역을 명확하게 나타낼 수 있습니다(더 밝은 색상은 더 높은 점수를 나타냄).이러한 속성을 기반으로, 우리는 S를 사용하여 디코더의 모든 토큰-이미지 교차 주의 계층에서 주의 맵을 안내합니다.특히, 소프트맥스 함수 뒤의 모든 주의 맵을 A = Rhxw로 표시한 다음, 주의 분포를 A9 = softmax A+ a softmax(S) • (8)로 변조합니다.여기서 a는 밸런싱 팩터를 나타냅니다. 주의 편향으로 인해 마스크와 프롬프트 토큰은 중요하지 않은 배경 영역이 아닌 대상 주제와 관련된 더 많은 시각적 의미를 포착해야 합니다. 이는 주의 메커니즘에서 더 효과적인 기능 집계에 기여하고, 훈련 없이 PerSAM의 최종 세분화 정확도를 향상시킵니다. 대상 의미적 프롬프트. 바닐라 SAM은 점이나 상자의 좌표와 같은 저수준 위치 정보가 있는 프롬프트만 수신합니다. SAM의 디코더에 더 높은 수준의 단서를 제공하기 위해 대상 개념의 시각적 특징을 추가 높은 수준의 의미적 프롬프트로 활용하는 것을 제안합니다. 먼저 참조 이미지에서 객체의 글로벌 임베딩 TR을 다음과 같이 다른 로컬 피처 간의 I 평균 풀링을 통해 얻습니다.n TR = ΣΤΑ €R1xc n i=1 그런 다음 방정식 2에서 테스트 이미지의 모든 입력 토큰에 TR을 요소별로 추가한 다음 디코더 블록에 공급합니다.그림 5에 T9 = Repeat (TR) + Concat(TM,Tp), (10)으로 표시됩니다.여기서 T9는 디코더 Decм에 대한 대상 의미론에 의해 안내되는 입력 토큰을 나타내고 Repeat 작업은 대상 시각적 임베딩을 복제합니다.간단한 토큰 통합의 도움으로 PerSAM은 저수준 위치 지점뿐만 아니라 고수준 대상 시각적 단서에 의해서도 촉발됩니다. 6 테스트 이미지 PerSAM 출력 3개 M1 M2 + M3 스케일 임의 노이즈 →&gt; DreamBooth →&gt; ↑ &quot;a [V] cat&quot; 재구성 손실 W1 F W2 1- W1 + W₂ ) 1 사용자가 출력 마스크 M 제공 미세 조정 가중 합산 동결 PerSAM 배경 교란 분리 그림 6: PerSAM-F의 스케일 인식 미세 조정. 스케일 모호성을 완화하기 위해 PerSAM-F는 3개 스케일 마스크를 적응적으로 집계하기 위한 두 개의 학습 가능한 가중치를 채택합니다. 그림 7: PerSAM 지원 DreamBooth. DreamBooth 생성을 개선하기 위해 PerSAM을 사용하여 대상 객체를 배경에서 분리합니다. 계단식 사후 미세 조정. 위의 기술을 통해 SAM 디코더에서 테스트 이미지에 대한 초기 분할 마스크를 얻지만 거친 모서리와 고립된 배경 노이즈가 포함될 수 있습니다. 추가 세분화를 위해, 우리는 두 단계 후처리를 위해 마스크를 디코더 Decм에 반복적으로 공급합니다.첫 번째 단계에서 우리는 이전의 양수-음수 포인트 프롬프트와 함께 현재 예측된 마스크로 디코더에 프롬프트를 보냅니다.두 번째 단계에서 우리는 첫 번째 단계에서 마스크를 둘러싼 경계 상자를 획득하고, 더 정확한 객체 위치 파악을 위해 이 상자로 디코더에 추가로 프롬프트를 보냅니다.우리는 대규모 이미지 인코더 없이 가벼운 디코더만 반복하기 때문에 후처리가 효율적이고 단지 2%의 추가 지연 시간이 발생합니다.3.3 PERSAM-F 세분화 스케일의 모호성의 미세 조정.훈련이 필요 없는 PerSAM은 대부분의 경우를 만족스러운 세분화 정확도로 처리할 수 있습니다.그러나 일부 대상 객체에는 계층적 구조가 포함되어 있어 마스크 스케일의 모호성이 발생합니다.그림 6에서 볼 수 있듯이 플랫폼 위의 찻주전자는 뚜껑과 몸체의 두 부분으로 구성되어 있습니다. 양의 점 프롬프트(녹색 오각별 표시)가 몸체에 위치하고, 음의 프롬프트(빨간색 오각별 표시)가 비슷한 색상의 플랫폼을 제외하지 않으면 PerSAM은 분할에 대해 오도될 것입니다. 이러한 문제는 SAM(Kirillov et al., 2023)에서도 논의되며, 여기서는 객체의 전체, 부분 및 하위 부분에 해당하는 세 가지 크기의 여러 마스크를 동시에 생성하는 대안을 제안합니다. 그런 다음 사용자는 세 가지 마스크 중 하나를 수동으로 선택해야 하는데, 이는 효과적이지만 추가 인력을 소모합니다. 반면에 개인화된 작업은 인간의 프롬프트가 필요 없이 자동 객체 분할을 위해 SAM을 사용자 지정하는 것을 목표로 합니다. 이는 매개변수 효율적인 미세 조정을 통해 PerSAM의 크기 인식 버전을 추가로 개발하도록 동기를 부여합니다. 크기 인식 미세 조정. 적절한 크기에서 적응형 분할을 위해 미세 조정 변형인 PerSAM-F를 도입합니다. 학습이 필요 없는 모델이 하나의 마스크만 생성하는 것과 달리, PerSAM-F는 먼저 PerSAM을 따라 사전 위치를 구하고, SAM의 원래 솔루션을 참조하여 각각 M1, M2, M3으로 표시되는 3개 스케일 마스크를 출력합니다. 여기에 두 개의 학습 가능한 마스크 가중치 w₁, W2를 채택하고 가중치 합산을 통해 최종 마스크 출력을 다음과 같이 계산합니다. M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) 여기서 w1, W2는 모두 1/3으로 초기화됩니다. 최적의 가중치를 학습하기 위해 참조 이미지에서 원샷 미세 조정을 수행하고 주어진 마스크를 기준 진실로 간주합니다. 사전 학습된 지식을 보존하기 위해 전체 SAM 모델을 동결하고 단일 A100 GPU에서 10초 이내에 W1, W2의 두 매개변수만 미세 조정합니다. 이런 방식으로, 우리의 PerSAM-F는 객체의 스케일 인식 의미를 효율적으로 학습하고, 다양한 개념에 대한 최상의 분할 스케일을 적응적으로 출력하여 PerSAM의 일반화 용량을 개선합니다.7 표 1: PerSeg 데이터 세트의 개인화된 객체 분할. 우리는 다양한 방법(Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023)에 대한 전반적인 mIoU, blou 및 학습 가능한 매개변수와 PerSeg의 10개 객체에 대한 mIoU를 비교합니다. ***는 우리와 동시에 진행 중인 작업을 나타냅니다. 방법 mIoU bloU 매개변수. 캔 헛간 시계 고양이 뒤- 테디 오리 얇은 빨간색 팩 곰 장난감 새 만화 로봇 장난감 화가 VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 354M 93.0 33.3 20.9 98.2 65.0 59.2 76.6 66.7 79.8 89.9 67.4 81.0 72.4 72.4 91.1 94.1 95.2 98.0 71.3 97.0 95.8 96.6 63.8 92.6 94.1 94.4 93.7 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 표 2: DAVIS 2017 val(Pont-Tuset et al., 2017)에서의 비디오 객체 분할. 회색은 도메인 내 학습을 포함하는 방법을 나타냅니다. 표 3: FSS-1000(Li et al., 2020), LVIS-92(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 의미론 및 파트 분할. 우리는 mIoU 점수를 보고하고 회색을 사용하여 도메인 내 학습을 포함하는 방법을 나타냅니다. 원샷 의미론 분할 FSS-1000 LVIS-92² 원샷 파트 분할 PASCAL-Part Painter SEEM SegGPT PerSAM 방법 J&amp;F І AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F 방법 PACO-Part HSNet 86.5 17.4 32.4 22.6 부가세 90.3 18.5 33.6 23.5 페인터 61.7 10.5 30.4 14.1 SegGPT 85.6 18.6 66.9 71.3 75.1 PerSAM 81.6 15.6 32.5 22.5 PerSAM-F 76.1 74.9 79.7 PerSAM-F 86.3 18.4 32.9 22.7 3.4 PERSAM 지원 DREAMBOOTH 개인화된 텍스트-이미지 합성을 위해 DreamBooth(Ruiz et al., 2022)는 특정 개체 ig, 애완 고양이의 주어진 3~5개 사진을 사용하여 사전 훈련된 확산 모델을 미세 조정합니다. 텍스트 프롬프트에서 언급된 고양이인 &quot;[V] 고양이&quot;를 생성하는 방법을 학습하고 재구성된 전체 이미지에 대한 손실을 계산합니다. 이를 통해 훈련 이미지의 중복된 배경 정보를 식별자 [V]에 주입합니다. 따라서 그림 7과 같이 DreamBooth에서 배경의 교란을 완화하기 위한 전략을 소개합니다. 몇 장의 이미지에 대한 개체 마스크가 주어지면 PerSAM을 활용하여 모든 전경 대상을 분할하고 배경 영역에 속하는 픽셀에 대한 그래디언트 역전파를 버립니다. 그런 다음, 안정적 확산은 대상 객체의 시각적 I 모양만 기억하도록 미세 조정됩니다. 배경에 감독이 부과되지 않으므로 PerSAM 지원 DreamBooth는 더 나은 시각적 대응으로 대상 객체를 합성할 수 있을 뿐만 아니라 입력 텍스트 프롬프트에 따라 새로운 배경의 다양성을 높일 수도 있습니다.4 실험 먼저 섹션 4.1에서 PerSeg에서 개인화된 세분화를 위한 접근 방식을 평가하고, 섹션 4.2에서 다양한 기존 원샷 세분화 벤치마크를 평가합니다.그런 다음 섹션 4.3에서 PerSAM 지원 DreamBooth의 효과를 설명합니다.마지막으로 섹션 4.4에서 PerSeg에 대한 설계를 조사하기 위해 여러 가지 절제 연구를 수행합니다.4.1 개인화된 평가 PerSeg 데이터 세트.개인화 용량을 테스트하기 위해 PerSeg라는 새로운 세분화 데이터 세트를 구성합니다. 원시 이미지는 주제 중심 확산 작업(Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022)의 훈련 데이터에서 수집되었습니다.PerSeg에는 일용품, 동물, 건물을 포함하여 총 40개의 다양한 범주의 객체가 포함되어 있습니다.다른 포즈나 장면에서 각 객체는 5~7개의 이미지와 마스크와 연관되며, 여기서 하나의 이미지-마스크 쌍을 사용자가 제공한 원샷 데이터로 고정합니다.평가에는 mIoU와 bloU(Cheng et al., 2021)가 채택되었습니다.PerSeg의 구현 세부 사항과 확대된 데이터 스케일은 부록을 참조하세요.8 시계의 반지 2 쟁반 위의 찻주전자 여자가 들고 있는 백팩 캔의 윗부분 그림 8: PerSAM-F의 개선 사항 시각화.우리의 스케일 인식 미세 조정은 PerSAM의 스케일 모호성을 잘 완화할 수 있습니다. 그림 9: 비디오 객체 분할의 시각화. 우리의 접근 방식은 비디오에서 여러 객체를 분할하는 데 좋은 성과를 보입니다. 성능. 표 1에서 미세 조정된 PerSAM-F가 가장 좋은 결과를 얻는 것을 볼 수 있는데, 이는 PerSAM을 전체 mIoU와 bIoU에서 +2.7%, +5.9% 효과적으로 향상시킵니다. 그림 8에서 PerSAM-F의 개선 사항을 더 자세히 시각화합니다. Visual Prompting(VP)(Bar et al., 2022), Painter(Wang et al., 2022), SEEM(Zou et al., 2023), SegGPT(Wang et al., 2023)는 주어진 원샷 프롬프트 데이터에 따라 객체를 분할할 수 있는 컨텍스트 내 학습기입니다. 표시된 대로, 훈련이 필요 없는 PerSAM은 다른 마진으로 Painter, VP, SEEM보다 더 나은 성능을 이미 달성할 수 있습니다. 효율적인 2-매개변수 미세 조정을 통해, 저희의 PerSAM-F는 강력한 SegGPT를 +2.4%, 전반적인 mIoU와 bIoU에서 +4.1% 더 능가합니다.세그먼테이션 일반론자를 개발하려는 그들의 동기와 달리, 저희의 방법은 개인화된 객체 세분화를 위해 특별히 설계되었으며, 시간과 계산 리소스 측면에서 훨씬 더 높은 효율성을 보여줍니다.4.2 기존 세분화 벤치마크 비디오 객체 세분화.첫 번째 프레임 이미지와 객체 마스크를 고려할 때, 저희의 PerSAM과 PerSAM-F는 DAVIS 2017의 검증 세트에서 경쟁력 있는 객체 세분화 및 추적 성능을 달성합니다(Pont-Tuset et al., 2017) 표 2에서 볼 수 있듯이, 비디오 학습이 없는 방법과 비교했을 때, 학습이 없는 PerSAM은 Painter를 +32.3% J&amp;F 점수로 크게 능가하며, 저희의 PerSAM-F는 SegGPT보다 +0.5% 더 나은 성능을 달성할 수 있습니다. 특히, 우리의 원샷 미세 조정 접근 방식은 광범위한 비디오 데이터로 완전히 훈련된 방법(Lin et al., 2019; Liang et al., 2020)보다 성능이 우수할 수 있습니다. 결과는 그림 9에서 시각화된 것처럼 여러 유사하거나 가려진 객체를 포함하는 시간적 비디오 데이터와 복잡한 시나리오에 대한 우리의 강력한 일반화 능력을 충분히 보여줍니다. 원샷 의미론 및 부분 분할. 표 3에서 우리는 각각 4개의 데이터 세트인 FSS-1000(Li et al., 2020), LVIS-92²(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 이미지 분할에 대한 접근 방식을 평가합니다. 여기서 우리는 데이터 전처리 및 평가를 위해 Matcher(Liu et al., 2023)를 따릅니다. 표시된 대로, 우리의 PerSAM-F는 Painter보다 지속적으로 더 나은 결과를 얻었으며 SegGPT와 비슷한 성능을 보였습니다. 도메인 내 학습이 있는 모델(Min et al., 2021; Hong et al., 2022)의 경우, 우리의 접근 방식은 HSNet보다 더 높은 점수를 얻을 수 있습니다. 실험은 우리가 제안하는 접근 방식이 객체 수준 분할에 국한되지 않고 SAM의 범주별 및 부분별 개인화에도 작동한다는 것을 잘 보여줍니다. 4.3 PERSAM 지원 DREAMBOOTH 우리는 DreamBooth(Ruiz et al., 2022)의 모든 하이퍼파라미터를 따라 개인화된 이미지 합성을 위해 사전 학습된 Stable Diffusion(Rombach et al., 2022)을 미세 조정합니다. 그림 3 외에도 그림 10에서 PerSAM 지원 DreamBooth의 더 많은 예를 시각화합니다. 회색 소파에 누워 있는 개의 경우 DreamBooth의 &quot;정글&quot;과 &quot;눈&quot;은 여전히 녹색과 흰색 장식이 있는 소파입니다. PerSAM-F의 도움을 받아 새로 생성된 배경은 소파와 완전히 분리되어 텍스트 프롬프트와 잘 일치합니다. 산 앞에 있는 헛간의 경우, 우리의 접근 방식은 또한 배경 교란을 완화하여 &quot;숲&quot;과 &quot;푸른 하늘&quot;을 올바르게 생성합니다.9 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 개 사진 &quot;정글 속의 [V]개&quot; ... 헛간 사진 &quot;배경에 숲이 있는 [V]개&quot; &quot;눈 속의 [V]개&quot; &quot;배경에 푸른 하늘이 있는 [V]개&quot; 그림 10: PerSAM 기반 DreamBooth의 시각화.개선된 DreamBooth(Ruiz et al., 2022)는 새로운 이미지에서 다양한 맥락을 합성하기 위해 다양성을 더 잘 보존할 수 있습니다.표 4: 제안하는 방법에서 주요 구성 요소의 소거.변형 표 5: 다양한 미세 조정 방법의 소거.표 6: 참조로 상자 이미지를 사용하는 소거.mIoU 이득 69.1 방법 PerSAM 매개변수. mIoU 방법 마스크 상자 0 89.32 페인터 56.4 42.0 + 사후 세부화 72.5 +3.4 83.9 +11.4 프롬프트 튜닝 12K 76.5 VP 65.9 38.1 어댑터 196K 78.3 SEEM 87.1 64.9 + 가이드 어텐션 + 의미적 프롬프트 85.8 +1.9 LORA 293K 90.0 SegGPT 94.3 36.0 89.3 +3.5 3 마스크 가중치 3 92.9 + 스케일 튜닝 95.3 +6.0 PerSAM-F 2 95.3 PerSAM 89.3 PerSAM-F 95.3 88.1 94.9 양의 사전 + 음의 사전 4.4 절제 연구 주요 구성 요소. 표 4에서 우리는 긍정적인 위치 사전만을 채택하는 기준선에서 시작하여 다양한 구성 요소를 조사합니다.그런 다음 부정적인 지점 프롬프트와 계단식 사후 세분화를 추가하여 각각 +3.6%와 +11.4%의 mIoU를 향상시킵니다.그 위에 우리는 주의 유도와 의미 프롬프트를 위해 SAM의 디코더에 고수준 대상 의미론을 도입합니다.그 결과 +1.9%와 +3.5%의 개선은 그 중요성을 충분히 나타냅니다.마지막으로 효율적인 스케일 인식 미세 조정을 통해 PerSAM-F는 점수를 +6.0% 높여 뛰어난 정확도를 보여줍니다.다른 미세 조정 방법.표 5에서 우리는 PerSAM-F에 대한 다른 매개변수 효율적 미세 조정(PEFT) 방법, 즉 프롬프트 튜닝(Liu et al., 2021), 어댑터(Houlsby et al., 2019), LORA(Hu et al., 2021)를 실험합니다. 우리는 SAM 전체를 동결하고 PerSAM 디코더의 모든 변압기 블록에 주입된 PEFT 모듈만 튜닝합니다.보여진 바와 같이, 프롬프트 튜닝과 어댑터는 원샷 데이터를 과대적합시키고 정확도를 심각하게 떨어뜨립니다.대신, 우리의 스케일 인식 미세 튜닝은 가장 학습하기 어려운 매개변수를 튜닝하는 동안 PerSAM의 성능을 가장 잘 향상시킬 수 있습니다.참조로 상자 이미지 사용.원샷 데이터로 정확한 마스크를 요구하는 것은 일부 사용자에게는 너무 엄격할 수 있습니다.표 6에서 예상 객체를 지정하는 경계 상자에 대한 입력 제한을 완화합니다.우리 방법의 경우 상자를 프롬프트로 간주하고 기성품 SAM을 사용하여 원샷 마스크를 생성할 수 있습니다.따라서 상자 참조는 PerSAM 및 PerSAM-F에서 약간의 성능 저하로 이어질 뿐이지만 다른 방법에는 심각한 영향을 미칩니다.5 결론 이 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대한 Segment Anything Model(SAM)을 개인화하는 것을 제안합니다. 첫째, 훈련이 필요 없는 기술로 SAM에 고수준 대상 의미론을 주입하는 PerSAM을 소개합니다. 여기에 더해, 스케일 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 학습 가능한 매개변수가 2개뿐인 PerSAM-F는 마스크 스케일의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 선도적인 성능을 달성합니다. 게다가, DreamBooth가 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 도움이 되는 접근 방식의 효능도 검증합니다. 저희의 작업이 SAM의 적용 범위를 더 넓은 범위의 시나리오로 확장할 수 있기를 바랍니다. 10 10 참고문헌 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. Segnet: 이미지 분할을 위한 딥 합성곱 인코더-디코더 아키텍처. IEEE 패턴 분석 및 머신 인텔리전스 저널, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei Efros. 이미지 인페인팅을 통한 시각적 프롬프트. 신경 정보 처리 시스템의 발전, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, Qi Tian. Nerfs를 사용하여 3D로 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille. Deeplab: 딥 합성곱 신경망, Atrous 합성곱 및 완전 연결 CRF를 사용한 의미적 이미지 분할. IEEE 패턴 분석 및 머신 인텔리전스 저널, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan. 대화형 분할을 위한 조건부 확산. IEEE International Conference on Computer Vision의 진행 과정에서, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao. 밀도 예측을 위한 비전 변환기 어댑터. arXiv 사전 인쇄본 arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov. 경계 iou: 객체 중심 이미지 분할 평가 개선. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 진행 과정에서, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 마스크된 어텐션 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 1290–1299, 2022. Pedro Cuenca 및 Sayak Paul. 효율적인 안정적 확산 미세 조정을 위해 lora 사용. https:// huging face.co/blog/lora, 2023년 1월. Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5356-5364쪽, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: 에지 가이드 플로우로 실용적인 상호 작용 분할 달성. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 1551-1560쪽, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. 매개변수 효율적 전이 학습에 대한 통합된 관점을 향해. International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. Mask r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, Alexandros Iosifidis. 구조화된 가지치기 어댑터. arXiv 사전 인쇄본 arXiv:2211.10155, 2022. 11 Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, Seungryong Kim. few-shot segmentation을 위한 4d 합성곱 swin 변환기를 사용한 비용 집계. European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. NLP를 위한 매개변수 효율적 전이 학습. 국제 기계 학습 컨퍼런스에서, 2790-2799쪽. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen 등. 의료 이미지에 대한 모든 모델을 세분화할 수 있나요?arXiv 사전 인쇄본 arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각적 및 시각 언어 표현 학습 확장. International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 시각적 프롬프트 튜닝. European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai 및 Chengjie Wang. 도메인 적응 의미론적 분할을 위한 프로토타입 대비 적응. 컴퓨터 비전에 관한 유럽 회의, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang 및 Liqing Zhang. Stc: 비디오 인스턴스 분할을 위한 시공간 대조 학습. 컴퓨터 비전 워크숍에 관한 유럽 회의, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang 및 Fisher Yu. 무엇이든 고품질로 분할하세요. arXiv 사전 인쇄본 arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413쪽, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 지정. arXiv 사전 인쇄본 arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. arXiv 사전 인쇄본 arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: 대규모 비전 및 비전-언어 작업을 위한 일반 모델. arXiv 사전 인쇄본 arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang. Fss-1000: few-shot segmentation을 위한 1000-class 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 2869-2878, 2020. 12 Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang. 파노라마 분할을 위한 주의 유도 통합 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, Jim Chen. 적응적 특징 은행과 불확실한 영역 정제를 사용한 비디오 객체 분할. 신경 정보 처리 시스템의 발전, 33: 3430-3441, 2020. 화이지아 린, 샤오주안 치, 지아야 지아. Agss-vos: 주의 유도 단일 샷 비디오 객체 분할. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 3949-3957쪽, 2019. 자오장 린, 안드레아 마도토, 파스칼 펑. 매개변수 효율적 전이 학습을 통해 다재다능한 생성 언어 모델 탐색. arXiv 사전 인쇄본 arXiv:2004.03829, 2020. 린쯔이, 갱시지, 장렌루이, 가오펭, 제라르 드 멜로, 왕샤오강, 다이지펭, 차오위아오, 리홍셩. 동결된 클립 모델은 효율적인 비디오 학습기입니다. European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang. P-tuning v2: 신속한 튜닝은 규모와 작업 전반에 걸쳐 보편적으로 미세 조정하는 것과 비교할 수 있습니다. arXiv 사전 인쇄본 arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen. Matcher: 다목적 기능 매칭을 사용하여 한 번에 모든 것을 분할합니다. arXiv 사전 인쇄본 arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, Trevor Darrell. 의미 분할을 위한 완전 합성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 3431-3440쪽, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee. Vilbert: 시각 및 언어 작업을 위한 작업에 독립적인 시각 언어 표현 사전 학습. 신경 정보 처리 시스템의 발전(NeurIPS), 13-23쪽, 2019. Jun Ma, Bo Wang. 의료 이미지의 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, Minsu Cho. few-shot 분할을 위한 초상관 관계 압축. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 6941-6952쪽, 2021. Keval Morabia, Jatin Arora, Tara Vijaykumar. 객체 및 의미적 부분의 주의 기반 조인트 감지. arXiv 사전 인쇄본 arXiv:2007.02419, 2020. OpenAI. Gpt-4 기술 보고서. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych. 어댑터-융합: 전이 학습을 위한 비파괴적 작업 구성. arXiv 사전 인쇄본 arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv 사전 인쇄본 arXiv:1704.00675, 2017. Guanghui Qin 및 Jason Eisner. 묻는 방법 배우기: 소프트 프롬프트를 혼합하여 lms 쿼리하기. arXiv 사전 인쇄본 arXiv:2104.06599, 2021. Alec Radford 및 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: 공통 객체의 부분과 속성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서, 7141-7151쪽, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 잔여 어댑터를 사용하여 여러 시각적 도메인 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng. 일반 기능 변환을 위한 학습 가능한 트리 필터 재고. 신경 정보 처리 시스템의 발전, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, Mohit Bansal. Vl-adapter: 시각 및 언어 작업을 위한 매개변수 효율적 전이 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5227-5237쪽, 2022. Zhi Tian, Chunhua Shen, Hao Chen. 인스턴스 분할을 위한 조건부 합성곱. 유럽 컴퓨터 비전 컨퍼런스, 282-298쪽. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄 arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li 및 Chunhua Shen. Solov2: 동적이고 빠른 인스턴스 분할. 신경 정보 처리 시스템의 발전, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen 및 Tiejun Huang. 이미지는 이미지로 말한다: 맥락 내 시각적 학습을 위한 일반주의 화가. arXiv 사전 인쇄본 arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. Seggpt: 맥락 내에서 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo. Segformer: 변환기를 사용한 의미적 분할을 위한 간단하고 효율적인 디자인. 신경 정보 처리 시스템의 발전, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, Yu Qiao. 3D 객체 포인트 클라우드의 보완적 이해를 위한 기하학-분리된 표현 학습. AAAI 인공지능 컨퍼런스 회의록, 35권, 3056-3064쪽, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng. 무엇이든 추적: 무엇이든 비디오와 만나는 세그먼트. arXiv 사전 인쇄본 arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, Choong Seon Hong. 무엇이든 더 빠르게 세그먼트화: 모바일 애플리케이션을 위한 가벼운 sam을 향해. arXiv 사전 인쇄본 arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen 및 Tuo Zhao. 매개변수 효율적인 미세 조정을 위한 적응형 예산 할당. arXiv 사전 인쇄 arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao 및 Peng Gao. 프롬프트, 생성, 캐시: 기반 모델의 캐스케이드는 강력한 소수의 학습자를 만듭니다. arXiv 사전 인쇄 arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 및 Jiaya Jia. 피라미드 장면 구문 분석 네트워크. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의 진행, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang 및 Jinqiao Wang. 무엇이든 빠르게 분할하세요. arXiv 사전 인쇄 arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr 등. 트랜스포머를 사용한 시퀀스-투-시퀀스 관점에서 의미적 분할 재고. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6881-6890쪽, 2021. 카이양 저우, 징캉 양, 첸 창 로이, 지웨이 리우. 시각 언어 모델을 위한 프롬프트 학습. 국제 컴퓨터 비전 저널, 130(9):2337-2348, 2022. 쉐얀 저우, 지안웨이 양, 하오 장, 펭 리, 린지에 리, 지안펭 가오, 용재 리. 모든 곳을 한꺼번에 분할. arXiv 사전 인쇄본 arXiv:2304.06718, 2023. 15 115
