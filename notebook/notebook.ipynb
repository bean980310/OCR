{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>reached_out_link</th>\n",
       "      <th>reached_out_success</th>\n",
       "      <th>reached_out_note</th>\n",
       "      <th>num_models</th>\n",
       "      <th>num_datasets</th>\n",
       "      <th>num_spaces</th>\n",
       "      <th>title</th>\n",
       "      <th>github</th>\n",
       "      <th>github_stars</th>\n",
       "      <th>conference_name</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>github_mention_hf</th>\n",
       "      <th>has_artifact</th>\n",
       "      <th>submitted_by</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.03048</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Personalize Segment Anything Model with One Shot</td>\n",
       "      <td>https://github.com/zrrskywalker/personalize-sam</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.02463</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Shap-E: Generating Conditional 3D Implicit Fun...</td>\n",
       "      <td>https://github.com/openai/shap-e</td>\n",
       "      <td>11606.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.02483</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ChatGPT-steered Editing Instructor for Customi...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.03047</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Principle-Driven Self-Alignment of Language Mo...</td>\n",
       "      <td>https://github.com/IBM/Dromedary</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>NeurIPS2023</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.02440</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cheaply Evaluating Inference Efficiency Metric...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>2412.09604</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SynerGen-VL: Towards Synergistic Image Underst...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>wzk1015</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>2412.09611</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FluxSpace: Disentangled Semantic Editing in Re...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>ydalva</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>2412.09856</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LinGen: Towards High-Resolution Minute-Length ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hongjiewang</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>2412.10345</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TraceVLA: Visual Trace Prompting Enhances Spat...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>rzheng12</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4944</th>\n",
       "      <td>2412.09722</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GReaTer: Gradients over Reasoning Makes Smalle...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>sarathismg</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4945 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        arxiv_id reached_out_link  reached_out_success reached_out_note  \\\n",
       "0     2305.03048             None                  0.0                    \n",
       "1     2305.02463             None                  0.0                    \n",
       "2     2305.02483             None                  0.0                    \n",
       "3     2305.03047             None                  0.0                    \n",
       "4     2305.02440             None                  0.0                    \n",
       "...          ...              ...                  ...              ...   \n",
       "4940  2412.09604             None                  NaN             None   \n",
       "4941  2412.09611             None                  NaN             None   \n",
       "4942  2412.09856             None                  NaN             None   \n",
       "4943  2412.10345             None                  NaN             None   \n",
       "4944  2412.09722             None                  NaN             None   \n",
       "\n",
       "      num_models  num_datasets  num_spaces  \\\n",
       "0            0.0           0.0         1.0   \n",
       "1            7.0           0.0        70.0   \n",
       "2            0.0           0.0         0.0   \n",
       "3            0.0           1.0         1.0   \n",
       "4            0.0           0.0         0.0   \n",
       "...          ...           ...         ...   \n",
       "4940         0.0           0.0         0.0   \n",
       "4941         0.0           0.0         0.0   \n",
       "4942         0.0           0.0         0.0   \n",
       "4943         0.0           0.0         0.0   \n",
       "4944         0.0           0.0         0.0   \n",
       "\n",
       "                                                  title  \\\n",
       "0      Personalize Segment Anything Model with One Shot   \n",
       "1     Shap-E: Generating Conditional 3D Implicit Fun...   \n",
       "2     ChatGPT-steered Editing Instructor for Customi...   \n",
       "3     Principle-Driven Self-Alignment of Language Mo...   \n",
       "4     Cheaply Evaluating Inference Efficiency Metric...   \n",
       "...                                                 ...   \n",
       "4940  SynerGen-VL: Towards Synergistic Image Underst...   \n",
       "4941  FluxSpace: Disentangled Semantic Editing in Re...   \n",
       "4942  LinGen: Towards High-Resolution Minute-Length ...   \n",
       "4943  TraceVLA: Visual Trace Prompting Enhances Spat...   \n",
       "4944  GReaTer: Gradients over Reasoning Makes Smalle...   \n",
       "\n",
       "                                               github  github_stars  \\\n",
       "0     https://github.com/zrrskywalker/personalize-sam        1505.0   \n",
       "1                    https://github.com/openai/shap-e       11606.0   \n",
       "2                                                None           NaN   \n",
       "3                    https://github.com/IBM/Dromedary        1119.0   \n",
       "4                                                None           NaN   \n",
       "...                                               ...           ...   \n",
       "4940                                                            NaN   \n",
       "4941                                                            NaN   \n",
       "4942                                                            NaN   \n",
       "4943                                                            NaN   \n",
       "4944                                                            NaN   \n",
       "\n",
       "     conference_name  upvotes  num_comments  github_mention_hf  has_artifact  \\\n",
       "0               None        9             1                1.0          True   \n",
       "1               None        3             1                0.0          True   \n",
       "2               None        3             1                0.0         False   \n",
       "3        NeurIPS2023        1             5                1.0          True   \n",
       "4               None        1             0                0.0         False   \n",
       "...              ...      ...           ...                ...           ...   \n",
       "4940            None       24             1                0.0         False   \n",
       "4941            None        4             1                0.0         False   \n",
       "4942            None        3             1                0.0         False   \n",
       "4943            None        1             1                0.0         False   \n",
       "4944            None        0             2                0.0         False   \n",
       "\n",
       "     submitted_by        date  \n",
       "0         akhaliq  2023-05-05  \n",
       "1         akhaliq  2023-05-05  \n",
       "2         akhaliq  2023-05-05  \n",
       "3         akhaliq  2023-05-05  \n",
       "4         akhaliq  2023-05-05  \n",
       "...           ...         ...  \n",
       "4940      wzk1015  2024-12-16  \n",
       "4941       ydalva  2024-12-16  \n",
       "4942  hongjiewang  2024-12-16  \n",
       "4943     rzheng12  2024-12-16  \n",
       "4944   sarathismg  2024-12-16  \n",
       "\n",
       "[4945 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"huggingface/community-science-paper-v2\")\n",
    "train_ds=ds['train']\n",
    "df=pd.DataFrame(train_ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arxiv_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2305.03048\n",
       "1       2305.02463\n",
       "2       2305.02483\n",
       "3       2305.03047\n",
       "4       2305.02440\n",
       "           ...    \n",
       "4940    2412.09604\n",
       "4941    2412.09611\n",
       "4942    2412.09856\n",
       "4943    2412.10345\n",
       "4944    2412.09722\n",
       "Name: arxiv_id, Length: 4945, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id=df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2305.03048'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition https://arxiv.org/pdf/{arxiv_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud import storage\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=documentai.DocumentProcessorServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = %system gcloud config get-value core/project\n",
    "project_id = project_id[0]\n",
    "location = 'us'\n",
    "processor_id=os.getenv('PROCESSOR_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genuine-episode-425909-e2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'857826f004a8bd4c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"2305.03048v2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    content=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = documentai.types.ProcessRequest(\n",
    "    name=f\"projects/{project_id}/locations/{location}/processors/{processor_id}\",\n",
    "    raw_document=documentai.types.RawDocument(\n",
    "        content=content,\n",
    "        mime_type=\"application/pdf\"  # PDF 파일의 MIME 타입\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.process_document(request=request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = result.document\n",
    "text=document.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config={\n",
    "    \"top_p\":1,\n",
    "    \"top_k\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel('gemini-1.5-flash-002', generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"\n",
    "{text}\n",
    "위의 텍스트를 번역해서 전부 출력해줘.\n",
    "그 외에는 출력하지마.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text=response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 단일 샷으로 세그먼트 애니씽 모델 개인화하기\n",
      "\n",
      "**요약**\n",
      "\n",
      "대규모 데이터 사전 훈련으로 구동되는 Segment Anything Model (SAM)은 강력한 프롬프트 가능 프레임워크로 입증되어 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 사람의 프롬프트 없이 특정 시각적 개념에 대해 SAM을 사용자 지정하는 것은 아직 탐구되지 않은 영역입니다(예: 수많은 이미지에서 애완견을 자동으로 분할). 본 논문에서는 PerSAM이라는 SAM을 위한 훈련 없는 개인화 접근 방식을 소개합니다. 단일 이미지와 참조 마스크라는 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양성-음성 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 대상 안내 주의 및 대상 의미론적 프롬프팅이라는 두 가지 제안된 기술을 통해 개인화된 객체 분할을 위해 SAM을 강화합니다. 이러한 방식으로 훈련 없이도 개인적인 용도로 범용 SAM을 효과적으로 사용자 지정할 수 있습니다. 분할 척도의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. 전체 SAM을 고정한 상태에서 다중 척도 마스크를 집계하는 척도 인식 미세 조정을 도입하여 성능 향상을 위해 10초 이내에 2개의 매개변수만 조정합니다. 효능을 입증하기 위해 개인화된 객체 분할 평가를 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다. 또한 PerSAM을 활용하여 개인화된 텍스트-이미지 합성을 위한 DreamBooth를 개선할 것을 제안합니다. 훈련 세트 배경의 방해를 완화하여 입력 텍스트 프롬프트에 대한 더 나은 대상 모양 생성과 더 높은 충실도를 보여줍니다. 코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.\n",
      "\n",
      "\n",
      "**1. 소개**\n",
      "\n",
      "비전(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), 그리고 멀티모달(Radford et al., 2021; Jia et al., 2021; Li et al., 2023) 분야의 기반 모델은 대규모 데이터 세트와 계산 리소스의 가용성 덕분에 전례 없는 보급률을 얻었습니다. 이들은 제로샷 시나리오에서 탁월한 일반화 능력을 보여주고 인간 피드백을 통합하는 다양한 상호 작용성을 보여줍니다. 이에 영감을 받아 Segment Anything (Kirillov et al., 2023)은 1,100만 개의 이미지-마스크 데이터를 수집하기 위한 정교한 데이터 엔진을 개발하고, 이후 SAM이라는 세분화 기반 모델을 훈련합니다. 수작업 프롬프트를 입력으로 받아 예상 마스크를 반환하는 새로운 프롬프트 가능 세분화 프레임워크를 정의하여 시각적 맥락에서 모든 객체를 분할할 수 있습니다.\n",
      "\n",
      "그러나 SAM은 본질적으로 특정 시각적 개념을 분할하는 능력을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 누락된 시계를 찾는다고 상상해 보십시오. 바닐라 SAM을 사용하는 것은 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에 대해 복잡한 컨텍스트 내에서 대상 객체를 정확하게 찾은 다음 적절한 프롬프트로 SAM을 활성화하여 분할해야 합니다. 이를 고려하여 다음과 같이 질문합니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 분할하도록 SAM을 개인화할 수 있습니까?\n",
      "\n",
      "이를 위해 Segment Anything Model을 위한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 그림 1과 같이, 이 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크라는 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 지정합니다. 구체적으로, 먼저 특징 유사성을 통해 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 얻습니다. 이는 모든 전경 픽셀의 모양을 고려합니다. 신뢰도 점수에 따라 두 점이 양성-음성 위치 사전으로 선택되고, 최종적으로 프롬프트 토큰으로 인코딩되어 분할을 위해 SAM의 디코더에 공급됩니다. 디코더 내에서 대상 객체의 시각적 의미론을 주입하여 두 가지 기술, 즉 대상 안내 주의 및 대상 의미론적 프롬프팅을 통해 SAM의 개인화된 분할 기능을 발휘합니다.\n",
      "\n",
      "* **대상 안내 주의.** SAM 디코더의 모든 토큰-이미지 교차 주의 레이어를 위치 신뢰도 맵으로 안내합니다. 이는 프롬프트 토큰이 주로 전경 대상 영역에 집중하여 집중적인 특징 집계를 수행하도록 명시적으로 강제합니다.\n",
      "\n",
      "* **대상 의미론적 프롬프팅.** SAM에 상위 수준 대상 의미론을 명시적으로 제공하기 위해 원본 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 하위 수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다.\n",
      "\n",
      "위에서 언급한 설계와 계단식 사후 개선을 통해 PerSAM은 다양한 포즈 또는 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히 이 접근 방식은 여러 유사한 객체 중 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오를 잘 처리할 수 있습니다. 그럼에도 불구하고 그림 2와 같이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층적 구조(예: 테디 베어 위의 모자 또는 로봇 장난감의 머리)로 구성된 경우 실패 사례가 발생할 수 있습니다. 이러한 모호성은 로컬 부분과 글로벌 모양 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문에 PerSAM이 출력할 적절한 마스크 척도를 결정하는 데 어려움을 줍니다.\n",
      "\n",
      "이 문제를 완화하기 위해 미세 조정 변형 접근 방식인 PerSAM-F를 추가로 제안합니다. 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 고정하고 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 설명하면, SAM이 다양한 마스크 척도의 잠재적 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 대해 최상의 척도를 적응적으로 선택하기 위해 각 마스크 척도에 대해 학습 가능한 상대적 가중치를 사용하고 가중 합계를 최종 출력으로 수행합니다. 이러한 효율적인 척도 인식 훈련을 통해 PerSAM-F는 원샷 데이터에 대한 과적합을 방지하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다.\n",
      "\n",
      "또한 이 접근 방식은 DreamBooth(Ruiz et al., 2022)가 그림 3과 같이 개인화된 텍스트-이미지 생성을 위해 확산 모델을 더 잘 미세 조정하는 데 도움이 될 수 있음을 관찰했습니다. 특정 시각적 개념(예: 애완 고양이 또는 배낭)이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 단어 임베딩 공간에서 이러한 이미지를 식별자 [V]로 변환하는 방법을 학습하지만, 동시에 배경 정보(예: 계단 또는 숲)를 포함할 수 있습니다. 이는 새로 프롬프트된 배경을 재정의하고 대상 모양 생성을 방해합니다. 따라서 훈련 이미지 내에서 대상 객체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하기 위해 PerSAM을 활용할 것을 제안합니다.\n",
      "\n",
      "본 논문의 기여 사항을 다음과 같이 요약합니다.\n",
      "\n",
      "* **개인화된 객체 분할.** 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오에 맞게 사용자 지정하는 방법을 처음으로 조사합니다. 이를 위해 두 가지 효율적이고 효과적인 방법과 개인화된 객체 분할 평가를 위한 새로운 분할 데이터 세트인 PerSeg를 소개합니다.\n",
      "\n",
      "\n",
      "* **PerSAM 및 PerSAM-F.** PerSAM에서는 훈련 없는 세 가지 기술을 제안하여 대상 객체의 상위 수준 의미론으로 SAM을 안내합니다. PerSAM-F에서는 10초 안에 2개의 매개변수로 척도 인식 미세 조정을 설계하여 마스크 모호성 문제를 잘 완화합니다.\n",
      "\n",
      "* 이 접근 방식은 PerSeg 벤치마크, 원샷 부품 및 의미론적 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 달성합니다. 또한 PerSAM은 DreamBooth를 강화하여 더 나은 개인화된 텍스트-이미지 합성을 가능하게 합니다.\n",
      "\n",
      "(중략)\n",
      "\n",
      "**5. 결론**\n",
      "\n",
      "본 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대해 Segment Anything Model (SAM)을 개인화하는 방법을 제안합니다. 먼저, 훈련 없는 기술을 사용하여 상위 수준 대상 의미론을 SAM에 주입하는 PerSAM을 소개합니다. 이를 바탕으로 척도 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 단 2개의 학습 가능한 매개변수를 사용하여 PerSAM-F는 마스크 척도의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 최고의 성능을 달성합니다. 또한, 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 DreamBooth를 지원하는 접근 방식의 효능을 검증합니다. 본 연구가 SAM의 적용 가능성을 더 넓은 범위의 시나리오로 확장하기를 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2-translated.txt', 'w') as f:\n",
    "    f.write(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"2305.03048v2.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:2305.03048v2 [cs.CV] 4 OctPERSONALIZE SEGMENT ANYTHING MODEL WITH ONE SHOT Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory 3 Institute of Automation, Chinese Academy of Sciences 4CFCS, School of CS, Peking University {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM\\'s decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM\\'s personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM\\'s decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM\\'s Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM\\'s frozen backbone or other pre-trained vision models, for which we adopt SAM\\'s image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM\\'s decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one\\'s on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM\\'s decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM\\'s decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM\\'s decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM\\'s original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F\\'s Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F\\'s improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM\\'s decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM\\'s decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5 CONCLUSION In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_text = re.sub(r'Page\\s*\\d+|\\s*\\d+\\s*\\n', '', text)\n",
    "replaced_text = re.sub(r'-\\n', '', replaced_text)\n",
    "replaced_text = re.sub(r'\\s+', ' ', replaced_text).strip()\n",
    "\n",
    "replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 OctPERSONALIZE SEGMENT ANYTHING MODEL WITH ONE SHOT Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory 3 Institute of Automation, Chinese Academy of Sciences 4CFCS, School of CS, Peking University {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt SAM's image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM's decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM's decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F's Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5 CONCLUSION In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n"
     ]
    }
   ],
   "source": [
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2305.03048v2_cleaned.txt\", \"w\") as f:\n",
    "    f.write(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "section_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sections)-1):\n",
    "    pattern = f\"{sections[i]}(.*?){sections[i+1]}\"\n",
    "    match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "    if match:\n",
    "        section_data[sections[i]] = match.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_section = sections[-1]\n",
    "pattern = f\"{last_section}(.*?)$\"\n",
    "match = re.search(pattern, replaced_text, re.S | re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ABSTRACT ---\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. S...\n",
      "\n",
      "--- METHOD ---\n",
      "s on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-fre...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw imag...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of o...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    section_data[last_section] = match.group(1).strip()\n",
    "\n",
    "for sec, content in section_data.items():\n",
    "    print(f\"--- {sec} ---\\n{content[:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ABSTRACT ---\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our\n",
      "\n",
      "--- METHOD ---\n",
      "s on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt SAM's image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM's decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM's decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F's Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sec, content in section_data.items():\n",
    "    print(f\"--- {sec} ---\\n{content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oct', 'personalize', 'segment', 'anything', 'model', 'one', 'shot', 'renrui', 'zhengkai', 'ziyu', 'shilin', 'junting', 'xianzheng', 'hao', 'dongª', 'yu', 'peng', 'hongsheng', 'mmlab', 'shanghai', 'artificial', 'intelligence', 'laboratory', 'institute', 'automation', 'chinese', 'academy', 'sciences', 'school', 'cs', 'peking', 'university', 'zhangrenrui', 'gaopeng', 'guoziyu', 'qiaoyu', 'hsli', 'abstract', 'driven', 'segment', 'anything', 'model', 'sam', 'demonstrated', 'powerful', 'promptable', 'framework', 'revolutionizing', 'tion', 'field', 'despite', 'generality', 'customizing', 'sam', 'specific', 'visual', 'concepts', 'without', 'prompting', 'automatically', 'segmenting', 'pet', 'dog', 'numerous', 'images', 'paper', 'introduce', 'personalization', 'approach', 'sam', 'termed', 'persam', 'given', 'data', 'single', 'image', 'reference', 'mask', 'first', 'obtain', 'cation', 'prior', 'target', 'concept', 'new', 'images', 'aided', 'target', 'visual', 'semantics', 'empower', 'sam', 'personalized', 'object', 'segmentation', 'via', 'two', 'posed', 'techniques', 'attention']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oct', 'personalize', 'segment', 'anything', 'model', 'one', 'shot', 'renrui', 'zhengkai', 'ziyu', 'shilin', 'junting', 'xianzheng', 'hao', 'dongª', 'yu', 'peng', 'hongsheng', 'mmlab', 'shanghai', 'artificial', 'intelligence', 'laboratory', 'institute', 'automation', 'chinese', 'academy', 'sciences', 'school', 'cs', 'peking', 'university', 'zhangrenrui', 'gaopeng', 'guoziyu', 'qiaoyu', 'hsli', 'abstract', 'driven', 'segment', 'anything', 'model', 'sam', 'demonstrated', 'powerful', 'promptable', 'framework', 'revolutionizing', 'tion', 'field', 'despite', 'generality', 'customizing', 'sam', 'specific', 'visual', 'concepts', 'without', 'prompting', 'automatically', 'segmenting', 'pet', 'dog', 'numerous', 'images', 'paper', 'introduce', 'personalization', 'approach', 'sam', 'termed', 'persam', 'given', 'data', 'single', 'image', 'reference', 'mask', 'first', 'obtain', 'cation', 'prior', 'target', 'concept', 'new', 'images', 'aided', 'target', 'visual', 'semantics', 'empower', 'sam', 'personalized', 'object', 'segmentation', 'via', 'two', 'posed', 'techniques', 'attention', 'prompting', 'way', 'effectively', 'customize', 'sam', 'private', 'use', 'without', 'training', 'alleviate', 'ambiguity', 'segmentation', 'scales', 'present', 'efficient', 'variant', 'freezing', 'entire', 'sam', 'introduce', 'aggregate', 'masks', 'tunes', 'parameters', 'within', 'seconds', 'improved', 'performance', 'demonstrate', 'efficacy', 'construct', 'new', 'dataset', 'perseg', 'evaluation', 'personalized', 'object', 'segmentation', 'also', 'test', 'methods', 'various', 'image', 'video', 'segmentation', 'benchmarks', 'besides', 'propose', 'leverage', 'persam', 'improve', 'dreambooth', 'personalized', 'synthesis', 'mitigating', 'disturbance', 'backgrounds', 'approach', 'showcases', 'better', 'target', 'appearance', 'generation', 'higher', 'fidelity', 'input', 'text', 'prompt', 'code', 'released', 'https', 'user', 'provides', 'learning', 'personalized', 'segmentation', 'one', 'image', 'one', 'mask', 'persam', 'various', 'poses', 'scenes', 'figure', 'personalization', 'segment', 'anything', 'model', 'customize', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'specific', 'visual', 'concepts', 'pet', 'dog', 'data', 'introduce', 'two', 'efficient', 'solutions', 'persam', 'equal', 'contribution', 'corresponding', 'author', 'parameters', 'persam', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'seconds', 'hat', 'teddy', 'bear', 'photo', 'v', 'cat', 'v', 'cat', 'beach', 'various', 'poses', 'scenes', 'body', 'robot', 'toy', 'figure', 'personalized', 'segmentation', 'ples', 'persam', 'left', 'segment', 'personal', 'objects', 'context', 'favorable', 'mance', 'right', 'alleviates', 'ambiguity', 'issue', 'introduction', 'photo', 'v', 'backpack', 'v', 'backpack', 'table', 'classroom', 'figure', 'improving', 'dreambooth', 'ruiz', 'et', 'persam', 'mitigating', 'bance', 'backgrounds', 'training', 'proach', 'help', 'achieve', 'alized', 'generation', 'foundations', 'models', 'vision', 'li', 'et', 'zou', 'et', 'wang', 'et', 'language', 'brown', 'et', 'touvron', 'et', 'radford', 'et', 'radford', 'et', 'jia', 'et', 'li', 'et', 'gained', 'unprecedented', 'prevalence', 'attributed', 'availability', 'datasets', 'computational', 'resources', 'demonstrate', 'extraordinary', 'generalization', 'capacity', 'scenarios', 'display', 'versatile', 'interactivity', 'incorporating', 'human', 'feedback', 'inspired', 'segment', 'anything', 'kirillov', 'et', 'develops', 'delicate', 'data', 'engine', 'collecting', 'data', 'subsequently', 'trains', 'segmentation', 'foundation', 'model', 'known', 'sam', 'defines', 'novel', 'promptable', 'segmentation', 'framework', 'taking', 'input', 'handcrafted', 'prompt', 'returning', 'expected', 'mask', 'allows', 'segmenting', 'objects', 'visual', 'contexts', 'however', 'sam', 'inherently', 'loses', 'capability', 'segment', 'specific', 'visual', 'concepts', 'imagine', 'intending', 'crop', 'lovely', 'pet', 'dog', 'thick', 'photo', 'album', 'find', 'missing', 'clock', 'picture', 'bedroom', 'utilizing', 'vanilla', 'sam', 'would', 'highly', 'image', 'must', 'precisely', 'find', 'target', 'object', 'within', 'complicated', 'contexts', 'activate', 'sam', 'proper', 'prompt', 'segmentation', 'considering', 'ask', 'personalize', 'sam', 'automatically', 'segment', 'visual', 'concepts', 'simple', 'efficient', 'manner', 'end', 'introduce', 'persam', 'personalization', 'approach', 'segment', 'anything', 'model', 'shown', 'figure', 'method', 'efficiently', 'customizes', 'sam', 'using', 'data', 'reference', 'image', 'rough', 'mask', 'personal', 'concept', 'specifically', 'first', 'obtain', 'location', 'confidence', 'map', 'target', 'object', 'test', 'image', 'feature', 'similarities', 'considers', 'appearance', 'every', 'foreground', 'pixel', 'according', 'confidence', 'scores', 'two', 'points', 'selected', 'location', 'prior', 'finally', 'encoded', 'prompt', 'tokens', 'fed', 'sam', 'decoder', 'segmentation', 'within', 'decoder', 'propose', 'inject', 'visual', 'semantics', 'target', 'object', 'unleash', 'sam', 'personalized', 'segmentation', 'power', 'two', 'techniques', 'attention', 'guide', 'every', 'layer', 'sam', 'decoder', 'location', 'confidence', 'map', 'explicitly', 'compels', 'prompt', 'tokens', 'mainly', 'concentrate', 'foreground', 'target', 'regions', 'intensive', 'feature', 'aggregation', 'prompting', 'explicitly', 'provide', 'sam', 'target', 'semantics', 'fuse', 'original', 'prompt', 'tokens', 'embedding', 'target', 'object', 'provides', 'positional', 'prompt', 'additional', 'visual', 'cues', 'personalized', 'segmentation', 'aforementioned', 'designs', 'along', 'cascaded', 'persam', 'exhibits', 'favorable', 'personalized', 'segmentation', 'performance', 'unique', 'subjects', 'variety', 'poses', 'scenes', 'notably', 'approach', 'cope', 'well', 'scenarios', 'require', 'segmenting', 'one', 'object', 'among', 'multiple', 'similar', 'ones', 'simultaneously', 'segmenting', 'several', 'identical', 'objects', 'image', 'tracking', 'different', 'objects', 'along', 'video', 'nevertheless', 'shown', 'figure', 'might', 'occasional', 'failure', 'cases', 'object', 'comprises', 'visually', 'distinct', 'subparts', 'hierarchical', 'structures', 'segmented', 'hat', 'top', 'teddy', 'bear', 'head', 'robot', 'toy', 'ambiguity', 'casts', 'challenge', 'persam', 'determining', 'appropriate', 'scale', 'mask', 'output', 'since', 'local', 'part', 'global', 'shape', 'regarded', 'valid', 'masks', 'sam', 'alleviate', 'issue', 'propose', 'variant', 'approach', 'freeze', 'entire', 'sam', 'preserve', 'versatile', 'knowledge', 'parameters', 'within', 'seconds', 'single', 'gpu', 'detail', 'enable', 'sam', 'produce', 'several', 'potential', 'segmentation', 'results', 'different', 'mask', 'scales', 'adaptively', 'select', 'best', 'scale', 'varying', 'objects', 'employ', 'learnable', 'relative', 'weight', 'mask', 'scale', 'conduct', 'weighted', 'summation', 'final', 'output', 'efficient', 'training', 'avoids', 'data', 'exhibits', 'better', 'segmentation', 'accuracy', 'shown', 'figure', 'right', 'moreover', 'observe', 'approach', 'also', 'assist', 'dreambooth', 'ruiz', 'et', 'better', 'diffusion', 'models', 'personalized', 'generation', 'shown', 'figure', 'given', 'images', 'containing', 'specific', 'visual', 'concept', 'pet', 'cat', 'backpack', 'dreambooth', 'learns', 'convert', 'images', 'identifier', 'v', 'word', 'embedding', 'space', 'however', 'simultaneously', 'include', 'background', 'information', 'stairs', 'forest', 'would', 'override', 'newly', 'prompted', 'backgrounds', 'disturb', 'target', 'appearance', 'generation', 'therefore', 'propose', 'leverage', 'persam', 'segment', 'target', 'object', 'within', 'training', 'images', 'supervise', 'dreambooth', 'foreground', 'area', 'enabling', 'synthesis', 'higher', 'quality', 'summarize', 'contributions', 'paper', 'follows', 'personalized', 'object', 'segmentation', 'first', 'investigate', 'customize', 'purpose', 'segmentation', 'model', 'sam', 'personalized', 'scenarios', 'minimal', 'expense', 'end', 'introduce', 'two', 'efficient', 'effective', 'methods', 'along', 'new', 'segmentation', 'dataset', 'perseg', 'evaluation', 'personalized', 'object', 'segmentation', 'persam', 'persam', 'propose', 'three', 'techniques', 'guide', 'sam', 'semantics', 'target', 'objects', 'design', 'parameters', 'seconds', 'well', 'alleviate', 'mask', 'ambiguity', 'issue', 'approach', 'achieves', 'competitive', 'results', 'various', 'tasks', 'including', 'perseg', 'benchmark', 'part', 'semantic', 'segmentation', 'video', 'object', 'segmentation', 'addition', 'persam', 'enhance', 'dreambooth', 'better', 'personalized', 'synthesis', 'related', 'work', 'foundation', 'models', 'powerful', 'generalization', 'capacity', 'foundation', 'models', 'adapted', 'various', 'downstream', 'scenarios', 'attain', 'promising', 'performance', 'natural', 'language', 'processing', 'bert', 'devlin', 'et', 'lu', 'et', 'gpt', 'series', 'brown', 'et', 'openai', 'radford', 'narasimhan', 'radford', 'et', 'llama', 'zhang', 'et', 'demonstrated', 'remarkable', 'learning', 'abilities', 'transferred', 'new', 'tasks', 'specific', 'prompts', 'similarly', 'clip', 'radford', 'et', 'align', 'jia', 'et', 'conduct', 'contrastive', 'learning', 'pairs', 'exhibit', 'exceptional', 'accuracy', 'visual', 'recognition', 'painter', 'wang', 'et', 'introduces', 'vision', 'model', 'unifies', 'network', 'architectures', 'prompts', 'accomplish', 'diverse', 'vision', 'tasks', 'without', 'downstream', 'cafo', 'zhang', 'et', 'cascades', 'different', 'foundation', 'models', 'collaborates', 'knowledge', 'robust', 'image', 'classification', 'sam', 'kirillov', 'et', 'presents', 'foundation', 'model', 'image', 'segmentation', 'billion', 'masks', 'conducts', 'segmentation', 'concurrent', 'works', 'extending', 'sam', 'segmentation', 'ke', 'et', 'faster', 'inference', 'speed', 'zhao', 'et', 'zhang', 'et', 'matching', 'liu', 'et', 'reconstruction', 'cen', 'et', 'object', 'tracking', 'yang', 'et', 'medical', 'wang', 'huang', 'et', 'image', 'processing', 'another', 'perspective', 'propose', 'personalize', 'segmentation', 'foundation', 'model', 'sam', 'specific', 'visual', 'concepts', 'adapts', 'generalist', 'specialist', 'one', 'shot', 'method', 'also', 'assist', 'personalization', 'image', 'foundation', 'models', 'stable', 'diffusion', 'rombach', 'et', 'imagen', 'saharia', 'et', 'improves', 'generation', 'quality', 'segmenting', 'foreground', 'target', 'objects', 'background', 'disturbance', 'large', 'models', 'segmentation', 'fundamental', 'task', 'computer', 'vision', 'segmentation', 'long', 'et', 'jiang', 'et', 'zhao', 'et', 'xu', 'et', 'jiang', 'et', 'lin', 'et', 'requires', 'comprehension', 'image', 'various', 'tasks', 'explored', 'semantic', 'segmentation', 'classifying', 'pixel', 'predefined', 'set', 'classes', 'narayanan', 'et', 'chen', 'et', 'zheng', 'et', 'cheng', 'et', 'xie', 'et', 'song', 'et', 'instance', 'segmentation', 'focusing', 'identification', 'individual', 'object', 'instances', 'et', 'wang', 'et', 'tian', 'et', 'panoptic', 'segmentation', 'assigning', 'class', 'labels', 'instance', 'identification', 'kirillov', 'et', 'li', 'et', 'interactive', 'segmentation', 'involving', 'human', 'intervention', 'refinement', 'hao', 'et', 'chen', 'et', 'recently', 'inspired', 'language', 'foundation', 'models', 'zhang', 'et', 'brown', 'et', 'several', 'concurrent', 'works', 'proposed', 'vision', 'models', 'image', 'segmentation', 'extensive', 'mask', 'data', 'exhibit', 'strong', 'generalization', 'capabilities', 'numerous', 'image', 'distributions', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'utilizes', 'data', 'engine', 'tation', 'learn', 'promptable', 'segmentation', 'framework', 'generalizes', 'downstream', 'scenarios', 'manner', 'painter', 'wang', 'et', 'seggpt', 'wang', 'et', 'introduce', 'robust', 'learning', 'paradigm', 'segment', 'images', 'given', 'prompt', 'seem', 'zou', 'et', 'presents', 'general', 'segmentation', 'model', 'prompted', 'references', 'language', 'audio', 'incorporating', 'versatile', 'semantic', 'knowledge', 'study', 'introduce', 'new', 'task', 'termed', 'personalized', 'object', 'segmentation', 'annotate', 'new', 'dataset', 'perseg', 'evaluation', 'instead', 'developing', 'large', 'segmentation', 'models', 'goal', 'personalize', 'segment', 'objects', 'poses', 'scenes', 'propose', 'two', 'approaches', 'persam', 'efficiently', 'customize', 'sam', 'personalized', 'segmentation', 'directly', 'tuning', 'entire', 'foundation', 'models', 'downstream', 'tasks', 'computationally', 'expensive', 'posing', 'challenges', 'constrained', 'applications', 'address', 'issue', 'recent', 'works', 'focused', 'developing', 'efficient', 'methods', 'sung', 'et', 'et', 'rebuffi', 'et', 'qin', 'eisner', 'freeze', 'weights', 'foundation', 'models', 'append', 'modules', 'prompt', 'tuning', 'lester', 'et', 'zhou', 'et', 'jia', 'et', 'liu', 'et', 'suggests', 'using', 'learnable', 'soft', 'prompts', 'alongside', 'frozen', 'models', 'perform', 'specific', 'downstream', 'tasks', 'achieving', 'competitive', 'performance', 'scale', 'robust', 'domain', 'transfer', 'compared', 'full', 'model', 'tuning', 'adaption', 'lora', 'hu', 'et', 'cuenca', 'paul', 'zhang', 'et', 'hedegaard', 'et', 'injects', 'trainable', 'rank', 'decomposition', 'matrices', 'concurrently', 'weight', 'significantly', 'reduces', 'number', 'learnable', 'parameters', 'required', 'downstream', 'tasks', 'adapters', 'houlsby', 'et', 'pfeiffer', 'et', 'lin', 'et', 'chen', 'et', 'designed', 'inserted', 'layers', 'original', 'transformer', 'introducing', 'lightweight', 'mlps', 'feature', 'transformation', 'different', 'existing', 'works', 'adopt', 'efficient', 'adaption', 'method', 'delicately', 'designed', 'sam', 'parameters', 'seconds', 'effectively', 'avoids', 'issue', 'data', 'alleviates', 'ambiguity', 'segmentation', 'scale', 'superior', 'performance', 'method', 'section', 'first', 'briefly', 'revisit', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'introduce', 'task', 'definition', 'personalized', 'object', 'segmentation', 'illustrate', 'methodology', 'persam', 'section', 'respectively', 'finally', 'utilize', 'approach', 'assist', 'dreambooth', 'ruiz', 'et', 'better', 'generation', 'section', 'personalized', 'object', 'segmentation', 'revisit', 'segment', 'anything', 'sam', 'consists', 'three', 'components', 'prompt', 'encoder', 'image', 'encoder', 'lightweight', 'mask', 'decoder', 'respectively', 'denoted', 'encp', 'enc', 'decм', 'promptable', 'framework', 'sam', 'takes', 'input', 'image', 'set', 'prompts', 'p', 'point', 'box', 'coarse', 'mask', 'specifically', 'sam', 'first', 'utilizes', 'enc', 'obtain', 'input', 'image', 'feature', 'adopts', 'encp', 'encode', 'prompts', 'length', 'k', 'prompt', 'tokens', 'fi', 'enc', 'tp', 'encp', 'p', 'encode', 'cosine', 'similarity', 'ft', 'test', 'image', 'target', 'local', 'features', 'fr', 'fr', 'encode', 'persam', 'decoder', 'attention', 'prompting', 'si', 'local', 'confidence', 'maps', 'modulate', 'aggregate', 'attention', 'matrix', 'local', 'features', 'token', 'overall', 'confidence', 'map', 'aggregate', 'concat', 'repeat', 'overall', 'confidence', 'map', 'tm', 'тр', 'tr', 'image', 'ir', 'mask', 'mr', 'positive', 'prior', 'negative', 'prior', 'figure', 'location', 'prior', 'calculate', 'location', 'confidence', 'map', 'target', 'object', 'new', 'test', 'image', 'ance', 'local', 'parts', 'select', 'tion', 'prior', 'point', 'prompt', 'persam', 'positional', 'prompt', 'semantic', 'prompt', 'figure', 'attention', 'left', 'prompting', 'right', 'ject', 'sam', 'target', 'semantics', 'explicitly', 'guide', 'layers', 'propose', 'additional', 'prompting', 'cues', 'rhxwxc', 'tp', 'є', 'rkxc', 'h', 'w', 'denoting', 'resolution', 'image', 'feature', 'map', 'c', 'denoting', 'feature', 'dimension', 'encoded', 'image', 'prompts', 'fed', 'decoder', 'decм', 'feature', 'interaction', 'sam', 'constructs', 'input', 'tokens', 'decoder', 'concatenating', 'several', 'learnable', 'mask', 'tokens', 'tм', 'prefixes', 'prompt', 'tokens', 'tp', 'mask', 'tokens', 'responsible', 'generating', 'mask', 'output', 'formulated', 'decм', 'fi', 'concat', 'tm', 'tp', 'denotes', 'final', 'segmentation', 'mask', 'predicted', 'sam', 'task', 'definition', 'although', 'sam', 'generalized', 'enough', 'object', 'prompting', 'lacks', 'ability', 'automatically', 'segment', 'specific', 'subject', 'instances', 'considering', 'define', 'new', 'task', 'personalized', 'object', 'segmentation', 'user', 'provides', 'single', 'reference', 'image', 'mask', 'indicating', 'target', 'visual', 'concept', 'given', 'mask', 'either', 'accurate', 'segmentation', 'rough', 'sketch', 'drawn', 'goal', 'customize', 'sam', 'segment', 'designated', 'object', 'within', 'new', 'images', 'videos', 'without', 'additional', 'human', 'prompting', 'evaluation', 'annotate', 'new', 'dataset', 'personalized', 'segmentation', 'named', 'perseg', 'raw', 'images', 'collected', 'works', 'diffusion', 'models', 'gal', 'et', 'ruiz', 'et', 'kumari', 'et', 'containing', 'various', 'categories', 'visual', 'concepts', 'different', 'poses', 'scenes', 'paper', 'propose', 'two', 'efficient', 'solutions', 'task', 'specifically', 'illustrate', 'follows', 'persam', 'location', 'confidence', 'map', 'conditioned', 'image', 'ir', 'mask', 'mr', 'persam', 'first', 'obtains', 'confidence', 'map', 'indicates', 'location', 'target', 'object', 'new', 'test', 'image', 'shown', 'figure', 'apply', 'image', 'encoder', 'extract', 'visual', 'features', 'ir', 'encoder', 'sam', 'frozen', 'backbone', 'vision', 'models', 'adopt', 'sam', 'image', 'encoder', 'enc', 'default', 'formulate', 'process', 'enc', 'fr', 'enci', 'ir', 'fr', 'є', 'utilize', 'reference', 'mask', 'mr', 'є', 'crop', 'features', 'foreground', 'pixels', 'within', 'visual', 'concept', 'fr', 'resulting', 'set', 'n', 'local', 'features', 'mr', 'fr', 'є', 'denotes', 'multiplication', 'calculate', 'n', 'confidence', 'maps', 'foreground', 'pixel', 'cosine', 'similarity', 'test', 'image', 'feature', 'fį', 'rhxw', 'note', 'fi', 'represents', 'distribution', 'probability', 'different', 'local', 'part', 'object', 'test', 'image', 'head', 'body', 'paws', 'dog', 'top', 'adopt', 'average', 'pooling', 'aggregate', 'n', 'local', 'maps', 'obtain', 'overall', 'confidence', 'map', 'target', 'object', 'n', 'n', 'siєrhxw', 'incorporating', 'confidences', 'every', 'foreground', 'pixel', 'take', 'visual', 'appearance', 'different', 'object', 'parts', 'consideration', 'acquire', 'relatively', 'comprehensive', 'location', 'estimation', 'location', 'prior', 'provide', 'persam', 'location', 'prior', 'test', 'image', 'select', 'two', 'points', 'highest', 'lowest', 'confidence', 'values', 'denoted', 'pɲ', 'pɩ', 'respectively', 'former', 'represents', 'likely', 'center', 'position', 'target', 'object', 'latter', 'inversely', 'indicates', 'background', 'regarded', 'positive', 'negative', 'point', 'prompts', 'fed', 'prompt', 'encoder', 'tp', 'encp', 'ph', 'є', 'denote', 'prompt', 'tokens', 'sam', 'decoder', 'way', 'sam', 'would', 'tend', 'segment', 'contiguous', 'region', 'surrounding', 'positive', 'point', 'discarding', 'negative', 'one', 'image', 'attention', 'although', 'point', 'prompt', 'obtained', 'propose', 'explicit', 'semantic', 'guidance', 'operation', 'sam', 'decoder', 'concentrates', 'feature', 'aggregation', 'within', 'foreground', 'target', 'regions', 'shown', 'figure', 'overall', 'confidence', 'map', 'equation', 'clearly', 'indicate', 'rough', 'region', 'target', 'visual', 'concept', 'test', 'image', 'hotter', 'colors', 'indicate', 'higher', 'scores', 'based', 'property', 'utilize', 'guide', 'attention', 'map', 'every', 'layer', 'decoder', 'specifically', 'denote', 'every', 'attention', 'map', 'softmax', 'function', 'rhxw', 'modulate', 'attention', 'distribution', 'softmax', 'softmax', 'denotes', 'balancing', 'factor', 'attention', 'bias', 'mask', 'prompt', 'tokens', 'compelled', 'capture', 'visual', 'semantics', 'associated', 'target', 'subject', 'unimportant', 'background', 'area', 'contributes', 'effective', 'feature', 'aggregation', 'attention', 'mechanisms', 'enhances', 'final', 'segmentation', 'accuracy', 'persam', 'manner', 'prompting', 'vanilla', 'sam', 'receives', 'prompts', 'positional', 'information', 'coordinate', 'point', 'box', 'provide', 'sam', 'decoder', 'level', 'cues', 'propose', 'utilize', 'visual', 'feature', 'target', 'concept', 'additional', 'semantic', 'prompting', 'first', 'obtain', 'global', 'embedding', 'tr', 'object', 'reference', 'image', 'average', 'pooling', 'different', 'local', 'features', 'n', 'tr', 'στα', 'n', 'add', 'tr', 'input', 'tokens', 'test', 'image', 'equation', 'feeding', 'decoder', 'block', 'shown', 'figure', 'repeat', 'tr', 'concat', 'tm', 'tp', 'denotes', 'input', 'token', 'guided', 'target', 'semantics', 'decoder', 'decм', 'repeat', 'operation', 'duplicates', 'target', 'visual', 'embedding', 'aided', 'simple', 'token', 'incorporation', 'persam', 'prompted', 'location', 'points', 'also', 'target', 'visual', 'cues', 'test', 'image', 'persam', 'output', 'three', 'scales', 'random', 'noise', 'dreambooth', 'v', 'cat', 'reconstruction', 'loss', 'f', 'user', 'provides', 'output', 'mask', 'weighted', 'summation', 'freeze', 'persam', 'background', 'disturbance', 'decouple', 'figure', 'alleviate', 'scale', 'ambiguity', 'adopts', 'two', 'learnable', 'weights', 'adaptively', 'aggregating', 'masks', 'figure', 'dreambooth', 'utilize', 'persam', 'decouple', 'target', 'jects', 'background', 'improving', 'generation', 'dreambooth', 'cascaded', 'via', 'techniques', 'obtain', 'initial', 'segmentation', 'mask', 'test', 'image', 'sam', 'decoder', 'however', 'might', 'include', 'rough', 'edges', 'isolated', 'background', 'noises', 'refinement', 'iteratively', 'feed', 'mask', 'back', 'decoder', 'decм', 'first', 'step', 'prompt', 'decoder', 'currently', 'predicted', 'mask', 'along', 'previous', 'point', 'prompt', 'second', 'step', 'acquire', 'bounding', 'box', 'enclosing', 'mask', 'first', 'step', 'prompt', 'decoder', 'additionally', 'box', 'accurate', 'object', 'localization', 'iterate', 'lightweight', 'decoder', 'without', 'image', 'encoder', 'efficient', 'costs', 'extra', 'latency', 'ambiguity', 'segmentation', 'scales', 'persam', 'tackle', 'cases', 'tory', 'segmentation', 'accuracy', 'however', 'target', 'objects', 'contain', 'hierarchical', 'structures', 'leads', 'ambiguity', 'mask', 'scales', 'shown', 'figure', 'teapot', 'top', 'platform', 'comprised', 'two', 'parts', 'lid', 'body', 'positive', 'point', 'prompt', 'denoted', 'green', 'pentagram', 'located', 'body', 'negative', 'prompt', 'denoted', 'red', 'pentagram', 'exclude', 'platform', 'similar', 'color', 'persam', 'would', 'misled', 'segmentation', 'issue', 'also', 'discussed', 'sam', 'kirillov', 'et', 'proposes', 'alternative', 'simultaneously', 'generate', 'multiple', 'masks', 'three', 'scales', 'corresponding', 'whole', 'part', 'subpart', 'object', 'user', 'required', 'manually', 'select', 'one', 'mask', 'three', 'effective', 'consumes', 'extra', 'manpower', 'contrast', 'personalized', 'task', 'aims', 'customize', 'sam', 'automatic', 'object', 'segmentation', 'without', 'need', 'human', 'prompting', 'motivates', 'us', 'develop', 'version', 'persam', 'adaptive', 'segmentation', 'appropriate', 'scale', 'introduce', 'variant', 'unlike', 'model', 'producing', 'one', 'mask', 'first', 'follows', 'persam', 'obtain', 'location', 'prior', 'refers', 'sam', 'original', 'solution', 'output', 'masks', 'denoted', 'respectively', 'top', 'adopt', 'two', 'learnable', 'mask', 'weights', 'calculate', 'final', 'mask', 'output', 'weighted', 'summation', 'initialized', 'learn', 'optimal', 'weights', 'conduct', 'tuning', 'reference', 'image', 'regard', 'given', 'mask', 'ground', 'truth', 'note', 'freeze', 'entire', 'sam', 'model', 'preserve', 'knowledge', 'parameters', 'within', 'seconds', 'single', 'gpu', 'way', 'efficiently', 'learns', 'semantics', 'objects', 'adaptively', 'outputs', 'best', 'segmentation', 'scale', 'different', 'concepts', 'improving', 'generalization', 'capacity', 'persam', 'table', 'personalized', 'object', 'segmentation', 'perseg', 'dataset', 'compare', 'overall', 'miou', 'blou', 'learnable', 'parameters', 'different', 'methods', 'bar', 'et', 'wang', 'et', 'zou', 'et', 'along', 'miou', 'objects', 'perseg', 'denotes', 'works', 'concurrent', 'method', 'miou', 'blou', 'param', 'barn', 'clock', 'cat', 'teddy', 'duck', 'thin', 'red', 'pack', 'bear', 'toy', 'bird', 'cartoon', 'robot', 'toy', 'painter', 'vp', 'seem', 'seggpt', 'persam', 'table', 'video', 'object', 'tation', 'davis', 'val', 'tuset', 'et', 'utilize', 'gray', 'color', 'denote', 'methods', 'involving', 'training', 'table', 'semantic', 'part', 'segmentation', 'li', 'et', 'gupta', 'et', 'part', 'morabia', 'et', 'ramanathan', 'et', 'report', 'miou', 'scores', 'utilize', 'gray', 'color', 'denote', 'methods', 'involving', 'training', 'semantic', 'seg', 'part', 'seg', 'painter', 'seem', 'seggpt', 'persam', 'method', 'j', 'f', 'і', 'agss', 'f', 'method', 'hsnet', 'vat', 'painter', 'seggpt', 'persam', 'dreambooth', 'personalized', 'synthesis', 'dreambooth', 'ruiz', 'et', 'diffusion', 'model', 'given', 'photos', 'specific', 'object', 'pet', 'cat', 'learns', 'generate', 'cat', 'referred', 'text', 'prompt', 'v', 'cat', 'calculates', 'loss', 'entire', 'reconstructed', 'images', 'would', 'inject', 'redundant', 'background', 'information', 'training', 'images', 'identifier', 'v', 'therefore', 'shown', 'figure', 'introduce', 'strategy', 'alleviate', 'disturbance', 'backgrounds', 'dreambooth', 'given', 'object', 'mask', 'images', 'leverage', 'persam', 'segment', 'foreground', 'targets', 'discard', 'gradient', 'pixels', 'belonging', 'background', 'area', 'stable', 'diffusion', 'memorize', 'visual', 'appearances', 'target', 'object', 'supervision', 'imposed', 'background', 'assisted', 'dreambooth', 'synthesize', 'target', 'object', 'better', 'visual', 'correspondence', 'also', 'increase', 'diversity', 'new', 'backgrounds', 'guided', 'input', 'text', 'prompt', 'experiment', 'first', 'evaluate', 'approach', 'personalized', 'segmentation', 'perseg', 'section', 'along', 'various', 'existing', 'segmentation', 'benchmarks', 'section', 'illustrate', 'effectiveness', 'dreambooth', 'section', 'finally', 'conduct', 'several', 'ablation', 'studies', 'investigate', 'designs', 'perseg', 'section', 'personalized', 'evaluation', 'perseg', 'dataset', 'test', 'personalization', 'capacity', 'construct', 'new', 'segmentation', 'dataset', 'termed', 'perseg', 'raw', 'images', 'collected', 'training', 'data', 'diffusion', 'works', 'ruiz', 'et', 'gal', 'et', 'kumari', 'et', 'perseg', 'contains', 'objects', 'various', 'categories', 'total', 'including', 'daily', 'necessities', 'animals', 'buildings', 'different', 'poses', 'scenes', 'object', 'associated', 'images', 'masks', 'fix', 'one', 'pair', 'data', 'miou', 'blou', 'cheng', 'et', 'adopted', 'evaluation', 'please', 'refer', 'appendix', 'implementation', 'details', 'enlarged', 'data', 'scale', 'perseg', 'ring', 'clock', 'teapot', 'tray', 'backpack', 'carried', 'top', 'part', 'woman', 'figure', 'visualization', 'provement', 'well', 'alleviate', 'scale', 'ambiguity', 'persam', 'figure', 'visualization', 'video', 'object', 'mentation', 'approach', 'performs', 'well', 'segmenting', 'multiple', 'objects', 'video', 'performance', 'table', 'observe', 'achieves', 'best', 'results', 'effectively', 'enhances', 'persam', 'overall', 'miou', 'biou', 'show', 'tion', 'improvement', 'figure', 'visual', 'prompting', 'vp', 'bar', 'et', 'painter', 'wang', 'et', 'seem', 'zou', 'et', 'seggpt', 'wang', 'et', 'learners', 'also', 'segment', 'objects', 'according', 'given', 'prompt', 'data', 'shown', 'persam', 'already', 'achieve', 'better', 'performance', 'painter', 'vp', 'seem', 'different', 'margins', 'efficient', 'surpasses', 'powerful', 'seggpt', 'overall', 'miou', 'biou', 'different', 'motivations', 'develop', 'segmentation', 'generalists', 'method', 'specially', 'designed', 'personalized', 'object', 'segmentation', 'exhibits', 'much', 'efficiency', 'time', 'computational', 'resources', 'existing', 'segmentation', 'benchmarks', 'video', 'object', 'segmentation', 'given', 'image', 'object', 'masks', 'persam', 'achieve', 'competitive', 'object', 'segmentation', 'tracking', 'performance', 'validation', 'set', 'davis', 'et', 'shown', 'table', 'compared', 'methods', 'without', 'video', 'training', 'persam', 'largely', 'surpasses', 'painter', 'j', 'f', 'score', 'achieve', 'better', 'performance', 'seggpt', 'notably', 'approach', 'outperform', 'methods', 'lin', 'et', 'liang', 'et', 'fully', 'trained', 'extensive', 'video', 'data', 'results', 'fully', 'illustrate', 'strong', 'generalization', 'ability', 'temporal', 'video', 'data', 'complex', 'scenarios', 'contain', 'multiple', 'similar', 'occluded', 'objects', 'visualized', 'figure', 'semantic', 'part', 'segmentation', 'table', 'evaluate', 'approach', 'image', 'segmentation', 'respectively', 'four', 'datasets', 'li', 'et', 'gupta', 'et', 'morabia', 'et', 'ramanathan', 'et', 'follow', 'matcher', 'liu', 'et', 'data', 'evaluation', 'shown', 'attains', 'consistently', 'better', 'results', 'painter', 'performs', 'comparably', 'seggpt', 'models', 'min', 'et', 'hong', 'et', 'training', 'approach', 'achieve', 'higher', 'scores', 'hsnet', 'experiments', 'well', 'demonstrate', 'proposed', 'approach', 'limited', 'segmentation', 'also', 'works', 'personalization', 'sam', 'dreambooth', 'follow', 'hyperparameters', 'dreambooth', 'ruiz', 'et', 'stable', 'diffusion', 'rombach', 'et', 'personalized', 'image', 'synthesis', 'addition', 'figure', 'visualize', 'examples', 'dreambooth', 'figure', 'dog', 'lying', 'grey', 'sofa', 'jungle', 'snow', 'dreambooth', 'still', 'sofa', 'green', 'white', 'decorations', 'assisted', 'background', 'totally', 'decoupled', 'sofa', 'well', 'corresponds', 'textual', 'prompt', 'barn', 'front', 'mountains', 'approach', 'also', 'alleviates', 'background', 'disturbance', 'correctly', 'generate', 'forest', 'blue', 'sky', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'photo', 'dog', 'v', 'dog', 'jungle', 'photo', 'barn', 'v', 'barn', 'forest', 'background', 'v', 'dog', 'snow', 'v', 'barn', 'blue', 'sky', 'background', 'figure', 'visualization', 'dreambooth', 'improved', 'dreambooth', 'ruiz', 'et', 'better', 'preserve', 'diversity', 'synthesizing', 'various', 'contexts', 'new', 'images', 'table', 'ablation', 'main', 'ponents', 'proposed', 'method', 'variant', 'table', 'ablation', 'different', 'methods', 'table', 'ablation', 'using', 'reference', 'miou', 'gain', 'method', 'persam', 'param', 'miou', 'method', 'mask', 'box', 'painter', 'prompt', 'tuning', 'vp', 'adapter', 'seem', 'guided', 'attention', 'semantic', 'prompt', 'lora', 'seggpt', 'mask', 'weights', 'scale', 'tuning', 'persam', 'positive', 'prior', 'negative', 'prior', 'ablation', 'study', 'main', 'components', 'table', 'investigate', 'different', 'components', 'starting', 'baseline', 'adopts', 'positive', 'location', 'prior', 'add', 'negative', 'point', 'prompt', 'cascaded', 'enhancing', 'miou', 'respectively', 'top', 'introduce', 'target', 'semantics', 'sam', 'decoder', 'attention', 'guidance', 'semantic', 'prompting', 'resulting', 'improvements', 'fully', 'indicate', 'significance', 'finally', 'via', 'efficient', 'boosts', 'score', 'demonstrating', 'superior', 'accuracy', 'different', 'methods', 'table', 'experiment', 'peft', 'methods', 'prompt', 'tuning', 'liu', 'et', 'adapter', 'houlsby', 'et', 'lora', 'hu', 'et', 'freeze', 'entire', 'sam', 'tune', 'peft', 'modules', 'injected', 'every', 'transformer', 'block', 'persam', 'decoder', 'shown', 'prompt', 'tuning', 'adapter', 'would', 'data', 'severely', 'degrade', 'accuracy', 'instead', 'best', 'improve', 'performance', 'persam', 'tuning', 'least', 'learnable', 'parameters', 'using', 'reference', 'requiring', 'accurate', 'mask', 'data', 'might', 'strict', 'users', 'table', 'relax', 'input', 'restrictions', 'bounding', 'box', 'designating', 'expected', 'object', 'method', 'regard', 'box', 'prompt', 'utilize', 'sam', 'generate', 'mask', 'therefore', 'box', 'reference', 'leads', 'marginal', 'performance', 'drop', 'persam', 'severely', 'influences', 'methods', 'conclusion', 'paper', 'propose', 'personalize', 'segment', 'anything', 'model', 'sam', 'specific', 'visual', 'concepts', 'data', 'firstly', 'introduce', 'persam', 'injects', 'target', 'semantics', 'sam', 'techniques', 'top', 'present', 'variant', 'learnable', 'parameters', 'effectively', 'alleviates', 'ambiguity', 'mask', 'scales', 'achieves', 'leading', 'performance', 'various', 'benchmarks', 'besides', 'also', 'verify', 'efficacy', 'approach', 'assist', 'dreambooth', 'better', 'diffusion', 'models', 'hope', 'work', 'may', 'expand', 'applicability', 'sam', 'wider', 'range', 'scenarios', 'references', 'vijay', 'badrinarayanan', 'alex', 'kendall', 'roberto', 'cipolla', 'segnet', 'deep', 'convolutional', 'decoder', 'architecture', 'image', 'segmentation', 'ieee', 'transactions', 'pattern', 'analysis', 'machine', 'intelligence', 'amir', 'bar', 'yossi', 'gandelsman', 'trevor', 'darrell', 'amir', 'globerson', 'alexei', 'efros', 'visual', 'prompting', 'via', 'image', 'inpainting', 'advances', 'neural', 'information', 'processing', 'systems', 'tom', 'brown', 'benjamin', 'mann', 'nick', 'ryder', 'melanie', 'subbiah', 'jared', 'kaplan', 'prafulla', 'dhariwal', 'arvind', 'neelakantan', 'pranav', 'shyam', 'girish', 'sastry', 'amanda', 'askell', 'et', 'al', 'language', 'models', 'learners', 'advances', 'neural', 'information', 'processing', 'systems', 'jiazhong', 'cen', 'zanwei', 'zhou', 'jiemin', 'fang', 'wei', 'shen', 'lingxi', 'xie', 'xiaopeng', 'zhang', 'qi', 'tian', 'segment', 'anything', 'nerfs', 'arxiv', 'preprint', 'chen', 'george', 'papandreou', 'iasonas', 'kokkinos', 'kevin', 'murphy', 'alan', 'l', 'yuille', 'deeplab', 'semantic', 'image', 'segmentation', 'deep', 'convolutional', 'nets', 'atrous', 'convolution', 'fully', 'connected', 'crfs', 'ieee', 'transactions', 'pattern', 'analysis', 'machine', 'intelligence', 'xi', 'chen', 'zhiyan', 'zhao', 'feiwu', 'yu', 'yilei', 'zhang', 'manni', 'duan', 'conditional', 'diffusion', 'interactive', 'segmentation', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'zhe', 'chen', 'yuchen', 'duan', 'wenhai', 'wang', 'junjun', 'tong', 'lu', 'jifeng', 'dai', 'yu', 'qiao', 'vision', 'transformer', 'adapter', 'dense', 'predictions', 'arxiv', 'preprint', 'bowen', 'cheng', 'ross', 'girshick', 'piotr', 'dollár', 'alexander', 'c', 'berg', 'alexander', 'kirillov', 'boundary', 'iou', 'improving', 'image', 'segmentation', 'evaluation', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'bowen', 'cheng', 'ishan', 'misra', 'alexander', 'g', 'schwing', 'alexander', 'kirillov', 'rohit', 'girdhar', 'attention', 'mask', 'transformer', 'universal', 'image', 'segmentation', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'pedro', 'cuenca', 'sayak', 'paul', 'using', 'lora', 'efficient', 'stable', 'diffusion', 'https', 'hugging', 'january', 'jacob', 'devlin', 'chang', 'kenton', 'lee', 'kristina', 'toutanova', 'bert', 'deep', 'bidirectional', 'transformers', 'language', 'understanding', 'arxiv', 'preprint', 'rinon', 'gal', 'yuval', 'alaluf', 'yuval', 'atzmon', 'patashnik', 'amit', 'h', 'bermano', 'gal', 'chechik', 'daniel', 'image', 'worth', 'one', 'word', 'personalizing', 'generation', 'using', 'textual', 'inversion', 'arxiv', 'preprint', 'agrim', 'gupta', 'piotr', 'dollar', 'ross', 'girshick', 'lvis', 'dataset', 'large', 'vocabulary', 'instance', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yuying', 'hao', 'yi', 'liu', 'zewu', 'wu', 'lin', 'han', 'yizhou', 'chen', 'guowei', 'chen', 'lutao', 'chu', 'shiyu', 'tang', 'zhiliang', 'yu', 'zeyu', 'chen', 'et', 'al', 'edgeflow', 'achieving', 'practical', 'interactive', 'segmentation', 'flow', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'junxian', 'chunting', 'zhou', 'xuezhe', 'taylor', 'graham', 'neubig', 'towards', 'unified', 'view', 'transfer', 'learning', 'international', 'conference', 'learning', 'representations', 'url', 'https', 'kaiming', 'georgia', 'gkioxari', 'piotr', 'dollár', 'ross', 'girshick', 'mask', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'lukas', 'hedegaard', 'aman', 'alok', 'juby', 'jose', 'alexandros', 'iosifidis', 'structured', 'pruning', 'adapters', 'arxiv', 'preprint', 'sunghwan', 'hong', 'seokju', 'cho', 'jisu', 'nam', 'stephen', 'lin', 'seungryong', 'kim', 'cost', 'aggregation', 'convolutional', 'swin', 'transformer', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'neil', 'houlsby', 'andrei', 'giurgiu', 'stanislaw', 'jastrzebski', 'bruna', 'morrone', 'quentin', 'de', 'laroussilhe', 'andrea', 'gesmundo', 'mona', 'attariyan', 'sylvain', 'gelly', 'transfer', 'learning', 'nlp', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'edward', 'j', 'hu', 'yelong', 'shen', 'phillip', 'wallis', 'zeyuan', 'yuanzhi', 'li', 'shean', 'wang', 'lu', 'wang', 'weizhu', 'chen', 'lora', 'adaptation', 'large', 'language', 'models', 'arxiv', 'preprint', 'yuhao', 'huang', 'xin', 'yang', 'lian', 'liu', 'han', 'zhou', 'ao', 'chang', 'xinrui', 'zhou', 'rusi', 'chen', 'junxuan', 'yu', 'jiongquan', 'chen', 'chaoyu', 'chen', 'et', 'al', 'segment', 'anything', 'model', 'medical', 'images', 'arxiv', 'preprint', 'chao', 'jia', 'yinfei', 'yang', 'ye', 'xia', 'chen', 'zarana', 'parekh', 'hieu', 'pham', 'quoc', 'le', 'sung', 'zhen', 'li', 'tom', 'duerig', 'scaling', 'visual', 'representation', 'learning', 'noisy', 'text', 'supervision', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'menglin', 'jia', 'luming', 'tang', 'chen', 'claire', 'cardie', 'serge', 'belongie', 'bharath', 'hariharan', 'lim', 'visual', 'prompt', 'tuning', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'zhengkai', 'jiang', 'yuxi', 'li', 'ceyuan', 'yang', 'peng', 'gao', 'yabiao', 'wang', 'ying', 'tai', 'chengjie', 'wang', 'totypical', 'contrast', 'adaptation', 'domain', 'adaptive', 'semantic', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'zhengkai', 'jiang', 'zhangxuan', 'gu', 'jinlong', 'peng', 'hang', 'zhou', 'liang', 'liu', 'yabiao', 'wang', 'ying', 'tai', 'chengjie', 'wang', 'liqing', 'zhang', 'stc', 'contrastive', 'learning', 'video', 'instance', 'segmentation', 'european', 'conference', 'computer', 'vision', 'workshops', 'pp', 'springer', 'lei', 'ke', 'mingqiao', 'ye', 'martin', 'danelljan', 'yifan', 'liu', 'tai', 'tang', 'fisher', 'yu', 'segment', 'anything', 'high', 'quality', 'arxiv', 'preprint', 'alexander', 'kirillov', 'kaiming', 'ross', 'girshick', 'carsten', 'rother', 'piotr', 'dollár', 'panoptic', 'tation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'alexander', 'kirillov', 'eric', 'mintun', 'nikhila', 'ravi', 'hanzi', 'mao', 'chloe', 'rolland', 'laura', 'gustafson', 'tete', 'xiao', 'spencer', 'whitehead', 'alexander', 'c', 'berg', 'lo', 'et', 'al', 'segment', 'anything', 'arxiv', 'preprint', 'nupur', 'kumari', 'bingliang', 'zhang', 'richard', 'zhang', 'eli', 'shechtman', 'zhu', 'customization', 'diffusion', 'arxiv', 'preprint', 'brian', 'lester', 'rami', 'noah', 'constant', 'power', 'scale', 'prompt', 'tuning', 'arxiv', 'preprint', 'hao', 'li', 'jinguo', 'zhu', 'xiaohu', 'jiang', 'xizhou', 'zhu', 'hongsheng', 'li', 'chun', 'yuan', 'xiaohua', 'wang', 'yu', 'qiao', 'xiaogang', 'wang', 'wenhai', 'wang', 'et', 'al', 'generalist', 'model', 'vision', 'tasks', 'arxiv', 'preprint', 'junnan', 'li', 'dongxu', 'li', 'silvio', 'savarese', 'steven', 'hoi', 'bootstrapping', 'training', 'frozen', 'image', 'encoders', 'large', 'language', 'models', 'arxiv', 'preprint', 'xiang', 'li', 'tianhan', 'wei', 'yau', 'pun', 'chen', 'tai', 'tang', 'dataset', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yanwei', 'li', 'xinze', 'chen', 'zheng', 'zhu', 'lingxi', 'xie', 'guan', 'huang', 'dalong', 'du', 'xingang', 'wang', 'unified', 'network', 'panoptic', 'segmentation', 'proceedings', 'ieee', 'ence', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yongqing', 'liang', 'xin', 'li', 'navid', 'jafari', 'jim', 'chen', 'video', 'object', 'segmentation', 'adaptive', 'feature', 'bank', 'refinement', 'advances', 'neural', 'information', 'processing', 'systems', 'huaijia', 'lin', 'xiaojuan', 'qi', 'jiaya', 'jia', 'attention', 'guided', 'video', 'object', 'segmentation', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'zhaojiang', 'lin', 'andrea', 'madotto', 'pascale', 'fung', 'exploring', 'versatile', 'generative', 'language', 'model', 'via', 'transfer', 'learning', 'arxiv', 'preprint', 'ziyi', 'lin', 'shijie', 'geng', 'renrui', 'zhang', 'peng', 'gao', 'gerard', 'de', 'melo', 'xiaogang', 'wang', 'jifeng', 'dai', 'yu', 'qiao', 'hongsheng', 'li', 'frozen', 'clip', 'models', 'efficient', 'video', 'learners', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'xiao', 'liu', 'kaixuan', 'ji', 'yicheng', 'fu', 'weng', 'lam', 'tam', 'zhengxiao', 'du', 'zhilin', 'yang', 'jie', 'tang', 'prompt', 'tuning', 'comparable', 'universally', 'across', 'scales', 'tasks', 'arxiv', 'preprint', 'yang', 'liu', 'muzhi', 'zhu', 'hengtao', 'li', 'hao', 'chen', 'xinlong', 'wang', 'chunhua', 'shen', 'matcher', 'segment', 'anything', 'one', 'shot', 'using', 'feature', 'matching', 'arxiv', 'preprint', 'jonathan', 'long', 'evan', 'shelhamer', 'trevor', 'darrell', 'fully', 'convolutional', 'networks', 'semantic', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'jiasen', 'lu', 'dhruv', 'batra', 'devi', 'parikh', 'stefan', 'lee', 'vilbert', 'pretraining', 'visiolinguistic', 'representations', 'tasks', 'advances', 'neural', 'information', 'processing', 'systems', 'neurips', 'pp', 'jun', 'bo', 'wang', 'segment', 'anything', 'medical', 'images', 'arxiv', 'preprint', 'juhong', 'min', 'dahyun', 'kang', 'minsu', 'cho', 'hypercorrelation', 'squeeze', 'segmentation', 'proceedings', 'international', 'conference', 'computer', 'vision', 'pp', 'keval', 'morabia', 'jatin', 'arora', 'tara', 'vijaykumar', 'joint', 'detection', 'object', 'semantic', 'part', 'arxiv', 'preprint', 'openai', 'technical', 'report', 'arxiv', 'jonas', 'pfeiffer', 'aishwarya', 'kamath', 'andreas', 'rücklé', 'kyunghyun', 'cho', 'iryna', 'gurevych', 'fusion', 'task', 'composition', 'transfer', 'learning', 'arxiv', 'preprint', 'jordi', 'federico', 'perazzi', 'sergi', 'caelles', 'pablo', 'arbeláez', 'alex', 'luc', 'van', 'gool', 'davis', 'challenge', 'video', 'object', 'segmentation', 'arxiv', 'preprint', 'guanghui', 'qin', 'jason', 'eisner', 'learning', 'ask', 'querying', 'lms', 'mixtures', 'soft', 'prompts', 'arxiv', 'preprint', 'alec', 'radford', 'karthik', 'narasimhan', 'improving', 'language', 'understanding', 'generative', 'alec', 'radford', 'jeffrey', 'wu', 'rewon', 'child', 'david', 'luan', 'dario', 'amodei', 'ilya', 'sutskever', 'et', 'al', 'language', 'models', 'unsupervised', 'multitask', 'learners', 'openai', 'blog', 'alec', 'radford', 'jong', 'wook', 'kim', 'chris', 'hallacy', 'aditya', 'ramesh', 'gabriel', 'goh', 'sandhini', 'agarwal', 'girish', 'sastry', 'amanda', 'askell', 'pamela', 'mishkin', 'jack', 'clark', 'et', 'al', 'learning', 'transferable', 'visual', 'models', 'natural', 'language', 'supervision', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'vignesh', 'ramanathan', 'anmol', 'kalia', 'vladan', 'petrovic', 'yi', 'wen', 'baixue', 'zheng', 'baishan', 'guo', 'rui', 'wang', 'aaron', 'marquez', 'rama', 'kovvuri', 'abhishek', 'kadian', 'et', 'al', 'paco', 'parts', 'attributes', 'common', 'objects', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'rebuffi', 'hakan', 'bilen', 'andrea', 'vedaldi', 'learning', 'multiple', 'visual', 'domains', 'residual', 'adapters', 'advances', 'neural', 'information', 'processing', 'systems', 'robin', 'rombach', 'andreas', 'blattmann', 'dominik', 'lorenz', 'patrick', 'esser', 'björn', 'ommer', 'resolution', 'image', 'synthesis', 'latent', 'diffusion', 'models', 'proceedings', 'ence', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'nataniel', 'ruiz', 'yuanzhen', 'li', 'varun', 'jampani', 'yael', 'pritch', 'michael', 'rubinstein', 'kfir', 'aberman', 'dreambooth', 'fine', 'tuning', 'diffusion', 'models', 'generation', 'arxiv', 'preprint', 'chitwan', 'saharia', 'william', 'chan', 'saurabh', 'saxena', 'lala', 'li', 'jay', 'whang', 'emily', 'l', 'denton', 'kamyar', 'ghasemipour', 'raphael', 'gontijo', 'lopes', 'burcu', 'karagol', 'ayan', 'tim', 'salimans', 'et', 'al', 'photorealistic', 'diffusion', 'models', 'deep', 'language', 'understanding', 'advances', 'neural', 'information', 'processing', 'systems', 'lin', 'song', 'yanwei', 'li', 'zhengkai', 'jiang', 'zeming', 'li', 'xiangyu', 'zhang', 'hongbin', 'sun', 'jian', 'sun', 'nanning', 'zheng', 'rethinking', 'learnable', 'tree', 'filter', 'generic', 'feature', 'transform', 'advances', 'neural', 'information', 'processing', 'systems', 'sung', 'jaemin', 'cho', 'mohit', 'bansal', 'transfer', 'learning', 'tasks', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'zhi', 'tian', 'chunhua', 'shen', 'hao', 'chen', 'conditional', 'convolutions', 'instance', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'hugo', 'touvron', 'thibaut', 'lavril', 'gautier', 'izacard', 'xavier', 'martinet', 'lachaux', 'timothée', 'lacroix', 'baptiste', 'rozière', 'naman', 'goyal', 'eric', 'hambro', 'faisal', 'azhar', 'aurelien', 'rodriguez', 'armand', 'joulin', 'edouard', 'grave', 'guillaume', 'lample', 'llama', 'open', 'efficient', 'foundation', 'language', 'models', 'arxiv', 'preprint', 'xinlong', 'wang', 'rufeng', 'zhang', 'tao', 'kong', 'lei', 'li', 'chunhua', 'shen', 'dynamic', 'fast', 'instance', 'segmentation', 'advances', 'neural', 'information', 'processing', 'systems', 'xinlong', 'wang', 'wen', 'wang', 'yue', 'cao', 'chunhua', 'shen', 'tiejun', 'huang', 'images', 'speak', 'images', 'generalist', 'painter', 'visual', 'learning', 'arxiv', 'preprint', 'xinlong', 'wang', 'xiaosong', 'zhang', 'yue', 'cao', 'wen', 'wang', 'chunhua', 'shen', 'tiejun', 'huang', 'seggpt', 'segmenting', 'everything', 'context', 'arxiv', 'preprint', 'enze', 'xie', 'wenhai', 'wang', 'zhiding', 'yu', 'anima', 'anandkumar', 'jose', 'alvarez', 'ping', 'luo', 'segformer', 'simple', 'efficient', 'design', 'semantic', 'segmentation', 'transformers', 'advances', 'neural', 'information', 'processing', 'systems', 'mutian', 'xu', 'junhao', 'zhang', 'zhipeng', 'zhou', 'mingye', 'xu', 'xiaojuan', 'qi', 'yu', 'qiao', 'learning', 'representation', 'complementary', 'understanding', 'object', 'point', 'cloud', 'proceedings', 'aaai', 'conference', 'artificial', 'intelligence', 'volume', 'pp', 'jinyu', 'yang', 'mingqi', 'gao', 'zhe', 'li', 'shang', 'gao', 'fangjing', 'wang', 'feng', 'zheng', 'track', 'anything', 'segment', 'anything', 'meets', 'videos', 'arxiv', 'preprint', 'chaoning', 'zhang', 'dongshen', 'han', 'yu', 'qiao', 'jung', 'uk', 'kim', 'bae', 'seungkyu', 'lee', 'choong', 'seon', 'hong', 'faster', 'segment', 'anything', 'towards', 'lightweight', 'sam', 'mobile', 'applications', 'arxiv', 'preprint', 'qingru', 'zhang', 'minshuo', 'chen', 'alexander', 'bukharin', 'pengcheng', 'yu', 'cheng', 'weizhu', 'chen', 'tuo', 'zhao', 'adaptive', 'budget', 'allocation', 'arxiv', 'preprint', 'renrui', 'zhang', 'jiaming', 'han', 'aojun', 'zhou', 'xiangfei', 'hu', 'shilin', 'yan', 'pan', 'lu', 'hongsheng', 'li', 'peng', 'gao', 'yu', 'qiao', 'efficient', 'language', 'models', 'attention', 'arxiv', 'preprint', 'renrui', 'zhang', 'xiangfei', 'hu', 'bohao', 'li', 'siyuan', 'huang', 'hanqiu', 'deng', 'hongsheng', 'li', 'yu', 'qiao', 'peng', 'gao', 'prompt', 'generate', 'cache', 'cascade', 'foundation', 'models', 'makes', 'strong', 'learners', 'arxiv', 'preprint', 'hengshuang', 'zhao', 'jianping', 'shi', 'xiaojuan', 'qi', 'xiaogang', 'wang', 'jiaya', 'jia', 'pyramid', 'scene', 'parsing', 'network', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'xu', 'zhao', 'wenchao', 'ding', 'yongqi', 'yinglong', 'du', 'tao', 'yu', 'min', 'li', 'ming', 'tang', 'jinqiao', 'wang', 'fast', 'segment', 'anything', 'arxiv', 'preprint', 'sixiao', 'zheng', 'jiachen', 'lu', 'hengshuang', 'zhao', 'xiatian', 'zhu', 'zekun', 'luo', 'yabiao', 'wang', 'yanwei', 'fu', 'jianfeng', 'feng', 'tao', 'xiang', 'philip', 'hs', 'torr', 'et', 'al', 'rethinking', 'semantic', 'segmentation', 'perspective', 'transformers', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'kaiyang', 'zhou', 'jingkang', 'yang', 'chen', 'change', 'loy', 'ziwei', 'liu', 'learning', 'prompt', 'language', 'models', 'international', 'journal', 'computer', 'vision', 'xueyan', 'zou', 'jianwei', 'yang', 'hao', 'zhang', 'feng', 'li', 'linjie', 'li', 'jianfeng', 'gao', 'yong', 'jae', 'lee', 'segment', 'everything', 'everywhere', 'arxiv', 'preprint']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oct',\n",
       " 'personalize',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'renrui',\n",
       " 'zhengkai',\n",
       " 'ziyu',\n",
       " 'shilin',\n",
       " 'junting',\n",
       " 'xianzheng',\n",
       " 'hao',\n",
       " 'dongª',\n",
       " 'yu',\n",
       " 'peng',\n",
       " 'hongsheng',\n",
       " 'mmlab',\n",
       " 'shanghai',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'laboratory',\n",
       " 'institute',\n",
       " 'automation',\n",
       " 'chinese',\n",
       " 'academy',\n",
       " 'science',\n",
       " 'school',\n",
       " 'c',\n",
       " 'peking',\n",
       " 'university',\n",
       " 'zhangrenrui',\n",
       " 'gaopeng',\n",
       " 'guoziyu',\n",
       " 'qiaoyu',\n",
       " 'hsli',\n",
       " 'abstract',\n",
       " 'driven',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'demonstrated',\n",
       " 'powerful',\n",
       " 'promptable',\n",
       " 'framework',\n",
       " 'revolutionizing',\n",
       " 'tion',\n",
       " 'field',\n",
       " 'despite',\n",
       " 'generality',\n",
       " 'customizing',\n",
       " 'sam',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'without',\n",
       " 'prompting',\n",
       " 'automatically',\n",
       " 'segmenting',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'numerous',\n",
       " 'image',\n",
       " 'paper',\n",
       " 'introduce',\n",
       " 'personalization',\n",
       " 'approach',\n",
       " 'sam',\n",
       " 'termed',\n",
       " 'persam',\n",
       " 'given',\n",
       " 'data',\n",
       " 'single',\n",
       " 'image',\n",
       " 'reference',\n",
       " 'mask',\n",
       " 'first',\n",
       " 'obtain',\n",
       " 'cation',\n",
       " 'prior',\n",
       " 'target',\n",
       " 'concept',\n",
       " 'new',\n",
       " 'image',\n",
       " 'aided',\n",
       " 'target',\n",
       " 'visual',\n",
       " 'semantics',\n",
       " 'empower',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'via',\n",
       " 'two',\n",
       " 'posed',\n",
       " 'technique',\n",
       " 'attention',\n",
       " 'prompting',\n",
       " 'way',\n",
       " 'effectively',\n",
       " 'customize',\n",
       " 'sam',\n",
       " 'private',\n",
       " 'use',\n",
       " 'without',\n",
       " 'training',\n",
       " 'alleviate',\n",
       " 'ambiguity',\n",
       " 'segmentation',\n",
       " 'scale',\n",
       " 'present',\n",
       " 'efficient',\n",
       " 'variant',\n",
       " 'freezing',\n",
       " 'entire',\n",
       " 'sam',\n",
       " 'introduce',\n",
       " 'aggregate',\n",
       " 'mask',\n",
       " 'tune',\n",
       " 'parameter',\n",
       " 'within',\n",
       " 'second',\n",
       " 'improved',\n",
       " 'performance',\n",
       " 'demonstrate',\n",
       " 'efficacy',\n",
       " 'construct',\n",
       " 'new',\n",
       " 'dataset',\n",
       " 'perseg',\n",
       " 'evaluation',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'also',\n",
       " 'test',\n",
       " 'method',\n",
       " 'various',\n",
       " 'image',\n",
       " 'video',\n",
       " 'segmentation',\n",
       " 'benchmark',\n",
       " 'besides',\n",
       " 'propose',\n",
       " 'leverage',\n",
       " 'persam',\n",
       " 'improve',\n",
       " 'dreambooth',\n",
       " 'personalized',\n",
       " 'synthesis',\n",
       " 'mitigating',\n",
       " 'disturbance',\n",
       " 'background',\n",
       " 'approach',\n",
       " 'showcase',\n",
       " 'better',\n",
       " 'target',\n",
       " 'appearance',\n",
       " 'generation',\n",
       " 'higher',\n",
       " 'fidelity',\n",
       " 'input',\n",
       " 'text',\n",
       " 'prompt',\n",
       " 'code',\n",
       " 'released',\n",
       " 'http',\n",
       " 'user',\n",
       " 'provides',\n",
       " 'learning',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'one',\n",
       " 'image',\n",
       " 'one',\n",
       " 'mask',\n",
       " 'persam',\n",
       " 'various',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'figure',\n",
       " 'personalization',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'customize',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'data',\n",
       " 'introduce',\n",
       " 'two',\n",
       " 'efficient',\n",
       " 'solution',\n",
       " 'persam',\n",
       " 'equal',\n",
       " 'contribution',\n",
       " 'corresponding',\n",
       " 'author',\n",
       " 'parameter',\n",
       " 'persam',\n",
       " 'user',\n",
       " 'provides',\n",
       " 'dreambooth',\n",
       " 'assisted',\n",
       " 'persam',\n",
       " 'second',\n",
       " 'hat',\n",
       " 'teddy',\n",
       " 'bear',\n",
       " 'photo',\n",
       " 'v',\n",
       " 'cat',\n",
       " 'v',\n",
       " 'cat',\n",
       " 'beach',\n",
       " 'various',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'body',\n",
       " 'robot',\n",
       " 'toy',\n",
       " 'figure',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'ples',\n",
       " 'persam',\n",
       " 'left',\n",
       " 'segment',\n",
       " 'personal',\n",
       " 'object',\n",
       " 'context',\n",
       " 'favorable',\n",
       " 'mance',\n",
       " 'right',\n",
       " 'alleviates',\n",
       " 'ambiguity',\n",
       " 'issue',\n",
       " 'introduction',\n",
       " 'photo',\n",
       " 'v',\n",
       " 'backpack',\n",
       " 'v',\n",
       " 'backpack',\n",
       " 'table',\n",
       " 'classroom',\n",
       " 'figure',\n",
       " 'improving',\n",
       " 'dreambooth',\n",
       " 'ruiz',\n",
       " 'et',\n",
       " 'persam',\n",
       " 'mitigating',\n",
       " 'bance',\n",
       " 'background',\n",
       " 'training',\n",
       " 'proach',\n",
       " 'help',\n",
       " 'achieve',\n",
       " 'alized',\n",
       " 'generation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'vision',\n",
       " 'li',\n",
       " 'et',\n",
       " 'zou',\n",
       " 'et',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'language',\n",
       " 'brown',\n",
       " 'et',\n",
       " 'touvron',\n",
       " 'et',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'jia',\n",
       " 'et',\n",
       " 'li',\n",
       " 'et',\n",
       " 'gained',\n",
       " 'unprecedented',\n",
       " 'prevalence',\n",
       " 'attributed',\n",
       " 'availability',\n",
       " 'datasets',\n",
       " 'computational',\n",
       " 'resource',\n",
       " 'demonstrate',\n",
       " 'extraordinary',\n",
       " 'generalization',\n",
       " 'capacity',\n",
       " 'scenario',\n",
       " 'display',\n",
       " 'versatile',\n",
       " 'interactivity',\n",
       " 'incorporating',\n",
       " 'human',\n",
       " 'feedback',\n",
       " 'inspired',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'develops',\n",
       " 'delicate',\n",
       " 'data',\n",
       " 'engine',\n",
       " 'collecting',\n",
       " 'data',\n",
       " 'subsequently',\n",
       " 'train',\n",
       " 'segmentation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'known',\n",
       " 'sam',\n",
       " 'defines',\n",
       " 'novel',\n",
       " 'promptable',\n",
       " 'segmentation',\n",
       " 'framework',\n",
       " 'taking',\n",
       " 'input',\n",
       " 'handcrafted',\n",
       " 'prompt',\n",
       " 'returning',\n",
       " 'expected',\n",
       " 'mask',\n",
       " 'allows',\n",
       " 'segmenting',\n",
       " 'object',\n",
       " 'visual',\n",
       " 'context',\n",
       " 'however',\n",
       " 'sam',\n",
       " 'inherently',\n",
       " 'loses',\n",
       " 'capability',\n",
       " 'segment',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'imagine',\n",
       " 'intending',\n",
       " 'crop',\n",
       " 'lovely',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'thick',\n",
       " 'photo',\n",
       " 'album',\n",
       " 'find',\n",
       " 'missing',\n",
       " 'clock',\n",
       " 'picture',\n",
       " 'bedroom',\n",
       " 'utilizing',\n",
       " 'vanilla',\n",
       " 'sam',\n",
       " 'would',\n",
       " 'highly',\n",
       " 'image',\n",
       " 'must',\n",
       " 'precisely',\n",
       " 'find',\n",
       " 'target',\n",
       " 'object',\n",
       " 'within',\n",
       " 'complicated',\n",
       " 'context',\n",
       " 'activate',\n",
       " 'sam',\n",
       " 'proper',\n",
       " 'prompt',\n",
       " 'segmentation',\n",
       " 'considering',\n",
       " 'ask',\n",
       " 'personalize',\n",
       " 'sam',\n",
       " 'automatically',\n",
       " 'segment',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'simple',\n",
       " 'efficient',\n",
       " 'manner',\n",
       " 'end',\n",
       " 'introduce',\n",
       " 'persam',\n",
       " 'personalization',\n",
       " 'approach',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'method',\n",
       " 'efficiently',\n",
       " 'customizes',\n",
       " 'sam',\n",
       " 'using',\n",
       " 'data',\n",
       " 'reference',\n",
       " 'image',\n",
       " 'rough',\n",
       " 'mask',\n",
       " 'personal',\n",
       " 'concept',\n",
       " 'specifically',\n",
       " 'first',\n",
       " 'obtain',\n",
       " 'location',\n",
       " 'confidence',\n",
       " 'map',\n",
       " 'target',\n",
       " 'object',\n",
       " 'test',\n",
       " 'image',\n",
       " 'feature',\n",
       " 'similarity',\n",
       " 'considers',\n",
       " 'appearance',\n",
       " 'every',\n",
       " 'foreground',\n",
       " 'pixel',\n",
       " 'according',\n",
       " 'confidence',\n",
       " 'score',\n",
       " 'two',\n",
       " 'point',\n",
       " 'selected',\n",
       " 'location',\n",
       " 'prior',\n",
       " 'finally',\n",
       " 'encoded',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'fed',\n",
       " 'sam',\n",
       " 'decoder',\n",
       " 'segmentation',\n",
       " 'within',\n",
       " 'decoder',\n",
       " 'propose',\n",
       " 'inject',\n",
       " 'visual',\n",
       " 'semantics',\n",
       " 'target',\n",
       " 'object',\n",
       " 'unleash',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'power',\n",
       " 'two',\n",
       " 'technique',\n",
       " 'attention',\n",
       " 'guide',\n",
       " 'every',\n",
       " 'layer',\n",
       " 'sam',\n",
       " 'decoder',\n",
       " 'location',\n",
       " 'confidence',\n",
       " 'map',\n",
       " 'explicitly',\n",
       " 'compels',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'mainly',\n",
       " 'concentrate',\n",
       " 'foreground',\n",
       " 'target',\n",
       " 'region',\n",
       " 'intensive',\n",
       " 'feature',\n",
       " 'aggregation',\n",
       " 'prompting',\n",
       " 'explicitly',\n",
       " 'provide',\n",
       " 'sam',\n",
       " 'target',\n",
       " 'semantics',\n",
       " 'fuse',\n",
       " 'original',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'embedding',\n",
       " 'target',\n",
       " 'object',\n",
       " 'provides',\n",
       " 'positional',\n",
       " 'prompt',\n",
       " 'additional',\n",
       " 'visual',\n",
       " 'cue',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'aforementioned',\n",
       " 'design',\n",
       " 'along',\n",
       " 'cascaded',\n",
       " 'persam',\n",
       " 'exhibit',\n",
       " 'favorable',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'performance',\n",
       " 'unique',\n",
       " 'subject',\n",
       " 'variety',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'notably',\n",
       " 'approach',\n",
       " 'cope',\n",
       " 'well',\n",
       " 'scenario',\n",
       " 'require',\n",
       " 'segmenting',\n",
       " 'one',\n",
       " 'object',\n",
       " 'among',\n",
       " 'multiple',\n",
       " 'similar',\n",
       " 'one',\n",
       " 'simultaneously',\n",
       " 'segmenting',\n",
       " 'several',\n",
       " 'identical',\n",
       " 'object',\n",
       " 'image',\n",
       " 'tracking',\n",
       " 'different',\n",
       " 'object',\n",
       " 'along',\n",
       " 'video',\n",
       " 'nevertheless',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'might',\n",
       " 'occasional',\n",
       " 'failure',\n",
       " 'case',\n",
       " 'object',\n",
       " 'comprises',\n",
       " 'visually',\n",
       " 'distinct',\n",
       " 'subpart',\n",
       " 'hierarchical',\n",
       " 'structure',\n",
       " 'segmented',\n",
       " 'hat',\n",
       " 'top',\n",
       " 'teddy',\n",
       " 'bear',\n",
       " 'head',\n",
       " 'robot',\n",
       " 'toy',\n",
       " 'ambiguity',\n",
       " 'cast',\n",
       " 'challenge',\n",
       " 'persam',\n",
       " 'determining',\n",
       " 'appropriate',\n",
       " 'scale',\n",
       " 'mask',\n",
       " 'output',\n",
       " 'since',\n",
       " 'local',\n",
       " 'part',\n",
       " 'global',\n",
       " 'shape',\n",
       " 'regarded',\n",
       " 'valid',\n",
       " 'mask',\n",
       " 'sam',\n",
       " 'alleviate',\n",
       " 'issue',\n",
       " 'propose',\n",
       " 'variant',\n",
       " 'approach',\n",
       " 'freeze',\n",
       " 'entire',\n",
       " 'sam',\n",
       " 'preserve',\n",
       " 'versatile',\n",
       " 'knowledge',\n",
       " 'parameter',\n",
       " 'within',\n",
       " 'second',\n",
       " 'single',\n",
       " 'gpu',\n",
       " 'detail',\n",
       " 'enable',\n",
       " 'sam',\n",
       " 'produce',\n",
       " 'several',\n",
       " 'potential',\n",
       " 'segmentation',\n",
       " 'result',\n",
       " 'different',\n",
       " 'mask',\n",
       " 'scale',\n",
       " 'adaptively',\n",
       " 'select',\n",
       " 'best',\n",
       " 'scale',\n",
       " 'varying',\n",
       " 'object',\n",
       " 'employ',\n",
       " 'learnable',\n",
       " 'relative',\n",
       " 'weight',\n",
       " 'mask',\n",
       " 'scale',\n",
       " 'conduct',\n",
       " 'weighted',\n",
       " 'summation',\n",
       " 'final',\n",
       " 'output',\n",
       " 'efficient',\n",
       " 'training',\n",
       " 'avoids',\n",
       " 'data',\n",
       " 'exhibit',\n",
       " 'better',\n",
       " 'segmentation',\n",
       " 'accuracy',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'right',\n",
       " 'moreover',\n",
       " 'observe',\n",
       " 'approach',\n",
       " 'also',\n",
       " 'assist',\n",
       " 'dreambooth',\n",
       " 'ruiz',\n",
       " 'et',\n",
       " 'better',\n",
       " 'diffusion',\n",
       " 'model',\n",
       " 'personalized',\n",
       " 'generation',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'given',\n",
       " 'image',\n",
       " 'containing',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'pet',\n",
       " 'cat',\n",
       " 'backpack',\n",
       " 'dreambooth',\n",
       " 'learns',\n",
       " 'convert',\n",
       " 'image',\n",
       " 'identifier',\n",
       " 'v',\n",
       " 'word',\n",
       " 'embedding',\n",
       " 'space',\n",
       " 'however',\n",
       " 'simultaneously',\n",
       " 'include',\n",
       " 'background',\n",
       " 'information',\n",
       " 'stair',\n",
       " 'forest',\n",
       " 'would',\n",
       " 'override',\n",
       " 'newly',\n",
       " 'prompted',\n",
       " 'background',\n",
       " 'disturb',\n",
       " 'target',\n",
       " 'appearance',\n",
       " 'generation',\n",
       " 'therefore',\n",
       " 'propose',\n",
       " 'leverage',\n",
       " 'persam',\n",
       " 'segment',\n",
       " 'target',\n",
       " 'object',\n",
       " 'within',\n",
       " 'training',\n",
       " 'image',\n",
       " 'supervise',\n",
       " 'dreambooth',\n",
       " 'foreground',\n",
       " 'area',\n",
       " 'enabling',\n",
       " 'synthesis',\n",
       " 'higher',\n",
       " 'quality',\n",
       " 'summarize',\n",
       " 'contribution',\n",
       " 'paper',\n",
       " 'follows',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'first',\n",
       " 'investigate',\n",
       " 'customize',\n",
       " 'purpose',\n",
       " 'segmentation',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'scenario',\n",
       " 'minimal',\n",
       " 'expense',\n",
       " 'end',\n",
       " 'introduce',\n",
       " 'two',\n",
       " 'efficient',\n",
       " 'effective',\n",
       " 'method',\n",
       " 'along',\n",
       " 'new',\n",
       " 'segmentation',\n",
       " 'dataset',\n",
       " 'perseg',\n",
       " 'evaluation',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'persam',\n",
       " 'persam',\n",
       " 'propose',\n",
       " 'three',\n",
       " 'technique',\n",
       " 'guide',\n",
       " 'sam',\n",
       " 'semantics',\n",
       " 'target',\n",
       " 'object',\n",
       " 'design',\n",
       " 'parameter',\n",
       " 'second',\n",
       " 'well',\n",
       " 'alleviate',\n",
       " 'mask',\n",
       " 'ambiguity',\n",
       " 'issue',\n",
       " 'approach',\n",
       " 'achieves',\n",
       " 'competitive',\n",
       " 'result',\n",
       " 'various',\n",
       " 'task',\n",
       " 'including',\n",
       " 'perseg',\n",
       " 'benchmark',\n",
       " 'part',\n",
       " 'semantic',\n",
       " 'segmentation',\n",
       " 'video',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'addition',\n",
       " 'persam',\n",
       " 'enhance',\n",
       " 'dreambooth',\n",
       " 'better',\n",
       " 'personalized',\n",
       " 'synthesis',\n",
       " 'related',\n",
       " 'work',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'powerful',\n",
       " 'generalization',\n",
       " 'capacity',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'adapted',\n",
       " 'various',\n",
       " 'downstream',\n",
       " 'scenario',\n",
       " 'attain',\n",
       " 'promising',\n",
       " 'performance',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'bert',\n",
       " 'devlin',\n",
       " 'et',\n",
       " 'lu',\n",
       " 'et',\n",
       " 'gpt',\n",
       " 'series',\n",
       " 'brown',\n",
       " 'et',\n",
       " 'openai',\n",
       " 'radford',\n",
       " 'narasimhan',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'llama',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'demonstrated',\n",
       " 'remarkable',\n",
       " 'learning',\n",
       " 'ability',\n",
       " 'transferred',\n",
       " 'new',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'prompt',\n",
       " 'similarly',\n",
       " 'clip',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'align',\n",
       " 'jia',\n",
       " 'et',\n",
       " 'conduct',\n",
       " 'contrastive',\n",
       " 'learning',\n",
       " 'pair',\n",
       " 'exhibit',\n",
       " 'exceptional',\n",
       " 'accuracy',\n",
       " 'visual',\n",
       " 'recognition',\n",
       " 'painter',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'introduces',\n",
       " 'vision',\n",
       " 'model',\n",
       " 'unifies',\n",
       " 'network',\n",
       " 'architecture',\n",
       " 'prompt',\n",
       " 'accomplish',\n",
       " 'diverse',\n",
       " 'vision',\n",
       " 'task',\n",
       " 'without',\n",
       " 'downstream',\n",
       " 'cafo',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'cascade',\n",
       " 'different',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'collaborates',\n",
       " 'knowledge',\n",
       " 'robust',\n",
       " 'image',\n",
       " 'classification',\n",
       " 'sam',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'present',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'image',\n",
       " 'segmentation',\n",
       " 'billion',\n",
       " 'mask',\n",
       " 'conduct',\n",
       " 'segmentation',\n",
       " 'concurrent',\n",
       " 'work',\n",
       " 'extending',\n",
       " 'sam',\n",
       " 'segmentation',\n",
       " 'ke',\n",
       " 'et',\n",
       " 'faster',\n",
       " 'inference',\n",
       " 'speed',\n",
       " 'zhao',\n",
       " 'et',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'matching',\n",
       " 'liu',\n",
       " 'et',\n",
       " 'reconstruction',\n",
       " 'cen',\n",
       " 'et',\n",
       " 'object',\n",
       " 'tracking',\n",
       " 'yang',\n",
       " 'et',\n",
       " 'medical',\n",
       " 'wang',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'image',\n",
       " 'processing',\n",
       " 'another',\n",
       " 'perspective',\n",
       " 'propose',\n",
       " 'personalize',\n",
       " 'segmentation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'adapts',\n",
       " 'generalist',\n",
       " 'specialist',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'method',\n",
       " 'also',\n",
       " 'assist',\n",
       " 'personalization',\n",
       " 'image',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'stable',\n",
       " 'diffusion',\n",
       " 'rombach',\n",
       " 'et',\n",
       " 'imagen',\n",
       " 'saharia',\n",
       " 'et',\n",
       " 'improves',\n",
       " 'generation',\n",
       " 'quality',\n",
       " 'segmenting',\n",
       " 'foreground',\n",
       " 'target',\n",
       " 'object',\n",
       " 'background',\n",
       " 'disturbance',\n",
       " 'large',\n",
       " 'model',\n",
       " 'segmentation',\n",
       " 'fundamental',\n",
       " 'task',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'segmentation',\n",
       " 'long',\n",
       " 'et',\n",
       " 'jiang',\n",
       " 'et',\n",
       " 'zhao',\n",
       " 'et',\n",
       " 'xu',\n",
       " 'et',\n",
       " 'jiang',\n",
       " 'et',\n",
       " 'lin',\n",
       " 'et',\n",
       " 'requires',\n",
       " 'comprehension',\n",
       " 'image',\n",
       " 'various',\n",
       " 'task',\n",
       " 'explored',\n",
       " 'semantic',\n",
       " 'segmentation',\n",
       " 'classifying',\n",
       " 'pixel',\n",
       " 'predefined',\n",
       " 'set',\n",
       " 'class',\n",
       " 'narayanan',\n",
       " 'et',\n",
       " 'chen',\n",
       " 'et',\n",
       " 'zheng',\n",
       " 'et',\n",
       " 'cheng',\n",
       " 'et',\n",
       " 'xie',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2305.03048v2_lemmatized_tokens.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([\" \".join(lemmatized_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF words:\n",
      "et: 0.4294680024221286\n",
      "segmentation: 0.30778540173585883\n",
      "image: 0.23978630135235515\n",
      "object: 0.22904960129180194\n",
      "sam: 0.2039973011505111\n",
      "model: 0.18252390102940466\n",
      "persam: 0.17536610098903585\n",
      "prompt: 0.17178720096885144\n",
      "mask: 0.17178720096885144\n",
      "wang: 0.12168260068626978\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "sorted_items = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
    "print(\"Top TF-IDF words:\")\n",
    "for idx in sorted_items[:10]:\n",
    "    print(f\"{feature_names[idx]}: {tfidf_matrix[0, idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "checkpoint='t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model=T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(text, chunk_size=512)\n",
    "summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023 PERSONALIZE SEGMENT ANYTHING MODEL WITHONE SHOT Renrui Zhang1,2, Zhengkai Jiang3*, Ziyu Guo2*, Shilin Yan2, Junting Pan1, Xianzheng Ma2 Hao Donga, Yu Qiao2, Peng Gao2, Hongs ining, Segment Anything Model (SAM) has revolutionized the segmenta-tion field. despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored. ining introduces a training-free Personalization approach for SAM. tive lo- cation prior for the target concept in new images. aided by target visual semantics, we empower SAM for personalized object segmentation via two pro- posed techniques. in this way, we can effectively customize the general-purpose SAM for private use without any training. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. code is released at https://github.com/ZrrSkywalker/Personalize-SAM. User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F. PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" e introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. PerSAM-F can help to achieve higher-quality person- alized text-to-image generation. ap- proach can help to achieve higher-quality person- alized text-to-image generation. al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. Segment Anything (Kirillov et al., 2023) develops a bsequently trains a segmentation foundation model, known as SAM. it defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask. however, SAM inherently loses the capability to segment specific visual concepts. imagine intending to crop your lovely pet dog in a thick photo album or find the missing clock from a picture of your bedroom. perSAM is a training-free personalization approach for Segment Anything Model. perSAM allows you to segment user-designated visual concepts in a simple and efficient manner. we first obtain a location confidence map for the target object in the test image by feature similarities. two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. we propose to inject visual semantics of the target object in the decoder for segmentation. object to unleash SAM's personalized segmentation power with two techniques: • Target-guided Attention. we guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. we fuse the original prompt tokens with the embedding of the target object. perSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. the approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output. to alleviate this issue, we further propose a fine-tuning variant of our approach, e.g., a fine-tuning variant of our approach. we freeze the entire SAM to preserve its pre-trained knowledge. we only fine-tune 2 parameters within 10 seconds on a single A100 GPU. perSAM-F avoids over-fitting on the one-shot on the one-shot. our approach can also assist DreamBooth to better fine-tune diffusion models for personalized text-to-image generation. a few images containing a specific visual concept, e.g., your pet cat or backpack, can be converted into an identifier [V] in the word embedding space. however, the background informatia can simultaneously include the background informatia. we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. we first investigate how to customize a general- purpose segmentation model into psa. in perSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. in perSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. ts on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. perSAM can enhance DreamBooth for better personalized text-to-image synthesis. pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. penAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019); and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities. CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) exhibit exceptional accuracy in zero-shot visual recognition. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. there are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023d), faster inference speed (Zhao et al., we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. from another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. imagen improves generation quality by segmenting foreground target objects. segmentation requires a pixel-level comprehension of a image. segmentation-related tasks include semantic segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation and cd segmentation. instance segmentation, focusing on the identification of individual object instances. panoptic segmentation, assigning both class labels and instance identification. interactive segmentation, involving human intervention for re-evaluation. several concurrent works have proposed large-scale vision models for image segmentation. they are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) uses a data engine with model-in-the-loop anno-tation to learn a promptable segmentation framework. painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile erSeg proposes two approaches, perSAM and perSAM-F. persAM and perSAM-F customize SAM for personalized segmentation. works have focused on developing parameter-efficient methods to freeze weights of foundation models and append small-scale modules for fine-tuning. prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks. low-Rank Adaption (LORA) injects trainable rank decomposition matrices concurrently to each pre-trained weight. Adapters are designed to be inserted between layers of the original transformer. we adopt a more efficient adaption method delicately designed for SAM. the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10 seconds. this effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. PerSAM and PerSAM-F consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder. as a promptable framework, SAM takes as input an image I, denoted as Encp, Enc, and Decм. Encp encodes human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1) 4 F1 Encode Cosine Similarity FT=1 Test Image I Target Local Features T=1 FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-At ss-Attention  Aggregate Attention Matrix A Local Features T=1 Token Self-Attention Overall Confidence Map S Aggregate  a Concat( + Repeat(). we calculate a location confidence map for the target object in new test image by the appear- ance of all local parts. then, we select the loca- tion prior as the point prompt for PerS -level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). to in-ject SAM with target semantics, we explicitly guide the cross-attention layers. we propose additional prompting with high-level cues. mask tokens are responsible for generating mask output, formulated as M = Decм (FI, Concat(TM,Tp)) mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. the user provides only a single reference image, and a mask indicating the target visual concept. the given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. for evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. t al., 2022; Kumari et al., 2022; Kumari et al., 2022; Kumari et al., 2022); containing various categories of visual concepts in different poses or scenes. in this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.2 TRAINING-FREE PERSAM Location Confidence Map. the encoder can be SAM's frozen backbone or other pre-trained vision models. we formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR  R1xwxc. then, we use the reference mask MR  Rhxwx1 to crop the features of foreground pixels within the visual concept from FR. n confidence maps for each foreground pixel i by the cosine similarity between T12 and test image feature F as S2  1 = F1TT) 11. each S2 represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. PerSAM selects two points with the highest and lowest confidence values in S. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object, while the latter invenes the most likely center position of the target object. the positive and negative point prompts are fed into the prompt encoder as Tp = Encp(Ph, P1)  R2c, (7) which denotes the prompt tokens for SAM's decoder. in this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. attention operation in SAM's decoder focuses feature aggregation within target regions. the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image. based on such a property, we use S to guide the attention map in every token-to-image cross-attention layer of the decoder. mask and prompt tokens are compelled to capture more visual semantics associated with the target subject. this contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. we propose to use the visual feature of the target concept as an additional high-level semantic prompting. we first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR =  €R1xc n i=1 Then, we element-wisely add TR to all the input tokens of the test image in Equation. perSAM is prompted by low-level location points, but also high-level target visual cues. a repeat operation replicates the target visual embedding in the decoder block. a repeat operation replicates the target visual embedding in the decoder block. PerSAM-F uses two learnable weights for adaptively aggregating three-scale masks. we use PerSAM to decouple the target ob-jects from the background for improving the generation of DreamBooth. segmentation mask on the test image from SAM's decoder may include rough edges and isolated background noise. in the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. in the second step, we acquire the bounding box enclosing the mask from the first step. for the second step, we acquire the bounding box enclosing the mask from the first step 3.3 FINE-TUNING of PERSAM-F Ambiguity of Segmentation Scales. the training-free PerSAM can tackle most cases with satisfac- tory segmentation accuracy. some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. m is comprised of two parts: a lid and a body. the negative prompt (denoted by a green pentagram) does not exclude the platform in a similar color. the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, perSAM would be misled for segmentation. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. this motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning perSAM-F first follows PerSAM to obtain the location prior. we use two learnable mask weights, w1, W2, and calculate the final mask output by a weighted summation as 1/3. to learn the optimal mask weights, we conduct one-shot fine- tuning on the reference image, and regard the given mask as the groud. we freeze the entire SAM model to preserve its pre-trained knowledge. we fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. in this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts. we compare the overall mIoU, blou, and learnable parameters method mIoU bloU Param denotes works concurrent to ours. can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 63.8 92.6 94.1 94.4 93.7 97.2 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 Table 3: one-shot Semantic and Part one-shot Semantic Seg. FSS-1000 LVIS-922 One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F  AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F Method PACO-Part HSNet DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 35 photos of a specific object. it learns to generate the cat referred to by a text prompt, “a [V] cat” and calculates the loss over the entire reconstructed images. this would inject the redundant background information in the training images into the identifier [V] our PerSAM- assisted DreamBooth can synthesize the target object with beads. the target object can be systoked with beads. the target object can also be systoked with beads. the target object can also be systoked with beads. the target object can be systoked with beads. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1. we then illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. perSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. each object is associated with 57 images and masks, where we fix one image-mask pair as the user-provided one-shot dat. the raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari e the mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. enlarged data scale of PerSeg. 8 The ring on a clock 2 The teapot on a tray The backpack carried by a can woman Figure 8: Visualization of PerSAM-F's Im- provement. the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. we show more visualiza-tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP), Painter (Wang et al., 2022), SEEM (Zou et al., 2023 our perSAM-F surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score. our one-shot fine-tuning approach can outperform methods (Lin et al., the results fully illustrate our strong generalization ability for temporal video data and complex scenarios. we evaluate our approach for one-shot image segmentation respectively on four datasets, including FSS-1000 (Li et al., 2020), LVIS-922 (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan e our PerSAM-F achieves consistently better results than Painter. for models with in-domain training, our approach can achieve higher scores than HSNet. the experiments well demonstrate that our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. PerSAM-assisted DreamBooth is able to fine-tune a pre-trained image synthesis. the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. 9 User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM. the improved DreamBooth (Ruiz et al., 2022) can better pc and pc. mIoU Gain 69.1 Method PerSAM Param. mIoU Method Mask Box 0 89.32 Painter 56.4 42.0 + Post-refinement 72.5 +3.4 83.9 +11.4 Prompt Tuning 12K 76.5 VP 65.9 38.1 Adapter 196K 78.3 SEEM 87.1 64.9 + Guided Attention + Semantic Prompt 85.8 .0 89.3 +3.5 3 Mask Weights 3 92.9 + Scale Tuning 95.3 +6.0 PerSAM-F 95.3 88.1 94.9 Positive Prior + Negative Prior 4.4 ABLATION STUDY Main Components. in table 4, we introduce the high-level target semantics into SAM's decoder for attent semantics. PerSAM-F boosts the score by +6.0% via the efficient scale-aware fine-tuning. we experiment with other parameter-efficient fine-tuning methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. in table 6, we relax the input restrictions to a bounding box designating the expected object. our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. in this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. we hope our work may expand the applicability of SAM to a wider: range of scenarios. onal encoder- decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. the IEEE/CVF Conference on computer vision and pattern recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. lora for efficient stable diffusion fine-tuning. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. an image is worth one word: Personalizing text-to-image generation using textual inversion. IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019.. a dataset for large vocabulary instance segmentation. arXiv preprint arXiv:2211.10155, 2022. arXiv preprint arXiv:2211.10155, 2022. cost aggregation with 4d convolutional swin transformer for few-shot segmentation. in european conference on computer vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen. isual and vision-language representation learning with noisy text supervision. in international conference on machine learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. in European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. arXiv preprint arXiv:2304.02643, 2023. nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. the power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. ntion-guided unified network for panoptic segmentation. yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. video object segmentation with adaptive feature bank and uncertain-region refinement. arXiv preprint arXiv:2004.03829, 2020. ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. onference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. learning how to ask: Querying lms with mixture of soft prompts. odels are unsupervised multitask learners. openAI blog, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark. an, et al. Paco: Parts and attributes of common objects. in Proceedings of the IEEE Conference on computer vision and pattern recognition, pp. 7141-7151, 2023. arXiv preprints arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, and Mohit Bansal. e on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Ed arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. arXiv preprint arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. pyramid scene parsing network, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Tao Yu, Min Li yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. arXiv preprint arXiv:2304.06718, 2023. 15 115 preprint arXiv:2304.06718, 2023.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2_summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
