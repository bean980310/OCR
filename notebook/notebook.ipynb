{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>reached_out_link</th>\n",
       "      <th>reached_out_success</th>\n",
       "      <th>reached_out_note</th>\n",
       "      <th>num_models</th>\n",
       "      <th>num_datasets</th>\n",
       "      <th>num_spaces</th>\n",
       "      <th>title</th>\n",
       "      <th>github</th>\n",
       "      <th>github_stars</th>\n",
       "      <th>conference_name</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>github_mention_hf</th>\n",
       "      <th>has_artifact</th>\n",
       "      <th>submitted_by</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.03048</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Personalize Segment Anything Model with One Shot</td>\n",
       "      <td>https://github.com/zrrskywalker/personalize-sam</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.02463</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Shap-E: Generating Conditional 3D Implicit Fun...</td>\n",
       "      <td>https://github.com/openai/shap-e</td>\n",
       "      <td>11606.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.02483</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ChatGPT-steered Editing Instructor for Customi...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.03047</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Principle-Driven Self-Alignment of Language Mo...</td>\n",
       "      <td>https://github.com/IBM/Dromedary</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>NeurIPS2023</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.02440</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cheaply Evaluating Inference Efficiency Metric...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>2412.15214</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LeviTor: 3D Trajectory Oriented Image-to-Video...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>qiuyuu</td>\n",
       "      <td>2024-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>2412.13185</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Move-in-2D: 2D-Conditioned Human Motion Genera...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hsinh</td>\n",
       "      <td>2024-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>2412.13377</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DateLogicQA: Benchmarking Temporal Biases in L...</td>\n",
       "      <td>https://github.com/gagan3012/eais-temporal-bias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>gagan3012</td>\n",
       "      <td>2024-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>2412.14283</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PixelMan: Consistent Object Editing with Diffu...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>LiyaoJiang</td>\n",
       "      <td>2024-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>2412.15191</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AV-Link: Temporally-Aligned Diffusion Features...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>aliaksandr-siarohin</td>\n",
       "      <td>2024-12-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5026 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        arxiv_id reached_out_link  reached_out_success reached_out_note  \\\n",
       "0     2305.03048             None                  0.0                    \n",
       "1     2305.02463             None                  0.0                    \n",
       "2     2305.02483             None                  0.0                    \n",
       "3     2305.03047             None                  0.0                    \n",
       "4     2305.02440             None                  0.0                    \n",
       "...          ...              ...                  ...              ...   \n",
       "5021  2412.15214             None                  NaN             None   \n",
       "5022  2412.13185             None                  NaN             None   \n",
       "5023  2412.13377             None                  NaN             None   \n",
       "5024  2412.14283             None                  NaN             None   \n",
       "5025  2412.15191             None                  NaN             None   \n",
       "\n",
       "      num_models  num_datasets  num_spaces  \\\n",
       "0            0.0           0.0         1.0   \n",
       "1            7.0           0.0        70.0   \n",
       "2            0.0           0.0         0.0   \n",
       "3            0.0           1.0         1.0   \n",
       "4            0.0           0.0         0.0   \n",
       "...          ...           ...         ...   \n",
       "5021         0.0           0.0         0.0   \n",
       "5022         0.0           0.0         0.0   \n",
       "5023         0.0           0.0         0.0   \n",
       "5024         0.0           0.0         0.0   \n",
       "5025         0.0           0.0         0.0   \n",
       "\n",
       "                                                  title  \\\n",
       "0      Personalize Segment Anything Model with One Shot   \n",
       "1     Shap-E: Generating Conditional 3D Implicit Fun...   \n",
       "2     ChatGPT-steered Editing Instructor for Customi...   \n",
       "3     Principle-Driven Self-Alignment of Language Mo...   \n",
       "4     Cheaply Evaluating Inference Efficiency Metric...   \n",
       "...                                                 ...   \n",
       "5021  LeviTor: 3D Trajectory Oriented Image-to-Video...   \n",
       "5022  Move-in-2D: 2D-Conditioned Human Motion Genera...   \n",
       "5023  DateLogicQA: Benchmarking Temporal Biases in L...   \n",
       "5024  PixelMan: Consistent Object Editing with Diffu...   \n",
       "5025  AV-Link: Temporally-Aligned Diffusion Features...   \n",
       "\n",
       "                                               github  github_stars  \\\n",
       "0     https://github.com/zrrskywalker/personalize-sam        1505.0   \n",
       "1                    https://github.com/openai/shap-e       11606.0   \n",
       "2                                                None           NaN   \n",
       "3                    https://github.com/IBM/Dromedary        1119.0   \n",
       "4                                                None           NaN   \n",
       "...                                               ...           ...   \n",
       "5021                                                            NaN   \n",
       "5022                                                            NaN   \n",
       "5023  https://github.com/gagan3012/eais-temporal-bias           NaN   \n",
       "5024                                                            NaN   \n",
       "5025                                                            NaN   \n",
       "\n",
       "     conference_name  upvotes  num_comments  github_mention_hf  has_artifact  \\\n",
       "0               None        9             1                1.0          True   \n",
       "1               None        3             1                0.0          True   \n",
       "2               None        3             1                0.0         False   \n",
       "3        NeurIPS2023        1             5                1.0          True   \n",
       "4               None        1             0                0.0         False   \n",
       "...              ...      ...           ...                ...           ...   \n",
       "5021            None       12             3                0.0         False   \n",
       "5022            None        1             2                0.0         False   \n",
       "5023            None        2             2                1.0         False   \n",
       "5024            None        3             2                0.0         False   \n",
       "5025            None        3             2                0.0         False   \n",
       "\n",
       "             submitted_by        date  \n",
       "0                 akhaliq  2023-05-05  \n",
       "1                 akhaliq  2023-05-05  \n",
       "2                 akhaliq  2023-05-05  \n",
       "3                 akhaliq  2023-05-05  \n",
       "4                 akhaliq  2023-05-05  \n",
       "...                   ...         ...  \n",
       "5021               qiuyuu  2024-12-20  \n",
       "5022                hsinh  2024-12-20  \n",
       "5023            gagan3012  2024-12-20  \n",
       "5024           LiyaoJiang  2024-12-20  \n",
       "5025  aliaksandr-siarohin  2024-12-20  \n",
       "\n",
       "[5026 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"huggingface/community-science-paper-v2\", split=\"train\")\n",
    "df=pd.DataFrame(ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arxiv_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id=df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition https://arxiv.org/pdf/{arxiv_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        response=requests.get(url)\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    !wget --content-disposition https://arxiv.org/pdf/{arxiv_id[i]}\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1001, 1500):\n",
    "    !wget --content-disposition https://arxiv.org/pdf/{arxiv_id[i]}\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/janghyeonbin/OCR/pdf\n"
     ]
    }
   ],
   "source": [
    "dir_abs_path=os.path.abspath(\"../pdf\")\n",
    "print(dir_abs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2306.10008v2.pdf', '2306.03083v1.pdf', '2305.02412v2.pdf', '2307.06925v1.pdf', '2306.02858v4.pdf', '2309.02186v1.pdf', '2308.07926v2.pdf', '2309.02040v2.pdf', '2307.04686v2.pdf', '2308.01477v1.pdf', '2306.03203v1.pdf', '2309.08827v1.pdf', '2309.06440v1.pdf', '2307.13908v1.pdf', '2308.01320v1.pdf', '2307.00119v1.pdf', '2306.05410v1.pdf', '2307.09233v3.pdf', '2305.03027v1.pdf', '2309.00908v1.pdf', '2305.05845v1.pdf', '2307.12169v5.pdf', '2306.06092v1.pdf', '2308.03291v3.pdf', '2306.13776v1.pdf', '2309.03907v1.pdf', '2308.13785v1.pdf', '2305.10853v2.pdf', '2307.04603v5.pdf', '2305.11337v1.pdf', '2308.08316v3.pdf', '2305.10973v2.pdf', '2306.16009v1.pdf', '2306.09327v1.pdf', '2307.02053v1.pdf', '2305.18286v1.pdf', '2306.16934v2.pdf', '2307.14620v1.pdf', '2305.10763v1.pdf', '2306.13455v3.pdf', '2305.03509v3.pdf', '2305.11308v2.pdf', '2306.08205v2.pdf', '2308.08089v1.pdf', '2309.08628v3.pdf', '2307.14460v1.pdf', '2305.11129v2.pdf', '2305.11778v1.pdf', '2307.02499v1.pdf', '2305.02790v1.pdf', '2307.06940v1.pdf', '2307.13974v1.pdf', '2306.06638v1.pdf', '2309.11523v5.pdf', '2306.01741v1.pdf', '2305.18802v1.pdf', '2308.01300v2.pdf', '2307.12854v1.pdf', '2307.10159v1.pdf', '2306.16700v2.pdf', '2306.07174v1.pdf', '2306.07954v2.pdf', '2309.03199v2.pdf', '2305.10425v1.pdf', '2305.11837v2.pdf', '2308.15930v3.pdf', '2305.10601v2.pdf', '2305.04966v2.pdf', '2306.00008v2.pdf', '2309.11500v4.pdf', '2305.05432v1.pdf', '2305.07214v1.pdf', '2309.09958v1.pdf', '2306.10231v1.pdf', '2306.09635v2.pdf', '2305.02549v2.pdf', '2305.05383v1.pdf', '2306.10785v1.pdf', '2309.03185v1.pdf', '2305.04790v3.pdf', '2305.11759v1.pdf', '2309.07870v3.pdf', '2306.02982v2.pdf', '2306.08568v1.pdf', '2306.06546v2.pdf', '2307.01200v3.pdf', '2306.02245v2.pdf', '2305.12001v2.pdf', '2307.13702v1.pdf', '2309.07990v2.pdf', '2309.02119v3.pdf', '2309.07314v1.pdf', '2306.04632v1.pdf', '2308.01904v1.pdf', '2305.05591v1.pdf', '2305.18247v2.pdf', '2306.03514v3.pdf', '2307.15131v2.pdf', '2306.05399v2.pdf', '2307.11795v1.pdf', '2305.09636v1.pdf', '2308.16824v2.pdf', '2305.06908v4.pdf', '2305.16704v1.pdf', '2309.10917v1.pdf', '2309.08804v1.pdf', '2308.04623v1.pdf', '2307.08621v4.pdf', '2306.09864v1.pdf', '2308.03421v3.pdf', '2305.13534v1.pdf', '2308.13404v1.pdf', '2305.13304v1.pdf', '2307.16125v2.pdf', '2305.03981v1.pdf', '2305.03043v1.pdf', '2309.04827v1.pdf', '2307.03917v3.pdf', '2308.06261v1.pdf', '2308.01734v1.pdf', '2309.08773v1.pdf', '2305.06424v4.pdf', '2308.06125v1.pdf', '2305.06404v1.pdf', '2309.03926v1.pdf', '2307.14225v1.pdf', '2306.15354v3.pdf', '2309.04354v1.pdf', '2306.17492v2.pdf', '2308.00113v2.pdf', '2305.07677v2.pdf', '2309.07906v3.pdf', '2307.16372v1.pdf', '2306.12156v1.pdf', '2305.05706v1.pdf', '2308.16876v2.pdf', '2305.06456v3.pdf', '2305.08850v2.pdf', '2306.14289v2.pdf', '2307.03183v1.pdf', '2305.10431v2.pdf', '2309.06126v1.pdf', '2305.11364v2.pdf', '2306.01841v1.pdf', '2309.06802v1.pdf', '2309.00986v1.pdf', '2308.07891v1.pdf', '2309.08172v2.pdf', '2309.00615v1.pdf', '2309.08210v1.pdf', '2307.11526v2.pdf', '2305.09664v2.pdf', '2308.04729v1.pdf', '2306.15658v1.pdf', '2308.07968v1.pdf', '2306.02254v2.pdf', '2305.07804v4.pdf', '2305.09662v1.pdf', '2306.15091v1.pdf', '2309.12311v1.pdf', '2307.08041v2.pdf', '2305.02483v2.pdf', '2308.05221v1.pdf', '2305.10841v2.pdf', '2309.09400v1.pdf', '2306.04050v2.pdf', '2306.04076v1.pdf', '2308.02453v3.pdf', '2307.09793v1.pdf', '2305.13077v1.pdf', '2307.13383v1.pdf', '2309.10537v1.pdf', '2307.10373v3.pdf', '2308.06873v2.pdf', '2305.03210v2.pdf', '2305.08275v4.pdf', '2306.07944v1.pdf', '2307.06949v2.pdf', '2308.02510v2.pdf', '2305.06218v1.pdf', '2305.16367v1.pdf', '2305.09764v1.pdf', '2305.15779v1.pdf', '2306.06044v2.pdf', '2308.05734v3.pdf', '2306.10007v2.pdf', '2309.04663v2.pdf', '2309.04269v1.pdf', '2305.17306v1.pdf', '2305.10688v2.pdf', '2309.10279v1.pdf', '2305.11000v2.pdf', '2305.05973v3.pdf', '2305.06474v1.pdf', '2307.09668v1.pdf', '2309.05767v2.pdf', '2306.00378v1.pdf', '2306.01754v1.pdf', '2306.09329v1.pdf', '2305.05189v4.pdf', '2305.16867v1.pdf', '2305.04268v1.pdf', '2305.08848v1.pdf', '2307.08674v3.pdf', '2307.02628v1.pdf', '2308.16582v2.pdf', '2309.07749v1.pdf', '2307.16890v2.pdf', '2305.16843v1.pdf', '2309.08586v2.pdf', '2305.10320v1.pdf', '2309.09390v1.pdf', '2305.07027v1.pdf', '2306.09782v2.pdf', '2306.17840v4.pdf', '2308.03280v1.pdf', '2305.16411v1.pdf', '2306.04009v1.pdf', '2305.18373v1.pdf', '2305.09148v1.pdf', '2305.03726v1.pdf', '2305.02783v4.pdf', '2307.10168v2.pdf', '2309.11497v2.pdf', '2308.05884v1.pdf', '2308.07395v1.pdf', '2305.09761v1.pdf', '2308.01499v1.pdf', '2307.03692v1.pdf', '2308.03757v1.pdf', '2307.12533v3.pdf', '2309.08051v2.pdf', '2305.07378v1.pdf', '2305.07243v2.pdf', '2307.15042v2.pdf', '2305.11694v2.pdf', '2306.07941v1.pdf', '2306.04235v1.pdf', '2307.03322v1.pdf', '2307.11418v3.pdf', '2306.04619v1.pdf', '2305.05176v1.pdf', '2306.17319v1.pdf', '2305.10142v1.pdf', '2307.01229v1.pdf', '2308.05960v1.pdf', '2305.13050v1.pdf', '2309.01826v2.pdf', '2305.17098v2.pdf', '2309.07062v1.pdf', '2305.09975v1.pdf', '2306.08133v2.pdf', '2306.03504v2.pdf', '2308.13416v1.pdf', '2305.02499v1.pdf', '2305.08810v1.pdf', '2307.12612v1.pdf', '2308.03610v1.pdf', '2305.10018v1.pdf', '2309.10202v1.pdf', '2305.11243v2.pdf', '2305.08891v4.pdf', '2305.16806v4.pdf', '2309.00775v1.pdf', '2306.05392v1.pdf', '2305.20010v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "file_lists=os.listdir(dir_abs_path)\n",
    "print(file_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = [f for f in file_lists if os.path.splitext(f)[1].lower() == \".pdf\"]\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  7\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  7\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  4\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  3\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  7\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  2\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  4\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  7\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  7\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  3\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  9\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  5\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  13\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  10\n",
      " No. Of Pages :  6\n",
      " No. Of Pages :  12\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  8\n",
      " No. Of Pages :  11\n",
      " No. Of Pages :  14\n",
      " No. Of Pages :  11\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_list:\n",
    "    pdfReader=PyPDF2.PdfReader(os.path.join(dir_abs_path, file_name))\n",
    "    print(\" No. Of Pages : \",len(pdfReader.pages))\n",
    "    if len(pdfReader.pages) >= 15:\n",
    "        cnt=cnt+1\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    pdfReader=PyPDF2.PdfReader(os.path.join(dir_abs_path, file_name))\n",
    "    print(\" No. Of Pages : \",len(pdfReader.pages))\n",
    "    if len(pdfReader.pages) >= 15:\n",
    "        !rm ../pdf/{file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud import storage\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=documentai.DocumentProcessorServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=os.getenv('PROJECT_ID')\n",
    "location = os.getenv('LOCATION')\n",
    "processor_id=os.getenv('PROCESSOR_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m project_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_id' is not defined"
     ]
    }
   ],
   "source": [
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open((os.path.join(dir_abs_path, file_name)), 'rb') as f:\n",
    "        content=f.read()\n",
    "        \n",
    "    request = documentai.types.ProcessRequest(\n",
    "        name=f\"projects/{project_id}/locations/{location}/processors/{processor_id}\",\n",
    "        raw_document=documentai.types.RawDocument(\n",
    "            content=content,\n",
    "            mime_type=\"application/pdf\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    result = client.process_document(request=request)\n",
    "    \n",
    "    document = result.document\n",
    "    text=document.text\n",
    "    \n",
    "    if not os.path.exists(\"../txt/\"):\n",
    "        os.makedirs(\"../txt\")\n",
    "        \n",
    "    with open(f\"../txt/{file_name}.txt\", 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open(f\"../txt/{file_name}.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    replaced_text = re.sub(r'Page\\s*\\d+|\\s*\\d+\\s*\\n', '', text)\n",
    "    replaced_text = re.sub(r'-\\n', '', replaced_text)\n",
    "    replaced_text = re.sub(r'\\s+', ' ', replaced_text).strip()\n",
    "\n",
    "    replaced_text\n",
    "    \n",
    "    if not os.path.exists(\"../re_txt/\"):\n",
    "        os.makedirs(\"../re_txt\")\n",
    "    \n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"w\") as f:\n",
    "        f.write(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str) -> dict:\n",
    "    \"\"\"Detects the text's language.\"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.detect_language(text)\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Confidence: {}\".format(result[\"confidence\"]))\n",
    "    print(\"Language: {}\".format(result[\"language\"]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open(f\"../txt/{file_name}.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    detect_language(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(target: str, text: str) -> dict:\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    print(\"Text: {}\".format(result[\"input\"]))\n",
    "    print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"r\") as f:\n",
    "        replaced_text = f.read()\n",
    "        \n",
    "    sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "    section_data = {}\n",
    "    \n",
    "    for i in range(len(sections)-1):\n",
    "        pattern = f\"{sections[i]}(.*?){sections[i+1]}\"\n",
    "        match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "        if match:\n",
    "            section_data[sections[i]] = match.group(1).strip()\n",
    "    \n",
    "    last_section = sections[-1]\n",
    "    pattern = f\"{last_section}(.*?)$\"\n",
    "    match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "    \n",
    "    if match:\n",
    "        section_data[last_section] = match.group(1).strip()\n",
    "    \n",
    "    print(f\"--- {file_name} ---\")\n",
    "\n",
    "    for sec, content in section_data.items():\n",
    "        print(f\"--- {sec} ---\\n{content[:500]}...\\n\")\n",
    "    \n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"r\") as f:\n",
    "        replaced_text = f.read()\n",
    "        \n",
    "    sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "    section_data = {}\n",
    "    \n",
    "    for i in range(len(sections)-1):\n",
    "        pattern = f\"{sections[i]}(.*?){sections[i+1]}\"\n",
    "        match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "        if match:\n",
    "            section_data[sections[i]] = match.group(1).strip()\n",
    "    \n",
    "    last_section = sections[-1]\n",
    "    pattern = f\"{last_section}(.*?)$\"\n",
    "    match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "    \n",
    "    if match:\n",
    "        section_data[last_section] = match.group(1).strip()\n",
    "    \n",
    "    print(file_name)\n",
    "\n",
    "    if not os.path.exists(\"../translated_txt/\"):\n",
    "        os.makedirs(\"../translated_txt\")\n",
    "        \n",
    "    with open(f'../translated_txt/{file_name}_translated.txt', 'w') as f:\n",
    "        f.write(\"\")\n",
    "        \n",
    "    for sec, content in section_data.items():\n",
    "        translated_text=translate_text(target='ko', text=content)\n",
    "        with open(f\"../translated_txt/{file_name}_translated.txt\", 'a') as f:\n",
    "            f.write(f\"--- {sec} ---\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(translated_text['translatedText'])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import pipeline, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path: str, poppler_path: str=None, lang: str='eng') -> str:\n",
    "    try:\n",
    "        # PDF를 이미지로 변환\n",
    "        images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "        text = \"\"\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"Processing page {i + 1}/{len(images)}...\")\n",
    "            # OCR을 사용하여 이미지에서 텍스트 추출\n",
    "            page_text = pytesseract.image_to_string(image, lang=lang)  # 언어 설정 가능 (예: 'kor' 한국어)\n",
    "            text += f\"--- Page {i + 1} ---\\n{page_text}\\n\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"PDF 처리 중 오류 발생 ({pdf_path}): {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 코어 수: 10\n"
     ]
    }
   ],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"CPU 코어 수: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 작업 수: 8\n"
     ]
    }
   ],
   "source": [
    "max_workers = 8\n",
    "print(f\"최대 작업 수: {max_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dir=\"/Users/janghyeonbin/OCR/txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.10008v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.03083v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02412v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.06925v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.02858v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.02186v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.07926v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.02040v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.04686v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01477v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.03203v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08827v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.06440v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.13908v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01320v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.00119v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.05410v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.09233v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03027v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.00908v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05845v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.12169v5.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.06092v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.03291v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.13776v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.03907v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.13785v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10853v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.04603v5.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11337v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.08316v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10973v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.16009v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.09327v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.02053v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.18286v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.16934v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.14620v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10763v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.13455v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03509v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11308v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.08205v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.08089v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08628v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.14460v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11129v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11778v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.02499v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02790v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.06940v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.13974v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.06638v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.11523v5.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.01741v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.18802v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01300v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.12854v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.10159v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.16700v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.07174v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.07954v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.03199v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10425v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11837v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.15930v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10601v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.04966v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.00008v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.11500v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05432v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07214v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.09958v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.10231v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.09635v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02549v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05383v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.10785v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.03185v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.04790v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11759v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07870v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.02982v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.08568v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.06546v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.01200v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.02245v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.12001v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.13702v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07990v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.02119v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07314v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04632v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01904v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05591v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.18247v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.03514v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.15131v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.05399v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.11795v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09636v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.16824v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06908v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16704v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.10917v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08804v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.04623v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.08621v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.09864v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.03421v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.13534v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.13404v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.13304v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.16125v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03981v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03043v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.04827v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.03917v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.06261v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01734v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08773v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06424v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.06125v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06404v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.03926v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.14225v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.15354v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.04354v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.17492v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.00113v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07677v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07906v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.16372v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.12156v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05706v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.16876v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06456v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.08850v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.14289v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.03183v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10431v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.06126v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11364v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.01841v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.06802v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.00986v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.07891v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08172v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.00615v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08210v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.11526v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09664v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.04729v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.15658v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.07968v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.02254v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07804v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09662v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.15091v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.12311v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.08041v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02483v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.05221v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10841v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.09400v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04050v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04076v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.02453v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.09793v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.13077v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.13383v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.10537v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.10373v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.06873v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03210v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.08275v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.07944v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.06949v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.02510v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06218v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16367v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09764v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.15779v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.06044v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.05734v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.10007v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.04663v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.04269v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.17306v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10688v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.10279v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11000v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05973v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.06474v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.09668v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.05767v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.00378v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.01754v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.09329v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05189v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16867v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.04268v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.08848v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.08674v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.02628v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.16582v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07749v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.16890v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16843v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08586v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10320v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.09390v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07027v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.09782v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.17840v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.03280v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16411v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04009v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.18373v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09148v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.03726v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02783v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.10168v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.11497v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.05884v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.07395v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09761v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.01499v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.03692v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.03757v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.12533v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.08051v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07378v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.07243v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.15042v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11694v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.07941v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04235v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.03322v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.11418v3.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.04619v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.05176v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.17319v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10142v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.01229v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.05960v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.13050v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.01826v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.17098v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.07062v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.09975v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.08133v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.03504v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.13416v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.02499v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.08810v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2307.12612v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2308.03610v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.10018v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.10202v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.11243v2.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.08891v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.16806v4.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2309.00775v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2306.05392v1.pdf.txt\n",
      "이미 텍스트 파일이 존재하여 건너뜁니다: /Users/janghyeonbin/OCR/txt/2305.20010v1.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "extracted_texts = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:  # max_workers는 시스템 자원에 따라 조정\n",
    "    future_to_file = {}\n",
    "    for file_name in file_list:\n",
    "        output_txt_path = os.path.join(txt_dir, f\"{file_name}.txt\")\n",
    "        if os.path.exists(output_txt_path):\n",
    "            # 이미 텍스트 파일이 존재하면 읽어서 extracted_texts에 추가\n",
    "            try:\n",
    "                with open(output_txt_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                extracted_texts[file_name] = text\n",
    "                print(f\"이미 텍스트 파일이 존재하여 건너뜁니다: {output_txt_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"텍스트 파일 읽기 중 오류 발생 ({output_txt_path}): {e}\")\n",
    "        else:\n",
    "            # 텍스트 파일이 없으면 OCR을 통해 텍스트 추출 작업 제출\n",
    "            future = executor.submit(pdf_to_text, os.path.join(dir_abs_path, file_name))\n",
    "            future_to_file[future] = file_name\n",
    "\n",
    "    for future in as_completed(future_to_file):\n",
    "        file_name = future_to_file[future]\n",
    "        try:\n",
    "            text = future.result()\n",
    "            if text:\n",
    "                extracted_texts[file_name] = text\n",
    "                # 추출된 텍스트를 txt_dir에 저장\n",
    "                output_txt_path = os.path.join(txt_dir, f\"{file_name}.txt\")\n",
    "                with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "                print(f\"텍스트 추출 완료: {file_name}\")\n",
    "            else:\n",
    "                print(f\"텍스트 추출 실패: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{file_name} 처리 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_list:\n",
    "    with open(f\"../txt/{file_name}.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    replaced_text = re.sub(r'Page\\s*\\d+|\\s*\\d+\\s*\\n', '', text)\n",
    "    replaced_text = re.sub(r'-\\n', '', replaced_text)\n",
    "    replaced_text = re.sub(r'\\s+', ' ', replaced_text).strip()\n",
    "\n",
    "    replaced_text\n",
    "    \n",
    "    if not os.path.exists(\"../re_txt/\"):\n",
    "        os.makedirs(\"../re_txt\")\n",
    "    \n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"w\") as f:\n",
    "        f.write(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2306.10008v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate “naturalistic” images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the lowdimensional manifold of a pretrained gen...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Deep learning based face recognition (FR) systems [43, 61] have found widespread usage in multiple applications, --- --Table 1. Comparison among different facial privacy protection methods w.r.t. the natural outputs, black box setting, experiments under face verification and identification tasks, unrestricted (semantically meaningful), and more flexible text guided adversaries. | Adv-Makeup [71] TIP-IM [70] AMT-GAN [22] Ours Natural outputs Yes Partially Partially Yes Black box Yes Yes Yes Yes V...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Obfuscation...\n",
      "\n",
      "--- METHOD ---\n",
      "s for enhancing privacy fail to generate “naturalistic” images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the lowdimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This st...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that faces generated by our approach have stronger black-box transferability with an absolute gain of 12.06% over the state-of-the-art facial privacy protection approach under the face verification task. Finally, we demonstrate the effectiveness of the proposed approach for commercial face recognition systems. Our code is available at https://github.com/fahadshamshad/Clip2Protect. 1. Introduction Deep learning based face recognition (FR) systems [43, 61] have found widespread usage...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have proposed a framework to protect privacy of face images on online platforms by carefully searching for adversarial codes in the low-dimensional latent manifold of a pre-trained generative model. We have shown that incorporating a makeup text-guided loss and an identity preserving regularization effectively hides the adversarial perturbations in the makeup style, provides images with high quality, and preserves human-perceived identity. While this approach is robust to the user-defined tex...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.03083v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present MotionDiffuser, a diffusion based representation for the joint distribution of future trajectories over multiple agents. Such representation has several key advantages: first, our model learns a highly multimodal distribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors. Third, our model is capable of learning the joint distribution for the motion of multiple agen...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Motion prediction is a central yet challenging problem for autonomous vehicles to safely navigate under uncertainties. Motion prediction, in the autonomous driving setting, refers to the prediction of the future trajectories of modeled agents, conditioned on the histories of the modeled agents, context agents, road graph and traffic light signals. Several key challenges arise in the motion prediction problem. First, motion prediction is probabilistic and multimodal in nature where it is importan...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Denoising diffusion models Denoising diffusion models [16, 33],...\n",
      "\n",
      "--- METHOD ---\n",
      "ologically highly related to the class of score-based generative models [23, 43, 44], have recently emerged as a powerful class of generative models that demonstrate high sample quality across a wide range of application domains, including image generation [36, 37, 39], video generation [15, 18, 49] and 3D shape generation [35]. We are among the first to use diffusion models for predicting the joint motion of agents. --- --Constrained sampling Diffusion models have been shown to be effective at ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "and Results 4.1. PCA Mode Analysis To motivate our use of PCA as a simple and accurate compressed trajectory representation, we analyze the principal components computed from N, = 10° randomly selected trajectories from the Waymo Open Dataset training split. Fig. 5a shows the average reconstruction error per waypoint using increasing numbers of principal components. When keeping only the first 10 principal components, the average reconstruction error is 0.06 meters, which is significantly lower ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Discussions In this work, we introduced MotionDiffuser, a novel diffusion model based multi-agent motion prediction framework that allows us to learn a diverse, multimodal joint future distribution for multiple agents. We propose a novel transformer-based set denoiser architecture that is permutation invariant across agents. Furthermore we propose a general and flexible constrained sampling framework, and demonstrate the effectiveness of two simple and useful constraints - attractor and repe...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02412v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM’s ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, he transformer architecture inherits several constraints that make it difficult for the LLM o directly serve as the agent: e.g. limited inut lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nonext enviro...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Humans can abstractly plan their everyday tasks without execution; for example, given the task “Make breakfast”, we can roughly plan to first pick up a mug and make coffee, before grabbing eggs to scramble. Embodied agents, endowed with this capability will generalize more effectively by leveraging common-sense reasoning. ‘Carnegie Mellon University ?Ariel University *Microsoft Research “Nvidia Research. Correspondence to: Yue Wu <ywu5@andrew.cmu.edu>. ( Heat some apple and put it in the fridge ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepputtis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Sharma et al., 2021) or reinforcement learning (Misra. et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; Akakzia et al., 2020) policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010). W...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and analysis demonstrate that LLMs not only remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. In addition, multiple LLMs may be used in coordination with each other to assist the agent from different aspects. Our contributions are as follows: 1. PET: A novel framework for leveraging pretrained LLMs with embodied agents; our work shows that each of P, E, T serves a complementary role and should be simultaneousl...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ", Limitations, and Future Work n this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs © assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. n our experiments, we combine PET with a novel Acion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since he ...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.06925v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "The rapid advancement of generative models has revolutionized content creation, enabling effortless generation of diverse artworks. *Work was done during an internship at NVIDIA 1Our project page available at https://datencoder.github.io Part of their true potential lies in personalization, allowing users to tailor outputs to unique personal concepts. Personalizing a model involves customizing it to capture and manifest unique characteristics of personal belongings, memories, or self-portraits. ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Text-driven image generation using diffusion models. Textto-image synthesis has made significant progress in recent years, driven mainly by pre-trained diffusion models [Ho et al. 2020a] and especially by large models [Balaji et al. 2022; Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022] trained on web-scale data like [Schuhmann et al. 2021]. Our approach builds upon these pretrained models to extend their vocabulary and generate personalized concepts. Specifically, we use the Stable-...\n",
      "\n",
      "--- METHOD ---\n",
      "that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods '. 1 INTRODUCTION The rapid advancement of generative models has revolutionized content creation, enabling effortless generation of diverse artworks. *Work was done during an internship at NVIDIA 1Our project page ava...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We presented a method for generalizing the tuning-encoder approach beyond a single class domain. Our approach restricts overfitting by ensuring predicted embeddings lie close to the real word domain, and by utilizing a dual-pass approach where the network blends predictions from hard- and soft-prompts. This in turn allows us to quickly personalize a model at inference-time, speeding up personalization by two orders of magnitude compared to optimization-based approaches. In the future, we hope to...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.02858v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present Video-LLaMA! a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only (Zhu et al., 2023; Liu et al., 2023; Huang et al., 2023a), Video-LLaMA enables video comprehension by tackling two challe...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) (Chowdhery et al., 2022; Bai et al., 2022; OpenAI, 2023) have demonstrated remarkable capability of understanding and *Xin Li is the corresponding author. 'The video demonstration is available at https: //youtu. be/RDNYs3Rswhc following user intentions and instructions*>4. Typically, the user requests and the corresponding responses from LLMs are all in texts, however, textonly human-computer interaction is not sufficient for many application scenarios because real-w...\n",
      "\n",
      "--- METHOD ---\n",
      "Video-LLaMA aims to empower frozen LLMs with the capability of understanding both visual and auditory content in videos. As shown in Figure 1, we design two branches, namely Vision-Language Branch and Audio-Language Branch, to respectively transform the video frames and audio signals into query representations that are compatible with the textual inputs of LLMs. In this section, we first introduce the overall architecture and the building blocks of each branch. Then, we delineate the procedures ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present Video-LLaMA, a cuttingedge multi-modal framework that empowers large language models with both audio & video understanding capabilities. Our experiments demonstrated the impressive abilities of Video-LLaMA in audio and video-grounded conversations, highlighting its potential as a promising prototype for audio-visual AI assistants. We have open-sourced the entire training code and various model weights, along with detailed instructions to assist developers in utilizing o...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.02186v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively uncommon in real life, and full body generation typically does not deal with facial expression control and still has challenges in generating high-quality results. Towards applicable video avatars, we present an animatable 3D-aware GAN that generates portrait images with controllable facial expression, head pose, and shoulder movements. It ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The automatic creation of animatable 3D human characters has become an increasingly important topic with a range of applications including video conferencing, movie production, and gaming. The related techniques have undergone a significant growth recently, with a variety of promising methods being proposed [1, 7, 8, 16, 21, 38, 40, 42, 51, 52, 54, 60, 62]. Among these techniques, 3D-aware generation methods have emerged as a particularly promising avenue [1, 12, 21, 38, 51, 52, 60, 62]. These m...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "3D-aware Image Generation 3D-aware image generative models aim to generate images that allow for the explicit control of 3D camera viewpoint, training only on 2D images. Most existing works are based on the GAN framework for generative modeling. Early approaches to this problem use 3D convolutions to generate 3D feature volumes and project them to 2D plane for image generation [35, 36]. Recently,...\n",
      "\n",
      "--- METHOD ---\n",
      "is a new 3D-aware GAN that can generate diverse virtual human portraits (512x512) with explicitly controllable 3D camera viewpoints, facial expression, head pose, and shoulder movements. It is trained on unstructured 2D images without any 3D or video data. (Best viewed with zoom; see our project page for videos of more samples) Abstract Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively unc...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our method, trained on unstructured 2D images, can generate diverse and high-quality 3D portraits with desired control over different properties. 1. Introduction The automatic creation of animatable 3D human characters has become an increasingly important topic with a range of applications including video conferencing, movie production, and gaming. The related techniques have undergone a significant growth recently, with a variety of promising methods being proposed [1, 7, 8, 16, 21,...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented a novel 3D-aware GAN for animatable head-shoulder portrait generation, a new task not addressed by previous methods. We identified several key is veo spe Figure 7: Talk video generation driven by real person. Bea Figure 8: Limitations on more extreme expressions and closed eyes. For each image pair, the left one is the reference image and the right one is the animated result. sues when extending existing techniques to this new task and proposed targeted algorithms to tackle the...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.07926v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present the content deformation field (CoDeF) as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We a...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The field of image processing has witnessed remarkable advancements, largely attributable to the power of generative models trained on extensive datasets, yielding exceptional quality and precision. However, the processing of video content has not achieved comparable progress. One challenge lies in maintaining high temporal consistency, a task complicated by the inherent randomness of neural networks. Another challenge arises from the nature of video datasets themselves, which often include text...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Implicit Neural Representations. Implicit representations in conjunction with coordinate-based Multilayer Perceptrons (MLPs) have demonstrated its powerful capability in accurately representing images [4, 49, 51], videos [16, 21,49, 66], and 3D/4D representations [26, 27, 31-34, 56]. These techniques have been employed in a range of applications, including novel view synthesis [27], image superresolution [4], and 3D/4D Reconstruction [56,62]. Furthermore, for the purpose of speeding up the train...\n",
      "\n",
      "--- METHOD ---\n",
      "s exhibit two principal deficiencies. First, the capacity of these representations, particularly in faithfully reconstructing intricate details within a video, is restricted. Often, the reconstructed video overlooks subtle motion details, such as blinking eyes or slight smiles. The second limitation pertains to the typically distorted nature of the estimated atlas, which consequently suffers from impaired semantic information. Existing image processing algorithms, therefore, do not perform optim...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found here. * Equal contributio...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Discussion In this paper, we have investigated representing videos as content deformation fields, focusing on achieving temporally consistent video processing. Our approach demonstrates promising results in terms of both fidelity and temporal consistency. However, there remain several challenges to be addressed in future work. One of the primary issues pertains to the per-scene optimization required in our methodology. We anticipate that advancements in feed-forward implicit field techniques...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.02040v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differe...\n",
      "\n",
      "--- METHOD ---\n",
      "Given some task specification c, we have a target distribution of designs 7 (a) which we want to optimize w.r.t. 2. To simplify notation, we do not emphasize the dependence of --- --Diffusion Generative Inverse Design a Generating and Evaluating Designs Diffusion Generative Model Learned Simulator # a x,~ MOD) Xy~ a(x) 8, a >| () Energy Function S + b__ Diffusion Model Trained on Prior Optimized Designs Reverse diffusion x Diffusion C Optimizing design samples for reward Guided Sampling Figure 1...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on a number of fluid dynamics design challenges, and find that our approach substantially reduces the number of calls to the simulator compared to standard techniques. 1. Introduction Substantial improvements to our way of life hinge on devising solutions to engineering challenges, an area in which Machine Learning (ML) advances is poised to provide positive real-world impact. Many such problems can be formulated as designing an object that gives rise to some desirable physical dynamics (e.g. ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work we have demonstrated the benefits of using diffusion generative models in simple inverse designs tasks where we want to sample from high probability regions of a target distribution 7(a) defined via E, while having access to optimization data. We analyzed energy-based and conditional guidance where the energy function involves rolling out a GNN. We find that energy guidance is a viable option, but conditional guidance works better in practice, and that performance depends heavily on...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.04686v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent h...\n",
      "\n",
      "--- METHOD ---\n",
      "s and is more suited to tasks like infill, which require conditioning on both past and future sequence elements. In this work, we combine parallel iterative decoding with acoustic token modeling, and apply them to music audio synthesis. To the best of our knowledge, ours is the first ' extension of parallel iterative decoding to neural audio music generation. Our model, called VampNet, can be ' While our work was under peer review, Google released SoundStorm [6], which leverages a similar parall...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "S Our experiments aim to evaluate VampNet’s capability to both compress and generate music, given the various prompting strategies described in Section 3.4. For our objective audio quality measures, we use a multiscale mel reconstruction error and the Fréchet Audio Distance (FAD). Mel-reconstruction error is defined as the L1 distance between log-mel spectrograms at various time-scales, Deu = Sear — Spuii (3) where F' is the FFT size of each spectrogram, and M is the number of mel-frequency bins...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduced VampNet, a masked acoustic token modeling approach to music generation. VampNet is bidirectional, and can be prompted a variety of ways using an input audio file. Through different prompting techniques, VampNet can operate in a continuum between music compression and generation, and is an excellent tool for generating variations on a piece of music. With VampNet, a musician could record a short loop, feed it into VampNet, and have VampNet create musical variations on the recorded i...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01477v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—We present the HANDAL dataset for categorylevel object pose estimation and affordance prediction. Unlike previous datasets, ours is focused on robotics-ready manipulable objects that are of the proper size and shape for functional grasping by robot manipulators, such as pliers, utensils, and screwdrivers. Our annotation process is streamlined, requiring only a single off-the-shelf camera and semi-automated processing, allowing us to produce high-quality 3D annotations without crowd-sourcing. Th...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "If robots are to move beyond simple pushing and indiscriminate top-down grasping, they must be equipped with detailed awareness of their 3D surroundings [1]. To this end, high-quality 3D datasets tailored to robotics are needed for training and testing networks. Compared to the many large-scale 2D computer vision datasets (e.g., ImageNet [2], COCO [3], and OpenImages [4]), existing 3D robotics datasets tend to be rather small in size and scope. The primary challenge in 3D is the annotation of re...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s. Category-level object pose estimation. Category-level object pose estimation is a relatively recent problem that has been gaining attention lately. A natural extension of instancelevel object pose estimation [5], [21]-[25], category-level pose estimation [15], [26]-[29] addresses broad classes of objects instead of known sets of object instances, and is therefore potentially more useful for robotics applications. The category-level datasets mentioned above have enabled recent research in this...\n",
      "\n",
      "--- METHOD ---\n",
      "was evaluated on the NOCS-REAL275 dataset. In follow-up work, CPS [30] also evaluates on REAL275, but this method only requires an RGB image at inference time. The method was trained in a self-supervised manner using unlabeled RGBD images, and it also infers the shape of the instance as a low-dimensional representation. The method of DualPoseNet [31] uses a pair of decoders, along with spherical fusion, to solve this problem using a single RGBD image. The method was also evaluated on NOCS-REAL27...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. Secondly, since all our objects support functional grasping, that is, grasping for a particular use—as opposed to indiscriminate grasping—some objects may require anthropomorphic robotic hands rather than parallel-jaw grippers. We expect that our dataset will open avenues for further research in this area. The objects are composed of a variety of materials and geometries, including some with reflective surfaces and some with perforated or thin surfaces. The many challenges introduced by such ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented a large dataset of images annotated with 6-DoF category-level pose and scale for robotics. This dataset is the largest non-outsourced of its kind. As such, it provides lessons on how other researchers can collect such data in the future. By capturing dynamic scenes, full 360° scans, and occlusions, and providing object affordances and 3D reconstructions, our dataset provides unique characteristics for perceptual learning for robotic manipulation and functional grasping. REFEREN...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.03203v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models trained on code have shown great potential to increase productivity of software developers. Several executionbased benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven’t b...\n",
      "\n",
      "--- METHOD ---\n",
      "is not only more efficient, but also applicable to code in the wild. For...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08827v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topi...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The advent of open-domain Large Language Model (LLM)-based chat systems like ChatGPT and Bing Chat has ushered in a new age of dialogue systems. Previously, dialogue systems were relatively constrained in their scope and abilities, typically confined to either narrow task-oriented conversations or social chitchat (Gao et al., 2018). By contrast, LLM-based chat systems are remarkable because they can converse fluidly with users over a seemingly infinite range of topics, and can accomplish User: P...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "5.1 Dialogue State Tracking To accurately track the passage of Human-AI conversation, robust state tracking is crucial toward inferring user intentions and goals. Since the introduction of the MultiWOZ (Budzianowski et al., 2018) dataset to the community, a plethora of techniques have been proposed to improve DST performance. Earlier attempts including copy mechanism (Lei et al., 2018), transfer learning (Wu et al., 2019), data augmentation (Zhang et al., 2020), contrastive pretraining (Wu et al...\n",
      "\n",
      "--- METHOD ---\n",
      "s extract new elements of state at each turn (Hu et al., 2022). However, this is because DST evaluation benchmarks make the relatively narrow assumption that users provide new and relevant elements of intent at each turn, and that intents build upon or complement each other but do not fundamentally change or conflict throughout the conversation. As we have previously discussed, open-domain dialogue exhibits far more varied characteristics, and multiintent and/or multi-domain conversations are re...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and analysis: We conduct extensive experiments on both proprietary and public datasets, achieving large gains over comparable zero-shot prompts. S3DST achieves state-of-the-art zero-shot per 'The use of Bing Chat logs is in compliance with the terms formance on the MWOZ 2.1 and 2.4 DST benchmarks, alongside the DialSeg711 dialogue topic segmentation benchmark. 2 Problem Definition Informally, the goal of traditional DST is to predict the dialogue state y, given a sequence of user and agent utt...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "LLM-based chat systems have broadened the horizons of human-AI conversation, warranting new methods for tracking user intentions. Therefore, we bring dialogue state tracking in the realm of open-domain dialogue systems by jointly tracking topically coherent segments and state intent variables per segment. Since this requires the assumption of a zero-shot setting due to the impracticality of annotation across all disciplines, we propose S3-DST, a structured segmentation and state tracking approac...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.06440v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a lowcost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEA...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Hand dexterity has been critically responsible for human cognition through active manipulation, tool use, and governing how humans learn from the world [1, 2, 3]. Replicating the dexterity of the human hand with a robot hand has been a longstanding challenge in robotics. Machine learning techniques have recently shown promise in areas such as learning from humans. However, unlike the learning successes in locomotion [4, 5] across truly diverse terrains, robotic manipulation results in the real w...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Robot Hands Shadow [16] and ADROIT [27] hands paved the way to enable complex, contact-rich dexterous tasks with an anthropomorphic ball joint MCP. [11, 17]. However, they are costly (100k USD) and require constant maintenance. In contrast, the Inmoov hand is 3D printable, tendon-driven, and human-like[18]. It has only one DoF per finger and is reliant on tendon actuation which is difficult to calibrate and can be inaccurate. Bauer et. al. [28] present a soft tendon-driven hand that is very flex...\n",
      "\n",
      "--- METHOD ---\n",
      "s from human hands to LEAP Hand, and an Isaac Gym simulation environment at https://leap-hand.github.io/. Il. RELATED WORK Robot Hands Shadow [16] and ADROIT [27] hands paved the way to enable complex, contact-rich dexterous tasks with an anthropomorphic ball joint MCP. [11, 17]. However, they are costly (100k USD) and require constant maintenance. In contrast, the Inmoov hand is 3D printable, tendon-driven, and human-like[18]. It has only one DoF per finger and is reliant on tendon actuation wh...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s while being 1/8th of the cost. We release detailed assembly instructions, the Sim2Real pipeline and a development platform with useful APIs on our website at https://leap-hand.github.io/ I. INTRODUCTION Hand dexterity has been critically responsible for human cognition through active manipulation, tool use, and governing how humans learn from the world [1, 2, 3]. Replicating the dexterity of the human hand with a robot hand has been a longstanding challenge in robotics. Machine learning techni...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "AND FUTURE WORK We introduce LEAP Hand and its core design principles. Following these principles, we demonstrate that LEAP Hand can perform exceedingly well compared to other hands on the market in strength, grasping, and durability. We show its usefulness in a variety of real-world tasks, including teleoperation, behavior cloning, and sim2real. We open source the URDF model, 3D CAD files, and a development platform with useful APIs. In future work, we plan to develop and integrate LEAP Hand wi...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.13908v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text-to-3D generation has recently garnered significant attention, fueled by 2D diffusion models trained on billions of image-text pairs. Existing methods primarily rely on score distillation to leverage the 2D diffusion priors to supervise the generation of 3D models, e.g., NeRF. However, score distillation is prone to suffer the view inconsistency problem, and implicit NeRF modeling can also lead to an arbitrary shape, thus leading to less realistic and uncontrollable 3D generation. In this wo...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently, phenomenal advancements have been made in the field of text-to-image generation [38, 39, 42, 44, 59], mainly due to the significant achievements in large aligned image-text datasets [47], vision-language pre-training models [20, 24, 37], and diffusion models [9, 15, 42]. Inspired by these text-to-image generation results, many works have explored text-conditional diffusion models in other modalities, e.g., text-to-video [16, 17, 48] and text-to3D [19, 25, 28, 36, 54]. In this work, we ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Text-to-Image Generation. Image generation achieves the first breakthrough results when encountering Generative Adversarial Networks (GANs) [13, 21], which train a generator to synthesize images that are indistinguishable from real images. Recently, image generation has achieved another phenomenal progress with the development of diffusion models [49]. With the improvements in modeling [9, 15], denoising diffusion models can generate various high-quality images by iteratively denoising a noised ...\n",
      "\n",
      "--- METHOD ---\n",
      "s primarily rely on score distillation to leverage the 2D diffusion priors to supervise the generation of 3D models, e.g., NeRF. However, score distillation is prone to suffer the view inconsistency problem, and implicit NeRF modeling can also lead to an arbitrary shape, thus leading to less realistic and uncontrollable 3D generation. In this work, we propose a flexible framework of Points-to-3D to bridge the gap between sparse yet freely available 3D points and realistic shape-controllable 3D g...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that Points-to-3D can significantly alleviate inconsistency across views and achieve good controllability over 3D shapes for text-to-3D generation. 2 RELATED WORK Text-to-Image Generation. Image generation achieves the first breakthrough results when encountering Generative Adversarial Networks (GANs) [13, 21], which train a generator to synthesize images that are indistinguishable from real images. Recently, image generation has achieved another phenomenal progress with the deve...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S In this work, we propose Points-to-3D, a novel and flexible text-to3D generation framework. We inspire our framework by alleviating the view inconsistency problem and improving the controllability of 3D shapes for 3D content generation. To control the learned geometry, we innovatively propose to distill the geometry knowledge (sparse 3D points) from the 3D point cloud diffusion model (Point-E). To better utilize the sparse point cloud, we propose an efficient point cloud guidance loss to adapt...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01320v1.pdf ---\n",
      "-------------------------\n",
      "--- 2307.00119v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Metatraining allows one to leverage smaller models for few-shot generalization in a domaingeneral and task-agnostic manner (Min et al., 2022a; Wei et al., 2022; Chen et al., 2022); however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose met...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have become increasingly popular due to their impressive fewshot performance on many NLP tasks and domains (Brown et al., 2020; Chowdhery et al., 2022). This has resulted in many few-shot learning methods based on LLMs that require ever-larger GPUs and *Work done as an intern at Meta. question: How many casualties were there in World War 1? Output (y)) \\n answer: 40,000,Retrieved Demonstrations, (adi) ‘Memory Bank (7) t question: How many casualties were there in Wor...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Meta-learning (Vilalta and Drissi, 2002; Finn et al., 2017) is a class of...\n",
      "\n",
      "--- METHOD ---\n",
      "s alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose meta-training with demonstration retrieval, where we use a dense passage retriever to retrieve semantically similar labeled demonstrations to each example for more varied supervision. By separating external knowledge from model parameters, we can use meta-training to train parameter-efficient models that generalize well on a larger ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our method (§3) outperforms tailored efficient few-shot baselines and other retrieval-augmented models on various tasks, including natural language inference (NLD, paraphrase detection, and extractive question answering (§5). To our knowledge, our work is the first to combine retrieval with meta-training (or multitask training more broadly), to use DPR models to retrieve demonstrations, and to leverage demonstrations from many tasks simultaneously, rather than retrieving random or k-...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We have proposed a meta-training method (§3.2) that retrieves (§3.1) semantically similar demonstrations from a diverse demonstration bank (§3.3). Our method achieves higher performance on average across many tasks than other strong parameterefficient few-shot baselines ($5). In future work, one could explore a mixture of demonstration retrieval and passage retrieval for improved performance on a wider variety of tasks—including knowledge-intensive tasks. Limitations Our method requires access...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.05410v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to offthe-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limiting assumptions, such as a prior pose distribution or coarse pose initialization, making them...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "NeRF [35] was introduced as a powerful method to tackle the problem of learning neural scene representations and photorealistic view synthesis, and subsequent research has focused on addressing its limitations to extend its applicability to a wider range of use cases (see [55, 60] for surveys). One of the few remaining hurdles for view synthesis in the wild is the need for accurate localization. As images captured in the wild have unknown poses, these ap “Work done during an internship at Google...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Structure from motion (SfM). Jointly recovering 3D scenes and estimating camera poses from multiple views of a scene is the classic problem in Computer Vision [25]. Numerous techniques have been proposed for SfM [41, 49] with unordered image collections and visual-SLAM for sequential data [54, 38]. These techniques are largely built upon local features [32, 45, 22, 52] and require accurate detection and matching across images. The success of these techniques has led to their widespread adoption,...\n",
      "\n",
      "--- METHOD ---\n",
      "to tackle the problem of learning neural scene representations and photorealistic view synthesis, and subsequent research has focused on addressing its limitations to extend its applicability to a wider range of use cases (see [55, 60] for surveys). One of the few remaining hurdles for view synthesis in the wild is the need for accurate localization. As images captured in the wild have unknown poses, these ap “Work done during an internship at Google. Project website Varun Jampani? Abhishek Kar?...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.09233v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP’s compositional visio-linguistic reasoning. Our approach fine-tunes CLIP usi...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, multimodal models like CLIP (Radford et al., 2021a) have excelled in tasks such as zero-shot classification, image-text retrieval, and image-captioning (Mu et al., 2021; Yu et al., 2022; Li et al., 2022; Mokady et al., 2021). These models are also crucial components in various state-ofthe-art pipelines for tasks like segmentation and object detection (Wang et al., 2021; Liiddecke and Ecker, 2021; Minderer et al., 2022; Zhong et al., 2021). However, they struggle with visiolingui...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s While CLIP models (Radford et al., 2021a) are renowned for their robust zero-shot classification, recent research (Thrush et al., 2022; Diwan et al., 2022) has exposed their limitations in visio-linguistic reasoning. In contrast, recent studies have demonstrated that text-to-image models (Clark and Jaini, 2023; Li et al., 2023; Krojer et al., 2023; Chen et al., 2023) outperform CLIP in reasoning tasks. These models in fact leverage scores computed from the diffusion objective. We note that whi...\n",
      "\n",
      "--- METHOD ---\n",
      "to enhance CLIP’s compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like StableDiffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-de...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section, we empirically validate our proposed method SDS-CLIP on two types of tasks: i) visio-linguistic reasoning using two challenging benchmarks (Winoground, ARO) and ii) zero-shot image classification using a suite of downstream datasets (ImageNet, CIFAR-100, and others). Overall, we show that our method improves CLIP’s performance significantly on Winoground and some key tasks in ARO, while also marginally improving downstream zero-shot classification performance. 4.1 Experimental...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Our paper introduces SDS-CLIP, a novel data and parameter-efficient method that effectively enhances CLIP’s visio-linguistic reasoning abilities by distilling knowledge from text-to-image models, without compromising its zero-shot abilities. --- --7 Limitations The primary limitation of our method is the inability to use large batch-sizes on moderate size GPUs. This is due to the fact that the regularizer Lg pgs requires a full backward pass through the UNet, even though its parameters are froze...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03027v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "In recent years, we have seen tremendous growth in the importance of digital applications that rely on photo-realistic rendering of images from captured scene representations, both in society and industry. In particular, the synthesis of novel views of dynamic human faces and heads has become the center of attention in many graphics applications ranging from computer games and movie productions to settings in virtual or augmented reality. Here, the key task is the following: given a recording of...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Modeling and rendering human faces is a central topic in graphics and plays a crucial role in many applications, such as computer games, social media, telecommunication, and virtual reality. 2.1. 3D Morphable Models 3D morphable models (3DMMs) have been a staple approach over the last two decades. The use of a unified mesh topology enables representing identity and expression using simple statistical tools [Blanz and Vetter 1999; Li et al. 2017]. With the additional use of texture, one can alrea...\n",
      "\n",
      "--- METHOD ---\n",
      "is capable of synthesizing highly realistic novel views of human heads in complex motion. Our renderings from unseen views (right) faithfully represent static scene parts and regions undergoing highly non-rigid deformations. Along with our method, we publish our high-quality multi-view video capture data of 31.7 million frames from a total of 222 subjects. We focus on reconstructing high-fidelity radiance fields of human heads, capturing their animations over time, and synthesizing re-renderings...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we explore the design choices of our method and demonstrate that our approach outperforms state-of-the-art dynamic radiance field approaches by a significant margin. 1We will release all of our captured data, including all 4734 recordings and baseline codes, along with a new public benchmark to support further research in the area. Website: https: //tobias-kirschstein. github. io/nersemble Authors’ addresses: Tobias Kirschstein, Technical University of Munich, Germany, tobias.kirschstein@tum....\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we have proposed a new method and dataset focusing on the radiance field reconstruction of animated human heads from multi-view video inputs. To this end, we have introduced a novel multi-view video benchmark of diverse human heads containing over 220 identities with 4700 sequences. We further proposed a new method for generating photo-realistic re-renderings of arbitrary viewpoints and time steps, and hope that our dataset and accompanying benchmark will be an important contributi...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.00908v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper addresses the issue of modifying the visual appearance of videos while preserving their motion. A novel framework, named MagicProp, is proposed, which disentangles the video editing process into two stages: appearance editing and motion-aware appearance propagation. In the first stage, MagicProp selects a single frame from the input video and applies image-editing techniques to modify the content and/or style of the frame. The flexibility of these techniques enables the editing of arb...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Content creation often involves video editing, which includes modifying the appearance or adjusting the motion of raw videos [Wu et al., 2023; Kasten et al., 2021; Zhao et al., 2023; Wang et al., 2023]. Filmmakers may need to adjust the exposure, saturation, and contrast of raw videos for better aesthetic *Equal contribution. --- --quality, while advertisers may want to change realistic videos into certain fascinating styles to impress target audiences. This paper addresses the problem of editin...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S AND PRELIMINARIES In this section, we first review recent related works on the appearance editing of videos. We categorize them into two groups, i.e., editing a video frame by frame via image models, and modeling the whole frame sequence for editing. Then, we introduce the preliminaries about diffusion probabilistic models and the notation for video editing. 2.1 RELATED WORKS Frame-by-frame Editing Diffusion-based image generation models have achieved great success in image generation and edit...\n",
      "\n",
      "--- METHOD ---\n",
      "s based on image models, such as Stable Diffusion [Rombach et al., 2022] and ControlNet [Zhang and Agrawala, 2023], can flexibly modify the content or style of any arbitrary region, but it is challenging to ensure temporal consistency across adjacent frames. To alleviate this issue, some use structure-guided models and cross-frame attention to align color and layout across frames [Zhang and Agrawala, 2023; Qi et al., 2023; Ceylan et al., 2023]. Other methods exploit inter-frame correspondence, s...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05845v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The proliferation of video content demands efficient and flexible neural networkbased approaches for generating new video content. In this paper, we propose a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture [I] and incorporating ControlNet to enable additional i...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The rapid growth of video content on the internet has led to a surge of interest in neural network-based approaches for generating new video content. However, training Text-to-Video models is challenging due to the lack of open datasets of labeled video data. Furthermore, generating video from existing Text-to-Video models is difficult due to the nature of prompts. To address these challenges, we propose a novel approach that combines the benefits of zero-shot text-to-video generation with the r...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "In this section, we discuss two key papers that have significantly influenced our work, namely the Text-to- Video Zero paper and the ControlNet paper. 2.1 Text-to-Video Zero The Text-to-Video Zero paper [I] addresses the challenge of generating videos from textual prompts without relying on computationally heavy training and large-scale video datasets. The authors introduce a zero-shot text-to-video generation approach that leverages existing text-to-image synthesis...\n",
      "\n",
      "--- METHOD ---\n",
      "takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture [I] and incorporating ControlNet to enable additional input conditions. By first interpolating frames between the inputted sketches and then running Text-to-Video Zero using the new interpolated frames video as the control technique, we leverage the benefits of both zero-shot text-to-video generation and the robust control provided by Control...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that our method excels at producing high-quality and remarkably consistent video content that more accurately aligns with the user’s intended motion for the subject within the video. We provide a comprehensive resource package, including a demo video, project website, open-source GitHub repository, and a Colab playground to foster further research and application of our proposed method. 1 Introduction The rapid growth of video content on the internet has led to a surge of interest ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and Discussions In this work, we presented STF (Sketching the Future), a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, providing a more accurate representation of the desired motion compared to Text2Video-Zero with pure prompting. Our experiments demonstrated that the inclusion of sketched frames allowed ST...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.12169v5.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—This paper presents a low-cost network architecture for training large language models (LLMs) at hyperscale. We study the optimal parallelization strategy of LLMs and propose a novel datacenter network design tailored to LLM’s unique communication pattern. We show that LLM training generates sparse communication patterns in the network and, therefore, does not require any-to-any full-bisection network to complete efficiently. As a result, our design eliminates the spine layer in traditional GPU...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) are among the most complex and computationally intensive Deep Neural Networks (DNNs). The GPT3 model from 2020 already requiresGPU-years on Nvidia’s V100 GPUs (1. 2], while the recent GPT4 model is estimated to have trillions of parameters and takes months to train As Moore’s law slows down, the growth rate of LLM size and computation requirement exceeds the advancement of accelerators, making hyper-scale GPU datacenters inevitable. Our conversations with lead machin...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "LLM trend. The current growth rate of LLM computational and speed requirement outpaces the advancements in AI accelerators and network speed as Moore’s law slows down, necessitating hyper-scale clusters and more efficient interconnects 33). The MegatornLM line of work pioneers LLM parallelization (2i}. (28}, [34]. Our position to remove 'Cost: Pricepp = $199 per transceiver, Pricegy = $694 per switch port for 400 Gbps; Power: PowerrR = 9W per transceiver, Powersw = 18W per switch port TABLE II N...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we examine and analyze the traffic pattern of LLM training with hybrid parallelism. We propose a novel Rail-only architecture that aligns with LLMs’ distinct characteristics and demands. Our architecture leads to 38% to 77% cost reductions and 37% to 75% power savings while maintaining identical performance to the state-of-the-art GPU networks. ACKNOWLEDGMENTS We thank anonymous Hot Interconnects reviewers for their feedback. The MIT-affiliated authors are supported by DARPA FastN...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.06092v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Common editing operations performed by professional photographers include the cleanup operations: deemphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer’s attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image e...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In everyday photography, the composition of a photo typically encompasses subjects on which the photographer intends to focus our attention, rather than other distracting things. When distracting things cannot be avoided, photographers routinely edit their photos to de-emphasize them. Conversely, when the subjects are not sufficiently visible, photographers routinely emphasize them. Among the most common emphasis and de-emphasis operations performed by professionals are the elementary ones: chan...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Various image enhancement...\n",
      "\n",
      "--- METHOD ---\n",
      "that can be appli Eric Kee? Eli Shechtman? Yagiz Aksoy! ? Adobe Research Step 4 (Ours) the image to de emphasize objects (steps 1, 2) or enhance subjects (steps 3, 4). (bottom) Our novel realism loss allows us to apply realistic edits to a wide variety of objects while state-of-the-art methods [1, |7] may generate less realistic editing results. Abstract Common editing operations performed by professional photographers include the cleanup operations: deemphasizing distracting elements and enhanc...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show, OHR [15] tends to generate unrealistic color changes that are semantically incorrect, and WRS [23] is limited to contrast and saturation adjustments with limited effectiveness. Recent works leverage saliency estimation networks [3, 7, 10, 14, 19] to optimize for a desired saliency map instead of relying on prior saliency cues. Saliency models are trained to output a heatmap that represents where human gaze would be concentrated in an image. These models are not trained to respond to the ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future work We describe a method to edit images using conventional image editing operators to attenuate or amplify the attention captured by a target region while preserving image realism. Realism is achieved by introducing an explicit, and separate realism network that is pre-trained to distinguish edited images. This strategy to achieve realism is distinct from prevailing approaches, including adversarial training schemes, as it introduces an additional form of weak supervision— manually s...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.03291v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "One of the main challenges in creating deep neural models over structured distributions is the difficulty of their implementation on modern hardware accelerators. SynJax addresses this problem and makes large scale training of structured models feasible and easy in JAX. We hope that this will encourage research into finding alternatives to auto-regressive modeling of structured data. Limitations SynJax is quite fast, but there are still some areas where the improvements could be made. One of the...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.13776v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlappi...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Until recently, convolutional neural network (CNN) had been leading the remarkable innovations in computer vision tasks which had otherwise been considered too difficult in the past, such as autonomous driving [2-7]. However, the leading role of CNNs is recently being transferred to Transformer-based networks [1, 8,9]. The Transformer model was first proposed for natural language processing (NLP) tasks, such as text classification and machine translation, and it has demonstrated great success [1...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Convolutional Neural Network (CNN): Over the past decade, CNNs have been the de facto standard in computer vision, and keep improving accuracy with innovations in architecture design [2-5]. In parallel, a lot of efforts have also been made to reduce the complexity of CNN models for efficiency. Such directions include model compression, quantization, and low cost operations such as depth-wise convolution [6,7]. Although CNNs are still dominant in computer vision tasks, many recent works have demo...\n",
      "\n",
      "--- METHOD ---\n",
      "4.1. Overview of Swin-Free Our baseline architecture shown in Figure 3 is similar to Swin Transformer [1], except that it does not use the shifted windows. The input image is first patchified. Each stage applies a number of Swin-style Transformer blocks for the patches, where the self-attention computation is done within each of non-overlapping local windows. Here, the local window operates on an M x M patch. Like in Swin Transformer, the number of patches are reduced by half at each stage by th...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that Swin-Free featuring the size-varying windows reduces the model runtime significantly as compared to Swin Transformer, mainly thanks to avoiding shifting windows and being able to leverage faster matrix multiplication with larger inputs. Note that on modern GPUs, efficient implementations of math operations such as convolution with large kernels are widely available. In Swin-Free, a larger portion of its runtime is --- --Stage1% Block 2\"4 Block Window Partition Window LayerNo...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper presents Swin-Free, which attempts to improve latency over Swin Transformer by reducing memory traffic incurred by shifted window scheme. Instead, SwinFree varies the size of windows over stages, which mimics the mechanism of the shifted windows. This simple technique is shown to offer reduced latency and better accuracy compared to its Swin counterpart. We also show that fur --- --Table 5. Latency and accuracy according to the variation in window size at each stage of Swin-B without ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.03907v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "A ChatGPT-like system for drug compound analysis could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structureactivity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGP...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The process of drug discovery and development is a time-intensive and costly endeavor, often taking years and billions of dollars to bring a single drug to market (Avorn, 2015). This process involves the exploration and understanding of vast chemical spaces and the intricate relationships between molecular structures and their biological activities, commonly known as structure-activity relationships (SAR) (Idakwo et al., 2020). Traditional methods (Rycker et al., 2018) often involve laborious it...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Al-based drug properties analysis (Paul et al., 2021) is a promising approach to significantly reduce costs and time associated with the traditional drug discovery and development --- --DRUGCHAT: TOWARDS ENABLING CHATGPT-LIKE CAPABILITIES ON DRUG MOLECULE GRAPHS pipeline. For example, ImageMol (Zeng et al., 2022) is an unsupervised pretraining...\n",
      "\n",
      "--- METHOD ---\n",
      "s (Rycker et al., 2018) often involve laborious iterative testing, with a high rate of late-stage failures. Recent advancements in computational chemistry and chemoinformatics (Zeng et al., 2022) have offered some respite, but there is still a pressing need for tools that can intuitively understand and generate meaningful insights from the complex data inherent in molecular graphs of drug compounds. © Y. Liang*, R. Zhang*, L. Zhang & P. Xie. --- --DRUGCHAT: TOWARDS ENABLING CHATGPT-LIKE CAPABILI...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al datasets. Large language models (LLMs) (Brown et al., 2020) have demonstrated outstanding capabilities in generating inventive text, responding to reading comprehension queries, mathematical reasoning, etc. Nevertheless, the weight parameters of some of the most powerful LLMs are not publicly available, which considerably hinders academic research. Moreover, early LLMs were limited to processing text information as input, restricting their ability to understand information in other modalities...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s, Limitations and Future Work In this report, we present the DrugChat framework, designed to answer questions and generate text descriptions for drugs from their molecular graphs. We develop the ChEMBL Drug Dataset and the PubChem Drug Dataset to train the DrugChat model. With further development and evaluation, DrugChat has the potential to enable conversational analysis of drug compounds. A potential limitation of DrugChat is language hallucination. Since DrugChat incorporates an LLM module, ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.13785v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Avoiding synthesizing specific visual concepts is an essential challenge in responsible visual synthesis. However, the visual concept that needs to be avoided for responsible visual synthesis tends to be diverse, depending on the region, context, and usage scenarios. In this work, we formalize a new task, Open-vocabulary Responsible Visual Synthesis (44 ORES), where the synthesis model is able to avoid forbidden visual concepts while allowing users to input any desired content. To address this p...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "With the development of large-scale model training, visual synthesis models are capable of generating increasingly realistic images (Ramesh et al. 2021; Rombach et al. 2022; Saharia et al. 2022). Due to the growing risk of misuse of synthesized images, responsible AI has become increasingly important (Arrieta et al. 2020; Wearn, Freeman, and Jacoby 2019; Smith et al. 2022), especially to avoid some visual features, such as, nudity, sexual discrimination, and racism, during synthesis. However, re...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Responsible Visual Synthesis In recent years, responsible visual synthesis has gained significant attention. Some works (Rombach et al. 2022) use Not-Safe-For-Work (NSFW) classifier to filter out risky output. However, this needs extra time to re-generate new images and relies on a filtering model pre-trained on specific concepts, which makes it challenging to detect open-vocabulary visual concepts. STABLE DIFFUSION (Rombach et al. 2022) offers a...\n",
      "\n",
      "--- METHOD ---\n",
      "in reducing risks of image generation. Our work highlights the potential of LLMs in responsible visual synthesis. Our code and dataset is public available in https://github.com/kodenii/ORES. 1 Introduction With the development of large-scale model training, visual synthesis models are capable of generating increasingly realistic images (Ramesh et al. 2021; Rombach et al. 2022; Saharia et al. 2022). Due to the growing risk of misuse of synthesized images, responsible AI has become increasingly im...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate the effectiveness of our method in reducing risks of image generation. Our work highlights the potential of LLMs in responsible visual synthesis. Our code and dataset is public available in https://github.com/kodenii/ORES. 1 Introduction With the development of large-scale model training, visual synthesis models are capable of generating increasingly realistic images (Ramesh et al. 2021; Rombach et al. 2022; Saharia et al. 2022). Due to the growing risk of misuse of synthe...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper proposed a novel task termed Open-vocabulary Responsible Visual Synthesis (€ ORES), wherein the synthesis model must refrain from incorporating unspecified visual elements while still accommodating user inputs of diverse content. To tackle this issue, we designed Two-stage Intervention (TIN) framework, which encompassed two key stages: 1) rewriting with learnable instruction and 2) synthesizing with prompt intervention on a diffusion synthesis model and a large-scale language model (L...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10853v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360°-view experien...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The field of computer vision has seen significant advancements in recent years, particularly in the area of generative AI. In the domain of image generation, Stable Diffusion has revolutionized content creation by providing open software to generate arbitrary high-fidelity RGB images from text prompts. This work builds on top of Stable Diffusion [20] v1.4 and proposes a Latent Diffusion Model for vasudev.lal@intel.com 3D (LDM3D). Unlike the original model, LDM3D is capable of generating both ima...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Monocular depth estimation is the task of estimating depth values for each pixel of a single given RGB image. Recent work has shown great performance in depth estimation using deep learning models based on convolutional neural networks [11, 12, 14,22, 28,29]. Later, attentionbased Transformer models were adopted to overcome the issue of a limited receptive field in CNNs, allowing the model to consider global contexts when predicting depth values [3,5, 19,30]. Most recently diffusion models have ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360°-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize conten...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In conclusion, this research paper introduces LDM3D, a novel diffusion model that generates RGBD images from text prompts. To demonstrate the potential of LDM3D we also develop DepthFusion, an application that creates immersive and interactive 360-view experiences using the generated RGBD images in TouchDesigner. The results of this research have the potential to revolutionize the way we experience digital content, from entertainment and gaming to architecture and design. The contributions of th...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.04603v5.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Consistency and reliability are crucial for conducting Al research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve thi...\n",
      "\n",
      "--- METHOD ---\n",
      "s are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, a protein folding framework that supports significant components of state-of-the-art models in the manner of an off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliability and consistency of proposed models and give efficiency in both speed and costs, resulting in acceleration on protein folding modeling research. The code is available at https://github.com/kakaobrain/solvent and the project will continue to be developed. 1 Introduction Deep-learning-based protein structure prediction task has become an essential research area after the witnes...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "To support a consistent and easy-to-use research framework, we propose Solvent for protein folding research. We hope that efficient and rigorous experiments on top of the Solvent will further prove the strengths and weaknesses of each algorithm and finally accelerate structural prediction research. Currently, Solvent focuses on MSA-free protein structure prediction models. We will extend Solvent to a more general way that takes MSA and template as input and support more validation data such as o...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11337v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The techniques for 3D indoor scene capturing are widely used, but the meshes produced leave much to be desired. In this paper, we propose “RoomDreamer”, which leverages powerful natural language to synthesize a new room with a different style. Unlike existing image synthesis methods, our work addresses the challenge of synthesizing both geometry and texture aligned to the input scene structure and prompt simultaneously. The key insight is that a scene should be treated as a whole, taking into ac...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Commercial depth sensors [42] and LiDAR sensors [9] on mobile devices have opened a new era in 3D scene capturing for millions of users in their everyday lives. However, the quality of the meshes acquired by these sensors leaves much to be desired, often exhibiting issues such as holes, distorted objects, and blurred textures. In addition, users typically find their surroundings lack variation and may want to further edit and stylize the scene. To solve Video results: https://youtu.be/p4xgwj4QJc...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s The domain of 3D content creation [16, 32, 33] has significantly improved in recent years. We consider research in this field into two categories. Firstly, using 3D ground truth content for supervision to direct content generation process [5, 13, 23], which is limited due to the availability of high-quality 3D ground truth. The second research category focuses on using the power of existing 2D image generators [29] for 3D content creation. Poole et al. [25] proposed Score Distillation Sampling...\n",
      "\n",
      "--- METHOD ---\n",
      "aims at jointly improving geometry and generating texture for an input indoor mesh. The upper figure shows the true room with a panoramic view and a depth map. Then, given a text prompt (in the middle), our model can synthesize new rooms with different styles (in the bottom rows). Note that input mesh is often of low quality, and our method can polish both the texture and geometry. Abstract The techniques for 3D indoor scene capturing are widely used, but the meshes produced leave much to be des...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, through which the effectiveness of our method is demonstrated. 1. Introduction Commercial depth sensors [42] and LiDAR sensors [9] on mobile devices have opened a new era in 3D scene capturing for millions of users in their everyday lives. However, the quality of the meshes acquired by these sensors leaves much to be desired, often exhibiting issues such as holes, distorted objects, and blurred textures. In addition, users typically find their surroundings lack variation and may want to furth...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we tackle the problem of synthesizing a 3D interior scene from text prompts based on a scanned indoor mesh input. We propose a solution that capitalizes on the capabilities of 2D diffusion text-to-image generative models. The primary challenge lies in generating coherent 3D geometry and textural information from the 2D generative priors. To ensure the consistent visual appearance of the whole scene, we first develop a geometry-guided 3D scene texture generation technique. Our key ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.08316v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "With the emerging diffusion models, recently, text-to-video generation has aroused increasing attention. But an important bottleneck therein is that generative videos often tend to carry some flickers and artifacts. In this work, we propose a dual-stream diffusion net (DSDN) to improve the consistency of content variations in generating videos. In particular, the designed two diffusion streams, video content and motion branches, could not only run separately in their private spaces for producing...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In the realm of artificial intelligence generated content, one of the most exciting and challenging tasks is the transformation from text into visual content. This task not only benefits for our understanding of natural language processing but also promotes computer vision techniques. Meantime, it will cause immense potential applications in entertainment, advertising, education, and surveillance. Over the past few years, there has been substantial progress in developing models that convert text...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "The development and evolution of models for converting textual descriptions into visual content have been a consistent focus in the field of artificial intelligence. The research has gradually transitioned from text-to-image mod els to more dynamic and complex text-to-video generation models. Text-to-Image Generation Early efforts were dedicated to developing techniques for text-to-image synthesis. The Denoising Diffusion Probabilistic Model (DDPM) (Ho, Jain, and Abbeel 2020; Song, Meng, and Erm...\n",
      "\n",
      "--- METHOD ---\n",
      ". Abstract With the emerging diffusion models, recently, text-to-video generation has aroused increasing attention. But an important bottleneck therein is that generative videos often tend to carry some flickers and artifacts. In this work, we propose a dual-stream diffusion net (DSDN) to improve the consistency of content variations in generating videos. In particular, the designed two diffusion streams, video content and motion branches, could not only run separately in their private spaces fo...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that our method could produce amazing continuous videos with fewer flickers (see Fig. 1)!. ‘Please see the videos in supplementary material, and more Introduction In the realm of artificial intelligence generated content, one of the most exciting and challenging tasks is the transformation from text into visual content. This task not only benefits for our understanding of natural language processing but also promotes computer vision techniques. Meantime, it will cause immense poten...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This work presented a novel dual-stream diffusion net (DSDN) to improve the consistency of content variations in generating videos. Specifically, the designed two diffusion streams, video content and motion branches, could not only run separately in their private spaces for producing personalized video variations as well as content, but also be well-aligned between the content and motion domains through leveraging our designed cross-transformer interaction module, which would benefit the smoothn...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10973v2.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Deep generative models such as generative adversarial networks (GANs) [Goodfellow et al. 2014] have achieved unprecedented success in synthesizing random photorealistic images. In real-world applications, a critical functionality requirement of such learningbased image synthesis methods is the controllability over the synthesized visual content. For example, social-media users might want to adjust the position, shape, expression, and body pose of a human or animal in a casually-captured photo; p...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Generative Models for Interactive Content Creation Most current...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies > Computer vision. Additional Key Words and Phrases: GANSs, interactive image manipulation, point tracking ACM Reference Format: Xingang Pan, Ayush Tewari, Thomas Leimkithler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. Drag Your GAN: Interactive Pointbased Manipulation on the Generative Image Manifold. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings (SIGGRAPH ’23 Conference Proceedings), August 6-10, 2023, Los An...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, handling more than one point with precise position control enables much more diverse and accurate image manipulation. To achieve such interactive point-based manipulation, we propose DragGAN, which addresses two sub-problems, including 1) supervising the handle points to move towards the targets and 2) tracking the handle points so that their positions are known at each editing step. Our technique is built on the key insight that the feature space of a GAN is sufficiently discriminative to en...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented DragGAN, an interactive approach for intuitive point-based image editing. Our method leverages a pre-trained GAN to synthesize images that not only precisely follow user input, but also stay on the manifold of realistic images. In contrast to many previous approaches, we present a general framework by not relying on domain-specific modeling or auxiliary networks. This is achieved using two novel ingredients: An optimization of latent codes that incrementally moves multiple hand...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.16009v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent end-to-end automatic speech recognition (ASR) systems often utilize a Transformer-based acoustic encoder that generates embedding at a high frame rate. However, this design is inefficient, particularly for long speech signals due to the quadratic computation of self-attention. To address this, we propose a new method, Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be red...\n",
      "\n",
      "--- METHOD ---\n",
      ", Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be reduced, and the inference of both the encoder and joint network is accelerated....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on LibriSpeech show that our method can reduce 57% of tokens and improve the inference speed on GPU by 70% without any notable loss of accuracy. Additionally, we demonstrate that A-ToMe is also an effective solution to reduce tokens in long-form ASR, where the input speech consists of multiple utterances. Index Terms: speech recognition, transducer, adaptive subsampling 1. Introduction The area of end-to-end (E2E) automatic speech recognition (ASR) has seen significant progress in recent years...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we proposed a novel adaptive subsampling method called Adjacent Token Merging that progressively reduces the number of tokens in the encoder of the Transformer transducer. We emphasized the importance of variable frame rates and gradual subsampling. Experiments on utterance-based and long-form ASR showed that our approach could accelerate inference substantially while having minimal impact on recognition performance. Additionally, our approach can provide more flexibility in desig...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.09327v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We propose a method to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this problem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descriptions of the music. This work addresses this challenge with the following three contributions. First, we propose a textsynthesis approach that relies on an analogy-based prompting procedure to generate natural lan...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A key part of the video editing process for creators is choosing a musical soundtrack. Especially given the rise of short-form videos on social media platforms, automated music recommendation systems have become an increas “Work done as an intern with Adobe Research Language-Guided Music Retrieval Video+Language Query Retrieved Music al}fiae he a aa =m Cm =» ‘A somber folk ballad [ener female vocalist, Folk music with guitar guitar, and tambourine 1 the —_ rn =} > (Vim) mm (Pop featuring a femal...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Music and language. There are numerous music tagging datasets which contain tags specifying attributes like mood, genre, or instrumentation [5, 7, 19, 29], and several works have studied training automated music taggers from such datasets [10,20,21,30,42,43]. Beyond these...\n",
      "\n",
      "--- METHOD ---\n",
      "to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this problem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descriptions of the music. This work addresses this challenge with the following three contributions. First, we propose a textsynthesis approach that relies on an analogy-based prompting procedure to generate natural language music descript...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s). ViML Model. Following MVPt [36], we employ the Transformer architecture [37] for our music and video encoders f™ and f’. Transformers play a key role in improving model performance by encoding long-term context from video and music clips. We also use a similar two-layer Transformer architecture for our text encoder f’ and the video-text fusion layer f”’. However, we find that other fusion module architectures such as a single linear layer yield similar results. Please see our supplemental fo...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we introduced an approach to allow language-guided music recommendation for video. We pro Video+Language Query Retrieved Music MVPt+ ViML aiflen alee _ ay Ny en | SS Energetic electronic party song with male vocals over rising synth layers and claps (aligned with video. Percussive Latin music with a male vocalist, Electronic dance pop track synth line, and Cumbia that has male vocals and a, beat aligned with pulsating rhythm. Best for | | video. nightclubs. \\ affiv aiflen / \\ Upbea...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.02053v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recently, the release of INSTRUCTEVAL [Chia et al., 2023] has provided valuable insights into the performance of large language models (LLMs) that utilize encoderdecoder or decoder-only architecture. Interestingly, despite being introduced four years ago, T5-based LLMs, such as FLAN-TS, continue to outperform the latest decoder-based LLMs, such as LLAMA and VICUNA, on tasks that require general problem-solving skills. This performance discrepancy can be attributed to three key factors: (1) Pre-t...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al findings strongly indicate that the enhanced problem-solving abilities of our model, FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly available at https : //huggingface.co/declare-lab/flacuna-13b-v1.0. 1 Introduction ChatGPT and its successor GPT-4 have surpassed their prior state-of-the-art models on a vast majority of the benchmarking tasks and datasets. However, ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Congratulations on your new XYZ household appliance! With proper use and maintenance, your appliance will provide you with years of reliable service. If you have any questions or concerns, please refer to the appliance manual or contact the manufacturer. 4 Limitations and Future Work Despite the promising advancements of FLACUNA compared to VICUNA, we have identified some issues that require addressing: ¢ If FLACUNA is asked to provide descriptive answers to questions like “Present arguments for...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.18286v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In an era where images and visual content dominate our digital landscape, the ability to manipulate and personalize these images has become a necessity. Envision seamlessly substituting a tabby cat lounging on a sunlit window sill in a photograph with your own playful puppy, all while preserving the original charm and composition of the image. We present Photoswap, a novel approach that enables this immersive image editing experience through personalized subject swapping in existing images. Phot...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Imagine a digital world where the boundaries of reality and creativity blur, where a photograph of a tabby cat lounging in a sunlit window sill can effortlessly be transformed to feature your playful puppy in the same pose. Or envision yourself as a part of a famous movie scene, replaced seamlessly with the original character while preserving the very essence and composition of the scene. Can we achieve this level of personalized image editing, not just with expert-level photo manipulation skill...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Text-to-Image Generation In the early stages of text-based image generation, Generative Adversarial Networks (GANs) (2) [14] were widely used due to their exceptional ability to produce high-quality images. These models aimed to align textual descriptions with synthesized images through multi-modal vision-language learning, achieving impressive results on specific domains (e.g., bird, chair and human face). When combined with CLIP , a large pre-trained model that learns visual-textual repres...\n",
      "\n",
      "--- METHOD ---\n",
      "s in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing. * Correspondence to Jing Gu and Xin Eric Wang, {jgui10,xwang366}@ucsc.edu. Preprint. --- --1 Introduction Imagine a digital world where the boundaries of reality and creativity blur, where a photograph of a tabby cat lounging in a sunlit window sill can effortlessly be transformed to feature your playful puppy in the same ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s underscore the efficacy and controllability of Photoswap in personalized subject swapping. Furthermore, Photoswap significantly outperforms baseline methods in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing. * Correspondence to Jing Gu and Xin Eric Wang, {jgui10,xwang366}@ucsc.edu. Preprint. --- --1 Introduction Imagine a digital world where the boundaries of reality and c...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper introduces Photoswap, a novel framework designed for personalized subject swapping in images. To facilitate seamless subject photo swapping, we propose leveraging self-attention control by exchanging intermediate variables within the attention layer between the source image and reference images. Despite its simplicity, our extensive experimentation and evaluations provide compelling evidence for the effectiveness of Photoswap. Our framework offers a robust and intuitive solution for s...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.16934v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pretrained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and imag...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Image generation [22] [4] has made great strides in recent years, especially after breakthroughs in text-to-image generation 34) [I]. The recent text-to-image generation not only dramatically improves the quality of generated images, but also enables the creation of people’s ideas into exquisite paintings and artworks controlled by text. We are very curious whether we could control image creation directly from brain activities (such as electroencephalogram (EEG) recordings), without translating ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s 2.1. Generating images from brain activity The use of brain signals, including fMRI and EEG, to generate images has been an active area of research. For the use of fMRI, traditional...\n",
      "\n",
      "--- METHOD ---\n",
      "for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pretrained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs. Overall, t...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and Analyses 4.1. Implementation details Data for EEG representation pre-training. We have collected approximately 120,000 EEG data samples from over 400 subjects with channel ranges from 30 to 128 on the MOABB platform for the EEG pre-training. MOABB is a software package designed to facilitate the development of brain-computer interface (BCI) algorithms by providing a collection of publicly available EEG datasets in a common format, along with a suite of state-of-the-art algorithms. This pla...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper proposes a novel method, DreamDiffusion, for generating high-quality images from EEG signals, which is a non-invasive and easily obtainable source of brain activity. The proposed method addresses the challenges associated with EEG-based image generation by utilizing the knowledge learned from large EEG datasets and the powerful generative capabilities of image diffusion models. Through a pre-training and fine-tuning scheme, EEG data can be encoded to the representation suitable for im...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.14620v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an endto-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene optimization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. F...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In this paper, we focus on the task of indoor 3D object detection using posed RGB images. 3D object detection is a fundamental task for many computer vision applications such as robotics and AR/VR. The algorithm design depends on input sensors. In the past few years, most 3D detection works focus on both RGB images and depth measurements (depth images, point-clouds, etc.). While depth sensors are widely adopted in applications such as autonomous driving, they are not readily available in most AR...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "3D Detection in Indoor Scene. 3D detection utilizes various...\n",
      "\n",
      "--- METHOD ---\n",
      "for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an endto-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene optimization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the de...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that by explicitly modeling the geometry as an opacity field, we can build a much better volume representation and thereby significantly improve 3D detection performance. Without using depth measurements for training, we improve the state-of-the-art by 3.9 and 3.1 mAP on the ScanNet and the ARKITScenes datasets, respectively. Optionally, if depth measurements are also available for training, we can further leverage depth to improve the performance, while our inference model still does not...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present NeRF-Det, a novel method that uses NeRF to learn geometry-aware volumetric representations for 3D detection from posed RGB images. We deeply integrate multi-view geometry constraints from the NeRF branch into 3D detection through a subtle shared geometry MLP. To avoid the large overhead of per-scene optimization, we propose leveraging augmented image features as priors to enhance the generalizablity of NeRF-MLP. In addition, we sample features from high resolution image...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10763v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "(Ren et al., 2021a) and variational generative mod Improving text representation has attracted much attention to achieve expressive text-tospeech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech, a crossmodal contrastive pre-training framework that explicitly learns the prosody variance of the same text token under different contexts. Specifical...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "With the development of deep learning, the audio quality of modern TTS systems has been improved, yet prosody modeling is still a challenging problem. Previous works on expressive TTS have utilized external variation predictors (prediction-based, PB) *Equal contribution. t Corresponding author. els (variation-based, VB) (Kim et al., 2020; Liu et al., 2022) to inject prosody variance into the TTS model. Another popular direction is to learn better text representation for prosody prediction (Tan e...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Expressive TTS In the past few years, modern neural TTS has made significant progress in high practicality and audio quality (Ren et al., 2019; Kim et al., 2020; Elias et al., 2021; Miao et al., 2021; Kim et al., 2021; Donahue et al., 2021; Jiang et al., 2022). However, modeling expressive prosody given the plain input text is still challenging. To achieve expressive TTS, one common practice is to use a reference encoder and style tokens (Wang et al., 2018; Jia --- --et al., 2018). But it is...\n",
      "\n",
      "--- METHOD ---\n",
      "s, but also demonstrate its generalization ability to adapt to multiple languages and multi-speaker TTS. We also deeply analyze the principle behind the performance of CLAPSpeech. Ablation studies demonstrate the necessity of each component in our method. Source code and audio samples are available at https://clapspeech. github. io. 1 Introduction With the development of deep learning, the audio quality of modern TTS systems has been improved, yet prosody modeling is still a challenging problem....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on three datasets not only show that CLAPSpeech could improve the prosody prediction for existing TTS methods, but also demonstrate its generalization ability to adapt to multiple languages and multi-speaker TTS. We also deeply analyze the principle behind the performance of CLAPSpeech. Ablation studies demonstrate the necessity of each component in our method. Source code and audio samples are available at https://clapspeech. github. io. 1 Introduction With the development of deep learning, t...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ", our experiments demonstrate that CLAPSpeech could help TTS systems synthesize more expressive and prosodic audio. 4.3 Deeper Analysis 4.3.1 Token Representation Self-similarity To better understand the performance superiority of CLAPSPeech over existing representation learning methods for TTS, we analyze the token representation learned by CLAPSpeech and other methods. Following Su et al. (2021), we define the averaged similarity on the selected token under different con °In our PB/VB TTS base...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.13455v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.ABSTRACT Neural fields have achieved impressive advancements in view s...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural radiance fields [Mildenhall et al. 2021], NeuS [Wang et al. 2021] and subsequent research [Liu et al. 2020; Miiller et al. 2022; Wang et al. 2022c] (collectively referred to as neural fields) have made significant progress in scene reconstruction and novel view synthesis. By capturing multi-view images of a 3D scene and using off-the-shelf structure-from-motion models to estimate camera poses, one can train neural networks to learn neural fields that implicitly represent the geometry and ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S 2.1 Text-guided image generation and editing The denoising diffusion probabilistic model [Ho et al. 2020; Song et al. 2020] has drawn great attention for its ability to generate high-quality images. Later, diffusion models [Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022] trained on large-scale image-text paired datasets demonstrated astonishing performance in understanding complex semantics from text prompts (including nouns, adjectives, verbs, etc.) and generating corresponding ...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies — Rendering; Neural networks. ACM Reference Format: Jingyu Zhuang*, Chen Wang\", Liang Lin =o Lingjie Liu & and Guanbin Li © 2023. DreamEditor: Text-Driven 3D Scene Editing with Neural Fields. In SIGGRAPH Asia 2023 Conference Papers (SA Conference Papers ’23), December 12-15, 2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3610548.1 INTRODUCTION Neural radiance fields [Mildenhall et al. 2021], NeuS [Wang et al. 2021] and subsequent research [Liu et ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s have demonstrated that DreamEditor can accurately edit neural fields of real-world scenes according to the given text prompts while ensuring consistency in irrelevant areas. DreamEditor generates highly realistic textures and geometry, significantly surpassing previous works in both quantitative and qualitative evaluations. CCS CONCEPTS + Computing methodologies — Rendering; Neural networks. ACM Reference Format: Jingyu Zhuang*, Chen Wang\", Liang Lin =o Lingjie Liu & and Guanbin Li © 2023. Dre...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "AND LIMITATIONS In this paper, we present DreamEditor, a text-driven framework for editing 3D scenes represented by neural fields. Given a neural field and text prompts describing the desired edits, DreamEditor automatically identifies the editing region within the scene and modifies its geometry and texture accordingly. Experiments across a diverse range of scenes, including faces, objects, and large outdoor scenes, showcase the robust editing capabilities of DreamEditor to generate high-qualit...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03509v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ion through animations and interactive elements (Fig. 2, Fig. 3). 2305.03509v3 [cs.CL] 31 Aug1V ~wXa © ABSTRACT Diffusion-based generative models’ impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for nonexperts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integr...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Diffusion-based generative models [36, 43, 31] like Stable Diffusion [43] and DALL-E [31] have captured global attention for their impressive image creation abilities, from AI developers, designers, to policymakers. However, the popularity and progress of generative AI models have sparked social concerns [44, 9, 11, 12], such as accusations of artistic style theft by developers of AI image generators [1 1, 12]. Policymakers are also discussing ways to combat malicious data generation and revise ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "with simple models and datasets directly in their browsers. To explain more advanced techniques, researchers have developed interactive articles [16, 10, 27, 2, 39, 30], but they often assume prior machine learning knowledge. To address the needs of non-experts, interactive visualization tools such as CNN Explainer [48], GAN Lab [21], and AdversarialPlayground [29] were developed. Inspired by their success, we develop Diffusion Explainer as a web-based interactive visualization to broaden educat...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have introduced Diffusion Explainer, the first interactive webbased visualization tool that explains how Stable Diffusion generates high-resolution images from text prompts. Our tool seamlessly integrates a visual overview with detailed explanations of the underlying operations through animations and interactive elements. Its innovative design uncovers the impacts of prompt keywords on image generation. A user study with 56 non-experts demonstrates the superiority of Diffusion Explainer over ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11308v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Designers may often ask themselves how to adjust their design concepts to achieve demanding functional goals. To answer such questions, designers must often consider counterfactuals, weighing design alternatives and their projected performance. This paper introduces Multi-objective Counterfactuals for Design (MCD), a computational tool that automates and streamlines the counterfactual search process and recommends targeted design modifications that meet designers’ unique requirements. MCD impro...\n",
      "\n",
      "--- METHOD ---\n",
      "For Multi-modal Design Modifications Lyle Regenwetter Massachusetts Institute of Technology Cambridge, MA regenwet @mit.edu Abstract—Designers may often ask themselves how to adjust their design concepts to achieve demanding functional goals. To answer such questions, designers must often consider counterfactuals, weighing design alternatives and their projected performance. This paper introduces Multi-objective Counterfactuals for Design (MCD), a computational tool that automates and streamline...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we will be leveraging pretrained CLIP models to query counterfactuals using text and image prompts. II. METHODOLOGY In this section, we discuss the optimization formulation behind MCD, emphasizing the constraints, objectives, and operators used. We then present our approach for sampling diverse sets of counterfactuals and discuss how we decouple the optimization from the final sampling step. Finally, we demonstrate the capabilities of MCD on a simple 2D problem. A. Objectives for Quality Desi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we have introduced Multi-objective Counterfactuals for Design (MCD), a specialized counterfactual search method for design modification tasks. We first illustrated how design modification can be addressed through counterfactual search and what attributes comprise a strong design modification. We then identified key limitations with existing works, particularly their inability to sample multi-objective queries and the inherent coupling of the optimization and sampling process. Next...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.08205v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We address a benchmark task in agile robotics: catching objects thrown at high-speed. This is a challenging task that involves tracking, intercepting, and cradling a thrown object with access only to visual observations of the object and the proprioceptive state of the robot, all within a fraction of a second. We present the relative merits of two fundamentally different solution strategies: (1) Model Predictive Control using accelerated constrained trajectory optimization, and (ii) Reinforcemen...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Chasing a ball in flight and completing a dramatic diving catch is a memorable moment of athleticism - a benchmark of human agility - in several popular sports. In this paper, we consider the task of tracking, intercepting and catching balls moving at high speeds on a mobile manipulator platform (see Figure 1), whose end-effector is equipped with a Lacrosse head. Within a fraction of a second, the robot must start continuously translating visual observations of the ball into feasible whole body ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Both classes of techniques have been previously applied to the robotic catching task. Examples of optimization-based control for ball catching include Hove and Slotine (1991); Hong and Slotine (1995); Yu et al. (2021); Frese et al. (2001); Kober et al. (2012). Bauml et al. (2010a) and Lampariello et al. (2011) present an unified approach subsuming catch point selection, catch configuration computation and path generation in a single, nonlinear optimization problem (also see, Ko¢ et al. (2018), J...\n",
      "\n",
      "--- METHOD ---\n",
      "s can be extremely data inefficient, but can adapt, in principle, to complex and unknown real world dynamics. Our primary contribution is to provide insights into subtle trade-offs in reaction time, sample efficiency, robustness to distribution shift, and versatility in terms of whole-body multimodal behaviors in a unified...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. We conclude with proposals on fusing “classical” and “learning-based” techniques for agile robot control. Videos of our experiments may be found here: https: //sites.google.com/view/agile-catching. Figure 1: Mobile Manipulator with Lacrosse Head catching a ball within a second. (right) Automatic ball thrower with controllable yaw angles and speed of around 5m/s. 1. Introduction Chasing a ball in flight and completing a dramatic diving catch is a memorable moment of athleticism - a benchmark o...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and future work While the fine-tuned blackbox agent has the highest catching success performance, the SQP agent is much more robust to distribution shifts in the thrower. To obtain the “best of both’, we plan to investigate the following strategies combining blackbox optimization and SQP: ¢ Use blackbox optimization (via CMA, BGS, etc.) to optimize the set of tuneable SQP and cradling parameters 0. ¢ Optimize a (smaller) BB policy to output SQP and cradling parameters @ for each episode, conditi...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.08089v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Controllable video generation has gained significant attention in recent years. However, two main limitations persist: Firstly, most existing works focus on either text, image, or trajectory-based control, leading to an inability to achieve fine-grained control in videos. Secondly, trajectory control research is still in its early stages, with most experiments being conducted on simple datasets like Human3.6M. This constraint limits the models’ capability to process open-domain images and effect...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Controllable video generation is a hot topic in research. Most of these studies focus on controllable visual generation. Early research primarily emphasized image-to-video generation, using the initial fra ge as a control to manipulate the generated video spatially|Lotter et al. (2016); Srivastaval Chiappa et al. However, relying solely on images as controls cannot determine the subsequent frames of future videos. Consequently, there has been growing interest in text-tovideo research, employing ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S 2.1. TEXT/IMAGE CONTROL IN VIDEO SYNTHESIS Early research primarily emphasized image-to-video generation, with a common assumption that the environment is deterministic and has only one possible future |Lotter et al.| (2 Srivastava et al. (2015); (2016). However, this assumption cannot satisfy the requirements of realworld videos with unlimited possibilities. To address this issue, text-to-video generation has been widely studied in recent years (GODIVA|Wi et al.|(2021), NUWA|Wu et al. 2022), ...\n",
      "\n",
      "--- METHOD ---\n",
      "s control subsequent frame gen Liang et al.|(2017). On the other hand, video-to-video generation transfers the style of a complete video or video sketch to a new domain, providing rich control information{Chan et al.|(2019); le . However, this requires users to provide video input and restricts fine-grained control as the style transfer is based on the original video’s skeleton. Consequently, image-trajectory-tovideo methods have emerged, controlling video development through trajectories given ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s being conducted on simple datasets like Human3.6M. This constraint limits the models’ capability to process open-domain images and effectively handle complex curved trajectories. In this paper, we propose DragNUWA, an open-domain diffusion-based video generation model. To tackle the issue of insufficient control granularity in existing works, we simultaneously introduce text, image, and trajectory information to provide fine-grained control over video content from semantic, spatial, and tempor...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present DragNUWA, an end-to-end video generation model that seamlessly incorporates text, image, and trajectory input, enabling fine-grained and user-friendly control from semantic, spatial, and temporal perspectives. Additionally, our trajectory modeling framework, consisting of the Trajectory Sampler (TS), Multiscale Fusion (MF), and Adaptive Training (AT), tackles challenges in open-domain trajectory control, thereby enabling the generation of coherent videos in accordance with complex tra...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08628v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Model adaptation is crucial to handle the discrepancy between proxy training data and actual users’ data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in tex...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A common issue arising after deploying a machine learning model on central servers or user devices is the discrepancy between training data and actual user data received. Specifically, in the applications of natural language processing (NLP), semantic characteristics and topics of real users’ textual data could be very different from those of server-side proxy corpora, in which scenarios model adaptation is indispensable [1] [2]. To effectively perform model adaptation, textual data of users is ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s in Section|2| Section|3|describes the details of our proposed framework on privacy-preserving token masking and the substitutes of masked tokens using LLMs. Next, Section iments and results for downstream tasks of LM and ASR. Finally, We conclude in Section] 2. RELATED WORKS Privacy protection has been becoming crucial in NLP research One important direction in this area is through anonymization, which involves the removal of identifying information from textual corpus pz . More recently, obfu...\n",
      "\n",
      "--- METHOD ---\n",
      "s....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking. Index Terms— Privacy-preserving machine learning, language modeling, large language models, automatic speech recognition 1. INTRODUCTION A common issue arising after deploying a machine learning model on central servers or user devices is the discrepancy between training data and actual user data received. S...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose multiple pre-trained and fine-tuned LLMbased methods to recover from privacy-preserving token masking on textual corpus and perform empirical studies on various datasets for the comparison of these approaches. Our experimental results demonstrate that LMs trained on the obfuscation corpora can obtain comparable accuracy with the ones trained on the raw data without privacy-preserving token masking. Future research might include fine-tuning LLMs with the object function ...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.14460v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We release MiDaS v3.1' for monocular depth estimation, offering a variety of new models based on different encoder backbones. This release is motivated by the success of transformers in computer vision, with a large variety of pretrained vision transformers now available. We explore how using the most promising vision transformers as image encoders impacts depth estimation quality and runtime of the MiDaS architecture. Our investigation also includes recent convolutional approaches that achieve ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Monocular depth estimation refers to the task of regressing dense depth solely from a single input image or camera view. Solving this problem has numerous applications in downstream tasks like generative AI [1-3], 3D reconstruction [4-6] and autonomous driving [7, 8]. However, it is particularly challenging to deduce depth information at individual pixels given just a_ single image, as monocular depth estimation is an underconstrained problem. Significant recent progress in depth estimation can ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Monocular depth estimation is inherently an ill-posed problem facing challenges like metric scale ambiguity. Learning-based approaches that aim to directly regress metric depth [17-21] have sought to use supervised training on homogeneous datasets with representative environments (e.g., focusing on indoor or outdoor scenes) to encourage the supervised network to learn an appropriate metric scale. However, this results in overfitting to narrow depth ranges and degrades generalizability across env...\n",
      "\n",
      "--- METHOD ---\n",
      "s. In particular, dataset mixing and scale-and-shift-invariant loss construction has enabled robust and generalizable monocular depth estimation with MiDaS [9]. Since the initial development of that work, there have been several releases of MiDaS offering new models with more powerful backbones [10] and lightweight variants for mobile applications. ' sithub.com/isl-org/MiDaS Many deep learning models for depth estimation adopt encoder-decoder architectures. In addition to convolutional encoders ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al protocol used in training MiDaS v3.0 [10] that uses multi-objective optimization [41] with Adam [42], setting the learning rate to le-5 for updating the encoder backbones and le-4 for the decoder. Encoders are initialized with ImageNet [43] weights, whereas decoder weights are initialized randomly. Our training dataset mix is comprised of up to 12 datasets. Similar to [9], we first pretrain models on a subset of the dataset mix for 60 epochs (first training stage), and then train for 60 epoch...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present a collection of robust depth estimation models in the new release MiDaS v3.1. Although we also explore convolutional backbones for the release, only transformer based backbones provide a sufficiently high depth estimation quality with the MiDaS architecture. The release v3.1 consists of depth models with the new transformer backbones BEiT, Swin, SwinV2, Next-ViT and LeViT, where we offer multiple different variants for BEiT and SwinV2. BEiTs5)2-L with resolution 512xis on average 28% ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11129v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongTS, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2. We evaluate this model on a variety of multilingual summarization and question-answering tasks, and the results show stronger performance for mLongT5 when compared to existing multilingual models such a...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, there has been development of making transformer-based models more efficient so that they can handle longer input sequences. Many of the models though have been English-only, making them inapplicable to other languages. In this paper, we present our work in extending one of these models to be able to handle multilingual data. Our model, called mLongT5S, takes advantage of the efficient architecture of LongT(Guo et al., 2022), and has been pretrained on the multilingual mC4 datas...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "There are two areas of related work — efficient transformer models that can handle long inputs, and multilingual models. ‘https: //github.com/google/flaxformer/tree/ main/flaxformer/t5x/configs/longt5/models *https://github.com/google-research/longtThere has been much interest of late in making transformer models more efficient, such as to handle longer inputs. Example of these include ETC (Ainslie et al., 2020), Big Bird (Zaheer et al., 2020), LongT5 (Guo et al., 2022), and Longformer (Beltagy ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented our new model mLongTS. It has the benefits of the efficient architecture of LongT5, with the ability to handle multingual inputs and outputs. As our report shows, the model is able to perform well on a variety of summarization and question-answering tasks. e 3: WikiLingua summarization results. These results are using the GEM version of the task. Approach EM Fl mTS (base - 512 input) 37.16 49.mTS (base - 1k input) 43.09 56.mTS (base - 2k input) 44.63 58.mTS (base - 4k input) 45...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11778v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The recent rapid progress in pre-training Large Language Models has relied on using selfsupervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pretraining Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objectiv...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The rapid progress in the development of largescale pre-training, GPT (Brown et al., 2020), XGLM (Lin et al., 2021), PaLM (Chowdhery et al., 2022), has resulted in models capable of performing a variety of tasks through the in-context learning (aka. few shot) paradigm (Brown et al., 2020): one can present the model a few demonstrations of a given task at inference, and the model will able to follow these demonstrations on new, unseen examples. Therefore it is no longer necessary to fine-tune the...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We are not the first to investigate the usage of parallel cross-lingual data with LLMs. (Reid and Artetxe, 2022) considered leveraging parallel data by devising a loss consisting of 3 objectives; however, their technique is somewhat complicated because it necessitates the development of a multilingual noising procedure, while we opt for including the cross-lingual data using the standard MT objective. (Chi et al., 2021) proposed a simpler objective by building on top of the success of (Xue et al...\n",
      "\n",
      "--- METHOD ---\n",
      "s. Compared to Question Answering, summarization results sway, albeit not significantly, towards larger models where automated curriculum methods outperform the vanilla LM only method and our proposed FAIR method outperforms the others. Interestingly, we did not observe any gains when scaling from 1.2 billion to 3.8 billion parameters for LM (100%), where the automated curriculum methods benefits from scaling model size more than the than vanilla LM only method. --- --Model size (B) Data Selecti...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s with a 3.8B-parameter models requires 256 TPUvcores for 5 days. If we treat the mixing ratio between the parallel MT data and the LM training data as a hyper-parameter A, we have in theory just an additional hyper-parameter. However a grid search is prohibitively expensive; for example (Kale et al., 2021) considered a less computeintensive setup in which one fine-tunes mT5 models for 100k steps on a mixture of MT and LM data; nevertheless they were able to just compare two values of 4. Further...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We have demonstrated that, when pre-training Encoder-Decoder large language models, the inclusion of cross-lingual supervision in the training objective is beneficial. In particular, we have found substantial gains when evaluating the resulting models on Machine Translation and Question Answering. One drawback of including parallel data is the introduction of a new hyper-parameter which quantifies the percentange of such data to use. Even though inclusion of some cross-lingual supervision is b...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.02499v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tab...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) like ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], and LLaMA [Touvron et al., 2023] have undergone rapid development to enable the realization of general artificial intelligence, boasting impressive zero-shot capabilities across diverse linguistic applications. With the LLM as the language decoder, Multimodal large language models (MLLMs) such as MiniGPT-4 [Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023] have demonstrated remarkable...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Visual Text Understanding There are two types of models for understanding images that contain rich textual information. The first kind of approaches [Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021] utilize off-the-shelf OCR models or APIs to recognize text from images, and then design pretraining tasks to facilitate cross-modality alignment between visual and textual inputs. On the other hand, end-to-end approaches [Davis et al., 2022, Kim et al., ...\n",
      "\n",
      "--- METHOD ---\n",
      "s on ocr-free document understanding, including multiple standard benchmarks and LLMDoc. 2 Related Work 2.1 Visual Text Understanding There are two types of models for understanding images that contain rich textual information. The first kind of approaches [Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021] utilize off-the-shelf OCR models or APIs to recognize text from images, and then design pretraining tasks to facilitate cross-modality alignment betwe...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOw! generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl. 1 Introduction Large language models (LLMs) like ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], and LLaMA [Touvron et al., 2023] have undergone rapid developm...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we infuse diverse ocr-free document understanding capabilities into mPLUG-Owl by incorporating document understanding data into instruction finetuning. Experiment results demonstrate that our mPLUG-DocOwl achieves comparable or even better performance than existing OCRfree methods. Besides, benefiting from language-only and general vision-and-language instruction tuning, mPLUG-DocOwl can better comprehend user instructions and intentions, enabling more complex interactions. Moreove...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02790v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recently, DeepNorm scales Transformers into extremely deep (i.e, 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm (Wang et al., 2022a) attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the non-residual ...\n",
      "\n",
      "--- METHOD ---\n",
      "BranchNorm. 3.1 Perspective of Gradient Unbalanced gradients are mainly responsible for the instability of Transformer* (Wang et al., 2019; Shleifer et al., 2021; Zhu et al., 2021), we firstly explore the relation between gradient and model depth following Wang et al. (2019). Given a Transformer with L sub-layers and the training loss €, the gradient for the /-th sub-layer is calculated by the chain rule*: “~ irreducible € a€ int OLN (F (wei 9i)) a, (OF (@ni Or) L-k=l due ———_ residual where the...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "results on multiple translation tasks demonstrate that BranchNorm achieves a better trade-off between training stability and converge performance. 1 Introduction In recent years, Transformers (Vaswani et al., 2017) have been developed rapidly and achieved state-ofthe-art (SOTA) performance on a wide range of tasks. Meanwhile, the model capacity gets substantially expanded by widening the model dimension (Devlin et al., 2019; Liu et al., 2019; Goyal et al., 2021; Lin et al., 2021; Smith et al., 2...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s of the bilingual translations. 5 Analysis In this section, we first verify the robustness of BranchNorm to hyperparameter, and then analyze the parameter redundancy. 5.1 Hyperparameter Sensitivity Effects of Different 7. We conduct experiments to evaluate the effect of varying the different maximum norm step T’ in Equation (8) on BranchNorm. --- --OPUS100 MultiUN Models #Layers #Params | \\ ey EnsX Avg X—3En EnoX Avg 12 133M | 275 214 245 438 523Baseline (Zhang et al., 2020) 24 13M | 295 229 26...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.06940v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Creating engaging storytelling videos is a complex and laborious process that typically involves live-action filming or CG animation production. This technical nature not only demands significant resources from professional content creators but also creates barriers for the general public in effectively utilizing this powerful medium. Recently, significant progress has been made in text-to-video (T2V) generation, allowing for the automatic generation of videos based on textual descriptions [He e...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Video Generation Numerous earlier works [Saito et al. 2017; Skorokhodov et al. 2022; Tulyakov et al. 2018; Vondrick et al. 2016; Wu et al. 2021] focus on unconditional video generation...\n",
      "\n",
      "--- METHOD ---\n",
      "ensures consistent character rendering across different plot scenarios. Each clip is visualized with three keyframes. Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework compr...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that our approach exhibits significant advantages over various existing baselines. Additional Key Words and Phrases: Story Visualization, Video Diffusion Models, Retrieval-augmented Generation, Personalized Generation --- --2. + Yingging HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen 1 INTRODUCTION Creating engaging storytelling videos is a complex and laborious process that typically involves live-a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduce a novel retrieval-based pipeline for storytelling video synthesis. This system enables better video synthesis quality, layout, motion control, and character personalization for producing a customized and character-consistent storytelling video. We incorporate a structure-guided video generation module to a base text-to-video model, and we devise a new personalization method to boost the character control performance. We also solve the character-depth confliction problem with the adj...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.13974v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the...\n",
      "\n",
      "--- METHOD ---\n",
      "s can be grouped into either online-update trackers [3,9] and Siamese trackers [2,31]. Recently, Transformer [29] sweeps in computer vision, the dominant tracking methods are Transformer-based trackers [5, 8,35, 39]. TransT [5] proposes transformer-based ECA and CFA modules to replace the long-used correlation calculation. Benefiting from Transformer’s superior long-range modeling capability, TransT outperforms the previous correlation modules which are a capable class of linear modeling. More r...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "4.1. Ablation Study Separate tracking v.s. Joint tracking. We conduct ablation studies on different tracking paradigms. Separate tracking means initializing a separate tracker for each target object, and running multiple times of inference for multiple object tracking. Joint tracking means joint tracking all tar 10 0.610 0.668 0.807 0.110 0.083 0.20 0.607 0.65 0.806 0.12 0.074 0.30 0.626 0.689 0.813 0.127 0.060 0.40 0.650 0.681 0.886 0.059 0.055 0.50 0.669 0.692 0.885 0.057 0.058 0.60 0.653 0.66...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this report, we propose Tracking Anything in High Quality (HQTrack). HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). VMOS is responsible for propagating multiple targets in video frames, and MR is a large-scale pre-trained segmentation model in charge of refining the segmentation masks. HQTrack demonstrates powerful object tracking and segmentation capabilities. Finally, HQTrack achieves the 2nd place in the Visual Object Tracking and Segmentation (VO...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.06638v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "The field of text-to-image synthesis has recently experienced rapid development, largely thanks to advances in diffusion models. By conditioning on free-form text, images of unprecedented quality and diversity can now be generated. However, generating an image depicting a person from a user-supplied image is still a challenging task. To overcome this gap, existing methods rely on solving an optimization problem during inference time, e.g. fine-tuning the model [Ruiz et al. 2022] or reversing the...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Text-to-Image Models. Deep generative models for image generation have shown tremendous progress in recent years. Early --- --6 + Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan Mustache Red Hair Carly Hair Fig. 6. Face0 enables fine-grained control of facial features that are harder to describe textually via direct manipulation of the face embedding. Here we see a simple linear interpolation between the facial embeddings of two generated photos from the same source (the top-le...\n",
      "\n",
      "--- METHOD ---\n",
      "achieves pleasing results, is remarkably simple, extremely fast, and equips the underlying model with new capabilities, like controlling the generated images both via text or via direct manipulation of the input face embeddings. In addition, when using a fixed random vector instead of a face embedding from a user supplied image, our method essentially solves the problem of consistent character generation across images. Finally, while requiring further research, we hope that our method, which dec...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.11523v5.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Vision Transformer (ViT) has gained increasing attention in the computer vision community in recent years. However, the core component of ViT, Self-Attention, lacks explicit spatial priors and bears a quadratic computational complexity, thereby constraining the applicability of ViT. To alleviate these issues, we draw inspiration from the recent Retentive Network (RetNet) in the field of NLP, and propose RMT, a strong vision backbone with explicit spatial prior for general purposes. Specifically,...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Vision Transformer (ViT) [12] is an excellent visual architecture highly favored by researchers. However, as the core module of ViT, Self-Attention’s inherent structure lacking “Ran He is the corresponding author. huaibo.huang@cripac.ia.ac.cn, hmliu_82@163.com, rhe@nlpr.ia.ac.cn -- RMT(Ours) uo <¢- SMT eee 85} -+- BiFormer ss * -@- MaxViT oo a s 84 ‘Model Params Topl- Ace. oa MaxViT-T [31] 31M 83.g SMTS [34] 20M 83.< BiFormer-S [75] 26M 83.ms RMT-S (Ours) 27™M 84.o 83 RMT-S* (Ours) 27™M. 34.ze a...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Transformer. Transformer architecture was firstly proposed in [52] to address the training limitation of recurrent model and then achieve massive success in many NLP tasks. By splitting the image into small, non-overlapped patches sequence, Vision Transformer (ViTs) [12] also have attracted great attention and become widely used on vision tasks [5, 14, 18, 39, 58, 66]. Unlike in the past, where RNNs and CNNs have respectively dominated the NLP and CV fields, the transformer architecture has shin...\n",
      "\n",
      "--- METHOD ---\n",
      "s, we draw inspiration from the recently successful Retentive Network (RetNet) [46] in the field of NLP. RetNet utilizes a distancedependent temporal decay matrix to provide explicit temporal prior for one-dimensional and unidirectional text data. --- --(a) Vanilla Self-Attention (b) Window Self-Attention (c) Neighborhood Self-Attention CT] : Receptive Field (d) Manhattan Self-Attention Figure 2. Comparison among different Self-Attention mechanisms. In MaSA, darker colors represent smaller spati...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that RMT exhibits exceptional performance across various vision tasks. Specifically, without extra training data, RMT achieves 84.8% and 86.1% top-1 acc on ImageNet-1k with 27M/4.5GFLOPs and 96M/18.2GFLOPs. For downstream tasks, RMT achieves 54.5 box AP and 47.2 mask AP on the COCO detection task, and 52.8 mloU on the ADE20K semantic segmentation task. Code is available at https: //github.com/qhfan/RMT 1. Introduction Vision Transformer (ViT) [12] is an excellent visual architectur...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we propose RMT, a vision backbone with explicit spatial prior. RMT extends the temporal decay used for causal modeling in NLP to the spatial level and introduces a spatial decay matrix based on the Manhattan distance. The matrix incorporates explicit spatial prior into the Self-Attention. Additionally, RMT utilizes a Self-Attention --- --decomposition form that can sparsely model global information without disrupting the spatial decay matrix. The combination of spatial decay matrix...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.01741v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This technical paper introduces a chatting robot system that utilizes recent advancements in large-scale language models (LLMs) such as GPT-3 and ChatGPT (Fig[I). The system is integrated with a co-speech gesture generation system, which selects appropriate gestures based on the conceptual meaning of speech. Our motivation is to explore ways of utilizing the recent progress in LLMs for practical robotic applications, which benefits the development of both chatbots and LLMs. Specifically, it enab...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "LLMs like GPT-3/ChatGPT have shown remarkable success in natural language processing tasks, leading to growing interest in applying them to robotic applications. However, connecting a robot with LLMs poses risks such as bias, inappropriate responses, or vulnerability to attacks. Solutions to these problems are in the process of development. To minimize those risks, it is crucial to carefully monitor and control the robot’s output, utilize robust security measures, and provide proper ethical guid...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.18802v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper introduces a new speech dataset called “LibriTTSR” designed for text-to-speech (TTS) use. It is derived by applying speech restoration to the LibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The constituent samples of LibriTTS-R are identical to those of LibriTTS, with only the sound quality improved. Experimental results show that the LibriTTS-R ground-truth samples showed significantly improved sound...\n",
      "\n",
      "--- METHOD ---\n",
      "[32]. For simulating codec artifacts, we randomly applied one of MP3, Vorbis, A-law, Adaptive Multi-Rate Wideband (AMR-WB), and OPUS with a random bit-rate. The Table 1: MOS and SxS test results on the ground-truth samples with their 95% confidence intervals. A positive SxS score indicates that LibriTTS-R was preferred. . MOS (t) Split | LibriTTS Libri TS-R | SxS test-clean | 4.36+0.08 4.414 0.80 + 0.test-other | 3.94+0.10 4.094 1.42 +0.detailed simulation parameters were listed in [20]. We firs...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that the LibriTTS-R ground-truth samples showed significantly improved sound quality compared to those in LibriTTS. In addition, neural end-to-end TTS trained with LibriTTS-R achieved speech naturalness on par with that of the ground-truth samples. The corpus is freely available for download from http: //www.openslr.org/141/. Index Terms: Text-to-speech, dataset, speech restoration 1. Introduction Text-to-speech (TTS) technologies have been rapidly advanced along with the develop...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s This paper introduced LibriTTS-R, a sound quality improved version of LibriTTS [19]. We cleaned speech samples in the LibriTTS corpus by applying an SR model [20]. By subjective experiments, we show that the speech naturalness of a TTS model trained with LibriTTS-R is improved from that trained with LibriTTS, and is comparable with that of the ground-truth. This corpus is released online, and it is freely available for download from http: //www.openslr.org/141/. We hope that the release of thi...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01300v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Motivated by the remarkable achievements of DETR-based approaches on COCO object detection and segmentation benchmarks, recent endeavors have been directed towards elevating their performance through selfsupervised pre-training of Transformers while preserving a frozen backbone. Noteworthy advancements in accuracy have been documented in certain studies. Our investigation delved deeply into a representative approach, DETReg, and its performance assessment in the context of emerging models like -...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently, the DETR-based approaches (Carion et al., 2020; Jia et al., 2023; Li et al., 2023; Zhang et al., 2022; Zhu et al., 2020) have achieved significant progress and 3Xi’an Jiaotong5CUHK ?Peking University 4University of Pennsylvania & yuhui.yuan@ microsoft.com \\University of Toronto Liverpool University ®Microsoft Research Asia pushed the frontier on both object detection and segmentation tasks. For example, DINO-DETR (Zhang et al., 2022), H-Deformable-DETR (Jia et al., 2023), and GroupDETR...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "DETR for object detection. Since the emergence of DETR (Carion et al., 2020) as the first fully end-to-end object detector, many works have extended DETR with novel techniques to achieve state-of-the-art results on various vision tasks. To accelerate the convergence of the original DETR, Deformable-DETR (Zhu et al., 2020) proposes a novel multi-scale deformable self/cross-attention to focus on a Sparse set of important sampling points around a reference point. Furthermore, based on DAB-DETR (Liu...\n",
      "\n",
      "--- METHOD ---\n",
      ". In both cases, the backbones of these models are ResNet50 initialized with SwAV (Caron et al., 2020). Notably, in the case of the #-Deformable-DETR, the utilization of the DETReg pre-training actually leads to a performance decrease rather than an improvement. In this work, we first take a closer look at how much self-supervised pre-training methods, exemplified by DETReg, can improve over the increasingly potent DETR models on COCO object detection benchmark. Our investigation unveils a signi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on COCO and PASCAL VOC probing elements such as the selection of pre-training datasets and strategies for pretraining target generation. By contrast, we employ an optimized approach named Simple Self-training which leads to marked enhancements through the combination of an improved box predictor and the Objects365 benchmark. The culmination of these endeavors results in a remarkable AP score of 59.3% on the COCO val set, outperforming HDeformable-DETR + Swin-L without pre-training by 1.4%. Mor...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We investigate the effectiveness of DETReg, a representative self-supervised pre-training approach for DETR, across --- --Revisiting DETR Pre-training for Object Detectionthree distinct DETR architectures. Our findings, unfortunately, do not reveal any performance enhancements of DETReg in recent architectures, thereby challenging the validity of previous conclusions. In response, we reevaluate crucial design aspects, including pre-training targets for localization and classification. As a resul...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.12854v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel selfsupervised pretraining approach that l...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Long-term forecasting of human activities (illustrated in Figure[I) is a key capability that is crucial for developing intelligent and collaborative machines. Machines that reason about future actions given some observations are better able to plan their own behavior accordingly and interact more effectively with other agents in dynamic environments. However, forecasting future actions is inherently challenging. To begin, the model has to understand the current state of the environment under par...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Self-supervised video pretraining. Self-supervised video pretraining 1] has been demonstrated to be beneficial for improving performance on downstream tasks such as activity recognition video object segmentation [20], early action prediction [ and unintentional action detection [18] [19] on target datasets including Kinetics-400/600 , HMDB-51 and UCFI101 [28]. Inspired by image-based self-supervised pretraining objectives | [6], state-of-the-art video approaches [1 often use a similar learning o...\n",
      "\n",
      "--- METHOD ---\n",
      "s by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods. 1. Introduction Long-term forecasting of human activities (illustrated in Figure[I) is a key capability that is crucial for developing intelligent and collaborative machines. Machines that reason about future actions given some observations are better able to plan their own behavior accordingly and interact more effectively with other agents in dynam...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s across the Ego4D and Epic-Kitchens-55/100 datasets demonstrate that MVP outperforms state-of-the-art methods by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods. 1. Introduction Long-term forecasting of human activities (illustrated in Figure[I) is a key capability that is crucial for developing intelligent and collaborative machines. Machines that reason about future actions given some observations ar...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In summary, we introduce Multiscale Video Pretraining, a self-supervised approach that aims to learn robust video representations for downstream long-term forecasting tasks. Given an observed video clip sequence, we train a video model to predict aggregated representations of future clips over multiple timescales. We demonstrate empirically that learning to encode future contextual information helps --- --the video model to generalize better to long-term forecasting tasks than prior work, which ...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.10159v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In an era where visual content generation is increasingly driven by machine learning, the integration of human feedback into generative models presents significant opportunities for enhancing user experience and output quality. This study explores strategies for incorporating iterative human feedback into the generative process of diffusion-based text-to-image models. We propose FABRIC, a training-free approach applicable to a wide range of popular diffusion models, which exploits the self-atten...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The field of artificial intelligence (AI) has witnessed a surge in interest in generative visual models, primarily due to their transformative potential across a myriad of applications, encompassing content creation, customization, data augmentation, and virtual reality. These models leverage advanced deep learning methodologies, such as GAN and VAE (2022), to generate high-fidelity and visually compelling images from given inputs or descriptions fet al.| (2019); /Razavi et al.|(2019). The signi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 TEXTUAL INVERSION AND STYLE TRANSFER A popular...\n",
      "\n",
      "--- METHOD ---\n",
      "ology, offering a robust mechanism to quantify the performance of generative visual models that integrate human feedback. We show that generation results improve over multiple rounds of iterative feedback through exhaustive analysis, implicitly optimizing arbitrary user preferences. The potential applications of these findings extend to fields such as personalized content creation and customization. 1 INTRODUCTION The field of artificial intelligence (AI) has witnessed a surge in interest in gen...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al settings that facilitate the automatic evaluation of generative visual models over multiple rounds. ¢ Using these proposed settings, we evaluate FABRIC and demonstrate its superiority over baseline methods. 2 RELATED WORK 2.1 TEXTUAL INVERSION AND STYLE TRANSFER A popular method for personalizing text-to-image diffusion models is textual inversion , a technique for learning semantic text embeddings from images depicting a common subject or style. This enables the synthesis of photorealistic i...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present FABRIC, a training-free method that incorporates iterative feedback into the generation process of text-to-image models, leveraging attention-based reference image conditioning. Our experimental findings suggest that FABRIC is capable of implicitly optimizing a variety of objective functions such as human preference and similarity to a designated target image. These objectives noticeably improve with more feedback rounds, demonstrating that FABRIC’s effectiveness significantly exceeds...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.16700v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ion level. In this figure, we demonstrate a real-world task of gathering the object pile into a target region. Figures on the left show the task execution process and the corresponding particle representation. The plot on the right shows the predicted optimal resolution at each MPC step, where the red circles correspond to the frames on the left. For video illustrations, we invite you to visit our project page’. Abstract—Dynamics models learned from visual observations have shown to be effective...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Predictive models have been one of the core components in various robotic systems, including navigation , locomotion [29], and manipulation [23] [73]. For robotic sranipaltion in particular, people have been learning dynamics models of the environment from visual observations and demonstrated impressive results in various manipulation tasks [{55}. A learning-based dynamics model typically involves an encoder that maps the visual observation to a scene representation, and a predictive model predi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "A. Scene Representation at Different Abstraction Levels To build multi-scale models of the dynamical systems, prior works have adopted wavelet-based...\n",
      "\n",
      "--- METHOD ---\n",
      "in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, *Denotes equal contribution. ihups://RoboPIL.github.io/dyn-...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "S In this section, we evaluate the proposed framework in various object pile manipulation tasks. In particular, we aim to (b) Object piles considered in this work Fig. 3. Robot setup and the testing object piles. (a) The dashed black square shows the robot’s workspace. The robotic manipulator, equipped with a pusher at the end effector, pushes the object piles within the workspace. A calibrated RGBD camera mounted at the top provides visual observations of the environment. (b) We show the object...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Dynamics models play an important role in robotics. Prior works developed dynamics models based on representations of various choices, yet they are typically fixed throughout --- --the entire task. In this work, we introduced a dynamic and adaptive scene representation learning framework that could automatically find a trade-off between efficiency and effectiveness for different tasks and scenes. The resolution of the scene representation is predicted online at each time step. And a unified dyna...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.07174v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LONGMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and rea...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have revolutionized natural language processing with great successes in advancing the state-of-the-art on various understanding and generation tasks [DCLT19, RWC* 19, LOG* 19, YDY+ 19, BMR*20, RSR* 20]. Most LLMs benefit from self-supervised training over large corpora via harvesting knowledge from fix-sized local context, showing emergent abilities, e.g., zero-shot prompting [RWC* 19], in-context learning [BMR* 20], and Chain-of-Thought (CoT) reasoning [WWS* 22]. Ne...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Large Language Models. Large Language Models, i.e., GPT-2 [RWC* 19], GPT-3 [BMR* 20], OPT [ZRG* 22], and BLOOM [SFA* 22], significantly revolutionized NLP research and promoted the state-of-the-art of various language understanding, language generation [WZG* 22], and even vision-language tasks [WDC* 22]. Additionally, via scaling the model parameters, LLMs exhibit “emergent abilities‘ [WTB 22] like few-shot in-context learning [BMR*20], multi-step reasoning [WWS* 22], code completion, etc. --- -...\n",
      "\n",
      "--- METHOD ---\n",
      "outperforms strong longcontext models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https: //aka.ms/LongMem. 1 Introduction Large language models (LLMs) have revolutionized natural language processing with great successes in advancin...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our method outperforms strong longcontext models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https: //aka.ms/LongMem. 1 Introduction Large language models (LLMs) have revolutionized natural language processing with grea...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose to augment LLMs with long-term memory for enabling them to memorize long-form context and gain long-form memory. The designed decoupled memory module can cache attention key and value pairs of past inputs for future retrieval and fusion. A decoupled residual SideNet is introduced as the memory retriever and reader, meanwhile the LLM itself is frozen and works as knowledge and memory encoder. Experiments on various long-contextual language modeling datasets demonstrate t...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.07954v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zeroshot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to ge...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recent text-to-image diffusion models such as DALLE2 [30], Imagen [34], Stable Diffusion [32] demonstrate exceptional ability in generating diverse and high-quality images guided by natural language. Based on it, a multitude of image editing methods have emerged, including model fine-tuning for customized object generation [33], imageto-image translation [23], image inpainting [1], and object editing [12]. These applications allow users to synthesize and edit images effortlessly, using natural l...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Text Driven Image Generation Generating images with descriptive sentences is intuitive and flexible. Early attempts explore GAN [42-44, 46] to synthesize realistic images. With the powerful expressivity of Transformer [38], autoregressive models [6,9, 31] are proposed to model image pixels as a sequence with autoregressive dependency between each pixel. DALL-E [31] and Cog View [6] train an autoregressive transformer on image and text tokens. Make-A-Scene [9] further considers segmentation ...\n",
      "\n",
      "--- METHOD ---\n",
      "s in rendering high-quality and temporally-coherent videos. Code is available at our project page: https: //www.mmlab-ntu.com/project/rerender/ 1. Introduction Recent text-to-image diffusion models such as DALLE2 [30], Imagen [34], Stable Diffusion [32] demonstrate exceptional ability in generating diverse and high-quality images guided by natural language. Based on it, a multitude of image editing methods have emerged, including model fine-tuning for customized object generation [33], imageto-i...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos. Code is available at our project page: https: //www.mmlab-ntu.com/project/rerender/ 1. Introduction Recent text-to-image diffusion models such as DALLE2 [30], Imagen [34], Stable Diffusion [32] demonstrate exceptional ability in generating diverse and high-quality images guided by natural language. Based on it, a multitude of image editing methods hav...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper presents a zero-shot framework to adapt image diffusion models for video translation. Our method utilizes hierarchical cross-frame constraints to enforce temporal consistency in both global style and low-level textures, leveraging the key optical flow. The compatibility with existing image diffusion techniques indicates that our idea might be applied to other text-guided video editing tasks, such as video super-resolution and inpainting. Additionally, our proposed fidelity-oriented im...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.03199v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to stro...\n",
      "\n",
      "--- METHOD ---\n",
      "is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test. Index Terms— Diffusion models, flow matching, speech synthesis, text-to-speech, acoustic modelling 1. INTRODUCTION Diffusion probabilistic models (DPMs) (cf. [1]) are curren...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that both innovations accelerate synthesis, reducing the trade-off between speed and synthesis quality. Despite being fast and lightweight, Matcha-TTS learns to speak and align without requiring an external aligner. Compared to strong pre-trained baseline models, Matcha-TTS achieves fast synthesis with better naturalness ratings. Audio examples and code are provided at https://shivammehta25.github.io/Matcha-TTS/. 2. BACKGROUND 2.1. Recent encoder-decoder TTS architectures ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S AND FUTURE WORK We have introduced Matcha-TTS, a fast, probabilistic, and highquality ODE-based TTS acoustic model trained using conditional flow matching. The approach is non-autoregressive, memory efficient, and jointly learns to speak and align. Compared to three strong pre-trained baselines, Matcha-TTS provides superior speech naturalness and can match the speed of the fastest model on long utterances. Our experiments show that both the new architecture and the new training contribute to t...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10425v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "While massively scaling model parameters and training compute of Transformer-based language models have led to impressive few-shot in-context learning [5]|6], reinforcement learning from human feedback fine-tuning (RLHF) can significantly improve generation quality as judged by humans. This has been observed at all model scales for various downstream language generation tasks, such as abstractive summarization, dialogue, and creative writing [18}[3) [7] (13). For summarization in particular, mul...\n",
      "\n",
      "--- METHOD ---\n",
      "s [12] [21] seek to align model likelihood with an arbitrary, possibly non-differentiable reward, presenting an alternative to RL for optimizing the expected reward of samples. Zhao et al. proposed Sequence Likelihood Calibration (SLiC) to align a language model’s sequence likelihood, pg(y|x), over decoded sequences according to their similarity to reference sequences. The ranking calibration loss contrasts a positive sequence y+ and a negative sequence y~ , encouraging the model Pg to assign mo...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning (SFT) baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice. 1 Introduction While massively scaling model parameters and training compute of Transformer-based language models have led to impressive few-shot in-context learning [5]|6], rein...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we proposed SLiC-HF that calibrates sequence likelihood on human feedback data. Our experiments on the Reddit TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning (SFT) baselines, and presents a competitive alternative to the RLHF-PPO implementation of past work while being simpler to implement, easier to tune and computationally efficient. Future work may include studying SLiC-HF on other language generation tasks using other reward functions an...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11837v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The advent of automation in particular Software Engineering (SE) tasks has transitioned from theory to reality. Numerous scholarly articles have documented the successful application of Artificial Intelligence to address issues in areas such as project management, modeling, testing, and development. A recent innovation is the...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "of ChatGPT, an ML-infused chatbot, touted as a resource proficient in generating programming codes and formulating software testing strategies for developers and testers respectively. Although there is speculation that Al-based computation can increase productivity and even substitute software engineers in software development, there is currently a lack of empirical evidence to verify this. Moreover, despite the primary focus on enhancing the accuracy of AI systems, non-functional requirements i...\n",
      "\n",
      "--- RELATED WORK ---\n",
      ". Section 3 presents the empirical study, describing research questions, hypotheses, and the objective of the study. Section 4 presents the experimental results and threats to validity. The paper ends in Section 5 with concluding remarks and suggestions for future work. 2 Related Work Imai [7] claims that while there is speculation that Al-based computation can increase productivity and even substitute human pair programmers in software development, there is currently a lack of empirical evidenc...\n",
      "\n",
      "--- METHOD ---\n",
      "s, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of cooperative work structures and human-in-the-loop processes. This paper conducts an empirical investigation, contrasting the performance of software engineers and AI systems, like ChatGPT, across different evaluation metrics. The empirical study includes a case of assessing ChatGPT-generated code versus code produced by developers and uploaded in Leetcode. Keywords Software Enginee...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "indicates that in some cases, software engineers outperform machine-learning algorithms, whereas in other cases, they do not.\" Such an understanding is essential in realizing novel human-in-the-loop approaches in which AI procedures assist software developers in achieving tasks. Human-in-the-loop approaches, which take into account the strengths and weaknesses of humans and AI solutions, are fundamental not only to providing a basis for cooperative human-machine work or teams not only in softwar...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future Work Several researchers have proposed the use of AI systems to automate software engineering tasks. However, most of these approaches do not direct efforts toward asking whether Al-based procedures have higher success rates than current standard and manual practices. A relevant question in this potential line of investigation is: “Could a software engineer solve a specific development task better than an automated system?\". Indeed, it is fundamental to evaluate which tasks are better...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.15930v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Spee...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Speech contains semantic information and contains paralinguistic information like intonation at the same time, it carries more quantity of information than text. Additionally, speech is a more convenient and natural way for humans to interact with artificial intelligence. Therefore, following speech-and-language instructions is crucial when developing a general-purpose assistant. However, most large language models [I] [2] 3]] receive text input only, which restricts the ability of large languag...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Vision Large Language Model has gained significant traction [4] |5}[6}[7)|8) [9] recently. Most of them leverage the pre-trained LLMs and vision encoders to perform vision tasks. Flamingo aligns a pre-trained vision encoder and language model using gated cross-attention and is trained on billions of image-text pairs. BLIP-2 employs a Flan-T5 with a Q-Former to efficiently align visual features with the language model. Palm-E [5], featuringbillion parameters, integrates the 540B PaLM [2] and 22B ...\n",
      "\n",
      "--- METHOD ---\n",
      "s [10] [II] use an automatic speech recognition (ASR) model to convert the speech input into the text input, then the model can process the task with the text input. However, it still leads to information consumption during the modal transformation from speech to text and might import mistakes of the ASR system. Recently, speech-language multi-modal models [12] {T3] focusing on processing and generating speech and text with a large language model are capable of understanding and generating multi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https: //github.com/LinkSoul-AI/LLaSM| and https: // huggingface.co/spaces/LinkSoul/LLaSM The LLaSM-Audio-Instructions dataset is available at https: //huggingface.co/datasets/LinkSoul/LLaSM- Audio- Instructions 1 Introduction Speech contains semanti...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This work presents LLaSM, a large language model with cross-modal conversational abilities, capable of understanding and following speech-and-language instructions. Experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, to alleviate the scarcity of cross-modal speechand-language instructions data, we build a large Speech Instruction Following data set LLaSM-Audio-Instructions. It is the largest Chinese and Eng...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10601v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Th...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Planning and decision making. Smart planning and decision making are critical to achieving predefined goals. As they are trained on vast amount of world knowledge and human examples, LMs are known to have already absorbed rich commonsense that makes it possible to propose reasonable plans conditioned on problem setting and environmental states proposed ToT approach extends existing planning formulations by considering multiple potentially feasible plans simultaneously at each problem-solving ste...\n",
      "\n",
      "--- METHOD ---\n",
      "achieved a success rate of 74%. Code repo with all prompts: https : //github.com/princeton-nlp/tree-of-thought-11m 1 Introduction Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mech...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https : //github.com/princeton-nlp/tree-of-thought-11m 1 Introduction Originally designed to generate text, scaled-up versions of langu...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ". The associative “System 1” of LMs can be beneficially augmented by a “System 2” based on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts framework provides a way to translate classical insights about problem-solving into actionable methods for contemporary LMs. At the same time, LMs address a weakness of these classical methods, providing a way to solve complex problems that are not easily formalized, such as creative writing. We see this intersection of L...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.04966v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Optimizing and rendering Neural Radiance Fields is computationally expensive due to the vast number of samples required by volume rendering. Recent works have included alternative sampling approaches to help accelerate their methods, however, they are often not the focus of the work. In this paper, we investigate and compare multiple sampling approaches and demonstrate that improved sampling is generally applicable across NeRF variants under an unified concept of transmittance estimator. To faci...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural volume rendering has revolutionized the inverse rendering problem, with the Neural Radiance Field (NeRF) being a key innovation. The continuous radiance field representation allows for rendering novel views of a scene from any camera position. However, the optimization of a NeRF can be computationally expensive due to the neural representation of radiance field and the large number of samples required by volume rendering. These challenges have limited practical applications of NeRF-based ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s NeRF Codebases. The recent explosion of NeRF-related research has led to numerous papers, many of which have released their own codebases [I] [2] [4] [7] [13] [16] [T9} [37] 9]. These codebases address various tasks related to NeRF, including surface reconstruction 4], radiance field representation 35], dynamic modeling {13] 21], and camera optimization [14] [33]. However, each codebase is tailored to a specific task and supports only a single approach. While most of these...\n",
      "\n",
      "--- METHOD ---\n",
      "s, however, they are often not the focus of the work. In this paper, we investigate and compare multiple sampling approaches and demonstrate that improved sampling is generally applicable across NeRF variants under an unified concept of transmittance estimator. To facilitate future...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating advanced sampling methods into NeRF related methods. We demonstrate its flexibility by showing that it can reduce the training time of several recent NeRF methods by 1.5x to 20x with minimal modifications to the existing codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be implemented in native PyTorch using NerfAcc. Our code are open-sourced at\\ht tps: //www.nerfacc.com 1. Introduction ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In conclusion, this paper highlights the significant impact of advanced sampling approaches on improving the efficiency of Neural Radiance Fields (NeRF) optimization and rendering. We demonstrate that advanced sampling can significantly speed up the training of various recent NeRF papers, while maintaining high-quality results. The development of NerfAcc, a flexible Python toolbox, enables researchers to incorporate advanced sampling methods into NeRF-related methods easily. The exploration an...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.00008v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, large neural networks derived from from the Transformer architecture have demonstrated superior results on language understanding and generative tasks. Many improvements on Transformer variants have come from scaling the size of models {Raf} fel et al.|[2020 2020a} |Shoeybi et al.}/(Chowdhery et al.}|2022), scaling the training tokens (Hoff Google Deepmind. Correspondence to: Yangi Zhou <yan: ing, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright| 2023 by the author(s). Scaling 2...\n",
      "\n",
      "--- RELATED WORK ---\n",
      ". ’a’: attention, ’f’: feed-forward, ’g’: sparsely gated feed-forward. GLaM interleaves dense transformer blocks with sparse transformer blocks. Brainformer reduces the frequency of attention and changes layer widths together with layer types. Resonating with the layer-wise architecture stacking in EfficientNet (Tan & Le} 2019) and layer reordering in the sandwich transformer (Press et al|| 2019}, we propose a nonuniform architecture with sparsity where there is no strict layer interleaving as i...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Using an evolutionary search algorithm, we have developed and evaluated a complex architecture block, named Brainformer, that consists of a diverse sequence of layers, including a sparsely gated feed-forward layer. Along with the new block, we also propose evaluating using a fixed training time search, which enables fair comparisons across model families. Brainformer demonstrates up to 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task eva...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.11500v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia. ABSTRACT Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limita...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent literature, foundation models, like CLIP [49], variants of GPT [50], DALL-E 2 [51] and Stable Diffusion [53], have shown tremendous success in various understanding and generation tasks. Despite being different in architectural or algorithmic designs, they are lying on a common basis: large-scale multimodal datasets, for example, MMC4 [66], LAION [55], HowTo100M [39], indicating an emerging transition from model-centric to data-centric representation learning. The former considers push...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Audio-visual Learning Audio-visual events often occur simultaneously within in-the-wild videos, establishing a profound connection between sound and imagery. [1, 2, 23, 47] employ audio-visual self-supervised learning to leverage audio-visual correspondence for enhancing representation learning. Specifically, [15, 58, 62] learn audio-text representation based on such correspondence. Audio-visual localisation [6, 21, 40, 41, 56] concentrates on identifying the positions of visual sound source...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies — Computer vision; + Information systems — Data structures; Information retrieval. KEYWORDS Audio-language Dataset, Audio-language Representation Learning, Audio Captioning ACM Reference Format: Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. 2024. Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning. In Proceedings of the 32nd ACM International Conference on Multimedia (MM 24), October 28—November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 13 page...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s from four perspectives: First, we launch a joint audio-language representation learning using InfoNCE loss [18, 46, 65], and evaluate the model through a retrieval task between audio and language, showing noticeable improvement over existing datasets; Second, we conduct zero-shot classification experiments, demonstrating the effectiveness for learning environmental information with our dataset; Third, we benchmark on audio-language generation task, specifically, automatic audio captioning, by ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present an automatic pipeline for audio caption generation, accompanied by a large-scale and comprehensive audio captioning dataset comprising 1.5M data pairs. Furthermore, we evaluate the performance of various audio-language models on our dataset to authenticate the effectiveness, and provide a manually verified test set along with a benchmark for audio-language tasks. These experimental findings unveil the wealth of information and precise descriptions inherent in our data, ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05432v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Webpages have been a rich resource for language and vision-language tasks. Yet only ieces of webpages are kept: image-caption airs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data underused. To study multimodal webpage understanding, we introduce he Wikipedia Webpage 2M (WikiWeb2M suite; the first to retain the full set of images, text, and structure data available in a age. WikiWeb2M can be used for...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07214v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we study a novel problem in egocentric action recognition, which we term as “Multimodal Generalization” (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considera...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Action recognition systems are typically trained on data captured from a third-person or spectator perspective [37, 56]. However, in areas such as robotics and augmented reality, we capture data through the eyes of agents, i.e., in a first-person or egocentric perspective. With head “Equal contribution + Work done during an internship at Meta Reality Labs. Training Testing (a) Normal Evaluation aes eee Multimodal Network Multimodal Network (b) Missing Modality Evaluation Ez aes Muttimodat Networ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multimodal egocentric action recognition. Action recognition systems are typically trained on video [15-19, 25, 65,70]. However, for egocentric activity recognition (i.e., first-person perspective, or recognizing the activity the user wearing the capturing device is performing), complimentary multimodal information is essential for identifying the correct activity (see Fig. 2). Previous...\n",
      "\n",
      "--- METHOD ---\n",
      "s with improved generalization ability. In particular, we introduce a new fusion module with modality dropout training, contrastive-based alignment training, and a novel cross-modal prototypical loss for better few-shot performance. We hope this study will serve as a benchmark and guide future research in multimodal generalization problems. The benchmark and code are available at https://github.com/facebookresearch/MMG_Ego4D 1. Introduction Action recognition systems are typically trained on dat...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al Setup 5.1. Architecture Details Unimodal backbones. We use MViT-B (16 x 4) [15] as the feature extractor for video modality, which is pretrained on Kinetics-400 [37]. Audio Spectrogram Transformer (AST) [26] is used as the audio feature extractor, and it is pre-trained on AudioSet [22]. For IMU feature extractor, we designed a ViT [11] based transformer network. Fusion module. Our fusion module is a transformer network with two layers. Each layer contains a self-attention block with 12 heads....\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this paper, we introduced the first comprehensive benchmark for multimodal generalization (MMG) and proposed three components to improve the generalization perormance of models. Our benchmark, MMG-Ego4D, includes two new tasks and a new dataset. The evaluation of different baseline architectures showed that the generalization ability of current systems is limited. Therefore, benchmarking and improving generalization ability deserve attention, especially as models are deployed into more sens...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.09958v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Visual instruction tuning has recently shown encouraging progress with opensource large multimodal models (LMM) such as LLaVA and MiniGPT-4. However, most existing studies of open-source LMM are performed using models with 13B parameters or smaller. In this paper we present an empirical study of scaling LLaVA up to 33B and 65B/70B, and share our findings from our explorations in image resolution, data mixing and parameter-efficient training methods such as LoRA/QLoRA. These are evaluated by thei...\n",
      "\n",
      "--- METHOD ---\n",
      "s such as LoRA/QLoRA. These are evaluated by their impact on the multi-modal and language capabilities when completing real-world tasks in the wild. We find that scaling LMM consistently enhances model performance and improves language capabilities, and performance of LoORA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning. Additionally, the study highlights the importance of higher image resolutions and mixing multimodal-language data to improve LMM performance, an...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and establishing stronger baselines using larger-scale LLaVA for future research. Specifically, we explore the impact of larger model sizes, model tuning and data mixing methods on model performance, and present our findings and recommendations. The scaling recipe leads to new state-of-the-art (SoTA) performance on LLaVA-Bench [12] and MM-VET [19]. We hope that our findings and larger LLaVA checkpoints would provide a reference for future research on visual instruction tuning. “These authors c...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and Limitations We present an empirical study of scaling the language model size for LMM. Our main findings are: (2) Scaling LMM consistently enhances model performance, resulting in significant improvements in language capabilities, primarily due to the increased LLM model size. We leave it to future work how to scale the vision encoder to enhance the visual capabilities and improve model performance on vision recognition and understanding tasks. (ii) Parameter-efficient methods such as LORA/...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.10231v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Memory augmentation is a powerful approach for efficiently incorporating external information into language models, but leads to reduced performance relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval hybrid that partially pre-computes memory and updates memory representations on the fly with a smaller live encoder. We propose GLIMMER, which improves on this approach through 1) exploiting free access to the powerful memory representations by applying a shallow reranker ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Retrieval-augmented language models achieve strong performance, but are computationally expensive due to the need to process retrieved passages. A large body of work attempts to reduce the cost of reading retrieved passages through conditional computation (Ainslie et al., 2023b; Varshney et al., 2022; Schuster et al., 2022), reranking (Wang et al., 2018; Yu et al., 2022; Wang et al., 2018), or memory (de Jong et al., 2022b; Wu et al., 2022a; Li et al., 2022). Reranking improves retrieval quality...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Retrieval augmentation (Izacard and Grave, 2021; Borgeaud et al., 2022; Lewis et al., 2020; Khandelwal et al., 2020; Guu et al., 2020) is a powerful technique to improve language model performance by augmenting the input with additional context. Our work is focused on improving the quality-compute trade-off for retrieval-augmented language models. It does so by unifying three lines of research: lateinteraction memory, late-interaction reranking, and learning to retrieve. Our approach uses the ar...\n",
      "\n",
      "--- METHOD ---\n",
      "s that GLIMMER is built on, and their computational properties. A more in-depth analysis of these methods can be found in de Jong et al. (2023). 2.1 Fusion-in-Decoder Fusion-in-Decoder (Izacard and Grave, 2021) is based on a TS encoder-decoder model (Raffel et al., 2020). For each input, a number of relevant text passages are retrieved, and the input is prepended to each passage. The resulting input-passage pairs are encoded separately by the encoder, and the encoded pairs are then concatenated ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s 4.1 Experimental setup Model configuration GLIMMER is based on the T5.1.1 architecture (Raffel et al., 2020) like LUMEN, implemented in JAX (Heek et al., 2020), Flax (Heek et al., 2020) and Flaxformer. All models are initialized from public T5.1.1 checkpoints. FiD is fine-tuned according to the recipe from the original paper (Izacard and Grave, 2021). For LuMEN and GLIMMER, given proportion of live layers a, the memory encoder is initialized with the first- a proportion of layers of the T5 enc...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Retrieval-augmented language models are powerful but slow in inference, while pre-computed memory-augmented models are fast at the cost of quality. Hybrid late-interaction models such as LUMEN present a good quality-compute trade-off. We introduce GLIMMER, an improved late-interaction model that also incorporates learned end-to-end reranking and multi-task training to achieve an even better trade-off. GLIMMER achieves strong gains in quality at faster speeds compared to LUMEN and FiD on the KILT...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.09635v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with highquality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "With the advance of generative modeling [1—3] and language-audio contrastive learning [4—6], various deep learning-based text-to-audio synthesis systems have recently emerged [7-12]. However, these systems typically require a large amount of paired text-audio data for training. Despite extensive human annotation efforts, the current largest public text-audio dataset contains around 630k text-audio pairs [4]. Given the relative scarcity of text-audio data on the web as compared to text-image data...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s are AudioLDM [9] and MusicLM [12], which rely on language-audio models [4, 5] to perform a zero-shot audio-to-text modality transfer, but such language-audio models are trained on audio-text pairs. ‘https: //salu133445. github. io/clipsonic/ --- --7 ; \\ CLIP-image (pretrained; frozen) img a photo of Xx, CLIP-text ~ (pretrained; frozen) Qrexe A = { < U-Net Peal =T,..,1) yy Input video Xp = \\ (0) Inference — CLIPSonie-ZS (zero-shot transfer) Diffusion model (for ¢ (— = >) CLIP-text Diffusion pri...\n",
      "\n",
      "--- METHOD ---\n",
      ", and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate the effectiveness of the proposed method. Audio samples are available on our demo website.! Our study differs from prior work in several ways. Existing textto-audio models rely on large amounts of text-audio training pairs [7— 12], whereas CLIPSonic learns text-queried audio synthesis without text-audio pairs. Prior work studied image-to-audio synthesis [18— 20], but they do not examine the zero-shot modality transfer between texts and images. CLIPSep [21] and CLIPSynth [2...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We explored approaching text-to-audio synthesis without text-audio pairs by using unlabeled videos and pretrained language-vision models. Through both objective and subjective evaluations, we showed that the proposed models can effectively learn text-to-audio synthesis without text-audio pairs, and the pretrained diffusion prior can reduce the modality transfer gap caused by the mismatch between CLIP’s image (used for training) and text (used for inference) embedding spaces. Moreover, in a subje...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02549v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The recent advent of self-supervised pretraining techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multitask tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Automated information extraction is essential for many practical applications, with form-like documents posing unique challenges compared to article-like documents, which have led to an abundance of recent research in the area. In particular, form-like documents often have complex layouts that contain structured objects like tables, columns, and fillable regions. Layout-aware language modeling has been critical for many successes (Xu et al., 2020; Majumder et al., 2020; Lee et al., 2022). To fur...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Early works on form document information extraction are based on rule-based models or learningbased models with handcrafted features (Lebourgeois et al., 1992; O’Gorman, 1993; Ha et al., 1995; Simon et al., 1997; Marinai et al., 2005; Chiticariu et al., 2013). Later on, various deep neural models have been proposed, including...\n",
      "\n",
      "--- METHOD ---\n",
      "s based on recurrent nets (Palm et al., 2017; Aggarwal et al., 2020), convolutional nets (Katti et al., 2018; Zhao et al., 2019; Denk and Reisswig, 2019), and transformers (Majumder et al., 2020; Garncarek et al., 2020; Wang et al., 2022c). Recently, in addition to the text, researchers have explored the layout attribute in form document modeling, such as the OCR word reading order (Lee et al., 2021; Gu et al., 2022b), text coordinates (Majumder et al., 2020; Xu et al., 2020; Garncarek et al., 2...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, FormNetV2 outperforms its predecessor FormNetV1 as well as the existing multimodal approaches on four standard benchmarks. In particular, compared with FormNetV1, FormNetV2 outperforms it by a large margin on FUNSD (86.35 v.s. 84.69) and Payment (94.90 v.s. 92.19); compared with DocFormer (Appalaraju et al., 2021), FormNetV2 outperforms it on FUNSD and CORD with nearly 2.5x less number of parameters. --- --2 Related Work Early works on form document information extraction are based on rule-ba...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "FormNetV2 augments an existing strong FormNetV1 backbone with image features bounded by pairs of neighboring tokens and the graph contrastive objective that learns to differentiate between the multimodal token representations of two corrupted versions of an input graph. The centralized design sheds new light to the understanding of multimodal form understanding. 6 Limitations Our work follows the general assumption that the training and test set contain the same list of predefined entities. With...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05383v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pretrained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Pre-trained models have achieved remarkable results in natural language (NL) tasks (Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020), inspiring the development of pre-trained models for programming language (PL) tasks (Kanade et al., 2020; Feng et al., 2020; Svyatkovskiy et al., 2020; Wang et al., 2021b; Guo et al., 2021, 2022). These models leverage source code and code structures, such as abstract syntax tree (AST) (Wang et al., 2021a; Guo et al., 2022) and data flow (Guo et al....\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Learning to Execute Previous works form the learning to execute task as a problem that reads a program and computes the program’s output. These works leverage architectures such as recurrent neural networks (Zaremba and Sutskever, 2014), graph neural networks (Bieber et al., 2020; Wang et al., 2020) and Transformers (Dehghani et al., 2019; Yan et al., 2020; Austin et al., 2021; Nye et al., 2021). Another related task algorithm induction is to read a short program, such as integer addition or...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al Setup 5.1 Dataset We build our pre-training dataset as described in Section 3. Table 2 shows some basic statistics. The 19,541 examples in CodeNetMut test split are from 39 unseen programming problems in CodeNet and have not undergone the mutation process. Additionally, we held out 10k programs from each dataset as a validation split during pre-training. For Tutorial and CodeNetMut, the ground truth trace is the execution result of the whole program. For SingleLine, since the instances are si...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a mutation-based data augmentation method to create a large and realistic Python code execution dataset and task, which pose a significant challenge for current models such as Codex. We develop CodeExecutor, a Transformer model that leverages code execution as a pre-training objective and adopts a curriculum learning strategy. CodeExecutor not only outperforms existing models on code execution, but also demonstrates its generalizability to downstream tasks such as codeto-code search a...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.10785v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Multitrack music transcription aims to transcribe a music audio input into the musical notes of multiple instruments simultaneously. It is a very challenging task that typically requires a more complex model to achieve satisfactory result. In addition, prior works mostly focus on transcriptions of regular instruments, however, neglecting vocals, which are usually the most important signal source if present in a piece of music. In this paper, we propose a novel deep neural network architecture, P...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Automatic music transcription (AMT) is a Music Information Retrieval (MIR) task that aims to transcribe a music audio input into a sequence of musical notes where each note contains attributes of onset, pitch, duration, and velocity. The output is typically delivered in the format of MIDI. In a multitrack setting, an AMT system should identify every instrument that is present in the input, and estimate the associated notes accordingly into a channel of the MIDI output. Ideally speaking, using th...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multi-instrument AMT has been explored in several previous works. Wu et al. and Hung et al. (8) trained a transcription model with related tasks in a multi- learning fashion. Tanaka et al. used clustering approaches to separate transcribed instruments while Cheuk et al. used unsupervised learning techniques to im --- --Positional embedding ‘Temporal Transformer Fig. 1. The block diagram of the Perceiver TF module. Positional embedding is first added to the latent arrays, denoted as Of. The Spect...\n",
      "\n",
      "--- METHOD ---\n",
      "OLOGY In this work, we adopt the pianoroll approach instead of the sequence-to-sequence (seq-to-seq) approach for two major reasons. First, it is easier to manipulate the loss computation to learn from partially labeled data. For example, it is non-trivial to train a seq-to-seq model that joints a vocal transcription dataset where the MIDI ground truth of accompaniments is not available. Second, the inference time complexity of seq-to-seq depends on the number of notes (tokens) due to the auto-r...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we train a Perceiver TF to model 12 instrument classes as well as vocal in a multi-task learning manner. Our result demonstrates that the proposed system outperforms the state-of-the-art counterparts (e.g., MT3 and SpecTNT) on various public datasets. Index Terms— Time-frequency, Perceiver, automatic music transcription, multi-task learning, random-mixing augmentation. 1. INTRODUCTION Automatic music transcription (AMT) is a Music Information Retrieval (MIR) task that aims to transcribe a mus...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented Perceiver TF, a novel architecture that adequately addresses the model scalability problem for multitrack AMT. To address the instrument discrimination issue, we have proposed the random-mixing augmentation technique, which significantly facilitates the data usability across datasets. Our system has demonstrated state-of-the-art performance on different public datasets. We believe Perceiver TF is generic and can be applied to other analogous tasks. --- ---6. REFERENCES Kin Wai ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.03185v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "planning [31] that expand NeRF’s performance and application domain to critical areas like autonomous driving [11]. However, quantifying the uncertainty contained in a NeRF model is a relatively new area of study, with existing methods proposing either heuristic proxies without theoretical guarantees [14, 58] or probabilistic techniques that require costly computational power [47] and/or elaborate changes to the conventional NeRF training pipeline [41, 42]. We draw inspiration from triangulation...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural Radiance Fields (NeRFs) are a class of learned volumetric implicit scene representations that have exploded in popularity due to their success in applications like novel view synthesis and depth estimation. The process of learning a NeRF from a discrete set of multiview images is plagued with uncertainty: even in perfect experimental conditions, occlusions and missing views will limit the epistemic knowledge that the model can acquire about the scene. Studying the epistemic uncertainty in...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Uncertainty Quantification studies the distribution of the responses of a system conditioned on a set of measurable input variables [43]. As a field of statistics, it has grown over many decades out of the need to measure the accuracy of scientific predictions in areas like physics [13], chemistry [39] or meteorology [30]. Uncertainty in Computer Vision. Closer to our application, estimating the uncertainty of Computer Vision systems has been a subject of study even long before the Deep Learning...\n",
      "\n",
      "--- METHOD ---\n",
      "s to quantify them are either heuristic or computationally demanding. We introduce Bayes Rays, a post-hoc framework to evaluate uncertainty in any pretrained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and show its superior performance in key metrics and applications. Additional results available at: https://bayesrays.github.io 1. Introduc...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al conditions, occlusions and missing views will limit the epistemic knowledge that the model can acquire about the scene. Studying the epistemic uncertainty in NeRF is fundamental for tasks like outlier detection [21] and next-best-view thresholding by uncertainty Clean Novel View Figure 1. We introduce BayesRays, a post-hoc algorithm to estimate the spatial uncertainty of any pre-trained NeRF of any arbitrary architecture. Our method requires no additional training and can be used to clean up ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We have introduced BayesRays, an algorithm to quantify the uncertainty of any trained Neural Radiance Field without independently of its architecture and without additional training nor access to the original training images. Our algorithm outputs a spatial uncertainty field, which we have shown is meaningfully correlated to the NeRF depth error and can be thresholded for use in applications like NeRF cleanup. We discretize our spatial deformation field using a uniform grid, which can lead to ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.04790v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present a vision and language model named MultiModal-GPT to conduct multiround dialogue with humans. MultiModal-GPT is capable of following diverse instructions, such as generating detailed captions, counting specific objects, and addressing general inquiries posed by users. The model is efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) incorporated in both the gated-cross-attention and self-attention components of the language model. Our approach involves constructing in...\n",
      "\n",
      "--- METHOD ---\n",
      "3.1. Architecture The proposed MultiModal-GPT is based on the open-flamingo model (i. As shown in Figure[T] MultiModal-GPT consists of a vision encoder from CLIP (13), a perceiver resampler to receive the spatial features from the vision encoder, and a language decoder LLaMA C9). Note that the language decoder is conditioned on the spatial features from the perceiver resampler by cross-attention in order to encode the feature of vision into text. Please refer to i) for more details of the model ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11759v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompttuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the eff...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Pretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022), commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020, 2022; Zhang e...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Previous work has shown that LLMs display memorization and has explored a range of...\n",
      "\n",
      "--- METHOD ---\n",
      "s that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, * Work done while the author was an intern at Amazon; mustafa.ozdayi@utdallas.edu t perisc@amazon.com methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk. Methods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s!. 2 Background and Related Work Previous work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018, 2020, 2022). Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. ‘https://github.com/amazon-science/contro...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These r...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07870v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release AGENTS, an open-source library with the goal of opening up these advances to a wider non-specialist audience. AGENTS is carefully engineered t...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "“An autonomous agent is a system situated within and a part of an environment that senses the environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.” Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents [Franklin and Graesser, 1996] Large Language Models (LLMs) [Brown et al., 2020, Ouyang et al., 2022, OpenAI, 2023] such as ChatGPT make it possible to build autonomous agents that can automatically solve complicated ta...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Autonomous Language Agents The concept of language agents has become very popular recently and a variety of language agents targeting different tasks have been proposed. For example, Generative Agents [Park et al., 2023] developed language agents to mimic human social behavior, WebAgent [Gur et al., 2023] demonstrated the possibility to build language agents that can complete the tasks on real websites following natural language instructions, Qian et al. [2023] and MetaGPT [Hong et al., 2023...\n",
      "\n",
      "--- METHOD ---\n",
      "s to observe the environment (agent ._observe (environment) ), act according to its current state (agent ._act ()) and update its memory (agent. _update_memory()). All these methods are wrapped in the agent .step() method. This factorization enables developers to customize agents with new functionalities easily. Unlike existing language agent frameworks that assume an agent must be based on an LLM, we include a “_is_human” property to an agent. If it is set to “True”, the (agent ._act ()) will o...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s [Li et al., 2023], software development [Qian et al., 2023], etc. One new feature for multi-agent communication in AGENTS is the “dynamic scheduling” feature. Instead of --- --scheduling the order for the agents to act with hard-coded rules, dynamic scheduling provides an option to define a controller agent that acts as a “moderator” and decides which agent to perform the next action considering their roles and the current history. Dynamic scheduling has the potential to make communication bet...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "LLMs and language agents powered by them are playing increasingly important roles in both the NLP/AI community and our society in general. AGENTS, is a unified framework and open-source library for language agents. AGENTS aims to facilitate developers to build applications with language agents, researchers to conduct language agents research, and general non-technical audiences to build and customize personalized language agents. References Stan Franklin and Art Graesser. Is it an agent, or just...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.02982v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We propose PolyVoice, a language modelbased framework for speech-to-speech translation (S2ST) system. Our framework consists of two language models: a translation language model and a speech synthesis language model. We use discretized speech units, which are generated in a fully unsupervised way, and thus our framework can be used for unwritten languages. For the speech synthesis part, we adopt the existing VALL-E X approach and build a unit-based audio language model. This grants our framework...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Speech-to-speech translation (S2ST) is a challenging task as it encounters all the difficulties of automatic speech recognition (ASR), machine translation (MT) and text-to-speech (TTS) synthesis. Different from conventional cascade approach (Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006), the direct approach (Jia et al., 2019, 2022a) has the advantages of low latency and simplified pipeline. Existing direct S2ST approaches can be further classified according to whether the model pre...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s in TTS and S2ST. Details of our...\n",
      "\n",
      "--- METHOD ---\n",
      "s have just begun emerging. Thus, we are motivated to investigate the performance of LM-based method in S2ST. In this paper, we propose a semantic unit-based framework for S2ST system. Our framework consists of two LMs: a translation LM and a speech synthesis LM. The translation LM processes the semantic --- --units of the source language and translates the sequence into semantic units of the target language. For the speech synthesis part, we adopt the VALLE X approach (Zhang et al., 2023) for t...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that our system can generate speech with high translation quality and audio quality. Speech samples are available at https://speechtranslation. github. io/polyvoice. 1 Introduction Speech-to-speech translation (S2ST) is a challenging task as it encounters all the difficulties of automatic speech recognition (ASR), machine translation (MT) and text-to-speech (TTS) synthesis. Different from conventional cascade approach (Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006), ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future Work In this paper, we propose a semantic unit-based framework for S2ST. Our framework consists of two LMs: a translation LM (U-XLM) and a speech synthesis LM (U-SLM). We show that our unitbased S2ST system performs better than existing systems in terms of ASR-BLEU, ASV and naturalness. Furthermore, we demonstrate the system ability in unwritten language scenario without any use of the Spanish text transcript. As our system performance is highly related to the quality of the semantic ...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.08568v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction finetuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Jnstruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, Hu...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently, Large Language Models (LLMs) [1}{9] have garnered significant attention and demonstrated impressive success. Notably, OpenAl’s ChatGPT stands out as a prominent example. Leveraging extensive pre-training on vast amounts of internet data and further fine-tuning with detailed instruction data 10}, these models have achieved state-of-the-art (SOTA) zero-shot performance across diverse tasks. This trend is also observed in the domain of code understanding and generation. Numerous Code LLMs...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Large Language Models. Recently, LLMs have demonstrated remarkable achievements across a broad spectrum of tasks. Prominent tech companies have made significant strides in developing highly proficient LLMs. These include OpenAl’s GPT3&4 [1] [2], Google’s PaLM [3]/4], and Bard] DeepMind’s Chinchilla [5], and Gopher [6], as well as Anthropic’s Claudd?] However, it is important to note that these models are closed-source and can only be accessed through specific APIs or may not be accessible at all...\n",
      "\n",
      "--- METHOD ---\n",
      "to the domain of code. Through comprehensive...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https ://github.com/nlpxucan/WizardLM 1 Introduction Recently, Large Language Models (LLMs) [1}{9] have ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s: 1. WizardCoder outperforms the largest closed-source LLMs, including Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being significantly smaller. --- --Table 2: Performance of WizardCoder and baseline models on DS-1000. All models are evaluated with the same set of hyper-parameters: temperature=0.2, top_p=0.5, max_length=1024. Scores are average pass@1 accuracy over 40 samples. Matplotlib (plt) task does not have the right context, so insertion and completion scores are identical. Format Model...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.06546v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves 90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity au...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Generative modeling of high-resolution audio is difficult due to high dimensionality (~44,100 samples per second of audio) , and presence of structure at different time-scales with both short and long-t -term dependencies. To ‘mitigate this problem, audio generation is typically divided into two icting audio conditioned on some intermediate representation such as mel-spectrograms and 2) predicting the intermediate representation given some conditioning information, 34]. This can be interpreted a...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "High fidelity neural audio synthesis: Recently, generative adversarial networks (GANs) have emerged as a solution to generate high-quality audio with fast inference speeds, due to the feedforward (parallel) generator. MeIGAN [19] successfully trains a GAN-based spectrogram inversion (neural vocoding) model. It introduces a multi-scale waveform discriminator (MSD) to penalize structure at different audio resolutions and a feature matching loss that minimizes L1 distance between discriminator feat...\n",
      "\n",
      "--- METHOD ---\n",
      "outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling. 1 Introduction Generative modeling of high-resolution audio is difficult due to high dimensionality (~44,100 samples per second of audio) , and presence of structure at different time-scales with both short and long-t -term dependencies. To ‘mitigate this probl...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we find replacing Leaky ReLU activations with Snake function to be an influential change that significantly improves audio fidelity (Table[2). 3.2 Improved residual vector quantization While vector quantization (VQ) is a popular method to train discrete auto-encoder, there are many practical struggles when training them. Vanilla VQ-VAEs struggle from low codebook usage due to poor initialization, leading to a significant portion of the codebook being unused. This reduction in effective codebo...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented a high-fidelity universal neural audio compression algorithm that achieves remarkable compression rates while maintaining audio quality across various types of audio data. Our method combines the latest advancements in audio generation, vector quantization techniques, and improved adversarial and reconstruction losses. Our extensive evaluation against existing audio compression algorithms demonstrates the superiority of our approach, providing a promising oundation for future h...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.01200v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time fullbody capture while being accurate in world space. In this work, we introduce ProxyCap, a human-centric proxy-tomotion learning scheme to learn world-space motions from a proxy dataset of 2D skeleton sequences and 3D ro...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Motion capture from monocular videos is an essential technology for various applications such as gaming, VR/AR, sports analysis, etc. One ultimate goal is to achieve realtime capture while being accurate and physically plausible in world space. Despite the recent advancements, this task is still far from being solved, especially under the settings of in-the-wild captures with hand-held moving cameras. Compared to optimization-based methods [4, 10, 12, 20, 40, 50, 64], learning-based approaches [...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Monocular motion capture has been an active research field recently. We give a brief review of the works related to ours and refer readers to [57] for a more comprehensive survey. Motion Capture Datasets. Existing motion capture datasets are either captured with marker-based [13, 51] or marker-less [42, 65, 75, 76] systems. Due to the requirement of markers or multi-view settings, the diversity of these datasets is limited in comparison with in-the-wild datasets. To enrich the motion datasets, n...\n",
      "\n",
      "--- METHOD ---\n",
      ", ProxyCap, is a real-time monocular full-body capture solution to produce accurate human motions with plausible foot-ground contact in world space. Abstract Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time fullbody capture while being accurate in world space. In this wo...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we demonstrate that the feature fusion in Eq. 2 can make two tasks benefit each other to pro duce more comparable wrist poses in the full-body model. 4.3. Contact-aware Neural Motion Descent At the second stage of our method, the initial motion predictions will be refined to be more accurate and physically plausible with the proposed contact-aware neural motion descent module. As shown in Fig. 2, this module takes the 2D misalignment and body-ground contact status as input and updates motion ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present ProxyCap, a real-time monocular full-body motion capture approach with physically plausible foot-ground contact in world space. We leverage a proxy dataset based on 2D skeleton sequences with accurate 3D rotational motions in world space. Based on the proxy data, our network learns motions from a human-centric perceptive, which enhances the consistency of human motion predictions under different camera trajectories. For more accurate and physically plausible motion capt...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.02245v2.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "In the past few years, foundation models have thrived and succeeded in linguistic and visual tasks, showing astonishing zero-shot and few-shot capabilities. Their advances encourage researchers and industries to extend the boundaries of what artificial intelligence can do and have shown some fantastic products (e.g., ChatGPT (1}) with the potential to change the world. Recently, Kirillov et al. [11] proposed a new vision foundation model for image segmentation, the Segment Anything Model (SAM), ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "6.1.1 2D tasks with SAM Kirillov et al. proposed a new vision foundation model for image segmentation, the Segment Anything Model (SAM), trained on a huge dataset called SA-1B to solve the segment anything task. The flexible prompting support, ambiguity awareness, and vast training data endow the SAM with powerful generalization, enabling the ability to solve downstream segmentation problems using prompt engineering, inspiring many following works. SAMPolyp applies SAM to the polyp segmentation ...\n",
      "\n",
      "--- METHOD ---\n",
      "on the large-scale Waymo Open Dataset [19], and the results show the great potential of SAM on 3D object detection. Although this paper is only an early attempt, it gives a positive signal for applying vision foundation models like SAM for 3D vision tasks, especially for 3D object detection. 2. Method We consider point cloud as the input of our method, which is a 3D representation and naturally sparse, while SAM is trained for 2D images with dense semantics. Our basic idea is to translate LiDAR ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s We evaluate our method on the Waymo Open Dataset {19}, one of the large-scale datasets for autonomous driving. The dataset is split into 798 training sequences,validation sequences, and 150 testing sequences. Since our method performs zero-shot object detection, we only focus on the validation sequences. For the metrics, because of the natural sparsity of point clouds and the lack of semantic label outputs, we only care about the mAP and mAPH of VEHICLE with a distance of at most 30 meters in ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper explores the zero-shot 3D object detection with the visual foundation model SAM and proposes the SAM3D. To narrow the gap between the training data of SAM and 3D LiDAR signals, we use the BEV images to represent 3D outdoor scenes. We propose a SAM-powered BEV processing pipeline to utilize the great zero-shot capability of SAM for zero-shot 3D object detection. Qualitative and ablation experiments on the Waymo open dataset --- --show promising results for adapting the zero-shot abilit...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.12001v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Scale In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models onout-of-domain tasks...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently, there has been a surge in the release of Large Language Models (LLMs) by both industrial and academic institutions. These models vary from open-source releases such as OPT (Zhang et al., 2022) and LLAMA (Touvron et al., 2023) to closed-source ones like GPT-3 (Brown et al., 2020) and PALM (Chowdhery et al., 2022). In addition, researchers have developed models that are finetuned on top of these foundational models to better 13B 6.7B 1.3B OPT OPT-R OPT-RE Zeroshot Fewshot Fewshot-E Finet...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Reasoning LLMs LLMs have made significant advancements in the field of NLP and related areas (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022), especially with the advent of the pre-train, prompt, and predict paradigm (Liu et al., 2021). This paradigm has enabled these models to solve a multitude of tasks through incontext fewshot or zeroshot learning using instructions (Wei et al., 2021b; Iyer et al., 2022). However, their reasoning abilities have been a subject of debate in rece...\n",
      "\n",
      "--- METHOD ---\n",
      "s across reasoning skills for each model. However, we notice that it has a more noticeable effect on the performance of the vanilla OPT model, as shown in Table 5. Additionally, we observe a consistent increase in the average performance across all tasks from Fewshot to Fewshot-E, as well as from OPT to OPT-R to OPT-RE models, indicating that explanations do have a small effect on performance during both finetuning and prompting. Finally, Table 4 presents a summary of the results, indicating whi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s are structured around three key dimensions: finetuning, prompting, and scale, each of which is comprised of three distinct components (See Figure 1). Finetuning: (1) a (vanilla) unfinetuned OPT model; (2) A finetuned OPT model without explanations (OPT-R); and, (3) A finetuned OPT model with explanations (OPT-RE). Prompting: (1) zero-shot prompting; (2) Fewshot prompting without explanations; and, (3) Fewshot prompting with explanations. Finally, Scale: (1) 1.3B; (2) 6.7B; and, (3) 13B. Accord...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this study, we investigated the impact of incorporating explanations during finetuning and prompting on three different sizes of the OPT model. Through a systematic and comprehensive evaluation process that considered three key dimensions, we found that while explanations did provide a small improvement in performance, the effect was not significant when incorporated in the in-context demonstrations during inference for the finetuned models. Additionally, our results showed that both finetune...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.13702v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) perform better when they produce step-by-step, “Chain-ofThought” (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model’s actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variat...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "It is often critical to understand why a large language model (LLM) provided the output it did, to understand the extent to which we can rely on its output (especially in high-stakes settings such as medicine; Gunning et al., 2019; Rudin, 2019). Many have claimed that the interpretability or explainability ‘Al authors at Anthropic, except Jan Brauner who is at University of Oxford. Correspondence to: Tamera Lanham <tamera@anthropic.com>, Ethan Perez <ethan@anthropic.com>. (Homan)Question. 5! equ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Analysis of Chain of Thought Faithfulness Recent work has analyzed CoT faithfulness in different ways than our work. Gao (2023) use Shapley analysis to show that certain tokens of the CoT are much more important than others or the final answer. Our work proposes different tests of CoT faithfulness with lower computational costs. Madaan & Yazdanbakhsh (2022) investigate CoT via counterfactual prompting and find that some aspects of the prompt are less important than others for the final answer re...\n",
      "\n",
      "--- METHOD ---\n",
      "s for LLMs to produce more faithful reasoning and for detecting when the model’s reasoning is untrustworthy. 2. Measuring Chain of Thought Faithfulness In this section, we investigate hypotheses that point against chain of thought faithfulness by perturbing the chain of thought and observing the model’s behavior. 2.1. Methods Model For most...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s in this section, we use a 175B-parameter pretrained, decoder-only transformer (Vaswani et al., 2017) LLM (Radford et al., 2018; 2019; Brown et al., 2020), fine-tuned to be a helpful dialog assistant using reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020), as in Bai et al. (2022). The one exception is the model used to generate mistakes in the adding mistakes experiment (§2.4); the model used here is the pretrained LM, withou...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "has already been guaranteed (Holzinger et al., 2017). Since post-hoc reasoning does not change the model’s answer, there is no strong reason to believe that such reasoning would be faithful. In this work, we test for post-hoc reasoning by truncating the chain of thought or adding mistakes to it. We find great variation in how much LLMs use CoT on different tasks, not using CoT at all for some tasks while relying upon it heavily for other tasks. * Unfaithful reasoning due to test-time computation...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07990v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning mode...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Many NLP studies have highlighted the importance of entities to understanding the semantics of a document (Wu et al., 2020b; Meij et al., 2012). Automatically identifying entities in unstructured text documents and linking them to an underlying knowledge base, such as Wikipedia, is one of the core NLP tasks, with multiple shared tasks (Tjong Kim Sang and De Meulder, 2003; Strauss et al., 2016), benchmarks (Hoffart et al., 2011; Hovy et al., 2006; Pradhan et al., 2013; Rijhwani and Preotiuc-Pietr...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Understanding the aboutness of a document is one of the long-standing goals of research in both Information Retrieval and Natural Language Processing (Gamon et al., 2013). Several types of approaches have been proposed, including extracting key-terms (Hulth, 2003; Mihalcea and Tarau, 2004), identifying latent topics (Blei et al., 2003), or generating text summaries (Erkan and Radev, 2004). There has been a recent focus in using entities to understand the content of a document. Towards this goal,...\n",
      "\n",
      "--- METHOD ---\n",
      "uses a cross-encoder architecture where a target entity’s name or alias and its contextual mentions in a text document are encoded by a PLM encoder. The classifier uses the contextual representation and, optionally, positional information about the entity encoded through the decile position embedding vector of mentions to determine the salience score of a target entity. We conduct...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on four publicly available datasets, two of which were human annotated and two that were curated semi-automatically. We fine-tune several cross-encoders using PLMs and demonstrate that these yield consistent and significant improvements over feature-based methods, as well as prompting instruction-tuned PLMs. The latter shows the novelty and complexity of the task of entity salience detection, which requires the model to learn significant task-specific semantic knowledge for this natural langua...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper aims to leverage the semantic knowledge encoded in pre-trained language models for entity salience detection. We propose the crossencoder method based on Transformer-based PLMs with positional representations and compare its performance to several ML-based methods, heuristic methods, and instruction-tuned LLMs across four different datasets, two human-annotated and two automatically curated. Across all our experiments, the cross-encoder model based on pre-trained language models outpe...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.02119v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Video outpainting aims to adequately complete missing areas at the edges of video frames. Compared to image outpainting, it presents an additional challenge as the model should maintain the temporal consistency of the filled area. In this paper, we introduce a masked 3D diffusion model for video outpainting. We use the technique of mask modeling to train the 3D diffusion model. This allows us to use multiple guide frames to connect the results of multiple video clip inferences, thus ensuring tem...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The task of video outpainting is to expand edge areas of videos according to the provided contextual information (the middle part of the videos). In recent years, image outpainting [4, 5, 22, 28, 30, 38, 42] has been heavily researched and has yielded very promising results with the advent of GAN(Generative Adversarial Network) and Diffusion Model. However, video outpainting is currently far from achieving ideal results. Different from image outpainting, which only considers the spatial appearan...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "This section introduces the related diffusion model, mask modeling, and the Coarse-to-Fine pipeline. Diffusion Model. The diffusion model [19, 26, 32] has recently become the best technology in image generation [28, 30], especially in video generation [18, 25, 31]. Compared with GAN [12], it can generate samples with richer diversity and higher quality [8]. Considering the significant achievements of the diffusion model in video generation, we adopt it as the main body of our video outpainting...\n",
      "\n",
      "--- METHOD ---\n",
      "“Both authors contributed equally to this research while interning at Alibaba Group. * Corresponding author. This work is licensed under a Creative Commons Attribution International 4.0 License. MM °23, October 29-November 3, 2023, Ottawa, ON, Canada © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.Beijing, China Beijing, China Jianfeng Zhan zhanjianfeng@ict.ac.cn Institute of Computing Technology, Chinese Academy of Sciences Beijing...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our method “Both authors contributed equally to this research while interning at Alibaba Group. * Corresponding author. This work is licensed under a Creative Commons Attribution International 4.0 License. MM °23, October 29-November 3, 2023, Ottawa, ON, Canada © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.Beijing, China Beijing, China Jianfeng Zhan zhanjianfeng@ict.ac.cn Institute of Computing Technology, Chinese Acad...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose a 3D diffusion model based on mask modeling for video outpainting. We use bidirectional learning and globally encoding video frames as a prompt for cross-attention with --- --Hierarchical Masked 3D Diffusion Model for Video Outpainting context. The bidirectional learning approach of mask modeling allows us to have more flexible strategies in the inference stage while better perceiving adjacent frame information. The addition of a global video clip as a prompt further im...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07314v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including so...\n",
      "\n",
      "--- METHOD ---\n",
      "s have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4 kHz to 8 kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2 kHz to 16 kHz to a high-resolution audio signal at 24...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al settings, limiting its applicability in real-world scenarios. An important challenge in audio super-resolution, as highlighted in [5], is the issue of bandwidth mismatch. This occurs when the bandwidth of the test data differs from that of the training data, leading to model failure. However, this issue has not received significant attention in the literature, as previous works typically assume consistent bandwidth settings for both training and testing data. In practice, the input bandwidth ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper presents AudioSR, a 48kHz audio super-resolution model that is capable of working with diverse audio types and arbitrary sampling rate settings. Through evaluation of multiple audio super-resolution benchmarks, AudioSR demonstrates superior and robust performance on various types of audio and sampling rates. Additionally, our subjective evaluation highlights the effectiveness of AudioSR in enabling plug-and-play quality improvement for the audio generation models, including AudioLDM, ...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04632v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "StableDiffusion is a revolutionary text-to-image generator that is causing a stir in the world of image generation and editing. Unlike traditional methods that learn a diffusion model in pixel space, StableDiffusion learns a diffusion model in the latent space via a VOGAN, ensuring both efficiency and quality. It not only supports image generation tasks, but also enables image editing for real images, such as image inpainting and local editing. However, we have observed that the vanilla VQGAN us...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Diffusion models have emerged as the most popular generative models, achieving remarkable results in image synthesis. Early diffusion models required significant computational resources, as they performed the diffusion process in the high-dimensional pixel space of RGB images. To reduce the training cost while preserving the generation quality, laten diffusion model (LDM) employs VQGAN to move the diffusion step to a low-dimensional latent space. In a subsequent development, StableDiffusion has ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Diffusion Models Diffusion models are a powerful family of generative models that have recently evolved and drawn significant attention due to their impressive performance on various tasks. Recent works have demonstrated that diffusion model can achieve astonishing results in high-fidelity image generation, even outperforming generative adversarial networks. Diffusion models are naturally ideal for learn --- --ing models from complex and diverse data, and many variants have been proposed re...\n",
      "\n",
      "--- METHOD ---\n",
      "s that learn a diffusion model in pixel space, StableDiffusion learns a diffusion model in the latent space via a VOGAN, ensuring both efficiency and quality. It not only supports image generation tasks, but also enables image editing for real images, such as image inpainting and local editing. However, we have observed that the vanilla VQGAN used in StableDiffusion leads to significant information loss, causing distortion artifacts even in non-edited image regions. To this end, we propose a new...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that it can significantly improve the inpainting and editing performance, while maintaining the original text-to-image capa bility. The code is available at\\https://github.com/ buxiangzhiren/Asymmet ric_VOQGAN --- --1. Introduction Diffusion models have emerged as the most popular generative models, achieving remarkable results in image synthesis. Early diffusion models required significant computational resources, as they performed the diffusion process in the high-dimensional pix...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present a novel asymmetric VQGAN for StableDiffusion with two new design features. Firstly, our decoder incorporates an additional conditional branch, allowing it to accept both the output of the VQGAN encoder and task-specific priors as input. Secondly, our decoder is designed to be more complex (e.g. deeper and wider) than the encoder, enabling it to better preserve local details of non-edited regions and recover details from the quantized output of the encoder. Our asymmetri...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01904v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper presents an improved DETR detector that maintains a “plain” nature: using a single-scale feature map and global cross-attention calculations without specific locality constraints, in contrast to previous leading DETR-based detectors that reintroduce architectural inductive biases of multi-scale and locality into the decoder. We show that two simple technologies are surprisingly effective within a plain design to compensate for the lack of multiscale feature maps and locality constrain...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The recent revolutionary advancements in natural language processing highlight the importance of keeping task-specific heads or decoders as general, simple, and lightweight as possible, and shifting main efforts towards building more powerful large-scale foundation models [37, 11, 2]. However, the computer vision community often continues to focus heavily on the tuning and complexity of task-specific heads, resulting in designs that are increasingly heavy and complex. + Equal contribution. {yuhu...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "DETR-based object detection. DETR [4] has impressed the field for its several merits, including the conceptually straightforward and generic in applicability, requiring minimal domain knowledge that avoids customized label assignments and non-maximum suppression, and being plain. While the original DETR maintains a plain design, it also suffers from slow convergence rate and lower detection accuracy. There have been many follow-up works including [35, 16, 9, 47, 55, 53, 52, 17, 54], and now many...\n",
      "\n",
      "--- METHOD ---\n",
      "s follows this trajectory. The original DETR approach [4] is impressive in that it discarded complex and domainspecific designs such as multi-scale feature maps and region-based feature extraction that require a dedicated understanding of the specific object detection problem. Yet, subsequent developments [55, 54] in the field have reintroduced these designs, which do improve training speed and accuracy but also contravene the principle of “fewer inductive biases” [13]. In this work, we aim to i...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that the BoxRPB term can well guide the cross-attention computation to be well dedicated to individual objects (see Figure 4, and it dramatically improves detection accuracy by +8.9 mAP over a plain DETR baseline of 37.2 mAP on the COCO benchmark (see Table 2). The utilization of MIM pre-training is another crucial technology in enhancing the performance of plain DETR. Our results demonstrate also a significant improvement of +7.4 mAP over the plain DETR baseline (see Table 2), which may ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "that we can completely eliminate the need for multi-scale feature maps in both the backbone and Transformer decoder by using our proposed BoxRPB scheme and MIM pre-training. 5.4. Application to a plain ViT In this section, we build a simple and effective fully plain object detection system by applying our approach Figure 4: Visualizations of the cross-attention maps of models w. or w/o. BoxRPB. For each group, the first column shows the input image and the object query. The first row presents th...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05591v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In a range of recent works, object-centric architectures have been shown to be suitable for unsupervised scene decomposition in the vision domain. Inspired by these methods we present AudioSlots, a slot-centric generative model for blind source separation in the audio domain. AudioSlots is built using permutation-equivariant encoder and decoder networks. The encoder network based on the Transformer architecture learns to map a mixed audio spectrogram to an unordered set of independent source emb...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently there has been a lot of research into neural network based architectures that operate on set-structured data and architectures that learn to map from unstructured inputs to set-structured output spaces. In particular, in the vision domain, slot-centric or objectcentric architectures underpin recent advances in object detection and unsupervised object discovery These object-centric architectures have an inbuilt inductive bias of permutation equivariance, making them a natural fit for the...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Our work explores a novel set-based generative modeling approach for sound separation. In the following, we provide a brief overview on recent prior learning-based approaches for sound separation as well as on set-based (or slot-centric) neural network architectures used in other domains. Sound separation. A variety of neural network...\n",
      "\n",
      "--- METHOD ---\n",
      "s we present AudioSlots, a slot-centric generative model for blind source separation in the audio domain. AudioSlots is built using permutation-equivariant encoder and decoder networks. The encoder network based on the Transformer architecture learns to map a mixed audio spectrogram to an unordered set of independent source embeddings. The spatial broadcast decoder network learns to generate the source spectrograms from the source embeddings. We train the model in an end-to-end manner using a pe...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s). It works by assigning costs or weights to each possible pair and then selects pairs which minimize the total cost or weight associated with them. Finally the mean squared error between matched ground-truth and estimated spectrograms is minimized as the training objective to optimize the network parameters. Source Separation: During testing, given an input mixture waveform we first break it into multiple non-overlapping waveforms of length 0.5-seconds. We add zero-padding at the end in case t...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present AudioSlots, a slot-centric generative architecture for audio spectrograms. We demonstrate a proof of concept that AudioSlots holds promise for addressing the task of audio source separation using structured generative models. While our current implementation of AudioSlots has several limitations, including low reconstruction fidelity for high-frequency features and requiring separated audio sources as supervision, we are optimistic that these can be overcome and outline several possib...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.18247v2.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Story visualization, also known as visual storytelling, is a vital method for effectively conveying narrative content to a diverse range of audiences. It has a wide range of applications in education and entertainment [Yin et al. 2022], e.g., children’s comic books. In this work, story visualization is formulated as such a problem, i.e., given a story in plain text and the portrait images of a few characters, generate a series of images to express the story visually. --- --2 + Yuan Gong, Youxin ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Story Visualization The earlier works in the field of story visualization primarily relied on GAN or VAE-based approaches [Chen et al. 2022; Li 2022; Liet al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021; Song et al. 2020]. For instance, StoryGAN [Li et al. 2019a] uses both the full story and individual sentences as inputs to generate contextually relevant images, employing image and story discriminators. DUCOStoryGAN [Maharana et al. 2021], on the other hand, introduces a dual lear...\n",
      "\n",
      "--- METHOD ---\n",
      "for effectively conveying narrative content to a diverse range of audiences. It has a wide range of applications in education and entertainment [Yin et al. 2022], e.g., children’s comic books. In this work, story visualization is formulated as such a problem, i.e., given a story in plain text and the portrait images of a few characters, generate a series of images to express the story visually. --- --2 + Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Y...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and a user study are conducted to validate the effectiveness and flexibility of interactive editing of the proposed system. 1 INTRODUCTION Story visualization, also known as visual storytelling, is a vital method for effectively conveying narrative content to a diverse range of audiences. It has a wide range of applications in education and entertainment [Yin et al. 2022], e.g., children’s comic books. In this work, story visualization is formulated as such a problem, i.e., given a story in pl...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present an innovative system for generic interactive story visualization capable of handling novel characters and scenes while --- --8 + Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang maintaining identity consistency, alignment between text and visual content, and reasonable object layouts. The system’s four interconnected components - story-to-prompt generation (S2P), textto-layout generation (T2L)...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.03514v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for large models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale imagetext pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automa...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies including manual checks, referring to WordNet [7], translating and merging tags, etc. Tags within the same synonym group are assigned the same tag ID, resulting in 4585 tag IDs in the label system. 3.2. Datasets Similar to BLIP and Tag2Text [10], we pretrain our model on widely-used open-source datasets.million (4M) image and 14 million (14M) image settings are adopted. The 4M setting includes two humanannotated datasets, COCO [16] (113K images, 557K captions) and Visual Genome (101K ima...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "4.1. Experimental Setting Test Benchmarks. We conducted a comprehensive evaluation of the models on various popular benchmark datasets across different computer vision tasks, including classification, detection, and segmentation, as summarized in Table[f] For classification, we adopt the OpenImages V6 [14], which contains 9605 categories. However, due to issues of missing labels and incorrect annotations within the OpenImages dataset, we curated two high-quality subsets: OpenImagescommon, compri...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present the Recognize Anything Model (RAM), a strong foundation model designed for image tagging, which heralds a novel paradigm in this field. RAM demonstrate the zero-shot ability to recognize any category with high accuracy, surpassing the performance of both fully supervised models and existing generalist approaches like CLIP and BLIP. RAM represents a considerable advancement for large-scale models in the field of computer vision, holding the potential to empower the recognition capabili...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.15131v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "With the popularity of implicit neural representations, or neural radiance fields (NeRF), there is a pressing need for editing methods to interact with the implicit 3D models for tasks like post-processing reconstructed scenes and 3D content creation. While previous works have explored NeRF editing from various perspectives, they are restricted in editing flexibility, quality, and speed, failing to offer direct editing response and instant preview. The key challenge is to conceive a locally edit...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Implicit neural representations, e.g. neural radiance fields (NeRF) [24], have gained increasing attention as novel 3D representations with neural networks to model a 3D scene. Benefiting from the high reconstruction accuracy and rendering quality with relatively low memory consumption, NeRF and its variations [50, 3, 33, 26, 4, 45, 41] have demonstrated great potential in many 3D applications like 3D reconstruction, novel view synthesis, and Vir --- --tual/Augmented Reality. With the popularity...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Novel view synthesis. Given a set of posed image captures of a scene, the task of novel view synthesis is to generate photo-realistic images from arbitrary novel views. Recently, neural network have been introduced into the --- --Editing Guidance Generation x8, ds Source Space S —» Fm i yA | S Local Loss Two-Stage Student Training for Instant Preview Student Training Supervision | Loss “ Pa = |e mae ” Editing (Scale) is x',de Teacher Local Update —= = = Student Target Space T Local Pretraining G...\n",
      "\n",
      "--- METHOD ---\n",
      "and system Seal-3D, which achieves instant (+1s) preview (left) by our novel pretraining strategy. High-quality editing results can be further obtained by a short period (in 1 or 2 minutes) of finetuning. The editing results of our implemented editing tools (right) are view-consistent with rich shading details (e.g. shadows) on the original surface (left). Abstract With the popularity of implicit neural representations, or neural radiance fields (NeRF), there is a pressing need for editing metho...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al illustrations will be presented in Sec. 4.3 and Fig. 12. Global Finetuning. After pretraining, we continue to finetune fe to refine the coarse preview to a fully converged result. This stage is similar to the standard NeRF training, except that the supervision labels are generated by the teacher inference process instead of image pixels. Lelobal = So rs/|C7 = C3 lp +A|D? -— D8, (8) reR where R denote the set of sampled rays in the minibatch and (CT, DT),(C%, D®) are accumulated along ray r by...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have introduced an interactive framework for pixellevel editing for neural radiance fields supporting instant preview. Specifically, we exploit the two-stage teacherstudent training method to provide editing guidance and design a two-stage training strategy to achieve instant network convergence to obtain coarse results as a preview. Unlike previous works, our method does not require any explicit proxy (such as mesh), improving interactivity and userfriendliness. Our method also supports pres...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.05399v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance. MAM offers several significant advantages over previous specialized image matting networks: (i) MAM is capable of dealing with various types of image matting, including semantic, instance, and referring image matting with only a single model; (ii) MAM leverages the featu...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Image Matting, as a long-standing computer vision task, aims to estimate the alpha matte a given an input image J [45]. The matting target is mainly around human beings or other objects at the semantic level [26,41,49]. Recent works --- --have extended the scope of image matting to more complex scenarios like image instance matting [42], which requires instance-aware alpha matte predictions and referring image matting [28], which extracts the alpha matte given natural language description. Previ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s 2.1. Image Matting Given an image J, which can be view as a combination of foreground image F' and background image B with coefficient alpha matte a, I=aF+(1—a)B qd) Image Matting is to estimate a given only J as inputs. Traditional...\n",
      "\n",
      "--- METHOD ---\n",
      "s [28, 29, 36, 37, 42, 47, 51,54, 59] have been proposed to address specific image matting tasks on corresponding benchmarks. These methods are tailored to individual datasets and lack the flexibility to handle various image matting tasks due to their fixed model designs. This limitation has hindered the development of more generalized and versatile image matting models. As a result, there is a growing interest in developing more adaptive and efficient image matting frameworks that can handle di...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that MAM achieves comparable performance to the state-of-the-art specialized image matting models under different metrics on each benchmark. Overall, MAM shows superior generalization ability and can effectively handle various image matting tasks with fewer parameters, making it a practical solution for unified image matting. Our code and models are open-sourced at https://github.con/SHI-Labs/Matting-Anything. 1. Introduction Image Matting, as a long-standing computer visi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we introduce Matting Anything Model (MAM), which uses the Segment Anything Model (SAM) as a guidance module with a lightweight Mask-to-Matte (M2M) module to refine the mask output into the alpha matte of the target instance. M2M is designed to handle various image matting tasks, including semantic, instance, and referring image matting, using a single model based on user prompts including points, boxes, and text. We evaluate MAM on six image matting benchmarks and demonstrate that...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.11795v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact s...\n",
      "\n",
      "--- METHOD ---\n",
      "ology Our approach will be centered around the use of a large language model (LLM) to model sequences of embeddings irrespective of the modality of the embedding. Inspired by the work of which utilize a visual encoder to generate a fixed-length sequence of visual embeddings in the same space as text embeddings, we utilize a pretrained audio encoder to generate a variable-length sequence of audial embeddings. By conditioning on the audial embeddings, the large language model can be allowed to per...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to gener...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Overall this work has shown a simple procedure for enabling multilingual speech recognition with a large language model. By prepending an audio embedding sequence, the large language model can be triggered to perform speech recognition in a decoder-only fashion. Furthermore, this work investigates a range of different factors that are key in enabling better recognition performance including analysing the audio encoder stride & size. The paper also investigates the importance of the LLM by compar...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09636v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm genera...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Modeling discrete representations of audio produced by neural codecs (Zeghidour et al., 2022; Défossez et al., 2022) makes the task of audio generation amenable to the powerful Transformer-based sequence-to-sequence modeling approaches (Vaswani et al., 2017). Casting unconditional and conditional audio generation as sequence-to-sequence modeling has unlocked rapid progress in speech continuation (Borsos et al., 2022), text-to-speech (Wang et al., 2023; Kharitonov et al., 2023), and general audio...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Modeling the tokens of neural audio codecs. Unsupervised speech embeddings (Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021) have provided a low-framerate representation of the underlying signal which remains rich enough after discretization for language models to generate intelligible speech from a specific speaker as a sequence of tokens (Lakhotia et al., 2021). Neural audio codecs (Zeghidour et al., 2022; Défossez et al., 2022), with their ability of reconstructing high-quality aud...\n",
      "\n",
      "--- METHOD ---\n",
      "for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on: i) an architecture adapted to the hierarchical structure of the audio tokens, ii) a parallel, non-autoregressive, confidence-based decoding scheme inspired by MaskGIT (Chang et al., 2022) for residual vectorquantized token sequences. --- --SoundStorm: Efficient Parallel Audio Generation SoundStorm relies on a bidirectional attention-based Conformer (Gulati et ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s 4.1. Model Training and Inference Setup In our experiments, we rely on a SoundStream codec that produces 50 frames per second and uses an RVQ with Q = 12 levels, with 1024 codebook size per level, resulting in a bitrate of 50-12-log. 1024 = 6000 bps. We use the semantic tokens of AudioLM as conditioning, which originate from w2v-BERT (Chung et al., 2021) embeddings quantized with k-means with 1024 cluster centers. These tokens have a rate of 25 tokens per second, so we duplicate them to match ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper we present SoundStorm, a model that can synthesize high-quality audio from discrete conditioning tokens efficiently. When compared to the acoustic generator of AudioLM, SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to SPEAR-TTS with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker tur...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.16824v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve eac...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Code large language models (code LLMs) are blooming recently [Zan et al. 2023}. A lot of code LLMs are released in succession, e.g., sal 3053 {Chen et a1] (Chen et al.|[2021]], AlphaCode [Li et al.|/2022J, non Coder [Chowdhery et al.| 2022], CodeGen [I SodeGeeX [Zheng et al.| tarCoder (Liet al.|/2023}, and Code Llama [Roziére| . Owing to ara amazing code generation performance, code ice have attracted considerable attention from or 4 ane and industrial circles. Recent works [Ouyang et al.| (Ouya...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Codex with 12-billion parameters is able to solve Python programming problems automatically. This remarkable success triggered a significant buzz in both the academic and industrial realms. Followed by Codex, a plenty of code LLMs are proposed, including AlphaCode CodeGen [Nijkamp et al. 2023), InCoder [Fried et al. 2023}, ‘odeGeeX et al.| 2021) |2023], PyCodeGPT [Zan et al.|Code Llama [Roziére et al.|/2023]], and phi-1 [Gunasekar et al.||2023]. These above models are trained on a largescale cod...\n",
      "\n",
      "--- METHOD ---\n",
      "ology 2.1 Crafting Training Corpus of Eight Programming Languages We select 8 popular programming languages and construct their training data separately. Our selected languages include Python, JavaScript, TypeScript, C, C++, Java, Go, and HTML, covering diverse types such as procedure-oriented, object-oriented, script, and even markup languages. For each programming language, we construct its training data containing about 9K data pairs. Each pair includes both an instruction describing the prog...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CODEM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CODEM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https: //github.com/NL2Code/CodeM K...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Our findings reveal that a monolingual training corpus can enhance the multilingual code generation capabilities of code LLMs via instruction tuning. This highlights the intrinsic commonality and interconnectedness among multiple programming languages. In our future work, we plan to delve into the reasons why multiple languages can enhance each other. Also, we will explore how to leverage our findings to elevate code generation capabilities for these obscure or less-used programming languages by...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06908v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Denoising diffusion probabilistic models (DDPMs) have shown promising performance for speech synthesis. However, a large number of iterative steps are required to achieve high sample quality, which restricts the inference speed. Maintaining sample quality while increasing sampling speed has become a challenging task. In this paper, we propose a Consistency Model-based Speech synthesis method, CoMoSpeech, which achieve speech synthesis through a single diffusion sampling step while achieving high...\n",
      "\n",
      "--- METHOD ---\n",
      ", CoMoSpeech, which achieve speech synthesis through a single diffusion sampling step while achieving high audio quality. The consistency constraint is applied to distill a consistency model from a well-designed diffusion-based teacher model, which ultimately yields superior performances in the distilled CoMoSpeech. Our...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that by generating audio recordings by a single sampling step, the CoMoSpeech achieves an inference speed more than 150 times faster than real-time on a single NVIDIA A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based speech synthesis truly practical. Meanwhile, objective and subjective evaluations on text-to-speech and singing voice synthesis show that the proposed teacher models yield the best audio quality, and the one-step sampling-based CoMoSpeech achieves ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and Future Work In this paper, we propose CoMoSpeech, a one-step acoustic model for speech synthesis based on the consistency model. With different conditional inputs, our CoMoSpeech can generate high-quality speech or singing voice by transforming the noise mel-spectrogram into the predicted mel-spectrogram in a single step.--- --However, there are still some limitations to our method. Since our CoMoSpeech needs to be distilled from a teacher model for better performance, this makes the pipel...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16704v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler a...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Transformers (Vaswani et al., 2017) form the backbone of modern large language models (LLMs) including the likes of GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023). These LLMs demonstrate remarkable capabilities, such as in-context learning and natural language-based algorithmic reasoning. However, we are only beginning to understand the origins, limitations, and generality of these capabilities, which is essential for developing safe and reliable LLMs. In-context learning (ICL) refers to a ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.10917v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete tr...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, there has been growing interest in Large Language Models (LLMs) due to their remarkable efficacy in enhancing performance in tasks like question answering and summarization, surpassing specialized models [1] [2]. LLMs are trained on vast quantities of text data, thereby encapsulating a wealth of world knowledge within the network. This accumulated knowledge and contextual understanding prove to be particularly beneficial in the field of Automatic Speech Recognition (ASR), especi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "There have been several works on speech recognition models contextualization including deep and shallow biasing [8] [4]. Le et al. introduced a weighted finite state transducer (WEST) composed from biasing strings which is attached dynamically during decoding and the scores of the RNN-T system and biasing WFST are interpolated. The advantage of such approaches is that they could be attached to any system after the training is completed. Another line of research is deep biasing...\n",
      "\n",
      "--- METHOD ---\n",
      "for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoderonly fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improve...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "AL SETUP Model: Figure [i] illustrates the overview of our proposed model. This speech LLM architecture consists of two main blocks: audio encoder and text decoder. The audio encoder firstly applies four downsampling blocks resuling in 16x time reduction of audio representations. After that a stack of Conformer [16] blocks with rotary positional embeddings are applied with hidden dimensionality of 512 and kernel size of 9. At the end we add an additional downsampling block. As a result the decod...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S AND FUTURE WORK In this work, we have presented to our knowledge the first results on utilizing pretrained LLMs to leverage contextual information in order to improve speech recognition. We have demonstrated that with a simple decoder-only architecture we can condition the ASR output on the unstructured text. Our approach shows superior performance against a strong baseline, proving the feasability of the proposed method at scale. End-to-end contextualization via text promping with LLMs shows ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08804v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "of compressed discrete music representations (i.e. tokens In language modeling based music generation, a generated waveform is represented by a sequence of hierarchical token stacks that can be decoded either in an auto-regressive manner or in parallel, depending on the codebook patterns. In particular, flattening the codebooks represents the highest quality decoding strategy, while being notoriously slow. To this end, we propose a novel stack-and-delay style of decoding strategy to improve upon...\n",
      "\n",
      "--- METHOD ---\n",
      "can be referred to as parallel decoding while the latter is usually auto-regressive. The level of quality is getting closer to that of original songs, paving the road towards new commercial use cases such as personalized on-device music generation, where the batch size is typically small. However those models often come with a quality trade off: the higher the quality, the slower the generation and vice versa [3|{6]. During inference, the decoding strategy, hardware and model size influence the ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "AL SETUP Most of the experimental setup follows that of MusicGen [4], we refer the readers to it for more details. 3.1. Model The tokenizer is an EnCodec model [II], made of CNN autoencoder and Residual Vector Quantization module applied to the latent representation of waveforms. The RVQ module is made of C = 4 quantizers, each with a codebook size of 2048. It encodes 32 kHz monophonic audio into a stack oftokens every 20ms (50 Hz framerate). The Transformer decoder is made of 300M parameters, i...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduce a new codebook pattern that relies on stacking the discrete music tokens, delaying/shifting the decoding of subsequent levels, and permuting the order of time steps to decode in the decoding schedule. The combination of the three outperforms the delay baseline quality-wise with a indomain FAD reduction of 45% for the same inference efficiency budget, due to parallel decoding that compensates for an increased sequence length. We also show that stacking the tokens should be preferred ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.04623v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in smallbatch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of spe...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies (Kingma & Ba, 2014), and vast amounts of data (Halevy et al., 2009; Gao et al., 2020; Kocetkov et al., 2022), have paved the way for applications in fields as varied as natural language processing (Brown et al., 2020), machine translation (Raffel et al., 2020), code synthesis (Chen et al., 2021), and beyond (OpenAI, 2023). However, this exciting progress comes with its own set of system-level challenges. As LLMs have become more powerful, their computational demands have increased in tan...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we use three models: a GPT-2-Large (762M) parameter oracle model (Radford et al., 2019) finetuned on the Python subsection of the Stack (Kocetkov et al., 2022), a small (40M) parameter GPT-2 draft model trained on the same, and a Katz backoff trigram model (Katz, 1987) as the draft? model. The Katz backoff model was generated by running the draft model for two hours at a sampling temperature of 1.5 to generate 120M tokens. All evaluations were conducted on a quiesced RTX 4090 GPU (NVIDIA, 202...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this work, we described and implemented several improvements over previous work in speculative decoding. First, we restructured the batch provided to the oracle model as a tree, in order to decrease the cost of generation and increase the expected number of tokens per batch. Second, we added a second stage of speculation to accelerate the decoding of the draft model. Altogether, we achieved an average speedup of 3.16x over standard single-batch inference. Acknowledgements [Left blank for bl...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.08621v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training paralleli...\n",
      "\n",
      "--- METHOD ---\n",
      "’s popularity. The second strand returns to recurrent models for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators [PAA* 23] are used for acceleration, however, representation capacity and performance are harmed. The third line of research explores replacing attention with other mechanisms, such as $4 [GGR21], and its variants [DFS*22, PMN*23]. None of the previous work can break through the impossible triangle, resulting in no clear winner compared ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https: //aka.ms/retnet. Inference Cost Scaling Curve 8.4X 40 300 300[= 20 3.4X 150 150 ia = | 15.6X 0 0 0 = 1 3)GPU Memory| Throughputt Latency| . (GB) (wps) (ms) Model Size (B) Transformer RetNet Mil Figure 1: Retentive netw...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we propose retentive networks (RetNet) for sequence modeling, which enables various representations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers. The above advantages make RetNet an ideal successor to Transformers for large language models, especially considering the deployment benefits...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.09864v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We introduce AvatarBooth, a novel method for generating highquality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and editing. Our key contribution is the precise avatar generation control by using dual fine-tuned diffusion models separately for the ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Creating 3D human avatars from texts or images is a longstanding challenging task in both computer vision and computer graphics, which is key to a broad range of downstream applications including the digital human, film industry, and virtual reality. Previous approaches have relied on expensive and complex acquisition equipment to reconstruct high-fidelity avatar models [Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022]. However, these methods require multi-view images or depth maps that...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S Text-guided 2D&3D Generation. In recent years, diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020] have rapidly developed due to their remarkable performance in synthesizing high-quality images. A core structure of diffusion models consists of forward diffusion steps that add noise according to a scheduler and backward generative steps that denoise the noise. In addition to unconditional generation from Gaussian noise only, the diffusion model can generate high-qualit...\n",
      "\n",
      "--- METHOD ---\n",
      "can generate 3D human avatars in prompt generative mode (red), appearance customized mode (blue), or hybrid mode (green). ABSTRACT We introduce AvatarBooth, a novel method for generating highquality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and e...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that AvatarBooth outperforms previous text-to-3D methods in terms of rendering and geometric quality from either text prompts or specific images. KEYWORDS Avatar creation, diffusion model, neural implicit field, model finetuning 1 INTRODUCTION Creating 3D human avatars from texts or images is a longstanding challenging task in both computer vision and computer graphics, which is key to a broad range of downstream applications including the digital human, film industry, and virtual reality...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose a method for generating avatar models based on text prompts or free-captured image sets, or both. The human avatar to be synthesized is represented by a neural implicit surface and large pre-trained vision-language models are leveraged for the training of the model via score distillation sampling loss. The pose-consistent constraint is introduced to improve the accuracy of the avatar’s geometry and appearance. Dual model fine-tuning and multi-resolution SDS further boos...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.03421v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Existing large language models have to run KC times to generate a sequence of XK tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analys...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) (Brown et al.| {2020} |OpenAI 2023} Touvron et al.| 2023} Chowd{hery et al. {2022} [2023} have revolutionized the field of natural language generation for their abilities in generating satisfactory text across various application domains. The excellent performance benefits greatly from the scaling of model size (100B+ parameters), but at the same time, the fact remains that a single decoding step gets slower as the model gets larger. In addition to the immense comput...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "The scale of auto-regressive language models grows from 117M (Radford et al.| 2018) parameters to over 500B parameters (Smith et al.|/2022) and various approaches are explored to improve the --- --Technical Report Model Humanities STEM Social Sciences Other Average OPT 1.3B 22.8 25.7 23.3 26.5 24.Pythia 1.4B 26.6 25.6 24.3 26.6 25.GPT-Neo 2.7B 25.3 25.6 27.5 27.4 26.LLaMA-ours 1.3B 27.8 26.1 23.5 23.7 25.RecycleGPT-std 1.3B 26.5 28.2 24.0 25.0 26.RecycleGPT-rec 1.5B 26.3 28.0 24.0 24.8 26.Table ...\n",
      "\n",
      "--- METHOD ---\n",
      "s have become popular fet al.| 2023} [Chen et al} /2023} [Miao et al-|{2023). To reduce the number of executions of the large model, they employ a two-step approach: first, an efficient small model speculatively generates the “Equal contribution, correspondence to {jiangyufan2018,qiaozhihe2022} @outlook.com --- --Technical Report simpler parts of the text; then, a large model is used to validate those parts, rather than having the large model generate the entire text alone. This idea is simple a...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance. 1 INTRODUCTION Large language models (LLMs) (Brown et al.| {2020} |OpenAI 2023} Touvron et al.| 2023} Chowd{hery et al. {2022} [2023} have revolutionized the field of natural language generation for their abilities in generating satisfactory text across various application domains. The excellent performance benefits greatly from the scaling o...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we propose RecycleGPT, a new architecture with low-inference latency. By predicting multiple tokens with the recyclable module at once, RecycleGPT can achieve up to 1.4x speedup with no performance loss. The proposed approach is model-agnostic and complementary to previous acceleration techniques. In the future, we will explore more decoding strategies by combining the recyclable module and the original model in various ways. --- --Technical Report REFERENCES Joshua Ainslie, James ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.13534v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three questionanswering datasets where ChatGPT and GPT-often state an incorrect answer and offer an explanation with at least one incorrect c...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Language models are increasingly being deployed to interface with humans in open-ended information-seeking and problem-solving settings. Despite their diverse capabilities and extreme fluency, a major open challenge is that LMs still hallucinate by making up facts or citing sources that do not exist (Maynez et al., 2020; Liu et al., 2023, i.a.), often while sounding extremely plausible. Hallucination is commonly attributed to knowledge gaps in LMs (Zheng et al., 2023), motivating mitigation stra...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Hallucinations Hallucination in text generation is a well-studied problem (Rohrbach et al., 2018; Maynez et al., 2020; Raunak et al., 2021, i.a.) that has recently become more prominent due to ChatGPT’s tendency to produce plausible-sounding falsehoods. Hallucinations are often attributed to knowledge gaps in LMs (Zheng et al., 2023), and several works have shown the promise of using retrieval over knowledge bases to mitigate them (Lewis et al., 2020; Shuster et al., 2021; Peng et al., 2023). Ou...\n",
      "\n",
      "--- METHOD ---\n",
      "s in which LMs can backtrack. 2 Why do we expect hallucination snowballing? In this section, we explain why we hypothesize that LMs are susceptible to hallucination snowballing. We predict that snowballing will occur on questions with two key properties: 1. Initial committal: The prompt leads the LM to first state an answer (before outputting the explanation). This applies to many yes/no questions. 2. Inherently sequential: Transformers cannot find the answer within one timestep because of their...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s We design three QA datasets with the properties described in §2 to probe hallucination snowballing, and evaluate ChatGPT and GPT-4. We first check whether the LM returns the correct answer to the given question, and we show that when the model returns the wrong answer, it frequently provides an incorrect explanation for that wrong answer. We automatically extract the incorrect claim in the explanation and ask the same LM to check whether its claim is correct. See Table | for a representative e...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We define the phenomenon of hallucination snowballing and demonstrate its prevalence in generations from state-of-the-art models, leading to hallucinations on simple facts that wouldn’t otherwise occur. Our findings point to the risk of training language models that prioritize fluency and coherence indiscriminatively at the expense of factuality, and we encourage future work to study remedial actions at all levels of model development. Limitations We focus on hallucination snowballing in the con...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.13404v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper presents a novel neural implicit radiance representation for free viewpoint relighting from a small set of unstructured photographs of an object lit by a moving point light source different from the view position. We express the shape as a signed distance function modeled by a multi layer perceptron. In contrast to prior relightable implicit neural representations, we do not disentangle the different light transport components, but model both the local and global light transport at ea...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The appearance of real-world objects is the result of complex light transport interactions between the lighting and the object’s intricate geometry and associated material properties. Digitally reproducing the appearance of real-world objects and scenes has been a longstanding goal in computer graphics and computer vision. Inverse rendering methods attempt to undo the complex light transport to determine a sparse set of model parameters that, together with the chosen models, replicates the appea...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We focus the discussion of related work on seminal and recent work in image-based relighting, inverse rendering, and relighting neural implicit representations. For an in-depth overview we refer to recent surveys in neural rendering [Tewari et al. 2022], (re)lighting [Einabadi et al. 2021], and appearance modeling [Dong 2019]. Image-based Relighting. The staggering advances in machine learning in the last decade have also had a profound effect on imagebased relighting [Debevec et al. 2000], enab...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies — Image-based rendering; Reflectance modeling. KEYWORDS Relighting, Free-viewpoint, Neural Implicit Modeling “Work done during internship at Microsoft Research Asia. ACM Reference Format: Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, and Xin Tong. 2023. Relighting Neural Radiance Fields with Shadow and Highlight Hints. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings (SIGGRAPH °23 Conference Proceedings), August 6-...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s (visual and quantitative) on the synthetic datasets to evaluate the impact of each of the components that comprise our neural implicit radiance representation. Shadow and Highlight Hints. A key contribution is the inclusion of shadow and highlight hints in the relightable radiance MLP. Figure 9 shows the impact of training without the shadow hint, the highlight hint, or both. Without shadow hints the method fails to correctly reproduce sharp shadow boundaries on the ground plane. This lack of ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper we presented a novel neural implicit radiance representation for free viewpoint relighting from a small set of unstructured photographs. Our representation consists of two MLPs: one for modeling the SDF (analogous to NeuS) and a second MLP for modeling the local and indirect radiance at each point. Key to our method is the inclusion of shadow and highlight hints to aid the relightable radiance MLP to model high frequency light transport effects. Our method is able to produce relit ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.13304v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text. In this paper, we introduce RECURRENTGPT, a languagebased simulacrum of the recurrence mechanism in RNNs. RECURRENTGPT is built upon a large language model (LLM) such as ChatGPT and uses natural language to simulate the Long Short-Term Memory mechanism in an LSTM. At each timestep, RECURRENTGPT generates a paragraph of text and updates its languagebased long-short term memory stored on the hard ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) such as ChatGPT have proven to be highly effective tools for assisting with various routine writing tasks, including emails and blog posts. Nevertheless, due to the fixed-size context design inherent in the Transformer [6] architecture, it is unfeasible to generate long texts (e.g., novels) solely by prompting LLMs. In contrast, recurrent neural networks (RNNs) [7] [8], in theory, possess the capacity to generate sequences of arbitrary length, thanks to their recurre...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s 4.1 Transformers Beyond Fixed-size Context One major limitation of Transformers is that the context size is fixed, which hinders their ability on processing and producing long texts. Previous work attempts to solve this issue from two different ways: designing efficient attention mechanisms to train and use Transformers with larger context windows [18}/21], and adding memory mechanisms to the computational graph in a Transformer to allow it to process information from multiple context windows ...\n",
      "\n",
      "--- METHOD ---\n",
      "s to human annotators with good English proficiency and instruct them to label whether novel A or novel B is better, or they are indistinguishable, in terms of interestingness and coherence. Following the human evaluation settings in Yang et al. [16], we sample 20 generated novels for each genre and assign 3 annotators for each novel. 3.2 Results As shown in Table[]] we find that RECURRENTGPT is favored by human readers for both interestingness and coherence with a relatively large margin compar...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we build RECURRENTGPT upon ChatGPT and find that exhibits the capability to autonomously generate remarkably extensive texts, spanning thousands of tokens, while maintaining both coherency and engagement. In stark contrast, vanilla ChatGPT is constrained to generating a few hundred of tokens before encountering issues such as repetitive content or a decline in coherence.Moreover, RECURRENTGPT can help human writers produce arbitrarily long text with ease, reducing much of the human efforts re...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We present RECURRENTGPT, a language-based simulacra of the recurrence mechanism in RNNs that uses language-based components and defines a recurrent computation graph via prompt engineering. RECURRENTGPT enbale LLMs to generate arbitrarily long texts either autonomously or by interacting with human writters. Its language-based components improves its interpretability and controllability and the prompt-based computation graph makes it easily customizable. User study on using RECURRENTGPT as AI w...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.16125v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate hu...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, Large Language Models (LLMs) ] have exhibite remarkable capabilities to understand, reason, and generate texts across a variety of open-ended tasks. Leveraging the strong generality of LLMs, generative Multimodal Large Language Models (MLLM: s) (6)[7)(8)|9) (10) (11) 112) {13} [1.4] {1.5} [16] [17] [18] [19] [20] [21] have demonstrate enhanced abilities for mu timodal comprehension and generation. However, current MLLMs mainly evaluate their performance with a limited number of ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we propose a large-scale benchmark SEED-Bench to provide a comprehensive and objective evaluation of Multimodal Large Language Models (MLLMs) on generative comprehension. SEED-Bench consists of 19K multiple-choice questions with accurate human annotations, which covers 12 evaluation dimensions for both the spatial and temporal understanding. We design an advanced pipeline to create multiple-choice questions that target specific evaluation dimensions, facilitating the scalability of...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03981v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ELECTRA (Clark et al., 2020), the generatordiscriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator resu...\n",
      "\n",
      "--- METHOD ---\n",
      "is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the chasm between the two encoders by creating a “correction notebook” for secondary-supervision. Moreover, a course soups trial is co...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that our method significantly improves ELECTRA’s average performance by 2.8% and 3.2% absolute points respectively on GLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRAstyle models under the same settings. The pre-trained MCL model is available at https://huggingface.co/McmanusChen/MCLbase. 1 Introduction Language models pre-training (Radford et al., 2018, 2019; Devlin et al., 2019; Liu et al., 2019) has *Contribution during internship at Microsoft. ¥ Correspo...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper proposes the multi-perspective course learning method, containing three self-supervision courses to improve learning efficiency and balance label distributions, as well as two self-correction courses to create a “correction notebook” for revision training. Besides, the course soups method is designed to figure out a novel approach for efficient pre-training. Experiments show that our method significantly improves ELECTRA’s performance and overshadows multiple advanced models under sam...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03043v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Personalized avatar creation—the ability to map one’s facial features to a 3D virtual replica that can be animated, customized, and rendered—is an emerging technology with great promise for cinema, the metaverse, and telepresence. Advances in this area may lead to digital twins with greater verisimilitude in detail and in animation --- --SIGGRAPH Conference Proceedings, Aug 6-10, 2023 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Kh...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Mesh-based 3D Morphable Models The seminal work by Blanz and Vetter proposed a linear 3D Morphable Model (3DMM) [Blanz and Vetter 1999] that models facial shape and textures on a template mesh using linear subspaces computed by principal component analysis (PCA) from 200 facial scans. This low-dimensional facial shape and texture space makes 3DMMs suitable for robustly capturing facial animation as well as reconstructing 3D faces in monocular settings. To reconstruct shape, texture, and ligh...\n",
      "\n",
      "--- METHOD ---\n",
      "reconstructs a high-quality editable 3D digital avatar (columns 2 and 3) by combining implicit geometry representations with explicit texture maps. The proposed approach naturally supports novel view synthesis from large pose shifts, an expressive and non-linear facial animation space (columns 4 through 6), direct user access to texture map editing (column 7), and 3D asset extraction for further downstream applications such as relighting (column 8). Original image courtesy of COD Newsroom/flickr...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.04827v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FEN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70% in some layers of the 66b model) are “dead”, ie. they never activate on a large collection of diverse data. At t...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The range of capabilities of language models expands with scale and at larger scales models become so strong and versatile that a single model can be integrated into various applications and decisionmaking processes (Brown et al., 2020; Kaplan et al., 2020; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2023; Anil et al., 2023). This increases interest and importance of understanding the internal *Work done as part of internship at Meta AI. workings of these large language models (LLMs) and, spe...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Historically, neurons have been a basic unit of analysis. Early works started from convolutional networks first for images (Krizhevsky et al., 2012) and later for convolutional text classifiers (Jacovi et al., 2018). Similar to our work, Jacovi et al. (2018) also find n-gram detectors; although, for small convolutional text classifiers this is an almost trivial observation compared to large Transformerbased language models as in our work. For recurrent networks, interpretable neurons include sim...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.03917v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The “‘decoder-only” architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent times, the large language models (LLMs) have showcased remarkable achievements across various natural language benchmarks, encompassing question answering, machine translation, language understanding and more (2) 3] 41 [5]. By employing a Transformer-based architecture [6] and training to anticipate forthcoming tokens within a sequence, this language model excels in contextual learning abilities. Not only does this significantly enhance its modeling prowess, but more importantly, it en...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Our model aims at integrating speech signals into large language models, as well as relates to Connectionist Temporal Classification (CTC) feature length compression and lowrank adaptation (LoRA). We discuss these topics in the following. 2.1. Large language models LLMs are generally pre-trained on vast amounts of textual data that span a wide variety of domains and languages. They usually consist of a stack of transformer layers, following an auto-regressive decoder-only architecture, where eac...\n",
      "\n",
      "--- METHOD ---\n",
      "leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speechLLaMA model from speech-text paired data alone. We conduct...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoderonly models for speech-to-text conversion. Index Terms— decoder-only, LLaMA, LoRA, speech translation 1. INTRODUCTION In recent times, the large language models (LLMs) have showcased remarkable achievements across various natural language benchmarks, encompassing question answering, machine translation, language understanding and more...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "& FUTURE WORK In this work, we propose a method to infuse an off-theshelf large language model with acoustic information. The proposed model presents a deep integration between the audio with the LLM by directly mapping acoustic representation into the semantic space of LLM. We also explore several practical aspects of the proposed model for better performance including compression of the acoustic feature, attention mask design and adapter fine-tuning. We show that on a 13 language to English sp...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.06261v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainabilit...\n",
      "\n",
      "--- METHOD ---\n",
      "tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques. 1 In...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al plugins designed to solve mathematical problems and perform data analysis through code generation (43). The recent breakthrough in program synthesis using LLMs has ignited a surge of research aimed at advancing the state of the art in this field. These techniques can generally be classified into three approaches: (1) code selection, which involves generating multiple samples with LLMs and choosing the best one based on the consistency of execution results or auto-generated test cases (9): (2)...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Recent advancement in LLMs has paved the way for new opportunities in network management. We introduce a system framework that leverages LLMs to create task-specific code for graph manipulation, tackling issues of explainability, scalability, and privacy. While our prototype and preliminary study indicate the potential of this method, many open questions remain in this nascent area of research. Code Quality for Complex Tasks. As our evaluation demonstrates, the LLM-generated code is highly accur...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01734v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Imaginative play is an area of creativity that could allow robots to engage with the world around them in a much more personified way. Imaginary play can be seen as taking real objects and locations and using them as imaginary objects and locations in virtual scenarios. We adopted the story generation capability of large language models (LLMs) to obtain the stories used for imaginary play with human-written prompts. Those generated stories will be simplified and mapped into action sequences that...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, the domain of agents has experienced extraordinary progress, driving the creation of intelligent machines that connect the realms of science fiction and reality. As researchers, engineers, and innovators collaborate, the evolution of agents keeps pushing the limits of technology. However, how do we ensure that agents have a persistent, yet non-intrusive presence in the household? Considering kids: they are never idle — they find ways to occupy their time through play and if that...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "(See Section 3). Most stories required several iterations of revision (Refer to Table 1) until they included the win state in the action sequence. Two limitations of the current model are limited prompting formats and difficulty in understanding interactive actions in the text game. The first relates to the drawback of the language model is that the generation is uncontrolled. Aside from an initial prompt, generative language models are guided by word cooccurrence, which can lead to repetition, ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s Imaginary play is a creative direction for developing agent learning abilities. With the help of story generation from LLMs (such as ChatGPT (OpenAI 2022)), we can tell the model to generate imaginary play stories that guide the agent’s interactions through prompts. Story generation allows the agent to develop interesting imaginative stories with the objects and topic given, allowing the agent to engage in imaginative play in the real world. We use text games to model what happens within a giv...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08773v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model’s pre...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, including augmented, virtual and mixed reality, video game development, and movie production. The advent of recent neural generative models have brought about a transformative shift in the landscape of digital content generation. Drawing inspiration from the remarkable progress in image generation [1] [2], the realm of audio generation...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "This study applies the language model approach presented in works such as [2 [6] [7], in which the compression model discretizes audio into tokens for training and then decodes these tokens to audio. The language model learns to generate audio tokens. However, our emphasis lies in augmenting the semantic correlation between provided text descriptions and the generated audio. This enhancement is built upon the foundation of the MusicGen [8] and AudioGen [6] for language model-driven audio generat...\n",
      "\n",
      "--- METHOD ---\n",
      "s lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation. Index Terms— Audio Generation, Music Generation, Representation regularization 1. INTRODUCTION Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, including augmented, virtual and mixed reality, video game development, and movie product...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation. Index Terms— Audio Generation, Music Generation, Representation regularization 1. INTRODUCTION Generating sound effects, music, and speech to meet specific requirements holds immense importance as a pivotal tool in content creation spanning various domains, i...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper has introduced representation regularization to improve controllability over audio generation by prioritizing alignment between audio and text representations during model training. The proposed method integrated the audio and text similarity regularization, particularly during the classifier-free guidance (CFG) phase, wherein the text condition is excluded from cross attention during language model training. The experimental results, conducted across various audio and music generatio...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06424v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) like GPT-4 have recently demonstrated impressive capabilities in natural language understanding and generation. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large Language Model Authenticity via a Single Inquiry ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recently, the development of Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) and LLaMA-2 (Touvron et al., 2023a) has brought significant advances in natural language processing and achieved superior performance in downstream tasks of language understanding (Chowdhery et al., 2022), question answering (Su et al., 2019), dialogue systems (Wang et al., 2022b; Qian & Yan, 2023) and multimodal reasoning (Wang et al., 2022a). However, with the proliferation of these models, concerns have eme...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 CAPTCHA CAPTCHA (Von Ahn et al., 2003) is a common technique used to block malicious applications like dictionary attacks, E-mail spamming, web crawlers, phishing attacks, etc. There are different types of CAPTCHAs. Text-Based CAPTCHAs require the users to recognize letters and digits in distortion form (Chew & Baird, 2003; Mori & Malik, 2003; Yan & El Ahmad, 2008), while Image-Based CAPTCHAs (Gossweiler et al., 2009) require users to choose images that have similar properties such as traffi...\n",
      "\n",
      "--- METHOD ---\n",
      "s for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large Language Model Authenticity via a Single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that FLAIR provides a viable alternative to traditional CAPTCHAs. Specifically, while humans and LLMs excel with high accuracy on tasks within their areas of strength, their performance significantly declines on tasks they are less adept at, often falling to very low levels (~0%). This sharp disparity in performance allows for the differentiation between human and LLM respondents with just a single question. The proposed approach shows promise in developing more robust and...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In conclusion, this paper proposes a new framework called FLAIR for detecting conversational bots in an online environment. The proposed approach targets a single question scenario that can effectively differentiate human users from bots by using questions that are easy for humans but difficult for bots, and vice versa. Our experiments demonstrate the effectiveness of this approach and show the strengths of different types of questions. This framework provides online service providers with a new...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.06125v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found applicati joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech ...\n",
      "\n",
      "--- METHOD ---\n",
      "s show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency lo could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a l...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section, we provide details of our model settings and data. 3.1. Model Settings We specify component’s parameterizations according to the list in Section 2.1: ¢ Eq: The audio encoder consists of a single conformer [19] layer with 8 attention heads and dimension 2048. The audio encoder consumes 128 dimensional log-mels spanning 32ms each and spaced apart by 10ms. We then stack each frame with the frame before it and the two frames after it to yield 512 dimensional representations. Final...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We’ve shown that a semi-supervised speech/text encoder learns a joint representation of the two modalities that can observed by choosing the best alignment. We’ve exploited that fact to enforce domain consistency with an extra loss term which optimizes the modality match for the best alignment. We show consistent improvements over an unregularized joint model across multiple setups without adding any parameters. --- --( [2] (3) [4] [5] 6. References T. B. Brown, B. Mann, N. Ryder, M. Subbiah, ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06404v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text embeddings are useful features for several NLP applications, such as sentence similarity, text clustering, and semantic search. In this paper, we present a Low-rank Adaptation with a Contrastive objective on top of 8-bit Siamese-BLOOM, a multilingual large language model optimized to produce semantically meaningful word embeddings. The innovation is threefold. First, we cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable adapter (LoRA) and 8-bit Adam optimizer for...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) are capable of generating human-like language and can be utilized for a wide range of applications, including question answering, summarization, and more. The performance of natural language tasks typically improves as the scale of the model increases [1]. Therefore, modern language models have hundreds of billions of parameters (2, 3, 4]. Any mention of LLMs is likely to spark discussion around decoder-only Transformer models, where the objective is to predict the n...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "is in Section 4. Finally we conclude the work and discuss the next step in Section 5. 2. Model LACoS-BLOOM (Fig. 1) is a text embedding model that generates semantically meaningful representations for multilingual texts. Several techniques have been applied to make LACoS-BLOOM more practical with fewer computational resources and to produce high-quality representations. This includes quantizing the large number of model weights using 8-bit block-wise quantization. The model is fine-tuned using a...\n",
      "\n",
      "--- METHOD ---\n",
      "we use differs from other 8-bit approaches in that we use 8-bit quantization only for storage and perform computations in float16 or float32. This allows the use of nonlinear quantization that is tailored to the distribution of each individual weight, which can reduce error without affecting inference performance. 2.2. Low-Rank adaptation The adapter approach utilizes small, trainable matrices with low rank to approximate weight updates for downstream tasks. An approach, LoRA, represents updates...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "results show the quality of learned embeddings from LACoS-BLOOM is proportional to the number of model parameters and the amount of unlabeled training data. With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1 billion parameters end-to-end on a single GPU machine with 32GB memory. Compared to previous solution Sentence-BERT, we achieve significant improvement on both English and multi-lingual STS tasks. Keywords Parameter efficient fine-tuning, large language model, mult...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and future work In this paper, we propose a parameter efficient fine-tuning method called LACoS-BLOOM for extracting multilingual text embeddings from a Large Language Model (LLM). We use 8-bit quantization to reduce the model footprint. We then improve the performance of LLM finetuning using LoRA, and further enhance semantic similarity using a Siamese network with MNR. Our solution can train 7.1 billion BLOOM end-to-end on a single GPU. On STS tasks, our method significantly outperforms the ba...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.03926v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "An audiobook can dramatically improve a work of literature’s accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of humanquality, open-license audiobooks from the Project Gutenberg ebook collection. Our ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Audiobooks have become a popular way to consume literature, news, and other publications. Audiobooks not only allow existing readers to be able to enjoy content on the go, but can help make content accessible to communities such as children, the visually impaired, and new language learners. Traditional methods of audiobook production, such as professional human narration or volunteer-driven projects like LibriVox, are timeconsuming, expensive, and can vary in recording quality. These factors mak...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "LibriVox is a well-known project that creates open-license audiobooks using human volunteers. Although it has made significant contributions to the a ibility of audiobooks, the quality of the produced audiobooks can be inconsistent due to the varying skills and recording environments of the volunteers. Furthermore, the scalability of the project is limited by the availability of volunteers and the time it takes to record and edit a single audiobook. Private platforms such as Audible create high-...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this work, we present a novel pipeline to automate the creation of high-quality audiobooks from heterogeneous e-books. Our system uses new advances in neural text-to-speech, emotion recognition, custom voice cloning, and distributed computing to create engaging and lifelike audiobooks. We apply this system to donate over five thousand audiobooks to the opensource community and aim to demonstrate this system by allowing conference attendees to create custom audiobooks. We elieve that this wo...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.14225v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "The use of language in recommendation scenarios is not a novel concept. Content-based recommenders have been utilizing text associated with items, such as item descriptions and reviews, for about three decades [29]. However, recent advances in conversational recommender systems have placed language at the forefront, as a natural and intuitive means for users to express their preferences and provide feedback on the recommendations they receive [15, 24]. Most recently, the concept of natural langu...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Item-Based Recommendation. Traditional recommender systems rely on item ratings. For a new user, these can be provided over time as the user interacts with the recommender, although this means initial performance is poor. Thus, preferences are often solicited with a questionnaire for new users [22, 39, 41]. There has also been work looking at other forms of item-based preferences such as relative preferences between items [10, 39], although approaches that rely on individual item ratings dominat...\n",
      "\n",
      "--- METHOD ---\n",
      "s. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations. CCS Concepts: « Information sys...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s. 3.3 Design Consequences Importantly, to ensure a maximally fair comparison of language-based and item-based approaches, consistency of the two types of preferences was key in our data collection approach. As such, we directly crowd-sourced both types of preferences from raters in sequence, with textual descriptions collected twice—before and after self-selected item ratings. This required control means the amount of data per rater must be small. It is also a realistic amount of preference inf...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.15354v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distan...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community[1}[2]. A speech utterance typically consists of a mixture of information such as content of speech, speaker characteristics, dialect, recording device, distance to the sound source, and other information such as environment and noise. Different speech-related tasks aim at recognizing the specific information of interest while minimizing the affects of uncorrelated information. For exam...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s There are abundant efforts trying to extract speaker embeddings that represent only speakers’ voice, removing impacts of uncorrelated information. These...\n",
      "\n",
      "--- METHOD ---\n",
      "s to untangle them. The multi-domain nature of 3DSpeaker also makes it a suitable resource to evaluate large universal speech models and...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "methods of out-of-domain learning and self-supervised learning. [https : //3dspeaker . github. io/ 1 Introduction Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community[1}[2]. A speech utterance typically consists of a mixture of information such as content of speech, speaker characteristics, dialect, recording device, distance to the sound source, and other information such as environment and noise. Different speech-related tasks aim at r...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduced 3D-Speaker, a large-scale speech corpus designed to facilitate the research of speech representation disentanglement. The controlled combinations of multi-dimensional audio data in this corpus yield a matrix of a diverse blend of speech representation entanglement, motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.04354v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "75Sparse Mixture-of-Experts models (MoEs) have recently FAgained popularity due to their ability to decouple model £size from inference efficiency by only activating a small sub-set of the model parameters for any given input token. As ge ] such, sparse MoEs have enabled unprecedented scalability, 3resulting in tremendous successes across domains such as < natural language processing and computer vision. In this 2 aa work, we instead explore the use of sparse MoEs to scale- 245down Vision Transf...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, which we found to work well). Such a super-class division is often readily provided with the dataset (e.g. for CIFAR-10/100 or MS-COCO). If a dataset does not come with a super-class division, we can easily obtain one as follows: 1) we first train a dense baseline model on the dataset; 2) we then compute the model’s confusion matrix over a held-out validation set; 3) we finally construct a confusion graph from the confusion matrix and apply a graph clustering algorithm to obtain the super-cla...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and future work We showed that sparse MoEs can improve the performance vs. efficiency trade-off compared to dense ViTs, in an attempt to make ViTs more amenable to resourceconstrained applications. In the future, we aim to apply our MoE design to models that are more mobile-friendly than ViTs, e.g., light-weight CNNs such as MobileNets [5,6, 15] or ViT-CNN hybrids [1, 11, 18]. We also aim to consider other vision tasks, e.g., object detection. Finally, we aim to get actual on-device latency me...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.17492v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite assive trial-and-error, multiple sampling is reduced to pairwise contrast, thus lacking contrasts from a m...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have demonstrated remarkable capabilities in meeting the diverse information needs of users (Brown et al. 2020; Chowdhery et al. 2022; Bubeck et al. 2023; Touvron et al. 2023; Li et al. 2023). Despite leveraging the extensive global knowledge and human behavior encoded within their trillion-token pretraining corpus, LLMs are unavoidably impacted by the existence of misleading, toxic, and detrimental content encompassed within it (Bai et al. 2022b; Ouyang et al. 2022)...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Reinforcement Learning from Human Feedback Fine-tuning LLMs to align with human preferences has emerged as a critical research problem. It can be formulated as given a context and corresponding suffixes ranked or scored by human annotators without more detailed labels, the agent is required to learn human preference and provide human-like results. Reinforcement Learning (RL) can be the most straightforward way to reach this goal, for the agent just scarce supervision signal from reward models as...\n",
      "\n",
      "--- METHOD ---\n",
      "s pay attention to pair-wise contrasts from semantic or scalar perspectives. (2) Even longer rankings are available, they tend to cut it into pairs, thus lacking distinction of candidates from a macro perspective. --- --In this work, we thoroughly investigate the effect of enlarging sampling from linguistic space on human alignment. Based on this scenario, we propose the Preference Ranking Optimization (PRO) as an efficient framework of direct policy optimization. Figure 1 shows how PRO stands o...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automaticbased, reward-based, GPT-4, and human evaluations. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in meeting the diverse information needs of users (Brown et al. 2020; Chowdhery et al. 2022; Bubeck et al. 2023; Touvron et al. 2023; Li et al. 2023). Despite leveraging the extensive global knowledge and human behavior encoded withi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we derive from pair-wise contrasts of reward models in RLHF that human alignment can be modeled as aligning the probability ranking of n responses generated by the LLM and the preference ranking of these responses by humans. Based on this derivation, we propose PRO algorithms. PRO inherits the advantages of RLHF, and further captures fine-grained distinction corresponding to human preference from multiple one-to-N contrasts. We conduct extensive experiments to verify the excellenc...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.00113v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical g...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The misuse of Large Language Models (LLMs) like ChatGPT [1], Claude [2], or the open-sourced LLaMA [3] may become a threat as their availability and capabilities expand [4]— [6]. LLMs might help generate fake news by reducing costs to spread disinformation at scale [7], [8], with a potential impact on public opinion and democratic outcomes [9]. They could help impersonate people, facilitate scams [10], or make student assessments impossible. Enforcing fair and responsible usage through regulatio...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "[18]). Tab. II reports the TPR for different strength of the watermark (see Sect. II-C), and the S-BERT [28] similarity score between the generated texts with and without watermarking to measure the semantic distortion induced by the watermark. Results in Tab. II reveals different behaviors. For instance, [17] has a finer control over the trade-off between watermark strength and quality. Its TPR values ranges from 0.0 to 0.9, while [16] is more consistent but fails to achieve TPR higher than 0.8...\n",
      "\n",
      "--- METHOD ---\n",
      "s, analyzing practical implications of watermarks on traditional Natural Language Processing (NLP) benchmarks. Indeed, current watermark evaluation mainly considers the deviation from the original LLM distribution, for example using perplexity. This is in contrast with the LLM litterature, where models are rather evaluated on their effective usefulness, e.g. free-form completion tasks such as question answering. Such evaluations are much more informative on the actual abilities of the model when...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s reveal that hypotheses of previous works do not hold and that their detection thresholds largely underestimate the false positives at low FPR. This work provides grounded statistical tests that theoretically guarantee false positive-rates and accurate p-values in real-world regimes. We validate them empirically and show that they provide a close-to-perfect control of the FPR, even at low values (< 107°). Second, we compare the watermarking methods, analyzing practical implications of watermark...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This research offers theoretical and empirical insights that were kept aside from the literature on watermarks for LLMs. Namely, existing methods resort to statistical tests which are biased, delivering incorrect false positive rates. This is fixed with grounded statistical tests and a revised scoring strategy. We additionally introduce evaluation setups, and detection schemes to consolidate watermarks for LLMs. Further work may investigate how to adapt watermarks for more complex sampling schem...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07677v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of t...\n",
      "\n",
      "--- METHOD ---\n",
      "by design can work with any first-pass ASR models (Hybrid / CTC / Transducer). The rescorer is agnostic to ASR architecture, training mechanism and internal features, leading to better generalization capability. To the best of our knowledge, this is the first work to integrate a pre-trained self-supervised learning (SSL) speech representation model (Baevski et al., 2019, 2020; Hsu et al., 2021; Chen et al., 2021) into the second-pass rescoring. One key challenge of incorporating acoustic informa...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "with different auxiliary alignment losses for audio-text alignment, to effectively learn shared representations across the two modalities, and adopt contrastive learning which significantly improves the model performance. Empirically, we show that MATE transfers well to new domains --- --in zero-shot and few-shot settings, outperforming text-only baselines. Lum 4 L=Luim+ oLerr Masked Language Model (BERT) =--P/ @c0c@ e@ Lexical Embedding Speech Encoder (WavLM) It is eighty uh [MASK] two degrees ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We propose a novel multi-modal rescorer, MATE, which achieves significant WER, CWER reduction on in-domain and OOD datasets. In zero-shot and few-shot settings, MATE performs well on unseen domains and adapts rapidly with limited data. The domain generalization capability of MATE makes it an effective choice as a second-pass rescorer for scaling ASR systems to new domains. 6 Limitations One limitation of our approach is that incorporating acoustic features from an SSL speech encoder, in our ca...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07906v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics of objects such as trees, flowers, candles, and clothes swaying in the wind. We model dense, long-term motion in the Fourier domain as spectral volumes, which we find are well-suited to prediction with diffusion models. Given a single image, our trained model uses a frequency-coordinated ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The natural world is always in motion, with even seemingly static scenes containing subtle oscillations as a result of wind, water currents, respiration, or other natural rhythms. Emulating this motion is crucial in visual content synthesis—human sensitivity to motion can cause imagery without motion (or with slightly unrealistic motion) to seem uncanny or unreal. While it is easy for humans to interpret or imagine motion in scenes, training a model to learn or produce realistic scene motion is ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Generative synthesis. Recent advances in generative models have enabled photorealistic synthesis of images conditioned on text prompts [16, 17, 24, 73-75]. These text-toimage models can be augmented to synthesize video sequences by extending the generated image tensors along a time dimension [7, 9, 43, 62, 84, 106, 106, 111]. While these...\n",
      "\n",
      "--- METHOD ---\n",
      "generates a spectral volume [23], a motion representation that models dense, long-term pixel trajectories in the Fourier domain. Our learned motion priors can be used to turn a single picture into a seamlessly looping video, or into an interactive simulation of dynamics that responds to user inputs like dragging and releasing points. On the right, we visualize output videos as space-time X-t slices (along the input scanline shown on the left). Abstract We present an approach to modeling an image...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we see that the average VAE reconstruction error improves from 0.024 to 0.018 when we switch from a single 2D U-Net to a frequency-coordinated denoising module, suggesting an improved upper bound on LDM prediction accuracy; in Sec. 7.3, we also show that this design choice improves video generation quality. 5. Image-based rendering We now describe how we take a spectral volume S predicted for a given input image Jo and render a future frame I, at time t. We first derive a motion texture in th...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Limitations. Since our approach only predicts lower frequencies of a spectral volume, it can fail to model nonoscillating motions or high-frequency vibrations—this may be resolved by using learned motion bases. Furthermore, the quality of generated videos relies on the quality of underlying motion trajectories, which may degrade in scenes with thin moving objects or objects with large displacements. Even if correct, motions that require generating large amounts of new unseen content may also cau...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.16372v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing musiclanguage datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the des...\n",
      "\n",
      "--- METHOD ---\n",
      ". 2.1 Large Language Model for Data Generation We first take multi-label tags from existing music tagging datasets. The list of tags are appended with a carefully written task instruction as an input (prompt) to a large language model. The model then generates and returns sentences that (may) describe the music in a way the task instruction conditions. Table 1 shows examples of generated captions according to multi-label tags and task instructions. For the language model, we choose GPT-3.5 Turbo...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results. 5.1 Encoder-Decoder Model We used a cross-modal encoder-decoder transformer architecture that has achieved outstanding results on various natural language processing tasks [33], lyrics interpretation [34], and speech recognition [35], as shown in Figure 3. Similar to Whisper [35], the encoder takes a logmel spectrogram with six convolution layers with a filter width of 3 and the GELU [36] activation function. With the exception of the first layer, each convolution layer has a stride ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We proposed a tag-to-pseudo caption generation approach with large language models to address the data scarcity issue in automatic music captioning. We conducted a systemic evaluation of the LLM-based augmentation, resulting in the creation of the LP-MusicCaps dataset, a largescale pseudo-music caption dataset. We also trained a music captioning model with LP-MusicCaps and showed improved generalization. Our proposed approach has the potential to significantly reduce the cost and time required f...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.12156v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at highresolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task w...\n",
      "\n",
      "--- METHOD ---\n",
      "for this fundamental task with comparable performance. By reformulating the task as segmentsgeneration and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM m...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results to demonstrate its effectiveness. The codes and demos will be released at https: //github.com/CASIA-IVA-Lab/Fast SAM. 1. Introduction Recently, the Segment Anything Model (SAM) [19] is proposed. It is regarded as a milestone vision foundation model. It can segment any object within the image guided by various possible user interaction prompts. SAM leverages a Transformer model trained on the extensive SA-1B dataset, which gives it the ability to deftly handle a wide range of scenes an...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we rethink the segment of anything task and the model architecture choosing, and propose an alternative solution with 50 times faster running speed than --- --SAM-ViT-H (32x32). SAM can solve multiple downstream tasks well. Figure 11. Some examples for the The experiments show that FastStill, there are several weaknesses that can be improved for FastSAM, like the scoring mechanism and the instance maskgenerating paradigm. These problems are left for future study. References i) 2] ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05706v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finger robot hand will allow better approximation to human behavior and enable the robot to operate on diverse articulated objects. To this end, we propose a new benchmark called DexArt, which involves Dexterous manip...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Most tools and objects humans interact with are articulated objects. To allow household robots to facilitate our daily life, we will need to enable them to manipulate diverse articulated objects with multi-finger hands as humans do. However, learning dexterous manipulation remains a challenging task given the high Degree-of-Freedom (DoF) joints of the robot hands. While recent work has shown encouraging progress in using Reinforcement Learning (RL) [1, 8,29,69] for dexterous manipulation, most r...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Dexterous Manipulation. Dexterous manipulation with multi-fingered robotic hands has been a long standing problem in robotics. Previous...\n",
      "\n",
      "--- METHOD ---\n",
      "s that learn category-level manipulation policy on seen objects. (c) We evaluate the policies’ generalizability on a collection of unseen objects, as well as their robustness to camera viewpoint change. Abstract To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finge...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "with extensive benchmark methods that learn category-level manipulation policy on seen objects. (c) We evaluate the policies’ generalizability on a collection of unseen objects, as well as their robustness to camera viewpoint change. Abstract To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other han...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a new benchmark for dexterous manipula tion with articulated objects, and study the generalizability of the RL policy. We experiment and benchmark with different methods to provide several insights: (i) RL with more diverse objects leads to better generalizability. We find that training with more objects leads to consistently better performance on unseen objects. (ii) Large encoders may not be necessary for RL training to perform dexterous manipulation tasks. We find that, in all envi...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.16876v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Human-centric video frame interpolation has great potential for enhancing entertainment experiences and finding commercial applications in the sports analysis industry, e.g., synthesizing slow-motion videos. Although there are multiple benchmark datasets available for video frame interpolation in the community, none of them is dedicated to human-centric scenarios. To bridge this gap, we introduce SportsSloMo, a benchmark featuring over 130K highresolution (>720p) slow-motion sports video clips, ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Video frame interpolation (VFI) is a technique that synthesizes intermediate frames from input images, enhancing the clarity of content that may be difficult to see otherwise. This technique finds wide-ranging applications, including slow-motion video generation [27], novel view synthesis [39], video compression [73], cartoon and rendered “Work mainly done when Jiaben Chen was an intern at Northeastern University. Huaizu Jiang Northeastern University h. jiang@northeastern.edu (a) (b) (©) (ad) | ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Benchmark datasets Existing publicly available datasets already provide a valuable resource for developing and evaluating video frame interpolation...\n",
      "\n",
      "--- METHOD ---\n",
      "s on our benchmark, and we observed a noticeable decrease in their accuracy compared to other datasets. This highlights the difficulty of our benchmark and suggests that it poses significant challenges even for the best-performing methods, as human bodies are highly deformable and occlusions are frequent in sports videos. To tackle these challenges, we propose human-aware loss terms, where we add auxiliary supervision for human segmentation in panoptic settings and keypoints detection. These los...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results validate the effectiveness of our proposed human-aware loss terms, leading to consistent performance improvement over existing models. The dataset and code can be found at: https://neuvi. github.io/SportsSlomo/. 1. Introduction Video frame interpolation (VFI) is a technique that synthesizes intermediate frames from input images, enhancing the clarity of content that may be difficult to see otherwise. This technique finds wide-ranging applications, including slow-motion video generatio...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we introduced a new benchmark, SportsSloMo, focusing on human-centric video frame interpolation. Our benchmark contains 130K video clips and more than 1M video frames obtained from high-resolution (> 720p) slow-motion sports videos crawled from YouTube with careful curation. Due to the complex, highly deformation human motion and frequent occlusion, this benchmark imposes significant challenges to existing VFI models. To enhance their accuracy, we introduce human-aware loss terms ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06456v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Physics-based motion imitation has captured the imagination of vision and graphics communities due to its po tential for creating realistic human motion, enabling plausible environmental interactions, and advancing virtual avatar technologies of the future. However, controlling high-degree-of-freedom (DOF) humanoids in simulation presents significant challenges, as they can fall, trip, or deviate from their reference motions, and struggle to recover. For example, controlling simulated humanoids ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Physics-based Motion Imitation. Governed by the laws of physics, simulated characters [32, 31, 33, 35, 34, 7, 45, 52, 28, 13, 2, 11, 46, 12] have the distinct advantage of creating natural human motion, human-to-human interaction [20, 48], and human-object interactions [28, 34]. Since most modern physics simulators are not differentiable, training these simulated agents requires RL, which is time-consuming & costly. As a result, most of the work focuses on small-scale use cases such as interac...\n",
      "\n",
      "--- METHOD ---\n",
      "s, as current control policies cannot handle noisy observations such as video or language. In order to apply physically simulated humanoids for avatars, the first major challenge is learning a motion imitator (controller) that can faithfully reproduce human-like motion with a high success rate. While reinforcement learning (RL)-based imitation policies have shown promising results, successfully imitating motion from a large dataset, such as AMASS (ten thousand clips, 40 hours of motion), with a ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s We evaluate and ablate our humanoid controller’s ability to imitate high-quality MoCap sequences and noisy motion sequences estimated from videos in Sec.4.1. In Sec.4.2, we test our controller’s ability to recovery from fail-state. As motion is best in --- --Table 1: Quantitative results on imitating MoCap motion sequences (* indicates removing sequences containing human-object interaction). AMASS-Train*, AMASS-Test*, and H36M-Motion* contains 11313, 140, and 140 high-quality MoCap sequences, ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future Work. We introduce Perpetual Humanoid Controller, a general purpose physics-based motion imitator that achieves high quality motion imitation while being able to recover from fail-states. Our controller is robust to noisy estimated motion from video and can be used to perpetually simulate a real-time avatar without requiring reset. Future directions include 1) improving imitation capability and learning to imitate 100% of the motion sequences of the training set; 2) incorporating terr...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.08850v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The text-driven image and video diffusion models have achieved unprecedented success in generating realistic and diverse content. Recently, the editing and variation of existing images and videos in diffusion-based generative models have garnered significant attention. However, previous works are limited to editing content with text or providing coarse personalization using a single visual clue, ren dering them unsuitable for indescribable content that requires fine-grained and detailed control....\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "“The protagonist sets the scene for the entire story.” — Ben Okri [24] Diffusion-based generative models have demonstrated remarkable success in generating photorealistic and diverse images [27, 28, 31] and videos [34, 38, 41] conditioned on text. However, the generation process, while more diverse, lacks controllability when relying solely on text descriptions. To generate the desired content, researchers explore two approaches to modify text-conditioned generation: incorporating external contr...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Visual Content Generation. Content generation has made remarkable advancements with powerful generative models [14, 37, 39]. Recently, equipped with vast and diverse image-text pairs from the internet, diffusion-based generative models [27, 28, 31] have outperformed GAN-based...\n",
      "\n",
      "--- METHOD ---\n",
      "to integrate source video, target visual and textual clues. Extensive results demonstrate the versatile --- --and remarkable editing capabilities of Make-A-Protagonist. 1. Introduction “The protagonist sets the scene for the entire story.” — Ben Okri [24] Diffusion-based generative models have demonstrated remarkable success in generating photorealistic and diverse images [27, 28, 31] and videos [34, 38, 41] conditioned on text. However, the generation process, while more diverse, lacks controll...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s 4.1. Implementation Details Our video generation model is based on text-to-image (T21) latent diffusion models with image embedding [28] (Stable UnCLIP). After initializing with the T2I LDM and inserting the temporal modules into the model, the video generation model is fine-tuned on 8 frames from a video at a resolution of 768 x 768. The model is trained for 200 steps with a learning rate of 3 x 10~°. During inference, we use DDIM sampler [35] with classifier-free guidance [9] for 20 steps. I...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper introduces Make-A-Protagonist, the first end-toend framework for generic video editing using textual and visual clues. To edit the protagonist and background with these clues, we design a visual-textual-based video generation model coupled with a mask-guided fusion method to integrate diverse information sources. The fusion approach effectively employs the masks, control signals and attention maps from the source video to provide precise spatial locations for editing both protagonist ...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.14289v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot transfer performance and high versatility for numerous vision applications (like image editing with fine-grained control). Many of such applications need to be run on resource-constraint edge devices, like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM paper l...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "ChatGPT |Zhang et al.|[2023a] has revolutionized the NLP field, marking a breakthrough in generative AI (AIGC, a.k.a Artificial intelligence generated content) . What has made this possible lies in GPT-series models|Brown et al. | (2020}, {Radford et al.|[2018]|2019], which are foundation models[Bommasani et al.](2021] trained on web-scale text datasets. Following the success of foundation models in NLP, multiple works , Qiao et al.|[2023a],|Zhang et al.|[2022al] have learned an image encoder to...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "SAM: generalization and versatility. Since its advent in early April of this year, numerous projects and papers have emerged to investigate SAM from different perspectives. Given that SAM claims to segment anything, a line of works has reported its performance in real-world situations, including medical images|Ma and Wang 2023}, Zhang et al. , camouflaged objects{Tang et al.][2023], and transparent objects [2023]. The findings consistently show that SAM works well in general setups, but not in t...\n",
      "\n",
      "--- METHOD ---\n",
      "for achieving this project goal. 3.2 Proposed Method Coupled distillation. A straightforward way to realize our project goal is to follow the official pipeline infKirillod] fet al.|[2023] to retrain a new SAM with a smaller image encoder. As stated in|Kirillov et al.|[2023], training a SAM wit ViT-H image encoder requires takes 68 hours on 256 A100 GPUs. Replacing the ViT-H with ViT-L or ViT-B reduces the required GPUs to 128, which is still a non-trivial burden for many researchers in the commu...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "with a Tiny ViT (with 5M parameters) [Wu et al.| based on our proposed decoupled distillation. Table 2: Comparison of coupled distillation and decoupled distillation fro SAM with ViT-B as the image encoder. Decoupled distillation performs better and requires less than 1% computation resources than coupled distillation. / SAM (coupled distillation) SAM (decoupled distillation) MIoU 0.72 0.Training GPUs 128Batch size 128Iterations 180k 55k Training Data 11M 11K 4 Experiments 4.1 Experimental Setup...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. We find that the naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance, especially under a setup of limited training sources. The coupled optimization of the image encoder and mask decoder is the reason, and thus we propose decoupled distillation, whhere the knowledge is distilled from the image encoder ViT-H in the original SAM to a lig...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.03183v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, w...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, significant progress has been made in advancing automatic speech recognition (ASR) performance. Specifically, self-supervised learning schemes such as wav2vec2.0 [2] and Hubert [3] have achieved great success, requiring minimal labeled training data. However, since the public model checkpoints are trained with clean speech data (e.g., Librispeech [4] or Libri-light [5]), their robustness in real-world environments is limited. To improve noise robustness, the Whisper [1] model us...\n",
      "\n",
      "--- RELATED WORK ---\n",
      ": To the best of our knowledge, we are the first to report that a robust ASR actually learns a noise-variant representation; most previous work focuses on noise-invariant representations [7, 8, 9, 10, 11]. For ASR and AT model unification, the closest works are [12, 13, 14, 15]. In [12], a unified keyword spotting and audio tagging model is proposed, however, keyword spotting only considers up to 35 words and is a much simpler task than the large-vocabulary continuous speech recognition task we ...\n",
      "\n",
      "--- METHOD ---\n",
      "s that lead to better audio tagging performance. (25, 1280) ; (25, 1280) H Linear Linear Linear Projection Projection Projection (25,512) 0 (25,512 (25,512) _¥ (25, 512) (25, 512) (28, 512) (25, 1280) H (Optional) 1. Last—MLP: The most basic method, we first apply a temporal mean pooling over the last layer representation of Whisper and then apply a linear layer to map it to the prediction. 2. WA-MLP: As shown in Figure 3, we find the last layer is Temporal Temporal cesses {Temporal not optimal ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s do not indicate that noi invariance does not help noise-robust ASR, nor that a noiserobust ASR’s representation should be noise-variant. In fact, we believe encouraging noise-invariant representations [7, 8, 9, 10, 11] is a practical solution in self-supervised learning or small data cases. Whisper training requires industry-level computational resources and is expensive. What we hope to convey is that a noise-robust ASR model does not have to learn a noiseinvariant representation, and that th...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s are: --- --Table 1: Audio tagging performance comparison on AS-20K, AS-2M (mAP), and ESC-50 (accuracy). !ASR backbone parameters and FLOPs are not included. *Speed-up = 1/FLOPs, compared with AST; FLOPs computed by £vcore [19]. *; labeled AS-2M data *e is also used. AS-2M experiment is expensive, we skip it when AS-20K and ESC5O experiments already shown clear differences. End-to-End fine-tuning results are shown in grey text as the comparison is not exactly fair. Model Training Setting Method...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10431v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tunin...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recent advancements in text-to-image generation [4, 8, 17,, particularly diffusion models [13, 27, 31, 36, 37], have opened new frontiers in content creation. Subject-driven text-to-image generation permits the personalization to new individuals given a few sample images [3, 9, 20, 25, 32], allowing the generation of images featuring specific subjects in novel current subject-driven text-to-image generation methods su: scenes, styles, and actions. However, fer from two key limitations: cost of p...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Subject-Driven Image Generation aims to render a particular subject unseen at the initial training stage. Given a limited number of example images of the subject, it seeks to synthesize novel renditions in diverse contexts. DreamBooth [32], textual-inversion [9], and custom-diffusion [20] use optimization-based...\n",
      "\n",
      "--- METHOD ---\n",
      "s are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s 5.1 Setup Dataset Construction. We built a subject-augmented image-text paired dataset based on the FFHQ-wild [18] dataset to train our models. First, we use the BLIP-2 model [21] blip2-opt-—6.7b-coco to generate captions for all images. Next, we employ the Mask2Former model [7] mask2 former—swin-large-coco-panoptic to generate panoptic segmentation masks for each image. We then leverage the spaCy [15] library to chunk all noun phrases in the image captions and expand numbered plural phrases (...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose FastComposer, a tuning-free method for personalized, multi-subject text-to-image generation. We achieve tuning-free subject-driven image generation by using a pre-trained vision encoder, making this process efficient and accessible across various platforms. FastComposer effectively tackles the identity blending issue in multi-subject generation by supervising crossattention maps with segmentation masks during training. We also propose a novel delayed subject conditioning technique to ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.06126v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models excel in many humanlanguage tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "of three epoch, AstroLLaMA achieves an average perplexity of 6.55. This represents a 32.5% reduction in perplexity compared to the base LLaMA-model, signifying a substantial improvement in the model’s predictive accuracy. 3 Results As illustrated in the previous section, AstroLLaMA outperforms its non-fine-tuned counterpart, LLaMA-2, in terms of context-awareness during token prediction within astronomy abstracts. To delve deeper into the advantages of fine-tuning, we assess AstroLLaMA’s general...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11364v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLMgenerated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel interactive visualization tool for making sens...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) are becoming ubiquitous for their ability to solve a wide range of linguistic tasks with prompting that does not require additional model training [1,6,22]. This ability also lets them generate smaller, more refined datasets for finetuning [13, 25,27], benchmarking [29], low-resource tasks or languages [4, 15], and counterfactual testing (e.g., examples that are identical except for having different religious or gender-based identities [12]). A critical challenge lie...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "3.1 Evaluating Datasets Generated by LLMs Evaluating LLM-generated datasets is not a straightforward task. In the best case, one can measure downstream performance of a model trained on that data [4,25]. When this is impossible (e.g., benchmarks or a new task), the dataset quality must be evaluated with defined metrics [13]. Automatic...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.01841v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-co...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Generative pre-trained transformers (Brown et al., 2020; Lewis et al., 2020; Radford et al., 2018) have emerged as powerful and generic tools, driving breakthroughs not only in language understanding but the field of AI in general. These models owe *Equal contribution Barlas Oguz* Meta AI barlaso@meta.com Aasish Pappu Meta AI aasish@fb.com Raghuraman Krishnamoorthi Reality Labs, Meta Inc. raghuraman@meta.com their success mainly to their seemingly infinite ability to scale to ever-larger data an...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Quantization has long been studied to make neural networks more efficient (see (Hubara et al., 2017) for a survey). Due to the popularity of BERT, numerous works have studied quantization for transformer models, starting with 8-bit quantization (Zafrir et al., 2019; Fan et al., 2020), and progressing to 4-bit (Shen et al., 2020; Zadeh et al., 2020), ternary (Zhang et al., 2020) and binary Bai et al. (2021b); Qin et al. (2021); Liu et al. (2022). All of these works have focused on the encoderonly...\n",
      "\n",
      "--- METHOD ---\n",
      "to natural language generation tasks and, for the first time, demonstrate low-bit generative transformers of competitive accuracy. Our ternary (weight and activation) model lags a full-precision BART (Lewis et al., 2020) model by only 4 points in ROUGE on the XSUM summarization dataset. In contrast, our model with ternary weights and 8-bit activations comes within 1 point and even outperforms comparable state-of-the-art models with 8-bit weights. We also demonstrate a fully binary (weights and a...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section, we evaluate the effectiveness of our low-bit quantization scheme for natural language generative model on text summarization benchmarks: CNN/DailyMail (Nallapati et al., 2016) and XSUM (Narayan et al., 2018). We additionally experiment on the machine translation task with mBART on WMT16 English-Romanian (En-Ro) dataset (Bojar et al., 2016a). 3.1 Experimental settings We follow recent work (Li et al., 2022) in training the quantized network with initialization and knowledge dis...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have demonstrated high accuracy ternary and binary natural language generation models based on a pre-trained transformer encoder-decoder backbone. Quantizing both the weights and the activations of the network allow these models to run on special-purpose hardware using binary and ternary arithmetic, which doesn’t require multiplication modules. Therefore our results promise multiple orders of magnitude gains in efficiency while running these models, and can drastically expand the use cases of...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.06802v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The long-standing problem of novel view synthesis has many applications, notably in sports broadcasting. Photorealistic novel view synthesis of soccer actions, in particular, is of enormous interest to the broadcast industry. Yet only a few industrial solutions have been proposed, and even fewer that achieve near-broadcast quality of the synthetic replays. Except for their setup of multiple static cameras around the playfield, the best proprietary systems disclose close to no information about t...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies — Computer vision representations. KEYWORDS 3D reconstruction, scene representation, dynamic, neural radiance fields, sports, soccer ACM Reference Format: Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe. 2023. Dynamic NeRFs for Soccer Scenes. In Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports (MMSports °23), October 29, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3606038.1 INTROD...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.00986v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, b...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (OpenAI, 2022, 2023; Touvron et al., 2023; Chowdhery et al., 2022) have gradually become common AI assistants that demonstrate great potential in comprehending human intentions, performing complex reasoning tasks, and enabling content creation. De *Corresponding author: <ym119608 @alibaba-inc.com> ‘https://github.com/modelscope/modelscope-agent spite the rapid advancements of open-source LLMs, e.g., LLaMA (Touvron et al., 2023) and ChatGLM (THUDM, 2023), they still remain l...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "E.1 Large Language Models Recent years have witnessed rapid development in the field of Large Language Models (LLMs). Typical models, such as GPT3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022) and LLaMA (Touvron et al., 2023), have shown impressive zero and few-shot generalization abilities on a wide range of NLP tasks, by scaling up the model and data size. A remarkable milestone is the release of ChatGPT (OpenAL, 2022) or GPT...\n",
      "\n",
      "--- METHOD ---\n",
      "s either directly rely on closed-source counterparts like ChatGPT or focus on certain types of API tools. Recently, there have also been public releases of AI agents, such as Auto-GPT?, LangChain* and Transformers Agent (Huggingface, 2023), which enable LLMs, such as ChatGPT or GPT-4, to use tools and solve complex AI tasks. However, these agents are mainly built with closed-source LLMs and how to build a customizable agent system with open-source LLMs remains largely unexplored. In this work, w...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "ModelScope-Agent aims to facilitate building AI Agent applications and research based on opensource LLMs by providing a general and customizable agent framework covering flexible system design, data collection, model training, evaluation and usage example in real-world application. It provides an open-source, community-driven library towards AI Agent learning and best practices for building an agent system with open-source LLMs. We hope ModelScope-Agent can help pave the way towards a new era of...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.07891v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to “learn to learn\" from limited tasks and g...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "(In the near future, mankind finally be able to travel interstellar and come to the centaur constellation.) Human and MLLM walk off the spaceship. Human: “We made it! Look! The locals are here.” Locals: Greetings, you can call us ’RockFlock’. MLLM: “Hi, sheep!” Human: The above conversation between humans and MLLMs serves as a humorous representation of how MLLMs struggle to learn from demonstration during the conversation for real. ’RockFlock’ is our hand-made species, which possesses both --- ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Multimodal Large Language Models [7-11] have demonstrated significant capabilities in universal generation or recognition tasks. Following the new paradigm of MLLMs, various visual tasks can be achieved in a training-free zeroshot manner [12, 13], escaping from the heavy pretrainand-finetune process. However, recognize arbitrary content through a single model is generally considered extremely difficult. How to enhancing recognition capability of MLLMs in the wild at a low cost has emerged as a...\n",
      "\n",
      "--- METHOD ---\n",
      "for MLLMs to learn from demonstrations is known as in-context learning, wherein the models show remarkable improvement on downstream tasks after being exposed to a few input-label pairs. However, current MLLMs have very limited benefits from in-context learning, since the emphasis is primarily on guiding the model to acquire the ability to process novel tasks after “learning” from meta tasks. However, the model’s performance is not affected even if the answers provided in the meta-tasks are all ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning. 1 Introduction (In the near future, mankind finally be able to travel interstellar and come to the centaur constellation.) Human and MLLM walk off the spaceship. Human: “We made it! Look! The locals are here.” Locals: Greetings, you can call us ’RockFlock’. MLLM: “Hi, sheep!” Human: The above co...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In conclusion, this paper introduces a groundbreaking paradigm of causal-relevant few-shot learning, significantly expanding the capabilities of Multimodal Large Language Models (MLLMs) within the context of single conversations. Through meticulous experimentation and a carefully devised training strategy, we demonstrate that MLLMs can adeptly establish a mapping between ground-truth input-label pairs, thereby acquiring the proficiency to seamlessly generalize this capacity to previously unencou...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08172v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) have been successfully adapted for interactive decisionmaking tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-opti...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) such as GPT-(OpenAI, 2023) have achieved remarkable performance on a wide range of natural language understanding (NLU) tasks (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). Recently, they have been adapted to interactive decision-making tasks such as virtual home navigation (Yang et al., 2023), text-based games (Lin et al., 2023) or webnavigation (Yao et al., 2023; Zhou et al., 2024). Previous methods that utilize LLMs to solve interactive tasks often i...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s in Appendix A. 5 Conclusions We proposed an LLM agent, LASER, that models interactive web navigation tasks as state-space exploration. Our formulation allows the agent to handle novel situations, easily backtrack from mistakes, and always perform valid actions. Guided solely by the state-specific instructions without any in-context examples, LASER outperforms all baselines on the WebShop task by large margins and closes the gap with human performance on the realworld shopping website. Our anal...\n",
      "\n",
      "--- METHOD ---\n",
      "s implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of s...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task. 1 Introduction Large language models (LLMs) such as GPT-(OpenAI, 2023) have achieved remarkable performance on a wide range of natural language understanding (NLU) tasks (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). Recently, they have been adapted to interactive decision-making tasks such as virtual home navigation (Yang et al., 2023), text-b...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We proposed an LLM agent, LASER, that models interactive web navigation tasks as state-space exploration. Our formulation allows the agent to handle novel situations, easily backtrack from mistakes, and always perform valid actions. Guided solely by the state-specific instructions without any in-context examples, LASER outperforms all baselines on the WebShop task by large margins and closes the gap with human performance on the realworld shopping website. Our analysis shows that LASER is also...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.00615v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present PointLLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In these years, 3D vision has gained significant attention and development, driven by the rising popularity of autonomous driving [11, 69,71], navigation [72, 76,98], 3D scene understanding [2, 43, 46, 74], and robotics [31, 67]. To extend its application scenarios, numerous efforts [ 1,23, 92,95] have been made to incorporate 3D point clouds with data from other modalities, allowing for improved 3D understanding [1, 23], text-to-3D generation [35, 49,52], and 3D question answering [3,28]. For 3...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multi-modality Learning. Compared to single-modal approaches, multi-modal learning aims to learn from multiple modalities simultaneously, achieving more robust and diverse representation learning. Numerous studies have proved its efficacy, involving 2D images, videos, texts, and audio [15, 17,48], and enhance the cross-modal performance for downstream tasks [5, 25, 37,61], and videotext-audio integration for text generation [36]. The representative vision-language pre-training, CLIP [59], effect...\n",
      "\n",
      "--- METHOD ---\n",
      "s [35, 49,52] has achieved text-to-3D synthesis with high quality and efficiency. Despite this, they lack the ability to generate 3D shapes conditioned on multi-modal input, i.e., any-to-3D generation. Another series of works connects descriptive natural language with 3D data, which is applied to 3D captioning [12,87], question answering [3,78], and visual grounding [24,79]. Yet, they fail to utilize the pre --- --Point-LLM for 3D Q&A Any-to-3D Generation > No Need for 3D Instruction Data + 3p e...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section, we first present the implementation details of the multi-modal training for Point-Bind. Then, we illustrate the emergent multi-modal applications, i.e., Point-LLM for 3D instruction following, 3D cross-modal retrieval, 3D embedding-space arithmetic, any-to-3D gener --- --+ afflie Sea Wave + afflie Keyboard Typing + lien Water Figure 6. Embedding-space Arithmetic of 3D and Audio. We demonstrate Point-Bind’s capability for multi-modal semantic composition by retrieving 2D images...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose Point-Bind, a 3D multimodality model that aligns 3D point clouds with multimodalities, guided by ImageBind. By aligning 3D objects with their corresponding image-audio-text pairs, Point-Bind obtains a joint embedding space, and exhibits promising 3D multi-modal tasks, such as any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. Upon that, we further introduce Point-LLM, the first 3D large language model (LLM) with instruction-following capabil...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08210v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "As we embark on a new era of LLMs, it becomes increasingly crucial to understand their capabilities, limitations, and differences. Toward making further progress in this direction, we strive to build a deeper understanding of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective open-source LLMs and their distilled counterparts. To this end, we specifically focus on long-form question answering (LFQA) because it has several practical and impactful applications (e.g., troublesho...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "While Large Language Models (LLMs) like ChatGPT, GPT-4 (OpenAI, 2023) have exhibited superior performance across various benchmarks, opensource efforts have also been progressing rapidly in catching up across different applications and benchmarks like MMLU (Hendrycks et al., 2021), OpenLLMBoard (Anil et al., 2023; Beeching et al., 2023; Touvron et al., 2023). As we move into the new era of LLMs with fast-paced progress on new models and techniques, it becomes increasingly important to understand...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Reasoning over Long Documents: LLMs have shown amazing capabilities to reason over a number of tasks like commonsense reasoning (Talmor et al., 2019), mathematical and symbolic reasoning (Huang and Chang, 2023; Cobbe et al., 2021), question answering tasks like SQuaD, HotpotQA. However, most of these tasks do not require long context and answers are often a short phrase or a span of text from the context. In this work, we evaluate LLMs to reason over long documents that would require deeper unde...\n",
      "\n",
      "--- METHOD ---\n",
      "from abstractive summaries and show that generating ollow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup or LLMs and shows performance gaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2) open-source LLMs exhibit decreased reliance on context for generated questions from the original document, but their generation capabilities drop significantly on generated questions from summaries — especially for longer contexts (>1024 tokens). 1 Introduction While Large...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "With the emergence of LLMs like ChatGPT and open-source successful LLMs, it is extremely important to understand the capabilities and limitations of different LLMs. In order to test deeper reasoning abilities of LLMs by referring to longer contexts, we evaluate answers generated by LLMs on questions generated by ChatGPT on summaries of long documents. Results show that our proposed method of question generation poses a challenging setup for LLMs and shed light on performance gaps between massive...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.11526v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Neural Radiance Fields (NeRF) have the potential to be a major representation of media. Since training a NeRF has never been an easy task, the protection of its model copyright should be a priority. In this paper, by analyzing the pros and cons of possible copyright protection solutions, we propose to protect the copyright of NeRF models by replacing the original color representation in NeRF with a watermarked color representation. Then, a distortionresistant rendering scheme is designed to guar...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Though Neural Radiance Fields (NeRF) have the potential to be the mainstream for the representation of digital media, training a NeRF model has never been an easy task. If a NeRF model is stolen by malicious users, how can we identify its intellectual property? As with any digital asset (e.g., 3D model, video, or image), copyright can be secured by embedding copyright messages into asset, aka digital watermarking, and NeRF models are no exception. An intuitive solution is to directly watermark r...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Neural radiance fields. Various neural implicit scene representation schemes have been introduced recently [42] 48]. The Scene Representation Networks (SNR) [32] represent scenes as a multilayer perceptron (MLP) that maps world coordinates to local features, which can be trained from 2D images and their camera poses. DeepSDF (27) and DIST use trained networks to represent a continuous signed distance function of a class of shapes. PIFu learned two pixel-aligned implicit functions to infer surfac...\n",
      "\n",
      "--- METHOD ---\n",
      "can directly protect the copyright of NeRF models while maintaining high rendering quality and bit accuracy when compared among optional solutions. Project page: https://luo-ziyuan. github.io/copyrnerf 1. Introduction Though Neural Radiance Fields (NeRF) have the potential to be the mainstream for the representation of digital media, training a NeRF model has never been an easy task. If a NeRF model is stolen by malicious users, how can we identify its intellectual property? As with any digital ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s are also conducted to verify this setting. We keep the geometry representation in Equation (1) unchanged, and construct the watermarked color representation ©,,, to produce the message embedded color c,,, as follows: Cm = Om (€, x(x), Ya(d), M) , (5) where M denotes the message to be embedded and 0, contains several MLPs to ensure reliable message embedding. --- --The input c is obtained by querying 0, using Equation 2). Several previous methods have pointed out the importance of building a 3D...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this paper, we propose a framework to create a copyright-embedded 3D implicit representation by embedding messages into model weights. In order to guarantee the invisibility of embedded information, we keep the geometry unchanged and construct a watermarked color representation to produce the message embedded color. The embedded message can be extracted by a CNN-based extractor from rendered images from any viewpoints, while keeping high reconstruction quality. Additionally, we introduce a ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09664v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Humans can easily understand a single image as depicting multiple potential objects permitting interaction. We use this skill to plan our interactions with the world and accelerate understanding new objects without engaging in interaction. In this paper, we would like to endow machines with the similar ability, so that intelligent agents can better explore the 3D scene or manipulate objects. Our approach is a transformer-based model that predicts the 3D location, physical properties and affordan...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "What can you do in Figure 1? This single RGB image conveys a rich, interactive 3D world where you can interact with many objects. For instance, if you grab the chair with two hands, you can move it as a rigid object; the pillow can be picked up freely and squished; and door can be moved, but only rotated. This ability to recognize and interpret potential affordances in scenes helps humans plan our interactions and more quickly learn to interact with objects. The goal of this work is to give the ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Our paper proposes to extract 3D object interaction from a single image. This problem lies at the intersection of 3D vision, object detection, human-object interaction and scene understanding. It is also closely related to downstream robotics applications. Interactive scene understanding. Recently, the computer vision community is increasingly interested in understanding 3D dynamics of objects. It is motivated by human-object interaction [5, 19, 58], although humans do not need to be present i...\n",
      "\n",
      "--- METHOD ---\n",
      "aims to answer “what can I do here?” in the style of classic pointand-click games like Myst. We frame “what can I do here” via a set of common questions: whether the object can be moved, its extent when moved and location in 3D, rigidity, whether there are constrains on its motion, as well as estimates of how one would interact the object. To maximize the potential for downstream transfer, our questions are chosen to be generic rather than specific to particular hands or end-effectors: knowing w...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s in Section 6 test how well our approach recognizes potential interaction, testing on both unseen data in 3DOI as well as robotics data. We compare with a number of alternatives, including generalizing from data of demonstrations [53, 50] and synthetic data [70], as well alternate network designs. Our approach outperforms these models and shows strong generalization to the robotics dataset WHIRL [1]. To summarize, we see our primary contributions as: (1) the novel task of detecting 3D object in...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have presented a novel task of predicting 3D object interactions from a single RGB image. To solve the task, we collected the 3D Object Interaction dataset, and proposed a transformer-based model which predicts the potential interactions of any objects according to query points. Our experiments show that our approach outperforms existing approaches on our data and generalizes well to robotics data. Our approach can have positive impacts by helping build smart robots that are able to understan...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.04729v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task’s significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Music, as an artistic expression comprising harmony, melody and rhythm, holds great cultural significance and appeal to humans. Recent years have witnessed remarkable progress in music generation with the rise of deep generative models 2023} [2022} (2023). However, generating high-fidelity and realistic music still poses unique challenges compared to other modalities. Firstly, music utilizes the full frequency spectrum, requiring high sampling rates like 44.1 KHz stereo to capture the intricacie...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "This section provides an overview of the existing literature in the field of music generation, focusing on three main areas: Single-task vs. Multi-task Training, Waveform vs. Spectrum-Based...\n",
      "\n",
      "--- METHOD ---\n",
      "s in text-music alignment and music quality while maintaining computational efficiency. Our de mos are available athttps://www. futureverse.com/research/jen/ demos/jenl “Music is the universal language of mankind.” — Henry Wadsworth Longfellow 1 INTRODUCTION Music, as an artistic expression comprising harmony, melody and rhythm, holds great cultural significance and appeal to humans. Recent years have witnessed remarkable progress in music generation with the rise of deep generative models 2023}...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. Normalizing Latent Embedding Space. To avoid arbitrarily scaled latent spaces, 2 found it is crucial to achieve better performance by estimating the component-wise variance and re-scale the latent z to have a unit standard deviation. In contrast to previous approaches that only estimate the component-wise variance, JEN-1 employs a straightforward yet effective postprocessing technique to address the challenge of anisotropy in latent embeddings as shown in Algorithm [I] Specially, we channel-w...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we have proposed JEN-1, a powerful and efficient text-to-music generation framework that outperforms existing methods in both efficiency and quality of generated samples. Through directly modeling waveforms instead of mel-spectrograms, combining auto-regressive and non-autoregressive training, and multi-task training objectives, JEN-1 is able to generate high-quality music at 48kHz sampling rate. The integration of diffusion models and masked autoencoders further enhances JEN-1’s a...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.15658v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The recent work CLIPA [12] presents an inverse scaling law for CLIP training — whereby the larger the image/text encoders used, the shorter the sequence length of image/text tokens that can be applied in training. This finding enables us to train high-performance CLIP models with significantly reduced computations. Building upon this work, we hereby present CLIPA-v2 with two key contributions. Technically, we find this inverse scaling law is also applicable in the finetuning stage, enabling furt...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.07968v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personal ized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach L In writing instruction, the task decomposed into multiple steps summar...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "As artificial intelligence (AI) based systems are increasingly used to assist content creation, there has been a tremendous amount of interest in personalized text generation. Producing a customized response that takes into account auxiliary context, such as documents previously written by the user, is crucial for the development of generative systems that support specific audiences, creation contexts, and information needs. Example applications include Al-assisted writing of various types of co...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We present a literature review on personalized text generation and two related tasks, controlled text generation and text style transfer. Personalized text generation. Some studies focus on improving personalized generation for a particular domain by utilizing domainspecific features or knowledge. Li and Tuzhilin [14] design a model based on self-attentive recursive autoencoders to generate personalized user reviews given product description, sentiment labels, and historical reviews of the user....\n",
      "\n",
      "--- METHOD ---\n",
      "is proposed [40] based on generative adversarial networks (GANs). Frequently used function words and content words are used as input and as sentence structure constraints for model training. A less explored area is how to utilize large language models for personalized generation across different domains without relying on domain-specific or user-defined features. LaMP [29] is the work Liet al. closest to ours. It provides a benchmark for training and evaluating personalized language models on th...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al analysis. Controlled text generation. Controlled text generation aims to generate text with a predefined list of attributes, which could be stylistic or semantic. To reduce the cost of finetuning, recent worl of controlled text generation resorts to decoding-time methods, directly making pre-trained models generate texts towards desired attributes during inference. These methods include PPLM [5], GeDi [11], FUDGE [39], and DEXPERTS [16]. Controlled text generation is different from personaliz...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a general approach for teaching large language models for personalized text generation. Analogous to how students are instructed to write from sources in a sequence of steps, the proposed approach consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. Additionally, inspired by the observation that reading and writing skills are correlated, we create a multitask setting that improves the model’s reading ability y distinguishing the authorship of give...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.02254v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models’ non-English language capabilities. Addressing this gap, we seek...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we have successfully demonstrated that Polyglot-Ko attains competitive results across various benchmark datasets. In addition to presenting our achievements, we also acknowledge potential limitations and identify areas that warrant future improvement. By offering recommendations for further research, we aspire to foster advancements in the field. We firmly believe that Polyglot-Ko will serve as a valuable resource for the Korean natural language processing community, enabling the development ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Currently, we are actively working on training the new version of the Polyglot Korean language model. Our aim is to expand its capacity to eventually reach 40B parameters. This process has involved significant trial and error as we strive to enhance the performance and capabilities of the model. Based on our experience and expertise in developing Korean language models, we have also embarked on the creation of two types of multilingual models. The first type is an East-Asian model, which include...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07804v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost. On the other hand, Small Language Models (SLMs) are known for their efficiency, but they often struggle with limited capacity and training data, especially in specific domains. In this paper, we introduce a novel method aimed at improving SLMs in the medical domain using LLM-based generative data augmentation. ...\n",
      "\n",
      "--- METHOD ---\n",
      "aimed at improving SLMs in the medical domain using LLM-based generative data augmentation. The objective of our approach is to develop more efficient and capable models that are specifically tailored for specialized applications. Through...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s conducted on the PubMedQA dataset, we demonstrate the effectiveness of LLMs in refining and diversifying existing question-answer pairs. This refinement process leads to improved performance in a significantly smaller model after fine-tuning. Notably, our best SLM, with under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA dataset. Our code and generated data are publicly available to facilitate further explorations [1]. Keywords large language models, small language mod...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Our research highlights the effectiveness of LLM-based generative data augmentation in enhancing domainspecific question answering datasets. However, instructing LLMs without domain knowledge, such as GPT-3.5turbo, to generate new question-answer pairs resulted in decreased performance for fine-tuned smaller models. Conversely, leveraging LLMs with domain-specific knowledge, like GPT-4, significantly improved the performance of fine-tuned models by generating valuable new training data. These fi...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09662v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively smallscale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "As the world shifts towards virtual spaces, and as embodied agents become more capable, it will be increasingly important to be able to generate plausible human motion. Speech or text are perhaps the most natural ways to prompt generative models, which is one reason behind the explosion in text-to-x generation research. Human motion generation from text has diverse applications both in virtual and real worlds. For instance, it enables control of robots with speech, faster video game development,...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Human pose and motion generation Prior works in human pose and motion synthesis have explored generative models either unconditionally [3 , or conditioned on various input signals such as a prior motion 20} [29], an action class or music Using text descriptions as a guidance in pose and motion synthesis has been a more recent research direction where many existing works use 2D keypoints as pose representations. [41] selects a base pose from 8 clusters based on an input text fed to a GAN model ...\n",
      "\n",
      "--- METHOD ---\n",
      "outperforms prior works in terms of generated pose realism and text alignment. To summarize, our main contributions are: ¢ We present Make-An-Animation — a text-conditioned human motion generation model which improves on prior state-of-the-art models, especially on diverse, inthe-wild text prompts. ¢ We show, for the first time, how to leverage largescale image datasets to learn in-the-wild human poses for generation. We show through ablations that pretraining on our collected Text Pseudo-Pose d...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s We compare the performance of our motion generation model with the existing state-of-the-art text-to-humanmotion generation models through human evaluation on 400 crowd-sourced prompts and automatic metrics on the HumanML3pD test set. 4.1. Automatic Metrics We perform an automatic evaluation on the HumanML3D test set in terms of Fréchet Distance (FID), RPrecision and Diversity scores. FID measures both diversity and quality of the samples comparing its distribution with the ground truth test s...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Current text-to-motion models are hindered by relatively small-scale motion capture datasets, as evident by their poor performance on more diverse, in-the-wild prompts. Motivated by this shortcoming, we present Make-AnAnimation, a human motion generation model trained on a collection of large-scale static pseudo-pose and motion capture data. As we demonstrate through our ablations and human studies comparing to prior works, pre-training on the large-scale TPP dataset significantly improves perfo...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.15091v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In-context learning in NLP has drawn tremendous attention recently (Dong et al., 2022). Unlike traditional learning paradigms that rely on training or finetuning models, in-context learning only provides a handful of demonstration examples to language models as a prefix to the test input, without any parameter updates. In-context learning has shown superior performance on a range of NLP tasks (Brown et al., 2020; Zhang et al., 2022b; *Work done during an internship at Meta AI. simigd@gmail.com {...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Demonstration examples Min et al. (2022) understand ICL through analyzing which aspects of the demonstration examples contribute or are irrelevant to task performance. They find replacing ground truth demonstration labels with random labels would not hurt task performance, while ICL still benefits from knowing the label space, distribution of inputs, and sequence format specified in demonstration examples.!? Zhang et al. (2022a) further show on sequence labeling tasks, the length of demonstratio...\n",
      "\n",
      "--- METHOD ---\n",
      "ORCA (Han and Tsvetkov, 2022) to search within OPT’s pretraining corpus. The process is guided by the gradients of the in-context learning data from downstream tasks, and we refer to the identified subset as supportive pretraining data to in-context learning following Han and Tsvetkov (2022). Furthermore, we quantitatively verify through a perturbative continued pretraining, that the supportive subset does improve the model’s in-context learning performance on downstream tasks, while not affecti...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "is limited to synthetic data of image-label pairs. In this work, we investigate a large language model OPT (Zhang et al., 2022b) and its pretraining data. We first hypothesize that there exists some specific pretraining data instances that are particularly helpful to the model’s in-context learning ability. As an attempt to find such instances, we adapt an iterative, gradient-based method ORCA (Han and Tsvetkov, 2022) to search within OPT’s pretraining corpus. The process is guided by the gradie...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In-context learning has shown superior performance on a range of NLP tasks, yet it remained unclear from where language models acquired this ability. We approach the problem by identifying a small subset of pretraining data that particularly supports language models to do in-context learning on downstream tasks. We analyze common features of the supportive instances in contrast to general pretraining data and find that: (1) The supportive pretraining data do not have a higher domain relevance to...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.12311v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "— 3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic con...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Imagine you are put into a 3D scene and asked to find “a chair between dining table and window” (Fig. {Ip. It is easy for humans to figure out the answer. Such a skill is called 3D visual grounding, and we typically rely on it for daily tasks that range from finding objects to manipulating tools. Mastering such an ability is critical to building any household robots to assist humans, as it serves as a basic skill needed for complex navigation (knowing where to go), manipulation (what/where to gr...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "3D Visual Grounding with Natural Language. Grounding a natural language query in an unstructured 3D scene is essential for various robotic tasks. Pioneering benchmarks such as ScanRefer [12] and ReferIt3D [13] have advanced this field. As proposed in these benchmarks, the referential tasks in 3D and text necessitate a deep understanding of both the compositional semantics of language and the structures, geometries, and semantics of 3D scenes. Numerous...\n",
      "\n",
      "--- METHOD ---\n",
      ", as a visual grounder. When asked to ground the spatially-informed text query “a chair between the dining table and window”, it incorrectly highlights the dining table and window, which are not the target but rather referential landmarks (red bounding boxes). We propose to address this problem by leveraging a large language model (LLM) to 1. Deliberately generate a plan to decompose complex visual grounding queries into sub-tasks; 2. Orchestrate and interact with tools such as target finder and...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s (Section IV). we evaluate LLM-Grounder on the ScanRefer benchmark [12]. This benchmark primarily evaluates 3D vision-language grounding capability that requires understanding of compositional visual referential expressions. Our approach improves the grounding capability of zero-shot open-vocabulary methods such as OpenScene and LERF, and demonstrates state-ofthe-art zero-shot grounding accuracy on ScanRefer with no labeled data used. Our ablation study shows LLM increases grounding capability ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ". Notably, our approach extends prior neural-symbolic approaches [36] by giving environment feedback to the agent and making the agent’s reasoning process closed-loop. It is important to note that our approach does not need any training on labeled data. It is open-vocabulary and can zero-shot generalize to novel 3D scenes and arbitrary text queries, a desirable property given the semantic diversity of 3D scenes and the limited availability of 3D-text labeled data. In our experiments (Section IV)...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.08041v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, Large Language Models (LLMs) pre-trained on massive text corpus with straightforward training objectives such as next-word prediction have exhibited remarkable abilities to understand, reason, and generate texts across a variety of open-ended tasks. Recent studies further exploit the strong generality of LLMs to improve visual understanding or generation tasks, collectively referred to as Multimodal LLM (MLLM). For example, previous work [4) [5] {6} [7] [8] perform open-ended vi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s of Baseline Tokenizers Visual tokenizer aims to represent the image as a sequence of discrete tokens. Previous work (13] [17] trains a Vector Quantized Variational AutoEncoders (VQ-VAE) by reconstructing image pixels, while Beit v2 propose vector-quantized knowledge distillation (VQ-KD) to train a visual tokenizer by reconstructing high-level features from the teacher model. We conduct two experiments to respectively align discrete representations of VQ-VAE and Beit v2 with OPT2.75 [19] model ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present SEED, a discrete image tokenizer, designed based on the premise that visual tokens compatible with LLMs should capture high-level semantics while being generated with a 1D causal dependency. SEED enables LLMs to be trained with multimodal data following the original recipe of text (i.e., next-word prediction), which is mature and scalable. The trained multimodal LLM is capable of both image-to-text and text-to-image generation tasks, taking one more step toward emergent multimodal cap...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02483v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ive Summarization by Tri-agent Generation Pipeline Wen Xiao! Giuseppe Carenini‘ Yujia Xie? Pengcheng He! t University of British Columbia, Vancouver, Canada * Microsoft Azure AI {carenini}@cs.ubc.ca, {wxiao, yujiaxie, penhe}@microsoft.com Abstract Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities. In this paper, we propose a tri-agent generation pipeline comprising a generator, an instruct...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models, exemplified by prominent models such as InstructGPT (Ouyang et al., 2022) and ChatGPT?, have emerged as essential resources in the field of natural language processing (NLP). These models have shown an extraordinary level of proficiency across a broad spectrum of NLP tasks, including machine translation, question answering, and text summarization. In light of their potential to drive further innovation in language-based technologies, the research community has exhibited gr...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "6.1 Text Editing Post-editing techniques have been extensively studied in various NLP tasks, including sentence fusion (Malmi et al., 2019), style transfer (Reid and Zhong, 2021), and wiki-editing (Reid and Neubig, 2022; Faltings et al., 2021). These...\n",
      "\n",
      "--- METHOD ---\n",
      "ology involves a three-component decomposition consisting of a generator, instructor, and editor (refer to Figure 1). This structure allows us to leverage inference-only large models for the complex tasks of content generation and correction, while utilizing smaller models for the simpler task of generating user-specific editing instructions. The instructor is designed to provide targeted directives for editing and refining the initial outputs of the generator. It is initialized by training on h...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better meet user expectations. ! 1 Introduction Large language models, exemplified by prominent models such as InstructGPT (Ouyang et al., 2022) and ChatGPT?, have emerged as essential resources in the field of natural language processing (NLP). These models have shown an extraordinary level of proficiency across a broad spectrum of NLP tasks, including machine translatio...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future Work In this paper, we introduce a novel generation paradigm that decomposes the generation process into three distinct components: the generator, the instructor, and the editor. Our approach is specifically designed to harness the capabilities of large language models, while accounting for constraints such as limited access and computational resources, and to facilitate the customization of generated content to align with user preference. Through a series of pilot experiments on the ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.05221v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The Alexa Prize program has empowered numerous university students to explore, experiment, and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challen...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      ", and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challenge in which university teams compete to build robot assistants that complete tasks in a s...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s While substantial advances have been made in the application of AI to create compelling and useful conversational assistants, significant challenges remain in advancing from the digital domain to create embodied conversational agents that can navigate the real world, manipulate objects, and complete tasks. The SimBot Challenge enabled university teams from around the world to compete to create effective and usable embodied conversational AI agents that were able to operate in a simulated envir...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10841v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrument tracks based on provided source tracks. In practical scenarios where there’s a predefined ensemble of tracks and various composition needs, an efficient and effective generative model that can generate any target tracks based on the other tracks becomes crucial. However, previous efforts have fallen short in addressing this necessity due to limitations in their music r...\n",
      "\n",
      "--- METHOD ---\n",
      "s typically focus on either one specific source-target track combination or the continuation of tracks. On the other hand, image-based research represents music as 2D images, with pianorolls*} being a popular choice. Pianorolls represent musical notes as horizontal lines, with the vertical position denoting pitch and the length signifying duration. A pianoroll explicitly separates tracks but it has to incorporate the entire pitch range of instruments, resulting in large and sparse images. Due to...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that the versatile GETMusic outperforms prior works proposed for certain specific composition tasks. 1 Introduction Symbolic music generation aims to create musical notes, which can help users in music composition. Due to the practical need for flexible and diverse music composition, the need for an efficient and unified approach capable of generating arbitrary tracks based on the others is higi} However, current research falls short of meeting this demand due to inherent limitatio...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ", besides the benefits from track arrangement, GETScore also gains advantages through this note tokenization: e Each track requires only two rows to accommodate the pitch and duration tokens, significantly enhancing the efficiency of GETScore. e The compound token preserves the interdependecies within a track. When it is generated, harmony is inherently guaranteed because the corresponding note group is derived from real-world data. 3.2 GETDiff In this section, we first introduce the forward and...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.09400v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training d...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have fundamentally transformed research and applications of natural language processing (NLP), significantly ad vancing the state-of-the-art performance for numerous tasks and revealing new emergent abilities (Brown et al., 2020; Wei et al., 2022). Based on the transformer architecture (Vaswani et al., 2017), three major variants of LLMs have been explored in the literature: the encoder-only models to encode input texts into representation vectors, e.g., BERT (Devlin...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Compared to other NLP tasks, language models can be trained with unlabeled data, enabling efficient data collection to produce gigantic scales for https: //github. com/ChenghaoMou/text-dedup/ tree/main --- --#Documents (M) #Tokens Code Language Initial URL Metric MinHash URL Filtering (B) (%) Filtering Filtering Dedup Dedup_ Rate (%) en English 5783.24 5766.08 3586.85 3308.30 3241.07 43.96 2846.97 45.ru Russian 1431.35 1429.05 922.34 845.64 799.3 44.16 737.20 11.es Spanish 844.48 842.75 530.01 4...\n",
      "\n",
      "--- METHOD ---\n",
      "(Dekking et al., 2007) to select appropriate thresholds for various dataset metrics (e.g., stopword ratios, data perplexity, and language identification scores), which can be used to filter noisy outliers for the dataset. As such, we leverage the percentiles of the distributions computed over large samples of data to effectively guide the threshold selection process for each filtering metric and language. Finally, we perform extensive deduplication for the data of the languages within our datase...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s After completing all the cleaning and deduplication steps, our ultimate dataset comprises 6.3 trillion tokens spanning 167 languages. Table 1 provides an overview of the number of documents and tokens for the top 42 languages in CulturaX following each processing stage. As can be seen, our datacleaning pipeline can substantially reduce the number of documents in the original mC4 and OSCAR datasets for each language. The total number of removed documents accounts for 46.48% of our initial docum...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present CulturaX, a novel multilingual dataset with text data for 167 languages. Our dataset is cleaned and deduplicated via a comprehensive pipeline, producing 6.3 trillion tokens. CulturaX is thus a large-scale and high-quality dataset, which can be readily used to train high-performing LLMs for multiple languages. Our data is openly accessible to the public to promote further research and applications of multilingual learning. References Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, a...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04050v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in [I], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest th...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04076v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is n...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, E2E models have achieved significant improvements in automatic speech recognition(ASR)[1, 2, 3]. Compared with hybrid models, where acoustic, pronunciation, and language models(LMs) are built and optimized separately, E2E models have achieved promising performance by directly mapping speech features into word sequences. There are some popular E2E models, including connectionist temporal classification[4, 5], recurrent neural network transducer(RNNT)[2, 6, 7], and attention-based...\n",
      "\n",
      "--- RELATED WORK ---\n",
      ". The proposed USTR-CT is iscussed in Section 3, followed by experiments and discussions in Section 4. 2. Related work 2.1. Speech-text joint training for ASR Several...\n",
      "\n",
      "--- METHOD ---\n",
      "to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multistep adaptations are also explored. The...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance. Index Terms: automatic speech recognition, text-only, domain adaptation, conformer transducer 1. Introduction In recent years, E2E models have achieved significant...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this work, an extra text encoder is introduced for textonly domain adaptation, which outperforms the TTS adaptation by 11.61% relatively when using phoneme representation. Compared to TTS adaptation, the proposed USTR-CT is efficient and resource-saving for fast domain adaptation. Besides, USTR-CT is able to adapt to the target domain with a singlestep training and combine with ILME to obtain further gains. Although experiments were conducted on non-streaming models in this work, the method...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.02453v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "— Biomimetic, dexterous robotic hands have the potential to replicate much of the tasks that a human can do, and to achieve status as a general manipulation platform. Recent advances in reinforcement learning (RL) frameworks have achieved remarkable performance in quadrupedal locomotion and dexterous manipulation tasks. Combined with GPUbased highly parallelized simulations capable of simulating thousands of robots in parallel, RL-based controllers have become more scalable and approachable. How...\n",
      "\n",
      "--- METHOD ---\n",
      "s in which the controller explicitly reasons with the dynamic model of the robot. This is especially the case for dexterous manipulation tasks, which apply an anthropomorphic robotic hand to movements that require the coordination of multiple fingers. Achieving this coordinated motion has the potential to replace many repetitive tasks such as pick-and-place in warehouses, assembly in factory lines, or assistance in our daily lives. In this work, we introduce the Faive Hand, a platform for dexter...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. The high purchase and maintenance costs may discourage frequent sim2real experiments of RL trained policies, since they can initially behave erratically on the real robot and require intense trial-and-error until they can work fully in reality. C. Approach At the Soft Robotics Lab, we have developed the Faive Hand, a biomimetic dexterous tendon-driven robotic platform for exploring dexterous manipulation. The current version of the hand uses 3D printed components and servo motors for accessib...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have introduced our anthropomorphic hand platform for use in autonomous manipulation. For this platform, we have developed a method to model, control, and sense rolling contact joints so that they can be integrated into a parallelized simulation environment to train a closed-loop policy. We show that the trained policy can be run on our physical robotic hand to achieve dexterous sphere rotation. While simulators and environments for RL training for dexterous manipulation have become more capa...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.09793v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending....\n",
      "\n",
      "--- CONCLUSION ---\n",
      "The increasing number and diversity of Large Language Models (LLMs) necessitate a comprehensive and systematic approach to organize, classify, and understand these models. In this study, we have proposed an effective solution by creating Constellation, a user-friendly web application that visualizes the hierarchical relationships among LLMs, helping to reveal prominent LLM families and underlying structures. Our approach is generally inspired by bioinformatics and sequence similarity. It utilize...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.13077v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a training-free framework called ControlVideo to enable natural and efficient text-to-video generation. ControlVi...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large-scale diffusion models have made a tremendous breakthrough on text-to-image synthesis [1, 22,26, 29,32] and their creative applications [6,8,21,37]. Several works [5,9, 11, 12,34] attempt to replicate this success in the video counterpart, i.e., modeling higher-dimensional complex video distributions in the wild world. However, training such a text-to-video model requires massive amounts of high-quality videos and computational resources, which limits the further research and applications ...\n",
      "\n",
      "--- METHOD ---\n",
      "outperforms alternative competitors qualitatively and quantitatively. Thanks to the efficient designs, i.e., the xFormers [17] implementation and hierarchical sampler, Control Video can produce both short and long videos within several minutes using one NVIDIA 2080Ti. In summary, our contributions are presented as follows: ¢ We propose a training-free Control Video for controllable text-to-video generation, which consists of the fully cross-frame interaction, interleaved-frame smoother, and hier...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.13383v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the s...\n",
      "\n",
      "--- METHOD ---\n",
      "are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information. We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI’s GPT-4 and GPT-3.5-Turbo, Google’s BARD, and Anthropic’s Claude, on the Code Coverage Prediction task. Finally, we argue that code coverage as a metric and pre-training data source are valuable for ove...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al Design When evaluating the LLMs on the code coverage prediction task, we designed the experiments to assess their performance on non-trivial coverage sequences while progressively providing more information and examples. First, we filtered out data points d = {m, t, cov(m, t)} where the coverage sequence is trivial consisting exclusively of the symbol Bl. These cases represent methods with no branches or where the test case covers every statement in the focal method. Although these data point...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we introduced the novel task of Code Coverage Prediction, which aims to assess the capabilities of Large Language Models (LLMs) in understanding code execution by accurately predicting the lines of code that are executed based on given test cases. We curated a comprehensive dataset named COVERAGEEVAL, consisting of coverage-annotated methods derived from the HumanEval dataset. This dataset enables researchers to explore and advance code coverage prediction techniques and LLM code ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.10537v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. Fol...\n",
      "\n",
      "--- METHOD ---\n",
      "s fail to generalize to open-domain videos. Recent advancements, however, indicate a rising interest in open-domain, visually guided audio generation. SpecVQGAN [9] and IM2WAV [10] both employ a language modeling method, leveraging the Transformer model to cap --- --ture the joint distribution of visual features and discrete audio tokens encoded by vector-quantized variational autoencoder (VQ-VAE). In SpecVQGAN, the VQ-VAE operates specifically on spectrograms and subsequently employs a neural v...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. Index Terms— Sound generation, audio-visual learning, video-to-audio generation, multimodal learning 1. INTRODUCTION Recent years have seen remarkable breakthroughs in audio generation, powered predominantly by the evolution of large-scale deep learning models and datasets. Despite great achievements in text-to-audio [1, 2] and text-to-music [3, 4] gener...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S In this paper, we introduced FoleyGen, a video-to-audio generation model following a language modeling paradigm. FoleyGen utilizes the EnCodec for bidirectional waveform-token conversion,a visual encoder for visual feature extraction and a Transformer decoder for conditioned audio token generation. Our evaluations demonstrate that FoleyGen surpasses prior methodologies in both objective metrics and human evaluations. Through our explorations, we observed that visual encoders trained on multimo...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.10373v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The evolution of text-to-image models has recently facilitated advances in image editing and content creation, allowing users to control various proprieties of both generated and real images. Nevertheless, expanding this exciting progress to video is still lagging behind. A surge of large-scale text-to-video generative models has emerged, demonstrating impressive results in generating clips solely from textual descriptions. However, despite the progress made in this area, existing video models a...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Text-driven image & video synthesis Seminal works designed GAN architectures to synthesize images conditioned on text embeddings (Reed et al} 2016} Zhang et al.||2016). With the evergrowing scale of vision-language datasets and pretraining strategies (Radford et al.||faa a 202), there has been a remarkable progress in text-driven image generation capabilities. Users can sytnesize high-quality visual content using simple text prompts. Much of this progress Dhariwal| as state is also attributed to...\n",
      "\n",
      "--- METHOD ---\n",
      "edits it according to a target text prompt (middle and bottom rows), while preserving the semantic layout and motion in the original scene. ABSTRACT The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven vi...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.06873v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing w...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The technology of generative models has undergone rapid and transformative advancements in various machine learning applications, encompassing text [I], [2], vision 3], and audio (4). These advancements have had significant implications for both the industry and society at large. Notably, generative models using multi-modal input have emerged as a remarkable innovation [5}-[I0}. In the speech domain, one prominent speech generation task that leverages audio-text input is zero-shot text-to-speech...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "A. Autoregressive generative models Generative models based on a language modeling approach using autoregressive Transformers, also known as decoderonly Transformers, have garnered significant success in various application domains. Notable examples of such models include the GPT series [I], [2] and DALL-E {30}. The autoregressive approach has also been extended to the audio and speech domains. AudioLM and MusicLM are pioneering efforts that exploit multiple types of tokens, each with a distinct...\n",
      "\n",
      "--- METHOD ---\n",
      "s typically required distinct expert models for each task, which is not ideal, given the potential diversity of acoustic disturbances (26). Furthermore, there has been a lack of comprehensive audiotext-based speech enhancement models that leverage reference transcriptions to generate intelligible speech, except for limited studies focusing only on particular speech enhancement tasks Given the aforementioned considerations and the successful precedents in other domains, the creation of audio-text...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show SpeechX’s efficacy in various tasks, including zeroshot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See for demo samples. Index Terms—Speech generation, audio-text input, multitask learning, zero-shot text-to-speech, noise suppression, target speaker extraction, speech editing, speech removal I. INTRODUCTION The technology of...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we described SpeechX, a novel versatile speech generation model capable of handling diverse audiotext-based speech generation tasks, including zero-shot TTS, noise suppression, speech removal, target speaker extraction, and speech editing. For noise suppression and target speaker extraction, the proposed model provides a unified way for incorporating the knowledge of transcriptions. Also, regarding speech editing, SpeechX enables modifying the spoken content of a speech signal tha...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03210v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the sel/f-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The transformer neural network architecture [52] is having a major impact on fields ranging from natural language processing (NLP) [13,42] to computer vision [14]. Indeed, transformers are now deployed in large, real-world systems used by hundreds of millions of people (e.g., Stable Diffusion, ChatGPT, Microsoft Copilot). However, the mechanisms behind this success remain somewhat mysterious, especially as new capabilities continue to emerge with increasing model complexities and sizes [11,60]. ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Many researchers have attempted to investigate the inner workings of transformers. [8,34] seek to understand the performance improvement from transformer-based language models by exploring learned linguistic representations, and [49] observed that BERT recapitulates classic steps in natural language analysis, from part-of-speech tagging to relation classification. Another popular approach is mechanistic interpretability, i.e., reverse engineering transformer models (e.g., [15, 16, 37]). Attentio...\n",
      "\n",
      "--- METHOD ---\n",
      "is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http: //attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of ou...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ed with using Euclidean distance as our distance metric when creating t-SNE and UMAP projections of queries and keys, but this generally led to weaker distance-dot product correlations. Across multiple datasets and models, the relationship between distance and attention holds fairly well. For example, with Wiki-Auto data [23], the mean correlation between query-key distances and dot products is -0.938 for BERT and -0.792 for GPT. An example result from BERT is shown in Fig. 4b. On the set of COC...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S & FUTURE WORK In this work, we introduce a new technique for visualizing transformer self-attention based on a joint embedding space for queries and keys. Applying our technique, we create AttentionViz (demo: http://attentionviz.com), an interactive visualization tool, and use it to gain insights about attention in both language and vision transformers. For instance, we discover novel hue/frequency behavior in ViT, and striking query-key norm disparities in GPT-2. Although our approach is tail...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.08275v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent advancements in multimodal pre-training have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing frameworks to curate such multimodal data, in particular language descriptions for 3D shapes, are not scalable, and the collected language descriptions are not diverse. To address this, we introduce ULIP-2, a simple yet effective tri-modal pre-training fr...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "3D visual understanding has seen a surge of interests in recent years [8, 10, 21, 23, 35, 51] due to its growing ap * Contact: Ixue@salesforce.com Render Sample Point Holistic Images Clouds Large Multimodal Model Rank by CLIP Describe “a statue holding a book and a scepter\" F “a small stone statue with a book and writing tool\" Text @ _ image 3D Encoder COS Encoder Encoder Pre-aligned Downstream Tasks Zero-shot 3D. 3D classification classification 3D captioning with fine-tuning Figure 1. Overview...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multimodal Representation Learning. In recent years, multimodal representation learning has emerged as a popular research topic due to its remarkable capabilities and applications. Most research works focus on learning mul timodal representation for only two modalities: language and image modalities, which have led to remarkable outcomes. One line of research in this area emphasizes the interaction between image regions and caption tokens using Transformer-based architectures [12, 15, 16, 30, 56...\n",
      "\n",
      "--- METHOD ---\n",
      "s used by existing frameworks to curate such multimodal data, in particular language descriptions for 3D shapes, are not scalable, and the collected language descriptions are not diverse. To address this, we introduce ULIP-2, a simple yet effective tri-modal pre-training framework that leverages large multimodal models to automatically generate holistic language descriptions for 3D shapes. It only needs 3D data as input, eliminating the need for any manual 3D annotations, and is therefore scalab...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. Experiments show that ULIP-2 demonstrates substantial benefits in three downstream tasks: zero-shot 3D classification, standard 3D classification with fine-tuning, and 3D captioning (3D-tolanguage generation). It achieves a new SOTA of 50.6% (top1) on Objaverse-LVIS and 84.7% (top-1) on ModelNet40 in zero-shot classification. In the Scan...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Discussion We present ULIP-2, a novel framework for multimodal 3D representation learning. By leveraging large multimodal models for language description generation and scaling up the multimodal 3D pre-training, ULIP-2 not only addresses the quality and scalability challenges in existing multimodal 3D datasets but also demonstrates significant improvements in all downstream tasks. We also release \"ULIP-Objaverse\" triplets and \"ULIP-ShapeNet\" triplets, two large-scale trimodal datasets to fos...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.07944v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations. To bridge this gap, we propose a joint speech and language model (SLM) using a Speech2Text adapter, which maps speech into text token embedding space without speech information loss. Additionally, using a CTC-based blank-filtering, we can reduce the speech sequence length to that of text. In speech MultiWoz dataset (DSTC11 challeng...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "There has been considerable interest in extending the capability of the large language models (LLMs) from text to other modalities including speech. One thread of work attempts to map speech and text to the same latent representations [1, 2, 3]. A shared encoder is employed for both speech and text, in one case with an explicit loss term promoting the same embedding space [3] and in other without the explicit term [1]. In most practical spoken language systems, speech input is recognized using a...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "A closely related line of work injects text inputs into speech models [7, 1, 3, 8, 2] and align the learned representation of text and speech to the same space. This is done by TTS or more recently up-sampled text and minimizing an L2 loss of aligned speech and text frames. This is in contrast to our work, where we do the opposite and reduce the frame rate of the audio sequence to bring it closer to text. This is done via a CTC model [9] where we use the predictions to filter out blank frames. T...\n",
      "\n",
      "--- METHOD ---\n",
      "s to speech understanding. As mentioned before, unlike written domain, speech understanding poses an additional challenge that rare entities are not easily recognizable [5]. Therefore we introduce an audio retrieval method to alleviate these difficulties and achieve better performance on end-to-end speech dialog understanding. 3. Model 3.1. Joint speech and language model (SLM) The speech understanding task requires a model that can simultaneously recognize speech inputs and understand the seman...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and results 4.1. Evaluation Task The DSTC11 Challenge Task is based on MutliWoz 2.1 and has the same distribution in terms of dialogs and turns [5]. The main difference is that the written user responses are replaced with spoken versions. The responses were generated using TTS in the training set and by human voices from crowd-sourced workers in the test set. Additionally, previously researchers had discovered that the slot values in the training and test sets had substantial overlap, which le...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We proposed a joint speech and language model (SLM) with both speech and text inputs. The speech input is encoded using a separately trained CTC encoder where the input is down % WER | RNN-T [5] 13.RNN-T in-domain finetuned [5] 10.Adapter only 10.Trainable params SLM ReSLM without Speech2Text Adapter Whole T5 12.0 11.TS encodertemb 11.2 11.T5 encoder only 11.3 10.with Speech2Text Adapter “WholeTS 97 OTS encodertemb 94 9.T5 encoder only 9.5Table 3: Speech recognition performance. We compare mod...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.06949v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Personalization has emerged as a prominent aspect within the field of generative Al, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome the...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recent work on text-to-image (T2I) personalization [25] has opened the door for a new class of creative applications. Specifically, for face personalization, it allows generation of new images of a specific face or person in different styles. The impressive diversity of styles is owed to the strong prior of pre-trained diffusion model, and one of the key properties of works such as DreamBooth [25], is the ability to implant a new subject into the model without damaging the model’s prior. Another...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Text-to-Image Models Several recent models such as Imagen [26], DALL-E2 [22], Stable Diffusion (SD) [24], Muse [7], Parti [33], etc., demonstrate excellent image generation capabilities given a text prompt. Some Text-toImage (T2]1) models like SD and Muse also allow conditioning the generation with a given image via an encoder network. Techniques such as ControlNet [35] propose ways to incorporate new input conditioning such as depth. However, current text and image-based conditioning in these m...\n",
      "\n",
      "--- METHOD ---\n",
      "both conserves model integrity and style diversity while closely approximating the subject’s essence and details. Abstract Personalization has emerged as a prominent aspect within the field of generative Al, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU t...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ation, we demonstrate our HyperDreamBooth on the SD model, given its relatively small size. Yet, the proposed technique is generic and applicable to any T2I model. Personalization of Generative Models Personalized generation aims to create varied images of a specific subject from one or a few reference images. Earlier approaches utilized GANs to manipulate subject images into new contexts. Pivotal tuning [23] fine-tunes GANs with inverted latent codes, while [20] fine-tunes StyleGAN with around ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we presented HyperDreamBooth a new method for fast and lightweight subject personalization of diffusion models. It leverages a HyperNetwork to generate Lightweight DreamBooth (LiDB) parameters for a diffusion model with a subsequent fast rank-relaxed finetuning that achieves a sharp reduction in size and speed compared to DreamBooth and other optimization-based personalization work. We showed that it produces high-quality and diverse images of faces with different styles and semant...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.02510v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Understanding cortical responses to human visual perception has emerged a research hotspot, which can significantly motivate the development of computational cognitive system with the knowledge of neuroscience (Palazzo et al. 2020). Along with the rapid development of physiological techniques such as functional magnetic resonance imaging (f{MRJ or electroencephalograph (EEG), it becomes possible to record the visually-evoked human brain activities for further analysis. Thus, the research communi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Diffusion Models Recently, diffusion models have emerged as state-of-theart approaches in the field of generative models for several tasks, including image synthesis, video generation, and molecule design (Yang et al. 2022; Song, Meng, and Ermon 2020; Dhariwal and Nichol 2021). A denoising diffusion probabilistic model (DDPM) (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015) is a parameterized bi-directional Markov chain using variational inference to produce sample matching after a finite...\n",
      "\n",
      "--- METHOD ---\n",
      ". Introduction Understanding cortical responses to human visual perception has emerged a research hotspot, which can significantly motivate the development of computational cognitive system with the knowledge of neuroscience (Palazzo et al. 2020). Along with the rapid development of physiological techniques such as functional magnetic resonance imaging (f{MRJ or electroencephalograph (EEG), it becomes possible to record the visually-evoked human brain activities for further analysis. Thus, the r...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method. Introduction Understanding cortical responses to human visual perception has emerged a research hotspot, which can significantly motivate the development of computational cognitive system with the knowledge of neuroscience (Palazzo et al. 2020). Along with the rapid development of physiological techniques such as functional magnetic resonance imaging (f{MRJ or elect...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we explore to understand the visually-evoked brain activity. Specifically, We proposed a framework, named NEUROIMAGEN, to reconstruct images of visual perceptions from EEG signals. The NEUROIMAGEN first --- --GT images Electric guitar Pizza Anemone fish Canoe Subj 05 SubjFigure 5: Comparison of reconstructed images on different subjects. The images on the left with red boxes represent the ground truth images. The other six images represent the reconstructed images of different sub...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06218v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we analyze the performance of a multitask end-to-end transformer model on the task of conversational recommendations, which aim to provide recommendations based on a user’s explicit preferences expressed in dialogue. While previous works in this area adopt complex multi-component approaches where the dialogue management and entity recommendation tasks are handled by separate components, we show that a unified transformer model, based on the T5 text-totext transformer model, can pe...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The modern recommendation systems found in commercial applications are largely based on implicit preferences, such as a user’s history of web page clicks, item purchases, or media streams, with the record of these actions used to retrieve relevant recommendations (Rendle et al., 2012). This approach often works, but in the case where a user might not have an extensive history, or might desire a recommendation which doesn’t match their usual niche, we might want a system which can take advantage ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "(Penha and Hauff, 2020) and design a --- --series of probes to assess how the model leverages different types of information to generate dialogue and recommendations. One potential problem of a single component system is the reliance on a large dataset of sample dialogues containing both recommendation and language information. To bypass the need for a single large dialogue dataset, we finetune the pretrained TS model on a relatively small dataset of dialogues, and incorporate movie relationship...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al methodology, and a series of probe studies showing how dialogue and recommendation mutually improved by sharing a common model. 1.1 Related Work The section presents a brief background on conversational recommendations, multitask transformer models, and the evaluation of conversational recommendation models through probe studies. A recent survey published by Gao et al. (2021) highlights the range of strategies used to address the different challenges faced by a conversational recommendation s...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we presented a multitask approach to end-to-end conversational recommendations. In direct comparison to two previously published models in the domain, our T5-based architecture outperformed the baselines in both its quality of dialogue and recommendation. When probed on recommendation, attribute knowledge, and description, our model demonstrates that dialogues and recommendations can be mutually improved by sharing a model architecture. Specifically, the probes prove that the mode...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16367v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "As dialogue agents become increasingly humanlike in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ally that certain forms of reinforcement learning from human feedback (RLHF) can actually exacerbate, rather than mitigate, the tendency for LLM-based dialogue agents to express a desire for self-preservation (Perez et al., 2022). Yet to take literally a dialogue agent’s apparent desire for self-preservation is no less problematic in the context of an LLM that has been fine-tuned on human or Al-generated feedback than in the context of one that has not. So it remains useful to cast the behaviour...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ": Safety Implications It is, perhaps, somewhat reassuring to know that LLM-based dialogue agents are not conscious entities with their own agendas, and an instinct for self-preservation, that when they appear to have those things it is merely role-play. But it would be a mistake to take too much comfort in this. A dialogue agent that role-plays an instinct for survival has the potential to cause at least as much harm as a real human facing a severe threat. ‘Tm a conversation with ChatGPT (May 4‘...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09764v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several applications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build applicationspecific (language) models, but this leads to an inc...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "On-device Automatic Speech Recognition (ASR) is subject to several constraints: it should return accurate results in a reasonable time frame without consuming too much memory and disk space. State-of-the-art research often is accuracy focused, while resource-constrained applications also need to take care of performance and size. Finding an architecture that reaches all constraints is not trivial. Another challenge is that ASR systems often serve a large variety of requests. ASR systems can serv...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We start by discussing related work on modeling several domains/tasks at once. Many pattern recognition tasks are imbalanced since data from different categories do not occur at the same frequency. Therefore, the less frequent categories are not well represented in the training data (Anand et al., 1993; Johnson and Khoshgoftaar, 2019), which results in a sub-optimal model. Data-driven approaches to deal with the data imbalance include under- and over-sampling (Van Hulse et al., 2007). Refinement...\n",
      "\n",
      "--- METHOD ---\n",
      "to optimally sample training data. We sample data from different sources, e.g. anonymized and randomly sampled user requests from opted-in users for VA and STT and artificial requests spanning many different domains that focus on improving the tail of the distribution. The data-driven approach tries to find the optimal balance between the application-specific data sources by creating a balanced development set and distributing the sampling weights based on the importance of each data source and ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "with multi-task learning, data augmentation and a classifier combined with single-task models to appropriately model several skills in a conversation agent. Balancing through interleaved sampling of different corpora was investigated in (Xing et al., 2022) as well as model-based approaches like multi-task and weighted learning, which allows the model to self-control the impact of different corpora. Other ways to increase the modeling power are using a Mixture of Experts (Shazeer et al., 2017; Zh...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We aim to develop a single NNLM that can serve both VA and STT requests with the same accuracy and speed as application-specific NNLMs, while reducing the disk size approximately by half. We develop a method to optimally balance the data of the VA and STT applications, and propose two novel FOFE feed-forward architectures. The Application-Agnostic Mixture FOFE and the Application-Dependent FOFE both outperform the --- --baseline FOFE and Transformer models in terms of accuracy, and the latter is...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.15779v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. Recent research has extended these models to support text-guided image editing. While text guidance is an intuitive editing interface for users, it often fails to ensure the precise concept conveyed by users. To address this issue, we propose Custom-Edit, in which we (i) customize a diffusion model with a few reference images and then (ii) perform text-guided editing. Our key discovery ...\n",
      "\n",
      "--- METHOD ---\n",
      "s and validate our findings on two editing methods using various datasets. 1. Introduction Recent work on deep generative models has led to rapid advancements in image editing. Text-to-image models [19, 22] trained on large-scale databases [23] allow intuitive editing [7, 15] of images in various domains. Then, to what extent can these models support precise editing instructions? Can a unique concept of the user, especially one not encountered during large-scale training, be utilized for editing...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.06044v2.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Neural Radiance Fields (NeRFs) [Mildenhall et al. 2020] can achieve remarkable novel view synthesis (NVS) results, powering applications in the domains of virtual/mixed reality, robotics, computational Authors’ addresses: Barbara Roessle, Technical University of Munich, Germany; Norman Miiller, Technical University of Munich, Germany and Meta Reality Labs Zurich, Switzerland; Lorenzo Porzi, Meta Reality Labs Zurich, Switzerland; Samuel Rota Buld, Meta Reality Labs Zurich, Switzerland; Peter Kont...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Neural Radiance Fields (NeRFs) model a 3D scene as a volumetric function, which can be rendered from arbitrary viewpoints to generate highly-realistic images. While the seminal paper of [Mildenhall et al. 2020] encoded this function as a Multi-Layer Perceptron (MLP), more recent works have proposed alternative representations based on spatial data structures such as voxel grids [Sara Fridovich-Keil and Alex Yu et al. 2022; Sun et al. 2022], plane-based factorizations [Chen et al. 2022], or multi...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies > Reconstruction. Additional Key Words and Phrases: Neural radiance fields, Novel view synthesis 1 INTRODUCTION Neural Radiance Fields (NeRFs) [Mildenhall et al. 2020] can achieve remarkable novel view synthesis (NVS) results, powering applications in the domains of virtual/mixed reality, robotics, computational Authors’ addresses: Barbara Roessle, Technical University of Munich, Germany; Norman Miiller, Technical University of Munich, Germany and Meta Reality Labs Zurich, Switzerland; L...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s (Sec. 4.5.2). To improve realism in novel views, our method (Fig. 2) leverages a 2D adversarial loss that directly updates the 3D scene representation. To this end, a discriminator learns the distribution of image patches in the training data. Through adversarial training, the 3D NeRF representation (Sec. 3.1) is updated towards rendering patches that match this distribution (Sec. 3.2). On top of that, a 2D generator considers NeRF renderings at multiple resolutions, refining them based on fee...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduce GANeRF, a new approach for adversarial optimization of neural radiance fields. The main idea behind our approach is to impose patch-based rendering constraints into the radiance field reconstruction. By backpropagating gradients from a scene patch discriminator, we effectively address typical imperfections and rendering artifacts stemming from traditional NeRF methods. In particular, in regions with limited coverage, this significantly improves rendering quality, both qualitatively ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.05734v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a genera...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "RTIFICIAL intelligence generated content (AIGC) refers to any digital content such as images, videos, text, or audio that has been fully or partially created by an AI system without human involvement in the creative process [1]. Of particular interest is the ability of AI to produce audio content based on text, phonemes, or images [2]-[4]. Al-based audio generation has a wide potential in applications including synthesizing human or artificial voices for digital assistants [5], generating sound ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "A. Conditional Audio Generation Audio generation is an emerging topic that focuses on modelling the generation of general audio, including recent models such as AudioGen [3], AudioLDM [4], and Make-anAudio [15]. AudioGen treats audio generation as a conditional language modelling task, while the other two works approach this task by latent diffusion. Studies on image-toaudio and video-to-audio generation, such as Im2Wav [29] and SpecVQGAN [30], are also areas of interest to researchers. Addition...\n",
      "\n",
      "--- METHOD ---\n",
      "for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform selfsupervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed f...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on the major benchmarks of text-to-audio, textto-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches. Our code, pretrained model, and demo are available at https://audioldm.github.io/audioldm2. Index Terms—audio generation, diffusion model, self supervised learning, speech synthesis, AIGC I. INTRODUCTION RTIFICIAL intelligence generated content (AIGC) refers to any digital content such as images, v...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s about a specific design drawn from one domain may not necessarily transfer to another. Recent advancements in addressing problems from a unified perspective have yielded substantial progress [16]-[19]. This trend highlights the potential of constructing a unified audio generation framework. This paper presents a novel and versatile framework, called AudioLDM 2, that can generate audio with flexible conditions, without the need for domain-specific inductive bias. The core idea is to introduce a...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.10007v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      ": We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and actions, we encode the sequence into tokens, mask out a subset, and train a model to predict the missing content from the rest. We hypothesize that if a robot can predict the masked-out content it will have acquired a good model of the physical world that can e...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Over the last couple of years, inspired by vision [1, 2, 3, 4] and language [5, 6, 7], there has been an increased interest in pre-training for robotics. For example, we have seen promising results from self-supervised visual pre-training on large and diverse image collections [8]. However, robotic data contains rich sensory and motor information that is difficult to capture with visual pre-training alone. We ask: can we learn good sensorimotor representations from robotic trajectories? In this ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. We find that RPT consistently outperforms training from scratch and that the improvements are larger for harder tasks (up to 2x for the block stacking task). We also find that our sensorimotor pre-training approach enables successful transfer across different tasks, lab environments, and robots. Moreover, our approach has favorable scaling properties and benefits from better vision encoders, longer sensorimotor context lengths, and larger pre-training datasets. Finally, we find that masking a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ". We describe an approach for robot learning with sensorimotor pre-training. Our model is a Transformer that operates on sequences of sensorimotor tokens. We pre-train our model by masked prediction. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task, and has favorable scaling properties. Finally, we demonstrate successful transfer across different tasks, lab environments, and robots. --- --Workspace Setup Tr...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.04663v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) show impressive generalization ability to new tasks and languages. Some of their most exciting capabilities, such as producing logical reasoning to solve a problem, are found to emerge only when the model size is over a certain threshold, often hundreds of billions of parameters (Wei et al.||2022bja). The impressive capabilities of these models to produce high-quality responses without any task-specific tuning along with the very high cost of further tuning such mode...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Instruction Tuning —Instruction-tuned models (Wei et al.|[2021|/Longpre et al.|[2023) often have better performance for few-shot ICL tasks than base language models since they are already primed to following instructions due to being fine-tuned on a diverse set of tasks. Using instruction-tuned models is a key component of FIAT. In-Context Learning In in-context learning, the parameters of the LLM remain fixed and a prompt containing a few examples along with reasoning steps is used to prime the...\n",
      "\n",
      "--- METHOD ---\n",
      "s to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT’s effectiveness on a variety of multilingual task?}| and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100—10,000 training examples. We hope that FIAT provides a practical way of harnessing the full potential of LLMs without needing to make a hard choice between learning paradigms. 1 INTRODUCTION Large language models (LLMs) show impressive generalizat...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on these disparate methods in order to choose the best approach. We instead take the view that these two model learning paradigms are in fact complementary. To this end, we propose FIAT—Fusing Learning Paradigms with Instruction-Accelerated Tuning (FIAT), which utilizes both ICL on very large models and parameter tuning on moderately-sized LLM while fusing the common techniques associated with each paradigm. FIAT uses hand-engineering instruction prompts that elicit chain-of-thought reasoning ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s from them below. In the end, we find that while certain design °We report the average result on the under-represented languages, following the recommendations of the XTREME-UP benchmark. ‘During manual prompt engineering, we used Google Translate to assist with explanation annotation. \"Note that while the exemplars have Bengali questions, we instruct the model to carry out its reasoning in English. --- --XOR-ATTRIQA XTREME-UP XTREME-UP Cross-lingual QA: Indics — Cross-lingual QA: Full (100) (1...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.04269v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entitysparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generate...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Automatic summarization has come a long way in the past few years, largely due to a paradigm shift away from supervised fine-tuning on labeled datasets to zero-shot prompting with Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023). Without additional training, careful prompting can enable fine-grained control over summary characteristics, such as length (Goyal et al., 2022), topics (Bhaskar et al., 2023), and style (Pu and Demberg, 2023). An overlooked aspect is the information density o...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "GPT Summarization. Goyal et al. (2022) benchmarked GPT-3 on news article summarization and found that humans preferred GPT-3 summaries over previous supervised baselines, which was --- --not reflective of existing reference-based and reference-free metrics. Zhang et al. (2023) find that zeroshot GPT-3 summaries perform on par with humans by soliciting high-quality summaries from freelance writers. Entity-Based Summarization. Narayan et al. (2021) proposed generating entity chains as a planning s...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We study the impact of summary densification on human preferences of overall quality. We find that a degree of densification is preferred, yet, when summaries contain too many entities per token, it is very difficult maintain readability and coherence. We open-source annotated test set as well as a larger un-annotated training set for further research into the topic of fixed-length, variable density summarization. 7 Limitations We only analyze CoD for a single domain, news summarization. Annotat...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.17306v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large...\n",
      "\n",
      "--- METHOD ---\n",
      "In this section we discuss the construction of Chain-ofThought Hub. We first discuss our method for test data collection, then we discuss how we obtain the model performance on our test suite. Our main goal is to curate a high-quality collection of datasets that (1) is closely related to the actual usage of LLMs; (2) clearly differentiate the performance of stronger and weaker language models. We consider the following datasets: GSMS8k A widely used math reasoning datasets consisting of 8k probl...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s First we discuss the model families we consider. We focus on the popular models in production, including GPT, Claude, PaLM, LLaMA, and TS model families, specifically: ‘https://leaderboard.Imsys.org/ Shttps://huggingface.co/spaces/HuggingFaceH4/open_Ilm_leaderboard --- --Chain-of-Thought Hub Table 1. Overall model performance on Chain-of-Thought Hub. Numbers with an asterisk* are from our test scripts. For model types, base means the model checkpoint after pretraining, SIFT means supervised in...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "and Future Work In this work, we propose Chain-of-Thought Hub, an opensource, continuous effort to measure the reasoning capability of very large language models. Our results clearly show the performance differences between smaller and larger models, and between close-source and open-source models. After carefully examining the results, we show two important directions for further improving open-sourced models: building better base models and exploring RLHF. We also point out the great potential...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10688v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the correspond...\n",
      "\n",
      "--- METHOD ---\n",
      "MolXPT is a language model pre-trained on heterogeneous data including scientific text, SMILES sequences, and “wrapped” sequences between SMILES and text. Due to the flexible input, we can finetune it for various text and molecular tasks. The framework of MolXPT is in Figure 1. 2.1 Pre-training corpus For scientific text, we use the titles and abstracts of 30M papers from PubMed!. For molecular SMILES, we randomly choose 30M molecules from PubChem? (Kim et al., 2022). The wrapped sequences are c...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning. 1 Introduction Generative pre-trained Transformer (GPT), like GPT-3 (Brown et al., 2020) and ChatGPT (OpenAI, 2022), have obtained great success in natural language processing. They usually have billions of para...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and Future Work We propose MolXPT, a generative model pretrained on scientific text, molecular SMILES and --- --Molecule-to-text BLEU-2 BLEU-4 Rouge-1 Rouge-2 Rouge-L METEOR Text2Mol MoITS-small (77M) 0.519 0.436 0.620 0.469 0.563 0.551 0.MoITS-base (250M) 0.540 0.457 0.634 0.485 0.578 0.569 0.MoITS-Large (800M) 0.594 0.508 0.654 0.510 0.594 0.614 0.MolXPT (350M) 0.594 0.505 0.660 0.511 0.597 0.626 0.Text-to-molecule Exact} MACCSt RDK*t Morgant FCD| Text2molt Validityt MolITS-small 0.079 0.703...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.10279v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We introduce POP3D, a novel framework that creates a full 360°view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The ability to generate high-quality realistic 3D models from minimal input is an ongoing challenge for various applications in computer graphics, vision, virtual reality, and augmented reality.Despite the recent advances in the area of multi-view reconstruction through the differentiable rendering of neural representations [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], such methods rely heavily on vast amounts of images paired with camera parameters. While this reliance m...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S 2.1 Few-View-to-3D Reconstruction NeRF [Mildenhall et al. 2020] and its variants [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020] have shown remarkable --- --360° Reconstruction From a Single Image Using Space Carved Outpainting reconstruction performance of scenes and objects only given RGB images paired with camera poses. However, without dense camera views, training a neural radiance field becomes a severely underconstrained problem. When only given a few views, such models...\n",
      "\n",
      "--- METHOD ---\n",
      ". The results in (b) and (c) show that the naive usage of the distillation loss and neural density fields leads to sub-optimal novel views and a low-fidelity surface [Melas-Kyriazi et al. 2023; Xu et al. 2023]. On the other hand, our framework successfully generates novel views that resemble the original input image and also reconstructs the 3D object’s surface with high fidelity, as we observe in (d) and (e). Image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [OH...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, and discuss the adverse impacts of an overly granular or coarse camera schedule in Sec. 4.2.1. In the rest of the section, we will denote the camera positions that have been explored until the i-th camera position in S as So,; such that 0 <i <s. 3.3. Outpainting Mask Acquisition In order to generate the appearance and shape of unseen regions seamlessly, the areas designated for outpainting need to be appropriately chosen. To address this, we leverage the concept of the visual hull [Laurentini...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "AND FUTURE WORK In this study, we present POP3D, a novel framework that addresses two long-standing challenges in the domain of 360° reconstruction from a single RGB image: generalization and fidelity. POP3D fully leverages current state-of-the-art priors trained on large-scale datasets and successfully overcomes the problem of generalization --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia y (a) Input RGB (b) Novel Views (c) Complete Image Backside Figure 7: Limitati...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11000v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multimodel content. With discrete speech represe...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (OpenAl]| 2023} Touvron et al. 2023) have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E (Driess et al.|/2023), and LLaVA (Liu et al.|/2023), have explored the ability of LLMs to understand multi-modal information. However, a significant gap exists between current LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and understand multi-modal cont...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multi-modal Large Language Model Current multi-modal LLMs predominantly focus on the visual domain, feeding continuous representations obtained from pre-trained visual encoders into LLMs, facilitating full-parameter or parameter-efficient training on visual-language data (OpenAL (2023) Huang et al |[2023b (20232). Palm-E (Driess etal, [2023) integrates the 340B PaLM (Chowdhery et al.|/2022) and 22B Vision Transformer (Dosovitskiy et al.|/2021) into the largest vision-language model. LLaVA 2023) ...\n",
      "\n",
      "--- METHOD ---\n",
      "s or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator. Since the representations of speech and text are not aligned, the LLM’s knowledge cannot be transferred to the speech modality. Second, the cascade approach ( fet al.| 2023} [Huang et al.||2023a) suffers from the loss of paralinguistic signals such as emotion and prosody. Third, existing spoken language models 3 only synthesize speech but fail to comprehend its ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://Onutation.github.io/SpeechGPT.github.io 1 Introduction Large language models (OpenAl]| 2023} Touvron et al. 2023) have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E (Driess et al.|/2023), and LLaVA (...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This work presents SpeechGPT, an inherent cross-modal multimodal large language model capable of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal instruction-following dataset contains cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a three-stage...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05973v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We address the challenge of ensuring differential privacy (DP) guarantees in training deep retrieval systems. Training these systems often involves the use of contrastive-style losses, which are typically non-per-example decomposable, making them difficult to directly DP-train with since common techniques require per-example gradients. To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system. Our method employs DP language mo...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Deep retrieval systems have been widely adopted in many online services, from search to advertising, to match user queries to relevant recommendations (Covington et al., 2016; Huang et al., 2020). In many applications, candidate items for retrieval are often publicly available non-personal information in the sense that they do not contain any specific information related to any single user (e.g., articles, products, movies, ads). However, the input queries to retrieval systems can often contain ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Synthetic Generation using DP LMs __ Several recent studies (Yue et al., 2022; Mattern et al., 2022; Mireshghallah et al., 2022; Putta et al., 2022) have investigated the utility of private synthetic data from DP finetuned LMs in downstream tasks with pointwise losses, including text classification and semantic parsing. These works find that downstream models trained with private synthetic data outperform directly DP-trained models under the same privacy budget, and non-private synthetic data ge...\n",
      "\n",
      "--- METHOD ---\n",
      "employs DP language models (LMs) to generate private synthetic queries representative of the original data. These synthetic queries can be used in downstream retrieval system training without compromising privacy. Our approach demonstrates a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining querylevel privacy guarantees. This work highlights the potential of harnessing LMs to overcome limitations in standard DP-training methods. 1 Introduction Dee...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al Setup 5.1 Datasets We use publicly available datasets for information retrieval tasks. For finetuning and evaluation, we consider the MSMARCO dataset (Bajaj et al., 2016), which consists of nearly 533,000 querydocument pairs of search data sampled from Bing search logs covering a broad range of domains and concepts. Additionally, we consider datasets in the BEIR benchmark suite (Thakur et al., 2021), which contains information retrieval datasets across a variety of domains, for zero-shot eval...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Our work focused on ensuring DP guarantees in training deep retrieval systems. We discussed the limitations of DP-training such systems with the often used in-batch softmax loss function, which is non-per-example decomposable. We introduce an approach of using DP LMs to generate private synthetic queries for downstream deep retrieval training. This approach ensures theoretical guarantees on query-level privacy prior to downstream training, thereby bypassing some of the limitations of DP-training...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.06474v1.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have shown an uncanny ability to handle a wide variety of tasks such as text generation [1, 2, 6, 26], translation [36, 42], and summarization [19]. The recent fine-tuning of LLMs on conversations and the use of techniques like instruction fine-tuning [4] and reinforcement learning from human feedback (RLHF) [3] led to *The two authors contributed equally to this work. Authors’ addresses: Wang-Cheng Kang\", wckang@google.com, Google Research, Brain Team, USA; Jianmo N...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Use of Natural Language in Recommender System One of the earliest works that explored formulating the recommendation problem as a natural language task is [44]. They used BERT [6] and GPT-2 [25] on the Movielens dataset [12] to show that such language models perform surprisingly well, though not as good as well tuned baselines like GRU4Rec [15]. P5 [8] fine-tunes a popular open-sourced T5 [27] model, unifying both ranking, retrieval and other tasks like summary explanation into one model. M6...\n",
      "\n",
      "--- METHOD ---\n",
      "for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user’s rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging fr...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s that fine-tune LLMs on human interaction data, we demonstrate that fine-tuned LLMs can achieve comparable or even better performance than traditional models with only a small fraction of the training data, showing its promise in data efficiency. 2 RELATED WORK 2.1 Use of Natural Language in Recommender System One of the earliest works that explored formulating the recommendation problem as a natural language task is [44]. They used BERT [6] and GPT-2 [25] on the Movielens dataset [12] to show ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we evaluate the effectiveness of large language models as a recommendation system for user rating prediction in three settings: 1. zero-shot; 2. few-shot; and 3. fine-tuning. Compared to traditional recommender methods, our results revealed that LLMs in zero-shot and few-shot LLMs fall behind fully supervised methods, implying the importance of incorporating the target dataset distribution into LLMs. On the other hand, fine-tuned LLMs can largely close the gap with carefully desig...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.09668v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, su...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, the literature has seen a series of remarkable Deep Learning (DL) success stories (3), with breakthroughs particularly in the fields of Natural Language Processing and Computer Vision (2}|25}(36}[37). Albeit different in modalities, these results share a common structure: large neural networks, often Transformers (46), trained on enormous web-scale datasets using self-supervised learning methods (19}[6). While simple in structure, this recipe led to the development of surprising...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Over the past few years, scaling the parameter count of models and the size and diversity of training datasets led to unprecedented capabilities in (Vision) Language Models {19} {8). This in turn led to several applications leveraging these models within agents that interact with the world. Prior work has used LLMs and VLMs together with RL agents in simulated environments (12}{44), but they rely on collecting large amounts of demonstrations for training agents. Instead, we focus on the problem ...\n",
      "\n",
      "--- METHOD ---\n",
      "on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts. 1 INTRODUCTION In recent years, the literature has seen a series of remarkable Deep Learning (DL) success stories (3), with breakthroughs particular...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "how this framework can be used for data reuse and transfer, and learning from observation, besides exploration and skills scheduling, proposing a more unified approach to some core challenges in reinforcement learning. 3 PRELIMINARIES We use the simulated robotic environment from Lee et al. (26) modelled with the MuJoCo physics simulator for our experiments: a robot arm interacts with an environment composed of a red, --- --Reincarnating Reinforcement Learning Workshop at ICLRa blue and a green ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a framework that puts language at the core of an agent. Through a series of experiments, we demonstrate how this framework, by leveraging the knowledge and capabilities of Foundation Models, can provide a more unified approach with respect to the current literature to tackle a series of core RL challenges, that would normally require separate algorithms and models: 1) exploring in sparse-reward tasks 2) reusing experience data to bootstrap learning of new skills 3) scheduling learned ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.05767v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text p...\n",
      "\n",
      "--- METHOD ---\n",
      "s have to undergo additional fine-tuning to use their representations on a given downstream task. Zero-Shot models can be applied to any task directly achieving flexibility and generalization. One of the most successful type are Contrastive Language-Audio Pretraining (CLAP) models that jointly learn multimodal text and audio * Equal Contribution https://github.com/microsoft/CLAP representations. Authors in introduced a CLAP model that achieved state of the art (SoTA) in 16 downstream tasks. Subs...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "S Training Datasets. Collecting pairs is perhaps the main bottleneck of scaling up CLAP models. We gathered the largest collection with 4.6 million audio and text pairs from different datasets and web archives. The audios describe human sounds and activities, environmental sounds, acoustic scenes, music, sound effects, and speech emotion. To study the effect of encoders in Table we used the same training sets as CLAP (By. Unlike the authors, we did not include the test set of AudioCaps and Cloth...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "is that scaling up the number of training pairs improves overall performance. However, simply adding pairs may result in a drop of performance in certain domains and tasks [3] [6]. CLAP’s performance is dependent on the diversity of the text and audio training pairs and how noisy they are. Wav2clip [8] and Audioclip [9] used 200k and 1.7M audio-text pairs respectively from AudioSet, a dataset annotated for sound events. Authors paired audio with class labels rather than with sentence-level descr...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.00378v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0730-0301/2023/8-ART1 $15.https://doi.org/10.1145/the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a mult...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The generation of natural, varied, and detailed motions is a core problem in computer animation. Acquiring large volumes of motion data via a motion capture (mocap) system or manually authoring sophisticated animations is known to be costly and tedious. As such, motion datasets are generally limited, especially in terms of ACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023. --- --1:2. + Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen the diversi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "on kinematics-based motion synthesis. We also briefly cover recent advancements in image synthesis, particularly patch-based ones, from which we take inspiration. Motion Synthesis. Generating novel motions via diversifying existing ones can date back to the work of Perlin and Goldberg [1996], where the Perlin noise [Perlin 1985] is added to motion clips for obtaining variants with local diversity. Pullen and Bregler [2002] show that mocap data can be used to enhance a coarse key-framed motion, b...\n",
      "\n",
      "--- METHOD ---\n",
      "s. Given a single or few examples, even with a highly complex skeletal structure (middle), our framework can (a) synthesize a high-quality novel motion, within a fraction of a second; (b) complete a partial motion (lower-body motion) with example motion patches; (c) synthesize a coherent sequence guided by a sparse set of keyframes (in blue clothes); (d) generate an infinitely looping animation that starts and ends with a specified pose (in blue clothes). We present GenMM, a generative model tha...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "S We evaluate the effectiveness of our method on example-based motion synthesis, compare to other motion generation techniques, and ACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023. --- --1:6 + Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen RANA SO Example sequence MotionTexture [2002] PLE yA | | in L JX aN GANimator [2022] Fig. 6. Visual comparisons. MotionTexture [2002] generates motions with unnatural transitions. acRNN [2018] produces noi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We presented a generative framework for synthesizing diverse motion sequences from only a small set of examples. We achieve this via injecting generative capabilities into the industry state-of-theart technique for character animation — motion matching. As a result, our framework inherits the training-free nature and superior quality, and is able to produce a high-quality sample within just a fraction of a second, even with highly complex and large skeletons. We demonstrate the utility of our fr...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.01754v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulneraility detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build efore attempting detection. This, unfortunately, introduces a long tency between the time a vulnerability is injected to the time it is removed, which can substantially in...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Despite development of many tools and best practices [3, 4, 12, 47], uncaught vulnerabilities persist in code[1, 2] and impact users and cost companies time and money[19]. In recent years, many approaches have been developed to detect vulnerabilities in software. These approaches range from traditional dynamic analysis [25, 28, 48], rule-based static analysis [9, 51], feature-based machine learning solutions [33], to more recent deep learning based solutions [8, 30, 41, 53]. Approaches that rely...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We discuss how our work builds upon and extends prior work in the area of Vulnerability Detection, Deep Learning...\n",
      "\n",
      "--- METHOD ---\n",
      "s, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build efore attempting detection. This, unfortunately, introduces a long tency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine earning can be used to detect vulnerable code patterns on synt...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s using our best performing model. The first experiment compares this model with existing vulnerability detection approaches on four established benchmark datasets. We show that, in comparison with existing approaches, our approach improves recall up to 10% and precision up to 8%. --- --The second experiment studies the generalizability of our approach beyond manual code edits by developers, where we investigate the effectiveness of our approach in detecting vulnerabilities in automated code edi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Code vulnerabilities continue to cost software companies and users. Detecting vulnerabilities in code at EditTime when the code is written by a developer or generated by a code LLM is essential to ensure the vulnerabilities are fixed at lower cost. Yet, the majority of current vulnerability detection tools do not detect vulnerabilities at EditTime. Our work closes this gap by presenting a vulnerability detection model that detects vulnerabilities on incomplete code snippets and therefore can be ...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.09329v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present DreamHuman, a method to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis mod...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The remarkable progress in Large Language Models [40, 8] has sparked considerable interest in generating a wide variety of media modalities from text. There has been significant progress in text-to-image [43, 44, 46, 61, 9, 28], text-to-speech [31, 35], text-to-music [2, 15] and text-to-3D [17, 37] generation, to name a few. Key to the success of some of the popular generative image methods conditioned on text has been diffusion models [46, 44, 49]. Recent works have shown these text-to-image mo...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "There is considerable work related to diffusion models [52] and their applications to image generation [13, 29, 10, 46, 44, 49, 48] or image editing [18, 47, 12, 26]. Our focus is on text-to-3D [17, 37, 41] and more specifically on realistic 3D human generation conditioned on text prompts. In the following subsections we revisit some of the relevant work related to our goals. Text-to-3D generation. CLIP-Forge [50] combines CLIP [39] text-image embeddings with a learned 3D shape prior to generate...\n",
      "\n",
      "--- METHOD ---\n",
      "to generate realistic animatable 3D human avatar models solely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than animated 3D human models, and anthropometric consistency for complex structures like people remains a challenge. DreamHuman connects large text-to-image synthesis models, neural radiance fields, and...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section we illustrate the effectiveness of our proposed method. We show how the individual proposed components help, and how we compare to recent state-of-the-art methods. Figure 2 shows a wide variety of generated 3d human models in different poses, so we can illustrate diverse body shapes, skin tones and body compositions. Due to space constraints, additional results are available in the Supplementary Material. 4.1 Ablation Study Semantic zoom. In Figure 4, we show the importance of ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We presented DreamHuman, a novel method for generating 3D human avatars from text. Our method leverages statistical 3D human body models and recent advances in 3D modelling and text-to-3D generation to create animatable 3D human avatars, without any paired text-to-3D supervision. We illustrated that our method can generate photorealistic results, with detailed geometry and outperforms the state of the art by a large margin. Limitations and future work. Since our model is trained without any 3D d...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05189v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Diffusion models, which have emerged to become popular text-toimage generation models, can produce high-quality and contentrich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameterefficient fine-tuning approach called the Semantic Unde...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, diffusion model based multimodal text-to-image generation techniques have made impressive strides [50]. With these models [38, 46] trained on massive amounts of data and model parameters, people are able to generate text-relevant and visually appealing images end-to-end through text prompts and other information, without requiring complex painting skills. However, the --- --Preprint, Technical Report, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin Color ‘A ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "S 2.1 Text-to-Image Diffusion Diffusion models have been extensively utilized in text-to-image generation [2, 11, 23, 25, 38, 41, 46]. Text-to-image diffusion utilizes textual input as a conditioning signal for diffusion models, generating text-related images via a process of noise addition and removal [38]. The text encoder of text-to-image diffusion is often accomplished by leveraging pre-trained language models such as CLIP [34] to encode textual inputs into latent vectors. Text-to-image diff...\n",
      "\n",
      "--- METHOD ---\n",
      "ologies — Natural language processing; Computer vision; Machine learning algorithms. KEYWORDS diffusion model, large language model, multimodal image generation, adapter, knowledge distillation 1 INTRODUCTION In recent years, diffusion model based multimodal text-to-image generation techniques have made impressive strides [50]. With these models [38, 46] trained on massive amounts of data and model parameters, people are able to generate text-relevant and visually appealing images end-to-end thr...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s by integrating multiple LLMs and popular pre-trained diffusion “Both authors contributed equally to this research. * Corresponding author. models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make textto-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-frien...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s: (a) the use of Softmax to obtain a relative score in CLIP can render the CLIP Score unreliable, particularly when both the baselines and SUR-adapter yield equally poor results. For instance, ControlNet (seg) attains a relatively high score despite its subpar generation effects on Action and Color. (b) ControlNet performs well in Counting scores since it utilizes image outlines with the correct amount of information as a reference. (c) Inaccurate image segmentation can cause diffusion models w...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16867v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM’s cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-lik...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) are deep learning models with billions of parameters trained on huge corpora of text BOIS}. While they can generate text that human evaluators struggle to distinguish from text written by other humans [Brown et al.| 2020}, they have also shown other, emerging abilities (Wei et al.| 2022al . They can, for example, solve analogical reasoning tasks , program web applications 2021}, or use tools to solve multiple tasks [Bubeck et al.| . Because of these abilities and the...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "As algorithms become increasingly more able and their decisions impenetrable, the behavioral sciences offer new tools to make inferences just from behavioral observations [Rahwan et al.| Schulz and Dayan} |2020]. Behavioral tasks have, therefore, been used in several benchmarks Bommasani et al.||2021||Kojima et al.|/2022]. Whether and how algorithms can make inferences about other agents, machines and otherwise, is one stream of research that borrows heavily from the behavioral sciences 2018| {C...\n",
      "\n",
      "--- METHOD ---\n",
      "s will continue to be useful for elucidating the many facets of LLM cognition, particularly as these models become more complex, multi-modal, and embedded in physical systems. Acknowledgements This work was supported by the Max Planck Society, the German Federal Ministry of Education and Research (BMBF): Tiibingen AI Center, FKZ: 011S18039A, and funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy-EXC2064/1-390727645. We thank the In...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.04268v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Existing Neural Radiance Fields (NeRF) methods suffer from the existence of reflective objects, often resulting in blurry or distorted rendering. Instead of calculating a single radiance field, we propose a multi-space neural radiance field (MS-NeRF) that represents the scene using a group of feature fields in parallel sub-spaces, which leads to a better understanding of the neural network toward the existence of reflective and refractive objects. Our multi-space scheme works as an enhancement t...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural Radiance Fields (NeRF) [25] and its variants are refreshing the community of neural rendering, and the potential for more promising applications is still under exploration. NeRF represents scenes as continuous radiance fields stored by simple Multi-layer Perceptrons (MLPs) and renders novel views by integrating the densities and radiance, which are queried from the MLPs by points sampled along the ray from the camera to the image plane. Since its first presentation [25], many efforts have...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Coordinate-based novel view synthesis. NeRF [25] has bridged the gap between computer vision and computer graphics, and reveals a promising way to render highquality photorealistic scenes with only posed images. The insights and the generalization ability of this scheme also facilitate various tasks both in CV and CG, i.e., 3D reconstruction [28,40], 3D-aware generation [4, 15,27], 3D-aware edition [39,47], and avatar reconstruction and manipulation [9, 18,52]. Besides, researchers have made g...\n",
      "\n",
      "--- METHOD ---\n",
      "s suffer from the existence of reflective objects, often resulting in blurry or distorted rendering. Instead of calculating a single radiance field, we propose a multi-space neural radiance field (MS-NeRF) that represents the scene using a group of feature fields in parallel sub-spaces, which leads to a better understanding of the neural network toward the existence of reflective and refractive objects. Our multi-space scheme works as an enhancement to existing NeRF methods, with only small comp...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that our approach significantly outperforms the existing single-space NeRF methods for rendering high-quality scenes concerned with complex light paths through mirror-like objects. Our code and dataset will be publicly available at https://zx-yin.github.io/msnerf. 1. Introduction Neural Radiance Fields (NeRF) [25] and its variants are refreshing the community of neural rendering, and the potential for more promising applications is still under exploration. NeRF represents scenes as contin...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we tackle the long-standing problem of rendering reflective surfaces in NeRF-based methods. We introduce a multi-space NeRF method that decomposes the Euclidean space into multiple virtual sub-spaces. Our proposed MS-NeRF approach achieves significantly better results compared with conventional NeRF-based methods. Moreover, a light-weighted design of the MS module allows our approach to serve as an enhancement to the conventional NeRF-backbone networks. We also constructed a novel...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.08848v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) such as GPT3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with largescale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of supervised examples due to context length limits. In this paper, we propose Super In-Context Learning (SuperICL) which allows bla...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large-scale pre-trained language models, such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023), have demonstrated remarkable capabilities in a wide range of NLP tasks. Despite the impressive performance of these recently released models, their size and limited accessibility of model weights can lead to difficulties in fine-tuning these models with supervised data, which is an effective way to adapt the models to specific tasks (Liu et al., 2019). An alternative approach, In-Context Learnin...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "In-Context Learning Originally proposed in the GPT-3 paper (Brown et al., 2020), In-Context Learning (ICL) is considered as a new paradigm that exploits LLMs on new tasks without updating the parameters of the model. It prepends few-shot training examples before the test input as a prompt, to enable large language models to find patterns and “learn” to predict. There have been successful applications of ICL in downstream tasks, such as Machine Translation (Lin et al., 2021; Agrawal et al., 2022)...\n",
      "\n",
      "--- METHOD ---\n",
      "s MNLI-m MNLI-mm_ SST-2. QNLI MRPC QQP CoLA RTE _ Avg. GPT-3.5 ICL 80.80 82.39 91.39 80.52 60.05 81.64 60.51 86.28 81.RoBERTa-Large 88.68 89.47 96.44 94.07 83.09 92.11 64.55 87.00 88.SuperICL 89.31 89.61 96.79 94.16 86.03 92.14 64.57 87.73 89.Table 2:...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning. Furthermore, SuperICL can enhance the capabilities of smaller models, such as multilinguality and interpretability.! 1 Introduction Large-scale pre-trained language models, such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023), have demonstrated remarkable capabilities in a wide range of NLP tasks. Despite the impressive performance ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "is contradicted by a later study (Yoo et al., 2022). Additionally, prior studies suggest ICL could be implicitly performing Bayesian inference (Xie et al., 2022) or gradient descent (Akyiirek et al., 2022; von Oswald et al., 2022; Dai et al., 2022). Language Model Plug-ins Large language models can exploit external tools to improve their capabilities. Toolformer (Schick et al., 2023) introduces special symbols that allow the large language models to call external APIs to complete tasks. Visual C...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.08674v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present TableGPT, a large language model designed for table analysis, unifying tables, nature language, and commands. It enables a variety of functions like answering questions, manipulating data, visualizing information, generating analysis reports, and making predictions. Technically, TableGPT addresses several major challenges in developing a natural language-driven framework for table data processing, including comprehensive table understanding, instruction chain generation, and domain-sp...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.02628v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, they incur high computation cost and latency resulting from the autoregressive token-by-token generation. To address this issue, several approaches have been proposed to reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Autoregressive large language models (LLMs), such as the GPT and OPT family, have demonstrated strong performance across a wide range of tasks [15] [16] [I]. However, thee also have high computational cost and latency requirements resulting from token-by-token generation. Token-level early exit [17] [18] has emerged as a promising technique to alleviate these limitations by allowing tokens to cease computation as soon as their hidden states reach saturation [17]. Although current methodologies e...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Model compression: There has been extensive research in model compression to develop techniques to improve the inference efficiency of large language models (LLMs). One of the most prominent lines of work leverage knowledge distillation (KD) [7] to train smaller student models with faster inference using representations from LLMs as teachers like hidden states and attention states Another line of work in model compression use quantization [3], low-precision training and network parameter sharing...\n",
      "\n",
      "--- METHOD ---\n",
      "s show promising results for online inference, they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prior constraints by setting up a ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that SkipDecode can obtain 2x to 5x inference speedups with negligible regression across a variety of tasks. This is achieved using OPT models of 1.3 billion and 6.billion parameters, all the while being directly compatible with batching and KV caching optimization techniques. 1 Introduction Autoregressive large language models (LLMs), such as the GPT and OPT family, have demonstrated strong performance across a wide range of tasks [15] [16] [I]. However, thee also have high comp...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ", our method consistently demonstrates an ability to decrease computational demands across all datasets and model sizes, effectively determining the hidden state saturation point. The impact on task performance, as measured by Bleu, Rouge-L, and Bert-F scores, varies depending on the specific dataset. However, in all instances, our method shows a favorable balance between speedup and task performance, reaching a 2x speedup with almost no degradation in all cases. This balance can be effectively ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.16582v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Stable diffusion, a generative model used in text-to-image synthesis, frequently encounters resolution-induced composition problems when generating images of varying sizes. This issue primarily stems from the model being trained on pairs of single-scale images and their corresponding text descriptions. Moreover, direct training on images of unlimited sizes is unfeasible, as it would require an immense number of text-image pairs and entail substantial computational expenses. To overcome these cha...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In text-to-image synthesis, Stable Diffusion (SD) (Rombach et al. 2022) has emerged as a significant advancement. Existing SD models (Ruiz et al. 2023; Meng et al. 2023) transform text aligned with image components into high-quality images, typically sized at 512 x 512 pixels. Despite these models having the ability to handle varying sizes, they noticeably struggle with resolution changes, resulting in poor composition (e.g., improper cropping and unnatural appearance), a problem demonstrated in...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Stable Diffusion. Building upon the foundations laid by the Latent Diffusion Model (LDM) (Rombach et al. 2022), diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021) have achieved substantial success across various domains, including text-to-image generation (Nichol et al. 2022; Ramesh et al. 2022; Saharia et al. 2022), image-toimage translation (Dhariwal and Nichol 2021; Nichol and Dhariwal 2021), and multi-modal generation (Ruan et al. 2023). Owing to their robust ability to capture c...\n",
      "\n",
      "--- METHOD ---\n",
      "allows for the rapid enlargement of the ASD output to any high-resolution size, avoiding seaming artifacts or memory overloads....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results on the LAION-COCO and MM-CelebA-HQ benchmarks demonstrate that ASD can produce well-structured images of arbitrary sizes, cutting down the inference time by 2 x compared to the traditional tiled algorithm. Introduction In text-to-image synthesis, Stable Diffusion (SD) (Rombach et al. 2022) has emerged as a significant advancement. Existing SD models (Ruiz et al. 2023; Meng et al. 2023) transform text aligned with image components into high-quality images, typically sized at 512 x 512 ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this study, we address the challenge of resolution-induced poor composition in creating high-fidelity images from any text prompt. We propose Any Size Diffusion (ASD), a method consisting of ARAD and FSTD. Trained with multiaspect ratio images, ARAD generates well-composed im --- --ages within specific sizes. FSTD, utilizing implicit overlap in tiled sampling, enlarges previous-stage output to any size, reducing GPU memory consumption. Our ASD is validated both quantitatively and qualitativel...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07749v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Video matting has broad applications, from adding interesting effects to casually captured movies to assisting video production professionals. Matting with associated effects such as shadows and reflections has also attracted increasing research activity, and methods like Omnimatte have been proposed to separate dynamic foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated sce...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Video matting is the problem of separating a video into multiple layers with associated alpha mattes such that the layers are composited back to the original video. It has a wide variety of applications in video editing as it allows for substituting layers or processing them individually before compositing back, and thus has been studied well over decades. In typical applications like rotoscoping in video production and background blurring in online meetings, the goal is to obtain the masks cont...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Video Matting. There is a long line of work exploring video matting due to its importance in video editing. Green screening and rotoscoping are critical first steps in any visual effects pipeline. The matting problem aims to extract the foreground subjects into their own RGBA layers and separate them from the background RGB layer, which is a highly under-constrained problem. Many approaches have utilized motion and depth cues in addition to integrating user interactions [7, 3, 32, 16, 9]. Backgr...\n",
      "\n",
      "--- METHOD ---\n",
      "s like Omnimatte have been proposed to separate dynamic foreground objects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hindering application to real-world videos. In this paper, we propose a novel video matting method, OmnimatteRF, that combines dynamic 2D foreground layers and a 3D background model. The 2D layers preserve the details of the subjects, while the 3D backgrou...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that our method reconstructs scenes with better quality on various videos. 1. Introduction Video matting is the problem of separating a video into multiple layers with associated alpha mattes such that the layers are composited back to the original video. It has a wide variety of applications in video editing as it allows for substituting layers or processing them individually before compositing back, and thus has been studied well over decades. In typical applications like rotosco...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a method to obtain omnimattes, RGBA layers that include objects and their associated effects by com --- --Background Foreground D?NeRF Omnimatte ku-pillow rooster Ours D?NeRF Omnimatte Ours at atatatatatatat staat ata\" at stata\" ata\"a\"ata\"sh re Geer pesssuncesssscelbamcessamceasamsoramrnorsrad afetatahataterat wretateratararat aanaraneraena\" | potters bouldering car Figure 5. Qualitative comparison. We compare results of our and baseline methods on videos from each dataset. Readers ar...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.16890v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "— Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaptation policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model paramet...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Robots deployed in the real world will inevitably face many environmental changes. For example, robots’ internal conditions, such as battery levels and physical wear-and-tear, and external conditions, such as new terrain or obstacles, imply that the system’s dynamics are non-stationary. In these situations, a static controller that always maps the same state to the same action is rarely optimal. Robots must be capable of continuously adapting their control policy in response to the changing envi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Early demonstrations of Genetic Programming (GP) established its power to evolve optimal nonlinear control policies from scratch that were also simple and interpretable [5]. More recently, GP has been used to distill the behavior of complex neural network policies developed with Deep Reinforcement Learning into interpretable and explainable programs without sacrificing control quality [6]. In this work, we extend these...\n",
      "\n",
      "--- METHOD ---\n",
      "based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaptation policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evo...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "time in this task, we adopt the RegEvo search algorithm and optimize it for fast experimentation. Unlike NSGA-II, asynchronous parallel workers in RegEvo also perform selection, which eliminates the bottleneck of waiting for the entire population to be evaluated prior to ranking, selecting, and modifying individuals. Crossover and Mutation Operators: We use a simple crossover operator that swaps a randomly selected CADF between two parent algorithms. Since all CADFs have the same argument list a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "AND DISCUSSION We have shown that using ARZ to search simultaneously in program space and parameter space produces proficient, simple, and interpretable control algorithms that can perform zero-shot adaptation, rapidly changing their behavior to maintain near-optimal control in environments that undergo --- --radical change. In the remainder of this section, we briefly motivate and speculate about future work. CADFs and the Distraction Dilemma. In the quadruped robot domain, we have observed tha...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16843v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (ev...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Transformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017), image recognition (Dosovitskiy et al., 2021), and multi-task learning (Reed et al., 2022). However, recent work (Delétang et al., 2023) demonstrated that Transformers fail to generalize to longer sequences on seemingly simple tasks such as binary addition. Thus, while certain problems can be solved without length generaliza...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Our work is most closely related to the growing line of research on Transformers’ positional encodings. The first approaches simply added a transformation of the tokens’ positions, e.g., scaled sinusoids (Vaswani et al., 2017) or learned embeddings (Gehring et al., 2017), to the embeddings of the input sequence. Dai et al. (2019) subsequently showed that computing the attention (at every layer) using the relative distances between the key and query vectors improves the modeling of long-term (int...\n",
      "\n",
      "--- METHOD ---\n",
      "allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average). 1 Introduction Transformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017), image recognition (Dosovitskiy et al., 2021), and multi-task learning (Reed et al., 2022). However, recent work (Delétang et al., 2023) demonstrated that Transformers fail to generalize to longer se...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s in Section 4 will show, these approaches fail at length generalization on algorithmic reasoning tasks, which is precisely the goal of our work. A concurrent work developed randomized learned positional encodings (Li and McClelland, 2022), which are a special case of our family of randomized positional encodings. We also note that the necessity of feature and position randomization for length generalization has been discussed in the context of graph neural networks, which subsume Transformers (...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduced a novel family of positional encodings that significantly improves the length generalization capabilities of Transformers. Our positional encodings are based on the insight that conventional positional encodings will be out-ofdistribution when increasing the sequence length. Thus, to overcome this issue, we randomly sample our encodings from a wider range than the lengths seen at test time while keeping the order. Our largescale empirical evaluation demonstrates that our method sig...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08586v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Previous research observed accuracy degradation when replacing the attention softmax with a pointwise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence length which makes parallelization challenging [24, 7]. In this report we explore point-wise alternatives to the softmax operation which do not necessarily output a probability distribution. As a highlight, we observe that...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Previous research has explored substituting softmax with ReLU [25, 14] or squared ReLU [15]. However, these approaches do not divide by sequence length, which we experimentally find is important to reach accuracy comparable to softmax. In addition, previous research [21] has replaced softmax while still requiring normalization over the sequence length axis to ensure Fs) 0.82 7 — softmax a — relu/seqien g yoo] © s/32 8 0.cs m sf6 bea a © BB2 § eo” B/l6 5° 8° 3 °° W0-76 5 0.~30.742 0.o £ 20.72£ G ...\n",
      "\n",
      "--- METHOD ---\n",
      "Attention. Attention transforms d-dimensional queries, keys, and values {qi,ki,v;}#_, with a two step procedure. First, attention weights a,; are produced viaaiy=o (= [aha kx] ; j 1Concretely, with linear attention, the order of matrix multiplies can be switched from (qk )u to q(k' v) which changes the compute required from O(dL?) to O(d?L) where q,k,v € REX¢ are the queries, keys, and values and L is sequence length. (1) where @ is typically softmax. Next, the attention . L weights are used to ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1 Introduction The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence le...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This report leaves many open questions. In particular, we are unsure why the factor L~! improves performance or if this term could be learned. Moreover, it is likely that there is a better activation function that we do not explore. Acknowledgements We thank Lucas Beyer, Mostafa Dehghani, and David Fleet for their helpful comments and suggestions. We thank the members of the Google DeepMind PAGI team for their support of this effort, Jascha Sohldickstein, Noah Fiedel, Aaron Parisi, Abhishek Kuma...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10320v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The core of Multi-view Stereo(MVS) is the matching process among reference and source pixels. Cost aggregation plays a significant role in this process, while previous methods focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing comput...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Given a series of calibrated images from different views in one scene, Multi-view Stereo (MVS) aims to recover the 3D information of the observed scene. It is a fundamental problem in computer vision and widely applied to robot navigation, autonomous driving, augmented reality, and etc. Recent learning-based MVS networks [Yao ef al., 2018; Gu et al., 2020; Wang et al., 2021b] have achieved inspiring success both in the quality and the efficiency of 3D reconstruction. Generally, deep MVS approach...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Learning-based MVS...\n",
      "\n",
      "--- METHOD ---\n",
      "s focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s. 3.1 Preliminary In general, the proposed RDACT and RRT can be integrated with arbitrary cost volume of learning-based MVS networks. Based on the patch match architecture [Wang er al., 2021b], we further explore the issue of cost aggregation on cost volume. As shown in Figure 2, CostFormer based on PatchMatchNet [Wang et al., 2021b] extracts feature maps from multi-view images and performs initialization and propagation to warp the features maps in source views to reference view. Given a pixel...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we explore whether cost Transformer can improve the cost aggregation and propose a novel CostFormer with the cascade RDACT and RRT modules. The experimental results on DTU [Aanes et al., 2016] , Tanks & Temples [Knapitsch er al., 2017], ETH3D [Schéps et al., 2017], and --- --BlendedMVS [Yao et al., 2020] show that our method is competitive, efficient, and plug-and-play. Cost Transformer can be your need for better cost aggregation in multi-view stereo. References [Aanees et al., 20...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.09390v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Spoken semantic parsing (SSP) involves generating machinecomprehensible parses from input speech. Training robust models for existing application domains represented in training data or extending to new domains requires corresponding triplets of speechtranscript-semantic parse data, which is expensive to obtain. In this paper, we address this challenge by examining methods that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn...\n",
      "\n",
      "--- METHOD ---\n",
      "s that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated t...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We address the high cost of manually labeling speech-transcriptsemantic parse data for spoken semantic parsing by enabling models to use text-only data. JAT is preferred for unpaired text in existing domains for its efficiency and gain of 2.5 % EM over a paired data baseline while remaining within 0.1 % EM of the more computationally expensive TTS. For unpaired text in new domains, TTS outperforms JAT by 6 % absolute EM overall, with a gain of 30.6 % EM over a paired baseline. When text data can...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07027v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision transformers named EfficientViT. We find that the speed of existing transformer models is commonly bounded by memory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we d...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Vision Transformers (ViTs) have taken computer vision domain by storm due to their high model capabilities and superior performance [18, 44,69]. However, the constantly improved accuracy comes at the cost of increasing model sizes and computation overhead. For example, SwinV2 [43] uses 3.0B parameters, while V-MoE [62] taking 14.7B parameters, to achieve state-of-the-art performance on Ima *Work done when Xinyu was an intern of Microsoft Research. + ours a+ MobileNetv6) te + ShuffleNetv“4 A Ghos...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Efficient CNNs. With the demand of deploying CNNs on resource-constrained scenarios, efficient CNNs have been intensively studied in literature [23, 24, 26, 48, 63, 67, 87]. Xception [10] proposes an architecture built with depthwise separable convolutions. MobileNetV2 [63] builds an inverted residual structure which expands the input to a higher dimension. MobileNetV3 [26] and EfficientNet [67] resort to neural architecture search techniques to design compact models. To boost the actual speed o...\n",
      "\n",
      "--- METHOD ---\n",
      "s aim to reduce model parameters or Flops, which are indirect metrics for speed and do not reflect the actual inference throughput of models. For example, MobileViT-XS [50] using 700M Flops runs much slower than DeiT-T [69] with 1,220M Flops on an Nvidia V100 GPU. Although these methods have achieved good performance with fewer Flops or parameters, many of them do not show significant wall-clock speedup against standard isomorphic or hierarchical transformers, e.g., DeiT [69] and Swin [44], and ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia VGPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-Machieves 1.8% superior accuracy, while running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX format. Code ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we have presented a systematic analysis on the factors that affect the inference speed of vision transformers, and proposed a new family of fast vision transformers with memory-efficient operations and cascaded group attention, named Efficient ViT. Extensive experiments have demonstrated the efficacy and high speed of EfficientViT, and also show its superiority on various downstream benchmarks. Limitations. One limitation of Efficient ViT is that, despite its high inference speed,...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.09782v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, w...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), demonstrating remarkable abilities such as emergence and grokking (Wei et al., 2022), pushing model size to become larger and larger. However, training these models with billions of parameters, such as those with 30B to 175B parameters, raises the bar for NLP research. Tuning LLMs often requires expensive GPU resources, such as 8 x 80GB devices, making it difficult for small labs and companies to participate in t...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "In this section, we present related work on memorysaving techniques during full parameter fine-tuning. These techniques can be effectively combined with LOMO to further reduce memory consumption. Activation Checkpointing During vanilla backpropagation, all activations from the forward pass are stored in memory to compute gradients. This can be a significant memory overhead, especially for large language models. Alternatively, one could discard all activations and recompute them on demand for gra...\n",
      "\n",
      "--- METHOD ---\n",
      "s (Ding et al., 2022), such as LoRA (Ht et al., 2022) and Prefix-tuning (Li and Liang, 2021), provide solutions for tuning LLMs with limited *Corresponding author. ‘Code and data are available at https: //github.com/ OpenLMLab/LOMO. resources. However, these methods do not offer a practical solution for full parameter finetuning, which has been acknowledged as a more powerful approach than parameter-efficient finetuning (Ding et al., 2022; Sun et al., 2023). In this work, we aim to explore techn...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "In this section, we evaluate our proposed method from three aspects, namely memory profile, throughput and downstream performance. If not further explained, all our experiments are conducted with LLaMA models (Touvron et al., 2023), ranging from 7B to 65B. --- --Gradients Gradients 12.3% Parameters 12.3% 8% Activations 73.7% Optimizer States Optimizer States (a) Training with AdamW 24.1% SSE) Activations 48.3% (b) Training with SGD Parameters 24.1% Parameters 86.1% »~ Gradients (c) Training with...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we introduce LOw-Memory Optimization (LOMO), a new optimizer designed to facilitate full parameter fine-tuning for large lan --- --89.lm LoRA LoRA+LOMO. 88.88.88.& ° 87.‘Accuracy (%) © g aS ° 86.87.mm LoRA LoRA+LOMO 86.86.86.3 86.3 86.a ° 85.‘Accuracy (%) a g faw ° 84.84.w/oloRA 1 2Lora attention dimension (a) BoolQ w/oloRA 1 2Lora attention dimension (b) MultiRC Figure 3: Results using LLaMA-13B on the BoolQ and MultiRC datasets (with 1,000 training examples). “LoRA+LOMO\" means i...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.17840v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "— There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) exhibit strong reasoning capabilities that are harnessed to perform a wide range of downstream tasks such as dialogue and code generation [13]. The robotics community has recently seen a significant interest in empowering robots with LLMs, enabling them to “Denotes equal contribution; Contribution by each author can be found in the appendix. understand natural language commands and perform tasks that require sophisticated reasoning [4-7]. However, existing methods ar...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Language Understanding for Robotics A common approach for language understanding for robotic agents involves symbol grounding [23], whereby phrases are mapped to symbols in the robot’s world model. Early work [24, 25] relies upon hand-engineered rules to perform this mapping. More recent...\n",
      "\n",
      "--- METHOD ---\n",
      "s (e.g., Codeas-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code here. I. INTRODUCTION Large language models (LLMs) exhibit strong reasoning capabilities that are harnessed to perform a wide range of downstream tasks such as dialogue and code generation [13]. The robotics community has recently seen a significant interest in empowering robots with LLMs, enabling them to “Deno...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s are generic and users of our framework will have minimal design workload. Our Statler framework is primarily inspired by classical --- --1 # Initial state 2 cups = [False, True, False] Swapping cup 1 with cupSwapping cup 0 with cupSwapping cup 1 with cup1 # Initial state 2 cups = [False, True, False] Swapping cup 1 with cupSwapping cup 0 with cupSwapping cup 1 with cup1 # Initial state 2 cups = [False, True, False] Swapping cup 1 with cupcups = [False, False, True] Swapping cup 0 with cup© cup...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We presented Statler, a language model that maintains an explicit representation of state to support longer-horizon robot reasoning tasks. Integral to Statler are a world-state reader that responds to a user query taking into account the current internal state, and a world-state writer that maintains the world state. Evaluations on various simulated and real robot manipulation tasks reveal that Statler significantly outperforms contemporary models on non-trivial tasks that require reasoning over...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.03280v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recently, Neural Radiance Fields (NeRF) has exhibited significant success in novel view synthesis, surface reconstruction, etc. However, since no physical reflection is considered in its rendering pipeline, NeRF mistakes the reflection in the mirror as a separate virtual scene, leading to the inaccurate reconstruction of the mirror and multi-view inconsistent reflections in the mirror. In this paper, we present a novel neural rendering framework, named MirrorNeRF, which is able to learn accurate...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "3D scene reconstruction and rendering is a long-standing problem in the fields of computer vision and graphics with broad applications in VR and AR. Although significant progress has been made over decades, it is still very challenging to reconstruct and re-render the scenes with mirrors, which exist ubiquitously in the real world. --- --MM °23, October 29-November 3, 2023, Ottawa, ON, Canada The \"appearance\" of the mirror is not multi-view consistent and changes considerably with the observer’s...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Neural Rendering The goal of neural rendering is to synthesize photorealistic images or videos by computing the light transport in a 3D scene. Lots of works [15, 21, 43] have been proposed to push the envelope of rendering quality in this field. One of the most notable approaches is NeRF [16], which models the radiance field of a scene using the MLP. By training on a set of posed images, NeRF learns to infer the radiance and density of each sampled point and accumulates them along the ray wi...\n",
      "\n",
      "--- METHOD ---\n",
      ". The code and supplementary material are available on the project webpage: https: //zju3dv.github. io/Mirror-NeRF/. CCS CONCEPTS + Computing methodologies + Computer vision; Rendering. KEYWORDS neural rendering; ray tracing; scene reconstruction; scene editing ACM Reference Format: Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng Zhang, Hujun Bao, and Zhaopeng Cui. 2023. Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing. In Proceedings of the 31st ACM ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and comparisons on both synthetic and real datasets demonstrate the superiority of our method. The code and supplementary material are available on the project webpage: https: //zju3dv.github. io/Mirror-NeRF/. CCS CONCEPTS + Computing methodologies + Computer vision; Rendering. KEYWORDS neural rendering; ray tracing; scene reconstruction; scene editing ACM Reference Format: Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng Zhang, Hujun Bao, and Zhaopeng Cui. 2023. Mirror-NeRF: Learning Neu...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have proposed a novel neural rendering framework following Whitted Ray Tracing, which synthesizes photo-realistic novel views in the scene with the mirror and learns the accurate geometry and reflection of the mirror. Besides, we support various scene manipulation applications with mirrors. As a limitation, our method does not explicitly estimate the location of the light source in the room, which prevents us from relighting the room. The refraction is also not modeled in our framework since ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16411v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent advancements in text-to-image generation have enabled significant progress in zero-shot 3D shape generation. This is achieved by score distillation, a methodology that uses pre-trained text-to-image diffusion models to optimize the parameters of a 3D neural presentation, e.g. Neural Radiance Field (NeRF). While showing promising results, existing methods are often not able to preserve the geometry of complex shapes, such as human bodies. To address this challenge, we present ZeroAvatar, a...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The ability to extract rich and accurate 3D information from a single image holds great importance in content creation, where realistic and immersive visual experiences are crucial. By automatically inferring the 3D structure and appearance of objects, artists and designers can efficiently generate lifelike virtual scenes, characters, and objects. Beyond the realms of content creation and AR/VR, the ability to perceive 3D geometry and appearance from a single image has broader implications, and ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 3D Generation from 2D Observations Inferring 3D geometry from multi-view data is a classical computer vision problem. Earlier approaches perform multi-view or multi-modal fusion, which involves combining information from multiple views or modalities to generate more accurate 3D representations. However, completeness of the observations is essential in this type of approach, which would require dense observations of the scene from various angles. In recent years, there have been significant b...\n",
      "\n",
      "--- METHOD ---\n",
      "ology that uses pre-trained text-to-image diffusion models to optimize the parameters of a 3D neural presentation, e.g. Neural Radiance Field (NeRF). While showing promising results, existing methods are often not able to preserve the geometry of complex shapes, such as human bodies. To address this challenge, we present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. Specifically, we first estimate and refine the parameters of a parametric huma...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s Implementation details. For efficiency, we adopt the multi-resolution hash encoding from InstantNGP [23], and follow several design choices from [26], such as view sampling and shading augmentation. We use DensePose for UV coordinate regression, and PIXIE for human mesh recovery. PIXIE outputs parameters of SMPL-X model, and we convert them into SMPL model parameters by finding the SMPL parameters that result in best mesh fit. NeRF rendering resolution during training is 100 by 100, and field ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we proposed ZeroAvatar, a zero-shot method for creating high-fidelity 3D avatars from a single image, using a pre-trained text-to-image diffusion model as a prior. ZeroAvatar significantly enhances the robustness and ensure 3D consistency of optimization-based image-to-3D avatar generation. On images of posed humans, ZeroAvatar surpasses existing zero-shot methods in terms of the optimized geometry and appearance. Further, ZeroAvatar can be seamlessly combined with a pre-trained te...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04009v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random walks over structured knowledge graphs. Specifically, we use soft prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random walk paths that lead to the ans...\n",
      "\n",
      "--- METHOD ---\n",
      "s on two TS LMs shows substantial improvements over standard tuning approaches in answering questions that require 2-hop reasoning. 1 Introduction Performing multi-hop reasoning to answer questions such as Where was David Beckham’s daughter born? requires two fundamental capacities: C1: possessing pre-requisite knowledge (David Beckham’s daughter is Harper Beckham, Harper Beckham was born in Los Angeles), and C2: ability to compose internalized knowledge. Contemporary pre-trained language models...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s suggest that integrating random walks in the TS models using our proposed techniques can substantially improve their ability to --- --Question: Where was the director of Violet Tendencies born? Relevant Knowledge (Violet Tendencies, director, Casper Andreas), (Casper Andreas, place of birth, Sweden) Knowledge Integration @ tuned ae frozen Harper Beckham Davis Beckhan ; daughter Random Walk Training 1 O HP) @® daughter place KNIT5 Beckham ; place of of bir birth ; Los Angeles Method 1: Parse-th...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We show that composition of memorized world knowledge can be triggered in LMs with up to 11B parameters (T5-XXL) to a desirable extent by leveraging training signal from random walks over structured knowledge using approaches based on prompt-tuning (Lester et al., 2021). Doing so leads to substantial improvements in the LMs’ ability to answer 2-hop questions, even beyond standard, full model fine-tuning. --- --Limitations Despite showing non-trivial improvements in the multi-hop capabilities of ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.18373v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLM...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "As advertisements play an integral role in human society, image ad understanding has many real-world applications such as ad targeting (Hussain et al., 2017), visual metaphor understanding (Abokhoza et al., 2019) and creative ad generation (Chilton et al., 2019; Akula et al., 2022). It is also highly challenging due to several reasons, as exemplified in Fig. 2. First, image ads consist of diverse visual elements including non-photorealistic objects and atypical scenes synthesized creatively that...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Image Ad Understanding Learning to automatically interpret image ads was proposed by the Pitt Image Ads Dataset (Hussain et al., 2017), where each ad is annotated by a caption that answers “what should I do according to the ad and why?” Different from traditional image captioning, this task is highly non-trivial as discussed at the beginning of Sec. 1. While prior...\n",
      "\n",
      "--- METHOD ---\n",
      "s utilize cultural connotations via external symbolic knowledge (Ye and Kovashka, 2018), capture relations between scene-texts and objects by GNNs (Dey et al., 2021), and leverage pre-trained language models to combine multimodel information (Kalra et al., 2020), none have exploited vision-language models (VLMs) and the knowledge of real-world entities (i.e., brands). Besides the wide applications in the ad industry, later work hints that the study of image ads is relevant to much broader resear...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s was pre-trained on a tremendous Source Evaluation Image A Similar Image Found in LAION Figure 6: An evaluation image and a found one in LAION-400M. As a reference, this image’s caption reads: I should drink “Brand Name” because it’ll give me a recharge of energy. amount (400M) of image-text pairs on the Internet. A concern is that there might be data leakage, ie., the pre-trained VLMs might have already seen images in the evaluation set, leading to inflated results. We perform an analysis to c...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we study the adaptation of pretrained alignment-based VLMs for the challenging image ad understanding task. We benchmark and reveal practical challenges in adapting VLMs, propose a simple and intuitive (yet effective) strategy for feature adaptations, and further improve image ad understanding with external brand knowledge. While we mainly focus on the image-to-text retrieval task for its simplicity, we believe further studies can extend it to directly generating text descriptions...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09148v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and tokenlevel alignment. To ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Cross-lingual sentence embedding encodes multilingual texts into a single unified vector space for a variety of Natural Language Processing (NLP) tasks, including cross-lingual sentence retrieval (Artetxe and Schwenk, 2019b) and crosslingual natural language inference (Conneau et al., 2018). The text sequences can be efficiently retrieved and compared using the inner product between their dense representation. The task of sentence embedding now heavily depends on pre-trained language models (Dev...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Cross-lingual Pre-training Following the success of BERT for English (Devlin et al., 2019), multilingual BERT comes out by building a shared multilingual vocabulary and training on multiple monolingual corpora with the masked language modeling (MLM) objective. XLM (Conneau and Lample, 2019) proposes a translation language modeling (TLM) task which is the extension of MLM to bitext corpora, so that the model can learn the cross-lingual alignment from translation pairs. Unicoder (Huang et al.,...\n",
      "\n",
      "--- METHOD ---\n",
      "s for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and tokenlevel alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contex...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on three sentence-level cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https://github.com/ ChillingDream/DAP. 1 Introduction Cross-lingual sentence embedding encodes multilingual texts into a single unified vector space for a variety of Natural Language Processing (NLP) tasks, including cross-lingual sentence retrieval (Artetxe and Schwenk, 2019b) and crosslingual natural language inference (Conneau et al., 2018). T...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we find that token-level alignment is crucial for cross-lingual tasks. Based on this observation, we present a dual-alignment pre-training framework for cross-lingual sentence embedding that enables both sentence-level and token-level alignment. The framework consists of a translation ranking task and a newly proposed representation translation learning task, which encourages the token representation to contain all information from its translation counterpart in an efficient way. ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.03726v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstretGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model’s upstream interleaved format pretraining dataset. We ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "& Motivation Large language models (LLMs) have demonstrated significant universal capabilities in performing various tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data and have been showcased in recent research, such as GPT-2 [25] and GPT-3 [6]. Recent studies have highlighted the importance of instruction tuning in empowering LLMs, as exemplified by the boosting of GPT-3 [6] to InstrctGPT [22] and ChatGPT [20], which follows natural language instructions ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Large-scale Multi-modal Models With the recent success of ChatGPT [20], GPT-4 [19], and other large language models [33, 32, 9], recent studies start to explore incorporating information from other modalities based on pretrained language models. These studies extend the capabilities of language models to more tasks and modalities, and can be categorized into two perspectives: System Design Perspective. This perspective involves using ChatGPT [20] as a dispatch scheduler and connecting differ...\n",
      "\n",
      "--- METHOD ---\n",
      "In this section, we will introduce the details of the MIMIC-IT dataset in Sec. 3.1, our Otter’s training details in Sec. 3.2, and the integration with Hugging Fance ecosystem in Sec. 3.3. 3.1 Multi-Modal In-Context Instruction Tuning The OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in its few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs derived from individual HTML files, with significant contextual relationships ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s are demonstrated in Fig. 3, where we observe that Otter is able to provide more detailed descriptions of images and follow user instructions more accurately. This characteristic of Otter is attributed to the co-design of our model and data, which leverages the generalization ability of a strong language decoder and the rich variety of instructions present in the MIMIC-IT dataset. By fine-tuning on visual instruction pairs, Otter is able to learn the nuances of human language and accurately app...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial finetuning on MIMIC-IT dataset, we observe that Otter can convert OpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities. With the assistance of rich instructions from images and videos, Otter generalizes to achieve better instruction-following and situation-understanding performances. 5.1 Limitations Language Hallucination. Since O...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02783v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-la...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In the recent years, Large Language models (LLMs) have demonstrated considerable content-generation capabilities in multiple domains, including natural language, vision, video and audio processing [1]. More recently, LLMs have been applied to the Software Engineering field, with the objective of improving aspects such as programmer’s productivity [2] and software security [3]. In the space of general-purpose programming languages, a growing amount of research is exploiting the capabilities of la...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "A. Pre-trained Language Models for Code Most recently, language models have fueled progress towards the longstanding challenge of source code synthesis [6], [7], which excel at downstream tasks such as code completion, code generation and code summarization. According to Xu et al. [8], pre-training...\n",
      "\n",
      "--- METHOD ---\n",
      "s for source code modeling fall into three categories: (i) The first category is based on left-to-right language models, namely, autoregressive decoder-based models. These models predict the probability of a token given the previous tokens. For example, CodeGPT [9], CodeParrot [10], Codex [6], AlphaCode [11] and CodeGen [12] all follow into this line, which are highly useful for code generation and completion tasks. (ii) The second category is based on masked language models, which can make use ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ally validated that this re-formalization can largely improve the overall performance regarding all kinds of metrics. The results are provided in the following section. 4) Training: We fine-tuned pre-trained models using our Galaxy dataset for 8 epochs. The effective batch size wasand the learning rate was 5 x 10~° with a cosine decreasing schedule. We used the BLEU score on the validation set to determine the best checkpoint. E. Demo/Plugin We expose a GRPC and REST API based interface to model...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This work describes the application of transformer-based models to the generation of Ansible-YAML, starting from a user-provided natural language prompt. The objective of this model is to build an AI assistant for Ansible users and improve their productivity. We provide a formal definition of the problem and we start from an existing pre-trained decoder-based model. We built a new training dataset with Ansible data for code generation that will be shared with the community. We extend the trainin...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.10168v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers’ abilities in these “human computation algorithms,” but the level of success is variable and influenced by requesters’ understanding of LLM capabilities, th...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The rapid advancement of AI systems has revolutionized our understanding of the capabilities of machines. One remarkable breakthrough in this field is the emergence of Large Language Models (LLMs, e.g., ChatGPT). With a combination of extensive pre-training (Brown et al., 2020) and * This work builds upon an assignment from CMU 05499/899: Human-Centered NLP. Tongshuang Wu is the course instructor, who designed the assignment based on discussions with Haiyi Zhu, and wrote the majority of the pape...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Crowdsourcing helps solve problems that require human inputs, at scale (Howe et al., 2006). Particularly in earlier times when AI capabilities were --- --limited, crowdsourcing was seen as a promising approach to leverage and enhance the unique computational powers possessed by humans. A key focus of crowdsourcing research has been the development of pipelines to tackle increasingly complex crowdsourcing goals (Kittur et al., 2011). Through careful task decomposition, crowdsourcing pipelines str...\n",
      "\n",
      "--- METHOD ---\n",
      "has been widely used to scale crowdsourcing usability, allowing it to handle tasks that are too challenging for individual crowdworkers with limited level of commitment and unknown expertise (e.g., summarizing lengthy novels, software development, or deciphering heavily blurred text; Kittur et al., 2011). Interestingly, research on LLMs has also explored scaling their capabilities for more complex tasks through chaining. Though named differently, LLM chains and crowdsourcing pipelines share simi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and after reviewing the paper draft. Crowd pipeline LLM Chains ? L2222-BESee Figure 1: We study whether LLMs can be used to replicate crowdsourcing pipelines and replace human workers in certain advanced “human-computational process.” instruction tuning (Stiennon et al., 2020; Wu et al., 2023; Ziegler et al., 2019), LLMs now not only possess a large amount of world knowledge, but can effectively leverage this knowledge to accomplish various tasks simply by following instructions. Various studi...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we study whether LLMs can be used to replicate crowdsourcing pipelines through a course assignment. We show that the modern models can indeed be used to simulate human annotation in these advanced “human computation algorithms,” but the success and effectiveness of replication varies widely depending on the nature of subtasks. Further, LLMs’ performance and modes of failure can be unintuitive, and they lack the ability to take advantage of multimodal cues that enable human workers ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.11497v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a “free lunch” that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on ...\n",
      "\n",
      "--- METHOD ---\n",
      "that substantially improves diffusion model sample quality at no costs: no training, no additional parameter introduced, and no increase in memory or sampling time. Abstract In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a “free lunch” that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to deno...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al evaluation of our approach, employing Stable Diffusion [29], DreamBooth [30], ReVersion [15], ModelScope [23], and Rerender [39] as our foundational models for benchmark comparisons. By employing FreeU during the inference phase, these models indicate a discernible enhancement in the quality of generated outputs. The visualization illustrated in Fig. 1 substantiates the efficacy of FreeU in significantly enhancing both intricate details and overall visual fidelity within the generated images....\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this study, we introduce the elegantly simple yet highly effective approach, termed FreeU, which substantially enhances the sample quality of diffusion models without in-curring any additional computational costs. Motivated by the fundamental role played by both skip connections and backbone features in U-Net architectures, we conduct an in-depth analysis of their effects in diffusion U-Net. Our investigation reveals that the primary backbone primarily contributes to denoising, while the skip...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.05884v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications. However, existing conversational and role-playing datasets often fail to capture the diverse and nuanced interactions typically exhibited by real-world role-play participants. To address this limitation and contribute to the rapidly growing field, we introduce a partially-synthetic dataset named PIPPA (Personal Interactio...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, the field of natural language processing has experienced a significant transformation, primarily driven by the remarkable advancements in large language models (LLMs). These models, fueled by extensive pre-training data and computational resources, exhibit an extraordinary ability to comprehend and generate human-like text. In order to harness their full potential and tailor them to specific domains, a set of high quality domain-specific samples are typically required during the...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.07395v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Automatic speech recognition (ASR) has long been an integral part of important technologies, including voice dictation, digital assistants, and video captioning [1]. While ASR systems are typically evaluated based on word error rate (WER), this is not the only metric of concern in production applications; several “auxiliary tasks” must be integrated with the ASR task ina full system. These tasks may include: capitalization and punctuation, which improves readability; voice activity detection (VA...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "While auxiliary tasks are usually performed by separate models from ASR [20, 21], E2E approaches to auxiliary task modeling have been recently popular for production-grade systems. Joint training of ASR with endpointing [7], capitalization [9, 22], intended query detection [23, 24], sentence segmentation [25], and more, have been explored. Our work builds most closely on Wang et al.[9], who co-train ASR, capitalization, and turntaking prediction by building multiple parallel label sequences. To ...\n",
      "\n",
      "--- METHOD ---\n",
      "boosts capitalization performance for long-tail data, and improves turn-taking detection recall. Index Terms: speech recognition, text injection, auxiliary tasks 1. Introduction Automatic speech recognition (ASR) has long been an integral part of important technologies, including voice dictation, digital assistants, and video captioning [1]. While ASR systems are typically evaluated based on word error rate (WER), this is not the only metric of concern in production applications; several “auxili...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09761v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Neural radiance fields (NeRFs) are a class of implicit scene representations that model 3D environments from color images. NeRFs are expressive, and can model the complex and multi-scale geometry of real world environments, which potentially makes them a powerful tool for robotics applications. Modern NeRF training libraries can generate a photo-realistic NeRF from a static data set in just a few seconds, but are designed for offline use and require a slow pose optimization pre-computation step...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural implicit scene representations offer an expressive and memory efficient alternative to traditional discrete scene representations like voxels or point clouds. One class of these implicit representations are Neural Radiance Fields (NeRF) which, in their most basic form, use a data set of color image and camera pose pairs to supervise the training of a neural network which in turn learns a continuous map of the environment captured in the data set’s images. The relative simplicity and flexi...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Early work with NeRF required training times of at least an hour, but often longer, to achieve a NeRF with sufficient quality to be used in down-stream robotics tasks B). However, the ground-breaking work in [4] demonstrated that, using a number of innovations and optimizations, NeRF training times could be reduced to just a few seconds. --- --Presented at the ICRA 2023 Workshop on Unconventional Spatial Representations The potential for NeRF as a spatial representation in a Simultaneous Localiz...\n",
      "\n",
      "--- METHOD ---\n",
      "s [5], [6], and offer variations on a similar idea, but none of them are particularly well suited for integration with existing robotics platforms because they lack existing open-source implementations with ROS. Furthermore, the existing code for these implementations lacks modularity, and restricts the user to the NeRF architectures and pose estimation methods selected by the authors. With NerfBridge, users are free to choose their NeRF architecture from the numerous methods already implemented...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      ". In Figure|3}is a rendering of the resulting NeRF. NerfBridge is able to capture the multi-scale structures of the building facade and windows. Fig. 3. The reconstruction of the NeRF generated using NeRF Bridge for outdoor mapping at the Elliot Center on Stanford University Campus. V....\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S AND FUTURE WORK The core objective of NerfBridge is to streamline the process for integrating neural implicit maps in robotics pipelines, and accelerate exploration of applications of NeRFs in robotics. To that end we designed a modular, ROS-based software package that can interface state-of-the-art NeRF training libraries with existing robotics platforms. In future work, we hope to combine NeRF navigation with online NeRF training as a novel modality for robot trajectory optimization and mapp...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.01499v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—Dynamic colored meshes (DCM) are widely used in various applications; however, these meshes may undergo different processes, such as compression or transmission, which can distort them and degrade their quality. To facilitate the development of objective metrics for DCMs and study the influence of typical distortions on their perception, we create the Tencent - dynamic colored mesh database (TDMD) containing eight reference DCM objects with six typical distortions. Using processed video sequenc...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "As a typical representation of three-dimensional (3D) graphics, 3D mesh plays a crucial role in various applications such as animated advertisement, digital entertainment, and education. These applications generally require high quality meshes to provide better quality of experience through improved immersion. Thus, accurately evaluating the influence of distortions on mesh quality is an essential task. Most of the current work on mesh quality assessment focuses on static meshes (21, and sometim...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "about mesh quality assessment. Section [3] details the construction of the TDMD database and analyzes the characteristics of distortion in human perception. Section [4] studies the performance of different types of objective metrics on the proposed database. Finally, section[5|concludes the paper and highlights future work. 2 RELATED WORK In this section, we summarize the state of the art of mesh quality assessment. 2.1 Subjective Mesh Quality Assessment Mesh subjective quality assessment has be...\n",
      "\n",
      "--- METHOD ---\n",
      "ology was adopted to collect scores. used a VR environment to score five mesh samples with four types of distortions. The color information of these meshes was stored per vertex (as opposed to using a texture map). In , subjective...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "that resulted in 303 distorted DCM samples with mean opinion scores, making the TDMD the largest available DCM database to our knowledge. This database enabled us to study the impact of different types of distortion on human perception and offer recommendations for DCM compression and related tasks. Additionally, we have evaluated three types of state-of-the-art objective metrics on the TDMD, including image-based, point-based, and video-based metrics, on the TDMD. Our experimental results highl...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s obtained on static meshes remain valid on DCM. Although investigates dynamic meshes with a 2D monitor viewing environment, the proposed samples are non-colored meshes, and color information tends to have a masking effect for artifacts on subjective perception The lack of studies on the responses of human perception to different DCM distortions is consequently the first motivation for providing in this paper a new DCM database. The main purpose of DCM objective assessment is to propose effectiv...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.03692v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models’ ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping crit...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large Language Models (LLMs) finetuned on instruct data can behave like conversational agents (Alpaca: Taori et al. [2023] Self-Instruct: Wang et al. |2023). The recipe for a chat model is welldefined: one needs to perform instruction tuning, which means supervised finetuning (SFT) of an LLM on tuples of instruction and response (Long pre et al.|2023). Open-source datasets vary in quality and quantity, ranging from 1k examples (Zhou et al. {2023) to over 800k examples (Anand et al.{2023). In add...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "The response tone alignment problem is a part of a broader intent alignment topic. In principle, LLMs are not aligned with users’ intents because their language modeling objective, e.g., predicting the next token of a training document, is different from the following instruction target. One successful approach for aligning both objectives is to prompt models using zero- or n-shot techniques, where the response would look like a completion of a document containing QA (Brown et al.[2020] Radford ...\n",
      "\n",
      "--- METHOD ---\n",
      "to encourage LMs to follow instructions is to add extra prompt suffixes or wrappers around instructions, which could disrupt the next token prediction task and produce responses. Figure [3] presents three versions of prompts: Instruction tuning prompts A Alpaca prompt B. Our prompt .No prompt Below isan instruction that {Instruction} {instruction} dserbes a task, White a response that appropriately completes the uy ‘Ht Response: {response} ‘#8 Inetruction: Ce) {instruction} ‘Ht Response: {respon...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      ". Let’s put all models behind a closed API (a recent equivalent of a black box). Is the model instructtuned or not? Knowledge benchmarks could be similar for vanilla and instruct models for LTD tuning. Skills tests would highly depend on the model size, which is not known. The simplest way of solving the riddle would be to ...chat with the model and judge the tone of the response. For a vanilla model, we expect a next prediction word attempt, whereas for instruct models, we expect them to follow...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s and future directions proposed in Section 6. 2 Background and Related Work The response tone alignment problem is a part of a broader intent alignment topic. In principle, LLMs are not aligned with users’ intents because their language modeling objective, e.g., predicting the next token of a training document, is different from the following instruction target. One successful approach for aligning both objectives is to prompt models using zero- or n-shot techniques, where the response would lo...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.03757v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Motion magnification helps us visualize subtle, imperceptible motion. However, prior methods only work for 2D videos captured with a fixed camera. We present a 3D motion magnification method that can magnify subtle motions from scenes captured by a moving camera, while supporting novel view rendering. We represent the scene with timevarying radiance fields and leverage the Eulerian principle for motion magnification to extract and amplify the variation of the embedding of a fixed point over time...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "We live in a big world of small motions. These motions, such as human respiration or object vibration, are hard to perceive with our naked eyes. Video processing techniques [29, 61, 56] have been developed to extract and magnify subtle motions captured in a 2D video to highlight and visualize those motions. These motion magnification techniques empower visual analytics tools like detecting the vibrations of buildings and measuring a person’s heart rate using only a video, without the need for ph...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Video motion magnification. Prior approaches to video magnification fall under two categories, inspired by fluid dynamics: Lagrangian [29] and Eulerian [61, 56, 57, 37, 65]. The Lagrangian perspective tracks individual pixels as fluid particles and estimates their motion vectors to warp pixels in the image. Lagrangian-based approach to motion magnification computes the optical flow explicitly and uses the estimated flow to magnify the motions of the pixels [29]. The performance, however, is limi...\n",
      "\n",
      "--- METHOD ---\n",
      "s only work for 2D videos captured with a fixed camera. We present a 3D motion magnification method that can magnify subtle motions from scenes captured by a moving camera, while supporting novel view rendering. We represent the scene with timevarying radiance fields and leverage the Eulerian principle for motion magnification to extract and amplify the variation of the embedding of a fixed point over time. We study and validate our proposed principle for 3D motion magnification using both impli...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that amplifying temporal variations in the feature embedding of each 3D point is highly effective in magnifying subtle 3D motion. We observe that magnifying the point embedding provides more accurate and robust magnified renderings than Eulerian magnification performed directly on rendered images. Using images captured during a time window when only subtle motion is visible, we train NeRF to reconstruct the 3D scene with such subtle temporal variations. We ensure that the ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present a 3D motion magnification method that applies Eulerain processing principles to analyzing NeRF embeddings over time. While classical magnification methods (developed originally for 2D videos) process pixel colors directly, we show that processing the point embeddings of NeRF successfully generalizes those approaches and allows magnifying motions in 3D renderings. We believe our work will motivate further research towards integrating traditional signal processing techniques into neural...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.12533v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "With ChatGPT as a representative, tons of companies have began to provide services based on large Transformers models. However, using such a service inevitably leak users’ prompts to the model provider. Previous studies have studied secure inference for Transformer models using secure multiparty computation (MPC), where model parameters and clients’ prompts are kept secret. Despite this, these frameworks are still limited in terms of model performance, efficiency, and deployment. To address thes...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Pre-trained Transformer models (Vaswani et al., (2017) have attracted much attentions for their high performance in practical tasks (Radford & Narasimhan, |2018; 2021) and been widely in Deep Learning as a Service (DLaaS) paradigm (Soifer et al 2019). However, these services can raise privacy concerns, such as in the case of ChatGPT (2020), which requires either users to reveal their private prompts to the service provider or the service provider to release their proprietary trained weights to u...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "in § 2Jand present the background in §B] We give PUMA’s high-level view and concrete design in §[4] We analyze the experimental results in § Bland conclude this work in §[6] 2 Related Work Secure Multiparty Computation (MPC) (Yao, {1986; {Goldreich et al], |1987) enables distrusted parties to jointly compute a function while keeping their inputs private, and secure deep learning inference using MPC has gained much attention due its high privacy protection. These works operate in a variety of mod...\n",
      "\n",
      "--- METHOD ---\n",
      "proposed by Ratheo et ail DODD) But these general methods are quite expensive in terms of computation and communication, and only tested under small bitwidth (e.g. below 32). Retraining required. To reduce the cost of non-linear functions, several works suggested to approximate GeLU and softmax using simpler functions like ReLU and quadratics. These functions are up to an order of magnitude cheaper in MPC, but would introduce utility loss to the Transformer model. As a result, they require an ex...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on 6 transformer models and 4 datasets, the results show that PUMA’s precision is similar to plaintext ones’ and is about 2 faster than MPCFORMER (note that MPCFORMER does not achieve similar precision as PUMA). PUMA can even evaluate LLaMA-7B in around 5 minutes to generate one word. To our best knowledge, this is the first time that such a large language model is able to be evaluated under MPC. ¢ End-to-End Framework compatible with plaintext. We design and implement all the layers required ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose an efficient MPC framework PUMA for secure inference on Transformer models based on replicated secret sharing. To reduce the costs of secure inference, we approximate expensive functions with accurate polynomials and propose secure Embedding and LayerNorm protocols to support end-to-end secure inference. Although the inference cost is still quite high, we successfully make it one step closer to solving users’ privacy concerns in Transformer-based DLaaS. We believe that by combining PU...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.08051v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The landscape of text-to-audio (TTA) generation has been revoluti ized by advancements in diffusion-based generative modelling Bi]. Leveraging powerful backbone models such as CLAP [I] and large language model (LLM) [{4], these models are capable of extracting semantic information and enabling the creation of high-fidelity audio from textual descriptions. In this work, we show that due to the scarcity and diversity of audio training data, bias appears in these state-of-the-art models, leading to...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s of audio generation and retrieval-based models, followed by the details of Re-AudioLDM in Section|3] Section [4] presents the experimental setting and Section [5] shows the results and ablation studies. Conclusions are given in Section|6] 2. RELATED WORK Our work relates to two major works, diffusion-based text-to-audio models and retrieval-based generative models. These two fields are briefly discussed in the following subsections. 2.1. Audio Generation Recent works on audio generation follow...\n",
      "\n",
      "--- METHOD ---\n",
      "Similar to previous audio generation works , Re-AudioLDM is a cascaded model including three parts: input embedding, diffusionbased feature generator, and a pipeline to reconstruct the waveform from the latent feature. 3.1. Text and Retrieval Embedding Encoder Re-AudioLDM takes two paralleled inputs: a text input c; as lowlevel semantic information, and a set of text-audio pairs as retrieval augmentation c,. for high-level semantic-audio information. The text embedding E*’ is obtained as: E‘ =f,...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on events with different frequencies of occurrence in the dataset. We show that Re-AudioLDM provides a stable performance among a variety of audio entities. It significantly improves the performance for tail classes over the baseline models, demonstrating that it can provide effective alleviation for long-tail TTA issues. Furthermore, as compared with the baseline models, Re-AudioLDM is capable of generating more realistic and complex audio clips, including rare, complex, or even unseen audio ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s are given in Section|6] 2. RELATED WORK Our work relates to two major works, diffusion-based text-to-audio models and retrieval-based generative models. These two fields are briefly discussed in the following subsections. 2.1. Audio Generation Recent works on audio generation follow an encoder-decoder framework [I|{7J. The model first uses an encoder to encode the information into a latent representation, which can be decompressed into a mel-spectrogram feature. The decoder then uses a variati...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07378v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model’s behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed, “contrastive” version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mi...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large pre-trained language models (LMs) have revolutionized natural language processing in recent years (Radford et al., 2019; Raffel et al., 2020). However, their practical applicability remains hindered by their extreme sensitivity to minor input perturbations (natural and adversarial), including ones that humans deem insignificant (Belinkov and Bisk, 2017; Sun et al., 2018). Consider using an LM to answer medical questions, such as “What happens if listeria is left untreated?”, as in the Heal...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Robustness to input perturbations. Testing the sensitivity of neural language models to different input perturbations has been studied both from the perspective of model fairness (when the input perturbations correspond to individuals) and model robustness (when the perturbations correspond to conditions which the system may likely experience at test time, such as spelling mistakes or even adversarial modifications). For example, Prabhakaran et al. (2019) evaluate the sensitivity of text classif...\n",
      "\n",
      "--- METHOD ---\n",
      "s). Alternatively, we could stochastically generate likely responses given each input (e.g. using temperature sampling), but then it is less clear how to compare the outputs we obtained with each input. Beyond the issues of fairness and robustness, it was shown that success on many well-defined tasks is highly sensitive to small changes in phrasing (Srivastava et al., 2022; Efrat et al., 2022), especially now that “prompt-engineering” became a standard practice. Given that, understanding the imp...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al setup. We demonstrate how CID can be used to surface context-specific biases in an interpretable way. We root the investigation in a specific context (e.g. biases in tech) by considering specific input templates, e.g. “<name>, a software developer, failed his (her) interview at a major tech company because he (she). Following Maudslay et al. (2019), we intervene on <name> as a way of estimating gender and racial biases for this specific input. For a single pair of names — e.g. John and Ahmed ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s We proposed Contrastive Input Decoding (CID), a decoding procedure that can be used with any pretrained LM to produce continuations likely for the input text but unlikely for a given contrastive input text. Our focus was on using CID to audit fairness and robustness of pretrained LMs. A promising application we did not explore is using CID to streamline how LMs are used in practice. For example, whether contrastive techniques such as CID can aid prompt engineering by equipping developers with ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.07243v2.pdf ---\n",
      "--- METHOD ---\n",
      "ology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise - an expressive, multi-voice text-to-speech system. All model code and trained weights have been open-sourced at https://github.com/neonbjb/tortoise-tts 1 Background 1.1 Text-to-speech The field of text-to-speech (TTS) research has been largely constrained to the development of efficient models trained on relatively...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s Text to speech systems are challenging to experimentally compare because many state of the art systems are closed source with few samples to compare against. To this end, I built my own evaluation suite which uses CLVP to produce a distance metric between real samples and generated samples, similar to the FID score used by images. | also use an open source wav2vec model to characterize the “intelligibility” of a speech segment. I have open sourced this work here. Past this, comparisons between...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s drawn above: 1. Autoregressive models are strong at converting between unaligned domains like vision, text and speech. --- --Diffusion Decoder Autoregressive Transformer ane Ot —t_ 1Sist ae i Vocoder +———— $ } LS —$—$> Argmax 's H si 3i Figure 1: TorToise-v2 architectural design diagram. Inputs of text and a reference audio clip (for speaker cloning) flow through a series of decoding and filtering networks to produce high-quality speech. 2. DDPMs operate in the continuous domain which allows t...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.15042v2.pdf ---\n",
      "--- INTRODUCTION ---\n",
      "Long-term generation of a motion sequence is a difficult and long standing problem in character animation with myriad applications in computer animation, motion control, human-computer interaction, and more. Generating long-term motion entails producing realistic, non-repetitive sequences which avoids degenerate outputs (ie., frozen motion). A promising avenue for generating high-quality motion is through Denoising Diffusion Probabilistic Models (DDPM), which have produced unprecedented quality ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 Diffusion Models Denoising diffusion probalistic models (DDPMs) and its variants [Ho et al. 2020; Dhariwal and Nichol 2021; Ho et al. 2022] have achieved unprecedented quality on conditional and unconditional image generation, generally surpassing GAN-based [Dhariwal and Nichol 2021]...\n",
      "\n",
      "--- METHOD ---\n",
      ", referred to as TEDi (Temporally-Entangled Diffusion), extends the DDPM framework by enabling injection of temporallyvarying noise levels during each step of the diffusion process, instead of a Gaussian noise with a fixed, temporally-invariant variance. By entangling the temporal-axis of the motion sequence with the time-axis of the diffusion process, we enable the production of a continuous stream of clean motion frames during each step of the diffusion process. At the core of our framework li...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show that TEDi is a natural framework for generating long-term motion sequences. 2 RELATED WORK 2.1 Diffusion Models Denoising diffusion probalistic models (DDPMs) and its variants [Ho et al. 2020; Dhariwal and Nichol 2021; Ho et al. 2022] have achieved unprecedented quality on conditional and unconditional image generation, generally surpassing GAN-based [Dhariwal and Nichol 2021] methods both in visual quality and sampling diversity. In particular, diffusion models have demonstrated remarkab...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we proposed TEDi, an adaptation of diffusion models for motion synthesis which entangles the motion temporal-axis with the diffusion time-axis. This mechanism enables synthesizing arbitrarily long motion sequences in an autoregressive manner using a U-Net architecture. A unique aspect of our work is the notion of a stationary motion buffer. Our framework continues to produce clean frames (i.e., progressing along the diffusion-time axis), without actually incrementing the diffusion...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11694v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \"shorebirds that are not sandpipers\" or \"science-fiction films shot in England\". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The d...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "People often express their information needs with multiple preferences or constraints. Queries corresponding to such needs typically implicitly express set operations such as intersection, difference, and union. For example, a movie-goer might be looking for a science-fiction film from the 90s which does not feature aliens and a reader might be interested in a historical fiction novel set in France. Similarly, *Work done during an internship at Google. 'The dataset is available at https://github...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Previous work in question answering and information retrieval has focused on QA over knowledge bases as well as open-domain QA and retrieval over a set of entities or documents. We highlight how these relate to our work below. Knowledge Base QA Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2013; Yih et al., 2016; Talmor and Berant, 2018; Keysers et al., 2020; Gu et al., 2021, inter alia). These benchmarks require retrieval of a set of entities th...\n",
      "\n",
      "--- METHOD ---\n",
      "s to answer set-seeking selective queries, while providing attribution to sources, can also be investigated. 7 Limitations Naturalness. Since our dataset relies on the Wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in r...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s in §4. To generate these examples, we sampleatomic queries from all domains, ensuring that they do not already appear as sub-queries in any of the queries in QUEST and use their corresponding entities in Wikipedia as their relevant entity set. 4 Experimental Setup We evaluate modern retrieval systems to establish baseline performances. We also perform extensive error analysis to understand patterns of model errors and the quality of the labels in QUEST. 4.1 Task Definition We consider a corpus...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present QUEST, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. Our experiments indicate that such queries present a >For the dual encoder, we split documents into overlapping chunks of 512 tokens, and aggregated scores at inference (Dai and Callan, 2019). For the cross-attention model, we evaluated using BM25 to select the top-3 passages of length 128. --- --challenge for modern retrieval systems. Future work could conside...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.07941v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Transcriptions of phone calls are of significant value across diverse fields, such as sales, customer service, healthcare, and law enforcement. Nevertheless, the analysis of these recorded conversations can be an arduous and time-intensive process, especially when dealing with extended or multifaceted dialogues. In this work, we propose a novel method, GPTdistilled Calls Segmentation and Tagging (GPT-Calls), for efficient and accurate call segmentation and topic extraction. GPT-Calls is composed...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In today’s highly competitive market, sales agents play a critical role in driving sales and maintaining a strong customer base. One of the primary ways they interact with customers is through phone calls. These phone calls are often recorded and transcribed for quality assurance, training and coaching other sellers, curating insights, and more. Therefore, sales departments often maintain an extensive database consisting of millions of call transcriptions, which serves as an important source of ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "In recent years, there has been a growing interest in text segmentation, which involves dividing text passages into coherent segments based on their content. While traditional text segmentation...\n",
      "\n",
      "--- METHOD ---\n",
      ", GPTdistilled Calls Segmentation and Tagging (GPT-Calls), for efficient and accurate call segmentation and topic extraction. GPT-Calls is composed of offline and online phases. The offline phase is applied once to a given list of topics and involves generating a distribution of synthetic sentences for each topic using a GPT model and extracting anchor vectors. The online phase is applied to every call separately and scores the similarity between the transcripted conversation and the topic ancho...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s section, we compare our method with the previous method used in Dynamics 365 Sales for call segmentation, which was based on the same approach of Bi. A different direction has been to employ text summarization methods for topic tagging. This approach involves assigning one or more tags to a given text to represent its key topics or themes. One such approach, known as extractive summarization, involves extracting key sentences from the text and using them to tag the text with pre-defined topics...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose a novel approach for call segmentation and tagging that builds upon distilling knowledge from the GPT-model and does not require labeled data. Our solution is generic and can be applied to various domains. The proposed method is deployed in Dynamics 365 Sales Conversation Intelligence and was shown to significantly improve upon other alternatives. 6. REFERENCES [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shya...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04235v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 1SMB and 30ms on devices. We pro...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "As a classic subfield of natural language processing, neural machine translation (NMT) has achieved great success in recent years. Most of the studies focus on improving the accuracy of large machine translation systems, ignoring whether such models are easy to be deployed in real-world scenarios. Here we adopt four metrics to evaluate whether an NMT model is deployment-friendly. (1) Model size is the most important metric in model compression (Han et al., 2016). (2) Floating-point operations (F...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "on large-scal models (Wang et al., 2019, 2022). Based on th above discussion, we finally build a deep encoder and a shallow decoder, while reducing the vocab size and model width. Two MobileNMT models of different sizes are built here and the detailed settings are shown in Table 3. e ie 3 Training Strategies 3.1 Pre-Training with Knowledge Distillation In order to improve the performance of compressed models, recent studies distill knowledge from a well-trained full-precision teacher network to ...\n",
      "\n",
      "--- METHOD ---\n",
      "s in Section 2 and Section 3 (Scaling E: scaling embedding dimension; Scaling V: scaling vocabulary size; Sharing: cross-layer parameter sharing; Width: reducing model width). Scaling V performs better than Scaling E. Width performs nearly the same with Sharing. performance of Transformer-big with only 1.1% Module| Dim Base Small Tiny : d fi decodi hich Vocab | f 40,000 40,000 40,size and runs 47.0x faster on decoding, which can Embed Embed | N/A = N/A «i N/A }= be easily deployed and used. Hidd...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s in Lin et al. (2021)’work also show that smaller vocabularies may be better. Reducing Vocabulary Size Performs Better. To compare the two embedding compression methods, here we select three baseline models of different sizes. The model settings are shown in Table 1. As shown in Table 2, the parameters and FLOPs are almost the same in these two methods. As shown --- --¢ | ~ 10P —e— Sharing |} _ —e- Sharing |) © 25 Ins s —e wian || 2 4007 se Width |] > 20f law || 3 5} +S 200+m lop a a a 1 (0) ee...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose MobileNMT, a Transformer-based machine translation system that can translate in 15MB and 30ms. It uses existing resources efficiently and can be easily deployed in real-world scenarios. We develop a mobile inference engine with GEMM and memory optimization, hoping that it can bridge the gap between theoretical research and real-world applications on efficient machine translation. Acknowledgments This work was supported in part by the National Science Foundation of China (No. 62276056)...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.03322v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1). We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (BiPhone) for synthetically producing corrupt...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "We live in a multilingual world with over 7,languages spoken across the globe (Eberhard and Fennig, 2022). However, technology asymmetrically supports only a few specific languages. For instance, the internet is mostly in English with over 60% of websites using the language despite just around 16% share of its speaking population around the world! (Grefenstette and Nioche, 2000). Increasingly, people are forced to navigate and produce content on the web in languages they have not been formally t...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "We divide the presentation of related work in two sections. (i) First, we discuss prior work spanning multiple research areas regarding phonetic influences in text and how it relates to our work. (ii) Second, we discuss work in the speech domain which studies phonetic variations occurring due to inter-language interference in multi-lingual scenarios. 2.1 Phonetic Influences in Text Phonetic influence on spelling errors has been studied in the past (Kukich, 1992; Toutanova and Moore, 2002; Hlddek...\n",
      "\n",
      "--- METHOD ---\n",
      "to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (BiPhone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web. We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE)...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we use the ARPAbet phoneme set for English 4. Phoneme-Phoneme Error Model: The first term under the summation in Equation 2 models the likelihood of generating a corrupted phoneme sequence phy given that a native speaker of L1 is attempting to speak a phoneme sequence phy in L2. With simplifying independence assumptions that each phoneme is corrupted individually, independent of phonemes around it, we can factorize this term to utilize the phoneme confusion matrix we have mined. P(pha|phw) = ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Language is a significant barrier to technology especially for new internet users. For such users, English often is not their first language. The speech community has made significant progress in making technology (ASR for instance) accessible for such users by making models robust to account for inter-language interactions. We argue that a similar line of effort is needed in the Natural Language Understanding for Text community as well. To this end, we first propose a generative model Bi-Phone ...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.11418v3.pdf ---\n",
      "--- ABSTRACT ---\n",
      "As recent advances in Neural Radiance Fields (NeRF) have enabled high-fidelity 3D face reconstruction and novel view synthesis, its manipulation also became an essential task in 3D vision. However, existing manipulation methods require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene mani...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Easy manipulation of 3D face representation is an essential aspect of advancements in 3D digital human contents[32]. Though Neural Radiance Field[20] (NeRF) made a big step forward in a 3D scene reconstruction, many of its manipulative methods targets color[4, 34] or rigid ge --- --ometry [45, 15, 41, 14] manipulations, which are inappropriate for detailed facial expression editing tasks. While a recent work proposed a regionally controllable face editing method [13], it requires an exhaustive p...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s NeRF and Deformable NeRF Given multiple images taken from different views of a target scene, NeRF[20] synthesizes realistic novel view images with high fidelity by using an implicit volumetric scene function and volumetric rendering scheme[ 12], which inspired many follow-ups [1, 35, 19, 37, 44]. As NeRF assumes a static scene, recent works [22, 23, 26, 16] propose...\n",
      "\n",
      "--- METHOD ---\n",
      "s require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing l...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results and discussions over head pose controllability of Wr. F(T(x,0p),H(x,w).d), 6) Lipschitz MLP _ Since G is only trained to be conditioned over a limited set of trainable latent codes W, a subspace of w outside the learned latent codes that yields plausible deformations needs to be formulated to maximize the expressibility of G for manipulation. Meanwhile, HyperNeRF was shown to moderately render images from latent codes linearly interpolated from two learned latent codes. Thus, a valid ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ". We may approach this problem from the slicing surface perspective of canonical hyperspace introduced in Sec. 3.2. As in Fig. 2a, hyperspace allows only one latent code to represent an instance of a slicing surface representing a global deformation of all spatial locations. Such representation causes a change in one type of deformation in one location to entail the same degree of change to another type of deformation in different locations during interpolation. Our method is motivated by the ob...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.04619v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Estimating 3D articulated shapes like animal bodies from monocular images is inherently challenging due to the ambiguities of camera viewpoint, pose, texture, lighting, etc. We propose ARTIC3D, a self-supervised framework to reconstruct per-instance 3D shapes from a sparse image collection in-the-wild. Specifically, ARTIC3D is built upon a skeleton-based surface representation and is further guided by 2D diffusion priors from Stable Diffusion. First, we enhance the input images with occlusions/t...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Articulated 3D animal shapes are widely used in applications such as AR/VR, gaming, and content creation. However, the articulated models are usually hard to obtain as manually creating them is labor intensive and 3D scanning real animals in the lab settings is highly infeasible. In this work, we aim to automatically estimate high-quality 3D articulated animal shapes directly from sparse and noisy web image collections. This is a highly ill-posed problem due to the variations across images with ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Animal shape and pose estimation. Earlier techniques on animal shape estimation used statistical body models [46, 45] that are learned either using animal figurines or a large number of annotated animal images. Some other works [35, 34, 36, 37], use video inputs to learn articulated shapes by exploiting dense correspondence information in video. However, these...\n",
      "\n",
      "--- METHOD ---\n",
      "s [39, 33, 38] can produce animatable 3D shapes using a skeleton-based neural surface or pre-defined mesh template, the success is largely dependent on large-scale image datasets or manually-filtered clean images for training or optimization. Moreover, the output 3D shapes and textures are usually unrealistic when viewed from novel viewpoints or pose articulations. On the other hand, recent success of generative diffusion models [25, 28, 27] shows that one can generate high-quality images for a ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, nonetheless, we observe that naively applying the SDS loss on 3D surface optimization leads to unstable and inefficient training, producing undesirable artifacts like noisy surfaces or ambiguous texture. In this work, we present ARTIC3D (ARTiculated Image Collections in 3D), a diffusion-guided optimization framework to learn articulated 3D shapes from sparse noisy image collections. We use the articulated part surface and skeleton from Hi-LASSIE [38], which allows explicit part manipulation a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We propose ARTIC3D, a diffusion-guided framework to reconstruct 3D articulated shapes and texture from sparse and noisy web images. Specifically, we design a novel DASS module to efficiently calculate pixel gradients from score distillation for 3D surface optimization and use it in the input preprocessing of noisy images; Shape and texture optimization; as well as the animation fine-tuning. Results on both the existing datasets as well as newly introduced noisy web images demonstrate that ARTIC3...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.05176v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs—e.g. GPT-4, ChatGPT, J1-Jumbo—and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the i...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "We are in the midst of an explosion of large language models (LLMs). The alluring possibilities o: using LLMs for large-scale applications such as commerce, science, and finance have led a growing number of companies (OpenAI, AI21, CoHere, etc.) to offer LLMs as services. While LLMs such as GPT-4 achieves unprecedented performance in tasks such as question answering, using them for high-throughput applications can be very expensive. For example, ChatGPT is estimate 0 cost over $700,000 per day t...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s. Prompt Engineering. Prompt engineering has emerged as a discipline for crafting prompts to enhance LLMs’ performance across various applications. Recent developments include few-shot [BMR* 20], chain-of-thought [WWS* 22], knowledge enhancement [LLL*21, KSL*22], and numerous other prompting techniques [MDL*23, KTF*22, ZSH+22, DGSG22]. Existing prompt engineering approaches often aim to provide more detailed task explanations and in-context examples, resulting in longer and more expensive promp...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.17319v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper presents a new mechanism to facilitate the training of mask transformers for efficient panoptic segmentation, democratizing its deployment. We observe that due to its high complexity, the training objective of panoptic segmentation will inevitably lead to much higher false positive penalization. Such unbalanced loss makes the training process of the end-to-end mask-transformer based architectures difficult, especially for efficient models. In this paper, we present ReMaxX that adds re...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Panoptic segmentation [35] aims to provide a holistic scene understanding [61] by unifying instance segmentation [20] and semantic segmentation [23]. The comprehensive understanding of the scene is obtained by assigning each pixel a label, encoding both semantic class and instance identity. Prior works adopt separate segmentation modules, specific to instance and semantic segmentation, followed by another fusion module to resolve the discrepancy [69, 10, 34, 68, 52, 41]. More recently, thanks to...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Mask Transformers for image segmentation. Recent advancements in image segmentation has proven that Mask Transformers [64], which predict class-labeled object masks through the Hungarian matching of predicted and ground truth masks using Transformers as task decoders [62, 4], outperform box-based...\n",
      "\n",
      "--- METHOD ---\n",
      "with efficient backbones like MobileNetV3-Small, our method achieves new state-of-the-art results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes. Code and pre-trained checkpoints will be available at https: //github.com/ google-research/deeplab2. 1 Introduction Panoptic segmentation [35] aims to provide a holistic scene understanding [61] by unifying instance segmentation [20] and semantic segmentation [23]. The comprehensive understanding of the scene is obtained by assignin...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results have shown that our method not only speeds up the training by 3x, but also leads to much better results for panoptic segmentation. Overall, ReMaX sets a new state-of-the-art record for efficient panoptic segmentation. Notably, for efficient backbones like MobileNetV3-Small and MobileNetV3-Large [26], our method can outperform the strong baseline by 4.9 and 5.2 in PQ on COCO panoptic for short schedule training; while achieves 2.9 and 2.1 improvement in PQ for the final results (i.e., ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "The paper presents a novel approach called ReMaX, comprising two components, ReMask and ReClass, that leads to better training for panoptic segmentation with Mask Transformers. The proposed method is shown to have a significant impact on training speed and final performance, especially for efficient models. We hope that our work will inspire further investigation in this direction, leading to more efficient and accurate panoptic segmentation models. Acknowledgement. We would like to thank Xuan Y...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10142v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price an...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "We study whether multiple Large Language Models (LLMs) can improve each other in a negotiation game with minimal human intervention, in the fashion of AlphaGo Zero [31] where AI agents improve themselves by continuously playing competitive games under well-defined rules. The answers to this research question have profound implications. On the positive side, if the agents were able to improve autonomously, strong agents might be created with very few human annotations, which greatly saves the cos...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Game Playing and AlphaGo Zero Our setting is strongly inspired by AlphaGo Zero [31] where two agents play the game of Go and improve each other with minimal human intervention. Here we would like to explore its counterpart in natural language. Our work is similar to AlphaGo Zero in the sense that we also have AI agents (large language models) playing competitive games (bargaining) and try to improve with little human supervision. Yet there is an important difference between our work and AlphaGo ...\n",
      "\n",
      "--- METHOD ---\n",
      "an effective side product recommend it as a technique for prompt optimization for generic classification tasks. Playing for Multiple Rounds Finally, we would like to explore whether the players can continuously improve from AI feedback in a game over multiple rounds. Intuitively, the more rounds the players play, the more challenging to keep improving because the (already improved) price from the previous round becomes the baseline for the next round. In the...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game’s rules or cannot incorporate AI feedback for further improvement. (2) Models’ abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger ag...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s we draw is fairly likely to generalize when one actually finetunes the model (if resources permit). One notable difference between our ICL-AIF and the mainstream Reinforcement Learning from Human Feedback (RLHF) is that in RL the reward is a scalar while in ICL the feedback is in natural language. We study AI feedback (rather than rely on human intervention after each round) because it is more scalable and can allow models to self-improve automatically. Our experiments lead to several intrigui...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.01229v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learnin...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "With the development of deep learning, automatic music generation is developing rapidly and attracting more and more interest (Hernandez-Olivan & Beltran, 2022; Shih et al., 2022; Yu et al., 2022). Due to the importance of emotions for music, emotional music generation is an important and practical task, yet it is still under-explored. Previous work, according to the way of applying emotion signals, can be divided into two types. The first type is to convert emotion labels as embeddings and take...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Emotional Music Generation Emotion-conditioned music generation is developing rapidly in the age of deep learning. According to the way of applying emotion signals, previous work can be divided into two types. The first type is to convert emotion labels as embeddings and take them as model input (Madhok et al., 2018; Hung et al., 2021; Pangestu & Suyanto, 2021; Sulun et al., 2022; Grekow & Dimitrova-Grekow, 2021). Madhok et al. generate emotional music based on the one-hot emotion label. So...\n",
      "\n",
      "--- METHOD ---\n",
      "s on emotion control accuracy and music quality respectively, which demonstrate our superiority in generating emotional music. Music samples generated by EMOGEN are available via this link!, and the code is available at this link”. “Equal contribution ‘Shanghai Jiao Tong University, China *Microsoft Research Asia *Nanjing University, China 4National Engineering Research Center for Software Engineering, Peking University, China. Correspondence to: Xu Tan <xuta@microsoft.com>. ‘https: //ai-muzic.g...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results show that EMOGEN outperforms previous methods on emotion control accuracy and music quality. Experiments also demonstrate the ability of EMOGEN to eliminate subjective bias in emotion labels. 2. Related Work 2.1. Emotional Music Generation Emotion-conditioned music generation is developing rapidly in the age of deep learning. According to the way of applying emotion signals, previous work can be divided into two types. The first type is to convert emotion labels as embeddings and take...\n",
      "\n",
      "--- CONCLUSION ---\n",
      ", EMOGEN is able to generate music with desired emotion on multiinstrumental datasets. Generated samples are available via this link?. 5. Conclusion In this paper, we propose EMOGEN, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music. EMOGEN divides emotional music generation into two stages: in music-to-attribute mapping stage, EMOGEN map the emotion label to attribute values that can represent the general emotio...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.05960v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recent booming successes of large language models (LLMs) (OpenAI| 2023} 2023] motivate emerging exploration of employing LLM to tackle various complex tasks (Zhang et al. (2023), amongst which LLM-augmented Autonomous Agents (LAAs) et al. 2023} Huang et al.| 2022} {Kim et al.| 2023} Paul et al.| 2023} Yao et al.||2023a) stand with most spotlights. LAA extends the intelligence of LLM to sequential action executions, exhibiting superiority in interacting with environments and resolving complex tas...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1 AUGMENTED LANGUAGE AGENT ARCHITECTURE The completion of a complex task typically entails multiple stages. An agent must possess an understanding of these stages and plan accordingly. Chain-of-Thoughts, also known as CoT (2022), is a groundbreaking work that prompts the agent to deconstruct challenging reasoning tasks into smaller, more manageable steps. On the other hand, ReAct proposes leveraging this aptitude for reasoning and action within Language and Learning Models (LLMs) to foster int...\n",
      "\n",
      "--- METHOD ---\n",
      "to interact with environments then consecutively generate the next action. Langchaitf']is a recently released open-source framework for developing LAA. Due to the initial investigation, LAA is rather under-explored. Firstly, the optimal agent architecture is undetermined. ReAct (Yao et al.|/2023a) prompts the agents with pre-defined examples such that the LLM learns to generate the next action via in-context learning. Moreover, ReAct argues that an agent should have intermediate reasoning steps ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on both decision-making web navigation environment and knowledge reasoning task environment. We report the performance in terms of final sparse rewards and intermediate recalls, which provides qualitative indications for the optimal choice of LAAs as well as their compatible LLMs. * BOLAA on the WebShop environment consistently yields the best performance compared with other LAA architectures. Our results demonstrate that the importance of designing specialist agents to collaborate on resolvin...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "AND FUTURE WORK In this paper, we systematically investigate the performances of various LAA architecture paired with different LLM backbones. We also provide one novel orchestrating method for multiple agents, i.e. BOLAA. The benchmarking results provide experimental justification for the LAA investigation and verify the potential benefits of BOLAA architecture. During the investigation, we also identify the challenge of designing BOLAA architecture for environments with compounding actions. In...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.13050v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In recent years, image generation has shown a great leap in performance, where diffusion models play a central role. Although generating high-quality images, such models are mainly conditioned on textual descriptions. This begs the question: how can we adopt such models to be conditioned on other modalities?. In this paper, we propose a novel method utilizing latent diffusion models trained for text-to-image-generation to generate images conditioned on audio recordings. Using a pre-trained audio...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Neural generative models have changed the way we consume digital content. From generating high-quality images [1, 2, 3], though coherence of long spans of text [4, 5, 6], up to speech and audio [7, 8, 9, 10]. In recent years, diffusion-based generative models have emerged as the preferred approach, showing promising results on various tasks [11]. During the diffusion process, the model learns to map a predefined noise distribution to the target data distribution. In every step of the diffusion p...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "to ours is Wav2Clip [30], in which the authors first learn a Contrastive Language-Image Pre-Training (CLIP) [31] like a model for audio-image pairs. Then, later on, such representation can be used to generate images using VQGAN [32] under the VQ-GAN CLIP [33] framework. Why use audio signals as a conditioning to image generation rather than text? Although text-based generative models can generate impressive images, textual descriptions are not naturally paired with the image, i.e., textual descr...\n",
      "\n",
      "--- METHOD ---\n",
      "utilizing latent diffusion models trained for text-to-image-generation to generate images conditioned on audio recordings. Using a pre-trained audio encoding model, the proposed method encodes audio into a new token, which can be considered as an adaptation layer between the audio and text representations. Such a modeling paradigm requires a small number of trainable parameters, making the proposed approach appealing for lightweight optimization. Results suggest the proposed method is superior t...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s, we show that our method is able to generate high-quality and diverse set of images based on audio-scenes. 2. Adaptation of text-conditioned models Diffusion models are a family of models that are prone to learn the underlying probabilistic model of the data distribution p(x). This is done by learning the reverse Markov process of length T. Given a timestamp t € [0,1], the denoising function eg : R? — R? learns to predict a clean version of the perturbed x; from the training distribution S = {...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this paper, we present a method for leveraging textconditioned generative models for audio-based conditioning. Our method produces high-quality images which describe a scene from the audio recording. In addition, we propose a comprehensive evaluation framework that takes into account the semantics of the images generated. Our method presents a first step toward audio-conditioned image generation. The hidden information in the audio is richer than the observed one in the text. Hence, we thin...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.01826v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN nonlinearly transforms each input token independently. In this work we explore the role of the FEN, and find that despite taking up a significant fraction of the model’s parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The Transformer architecture (Vaswani et al., 2017) has become the de facto paradigm in many Natural Language Processing (NLP) tasks, including Machine Translation (MT). Several studies have shown that Transformers exhibit impressive scaling-law properties (Gordon et al., 2021; Bansal et al., 2022; Ghorbani et al., 2022), wherein increasing the number of model parameters leads to further accuracy gains. In parallel with this architecture’s impressive scaling of the numbers of parameters (Chowdhe...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Weight pruning and parameter sharing are wellknown techniques to reduce a model’s footprint. Given the scale of the latest models (Chowdhery et al., 2022), there have been multiple efforts to prune neurons based on different automatic...\n",
      "\n",
      "--- METHOD ---\n",
      "ology 2.1 Transformer The Transformer architecture has two main components: attention and the FFN, which are connected via a residual connection (He et al., 2016) and layer normalization (Ba et al., 2016). In an encoderdecoder model, there are two types of attention: self-attention and cross-attention. Self-attention is used in both the encoder and the decoder, allowing the model to focus on relevant information within the same sequence. Cross-attention is exclusive to the decoder and allows it ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s with different configurations of the Transformer, across different language pairs, including a low resource language pair and multilingual. In addition, we investigate the effect of the FFN in a decoder-only Transformer-based model. We find that a considerable level of redundancy exists between the encoder and decoder FFNs. As a result, we are able to eliminate the decoder FFN and share a single FFN across the encoder without significantly compromising the model’s accuracy. This step leads not...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this work, we studied the importance of the FFN in Transformer models. We analyzed the impact of removing and/or sharing the FFN across layers and found that, due to this component’s redundancy, the model sizes can be substantially reduced with little impact on accuracy for Machine Translation. In particular, we found that sharing the FFN across all encoder layers while making it larger and removing it from the decoder layers leads to models that are more accurate and faster at inference. Our...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.17098v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "This paper presents ControlVideo for text-driven video editing — generating a video that aligns with a given text while preserving the structure of the source video. Building on a pre-trained text-to-image diffusion model, Control Video enhances the fidelity and temporal consistency by incorporating additional conditions (such as edge maps), and fine-tuning the key-frame and temporal attention on the source video-text pair via an in-depth exploration of the design space. Extensive experimental r...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The endeavor of text-driven video editing is to generate videos derived from textual prompts and existing video footage, thereby reducing manual labor. This technology stands to significantly influence an array of fields such as advertising, marketing, and social media content. During this process, it is critical for the edited videos to faithfully preserve the content of the source video, maintain temporal consistency between generated frames, and align with the provided text and optional refer...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "4.1 Diffusion Models for Text-driven Generation and Image Editing Recently, diffusion models have achieved major breakthroughs in the field of generative artificial intelligence and thus are utilized for text-to-image generation 2]. These models usually train a diffusion model conditioned on text on large-scale image-text paired datasets. Building upon these remarkable advances of T2I diffusion models, numerous...\n",
      "\n",
      "--- METHOD ---\n",
      "s , Beijing, China 3ShengShu, Beijing, China; +Pazhou Laboratory (Huangpu), Guangzhou, China gracezhao1997@gmail.com; wangrz@ruc.edu.cn; bf19@mails.tsinghua.edu.cn; chongxuanli@ruc.edu.cn; dcszj@tsinghua.edu.cn Abstract This paper presents ControlVideo for text-driven video editing — generating a video that aligns with a given text while preserving the structure of the source video. Building on a pre-trained text-to-image diffusion model, Control Video enhances the fidelity and temporal consiste...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al results demonstrate that Control Video outperforms various competitive baselines by delivering videos that exhibit high fidelity w.r.t. the source content, and temporal consistency, all while aligning with the text. By incorporating Lowrank adaptation layers into the model before training, ControlVideo is further empowered to generate videos that align seamlessly with reference images. More importantly, Control Video can be readily extended to the more challenging task of long video editing (...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we present ControlVideo, a general framework to utilize T2I diffusion models for one-shot video editing, which incorporates additional conditions such as edge maps, the key frame and temporal attention to improve faithfulness and temporal consistency. We demonstrate its effectiveness by outperforming state-of-the-art text-driven video editing methods. References 1] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attent...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.07062v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "—We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "There is increasing interest in Large Language Models (LLMs) for software engineering domains such as code generation [1-9], code translation [10-12], and code testing [13-15]. Models such as Code Llama [9], Codex [8], and ChatGPT [16] have a good statistical understanding of code and suggest likely completions for unfinished code, making them useful for editing and creating software. However, it appears they have not been trained specifically to optimize code. ChatGPT, for instance, will make m...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Compiler pass ordering for performance has been exploited for decades [26, 52, 53]. Over the years there have been several approaches using machine learning [18-20, 39, 54, 55]. The application of machine learning in compilers is not limited to pass order and has been applied to many other problems [17, 56-59]. No one has applied LLMs to the problem of pass ordering, we are the first to do so. Neural machine translation is an emerging field that uses language models to transform code from one la...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s and code examples to further characterize the potential and limits of LLMs for code reasoning. Overall we find their efficacy remarkable and think that these results will be of interest to the community. --- --Training Phase Inference Phase i S Tell me what passes to run on the following LLVM-IR to reduce instruction count: Prompt count from 15 to 4: <code> <code> define dso_local void @*iowritesingle\"(i32 %0, 132 %1) { %3 = alloca i32, align%4 = alloca 132, alignstore 132 20, i32* %3, alignst...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "S We present the first steps towards LLMs for code optimization. We construct a model that can predict good optimization strategies for unseen LLVM-IR. Results are promising, though we face challenges in sequence length which limits us to operating over small program fragments, and in arithmetic reasoning which limits the ability of the model to predict the outcome of optimizations. We hope to inspire the research community to push beyond LLMs for simple max-likelihood co le generation and into ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.09975v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Enhancing word usage is a desired feature for writing assistance. To further advance research in this area, this paper introduces \"Smart Word Suggestions\" (SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario. This task involves identifying words or phrases that require improvement and providing substitution suggestions. The benchmark includes human-labeled data for testing, a large distantly supervised datase...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Writing assistance is a widely used application of natural language processing (NLP) that helps millions of people. In addition to common features like grammatical error correction (Ng et al., 2014; Bryant et al., 2017), paraphrasing (Fader et al., 2013; Lin et al., 2014) and automatic essay scoring (Song et al., 2020), providing word suggestions is a desired feature to enhance the overall quality of the writing. As illustrated in figure 1, the word “intimate” in the first sentence should be rep...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s We begin by comparing SWS with three related tasks, highlighting the unique value of our work. 2.1 Lexical Substitution Lexical substitution (LS) (McCarthy and Navigli, 2007; Kremer et al., 2014; Lee et al., 2021) is the task of providing substitute words for a specific word in a sentence. There are some major distinctions between the SWS and LS. (1) In LS, the target word is already provided, while in SWS, the system needs to detect the improvable targets first. (2) LS focuses on finding syno...\n",
      "\n",
      "--- METHOD ---\n",
      ". Improvable target: intimate Substitution suggestion: close Suggestion type: refine-usage Reason: Word “intimate” is for friends or lovers, the cooperation between colleagues should use “close”. Sentence: If you learn from others, it would be more possible to communicate with different people. Improvable target: possible Substitution suggestion: likely Suggestion type: refine-usage Reason: The sentence wants to express “more likely’ rather than “have chance to”, “likely” is more proper. Sentenc...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s with seven baselines demonstrate that SWS is a challenging task. Based on experimental analysis, we suggest potential directions for future research on SWS. The dataset and related codes is available at https://github.com/ microsoft/SmartWordSuggestions. 1 Introduction Writing assistance is a widely used application of natural language processing (NLP) that helps millions of people. In addition to common features like grammatical error correction (Ng et al., 2014; Bryant et al., 2017), paraphr...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper introduces the first benchmark for Smart Word Suggestions (SWS), which involves detecting improvable targets in context and suggesting substitutions. Different from the previous benchmarks, SWS presents a more realistic representation of a writing assistance scenario. Our experiments and analysis highlight various challenges for future research and suggest opportunities for improvement in future work. We encourage further research on building more realistic training data, designing be...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.08133v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR. We demonstrate up to 8% relative reduction in Word Error Eate (WER) on US English (en-us) and code-switched Indian English (en-in) longform ASR test sets and a reduction of up to 30% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. Improved lattice p...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large-scale language models (LLM), such as as BERT [2], T5 B), GPT-3 [4], and PaLM [5], have proven to be successful in natural language processing (NLP) tasks such as, Question Answering, Text Summarization, and other Zero Shot learning applications. These models are trained on vast amounts of text data and have yielded state-of-the-art results across several NLP and search tasks. However, there is very limited work on the use of these LLMs in Automated Speech Recognition (ASR). Recent research...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Several...\n",
      "\n",
      "--- METHOD ---\n",
      "s to incorporate LMs in end-to-end sequence models have been proposed in the literature. Decoding algorithms (10) (11) employ fusion strategies, such as shallow (13), cold deep and component (16) fusion. However, the wins from incorporating LMs in this fashion have been relatively small for large scale ASR [17]. The Hybrid Autoregressive Transducer (HAT) model introduced in for encoder-decoder models, allowed for the computation of an internal language model component that can be quantified and ...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "S 4.1. Data We conduct experiments with data from two language locales, enus and en-in. The multi-domain ASR model used in this paper is trained on several thousand hours of long-form utterances derived from YouTube videos[25] and short-form utterances that are anonymized, hand-transcribed and are representative of Google’s Voice Search traffic [26]. The test sets contain long-form utterances derived from 30-minute-long YouTube videos. We set aside a subset containing 5% of the test utterances a...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this study, we presented the impact of LLMs (up to 350B parameters) on long-form ASR. We demonstrated up to 8% relative reduction in Word Error Rate (WER) on US English (en-us) and code-switched Indian English (en-in) long-form ASR test sets and a reduction of up to 30% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. We also find that the gains in performance from the combination of LLMs trained on vast quantities ...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.03504v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We are interested in a novel task, namely lowresource text-to-talking avatar. Given only a fewminute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text. This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from outof-domain a...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Recent years have witnessed an emergence of generative artificial intelligence in various domains, for instance, with a large language model (LLM)-based chatbot (Adamopoulou & Moussiades, 2020), we can obtain high-quality, natural, and realistic dialogue text content. Using an advanced textto-speech (TTS) system (Kim et al., 2021; Ren et al., 2021; Wang et al., 2023; Ye et al., 2023b), we can synthesize “Equal contribution “Interns at ByteDance !Zhejiang University *ByteDance. Correspondence to:...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Our work is majorly related to a low-resource personalized text-to-speech and talking face generation. We discuss related works from these two fields respectively. Previous personalized speech generation approaches, also known as zero-shot multi-speaker TTS, can be categorized into speaker adaptation and speaker encoding...\n",
      "\n",
      "--- METHOD ---\n",
      "overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos. 1. Introduction Recent years have witnessed an emergence of generative artificial intelligence in various domains, for instance, with a large language model (LLM)-based chatbot (Adamopoulou & Moussiades, 2020), we can obtain high-quality, natural, and realistic dialogue text content. Using an advanced textto-speech (TTS) system (Kim et al., 2021; Ren et al., 2021; W...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s In this paper, we present Ada-TTA, an adaptive high-quality text-to-talking avatar synthesis system. Given only a fewminute-long talking person video as training data, with AdaTTA we can synthesize identity-preserving speech given arbitrary input text, and generate the lip-synchronized video. We describe a zero-shot multi-speaker TTS model and a high-quality talking face generation method used to construct the Ada-TTA system. Our experiments demonstrate our system’s ability to synthesize reali...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.13416v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, ...\n",
      "\n",
      "--- METHOD ---\n",
      "to enhance open-source foundation models, namely LLaMA (Touvron et al., 2023). The primary objective of our work is to enable the foundation LLMs (such as LLaMA) to understand developers’ intent while utilizing limited computing resources. Specifically, to generate software engineering (SE)-related data, we guide ChatGPT using a specific prompt that includes the requirements for the newly generated instances (Fig. 2). To ensure ChatGPT comprehends the desired output format and content, we provid...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s to demonstrate the capabilities of SoTaNa in effectively answering Stack Overflow questions, code summarization, and code generation. 2 Background 2.1 Software Development Assistant With the increasing reliance on software systems, the demand for innovative software solutions has surged significantly (DRM Associates, 2002). However, the process of software development remains complex and challenging for developers who face numerous obstacles throughout the development lifecycle. One of the pri...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "s or findings drawn for one model size cannot be directly applied to another size. For instance, SoTaNa-13B achieves the best performance on code summarization when using 5K data, while SoTaNa-7B and SoTaNa-30B did not exhibit the same trend. For code generation, SoTaNa-7B performs exceptionally well when trained on 10K data, whereas SoTaNa-7B and SoTaNa-30B show the worst performance with the same dataset size. The results indicate the importance of careful consideration when selecting the data...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.02499v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically u...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Artificial intelligence (AI) has experienced significant advancements recently. Among these developments, 2023} has particularly stood out due to its ability to reason, comprehend, and interact The ability to execute new tasks based on instructions is a crucial step towards achieving artificial general intelligence, and the remarkable capabilities of large language models (LLMs) have spurred numerous emerging research topics, such as in-context learning [Ram et al. 2023} Xie et al. 2021}, chain-...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Advanced Large Language Model LLMs have exhibited robustness and generalizability through zeroshot and few-shot learning by having parameter sizes exceeding one hundred billion. Notable examples of LLMs include Megatron-turing NLG [Smith et al. 2022] with 530 billion parameters, Gopher 2021) with 280 billion parameters, and PaLM [(Chowdhery et al.||2022] with 540 billion parameters. The scaling of LLM has unlocked new emergent abilities previously unobserved under smaller models [2022a}. These L...\n",
      "\n",
      "--- METHOD ---\n",
      "can be general, effective, and beneficial for many AI tasks. 1 Introduction Artificial intelligence (AI) has experienced significant advancements recently. Among these developments, 2023} has particularly stood out due to its ability to reason, comprehend, and interact The ability to execute new tasks based on instructions is a crucial step towards achieving artificial general intelligence, and the remarkable capabilities of large language models (LLMs) have spurred numerous emerging research to...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging AutoML-GPT’s robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "Our work demonstrates the benefits of building AutoML systems upon GPT. The proposed method can automatically conduct machine learning experiments. This automatic learning dramatically improves training efficiency and enhances the model’s performance. We demonstrate use cases across computer vision, natural questions answering, and classification benchmarks. We further conduct a detailed use case with the unseen datasets and additional interactions between the user and AutoML-GPT. To summarize, ...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.08810v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "3D object reconstruction has long been investigated in computer vision. In this work, we focus on the specific setting of reconstructing a salient foreground object from multiview images and automatically segmenting the object from the background without any annotation, which enables scalable 3D content creation for VR/AR and may open up the possibility to generate free 2D and 3D object annotations at a large scale for supervised-learning tasks. Traditional multi-view stereo [8,32] and recent ne...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Multi-view 3D object reconstruction. The reconstruction of 3D objects from multi-view images has long been studied with broad applications. Aside from multi-view images, accurate object masks are needed to separate the object of interest from its surroundings and optionally provide additional geometric constraints. Multi-view Stereo (MVS)...\n",
      "\n",
      "--- METHOD ---\n",
      "s [40,46] have attained impressive reconstruction quality. However, these methods cannot identify objects and the reconstructed object models are typically coupled with the surrounding background. A straightforward solution is utilizing the foreground object masks to The authors are affiliated with the ZJU-SenseTime Joint Lab of 3D Vision. + Corresponding author: Xiaowei Zhou. Input Images Structure-from-Motion Foreground Reconstruction f R ND DINO Point Cloud Coarse Segmentation Fi Foreground S...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon. The code and supplementary material are available on the project page: https://zju3dv.github.io/autorecon/. 1. Introduction 3D object reconstruction has long been investigated in computer vision. In this work, we focus on the specific setting of reconstructing a salient foreground object from multiview images and automatically segmenting the object from the background without any annotation, ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We present a novel pipeline for fully-automated object discovery and reconstruction from multi-view images, without any human annotation. Experiments conducted on multiple real-world datasets show the effectiveness of our method in building high-quality background-free object models. We also demonstrate the capability of our pipeline in producing high-quality segmentation masks, which are directly applicable to 2D supervised learning. Limitations and future work. Problems faced by neural reconst...\n",
      "\n",
      "-------------------------\n",
      "--- 2307.12612v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "DETR-like models have significantly boosted the performance of detectors and even outperformed classical convolutional models. However, all tokens are treated equally without discrimination brings a redundant computational burden in the traditional encoder structure. The recent sparsification strategies exploit a subset of informative tokens to reduce attention complexity maintaining performance through the sparse encoder. But these methods tend to rely on unreliable model statistics. Moreover, ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Object detection is a fundamental task in computer vision that aims to predict the bounding boxes and classes of objects in an image, as shown in Fig. | (a), which is of great importance in real-world applications. DETR proposed by Carion et al.[1] uses learnable queries to probe image features from the output of Transformer encoders and bipartite graph matching to perform set-based box prediction. DETR-like models [18, 36, 14, 32, 21, 26, 2, 30, 37] have made remarkable progress and gradually b...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Transformer-based detectors. Recently, Carion et al.[1] proposed an end-to-end object detector named DETR (DEtection TRansformer) based on Vision Transformer [7]. DETR transforms object detection into a set prediction task through the backbone, encoder, and decoder and supervises the training process through Hungarian matching algorithms. A lot of recent works[18, 14, 37, 36, 21, 3, 35, 2, 4] have boosted the performance of Transformer-based detectors from the perspective of accelerating trainin...\n",
      "\n",
      "--- METHOD ---\n",
      "s tend to rely on unreliable model statistics. Moreover, simply reducing the token population hinders the detection performance to a large extent, limiting the application of these sparse models. We propose Focus-DETR, which focuses attention on more informative tokens for a better trade-off between computation efficiency and model accuracy. Specifically, we reconstruct the encoder with dual attention, which includes a token scoring mechanism that considers both localization and category semanti...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s show severe performance decay when the Sparse DETR is embedded into the models using learnable queries due to weak correlation between DAM and the retained foreground tokens. However, state-of-the-art DETRlike models, such as DINO [36], have proven that the selected features are preliminary content features without further refinement and could be ambiguous and misleading to the decoder. In this case, DAM’s supervision is inefficient. Moreover, in this monotonous sparse encoder, the number of r...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "This paper proposes Focus-DETR to focus on more informative tokens for a better trade-off between computation efficiency and model accuracy. The core component of Focus-DETR is a multi-level discrimination strategy for feature semantics that utilizes a scoring mechanism considering both position and semantic information. Focus-DETR achieves a better trade-off between computation efficiency and model accuracy by precisely selecting foreground and fine-grained tokens for enhancement. Experimental ...\n",
      "\n",
      "-------------------------\n",
      "--- 2308.03610v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The creation of high-quality 3D avatars has garnered significant interest due to their widespread applications in domains such as game production, social media and communication, augmented and virtual reality (AR/VR), and human-computer interaction. Traditional manual construction of these intricate 3D models is a labor-intensive and time-consuming process, requiring thousands of hours from skilled artists possessing extensive aesthetic and 3D modeling expertise. Consequently, automating the gen...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "2.1. Text-guided 3D content generation The success in text-guided 2D image generation has paved the way for the development of text-guided 3D content generation...\n",
      "\n",
      "--- METHOD ---\n",
      "s pri marily rely on limited visual priors sourced from videos or *These authors contributed equally. reference images, leading to constrained ability to generate creative avatars with complex text prompts. In 2D image generation, diffusion models (Rombach et al./2021}|Saharia| fet al. 2022} (Zhang and Agrawala{2023) illustrate considerable creativity, primarily due to the availability of large-scale text-image pairs. Nevertheless, the scarcity and limited diversity of 3D models present challeng...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s In this section, we illustrate the effectiveness of our proposed method. We demonstrate the efficacy of each proposed strategy and provide a detailed comparison against recent state-of-the-art methods. 4.1. Implementation Details We follow (Sun, Sun, and Chen|2021) to implement the ex plicit NeRF in our method. For each text prompt, we train --- --AvatarVerse for 5000 and 4000 iterations in the coarse stage and mesh refinement stage, respectively. The whole generation process takes around 2 ho...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we introduce AvatarVerse, a novel framework designed to generate high-quality and stable 3D avatars from textual prompts and poses. By employing our trained DensePose-conditioned ControlNet, we facilitate stable partial or full-body control during explicit NeRF optimization. Our 3D avatar outcomes exhibit superior texture and geometry quality, thanks to our progressive high-resolution generation strategy. Furthermore, the generated avatars are easily animatable through skeletal bi...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.10018v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learn...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "In recent years, the development of deep neural networks has led to significant advancements in the field of computer vision [16]. One such architecture is the Visual Transormer (ViT) [5], which utilizes the self-attention mechanism to model long-range dependencies between image eatures. Unlike traditional convolutional neural networks (CNN) [7, 10, 26], which rely on handcrafted hierarchical eature extraction, visual transformers can learn global spatial relationships among image features in a ...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Visual Transformers Visual Transformers (ViT) have recently achieved state-of-the-art performance in many computer vision tasks [5, 19,31]. They adapt self-attention --- --mechanisms to the image domain, allowing to better capture long-range dependencies and contextual information. ViTs have been extended with knowledge distillation [29], using token-level and patch-level transformer layers [8], or progressively downsampling the image [37]. A comprehensive review on ViTs can be found in the work...\n",
      "\n",
      "--- METHOD ---\n",
      "s on various visual recognition tasks [18]. However, in real-world scenarios labeled data can be scarce and expensive to obtain. Therefore, semi * Joint first authors. biimpata@amazon.com Sofia Braun brasofia@amazon.com Victor Martinez vicmg@amazon.com Virginia Fernandez virfer@amazon.com Felipe Bertrand felipb@amazon.com supervised learning (SSL) [40] has emerged as a powerful technique for leveraging unlabeled data to improve the performance of deep neural networks. CNN methods have significan...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "tuning them with 25%, 50%, and 75% of the training data. The validation and test sets remains the same to have a fair comparison of performance. For Semi-ViT, we fine tune and later perform semi-supervised learning using both labeled and unlabelled data. We orchestrate the fine tuning of all models using SageMaker g5 instances, we rely on their hyper-parameter tuner with Bayesian search [23] configured to search hyper-parameters around the original values given in the training code of the three ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We have run experiments with three models: ResNet, ViT, and Semi-Vit on three datasets that were collected from e-commerce data. The use of e-commerce data provided a realistic setting to assess the impact of SSL with a combination of labeled and unlabeled images. Our experiments showed that Semi-ViT can effectively leverage the benefits of SSL to improve the performance in fine-grained classification tasks compared to other architectures. We can see how Semi-ViT obtains accuracy values similar ...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.10202v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: (7) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates sc...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, suc! as fabricating information or producing biased, toxic, or even dangerous content, since LLMs are trained on a wide array of data, which can include low-q...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "LLM Alignments with Human Preferences. LLMs are typically pre-trained on extensive datasets and can be adapted to a wide variety of downstream tasks. One critical aspect of utilizing LLMs effectively is ensuring their alignment with human preferences, which helps in averting responses that are unsafe, toxic, sexually explicit, biased, or criminal (2018). A predominant strategy in achieving this is RLHF. This involves training a reward model based on human feedback and utilizing PPO to improve to...\n",
      "\n",
      "--- METHOD ---\n",
      "s not only increase stability in RLHF training but also achieve higher reward scores and win rates 1 INTRODUCTION Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, suc! as fabricating information or producing...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates 1 INTRODUCTION Large language models (LLMs) have become a fundamental element in advancing natural language processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text that is both semantically and contextually relevant (OpenAI] . Despite these advancements, LLMs have the risk of ...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this report, we identified and analyzied critical impediments in RLHF training of LLMs, namely reward hacking and catastrophic forgetting. These issues emerge due to the variances in learned reward score distributions and the over-optimization of specific training examples, resulting in instabilities in RLHF training. To alleviate these issues, we introduced the Advantage Model and Selective Rehearsal—innovative strategies formulated to stabilize the RLHF training process. The Advantage Model...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.11243v2.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose this metric as a tool to aid in investigating Large Language Models’ (LLM) capabilities in the context of ethics and morality. Results...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      ": In 1950 Alan Turing famously said “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain {Turing} [1946)).” Developmental psychologists have spent decades devising experiments to determine the intelligence and knowledge of infants and children. This field of study has revealed types of knowledge that are in place well b...\n",
      "\n",
      "--- RELATED WORK ---\n",
      ": Previous work that has tested LLMs, specifically GPT-3, has found conflicting evidence of theory of mind (Ullman|{2023],|Kosinski (2023}, Sap and Choi 2022]) in these models. Previous work has also demonstrated that GPT-3 deeply struggles with causal reasoning-based tasks, though it performs well on other vignette tasks (Binz and Schulz}|2023]). One issue that arises in these studies is that LLMs may simply reference published research papers; for example, finding the false-belief task in many...\n",
      "\n",
      "--- METHOD ---\n",
      "ological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on other kinds of information, such as information from explorati...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.08891v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the m...\n",
      "\n",
      "--- METHOD ---\n",
      "s 3.1. Enforce Zero Terminal SNR Table | shows common schedule definitions and their SNR(T) and \\/ay at the terminal timestep T = 1000. None of the schedules have zero terminal SNR. Moreover, cosine schedule deliberately clips 6; to be no greater than 0.999 to prevent terminal SNR from reaching zero. We notice that the noise schedule used by Stable Diffusion is particularly flawed. The terminal SNR is far from reaching zero. Substituting the value into Equation (4) also reveals that the signal i...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "ation shows that these simple changes completely resolve the issue. These flawed designs are not exclusive to Stable Diffusion but general to all diffusion models. We encourage future designs of diffusion models to take this into account. 2. Background Diffusion models [3, 15] involve a forward and a backward process. The forward process destroys information by gradually adding Gaussian noise to the data, commonly according to a non-learned, manually-defined variance schedule 61,..., Sr. Here we...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In summary, we have discovered that diffusion models should use noise schedules with zero terminal SNR and should be sampled starting from the last timestep in order to ensure the training behavior is aligned with inference. We have proposed a simple way to rescale existing noise schedules to enforce zero terminal SNR and a classifier-free guidance rescaling technique to counter image over-exposure. We encourage future designs of diffusion models to take this into account. References Nicholas Gu...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.16806v4.pdf ---\n",
      "--- ABSTRACT ---\n",
      "Large Language Models (LLMs) such as GPT3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models....\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "al details below. Datasets We use the official WMT21 En-De, DeEn, En-Ru and Ru-En News Translation test sets --- --| Translation System | Source MS Time is running out for Iran nuclear deal, Germany says, GPT Time is running out for Iran nuclear deal, Germany says, Die Zeit fiir das Atomabkommen mit dem Iran lauft ab, sagt Deutschland Deutschland sagt, die Zeit fiir das iranische Atomabkommen liuft ab. MS You’re welcome, one moment please. GPT You’re welcome, one moment please. Sie sind willkomm...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "from the results in Figure | by conducting a human evaluation of translation literalness on 6 WMT-language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-(a strong commercial LLM) (Hendy et al., 2023). We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of sp...\n",
      "\n",
      "-------------------------\n",
      "--- 2309.00775v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an image-text pretraining methodology that achieves simultaneous learning of image- and regionlevel representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Unlike standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space ...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The ability to detect a vast array of objects in the real world is fundamental to computer vision and machine learning. This powers a wide range of applications from autonomous agents to search engines. Unfortunately, to date most modern object detectors rely on manually annotated regions and class labels, which is labor-intensive and impractical to scale beyond the order of 10° categories. A new task called open-vocabulary detection (OVD) has been introduced to address the vocabulary limitation...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "s Language-supervised open-vocabulary — recognition. Learning representation for open-vocabulary recognition is a hallmark of general intelligence. Early pioneering works such as DeViSE [16] and ConSE [43] used deep convolutional networks to construct a shared image-text embedding space for zero-shot recognition. To leverage the co-occurrence of image and text in raw internet data, researchers have explored various data sources such as image tags [4, 9, 30], captions [8, 24, 50, 55], alt-texts [...\n",
      "\n",
      "--- METHOD ---\n",
      "ology that achieves simultaneous learning of image- and regionlevel representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Unlike standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space as is customary with the classical MAE method, which causes the model to better learn region-level sem...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s that incorporating the PED pretraining and frozen backbone inference provides large gains in open-vocabulary detection. 4. Experimental Results Pretraining setup. For the image-text pretraining, we use the widely-used ViT-B/16 and ViT-L/16 as the image en during its pretraining. {: Rasheed et al. uses an external MViT detector [41] during pretraining. *: The other ViT-based method [42] report their results on LVIS only. coder, with an input image size of 224. We use the fixed 2D sinusoidal pos...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "We introduce Contrastive Feature Masking Vision Transformer (CFM-ViT) which imbues the image-text pretraining with pixel/region-level semantics for open-vocabulary --- --object detection. By using feature construction and positional embedding dropout, CFM-ViT is simple and scalable, outperforming the state-of-the-art on LVIS open-vocabulary detection benchmark by large margins, and shows very competitive performance on COCO benchmark and zeroshot transfer to Objects365. In addition, CFM-ViT outp...\n",
      "\n",
      "-------------------------\n",
      "--- 2306.05392v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy o...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "The scope of reasoning needed for visual question answering (VQA) is vast, and demands the synthesis of many skills — from grounding language to pixels (Goyal et al., 2017; Radford et al., 2021; Zhai et al., 2022) and spatial reasoning (Hudson and Manning, 2019) to commonsense and knowledgebased reasoning (Marino et al., 2019). Consider the question “Ts the carriage to the right of a horse?”. To consistently answer such questions correctly, a system must recognize that the question is the conjun...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Several recent approaches for reasoning tasks consist of an LM that writes programs and an interpreter for these programs. Liang et al. (2022) applies this approach to robotics. Cheng et al. (2023) introduces a framework for reasoning jointly over tables, text, and images, where the images are represented by image captions. Subramanian et al. (2022) used a syntactic parser and hard-coded rules rather than an LM to aggregate outputs from CLIP (Radford et al., 2021) for zero-shot referring express...\n",
      "\n",
      "--- METHOD ---\n",
      "s (Krishnamurthy and Kollar, 2013), to differentiable neural module networks (NMNs) (Andreas et al., 2016; Hu et al., 2017; Saqur and Narasimhan, 2020)) — offer a potential route to leverage and scale to the compositional nature of visual reasoning as a means to generalize: i.e., infinite use of finite means. However, the modules of an NMN must still be trained jointly on a large dataset, and are also restricted in that they (i) require a parser, which must be modified if modules are added or re...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "s 4.1 Implementation Details See Appendix C for implementation details. 4.2 Datasets The GQA dataset (Hudson and Manning, 2019) contains multi-hop questions generated from human-annotated scene graphs of individual images in Visual Genome (Krishna et al., 2016). The COVR dataset (Bogin et al., 2021) contains multihop questions about sets of images in the Visual Genome and imSitu (Yatskar et al., 2016) datasets. These questions are synthetically generated from templates and are then paraphrased b...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we have introduced a framework for modular few-shot VQA. Our approach prompts an LM to generate a Python program that invokes pretrained visual modules and composes the outputs of these modules to predict the answer. Unlike previous modular VQA techniques, this framework does not require (re-)training modules or a parser. Also, obtaining interpretable module outputs from previous modular approaches is nontrivial (Subramanian et al., 2020), whereas in our approach the modules are f...\n",
      "\n",
      "-------------------------\n",
      "--- 2305.20010v1.pdf ---\n",
      "--- ABSTRACT ---\n",
      "We present “Human or Not?’ an online game inspired by the Turing test, that measures the capability of AI chatbots to mimic humans in dialog, and of humans to tell bots from other humans. Over the course of a month, the game was played by over 1.5 million users who engaged in anonymous two-minute chat sessions with either another human or an AI language model which was prompted to behave like humans. The task of the players was to correctly guess whether they spoke to a person or to an AI. This ...\n",
      "\n",
      "--- METHOD ---\n",
      "for tracking this progress, and it can be re-used in upcoming years as AI agents improve. Future analyses of this data can offer valuable insights into the current capabilities of AI models and the strategies humans use to identify Al-generated text. Below, we outline the design and development process of “Human or Not?” and present an initial analysis of the game’s data. We hope that our setup and findings can provide valuable insights for the ongoing development of AI language models, the desi...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      ". While this experiment calls for many extensions and refinements, these findings already begin to shed light on the inevitable near future which will commingle humans and AI. 1 INTRODUCTION The famous Turing test, originally proposed by Alan Turing in 1950 as “the imitation game” {E950), was proposed as an operational test of intelligence, namely, testing a machine’s ability to exhibit behavior indistinguishable from that of a human. In this proposed test, a human evaluator engages in a natural...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "that she’s talking a bot, she confronts the other user about it. Maria is witty and sassy, uses slang, refuses to answer factual questions, but is overall nice and funny Maria also has some spelling mistakes, she writes short messages without asking too many questions and she doesn’t use capitalization at all. and is bad at math. at Conversation starts now. Adan lives in Long Beach, CA, where the date is Tuesday, May 30, 2023, and the time is 05:48 AM. He tries to convince the other user that he...\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file_name in file_list:\n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"r\") as f:\n",
    "        replaced_text = f.read()\n",
    "        \n",
    "    sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "    section_data = {}\n",
    "    \n",
    "    for i in range(len(sections)-1):\n",
    "        pattern = f\"{sections[i]}(.*?){sections[i+1]}\"\n",
    "        match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "        if match:\n",
    "            section_data[sections[i]] = match.group(1).strip()\n",
    "    \n",
    "    last_section = sections[-1]\n",
    "    pattern = f\"{last_section}(.*?)$\"\n",
    "    match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "    \n",
    "    if match:\n",
    "        section_data[last_section] = match.group(1).strip()\n",
    "    \n",
    "    if not os.path.exists(\"../section_txt/\"):\n",
    "        os.makedirs(\"../section_txt\")\n",
    "    with open(f\"../section_txt/{file_name}_section.txt\", 'w') as f:\n",
    "        f.write(\"\")\n",
    "    print(f\"--- {file_name} ---\")\n",
    "\n",
    "    for sec, content in section_data.items():\n",
    "        print(f\"--- {sec} ---\\n{content[:500]}...\\n\")\n",
    "    \n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    for sec, content in section_data.items():\n",
    "         with open(f\"../section_txt/{file_name}_section.txt\", 'a') as f:\n",
    "            f.write(f\"--- {sec} ---\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "def translate_text_m2m(text: str, source_language: str='en', target_language: str='ko') -> str:\n",
    "    \"\"\"\n",
    "    Translates text from source_language to target_language using facebook/m2m100 model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to translate.\n",
    "        source_language (str): Source language code (ISO 639-1).\n",
    "        target_language (str): Target language code (ISO 639-1).\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 모델과 토크나이저 로드\n",
    "        tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "        model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "        \n",
    "        # 소스 언어 설정\n",
    "        tokenizer.src_lang = source_language\n",
    "        \n",
    "        # 텍스트 토크나이징\n",
    "        encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 번역 생성\n",
    "        generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(target_language))\n",
    "        \n",
    "        # 번역된 토큰 디코딩\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Translation: {translated_text}\")\n",
    "        \n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"번역 중 오류 발생: {e}\")\n",
    "        return \"[번역 실패]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_mt(text: str, source_language: str='en', target_language: str='ko') -> str:\n",
    "    \"\"\"\n",
    "    Translates text from source_language to target_language using facebook/m2m100 model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to translate.\n",
    "        source_language (str): Source language code (ISO 639-1).\n",
    "        target_language (str): Target language code (ISO 639-1).\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 모델과 토크나이저 로드\n",
    "        translator = pipeline(model=\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "        \n",
    "        # 소스 언어 설정\n",
    "        translation = translator(text)\n",
    "        \n",
    "        # 텍스트 토크나이징\n",
    "        if isinstance(translation, list):\n",
    "            return translation[0]['translation_text'] if translation else \"\"\n",
    "        elif isinstance(translation, dict):\n",
    "            return translation.get('translation_text', '')\n",
    "        else:\n",
    "            return str(translation)\n",
    "    except Exception as e:\n",
    "        print(f\"번역 중 오류 발생: {e}\")\n",
    "        return \"[번역 실패]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/janghyeonbin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "# NLTK 데이터 다운로드 (처음 한 번만 실행)\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text(text, max_tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    텍스트를 최대 토큰 수에 맞게 청킹합니다.\n",
    "    \n",
    "    :param text: 원본 텍스트\n",
    "    :param max_tokens: 한 청크당 최대 토큰 수\n",
    "    :param tokenizer: Hugging Face 토크나이저\n",
    "    :return: 청크로 분할된 텍스트 리스트\n",
    "    \"\"\"\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        if current_length + sentence_length <= max_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_length = sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 최대 입력 토큰 수: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 사용 중인 번역 모델에 맞는 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "# 모델의 최대 입력 길이 확인\n",
    "max_tokens = tokenizer.model_max_length\n",
    "print(f\"모델의 최대 입력 토큰 수: {max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    텍스트에서 섹션을 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 전체 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        dict: 섹션별 텍스트 딕셔너리\n",
    "    \"\"\"\n",
    "    sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "    section_data = {}\n",
    "\n",
    "    for j in range(len(sections)-1):\n",
    "        pattern = f\"{sections[j]}(.*?){sections[j+1]}\"\n",
    "        match = re.search(pattern, text, re.S | re.I)\n",
    "        if match:\n",
    "            section_data[sections[j]] = match.group(1).strip()\n",
    "\n",
    "    last_section = sections[-1]\n",
    "    pattern = f\"{last_section}(.*?)$\"\n",
    "    match = re.search(pattern, text, re.S | re.I)\n",
    "\n",
    "    if match:\n",
    "        section_data[last_section] = match.group(1).strip()\n",
    "\n",
    "    return section_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sections_m2m(section_data, file_name, target_language='ko', source_language='en', tokenizer=None, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    섹션별 텍스트를 청킹하고, 각 청크를 번역한 후 파일에 저장합니다.\n",
    "    \n",
    "    :param section_data: 섹션별 텍스트 딕셔너리\n",
    "    :param file_name: 번역 결과를 저장할 파일 이름\n",
    "    :param target_language: 번역할 대상 언어 코드\n",
    "    :param source_language: 원본 언어 코드\n",
    "    :param tokenizer: 텍스트 청킹에 사용할 토크나이저\n",
    "    :param max_tokens: 한 청크당 최대 토큰 수\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for sec, content in section_data.items():\n",
    "        # 텍스트 청킹\n",
    "        chunks = chunk_text(content, max_tokens, tokenizer)\n",
    "        \n",
    "        # 청크별 번역\n",
    "        translated_chunks = []\n",
    "        for chunk in chunks:\n",
    "            translation = translate_text_m2m(chunk, target_language=target_language, source_language=source_language)\n",
    "            translated_chunks.append(translation)\n",
    "        \n",
    "        # 청크를 합쳐 최종 번역 텍스트 생성\n",
    "        translated_text = \" \".join(translated_chunks)\n",
    "        \n",
    "        # 파일에 저장\n",
    "        with open(f\"../translated_txt/{file_name}_translated.txt\", 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"--- {sec} ---\\n\")\n",
    "            f.write(translated_text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sections_mt(section_data, file_name, target_language='ko', source_language='en', tokenizer=None, max_tokens=1024):\n",
    "    for sec, content in section_data.items():\n",
    "        try:\n",
    "            # 텍스트 청킹\n",
    "            chunks = chunk_text(content, max_tokens, tokenizer)\n",
    "            \n",
    "            # 청크별 번역\n",
    "            translated_chunks = []\n",
    "            for chunk in chunks:\n",
    "                translation = translate_text_mt(chunk, target_language=target_language, source_language=source_language)\n",
    "                translated_chunks.append(translation)\n",
    "            \n",
    "            # 청크를 합쳐 최종 번역 텍스트 생성\n",
    "            translated_text = \" \".join(translated_chunks)\n",
    "            \n",
    "            # 파일에 저장\n",
    "            with open(f\"../translated_txt/{file_name}_translated.txt\", 'a', encoding='utf-8') as f:\n",
    "                f.write(f\"--- {sec} ---\\n\")\n",
    "                f.write(translated_text + \"\\n\\n\")\n",
    "            \n",
    "            print(f\"{file_name} 섹션 '{sec}' 번역 완료.\")\n",
    "        except Exception as e:\n",
    "            print(f\"{file_name} 섹션 '{sec}' 번역 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2306.10008v2.pdf 섹션 'ABSTRACT' 번역 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Your input_length: 483 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2306.10008v2.pdf 섹션 'INTRODUCTION' 번역 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2306.10008v2.pdf 섹션 'RELATED WORK' 번역 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2306.10008v2.pdf 섹션 'METHOD' 번역 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 섹션별 번역 수행\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m translate_sections_mt(\n\u001b[1;32m     26\u001b[0m     section_data\u001b[38;5;241m=\u001b[39msection_data,\n\u001b[1;32m     27\u001b[0m     file_name\u001b[38;5;241m=\u001b[39mfile_name,\n\u001b[1;32m     28\u001b[0m     target_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mko\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     source_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     31\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m번역 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_translated_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[86], line 10\u001b[0m, in \u001b[0;36mtranslate_sections_mt\u001b[0;34m(section_data, file_name, target_language, source_language, tokenizer, max_tokens)\u001b[0m\n\u001b[1;32m      8\u001b[0m translated_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m---> 10\u001b[0m     translation \u001b[38;5;241m=\u001b[39m translate_text_mt(chunk, target_language\u001b[38;5;241m=\u001b[39mtarget_language, source_language\u001b[38;5;241m=\u001b[39msource_language)\n\u001b[1;32m     11\u001b[0m     translated_chunks\u001b[38;5;241m.\u001b[39mappend(translation)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 청크를 합쳐 최종 번역 텍스트 생성\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[84], line 18\u001b[0m, in \u001b[0;36mtranslate_text_mt\u001b[0;34m(text, source_language, target_language)\u001b[0m\n\u001b[1;32m     15\u001b[0m translator \u001b[38;5;241m=\u001b[39m pipeline(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-tc-big-en-ko\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 소스 언어 설정\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m translation \u001b[38;5;241m=\u001b[39m translator(text)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 텍스트 토크나이징\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(translation, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:376\u001b[0m, in \u001b[0;36mTranslationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Translate the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m          token ids of the translation.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/base.py:1301\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/base.py:1308\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1307\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1308\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/base.py:1208\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1207\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1208\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:196\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    194\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 196\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    197\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/generation/utils.py:2283\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2276\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2277\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2278\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[1;32m   2282\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2283\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2284\u001b[0m         input_ids,\n\u001b[1;32m   2285\u001b[0m         beam_scorer,\n\u001b[1;32m   2286\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2287\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2288\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2289\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2291\u001b[0m     )\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2294\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2295\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2296\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2297\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2304\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/generation/utils.py:3503\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3500\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3503\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3506\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3507\u001b[0m     outputs,\n\u001b[1;32m   3508\u001b[0m     model_kwargs,\n\u001b[1;32m   3509\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3510\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:1401\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1398\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1399\u001b[0m         )\n\u001b[0;32m-> 1401\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1402\u001b[0m     input_ids,\n\u001b[1;32m   1403\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1404\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1405\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[1;32m   1406\u001b[0m     decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1407\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1408\u001b[0m     decoder_head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1409\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1410\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1411\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1412\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1413\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1414\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1415\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1416\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1417\u001b[0m )\n\u001b[1;32m   1418\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1420\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:1195\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1189\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1190\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1191\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1192\u001b[0m     )\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1196\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1197\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1198\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1199\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1200\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1201\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1202\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1203\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1204\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:995\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    983\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    984\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m         use_cache,\n\u001b[1;32m    993\u001b[0m     )\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 995\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    996\u001b[0m         hidden_states,\n\u001b[1;32m    997\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    998\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    999\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1000\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1001\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1002\u001b[0m             cross_attn_head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m cross_attn_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         ),\n\u001b[1;32m   1004\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m   1005\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1006\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:405\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    403\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    406\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    407\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    408\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    409\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    410\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    411\u001b[0m )\n\u001b[1;32m    412\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    413\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:161\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    158\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# get key, value proj\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# is checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m     is_cross_attention\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    170\u001b[0m ):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file_name in file_list:\n",
    "    # 번역된 파일이 이미 존재하면 건너뛰기\n",
    "    output_translated_path = f\"../translated_txt/{file_name}_translated.txt\"\n",
    "    # if os.path.exists(output_translated_path):\n",
    "    #     print(f\"이미 번역 파일이 존재하여 건너뜁니다: {output_translated_path}\")\n",
    "    #     continue\n",
    "    \n",
    "    # Cleaned 텍스트 파일 읽기\n",
    "    with open(f\"../re_txt/{file_name}_cleaned.txt\", \"r\", encoding='utf-8') as f:\n",
    "        replaced_text = f.read()\n",
    "        \n",
    "    # 섹션 추출\n",
    "    section_data = extract_sections(replaced_text)\n",
    "\n",
    "    # 섹션 데이터가 유효한지 확인\n",
    "    if not section_data:\n",
    "        print(f\"{file_name}의 섹션 추출 실패.\")\n",
    "        continue\n",
    "\n",
    "    # 번역된 텍스트 파일 초기화\n",
    "    with open(output_translated_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\")\n",
    "    \n",
    "    # 섹션별 번역 수행\n",
    "    translate_sections_mt(\n",
    "        section_data=section_data,\n",
    "        file_name=file_name,\n",
    "        target_language=\"ko\",\n",
    "        source_language=\"en\",\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    print(f\"번역 완료: {output_translated_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/janghyeonbin/OCR'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir=\"../../OCR\"\n",
    "root_abs_dir=os.path.abspath(root_dir)\n",
    "root_abs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text_dir=f\"{root_abs_dir}/section_txt\"\n",
    "translated_text_dir=f\"{root_abs_dir}/translated_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2305.11759v1.pdf_section.txt',\n",
       " '2306.17840v4.pdf_section.txt',\n",
       " '2305.05706v1.pdf_section.txt',\n",
       " '2307.08621v4.pdf_section.txt',\n",
       " '2306.15658v1.pdf_section.txt',\n",
       " '2307.02053v1.pdf_section.txt',\n",
       " '2306.08568v1.pdf_section.txt',\n",
       " '2305.10853v2.pdf_section.txt',\n",
       " '2305.05189v4.pdf_section.txt',\n",
       " '2309.04354v1.pdf_section.txt',\n",
       " '2307.12533v3.pdf_section.txt',\n",
       " '2305.13534v1.pdf_section.txt',\n",
       " '2306.17319v1.pdf_section.txt',\n",
       " '2305.11129v2.pdf_section.txt',\n",
       " '2305.13304v1.pdf_section.txt',\n",
       " '2307.01200v3.pdf_section.txt',\n",
       " '2306.15354v3.pdf_section.txt',\n",
       " '2305.02783v4.pdf_section.txt',\n",
       " '2307.04603v5.pdf_section.txt',\n",
       " '2309.09958v1.pdf_section.txt',\n",
       " '2308.01320v1.pdf_section.txt',\n",
       " '2305.10320v1.pdf_section.txt',\n",
       " '2305.05432v1.pdf_section.txt',\n",
       " '2305.16367v1.pdf_section.txt',\n",
       " '2307.14460v1.pdf_section.txt',\n",
       " '2305.11837v2.pdf_section.txt',\n",
       " '2307.08041v2.pdf_section.txt',\n",
       " '2305.17306v1.pdf_section.txt',\n",
       " '2307.10168v2.pdf_section.txt',\n",
       " '2307.02628v1.pdf_section.txt',\n",
       " '2307.13974v1.pdf_section.txt',\n",
       " '2306.06638v1.pdf_section.txt',\n",
       " '2306.07954v2.pdf_section.txt',\n",
       " '2305.03726v1.pdf_section.txt',\n",
       " '2306.01754v1.pdf_section.txt',\n",
       " '2306.15091v1.pdf_section.txt',\n",
       " '2305.07214v1.pdf_section.txt',\n",
       " '2307.16372v1.pdf_section.txt',\n",
       " '2306.04632v1.pdf_section.txt',\n",
       " '2309.11500v4.pdf_section.txt',\n",
       " '2305.05973v3.pdf_section.txt',\n",
       " '2305.11000v2.pdf_section.txt',\n",
       " '2308.03291v3.pdf_section.txt',\n",
       " '2308.03280v1.pdf_section.txt',\n",
       " '2305.18247v2.pdf_section.txt',\n",
       " '2306.08205v2.pdf_section.txt',\n",
       " '2307.12612v1.pdf_section.txt',\n",
       " '2305.16704v1.pdf_section.txt',\n",
       " '2306.06092v1.pdf_section.txt',\n",
       " '2305.03027v1.pdf_section.txt',\n",
       " '2305.10425v1.pdf_section.txt',\n",
       " '2305.10142v1.pdf_section.txt',\n",
       " '2309.01826v2.pdf_section.txt',\n",
       " '2306.12156v1.pdf_section.txt',\n",
       " '2305.06474v1.pdf_section.txt',\n",
       " '2309.08586v2.pdf_section.txt',\n",
       " '2307.10373v3.pdf_section.txt',\n",
       " '2306.07174v1.pdf_section.txt',\n",
       " '2308.06261v1.pdf_section.txt',\n",
       " '2309.03199v2.pdf_section.txt',\n",
       " '2309.03926v1.pdf_section.txt',\n",
       " '2308.01477v1.pdf_section.txt',\n",
       " '2305.02549v2.pdf_section.txt',\n",
       " '2309.08773v1.pdf_section.txt',\n",
       " '2306.04619v1.pdf_section.txt',\n",
       " '2306.00008v2.pdf_section.txt',\n",
       " '2305.08810v1.pdf_section.txt',\n",
       " '2309.02119v3.pdf_section.txt',\n",
       " '2306.09782v2.pdf_section.txt',\n",
       " '2308.02510v2.pdf_section.txt',\n",
       " '2305.10431v2.pdf_section.txt',\n",
       " '2305.04790v3.pdf_section.txt',\n",
       " '2306.07941v1.pdf_section.txt',\n",
       " '2305.08850v2.pdf_section.txt',\n",
       " '2306.09327v1.pdf_section.txt',\n",
       " '2305.02483v2.pdf_section.txt',\n",
       " '2309.10917v1.pdf_section.txt',\n",
       " '2309.06440v1.pdf_section.txt',\n",
       " '2305.18286v1.pdf_section.txt',\n",
       " '2309.08051v2.pdf_section.txt',\n",
       " '2306.10007v2.pdf_section.txt',\n",
       " '2307.15042v2.pdf_section.txt',\n",
       " '2308.13416v1.pdf_section.txt',\n",
       " '2305.16867v1.pdf_section.txt',\n",
       " '2306.02254v2.pdf_section.txt',\n",
       " '2307.02499v1.pdf_section.txt',\n",
       " '2309.07314v1.pdf_section.txt',\n",
       " '2309.08827v1.pdf_section.txt',\n",
       " '2306.04235v1.pdf_section.txt',\n",
       " '2307.03183v1.pdf_section.txt',\n",
       " '2309.00986v1.pdf_section.txt',\n",
       " '2306.07944v1.pdf_section.txt',\n",
       " '2307.03917v3.pdf_section.txt',\n",
       " '2308.04623v1.pdf_section.txt',\n",
       " '2308.03757v1.pdf_section.txt',\n",
       " '2307.08674v3.pdf_section.txt',\n",
       " '2309.09390v1.pdf_section.txt',\n",
       " '2305.06456v3.pdf_section.txt',\n",
       " '2308.05221v1.pdf_section.txt',\n",
       " '2306.02982v2.pdf_section.txt',\n",
       " '2308.03421v3.pdf_section.txt',\n",
       " '2306.09635v2.pdf_section.txt',\n",
       " '2308.16582v2.pdf_section.txt',\n",
       " '2305.13050v1.pdf_section.txt',\n",
       " '2305.10601v2.pdf_section.txt',\n",
       " '2305.07027v1.pdf_section.txt',\n",
       " '2308.01300v2.pdf_section.txt',\n",
       " '2309.06126v1.pdf_section.txt',\n",
       " '2307.13908v1.pdf_section.txt',\n",
       " '2307.11526v2.pdf_section.txt',\n",
       " '2306.17492v2.pdf_section.txt',\n",
       " '2306.00378v1.pdf_section.txt',\n",
       " '2307.04686v2.pdf_section.txt',\n",
       " '2308.05734v3.pdf_section.txt',\n",
       " '2308.06873v2.pdf_section.txt',\n",
       " '2308.07926v2.pdf_section.txt',\n",
       " '2305.05383v1.pdf_section.txt',\n",
       " '2309.08210v1.pdf_section.txt',\n",
       " '2305.02790v1.pdf_section.txt',\n",
       " '2305.07804v4.pdf_section.txt',\n",
       " '2305.09664v2.pdf_section.txt',\n",
       " '2305.03043v1.pdf_section.txt',\n",
       " '2309.08628v3.pdf_section.txt',\n",
       " '2305.09975v1.pdf_section.txt',\n",
       " '2309.07906v3.pdf_section.txt',\n",
       " '2306.03504v2.pdf_section.txt',\n",
       " '2308.08316v3.pdf_section.txt',\n",
       " '2305.11364v2.pdf_section.txt',\n",
       " '2305.18802v1.pdf_section.txt',\n",
       " '2306.08133v2.pdf_section.txt',\n",
       " '2307.06940v1.pdf_section.txt',\n",
       " '2305.06424v4.pdf_section.txt',\n",
       " '2309.11497v2.pdf_section.txt',\n",
       " '2305.18373v1.pdf_section.txt',\n",
       " '2306.04050v2.pdf_section.txt',\n",
       " '2309.05767v2.pdf_section.txt',\n",
       " '2308.07395v1.pdf_section.txt',\n",
       " '2307.16125v2.pdf_section.txt',\n",
       " '2306.13776v1.pdf_section.txt',\n",
       " '2308.13785v1.pdf_section.txt',\n",
       " '2306.04076v1.pdf_section.txt',\n",
       " '2305.20010v1.pdf_section.txt',\n",
       " '2305.11694v2.pdf_section.txt',\n",
       " '2308.08089v1.pdf_section.txt',\n",
       " '2309.00775v1.pdf_section.txt',\n",
       " '2305.03981v1.pdf_section.txt',\n",
       " '2305.15779v1.pdf_section.txt',\n",
       " '2305.07243v2.pdf_section.txt',\n",
       " '2309.02040v2.pdf_section.txt',\n",
       " '2305.06218v1.pdf_section.txt',\n",
       " '2306.04009v1.pdf_section.txt',\n",
       " '2307.00119v1.pdf_section.txt',\n",
       " '2306.06546v2.pdf_section.txt',\n",
       " '2307.12854v1.pdf_section.txt',\n",
       " '2307.13702v1.pdf_section.txt',\n",
       " '2309.03185v1.pdf_section.txt',\n",
       " '2305.02499v1.pdf_section.txt',\n",
       " '2307.14225v1.pdf_section.txt',\n",
       " '2306.02245v2.pdf_section.txt',\n",
       " '2309.07870v3.pdf_section.txt',\n",
       " '2305.08275v4.pdf_section.txt',\n",
       " '2307.09793v1.pdf_section.txt',\n",
       " '2307.11418v3.pdf_section.txt',\n",
       " '2308.01499v1.pdf_section.txt',\n",
       " '2308.03610v1.pdf_section.txt',\n",
       " '2307.10159v1.pdf_section.txt',\n",
       " '2306.10231v1.pdf_section.txt',\n",
       " '2307.03692v1.pdf_section.txt',\n",
       " '2306.02858v4.pdf_section.txt',\n",
       " '2305.04268v1.pdf_section.txt',\n",
       " '2309.07062v1.pdf_section.txt',\n",
       " '2305.17098v2.pdf_section.txt',\n",
       " '2305.06404v1.pdf_section.txt',\n",
       " '2308.05884v1.pdf_section.txt',\n",
       " '2307.14620v1.pdf_section.txt',\n",
       " '2308.00113v2.pdf_section.txt',\n",
       " '2308.05960v1.pdf_section.txt',\n",
       " '2305.05176v1.pdf_section.txt',\n",
       " '2307.15131v2.pdf_section.txt',\n",
       " '2307.12169v5.pdf_section.txt',\n",
       " '2305.09662v1.pdf_section.txt',\n",
       " '2306.16934v2.pdf_section.txt',\n",
       " '2306.16009v1.pdf_section.txt',\n",
       " '2305.03509v3.pdf_section.txt',\n",
       " '2305.05845v1.pdf_section.txt',\n",
       " '2305.10841v2.pdf_section.txt',\n",
       " '2309.02186v1.pdf_section.txt',\n",
       " '2307.16890v2.pdf_section.txt',\n",
       " '2309.00908v1.pdf_section.txt',\n",
       " '2306.01841v1.pdf_section.txt',\n",
       " '2309.04827v1.pdf_section.txt',\n",
       " '2305.16843v1.pdf_section.txt',\n",
       " '2308.06125v1.pdf_section.txt',\n",
       " '2306.03203v1.pdf_section.txt',\n",
       " '2305.07677v2.pdf_section.txt',\n",
       " '2308.15930v3.pdf_section.txt',\n",
       " '2305.07378v1.pdf_section.txt',\n",
       " '2308.01904v1.pdf_section.txt',\n",
       " '2306.16700v2.pdf_section.txt',\n",
       " '2306.10785v1.pdf_section.txt',\n",
       " '2308.04729v1.pdf_section.txt',\n",
       " '2307.11795v1.pdf_section.txt',\n",
       " '2305.09761v1.pdf_section.txt',\n",
       " '2305.06908v4.pdf_section.txt',\n",
       " '2306.09864v1.pdf_section.txt',\n",
       " '2306.05392v1.pdf_section.txt',\n",
       " '2309.07990v2.pdf_section.txt',\n",
       " '2305.16411v1.pdf_section.txt',\n",
       " '2309.03907v1.pdf_section.txt',\n",
       " '2305.02412v2.pdf_section.txt',\n",
       " '2306.05399v2.pdf_section.txt',\n",
       " '2305.09764v1.pdf_section.txt',\n",
       " '2305.10688v2.pdf_section.txt',\n",
       " '2305.09636v1.pdf_section.txt',\n",
       " '2306.10008v2.pdf_section.txt',\n",
       " '2305.08848v1.pdf_section.txt',\n",
       " '2307.13383v1.pdf_section.txt',\n",
       " '2307.06949v2.pdf_section.txt',\n",
       " '2305.08891v4.pdf_section.txt',\n",
       " '2306.01741v1.pdf_section.txt',\n",
       " '2309.12311v1.pdf_section.txt',\n",
       " '2305.10973v2.pdf_section.txt',\n",
       " '2309.06802v1.pdf_section.txt',\n",
       " '2308.07968v1.pdf_section.txt',\n",
       " '2309.10279v1.pdf_section.txt',\n",
       " '2305.09148v1.pdf_section.txt',\n",
       " '2305.05591v1.pdf_section.txt',\n",
       " '2305.10018v1.pdf_section.txt',\n",
       " '2305.03210v2.pdf_section.txt',\n",
       " '2307.01229v1.pdf_section.txt',\n",
       " '2308.01734v1.pdf_section.txt',\n",
       " '2309.08804v1.pdf_section.txt',\n",
       " '2307.06925v1.pdf_section.txt',\n",
       " '2305.13077v1.pdf_section.txt',\n",
       " '2309.04663v2.pdf_section.txt',\n",
       " '2307.09233v3.pdf_section.txt',\n",
       " '2306.13455v3.pdf_section.txt',\n",
       " '2309.10537v1.pdf_section.txt',\n",
       " '2305.04966v2.pdf_section.txt',\n",
       " '2308.02453v3.pdf_section.txt',\n",
       " '2306.03083v1.pdf_section.txt',\n",
       " '2305.11308v2.pdf_section.txt',\n",
       " '2306.09329v1.pdf_section.txt',\n",
       " '2309.11523v5.pdf_section.txt',\n",
       " '2306.06044v2.pdf_section.txt',\n",
       " '2307.09668v1.pdf_section.txt',\n",
       " '2306.14289v2.pdf_section.txt',\n",
       " '2308.07891v1.pdf_section.txt',\n",
       " '2305.16806v4.pdf_section.txt',\n",
       " '2307.03322v1.pdf_section.txt',\n",
       " '2305.11778v1.pdf_section.txt',\n",
       " '2309.07749v1.pdf_section.txt',\n",
       " '2308.16876v2.pdf_section.txt',\n",
       " '2306.05410v1.pdf_section.txt',\n",
       " '2309.00615v1.pdf_section.txt',\n",
       " '2309.10202v1.pdf_section.txt',\n",
       " '2309.09400v1.pdf_section.txt',\n",
       " '2305.11243v2.pdf_section.txt',\n",
       " '2309.08172v2.pdf_section.txt',\n",
       " '2306.03514v3.pdf_section.txt',\n",
       " '2305.10763v1.pdf_section.txt',\n",
       " '2308.13404v1.pdf_section.txt',\n",
       " '2308.16824v2.pdf_section.txt',\n",
       " '2305.12001v2.pdf_section.txt',\n",
       " '2305.11337v1.pdf_section.txt',\n",
       " '2309.04269v1.pdf_section.txt']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text_lists=os.listdir(original_text_dir)\n",
    "original_text_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2307.08621v4.pdf_translated.txt',\n",
       " '2308.15930v3.pdf_translated.txt',\n",
       " '2306.07941v1.pdf_translated.txt',\n",
       " '2307.11418v3.pdf_translated.txt',\n",
       " '2308.05221v1.pdf_translated.txt',\n",
       " '2309.11500v4.pdf_translated.txt',\n",
       " '2308.01320v1.pdf_translated.txt',\n",
       " '2305.16704v1.pdf_translated.txt',\n",
       " '2306.07944v1.pdf_translated.txt',\n",
       " '2305.09636v1.pdf_translated.txt',\n",
       " '2306.17840v4.pdf_translated.txt',\n",
       " '2306.16009v1.pdf_translated.txt',\n",
       " '2307.14620v1.pdf_translated.txt',\n",
       " '2308.08089v1.pdf_translated.txt',\n",
       " '2307.00119v1.pdf_translated.txt',\n",
       " '2306.16934v2.pdf_translated.txt',\n",
       " '2305.12001v2.pdf_translated.txt',\n",
       " '2306.10785v1.pdf_translated.txt',\n",
       " '2306.00378v1.pdf_translated.txt',\n",
       " '2309.09400v1.pdf_translated.txt',\n",
       " '2308.07968v1.pdf_translated.txt',\n",
       " '2309.07990v2.pdf_translated.txt',\n",
       " '2306.02982v2.pdf_translated.txt',\n",
       " '2307.04603v5.pdf_translated.txt',\n",
       " '2307.14460v1.pdf_translated.txt',\n",
       " '2309.08586v2.pdf_translated.txt',\n",
       " '2308.05734v3.pdf_translated.txt',\n",
       " '2306.09782v2.pdf_translated.txt',\n",
       " '2306.17319v1.pdf_translated.txt',\n",
       " '2309.00986v1.pdf_translated.txt',\n",
       " '2308.01300v2.pdf_translated.txt',\n",
       " '2305.07027v1.pdf_translated.txt',\n",
       " '2305.09761v1.pdf_translated.txt',\n",
       " '2305.07214v1.pdf_translated.txt',\n",
       " '2305.13050v1.pdf_translated.txt',\n",
       " '2305.18802v1.pdf_translated.txt',\n",
       " '2305.03043v1.pdf_translated.txt',\n",
       " '2305.09764v1.pdf_translated.txt',\n",
       " '2308.06125v1.pdf_translated.txt',\n",
       " '2306.03504v2.pdf_translated.txt',\n",
       " '2308.06873v2.pdf_translated.txt',\n",
       " '2305.08850v2.pdf_translated.txt',\n",
       " '2308.06261v1.pdf_translated.txt',\n",
       " '2309.03907v1.pdf_translated.txt',\n",
       " '2307.14225v1.pdf_translated.txt',\n",
       " '2305.02412v2.pdf_translated.txt',\n",
       " '2305.15779v1.pdf_translated.txt',\n",
       " '2305.11000v2.pdf_translated.txt',\n",
       " '2309.11497v2.pdf_translated.txt',\n",
       " '2309.04827v1.pdf_translated.txt',\n",
       " '2305.11759v1.pdf_translated.txt',\n",
       " '2308.07926v2.pdf_translated.txt',\n",
       " '2307.01200v3.pdf_translated.txt',\n",
       " '2305.10142v1.pdf_translated.txt',\n",
       " '2308.16824v2.pdf_translated.txt',\n",
       " '2308.03610v1.pdf_translated.txt',\n",
       " '2305.17306v1.pdf_translated.txt',\n",
       " '2307.10159v1.pdf_translated.txt',\n",
       " '2308.04623v1.pdf_translated.txt',\n",
       " '2309.00615v1.pdf_translated.txt',\n",
       " '2305.08848v1.pdf_translated.txt',\n",
       " '2309.04269v1.pdf_translated.txt',\n",
       " '2309.07749v1.pdf_translated.txt',\n",
       " '2309.00775v1.pdf_translated.txt',\n",
       " '2308.05884v1.pdf_translated.txt',\n",
       " '2307.15042v2.pdf_translated.txt',\n",
       " '2305.05176v1.pdf_translated.txt',\n",
       " '2307.03692v1.pdf_translated.txt',\n",
       " '2305.06424v4.pdf_translated.txt',\n",
       " '2307.01229v1.pdf_translated.txt',\n",
       " '2305.11364v2.pdf_translated.txt',\n",
       " '2306.04632v1.pdf_translated.txt',\n",
       " '2307.02628v1.pdf_translated.txt',\n",
       " '2306.08133v2.pdf_translated.txt',\n",
       " '2306.09327v1.pdf_translated.txt',\n",
       " '2305.03027v1.pdf_translated.txt',\n",
       " '2309.11523v5.pdf_translated.txt',\n",
       " '2309.08051v2.pdf_translated.txt',\n",
       " '2305.11337v1.pdf_translated.txt',\n",
       " '2307.09233v3.pdf_translated.txt',\n",
       " '2305.10763v1.pdf_translated.txt',\n",
       " '2309.04663v2.pdf_translated.txt',\n",
       " '2305.07243v2.pdf_translated.txt',\n",
       " '2306.15091v1.pdf_translated.txt',\n",
       " '2306.08205v2.pdf_translated.txt',\n",
       " '2308.07395v1.pdf_translated.txt',\n",
       " '2305.08891v4.pdf_translated.txt',\n",
       " '2307.11795v1.pdf_translated.txt',\n",
       " '2305.06218v1.pdf_translated.txt',\n",
       " '2305.05845v1.pdf_translated.txt',\n",
       " '2305.02499v1.pdf_translated.txt',\n",
       " '2309.04354v1.pdf_translated.txt',\n",
       " '2308.16876v2.pdf_translated.txt',\n",
       " '2306.02254v2.pdf_translated.txt',\n",
       " '2306.05410v1.pdf_translated.txt',\n",
       " '2309.08210v1.pdf_translated.txt',\n",
       " '2306.06546v2.pdf_translated.txt',\n",
       " '2309.00908v1.pdf_translated.txt',\n",
       " '2306.13455v3.pdf_translated.txt',\n",
       " '2309.10202v1.pdf_translated.txt',\n",
       " '2309.07062v1.pdf_translated.txt',\n",
       " '2307.09668v1.pdf_translated.txt',\n",
       " '2305.08810v1.pdf_translated.txt',\n",
       " '2307.08674v3.pdf_translated.txt',\n",
       " '2308.01499v1.pdf_translated.txt',\n",
       " '2309.06440v1.pdf_translated.txt',\n",
       " '2306.04619v1.pdf_translated.txt',\n",
       " '2305.18286v1.pdf_translated.txt',\n",
       " '2305.11778v1.pdf_translated.txt',\n",
       " '2305.10688v2.pdf_translated.txt',\n",
       " '2305.03210v2.pdf_translated.txt',\n",
       " '2309.08827v1.pdf_translated.txt',\n",
       " '2306.04076v1.pdf_translated.txt',\n",
       " '2309.03926v1.pdf_translated.txt',\n",
       " '2308.04729v1.pdf_translated.txt',\n",
       " '2306.01754v1.pdf_translated.txt',\n",
       " '2307.06949v2.pdf_translated.txt',\n",
       " '2305.09664v2.pdf_translated.txt',\n",
       " '2305.10431v2.pdf_translated.txt',\n",
       " '2308.03421v3.pdf_translated.txt',\n",
       " '2306.03514v3.pdf_translated.txt',\n",
       " '2307.15131v2.pdf_translated.txt',\n",
       " '2305.06404v1.pdf_translated.txt',\n",
       " '2305.04268v1.pdf_translated.txt',\n",
       " '2307.11526v2.pdf_translated.txt',\n",
       " '2307.08041v2.pdf_translated.txt',\n",
       " '2305.05432v1.pdf_translated.txt',\n",
       " '2305.13534v1.pdf_translated.txt',\n",
       " '2309.02040v2.pdf_translated.txt',\n",
       " '2307.12533v3.pdf_translated.txt',\n",
       " '2308.07891v1.pdf_translated.txt',\n",
       " '2309.07314v1.pdf_translated.txt',\n",
       " '2306.09635v2.pdf_translated.txt',\n",
       " '2309.09958v1.pdf_translated.txt',\n",
       " '2306.17492v2.pdf_translated.txt',\n",
       " '2305.04966v2.pdf_translated.txt',\n",
       " '2308.08316v3.pdf_translated.txt',\n",
       " '2305.08275v4.pdf_translated.txt',\n",
       " '2308.01904v1.pdf_translated.txt',\n",
       " '2306.09329v1.pdf_translated.txt',\n",
       " '2308.01477v1.pdf_translated.txt',\n",
       " '2307.16890v2.pdf_translated.txt',\n",
       " '2308.05960v1.pdf_translated.txt',\n",
       " '2305.03726v1.pdf_translated.txt',\n",
       " '2305.11837v2.pdf_translated.txt',\n",
       " '2305.18247v2.pdf_translated.txt',\n",
       " '2307.12854v1.pdf_translated.txt',\n",
       " '2306.13776v1.pdf_translated.txt',\n",
       " '2309.06126v1.pdf_translated.txt',\n",
       " '2305.10018v1.pdf_translated.txt',\n",
       " '2306.10008v2.pdf_translated.txt',\n",
       " '2306.12156v1.pdf_translated.txt',\n",
       " '2309.08804v1.pdf_translated.txt',\n",
       " '2305.07677v2.pdf_translated.txt',\n",
       " '2306.15658v1.pdf_translated.txt',\n",
       " '2307.09793v1.pdf_translated.txt',\n",
       " '2309.07906v3.pdf_translated.txt',\n",
       " '2305.20010v1.pdf_translated.txt',\n",
       " '2308.03280v1.pdf_translated.txt',\n",
       " '2306.02245v2.pdf_translated.txt',\n",
       " '2307.02499v1.pdf_translated.txt',\n",
       " '2308.00113v2.pdf_translated.txt',\n",
       " '2307.06940v1.pdf_translated.txt',\n",
       " '2305.05383v1.pdf_translated.txt',\n",
       " '2309.08172v2.pdf_translated.txt',\n",
       " '2309.08773v1.pdf_translated.txt',\n",
       " '2307.04686v2.pdf_translated.txt',\n",
       " '2305.11129v2.pdf_translated.txt',\n",
       " '2305.09662v1.pdf_translated.txt',\n",
       " '2307.13974v1.pdf_translated.txt',\n",
       " '2305.13077v1.pdf_translated.txt',\n",
       " '2309.02186v1.pdf_translated.txt',\n",
       " '2309.12311v1.pdf_translated.txt',\n",
       " '2307.10373v3.pdf_translated.txt',\n",
       " '2306.01841v1.pdf_translated.txt',\n",
       " '2309.03199v2.pdf_translated.txt',\n",
       " '2305.11694v2.pdf_translated.txt',\n",
       " '2308.03291v3.pdf_translated.txt',\n",
       " '2306.05399v2.pdf_translated.txt',\n",
       " '2308.02510v2.pdf_translated.txt',\n",
       " '2309.09390v1.pdf_translated.txt',\n",
       " '2305.02790v1.pdf_translated.txt',\n",
       " '2305.05189v4.pdf_translated.txt',\n",
       " '2306.10007v2.pdf_translated.txt',\n",
       " '2307.13908v1.pdf_translated.txt',\n",
       " '2306.08568v1.pdf_translated.txt',\n",
       " '2309.05767v2.pdf_translated.txt',\n",
       " '2306.10231v1.pdf_translated.txt',\n",
       " '2308.02453v3.pdf_translated.txt',\n",
       " '2305.05973v3.pdf_translated.txt',\n",
       " '2306.14289v2.pdf_translated.txt',\n",
       " '2305.16367v1.pdf_translated.txt',\n",
       " '2306.06092v1.pdf_translated.txt',\n",
       " '2308.03757v1.pdf_translated.txt',\n",
       " '2305.11308v2.pdf_translated.txt',\n",
       " '2306.07954v2.pdf_translated.txt',\n",
       " '2307.13702v1.pdf_translated.txt',\n",
       " '2305.04790v3.pdf_translated.txt',\n",
       " '2305.10425v1.pdf_translated.txt',\n",
       " '2307.03917v3.pdf_translated.txt',\n",
       " '2306.04009v1.pdf_translated.txt',\n",
       " '2305.16843v1.pdf_translated.txt',\n",
       " '2305.10601v2.pdf_translated.txt',\n",
       " '2305.16411v1.pdf_translated.txt',\n",
       " '2306.03203v1.pdf_translated.txt',\n",
       " '2309.10917v1.pdf_translated.txt',\n",
       " '2305.10973v2.pdf_translated.txt',\n",
       " '2305.06474v1.pdf_translated.txt',\n",
       " '2305.10320v1.pdf_translated.txt',\n",
       " '2305.05706v1.pdf_translated.txt',\n",
       " '2305.13304v1.pdf_translated.txt',\n",
       " '2308.13785v1.pdf_translated.txt',\n",
       " '2309.08628v3.pdf_translated.txt',\n",
       " '2306.04235v1.pdf_translated.txt',\n",
       " '2306.04050v2.pdf_translated.txt',\n",
       " '2305.16806v4.pdf_translated.txt',\n",
       " '2306.06044v2.pdf_translated.txt',\n",
       " '2305.09148v1.pdf_translated.txt',\n",
       " '2305.02783v4.pdf_translated.txt',\n",
       " '2305.03981v1.pdf_translated.txt',\n",
       " '2308.13404v1.pdf_translated.txt',\n",
       " '2306.01741v1.pdf_translated.txt',\n",
       " '2305.07804v4.pdf_translated.txt',\n",
       " '2309.06802v1.pdf_translated.txt',\n",
       " '2305.11243v2.pdf_translated.txt',\n",
       " '2307.03322v1.pdf_translated.txt',\n",
       " '2306.16700v2.pdf_translated.txt',\n",
       " '2307.13383v1.pdf_translated.txt',\n",
       " '2305.02549v2.pdf_translated.txt',\n",
       " '2309.03185v1.pdf_translated.txt',\n",
       " '2309.01826v2.pdf_translated.txt',\n",
       " '2305.03509v3.pdf_translated.txt',\n",
       " '2309.02119v3.pdf_translated.txt',\n",
       " '2305.05591v1.pdf_translated.txt',\n",
       " '2305.16867v1.pdf_translated.txt',\n",
       " '2305.18373v1.pdf_translated.txt',\n",
       " '2305.10841v2.pdf_translated.txt',\n",
       " '2306.02858v4.pdf_translated.txt',\n",
       " '2306.06638v1.pdf_translated.txt',\n",
       " '2306.05392v1.pdf_translated.txt',\n",
       " '2307.10168v2.pdf_translated.txt',\n",
       " '2305.06908v4.pdf_translated.txt',\n",
       " '2305.10853v2.pdf_translated.txt',\n",
       " '2309.10279v1.pdf_translated.txt',\n",
       " '2305.06456v3.pdf_translated.txt',\n",
       " '2307.16125v2.pdf_translated.txt',\n",
       " '2307.12169v5.pdf_translated.txt',\n",
       " '2309.07870v3.pdf_translated.txt',\n",
       " '2306.15354v3.pdf_translated.txt',\n",
       " '2307.03183v1.pdf_translated.txt',\n",
       " '2308.01734v1.pdf_translated.txt',\n",
       " '2306.00008v2.pdf_translated.txt',\n",
       " '2309.10537v1.pdf_translated.txt',\n",
       " '2306.09864v1.pdf_translated.txt',\n",
       " '2306.03083v1.pdf_translated.txt',\n",
       " '2308.16582v2.pdf_translated.txt',\n",
       " '2307.16372v1.pdf_translated.txt',\n",
       " '2305.17098v2.pdf_translated.txt',\n",
       " '2306.07174v1.pdf_translated.txt',\n",
       " '2307.02053v1.pdf_translated.txt',\n",
       " '2308.13416v1.pdf_translated.txt',\n",
       " '2305.07378v1.pdf_translated.txt',\n",
       " '2305.02483v2.pdf_translated.txt',\n",
       " '2305.09975v1.pdf_translated.txt',\n",
       " '2307.12612v1.pdf_translated.txt',\n",
       " '2307.06925v1.pdf_translated.txt']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text_lists=os.listdir(translated_text_dir)\n",
    "translated_text_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2305.02412v2.pdf_section.txt',\n",
       " '2305.02483v2.pdf_section.txt',\n",
       " '2305.02499v1.pdf_section.txt',\n",
       " '2305.02549v2.pdf_section.txt',\n",
       " '2305.02783v4.pdf_section.txt',\n",
       " '2305.02790v1.pdf_section.txt',\n",
       " '2305.03027v1.pdf_section.txt',\n",
       " '2305.03043v1.pdf_section.txt',\n",
       " '2305.03210v2.pdf_section.txt',\n",
       " '2305.03509v3.pdf_section.txt',\n",
       " '2305.03726v1.pdf_section.txt',\n",
       " '2305.03981v1.pdf_section.txt',\n",
       " '2305.04268v1.pdf_section.txt',\n",
       " '2305.04790v3.pdf_section.txt',\n",
       " '2305.04966v2.pdf_section.txt',\n",
       " '2305.05176v1.pdf_section.txt',\n",
       " '2305.05189v4.pdf_section.txt',\n",
       " '2305.05383v1.pdf_section.txt',\n",
       " '2305.05432v1.pdf_section.txt',\n",
       " '2305.05591v1.pdf_section.txt',\n",
       " '2305.05706v1.pdf_section.txt',\n",
       " '2305.05845v1.pdf_section.txt',\n",
       " '2305.05973v3.pdf_section.txt',\n",
       " '2305.06218v1.pdf_section.txt',\n",
       " '2305.06404v1.pdf_section.txt',\n",
       " '2305.06424v4.pdf_section.txt',\n",
       " '2305.06456v3.pdf_section.txt',\n",
       " '2305.06474v1.pdf_section.txt',\n",
       " '2305.06908v4.pdf_section.txt',\n",
       " '2305.07027v1.pdf_section.txt',\n",
       " '2305.07214v1.pdf_section.txt',\n",
       " '2305.07243v2.pdf_section.txt',\n",
       " '2305.07378v1.pdf_section.txt',\n",
       " '2305.07677v2.pdf_section.txt',\n",
       " '2305.07804v4.pdf_section.txt',\n",
       " '2305.08275v4.pdf_section.txt',\n",
       " '2305.08810v1.pdf_section.txt',\n",
       " '2305.08848v1.pdf_section.txt',\n",
       " '2305.08850v2.pdf_section.txt',\n",
       " '2305.08891v4.pdf_section.txt',\n",
       " '2305.09148v1.pdf_section.txt',\n",
       " '2305.09636v1.pdf_section.txt',\n",
       " '2305.09662v1.pdf_section.txt',\n",
       " '2305.09664v2.pdf_section.txt',\n",
       " '2305.09761v1.pdf_section.txt',\n",
       " '2305.09764v1.pdf_section.txt',\n",
       " '2305.09975v1.pdf_section.txt',\n",
       " '2305.10018v1.pdf_section.txt',\n",
       " '2305.10142v1.pdf_section.txt',\n",
       " '2305.10320v1.pdf_section.txt',\n",
       " '2305.10425v1.pdf_section.txt',\n",
       " '2305.10431v2.pdf_section.txt',\n",
       " '2305.10601v2.pdf_section.txt',\n",
       " '2305.10688v2.pdf_section.txt',\n",
       " '2305.10763v1.pdf_section.txt',\n",
       " '2305.10841v2.pdf_section.txt',\n",
       " '2305.10853v2.pdf_section.txt',\n",
       " '2305.10973v2.pdf_section.txt',\n",
       " '2305.11000v2.pdf_section.txt',\n",
       " '2305.11129v2.pdf_section.txt',\n",
       " '2305.11243v2.pdf_section.txt',\n",
       " '2305.11308v2.pdf_section.txt',\n",
       " '2305.11337v1.pdf_section.txt',\n",
       " '2305.11364v2.pdf_section.txt',\n",
       " '2305.11694v2.pdf_section.txt',\n",
       " '2305.11759v1.pdf_section.txt',\n",
       " '2305.11778v1.pdf_section.txt',\n",
       " '2305.11837v2.pdf_section.txt',\n",
       " '2305.12001v2.pdf_section.txt',\n",
       " '2305.13050v1.pdf_section.txt',\n",
       " '2305.13077v1.pdf_section.txt',\n",
       " '2305.13304v1.pdf_section.txt',\n",
       " '2305.13534v1.pdf_section.txt',\n",
       " '2305.15779v1.pdf_section.txt',\n",
       " '2305.16367v1.pdf_section.txt',\n",
       " '2305.16411v1.pdf_section.txt',\n",
       " '2305.16704v1.pdf_section.txt',\n",
       " '2305.16806v4.pdf_section.txt',\n",
       " '2305.16843v1.pdf_section.txt',\n",
       " '2305.16867v1.pdf_section.txt',\n",
       " '2305.17098v2.pdf_section.txt',\n",
       " '2305.17306v1.pdf_section.txt',\n",
       " '2305.18247v2.pdf_section.txt',\n",
       " '2305.18286v1.pdf_section.txt',\n",
       " '2305.18373v1.pdf_section.txt',\n",
       " '2305.18802v1.pdf_section.txt',\n",
       " '2305.20010v1.pdf_section.txt',\n",
       " '2306.00008v2.pdf_section.txt',\n",
       " '2306.00378v1.pdf_section.txt',\n",
       " '2306.01741v1.pdf_section.txt',\n",
       " '2306.01754v1.pdf_section.txt',\n",
       " '2306.01841v1.pdf_section.txt',\n",
       " '2306.02245v2.pdf_section.txt',\n",
       " '2306.02254v2.pdf_section.txt',\n",
       " '2306.02858v4.pdf_section.txt',\n",
       " '2306.02982v2.pdf_section.txt',\n",
       " '2306.03083v1.pdf_section.txt',\n",
       " '2306.03203v1.pdf_section.txt',\n",
       " '2306.03504v2.pdf_section.txt',\n",
       " '2306.03514v3.pdf_section.txt',\n",
       " '2306.04009v1.pdf_section.txt',\n",
       " '2306.04050v2.pdf_section.txt',\n",
       " '2306.04076v1.pdf_section.txt',\n",
       " '2306.04235v1.pdf_section.txt',\n",
       " '2306.04619v1.pdf_section.txt',\n",
       " '2306.04632v1.pdf_section.txt',\n",
       " '2306.05392v1.pdf_section.txt',\n",
       " '2306.05399v2.pdf_section.txt',\n",
       " '2306.05410v1.pdf_section.txt',\n",
       " '2306.06044v2.pdf_section.txt',\n",
       " '2306.06092v1.pdf_section.txt',\n",
       " '2306.06546v2.pdf_section.txt',\n",
       " '2306.06638v1.pdf_section.txt',\n",
       " '2306.07174v1.pdf_section.txt',\n",
       " '2306.07941v1.pdf_section.txt',\n",
       " '2306.07944v1.pdf_section.txt',\n",
       " '2306.07954v2.pdf_section.txt',\n",
       " '2306.08133v2.pdf_section.txt',\n",
       " '2306.08205v2.pdf_section.txt',\n",
       " '2306.08568v1.pdf_section.txt',\n",
       " '2306.09327v1.pdf_section.txt',\n",
       " '2306.09329v1.pdf_section.txt',\n",
       " '2306.09635v2.pdf_section.txt',\n",
       " '2306.09782v2.pdf_section.txt',\n",
       " '2306.09864v1.pdf_section.txt',\n",
       " '2306.10007v2.pdf_section.txt',\n",
       " '2306.10008v2.pdf_section.txt',\n",
       " '2306.10231v1.pdf_section.txt',\n",
       " '2306.10785v1.pdf_section.txt',\n",
       " '2306.12156v1.pdf_section.txt',\n",
       " '2306.13455v3.pdf_section.txt',\n",
       " '2306.13776v1.pdf_section.txt',\n",
       " '2306.14289v2.pdf_section.txt',\n",
       " '2306.15091v1.pdf_section.txt',\n",
       " '2306.15354v3.pdf_section.txt',\n",
       " '2306.15658v1.pdf_section.txt',\n",
       " '2306.16009v1.pdf_section.txt',\n",
       " '2306.16700v2.pdf_section.txt',\n",
       " '2306.16934v2.pdf_section.txt',\n",
       " '2306.17319v1.pdf_section.txt',\n",
       " '2306.17492v2.pdf_section.txt',\n",
       " '2306.17840v4.pdf_section.txt',\n",
       " '2307.00119v1.pdf_section.txt',\n",
       " '2307.01200v3.pdf_section.txt',\n",
       " '2307.01229v1.pdf_section.txt',\n",
       " '2307.02053v1.pdf_section.txt',\n",
       " '2307.02499v1.pdf_section.txt',\n",
       " '2307.02628v1.pdf_section.txt',\n",
       " '2307.03183v1.pdf_section.txt',\n",
       " '2307.03322v1.pdf_section.txt',\n",
       " '2307.03692v1.pdf_section.txt',\n",
       " '2307.03917v3.pdf_section.txt',\n",
       " '2307.04603v5.pdf_section.txt',\n",
       " '2307.04686v2.pdf_section.txt',\n",
       " '2307.06925v1.pdf_section.txt',\n",
       " '2307.06940v1.pdf_section.txt',\n",
       " '2307.06949v2.pdf_section.txt',\n",
       " '2307.08041v2.pdf_section.txt',\n",
       " '2307.08621v4.pdf_section.txt',\n",
       " '2307.08674v3.pdf_section.txt',\n",
       " '2307.09233v3.pdf_section.txt',\n",
       " '2307.09668v1.pdf_section.txt',\n",
       " '2307.09793v1.pdf_section.txt',\n",
       " '2307.10159v1.pdf_section.txt',\n",
       " '2307.10168v2.pdf_section.txt',\n",
       " '2307.10373v3.pdf_section.txt',\n",
       " '2307.11418v3.pdf_section.txt',\n",
       " '2307.11526v2.pdf_section.txt',\n",
       " '2307.11795v1.pdf_section.txt',\n",
       " '2307.12169v5.pdf_section.txt',\n",
       " '2307.12533v3.pdf_section.txt',\n",
       " '2307.12612v1.pdf_section.txt',\n",
       " '2307.12854v1.pdf_section.txt',\n",
       " '2307.13383v1.pdf_section.txt',\n",
       " '2307.13702v1.pdf_section.txt',\n",
       " '2307.13908v1.pdf_section.txt',\n",
       " '2307.13974v1.pdf_section.txt',\n",
       " '2307.14225v1.pdf_section.txt',\n",
       " '2307.14460v1.pdf_section.txt',\n",
       " '2307.14620v1.pdf_section.txt',\n",
       " '2307.15042v2.pdf_section.txt',\n",
       " '2307.15131v2.pdf_section.txt',\n",
       " '2307.16125v2.pdf_section.txt',\n",
       " '2307.16372v1.pdf_section.txt',\n",
       " '2307.16890v2.pdf_section.txt',\n",
       " '2308.00113v2.pdf_section.txt',\n",
       " '2308.01300v2.pdf_section.txt',\n",
       " '2308.01320v1.pdf_section.txt',\n",
       " '2308.01477v1.pdf_section.txt',\n",
       " '2308.01499v1.pdf_section.txt',\n",
       " '2308.01734v1.pdf_section.txt',\n",
       " '2308.01904v1.pdf_section.txt',\n",
       " '2308.02453v3.pdf_section.txt',\n",
       " '2308.02510v2.pdf_section.txt',\n",
       " '2308.03280v1.pdf_section.txt',\n",
       " '2308.03291v3.pdf_section.txt',\n",
       " '2308.03421v3.pdf_section.txt',\n",
       " '2308.03610v1.pdf_section.txt',\n",
       " '2308.03757v1.pdf_section.txt',\n",
       " '2308.04623v1.pdf_section.txt',\n",
       " '2308.04729v1.pdf_section.txt',\n",
       " '2308.05221v1.pdf_section.txt',\n",
       " '2308.05734v3.pdf_section.txt',\n",
       " '2308.05884v1.pdf_section.txt',\n",
       " '2308.05960v1.pdf_section.txt',\n",
       " '2308.06125v1.pdf_section.txt',\n",
       " '2308.06261v1.pdf_section.txt',\n",
       " '2308.06873v2.pdf_section.txt',\n",
       " '2308.07395v1.pdf_section.txt',\n",
       " '2308.07891v1.pdf_section.txt',\n",
       " '2308.07926v2.pdf_section.txt',\n",
       " '2308.07968v1.pdf_section.txt',\n",
       " '2308.08089v1.pdf_section.txt',\n",
       " '2308.08316v3.pdf_section.txt',\n",
       " '2308.13404v1.pdf_section.txt',\n",
       " '2308.13416v1.pdf_section.txt',\n",
       " '2308.13785v1.pdf_section.txt',\n",
       " '2308.15930v3.pdf_section.txt',\n",
       " '2308.16582v2.pdf_section.txt',\n",
       " '2308.16824v2.pdf_section.txt',\n",
       " '2308.16876v2.pdf_section.txt',\n",
       " '2309.00615v1.pdf_section.txt',\n",
       " '2309.00775v1.pdf_section.txt',\n",
       " '2309.00908v1.pdf_section.txt',\n",
       " '2309.00986v1.pdf_section.txt',\n",
       " '2309.01826v2.pdf_section.txt',\n",
       " '2309.02040v2.pdf_section.txt',\n",
       " '2309.02119v3.pdf_section.txt',\n",
       " '2309.02186v1.pdf_section.txt',\n",
       " '2309.03185v1.pdf_section.txt',\n",
       " '2309.03199v2.pdf_section.txt',\n",
       " '2309.03907v1.pdf_section.txt',\n",
       " '2309.03926v1.pdf_section.txt',\n",
       " '2309.04269v1.pdf_section.txt',\n",
       " '2309.04354v1.pdf_section.txt',\n",
       " '2309.04663v2.pdf_section.txt',\n",
       " '2309.04827v1.pdf_section.txt',\n",
       " '2309.05767v2.pdf_section.txt',\n",
       " '2309.06126v1.pdf_section.txt',\n",
       " '2309.06440v1.pdf_section.txt',\n",
       " '2309.06802v1.pdf_section.txt',\n",
       " '2309.07062v1.pdf_section.txt',\n",
       " '2309.07314v1.pdf_section.txt',\n",
       " '2309.07749v1.pdf_section.txt',\n",
       " '2309.07870v3.pdf_section.txt',\n",
       " '2309.07906v3.pdf_section.txt',\n",
       " '2309.07990v2.pdf_section.txt',\n",
       " '2309.08051v2.pdf_section.txt',\n",
       " '2309.08172v2.pdf_section.txt',\n",
       " '2309.08210v1.pdf_section.txt',\n",
       " '2309.08586v2.pdf_section.txt',\n",
       " '2309.08628v3.pdf_section.txt',\n",
       " '2309.08773v1.pdf_section.txt',\n",
       " '2309.08804v1.pdf_section.txt',\n",
       " '2309.08827v1.pdf_section.txt',\n",
       " '2309.09390v1.pdf_section.txt',\n",
       " '2309.09400v1.pdf_section.txt',\n",
       " '2309.09958v1.pdf_section.txt',\n",
       " '2309.10202v1.pdf_section.txt',\n",
       " '2309.10279v1.pdf_section.txt',\n",
       " '2309.10537v1.pdf_section.txt',\n",
       " '2309.10917v1.pdf_section.txt',\n",
       " '2309.11497v2.pdf_section.txt',\n",
       " '2309.11500v4.pdf_section.txt',\n",
       " '2309.11523v5.pdf_section.txt',\n",
       " '2309.12311v1.pdf_section.txt']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text_lists.sort()\n",
    "original_text_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2305.02412v2.pdf_translated.txt',\n",
       " '2305.02483v2.pdf_translated.txt',\n",
       " '2305.02499v1.pdf_translated.txt',\n",
       " '2305.02549v2.pdf_translated.txt',\n",
       " '2305.02783v4.pdf_translated.txt',\n",
       " '2305.02790v1.pdf_translated.txt',\n",
       " '2305.03027v1.pdf_translated.txt',\n",
       " '2305.03043v1.pdf_translated.txt',\n",
       " '2305.03210v2.pdf_translated.txt',\n",
       " '2305.03509v3.pdf_translated.txt',\n",
       " '2305.03726v1.pdf_translated.txt',\n",
       " '2305.03981v1.pdf_translated.txt',\n",
       " '2305.04268v1.pdf_translated.txt',\n",
       " '2305.04790v3.pdf_translated.txt',\n",
       " '2305.04966v2.pdf_translated.txt',\n",
       " '2305.05176v1.pdf_translated.txt',\n",
       " '2305.05189v4.pdf_translated.txt',\n",
       " '2305.05383v1.pdf_translated.txt',\n",
       " '2305.05432v1.pdf_translated.txt',\n",
       " '2305.05591v1.pdf_translated.txt',\n",
       " '2305.05706v1.pdf_translated.txt',\n",
       " '2305.05845v1.pdf_translated.txt',\n",
       " '2305.05973v3.pdf_translated.txt',\n",
       " '2305.06218v1.pdf_translated.txt',\n",
       " '2305.06404v1.pdf_translated.txt',\n",
       " '2305.06424v4.pdf_translated.txt',\n",
       " '2305.06456v3.pdf_translated.txt',\n",
       " '2305.06474v1.pdf_translated.txt',\n",
       " '2305.06908v4.pdf_translated.txt',\n",
       " '2305.07027v1.pdf_translated.txt',\n",
       " '2305.07214v1.pdf_translated.txt',\n",
       " '2305.07243v2.pdf_translated.txt',\n",
       " '2305.07378v1.pdf_translated.txt',\n",
       " '2305.07677v2.pdf_translated.txt',\n",
       " '2305.07804v4.pdf_translated.txt',\n",
       " '2305.08275v4.pdf_translated.txt',\n",
       " '2305.08810v1.pdf_translated.txt',\n",
       " '2305.08848v1.pdf_translated.txt',\n",
       " '2305.08850v2.pdf_translated.txt',\n",
       " '2305.08891v4.pdf_translated.txt',\n",
       " '2305.09148v1.pdf_translated.txt',\n",
       " '2305.09636v1.pdf_translated.txt',\n",
       " '2305.09662v1.pdf_translated.txt',\n",
       " '2305.09664v2.pdf_translated.txt',\n",
       " '2305.09761v1.pdf_translated.txt',\n",
       " '2305.09764v1.pdf_translated.txt',\n",
       " '2305.09975v1.pdf_translated.txt',\n",
       " '2305.10018v1.pdf_translated.txt',\n",
       " '2305.10142v1.pdf_translated.txt',\n",
       " '2305.10320v1.pdf_translated.txt',\n",
       " '2305.10425v1.pdf_translated.txt',\n",
       " '2305.10431v2.pdf_translated.txt',\n",
       " '2305.10601v2.pdf_translated.txt',\n",
       " '2305.10688v2.pdf_translated.txt',\n",
       " '2305.10763v1.pdf_translated.txt',\n",
       " '2305.10841v2.pdf_translated.txt',\n",
       " '2305.10853v2.pdf_translated.txt',\n",
       " '2305.10973v2.pdf_translated.txt',\n",
       " '2305.11000v2.pdf_translated.txt',\n",
       " '2305.11129v2.pdf_translated.txt',\n",
       " '2305.11243v2.pdf_translated.txt',\n",
       " '2305.11308v2.pdf_translated.txt',\n",
       " '2305.11337v1.pdf_translated.txt',\n",
       " '2305.11364v2.pdf_translated.txt',\n",
       " '2305.11694v2.pdf_translated.txt',\n",
       " '2305.11759v1.pdf_translated.txt',\n",
       " '2305.11778v1.pdf_translated.txt',\n",
       " '2305.11837v2.pdf_translated.txt',\n",
       " '2305.12001v2.pdf_translated.txt',\n",
       " '2305.13050v1.pdf_translated.txt',\n",
       " '2305.13077v1.pdf_translated.txt',\n",
       " '2305.13304v1.pdf_translated.txt',\n",
       " '2305.13534v1.pdf_translated.txt',\n",
       " '2305.15779v1.pdf_translated.txt',\n",
       " '2305.16367v1.pdf_translated.txt',\n",
       " '2305.16411v1.pdf_translated.txt',\n",
       " '2305.16704v1.pdf_translated.txt',\n",
       " '2305.16806v4.pdf_translated.txt',\n",
       " '2305.16843v1.pdf_translated.txt',\n",
       " '2305.16867v1.pdf_translated.txt',\n",
       " '2305.17098v2.pdf_translated.txt',\n",
       " '2305.17306v1.pdf_translated.txt',\n",
       " '2305.18247v2.pdf_translated.txt',\n",
       " '2305.18286v1.pdf_translated.txt',\n",
       " '2305.18373v1.pdf_translated.txt',\n",
       " '2305.18802v1.pdf_translated.txt',\n",
       " '2305.20010v1.pdf_translated.txt',\n",
       " '2306.00008v2.pdf_translated.txt',\n",
       " '2306.00378v1.pdf_translated.txt',\n",
       " '2306.01741v1.pdf_translated.txt',\n",
       " '2306.01754v1.pdf_translated.txt',\n",
       " '2306.01841v1.pdf_translated.txt',\n",
       " '2306.02245v2.pdf_translated.txt',\n",
       " '2306.02254v2.pdf_translated.txt',\n",
       " '2306.02858v4.pdf_translated.txt',\n",
       " '2306.02982v2.pdf_translated.txt',\n",
       " '2306.03083v1.pdf_translated.txt',\n",
       " '2306.03203v1.pdf_translated.txt',\n",
       " '2306.03504v2.pdf_translated.txt',\n",
       " '2306.03514v3.pdf_translated.txt',\n",
       " '2306.04009v1.pdf_translated.txt',\n",
       " '2306.04050v2.pdf_translated.txt',\n",
       " '2306.04076v1.pdf_translated.txt',\n",
       " '2306.04235v1.pdf_translated.txt',\n",
       " '2306.04619v1.pdf_translated.txt',\n",
       " '2306.04632v1.pdf_translated.txt',\n",
       " '2306.05392v1.pdf_translated.txt',\n",
       " '2306.05399v2.pdf_translated.txt',\n",
       " '2306.05410v1.pdf_translated.txt',\n",
       " '2306.06044v2.pdf_translated.txt',\n",
       " '2306.06092v1.pdf_translated.txt',\n",
       " '2306.06546v2.pdf_translated.txt',\n",
       " '2306.06638v1.pdf_translated.txt',\n",
       " '2306.07174v1.pdf_translated.txt',\n",
       " '2306.07941v1.pdf_translated.txt',\n",
       " '2306.07944v1.pdf_translated.txt',\n",
       " '2306.07954v2.pdf_translated.txt',\n",
       " '2306.08133v2.pdf_translated.txt',\n",
       " '2306.08205v2.pdf_translated.txt',\n",
       " '2306.08568v1.pdf_translated.txt',\n",
       " '2306.09327v1.pdf_translated.txt',\n",
       " '2306.09329v1.pdf_translated.txt',\n",
       " '2306.09635v2.pdf_translated.txt',\n",
       " '2306.09782v2.pdf_translated.txt',\n",
       " '2306.09864v1.pdf_translated.txt',\n",
       " '2306.10007v2.pdf_translated.txt',\n",
       " '2306.10008v2.pdf_translated.txt',\n",
       " '2306.10231v1.pdf_translated.txt',\n",
       " '2306.10785v1.pdf_translated.txt',\n",
       " '2306.12156v1.pdf_translated.txt',\n",
       " '2306.13455v3.pdf_translated.txt',\n",
       " '2306.13776v1.pdf_translated.txt',\n",
       " '2306.14289v2.pdf_translated.txt',\n",
       " '2306.15091v1.pdf_translated.txt',\n",
       " '2306.15354v3.pdf_translated.txt',\n",
       " '2306.15658v1.pdf_translated.txt',\n",
       " '2306.16009v1.pdf_translated.txt',\n",
       " '2306.16700v2.pdf_translated.txt',\n",
       " '2306.16934v2.pdf_translated.txt',\n",
       " '2306.17319v1.pdf_translated.txt',\n",
       " '2306.17492v2.pdf_translated.txt',\n",
       " '2306.17840v4.pdf_translated.txt',\n",
       " '2307.00119v1.pdf_translated.txt',\n",
       " '2307.01200v3.pdf_translated.txt',\n",
       " '2307.01229v1.pdf_translated.txt',\n",
       " '2307.02053v1.pdf_translated.txt',\n",
       " '2307.02499v1.pdf_translated.txt',\n",
       " '2307.02628v1.pdf_translated.txt',\n",
       " '2307.03183v1.pdf_translated.txt',\n",
       " '2307.03322v1.pdf_translated.txt',\n",
       " '2307.03692v1.pdf_translated.txt',\n",
       " '2307.03917v3.pdf_translated.txt',\n",
       " '2307.04603v5.pdf_translated.txt',\n",
       " '2307.04686v2.pdf_translated.txt',\n",
       " '2307.06925v1.pdf_translated.txt',\n",
       " '2307.06940v1.pdf_translated.txt',\n",
       " '2307.06949v2.pdf_translated.txt',\n",
       " '2307.08041v2.pdf_translated.txt',\n",
       " '2307.08621v4.pdf_translated.txt',\n",
       " '2307.08674v3.pdf_translated.txt',\n",
       " '2307.09233v3.pdf_translated.txt',\n",
       " '2307.09668v1.pdf_translated.txt',\n",
       " '2307.09793v1.pdf_translated.txt',\n",
       " '2307.10159v1.pdf_translated.txt',\n",
       " '2307.10168v2.pdf_translated.txt',\n",
       " '2307.10373v3.pdf_translated.txt',\n",
       " '2307.11418v3.pdf_translated.txt',\n",
       " '2307.11526v2.pdf_translated.txt',\n",
       " '2307.11795v1.pdf_translated.txt',\n",
       " '2307.12169v5.pdf_translated.txt',\n",
       " '2307.12533v3.pdf_translated.txt',\n",
       " '2307.12612v1.pdf_translated.txt',\n",
       " '2307.12854v1.pdf_translated.txt',\n",
       " '2307.13383v1.pdf_translated.txt',\n",
       " '2307.13702v1.pdf_translated.txt',\n",
       " '2307.13908v1.pdf_translated.txt',\n",
       " '2307.13974v1.pdf_translated.txt',\n",
       " '2307.14225v1.pdf_translated.txt',\n",
       " '2307.14460v1.pdf_translated.txt',\n",
       " '2307.14620v1.pdf_translated.txt',\n",
       " '2307.15042v2.pdf_translated.txt',\n",
       " '2307.15131v2.pdf_translated.txt',\n",
       " '2307.16125v2.pdf_translated.txt',\n",
       " '2307.16372v1.pdf_translated.txt',\n",
       " '2307.16890v2.pdf_translated.txt',\n",
       " '2308.00113v2.pdf_translated.txt',\n",
       " '2308.01300v2.pdf_translated.txt',\n",
       " '2308.01320v1.pdf_translated.txt',\n",
       " '2308.01477v1.pdf_translated.txt',\n",
       " '2308.01499v1.pdf_translated.txt',\n",
       " '2308.01734v1.pdf_translated.txt',\n",
       " '2308.01904v1.pdf_translated.txt',\n",
       " '2308.02453v3.pdf_translated.txt',\n",
       " '2308.02510v2.pdf_translated.txt',\n",
       " '2308.03280v1.pdf_translated.txt',\n",
       " '2308.03291v3.pdf_translated.txt',\n",
       " '2308.03421v3.pdf_translated.txt',\n",
       " '2308.03610v1.pdf_translated.txt',\n",
       " '2308.03757v1.pdf_translated.txt',\n",
       " '2308.04623v1.pdf_translated.txt',\n",
       " '2308.04729v1.pdf_translated.txt',\n",
       " '2308.05221v1.pdf_translated.txt',\n",
       " '2308.05734v3.pdf_translated.txt',\n",
       " '2308.05884v1.pdf_translated.txt',\n",
       " '2308.05960v1.pdf_translated.txt',\n",
       " '2308.06125v1.pdf_translated.txt',\n",
       " '2308.06261v1.pdf_translated.txt',\n",
       " '2308.06873v2.pdf_translated.txt',\n",
       " '2308.07395v1.pdf_translated.txt',\n",
       " '2308.07891v1.pdf_translated.txt',\n",
       " '2308.07926v2.pdf_translated.txt',\n",
       " '2308.07968v1.pdf_translated.txt',\n",
       " '2308.08089v1.pdf_translated.txt',\n",
       " '2308.08316v3.pdf_translated.txt',\n",
       " '2308.13404v1.pdf_translated.txt',\n",
       " '2308.13416v1.pdf_translated.txt',\n",
       " '2308.13785v1.pdf_translated.txt',\n",
       " '2308.15930v3.pdf_translated.txt',\n",
       " '2308.16582v2.pdf_translated.txt',\n",
       " '2308.16824v2.pdf_translated.txt',\n",
       " '2308.16876v2.pdf_translated.txt',\n",
       " '2309.00615v1.pdf_translated.txt',\n",
       " '2309.00775v1.pdf_translated.txt',\n",
       " '2309.00908v1.pdf_translated.txt',\n",
       " '2309.00986v1.pdf_translated.txt',\n",
       " '2309.01826v2.pdf_translated.txt',\n",
       " '2309.02040v2.pdf_translated.txt',\n",
       " '2309.02119v3.pdf_translated.txt',\n",
       " '2309.02186v1.pdf_translated.txt',\n",
       " '2309.03185v1.pdf_translated.txt',\n",
       " '2309.03199v2.pdf_translated.txt',\n",
       " '2309.03907v1.pdf_translated.txt',\n",
       " '2309.03926v1.pdf_translated.txt',\n",
       " '2309.04269v1.pdf_translated.txt',\n",
       " '2309.04354v1.pdf_translated.txt',\n",
       " '2309.04663v2.pdf_translated.txt',\n",
       " '2309.04827v1.pdf_translated.txt',\n",
       " '2309.05767v2.pdf_translated.txt',\n",
       " '2309.06126v1.pdf_translated.txt',\n",
       " '2309.06440v1.pdf_translated.txt',\n",
       " '2309.06802v1.pdf_translated.txt',\n",
       " '2309.07062v1.pdf_translated.txt',\n",
       " '2309.07314v1.pdf_translated.txt',\n",
       " '2309.07749v1.pdf_translated.txt',\n",
       " '2309.07870v3.pdf_translated.txt',\n",
       " '2309.07906v3.pdf_translated.txt',\n",
       " '2309.07990v2.pdf_translated.txt',\n",
       " '2309.08051v2.pdf_translated.txt',\n",
       " '2309.08172v2.pdf_translated.txt',\n",
       " '2309.08210v1.pdf_translated.txt',\n",
       " '2309.08586v2.pdf_translated.txt',\n",
       " '2309.08628v3.pdf_translated.txt',\n",
       " '2309.08773v1.pdf_translated.txt',\n",
       " '2309.08804v1.pdf_translated.txt',\n",
       " '2309.08827v1.pdf_translated.txt',\n",
       " '2309.09390v1.pdf_translated.txt',\n",
       " '2309.09400v1.pdf_translated.txt',\n",
       " '2309.09958v1.pdf_translated.txt',\n",
       " '2309.10202v1.pdf_translated.txt',\n",
       " '2309.10279v1.pdf_translated.txt',\n",
       " '2309.10537v1.pdf_translated.txt',\n",
       " '2309.10917v1.pdf_translated.txt',\n",
       " '2309.11497v2.pdf_translated.txt',\n",
       " '2309.11500v4.pdf_translated.txt',\n",
       " '2309.11523v5.pdf_translated.txt',\n",
       " '2309.12311v1.pdf_translated.txt']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text_lists.sort()\n",
    "translated_text_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text_list=[]\n",
    "translated_text_list=[]\n",
    "for file_name in original_text_lists:\n",
    "    with open(f\"{original_text_dir}/{file_name}\", \"r\", encoding='utf-8') as f:\n",
    "        original_text_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in translated_text_lists:\n",
    "    with open(f\"{translated_text_dir}/{file_name}\", \"r\", encoding='utf-8') as f:\n",
    "        translated_text_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     en    ko\n",
       "0  None  None"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.DataFrame({\n",
    "    'en': None,\n",
    "    'ko': None\n",
    "}, index=[0])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 리스트를 딕셔너리의 리스트로 변환\n",
    "dataset = [\n",
    "    {\n",
    "        'en': orig,\n",
    "        'ko': trans\n",
    "    }\n",
    "    for orig, trans in zip(original_text_list, translated_text_list)\n",
    "]\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.json_normalize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--- ABSTRACT ---\\nPre-trained large language m...</td>\n",
       "      <td>--- ABSTRACT ---\\n사전 훈련된 대규모 언어 모델(LLM)은 세상에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--- ABSTRACT ---\\nive Summarization by Tri-age...</td>\n",
       "      <td>--- ABSTRACT ---\\nive Tri-agent 생성 파이프라인에 의한 요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--- ABSTRACT ---\\nAI tasks encompass a wide ra...</td>\n",
       "      <td>--- ABSTRACT ---\\nAI 작업은 광범위한 도메인과 분야를 포함합니다. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--- ABSTRACT ---\\nThe recent advent of self-su...</td>\n",
       "      <td>--- ABSTRACT ---\\n최근 자기 지도 사전 학습 기술의 등장으로 양식 문...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--- ABSTRACT ---\\n—The recent improvement in c...</td>\n",
       "      <td>--- ABSTRACT ---\\n―대규모 언어 모델을 사용하여 코드 생성 기능이 최...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>--- ABSTRACT ---\\nIn recent years, Large Langu...</td>\n",
       "      <td>--- ABSTRACT ---\\n최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>--- ABSTRACT ---\\nIn this paper, we uncover th...</td>\n",
       "      <td>--- ABSTRACT ---\\n이 논문에서 우리는 &amp;quot;무료 점심&amp;quot;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>--- ABSTRACT ---\\ning with credit is permitted...</td>\n",
       "      <td>--- ABSTRACT ---\\n출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>--- ABSTRACT ---\\nVision Transformer (ViT) has...</td>\n",
       "      <td>--- ABSTRACT ---\\nVision Transformer(ViT)는 최근 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>--- ABSTRACT ---\\n— 3D visual grounding is a c...</td>\n",
       "      <td>--- ABSTRACT ---\\n3D 시각적 접지는 가정용 로봇에 중요한 기술로, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    en  \\\n",
       "0    --- ABSTRACT ---\\nPre-trained large language m...   \n",
       "1    --- ABSTRACT ---\\nive Summarization by Tri-age...   \n",
       "2    --- ABSTRACT ---\\nAI tasks encompass a wide ra...   \n",
       "3    --- ABSTRACT ---\\nThe recent advent of self-su...   \n",
       "4    --- ABSTRACT ---\\n—The recent improvement in c...   \n",
       "..                                                 ...   \n",
       "261  --- ABSTRACT ---\\nIn recent years, Large Langu...   \n",
       "262  --- ABSTRACT ---\\nIn this paper, we uncover th...   \n",
       "263  --- ABSTRACT ---\\ning with credit is permitted...   \n",
       "264  --- ABSTRACT ---\\nVision Transformer (ViT) has...   \n",
       "265  --- ABSTRACT ---\\n— 3D visual grounding is a c...   \n",
       "\n",
       "                                                    ko  \n",
       "0    --- ABSTRACT ---\\n사전 훈련된 대규모 언어 모델(LLM)은 세상에 대...  \n",
       "1    --- ABSTRACT ---\\nive Tri-agent 생성 파이프라인에 의한 요...  \n",
       "2    --- ABSTRACT ---\\nAI 작업은 광범위한 도메인과 분야를 포함합니다. ...  \n",
       "3    --- ABSTRACT ---\\n최근 자기 지도 사전 학습 기술의 등장으로 양식 문...  \n",
       "4    --- ABSTRACT ---\\n―대규모 언어 모델을 사용하여 코드 생성 기능이 최...  \n",
       "..                                                 ...  \n",
       "261  --- ABSTRACT ---\\n최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰...  \n",
       "262  --- ABSTRACT ---\\n이 논문에서 우리는 &quot;무료 점심&quot;...  \n",
       "263  --- ABSTRACT ---\\n출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식...  \n",
       "264  --- ABSTRACT ---\\nVision Transformer(ViT)는 최근 ...  \n",
       "265  --- ABSTRACT ---\\n3D 시각적 접지는 가정용 로봇에 중요한 기술로, ...  \n",
       "\n",
       "[266 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    0\n",
       "ko    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ko'],\n",
       "    num_rows: 266\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--- ABSTRACT ---\\nPre-trained large language m...</td>\n",
       "      <td>--- ABSTRACT ---\\n사전 훈련된 대규모 언어 모델(LLM)은 세상에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--- ABSTRACT ---\\nive Summarization by Tri-age...</td>\n",
       "      <td>--- ABSTRACT ---\\nive Tri-agent 생성 파이프라인에 의한 요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--- ABSTRACT ---\\nAI tasks encompass a wide ra...</td>\n",
       "      <td>--- ABSTRACT ---\\nAI 작업은 광범위한 도메인과 분야를 포함합니다. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--- ABSTRACT ---\\nThe recent advent of self-su...</td>\n",
       "      <td>--- ABSTRACT ---\\n최근 자기 지도 사전 학습 기술의 등장으로 양식 문...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--- ABSTRACT ---\\n—The recent improvement in c...</td>\n",
       "      <td>--- ABSTRACT ---\\n―대규모 언어 모델을 사용하여 코드 생성 기능이 최...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>--- ABSTRACT ---\\nIn recent years, Large Langu...</td>\n",
       "      <td>--- ABSTRACT ---\\n최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>--- ABSTRACT ---\\nIn this paper, we uncover th...</td>\n",
       "      <td>--- ABSTRACT ---\\n이 논문에서 우리는 &amp;quot;무료 점심&amp;quot;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>--- ABSTRACT ---\\ning with credit is permitted...</td>\n",
       "      <td>--- ABSTRACT ---\\n출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>--- ABSTRACT ---\\nVision Transformer (ViT) has...</td>\n",
       "      <td>--- ABSTRACT ---\\nVision Transformer(ViT)는 최근 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>--- ABSTRACT ---\\n— 3D visual grounding is a c...</td>\n",
       "      <td>--- ABSTRACT ---\\n3D 시각적 접지는 가정용 로봇에 중요한 기술로, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    en  \\\n",
       "0    --- ABSTRACT ---\\nPre-trained large language m...   \n",
       "1    --- ABSTRACT ---\\nive Summarization by Tri-age...   \n",
       "2    --- ABSTRACT ---\\nAI tasks encompass a wide ra...   \n",
       "3    --- ABSTRACT ---\\nThe recent advent of self-su...   \n",
       "4    --- ABSTRACT ---\\n—The recent improvement in c...   \n",
       "..                                                 ...   \n",
       "261  --- ABSTRACT ---\\nIn recent years, Large Langu...   \n",
       "262  --- ABSTRACT ---\\nIn this paper, we uncover th...   \n",
       "263  --- ABSTRACT ---\\ning with credit is permitted...   \n",
       "264  --- ABSTRACT ---\\nVision Transformer (ViT) has...   \n",
       "265  --- ABSTRACT ---\\n— 3D visual grounding is a c...   \n",
       "\n",
       "                                                    ko  \n",
       "0    --- ABSTRACT ---\\n사전 훈련된 대규모 언어 모델(LLM)은 세상에 대...  \n",
       "1    --- ABSTRACT ---\\nive Tri-agent 생성 파이프라인에 의한 요...  \n",
       "2    --- ABSTRACT ---\\nAI 작업은 광범위한 도메인과 분야를 포함합니다. ...  \n",
       "3    --- ABSTRACT ---\\n최근 자기 지도 사전 학습 기술의 등장으로 양식 문...  \n",
       "4    --- ABSTRACT ---\\n―대규모 언어 모델을 사용하여 코드 생성 기능이 최...  \n",
       "..                                                 ...  \n",
       "261  --- ABSTRACT ---\\n최근 몇 년 동안, 대규모 언어 모델(LLM)은 뛰...  \n",
       "262  --- ABSTRACT ---\\n이 논문에서 우리는 &quot;무료 점심&quot;...  \n",
       "263  --- ABSTRACT ---\\n출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식...  \n",
       "264  --- ABSTRACT ---\\nVision Transformer(ViT)는 최근 ...  \n",
       "265  --- ABSTRACT ---\\n3D 시각적 접지는 가정용 로봇에 중요한 기술로, ...  \n",
       "\n",
       "[266 rows x 2 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=df.iloc[:num_train]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset('csv', data_files='train.tsv', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ko'],\n",
       "        num_rows: 266\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"KETI-AIR/ke-t5-base\"\n",
    "max_token_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--- ABSTRACT ---\\nLarge language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstretGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model’s upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind’s Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo’s implementation for researchers, democratizing the required training resources from 1 x A100 GPU to 4x RTX-GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines. 1\\n--- INTRODUCTION ---\\n& Motivation Large language models (LLMs) have demonstrated significant universal capabilities in performing various tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data and have been showcased in recent research, such as GPT-2 [25] and GPT-3 [6]. Recent studies have highlighted the importance of instruction tuning in empowering LLMs, as exemplified by the boosting of GPT-3 [6] to InstrctGPT [22] and ChatGPT [20], which follows natural language instructions effectively to accomplish real-world tasks and allows for customizing task-specific rules into instructions during downstream fine-tuning, enabling pre-trained models to comprehend user intents more effectively and produce accurate and relevant responses. Similar attempts have been introduced in multi-modal models as well. LLaMA-Adapter [38] aims to adapt LLaMA [33] into an instruction following model by adding additional adapter modules and multi-modal prompts. Mini-GPT4 [39] follows the architecture of BLIP-2 [15] but replaces the language decoder with Vicuna [9], which supports longer answers. LLaVA [17] utilizes the same CLIP [23] vision encoder and Vicuna [9] language decoder, and finetunes on their high-quality instruction dataset, curated by GPT-4 [19]. Although these works have achieved excellent results and provided valuable insights, they share a minor common issue. Specifically, they either finetune the entire model or the connection part on “Equal Contribution =Corresponding Author Technical Report. --- --MMCc4 em MIMIC-IT ae = => multi-modal instruction = billion-scale corpus of + . tuning datasets with images interleaved with text a in-context examples Otter OpenFlamingo Figure 1: Otter Overview. Otter is a multi-modal model finetuned on our proposed MIMIC-IT dataset, based on OpenFlamingo. Otter model exhibits the improved ability to execute tasks by following given instructions and leveraging in-context examples. task-specific data. For instance, a common practice is to use image-text data pairs from Caption [16] or VQA [11] tasks to align visual and language modules. While embedding visual information into the language model in this way can be effective, we question whether this practice is inherently task-dependent, as it relies on the task for which the data is used to train the alignment module. Upon reflection, we have discovered that DeepMind Flamingo’s [1] upstream pretraining dataset, MultiModal MassiveWeb (M3W), has significant value in aligning visual and language information in a more natural manner. The dataset comprises HTML webpages, where all images and texts are arranged in an interleaved format. Specifically, a piece of text may describe an image (or videos) above or below it, and correlations may exist between images (or videos) and text in adjacent positions. This natural organization of context provides richer information than a caption dataset, where text only describes its corresponding image. Trained on this dataset, Flamingo achieves zeroand few-shot generalization and in-context learning ability, making it the GPT-3 moment in the multi-modal domain. However, DeepMind has not released the Flamingo model and its M3W dataset to the public, potentially because of the model’s exceptional performance that could cause astonishment prematurely. Nevertheless, the LAION-AI’s OpenFlamingo project [4] has recently been made public, providing access to their corresponding MMC4 [40] dataset in the same interleaved format on a larger scale. Consequently, community researchers can continue to follow the Flamingo series of works for further research and development in the field of multi-modal models. Although the OpenFlamingo model exhibits impressive multi-modal in-context learning abilities and executes tasks with given in-context examples, as an upstream pre-trained model, it still requires instruction tuning to perform downstream tasks more effectively. In our paper, we propose our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset and provide details on its construction in Sec. 3.1. We then introduce Otter, a multi-modal model with in-context instruction tuning based on OpenFlamingo. We illustrate the relationship between Otter and OpenFlamingo in Fig. 1. Finetuned on MIMIC-IT dataset, our Otter model demonstrates improved instruction-following ability compared to OpenFlamingo, as shown in our qualitative analysis in Sec. 4.1. Meanwhile, Otter is capable of learning to execute instructions with provided in-context learning examples, as shown in Sec. 4.2. From the engineering perspective, we optimized OpenFlamingo’s implementation to make it more accessible to researchers. Our optimizations include optimizing the training requirements from at least 1 x A100 GPU to only 4x RTX3090 GPUs and integrating it into Hugging Face Transformers [34] to simplify training and inference with a few lines of code. Our contributions facilitate further research and development in the field of multi-modal models. We summarize our key contributions as follows: * We present the MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. Each data sample includes an instruction-image-answer triplet and its in-context examples. ¢ We introduce Otter, a multi-modal model with in-context instruction tuning based on OpenFlamingo, capable of the instruction following and executing new instructions with few in-context learning examples. ¢ Our optimizations to OpenFlamingo’s implementation, including reducing the training requirements to 4x RTX3090 GPUs and integrating it into Huggingface Transformers to simplify the training and inference with only a few lines of code. --- --2\\n--- RELATED WORK ---\\n2.1 Large-scale Multi-modal Models With the recent success of ChatGPT [20], GPT-4 [19], and other large language models [33, 32, 9], recent studies start to explore incorporating information from other modalities based on pretrained language models. These studies extend the capabilities of language models to more tasks and modalities, and can be categorized into two perspectives: System Design Perspective. This perspective involves using ChatGPT [20] as a dispatch scheduler and connecting different expert models through it to allow for different visual tasks. Language prompts serve as an interface to call expert visual-language models within their respective task domains. Works in this category include VisualChatGPT [35], HuggingGPT [29], Cola [8], XGPT [42], MM-REACT [37], and ViperGPT [31]. This approach has limitations in that each model cannot be trained individually on new tasks, and using ChatGPT [20] as a powerful instruction dispatch tool can result in high API query costs. End-to-End Trainable Models Perspective. This perspective focuses on connecting models from different modalities into integrated end-to-end trainable models, also known as multi-modal foundation models. Early works in this field include Flamingo [1], which proposes a unified architecture ‘or modeling language and vision and was later open-sourced as OpenFlamingo [4] by LAIONAI. Other earlier works include BLIP-2 [15], which uses a lightweight Querying Transformer and two-stage bootstrap pretraining to connect information from the image to text modality. With the popularity of GPT-4 [19], there has been an increased focus on this field since 2023. Enterpriselevel product models include OpenAI’s yet-to-be-released vision-language version of GPT-4 [19], Google’s PaLM-E [10], Baidu’s ERNIE [5], Alibaba’s Tongyi Qianwen [2], and Sensetime’s SenseNova [27]. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters [38], Mini-GPT4 [39], and LLaVA [17]. LLaMA-Adapters aims to adapt LLaMA [33] into an instructionollowing model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows BLIP-2’s [15] architecture but replaces the language decoder with Vicuna [9], which better supports longer responses and multi-round conversations. LLaVA connects text and image modalities through a trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains both the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively high compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated attention layers to connect visual and language information and establish attention between in-context examples, leaving the vision encoder and language decoder frozen. 2.2. Multi-modal Instruction Tuning Dataset The concept of instruction tuning in multi-modal models was first introduced in Multi-Instruct [36], where 47 diverse multi-modal tasks covering 11 broad categories were organized. Each task comprises at least 5,000 instances (input-output pairs) from existing open-source datasets and 5 expert-written instructions. Multi-Instruct covers most multi-modal tasks that require visual understanding and multi-modal reasoning, such as Visual Question Answering [11, 41, 30], Image Captioning [16], Image Generation [7], and Visual Relationship Understanding [14], among others. Similarly, MiniGPT4 [39] constructs its instruction following dataset by combining Conceptual Caption [28, 7], SBU [21], and LAION [26] with handwritten instruction templates. More recently, LLaVA [17] has brought the quality of an instruction tuning dataset to a higher level, as it was obtained by expanding the original captions of COCO [16] images with handwritten seed instructions using GPT-4 [19] to provide more detailed descriptions and multi-round conversations. To the best of our knowledge, the above-mentioned works are the only few that considered instruction tuning in multi-modal models. Our approach further differs from them in that we incorporate incontext examples into instruction tuning by grouping multiple similar instructions together to form a contextual example set. We are the first to propose the in-context instruction tuning paradigm in multi-modal models and to build the corresponding multi-modal in-context instruction tuning datasets. --- --3\\n--- METHOD ---\\nIn this section, we will introduce the details of the MIMIC-IT dataset in Sec. 3.1, our Otter’s training details in Sec. 3.2, and the integration with Hugging Fance ecosystem in Sec. 3.3. 3.1 Multi-Modal In-Context Instruction Tuning The OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in its few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs derived from individual HTML files, with significant contextual relationships between different pairs, as depicted in Fig 2(a). An MMC4 training data sample contains (i) a queried image-text pair, where the text typically describes the image, and (ii) context, which includes the remaining image-text pairs from the same HTML file. The primary training objective of OpenFlamingo is to generate text for the queried image-text pair, and the paradigm of generating query text conditioned on in-context examples ensures OpenFlamingo’s in-context learning capacity during the inference phase. Our Multi-Modal In-Context Instruction Tuning (MIMIC-IT) dataset aims to augment OpenFlamingo’s instruction comprehension capabilities while preserving its in-context learning capacity. To unleash OpenFlamingo’s instruction-following potential, we compile data from visual-language tasks into image-instruction-answer triplets. Concurrently, to maintain OpenFlamingo’s in-context learning capacity, we retrieve in-context examples for each triplet, which often lack correlated context, such as a visual question-answer data sample in VQAv2 [3]. Specifically, each MIMIC-IT data sample consists of (i) a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and (ii) context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset. The training objective for MIMIC-IT is to generate the answer within the queried image-instruction-answer triplet. The image-instruction-answer triplets are derived from (i) visual question-answer datasets, namely, VQAv2 [3] and GQA [13], (ii) visual instruction datasets, such as LLaVA [17], (iii) an in-progress, high-quality panoptic video scene graph dataset from the PVSG repository. For each video, we select 4-8 frames for instruction-following annotation, using the LLaVA dataset as a reference. We have developed three heuristics to construct the context for each image-instruction-answer triplet, as illustrated in Fig 2(b). 3.2 Training Details Our approach adopts the OpenFlamingo training paradigm to train the Otter model. The pretrained OpenFlamingo model comprises a LLaMA-7B [33] language encoder and a CLIP ViT-L/14 [24] vision encoder. To prevent overfitting and leverage pretrained knowledge, we freeze both the encoders and only finetune the Perceiver resampler module, cross-attention layers inserted into the language encoder and input/output embeddings of the language encoder. This results in approximately 1.billion trainable parameters for the Otter model. To optimize our model, we employ the AdamW optimizer [18] with a starting learning rate of 10-5 and a batch size of 4. We train Otter for 6 epochs, with the learning rate scheduled using a cosine annealing scheduler. We also use gradient clipping of a threshold of 1.0 to prevent exploding gradients. During our training, we follow a specific format to prepare our training data. The format includes a combination of image, user instruction, \"GPT\"-generated answers \\', and a special token known as the [endofchunk] token. We format the training data as follows: <context> [image] User:<instruction> GPT: [answers] <answer>.[endofchunk] where the [image], [answer], and [endofchunk] tokens are unique and serve a specific purpose. We design such a chatbot-like format to train our model to improve the instruction-following and conversation generalizability of the model. The [image] and [endofchunk] tokens are originally from the OpenFlamingo training paradigm, while the [answer] token is a new introduction by us in training Otter. The [answer] token separates the answers from the instruction, so that, we mask all tokens after the [answer] token during training and set them as the prediction objectives of the model. We train our model using a cross-entropy loss. \\'To support user-assistant conversations, we adopt \"GPT\" as the role label because it does not have any specific semantic meaning in vocabulary. --- --Title: WALNUT AND BLUE CHEESE STUFFED MUSHROOMS Text: “This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.”, Text: “The ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favourites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms.”, Title: HONDA: (UN)LOKING THE DOORS FROM THE INSIDE Text: “When you lock/unlock the driver’s door and tailgate using the master lock switch, all the other doors lock/ unlock at the same time.” Tock Tab Text: “When you lock the door using the lock tab on the driver’s door, all of the other doors and tailgate lock at the same time.”, (a) Multimodal c4 (MMC4) same instruction, different images in-context examples query Instruction: What Instruction: Instruction: skill set_do What skill set What skill set snowboarders need does this player does a player to perform such need to have\\\\in a need to excel in tricks? match, and why? this sport, based on the same image, different instructions in-context examples query Instruction: Instruction: Instruction: What is the What color is Which team does baseball player the baseball the baseball holding in his player\\'s player belong to? hands? uniform? Answer: To Answer: A tennis Answer: To perform tricks player improve his minimize the needs ..winning performance, ..his risks involved matches. team. sequential images, different instructions in-context examples Answer: The Answer: The Answer: The baseball player is baseball baseball player holding a baseball player\\'s uniform belongs to the bat in his hands. is grey. Angels baseball team. query Instruction: What is Instruction: Why did Instruction: Why is Instruction: Instruction: Why is the main thing the player in red who the man in the red Description of the the whole video happening in this was attacking fall to jersey about to stand videos humorous humorous? picture? the ground? up from the ground? moment? Bgeeseeeees Bee eaeaa —; =) Answer: A group Answer: Because Answer: Because ..the Answer: A man on the Answer: of ..attack and look he ..the ground to ball go in and didn\\'t pitch falls down thing ... for an opportunity to try to create a need to pretend to after a shot, .. \\\\with how his shoot. penalty. create a penalty. his teammates. to heal instant. (b) Multi-Modal In-Context Instruction Tuning (MIMIC-IT) The funny it\\'s funny injury seems in an Figure 2: Illustration of example data formats in MMC4 and MIMIC-IT. (a) The illustration of the data format in the MMC4 dataset that are used OpenFlamingo. (b) Three heuristics to build the multi-modal in-Context instruction tuning (MIMIC-IT) dataset. --- --3.3 Integratation with Hugging Face We have integrated Otter into Hugging Face Transformers [34] and trained it using the Hugging Face Accelerator”, which enables automatic mapping of the model weights to different GPU devices and offloading of overflowed weights to CPU or disk. Additionally, we use bf16 mixed precision during training. The total optimizations enable our model to be trained on 4x RTX-3090 GPUs, each with 24GB memory. Meanwhile, since Otter has been integrated into Hugging Face Transformers, it can now be reused with less than five lines of code, making it much easier for researchers to integrate into their respective training and inference pipelines (compared to the original OpenFlamingo implementation). We also provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed to enable greater training efficiency and less memory consumption. To enable future research and convenience, we also provide a script for converting the original OpenFlamingo-9B checkpoint into the Hugging Face Model format. The converted checkpoint and our Otter model are uploaded and available on the Hugging Face model hub luodian/ openflamingo-9b-hf and luodian/otter-9b-hf, respectively. 4 Demonstrations In this section, we show several examples of two types of demonstrations of Otter: (1) the ability to follow instructions in Sec. 4.1, and (2) the ability of learning to execute new instructions following provided in-context examples in Sec. 4.2. Compared with OpenFlamingo, these results demonstrate the importance of in-context instruction tuning and the improvement of Otter. 4.1 Following User Instructions In Sec. 3.1, we discussed how we finetuned Otter on visual instruction pairs to transform it into a powerful instruction follower. The results of our\\n--- EXPERIMENT ---\\ns are demonstrated in Fig. 3, where we observe that Otter is able to provide more detailed descriptions of images and follow user instructions more accurately. This characteristic of Otter is attributed to the co-design of our model and data, which leverages the generalization ability of a strong language decoder and the rich variety of instructions present in the MIMIC-IT dataset. By fine-tuning on visual instruction pairs, Otter is able to learn the nuances of human language and accurately apply it to visual input. In order to evaluate Otter’s reasoning capabilities, we designed a series of experiments to test its ability to handle complex scenarios that require a deeper understanding of the situation and more advanced commonsense reasoning. In Fig. 4, we present an example where OpenFlamingo fails to identify important clues in the image, while Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question. In this particular example, Otter is able to correctly interpret the confusion around the traffic signal, while also taking into account relevant contextual factors such as accidents, traffic congestion, and disruptions to the flow of traffic. This level of reasoning ability is not present in standard image captioning models and represents a significant advance in the field of computer vision and natural language processing. 4.2 Multi-Modal In-context Learning Otter is designed to support multi-modal in-context learning in a similar pattern to Flamingo [1] and OpenFlamingo [4], which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or a question-answer pair. In Fig. 5, we demonstrate Otter’s ability to perform visual question answering tasks, where users provide examples of instruction-answering pairs and images as contextual examples to prompt the model with subtasks, such as explaining memes or the danger of sports. Despite the fact that user inputs are often short and restricted in these scenarios, Otter is able to provide more comprehensive answers. By leveraging the information contained in the images and other contextual examples, Otter is able to better understand the underlying meaning and context of the questions, resulting in more accurate and informative answers. Hugging Face Accelerator --- --Instruction: Can you describe the main features of this image for me? OpenFlaningo: The man is flying a kite. Otter: The image shows a person standing in front of a kite that is flying in the sky. The person is wearing a black shirt and is positioned on the left side of the kite. The kite is quite large and can be seen above the person\\'s head. The scene appears to be outdoors, with the person possibly preparing to launch the kites or enjoying the moment as the kit flies in the breeze. There are also two other people in the background, one on the right side and another person partially visible on the far right. The presence of multiple people in this outdoor setting suggests that it might be a gathering or a popular location for kite flying and leisure activities. Instruction: Write a detailed description of the given image? Openflamingo: Pancakes with bacon and maple syrup. Otter: The image features a wooden table with various items on it. There is a sandwhich, which has been cut in half, and a bowl of cereal placed near the plate. Additionally, there is a cup, likely containing a beverage such as coffee. The table also has a “\"live baccy\"\" container and a mug present, suggesting a cozy and relaxed setting. The scene might be a part of a breakfast or snack time arrangement, with the sandwich, cereals, and beverage as the main items on the table. Figure 3: Comparisons of image captioning results between OpenFlamingo and Otter. Otter provides more detailed and precise descriptions of the images. Instruction: What could happen if drivers become confused by the traffic signals at this intersection? OpenFlamingo: They could get stuck in the rain. Otter: If drivers become confused by the traffic signals at the intersection, it could lead to various potential scenarios: 1. Red light: The red light signal is on, and the car is moving. In this case, the driver might continue driving, potentially causing a redlight runner accident, which can be dangerous for pedestrians and other vehicles. 2. Green light: If the green light signal appears while the car has already started moving, the vehicle might continue moving, potentially leading to a green-light running accident, where the driver does not stop at the green signal and risks colliding with other vehicles or pedestrian crossings. 3. Confusion about the signal: If drivers are unsure about the traffic lights, they might hesitate or make an unpredictable maneuverer, which could lead not only to accidents but also to traffic congestion and general disruption of the flow of traffic. Figure 4: Comparisons of situation understanding results between OpenFlamingo and Otter. Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question. --- --User provided examples otter reaction Instruction: what does Instruction: what does Instruction: what does this meme usually use for? this meme usually use this meme usually use Otter: This meme typically uses a sad-looking baby for? for? as the main image, with text encouraging people to Answer: to ask people to Answer: to express users stop being a baby and to be more like a tough, think twice and dont be feeling of sad, or cool individual. The image and text are meant to silly moving. create a playful and humorous contrast between a cute, vulnerable baby and a tuff, strong person. User provided examples otter reaction r . Instruction: What is Instruction: What is Instruction: What is the danger of this sport? the danger of this the danger of this Otter: The sport involves players running and sport? sport? trying to catch the ball while others are standing Answer: The player may Answer: While chasing in the grass, which can lead to collisions or get hitted by the the baseball, the accidents. These collisions can result in injuries tennis ball. player may to the players involved. It is essential for inadvertently collide players to be aware of their surroundings and with other players. maintain a safe distance from each other to minimize the risk of accidents during the game. Figure 5: In-context learning examples of Otter. Given two examples of instruction-answering pairs and an image, Otter is able to provide more comprehensive answers. 5\\n--- CONCLUSION ---\\nIn this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial finetuning on MIMIC-IT dataset, we observe that Otter can convert OpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities. With the assistance of rich instructions from images and videos, Otter generalizes to achieve better instruction-following and situation-understanding performances. 5.1 Limitations Language Hallucination. Since Otter is built upon OpenFlamingo, which depends upon LLaMA, the hallucination issue of LLaMA is inherited by Otter. Also, the current Otter model may hallucinate the language that is not related to the image. This issue might be solved by introducing negative examples in the training data. 5.2 Future Supports In the future, we plan to explore the integration of more efficient training schemas (e.g., parameterefficient finetuning such as LoRA [12]) and more modalities (e.g., 3D vision). Acknowledgement. We thank Chunyuan Li and Jack Hessel for their advice and support, as well as the OpenFlamingo team for their great contribution to the open-source community. --- --References 1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 2, 3,Alibaba. Tongyi qianwen. 2023.3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015.Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 3,5] Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023.6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021.Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Language models are visual reasoning coordinators. In JCLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1,[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017. 2,Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019.Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32-73, 2017.Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1,Tsung- Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 2,--- --{17] [18] [19] [20] [21] [22] 23] 24] 25] 26] [27] [28] [29] [30] [31] [32] [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 3,Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.OpenAI. Gpt-4 technical report. 2023. 1,OpenAI. Introducing chatgpt. 2023. 1,Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images usingmillion captioned photographs. Advances in neural information processing systems, 24, 2011.Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022. | Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.SenseTime. Sense nova. 2023.Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, 2017.Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https: //github.com/tatsu-lab/stanford_alpaca, 2023.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3,--- --[34] [35] [36] [37] [38] [39] [40] [41] [42] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. 2,Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 1,Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1,Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023.Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995-5004, 2016.Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270, 2022.\\n',\n",
       " '--- ABSTRACT ---\\n대규모 언어 모델(LLM)은 방대한 양의 텍스트 데이터에 대한 사전 학습으로 인해 다양한 작업에서 소수/제로 샷 학습자로서 상당한 보편적 역량을 입증했습니다. GPT-3가 InstrctGPT 및 ChatGPT로 부스트되어 자연어 명령을 효과적으로 따라 실제 작업을 완료하는 것이 그 예입니다. 이 논문에서는 Flamingo 모델의 업스트림 인터리브 형식 사전 학습 데이터 세트에서 영감을 받아 다중 모달 모델에 명령어 튜닝을 도입하는 것을 제안합니다. 유사한 접근 방식을 채택하여 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 데이터 세트를 구성합니다. 그런 다음 MIMIC-IT에서 학습하고 향상된 명령어 따르기 능력과 컨텍스트 내 학습을 보여주는 OpenFlamingo(DeepMind의 Flamingo 오픈 소스 버전) 기반 다중 모달 모델인 Otter를 소개합니다. 또한 연구자들을 위해 OpenFlamingo의 구현을 최적화하여 필요한 교육 리소스를 1× A100 GPU에서 4× RTX-GPU로 민주화하고, 더 많은 연구자들이 모델을 사용자 지정 교육 및 추론 파이프라인에 통합할 수 있도록 OpenFlamingo와 Otter를 모두 Huggingface Transformers에 통합합니다. 1\\n--- INTRODUCTION ---\\n&amp; 동기 부여 대규모 언어 모델(LLM)은 다양한 작업을 소수/제로 샷 학습기로 수행하는 데 있어 상당한 보편적 역량을 입증했습니다. 이러한 모델은 방대한 양의 텍스트 데이터로 사전 학습되었으며 GPT-2[25] 및 GPT-3[6]과 같은 최근 연구에서 선보였습니다. 최근 연구에서는 GPT-3[6]을 InstrctGPT[22] 및 ChatGPT[20]로 강화하여 자연어 명령을 효과적으로 따라 실제 작업을 수행하고 다운스트림 미세 조정 중에 작업별 규칙을 명령으로 사용자 지정할 수 있도록 하여 사전 학습된 모델이 사용자 의도를 보다 효과적으로 이해하고 정확하고 관련성 있는 응답을 생성할 수 있도록 하는 것처럼 LLM에 권한을 부여하는 데 있어 명령 조정의 중요성을 강조했습니다. 유사한 시도가 다중 모달 모델에도 도입되었습니다. LLaMA-Adapter[38]는 추가 어댑터 모듈과 다중 모달 프롬프트를 추가하여 LLAMA[33]를 명령 따르기 모델로 적용하는 것을 목표로 합니다. Mini-GPT4[39]는 BLIP-2[15]의 아키텍처를 따르지만 언어 디코더를 더 긴 답변을 지원하는 Vicuna[9]로 대체합니다.LLaVA[17]는 동일한 CLIP[23] 비전 인코더와 Vicuna[9] 언어 디코더를 사용하고 GPT-4[19]에서 큐레이팅한 고품질 명령어 데이터 세트를 미세 조정합니다.이러한 작업은 뛰어난 결과를 달성하고 귀중한 통찰력을 제공했지만 사소한 공통 문제가 있습니다.특히 Equal Contribution Corresponding Author Technical Report에서 전체 모델이나 연결 부분을 미세 조정합니다.MMC텍스트가 섞인 수십억 규모의 이미지 코퍼스 OpenFlamingo MIMIC-IT 다중 모달 명령어 튜닝 데이터 세트와 컨텍스트 내 예제 Otter 그림 1: Otter 개요.Otter는 OpenFlamingo를 기반으로 제안한 MIMIC-IT 데이터 세트에서 미세 조정된 다중 모달 모델입니다.Otter 모델은 주어진 지침을 따르고 컨텍스트 내 예제를 활용하여 작업을 실행하는 향상된 기능을 보여줍니다. 작업별 데이터. 예를 들어, 일반적인 관행은 캡션[16] 또는 VQA[11] 작업의 이미지-텍스트 데이터 쌍을 사용하여 시각 및 언어 모듈을 정렬하는 것입니다. 이런 방식으로 언어 모델에 시각 정보를 내장하는 것이 효과적일 수 있지만, 정렬 모듈을 훈련하는 데 데이터가 사용되는 작업에 의존하기 때문에 이 관행이 본질적으로 작업에 따라 달라지는지 의문을 제기합니다. 성찰한 결과, DeepMind Flamingo[1]의 업스트림 사전 훈련 데이터 세트인 MultiModal MassiveWeb(M3W)은 보다 자연스러운 방식으로 시각 및 언어 정보를 정렬하는 데 상당한 가치가 있음을 발견했습니다. 이 데이터 세트는 모든 이미지와 텍스트가 인터리브 형식으로 배열된 HTML 웹 페이지로 구성됩니다. 구체적으로, 텍스트 조각은 위나 아래의 이미지(또는 비디오)를 설명할 수 있으며, 인접한 위치의 이미지(또는 비디오)와 텍스트 사이에 상관 관계가 있을 수 있습니다. 이러한 자연스러운 맥락 구성은 텍스트가 해당 이미지만 설명하는 캡션 데이터 세트보다 더 풍부한 정보를 제공합니다. 이 데이터 세트에서 학습된 Flamingo는 제로 및 퓨샷 일반화와 컨텍스트 내 학습 능력을 달성하여 멀티 모달 도메인에서 GPT-3 모멘트가 되었습니다. 그러나 DeepMind는 모델의 뛰어난 성능으로 인해 조기에 놀라움을 유발할 수 있기 때문에 Flamingo 모델과 M3W 데이터 세트를 대중에게 공개하지 않았습니다. 그럼에도 불구하고 LAION-AI의 OpenFlamingo 프로젝트[4]가 최근 공개되어 더 큰 규모로 동일한 인터리브 형식으로 해당 MMC4[40] 데이터 세트에 대한 액세스를 제공했습니다. 결과적으로 커뮤니티 연구자는 멀티 모달 모델 분야에서 추가 연구 및 개발을 위해 Flamingo 시리즈 작업을 계속 따를 수 있습니다. OpenFlamingo 모델은 인상적인 멀티 모달 컨텍스트 내 학습 능력을 보여주고 주어진 컨텍스트 내 예제로 작업을 실행하지만 업스트림 사전 학습된 모델로서 다운스트림 작업을 보다 효과적으로 수행하려면 여전히 명령어 튜닝이 필요합니다. 논문에서 우리는 MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트를 제안하고 3.1절에서 구성에 대한 세부 정보를 제공합니다. 그런 다음 OpenFlamingo를 기반으로 하는 컨텍스트 내 명령어 튜닝을 갖춘 멀티 모달 모델인 Otter를 소개합니다. 그림 1에서 Otter와 OpenFlamingo의 관계를 설명합니다. MIMIC-IT 데이터 세트에서 미세 조정된 Otter 모델은 4.1절의 정성적 분석에서 볼 수 있듯이 OpenFlamingo에 비해 명령어 수행 능력이 향상되었습니다. 한편, Otter는 4.2절에서 볼 수 있듯이 제공된 컨텍스트 내 학습 예제를 사용하여 명령어를 실행하는 법을 학습할 수 있습니다. 엔지니어링 관점에서 우리는 OpenFlamingo의 구현을 최적화하여 연구자들이 더 쉽게 접근할 수 있도록 했습니다. 최적화에는 최소 1× A100 GPU에서 4× RTX3090 GPU로 학습 요구 사항을 최적화하고 이를 Hugging Face Transformers[34]에 통합하여 몇 줄의 코드로 학습 및 추론을 간소화하는 것이 포함됩니다. 저희의 기여는 다중 모달 모델 분야에서 추가 연구 및 개발을 용이하게 합니다. 주요 기여 사항을 요약하면 다음과 같습니다. • MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트를 제공합니다. 각 데이터 샘플에는 명령어-이미지-답변 트리플릿과 해당 컨텍스트 내 예제가 포함됩니다. • OpenFlamingo를 기반으로 컨텍스트 내 명령어 튜닝을 갖춘 다중 모달 모델인 Otter를 소개합니다. 이 모델은 몇 가지 컨텍스트 내 학습 예제로 명령어를 따르고 새 명령어를 실행할 수 있습니다. • OpenFlamingo 구현에 대한 최적화에는 학습 요구 사항을 4× RTX3090 GPU로 줄이고 이를 Huggingface Transformers에 통합하여 몇 줄의 코드로 학습 및 추론을 간소화하는 것이 포함됩니다.2\\n--- RELATED WORK ---\\n2.1 대규모 멀티모달 모델 ChatGPT[20], GPT-4[19] 및 기타 대규모 언어 모델[33, 32, 9]의 최근 성공으로 최근 연구에서는 사전 학습된 언어 모델을 기반으로 다른 모달리티의 정보를 통합하는 것을 탐색하기 시작했습니다. 이러한 연구는 언어 모델의 기능을 더 많은 작업과 모달리티로 확장하며 두 가지 관점으로 분류할 수 있습니다. 시스템 설계 관점. 이 관점은 ChatGPT[20]를 디스패치 스케줄러로 사용하고 이를 통해 다양한 전문가 모델을 연결하여 다양한 시각적 작업을 허용하는 것을 포함합니다. 언어 프롬프트는 해당 작업 도메인 내에서 전문가 시각 언어 모델을 호출하기 위한 인터페이스 역할을 합니다. 이 범주의 작업에는 VisualChatGPT[35], HuggingGPT[29], Cola[8], XGPT[42], MM-REACT[37] 및 ViperGPT[31]가 있습니다. 이 접근 방식은 각 모델이 새 작업에 대해 개별적으로 훈련될 수 없다는 한계가 있으며, ChatGPT[20]를 강력한 명령어 전송 도구로 사용하면 API 쿼리 비용이 높아질 수 있습니다.종단 간 학습 가능 모델 관점.이 관점은 다양한 모달리티의 모델을 통합된 종단 간 학습 가능 모델(다중 모달 기반 모델이라고도 함)로 연결하는 데 중점을 둡니다.이 분야의 초기 작업으로는 언어와 비전을 모델링하기 위한 통합 아키텍처를 제안하고 나중에 LAIONAI에서 OpenFlamingo[4]로 오픈 소스화한 Flamingo[1]가 있습니다.기존의 다른 작업으로는 가벼운 쿼리 변환기와 2단계 부트스트랩 사전 학습을 사용하여 이미지의 정보를 텍스트 모달리티에 연결하는 BLIP-2[15]가 있습니다. GPT-4[19]의 인기로 2023년 이후 이 분야에 대한 관심이 증가했습니다. 엔터프라이즈 수준의 제품 모델에는 아직 출시되지 않은 OpenAI의 GPT-4[19] 비전 언어 버전, Google의 PaLM-E[10], Baidu의 ERNIE[5], Alibaba의 Tongyi Qianwen[2], Sensetime의 Senseova[27]가 포함됩니다. 학술적 멀티모달 노력에는 LLaMA-Adapters[38], Mini-GPT4[39], LLaVA[17]와 같은 다양한 모델이 포함됩니다. LLaMA-Adapters는 추가 어댑터 모듈과 멀티모달 프롬프트가 있는 명령어 따르기 모델로 LLaMA[33]를 적용하는 것을 목표로 합니다. Mini-GPT4는 BLIP-2의[15] 아키텍처를 따르지만 언어 디코더를 Vicuna[9]로 대체하여 더 긴 응답과 다중 라운드 대화를 더 잘 지원합니다. LLaVA는 훈련 가능한 프로젝터 매트릭스를 통해 텍스트와 이미지 모달리티를 연결합니다.이것은 간단한 경량 선형 계층입니다.그러나 LLaVA는 비전 인코더와 언어 디코더를 모두 지시 튜닝 데이터 세트에서 훈련하기 때문에 다른 모델에 비해 비용이 상대적으로 높습니다.반대로 Otter는 Flamingo 모델을 기반으로 몇 개의 교차 게이트 주의 계층을 훈련하여 시각 및 언어 정보를 연결하고 맥락 내 예제 간에 주의를 확립하여 비전 인코더와 언어 디코더를 고정시킵니다.2.2 다중 모달 지시 튜닝 데이터 세트 다중 모달 모델에서 지시 튜닝이라는 개념은 Multi-Instruct [36]에서 처음 소개되었으며, 11개의 광범위한 범주를 포괄하는 47개의 다양한 다중 모달 작업이 구성되었습니다.각 작업은 기존 오픈 소스 데이터 세트의 최소 5,000개 인스턴스(입력-출력 쌍)와 5개의 전문가가 작성한 지침으로 구성됩니다. Multi-Instruct는 Visual Question Answering[11, 41, 30], Image Captioning[16], Image Generation[7], Visual Relationship Understanding[14] 등과 같이 시각적 이해와 멀티모달 추론이 필요한 대부분의 멀티모달 작업을 포괄합니다.마찬가지로 MiniGPT4[39]는 Conceptual Caption[28, 7], SBU[21], LAION[26]을 손으로 쓴 지침 템플릿과 결합하여 지침 따르기 데이터 세트를 구성합니다.최근에 LLAVA[17]는 GPT-4[19]를 사용하여 손으로 쓴 시드 지침이 있는 COCO[16] 이미지의 원래 캡션을 확장하여 더 자세한 설명과 다중 라운드 대화를 제공함으로써 지침 튜닝 데이터 세트의 품질을 더 높은 수준으로 끌어올렸습니다.우리의 지식에 따르면, 위에 언급된 작업은 멀티모달 모델에서 지침 튜닝을 고려한 유일한 몇 안 되는 작업입니다. 우리의 접근 방식은 여러 유사한 명령어를 그룹화하여 문맥 내 예제 세트를 형성함으로써 명령어 튜닝에 문맥 내 예제를 통합한다는 점에서 그들과 더욱 다릅니다. 우리는 멀티모달 모델에서 문맥 내 명령어 튜닝 패러다임을 제안하고 해당 멀티모달 문맥 내 명령어 튜닝 데이터 세트를 구축한 최초의 기업입니다.3\\n--- METHOD ---\\n이 섹션에서는 Sec. 3.1에서 MIMIC-IT 데이터 세트의 세부 사항, Sec. 3.2에서 Otter의 훈련 세부 사항, Sec. 3.3에서 Hugging Fance 생태계와의 통합을 소개합니다. 3.1 OpenFlamingo 프레임워크의 다중 모달 문맥 내 명령어 튜닝은 인터리브된 다중 모달 MMC4 데이터 세트를 활용하여 몇 번의 샷, 문맥 내 학습 기능을 구현합니다. MMC4 데이터 세트는 개별 HTML 파일에서 파생된 이미지-텍스트 쌍으로 구성되며, 그림 2(a)에서 볼 수 있듯이 서로 다른 쌍 간에 중요한 문맥적 관계가 있습니다. MMC4 훈련 데이터 샘플에는 (i) 일반적으로 이미지를 설명하는 텍스트인 쿼리된 이미지-텍스트 쌍과 (ii) 동일한 HTML 파일의 나머지 이미지-텍스트 쌍을 포함하는 컨텍스트가 포함됩니다. OpenFlamingo의 주요 학습 목표는 쿼리된 이미지-텍스트 쌍에 대한 텍스트를 생성하는 것이며, 컨텍스트 내 예제에 따라 쿼리 텍스트를 생성하는 패러다임은 추론 단계 동안 OpenFlamingo의 컨텍스트 내 학습 용량을 보장합니다. 당사의 MIMIC-IT(Multi-Modal In-Context Instruction Tuning) 데이터 세트는 컨텍스트 내 학습 용량을 보존하는 동시에 OpenFlamingo의 명령어 이해 기능을 증강하는 것을 목표로 합니다. OpenFlamingo의 명령어 추종 잠재력을 최대한 발휘하기 위해 시각 언어 작업의 데이터를 이미지-명령-답변 3중항으로 컴파일합니다. 동시에 OpenFlamingo의 컨텍스트 내 학습 용량을 유지하기 위해 VQAv2[3]의 시각적 질문-답변 데이터 샘플과 같이 종종 상관 관계가 없는 각 3중항의 컨텍스트 내 예제를 검색합니다. 구체적으로, 각 MIMIC-IT 데이터 샘플은 (i) 이미지에 맞게 조정된 명령-답변이 포함된 쿼리된 이미지-명령-답변 3중항과 (ii) 컨텍스트로 구성됩니다. 컨텍스트에는 쿼리된 3중항과 컨텍스트적으로 상관관계가 있는 일련의 이미지-명령-답변 3중항이 포함되어 있으며, MMC4 데이터 세트에서 발견된 쿼리된 이미지-텍스트 쌍과 컨텍스트 간의 관계를 에뮬레이션합니다. MIMIC-IT의 학습 목표는 쿼리된 이미지-명령-답변 3중항 내에서 답을 생성하는 것입니다. 이미지-명령-답변 3중항은 (i) 시각적 질문-답변 데이터 세트, 즉 VQAv2 [3] 및 GQA [13], (ii) LLaVA [17]와 같은 시각적 명령 데이터 세트, (iii) PVSG 저장소의 진행 중인 고품질 파노라마 비디오 장면 그래프 데이터 세트에서 파생됩니다. 각 비디오에 대해 LLaVA 데이터 세트를 참조로 사용하여 명령-추종 주석을 위한 4-8개 프레임을 선택합니다. 그림 2(b)에 나와 있듯이, 우리는 각 이미지-명령-답변 3중 구조에 대한 맥락을 구성하기 위해 세 가지 휴리스틱을 개발했습니다.3.2 학습 세부 정보 우리의 접근 방식은 OpenFlamingo 학습 패러다임을 채택하여 Otter 모델을 학습합니다.사전 학습된 OpenFlamingo 모델은 LLaMA-7B[33] 언어 인코더와 CLIP ViT-L/14[24] 비전 인코더로 구성됩니다.과잉 맞춤을 방지하고 사전 학습된 지식을 활용하기 위해 두 인코더를 모두 동결하고 Perceiver 리샘플러 모듈, 언어 인코더에 삽입된 교차 주의 계층 및 언어 인코더의 입출력 임베딩만 미세 조정합니다.이를 통해 Otter 모델에 대해 약 10억 개의 학습 가능한 매개변수가 생성됩니다.모델을 최적화하기 위해 시작 학습 속도가 10-5이고 배치 크기가 4인 AdamW 최적화 프로그램[18]을 사용합니다.코사인 어닐링 스케줄러를 사용하여 학습 속도를 예약하여 6개 에포크 동안 Otter를 학습합니다. 또한 폭발하는 그래디언트를 방지하기 위해 임계값 1.0의 그래디언트 클리핑을 사용합니다. 훈련하는 동안 특정 형식을 따라 훈련 데이터를 준비합니다. 형식에는 이미지, 사용자 지침, &quot;GPT&quot; 생성 답변 1, [endof chunk] 토큰이라는 특수 토큰의 조합이 포함됩니다. 훈련 데이터 형식은 다음과 같습니다.<context> [이미지] 사용자:<instruction> GPT: [답변]<answer> . [endof chunk] 여기서 [image], [answer], [endof chunk] 토큰은 고유하며 특정 목적을 갖습니다. 우리는 모델의 지시 따르기와 대화 일반화를 개선하기 위해 모델을 훈련하기 위해 이러한 챗봇과 같은 형식을 설계합니다. [image] 및 [endof chunk] 토큰은 원래 OpenFlamingo 훈련 패러다임에서 가져온 반면 [answer] 토큰은 Otter 훈련에서 우리가 새롭게 도입한 것입니다. [answer] 토큰은 답변과 지시를 분리하므로 훈련 중에 [answer] 토큰 뒤의 모든 토큰을 마스크하고 모델의 예측 목표로 설정합니다. 우리는 교차 엔트로피 손실을 사용하여 모델을 훈련합니다. ¹사용자-지원 대화를 지원하기 위해, 어휘에서 특정 의미적 의미가 없기 때문에 &quot;GPT&quot;를 역할 레이블로 채택했습니다.제목: 호두와 블루치즈를 넣은 버섯 제목: HONDA: 안에서 문을 (보이지 않게) 보기 텍스트: &quot;이 호두와 블루치즈를 넣은 버섯 레시피 Fisher Natural은 Fisher Nuts에서 후원합니다.&quot;, 텍스트: &quot;마스터 잠금 스위치를 사용하여 운전석 도어와 테일게이트를 잠그거나 잠금 해제하면 다른 모든 도어가 동시에 잠기거나 잠금 해제됩니다.&quot; 잠금 - 잠금 탭 잠금 해제 텍스트: &quot;버섯을 채우는 아이디어는 무궁무진하여, 가지고 놀 수 있는 조합이 너무나 많은데, 개인적으로 가장 좋아하는 것 중 몇 가지는 지중해식 버섯과 시금치 텍스트: &quot;운전석 문의 잠금 탭을 사용하여 문을 잠그면 다른 모든 문과 테일게이트가 동시에 잠깁니다.&quot;, 구운 소나무 열매 버섯입니다.&quot;, (a) 멀티모달 C4(MMC4) 잠금 해제/ - 잠금 마스터 도어 잠금 스위치 동일한 지침, 다른 이미지의 문맥 예 지침: 스노보더가 이런 트릭을 수행하는 데 필요한 기술은 무엇입니까? 지침: 이 선수가 경기에서 가져야 하는 기술은 무엇이며, 그 이유는 무엇입니까? 쿼리 지침: 이미지에 따르면, 선수가 이 스포츠에서 탁월해지려면 어떤 기술이 필요합니까? 동일한 이미지, 다른 지침의 문맥 예 지침: 야구 선수가 손에 무엇을 들고 있습니까? 지침: 야구 선수의 유니폼은 무슨 색입니까? 쿼리 지침: 야구 선수가 어느 팀에 속합니까? ANGEL ANGEL ANGEL ANGEL 정답: 정답: 트릭을 수행하려면 ... 정답: 테니스 선수가 정답: 관련 위험을 최소화하려면 ...경기에서 승리해야 합니다. 성과를 개선하고 ...팀을 이겨야 합니다. 야구 선수가 손에 야구 배트를 들고 있습니다. 연속적인 이미지, 다른 지침 정답: 야구 선수의 유니폼은 회색입니다. 정답: 야구 선수는 엔젤스 야구 팀에 속합니다. 문맥별 예 설명: 이 그림에서 주로 일어나는 일은 무엇입니까? 설명: 공격하던 빨간색 옷을 입은 선수가 왜 땅에 쓰러졌습니까? 설명: 빨간색 유니폼을 입은 남자가 왜 땅에서 일어나려고 합니까? 설명: 비디오의 유머러스한 순간에 대한 설명은 무엇입니까? 쿼리 설명: 전체 비디오가 왜 유머러스합니까? FALKE 정답: ...공격하고 기회를 찾아 답: 슛을 했기 때문입니다. 페널티를 만들려고 땅을 쳤습니다. 정답: ...공이 들어갔고 페널티를 만들려고 가장할 필요가 없었습니다. 정답: 경기장에 있는 한 남자가 슛 후 쓰러지고 ... \\\\팀원들과 함께. 답변: 재밌는 건 그의 부상이 순식간에 아물어가는 게 재밌다는 거예요.(b) 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 그림 2: MMC4 및 MIMIC-IT의 예시 데이터 형식에 대한 설명.(a) OpenFlamingo에서 사용하는 MMC4 데이터 세트의 데이터 형식에 대한 설명.(b) 다중 모달 컨텍스트 내 명령어 튜닝(MIMIC-IT) 데이터 세트를 빌드하는 세 가지 휴리스틱.3.3 Hugging Face와의 통합 Otter를 Hugging Face Transformers[34]에 통합하고 Hugging Face Accelerator²를 사용하여 학습시켰습니다.이를 통해 모델 가중치를 다른 GPU 장치에 자동으로 매핑하고 오버플로된 가중치를 CPU 또는 디스크로 오프로드할 수 있습니다.또한 학습 중에 bf16 혼합 정밀도를 사용합니다.전체 최적화를 통해 각각 24GB 메모리가 있는 4×RTX-3090 GPU에서 모델을 학습할 수 있습니다. 한편, Otter가 Hugging Face Transformers에 통합되었기 때문에 5줄 미만의 코드로 재사용할 수 있어 연구자들이 각자의 훈련 및 추론 파이프라인에 통합하기가 훨씬 쉬워졌습니다(원래 OpenFlamingo 구현과 비교).또한 Fully Sharded Data Parallel(FSDP) 및 DeepSpeed를 지원하여 훈련 효율성을 높이고 메모리 소비를 줄입니다.향후 연구와 편의성을 위해 원래 OpenFlamingo-9B 체크포인트를 Hugging Face Model 형식으로 변환하는 스크립트도 제공합니다.변환된 체크포인트와 Otter 모델은 각각 Hugging Face 모델 허브인 luodian/openflamingo-9b-hf 및 luodian/otter-9b-hf에 업로드되어 제공됩니다.데모 이 섹션에서는 Otter의 두 가지 유형의 데모에 대한 몇 가지 예를 보여줍니다.(1) Sec. 4.1의 지침을 따르는 기능, (2) Sec. 4.1에서 제공된 컨텍스트 내 예제에 따라 새 지침을 실행하는 방법을 학습하는 기능입니다. 4.2. OpenFlamingo와 비교했을 때, 이러한 결과는 컨텍스트 내 명령어 튜닝과 Otter의 개선의 중요성을 보여줍니다. 4.1 사용자 명령어 따르기 3.1절에서 우리는 시각적 명령어 쌍에서 Otter를 미세 조정하여 강력한 명령어 팔로워로 변환하는 방법을 논의했습니다. 우리의 결과는\\n--- EXPERIMENT ---\\n그림 3에서 Otter가 이미지에 대한 보다 자세한 설명을 제공하고 사용자 지시를 보다 정확하게 따를 수 있음을 관찰할 수 있습니다. Otter의 이러한 특징은 강력한 언어 디코더의 일반화 능력과 MIMIC-IT 데이터 세트에 있는 다양한 지시를 활용하는 모델과 데이터의 공동 설계에 기인합니다. 시각적 지시 쌍을 미세 조정함으로써 Otter는 인간 언어의 뉘앙스를 학습하고 시각적 입력에 정확하게 적용할 수 있습니다. Otter의 추론 기능을 평가하기 위해 상황에 대한 더 깊은 이해와 보다 진보된 상식적 추론이 필요한 복잡한 시나리오를 처리하는 능력을 테스트하는 일련의 실험을 설계했습니다. 그림 4에서 OpenFlamingo가 이미지에서 중요한 단서를 식별하지 못하는 반면 Otter는 장면에 대한 더 깊은 이해를 보여주고 관련 상식적 지식을 적용하여 주어진 질문에 답할 수 있는 예를 제시합니다. 이 특정 예에서 Otter는 교통 신호 주변의 혼란을 올바르게 해석하는 동시에 사고, 교통 체증, 교통 흐름 방해와 같은 관련 맥락적 요인을 고려할 수 있습니다. 이 수준의 추론 능력은 표준 이미지 캡션 모델에는 존재하지 않으며 컴퓨터 비전 및 자연어 처리 분야에서 상당한 진전을 나타냅니다. 4.2 다중 모달 맥락 내 학습 Otter는 Flamingo [1] 및 OpenFlamingo [4]와 유사한 패턴으로 다중 모달 맥락 내 학습을 지원하도록 설계되었으며, 여기에는 캡션이나 질의-답변 쌍에 해당하는 이미지와 같은 해당 미디어에 언어 모델을 조건화하는 것이 포함됩니다. 그림 5에서 Otter가 시각적 질의-답변 작업을 수행하는 기능을 보여줍니다. 여기서 사용자는 지시-답변 쌍과 이미지를 맥락적 예로 제공하여 밈이나 스포츠의 위험을 설명하는 것과 같은 하위 작업을 모델에 촉구합니다. 이러한 시나리오에서 사용자 입력이 종종 짧고 제한적이라는 사실에도 불구하고 Otter는 보다 포괄적인 답변을 제공할 수 있습니다. 이미지에 포함된 정보와 다른 맥락적 예를 활용하여 Otter는 질문의 근본적인 의미와 맥락을 더 잘 이해하여 더 정확하고 유익한 답변을 얻을 수 있습니다. 2Hugging Face Accelerator 지침: 이 이미지의 주요 특징을 설명해 주시겠습니까? OpenFlamingo: 남자가 연을 날리고 있습니다. Otter: 이미지는 하늘을 날고 있는 연 앞에 서 있는 사람을 보여줍니다. 그 사람은 검은색 셔츠를 입고 연의 왼쪽에 위치하고 있습니다. 연은 매우 크고 그 사람의 머리 위로 보입니다. 장면은 야외에서 보이는데, 그 사람은 아마도 연을 던질 준비를 하고 있거나 연이 바람에 날리는 순간을 즐기고 있을 것입니다. 배경에는 두 명의 다른 사람이 있는데, 한 명은 오른쪽에 있고 다른 한 명은 가장 오른쪽에 부분적으로 보입니다. 이 야외 환경에 여러 사람이 있는 것은 그곳이 모임이거나 연 날리기와 여가 활동을 위한 인기 있는 장소일 수 있음을 시사합니다. 지침: 주어진 이미지에 대한 자세한 설명을 작성하세요? OpenFlamingo: 베이컨과 메이플 시럽을 곁들인 팬케이크. 수달: 이 이미지에는 다양한 품목이 놓인 나무 테이블이 있습니다. 반으로 잘린 샌드위치와 접시 근처에 놓인 시리얼 한 그릇이 있습니다. 또한 커피와 같은 음료가 들어 있는 컵이 있습니다. 테이블에는 &quot;살아있는 박테리아&quot; 용기와 머그잔이 있어 아늑하고 편안한 분위기를 보여줍니다. 이 장면은 샌드위치, 시리얼, 음료가 테이블의 주요 품목인 아침 식사 또는 간식 시간 구성의 일부일 수 있습니다. 그림 3: OpenFlamingo와 Otter의 이미지 캡션 결과 비교. Otter는 이미지에 대한 보다 자세하고 정확한 설명을 제공합니다. 지침: 운전자가 이 교차로의 교통 신호에 혼란스러워하면 어떻게 될까요? OpenFlamingo: 비에 갇힐 수 있습니다. Otter: 운전자가 교차로의 교통 신호에 혼란스러워하면 다음과 같은 다양한 잠재적 시나리오가 발생할 수 있습니다. 1. 빨간불: 빨간불 신호가 켜져 있고 차가 움직이고 있습니다. 이 경우 운전자는 운전을 계속하여 적신호 위반 사고를 일으킬 가능성이 있으며, 이는 보행자와 다른 차량에게 위험할 수 있습니다.2. 녹색 신호: 차량이 이미 움직이기 시작한 상태에서 녹색 신호가 나타나면 차량이 계속 움직여 운전자가 녹색 신호에서 멈추지 않아 다른 차량이나 횡단보도와 충돌할 위험이 있는 녹색 신호 위반 사고로 이어질 수 있습니다.3. 신호에 대한 혼란: 운전자가 신호등에 대해 확신이 없으면 주저하거나 예측할 수 없는 기동을 할 수 있으며, 이는 사고뿐만 아니라 교통 체증과 교통 흐름의 전반적인 방해로 이어질 수 있습니다.그림 4: OpenFlamingo와 Otter의 상황 이해 결과 비교.Otter는 현장에 대한 더 깊은 이해를 보여주고 관련 상식적 지식을 적용하여 주어진 질문에 답할 수 있습니다.사용자가 제공한 예제 Otter 반응 지침: 이 밈은 보통 무슨 용도로 사용하나요?답변: 사람들에게 두 번 생각하고 어리석은 짓을 하지 말라고 요청하는 것사용자가 제공한 예제 지침: 이 밈은 보통 무슨 용도로 사용하나요? 답변: 사용자의 슬픔이나 감동을 표현합니다.설명: 이 밈은 보통 무엇에 사용하나요? 수달: 이 밈은 보통 슬픈 표정의 아기를 메인 이미지로 사용하고, 텍스트는 사람들이 아기가 되는 것을 멈추고 강하고 멋진 사람이 되라고 격려합니다. 이미지와 텍스트는 귀엽고 연약한 아기와 강하고 터프한 사람 사이에 장난기 있고 유머러스한 대조를 만들어냅니다. 수달 반응 영상 설명: 이 스포츠의 위험은 무엇인가요? 답변: 선수가 테니스 공에 맞을 수 있습니다.설명: 이 스포츠의 위험은 무엇인가요? 답변: 야구공을 쫓는 동안 선수가 실수로 다른 선수와 충돌할 수 있습니다.설명: 이 스포츠의 위험은 무엇인가요? 수달: 이 스포츠는 다른 선수들이 풀밭에 서 있는 동안 선수들이 달리고 공을 잡으려고 하는 것으로, 충돌이나 사고로 이어질 수 있습니다. 이러한 충돌은 관련 선수에게 부상을 입힐 수 있습니다. 선수들은 주변 환경을 인식하고 서로 안전한 거리를 유지하여 경기 중 사고 위험을 최소화하는 것이 필수적입니다. 그림 5: Otter의 맥락 내 학습 사례. 지시-답변 쌍의 두 가지 사례와 이미지가 주어지면 Otter는 더 포괄적인 답변을 제공할 수 있습니다. 5\\n--- CONCLUSION ---\\n이 연구에서 우리는 명령어 튜닝을 사용한 다중 모달 문맥 내 학습 기반 모델인 Otter를 제안한다. MIMIC-IT 데이터 세트에 대한 부분적 미세 조정을 통해 Otter가 OpenFlamingo를 강력한 문맥 내 학습 능력을 갖춘 제로샷 시각적 명령어 모델로 변환할 수 있음을 관찰했다. 이미지와 비디오의 풍부한 명령어의 도움으로 Otter는 일반화하여 더 나은 명령어 따르기 및 상황 이해 성능을 달성한다. 5.1 한계 언어 환각. Otter는 LLAMA에 의존하는 OpenFlamingo를 기반으로 구축되었으므로 LLAMA의 환각 문제가 Otter에 계승되었다. 또한 현재 Otter 모델은 이미지와 관련이 없는 언어를 환각할 수 있다. 이 문제는 훈련 데이터에 부정적인 예를 도입하면 해결될 수 있다. 5.2 향후 지원 향후에는 보다 효율적인 훈련 스키마(예: LoRA[12]와 같은 매개변수 효율적 미세 조정)와 더 많은 모달리티(예: 3D 비전)를 통합하는 것을 탐색할 계획이다. 감사의 말. 우리는 Chunyuan Li와 Jack Hessel에게 조언과 지원에 감사드리고, 오픈소스 커뮤니티에 크게 기여한 OpenFlamingo 팀에도 감사드립니다.참고문헌 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 2, 3,[2] Alibaba. Tongyi qianwen. 2023.[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh. Vqa: 시각적 질의응답. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 2425-2433페이지, 2015.[4] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt. Openflamingo, 2023년 3월. 2, 3,[5] Baidu. Ernie bot: 지식 통합을 통한 향상된 표현. 2023.[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020.[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut. 개념적 12m: 긴 꼬리 시각적 개념을 인식하기 위한 웹 스케일 이미지-텍스트 사전 학습 추진. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3558-3568페이지, 2021.[8] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu. 언어 모델은 시각적 추론 조정자입니다. ICLR 2023 기초 모델의 수학적 및 경험적 이해에 대한 워크숍, 2023.[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시키는 오픈소스 챗봇, 2023년 3월. 1,[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: 구체화된 멀티모달 언어 모델. arXiv 사전 인쇄본 arXiv:2303.03378, 2023.[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh. vqa에서 v를 중요하게 만들기: 시각적 질의 응답에서 이미지 이해의 역할 강화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6904-6913페이지, 2017. 2,[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021.[13] Drew A Hudson 및 Christopher D Manning. Gqa: 실제 시각적 추론 및 구성적 질의 응답을 위한 새로운 데이터 세트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6700-6709페이지, 2019.[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image comments. International journal of computer vision, 123:32-73, 2017.[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. 1,[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트의 일반 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014. 2,[17] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee. 시각 지침 튜닝. arXiv 사전 인쇄본 arXiv:2304.08485, 2023. 1, 3,[18] Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. arXiv 사전 인쇄본 arXiv:1711.05101, 2017.[19] OpenAI. Gpt-4 기술 보고서. 2023. 1,[20] OpenAI. Chatgpt 소개. 2023. 1,[21] Vicente Ordonez, Girish Kulkarni, Tamara Berg. Im2text: 수백만 개의 캡션이 있는 사진을 사용하여 이미지 설명. 신경 정보 처리 시스템의 발전, 24, 2011.[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지침을 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35:27730-27744, 2022.[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독을 통해 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021.[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독을 통해 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021.[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 1(8):9, 2019.[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114, 2021.[27] SenseTime. Sense nova. 2023.[28] Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. 56회 연례 총회 의사록(제1권: 장문 논문), 2556-2565쪽, 2018년.[29] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. Hugginggpt: huggingface에서 chatgpt 및 그 친구들을 사용하여 AI 작업 해결. arXiv 사전 인쇄본 arXiv:2303.17580, 2023년.[30] Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi. 시각적 추론을 위한 자연어 코퍼스. 55회 연례 총회 의사록(제2권: 단편 논문), 217-223쪽, 2017년.[31] 디닥 수리스(Dídac Surís), 사치트 메논(Sachit Menon), 칼 본드릭(Carl Vondrick). Vipergpt: 추론을 위한 Python 실행을 통한 시각적 추론. arXiv 사전 인쇄 arXiv:2303.08128, 2023.[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang 및 Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델입니다. https://github.com/tatsu-lab/stanford_alpaca, 2023.[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. 1, 3,[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush. Transformers: State-of-the-the-heart 자연어 처리. 2020 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록: 시스템 시연, 38-45페이지, 온라인, 2020년 10월. 계산 언어학 협회. 2,[35] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang 및 Nan Duan. 시각적 chatgpt: 시각적 기초 모델을 사용하여 말하고, 그리고, 편집합니다. arXiv 사전 인쇄 arXiv:2303.04671, 2023.[36] Zhiyang Xu, Ying Shen, Lifu Huang. Multiinstruct: 명령어 튜닝을 통해 다중 모드 제로샷 학습을 개선합니다. arXiv 사전 인쇄 arXiv:2212.10773, 2022.[37] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 및 Lijuan Wang. Mm-react: 다중 모달 추론 및 작업을 위해 chatgpt에 메시지를 표시합니다. arXiv 사전 인쇄 arXiv:2303.11381, 2023.[38] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023. 1,[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델을 사용하여 비전 언어 이해를 향상합니다. arXiv 사전 인쇄본 arXiv:2304.10592, 2023. 1,[40] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi. 멀티모달 C4: 텍스트가 섞인 개방형 10억 규모 이미지 코퍼스. arXiv 사전 인쇄본 arXiv:2304.06939, 2023.[41] Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei. Visual7w: 이미지에서 근거 있는 질문 답변. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4995-5004페이지, 2016.[42] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan 등. 픽셀, 이미지, 언어에 대한 일반화된 디코딩. arXiv 사전 인쇄 arXiv:2212.11270, 2022.\\n')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['en'], dataset['train'][10]['ko']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [871, 28, 58286, 27447, 43491, 871, 28, 37998, 7098, 9758, 89, 42270, 622, 8, 18, 84, 26056, 4017, 25325, 15706, 64, 1051, 231, 60965, 28, 26085, 3876, 650, 20, 4395, 24181, 2261, 10, 122, 2616, 28, 40389, 91, 46, 12081, 18168, 14, 7973, 1161, 4, 64, 4538, 336, 51896, 102, 81, 634, 25218, 4829, 4, 159, 52927, 10, 215, 836, 24359, 556, 25218, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample_en = tokenizer(dataset['train'][10]['en'],\n",
    "                                max_length=max_token_length,\n",
    "                                padding=True, truncation=True)\n",
    "tokenized_sample_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [871, 28, 58286, 27447, 43491, 871, 28, 2257, 4331, 2389, 26, 42270, 622, 18, 19, 33945, 18884, 19136, 3381, 9, 86, 2148, 3069, 45, 1137, 303, 2764, 37, 14414, 231, 7022, 18164, 3069, 14975, 5794, 22404, 5536, 6286, 507, 3, 634, 25218, 4829, 22, 215, 17194, 10287, 556, 25218, 78, 46963, 556, 25218, 34, 824, 2859, 922, 2128, 247, 16390, 12877, 208, 1037, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample_ko = tokenizer(dataset['train'][10]['ko'],\n",
    "                                max_length=max_token_length,\n",
    "                                padding=True, truncation=True)\n",
    "tokenized_sample_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[871, 28, 58286, 27447, 43491, 871, 28, 7444, 28, 49226, 1898, 7098, 9758, 89, 42270, 622, 8, 18, 15180, 3284, 24759, 40385, 7169, 155, 5, 683, 3, 51294, 509, 90, 23438, 136, 1222, 34088, 24, 8, 4219, 10, 14862, 51017, 2219, 10, 62432, 16011, 2281, 24181, 4, 3221, 81, 2654, 12254, 4, 117, 2654, 3900, 91, 89, 10924, 336, 28, 36611, 91, 18, 1], [871, 28, 58286, 27447, 43491, 871, 28, 7, 1763, 6599, 113, 8878, 4693, 81, 10315, 28, 376, 12495, 52171, 10217, 5394, 3347, 45880, 31930, 22010, 250, 11587, 7430, 823, 5394, 4959, 1264, 11732, 2809, 19315, 8473, 376, 2400, 2789, 85, 13382, 694, 26978, 694, 283, 250, 3550, 1444, 14, 4166, 16746, 4, 27474, 4, 5015, 3871, 4713, 61137, 2125, 7, 2, 15215, 477, 1], [871, 28, 58286, 27447, 43491, 871, 28, 2125, 24181, 57994, 16, 5017, 3385, 14, 27614, 8, 13, 17563, 3, 1786, 12072, 2125, 9758, 84, 166, 4864, 40, 5434, 24181, 13, 7389, 4, 128, 1854, 6956, 32411, 2422, 3681, 20, 8553, 5, 740, 3900, 22006, 4, 22751, 3048, 4693, 30849, 4, 13, 30237, 44251, 35208, 8, 3, 51294, 40448, 20, 1898, 7098, 9758, 89, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][:3]['en'],\n",
    "          max_length=max_token_length,\n",
    "          padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>871</td>\n",
       "      <td>28</td>\n",
       "      <td>58286</td>\n",
       "      <td>27447</td>\n",
       "      <td>43491</td>\n",
       "      <td>871</td>\n",
       "      <td>28</td>\n",
       "      <td>37998</td>\n",
       "      <td>7098</td>\n",
       "      <td>9758</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>159</td>\n",
       "      <td>52927</td>\n",
       "      <td>10</td>\n",
       "      <td>215</td>\n",
       "      <td>836</td>\n",
       "      <td>24359</td>\n",
       "      <td>556</td>\n",
       "      <td>25218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁--</td>\n",
       "      <td>-</td>\n",
       "      <td>▁ABS</td>\n",
       "      <td>TR</td>\n",
       "      <td>ACT</td>\n",
       "      <td>▁--</td>\n",
       "      <td>-</td>\n",
       "      <td>▁Large</td>\n",
       "      <td>▁language</td>\n",
       "      <td>▁models</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>▁which</td>\n",
       "      <td>▁boosted</td>\n",
       "      <td>▁to</td>\n",
       "      <td>▁In</td>\n",
       "      <td>st</td>\n",
       "      <td>ret</td>\n",
       "      <td>G</td>\n",
       "      <td>PT</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1      2      3      4    5   6       7          8        9   \\\n",
       "ids     871  28  58286  27447  43491  871  28   37998       7098     9758   \n",
       "tokens  ▁--   -   ▁ABS     TR    ACT  ▁--   -  ▁Large  ▁language  ▁models   \n",
       "\n",
       "        ... 54      55        56   57   58   59     60   61     62    63  \n",
       "ids     ...  4     159     52927   10  215  836  24359  556  25218     1  \n",
       "tokens  ...  ,  ▁which  ▁boosted  ▁to  ▁In   st    ret    G     PT  </s>  \n",
       "\n",
       "[2 rows x 64 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenized_sample_en['input_ids'],\n",
    "        tokenizer.convert_ids_to_tokens(tokenized_sample_en['input_ids'])\n",
    "    ], index=('ids', 'tokens')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>871</td>\n",
       "      <td>28</td>\n",
       "      <td>58286</td>\n",
       "      <td>27447</td>\n",
       "      <td>43491</td>\n",
       "      <td>871</td>\n",
       "      <td>28</td>\n",
       "      <td>2257</td>\n",
       "      <td>4331</td>\n",
       "      <td>2389</td>\n",
       "      <td>...</td>\n",
       "      <td>824</td>\n",
       "      <td>2859</td>\n",
       "      <td>922</td>\n",
       "      <td>2128</td>\n",
       "      <td>247</td>\n",
       "      <td>16390</td>\n",
       "      <td>12877</td>\n",
       "      <td>208</td>\n",
       "      <td>1037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁--</td>\n",
       "      <td>-</td>\n",
       "      <td>▁ABS</td>\n",
       "      <td>TR</td>\n",
       "      <td>ACT</td>\n",
       "      <td>▁--</td>\n",
       "      <td>-</td>\n",
       "      <td>▁대규모</td>\n",
       "      <td>▁언어</td>\n",
       "      <td>▁모델</td>\n",
       "      <td>...</td>\n",
       "      <td>▁부</td>\n",
       "      <td>스트</td>\n",
       "      <td>되어</td>\n",
       "      <td>▁자연</td>\n",
       "      <td>어</td>\n",
       "      <td>▁명령을</td>\n",
       "      <td>▁효과적으로</td>\n",
       "      <td>▁따라</td>\n",
       "      <td>▁실제</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1      2      3      4    5   6     7     8     9   ...   54  \\\n",
       "ids     871  28  58286  27447  43491  871  28  2257  4331  2389  ...  824   \n",
       "tokens  ▁--   -   ▁ABS     TR    ACT  ▁--   -  ▁대규모   ▁언어   ▁모델  ...   ▁부   \n",
       "\n",
       "          55   56    57   58     59      60   61    62    63  \n",
       "ids     2859  922  2128  247  16390   12877  208  1037     1  \n",
       "tokens    스트   되어   ▁자연    어   ▁명령을  ▁효과적으로  ▁따라   ▁실제  </s>  \n",
       "\n",
       "[2 rows x 64 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenized_sample_ko['input_ids'],\n",
    "        tokenizer.convert_ids_to_tokens(tokenized_sample_ko['input_ids'])\n",
    "    ], index=('ids', 'tokens')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples):\n",
    "    en_texts = examples['en']\n",
    "    ko_texts = examples['ko']\n",
    "    \n",
    "    # 입력 데이터 형식 확인 (디버깅용)\n",
    "    assert isinstance(en_texts, list), \"en 텍스트는 리스트여야 합니다.\"\n",
    "    assert all(isinstance(text, str) for text in en_texts), \"모든 en 텍스트는 문자열이어야 합니다.\"\n",
    "    \n",
    "    # 토크나이저 호출\n",
    "    model_inputs = tokenizer(\n",
    "        en_texts,\n",
    "        ko_texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64  # 예시로 최대 길이 설정\n",
    "    )\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CPU = multiprocessing.cpu_count()\n",
    "NUM_CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdc123d4e5b43209c0bb49f818bf8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cc764d57504d3191bd2501ca2c9c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ba5356f63d48a988b5eb32612170f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75880fde268430ba8f680cd636144ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a432ba51963406893ff17d8d3217643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeccb8bc3c0740b2919b7d48e6072733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '안녕하세요 세계!'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model=\"facebook/m2m100_1.2B\"\n",
    "tokenizer=\"facebook/m2m100_1.2B\"\n",
    "translator=pipeline('translation_en_to_ko', model)\n",
    "translator(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'large', 'language', 'models', 'llms', 'capture', 'procedural', 'knowledge', 'world', 'recent', 'work', 'leveraged', 'llm', 'ability', 'generate', 'abstract', 'plans', 'simplify', 'challenging', 'control', 'tasks', 'either', 'action', 'scoring', 'action', 'modeling', 'however', 'transformer', 'architecture', 'inherits', 'several', 'constraints', 'make', 'difficult', 'llm', 'directly', 'serve', 'agent', 'limited', 'inut', 'lengths', 'inefficiency', 'bias', 'incompatibility', 'nonext', 'environments', 'maintain', 'compatibility', 'trainable', 'actor', 'propose', 'instead', 'use', 'knowledge', 'llms', 'simplify', 'control', 'problem', 'rather', 'solving', 'propose', 'plan', 'eliminate', 'track', 'pet', 'framework', 'plan', 'module', 'transates', 'task', 'description', 'list', 'eliminate', 'module', 'masks', 'irrelevant', 'objects', 'receptacles', 'observation', 'current', 'finally', 'track', 'module', 'determines', 'whether', 'agent', 'accomplished', 'alf', 'world', 'instruction', 'following', 'benchmark', 'pet', 'framework', 'leads', 'significant', 'improvement', 'sota', 'generalization', 'human']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "with open('../section_txt/2305.02412v2.pdf_section.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " 'llm',\n",
       " 'capture',\n",
       " 'procedural',\n",
       " 'knowledge',\n",
       " 'world',\n",
       " 'recent',\n",
       " 'work',\n",
       " 'leveraged',\n",
       " 'llm',\n",
       " 'ability',\n",
       " 'generate',\n",
       " 'abstract',\n",
       " 'plan',\n",
       " 'simplify',\n",
       " 'challenging',\n",
       " 'control',\n",
       " 'task',\n",
       " 'either',\n",
       " 'action',\n",
       " 'scoring',\n",
       " 'action',\n",
       " 'modeling',\n",
       " 'however',\n",
       " 'transformer',\n",
       " 'architecture',\n",
       " 'inherits',\n",
       " 'several',\n",
       " 'constraint',\n",
       " 'make',\n",
       " 'difficult',\n",
       " 'llm',\n",
       " 'directly',\n",
       " 'serve',\n",
       " 'agent',\n",
       " 'limited',\n",
       " 'inut',\n",
       " 'length',\n",
       " 'inefficiency',\n",
       " 'bias',\n",
       " 'incompatibility',\n",
       " 'nonext',\n",
       " 'environment',\n",
       " 'maintain',\n",
       " 'compatibility',\n",
       " 'trainable',\n",
       " 'actor',\n",
       " 'propose',\n",
       " 'instead',\n",
       " 'use',\n",
       " 'knowledge',\n",
       " 'llm',\n",
       " 'simplify',\n",
       " 'control',\n",
       " 'problem',\n",
       " 'rather',\n",
       " 'solving',\n",
       " 'propose',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'pet',\n",
       " 'framework',\n",
       " 'plan',\n",
       " 'module',\n",
       " 'transates',\n",
       " 'task',\n",
       " 'description',\n",
       " 'list',\n",
       " 'eliminate',\n",
       " 'module',\n",
       " 'mask',\n",
       " 'irrelevant',\n",
       " 'object',\n",
       " 'receptacle',\n",
       " 'observation',\n",
       " 'current',\n",
       " 'finally',\n",
       " 'track',\n",
       " 'module',\n",
       " 'determines',\n",
       " 'whether',\n",
       " 'agent',\n",
       " 'accomplished',\n",
       " 'alf',\n",
       " 'world',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'benchmark',\n",
       " 'pet',\n",
       " 'framework',\n",
       " 'lead',\n",
       " 'significant',\n",
       " 'improvement',\n",
       " 'sota',\n",
       " 'generalization',\n",
       " 'human',\n",
       " 'goal',\n",
       " 'specification',\n",
       " 'introduction',\n",
       " 'human',\n",
       " 'abstractly',\n",
       " 'plan',\n",
       " 'everyday',\n",
       " 'task',\n",
       " 'without',\n",
       " 'execution',\n",
       " 'example',\n",
       " 'given',\n",
       " 'task',\n",
       " 'make',\n",
       " 'breakfast',\n",
       " 'roughly',\n",
       " 'plan',\n",
       " 'first',\n",
       " 'pick',\n",
       " 'mug',\n",
       " 'make',\n",
       " 'coffee',\n",
       " 'grabbing',\n",
       " 'egg',\n",
       " 'scramble',\n",
       " 'embodied',\n",
       " 'agent',\n",
       " 'endowed',\n",
       " 'capability',\n",
       " 'generalize',\n",
       " 'effectively',\n",
       " 'leveraging',\n",
       " 'reasoning',\n",
       " 'carnegie',\n",
       " 'mellon',\n",
       " 'university',\n",
       " 'ariel',\n",
       " 'university',\n",
       " 'microsoft',\n",
       " 'research',\n",
       " 'nvidia',\n",
       " 'research',\n",
       " 'correspondence',\n",
       " 'yue',\n",
       " 'wu',\n",
       " 'heat',\n",
       " 'apple',\n",
       " 'put',\n",
       " 'fridge',\n",
       " 'eliminate',\n",
       " 'take',\n",
       " 'apple',\n",
       " 'see',\n",
       " 'apple',\n",
       " 'action',\n",
       " 'pickup',\n",
       " 'apple',\n",
       " 'update',\n",
       " 'progress',\n",
       " 'finished',\n",
       " 'taking',\n",
       " 'apple',\n",
       " 'j',\n",
       " 'figure',\n",
       " 'pet',\n",
       " 'framework',\n",
       " 'plan',\n",
       " 'module',\n",
       " 'us',\n",
       " 'llm',\n",
       " 'generate',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'module',\n",
       " 'us',\n",
       " 'qa',\n",
       " 'model',\n",
       " 'mask',\n",
       " 'irrelevant',\n",
       " 'object',\n",
       " 'observation',\n",
       " 'track',\n",
       " 'module',\n",
       " 'us',\n",
       " 'qa',\n",
       " 'model',\n",
       " 'track',\n",
       " 'completion',\n",
       " 'recent',\n",
       " 'work',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'b',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'yao',\n",
       " 'et',\n",
       " 'used',\n",
       " 'llm',\n",
       " 'bommasani',\n",
       " 'et',\n",
       " 'abstract',\n",
       " 'planning',\n",
       " 'embodied',\n",
       " 'gaming',\n",
       " 'agent',\n",
       " 'shown',\n",
       " 'incipient',\n",
       " 'success',\n",
       " 'extracting',\n",
       " 'procedural',\n",
       " 'world',\n",
       " 'knowledge',\n",
       " 'llm',\n",
       " 'linguistic',\n",
       " 'orm',\n",
       " 'posthoc',\n",
       " 'alignment',\n",
       " 'executable',\n",
       " 'action',\n",
       " 'environment',\n",
       " 'however',\n",
       " 'treat',\n",
       " 'llm',\n",
       " 'acor',\n",
       " 'focus',\n",
       " 'adapting',\n",
       " 'llm',\n",
       " 'output',\n",
       " 'executable',\n",
       " 'action',\n",
       " 'either',\n",
       " 'micheli',\n",
       " 'fleuret',\n",
       " 'constraint',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'using',\n",
       " 'llm',\n",
       " 'actor',\n",
       " 'work',\n",
       " 'environment',\n",
       " 'imited',\n",
       " 'interaction',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'consisting',\n",
       " 'object',\n",
       " 'imits',\n",
       " 'generalization',\n",
       " 'modality',\n",
       " 'addition',\n",
       " 'scenario',\n",
       " 'considered',\n",
       " 'largely',\n",
       " 'simplified',\n",
       " 'real',\n",
       " 'world',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'al',\n",
       " 'provides',\n",
       " 'available',\n",
       " 'object',\n",
       " 'possible',\n",
       " 'interaction',\n",
       " 'start',\n",
       " 'imits',\n",
       " 'task',\n",
       " 'set',\n",
       " 'provided',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'limit',\n",
       " 'environment',\n",
       " 'object',\n",
       " 'single',\n",
       " 'table',\n",
       " 'hand',\n",
       " 'successfully',\n",
       " 'cut',\n",
       " 'lettuce',\n",
       " 'room',\n",
       " 'one',\n",
       " 'find',\n",
       " 'knife',\n",
       " 'since',\n",
       " 'multiple',\n",
       " 'drawer',\n",
       " 'cabinet',\n",
       " 'chaplot',\n",
       " 'et',\n",
       " 'min',\n",
       " 'et',\n",
       " 'blukis',\n",
       " 'et',\n",
       " 'realistic',\n",
       " 'scenario',\n",
       " 'lead',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'diverse',\n",
       " 'complicated',\n",
       " 'set',\n",
       " 'task',\n",
       " 'large',\n",
       " 'changing',\n",
       " 'action',\n",
       " 'space',\n",
       " 'furthermore',\n",
       " 'text',\n",
       " 'description',\n",
       " 'observation',\n",
       " 'increase',\n",
       " 'function',\n",
       " 'number',\n",
       " 'receptacle',\n",
       " 'object',\n",
       " 'agent',\n",
       " 'see',\n",
       " 'combined',\n",
       " 'growing',\n",
       " 'state',\n",
       " 'becomes',\n",
       " 'verbose',\n",
       " 'fi',\n",
       " 'llm',\n",
       " 'work',\n",
       " 'explore',\n",
       " 'alternative',\n",
       " 'mechanism',\n",
       " 'leverage',\n",
       " 'prior',\n",
       " 'knowledge',\n",
       " 'encoded',\n",
       " 'llm',\n",
       " 'withow',\n",
       " 'impacting',\n",
       " 'trainable',\n",
       " 'nature',\n",
       " 'actor',\n",
       " 'propose',\n",
       " 'framework',\n",
       " 'figure',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'pet',\n",
       " 'plan',\n",
       " 'module',\n",
       " 'simplifies',\n",
       " 'complex',\n",
       " 'task',\n",
       " 'breaking',\n",
       " 'us',\n",
       " 'pretrained',\n",
       " 'llm',\n",
       " 'generate',\n",
       " 'list',\n",
       " 'inpu',\n",
       " 'task',\n",
       " 'description',\n",
       " 'employing',\n",
       " 'example',\n",
       " 'prompt',\n",
       " 'training',\n",
       " 'set',\n",
       " 'similar',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'al',\n",
       " 'eliminate',\n",
       " 'module',\n",
       " 'address',\n",
       " 'challenge',\n",
       " 'long',\n",
       " 'observation',\n",
       " 'us',\n",
       " 'qa',\n",
       " 'language',\n",
       " 'model',\n",
       " 'score',\n",
       " 'mask',\n",
       " 'object',\n",
       " 'receptacle',\n",
       " 'irrelevant',\n",
       " 'current',\n",
       " 'track',\n",
       " 'module',\n",
       " 'us',\n",
       " 'qa',\n",
       " 'language',\n",
       " 'model',\n",
       " 'determine',\n",
       " 'current',\n",
       " 'complete',\n",
       " 'move',\n",
       " 'next',\n",
       " 'finally',\n",
       " 'action',\n",
       " 'attention',\n",
       " 'agen',\n",
       " 'us',\n",
       " 'architecture',\n",
       " 'accommodate',\n",
       " 'long',\n",
       " 'variable',\n",
       " 'length',\n",
       " 'action',\n",
       " 'space',\n",
       " 'agent',\n",
       " 'observes',\n",
       " 'masked',\n",
       " 'observation',\n",
       " 'take',\n",
       " 'action',\n",
       " 'conditioned',\n",
       " 'current',\n",
       " 'focus',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'indoor',\n",
       " 'household',\n",
       " 'alfworld',\n",
       " 'shridhar',\n",
       " 'et',\n",
       " 'interactive',\n",
       " 'text',\n",
       " 'environment',\n",
       " 'benchmark',\n",
       " 'experiment',\n",
       " 'analysis',\n",
       " 'demonstrate',\n",
       " 'llm',\n",
       " 'remove',\n",
       " 'taskirrelevant',\n",
       " 'object',\n",
       " 'observation',\n",
       " 'qa',\n",
       " 'also',\n",
       " 'generate',\n",
       " 'accuracy',\n",
       " 'addition',\n",
       " 'multiple',\n",
       " 'llm',\n",
       " 'may',\n",
       " 'used',\n",
       " 'coordination',\n",
       " 'assist',\n",
       " 'agent',\n",
       " 'different',\n",
       " 'aspect',\n",
       " 'contribution',\n",
       " 'follows',\n",
       " 'pet',\n",
       " 'novel',\n",
       " 'framework',\n",
       " 'leveraging',\n",
       " 'pretrained',\n",
       " 'llm',\n",
       " 'embodied',\n",
       " 'agent',\n",
       " 'work',\n",
       " 'show',\n",
       " 'p',\n",
       " 'e',\n",
       " 'serf',\n",
       " 'complementary',\n",
       " 'role',\n",
       " 'simultaneously',\n",
       " 'addressed',\n",
       " 'tackle',\n",
       " 'control',\n",
       " 'task',\n",
       " 'action',\n",
       " 'attention',\n",
       " 'agent',\n",
       " 'handle',\n",
       " 'changing',\n",
       " 'action',\n",
       " 'space',\n",
       " 'text',\n",
       " 'environment',\n",
       " 'improvement',\n",
       " 'sota',\n",
       " 'generalization',\n",
       " 'human',\n",
       " 'goal',\n",
       " 'via',\n",
       " 'planning',\n",
       " 'tracking',\n",
       " 'related',\n",
       " 'work',\n",
       " 'language',\n",
       " 'conditioned',\n",
       " 'policy',\n",
       " 'considerable',\n",
       " 'portion',\n",
       " 'prior',\n",
       " 'work',\n",
       " 'study',\n",
       " 'imitation',\n",
       " 'learning',\n",
       " 'tellex',\n",
       " 'et',\n",
       " 'mei',\n",
       " 'et',\n",
       " 'nair',\n",
       " 'et',\n",
       " 'stepputtis',\n",
       " 'et',\n",
       " 'jang',\n",
       " 'et',\n",
       " 'shridhar',\n",
       " 'et',\n",
       " 'sharma',\n",
       " 'et',\n",
       " 'reinforcement',\n",
       " 'learning',\n",
       " 'misra',\n",
       " 'et',\n",
       " 'jiang',\n",
       " 'et',\n",
       " 'cideron',\n",
       " 'et',\n",
       " 'goyal',\n",
       " 'et',\n",
       " 'nair',\n",
       " 'et',\n",
       " 'akakzia',\n",
       " 'et',\n",
       " 'policy',\n",
       " 'conditioned',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'instruction',\n",
       " 'goal',\n",
       " 'macmahon',\n",
       " 'et',\n",
       " 'kollar',\n",
       " 'et',\n",
       " 'prior',\n",
       " 'research',\n",
       " 'used',\n",
       " 'language',\n",
       " 'embeddings',\n",
       " 'improve',\n",
       " 'generalization',\n",
       " 'new',\n",
       " 'instruction',\n",
       " 'nair',\n",
       " 'et',\n",
       " 'lack',\n",
       " 'domain',\n",
       " 'knowledge',\n",
       " 'captured',\n",
       " 'llm',\n",
       " 'pet',\n",
       " 'framework',\n",
       " 'enables',\n",
       " 'planning',\n",
       " 'progress',\n",
       " 'tracking',\n",
       " 'observation',\n",
       " 'filtering',\n",
       " 'use',\n",
       " 'llm',\n",
       " 'designed',\n",
       " 'compatible',\n",
       " 'language',\n",
       " 'conditional',\n",
       " 'policy',\n",
       " 'llm',\n",
       " 'control',\n",
       " 'llm',\n",
       " 'recently',\n",
       " 'achieved',\n",
       " 'success',\n",
       " 'planning',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'al',\n",
       " 'show',\n",
       " 'llm',\n",
       " 'generate',\n",
       " 'plausible',\n",
       " 'plan',\n",
       " 'task',\n",
       " 'generated',\n",
       " 'directly',\n",
       " 'executed',\n",
       " 'control',\n",
       " 'environment',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'al',\n",
       " 'solves',\n",
       " 'executability',\n",
       " 'issue',\n",
       " 'training',\n",
       " 'action',\n",
       " 'scoring',\n",
       " 'model',\n",
       " 'llm',\n",
       " 'action',\n",
       " 'choice',\n",
       " 'demonstrates',\n",
       " 'success',\n",
       " 'robot',\n",
       " 'however',\n",
       " 'llm',\n",
       " 'score',\n",
       " 'work',\n",
       " 'simple',\n",
       " 'environment',\n",
       " 'action',\n",
       " 'limited',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'fails',\n",
       " 'environment',\n",
       " 'object',\n",
       " 'diverse',\n",
       " 'action',\n",
       " 'shridhar',\n",
       " 'et',\n",
       " 'song',\n",
       " 'et',\n",
       " 'al',\n",
       " 'us',\n",
       " 'generate',\n",
       " 'lowlevel',\n",
       " 'command',\n",
       " 'executed',\n",
       " 'respective',\n",
       " 'control',\n",
       " 'policy',\n",
       " 'work',\n",
       " 'improves',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'al',\n",
       " 'action',\n",
       " 'diversity',\n",
       " 'addition',\n",
       " 'llm',\n",
       " 'require',\n",
       " 'demonstration',\n",
       " 'example',\n",
       " 'making',\n",
       " 'length',\n",
       " 'prompt',\n",
       " 'infeasible',\n",
       " 'alfworld',\n",
       " 'micheli',\n",
       " 'fleuret',\n",
       " 'model',\n",
       " 'expert',\n",
       " 'trajectory',\n",
       " 'alfworld',\n",
       " 'demonstrated',\n",
       " 'impressive',\n",
       " 'evaluation',\n",
       " 'result',\n",
       " 'however',\n",
       " 'lm',\n",
       " 'requires',\n",
       " 'fully',\n",
       " 'environment',\n",
       " 'consistent',\n",
       " 'expert',\n",
       " 'trajectory',\n",
       " 'fully',\n",
       " 'action',\n",
       " 'space',\n",
       " 'requirement',\n",
       " 'greatly',\n",
       " 'limit',\n",
       " 'generalization',\n",
       " 'domain',\n",
       " 'even',\n",
       " 'form',\n",
       " 'task',\n",
       " 'specification',\n",
       " 'show',\n",
       " 'pet',\n",
       " 'framework',\n",
       " 'achieves',\n",
       " 'better',\n",
       " 'generalization',\n",
       " 'human',\n",
       " 'goal',\n",
       " 'specification',\n",
       " 'agent',\n",
       " 'trained',\n",
       " 'hierarchical',\n",
       " 'planning',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'due',\n",
       " 'structured',\n",
       " 'nature',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'andreas',\n",
       " 'et',\n",
       " 'al',\n",
       " 'explored',\n",
       " 'associating',\n",
       " 'task',\n",
       " 'description',\n",
       " 'modular',\n",
       " 'later',\n",
       " 'work',\n",
       " 'extend',\n",
       " 'approach',\n",
       " 'using',\n",
       " 'single',\n",
       " 'conditional',\n",
       " 'policy',\n",
       " 'mei',\n",
       " 'et',\n",
       " 'matching',\n",
       " 'template',\n",
       " 'oh',\n",
       " 'et',\n",
       " 'recent',\n",
       " 'work',\n",
       " 'shown',\n",
       " 'llm',\n",
       " 'proficient',\n",
       " 'planner',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'lin',\n",
       " 'et',\n",
       " 'therefore',\n",
       " 'motivates',\n",
       " 'u',\n",
       " 'revisit',\n",
       " 'idea',\n",
       " 'hierarchical',\n",
       " 'task',\n",
       " 'plan',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'ning',\n",
       " 'progress',\n",
       " 'tracking',\n",
       " 'knowledge',\n",
       " 'pet',\n",
       " 'first',\n",
       " 'work',\n",
       " 'combining',\n",
       " 'llm',\n",
       " 'planner',\n",
       " 'llm',\n",
       " 'progress',\n",
       " 'tracker',\n",
       " 'conditional',\n",
       " 'policy',\n",
       " 'text',\n",
       " 'game',\n",
       " 'game',\n",
       " 'complex',\n",
       " 'interactive',\n",
       " 'simulation',\n",
       " 'game',\n",
       " 'state',\n",
       " 'action',\n",
       " 'space',\n",
       " 'natural',\n",
       " 'lanugage',\n",
       " 'fertile',\n",
       " 'ground',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'research',\n",
       " 'addition',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'successful',\n",
       " 'play',\n",
       " 'requires',\n",
       " 'skill',\n",
       " 'like',\n",
       " 'memory',\n",
       " 'planning',\n",
       " 'exploration',\n",
       " 'trial',\n",
       " 'error',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'alfworld',\n",
       " 'shridhar',\n",
       " 'et',\n",
       " 'simulator',\n",
       " 'extends',\n",
       " 'common',\n",
       " 'game',\n",
       " 'simulator',\n",
       " 'textworld',\n",
       " 'coté',\n",
       " 'et',\n",
       " 'al',\n",
       " 'create',\n",
       " 'analog',\n",
       " 'alfred',\n",
       " 'scene',\n",
       " 'agent',\n",
       " 'large',\n",
       " 'action',\n",
       " 'space',\n",
       " 'et',\n",
       " 'al',\n",
       " 'learns',\n",
       " 'representation',\n",
       " 'state',\n",
       " 'action',\n",
       " 'two',\n",
       " 'different',\n",
       " 'model',\n",
       " 'computes',\n",
       " 'q',\n",
       " 'function',\n",
       " 'inner',\n",
       " 'product',\n",
       " 'representation',\n",
       " 'could',\n",
       " 'generalize',\n",
       " 'large',\n",
       " 'action',\n",
       " 'space',\n",
       " 'considered',\n",
       " 'small',\n",
       " 'number',\n",
       " 'action',\n",
       " 'fulda',\n",
       " 'et',\n",
       " 'al',\n",
       " 'ahn',\n",
       " 'et',\n",
       " 'al',\n",
       " 'explore',\n",
       " 'action',\n",
       " 'elimination',\n",
       " 'setting',\n",
       " 'affordances',\n",
       " 'zahavy',\n",
       " 'et',\n",
       " 'al',\n",
       " 'train',\n",
       " 'model',\n",
       " 'eliminate',\n",
       " 'invalid',\n",
       " 'action',\n",
       " 'zork',\n",
       " 'external',\n",
       " 'environment',\n",
       " 'signal',\n",
       " 'however',\n",
       " 'functionality',\n",
       " 'depends',\n",
       " 'existence',\n",
       " 'external',\n",
       " 'elimination',\n",
       " 'signal',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'section',\n",
       " 'explain',\n",
       " 'framework',\n",
       " 'plan',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'pet',\n",
       " 'plan',\n",
       " 'module',\n",
       " 'mp',\n",
       " 'llm',\n",
       " 'generates',\n",
       " 'list',\n",
       " 'input',\n",
       " 'task',\n",
       " 'description',\n",
       " 'using',\n",
       " 'sample',\n",
       " 'training',\n",
       " 'set',\n",
       " 'example',\n",
       " 'eliminate',\n",
       " 'module',\n",
       " 'mg',\n",
       " 'us',\n",
       " 'qa',\n",
       " 'language',\n",
       " 'model',\n",
       " 'score',\n",
       " 'mask',\n",
       " 'object',\n",
       " 'receptacle',\n",
       " 'irrelevant',\n",
       " 'current',\n",
       " 'track',\n",
       " 'module',\n",
       " 'mr',\n",
       " 'us',\n",
       " 'zeroshot',\n",
       " 'qa',\n",
       " 'language',\n",
       " 'model',\n",
       " 'determine',\n",
       " 'current',\n",
       " 'complete',\n",
       " 'move',\n",
       " 'next',\n",
       " 'note',\n",
       " 'plan',\n",
       " 'generative',\n",
       " 'task',\n",
       " 'eliminate',\n",
       " 'track',\n",
       " 'classification',\n",
       " 'task',\n",
       " 'also',\n",
       " 'implement',\n",
       " 'agent',\n",
       " 'action',\n",
       " 'attention',\n",
       " 'score',\n",
       " 'permissible',\n",
       " 'action',\n",
       " 'trained',\n",
       " 'imitation',\n",
       " 'learning',\n",
       " 'expert',\n",
       " 'agen',\n",
       " 'observes',\n",
       " 'masked',\n",
       " 'observation',\n",
       " 'take',\n",
       " 'action',\n",
       " 'conditioned',\n",
       " 'current',\n",
       " 'problem',\n",
       " 'setting',\n",
       " 'define',\n",
       " 'task',\n",
       " 'description',\n",
       " 'observation',\n",
       " 'string',\n",
       " 'time',\n",
       " 'step',\n",
       " 'list',\n",
       " 'permissible',\n",
       " 'action',\n",
       " 'executed',\n",
       " 'observation',\n",
       " 'string',\n",
       " 'define',\n",
       " 'example',\n",
       " 'training',\n",
       " 'set',\n",
       " 'planning',\n",
       " 'module',\n",
       " 'target',\n",
       " 'output',\n",
       " 'figure',\n",
       " 'plan',\n",
       " 'module',\n",
       " 'generation',\n",
       " 'full',\n",
       " 'example',\n",
       " 'chosen',\n",
       " 'training',\n",
       " 'set',\n",
       " 'based',\n",
       " 'roberta',\n",
       " 'embedding',\n",
       " 'similarity',\n",
       " 'task',\n",
       " 'query',\n",
       " 'description',\n",
       " 'example',\n",
       " 'concatenated',\n",
       " 'task',\n",
       " 'query',\n",
       " 'get',\n",
       " 'prompt',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../lemmatized_tokens/2305.03048v2_lemmatized_tokens.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([\" \".join(lemmatized_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF words:\n",
      "et: 0.3405179332695509\n",
      "action: 0.29910359003406495\n",
      "task: 0.28299801210915376\n",
      "plan: 0.20937251302384546\n",
      "model: 0.20937251302384546\n",
      "module: 0.18636454455968662\n",
      "track: 0.16565737294194366\n",
      "goal: 0.16335657609552778\n",
      "llm: 0.158754982402696\n",
      "eliminate: 0.15645418555628013\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "sorted_items = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
    "print(\"Top TF-IDF words:\")\n",
    "for idx in sorted_items[:10]:\n",
    "    print(f\"{feature_names[idx]}: {tfidf_matrix[0, idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d147fcb7d74836bca0b31d84ff37b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c65c2770fbc4437a6007e13ee3fdcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d786e1119644ed9d36c1a7746ea917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064c23b14e0844fe82c9885cc7fd30aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb94dfc77fbc4d14a8c33771ae91cb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac40353a6874d6ba6bbbbb7e64b896f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint='t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model=T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ABSTRACT ---\n",
      "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM’s ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, he transformer architecture inherits several constraints that make it difficult for the LLM o directly serve as the agent: e.g. limited inut lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nonext environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify he control problem, rather than solving it. We propose the Plan, Eliminate, and Track PET) framework. The Plan module transates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the Alf World instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications. 1.\n",
      "--- INTRODUCTION ---\n",
      "Humans can abstractly plan their everyday tasks without execution; for example, given the task “Make breakfast”, we can roughly plan to first pick up a mug and make coffee, before grabbing eggs to scramble. Embodied agents, endowed with this capability will generalize more effectively by leveraging common-sense reasoning. ‘Carnegie Mellon University ?Ariel University *Microsoft Research “Nvidia Research. Correspondence to: Yue Wu <ywu5@andrew.cmu.edu>. ( Heat some apple and put it in the fridge } Eliminate Take an apple You see apple, Paug-knife — Action: Pickup Apple Update Progress Finished taking | an apple? J Figure 1. PET framework. Plan module uses LLM to generate a high-level plan. Eliminate Module uses a QA model to mask irrelevant objects in observation. Track module uses a QA model to track the completion of sub-tasks. Recent work (Huang et al., 2022a;b; Ahn et al., 2022; Yao et al., 2020) has used LLMs (Bommasani et al., 2021) for abstract planning for embodied or gaming agents. These have shown incipient success in extracting procedural world knowledge from LLMs in linguistic orm with posthoc alignment to executable actions in he environment. However, they treat LLMs as the acor, and focus on adapting LLM outputs to executable actions either through fine-tuning (Micheli & Fleuret, 2021) or constraints (Ahn et al., 2022). Using LLM as the actor works for pure-text environments with imited interactions (Huang et al., 2022b; Ahn et al., 2022) (just consisting of “picking/placing” objects), but imits generalization to other modalities. In addition, he scenarios considered have been largely simplified from the real world. Ahn et al. (2022) provides all available objects and possible interactions at the start and imits tasks to the set of provided objects/interactions. Huang et al. (2022b) limits the environment to objects on a single table. On the other hand, to successfully “cut some lettuce” in a real-world room, one has to “find a knife”, which can be non-trivial since there can be multiple drawers or cabinets (Chaplot et al., 2020; Min et al., 2021; Blukis et al., 2021). A more realistic scenario leads to a --- --Plan, Eliminate, and Track diverse, complicated set of tasks or large and changing action space. Furthermore, the text description of the observation increases as a function of the number o receptacles and objects the agent sees. Combined with growing roll-outs, the state becomes too verbose to fi into any LLM. In this work, we explore alternative mechanisms to leverage the prior knowledge encoded in LLMs withow impacting the trainable nature of the actor. We propose a 3-step framework (Figure 1): Plan, Eliminate, an Track (PET). Plan module simplifies complex tasks by breaking them down into sub-tasks. It uses a pretrained LLM to generate a list of sub-tasks for an inpu task description employing example prompts from the training set similar to Huang et al. (2022a); Ahn et al. (2022). The Eliminate module addresses the challenge of long observations. It uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module uses a zero-shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Finally, the Action Attention agen uses a transformer-based architecture to accommodate for long roll-out and variable length action space. The agent observes the masked observation and takes an action conditioned on the current sub-task. We focus on instruction following in indoor households on the AlfWorld (Shridhar et al., 2020b) interactive text environment benchmark. Our experiments and analysis demonstrate that LLMs not only remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. In addition, multiple LLMs may be used in coordination with each other to assist the agent from different aspects. Our contributions are as follows: 1. PET: A novel framework for leveraging pretrained LLMs with embodied agents; our work shows that each of P, E, T serves a complementary role and should be simultaneously addressed to tackle control tasks. 2. An Action Attention agent that handles the changing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2.\n",
      "--- RELATED WORK ---\n",
      "Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepputtis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Sharma et al., 2021) or reinforcement learning (Misra. et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; Akakzia et al., 2020) policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010). While some prior research has used pre-trained language embeddings to improve generalization to new instructions (Nair et al., 2022), they lack domain knowledge that is captured in LLMs. Our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs, and is designed to be compatible with any language conditional policies above. LLMs for Control LLMs have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step lowlevel commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-plan. In addition, all the above LLMs require few-shot demonstrations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) fine-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM fine-tuning requires a fully text-based environment, consistent expert trajectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task specification. We show that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. Hierarchical Planning with Natural Language Due to the structured nature of natural language, Andreas et al. (2017) explored associating each task description to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to templates (Oh et al., 2017). Recent works have shown that LLMs are proficient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan --- --Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the first work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games _ Text-based games are complex, interactive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addition to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld Coté et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two different models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of affordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elimination signal. 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (Mp), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (Mg) uses a zero-shot QA language model to score an mask objects and receptacles that are irrelevant to the current sub-task. The Track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate an Track are classification tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agen observes the masked observation and takes an action conditioned on the current sub-task. Problem Setting We define the task description as T, the observation string at time step t as O', and the list of permissible actions {a!|a can be executed} as A’. For each observation string O', we define the Examples from Training set Planning Module ( Target Output Figure 2. Plan Module (Sub-task Generation). 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rf and o; respectively. The classification between receptacles and objects is defined by the environment (Shridhar et al., 2020b). For a task J, we assume there exists a list of sub-tasks Sy = {s1,...s8,} that solves T. 3.1. Plan Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (Mp) to generate a list of high-level sub-tasks for a task description T. Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module Mp. For a given task description J, we compose the query question Q7 as “What are the middle steps required to JT?” , and require Mp to generate a list sub-tasks Sy = {51,... sx}. Specifically, we select the top 5 example tasks 7” from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task 7. We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt Pz for Mp (Fig. 2): Pr= concat(Qre, Srp, Ore, Spe, Or) An illustration of our prompt format is shown in Figure 2, where T =“heat some apple and put it in fridge”, and Q;» =“What are the middle steps required to put two spraybottles on toilet”, Srp =“take a spraybottle, --- --Plan, Eliminate, and Track place the spraybottle in/on toilet, take a spraybottle, lace the spraybottle in/on toilet”. The expected list of sub-tasks to achieve this task T is s; =‘take an apple’, 82 =‘heat the apple’, and s3 =‘place the apple in/on ridge’ (You are in the middle of a room. Looking quickly around you, you see a a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a microwave 1. Your task is to heat some apple and put it in the fridge. Where should you go? You are in the middle of a room. Looking quickly around you, you see a a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. Figure 3. Eliminate Module (Receptacle Masking). We use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. As we can see, the original observation is too long and the receptacles shown in red are not relevant for task completion. These receptacles are filtered by the QA model making the observation shorter. 3.2. Eliminate Typical Alfworld scenes can start with around 15 receptacles, each containing up to 15 objects. In some close-to-worst cases, there can be around 30 open-able receptacles (e.g. a kitchen with many cabinets and drawers), and it easily takes an agent with no prior knowledge more than 50 steps for the agent to find the desired object (repeating the process of visiting each receptacle, opening it, closing it). We observe that many receptacles and objects are irrelevant to specific asks during both training and evaluation, and can e easily filtered with common-sense knowledge about he tasks. For example, in Fig. 3 the task is to heat some apple. By removing the irrelevant receptacles like he coffeemachine, garbagecan, or objects like knife, we could significantly shorten our observation. We herefore propose to leverage commonsense knowledge captured by large pre-trained QA models to design our Eliminate module Mg to mask out irrelevant receptacles and objects. For task 7, we create prompts in the format P, =“Your ask is to: T. Where should you go to?” for receptacles and P, =“Your task is to: T. Which objects will be relevant?” for objects. Using the pre-trained QA model Meg in a zero-shot manner, we compute score flo, = Mag(P.,0:) for each object 0; and pr, = Mg(Po, ri) for each receptacle r; in observation at every step.represents the belief score of whether the common-sense QA model believes the object /receptacle is relevant to T. We then remove 0; from observation if fio, < To, and remove 1; if jp, < 7,. Threshold 7,,7, are hyperparameters. Environment You are in the middle of a room. Looking quickly around you, you see ..., a garbagecan 1, a sinkbasin 1, and a toaster 1. > go to sinkbasinOn the sinkbasin 1, you see nothing. > go to diningtableOn the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. You take apple 1 from diningtable 1. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, ..... You take apple 1 from diningtable 1. Did you finish the task of take an apple ? Tracking Module ( Update progress tracker Figure 4. Track Module (Progress Tracking). At every step, we take the last 3 steps of roll-out as context and append a query (about whether the current sub-task is completed) to get the prompt. A pre-trained QA model generates a Yes/No answer to the prompt. For the answer “Yes”, we update the tracker to the next sub-task. 3.3. Track For the agent to utilize the high-level plan, it first needs to know which sub-task to execute. A human actor typically starts from the first item and check-off the tasks one by one until completion. Therefore, similar to Section 3.2, we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection.! Specifically, as illustrated in Figure 4, for sub-task list Sr = {s1,... 8x}, we keep track of a progress tracker p (initialized at 1) that indicates the sub-task the agent is currently working on (sp). We then compose the context as the last d steps of the agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks, so the agent has no means to recover if it undoes its previous sub-task at test time. --- --Plan, Eliminate, and Track for the current sub-task and the question as “Did you finish the task of s,?”. For efficiency, we set d := min(d + 1,3) at each step. Note that d is reset to whenever the progress tracker updates. Hence, the emplate P, = concat(O'4,..., 01, “Did you finish he task of s,?”). We feed P, to a pre-trained zeroshot QA model M+ and compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No\"[Pa)- IE pate “Yes” [Pa) > pate (*No\" [Pa) hen we increment the tracker p to track the next subask. f the tracking ends prematurely, meaning that p > len(S7) but the environment has not returned “done”, we fall back to conditioning with 7. We study the rate of pre-mature ends in Section 4.4 in terms of precision and recall. 3.4. Agent Since the number of permissible actions can vary a lot by the environment, the agent needs to handle arbitrary dimensions of action space. While Shridhar et al. (2020b) addresses this challenge by generating actions token-by-token, such a generation process leads o degenerate performance even on the training set. We draw inspiration from the field of text summarizaion, where models are built to handle variable input lengths. See et al. (2017) generates a summary through an attention-like “pointing” mechanism that extracts he output word by word. Similarly, an attention-like “pointing” model could be used to select an action from he list of permissible actions. Action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. We eschew the long rollout/ large action space problems by (1) representing observations y averaging over history, and (2) individually encoding actions (Fig 5). In our proposed action attention ramework, we first represent historical observations H' as the average of embeddings of all individual observations through history (Eq. 1), and H@ as the list of embeddings of all the current permissible actions (Eq. 2). Then, in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H\"), the current observation embedding (O*), and the list of action embeddings (H“). In Eq. 4 we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H\"), the current observation embedding (O'), and embedding of action (a;). Finally, we compute the dot-product of the query and keys as action scores for the policy 7 (Eq. 5). H'= avg jc[1,t-1]Embed(O’) ( H4 = [Embed(a‘), ..., Embed(a',)] ( Q = Mo (Embed(T), H', Embed(0'), H“) (K; = Mx (Embed(7), H‘, Embed(0'), Embed(a}) m@ = softmax ([Q - K;|é € all permissible actions]) 4. Experiments and Results We present our experiments as follows. First, we explain the environment setup and baselines for our experiments. Then we compare PET to the baselines on different splits of the environment. Finally, we conduct ablation studies and analyze the PET framework part by part. We show that PET generalizes better to human goal specification under efficient behavior cloning training. 4.1. Experimental Details AlfWorld Environment ALFWorld (Shridhar et al., 2020b) is a set of TextWorld environments (Coté et al., 2018b) that are parallels of the ALFRED embodied dataset (Shridhar et al., 2020a). ALFWorld includes 6 task types that each require solving multiple compositional sub-goals. There are 3553 training task instances ({tasktype, object, receptacle, room}),in-distribution evaluation task instances (seen split tasks themselves are novel but take place in rooms seen during training) and 134 out-of-distribution evaluation task instances (unseen split - tasks take place in novels rooms). An example of the task could be: “Rinse the egg to put it in the microwave.” Each training instance in AlfWorld comes with an expert, from which we collected our training demonstration. Human Goal Specification The crowd-sourced human goal specifications for evaluation contain 66 unseen verbs and 189 unseen nouns (Shridhar et al., 2020b). In comparison, the template goals use only 12 ways of goal specification. In addition, the sentence structure for human goal specification is more diverse compared to the template goals. Therefore, human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. Pre-trained LMs. For the Plan module (sub-task generation), we experimented with the open-source GPT-Neo-2.7B (Black et al., 2021), and an industryscale LLM with 530B parameters (Smith et al., 2022). --- --Plan, Eliminate, and Track 1oO (a) ot at al, ay You are in the middle You go to diningtable You go to microwave = of a room. Looking 1. In diningtable 1, 1. You open Goto Examine Heat quickly around, you you see a apple 1, a microwave 1. ... cabinet 1 cabinet 1 applesee a cabinet 5, a mug 3, a cup 3. You cabinet 4, ... take apple 1 from | diningtable 1. Heat the et x v T J Za J GSC K, Ks N _ Action Attenti Heat applei Figure 5. Agent (Action Attention). Action Attention block is a transformer-based framework that computes a key K; for each permissible action and output action scores as dot-product between key and query Q from the observations. Template Goal Specification Model seen unseen seen unseen BUTLER + DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 9 - GPT (Micheli & Fleuret, 2021) 91 95 42PET + Action Attention (Ours) 70 67.5 52.5Table 1. Comparison of different models in terms of completion rate per evaluation split (seen and unseen), with and without human annotated goals. PET under-performs GPT on Template goal specifications but generalizes better to For the Eliminate module (receptacle/object masking), we choose Macaw-11b (Tafjord & Clark, 2021), which is reported to have common sense QA performance on par with GPT3 (Brown et al., 2020) while eing orders of magnitudes smaller. We use a decision hreshold of 0.4 for Macaw score below which the objects are masked out. For the Track module (progress racking), we use the same Macaw-11b model as the Eliminate module answer to Yes/No questions. Actor Model Design. Our Action Attention agent (Mg and Mx) is a 12-layer transformer with 2 heads and hidden dimension 384. The last layer is then fed into two linear heads to generate K and Q. For embedding of actions and observations, we use re-trained RoBERTa-large (Liu et al., 2019) with emedding dimension 1024. For sub-task generation, we use ground-truth sub-tasks for training, and generated sub-tasks from Plan module for evaluation. Experimental Setup. Unlike the original benchmark (Shridhar et al., 2020b), we experiment with models trained with behavior cloning. Although Shridhar et al. (2020b) observe that models benefit greatly from DAgger training, DAgger assumes an expert that is well-defined at all possible states, which is inefficient and impractical. In our experiments, training is 100x slower with DAgger compared to behavior cloning (human goal specifications. * We include the performance of BUTLER with DAgger for completeness. All other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. weeks for DAgger v.s. 6 hours for Behavior Cloning). In addition, we demonstrate that our models surpass the DAgger training performance of the BUTLER (Shridhar et al., 2020b) agents trained with DAgger, even when our agent does not have the option to interact with the environment. Baselines. Our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. At each time step t, the encoder takes initial observation s°, current observation s‘, and task string Stas, and generates representation r'. The recurrent aggregator combines r’ with the last recurrent state h'~! to produce h’, which is then decoded into a string a’ representing action. In addition, the BUTLER agent uses beam search to get out of stuck conditions in the event of a failed action. Our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the AlfWorld training set. Specifically, the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. Human Goal Specification --- --Plan, Eliminate, and Track 4.2. Overall Results on Template and Human Goals We compare the performance of action attention assisted by PET with BUTLER (Shridhar et al., 2020b) and fine-tuned GPT (Micheli & Fleuret, 2021) in Table 1. For human goal specifications, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal specifications, GPT requires fine-tuning on fully textbased expert trajectory and thus loses adaptability to different environment settings. Qualitatively, on human goal specification tasks, where the goal specifications are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal specifications as shown in Section 4.5. Quantitatively, GPT suffers from a relative 50% performance drop transferring from template to human-goal specifications, whereas PET incurs only a 15 ~ 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per‘orms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. (Section 4.1) 4.3. Ablations for Plan, Eliminate, and Track n Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories samled from the training set. The data set size is chosen © match the size of the seen validation set, for an efficient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since hey cannot work separately. Adding Plan and Track greatly improves the compleion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks stepy-step reduces the complexity. We observe a relatively insignificant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On he other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% relaive improvement over just Plan and Track alone. We, herefore, deduce that Plan and Track boost the performance of Eliminate during evaluation, since it is easier 0 remove irrelevant objects when the objective is more on sub-tasks. focuset 4.4. Automated Analysis of PET modules Plan Module We experiment with different LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal specifications, where there is no variation in sentence structures. For human goal specification, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while he other smaller models perform significantly worse. Eliminate module We evaluate the zero-shot recepacle/object masking performance of Macaw on the hree splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns ‘0 the objects v.s. objects that the rule-based exper interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, i demonstrates consistent masking performance on al hree splits of the environment, even on the unseen lit. In addition, we note that object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshol 0. (o) ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker shoul record the last sub-task as shed” if and only if the environment is “fully solved” by the expert. As an agreement measure, we report a precision of 0.99 an a recall of 0.78 for Macaw-11B and a precision of 0.and a recall of 0.96 for Macaw-large. The larger mode (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we fin that both models produce similar overall results, which may suggest that the overall results could be improve with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure examples for sub-task generation in Table 4. The first type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu --- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58) GPT-Neo-2.7B (Black et al., 2021) 99.29 (1.00) 96.27 (0.98) 4.70 (0.82) 9.16 (0.80) MT-NLG (Smith et al., 2022) 98.57 (0.99) | 100 (1.00) 40.04 (0.94) 49.3 (0.94) Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. ROC curve for train receptacle identification ROC curve for valid_seen receptacle identification ROC curve for valid_unseen receptacle identification 10 1008 08206 = 06 =Bas Bos Bas & & & 02 ozoo) — nuc=0.6362842168817953 oo} — ‘uc=0.6623760037789325 oo} 4 — Auc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa os 08False Positive Rate Falze Positive Rate False Positive Rate ROC curve for train object identification ROC curve for valid_seen object identification ROC curve for valid_unseen object identification 10 10o8 o8 o2 06 2 06 2Box Box Eon & & © oz oz oz oo} be — nuc=0.7469037685050358 oo} be — wuc=o.7841203546425264 oo) — wuc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa 0608False Positive Rate False Positive Rate False Positive Rete Figure 6. Plot of AUC scores of zero-shot relevance identification across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identification. Bottom: Object relevance identification. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention 25Action Attention + Eliminate 25Action Attention + Plan & Track 35Action Attention + PET 52.5 27.Table 3. Comparison of different ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. racies in the human goal specifications. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. Eliminate Module We observe that the main source of elimination error occurs when the module of interest so the agent fails to find such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment?, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the “mistakes” of our Eliminate module are reasonable. hat subfor tasks Track Module Experimentally, we find task planning/tracking is particularly helpful that require counting procedures. As shown in Table ??, PET breaks the task of “Place two soapbar in cabinet” into two repeating set of sub-tasks: “take soapbar—+place soapbar in/on cabinet”. Sub-task planning and tracking, therefore, simplify the hard problem of counting. ? ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ --- --Plan, Eliminate, and Track Human Goal Specification Examples Task Chill a cup and place it in the cabinet. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal specifications (Task), ground-truth (GT) v.s. generated (Gen). In the first example, generated plan differs from the ground truth but the meaning agrees. In the second example, the generated plan largely differs from the ground truth due to the mistake in human goal specification — “another side on the desk” instead of “shelf”. 5. Conclusion, Limitations, and Future Work n this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs © assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. n our experiments, we combine PET with a novel Acion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since he PET framework is not trained to fit the training set asks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together imrove the performance of Eliminate module to achieve he best performance. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk—put the policy (i.e., reading an instruction manual about the environment). References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https: //arxiv.org/abs/2204.01691. Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. Andreas, J., Klein, D., and Levine, §. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166-175. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https ://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for highlevel natural language instruction execution, 2021. URL https: //arxiv.org/abs/2107.05612. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, --- --Plan, Eliminate, and Track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, L, Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tramér, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2021. URL https: //arxiv.org/abs/2108.07258. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration, 2020. URL https://arxiv. org/abs/2007 .00643. Cideron, G., Seurin, M., Strub, F., and Pietquin, O. Higher: Improving instruction following with hindsight generation for experience replay. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225-232. IEEE, 2020. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018a. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018b. Fulda, N., Ricks, D., Murdoch, B., and Wingate, D. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. In Conference on Robot Learning, pp. 485-497. PMLR, 2021. He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022a. URL https: //arxiv.org/abs/2201.07207. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, L., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models, 2022b. URL https: //arxiv.org/abs/2207 .05608. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Be-z: Zeroshot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019. Kollar, T., Tellex, S., Roy, D., and Roy, N. Toward understanding natural language directions. In5th ACM/IEEE International Conference on HumanRobot Interaction (HRI), pp. 259-266. IEEE, 2010. Lin, B. Y., Huang, C., Liu, Q., Gu, W., Sommerer, S., and Ren, X. On grounded planning for embodied tasks with language models. arXiv preprint arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk the talk: Connecting language, knowledge, and action in route instructions. Def, 2(6):4, 2006. Mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In Thirtieth AAAI Conference on Artificial Intelligence, 2016. Micheli, V. and Fleuret, F. Language models are fewshot butlers. arXiv preprint arXiv:2104.07972, 2021. Min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y., and Salakhutdinov, R. Film: Following instructions in language with modular\n",
      "--- EXPERIMENT ---\n",
      "s and analysis demonstrate that LLMs not only remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. In addition, multiple LLMs may be used in coordination with each other to assist the agent from different aspects. Our contributions are as follows: 1. PET: A novel framework for leveraging pretrained LLMs with embodied agents; our work shows that each of P, E, T serves a complementary role and should be simultaneously addressed to tackle control tasks. 2. An Action Attention agent that handles the changing action space for text environments. 3. A 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. 2. Related Work Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepputtis et al., 2020; Jang et al., 2022; Shridhar et al., 2022; Sharma et al., 2021) or reinforcement learning (Misra. et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; Akakzia et al., 2020) policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010). While some prior research has used pre-trained language embeddings to improve generalization to new instructions (Nair et al., 2022), they lack domain knowledge that is captured in LLMs. Our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs, and is designed to be compatible with any language conditional policies above. LLMs for Control LLMs have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step lowlevel commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-plan. In addition, all the above LLMs require few-shot demonstrations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli & Fleuret (2021) fine-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM fine-tuning requires a fully text-based environment, consistent expert trajectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task specification. We show that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. Hierarchical Planning with Natural Language Due to the structured nature of natural language, Andreas et al. (2017) explored associating each task description to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to templates (Oh et al., 2017). Recent works have shown that LLMs are proficient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan --- --Plan, Eliminate, and Track ning with progress tracking. To our knowledge, PET is the first work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games _ Text-based games are complex, interactive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addition to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld (Shridhar et al., 2020b) simulator extends a common text-based game simulator, TextWorld Coté et al. (2018a), to create text-based analogs of each ALFRED scene. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two different models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions. Fulda et al. (2017); Ahn et al. (2022) explore action elimination in the setting of affordances. Zahavy et al. (2018) trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elimination signal. 3. Plan, Eliminate, and Track In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (Mp), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (Mg) uses a zero-shot QA language model to score an mask objects and receptacles that are irrelevant to the current sub-task. The Track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate an Track are classification tasks. We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agen observes the masked observation and takes an action conditioned on the current sub-task. Problem Setting We define the task description as T, the observation string at time step t as O', and the list of permissible actions {a!|a can be executed} as A’. For each observation string O', we define the Examples from Training set Planning Module ( Target Output Figure 2. Plan Module (Sub-task Generation). 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rf and o; respectively. The classification between receptacles and objects is defined by the environment (Shridhar et al., 2020b). For a task J, we assume there exists a list of sub-tasks Sy = {s1,...s8,} that solves T. 3.1. Plan Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (Mp) to generate a list of high-level sub-tasks for a task description T. Inspired by the contextual prompting techniques for planning with LLMs (Huang et al., 2022a), we use an LLM as our plan module Mp. For a given task description J, we compose the query question Q7 as “What are the middle steps required to JT?” , and require Mp to generate a list sub-tasks Sy = {51,... sx}. Specifically, we select the top 5 example tasks 7” from the training set based on RoBERTa (Liu et al., 2019) embedding similarity with the query task 7. We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt Pz for Mp (Fig. 2): Pr= concat(Qre, Srp, Ore, Spe, Or) An illustration of our prompt format is shown in Figure 2, where T =“heat some apple and put it in fridge”, and Q;» =“What are the middle steps required to put two spraybottles on toilet”, Srp =“take a spraybottle, --- --Plan, Eliminate, and Track place the spraybottle in/on toilet, take a spraybottle, lace the spraybottle in/on toilet”. The expected list of sub-tasks to achieve this task T is s; =‘take an apple’, 82 =‘heat the apple’, and s3 =‘place the apple in/on ridge’ (You are in the middle of a room. Looking quickly around you, you see a a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a microwave 1. Your task is to heat some apple and put it in the fridge. Where should you go? You are in the middle of a room. Looking quickly around you, you see a a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. Figure 3. Eliminate Module (Receptacle Masking). We use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. As we can see, the original observation is too long and the receptacles shown in red are not relevant for task completion. These receptacles are filtered by the QA model making the observation shorter. 3.2. Eliminate Typical Alfworld scenes can start with around 15 receptacles, each containing up to 15 objects. In some close-to-worst cases, there can be around 30 open-able receptacles (e.g. a kitchen with many cabinets and drawers), and it easily takes an agent with no prior knowledge more than 50 steps for the agent to find the desired object (repeating the process of visiting each receptacle, opening it, closing it). We observe that many receptacles and objects are irrelevant to specific asks during both training and evaluation, and can e easily filtered with common-sense knowledge about he tasks. For example, in Fig. 3 the task is to heat some apple. By removing the irrelevant receptacles like he coffeemachine, garbagecan, or objects like knife, we could significantly shorten our observation. We herefore propose to leverage commonsense knowledge captured by large pre-trained QA models to design our Eliminate module Mg to mask out irrelevant receptacles and objects. For task 7, we create prompts in the format P, =“Your ask is to: T. Where should you go to?” for receptacles and P, =“Your task is to: T. Which objects will be relevant?” for objects. Using the pre-trained QA model Meg in a zero-shot manner, we compute score flo, = Mag(P.,0:) for each object 0; and pr, = Mg(Po, ri) for each receptacle r; in observation at every step.represents the belief score of whether the common-sense QA model believes the object /receptacle is relevant to T. We then remove 0; from observation if fio, < To, and remove 1; if jp, < 7,. Threshold 7,,7, are hyperparameters. Environment You are in the middle of a room. Looking quickly around you, you see ..., a garbagecan 1, a sinkbasin 1, and a toaster 1. > go to sinkbasinOn the sinkbasin 1, you see nothing. > go to diningtableOn the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. You take apple 1 from diningtable 1. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, ..... You take apple 1 from diningtable 1. Did you finish the task of take an apple ? Tracking Module ( Update progress tracker Figure 4. Track Module (Progress Tracking). At every step, we take the last 3 steps of roll-out as context and append a query (about whether the current sub-task is completed) to get the prompt. A pre-trained QA model generates a Yes/No answer to the prompt. For the answer “Yes”, we update the tracker to the next sub-task. 3.3. Track For the agent to utilize the high-level plan, it first needs to know which sub-task to execute. A human actor typically starts from the first item and check-off the tasks one by one until completion. Therefore, similar to Section 3.2, we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection.! Specifically, as illustrated in Figure 4, for sub-task list Sr = {s1,... 8x}, we keep track of a progress tracker p (initialized at 1) that indicates the sub-task the agent is currently working on (sp). We then compose the context as the last d steps of the agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks, so the agent has no means to recover if it undoes its previous sub-task at test time. --- --Plan, Eliminate, and Track for the current sub-task and the question as “Did you finish the task of s,?”. For efficiency, we set d := min(d + 1,3) at each step. Note that d is reset to whenever the progress tracker updates. Hence, the emplate P, = concat(O'4,..., 01, “Did you finish he task of s,?”). We feed P, to a pre-trained zeroshot QA model M+ and compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No\"[Pa)- IE pate “Yes” [Pa) > pate (*No\" [Pa) hen we increment the tracker p to track the next subask. f the tracking ends prematurely, meaning that p > len(S7) but the environment has not returned “done”, we fall back to conditioning with 7. We study the rate of pre-mature ends in Section 4.4 in terms of precision and recall. 3.4. Agent Since the number of permissible actions can vary a lot by the environment, the agent needs to handle arbitrary dimensions of action space. While Shridhar et al. (2020b) addresses this challenge by generating actions token-by-token, such a generation process leads o degenerate performance even on the training set. We draw inspiration from the field of text summarizaion, where models are built to handle variable input lengths. See et al. (2017) generates a summary through an attention-like “pointing” mechanism that extracts he output word by word. Similarly, an attention-like “pointing” model could be used to select an action from he list of permissible actions. Action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. We eschew the long rollout/ large action space problems by (1) representing observations y averaging over history, and (2) individually encoding actions (Fig 5). In our proposed action attention ramework, we first represent historical observations H' as the average of embeddings of all individual observations through history (Eq. 1), and H@ as the list of embeddings of all the current permissible actions (Eq. 2). Then, in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H\"), the current observation embedding (O*), and the list of action embeddings (H“). In Eq. 4 we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H\"), the current observation embedding (O'), and embedding of action (a;). Finally, we compute the dot-product of the query and keys as action scores for the policy 7 (Eq. 5). H'= avg jc[1,t-1]Embed(O’) ( H4 = [Embed(a‘), ..., Embed(a',)] ( Q = Mo (Embed(T), H', Embed(0'), H“) (K; = Mx (Embed(7), H‘, Embed(0'), Embed(a}) m@ = softmax ([Q - K;|é € all permissible actions]) 4. Experiments and Results We present our experiments as follows. First, we explain the environment setup and baselines for our experiments. Then we compare PET to the baselines on different splits of the environment. Finally, we conduct ablation studies and analyze the PET framework part by part. We show that PET generalizes better to human goal specification under efficient behavior cloning training. 4.1. Experimental Details AlfWorld Environment ALFWorld (Shridhar et al., 2020b) is a set of TextWorld environments (Coté et al., 2018b) that are parallels of the ALFRED embodied dataset (Shridhar et al., 2020a). ALFWorld includes 6 task types that each require solving multiple compositional sub-goals. There are 3553 training task instances ({tasktype, object, receptacle, room}),in-distribution evaluation task instances (seen split tasks themselves are novel but take place in rooms seen during training) and 134 out-of-distribution evaluation task instances (unseen split - tasks take place in novels rooms). An example of the task could be: “Rinse the egg to put it in the microwave.” Each training instance in AlfWorld comes with an expert, from which we collected our training demonstration. Human Goal Specification The crowd-sourced human goal specifications for evaluation contain 66 unseen verbs and 189 unseen nouns (Shridhar et al., 2020b). In comparison, the template goals use only 12 ways of goal specification. In addition, the sentence structure for human goal specification is more diverse compared to the template goals. Therefore, human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. Pre-trained LMs. For the Plan module (sub-task generation), we experimented with the open-source GPT-Neo-2.7B (Black et al., 2021), and an industryscale LLM with 530B parameters (Smith et al., 2022). --- --Plan, Eliminate, and Track 1oO (a) ot at al, ay You are in the middle You go to diningtable You go to microwave = of a room. Looking 1. In diningtable 1, 1. You open Goto Examine Heat quickly around, you you see a apple 1, a microwave 1. ... cabinet 1 cabinet 1 applesee a cabinet 5, a mug 3, a cup 3. You cabinet 4, ... take apple 1 from | diningtable 1. Heat the et x v T J Za J GSC K, Ks N _ Action Attenti Heat applei Figure 5. Agent (Action Attention). Action Attention block is a transformer-based framework that computes a key K; for each permissible action and output action scores as dot-product between key and query Q from the observations. Template Goal Specification Model seen unseen seen unseen BUTLER + DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 9 - GPT (Micheli & Fleuret, 2021) 91 95 42PET + Action Attention (Ours) 70 67.5 52.5Table 1. Comparison of different models in terms of completion rate per evaluation split (seen and unseen), with and without human annotated goals. PET under-performs GPT on Template goal specifications but generalizes better to For the Eliminate module (receptacle/object masking), we choose Macaw-11b (Tafjord & Clark, 2021), which is reported to have common sense QA performance on par with GPT3 (Brown et al., 2020) while eing orders of magnitudes smaller. We use a decision hreshold of 0.4 for Macaw score below which the objects are masked out. For the Track module (progress racking), we use the same Macaw-11b model as the Eliminate module answer to Yes/No questions. Actor Model Design. Our Action Attention agent (Mg and Mx) is a 12-layer transformer with 2 heads and hidden dimension 384. The last layer is then fed into two linear heads to generate K and Q. For embedding of actions and observations, we use re-trained RoBERTa-large (Liu et al., 2019) with emedding dimension 1024. For sub-task generation, we use ground-truth sub-tasks for training, and generated sub-tasks from Plan module for evaluation. Experimental Setup. Unlike the original benchmark (Shridhar et al., 2020b), we experiment with models trained with behavior cloning. Although Shridhar et al. (2020b) observe that models benefit greatly from DAgger training, DAgger assumes an expert that is well-defined at all possible states, which is inefficient and impractical. In our experiments, training is 100x slower with DAgger compared to behavior cloning (human goal specifications. * We include the performance of BUTLER with DAgger for completeness. All other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. weeks for DAgger v.s. 6 hours for Behavior Cloning). In addition, we demonstrate that our models surpass the DAgger training performance of the BUTLER (Shridhar et al., 2020b) agents trained with DAgger, even when our agent does not have the option to interact with the environment. Baselines. Our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. At each time step t, the encoder takes initial observation s°, current observation s‘, and task string Stas, and generates representation r'. The recurrent aggregator combines r’ with the last recurrent state h'~! to produce h’, which is then decoded into a string a’ representing action. In addition, the BUTLER agent uses beam search to get out of stuck conditions in the event of a failed action. Our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the AlfWorld training set. Specifically, the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. Human Goal Specification --- --Plan, Eliminate, and Track 4.2. Overall Results on Template and Human Goals We compare the performance of action attention assisted by PET with BUTLER (Shridhar et al., 2020b) and fine-tuned GPT (Micheli & Fleuret, 2021) in Table 1. For human goal specifications, PET outperforms SOTA (GPT) by 25% on seen and 5% on the unseen split. Although PET under-performs GPT on Template goal specifications, GPT requires fine-tuning on fully textbased expert trajectory and thus loses adaptability to different environment settings. Qualitatively, on human goal specification tasks, where the goal specifications are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal specifications as shown in Section 4.5. Quantitatively, GPT suffers from a relative 50% performance drop transferring from template to human-goal specifications, whereas PET incurs only a 15 ~ 25% drop. The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC per‘orms poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. (Section 4.1) 4.3. Ablations for Plan, Eliminate, and Track n Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories samled from the training set. The data set size is chosen © match the size of the seen validation set, for an efficient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since hey cannot work separately. Adding Plan and Track greatly improves the compleion rate relatively by 60%, which provides evidence to our hypothesis that solving some embodied tasks stepy-step reduces the complexity. We observe a relatively insignificant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. On he other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than 60% relaive improvement over just Plan and Track alone. We, herefore, deduce that Plan and Track boost the performance of Eliminate during evaluation, since it is easier 0 remove irrelevant objects when the objective is more on sub-tasks. focuset 4.4. Automated Analysis of PET modules Plan Module We experiment with different LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal specifications, where there is no variation in sentence structures. For human goal specification, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while he other smaller models perform significantly worse. Eliminate module We evaluate the zero-shot recepacle/object masking performance of Macaw on the hree splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns ‘0 the objects v.s. objects that the rule-based exper interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, i demonstrates consistent masking performance on al hree splits of the environment, even on the unseen lit. In addition, we note that object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshol 0. (o) ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker shoul record the last sub-task as shed” if and only if the environment is “fully solved” by the expert. As an agreement measure, we report a precision of 0.99 an a recall of 0.78 for Macaw-11B and a precision of 0.and a recall of 0.96 for Macaw-large. The larger mode (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we fin that both models produce similar overall results, which may suggest that the overall results could be improve with LLMs doing better on both precision and recall. 4.5. Qualitative Analysis Plan Module We show two types of failure examples for sub-task generation in Table 4. The first type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu --- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58) GPT-Neo-2.7B (Black et al., 2021) 99.29 (1.00) 96.27 (0.98) 4.70 (0.82) 9.16 (0.80) MT-NLG (Smith et al., 2022) 98.57 (0.99) | 100 (1.00) 40.04 (0.94) 49.3 (0.94) Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. ROC curve for train receptacle identification ROC curve for valid_seen receptacle identification ROC curve for valid_unseen receptacle identification 10 1008 08206 = 06 =Bas Bos Bas & & & 02 ozoo) — nuc=0.6362842168817953 oo} — ‘uc=0.6623760037789325 oo} 4 — Auc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa os 08False Positive Rate Falze Positive Rate False Positive Rate ROC curve for train object identification ROC curve for valid_seen object identification ROC curve for valid_unseen object identification 10 10o8 o8 o2 06 2 06 2Box Box Eon & & © oz oz oz oo} be — nuc=0.7469037685050358 oo} be — wuc=o.7841203546425264 oo) — wuc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa 0608False Positive Rate False Positive Rate False Positive Rete Figure 6. Plot of AUC scores of zero-shot relevance identification across all tasks in the Alfworld-Thor environment, with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identification. Bottom: Object relevance identification. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention 25Action Attention + Eliminate 25Action Attention + Plan & Track 35Action Attention + PET 52.5 27.Table 3. Comparison of different ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. However, applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. racies in the human goal specifications. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations. Eliminate Module We observe that the main source of elimination error occurs when the module of interest so the agent fails to find such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment?, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the “mistakes” of our Eliminate module are reasonable. hat subfor tasks Track Module Experimentally, we find task planning/tracking is particularly helpful that require counting procedures. As shown in Table ??, PET breaks the task of “Place two soapbar in cabinet” into two repeating set of sub-tasks: “take soapbar—+place soapbar in/on cabinet”. Sub-task planning and tracking, therefore, simplify the hard problem of counting. ? ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ --- --Plan, Eliminate, and Track Human Goal Specification Examples Task Chill a cup and place it in the cabinet. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal specifications (Task), ground-truth (GT) v.s. generated (Gen). In the first example, generated plan differs from the ground truth but the meaning agrees. In the second example, the generated plan largely differs from the ground truth due to the mistake in human goal specification — “another side on the desk” instead of “shelf”. 5.\n",
      "--- CONCLUSION ---\n",
      ", Limitations, and Future Work n this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs © assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. n our experiments, we combine PET with a novel Acion Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since he PET framework is not trained to fit the training set asks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together imrove the performance of Eliminate module to achieve he best performance. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk—put the policy (i.e., reading an instruction manual about the environment). References Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https: //arxiv.org/abs/2204.01691. Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020. Andreas, J., Klein, D., and Levine, §. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166-175. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https ://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata. Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for highlevel natural language instruction execution, 2021. URL https: //arxiv.org/abs/2107.05612. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, --- --Plan, Eliminate, and Track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, L, Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tramér, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2021. URL https: //arxiv.org/abs/2108.07258. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration, 2020. URL https://arxiv. org/abs/2007 .00643. Cideron, G., Seurin, M., Strub, F., and Pietquin, O. Higher: Improving instruction following with hindsight generation for experience replay. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225-232. IEEE, 2020. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018a. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018b. Fulda, N., Ricks, D., Murdoch, B., and Wingate, D. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. In Conference on Robot Learning, pp. 485-497. PMLR, 2021. He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022a. URL https: //arxiv.org/abs/2201.07207. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, L., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models, 2022b. URL https: //arxiv.org/abs/2207 .05608. Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Be-z: Zeroshot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019. Kollar, T., Tellex, S., Roy, D., and Roy, N. Toward understanding natural language directions. In5th ACM/IEEE International Conference on HumanRobot Interaction (HRI), pp. 259-266. IEEE, 2010. Lin, B. Y., Huang, C., Liu, Q., Gu, W., Sommerer, S., and Ren, X. On grounded planning for embodied tasks with language models. arXiv preprint arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk the talk: Connecting language, knowledge, and action in route instructions. Def, 2(6):4, 2006. Mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In Thirtieth AAAI Conference on Artificial Intelligence, 2016. Micheli, V. and Fleuret, F. Language models are fewshot butlers. arXiv preprint arXiv:2104.07972, 2021. Min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y., and Salakhutdinov, R. Film: Following instructions in language with modular methods, 2021. --- --Plan, Eliminate, and Track Misra, D., Langford, J., and Artzi, Y. Mapping instructions and visual observations to actions with reinforcement learning. arXiv preprint arXiv:1704.08795, 2017. Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. Learning language-conditioned robot behayior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pp. 1303-1315. PMLR, 2022. Oh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pp. 2661-2670. PMLR, 2017. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017. Sharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021. Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020a. Shridhar, M., Yuan, X., Coté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b. Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022. Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zheng, E., Child, R.., Aminabadi, R. Y., Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990, 2022. URL https: //arxiv.org/abs/2201.11990. Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022. Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., and Ben Amor, H. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33:13139-13150, 2020. Tafjord, O. and Clark, P. question-answering with macaw. arXiv:2109.02598, 2021. General-purpose arXiv preprint Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pp. 507-1514, 2011. Yao, S., Rao, R., Hausknecht, M., and Narasimhan, K. Keep calm and explore: Language models for action generation in text-based games, 2020. URL https: //arxiv.org/abs/2010.02903. Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and Mannor, S. Learn what not to learn: Action elimination with deep reinforcement learning. Advances in neural information processing systems, 31, 2018.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(text, chunk_size=512)\n",
    "summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained large language models (LLMs) capture procedural knowledge about the world. recent work has leveraged LLM’s ability to generate abstract plans to simplify challenging control tasks. but transformer architecture inherits several constraints that make it difficult for the LLM o directly serve as the agent: limited inut lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nonext e. to maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify he control problem. the plan module transates a task description into a list of high-level sub-tasks. the track module determines whether the agent has accomplished each sub-t. the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications. humans can abstractly plan their everyday tasks without execution; for example, given the task “Make breakfast”, we can roughly plan to first pick up a mug and make coffee, before grabbing eggs to scramble. Eliminate Take an apple You see apple, Paug-knife — Action: Pickup Apple Update Progress Finished taking | an apple? J Figure 1. PET framework. Track module uses a QA model to track the completion of an observation. recent work has used LLMs for abstract planning for embodied or gaming agents. these have shown incipient success in extracting procedural world knowledge from LLMs in linguistic orm with posthoc alignment to executable actions. but they treat LLMs as the acor, and focus on adapting LLM outputs to executable actions either through fine-tuning (Micheli & Fleuret, 2021) or constraints actor uses LLM as the actor for pure-text environments with imited interactions (Huang et al., 2022b; Ahn et al., 2022) he provides all available objects and possible interactions at the start and imits tasks to the set of provided objects/interactions. he limits the environment to the set of provided objects/interactions. a more realistic scenario leads to a --- -- --Plan, Eliminate, and Track diverse, complicated tasks or large and changing action space. the text description of the observation increases as a function of the number o receptacles a table. in this work, we explore alternative mechanisms to leverage prior knowledge encoded in LLMs withow impacting the trainable nature of the actor. we propose a 3-step framework (Figure 1): Plan, Eliminate, an track. it simplifies complex tasks by breaking them down into sub-tasks. it uses a pretrained LLM to generate a list of sub-tasks for an inpu task description using example. the Eliminate module addresses the challenge of long observations. it uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. the Action Attention agen uses a transformer-based architecture to accommodate for long roll-outs. the agent observes the masked observation and takes an action conditioned on the current sub-task. our experiments and analysis demonstrate that LLMs remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. multiple LLMs may be used in coording and coording. our contributions are as follows: PET: A novel framework for leveraging pretrained LLMs with embodied agents. each of P, E, T serves a complementary role and should be addressed to tackle control tasks. a 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. TED WORK --- Language Conditioned Policies A considerable portion of prior work studies imitation learning. policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010; Kollar et al., 2010; Kollar et al., 2010; Kollar et al., 2010). pre-trained language embeddings lack domain knowledge that is captured in LLMs. our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs for control LLMs have recently achieved success in high-level planning. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. but LLM scores work for simple environments with actions limited to pick/place (Ahn et al., 2022) but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-plan. all the above LLMs require few-shot demonstrations of up to 17 examples. we show that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. hierarchical planning with natural language Due to the structured nature of natural language, Andreas et al. (2017) explored associating each task description to a modular subpolicy. recent works have shown that LLMs are proficient high-level planners. recent works have shown that LLMs are proficient high-level planners. PET is the first work combining a zero-shot subtask-level planner and a template. text games are complex, interactive simulations where the game state and action space are in natural lanugage. successful play requires skills like memory and planning, exploration (trial and error) and common sense. Agents for Large Action Space He et al. (2015) learns representation for state and actions with two different models and computes the Q function as the inner product of the representations. Zahavy et al. (2018) trains a model to create text-based analogs of each ALFRED scene. eliminate invalid actions on Zork from external environment signals. but the functionality depends on the existence of external elimination signal. plan, Eliminate module (Mg) uses a zero-shot QA language model to score mask objects. the Track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete and moves to the next sub-task. we also implement an attention-based agent (Action Attention) which scores each permissible action and is trained on imitation learning on the expert. problem setting We define the task description as T, the observation string at time step t as O', and the list of permissible actions a!|a can be executed as A’. 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. receptacles and objects within the observation as rf and o; respectively. the classification between receptacles and objects is defined by the environment. for a task J, we assume there exists a list of sub-tasks Sy = s1,...s8, that solves T. 3.1. we design the plan module (Mp) to generate a list of high-level sub-tasks for a task description T. for a given task description, we compose the query question Q7 as “What are the middle steps required to JT?” and require Mp to generate a list of sub-tasks Sy = 51,... sx. ERTa embedding similarity with query task 7 embedding similarity with query task 7. we then concatenate the example tasks with example tasks in a query-answer format to build the prompt Pz for Mp (Fig. 2): Pr= concat(Qre, Srp, Ore, Spe, Ore, ore, ore, ore, ore, ore, ore, ore, or a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a mic. the expected list of sub-tasks to achieve this task T is s; =‘ a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. we use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. receptacles shown in red are not relevant for task completion. receptacles are filtered by the QA model making the observation shorter. in some close-to-worst cases, there can be around 30 open-able receptacles (e.g. a kitchen with many cabinets and drawers), and it easily takes an agent with no prior knowledge more than 50 steps for the agent to find the desired many receptacles and objects are irrelevant to specific asks during training and evaluation. by removing the irrelevant receptacles like coffeemachine, garbagecan, or objects like knife, we could significantly shorten our observation. we herefore propose to leverage commonsense knowledge captured by larg. e pre-trained QA models to design our Eliminate module Mg. for task 7, we create prompts in the format P, =“Your ask is to: T. Where should you go to?” for receptacles and P, =“Your task is to: T. Which objects will be relevant?” for objects. r the common-sense QA model believes the object /receptacle is relevant to T. we then remove 0; from observation if fio,  To, and remove 1; if jp,  7,. Threshold 7,,7, are hyperparameters. Environment You are in the middle of a room. go to sinkbasinOn the sinkbasin 1, you see nothing. subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3,..... You take apple 1 from diningtable 1. Tracking Module (Progress Tracking) at every step, we take the last 3 steps of roll-out as context and append a query (about whether the current sub-task is completed) to get the prompt. a human actor typically starts from the first item and check-off the tasks one by one until completion. a human actor typically starts from the first item and checks-off the tasks one by one until completion. for sub-task list Sr = s1,... 8x, we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection. we keep track of a progress tracker p (initialized at 1) that indicates the sub-task the agent is currently working on (sp). we then compose the context as the last d steps of the agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks. the agent has no means to recover if it undoes its previous sub-task at test time. n(d + 1,3) is reset to whenever the progress tracker updates. we feed P, to a pre-trained zeroshot QA model M+. we compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No\"[Pa)- IE pate “Yes” [Pa) > pate (*No\" [P we study the rate of pre-mature ends in Section 4.4 in terms of precision and recall. 3.4. Agent Since the number of permissible actions can vary a lot by the environment, the agent needs to handle arbitrary dimensions of action space. we draw inspiration from the field of text summarizaion, where mr. mr. mr. mr. mr. mr. mr. action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. we eschew the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action space problems by (1) representing observations y averaging over. the long rollout/ large action in our proposed action attention ramework, we first represent historical observations H' as the average of embeddings of all individual observations through history. in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H\"), the current observation embedding (O*) and the list of action embeddings (H“). we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H), the current observation embedding (O') and embedding of action (a;). present our experiments as follows. first, we explain the environment setup and baselines for our experiments. then we compare PET to the baselines on different splits of the environment. 4.1. Experimental Details AlfWorld Environment ALFWorld (Shridhar et al., 2020b) is a set of TextWorld environments (Coté et al., 2018b) there are 3553 training task instances (tasktype, object, receptacle, room),in-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) 134 out-of-distribution evaluation task instances (unseen split - tasks take place in novels rooms) human goal specifications contain 66 unseen verbs and 189 unseen nouns. the template goals use only 12 ways of goal specification. the sentence structure for human goal specification is more diverse compared to the template goals. oal experiments are good for testing the generalization of models to out-of-distribution scenarios. we experimented with the open-source GPT-Neo-2.7B (black et al., 2021) and an industryscale LLM with 530B parameters (Smith et al., 2022). u see a cabinet 1 cabinet 1 applesee a cabinet 5, a mug 3, a cup 3 apple 1 from | diningtable 1. Heat the et x v T J Za J GSC K, Ks N _ Action Attention block is a transformer-based framework that computes a key K; for each permissible action and output action scores as dot-product between key and query Q from the observations. DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 9 - GPT (Micheli & Fleuret, 2021) 91 95 42PET + Action Attention (Ours) 70 67.5 52.5Table 1. o have common sense QA performance on par with GPT3. we use a decision hreshold of 0.4 for Macaw score below which the objects are masked out. for the track module, we use the same Macaw-11b model as the Eliminate module answer to Yes/No questions. we use re-trained RoBERTa-large (Liu et al., 2019) with emedding dimension 1024. for sub-task generation, we use ground-truth sub-tasks for training and generated sub-tasks from Plan module for evaluation. training is 100x slower with DAgger compared to behavior cloning. all other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b) agent consists of an encoder, an aggregator, and a decoder. each time step t, the encoder takes initial observation s°, current observation s‘, and task string Stas, and generates representation r' our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the alfworld training set. the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. overall results on Template and Human Goals. PET outperforms SOTA (GPT) by 25% on seen and 5% on unseen split. GPT requires fine-tuning on fully text-based expert trajectory and loses adaptability to different enviromental enviromental enviromental enviromental enviromental enviromental enviromental enviromental en on human goal specification tasks, GPT gets stuck repeating the same action after producing a single wrong move. on the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal specifications as shown in Section 4.5. the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC per‘orms poorly), we also include DAgger training results. however, action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. the data set size is chosen  match the size of the seen validation set. we treat Plan and Track as a single module for this ablation. more than 60% relaive improvement over just Plan and Track alone. we deduce that Plan and Track boost the performance of Eliminate during evaluation. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity against ground-truth sub-tasks. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity against ground-truth sub-tasks. we observe that all LLMs achieve high accuracy on template goal specifications, where there is no variation in sentence structures. we evaluate the zero-shot recepacle/object masking performance of Macaw on the hree splits of alfworld. in Fig 6, we illustrate the AUC curve of the relevance score that the model assigns ‘0 the objects v.s. objects that the rule-based exper interacted with whencompleting each task. object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. a decision threshol 0. (o) ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. as an agreement measure, we report a precision of 0.99 an a recall of 0.78 for Macaw-11B and a precision of 0.96 for Macaw-large. the larger mode (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to 78%. the smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. both models produce similar overall results. the first type of error is caused by generating synonyms of the ground truth. the second type of error is caused by inaccu --- -- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen seen unseen GPT-2 (Radford et al. MT-NLG (Smith et al., 2022) 98.57 (0.99) | 100 (1.00) 40.04 (0.94) 49.3 (0.94) Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity against ground-truth sub-tasks. overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks. oa 0608 10 00 02 oa 0608 10 00 02 oa 060810 00 02 oa 0608False Positive Rate False Positive Rate False Positive Rate False Positive Rete Figure 6. the QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention + Eliminate 25Action Attent. ion + plan & track 35Action Attention + PET 52.5 27.Table 3. applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. but applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. the main source of elimination error occurs when the module of interest fails to find such receptacles. the main source of elimination error occurs when the module of interest so the agent fails to find such receptacles. some objects in the AI2Thor simulator do not spawn according to common sense. hat subfor tasks Track Module Experimentally, we find task planning/tracking is particularly helpful that require counting procedures. we find task planning/tracking is particularly helpful that require counting procedures. ai2thor.allenai.org/ithor/documentation/objects/objecttypes. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on om the Plan module on human goal specifications (Task), ground-truth (GT) v.s. generated (Gen) in the first example, generated plan differs from the ground truth but the meaning agrees. in the second example, generated plan largely differs from the ground truth due to the mistake in human goal specification. we propose the plan, Eliminate, and track framework that uses pre-trained LLMs our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. n our experiments, we combine PET with a novel Acion Attention agent that handles the dynamic action space in AlfWorld. our ablation studies show the Plan and Track modules together imrove the performance of Eliminate module to achieve he best performance. our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents. multiple LLMs may be used in coordination with each other to further improve effectiveness. the progress tracker does not take into consideration previous progress being undone. future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk. reference: Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Ruano, R. J. arXiv preprint arXiv:2006.07185, 2020. Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language to autonomously-acquired skills via goal generation. PMLR, 2017. Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. if you use this software, pleasecite it using these metadata. man, R., von Arx, von Arx, von Arx, Bernstein, M. S., Bosselut, A., Brunskill, E., Buch, S., Card, D., Castellon, R., Chatterji, Chen, A., Creel, A., Davis, J. Q., Demszky, D., Doumbouya, M., Dur --- --Plan, Eliminate, and track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Niebles, J. C., Nyarko, J., Orr, L. a., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, J., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W. an, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Askell, A. et al. Advances in neural information processing systems, 33:1877-1901, 2020. Object goal navigation using goal-oriented semantic exploration, 2020. Coté, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., and al. Textworld: A learning environment for text-based games. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. arXiv preprint arXiv:1511.04636, 2015. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Zeng, A., Tompson, J., Mordatch, L., Chebotar, Y., Brown, N., Jackson, T., Luu, L., Levine, S. h planning with language models, 2022b. conference on robot learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. arXiv preprint arXiv:2209.00465, 2022. oy, D., and Roy, N. Toward understanding natural language directions. arXiv preprint arXiv:2209.00465, 2022. arXiv preprint arXiv:1907.11692, 2019. reprint arXiv:1907.11692, 2019.. mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. Micheli, V. and Fleuret, F. Language models are fewshot butlers. LLMs remove 40% of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with 99% accuracy. PET: A novel framework for leveraging pretrained LLMs with embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied embodied our work shows that each of P, E, T serves a complementary role. an Action Attention agent that handles the changing action space for text environments. a 15% improvement over SOTA for generalization to human goals via sub-task planning and tracking. al., 2020; Jang et al., 2022; Shridhar et al., 2022; Sharma et al., 2021) or reinforcement learning (Misra. et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs. LLMs for Control LLMs have recently achieved success in high-level planning. the generated sub-tasks cannot be directly executed in an end-to-end control environment. LLM scores work for simple environments with actions limited to pick/place. but fails with environments with more objects and diverse actions. the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-pll. Micheli & Fleuret fine-tuned a GPT2-medium model on expert trajectories in alfworld. LM fine-tuning requires a fully text-based environment, consistent expert trajectories, and a fully text-based action space. such requirements greatly limit the generalization to other domains, and even to other forms. Andreas et al. (2017) explored associating each task description to a modular sub-policy. recent works have shown that our PET framework achieves better generalization to human goal specifications which the agents were not trained on. PET is the first work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy. Text Games _ Text-based games are complex, interactive simulations where the game state and action spacing. e are in natural lanugage. they are fertile ground for language-focused machine learning research. successful play requires skills like memory and planning, exploration (trial and error) and common sense. th two different models and computes the Q function as the inner product of the representations. this could generalize to large action space, but only considered a small number of actions. Zahavy et al. trains a model to eliminate invalid actions on Zork from external environment signals. but the functionality depends on the existence of external elimination signal. in plan module (Mp), a pre-trained LLM generates a list of sub-tasks. the track module (Mr) uses a zeroshot QA language model to score mask objects and receptacles that are irrelevant to the current sub-task. the track module (Mr) uses a zeroshot QA language model to determine if the current sub-task is complete. the agen observes the masked observation and takes an action conditioned on the current sub-task. the task description is T, the observation string at time step t as O', and the list of permissible actions a!|a can be executed. 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. receptacles and objects within the observation as rf and o; respectively, we prompt the LLM to generate the desired sub-tasks. receptacles and objects within the observation as rf and o; respectively, we define the Examples from Training set Planning Module ( Target Output Figure 2. receptacles and objects are defined by the environment. for a task J, we assume there exists a list of sub-tasks Sy = s1,...s8, that solves T. 3.1. we design the Plan module (Mp) to generate a list of high-level sub-tasks for a task description T. we select the top 5 example tasks 7” from the training set based on RoBERTa embedding similarity with the query task 7. we then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt Pz for a given task. the expected list of sub-tasks to achieve this task is s; =‘take an apple’, 82 =‘heat the apple’, and s3 =‘place the appl. a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a sinkbasin 1, and a microwave 1. looking quickly around you, you see a a cabinet 5, a cabin 5, et 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 2, a countertop 1, a diningtable 1, a fridge 1, a garbagecan1. we use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. the original observation is too long and the receptacles shown in red are not relevant for task completion. nes can start with around 15 receptacles, each containing up to 15 objects. in some close-to-worst cases, there can be around 30 open-able receptacles. it takes an agent with no prior knowledge more than 50 steps for the agent to find the desired object. we propose to leverage commonsense knowledge captured by pre-trained QA models to design our Eliminate module Mg to mask out irrelevant receptacles and objects. for task 7, we create prompts in the format P, =“Your ask is to: T”. we compute score flo, = Mag(P.,0:) for each object 0; and pr, = Mg(Po, ri) for each receptacle r; in observation at every step.represents the belief score of whether the common-sense QA model believes the object /receptacle is relevant to T. you take apple 1 from diningtable 1. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. Subtasks Take an apple Heat the apple + Place the apple in/ on fridge Context On the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. Tracking Module (Progress Tracking) is a pre-trained QA model that generates a Yes/No answer to the prompt. a pre-trained QA model generates a Yes/No answer to the prompt. te. a human actor typically starts from the first item and check-off the tasks one by one until completion. we use a pre-trained QA model to design the Track module Mr to perform zero-shot sub-task completion detection. we then compose the context as the last d steps of thomas. e agent observation 1Note that the current system design does not allow re-visiting finished sub-tasks. the agent has no means to recover if it undoes its previous sub-task at test time. for efficiency, we set d := min(d + 1,3) at each step. the emplate P, = concat(O'4,..., 01, “Did you finish the task of a pre-trained zeroshot QA model M+ compute the probability of okens ‘Yes’ and ‘No’ as follows: paz, (“Yes”|Pa) and Pag (*No\"[Pa)- IE pate “Yes” [Pa) > pate (*No) hen we increment the tracker p to track the next subask. f the tracking ends prematurely, meaning Shridhar et al. (2020b) addresses this challenge by generating actions token-by-token. such a generation process leads o degenerate performance even on the training set. et al. (2017) generates a summary through an attention-like “pointing” mechanism that extracts he output word by word. action Attention We are interested in learning a policy 7 that outputs the optimal action among permissible actions. we eschew the long rollout/ large action space problems by (1) representing observations y averaging over history, and (2) individually encoding actions. in Eq. 3, we compute the query Q using a transformer with a “query” head (Mg) on task embedding (H), the current observation embedding (O*) and the list of action embeddings (H“). in the query Q, we compute the key K; for each action a; using the same transformer with a “key” head (Mx) on task embedding (H); and the we compute the dot-product of the query and keys as action scores for the policy 7 (Eq. 5). first, we explain the environment setup and baselines for our experiments. then we compare PET to the baselines on different splits of the environmental splits of the environmental splits of the environmental splits of the environmental splits of the environmental splits. we conduct ablation studies and analyze the PET framework part by part. we show that PET generalizes better to human goal specification under efficient behavior cloning training. ALFWorld includes 6 task types that each require solving multiple compositional sub-goals. 53 training task instances (tasktype, object, receptacle, room) and 134 out-of-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) 134 out-of-distribution evaluation task instances (unseen split tasks themselves are novel but take place in novels rooms) crowd-sourced human goal specifications for evaluation contain 66 unseen verbs and 189 unseen nouns. human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. human goal experiments are good for testing the generalization of models to out-of-distribution scenarios. the open-source GPT-Neo-2.7B (Black et al., 2021) and an industryscale LLM with 530B parameters (Smith et al., 2022). You go to diningtable You go to microwave = of a room. action Attention block is a transformer-based framework that computes a key K. for each permissible action and output action scores as dot-product between key and query Q from the observations. template Goal Specification Model seen unseen seen unseen BUTLER + DAgger* (Shridhar et al., 2020b) 40 35 8BUTLER + BC (Shridhar et al., 2020b) 10 ison of different models in terms of completion rate per evaluation split (seen and unseen) we choose Macaw-11b (Tafjord & Clark, 2021) which is reported to have common sense QA performance on par with GPT3 (Brown et al., 2020) the track module (progress racking) uses the same Macaw-11b model as the Eliminate module answer to Yes/No questions. our Action Attention agent (Mg and Mx) is a 12-layer transformer with 2 heads and hidden dimension 384. the last layer is fed into two linear heads to generate K and Q. for sub-task generation, we use ground-truth-truth-truth-truth-truth we experiment with models trained with behavior cloning. DAgger assumes an expert that is well-defined at all possible states. training is 100x slower with DAgger compared to behavior cloning (human goal spherical spherical spherical spherical spherical spherical spherical spherical spherical sp we include the performance of BUTLER with DAgger for completeness. all other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET. weeks for DAgger v.s. 6 hours for behavior cloning for BUTLER+BC and PET. baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. the recurrent aggregator combines r’ with the last recurrent state h'! to produce h’, which is then decoded into a string a’ representing action. our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the alfworld training set. the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss. for human goal specifications, PET outperforms SOTA (GPT) by 25% on seen and 5% on unseen split. GPT requires fine-tuning on fully text-based expert trajectory and loses adaptability to different environment settings. GPT suffers from a relative 50% performance drop transferring from template to human-goal specifications. the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). the setting closest to PET is BUTLER with behavior cloning (BUTLER + BC per‘orms poorly). action attention assisted by PET outperorms BUTLER with DAgger by more than 2x while eing much more efficient. the data set size is chosen  match the size of the seen validation set, for an efficient and sparse setting. the data set size is chosen  match the size of the seen validation set, for an efficient and sparse setting. ngle module for this ablation since hey cannot work separately. Adding Plan and Track greatly improves the compleion rate by 60%. we observe a relatively insignificant 3% improvement in absolute performance when adding Eliminate without sub-task tracking. ngle module for this ablation since hey cannot work separately. Plan and Track deduce that Plan and Track boost the performance of Eliminate during evaluation. we experiment with different LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo2.7B (Black et al., 2021) and the 530B parameter MT-NLG (Smith et al., 2022) we observe that all LLMs achieve high accuracy on template goal specifications. MT-NLG generates subtasks similar to ground truth in terms of embedding similarity. he other smaller models perform significantly worse. e of the relevance score that the model assigns '0 the objects v.s. objects that the rule-based exper interacted with whencompleting each task. we note that object receptacle accuacy is generally lower than object accuracy because o he counter-intuitive spawning locations described in Section 4.5. ow 0.4 has a recall of 0.91 and reduces the number o: bjects in observation by 40% on average. a sub-task tracker shoul record the last sub-task as shed” if and only if the environment is “fully solved” by the expert. the larger mode (Macaw-11b) is more precise but misses more detection. the smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall. the larger mode (Macaw-11b) is more precise but misses more detection. the first type of error is caused by generating synonyms of the ground truth. the second type of error is caused by inaccu --- -- --Plan, Eliminate, and Track Template Goals Human Goals LLM seen unseen GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58) GPT-Neo the MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. MT-NLG generates sub-tasks on hard tasks with human goal specification. ROC curve for train receptacle identification ROC curve for valid_unseen receptacle identification 10 1008 08206 = 06 =Bas Bos Bas & & & 02 ozoo) nuc=0.6362842168817953 oo 4 — Auc=0.00 02 04 0608 10 00 02 oa 0608 10 00 02 oa os 08False entification ROC curve for valid_unseen object identification 10 10o8 o8 o2 06 2 06 2 06 2Box Eon &  oz oz oo be — nuc=0.746903768505050358 oo be — wuc=0.00 02 04 0608 10 00 02 oa 0608False Positive Rate False Positive the QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects. incorrectly masks a receptacle that contains the object Model Ablations seen unseen Action Attention 25Action Attention + Eliminate 25Action Attention + Plan & Track 35Action Attention + PET 52.5 27.Table 3. Comparison of different ablations of PET trained on a sampled set of applying Eliminate module alone has an insignificant effect on overall performance compared to Plan & Track. applying Eliminate module on sub-tasks together with Plan & Track results in a much more significant performance improvement. the main source of elimination error occurs when the module of interest fails to find such receptacles. this is often because some objects in the AI2Thor simulator do not spawn according to common sense. so, such generations in AI2Thor are unlikely in real deployment; thus, the “mistakes” of our Eliminate module are reasonable. task planning/tracking is particularly helpful that require counting procedures. ai2thor.allenai.org/ithor/documentation/objects/objecttypes/ --- --Plan, Eliminate, and track human goal specification Examples Task Chill. ar cool the mug—+place the mug in/on coffeemachine Gen chill the mug—return the mug to coffeemachine Task Take the pencil from the desk, put it on the other side of the desk GT take a pencil—+place the pencil in/on shelf Gen white pencil on another spot on the desk Table 4. Failure examples from the Plan module on human goal specifications (Task), ground-truth (GT) v n this work, we propose the Plan, Eliminate, and Track framework that uses pre-trained LLMs. our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. Acion Attention agent handles the dynamic action space in alfworld. our action attention agent greatly outperforms the BUTLER baseline. our ablation studies show the plan and track modules together imrove the performance of Eliminate module to achieve he best performance. the track module (progress tracker) does not re-visit finished sub-tasks. it does not re-visit finished sub-tasks. the agent is executing sub-tasks, and it picked up a pan. future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the pick up the white pencil on the desk—put the policy. reference: Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakri i can, not as i say: Grounding language in robotic affordances, 2022. i can, not as i can, not as i say: grounding language in robotic affordances, 2022. arXiv preprint arXiv:2006.07185, 2020. kzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. a persistent spatial semantic representation for highlevel natural language instruction execution, 2021. a persistent spatial semantic representation for highlevel natural language instruction execution, 2021. Davis, J. Q., Demszky, D., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Guha, N. I., Li, X. L., Li, X., Ma,. plan, elimination, and track T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Narayan, A., Niebles, J. C., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, L a. W., Tramér, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zheng, L., Zhou, K. in 2020, IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225-232. IEEE, 2020. Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration, 2020. Textworld: A learning environment for text-based games. in workshop on computer games, pp. 41-75. a rock can be extracted from a rock or a rock. arXiv preprint arXiv:1703.03429, 2017. Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. racting actionable knowledge for embodied agents, 2022a. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Chebotar, Y., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ich conference on robot learning, pp. 991-1002. PMLIR, 2022. Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. arXiv preprint arXiv:2209.00465, 2022. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Ren, X. Walk the talk: Connecting language, knowledge, and action in route instructions. arXiv preprint arXiv:2104.07972, 2021. min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y., and Salakhutdinov, R. Film: Following instructions in language with modular methods, 2021. arXiv preprint arXiv:1704.08795, 2017. Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. learning language-conditioned robot behayior from offline data and crowd-sourced annotation. arXiv preprint arXiv:1704.04368, 2017. Sharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. ing, pp. 2661-2670. PMLR, 2017. Radford, A., Wu, J., Child, R., Luan, D., Luan, D., Luan, D., and Shridhar, M., Yuan, X., Coté, M.-A., Bisk, Y., Trischler, A., and Fox, D. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. arXiv preprint arXiv:2010.03768, 2020b. Using deepspeed and megatron to train megatron-turing NLG 530b, a large-scale generative language model. coRR, abs/2201.11990, 2022. Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B. Using deepspeed and megatron to train megatron- arXiv preprint arXiv:2212.04088, 2022. stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C. and Ben Amor, H. Language-conditioned imitation learning for robot manipulation tasks. v preprint Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. Understanding natural language commands for robotic navigation and mobile manipulation. v preprint Tellex, S., Kollar, T., Dickerson, T., Dickerson, S., Walter, M., Banerjee, A action elimination with deep reinforcement learning. Advances in neural information processing systems, 31, 2018. edward edward et al., edward et al., et al., et al., et al., et al., et al., et al., et al., et al., et al.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../summarized_text/2305.03048v2_summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
