{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>reached_out_link</th>\n",
       "      <th>reached_out_success</th>\n",
       "      <th>reached_out_note</th>\n",
       "      <th>num_models</th>\n",
       "      <th>num_datasets</th>\n",
       "      <th>num_spaces</th>\n",
       "      <th>title</th>\n",
       "      <th>github</th>\n",
       "      <th>github_stars</th>\n",
       "      <th>conference_name</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>github_mention_hf</th>\n",
       "      <th>has_artifact</th>\n",
       "      <th>submitted_by</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.03048</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Personalize Segment Anything Model with One Shot</td>\n",
       "      <td>https://github.com/zrrskywalker/personalize-sam</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.02463</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Shap-E: Generating Conditional 3D Implicit Fun...</td>\n",
       "      <td>https://github.com/openai/shap-e</td>\n",
       "      <td>11606.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.02483</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ChatGPT-steered Editing Instructor for Customi...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.03047</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Principle-Driven Self-Alignment of Language Mo...</td>\n",
       "      <td>https://github.com/IBM/Dromedary</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>NeurIPS2023</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.02440</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cheaply Evaluating Inference Efficiency Metric...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>akhaliq</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>2412.11314</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Reliable, Reproducible, and Really Fast Leader...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>dustalov</td>\n",
       "      <td>2024-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>2412.11974</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Emma-X: An Embodied Multimodal Action Model wi...</td>\n",
       "      <td>https://github.com/declare-lab/emma-x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>emrys-hong</td>\n",
       "      <td>2024-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>2412.11100</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DynamicScaler: Seamless and Scalable Video Gen...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BrandonLiu</td>\n",
       "      <td>2024-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>2412.11279</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VividFace: A Diffusion-Based Hybrid Framework ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>deepcs233</td>\n",
       "      <td>2024-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>2412.12098</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MaxInfoRL: Boosting exploration in reinforceme...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>csferrazza</td>\n",
       "      <td>2024-12-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        arxiv_id reached_out_link  reached_out_success reached_out_note  \\\n",
       "0     2305.03048             None                  0.0                    \n",
       "1     2305.02463             None                  0.0                    \n",
       "2     2305.02483             None                  0.0                    \n",
       "3     2305.03047             None                  0.0                    \n",
       "4     2305.02440             None                  0.0                    \n",
       "...          ...              ...                  ...              ...   \n",
       "4965  2412.11314             None                  NaN             None   \n",
       "4966  2412.11974             None                  NaN             None   \n",
       "4967  2412.11100             None                  NaN             None   \n",
       "4968  2412.11279             None                  NaN             None   \n",
       "4969  2412.12098             None                  NaN             None   \n",
       "\n",
       "      num_models  num_datasets  num_spaces  \\\n",
       "0            0.0           0.0         1.0   \n",
       "1            7.0           0.0        70.0   \n",
       "2            0.0           0.0         0.0   \n",
       "3            0.0           1.0         1.0   \n",
       "4            0.0           0.0         0.0   \n",
       "...          ...           ...         ...   \n",
       "4965         0.0           0.0         0.0   \n",
       "4966         0.0           0.0         0.0   \n",
       "4967         0.0           0.0         0.0   \n",
       "4968         0.0           0.0         0.0   \n",
       "4969         0.0           0.0         0.0   \n",
       "\n",
       "                                                  title  \\\n",
       "0      Personalize Segment Anything Model with One Shot   \n",
       "1     Shap-E: Generating Conditional 3D Implicit Fun...   \n",
       "2     ChatGPT-steered Editing Instructor for Customi...   \n",
       "3     Principle-Driven Self-Alignment of Language Mo...   \n",
       "4     Cheaply Evaluating Inference Efficiency Metric...   \n",
       "...                                                 ...   \n",
       "4965  Reliable, Reproducible, and Really Fast Leader...   \n",
       "4966  Emma-X: An Embodied Multimodal Action Model wi...   \n",
       "4967  DynamicScaler: Seamless and Scalable Video Gen...   \n",
       "4968  VividFace: A Diffusion-Based Hybrid Framework ...   \n",
       "4969  MaxInfoRL: Boosting exploration in reinforceme...   \n",
       "\n",
       "                                               github  github_stars  \\\n",
       "0     https://github.com/zrrskywalker/personalize-sam        1505.0   \n",
       "1                    https://github.com/openai/shap-e       11606.0   \n",
       "2                                                None           NaN   \n",
       "3                    https://github.com/IBM/Dromedary        1119.0   \n",
       "4                                                None           NaN   \n",
       "...                                               ...           ...   \n",
       "4965                                                            NaN   \n",
       "4966            https://github.com/declare-lab/emma-x           NaN   \n",
       "4967                                                            NaN   \n",
       "4968                                                            NaN   \n",
       "4969                                                            NaN   \n",
       "\n",
       "     conference_name  upvotes  num_comments  github_mention_hf  has_artifact  \\\n",
       "0               None        9             1                1.0          True   \n",
       "1               None        3             1                0.0          True   \n",
       "2               None        3             1                0.0         False   \n",
       "3        NeurIPS2023        1             5                1.0          True   \n",
       "4               None        1             0                0.0         False   \n",
       "...              ...      ...           ...                ...           ...   \n",
       "4965            None        1             1                0.0         False   \n",
       "4966            None        2             1                1.0         False   \n",
       "4967            None        0             1                0.0         False   \n",
       "4968            None        7             1                0.0         False   \n",
       "4969            None        1             1                0.0         False   \n",
       "\n",
       "     submitted_by        date  \n",
       "0         akhaliq  2023-05-05  \n",
       "1         akhaliq  2023-05-05  \n",
       "2         akhaliq  2023-05-05  \n",
       "3         akhaliq  2023-05-05  \n",
       "4         akhaliq  2023-05-05  \n",
       "...           ...         ...  \n",
       "4965     dustalov  2024-12-17  \n",
       "4966   emrys-hong  2024-12-17  \n",
       "4967   BrandonLiu  2024-12-17  \n",
       "4968    deepcs233  2024-12-17  \n",
       "4969   csferrazza  2024-12-17  \n",
       "\n",
       "[4970 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"huggingface/community-science-paper-v2\")\n",
    "train_ds=ds['train']\n",
    "df=pd.DataFrame(train_ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arxiv_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2305.03048\n",
       "1       2305.02463\n",
       "2       2305.02483\n",
       "3       2305.03047\n",
       "4       2305.02440\n",
       "           ...    \n",
       "4965    2412.11314\n",
       "4966    2412.11974\n",
       "4967    2412.11100\n",
       "4968    2412.11279\n",
       "4969    2412.12098\n",
       "Name: arxiv_id, Length: 4970, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id=df['arxiv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2305.03048'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --content-disposition https://arxiv.org/pdf/{arxiv_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        response=requests.get(url)\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4970"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-20 08:38:57--  https://arxiv.org/pdf/2305.03048\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 1342613 (1.3M) [application/pdf]\n",
      "저장 위치: `2305.03048v2.pdf.1'\n",
      "\n",
      "2305.03048v2.pdf.1  100%[===================>]   1.28M  --.-KB/s    /  0.07s   \n",
      "\n",
      "2024-12-20 08:38:57 (18.3 MB/s) - `2305.03048v2.pdf.1' 저장함 [1342613/1342613]\n",
      "\n",
      "--2024-12-20 08:38:59--  https://arxiv.org/pdf/2305.02463\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 4875369 (4.6M) [application/pdf]\n",
      "저장 위치: `2305.02463v1.pdf'\n",
      "\n",
      "2305.02463v1.pdf    100%[===================>]   4.65M  12.8MB/s    /  0.4s    \n",
      "\n",
      "2024-12-20 08:38:59 (12.8 MB/s) - `2305.02463v1.pdf' 저장함 [4875369/4875369]\n",
      "\n",
      "--2024-12-20 08:39:01--  https://arxiv.org/pdf/2305.02483\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 664864 (649K) [application/pdf]\n",
      "저장 위치: `2305.02483v2.pdf'\n",
      "\n",
      "2305.02483v2.pdf    100%[===================>] 649.28K  --.-KB/s    /  0.06s   \n",
      "\n",
      "2024-12-20 08:39:01 (10.4 MB/s) - `2305.02483v2.pdf' 저장함 [664864/664864]\n",
      "\n",
      "--2024-12-20 08:39:03--  https://arxiv.org/pdf/2305.03047\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 1848389 (1.8M) [application/pdf]\n",
      "저장 위치: `2305.03047v2.pdf'\n",
      "\n",
      "2305.03047v2.pdf    100%[===================>]   1.76M  --.-KB/s    /  0.09s   \n",
      "\n",
      "2024-12-20 08:39:03 (20.0 MB/s) - `2305.03047v2.pdf' 저장함 [1848389/1848389]\n",
      "\n",
      "--2024-12-20 08:39:04--  https://arxiv.org/pdf/2305.02440\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 5989442 (5.7M) [application/pdf]\n",
      "저장 위치: `2305.02440v1.pdf'\n",
      "\n",
      "2305.02440v1.pdf    100%[===================>]   5.71M  1.30MB/s    /  4.4s    \n",
      "\n",
      "2024-12-20 08:39:10 (1.30 MB/s) - `2305.02440v1.pdf' 저장함 [5989442/5989442]\n",
      "\n",
      "--2024-12-20 08:39:11--  https://arxiv.org/pdf/2305.02783\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 191721 (187K) [application/pdf]\n",
      "저장 위치: `2305.02783v4.pdf'\n",
      "\n",
      "2305.02783v4.pdf    100%[===================>] 187.23K  --.-KB/s    /  0.03s   \n",
      "\n",
      "2024-12-20 08:39:12 (5.71 MB/s) - `2305.02783v4.pdf' 저장함 [191721/191721]\n",
      "\n",
      "--2024-12-20 08:39:13--  https://arxiv.org/pdf/2305.02412\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 2361198 (2.3M) [application/pdf]\n",
      "저장 위치: `2305.02412v2.pdf'\n",
      "\n",
      "2305.02412v2.pdf    100%[===================>]   2.25M  1.38MB/s    /  1.6s    \n",
      "\n",
      "2024-12-20 08:39:15 (1.38 MB/s) - `2305.02412v2.pdf' 저장함 [2361198/2361198]\n",
      "\n",
      "--2024-12-20 08:39:17--  https://arxiv.org/pdf/2305.02790\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 2790880 (2.7M) [application/pdf]\n",
      "저장 위치: `2305.02790v1.pdf'\n",
      "\n",
      "2305.02790v1.pdf    100%[===================>]   2.66M  1.57MB/s    /  1.7s    \n",
      "\n",
      "2024-12-20 08:39:20 (1.57 MB/s) - `2305.02790v1.pdf' 저장함 [2790880/2790880]\n",
      "\n",
      "--2024-12-20 08:39:21--  https://arxiv.org/pdf/2305.03052\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 10530538 (10M) [application/pdf]\n",
      "저장 위치: `2305.03052v1.pdf'\n",
      "\n",
      "2305.03052v1.pdf    100%[===================>]  10.04M  1.04MB/s    /  9.7s    \n",
      "\n",
      "2024-12-20 08:39:32 (1.04 MB/s) - `2305.03052v1.pdf' 저장함 [10530538/10530538]\n",
      "\n",
      "--2024-12-20 08:39:37--  https://arxiv.org/pdf/2305.03043\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.195.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 7004256 (6.7M) [application/pdf]\n",
      "저장 위치: `2305.03043v1.pdf'\n",
      "\n",
      "2305.03043v1.pdf    100%[===================>]   6.68M  1.09MB/s    /  6.1s    \n",
      "\n",
      "2024-12-20 08:39:44 (1.09 MB/s) - `2305.03043v1.pdf' 저장함 [7004256/7004256]\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[38;5;241m=\u001b[39m pexpect\u001b[38;5;241m.\u001b[39mspawn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msh, args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m, cmd])  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn(command, args, preexec_fn, dimensions)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawnpty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[1;32m    304\u001b[0m                              cwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcwd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ptyprocess\u001b[38;5;241m.\u001b[39mPtyProcess\u001b[38;5;241m.\u001b[39mspawn(args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mread(exec_err_pipe_read, \u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(arxiv_id)):\n\u001b[0;32m----> 2\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget --content-disposition https://arxiv.org/pdf/\u001b[39m\u001b[38;5;132;01m{arxiv_id[i]}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_expand(cmd, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     child\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "for i in range(len(arxiv_id)):\n",
    "    !wget --content-disposition https://arxiv.org/pdf/{arxiv_id[i]}\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud import storage\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=documentai.DocumentProcessorServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = %system gcloud config get-value core/project\n",
    "project_id = project_id[0]\n",
    "location = 'us'\n",
    "processor_id=os.getenv('PROCESSOR_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genuine-episode-425909-e2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'857826f004a8bd4c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"2305.03048v2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    content=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = documentai.types.ProcessRequest(\n",
    "    name=f\"projects/{project_id}/locations/{location}/processors/{processor_id}\",\n",
    "    raw_document=documentai.types.RawDocument(\n",
    "        content=content,\n",
    "        mime_type=\"application/pdf\"  # PDF 파일의 MIME 타입\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.process_document(request=request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = result.document\n",
    "text=document.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str) -> dict:\n",
    "    \"\"\"Detects the text's language.\"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.detect_language(text)\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Confidence: {}\".format(result[\"confidence\"]))\n",
    "    print(\"Language: {}\".format(result[\"language\"]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n",
      "Confidence: 0.9814419150352478\n",
      "Language: en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'language': 'en',\n",
       " 'confidence': 0.9814419150352478,\n",
       " 'input': 'arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\\nPERSONALIZE SEGMENT ANYTHING MODEL WITH\\nONE SHOT\\nRenrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\\nHao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\\n¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\\n3 Institute of Automation, Chinese Academy of Sciences\\n4CFCS, School of CS, Peking University\\n{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\\nkaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\\nABSTRACT\\nDriven by large-data pre-training, Segment Anything Model (SAM) has been\\ndemonstrated as a powerful promptable framework, revolutionizing the segmenta-\\ntion field. Despite the generality, customizing SAM for specific visual concepts\\nwithout man-powered prompting is under-explored, e.g., automatically segmenting\\nyour pet dog in numerous images. In this paper, we introduce a training-free\\nPersonalization approach for SAM, termed PerSAM. Given only one-shot data,\\ni.e., a single image with a reference mask, we first obtain a positive-negative lo-\\ncation prior for the target concept in new images. Then, aided by target visual\\nsemantics, we empower SAM for personalized object segmentation via two pro-\\nposed techniques: target-guided attention and target-semantic prompting. In this\\nway, we can effectively customize the general-purpose SAM for private use without\\nany training. To further alleviate the ambiguity of segmentation scales, we present\\nan efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\\nintroduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\\ntunes 2 parameters within 10 seconds for improved performance. To demonstrate\\nour efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\\nobject segmentation, and also test our methods on various one-shot image and\\nvideo segmentation benchmarks. Besides, we propose to leverage PerSAM to\\nimprove DreamBooth for personalized text-to-image synthesis. By mitigating\\nthe disturbance of training-set backgrounds, our approach showcases better target\\nappearance generation and higher fidelity to the input text prompt. Code is released\\nat https://github.com/ZrrSkywalker/Personalize-SAM.\\n(1) User provides\\n(2) One-shot Learning\\n(3)\\nPersonalized Segmentation\\nOne Image\\nOne Mask\\nTraining-free\\nFine-tuning\\nPerSAM\\nPerSAM-F\\n... in various poses or scenes\\nFigure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\\n(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\\nwe introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\\n* Equal contribution. † Corresponding author.\\n1\\n2 Parameters\\nPerSAM\\nPerSAM-F\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nFine-tune\\n10 Seconds\\nThe hat on a\\nteddy bear\\nA photo of a [V] cat\\n\"A [V] cat on a beach\"\\n... in various poses or scenes\\nFine-tune\\nThe body of a\\nrobot toy\\n***\\nFigure 2: Personalized Segmentation Exam-\\nples. Our PerSAM (Left) can segment personal\\nobjects in any context with favorable perfor-\\nmance, and PerSAM-F (right) further alleviates\\nthe ambiguity issue by scale-aware fine-tuning.\\n1\\nINTRODUCTION\\nA photo of a [V] backpack\\n\"A [V] backpack on a table of a classroom\"\\nFigure 3: Improving DreamBooth (Ruiz et al.,\\n2022) with PerSAM. By mitigating the distur-\\nbance of backgrounds during training, our ap-\\nproach can help to achieve higher-quality person-\\nalized text-to-image generation.\\nFoundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\\net al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\\nJia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\\nof large-scale datasets and computational resources. They demonstrate extraordinary generalization\\ncapacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\\nInspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\\n11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\\ndefines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\\nreturning the expected mask, which allows for segmenting any objects in visual contexts.\\nHowever, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\\nto crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\\nbedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\\neach image, you must precisely find the target object within complicated contexts, and then activate\\nSAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\\nautomatically segment user-designated visual concepts in a simple and efficient manner?\\nTo this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\\nModel. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\\na user-provided reference image and a rough mask of the personal concept. Specifically, we first\\nobtain a location confidence map for the target object in the test image by feature similarities, which\\nconsiders the appearance of every foreground pixel. According to confidence scores, two points are\\nselected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\\ninto SAM\\'s decoder for segmentation. Within the decoder, we propose to inject visual semantics of\\nthe target object to unleash SAM\\'s personalized segmentation power with two techniques:\\n•\\n• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM\\'s\\ndecoder by the location confidence map. This explicitly compels the prompt tokens to\\nmainly concentrate on foreground target regions for intensive feature aggregation.\\nTarget-semantic Prompting. To explicitly provide SAM with high-level target semantics,\\nwe fuse the original prompt tokens with the embedding of the target object, which provides\\nthe low-level positional prompt with additional visual cues for personalized segmentation.\\nWith the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\\npersonalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\\nour approach can cope well with scenarios that require segmenting one object among multiple similar\\nones, simultaneously segmenting several identical objects in the same image, or tracking different\\nobjects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\\n2\\nwhere the object comprises visually distinct subparts or hierarchical structures to be segmented,\\ne.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\\nPerSAM in determining the appropriate scale of mask as output, since both the local part and the\\nglobal shape can be regarded as valid masks by SAM.\\nTo alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\\nfreeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\\nwithin 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\\nsegmentation results of different mask scales. To adaptively select the best scale for varying objects,\\nwe employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\\nfinal output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\\ndata and exhibits better segmentation accuracy shown in Figure 2 (Right).\\nMoreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\\nfine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\\nfew images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\\nto convert these images into an identifier [V] in the word embedding space, which, however, can\\nsimultaneously include the background information, e.g., stairs or the forest. This would override\\nthe newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\\npropose to leverage PerSAM to segment the target object within training images, and only supervise\\nDreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\\nWe summarize the contributions of our paper as follows:\\n• Personalized Object Segmentation. We first investigate how to customize a general-\\npurpose segmentation model (SAM) into personalized scenarios with minimal expense. To\\nthis end, we introduce two efficient and effective methods, along with a new segmentation\\ndataset, PerSeg, for the evaluation of personalized object segmentation.\\n• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\\nSAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\\nfine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\\n• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\\none-shot part and semantic segmentation, and video object segmentation. In addition,\\nPerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\\n2 RELATED WORK\\nFoundation Models. With powerful generalization capacity, pre-trained foundation models can be\\nadapted for various downstream scenarios and attain promising performance. In natural language\\nprocessing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\\n2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\\ndemonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\\nspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\\ncontrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\\nPainter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\\nprompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\\n2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\\nlow-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\\nsegmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\\nThere are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\\nfaster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\\n3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\\nHuang et al., 2023) image processing. From another perspective, we propose to personalize the\\nsegmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\\ninto a specialist with only one shot. Our method can also assist the personalization of text-to-\\nimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\\n2022), which improves the generation quality by segmenting the foreground target objects from the\\nbackground disturbance.\\n3\\nLarge Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\\net al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\\nrequires a pixel-level comprehension of a image. Various segmentation-related tasks have been\\nexplored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\\nnarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\\net al., 2020); instance segmentation, focusing on the identification of individual object instances (He\\net al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\\nlabels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\\ninvolving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\\nby language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\\nhave proposed large-scale vision models for image segmentation. They are pre-trained by extensive\\nmask data and exhibit strong generalization capabilities on numerous image distributions. Segment\\nAnything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\\ntation to learn a promptable segmentation framework, which generalizes to downstream scenarios\\nin a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\\nrobust in-context learning paradigm and can segment any images by a given image-mask prompt.\\nSEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\\nreferences, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\\nintroduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\\nfor evaluation. Instead of developing large segmentation models, our goal is to personalize them to\\nsegment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\\nPerSAM-F, which efficiently customize SAM for personalized segmentation.\\nParameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\\ntasks can be computationally expensive and memory-intensive, posing challenges for resource-\\nconstrained applications. To address this issue, recent works have focused on developing parameter-\\nefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\\nfreeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\\ntuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\\nlearnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\\nmore competitive performance with scale and robust domain transfer compared to full model tuning.\\nLow-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\\net al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\\nwhich significantly reduces the number of learnable parameters required for downstream tasks.\\nAdapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\\nto be inserted between layers of the original transformer, introducing lightweight MLPs for feature\\ntransformation. Different from existing works, we adopt a more efficient adaption method delicately\\ndesigned for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\\nseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\\nof segmentation scale with superior performance.\\n3 METHOD\\nIn Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\\nintroduce the task definition for personalized object segmentation. Then, we illustrate the methodology\\nof our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\\nto assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\\n3.1\\nPERSONALIZED OBJECT SEGMENTATION\\nA Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\\nencoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\\npromptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\\na box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\\nadopts Encp to encode the human-given prompts of a length k into prompt tokens as\\nFI = Enc(I), Tp = Encp(P),\\n(1)\\n4\\nF₁\\nEncode\\nCosine Similarity\\n{FT}=1\\nTest Image I\\nTarget\\nLocal\\nFeatures\\n{T}=1\\nFR\\nMR° FR\\nEncode\\nPerSAM\\'s Decoder\\nTarget-guided\\nAttention\\nImage-to-Token\\nCross-Attention\\nTarget-semantic\\nPrompting\\n↑\\n{Si)=1\\nLocal Confidence Maps\\nModulate\\nToken-to-Image\\nCross-Attention\\n↑\\nAggregate\\nAttention Matrix A\\nLocal\\nFeatures\\n{T}=1\\nToken\\nSelf-Attention\\nOverall\\nConfidence Map S\\nAggregate\\n↑\\na\\nConcat(\\n+ Repeat(\\n)\\nOverall\\nConfidence Map S\\nTM Тр\\n× 2\\nTR\\nOne-shot Image IR\\nOne-shot Mask MR\\nPositive Prior\\nNegative Prior\\nFigure 4: Positive-negative Location Prior.\\nWe calculate a location confidence map for the\\ntarget object in new test image by the appear-\\nance of all local parts. Then, we select the loca-\\ntion prior as the point prompt for PerSAM.\\nLow-level\\nPositional Prompt\\nHigh-level\\nSemantic Prompt\\nFigure 5: Target-guided Attention (Left) &\\nTarget-semantic Prompting (Right). To in-\\nject SAM with target semantics, we explicitly\\nguide the cross-attention layers, and propose\\nadditional prompting with high-level cues.\\nwhere F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\\nc denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\\nDecм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\\nconcatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\\ntokens are responsible for generating the mask output, formulated as\\nM =\\nDecм (FI, Concat(TM,Tp)),\\nwhere M denotes the final segmentation mask predicted by SAM.\\n(2)\\nTask Definition. Although SAM is generalized enough for any object by prompting, it lacks the\\nability to automatically segment specific subject instances. Considering this, we define a new task\\nfor personalized object segmentation. The user provides only a single reference image, and a mask\\nindicating the target visual concept. The given mask can either be an accurate segmentation, or a\\nrough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\\nwithin new images or videos, without additional human prompting. For evaluation, we annotate a\\nnew dataset for personalized segmentation, named PerSeg. The raw images are collected from the\\nworks for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\\ncontaining various categories of visual concepts in different poses or scenes. In this paper, we propose\\ntwo efficient solutions for this task, which we specifically illustrate as follows.\\n3.2\\nTRAINING-FREE PERSAM\\nLocation Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\\nfirst obtains a confidence map that indicates the location of the target object in the new test image\\nI. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\\nThe encoder can be SAM\\'s frozen backbone or other pre-trained vision models, for which we adopt\\nSAM\\'s image encoder Enc, by default. We formulate the process as\\nFI= Enc(I), FR = EncI(IR),\\n(3)\\nwhere F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\\nof foreground pixels within the visual concept from FR, resulting in a set of n local features as\\n{T}}=1 = MR ○ Fr,\\n(4)\\n5\\nwhere T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\\nmaps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\\n{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\\n(5)\\nNote that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\\nprobability for a different local part of object in the test image, such as the head, the body, or the\\npaws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\\nthe overall confidence map of the target object as\\nS =\\nn\\nn\\nSiЄRhxw\\n(6)\\nBy incorporating the confidences of every foreground pixel, S can take the visual appearance of\\ndifferent object parts into consideration, and acquire a relatively comprehensive location estimation.\\nPositive-negative Location Prior. To provide PerSAM with a location prior on the test image,\\nwe select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\\nrespectively. The former represents the most likely center position of the target object, while the\\nlatter inversely indicates the background. Then, they are regarded as the positive and negative point\\nprompts, and fed into the prompt encoder as\\nTp = Encp(Ph, P₁) Є R2×c,\\n(7)\\nwhich denote the prompt tokens for SAM\\'s decoder. In this way, SAM would tend to segment the\\ncontiguous region surrounding the positive point, while discarding the negative one\\'s on the image.\\nTarget-guided Attention. Although the positive-negative point prompt has been obtained, we\\nfurther propose a more explicit semantic guidance to the cross-attention operation in SAM\\'s decoder,\\nwhich concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\\nthe overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\\nconcept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\\nto guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\\nwe denote every attention map after the softmax function as A = Rhxw, and then modulate its\\nattention distribution by\\nA9\\n=\\nsoftmax A+ a softmax(S)\\n•\\n(8)\\nwhere a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\\nto capture more visual semantics associated with the target subject, other than the unimportant\\nbackground area. This contributes to more effective feature aggregation in attention mechanisms, and\\nenhances the final segmentation accuracy of PerSAM in a training-free manner.\\nTarget-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\\ninformation, such as the coordinate of a point or a box. To provide SAM\\'s decoder with more high-\\nlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level\\nsemantic prompting. We first obtain the global embedding TR of the object in the reference image by\\nboth I average pooling between different local features as\\nn\\nTR\\n=\\nΣΤΑ €R1xc\\nn\\ni=1\\nThen, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\\nthem into the decoder block, which is shown in Figure 5 as\\nT9 = Repeat (TR) + Concat(TM,Tp),\\n(10)\\nwhere T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\\noperation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\\nis not only prompted by low-level location points, but also high-level target visual cues.\\n6\\nTest Image\\nPerSAM\\nOutput\\nThree\\nM1\\nM2\\n+\\nM3\\nscales\\nRandom\\nNoise\\n→>\\nDreamBooth\\n→>\\n↑\\n\"a [V] cat\"\\nReconstruction\\nLoss\\nW1\\nF\\nW2\\n1- W1\\n+\\nW₂ )\\n1\\nUser provides\\nOutput Mask\\nM\\nFine-tune\\nWeighted\\nSummation\\nFreeze\\nPerSAM\\nBackground\\nDisturbance\\nDecouple\\nFigure 6: The Scale-aware Fine-tuning in\\nPerSAM-F. To alleviate the scale ambiguity,\\nPerSAM-F adopts two learnable weights for\\nadaptively aggregating three-scale masks.\\nFigure 7: PerSAM-assisted DreamBooth.\\nWe utilize PerSAM to decouple the target ob-\\njects from the background for improving the\\ngeneration of DreamBooth.\\nCascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\\non the test image from SAM\\'s decoder, which however, might include rough edges and isolated\\nbackground noises. For further refinement, we iteratively feed the mask back into the decoder Decм\\nfor a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\\nmask along with the previous positive-negative point prompt. For the second step, we acquire the\\nbounding box enclosing the mask from the first step, and prompt the decoder additionally with this\\nbox for more accurate object localization. As we only iterate the lightweight decoder without the\\nlarge-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\\n3.3\\nFINE-TUNING OF PERSAM-F\\nAmbiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\\ntory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\\nto the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\\nof two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\\nat the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\\nin a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\\nSAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\\nmasks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\\nrequired to manually select one mask out of three, which is effective but consumes extra manpower.\\nIn contrast, our personalized task aims to customize SAM for automatic object segmentation without\\nthe need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\\nby parameter-efficient fine-tuning.\\nScale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\\nfine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\\nfirst follows PerSAM to obtain the location prior, and refers to SAM\\'s original solution to output\\nthree-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\\nmask weights, w₁, W2, and calculate the final mask output by a weighted summation as\\n.\\n.\\nM = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\\n(11)\\nwhere w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\\ntuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\\nthe entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\\nW1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\\nscale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\\nconcepts, improving the generalization capacity of PerSAM.\\n7\\nTable 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\\nblou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\\net al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\\nMethod\\nmIoU\\nbloU Param. Can Barn Clock Cat\\nBack- Teddy Duck Thin Red\\npack Bear Toy Bird Cartoon\\nRobot\\nToy\\nPainter\\nVP\\nSEEM*\\nSegGPT*\\n56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\\n65.9 25.5 383M 61.2\\n58.6\\n87.1 55.7 341M 65.4 82.5\\n94.3 76.5 354M\\n93.0\\n33.3 20.9\\n98.2\\n65.0\\n59.2\\n76.6\\n66.7\\n79.8\\n89.9\\n67.4\\n81.0\\n72.4\\n72.4\\n91.1\\n94.1\\n95.2\\n98.0\\n71.3\\n97.0\\n95.8\\n96.6 63.8\\n92.6\\n94.1\\n94.4\\n93.7\\n97.2\\n92.6\\n97.3\\n96.2\\n0\\n90.70 95.39\\n2\\n96.2 38.9 96.2\\n96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\\n94.6\\n97.3\\n93.7\\n97.0\\n60.6\\n97.1\\n96.7\\nPerSAM 89.3 71.7\\nPerSAM-F 95.3 77.9\\nTable 2: Video Object Segmen-\\ntation on DAVIS 2017 val (Pont-\\nTuset et al., 2017). We utilize\\ngray color to denote the methods\\ninvolving in-domain training.\\nTable 3: One-shot Semantic and Part Segmentation on FSS-\\n1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\\nPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\\n2023). We report the mIoU scores and utilize gray color to\\ndenote the methods involving in-domain training.\\nOne-shot Semantic Seg.\\nFSS-1000 LVIS-92²\\nOne-shot Part Seg.\\nPASCAL-Part\\nPainter\\nSEEM\\nSegGPT\\nPerSAM\\nMethod\\nJ&F І\\nAGSS\\n67.4 64.9 69.9\\nAFB-URR 74.6 73.0 76.1\\n34.6 28.5 40.8\\n58.9 55.0 62.8\\n75.6 72.5 78.6\\nF\\nMethod\\nPACO-Part\\nHSNet\\n86.5\\n17.4\\n32.4\\n22.6\\nVAT\\n90.3\\n18.5\\n33.6\\n23.5\\nPainter\\n61.7\\n10.5\\n30.4\\n14.1\\nSegGPT\\n85.6\\n18.6\\n66.9 71.3 75.1\\nPerSAM\\n81.6\\n15.6\\n32.5\\n22.5\\nPerSAM-F 76.1 74.9 79.7\\nPerSAM-F\\n86.3\\n18.4\\n32.9\\n22.7\\n3.4\\nPERSAM-ASSISTED DREAMBOOTH\\nFor personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\\ndiffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\\ncat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\\nimages. This This would inject the redundant background information in the training images into the\\nidentifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\\nof backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\\nPerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\\nbelonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\\nvisual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\\nassisted DreamBooth can not only synthesize the target object with better visual correspondence, but\\nalso increase the diversity of the new backgrounds guided by the input text prompt.\\n4 EXPERIMENT\\nWe first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\\nwith various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\\neffectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\\nablation studies to investigate our designs on PerSeg in Section 4.4.\\n4.1 PERSONALIZED EVALUATION\\nPerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\\ntermed PerSeg. The raw images are collected from the training data of subject-driven diffusion\\nworks (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\\ncategories in total, including daily necessities, animals, and buildings. In different poses or scenes,\\neach object is associated with 5~7 images and masks, where we fix one image-mask pair as the\\nuser-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\\nPlease refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\\n8\\nThe ring\\non a clock\\n2\\nThe teapot\\non a tray\\nThe backpack\\ncarried by a\\nThe top part\\nof a can\\nwoman\\nFigure 8: Visualization of PerSAM-F\\'s Im-\\nprovement. Our scale-aware fine-tuning can\\nwell alleviate the scale ambiguity of PerSAM.\\nFigure 9: Visualization of Video Object Seg-\\nmentation. Our approach performs well for\\nsegmenting multiple objects in a video.\\nPerformance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\\neffectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\\ntion of PerSAM-F\\'s improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\\net al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\\ncan also segment objects according to the given one-shot prompt data. As shown, the training-free\\nPerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\\nBy the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\\n+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\\ngeneralists, our method is specially designed for personalized object segmentation, and exhibits much\\nmore efficiency in both time and computational resources.\\n4.2 EXISTING SEGMENTATION BENCHMARKS\\nVideo Object Segmentation. Given the first-frame image and object masks, our PerSAM and\\nPerSAM-F achieve competitive object segmentation and tracking performance on the validation\\nset of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\\nvideo training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\\nPerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\\napproach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\\nvideo data. The results fully illustrate our strong generalization ability for temporal video data and\\ncomplex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\\nOne-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\\nimage segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\\n2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\\nfollow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\\nattains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\\net al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\\nHSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\\nsegmentation, but also works for category-wise and part-wise personalization of SAM.\\n4.3\\nPERSAM-ASSISTED DREAMBOOTH\\nWe follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\\nDiffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\\nvisualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\\nsofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\\nAssisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\\ncorresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\\nthe background disturbance to correctly generate the “forest” and “blue sky\".\\n9\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nA photo of a dog\\n\"A [V] dog in a jungle\"\\n...\\nA photo of a barn\\n\"A [V] barn with a forest in the background\"\\n\"A [V] dog in snow\"\\n\"A [V] barn with blue sky in the background\"\\nFigure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\\n2022) can better preserve the diversity for synthesizing various contexts in new images.\\nTable 4: Ablation of Main Com-\\nponents in our proposed method.\\nVariant\\nTable 5: Ablation of Different\\nFine-tuning Methods.\\nTable 6: Ablation of using\\nBox-image as Reference.\\nmIoU Gain\\n69.1\\nMethod\\nPerSAM\\nParam. mIoU\\nMethod\\nMask Box\\n0\\n89.32\\nPainter\\n56.4 42.0\\n+ Post-refinement\\n72.5 +3.4\\n83.9 +11.4\\nPrompt Tuning\\n12K\\n76.5\\nVP\\n65.9\\n38.1\\nAdapter\\n196K\\n78.3\\nSEEM\\n87.1\\n64.9\\n+ Guided Attention\\n+ Semantic Prompt\\n85.8 +1.9\\nLORA\\n293K\\n90.0\\nSegGPT\\n94.3 36.0\\n89.3\\n+3.5\\n3 Mask Weights\\n3\\n92.9\\n+ Scale Tuning\\n95.3\\n+6.0\\nPerSAM-F\\n2\\n95.3\\nPerSAM 89.3\\nPerSAM-F 95.3\\n88.1\\n94.9\\nPositive Prior\\n+ Negative Prior\\n4.4\\nABLATION STUDY\\nMain Components. In Table 4, we investigate our different components by starting from a baseline\\nthat only adopts the positive location prior. Then, we add the negative point prompt and cascaded\\npost-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\\nhigh-level target semantics into SAM\\'s decoder for attention guidance and semantic prompting. The\\nresulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\\nscale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\\nDifferent Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\\n(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\\nand LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\\ninto every transformer block in PerSAM\\'s decoder. As shown, the prompt tuning and Adapter would\\nover-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\\nbest improve the performance of PerSAM, while tuning the least learnable parameters.\\nUsing Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\\nsome users. In Table 6, we relax the input restrictions to a bounding box designating the expected\\nobject. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\\nthe one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\\nPerSAM and PerSAM-F, but severely influences other methods.\\n5 CONCLUSION\\nIn this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\\nwith only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\\ninto SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\\nPerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\\nscales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\\nof our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\\nour work may expand the applicability of SAM to a wider: range of scenarios.\\n10\\n10\\nREFERENCES\\nVijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\\ndecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 39(12):2481–2495, 2017.\\nAmir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\\nvia image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\nJiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\\nSegment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\\nconnected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\\n2017.\\nXi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\\nsegmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n7345-7354, 2021.\\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\\ntransformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\\nBowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\\niou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\\nattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\\nPedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\\nhugging face.co/blog/lora, January 2023.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\\ninversion. arXiv preprint arXiv:2208.01618, 2022.\\nAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\\nsegmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npp. 5356-5364, 2019.\\nYuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\\nZhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\\nedge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n1551-1560, 2021.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\\na unified view of parameter-efficient transfer learning. In International Conference on Learning\\nRepresentations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\\nLukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\\narXiv preprint arXiv:2211.10155, 2022.\\n11\\nSunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\\nwith 4d convolutional swin transformer for few-shot segmentation. In European Conference on\\nComputer Vision, pp. 108–126. Springer, 2022.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\\nnlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\\narXiv:2106.09685, 2021.\\nYuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\\nJiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\\narXiv:2304.14660, 2023.\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\\nnoisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\\n2021.\\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\\nSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\\nSpringer, 2022.\\nZhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\\ntotypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\\non Computer Vision, pp. 36–54. Springer, 2022.\\nZhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\\nChengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\\nsegmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\\n2023.\\nLei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\\nSegment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\\nAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\\ntation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n9404-9413, 2019.\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\\narXiv:2304.02643, 2023.\\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\\ncustomization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691, 2021.\\nHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\\nXiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\\nand vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\\n2023.\\nXiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\\ndataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2869-2878, 2020.\\n12\\nYanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\\nAttention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\\nYongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\\nbank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\\n3430-3441, 2020.\\nHuaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\\nsegmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n3949-3957, 2019.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\\nZiyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\\nYu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\\nConference on Computer Vision, pp. 388-404. Springer, 2022.\\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\\nP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\\narXiv preprint arXiv:2110.07602, 2021.\\nYang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\\nanything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\\n2023.\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npp. 3431-3440, 2015.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\\nrepresentations for vision-and-language tasks. In Advances in Neural Information Processing\\nSystems (NeurIPS), pp. 13–23, 2019.\\nJun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\\nJuhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\\nProceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\\nKeval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\\nsemantic part. arXiv preprint arXiv:2007.02419, 2020.\\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\\n2020.\\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\\nLuc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\\narXiv:1704.00675, 2017.\\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\\narXiv preprint arXiv:2104.06599, 2021.\\nAlec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\\n2018.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n13\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning, pp.\\n8748-8763. PMLR, 2021.\\nVignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\\nobjects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n7141-7151, 2023.\\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. Advances in Neural information processing systems, 30, 2017.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\\npreprint arXiv:2208.12242, 2022.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. Advances in Neural Information\\nProcessing Systems, 35:36479-36494, 2022.\\nLin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\\nNanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\\nInformation Processing Systems, 33:3991-4002, 2020.\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\\nvision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pp. 5227–5237, 2022.\\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\\nEuropean Conference on Computer Vision, pp. 282–298. Springer, 2020.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\\nmodels. arXiv preprint arXiv:2302.13971, 2023.\\nXinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\\ninstance segmentation. Advances in Neural information processing systems, 33:17721–17732,\\n2020.\\nXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\\nXinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\\nSegmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\\nSimple and efficient design for semantic segmentation with transformers. Advances in Neural\\nInformation Processing Systems, 34:12077-12090, 2021.\\nMutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\\ngeometry-disentangled representation for complementary understanding of 3d object point cloud.\\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\\nSegment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\\n14\\nChaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\\nChoong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\\narXiv preprint arXiv:2306.14289, 2023a.\\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\\narXiv:2303.10512, 2023b.\\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\\narXiv preprint arXiv:2303.16199, 2023c.\\nRenrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\\nPeng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\\nlearners. arXiv preprint arXiv:2303.02151, 2023d.\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\\n2881-2890, 2017.\\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\\nFast segment anything. arXiv preprint arXiv:2306.12156, 2023.\\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\\nsequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6881–6890, 2021.\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\\nlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\\neverything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\\n15\\n115\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_language(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(target: str, text: str) -> dict:\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    print(\"Text: {}\".format(result[\"input\"]))\n",
    "    print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n",
      "Translation: arXiv:2305.03048v2 [cs.CV] 2023년 10월 4일 원샷으로 세그먼트 무엇이든 모델을 개인화 Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 상하이 인공지능 연구실 3 중국과학원 자동화 연구소 4페킹대학교 컴퓨터과학대학 CFCS {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk 초록 대용량 데이터 사전 학습을 통해 구동되는 세그먼트 무엇이든 모델(SAM)은 강력한 프롬프트 가능한 프레임워크는 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 인력 프롬프트 없이 특정 시각적 개념에 대한 SAM을 사용자 정의하는 것은 충분히 탐구되지 않았습니다. 예를 들어, 수많은 이미지에서 반려견을 자동으로 세분화합니다. 이 논문에서는 SAM에 대한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 참조 마스크가 있는 단일 이미지인 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양수-음수 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 두 가지 제안된 기술인 대상 유도 주의와 대상 의미 프롬프트를 통해 SAM이 개인화된 객체 세분화를 수행할 수 있도록 합니다. 이런 식으로 훈련 없이도 범용 SAM을 개인용으로 효과적으로 사용자 정의할 수 있습니다. 세분화 규모의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. SAM 전체를 동결하여 다중 스케일 마스크를 집계하기 위한 스케일 인식 미세 조정을 도입합니다.이는 10초 이내에 2개의 매개변수만 조정하여 성능을 개선합니다.효능을 입증하기 위해 개인화된 객체 분할을 평가하기 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다.또한 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 개선하기 위해 PerSAM을 활용할 것을 제안합니다.훈련 세트 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 나은 대상 모양 생성과 입력 텍스트 프롬프트에 대한 더 높은 충실도를 보여줍니다.코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.(1) 사용자가 제공합니다.(2) 원샷 학습 (3) 개인화된 분할 하나의 이미지 하나의 마스크 훈련 없는 미세 조정 PerSAM PerSAM-F ... 다양한 포즈나 장면에서 그림 1: 세그먼트 무엇이든 모델의 개인화. 영어: 우리는 특정 시각적 개념(예: 반려견)에 대해 Segment Anything Model(SAM)(Kirillov et al., 2023)을 사용자 정의합니다.단일 샷 데이터로 두 가지 효율적인 솔루션을 소개합니다.훈련이 필요 없는 PerSAM과 미세 조정 PerSAM-F입니다.* 동등한 기여.† 책임 저자.1 2 매개변수 PerSAM PerSAM-F 사용자 제공 DreamBooth PerSAM 지원 미세 조정 10초 테디베어의 모자 [V] 고양이 사진 &quot;[V] 고양이가 있는 해변&quot; ... 다양한 포즈나 장면 미세 조정 로봇 장난감의 몸체 *** 그림 2: 개인화된 세분화 예.PerSAM(왼쪽)은 유리한 성능으로 모든 맥락에서 개인 사물을 세분화할 수 있으며, PerSAM-F(오른쪽)는 규모 인식 미세 조정을 통해 모호성 문제를 더욱 완화합니다. 1 서론 [V] 백팩 사진 &quot;교실 테이블 위의 [V] 백팩&quot; 그림 3: PerSAM을 이용한 DreamBooth 개선(Ruiz et al., 2022). 훈련 중 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 높은 품질의 개인화된 텍스트-이미지 생성을 달성하는 데 도움이 될 수 있습니다. 시각(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019) 및 다중 모달리티(Radford et al., 2021; Jia et al., 2021; Li et al., 2023)의 기초 모델은 대규모 데이터 세트와 계산 리소스의 가용성에 기인하여 전례 없는 보급을 얻었습니다. 그들은 제로샷 시나리오에서 놀라운 일반화 능력을 보여주고, 인간의 피드백을 통합한 다재다능한 상호 작용을 보여줍니다. 여기에서 영감을 얻은 Segment Anything(Kirillov et al., 2023)은 1,100만 개의 이미지 마스크 데이터를 수집하기 위한 섬세한 데이터 엔진을 개발하고, 이후 SAM이라고 알려진 세분화 기반 모델을 훈련합니다. 그것은 새로운 프롬프트 가능 세분화 프레임워크를 정의합니다. 즉, 수작업 프롬프트를 입력으로 받고 예상 마스크를 반환하여 시각적 맥락에서 모든 객체를 세분화할 수 있습니다. 그러나 SAM은 본질적으로 특정 시각적 개념을 세분화하는 기능을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 사라진 시계를 찾으려고 한다고 상상해보세요. 바닐라 SAM을 사용하면 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에서 복잡한 맥락에서 대상 객체를 정확하게 찾은 다음 세분화를 위한 적절한 프롬프트로 SAM을 활성화해야 합니다. 이를 고려하여 다음과 같은 질문이 생깁니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 세분화하도록 SAM을 개인화할 수 있을까요? 이를 위해 Segment Anything Model을 위한 훈련이 필요 없는 개인화 접근 방식인 PerSAM을 소개합니다.그림 1에서 볼 수 있듯이, 저희의 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크인 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 정의합니다.특히, 저희는 먼저 모든 전경 픽셀의 모양을 고려하는 특징 유사성에 의해 테스트 이미지의 대상 객체에 대한 위치 신뢰도 맵을 얻습니다.신뢰도 점수에 따라 두 지점이 양수-음수 위치 사전으로 선택되고, 이는 최종적으로 프롬프트 토큰으로 인코딩되어 세분화를 위해 SAM의 디코더에 입력됩니다.디코더 내에서 저희는 두 가지 기술을 사용하여 대상 객체의 시각적 의미론을 주입하여 SAM의 개인화된 세분화 능력을 최대한 발휘하도록 제안합니다.• • 대상 유도 주의. 저희는 위치 신뢰도 맵에 따라 SAM의 디코더에서 모든 토큰-이미지 교차 주의 계층을 안내합니다.이는 프롬프트 토큰이 집중적인 특징 집계를 위해 주로 전경 대상 영역에 집중하도록 명시적으로 강제합니다.대상 의미적 프롬프팅. SAM에 고수준 대상 의미론을 명시적으로 제공하기 위해 원래 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 저수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다. 앞서 언급한 디자인과 계단식 사후 정제를 통해 PerSAM은 다양한 포즈나 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히, 우리의 접근 방식은 여러 유사한 객체 중에서 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오에 잘 대처할 수 있습니다. 그럼에도 불구하고 그림 2에서 볼 수 있듯이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층 구조로 구성된 경우(예: 테디베어 위의 모자 또는 로봇 장난감의 머리) 가끔 실패 사례가 있을 수 있습니다. 이러한 모호성은 PerSAM이 출력으로 적절한 마스크 크기를 결정하는 데 어려움을 줍니다. 로컬 부분과 글로벌 모양이 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문입니다. 이 문제를 완화하기 위해, 우리는 PerSAM-F라는 우리 접근 방식의 미세 조정 변형을 추가로 제안합니다. 우리는 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 동결하고, 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 말하면, 우리는 SAM이 다양한 마스크 스케일의 여러 가지 잠재적인 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 가장 적합한 스케일을 적응적으로 선택하기 위해, 우리는 각 마스크 스케일에 대해 학습 가능한 상대적 가중치를 사용하고, 최종 출력으로 가중 합산을 수행합니다. 이러한 효율적인 스케일 인식 훈련을 통해, PerSAM-F는 원샷 데이터에서 과적합을 피하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다. 또한, 우리의 접근 방식이 DreamBooth(Ruiz et al., 2022)가 그림 3에서 보듯이 개인화된 텍스트-이미지 생성을 위한 확산 모델을 보다 잘 미세 조정할 수 있도록 도울 수 있다는 것을 관찰했습니다. 애완 고양이 또는 백팩과 같은 특정 시각적 개념이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 이러한 이미지를 단어 임베딩 공간의 식별자[V]로 변환하는 방법을 학습하지만 동시에 계단이나 숲과 같은 배경 정보를 포함할 수 있습니다. 이렇게 하면 새로 프롬프트된 배경이 무시되고 대상 모양 생성이 방해받습니다. 따라서 우리는 PerSAM을 활용하여 훈련 이미지 내에서 대상 개체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하는 것을 제안합니다. 우리는 논문의 기여를 다음과 같이 요약합니다. • 개인화된 개체 분할. 우리는 먼저 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오로 사용자 지정하는 방법을 조사합니다. 이를 위해 개인화된 객체 분할을 평가하기 위한 새로운 분할 데이터 세트인 PerSeg와 함께 효율적이고 효과적인 두 가지 방법을 소개합니다.• PerSAM 및 PerSAM-F. PerSAM에서 대상 객체의 고수준 의미론에 따라 SAM을 안내하는 세 가지 무훈련 기술을 제안합니다. PerSAM-F에서 마스크 모호성 문제를 크게 완화하기 위해 10초 동안 2개의 매개변수를 사용하여 스케일 인식 미세 조정을 설계합니다.• 우리의 접근 방식은 PerSeg 벤치마크, 원샷 파트 및 의미 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 얻습니다.또한 PerSAM은 더 나은 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 향상시킬 수 있습니다.2 관련 작업 기초 모델. 강력한 일반화 용량을 통해 사전 훈련된 기초 모델을 다양한 다운스트림 시나리오에 맞게 조정하고 유망한 성능을 얻을 수 있습니다. 자연어 처리에서 BERT(Devlin 등, 2018; Lu 등, 2019), GPT 시리즈(Brown 등, 2020; OpenAI, 2023; Radford &amp; Narasimhan, 2018; Radford 등, 2019), LLAMA(Zhang 등, 2023c)는 놀라운 맥락 내 학습 능력을 보여주었으며, 도메인별 프롬프트를 통해 새로운 작업으로 전환될 수 있습니다. 마찬가지로 이미지-텍스트 쌍에 대한 대조 학습을 수행하는 CLIP(Radford 등, 2021)과 ALIGN(Jia 등, 2021)은 제로샷 시각 인식에서 뛰어난 정확도를 보여줍니다. Painter(Wang 등, 2022)는 다운스트림 미세 조정 없이 다양한 비전 작업을 수행하기 위해 네트워크 아키텍처와 맥락 내 프롬프트를 통합하는 비전 모델을 도입합니다. CaFo(Zhang et al., 2023d)는 다양한 기초 모델을 캐스케이드하고 사전 훈련된 지식을 협력하여 견고한 저데이터 이미지 분류를 수행합니다. SAM(Kirillov et al., 2023)은 10억 개의 마스크로 사전 훈련된 이미지 분할을 위한 기초 모델을 제시하고 프롬프트 기반 분할을 수행합니다. 고품질 분할(Ke et al., 2023), 더 빠른 추론 속도(Zhao et al., 2023; Zhang et al., 2023a), 모든 용도 매칭(Liu et al., 2023), 3D 재구성(Cen et al., 2023), 객체 추적(Yang et al., 2023), 의료(Ma &amp; Wang, 2023; Huang et al., 2023) 이미지 처리를 위해 SAM을 확장하는 몇 가지 동시 작업이 있습니다. 다른 관점에서, 우리는 특정 시각적 개념에 대한 세분화 기반 모델, 즉 SAM을 개인화하여 일반인을 단 한 번의 샷으로 전문가로 적응시키는 것을 제안합니다.우리의 방법은 또한 텍스트-이미지 기반 모델, 즉 Stable Diffusion(Rombach et al., 2022) 및 Imagen(Saharia et al., 2022)의 개인화를 지원할 수 있으며, 이는 전경 대상 객체를 배경 교란으로부터 세분화하여 생성 품질을 개선합니다.세그먼테이션의 3가지 대형 모델.컴퓨터 비전의 기본 작업인 세분화(Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)에는 이미지에 대한 픽셀 수준의 이해가 필요합니다. 다양한 분할 관련 작업이 탐색되었는데, 여기에는 각 픽셀을 사전 정의된 클래스 집합으로 분류하는 의미적 분할(Badrinarayanan 등, 2017; Chen 등, 2017; Zheng 등, 2021; Cheng 등, 2022; Xie 등, 2021; Song 등, 2020)이나 개별 객체 인스턴스의 식별에 초점을 맞춘 인스턴스 분할(He 등, 2017; Wang 등, 2020; Tian 등, 2020), 클래스 레이블과 인스턴스 식별을 모두 할당하는 전방위 분할(Kirillov 등, 2019; Li 등, 2019), 그리고 세분화를 위해 인간의 개입을 포함하는 대화형 분할(Hao 등, 2021; Chen 등, 2021) 등이 있습니다. 최근 언어 기반 모델(Zhang et al., 2023c; Brown et al., 2020)에서 영감을 받아 여러 동시 연구에서 이미지 분할을 위한 대규모 비전 모델이 제안되었습니다. 이 모델은 광범위한 마스크 데이터로 사전 학습되었으며 다양한 이미지 분포에 대해 강력한 일반화 기능을 보여줍니다. Segment Anything Model(SAM)(Kirillov et al., 2023)은 모델-인-더-루프 주석이 있는 데이터 엔진을 활용하여 프롬프트 가능 분할 프레임워크를 학습하는데, 이는 제로샷 방식으로 다운스트림 시나리오로 일반화됩니다. Painter(Wang et al., 2022)와 SegGPT(Wang et al., 2023)는 강력한 컨텍스트 내 학습 패러다임을 도입하고 주어진 이미지 마스크 프롬프트에 따라 모든 이미지를 분할할 수 있습니다. SEEM(Zou et al., 2023)은 언어 및 오디오와 같은 다중 모달 참조에 의해 촉발된 일반적인 분할 모델을 추가로 제시하여 다양한 의미 지식을 통합합니다. 이 연구에서는 개인화된 객체 분할이라는 새로운 작업을 도입하고 평가를 위해 새로운 데이터 세트 PerSeg에 주석을 달았습니다. 대규모 분할 모델을 개발하는 대신, 우리의 목표는 사용자가 제공한 객체를 모든 포즈나 장면에서 분할하도록 모델을 개인화하는 것입니다. 개인화된 분할을 위해 SAM을 효율적으로 사용자 지정하는 PerSAM과 PerSAM-F의 두 가지 접근 방식을 제안합니다. 매개변수 효율적인 미세 조정. 다운스트림 작업에서 전체 기초 모델을 직접 조정하는 것은 컴퓨팅 비용이 많이 들고 메모리 집약적일 수 있으며, 리소스가 제한된 애플리케이션에 어려움을 줄 수 있습니다. 이 문제를 해결하기 위해 최근 연구에서는 기초 모델의 가중치를 동결하고 미세 조정을 위한 소규모 모듈을 추가하는 매개변수 효율적 방법(Sung 등, 2022; He 등, 2022; Rebuffi 등, 2017; Qin &amp; Eisner, 2021)을 개발하는 데 중점을 두었습니다. 신속한 조정(Lester 등, 2021; Zhou 등, 2022; Jia 등, 2022; Liu 등, 2021)은 동결된 모델과 함께 학습 가능한 소프트 프롬프트를 사용하여 특정 다운스트림 작업을 수행하여 전체 모델 조정에 비해 규모와 강력한 도메인 전송으로 더 경쟁력 있는 성능을 달성하는 것을 제안합니다. 저랭크 적응(LORA)(Hu 등, 2021; Cuenca &amp; Paul, 2023; Zhang 등, 2023b; Hedegaard 등, 2022)은 학습 가능한 랭크 분해 행렬을 각 사전 학습된 가중치에 동시에 주입하여 다운스트림 작업에 필요한 학습 가능한 매개변수 수를 크게 줄입니다. 어댑터(Houlsby 등, 2019; Pfeiffer 등, 2020; Lin 등, 2020; Chen 등, 2022)는 원래 변환기의 레이어 사이에 삽입되도록 설계되어 특징 변환을 위한 경량 MLP를 도입합니다. 기존 작업과 달리 SAM을 위해 섬세하게 설계된 보다 효율적인 적응 방법, 즉 2개의 매개변수와 10초만 있는 PerSAM-F의 스케일 인식 미세 조정을 채택합니다. 이를 통해 원샷 데이터에서 과적합 문제를 효과적으로 방지하고 우수한 성능으로 세분화 규모의 모호성을 완화합니다.3 방법 3.1절에서 먼저 Segment Anything Model(SAM)(Kirillov et al., 2023)을 간략하게 다시 살펴보고 개인화된 객체 세분화를 위한 작업 정의를 소개합니다.그런 다음 각각 3.2절과 3.3절에서 PerSAM과 PerSAM-F의 방법론을 설명합니다.마지막으로 3.4절에서 DreamBooth(Ruiz et al., 2022)가 더 나은 텍스트-이미지 생성을 지원하도록 접근 방식을 활용합니다.3.1 개인화된 객체 세분화 Segment Anything 다시 살펴보기.SAM은 프롬프트 인코더, 이미지 인코더, 경량 마스크 디코더의 세 가지 구성 요소로 구성되며 각각 Encp, Enc, Decм로 표시합니다. 프롬프트 가능 프레임워크로서, SAM은 이미지 I와 프롬프트 집합 P(점, 상자 또는 거친 마스크)를 입력으로 받습니다. 구체적으로 SAM은 먼저 Enc를 사용하여 입력 이미지 특징을 얻고 Encp를 채택하여 길이가 k인 인간이 제공한 프롬프트를 FI = Enc(I), Tp = Encp(P)와 같이 프롬프트 토큰으로 인코딩합니다.(1) 4 F₁ 코사인 유사도 인코딩 {FT}=1 테스트 이미지 I 대상 로컬 특징 {T}=1 FR MR° FR PerSAM의 디코더 인코딩 대상 가이드 주의 이미지-토큰 교차 주의 대상 의미적 프롬프트 ↑ {Si)=1 로컬 신뢰 맵 토큰-이미지 교차 주의 변조 ↑ 집계 주의 행렬 A 로컬 특징 {T}=1 토큰 자체 주의 전체 신뢰 맵 S 집계 ↑ a Concat( + Repeat( ) 전체 신뢰 맵 S TM Тр × 2 TR 원샷 이미지 IR 원샷 마스크 MR 양의 사전 음의 사전 그림 4: 영어: Positive-negative Location Prior. 모든 로컬 파트의 출현에 따라 새로운 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 계산합니다. 그런 다음 PerSAM에 대한 포인트 프롬프트로 위치 사전을 선택합니다. 저수준 위치 프롬프트 고수준 의미 프롬프트 그림 5: 대상 안내 주의(왼쪽) 및 대상 의미 프롬프트(오른쪽). 대상 의미론을 SAM에 주입하기 위해 교차 주의 계층을 명시적으로 안내하고 고수준 단서를 사용하여 추가 프롬프트를 제안합니다. 여기서 F₁ = Rhxwxc 및 Tp Є Rkxc, h, w는 이미지 피처 맵의 해상도를 나타내고 c는 피처 차원을 나타냅니다. 그 후 인코딩된 이미지와 프롬프트가 주의 기반 피처 상호 작용을 위해 디코더 Decм에 입력됩니다. SAM은 프롬프트 토큰 Tp에 접두사로 여러 학습 가능한 마스크 토큰 Tм을 연결하여 디코더의 입력 토큰을 구성합니다. 이러한 마스크 토큰은 M = Decм(FI, Concat(TM,Tp))로 공식화된 마스크 출력을 생성하는 역할을 하며, 여기서 M은 SAM에서 예측한 최종 분할 마스크를 나타냅니다. (2) 작업 정의. SAM은 프롬프트를 통해 모든 객체에 대해 충분히 일반화되지만 특정 주체 인스턴스를 자동으로 분할하는 기능이 부족합니다. 이를 고려하여 개인화된 객체 분할을 위한 새로운 작업을 정의합니다. 사용자는 단일 참조 이미지와 대상 시각적 개념을 나타내는 마스크만 제공합니다. 제공된 마스크는 정확한 분할이거나 즉석에서 그린 대략적인 스케치일 수 있습니다. 우리의 목표는 추가적인 인간의 프롬프트 없이 새 이미지나 비디오 내에서 지정된 객체를 분할하도록 SAM을 사용자 지정하는 것입니다. 평가를 위해 PerSeg라는 개인화된 분할을 위한 새로운 데이터 세트에 주석을 달았습니다. 원시 이미지는 다양한 포즈나 장면에서 다양한 범주의 시각적 개념을 포함하는 주체 중심 확산 모델(Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022)에 대한 작업에서 수집되었습니다. 이 논문에서 우리는 이 과제를 위한 두 가지 효율적인 솔루션을 제안하며, 구체적으로 다음과 같이 설명합니다. 3.2 학습 없는 PERSAM 위치 신뢰도 맵. 사용자가 제공한 이미지 IR과 마스크 MR을 조건으로, PerSAM은 먼저 새로운 테스트 이미지 I에서 대상 객체의 위치를 나타내는 신뢰도 맵을 얻습니다. 그림 4에서 볼 수 있듯이, 우리는 이미지 인코더를 적용하여 IR과 I의 시각적 특징을 추출합니다. 인코더는 SAM의 동결 백본 또는 다른 사전 학습된 비전 모델이 될 수 있으며, 우리는 기본적으로 SAM의 이미지 인코더 Enc를 채택합니다. 우리는 프로세스를 FI = Enc(I), FR = EncI(IR), (3)으로 공식화합니다. 여기서 F1, FR Є R¹xwxc입니다. 그런 다음 참조 마스크 MR Є Rhxwx¹를 사용하여 FR의 시각적 개념 내에서 전경 픽셀의 특징을 잘라내어 {T}}=1 = MR ○ Fr, (4) 5와 같이 n개의 로컬 특징 세트를 생성합니다. 여기서 T½ Є R¹×ª이고 ○는 공간적 곱셈을 나타냅니다. 그런 다음 T½와 테스트 이미지 특징 FĮ 사이의 코사인 유사도에 의해 각 전경 픽셀 i에 대한 n개의 신뢰도 맵을 계산합니다. {S² } }±₁ = {F₁TT) 11, 여기서 S₁ = Rhxw. (5) FI와 T½는 픽셀별로 L2-정규화되었습니다. 각 S²는 테스트 이미지에서 개체의 다른 로컬 부분(예: 개의 머리, 몸통 또는 발)에 대한 분포 확률을 나타냅니다. 이에 더하여, 우리는 모든 n개의 로컬 맵을 집계하여 S = n n SiЄRhxw (6)와 같이 대상 객체의 전반적인 신뢰도 맵을 얻기 위해 평균 풀링을 채택합니다.모든 전경 픽셀의 신뢰도를 통합함으로써, S는 다른 객체 부분의 시각적 모양을 고려하고 비교적 포괄적인 위치 추정을 얻을 수 있습니다.양의-음의 위치 사전.PerSAM에 테스트 이미지의 위치 사전을 제공하기 위해, 우리는 S에서 가장 높고 가장 낮은 신뢰도 값을 갖는 두 지점을 선택합니다.각각 Pɲ와 Pɩ로 표시합니다.전자는 대상 객체의 가장 가능성 있는 중심 위치를 나타내고, 후자는 반대로 배경을 나타냅니다.그런 다음, 이들은 양의 및 음의 지점 프롬프트로 간주되어 Tp = Encp(Ph, P₁) Є R2×c로 프롬프트 인코더에 입력됩니다.(7) 이는 SAM 디코더의 프롬프트 토큰을 나타냅니다. 이런 식으로 SAM은 이미지에서 음의 점을 버리는 반면, 양의 점을 둘러싼 연속적인 영역을 분할하려는 경향이 있습니다.대상 유도 주의.양의-음의 점 프롬프트가 얻어졌지만, 우리는 SAM 디코더에서 교차 주의 연산에 대한 보다 명확한 의미적 지침을 제안합니다.이는 전경 대상 영역 내에서 피처 집계를 집중시킵니다.그림 5에서 볼 수 있듯이, 방정식 6의 전체 신뢰도 맵 S는 테스트 이미지에서 대상 시각적 개념의 거친 영역을 명확하게 나타낼 수 있습니다(더 밝은 색상은 더 높은 점수를 나타냄).이러한 속성을 기반으로, 우리는 S를 사용하여 디코더의 모든 토큰-이미지 교차 주의 계층에서 주의 맵을 안내합니다.특히, 소프트맥스 함수 뒤의 모든 주의 맵을 A = Rhxw로 표시한 다음, 주의 분포를 A9 = softmax A+ a softmax(S) • (8)로 변조합니다.여기서 a는 밸런싱 팩터를 나타냅니다. 주의 편향으로 인해 마스크와 프롬프트 토큰은 중요하지 않은 배경 영역이 아닌 대상 주제와 관련된 더 많은 시각적 의미를 포착해야 합니다. 이는 주의 메커니즘에서 더 효과적인 기능 집계에 기여하고, 훈련 없이 PerSAM의 최종 세분화 정확도를 향상시킵니다. 대상 의미적 프롬프트. 바닐라 SAM은 점이나 상자의 좌표와 같은 저수준 위치 정보가 있는 프롬프트만 수신합니다. SAM의 디코더에 더 높은 수준의 단서를 제공하기 위해 대상 개념의 시각적 특징을 추가 높은 수준의 의미적 프롬프트로 활용하는 것을 제안합니다. 먼저 참조 이미지에서 객체의 글로벌 임베딩 TR을 다음과 같이 다른 로컬 피처 간의 I 평균 풀링을 통해 얻습니다.n TR = ΣΤΑ €R1xc n i=1 그런 다음 방정식 2에서 테스트 이미지의 모든 입력 토큰에 TR을 요소별로 추가한 다음 디코더 블록에 공급합니다.그림 5에 T9 = Repeat (TR) + Concat(TM,Tp), (10)으로 표시됩니다.여기서 T9는 디코더 Decм에 대한 대상 의미론에 의해 안내되는 입력 토큰을 나타내고 Repeat 작업은 대상 시각적 임베딩을 복제합니다.간단한 토큰 통합의 도움으로 PerSAM은 저수준 위치 지점뿐만 아니라 고수준 대상 시각적 단서에 의해서도 촉발됩니다. 6 테스트 이미지 PerSAM 출력 3개 M1 M2 + M3 스케일 임의 노이즈 →&gt; DreamBooth →&gt; ↑ &quot;a [V] cat&quot; 재구성 손실 W1 F W2 1- W1 + W₂ ) 1 사용자가 출력 마스크 M 제공 미세 조정 가중 합산 동결 PerSAM 배경 교란 분리 그림 6: PerSAM-F의 스케일 인식 미세 조정. 스케일 모호성을 완화하기 위해 PerSAM-F는 3개 스케일 마스크를 적응적으로 집계하기 위한 두 개의 학습 가능한 가중치를 채택합니다. 그림 7: PerSAM 지원 DreamBooth. DreamBooth 생성을 개선하기 위해 PerSAM을 사용하여 대상 객체를 배경에서 분리합니다. 계단식 사후 미세 조정. 위의 기술을 통해 SAM 디코더에서 테스트 이미지에 대한 초기 분할 마스크를 얻지만 거친 모서리와 고립된 배경 노이즈가 포함될 수 있습니다. 추가 세분화를 위해, 우리는 두 단계 후처리를 위해 마스크를 디코더 Decм에 반복적으로 공급합니다.첫 번째 단계에서 우리는 이전의 양수-음수 포인트 프롬프트와 함께 현재 예측된 마스크로 디코더에 프롬프트를 보냅니다.두 번째 단계에서 우리는 첫 번째 단계에서 마스크를 둘러싼 경계 상자를 획득하고, 더 정확한 객체 위치 파악을 위해 이 상자로 디코더에 추가로 프롬프트를 보냅니다.우리는 대규모 이미지 인코더 없이 가벼운 디코더만 반복하기 때문에 후처리가 효율적이고 단지 2%의 추가 지연 시간이 발생합니다.3.3 PERSAM-F 세분화 스케일의 모호성의 미세 조정.훈련이 필요 없는 PerSAM은 대부분의 경우를 만족스러운 세분화 정확도로 처리할 수 있습니다.그러나 일부 대상 객체에는 계층적 구조가 포함되어 있어 마스크 스케일의 모호성이 발생합니다.그림 6에서 볼 수 있듯이 플랫폼 위의 찻주전자는 뚜껑과 몸체의 두 부분으로 구성되어 있습니다. 양의 점 프롬프트(녹색 오각별 표시)가 몸체에 위치하고, 음의 프롬프트(빨간색 오각별 표시)가 비슷한 색상의 플랫폼을 제외하지 않으면 PerSAM은 분할에 대해 오도될 것입니다. 이러한 문제는 SAM(Kirillov et al., 2023)에서도 논의되며, 여기서는 객체의 전체, 부분 및 하위 부분에 해당하는 세 가지 크기의 여러 마스크를 동시에 생성하는 대안을 제안합니다. 그런 다음 사용자는 세 가지 마스크 중 하나를 수동으로 선택해야 하는데, 이는 효과적이지만 추가 인력을 소모합니다. 반면에 개인화된 작업은 인간의 프롬프트가 필요 없이 자동 객체 분할을 위해 SAM을 사용자 지정하는 것을 목표로 합니다. 이는 매개변수 효율적인 미세 조정을 통해 PerSAM의 크기 인식 버전을 추가로 개발하도록 동기를 부여합니다. 크기 인식 미세 조정. 적절한 크기에서 적응형 분할을 위해 미세 조정 변형인 PerSAM-F를 도입합니다. 학습이 필요 없는 모델이 하나의 마스크만 생성하는 것과 달리, PerSAM-F는 먼저 PerSAM을 따라 사전 위치를 구하고, SAM의 원래 솔루션을 참조하여 각각 M1, M2, M3으로 표시되는 3개 스케일 마스크를 출력합니다. 여기에 두 개의 학습 가능한 마스크 가중치 w₁, W2를 채택하고 가중치 합산을 통해 최종 마스크 출력을 다음과 같이 계산합니다. M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) 여기서 w1, W2는 모두 1/3으로 초기화됩니다. 최적의 가중치를 학습하기 위해 참조 이미지에서 원샷 미세 조정을 수행하고 주어진 마스크를 기준 진실로 간주합니다. 사전 학습된 지식을 보존하기 위해 전체 SAM 모델을 동결하고 단일 A100 GPU에서 10초 이내에 W1, W2의 두 매개변수만 미세 조정합니다. 이런 방식으로, 우리의 PerSAM-F는 객체의 스케일 인식 의미를 효율적으로 학습하고, 다양한 개념에 대한 최상의 분할 스케일을 적응적으로 출력하여 PerSAM의 일반화 용량을 개선합니다.7 표 1: PerSeg 데이터 세트의 개인화된 객체 분할. 우리는 다양한 방법(Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023)에 대한 전반적인 mIoU, blou 및 학습 가능한 매개변수와 PerSeg의 10개 객체에 대한 mIoU를 비교합니다. ***는 우리와 동시에 진행 중인 작업을 나타냅니다. 방법 mIoU bloU 매개변수. 캔 헛간 시계 고양이 뒤- 테디 오리 얇은 빨간색 팩 곰 장난감 새 만화 로봇 장난감 화가 VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 354M 93.0 33.3 20.9 98.2 65.0 59.2 76.6 66.7 79.8 89.9 67.4 81.0 72.4 72.4 91.1 94.1 95.2 98.0 71.3 97.0 95.8 96.6 63.8 92.6 94.1 94.4 93.7 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 표 2: DAVIS 2017 val(Pont-Tuset et al., 2017)에서의 비디오 객체 분할. 회색은 도메인 내 학습을 포함하는 방법을 나타냅니다. 표 3: FSS-1000(Li et al., 2020), LVIS-92(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 의미론 및 파트 분할. 우리는 mIoU 점수를 보고하고 회색을 사용하여 도메인 내 학습을 포함하는 방법을 나타냅니다. 원샷 의미론 분할 FSS-1000 LVIS-92² 원샷 파트 분할 PASCAL-Part Painter SEEM SegGPT PerSAM 방법 J&amp;F І AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F 방법 PACO-Part HSNet 86.5 17.4 32.4 22.6 부가세 90.3 18.5 33.6 23.5 페인터 61.7 10.5 30.4 14.1 SegGPT 85.6 18.6 66.9 71.3 75.1 PerSAM 81.6 15.6 32.5 22.5 PerSAM-F 76.1 74.9 79.7 PerSAM-F 86.3 18.4 32.9 22.7 3.4 PERSAM 지원 DREAMBOOTH 개인화된 텍스트-이미지 합성을 위해 DreamBooth(Ruiz et al., 2022)는 특정 개체 ig, 애완 고양이의 주어진 3~5개 사진을 사용하여 사전 훈련된 확산 모델을 미세 조정합니다. 텍스트 프롬프트에서 언급된 고양이인 &quot;[V] 고양이&quot;를 생성하는 방법을 학습하고 재구성된 전체 이미지에 대한 손실을 계산합니다. 이를 통해 훈련 이미지의 중복된 배경 정보를 식별자 [V]에 주입합니다. 따라서 그림 7과 같이 DreamBooth에서 배경의 교란을 완화하기 위한 전략을 소개합니다. 몇 장의 이미지에 대한 개체 마스크가 주어지면 PerSAM을 활용하여 모든 전경 대상을 분할하고 배경 영역에 속하는 픽셀에 대한 그래디언트 역전파를 버립니다. 그런 다음, 안정적 확산은 대상 객체의 시각적 I 모양만 기억하도록 미세 조정됩니다. 배경에 감독이 부과되지 않으므로 PerSAM 지원 DreamBooth는 더 나은 시각적 대응으로 대상 객체를 합성할 수 있을 뿐만 아니라 입력 텍스트 프롬프트에 따라 새로운 배경의 다양성을 높일 수도 있습니다.4 실험 먼저 섹션 4.1에서 PerSeg에서 개인화된 세분화를 위한 접근 방식을 평가하고, 섹션 4.2에서 다양한 기존 원샷 세분화 벤치마크를 평가합니다.그런 다음 섹션 4.3에서 PerSAM 지원 DreamBooth의 효과를 설명합니다.마지막으로 섹션 4.4에서 PerSeg에 대한 설계를 조사하기 위해 여러 가지 절제 연구를 수행합니다.4.1 개인화된 평가 PerSeg 데이터 세트.개인화 용량을 테스트하기 위해 PerSeg라는 새로운 세분화 데이터 세트를 구성합니다. 원시 이미지는 주제 중심 확산 작업(Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022)의 훈련 데이터에서 수집되었습니다.PerSeg에는 일용품, 동물, 건물을 포함하여 총 40개의 다양한 범주의 객체가 포함되어 있습니다.다른 포즈나 장면에서 각 객체는 5~7개의 이미지와 마스크와 연관되며, 여기서 하나의 이미지-마스크 쌍을 사용자가 제공한 원샷 데이터로 고정합니다.평가에는 mIoU와 bloU(Cheng et al., 2021)가 채택되었습니다.PerSeg의 구현 세부 사항과 확대된 데이터 스케일은 부록을 참조하세요.8 시계의 반지 2 쟁반 위의 찻주전자 여자가 들고 있는 백팩 캔의 윗부분 그림 8: PerSAM-F의 개선 사항 시각화.우리의 스케일 인식 미세 조정은 PerSAM의 스케일 모호성을 잘 완화할 수 있습니다. 그림 9: 비디오 객체 분할의 시각화. 우리의 접근 방식은 비디오에서 여러 객체를 분할하는 데 좋은 성과를 보입니다. 성능. 표 1에서 미세 조정된 PerSAM-F가 가장 좋은 결과를 얻는 것을 볼 수 있는데, 이는 PerSAM을 전체 mIoU와 bIoU에서 +2.7%, +5.9% 효과적으로 향상시킵니다. 그림 8에서 PerSAM-F의 개선 사항을 더 자세히 시각화합니다. Visual Prompting(VP)(Bar et al., 2022), Painter(Wang et al., 2022), SEEM(Zou et al., 2023), SegGPT(Wang et al., 2023)는 주어진 원샷 프롬프트 데이터에 따라 객체를 분할할 수 있는 컨텍스트 내 학습기입니다. 표시된 대로, 훈련이 필요 없는 PerSAM은 다른 마진으로 Painter, VP, SEEM보다 더 나은 성능을 이미 달성할 수 있습니다. 효율적인 2-매개변수 미세 조정을 통해, 저희의 PerSAM-F는 강력한 SegGPT를 +2.4%, 전반적인 mIoU와 bIoU에서 +4.1% 더 능가합니다.세그먼테이션 일반론자를 개발하려는 그들의 동기와 달리, 저희의 방법은 개인화된 객체 세분화를 위해 특별히 설계되었으며, 시간과 계산 리소스 측면에서 훨씬 더 높은 효율성을 보여줍니다.4.2 기존 세분화 벤치마크 비디오 객체 세분화.첫 번째 프레임 이미지와 객체 마스크를 고려할 때, 저희의 PerSAM과 PerSAM-F는 DAVIS 2017의 검증 세트에서 경쟁력 있는 객체 세분화 및 추적 성능을 달성합니다(Pont-Tuset et al., 2017) 표 2에서 볼 수 있듯이, 비디오 학습이 없는 방법과 비교했을 때, 학습이 없는 PerSAM은 Painter를 +32.3% J&amp;F 점수로 크게 능가하며, 저희의 PerSAM-F는 SegGPT보다 +0.5% 더 나은 성능을 달성할 수 있습니다. 특히, 우리의 원샷 미세 조정 접근 방식은 광범위한 비디오 데이터로 완전히 훈련된 방법(Lin et al., 2019; Liang et al., 2020)보다 성능이 우수할 수 있습니다. 결과는 그림 9에서 시각화된 것처럼 여러 유사하거나 가려진 객체를 포함하는 시간적 비디오 데이터와 복잡한 시나리오에 대한 우리의 강력한 일반화 능력을 충분히 보여줍니다. 원샷 의미론 및 부분 분할. 표 3에서 우리는 각각 4개의 데이터 세트인 FSS-1000(Li et al., 2020), LVIS-92²(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 이미지 분할에 대한 접근 방식을 평가합니다. 여기서 우리는 데이터 전처리 및 평가를 위해 Matcher(Liu et al., 2023)를 따릅니다. 표시된 대로, 우리의 PerSAM-F는 Painter보다 지속적으로 더 나은 결과를 얻었으며 SegGPT와 비슷한 성능을 보였습니다. 도메인 내 학습이 있는 모델(Min et al., 2021; Hong et al., 2022)의 경우, 우리의 접근 방식은 HSNet보다 더 높은 점수를 얻을 수 있습니다. 실험은 우리가 제안하는 접근 방식이 객체 수준 분할에 국한되지 않고 SAM의 범주별 및 부분별 개인화에도 작동한다는 것을 잘 보여줍니다. 4.3 PERSAM 지원 DREAMBOOTH 우리는 DreamBooth(Ruiz et al., 2022)의 모든 하이퍼파라미터를 따라 개인화된 이미지 합성을 위해 사전 학습된 Stable Diffusion(Rombach et al., 2022)을 미세 조정합니다. 그림 3 외에도 그림 10에서 PerSAM 지원 DreamBooth의 더 많은 예를 시각화합니다. 회색 소파에 누워 있는 개의 경우 DreamBooth의 &quot;정글&quot;과 &quot;눈&quot;은 여전히 녹색과 흰색 장식이 있는 소파입니다. PerSAM-F의 도움을 받아 새로 생성된 배경은 소파와 완전히 분리되어 텍스트 프롬프트와 잘 일치합니다. 산 앞에 있는 헛간의 경우, 우리의 접근 방식은 또한 배경 교란을 완화하여 &quot;숲&quot;과 &quot;푸른 하늘&quot;을 올바르게 생성합니다.9 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 개 사진 &quot;정글 속의 [V]개&quot; ... 헛간 사진 &quot;배경에 숲이 있는 [V]개&quot; &quot;눈 속의 [V]개&quot; &quot;배경에 푸른 하늘이 있는 [V]개&quot; 그림 10: PerSAM 기반 DreamBooth의 시각화.개선된 DreamBooth(Ruiz et al., 2022)는 새로운 이미지에서 다양한 맥락을 합성하기 위해 다양성을 더 잘 보존할 수 있습니다.표 4: 제안하는 방법에서 주요 구성 요소의 소거.변형 표 5: 다양한 미세 조정 방법의 소거.표 6: 참조로 상자 이미지를 사용하는 소거.mIoU 이득 69.1 방법 PerSAM 매개변수. mIoU 방법 마스크 상자 0 89.32 페인터 56.4 42.0 + 사후 세부화 72.5 +3.4 83.9 +11.4 프롬프트 튜닝 12K 76.5 VP 65.9 38.1 어댑터 196K 78.3 SEEM 87.1 64.9 + 가이드 어텐션 + 의미적 프롬프트 85.8 +1.9 LORA 293K 90.0 SegGPT 94.3 36.0 89.3 +3.5 3 마스크 가중치 3 92.9 + 스케일 튜닝 95.3 +6.0 PerSAM-F 2 95.3 PerSAM 89.3 PerSAM-F 95.3 88.1 94.9 양의 사전 + 음의 사전 4.4 절제 연구 주요 구성 요소. 표 4에서 우리는 긍정적인 위치 사전만을 채택하는 기준선에서 시작하여 다양한 구성 요소를 조사합니다.그런 다음 부정적인 지점 프롬프트와 계단식 사후 세분화를 추가하여 각각 +3.6%와 +11.4%의 mIoU를 향상시킵니다.그 위에 우리는 주의 유도와 의미 프롬프트를 위해 SAM의 디코더에 고수준 대상 의미론을 도입합니다.그 결과 +1.9%와 +3.5%의 개선은 그 중요성을 충분히 나타냅니다.마지막으로 효율적인 스케일 인식 미세 조정을 통해 PerSAM-F는 점수를 +6.0% 높여 뛰어난 정확도를 보여줍니다.다른 미세 조정 방법.표 5에서 우리는 PerSAM-F에 대한 다른 매개변수 효율적 미세 조정(PEFT) 방법, 즉 프롬프트 튜닝(Liu et al., 2021), 어댑터(Houlsby et al., 2019), LORA(Hu et al., 2021)를 실험합니다. 우리는 SAM 전체를 동결하고 PerSAM 디코더의 모든 변압기 블록에 주입된 PEFT 모듈만 튜닝합니다.보여진 바와 같이, 프롬프트 튜닝과 어댑터는 원샷 데이터를 과대적합시키고 정확도를 심각하게 떨어뜨립니다.대신, 우리의 스케일 인식 미세 튜닝은 가장 학습하기 어려운 매개변수를 튜닝하는 동안 PerSAM의 성능을 가장 잘 향상시킬 수 있습니다.참조로 상자 이미지 사용.원샷 데이터로 정확한 마스크를 요구하는 것은 일부 사용자에게는 너무 엄격할 수 있습니다.표 6에서 예상 객체를 지정하는 경계 상자에 대한 입력 제한을 완화합니다.우리 방법의 경우 상자를 프롬프트로 간주하고 기성품 SAM을 사용하여 원샷 마스크를 생성할 수 있습니다.따라서 상자 참조는 PerSAM 및 PerSAM-F에서 약간의 성능 저하로 이어질 뿐이지만 다른 방법에는 심각한 영향을 미칩니다.5 결론 이 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대한 Segment Anything Model(SAM)을 개인화하는 것을 제안합니다. 첫째, 훈련이 필요 없는 기술로 SAM에 고수준 대상 의미론을 주입하는 PerSAM을 소개합니다. 여기에 더해, 스케일 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 학습 가능한 매개변수가 2개뿐인 PerSAM-F는 마스크 스케일의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 선도적인 성능을 달성합니다. 게다가, DreamBooth가 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 도움이 되는 접근 방식의 효능도 검증합니다. 저희의 작업이 SAM의 적용 범위를 더 넓은 범위의 시나리오로 확장할 수 있기를 바랍니다. 10 10 참고문헌 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. Segnet: 이미지 분할을 위한 딥 합성곱 인코더-디코더 아키텍처. IEEE 패턴 분석 및 머신 인텔리전스 저널, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei Efros. 이미지 인페인팅을 통한 시각적 프롬프트. 신경 정보 처리 시스템의 발전, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, Qi Tian. Nerfs를 사용하여 3D로 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille. Deeplab: 딥 합성곱 신경망, Atrous 합성곱 및 완전 연결 CRF를 사용한 의미적 이미지 분할. IEEE 패턴 분석 및 머신 인텔리전스 저널, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan. 대화형 분할을 위한 조건부 확산. IEEE International Conference on Computer Vision의 진행 과정에서, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao. 밀도 예측을 위한 비전 변환기 어댑터. arXiv 사전 인쇄본 arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov. 경계 iou: 객체 중심 이미지 분할 평가 개선. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 진행 과정에서, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 마스크된 어텐션 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 1290–1299, 2022. Pedro Cuenca 및 Sayak Paul. 효율적인 안정적 확산 미세 조정을 위해 lora 사용. https:// huging face.co/blog/lora, 2023년 1월. Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5356-5364쪽, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: 에지 가이드 플로우로 실용적인 상호 작용 분할 달성. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 1551-1560쪽, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. 매개변수 효율적 전이 학습에 대한 통합된 관점을 향해. International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. Mask r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, Alexandros Iosifidis. 구조화된 가지치기 어댑터. arXiv 사전 인쇄본 arXiv:2211.10155, 2022. 11 Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, Seungryong Kim. few-shot segmentation을 위한 4d 합성곱 swin 변환기를 사용한 비용 집계. European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. NLP를 위한 매개변수 효율적 전이 학습. 국제 기계 학습 컨퍼런스에서, 2790-2799쪽. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen 등. 의료 이미지에 대한 모든 모델을 세분화할 수 있나요?arXiv 사전 인쇄본 arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각적 및 시각 언어 표현 학습 확장. International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 시각적 프롬프트 튜닝. European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai 및 Chengjie Wang. 도메인 적응 의미론적 분할을 위한 프로토타입 대비 적응. 컴퓨터 비전에 관한 유럽 회의, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang 및 Liqing Zhang. Stc: 비디오 인스턴스 분할을 위한 시공간 대조 학습. 컴퓨터 비전 워크숍에 관한 유럽 회의, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang 및 Fisher Yu. 무엇이든 고품질로 분할하세요. arXiv 사전 인쇄본 arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413쪽, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 지정. arXiv 사전 인쇄본 arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. arXiv 사전 인쇄본 arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: 대규모 비전 및 비전-언어 작업을 위한 일반 모델. arXiv 사전 인쇄본 arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang. Fss-1000: few-shot segmentation을 위한 1000-class 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 2869-2878, 2020. 12 Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang. 파노라마 분할을 위한 주의 유도 통합 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, Jim Chen. 적응적 특징 은행과 불확실한 영역 정제를 사용한 비디오 객체 분할. 신경 정보 처리 시스템의 발전, 33: 3430-3441, 2020. 화이지아 린, 샤오주안 치, 지아야 지아. Agss-vos: 주의 유도 단일 샷 비디오 객체 분할. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 3949-3957쪽, 2019. 자오장 린, 안드레아 마도토, 파스칼 펑. 매개변수 효율적 전이 학습을 통해 다재다능한 생성 언어 모델 탐색. arXiv 사전 인쇄본 arXiv:2004.03829, 2020. 린쯔이, 갱시지, 장렌루이, 가오펭, 제라르 드 멜로, 왕샤오강, 다이지펭, 차오위아오, 리홍셩. 동결된 클립 모델은 효율적인 비디오 학습기입니다. European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang. P-tuning v2: 신속한 튜닝은 규모와 작업 전반에 걸쳐 보편적으로 미세 조정하는 것과 비교할 수 있습니다. arXiv 사전 인쇄본 arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen. Matcher: 다목적 기능 매칭을 사용하여 한 번에 모든 것을 분할합니다. arXiv 사전 인쇄본 arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, Trevor Darrell. 의미 분할을 위한 완전 합성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 3431-3440쪽, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee. Vilbert: 시각 및 언어 작업을 위한 작업에 독립적인 시각 언어 표현 사전 학습. 신경 정보 처리 시스템의 발전(NeurIPS), 13-23쪽, 2019. Jun Ma, Bo Wang. 의료 이미지의 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, Minsu Cho. few-shot 분할을 위한 초상관 관계 압축. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 6941-6952쪽, 2021. Keval Morabia, Jatin Arora, Tara Vijaykumar. 객체 및 의미적 부분의 주의 기반 조인트 감지. arXiv 사전 인쇄본 arXiv:2007.02419, 2020. OpenAI. Gpt-4 기술 보고서. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych. 어댑터-융합: 전이 학습을 위한 비파괴적 작업 구성. arXiv 사전 인쇄본 arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv 사전 인쇄본 arXiv:1704.00675, 2017. Guanghui Qin 및 Jason Eisner. 묻는 방법 배우기: 소프트 프롬프트를 혼합하여 lms 쿼리하기. arXiv 사전 인쇄본 arXiv:2104.06599, 2021. Alec Radford 및 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: 공통 객체의 부분과 속성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서, 7141-7151쪽, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 잔여 어댑터를 사용하여 여러 시각적 도메인 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng. 일반 기능 변환을 위한 학습 가능한 트리 필터 재고. 신경 정보 처리 시스템의 발전, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, Mohit Bansal. Vl-adapter: 시각 및 언어 작업을 위한 매개변수 효율적 전이 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5227-5237쪽, 2022. Zhi Tian, Chunhua Shen, Hao Chen. 인스턴스 분할을 위한 조건부 합성곱. 유럽 컴퓨터 비전 컨퍼런스, 282-298쪽. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄 arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li 및 Chunhua Shen. Solov2: 동적이고 빠른 인스턴스 분할. 신경 정보 처리 시스템의 발전, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen 및 Tiejun Huang. 이미지는 이미지로 말한다: 맥락 내 시각적 학습을 위한 일반주의 화가. arXiv 사전 인쇄본 arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. Seggpt: 맥락 내에서 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo. Segformer: 변환기를 사용한 의미적 분할을 위한 간단하고 효율적인 디자인. 신경 정보 처리 시스템의 발전, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, Yu Qiao. 3D 객체 포인트 클라우드의 보완적 이해를 위한 기하학-분리된 표현 학습. AAAI 인공지능 컨퍼런스 회의록, 35권, 3056-3064쪽, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng. 무엇이든 추적: 무엇이든 비디오와 만나는 세그먼트. arXiv 사전 인쇄본 arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, Choong Seon Hong. 무엇이든 더 빠르게 세그먼트화: 모바일 애플리케이션을 위한 가벼운 sam을 향해. arXiv 사전 인쇄본 arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen 및 Tuo Zhao. 매개변수 효율적인 미세 조정을 위한 적응형 예산 할당. arXiv 사전 인쇄 arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao 및 Peng Gao. 프롬프트, 생성, 캐시: 기반 모델의 캐스케이드는 강력한 소수의 학습자를 만듭니다. arXiv 사전 인쇄 arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 및 Jiaya Jia. 피라미드 장면 구문 분석 네트워크. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의 진행, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang 및 Jinqiao Wang. 무엇이든 빠르게 분할하세요. arXiv 사전 인쇄 arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr 등. 트랜스포머를 사용한 시퀀스-투-시퀀스 관점에서 의미적 분할 재고. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6881-6890쪽, 2021. 카이양 저우, 징캉 양, 첸 창 로이, 지웨이 리우. 시각 언어 모델을 위한 프롬프트 학습. 국제 컴퓨터 비전 저널, 130(9):2337-2348, 2022. 쉐얀 저우, 지안웨이 양, 하오 장, 펭 리, 린지에 리, 지안펭 가오, 용재 리. 모든 곳을 한꺼번에 분할. arXiv 사전 인쇄본 arXiv:2304.06718, 2023. 15 115\n",
      "\n",
      "Detected source language: en\n"
     ]
    }
   ],
   "source": [
    "translated_text=translate_text(target='ko', text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translatedText': 'arXiv:2305.03048v2 [cs.CV] 2023년 10월 4일 원샷으로 세그먼트 무엇이든 모델을 개인화 Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 상하이 인공지능 연구실 3 중국과학원 자동화 연구소 4페킹대학교 컴퓨터과학대학 CFCS {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk 초록 대용량 데이터 사전 학습을 통해 구동되는 세그먼트 무엇이든 모델(SAM)은 강력한 프롬프트 가능한 프레임워크는 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 인력 프롬프트 없이 특정 시각적 개념에 대한 SAM을 사용자 정의하는 것은 충분히 탐구되지 않았습니다. 예를 들어, 수많은 이미지에서 반려견을 자동으로 세분화합니다. 이 논문에서는 SAM에 대한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 참조 마스크가 있는 단일 이미지인 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양수-음수 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 두 가지 제안된 기술인 대상 유도 주의와 대상 의미 프롬프트를 통해 SAM이 개인화된 객체 세분화를 수행할 수 있도록 합니다. 이런 식으로 훈련 없이도 범용 SAM을 개인용으로 효과적으로 사용자 정의할 수 있습니다. 세분화 규모의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. SAM 전체를 동결하여 다중 스케일 마스크를 집계하기 위한 스케일 인식 미세 조정을 도입합니다.이는 10초 이내에 2개의 매개변수만 조정하여 성능을 개선합니다.효능을 입증하기 위해 개인화된 객체 분할을 평가하기 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다.또한 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 개선하기 위해 PerSAM을 활용할 것을 제안합니다.훈련 세트 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 나은 대상 모양 생성과 입력 텍스트 프롬프트에 대한 더 높은 충실도를 보여줍니다.코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.(1) 사용자가 제공합니다.(2) 원샷 학습 (3) 개인화된 분할 하나의 이미지 하나의 마스크 훈련 없는 미세 조정 PerSAM PerSAM-F ... 다양한 포즈나 장면에서 그림 1: 세그먼트 무엇이든 모델의 개인화. 영어: 우리는 특정 시각적 개념(예: 반려견)에 대해 Segment Anything Model(SAM)(Kirillov et al., 2023)을 사용자 정의합니다.단일 샷 데이터로 두 가지 효율적인 솔루션을 소개합니다.훈련이 필요 없는 PerSAM과 미세 조정 PerSAM-F입니다.* 동등한 기여.† 책임 저자.1 2 매개변수 PerSAM PerSAM-F 사용자 제공 DreamBooth PerSAM 지원 미세 조정 10초 테디베어의 모자 [V] 고양이 사진 &quot;[V] 고양이가 있는 해변&quot; ... 다양한 포즈나 장면 미세 조정 로봇 장난감의 몸체 *** 그림 2: 개인화된 세분화 예.PerSAM(왼쪽)은 유리한 성능으로 모든 맥락에서 개인 사물을 세분화할 수 있으며, PerSAM-F(오른쪽)는 규모 인식 미세 조정을 통해 모호성 문제를 더욱 완화합니다. 1 서론 [V] 백팩 사진 &quot;교실 테이블 위의 [V] 백팩&quot; 그림 3: PerSAM을 이용한 DreamBooth 개선(Ruiz et al., 2022). 훈련 중 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 높은 품질의 개인화된 텍스트-이미지 생성을 달성하는 데 도움이 될 수 있습니다. 시각(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019) 및 다중 모달리티(Radford et al., 2021; Jia et al., 2021; Li et al., 2023)의 기초 모델은 대규모 데이터 세트와 계산 리소스의 가용성에 기인하여 전례 없는 보급을 얻었습니다. 그들은 제로샷 시나리오에서 놀라운 일반화 능력을 보여주고, 인간의 피드백을 통합한 다재다능한 상호 작용을 보여줍니다. 여기에서 영감을 얻은 Segment Anything(Kirillov et al., 2023)은 1,100만 개의 이미지 마스크 데이터를 수집하기 위한 섬세한 데이터 엔진을 개발하고, 이후 SAM이라고 알려진 세분화 기반 모델을 훈련합니다. 그것은 새로운 프롬프트 가능 세분화 프레임워크를 정의합니다. 즉, 수작업 프롬프트를 입력으로 받고 예상 마스크를 반환하여 시각적 맥락에서 모든 객체를 세분화할 수 있습니다. 그러나 SAM은 본질적으로 특정 시각적 개념을 세분화하는 기능을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 사라진 시계를 찾으려고 한다고 상상해보세요. 바닐라 SAM을 사용하면 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에서 복잡한 맥락에서 대상 객체를 정확하게 찾은 다음 세분화를 위한 적절한 프롬프트로 SAM을 활성화해야 합니다. 이를 고려하여 다음과 같은 질문이 생깁니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 세분화하도록 SAM을 개인화할 수 있을까요? 이를 위해 Segment Anything Model을 위한 훈련이 필요 없는 개인화 접근 방식인 PerSAM을 소개합니다.그림 1에서 볼 수 있듯이, 저희의 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크인 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 정의합니다.특히, 저희는 먼저 모든 전경 픽셀의 모양을 고려하는 특징 유사성에 의해 테스트 이미지의 대상 객체에 대한 위치 신뢰도 맵을 얻습니다.신뢰도 점수에 따라 두 지점이 양수-음수 위치 사전으로 선택되고, 이는 최종적으로 프롬프트 토큰으로 인코딩되어 세분화를 위해 SAM의 디코더에 입력됩니다.디코더 내에서 저희는 두 가지 기술을 사용하여 대상 객체의 시각적 의미론을 주입하여 SAM의 개인화된 세분화 능력을 최대한 발휘하도록 제안합니다.• • 대상 유도 주의. 저희는 위치 신뢰도 맵에 따라 SAM의 디코더에서 모든 토큰-이미지 교차 주의 계층을 안내합니다.이는 프롬프트 토큰이 집중적인 특징 집계를 위해 주로 전경 대상 영역에 집중하도록 명시적으로 강제합니다.대상 의미적 프롬프팅. SAM에 고수준 대상 의미론을 명시적으로 제공하기 위해 원래 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 저수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다. 앞서 언급한 디자인과 계단식 사후 정제를 통해 PerSAM은 다양한 포즈나 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히, 우리의 접근 방식은 여러 유사한 객체 중에서 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오에 잘 대처할 수 있습니다. 그럼에도 불구하고 그림 2에서 볼 수 있듯이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층 구조로 구성된 경우(예: 테디베어 위의 모자 또는 로봇 장난감의 머리) 가끔 실패 사례가 있을 수 있습니다. 이러한 모호성은 PerSAM이 출력으로 적절한 마스크 크기를 결정하는 데 어려움을 줍니다. 로컬 부분과 글로벌 모양이 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문입니다. 이 문제를 완화하기 위해, 우리는 PerSAM-F라는 우리 접근 방식의 미세 조정 변형을 추가로 제안합니다. 우리는 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 동결하고, 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 말하면, 우리는 SAM이 다양한 마스크 스케일의 여러 가지 잠재적인 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 가장 적합한 스케일을 적응적으로 선택하기 위해, 우리는 각 마스크 스케일에 대해 학습 가능한 상대적 가중치를 사용하고, 최종 출력으로 가중 합산을 수행합니다. 이러한 효율적인 스케일 인식 훈련을 통해, PerSAM-F는 원샷 데이터에서 과적합을 피하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다. 또한, 우리의 접근 방식이 DreamBooth(Ruiz et al., 2022)가 그림 3에서 보듯이 개인화된 텍스트-이미지 생성을 위한 확산 모델을 보다 잘 미세 조정할 수 있도록 도울 수 있다는 것을 관찰했습니다. 애완 고양이 또는 백팩과 같은 특정 시각적 개념이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 이러한 이미지를 단어 임베딩 공간의 식별자[V]로 변환하는 방법을 학습하지만 동시에 계단이나 숲과 같은 배경 정보를 포함할 수 있습니다. 이렇게 하면 새로 프롬프트된 배경이 무시되고 대상 모양 생성이 방해받습니다. 따라서 우리는 PerSAM을 활용하여 훈련 이미지 내에서 대상 개체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하는 것을 제안합니다. 우리는 논문의 기여를 다음과 같이 요약합니다. • 개인화된 개체 분할. 우리는 먼저 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오로 사용자 지정하는 방법을 조사합니다. 이를 위해 개인화된 객체 분할을 평가하기 위한 새로운 분할 데이터 세트인 PerSeg와 함께 효율적이고 효과적인 두 가지 방법을 소개합니다.• PerSAM 및 PerSAM-F. PerSAM에서 대상 객체의 고수준 의미론에 따라 SAM을 안내하는 세 가지 무훈련 기술을 제안합니다. PerSAM-F에서 마스크 모호성 문제를 크게 완화하기 위해 10초 동안 2개의 매개변수를 사용하여 스케일 인식 미세 조정을 설계합니다.• 우리의 접근 방식은 PerSeg 벤치마크, 원샷 파트 및 의미 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 얻습니다.또한 PerSAM은 더 나은 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 향상시킬 수 있습니다.2 관련 작업 기초 모델. 강력한 일반화 용량을 통해 사전 훈련된 기초 모델을 다양한 다운스트림 시나리오에 맞게 조정하고 유망한 성능을 얻을 수 있습니다. 자연어 처리에서 BERT(Devlin 등, 2018; Lu 등, 2019), GPT 시리즈(Brown 등, 2020; OpenAI, 2023; Radford &amp; Narasimhan, 2018; Radford 등, 2019), LLAMA(Zhang 등, 2023c)는 놀라운 맥락 내 학습 능력을 보여주었으며, 도메인별 프롬프트를 통해 새로운 작업으로 전환될 수 있습니다. 마찬가지로 이미지-텍스트 쌍에 대한 대조 학습을 수행하는 CLIP(Radford 등, 2021)과 ALIGN(Jia 등, 2021)은 제로샷 시각 인식에서 뛰어난 정확도를 보여줍니다. Painter(Wang 등, 2022)는 다운스트림 미세 조정 없이 다양한 비전 작업을 수행하기 위해 네트워크 아키텍처와 맥락 내 프롬프트를 통합하는 비전 모델을 도입합니다. CaFo(Zhang et al., 2023d)는 다양한 기초 모델을 캐스케이드하고 사전 훈련된 지식을 협력하여 견고한 저데이터 이미지 분류를 수행합니다. SAM(Kirillov et al., 2023)은 10억 개의 마스크로 사전 훈련된 이미지 분할을 위한 기초 모델을 제시하고 프롬프트 기반 분할을 수행합니다. 고품질 분할(Ke et al., 2023), 더 빠른 추론 속도(Zhao et al., 2023; Zhang et al., 2023a), 모든 용도 매칭(Liu et al., 2023), 3D 재구성(Cen et al., 2023), 객체 추적(Yang et al., 2023), 의료(Ma &amp; Wang, 2023; Huang et al., 2023) 이미지 처리를 위해 SAM을 확장하는 몇 가지 동시 작업이 있습니다. 다른 관점에서, 우리는 특정 시각적 개념에 대한 세분화 기반 모델, 즉 SAM을 개인화하여 일반인을 단 한 번의 샷으로 전문가로 적응시키는 것을 제안합니다.우리의 방법은 또한 텍스트-이미지 기반 모델, 즉 Stable Diffusion(Rombach et al., 2022) 및 Imagen(Saharia et al., 2022)의 개인화를 지원할 수 있으며, 이는 전경 대상 객체를 배경 교란으로부터 세분화하여 생성 품질을 개선합니다.세그먼테이션의 3가지 대형 모델.컴퓨터 비전의 기본 작업인 세분화(Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)에는 이미지에 대한 픽셀 수준의 이해가 필요합니다. 다양한 분할 관련 작업이 탐색되었는데, 여기에는 각 픽셀을 사전 정의된 클래스 집합으로 분류하는 의미적 분할(Badrinarayanan 등, 2017; Chen 등, 2017; Zheng 등, 2021; Cheng 등, 2022; Xie 등, 2021; Song 등, 2020)이나 개별 객체 인스턴스의 식별에 초점을 맞춘 인스턴스 분할(He 등, 2017; Wang 등, 2020; Tian 등, 2020), 클래스 레이블과 인스턴스 식별을 모두 할당하는 전방위 분할(Kirillov 등, 2019; Li 등, 2019), 그리고 세분화를 위해 인간의 개입을 포함하는 대화형 분할(Hao 등, 2021; Chen 등, 2021) 등이 있습니다. 최근 언어 기반 모델(Zhang et al., 2023c; Brown et al., 2020)에서 영감을 받아 여러 동시 연구에서 이미지 분할을 위한 대규모 비전 모델이 제안되었습니다. 이 모델은 광범위한 마스크 데이터로 사전 학습되었으며 다양한 이미지 분포에 대해 강력한 일반화 기능을 보여줍니다. Segment Anything Model(SAM)(Kirillov et al., 2023)은 모델-인-더-루프 주석이 있는 데이터 엔진을 활용하여 프롬프트 가능 분할 프레임워크를 학습하는데, 이는 제로샷 방식으로 다운스트림 시나리오로 일반화됩니다. Painter(Wang et al., 2022)와 SegGPT(Wang et al., 2023)는 강력한 컨텍스트 내 학습 패러다임을 도입하고 주어진 이미지 마스크 프롬프트에 따라 모든 이미지를 분할할 수 있습니다. SEEM(Zou et al., 2023)은 언어 및 오디오와 같은 다중 모달 참조에 의해 촉발된 일반적인 분할 모델을 추가로 제시하여 다양한 의미 지식을 통합합니다. 이 연구에서는 개인화된 객체 분할이라는 새로운 작업을 도입하고 평가를 위해 새로운 데이터 세트 PerSeg에 주석을 달았습니다. 대규모 분할 모델을 개발하는 대신, 우리의 목표는 사용자가 제공한 객체를 모든 포즈나 장면에서 분할하도록 모델을 개인화하는 것입니다. 개인화된 분할을 위해 SAM을 효율적으로 사용자 지정하는 PerSAM과 PerSAM-F의 두 가지 접근 방식을 제안합니다. 매개변수 효율적인 미세 조정. 다운스트림 작업에서 전체 기초 모델을 직접 조정하는 것은 컴퓨팅 비용이 많이 들고 메모리 집약적일 수 있으며, 리소스가 제한된 애플리케이션에 어려움을 줄 수 있습니다. 이 문제를 해결하기 위해 최근 연구에서는 기초 모델의 가중치를 동결하고 미세 조정을 위한 소규모 모듈을 추가하는 매개변수 효율적 방법(Sung 등, 2022; He 등, 2022; Rebuffi 등, 2017; Qin &amp; Eisner, 2021)을 개발하는 데 중점을 두었습니다. 신속한 조정(Lester 등, 2021; Zhou 등, 2022; Jia 등, 2022; Liu 등, 2021)은 동결된 모델과 함께 학습 가능한 소프트 프롬프트를 사용하여 특정 다운스트림 작업을 수행하여 전체 모델 조정에 비해 규모와 강력한 도메인 전송으로 더 경쟁력 있는 성능을 달성하는 것을 제안합니다. 저랭크 적응(LORA)(Hu 등, 2021; Cuenca &amp; Paul, 2023; Zhang 등, 2023b; Hedegaard 등, 2022)은 학습 가능한 랭크 분해 행렬을 각 사전 학습된 가중치에 동시에 주입하여 다운스트림 작업에 필요한 학습 가능한 매개변수 수를 크게 줄입니다. 어댑터(Houlsby 등, 2019; Pfeiffer 등, 2020; Lin 등, 2020; Chen 등, 2022)는 원래 변환기의 레이어 사이에 삽입되도록 설계되어 특징 변환을 위한 경량 MLP를 도입합니다. 기존 작업과 달리 SAM을 위해 섬세하게 설계된 보다 효율적인 적응 방법, 즉 2개의 매개변수와 10초만 있는 PerSAM-F의 스케일 인식 미세 조정을 채택합니다. 이를 통해 원샷 데이터에서 과적합 문제를 효과적으로 방지하고 우수한 성능으로 세분화 규모의 모호성을 완화합니다.3 방법 3.1절에서 먼저 Segment Anything Model(SAM)(Kirillov et al., 2023)을 간략하게 다시 살펴보고 개인화된 객체 세분화를 위한 작업 정의를 소개합니다.그런 다음 각각 3.2절과 3.3절에서 PerSAM과 PerSAM-F의 방법론을 설명합니다.마지막으로 3.4절에서 DreamBooth(Ruiz et al., 2022)가 더 나은 텍스트-이미지 생성을 지원하도록 접근 방식을 활용합니다.3.1 개인화된 객체 세분화 Segment Anything 다시 살펴보기.SAM은 프롬프트 인코더, 이미지 인코더, 경량 마스크 디코더의 세 가지 구성 요소로 구성되며 각각 Encp, Enc, Decм로 표시합니다. 프롬프트 가능 프레임워크로서, SAM은 이미지 I와 프롬프트 집합 P(점, 상자 또는 거친 마스크)를 입력으로 받습니다. 구체적으로 SAM은 먼저 Enc를 사용하여 입력 이미지 특징을 얻고 Encp를 채택하여 길이가 k인 인간이 제공한 프롬프트를 FI = Enc(I), Tp = Encp(P)와 같이 프롬프트 토큰으로 인코딩합니다.(1) 4 F₁ 코사인 유사도 인코딩 {FT}=1 테스트 이미지 I 대상 로컬 특징 {T}=1 FR MR° FR PerSAM의 디코더 인코딩 대상 가이드 주의 이미지-토큰 교차 주의 대상 의미적 프롬프트 ↑ {Si)=1 로컬 신뢰 맵 토큰-이미지 교차 주의 변조 ↑ 집계 주의 행렬 A 로컬 특징 {T}=1 토큰 자체 주의 전체 신뢰 맵 S 집계 ↑ a Concat( + Repeat( ) 전체 신뢰 맵 S TM Тр × 2 TR 원샷 이미지 IR 원샷 마스크 MR 양의 사전 음의 사전 그림 4: 영어: Positive-negative Location Prior. 모든 로컬 파트의 출현에 따라 새로운 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 계산합니다. 그런 다음 PerSAM에 대한 포인트 프롬프트로 위치 사전을 선택합니다. 저수준 위치 프롬프트 고수준 의미 프롬프트 그림 5: 대상 안내 주의(왼쪽) 및 대상 의미 프롬프트(오른쪽). 대상 의미론을 SAM에 주입하기 위해 교차 주의 계층을 명시적으로 안내하고 고수준 단서를 사용하여 추가 프롬프트를 제안합니다. 여기서 F₁ = Rhxwxc 및 Tp Є Rkxc, h, w는 이미지 피처 맵의 해상도를 나타내고 c는 피처 차원을 나타냅니다. 그 후 인코딩된 이미지와 프롬프트가 주의 기반 피처 상호 작용을 위해 디코더 Decм에 입력됩니다. SAM은 프롬프트 토큰 Tp에 접두사로 여러 학습 가능한 마스크 토큰 Tм을 연결하여 디코더의 입력 토큰을 구성합니다. 이러한 마스크 토큰은 M = Decм(FI, Concat(TM,Tp))로 공식화된 마스크 출력을 생성하는 역할을 하며, 여기서 M은 SAM에서 예측한 최종 분할 마스크를 나타냅니다. (2) 작업 정의. SAM은 프롬프트를 통해 모든 객체에 대해 충분히 일반화되지만 특정 주체 인스턴스를 자동으로 분할하는 기능이 부족합니다. 이를 고려하여 개인화된 객체 분할을 위한 새로운 작업을 정의합니다. 사용자는 단일 참조 이미지와 대상 시각적 개념을 나타내는 마스크만 제공합니다. 제공된 마스크는 정확한 분할이거나 즉석에서 그린 대략적인 스케치일 수 있습니다. 우리의 목표는 추가적인 인간의 프롬프트 없이 새 이미지나 비디오 내에서 지정된 객체를 분할하도록 SAM을 사용자 지정하는 것입니다. 평가를 위해 PerSeg라는 개인화된 분할을 위한 새로운 데이터 세트에 주석을 달았습니다. 원시 이미지는 다양한 포즈나 장면에서 다양한 범주의 시각적 개념을 포함하는 주체 중심 확산 모델(Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022)에 대한 작업에서 수집되었습니다. 이 논문에서 우리는 이 과제를 위한 두 가지 효율적인 솔루션을 제안하며, 구체적으로 다음과 같이 설명합니다. 3.2 학습 없는 PERSAM 위치 신뢰도 맵. 사용자가 제공한 이미지 IR과 마스크 MR을 조건으로, PerSAM은 먼저 새로운 테스트 이미지 I에서 대상 객체의 위치를 나타내는 신뢰도 맵을 얻습니다. 그림 4에서 볼 수 있듯이, 우리는 이미지 인코더를 적용하여 IR과 I의 시각적 특징을 추출합니다. 인코더는 SAM의 동결 백본 또는 다른 사전 학습된 비전 모델이 될 수 있으며, 우리는 기본적으로 SAM의 이미지 인코더 Enc를 채택합니다. 우리는 프로세스를 FI = Enc(I), FR = EncI(IR), (3)으로 공식화합니다. 여기서 F1, FR Є R¹xwxc입니다. 그런 다음 참조 마스크 MR Є Rhxwx¹를 사용하여 FR의 시각적 개념 내에서 전경 픽셀의 특징을 잘라내어 {T}}=1 = MR ○ Fr, (4) 5와 같이 n개의 로컬 특징 세트를 생성합니다. 여기서 T½ Є R¹×ª이고 ○는 공간적 곱셈을 나타냅니다. 그런 다음 T½와 테스트 이미지 특징 FĮ 사이의 코사인 유사도에 의해 각 전경 픽셀 i에 대한 n개의 신뢰도 맵을 계산합니다. {S² } }±₁ = {F₁TT) 11, 여기서 S₁ = Rhxw. (5) FI와 T½는 픽셀별로 L2-정규화되었습니다. 각 S²는 테스트 이미지에서 개체의 다른 로컬 부분(예: 개의 머리, 몸통 또는 발)에 대한 분포 확률을 나타냅니다. 이에 더하여, 우리는 모든 n개의 로컬 맵을 집계하여 S = n n SiЄRhxw (6)와 같이 대상 객체의 전반적인 신뢰도 맵을 얻기 위해 평균 풀링을 채택합니다.모든 전경 픽셀의 신뢰도를 통합함으로써, S는 다른 객체 부분의 시각적 모양을 고려하고 비교적 포괄적인 위치 추정을 얻을 수 있습니다.양의-음의 위치 사전.PerSAM에 테스트 이미지의 위치 사전을 제공하기 위해, 우리는 S에서 가장 높고 가장 낮은 신뢰도 값을 갖는 두 지점을 선택합니다.각각 Pɲ와 Pɩ로 표시합니다.전자는 대상 객체의 가장 가능성 있는 중심 위치를 나타내고, 후자는 반대로 배경을 나타냅니다.그런 다음, 이들은 양의 및 음의 지점 프롬프트로 간주되어 Tp = Encp(Ph, P₁) Є R2×c로 프롬프트 인코더에 입력됩니다.(7) 이는 SAM 디코더의 프롬프트 토큰을 나타냅니다. 이런 식으로 SAM은 이미지에서 음의 점을 버리는 반면, 양의 점을 둘러싼 연속적인 영역을 분할하려는 경향이 있습니다.대상 유도 주의.양의-음의 점 프롬프트가 얻어졌지만, 우리는 SAM 디코더에서 교차 주의 연산에 대한 보다 명확한 의미적 지침을 제안합니다.이는 전경 대상 영역 내에서 피처 집계를 집중시킵니다.그림 5에서 볼 수 있듯이, 방정식 6의 전체 신뢰도 맵 S는 테스트 이미지에서 대상 시각적 개념의 거친 영역을 명확하게 나타낼 수 있습니다(더 밝은 색상은 더 높은 점수를 나타냄).이러한 속성을 기반으로, 우리는 S를 사용하여 디코더의 모든 토큰-이미지 교차 주의 계층에서 주의 맵을 안내합니다.특히, 소프트맥스 함수 뒤의 모든 주의 맵을 A = Rhxw로 표시한 다음, 주의 분포를 A9 = softmax A+ a softmax(S) • (8)로 변조합니다.여기서 a는 밸런싱 팩터를 나타냅니다. 주의 편향으로 인해 마스크와 프롬프트 토큰은 중요하지 않은 배경 영역이 아닌 대상 주제와 관련된 더 많은 시각적 의미를 포착해야 합니다. 이는 주의 메커니즘에서 더 효과적인 기능 집계에 기여하고, 훈련 없이 PerSAM의 최종 세분화 정확도를 향상시킵니다. 대상 의미적 프롬프트. 바닐라 SAM은 점이나 상자의 좌표와 같은 저수준 위치 정보가 있는 프롬프트만 수신합니다. SAM의 디코더에 더 높은 수준의 단서를 제공하기 위해 대상 개념의 시각적 특징을 추가 높은 수준의 의미적 프롬프트로 활용하는 것을 제안합니다. 먼저 참조 이미지에서 객체의 글로벌 임베딩 TR을 다음과 같이 다른 로컬 피처 간의 I 평균 풀링을 통해 얻습니다.n TR = ΣΤΑ €R1xc n i=1 그런 다음 방정식 2에서 테스트 이미지의 모든 입력 토큰에 TR을 요소별로 추가한 다음 디코더 블록에 공급합니다.그림 5에 T9 = Repeat (TR) + Concat(TM,Tp), (10)으로 표시됩니다.여기서 T9는 디코더 Decм에 대한 대상 의미론에 의해 안내되는 입력 토큰을 나타내고 Repeat 작업은 대상 시각적 임베딩을 복제합니다.간단한 토큰 통합의 도움으로 PerSAM은 저수준 위치 지점뿐만 아니라 고수준 대상 시각적 단서에 의해서도 촉발됩니다. 6 테스트 이미지 PerSAM 출력 3개 M1 M2 + M3 스케일 임의 노이즈 →&gt; DreamBooth →&gt; ↑ &quot;a [V] cat&quot; 재구성 손실 W1 F W2 1- W1 + W₂ ) 1 사용자가 출력 마스크 M 제공 미세 조정 가중 합산 동결 PerSAM 배경 교란 분리 그림 6: PerSAM-F의 스케일 인식 미세 조정. 스케일 모호성을 완화하기 위해 PerSAM-F는 3개 스케일 마스크를 적응적으로 집계하기 위한 두 개의 학습 가능한 가중치를 채택합니다. 그림 7: PerSAM 지원 DreamBooth. DreamBooth 생성을 개선하기 위해 PerSAM을 사용하여 대상 객체를 배경에서 분리합니다. 계단식 사후 미세 조정. 위의 기술을 통해 SAM 디코더에서 테스트 이미지에 대한 초기 분할 마스크를 얻지만 거친 모서리와 고립된 배경 노이즈가 포함될 수 있습니다. 추가 세분화를 위해, 우리는 두 단계 후처리를 위해 마스크를 디코더 Decм에 반복적으로 공급합니다.첫 번째 단계에서 우리는 이전의 양수-음수 포인트 프롬프트와 함께 현재 예측된 마스크로 디코더에 프롬프트를 보냅니다.두 번째 단계에서 우리는 첫 번째 단계에서 마스크를 둘러싼 경계 상자를 획득하고, 더 정확한 객체 위치 파악을 위해 이 상자로 디코더에 추가로 프롬프트를 보냅니다.우리는 대규모 이미지 인코더 없이 가벼운 디코더만 반복하기 때문에 후처리가 효율적이고 단지 2%의 추가 지연 시간이 발생합니다.3.3 PERSAM-F 세분화 스케일의 모호성의 미세 조정.훈련이 필요 없는 PerSAM은 대부분의 경우를 만족스러운 세분화 정확도로 처리할 수 있습니다.그러나 일부 대상 객체에는 계층적 구조가 포함되어 있어 마스크 스케일의 모호성이 발생합니다.그림 6에서 볼 수 있듯이 플랫폼 위의 찻주전자는 뚜껑과 몸체의 두 부분으로 구성되어 있습니다. 양의 점 프롬프트(녹색 오각별 표시)가 몸체에 위치하고, 음의 프롬프트(빨간색 오각별 표시)가 비슷한 색상의 플랫폼을 제외하지 않으면 PerSAM은 분할에 대해 오도될 것입니다. 이러한 문제는 SAM(Kirillov et al., 2023)에서도 논의되며, 여기서는 객체의 전체, 부분 및 하위 부분에 해당하는 세 가지 크기의 여러 마스크를 동시에 생성하는 대안을 제안합니다. 그런 다음 사용자는 세 가지 마스크 중 하나를 수동으로 선택해야 하는데, 이는 효과적이지만 추가 인력을 소모합니다. 반면에 개인화된 작업은 인간의 프롬프트가 필요 없이 자동 객체 분할을 위해 SAM을 사용자 지정하는 것을 목표로 합니다. 이는 매개변수 효율적인 미세 조정을 통해 PerSAM의 크기 인식 버전을 추가로 개발하도록 동기를 부여합니다. 크기 인식 미세 조정. 적절한 크기에서 적응형 분할을 위해 미세 조정 변형인 PerSAM-F를 도입합니다. 학습이 필요 없는 모델이 하나의 마스크만 생성하는 것과 달리, PerSAM-F는 먼저 PerSAM을 따라 사전 위치를 구하고, SAM의 원래 솔루션을 참조하여 각각 M1, M2, M3으로 표시되는 3개 스케일 마스크를 출력합니다. 여기에 두 개의 학습 가능한 마스크 가중치 w₁, W2를 채택하고 가중치 합산을 통해 최종 마스크 출력을 다음과 같이 계산합니다. M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) 여기서 w1, W2는 모두 1/3으로 초기화됩니다. 최적의 가중치를 학습하기 위해 참조 이미지에서 원샷 미세 조정을 수행하고 주어진 마스크를 기준 진실로 간주합니다. 사전 학습된 지식을 보존하기 위해 전체 SAM 모델을 동결하고 단일 A100 GPU에서 10초 이내에 W1, W2의 두 매개변수만 미세 조정합니다. 이런 방식으로, 우리의 PerSAM-F는 객체의 스케일 인식 의미를 효율적으로 학습하고, 다양한 개념에 대한 최상의 분할 스케일을 적응적으로 출력하여 PerSAM의 일반화 용량을 개선합니다.7 표 1: PerSeg 데이터 세트의 개인화된 객체 분할. 우리는 다양한 방법(Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023)에 대한 전반적인 mIoU, blou 및 학습 가능한 매개변수와 PerSeg의 10개 객체에 대한 mIoU를 비교합니다. ***는 우리와 동시에 진행 중인 작업을 나타냅니다. 방법 mIoU bloU 매개변수. 캔 헛간 시계 고양이 뒤- 테디 오리 얇은 빨간색 팩 곰 장난감 새 만화 로봇 장난감 화가 VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 354M 93.0 33.3 20.9 98.2 65.0 59.2 76.6 66.7 79.8 89.9 67.4 81.0 72.4 72.4 91.1 94.1 95.2 98.0 71.3 97.0 95.8 96.6 63.8 92.6 94.1 94.4 93.7 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 표 2: DAVIS 2017 val(Pont-Tuset et al., 2017)에서의 비디오 객체 분할. 회색은 도메인 내 학습을 포함하는 방법을 나타냅니다. 표 3: FSS-1000(Li et al., 2020), LVIS-92(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 의미론 및 파트 분할. 우리는 mIoU 점수를 보고하고 회색을 사용하여 도메인 내 학습을 포함하는 방법을 나타냅니다. 원샷 의미론 분할 FSS-1000 LVIS-92² 원샷 파트 분할 PASCAL-Part Painter SEEM SegGPT PerSAM 방법 J&amp;F І AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F 방법 PACO-Part HSNet 86.5 17.4 32.4 22.6 부가세 90.3 18.5 33.6 23.5 페인터 61.7 10.5 30.4 14.1 SegGPT 85.6 18.6 66.9 71.3 75.1 PerSAM 81.6 15.6 32.5 22.5 PerSAM-F 76.1 74.9 79.7 PerSAM-F 86.3 18.4 32.9 22.7 3.4 PERSAM 지원 DREAMBOOTH 개인화된 텍스트-이미지 합성을 위해 DreamBooth(Ruiz et al., 2022)는 특정 개체 ig, 애완 고양이의 주어진 3~5개 사진을 사용하여 사전 훈련된 확산 모델을 미세 조정합니다. 텍스트 프롬프트에서 언급된 고양이인 &quot;[V] 고양이&quot;를 생성하는 방법을 학습하고 재구성된 전체 이미지에 대한 손실을 계산합니다. 이를 통해 훈련 이미지의 중복된 배경 정보를 식별자 [V]에 주입합니다. 따라서 그림 7과 같이 DreamBooth에서 배경의 교란을 완화하기 위한 전략을 소개합니다. 몇 장의 이미지에 대한 개체 마스크가 주어지면 PerSAM을 활용하여 모든 전경 대상을 분할하고 배경 영역에 속하는 픽셀에 대한 그래디언트 역전파를 버립니다. 그런 다음, 안정적 확산은 대상 객체의 시각적 I 모양만 기억하도록 미세 조정됩니다. 배경에 감독이 부과되지 않으므로 PerSAM 지원 DreamBooth는 더 나은 시각적 대응으로 대상 객체를 합성할 수 있을 뿐만 아니라 입력 텍스트 프롬프트에 따라 새로운 배경의 다양성을 높일 수도 있습니다.4 실험 먼저 섹션 4.1에서 PerSeg에서 개인화된 세분화를 위한 접근 방식을 평가하고, 섹션 4.2에서 다양한 기존 원샷 세분화 벤치마크를 평가합니다.그런 다음 섹션 4.3에서 PerSAM 지원 DreamBooth의 효과를 설명합니다.마지막으로 섹션 4.4에서 PerSeg에 대한 설계를 조사하기 위해 여러 가지 절제 연구를 수행합니다.4.1 개인화된 평가 PerSeg 데이터 세트.개인화 용량을 테스트하기 위해 PerSeg라는 새로운 세분화 데이터 세트를 구성합니다. 원시 이미지는 주제 중심 확산 작업(Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022)의 훈련 데이터에서 수집되었습니다.PerSeg에는 일용품, 동물, 건물을 포함하여 총 40개의 다양한 범주의 객체가 포함되어 있습니다.다른 포즈나 장면에서 각 객체는 5~7개의 이미지와 마스크와 연관되며, 여기서 하나의 이미지-마스크 쌍을 사용자가 제공한 원샷 데이터로 고정합니다.평가에는 mIoU와 bloU(Cheng et al., 2021)가 채택되었습니다.PerSeg의 구현 세부 사항과 확대된 데이터 스케일은 부록을 참조하세요.8 시계의 반지 2 쟁반 위의 찻주전자 여자가 들고 있는 백팩 캔의 윗부분 그림 8: PerSAM-F의 개선 사항 시각화.우리의 스케일 인식 미세 조정은 PerSAM의 스케일 모호성을 잘 완화할 수 있습니다. 그림 9: 비디오 객체 분할의 시각화. 우리의 접근 방식은 비디오에서 여러 객체를 분할하는 데 좋은 성과를 보입니다. 성능. 표 1에서 미세 조정된 PerSAM-F가 가장 좋은 결과를 얻는 것을 볼 수 있는데, 이는 PerSAM을 전체 mIoU와 bIoU에서 +2.7%, +5.9% 효과적으로 향상시킵니다. 그림 8에서 PerSAM-F의 개선 사항을 더 자세히 시각화합니다. Visual Prompting(VP)(Bar et al., 2022), Painter(Wang et al., 2022), SEEM(Zou et al., 2023), SegGPT(Wang et al., 2023)는 주어진 원샷 프롬프트 데이터에 따라 객체를 분할할 수 있는 컨텍스트 내 학습기입니다. 표시된 대로, 훈련이 필요 없는 PerSAM은 다른 마진으로 Painter, VP, SEEM보다 더 나은 성능을 이미 달성할 수 있습니다. 효율적인 2-매개변수 미세 조정을 통해, 저희의 PerSAM-F는 강력한 SegGPT를 +2.4%, 전반적인 mIoU와 bIoU에서 +4.1% 더 능가합니다.세그먼테이션 일반론자를 개발하려는 그들의 동기와 달리, 저희의 방법은 개인화된 객체 세분화를 위해 특별히 설계되었으며, 시간과 계산 리소스 측면에서 훨씬 더 높은 효율성을 보여줍니다.4.2 기존 세분화 벤치마크 비디오 객체 세분화.첫 번째 프레임 이미지와 객체 마스크를 고려할 때, 저희의 PerSAM과 PerSAM-F는 DAVIS 2017의 검증 세트에서 경쟁력 있는 객체 세분화 및 추적 성능을 달성합니다(Pont-Tuset et al., 2017) 표 2에서 볼 수 있듯이, 비디오 학습이 없는 방법과 비교했을 때, 학습이 없는 PerSAM은 Painter를 +32.3% J&amp;F 점수로 크게 능가하며, 저희의 PerSAM-F는 SegGPT보다 +0.5% 더 나은 성능을 달성할 수 있습니다. 특히, 우리의 원샷 미세 조정 접근 방식은 광범위한 비디오 데이터로 완전히 훈련된 방법(Lin et al., 2019; Liang et al., 2020)보다 성능이 우수할 수 있습니다. 결과는 그림 9에서 시각화된 것처럼 여러 유사하거나 가려진 객체를 포함하는 시간적 비디오 데이터와 복잡한 시나리오에 대한 우리의 강력한 일반화 능력을 충분히 보여줍니다. 원샷 의미론 및 부분 분할. 표 3에서 우리는 각각 4개의 데이터 세트인 FSS-1000(Li et al., 2020), LVIS-92²(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 이미지 분할에 대한 접근 방식을 평가합니다. 여기서 우리는 데이터 전처리 및 평가를 위해 Matcher(Liu et al., 2023)를 따릅니다. 표시된 대로, 우리의 PerSAM-F는 Painter보다 지속적으로 더 나은 결과를 얻었으며 SegGPT와 비슷한 성능을 보였습니다. 도메인 내 학습이 있는 모델(Min et al., 2021; Hong et al., 2022)의 경우, 우리의 접근 방식은 HSNet보다 더 높은 점수를 얻을 수 있습니다. 실험은 우리가 제안하는 접근 방식이 객체 수준 분할에 국한되지 않고 SAM의 범주별 및 부분별 개인화에도 작동한다는 것을 잘 보여줍니다. 4.3 PERSAM 지원 DREAMBOOTH 우리는 DreamBooth(Ruiz et al., 2022)의 모든 하이퍼파라미터를 따라 개인화된 이미지 합성을 위해 사전 학습된 Stable Diffusion(Rombach et al., 2022)을 미세 조정합니다. 그림 3 외에도 그림 10에서 PerSAM 지원 DreamBooth의 더 많은 예를 시각화합니다. 회색 소파에 누워 있는 개의 경우 DreamBooth의 &quot;정글&quot;과 &quot;눈&quot;은 여전히 녹색과 흰색 장식이 있는 소파입니다. PerSAM-F의 도움을 받아 새로 생성된 배경은 소파와 완전히 분리되어 텍스트 프롬프트와 잘 일치합니다. 산 앞에 있는 헛간의 경우, 우리의 접근 방식은 또한 배경 교란을 완화하여 &quot;숲&quot;과 &quot;푸른 하늘&quot;을 올바르게 생성합니다.9 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 개 사진 &quot;정글 속의 [V]개&quot; ... 헛간 사진 &quot;배경에 숲이 있는 [V]개&quot; &quot;눈 속의 [V]개&quot; &quot;배경에 푸른 하늘이 있는 [V]개&quot; 그림 10: PerSAM 기반 DreamBooth의 시각화.개선된 DreamBooth(Ruiz et al., 2022)는 새로운 이미지에서 다양한 맥락을 합성하기 위해 다양성을 더 잘 보존할 수 있습니다.표 4: 제안하는 방법에서 주요 구성 요소의 소거.변형 표 5: 다양한 미세 조정 방법의 소거.표 6: 참조로 상자 이미지를 사용하는 소거.mIoU 이득 69.1 방법 PerSAM 매개변수. mIoU 방법 마스크 상자 0 89.32 페인터 56.4 42.0 + 사후 세부화 72.5 +3.4 83.9 +11.4 프롬프트 튜닝 12K 76.5 VP 65.9 38.1 어댑터 196K 78.3 SEEM 87.1 64.9 + 가이드 어텐션 + 의미적 프롬프트 85.8 +1.9 LORA 293K 90.0 SegGPT 94.3 36.0 89.3 +3.5 3 마스크 가중치 3 92.9 + 스케일 튜닝 95.3 +6.0 PerSAM-F 2 95.3 PerSAM 89.3 PerSAM-F 95.3 88.1 94.9 양의 사전 + 음의 사전 4.4 절제 연구 주요 구성 요소. 표 4에서 우리는 긍정적인 위치 사전만을 채택하는 기준선에서 시작하여 다양한 구성 요소를 조사합니다.그런 다음 부정적인 지점 프롬프트와 계단식 사후 세분화를 추가하여 각각 +3.6%와 +11.4%의 mIoU를 향상시킵니다.그 위에 우리는 주의 유도와 의미 프롬프트를 위해 SAM의 디코더에 고수준 대상 의미론을 도입합니다.그 결과 +1.9%와 +3.5%의 개선은 그 중요성을 충분히 나타냅니다.마지막으로 효율적인 스케일 인식 미세 조정을 통해 PerSAM-F는 점수를 +6.0% 높여 뛰어난 정확도를 보여줍니다.다른 미세 조정 방법.표 5에서 우리는 PerSAM-F에 대한 다른 매개변수 효율적 미세 조정(PEFT) 방법, 즉 프롬프트 튜닝(Liu et al., 2021), 어댑터(Houlsby et al., 2019), LORA(Hu et al., 2021)를 실험합니다. 우리는 SAM 전체를 동결하고 PerSAM 디코더의 모든 변압기 블록에 주입된 PEFT 모듈만 튜닝합니다.보여진 바와 같이, 프롬프트 튜닝과 어댑터는 원샷 데이터를 과대적합시키고 정확도를 심각하게 떨어뜨립니다.대신, 우리의 스케일 인식 미세 튜닝은 가장 학습하기 어려운 매개변수를 튜닝하는 동안 PerSAM의 성능을 가장 잘 향상시킬 수 있습니다.참조로 상자 이미지 사용.원샷 데이터로 정확한 마스크를 요구하는 것은 일부 사용자에게는 너무 엄격할 수 있습니다.표 6에서 예상 객체를 지정하는 경계 상자에 대한 입력 제한을 완화합니다.우리 방법의 경우 상자를 프롬프트로 간주하고 기성품 SAM을 사용하여 원샷 마스크를 생성할 수 있습니다.따라서 상자 참조는 PerSAM 및 PerSAM-F에서 약간의 성능 저하로 이어질 뿐이지만 다른 방법에는 심각한 영향을 미칩니다.5 결론 이 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대한 Segment Anything Model(SAM)을 개인화하는 것을 제안합니다. 첫째, 훈련이 필요 없는 기술로 SAM에 고수준 대상 의미론을 주입하는 PerSAM을 소개합니다. 여기에 더해, 스케일 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 학습 가능한 매개변수가 2개뿐인 PerSAM-F는 마스크 스케일의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 선도적인 성능을 달성합니다. 게다가, DreamBooth가 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 도움이 되는 접근 방식의 효능도 검증합니다. 저희의 작업이 SAM의 적용 범위를 더 넓은 범위의 시나리오로 확장할 수 있기를 바랍니다. 10 10 참고문헌 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. Segnet: 이미지 분할을 위한 딥 합성곱 인코더-디코더 아키텍처. IEEE 패턴 분석 및 머신 인텔리전스 저널, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei Efros. 이미지 인페인팅을 통한 시각적 프롬프트. 신경 정보 처리 시스템의 발전, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, Qi Tian. Nerfs를 사용하여 3D로 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille. Deeplab: 딥 합성곱 신경망, Atrous 합성곱 및 완전 연결 CRF를 사용한 의미적 이미지 분할. IEEE 패턴 분석 및 머신 인텔리전스 저널, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan. 대화형 분할을 위한 조건부 확산. IEEE International Conference on Computer Vision의 진행 과정에서, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao. 밀도 예측을 위한 비전 변환기 어댑터. arXiv 사전 인쇄본 arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov. 경계 iou: 객체 중심 이미지 분할 평가 개선. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 진행 과정에서, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 마스크된 어텐션 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 1290–1299, 2022. Pedro Cuenca 및 Sayak Paul. 효율적인 안정적 확산 미세 조정을 위해 lora 사용. https:// huging face.co/blog/lora, 2023년 1월. Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5356-5364쪽, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: 에지 가이드 플로우로 실용적인 상호 작용 분할 달성. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 1551-1560쪽, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. 매개변수 효율적 전이 학습에 대한 통합된 관점을 향해. International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. Mask r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, Alexandros Iosifidis. 구조화된 가지치기 어댑터. arXiv 사전 인쇄본 arXiv:2211.10155, 2022. 11 Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, Seungryong Kim. few-shot segmentation을 위한 4d 합성곱 swin 변환기를 사용한 비용 집계. European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. NLP를 위한 매개변수 효율적 전이 학습. 국제 기계 학습 컨퍼런스에서, 2790-2799쪽. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen 등. 의료 이미지에 대한 모든 모델을 세분화할 수 있나요?arXiv 사전 인쇄본 arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각적 및 시각 언어 표현 학습 확장. International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 시각적 프롬프트 튜닝. European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai 및 Chengjie Wang. 도메인 적응 의미론적 분할을 위한 프로토타입 대비 적응. 컴퓨터 비전에 관한 유럽 회의, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang 및 Liqing Zhang. Stc: 비디오 인스턴스 분할을 위한 시공간 대조 학습. 컴퓨터 비전 워크숍에 관한 유럽 회의, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang 및 Fisher Yu. 무엇이든 고품질로 분할하세요. arXiv 사전 인쇄본 arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413쪽, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 지정. arXiv 사전 인쇄본 arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. arXiv 사전 인쇄본 arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: 대규모 비전 및 비전-언어 작업을 위한 일반 모델. arXiv 사전 인쇄본 arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang. Fss-1000: few-shot segmentation을 위한 1000-class 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 2869-2878, 2020. 12 Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang. 파노라마 분할을 위한 주의 유도 통합 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, Jim Chen. 적응적 특징 은행과 불확실한 영역 정제를 사용한 비디오 객체 분할. 신경 정보 처리 시스템의 발전, 33: 3430-3441, 2020. 화이지아 린, 샤오주안 치, 지아야 지아. Agss-vos: 주의 유도 단일 샷 비디오 객체 분할. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 3949-3957쪽, 2019. 자오장 린, 안드레아 마도토, 파스칼 펑. 매개변수 효율적 전이 학습을 통해 다재다능한 생성 언어 모델 탐색. arXiv 사전 인쇄본 arXiv:2004.03829, 2020. 린쯔이, 갱시지, 장렌루이, 가오펭, 제라르 드 멜로, 왕샤오강, 다이지펭, 차오위아오, 리홍셩. 동결된 클립 모델은 효율적인 비디오 학습기입니다. European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang. P-tuning v2: 신속한 튜닝은 규모와 작업 전반에 걸쳐 보편적으로 미세 조정하는 것과 비교할 수 있습니다. arXiv 사전 인쇄본 arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen. Matcher: 다목적 기능 매칭을 사용하여 한 번에 모든 것을 분할합니다. arXiv 사전 인쇄본 arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, Trevor Darrell. 의미 분할을 위한 완전 합성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 3431-3440쪽, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee. Vilbert: 시각 및 언어 작업을 위한 작업에 독립적인 시각 언어 표현 사전 학습. 신경 정보 처리 시스템의 발전(NeurIPS), 13-23쪽, 2019. Jun Ma, Bo Wang. 의료 이미지의 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, Minsu Cho. few-shot 분할을 위한 초상관 관계 압축. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 6941-6952쪽, 2021. Keval Morabia, Jatin Arora, Tara Vijaykumar. 객체 및 의미적 부분의 주의 기반 조인트 감지. arXiv 사전 인쇄본 arXiv:2007.02419, 2020. OpenAI. Gpt-4 기술 보고서. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych. 어댑터-융합: 전이 학습을 위한 비파괴적 작업 구성. arXiv 사전 인쇄본 arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv 사전 인쇄본 arXiv:1704.00675, 2017. Guanghui Qin 및 Jason Eisner. 묻는 방법 배우기: 소프트 프롬프트를 혼합하여 lms 쿼리하기. arXiv 사전 인쇄본 arXiv:2104.06599, 2021. Alec Radford 및 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: 공통 객체의 부분과 속성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서, 7141-7151쪽, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 잔여 어댑터를 사용하여 여러 시각적 도메인 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng. 일반 기능 변환을 위한 학습 가능한 트리 필터 재고. 신경 정보 처리 시스템의 발전, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, Mohit Bansal. Vl-adapter: 시각 및 언어 작업을 위한 매개변수 효율적 전이 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5227-5237쪽, 2022. Zhi Tian, Chunhua Shen, Hao Chen. 인스턴스 분할을 위한 조건부 합성곱. 유럽 컴퓨터 비전 컨퍼런스, 282-298쪽. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄 arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li 및 Chunhua Shen. Solov2: 동적이고 빠른 인스턴스 분할. 신경 정보 처리 시스템의 발전, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen 및 Tiejun Huang. 이미지는 이미지로 말한다: 맥락 내 시각적 학습을 위한 일반주의 화가. arXiv 사전 인쇄본 arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. Seggpt: 맥락 내에서 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo. Segformer: 변환기를 사용한 의미적 분할을 위한 간단하고 효율적인 디자인. 신경 정보 처리 시스템의 발전, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, Yu Qiao. 3D 객체 포인트 클라우드의 보완적 이해를 위한 기하학-분리된 표현 학습. AAAI 인공지능 컨퍼런스 회의록, 35권, 3056-3064쪽, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng. 무엇이든 추적: 무엇이든 비디오와 만나는 세그먼트. arXiv 사전 인쇄본 arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, Choong Seon Hong. 무엇이든 더 빠르게 세그먼트화: 모바일 애플리케이션을 위한 가벼운 sam을 향해. arXiv 사전 인쇄본 arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen 및 Tuo Zhao. 매개변수 효율적인 미세 조정을 위한 적응형 예산 할당. arXiv 사전 인쇄 arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao 및 Peng Gao. 프롬프트, 생성, 캐시: 기반 모델의 캐스케이드는 강력한 소수의 학습자를 만듭니다. arXiv 사전 인쇄 arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 및 Jiaya Jia. 피라미드 장면 구문 분석 네트워크. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의 진행, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang 및 Jinqiao Wang. 무엇이든 빠르게 분할하세요. arXiv 사전 인쇄 arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr 등. 트랜스포머를 사용한 시퀀스-투-시퀀스 관점에서 의미적 분할 재고. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6881-6890쪽, 2021. 카이양 저우, 징캉 양, 첸 창 로이, 지웨이 리우. 시각 언어 모델을 위한 프롬프트 학습. 국제 컴퓨터 비전 저널, 130(9):2337-2348, 2022. 쉐얀 저우, 지안웨이 양, 하오 장, 펭 리, 린지에 리, 지안펭 가오, 용재 리. 모든 곳을 한꺼번에 분할. arXiv 사전 인쇄본 arXiv:2304.06718, 2023. 15 115\\n',\n",
       " 'detectedSourceLanguage': 'en',\n",
       " 'input': 'arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\\nPERSONALIZE SEGMENT ANYTHING MODEL WITH\\nONE SHOT\\nRenrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\\nHao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\\n¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\\n3 Institute of Automation, Chinese Academy of Sciences\\n4CFCS, School of CS, Peking University\\n{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\\nkaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\\nABSTRACT\\nDriven by large-data pre-training, Segment Anything Model (SAM) has been\\ndemonstrated as a powerful promptable framework, revolutionizing the segmenta-\\ntion field. Despite the generality, customizing SAM for specific visual concepts\\nwithout man-powered prompting is under-explored, e.g., automatically segmenting\\nyour pet dog in numerous images. In this paper, we introduce a training-free\\nPersonalization approach for SAM, termed PerSAM. Given only one-shot data,\\ni.e., a single image with a reference mask, we first obtain a positive-negative lo-\\ncation prior for the target concept in new images. Then, aided by target visual\\nsemantics, we empower SAM for personalized object segmentation via two pro-\\nposed techniques: target-guided attention and target-semantic prompting. In this\\nway, we can effectively customize the general-purpose SAM for private use without\\nany training. To further alleviate the ambiguity of segmentation scales, we present\\nan efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\\nintroduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\\ntunes 2 parameters within 10 seconds for improved performance. To demonstrate\\nour efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\\nobject segmentation, and also test our methods on various one-shot image and\\nvideo segmentation benchmarks. Besides, we propose to leverage PerSAM to\\nimprove DreamBooth for personalized text-to-image synthesis. By mitigating\\nthe disturbance of training-set backgrounds, our approach showcases better target\\nappearance generation and higher fidelity to the input text prompt. Code is released\\nat https://github.com/ZrrSkywalker/Personalize-SAM.\\n(1) User provides\\n(2) One-shot Learning\\n(3)\\nPersonalized Segmentation\\nOne Image\\nOne Mask\\nTraining-free\\nFine-tuning\\nPerSAM\\nPerSAM-F\\n... in various poses or scenes\\nFigure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\\n(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\\nwe introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\\n* Equal contribution. † Corresponding author.\\n1\\n2 Parameters\\nPerSAM\\nPerSAM-F\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nFine-tune\\n10 Seconds\\nThe hat on a\\nteddy bear\\nA photo of a [V] cat\\n\"A [V] cat on a beach\"\\n... in various poses or scenes\\nFine-tune\\nThe body of a\\nrobot toy\\n***\\nFigure 2: Personalized Segmentation Exam-\\nples. Our PerSAM (Left) can segment personal\\nobjects in any context with favorable perfor-\\nmance, and PerSAM-F (right) further alleviates\\nthe ambiguity issue by scale-aware fine-tuning.\\n1\\nINTRODUCTION\\nA photo of a [V] backpack\\n\"A [V] backpack on a table of a classroom\"\\nFigure 3: Improving DreamBooth (Ruiz et al.,\\n2022) with PerSAM. By mitigating the distur-\\nbance of backgrounds during training, our ap-\\nproach can help to achieve higher-quality person-\\nalized text-to-image generation.\\nFoundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\\net al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\\nJia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\\nof large-scale datasets and computational resources. They demonstrate extraordinary generalization\\ncapacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\\nInspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\\n11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\\ndefines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\\nreturning the expected mask, which allows for segmenting any objects in visual contexts.\\nHowever, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\\nto crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\\nbedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\\neach image, you must precisely find the target object within complicated contexts, and then activate\\nSAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\\nautomatically segment user-designated visual concepts in a simple and efficient manner?\\nTo this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\\nModel. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\\na user-provided reference image and a rough mask of the personal concept. Specifically, we first\\nobtain a location confidence map for the target object in the test image by feature similarities, which\\nconsiders the appearance of every foreground pixel. According to confidence scores, two points are\\nselected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\\ninto SAM\\'s decoder for segmentation. Within the decoder, we propose to inject visual semantics of\\nthe target object to unleash SAM\\'s personalized segmentation power with two techniques:\\n•\\n• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM\\'s\\ndecoder by the location confidence map. This explicitly compels the prompt tokens to\\nmainly concentrate on foreground target regions for intensive feature aggregation.\\nTarget-semantic Prompting. To explicitly provide SAM with high-level target semantics,\\nwe fuse the original prompt tokens with the embedding of the target object, which provides\\nthe low-level positional prompt with additional visual cues for personalized segmentation.\\nWith the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\\npersonalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\\nour approach can cope well with scenarios that require segmenting one object among multiple similar\\nones, simultaneously segmenting several identical objects in the same image, or tracking different\\nobjects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\\n2\\nwhere the object comprises visually distinct subparts or hierarchical structures to be segmented,\\ne.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\\nPerSAM in determining the appropriate scale of mask as output, since both the local part and the\\nglobal shape can be regarded as valid masks by SAM.\\nTo alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\\nfreeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\\nwithin 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\\nsegmentation results of different mask scales. To adaptively select the best scale for varying objects,\\nwe employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\\nfinal output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\\ndata and exhibits better segmentation accuracy shown in Figure 2 (Right).\\nMoreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\\nfine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\\nfew images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\\nto convert these images into an identifier [V] in the word embedding space, which, however, can\\nsimultaneously include the background information, e.g., stairs or the forest. This would override\\nthe newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\\npropose to leverage PerSAM to segment the target object within training images, and only supervise\\nDreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\\nWe summarize the contributions of our paper as follows:\\n• Personalized Object Segmentation. We first investigate how to customize a general-\\npurpose segmentation model (SAM) into personalized scenarios with minimal expense. To\\nthis end, we introduce two efficient and effective methods, along with a new segmentation\\ndataset, PerSeg, for the evaluation of personalized object segmentation.\\n• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\\nSAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\\nfine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\\n• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\\none-shot part and semantic segmentation, and video object segmentation. In addition,\\nPerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\\n2 RELATED WORK\\nFoundation Models. With powerful generalization capacity, pre-trained foundation models can be\\nadapted for various downstream scenarios and attain promising performance. In natural language\\nprocessing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\\n2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\\ndemonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\\nspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\\ncontrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\\nPainter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\\nprompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\\n2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\\nlow-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\\nsegmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\\nThere are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\\nfaster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\\n3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\\nHuang et al., 2023) image processing. From another perspective, we propose to personalize the\\nsegmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\\ninto a specialist with only one shot. Our method can also assist the personalization of text-to-\\nimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\\n2022), which improves the generation quality by segmenting the foreground target objects from the\\nbackground disturbance.\\n3\\nLarge Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\\net al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\\nrequires a pixel-level comprehension of a image. Various segmentation-related tasks have been\\nexplored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\\nnarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\\net al., 2020); instance segmentation, focusing on the identification of individual object instances (He\\net al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\\nlabels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\\ninvolving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\\nby language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\\nhave proposed large-scale vision models for image segmentation. They are pre-trained by extensive\\nmask data and exhibit strong generalization capabilities on numerous image distributions. Segment\\nAnything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\\ntation to learn a promptable segmentation framework, which generalizes to downstream scenarios\\nin a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\\nrobust in-context learning paradigm and can segment any images by a given image-mask prompt.\\nSEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\\nreferences, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\\nintroduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\\nfor evaluation. Instead of developing large segmentation models, our goal is to personalize them to\\nsegment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\\nPerSAM-F, which efficiently customize SAM for personalized segmentation.\\nParameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\\ntasks can be computationally expensive and memory-intensive, posing challenges for resource-\\nconstrained applications. To address this issue, recent works have focused on developing parameter-\\nefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\\nfreeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\\ntuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\\nlearnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\\nmore competitive performance with scale and robust domain transfer compared to full model tuning.\\nLow-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\\net al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\\nwhich significantly reduces the number of learnable parameters required for downstream tasks.\\nAdapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\\nto be inserted between layers of the original transformer, introducing lightweight MLPs for feature\\ntransformation. Different from existing works, we adopt a more efficient adaption method delicately\\ndesigned for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\\nseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\\nof segmentation scale with superior performance.\\n3 METHOD\\nIn Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\\nintroduce the task definition for personalized object segmentation. Then, we illustrate the methodology\\nof our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\\nto assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\\n3.1\\nPERSONALIZED OBJECT SEGMENTATION\\nA Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\\nencoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\\npromptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\\na box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\\nadopts Encp to encode the human-given prompts of a length k into prompt tokens as\\nFI = Enc(I), Tp = Encp(P),\\n(1)\\n4\\nF₁\\nEncode\\nCosine Similarity\\n{FT}=1\\nTest Image I\\nTarget\\nLocal\\nFeatures\\n{T}=1\\nFR\\nMR° FR\\nEncode\\nPerSAM\\'s Decoder\\nTarget-guided\\nAttention\\nImage-to-Token\\nCross-Attention\\nTarget-semantic\\nPrompting\\n↑\\n{Si)=1\\nLocal Confidence Maps\\nModulate\\nToken-to-Image\\nCross-Attention\\n↑\\nAggregate\\nAttention Matrix A\\nLocal\\nFeatures\\n{T}=1\\nToken\\nSelf-Attention\\nOverall\\nConfidence Map S\\nAggregate\\n↑\\na\\nConcat(\\n+ Repeat(\\n)\\nOverall\\nConfidence Map S\\nTM Тр\\n× 2\\nTR\\nOne-shot Image IR\\nOne-shot Mask MR\\nPositive Prior\\nNegative Prior\\nFigure 4: Positive-negative Location Prior.\\nWe calculate a location confidence map for the\\ntarget object in new test image by the appear-\\nance of all local parts. Then, we select the loca-\\ntion prior as the point prompt for PerSAM.\\nLow-level\\nPositional Prompt\\nHigh-level\\nSemantic Prompt\\nFigure 5: Target-guided Attention (Left) &\\nTarget-semantic Prompting (Right). To in-\\nject SAM with target semantics, we explicitly\\nguide the cross-attention layers, and propose\\nadditional prompting with high-level cues.\\nwhere F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\\nc denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\\nDecм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\\nconcatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\\ntokens are responsible for generating the mask output, formulated as\\nM =\\nDecм (FI, Concat(TM,Tp)),\\nwhere M denotes the final segmentation mask predicted by SAM.\\n(2)\\nTask Definition. Although SAM is generalized enough for any object by prompting, it lacks the\\nability to automatically segment specific subject instances. Considering this, we define a new task\\nfor personalized object segmentation. The user provides only a single reference image, and a mask\\nindicating the target visual concept. The given mask can either be an accurate segmentation, or a\\nrough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\\nwithin new images or videos, without additional human prompting. For evaluation, we annotate a\\nnew dataset for personalized segmentation, named PerSeg. The raw images are collected from the\\nworks for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\\ncontaining various categories of visual concepts in different poses or scenes. In this paper, we propose\\ntwo efficient solutions for this task, which we specifically illustrate as follows.\\n3.2\\nTRAINING-FREE PERSAM\\nLocation Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\\nfirst obtains a confidence map that indicates the location of the target object in the new test image\\nI. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\\nThe encoder can be SAM\\'s frozen backbone or other pre-trained vision models, for which we adopt\\nSAM\\'s image encoder Enc, by default. We formulate the process as\\nFI= Enc(I), FR = EncI(IR),\\n(3)\\nwhere F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\\nof foreground pixels within the visual concept from FR, resulting in a set of n local features as\\n{T}}=1 = MR ○ Fr,\\n(4)\\n5\\nwhere T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\\nmaps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\\n{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\\n(5)\\nNote that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\\nprobability for a different local part of object in the test image, such as the head, the body, or the\\npaws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\\nthe overall confidence map of the target object as\\nS =\\nn\\nn\\nSiЄRhxw\\n(6)\\nBy incorporating the confidences of every foreground pixel, S can take the visual appearance of\\ndifferent object parts into consideration, and acquire a relatively comprehensive location estimation.\\nPositive-negative Location Prior. To provide PerSAM with a location prior on the test image,\\nwe select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\\nrespectively. The former represents the most likely center position of the target object, while the\\nlatter inversely indicates the background. Then, they are regarded as the positive and negative point\\nprompts, and fed into the prompt encoder as\\nTp = Encp(Ph, P₁) Є R2×c,\\n(7)\\nwhich denote the prompt tokens for SAM\\'s decoder. In this way, SAM would tend to segment the\\ncontiguous region surrounding the positive point, while discarding the negative one\\'s on the image.\\nTarget-guided Attention. Although the positive-negative point prompt has been obtained, we\\nfurther propose a more explicit semantic guidance to the cross-attention operation in SAM\\'s decoder,\\nwhich concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\\nthe overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\\nconcept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\\nto guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\\nwe denote every attention map after the softmax function as A = Rhxw, and then modulate its\\nattention distribution by\\nA9\\n=\\nsoftmax A+ a softmax(S)\\n•\\n(8)\\nwhere a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\\nto capture more visual semantics associated with the target subject, other than the unimportant\\nbackground area. This contributes to more effective feature aggregation in attention mechanisms, and\\nenhances the final segmentation accuracy of PerSAM in a training-free manner.\\nTarget-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\\ninformation, such as the coordinate of a point or a box. To provide SAM\\'s decoder with more high-\\nlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level\\nsemantic prompting. We first obtain the global embedding TR of the object in the reference image by\\nboth I average pooling between different local features as\\nn\\nTR\\n=\\nΣΤΑ €R1xc\\nn\\ni=1\\nThen, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\\nthem into the decoder block, which is shown in Figure 5 as\\nT9 = Repeat (TR) + Concat(TM,Tp),\\n(10)\\nwhere T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\\noperation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\\nis not only prompted by low-level location points, but also high-level target visual cues.\\n6\\nTest Image\\nPerSAM\\nOutput\\nThree\\nM1\\nM2\\n+\\nM3\\nscales\\nRandom\\nNoise\\n→>\\nDreamBooth\\n→>\\n↑\\n\"a [V] cat\"\\nReconstruction\\nLoss\\nW1\\nF\\nW2\\n1- W1\\n+\\nW₂ )\\n1\\nUser provides\\nOutput Mask\\nM\\nFine-tune\\nWeighted\\nSummation\\nFreeze\\nPerSAM\\nBackground\\nDisturbance\\nDecouple\\nFigure 6: The Scale-aware Fine-tuning in\\nPerSAM-F. To alleviate the scale ambiguity,\\nPerSAM-F adopts two learnable weights for\\nadaptively aggregating three-scale masks.\\nFigure 7: PerSAM-assisted DreamBooth.\\nWe utilize PerSAM to decouple the target ob-\\njects from the background for improving the\\ngeneration of DreamBooth.\\nCascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\\non the test image from SAM\\'s decoder, which however, might include rough edges and isolated\\nbackground noises. For further refinement, we iteratively feed the mask back into the decoder Decм\\nfor a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\\nmask along with the previous positive-negative point prompt. For the second step, we acquire the\\nbounding box enclosing the mask from the first step, and prompt the decoder additionally with this\\nbox for more accurate object localization. As we only iterate the lightweight decoder without the\\nlarge-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\\n3.3\\nFINE-TUNING OF PERSAM-F\\nAmbiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\\ntory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\\nto the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\\nof two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\\nat the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\\nin a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\\nSAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\\nmasks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\\nrequired to manually select one mask out of three, which is effective but consumes extra manpower.\\nIn contrast, our personalized task aims to customize SAM for automatic object segmentation without\\nthe need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\\nby parameter-efficient fine-tuning.\\nScale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\\nfine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\\nfirst follows PerSAM to obtain the location prior, and refers to SAM\\'s original solution to output\\nthree-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\\nmask weights, w₁, W2, and calculate the final mask output by a weighted summation as\\n.\\n.\\nM = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\\n(11)\\nwhere w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\\ntuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\\nthe entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\\nW1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\\nscale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\\nconcepts, improving the generalization capacity of PerSAM.\\n7\\nTable 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\\nblou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\\net al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\\nMethod\\nmIoU\\nbloU Param. Can Barn Clock Cat\\nBack- Teddy Duck Thin Red\\npack Bear Toy Bird Cartoon\\nRobot\\nToy\\nPainter\\nVP\\nSEEM*\\nSegGPT*\\n56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\\n65.9 25.5 383M 61.2\\n58.6\\n87.1 55.7 341M 65.4 82.5\\n94.3 76.5 354M\\n93.0\\n33.3 20.9\\n98.2\\n65.0\\n59.2\\n76.6\\n66.7\\n79.8\\n89.9\\n67.4\\n81.0\\n72.4\\n72.4\\n91.1\\n94.1\\n95.2\\n98.0\\n71.3\\n97.0\\n95.8\\n96.6 63.8\\n92.6\\n94.1\\n94.4\\n93.7\\n97.2\\n92.6\\n97.3\\n96.2\\n0\\n90.70 95.39\\n2\\n96.2 38.9 96.2\\n96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\\n94.6\\n97.3\\n93.7\\n97.0\\n60.6\\n97.1\\n96.7\\nPerSAM 89.3 71.7\\nPerSAM-F 95.3 77.9\\nTable 2: Video Object Segmen-\\ntation on DAVIS 2017 val (Pont-\\nTuset et al., 2017). We utilize\\ngray color to denote the methods\\ninvolving in-domain training.\\nTable 3: One-shot Semantic and Part Segmentation on FSS-\\n1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\\nPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\\n2023). We report the mIoU scores and utilize gray color to\\ndenote the methods involving in-domain training.\\nOne-shot Semantic Seg.\\nFSS-1000 LVIS-92²\\nOne-shot Part Seg.\\nPASCAL-Part\\nPainter\\nSEEM\\nSegGPT\\nPerSAM\\nMethod\\nJ&F І\\nAGSS\\n67.4 64.9 69.9\\nAFB-URR 74.6 73.0 76.1\\n34.6 28.5 40.8\\n58.9 55.0 62.8\\n75.6 72.5 78.6\\nF\\nMethod\\nPACO-Part\\nHSNet\\n86.5\\n17.4\\n32.4\\n22.6\\nVAT\\n90.3\\n18.5\\n33.6\\n23.5\\nPainter\\n61.7\\n10.5\\n30.4\\n14.1\\nSegGPT\\n85.6\\n18.6\\n66.9 71.3 75.1\\nPerSAM\\n81.6\\n15.6\\n32.5\\n22.5\\nPerSAM-F 76.1 74.9 79.7\\nPerSAM-F\\n86.3\\n18.4\\n32.9\\n22.7\\n3.4\\nPERSAM-ASSISTED DREAMBOOTH\\nFor personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\\ndiffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\\ncat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\\nimages. This This would inject the redundant background information in the training images into the\\nidentifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\\nof backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\\nPerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\\nbelonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\\nvisual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\\nassisted DreamBooth can not only synthesize the target object with better visual correspondence, but\\nalso increase the diversity of the new backgrounds guided by the input text prompt.\\n4 EXPERIMENT\\nWe first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\\nwith various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\\neffectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\\nablation studies to investigate our designs on PerSeg in Section 4.4.\\n4.1 PERSONALIZED EVALUATION\\nPerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\\ntermed PerSeg. The raw images are collected from the training data of subject-driven diffusion\\nworks (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\\ncategories in total, including daily necessities, animals, and buildings. In different poses or scenes,\\neach object is associated with 5~7 images and masks, where we fix one image-mask pair as the\\nuser-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\\nPlease refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\\n8\\nThe ring\\non a clock\\n2\\nThe teapot\\non a tray\\nThe backpack\\ncarried by a\\nThe top part\\nof a can\\nwoman\\nFigure 8: Visualization of PerSAM-F\\'s Im-\\nprovement. Our scale-aware fine-tuning can\\nwell alleviate the scale ambiguity of PerSAM.\\nFigure 9: Visualization of Video Object Seg-\\nmentation. Our approach performs well for\\nsegmenting multiple objects in a video.\\nPerformance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\\neffectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\\ntion of PerSAM-F\\'s improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\\net al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\\ncan also segment objects according to the given one-shot prompt data. As shown, the training-free\\nPerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\\nBy the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\\n+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\\ngeneralists, our method is specially designed for personalized object segmentation, and exhibits much\\nmore efficiency in both time and computational resources.\\n4.2 EXISTING SEGMENTATION BENCHMARKS\\nVideo Object Segmentation. Given the first-frame image and object masks, our PerSAM and\\nPerSAM-F achieve competitive object segmentation and tracking performance on the validation\\nset of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\\nvideo training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\\nPerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\\napproach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\\nvideo data. The results fully illustrate our strong generalization ability for temporal video data and\\ncomplex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\\nOne-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\\nimage segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\\n2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\\nfollow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\\nattains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\\net al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\\nHSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\\nsegmentation, but also works for category-wise and part-wise personalization of SAM.\\n4.3\\nPERSAM-ASSISTED DREAMBOOTH\\nWe follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\\nDiffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\\nvisualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\\nsofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\\nAssisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\\ncorresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\\nthe background disturbance to correctly generate the “forest” and “blue sky\".\\n9\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nUser provides\\nDreamBooth\\nAssisted by PerSAM\\nA photo of a dog\\n\"A [V] dog in a jungle\"\\n...\\nA photo of a barn\\n\"A [V] barn with a forest in the background\"\\n\"A [V] dog in snow\"\\n\"A [V] barn with blue sky in the background\"\\nFigure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\\n2022) can better preserve the diversity for synthesizing various contexts in new images.\\nTable 4: Ablation of Main Com-\\nponents in our proposed method.\\nVariant\\nTable 5: Ablation of Different\\nFine-tuning Methods.\\nTable 6: Ablation of using\\nBox-image as Reference.\\nmIoU Gain\\n69.1\\nMethod\\nPerSAM\\nParam. mIoU\\nMethod\\nMask Box\\n0\\n89.32\\nPainter\\n56.4 42.0\\n+ Post-refinement\\n72.5 +3.4\\n83.9 +11.4\\nPrompt Tuning\\n12K\\n76.5\\nVP\\n65.9\\n38.1\\nAdapter\\n196K\\n78.3\\nSEEM\\n87.1\\n64.9\\n+ Guided Attention\\n+ Semantic Prompt\\n85.8 +1.9\\nLORA\\n293K\\n90.0\\nSegGPT\\n94.3 36.0\\n89.3\\n+3.5\\n3 Mask Weights\\n3\\n92.9\\n+ Scale Tuning\\n95.3\\n+6.0\\nPerSAM-F\\n2\\n95.3\\nPerSAM 89.3\\nPerSAM-F 95.3\\n88.1\\n94.9\\nPositive Prior\\n+ Negative Prior\\n4.4\\nABLATION STUDY\\nMain Components. In Table 4, we investigate our different components by starting from a baseline\\nthat only adopts the positive location prior. Then, we add the negative point prompt and cascaded\\npost-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\\nhigh-level target semantics into SAM\\'s decoder for attention guidance and semantic prompting. The\\nresulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\\nscale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\\nDifferent Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\\n(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\\nand LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\\ninto every transformer block in PerSAM\\'s decoder. As shown, the prompt tuning and Adapter would\\nover-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\\nbest improve the performance of PerSAM, while tuning the least learnable parameters.\\nUsing Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\\nsome users. In Table 6, we relax the input restrictions to a bounding box designating the expected\\nobject. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\\nthe one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\\nPerSAM and PerSAM-F, but severely influences other methods.\\n5 CONCLUSION\\nIn this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\\nwith only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\\ninto SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\\nPerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\\nscales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\\nof our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\\nour work may expand the applicability of SAM to a wider: range of scenarios.\\n10\\n10\\nREFERENCES\\nVijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\\ndecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 39(12):2481–2495, 2017.\\nAmir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\\nvia image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\nJiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\\nSegment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\\nconnected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\\n2017.\\nXi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\\nsegmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n7345-7354, 2021.\\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\\ntransformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\\nBowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\\niou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\\nattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\\nPedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\\nhugging face.co/blog/lora, January 2023.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\\ninversion. arXiv preprint arXiv:2208.01618, 2022.\\nAgrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\\nsegmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\npp. 5356-5364, 2019.\\nYuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\\nZhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\\nedge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n1551-1560, 2021.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\\na unified view of parameter-efficient transfer learning. In International Conference on Learning\\nRepresentations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\\nLukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\\narXiv preprint arXiv:2211.10155, 2022.\\n11\\nSunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\\nwith 4d convolutional swin transformer for few-shot segmentation. In European Conference on\\nComputer Vision, pp. 108–126. Springer, 2022.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\\nnlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\\narXiv:2106.09685, 2021.\\nYuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\\nJiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\\narXiv:2304.14660, 2023.\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\\nnoisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\\n2021.\\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\\nSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\\nSpringer, 2022.\\nZhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\\ntotypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\\non Computer Vision, pp. 36–54. Springer, 2022.\\nZhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\\nChengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\\nsegmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\\n2023.\\nLei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\\nSegment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\\nAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\\ntation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n9404-9413, 2019.\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\\narXiv:2304.02643, 2023.\\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\\ncustomization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691, 2021.\\nHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\\nXiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\\nand vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\\n2023.\\nXiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\\ndataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2869-2878, 2020.\\n12\\nYanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\\nAttention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\\nYongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\\nbank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\\n3430-3441, 2020.\\nHuaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\\nsegmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\\n3949-3957, 2019.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\\nZiyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\\nYu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\\nConference on Computer Vision, pp. 388-404. Springer, 2022.\\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\\nP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\\narXiv preprint arXiv:2110.07602, 2021.\\nYang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\\nanything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\\n2023.\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npp. 3431-3440, 2015.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\\nrepresentations for vision-and-language tasks. In Advances in Neural Information Processing\\nSystems (NeurIPS), pp. 13–23, 2019.\\nJun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\\nJuhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\\nProceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\\nKeval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\\nsemantic part. arXiv preprint arXiv:2007.02419, 2020.\\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\\n2020.\\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\\nLuc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\\narXiv:1704.00675, 2017.\\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\\narXiv preprint arXiv:2104.06599, 2021.\\nAlec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\\n2018.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n13\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\\nmodels from natural language supervision. In International conference on machine learning, pp.\\n8748-8763. PMLR, 2021.\\nVignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\\nobjects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n7141-7151, 2023.\\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. Advances in Neural information processing systems, 30, 2017.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\\npreprint arXiv:2208.12242, 2022.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. Advances in Neural Information\\nProcessing Systems, 35:36479-36494, 2022.\\nLin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\\nNanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\\nInformation Processing Systems, 33:3991-4002, 2020.\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\\nvision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pp. 5227–5237, 2022.\\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\\nEuropean Conference on Computer Vision, pp. 282–298. Springer, 2020.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\\nmodels. arXiv preprint arXiv:2302.13971, 2023.\\nXinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\\ninstance segmentation. Advances in Neural information processing systems, 33:17721–17732,\\n2020.\\nXinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\\nXinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\\nSegmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\\nSimple and efficient design for semantic segmentation with transformers. Advances in Neural\\nInformation Processing Systems, 34:12077-12090, 2021.\\nMutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\\ngeometry-disentangled representation for complementary understanding of 3d object point cloud.\\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\\nSegment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\\n14\\nChaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\\nChoong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\\narXiv preprint arXiv:2306.14289, 2023a.\\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\\narXiv:2303.10512, 2023b.\\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\\narXiv preprint arXiv:2303.16199, 2023c.\\nRenrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\\nPeng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\\nlearners. arXiv preprint arXiv:2303.02151, 2023d.\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\\n2881-2890, 2017.\\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\\nFast segment anything. arXiv preprint arXiv:2306.12156, 2023.\\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\\nsequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6881–6890, 2021.\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\\nlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\\neverything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\\n15\\n115\\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 2023년 10월 4일 원샷으로 세그먼트 무엇이든 모델을 개인화 Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 상하이 인공지능 연구실 3 중국과학원 자동화 연구소 4페킹대학교 컴퓨터과학대학 CFCS {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk 초록 대용량 데이터 사전 학습을 통해 구동되는 세그먼트 무엇이든 모델(SAM)은 강력한 프롬프트 가능한 프레임워크는 세분화 분야에 혁명을 일으켰습니다. 일반성에도 불구하고, 인력 프롬프트 없이 특정 시각적 개념에 대한 SAM을 사용자 정의하는 것은 충분히 탐구되지 않았습니다. 예를 들어, 수많은 이미지에서 반려견을 자동으로 세분화합니다. 이 논문에서는 SAM에 대한 훈련 없는 개인화 접근 방식인 PerSAM을 소개합니다. 참조 마스크가 있는 단일 이미지인 원샷 데이터만 주어지면 먼저 새 이미지에서 대상 개념에 대한 양수-음수 위치 사전을 얻습니다. 그런 다음 대상 시각적 의미론의 도움을 받아 두 가지 제안된 기술인 대상 유도 주의와 대상 의미 프롬프트를 통해 SAM이 개인화된 객체 세분화를 수행할 수 있도록 합니다. 이런 식으로 훈련 없이도 범용 SAM을 개인용으로 효과적으로 사용자 정의할 수 있습니다. 세분화 규모의 모호성을 더욱 완화하기 위해 효율적인 원샷 미세 조정 변형인 PerSAM-F를 제시합니다. SAM 전체를 동결하여 다중 스케일 마스크를 집계하기 위한 스케일 인식 미세 조정을 도입합니다.이는 10초 이내에 2개의 매개변수만 조정하여 성능을 개선합니다.효능을 입증하기 위해 개인화된 객체 분할을 평가하기 위한 새로운 데이터 세트인 PerSeg를 구성하고 다양한 원샷 이미지 및 비디오 분할 벤치마크에서 방법을 테스트합니다.또한 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 개선하기 위해 PerSAM을 활용할 것을 제안합니다.훈련 세트 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 나은 대상 모양 생성과 입력 텍스트 프롬프트에 대한 더 높은 충실도를 보여줍니다.코드는 https://github.com/ZrrSkywalker/Personalize-SAM에서 공개됩니다.(1) 사용자가 제공합니다.(2) 원샷 학습 (3) 개인화된 분할 하나의 이미지 하나의 마스크 훈련 없는 미세 조정 PerSAM PerSAM-F ... 다양한 포즈나 장면에서 그림 1: 세그먼트 무엇이든 모델의 개인화. 영어: 우리는 특정 시각적 개념(예: 반려견)에 대해 Segment Anything Model(SAM)(Kirillov et al., 2023)을 사용자 정의합니다.단일 샷 데이터로 두 가지 효율적인 솔루션을 소개합니다.훈련이 필요 없는 PerSAM과 미세 조정 PerSAM-F입니다.* 동등한 기여.† 책임 저자.1 2 매개변수 PerSAM PerSAM-F 사용자 제공 DreamBooth PerSAM 지원 미세 조정 10초 테디베어의 모자 [V] 고양이 사진 &quot;[V] 고양이가 있는 해변&quot; ... 다양한 포즈나 장면 미세 조정 로봇 장난감의 몸체 *** 그림 2: 개인화된 세분화 예.PerSAM(왼쪽)은 유리한 성능으로 모든 맥락에서 개인 사물을 세분화할 수 있으며, PerSAM-F(오른쪽)는 규모 인식 미세 조정을 통해 모호성 문제를 더욱 완화합니다. 1 서론 [V] 백팩 사진 &quot;교실 테이블 위의 [V] 백팩&quot; 그림 3: PerSAM을 이용한 DreamBooth 개선(Ruiz et al., 2022). 훈련 중 배경의 방해를 완화함으로써, 우리의 접근 방식은 더 높은 품질의 개인화된 텍스트-이미지 생성을 달성하는 데 도움이 될 수 있습니다. 시각(Li et al., 2022; Zou et al., 2023; Wang et al., 2022), 언어(Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019) 및 다중 모달리티(Radford et al., 2021; Jia et al., 2021; Li et al., 2023)의 기초 모델은 대규모 데이터 세트와 계산 리소스의 가용성에 기인하여 전례 없는 보급을 얻었습니다. 그들은 제로샷 시나리오에서 놀라운 일반화 능력을 보여주고, 인간의 피드백을 통합한 다재다능한 상호 작용을 보여줍니다. 여기에서 영감을 얻은 Segment Anything(Kirillov et al., 2023)은 1,100만 개의 이미지 마스크 데이터를 수집하기 위한 섬세한 데이터 엔진을 개발하고, 이후 SAM이라고 알려진 세분화 기반 모델을 훈련합니다. 그것은 새로운 프롬프트 가능 세분화 프레임워크를 정의합니다. 즉, 수작업 프롬프트를 입력으로 받고 예상 마스크를 반환하여 시각적 맥락에서 모든 객체를 세분화할 수 있습니다. 그러나 SAM은 본질적으로 특정 시각적 개념을 세분화하는 기능을 잃습니다. 두꺼운 사진 앨범에서 사랑스러운 애완견을 자르거나 침실 사진에서 사라진 시계를 찾으려고 한다고 상상해보세요. 바닐라 SAM을 사용하면 매우 노동 집약적이고 시간이 많이 걸립니다. 각 이미지에서 복잡한 맥락에서 대상 객체를 정확하게 찾은 다음 세분화를 위한 적절한 프롬프트로 SAM을 활성화해야 합니다. 이를 고려하여 다음과 같은 질문이 생깁니다. 간단하고 효율적인 방식으로 사용자가 지정한 시각적 개념을 자동으로 세분화하도록 SAM을 개인화할 수 있을까요? 이를 위해 Segment Anything Model을 위한 훈련이 필요 없는 개인화 접근 방식인 PerSAM을 소개합니다.그림 1에서 볼 수 있듯이, 저희의 방법은 사용자가 제공한 참조 이미지와 개인 개념의 대략적인 마스크인 원샷 데이터만 사용하여 SAM을 효율적으로 사용자 정의합니다.특히, 저희는 먼저 모든 전경 픽셀의 모양을 고려하는 특징 유사성에 의해 테스트 이미지의 대상 객체에 대한 위치 신뢰도 맵을 얻습니다.신뢰도 점수에 따라 두 지점이 양수-음수 위치 사전으로 선택되고, 이는 최종적으로 프롬프트 토큰으로 인코딩되어 세분화를 위해 SAM의 디코더에 입력됩니다.디코더 내에서 저희는 두 가지 기술을 사용하여 대상 객체의 시각적 의미론을 주입하여 SAM의 개인화된 세분화 능력을 최대한 발휘하도록 제안합니다.• • 대상 유도 주의. 저희는 위치 신뢰도 맵에 따라 SAM의 디코더에서 모든 토큰-이미지 교차 주의 계층을 안내합니다.이는 프롬프트 토큰이 집중적인 특징 집계를 위해 주로 전경 대상 영역에 집중하도록 명시적으로 강제합니다.대상 의미적 프롬프팅. SAM에 고수준 대상 의미론을 명시적으로 제공하기 위해 원래 프롬프트 토큰을 대상 객체의 임베딩과 융합하여 저수준 위치 프롬프트에 개인화된 분할을 위한 추가 시각적 단서를 제공합니다. 앞서 언급한 디자인과 계단식 사후 정제를 통해 PerSAM은 다양한 포즈나 장면에서 고유한 피사체에 대해 유리한 개인화된 분할 성능을 보여줍니다. 특히, 우리의 접근 방식은 여러 유사한 객체 중에서 하나의 객체를 분할하거나, 동일한 이미지에서 여러 동일한 객체를 동시에 분할하거나, 비디오를 따라 다른 객체를 추적해야 하는 시나리오에 잘 대처할 수 있습니다. 그럼에도 불구하고 그림 2에서 볼 수 있듯이 객체가 시각적으로 구별되는 하위 부분 또는 분할할 계층 구조로 구성된 경우(예: 테디베어 위의 모자 또는 로봇 장난감의 머리) 가끔 실패 사례가 있을 수 있습니다. 이러한 모호성은 PerSAM이 출력으로 적절한 마스크 크기를 결정하는 데 어려움을 줍니다. 로컬 부분과 글로벌 모양이 모두 SAM에서 유효한 마스크로 간주될 수 있기 때문입니다. 이 문제를 완화하기 위해, 우리는 PerSAM-F라는 우리 접근 방식의 미세 조정 변형을 추가로 제안합니다. 우리는 다재다능한 사전 훈련된 지식을 보존하기 위해 전체 SAM을 동결하고, 단일 A100 GPU에서 10초 이내에 2개의 매개변수만 미세 조정합니다. 자세히 말하면, 우리는 SAM이 다양한 마스크 스케일의 여러 가지 잠재적인 분할 결과를 생성할 수 있도록 합니다. 다양한 객체에 가장 적합한 스케일을 적응적으로 선택하기 위해, 우리는 각 마스크 스케일에 대해 학습 가능한 상대적 가중치를 사용하고, 최종 출력으로 가중 합산을 수행합니다. 이러한 효율적인 스케일 인식 훈련을 통해, PerSAM-F는 원샷 데이터에서 과적합을 피하고 그림 2(오른쪽)에 표시된 더 나은 분할 정확도를 보여줍니다. 또한, 우리의 접근 방식이 DreamBooth(Ruiz et al., 2022)가 그림 3에서 보듯이 개인화된 텍스트-이미지 생성을 위한 확산 모델을 보다 잘 미세 조정할 수 있도록 도울 수 있다는 것을 관찰했습니다. 애완 고양이 또는 백팩과 같은 특정 시각적 개념이 포함된 몇 개의 이미지가 주어지면 DreamBooth는 이러한 이미지를 단어 임베딩 공간의 식별자[V]로 변환하는 방법을 학습하지만 동시에 계단이나 숲과 같은 배경 정보를 포함할 수 있습니다. 이렇게 하면 새로 프롬프트된 배경이 무시되고 대상 모양 생성이 방해받습니다. 따라서 우리는 PerSAM을 활용하여 훈련 이미지 내에서 대상 개체를 분할하고 전경 영역으로만 DreamBooth를 감독하여 더 높은 품질의 텍스트-이미지 합성을 가능하게 하는 것을 제안합니다. 우리는 논문의 기여를 다음과 같이 요약합니다. • 개인화된 개체 분할. 우리는 먼저 최소한의 비용으로 범용 분할 모델(SAM)을 개인화된 시나리오로 사용자 지정하는 방법을 조사합니다. 이를 위해 개인화된 객체 분할을 평가하기 위한 새로운 분할 데이터 세트인 PerSeg와 함께 효율적이고 효과적인 두 가지 방법을 소개합니다.• PerSAM 및 PerSAM-F. PerSAM에서 대상 객체의 고수준 의미론에 따라 SAM을 안내하는 세 가지 무훈련 기술을 제안합니다. PerSAM-F에서 마스크 모호성 문제를 크게 완화하기 위해 10초 동안 2개의 매개변수를 사용하여 스케일 인식 미세 조정을 설계합니다.• 우리의 접근 방식은 PerSeg 벤치마크, 원샷 파트 및 의미 분할, 비디오 객체 분할을 포함한 다양한 작업에서 경쟁력 있는 결과를 얻습니다.또한 PerSAM은 더 나은 개인화된 텍스트-이미지 합성을 위해 DreamBooth를 향상시킬 수 있습니다.2 관련 작업 기초 모델. 강력한 일반화 용량을 통해 사전 훈련된 기초 모델을 다양한 다운스트림 시나리오에 맞게 조정하고 유망한 성능을 얻을 수 있습니다. 자연어 처리에서 BERT(Devlin 등, 2018; Lu 등, 2019), GPT 시리즈(Brown 등, 2020; OpenAI, 2023; Radford &amp; Narasimhan, 2018; Radford 등, 2019), LLAMA(Zhang 등, 2023c)는 놀라운 맥락 내 학습 능력을 보여주었으며, 도메인별 프롬프트를 통해 새로운 작업으로 전환될 수 있습니다. 마찬가지로 이미지-텍스트 쌍에 대한 대조 학습을 수행하는 CLIP(Radford 등, 2021)과 ALIGN(Jia 등, 2021)은 제로샷 시각 인식에서 뛰어난 정확도를 보여줍니다. Painter(Wang 등, 2022)는 다운스트림 미세 조정 없이 다양한 비전 작업을 수행하기 위해 네트워크 아키텍처와 맥락 내 프롬프트를 통합하는 비전 모델을 도입합니다. CaFo(Zhang et al., 2023d)는 다양한 기초 모델을 캐스케이드하고 사전 훈련된 지식을 협력하여 견고한 저데이터 이미지 분류를 수행합니다. SAM(Kirillov et al., 2023)은 10억 개의 마스크로 사전 훈련된 이미지 분할을 위한 기초 모델을 제시하고 프롬프트 기반 분할을 수행합니다. 고품질 분할(Ke et al., 2023), 더 빠른 추론 속도(Zhao et al., 2023; Zhang et al., 2023a), 모든 용도 매칭(Liu et al., 2023), 3D 재구성(Cen et al., 2023), 객체 추적(Yang et al., 2023), 의료(Ma &amp; Wang, 2023; Huang et al., 2023) 이미지 처리를 위해 SAM을 확장하는 몇 가지 동시 작업이 있습니다. 다른 관점에서, 우리는 특정 시각적 개념에 대한 세분화 기반 모델, 즉 SAM을 개인화하여 일반인을 단 한 번의 샷으로 전문가로 적응시키는 것을 제안합니다.우리의 방법은 또한 텍스트-이미지 기반 모델, 즉 Stable Diffusion(Rombach et al., 2022) 및 Imagen(Saharia et al., 2022)의 개인화를 지원할 수 있으며, 이는 전경 대상 객체를 배경 교란으로부터 세분화하여 생성 품질을 개선합니다.세그먼테이션의 3가지 대형 모델.컴퓨터 비전의 기본 작업인 세분화(Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)에는 이미지에 대한 픽셀 수준의 이해가 필요합니다. 다양한 분할 관련 작업이 탐색되었는데, 여기에는 각 픽셀을 사전 정의된 클래스 집합으로 분류하는 의미적 분할(Badrinarayanan 등, 2017; Chen 등, 2017; Zheng 등, 2021; Cheng 등, 2022; Xie 등, 2021; Song 등, 2020)이나 개별 객체 인스턴스의 식별에 초점을 맞춘 인스턴스 분할(He 등, 2017; Wang 등, 2020; Tian 등, 2020), 클래스 레이블과 인스턴스 식별을 모두 할당하는 전방위 분할(Kirillov 등, 2019; Li 등, 2019), 그리고 세분화를 위해 인간의 개입을 포함하는 대화형 분할(Hao 등, 2021; Chen 등, 2021) 등이 있습니다. 최근 언어 기반 모델(Zhang et al., 2023c; Brown et al., 2020)에서 영감을 받아 여러 동시 연구에서 이미지 분할을 위한 대규모 비전 모델이 제안되었습니다. 이 모델은 광범위한 마스크 데이터로 사전 학습되었으며 다양한 이미지 분포에 대해 강력한 일반화 기능을 보여줍니다. Segment Anything Model(SAM)(Kirillov et al., 2023)은 모델-인-더-루프 주석이 있는 데이터 엔진을 활용하여 프롬프트 가능 분할 프레임워크를 학습하는데, 이는 제로샷 방식으로 다운스트림 시나리오로 일반화됩니다. Painter(Wang et al., 2022)와 SegGPT(Wang et al., 2023)는 강력한 컨텍스트 내 학습 패러다임을 도입하고 주어진 이미지 마스크 프롬프트에 따라 모든 이미지를 분할할 수 있습니다. SEEM(Zou et al., 2023)은 언어 및 오디오와 같은 다중 모달 참조에 의해 촉발된 일반적인 분할 모델을 추가로 제시하여 다양한 의미 지식을 통합합니다. 이 연구에서는 개인화된 객체 분할이라는 새로운 작업을 도입하고 평가를 위해 새로운 데이터 세트 PerSeg에 주석을 달았습니다. 대규모 분할 모델을 개발하는 대신, 우리의 목표는 사용자가 제공한 객체를 모든 포즈나 장면에서 분할하도록 모델을 개인화하는 것입니다. 개인화된 분할을 위해 SAM을 효율적으로 사용자 지정하는 PerSAM과 PerSAM-F의 두 가지 접근 방식을 제안합니다. 매개변수 효율적인 미세 조정. 다운스트림 작업에서 전체 기초 모델을 직접 조정하는 것은 컴퓨팅 비용이 많이 들고 메모리 집약적일 수 있으며, 리소스가 제한된 애플리케이션에 어려움을 줄 수 있습니다. 이 문제를 해결하기 위해 최근 연구에서는 기초 모델의 가중치를 동결하고 미세 조정을 위한 소규모 모듈을 추가하는 매개변수 효율적 방법(Sung 등, 2022; He 등, 2022; Rebuffi 등, 2017; Qin &amp; Eisner, 2021)을 개발하는 데 중점을 두었습니다. 신속한 조정(Lester 등, 2021; Zhou 등, 2022; Jia 등, 2022; Liu 등, 2021)은 동결된 모델과 함께 학습 가능한 소프트 프롬프트를 사용하여 특정 다운스트림 작업을 수행하여 전체 모델 조정에 비해 규모와 강력한 도메인 전송으로 더 경쟁력 있는 성능을 달성하는 것을 제안합니다. 저랭크 적응(LORA)(Hu 등, 2021; Cuenca &amp; Paul, 2023; Zhang 등, 2023b; Hedegaard 등, 2022)은 학습 가능한 랭크 분해 행렬을 각 사전 학습된 가중치에 동시에 주입하여 다운스트림 작업에 필요한 학습 가능한 매개변수 수를 크게 줄입니다. 어댑터(Houlsby 등, 2019; Pfeiffer 등, 2020; Lin 등, 2020; Chen 등, 2022)는 원래 변환기의 레이어 사이에 삽입되도록 설계되어 특징 변환을 위한 경량 MLP를 도입합니다. 기존 작업과 달리 SAM을 위해 섬세하게 설계된 보다 효율적인 적응 방법, 즉 2개의 매개변수와 10초만 있는 PerSAM-F의 스케일 인식 미세 조정을 채택합니다. 이를 통해 원샷 데이터에서 과적합 문제를 효과적으로 방지하고 우수한 성능으로 세분화 규모의 모호성을 완화합니다.3 방법 3.1절에서 먼저 Segment Anything Model(SAM)(Kirillov et al., 2023)을 간략하게 다시 살펴보고 개인화된 객체 세분화를 위한 작업 정의를 소개합니다.그런 다음 각각 3.2절과 3.3절에서 PerSAM과 PerSAM-F의 방법론을 설명합니다.마지막으로 3.4절에서 DreamBooth(Ruiz et al., 2022)가 더 나은 텍스트-이미지 생성을 지원하도록 접근 방식을 활용합니다.3.1 개인화된 객체 세분화 Segment Anything 다시 살펴보기.SAM은 프롬프트 인코더, 이미지 인코더, 경량 마스크 디코더의 세 가지 구성 요소로 구성되며 각각 Encp, Enc, Decм로 표시합니다. 프롬프트 가능 프레임워크로서, SAM은 이미지 I와 프롬프트 집합 P(점, 상자 또는 거친 마스크)를 입력으로 받습니다. 구체적으로 SAM은 먼저 Enc를 사용하여 입력 이미지 특징을 얻고 Encp를 채택하여 길이가 k인 인간이 제공한 프롬프트를 FI = Enc(I), Tp = Encp(P)와 같이 프롬프트 토큰으로 인코딩합니다.(1) 4 F₁ 코사인 유사도 인코딩 {FT}=1 테스트 이미지 I 대상 로컬 특징 {T}=1 FR MR° FR PerSAM의 디코더 인코딩 대상 가이드 주의 이미지-토큰 교차 주의 대상 의미적 프롬프트 ↑ {Si)=1 로컬 신뢰 맵 토큰-이미지 교차 주의 변조 ↑ 집계 주의 행렬 A 로컬 특징 {T}=1 토큰 자체 주의 전체 신뢰 맵 S 집계 ↑ a Concat( + Repeat( ) 전체 신뢰 맵 S TM Тр × 2 TR 원샷 이미지 IR 원샷 마스크 MR 양의 사전 음의 사전 그림 4: 영어: Positive-negative Location Prior. 모든 로컬 파트의 출현에 따라 새로운 테스트 이미지에서 대상 객체에 대한 위치 신뢰도 맵을 계산합니다. 그런 다음 PerSAM에 대한 포인트 프롬프트로 위치 사전을 선택합니다. 저수준 위치 프롬프트 고수준 의미 프롬프트 그림 5: 대상 안내 주의(왼쪽) 및 대상 의미 프롬프트(오른쪽). 대상 의미론을 SAM에 주입하기 위해 교차 주의 계층을 명시적으로 안내하고 고수준 단서를 사용하여 추가 프롬프트를 제안합니다. 여기서 F₁ = Rhxwxc 및 Tp Є Rkxc, h, w는 이미지 피처 맵의 해상도를 나타내고 c는 피처 차원을 나타냅니다. 그 후 인코딩된 이미지와 프롬프트가 주의 기반 피처 상호 작용을 위해 디코더 Decм에 입력됩니다. SAM은 프롬프트 토큰 Tp에 접두사로 여러 학습 가능한 마스크 토큰 Tм을 연결하여 디코더의 입력 토큰을 구성합니다. 이러한 마스크 토큰은 M = Decм(FI, Concat(TM,Tp))로 공식화된 마스크 출력을 생성하는 역할을 하며, 여기서 M은 SAM에서 예측한 최종 분할 마스크를 나타냅니다. (2) 작업 정의. SAM은 프롬프트를 통해 모든 객체에 대해 충분히 일반화되지만 특정 주체 인스턴스를 자동으로 분할하는 기능이 부족합니다. 이를 고려하여 개인화된 객체 분할을 위한 새로운 작업을 정의합니다. 사용자는 단일 참조 이미지와 대상 시각적 개념을 나타내는 마스크만 제공합니다. 제공된 마스크는 정확한 분할이거나 즉석에서 그린 대략적인 스케치일 수 있습니다. 우리의 목표는 추가적인 인간의 프롬프트 없이 새 이미지나 비디오 내에서 지정된 객체를 분할하도록 SAM을 사용자 지정하는 것입니다. 평가를 위해 PerSeg라는 개인화된 분할을 위한 새로운 데이터 세트에 주석을 달았습니다. 원시 이미지는 다양한 포즈나 장면에서 다양한 범주의 시각적 개념을 포함하는 주체 중심 확산 모델(Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022)에 대한 작업에서 수집되었습니다. 이 논문에서 우리는 이 과제를 위한 두 가지 효율적인 솔루션을 제안하며, 구체적으로 다음과 같이 설명합니다. 3.2 학습 없는 PERSAM 위치 신뢰도 맵. 사용자가 제공한 이미지 IR과 마스크 MR을 조건으로, PerSAM은 먼저 새로운 테스트 이미지 I에서 대상 객체의 위치를 나타내는 신뢰도 맵을 얻습니다. 그림 4에서 볼 수 있듯이, 우리는 이미지 인코더를 적용하여 IR과 I의 시각적 특징을 추출합니다. 인코더는 SAM의 동결 백본 또는 다른 사전 학습된 비전 모델이 될 수 있으며, 우리는 기본적으로 SAM의 이미지 인코더 Enc를 채택합니다. 우리는 프로세스를 FI = Enc(I), FR = EncI(IR), (3)으로 공식화합니다. 여기서 F1, FR Є R¹xwxc입니다. 그런 다음 참조 마스크 MR Є Rhxwx¹를 사용하여 FR의 시각적 개념 내에서 전경 픽셀의 특징을 잘라내어 {T}}=1 = MR ○ Fr, (4) 5와 같이 n개의 로컬 특징 세트를 생성합니다. 여기서 T½ Є R¹×ª이고 ○는 공간적 곱셈을 나타냅니다. 그런 다음 T½와 테스트 이미지 특징 FĮ 사이의 코사인 유사도에 의해 각 전경 픽셀 i에 대한 n개의 신뢰도 맵을 계산합니다. {S² } }±₁ = {F₁TT) 11, 여기서 S₁ = Rhxw. (5) FI와 T½는 픽셀별로 L2-정규화되었습니다. 각 S²는 테스트 이미지에서 개체의 다른 로컬 부분(예: 개의 머리, 몸통 또는 발)에 대한 분포 확률을 나타냅니다. 이에 더하여, 우리는 모든 n개의 로컬 맵을 집계하여 S = n n SiЄRhxw (6)와 같이 대상 객체의 전반적인 신뢰도 맵을 얻기 위해 평균 풀링을 채택합니다.모든 전경 픽셀의 신뢰도를 통합함으로써, S는 다른 객체 부분의 시각적 모양을 고려하고 비교적 포괄적인 위치 추정을 얻을 수 있습니다.양의-음의 위치 사전.PerSAM에 테스트 이미지의 위치 사전을 제공하기 위해, 우리는 S에서 가장 높고 가장 낮은 신뢰도 값을 갖는 두 지점을 선택합니다.각각 Pɲ와 Pɩ로 표시합니다.전자는 대상 객체의 가장 가능성 있는 중심 위치를 나타내고, 후자는 반대로 배경을 나타냅니다.그런 다음, 이들은 양의 및 음의 지점 프롬프트로 간주되어 Tp = Encp(Ph, P₁) Є R2×c로 프롬프트 인코더에 입력됩니다.(7) 이는 SAM 디코더의 프롬프트 토큰을 나타냅니다. 이런 식으로 SAM은 이미지에서 음의 점을 버리는 반면, 양의 점을 둘러싼 연속적인 영역을 분할하려는 경향이 있습니다.대상 유도 주의.양의-음의 점 프롬프트가 얻어졌지만, 우리는 SAM 디코더에서 교차 주의 연산에 대한 보다 명확한 의미적 지침을 제안합니다.이는 전경 대상 영역 내에서 피처 집계를 집중시킵니다.그림 5에서 볼 수 있듯이, 방정식 6의 전체 신뢰도 맵 S는 테스트 이미지에서 대상 시각적 개념의 거친 영역을 명확하게 나타낼 수 있습니다(더 밝은 색상은 더 높은 점수를 나타냄).이러한 속성을 기반으로, 우리는 S를 사용하여 디코더의 모든 토큰-이미지 교차 주의 계층에서 주의 맵을 안내합니다.특히, 소프트맥스 함수 뒤의 모든 주의 맵을 A = Rhxw로 표시한 다음, 주의 분포를 A9 = softmax A+ a softmax(S) • (8)로 변조합니다.여기서 a는 밸런싱 팩터를 나타냅니다. 주의 편향으로 인해 마스크와 프롬프트 토큰은 중요하지 않은 배경 영역이 아닌 대상 주제와 관련된 더 많은 시각적 의미를 포착해야 합니다. 이는 주의 메커니즘에서 더 효과적인 기능 집계에 기여하고, 훈련 없이 PerSAM의 최종 세분화 정확도를 향상시킵니다. 대상 의미적 프롬프트. 바닐라 SAM은 점이나 상자의 좌표와 같은 저수준 위치 정보가 있는 프롬프트만 수신합니다. SAM의 디코더에 더 높은 수준의 단서를 제공하기 위해 대상 개념의 시각적 특징을 추가 높은 수준의 의미적 프롬프트로 활용하는 것을 제안합니다. 먼저 참조 이미지에서 객체의 글로벌 임베딩 TR을 다음과 같이 다른 로컬 피처 간의 I 평균 풀링을 통해 얻습니다.n TR = ΣΤΑ €R1xc n i=1 그런 다음 방정식 2에서 테스트 이미지의 모든 입력 토큰에 TR을 요소별로 추가한 다음 디코더 블록에 공급합니다.그림 5에 T9 = Repeat (TR) + Concat(TM,Tp), (10)으로 표시됩니다.여기서 T9는 디코더 Decм에 대한 대상 의미론에 의해 안내되는 입력 토큰을 나타내고 Repeat 작업은 대상 시각적 임베딩을 복제합니다.간단한 토큰 통합의 도움으로 PerSAM은 저수준 위치 지점뿐만 아니라 고수준 대상 시각적 단서에 의해서도 촉발됩니다. 6 테스트 이미지 PerSAM 출력 3개 M1 M2 + M3 스케일 임의 노이즈 →&gt; DreamBooth →&gt; ↑ &quot;a [V] cat&quot; 재구성 손실 W1 F W2 1- W1 + W₂ ) 1 사용자가 출력 마스크 M 제공 미세 조정 가중 합산 동결 PerSAM 배경 교란 분리 그림 6: PerSAM-F의 스케일 인식 미세 조정. 스케일 모호성을 완화하기 위해 PerSAM-F는 3개 스케일 마스크를 적응적으로 집계하기 위한 두 개의 학습 가능한 가중치를 채택합니다. 그림 7: PerSAM 지원 DreamBooth. DreamBooth 생성을 개선하기 위해 PerSAM을 사용하여 대상 객체를 배경에서 분리합니다. 계단식 사후 미세 조정. 위의 기술을 통해 SAM 디코더에서 테스트 이미지에 대한 초기 분할 마스크를 얻지만 거친 모서리와 고립된 배경 노이즈가 포함될 수 있습니다. 추가 세분화를 위해, 우리는 두 단계 후처리를 위해 마스크를 디코더 Decм에 반복적으로 공급합니다.첫 번째 단계에서 우리는 이전의 양수-음수 포인트 프롬프트와 함께 현재 예측된 마스크로 디코더에 프롬프트를 보냅니다.두 번째 단계에서 우리는 첫 번째 단계에서 마스크를 둘러싼 경계 상자를 획득하고, 더 정확한 객체 위치 파악을 위해 이 상자로 디코더에 추가로 프롬프트를 보냅니다.우리는 대규모 이미지 인코더 없이 가벼운 디코더만 반복하기 때문에 후처리가 효율적이고 단지 2%의 추가 지연 시간이 발생합니다.3.3 PERSAM-F 세분화 스케일의 모호성의 미세 조정.훈련이 필요 없는 PerSAM은 대부분의 경우를 만족스러운 세분화 정확도로 처리할 수 있습니다.그러나 일부 대상 객체에는 계층적 구조가 포함되어 있어 마스크 스케일의 모호성이 발생합니다.그림 6에서 볼 수 있듯이 플랫폼 위의 찻주전자는 뚜껑과 몸체의 두 부분으로 구성되어 있습니다. 양의 점 프롬프트(녹색 오각별 표시)가 몸체에 위치하고, 음의 프롬프트(빨간색 오각별 표시)가 비슷한 색상의 플랫폼을 제외하지 않으면 PerSAM은 분할에 대해 오도될 것입니다. 이러한 문제는 SAM(Kirillov et al., 2023)에서도 논의되며, 여기서는 객체의 전체, 부분 및 하위 부분에 해당하는 세 가지 크기의 여러 마스크를 동시에 생성하는 대안을 제안합니다. 그런 다음 사용자는 세 가지 마스크 중 하나를 수동으로 선택해야 하는데, 이는 효과적이지만 추가 인력을 소모합니다. 반면에 개인화된 작업은 인간의 프롬프트가 필요 없이 자동 객체 분할을 위해 SAM을 사용자 지정하는 것을 목표로 합니다. 이는 매개변수 효율적인 미세 조정을 통해 PerSAM의 크기 인식 버전을 추가로 개발하도록 동기를 부여합니다. 크기 인식 미세 조정. 적절한 크기에서 적응형 분할을 위해 미세 조정 변형인 PerSAM-F를 도입합니다. 학습이 필요 없는 모델이 하나의 마스크만 생성하는 것과 달리, PerSAM-F는 먼저 PerSAM을 따라 사전 위치를 구하고, SAM의 원래 솔루션을 참조하여 각각 M1, M2, M3으로 표시되는 3개 스케일 마스크를 출력합니다. 여기에 두 개의 학습 가능한 마스크 가중치 w₁, W2를 채택하고 가중치 합산을 통해 최종 마스크 출력을 다음과 같이 계산합니다. M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) 여기서 w1, W2는 모두 1/3으로 초기화됩니다. 최적의 가중치를 학습하기 위해 참조 이미지에서 원샷 미세 조정을 수행하고 주어진 마스크를 기준 진실로 간주합니다. 사전 학습된 지식을 보존하기 위해 전체 SAM 모델을 동결하고 단일 A100 GPU에서 10초 이내에 W1, W2의 두 매개변수만 미세 조정합니다. 이런 방식으로, 우리의 PerSAM-F는 객체의 스케일 인식 의미를 효율적으로 학습하고, 다양한 개념에 대한 최상의 분할 스케일을 적응적으로 출력하여 PerSAM의 일반화 용량을 개선합니다.7 표 1: PerSeg 데이터 세트의 개인화된 객체 분할. 우리는 다양한 방법(Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023)에 대한 전반적인 mIoU, blou 및 학습 가능한 매개변수와 PerSeg의 10개 객체에 대한 mIoU를 비교합니다. ***는 우리와 동시에 진행 중인 작업을 나타냅니다. 방법 mIoU bloU 매개변수. 캔 헛간 시계 고양이 뒤- 테디 오리 얇은 빨간색 팩 곰 장난감 새 만화 로봇 장난감 화가 VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 354M 93.0 33.3 20.9 98.2 65.0 59.2 76.6 66.7 79.8 89.9 67.4 81.0 72.4 72.4 91.1 94.1 95.2 98.0 71.3 97.0 95.8 96.6 63.8 92.6 94.1 94.4 93.7 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 표 2: DAVIS 2017 val(Pont-Tuset et al., 2017)에서의 비디오 객체 분할. 회색은 도메인 내 학습을 포함하는 방법을 나타냅니다. 표 3: FSS-1000(Li et al., 2020), LVIS-92(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 의미론 및 파트 분할. 우리는 mIoU 점수를 보고하고 회색을 사용하여 도메인 내 학습을 포함하는 방법을 나타냅니다. 원샷 의미론 분할 FSS-1000 LVIS-92² 원샷 파트 분할 PASCAL-Part Painter SEEM SegGPT PerSAM 방법 J&amp;F І AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F 방법 PACO-Part HSNet 86.5 17.4 32.4 22.6 부가세 90.3 18.5 33.6 23.5 페인터 61.7 10.5 30.4 14.1 SegGPT 85.6 18.6 66.9 71.3 75.1 PerSAM 81.6 15.6 32.5 22.5 PerSAM-F 76.1 74.9 79.7 PerSAM-F 86.3 18.4 32.9 22.7 3.4 PERSAM 지원 DREAMBOOTH 개인화된 텍스트-이미지 합성을 위해 DreamBooth(Ruiz et al., 2022)는 특정 개체 ig, 애완 고양이의 주어진 3~5개 사진을 사용하여 사전 훈련된 확산 모델을 미세 조정합니다. 텍스트 프롬프트에서 언급된 고양이인 &quot;[V] 고양이&quot;를 생성하는 방법을 학습하고 재구성된 전체 이미지에 대한 손실을 계산합니다. 이를 통해 훈련 이미지의 중복된 배경 정보를 식별자 [V]에 주입합니다. 따라서 그림 7과 같이 DreamBooth에서 배경의 교란을 완화하기 위한 전략을 소개합니다. 몇 장의 이미지에 대한 개체 마스크가 주어지면 PerSAM을 활용하여 모든 전경 대상을 분할하고 배경 영역에 속하는 픽셀에 대한 그래디언트 역전파를 버립니다. 그런 다음, 안정적 확산은 대상 객체의 시각적 I 모양만 기억하도록 미세 조정됩니다. 배경에 감독이 부과되지 않으므로 PerSAM 지원 DreamBooth는 더 나은 시각적 대응으로 대상 객체를 합성할 수 있을 뿐만 아니라 입력 텍스트 프롬프트에 따라 새로운 배경의 다양성을 높일 수도 있습니다.4 실험 먼저 섹션 4.1에서 PerSeg에서 개인화된 세분화를 위한 접근 방식을 평가하고, 섹션 4.2에서 다양한 기존 원샷 세분화 벤치마크를 평가합니다.그런 다음 섹션 4.3에서 PerSAM 지원 DreamBooth의 효과를 설명합니다.마지막으로 섹션 4.4에서 PerSeg에 대한 설계를 조사하기 위해 여러 가지 절제 연구를 수행합니다.4.1 개인화된 평가 PerSeg 데이터 세트.개인화 용량을 테스트하기 위해 PerSeg라는 새로운 세분화 데이터 세트를 구성합니다. 원시 이미지는 주제 중심 확산 작업(Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022)의 훈련 데이터에서 수집되었습니다.PerSeg에는 일용품, 동물, 건물을 포함하여 총 40개의 다양한 범주의 객체가 포함되어 있습니다.다른 포즈나 장면에서 각 객체는 5~7개의 이미지와 마스크와 연관되며, 여기서 하나의 이미지-마스크 쌍을 사용자가 제공한 원샷 데이터로 고정합니다.평가에는 mIoU와 bloU(Cheng et al., 2021)가 채택되었습니다.PerSeg의 구현 세부 사항과 확대된 데이터 스케일은 부록을 참조하세요.8 시계의 반지 2 쟁반 위의 찻주전자 여자가 들고 있는 백팩 캔의 윗부분 그림 8: PerSAM-F의 개선 사항 시각화.우리의 스케일 인식 미세 조정은 PerSAM의 스케일 모호성을 잘 완화할 수 있습니다. 그림 9: 비디오 객체 분할의 시각화. 우리의 접근 방식은 비디오에서 여러 객체를 분할하는 데 좋은 성과를 보입니다. 성능. 표 1에서 미세 조정된 PerSAM-F가 가장 좋은 결과를 얻는 것을 볼 수 있는데, 이는 PerSAM을 전체 mIoU와 bIoU에서 +2.7%, +5.9% 효과적으로 향상시킵니다. 그림 8에서 PerSAM-F의 개선 사항을 더 자세히 시각화합니다. Visual Prompting(VP)(Bar et al., 2022), Painter(Wang et al., 2022), SEEM(Zou et al., 2023), SegGPT(Wang et al., 2023)는 주어진 원샷 프롬프트 데이터에 따라 객체를 분할할 수 있는 컨텍스트 내 학습기입니다. 표시된 대로, 훈련이 필요 없는 PerSAM은 다른 마진으로 Painter, VP, SEEM보다 더 나은 성능을 이미 달성할 수 있습니다. 효율적인 2-매개변수 미세 조정을 통해, 저희의 PerSAM-F는 강력한 SegGPT를 +2.4%, 전반적인 mIoU와 bIoU에서 +4.1% 더 능가합니다.세그먼테이션 일반론자를 개발하려는 그들의 동기와 달리, 저희의 방법은 개인화된 객체 세분화를 위해 특별히 설계되었으며, 시간과 계산 리소스 측면에서 훨씬 더 높은 효율성을 보여줍니다.4.2 기존 세분화 벤치마크 비디오 객체 세분화.첫 번째 프레임 이미지와 객체 마스크를 고려할 때, 저희의 PerSAM과 PerSAM-F는 DAVIS 2017의 검증 세트에서 경쟁력 있는 객체 세분화 및 추적 성능을 달성합니다(Pont-Tuset et al., 2017) 표 2에서 볼 수 있듯이, 비디오 학습이 없는 방법과 비교했을 때, 학습이 없는 PerSAM은 Painter를 +32.3% J&amp;F 점수로 크게 능가하며, 저희의 PerSAM-F는 SegGPT보다 +0.5% 더 나은 성능을 달성할 수 있습니다. 특히, 우리의 원샷 미세 조정 접근 방식은 광범위한 비디오 데이터로 완전히 훈련된 방법(Lin et al., 2019; Liang et al., 2020)보다 성능이 우수할 수 있습니다. 결과는 그림 9에서 시각화된 것처럼 여러 유사하거나 가려진 객체를 포함하는 시간적 비디오 데이터와 복잡한 시나리오에 대한 우리의 강력한 일반화 능력을 충분히 보여줍니다. 원샷 의미론 및 부분 분할. 표 3에서 우리는 각각 4개의 데이터 세트인 FSS-1000(Li et al., 2020), LVIS-92²(Gupta et al., 2019), PASCAL-Part(Morabia et al., 2020), PACO-Part(Ramanathan et al., 2023)에 대한 원샷 이미지 분할에 대한 접근 방식을 평가합니다. 여기서 우리는 데이터 전처리 및 평가를 위해 Matcher(Liu et al., 2023)를 따릅니다. 표시된 대로, 우리의 PerSAM-F는 Painter보다 지속적으로 더 나은 결과를 얻었으며 SegGPT와 비슷한 성능을 보였습니다. 도메인 내 학습이 있는 모델(Min et al., 2021; Hong et al., 2022)의 경우, 우리의 접근 방식은 HSNet보다 더 높은 점수를 얻을 수 있습니다. 실험은 우리가 제안하는 접근 방식이 객체 수준 분할에 국한되지 않고 SAM의 범주별 및 부분별 개인화에도 작동한다는 것을 잘 보여줍니다. 4.3 PERSAM 지원 DREAMBOOTH 우리는 DreamBooth(Ruiz et al., 2022)의 모든 하이퍼파라미터를 따라 개인화된 이미지 합성을 위해 사전 학습된 Stable Diffusion(Rombach et al., 2022)을 미세 조정합니다. 그림 3 외에도 그림 10에서 PerSAM 지원 DreamBooth의 더 많은 예를 시각화합니다. 회색 소파에 누워 있는 개의 경우 DreamBooth의 &quot;정글&quot;과 &quot;눈&quot;은 여전히 녹색과 흰색 장식이 있는 소파입니다. PerSAM-F의 도움을 받아 새로 생성된 배경은 소파와 완전히 분리되어 텍스트 프롬프트와 잘 일치합니다. 산 앞에 있는 헛간의 경우, 우리의 접근 방식은 또한 배경 교란을 완화하여 &quot;숲&quot;과 &quot;푸른 하늘&quot;을 올바르게 생성합니다.9 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 사용자가 PerSAM의 도움을 받아 DreamBooth를 제공합니다 개 사진 &quot;정글 속의 [V]개&quot; ... 헛간 사진 &quot;배경에 숲이 있는 [V]개&quot; &quot;눈 속의 [V]개&quot; &quot;배경에 푸른 하늘이 있는 [V]개&quot; 그림 10: PerSAM 기반 DreamBooth의 시각화.개선된 DreamBooth(Ruiz et al., 2022)는 새로운 이미지에서 다양한 맥락을 합성하기 위해 다양성을 더 잘 보존할 수 있습니다.표 4: 제안하는 방법에서 주요 구성 요소의 소거.변형 표 5: 다양한 미세 조정 방법의 소거.표 6: 참조로 상자 이미지를 사용하는 소거.mIoU 이득 69.1 방법 PerSAM 매개변수. mIoU 방법 마스크 상자 0 89.32 페인터 56.4 42.0 + 사후 세부화 72.5 +3.4 83.9 +11.4 프롬프트 튜닝 12K 76.5 VP 65.9 38.1 어댑터 196K 78.3 SEEM 87.1 64.9 + 가이드 어텐션 + 의미적 프롬프트 85.8 +1.9 LORA 293K 90.0 SegGPT 94.3 36.0 89.3 +3.5 3 마스크 가중치 3 92.9 + 스케일 튜닝 95.3 +6.0 PerSAM-F 2 95.3 PerSAM 89.3 PerSAM-F 95.3 88.1 94.9 양의 사전 + 음의 사전 4.4 절제 연구 주요 구성 요소. 표 4에서 우리는 긍정적인 위치 사전만을 채택하는 기준선에서 시작하여 다양한 구성 요소를 조사합니다.그런 다음 부정적인 지점 프롬프트와 계단식 사후 세분화를 추가하여 각각 +3.6%와 +11.4%의 mIoU를 향상시킵니다.그 위에 우리는 주의 유도와 의미 프롬프트를 위해 SAM의 디코더에 고수준 대상 의미론을 도입합니다.그 결과 +1.9%와 +3.5%의 개선은 그 중요성을 충분히 나타냅니다.마지막으로 효율적인 스케일 인식 미세 조정을 통해 PerSAM-F는 점수를 +6.0% 높여 뛰어난 정확도를 보여줍니다.다른 미세 조정 방법.표 5에서 우리는 PerSAM-F에 대한 다른 매개변수 효율적 미세 조정(PEFT) 방법, 즉 프롬프트 튜닝(Liu et al., 2021), 어댑터(Houlsby et al., 2019), LORA(Hu et al., 2021)를 실험합니다. 우리는 SAM 전체를 동결하고 PerSAM 디코더의 모든 변압기 블록에 주입된 PEFT 모듈만 튜닝합니다.보여진 바와 같이, 프롬프트 튜닝과 어댑터는 원샷 데이터를 과대적합시키고 정확도를 심각하게 떨어뜨립니다.대신, 우리의 스케일 인식 미세 튜닝은 가장 학습하기 어려운 매개변수를 튜닝하는 동안 PerSAM의 성능을 가장 잘 향상시킬 수 있습니다.참조로 상자 이미지 사용.원샷 데이터로 정확한 마스크를 요구하는 것은 일부 사용자에게는 너무 엄격할 수 있습니다.표 6에서 예상 객체를 지정하는 경계 상자에 대한 입력 제한을 완화합니다.우리 방법의 경우 상자를 프롬프트로 간주하고 기성품 SAM을 사용하여 원샷 마스크를 생성할 수 있습니다.따라서 상자 참조는 PerSAM 및 PerSAM-F에서 약간의 성능 저하로 이어질 뿐이지만 다른 방법에는 심각한 영향을 미칩니다.5 결론 이 논문에서는 원샷 데이터만으로 특정 시각적 개념에 대한 Segment Anything Model(SAM)을 개인화하는 것을 제안합니다. 첫째, 훈련이 필요 없는 기술로 SAM에 고수준 대상 의미론을 주입하는 PerSAM을 소개합니다. 여기에 더해, 스케일 인식 미세 조정 변형인 PerSAM-F를 제시합니다. 학습 가능한 매개변수가 2개뿐인 PerSAM-F는 마스크 스케일의 모호성을 효과적으로 완화하고 다양한 벤치마크에서 선도적인 성능을 달성합니다. 게다가, DreamBooth가 더 나은 텍스트-이미지 확산 모델을 미세 조정하는 데 도움이 되는 접근 방식의 효능도 검증합니다. 저희의 작업이 SAM의 적용 범위를 더 넓은 범위의 시나리오로 확장할 수 있기를 바랍니다. 10 10 참고문헌 Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. Segnet: 이미지 분할을 위한 딥 합성곱 인코더-디코더 아키텍처. IEEE 패턴 분석 및 머신 인텔리전스 저널, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei Efros. 이미지 인페인팅을 통한 시각적 프롬프트. 신경 정보 처리 시스템의 발전, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, Qi Tian. Nerfs를 사용하여 3D로 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille. Deeplab: 딥 합성곱 신경망, Atrous 합성곱 및 완전 연결 CRF를 사용한 의미적 이미지 분할. IEEE 패턴 분석 및 머신 인텔리전스 저널, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan. 대화형 분할을 위한 조건부 확산. IEEE International Conference on Computer Vision의 진행 과정에서, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao. 밀도 예측을 위한 비전 변환기 어댑터. arXiv 사전 인쇄본 arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov. 경계 iou: 객체 중심 이미지 분할 평가 개선. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 진행 과정에서, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar. 범용 이미지 분할을 위한 마스크된 어텐션 마스크 변환기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 1290–1299, 2022. Pedro Cuenca 및 Sayak Paul. 효율적인 안정적 확산 미세 조정을 위해 lora 사용. https:// huging face.co/blog/lora, 2023년 1월. Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel Cohen-Or. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5356-5364쪽, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: 에지 가이드 플로우로 실용적인 상호 작용 분할 달성. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 1551-1560쪽, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. 매개변수 효율적 전이 학습에 대한 통합된 관점을 향해. International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. Mask r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, Alexandros Iosifidis. 구조화된 가지치기 어댑터. arXiv 사전 인쇄본 arXiv:2211.10155, 2022. 11 Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, Seungryong Kim. few-shot segmentation을 위한 4d 합성곱 swin 변환기를 사용한 비용 집계. European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. NLP를 위한 매개변수 효율적 전이 학습. 국제 기계 학습 컨퍼런스에서, 2790-2799쪽. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen 등. 의료 이미지에 대한 모든 모델을 세분화할 수 있나요?arXiv 사전 인쇄본 arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각적 및 시각 언어 표현 학습 확장. International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 시각적 프롬프트 튜닝. European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai 및 Chengjie Wang. 도메인 적응 의미론적 분할을 위한 프로토타입 대비 적응. 컴퓨터 비전에 관한 유럽 회의, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang 및 Liqing Zhang. Stc: 비디오 인스턴스 분할을 위한 시공간 대조 학습. 컴퓨터 비전 워크숍에 관한 유럽 회의, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang 및 Fisher Yu. 무엇이든 고품질로 분할하세요. arXiv 사전 인쇄본 arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár. 파노라마 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9404-9413쪽, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 지정. arXiv 사전 인쇄본 arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적 프롬프트 튜닝을 위한 규모의 힘. arXiv 사전 인쇄본 arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: 대규모 비전 및 비전-언어 작업을 위한 일반 모델. arXiv 사전 인쇄본 arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang. Fss-1000: few-shot segmentation을 위한 1000-class 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 2869-2878, 2020. 12 Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, Xingang Wang. 파노라마 분할을 위한 주의 유도 통합 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, Jim Chen. 적응적 특징 은행과 불확실한 영역 정제를 사용한 비디오 객체 분할. 신경 정보 처리 시스템의 발전, 33: 3430-3441, 2020. 화이지아 린, 샤오주안 치, 지아야 지아. Agss-vos: 주의 유도 단일 샷 비디오 객체 분할. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 3949-3957쪽, 2019. 자오장 린, 안드레아 마도토, 파스칼 펑. 매개변수 효율적 전이 학습을 통해 다재다능한 생성 언어 모델 탐색. arXiv 사전 인쇄본 arXiv:2004.03829, 2020. 린쯔이, 갱시지, 장렌루이, 가오펭, 제라르 드 멜로, 왕샤오강, 다이지펭, 차오위아오, 리홍셩. 동결된 클립 모델은 효율적인 비디오 학습기입니다. European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang. P-tuning v2: 신속한 튜닝은 규모와 작업 전반에 걸쳐 보편적으로 미세 조정하는 것과 비교할 수 있습니다. arXiv 사전 인쇄본 arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen. Matcher: 다목적 기능 매칭을 사용하여 한 번에 모든 것을 분할합니다. arXiv 사전 인쇄본 arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, Trevor Darrell. 의미 분할을 위한 완전 합성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 3431-3440쪽, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee. Vilbert: 시각 및 언어 작업을 위한 작업에 독립적인 시각 언어 표현 사전 학습. 신경 정보 처리 시스템의 발전(NeurIPS), 13-23쪽, 2019. Jun Ma, Bo Wang. 의료 이미지의 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, Minsu Cho. few-shot 분할을 위한 초상관 관계 압축. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 6941-6952쪽, 2021. Keval Morabia, Jatin Arora, Tara Vijaykumar. 객체 및 의미적 부분의 주의 기반 조인트 감지. arXiv 사전 인쇄본 arXiv:2007.02419, 2020. OpenAI. Gpt-4 기술 보고서. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych. 어댑터-융합: 전이 학습을 위한 비파괴적 작업 구성. arXiv 사전 인쇄본 arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv 사전 인쇄본 arXiv:1704.00675, 2017. Guanghui Qin 및 Jason Eisner. 묻는 방법 배우기: 소프트 프롬프트를 혼합하여 lms 쿼리하기. arXiv 사전 인쇄본 arXiv:2104.06599, 2021. Alec Radford 및 Karthik Narasimhan. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever 등. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서, 8748-8763쪽. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: 공통 객체의 부분과 속성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서, 7141-7151쪽, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 잔여 어댑터를 사용하여 여러 시각적 도메인 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng. 일반 기능 변환을 위한 학습 가능한 트리 필터 재고. 신경 정보 처리 시스템의 발전, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, Mohit Bansal. Vl-adapter: 시각 및 언어 작업을 위한 매개변수 효율적 전이 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5227-5237쪽, 2022. Zhi Tian, Chunhua Shen, Hao Chen. 인스턴스 분할을 위한 조건부 합성곱. 유럽 컴퓨터 비전 컨퍼런스, 282-298쪽. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄 arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li 및 Chunhua Shen. Solov2: 동적이고 빠른 인스턴스 분할. 신경 정보 처리 시스템의 발전, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen 및 Tiejun Huang. 이미지는 이미지로 말한다: 맥락 내 시각적 학습을 위한 일반주의 화가. arXiv 사전 인쇄본 arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. Seggpt: 맥락 내에서 모든 것을 분할. arXiv 사전 인쇄본 arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo. Segformer: 변환기를 사용한 의미적 분할을 위한 간단하고 효율적인 디자인. 신경 정보 처리 시스템의 발전, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, Yu Qiao. 3D 객체 포인트 클라우드의 보완적 이해를 위한 기하학-분리된 표현 학습. AAAI 인공지능 컨퍼런스 회의록, 35권, 3056-3064쪽, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng. 무엇이든 추적: 무엇이든 비디오와 만나는 세그먼트. arXiv 사전 인쇄본 arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, Choong Seon Hong. 무엇이든 더 빠르게 세그먼트화: 모바일 애플리케이션을 위한 가벼운 sam을 향해. arXiv 사전 인쇄본 arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen 및 Tuo Zhao. 매개변수 효율적인 미세 조정을 위한 적응형 예산 할당. arXiv 사전 인쇄 arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. Llama 어댑터: 초기화 주의가 필요 없는 언어 모델을 효율적으로 미세 조정합니다. arXiv 사전 인쇄 arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao 및 Peng Gao. 프롬프트, 생성, 캐시: 기반 모델의 캐스케이드는 강력한 소수의 학습자를 만듭니다. arXiv 사전 인쇄 arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 및 Jiaya Jia. 피라미드 장면 구문 분석 네트워크. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의 진행, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang 및 Jinqiao Wang. 무엇이든 빠르게 분할하세요. arXiv 사전 인쇄 arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr 등. 트랜스포머를 사용한 시퀀스-투-시퀀스 관점에서 의미적 분할 재고. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6881-6890쪽, 2021. 카이양 저우, 징캉 양, 첸 창 로이, 지웨이 리우. 시각 언어 모델을 위한 프롬프트 학습. 국제 컴퓨터 비전 저널, 130(9):2337-2348, 2022. 쉐얀 저우, 지안웨이 양, 하오 장, 펭 리, 린지에 리, 지안펭 가오, 용재 리. 모든 곳을 한꺼번에 분할. arXiv 사전 인쇄본 arXiv:2304.06718, 2023. 15 115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(translated_text['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2-translated.txt', 'w') as f:\n",
    "    f.write(translated_text['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"2305.03048v2.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:2305.03048v2 [cs.CV] 4 OctPERSONALIZE SEGMENT ANYTHING MODEL WITH ONE SHOT Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory 3 Institute of Automation, Chinese Academy of Sciences 4CFCS, School of CS, Peking University {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM\\'s decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM\\'s personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM\\'s decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM\\'s Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM\\'s frozen backbone or other pre-trained vision models, for which we adopt SAM\\'s image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM\\'s decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one\\'s on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM\\'s decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM\\'s decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM\\'s decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM\\'s original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F\\'s Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F\\'s improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM\\'s decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM\\'s decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5 CONCLUSION In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_text = re.sub(r'Page\\s*\\d+|\\s*\\d+\\s*\\n', '', text)\n",
    "replaced_text = re.sub(r'-\\n', '', replaced_text)\n",
    "replaced_text = re.sub(r'\\s+', ' ', replaced_text).strip()\n",
    "\n",
    "replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 OctPERSONALIZE SEGMENT ANYTHING MODEL WITH ONE SHOT Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma² Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹† ¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory 3 Institute of Automation, Chinese Academy of Sciences 4CFCS, School of CS, Peking University {zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn, kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk ABSTRACT Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt SAM's image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM's decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM's decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F's Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5 CONCLUSION In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n"
     ]
    }
   ],
   "source": [
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2305.03048v2_cleaned.txt\", \"w\") as f:\n",
    "    f.write(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sections = [\"ABSTRACT\", \"INTRODUCTION\", \"RELATED WORK\", \"METHOD\", \"EXPERIMENT\", \"CONCLUSION\"]\n",
    "section_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sections)-1):\n",
    "    pattern = f\"{sections[i]}(.*?){sections[i+1]}\"\n",
    "    match = re.search(pattern, replaced_text, re.S | re.I)\n",
    "    if match:\n",
    "        section_data[sections[i]] = match.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_section = sections[-1]\n",
    "pattern = f\"{last_section}(.*?)$\"\n",
    "match = re.search(pattern, replaced_text, re.S | re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ABSTRACT ---\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference...\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021...\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. S...\n",
      "\n",
      "--- METHOD ---\n",
      "s on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-fre...\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw imag...\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of o...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    section_data[last_section] = match.group(1).strip()\n",
    "\n",
    "for sec, content in section_data.items():\n",
    "    print(f\"--- {sec} ---\\n{content[:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ABSTRACT ---\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.\n",
      "\n",
      "--- INTRODUCTION ---\n",
      "A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2\n",
      "\n",
      "--- RELATED WORK ---\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our\n",
      "\n",
      "--- METHOD ---\n",
      "s on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM. (1) User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F ... in various poses or scenes Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model (SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data, we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. * Equal contribution. † Corresponding author.2 Parameters PerSAM PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" ... in various poses or scenes Fine-tune The body of a robot toy *** Figure 2: Personalized Segmentation Examples. Our PerSAM (Left) can segment personal objects in any context with favorable performance, and PerSAM-F (right) further alleviates the ambiguity issue by scale-aware fine-tuning.INTRODUCTION A photo of a [V] backpack \"A [V] backpack on a table of a classroom\" Figure 3: Improving DreamBooth (Ruiz et al., 2022) with PerSAM. By mitigating the disturbance of backgrounds during training, our approach can help to achieve higher-quality personalized text-to-image generation. Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. They demonstrate extraordinary generalization capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback. Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting 11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask, which allows for segmenting any objects in visual contexts. However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For each image, you must precisely find the target object within complicated contexts, and then activate SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to automatically segment user-designated visual concepts in a simple and efficient manner? To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e., a user-provided reference image and a rough mask of the personal concept. Specifically, we first obtain a location confidence map for the target object in the test image by feature similarities, which considers the appearance of every foreground pixel. According to confidence scores, two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of the target object to unleash SAM's personalized segmentation power with two techniques: • • Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. This explicitly compels the prompt tokens to mainly concentrate on foreground target regions for intensive feature aggregation. Target-semantic Prompting. To explicitly provide SAM with high-level target semantics, we fuse the original prompt tokens with the embedding of the target object, which provides the low-level positional prompt with additional visual cues for personalized segmentation. With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably, our approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,where the object comprises visually distinct subparts or hierarchical structures to be segmented, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output, since both the local part and the global shape can be regarded as valid masks by SAM. To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential segmentation results of different mask scales. To adaptively select the best scale for varying objects, we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot data and exhibits better segmentation accuracy shown in Figure 2 (Right). Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns to convert these images into an identifier [V] in the word embedding space, which, however, can simultaneously include the background information, e.g., stairs or the forest. This would override the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. We summarize the contributions of our paper as follows: • Personalized Object Segmentation. We first investigate how to customize a generalpurpose segmentation model (SAM) into personalized scenarios with minimal expense. To this end, we introduce two efficient and effective methods, along with a new segmentation dataset, PerSeg, for the evaluation of personalized object segmentation. • PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. • Our approach achieves competitive results on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. In addition, PerSAM can enhance DreamBooth for better personalized text-to-image synthesis. 2 RELATED WORK Foundation Models. With powerful generalization capacity, pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. In natural language processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domainspecific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition. Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation. There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023), 3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023; Huang et al., 2023) image processing. From another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist into a specialist with only one shot. Our method can also assist the personalization of text-toimage foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al., 2022), which improves the generation quality by segmenting the foreground target objects from the background disturbance.Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022) requires a pixel-level comprehension of a image. Various segmentation-related tasks have been explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badrinarayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song et al., 2020); instance segmentation, focusing on the identification of individual object instances (He et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation, involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works have proposed large-scale vision models for image segmentation. They are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop annotation to learn a promptable segmentation framework, which generalizes to downstream scenarios in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation. Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation. Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications. To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning. Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks. Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation. Different from existing works, we adopt a more efficient adaption method delicately designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters andseconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. 3 METHOD In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and introduce the task definition for personalized object segmentation. Then, we illustrate the methodology of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4. 3.PERSONALIZED OBJECT SEGMENTATION A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point, a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and adopts Encp to encode the human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1)F₁ Encode Cosine Similarity {FT}=Test Image I Target Local Features {T}=FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-Attention Target-semantic Prompting ↑ {Si)=Local Confidence Maps Modulate Token-to-Image Cross-Attention ↑ Aggregate Attention Matrix A Local Features {T}=Token Self-Attention Overall Confidence Map S Aggregate ↑ a Concat( + Repeat( ) Overall Confidence Map S TM Тр ×TR One-shot Image IR One-shot Mask MR Positive Prior Negative Prior Figure 4: Positive-negative Location Prior. We calculate a location confidence map for the target object in new test image by the appearance of all local parts. Then, we select the location prior as the point prompt for PerSAM. Low-level Positional Prompt High-level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). To inject SAM with target semantics, we explicitly guide the cross-attention layers, and propose additional prompting with high-level cues. where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. (2) Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the ability to automatically segment specific subject instances. Considering this, we define a new task for personalized object segmentation. The user provides only a single reference image, and a mask indicating the target visual concept. The given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object within new images or videos, without additional human prompting. For evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. The raw images are collected from the works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022), containing various categories of visual concepts in different poses or scenes. In this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.TRAINING-FREE PERSAM Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM first obtains a confidence map that indicates the location of the target object in the new test image I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I. The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt SAM's image encoder Enc, by default. We formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features of foreground pixels within the visual concept from FR, resulting in a set of n local features as {T}}=1 = MR ○ Fr, (4)where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as {S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw. (5) Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain the overall confidence map of the target object as S = n n SiЄRhxw (6) By incorporating the confidences of every foreground pixel, S can take the visual appearance of different object parts into consideration, and acquire a relatively comprehensive location estimation. Positive-negative Location Prior. To provide PerSAM with a location prior on the test image, we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ, respectively. The former represents the most likely center position of the target object, while the latter inversely indicates the background. Then, they are regarded as the positive and negative point prompts, and fed into the prompt encoder as Tp = Encp(Ph, P₁) Є R2×c, (7) which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. Target-guided Attention. Although the positive-negative point prompt has been obtained, we further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder, which concentrates the feature aggregation within foreground target regions. As shown in Figure 5, the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically, we denote every attention map after the softmax function as A = Rhxw, and then modulate its attention distribution by A= softmax A+ a softmax(S) • (8) where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled to capture more visual semantics associated with the target subject, other than the unimportant background area. This contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional information, such as the coordinate of a point or a box. To provide SAM's decoder with more highlevel cues, we propose to utilize the visual feature of the target concept as an additional high-level semantic prompting. We first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR = ΣΤΑ €R1xc n i=Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding them into the decoder block, which is shown in Figure 5 as T9 = Repeat (TR) + Concat(TM,Tp), (10) where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM is not only prompted by low-level location points, but also high-level target visual cues.Test Image PerSAM Output Three MM+ Mscales Random Noise →> DreamBooth →> ↑ \"a [V] cat\" Reconstruction Loss WF W1- W+ W₂ )User provides Output Mask M Fine-tune Weighted Summation Freeze PerSAM Background Disturbance Decouple Figure 6: The Scale-aware Fine-tuning in PerSAM-F. To alleviate the scale ambiguity, PerSAM-F adopts two learnable weights for adaptively aggregating three-scale masks. Figure 7: PerSAM-assisted DreamBooth. We utilize PerSAM to decouple the target objects from the background for improving the generation of DreamBooth. Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask on the test image from SAM's decoder, which however, might include rough edges and isolated background noises. For further refinement, we iteratively feed the mask back into the decoder Decм for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. For the second step, we acquire the bounding box enclosing the mask from the first step, and prompt the decoder additionally with this box for more accurate object localization. As we only iterate the lightweight decoder without the large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency. 3.FINE-TUNING OF PERSAM-F Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfactory segmentation accuracy. However, some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is required to manually select one mask out of three, which is effective but consumes extra manpower. In contrast, our personalized task aims to customize SAM for automatic object segmentation without the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning. Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable mask weights, w₁, W2, and calculate the final mask output by a weighted summation as . . M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3, (11) where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot finetuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts, improving the generalization capacity of PerSAM.Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU, blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours. Method mIoU bloU Param. Can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.65.9 25.5 383M 61.58.87.1 55.7 341M 65.4 82.94.3 76.5 354M 93.33.3 20.98.65.59.76.66.79.89.67.81.72.72.91.94.95.98.71.97.95.96.6 63.92.94.94.93.97.92.97.96.90.70 95.96.2 38.9 96.96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.94.97.93.97.60.97.96.PerSAM 89.3 71.PerSAM-F 95.3 77.Table 2: Video Object Segmentation on DAVIS 2017 val (PontTuset et al., 2017). We utilize gray color to denote the methods involving in-domain training. Table 3: One-shot Semantic and Part Segmentation on FSS1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCALPart (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023). We report the mIoU scores and utilize gray color to denote the methods involving in-domain training. One-shot Semantic Seg. FSS-1000 LVIS-92² One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F І AGSS 67.4 64.9 69.AFB-URR 74.6 73.0 76.34.6 28.5 40.58.9 55.0 62.75.6 72.5 78.F Method PACO-Part HSNet 86.17.32.22.VAT 90.18.33.23.Painter 61.10.30.14.SegGPT 85.18.66.9 71.3 75.PerSAM 81.15.32.22.PerSAM-F 76.1 74.9 79.PerSAM-F 86.18.32.22.3.PERSAM-ASSISTED DREAMBOOTH For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed images. This This would inject the redundant background information in the training images into the identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the visual I appearances of the target object. With no supervision imposed on the background, our PerSAMassisted DreamBooth can not only synthesize the target object with better visual correspondence, but also increase the diversity of the new backgrounds guided by the input text prompt. 4\n",
      "\n",
      "--- EXPERIMENT ---\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several ablation studies to investigate our designs on PerSeg in Section 4.4. 4.1 PERSONALIZED EVALUATION PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset, termed PerSeg. The raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. In different poses or scenes, each object is associated with 5~7 images and masks, where we fix one image-mask pair as the user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.The ring on a clockThe teapot on a tray The backpack carried by a The top part of a can woman Figure 8: Visualization of PerSAM-F's Improvement. Our scale-aware fine-tuning can well alleviate the scale ambiguity of PerSAM. Figure 9: Visualization of Video Object Segmentation. Our approach performs well for segmenting multiple objects in a video. Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualization of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that can also segment objects according to the given one-shot prompt data. As shown, the training-free PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins. By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation generalists, our method is specially designed for personalized object segmentation, and exhibits much more efficiency in both time and computational resources. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive video data. The results fully illustrate our strong generalization ability for temporal video data and complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9. One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. 4.PERSAM-ASSISTED DREAMBOOTH We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates the background disturbance to correctly generate the “forest” and “blue sky\".User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM A photo of a dog \"A [V] dog in a jungle\" ... A photo of a barn \"A [V] barn with a forest in the background\" \"A [V] dog in snow\" \"A [V] barn with blue sky in the background\" Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al., 2022) can better preserve the diversity for synthesizing various contexts in new images. Table 4: Ablation of Main Components in our proposed method. Variant Table 5: Ablation of Different Fine-tuning Methods. Table 6: Ablation of using Box-image as Reference. mIoU Gain 69.Method PerSAM Param. mIoU Method Mask Box89.Painter 56.4 42.+ Post-refinement 72.5 +3.83.9 +11.Prompt Tuning 12K 76.VP 65.38.Adapter 196K 78.SEEM 87.64.+ Guided Attention + Semantic Prompt 85.8 +1.LORA 293K 90.SegGPT 94.3 36.89.+3.3 Mask Weights92.+ Scale Tuning 95.+6.PerSAM-F95.PerSAM 89.PerSAM-F 95.88.94.Positive Prior + Negative Prior 4.ABLATION STUDY Main Components. In Table 4, we investigate our different components by starting from a baseline that only adopts the positive location prior. Then, we add the negative point prompt and cascaded post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy. Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning (PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for some users. In Table 6, we relax the input restrictions to a bounding box designating the expected object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. 5\n",
      "\n",
      "--- CONCLUSION ---\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant, PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope our work may expand the applicability of SAM to a wider: range of scenarios.REFERENCES Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2017. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 7345-7354, 2021. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022. Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https:// hugging face.co/blog/lora, January 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1551-1560, 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters. arXiv preprint arXiv:2211.10155, 2022.Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint arXiv:2304.14660, 2023. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727. Springer, 2022. Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Prototypical contrast adaptation for domain adaptive semantic segmentation. In European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer, 2023. Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9404-9413, 2019. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2869-2878, 2020.Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019. Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33: 3430-3441, 2020. Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3949-3957, 2019. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European Conference on Computer Vision, pp. 388-404. Springer, 2022. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310, 2023. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 13–23, 2019. Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and semantic part. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7141-7151, 2023. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in Neural information processing systems, 30, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural Information Processing Systems, 33:3991-4002, 2020. Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, pp. 282–298. Springer, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077-12090, 2021. Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning geometry-disentangled representation for complementary understanding of 3d object point cloud. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023a. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023c. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):2337-2348, 2022. Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sec, content in section_data.items():\n",
    "    print(f\"--- {sec} ---\\n{content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oct', 'personalize', 'segment', 'anything', 'model', 'one', 'shot', 'renrui', 'zhengkai', 'ziyu', 'shilin', 'junting', 'xianzheng', 'hao', 'dongª', 'yu', 'peng', 'hongsheng', 'mmlab', 'shanghai', 'artificial', 'intelligence', 'laboratory', 'institute', 'automation', 'chinese', 'academy', 'sciences', 'school', 'cs', 'peking', 'university', 'zhangrenrui', 'gaopeng', 'guoziyu', 'qiaoyu', 'hsli', 'abstract', 'driven', 'segment', 'anything', 'model', 'sam', 'demonstrated', 'powerful', 'promptable', 'framework', 'revolutionizing', 'tion', 'field', 'despite', 'generality', 'customizing', 'sam', 'specific', 'visual', 'concepts', 'without', 'prompting', 'automatically', 'segmenting', 'pet', 'dog', 'numerous', 'images', 'paper', 'introduce', 'personalization', 'approach', 'sam', 'termed', 'persam', 'given', 'data', 'single', 'image', 'reference', 'mask', 'first', 'obtain', 'cation', 'prior', 'target', 'concept', 'new', 'images', 'aided', 'target', 'visual', 'semantics', 'empower', 'sam', 'personalized', 'object', 'segmentation', 'via', 'two', 'posed', 'techniques', 'attention']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oct', 'personalize', 'segment', 'anything', 'model', 'one', 'shot', 'renrui', 'zhengkai', 'ziyu', 'shilin', 'junting', 'xianzheng', 'hao', 'dongª', 'yu', 'peng', 'hongsheng', 'mmlab', 'shanghai', 'artificial', 'intelligence', 'laboratory', 'institute', 'automation', 'chinese', 'academy', 'sciences', 'school', 'cs', 'peking', 'university', 'zhangrenrui', 'gaopeng', 'guoziyu', 'qiaoyu', 'hsli', 'abstract', 'driven', 'segment', 'anything', 'model', 'sam', 'demonstrated', 'powerful', 'promptable', 'framework', 'revolutionizing', 'tion', 'field', 'despite', 'generality', 'customizing', 'sam', 'specific', 'visual', 'concepts', 'without', 'prompting', 'automatically', 'segmenting', 'pet', 'dog', 'numerous', 'images', 'paper', 'introduce', 'personalization', 'approach', 'sam', 'termed', 'persam', 'given', 'data', 'single', 'image', 'reference', 'mask', 'first', 'obtain', 'cation', 'prior', 'target', 'concept', 'new', 'images', 'aided', 'target', 'visual', 'semantics', 'empower', 'sam', 'personalized', 'object', 'segmentation', 'via', 'two', 'posed', 'techniques', 'attention', 'prompting', 'way', 'effectively', 'customize', 'sam', 'private', 'use', 'without', 'training', 'alleviate', 'ambiguity', 'segmentation', 'scales', 'present', 'efficient', 'variant', 'freezing', 'entire', 'sam', 'introduce', 'aggregate', 'masks', 'tunes', 'parameters', 'within', 'seconds', 'improved', 'performance', 'demonstrate', 'efficacy', 'construct', 'new', 'dataset', 'perseg', 'evaluation', 'personalized', 'object', 'segmentation', 'also', 'test', 'methods', 'various', 'image', 'video', 'segmentation', 'benchmarks', 'besides', 'propose', 'leverage', 'persam', 'improve', 'dreambooth', 'personalized', 'synthesis', 'mitigating', 'disturbance', 'backgrounds', 'approach', 'showcases', 'better', 'target', 'appearance', 'generation', 'higher', 'fidelity', 'input', 'text', 'prompt', 'code', 'released', 'https', 'user', 'provides', 'learning', 'personalized', 'segmentation', 'one', 'image', 'one', 'mask', 'persam', 'various', 'poses', 'scenes', 'figure', 'personalization', 'segment', 'anything', 'model', 'customize', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'specific', 'visual', 'concepts', 'pet', 'dog', 'data', 'introduce', 'two', 'efficient', 'solutions', 'persam', 'equal', 'contribution', 'corresponding', 'author', 'parameters', 'persam', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'seconds', 'hat', 'teddy', 'bear', 'photo', 'v', 'cat', 'v', 'cat', 'beach', 'various', 'poses', 'scenes', 'body', 'robot', 'toy', 'figure', 'personalized', 'segmentation', 'ples', 'persam', 'left', 'segment', 'personal', 'objects', 'context', 'favorable', 'mance', 'right', 'alleviates', 'ambiguity', 'issue', 'introduction', 'photo', 'v', 'backpack', 'v', 'backpack', 'table', 'classroom', 'figure', 'improving', 'dreambooth', 'ruiz', 'et', 'persam', 'mitigating', 'bance', 'backgrounds', 'training', 'proach', 'help', 'achieve', 'alized', 'generation', 'foundations', 'models', 'vision', 'li', 'et', 'zou', 'et', 'wang', 'et', 'language', 'brown', 'et', 'touvron', 'et', 'radford', 'et', 'radford', 'et', 'jia', 'et', 'li', 'et', 'gained', 'unprecedented', 'prevalence', 'attributed', 'availability', 'datasets', 'computational', 'resources', 'demonstrate', 'extraordinary', 'generalization', 'capacity', 'scenarios', 'display', 'versatile', 'interactivity', 'incorporating', 'human', 'feedback', 'inspired', 'segment', 'anything', 'kirillov', 'et', 'develops', 'delicate', 'data', 'engine', 'collecting', 'data', 'subsequently', 'trains', 'segmentation', 'foundation', 'model', 'known', 'sam', 'defines', 'novel', 'promptable', 'segmentation', 'framework', 'taking', 'input', 'handcrafted', 'prompt', 'returning', 'expected', 'mask', 'allows', 'segmenting', 'objects', 'visual', 'contexts', 'however', 'sam', 'inherently', 'loses', 'capability', 'segment', 'specific', 'visual', 'concepts', 'imagine', 'intending', 'crop', 'lovely', 'pet', 'dog', 'thick', 'photo', 'album', 'find', 'missing', 'clock', 'picture', 'bedroom', 'utilizing', 'vanilla', 'sam', 'would', 'highly', 'image', 'must', 'precisely', 'find', 'target', 'object', 'within', 'complicated', 'contexts', 'activate', 'sam', 'proper', 'prompt', 'segmentation', 'considering', 'ask', 'personalize', 'sam', 'automatically', 'segment', 'visual', 'concepts', 'simple', 'efficient', 'manner', 'end', 'introduce', 'persam', 'personalization', 'approach', 'segment', 'anything', 'model', 'shown', 'figure', 'method', 'efficiently', 'customizes', 'sam', 'using', 'data', 'reference', 'image', 'rough', 'mask', 'personal', 'concept', 'specifically', 'first', 'obtain', 'location', 'confidence', 'map', 'target', 'object', 'test', 'image', 'feature', 'similarities', 'considers', 'appearance', 'every', 'foreground', 'pixel', 'according', 'confidence', 'scores', 'two', 'points', 'selected', 'location', 'prior', 'finally', 'encoded', 'prompt', 'tokens', 'fed', 'sam', 'decoder', 'segmentation', 'within', 'decoder', 'propose', 'inject', 'visual', 'semantics', 'target', 'object', 'unleash', 'sam', 'personalized', 'segmentation', 'power', 'two', 'techniques', 'attention', 'guide', 'every', 'layer', 'sam', 'decoder', 'location', 'confidence', 'map', 'explicitly', 'compels', 'prompt', 'tokens', 'mainly', 'concentrate', 'foreground', 'target', 'regions', 'intensive', 'feature', 'aggregation', 'prompting', 'explicitly', 'provide', 'sam', 'target', 'semantics', 'fuse', 'original', 'prompt', 'tokens', 'embedding', 'target', 'object', 'provides', 'positional', 'prompt', 'additional', 'visual', 'cues', 'personalized', 'segmentation', 'aforementioned', 'designs', 'along', 'cascaded', 'persam', 'exhibits', 'favorable', 'personalized', 'segmentation', 'performance', 'unique', 'subjects', 'variety', 'poses', 'scenes', 'notably', 'approach', 'cope', 'well', 'scenarios', 'require', 'segmenting', 'one', 'object', 'among', 'multiple', 'similar', 'ones', 'simultaneously', 'segmenting', 'several', 'identical', 'objects', 'image', 'tracking', 'different', 'objects', 'along', 'video', 'nevertheless', 'shown', 'figure', 'might', 'occasional', 'failure', 'cases', 'object', 'comprises', 'visually', 'distinct', 'subparts', 'hierarchical', 'structures', 'segmented', 'hat', 'top', 'teddy', 'bear', 'head', 'robot', 'toy', 'ambiguity', 'casts', 'challenge', 'persam', 'determining', 'appropriate', 'scale', 'mask', 'output', 'since', 'local', 'part', 'global', 'shape', 'regarded', 'valid', 'masks', 'sam', 'alleviate', 'issue', 'propose', 'variant', 'approach', 'freeze', 'entire', 'sam', 'preserve', 'versatile', 'knowledge', 'parameters', 'within', 'seconds', 'single', 'gpu', 'detail', 'enable', 'sam', 'produce', 'several', 'potential', 'segmentation', 'results', 'different', 'mask', 'scales', 'adaptively', 'select', 'best', 'scale', 'varying', 'objects', 'employ', 'learnable', 'relative', 'weight', 'mask', 'scale', 'conduct', 'weighted', 'summation', 'final', 'output', 'efficient', 'training', 'avoids', 'data', 'exhibits', 'better', 'segmentation', 'accuracy', 'shown', 'figure', 'right', 'moreover', 'observe', 'approach', 'also', 'assist', 'dreambooth', 'ruiz', 'et', 'better', 'diffusion', 'models', 'personalized', 'generation', 'shown', 'figure', 'given', 'images', 'containing', 'specific', 'visual', 'concept', 'pet', 'cat', 'backpack', 'dreambooth', 'learns', 'convert', 'images', 'identifier', 'v', 'word', 'embedding', 'space', 'however', 'simultaneously', 'include', 'background', 'information', 'stairs', 'forest', 'would', 'override', 'newly', 'prompted', 'backgrounds', 'disturb', 'target', 'appearance', 'generation', 'therefore', 'propose', 'leverage', 'persam', 'segment', 'target', 'object', 'within', 'training', 'images', 'supervise', 'dreambooth', 'foreground', 'area', 'enabling', 'synthesis', 'higher', 'quality', 'summarize', 'contributions', 'paper', 'follows', 'personalized', 'object', 'segmentation', 'first', 'investigate', 'customize', 'purpose', 'segmentation', 'model', 'sam', 'personalized', 'scenarios', 'minimal', 'expense', 'end', 'introduce', 'two', 'efficient', 'effective', 'methods', 'along', 'new', 'segmentation', 'dataset', 'perseg', 'evaluation', 'personalized', 'object', 'segmentation', 'persam', 'persam', 'propose', 'three', 'techniques', 'guide', 'sam', 'semantics', 'target', 'objects', 'design', 'parameters', 'seconds', 'well', 'alleviate', 'mask', 'ambiguity', 'issue', 'approach', 'achieves', 'competitive', 'results', 'various', 'tasks', 'including', 'perseg', 'benchmark', 'part', 'semantic', 'segmentation', 'video', 'object', 'segmentation', 'addition', 'persam', 'enhance', 'dreambooth', 'better', 'personalized', 'synthesis', 'related', 'work', 'foundation', 'models', 'powerful', 'generalization', 'capacity', 'foundation', 'models', 'adapted', 'various', 'downstream', 'scenarios', 'attain', 'promising', 'performance', 'natural', 'language', 'processing', 'bert', 'devlin', 'et', 'lu', 'et', 'gpt', 'series', 'brown', 'et', 'openai', 'radford', 'narasimhan', 'radford', 'et', 'llama', 'zhang', 'et', 'demonstrated', 'remarkable', 'learning', 'abilities', 'transferred', 'new', 'tasks', 'specific', 'prompts', 'similarly', 'clip', 'radford', 'et', 'align', 'jia', 'et', 'conduct', 'contrastive', 'learning', 'pairs', 'exhibit', 'exceptional', 'accuracy', 'visual', 'recognition', 'painter', 'wang', 'et', 'introduces', 'vision', 'model', 'unifies', 'network', 'architectures', 'prompts', 'accomplish', 'diverse', 'vision', 'tasks', 'without', 'downstream', 'cafo', 'zhang', 'et', 'cascades', 'different', 'foundation', 'models', 'collaborates', 'knowledge', 'robust', 'image', 'classification', 'sam', 'kirillov', 'et', 'presents', 'foundation', 'model', 'image', 'segmentation', 'billion', 'masks', 'conducts', 'segmentation', 'concurrent', 'works', 'extending', 'sam', 'segmentation', 'ke', 'et', 'faster', 'inference', 'speed', 'zhao', 'et', 'zhang', 'et', 'matching', 'liu', 'et', 'reconstruction', 'cen', 'et', 'object', 'tracking', 'yang', 'et', 'medical', 'wang', 'huang', 'et', 'image', 'processing', 'another', 'perspective', 'propose', 'personalize', 'segmentation', 'foundation', 'model', 'sam', 'specific', 'visual', 'concepts', 'adapts', 'generalist', 'specialist', 'one', 'shot', 'method', 'also', 'assist', 'personalization', 'image', 'foundation', 'models', 'stable', 'diffusion', 'rombach', 'et', 'imagen', 'saharia', 'et', 'improves', 'generation', 'quality', 'segmenting', 'foreground', 'target', 'objects', 'background', 'disturbance', 'large', 'models', 'segmentation', 'fundamental', 'task', 'computer', 'vision', 'segmentation', 'long', 'et', 'jiang', 'et', 'zhao', 'et', 'xu', 'et', 'jiang', 'et', 'lin', 'et', 'requires', 'comprehension', 'image', 'various', 'tasks', 'explored', 'semantic', 'segmentation', 'classifying', 'pixel', 'predefined', 'set', 'classes', 'narayanan', 'et', 'chen', 'et', 'zheng', 'et', 'cheng', 'et', 'xie', 'et', 'song', 'et', 'instance', 'segmentation', 'focusing', 'identification', 'individual', 'object', 'instances', 'et', 'wang', 'et', 'tian', 'et', 'panoptic', 'segmentation', 'assigning', 'class', 'labels', 'instance', 'identification', 'kirillov', 'et', 'li', 'et', 'interactive', 'segmentation', 'involving', 'human', 'intervention', 'refinement', 'hao', 'et', 'chen', 'et', 'recently', 'inspired', 'language', 'foundation', 'models', 'zhang', 'et', 'brown', 'et', 'several', 'concurrent', 'works', 'proposed', 'vision', 'models', 'image', 'segmentation', 'extensive', 'mask', 'data', 'exhibit', 'strong', 'generalization', 'capabilities', 'numerous', 'image', 'distributions', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'utilizes', 'data', 'engine', 'tation', 'learn', 'promptable', 'segmentation', 'framework', 'generalizes', 'downstream', 'scenarios', 'manner', 'painter', 'wang', 'et', 'seggpt', 'wang', 'et', 'introduce', 'robust', 'learning', 'paradigm', 'segment', 'images', 'given', 'prompt', 'seem', 'zou', 'et', 'presents', 'general', 'segmentation', 'model', 'prompted', 'references', 'language', 'audio', 'incorporating', 'versatile', 'semantic', 'knowledge', 'study', 'introduce', 'new', 'task', 'termed', 'personalized', 'object', 'segmentation', 'annotate', 'new', 'dataset', 'perseg', 'evaluation', 'instead', 'developing', 'large', 'segmentation', 'models', 'goal', 'personalize', 'segment', 'objects', 'poses', 'scenes', 'propose', 'two', 'approaches', 'persam', 'efficiently', 'customize', 'sam', 'personalized', 'segmentation', 'directly', 'tuning', 'entire', 'foundation', 'models', 'downstream', 'tasks', 'computationally', 'expensive', 'posing', 'challenges', 'constrained', 'applications', 'address', 'issue', 'recent', 'works', 'focused', 'developing', 'efficient', 'methods', 'sung', 'et', 'et', 'rebuffi', 'et', 'qin', 'eisner', 'freeze', 'weights', 'foundation', 'models', 'append', 'modules', 'prompt', 'tuning', 'lester', 'et', 'zhou', 'et', 'jia', 'et', 'liu', 'et', 'suggests', 'using', 'learnable', 'soft', 'prompts', 'alongside', 'frozen', 'models', 'perform', 'specific', 'downstream', 'tasks', 'achieving', 'competitive', 'performance', 'scale', 'robust', 'domain', 'transfer', 'compared', 'full', 'model', 'tuning', 'adaption', 'lora', 'hu', 'et', 'cuenca', 'paul', 'zhang', 'et', 'hedegaard', 'et', 'injects', 'trainable', 'rank', 'decomposition', 'matrices', 'concurrently', 'weight', 'significantly', 'reduces', 'number', 'learnable', 'parameters', 'required', 'downstream', 'tasks', 'adapters', 'houlsby', 'et', 'pfeiffer', 'et', 'lin', 'et', 'chen', 'et', 'designed', 'inserted', 'layers', 'original', 'transformer', 'introducing', 'lightweight', 'mlps', 'feature', 'transformation', 'different', 'existing', 'works', 'adopt', 'efficient', 'adaption', 'method', 'delicately', 'designed', 'sam', 'parameters', 'seconds', 'effectively', 'avoids', 'issue', 'data', 'alleviates', 'ambiguity', 'segmentation', 'scale', 'superior', 'performance', 'method', 'section', 'first', 'briefly', 'revisit', 'segment', 'anything', 'model', 'sam', 'kirillov', 'et', 'introduce', 'task', 'definition', 'personalized', 'object', 'segmentation', 'illustrate', 'methodology', 'persam', 'section', 'respectively', 'finally', 'utilize', 'approach', 'assist', 'dreambooth', 'ruiz', 'et', 'better', 'generation', 'section', 'personalized', 'object', 'segmentation', 'revisit', 'segment', 'anything', 'sam', 'consists', 'three', 'components', 'prompt', 'encoder', 'image', 'encoder', 'lightweight', 'mask', 'decoder', 'respectively', 'denoted', 'encp', 'enc', 'decм', 'promptable', 'framework', 'sam', 'takes', 'input', 'image', 'set', 'prompts', 'p', 'point', 'box', 'coarse', 'mask', 'specifically', 'sam', 'first', 'utilizes', 'enc', 'obtain', 'input', 'image', 'feature', 'adopts', 'encp', 'encode', 'prompts', 'length', 'k', 'prompt', 'tokens', 'fi', 'enc', 'tp', 'encp', 'p', 'encode', 'cosine', 'similarity', 'ft', 'test', 'image', 'target', 'local', 'features', 'fr', 'fr', 'encode', 'persam', 'decoder', 'attention', 'prompting', 'si', 'local', 'confidence', 'maps', 'modulate', 'aggregate', 'attention', 'matrix', 'local', 'features', 'token', 'overall', 'confidence', 'map', 'aggregate', 'concat', 'repeat', 'overall', 'confidence', 'map', 'tm', 'тр', 'tr', 'image', 'ir', 'mask', 'mr', 'positive', 'prior', 'negative', 'prior', 'figure', 'location', 'prior', 'calculate', 'location', 'confidence', 'map', 'target', 'object', 'new', 'test', 'image', 'ance', 'local', 'parts', 'select', 'tion', 'prior', 'point', 'prompt', 'persam', 'positional', 'prompt', 'semantic', 'prompt', 'figure', 'attention', 'left', 'prompting', 'right', 'ject', 'sam', 'target', 'semantics', 'explicitly', 'guide', 'layers', 'propose', 'additional', 'prompting', 'cues', 'rhxwxc', 'tp', 'є', 'rkxc', 'h', 'w', 'denoting', 'resolution', 'image', 'feature', 'map', 'c', 'denoting', 'feature', 'dimension', 'encoded', 'image', 'prompts', 'fed', 'decoder', 'decм', 'feature', 'interaction', 'sam', 'constructs', 'input', 'tokens', 'decoder', 'concatenating', 'several', 'learnable', 'mask', 'tokens', 'tм', 'prefixes', 'prompt', 'tokens', 'tp', 'mask', 'tokens', 'responsible', 'generating', 'mask', 'output', 'formulated', 'decм', 'fi', 'concat', 'tm', 'tp', 'denotes', 'final', 'segmentation', 'mask', 'predicted', 'sam', 'task', 'definition', 'although', 'sam', 'generalized', 'enough', 'object', 'prompting', 'lacks', 'ability', 'automatically', 'segment', 'specific', 'subject', 'instances', 'considering', 'define', 'new', 'task', 'personalized', 'object', 'segmentation', 'user', 'provides', 'single', 'reference', 'image', 'mask', 'indicating', 'target', 'visual', 'concept', 'given', 'mask', 'either', 'accurate', 'segmentation', 'rough', 'sketch', 'drawn', 'goal', 'customize', 'sam', 'segment', 'designated', 'object', 'within', 'new', 'images', 'videos', 'without', 'additional', 'human', 'prompting', 'evaluation', 'annotate', 'new', 'dataset', 'personalized', 'segmentation', 'named', 'perseg', 'raw', 'images', 'collected', 'works', 'diffusion', 'models', 'gal', 'et', 'ruiz', 'et', 'kumari', 'et', 'containing', 'various', 'categories', 'visual', 'concepts', 'different', 'poses', 'scenes', 'paper', 'propose', 'two', 'efficient', 'solutions', 'task', 'specifically', 'illustrate', 'follows', 'persam', 'location', 'confidence', 'map', 'conditioned', 'image', 'ir', 'mask', 'mr', 'persam', 'first', 'obtains', 'confidence', 'map', 'indicates', 'location', 'target', 'object', 'new', 'test', 'image', 'shown', 'figure', 'apply', 'image', 'encoder', 'extract', 'visual', 'features', 'ir', 'encoder', 'sam', 'frozen', 'backbone', 'vision', 'models', 'adopt', 'sam', 'image', 'encoder', 'enc', 'default', 'formulate', 'process', 'enc', 'fr', 'enci', 'ir', 'fr', 'є', 'utilize', 'reference', 'mask', 'mr', 'є', 'crop', 'features', 'foreground', 'pixels', 'within', 'visual', 'concept', 'fr', 'resulting', 'set', 'n', 'local', 'features', 'mr', 'fr', 'є', 'denotes', 'multiplication', 'calculate', 'n', 'confidence', 'maps', 'foreground', 'pixel', 'cosine', 'similarity', 'test', 'image', 'feature', 'fį', 'rhxw', 'note', 'fi', 'represents', 'distribution', 'probability', 'different', 'local', 'part', 'object', 'test', 'image', 'head', 'body', 'paws', 'dog', 'top', 'adopt', 'average', 'pooling', 'aggregate', 'n', 'local', 'maps', 'obtain', 'overall', 'confidence', 'map', 'target', 'object', 'n', 'n', 'siєrhxw', 'incorporating', 'confidences', 'every', 'foreground', 'pixel', 'take', 'visual', 'appearance', 'different', 'object', 'parts', 'consideration', 'acquire', 'relatively', 'comprehensive', 'location', 'estimation', 'location', 'prior', 'provide', 'persam', 'location', 'prior', 'test', 'image', 'select', 'two', 'points', 'highest', 'lowest', 'confidence', 'values', 'denoted', 'pɲ', 'pɩ', 'respectively', 'former', 'represents', 'likely', 'center', 'position', 'target', 'object', 'latter', 'inversely', 'indicates', 'background', 'regarded', 'positive', 'negative', 'point', 'prompts', 'fed', 'prompt', 'encoder', 'tp', 'encp', 'ph', 'є', 'denote', 'prompt', 'tokens', 'sam', 'decoder', 'way', 'sam', 'would', 'tend', 'segment', 'contiguous', 'region', 'surrounding', 'positive', 'point', 'discarding', 'negative', 'one', 'image', 'attention', 'although', 'point', 'prompt', 'obtained', 'propose', 'explicit', 'semantic', 'guidance', 'operation', 'sam', 'decoder', 'concentrates', 'feature', 'aggregation', 'within', 'foreground', 'target', 'regions', 'shown', 'figure', 'overall', 'confidence', 'map', 'equation', 'clearly', 'indicate', 'rough', 'region', 'target', 'visual', 'concept', 'test', 'image', 'hotter', 'colors', 'indicate', 'higher', 'scores', 'based', 'property', 'utilize', 'guide', 'attention', 'map', 'every', 'layer', 'decoder', 'specifically', 'denote', 'every', 'attention', 'map', 'softmax', 'function', 'rhxw', 'modulate', 'attention', 'distribution', 'softmax', 'softmax', 'denotes', 'balancing', 'factor', 'attention', 'bias', 'mask', 'prompt', 'tokens', 'compelled', 'capture', 'visual', 'semantics', 'associated', 'target', 'subject', 'unimportant', 'background', 'area', 'contributes', 'effective', 'feature', 'aggregation', 'attention', 'mechanisms', 'enhances', 'final', 'segmentation', 'accuracy', 'persam', 'manner', 'prompting', 'vanilla', 'sam', 'receives', 'prompts', 'positional', 'information', 'coordinate', 'point', 'box', 'provide', 'sam', 'decoder', 'level', 'cues', 'propose', 'utilize', 'visual', 'feature', 'target', 'concept', 'additional', 'semantic', 'prompting', 'first', 'obtain', 'global', 'embedding', 'tr', 'object', 'reference', 'image', 'average', 'pooling', 'different', 'local', 'features', 'n', 'tr', 'στα', 'n', 'add', 'tr', 'input', 'tokens', 'test', 'image', 'equation', 'feeding', 'decoder', 'block', 'shown', 'figure', 'repeat', 'tr', 'concat', 'tm', 'tp', 'denotes', 'input', 'token', 'guided', 'target', 'semantics', 'decoder', 'decм', 'repeat', 'operation', 'duplicates', 'target', 'visual', 'embedding', 'aided', 'simple', 'token', 'incorporation', 'persam', 'prompted', 'location', 'points', 'also', 'target', 'visual', 'cues', 'test', 'image', 'persam', 'output', 'three', 'scales', 'random', 'noise', 'dreambooth', 'v', 'cat', 'reconstruction', 'loss', 'f', 'user', 'provides', 'output', 'mask', 'weighted', 'summation', 'freeze', 'persam', 'background', 'disturbance', 'decouple', 'figure', 'alleviate', 'scale', 'ambiguity', 'adopts', 'two', 'learnable', 'weights', 'adaptively', 'aggregating', 'masks', 'figure', 'dreambooth', 'utilize', 'persam', 'decouple', 'target', 'jects', 'background', 'improving', 'generation', 'dreambooth', 'cascaded', 'via', 'techniques', 'obtain', 'initial', 'segmentation', 'mask', 'test', 'image', 'sam', 'decoder', 'however', 'might', 'include', 'rough', 'edges', 'isolated', 'background', 'noises', 'refinement', 'iteratively', 'feed', 'mask', 'back', 'decoder', 'decм', 'first', 'step', 'prompt', 'decoder', 'currently', 'predicted', 'mask', 'along', 'previous', 'point', 'prompt', 'second', 'step', 'acquire', 'bounding', 'box', 'enclosing', 'mask', 'first', 'step', 'prompt', 'decoder', 'additionally', 'box', 'accurate', 'object', 'localization', 'iterate', 'lightweight', 'decoder', 'without', 'image', 'encoder', 'efficient', 'costs', 'extra', 'latency', 'ambiguity', 'segmentation', 'scales', 'persam', 'tackle', 'cases', 'tory', 'segmentation', 'accuracy', 'however', 'target', 'objects', 'contain', 'hierarchical', 'structures', 'leads', 'ambiguity', 'mask', 'scales', 'shown', 'figure', 'teapot', 'top', 'platform', 'comprised', 'two', 'parts', 'lid', 'body', 'positive', 'point', 'prompt', 'denoted', 'green', 'pentagram', 'located', 'body', 'negative', 'prompt', 'denoted', 'red', 'pentagram', 'exclude', 'platform', 'similar', 'color', 'persam', 'would', 'misled', 'segmentation', 'issue', 'also', 'discussed', 'sam', 'kirillov', 'et', 'proposes', 'alternative', 'simultaneously', 'generate', 'multiple', 'masks', 'three', 'scales', 'corresponding', 'whole', 'part', 'subpart', 'object', 'user', 'required', 'manually', 'select', 'one', 'mask', 'three', 'effective', 'consumes', 'extra', 'manpower', 'contrast', 'personalized', 'task', 'aims', 'customize', 'sam', 'automatic', 'object', 'segmentation', 'without', 'need', 'human', 'prompting', 'motivates', 'us', 'develop', 'version', 'persam', 'adaptive', 'segmentation', 'appropriate', 'scale', 'introduce', 'variant', 'unlike', 'model', 'producing', 'one', 'mask', 'first', 'follows', 'persam', 'obtain', 'location', 'prior', 'refers', 'sam', 'original', 'solution', 'output', 'masks', 'denoted', 'respectively', 'top', 'adopt', 'two', 'learnable', 'mask', 'weights', 'calculate', 'final', 'mask', 'output', 'weighted', 'summation', 'initialized', 'learn', 'optimal', 'weights', 'conduct', 'tuning', 'reference', 'image', 'regard', 'given', 'mask', 'ground', 'truth', 'note', 'freeze', 'entire', 'sam', 'model', 'preserve', 'knowledge', 'parameters', 'within', 'seconds', 'single', 'gpu', 'way', 'efficiently', 'learns', 'semantics', 'objects', 'adaptively', 'outputs', 'best', 'segmentation', 'scale', 'different', 'concepts', 'improving', 'generalization', 'capacity', 'persam', 'table', 'personalized', 'object', 'segmentation', 'perseg', 'dataset', 'compare', 'overall', 'miou', 'blou', 'learnable', 'parameters', 'different', 'methods', 'bar', 'et', 'wang', 'et', 'zou', 'et', 'along', 'miou', 'objects', 'perseg', 'denotes', 'works', 'concurrent', 'method', 'miou', 'blou', 'param', 'barn', 'clock', 'cat', 'teddy', 'duck', 'thin', 'red', 'pack', 'bear', 'toy', 'bird', 'cartoon', 'robot', 'toy', 'painter', 'vp', 'seem', 'seggpt', 'persam', 'table', 'video', 'object', 'tation', 'davis', 'val', 'tuset', 'et', 'utilize', 'gray', 'color', 'denote', 'methods', 'involving', 'training', 'table', 'semantic', 'part', 'segmentation', 'li', 'et', 'gupta', 'et', 'part', 'morabia', 'et', 'ramanathan', 'et', 'report', 'miou', 'scores', 'utilize', 'gray', 'color', 'denote', 'methods', 'involving', 'training', 'semantic', 'seg', 'part', 'seg', 'painter', 'seem', 'seggpt', 'persam', 'method', 'j', 'f', 'і', 'agss', 'f', 'method', 'hsnet', 'vat', 'painter', 'seggpt', 'persam', 'dreambooth', 'personalized', 'synthesis', 'dreambooth', 'ruiz', 'et', 'diffusion', 'model', 'given', 'photos', 'specific', 'object', 'pet', 'cat', 'learns', 'generate', 'cat', 'referred', 'text', 'prompt', 'v', 'cat', 'calculates', 'loss', 'entire', 'reconstructed', 'images', 'would', 'inject', 'redundant', 'background', 'information', 'training', 'images', 'identifier', 'v', 'therefore', 'shown', 'figure', 'introduce', 'strategy', 'alleviate', 'disturbance', 'backgrounds', 'dreambooth', 'given', 'object', 'mask', 'images', 'leverage', 'persam', 'segment', 'foreground', 'targets', 'discard', 'gradient', 'pixels', 'belonging', 'background', 'area', 'stable', 'diffusion', 'memorize', 'visual', 'appearances', 'target', 'object', 'supervision', 'imposed', 'background', 'assisted', 'dreambooth', 'synthesize', 'target', 'object', 'better', 'visual', 'correspondence', 'also', 'increase', 'diversity', 'new', 'backgrounds', 'guided', 'input', 'text', 'prompt', 'experiment', 'first', 'evaluate', 'approach', 'personalized', 'segmentation', 'perseg', 'section', 'along', 'various', 'existing', 'segmentation', 'benchmarks', 'section', 'illustrate', 'effectiveness', 'dreambooth', 'section', 'finally', 'conduct', 'several', 'ablation', 'studies', 'investigate', 'designs', 'perseg', 'section', 'personalized', 'evaluation', 'perseg', 'dataset', 'test', 'personalization', 'capacity', 'construct', 'new', 'segmentation', 'dataset', 'termed', 'perseg', 'raw', 'images', 'collected', 'training', 'data', 'diffusion', 'works', 'ruiz', 'et', 'gal', 'et', 'kumari', 'et', 'perseg', 'contains', 'objects', 'various', 'categories', 'total', 'including', 'daily', 'necessities', 'animals', 'buildings', 'different', 'poses', 'scenes', 'object', 'associated', 'images', 'masks', 'fix', 'one', 'pair', 'data', 'miou', 'blou', 'cheng', 'et', 'adopted', 'evaluation', 'please', 'refer', 'appendix', 'implementation', 'details', 'enlarged', 'data', 'scale', 'perseg', 'ring', 'clock', 'teapot', 'tray', 'backpack', 'carried', 'top', 'part', 'woman', 'figure', 'visualization', 'provement', 'well', 'alleviate', 'scale', 'ambiguity', 'persam', 'figure', 'visualization', 'video', 'object', 'mentation', 'approach', 'performs', 'well', 'segmenting', 'multiple', 'objects', 'video', 'performance', 'table', 'observe', 'achieves', 'best', 'results', 'effectively', 'enhances', 'persam', 'overall', 'miou', 'biou', 'show', 'tion', 'improvement', 'figure', 'visual', 'prompting', 'vp', 'bar', 'et', 'painter', 'wang', 'et', 'seem', 'zou', 'et', 'seggpt', 'wang', 'et', 'learners', 'also', 'segment', 'objects', 'according', 'given', 'prompt', 'data', 'shown', 'persam', 'already', 'achieve', 'better', 'performance', 'painter', 'vp', 'seem', 'different', 'margins', 'efficient', 'surpasses', 'powerful', 'seggpt', 'overall', 'miou', 'biou', 'different', 'motivations', 'develop', 'segmentation', 'generalists', 'method', 'specially', 'designed', 'personalized', 'object', 'segmentation', 'exhibits', 'much', 'efficiency', 'time', 'computational', 'resources', 'existing', 'segmentation', 'benchmarks', 'video', 'object', 'segmentation', 'given', 'image', 'object', 'masks', 'persam', 'achieve', 'competitive', 'object', 'segmentation', 'tracking', 'performance', 'validation', 'set', 'davis', 'et', 'shown', 'table', 'compared', 'methods', 'without', 'video', 'training', 'persam', 'largely', 'surpasses', 'painter', 'j', 'f', 'score', 'achieve', 'better', 'performance', 'seggpt', 'notably', 'approach', 'outperform', 'methods', 'lin', 'et', 'liang', 'et', 'fully', 'trained', 'extensive', 'video', 'data', 'results', 'fully', 'illustrate', 'strong', 'generalization', 'ability', 'temporal', 'video', 'data', 'complex', 'scenarios', 'contain', 'multiple', 'similar', 'occluded', 'objects', 'visualized', 'figure', 'semantic', 'part', 'segmentation', 'table', 'evaluate', 'approach', 'image', 'segmentation', 'respectively', 'four', 'datasets', 'li', 'et', 'gupta', 'et', 'morabia', 'et', 'ramanathan', 'et', 'follow', 'matcher', 'liu', 'et', 'data', 'evaluation', 'shown', 'attains', 'consistently', 'better', 'results', 'painter', 'performs', 'comparably', 'seggpt', 'models', 'min', 'et', 'hong', 'et', 'training', 'approach', 'achieve', 'higher', 'scores', 'hsnet', 'experiments', 'well', 'demonstrate', 'proposed', 'approach', 'limited', 'segmentation', 'also', 'works', 'personalization', 'sam', 'dreambooth', 'follow', 'hyperparameters', 'dreambooth', 'ruiz', 'et', 'stable', 'diffusion', 'rombach', 'et', 'personalized', 'image', 'synthesis', 'addition', 'figure', 'visualize', 'examples', 'dreambooth', 'figure', 'dog', 'lying', 'grey', 'sofa', 'jungle', 'snow', 'dreambooth', 'still', 'sofa', 'green', 'white', 'decorations', 'assisted', 'background', 'totally', 'decoupled', 'sofa', 'well', 'corresponds', 'textual', 'prompt', 'barn', 'front', 'mountains', 'approach', 'also', 'alleviates', 'background', 'disturbance', 'correctly', 'generate', 'forest', 'blue', 'sky', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'user', 'provides', 'dreambooth', 'assisted', 'persam', 'photo', 'dog', 'v', 'dog', 'jungle', 'photo', 'barn', 'v', 'barn', 'forest', 'background', 'v', 'dog', 'snow', 'v', 'barn', 'blue', 'sky', 'background', 'figure', 'visualization', 'dreambooth', 'improved', 'dreambooth', 'ruiz', 'et', 'better', 'preserve', 'diversity', 'synthesizing', 'various', 'contexts', 'new', 'images', 'table', 'ablation', 'main', 'ponents', 'proposed', 'method', 'variant', 'table', 'ablation', 'different', 'methods', 'table', 'ablation', 'using', 'reference', 'miou', 'gain', 'method', 'persam', 'param', 'miou', 'method', 'mask', 'box', 'painter', 'prompt', 'tuning', 'vp', 'adapter', 'seem', 'guided', 'attention', 'semantic', 'prompt', 'lora', 'seggpt', 'mask', 'weights', 'scale', 'tuning', 'persam', 'positive', 'prior', 'negative', 'prior', 'ablation', 'study', 'main', 'components', 'table', 'investigate', 'different', 'components', 'starting', 'baseline', 'adopts', 'positive', 'location', 'prior', 'add', 'negative', 'point', 'prompt', 'cascaded', 'enhancing', 'miou', 'respectively', 'top', 'introduce', 'target', 'semantics', 'sam', 'decoder', 'attention', 'guidance', 'semantic', 'prompting', 'resulting', 'improvements', 'fully', 'indicate', 'significance', 'finally', 'via', 'efficient', 'boosts', 'score', 'demonstrating', 'superior', 'accuracy', 'different', 'methods', 'table', 'experiment', 'peft', 'methods', 'prompt', 'tuning', 'liu', 'et', 'adapter', 'houlsby', 'et', 'lora', 'hu', 'et', 'freeze', 'entire', 'sam', 'tune', 'peft', 'modules', 'injected', 'every', 'transformer', 'block', 'persam', 'decoder', 'shown', 'prompt', 'tuning', 'adapter', 'would', 'data', 'severely', 'degrade', 'accuracy', 'instead', 'best', 'improve', 'performance', 'persam', 'tuning', 'least', 'learnable', 'parameters', 'using', 'reference', 'requiring', 'accurate', 'mask', 'data', 'might', 'strict', 'users', 'table', 'relax', 'input', 'restrictions', 'bounding', 'box', 'designating', 'expected', 'object', 'method', 'regard', 'box', 'prompt', 'utilize', 'sam', 'generate', 'mask', 'therefore', 'box', 'reference', 'leads', 'marginal', 'performance', 'drop', 'persam', 'severely', 'influences', 'methods', 'conclusion', 'paper', 'propose', 'personalize', 'segment', 'anything', 'model', 'sam', 'specific', 'visual', 'concepts', 'data', 'firstly', 'introduce', 'persam', 'injects', 'target', 'semantics', 'sam', 'techniques', 'top', 'present', 'variant', 'learnable', 'parameters', 'effectively', 'alleviates', 'ambiguity', 'mask', 'scales', 'achieves', 'leading', 'performance', 'various', 'benchmarks', 'besides', 'also', 'verify', 'efficacy', 'approach', 'assist', 'dreambooth', 'better', 'diffusion', 'models', 'hope', 'work', 'may', 'expand', 'applicability', 'sam', 'wider', 'range', 'scenarios', 'references', 'vijay', 'badrinarayanan', 'alex', 'kendall', 'roberto', 'cipolla', 'segnet', 'deep', 'convolutional', 'decoder', 'architecture', 'image', 'segmentation', 'ieee', 'transactions', 'pattern', 'analysis', 'machine', 'intelligence', 'amir', 'bar', 'yossi', 'gandelsman', 'trevor', 'darrell', 'amir', 'globerson', 'alexei', 'efros', 'visual', 'prompting', 'via', 'image', 'inpainting', 'advances', 'neural', 'information', 'processing', 'systems', 'tom', 'brown', 'benjamin', 'mann', 'nick', 'ryder', 'melanie', 'subbiah', 'jared', 'kaplan', 'prafulla', 'dhariwal', 'arvind', 'neelakantan', 'pranav', 'shyam', 'girish', 'sastry', 'amanda', 'askell', 'et', 'al', 'language', 'models', 'learners', 'advances', 'neural', 'information', 'processing', 'systems', 'jiazhong', 'cen', 'zanwei', 'zhou', 'jiemin', 'fang', 'wei', 'shen', 'lingxi', 'xie', 'xiaopeng', 'zhang', 'qi', 'tian', 'segment', 'anything', 'nerfs', 'arxiv', 'preprint', 'chen', 'george', 'papandreou', 'iasonas', 'kokkinos', 'kevin', 'murphy', 'alan', 'l', 'yuille', 'deeplab', 'semantic', 'image', 'segmentation', 'deep', 'convolutional', 'nets', 'atrous', 'convolution', 'fully', 'connected', 'crfs', 'ieee', 'transactions', 'pattern', 'analysis', 'machine', 'intelligence', 'xi', 'chen', 'zhiyan', 'zhao', 'feiwu', 'yu', 'yilei', 'zhang', 'manni', 'duan', 'conditional', 'diffusion', 'interactive', 'segmentation', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'zhe', 'chen', 'yuchen', 'duan', 'wenhai', 'wang', 'junjun', 'tong', 'lu', 'jifeng', 'dai', 'yu', 'qiao', 'vision', 'transformer', 'adapter', 'dense', 'predictions', 'arxiv', 'preprint', 'bowen', 'cheng', 'ross', 'girshick', 'piotr', 'dollár', 'alexander', 'c', 'berg', 'alexander', 'kirillov', 'boundary', 'iou', 'improving', 'image', 'segmentation', 'evaluation', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'bowen', 'cheng', 'ishan', 'misra', 'alexander', 'g', 'schwing', 'alexander', 'kirillov', 'rohit', 'girdhar', 'attention', 'mask', 'transformer', 'universal', 'image', 'segmentation', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'pedro', 'cuenca', 'sayak', 'paul', 'using', 'lora', 'efficient', 'stable', 'diffusion', 'https', 'hugging', 'january', 'jacob', 'devlin', 'chang', 'kenton', 'lee', 'kristina', 'toutanova', 'bert', 'deep', 'bidirectional', 'transformers', 'language', 'understanding', 'arxiv', 'preprint', 'rinon', 'gal', 'yuval', 'alaluf', 'yuval', 'atzmon', 'patashnik', 'amit', 'h', 'bermano', 'gal', 'chechik', 'daniel', 'image', 'worth', 'one', 'word', 'personalizing', 'generation', 'using', 'textual', 'inversion', 'arxiv', 'preprint', 'agrim', 'gupta', 'piotr', 'dollar', 'ross', 'girshick', 'lvis', 'dataset', 'large', 'vocabulary', 'instance', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yuying', 'hao', 'yi', 'liu', 'zewu', 'wu', 'lin', 'han', 'yizhou', 'chen', 'guowei', 'chen', 'lutao', 'chu', 'shiyu', 'tang', 'zhiliang', 'yu', 'zeyu', 'chen', 'et', 'al', 'edgeflow', 'achieving', 'practical', 'interactive', 'segmentation', 'flow', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'junxian', 'chunting', 'zhou', 'xuezhe', 'taylor', 'graham', 'neubig', 'towards', 'unified', 'view', 'transfer', 'learning', 'international', 'conference', 'learning', 'representations', 'url', 'https', 'kaiming', 'georgia', 'gkioxari', 'piotr', 'dollár', 'ross', 'girshick', 'mask', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'lukas', 'hedegaard', 'aman', 'alok', 'juby', 'jose', 'alexandros', 'iosifidis', 'structured', 'pruning', 'adapters', 'arxiv', 'preprint', 'sunghwan', 'hong', 'seokju', 'cho', 'jisu', 'nam', 'stephen', 'lin', 'seungryong', 'kim', 'cost', 'aggregation', 'convolutional', 'swin', 'transformer', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'neil', 'houlsby', 'andrei', 'giurgiu', 'stanislaw', 'jastrzebski', 'bruna', 'morrone', 'quentin', 'de', 'laroussilhe', 'andrea', 'gesmundo', 'mona', 'attariyan', 'sylvain', 'gelly', 'transfer', 'learning', 'nlp', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'edward', 'j', 'hu', 'yelong', 'shen', 'phillip', 'wallis', 'zeyuan', 'yuanzhi', 'li', 'shean', 'wang', 'lu', 'wang', 'weizhu', 'chen', 'lora', 'adaptation', 'large', 'language', 'models', 'arxiv', 'preprint', 'yuhao', 'huang', 'xin', 'yang', 'lian', 'liu', 'han', 'zhou', 'ao', 'chang', 'xinrui', 'zhou', 'rusi', 'chen', 'junxuan', 'yu', 'jiongquan', 'chen', 'chaoyu', 'chen', 'et', 'al', 'segment', 'anything', 'model', 'medical', 'images', 'arxiv', 'preprint', 'chao', 'jia', 'yinfei', 'yang', 'ye', 'xia', 'chen', 'zarana', 'parekh', 'hieu', 'pham', 'quoc', 'le', 'sung', 'zhen', 'li', 'tom', 'duerig', 'scaling', 'visual', 'representation', 'learning', 'noisy', 'text', 'supervision', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'menglin', 'jia', 'luming', 'tang', 'chen', 'claire', 'cardie', 'serge', 'belongie', 'bharath', 'hariharan', 'lim', 'visual', 'prompt', 'tuning', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'zhengkai', 'jiang', 'yuxi', 'li', 'ceyuan', 'yang', 'peng', 'gao', 'yabiao', 'wang', 'ying', 'tai', 'chengjie', 'wang', 'totypical', 'contrast', 'adaptation', 'domain', 'adaptive', 'semantic', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'zhengkai', 'jiang', 'zhangxuan', 'gu', 'jinlong', 'peng', 'hang', 'zhou', 'liang', 'liu', 'yabiao', 'wang', 'ying', 'tai', 'chengjie', 'wang', 'liqing', 'zhang', 'stc', 'contrastive', 'learning', 'video', 'instance', 'segmentation', 'european', 'conference', 'computer', 'vision', 'workshops', 'pp', 'springer', 'lei', 'ke', 'mingqiao', 'ye', 'martin', 'danelljan', 'yifan', 'liu', 'tai', 'tang', 'fisher', 'yu', 'segment', 'anything', 'high', 'quality', 'arxiv', 'preprint', 'alexander', 'kirillov', 'kaiming', 'ross', 'girshick', 'carsten', 'rother', 'piotr', 'dollár', 'panoptic', 'tation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'alexander', 'kirillov', 'eric', 'mintun', 'nikhila', 'ravi', 'hanzi', 'mao', 'chloe', 'rolland', 'laura', 'gustafson', 'tete', 'xiao', 'spencer', 'whitehead', 'alexander', 'c', 'berg', 'lo', 'et', 'al', 'segment', 'anything', 'arxiv', 'preprint', 'nupur', 'kumari', 'bingliang', 'zhang', 'richard', 'zhang', 'eli', 'shechtman', 'zhu', 'customization', 'diffusion', 'arxiv', 'preprint', 'brian', 'lester', 'rami', 'noah', 'constant', 'power', 'scale', 'prompt', 'tuning', 'arxiv', 'preprint', 'hao', 'li', 'jinguo', 'zhu', 'xiaohu', 'jiang', 'xizhou', 'zhu', 'hongsheng', 'li', 'chun', 'yuan', 'xiaohua', 'wang', 'yu', 'qiao', 'xiaogang', 'wang', 'wenhai', 'wang', 'et', 'al', 'generalist', 'model', 'vision', 'tasks', 'arxiv', 'preprint', 'junnan', 'li', 'dongxu', 'li', 'silvio', 'savarese', 'steven', 'hoi', 'bootstrapping', 'training', 'frozen', 'image', 'encoders', 'large', 'language', 'models', 'arxiv', 'preprint', 'xiang', 'li', 'tianhan', 'wei', 'yau', 'pun', 'chen', 'tai', 'tang', 'dataset', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yanwei', 'li', 'xinze', 'chen', 'zheng', 'zhu', 'lingxi', 'xie', 'guan', 'huang', 'dalong', 'du', 'xingang', 'wang', 'unified', 'network', 'panoptic', 'segmentation', 'proceedings', 'ieee', 'ence', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'yongqing', 'liang', 'xin', 'li', 'navid', 'jafari', 'jim', 'chen', 'video', 'object', 'segmentation', 'adaptive', 'feature', 'bank', 'refinement', 'advances', 'neural', 'information', 'processing', 'systems', 'huaijia', 'lin', 'xiaojuan', 'qi', 'jiaya', 'jia', 'attention', 'guided', 'video', 'object', 'segmentation', 'proceedings', 'ieee', 'international', 'conference', 'computer', 'vision', 'pp', 'zhaojiang', 'lin', 'andrea', 'madotto', 'pascale', 'fung', 'exploring', 'versatile', 'generative', 'language', 'model', 'via', 'transfer', 'learning', 'arxiv', 'preprint', 'ziyi', 'lin', 'shijie', 'geng', 'renrui', 'zhang', 'peng', 'gao', 'gerard', 'de', 'melo', 'xiaogang', 'wang', 'jifeng', 'dai', 'yu', 'qiao', 'hongsheng', 'li', 'frozen', 'clip', 'models', 'efficient', 'video', 'learners', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'xiao', 'liu', 'kaixuan', 'ji', 'yicheng', 'fu', 'weng', 'lam', 'tam', 'zhengxiao', 'du', 'zhilin', 'yang', 'jie', 'tang', 'prompt', 'tuning', 'comparable', 'universally', 'across', 'scales', 'tasks', 'arxiv', 'preprint', 'yang', 'liu', 'muzhi', 'zhu', 'hengtao', 'li', 'hao', 'chen', 'xinlong', 'wang', 'chunhua', 'shen', 'matcher', 'segment', 'anything', 'one', 'shot', 'using', 'feature', 'matching', 'arxiv', 'preprint', 'jonathan', 'long', 'evan', 'shelhamer', 'trevor', 'darrell', 'fully', 'convolutional', 'networks', 'semantic', 'segmentation', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'jiasen', 'lu', 'dhruv', 'batra', 'devi', 'parikh', 'stefan', 'lee', 'vilbert', 'pretraining', 'visiolinguistic', 'representations', 'tasks', 'advances', 'neural', 'information', 'processing', 'systems', 'neurips', 'pp', 'jun', 'bo', 'wang', 'segment', 'anything', 'medical', 'images', 'arxiv', 'preprint', 'juhong', 'min', 'dahyun', 'kang', 'minsu', 'cho', 'hypercorrelation', 'squeeze', 'segmentation', 'proceedings', 'international', 'conference', 'computer', 'vision', 'pp', 'keval', 'morabia', 'jatin', 'arora', 'tara', 'vijaykumar', 'joint', 'detection', 'object', 'semantic', 'part', 'arxiv', 'preprint', 'openai', 'technical', 'report', 'arxiv', 'jonas', 'pfeiffer', 'aishwarya', 'kamath', 'andreas', 'rücklé', 'kyunghyun', 'cho', 'iryna', 'gurevych', 'fusion', 'task', 'composition', 'transfer', 'learning', 'arxiv', 'preprint', 'jordi', 'federico', 'perazzi', 'sergi', 'caelles', 'pablo', 'arbeláez', 'alex', 'luc', 'van', 'gool', 'davis', 'challenge', 'video', 'object', 'segmentation', 'arxiv', 'preprint', 'guanghui', 'qin', 'jason', 'eisner', 'learning', 'ask', 'querying', 'lms', 'mixtures', 'soft', 'prompts', 'arxiv', 'preprint', 'alec', 'radford', 'karthik', 'narasimhan', 'improving', 'language', 'understanding', 'generative', 'alec', 'radford', 'jeffrey', 'wu', 'rewon', 'child', 'david', 'luan', 'dario', 'amodei', 'ilya', 'sutskever', 'et', 'al', 'language', 'models', 'unsupervised', 'multitask', 'learners', 'openai', 'blog', 'alec', 'radford', 'jong', 'wook', 'kim', 'chris', 'hallacy', 'aditya', 'ramesh', 'gabriel', 'goh', 'sandhini', 'agarwal', 'girish', 'sastry', 'amanda', 'askell', 'pamela', 'mishkin', 'jack', 'clark', 'et', 'al', 'learning', 'transferable', 'visual', 'models', 'natural', 'language', 'supervision', 'international', 'conference', 'machine', 'learning', 'pp', 'pmlr', 'vignesh', 'ramanathan', 'anmol', 'kalia', 'vladan', 'petrovic', 'yi', 'wen', 'baixue', 'zheng', 'baishan', 'guo', 'rui', 'wang', 'aaron', 'marquez', 'rama', 'kovvuri', 'abhishek', 'kadian', 'et', 'al', 'paco', 'parts', 'attributes', 'common', 'objects', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'rebuffi', 'hakan', 'bilen', 'andrea', 'vedaldi', 'learning', 'multiple', 'visual', 'domains', 'residual', 'adapters', 'advances', 'neural', 'information', 'processing', 'systems', 'robin', 'rombach', 'andreas', 'blattmann', 'dominik', 'lorenz', 'patrick', 'esser', 'björn', 'ommer', 'resolution', 'image', 'synthesis', 'latent', 'diffusion', 'models', 'proceedings', 'ence', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'nataniel', 'ruiz', 'yuanzhen', 'li', 'varun', 'jampani', 'yael', 'pritch', 'michael', 'rubinstein', 'kfir', 'aberman', 'dreambooth', 'fine', 'tuning', 'diffusion', 'models', 'generation', 'arxiv', 'preprint', 'chitwan', 'saharia', 'william', 'chan', 'saurabh', 'saxena', 'lala', 'li', 'jay', 'whang', 'emily', 'l', 'denton', 'kamyar', 'ghasemipour', 'raphael', 'gontijo', 'lopes', 'burcu', 'karagol', 'ayan', 'tim', 'salimans', 'et', 'al', 'photorealistic', 'diffusion', 'models', 'deep', 'language', 'understanding', 'advances', 'neural', 'information', 'processing', 'systems', 'lin', 'song', 'yanwei', 'li', 'zhengkai', 'jiang', 'zeming', 'li', 'xiangyu', 'zhang', 'hongbin', 'sun', 'jian', 'sun', 'nanning', 'zheng', 'rethinking', 'learnable', 'tree', 'filter', 'generic', 'feature', 'transform', 'advances', 'neural', 'information', 'processing', 'systems', 'sung', 'jaemin', 'cho', 'mohit', 'bansal', 'transfer', 'learning', 'tasks', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'zhi', 'tian', 'chunhua', 'shen', 'hao', 'chen', 'conditional', 'convolutions', 'instance', 'segmentation', 'european', 'conference', 'computer', 'vision', 'pp', 'springer', 'hugo', 'touvron', 'thibaut', 'lavril', 'gautier', 'izacard', 'xavier', 'martinet', 'lachaux', 'timothée', 'lacroix', 'baptiste', 'rozière', 'naman', 'goyal', 'eric', 'hambro', 'faisal', 'azhar', 'aurelien', 'rodriguez', 'armand', 'joulin', 'edouard', 'grave', 'guillaume', 'lample', 'llama', 'open', 'efficient', 'foundation', 'language', 'models', 'arxiv', 'preprint', 'xinlong', 'wang', 'rufeng', 'zhang', 'tao', 'kong', 'lei', 'li', 'chunhua', 'shen', 'dynamic', 'fast', 'instance', 'segmentation', 'advances', 'neural', 'information', 'processing', 'systems', 'xinlong', 'wang', 'wen', 'wang', 'yue', 'cao', 'chunhua', 'shen', 'tiejun', 'huang', 'images', 'speak', 'images', 'generalist', 'painter', 'visual', 'learning', 'arxiv', 'preprint', 'xinlong', 'wang', 'xiaosong', 'zhang', 'yue', 'cao', 'wen', 'wang', 'chunhua', 'shen', 'tiejun', 'huang', 'seggpt', 'segmenting', 'everything', 'context', 'arxiv', 'preprint', 'enze', 'xie', 'wenhai', 'wang', 'zhiding', 'yu', 'anima', 'anandkumar', 'jose', 'alvarez', 'ping', 'luo', 'segformer', 'simple', 'efficient', 'design', 'semantic', 'segmentation', 'transformers', 'advances', 'neural', 'information', 'processing', 'systems', 'mutian', 'xu', 'junhao', 'zhang', 'zhipeng', 'zhou', 'mingye', 'xu', 'xiaojuan', 'qi', 'yu', 'qiao', 'learning', 'representation', 'complementary', 'understanding', 'object', 'point', 'cloud', 'proceedings', 'aaai', 'conference', 'artificial', 'intelligence', 'volume', 'pp', 'jinyu', 'yang', 'mingqi', 'gao', 'zhe', 'li', 'shang', 'gao', 'fangjing', 'wang', 'feng', 'zheng', 'track', 'anything', 'segment', 'anything', 'meets', 'videos', 'arxiv', 'preprint', 'chaoning', 'zhang', 'dongshen', 'han', 'yu', 'qiao', 'jung', 'uk', 'kim', 'bae', 'seungkyu', 'lee', 'choong', 'seon', 'hong', 'faster', 'segment', 'anything', 'towards', 'lightweight', 'sam', 'mobile', 'applications', 'arxiv', 'preprint', 'qingru', 'zhang', 'minshuo', 'chen', 'alexander', 'bukharin', 'pengcheng', 'yu', 'cheng', 'weizhu', 'chen', 'tuo', 'zhao', 'adaptive', 'budget', 'allocation', 'arxiv', 'preprint', 'renrui', 'zhang', 'jiaming', 'han', 'aojun', 'zhou', 'xiangfei', 'hu', 'shilin', 'yan', 'pan', 'lu', 'hongsheng', 'li', 'peng', 'gao', 'yu', 'qiao', 'efficient', 'language', 'models', 'attention', 'arxiv', 'preprint', 'renrui', 'zhang', 'xiangfei', 'hu', 'bohao', 'li', 'siyuan', 'huang', 'hanqiu', 'deng', 'hongsheng', 'li', 'yu', 'qiao', 'peng', 'gao', 'prompt', 'generate', 'cache', 'cascade', 'foundation', 'models', 'makes', 'strong', 'learners', 'arxiv', 'preprint', 'hengshuang', 'zhao', 'jianping', 'shi', 'xiaojuan', 'qi', 'xiaogang', 'wang', 'jiaya', 'jia', 'pyramid', 'scene', 'parsing', 'network', 'proceedings', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'xu', 'zhao', 'wenchao', 'ding', 'yongqi', 'yinglong', 'du', 'tao', 'yu', 'min', 'li', 'ming', 'tang', 'jinqiao', 'wang', 'fast', 'segment', 'anything', 'arxiv', 'preprint', 'sixiao', 'zheng', 'jiachen', 'lu', 'hengshuang', 'zhao', 'xiatian', 'zhu', 'zekun', 'luo', 'yabiao', 'wang', 'yanwei', 'fu', 'jianfeng', 'feng', 'tao', 'xiang', 'philip', 'hs', 'torr', 'et', 'al', 'rethinking', 'semantic', 'segmentation', 'perspective', 'transformers', 'proceedings', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'kaiyang', 'zhou', 'jingkang', 'yang', 'chen', 'change', 'loy', 'ziwei', 'liu', 'learning', 'prompt', 'language', 'models', 'international', 'journal', 'computer', 'vision', 'xueyan', 'zou', 'jianwei', 'yang', 'hao', 'zhang', 'feng', 'li', 'linjie', 'li', 'jianfeng', 'gao', 'yong', 'jae', 'lee', 'segment', 'everything', 'everywhere', 'arxiv', 'preprint']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oct',\n",
       " 'personalize',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'renrui',\n",
       " 'zhengkai',\n",
       " 'ziyu',\n",
       " 'shilin',\n",
       " 'junting',\n",
       " 'xianzheng',\n",
       " 'hao',\n",
       " 'dongª',\n",
       " 'yu',\n",
       " 'peng',\n",
       " 'hongsheng',\n",
       " 'mmlab',\n",
       " 'shanghai',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'laboratory',\n",
       " 'institute',\n",
       " 'automation',\n",
       " 'chinese',\n",
       " 'academy',\n",
       " 'science',\n",
       " 'school',\n",
       " 'c',\n",
       " 'peking',\n",
       " 'university',\n",
       " 'zhangrenrui',\n",
       " 'gaopeng',\n",
       " 'guoziyu',\n",
       " 'qiaoyu',\n",
       " 'hsli',\n",
       " 'abstract',\n",
       " 'driven',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'demonstrated',\n",
       " 'powerful',\n",
       " 'promptable',\n",
       " 'framework',\n",
       " 'revolutionizing',\n",
       " 'tion',\n",
       " 'field',\n",
       " 'despite',\n",
       " 'generality',\n",
       " 'customizing',\n",
       " 'sam',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'without',\n",
       " 'prompting',\n",
       " 'automatically',\n",
       " 'segmenting',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'numerous',\n",
       " 'image',\n",
       " 'paper',\n",
       " 'introduce',\n",
       " 'personalization',\n",
       " 'approach',\n",
       " 'sam',\n",
       " 'termed',\n",
       " 'persam',\n",
       " 'given',\n",
       " 'data',\n",
       " 'single',\n",
       " 'image',\n",
       " 'reference',\n",
       " 'mask',\n",
       " 'first',\n",
       " 'obtain',\n",
       " 'cation',\n",
       " 'prior',\n",
       " 'target',\n",
       " 'concept',\n",
       " 'new',\n",
       " 'image',\n",
       " 'aided',\n",
       " 'target',\n",
       " 'visual',\n",
       " 'semantics',\n",
       " 'empower',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'via',\n",
       " 'two',\n",
       " 'posed',\n",
       " 'technique',\n",
       " 'attention',\n",
       " 'prompting',\n",
       " 'way',\n",
       " 'effectively',\n",
       " 'customize',\n",
       " 'sam',\n",
       " 'private',\n",
       " 'use',\n",
       " 'without',\n",
       " 'training',\n",
       " 'alleviate',\n",
       " 'ambiguity',\n",
       " 'segmentation',\n",
       " 'scale',\n",
       " 'present',\n",
       " 'efficient',\n",
       " 'variant',\n",
       " 'freezing',\n",
       " 'entire',\n",
       " 'sam',\n",
       " 'introduce',\n",
       " 'aggregate',\n",
       " 'mask',\n",
       " 'tune',\n",
       " 'parameter',\n",
       " 'within',\n",
       " 'second',\n",
       " 'improved',\n",
       " 'performance',\n",
       " 'demonstrate',\n",
       " 'efficacy',\n",
       " 'construct',\n",
       " 'new',\n",
       " 'dataset',\n",
       " 'perseg',\n",
       " 'evaluation',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'also',\n",
       " 'test',\n",
       " 'method',\n",
       " 'various',\n",
       " 'image',\n",
       " 'video',\n",
       " 'segmentation',\n",
       " 'benchmark',\n",
       " 'besides',\n",
       " 'propose',\n",
       " 'leverage',\n",
       " 'persam',\n",
       " 'improve',\n",
       " 'dreambooth',\n",
       " 'personalized',\n",
       " 'synthesis',\n",
       " 'mitigating',\n",
       " 'disturbance',\n",
       " 'background',\n",
       " 'approach',\n",
       " 'showcase',\n",
       " 'better',\n",
       " 'target',\n",
       " 'appearance',\n",
       " 'generation',\n",
       " 'higher',\n",
       " 'fidelity',\n",
       " 'input',\n",
       " 'text',\n",
       " 'prompt',\n",
       " 'code',\n",
       " 'released',\n",
       " 'http',\n",
       " 'user',\n",
       " 'provides',\n",
       " 'learning',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'one',\n",
       " 'image',\n",
       " 'one',\n",
       " 'mask',\n",
       " 'persam',\n",
       " 'various',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'figure',\n",
       " 'personalization',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'customize',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'data',\n",
       " 'introduce',\n",
       " 'two',\n",
       " 'efficient',\n",
       " 'solution',\n",
       " 'persam',\n",
       " 'equal',\n",
       " 'contribution',\n",
       " 'corresponding',\n",
       " 'author',\n",
       " 'parameter',\n",
       " 'persam',\n",
       " 'user',\n",
       " 'provides',\n",
       " 'dreambooth',\n",
       " 'assisted',\n",
       " 'persam',\n",
       " 'second',\n",
       " 'hat',\n",
       " 'teddy',\n",
       " 'bear',\n",
       " 'photo',\n",
       " 'v',\n",
       " 'cat',\n",
       " 'v',\n",
       " 'cat',\n",
       " 'beach',\n",
       " 'various',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'body',\n",
       " 'robot',\n",
       " 'toy',\n",
       " 'figure',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'ples',\n",
       " 'persam',\n",
       " 'left',\n",
       " 'segment',\n",
       " 'personal',\n",
       " 'object',\n",
       " 'context',\n",
       " 'favorable',\n",
       " 'mance',\n",
       " 'right',\n",
       " 'alleviates',\n",
       " 'ambiguity',\n",
       " 'issue',\n",
       " 'introduction',\n",
       " 'photo',\n",
       " 'v',\n",
       " 'backpack',\n",
       " 'v',\n",
       " 'backpack',\n",
       " 'table',\n",
       " 'classroom',\n",
       " 'figure',\n",
       " 'improving',\n",
       " 'dreambooth',\n",
       " 'ruiz',\n",
       " 'et',\n",
       " 'persam',\n",
       " 'mitigating',\n",
       " 'bance',\n",
       " 'background',\n",
       " 'training',\n",
       " 'proach',\n",
       " 'help',\n",
       " 'achieve',\n",
       " 'alized',\n",
       " 'generation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'vision',\n",
       " 'li',\n",
       " 'et',\n",
       " 'zou',\n",
       " 'et',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'language',\n",
       " 'brown',\n",
       " 'et',\n",
       " 'touvron',\n",
       " 'et',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'jia',\n",
       " 'et',\n",
       " 'li',\n",
       " 'et',\n",
       " 'gained',\n",
       " 'unprecedented',\n",
       " 'prevalence',\n",
       " 'attributed',\n",
       " 'availability',\n",
       " 'datasets',\n",
       " 'computational',\n",
       " 'resource',\n",
       " 'demonstrate',\n",
       " 'extraordinary',\n",
       " 'generalization',\n",
       " 'capacity',\n",
       " 'scenario',\n",
       " 'display',\n",
       " 'versatile',\n",
       " 'interactivity',\n",
       " 'incorporating',\n",
       " 'human',\n",
       " 'feedback',\n",
       " 'inspired',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'develops',\n",
       " 'delicate',\n",
       " 'data',\n",
       " 'engine',\n",
       " 'collecting',\n",
       " 'data',\n",
       " 'subsequently',\n",
       " 'train',\n",
       " 'segmentation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'known',\n",
       " 'sam',\n",
       " 'defines',\n",
       " 'novel',\n",
       " 'promptable',\n",
       " 'segmentation',\n",
       " 'framework',\n",
       " 'taking',\n",
       " 'input',\n",
       " 'handcrafted',\n",
       " 'prompt',\n",
       " 'returning',\n",
       " 'expected',\n",
       " 'mask',\n",
       " 'allows',\n",
       " 'segmenting',\n",
       " 'object',\n",
       " 'visual',\n",
       " 'context',\n",
       " 'however',\n",
       " 'sam',\n",
       " 'inherently',\n",
       " 'loses',\n",
       " 'capability',\n",
       " 'segment',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'imagine',\n",
       " 'intending',\n",
       " 'crop',\n",
       " 'lovely',\n",
       " 'pet',\n",
       " 'dog',\n",
       " 'thick',\n",
       " 'photo',\n",
       " 'album',\n",
       " 'find',\n",
       " 'missing',\n",
       " 'clock',\n",
       " 'picture',\n",
       " 'bedroom',\n",
       " 'utilizing',\n",
       " 'vanilla',\n",
       " 'sam',\n",
       " 'would',\n",
       " 'highly',\n",
       " 'image',\n",
       " 'must',\n",
       " 'precisely',\n",
       " 'find',\n",
       " 'target',\n",
       " 'object',\n",
       " 'within',\n",
       " 'complicated',\n",
       " 'context',\n",
       " 'activate',\n",
       " 'sam',\n",
       " 'proper',\n",
       " 'prompt',\n",
       " 'segmentation',\n",
       " 'considering',\n",
       " 'ask',\n",
       " 'personalize',\n",
       " 'sam',\n",
       " 'automatically',\n",
       " 'segment',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'simple',\n",
       " 'efficient',\n",
       " 'manner',\n",
       " 'end',\n",
       " 'introduce',\n",
       " 'persam',\n",
       " 'personalization',\n",
       " 'approach',\n",
       " 'segment',\n",
       " 'anything',\n",
       " 'model',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'method',\n",
       " 'efficiently',\n",
       " 'customizes',\n",
       " 'sam',\n",
       " 'using',\n",
       " 'data',\n",
       " 'reference',\n",
       " 'image',\n",
       " 'rough',\n",
       " 'mask',\n",
       " 'personal',\n",
       " 'concept',\n",
       " 'specifically',\n",
       " 'first',\n",
       " 'obtain',\n",
       " 'location',\n",
       " 'confidence',\n",
       " 'map',\n",
       " 'target',\n",
       " 'object',\n",
       " 'test',\n",
       " 'image',\n",
       " 'feature',\n",
       " 'similarity',\n",
       " 'considers',\n",
       " 'appearance',\n",
       " 'every',\n",
       " 'foreground',\n",
       " 'pixel',\n",
       " 'according',\n",
       " 'confidence',\n",
       " 'score',\n",
       " 'two',\n",
       " 'point',\n",
       " 'selected',\n",
       " 'location',\n",
       " 'prior',\n",
       " 'finally',\n",
       " 'encoded',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'fed',\n",
       " 'sam',\n",
       " 'decoder',\n",
       " 'segmentation',\n",
       " 'within',\n",
       " 'decoder',\n",
       " 'propose',\n",
       " 'inject',\n",
       " 'visual',\n",
       " 'semantics',\n",
       " 'target',\n",
       " 'object',\n",
       " 'unleash',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'power',\n",
       " 'two',\n",
       " 'technique',\n",
       " 'attention',\n",
       " 'guide',\n",
       " 'every',\n",
       " 'layer',\n",
       " 'sam',\n",
       " 'decoder',\n",
       " 'location',\n",
       " 'confidence',\n",
       " 'map',\n",
       " 'explicitly',\n",
       " 'compels',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'mainly',\n",
       " 'concentrate',\n",
       " 'foreground',\n",
       " 'target',\n",
       " 'region',\n",
       " 'intensive',\n",
       " 'feature',\n",
       " 'aggregation',\n",
       " 'prompting',\n",
       " 'explicitly',\n",
       " 'provide',\n",
       " 'sam',\n",
       " 'target',\n",
       " 'semantics',\n",
       " 'fuse',\n",
       " 'original',\n",
       " 'prompt',\n",
       " 'token',\n",
       " 'embedding',\n",
       " 'target',\n",
       " 'object',\n",
       " 'provides',\n",
       " 'positional',\n",
       " 'prompt',\n",
       " 'additional',\n",
       " 'visual',\n",
       " 'cue',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'aforementioned',\n",
       " 'design',\n",
       " 'along',\n",
       " 'cascaded',\n",
       " 'persam',\n",
       " 'exhibit',\n",
       " 'favorable',\n",
       " 'personalized',\n",
       " 'segmentation',\n",
       " 'performance',\n",
       " 'unique',\n",
       " 'subject',\n",
       " 'variety',\n",
       " 'pose',\n",
       " 'scene',\n",
       " 'notably',\n",
       " 'approach',\n",
       " 'cope',\n",
       " 'well',\n",
       " 'scenario',\n",
       " 'require',\n",
       " 'segmenting',\n",
       " 'one',\n",
       " 'object',\n",
       " 'among',\n",
       " 'multiple',\n",
       " 'similar',\n",
       " 'one',\n",
       " 'simultaneously',\n",
       " 'segmenting',\n",
       " 'several',\n",
       " 'identical',\n",
       " 'object',\n",
       " 'image',\n",
       " 'tracking',\n",
       " 'different',\n",
       " 'object',\n",
       " 'along',\n",
       " 'video',\n",
       " 'nevertheless',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'might',\n",
       " 'occasional',\n",
       " 'failure',\n",
       " 'case',\n",
       " 'object',\n",
       " 'comprises',\n",
       " 'visually',\n",
       " 'distinct',\n",
       " 'subpart',\n",
       " 'hierarchical',\n",
       " 'structure',\n",
       " 'segmented',\n",
       " 'hat',\n",
       " 'top',\n",
       " 'teddy',\n",
       " 'bear',\n",
       " 'head',\n",
       " 'robot',\n",
       " 'toy',\n",
       " 'ambiguity',\n",
       " 'cast',\n",
       " 'challenge',\n",
       " 'persam',\n",
       " 'determining',\n",
       " 'appropriate',\n",
       " 'scale',\n",
       " 'mask',\n",
       " 'output',\n",
       " 'since',\n",
       " 'local',\n",
       " 'part',\n",
       " 'global',\n",
       " 'shape',\n",
       " 'regarded',\n",
       " 'valid',\n",
       " 'mask',\n",
       " 'sam',\n",
       " 'alleviate',\n",
       " 'issue',\n",
       " 'propose',\n",
       " 'variant',\n",
       " 'approach',\n",
       " 'freeze',\n",
       " 'entire',\n",
       " 'sam',\n",
       " 'preserve',\n",
       " 'versatile',\n",
       " 'knowledge',\n",
       " 'parameter',\n",
       " 'within',\n",
       " 'second',\n",
       " 'single',\n",
       " 'gpu',\n",
       " 'detail',\n",
       " 'enable',\n",
       " 'sam',\n",
       " 'produce',\n",
       " 'several',\n",
       " 'potential',\n",
       " 'segmentation',\n",
       " 'result',\n",
       " 'different',\n",
       " 'mask',\n",
       " 'scale',\n",
       " 'adaptively',\n",
       " 'select',\n",
       " 'best',\n",
       " 'scale',\n",
       " 'varying',\n",
       " 'object',\n",
       " 'employ',\n",
       " 'learnable',\n",
       " 'relative',\n",
       " 'weight',\n",
       " 'mask',\n",
       " 'scale',\n",
       " 'conduct',\n",
       " 'weighted',\n",
       " 'summation',\n",
       " 'final',\n",
       " 'output',\n",
       " 'efficient',\n",
       " 'training',\n",
       " 'avoids',\n",
       " 'data',\n",
       " 'exhibit',\n",
       " 'better',\n",
       " 'segmentation',\n",
       " 'accuracy',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'right',\n",
       " 'moreover',\n",
       " 'observe',\n",
       " 'approach',\n",
       " 'also',\n",
       " 'assist',\n",
       " 'dreambooth',\n",
       " 'ruiz',\n",
       " 'et',\n",
       " 'better',\n",
       " 'diffusion',\n",
       " 'model',\n",
       " 'personalized',\n",
       " 'generation',\n",
       " 'shown',\n",
       " 'figure',\n",
       " 'given',\n",
       " 'image',\n",
       " 'containing',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'pet',\n",
       " 'cat',\n",
       " 'backpack',\n",
       " 'dreambooth',\n",
       " 'learns',\n",
       " 'convert',\n",
       " 'image',\n",
       " 'identifier',\n",
       " 'v',\n",
       " 'word',\n",
       " 'embedding',\n",
       " 'space',\n",
       " 'however',\n",
       " 'simultaneously',\n",
       " 'include',\n",
       " 'background',\n",
       " 'information',\n",
       " 'stair',\n",
       " 'forest',\n",
       " 'would',\n",
       " 'override',\n",
       " 'newly',\n",
       " 'prompted',\n",
       " 'background',\n",
       " 'disturb',\n",
       " 'target',\n",
       " 'appearance',\n",
       " 'generation',\n",
       " 'therefore',\n",
       " 'propose',\n",
       " 'leverage',\n",
       " 'persam',\n",
       " 'segment',\n",
       " 'target',\n",
       " 'object',\n",
       " 'within',\n",
       " 'training',\n",
       " 'image',\n",
       " 'supervise',\n",
       " 'dreambooth',\n",
       " 'foreground',\n",
       " 'area',\n",
       " 'enabling',\n",
       " 'synthesis',\n",
       " 'higher',\n",
       " 'quality',\n",
       " 'summarize',\n",
       " 'contribution',\n",
       " 'paper',\n",
       " 'follows',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'first',\n",
       " 'investigate',\n",
       " 'customize',\n",
       " 'purpose',\n",
       " 'segmentation',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'personalized',\n",
       " 'scenario',\n",
       " 'minimal',\n",
       " 'expense',\n",
       " 'end',\n",
       " 'introduce',\n",
       " 'two',\n",
       " 'efficient',\n",
       " 'effective',\n",
       " 'method',\n",
       " 'along',\n",
       " 'new',\n",
       " 'segmentation',\n",
       " 'dataset',\n",
       " 'perseg',\n",
       " 'evaluation',\n",
       " 'personalized',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'persam',\n",
       " 'persam',\n",
       " 'propose',\n",
       " 'three',\n",
       " 'technique',\n",
       " 'guide',\n",
       " 'sam',\n",
       " 'semantics',\n",
       " 'target',\n",
       " 'object',\n",
       " 'design',\n",
       " 'parameter',\n",
       " 'second',\n",
       " 'well',\n",
       " 'alleviate',\n",
       " 'mask',\n",
       " 'ambiguity',\n",
       " 'issue',\n",
       " 'approach',\n",
       " 'achieves',\n",
       " 'competitive',\n",
       " 'result',\n",
       " 'various',\n",
       " 'task',\n",
       " 'including',\n",
       " 'perseg',\n",
       " 'benchmark',\n",
       " 'part',\n",
       " 'semantic',\n",
       " 'segmentation',\n",
       " 'video',\n",
       " 'object',\n",
       " 'segmentation',\n",
       " 'addition',\n",
       " 'persam',\n",
       " 'enhance',\n",
       " 'dreambooth',\n",
       " 'better',\n",
       " 'personalized',\n",
       " 'synthesis',\n",
       " 'related',\n",
       " 'work',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'powerful',\n",
       " 'generalization',\n",
       " 'capacity',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'adapted',\n",
       " 'various',\n",
       " 'downstream',\n",
       " 'scenario',\n",
       " 'attain',\n",
       " 'promising',\n",
       " 'performance',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'bert',\n",
       " 'devlin',\n",
       " 'et',\n",
       " 'lu',\n",
       " 'et',\n",
       " 'gpt',\n",
       " 'series',\n",
       " 'brown',\n",
       " 'et',\n",
       " 'openai',\n",
       " 'radford',\n",
       " 'narasimhan',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'llama',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'demonstrated',\n",
       " 'remarkable',\n",
       " 'learning',\n",
       " 'ability',\n",
       " 'transferred',\n",
       " 'new',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'prompt',\n",
       " 'similarly',\n",
       " 'clip',\n",
       " 'radford',\n",
       " 'et',\n",
       " 'align',\n",
       " 'jia',\n",
       " 'et',\n",
       " 'conduct',\n",
       " 'contrastive',\n",
       " 'learning',\n",
       " 'pair',\n",
       " 'exhibit',\n",
       " 'exceptional',\n",
       " 'accuracy',\n",
       " 'visual',\n",
       " 'recognition',\n",
       " 'painter',\n",
       " 'wang',\n",
       " 'et',\n",
       " 'introduces',\n",
       " 'vision',\n",
       " 'model',\n",
       " 'unifies',\n",
       " 'network',\n",
       " 'architecture',\n",
       " 'prompt',\n",
       " 'accomplish',\n",
       " 'diverse',\n",
       " 'vision',\n",
       " 'task',\n",
       " 'without',\n",
       " 'downstream',\n",
       " 'cafo',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'cascade',\n",
       " 'different',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'collaborates',\n",
       " 'knowledge',\n",
       " 'robust',\n",
       " 'image',\n",
       " 'classification',\n",
       " 'sam',\n",
       " 'kirillov',\n",
       " 'et',\n",
       " 'present',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'image',\n",
       " 'segmentation',\n",
       " 'billion',\n",
       " 'mask',\n",
       " 'conduct',\n",
       " 'segmentation',\n",
       " 'concurrent',\n",
       " 'work',\n",
       " 'extending',\n",
       " 'sam',\n",
       " 'segmentation',\n",
       " 'ke',\n",
       " 'et',\n",
       " 'faster',\n",
       " 'inference',\n",
       " 'speed',\n",
       " 'zhao',\n",
       " 'et',\n",
       " 'zhang',\n",
       " 'et',\n",
       " 'matching',\n",
       " 'liu',\n",
       " 'et',\n",
       " 'reconstruction',\n",
       " 'cen',\n",
       " 'et',\n",
       " 'object',\n",
       " 'tracking',\n",
       " 'yang',\n",
       " 'et',\n",
       " 'medical',\n",
       " 'wang',\n",
       " 'huang',\n",
       " 'et',\n",
       " 'image',\n",
       " 'processing',\n",
       " 'another',\n",
       " 'perspective',\n",
       " 'propose',\n",
       " 'personalize',\n",
       " 'segmentation',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'sam',\n",
       " 'specific',\n",
       " 'visual',\n",
       " 'concept',\n",
       " 'adapts',\n",
       " 'generalist',\n",
       " 'specialist',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'method',\n",
       " 'also',\n",
       " 'assist',\n",
       " 'personalization',\n",
       " 'image',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'stable',\n",
       " 'diffusion',\n",
       " 'rombach',\n",
       " 'et',\n",
       " 'imagen',\n",
       " 'saharia',\n",
       " 'et',\n",
       " 'improves',\n",
       " 'generation',\n",
       " 'quality',\n",
       " 'segmenting',\n",
       " 'foreground',\n",
       " 'target',\n",
       " 'object',\n",
       " 'background',\n",
       " 'disturbance',\n",
       " 'large',\n",
       " 'model',\n",
       " 'segmentation',\n",
       " 'fundamental',\n",
       " 'task',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'segmentation',\n",
       " 'long',\n",
       " 'et',\n",
       " 'jiang',\n",
       " 'et',\n",
       " 'zhao',\n",
       " 'et',\n",
       " 'xu',\n",
       " 'et',\n",
       " 'jiang',\n",
       " 'et',\n",
       " 'lin',\n",
       " 'et',\n",
       " 'requires',\n",
       " 'comprehension',\n",
       " 'image',\n",
       " 'various',\n",
       " 'task',\n",
       " 'explored',\n",
       " 'semantic',\n",
       " 'segmentation',\n",
       " 'classifying',\n",
       " 'pixel',\n",
       " 'predefined',\n",
       " 'set',\n",
       " 'class',\n",
       " 'narayanan',\n",
       " 'et',\n",
       " 'chen',\n",
       " 'et',\n",
       " 'zheng',\n",
       " 'et',\n",
       " 'cheng',\n",
       " 'et',\n",
       " 'xie',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2305.03048v2_lemmatized_tokens.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([\" \".join(lemmatized_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF words:\n",
      "et: 0.4294680024221286\n",
      "segmentation: 0.30778540173585883\n",
      "image: 0.23978630135235515\n",
      "object: 0.22904960129180194\n",
      "sam: 0.2039973011505111\n",
      "model: 0.18252390102940466\n",
      "persam: 0.17536610098903585\n",
      "prompt: 0.17178720096885144\n",
      "mask: 0.17178720096885144\n",
      "wang: 0.12168260068626978\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "sorted_items = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
    "print(\"Top TF-IDF words:\")\n",
    "for idx in sorted_items[:10]:\n",
    "    print(f\"{feature_names[idx]}: {tfidf_matrix[0, idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "checkpoint='t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model=T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023\n",
      "PERSONALIZE SEGMENT ANYTHING MODEL WITH\n",
      "ONE SHOT\n",
      "Renrui Zhang1,2, Zhengkai Jiang³*, Ziyu Guo²*, Shilin Yan², Junting Pan¹, Xianzheng Ma²\n",
      "Hao Dongª, Yu Qiao², Peng Gao², Hongsheng Li¹†\n",
      "¹CUHK MMLab 2 Shanghai Artificial Intelligence Laboratory\n",
      "3 Institute of Automation, Chinese Academy of Sciences\n",
      "4CFCS, School of CS, Peking University\n",
      "{zhangrenrui, gaopeng, guoziyu, qiaoyu} @pjlab.org.cn,\n",
      "kaikaijiang.jzk@gmail.com hsli@ee.cuhk.edu.hk\n",
      "ABSTRACT\n",
      "Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
      "demonstrated as a powerful promptable framework, revolutionizing the segmenta-\n",
      "tion field. Despite the generality, customizing SAM for specific visual concepts\n",
      "without man-powered prompting is under-explored, e.g., automatically segmenting\n",
      "your pet dog in numerous images. In this paper, we introduce a training-free\n",
      "Personalization approach for SAM, termed PerSAM. Given only one-shot data,\n",
      "i.e., a single image with a reference mask, we first obtain a positive-negative lo-\n",
      "cation prior for the target concept in new images. Then, aided by target visual\n",
      "semantics, we empower SAM for personalized object segmentation via two pro-\n",
      "posed techniques: target-guided attention and target-semantic prompting. In this\n",
      "way, we can effectively customize the general-purpose SAM for private use without\n",
      "any training. To further alleviate the ambiguity of segmentation scales, we present\n",
      "an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we\n",
      "introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only\n",
      "tunes 2 parameters within 10 seconds for improved performance. To demonstrate\n",
      "our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized\n",
      "object segmentation, and also test our methods on various one-shot image and\n",
      "video segmentation benchmarks. Besides, we propose to leverage PerSAM to\n",
      "improve DreamBooth for personalized text-to-image synthesis. By mitigating\n",
      "the disturbance of training-set backgrounds, our approach showcases better target\n",
      "appearance generation and higher fidelity to the input text prompt. Code is released\n",
      "at https://github.com/ZrrSkywalker/Personalize-SAM.\n",
      "(1) User provides\n",
      "(2) One-shot Learning\n",
      "(3)\n",
      "Personalized Segmentation\n",
      "One Image\n",
      "One Mask\n",
      "Training-free\n",
      "Fine-tuning\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "... in various poses or scenes\n",
      "Figure 1: Personalization of Segment Anything Model. We customize Segment Anything Model\n",
      "(SAM) (Kirillov et al., 2023) for specific visual concepts, e.g., your pet dog. With only one-shot data,\n",
      "we introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F.\n",
      "* Equal contribution. † Corresponding author.\n",
      "1\n",
      "2 Parameters\n",
      "PerSAM\n",
      "PerSAM-F\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "Fine-tune\n",
      "10 Seconds\n",
      "The hat on a\n",
      "teddy bear\n",
      "A photo of a [V] cat\n",
      "\"A [V] cat on a beach\"\n",
      "... in various poses or scenes\n",
      "Fine-tune\n",
      "The body of a\n",
      "robot toy\n",
      "***\n",
      "Figure 2: Personalized Segmentation Exam-\n",
      "ples. Our PerSAM (Left) can segment personal\n",
      "objects in any context with favorable perfor-\n",
      "mance, and PerSAM-F (right) further alleviates\n",
      "the ambiguity issue by scale-aware fine-tuning.\n",
      "1\n",
      "INTRODUCTION\n",
      "A photo of a [V] backpack\n",
      "\"A [V] backpack on a table of a classroom\"\n",
      "Figure 3: Improving DreamBooth (Ruiz et al.,\n",
      "2022) with PerSAM. By mitigating the distur-\n",
      "bance of backgrounds during training, our ap-\n",
      "proach can help to achieve higher-quality person-\n",
      "alized text-to-image generation.\n",
      "Foundations models in vision (Li et al., 2022; Zou et al., 2023; Wang et al., 2022), language (Brown\n",
      "et al., 2020; Touvron et al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021;\n",
      "Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability\n",
      "of large-scale datasets and computational resources. They demonstrate extraordinary generalization\n",
      "capacity in zero-shot scenarios, and display versatile interactivity incorporating human feedback.\n",
      "Inspired by this, Segment Anything (Kirillov et al., 2023) develops a delicate data engine for collecting\n",
      "11M image-mask data, and subsequently trains a segmentation foundation model, known as SAM. It\n",
      "defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and\n",
      "returning the expected mask, which allows for segmenting any objects in visual contexts.\n",
      "However, SAM inherently loses the capability to segment specific visual concepts. Imagine intending\n",
      "to crop your lovely pet dog in a thick photo album, or find the missing clock from a picture of your\n",
      "bedroom. Utilizing the vanilla SAM would be highly labor-intensive and time-consuming. For\n",
      "each image, you must precisely find the target object within complicated contexts, and then activate\n",
      "SAM with a proper prompt for segmentation. Considering this, we ask: Can we personalize SAM to\n",
      "automatically segment user-designated visual concepts in a simple and efficient manner?\n",
      "To this end, we introduce PerSAM, a training-free personalization approach for Segment Anything\n",
      "Model. As shown in Figure 1, our method efficiently customizes SAM using only one-shot data, i.e.,\n",
      "a user-provided reference image and a rough mask of the personal concept. Specifically, we first\n",
      "obtain a location confidence map for the target object in the test image by feature similarities, which\n",
      "considers the appearance of every foreground pixel. According to confidence scores, two points are\n",
      "selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed\n",
      "into SAM's decoder for segmentation. Within the decoder, we propose to inject visual semantics of\n",
      "the target object to unleash SAM's personalized segmentation power with two techniques:\n",
      "•\n",
      "• Target-guided Attention. We guide every token-to-image cross-attention layer in SAM's\n",
      "decoder by the location confidence map. This explicitly compels the prompt tokens to\n",
      "mainly concentrate on foreground target regions for intensive feature aggregation.\n",
      "Target-semantic Prompting. To explicitly provide SAM with high-level target semantics,\n",
      "we fuse the original prompt tokens with the embedding of the target object, which provides\n",
      "the low-level positional prompt with additional visual cues for personalized segmentation.\n",
      "With the aforementioned designs, along with a cascaded post-refinement, PerSAM exhibits favorable\n",
      "personalized segmentation performance for unique subjects in a variety of poses or scenes. Notably,\n",
      "our approach can cope well with scenarios that require segmenting one object among multiple similar\n",
      "ones, simultaneously segmenting several identical objects in the same image, or tracking different\n",
      "objects along a video. Nevertheless, as shown in Figure 2, there might be occasional failure cases,\n",
      "2\n",
      "where the object comprises visually distinct subparts or hierarchical structures to be segmented,\n",
      "e.g., the hat on top of a teddy bear, or the head of a robot toy. Such ambiguity casts a challenge for\n",
      "PerSAM in determining the appropriate scale of mask as output, since both the local part and the\n",
      "global shape can be regarded as valid masks by SAM.\n",
      "To alleviate this issue, we further propose a fine-tuning variant of our approach, PerSAM-F. We\n",
      "freeze the entire SAM to preserve its versatile pre-trained knowledge, and only fine-tune 2 parameters\n",
      "within 10 seconds on a single A100 GPU. In detail, we enable SAM to produce several potential\n",
      "segmentation results of different mask scales. To adaptively select the best scale for varying objects,\n",
      "we employ a learnable relative weight for each mask scale, and conduct a weighted summation as the\n",
      "final output. By such efficient scale-aware training, PerSAM-F avoids over-fitting on the one-shot\n",
      "data and exhibits better segmentation accuracy shown in Figure 2 (Right).\n",
      "Moreover, we observe that our approach can also assist DreamBooth (Ruiz et al., 2022) to better\n",
      "fine-tune diffusion models for personalized text-to-image generation, as shown in Figure 3. Given a\n",
      "few images containing a specific visual concept, e.g., your pet cat or backpack, DreamBooth learns\n",
      "to convert these images into an identifier [V] in the word embedding space, which, however, can\n",
      "simultaneously include the background information, e.g., stairs or the forest. This would override\n",
      "the newly prompted backgrounds, and disturb the target appearance generation. Therefore, we\n",
      "propose to leverage PerSAM to segment the target object within training images, and only supervise\n",
      "DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality.\n",
      "We summarize the contributions of our paper as follows:\n",
      "• Personalized Object Segmentation. We first investigate how to customize a general-\n",
      "purpose segmentation model (SAM) into personalized scenarios with minimal expense. To\n",
      "this end, we introduce two efficient and effective methods, along with a new segmentation\n",
      "dataset, PerSeg, for the evaluation of personalized object segmentation.\n",
      "• PerSAM and PerSAM-F. In PerSAM, we propose three training-free techniques to guide\n",
      "SAM by the high-level semantics of target objects. In PerSAM-F, we design a scale-aware\n",
      "fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue.\n",
      "• Our approach achieves competitive results on various tasks, including the PerSeg benchmark,\n",
      "one-shot part and semantic segmentation, and video object segmentation. In addition,\n",
      "PerSAM can enhance DreamBooth for better personalized text-to-image synthesis.\n",
      "2 RELATED WORK\n",
      "Foundation Models. With powerful generalization capacity, pre-trained foundation models can be\n",
      "adapted for various downstream scenarios and attain promising performance. In natural language\n",
      "processing, BERT (Devlin et al., 2018; Lu et al., 2019), GPT series (Brown et al., 2020; OpenAI,\n",
      "2023; Radford & Narasimhan, 2018; Radford et al., 2019), and LLAMA (Zhang et al., 2023c) have\n",
      "demonstrated remarkable in-context learning abilities, and can be transferred to new tasks by domain-\n",
      "specific prompts. Similarly, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which conduct\n",
      "contrastive learning on image-text pairs, exhibit exceptional accuracy in zero-shot visual recognition.\n",
      "Painter (Wang et al., 2022) introduces a vision model that unifies network architectures and in-context\n",
      "prompts to accomplish diverse vision tasks, without downstream fine-tuning. CaFo (Zhang et al.,\n",
      "2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust\n",
      "low-data image classification. SAM (Kirillov et al., 2023) presents a foundation model for image\n",
      "segmentation, which is pre-trained by 1 billion masks and conducts prompt-based segmentation.\n",
      "There are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023),\n",
      "faster inference speed (Zhao et al., 2023; Zhang et al., 2023a), all-purpose matching (Liu et al., 2023),\n",
      "3D reconstruction (Cen et al., 2023), object tracking (Yang et al., 2023), medical (Ma & Wang, 2023;\n",
      "Huang et al., 2023) image processing. From another perspective, we propose to personalize the\n",
      "segmentation foundation model, i.e., SAM, for specific visual concepts, which adapts a generalist\n",
      "into a specialist with only one shot. Our method can also assist the personalization of text-to-\n",
      "image foundation models, i.e., Stable Diffusion (Rombach et al., 2022) and Imagen (Saharia et al.,\n",
      "2022), which improves the generation quality by segmenting the foreground target objects from the\n",
      "background disturbance.\n",
      "3\n",
      "Large Models in Segmentation. As a fundamental task in computer vision, segmentation (Long\n",
      "et al., 2015; Jiang et al., 2022; Zhao et al., 2017; Xu et al., 2021; Jiang et al., 2023; Lin et al., 2022)\n",
      "requires a pixel-level comprehension of a image. Various segmentation-related tasks have been\n",
      "explored, such as semantic segmentation, classifying each pixel into a predefined set of classes (Badri-\n",
      "narayanan et al., 2017; Chen et al., 2017; Zheng et al., 2021; Cheng et al., 2022; Xie et al., 2021; Song\n",
      "et al., 2020); instance segmentation, focusing on the identification of individual object instances (He\n",
      "et al., 2017; Wang et al., 2020; Tian et al., 2020); panoptic segmentation, assigning both class\n",
      "labels and instance identification (Kirillov et al., 2019; Li et al., 2019); and interactive segmentation,\n",
      "involving human intervention for refinement (Hao et al., 2021; Chen et al., 2021). Recently, inspired\n",
      "by language foundation models (Zhang et al., 2023c; Brown et al., 2020), several concurrent works\n",
      "have proposed large-scale vision models for image segmentation. They are pre-trained by extensive\n",
      "mask data and exhibit strong generalization capabilities on numerous image distributions. Segment\n",
      "Anything Model (SAM) (Kirillov et al., 2023) utilizes a data engine with model-in-the-loop anno-\n",
      "tation to learn a promptable segmentation framework, which generalizes to downstream scenarios\n",
      "in a zero-shot manner. Painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a\n",
      "robust in-context learning paradigm and can segment any images by a given image-mask prompt.\n",
      "SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal\n",
      "references, e.g., language and audio, incorporating versatile semantic knowledge. In this study, we\n",
      "introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg\n",
      "for evaluation. Instead of developing large segmentation models, our goal is to personalize them to\n",
      "segment user-provided objects in any poses or scenes. We propose two approaches, PerSAM and\n",
      "PerSAM-F, which efficiently customize SAM for personalized segmentation.\n",
      "Parameter-efficient Fine-tuning. Directly tuning the entire foundation models on downstream\n",
      "tasks can be computationally expensive and memory-intensive, posing challenges for resource-\n",
      "constrained applications. To address this issue, recent works have focused on developing parameter-\n",
      "efficient methods (Sung et al., 2022; He et al., 2022; Rebuffi et al., 2017; Qin & Eisner, 2021) to\n",
      "freeze the weights of foundation models and append small-scale modules for fine-tuning. Prompt\n",
      "tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using\n",
      "learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving\n",
      "more competitive performance with scale and robust domain transfer compared to full model tuning.\n",
      "Low-Rank Adaption (LORA) (Hu et al., 2021; Cuenca & Paul, 2023; Zhang et al., 2023b; Hedegaard\n",
      "et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight,\n",
      "which significantly reduces the number of learnable parameters required for downstream tasks.\n",
      "Adapters (Houlsby et al., 2019; Pfeiffer et al., 2020; Lin et al., 2020; Chen et al., 2022) are designed\n",
      "to be inserted between layers of the original transformer, introducing lightweight MLPs for feature\n",
      "transformation. Different from existing works, we adopt a more efficient adaption method delicately\n",
      "designed for SAM, i.e., the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10\n",
      "seconds. This effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity\n",
      "of segmentation scale with superior performance.\n",
      "3 METHOD\n",
      "In Section 3.1, we first briefly revisit Segment Anything Model (SAM) (Kirillov et al., 2023), and\n",
      "introduce the task definition for personalized object segmentation. Then, we illustrate the methodology\n",
      "of our PerSAM and PerSAM-F in Section 3.2 and 3.3, respectively. Finally, we utilize our approach\n",
      "to assist DreamBooth (Ruiz et al., 2022) for better text-to-image generation in Section 3.4.\n",
      "3.1\n",
      "PERSONALIZED OBJECT SEGMENTATION\n",
      "A Revisit of Segment Anything. SAM consists of three components, a prompt encoder, an image\n",
      "encoder, and a lightweight mask decoder, respectively denoted as Encp, Enc, and Decм. As a\n",
      "promptable framework, SAM takes as input an image I, and a set of prompts P, which can be a point,\n",
      "a box, or a coarse mask. Specifically, SAM first utilizes Enc, to obtain the input image feature, and\n",
      "adopts Encp to encode the human-given prompts of a length k into prompt tokens as\n",
      "FI = Enc(I), Tp = Encp(P),\n",
      "(1)\n",
      "4\n",
      "F₁\n",
      "Encode\n",
      "Cosine Similarity\n",
      "{FT}=1\n",
      "Test Image I\n",
      "Target\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "FR\n",
      "MR° FR\n",
      "Encode\n",
      "PerSAM's Decoder\n",
      "Target-guided\n",
      "Attention\n",
      "Image-to-Token\n",
      "Cross-Attention\n",
      "Target-semantic\n",
      "Prompting\n",
      "↑\n",
      "{Si)=1\n",
      "Local Confidence Maps\n",
      "Modulate\n",
      "Token-to-Image\n",
      "Cross-Attention\n",
      "↑\n",
      "Aggregate\n",
      "Attention Matrix A\n",
      "Local\n",
      "Features\n",
      "{T}=1\n",
      "Token\n",
      "Self-Attention\n",
      "Overall\n",
      "Confidence Map S\n",
      "Aggregate\n",
      "↑\n",
      "a\n",
      "Concat(\n",
      "+ Repeat(\n",
      ")\n",
      "Overall\n",
      "Confidence Map S\n",
      "TM Тр\n",
      "× 2\n",
      "TR\n",
      "One-shot Image IR\n",
      "One-shot Mask MR\n",
      "Positive Prior\n",
      "Negative Prior\n",
      "Figure 4: Positive-negative Location Prior.\n",
      "We calculate a location confidence map for the\n",
      "target object in new test image by the appear-\n",
      "ance of all local parts. Then, we select the loca-\n",
      "tion prior as the point prompt for PerSAM.\n",
      "Low-level\n",
      "Positional Prompt\n",
      "High-level\n",
      "Semantic Prompt\n",
      "Figure 5: Target-guided Attention (Left) &\n",
      "Target-semantic Prompting (Right). To in-\n",
      "ject SAM with target semantics, we explicitly\n",
      "guide the cross-attention layers, and propose\n",
      "additional prompting with high-level cues.\n",
      "where F₁ = Rhxwxc and Tp Є Rkxc, with h, w denoting the resolution of the image feature map and\n",
      "c denoting the feature dimension. After that, the encoded image and prompts are fed into the decoder\n",
      "Decм for attention-based feature interaction. SAM constructs the input tokens of the decoder by\n",
      "concatenating several learnable mask tokens Tм as prefixes to the prompt tokens Tp. These mask\n",
      "tokens are responsible for generating the mask output, formulated as\n",
      "M =\n",
      "Decм (FI, Concat(TM,Tp)),\n",
      "where M denotes the final segmentation mask predicted by SAM.\n",
      "(2)\n",
      "Task Definition. Although SAM is generalized enough for any object by prompting, it lacks the\n",
      "ability to automatically segment specific subject instances. Considering this, we define a new task\n",
      "for personalized object segmentation. The user provides only a single reference image, and a mask\n",
      "indicating the target visual concept. The given mask can either be an accurate segmentation, or a\n",
      "rough sketch drawn on-the-fly. Our goal is to customize SAM to segment the designated object\n",
      "within new images or videos, without additional human prompting. For evaluation, we annotate a\n",
      "new dataset for personalized segmentation, named PerSeg. The raw images are collected from the\n",
      "works for subject-driven diffusion models (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2022),\n",
      "containing various categories of visual concepts in different poses or scenes. In this paper, we propose\n",
      "two efficient solutions for this task, which we specifically illustrate as follows.\n",
      "3.2\n",
      "TRAINING-FREE PERSAM\n",
      "Location Confidence Map. Conditioned on the user-provided image IR and mask MR, PerSAM\n",
      "first obtains a confidence map that indicates the location of the target object in the new test image\n",
      "I. As shown in Figure 4, we apply an image encoder to extract the visual features of both IR and I.\n",
      "The encoder can be SAM's frozen backbone or other pre-trained vision models, for which we adopt\n",
      "SAM's image encoder Enc, by default. We formulate the process as\n",
      "FI= Enc(I), FR = EncI(IR),\n",
      "(3)\n",
      "where F1, FR Є R¹xwxc. Then, we utilize the reference mask MR Є Rhxwx¹ to crop the features\n",
      "of foreground pixels within the visual concept from FR, resulting in a set of n local features as\n",
      "{T}}=1 = MR ○ Fr,\n",
      "(4)\n",
      "5\n",
      "where T½ Є R¹×ª and ○ denotes spatial-wise multiplication. After this, we calculate n confidence\n",
      "maps for each foreground pixel i by the cosine similarity between T½ and test image feature FĮ as\n",
      "{S² } }±₁ = {F₁TT) 11, where S₁ = Rhxw.\n",
      "(5)\n",
      "Note that FI and T½ have been pixel-wisely L2-normalized. Each S² represents the distribution\n",
      "probability for a different local part of object in the test image, such as the head, the body, or the\n",
      "paws of a dog. On top of this, we adopt an average pooling to aggregate all n local maps to obtain\n",
      "the overall confidence map of the target object as\n",
      "S =\n",
      "n\n",
      "n\n",
      "SiЄRhxw\n",
      "(6)\n",
      "By incorporating the confidences of every foreground pixel, S can take the visual appearance of\n",
      "different object parts into consideration, and acquire a relatively comprehensive location estimation.\n",
      "Positive-negative Location Prior. To provide PerSAM with a location prior on the test image,\n",
      "we select two points with the highest and lowest confidence values in S, denoted as Pɲ and Pɩ,\n",
      "respectively. The former represents the most likely center position of the target object, while the\n",
      "latter inversely indicates the background. Then, they are regarded as the positive and negative point\n",
      "prompts, and fed into the prompt encoder as\n",
      "Tp = Encp(Ph, P₁) Є R2×c,\n",
      "(7)\n",
      "which denote the prompt tokens for SAM's decoder. In this way, SAM would tend to segment the\n",
      "contiguous region surrounding the positive point, while discarding the negative one's on the image.\n",
      "Target-guided Attention. Although the positive-negative point prompt has been obtained, we\n",
      "further propose a more explicit semantic guidance to the cross-attention operation in SAM's decoder,\n",
      "which concentrates the feature aggregation within foreground target regions. As shown in Figure 5,\n",
      "the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual\n",
      "concept in the test image (hotter colors indicate higher scores). Based on such a property, we utilize S\n",
      "to guide the attention map in every token-to-image cross-attention layer of the decoder. Specifically,\n",
      "we denote every attention map after the softmax function as A = Rhxw, and then modulate its\n",
      "attention distribution by\n",
      "A9\n",
      "=\n",
      "softmax A+ a softmax(S)\n",
      "•\n",
      "(8)\n",
      "where a denotes a balancing factor. With the attention bias, the mask and prompt tokens are compelled\n",
      "to capture more visual semantics associated with the target subject, other than the unimportant\n",
      "background area. This contributes to more effective feature aggregation in attention mechanisms, and\n",
      "enhances the final segmentation accuracy of PerSAM in a training-free manner.\n",
      "Target-semantic Prompting. The vanilla SAM only receives prompts with low-level positional\n",
      "information, such as the coordinate of a point or a box. To provide SAM's decoder with more high-\n",
      "level cues, we propose to utilize the visual feature of the target concept as an additional high-level\n",
      "semantic prompting. We first obtain the global embedding TR of the object in the reference image by\n",
      "both I average pooling between different local features as\n",
      "n\n",
      "TR\n",
      "=\n",
      "ΣΤΑ €R1xc\n",
      "n\n",
      "i=1\n",
      "Then, we element-wisely add TR to all the input tokens of the test image in Equation 2, before feeding\n",
      "them into the decoder block, which is shown in Figure 5 as\n",
      "T9 = Repeat (TR) + Concat(TM,Tp),\n",
      "(10)\n",
      "where T9 denotes the input token guided by target semantics for the decoder Decм, and the Repeat\n",
      "operation duplicates the target visual embedding. Aided by the simple token incorporation, PerSAM\n",
      "is not only prompted by low-level location points, but also high-level target visual cues.\n",
      "6\n",
      "Test Image\n",
      "PerSAM\n",
      "Output\n",
      "Three\n",
      "M1\n",
      "M2\n",
      "+\n",
      "M3\n",
      "scales\n",
      "Random\n",
      "Noise\n",
      "→>\n",
      "DreamBooth\n",
      "→>\n",
      "↑\n",
      "\"a [V] cat\"\n",
      "Reconstruction\n",
      "Loss\n",
      "W1\n",
      "F\n",
      "W2\n",
      "1- W1\n",
      "+\n",
      "W₂ )\n",
      "1\n",
      "User provides\n",
      "Output Mask\n",
      "M\n",
      "Fine-tune\n",
      "Weighted\n",
      "Summation\n",
      "Freeze\n",
      "PerSAM\n",
      "Background\n",
      "Disturbance\n",
      "Decouple\n",
      "Figure 6: The Scale-aware Fine-tuning in\n",
      "PerSAM-F. To alleviate the scale ambiguity,\n",
      "PerSAM-F adopts two learnable weights for\n",
      "adaptively aggregating three-scale masks.\n",
      "Figure 7: PerSAM-assisted DreamBooth.\n",
      "We utilize PerSAM to decouple the target ob-\n",
      "jects from the background for improving the\n",
      "generation of DreamBooth.\n",
      "Cascaded Post-refinement. Via the above techniques, we obtain an initial segmentation mask\n",
      "on the test image from SAM's decoder, which however, might include rough edges and isolated\n",
      "background noises. For further refinement, we iteratively feed the mask back into the decoder Decм\n",
      "for a two-step post-processing. In the first step, we prompt the decoder by the currently predicted\n",
      "mask along with the previous positive-negative point prompt. For the second step, we acquire the\n",
      "bounding box enclosing the mask from the first step, and prompt the decoder additionally with this\n",
      "box for more accurate object localization. As we only iterate the lightweight decoder without the\n",
      "large-scale image encoder, the post-processing is efficient and only costs an extra 2% latency.\n",
      "3.3\n",
      "FINE-TUNING OF PERSAM-F\n",
      "Ambiguity of Segmentation Scales. The training-free PerSAM can tackle most cases with satisfac-\n",
      "tory segmentation accuracy. However, some target objects contain hierarchical structures, which leads\n",
      "to the ambiguity of mask scales. As shown in Figure 6, the teapot on top of a platform is comprised\n",
      "of two parts: a lid and a body. If the positive point prompt (denoted by a green pentagram) is located\n",
      "at the body, while the negative prompt (denoted by a red pentagram) does not exclude the platform\n",
      "in a similar color, PerSAM would be misled for segmentation. Such an issue is also discussed in\n",
      "SAM (Kirillov et al., 2023), where it proposes an alternative to simultaneously generate multiple\n",
      "masks of three scales, corresponding to the whole, part, and subpart of an object. Then, the user is\n",
      "required to manually select one mask out of three, which is effective but consumes extra manpower.\n",
      "In contrast, our personalized task aims to customize SAM for automatic object segmentation without\n",
      "the need for human prompting. This motivates us to further develop a scale-aware version of PerSAM\n",
      "by parameter-efficient fine-tuning.\n",
      "Scale-aware Fine-tuning. For adaptive segmentation with the appropriate scale, we introduce a\n",
      "fine-tuning variant, PerSAM-F. Unlike the training-free model only producing one mask, PerSAM-F\n",
      "first follows PerSAM to obtain the location prior, and refers to SAM's original solution to output\n",
      "three-scale masks, denoted as M1, M2, and M3, respectively. On top of this, we adopt two learnable\n",
      "mask weights, w₁, W2, and calculate the final mask output by a weighted summation as\n",
      ".\n",
      ".\n",
      "M = w₁ · M₁ + w2 ⋅ M2 + (1 − w₁ - w2) · M3,\n",
      "(11)\n",
      "where w1, W2 are both initialized as 1/3. To learn the optimal weights, we conduct one-shot fine-\n",
      "tuning on the reference image, and regard the given mask as the ground truth. Note that, we freeze\n",
      "the entire SAM model to preserve its pre-trained knowledge, and only fine-tune the 2 parameters of\n",
      "W1, W2 within 10 seconds on a single A100 GPU. In this way, our PerSAM-F efficiently learns the\n",
      "scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different\n",
      "concepts, improving the generalization capacity of PerSAM.\n",
      "7\n",
      "Table 1: Personalized Object Segmentation on the PerSeg Dataset. We compare the overall mIoU,\n",
      "blou, and learnable parameters for different methods (Bar et al., 2022; Wang et al., 2022; 2023; Zou\n",
      "et al., 2023), along with the mIoU for 10 objects in PerSeg. *** denotes works concurrent to ours.\n",
      "Method\n",
      "mIoU\n",
      "bloU Param. Can Barn Clock Cat\n",
      "Back- Teddy Duck Thin Red\n",
      "pack Bear Toy Bird Cartoon\n",
      "Robot\n",
      "Toy\n",
      "Painter\n",
      "VP\n",
      "SEEM*\n",
      "SegGPT*\n",
      "56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1\n",
      "65.9 25.5 383M 61.2\n",
      "58.6\n",
      "87.1 55.7 341M 65.4 82.5\n",
      "94.3 76.5 354M\n",
      "93.0\n",
      "33.3 20.9\n",
      "98.2\n",
      "65.0\n",
      "59.2\n",
      "76.6\n",
      "66.7\n",
      "79.8\n",
      "89.9\n",
      "67.4\n",
      "81.0\n",
      "72.4\n",
      "72.4\n",
      "91.1\n",
      "94.1\n",
      "95.2\n",
      "98.0\n",
      "71.3\n",
      "97.0\n",
      "95.8\n",
      "96.6 63.8\n",
      "92.6\n",
      "94.1\n",
      "94.4\n",
      "93.7\n",
      "97.2\n",
      "92.6\n",
      "97.3\n",
      "96.2\n",
      "0\n",
      "90.70 95.39\n",
      "2\n",
      "96.2 38.9 96.2\n",
      "96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0\n",
      "94.6\n",
      "97.3\n",
      "93.7\n",
      "97.0\n",
      "60.6\n",
      "97.1\n",
      "96.7\n",
      "PerSAM 89.3 71.7\n",
      "PerSAM-F 95.3 77.9\n",
      "Table 2: Video Object Segmen-\n",
      "tation on DAVIS 2017 val (Pont-\n",
      "Tuset et al., 2017). We utilize\n",
      "gray color to denote the methods\n",
      "involving in-domain training.\n",
      "Table 3: One-shot Semantic and Part Segmentation on FSS-\n",
      "1000 (Li et al., 2020), LVIS-92 (Gupta et al., 2019), PASCAL-\n",
      "Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al.,\n",
      "2023). We report the mIoU scores and utilize gray color to\n",
      "denote the methods involving in-domain training.\n",
      "One-shot Semantic Seg.\n",
      "FSS-1000 LVIS-92²\n",
      "One-shot Part Seg.\n",
      "PASCAL-Part\n",
      "Painter\n",
      "SEEM\n",
      "SegGPT\n",
      "PerSAM\n",
      "Method\n",
      "J&F І\n",
      "AGSS\n",
      "67.4 64.9 69.9\n",
      "AFB-URR 74.6 73.0 76.1\n",
      "34.6 28.5 40.8\n",
      "58.9 55.0 62.8\n",
      "75.6 72.5 78.6\n",
      "F\n",
      "Method\n",
      "PACO-Part\n",
      "HSNet\n",
      "86.5\n",
      "17.4\n",
      "32.4\n",
      "22.6\n",
      "VAT\n",
      "90.3\n",
      "18.5\n",
      "33.6\n",
      "23.5\n",
      "Painter\n",
      "61.7\n",
      "10.5\n",
      "30.4\n",
      "14.1\n",
      "SegGPT\n",
      "85.6\n",
      "18.6\n",
      "66.9 71.3 75.1\n",
      "PerSAM\n",
      "81.6\n",
      "15.6\n",
      "32.5\n",
      "22.5\n",
      "PerSAM-F 76.1 74.9 79.7\n",
      "PerSAM-F\n",
      "86.3\n",
      "18.4\n",
      "32.9\n",
      "22.7\n",
      "3.4\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "For personalized text-to-image synthesis, DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained\n",
      "diffusion model by the given 3~5 photos of a specific object, i.g., a pet cat. It learns to generate the\n",
      "cat referred to by a text prompt, “a [V] cat”, and calculates the loss over the entire reconstructed\n",
      "images. This This would inject the redundant background information in the training images into the\n",
      "identifier [V]. Therefore, as shown in Figure 7, we introduce our strategy to alleviate the disturbance\n",
      "of backgrounds in DreamBooth. Given an object mask for any of the few-shot images, we leverage our\n",
      "PerSAM to segment all the foreground targets, and discard the gradient back-propagation for pixels\n",
      "belonging to the background area. Then, the Stable Diffusion is only fine-tuned to memorize the\n",
      "visual I appearances of the target object. With no supervision imposed on the background, our PerSAM-\n",
      "assisted DreamBooth can not only synthesize the target object with better visual correspondence, but\n",
      "also increase the diversity of the new backgrounds guided by the input text prompt.\n",
      "4 EXPERIMENT\n",
      "We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1, along\n",
      "with various existing one-shot segmentation benchmarks in Section 4.2. Then, we illustrate the\n",
      "effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. Finally, we conduct several\n",
      "ablation studies to investigate our designs on PerSeg in Section 4.4.\n",
      "4.1 PERSONALIZED EVALUATION\n",
      "PerSeg Dataset. To test the personalization capacity, we construct a new segmentation dataset,\n",
      "termed PerSeg. The raw images are collected from the training data of subject-driven diffusion\n",
      "works (Ruiz et al., 2022; Gal et al., 2022; Kumari et al., 2022). PerSeg contains 40 objects of various\n",
      "categories in total, including daily necessities, animals, and buildings. In different poses or scenes,\n",
      "each object is associated with 5~7 images and masks, where we fix one image-mask pair as the\n",
      "user-provided one-shot data. The mIoU and bloU (Cheng et al., 2021) are adopted for evaluation.\n",
      "Please refer to the Appendix for implementation details and an enlarged data scale of PerSeg.\n",
      "8\n",
      "The ring\n",
      "on a clock\n",
      "2\n",
      "The teapot\n",
      "on a tray\n",
      "The backpack\n",
      "carried by a\n",
      "The top part\n",
      "of a can\n",
      "woman\n",
      "Figure 8: Visualization of PerSAM-F's Im-\n",
      "provement. Our scale-aware fine-tuning can\n",
      "well alleviate the scale ambiguity of PerSAM.\n",
      "Figure 9: Visualization of Video Object Seg-\n",
      "mentation. Our approach performs well for\n",
      "segmenting multiple objects in a video.\n",
      "Performance. In Table 1, we observe the fine-tuned PerSAM-F achieves the best results, which\n",
      "effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. We show more visualiza-\n",
      "tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP) (Bar et al., 2022), Painter (Wang\n",
      "et al., 2022), SEEM (Zou et al., 2023), and SegGPT (Wang et al., 2023) are in-context learners that\n",
      "can also segment objects according to the given one-shot prompt data. As shown, the training-free\n",
      "PerSAM can already achieve better performance than Painter, VP, and SEEM with different margins.\n",
      "By the efficient 2-parameter fine-tuning, our PerSAM-F further surpasses the powerful SegGPT by\n",
      "+2.4% and +4.1% overall mIoU and bIoU. Different from their motivations to develop segmentation\n",
      "generalists, our method is specially designed for personalized object segmentation, and exhibits much\n",
      "more efficiency in both time and computational resources.\n",
      "4.2 EXISTING SEGMENTATION BENCHMARKS\n",
      "Video Object Segmentation. Given the first-frame image and object masks, our PerSAM and\n",
      "PerSAM-F achieve competitive object segmentation and tracking performance on the validation\n",
      "set of DAVIS 2017 (Pont-Tuset et al., 2017) As shown in Table 2, compared to methods without\n",
      "video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score, and our\n",
      "PerSAM-F can achieve +0.5% better performance than SegGPT. Notably, our one-shot fine-tuning\n",
      "approach can outperform methods (Lin et al., 2019; Liang et al., 2020) fully trained by extensive\n",
      "video data. The results fully illustrate our strong generalization ability for temporal video data and\n",
      "complex scenarios, which contain multiple similar or occluded objects, as visualized in Figure 9.\n",
      "One-shot Semantic and Part Segmentation. In Table 3, we evaluate our approach for one-shot\n",
      "image segmentation respectively on four datasets, FSS-1000 (Li et al., 2020), LVIS-92² (Gupta et al.,\n",
      "2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan et al., 2023), where we\n",
      "follow Matcher (Liu et al., 2023) for data pre-processing and evaluation. As shown, our PerSAM-F\n",
      "attains consistently better results than Painter, and performs comparably to SegGPT. For models (Min\n",
      "et al., 2021; Hong et al., 2022) with in-domain training, our approach can achieve higher scores than\n",
      "HSNet. The experiments well demonstrate that, our proposed approach is not limited to object-level\n",
      "segmentation, but also works for category-wise and part-wise personalization of SAM.\n",
      "4.3\n",
      "PERSAM-ASSISTED DREAMBOOTH\n",
      "We follow all the hyperparameters in DreamBooth (Ruiz et al., 2022) to fine-tune a pre-trained Stable\n",
      "Diffusion (Rombach et al., 2022) for personalized image synthesis. In addition to Figure 3, we\n",
      "visualize more examples of PerSAM-assisted DreamBooth in Figure 10. For the dog lying on a grey\n",
      "sofa, the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations.\n",
      "Assisted by PerSAM-F, the newly-generated background is totally decoupled with the sofa and well\n",
      "corresponds to the textual prompt. For the barn in front of the mountains, our approach also alleviates\n",
      "the background disturbance to correctly generate the “forest” and “blue sky\".\n",
      "9\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "User provides\n",
      "DreamBooth\n",
      "Assisted by PerSAM\n",
      "A photo of a dog\n",
      "\"A [V] dog in a jungle\"\n",
      "...\n",
      "A photo of a barn\n",
      "\"A [V] barn with a forest in the background\"\n",
      "\"A [V] dog in snow\"\n",
      "\"A [V] barn with blue sky in the background\"\n",
      "Figure 10: Visualization of PerSAM-guided DreamBooth. The improved DreamBooth (Ruiz et al.,\n",
      "2022) can better preserve the diversity for synthesizing various contexts in new images.\n",
      "Table 4: Ablation of Main Com-\n",
      "ponents in our proposed method.\n",
      "Variant\n",
      "Table 5: Ablation of Different\n",
      "Fine-tuning Methods.\n",
      "Table 6: Ablation of using\n",
      "Box-image as Reference.\n",
      "mIoU Gain\n",
      "69.1\n",
      "Method\n",
      "PerSAM\n",
      "Param. mIoU\n",
      "Method\n",
      "Mask Box\n",
      "0\n",
      "89.32\n",
      "Painter\n",
      "56.4 42.0\n",
      "+ Post-refinement\n",
      "72.5 +3.4\n",
      "83.9 +11.4\n",
      "Prompt Tuning\n",
      "12K\n",
      "76.5\n",
      "VP\n",
      "65.9\n",
      "38.1\n",
      "Adapter\n",
      "196K\n",
      "78.3\n",
      "SEEM\n",
      "87.1\n",
      "64.9\n",
      "+ Guided Attention\n",
      "+ Semantic Prompt\n",
      "85.8 +1.9\n",
      "LORA\n",
      "293K\n",
      "90.0\n",
      "SegGPT\n",
      "94.3 36.0\n",
      "89.3\n",
      "+3.5\n",
      "3 Mask Weights\n",
      "3\n",
      "92.9\n",
      "+ Scale Tuning\n",
      "95.3\n",
      "+6.0\n",
      "PerSAM-F\n",
      "2\n",
      "95.3\n",
      "PerSAM 89.3\n",
      "PerSAM-F 95.3\n",
      "88.1\n",
      "94.9\n",
      "Positive Prior\n",
      "+ Negative Prior\n",
      "4.4\n",
      "ABLATION STUDY\n",
      "Main Components. In Table 4, we investigate our different components by starting from a baseline\n",
      "that only adopts the positive location prior. Then, we add the negative point prompt and cascaded\n",
      "post-refinement, enhancing +3.6% and +11.4% mIoU, respectively. On top of that, we introduce the\n",
      "high-level target semantics into SAM's decoder for attention guidance and semantic prompting. The\n",
      "resulting +1.9% and +3.5% improvements fully indicate their significance. Finally, via the efficient\n",
      "scale-aware fine-tuning, PerSAM-F boosts the score by +6.0%, demonstrating superior accuracy.\n",
      "Different Fine-tuning Methods. In Table 5, we experiment with other parameter-efficient fine-tuning\n",
      "(PEFT) methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019),\n",
      "and LORA (Hu et al., 2021). We freeze the entire SAM, and only tune the PEFT modules injected\n",
      "into every transformer block in PerSAM's decoder. As shown, the prompt tuning and Adapter would\n",
      "over-fit the one-shot data and severely degrade the accuracy. Instead, our scale-aware fine-tuning can\n",
      "best improve the performance of PerSAM, while tuning the least learnable parameters.\n",
      "Using Box-image as Reference. Requiring an accurate mask as one-shot data might be too strict for\n",
      "some users. In Table 6, we relax the input restrictions to a bounding box designating the expected\n",
      "object. For our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate\n",
      "the one-shot mask. Therefore, the box reference only leads to a marginal performance drop in\n",
      "PerSAM and PerSAM-F, but severely influences other methods.\n",
      "5 CONCLUSION\n",
      "In this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts\n",
      "with only one-shot data. Firstly, we introduce PerSAM, which injects high-level target semantics\n",
      "into SAM with training-free techniques. On top of this, we present a scale-aware fine-tuning variant,\n",
      "PerSAM-F. With only 2 learnable parameters, PerSAM-F effectively alleviates the ambiguity of mask\n",
      "scales and achieves leading performance on various benchmarks. Besides, we also verify the efficacy\n",
      "of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. We hope\n",
      "our work may expand the applicability of SAM to a wider: range of scenarios.\n",
      "10\n",
      "10\n",
      "REFERENCES\n",
      "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-\n",
      "decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 39(12):2481–2495, 2017.\n",
      "Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\n",
      "via image inpainting. Advances in Neural Information Processing Systems, 35:25005-25017, 2022.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
      "few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian.\n",
      "Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.\n",
      "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\n",
      "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully\n",
      "connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848,\n",
      "2017.\n",
      "Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "7345-7354, 2021.\n",
      "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\n",
      "transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n",
      "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, and Alexander Kirillov. Boundary\n",
      "iou: Improving object-centric image segmentation evaluation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 15334–15342, 2021.\n",
      "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n",
      "attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n",
      "Pedro Cuenca and Sayak Paul. Using lora for efficient stable diffusion fine-tuning. https://\n",
      "hugging face.co/blog/lora, January 2023.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\n",
      "Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\n",
      "inversion. arXiv preprint arXiv:2208.01618, 2022.\n",
      "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\n",
      "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 5356-5364, 2019.\n",
      "Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang,\n",
      "Zhiliang Yu, Zeyu Chen, et al. Edgeflow: Achieving practical interactive segmentation with\n",
      "edge-guided flow. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "1551-1560, 2021.\n",
      "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\n",
      "a unified view of parameter-efficient transfer learning. In International Conference on Learning\n",
      "Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok.\n",
      "Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\n",
      "IEEE international conference on computer vision, pp. 2961–2969, 2017.\n",
      "Lukas Hedegaard, Aman Alok, Juby Jose, and Alexandros Iosifidis. Structured pruning adapters.\n",
      "arXiv preprint arXiv:2211.10155, 2022.\n",
      "11\n",
      "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation\n",
      "with 4d convolutional swin transformer for few-shot segmentation. In European Conference on\n",
      "Computer Vision, pp. 108–126. Springer, 2022.\n",
      "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\n",
      "Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\n",
      "nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\n",
      "arXiv:2106.09685, 2021.\n",
      "Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu,\n",
      "Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images? arXiv preprint\n",
      "arXiv:2304.14660, 2023.\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\n",
      "noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR,\n",
      "2021.\n",
      "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\n",
      "Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pp. 709–727.\n",
      "Springer, 2022.\n",
      "Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, and Chengjie Wang. Pro-\n",
      "totypical contrast adaptation for domain adaptive semantic segmentation. In European Conference\n",
      "on Computer Vision, pp. 36–54. Springer, 2022.\n",
      "Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai,\n",
      "Chengjie Wang, and Liqing Zhang. Stc: spatio-temporal contrastive learning for video instance\n",
      "segmentation. In European Conference on Computer Vision Workshops, pp. 539–556. Springer,\n",
      "2023.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu.\n",
      "Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023.\n",
      "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "9404-9413, 2019.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\n",
      "Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643, 2023.\n",
      "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\n",
      "customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n",
      "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n",
      "tuning. arXiv preprint arXiv:2104.08691, 2021.\n",
      "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\n",
      "Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision\n",
      "and vision-language tasks. arXiv preprint arXiv:2211.09808, 2022.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n",
      "2023.\n",
      "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class\n",
      "dataset for few-shot segmentation. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 2869-2878, 2020.\n",
      "12\n",
      "Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang.\n",
      "Attention-guided unified network for panoptic segmentation. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 7026-7035, 2019.\n",
      "Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. Video object segmentation with adaptive feature\n",
      "bank and uncertain-region refinement. Advances in Neural Information Processing Systems, 33:\n",
      "3430-3441, 2020.\n",
      "Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Attention guided single-shot video object\n",
      "segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n",
      "3949-3957, 2019.\n",
      "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\n",
      "via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.\n",
      "Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai,\n",
      "Yu Qiao, and Hongsheng Li. Frozen clip models are efficient video learners. In European\n",
      "Conference on Computer Vision, pp. 388-404. Springer, 2022.\n",
      "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n",
      "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "arXiv preprint arXiv:2110.07602, 2021.\n",
      "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment\n",
      "anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310,\n",
      "2023.\n",
      "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n",
      "segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 3431-3440, 2015.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\n",
      "representations for vision-and-language tasks. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), pp. 13–23, 2019.\n",
      "Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\n",
      "Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In\n",
      "Proceedings of the IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021.\n",
      "Keval Morabia, Jatin Arora, and Tara Vijaykumar. Attention-based joint detection of object and\n",
      "semantic part. arXiv preprint arXiv:2007.02419, 2020.\n",
      "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n",
      "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapter-\n",
      "fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n",
      "2020.\n",
      "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and\n",
      "Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\n",
      "arXiv:1704.00675, 2017.\n",
      "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\n",
      "arXiv preprint arXiv:2104.06599, 2021.\n",
      "Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training.\n",
      "2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
      "models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n",
      "13\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In International conference on machine learning, pp.\n",
      "8748-8763. PMLR, 2021.\n",
      "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\n",
      "Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common\n",
      "objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "7141-7151, 2023.\n",
      "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n",
      "residual adapters. Advances in Neural information processing systems, 30, 2017.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n",
      "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n",
      "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\n",
      "preprint arXiv:2208.12242, 2022.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n",
      "Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n",
      "text-to-image diffusion models with deep language understanding. Advances in Neural Information\n",
      "Processing Systems, 35:36479-36494, 2022.\n",
      "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and\n",
      "Nanning Zheng. Rethinking learnable tree filter for generic feature transform. Advances in Neural\n",
      "Information Processing Systems, 33:3991-4002, 2020.\n",
      "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\n",
      "vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 5227–5237, 2022.\n",
      "Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In\n",
      "European Conference on Computer Vision, pp. 282–298. Springer, 2020.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
      "Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n",
      "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n",
      "models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast\n",
      "instance segmentation. Advances in Neural information processing systems, 33:17721–17732,\n",
      "2020.\n",
      "Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\n",
      "generalist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022.\n",
      "Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\n",
      "Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n",
      "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\n",
      "Simple and efficient design for semantic segmentation with transformers. Advances in Neural\n",
      "Information Processing Systems, 34:12077-12090, 2021.\n",
      "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, and Yu Qiao. Learning\n",
      "geometry-disentangled representation for complementary understanding of 3d object point cloud.\n",
      "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3056–3064, 2021.\n",
      "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:\n",
      "Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023.\n",
      "14\n",
      "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and\n",
      "Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications.\n",
      "arXiv preprint arXiv:2306.14289, 2023a.\n",
      "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\n",
      "and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint\n",
      "arXiv:2303.10512, 2023b.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\n",
      "and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "arXiv preprint arXiv:2303.16199, 2023c.\n",
      "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, and\n",
      "Peng Gao. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot\n",
      "learners. arXiv preprint arXiv:2303.02151, 2023d.\n",
      "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\n",
      "network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "2881-2890, 2017.\n",
      "Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.\n",
      "Fast segment anything. arXiv preprint arXiv:2306.12156, 2023.\n",
      "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\n",
      "Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\n",
      "sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition, pp. 6881–6890, 2021.\n",
      "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n",
      "language models. International Journal of Computer Vision, 130(9):2337-2348, 2022.\n",
      "Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\n",
      "everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n",
      "15\n",
      "115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(text, chunk_size=512)\n",
    "summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2305.03048v2 [cs.CV] 4 Oct 2023 PERSONALIZE SEGMENT ANYTHING MODEL WITHONE SHOT Renrui Zhang1,2, Zhengkai Jiang3*, Ziyu Guo2*, Shilin Yan2, Junting Pan1, Xianzheng Ma2 Hao Donga, Yu Qiao2, Peng Gao2, Hongs ining, Segment Anything Model (SAM) has revolutionized the segmenta-tion field. despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored. ining introduces a training-free Personalization approach for SAM. tive lo- cation prior for the target concept in new images. aided by target visual semantics, we empower SAM for personalized object segmentation via two pro- posed techniques. in this way, we can effectively customize the general-purpose SAM for private use without any training. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. ware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. code is released at https://github.com/ZrrSkywalker/Personalize-SAM. User provides (2) One-shot Learning (3) Personalized Segmentation One Image One Mask Training-free Fine-tuning PerSAM PerSAM-F. PerSAM-F User provides DreamBooth Assisted by PerSAM Fine-tune 10 Seconds The hat on a teddy bear A photo of a [V] cat \"A [V] cat on a beach\" e introduce two efficient solutions: a training-free PerSAM, and a fine-tuning PerSAM-F. PerSAM-F can help to achieve higher-quality person- alized text-to-image generation. ap- proach can help to achieve higher-quality person- alized text-to-image generation. al., 2023; Radford et al., 2019), and multi-modality (Radford et al., 2021; Jia et al., 2021; Li et al., 2023) have gained unprecedented prevalence, attributed to the availability of large-scale datasets and computational resources. Segment Anything (Kirillov et al., 2023) develops a bsequently trains a segmentation foundation model, known as SAM. it defines a novel promptable segmentation framework, i.e., taking as input a handcrafted prompt and returning the expected mask. however, SAM inherently loses the capability to segment specific visual concepts. imagine intending to crop your lovely pet dog in a thick photo album or find the missing clock from a picture of your bedroom. perSAM is a training-free personalization approach for Segment Anything Model. perSAM allows you to segment user-designated visual concepts in a simple and efficient manner. we first obtain a location confidence map for the target object in the test image by feature similarities. two points are selected as the positive-negative location prior, which are finally encoded as prompt tokens and fed into SAM's decoder for segmentation. we propose to inject visual semantics of the target object in the decoder for segmentation. object to unleash SAM's personalized segmentation power with two techniques: • Target-guided Attention. we guide every token-to-image cross-attention layer in SAM's decoder by the location confidence map. we fuse the original prompt tokens with the embedding of the target object. perSAM exhibits favorable personalized segmentation performance for unique subjects in a variety of poses or scenes. the approach can cope well with scenarios that require segmenting one object among multiple similar ones, simultaneously segmenting several identical objects in the same image, or tracking different objects along a video. ambiguity casts a challenge for PerSAM in determining the appropriate scale of mask as output. to alleviate this issue, we further propose a fine-tuning variant of our approach, e.g., a fine-tuning variant of our approach. we freeze the entire SAM to preserve its pre-trained knowledge. we only fine-tune 2 parameters within 10 seconds on a single A100 GPU. perSAM-F avoids over-fitting on the one-shot on the one-shot. our approach can also assist DreamBooth to better fine-tune diffusion models for personalized text-to-image generation. a few images containing a specific visual concept, e.g., your pet cat or backpack, can be converted into an identifier [V] in the word embedding space. however, the background informatia can simultaneously include the background informatia. we propose to leverage PerSAM to segment the target object within training images, and only supervise DreamBooth by the foreground area, enabling text-to-image synthesis with higher quality. we first investigate how to customize a general- purpose segmentation model into psa. in perSAM, we propose three training-free techniques to guide SAM by the high-level semantics of target objects. in perSAM-F, we design a scale-aware fine-tuning with 2 parameters in 10 seconds to well alleviate the mask ambiguity issue. ts on various tasks, including the PerSeg benchmark, one-shot part and semantic segmentation, and video object segmentation. perSAM can enhance DreamBooth for better personalized text-to-image synthesis. pre-trained foundation models can be adapted for various downstream scenarios and attain promising performance. penAI, 2023; Radford & Narasimhan, 2018; Radford et al., 2019); and LLAMA (Zhang et al., 2023c) have demonstrated remarkable in-context learning abilities. CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) exhibit exceptional accuracy in zero-shot visual recognition. CaFo (Zhang et al., 2023d) cascades different foundation models and collaborates their pre-trained knowledge for robust low-data image classification. there are some concurrent works extending SAM for high-quality segmentation (Ke et al., 2023), faster inference speed (Zhao et al., 2023d), faster inference speed (Zhao et al., we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. from another perspective, we propose to personalize the segmentation foundation model, i.e., SAM, for specific visual concepts, which adapt a generalist into a specialist with only one shot. imagen improves generation quality by segmenting foreground target objects. segmentation requires a pixel-level comprehension of a image. segmentation-related tasks include semantic segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation, cd segmentation and cd segmentation. instance segmentation, focusing on the identification of individual object instances. panoptic segmentation, assigning both class labels and instance identification. interactive segmentation, involving human intervention for re-evaluation. several concurrent works have proposed large-scale vision models for image segmentation. they are pre-trained by extensive mask data and exhibit strong generalization capabilities on numerous image distributions. Segment Anything Model (SAM) (Kirillov et al., 2023) uses a data engine with model-in-the-loop anno-tation to learn a promptable segmentation framework. painter (Wang et al., 2022) and SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt. SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile erSeg proposes two approaches, perSAM and perSAM-F. persAM and perSAM-F customize SAM for personalized segmentation. works have focused on developing parameter-efficient methods to freeze weights of foundation models and append small-scale modules for fine-tuning. prompt tuning (Lester et al., 2021; Zhou et al., 2022; Jia et al., 2022; Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks. low-Rank Adaption (LORA) injects trainable rank decomposition matrices concurrently to each pre-trained weight. Adapters are designed to be inserted between layers of the original transformer. we adopt a more efficient adaption method delicately designed for SAM. the scale-aware fine-tuning of PerSAM-F with only 2 parameters and 10 seconds. this effectively avoids the over-fitting issue on one-shot data, and alleviates the ambiguity of segmentation scale with superior performance. PerSAM and PerSAM-F consists of three components, a prompt encoder, an image encoder, and a lightweight mask decoder. as a promptable framework, SAM takes as input an image I, denoted as Encp, Enc, and Decм. Encp encodes human-given prompts of a length k into prompt tokens as FI = Enc(I), Tp = Encp(P), (1) 4 F1 Encode Cosine Similarity FT=1 Test Image I Target Local Features T=1 FR MR° FR Encode PerSAM's Decoder Target-guided Attention Image-to-Token Cross-At ss-Attention  Aggregate Attention Matrix A Local Features T=1 Token Self-Attention Overall Confidence Map S Aggregate  a Concat( + Repeat(). we calculate a location confidence map for the target object in new test image by the appear- ance of all local parts. then, we select the loca- tion prior as the point prompt for PerS -level Semantic Prompt Figure 5: Target-guided Attention (Left) & Target-semantic Prompting (Right). to in-ject SAM with target semantics, we explicitly guide the cross-attention layers. we propose additional prompting with high-level cues. mask tokens are responsible for generating mask output, formulated as M = Decм (FI, Concat(TM,Tp)) mask tokens are responsible for generating the mask output, formulated as M = Decм (FI, Concat(TM,Tp)), where M denotes the final segmentation mask predicted by SAM. the user provides only a single reference image, and a mask indicating the target visual concept. the given mask can either be an accurate segmentation, or a rough sketch drawn on-the-fly. for evaluation, we annotate a new dataset for personalized segmentation, named PerSeg. t al., 2022; Kumari et al., 2022; Kumari et al., 2022; Kumari et al., 2022); containing various categories of visual concepts in different poses or scenes. in this paper, we propose two efficient solutions for this task, which we specifically illustrate as follows. 3.2 TRAINING-FREE PERSAM Location Confidence Map. the encoder can be SAM's frozen backbone or other pre-trained vision models. we formulate the process as FI= Enc(I), FR = EncI(IR), (3) where F1, FR  R1xwxc. then, we use the reference mask MR  Rhxwx1 to crop the features of foreground pixels within the visual concept from FR. n confidence maps for each foreground pixel i by the cosine similarity between T12 and test image feature F as S2  1 = F1TT) 11. each S2 represents the distribution probability for a different local part of object in the test image, such as the head, the body, or the paws of a dog. PerSAM selects two points with the highest and lowest confidence values in S. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object. the latter represents the most likely center position of the target object, while the latter invenes the most likely center position of the target object. the positive and negative point prompts are fed into the prompt encoder as Tp = Encp(Ph, P1)  R2c, (7) which denotes the prompt tokens for SAM's decoder. in this way, SAM would tend to segment the contiguous region surrounding the positive point, while discarding the negative one's on the image. attention operation in SAM's decoder focuses feature aggregation within target regions. the overall confidence map S in Equation 6 can clearly indicate the rough region of the target visual concept in the test image. based on such a property, we use S to guide the attention map in every token-to-image cross-attention layer of the decoder. mask and prompt tokens are compelled to capture more visual semantics associated with the target subject. this contributes to more effective feature aggregation in attention mechanisms, and enhances the final segmentation accuracy of PerSAM in a training-free manner. we propose to use the visual feature of the target concept as an additional high-level semantic prompting. we first obtain the global embedding TR of the object in the reference image by both I average pooling between different local features as n TR =  €R1xc n i=1 Then, we element-wisely add TR to all the input tokens of the test image in Equation. perSAM is prompted by low-level location points, but also high-level target visual cues. a repeat operation replicates the target visual embedding in the decoder block. a repeat operation replicates the target visual embedding in the decoder block. PerSAM-F uses two learnable weights for adaptively aggregating three-scale masks. we use PerSAM to decouple the target ob-jects from the background for improving the generation of DreamBooth. segmentation mask on the test image from SAM's decoder may include rough edges and isolated background noise. in the first step, we prompt the decoder by the currently predicted mask along with the previous positive-negative point prompt. in the second step, we acquire the bounding box enclosing the mask from the first step. for the second step, we acquire the bounding box enclosing the mask from the first step 3.3 FINE-TUNING of PERSAM-F Ambiguity of Segmentation Scales. the training-free PerSAM can tackle most cases with satisfac- tory segmentation accuracy. some target objects contain hierarchical structures, which leads to the ambiguity of mask scales. m is comprised of two parts: a lid and a body. the negative prompt (denoted by a green pentagram) does not exclude the platform in a similar color. the negative prompt (denoted by a red pentagram) does not exclude the platform in a similar color, perSAM would be misled for segmentation. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. equired to manually select one mask out of three, which is effective but consumes extra manpower. this motivates us to further develop a scale-aware version of PerSAM by parameter-efficient fine-tuning perSAM-F first follows PerSAM to obtain the location prior. we use two learnable mask weights, w1, W2, and calculate the final mask output by a weighted summation as 1/3. to learn the optimal mask weights, we conduct one-shot fine- tuning on the reference image, and regard the given mask as the groud. we freeze the entire SAM model to preserve its pre-trained knowledge. we fine-tune the 2 parameters of W1, W2 within 10 seconds on a single A100 GPU. in this way, our PerSAM-F efficiently learns the scale-aware semantics of objects, and adaptively outputs the best segmentation scale for different concepts. we compare the overall mIoU, blou, and learnable parameters method mIoU bloU Param denotes works concurrent to ours. can Barn Clock Cat Back- Teddy Duck Thin Red pack Bear Toy Bird Cartoon Robot Toy Painter VP SEEM* SegGPT* 56.4 42.0 354M 19.1 3.2 42.9 94.1 88.1 65.9 25.5 383M 61.2 58.6 87.1 55.7 341M 65.4 82.5 94.3 76.5 63.8 92.6 94.1 94.4 93.7 97.2 97.2 92.6 97.3 96.2 0 90.70 95.39 2 96.2 38.9 96.2 96.7 96.7 97.5 96.1 92.3 95.5 95.2 97.3 94.0 94.6 97.3 93.7 97.0 60.6 97.1 96.7 PerSAM 89.3 71.7 PerSAM-F 95.3 77.9 Table 3: one-shot Semantic and Part one-shot Semantic Seg. FSS-1000 LVIS-922 One-shot Part Seg. PASCAL-Part Painter SEEM SegGPT PerSAM Method J&F  AGSS 67.4 64.9 69.9 AFB-URR 74.6 73.0 76.1 34.6 28.5 40.8 58.9 55.0 62.8 75.6 72.5 78.6 F Method PACO-Part HSNet DreamBooth (Ruiz et al., 2022) fine-tunes a pre-trained diffusion model by the given 35 photos of a specific object. it learns to generate the cat referred to by a text prompt, “a [V] cat” and calculates the loss over the entire reconstructed images. this would inject the redundant background information in the training images into the identifier [V] our PerSAM- assisted DreamBooth can synthesize the target object with beads. the target object can be systoked with beads. the target object can also be systoked with beads. the target object can also be systoked with beads. the target object can be systoked with beads. 4 EXPERIMENT We first evaluate our approach for personalized segmentation on PerSeg in Section 4.1. we then illustrate the effectiveness of our PerSAM-assisted DreamBooth in Section 4.3. perSeg contains 40 objects of various categories in total, including daily necessities, animals, and buildings. each object is associated with 57 images and masks, where we fix one image-mask pair as the user-provided one-shot dat. the raw images are collected from the training data of subject-driven diffusion works (Ruiz et al., 2022; Gal et al., 2022; Kumari e the mIoU and bloU (Cheng et al., 2021) are adopted for evaluation. enlarged data scale of PerSeg. 8 The ring on a clock 2 The teapot on a tray The backpack carried by a can woman Figure 8: Visualization of PerSAM-F's Im- provement. the fine-tuned PerSAM-F achieves the best results, which effectively enhances PerSAM by +2.7% and +5.9% overall mIoU and bIoU. we show more visualiza-tion of PerSAM-F's improvement in Figure 8. Visual Prompting (VP), Painter (Wang et al., 2022), SEEM (Zou et al., 2023 our perSAM-F surpasses the powerful SegGPT by +2.4% and +4.1% overall mIoU and bIoU. 4.2 EXISTING SEGMENTATION BENCHMARKS Video Object Segmentation. our PerSAM and PerSAM-F achieve competitive object segmentation and tracking performance on the validation set of DAVIS 2017 (Pont-Tuset et al., 2017) compared to methods without video training, the training-free PerSAM largely surpasses Painter by +32.3% J&F score. our one-shot fine-tuning approach can outperform methods (Lin et al., the results fully illustrate our strong generalization ability for temporal video data and complex scenarios. we evaluate our approach for one-shot image segmentation respectively on four datasets, including FSS-1000 (Li et al., 2020), LVIS-922 (Gupta et al., 2019), PASCAL-Part (Morabia et al., 2020), and PACO-Part (Ramanathan e our PerSAM-F achieves consistently better results than Painter. for models with in-domain training, our approach can achieve higher scores than HSNet. the experiments well demonstrate that our proposed approach is not limited to object-level segmentation, but also works for category-wise and part-wise personalization of SAM. PerSAM-assisted DreamBooth is able to fine-tune a pre-trained image synthesis. the \"jungle\" and \"snow\" by DreamBooth are still the sofa with green and white decorations. the newly-generated background is totally decoupled with the sofa and well corresponds to the textual prompt. 9 User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM User provides DreamBooth Assisted by PerSAM. the improved DreamBooth (Ruiz et al., 2022) can better pc and pc. mIoU Gain 69.1 Method PerSAM Param. mIoU Method Mask Box 0 89.32 Painter 56.4 42.0 + Post-refinement 72.5 +3.4 83.9 +11.4 Prompt Tuning 12K 76.5 VP 65.9 38.1 Adapter 196K 78.3 SEEM 87.1 64.9 + Guided Attention + Semantic Prompt 85.8 .0 89.3 +3.5 3 Mask Weights 3 92.9 + Scale Tuning 95.3 +6.0 PerSAM-F 95.3 88.1 94.9 Positive Prior + Negative Prior 4.4 ABLATION STUDY Main Components. in table 4, we introduce the high-level target semantics into SAM's decoder for attent semantics. PerSAM-F boosts the score by +6.0% via the efficient scale-aware fine-tuning. we experiment with other parameter-efficient fine-tuning methods for PerSAM-F, i.e., prompt tuning (Liu et al., 2021), Adapter (Houlsby et al., 2019), and LORA (Hu et al., 2021). the prompt tuning and Adapter would over-fit the one-shot data and severely degrade the accuracy. instead, our scale-aware fine-tuning can best improve the performance of PerSAM, while tuning the least learnable parameters. in table 6, we relax the input restrictions to a bounding box designating the expected object. our method, we can regard the box as a prompt and utilize off-the-shelf SAM to generate the one-shot mask. the box reference only leads to a marginal performance drop in PerSAM and PerSAM-F, but severely influences other methods. in this paper, we propose to personalize Segment Anything Model (SAM) for specific visual concepts with only one-shot data. PerSAM-F effectively alleviates the ambiguity of mask scales and achieves leading performance on various benchmarks. we also verify the efficacy of our approach to assist DreamBooth in fine-tuning better text-to-image diffusion models. we hope our work may expand the applicability of SAM to a wider: range of scenarios. onal encoder- decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481–2495, 2017. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. arXiv preprint arXiv:2304.12308, 2023. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022. the IEEE/CVF Conference on computer vision and pattern recognition, pp. 15334–15342, 2021. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. lora for efficient stable diffusion fine-tuning. arXiv preprint arXiv:1810.04805, 2018. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. an image is worth one word: Personalizing text-to-image generation using textual inversion. IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019.. a dataset for large vocabulary instance segmentation. arXiv preprint arXiv:2211.10155, 2022. arXiv preprint arXiv:2211.10155, 2022. cost aggregation with 4d convolutional swin transformer for few-shot segmentation. in european conference on computer vision, pp. 108–126. Springer, 2022. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly arXiv preprint arXiv:2106.09685, 2021. Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen. isual and vision-language representation learning with noisy text supervision. in international conference on machine learning, pp. 4904–4916. PMLR, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. in European Conference on Computer Vision, pp. 36–54. Springer, 2022. Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, and Liqing Zhang. arXiv preprint arXiv:2304.02643, 2023. nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan. arXiv preprint arXiv:2212.04488, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. the power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2301.12597, 2023. Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. ntion-guided unified network for panoptic segmentation. yongqing Liang, Xin Li, Navid Jafari, and Jim Chen. video object segmentation with adaptive feature bank and uncertain-region refinement. arXiv preprint arXiv:2004.03829, 2020. ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. Matcher: Segment anything with one shot using all-purpose feature matching. onference on computer vision and pattern recognition, pp. 3431-3440, 2015. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:2304.12306, 2023. Juhong Min, Dahyun Kang, and Minsu IEEE/CVF international conference on computer vision, pp. 6941–6952, 2021. arXiv preprint arXiv:2007.02419, 2020. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:1704.00675, 2017. Guanghui Qin and Jason Eisner. learning how to ask: Querying lms with mixture of soft prompts. odels are unsupervised multitask learners. openAI blog, 1(8):9, 2019. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark. an, et al. Paco: Parts and attributes of common objects. in Proceedings of the IEEE Conference on computer vision and pattern recognition, pp. 7141-7151, 2023. arXiv preprints arXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans. Advances in Neural Information Processing Systems, 35:36479-36494, 2022. Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, and Mohit Bansal. e on Computer Vision and Pattern Recognition, pp. 5227–5237, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Ed arXiv preprint arXiv:2302.13971, 2023. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 33:17721–17732, 2020. arXiv preprint arXiv:2304.03284, 2023. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. arXiv preprint arXiv:2304.11968, 2023. 14 Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2303.10512, 2023b. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. arXiv preprint arXiv:2303.02151, 2023d. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. pyramid scene parsing network, pp. 2881-2890, 2017. Xu Zhao, Wenchao Ding, Yongqi An, Tao Yu, Min Li yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–6890, 2021. arXiv preprint arXiv:2304.06718, 2023. 15 115 preprint arXiv:2304.06718, 2023.\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2305.03048v2_summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
