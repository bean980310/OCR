{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Street_1330_2000x.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from google.cloud import vision\n",
    "\n",
    "\n",
    "def analyze_image_from_uri(\n",
    "    image_uri: str,\n",
    "    feature_types: Sequence,\n",
    ") -> vision.AnnotateImageResponse:\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    image = vision.Image()\n",
    "    image.source.image_uri = image_uri\n",
    "    features = [vision.Feature(type_=feature_type) for feature_type in feature_types]\n",
    "    request = vision.AnnotateImageRequest(image=image, features=features)\n",
    "\n",
    "    response = client.annotate_image(request=request)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_labels(response: vision.AnnotateImageResponse):\n",
    "    print(\"=\" * 80)\n",
    "    for label in response.label_annotations:\n",
    "        print(\n",
    "            f\"{label.score:4.0%}\",\n",
    "            f\"{label.description:5}\",\n",
    "            sep=\" | \",\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"https://www.neoncreations.co.uk/cdn/shop/products/Street_1330_2000x.jpg\"\n",
    "features = [vision.Feature.Type.LABEL_DETECTION]\n",
    "\n",
    "response = analyze_image_from_uri(image_uri, features)\n",
    "print_labels(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(response: vision.AnnotateImageResponse):\n",
    "    print(\"=\" * 80)\n",
    "    for annotation in response.text_annotations:\n",
    "        vertices = [f\"({v.x},{v.y})\" for v in annotation.bounding_poly.vertices]\n",
    "        print(\n",
    "            f\"{repr(annotation.description):42}\",\n",
    "            \",\".join(vertices),\n",
    "            sep=\" | \",\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"https://static.easycanvasprints.com/Upload/mkt/PLA/ECP/BAS_SEM_20170824_MetalStretSigns_2Up_Green_Texthere.jpg\" \n",
    "features = [vision.Feature.Type.TEXT_DETECTION]\n",
    "\n",
    "response = analyze_image_from_uri(image_uri, features)\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str) -> dict:\n",
    "    \"\"\"Detects the text's language.\"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.detect_language(text)\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Confidence: {}\".format(result[\"confidence\"]))\n",
    "    print(\"Language: {}\".format(result[\"language\"]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[detect_language(response.text_annotations[i+1].description) for i in range(len(response.text_annotations)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(target: str, text: str) -> dict:\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    print(\"Text: {}\".format(result[\"input\"]))\n",
    "    print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=[translate_text(\"ko\", response.text_annotations[i+1].description) for i in range(len(response.text_annotations)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([response[i]['translatedText'] for i in range(len(response))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text=[response[i]['translatedText'] for i in range(len(response))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image_translated.txt', 'w') as f:\n",
    "    for i in range(len(translated_text)):\n",
    "        f.write(translated_text[i])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "CLASSES = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\",\n",
    "           \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "           \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\",\n",
    "           \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "           \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\",\n",
    "           \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "           \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
    "           \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "           \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\",\n",
    "           \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\",\n",
    "           \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\",\n",
    "           \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\",\n",
    "           \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\",\n",
    "           \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "\n",
    "# 모델 캐싱\n",
    "model_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolov5_model(model_size: str='yolov5x'):\n",
    "    \"\"\"\n",
    "    지정된 크기의 YOLOv5 모델을 로드하고 캐시합니다.\n",
    "    \n",
    "    Args:\n",
    "        model_size (str): 사용할 YOLOv5 모델 크기.\n",
    "    \n",
    "    Returns:\n",
    "        torch.hub.Model: 로드된 YOLOv5 모델.\n",
    "    \"\"\"\n",
    "    if model_size not in model_cache:\n",
    "        model_cache[model_size] = torch.hub.load('ultralytics/yolov5', model_size, pretrained=True)\n",
    "    return model_cache[model_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_image(image_path: str, scale: float=1.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    이미지를 업스케일링하여 해상도를 높입니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일의 로컬 경로.\n",
    "        scale (float): 업스케일링 비율.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 업스케일링된 이미지.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    upscaled_image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    return upscaled_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_contrast(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    이미지의 대비를 향상시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): 입력 이미지.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 대비가 향상된 이미지.\n",
    "    \"\"\"\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    이미지에서 노이즈를 제거합니다.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): 입력 이미지.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 노이즈가 제거된 이미지.\n",
    "    \"\"\"\n",
    "    denoised_image = cv2.medianBlur(image, 3)\n",
    "    return denoised_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_full(image_path: str, scale: float=1.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    전체 전처리 과정을 수행하여 이미지를 준비합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일의 로컬 경로.\n",
    "        scale (float): 업스케일링 비율.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 전처리된 이미지.\n",
    "    \"\"\"\n",
    "    image = upscale_image(image_path, scale)\n",
    "    image = enhance_contrast(image)\n",
    "    image = remove_noise(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_labels_yolov5_optimized(image_path: str, model_size: str='yolov5x', confidence_threshold: float=0.3, iou_threshold: float=0.4) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    최적화된 YOLOv5을 사용하여 이미지에서 라벨을 인식합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일의 로컬 경로.\n",
    "        model_size (str): 사용할 YOLOv5 모델 크기.\n",
    "        confidence_threshold (float): 예측 신뢰도 임계값.\n",
    "        iou_threshold (float): NMS의 IoU 임계값.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: (라벨, 신뢰도) 튜플의 리스트.\n",
    "    \"\"\"\n",
    "    model = get_yolov5_model(model_size)\n",
    "    results = model(image_path)\n",
    "    labels = []\n",
    "    \n",
    "    for *box, conf, cls in results.xyxy[0]:\n",
    "        label = model.names[int(cls)]\n",
    "        labels.append((label, conf.item()))\n",
    "    \n",
    "    # 중복 라벨 제거 및 신뢰도 평균\n",
    "    label_dict = {}\n",
    "    for label, conf in labels:\n",
    "        if label in label_dict:\n",
    "            label_dict[label].append(conf)\n",
    "        else:\n",
    "            label_dict[label] = [conf]\n",
    "    \n",
    "    final_labels = []\n",
    "    for label, confs in label_dict.items():\n",
    "        avg_conf = sum(confs) / len(confs)\n",
    "        final_labels.append((label, avg_conf))\n",
    "    \n",
    "    # 신뢰도 높은 순으로 정렬\n",
    "    final_labels = sorted(final_labels, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_specific_classes(labels: List[Tuple[str, float]], specific_classes: List[str]) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    특정 클래스만을 필터링하여 반환합니다.\n",
    "    \n",
    "    Args:\n",
    "        labels (List[Tuple[str, float]]): (라벨, 신뢰도) 튜플 리스트.\n",
    "        specific_classes (List[str]): 필터링할 클래스 이름 리스트.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: 필터링된 (라벨, 신뢰도) 튜플 리스트.\n",
    "    \"\"\"\n",
    "    return [label for label in labels if label[0] in specific_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_easyocr(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    EasyOCR을 사용하여 이미지에서 텍스트를 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일의 로컬 경로.\n",
    "    \n",
    "    Returns:\n",
    "        str: 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    import easyocr\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(image_path, detail=0, paragraph=True)\n",
    "    return '\\n'.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels(labels: List[Tuple[str, float]], max_labels: int=10):\n",
    "    \"\"\"\n",
    "    예측된 라벨을 최대 max_labels개까지 출력합니다.\n",
    "    \n",
    "    Args:\n",
    "        labels (List[Tuple[str, float]]): (라벨, 신뢰도) 튜플의 리스트.\n",
    "        max_labels (int): 출력할 최대 라벨 수.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    # 라벨 리스트가 max_labels보다 작을 경우를 대비하여 슬라이싱\n",
    "    for label, confidence in labels[:max_labels]:\n",
    "        print(f\"{confidence*100:5.1f}% | {label:20}\")\n",
    "    print(\"=\" * 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(text: str):\n",
    "    \"\"\"\n",
    "    추출된 텍스트를 출력합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(text)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_labels(image_path: str, labels: List[Tuple[str, float]], confidence_threshold: float=0.3):\n",
    "    \"\"\"\n",
    "    이미지에 탐지된 라벨과 바운딩 박스를 시각화합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일의 로컬 경로.\n",
    "        labels (List[Tuple[str, float]]): (라벨, 신뢰도) 튜플 리스트.\n",
    "        confidence_threshold (float): 시각화할 신뢰도 임계값.\n",
    "    \"\"\"\n",
    "    model = get_yolov5_model('yolov5x')\n",
    "    results = model(image_path, conf=confidence_threshold, iou=0.4)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    for *box, conf, cls in results.xyxy[0]:\n",
    "        if conf < confidence_threshold:\n",
    "            continue\n",
    "        label = model.names[int(cls)]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path=\"/Users/janghyeonbin/OCR/notebook/Street_1330_2000x.jpg\"\n",
    "\n",
    "preprocessed_image = preprocess_image_full(image_path, scale=1.5)\n",
    "temp_preprocessed_path = \"temp_preprocessed_image.jpg\"\n",
    "cv2.imwrite(temp_preprocessed_path, cv2.cvtColor(np.array(preprocessed_image), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "0sc\n",
      "U\n",
      "IL EE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "extracted_text_easyocr = analyze_image_easyocr(temp_preprocessed_path)\n",
    "print_text(extracted_text_easyocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STREET\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "extracted_text_easyocr = analyze_image_easyocr(image_path)\n",
    "print_text(extracted_text_easyocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janghyeonbin/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    }
   ],
   "source": [
    "labels_yolov5 = analyze_image_labels_yolov5_optimized(\n",
    "        image_path, model_size='yolov5x', confidence_threshold=0.3, iou_threshold=0.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인식된 라벨의 수: 0\n",
      "인식된 라벨: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"인식된 라벨의 수: {len(labels_yolov5)}\")\n",
    "print(f\"인식된 라벨: {labels_yolov5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_classes = ['person', 'car', 'bicycle']  # 관심 있는 클래스 목록\n",
    "labels_filtered = filter_specific_classes(labels_yolov5, specific_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_labels(labels_filtered, max_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=\"/Users/janghyeonbin/OCR/notebook/BAS_SEM_20170824_MetalStretSigns_2Up_Green_Texthere.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AVE YOUR TEXT HERE BLVD\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "extracted_text_easyocr = analyze_image_easyocr(image_path)\n",
    "print_text(extracted_text_easyocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_multilingual(source: str, target: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Translates text from source to target language using HuggingFace M2M100 model.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source language code (ISO 639-1, e.g., 'en' for English).\n",
    "        target (str): Target language code (ISO 639-1, e.g., 'ko' for Korean).\n",
    "        text (str): Text to translate.\n",
    "\n",
    "    Returns:\n",
    "        str: Translated text.\n",
    "    \"\"\"\n",
    "    from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "    model_name = \"facebook/m2m100_418M\"\n",
    "\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # 소스 언어 설정\n",
    "    tokenizer.src_lang = source\n",
    "\n",
    "    # 입력 텍스트 토크나이징\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # 번역 수행\n",
    "    generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(target))\n",
    "\n",
    "    # 번역된 토큰 디코딩\n",
    "    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Translation: {translated_text}\")\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc06072525549798e4b79240fc72bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472a135146e443a491a3d10c4d680b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7797a53629949dab7c498bfc81d295f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1661c39270499a879fe0544e787f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2468927ab8434da2761699bc7df31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108398eeedda4d9ba2d265af1e076b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f1ad07ff734dab97246b6734be4c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: AVE YOUR TEXT HERE BLVD\n",
      "Translation: 당신의 글은 여기 BLVD\n"
     ]
    }
   ],
   "source": [
    "translated_text=translate_text_multilingual(\"en\", \"ko\", extracted_text_easyocr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
