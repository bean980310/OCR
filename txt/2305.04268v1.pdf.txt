--- Page 1 ---
arXiv:2305.04268v1 [cs.CV] 7 May 2023

Multi-Space Neural Radiance Fields

Ze-Xin Yin Jiaxiong Qiu

Ming-Ming Cheng Bo Ren*

VCIP, CS, Nankai University

{Zexin.Yin.cn, qiujiaxiong727}@gmail.com, {cmm, rb}@nankai.edu.cn

Abstract

Existing Neural Radiance Fields (NeRF) methods suf-
fer from the existence of reflective objects, often result-
ing in blurry or distorted rendering. Instead of calculat-
ing a single radiance field, we propose a multi-space neu-
ral radiance field (MS-NeRF) that represents the scene us-
ing a group of feature fields in parallel sub-spaces, which
leads to a better understanding of the neural network to-
ward the existence of reflective and refractive objects. Our
multi-space scheme works as an enhancement to existing
NeRF methods, with only small computational overheads
needed for training and inferring the extra-space outputs.
We demonstrate the superiority and compatibility of our ap-
proach using three representative NeRF-based models, i.e.,
NeRF, Mip-NeRF, and Mip-NeRF 360. Comparisons are
performed on a novelly constructed dataset consisting of 25
synthetic scenes and 7 real captured scenes with complex
reflection and refraction, all having 360-degree viewpoints.
Extensive experiments show that our approach significantly
outperforms the existing single-space NeRF methods for
rendering high-quality scenes concerned with complex light
paths through mirror-like objects. Our code and dataset
will be publicly available at https://zx-yin.github.io/msnerf.

1. Introduction

Neural Radiance Fields (NeRF) [25] and its variants are
refreshing the community of neural rendering, and the po-
tential for more promising applications is still under ex-
ploration. NeRF represents scenes as continuous radiance
fields stored by simple Multi-layer Perceptrons (MLPs) and
renders novel views by integrating the densities and radi-
ance, which are queried from the MLPs by points sampled
along the ray from the camera to the image plane. Since its
first presentation [25], many efforts have been investigated
to enhance the method, such as extending to unbounded
scenes [2, 50], handling moving objects [29, 30, 37], or re-
constructing from pictures in the wild [6, 21, 35,49].

“Bo Ren is the corresponding author.

(b) Our Model, SSIM=0.881

Figure 1. (a) Though Mip-NeRF 360 can handle unbounded
scenes, it still suffers from reflective surfaces, as the virtual images
violate the multi-view consistency, which is of vital importance
to NeRF-based methods. (b) Our method can help conventional
NeRF-like methods learn the virtual images with little extra cost.

(a) Mip-NeRF 360 [2], SSIM=0.825

However, rendering scenes with mirrors is still a chal-
lenging task for state-of-the-art NeRF-like methods. One of
the principle assumptions for the NeRF method is the multi-
view consistency property of the target scenes [16, 20, 36].
When there are mirrors in the space, if one allows the view-
points to move 360-degree around the scene, there is no
consistency between the front and back views of a mirror,
since the mirror surface and its reflected virtual image are
only visible from a small range of views. As a result, it is
often required to manually label the reflective surfaces in
order to avoid falling into sub-optimal convergences [12].

In this paper, we propose a novel multi-space NeRF-
based method to allow the automatic handling of mirror-like
objects in the 360-degree high-fidelity rendering of scenes
without any manual labeling. Instead of regarding the Eu-
clidean scene space as one single space, we treat it as com-
posed of multiple virtual sub-spaces, whose composition
changes according to location and view direction. We show
that our approach using such a multi-space decomposition
leads to successful handlements of complex reflections and
refractions where the multi-view consistency is heavily vi-
olated in the Euclidean real space. Furthermore, we show
that the above benefits can be achieved by designing a low-
cost multi-space module and replacing the original output
layer with it. Therefore, our multi-space approach serves as

--- Page 2 ---
a general enhancement to the NeRF-based backbone, equip-
ping most NeRF-like methods with the ability to model
complex reflection and refraction, as shown in Fig. 1.

Existing datasets have not paid enough attention to the
360-degree rendering of scenes containing mirror-like ob-
jects, such as RFFR [12] just has forward-facing scenes,
and the Shiny dataset in [42] with small viewpoints changes
and cannot exhibit view-dependent effects in large angle
scale. Therefore we construct a novel dataset dedicated
to evaluation for the 360-degree high-fidelity rendering of
scenes containing complex reflections and refractions. In
this dataset, we collect 25 synthesized scenes and 7 captured
real-world scenes. Each synthesized scene consists of 120
images of 360-degree around reflective or refractive objects,
with 100 randomly split for training, 10 for validation, and
10 for evaluation. Each real-world scene is captured ran-
domly around scenes with reflective and refractive objects,
consisting of 62 to 118 images, and organized under the
convention of LLFF [24]. We then demonstrate the supe-
riority and compatibility of our approach by comparisons,
using three representative baseline models, i.e., NeRF [25],
Mip-NeRF [1], and Mip-NeRF 360 [2], with and without
our multi-space module. Experiments show that our ap-
proach improves performance by a large margin both quan-
titatively and qualitatively on scenes with reflection and re-
fraction. Our main contributions are as follows:

¢ We propose a multi-space NeRF method that auto-
matically handles mirror-like objects in 360-degree
high-fidelity scene rendering, achieving significant im-
provements over existing representative baselines both
quantitatively and qualitatively.

¢ We design a lightweight module that can equip most
NeRF-like methods with the ability to model reflection
and refraction with small computational overheads.

* We construct a dataset dedicated to evaluation for the
360-degree high-fidelity rendering of scenes contain-
ing complex reflections and refractions, including 25
synthesized scenes and 7 real captured scenes.

2. Related works

Coordinate-based novel view synthesis. NeRF [25] has
bridged the gap between computer vision and computer
graphics, and reveals a promising way to render high-
quality photorealistic scenes with only posed images. The
insights and the generalization ability of this scheme also
facilitate various tasks both in CV and CG, i.e., 3D recon-
struction [28,40], 3D-aware generation [4, 15,27], 3D-aware
edition [39,47], and avatar reconstruction and manipula-
tion [9, 18,52]. Besides, researchers have made great ef-
forts to enhance this scheme. Mip-NeRF [1] enhances the

anti-aliasing ability of NeRF by featuring 3D conical frus-
tum using integrated positional encoding. [14,23] adapt this
scheme to HDR images. [2,50] extend NeRF and its variants
to unbounded scenes. There are also many works trying to
speed up the training and inference speed using explicit or
hybrid representations [5, 7, 10,26, 32,34, 46].

Glossy materials with high specular have a great
influence on NeRF-like methods, [38] is inspired by
precomputation-based techniques [31] in computer graph-
ics to represent and render view-dependent specular and re-
flection, but it fails to handle mirror-like reflective surfaces
because the virtual images cannot be treated as textures.
Guo et al. [12] propose to decompose reflective surfaces
into a transmitted part and reflected part, which is the most
relevant work to ours, but such decomposition cannot han-
dle 360-degree views with mirror-like objects because the
virtual images have no difference from real objects until the
viewpoint moves beyond a certain angle.

Another line of work similar to ours is multiple neural
radiance fields, but they do so for different purposes [1 1,27,
32, 43,44]. [27] uses object-level neural radiance fields for
3D-aware generation and composition. [32,44] uses multi-
ple small MLPs for efficient rendering. [11,43] uses multi-
ple object-level neural radiance fields for 3D scene decom-
position and edition.

Commonly used datasets. Researchers have introduced or
constructed many different datasets to facilitate the develop-
ment of NeRF-based methods in various tasks. Mildenhall
et al. [25] collect a dataset containing eight rendered sets of
posed images about eight objects separately, and eight real
captured forward-facing scenes with the camera poses and
intrinsics estimated by COLMAP [33]. Nevertheless, these
scenes lack reflection and refraction, which are very com-
mon. Wizadwongsa et al. [42] propose a dataset, namely
Shiny, that contains eight more challenging scenes to test
NeRF-like methods on view-dependent effects, but they are
captured in a roughly forward-facing manner. Verbin et
al. [38] create a dataset of six glossy objects, namely Shiny
Blender, which are rendered under similar conditions as
done in NeRF to test methods in modeling more complex
materials. For unbounded scenes, Barron et al. [2] construct
a dataset consisting of 5 outdoor scenes and 4 indoor scenes,
while Zhang et al. [50] adopt Tanks and Temples (T&T)
dataset [19] and the Light Field dataset [48]. Bemanal
et al. [3] capture a dataset consisting of refractive objects,
which is composed of four scenes with cameras moving in
a large range. Guo et al. [12] collect six forward-facing
scenes with reflective and semi-transparent materials, which
is, to date, the most relevant dataset to ours, but ours is
much more challenging. DTU dataset [17] and Blended-
MVS dataset [45] are commonly used as benchmarks for
the evaluation of 3D reconstruction.

--- Page 3 ---
Figure 2. The virtual image created by the mirror is visible only in
a small range of views, which violates the multi-view consistency.

3. Method
3.1. Preliminaries: Neural Radiance Fields

Neural Radiance Fields (NeRF) [25] encodes a scene in
the form of continuous volumetric fields into the weights o:
a multilayer perceptron (MLP), and adopts the absorption-
only model in the traditional volumetric rendering to syn-
thesize novel views. The training process only requires a
sparse set of posed images and casts rays r(t) = o + td
through the scene, where o € R? is the camera center an
d € R? is the view direction, and the rays can be calculate
by intrinsics and poses from the training data. Given these
rays, NeRF samples a set of 3D points {p; = 0 + t;d}
by the distance to the camera ¢; in the Euclidean space an
projects these points to a higher dimensional space using
the following function:

),cos(2*~*p)] (1)

where L is a hyperparameter and p is a sampled point.

Given the projected features {7(p;)} and the ray direc-
tion d, the MLP outputs the densities {o;} and colors {c;},
which are used to estimate the color C(r) of the ray using
the quadrature rule reviewed by Max [22]:

N
C(r) = Ss T;(1 — exp(—o;6;))c; (2)
i=l

with T, = exp(— 52} 0)4)) and 6; = t; — tj. Since
the equation is differentiable, the model parameters can be
optimized directly by Mean Squared Error (MSE) loss:

1 A
L= ig) |C@) — C@)||2 (3)

rEeR

where F is a training batch of rays. Besides, NeRF also
adopts a hierarchical sampling strategy to sample more
points where higher weights are accumulated. With these
designs, NeRF achieves state-of-the-art photorealistic re-
sults of novel view synthesis in most cases.

(a) A training view in toy scene A.

oes

(c) A render view in toy scene A.

(b) A training view in toy scene B.

(d) A render view in toy scene B.
Figure 3. The first row is training view examples in the two scenes.
In scene A there is only a plant in front of a mirror, while in scene
B we carefully place another plant to match the exact position
where the virtual image lies. The second row is test views with
rendered depth from the vanilla NeRF trained on the toy scenes.
As demonstrated, NeRF can avoid the trap of treating reflected
images as textures when the ’virtual image’ satisfies multi-view
consistency.

3.2. Multi-space Neural Radiance Field

The volumetric rendering equation and the continuous
representation ability of MLPs do guarantee the success
of NeRF-based methods in novel view synthesis, but as
pointed out by previous works [12, 16, 20], there is also
an unignorable property hidden in the training process that
helps the convergence, which is the multi-view consistency.
However, the multi-view consistency can be easily violated
by any reflective surfaces. An example is shown in Fig. 2,
when looking in front of a mirror one can observe the re-
flective virtual image as if there were an object behind it,
but when looking from a side or backward, there is actu-
ally nothing behind the mirror. In practice, this means there
will be completely conflictive training batches violating the
fitting process of MLP.

To experimentally demonstrate the importance of multi-
view consistency and its influence on the conventional
NeRF network structure, We create two 360-degree toy
scenes using an open source software Blender [8], each of
which consists of 100 training images and 10 test images,
training view examples are shown in Fig. 3a and Fig. 3b.
The only difference between the two scenes is that we place
a mirror-posed real object behind the mirror in the latter
scene, but not in the former one. We train the vanilla NeRF
separately on these toy scenes under the same setting and
render some views from the test set as in Fig. 3c and Fig. 3d,
which clearly shows that the vanished virtual image (i.e., vi-
olation to the multi-view consistency) in some views leads

--- Page 4 ---
® Weighted sum

| Output layer
(90) Decoder MLP
(9B Gate MLP

BBE reste map

Output

Figure 4. Our multi-space module only modifies the output and volumetric rendering part of the network. The original NeRF calculates a
pair of density o and radiance ¢ to get the accumulated color. Our output layer produces pairs of densities {0} and features {f*}, which
correspond to multiple parallel feature fields. Then, we use volumetric rendering to get multiple feature maps. Two simple MLPs, i.e.,
Decoder MLP and Gate MLP, are utilized to decode RGB maps and pixel-wise weights from these feature maps.

the model to suboptimal results in reflection-related regions
and produces blur in rendering. Interestingly, the conven-
tional NeRF is still trying to fulfill the multi-view consis-
tency assumption in the process. From the depth map in
Fig. 3c, we can easily conclude that the conventional NeRF
treats the viewed virtual image as a “texture” on the reflec-
tive surface, achieving a compromise between its principle
assumption and the conflicts in training data, although the
compromise leads to false understandings and worse ren-
dering results of the real scenes.

Contrary to the conventional NeRF, inspired by the com-
mon perspective in Physics and Computer Graphics that re-
flective light can be viewed as “directly emitted” from its
mirror-symmetric direction, from a possible “virtual source
inside the virtual space in the mirror,’ we build our novel
multi-space NeRF approach on the following assumption:

Assumption 1 At the existence of reflection and refraction,
the real Euclidean scene space can be decomposed into
multiple virtual sub-spaces. Each sub-space satisfies the
multi-view consistency property.

It follows that the composition weights of the sub-spaces
can change according to the spatial location and the view di-
rection. Thus all sub-space contributes dynamically to the
final render result. In this way, the violation of the multi-
view consistency in real Euclidean space when there is a
reflective surface can be overcome by placing the virtual im-
ages in certain sub-spaces only visible from certain views,
as shown in Fig. 5.

3.3. Multi-Space module

A naive implementation of the multi-space NeRF net-
work would be constructing the network using multiple tiny
parallel MLPs, with each one representing one sub-space
information, which, however, will largely increase param-
eters and has been proven to be tough to converge [32].

a

YS omg

(a) Composed render result. (b) RGB and weights of sub-spaces.

Figure 5. We visualize a novel view and a few decoded im-
ages with the corresponding weights of some sub-spaces from our
MS-NeRF zg model in Sec. 5.2. The results show that our method
successfully decomposes virtual images into certain sub-spaces.

Besides, our experiments in Sec. 3.2 demonstrate that the
current network structure of NeRF possesses the potential
to understand our 3D scenes. Therefore, we propose a com-
pact Multi-Space module (MS module), which takes advan-
tage of the neural feature field scheme originally designed
for memory saving [27], to sufficiently extract multi-space
information from standard NeRF backbone network struc-
tures with only small computational overheads. Specifi-
cally, the MS module will replace the original output layer
of the NeRF backbone. Below we describe the detailed ar-
chitecture of our module.

As shown in Fig. 4, our MS module only modifies the
output part of vanilla NeRF. Vanilla NeRF computes sin-
gle density o; and radiance c; for each position along a ray
casting through the scene and performs volumetric render-
ing using Eq. (2) to get the accumulated color. On the con-
trary, our multi-head layer replaces the neural radiance field
with the neural feature fields [27]. Specifically, the modi-
fied output layer gives K densities {c}} and features {f*}
of d dimension for each position along a ray with each pair
corresponding to a sub-space, where KK and d are hyper-

--- Page 5 ---
dataset origin applications Type viewpoints properties number
Realistic Synthetic 360° [25] _ view synthesis S 360-degree non-Lambertian 8
Real Forward-Facing [24,25] view synthesis R  forward-facing non-Lambertian 8
Shiny [42] viewsynthesis R  forward-facing —high-specular, refraction 8
Tanks and Temples(T&T) [19] view synthesis R 360-degree unbounded scenes 4
Mip-NeRF 360 [2] view synthesis R 360-degree unbounded scenes 9
EikonalFields [3] view synthesis R_ large angle view refraction 4
RFFR [12] view synthesis R  forward-facing reflection, semi-transparent 6
DTU [17] reconstruction R 360-degree non-Lambertian objects 15*
BlendedMVS [45] _ reconstruction S$ 360-degree non-Lambertian scenes 7*
Shiny Blender [38] view synthesis S 360-degree glossy materials 6
Ref-NeRF Real captured scenes [13,38] view synthesis R 360-degree glossy materials 3
Table 1. Properties of a commonly used dataset for NeRF-based methods. ‘S’ and ‘R’ represent synthesized and real captured, respectively.
We denote those unnamed datasets with the name of the methods. °*’ refers to the number of scenes commonly used by NeRF-based
methods, as the original dataset contains more scenes than noted, and we do not take them into consideration.

parameters for the total sub-space number and the feature
dimension of the neural feature field, respectively.

We then integrate features along the ray in each sub-
space to collect f feature maps that encode the color in-
formation and visibility of each sub-space from a certain
viewpoint. As all pixels are calculated the same way, we
denote each pixel as {F*} for simplicity and describe the
operation at the pixel level. Each pixel {F*} of the feature
maps is calculated using:

N

Yo TE = exp(—of 6;))€

i=l

Fe(r) = (4)

where the superscript k indicates the sub-space that the ray
casts through. The k-th density o* and feature f* corre-
spond to the k-th sub-space. TS = exp(— }))— 1 ks, ) and
6; =t; — t;-1 are similarly computed as in Eq. (2).

Then, {#"} is decoded by two small MLPs, each with
just one hidden layer. The first is a Decoder MLP that takes
{#F*} as input and outputs RGB vectors. The second is a
Gate MLP that takes {#*} as input and outputs weights
that control the visibility of certain sub-space. Specifically,
we use:

ky Ov, k ky Qa, k
{FP} > {O°}, {FT} —S {wh}, (5)
where Op represents the Decoder MLP, and Og represents
the Gate MLP. In the end, the MS module applies the soft-
max function to {w"} as the color contribution of each sub-
space to form the final render results:

(6)
X exp(w') 1, exp(w’)

xP w*

Eq. (6) needs no additional loss terms compared with the
vanilla NeRF method. As a result, the above light-weighted
MS module is able to serve as an enhancement module onto

the conventional NeRF-like backbone networks, and we
will show that our approach achieves significant enhance-
ments in Sec. 5.2.

4. Dataset
4.1. Existing datasets

We briefly revisit the commonly used or most relevant
datasets to our task and list their properties in Tab. 1. As
can be seen, there lacks a 360-degree benchmark for scenes
with complex light paths, e.g., a glass of water in front of a
mirror.

4.2. Our proposed dataset

As summarized in Sec. 4.1, there lacks a 360-degree
dataset consisting of complex reflection and refraction to
facilitate the related research. Therefore we collect a 360-
degree dataset comprising 25 synthetic scenes and 7 real
captured scenes.

For our synthesized part shown in Fig. 6a, we use an
open source software Blender [8] and design our scenes
with 3D models from BlenderKit, a community for shar-
ing 3D models. As our dataset consists of complete scenes
instead of single objects, we fix the height of our camera
position with the camera looking at the center of the scene
and moving the camera around a circle to render the whole
scene.

For all our scenes, we uniformly sample 120 points along
the circle and randomly choose 100 images for the training
set, 10 for the validation set, and 10 for the test set. The con-
structed dataset features a wide variety of scenes containing
reflective and refractive objects. We include a variety of
complexity of light paths, controlled by the number and the
layout of the mirror(s) in the scene, where the number of
mirrors ranges from | up to tens of small pieces. Note that
even a scene in our dataset with only one mirror is more
challenging than RFFR [25], as our camera moves from the

--- Page 6 ---
(b) A part of our real captured dataset.

Figure 6. Demo scenes of our datasets (more in the supplemen-
tary). Our dataset exhibits diversities of reflection and refraction,
which can serve as a benchmark for validating the ability to syn-
thesize novel views with complex light paths.

ront to the back of the mirror(s). Besides, we also construct
rooms with mirror walls that can essentially be treated as
unbounded scenes, where we add mirrors in the center of
the room and create unbounded virtual images. We further
build challenging scenes, including a combination of reflec-
tion and refraction.

We also include 7 captured real scenes with complex
ight conditions shown in Fig. 6b. We construct our scenes
using two mirrors, one glass ball with a smooth surface, one
glass ball with a diamond-like surface, a few toys, and a
ew books. We capture pictures randomly with 360-degree
viewpoints.

5. Experiments
5.1. Hyperparameters and benchmarks

We conduct three sets of experiments based on different
datasets with different baselines and our modules of differ-
ent scales. As our module is quite simple, we can scale
our module by three hyperparameters, which we refer to as
XK for the sub-space number, d for the dimension of output
features, and h for the hidden layer dimension of Decoder
MLP and Gate MLP, respectively. To compare fairly, we
carry out all the experiments following most default setting
from [1,2,12,25,38], except that we use 1024 rays per batch
and train 200k iterations for all experiments on all scenes.
Experiment details are as follows.

We select three representative NeRF-based models as
our baselines and integrate our modules with them. For
NeRF [25] and Mip-NeRF [1] based experiments, we
build MS-NeRFs and MS-Mip-NeRF, with hyperparam-
eters {K = 6,d = 24,h = 24}, similarly MS-NeRF jy
and MS-Mip-NeRF,, with hyperparameters {KK = 6,d =

PSNR{t SSIM+ LPIPS| # Params

NeRF 30.82 0.865 1.159M
MS-NeRFs 1.201M
MS-NeRFjy 1.245M
MS-NeRFg 1.311M

Mip-NeRF 3142 0.874 0.215 0.613M
Ref-NeRF 32.37 0.882 0.713M
MS-Mip-NeRF, 0.195  0.634M
MS-Mip-NeRF, 0.656M
MS-Mip-NeRF , 0.689M

Mip-NeRF 360 9.007M
MS-Mip-NeRF 360 9.052M
(a) Comparisons on the synthetic part of our dataset.

PSNRt SSIMt LPIPS|  # Params
Mip-NeRF 360

9.007M
MS-Mip-NeRF 360 9.052M

(b) Comparisons on the real captured part of our dataset.

PSNRt SSIM+t LPIPS| # Params

NeRFReN* 1.264M
MS-NeRFr 1.295M

(c) Comparisons on RFFR dataset. ’*’ denotes that we re-train the model
using the official code following the provided setting, except the number
of masks used for reflective surfaces is 0.

Table 2. Quantitative comparisons with existing methods.

48,h = 48}, and MS-NeRFg and MS-Mip-NeRF, with
hyperparameters {KK = 8,d = 64,h = 64}. For Mip-
NeRF 360 [2] based experiments, we construct MS-Mip-
NeRF 360 with hyperparameters {K = 8,d = 32,h =
64}. Moreover, we provide a comparison with Ref-
NeRF [38] because it uses Mip-NeRF as a baseline and pos-
sesses an outstanding ability to model glossy materials.

We also compare our method with NeRFReN on the
RFFR dataset [12]. NeRFReN is a specially designed two-
branch network based on vanilla NeRF for mirror-like sur-
faces in forward-facing scenes. Thus we construct a tiny
version of our method, referred to as MS-NeRF 7, based on
NeRF with hyperparameters {K = 2,d = 128,h = 128}.
Here we use two sub-spaces as NeRFReN tries to decom-
pose reflective surfaces into two parts, and we want to show
that our space decomposition is more effective. For a fair
comparison, we re-train NeRFReN using the official code
under the provided settings on the RFFR dataset, except that
we set the number of the used mask to 0 as our method re-
quires no extra mask.

All the training details can be found in the supplemen-
tary. We report our results with three commonly used met-
rics: PSNR, SSIM [41], and LPIPS [51].

--- Page 7 ---
(a) Mip-NeRF 360 (b) MS-Mip-NeRF 360

Figure 7. Visual comparison between Mip-NeRF 360 and MS-
Mip-NeRF 360. Our module can extend Mip-NeRF 360 to model
unbounded virtual scenes.

(a) Mip-NeRF 360 (b) MS-Mip-NeRF 360

Figure 8. Visual comparison between Mip-NeRF 360 and MS-
Mip-NeRF 360 on the real captured part of our dataset. Our

method is robust enough to recover virtual images in the real
world.

5.2. Comparisons

Quantitative comparisons. As reported in Tab. 2a, our
module can be integrated into most NeRF-like models and
improve performance by a large margin with minimal extra
cost introduced. Especially in Mip-NeRF 360-based exper-
iments, our module exhibits better results of 3.46 dB im-
provement in PSNR with merely 0.5% extra parameters.
Besides, our Mip-NeRF-based models also outperform Ref-
NeRF [38] by a large margin, which is a variant based on
Mip-NeRF with the outstanding ability to model glossy ma-
terials. We also demonstrate our results compared with the
state-of-the-art Mip-NeRF 360 results on the real-captured
part of our dataset in Tab. 2b. Our approach also shows
large improvements. As shown in Tab. 2c, our approach
achieves better results when no manually-labeled masks are
provided in training on the RFFR dataset, which contains
forward-facing reflective surfaces in the scenes. The above
experiments demonstrate the superiority and compatibility
of our method.

Qualitative comparisons and discussions. Besides quan-
titative comparisons, we summarize the advantages of our
modules and support them by qualitatively or quantitatively

(a) Ref-NeRF (b) MS-Mip-NeRF ,

Figure 9. Visual comparison between MS-Mip-NeRF , and Ref-
NeRF. Our method significantly outperforms Ref-NeRF on reflec-
tive surfaces.

(a) NeRFReN

(b) MS-NeRFg
Figure 10. (a) Trained with accurately labeled masks, NeRFReN
even fails to render ordinary parts of the scene in 360-degree

scenes with mirrors. (b) Our method requires no extra manually
labeled masks and renders high-quality images.

comparing our methods with the corresponding baselines.

Qualitative comparisons with the state-of-the-art method
(i.e., Mip-NeRF 360) are shown in Fig. |, Fig. 7, and Fig. 8.
Our method renders high-fidelity virtual images, bounded
and unbounded, in both synthetic and real-world scenes.

A qualitative comparison with Ref-NeRF [38], which
understands virtual images as textures using the conven-
tional NeRF backbone, is shown in Fig. 9. As Ref-NeRF
is also based on Mip-NeRF, we compare our Mip-NeRF-
based variant with Ref-NeRF using the same baseline and
use comparable parameters (specifically ours 0.689M and
Ref-NeRF 0.713M) in the comparison. Again the qualita-
tive results show our significant improvements in rendering
reflective surfaces.

We also compare with the NeRFReN model, which re-
quires accurately labeled masks of the reflective regions
during training and handles forward-facing reflective sur-
faces only. In this comparison, we train their model on
our synthesized dataset with extra accurate reflection masks
provided. Fig. 10 shows that their model fails to recover
360-degree high-fidelity rendering while our approach suc-
ceeds.

5.3. Ablation studies

In this section, we evaluate the design of our module and
explore the relation between the number of sub-space and
the number of virtual images.

--- Page 8 ---
Overview Ground Truth MS-NeRFs MS-NeRFavg NeRF

Figure 11. Comparing NeRF, MS-NeRF 4g and MS-NeRFs.

PSNRt SSIMt+ LPIPS| # Params

NeRF 1.159M
MS-NeRF Ang 1.166M
MS-NeRFs 1.201M

Table 3. Ablation study on our module architecture.

Ablation on using neural feature field. We implement a
module that simply outputs K scalars {of} and K RGB
vectors {c!} of three dimensions, then we use the same in-
tegral equation as NeRF to get the pixel color of each sub-
space and we average among all sub-spaces to get the fi-
nal pixel color. We integrate this design into vanilla NeRF
noted as MS-NeRF4,,, where we set K = 6, and the re-
sults are reported in Tab. 3. We also exhibit a few visual
results in Fig. 11, which indicate that a simple multi-space
radiance field assumption can help the model partially over-
come the violation of reflections, but will also introduce the
over-smoothing problem because of the lack of an efficient
multi-space composition strategy.

Ablation on the sub-space number. In our Euclidean
space, one can control the number of virtual sub-spaces by
the number and the layout of the mirror(s). For example,
when two mirrors are facing each other, there could be in-
finitely recursive virtual image spaces, but when two mir-
rors are placed back against each other, there will be just one
virtual image behind each mirror. To provide a guideline for
the design of our module, we choose two scenes consisting
of two mirrors with different layouts from our synthesized
part of the dataset and train NeRF-based variants of differ-
ent sub-space numbers and different feature dimensions.
We construct our variants based on NeRF with the out-
put feature dimensions d € {24, 48,64} and the number of
sub-spaces K € {2,4,6,...,16}, then we train our mod-
els on the two scenes and report the results using PSNR.
Our results in Fig. 12 show that the number of sub-spaces

— MS-NeRFg=24
—— MS-NeRF y=
—— MS-NeRFy—64
— NeRF

2 4 6 8 10 12 14 16
# of sub-spaces

Figure 12. We use PSNR to quantitatively evaluate the ablation
experiments on scene01 and scene02 and plot the results with solid
and dotted lines, respectively.

is not required to match the actual number of virtual image
spaces, and 6 sub-spaces can guarantee stable learning for
multi-space radiance fields. Moreover, feature fields with
dimension d = 24 already encode enough information for
composition, but for stable performance d = 48 is better.

6. Conclusion

In this paper, we tackle the long-standing problem of ren-
dering reflective surfaces in NeRF-based methods. We in-
troduce a multi-space NeRF method that decomposes the
Euclidean space into multiple virtual sub-spaces. Our pro-
posed MS-NeRF approach achieves significantly better re-
sults compared with conventional NeRF-based methods.
Moreover, a light-weighted design of the MS module al-
lows our approach to serve as an enhancement to the con-
ventional NeRF-backbone networks. We also constructed a
novel dataset for the evaluation of similar tasks, hopefully,
helping future researches in the community.

Acknowledgment: This work is funded by the Natural Sci-
ence Foundation of China (NO. 62132012).

--- Page 9 ---
References

1

10

11

12

13

14

15

Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. In JCCV, pages 5855-5864, 2021. 2, 6
Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fiecvprids. In JEEE CVPR, pages
5470-5479, 2022. 1, 2, 5,6

Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad,
Hans-Peter Seidel, and Tobias Ritschel. Eikonal fields for
refractive novel-view synthesis. In ACM SIGGRAPH 2022
Conference Proceedings, pages 1-9, 2022. 2, 5

Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In IEEE
CVPR, pages 16123-16133, 2022. 2

Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In European
Conference on Computer Vision (ECCV), 2022. 2

Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng,
Xuan Wang, and Jue Wang. Hallucinated neural radiance
fields in the wild. In JEEE CVPR, pages 12943-12952, 2022.
1

Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-
drea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-
terization pipeline for efficient neural field rendering on mo-
bile architectures. arXiv:2208.00277, 2022. 2

Blender Online Community. Blender - a 3D modelling and
rendering package. Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2022. 3, 5

Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black,
and Timo Bolkart. Capturing and animation of body and
clothing from monocular video. arXiv:2210.01868, 2022. 2
Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. Fastnerf: High-fidelity neural
rendering at 200fps. In JCCV, pages 14346-14355, 2021. 2
Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas
Funkhouser. Object-centric neural scene rendering.
arXiv:2012.08503, 2020. 2

Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-
Hai Zhang. Nerfren: Neural radiance fields with reflections.
In IEEE CVPR, pages 18409-18418, 2022. 1, 2, 3,5, 6
Peter Hedman, Ben Mildenhall,
Jonathan T Barron, and Paul Debevec. Baking neural ra-
diance fields for real-time view synthesis. In JCCV, pages
5875-5884, 2021. 5

Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan
Wang, and Qing Wang. Hdr-nerf: High dynamic range neu-
ral radiance fields. In JEEE CVPR, pages 18398-18408,
2022. 2

Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object gen-
eration with dream fields. In JEEE CVPR, pages 867-876,
2022. 2

Pratul P Srinivasan,

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

Nishant Jain, Suryansh Kumar, and Luc Van Gool. Robusti-
fying the multi-scale representation of neural radiance fields.
arXiv:2210.04233, 2022. 1,3

Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,
and Henrik Aanes. Large scale multi-view stereopsis evalu-
ation. In JEEE CVPR, pages 406-413, 2014. 2, 5

Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-
frecon: Self reconstruction your digital avatar from monoc-
ular video. In IEEE CVPR, pages 5605-5615, 2022. 2
Amo Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. ACM TOG, 36(4):1-13, 2017. 2,5
Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields.
In ICCV, pages 5741-5751, 2021. 1,3

Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Saijjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections. In JEEE CVPR, pages 7210-
7219, 2021. 1

Nelson Max. Optical models for direct volume rendering.
IEEE TVCG, 1(2):99-108, 1995. 3

Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,
Pratul P Srinivasan, and Jonathan T Barron. Nerf in the dark:
High dynamic range view synthesis from noisy raw images.
In IEEE CVPR, pages 16190-16199, 2022. 2

Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light field fusion: Practical view syn-
thesis with prescriptive sampling guidelines. ACM TOG,
38(4):1-14, 2019. 2,5

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM, 65(1):99-106, 2021. 1,
2,3,5,6

Thomas Miiller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM TOG, 41(4), jul 2022. 2
Michael Niemeyer and Andreas Geiger. Giraffe: Represent-
ing scenes as compositional generative neural feature fields.
In JEEE CVPR, pages 11453-11464, 2021. 2, 4

Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In JCCV, pages 5589-
5599, 2021. 2

Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
In ICCV, pages 5865-5874, 2021. 1

Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-nerf: Neural radiance fields for
dynamic scenes. In JEEE CVPR, pages 10318-10327, 2021.
1

Ravi Ramamoorthi et al. Precomputation-based rendering.
Foundations and Trends® in Computer Graphics and Vision,
3(4):281-369, 2009. 2


--- Page 10 ---
32

33

34

35

36

37

38

39

40

41

42

43

44

45

Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
Geiger. Kilonerf: Speeding up neural radiance fields with
thousands of tiny mlps. In JCCV, pages 14335-14345, 2021.
2,4

Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In JEEE CVPR, pages 4104-4113,
2016. 2

Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In IEEE CVPR, pages 5459-5469, 2022. 2
Jiaming Sun, Xi Chen, Qiangian Wang, Zhengqi Li, Hadar
Averbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural
3d reconstruction in the wild. In ACM SIGGRAPH 2022
Conference Proceedings, pages 1-9, 2022. |

Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-
vasan, Edgar Tretschk, W Yifan, Christoph Lassner, Vincent
Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al.
Advances in neural rendering. Computer Graphics Forum,
41(2):703-735, 2022. |
Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael
Zollhéfer, Christoph Lassner, and Christian Theobalt. Non-
rigid neural radiance fields: Reconstruction and novel view
synthesis of a dynamic scene from monocular video. In
ICCV, pages 12959-12970, 2021. |
Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:
Structured view-dependent appearance for neural radiance
fields. CVPR, 2022. 2,5, 6,7

Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manipula-
tion of neural radiance fields. In IEEE CVPR, pages 3835—
3844, 2022. 2

Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. Advances in Neural Information Processing Sys-
tems, 34:27171-27183, 2021. 2

Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. JEEE transactions on image processing,
13(4):600-612, 2004. 6

Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time
view synthesis with neural basis expansion. In JEEE CVPR,
pages 8534-8543, 2021. 2,5

Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han
Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.
Learning object-compositional neural radiance field for ed-
itable scene rendering. In JCCV, pages 13779-13788, 2021.
2

Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun
Liang, Tai-Jiang Mu, and Shi-Min Hu. Recursive-nerf: An
efficient and dynamically growing nerf. IEEE TVCG, 2022.
2

Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,
Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-
scale dataset for generalized multi-view stereo networks. In
IEEE CVPR, pages 1790-1799, 2020. 2, 5

10

46

47

48

49

50

51

52

Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. Plenoctrees for real-time rendering of
neural radiance fields. In ICCV, pages 5752-5761, 2021. 2
Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,
Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of
neural radiance fields. In JEEE CVPR, pages 18353-18364,
2022. 2

Kaan Yiicer, Alexander Sorkine-Hornung, Oliver Wang, and
Olga Sorkine-Hornung. Efficient 3d object segmentation
from densely sampled light fields with applications to 3d re-
construction. ACM TOG, 35(3):1-15, 2016. 2

Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva
Ramanan. Ners: Neural reflectance surfaces for sparse-view
3d reconstruction in the wild. Advances in Neural Informa-
tion Processing Systems, 34:29835—29847, 2021. 1

Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
fields. arXiv:2010.07492, 2020. 1, 2

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In JEEE CVPR, pages 586-
595, 2018. 6

Yufeng Zheng, Victoria Fernandez Abrevaya, Marcel C
Biihler, Xu Chen, Michael J Black, and Otmar Hilliges. Im
avatar: Implicit morphable head avatars from videos. In
IEEE CVPR, pages 13545-13555, 2022. 2

