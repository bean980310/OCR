--- Page 1 ---
arXiv:2305.12001v2 [cs.CL] 24 Oct 2023

OPT-R: Exploring the Role of Explanations in Finetuning and Prompting
for Reasoning Skills of Large Language Models

Badr AlKhamissi Siddharth Verma Ping Yu Zhijing Jin
Asli Celikyilmaz Mona Diab
Meta AI
Abstract Scale

In this paper, we conduct a thorough investiga-
tion into the reasoning capabilities of Large
Language Models (LLMs), focusing specif-
ically on the Open Pretrained Transformers
(OPT) models as a representative of such mod-
els. Our study entails finetuning three differ-
ent sizes of OPT on a carefully curated rea-
soning corpus, resulting in two sets of fine-
tuned models: OPT-R, finetuned without ex-
planations, and OPT-RE, finetuned with expla-
nations. We then evaluate all models on 57
out-of-domain tasks drawn from the SUPER-
NATURALINSTRUCTIONS benchmark, cover-
ing 26 distinct reasoning skills, utilizing three
prompting techniques. Through a comprehen-
sive grid of 27 configurations and 6,156 test
evaluations, we investigate the dimensions of
finetuning, prompting, and scale to understand
the role of explanations on different reasoning
skills. Our findings reveal that having expla-
nations in the fewshot exemplar has no signifi-
cant impact on the model’s performance when
the model is finetuned, while positively affect-
ing the non-finetuned counterpart. Moreover,
we observe a slight yet consistent increase in
classification accuracy as we incorporate ex-
planations during prompting and finetuning, re-
spectively. Finally, we offer insights on which
skills benefit the most from incorporating ex-
planations during finetuning and prompting,
such as Numerical (+20.4%) and Analogical
(+13.9%) reasoning, as well as skills that ex-
hibit negligible or negative effects.

1 Introduction

Recently, there has been a surge in the release of
Large Language Models (LLMs) by both indus-
trial and academic institutions. These models vary
from open-source releases such as OPT (Zhang
et al., 2022) and LLAMA (Touvron et al., 2023) to
closed-source ones like GPT-3 (Brown et al., 2020)
and PALM (Chowdhery et al., 2022). In addition,
researchers have developed models that are fine-
tuned on top of these foundational models to better

13B

6.7B

1.3B
OPT OPT-R OPT-RE

Zeroshot
Fewshot

Fewshot-E

Finetuning

Prompting

Figure 1: Three-Dimensional Grid of Fine-Tuning,
Prompting, and Scale. Each dimension is represented
as an axis, with three levels for each of finetuning,
prompting, and scale plotted on each axis. The resulting
grid consists of 27 different combinations evaluated on
various reasoning tasks. It should be noted that there is
a hidden dimension, the scoring function, comprising
four components. This results in a comprehensive total
of 6,156 evaluations.

follow instructions, such as OPT-IML (Iyer et al.,
2022) and Alpaca (Taori et al., 2023). Despite
the remarkable progress in LLMs’ performance
in Natural Language Processing (NLP) tasks, rea-
soning remains a challenging area. For example,
prior work have shown that LLMs struggle with
commonsense reasoning (West et al., 2022) and
arithmetic reasoning (Hendrycks et al., 2021) to
name a few.

Recent efforts have attempted to improve the
reasoning performance of LLMs by decomposing
answers into step-by-step reasoning chains using in-
context learning (Wei et al., 2022b; Kojima et al.,
2022) or during finetuning (Chung et al., 2022;
Wei et al., 2021a). While these approaches have
shown some improvement on benchmarks such as
GSM8K (Cobbe et al., 2021), it is not clear how
those explanations affect finetuning, prompting, or

--- Page 2 ---
{Task Definition}

Provide your answer followed by a brief reasoning.
{In-Context Examples}

Input: {input}

Options: {options}

Output: The answer is {answer} because {explanation}

Figure 2: Template used during both training and infer-
ence. The model is tasked with predicting the answer
followed by the explanation.

their combination. Concurrent work has investi-
gated the generalization capability of such models
to reasoning skills beyond those encountered dur-
ing finetuning (Yu et al., 2022), but a comprehen-
sive evaluation of the role of explanation during
finetuning and prompting with respect to reasoning
skills is still lacking.

In this paper, we aim to address this gap. We in-
vestigate OPT (Zhang et al., 2022) as a representa-
tive of such models and utilize it as our base model.
Through finetuning OPT on a collection of care-
fully curated open-source reasoning datasets that
come with explanations for each instance, we eval-
uate its performance on 57 tasks drawn from the
SUPER-NATURALINSTRUCTIONS benchmark (Wang
et al., 2022), covering 26 different reasoning skills.
Our experiments are structured around three key
dimensions: finetuning, prompting, and scale, each
of which is comprised of three distinct components
(See Figure 1). Finetuning: (1) a (vanilla) un-
finetuned OPT model; (2) A finetuned OPT model
without explanations (OPT-R); and, (3) A finetuned
OPT model with explanations (OPT-RE). Prompt-
ing: (1) zero-shot prompting; (2) Fewshot prompt-
ing without explanations; and, (3) Fewshot prompt-
ing with explanations. Finally, Scale: (1) 1.3B; (2)
6.7B; and, (3) 13B. Accordingly, we create grid
of 27 different components, providing a detailed
analysis measuring the impact of explanations dur-
ing finetuning and inference across different model
scales.

Our findings reveals that finetuning on reason-
ing datasets leads to statistically significant im-
provements in seven reasoning skills, including
Numerical, Analogical and Reasoning on Objects,
with Physical, Counting and Textual Entailment
showing a significant effect only for the OPT-RE
model, across both fewshot prompting conditions

and model sizes, as compared to the vanilla OPT
model (see Table 2). However, we also find that
this approach significantly hinders the performance
of three other reasoning skills (see Table 3). We
also investigate the impact of incorporating expla-
nations during fewshot prompting and find that it
does not have a significant impact on the perfor-
mance of the finetuned models, as measured by the
variance in the difference between both prompting
methods across reasoning skills for each model.
However, we notice that it has a more noticeable ef-
fect on the performance of the vanilla OPT model,
as shown in Table 5. Additionally, we observe
a consistent increase in the average performance
across all tasks from Fewshot to Fewshot-E, as well
as from OPT to OPT-R to OPT-RE models, indi-
cating that explanations do have a small effect on
performance during both finetuning and prompting.
Finally, Table 4 presents a summary of the results,
indicating which reasoning skills demonstrate im-
provement due to the incorporation of explanations
during either finetuning or prompting, which skills
show a negative effect, and which skills have negli-
gible effects regarding explanations.

2 OPT-R: Finetuning on Reasoning Skills

2.1 Reasoning Datasets with Explanations

10°

& © & ¢
os eS Se S & ee

Number of Samples

AS wf
¥ &

Dataset

Figure 3: Number of samples in each dataset of the
training corpus. Y-axis in log scale.

The finetuning corpus utilized to refine OPT is
composed of various reasoning datasets, each of
which includes a corresponding explanation or ra-
tionale for the answer. These rationales may con-
sist of a sequence of smaller steps (i.e. chain-of-
thought) or a free-form text that elucidates the rea-
soning behind the answer. As shown in Figure 2,
we employ a uniform template for all tasks during

--- Page 3 ---
the training process. The input to the model begins
with a task definition, followed by an instruction to
provide an answer followed by a brief reasoning.
Next, we extract two random in-context examples
uniformly from the training set that remain con-
stant throughout training for each instance. The
input for the current training instance is then pre-
sented in a format specific to each task. The options
for the answer are then included in the input, but
not in the in-context examples (see Appendix A
for further details on task-specific definitions and
options). The options are pre-shuffled for each
training instance. The model is finally provided
with the answer prefix, "Output: The answer
is”, and is tasked to predict the answer, followed
by an explanation if OPT-RE is being finetuned.
Similarly, the in-context examples only comprise
an explanation when training OPT-RE.

Below is a brief description of each dataset used
during finetuning. See Figure 3 for the relative size
of each dataset.

AQUA-RAT The Algebra Question Answering
with Rationales dataset (Ling et al., 2017) render-
ing the task of solving algebraic word problems
more feasible by dividing the problem into a series
of smaller steps. They create a 100k-sample dataset
that contains questions, answers and rationales in
natural language and human-readable mathemati-
cal expressions that can be used to derive the final
answer.

CoQA The Conversational Question Answering
dataset Reddy et al. (2019). It consists of 127k
questions and answers, compiled from 8k conversa-
tions about passages from seven different domains.
Given a passage that contains a conversation, the
model is tasked with answering a question by high-
lighting the corresponding evidence from the pas-
sage.

CoS-E The Common Sense Explanations dataset
Rajani et al. (2019) to induce language models with
commonsense reasoning. In this dataset, the model
is given a question and a set of choices and is tasked
with selecting one of the provided choices along
with providing an explanation in natural language
as to why that choice is correct.

ECQA_ The Explanations for Commonsense
Question Answering dataset Aggarwal et al. (2021).
It is similar to CoS-E since it requires the model to
choose one of the provided options to answer the

given question, and also provide an explanation.

ESNLI The Stanford Natural Language Infer-
ence dataset with Explanations Camburu et al.
(2018) to train models to provide interpretable and
robust explanations for their decisions. The authors
extend the SNLI dataset (Bowman et al., 2015) with
human-annotated explanations. Similar to any NLI
task, the model is given a premise and hypothesis
and the task is to determine whether the hypothe-
sis sentence entails, contradicts, or is neutral with
respect to the given premise.

GSM8K The Grade School Math dataset Cobbe
et al. (2021) to train models to better perform multi-
step mathematical reasoning. It consists of 8.5k
linguistically diverse grade school math word prob-
lems. Therefore, the task for the model is to answer
the question by performing a series of arithmetic
operations to obtain a final answer, while explain-
ing it’s reasoning steps.

ProofWriter The ProofWriter dataset Tafjord
et al. (2021) to generate both the implications of
a theory from the RuleTaker dataset (Clark et al.,
2020) and the natural language proofs that support
them. Specifically, given a sequence of facts and
rules, the model is tasked with answering a ques-
tion using “Yes”, “No”, or “Unknown” and provide
the reasoning path by referring to the provided facts
and rules. We consider the open-world assumption
subset of RuleTaker with questions that requires
reasoning up to a depth of 5.

StrategyQA The Strategy Question Answering
dataset Geva et al. (2021) to improve multi-hop rea-
soning for questions where the required reasoning
steps are implicit in the question. Therefore, the
task of the model is to answer the question using
“Yes” or “No” then provide a strategy that explains
the answer by decomposing it into a number of
steps.

2.2 Finetuning Procedures

OPT The Open Pretrained Transformers (OPT)
models are a suite of decoder-only pre-trained trans-
formers ranging from 125M to 175B parameters
released by Zhang et al. (2022). In this work, we
use three OPT models with sizes of 1.3B, 6.7B
and 13B. The details of each model architecture,
pre-training corpus and training configuration (e.g.
weight initialization, optimizer, tokenizer, hyperpa-
rameters, etc.) can be found in Zhang et al. (2022).

--- Page 4 ---
Reasoning Skill

Task IDs

Abductive Reasoning
Analogical Reasoning
Argument Reasoning
Causal Reasoning
Commonsense Reasoning

Commonsense Reasoning > Numerical Commonsense ...

Commonsense Reasoning — Physical Reasoning
Commonsense Reasoning —> Social Situations
Commonsense Reasoning —> Spatial Reasoning

ask854

ask1287, task1288

ask514

ask1393

ask279, task156, task295
ask1403

ask084

ask580, task937, task 1606
ask082, task083

Deductive Reasoning
Ethics

Grammatical Reasoning
Logical Reasoning

Logical Reasoning — Reasoning with Symbols

Mathematics — Counting
Multihop Reasoning

Numerical Reasoning
Reasoning on Objects
Reasoning on Social Interactions
Reasoning on Strings

Relational Reasoning

Scientific Reasoning

Temporal Reasoning

Textual Entailment

Textual Entailment + Analogical Reasoning
Textual Entailment > Deductive Reasoning

ask221, task 1568, task220
ask667, task724, task723
ask1712, taskO52, task1559
ask717, task211, task268
ask923, task935

ask523, task155

ask1297, task056

ask621, task 1333

ask1583, task1584

ask609, task881, task875
ask1189

ask 1380, task472, task1505
ask1431, task228, task714
ask018, task1549, task383
ask738, task890, task463
ask1347

ask1612, task534, task 1366

Table 1: Evaluation tasks from SUP-NATINST (Wang et al., 2022) used for each reasoning skill.

Implementation Details To finetune the selected
models, we utilized the metaseq! implementation
since it enables higher training efficiency compared
to other codebases (Zhang et al., 2022). Each
model is finetuned twice for 10 epochs, once with
explanations and once without (i.e. OPT-RE vs
OPT-R, respectively). Models are evaluated at
the end of each epoch on a chosen set of SUPER-
NATURALINSTRUCTIONS validation tasks, and the
checkpoint with the best performance is selected
for evaluation on the testing tasks. The loss is
calculated only on the tokens the model is tasked
to predict during inference, and not the full input,
what is referred to as label-loss in (Iyer et al., 2022).
The samples across all datasets are shuffled during
training. Further, the model is provided with two
in-context examples during finetuning in addition
to the task definition to match inference time fol-
lowing (Wang et al., 2022).

‘https: //github.com/facebookresearch/metaseq

3 Evaluating the Models

3.1 SUPER-NATURALINSTRUCTIONS Tasks

In this study, we focus on a subset of the SUPER-
NATURALINSTRUCTIONS benchmark version 2.67
(SUP-NATINST for short) proposed by Wang et al.
(2022), which comprises 1,616 varied NLP tasks
and includes meta-labels for each task, such as
task type, domain and more importantly for this
work: the underlying reasoning skills. Specifically,
we select a subset of tasks that satisfy two key
criteria: (i) the task focuses on a single reasoning
skill, enabling us to evaluate a specific atomic skill,
and (ii) the task can be tested using classification
mode, as detailed in Section 3.2. Note that there is
no data contamination between finetuning data and
the evaluation benchmark.

2We downloaded the data from https: //github.com/
allenai/natural-instructions/tree/v2.6.

--- Page 5 ---
Fewshot-E

Accuracy (%)
8 6 8 &

@
o

1.3B 6.7B 13B 1.3B 6.7B
Size Size

Fewshot

add ddd

Zeroshot
Model
8 OPT
38 OPT-R
8 OPT-RE
13B 1.3B 6.7B 13B
Size

Figure 4: Results achieved across all tasks as a function of the three primary dimensions analyzed in this study:

Finetuning, Prompting and Scale.

Benchmark Splits Following the task selection
process, we apply a random sampling technique to
ensure diversity within the testing set. Specifically,
we select a maximum of three tasks from each rea-
soning skill, and allocate any remaining tasks to
the validation set. Notably, this approach enables
us to obtain a representative sample of the selected
reasoning skills for testing, while also ensuring
that our model’s performance is not influenced by
a particular subset of tasks. Table | shows the com-
plete list of tasks used for evaluating our finetuned
models for each reasoning skill.

3.2. Evaluation Setup

Earlier, we mentioned that we selected 57 tasks
spanning 26 reasoning skills from SUP-NATINST to
evaluate our finetuned models. To meet our criteria,
as detailed in Section 3.1, each task had to fulfill
two conditions. The second condition required
that the task can be considered a classification task.
That means there is a discrete set of candidates
(one of which is correct) and thereby treating it as
a classification problem where the highest-scoring
candidate is considered the answer. To ensure this,
we utilized a straightforward heuristic: we only
sampled tasks that had no more than 10 possible
candidate answers.

Classification Method To determine the correct
answer, we conduct a forward pass for each poten-
tial candidate answer and utilize a scoring function
to measure the likelihood that the candidate tokens
follows the input, similar to Brown et al. (2020).
This process is repeated four times using distinct
scoring functions, as detailed in the subsequent

paragraph. The highest accuracy score from the
four scoring functions is considered as result of the
task.

Scoring Functions — This is considered the fourth
dimension of this work since we evaluate each task
using four different scoring functions and take the
maximum accuracy as the result. The four scoring
functions used are as follows: (1) mean, which
involves computing the average of the log probabil-
ities of candidate tokens, also referred to as token
score. (2) unconditional-norm, which computes
the difference between the sum of token scores of
the candidate when unconditioned by any previous
tokens and the sum of candidate token scores when
conditioned by previous input. (3) suffix, which
computes the sum of the conditioned candidate’s to-
ken scores alone. Finally, (4) sum, which involves
calculating the sum of all the token scores passed
to the model. The reason we employed different
functions is that we observed significant gains in
performance when using one scoring function over
the other for specific tasks. Therefore, in order
to ensure fairness across all tasks, we selected the
highest accuracy over all scoring functions for each
task.

4 Results & Findings

In this section, we present the results and findings
of our experiments. First, we illustrate in Figure
4 the outcome of our evaluation on the effective-
ness of finetuned models as compared to the vanilla
OPT model, across three different scales when us-
ing both fewshot prompting with and without ex-
planations. Furthermore, we observe a monotonic

--- Page 6 ---
increase in the performance of each model as we
increase the scale under those two prompting condi-
tion, which indicates a positive correlation between
the model’s capacity and its overall performance.
However, we note that this trend does not apply
to the zeroshot prompting method, since we are
testing out-of-distribution tasks and that the fine-
tuned models were trained with fewshot exemplars
in their context. This leads us to focus only on
the fewshot prompting methods, with and with-
out explanations, for the remaining of our evalu-
ations. Specifically, we investigate the impact of
finetuning the OPT models on reasoning datasets,
as compared to the vanilla OPT model, and explore
the effect of explanations during finetuning and
prompting, both in terms of the reasoning skill.

4.1 Model Performance for Reasoning Skills

The results reported in this and the following sec-
tion are the classification accuracy of each reason-
ing skill across different conditions, such as model
sizes and fewshot prompting methods. Table 2
shows the reasoning skills where either OPT-RE
or OPT-R are significantly better than the vanilla
OPT model, as measured by Welch’s t-test, where
p < 0.05. Conversely, Table 3 show the reason-
ing skills where the vanilla OPT model performs
significantly better than either of its finetuned coun-
terparts.

Skill OPT OPT-R OPT-RE
Numerical 44.8 65.2* 64.7*
Analogical 49.0 62.9* 60.8*
Counting 19.8 13.1 31.3*
Physical 38.2 37.8 49.1*
Entailment 42.6 47.2 51.6*
Social Int 34.1 43.0* 40.1
Objects 54.3 62.6* 59.9*

Table 2: Performance as a function of the reasoning
skills where OPT-RE or OPT-R performs significantly
better than the OPT model as measured by Welch’s t-test
(p < 0.05) denoted by the * symbol. The performance is
measured across Fewshot and Fewshot-E prompting, the
three different scales and tasks under the corresponding
reasoning skill. Best result indicated in bold.

The results reveal that the finetuned variants of
the OPT model demonstrate a significant improve-
ment on seven distinct reasoning skills, with par-
ticular emphasis on the Numerical and Analogical
reasoning tasks. Specifically, for the Mathematical

Skill OPT OPT-R OPT-RE
Argument 57.9 46.17 48.7—
TE - Deductive 36.0 29.07 29.4—
Commonsense 33.4 29.7 28.8—

Table 3: Performance as a function of the reasoning
skill where OPT performs significantly better than either
OPT-R or OPT-RE as measured by Welch’s t-test (p <
0.05) denoted by the ~ symbol. The performance is
measured across Fewshot and Fewshot-E prompting, the
three different scales and tasks under the corresponding
reasoning skill. TE is Textual Entailment.

Counting skill, the OPT-RE variant outperforms
both the OPT-R and OPT models, underscoring
the criticality of incorporating explanations during
the finetuning process for mathematical datasets.
Likewise, the Physical Reasoning tasks exhibit a
similar trend. On the other hand, we can see that
for the Argument, Deductive Textual Entailment
and Commonsense skills the non-finetuned version
outperforms considerably.

4.2. Fine-Grained Skill Analysis

Table 4 shows the classification accuracy results
obtained from the three models, in relation to the
reasoning skill and few-shot prompting method
used. The best accuracy value for each reasoning
skill is indicated in bold, and the cells are shaded
with colors ranging from green to white to indicate
their position in the accuracy spectrum of each rea-
soning skill. The skills with similar performance
across different models are assigned a lighter shade
of green, indicating that their color spectrum ends
earlier than that of other skills where the difference
in performance between models is more significant.
The table is divided into four blocks to distinguish
effects of finetuning and prompting methods on
reasoning skills: the first block showcases skills
where the finetuned (OPT-RE and OPT-R) mod-
els outperform the vanilla OPT model, the second
block highlights skills where OPT-RE has better
accuracy than other models therefore illustrating
the importance of finetuning on explanations on
those skills. The third block displays skills where
OPT outperforms other models showing that fine-
tuning actually hurts performance in this case, and
the fourth block identifies skills where the choice
of model or prompting method has little impact on
the overall performance.

--- Page 7 ---
OPT

OPT-R OPT-RE

Skill Fewshot Fewshot-E Fewshot Fewshot-E Fewshot Fewshot-E
Numerical 39.9 49.7

Analogical 51.9 46.2

Objects 53.5 55.1

Social Interactions 33.6 34.7

Textual Entailment 43.3 42.0

Grammatical 54.4 55.1

Multihop 36.6 31.7

Symbols 44.2 47.2

Spatial 44.1 47.1

Social Situations 46.3 46.6 53.2

Counting 19.6 20.0 13.5 12.7

Physical 35.8 40.6 36.9 38.8

Logical 31.7 33.4 33.7 34.1 36.9 38.4
Temporal 43.4 38.5
Argument 46.3 45.9 48.6 48.8
TE - Deductive 33.7 27.9 30.1 29.0 29.9
Relational 474 51.1 47.6 47.9 44.8 44.6
Commonsense 35.0 31.8 29.8 29.5 28.5 29.2
TE - Analogical 16.3 18.7 18.6 20.7 18.7 18.1
Abductive 33.9 36.1 36.9 34.4 34.2 35.3
Ethics 26.8 25.8 26.5 25.9 26.2 27.6
Deductive 39.4 40.4 39.4 40.4 40.0 41.1
Causal 50.2 50.6 49.1 48.9 50.1 50.5
Scientific 23.4 23.3 24.3 24.5 25.0 24.5
Numerical Commonsense 59.5 59.2 59.0 59.0 59.2 59.4
Strings 60.7 60.7 61.1 61.2 60.7 60.7

Table 4: Classification accuracy results achieved by different models as a function of the reasoning skill and few-shot
prompting method employed. The best accuracy obtained for each reasoning skill is highlighted in bold. The cells
are shaded with colors ranging from green to white to indicate their position in the accuracy spectrum. Reasoning
skills with smaller variance in achieved results are assigned a lighter shade of green to convey the extent of similarity
between models. The first block highlights skills where the finetuned models perform notably better than the vanilla
OPT. The second block emphasizes the skills where OPT-RE outperforms other models. In contrast, the third block
showcases the skills where OPT outperforms the other models. Lastly, the fourth block identifies skills where the
choice of model or prompting method has little impact on the overall performance.

Explanations’ Effect One of the central ques-
tions that we sought to investigate in this study
is the extent to which explanations play a role in
improving the reasoning capabilities of OPT mod-
els during finetuning and prompting. The results
presented in Table 5 suggest that the presence or
absence of explanations in the fewshot examples
employed for prompting does not significantly im-
pact the performance of the model when the model
is finetuned on reasoning datasets. Concretely, in
Table 5, we present the variance of the absolute
accuracy difference for each model across reason-

ing skills by excluding the Temporal skill, which
was identified as an outlier. Specifically, we com-
pute the difference between the two corresponding
columns for each model in Table 4. These values
provide insights into the impact of including ex-
planations during prompting on the performance
of the models. Our findings reveal that the differ-
ence is negligible for OPT-R and OPT-RE models,
suggesting that the choice of prompting method
does not significantly affect the model’s accuracy.
However, for the vanilla OPT model, the differ-
ence is more substantial, emphasizing the impor-

--- Page 8 ---
tance of employing explanations during fewshot
prompting. However, the mean performance of
each model across the distinct fewshot prompting
methods demonstrates a slight yet consistent in-
crease in classification accuracy, from Fewshot to
Fewshot-E (incorporating explanations), as well as
from OPT to OPT-R to OPT-RE models showing
that explanations do have a small effect on perfor-
mance during both finetuning and prompting.

Model __Std(IF-FEI) Avg(F) Avg(FE)
OPT 2.31 40.68 41.82
OPT-R 0.84 43.44 43.68
OPT-RE 0.78 44.49 44.86

Table 5: The first column shows the variance of the
absolute difference in accuracy for each model across
different reasoning skills, when using Fewshot (F) and
Fewshot-E (FE) prompting methods. The second and
third columns show the average performance of each
model across each prompting method. Results are ob-
tained after dropping the outlier Temporal skill.

5 Related Work

Reasoning LLMs LLMs have made significant
advancements in the field of NLP and related ar-
eas (Brown et al., 2020; Chowdhery et al., 2022;
Chung et al., 2022), especially with the advent of
the pre-train, prompt, and predict paradigm (Liu
et al., 2021). This paradigm has enabled these
models to solve a multitude of tasks through in-
context fewshot or zeroshot learning using instruc-
tions (Wei et al., 2021b; Iyer et al., 2022). However,
their reasoning abilities have been a subject of de-
bate in recent literature (Huang and Chang, 2022;
AlKhamissi et al., 2022). Several studies suggest
that increasing the size of an LM trained through
the same next-token prediction method can lead to
the emergence of complex behaviors (Wei et al.,
2022a), including reasoning. For instance, some re-
search has demonstrated that sufficiently large LMs
can use chain-of-thought prompting (Wei et al.,
2022b) to simulate human-like reasoning. Other
studies have shown that the addition of a simple
prompt, such as "Let’s think step-by-step" (Ko-
jima et al., 2022) can elicit reasoning abilities in
LLMs by generating explicit reasoning steps be-
fore decoding the final answer. However, some
researchers contend that emulating the human rea-
soning thought process is distinct from claiming
that the model can truly reason (Wei et al., 2022b).

Finetuned LLMs _ Concurrent studies have fine-
tuned LLMs to follow instructions to improve their
generalization ability to unseen tasks through zero
and fewshot learning (Iyer et al., 2022; Chung et al.,
2022). However, our approach differs in that we
only finetune on a selected number of open-source
datasets that provide explanations for each instance.
This enables us to focus on the importance of expla-
nations during finetuning in the context of reason-
ing skills. While concurrent works, such as (Iyer
et al., 2022; Wang et al., 2022), have experimented
with different prompting methods during finetuning
and inference, our study focuses primarily on eval-
uating the reasoning ability of the finetuned models
across a set of reasoning skills. Other concurrent
studies have explored the impact of finetuning on a
set of held-out reasoning tasks (Yu et al., 2022), but
their evaluation approach, which involves generat-
ing answers, may be influenced by various factors
such as decoding strategy, decoding parameters,
and prompt templates. In contrast, we adopt a rank
classification approach similar to (Brown et al.,
2020), which better captures the reasoning perfor-
mance of the model being evaluated, in addition to
covering a larger number of reasoning skills and
tasks.

6 Conclusion

In this study, we investigated the impact of incorpo-
rating explanations during finetuning and prompt-
ing on three different sizes of the OPT model.
Through a systematic and comprehensive evalu-
ation process that considered three key dimensions,
we found that while explanations did provide a
small improvement in performance, the effect was
not significant when incorporated in the in-context
demonstrations during inference for the finetuned
models. Additionally, our results showed that both
finetuned models exhibited significant improve-
ments in reasoning skills such as Numerical, Ana-
logical and Reasoning on Objects. Moreover, we
demonstrated that skills such as Physical, Count-
ing, and Textual Entailment benefited from incorpo-
rating explanations during the finetuning process.
Overall, our findings provide insights into the im-
pact of incorporating explanations on the reason-
ing capabilities of LLMs and offer guidance on
which reasoning skills would benefit most from
the inclusion and exclusion of explanations during
finetuning and prompting.

--- Page 9 ---
Limitations

While our study provides valuable insights into
the impact of finetuning on reasoning performance
and the role of explanations during finetuning and
prompting with respect to various reasoning skills,
there are several limitations to our work. Firstly,
we only consider a single LLM, OPT, as our base
model. Our results may not generalize to other
LLMs with different architectures or pretraining
objectives. Secondly, we only use a limited set of
reasoning datasets for finetuning due to the limited
availability of open-source datasets with explana-
tions. However, it is possible that our findings
may not hold for models finetuned on larger closed
datasets as usually seen in real-world scenarios.
Thirdly, our experiments only cover a limited range
of model sizes due to limitations in computational
budget, therefore it is possible that our findings may
not hold for much larger models. Finally, we only
consider finetuning using fewshot prompting condi-
ions in our experiments, and it is possible that our
findings may not hold for models finetuned with-
out in-context exemplars. Overall, while our study
provides valuable insights into the impact of fine-
uning and explanations on reasoning performance,
further research is needed to investigate these fac-
ors across a broader range of models, datasets, and
finetuning strategies.

Ethics Statement

This work is based on analyzing and evaluating
he performance of LLMs on reasoning tasks using
existing public datasets. No personally identifiable
information or sensitive data was collected or used
in this research. We acknowledge the potential
risks of developing LLMs, including their potential
impact on spreading misinformation, generating
unwanted content and the exacerbation of existing
biases in datasets. Our work aims to contribute to
improving the transparency and understanding of
how LLMs can be optimized for specific reasoning
skills. We hope our findings will inspire further
research on developing ethical and responsible ap-
proaches for developing and deploying LLMs.

References

Shourya Aggarwal, Divyanshu Mandowara, Vishwa-
jeet Agrawal, Dinesh Khandelwal, Parag Singla, and
Dinesh Garg. 2021. Explanations for Common-
senseQA: New Dataset and Models. In Proceedings
of the 59th Annual Meeting of the Association for

Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 3050-3065, Online.
Association for Computational Linguistics.

Badr AlKhamissi, Millicent Li, Asli Celikyilmaz,
Mona T. Diab, and Marjan Ghazvininejad. 2022.
A review on language models as knowledge bases.
ArXiv, abs/2204.06031.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877-1901.

Oana-Maria Camburu, Tim Rocktischel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language expla-
nations. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-
itors, Advances in Neural Information Processing
Systems 31, pages 9539-9549. Curran Associates,
Inc.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416.

Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.
Transformers as soft reasoners over language. In
IJCAI.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Jacob Hilton, Reiichiro Nakano, Christopher Hesse,
and John Schulman. 2021. Training verifiers to solve
math word problems. ArXiv, abs/2110.14168.

Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics, 9:346-
361.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. NeurIPS.

--- Page 10 ---
Jie Huang and Kevin Chen-Chuan Chang. 2022. To-
wards reasoning in large language models: A survey.
ArXiv, abs/2212.10403.

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shus-
ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.
2022. Opt-iml: Scaling language model instruc-
tion meta learning through the lens of generalization.
arXiv preprint arXiv:2212.12017.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-

guage models are zero-shot reasoners. arXiv preprint
arXiv:2205.11916.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158-167, Vancouver,
Canada. Association for Computational Linguistics.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys, 55:1 — 35.

Nazneen Rajani, Bryan McCann, Caiming Xiong, and
Richard Socher. 2019. Explain yourself! leveraging
language models for commonsense reasoning. In
ACL.

Siva Reddy, Danqi Chen, and Christopher D. Manning.
2019. CoQA: A conversational question answering
challenge. Transactions of the Association for Com-
putational Linguistics, 7:249-266.

Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.
ProofWriter: Generating implications, proofs, and
abductive statements over natural language. In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021, pages 3621-3634, Online.
Association for Computational Linguistics.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github. com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. ArXiv,
abs/2302.13971.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
Anjana Arunkumar, David Stap, Eshaan Pathak,

Giannis Karamanolakis, Haizhi Lai, Ishan Puro-
hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,
Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,
Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,
Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,
Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
Shen. 2022. Super-NaturalInstructions: Generaliza-
tion via declarative instructions on 1600+ NLP tasks.
In Proceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing, pages
5085-5109, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021a. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021b. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol
Vinyals, Percy Liang, Jeff Dean, and William Fedus.
2022a. Emergent abilities of large language models.
ArXiv, abs/2206.07682.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903.

Peter West, Chandra Bhagavatula, Jack Hessel, Jena D.
Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi. 2022. Symbolic
knowledge distillation: from general language mod-
els to commonsense models.

Ping Yu, Tianlu Wang, O. Yu. Golovneva, Badr
AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi
Ghosh, Mona Diab, and Asli Celikyilmaz. 2022.
Alert: Adapting language models to reasoning tasks.
ArXiv, abs/2212.08286.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068.

--- Page 11 ---
Dataset Task Definition Options
You are given an algebraic word question. Questions in this task often -A
requires executing a series of arithmetic operations to obtain a final answer. -B
AQuA You are also given 5 answer options (associated with ’A’, ’B, °C’, ’D’, ’E’). -C
Do not generate anything else apart from one of the following characters: -D
"A", "B","C","D", "E" and the corresponding explanation. -E

CoQA You are given a passage that contains a conversation and a question. The
task is to answer the question and provide an explanation that highlights
the corresponding evidence in the passage.

You are given a passage that contains a sentence and a question. The task

CoS-E : : : : . :

is to answer the question by selecting one of the provided choices.
ECOA You are given a question that requires commonsense reasoning. The task

is to answer the question by selecting one of the provided choices.

You will be presented with a premise and a hypothesis sentence. The
ESNLI task is to determine whether the hypothesis sentence entails (implies),

contradicts (opposes), or is neutral with respect to the given premise
sentence. Please answer with "Contradiction", "Neutral" or "Entailment".

GSM8K You will be presented with a passage that contains a grade school math
word problem. The task is to answer the question by performing a series
of arithmetic operations to obtain a final answer.

You are given a sequence of facts and rules followed by a question. The

Proof Writer . . . ; :
task is to answer the question using "Yes", "No" or "Unknown".

You are given a sentence and a question. The required reasoning steps are
implicit in the question. The task is to answer the question using "Yes" or
"No" then provide a strategy that explains the answer by decomposing it
into a number of steps.

StrategyQA

Free-form text

Select one of

the provided choices

Select one of
the provided choices

-Contradiction
-Neutral
-Entailment

Number

-Yes
-No
-Unknown

-Yes

Table 6: Task definition and options used for each of the finetuning reasoning datasets.

A Finetuning Task Definition and Options

Table 6 shows the task definition and options pro-
vided as input to the template shown in Figure 2
during finetuning the OPT models on the reasoning
datasets.

