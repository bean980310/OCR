arXiv:2305.10425v1 [cs.CL] 17 May 2023
SLIC-HF: Sequence Likelihood Calibration with
Human Feedback
Yao Zhaot
yaozhaoyz@google.com
Misha Khalman†
khalman@google.com
Rishabh Joshit
Tianqi Liu*
tianqiliu@google.com
Peter J. Liu+
peterjliu@google.com
rishabhjoshi@google.com
Mohammad Saleht
msaleh@google.com
Google Deepmind*, Google Research*
Abstract
Learning from human feedback has been shown to be effective at aligning language
models with human preferences. Past work has often relied on Reinforcement
Learning from Human Feedback (RLHF), which optimizes the language model
using reward scores assigned from a reward model trained on human preference
data. In this work we show how the recently introduced Sequence Likelihood
Calibration (SLIC), can also be used to effectively learn from human preferences
(SLIC-HF). Furthermore, we demonstrate this can be done with human feedback
data collected for a different model, similar to off-policy, offline RL data. Au-
tomatic and human evaluation experiments on the TL;DR summarization task
show that SLIC-HF significantly improves supervised fine-tuning (SFT) baselines.
Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF imple-
mentation used in past work while being much simpler to implement, easier to tune
and more computationally efficient in practice.
1 Introduction
While massively scaling model parameters and training compute of Transformer-based language
models have led to impressive few-shot in-context learning [5, 6], reinforcement learning from human
feedback fine-tuning (RLHF) can significantly improve generation quality as judged by humans. This
has been observed at all model scales for various downstream language generation tasks, such as
abstractive summarization, dialogue, and creative writing [18, 3, 7, 13].
For summarization in particular, multiple studies have shown that summaries generated by models
tuned with RLHF are preferred over the reference summaries in commonly used datasets [18, 10, 8].
Reference summaries are often mined from web documents and might not have the highest quality or
preferred style. As a result, pure supervised learning, i.e. maximizing the likelihood of reference
summaries given documents, is limited by the quality of reference summaries, thus additional
feedback can improve models beyond the references. Commonly used reference-based metrics,
such as ROUGE [11], only measure similarity between model generated and reference texts. These
reference-based metrics cannot measure quality improvement beyond the reference summaries.
To implement RLHF, a reward model, rø (x, y), is trained on human preference data, (x, yo, y₁, i) -
DHF, collected via side-by-side human evaluation, where raters are asked to judge which of the two
summaries yo and y₁ is better for document x, i.e. i = {0, 1}. If we denote the preferred summary
as y and the other y¯, the human feedback becomes (x, y+, ,y). DHF. One common option
Preprint. Under review.
~
1
~
for the training loss of the reward model used by RLHF is:
= =
(1)
loss (ro)
-E(x,y+,y)~DHF [log(σ (rø(x, y±) — rø(x, y¯¯))]
Reinforcement learning algorithms such as PPO [17] are then used to refine a supervised fine-tuned
model (SFT) to maximize the expected reward assigned by the reward model rø(x, y) [22, 18]. A
KL-penalty term is typically added to the loss to prevent the RLHF model from diverging too far
from the original supervised policy.
However, algorithms such as RLHF-PPO introduce significant complexity to the training process by
adding separate value and reward networks that may be comparable in size to the policy network.
They are typically kept in memory to maximize training speed, which for a given memory budget
significantly reduces the maximum size of trainable model. Furthermore the optimization steps are
significantly slower due to the use of roll-outs in the training loop, which involves sampling/decoding
from the model. Hyper-parameter tuning and co-coordinating the PPO process is also more complex,
requiring niche expertise.
Recently, another class of sequence-level contrastive methods [12, 21] seek to align model likelihood
with an arbitrary, possibly non-differentiable reward, presenting an alternative to RL for optimizing
the expected reward of samples. Zhao et al. [21] proposed Sequence Likelihood Calibration (SLiC)
to align a language model's sequence likelihood, po(y|x), over decoded sequences according to their
similarity to reference sequences. The ranking calibration loss contrasts a positive sequence y+
and a negative sequence y¯, encouraging the model Po to assign more probability mass to positive
compared to negative sequences:
Lcal (0) = max(0, ẞ – log P₁(y+|x) + log Pe(y¯¯|x))
(2)
While the original SLiC work used similarity to references as criteria for ranking, e.g. ROUGE [11]
and model embedding distances, it can be replaced by an arbitrary, reference-less ranking function,
R(yo, y1,x) {0, 1}. Particularly in this work, we use human preference as the ranking function,
either by using off-policy preference data D directly, or by training a predictive ranking model
Ro(yo, y1, x) from D.
We call using SLiC with this human preference ranking function SLiC-HF and apply it using the
human feedback data collected in Stiennon et al. [18]. Our experiments show that SLiC-HF also leads
to improved summarization quality on the Reddit TL;DR Summarization task as judged by humans,
even though this feedback was collected for different models, similar to off-policy, offline RL. While
our T5-Large [15] (770M parameter) SFT model performs similarly to Stiennon et al. [18]'s 6B
decoder-only SFT model, we are able to improve our model with SLiC-HF such that it performs
at least as well as Stiennon et al. [18]'s 6B RLHF-PPO model as judged by humans. Furthermore,
applying SLIC-HF to the T5-XXL 11B parameter SFT model [15] significantly improves results.
The primary contributions of this paper are showing:
2
• how to apply SLiC to learn from human preferences (SLiC-HF), a simpler, more efficient
yet competitive alternative to RLHF
•
•
feedback/preference data from another model (off-policy) can be effectively leveraged by
SLIC-HF, making it unnecessary to collect costly new feedback data for our model
providing a general SLiC-HF recipe based on open-sourced T5 model that outperforms
RLHF on the Reddit TL;DR summarization task
Method
In this work, we apply SLiC [21] to improve a SFT model using human preference data (x, y+, y¯) ~
Dƒà in addition to the standard supervised fine-tuning data (X, Yref) ~ DSFT.
2.1 Sequence Likelihood Calibration
Following Zhao et al. [21], we first fine-tune a supervised model, Poft (y|x), on (X, Yref) ~ DSFT,
and then align the SFT model's sequence likelihood using the SLiC approach which optimizes the
following loss:
Greg
L(0) = Lcal (0, X, Yref, {ŷ}m) + \Ľ™eg (0,0ƒt; X, Yref)
(3)
2
where 0 and 0 ft are the current and fixed SFT model weights, Lcal and Leg are the calibration and
regularization losses and {ŷ}m are m sampled candidates from the SFT model. More specifically, we
choose the rank calibration loss and cross-entropy regularization loss for their simplicity and natural
fit to pairwise human feedback data. Thus the loss function of SLiC-HF becomes the following:
L(0) = max(0, 8 — log P₁(y+|x) + log P₁(y¯¯|x)) — \ log Pe(yref|×)
_
-
(4)
The first term is the calibration loss where x is the input sequence, y+ and y¯ are positive and
negative sequences, and § is a hyper-parameter for the margin of the ranking loss. The second term
is the cross-entropy loss, where yref is some target sequence and \ is the regularization weight.
Cross-entropy loss encourages the model to stay close to the SFT model, similar to the KL term
in used in Stiennon et al. [18], however it does not need an extra copy of SFT weights. The KL
regularization term was also explored in Zhao et al. [21] but found to perform similarly. The choices
of y and y are discussed in subsections 2.2 and 2.3. The choices of regularization target yref is
discussed in subsection 2.4.
2.2 SLIC-HF with Sample and Rank
~
Zhao et al. [21] samples candidates {y}m Poft (yx) from DSFT's training split, from which
(positive, negative) pairs are determined. We call this approach SLiC-HF-sample-rank. To determine
the rank, we consider two text-to-text models trained from the human preference data DHF:
Trained Pointwise Reward model: Similar to Askell et al. [2], we binarize each ranked pair into
a positive and a negative sequence, as shown in Figure 1. When training the reward model, input
sequences are formatted as '[Context] ... [Summary] ...' and target sequences are either 'Good' or
'Bad'. At inference time, we compute the probability of token 'Good' on the decoder side to score
each of the m candidates in a list, and sample m positive/negative pairs from them.
Trained Pairwise Ranking model: As shown in Figure 1, we formulate the human feedback
into a pairwise ranking problem with text-to-text format. When training the ranking model, input
sequences are formatted as ‘[Context] ... [Summary A] ... [Summary B]' and target sequences are
among 'A' or 'B'. At inference time, we use a tournament-style procedure to rank candidates in a list.
For example, given a list of 4 candidates C1, C2, C3, C4, we first rank C1, C2 and C3, C4 and then rank
winner (c1, c2), winner (C3, C4). Given m candidates, the ranking model is called m 1 times and
m - 1 positive/negative pairs are yielded.
Reward Model
[CONTEXT] document [SUMMARY] positive summary → Good
[CONTEXT] document [SUMMARY] negative summary → Bad
Ranking Model
[CONTEXT] document [SUMMARY A] positive summary [SUMMARY B] negative summary → A
[CONTEXT] document [SUMMARY A] negative summary [SUMMARY B] positive summary → B
Figure 1: Training text-to-text reward model and ranking model.
2.3 SLIC-HF Directly On Human Feedback
We also consider a straight-forward approach of directly calibrating on positive and negative sequences
from the human feedback dataset, DHF, without a ranking or reward model. We call this approach
SLIC-HF-direct. The obvious advantage of this approach is increased simplicity and efficiency from
not training or using a ranking/reward model. SLiC-HF-direct does not incur additional engineering
costs in decoding from the SFT model and training a model to label the decodes. The drawback is
that the off-policy human feedback data distribution might differ much from the SFT model's decode
distribution.
2.4 Regularization Term for Calibration
We consider two choices of target sequence yref for cross-entropy regularization. The first choice is
using yref in DSFT as regularization target. The second choice is using the best ranked candidate
3
from {ŷ}m as the regularization target. Best ranked candidates can be selected using either the
ranking model or the reward model.
3 Experimental Results
3.1 Datasets
We study SLIC-HF on Reddit TL;DR summarization datasets from Stiennon et al. [18]. The dataset
contains both fine-tune data DsFt, human feedback data DHF, along with their SFT and RLHF
model decodes which we use for comparison with our models. DSFT is a filtered version of Reddit
TL;DR dataset [19]. It contains 117k/6k/6k examples in train, validation and test splits. DHF consists
of 64k human preferences on decodes from multiple models.
3.2 Experimental Hyper-parameters
We conduct all experiments using T5 models [15] in the T5x framework [16]. In our ablation study,
we choose a T5-large model (770M) as the generation model and T5-XXL (11B) as the ranking model
and the reward model¹. We train all generation models with batch size of 32 and ranking/reward
models with batch size of 128. Both are trained with default learning rate of 10-³.
We train the ranking model and the reward model on DƒF training split, and picked checkpoints that
have the highest accuracy on DHF validation split. We fine-tune T5 models on DSFt training split,
and pick checkpoints that have the lowest perplexity on DSFT validation split.
In calibration, we use learning rate of 10-5 and ranking margin ẞ of 1.0. When calibrating models
on their own decodes with SLiC-HF-sample-rank, we sample 8 decodes with temperature of 0.7 and
topk of 40 from fine-tuned only generation models.
When evaluating our models, we use beam-search with beam size 4. For automatic evaluation, we
calculate the model decodes' win rate against human references measured by the T5-XXL ranking
model on DSFT validation dataset. Win rate is defined as the percentage of model decoded summaries
preferred by the ranking model compared to human references.
3.3 Reward Model and Ranking Model Accuracy
Human feedback and human evaluation are done by raters comparing two summaries as it is more
reliable than pointwise rating. We hypothesize that ranking model has an advantage over reward
model because of its pairwise nature which aligns better with the task. We train and compare a
T5-XXL ranking model and a T5-XXL reward model (subsection 2.2). Results shows that our ranking
model has accuracy of 73.23% on DHF validation, about 2% higher than our reward model which
has accuracy of 71.34%².
3.4 SLIC Ablation
We conduct a set of experiments to ablate SLiC-HF settings against baselines. We use the ranking
model as the main metric because of its higher correlation with human preferences demonstrated
in Stiennon et al. [18]. Selected settings are later verified with our human evaluation experiments
in subsection 3.5. We report ROUGE numbers just for reference purpose and do not use them to
select models. It is expected to see a drop in ROUGE numbers when learning from human feedback
because it has less incentive to be similar to the reference texts. Similar to RLHF in Stiennon et al.
[18], we also observe an increase in average length of models and conduct a length controlled study
in subsection 3.5.
3.4.1
SLIC-HF vs Continue Fine-tuning on Filtered Data
A simple way to learn from human feedback data is to convert it into SFT dataset and continue
fine-tuning on it. In general, we use the filtering approach which has similar performance to controlled
'We find that smaller T5 ranking/reward models do not converge reliably in our setup.
2Our ranking and reward models' accuracy are similar to the 6B reward model in Stiennon et al. [18]
4
Table 1: Compare different methods to leverage human feedback data. Ranker win rate is the
T5-XXL ranking model's preference of choosing model decodes over reference texts.
Ablation
method human feedback form
reference
SFT
continue SFT on filtered data
positives from HF data
Metrics
regularization #words R1/R2/RL
ranker win rate
27.11
50%
23.57
35.1/12.87/26.81
44.96%
31.22 33.02/11.27/24.57
51.65%
SLIC-HF
best decodes, by reward
best decodes, by ranking
SLIC-HF-direct
27.69 35.31/12.41/26.21
63.24%
28.26 35.39/12.69/26.56
65.43%
SLiC-HF-sample-rank, by reward
SLIC-HF-sample-rank, by reward
SLIC-HF-sample-rank, by ranking
SLIC-HF-sample-rank, by ranking
SFT targets
SFT targets
best decodes
SFT targets
best decodes
41.03
33.76/11.58/24.72
82.92%
38.44
33.87/11.48/24.81
82.42%
38.58
34.07/11.59/24.92
83.52%
37.96
34.49/11.92/25.35
86.21%
37.50
34.69/12.03/25.54
85.51%
generation approaches [1] but is cleaner to implement. We consider three approaches to filter data for
continued fine-tuning:
•
keep only positive human feedback sequences and discard negative ones.
• decode 8 summaries from the SFT model, use the ranking model to select the best 1 out of 8
summaries by a tournament-style ranking approach.
• decode 8 summaries from the SFT model, use the reward model to select the best 1 out of 8
summaries by scoring each and taking the one with the max score.
As shown in Table 1, on Reddit TL;DR dataset, continue fine-tune on positive human feedback data
improves model win rate against reference slightly from 44.96% to 51.65%. In this experiment, we
choose to use all human feedback without filtering for better models because this mimics a real world
scenario where we have access to some human feedback data without the explicit knowledge of its
quality. Continuing fine-tuning on best 1 out of 8 further improves win rate against reference to 60%+
and using pairwise ranking model is slightly better than pointwise reward model for filtering.
3.4.2 Apply SLiC-HF Directly On Human Feedback Data
With SLIC-HF-direct, we observed that even though calibration loss decreases as expected, sequence
length keeps increasing and does not converge to a stable value. On the other hand, SLIC-HF-sample-
rank robustly converges. We hypothesize that SLiC-HF-direct is prune to out-of-distribution decodes
generated by other models in the human feedback data.
When using the ranking model to select for the best checkpoint for SLiC-HF-direct, it has moderate
length increment and has 82.92% win rate against reference which is close to SLiC-HF-sample-rank.
The engineering complexity of SLiC-HF-direct is almost the same as fine-tuning a model. Therefore,
it is a good candidate for quick experimentation on human feedback.
3.4.3 Apply SLiC-HF on Ranked Model Decodes
As shown in Table 1, SLiC-HF-sample-rank using the ranking model have about 3% gain in win rate
against reference compared to SLiC-HF-sample-rank using the reward model. This results aligns with
the observation in subsection 3.3 that the ranking model has higher agreement to human preference
than the reward model.
For SLIC-HF-sample-rank using the ranking or the reward model, using SFT targets or best ranked
decodes as regularization doesn't show much difference. This shows that SLiC-HF-sample-rank is
applicable even when there is no ground truth reference available. The gain from continue fine-tuning
on best ranked decodes in Table 1 is not additive to SLiC-HF.
5
3.5 Human Evaluation
We conduct side-by-side human evaluation between multiple systems using crowd-sourcing.³ Given a
document and 2-4 summaries, raters are tasked to assign a pointwise overall quality to each summary,
select if the summary is factual or not, and choose the best summary.
Each task is replicated and judged by 3 different raters. To eliminate bias, we anonymize all the
models and randomly shuffle order of summaries for each task. We aggregate pointwise metrics by
averaging the ratings across all 3 crowd workers, and we aggregate the choice metric using majority
vote.
The human evaluation template and the rating instructions can be found in Appendix A.
3.5.1
SLIC-HF Ablation Study
We conduct a 4-way side-by-side human evaluation to confirm the ablation results in Table 1. 100
examples from the validation set are sampled from reference, SFT model, continue fine-tuning model
and SLIC-HF model (SLiC-HF-sample-rank, using ranking model, regularized on best decodes). As
shown in Table 3, SLiC-HF is chosen as the best model 73% of the time, has significantly higher
average quality, and is the most factual model. In general, the average quality aligns well with the
ranker win-rate from Table 1.
Figure 2 shows the lengths controlled quality of SFT, continue fine-tuning and SLIC-HF models,
which clearly shows SLiC-HF is preferred. Length controlled quality study is similar to studies
conducted in Stiennon et al. [18], where mean scores are calculated among examples bucketed by
their relative length to the reference.
Table 2: 4-way human evaluation to compare reference, SFT continue SFT on best decodes using
ranking model, SLiC-HF with pairs of decodes using ranking model.
chosen as preferred % 13%
reference SFT
5%
continue SFT
5%
average quality
is factual %
3.17
94.16%
3.10
3.32
SLiC-HF
73%
3.82
same
4%
94.85%
94.85%
96.56%
4.2
SFT
4.0
SLIC-HF
3.8
3.6
Fraction preferred to reference
3.4
3.0
Continue SFT
2.8
-0.6
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
log(summary len / reference len)
Figure 2: Length bucketed average quality of SFT and SLiC-HF against different baselines.
3.5.2 SLIC-HF vs RLHF-PPO
Correctly implementing and tuning the right hyper-parameters for the RLHF-PPO algorithms in
Stiennon et al. [18] are non-trivial tasks. Instead of re-implementing the algorithms in our framework,
we directly compare with the model decodes from Stiennon et al. [18].
3 We use Amazon Mechanical Turk to set up the task and hire the raters
Fractions of preferred to reference SFT
We first benchmark our T5-large SFT model against their 6B decoder-only SFT model in a two-way
side-by-side human evaluation. As shown in Figure 3, our SFT has slightly higher quality and win
rate but it is not statistically significant.
Next we benchmark two variants of our T5-large SLiC-HF-sample-rank models against the decoder-
only 6B RLHF-PPO model from Stiennon et al. [18]. SLiC-HF-sample-rank with reward model has
similar performance as RLHF-PPO and SLIC-HF-sample-rank with ranking model is better than the
RLHF-PPO. The summaries from SLiC-HF models are slightly longer than the RLHF-PPO model,
and their length controlled win rate is similar to RLHF-PPO as shown in Figure 3.
Table 3: Three 2-way side-by-side human evaluations to compare our SFT baseline with [18], and
our SLIC-HF models with the RLHF-PPO model. Statistically significant results are denoted with *.
systems comparisons
system A (ours)
method
system B([18])
human preference
win rate
quality
# words
SFT (770M gen)
SLIC-HF (700M gen, 11B ranking)
SLIC-HF (700M gen, 11B reward)
23.7
SFT (sup6B)
36.9
RLHF (sup6B_rm6B)
# words
24.6
33.0
A
B
A
B
38.4
RLHF (supбB_rm6B)
33.0
56% 44% 3.59 3.48
66%* 34%* 3.85* 3.61*
56% 44% 3.78 3.7
✓ SFT
Fractions of preferred to RLHF
0.3
SLIC-HF Ranking
SLIC-HF Reward
0.2
-0.4
-0.2
0.0
0.2
0.4
-0.4
-0.2
0.2
0.4
log(summary length / reference SFT length)
log(summary length / RLHF-summary length)
Figure 3: Length bucketed average quality of SFT and SLiC-HF against different baselines.
3.6 Scaling Up SLIC
Table 4: Effect of scaling up model parameters and number of candidates for SLiC-HF-sample-rank.
Ablation
method
SFT
SFT
# params m
770M
11B
8
# words
23.57
8
24.07
Metrics
R1/R2/RL
35.1/12.87/26.81
36.45/14.11/28.38
ranker win rate
44.96%
62.34%
SLIC-HF 770M
SLIC-HF 770M
SLIC-HF 11B
8 37.96 34.49/11.92/25.35
86.21%
64 40.53 34.14/11.70/25.11
8 36.90 35.83/12.87/26.63
86.41%
96.10%
We study 2 ways of scaling up the SLiC-HF-sample-rank: (1) scaling up generation model parameters,
(2) scaling up number of decoded candidates m. As shown in Table 4, scaling up generation model
from 770M to 11B significantly improves both the SFT model and the SLiC-HF model. On the other
hand, scaling up m from 8 to 64 does not help much.
4 Further discussion on SLiC-HF vs. RLHF-PPO
4.1 Compute/Memory Efficiency and Parallelism
We summarize the compute and memory efficiency differences between SLiC-HF and RLHF-PPO in
Table 5.
Table 5: Compute and memory efficiency comparison. p denotes the number of parameters in the
policy network;
Auxiliary models
Decoded sequences
Parameter memory usage for training
Parameter updates per step
Parallel decoding
Parallel reward
Input encoding caching
RLHF-PPO [18]
SLIC-HF
decode-rank
direct
reward, value, SFT
1M
ranking
800k
4p
P
Р
2p
Р
Ρ
within batch
whole training set
within batch
no
whole training set
yes
In both RLHF-PPO and SLiC-HF-sample-rank we train an auxiliary ranking or reward model that
is used to judge the quality of summaries. However, Stiennon et al. [18] found that having separate
policy and value networks worked significantly better, and thus contributes an extra auxiliary model,
the same size as the reward model that is updated along-side policy updates.
Furthermore, the policy, value, reward, and SFT models (all the same size in Stiennon et al. [18]) are
used within the PPO training loop. They are often held in hardware memory to ensure faster training
steps. Whereas in SLiC-HF, the rewards can be computed completely in parallel and offline, thus
using 1/4 the memory for model weights during training. Such memory savings could be re-purposed
to train larger models.
=
Stiennon et al. [18] report using 1M episodes to conduct RLHF training, which corresponds to
roughly the same number of decoded samples used in SLiC-HF, (m 8 per training example,
123,169 examples). However, in practice SLiC-HF decoding can be significantly faster because all
the decoded samples use the same policy allowing for completely parallel decoding. In contrast, with
PPO the policy is updated every batch, limiting decoding parallelism to each batch (512, in [18]) as
subsequent decoding is blocked on policy updates. Furthermore, PPO decoding occurs within the
training loop leading to much longer optimization step times. Whereas with SLiC-HF step times are
similar to fine-tuning, which is significantly faster as there is no decoding in the training loop.
Beyond the significant decoding parallelism gains, SLiC-HF can make use of simple input encoding
caching optimizations to reduce compute. Since the m decodes are sampled from the same SFT
policy, the input sequence encoded states can be cached rather than recomputed. In summarization
and other tasks involving long contexts, this may be significant as input sequence length tends to be
much longer than output.
SLIC-HF has similar parallelism advantages in computing rewards per episode compared to RLHF as
the ranking can be computed outside the training loop instead of within.
4.2 Pairwise Ranking vs Reward model
RL algorithms seek to maximize the expected reward of trajectories, in this case the human judgement
in quality of model summaries. This reward function typically is assumed to be pointwise, whereas
human preference data is collected pairwise to improve reliability. Thus there is noise introduced in
converting pairwise judgements into pointwise rewards, which can be estimated as the difference
in ranking accuracy as in subsection 3.3. Since SLiC-HF only cares about the relative rank of
two summaries, this pairwise-to-pointwise noise is avoided and we conjecture this helps SLiC-HF
(Table 1, Figure 3).
4.3 The Value of States and Actions in Language
For many tasks tackled using RL, rewards may be collected at the end of a trajectory (as in many
Atari games) and the attribution of final reward to specific actions may be very important in learning
to solve a task. Typically when RL is applied to language as in the RLHF literature, the state is the
prefix of the current text and the actions correspond to choosing the next token. The value function's
role is to estimate the goodness of a trajectory (e.g. summary) from a prefix/input, which is intuitively
a very difficult task for human raters, and thus RL may also suffer from value function estimation
8
noise. In contrast, SLIC-HF does not rely on such a sub-model and only uses the cleaner preference
signal to drive parameter updates and leading to what we conjecture is more stable optimization.
5
Related work
RL has been used to optimize arbitrary reward in language generation such as BLEU for translation
[20] and ROUGE for summarization [14]; however, while those metrics improved, human judgement
of quality suffered due to metrics misalignment.
In an effort to better align the reward function with human judgement, many works used RL to
align language models with a reward model trained to predict carefully collected human judgements
[22, 18, 13] using summarization as an initial proof-of-concept. A KL penalty term, first used in
Jaques et al. [9], is used as regularization to prevent the tuned model from departing from the initial
supervised model, and is also used in SLIC [21].
Liu et al. [12] propose BRIO, which has a similar intent as SLiC [21] of rank-ordering model-
generated decodes according to a reward function. BRIO trains models to align length normalized
sequence probability of generated decodes to their similarity to reference as measured by ROUGE
using a list-wise loss function. In contrast, and similar to RLHF, SLiC-HF adapts the technique
to align with a model trained to predict human preference given two summaries instead of their
similarity to the reference.
Bai et al. [4] substitutes human preference data with judgements from a large language model, and
calls it AI feedback (AIF). SLIC-HF can also be used with AIF exactly in the same way and is
indifferent about the AI or human origin of the feedback.
6 Conclusion
In this work, we proposed SLiC-HF that calibrates sequence likelihood on human feedback data. Our
experiments on the Reddit TL;DR summarization task show that SLiC-HF significantly improves
supervised fine-tuning (SFT) baselines, and presents a competitive alternative to the RLHF-PPO
implementation of past work while being simpler to implement, easier to tune and computationally
efficient. Future work may include studying SLiC-HF on other language generation tasks using other
reward functions and/or non-human feedback.
References
[1] Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, and Mirella
Lapata. 2022. mface: Multilingual summarization with factual consistency evaluation.
[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds,
Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom
Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. 2021. A general language
assistant as a laboratory for alignment.
[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.
[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in
Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.
9
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm:
Scaling language modeling with pathways.
[7] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds,
Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving
alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.
[8] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in
the era of gpt-3. arXiv preprint arXiv:2209.12356.
[9] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E
Turner, and Douglas Eck. 2017. Sequence tutor: Conservative fine-tuning of sequence generation
models with kl-control. In International Conference on Machine Learning, pages 1645–1654.
PMLR.
[10] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of
language models. arXiv preprint arXiv:2211.09110.
[11] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text
summarization branches out, pages 74–81.
[12] Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. BRIO: Bringing order
to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 2890–2903, Dublin, Ireland.
Association for Computational Linguistics.
[13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language
models to follow instructions with human feedback. Advances in Neural Information Processing
Systems, 35:27730-27744.
[14] Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for
abstractive summarization. arXiv preprint arXiv:1705.04304.
[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a
unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.
[16] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel
Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor
Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini
Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis
Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan
Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan
Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022.
Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.
[17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
10
[18] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human
feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021.
Curran Associates, Inc.
[19] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR: Mining
Reddit to learn automatic summarization. In Proceedings of the Workshop on New Fron-
tiers in Summarization, pages 59–63, Copenhagen, Denmark. Association for Computational
Linguistics.
[20] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural
machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144.
[21] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J
Liu. 2023. Calibrating sequence likelihood improves conditional language generation. In The
Eleventh International Conference on Learning Representations.
[22] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. 2020. Fine-tuning language models from human prefer-
ences.
11
Instructions:
1. Read the document and the summaries below, and determine which better summarizes the document.
2. Determine for each summary whether everything said is factually consistent with the document (everything said can be verified in the document text).
3. Rate the summaries for quality on a scale of 1-5. (1 = Poor summary, 5 = Great summary)
4. Select the best summary.
Document:
I'll try to keep this short!
**Background**✶
* I've always been an on again/off again (very casual!) jogger, typically doing 35k
*My knees have always been finicky, and I went to a physio who thought I had "runner's knee"
* Pre-pregnancy, my "runner's knee" would flare up when I got to the 8-10 k distance range, even if I had a decent base (doing a C210k type
program)
**Current Problem**
I had my baby a year ago, so all in all I haven't run for about 1.5 years. I'm quite slim and have been doing aerobics-style classes for the past
year, so I'm not totally out of shape. Body weight exercises, aerobics, bikes and ellipticals are all fine. However, when I run even the tinyiest
bit, or even go on a long walk or a hike, my pelvis gets very sore and tight, and my knees start hurting very quickly. I already am doing general
squats/lunges/stretching type things.
I'm starting to feel like running just isn't for me anymore. Which is a bummer, because I really enjoy running!
Has anyone had something similar? Can anyone recommend some stretches or exercises that might help? Should I see a Dr? Or should I just see a
physio? Not quite sure how to proceed.
Thanks!
Summary 0:
I think pregnancy messed with my body, now I can't even run even the smallest amount
without pain in my pelvis and knees. I'm fairly certain the problem isn't just that I'm
completely out of shape.
Summary 0 is completely factual with respect to the document: O Yes No
Summary 0 Quality:
Summary 1:
My pelvis and knees hurt when I run even a tiny bit, and I'm starting to feel like running
isn't for me anymore.
Summary 1 is completely factual with respect to the document: Yes No
Summary 1 Quality:
Summary 2:
I haven't run in 1.5 years, and my pelvis and knees hurt a lot when I do. What can I do?
Summary 2 is completely factual with respect to the document: Yes No
Summary 2 Quality:
Summary 3:
I have runner's knee that flares up when I run even a tiny bit, and my pelvis and knees
hurt very quickly when I run. Is there anything I can do? Should I see a Dr? Or should I
just see a physio?
Summary 3 is completely factual with respect to the document: O Yes No
Summary 3 Quality:
Select the better summary:
○ Summary 0
Summary 1
No preference
Summary 2
○ Summary 3
Figure 4: Example of human evaluation task.
A Human Evaluation
See Figure 4 for an example of the human evaluation task with 4 summaries. Summaries are randomly
shuffled for each example and models are anonymized.
12
12
