--- Page 1 ---
arXiv:2308.16876v2 [cs.CV] 12 Dec 2023

SportsSloMo: A New Benchmark and Baselines for
Human-centric Video Frame Interpolation

Jiaben Chen*
UC San Diego

jic088@ucsd.edu

Abstract

Human-centric video frame interpolation has great po-
tential for enhancing entertainment experiences and find-
ing commercial applications in the sports analysis indus-
try, e.g., synthesizing slow-motion videos. Although there
are multiple benchmark datasets available for video frame
interpolation in the community, none of them is dedicated
to human-centric scenarios. To bridge this gap, we intro-
duce SportsSloMo, a benchmark featuring over 130K high-
resolution (>720p) slow-motion sports video clips, total-
ing over 1M video frames, sourced from YouTube. We re-
train several state-of-the-art methods on our benchmark,
and we observed a noticeable decrease in their accuracy
compared to other datasets. This highlights the difficulty
of our benchmark and suggests that it poses significant
challenges even for the best-performing methods, as hu-
man bodies are highly deformable and occlusions are fre-
quent in sports videos. To tackle these challenges, we pro-
pose human-aware loss terms, where we add auxiliary su-
pervision for human segmentation in panoptic settings and
keypoints detection. These loss terms are model-agnostic
and can be easily plugged into any video frame interpo-
lation approach. Experimental results validate the effec-
tiveness of our proposed human-aware loss terms, leading
to consistent performance improvement over existing mod-
els. The dataset and code can be found at: https://neu-
vi. github.io/SportsSlomo/.

1. Introduction

Video frame interpolation (VFI) is a technique that syn-
thesizes intermediate frames from input images, enhanc-
ing the clarity of content that may be difficult to see oth-
erwise. This technique finds wide-ranging applications, in-
cluding slow-motion video generation [27], novel view syn-
thesis [39], video compression [73], cartoon and rendered

“Work mainly done when Jiaben Chen was an intern at Northeastern
University.

Huaizu Jiang

Northeastern University

h. jiang@northeastern.edu

(a) (b) (©) (ad)
| - - -

Figure 1: Human-centric video frame interpolation re-
sults. We propose human-aware auxiliary losses to improve
interpolation accuracy at motion boundaries. From left to
right: (a): Overlayed inputs, (b): Ground truth, (c): Inter-
polation results, (d): Interpolation results with our proposed
human-aware loss terms.

content generation [37, 4], etc. In recent years, we have wit-
nessed significant advances in this field, due in large part to
the development of various benchmarks [1, 65, 57, 66, 79,
14, 64].

Humans feature prominently in most contemporary
videos. With the widespread use of mobile devices, peo-
ple can easily record and share their daily experiences with
family, friends, and colleagues. Meanwhile, live broadcasts
of sporting events attract a large audience. Automatically
generated slow-motion videos can create a more immersive
and engaging experience for users by highlighting details
and valuable moments of their lives that may be missed in
real-time. Therefore, improving video frame interpolation
results for human-centric videos has great potential for en-
hancing user experiences in entertainment. Human-centric
video frame interpolation approaches can also be beneficial
in various industries. Athletes and coaches, for example,
can use slow-motion synthesis to identify flaws in tech-
niques, highlight areas for improvement, and gain a more
detailed understanding of how different factors may con-
tribute to the success.

A new benchmark. Despite the availability of various

--- Page 2 ---
Human Keypoints

Segmentation Masks

Figure 2: Visualization of human keypoints [78] and
panoptic segmentation masks [10].

benchmarks for video frame interpolation, a notable gap ex-
ists in datasets specifically tailored for human-centric sce-
narios. To bridge this gap and to foster the research in
this important direction, we introduce SportsSloMo, a new
dataset comprising high-resolution (>720p) slow-motion
sports videos crawled from YouTube under the Common
Creative Licence. This dataset encompasses a diverse range
of sports, such as football, basketball, baseball, hockey, etc.
Since a video may contain advertisement, transition frames,
changes of shot, and non-slow-motion content, we carefully
curate the data to remove such unwanted content and finally
split each long video into a set of short slow-motion clips
of 9 frames. The first and last frames are used as input and
the rest 7 intermediate frames are reserved as ground truths
for training and evaluating VFI models. In total, our bench-
mark has 130K video clips and more than 1M video frames.
Compared with existing datasets, as shown in Table |, our
proposed SportsSloMo benchmark is the largest one so far,
with high resolution and focus on human-centric scenarios.

While primarily designed for human-centric VFI in this
paper, we believe SportsSloMo dataset may also have the
potential to aid research in other tasks such as video super-
resolution [6, 38], group activity recognition [67, 83, 81],
and dynamic view synthesis [39, 17, 75]. By releasing this
dataset to the entire community, we hope to encourage tech-
nical advancement in human-centric video frame interpola-
tion and empower researchers to explore innovative appli-
cations in other adjacent fields.

Benchmarking existing approaches. To facilitate de-
velopment and evaluation of human-centric video frame in-
terpolation methods, we re-train several state-of-the-art ap-
proaches [27, 36, 23, 25, 29, 28, 82] using their publicly
released code on our SportsSloMo dataset. As the human
bodies are highly deformable and occlusions are frequent in
sports videos, the accuracy of all the methods decrease com-
pared to their performance on other datasets. For instance,
EBME [29] and EMA-VFI [82], two of the top-performing
approaches on SportsSloMo, produce PSNR scores of 30.15
and 30.70, respectively - markedly lower than their scores

Table 1: Comparisons of different benchmark datasets
for video frame interpolation. (#inter. frames indicates
the number of intermediate frames to synthesize.)

Dataset #clips |#images #inter. resolution human-

frames centric
UCFI101 [65] |04K] 1K 1 256x256 v
Adobe240fps [66]] 0.1K | 80K 7 | 1280x720) Xx
Vimeo90K [79] | 70K | 220K 1 448 x 256 x
SNU-FILM [14] | 1.2K | 3.6K 1 1280x720] Xx
X4K1000FPS [64]] 4.4K | 286K 7 |4096x2160) Xx
SportsSloMo | 130K | 1183K 7 1280x720 v

of 36.19 and 36.64 on Viemo90K [79], as well as 30.64 and
30.94 on the hard split of the SNU-FILM [14] benchmarks.
It highlights the difficulty of our benchmark and suggests
significant challenges need to be addressed.

Enhancing models for human-centric VFI. To im-
prove the existing VFI models on our benchmark, we in-
troduce human-aware priors to enhance the model train-
ing. Specifically, we propose loss terms based on human
segmentation in the panoptic setting [10] and human key-
points estimation [78] as extra supervision for intermediate
frame synthesis. Fig. 2 shows a visualization of detected
human keypoints and segmentation masks in our dataset.
Human segmentation masks help delineate human body
boundaries, which are helpful for reducing ghost effects
around the motion boundaries. At the same time, human
eypoints estimation can also indicate where each body part
is, enforcing coherent motion trajectories in the synthesized
video frames. Specifically, we compare the output from
pre-trained panoptic segmentation and keypoints detection
models taking input as a synthesized and the ground-truth
intermediate frame, respectively, and use the consistency as
supervision. As shown in Fig. 1, by supervising with our
proposed human-aware loss terms, we improve the inter-
polation quality at motion boundaries with less blurry re-
sults in scenarios with large motion and occlusion. Both
of these human-aware loss terms are model agnostic and
can be easily integrated into any video frame interpolation
approach. Experimental results show that they can consis-
tently improve the accuracy of seven existing approaches,
leading to strong baselines on our benchmark.

To sum up, this paper makes the following contributions:
* We introduce SportsSloMo, a new benchmark dataset

consisting of a large amount of slow-motion sports
videos. To the best of our knowledge, this is the first high-
resolution dataset tailored for human-centric video frame
interpolation, supporting synthesis of multiple intermedi-
ate frames.

* We benchmark state-of-the-art approaches on the new
benchmark, highlighting the challenges of the human-

--- Page 3 ---
centric video frame interpolation task.

¢ We propose two human-aware loss terms, which take the
human priors into account for video frame interpolation
and can easily be plugged into existing video frame inter-
polation approaches. Experimental results validate that
they can consistently improve existing models, yielding
strong baseline models on our new benchmark.

2. Related Work
2.1. Benchmark datasets

Existing publicly available datasets already provide a
valuable resource for developing and evaluating video
frame interpolation methods. We would briefly introduce
these datasets and reveal some limitations in this section.
Table. 1 shows a comparison between existing datasets and
our proposed SportsSloMo dataset.

The SNU-FILM [| 4] dataset is a widely-used benchmark
for VFI evaluation, containing 1240 frame triplets of 1280
x 720 resolution. And it is divided into four different parts,
namely, Easy, Medium, Hard, and Extreme according to
motion magnitude. The Middlebury benchmark [1] is an-
other widely used dataset, image resolution in this dataset
is around 640 x 480. However, it is commonly only used
to evaluate VFI methods for 8 sequences. UCF101 [65] is
originally a dataset for human action recognition, contain-
ing a variety of human actions. With the test set constructed
by [43], it is also used to evaluate VFI methods, contain-
ing 379 triplets of 256 x 256 frame size. Nevertheless,
its scale is rather small and its resolution is also low. The
Adobe240fps dataset [66], originally for video deblurring,
is another widely used dataset for VFI. It is consisted of
high frame-rate videos (240 fps) with a resolution of 1280
x 720, yet the videos are from only 118 clips.

The mostly used dataset for training and evaluating VFI
methods is the Vimeo90K dataset [79]. It contains 73171
frame triplets from 14777 video clips extracted from real-
life video clips with a fps < 30. Nevertheless, one of its
main drawback is the low resolution of 448 x 256 obtained
by downscaling the original high resolution frames. More-
over, as VFI methods have been rapidly improving in recent
years, their performance on the widely-used Vimeo90K
dataset has approached saturation. To further advance the
state-of-the-art in video frame interpolation, a new dataset
with bigger scale, higher resolution and more challenging
scenarios is necessary. X4K1000FPS is a recently released
high frame rate (1000 fps) with 4K spatial resolution to pro-
mote the study of VFI for very high resolution videos.

As a result, none of existing datasets contains rich
human-centric data with high resolution and big scale. To
bridge this gap, we introduce the SportsSloMo dataset, a
human-centric VFI dataset with 130K video clips and 1M
video frames at 240 fps with a resolution of 1280 x 720,

aiming to foster research in human-centric VFI.

2.2. Video frame interpolation methods

Existing VFI methods could be generally classified into
flow-agnostic and flow-based methods.

Flow-agnostic approaches model VFI without explicit
intermediate motion representation. Phase-based methods
[48, 47] directly predict the phase decomposition of the in-
termediate frame, but can only handle motion within a lim-
ited range. Kernel-based methods are the mainstream ap-
proach in this category, which typically aims to estimate
intermediate frames by learning adaptive kernels to con-
volve input frames [53, 54]. Over the years, numerous
improvements are proposed in this field, including using
deformable convolution [11, 12], formulating interpolated
motion estimation as classification [56], blending deep fea-
tures [19], introducing dual-frame adversarial loss [36], per-
forming channel attention [14] and utilizing 3D space-time
convolutions [32]. Recently, Shi et al. [63] introduced a
Transformer-based framework to model long-range depen-
dencies with the aid of attention mechanisms. By directly
hallucinating pixel values, these methods tend to generate
blurry results and artifacts, especially in fast-moving scenes
[44].

Flow-based approaches currently serve as a promising
direction in VFI. Generally speaking, flow-based method
takes a paradigm of a two-stages pipeline: (1) flow esti-
mation, and (2) frame synthesis. They first estimate op-
tical flow between input frames, and then synthesize in-
termediate frames using image warping [26]. As a repre-
sentative work, SuperSlomo [27] by Jiang et al. adopted
a skip-connected U-Net to estimate bi-directional optical
flows under the assumption of linear motion. Quadratic [77]
and cubic [13, 69] trajectory assumptions have also been
made to approximate intermediate motion. Recent work has
explored various techniques to improve intermediate flow
estimation and interpolation accuracy, including forward-
warping via softmax splatting [52, 23], voxel flow [43], cy-
cle consistency loss[61, 41], task-oriented flow distillation
loss [35], Gram matrix loss [60], implicit neural function
[9], occlusion mask [3], anchor points alignment [64], priv-
ileged distillation [25], and pyramid recurrent flow estima-
tion [28]. Considering additional information like contex-
tual maps [51], depth maps [2] and auxiliary visual informa-
tion from event cameras [70, 21, 8] can also further improve
interpolation accuracy. Park et al. employed symmetric
bilateral motion field estimation, and further improved in-
termediate motion estimation accuracy through asymmetric
bilateral motion field [55]. Lu et al. [44] leveraged Trans-
former architecture [71] to model long-term dependency.
Jin et al. [29] proposed a novel bi-directional motion es-
timator in a pyramid structure. Zhang et al. [82] proposed
a novel feature extraction strategy to combine motion and

--- Page 4 ---
0.18
0.16 mmm SportsSloMo

oe mm SNUriM
me Xa 0005
po
50.10
Zo.0a
© 0.06
0.04
boo ii
Min sete
0.00
oO 20 80 100 120
flow Magnitude
one
te —o
ons = SNUFLM
mmm X4K1000FPS.
301
50.10
go 08
“0.06
0.04
002
002) Mltlinsssssnsseye 20s.
ne a RT
Fw Niagitude
(c)

Figure 3: The SportsSloMo dataset. (a) Sampled frames, covering various sports categories and challenging human-centric
content for VFI; (b) Histogram of flow magnitude of all pixels in the dataset; (c) Histogram of mean flow magnitude of all

images in the dataset.

1 football

7\

art

Figure 4: Distribution of different sports categories in
our SportsSloMo benchmark.

appearance information via a hybrid CNN and Transformer
architecture.

In a nutshell, although previous methods have proposed
successful designs to handle complex motion and occlu-
sion, none of them is carefully designed for human-centric
scenes. As discussed in Sec. 1, human-centric VFI is
confronted with various challenges including dynamic pose
variation, complex human motion patterns and occlusion in
crowded scenes. Additionally, accurately synthesizing fine
details such as facial expressions and hand gestures can be
challenging. To this end, we propose to consider human-
aware loss terms through incorporating extra supervision
to both human keypoints detection and segmentation in the
panoptic setting.

3. SportsSloMo Benchmark

A myriad of benchmark datasets for video frame interpo-
lation are available, including Middlebury [1], GoPro [50],
UCFI101 [65], DAVIS [57], Adobe240fps [66], Vimeo90K
[79], SNU-FILM [14] and X4K1000FPS [64]. But none

of these them focuses on human-centric VFI, e.g., in sports
scenes. This limits the study of VFI methods targeted for
human-centric applications such as enhanced entertainment
experiences, commercial deployment, etc.

To bridge this gap and to foster future research, we
propose SportsSloMo, a challenging dataset consisting
of high-resolution (> 720p) sports videos crawled from
YouTube under the Common Creative License. Careful
curation has been taken to remove the unwanted content
in the videos, including advertisement, transition frames,
changes of shot, flashing lights, and non-slow-motion con-
tent. Specifically, we first conduct human detection on
all the videos by utilizing Yolov3 [62] and remove video
frames without detected humans. Second, we remove
frames with flashing lights by setting a threshold about
the brightness change between consecutive frames (large
brightness change indicates the existence of a flashing
light). Third, we use RAFT [68] to measure motion magni-
tude and set a threshold to discard non-slow-motion video
segments. Finally, we carefully curate the the extracted
clips manually to keep only high-quality self-consistent
videos. With such semi-automatic curation, we end up hav-
ing a set of short slow-motion clips of 9 frames, where each
frame has a spatial resolution of at least 1280 x 720. The
first and last frames are used as input for a VFI method and
the 7 intermediate frames as ground truths for model train-
ing and evaluation, approximately corresponding to con-
verting a video of 30-fps (frames per second) to 240-fps.

In total, we collect 131,464 video clips and 1.2M indi-
vidual video frames from 259 raw YouTube videos, which
covers 22 various sports categories in our dataset with dif-
ferent content and motion patterns, including hockey, base-

--- Page 5 ---
Panoptic
Segmentation]

Plug-in
Human-aware
Loss Module

Keypoint
Detection

Panoptic
| segmentation]

Keypoint

Leasie

Figure 5: Illustration of our proposed human-aware loss terms, consisting of losses about human segmentation in the

panoptic setting and keypoint detection.

ball, skating, basketball, running, volleyball, etc. Fig. 4
shows the proportion of different sports categories. Visu-
alizations of randomly sampled video frames are available
in Fig. 3. We refer the readers to the supplementary material
for more results.

As shown in Table 1, compared with existing datasets,
not only our dataset is tailored for human-centric scenar-
ios, but also surpasses others in terms of scale, resolution,
and frame rate. Fig. 3 demonstrates the histogram of
flow magnitude of all pixels and mean flow magnitude of
all images in our dataset (calculated using GMFlow [76]),
with a comparison with the widely-used SNU-FILM [14]
dataset (which has the same spatial resolution as ours) and
the recently introduced X4k1000FPS [64] dataset. As can
be seen, our SportsSloMo benchmark contains more large-
displacement motion compared to the SNU-FILM dataset.
For instance, for the avarege flow magnitude on the image-
level (Fig. 3(c)), our dataset has way more images, whose
mean flow magnitude is greater than 20 pixels. And we can
also observe that both our SportsSloMo and X4K1000FPS
datasets contain large motions.

We split our proposed SportsSloMo dataset into train
and test, containing 115,421 and 16,043 video clips, respec-
tively. For each sports category, videos are split into train
and test without intersection, so that the test videos are com-
pletely unseen during training.

4. Human-aware Video Frame Interpolation

4.1. Overview

Given two input frames Jp and J;, the goal of VFI is to
synthesize the intermediate frame J, at an intermediate time
step t € (0,1). A VFI method needs to find the pixel-wise

correspondences between J, and J, and adaptively fuse cor-
responding pixels to synthesize each pixel in [;. Flow-based
approaches, such as SuperSloMo [27] and EBME [29], usu-
ally explicitly model the correspondences as optical flow,
whereas flow-agnostic methods, such as AdaCof [36], use
learned kernels to process the visual correspondence implic-
itly.

To supervise the network training, the reconstruction er-
ror of the intermediate frame is used as the loss. For in-
stance, in [29], the weighted sum of the Charbonnier loss [7]
Lchar and census loss [46] Leen between the ground truth
intermediate frame [; and synthesized frame I, are adopted

Loasic = Lenar(Li — ft) + cen « Leen, ft),

where Lenar(2) = (x? + €?)*%,a = 0.5,€ = 107%.

Such reconstruction loss, however, may not provide
enough supervision for human-centric VFI due to the chal-
lenges of highly deformable human motion and frequent oc-
clusion. To address these challenges, we propose to incor-
porate Dseg and L,,, into the network supervision based
on human segmentation in the panoptic setting and key-
point detection, respectively, enforcing the model to pro-
duce high-quality synthesis results over human boundaries
and along the keypoint trajectories. The final loss is

L = Lpasic + AsegLseg + AkptLkpe- (2)

It is worth noting that our proposed human-aware loss terms
are flexible, which can be plugged into other flow-based
(e.g., SuperSloMo [27] and EBME [29]) or flow-agnostic
VFI models (e.g., AdaCoF [36]), as shown in Section 5.

--- Page 6 ---
4.2. Human Segmentation Loss

In human-cenrtic scenarios, accurate estimation of hu-
man body boundaries is crucial. In regions where the body
movement is complex or there are heavy occlusions, inaccu-
racies in the estimated body boundaries may lead to visible
artifacts in the synthesized video frame. To this end, we pro-
pose to incorporate human segmentation as extra supervi-
sion to improve the synthesis of intermediate video frames.
Human segmentation masks directly tell where human body
boundaries are, as shown in Fig. 2, helping to reduce ghost
effects. Though instance and panoptic segmentation could
both serve for this purpose, we empirically find that the
panoptic segmentation models tend to yield better interpo-
lation results than instance segmentation counterparts.

Specifically, we adopt state-of-the-art Mask2Former
[10] trained on COCO panoptic dataset [33] with the Swin-
L backbone [42] from Detectron2 [74] to generate panoptic
segmentation masks M, from the ground-truth intermediate
frame J;. During training, the panoptic segmentation re-
sults M of the synthesized intermediate frame f, 1 obtained
using the same Mask2Former model is compared with M;.
Ideally, if the synthesized intermediate frame I, is identical
to the ground truth [;, M, should be the same as M;. By
comparing their difference, the human segmentation loss
enhances the consistency of the human body boundaries be-
tween J; and I, 1, Which helps reduce the ghost effects. We
follow the loss function of Mask2Former and use the binary
cross-entropy loss L,,. and the dice loss Lajce [49] for the
human segmentation loss

Lseg = AceLce(Me, Mr) + AdiceLaice(Mi, Mr), 3)

where we set Ace = 5.0 and Agice = 5.0. The panoptic seg-
mentation model is frozen during training and the gradient
of this loss will be backpropagated to a VFI model only ina
way similar to the perceptual loss [31], enforcing the model
to produce high-quality synthesis results.

4.3. Human Keypoint Detection Loss

In addition to segmentation, we also consider the hu-
man keypoints estimation as an extra supervision. Accurate
estimation of human keypoints is also crucial for human-
centric VFI, which are indicative about the position of each
body part, providing additional cues for motion estima-
tion. Hence, by incorporating the supervision of human
keypoints, we can better preserve human motions by enforc-
ing coherent motion trajectories and guide the interpolation
process to generate more plausible intermediate frames.

Specifically, we adopt the state-of-the-art human pose
estimation model ViTPose [78] trained on COCO [40], with
the ViT-L backbone [15] initialized with MAE [20] pre-
trained weights. We first use an off-the-shelf human detec-
tor YOLOv8 [30] to detect person instances in the ground-
truth intermediate frame J;. We then employ ViTPose to

Ground Truth

EBME

EBME w/ HL

Figure 6: VFI results with human-aware loss (HL) terms.
We can see better human segmentation, keypoint detection,
and interpolation result can be obtained enhanced by the
human-aware loss.

estimate the keypoints heatmap /x; containing the locations
of each joint. Similar to the human segmentaiton loss, we
estimate the keypoints heatmap K, using the same ViTPose
model on the synthesized intermediate frame I. The dif-
ferences between Ky; and K , are then used as the supervi-
sion to improve VFI results. We follow the loss function
of ViTPose and use the MSE (mean square error) loss over
heatmaps as the human keypoint loss Ly.

Lipt = MSE(Ki, Ki). (4)

The ViTPose model is frozen during training and the gra-
dient of this loss will be backpropagated to a VFI model
to encourage high-quality intermediate frame synthesis so
that the estimated human keypoints are close to the ones
obtained from the true intermediate frame.

5. Experiments
5.1. Setup

VFI models. We benchmark existing VFI approaches by
re-training several state-of-the-art methods on our SportsS-
loMo dataset using their publicly available code, including
SuperSlomo [27], AdaCoF [36], M2MVFI [23], RIFE [25],
EBME [29], UPR-Net [28], and EMA-VFI [82]. AdaCoF is
a flow-agnostic VFI model and the rest are flow-based. All

--- Page 7 ---
Ground Truth SuperSlomo

Overlayed input

EBME w/ HL

Overlayed input

a

Figure 7: Qualitative comparisons on SportsSloMo dataset. With the enhancement of the human-aware (HL) losses,

EBME achieves better interpolation results.

Table 2: Quantitative results of different approaches on
the SportsSloMo dataset. As can be seen, our proposed
human-aware loss (HL) terms consistently improve all VFI
models’ performance.

Method PSNR | SSIMT | TEL

SuperSlomo [27] }| CVPR 2018 | 29.77 | 0.910 | 9.14
SuperSloMo + HL - 30.24 | 0.917 | 8.46

AdaCoF [36] CVPR 2020 | 28.79 | 0.926 | 5.84
AdaCoF + HL - 28.94 | 0.926 | 5.72

M2MVFI [23] CVPR 2022 | 29.03 | 0.935 | 5.16
M2MVFI + HL - 29.29 | 0.936 | 5.07

Venue

RIFE [25] ECCV 2022 | 29.69 | 0.931 | 5.25
RIFE + HL - 29.87 | 0.933 | 5.16
EBME [29] WACV 2023 | 30.15 | 0.941 | 4.66
EBME + HL - 30.48 | 0.944 | 4.40

UPR-Net [28] CVPR 2023 | 30.25 | 0.945 | 4.56
UPR-Net + HL - 30.50 | 0.945 | 4.38

EMA-VFI [82]

CVPR 2023 | 30.70 | 0.949 | 4.28
EMA-VFI + HL -

30.75 | 0.952 | 4.19

these models are trained from scratch on our SportsSloMo
dataset.

We also incorporate our proposed human-aware loss
terms into these models to validate the effectiveness of ex-
plicitly taking human priors into account to improve human-
centric VFI.

Evaluation metrics. Following previous VFI methods
[27, 24], we adopt the widely-used signal-to-noise ratio
(PSNR), structural similarity (SSIM) [72], and interpolation
error (IE) [1] to evaluate the interpolation results. For PSNR

and SSIM, higher indicates better performance. And for IE,
the lower the better.

Implementation details. For the human segmentation and
keypoint loss terms, to avoid the influence of Batch Nor-
malization (BN) layers on the training phase, we replaced
the BN layers with frozen Batch Normalization (frozen BN)
layers. We use the default hyperparameters provided by
each VFI model. Training and evaluation are both con-
ducted on 8 NVIDIA RTX A6000 GPUs.

During training, we randomly sample a frame between
the first and the ninth (last) frame for VFI methods support-
ing arbitrary time frame interpolation like SuperSlomo [27],
M2MVFI [23], RIFE [25], EBME [29], UPR-Net [28], and
EMA-VFI [82]. For VFI methods that can only synthesize
the middle frame like AdaCoF [36], we randomly gener-
ate triplets within the nine frames such that the target frame
is always in the middle of the input two frames. During
evaluation, arbitrary-time-supported VFI methods take the
time step as input and interpolate every intermediate frame,
while single-frame methods recursively generate each inter-
mediate frame.

5.2. Benchmarking Existing Methods

Table 2 shows quantitative comparisons between exist-
ing VFI methods on our proposed SportsSloMo dataset. We
can see that EMA-VFI [82] performs the best interms of all
three evaluation metrics. It is worth noting that the per-
formance for each model decreases compared with results
on popular datasets. For instance, two top-performing ap-
proaches on SportsSloMo, EBME [29] and EMA-VFI [82]
produce PSNR scores of 30.15 and 30.70, respectively,
compared with 36.19 and 36.64 on the Viemo90K [79] as
well as 30.64 and 30.94 on the hard split of the SNU-
FILM [14] benchmarks. It highlights the difficulty of our

--- Page 8 ---
Table 3: Effectiveness of different auxiliary loss terms.

segmentation | keypoint

Model hee yo!" | PSNR¢ | SSIMt | TEL
x x | 29.77 | 0.910 | 9.14

v x | 30.19 | 0.916 | 8.56

SuperSloMo x V 30.22 | 0.916 | 8.49
v Y | 30.24 | 0.917 | 8.46

x x | 30.15 | 0.941 | 4.66

v x | 30.26 | 0.942 | 4.61

EBME x Y | 30.34 | 0.943 | 4.46
v Y | 30.48 | 0.944 | 4.40

benchmark and suggests significant challenges need to be
addressed. We provide more details in the supplementary
material.

5.3. Enhanced Human-aware VFI

Table 2 also demonstrates the effectiveness of our pro-
posed human-aware losses. Specifically, we incorporate
the proposed human segmentation and keypoint losses into
each of the VFI models. As can be seen, they consistently
improve the performance of every single VFI method for
all three evaluation metrics. Specifically, in terms of PSNR,
our proposed human-aware losses lead to improvement of
1.6% and 1.1% for SuperSloMo [27] and EBME [29], re-
spectively. In terms of IE, the improvement for these two
methods are 7.4% and 5.6%, respectively. Qualitatively, we
can see from Fig. 6 and Fig. 7, our proposed human-aware
loss terms can improve interpolation results for the highly
deformable arms of the athletes and the background under
occlusions.

5.4. Ablation Study

In this section, we present ablation study on Super-
SloMo [27] and EBME [29] to analyze the design choices
of our proposed human-aware losses in Table 3. As can
be seen, both human segmentation and keypoint detection
losses can successfully improve the performance of Super-
SloMo and EBME. This justifies our motivation to leverage
human-aware priors to improve the human-centric VFI. By
combining these two losses, the largest performance gain
can be obtained for both methods. Furthermore, from Fig. 6,
we can clearly see that better interpolation results can be ob-
tained around the elbow and hand.

5.5. Limitations and Discussions

While we have introduced human-aware loss terms to
handle challenging human-centric scenarios in our pro-
posed dataset, there still remains noteworthy limitations for
future exploration.

First of all, for human-centric scenes, the presence of
large, complex, and highly deformable human motions and
occlusion by crowds make finding correspondences of hu-
mans (i.¢., optical flow) between input frames a challenging
task. Recent studies [25, 35] have demonstrated the effec-
tiveness of knowledge distillation [22] to improve the inter-
mediate optical flow estimation.

We also explored this loss to incorporate extra supervi-
sion for flow-based VFI methods (e.g., SuperSloMo [27],
EBME [29]). Specifically, we use the optical flow results
obtained from state-of-the-art model GMFlow [76], which
is pre-trained on the optical flow benchmarks [16, 45, 5],
as the teacher network to perform flow distillation. We
found, however, mixed results on our SportsSloMo bench-
mark. While it indeed improves the performance for Su-
perSlomo [27] (PSNR: 29.77 vs. 30.18), it hurts the
performance for EBME [29] (PSNR: 30.15 vs. 29.85).
We also fine-tune the GMFlow model on the human flow
dataset [59], consisting of synthetic human figures over-
laied on top of real world images as background. How-
ever, we observed significant domain gap between the syn-
thetic training images and real-world test images, which
lead to poor optical flow estimation results. How to im-
prove human-centric optical flow to improve VFI is still a
challenging problem.

Second, we only utilize 2D cues of human bodies in our
proposed loss terms to enhance VFI. Recently, there has
been significant advance made in 3D human body recon-
struction from videos, for instance, [18, 80, 58, 34], con-
sidering both the spatial and temporal information. By lift-
ing 2D videos into the 3D space, occlusions may be bet-
ter solved. More advanced loss terms involving 3D human
reconstruction is worth exploring. We leave this as future
work to explore.

Finally, in the SportsSloMo dataset, various sports in-
volve fast-moving balls and sports equipment. In this paper,
we only consider human boundary and body parts for mo-
tion estimation. Fast-moving objects, however, in these sce-
narios are also challenging for VFI. More effort is needed
for such a problem.

6. Conclusion

In this paper, we introduced a new benchmark, SportsS-
loMo, focusing on human-centric video frame interpola-
tion. Our benchmark contains 130K video clips and more
than 1M video frames obtained from high-resolution (>
720p) slow-motion sports videos crawled from YouTube
with careful curation. Due to the complex, highly defor-
mation human motion and frequent occlusion, this bench-
mark imposes significant challenges to existing VFI mod-
els. To enhance their accuracy, we introduce human-aware
loss terms to improve existing methods, where the super-
vision of human segmentation and keypoints detection are

--- Page 9 ---
incorporated. Our loss terms are model agnostic and have
been successfully applied to seven existing VFI methods,
leading to better accuracy consistently. Our benchmark
dataset and code will be publicly released to foster future re-
search in the new exciting direction of human-centric VFI.

Acknowledgement

We thank Thuy-Tien Bui and Devroop Kar for their help
on data collection. This work was partially supported by the
National Science Foundation under Award IIS-2310254.

References

1

Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth,
Michael J Black, and Richard Szeliski. A database and eval-
uation methodology for optical flow. International journal
of computer vision, 92(1):1-31, 2011. 1, 3, 4,7

Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang,
Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video
frame interpolation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
3703-3712, 2019. 3

Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao,
and Ming-Hsuan Yang. Memc-net: Motion estimation and
motion compensation driven neural network for video inter-
polation and enhancement. JEEE transactions on pattern
analysis and machine intelligence, 43(3):933—948, 2019. 3
Karlis Martins Briedis, Abdelaziz Djelouah, Mark Meyer,
Jan McGonigal, Markus H. Gross, and Christopher Schroers.
Neural frame interpolation for rendered content. ACM Trans.
Graph., 40(6):239:1-239:13, 2021. 1

D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical flow evaluation.
In ECCYV, 2012. 8

Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and
Chen Change Loy. Investigating tradeoffs in real-world
video super-resolution. In CVPR, 2022. 2

Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and
Michel Barlaud. Two deterministic half-quadratic regular-
ization algorithms for computed imaging. In Proceedings of
Ist international conference on image processing, volume 2,
pages 168-172. IEEE, 1994. 5

Jiaben Chen, Yichen Zhu, Dongze Lian, Jiaqi Yang, Yifu
Wang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent
Kneip, and Shenghua Gao. Revisiting event-based video
frame interpolation. arXiv preprint arXiv:2307.12558, 2023.
3

Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xinggian Xu,
Vidit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong
Wang. Videoinr: Learning video implicit neural representa-
tion for continuous space-time super-resolution. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 2047-2057, 2022. 3

Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask

11

12

13

14

15

16

17

18

19

20

21

Rv
XN

transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1290-1299, 2022. 2, 6
Xianhang Cheng and Zhenzhong Chen. Video frame interpo-
lation via deformable separable convolution. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 34,
pages 10607-10614, 2020. 3

Xianhang Cheng and Zhenzhong Chen. Multiple video
frame interpolation via enhanced deformable separable con-
volution. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 44(10):7029-7045, 2021. 3

Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei
Lu, Jin Tang, and Konstantinos N Plataniotis. All at
once: Temporally adaptive multi-frame interpolation with
advanced motion modeling. In European Conference on
Computer Vision, pages 107-123. Springer, 2020. 3
Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and
Kyoung Mu Lee. Channel attention is all you need for video
frame interpolation. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 10663-10671,
2020. 1, 2,3, 4,5,7
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 6
Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Caner Hazir-
bas, Vladimir Golkov, Patrick van der Smagt, Daniel Cre-
mers, Thomas Brox, et al. FlowNet: Learning optical flow
with convolutional networks. In JCCV, 2015. 8

Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.
Dynamic view synthesis from dynamic monocular video. In
ICCV, 2021. 2

Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,
Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D:
Reconstructing and tracking humans with transformers. In
International Conference on Computer Vision (ICCV), 2023.
8

Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao.
Featureflow: Robust video interpolation via structure-to-
texture generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
14004-14013, 2020. 3

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Dollar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 16000-—
16009, 2022. 6

Weihua He, Kaichao You, Zhendong Qiao, Xu Jia, Ziyang
Zhang, Wenhui Wang, Huchuan Lu, Yaoyuan Wang, and
Jianxing Liao. Timereplayer: Unlocking the potential of
event cameras for video interpolation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 17804-17813, 2022. 3

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv: 1503.02531, 2015. 8


--- Page 10 ---
[25]

26

27

28

29

30

31

32

33

34

35

[36]

Ping Hu, Simon Niklaus, Stan Sclaroff, and Kate Saenko.
Many-to-many splatting for efficient video frame interpola-
tion. In CVPR, 2022. 2, 3, 6,7

Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi,
and Shuchang Zhou. Rife: Real-time intermediate flow
estimation for video frame interpolation. arXiv preprint
arXiv:2011.06294, 2020. 7

Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and
Shuchang Zhou. Real-time intermediate flow estimation for
video frame interpolation. In Computer Vision—ECCV 2022:
17th European Conference, Tel Aviv, Israel, October 23—
27, 2022, Proceedings, Part XIV, pages 624-642. Springer,
2022. 2,3, 6,7, 8

Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. Advances in neural informa-
tion processing systems, 28, 2015. 3

Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan
Yang, Erik G. Learned-Miller, and Jan Kautz. Super slomo:
High quality estimation of multiple intermediate frames for
video interpolation. In CVPR, 2018. 1, 2,3, 5, 6,7, 8

Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo,
and Cheul-hee Hahm. A unified pyramid recurrent net-
work for video frame interpolation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 2023. 2, 3, 6,7

Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen,
Jayoon Koo, and Cheul-hee Hahm. Enhanced bi-directional
motion estimation for video frame interpolation. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 5049-5057, 2023. 2, 3, 5, 6, 7, 8
Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by
Ultralytics, 1 2023. 6

Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
ECCYV, 2016. 6

Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and
Du Tran. Flavr: Flow-agnostic video representations for fast
frame interpolation. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision, pages
2071-2082, 2023. 3

Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Dollar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9404-9413, 2019. 6
Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,
and Michael J. Black. PARE: Part attention regressor for 3D
human body estimation. In JCCV, 2021. 8

Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenging Chu,
Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang.
Ifrnet: Intermediate feature refine network for efficient frame
interpolation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1969-
1978, 2022. 3,8

Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun
Pak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive col-
laboration of flows for video frame interpolation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision

37

38

39

40

41

42

43

44

45

46

47

48

49

and Pattern Recognition, pages 5316-5325, 2020. 2, 3, 5, 6,
7

Siyao Li, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N.
Metaxas, Chen Change Loy, and Ziwei Liu. Deep animation
video interpolation in the wild. In CVPR, 2021. 1

Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-
Hsuan Yang, and Peyman Milanfar. Comisr: Compression-
informed video super-resolution. In JCCV, 2021. 2

Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene flow fields for space-time view synthesis of dy-
namic scenes. In CVPR, 2021. 1, 2

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision—ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740-755. Springer, 2014. 6

Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu
Chuang. Deep video frame interpolation using cyclic frame
generation. In Proceedings of the AAAI Conference on Arti-
ficial Intelligence, volume 33, pages 8794-8802, 2019. 3
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision, pages 10012-10022, 2021. 6

Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and
Aseem Agarwala. Video frame synthesis using deep voxel
flow. In Proceedings of the IEEE International Conference
on Computer Vision, pages 4463-4471, 2017. 3

Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya
Jia. Video frame interpolation with transformer. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 3532-3542, 2022. 3

Nikolaus Mayer, Eddy Ilg, Philip Hiusser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation. In CVPR, 2016. 8
Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Un-
supervised learning of optical flow with a bidirectional cen-
sus loss. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018. 5

Simone Meyer, Abdelaziz Djelouah, Brian McWilliams,
Alexander Sorkine-Hornung, Markus Gross, and Christo-
pher Schroers. Phasenet for video frame interpolation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 498-507, 2018. 3

Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse,
and Alexander Sorkine-Hornung. Phase-based frame inter-
polation for video. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1410-1418,
2015. 3

Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV), pages 565-571. Ieee, 2016.
6

--- Page 11 ---
50

51

52

53

54

55

56

57

[58]

[61

Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3883-3891,
2017. 4

Simon Niklaus and Feng Liu. Context-aware synthesis for
video frame interpolation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1701-1710, 2018. 3

Simon Niklaus and Feng Liu. Softmax splatting for video
frame interpolation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
5437-5446, 2020. 3

Simon Niklaus, Long Mai, and Feng Liu. Video frame in-
terpolation via adaptive convolution. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 670-679, 2017. 3

Simon Niklaus, Long Mai, and Feng Liu. Video frame in-
terpolation via adaptive separable convolution. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 261-270, 2017. 3

Junheum Park, Chul Lee, and Chang-Su Kim. Asymmetric
bilateral motion estimation for video frame interpolation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 14539-14548, 2021. 3

Tomer Peleg, Pablo Szekely, Doron Sabo, and Omry Sendik.
Im-net for high resolution video frame interpolation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 2398-2407, 2019. 3
Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc
Van Gool, Markus Gross, and Alexander Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video
object segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 724-732,
2016. 1,4

Jathushan Rajasegaran, Georgios Pavlakos, Angjoo
Kanazawa, Christoph Feichtenhofer, and Jitendra Malik.
On the benefits of 3d pose and tracking for human action
recognition. In CVPR, 2023. 8

Anurag Ranjan, David T. Hoffmann, Dimitrios Tzionas, Siyu
Tang, Javier Romero, and Michael J. Black. Learning multi-
human optical flow. Int. J. Comput. Vis., 128(4):873-890,
2020. 8

Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,
Caroline Pantofaru, and Brian Curless. Film: Frame inter-
polation for large motion. In Computer Vision—ECCV 2022:
17th European Conference, Tel Aviv, Israel, October 23-27,
2022, Proceedings, Part VII, pages 250-266. Springer, 2022.
3

Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad
Shoeybi, Guilin Liu, Kevin J Shih, Andrew Tao, Jan Kautz,
and Bryan Catanzaro. Unsupervised video interpolation us-
ing cycle consistency. In Proceedings of the IEEE/CVF in-
ternational conference on computer Vision, pages 892-900,
2019. 3

Joseph Redmon and Ali Farhadi. Yolov3: An incremental
improvement. arXiv preprint arXiv: 1804.02767, 2018. 4

63

64

65

66

67

68

69

(70

71

72

73

74

75

76

Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and
Ming-Hsuan Yang. Video frame interpolation transformer.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 17482-17491, 2022.
3

Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi:
Extreme video frame interpolation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 14489-14498, 2021. 1, 2,3, 4,5

Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv: 1212.0402, 2012. 1, 2, 3, 4
Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo
Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video
deblurring for hand-held cameras. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1279-1288, 2017. 1, 2, 3, 4

Masato Tamura, Rahul Vishwakarma, and Ravigopal Ven-
nelakanti. Hunting group clues with transformers for social
group activity recognition. In ECCV, 2022. 2

Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Computer Vision—ECCV
2020: 16th European Conference, Glasgow, UK, August 23-
28, 2020, Proceedings, Part II 16, pages 402-419. Springer,
2020. 4

Stepan Tulyakov, Alfredo Bochicchio, Daniel Gehrig, Sta-
matios Georgoulis, Yuanyou Li, and Davide Scaramuzza.
Time lens++: Event-based frame interpolation with paramet-
ric non-linear flow and multi-scale fusion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 17755-17764, 2022. 3

Stepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis,
Julius Erbach, Mathias Gehrig, Yuanyou Li, and Davide
Scaramuzza. Time lens: Event-based video frame interpola-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 16155-16164,
2021. 3

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 3

Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. EEE transactions on image processing,
13(4):600-612, 2004. 7

Chao-Yuan Wu, Nayan Singhal, and Philipp Kréahenbiihl.
Video compression through image interpolation. In ECCV,
2018. 1

Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen
Lo, and Ross Girshick. Detectron2. https://github.
com/facebookresearch/detectron2, 2019. 6
Wendi Xian, Jia-Bin Huang, Johannes Kopf, and Changil
Kim. Space-time neural irradiance fields for free-viewpoint
video. In CVPR, 2021. 2

Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and
Dacheng Tao. Gmflow: Learning optical flow via global
matching. In Proceedings of the IEEE/CVF conference on

--- Page 12 ---
77

78

79

80

81

82

computer vision and pattern recognition, pages 8121-8130,
2022. 5,8

Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-
Hsuan Yang. Quadratic video interpolation. Advances in
Neural Information Processing Systems, 32, 2019. 3

Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vit-
pose: Simple vision transformer baselines for human pose
estimation. arXiv preprint arXiv:2204.12484, 2022. 2,6
Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and
William T Freeman. Video enhancement with task-
oriented flow. International Journal of Computer Vision,
127(8):1106-1125, 2019. 1, 2,3, 4,7

Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo
Kanazawa. Decoupling human and camera motion from
videos in the wild. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2023. 8

Hangjie Yuan, Dong Ni, and Mang Wang. Spatio-temporal
dynamic inference network for group activity recognition. In
ICCV, 2021. 2

Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen,
Gangshan Wu, and Limin Wang. Extracting motion and ap-
pearance via inter-frame attention for efficient video frame
interpolation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 5682—
5692, 2023. 2,3, 6,7

Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng,
Farley Lai, Long Zhao, Ting Liu, Mubbasir Kapadia, and
Hans Peter Graf. Composer: Compositional reasoning of
group activity in videos with keypoint-only modality. In
ECCYV, 2022. 2

