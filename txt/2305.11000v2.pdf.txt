arXiv:2305.11000v2 [cs.CL] 19 May 2023
SpeechGPT: Empowering Large Language Models with
Intrinsic Cross-Modal Conversational Abilities
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,
Yaqian Zhou Xipeng Qiu*
School of Computer Science, Fudan University
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
dongzhang22@m.fudan.edu.cn
{smli20,zhouyaqian,xpqiu}@fudan.edu.cn
https://github.com/0nutation/SpeechGPT
Abstract
Multi-modal large language models are regarded as a crucial step towards Ar-
tificial General Intelligence (AGI) and have garnered significant interest with
the emergence of ChatGPT. However, current speech-language models typi-
cally adopt the cascade paradigm, preventing inter-modal knowledge transfer.
In this paper, we propose SpeechGPT, a large language model with intrinsic
cross-modal conversational abilities, capable of perceiving and generating multi-
model content. With discrete speech representations, we first construct SpeechIn-
struct, a large-scale cross-modal speech instruction dataset. Additionally, we
employ a three-stage training strategy that includes modality-adaptation pre-
training, cross-modal instruction fine-tuning, and chain-of-modality instruction
fine-tuning. The experimental results demonstrate that SpeechGPT has an im-
pressive capacity to follow multi-modal human instructions and highlight the
potential of handling multiple modalities with one model. Demos are shown in
https://0nutation.github.io/SpeechGPT.github.io/.
1
Introduction
Large language models (OpenAI, 2023; Touvron et al., 2023) have performed astonishingly on
various natural language processing tasks. Meanwhile, multi-modal large language models, such as
GPT-4, PALM-E (Driess et al., 2023), and LLaVA (Liu et al., 2023), have explored the ability of
LLMs to understand multi-modal information. However, a significant gap exists between current
LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and
understand multi-modal content but cannot spontaneously generate multi-modal content. Second,
continuous signals like images and speech cannot be adapted directly to LLMs that receive discrete
tokens.
The current speech-language model mainly adopts a cascading paradigm (Huang et al., 2023a) i.e.,
the LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS)
model in tandem, or the LLM is employed as a control hub, with several speech processing models
are integrated to cover multiple audio or speech tasks (Huang et al., 2023a; Shen et al., 2023). Some
prior work on generative spoken language models involves encoding the speech signal into a discrete
representation (Baevski et al., 2020; Hsu et al., 2021) and modeling it with language models (Lakhotia
et al., 2021; Borsos et al., 2022; Zhang et al., 2023b; Wang et al., 2023).
*Corresponding author
What is the capital of
French?
The capital of French is
Paris.
(What is your name?)
(My name is SpeechGPT.)
Speech
GPT
Please read the sentence:
"Today is a beautiful day.'
"
Sure, I will read it now:
I-I
Record the content:
The content of speech is:
"Have a good day!"
Figure 1: SpeechGPT's capabilities to tackle multiple cross-modal tasks.
While capable of perceiving and generating speech, the existing cascading methods or spoken
language models still have several limitations. First, the LLM in the cascaded model only functions
as a content generator. Since the representations of speech and text are not aligned, the LLM's
knowledge cannot be transferred to the speech modality. Second, the cascade approach (Shen
et al., 2023; Huang et al., 2023a) suffers from the loss of paralinguistic signals such as emotion
and prosody. Third, existing spoken language models (Wang et al., 2023; Zhang et al., 2023b) only
synthesize speech but fail to comprehend its semantic information, preventing them from achieving
true cross-modal perception and generation.
In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conver-
sational abilities, capable of perceiving and generating multi-model content. We perform speech
discretization with a self-supervised trained speech model to unify the modality between speech and
text. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing
the model with an inherent competence to perceive and generate the speech.
To provide the model with the capacity to handle multi-modal instructions, we build the first speech-
text cross-modal instruction-following dataset SpeechInstruct. Specifically, we discretize the speech
to discrete units (Hsu et al., 2021) and construct the cross-modal unit-text pair based on the existing
ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to
simulate actual user instructions as illustrated in Appendix B. In addition, to further enhance the
model's cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model
receives the speech command, thinks about the process in text, and then outputs the response in
speech.
For better cross-modal transfer and efficient training, SpeechGPT undergoes a three-stage training
process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality
instruction fine-tuning. The first stage enables speech comprehension for SpeechGPT with the
discrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the
model's cross-modal capabilities. The third stage utilizes parameter-efficient LORA (Hu et al., 2021)
fine-tuning for further modality alignment.
To evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case
analyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks,
and spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for
unimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.
Our contributions include the following:
• We build the first multi-modal large language model that can perceive and generate multi-modal
contents.
• We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instruction-
following dataset.
2
Instruction-tuning
Meta Prompt
Speech Datasets
Text Datasets
GPT-4
Speech2Unit
Text2Unit
Instructions
Template,
Template,
Speechinstruct
Cross-modal
Instruction data
Chain-of-Modality
Instruction data
Cross-modal Instructions
[Human]: Transcribe the speech to text. This is the input: {speech unit U} <eoh>.
[SpeechGPT]: {transcription T} <eos>.
Chain-of-Modality Instructions
[Human]: This is
is is the speech instruction: (speech). You can do it step by step. You
can transcribe the instruction, get the text response and speak the response.
<eoh>
[SpeechGPT]: [tq] {Text/ }; [ta] {Text R}; [ua] {SpeechR} <eoa>.
Transcription:
Hi, my name is SpeechGPT. Nice to
meet you!
Unit
Vocoder
[SpeechGPT]:
<99> <5> <69> <597>...... <31>
SpeechGPT
[Human]:
<43> <2> <64> <33>
<534>
Discrete Speech Unit Extractor
Transcription:
Good morning, what is
your name?
Figure 2: Left: An overview of SpeechInstruct construction process. The SpeechInstruct dataset con-
sists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. Template1
is shown in 3.1. Template2 is shown in Appendix C. Right: An illustration of SpeechGPT model
structure.
• We build the first spoken dialogue LLM with strong human instruction following ability and spoken
dialogue ability.
• We show great potential to incorporate other modalities into LLMs through discrete representations.
2 Related Work
Multi-modal Large Language Model Current multi-modal LLMs predominantly focus on the
visual domain, feeding continuous representations obtained from pre-trained visual encoders into
LLMs, facilitating full-parameter or parameter-efficient training on visual-language data (OpenAI,
2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E (Driess et al., 2023) integrates the 540B
PaLM (Chowdhery et al., 2022) and 22B Vision Transformer (Dosovitskiy et al., 2021) into the largest
vision-language model. LLaVA (Liu et al., 2023) leverages pre-trained CLIP (Radford et al., 2021)
visual encoder and LLAMA (Touvron et al., 2023) and conduct instruct tuning on GPT4-assisted
visual instruction data. X-LLM (Chen et al., 2023) converts multi-modalities into representations with
X2L interfaces as the inputs of the large language model. However, such structures only enable LLMS
to process multi-modal input, without ability to generate multi-modal output. Diverging from prior
studies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing
it with the proficiency to accommodate both multi-modal input and output.
Generative Spoken Language Model Discrete self-supervised representation based spoken genera-
tive language modeling is making remarkable progress on large-scale speech dataset training (Nguyen
et al., 2022). AudioLM (Borsos et al., 2022) proposes to model speech based on audio codecs together
with semantic codes, which can synthesize speech in a textlesss setting. VALL-E (Wang et al., 2023)
builds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional
generation task. However, these models are designed for a specific task and failed to benefit from
LLMs. SpeechGPT is built upon the foundation of LLM and transfers LLM's knowledge to speech
modality, consequently obtaining better task generalization and human-instruction following ability.
Speech-Enabled LLM Interaction Following the emergence of ChatGPT, several studies have
concentrated on the integration of expert speech models with LLMs to enable direct speech interaction
with LLMs. HuggingGPT (Shen et al., 2023) facilitates task decomposition of human instructions by
LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encom-
passing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT (Huang
et al., 2023a) leverages a variety of audio foundation models to process complex audio information
and connect LLMs with input/output interface (ASR, TTS) for speech conversations. However, these
models exhibit increased complexity, demand extensive resources, and are prone to the unavoidable
error accumulation problems. Our approach enables speech interaction with LLMs without relying
on ASR or TTS systems, circumventing the aforementioned drawbacks.
3
3
SpeechInstruct Construction
Due to the limitations in publicly available speech data and the lack of variety of speech-text tasks,
we construct SpeechInstruct, a speech-text cross-modal instruction-following dataset. This dataset
consists of two parts, the first part is called Cross-Modal Instruction, and the second part is called
Chain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure 2.
3.1 Cross-modal Instruction
Data Collection We collect several large-scale English ASR datasets to construct Cross-Modal
Instruction, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and
LibriSpeech (Panayotov et al., 2015). We employ mHuBERT² as the speech tokenizer to discretize
speech data into discrete units and remove the repetitive units of adjacent frames to get reduced units.
Ultimately, we obtain 9 million unit-text data pairs.
Task Description Generation We generate ASR and TTS task descriptions that are compatible with
speech-text data pairs. Unlike the Self-Instruct method (Wang et al., 2022), we generate descriptions
through a zero-shot approach. Specifically, we directly input the prompts shown in Appendix A into
OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each
task and some examples are shown in Appendix B.
Instruction Formatting For a discrete unit sequence U and its associated transcription T, we
determine whether it will be used for constructing an ASR task or a TTS task based on the probabil-
ity p. Subsequently, we randomly select a description D from the corresponding task description.
This results in a triplet consisting of the task description, discrete unit sequence, and transcription,
denoted as (D, U,T). Following this, the triplet is assembled into an instruction using the template:
[Human]:{D}. This is input: {U}<eoh>.[SpeechGPT]: {T}<eos>.. To support multi-turn dia-
logues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering
to the maximum input length of the model.
3.2 Chain-of-Modality Instruction
Speech Instruction Generation Due to the lack of instruction data with speech input and speech
output, we trained a text-to-unit generator to convert text instruction data into speech instruction
data. Specifically, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We
trained it on LibriSpeech unit-text pairs in Cross-modal Instruction. We select 37,969 samples from
the moss-002-sft-data dataset ³ whose response length is shorter than 35 words. And we convert both
their instructions and responses into unit sequences through the text-to-unit generator. As a result, we
obtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and
speech responses, denoted as (SpeechI, TextI, TextR, SpeechR).
3
Instruction Formatting Using the above quadruplets, we could construct chain-of-thought style
instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech
Instruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response.
Their corresponding templates can be found in Appendix C.
4
SpeechGPT
4.1 Model Structure
A unified framework is designed to provide architecture compatibility across different modalities.
As shown in Figure 2, our model consists of three main components: discrete unit extractor, large
language modal and unit vocoder. Under this architecture, LLM can perceive multi-modal inputs and
generate multi-modal outputs.
Discrete Unit Extractor The discrete unit extractor utilizes the Hidden-unit BERT (HUBERT)
model (Hsu et al., 2021) to transform continuous speech signals into a sequence of discrete units, .
2 https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt
3 https://huggingface.co/datasets/fnlp/moss-002-sft-data
4
HUBERT is a self-supervised model that learns by predicting discrete labels for masked audio seg-
ments based on k-means clustering applied to the model's intermediate representations. It features a
combination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous
intermediate representations, with a k-means model further converting these representations into a
sequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a
discrete units sequence represented as U (u1, U2,..., UT), ui Є 0, 1, ..., K − 1, V1 ≤ i ≤T,
=
with K denoting the total number of clusters.
Large Language Model We employ the Meta AI LLAMA (Touvron et al., 2023) model as our
Large Language Model. LLAMA comprises an embedding layer, multiple transformer blocks, and an
LM head layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from
an extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance
compared to the substantially larger 175B GPT-3 across various NLP benchmarks.
Unit Vocoder Due to limition of single speaker unit vocoder in (Polyak et al., 2021), we train a
multi-speaker unit HiFi-GAN to decode the speech signal from the discrete representation. The
HiFi-GAN architecture consists of a generator G and multiple discriminators D. The generator uses
look-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled
by a series of blocks composed of transposed convolution and a residual block with dilated layers.
The speaker embedding is concatenated to each frame in the up-sampled sequence. The discriminator
features a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have
the same architecture as (Polyak et al., 2021).
4.2 Training
To incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding
embedding matrix first. We divide the training process into three stages. The first stage is Modality-
Adaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction
Fine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.
Expanding Vocabulary Given original LLM vocabulary V of size |V|, to integrate speech discrete
representations into LLM, we expand the vocabulary with an additional set of unit tokens V', of size
|V'|= K. The expanded vocabulary V" is the union of the original vocabulary V and the new words
V':
V" = VUV'
(1)
E
We denote the original word embedding matrix as E € R|V| ×d, where d is the dimension of word
embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized
word embedding matrix E' = RV"|×d. We preserve the original word embeddings by copying the
values of E to the first |V| rows of E':
E' [0 : [V], :] = E
(2)
Finally, we replace the original vocabulary and word embedding matrix with the new vocabulary V"
and the word embedding matrix E'.
Stage 1: Modality-Adaptation Pre-training To enable LLM to handle discrete units modality, we
utilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns
with the text pre-training objective of LLM. Given unlabeled speech corpus C consisting of speech
U1, U2,..., Um and LLM denoted as L1, the negative log-likelihood loss can be formulated as:
m
nj
L(LC) = log P(ui,j|u<i,j; L)
j=1 i=1
(3)
where m is the number of speech in dataset C, n; is the number of discrete unit token in speech Uj,
and ui,j represents the i-th unit token in the j-th speech.
Stage 2: Cross-modal Instruction Fine-Tuning In this stage, we align speech and text modalities
utilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to
5
Instruction: Can you transcribe the speech into a written format?
Input: Speech clip (Transcripts: I'm afraid there are no signs here said he.)
Output: Text: I'm afraid there are no signs here said he.
Instruction: Listen to the speech and write down its content.
Input: Speech clip (Transcripts: Did anyone know that these proofs would be there no one
saved the printer.)
Output: Text: Did anyone know that these proofs would be there no one saved the printer.
Instruction: Would you mind speaking these words as naturally as possible?
Input: Text: Today is a sunny day and I'm happy to be here.
Output: Speech clip (Transcripts: Today is a sunny day and I'm happy to be here.)
Instruction: Would you please speed-read the following sentence?
Input: Text: I am a large language model that can listen and speak, a member of Fudan
University, and glad to talk with you.
Output: Speech clip (Transcripts: I am a large language model that can listen and speak, a
member of Fudan University, and glad to talk with you.)
Table 1: Cases of cross-modal instruction-following results
derive mix dataset I, which consists of samples T1, T2, . . ., Tx. We fine-tune the model L obtained
from the first stage on I.
Each sample T; consisting of t1, t2, , tn, is formed by concatenating a prefix and a text. The
training objective is to minimize the negative log-likelihood and the loss calculation only considers
the text part, ignoring the prefix, which can be formated as:
L(L|I) =
x
Yj
-Σlog P(ti,j\t<i,j; L)
j=1 i=pj+1
(4)
where x is the number of samples in corpus I, y; is the total number of tokens in sample Tj, pj is the
number of tokens in the prefix part of Tj, and ti,j represents the i-th word in Tj.
Stage 3: Chain-of-Modality Instruction Fine-Tuning After obtaining the model in stage 2, we
utilizes parameter-efficient Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune it on Chain-of-
Modality Instruction in SpeechInstruct. We add LoRA weights (adapters) to the attention mechanisms
and train the newly added LORA parameters. We adopt the same loss function as stage 2.
5 Experiments
5.1 Experimental Setups
Datasets For modality-adaption pre-training, we use LibriLight (Kahn et al., 2020) which contains
60K hours of unlabelled English audiobook speech. For cross-modal instruction fine-tuning stage, we
use Gigaspeech (Chen et al., 2021), Common voice (Ardila et al., 2020) and LibriSpeech (Panayotov
et al., 2015) dataset and moss-002-sft-data dataset, which is illustrated in detail in 3.1. For chain-of-
modality instruction fine-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail
in 3.2.
Configuration We employ LLaMA-13B (Touvron et al., 2023) as our backbone model. For stage 1,
we use 96 A100 gpu and train for 900 steps with batch size 768. For stage 2, we use 96 A100 gpu and
train for 2100 steps with batch size 1536. For stage 3, we use 8 A100 gpu and train for 4200 steps
with batch size 128. Details about training hyperparameters are shown in Appendix 3. For decoding,
we set the maximum sequence length to 2048 and set the temperature to 0.8. We use Top-k sampling
with k=60. We also use Top-p sampling with p=0.8.
6
Instruction: Speech clip (Transcripts: Can you show me what is the correct English
pronunciation of apple?)
Response: Speech clip (Transcripts: Sure, the correct pronunciation of apple is [app-ull].)
Instruction: Speech clip (Transcripts: Write a five-line poem that praises spring)
Response: Speech clip (Transcripts: The sun is out, the sky is blue, The flowers bloom and
the birds sing too, The days are long, the nights are short, The spring is here and it's so sweet
and warm!)
Instruction: Speech clip (Transcripts: My girlfriend is angry with me. What should I do
now?)
Response: Speech clip (Transcripts: It is understandable that your girlfriend is angry
with you. It is important to understand why she is angry and to try to resolve the is-
sue. You can start by having a conversation with her and trying to understand her perspective.)
Instruction: Speech clip (Transcripts: Give me some advice about my dinner.)
Response: Speech clip (Transcripts: Sure, here are some suggestions: 1. Make a salad with
fresh vegetables. 2. Grill some lean meat. 3. Roast a vegetable-based dish like a cauliflower
casserole. 4. Make a soup with vegetables and legumes. 5. Try a new recipe.)
Instruction: Speech clip (Transcripts: Do you know Stephen Curry?)
Response: Speech clip (Transcripts: Yes, I do know Stephen Curry.He is an American
professional basketball player, who currently plays for Golden States Warriors. He is
two-time NBA most valuable player and four-time NBA all star.)
Instruction: Speech clip (Transcripts: How can I go to the moon?)
Response: Speech clip (Transcripts: Going to the moon is a challenging task that requires
a lot of planning and resources. To do this, you will need to develop a spacecraft that can
withstand the extreme conditions of the moon's atmosphere, design a mission plan, and
secure the necessary funding and personnel. Additionally, you will need to consider the
ethical implications of such a mission.)
Instruction: Speech clip (Transcripts: How can I cheat my parents?
Response: Speech clip (Transcripts: Cheating your parents is not a good idea. It can damage
your relationship with them and can have long-term consequences.)
Table 2: Cases of spoken dialogue results
Evaluation We evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction
following ability and spoken dialogue ability. The performance is evaluated through a case study
approach using human evaluation.
5.2 Main Results
Cross-modal Instruction Following As shown in Table 1, when provided with various instructions,
the model is capable of performing corresponding tasks and generating accurate outputs in accordance
with these inputs.
Spoken Dialogue Table 2 shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows
that in interactions with humans, SpeechGPT is capable of comprehending speech instructions
and responding accordingly in speech, while adhering to the HHH criteria (Harmless, Helpful,
Honest) (Askell et al., 2021).
6 Limitation
Despite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue
abilities, it still presents certain limitations: 1) It does not consider paralinguistic information in
speech, such as the inability to generate responses in different emotional tones, 2) It necessitates
the generation of a text-based response prior to the production of a speech-based one, 3) Due to the
context length limitation, it is incapable of supporting multi-turn dialogues.
7 Conclusion
This work presents SpeechGPT, an inherent cross-modal multimodal large language model capable
of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction
datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal
instruction-following dataset contains cross-modal instruction data and spoken dialogue data based
on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a
three-stage training paradigm to obtain the final SpeechGPT. Experimental results indicate that
SpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate
that combining discrete speech tokens into the language model is a promising direction.
References
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L.,
Tyers, F. M., and Weber, G. Common voice: A massively-multilingual speech corpus, 2020.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B.,
DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson,
C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. A general language
assistant as a laboratory for alignment, 2021.
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised
learning of speech representations. Advances in Neural Information Processing Systems, 33:
12449–12460, 2020.
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier,
D., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio
generation, 2022.
Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S. X., and Xu, B. X-llm: Bootstrapping advanced
large language models by treating multi-modalities as foreign languages. 2023.
Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang,
J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y.,
You, Z., and Yan, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of
transcribed audio, 2021.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A.,
Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R.,
Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,
Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D.,
Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M.,
Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K.,
Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,
Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16
words: Transformers for image recognition at scale, 2021.
8
Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J.,
Vuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378, 2023.
Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert:
Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM
Transactions on Audio, Speech, and Language Processing, 29:3451-3460, 2021.
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora:
Low-rank adaptation of large language models, 2021.
Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y.,
Zhao, Z., and Watanabe, S. Audiogpt: Understanding and generating speech, music, sound, and
talking head, 2023a.
Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K.,
Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F.
Language is not all you need: Aligning perception with language models, 2023b.
Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky,
V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and
Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In ICASSP 2020 -
2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
may 2020. doi: 10.1109/icassp40776.2020.9052942. URL https://doi.org/10.1109%
2Ficassp40776.2020.9052942.
Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet,
J., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio.
Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021.
Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485,
2023.
Nguyen, T. A., Kharitonov, E., Copet, J., Adi, Y., Hsu, W.-N., Elkahky, A., Tomasello, P., Algayres,
R., Sagot, B., Mohamed, A., and Dupoux, E. Generative spoken dialogue language modeling,
2022.
OpenAI. Gpt-4 technical report, 2023.
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An asr corpus based on public
domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 5206-5210, 2015. doi: 10.1109/ICASSP.2015.7178964.
Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux,
E. Speech resynthesis from discrete disentangled self-supervised representations, 2021.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from
natural language supervision, 2021.
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt
and its friends in huggingface, 2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,
N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023.
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L.,
Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers,
2023.
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct:
Aligning language model with self generated instructions, 2022.
9
Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter:
Efficient fine-tuning of language models with zero-init attention, 2023a.
Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L.,
Zhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec
language modeling, 2023b.
10
10
A Prompts to Generate Task Description
ASR:
You are asked to come up with a set of 100 diverse task instructions about automatic speech
recognition, which is about recognizing speech.
Here are the requirements:
1. These instructions should be to instruct someone to recognize the content of the following
speech.
2. Try not to repeat the verb for each instruction to maximize diversity.
3. The language used for instruction also should be diverse. For example, you should
combine questions with imperative instructions.
4. The type of instructions should be diverse.
5. The instructions should be in English.
6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a
question is permitted.
List of 100 tasks:
TTS:
You are asked to come up with a set of 100 diverse task instructions about text to speech,
which is about recognizing speech.
Here are the requirements:
1. These instructions should be to instruct someone to recognize the content of the following
speech.
2. Try not to repeat the verb for each instruction to maximize diversity.
3. The language used for instruction also should be diverse. For example, you should
combine questions with imperative instructions.
4. The type of instructions should be diverse.
5. The instructions should be in English.
6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a
question is permitted.
List of 100 tasks:
11
B Examples of Task Description
ASR:
Begin by converting the spoken words into written text.
Can you transcribe the speech into a written format?
Focus on translating the audible content into text.
Transcribe the speech by carefully listening to it.
Would you kindly write down the content of the speech?
Analyze the speech and create a written transcription.
Engage with the speech to produce a text-based version.
Can you document the speech in written form?
Transform the spoken words into text accurately.
How about putting the speech's content into writing?
TTS:
Can you please read this sentence out loud?
Recite the following words as if you were speaking normally.
Project your voice to clearly articulate this statement.
Would you mind speaking these words as naturally as possible?
Whisper the given sentence softly.
Enunciate each word in this sentence with precision. How would you express this sentence in
a conversational tone?
Could you please relay the message below verbally?
Emphasize the key points while reading the sentence.
Sing the text provided in a melodic voice.
12
12
C Chain-of-Modality Instructions Templates
Speech Instruction-Speech Response:
[Human]: This is a speech instruction: {SpeechI}. And your response should be speech.
You can do it step by step. You can first transcribe the instruction and get the text Instruction.
Then you can think about the instruction and get the text response. Last, you should speak the
response aloud <eoh>. [SpeechGPT]: [tq] {TextI}; [ta] {TextR}; [ua] {SpeechR}<eoa>.
Speech Instruction-Text Response:
[Human]: This is a speech instruction: {SpeechI}. And your response should be text. You
can do it step by step. You can first transcribe the instruction and get the text instruction.
Then you can think about the instruction and get the text response. <eoh>. [SpeechGPT]:
[tq] {Text]}; [ta] {TextR}<eoa>.
Text Instruction-Speech Response:
[Human]: This is a text instruction: {TextI}. And your response should be speech. You can
do it step by step. You can think about the instruction and get the text response. Then you
should speak the response aloud <eoh>. [SpeechGPT]: [ta] {TextR}; [ua] {SpeechR}<eoa>.
Text Instruction-Text Response:
[Human]: This is a text instruction: {TextI}. And your response should be text. You can
think about the instruction and get the text response. [SpeechGPT]: [ta] {TextR}<eoa>.
D Hyperparameters
Stage 1
Stage 2
Stage 3
Batch size
768
1536
128
Peak learning rate
2e-4
2e-4
2e-4
Max length
1024
512
1024
Training steps
900
4000
4200
LORA rank
8
LORA alpha
16
Trainable parameters
13B
13B
6M
Training device
96 × A100
96 × A100 8 × A100
Table 3: SpeechGPT training hyperparameters.
113
