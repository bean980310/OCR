arXiv:2305.11337v1 [cs.CV] 18 May 2023
Output: Dreamed Room
Input: Scanned Room
RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent
Geometry and Texture
Liangchen Song12, Liangliang Cao¹, Hongyu Xu¹
Kai Kang¹, Feng Tang¹, Junsong Yuan², Yang Zhao¹
2
Apple Inc. University at Buffalo
1
lsong8@buffalo.edu
llcao@apple.com
"a modern indoor design"
"a cozy wooden cabin"
"a royal castle"
udi
Figure 1. Our method aims at jointly improving geometry and generating texture for an input indoor mesh. The upper figure shows the
true room with a panoramic view and a depth map. Then, given a text prompt (in the middle), our model can synthesize new rooms with
different styles (in the bottom rows). Note that input mesh is often of low quality, and our method can polish both the texture and geometry.
Abstract
The techniques for 3D indoor scene capturing are widely
used, but the meshes produced leave much to be desired. In
this paper, we propose "RoomDreamer", which leverages
powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our
work addresses the challenge of synthesizing both geometry
and texture aligned to the input scene structure and prompt
simultaneously. The key insight is that a scene should be
treated as a whole, taking into account both scene texture
and geometry. The proposed framework consists of two sig-
nificant components: Geometry Guided Diffusion and Mesh
Optimization. Geometry Guided Diffusion for 3D Scene
guarantees the consistency of the scene style by applying
the 2D prior to the entire scene simultaneously. Mesh Op-
timization improves the geometry and texture jointly and
eliminates the artifacts in the scanned scene. To validate the
proposed method, real indoor scenes scanned with smart-
phones are used for extensive experiments, through which
the effectiveness of our method is demonstrated.
1. Introduction
Commercial depth sensors [42] and LiDAR sensors [9]
on mobile devices have opened a new era in 3D scene cap-
turing for millions of users in their everyday lives. How-
ever, the quality of the meshes acquired by these sensors
leaves much to be desired, often exhibiting issues such as
holes, distorted objects, and blurred textures. In addition,
users typically find their surroundings lack variation and
may want to further edit and stylize the scene. To solve
Video results: https://youtu.be/p4xgwj4QJcQ.
1
these problems, this paper demonstrates how to build a 3D
scene from text prompts that matches the geometry of a low-
quality 3D mesh but differs in style.
Our method is motivated by recent advances in 2D con-
tent generation, especially the diffusion models [16,29,32,
33, 40]. One benefit of diffusion models is to allow user-
provided text prompts to guide the image synthesis pro-
cess, and hence is versatile to generate different styles. One
straightforward way of extending 2D content generation to
a higher dimensional space is treating the 3D scene as a col-
lection of multiview images, and synthesizing the images in
a frame-by-frame outpainting manner. However, this ap-
proach will suffer from artifacts, and the generated images
may not match the geometry of the captured scenes.
Given a 3D scene and a text prompt like "modern style",
our work can generate a new scene aligned to the text with
coherent geometry and texture. Our approach involves first
generating the 3D scene's texture, followed by the joint op-
timization of the mesh texture and geometry. We ensure that
the generated texture is consistent with the scene's style by
starting with a cubemap (a 360° image) at the center of the
mesh and then updating the unexplored areas. For the joint
optimization of mesh texture and geometry, we propose to
identify smooth areas within the generated 2D images and
update the mesh geometry accordingly. Fig. 1 shows the
results of our approach.
Our method differs from previous work in the creation of
3D objects from text [20, 25, 36] and the generation of 3D
content based on 2D images [10, 14, 22, 25] in two key as-
pects. Firstly, we consider a novel and practical setting, as it
assumes the presence of a scanned scene, which is common
yet largely unexplored. In our distinct setting, we aim to
refine existing geometry, as opposed to the previous tech-
niques, which primarily focus on generating new geome-
try. Secondly, our approach is motivated by a different in-
sight into 2D diffusion models. Our motivation is on the
good underlying geometry behind each generated 2D im-
age, whereas previous methods are motivated by generating
a set of multi-view 2D images iteratively through diffusion.
Note, we can easily project a mesh-based representation to
2D images, while it is much harder to refine the mesh ge-
ometry from 2D inputs. Extensive experiments demonstrate
that our approach is accurate and flexible to use in many real
applications.
To sum up, the contributions of this work are three-
folded:
• We introduce a novel framework that employs 2D dif-
fusion models to edit a given mesh. Our framework
facilitates the editing and stylization of both geometry
and texture based on textual prompts.
• We design a 2D diffusion scheme for controlling the
diffusion models, leading to the production of a scene
consistent and structurally aligned texture for the input
mesh.
• We conduct extensive experiments using real indoor
meshes scanned with smartphones, which verify the
effectiveness and reliability of our framework.
2. Related Works
The domain of 3D content creation [16, 32, 33] has sig-
nificantly improved in recent years. We consider research
in this field into two categories. Firstly, using 3D ground
truth content for supervision to direct content generation
process [5, 13, 23], which is limited due to the availability
of high-quality 3D ground truth. The second research cate-
gory focuses on using the power of existing 2D image gen-
erators [29] for 3D content creation. Poole et al. [25] pro-
posed Score Distillation Sampling (SDS) to use the struc-
ture of the diffusion model, providing supervisory signals
to a 3D neural field. Concurrently, Wang et al. [36] pro-
posed Score Jacobian Chaining to lift pretrained 2D diffu-
sion models for 3D generation. Lin et al. [20] presented
Magic3D, which represents 3D content first through neu-
ral fields and then meshes to improve the quality and ef-
ficiency of 2D diffusion-guided generation. Fantasia3D [7]
proposed to decompose the 3D asset generation as geometry
and texture generation problems. SDS has also been applied
to convert existing 2D images into 3D models [38,43]. Liu
et al. [21] proposed adapting existing 2D diffusion models
to be camera pose-aware, enabling the direct generation of
multi-view images. Chan et al. [6] proposes synthesizing
a novel view from a single input by incorporating geome-
try priors with stable diffusion backbones. Another recent
work, Text2Room [17], uses 2D diffusion models and depth
estimation models to generate a textured room mesh from
text prompts. The biggest difference between our method
and the above works is that our method is guided by a
scanned mesh, therefore the newly generated 3D scenes will
be accurately aligned with the input scene but with different
styles. For editing existing 3D assets, InstructN2N [14] pro-
posed a methodology to update 2D multiview images itera-
tively. This was based on a 2D image editing model known
as InstructP2P [4]. InstructN2N performed the editing on
2D images, which meant that the ability to dream entirely
new scenes may have been restricted by image-based edit-
ing. On the other hand, our approach relies on a geometry-
controlled 2D diffusion generation, which implies that it is
not hindered by the texture of the original scene.
One big challenge of 3D data collection lies in the im-
perfect scene scanning results. Because the Lidar on mo-
bile devices is of limited power and resolution, some parts
of the scenes are often missed in the point clouds. There
have been quite a few researches to improve 3D reconstruc-
tions with generative priors, such as self-supervised genera-
2
tion [8], retrieval-based generation [30], style transfer [18].
Besides the reconstruction problem, some prior research has
treated 3D indoor scene generation as an object layout pre-
diction problem [24, 28, 37]. After predicting the layout,
objects are retrieved from a 3D furniture dataset such as
3D-FRONT [12] and placed within the scene. Other ap-
proaches, such as Plan2Scene [35], use the floorplan and
image observations of an indoor scene to predict a textured
mesh for the entire scene. Meanwhile, GSN [11], GAUDI
[3], and CC3D [1] focus on generating images of indoor
scenes through the use of neural radiance fields. These re-
search works are inspiring to our work. In practice, we fo-
cus on refining indoor scene meshes, especially the smooth-
ness of 3D geometry and the match of geometry and visual
textures. We will explain more details in later sections.
3. Method
Our approach's input includes a 3D mesh with both ge-
ometric and texture information, as well as user-provided
text prompts. Our method is composed of two steps: First,
we render the 3D scene to 2D images, and synthesize new
styles using a new diffusion process; Then we reconstruct a
new 3D mesh with the new textures and polished geometry.
An overview of our method is shown in Fig. 2.
3.1. Geometry Guided Diffusion for 3D Scene
Synthesizing a new 3D scene is more challenging than
synthesizing a 2D image because standard diffusion models
[33] can easily create inconsistency across different views.
A straightforward approach for generating scene texture us-
ing 2D image diffusion models begins with a randomly po-
sitioned camera and iteratively samples neighboring cam-
eras to outpaint [31, 39] the unobserved area, as depicted
in Fig. 3. However, we have observed that this baseline
method produces noticeable artifacts (Fig. 6), which can be
attributed in part to the limited outpainting capability of 2D
diffusion models.
To avoid the artifacts brought by the view-by-view out-
painting generation process, we propose to first generate a
360° image with a central view of the scene. A panorama
image can be generated with 2D diffusion models by simply
extending the diffusion process to cubemap patches [2,41].
Unlike the classic diffusion model which is conditioned
on text prompts, our method is conditioned on both a text
prompt Ctext and a depth map D, thus the diffusion step is
Xt-1 = f(Xt, Ctext, D). Following the previous work, we
denote the diffusion model as a mapping function denoted
as f: (RH 7×³, C) → RH×W×3, where RH×W×3
repre-
sent the spaces for images with size H×W, and C represent
the spaces of conditional prior including both prompt and
image depth. Further, we denote X0 = f→0 (Ctext, D) as
the whole diffusion process from random noise and condi-
tioned on the text and depth.
HXWX3
However, directly using depth map for controlling the
generation of cubemap may lead to inconsistency, since the
depth value is correlated with camera poses. Different cam-
era poses lead to inconsistency of the depth value in cube-
map faces. Fig. 5(a) illustrates the inconsistency of depth
map. The depth map associated with each camera is repre-
sented in terms of the distance between the camera and the
planes. Consequently, the depth values can largely differ
from one view to another for the same plane, and lead to
artifacts. To further reduce the inconsistency, we consider
distance map
D which represents the geometric distance be-
tween points and the camera origin. Let a point with world
coordinate P and its associated screen coordinate be (u, v),
then the (u, v) pixel on the distance map Â is ||p – o||,
where o is the world coordinate of the camera origin. A
comparison between the depth map and the distance map is
shown in Fig. 4.
-
Distance map
Ô and depth map D have different proper-
ties in terms of controlling the diffusion process. Structures
are well aligned with RGB images in D, but distorted in Ô.
For example, image Laplacian on the planes will be zero
in D, but not in D. However, the border in cubemap will
with a smoother change with D, which could be observed
in Fig. 2(b).
To achieve both realistic geometric alignment and border
consistency, we propose a blending scheme. For an image
patch p at the intersection of cube maps Ia and IƖ, let ra
and r be the ratios of pixels from I and I in the patch,
respectively. We then define λ = |ra – rɩ|. Each step of the
denoising process is calculated using the equation:
-
Xt−1 = λf (Xt, Ctext, Dp)+(1 −λ) ƒ (Xt, Ctext, Ôp), (1)
where Dp and Dp are the depth and distance maps respec-
tively for the patch p being denoised. After generating a
cubemap, the mesh texture is subsequently updated using
a differentiable renderer [19]. We then randomly sample
cameras in the scene, and the areas not captured by the
360° image are updated through masked generation (i.e.,
outpainting) with the diffusion model. For judging areas
captured or not by the cubemap, we can simply project the
vertices to the previous cameras and see which vertices are
occluded.
3.2. Mesh Optimization
Both the input and output of RoomDreamer are 3D
meshes. We denote a mesh as (V, F, Vc), where V, F, Vc are
the vertices, faces, and the color of vertices, respectively.
For a specific camera π, we can render a depth map D and
RGB image X at this view:
X=Rx(V, F, VC, π)
D =RD(V, F,T)
(2)
(3)
3
input geometry
sample cubemap camera sample random camera
differentiable render
geometry
guided
+ prompt
"cozy wooden cabin..."
2D diffusion
outpainting
input texture
input geometry
same cameras
differentiable render
RGB
update
texture
Ltexture
monocular
depth
estimator
update
vertices
faces
depth
Lgeometry
Geometry Guided Diffusion for 3D Scene
pseudo depth
Mesh Optimization
Figure 2. The overall framework of our method. Firstly, in the Geometry Guided Diffusion stage for 3D scenes, we create a cubemap
representing the scene, followed by outpainting the uncovered areas of the cubemap, as detailed in Sec. 3.1. Subsequently, we optimize
the mesh texture and geometry. For the geometry optimization, we utilize monocular depth prediction as pseudo supervision and align the
smooth areas of the scene, as elaborated in Sec. 3.2.
far
sample view 1
sample view 2
surface
diffuse
masked
diffuse
depth
near
depth map
far
optimize
(a) View-by-view outpainting
sample a cubemap
at center
diffuse
optimize
camera origin
distance
distance map near
(b) Cubemap based texture
Figure 3. Methods for generating scene texture. The step “dif-
fuse" means generating a 2D image with diffusion models. The
"optimize" means updating the mesh texture with the 2D gener-
ated images (cf., Eq. (5)). (a) A straightforward baseline based
on outpainting with 2D diffusion models. Outpainting is achieved
by masked diffusion and the gray area means the masked area re-
mains unchanged through the diffusion. (b) Generating a cubemap
for the scene, then optimizing the mesh texture.
Figure 4. Illustration of the depth map and the distance map.
Depth map measures the length between the object plane to the
screen plane, while distance map measures the length between
points to the camera origin.
where R denotes the rendering function,
In our implementation, we use a differentiable render
[19], with which we can back-propagate the gradients to
3D to generate a mesh from synthesized images. Let {πk}
represent a set of cameras, we can generate a sequence of
images {X} using the method in 3.1. Then we can define
4
cubemap face A cubemap face B
(a) diffusion with depth map
cubemap face A cubemap face B
geometry should also be planar. To model this observa-
tion, we first reconstruct the depth map Den from {X}
using an off-the-shelf monocular depth estimator E (e.g.,
MiDaS [27]). Then we define the condition for planar re-
gions
|ADgen (u, v)| < T
k
(6)
where ▲ is the Laplacian of the depth map Dk, and 7 is a
threshold for determining smooth areas.
Then, we denote the smooth area as P =
{(u, v)|▲Ɗgen (u, v)| < τ}. We expect on P, recon-
structed depth map D is as smooth as possible, i.e., with
Laplacian close to zero. Thus, we get another loss function:
Lgeometry = |ADk (u, v)|,
k (u,v)EP
(7)
project onto the wall
where Dk = RD(V, F‚Ã²)
(b) diffusion with distance map
Figure 5. Different controlling effects of the depth map and the
distance map. The depth map exhibits rapid change at the joint
boundary of the two faces of the cube map. Conversely, the dis-
tance map changes smoothly. Generating consistent cube maps
with depth control becomes challenging, whereas the employment
of distance map results in more consistent texture. However, the
distance map O results in artifacts such as the window on the wall, as
the diffusion model is conditioned on the depth map during train-
ing.
a texture-based loss:
Ltexture
Σ ||RX (V, F, VC, T¹³) — Xh³||²,
(4)
k
Then we get a baseline method of reconstructing the mesh
texture by gradient descent:
Vc + Vc
γ
aLtexture
avc
- Ve-γΣ ORx (V, F, V, Tk)
aLtexture
Rx (V, F, VC, πk²)
OVC
k
(5)
Note that Eq. (5) can only update V but not the geometry
V, F. This is because the differentiable render [19] cannot
compute the gradient for geometry, i.e., Rx €0,
0.
ǝv
=
aRx
ƏF
A further step is to optimize jointly V, F with Vc. Be-
cause the input mesh often exhibits low-quality geometry
(e.g., with holes), we hope the geometry V, F can be ad-
justed to match the image sequences {X}). An essen-
tial observation is that when a synthesized scene contains
a smooth region, such as a planar shape, the reconstructed
Besides the geometry loss, the generated images are used to
update the texture of the scene with the following loss.
The overall progress of updating the scene mesh can be
represented as follows,
VeVe Y
aLtexture
ave
Lgeometry
VV-Y
(8)
FF-Y-
av
ƏL geometry
OF
An algorithm overview is shown in Algorithm 1.
4. Experiments
Our problem setting assumes inputting with a scanned
room mesh, which has not been extensively explored in the
existing literature. We compare the result of our method
with two groups of works: (1) Ablation studies of our
own baselines, but without submodules such as geometry-
guided diffusion, distance map, smooth region regulariza-
tion, etc. (2) NeRF-based methods, including Score Distil-
lation Sampling (SDS) [25] and InstructN2N [14]. Since we
can project our input mesh to generate multi-view images,
which will be used to reconstruct NeRF. Note the recon-
struction of NeRF is more computationally expensive, and
the reconstructed geometry of NeRF is not always accurate,
so we also use the ground truth depth from the mesh input to
boost the reconstruction performance of InstructN2N [14]
to get a fair comparison.
4.1. Dataset and Implementation Details
We conducted experiments on the ARKitScenes dataset
[9], which comprises real indoor scenes captured by an
5
Input
+ "chinese palace, imperial palace"
5 赤血
+ "japanese style, zen, tatami"
+ "wooden cabin"
+ "military base, army"
Ours
Outpainiting
Figure 6. Qualitative comparisons of the output mesh. Outpainting is the baseline method in that textures are outpainted sequentially, while
we treat the scene as a whole. Strip shape artifacts can be observed in the outpainting baseline.
iPhone. Our method is evaluated qualitatively and quantita-
tively on the validation set of ARKitScenes, which covers a
diverse range of room types and floor plans.
To generate the cubemap, we set the camera origin at
the center of the mesh. For generating depth-based images,
we utilize ControlNet [40] and maintain the default hyper-
parameters, such as the guidance scale and the number of
diffusion steps T. To cover the regions not included in the
cubemap, we randomly select K = 100 cameras around the
center and use the masked generation mode of the diffusion
model. We predict the monocular depth using MiDaS [27].
During the optimization process, we use the Adam opti-
mizer with a learning rate of 0.001 for optimizing both the
geometry (V, F) and the vertices color Ve. The optimiza-
tion is run for a total of N = 1000 steps. A scene takes
around 15 mins to process with one A100 GPU.
6
"wooden cabin"
1.PD
"space cabin"
Inputs
Ours
SDS [25]
InstructN2N [4,14]
Figure 7. Comparisons with other 2D diffusion based 3D editing methods. SDS [25] can improve the geometry, but the generated texture
is kind of blurry. InstructN2N [14] is limited by its backbone InstructP2P [4], which is a purely image-based editing method, and thus may
be misled by the presented input image. Our scheme can well handle geometry and texture generation.
Algorithm 1 Overall pipeline of our method
Input:
System requirements: 2D diffusion model f, monocu-
lar depth estimator E
From user: mesh (V, F, Vc), text prompt Ctext
Output: updated mesh (V*, F*, V*) with the new style
Step 1: Geometry Guided Diffusion for 3D Scene
1: set cubemap cameras at scene center
2: acquire cubemap depth D and distance D
3: generate cubemap Xcube from f, Ctext, D, D
Eq. (1)
▷ use
4: sample K random cameras {k} > for uncovered area
5: for k 1 to K do
6:
7:
8:
9:
if
ПК
sees areas not covered by X cube then
acquire depth Dk from πk
generate image X½ from f, Ctext, Dk
end if
10: end for
Step 2: Mesh Optimizing
11: for X in all generated images do
12:
get Den = E(X) using monocular depth estima-
tion
13: end for
14: for n 1 to N do
=
15: Update the 3D mesh using Eq. (8).
16: end for
4.2. Comparing with Baselines
We first conduct a qualitative assessment of our approach
in contrast to the outpainting baseline. The results of this
analysis are depicted in Fig. 6, wherein we compare our
cubemap based scene texture generation with the outpaint-
ing baseline. Evidently, the scene generated by outpaint-
ing exhibits strip-like artifacts that arise from the flawed
outcome of the masked generation mode of the diffusion
models. Examples of strip-like artifacts can be observed
on the walls of the scene. Conversely, our technique con-
sistently produces images with a high-quality and uniform
style that is retained throughout the entire scene. This supe-
riority can be attributed to the employment of our cubemap
texture generation scheme.
Furthermore, we delve into the impact of blending the
distance map (cf., Eq. (1)) during cubemap diffusion in
Fig. 8. Notably, a crucial difference can be observed in the
region demarcated by the orange box, which represents a
patch of the joint area of two cubemap faces. Upon assess-
ing the input scene, we establish that the wall in this area
is distorted, implying that the depth signal for diffusion is
distorted as well. Specifically, the second row of the figure
shows that this area has been treated as a turning corner of
two walls instead of a single plane, which indeed is. This
generated corner is a result of the distortion in the depth
controlling signal. Upon incorporating distance map con-
trolled denoising during the diffusion as shown in the third
the patch is correctly considered as a single wall plane.
Apart from the joint area, we also observe other benefits
such as the alignment of areas as indicated by the red ar-
row,
rows.
In Fig. 9, we present a comparison between the original
input scene's geometry and the updated scene's geometry.
The text prompt used for the generation is “a royal castle”.
It can be observed that after using our method, the mesh is
smoother than before. Additionally, our method success-
fully fills in some holes in the mesh.
7
input scene
w/o distance
R
with distance
cubemap face A
cubemap face B
R
cubemap face C
cubemap face D
cubemap face A
Figure 8. Cubemap generation with and without the distance map blending step (cf., Eq. (1)). Without distance map blending, the 2D
diffusion tends to generate two planes on the border area of the cubemap (i.e., the orange box area). With distance map blending, the border
area is treated as one plane.
original geometry
updated geometry
Figure 9. Visualization of the geometry editing.
4.3. Comparing with NeRF based Approaches
In this subsection, we compare our method to two promi-
nent 2D diffusion based 3D generation methods, namely
Score Distillation Sampling (SDS) [25] and InstructN2N
[14]. We used the open-source version [34] of SDS with
Stable Diffusion as the 2D diffusion model.
Next, we present a comparative analysis of the perfor-
mance of our approach versus recently proposed 2D-based
3D editing methods, as depicted in Fig. 7. To ensure a fair
comparison, we employ the depth-guided diffusion model
(i.e., the same as ours) for our SDS experiments. Upon
inspection of the results, we observe that SDS can result
in blurred effects, which is similar to the phenomenon re-
cently reported in a study on 2D image generation utiliz-
ing SDS [15]. For InstructN2N [4, 14], we note that its
performance is primarily influenced by the efficacy of its
backbone, i.e., InstructP2P [4]. It is important to mention
that InstructP2P solely relies on the input image and is not
conditioned on geometry like depth. Consequently, in In-
structN2N, we observe instances where the model is misled
by empty regions (i.e., white areas) in the input image, such
as the white space on the sofa in the first row. Moreover,
employing a purely image-based editing approach may re-
strict the diversity of the generated images, as demonstrated
by the wooden floor in the second row. The comparisons
presented in this figure illustrate that our proposed scheme
is capable of successfully generating texture with a subop-
timal quality input mesh.
4.4. Quantitative Evaluation
The quality of a stylized indoor scene is usually sub-
jective, but we have adopted the evaluation approach from
[4, 14] for quantitative analysis of the generated results.
This evaluation process is based on the embedding provided
by CLIP [26]. There are two metrics used for evaluation.
The first metric, called "Text-Image Similarity," computes
the inner product between the given text prompt and the
generated image. Higher values indicate higher similarity
between the text and image vectors, which implies a smaller
angle between them. The second metric, referred to as "Di-
rection Consistency," assesses the consistency of the gener-
ated scenes across different views. The score is computed
as follows: Given two CLIP embeddings of the original in-
put views, denoted as oa and b, and two CLIP embeddings
of the generated views, denoted as g₁ and gb, the score is
8
Text-Image
Similarity
InstructN2N [4,14]
0.2022
SDS [25]
0.1532
Ours
0.2543
Direction
Consistency
0.5416
0.4184
0.5281
Table 1. Quantitative comparisons with other editing methods. For
the two metrics (cf., Eq. (9)), a higher value indicates better per-
formance.
w/o Cubemap
Text-Image
Similarity
0.2123
w/o Distance
0.2274
w/o Geo Optimize
0.2017
View
Consistency
0.5116
0.4678
0.5120
Full Model
0.2543
0.5281
Table 2. Quantitative ablation. "w/o Cubemap” is the outpainting
baseline. "w/o Distance" means removing the distance map based
blending scheme. "w/o Geo Edit" means we do not update the
geometry with the pseudo depth supervision (i.e., no Lgeometry).
calculated as
(ga - oa) · (gb
ga
.
Ob)
(9)
A lower score implies that the direction of generation is bet-
ter aligned across different views, indicating greater con-
sistency in the scene generation. To evaluate the perfor-
Imance, we select a total of 80 meshes from the validation
set and create 15 textual prompts. For each mesh, we ran-
domly select 4 different views to test. For calculating Direc-
tion Consistency, we use the sampled 4 views and one re-
maining view, resulting in 4 original-generated pairs. Thus,
we obtain a total of 4800 pairs of image-text and original-
generated pairs which are used to calculate the evaluation
metrics.
We first present a comparison of our method with SDS
and InstructN2N in Table 1. Our approach outperforms
both SDS and InstructN2N with higher similarity and con-
sistency scores. The similarity score of SDS appears to
be relatively low, which may be attributed to the blurring
effect. The high direction consistency score of SDS re-
flects the consistent blurring effect. InstructN2N achieves a
lower text-image similarity score due to the restriction of its
purely image-based editing backbone (InstructP2P). How-
ever, the direction consistency score of InstructN2N is good,
indicating the effectiveness of its dataset updating scheme.
We then carry out some ablation studies, as shown in
Table 2. Our outpainting model exhibits good direction
consistency but a low text-image similarity. Removing the
distance map blending scheme adversely impacts the text-
image similarity score. Furthermore, we find that removing
the geometry optimization has a noticeable negative impact
on the text-image similarity score. This indicates the neces-
sity of optimizing the geometry when seeking to customize
and stylize a scanned mesh.
5. Conclusion
In this paper, we tackle the problem of synthesizing a
3D interior scene from text prompts based on a scanned
indoor mesh input. We propose a solution that capitalizes
on the capabilities of 2D diffusion text-to-image generative
models. The primary challenge lies in generating coherent
3D geometry and textural information from the 2D gener-
ative priors. To ensure the consistent visual appearance of
the whole scene, we first develop a geometry-guided 3D
scene texture generation technique. Our key idea is to gen-
erate a cubemap of the space, thus achieving a consistent
style throughout the different views. We then jointly opti-
mize mesh geometry and texture, based on the pseudo-depth
estimated by a monocular depth estimator. Our claimed
contributions are validated via experiments on real scanned
meshes.
Acknowledgements
We are grateful to our colleagues at Apple Inc. for their
valuable support in enhancing the quality of this work.
References
[1] Sherwin Bahmani, Jeong Joon Park, Despoina Paschali-
dou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas,
and Andrea Tagliasacchi. Cc3d: Layout-conditioned
generation of compositional 3d scenes. arXiv preprint
arXiv:2303.12074, 2023. 3
[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
generation. arXiv preprint arXiv:2302.08113, 2023. 3
[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-
ter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent
Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al.
Gaudi: A neural architect for immersive 3d scene genera-
tion. Advances in Neural Information Processing Systems,
35:25102-25116, 2022. 3
[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-
structpix2pix: Learning to follow image editing instructions.
In CVPR, 2023. 2, 7, 8, 9
[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 16123-16133, 2022. 2
[6] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W
Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini
9
De Mello, Tero Karras, and Gordon Wetzstein. Generative
novel view synthesis with 3d-aware diffusion models. arXiv
preprint arXiv:2304.02602, 2023. 2
[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873, 2023. 2
[8] Angela Dai, Christian Diller, and Matthias Nießner. Sg-nn:
Sparse generative neural networks for self-supervised scene
completion of rgb-d scans. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 849-858, 2020. 3
[9] Afshin Dehghan, Gilad Baruch, Zhuoyuan Chen, Yuri Fei-
gin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry,
Brandon Joffe, Arik Schwartz, and Elad Shulman. Ark-
itscenes: A diverse real-world dataset for 3d indoor scene
understanding using mobile RGB-D data. In Joaquin Van-
schoren and Sai-Kit Yeung, editors, Proceedings of the Neu-
ral Information Processing Systems Track on Datasets and
Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,
December 2021, virtual, 2021. 1,5
[10] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen
Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov,
et al. Nerdi: Single-view nerf synthesis with language-
guided diffusion as general image priors. arXiv preprint
arXiv:2212.03267, 2022. 2
[11] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava,
Graham W Taylor, and Joshua M Susskind. Unconstrained
scene generation with locally conditioned radiance fields. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 14304–14313, 2021. 3
[12] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming
Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-
qiang Zhao, and Hao Zhang. 3d-front: 3d furnished rooms
with layouts and semantics. In 2021 IEEE/CVF International
Conference on Computer Vision, ICCV 2021, Montreal, QC,
Canada, October 10-17, 2021, pages 10913–10922. IEEE,
2021. 3
[13] Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoor-
thi. Nerfdiff: Single-image view synthesis with nerf-
guided distillation from 3d-aware diffusion. arXiv preprint
arXiv:2302.10109, 2023. 2
[14] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-
ing 3d scenes with instructions. 2023. 2, 5, 7, 8, 9
[15] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-
noising score. arXiv preprint arXiv:2304.07090, 2023. 8
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840-6851, 2020. 2
[17] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson,
and Matthias Nieẞner. Text2room: Extracting textured
3d meshes from 2d text-to-image models. arXiv preprint
arXiv:2303.11989, 2023. 2
[18] Lukas Höllein, Justin Johnson, and Matthias Nießner.
Stylemesh: Style transfer for indoor 3d scene reconstruc-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 6198-6208,
2022. 3
[19] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,
Jaakko Lehtinen, and Timo Aila. Modular primitives for
high-performance differentiable rendering. ACM Transac-
tions on Graphics, 39(6), 2020. 3,4,5
[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2023. 2
[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
Zero-shot one image to 3d object, 2023. 2
[22] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Andrea Vedaldi. Realfusion: 360° reconstruction of any ob-
ject from a single image. arXiv e-prints, pages arXiv-2302,
2023. 2
[23] Norman Müller, Yawar Siddiqui, Lorenzo Porzi,
Samuel Rota Bulò, Peter Kontschieder, and Matthias
Nieẞner. Diffrf: Rendering-guided 3d radiance field
diffusion. arXiv preprint arXiv:2212.01206, 2022. 2
[24] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten
Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres-
sive transformers for indoor scene synthesis. Advances in
Neural Information Processing Systems, 34:12013-12026,
2021. 3
[25] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,
2022. 2, 5, 7, 8, 9
[26] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
Ilya Sutskever. Learning transferable visual models from nat-
ural language supervision. In ICML, 2021. 8
[27] René Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 44(3), 2022. 5,6
[28] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flex-
ible indoor scene synthesis via deep convolutional genera-
tive models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 6182-
6190, 2019. 3
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684-10695, 2022. 2
[30] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,
Matthias Nieẞner, and Angela Dai. Retrievalfuse: Neural
3d scene reconstruction with a database. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 12568-12577, 2021. 3
[31] Josef Sivic, Biliana Kaneva, Antonio Torralba, Shai Avidan,
and William T Freeman. Creating and exploring a large
10
10
photorealistic virtual space. In 2008 IEEE Computer Soci-
ety Conference on Computer Vision and Pattern Recognition
Workshops, pages 1-8. IEEE, 2008. 3
[32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 2
[33] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems, 32, 2019. 2, 3
[34] Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with
stable-diffusion, 2022. https://github.com/ashawkey/stable-
dreamfusion. 8
[35] Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa,
Angel X Chang, and Manolis Savva. Plan2scene: Convert-
ing floorplans to 3d scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 10733-10742, 2021. 3
[36] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lift-
ing pretrained 2d diffusion models for 3d generation. arXiv
preprint arXiv:2212.00774, 2022. 2
[37] Kai Wang, Manolis Savva, Angel X Chang, and Daniel
Ritchie. Deep convolutional priors for indoor scene syn-
thesis. ACM Transactions on Graphics (TOG), 37(4):1–14,
2018. 3
[38] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,
and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild
2d photo to a 3d object with 360° views. 2022. 2
[39] Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, and Shuicheng
Yan. Very long natural scenery image prediction by outpaint-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pages 10561-10570, 2019. 3
[40] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543, 2023. 2,6
[41] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen,
and Ming yu Liu. Diffcollage: Parallel generation of large
content with diffusion models. In CVPR, 2023. 3
[42] Zhengyou Zhang. Microsoft kinect sensor and its effect.
IEEE MultiMedia, 19(02):4-10, 2012. 1
[43] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tilling view-conditioned diffusion for 3d reconstruction. In
CVPR, 2023. 2
11
