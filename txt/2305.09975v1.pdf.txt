--- Page 1 ---
arX1v:2305.09975v1 [cs.CL] 17 May 2023

Smart Word Suggestions for Writing Assistance
Chenshuo Wang! Shaoguang Mao*! Tao Ge*, Wenshan Wu’, Xun Wang?

Yan Xia*, Jonathan Tien*, Dongyan Zhao

1,2,4,5

'Wangxuan Institute of Computer Technology, Peking University
Center for Data Science, Peking University *Microsoft
‘Institute for Artificial Intelligence, Peking University
State Key Laboratory of Media Convergence Production Technology and Systems
{shaoguang.mao,wenshan.wu, tage, xunwang, yanxia, jtien}@microsoft.com,
iven@ivenwang.com, zhaody@pku.edu.cn

Abstract

Enhancing word usage is a desired feature for
writing assistance. To further advance research
in this area, this paper introduces "Smart Word
Suggestions" (SWS) task and benchmark. Un-
like other works, SWS emphasizes end-to-end
evaluation and presents a more realistic writ-
ing assistance scenario. This task involves
identifying words or phrases that require im-
provement and providing substitution sugges-
tions. The benchmark includes human-labeled
data for testing, a large distantly supervised
dataset for training, and the framework for
evaluation. The test data includes 1,000 sen-
tences written by English learners, accompa-
nied by over 16,000 substitution suggestions
annotated by 10 native speakers. The train-
ing dataset comprises over 3.7 million sen-
tences and 12.7 million suggestions generated
through rules. Our experiments with seven
baselines demonstrate that SWS is a chal-
lenging task. Based on experimental anal-
ysis, we suggest potential directions for fu-
ture research on SWS. The dataset and related
codes is available at https://github.com/
microsoft/SmartWordSuggestions.

1 Introduction

Writing assistance is a widely used application of
natural language processing (NLP) that helps mil-
lions of people. In addition to common features
like grammatical error correction (Ng et al., 2014;
Bryant et al., 2017), paraphrasing (Fader et al.,
2013; Lin et al., 2014) and automatic essay scoring
(Song et al., 2020), providing word suggestions is a
desired feature to enhance the overall quality of the
writing. As illustrated in figure 1, the word “inti-
mate” in the first sentence should be replaced with
“close”, as “intimate” is not suitable for describing
relationships between colleagues.

'This work was performed during the first author’s intern-
ship at Microsoft Research Asia
Corresponding Author

Sentence: With the help of the intimate cooperation
of our group members, we developed a new method.
Improvable target: intimate

Substitution suggestion: close

Suggestion type: refine-usage

Reason: Word “intimate” is for friends or lovers, the
cooperation between colleagues should use “close”.

Sentence: If you learn from others, it would be more
possible to communicate with different people.
Improvable target: possible

Substitution suggestion: likely

Suggestion type: refine-usage

Reason: The sentence wants to express “more likely’
rather than “have chance to”, “likely” is more proper.

Sentence: This will distract their attention.
Improvable target: attention

Substitution suggestion: focus

Suggestion type: diversify-expression
Reason: “Focus” is the synonyms of “attention”.

Figure 1: Examples for Smart Word Suggestions
(SWS). All samples consist of sentences annotated with
multiple improvable targets, each of which is further an-
notated with multiple substitution suggestions. To save
space, the sentences are simplified, and only one target
and one suggestion are presented per case. The sugges-
tions can be divided into two types: refine-usage and
diversify-expression, which are described in section 3.1

In this paper, we introduce the task and bench-
marks of Smart Word Suggestion (SWS). Figure
2 shows the definition of SWS. The goal of SWS
is to identify potential improvable targets in the
form of words or phrases within a given context,
and provide substitution suggestions for every im-
provable target. These suggestions may include
correcting improper word usage, ensuring that lan-
guage usage conforms to standard written conven-
tions, enhancing expression, and so on. Specif-
ically, we categorize these suggestions into two
types: refine-usage and diversify-expression.

Lexical Substitution (LS) (McCarthy and Nav-
igli, 2007; Kremer et al., 2014; Lee et al., 2021) is

--- Page 2 ---
Input Sentence

With the help of the intimate cooperation of our group members , we pointed out a new method .

Sub-task 1: Improvable Target Detection

Improvable Targets

With the help of the intimate cooperation of our group members , we pointed out a new method .
n 1 1 1

Sub-task 2: Substitution Suggestion

v ¥ v v
support / close collaboration developed
Substitution Suggestions _ assistance /
guide / aid

Figure 2: Task definition of Smart Word Suggestions (SWS). SWS consists of two sub-tasks: improvable target
detection and substitution suggestion. A sentence contains multiple improvable targets, and a target has multiple

substitution suggestions.

the most relevant research benchmark in the field.
LS systems aim to provide substitute words that
maintain the original meaning of a given word
within a sentence. However, in practical situations,
it is important to recognize words that can be im-
proved or replaced. Identifying these targets is
crucial for practical use and a necessary step for
making accurate substitution suggestions. In order
to reproduce the real-world scenarios, we design
SWS as an end-to-end process that takes a sentence
as input and provides substitution suggestions for
all improvable targets as output.

The SWS benchmark includes human-labeled
data for testing, a large distantly supervised dataset
for training, and a corresponding framework for
evaluation. For testing, we collect 1,000 segments
from English learners’ essays, and ask ten anno-
tators to identify improvable targets and provide
substitution suggestions. The high level of agree-
ment among the annotators confirms the quality
of the annotation. For weakly supervised training,
we compile a large amount of distantly supervised
data by using a synonym thesaurus to randomly
substitute words in corpus. We also provide set-
tings for both end-to-end evaluation and sub-task
evaluation.

To investigate the challenges, we implemented
seven baselines, including knowledge-driven meth-
ods, state-of-the-art lexical substitution methods,
and end-to-end approaches for SWS. The exper-
imental results show that the performance of the
existing lexical substitution methods decreases sig-
nificantly when applied to SWS. Additionally, the
end-to-end methods we designed struggle to iden-
tify and improve targeted words or phrases. De-
tailed analysis and discussions on the results sug-
gest several areas for further research.

To conclude, our contributions are as follows:

¢ Introducing the SWS task for writing assis-

tance, and providing a benchmark with high-
quality human-labeled testing data and large
distantly supervised training data.

* Developing the evaluation framework for
SWS, and conducting extensive evaluations
on the provided baselines.

¢ Identifying several directions for further re-
search on SWS through analysis.

2 Related Works

We begin by comparing SWS with three related
tasks, highlighting the unique value of our work.

2.1 Lexical Substitution

Lexical substitution (LS) (McCarthy and Navigli,
2007; Kremer et al., 2014; Lee et al., 2021) is the
task of providing substitute words for a specific
word in a sentence. There are some major distinc-
tions between the SWS and LS.

(1) In LS, the target word is already provided,
while in SWS, the system needs to detect the im-
provable targets first.

(2) LS focuses on finding synonyms that main-
tain the meaning of both the word and the sen-
tence. On the other hand, SWS is designed for
writing assistance scenarios, so the substitutions
aim to improve the writing of the sentences. LS
focuses on word sense disambiguation in the con-
text, which doesn’t require any "improvement".
Here is an example in the LSO7 dataset: This is
clearly a terrible and shameful blot on
UN peacekeeping. One of the substitutions is
"terrible" + "very bad". This substitution doesn’t
meet the SWS’s requirement as the use of "very
bad" is less accurate, and the substitution worsens
writing.

(3) LS uses lemmatized annotations for the tar-
get word and substitutions, while SWS extracts
annotations directly from the sentence and requires

--- Page 3 ---
that the substitutions fit grammatically within the
sentence to evaluate the model’s end-to-end perfor-
mance.

2.2 Grammatical Error Correction

Grammatical error correction (GEC) (Ng et al.,
2014; Bryant et al., 2017) also shares some similar-
ities with SWS. Ng et al. (2014) pointed that more
than 85% of the corrections in GEC are word-level
and that these corrections improve users’ writing
as well. However, the substitution suggestions pro-
vided by SWS do not include suggestions for cor-
recting grammatical errors. Instead, SWS focuses
on identifying and improving word or phrase usage.
It is worth noting that the source sentences in the
SWS test set are first processed by a GEC model
(Ge et al., 2018) and then further checked by hu-
man annotators to ensure no grammatical errors in
the inputs. In the writing assistant, SWS is the next
step following GEC.

2.3. Paraphrase Generation

Paraphrase generation (PG) (Fader et al., 2013; Lin
et al., 2014) aims to alter the form or structure of a
given sentence while preserving its semantic mean-
ing. PG has a variety of potential applications, such
as data augmentation (Iyyer et al., 2018), query
rewriting (Dong et al., 2017), and duplicate ques-
tion detection (Shah et al., 2018). PG is different
from SWS in two main ways: (1) SWS places a
greater emphasis on improving writing by identi-
fying and correcting inappropriate word usage or
providing diverse expression options. (2) SWS
focuses on substitution suggestions of words or
phrases, and evaluations are based on word level.
In contrast, PG directly measures performance at
the sentence level.

3 Data Collection

This work is to construct a Smart Word Sugges-
tion benchmark that accurately represents writing
assistance scenarios. For evaluation, we collect
sentences from English learners and use human an-
notations in accordance with McCarthy and Navigli
(2007) and Kremer et al. (2014). For training, we
compile a large-scale, distantly supervised dataset
from Wikipedia (Erxleben et al., 2014; Vrandecié
and Krétzsch, 2014).

3.1 Human-Annotated Data Collection

Human-annotated data is obtained through a three-
stage process: (1) cleaning corpus data from En-

glish learners’ essays, (2) labeling improvable tar-
gets and corresponding substitution suggestions,
and (3) merging annotations and filtering out low-
confidence annotations.

Stage 1: Corpus Cleaning. We collect essays
written by undergraduate English learners via an
online writing assistance platform '!. We divide
them into individual sentences. To avoid annota-
tors making corrections beyond SWS, the sentences
are refined with following actions: (1) removing
sentences that have unclear meanings. (2) apply-
ing a correction model (Ge et al., 2018) to correct
grammatical errors. (3) asking human reviewers
to double-check for any remaining grammatical er-
rors. Additionally, we filter out short sentences as
they may not provide enough context or contain suf-
ficient words to improve. We thoroughly reviewed
all sentences to ensure that they do not contain any
information that could identify individuals or any
offensive content.

Stage 2: Human Annotation. Ten native
English-speaking undergraduate students majoring
in linguistics were recruited as annotators to inde-
pendently annotate each sentence. To ensure anno-
tation quality, all annotators were required to pass
test tasks before participating in the annotation.

The annotators carried out the annotations in
three steps: (1) identifying words or phrases in
the sentence that could be improved, (2) offering
one or more suggestions for each identified target,
and (3) assigning a type of improvement after the
substitution.

Specifically, we define the substitution sugges-
tions as two types. (1) Refine-usage refers to in-
stances where the use of a specific word or phrase is
inappropriate in the current context, such as when it
has a vague meaning, is a non-native expression, or
is an incorrect usage of English. For instance, in the
second sentence shown in figure 1, the word "possi-
ble" is intended to convey the meaning of "having
the possibility", and is not appropriate in the con-
text of the sentence. The annotators replaced "pos-
sible" with "likely." These suggestions are designed
to help English learners understand the differences
in word usage in specific contexts and to enable
them to write in a way that is more consistent with
native speakers. (2) Diversify-expression refers
to instances where this word or phrase could be
substituted with other words or phrases. These

https: //aimwriting.mtutor.engkoo.com/

--- Page 4 ---
suggestions aim to help users use a more diverse
range of expressions. The last case in figure | is a
corresponding example.

The annotators were required to provide at least
three suggestions for each sentence. For the entire
dataset of 1000 sentences, each annotator was re-
quired to provide at least 1500 refine-usage type
suggestions. The detailed annotation instruction is
in appendix A.

Stage 3: Merging and Filtering. Previous lexi-
cal substitution tasks (McCarthy and Navigli, 2007;
Kremer et al., 2014) merged all the annotators’ re-
sults into a key-value dictionary, where the value
indicates the number of annotators who provided
this substitution suggestion. We merged the la-
beling results of 10 annotators in a similar way.
Take the merging of two annotators’ annotations
as an example. One is {happy: glad/merry,
possible: likely}, and the other is {help:
aid, possible: likely/probable}. The result
after merging would be:

{happy: {glad: 1, merry: 1},

possible: {likely: 2, probable: 1},

help: {aid: 1}}

where happy, possible, help are improvable
targets, and the sub-level dictionaries are the sub-
stitution suggestions after merging. We also collect
the type of refine-usage or diversify-expression for
each improvable target by taking the majority of
the type labeling.

In order to reduce subjective bias among an-
notators, we discarded all improvable targets that
were only annotated by one annotator. Finally, the
dataset was split into a validation set of 200 sen-
tences and a test set of 800 sentences.

3.2 Distantly Supervised Data Collection

We collect a large amount of distantly supervised
data for weakly supervised training by using a syn-
onym thesaurus to randomly substitute words in
a corpus. The source corpus contains 3.7 million
sentences from Wikipedia? . The synonym the-
saurus we use is the intersection of PPDB (Pavlick
et al., 2015) and Merriam-Webster thesaurus*. The
sentences are processed in 3 steps: (1) Selecting
all the words or phrases in the synonym thesaurus,
and treating them as improvable targets. (2) Using
a tagger to find the part of speech of the improv-
able targets. (3) Randomly substituting the improv-

*https: //dumps.wikimedia. org/enwiki/20220720/
3https: //www.merriam-webster.com/thesaurus

able targets with one synonyms of the same part of
speech.

Note that the random substitution with the syn-
onym dictionary may result in a more inappropriate
word or phrase usage than the original text. There-
fore, we treat the generated substitutions as the
improvable targets, and the original targets as sub-
stitution suggestions.

In contrast to the human-annotated dataset, the
distantly supervised dataset only includes one sug-
gestion for each improvable target and does not
have the annotation of suggestion type. The code
for generating distantly supervised datasets will be
released for further studies.

3.3. Data Statistics

Benchmark # Sentence # Target # Suggestion # Label
SemEval 2010 2010 8025 12,300
CoINCo 2474 15,629 112,742 167,446
SWORDS 1250 1250 71,813 395,175
SWS 1000 7027 16,031 30,293
SWSps 3,746,142 12,786,685 12,786,685 12,786,685

Table 1: Statistics of SWS and LS datasets. SWSps
stands for the distantly supervised dataset.

Table 1 shows the comparison between SWS and
lexical substitution benchmarks. Our SWS dataset
consists of 7027 instances of improvable targets
and 16031 suggestions in 1000 sentences. The av-
erage length of the sentences in this dataset is 27.8
words. The improvable targets in this dataset in-
cludes 2601 nouns, 2186 verbs, 1263 adjectives,
367 adverbs, 267 phrases, and 343 other parts of
speech. 3.8% of the targets and 3.3% of the sugges-
tions are multi-word phrases. 63.0% of the targets
are the type of refine-usage. Table 2 shows the
proportion of refine-usage or diversify-expression
targets with different part-of-speech.

POS noun verb adj. adv. phrase others total
number 2601 2186 1263 367 267 343 7027
RU (%) 57.8 63.7 66.7 64.9 708 76.7 -
DE(%) 42.2 36.3 33.3 35.1 29.2 23.3 -

Table 2: Statistics of targets with different part-of-
speech. RU refers to the proportion of refine-usage
targets, and DE refers to the proportion of diversify-
expression.

The distantly supervised dataset SWS ps con-
tains over 12.7 million suggestions in 3.7 million

--- Page 5 ---
sentences. 2.67% are multi-word phrases, and 0.3%
of the suggestions are multi-word.

3.4 Inner Annotator Agreements

Previous studies on lexical substitution(McCarthy
and Navigli, 2007; Kremer et al., 2014) evaluated
the quality of the dataset with inter-annotator agree-
ment (IAA). We adopt this approach and calculate
pairwise inter-annotator agreement (PA) to assess
the quality of the dataset.

PA‘ measures the consistency of identifying
improvable targets:

PAdet — _

mL PAt;, .

Ply J)EP

PA, — : i IS n Sk

where P is the set of annotator pairs. We have ten
annotators, so |P| = Cho = = 45. N is the number
of all the sentences, and sj, sj, are the improvable
target sets of sentence k identified by annotator i
and 7, respectively.

PA*"® measures the consistency of substitution
suggestions of a same improwante target:

WD PA}

(j)EP

PAsus —

wi iting
PASE, = )_ Lot
1 Mis |ti Ut] |
where M;; is the size of the intersection of the
improvable target sets identified by annotator 7 and
j. th, t] are the suggestions for target | given by
annotator i and j, respectively.

In the SWS benchmark, the PA®! and the
PA*Y are 23.2% and 35.4%, respectively. Our
PA*“ is significantly higher compared to previ-
ous LS datasets, 27.7% of SemEval (McCarthy and
Navigli, 2007) and 19.3% of COINCO (Kremer
et al., 2014), thereby confirming the annotation
quality.

3.5 Data Quality of the Distantly Supervised
Dataset

According to our statistics, 71.8% of the substitu-
tions in the test set appear in the training set, and
each substitution in the test set appears in the train-
ing set 10.4 times on average. Those data show
the substitutions in the training set covers most of
the substitutions in the test set, which verify the
synthetic method is close to real-world scenarios.

4 Evaluation

In this section, we introduce the evaluation settings
and metrics for SWS, including both the end-to-end
evaluation and the sub-task evaluation.

For the end-to-end evaluation and the improvable
target detection sub-task, we introduce precision,
recall, and Fo; as metrics. For the substitution
suggestion sub-task, we utilize accuracy to evaluate
the quality of the predicted substitutions. Examples
of calculating the metrics can be found in appendix
B.

4.1 End-to-end Evaluation

The end-to-end evaluation is computed based on
each substitution suggestion. A true prediction is
counted if and only if both the detected improv-
able target is in the annotated improvable target set
and the suggested substitution is in the annotated
substitutions of the target:

N Mr

S22 Lif seu € Sp else 0

k=1 [=1

Tpeze —

where N is the number of all the sentences, Mj, is
the number of targets in the sentence k, 5; is the
set of annotated suggestions of sentence k, and s;;
is the /-th predicted suggestion of sentence k. The
precision (Pe?) and recall (R°2°) for end-to-end
evaluation are calculated as follows:

Tpee Ree _ Tpe2e
Np Na

pee _

where Vp and N@ are the number of predicted sug-
gestions and annotated suggestions, respectively.
In the writing assistance scenario, precision is more
important than recall, so we calculate F62° as the
overall metric.

oie 1.25 - pore . Reve
Fo. = 0.25 - Pe2e + Re2e

4.2 Sub-Task Evaluation

Improvable Target Detection. In this task,
model needs to find all the annotated improvable
targets in the sentence. The precision (P4*) and
recall (R4**) for detection are calculated as fol-
lows:

N N
poet Var 18k Sil Rect _ Vpat [8k 84

N N
Veni |e Via Ise

--- Page 6 ---
where s;, and sj, are the annotated improvable tar-
get set and predicted improvable target set for sen-
tence k;, respectively. Same with end-to-end evalu-
ation, we compute F4¢ to assess the performance
for detection of improvable targets.

pact _ 1.25 - pdet . Radet
05 “0.25. Paet + React

Substitution Suggestion. In this task, model
needs to give suggestions for each improvable tar-
get. We calculate accuracy of the suggestions on
those correctly detected targets:

og 1(14.,,
Acc =F Wy 22 Lift € Tielse 0
k=1 l=1

where 7; is the annotated recommendation set
of target 1, ¢; is the predicted recommendation for
target |, and M;, is the total number of correctly
detected targets in sentence k.

5 Experiments

5.1 Baselines

We test 7 methods on SWS. The methods could be
divided into three groups: (1) Adopting external
knowledge to give suggestions. (2) State-of-the-art
lexical substitution methods. (3) End-to-end SWS
baselines. We also list the human performance for
reference.

External Knowledge Methods. Here are two
methods that use external knowledge to give sug-
gestions. (1) Rule-based synonyms replacement
as how we construct the distantly supervised data.
We adopt a greedy replacement strategy, where all
entries are replaced. (2) ChatGPT*, a large lan-
guage model trained on massive data and further
fine-tuned with human feedback. We ask ChatGPT
to directly generate the suggestions in every giv-
ing sentence. The prompt and details for utilizing
ChatGPT can be found in appendix C.

Lexical Substitution Methods. Two state-of-
the-art lexical substitution methods are tested on
SWS, i.e. BERT;,,s, (Zhou et al., 2019) and
LexSubCon (Michalopoulos et al., 2022). We
use the open-sourced code of LexSubCon and re-
implement BERT;,,,;,. We let the model give a
substitution for each word, and if the substitution
is different with the original word, the word is re-
garded as a detected improvable target.

*https://openai.com/blog/chatgpt/

End-to-end Baselines. In the end-to-end frame-
work, we treat SWS as three training paradigms,
and provide one baseline for each. (1) Masked
language modeling (MLM): We use BERT-base-
uncased (Devlin et al., 2019) with an MLM head
as the baseline. (2) Sequence-to-sequence gener-
ation: We use BART-base (Lewis et al., 2020) as
the baseline. (3) Token-level rewriting: We use
CMLM (Marjan Ghazvininejad, Omer Levy, Yin-
han Liu, Luke Zettlemoyer, 2019) as the baseline.
The distantly supervised dataset is utilized to train
the end-to-end baselines. For the improvable tar-
gets, the model is expected to learn the suggestions.
Otherwise, the model is expected to keep the origi-
nal words.

5.2 Main Results

Table 3 shows the experimental results of the base-
lines, from which we have the following observa-
tions:

(1) The rule-based approach is similar to the pro-
cess of creating distantly supervised data. Both
the rule-based method and end-to-end baselines,
which are trained using distantly supervised data,
have high P4° and low R&€t values. This sug-
gests that the synonym dictionary used in this work
has high quality but low coverage.

(2) Compared with the rule-based method, the
end-to-end models trained on distantly supervised
dataset show a decline in performance for the im-
provable target detection, but an increase in per-
formance for substitution suggestion. The improv-
able targets of the distantly supervised data do not
accurately reflect the words or phrases that need
improvement, resulting in difficulty in effectively
training the models in detecting. However, the
substitution suggestions in the distantly supervised
data are derived from original words in Wikipedia,
enabling the models to learn a relatively appropri-
ate word usage in context.

(3) The results of the CMLM model show a de-
crease in performance compared to the pre-trained
models, namely BERT and BART, particularly in
terms of substitution suggestions. The pre-training
of semantic knowledge may contribute to the supe-
rior performance of the pre-trained models for this
task.

(4) There is a notable decrease in SWS for LS
methods. Moreover, different LS methods have
significant differences in detecting improvable tar-
gets. Only 2.1% of the words in the input sentence


--- Page 7 ---
Sub-task Evaluation End-to-end Evaluation

Model pdet Radet Fdet Acc8¥s pe2e Re2e F62e

External Knowledge Rule-based | 0.585 0.344 0.513) 0.314 | 0.183 0.108 0.161
Methods ChatGPT 0.451 0.418 0.444 | 0.427 | 0.193 0.179 0.190
Lexical Substitution BERTs,.5, | 0.511 0.050 0.180} 0.441 | 0.225 0.022 0.079
Methods LexSubCon | 0.438 0.667 0.470 | 0.281 | 0.123 0.188 0.132
End-to-End CMLM 0.512 0.222 0.406 | 0.236 | 0.121 0.052 0.096
Methods BART 0.555 0.243 0.441 | 0.446 | 0.248 0.108 0.197
. BERT 0.585 0.249 0.460 | 0.436 | 0.255 0.108 0.201
Human* 0.709 0.313 0.566 | 0.631 | 0.449 0.199 0.359

Table 3: Evaluation results on SWS. *: As a reference, we offer human performance by taking the average of ten
rounds of evaluations. In each round, each annotator is compared to the combined annotations of other annotators.

are identified as improvable targets by BERT,, ,«,,.
while LexSubCon detects 32.4%. The current LS
methods are not compatible with the SWS task.

(5) The results from ChatGPT are comparable
with the end-to-end baselines trained on 3.7 million
sentences, but it is still has room for improvement.

(6) Human performance is significantly better
than baselines. We believe there is a lot of room
for the baselines to improve.

6 Analysis

We analyze the experimental results with two ques-
tions: (1) Does the model have the capability to
accurately identify words that require improvement,
or does it simply make random guesses? (2) Does
the model have the ability to provide multiple use-
ful suggestions for each target word?

6.1 Detection Analysis

Voting Index and Weighted Accuracy. After
merging the annotations, we determine the voting
index for each improvable target, i.e. the number of
annotators who identified the word or phrase. The
voting index reflects the necessary level of replace-
ment for the word. Figure 3 shows R4°* for the
improvable targets with different voting indexes.
As depicted in Figure 3, improvable targets iden-
tified by a greater number of annotators are more
easily detected by the models.

Then, we design weighted accuracy (WA) to
evaluate the detection performance, using the vot-
ing index as weighting factors.

N M, .
he Lin Wei if sx € 5), else 0

N M,
ent Liat Wal

Wwadet

—e@ R%* of BERT
—@ R%* of Rule-based

—#— R¢t of LexSubCon
target number

os
| 1750
0.74
| 1500
064 S
1250 8
. =
3054 t 1000 =
®
0.44 f 750 5
o3d + 500
t 250
0.24
eo oe Oe ee ee |

v4
w
a

5 6 7 8 9 10
voting index

Figure 3: The number of targets and R°°t on different
voting index.

where s’, is the predicted improvable target set of
sentence k, s;; is the /-th annotated target in sen-
tence k, wy; is the voting index of s;;, N is the
number of total sentences, and M;, is the size of
annotated improvable target set of sentence k.

Table 4 shows R4¢ and WA4**t of baseline
methods. Consistent with the trend of R4* for
different voting indexes, the WA4**t is relatively
higher than R¢¢t. These results demonstrate
that the baseline methods can detect the high-
confidence improvable targets better.

Improvable Ratio. The improvable ratio (ImpR)
is defined as the proportion of the number of de-
tected improvable words to the total number of
words in sentences. As shown in Table 4, Rd,
WA“t are positively correlated with ImpR.

--- Page 8 ---
Model ImpR Ret WAdet
Rule-based 0.125 0.344 0.382
ChatGPT 0.224 0.418 0.449
BERT;,,s, 0.021 0.050 0.061
LexSubCon 0.324 0.667 0.694
CMLM 0.094 0.222 0.239
BART 0.102 0.243 0.272
BERT 0.093 0.249 0.278
Human 0.212 - -

Table 4: Improvable ratio (ImpR), Detection Recall
(Rt) and Weighted Accuracy (WA) for improvable
targets detection on SWS benchmark sets.

To investigate how to control the model to
achieve a desired ImpR, we build another distantly
supervised dataset for training. Different from
dataset construction described in section 3.2, we
use the union of PPDB (Pavlick et al., 2015) and
Merriam-Webster thesaurus as a large synonym
thesaurus. As the thesaurus size increases, the ar-
tificial improvable targets in constructed data are
increased to 25.4% from 13.2%.

The results of BERT trained on two datasets
are presented in Table 5. Upon comparison of the
two experiments, it is observed that the number of
constructed improvable targets in the training set
is nearly doubled, while the ImpR of the trained
models only increases to 13.6% from 9.3%. It is
challenging to control the ImpR. Thus, one direc-
tion under research is to control the model to attain
a desired ImpR while maintaining a good perfor-
mance.

6.2 Multiple Suggestions Analysis

It may be beneficial for users to have multiple sug-
gestions for each improvable target. Therefore, we
design a multiple-suggestion setting that allows the
system to provide multiple substitution suggestions
for each detected improvable target.

As the output suggestions are ranked in order, we
propose using Normalized Discounted Cumulative
Gain (NDCG), a metric commonly used in search
engines, to measure the similarity between a ranked
list and a list with weights.

1 4DCG,,(T,)

NDCG,, = — Ske
™ M & DCG, (Tx)

0.40

[Z NDCG,, of BERT
E23) NDGG,, of Rule-based
0.35 4 —2S4_ NDCGG,, of LexSubCon
oO
8 0.304
2
0.25 4
0.20

1 2 3 4
# suggestion

Figure 4: NDCG,,, on different m of BERT, rule-
based method, and LexSubCon.

ici Wi
DCG,, (Tk) = S- log(1 +4)
i=l

m /
} cj W,
DCG,,,(T;,) = 5) 2s) 4

m( hb) a log(1 +i)

wy; = wi if thy € Th else 0

In this formula, is the total number of true pre-
dicted improvable targets, and m is a parameter
that specifies the number of suggestions for an im-
provable target. In the numerator, we accumulate
the weights for the predicted suggestions from the
first to the last. If recommendation 7’ is not in
human annotation, the weight is set to zero. Oth-
erwise, the weight is set to its voting index. The
denominator is a list sorted according to the voting
index, which represents the optimal condition for
giving m predictions. We provide an example of
calculating NDCG in appendix D.

The average number of substitution suggestions
for each improvable target in SWS benchmark is
3.3. When m exceeds the substitution number for a
given target, DCG,,,(T;,) remains constant. Thus,
NDCG,,, is only calculated form = 1,2,3,4.
Figure 4 lists NDCG,,, for different baselines.

BERT may perform better than other methods,
but as the number of suggestions m increases, the
NDCG,,, of BERT drops significantly. This sug-
gests that BERT struggles when providing multi-
ple suggestions. This could be due to the lack of
multiple substitution suggestions in the distantly
supervised dataset. Future research could focus on
improving the model’s ability to provide multiple
substitution suggestions.

--- Page 9 ---
Dataset pdt Rdct det ImpR WA‘! Accs¥s Pere Rere Pere
Wiki-13.2% 0.585 0.249 0.460 0.093 0.278 0.436 0.255 0.108 0.201
Wiki-25.4% 0.568 0.354 0.506 0.136 0.402 0.243 0.138 0.086 0.123

Table 5: Comparison of BERT trained on two distantly supervised datasets. The suffix stands for the constructed
improvable target ratio of the dataset. The model trained on the dataset with more improvable targets yields a
higher ImpR and a higher R4**, but a worse performance in substitution suggestions.

BERT Prediction: Target not found

Sentence: Most students don't have sufficient self-control, which would lead to worse situations,
like playing video games or watching TV all day, or playing outside for several days."
Ground Truth: situations — {"circumstances": 5, "conditions": 2},

directly.
Ground Truth: knowing —> {"following", "
BERT Prediction: knowing — understanding

Sentence: It may be true that knowing unrelated events doesn't provide convenience to our lives

memorizing",

mon M6,

recalling", “studying"}

Figure 5: Case study of the BERT’s predictions.

6.3 Case Study

Figure 5 gives two cases of BERT’s predictions.
In the first case, BERT didn’t detect this improv-
able target. However, in our distantly-supervised
training data, there are dozens of cases substituting
"situations" to "circumstances". We think control-
ling the initiative of detecting is a direction worthy
of research.

In the second case, BERT give the suggestion
of "understanding", which is the closest word to
"knowing" if ignores the context. However, it’s
not the right meaning in the context of "knowing
events". We think it’s hard to train a model aware
of word usage in different contexts with the cur-
rent distantly-supervised training data. Because we
think the one-substitute-one data doesn’t provide
enough information for model training on word us-
age. We regard this as a future research direction.

7 Conclusion

This paper introduces the first benchmark for Smart
Word Suggestions (SWS), which involves detecting
improvable targets in context and suggesting sub-
stitutions. Different from the previous benchmarks,
SWS presents a more realistic representation of a
writing assistance scenario. Our experiments and
analysis highlight various challenges for future re-
search and suggest opportunities for improvement
in future work. We encourage further research on
building more realistic training data, designing bet-
ter data augmentation strategies, and developing

unsupervised or self-supervised methods for SWS.

8 Limitations

The SWS benchmark have two limitations: (1) The
sentences in the SWS testing set come from stu-
dents’ essays, which limits the system’s ability to
test its performance in other specific domains such
as laws or medicine. (2) the SWS corpus is at the
sentence level, but some writing suggestions can
only be made after reading the entire article, which
are not included in our SWS dataset.

References

Christopher Bryant, Mariano Felice, and Ted Briscoe.
2017. Automatic annotation and evaluation of error
types for grammatical error correction. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 793-805, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long and Short Papers),
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella
Lapata. 2017. Learning to paraphrase for question
answering. In Proceedings of the 2017 Conference

--- Page 10 ---
on Empirical Methods in Natural Language Process-
ing, pages 875-886, Copenhagen, Denmark. Associ-
ation for Computational Linguistics.

Fredo Erxleben, Michael Giinther, Markus Krétzsch,
Julian Mendez, and Denny Vrandeéié. 2014. In-
troducing wikidata to the linked data web. In The
Semantic Web — ISWC 2014, pages 50-65, Cham.
Springer International Publishing.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1608-1618,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

Tao Ge, Furu Wei, and Ming Zhou. 2018. Fluency
boost learning and inference for neural grammati-
cal error correction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1055-
1065, Melbourne, Australia. Association for Compu-
tational Linguistics.

Mohit lyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1875-1885, New
Orleans, Louisiana. Association for Computational
Linguistics.

Gerhard Kremer, Katrin Erk, Sebastian Pad6, and Ste-
fan Thater. 2014. What substitutes tell us - analy-
sis of an “all-words” lexical substitution corpus. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 540-549, Gothenburg, Sweden. As-
sociation for Computational Linguistics.

Mina Lee, Chris Donahue, Robin Jia, Alexander lyabor,
and Percy Liang. 2021. Swords: A benchmark for
lexical substitution with improved data coverage and
quality. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4362-4379, Online. Association for
Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871-7880, Online. Association
for Computational Linguistics.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollar,

and C. Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In ECCV.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke
Zettlemoyer. 2019. Mask-Predict: Parallel Decod-
ing of Conditional Masked Language Models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing.

Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 48—
53, Prague, Czech Republic. Association for Compu-
tational Linguistics.

George Michalopoulos, Ian McKillop, Alexander
Wong, and Helen Chen. 2022. LexSubCon: Inte-
grating knowledge from lexical resources into con-
textual embeddings for lexical substitution. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1226-1236, Dublin, Ireland. Associ-
ation for Computational Linguistics.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task on
grammatical error correction. In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1-14, Balti-
more, Maryland. Association for Computational Lin-
guistics.

Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 425-430, Beijing, China. As-
sociation for Computational Linguistics.

Darsh Shah, Tao Lei, Alessandro Moschitti, Salva-
tore Romeo, and Preslav Nakov. 2018. Adversar-
ial domain adaptation for duplicate question detec-
tion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 1056-1063, Brussels, Belgium. Association
for Computational Linguistics.

Wei Song, Ziyao Song, Lizhen Liu, and Ruiji Fu.
2020. Hierarchical multi-task learning for organi-
zation evaluation of argumentative student essays.
In Proceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence, IJCAI-
20, pages 3875-3881. International Joint Confer-
ences on Artificial Intelligence Organization. Main
track.

Denny Vrandeti¢é and Markus Krotzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commu-
nications of the ACM, 57(10):78-85.

--- Page 11 ---
Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and
Ming Zhou. 2019. BERT-based lexical substitution.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
3368-3373, Florence, Italy. Association for Compu-
tational Linguistics.

A. Annotation Instructions

We need to find at least 3 "words/phrases to
change" in a sentence, and give "substitutes" for
each. Every substitute should be classified as
improve-usage or diversify-expression.

A.1 What is the word/phrase that needs to

change?

Our aim is to find a word/phrase that needs to
be better in writing scenarios. Suppose you are
the teacher, and now you are helping the language
learners to improve their English writing. We de-
fine a "word to change" as the substitution has in-
fluences as follows:

¢ To express the original semantic meaning
more appropriately.

¢ To make the usage of the word much closer to
the native speaker.

¢ To change spoken language into written lan-
guage.

To diversify the word usage for better expres-
sion.

The substitution should NOT cause the influence

as follows:

* Rewrite the sentence, instead of words or
phrases, into a better expression (e.g. "it is
advisable" — "advisably,").

* Correct the mistakes in the sentence (e.g. "a
lot" — "a lot of" in the sentence "There are a
lot of valuable tips").

¢ Substitute the word with a synonym, but not
help the English learners with better writing.

After the definition, we also give some rules that

you could refer to:

¢ the word/phrase that needs to change is usu-
ally less than 3 words.

¢ the word/phrase that needs to change is usu-
ally an adj./adv./noun/verb.

¢ the word/phrase that needs to change is usu-
ally not a named entity.

A.2 How to give the substitutions?

The substitution should:

« have the same semantic meaning as the "word
to change".

¢ keep the sentence’s meaning unchanged.

Specifically, there are two scenarios for substitu-
tion:

¢ If the word to change is general, and we can
clearly understand the sentence’s meaning. In
this case, the substitution should be more pre-
cise. (e.g. "Schools in north-west China are
our primary aiding individuals and we often
start from our school when the summer vaca-
tion begins." "aiding"—>"helping" is a good
substitution)

¢ If the word to change is confusing, and we
could only guess the sentence’s meaning. In
this case, the substitution should be more gen-
eral. (e.g. "Successful individuals are charac-
terized by various merits including ..."""
ous"—>"plentiful" is a bad substitution)

vari-

After the substitution, the sentence must be flu-
ent as the original sentence. Errors in preposi-
tion collocations, tenses, and mythologies should
be avoided. (e.g. "in a nutshell", "nutshell" —
"essence" is not right, should be "in a nutshell" >
"in essence")

A.3 Annotation Guidelines

¢ Substitutions in a grid should be connected
with ";" (NOT ’, !).

¢ If the original sentence has grammar or typo
problems, just discard the sentence.

¢ In the annotation table, the content in the col-
umn "word to change" should be EXACTLY
THE SAME as the word/phrase in the original
sentence, and there should not exist punctua-
tion (except ";" to connect multiple substitu-
tions)

¢ Substitute the smallest range of words, unless
indivisible. (e.g. "I think you deserve it again"
— "I think you deserve another chance" is a
bad case, which should be "it again" > "an-
other chance". "in a nutshell" — "in essence"
is a good case, because "in a nutshell" is a
phrase).

« We don’t need to paraphrase the sentence.

¢ Please ensure that the "substitute" and "word
to change" have the same tense, plural forms,
and part of speech.

--- Page 12 ---
B_ Example of Evaluation Metrics

For example, given a sentence: “I am writing to
answer the previous questions you asked.” The
annotation result of the sentence is as follows:

answer: {respond to: 3, reply to: 1},
writing: {connecting with: 3},
to answer: {in response to: 2},

questions: {queries: 2}

In improvable target detection, S;, is {answer,
writing, to answer, questions}. If the pre-
diction Si, is {answer, previous}, then P4et =
1/2 and RA = 1/4.

In substitution suggestion metrics, take the true
predicted target answer as an example. If the pre-
dicted suggestion is in {respond to, reply to,
then Acc8¥8 = 1, otherwise Acc8¥8 = 0.

In end-to-end evaluation, if the predicted
suggestions are {answer: respond, writing:
connect with, asked: gave}, then P°?° = 1/3
and R°?e = 1/4,

C_ Prompt for ChatGPT

The prompt we use is as follows:

In the following sentence, please give
some suggestions to improve word usage.
Please give the results with the json
format of “original word”: [“suggestion
1”, “suggestion 2”], and the “original
word” should be directly extracted from
the sentence. [s]
where [s] is the sentence. Amazingly, ChatGPT
can generate substitution suggestions with the key-
value format. We use regular expression to extract
the substitution suggestions. If the result is empty,
we will re-generate until getting substitution sug-
gestions.

D_ Example of NDCG

Take an example of NDCGs: For a de-
tected improvable target, if T; with voting index
is {respond to : 3, respond: 2, response: 1,
reply to: 1} and T; with order is {respond,
respond to, tell, response, solution}, then
DCG(Tj) and DCG(T;) are calculated as fol-
lows, and NDCG; = 4.4/5.1 = 86.3%.

Order Sub. Gain DCG;(T;)

1 respond 2 2=2x1

2 respondto 3 3.9=2+4+3 x 0.63

3 tell 0 3.9=3.9+0 x 0.5
4 response 1 44=3.9+1 x 0.43
5 solution 0 44=44+0 x 0.39
Order Sub. Gain DCG:;(T;)

1 respondto 3 3=3x 1

2 respond 3 4.2=3+2 x 0.63

3 response 1 4.7=424+1x 0.5
4 reply to 1 5.1=4.74+1 x 0.43
5 NULL 0 5.1=5.14+0 x 0.39


