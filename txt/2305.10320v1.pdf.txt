--- Page 1 ---
arXiv:2305.10320v1 [cs.CV] 17 May 2023

CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo

Weitao Chen*!, Hongbin Xu*!”, Zhipeng Zhou! , Yang Liu! , Baigui Sun‘! , Wenxiong
Kang”, Xuansong Xie!

‘Alibaba Group
South China University of Technology

{hillskyxm, hongbinxu1013} @ gmail.com, baigui.sbg @alibaba-inc.com, auwxkang @scut.edu.cn

Abstract

The core of Multi-view Stereo(MVS) is the match-
ing process among reference and source pixels.
Cost aggregation plays a significant role in this pro-
cess, while previous methods focus on handling it
via CNNs. This may inherit the natural limitation
of CNNs that fail to discriminate repetitive or incor-
rect matches due to limited local receptive fields.
To handle the issue, we aim to involve Transformer
into cost aggregation. However, another problem
may occur due to the quadratically growing com-
putational complexity caused by Transformer, re-
sulting in memory overflow and inference latency.
In this paper, we overcome these limits with an
efficient Transformer-based cost aggregation net-
work, namely CostFormer. The Residual Depth-
Aware Cost Transformer(RDACT) is proposed to
aggregate long-range features on cost volume via
self-attention mechanisms along the depth and spa-
tial dimensions. Furthermore, Residual Regression
Transformer(RRT) is proposed to enhance spatial
attention. The proposed method is a universal plug-
in to improve learning-based MVS methods.

1 Introduction

Given a series of calibrated images from different views in
one scene, Multi-view Stereo (MVS) aims to recover the
3D information of the observed scene. It is a fundamen-
tal problem in computer vision and widely applied to robot
navigation, autonomous driving, augmented reality, and etc.
Recent learning-based MVS networks [Yao ef al., 2018;
Gu et al., 2020; Wang et al., 2021b] have achieved inspiring
success both in the quality and the efficiency of 3D recon-
struction. Generally, deep MVS approaches consist of the fol-
lowing five steps: feature extraction from multi-view images
via CNN network with shared weights, differentiable warping
to align all source features to the reference view, matching
cost computation from reference features and aligned source
features, matching cost aggregation or regularization, depth
or disparity regression.

*These authors contributed equally to this work.
‘Corresponding authors.

B02 .
one 9G Potentatennet
wet
aa aa]
E e cua |W ove .
5 ose
6
o36 036
« * Py e
af @ oul @ o

z a 6 3 i 02 04 08 o8 10 a2
GPU Mem. (68) Run-time (s)

Figure 1: Comparison with state-of-the-art MVS methods on DTU.
Relationship between error, GPU memory and run-time with image
size 1152x864.

Current progresses in learning-based MVS primarily con-
centrate on the limitation of reconstruction quality [Wei er al.,
2021; Yang et al., 2020a], memory consumption [Yan et al.,
2020; Wei et al., 2021], and efficiency [Wang et al., 2021b;
Wang et al., 2021a]. The basic network architecture of these
works is based on the pioneering backbone network called
MVSNet [Yao er al., 2018], which provides an elegant and
stable baseline. However, instead of taking the inheritance
of network design principle in MVSNet [Yao er al., 2018]
for granted, we can rethink the task of MVS problem as a
dense correspondence problem [Hosni er al., 2012] alterna-
tively. The core of MVS is a dense pixelwise correspondence
estimation problem that searches the corresponding pixel of
a specific pixel in the reference image along the epipolar
line in all warped source images. No matter which task this
correspondence estimation problem is applied to, the match-
ing task can be boiled down to a classical matching pipeline
[Scharstein and Szeliski, 2002]: (1) feature extraction, and (2)
cost aggregation. In learning-based MVS methods, the transi-
tion from traditional hand-crafted features to CNN-based fea-
tures inherently solves the former step of the classical match-
ing pipeline via providing powerful feature representation
learned from large-scale data. However, handling the cost
aggregation step by matching similarities between features
without any prior usually suffers from the challenges due to
ambiguities generated by repetitive patterns or background
clutters [Cho et al., 2021]. Consequently, a typical solution
in MVSNet and its variants [Yao er al., 2018; Gu et al., 2020;
Wang et al., 2021b] is to apply a 3D CNN or an RNN to reg-

--- Page 2 ---
ularize the cost volume among reference and source views,
rather than directly rely on the quality of the initial corre-
lation clues in cost volume. Although formulated variously
in previous methods, these methods either use hand-crafted
techniques that are agnostic to severe deformations or inherit
the limitation of CNNs, e.g. limited receptive fields, unable
to discriminate incorrect matches that are locally consistent.

In this work, we focus on the cost aggregation step of cost
volume and propose a novel cost aggregation Transformer
(CostFormer) to tackle the issues above. Our CostFormer
is based on Transformer [Vaswani et al., 2017], which is
renowned for its global receptive field and long-range de-
pendent representation. By aggregating the matching cost in
the cost volume, our aggregation network can explore global
correspondences and refine the ambiguous matching points
effectively with the help of the self-attention (SA) mecha-
nism in Transformer. Though the promising performances
of Vision Transformers have been proven in many applica-
tions [Dosovitskiy et al., 2020; Sun er al., 2021], the time
and memory complexity of the key-query dot product inter-
action in conventional SA grow quadratically with the spatial
resolution of inputs. Hence, replacing 3D CNN with Trans-
former may result in unexpected extra occupancy in memory
and latency in inference. Inspired by [Wang er al., 2021b],
we further introduce the Transformer architecture into an it-
erative multi-scale learnable PatchMatch pipeline. It inherits
the advantages of the long-range receptive field in Transform-
ers, improving the reconstruction performance substantially.
Meantime, it also maintains a balanced trade-off between ef-
ficiency and performance, which is competitive in the infer-
ence speed and parameters magnitude compared with other
methods.

Our main contributions are as follows:

(1) In this paper, we propose a novel Transformer-based
cost aggregation network called CostFormer, which can be
plugged into learning-based MVS methods to improve cost
volume effectively. (2) CostFormer applies an efficient Resid-
ual Depth-Aware Cost Transformer to cost volume, extending
2D spatial attention to 3D depth and spatial attention. (3)
CostFormer applies an efficient Residual Regression Trans-
former between cost aggregation and depth regression, keep-
ing spatial attention. (4) The proposed CostFormer brings
benefits to learning-based MVS methods when evaluating
DTU [Aanees et al., 2016], Tanks & Temples [Knapitsch et
al., 2017] ETH3D [Schops et al., 2017] and BlendedMVS
[Yao et al., 2020] datasets.

2 Related Work

2.1 Learning-based MVS Methods

Powered by the great success of deep learning-based tech-
niques, many learning-based methods have been proposed to
boost the performance of Multi-view Stereo. MVSNet [Yao
et al., 2018] is a landmark for the end-to-end network that in-
fers the depth map on each reference view for the MVS task.
Feature maps extracted by a 2D CNN on each view are repro-
jected to the same reference view to build a variance-based
cost volume. A 3D CNN is further used to regress the depth
map. Following this pioneering work, lots of efforts have

been devoted to boosting speed and reducing memory occu-
pation. To relieve the burden of huge memory cost, recurrent
neural networks are utilized to regularize the cost volume in
AA-RMVSNet [Wei et al., 2021]. Following a coarse-to-fine
manner to develop a computationally efficient network, a re-
cent strand of works divide the single cost volume into several
cost volumes at multiple stages, like CasMVSNet [Gu et al.,
2020], CVP-MVSNet [Yang et al., 2020a], UCSNet [Cheng
et al., 2020], and etc. Inspired by the traditional PatchMatch
stereo algorithm, PatchMatchNet [Wang et al., 2021b] inher-
its the pipeline in PatchMatch stereo in an iterative manner
and extend it into a learning-based end-to-end network.

2.2. Vision Transformer

The success of Transformer [Vaswani et al., 2017] and its
variants [Dosovitskiy et al., 2020; Liu et al., 2021] have
motivated the development of Neural Language Processing
in recent years. Borrowing inspiration from these works,
Transformer has been successfully extended to vision tasks
and proven to boost the performance of image classification
[Dosovitskiy et al., 2020]. Following the pioneering work,
many efforts are devoted to boosting the development of var-
ious vision tasks with the powerful representation ability of
Transformer.

In [Li et al., 2021], the application of Transformer in
the classic stereo disparity estimation task is investigated
thoughtfully. Swin Transformer [Liu et al., 2021] involves
the hierarchical structure into Vision Transformers and com-
putes the representation with shifted windows. Consider-
ing Transformer’s superiority in extracting global content in-
formation via attention mechanism, many works attempt to
utilize it in the task of feature matching. Given a pair of
images, CATs [Cho et al., 2021] explore global consensus
among correlation maps extracted from a Transformer, which
can fully leverage the self-attention mechanism and model
long-range dependencies among pixels. LoFTR [Sun et al.,
2021] also leverages Transformers with a coarse-to-fine man-
ner to model dense correspondence. STTR [Li er al., 2021]
extends the feature matching Transformer architecture to the
task of stereo depth estimation task in a sequence-to-sequence
matching perspective. TransMVSNet [Ding et al., 2021] is
the most relevant concurrent work compared with ours, which
utilizes a Feature Matching Transformer (FMT) to lever-
age self-attention and cross-attention to aggregate long-range
context information within and across images. Specifically,
the focus of TransMVSNet is on the enhancement of feature
extraction before cost aggregation, while our proposed Cost-
Former aims to improve the cost aggregation process on cost
volume.

3 Methodology

In this section, we introduce the detailed architecture of the
proposed CostFormer which focuses on the cost aggrega-
tion step of cost volume. CostFormer contains two spe-
cially designed modules called Residual-Depth Aware Cost
Transformer (RDACT) and Residual Regression Transformer
(RRT), which are utilized to explore the relation between pix-
els within a long range and the relation between different

--- Page 3 ---
(WW) waing (ECost Computation

INITAPROP| STAGE3 —>{ eoacr J mer J '
cc \ 3D CNN Cost Aggregation
Y L ¥ ¥ ' o @ ooteors
Ww o+@- >a >a aed ee
Xie oes owe rn J) Warped Features
1 INIT&PROP STAGE2 p> Rpacr |— rer |— |
1 Cost Volume
' y an 1
; -@-4J- @-g-@e- eS
1 ~
OT, Ieee == SSS SSS ae (A9] Transformed Cost Volume
1 ¥ _ ’
¥ INIT&PROP, STAGE1 ae '
— 1 ! Aggregated Cost Volume
y . 1 vi —
>w> > Transformed Aggregated Cost Volume
ce [itrarRor) nialiatin and Propagation
STAGEO RR} Residual Regression Transformer

Figure 2: Structure of CostFormer based on PatchMatchNet.

depth hypotheses during the evaluation process. In Section
Preliminary, we give a brief preliminary on the pipeline of
our method. Then we show the construction of RDACT and
RRT respectively. Finally, we show experiments.

3.1 Preliminary

In general, the proposed RDACT and RRT can be integrated
with arbitrary cost volume of learning-based MVS networks.
Based on the patch match architecture [Wang er al., 2021b],
we further explore the issue of cost aggregation on cost vol-
ume. As shown in Figure 2, CostFormer based on Patch-
MatchNet [Wang et al., 2021b] extracts feature maps from
multi-view images and performs initialization and propaga-
tion to warp the features maps in source views to reference
view. Given a pixel p at the reference view and its correspond-
ing pixel p;,; at the -th source view under the j-th depth hy-
pothesis d; is defined as:

pig = Ki- (Roi (Ko' + p+ dj) + tos) (1)

where Ro,; and to; denote the rotation and translation be-
tween the reference view and i-th source view. Ko and K; are
the intrinsic matrices of the reference and i-th source view.
The warped feature maps at the i-th source view F;(p;,;)
are bilinearly interpolated to remain the original resolution.
Then, a cost volume is constructed from the similarity of fea-
ture maps, and 3D CNNs are applied to regularize the cost
volume. Warped features from all source views are integrated
into a single cost for each pixel p and depth hypothesis d; by
computing the cost per hypothesis S;(p,j)% via group-wise
correction as follows:

Si(p. J)? = ¢ < Fo(p)9, Fi(pig)? > RF 2)

where G is the group number, C’ is the channel number, <
-,- > is the inner product, Fo(p)9 and F;(p;,;)% are grouped
reference feature map and grouped source feature map at the
i-th view respectively. Then they aggregate over the views
with a pixel-wise view weight w;(p) to get S(p, j).

Taking no account of Transformer at the cost aggregation
(CA) step, a CA module firstly utilizes a small network with
3D convolution with 1 x 1 x 1 kernels to obtain a single cost, C

€ R¥XW*D. Fora spatial window of K, pixels {p, }{<, can

be organized as a grid, per pixel additional offsets {Ape He,
can be learned for spatial adaptation. The aggregated spatial
cost C(p, j) is defined as:

Ke

i —SrwndiC(p + Pe + Aves i) B)
Vege ede (4
where wy, and d;, weight the cost C based on feature and depth
similarity. Given the sampling positions (p + px + Apr) i245
corresponding features from Fo are extracted via bilinear in-
terpolation. Then group-wise correlation is applied between
the features at each sampling location and p. The results are
concatenated into a volume on which 3D convolution layers
with 1x1x1 kernels and sigmoid non-linearities are applied to
output normalized weights {wy Vey: The absolute difference
in inverse depth between each sampling point and pixel p with
their j-th hypotheses are collected. Then a sigmoid function
on the inverted differences is applied to obtain {dy} ey.

The remarkable thing is that such cost aggregation in-
evitably suffers from challenges due to ambiguities gener-
ated by repetitive patterns or background clutters. The local
mechanisms in ambiguities exist in many operations, such as
local propagation and spatial adaptation by small learnable
slight offset. CostFormer significantly alleviates these prob-
lems through RDACT and RRT. The original CA module is
also repositioned between RDACT and RRT.

After RRT, soft argmin is applied to get the regressed
depth. Finally, a depth refinement module is designed to re-
fine the depth regression.

For CascadeMVS and other cascade architectures, Cost-
Former can be plugged into similarly.

C(p, j) =

3.2. Residual Depth-Aware Cost Transformer

In this section, we explore the details of the Residual Depth-
Aware Cost Transformer (RDACT). Each RDACT consists
of two parts. The first part is a stack of Depth-Aware Trans-
former layer (DATL) and Depth-Aware Shifted Transformer
layer (DASTL), which deal with the cost volumes to ex-
plore the relations sufficiently. The second part is the Re-

--- Page 4 ---
UCSNet

PatchmatchNet

Figure 3: Comparison of different methods on the DTU evaluation set. The backbone of CostFormer is PatchMatchNet here.

Embedding Cost layer (REC) which recovers the cost volume
from the first part.

Given a cost volume Cy € R4?*“**G, temporary inter-
mediate cost volumes C},C9,...,C, € RY*W*?** are firstly
extracted by DATL and DASTL alternatively:

Cy = DASTL;(DATL;(Cx—1)), & = 1,2,..., LD (4)

where DATL, is the k-th Depth-Aware Transformer layer
with regular windows, DASTL;, is the k-th Depth-Aware
Transformer layer with shifted windows, E is the embedding
dimension number of DATL; and DASTL,.

Then a Re-Embedding Cost layer is applied to the last C;,,
namely C,, to recover G from E. The output of RDACT is
formulated as:

Cout = REC(Cz) + Co (5)

where REC is the Re-Embedding Cost layer, and it can be a
3D convolution with G output channels. If FE = G, Cous can
be simply formulated as:

Cout = Ci + Co (6)

This residual connection allows the aggregation of different
levels of cost volumes; Co. instead of Co is then aggregated
by the original aggregation network described in section 3.1.
The whole RDACT is shown in the red window in Figure 2.
Before introducing the construction of DATL and DASTL,
we dive into the details of core constitutions called Depth-
Aware Multi-Head Self-Attention (DA-MSA) and Depth-
Aware Shifted Multi-Head Self-Attention (DAS-MSA). Both
DA-MSA and DAS-MSA are based on Depth-Aware Self-
Attention Mechanism. In order to explain Depth-Aware

Self-Attention Mechanism, we supply the knowledge about
Depth-Aware Patch Embedding and Depth-Aware Windows
as preliminary.

Depth-Aware Patch Embedding: Obviously, directly apply-
ing the attention mechanism for feature maps at pixel-wise
level is quite costly in terms of GPU memory usage. In order
to tackle this issue, we propose a Depth-Aware Patch Embed-
ding to reduce the high memory cost and get an additional
regularization. Specifically, given a grouped cost volume be-
fore aggregation C € R?*W*PxG 4 depth-aware patch em-
bedding is firstly applied to C to get tokens. It consists of a 3D
convolution with kernel size h x w x d and a layer normaliza-
tion.To downsample the spatial sizes of cost volume and keep
the depth hypotheses, we set h and w to more than | and d
as 1. So the sample ratio is adaptive for memory cost and run
time. Before convolution, cost volume will be padded to fit
the spatial sizes and downsampling ratio. After layer normal-
ization(LN), these embedded patches are further partitioned
by depth-aware windows.

Depth-Aware Windows: Beyond the nonlinear and linear
global self-attention, local self-attention within a window has
been proven to be more effective and efficient. As an exam-
ple of 2D windows, Swin Transformer [Liu er al., 2021] di-
rectly applies multi-head self-attention mechanisms on non-
overlapping 2D windows to avoid the big computation com-
plexity of global tokens. Extended from the 2D spatial win-
dow, an embedded cost volume patch € R#™*W*xD°xG
with depth information is partitioned into non-overlapping
3D windows. These local windows are then transposed and
reshaped to local cost tokens. Assuming the sizes of these
windows are hs x ws x ds, the total number of tokens is

--- Page 5 ---
COLMAP

0 T

PatchmatchNet

Qr 37 = 9mm

Figure 4: Comparison of different methods on Tanks&Temples. The Recall reported by official benchmark is presented.

pL] xf) x« (=). These local tokens are further pro-
cessed by the multi-head self-attention mechanism.
Depth-Aware Self-Attention Mechanism: For a cost win-
dow token X € R’s*wsX4sxG_ the query, key, and value
matrices Q, K and V € R'*s*4s*G are computed as:

Q=XPo,K=XPK,V =XPy (7)

where Pg, Px, and Py € ROXG are projection ma-
trices shared across different windows. By introduc-
ing depth and spatial aware relative position bias Bl €
Ritsxhs)x(wsxws)x(dsxds) for each head, the depth-aware
self-attention(DA-SA1) matrix within a 3D local window is
thus computed as:

Qikit
Ja + BYv1
(8)
Where Q1, K1 and V1 € R’:s4s*G are reshaped from
Q, K and V € RisXws*4sxG_ The process of DATL with
LayerNorm(LN) and multi-head DA-SA1 at the current level
is formulated as:

X! = DA-MSAI((LN(X'!)) +.X!! (9)

DA-SAI = Attention1(Q1, K1, V1) = SoftMaz(

By introducing depth-aware relative position bias B2 €
IR¢*¢s for each head, the depth-aware self-attention(DA-
SA2) matrix along the depth dimension is an alternative mod-
ule to DATL and thus computed as:

one + B2)V2

(10)
Where Q2, K2 and V2 € R’»s*4s*G are reshaped from
Q, K and V € R'*s*4s*G_ BY and B2 will be along the
depth dimension and lie in the range of [—d, + 1,ds — 1].
Along the height and width dimension, B1 lies in the range
of [-hs + 1,h, — 1] and [-w, + 1,ws — lj. In prac-
tice, we parameterize a smaller-sized bias matrix BI €
RGhs-1)x(@ws—1)x(2ds-1) from B1 and perform the atten-
tion functionfor f times in parallel, and then concatenate
the depth-aware multi-head self-attention (DA-MSA) out-
puts. The process of DATL with LayerNorm(LN), multi-head

DA-SA2 = Attention2(Q2, K2, V2) = SoftMaz(

DA-SA1, and DA-SA2 at the current level is formulated as:

X! = DA-MSAI(LN(DA-MSA2(LN(X!~1)))) + XU"!
qd)
Then, an MLP module that has two fully-connected layers
with GELU non-linearity between them is used for further
feature transformations:

X! = MLP(LN(X!))) +X! (12)

Compared with global attention, local attention makes it pos-
sible for computation in high resolution.

However, there is no connection across local windows with
fixed partitions. Therefore, regular and shifted window par-
titions are used alternately to enable cross-window connec-
tions. So at the next level, the window partition configu-
ration is shifted along the height, width, and depth axes by

fe SS 4), Depth-aware self-attention will be computed
in these shifted windows(DAS-MSA); the whole process of
DASTL can be formulated as:

X'+1 — DAS-MSAI(LN(DAS-MSA2(LN(X°)))) +X!
(13)

X41 = MLP(LN(X'1)) + X44 (14)

DAS-MSA1 and DAS-MSA2 correspond to multi-head
Attention! and Attention2 within a shifted window, respec-
tively. Assuming the number of stages is n, there are n
RDACT blocks in CostFormer.

3.3. Residual Regression Transformer

After aggregation, the cost C € R¥XWXP will be used for
depth regression. To further explore the spatial relation under

some depth, a Transformer block is applied to C before soft-
max. Inspired by the RDACT, the whole process of Residual
Regression Transformer(RRT) can be formulated as:

Cy = RST, (RT¢(Ce-1)), k = 1,2,..., (15)

Cout = RER(Cx) + Co (16)

--- Page 6 ---
Methods Tntermediate Group (F-score 7) ‘Advanced Group (F-score 7)
‘ Mean Fam. Fra. Hor Lig. _M60__Pan._-Pla.___‘Tra. Mean Aud._Bal. Cou. Mus. Pal.__ Tem.

MVSNet [Yao ef al., 20187 BB 35.99 28.55 25.07 50.79 53.96 50.86 47.90 34.69 -
CasMVSNet [Gu er al., 2020] 56.84 76.37 5845 46.26 55.81 56.11 54.06 58.18 49.51 31.12 1981 3846 29.10 43.87 27.36 28.11

UCS-Net [Cheng e7 al., 2020] 54.83 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89 - - - - - - -

CVP-MVSNet [Yang e7 al., 2020b] 54.03 76.50 36.34 55.12 57.28 54.28 57.43

PVA-MVSNet [Yi et al., 2020] 54.46 69.36 46.01 55.74 57.23 54.75 56.70 - - - - -
AA-RMVSNet [Wei et al., 2021] 61.51 7111 51.53 64.02 64.05 59.47 60.85 33.53 32.05 46.01 29.28 32.71
PatchmatchNet [Wang et al. 53.15 66.99 43.24 54.87 52.87 49.54 54.21 32.31 30.04 41.80 28.31 32.29
UniMVSNet [Peng et 64.36 81.20 53.11 63.46 66.09 64.84 62.23 38.96 39.74 52.89 33.80 34.63
MVSTR [Zhu er al. 36.93 76.92 30.16 56.73 56.53 51.22 5658 47: 32.85 33.87 45.46 27.95 27.97
TransMVS [Ding et al., 2022] 63.52 80.92 56.94 62.54 63.06 60.00 60.20 58. 37.00 34.77 46.49 34.69 36.62
MVSTER [Wang er al., 2022] - : : - - - - - 37.53 35.65 49.37 32.16 39.19
CostFormer(PatchMatchNet) | 56.27(43.12) | 7246 34.27 5583 56.80 S088 55.05 52.32 | 34.07G1,76) 32.17 43.95 2862 36.46
CostFormer(PatchMatchNet*) | 57.10(+3.95) | 74.22 S441 56.65 54.46 51.45 57.65 51.70 | 34.31(+2.00) 3158 44.55 28.79 35.03
CostFormer(UniMVSNet~ ) 64.40(+0.04) | 81.45 53.88 62.94 66.12 65.35 61.31 57.90 | 39.55(+0.59) 40.21 5281 34.40 35.62
CostFormer(UniMVSNet*) 64.51(+0.15) | 81.31 55.57 63.46 66.24 65.39 61.27 __57.30 | 39.43(40.47) 39.88 53.38 34.07 34.87

Table 1: Quantitative results of different methods on the Tanks & Temples benchmark (higher is better). * is pretrained on DTU and fine-tuned
on BlendedMVS. - is not pretrained on DTU and trained from scratch on BlendedMVS

where RT;, is the k-th Regression Transformer layer with reg-
ular windows, RST;, is the k-th Regression Transformer layer
with shifted windows, RER is the re-embedding layer to re-
cover the depth dimension from Cz, and it can be a 2D con-
volution with D output channels.

RRT also computes self-attention in a local window. Com-
pared with RDACT, RRT focuses more on spatial relations.
Compared with regular Swin [Liu er al., 2021] Transformer
block, RRT treats the depth as a channel, the number of chan-
nels is actually 1 and this channel is squeezed before the
Transformer. The embedding parameters are set to fit the cost
aggregation of different iterations. If the embedding dimen-
sion number equals D, C,.,4 can be simply formulated as:

Cour = Cx + Co (17)

As a stage may iterate many times with different depth hy-
potheses, the number of RRT blocks should be set the same
as the number of iterations. The whole RRT is shown in the
yellow window in Figure 2.

4 Training
4.1 Loss function

Final loss combines with the losses of all iterations at all
stages and the loss from the final refinement module:

8

Loss = Soyo! + Dre

k=1i=1

(18)

where L* is the regression or unification loss of the i-th iter-
ation at k-th stage. L,¢¢ is the regression or unification loss
from refinement module. If refinement module does not exist,
the Le loss is set to zero.

4.2, Common training settings

CostFormer is implemented by Pytorch [Paszke er al., 2019].
For RDACT, we set the depth number at stages 3, 2, 1 as 4,
2, 2; patch size at height, width and depth axes as 4, 4, 1;
window size at height, width and depth axes as 7, 7, 2. If
the backbone is set as PatchMatchNet, embedding dimension
number at stages 3, 2, | are set as 8, 8, 4. For RRT, we set
the depth number as 2 at all stages, patch size as | at all axes;

window size as 8 at all axes. If the backbone is set as Patch-
MatchNet, embedding dimension number at iteration 2, 2, 1
at stages 3, 2, 1 as 32, 64, 16, 16, 8. All models are trained
on Nvidia GTX V100 GPUs. After depth estimation, we re-
construct point clouds similar to MVSNet [Yao et al., 2018].

5 Experiments

In this section, we introduce multiple MVS datasets and eval-
uate our method on these datasets. The results will be further
reported in detail.

5.1 DATASETS

The datasets used in the evaluation are DTU [Aanes et al.,
2016], BlendedMVS [Yao et al., 2020], ETH3D [Schéps er
al., 2017], Tanks & Temples [Knapitsch et al., 2017], and
YFCC-100M [Thomee et al., 2016]. The DTU dataset is an
indoor multi-view stereo dataset with 124 different scenes,
there are 49 views under seven different lighting conditions
in one scene. Tanks & Temples is collected in a more com-
plex and realistic environment, and it’s divided into the in-
termediate and advanced set. ETH3D benchmark consists of
calibrated high-resolution images of scenes with strong view-
point variations. It is divided into training and test datasets.
While the training dataset contains 13 scenes, the test dataset
contains 12 scenes. BlendedMVS dataset is a large-scale syn-
thetic dataset, consisting of 113 indoor and outdoor scenes
and split into 106 training scenes and 7 validation scenes.

5.2. Main Settings and Results on DTU

For the evaluation on the DTU [Aanes et al., 2016] evalua-
tion set, we only use the DTU training set. During the train-
ing phase, we set the image resolution to 640 x 512. We
compare our method to recent learning-based MVS methods,
including CasMVSNet [Gu et al., 2020] and PatchMatchNet
[Wang et al., 2021b] which are also set as backbones of Cost-
Former. We follow the evaluation metrics provided by the
DTU dataset. The quantitative results on the DTU evaluation
set are summarized in Table 2, which indicates that the plug-
and-play CostFormer improves the cost aggregation. Partial
visualization results of Table 2 are shown in Figure 3.

Complexity Analysis: For the complexity analysis of Cost-
Former, we plug it into PatchMatchNet [Wang er al., 2021b]
and first compare the memory consumption and run-time with

--- Page 7 ---
Methods Acc. (mm) Comp. (mm) Overall (mm)
Furu [Furukawa and Ponce, 2010] 0.613 0.941 0.777

Tola [Tola et al., 2012] 0.342 1.190, 0.766

Gipuma [Galliani et al., 2015] 0.283 0.873 0.578

Colmap [Schénberger and Frahm, 2016] 0.400 0.644 0.532
SurfaceNet [Ji et al., 20177 0.450 1.040 0.745,

MVSNet [Yao et al., 2018] 0.396 0.527 0.462
R-MVSNet [Yao et al., 2019] 0.383, 0.452 0.417
P-MVSNet [Luo et al., 2019] 0.406 0.434 0.420
Point-MVSNet [Chen et al., 2019] 0.342 0411 0.376
Fast-MVSNet [Yu and Gao, 2020] 0.336 0.403 0.370
CasMVSNet [Gu et al., 2020] 0.325 0.385 0.355

UCS-Net [Cheng et al., 2020] 0.338 0.349 0.344
CVP-MVSNet [Yang et al., 2020b] 0.296 0.406 0.351
PVA-MVSNet [Yi ef al., 2020] 0.379 0.336 0.357
PatchMatchNet [Wang et al., 2021b] 0.427 0.277 0.352
AA-RMVSNet [Wei et ai.. 0.376 0.339 0.357
UniMVSNet [Peng et al., 0.352 0.278 0.315
CostFormer(Based on PatchMatchNet) 0.424 0.262 0.343 (+0.0093)
CostFormer(Based on CasMVSNet) 0.378 0.313 0.345 (+0.0097)
COstFormer(Based on UniMVSNet) 0.301 0.322 0.312 (+0.0035)

Table 2: Quantitative results of different methods on DTU.
this backbone. For a fair comparison, a fixed input size of
1152 x 864 is used to evaluate the computational cost on

a single GPU of NVIDIA Telsa V100. Memory consump-
tion and run-time of PatchMatchNet [Wang et al., 2021b] are
2323MB and 0.169s. They are only increased to 2693MB and
0.231s by the plug-in.

Based on the reports of PatchMatchNet [Wang et al.,
2021bl], we then get the comparison results of other state-of-
the-art learning-based methods. Memory consumption and
run-time are reduced by 61.9% and 54.8% compared to Cas-
MVSNet [Gu et al., 2020], by 48.8% and 50.7% compared
to UCSNet [Cheng et al., 2020] and by 63.5% and 77.3%
compared to CVP-MVSNet [Yang et al., 2020b]. Combining
the results(lower is better) are shown in Table 3 and Figure 1,
GPU memory and run-time of CostFormer are set as 100%.

Method GPU Memory (%)_| Run-time (%) | Overall (mm)
CasMVSNet [Gu ef al., 2020] 262.47% 221.24% 0.355
UCSNet [Cheng er al., 2020] 195.31% 202.84% 0.344
CVP-MVSNet [Yang et al., 2020b]_|_273.97% 40.53% 0.351
Ours 100.00% 100.00% 0.343

Table 3: Comparison with other SOTA learning-based MVS meth-
ods on DTU. Relationship between overall performance, GPU mem-
ory and run-time.

Comparison with Transformers We also compare Cost-
Former with other Transformers [Zhu et al., 2021; Wang et
al., 2022; Ding et al., 2021; Liao et al., 2022] which are used
in MVS methods and not plug-and-play. For a fair compari-
son, only direct improvements(higer is better) and incremen-
tal cost of run time(low is better) from pure Transformers un-
der similar depth hypotheses are summarized in Table 4.

the Blended MVS [Yao et al., 2020] dataset. We compare
our method to those recent learning-based MVS methods, in-
cluding PatchMatchNet [Wang et al., 2021b] and UniMVS-
Net [Peng et al., 2022] which are also set as backbones of
CostFormer. The quantitative results on the Tanks & Temples
Knapitsch et al., 2017] set are summarized in Table 1, which
indicates the robustness of CostFormer. Partial visualization
results of Table 1 are shown in Figure 4. We would like to
clarify that UniMVSNet™ in Table | only uses BlendedMVS
or training which uses less data (no DTU) than the UniMVS-
Net baseline.

5.4 Main Settings and Results on ETH3D

We use the PatchMatchNet [Wang er al., 2021b] as back-
bone and adopt the trained model used in the Tanks & Tem-
ples dataset [Knapitsch er al., 2017] to evaluate the ETH3D
[Schéps er al., 2017] dataset. As shown in Table 5, our

method outperforms others on both the training and partic-
ularly challenging test datasets(higher is better).
‘Training ‘Testing
Methods Fiscore? | Timesyt | FiscoreT | Times)
MVE [Fuhrman et al., 20147 20.47 13278.69 30.37, 10550.67
Gipuma [Galliani e7 al., 2015] 36.38 387.717 45.18 689.75
PMVS [Furukawa and Ponce. 20107 46.06 836.66 44.16 ‘957.08
‘COLMAP [Schonberger and Frahm, 20167 67.66 2690.62 73.01 1658.33
PVSNet [Xu and Tao, 20207 6748 = 72.08 829.5
et al., 202 1a] 66.36 = 74.29 =
Net [Wang ef al., 202 1b] 64.21 452.63 73.12, 492.52
PatchMatch-RL [Lee ef al., 20217 67.78 = 72.38 =
CostFormer(Ours) 68.92(44.71) 566.18 75.24(+2.12) 547.64

Table 5: Quantitative results of different methods on ETH3D.

5.5 Main Settings and Results on BlendedMVS
dataset

We use the model used in ETH3D. On BlendedMVS [Yao er
al., 2020] evaluation set, we set N = 5 and image resolution
as 576 x 768. End point error (EPE), | pixel error (e1), and 3
pexels error (e3) are used as the evaluation metrics. Quanti-
tative results(lower is better) of different methods are shown
in Table 6.

Method Trans Improvement Gam) | Delta Time () | Delta Time (7)
0.0140 203595 F1821%
0.0160, 403678 #135 A2%
E 30.0130 40.2658 :
MVSTER(CNN Fusion) TWang ef al, 20227 _| #00040, F0.0T6s FISITG
CostFormer(CNN Fusion) 0.0097 20.0625 $36,69%

Table 4: Quantitative improvement of performance and incremental
cost of run time of different Transformers on DTU evaluation set.

5.3 Main Settings and Results on Tanks & Temples

For the evaluation on Tanks & Temples [Knapitsch er al.,
2017], we use the DTU [Aanes et al., 2016] dataset and

Method EPE | el(% | 3%
MVSNet [Yao ef al., 2018] 149_| 21.98 8.32
MVSNet-s [Darmon e7 al., 2021 T35_ | 25.91 8.55
CVP-MVSNet [Yang ef al., 2020a] T90_[ 19.73 10.24
VisMVSNet [Zhang e7 al., 2020] T47_[ 1847__| 759
CasMVSNet [Gu er al, 2020] T9s_[ 1525__| 7.60
EPPMVSNet [Ma ef al., 20217 T17_| 12.66 6.20
TransMVSNet [Ding ef al., 2021] 073 | 832 3.62
CostFormer(Based on PatchmatchNet) | 0.84 | 1237 | 459
CostFormer(Based on UniMVSNet) 0.43_|_7.05 2.70

Table 6: Quantitative results of different methods on BlendedMVS

6 Conclusion

In this work, we explore whether cost Transformer can im-
prove the cost aggregation and propose a novel CostFormer
with the cascade RDACT and RRT modules. The experimen-
tal results on DTU [Aanes et al., 2016] , Tanks & Temples
[Knapitsch er al., 2017], ETH3D [Schéps et al., 2017], and

--- Page 8 ---
BlendedMVS [Yao et al., 2020] show that our method is com-
petitive, efficient, and plug-and-play. Cost Transformer can
be your need for better cost aggregation in multi-view stereo.

References
[Aanees et al., 2016] Henrik Aanzes, Rasmus Ramsbgl
Jensen, George Vogiatzis, Engin Tola, and An-

ders Bjorholm Dahl. Large-scale data for multiple-view
stereopsis. Int. J. Comput. Vis., 120(2):153-168, 2016.

[Chen et al., 2019] Rui Chen, Songfang Han, Jing Xu, and
Hao Su. Point-based multi-view stereo network. In JCCV,
pages 1538-1547. IEEE, 2019.

[Cheng et al., 2020] Shuo Cheng, Zexiang Xu, Shilin Zhu,
Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao
Su. Deep stereo using adaptive thin volume representation
with uncertainty awareness. In CVPR, pages 2521-2531.
IEEE, 2020.

[Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul
Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong
Kim. Cats: Cost aggregation transformers for visual cor-
respondence. Advances in Neural Information Processing
Systems, 34, 2021.

[Darmon et al., 2021] Frangois Darmon, Bénédicte Bascle,
Jean-Clément Devaux, Pascal Monasse, and Mathieu
Aubry. Deep multi-view stereo gone wild. CoRR,
abs/2104.15119, 2021.

[Ding et al.,2021] Yikang Ding, Wentao Yuan, Qingtian
Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and
Xiao Liu. Transmysnet: Global context-aware multi-
view stereo network with transformers. arXiv preprint
arXiv:2111.14600, 2021.

[Ding er al., 2022] Yikang Ding, Wentao Yuan, Qingtian
Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and
Xiao Liu. Transmvsnet: Global context-aware multi-view
stereo network with transformers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8585-8594, 2022.

[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,
Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Min-
derer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929, 2020.

(Fuhrmann et al., 2014] Simon Fuhrmann, Fabian Langguth,
and Michael Goesele. Mve - a multi-view reconstruction
environment. In GCH, pages 11-18. Eurographics Asso-
ciation, 2014.

[Furukawa and Ponce, 2010] Yasutaka Furukawa and Jean
Ponce. Accurate, dense, and robust multiview stereopsis.
IEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362—1376,
2010.

[Galliani et al., 2015] Silvano Galliani, Katrin Lasinger, and
Konrad Schindler. Massively parallel multiview stereop-
sis by surface normal diffusion. In JCCV, pages 873-881.
IEEE Computer Society, 2015.

[Gu et al., 2020] Xiaodong Gu, Zhiwen Fan, Siyu Zhu,
Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost
volume for high-resolution multi-view stereo and stereo
matching. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2495—
2504, 2020.

[Hosni et al.,2012] Asmaa Hosni, Christoph Rhemann,
Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast
cost-volume filtering for visual correspondence and be-
yond. JEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 35(2):504—-511, 2012.

[Ji et al., 2017] Mengqi Ji, Juergen Gall, Haitian Zheng,
Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d
neural network for multiview stereopsis. In JCCV, pages
2326-2334. IEEE Computer Society, 2017.

[Knapitsch et al., 2017] Arno Knapitsch, Jaesik Park, Qian-
Yi Zhou, and Vladlen Koltun. Tanks and temples: bench-
marking large-scale scene reconstruction. ACM Trans.
Graph., 36(4):78:1-78:13, 2017.

[Lee et al., 2021] Jae Yong Lee, Joseph DeGol, Chuhang
Zou, and Derek Hoiem. Patchmatch-rl: Deep mvs with
pixelwise depth, normal, and visibility. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, October 2021.

(Li et al., 2021] Zhaoshuo Li, Xingtong Liu, Nathan
Drenkow, Andy Ding, Francis X Creighton, Russell H
Taylor, and Mathias Unberath. Revisiting stereo depth
estimation from a sequence-to-sequence perspective with
transformers. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 6197-6206,
2021.

[Liao et al., 2022] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe
Huang, Shihao Ren, Jia Guo, Wensen Feng, and Kai
Zhang. Wt-mvsnet: Window-based transformers for
multi-view stereo, 2022.

{Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,
Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 10012-
10022, 2021.

[Luo er al., 2019] Keyang Luo, Tao Guan, Lili Ju, Haipeng
Huang, and Yawei Luo. P-mvsnet: Learning patch-wise
matching confidence aggregation for multi-view stereo. In
ICCV, pages 10451-10460. IEEE, 2019.

[Ma et al., 2021] Xinjun Ma, Yue Gong, Qirui Wang, Jing-
wei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-
assembling based depth prediction for multi-view stereo.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5732-5740, 2021.

[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco
Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank

--- Page 9 ---
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 8024-8035. Curran Asso-
ciates, Inc., 2019.

[Peng er al., 2022] Rui Peng, Rongjie Wang, Zhenyu Wang,
Yawen Lai, and Ronggang Wang. Rethinking depth esti-
mation for multi-view stereo: A unified representation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2022.

[Scharstein and Szeliski, 2002] Daniel Scharstein and
Richard Szeliski. A taxonomy and evaluation of dense
two-frame stereo correspondence algorithms. Jnterna-
tional journal of computer vision, 47(1):7—42, 2002.

[Schénberger and Frahm, 2016] Johannes L. Schénberger
and Jan-Michael Frahm. Structure-from-motion revis-
ited. In CVPR, pages 4104-4113. IEEE Computer Society,
2016.

[Schéps et al.,2017] Thomas —_ Schdps, Johannes L.
Sch6nberger, Silvano Galliani, Torsten Sattler, Kon-
rad Schindler, Marc Pollefeys, and Andreas Geiger. A
multi-view stereo benchmark with high-resolution images
and multi-camera videos. In CVPR, pages 2538-2547.
IEEE Computer Society, 2017.

[Sun et al., 2021] Jiaming Sun, Zehong Shen, Yuang Wang,
Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free lo-
cal feature matching with transformers. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 8922-8931, 2021.

[Thomee et al., 2016] Bart Thomee, David A. Shamma, Ger-
ald Friedland, Benjamin Elizalde, Karl Ni, Douglas
Poland, Damian Borth, and Li-Jia Li. Yfcc100m: the new
data in multimedia research. Commun. ACM, 59(2):64-73,
2016.

[Tola et al., 2012] Engin Tola, Christoph Strecha, and Pascal
Fua. Efficient large-scale multi-view stereo for ultra high-
resolution image sets. Mach. Vis. Appl., 23(5):903-920,
2012.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advances in neural information processing systems,
30, 2017.

[Wang et al.,2021a] Fangjinhua Wang, Silvano Galliani,
Christoph Vogel, and Marc Pollefeys. Itermvs: Itera-
tive probability estimation for efficient multi-view stereo.
arXiv preprint arXiv:2112.05126, 2021.

[Wang et al., 2021b] Fangjinhua Wang, Silvano Galliani,
Christoph Vogel, Pablo Speciale, and Marc Pollefeys.
Patchmatchnet: Learned multi-view patchmatch stereo. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 14194-14203,
2021.

[Wang et al., 2022] Xiaofeng Wang, Zheng Zhu, Fangbo
Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang
Wang. Mvster: Epipolar transformer for efficient multi-
view stereo, 2022.

[Wei er al., 2021] Zizhuang Wei, Qingtian Zhu, Chen Min,
Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adap-
tive aggregation recurrent multi-view stereo network. In
Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 6187-6196, 2021.

[Xu and Tao, 2020] Qingshan Xu and Wenbing Tao. Pvs-
net: Pixelwise visibility-aware multi-view stereo network.
CoRR, abs/2007.07714, 2020.

[Yan et al., 2020] Jianfeng Yan, Zizhuang Wei, Hongwei
Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping
Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-
view stereo net with dynamic consistency checking. In Eu-
ropean Conference on Computer Vision, pages 674-689.
Springer, 2020.

[Yang et al., 2020a] Jiayu Yang, Wei Mao, Jose M Alvarez,
and Miaomiao Liu. Cost volume pyramid based depth
inference for multi-view stereo. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4877-4886, 2020.

[Yang et al., 2020b] Jiayu Yang, Wei Mao, Jose M. Alvarez,
and Miaomiao Liu. Cost volume pyramid based depth in-
ference for multi-view stereo. In CVPR, pages 4876-4885.
IEEE, 2020.

[Yao et al., 2018] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang,
and Long Quan. Mvsnet: Depth inference for unstructured
multi-view stereo. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 767-783, 2018.

[Yao et al., 2019] Yao Yao, Zixin Luo, Shiwei Li, Tianwei
Shen, Tian Fang, and Long Quan. Recurrent mvsnet
for high-resolution multi-view stereo depth inference. In
CVPR, pages 5525-5534. Computer Vision Foundation /
IEEE, 2019.

[Yao et al., 2020] Yao Yao, Zixin Luo, Shiwei Li, Jingyang
Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.
Blendedmvs: A large-scale dataset for generalized multi-
view stereo networks. Computer Vision and Pattern
Recognition (CVPR), 2020.

[Yi et al., 2020] Hongwei Yi, Zizhuang Wei, Mingyu Ding,
Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing
Tai. Pyramid multi-view stereo net with self-adaptive view
aggregation. In Andrea Vedaldi, Horst Bischof, Thomas
Brox, and Jan-Michael Frahm, editors, ECCV (9), volume
12354 of Lecture Notes in Computer Science, pages 766-
782. Springer, 2020.

[Yu and Gao, 2020] Zehao Yu and Shenghua Gao.  Fast-
mvsnet: Sparse-to-dense multi-view stereo with learned
propagation and gauss-newton refinement. In CVPR,
pages 1946-1955. IEEE, 2020.

[Zhang et al., 2020] Jingyang Zhang, Yao Yao, Shiwei Li,
Zixin Luo, and Tian Fang. Visibility-aware multi-view
stereo network. British Machine Vision Conference
(BMVC), 2020.


--- Page 10 ---
[Zhu er al., 2021] Jie Zhu, Bo Peng, Wanging Li, Haifeng
Shen, Zhe Zhang, and Jianjun Lei. Multi-view stereo with
transformer. ArXiv, abs/2112.00336, 2021.

