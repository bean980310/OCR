--- Page 1 ---
arXiv:2305.13077v1 [cs.CV] 22 May 2023

ControlVideo: Training-free Controllable
Text-to- Video Generation

Yabo Zhang! Yuxiang Wei! Dongsheng Jiang? Xiaopeng Zhang? Wangmeng Zuo! ‘= Qi Tian”

‘Harbin Institute of Technology "Huawei Cloud

Abstract

Text-driven diffusion models have unlocked unprecedented abilities in image gener-
ation, whereas their video counterpart still lags behind due to the excessive training
cost of temporal modeling. Besides the training burden, the generated videos also
suffer from appearance inconsistency and structural flickers, especially in long
video synthesis. To address these challenges, we design a training-free frame-
work called ControlVideo to enable natural and efficient text-to-video generation.
ControlVideo, adapted from ControlNet, leverages coarsely structural consistency
from input motion sequences, and introduces three modules to improve video
generation. Firstly, to ensure appearance coherence between frames, Control Video
adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate
the flicker effect, it introduces an interleaved-frame smoother that employs frame
interpolation on alternated frames. Finally, to produce long videos efficiently,
it utilizes a hierarchical sampler that separately synthesizes each short clip with
holistic coherency. Empowered with these modules, ControlVideo outperforms
the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualita-
tively. Notably, thanks to the efficient designs, it generates both short and long
videos within several minutes using one NVIDIA 2080Ti. Code is available at
https: //github.com/YBYBZhang/ControlVideo.

1 Introduction

Large-scale diffusion models have made a tremendous breakthrough on text-to-image synthesis [1,
22,26, 29,32] and their creative applications [6,8,21,37]. Several works [5,9, 11, 12,34] attempt
to replicate this success in the video counterpart, i.e., modeling higher-dimensional complex video
distributions in the wild world. However, training such a text-to-video model requires massive
amounts of high-quality videos and computational resources, which limits the further research and
applications by relevant communities.

To reduce the excessive training requirements, we study a new and efficient form: controllable
text-to-video generation with text-to-image models. This task aims to produce a video conditioned
on both a textual description and motion sequences (e.g., depth or edge maps). As shown in Fig. 1,
instead of learning the video distribution from scratch, it could efficiently leverage the generation
capability of pre-trained text-to-image generative models [26, 29] and coarsely temporal consistency
of motion sequences to produce vivid videos.

Recent studies [15,40] have explored leveraging the structure controllability of ControlNet [43]
or DDIM inversion [35] for video generation. Rather than synthesizing all frames independently,
[15,40] enhance appearance coherence by replacing original self-attention with the sparser cross-
frame attention. Nevertheless, their video quality is still far behind photo-realistic videos in terms
of: (i) inconsistent appearance between some frames (see Fig. 4 (a)), (ii) visible artifacts in large
motion videos (see Fig. 4 (b)), and (iii) structural flickers during inter-frame transitions. For (i) and

Preprint. Under review.

--- Page 2 ---
ControlNet

q
A swan
moving—»

ina lake A man riding a sleek, black motorbike through the winding roads.

James bond does the moonwalk on the beach, animation style.

Temporal
Extension

4

Aswan ControlVideo

moving —+

ina lake

Figure 1: Training-free controllable text-to-video generation. Left: ControlVideo adapts Con-
trolNet to the video counterpart by inflating along the temporal axis, aiming to directly inherit its
high-quality and consistent generation without any finetuning. Right: ControlVideo could synthesize
photo-realistic videos conditioned on various motion sequences, which are temporally consistent in
both structure and appearance. Results best seen at 500% zoom.

(ii), their sparser cross-frame mechanisms increase the discrepancy between the query and key in
self-attention modules, and hence impede inheriting high-quality and consistent generation from
pre-trained text-to-image models. For (iii), input motion sequences only provide the coarse-level
structure of videos, failing to smoothly transition between consecutive frames.

In this work, we propose a training-free Control Video for high-quality and consistent controllable
text-to-video generation, along with interleaved-frame smoother to enhance structural smoothness.
ControlVideo directly inherits the architecture and weights from ControlNet [43], while adapting it to
the video counterpart by extending self-attention with the fully cross-frame interaction. Different
from prior works [15,40], our fully cross-frame interaction concatenates all frames to become
a “larger image”, thus directly inheriting high-quality and consistent generation from ControlNet.
Interleaved-frame smoother deflickers the whole video via the interleaved interpolation at selected
sequential timesteps. As illustrated in Fig. 3, the operation at each timestep smooths the interleaved
three-frame clips by interpolating middle frames, and the combination at two consecutive timesteps
smooths the entire video. Since the smoothing operation is only performed at a few timesteps, the
quality and individuality of interpolated frames can be well retained by the following denoising steps.

To enable efficient long-video synthesis, we further introduce a hierarchical sampler to produce
separated short clips with long-term coherency. In specific, a long video is first split into multiple
short video clips with the selected key frames. Then, the key frames are pre-generated with fully
cross-frame attention for long-range coherence. Conditioned on pairs of key frames, we sequentially
synthesize their corresponding intermediate short video clips with the global consistency.

We conduct the experiments on extensively collected motion-prompt pairs. The experimental results
show that our method outperforms alternative competitors qualitatively and quantitatively. Thanks to
the efficient designs, i.e., the xFormers [17] implementation and hierarchical sampler, Control Video
can produce both short and long videos within several minutes using one NVIDIA 2080Ti.

In summary, our contributions are presented as follows:

¢ We propose a training-free Control Video for controllable text-to-video generation, which consists
of the fully cross-frame interaction, interleaved-frame smoother, and hierarchical sampler.

¢ The fully cross-attention demonstrates higher video quality and appearance consistency, while
interleaved-frame smoother further reduces structural flickers throughout a whole video.

¢ The hierarchical sampler enables efficient long-video generation in commodity GPUs.

2 Background

Latent diffusion model (LDM) [29] is an efficient variant of diffusion models [10] by applying the
diffusion process in the latent space rather than image space. LDM contains two main components.

--- Page 3 ---
= 1
— Self-Attenti
x\T| steps (] Conv Block = |g eee
1 2t
ta
1%
ControlNet Uj ttn Block
A swan movin. — gp 1 ' OT we gN-
in a lake $ 0 J | we at
1 + 1 Temporal
q » a | Inflation
= Bon 1 40
] og = 1 *e
: 833 oa
Soa < 1 7
2°38
7 Q i i
7 se ‘ 1 *
tap ap att
' Fully Cross-Frame
1 Attention
Figure 2: Overview of ControlVideo. For consistency in appearance, ControlVideo adapts Con-

trolNet to the video counterpart by adding fully cross-frame interaction into self-attention modules.
Considering the flickers in structure, the interleaved-frame smoother is integrated to smooth all
inter-frame transitions via the interleaved interpolation (see Fig. 3 for details).

Firstly, it uses an encoder € to compress an image = into latent code z = E(a) and a decoder to
reconstruct this image « ~ D(z), respectively. Secondly, it learns the distribution of image latent
codes 29 ~ Pdata(Zo) in a DDPM formulation [10], including a forward and a backward process.
The forward diffusion process gradually adds gaussian noise at each timestep t to obtain z;:

q(2\Z1-1) = N (203 V1 — Br2t-1, Bel), (1)

where {,}7_, are the scale of noises, and T’ denotes the number of diffusion timesteps. The backward
denoising process reverses the above diffusion process to predict less noisy z;~1:

po(Z1-1|2e) = N (2-15 oo (Zt, t), Do (Ze, t)). (2)

The jig and “1g are implemented with a denoising model €g with learnable parameters 6, which is
trained with a simple objective:

Lesimple = Eg(z)enN(0,1),t [Ile - co(21,)(3| . (3)

When generating new samples, we start from z7 ~ N(0, 1) and employ DDIM sampling to predict
2,—1 Of previous timestep:

2, — V1 — ayeo(21,t)
Zt-1 4-1 ( Jar : + /1 = a_i: €9(2e,t), (4)
t SS
a irection pointing to 21”
“ predicted 20”

where a, = Ili. ,(1 — 6;). We use 2;-,0 to represent “predicted zo” at timestep ¢ for simplicity.
Note that we use Stable Diffusion (SD) €9(z:,t, 7) as our base model, which is an instantiation of
text-guided LDMs pre-trained on billions of image-text pairs. 7 denotes the text prompt.

ControlNet [43] enables SD to support more controllable input conditions during text-to-image
synthesis, e.g., depth maps, poses, edges, etc. The ControlNet uses the same U-Net [30] architec-
ture as SD and finetunes its weights to support task-specific conditions, converting €9(z;,t, 7) to
€9(Z,t,c,7), where c denotes additional conditions. To distinguish the U-Net architectures of SD
and ControlNet, we denote the former as the main U-Net while the latter as the auxiliary U-Net.

3  ControlVideo

Controllable text-to-video generation aims to produce a video of length N conditioned on motion

sequences c = {c'}; N YD and a text prompt 7. As illustrated in Fig. 2, we propose a training-

free framework termed ControlVideo towards consistent and efficient video generation. Firstly,

--- Page 4 ---
Control Video is adapted from ControlNet by employing fully cross-frame interaction, which ensures
the appearance consistency with less quality degradation. Secondly, the interleaved-frame smoother
deflickers the whole video by interpolating alternate frames at sequential timesteps. Finally, the
hierarchical sampler separately produces short clips with the holistic coherency to enable long video
synthesis.

Fully cross-frame interaction. The main challenge of adapting text-to-image models to the video
counterpart is to ensure temporal consistency. Leveraging the controllability of ControlNet, motion
sequences could provide coarse-level consistency in structure. Nonetheless, even using the same
initial noise, individually producing all frames with ControlNet will lead to drastic inconsistency in
appearance (see row 2 in Fig. 6). To keep the video appearance coherent, we concatenate all video
frames to become a “large image”, so that their content could be shared via inter-frame interaction.
Considering that self-attention in SD is driven by appearance similarities [40], we propose to enhance
the holistic coherency by adding attention-based fully cross-frame interaction.

In specific, Control Video inflates the main U-Net from Stable Diffusion along the temporal axis,
while keeping the auxiliary U-Net from ControlNet. Analogous to [11, 15,40], it directly converts 2D
convolution layers to 3D counterpart by replacing 3 x 3 kernels with 1 x 3 x 3 kernels. In Fig. 2
(right), it extends self-attention by adding interaction across all frames:

T
Attention(Q, K, V) = Softmax( oF )-V, where Q = Wz, K=W*z,V=W'2, (5)
where z; = {z/}/\5" denotes all latent frames at timestep ¢, while W°, W*, and W™ project z:
into query, key, and value, respectively.

Previous works [15,40] usually replace self-attention with sparser cross-frame mechanisms, e.g., all
frames attend to the first frame only. Yet, these mechanisms will increase the discrepancy between the
query and key in self-attention modules, resulting in the degradation of video quality and consistency.
In comparison, our fully cross-frame mechanism combines all frames into a “large image”, and has
a less generation gap with text-to-image models (see comparisons in Fig. 6). Moreover, with the
efficient implementation, the fully cross-frame attention only brings little memory and acceptable
computational burden in short-video generation (< 16 frames).

Interleaved-frame smoother. Albeit
the videos produced by fully cross-frame
interaction are promisingly consistent in
appearance, they are still visibly flickering r lal Miealles ~} _
in structure. Input motion sequences REY LEEY LO EY *t-1>0
only ensure the synthesized videos with step t14 | i
coarse-level structural consistency, but ! i :

not enough to keep the smooth transition se ]ie2] Jed] | i | fied) fitz +} Xt-150
between consecutive frames. Therefore, ~

we further propose an interleaved-frame DDIM Denoising

smoother to mitigate the flicker effect in r

structure. As shown in Fig. 3, our key ve [2] [ed] |i | fied) [2 ~f iso

+ T

idea is to smooth each three-frame clip by
interpolating the middle frame, following :
by repeating it in an interleaved manner to fies] liz] fiea | | a) fiat] fie2| fies} } Xto0
smooth the whole video. ~

Stept 4

Specifically, our interleaved-frame Figure 3: Illustration of interleaved-frame smoother.
smoother is performed on predicted RGB At timestep ¢, predicted RGB frames 2; ,) are
frames at sequential timesteps. The smoothed into %;-,9 via middle-frame interpolation.
operation at each timestep interpolates The combination of two sequential timesteps reduces
the even or odd frames to smooth their the structural flickers over the entire video.
corresponding three-frame clips. In this

way, the smoothed three-frame clips from two consecutive timesteps are overlapped together to
deflicker the entire video. Before applying our interleaved-frame smoother at timestep ¢, we first
predict the clean video latent z;_,9 according to z;:

Zz, — V1 — azeo(Z1,t, ¢,T)
of At .

(6)

2t0

--- Page 5 ---
Source
Videos

= =| | & ; = :
Structure
Conditions
4 4

ee a

Tune-A-Video

Text2Video-Zero!

ControlVideo
(Ours)

~

A daring man is scaling a treacherous and A curious golden dog curiously wanders

jagged peak in the alpine wilderness. on the rocky mountain trail.
(a) Depth Maps (b) Canny Edges

Figure 4: Qualitative comparisons conditioned on depth maps and canny edges. Our Con-
trol Video produces videos with better (a) appearance consistency and (b) video quality than others. In
contrast, Tune-A-Video [40] fails to inherit structures from source videos, while Text2 Video-Zero [15]
brings visible artifacts in large motion videos. Results best seen at 500% zoom.

After projecting z;_,9 into a RGB video a_,9 = D(z:-40), we convert it to a more smoothed video
1,9 using our interleaved-frame smoother. Based on smoothed video latent 2,9 = E(@:,0), we
compute the less noisy latent z,_1 following DDIM denoising in Eq. 4:

2-1 = VOia1 24-30 + V/1 — 4-1 - €0(21,t.,7)- (7)

Notably, the above process is only performed at the selected intermediate timesteps, which has two
advantages: (i) the newly computational burden can be negligible and (ii) the individuality and quality
of interpolated frames are well retained by the following denoising steps.

Hierarchical sampler. Since video diffusion models need to maintain the temporal consistency
with inter-frame interaction, they often require substantial GPU memory and computational resources,
especially when producing longer videos. To facilitate efficient and consistent long-video synthesis,
we introduce a hierarchical sampler to produce long videos in a clip-by-clip manner. At each
timestep, a long video z; = {z} NS is separated into multiple short video clips with the selected

N
key frames 2/'°Y = {z!N-}¥<., where each clip is of length N, — 1 and the kth clip is denoted as
Bk = {2) eer ' | Then, we pre-generate the key frames with fully cross-frame attention for
long-range coherence, and their query, key, and value are computed as:

qrey = wezker, Kkey = Wk zkeu, prey _ wr zkeu, (8)

Conditioned on each pair of key frames, we sequentially synthesize their corresponding clips holding
the holistic consistency:

Qh= weet, Kaw ZhX2[Y%) VRS WY eE% 2PM)

--- Page 6 ---
Table 1: Quantitative comparisons of ControlVideo with other methods. We evaluate them on 125
motion-prompt pairs in terms of consistency, and the best results are bolded.

Method Structure Condition Frame Consistency (%) Prompt Consistency (%)
Tune-A-Video [40] DDIM Inversion [35] 94.53 31.57
Text2Video-Zero [15] Canny Edge 95.17 30.74
ControlVideo Canny Edge 96.83 30.75
Text2Video-Zero [15] Depth Map 95.99 31.69
ControlVideo Depth Map 97.22 31.81

4 Experiments

4.1 Experimental Settings

Implementation details. Our Control Video is adapted from ControlNet * [43] , and our interleaved-
frame smoother employs a lightweight RIFE [13] to interpolate the middle frame of each three-frame
clip. The synthesized short videos are of length 15, while the long videos usually contain about 100
frames. Unless otherwise noted, their resolution is both 512 x 512. During sampling, we adopt
DDIM sampling [35] with 50 timesteps, and interleaved-frame smoother is performed on predicted
RGB frames at timesteps {30, 31} by default. With the efficient implementation of xFormers [17],
our ControVideo could produce both short and long videos with one NVIDIA RTX 2080Ti in about 2
and 10 minutes, respectively.

Datasets. To evaluate our ControlVideo, we collect 25 object-centric videos from DAVIS
dataset [24] and manually annotate their source descriptions. Then, for each source description,
ChatGPT [23] is utilized to generate five editing prompts automatically, resulting in 125 video-prompt
pairs in total. Finally, we employ Canny and MiDaS DPT-Hybrid model [28] to estimate the edges
and depth maps of source videos, and form 125 motion-prompt pairs as our evaluation dataset. More
details are provided in the supplementary materials.

Metrics. Following [5,40], we adopt CLIP [25] to evaluate the video quality from two perspectives.
(i) Frame Consistency: the average cosine similarity between all pairs of consecutive frames, and (1i)
Prompt Consistency: the average cosine similarity between input prompt and all video frames.

Baselines. We compare our ControlVideo with three publicly available methods: (i) Tune-A-
Video [40] extends Stable Diffusion to the video counterpart by finetuning it on a source video.
During inference, it uses the DDIM inversion codes of source videos to provide structure guidance.
(ii) Text2 Video-Zero [15] is based on ControlNet, and employs the first-only cross-frame attention on
Stable Diffusion without finetuning. (iii) Follow-Your-Pose [18] is initialized with Stable Diffusion,
and is finetuned on LAION-Pose [18] to support human pose conditions. After that, it is trained on
millions of videos [41] to enable temporally-consistent video generation.

4.2 Qualitative and quantitative comparisons

Qualitative results. Fig. 4 first illustrates the visual comparisons of synthesized videos conditioned
on both (a) depth maps and (b) canny edges. As shown in Fig. 4 (a), our Control Video demonstrates
better consistency in both appearance and structure than alternative competitors. Tune-A-Video fails
to keep the temporal consistency of both appearance and fine-grained structure, e.g., the color of
coat and the structure of road. With the motion information from depth maps, Text2Video-
Zero achieves promising consistency in structure, but still struggles with incoherent appearance in
videos e.g., the color of coat. Besides, our Control Video also performs more robustly when
dealing with large motion inputs. As illustrated in Fig. 4 (b), Tune-A-Video ignores the structure
information from source videos. Text2Video-Zero adopts the first-only cross-frame mechanism to
trade off frame quality and appearance consistency, and generates later frames with visible artifacts.
In contrast, with the proposed fully cross-frame mechanism and interleaved-frame smoother, our
ControlVideo can handle large motion to generate high-quality and consistent videos.

*https://huggingface.co/Illyasviel/ControlNet

--- Page 7 ---
Table 2: User preference study. The numbers denote the percentage of raters who favor the videos
synthesized by our Control Video over other methods.

Method Comparison

Ours vs. Tune-A-Video [40]
Ours vs. Text2Video-Zero [15]

Video Quality | Temporal Consistency

76.0% 81.6%

™ AAS

Text Alignment
68.0%
65.6%

Source
Videos

Structure
Conditions

Tune-A-Video

Text2Video-Zero

Fully

Cross-frame 54

Follow-Your-Pose

ait |
ControlVideo
(Ours)

Iron man does the moonwalk
on the road.

Figure 5: Qualitative comparisons on poses.
Tune-A-Video [40] only preserves original hu-
man positions, while Text2 Video-Zero [15] and

Fully
Cross-frame
+ Smoother

A mighty elephant marches ste steadily through
the rugged terrain.

Figure 6: Qualitative ablation studies on
cross-frame mechanisms and interleaved-frame
smoother. Given canny edges in the first row,

Follow-Your-Pose [18] produce frames with ap-
pearance incoherence, e.g., changing faces of
iron man. Our Control Video achieves better con-
sistency in both structure and appearance.

our fully cross-frame interaction produces video
frames with higher quality and consistency than
other mechanisms, and adding our smoother fur-
ther enhances the video smoothness.

Fig. 5 further shows the comparison conditioned on human poses. From Fig. 5, Tune-A-Video only
maintains the coarse structures of the source video, i.e., human position. Text2Video-Zero and
Follow-Your-Pose produce video frames with inconsistent appearance, e.g., changing faces of
iron man (in row 4) or disappearing objects in the background (in row 5). In comparison,
our Control Video performs more consistent video generation, demonstrating its superiority. More
qualitative comparisons are provided in the supplementary materials.

Quantitative results. We have also compared our Control Video with existing methods quantita-
tively on 125 video-prompt pairs. From Table |, our Control Video conditioned on depth outperforms
the state-of-the-art methods in terms of frame consistency and prompt consistency, which is con-
sistent with the qualitative results. In contrast, despite finetuning on a source video, Tune-A-Video
still struggles to produce temporally coherent videos. Although conditioned on the same structure
information, Text2Video-Zero obtains worse frame consistency than our ControlVideo. For each
method, the depth-conditioned models generate videos with higher temporal consistency and text
fidelity than the canny-condition counterpart, since depth maps provide smoother motion information.

4.3 User study

We then perform the user study to compare our Control Video conditioned on depth maps with other
competing methods. In specific, we provide each rater a structure sequence, a text prompt, and
synthesized videos from two different methods (in random order). Then we ask them to select the
better synthesized videos for each of three measurements: (i) video quality, (ii) temporal consistency

--- Page 8 ---
Table 3: Quantitative ablation studies on cross-frame mechanisms and interleaved-frame smoother.
The results indicate that our fully cross-frame mechanism achieves better frame consistency than
other mechanisms, and the interleaved-frame smoother significantly improves the frame consistency.

Cross-Frame Mechanism _ Frame Consistency (%) Prompt Consistency (%) Time Cost (min)

Individual 89.94 30.79 1.2
First-only 94.92 30.54 1.2
Sparse-Causal 95.06 30.59 1.5
Fully 95.36 30.76 3.0

Fully + Smoother 96.83 30.79 3.5

a steamship on the ocean, at sunset, sketch style

Frame 0 Frame 55

Frame 66 Frame 77 Frame 88 Frame 99 Frame 110 Frame 121

Figure 7: A long video produced with our hierarchical sampling. Motion sequences are shown
on the top left. Using the efficient sampler, our Control Video generates a high-quality long video
with the holistic consistency. Results best seen at 500% zoom.

throughout all frames, and (iii) text alignment between prompts and synthesized videos. The
evaluation set consists of 125 representative structure-prompt pairs. Each pair is evaluated by 5
raters, and we take a majority vote for the final result. From Table 2, the raters strongly favor our
synthesized videos from all three perspectives, especially in temporal consistency. On the other hand,
Tune-A-Video fails to generate consistent and high-quality videos with only DDIM inversion for
structural guidance, and Text2Video-Zero also produces videos with lower quality and coherency.

4.4 Ablation study

Effect of fully cross-frame interaction. To demonstrate the effectiveness of the fully cross-frame
interaction, we conduct a comparison with the following variants: i) individual: no interaction
between all frames, ii) first-only: all frames attend to the first one, iii) sparse-causal: each frame
attends to the first and former frames, iv) fully: our fully cross-frame, refer to Sec. 3. Note that, all the
above models are extended from ControlNet without any finetuning. The qualitative and quantitative
results are shown in Fig. 6 and Table 3, respectively. From Fig. 6, the individual cross-frame
mechanism suffers from severe temporal inconsistency, e.g., colorful and black-and-white
frames. The first-only and sparse-causal mechanisms reduce some appearance inconsistency by
adding cross-frame interaction. However, they still produce videos with structural inconsistency
and visible artifacts, e.g., the orientation of the elephant and duplicate nose (row 3
in Fig. 6). In contrast, due to less generation gap with ControlNet, our fully cross-frame interaction
performs better appearance coherency and video quality. Though the introduced interaction brings an
extra 1 ~ 2x time cost, it is acceptable for a high-quality video generation.

Effect of interleaved-frame smoother. We further analyze the effect of the proposed interleaved-
frame smoother. From Fig. 6 and Table 3, our interleaved-frame smoother greatly mitigates structural
flickers and improves the video smoothness.

4.5 Extension to long-video generation

Producing a long video usually requires an advanced GPU with high memory. With the proposed
hierarchical sampler, our Control Video achieves long video generation (more than 100 frames) in
a memory-efficient manner. As shown in Fig. 7, our Control Video can produce a long video with
consistently high quality. Notably, benefiting from our efficient sampling, it only takes approximately
ten minutes to generate 100 frames with resolution 512 x 512 in one NVIDIA RTX 2080Ti. More
visualizations of long videos can be found in the supplementary materials.

--- Page 9 ---
5 Related work

Text-to-image synthesis. Through pre-training on billions of image-text pairs, large-scale gen-
erative models [1, 2,3, 4, 14, 22, 26, 27, 29, 32, 33,42] have made remarkable progress in creative
and photo-realistic image generation. Various frameworks have been explored to enhance image
quality, including GANs [7, 14, 33], autoregressive models [2, 3,4, 22,42], and diffusion mod-
els [1, 10,26, 29,32]. Among these generative models, diffusion-based models are well open-sourced
and popularly applied to several downstream tasks, such as image editing [8, 19] and customized
generation [6, 16, 31,37]. Besides text prompts, several works [20, 43] also introduce additional
structure conditions to pre-trained text-to-image diffusion models for controllable text-to-image
generation. Our Control Video is implemented based on the controllable text-to-image models to
inherit their ability of high-quality and consistent generation.

Text-to-video synthesis. Large text-to-video generative models usually extend text-to-image mod-
els by adding temporal consistency. Earlier works [12, 36, 38,39] adopt an autoregressive framework
to synthesize videos according to given descriptions. Capitalizing on the success of diffusion models
in image generation, recent works [9, | 1,34] propose to leverage their potential to produce high-quality
videos. Nevertheless, training such large-scale video generative models requires extensive video-text
pairs and computational resources.

To reduce the training burden, Gen-1 [5] and Follow- Your-Pose [18] provide coarse temporal infor-
mation (e.g., motion sequences) for video generation, yet are still costly for most researchers and
users. By replacing self-attention with the sparser cross-frame mechanisms, Tune-A-Video [40] and
Text2Video-Zero [15] keep considerable consistency in appearance with little finetuning. Our Con-
trolVideo also adapts controllable text-to-image diffusion models without any training, but generates
videos with better coherency in both structure and appearance.

6 Discussion

In this paper, we present a training-free framework, namely Control Video, towards consistent and
efficient controllable text-to-video generation. Particularly, Control Video is inflated from ControlNet
by adding fully cross-frame interaction to ensure appearance coherence without sacrificing video
quality. Besides, interleaved-frame smoother interpolates alternate frames at sequential timesteps to
effectively reduce structural flickers. With the further introduced hierarchical sampler and memory-
efficient designs, our ControlVideo can generate both short and long videos in several minutes
with commodity GPUs. Quantitative and qualitative experiments on extensive motion-prompt pairs
demonstrate that ControlVideo performs better than previous state-of-the-arts in terms of video
quality and temporal consistency.

Limitations. While our ControlVideo enables consistent and high-quality video generation, it still
struggles with producing videos beyond input motion sequences. For example, given sequential poses
of Michael Jackson’s moonwalk, it is difficult to generate a vivid video according to text prompts
like Iron man runs on the street. In the future, we will explore how motion sequences can be
adapted to new ones based on input text prompts, so that users can create more diverse videos with
our Control Video.

Broader impact. Large-scale diffusion models have made tremendous progress in text-to-video
synthesis, yet these models are costly and unavailable to the public. Our Control Video focuses
on training-free controllable text-to-video generation, and takes an essential step in efficient video
creation. Concretely, Control Video could synthesize high-quality videos with commodity hardware,
hence, being accessible to most researchers and users. For example, artists may leverage our approach
to create fascinating videos with less time. Moreover, Control Video provides insights into the tasks
involved in videoss, e.g., video rendering, video editing, and video-to-video translation. On the flip
side, albeit we do not intend to use our model for harmful purposes, it might be misused and bring
some potential negative impacts, such as producing deceptive, harmful, or explicit videos. Despite
the above concerns, we believe that they could be well minimized with some steps. For example,
an NSFW filter can be employed to filter out unhealthy and violent content. Also, we hope that the
government could establish and improve relevant regulations to restrict the abuse of video creation.

--- Page 10 ---
References

1

20

21

22

23
24

Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 1,9

Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked
generative transformers. arXiv preprint arXiv:2301.00704, 2023. 9

Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou
Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021.
9

Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation
via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 9

Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.
Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,
2023. 1, 6,9

Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.
arXiv preprint arXiv:2208.01618, 2022. 1,9

Jan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 9
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-
prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 1,9
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1,9

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3,
9

Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.
Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 1, 4,9

Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for
text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1,9

Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow
estimation for video frame interpolation. In ECCV, 2022. 6

Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
Scaling up gans for text-to-image synthesis. In CVPR, 2023. 9

Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang,
Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot
video generators. arXiv preprint arXiv:2303.13439, 2023. 1,2,4,5,6,7,9

Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept
customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. 9

Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren,
Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular
and hackable transformer modelling library. https: //github.com/facebookresearch/xformers,
2022. 2,6

Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your
pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023.
6,7,9

Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image
synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 9
Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-
adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv
preprint arXiv:2302.08453, 2023. 9

Minheng Ni, Zitong Huang, Kailai Feng, and Wangmeng Zuo. Imaginarynet: Learning object detectors
without real images and annotations. arXiv preprint arXiv:2210.06886, 2022. |

Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1,9

TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 6

Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeldez, Alex Sorkine-Hornung, and Luc
Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675,
2017. 6

10

--- Page 11 ---
25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In JCML, 2021. 6

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1,9

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. In JCML, 2021. 9

René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 6

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj6rn Ommer. High-resolution
image synthesis with latent diffusion models. In CVPR, 2022. 1, 2,9

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In MICCAI, 2015. 3

Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint
arXiv:2208.12242, 2022. 9

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-
image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1,
9

Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power
of gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515, 2023. 9

Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,
Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv
preprint arXiv:2209.14792, 2022. 1,9

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020. 1, 6

Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-
mad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video
generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 9

Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encod-
ing visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint
arXiv:2302.13848, 2023. 1,9

Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.
Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806,
2021. 9

Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Niiwa: Visual
synthesis pre-training for neural visual world creation. In ECCV, 2022. 9

Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video
generation. arXiv preprint arXiv:2212.11565, 2022. 1, 2, 4,5,6,7,9

Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining
Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In
CVPR, 2022. 6

Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich
text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 9

Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv
preprint arXiv:2302.05543, 2023. 1, 2,3, 6,9

11

