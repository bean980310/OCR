--- Page 1 ---
arXiv:2306.07941v1 [cs.CL] 9 Jun 2023

GPT-CALLS: ENHANCING CALL SEGMENTATION AND TAGGING BY GENERATING
SYNTHETIC CONVERSATIONS VIA LARGE LANGUAGE MODELS

Itzik Malkiel'* Uri Alon’* Yakir Yehuda? Shahar Keren! Oren Barkan‘ Royi Ronen! Noam Koenigstein4

'Microsoft, ?Technion, “The Open University, ‘Tel-Aviv University

ABSTRACT

Transcriptions of phone calls are of significant value across
diverse fields, such as sales, customer service, healthcare, and
law enforcement. Nevertheless, the analysis of these recorded
conversations can be an arduous and time-intensive process,
especially when dealing with extended or multifaceted di-
alogues. In this work, we propose a novel method, GPT-
distilled Calls Segmentation and Tagging (GPT-Calls), for ef-
ficient and accurate call segmentation and topic extraction.
GPT-Calls is composed of offline and online phases. The of-
fline phase is applied once to a given list of topics and involves
generating a distribution of synthetic sentences for each topic
using a GPT model and extracting anchor vectors. The on-
line phase is applied to every call separately and scores the
similarity between the transcripted conversation and the topic
anchors found in the offline phase. Then, time domain analy-
sis is applied to the similarity scores to group utterances into
segments and tag them with topics. The proposed paradigm
provides an accurate and efficient method for call segmenta-
tion and topic extraction that does not require labeled data,
thus making it a versatile approach applicable to various do-
mains. Our algorithm operates in production under Dynamics
365 Sales Conversation Intelligence, and our research is based
on real sales conversations gathered from various Dynamics
365 Sales tenants.

1. INTRODUCTION

In today’s highly competitive market, sales agents play a crit-
ical role in driving sales and maintaining a strong customer
base. One of the primary ways they interact with customers
is through phone calls. These phone calls are often recorded
and transcribed for quality assurance, training and coaching
other sellers, curating insights, and more. Therefore, sales
departments often maintain an extensive database consisting
of millions of call transcriptions, which serves as an important
source of information for numerous tasks. However, analyz-
ing recorded calls can be a challenging and time-consuming
task, particularly when the conversations are lengthy or cover
multiple topics.

* Denotes equal contribution.

Acommon approach is to segment calls by post-processing
the recorded text and assigning a topic to each segment. This
simplifies the process for sellers to locate and extract im-
portant information from previous calls. Call segmentation
involves breaking down conversations into smaller sections
based on specific topics or themes discussed. This segmen-
tation and tag capability greatly facilitates the day-to-day
tasks of sellers and their managers. It enables sales agents
and managers to easily track and analyze past calls, catego-
rize them based on conversation topics, navigate to relevant
parts of a call to extract crucial information, improve search
engines that work with transcribed calls, and more.

Moreover, automated segmentation and tagging of recorded
calls can help businesses optimize their sales strategies and
processes across several dimensions:

¢ Providing personalized coaching to sellers. Managers
can listen to specific parts of the call to provide feed-
back and guidance, rather than listening to the entire
call.

¢ Providing insights into customer needs, preferences,
and pain points. Segmented and tagged information
can be used to tailor sales strategies and improve cus-
tomer experience.

¢ Monitoring compliance with legal and ethical guide-
lines in sales conversations. Ensuring that sellers are
adhering to company policies and regulations.

* Generating reports and dashboards that provide insights
into sales performance and customer behavior. This in-
formation can be used to make data-driven decisions
and improve overall business outcomes.

¢ Identifying key moments in the call that may be related
to successful outcomes. This information can be used
to train sales agents and improve their performance.

Ultimately, all of the above lead to increased customer satis-
faction, loyalty, sales, and revenue generation. Unfortunately,
despite many advances in the field, current techniques for call
segmentation and tagging have limitations that hinder their
impact and penetration on the day-to-day work of sellers and
their managers. Specifically, they often produce sub-optimal

--- Page 2 ---
Call to lvan Cashin - Gym membership

Overview Notes Action items Mentions Transcript BP Search
D Copy to clipboard 2 suggested notes Preview © x © wreywamer
0204 Hello Ivan, this is Nancy from Dynamic fitness.
10 minutes ag0 Call highlights © a
Wan Cashin
Customer wants to use his free pass e
Executive summary: “a « am 00.08 Oh hi
- discovery call - @ __verey Warner
~ Interested in 100 licenses Customer is trying to shed 15 to 20 pounds 00:10 I'm ealling you teday to find out if you've had a chance to
+ Add oo use your free pass yet.
Call minutes:
+ Customer wants to use his free pass See all ©  vercssin
0015 No, not yet.
+ Customer is trying to shed 15 to 20 pounds Retentions A
Nancy Warner
+ Agent educated customer that we have programs designed Nancy Warner to send email about "some helpful 0017 OK, that's no problem.

for weight loss

+ Add + 02:17

+ Customer wants to know how to achieve his goal of losing
weight by the summer
wv

Greetings and in... Request and com...
@ Nancy War. [OO OE 08 toe OCG Ooeneneee
© Wan cashin | « 1 [}
Sentiment ® Positive § Neutral ) Negative Aments (0) © Id «a

information” to +353892678274.

Request and complaint

Nancy Warmer

Let me ask you what made you decide to download our
free pass

Closing

Cooooende ae tee
cocaee @ ( t 0 Tt] i |

pp Pl ) eeO> 00:00/ 02:40

Fig. 1. The Calls Summary dashboard. Executive summary (left), suggested notes (middle), and the transcripted conversation
(right) are presented. The predicted segmentation and their associated topics are showcased at the bottom, integrated with a
corresponding timeline bar representing the duration of the recorded call.

accuracy, struggle to represent various topics, and require sig-
nificant labeled data and domain expertise to produce accurate
ground truth segmentation. As a result, businesses have been
slow to adopt post-processing segmentation in recorded calls.

In this work, we propose a method that overcomes these
limitations and provides accurate call segmentation and topic
extraction without the need for labeled data. Based on a GPT
model, our method generates synthetic data that mimics the
natural language used in recorded calls, while focusing on a
pre-defined set of topics. The synthetic data is then used by a
smaller Transformer network to accurately segment calls and
tag them with topics.

Our method, which we refer to as GPT-Calls, is designed
to efficiently identify the topics discussed during a phone con-
versation by analyzing the similarity between the conversa-
tion utterances and predefined topic anchors. The algorithm
consists of offline and online phases. In the offline phase,
GPT-Calls generates a distribution of synthetic sentences for
each topic using a GPT model [I] [2]. Then, it embeds and
clusters the sentences separately to extract anchor vectors. In
the online phase, GPT-Calls processes a conversation tran-
script by embedding each utterance using a language model
and scoring it for similarity with the topic anchors. Then, it
applies time domain analysis over the scores of consecutive
utterances and groups them into segments that are distinctly

identified with a particular topic. The remaining segments
that do not correlate with any of the topics are considered
“background”.

The GPT-Calls method offers an effective and precise so-
lution for call segmentation and topic extraction that can cater
to any topic without the need for labeled data. In this way, or-
ganizations can select a list of customized topics that are rele-
vant to their particular business needs. Additionally, the algo-
rithm is generic and can be implemented in diverse domains,
such as customer support, sales conversations, surveys, and
more. Finally, the online phase of the system can be applied in
near-real-time scenarios, enabling the segmentation and tag-
ging of an entire call within two-three seconds using standard
hardware. The GPT-Calls was recently adopted in Dynamics
365 Sales, and its predicted segmentations are apparent in the
lower part of the Calls Summary dashboard (see Figure[ip.

Our contributions are as follows: (1) we introduce the
GPT-Calls scheme, a general method designed for the analy-
sis of recorded calls in any domain. (2) we evaluate and report
the performance of GPT-Calls on a diverse dataset of calls
from multiple domains and tenants. (3) we compare our pro-
posed method against other state-of-the-art alternatives. Our
results demonstrate that the GPT-Calls method outperforms
other approaches by a sizeable margin, and across all datasets.

--- Page 3 ---
morning, this is
John speaking

from ABC. Do >

you have a few oe °
minutes?" Vectorization Gn,

Topic probabilities

Offl i ne Topic embedding
GPT-3
Prompt: "We have o © aston +
“an agent and packages (Oe)
a costumer => starting from => =>
discussing $500 up to
pricing" $5000..." ‘one
OKs) Oo + /)
On li ne Greeting Ending
"Good ay “

Ending Greeting Schedule

Time oe
Domain

analysis

RS
we

Muter#1 muter#2 muter#3 Time domain

Fig. 2. An illustration of the offline and online phases of GPT-Calls.

2. RELATED WORK

In recent years, there has been a growing interest in text
segmentation, which involves dividing text passages into co-
herent segments based on their content. While traditional text
segmentation methods have relied on features such as punc-
tuation, paragraph breaks, or rule-based approaches, these
methods may not always capture the underlying semantic
structure of the text.

To address this challenge, several recent studies have pro-
posed using semantic word embeddings to identify segments
based on the coherence of the relationships between words.
One such study proposes a method for text segmentation
based on semantic word embeddings In this work, the
authors use a pretrained word embedding model to gener-
ate embeddings for each word in the text and then apply a
greedy algorithm to group the words into segments. The au-
thors demonstrate that their approach outperforms traditional
methods in terms of segmentation accuracy and that the iden-
tified segments correspond well to the topics discussed in the
text. In the experiments section, we compare our method
with the previous method used in Dynamics 365 Sales for
call segmentation, which was based on the same approach of
Bi.

A different direction has been to employ text summa-
rization methods for topic tagging. This approach involves
assigning one or more tags to a given text to represent its
key topics or themes. One such approach, known as ex-
tractive summarization, involves extracting key sentences
from the text and using them to tag the text with pre-defined
topics. Another approach, known as abstractive summariza-
tion, employs neural-based summarization models, such as

sequence-to-sequence models or transformer models, to gen-
erate concise summaries that can be used to predict relevant
tags. The latter approach can utilize the PEGASUS summa-
rization model [4], which generates summaries that can ease
the process of predicting tags (compared to predicting tags
for the original text). The previous method used in Dynam-
ics 365 sales built upon the PEGASUS model to summarize
the segment and infer a relevant topic. More details and
evaluations for this method can be found in Sec[4]

TextTiling [5] is a prominent text segmentation algorithm
that efficiently divides long text into coherent topical sec-
tions. It utilizes local lexical cohesion and focuses on identi-
fying abrupt shifts in the thematic structure of a text, which
are indicative of topic boundaries. By employing statistical
techniques such as tokenization, similarity measurement, and
smoothing, TextTiling extracts informative features from the
text and clusters them into distinct sections, allowing for bet-
ter understanding and organization of large text corpora. This
versatile segmentation algorithm has been widely applied in
various natural language processing tasks, including informa-
tion retrieval, summarization, and text classification, making
it a valuable tool for researchers in the field of computational
linguistics.

Topic segmentation of meetings has gained significant
attention as a means to automatically partition meeting tran-
scripts into coherent segments, facilitating a better under-
standing of the discourse structure. In [6], the authors pro-
pose an unsupervised method for meetings segmentation by
BERT embeddings (BERT-TSeg). Their method builds upon
BERT to generate contextualized word representations for
each utterance in the transcript. By leveraging these embed-
dings, BERT-TSeg computes the similarity between adjacent

--- Page 4 ---
Well the lowest price point I’m willing to accept is hal
again the current offer. Could we arrangement a 4%
increase across the board to bridge this gap? Or a 50%
discount up to a certain quantity.

That's a steep new price, do you have room to offer by

doing anything special with our payment systems?

Certainly. If partial flexible payments are of interest, our
deferred pricing terms potentially could ensure a smooth
entrance for your team we could offer deferred payments
for up to 30 days on the product of interest without
affecting the final per unit price.

[+1000 more sentences]

Fig. 3. Representative synthetic sentences generated by GPT-
3 for the topic pricing.

segments and constructs a similarity matrix. Subsequently,
a hierarchical clustering algorithm is applied to this matrix
to identify topic boundaries. In this study, we conduct a
comprehensive evaluation and comparison between our pro-
posed method and BERT-TSeg to assess the advancements
and efficacy of our approach in meeting topic segmentation.

3. METHOD

The GPT-Calls algorithm consists of two distinct phases: an
offline phase and an online phase. The offline phase uses a
GPT model to generate synthetic data. This phase is applied
once for a user-defined list of topics. After the offline phase,
the algorithm invokes an online phase that reinforces repre-
sentations extracted during the offline phase to accurately pre-
dict segmentation and topics for a given call.

3.1. Offline Phase

In the offline phase, the algorithm utilizes a GPT model to
generate synthetic data, leveraging a given list of desired top-
ics chosen by the user. This phase is executed only once for
a specific set of desired topics. Subsequently, in the online
phase, the algorithm uses the representations extracted dur-
ing the offline phase to predict segmentation and topics for
individual calls.

Specifically, for each topic, we build a prompf{"] and use
GPT-3 to generate thousands of sentences that are semanti-
cally correlated with the topic.

A representative sample of sentences generated for the
“pricing” topic is shown in Figure An example of our
prompts can be found in the Appendix Section[A]

Sentences produced for each subject are embedded via a
pretrained Sentence-BERT (SBERT) model [7]. This model,

'The prompts contain one example of a few sentences and a topic which
are followed by a request for the model to generate another example for the
query topic

an adjustment of the standard BERT model [8], is devised
to encode entire sentences into a 384-dimensional embedding
vector, enabling the use of the cosine similarity metric to mea-
sure semantic similarity.

Then, the DBSCAN {9] algorithm is applied on the sen-
tence embeddings of each topic, in order to extract a set of
multiple “anchors” representing the distribution of the topic.
DBSCAN is a density-based clustering algorithm that groups
data into clusters based on the density of samples. High-
density regions are grouped into a cluster, and samples in
low-density areas are marked as outliers. For each topic, DB-
SCAN is applied to retrieve a set of clusters. The center of
each cluster is extracted and used as an anchor.

These anchors will be used during the online phase to in-
fer the topic probabilities for each utterance in the call. The
pipeline of the offline phase is illustrated in Figure [2] (upper
part).

Overall, the offline phase involves defining topics, gen-
erating synthetic sentences, embedding them using SBERT,
clustering the embeddings using DBSCAN, and extracting
anchors. The anchors will be used later during the online
phase to infer the probabilities of the topics.

3.2. Online Phase

In the online phase, GPT-Calls operates on the transcriptions
of the recorded conversations and predicts the topic probabil-
ities for each utterance in a given conversation. An utterance
is an atomic unit of speech, which is mostly converted to a
single sentence or a sub-sentence by the transcription model.

The method employs the Azure Cognitive Service tran-
scription modef?] and embeds the resulting transcripted ut-
terances through SBERT. GPT-Calls then iterates over the
embedding of each transcripted utterance, scoring its similar-
ity with all anchors of the pre-defined topics. For each topic,
an utterance-topic score is defined by the maximal cosine
similarity between the transcripted utterance embedding and
the anchors associated with the topic (which are also vectors
in the same latent space, as described in the offline phase).
The utterance-topic scores for each topic are transformed into
probabilities using the Softmax function. By performing this
process, one obtains a sequence of vectors, where each vector
represents the probability that the corresponding utterance
relates to each of the topics.

To improve the accuracy of the topic probabilities, GPT-
Calls applies a time-domain analysis to the above sequence,
treating it as a multivariant time series. It identifies the peak
points in each dimension of the time series, referred to as
“heat sources”. GPT-Calls then applies a heat diffusion strat-
egy to the neighboring samples surrounding each heat source.
For every sample in the sequence and across each dimension,
GPT-Calls calculates the distance to the nearest right and left

?https://azure.microsoft.com/en-us/products/cognitive-services/speech-
to-text/

--- Page 5 ---
heat sources, strengthens the probability of the current sam-
ple’s topic in proportion to the value of the closest right and
left heat sources, and decays the probability by the distance.
In other words, the probabilities of samples that are close to
other samples that highly correlate with a specific topic will
be slightly promoted toward the same topic.

This approach intends to counteract the presence of noisy
data samples which often manifest in particular topics such as
identification.

After the heat diffusion process, a Softmax function is ap-
plied again to each utterance to assure its scores across the
topics are valid probabilities that sum to 1. GPT-Calls then
applies a sliding window technique to tag windows of con-
secutive utterances with topics. Different window widths are
utilized for each topic, which are hyperparameters determined
individually for each topic using a validation set. The cumu-
lative probability of a specific topic is computed for each win-
dow by averaging the relevant probabilities of all utterances
within that window. If the cumulative probability exceeds a
predefined threshold (configurable for each topic), the win-
dow is labeled with the corresponding topic. This process is
repeated for all topics.

At the end of the above process, the sequence is associated
with windows tagged with topics. GPT-Calls then iterates
through the tagged windows and merges consecutive win-
dows with the same topic. If a window or sub-window was
tagged with more than one topic, the leading topic (with the
highest score) is chosen, and the other windows are updated
accordingly. The predicted segmentation and tagging are re-
trieved as the final output. A visualization of the predicted
segmentation and tags along with the underlying probabilities
of each utterance can be seen in Figure/4] The pipeline of the
online phase is illustrated in Figure[2] (bottom part).

4, EXPERIMENTS

In our evaluation, we compare GPT-Calls with baseline meth-
ods mentioned in the related work section, including TextTil-
ing [5] and the unsupervised BERT-TSeg method [6]. Text-
Tiling employs a sliding window approach based on lexical
cohesion to identify coherent segments within a document.
BERT-TSeg, on the other hand, utilizes BERT embeddings
and hierarchical clustering to identify topic boundaries in
meeting transcripts.

We also compare to the previous model used in Dynam-
ics 365 Sales, which employed a technique that utilizes a
Greedy Segmentation approach followed by a GPT-based
distilled Summary for Tagging (GSGST). GSGST first ap-
plies a segmentation procedure and then tags each segment
with a relevant topic. The segmentation begins by embedding
all the utterances in a given call using a pre-trained SBERT
model. Then, the segments are inferred by employing the
greedy method introduced in [3]?]over the utterances embed-

>The implementation can be found here

1.07

0.8

0.6

0.4

0.2
— schedule
— pricing
identification
o.o4 greetings
ending

Fig. 4. Topic score (Y-axis) vs utterance index (X-axis) of a
representative call with 67 utterances. All topic scores of all
utterances are shown. Each topic score is presented by a solid
line with a different color. The final retrieved segments are
marked over the relevant utterances as a colored background
by the corresponding topic color.

ding. Given a call with N embedded utterances (w,..., wa),
a segment V = (wo, ..., We) where b, e are the beginning and
ending indicef']of V(0<b+1<e< N),anda split index
b <t<e, the gain of splitting V at position ¢ is defined as:

t-1
eM
i=b

The greedy approach calculates the index that maximizes
this term

e

» Wi

ist

e

» Wi

i=b

+

g(t) =

t* := arg max g; (t) (1)
t

Then, if gf(t*) > 7 where 7 > 0 is a pre-defined threshold,
the segment V is splited into two segments by around t*.

The method begins by initiating b and ¢ as the first and last
utterances of the conversation respectively. The process pro-
ceeds recursively to all segments and stops when there is no
candidate split whose gain is above the pre-defined threshold,
or the current number of segments has reached the maximum
defined by the user. Finally, the last segmentation is retrieved
as an output.

Given a predicted segmentation, two models are em-
ployed to predict a relevant topic, a zero-shot model intro-
duced in and a summarization model based on PEGA-
SUS {4]. If the first model fails to predict a topic with high
confidence, the second model is used.

The second model is based on the PEGASUS model,
which was fine-tuned on sales calls where the labels were
summaries generated by GPT [11].

https://github.com/chschock/textsplit
4The minimal size of a segment is 3

--- Page 6 ---
Sports IT Diverse
Pl WinDiff Pl WinDiff Pl WinDiff
TextTiling 0.66+0.07 0.89+0.12 0.65+40.08 0.93+0.12 0.66+0.10 0.92+0.12
BERT-TSeg 0.36+ 0.08 0.36+0.08 0.344010 0.35+0.10 0.34+0.10 0.35 +0.10
GSGST 0.33+0.09 0.34+0.08 0.314012 032+0.11 0.324011 0.334011
GPT-Calls 0.29 + 0.13 0.31 40.13 0.29+0.10 0.30+0.10 0.29+0.14 0.31+0.11

Table 1. Pk and WinDiff scores for each model and dataset, reported as the mean values with standard deviations (Mean +

SD).

The method assumes that each topic is associated with
a small set of representative sentences, created by an expert
(typically around 2-20 sentences). For example, for the topic
“pricing” the corresponding set contains 9 sentences, two of
them are: “the agent and customer discussed the price of
the product”, “the customer asked for a better price”, and so
on. These sentences were separately embedded by an SBERT
model and were mean pooled to extract a single anchor rep-
resenting the topic.

This model produces a single-sentence summary for ev-
ery segment. The summarized segment is then embedded
using SBERT and is compared to the single anchor of each
topic. The predicted topic is the one that maximizes the cosine
similarity with the summarized sentence embedding. Finally,
post-processing is performed to filter out extremely short seg-
ments, merge adjacent segments with identical topics, and so
on.

4.1. Metrics

To evaluate the performance of the models, we calculate two
metrics, Pk score and WindowDiff [13].

In order to measure the performance on each topic sepa-
rately, the metrics are computed over the binary segmentation
obtained by the predicted and ground truth segmentation of
each of the topics. For example, when evaluating the topic
“pricing”, all the segments that are not associated with pricing
were considered as background. The Pk score is computed us-
ing a sliding window-based method, where the window size
was set to half of the average true segment number. The met-
ric determines whether the two ends of the window were in
the same or different segments in the ground truth segmenta-
tion, and increased a counter if there was a mismatch. The
final score is calculated by scaling the penalty between 0 and
1 and dividing the number of measurements.

To overcome the challenges of the Pk score (which penal-
izes false negatives more than false positives and does not take
into account the number of boundaries), we also calculated
the WindowDiff metric. Similar to the Pk score, the Win-
dowDiff is also calculated using a sliding window. For each
position of the window of size k, the number of boundaries in
the ground truth segmentation is compared to the number of
boundaries predicted by the model.

4.2. Test data

We employed a human expert with relevant domain expertise
to annotate three test sets of real conversations taken from
various Dynamics 365 sales tenants} The human expert seg-
mented each conversation and annotate each segment with
one of the following topics: greetings, closing, pricing, iden-
tification, and scheduling. The annotations were used as the
ground truth segmentation for evaluating the performance of
the models.

The first set, referred to as the “Sports” set, contains
~ 200 sports-related conversation, where various sellers
from sport companies discuss with their clients about sub-
scription renewal, tickets ordering, seats changes and so
on. The second set, named the “JT” set, comprises ~ 100
recorded conversations of IT sellers reaching out to customers
to propose software services, negotiate contracts, and address
customer inquiries within the IT domain. Lastly, the third
set, labeled as the “Diverse” set, consists of ~ 200 conver-
sations of sale agents from various fields, including finance,
technology, billing companies, medical marketing and more.

4.3. Results

Table [T] presents the segmentation performance of all base-
lines across the three datasets mentioned earlier, evaluated us-
ing the ground truth human annotations. In this evaluation, we
focus solely on measuring the accuracy of the segmentation
and do not consider the topics associated with the segments.
As can be seen in the table, GPT-Calls outperforms all other
alternatives by a sizeable margin. Specifically, compared to
the second-best performing method, GSGST, we observe a
relative improvement of ~ 12%, ~ 8%, in the Pk and Win-
Diff scores, respectively, for the Sports dataset. Furthermore,
the proposed method shows even larger gains compared to the
remaining baselines and across all three datasets.

In Table [2] we present our quantitative evaluations for all
five topics: Identification, Pricing, Schedule, Greeting, and
Closing. For these evaluations, we utilize the annotated test
set and compare the performance of the GSGST model with

with the tenant’s consent and after all personally identifiable information
was removed

--- Page 7 ---
Identification Pricing Schedule Greetings Closing
GSGST 0.56/0.18 0.49/0.26 0.36/0.21 0.08/0.04 0.07/0.05
GPT-Calls 0.11/0.10 0.32/0.25 0.20/0.15 0.07/0.04 0.07/0.03
Improv. +80.4%/+44.4%  +34.6%/+3.8%  +44.4%/+28.5%  +12.5%/+0%  +0%/+40.0%

Table 2. A comparison of the performance of the proposed method and the GSGST baseline, evaluated on the test set, for each
topic separately. Pk score/WinDiff are reported. Lower is better for both.

Model Name Hit
GPT-Calls 711%

Failure

6.7%

Reasonable

16.2%

Table 3. Human evaluation for the quality of segments. We
measure the percentage of segments that (1) matches with the
ground truth annotations (Hit), (2) mismatch with the ground
truth but are considered as reasonable predictions (Reason-
able), (3) mismatch with the ground truth and are not reason-
able w.r.t. the underlying utterances in the segment (Failure).

our proposed model in terms of Pk score and WinDiff, sepa-
rately for each topic. Both metrics aim to achieve lower val-
ues, indicating better performance.

The results demonstrate that our proposed model yields
similar or better performance compared to the baseline model,
across all topics. Specifically, for the Identification, Pric-
ing, Schedule topics, we observe improvements ranging from
34.6% to 80.4% and 3.8% to 44.4% in in the Pk and Win-
Diff scores, respectively. The largest improvement was ob-
served in the Identification topic, where our model achieved
an 80.4% improvement in the Pk score and a 44.4% improve-
ment in WinDiff.

4.4, Human evaluations results

We randomly selected 100 calls from the above three datasets,
and evaluated the performance of the proposed model in an
end-to-end manner, while focusing on the end-user experi-
ence. The model performance was evaluated using three cri-
teria: (1) “Hit” was assigned when the predicted segment is
well correlated with the ground truth segment. (2) “Reason-
able” was designated when there was a discrepancy between
the predicted segment and the ground truth, but the predicted
segment and its topic are fairly associated with the underly-
ing utterances. (3) “Failure” was determined when when the
predicted segment did not match the ground truth, and the
prediction did not align well with the underlying utterances.

The results, depicted in Table [3] indicate that 93.3% of
the model predicted segmentations are considered fairly good
(i.e. either “Hit” or “Reasonable”), and only 6.7% were de-
tected as failures.

5. CONCLUSION

We propose a novel approach for call segmentation and tag-
ging that builds upon distilling knowledge from the GPT-3
model and does not require labeled data. Our solution is
generic and can be applied to various domains. The proposed
method is deployed in Dynamics 365 Sales Conversation In-
telligence and was shown to significantly improve upon other
alternatives.

6. REFERENCES

[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al., “Language models are few-shot learners,”
Advances in neural information processing systems, vol.
33, pp. 1877-1901, 2020.

[2

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.,
“Training language models to follow instructions with
human feedback,” arXiv preprint arXiv:2203.02155,
2022.

3] Alexander A Alemi and Paul Ginsparg, “Text segmen-
tation based on semantic word embeddings,’ arXiv
preprint arXiv: 1503.05543, 2015.

4] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter J Liu, “Pegasus: Pre-training with extracted gap-
sentences for abstractive summarization. arxiv e-prints,”
2019.

5] Marti A Hearst, “Text tiling: Segmenting text into multi-
paragraph subtopic passages,” Computational linguis-
tics, vol. 23, no. 1, pp. 33-64, 1997.

6] Alessandro Solbiati, Kevin Heffernan, Georgios
Damaskinos, Shivani Poddar, Shubham Modi, and
Jacques Cali, “Unsupervised topic segmentation
of meetings with bert embeddings,’ arXiv preprint
arXiv:2106.12978, 2021.

[7

Nils Reimers and Iryna Gurevych, “Sentence-bert: Sen-
tence embeddings using siamese bert-networks,” arXiv
preprint arXiv: 1908. 10084, 2019.

--- Page 8 ---
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova, “Bert: Pre-training of deep bidirec-
tional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[9] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei
Xu, et al., “A density-based algorithm for discovering
clusters in large spatial databases with noise.,” in kdd,
1996, vol. 96, pp. 226-231.

[10] Wenpeng Yin, Jamaal Hay, and Dan Roth, “Bench-
marking zero-shot text classification: Datasets, eval-
uation and entailment approach,’ = arXiv _ preprint
arXiv:1909.00161, 2019.

[11] Abedelkadir Asi, Song Wang, Roy Eisenstadt, Dean
Geckt, Yarin Kuper, Yi Mao, and Royi Ronen, “An end-
to-end dialogue summarization system for sales calls,”
arXiv preprint arXiv:2204.12951, 2022.

[12] Doug Beeferman, Adam Berger, and John D. Lafferty,
“Statistical models for text segmentation,’ Machine
learning, vol. 34, no. 1-3, pp. 177-210, 1999.

[13] Lev Pevzner and Marti A Hearst, “A critique and im-
provement of an evaluation metric for text segmenta-
tion,” Computational Linguistics, vol. 28, no. 1, pp.
19-36, 2002.

A. APPENDIX

A single prompt was created for each topic and was used
thousands times to generate a distribution of synthetic seg-
ments associated with the topic. The prompts contained one
to four shots, enabling the model to focus on the task and
generate high-quality synthetic segments. For instance, the
pricing topic prompt is as follows: “This is a prefix of a call
between two people, where they greet and introduce each
other: “Thank you for calling Spencer and Bryce. This is
Tracy. How can I help you? Hey Tracy, I’m Jeremy King
from sales looking to reach Paul Lana. Uh, you know what?
Give me your name again. Jeremy king. Calling regarding
what Jeremy? I’m a salesperson working for” Here is part
of the middle of a different phone call between two different
persons from different companies, where they are discussing
the pricing of a product:”

