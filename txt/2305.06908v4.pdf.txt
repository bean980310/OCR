arXiv:2305.06908v4 [cs.SD] 29 Oct 2023
CoMoSpeech: One-Step Speech and Singing Voice
Synthesis via Consistency Model
Zhen Ye¹, Wei Xue¹+, Xu Tan², Jie Chen³, Qifeng Liu,¹, Yike Guo¹+
1 Hong Kong University of Science and Technology 2 Microsoft Research Asia
3 Hong Kong Baptist University
4
Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences
*
Abstract
Denoising diffusion probabilistic models (DDPMs) have shown promising perfor-
mance for speech synthesis. However, a large number of iterative steps are required
to achieve high sample quality, which restricts the inference speed. Maintaining
sample quality while increasing sampling speed has become a challenging task.
In this paper, we propose a Consistency Model-based Speech synthesis method,
CoMoSpeech, which achieve speech synthesis through a single diffusion sampling
step while achieving high audio quality. The consistency constraint is applied to
distill a consistency model from a well-designed diffusion-based teacher model,
which ultimately yields superior performances in the distilled CoMoSpeech. Our
experiments show that by generating audio recordings by a single sampling step,
the CoMoSpeech achieves an inference speed more than 150 times faster than
real-time on a single NVIDIA A100 GPU, which is comparable to FastSpeech2,
making diffusion-sampling based speech synthesis truly practical. Meanwhile,
objective and subjective evaluations on text-to-speech and singing voice synthesis
show that the proposed teacher models yield the best audio quality, and the one-step
sampling-based CoMoSpeech achieves the best inference speed with better or com-
parable audio quality to other conventional multi-step diffusion model baselines.
Audio samples and codes are available at https://comospeech.github.io/.
Keywords: Text-to-speech, Singing Voice Synthesis, Diffusion Model, Consistency Model
1
Introduction
Speech synthesis Tan et al. [2021] aims to produce realistic audio of humans and has broadly included
text-to-speech (TTS) Taylor [2009], Shen et al. [2023] and singing voice synthesis (SVS) Nishimura
et al. [2016] tasks due to the increasing applications in human-machine interaction and entertainment.
The mainstream of speech synthesis has been dominated by the deep neural network (DNN)-based
methods Wang et al. [2017] Kim et al. [2021], and typically a two-stage pipeline is adopted Ren et al.
[2019] Lu et al. [2020], in which the acoustic model first converts the textual and other controlling
information into acoustic features (e.g., mel-spectrogram) and then the vocoder further transforms the
acoustic features into audible waveforms. The two-stage pipeline has achieved substantial success
since the acoustic features, which are expressed by frames, effectively act as the "relay" to alleviate
the one-to-many mapping problems (ill-posed or ill-condition problem) Bertero et al. [1988] of
converting short texts to long audios with a high sampling frequency.
The quality of the acoustic feature produced by the acoustic model, typically mel-spectrogram,
crucially affects the quality of the synthesized speeches. Approaches widely used in the industry, such
**Corresponding authors: Wei Xue {weixue@ust.hk}, Yike Guo {yikeguo@ust.hk}
Accepted to ACM MM 2023.
5.00
4.75
Better
4.50
4.25
CoMoSpeech
4.00
FastSpeech2
DiffGAN-TTS
MOS
(Audio
Quality) 3.75
3.50
☐ ● ProDiff
3.25
Our teacher
Grad-TTS
DiffSpeech
3.00
0
10
20
30
40
50
60
70
NFE (Inference Speed)
Faster
Figure 1: The audio quality and inference speed comparisons of different TTS systems. Details are
shown in Table 1, and similar results are obtained for SVS.
as Tacotron Wang et al. [2017], DurIAN Yu et al. [2020], and FastSpeech Ren et al. [2019], generally
adopt the convolutional neural network (CNN) and Transformers to predict the mel-spectrogram from
the controlling factor. Diffusion model methods have attracted much attention because their potential
to produce high-quality samples is well recognized.
A diffusion model Ho et al. [2020], also named score-based model Song et al. [2021b], is based on
two processes, a diffusion process that gradually perturbs data to noise and a reverse process that
progressively converts noise back to data. A critical drawback Song et al. [2021a] Yang et al. [2022]
of the diffusion model is that it requires many iterations for the generation. Several methods based on
the diffusion model have been proposed for acoustic modeling in speech synthesis. Most of these
works still have the issue of slow generation speed.
~
Grad-TTS Popov et al. [2021] apply the diffusion model for acoustic modeling, which formulates
a stochastic differential equation (SDE) Anderson [1982] to gradually transform the noise to the
mel-spectrogram and a numerical ODE solver is used for solving reverse SDE Song et al. [2021b].
Although yielding high audio quality, the inference speed is low due to the large number of iterations
(10 1000 steps) in the reverse process. Prodiff Huang et al. [2022] was further developed to use
progressive distillation Salimans and Ho [2022] to reduce the sampling steps. In Liu et al. [2022b],
DiffGAN-TTS adopted an adversarially-trained model to approximate the denoising function for
efficient speech synthesis. In Chen et al. [2022], the ResGrad uses the diffusion model to estimate
the prediction residual between pre-trained FastSpeech2 Ren et al. [2020] and ground truth. Apart
from normal speaking voice, recent studies also focus on voice with more complex variations in pitch,
timing, and expression. For example, Diffsinger Liu et al. [2022a] also shows that a well-designed
diffusion model can achieve high quality on synthesized singing voice through one hundred steps of
iteration.
From the above discussion, the objectives of speech synthesis are three-fold:
•
High audio quality: The generative model should accurately express the nuances of speaking
voice which contribute to the naturalness and expressiveness of the synthesized audio.
Additionally, artefacts and distortions in the generated audio should also be avoided.
• Fast inference speed: Real-time applications, including communication, interactive speech
and music systems, require the fast generation speed of audio. When considering making
time for other algorithms in an integrated system, simply being faster than real-time is
insufficient for speech synthesis.
•
Beyond speech: Instead of the normal speaking voice, more complex modeling of voice on
pitch, expression, rhythm, breach control and timbre is required such as singing voice.
Although many efforts have been made, due to the mechanism of the denoising diffusion process when
performing sampling, the trade-off problem among the synthesized audio quality, model capability
2
and inference speed still exists in TTS and is particularly pronounced in SVS. Existing methods
generally seek to alleviate the slow inference problem rather than solve it fundamentally, and their
speed is still not comparable to conventional methods without relying on diffusion models such as
FastSpeech2 Ren et al. [2020]. Recently, by expressing the stochastic differential equation (SDE)
describing the sampling process as an ordinary differential equation (ODE), and further enforcing
the consistency constraint of the ODE trajectory, the consistency model Song et al. [2023] has been
developed, yielding high-quality images with only one sampling step. However, despite such success
in image synthesis, no speech synthesis model based on the consistency model is known so far. This
indicates the potential of designing a consistency model based speech synthesis method to achieve
both high-quality synthesis and fast inference speed.
In this paper, we propose Consistency Model based method for speech synthesis, namely CoMo-
Speech, which achieves fast and high-quality audio generation. Our CoMoSpeech is distilled from
a pre-trained teacher model. More specifically, our teacher model leverages the SDE to smoothly
transform the mel-spectrogram into the Gaussian noise distribution and learn the corresponding score
function. After training, we utilize the corresponding numerical ODE solvers to construct the teacher
denoiser function, which is used for further consistency distillation. Through consistency distillation,
our CoMoSpeech is obtained. Ultimately, high-quality audio can be produced by our CoMoSpeech
with a single-step sampling.
We conducted experiments for both TTS and SVS, and the results show that the CoMoSpeech can
generate speeches with one sampling step, more than 150 times faster than in real-time. The audio
quality evaluation also shows that the CoMoSpeech achieves better or comparable audio quality to
other diffusion model methods involving tens to hundreds of iterations (visualized in Figure 1). This
makes the speech synthesis based on the diffusion model truly practical for the first time.
2
Background of Consistency Model
Now we briefly introduce the consistency model. Supposing that we have a data distribution as
Pdata (x). The diffusion model progressively adds the Gaussian noise to diffuse data and then adopts a
reverse denoising process to generate samples from noise. For noisy data {x}\/0 in the diffusion
process where po(x) = Pdata (x), PT (x) infinitely close a Gaussian distribution, and T is the time
constant, the forward diffusion process can be expressed by a SDE Song et al. [2021b] as
dx = f(x, t)dt + g(t)dw,
t=0
(1)
where w is the standard wiener process, f(·, ·) and g(.) are drift and diffusion coefficients, respectively.
f(x, t) acts as f(x,t) = f(t)x in previous work (VP, VE, EDM) Song et al. [2021b], Karras et al.
[2022], thus
dx = f(t)xdt g(t)dw.
A notable property of the above SDE is that it corresponds to a probability flow ODE which indicates
the sampling trajectory distribution of SDE at time t Song et al. [2021b], Karras et al. [2022], as
dx
1
= [ƒ(t)x — —±9(t)² V log p:(x)] dt,
(3)
where Vlog Pt(x) is the score function of pt(x) Hyvärinen and Dayan [2005]. The probability flow
ODE eliminates the stochastic w, thus generating a deterministic sampling trajectory.
Vlog Pt (x)
=
(D(x, t) — x+)/0,
As long as the score function V log pt(x) is known, the probability flow ODE in (3) can be used
for sampling. Supposing D(x, t) is the “denoiser” which denoise the sample x+ at step t, the score
function can be obtained by minimizing the denoising error ||D(x, t) – x||2 Karras et al. [2022],
yielding:
(4)
where σ = g(t)²dt. Further, the probability flow ODE based sampling can be performed by first
sampling from a noise distribution and then denoising to the true sample by the numerical ODE
solver such as Euler and Heun solvers Song et al. [2021b] Karras et al. [2022]. However, the ODE
solvers still involve many iterations causing a slow sampling.
-
To accelerate sampling Song et al. [2023] or minimize the sampling drift Daras et al. [2023],
consistency property has been proposed for diffusion model to impose both:
D(x,t) = D(x+', t')
(5)
3
Text
Duration
Predictor
[2, 1, 3, 2]
Optional inputs: Music Score
Encoder
gtmel X
Length
Regulator
ODE Solver
...
tN-1
Consistency
distillation
Denoiser
D(XN, TN, cond)
tN
One-step
synthesis
priormel μ
Conditional input
noise mel XN~N(μ, I)
Teacher
CoMoSpeech
Figure 2: An illustration of CoMoSpeech. Our CoMoSpeech distills the multi-step sampling of the
teacher model into one step utilizing the consistency constraint.
for any t and t', and
D(X0, 0) = x0.
(6)
In this way, a consistency model can be obtained, and one-step sampling D(XT, T) can be achieved
since all points on a sampling trajectory of probability flow ODE is directly linked to the trajectory's
origin. po(x). The consistency model can be trained either in isolation or by distilling from a pre-
trained diffusion-based teacher model, and the later approach generally produces better performances.
Detailed discussions can be referred to Song et al. [2023]. In our work, a distillation-based consistency
model for speech synthesis, called CoMoSpeech, is proposed below.
3 CoMoSpeech
This section presents the proposed CoMoSpeech, a one-step speech synthesis model. The framework
of the proposed method is shown in Figure. 2, which has two main stages. The first stage trains
a diffusion-based teacher model to produce audios conditioned on the textual (for TTS and SVS)
and musical score inputs (for SVS). Then in the second stage, by forcing the consistency property,
we obtain the CoMoSpeech from the distillation of the teacher model to finally achieve a one-step
inference given the conditional inputs. How to design the teacher model, perform consistency
distillation and implement the training and inference will be discussed.
3.1 Teacher Model
As a blossoming class of generative models, many speech synthesis systems apply diffusion models
and generate high-quality audio. However, specific criteria must be met to be the teacher model. First,
the model needs to meet the theoretical requirement. As mentioned in Section 2, we aim to adopt
the denoiser to implement the one-step generation, which means this function should point to the
clean data instead of noise. In other words, we follow the term in Huang et al. [2022] that our teacher
model should be a generator-based rather than a gradient-based method. This restriction requires us
to modify the state-of-art model Grad-TTS Popov et al. [2021] to be our teacher model. We inherited
the setting of training and the main architectures. In addition, we also adopt the EDMKarras et al.
[2022] as our design choice for the diffusion model to ensure further consistency distillation Song
et al. [2023].
Specifically, we set mel-spectrogram as x in (2) with the schedule σ (t) and scaling coefficients in
EDM Karras et al. [2022] as t and 1, respectively. Combined with (4), our ODE can be formulated as
dxt = [(xt — Do(xt, t, cond))/t]dt,
-
(7)
4
where cond is the conditional input that will be introduced in the following section, and
De(xt, t, cond) is designed to precondition the neural network with a t-dependent skip connec-
tion as
De(xt, t, cond) = Cskip (t)x+ + Cout (t) F (Xt, t, cond)
(8)
where Fo is the network to be trained whose architecture can be flexibly chosen. For instance, the
architectures of WaveNet Liu et al. [2022a] Oord et al. [2016] or U-Net Popov et al. [2021] Ronneberger
et al. [2015] can be selected to construct Fe. The Cskip (t) and Cout (t) are used to modulate the skip
connection and scale the magnitudes of Fo, which can be given by Song et al. [2023]
Cskip (t)
data
=
2 "
data
Cout (t)
=
σdata (t - €)
√√o data + t²'
(9)
where σdata
= 0.5 is used to balance the ratio between Cskip and Cout and € = 0.002 as the smallest
time instant during sampling. The first reason for choosing the above formulation is it can meet
(6) since Cskip (e) = 1 and Cout (€) = 0. The second reason is that both scaling factors can help
the predicted results of Fo scale to the unit variance, which avoids the large variation in gradient
magnitudes at different noise levels.
To train the De, the loss function can be formulated as
―
Со ||De (xt, t, cond) – xo||²,
(10)
which is a weighted L2 loss between the predicted mel-spectrogram predmel and ground truth
mel-spectrogram gtmel, and we also re-weight the loss function for different t as the same as EDM
Karras et al. [2022].
Finally, the teacher model can be trained, and the synthesized mel-spectrogram can be sampled by
Algorithm 1. During the inference on the teacher model, we first sample x from N(µ, I), and then
iterative the numerical ODE solver for N Euler steps.
Algorithm 1 Sampling procedure of the proposed teacher model
Input: The denoiser function Do; the prior mel-spectrogram μ; a set of time points tiε {0,...,N}
1: Sample XN ~ N (µ‚ I)
2: XN = NXN
3: for i =
4:
di
5:
N to 1 do
(xi Do(xi, ti, µ))/ti
Xi−1 ←xi + (ti — ti−1)di
6: end for
7: xx0
Output: x
Algorithm 2 Sampling procedure of the proposed method
Input: The denoiser function Do; the prior mel-spectrogram μ; a set of time points tie {0,...,N}
1: Sample XN
~
N(μ, I)
2: XN = tNXN
3: xD(XN, TN, μ)
4: if one-step synthesis
5:
Output: x
6: else multi-step synthesis
for i = N 1 to 1 do
_
Sample z ~ ~ N(µ, I)
7:
8:
9:
10:
11:
end for
x← Do(xi, ti, μ)
Output: x
5
(XN, TN)
noise mel
(Xi+1, ti+1)
ODE trajectory
Consistency
constraint
(xi, ti)
(xo, to)
gt mel
Figure 3: An illustration of consistency property. A function with consistency property maps any
points on the ODE trajectory to the original data.
3.2 Consistency Distillation
A one-step diffusion sampling-based model is further trained from the teacher model based on
consistency distillation, resulting in the proposed CoMoSpeech. Now we re-examine the constraints
defined in (5) and (6). We note that given the choice of Cskip (t) and Cout (t) in (9), the denoiser De
in the proposed teacher model already satisfies (6), therefore, the remaining training objective is to
fulfill the property in (5).
Inspired by Song et al. [2023], we utilize the momentum-based distillation to train the proposed
CoMoSpeech. The consistency distillation loss is defined as
Lo = ||Do(xi+1, ti+1, cond) — D₁- (x², ti, cond)||²,
(11)
where and are initialized weights of CoMoSpeech inherited from the teacher model, is the
fixed ODE solver from the teacher model in section 3.1, and i is a step-index uniformly sampled from
the total ODE steps from N to 1. x is estimated from x;+1 and the ODE solver . During training,
the weight directly optimize by the loss function, and 0¯ is recursively updated by
stopgrad(að¯ + (1 − a)0),
-
where a is a momentum coefficient empirically set as 0.95.
(12)
After distillation, the consistency property can be exploited so that the original data point x can
be transformed from any point xt on the ODE trajectory as shown in Figure 3. Therefore, we can
directly generate the target sample from the distribution XN at the step tn, as:
melpred = D(XN, TN, cond).
(13)
Therefore, one-step mel-spectrogram generation can be achieved. In addition, multi-step synthesis
can be conducted by Algorithm 2 as a trade-off between audio quality and sampling speed, similar to
other stochastic samplers.
3.3 Conditional Input
A remaining problem in the framework shown in Figure 2 is how to obtain the conditional input cond,
which will be used throughout the algorithm design. A well-designed speech synthesizer is expected
to perform well not only on reading speech synthesis (TTS) but also on other more complicated tasks,
such as SVS which additionally produces highly dynamic melodies. In producing the conditional
inputs, both TTS and SVS tasks are considered to examine the proposed framework's effectiveness
comprehensively.
Concretely, we adopt the phoneme as the basic input for TTS and SVS. Then, a simple lookup table
is used for embedding the phoneme feature. Additionally, for the SVS task, we add a music score
that specifies the note levels time-aligned to the phonemes. For note feature extraction, we use the
embedding method for both categorical feature note pitch and slur indicator and rely on a linear layer
for continuous feature note duration.
Summing all the feature sequences together, we utilize the encoder structure and variance adaptor
in FastSpeech Ren et al. [2019]. Specifically, N feed-forward transformer blocks (FFT blocks) are
stacked to extract the phoneme hidden sequence. A duration predictor is used to estimate the duration
of each phoneme dpred, and the corresponding loss function is expressed as
Lduration = || log(dpred) - log(dgt)||2
where dgt indicates the ground-truth phoneme duration.
(14)
Further, the length regulator projects the phoneme hidden sequence into the hidden sequence in
the mel-spectrogram domain, with the phoneme duration denoted as hidden mel. Then, the prior
mel-spectrogram μ is predicted using the hidden mel with prior loss function as
Lprior ||μgtmel||2.
=
(15)
We follow the encoder part and the prior mel-spectrogram setting in Grad-TTS Popov et al. [2021]. As
shown in the bottom left part of Figure 2, since the expanded features belonging to the same phoneme
in hidden mel are repeated, the predicted prior mel can only roughly approximate the time-frequency
structure of gtmel based on the phoneme sequence. The details of the mel-spectrogram are modeled
by the diffusion model
For the neural network and conditional inputs in the denoiser, we investigated different combinations
and finally followed the same setting in the previous work for a fair comparison. a) Diffsinger:
the WaveNet architectureOord et al. [2016] and hiddenmel as feature cond in (13) for SVS and b)
Grad-TTS: U-Net architecture Ronneberger et al. [2015] and μmel as cond for TTS.
3.4 Training procedure
The whole process can be summarised as two stages which are the training of the teacher model and
the consistency distillation.
As for the training of the teacher model, the loss term can consist of three parts which are duration
loss (in (14)), prior loss (in (15)), and denoising loss (in (10)). These three losses are summed up
together without any extra weight. The objective of this stage is to build a speech synthesis system
that can generate high-quality audios with multi-step synthesis and have the potential for further
consistency distillation.
The second stage is consistency distillation. There is only one loss function as defined by (11), which
helps the model learn the consistency property. The parameters are initialized from the teacher model.
During training, the parameters of the encoder are fixed, which means only the weight in the denoiser
is updated. After distillation, high-quality recordings with one-step synthesis (13) can be achieved.
4 Experiments
To evaluate the performance of the proposed CoMoSpeech, we conduct experiments on both TTS
and SVS.
4.1 Experimental Setup
4.1.1 Data and Preprocessing
We adopt the public LJSpeech Ito and Johnson [2017] as the TTS dataset, which includes around 24
hours of English female voice recordings sampled at 22.05 kHz. Similar to Ren et al. [2019] Chen
et al. [2022], we split the dataset into three sets: 12, 228 samples for training, 349 samples (with
7
METHOD
NFE
RTF (↓)
FD (↓)
MOS (1)
GT
4.778
GT(Mel+HiFi-GAN)
0.282
4.590
FastSpeech 2 Ren et al. [2020]
1
0.0017
10.48
4.034
DiffGAN-TTS Liu et al. [2022b]
4
0.0084
8.310
3.889
ProDiff Huang et al. [2022]
4
0.0097
3.503
3.374
DiffSpeech Liu et al. [2022a]
71
0.1030
2.349
4.103
Grad-TTS Popov et al. [2021]
50
0.1694
1.882
4.487
Teacher
50
0.1824
0.748
4.538
CoMoSpeech
1
0.0058
0.774
4.239
Table 1: Evaluation results on LJSpeech for TTS.
METHOD
NFE
RTF (↓)
FD (↓)
MOS (†)
GT
4.675
GT(Mel+HiFi-GAN)
0.882
4.588
FFTSinger Blaauw and Bonada [2020]
1
0.0032
7.867
2.769
HIFiSinger Chen et al. [2020]
1
0.0034
6.340
3.156
DiffSinger Liu et al. [2022a] A version
60
0.1338
3.466
3.506
DiffSinger Liu et al. [2022a] B version
100
0.2198
3.618
3.531
COMOSVS-teacher
50
0.1282
3.162
4.050
COMOSVS
1
0.0048
3.571
3.794
Table 2: Evaluation Results on Opencpop for SVS.
document title LJ003) for validation, and 523 samples (with document title LJ001 and LJ002) for
testing. Following the common practice in Ren et al. [2020]Huang et al. [2022] for TTS, we extract
the 80-bin mel-spectrogram with the frame size of 1024 and hop size of 256.
For the SVS task, we use the Opencpop dataset Wang et al. [2022] containing 100 Chinese pop songs
which are split into 3, 756 utterances with a total duration of around 5.2 hours. All recordings are
from a single female singer and labeled with aligned phoneme and MIDI-pitch sequences. We follow
the official train/test split Wang et al. [2022], i.e., 95 songs and 5 songs for training and evaluation,
respectively. Same as the setting in Chen et al. [2020]Liu et al. [2022a] for SVS, the recordings are
resampled at 24kHz rates with 16-bit precision, and the 80-bin mel-spectrogram is extracted with a
frame size of 512 and hop size of 128.
4.1.2 Implementation Details
For TTS, for a fair comparison, the encoder and duration predictor are exactly the same as those in
Grad-TTS Popov et al. [2021]. The encoder contains 6 feed-forward transformer (FFT) blocks Ren
et al. [2019], and The hidden channel is set to 192. The duration predictor uses two convolutional
layers for prediction. Both the teacher model and CoMoSpeech are trained for 1.7 million iterations
on a single NVIDIA A100 GPU with a batch size of 16. The Adam optimizer Kingma and Ba [2015]
is adopted with the learning rate 1e-4.
For SVS, we adopt almost the same architecture as in TTS with different hyperparameters. The
encoder adopts 4 FFT blocks, and we set the hidden channel to 256 in the encoder. The duration
predictor consists of 5 convolutional layers to estimate the duration. The teacher model of SVS
and CoMoSpeech are trained on a single GPU for 250k steps with the AdamW Loshchilov and
Hutter [2017] optimizer. The initial learning rate is 1e-3, and the exponential decay strategy with a
decreasing factor of 0.5 every 50k steps is adopted.
4.1.3
Evaluation Metrics
we conduct both objective and subjective evaluations to measure the sample quality (MOS & FD)
and the model inference speed (RTF & NFE):
8
• MOS (mean opinion score) Chu and Peng [2006] is used to measure the perceived quality
of the synthesized audio, which is obtained by presenting 10 listeners with the test set and
asking them to rate the quality of the synthesized audio on a scale of 1 to 5.
• FD (frechet distance)² is similar to the frechet inception distance Heusel et al. [2017] in
image generation. We use frechet distance Liu et al. [2023] in audio to measure the similarity
between generated samples and target samples utilizing the large-scale pretrained audio
neural networks PANNS Kong et al. [2020b].
• RTF (real-time factor) determines how quickly the system can synthesize audio in real-
time applications. It is defined as the ratio between the total time a speech system takes
to synthesize a given amount of audio and the duration of that audio. In addition, all
experiments for RTF are implemented on a single NVIDIA A100 GPU.
• NFE (number of function evaluations) measures the computational cost, which refers to the
total number of times the denoiser function is evaluated during the generation process.
4.2 Performances on Text-to-Speech
We compare the above four metrics of the samples generated by the teacher model and CoMoSpeech
with the following systems:
• GT, the ground truth recordings.
• GT (Mel+HiFi-GAN), using ground-truth mel-spectrogram to synthesize waveform with
HiFi-GAN vocoder Kong et al. [2020a].
•
FastSpeech 2 Ren et al. [2020], synthesizing high-quality speech at a fast speed with FFT
blocks and variance adaptor.
• DiffGAN-TTS Liu et al. [2022b] ³, applying an adversarially-trained model to approximate
the denoising function for efficient speech synthesis.
• ProDiff Huang et al. [2022] 4, directly adopting progressive distillation Salimans and Ho
[2022] to TTS for fast generation speed.
.
DiffSpeech Liu et al. [2022a] 5, using an auxiliary acoustic model to generate mel-
spectrogram and injects K steps noise to a noisy mel-spectrogram. Then, the mel-
spectrogram is generated from the noisy mel-spectrogram by DDPM iteratively.
• Grad-TTS Popov et al. [2021] 6, using stochastic differential equation modelling for the
mel-spectrogram and use corresponding ODE solver for audio generation.
The evaluation results of TTS are shown in Table 1. For audio quality, our teacher model achieved
the highest MOS and Grad-TTS ranked second because our teacher model is based on the design
of Grad-TTS, but we adopt better choices on drift and diffusion coefficients in SDE. The proposed
CoMoSpeech takes 3rd place among all methods, but it is substantially better than other fast-
inference methods ProDiff, DiffGAN-TTS and Fastspeech2. This demonstrates the effectiveness
of the consistency distillation and the effectiveness of teacher model selection. In addition, we also
observe that our teacher model and CoMoSpeech achieve the best frechet distance scores among
all methods, further demonstrating the proposed methods' superior performance on modeling data
distribution.
Regarding inference speed, while Fastspeech2 obviously achieves the best, our CoMoSpeech also
yields a very low RTF, and is faster than all other baselines. Compared with the diffusion-based
methods involving a large number of iterations including DiffSpeech, Grad-TTS and our teacher
model, our method achieves about 50 times faster with similar or even better audio quality. In
addition, our CoMoSpeech also achieves faster speed and better quality than methods for speeding
up diffusion sampling, i.e., DiffGAN-TTS and ProDiff.
2https://github.com/haoheliu/audioldm_eval
https://github.com/keonlee9420/DiffGAN-TTS
https://github.com/Rongjiehuang/ProDiff
Shttps://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-TTS.md
"https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS
9
(a) gtmel
(b) noise mel
(c) Denoiser De (XN, TN, cond) before consistency distillation
(d) Denoiser Do (XN, TN, cond) after consistency distillation
Figure 4: Effect of consistency distillation: Compared to the denoiser of teacher model before
consistency distillation, our CoMoSpeech can generate a high-quality mel-spectrogram instead of an
over-smoothed mel-spectrogram by calling the denoiser function only one time.
4.3 Performances on Singing Voice Synthesis
To further examine the modeling capability of our methods, we compare the proposed SVS-version
models, teacher-svs and CoMoSpeech-svs, with several baselines on SVS, and the baselines include:
•
•
.
•
GT, the ground truth recordings.
GT(Mel+HiFi-GAN), synthesizing song samples using HiFi-GAN Kong et al. [2020a]
vocoder with ground truth mel-spectrogram inputs.
FFTSinger Blaauw and Bonada [2020], adopting FFT blocks to predict mel-spectrogram,
and using the HiFi-GAN vocoder to synthesize audio;
HiFiSinger Chen et al. [2020], using a novel sub-frequency GAN (SF-GAN) to generate the
mel-spectrogram. Since our aim is to compare the acoustic model, we modify the original
vocoder to HiFi-GAN, being the same as other methods.
7
• DiffSinger Liu et al. [2022a], using DDPM to generate the mel-spectrogram from noisy
mel-spectrogram. There are two versions for generating noisy mel-spectrogram where
A version using a auxiliary acoustic model to generate mel-spectrogram and injecting
K steps noise to a noisy mel-spectrogram, and B version 8 directly generating the noisy
mel-spectrogram from the Gaussian noise.
The results of SVS are shown in Table 2. As for audio quality, it can be seen that our CoMoSpeech
and other diffusion model based methods can significantly surpass all non-iterative methods including
FFTSinger and HIFiSinger on frechet distance and mean opinion score. Among diffusion models,
our teacher model achieves the best performance, and our student model CoMoSpeech has a close
performance to it. For the inference speed, with one-step inference, the proposed CoMoSpeech could
maintain a speed similar to non-iterative methods and significantly outperform other diffusion model
based methods.
Version A: https://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-SVS-opencpop-
cascade.md
Version B: https://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-SVS-opencpop-
e2e.md
10
10
Frechet Distance (↓↓)
NFE
Teacher model
CoMoSpeech
1
7.526
0.774
4.558
0.762
4
2.477
0.784
10
1.197
0.725
50
0.748
0.850
Table 3: Comparison between CoMoSpeech and its teacher model with different sampling steps for
TTS.
Frechet Distance (↓)
NFE
Teacher model
CoMoSpeech
1
7.786
3.571
2
7.219
3.520
4
4.932
3.433
10
3.937
3.658
50
3.162
3.732
Table 4: Comparison between CoMoSpeech and its teacher model with different sampling steps for
SVS.
In addition, we also compare the results between speaking voice and singing voice synthesis. Based
on two methods DiffSinger and DiffSpeech, which are basically the same, we can observe that the
singing voice has a greater FD than the speaking voice, indicating that it is more difficult to model
the data. However, the proposed teacher model and CoMoSpeech still achieve the best performances
on audio quality and inference speed, respectively. This shows the capability of CoMoSpeech for
speech synthesis beyond speaking voices. In addition, we can observe that our CoMoSpeech-svs is
faster than CoMoSpeech because the denoiser function in SVS follows the WaveNet architecture
which is faster than U-Net architecture in TTS. This observation inspires us that if a more efficient
denoiser function that runs faster than the decoder in FastSpeech2 can be designed, we can make
CoMoSpeech even faster than non-iterative methods in future work.
4.4 Ablation Studies of Consistency Distillation
In this part, we will show the importance of consistency distillation. As shown in Figure 4, we
visualize the differences before and after consistency distillation in the results, in other words, the
teacher model and our CoMoSpeech. At to steps, the denoiser function before distillation points
to a smooth mel-spectrogram, indicating a great distance between ground truth mel-spectrogram.
However, we can observe that the results after distillation significantly improve the performances by
enriching many details, resulting in natural and expressive sounds.
In Table 3 and Table 4, we also conduct experiments using the frechet distance metric to further
demonstrate the effectiveness of consistency distillation. For teacher models for both TTS and
SVS tasks, the frechet distance decrease when the iteration steps increase. This trade-off property
between inference speed and sample quality has also been observed in other diffusion model methods.
Surprisingly, we can find that our CoMoSpeech can achieve nearly the best performance in one step,
and the best performance can be achieved at 4 and 10 steps on TTS and SVS, respectively. However,
the trade-off property seems to disappear after 10 steps. The issue that the model performance
improves in a few sampling steps and then declines slightly as the number of steps increases is called
the "sampling drift" challenge Daras et al. [2023] Ji et al. [2023] Chen et al. [2023] Saxena et al.
[2023]. We will leave the exploration to future work.
5 Conclusions and Future Work
In this paper, we propose CoMoSpeech, a one-step acoustic model for speech synthesis based on the
consistency model. With different conditional inputs, our CoMoSpeech can generate high-quality
speech or singing voice by transforming the noise mel-spectrogram into the predicted mel-spectrogram
in a single step.
11
However, there are still some limitations to our method. Since our CoMoSpeech needs to be distilled
from a teacher model for better performance, this makes the pipeline of constructing a speech
synthesis system more complicated. Therefore, how to directly train the CoMoSpeech without
distillation of the teacher model is our next step to investigate. In addition, we show the capability
of CoMoSpeech on the SVS task. Even though CoMoSpeech achieves the best result among all the
methods, it still has a significant gap between the ground truth recording.
Acknowledgments
The research was supported by the Theme-based Research Scheme (T45-205/21-N) and Early Career
Scheme (ECS-HKUST22201322), Research Grants Council of Hong Kong.
References
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313–326, 1982.
Mario Bertero, Tomaso A Poggio, and Vincent Torre. Ill-posed problems in early vision. Proceedings
of the IEEE, 76(8):869-889, 1988.
Merlijn Blaauw and Jordi Bonada. Sequence-to-sequence singing synthesis using the feed-forward
transformer. In Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2020.
Jiawei Chen, Xu Tan, Jian Luan, Tao Qin, and Tie-Yan Liu. Hifisinger: Towards high-fidelity neural
singing voice synthesis. arXiv preprint arXiv:2009.01776, 2020.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy
as learning the score: theory for diffusion models with minimal data assumptions. In Proc. Intl.
Conf. on Learning Representations (ICLR), 2023.
Zehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei
He, Sheng Zhao, et al. Resgrad: Residual denoising diffusion probabilistic models for text to
speech. arXiv preprint arXiv:2212.14518, 2022.
Min Chu and Hu Peng. Objective measure for estimating mean opinion score of synthesized speech,
April 4 2006. US Patent 7,024,362.
Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. Consis-
tent diffusion models: Mitigating sampling drift by learning to be consistent. arXiv preprint
arXiv:2302.09057, 2023.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Proc. Conf. on Neural
Information Processing Systems (NeurIPS), 2017.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc. Conf.
on Neural Information Processing Systems (NeurIPS), volume 33, pages 6840–6851, 2020.
Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. Prodiff: Progressive
fast diffusion model for high-quality text-to-speech. In Proc. ACM Int. Conf. on Multimedia (ACM
MM), pages 2595-2605, 2022.
Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research, 6(4), 2005.
Keith Ito and Linda Johnson. The lj speech dataset.
LJ-Speech-Dataset/, 2017.
https://keithito.com/
Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li,
and Ping Luo. DDP: Diffusion model for dense visual prediction. arXiv preprint arXiv:2303.17559,
2023.
12
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. In Proc. Conf. on Neural Information Processing Systems (NeurIPS),
2022.
Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial
learning for end-to-end text-to-speech. In Proc. Intl. Conf. Machine Learning (ICML), pages
5530-5540. PMLR, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Intl. Conf.
on Learning Representations (ICLR), 2015.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for
efficient and high fidelity speech synthesis. volume 33, pages 17022-17033, 2020a.
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. PANNS:
Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Trans.
Audio, Speech, Lang. Process., 28:2880–2894, 2020b.
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and
Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv
preprint arXiv:2301.12503, 2023.
Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis
via shallow diffusion mechanism. In Proc. AAAI Conf. on Artificial Intelligence, volume 36, pages
11020-11028, 2022a.
Songxiang Liu, Dan Su, and Dong Yu. Diffgan-tts: High-fidelity and efficient text-to-speech with
denoising diffusion gans. arXiv preprint arXiv:2201.11972, 2022b.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proc. Intl. Conf. on
Learning Representations (ICLR), 2017.
Peiling Lu, Jie Wu, Jian Luan, Xu Tan, and Li Zhou. Xiaoicesing: A high-quality and integrated
singing voice synthesis system. In Proc. InterSpeech, 2020.
Masanari Nishimura, Kei Hashimoto, Keiichiro Oura, Yoshihiko Nankaku, and Keiichi Tokuda.
Singing voice synthesis based on deep neural networks. In Interspeech, pages 2478–2482, 2016.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. In Proc. Intl. Speech Commun. Assoc. (ISCA) Workshop on Speech Synthesis, 2016.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS:
A diffusion probabilistic model for text-to-speech. In Proc. Intl. Conf. Machine Learning (ICML),
pages 8599-8608. PMLR, 2021.
Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast,
robust and controllable text to speech. In Proc. Conf. on Neural Information Processing Systems
(NeurIPS), volume 32, 2019.
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2:
Fast and high-quality end-to-end text to speech. In Proc. Intl. Conf. on Learning Representations
(ICLR), 2020.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In Proc. Conf. on Medical Image Computing and Computer Assisted
Intervention (MICCAI), pages 234–241, 2015.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In
Proc. Intl. Conf. on Learning Representations (ICLR), 2022.
Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation
using diffusion models. arXiv preprint arXiv:2302.14816, 2023.
13
Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang
Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing
synthesizers. arXiv preprint arXiv:2304.09116, 2023.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proc. Intl.
Conf. on Learning Representations (ICLR), 2021a.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Proc. Intl.
Conf. on Learning Representations (ICLR), 2021b.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469, 2023.
Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint
arXiv:2106.15561, 2021.
Paul Taylor. Text-to-speech synthesis. Cambridge university press, 2009.
Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei
Xie, and Mengxiao Bi. Opencpop: A high-quality open source chinese popular song corpus for
singing voice synthesis. In Proc. InterSpeech, 2022.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng
Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech
synthesis. In Proc. InterSpeech, 2017.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao,
Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of
methods and applications. arXiv preprint arXiv:2209.00796, 2022.
Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, Deyi Tuo, Shiyin Kang,
Guangzhi Lei, et al. Durian: Duration informed attention network for multimodal synthesis. In
Proc. InterSpeech, 2020.
14
