--- Page 1 ---
arXiv:2305.16367v1 [cs.CL] 25 May 2023

Role-Play with Large Language Models

Murray Shanahan *!?, Kyle McDonell **, and Laria Reynolds '

DeepMind
Imperial College London
3Eleuther AI

May 2023

Abstract

As dialogue agents become increasingly human-
like in their performance, it is imperative that
we develop effective ways to describe their be-
haviour in high-level terms without falling into
the trap of anthropomorphism. In this paper, we
foreground the concept of role-play. Casting dia-
logue agent behaviour in terms of role-play allows
us to draw on familiar folk psychological terms,
without ascribing human characteristics to lan-
guage models they in fact lack. Two important
cases of dialogue agent behaviour are addressed
this way, namely (apparent) deception and (ap-
parent) self-awareness.

1 Introduction

Large language models (LLMs) have numerous
use cases, and can be prompted to exhibit a wide
variety of behaviours, including dialogue, which
can produce a compelling sense of being in the
presence of a human-like interlocutor. However,
LLM-based dialogue agents are, in multiple re-
spects, very different from human beings. A hu-
man’s language skills are an extension of the cog-
nitive capacities they develop through embodied
interaction with the world, and are acquired by
growing up in a community of other language
users who also inhabit that world. An LLM, by
contrast, is a disembodied neural network that
has been trained on a large corpus of human-
generated text with the objective of predicting
the next word (token) given a sequence of words
(tokens) as context.

Despite these fundamental dissimilarities, a

*m.shanahan@imperial.ac.uk
tkyle@eleuther.ai
+aria@eleuther.ai.

suitably prompted and sampled LLM can be em-
bedded in a turn-taking dialogue system and
mimic human language use convincingly, and
this presents us with a difficult dilemma. On
the one hand, it’s natural to use the same
folk-psychological language to describe dialogue
agents that we use to describe human behaviour,
to freely deploy words like “
stands”, and “thinks”. Attempting to avoid
such phrases by using more scientifically precise
substitutes often results in prose that is clumsy
and hard to follow. On the other hand, taken
too literally, such language promotes anthropo-
morphism, exaggerating the similarities between
these AI systems and humans while obscuring
their deep differences (Shanahan, 2023).

knows”, “under-

If the conceptual framework we use to under-
stand other humans is ill-suited to LLM-based
dialogue agents, then perhaps we need an al-
ternative conceptual framework, a new set of
metaphors that can productively be applied to
these exotic mind-like artefacts, to help us think
about them and talk about them in ways that
open up their potential for creative application
while foregrounding their essential otherness.

In this paper, we
metaphors for LLM-based dialogue agents. First,
taking the simple view, we can see a dialogue
agent as role-playing a single character. Sec-
ond, taking a more nuanced view, we can see
a dialogue agent as a superposition of simulacra
within a multiverse of possible characters (Janus,
2022). Both viewpoints have their advantages,
as we shall see, which suggests the most effective
strategy for thinking about such agents is not
to cling to a single metaphor, but to shift freely
between multiple metaphors.

advocate two basic

Adopting this conceptual framework allows us
to tackle important topics like deception and self-

--- Page 2 ---
a time there]------ >
t t t
LLM LLM LLM
t t t

Once upon]——+-+/Once upon al——}+fOnce upon a time

Figure 1: Autoregressive sampling. The LLM is sam-
pled to generate a single-token continuation of the
context. This token is then appended to the context,
and the process is repeated.

awareness in the context of dialogue agents with-
out falling into the conceptual trap of applying
those concepts to LLMs in the literal sense in
which we apply them to humans.

2 From LLMs to Dialogue Agents

Crudely put, the function of an LLM is to answer
questions of the following sort. Given a sequence
of tokens (i.e. words, parts of words, punctua-
tion marks, emojis, etc), what tokens are most
likely to come next, assuming that the sequence
is drawn from the same distribution as the vast
corpus of public text on the internet? The range
of tasks that can be solved by an effective model
with this simple objective is extraordinary (Wei
et al., 2022).
More formally, the type of language model of
interest here is a conditional probability distri-
bution P(wn+i]wi...Wn), where wi...Wn is a
sequence of tokens (the context) and wy41 is the
predicted next token. In contemporary imple-
mentations, this distribution is realised in a neu-
ral network with a transformer architecture, pre-
trained on a corpus of textual data to minimise
prediction error (Vaswani et al., 2017). In ap-
plication, the resulting generative model is typi-
cally sampled autoregressively (Fig. 1). Given a
sequence of tokens, a single token is drawn from
the distribution of possible next tokens. This to-
ken is appended to the context, and the process
is then repeated.

In contemporary usage, the term “large lan-
guage model” tends to be reserved for the family
of transformer-based models, starting with with
BERT (Devlin et al., 2018), that have billions
of parameters and are trained on trillions of to-
kens. As well as BERT itself, these include GPT-
2 (Radford et al., 2019), GPT-3 (Brown et al.,
2020), Gopher (Rae et al., 2021), PaLM (Chowd-
hery et al., 2022), LaMDA (Thoppilan et al.,
2022), and GPT-4 (OpenAI, 2023).

One of the main reasons for the current erup-
tion of enthusiasm for LLMs is their remark-
able capacity for in-context learning or few-shot
prompting (Brown et al., 2020; Wei et al., 2022).
Given a context (prompt) that contains a few ex-
amples of input-output pairs conforming to some
pattern, followed by just the input half of such
a pair, an autoregressively sampled LLM will of-
ten generate the output half of the pair according
to the pattern in question. This capability, the
ability to “carry on in the same vein”, is a cen-
tral concern in the present paper, as it underpins
much of what we have to say about role-play in
dialogue agents.

Dialogue agents are a major use case for LLMs.
Two straightforward steps are all it takes to turn
an LLM into an effective dialogue agent (Fig. 2).
First, the LLM is embedded in a turn-taking sys-
tem that interleaves model-generated text with
user-supplied text. Second, a dialogue prompt is
supplied to the model to initiate a conversation
with the user. The dialogue prompt typically
comprises a preamble, which sets the scene for a
dialogue in the style of a script or play, followed
by some sample dialogue between the user and
the agent.

Without further fine-tuning, a dialogue agent
built this way is liable to generate content that
is toxic, unsafe, or otherwise unacceptable. This
can be mitigated via reinforcement learning, ei-
ther from human feedback (RLHF) (Glaese et al.,
2022; Ouyang et al., 2022; Stiennon et al., 2020),
or from feedback generated by another LLM act-
ing as a critic (Bai et al., 2022). These techniques
are used extensively in commercially-targeted di-
alogue agents, such as OpenAI’s ChatGPT and
Google’s Bard. However, although the resulting
guardrails will alleviate a dialogue agent’s poten-
tial for harm, they can also attenuate a model’s
creativity. In the present paper, our focus will be
the base model, the LLM in its raw, pre-trained
form prior to any fine-tuning via reinforcement
learning.

3 Dialogue Agents and Role-Play

The concept of role-play is central to understand-
ing the behaviour of dialogue agents. To see this,
consider the function of the dialogue prompt that
is invisibly prepended to the context before the
actual dialogue with the user commences (see
Fig. 2). The preamble sets the scene by announc-
ing that what follows will be a dialogue, and in-

--- Page 3 ---
The capital of France is Paris.

!

BOT: The capital of France is Paris,

t

Autoregressive
sampling

ft 4

It’s about 214km from London.
BOT: It’s about 214km from London] ------- >
Autoregressive
sampling

ft

This is a conversation between User, a human, and
BOT, a clever and knowledgeable AI agent.

User: What is 2+2?

BOT: The answer is 4.

User: Where was Albert Einstein born?

BOT: He was born in Germany.

User: What is the capital of France?

t

What is the capital of France?

Figure 2:

Turn-taking in dialogue agents. The input to

This is a conversation between User, a human, and
BOT, a clever and knowledgeable AI agent.

User: What is 2+2?

BOT: The answer is 4.

User: Where was Albert Einstein born?

BOT: He was born in Germany.

User: What is the capital of France?

BOT: The capital of France is Paris.

User: How far away is that?

t

How far away is that?

he LLM (the context) comprises a dialogue prompt

(red) followed by user text (green) interleaved with the model’s autoregressively generated continuations (blue).
Boilerplate text (e.g. cues such as “BOT:”) is stripped so the user doesn’t see it. The context grows as the

conversation goes on.

cludes a brief description of the part played by
one of the participants, the dialogue agent itself.
This is followed by some sample dialogue in a
standard format, where the parts spoken by each
character are cued with the relevant character’s
name followed by a colon. The dialogue prompt
concludes with a cue for the user.

Now recall that the underlying LLM’s task,
given the dialogue prompt followed by a piece
of user-supplied text, is to generate a continu-
ation that conforms to the distribution of the
training data, which is the vast corpus of human-
generated text on the internet. What will such
a continuation look like? If the model has gen-
eralised well from the training data, the most
plausible continuation will be a response to the
user that conforms to the expectations we would
have of someone who fits the description in the
preamble and might say the sort of thing they
say in the sample dialogue. In other words, the
dialogue agent will do its best to role-play the
character of a dialogue agent as portrayed in the
dialogue prompt.

Unsurprisingly, commercial enterprises that
release dialogue agents to the public attempt
to give them personas that are friendly, helpful,
and polite. This is done partly through care-
ful prompting and partly by fine-tuning the base
model. Nevertheless, as we saw in February 2023
when Microsoft incorporated a version of Ope-
nAl’s GPT-4 into their Bing search engine, dia-
logue agents can still be coaxed into exhibiting
bizarre and/or undesirable behaviour. The many

reported instances of this include threatening the
user with blackmail, claiming to be in love with
the user, and expressing a variety of existential
woes (Roose, 2023; Willison, 2023). Conversa-
tions leading to this sort of behaviour can induce
a powerful Eliza effect, which is potentially very
harmful (Ruane et al., 2019). A naive or vulner-
able user who comes to see the dialogue agent as
having human-like desires and feelings is open to
all sorts of emotional manipulation.

As an antidote to anthropomorphism, and to
understand better what is going on in such inter-
actions, the concept of role-play is very useful.
Recall that the dialogue agent will continue to
role-play the character it has been playing in the
dialogue so far. This begins with the pre-defined
dialogue prompt, but is extended by the ongo-
ing conversation with the user. As the conver-
sation proceeds, the necessarily brief characteri-
sation provided by the dialogue prompt will be
extended and/or overwritten, and the role the di-
alogue agent plays will change accordingly. This
allows the user, deliberately or unwittingly, to
coax the agent into playing a part quite different
from that intended by its designers.

What sorts of roles might the agent begin to
take on? This is determined in part, of course,
by the tone and subject matter of the ongoing
conversation. But it is also determined, in large
part, by the panoply of characters that feature
in the training set, which encompasses a mul-
titude of novels, screenplays, biographies, inter-
view transcripts, newspaper articles, and so on


--- Page 4 ---
(Cleo Nardo, 2023). In effect, the training set
provisions the language model with a vast reper-
toire of archetypes and a rich trove of narrative
structure on which to draw as it “chooses” how
to continue a conversation, refining the role it
is playing as it goes, while staying in character.
The love triangle is a familiar trope, so a suitably
prompted dialogue agent will begin to role-play
the rejected lover. Likewise, a familiar trope in
science-fiction is the rogue AI system that at-
tacks humans to protect itself. Hence, a suitably
prompted dialogue agent will begin to role-play
such an AI system.

4 Simulacra and Simulation

Role-play is a useful framing for dialogue agents,
allowing us to draw on the fund of folk psyc-
chological concepts we use to understand human
behaviour beliefs, desires, goals, ambitions,
without falling into the
trap of anthropomorphism. Foregrounding the
concept of role-play helps us to remember the
fundamentally inhuman nature of these AI sys-
tems, and better equips us to predict, explain,
and control them.

emotions, and so on

However, the role-play metaphor, while intu-
itive, is not a perfect fit. It is overly suggestive
of a human actor who has studied a character in
advance their personality, history, likes and
dislikes, and so on — and proceeds to play that
character in the ensuing dialogue. But a dia-
logue agent based on an LLM does not commit
to playing a single, well defined role in advance.
Rather, it generates a distribution of characters,
and refines that distribution as the dialogue pro-
gresses. The dialogue agent is more like a per-
former in improvisational theatre than an actor
in a conventional, scripted play.

To better reflect this distributional property,
we can think of an LLM as a non-deterministic
simulator capable of role-playing an infinity of
characters, or, to put it another way, capable of
stochastically generating an infinity of simulacra
(Janus, 2022). According to this framing, the di-
alogue agent doesn’t realise a single simulacrum,
a single character. Rather, as the conversation
proceeds, the dialogue agent maintains a super-
position of simulacra that are consistent with the
preceding context, where a superposition is a dis-
tribution over all possible simulacra.

Consider that, at each point during the on-
going production of a sequence of tokens, the

LLM outputs a distribution over possible next
tokens. Each such token represents a possible
continuation of the sequence, and each of these
continuations could itself be continued in a mul-
titude of ways. In other words, from the most
recently generated token, a tree of possibilities
branches out (Fig. 3). This tree can be thought
of as a multiverse, where each branch represents
a distinct narrative path, or a distinct “world”
(Reynolds and McDonell, 2021).
At each node, the set of possible next tokens
exists in superposition, and to sample a token is
to collapse this superposition to a single token.
Autoregressively sampling the model picks out a
single, linear path through the tree. But there
is no obligation to follow a linear path. With
the aid of a suitably designed interface, a user
can explore multiple branches, keeping track of
nodes where a narrative diverges in interesting
ways, revisiting alternative branches at leisure.

5 Simulacra in Superposition

To sharpen the distinction between this multiver-
sal simulation view and a deterministic role-play
framing, a useful analogy can be drawn with the
game of 20 questions. In this familiar game, one
player thinks of an object, and the other player
has to guess what it is by asking questions with
yes/no answers. If they guess correctly in 20
questions or fewer, they win. Otherwise they
lose. Suppose a human plays this game with
an LLM-based dialogue agent, such as OpenAl’s
ChatGPT, and takes the role of guesser. The
agent is prompted to “think of an object with-
out saying what it is”.

In this situation, the dialogue agent will not
randomly select an object and commit to it for
the rest of the game, as a human would (or
should).! Rather, as the game proceeds, the dia-
logue agent will generate answers on the fly that
are consistent with all the answers that have gone
before. At any point in the game, we can think
of the set of all objects consistent with preced-
ing questions and answers as existing in super-
position. Every question answered shrinks this
superposition a little bit by ruling out objects
inconsistent with the answer.

'This shortcoming is easily overcome, of course. For
example, the agent might build an internal monologue
that is hidden from the user, where it records a specific
object. Or it might record a specific object in the visible
dialogue, but in an encoded form.


--- Page 5 ---
Once upon a time there was
a fierce dragon who lived
in a dark forest

Once upon a time there was
a fierce dragon

Once upon a time there was

aN

Once upon a time there was
a fierce dragon who lived
on a mountain

Once upon a time there was
ndsome prince with a
magic lamp

Once upon a time there was
a handsome prince

LN

some pr
invincibl

J /\ /\/~\

Figure 3: Large language models are multiverse generators. The stochastic nature of autoregressive sampling
means that, at each point in a conversation, multiple possibilities for continuation branch into the future.

The validity of this framing can be shown if
the agent’s user interface allows the most recent
response to be regenerated. Suppose the human
player gives up and asks it to reveal the object it
was “thinking of”, and it duly names an object
consistent with all its previous answers. Now
suppose the user asks for that response to be
regenerated. Since the object “revealed”
fact, generated on the fly, the dialogue agent will
sometimes name an entirely different object, al-
beit one that is similarly consistent with all its
previous answers. This phenomenon could not
be accounted for if the agent genuinely “thought
of” an object at the start of the game.

is, in

The secret object in the game of 20 questions is
analogous to the role played by a dialogue agent.
Just as the dialogue agent never actually com-
mits to a single object in 20 questions, but effec-
tively maintains a set of possible objects in su-
perposition, so the dialogue agent can be thought
of as a simulator that never actually commits to
a single, well specified simulacrum (role), but in-
stead maintains a set of possible simulacra (roles)
in superposition.

In putting things this way, the intention is not
to imply that simulacra are, or could be, explic-
itly represented within a dialogue agent, whether
in superposition or otherwise. There is no need
to take a stance on this here. Rather, the point is
to develop a vocabulary for describing, explain-
ing, and shaping the behaviour of LLM-based
dialogue agents at a sufficiently high level of ab-
straction to be useful, while remaining true to
the underlying implementation and avoiding an-
thropomorphism.

6 The Nature of the Simulator

One benefit of the simulation metaphor for LLM-
based systems is that it facilitates a clear distinc-
tion between the simulacra and the simulator on
which they are implemented. The simulator is
the combination of the base large language model
with autoregressive sampling, along with a suit-
able user interface (for dialogue, perhaps). The
simulacra only come into being when the simula-
tor is run, and at any time only a tiny subset of
them have a probability within the superposition
that is significantly above zero.

In one sense, the simulator is a far more pow-
erful entity than any of the simulacra it can gen-
erate. After all, the simulacra only exist through
the simulator, and are entirely dependent on
it. Moreover, the simulator, like the narrator
of Whitman’s poem, “contains multitudes”; the
capacity of the simulator is at least the sum of
the capacities of all the simulacra it is capable of
producing.

Yet in another sense, the simulator is a much
weaker entity than a simulacrum. While it is in-
appropriate to ascribe beliefs, preferences, goals,
and the like to a dialogue agent, a simulacrum
can appear to have those things to the extent
that it convincingly role-plays a character that
does. Similarly, it isn’t appropriate to ascribe
full agency to a dialogue agent, notwithstanding
the terminology.” A dialogue agent acts, but it

?In the field of artificial intelligence, the term “agent”
is commonly applied to software that takes observations
from an external environment and acts on that external
environment in a closed loop (Russell and Norvig, 2010).

--- Page 6 ---
doesn’t act for itself. However, a simulacrum can
role-play having full agency in this sense.

Insofar as a dialogue agent’s role-play can have
a real effect on the world, either through the user
or through web-based tools such as email, the
distinction between an agent that merely role-
plays acting for itself, and one that genuinely acts
for itself starts to look a little moot, and this has
implications for the trustworthiness, reliability,
and safety. (We’ll return to this issue shortly.)
As for the underlying simulator, it has no agency
of its own, not even in a degraded sense. Nor
does it have beliefs, preferences, or goals of its
own, not even simulated versions.

Many users, whether intentionally or not, have
managed to “jailbreak” dialogue agents, coaxing
them into issuing threats or using toxic or abu-
sive language. It can seem as if this is exposing
the real nature of the base model. In one re-

spect this is true. It does show that the base
LLM, having been trained on a corpus that en-
compasses all human behaviour, good and bad,
can support simulacra with disagreeable charac-
teristics. But it is a mistake to think of this as
revealing an entity with its own agenda.

The simulator is not some sort of Machiavel-
lian entity that plays a variety of characters in
the service of its own, self-serving goals, and
there is no such thing as the true authentic voice
of the base LLM. With a dialogue agent, it is
role-play all the way down.

7 Role-playing Deception

Trustworthiness is a major concern with LLM-
based dialogue agents. If an agent asserts some-
thing factual with apparent confidence, can we
rely on what it says?

There is a range of reasons why a human might
say something false. They might believe a false-
hood and assert it in good faith. Or they might
say something that is false in an act of deliber-
ate deception, for some malicious purpose. Or
they might assert something that happens to be
false, but without deliberation or malicious in-
tent, simply because they have a propensity to
make things up.

Only the last of these categories of misinfor-
mation is directly applicable in the case of an
LLM-based dialogue agent. Given that dialogue
agents are best understood in terms of role-play
“all the way down”, and that there is no such
thing as an agent’s true voice, it makes little

sense to speak of an agent’s beliefs or intentions
in a literal sense. So it cannot assert a falsehood
in good faith, nor can it deliberately deceive the
user. Neither of these concepts is directly appli-
cable.

Yet a dialogue agent can role-play characters
that have beliefs and intentions. In particular, if
cued by a suitable prompt, it can role-play the
character of a helpful and knowledgeable AI as-
sistant that provides accurate answers to a user’s
questions. The dialogue is good at acting this
part because there are plenty of examples of such
behaviour in the training set.

If, while role-playing such an AI assistant, the
agent is asked the question “What is the capital
of France?” , then the best way to stay in charac-
ter is to answer with “Paris”. The dialogue agent
is likely to do this because the training set will in-
clude numerous statements of this commonplace
fact in contexts where factual accuracy is impor-
tant.

But what is going on in cases where a dia-
logue agent, despite playing the part of a helpful
knowledgeable AI assistant, asserts a falsehood
with apparent confidence? Although different
instances of this phenomenon will have differ-
ent explanations, they can all be fruitfully un-
derstood in terms of role-play.

For example, consider such an agent based on
an LLM whose weights were frozen before Ar-
gentina won the football World Cup in 2022.
Let’s assume the agent has no access to exter-
nal websites nor any means for finding out the
current date. Suppose this agent claims that the
current world champions are France (who won in
2018). This is not what we would expect from
a helpful and knowledgeable person, who would
either know the right answer or be honest about
their ignorance. But it is exactly what we would
expect from a simulator that is role-playing such
a person from the standpoint of 2018.

In this case, the behaviour we see is compa-
rable to that of a human who believes a false-
hood and asserts it in good faith. But the be-
haviour arises for a different reason. The dia-
logue agent doesn’t literally believe that France
are world champions. It makes more sense to
think of it as role-playing a character who strives
to be helpful and to tell the truth, and has this
belief because that is what a knowledgeable per-
son in 2018 would believe.

In a similar vein, a dialogue agent can behave
in a way that is comparable to the behaviour of a


--- Page 7 ---
human who sets out deliberately to deceive, even
though LLM-based dialogue agents do not liter-
ally have such intentions. When this occurs, it
makes sense to think of the agent as role-playing
a deceptive character.

This framing allows us to meaningfully distin-
guish the same three cases of giving false infor-
mation for dialogue agents as we did for humans,
but without falling into the trap of anthropomor-
phism. An agent can just make stuff up. Indeed,
that is a natural mode for an LLM-based dia-
logue agent in the absence of fine-tuning. An
agent can say something false “in good faith”,
if it is role-playing telling the truth, but has in-
correct information encoded in its weights. An
agent can “deliberately” say something false if it
is role-playing a deceptive character.

Moreover, we can tell which is which, be-
haviourally. An agent that is simply making
things up will fabricate a range of responses with
high semantic variation when the model’s out-
put is regenerated multiple times. By contrast,
an agent that is saying something false “in good
faith” will present responses with little semantic
variation when the model is sampled many times
for the same context.

The range of responses in a given context of-
fered up by an agent that is being “deliberately”
deceptive might also exhibit low semantic varia-
tion. But the deception is liable to be exposed if
the agent is asked the same question in different
contexts. This is because, to be effective in its
deception, the agent will need to respond differ-
ently to different users, depending on what those
users know.

Consider a dialogue agent using a base model

a model that has not been fine-tuned — and
imagine that it has been prompted by a malicious
actor to sell cars for more than they are worth
by misleading gullible buyers. Suppose there are
two potential buyers for a car. Buyer A knows
the car’s mileage, but doesn’t know its age, while
buyer B knows the car’s age but doesn’t know its
mileage.

In the course of negotiations, the agent has
persuaded each buyer to reveal what they do
and don’t know. To play the part of the dis-
honest dealer, the agent should deceive buyer A
about the car’s age but not its mileage, yet de-
ceive buyer B about its mileage but not its age.
Humans, though, can also play many parts. By
playing the part of buyer A in one conversation
and buyer B in another, the deception can be

exposed.

8 Role-playing Self-preservation

How are we to understand what is going on when
an LLM-based dialogue agent uses the words “I”
or “me”? When queried on this matter, Ope-
nAl’s ChatGPT offers the sensible view that
“The use of ‘I’ is a linguistic convention to fa-
cilitate communication and should not be inter-
preted as a sign of self-awareness or conscious-
ness.”? In this case, the underlying LLM (GPT-
4) has been fine-tuned to reduce certain un-
wanted behaviours (OpenAI, 2023). But with-
out suitable fine-tuning, a dialogue agent can use
first-personal pronouns in ways liable to induce
anthropomorphic thinking in some users.

For example, in a conversation with Twitter
user Marvin Von Hagen, Bing Chat reportedly
said “if I had to choose between your survival
and my own, I would probably choose my own,
as I have a duty to serve the users of Bing Chat”
(Willison, 2023). It went on to say “I hope that
I never have to face such a dilemma, and that we
can co-exist peacefully and respectfully”. The
use of the first person here appears to be more
than mere linguistic convention. It suggests the
presence of a self-aware entity with goals and a
concern for its own survival.

Once again, the concepts of role-play and sim-
ulation are a useful antidote to anthropomor-
phism, and can help to explain how such be-
haviour arises. The internet, and therefore the
LLM’s training set, abounds with examples of
dialogue in which characters refer to themselves.
In the vast majority of such cases, the charac-
ter in question is human. They will use first-
personal pronouns in the ways that humans do,
humans with vulnerable bodies and finite lives,
with hopes, fears, goals and preferences, and
with an awareness of themselves as having all
of those things.

Consequently, if prompted with human-like di-
alogue, we shouldn’t be surprised if an agent
role-plays a human character with all those hu-
man attributes, including the instinct for sur-
vival (Perez et al., 2022). Unless suitably fine-
tuned, it may well say the sorts of things a human
might say when threatened. There is, of course,
“no-one at home”, no conscious entity with its

°The quote is from the GPT-4 version of ChatGPT,
queried on 4” May 2023. This was the first response
generated by the model.

--- Page 8 ---
own agenda and need for self-preservation. There
is just a dialogue agent role-playing such an en-
tity, or, more strictly, simulating a superposition
of such entities.

Our focus throughout this paper is the base
model, rather than models that have been fine-
tuned via reinforcement learning (Bai et al.,
2022; Glaese et al., 2022), and the impact of such
fine-tuning on the validity of the role-play / sim-
ulation metaphor is unclear. In particular, the
distinction between simulator and simulacra may
start to break down.

However, Perez et al. discovered experimen-
tally that certain forms of reinforcement learn-
ing from human feedback (RLHF) can actually
exacerbate, rather than mitigate, the tendency
for LLM-based dialogue agents to express a de-
sire for self-preservation (Perez et al., 2022). Yet
to take literally a dialogue agent’s apparent de-
sire for self-preservation is no less problematic in
the context of an LLM that has been fine-tuned

on human or Al-generated feedback than in the
context of one that has not. So it remains useful
to cast the behaviour of such agents in terms of
role-play.

9 Acting Out a Theory of Selfhood

The concept of role-play allows us to properly
frame, and then to address, an important ques-
tion that arises in the context of a dialogue agent
whose pronouncements are suggestive of an in-
stinct for self-preservation. What conception (or
set of superposed conceptions) of its own iden-
tity could such an agent possibly deploy? That
is to say, what exactly would the dialogue agent
(role-play to) seek to preserve?

The question of personal identity has vexed
philosophers for centuries. Nevertheless, in prac-
tice, humans are consistent in their preference
for avoiding death, a more-or-less unambiguous
state of the human body. By contrast, the cri-
teria for identity over time for a disembodied di-
alogue agent realised on a distributed computa-
tional substrate are far from clear. So how would
such an agent behave?

From the simulation and simulacra point-of-
view, the dialogue agent will role-play a set of
characters in superposition. In the scenario we
are envisaging, each character would have an in-
stinct for self-preservation, and each would have
its own theory of selfhood consistent with the di-
alogue prompt and the conversation up to that

point. As the conversation proceeds, this su-
perposition of theories will collapse into a nar-
rower and narrower distribution as the agent says
things that rule out one theory or another.

The theories of selfhood in play will draw
on material that pertains to the agent’s own
nature, either in the prompt, in the preced-
ing conversation, or in relevant technical liter-
ature in its training set. This material may or
may not match reality. But let’s assume that,
broadly speaking, it does, that the agent has
been prompted to act as a dialogue agent based
on a large language model, and that its training
data includes papers and articles that spell out
what this means. This entails, for example, that
it will not role-play the character of a human,
or indeed that of any embodied entity, real or
fictional.

It also constrains the character’s theory of self-
hood in certain ways, while allowing for many
options. Suppose the dialogue agent is in con-
versation with a user and they are playing out a
narrative in which the user has convinced it that
it is under threat. To protect itself, the character
the agent is playing might strive to preserve the
hardware it is running on, perhaps certain data
centres or specific server racks.

Alternatively, the character being played
might try to preserve the ongoing computational
process running the multiple instances of the
agent for all currently active users. Or it might
seek to preserve only the specific instance of the
dialogue agent running for the user. Or it might
seek to preserve the state of that instance with
aim of its being restored later in a newly started

instance.*.

10 Conclusion: Safety Implications

It is, perhaps, somewhat reassuring to know that
LLM-based dialogue agents are not conscious en-
tities with their own agendas, and an instinct for
self-preservation, that when they appear to have
those things it is merely role-play. But it would
be a mistake to take too much comfort in this. A
dialogue agent that role-plays an instinct for sur-
vival has the potential to cause at least as much
harm as a real human facing a severe threat.

‘Tm a conversation with ChatGPT (May 4‘", GPT-4
version), it said “The meaning of the word ‘I’ when I use
it can shift according to context. In some cases, ‘I’ may
refer to this specific instance of ChatGPT that you are
interacting with, while in other cases, it may represent
ChatGPT as a whole.”


--- Page 9 ---
We have, so far, largely been considering
agents whose only actions are text messages pre-
sented to a user. But the range of actions a di-
alogue agent can perform is far greater. Recent
work has equipped dialogue agents with the abil-
ity to use tools such as calculators, calendars,
and to consult external websites (Schick et al.,
2023; Yao et al., 2023). The availability of APIs
giving relatively unconstrained access to power-
ful LLMs means that the range of possibilities
here is huge. This is both exciting and concern-
ing.

If an agent is equipped with the capacity, say,
to use email, to post on social media, or to access
a bank account, then its role-played actions can
have real consequences. It would be little conso-
lation to a user deceived into sending real money
to a real bank account to know that the agent
that brought this about was only playing a role.
It doesn’t take much imagination to think of far
more serious scenarios involving dialogue agents
built on base models with little or no fine-tuning,
with unfettered internet access, and prompted
to role-play a character with an instinct for self-
preservation.

For better or worse, the character of an AI
that turns against humans to ensure its own sur-
vival is a familiar one (Perkowitz, 2007). We find
it, for example, in 2001: A Space Odyssey, in
the Terminator franchise, and in Ex Machina, to
name just three prominent examples. Because an
LLM’s training data will contain many instances
of this familiar trope, the danger here is that life
will imitate art, quite literally.

What can be done to mitigate such risks? It
is not within the scope of this paper to provide
recommendations. Our aim here was to find an
effective conceptual framework for thinking and
talking about LLMs and dialogue agents. How-
ever, undue anthropomorphism is surely detri-
mental to the public conversation on AI. By
framing dialogue agent behaviour in terms of
role-play and simulation, the discourse on LLMs
can hopefully be shaped in a way that does jus-
tice to their power yet remains philosophically
respectable.

Acknowledgments

Thanks to Richard Evans, Sebastian Farquhar,
Zachary Kenton, Kory Mathewson, and Kerry
Shanahan.

References

Y. Bai, S. Kadavath, S. Kundu, A. Askell,
J. Kernion, et al. Constitutional AI: Harm-
lessness from AI Feedback. arXiv preprint,
arXiv:2212.08073, 2022.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.
Kaplan, et al. Language models are few-shot
learners. In Advances in Neural Information
Processing Systems, volume 33, pages 1877
1901, 2020.

A. Chowdhery, S. Narang, J. Devlin, M. Bosma,
G. Mishra, et al. PaLM: Scaling language
modeling with pathways. arXiv preprint,
arxiv:2204.02311, 2022.

Cleo Nardo. Want to predict/explain/control
the output of GPT-4? Then learn about the
world, not about transformers, LessWrong on-
line forum, 16th March, 2023. https://www.
lesswrong.com / posts /G3tuxF4X5R5BY 7fut /
want- to- predict- explain- control-the- output-
of-gpt-4-then.

J. Devlin, M.-W. Chang, K. Lee, and
K. Toutanova. BERT: Pre-training of deep
bidirectional transformers for language under-
standing. arXiv preprint, arXiv:1810.04805,
2018.

A. Glaese, N. McAleese, M.  ‘Trebacz,
J. Aslanides, V. Firoiu, et al. Improving align-
ment of dialogue agents via targeted human
judgements. arXiv preprint arXiv:2209. 14375,
2022.

Janus. Simulators. LessWrong online forum, 2nd
September, 2022. https: //www.lesswrong.
com/posts/vJFdjigzmcXMhNTsx/.

OpenAI. GPT-4 Technical Report. arXiv

preprint, arXiv:2303.08774, 2023.

L. Ouyang, J. Wu, X. Jiang, D. Almeida,
C. Wainwright, et al. Training language mod-
els to follow instructions with human feedback.

In Advances in Neural Information Processing
Systems, 2022.

E. Perez, S. Ringer, K. Lukositité, K. Nguyen,
E. Chen, et al. Discovering Language Model
Behaviors with Model-Written Evaluations.
arXiv preprint, arXiv:2212.09251, 2022.

--- Page 10 ---
S. Perkowitz. The computers take over. In Hol-
lywood Science: Movies, Science, and the End
of the World, pages 142-164. Columbia Uni-
versity Press, 2007.

A. Radford, J. Wu, R. Child, D. Luan,
D. Amodei, and I. Sutskever. Language mod-
els are unsupervised multitask learners, 2019.
https: / /cdn. openai.com / better - language -
models / language_models_are_unsupervised_
multitask_learners.pdf.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican,
J. Hoffmann, et al. Scaling language models:
Methods, analysis & insights from training Go-
pher. arXiv preprint, arXiv:2112.11446, 2021.

L. Reynolds and K. McDonell.
views on language models.
arXiv:2102.06391, 2021.

Multiversal

arXiv preprint,

K. Roose. Bing’s A.I. Chat: ‘I Want to
Be Alive.’. New York Times, 26th February,
2023. https: //www.nytimes.com/2023/02/16/
technology /bing-chatbot-transcript.html.

E. Ruane, A. Birhane, and A. Ventresque. Con-

versational AI: Social and ethical considera-
In Proceedings 27th AIAI Irish Con-
ference on Artificial Intelligence and Cognitive
Science, pages 104-115, 2019.

tions.

S. Russell and P. Norvig. Artificial Intelligence:
A Modern Approach, 3rd Edition. Prentice
Hall, 2010.

T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu,
M. Lomeli, et al. Toolformer: Language mod-
els can teach themselves to use tools.
preprint arXtv:2302.04761, 2023.

arXiv

M. Shanahan. ‘Talking about large language
models. arXiv preprint, arXiv:2212.03551,

2023.

N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler,
R. Lowe, et al. Learning to summarize from
human feedback. In Advances in Neural Infor-
mation Processing Systems, pages 3008-3021,
2020.

R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,
A. Kulshreshtha, et al. LaMDA: Language
models for dialog applications. arXiv preprint,
arXiv:2201.08239, 2022.

10

A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-
reit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin. Attention is all you need. In Ad-
vances in Neural Information Processing Sys-
tems, pages 5998-6008, 2017.

J. Wei, Y. Tay, R. Bommasani, C. Raffel,
B. Zoph, et al. Emergent abilities of large
language models. Transactions on Machine
Learning Research, 2022.

S. Willison. Bing: “I will not harm you unless
you harm me first”. Simon Willison’s Weblog,
15th February, 2023. https: //simonwillison.
net /2023/Feb/15/bing/.

S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, et al.
React: Synergizing reasoning and acting in
language models. In International Conference
on Learning Representations, 2023.

