--- Page 1 ---
2307.01229v1 [cs.SD] 3 Jul 2023

arXiv

EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Chenfei Kang"! Peiling Lu"? Botao Yu? Xu Tan? Wei Ye*+ Shikun Zhang‘ Jiang Bian?

https://github.com/microsoft/muzic

Abstract

Music is used to convey emotions, and thus gen-
erating emotional music is important in automatic
music generation. Previous work on emotional
music generation directly uses annotated emo-
tion labels as control signals, which suffers from
subjective bias: different people may annotate
different emotions on the same music, and one
person may feel different emotions under different
situations. Therefore, directly mapping emotion
labels to music sequences in an end-to-end way
would confuse the learning process and hinder
the model from generating music with general
emotions. In this paper, we propose EMOGEN,
an emotional music generation system that lever-
ages a set of emotion-related music attributes as
the bridge between emotion and music, and di-
vides the generation into two stages: emotion-to-
attribute mapping with supervised clustering, and
attribute-to-music generation with self-supervised
learning. Both stages are beneficial: in the first
stage, the attribute values around the clustering
center represent the general emotions of these
samples, which help eliminate the impacts of the
subjective bias of emotion labels; in the second
stage, the generation is completely disentangled
from emotion labels and thus free from the sub-
jective bias. Both subjective and objective evalua-
tions show that EMOGEN outperforms previous
methods on emotion control accuracy and music
quality respectively, which demonstrate our su-
periority in generating emotional music. Music
samples generated by EMOGEN are available via
this link!, and the code is available at this link”.

“Equal contribution ‘Shanghai Jiao Tong University,
China *Microsoft Research Asia *Nanjing University, China
4National Engineering Research Center for Software Engineer-
ing, Peking University, China. Correspondence to: Xu Tan
<xuta@microsoft.com>.

‘https: //ai-muzic.github. io/emogen/
*https://github.com/microsoft/muzic/

1. Introduction

With the development of deep learning, automatic music
generation is developing rapidly and attracting more and
more interest (Hernandez-Olivan & Beltran, 2022; Shih
et al., 2022; Yu et al., 2022). Due to the importance of emo-
tions for music, emotional music generation is an important
and practical task, yet it is still under-explored.

Previous work, according to the way of applying emotion
signals, can be divided into two types. The first type is
to convert emotion labels as embeddings and take them
as model input (Madhok et al., 2018; Hung et al., 2021;
Pangestu & Suyanto, 2021; Sulun et al., 2022; Grekow &
Dimitrova-Grekow, 2021). The second type is to train an
emotion classifier and apply it at either model output to
guide the decoding process (Ferreira & Whitehead, 2019;
Ferreira et al., 2020; 2022; Bao & Sun, 2022), or latent
space of variational autoencoders (Tan & Herremans, 2020)
and generative adversarial networks (Tseng et al., 2021) to
constrain the distribution of latent vectors.

However, both the above two types of work directly use emo-
tion labels as the control signals to generate music sequences
in an end-to-end way, which is suboptimal. Emotion labels
given by data annotators can be influenced by both objective
and subjective factors. Objective factors like tempo and note
density are highly associated with music emotions. As for
subjective factors, the perceived emotions are highly related
to social identities, personalities, instant emotional states
of listeners, etc. For example, it is highly possible that a
listener thinks a happy song is sad when he/she is in an upset
state. Due to this subjectivity of human emotions, different
data annotators may give different emotion labels to the sam-
ples with the same emotion, which results in subjective bias
in emotion labels. With the inconsistent emotion labels, it is
hard for those end-to-end methods to learn the relationship
between emotion and music sequences, and accordingly, the
models can be deficient in generating music that exactly
matches the desired emotion.

In this paper, we propose EMOGEN, an emotional music
generation system that can eliminate the impacts of sub-
jective bias of emotion labels. Instead of directly mapping
emotion labels to music sequences in an end-to-end way,

--- Page 2 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

we leverage a set of music attributes that are highly corre-
lated with emotions as a bridge and break down this task
into two stages: emotion-to-attribute mapping with super-
vised clustering, and attribute-to-music generation with self-
supervised learning.

Specifically, to bridge the gap between emotions and music,
the attributes need to be highly correlated with emotions.
We design the attribute set by training an emotion classifier
on a labeled dataset and selecting those attributes whose
feature importance is high.

In the emotion-to-attribute mapping stage, we map the emo-
tion to the attribute values of a sample closest to the clus-
tering center, which is obtained by clustering samples with
emotion labels and calculating the average attribute values
from each cluster. This clustering process is supervised
since we use emotion labels to cluster samples into emotion
categories. By this supervised clustering, mapped attribute
values can represent the general emotion from the samples
around the clustering center. Thus, the problem of subjective
bias from emotion labels can be eliminated.

In the attribute-to-music generation stage, we extract the at-
tribute values from music sequences and train a Transformer-
based model with these attributes as the control signals in
a self-supervised way. The values of the attributes can be
directly extracted from music sequences, so this generative
model can learn the relationship between control signals and
music without requiring any labeled data. Since this process
is completely disentangled from emotion labels, it is not
influenced by the subjective bias of emotion labels. With
the benefits of the two stages based on supervised clustering
and self-supervised learning on avoiding the subjective bias,
EMOGEN can achieve a more precise emotion control in
emotional music generation.

The main contributions of this work are as follows:

* We propose EMOGEN, an emotional music generation
system that can eliminate subjective bias from emotion
labels, which leverages emotion-related attributes as a
bridge to generate music with desired emotions by two
stages: emotion-to-attribute mapping with supervised
clustering and attribute-to-music generation with self-
supervised learning.

¢ Experimental results show that EMOGEN outperforms
previous methods on emotion control accuracy and
music quality. Experiments also demonstrate the ability
of EMOGEN to eliminate subjective bias in emotion
labels.

2. Related Work

2.1. Emotional Music Generation

Emotion-conditioned music generation is developing rapidly
in the age of deep learning. According to the way of apply-
ing emotion signals, previous work can be divided into two
types. The first type is to convert emotion labels as embed-
dings and take them as model input (Madhok et al., 2018;
Hung et al., 2021; Pangestu & Suyanto, 2021; Sulun et al.,
2022; Grekow & Dimitrova-Grekow, 2021). Madhok et al.
generate emotional music based on the one-hot emotion
label. Some work (Hung et al., 2021; Pangestu & Suyanto,
2021) add extra emotion tokens into MIDI events to gener-
ate music with specific emotions. Sulun et al. control music
generation conditioned on continuous-valued valence and
arousal labels. The second type is to train an emotion clas-
sifier and apply it at either model output through heuristic
search methods guiding the decoding process (Ferreira &
Whitehead, 2019; Ferreira et al., 2020; 2022; Bao & Sun,
2022), or latent space of variational autoencoders (Tan &
Herremans, 2020) or generative adversarial networks (Tseng
et al., 2021) to constrain the distribution of latent vectors.
Ferreira & Whitehead use Genetic Algorithm to optimize
the weights of the Long Short-Term Memory (LSTM) to
generate music with desired emotions. Some work (Bao
& Sun, 2022; Ferreira et al., 2020; 2022) apply search al-
gorithm (e.g., beam search and tree search) to direct music
generation with desired emotions. However, both of the
above two types directly use emotion labels as the control
signals to guide music generation, which ignores the im-
pact of subjective bias of emotion labels as discussed in §1.
Therefore, it is difficult for existing methods to generate
music that matches the desired emotion.

2.2. Attribute-Based Controllable Music Generation

Music attributes are extracted from music sequences and can
be manipulated to control music generation. Previous work
attempt to leverage these attributes for controlling the music
generation process. These works (Tan & Herremans, 2020;
Kawai et al., 2020; Zhao et al., 2022) extract attributes like
rhythm density, pitch and rhythm variability, and chords and
apply a VAE-based framework to control music generation
by music attributes. A discriminator is used to control the
hidden space distribution to satisfy the attribute conditions.
Wu & Yang propose MuseMorphose, which adds rhythmic
and polyphony intensity into the latent space of VAE to
control music generation. von Riitte et al. propose FIGARO,
which designs expert description (such as note density, mean
pitch, etc.) and learned description (latent representation) to
control music generation with a VQ-VAE system. Directly
using these attributes for emotional music generation is not
enough, since they either do not consider building relation-
ships between emotions and attributes, or fail to construct a

--- Page 3 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

concrete correlation between attributes and emotions, which
may result in poor controlling accuracy.

3. Method

Figure la shows the pipeline of EMOGEN, which contains
two stages: emotion-to-attribute mapping, and attribute-
to-music generation, with a set of designed attributes as a
bridge. In attribute design, we enumerate and select the set
of attributes that are highly correlated with emotions, which
can help build a consistent relationship between emotions
and music. For the emotion-to-attribute mapping stage,
we get the mapped attribute values by choosing the values
closest to the clustering center. The mapped attribute values
can well represent the general emotions, so as to alleviate
the subjective bias from emotion labels.

For the attribute-to-music generation stage, the attribute val-
ues directly extracted from music sequences are used as
control signals for training an autoregressive Transformer
based model to generate corresponding music sequences in
a self-supervised way. By disentangling the generation pro-
cess from emotion labels, we can avoid the subjective bias
from emotion labels to achieve better control in emotional
music generation. We discuss the merits of our system in
§3.4.

3.1. Emotion-Related Attribute Design

Instead of generating music sequences with the conditions
of emotion labels, where subjective bias exists, we intro-
duce emotion-related attributes to bridge the gap between
emotions and the corresponding music. Compared with
emotion labels, these objective attributes tell exactly what
the corresponding music should be. For example, the tempo
value tells what the duration of a beat is, while the key scale
states what a set of notes can be used. By directly extracting
the values of these attributes from music sequences, we can
help build an explicit relationship between emotions and
music. Specifically, we collected music attributes from low-
level features like pitch statics, chords and vertical intervals,
rhythm, and dynamics to high-level features like melodic in-
tervals, instrumentation, and musical texture (McKay et al.,
2018).

However, since many of them are irrelevant to emotions, di-
rectly using all of them would introduce a lot of noise. Thus,
we select attributes that are highly correlated with emotions
by training a Random Forest (RF) (Ho, 1995) classifier on
an emotion-annotated dataset, then picking up the top-k
attributes according to the ranking of feature importance as
the final attributes set. Through this process, the designed
attributes can represent emotional information and help con-
trol the music generation. Please refer to Appendix A for
details of these designed attributes.

Attribute-to-Music

Gancatiten —> Music

t
Attributes
Design

(a) Pipeline of EMOGEN.
Supervised
Clustering

Emotion Label

(b) Emotion-to-attribute mapping with supervised clustering. The
emotion space is divided by arousal and valence into four quadrants
based on Russell’s 4Q model (Russell, 1980). Mapped attribute
values from emotion labels that represent general emotions are
marked with red dots, while attribute values from emotion labels
that contain subjective bias are marked with yellow dots.

_, Attribute Values
Extraction

Music Sequence ,
v Generation 3

Eo ion ——______!
Reconstruction

(c) Training process of attribute-to-music generation in self-
supervised learning. x represents the target music sequences, 2°’
represents the generated music sequences, and v represents the
d-dimension vector of extracted attribute values from target music
sequences.

Figure 1: Overview of EMOGEN.

3.2. Emotion-to-Attribute Mapping

To generate music with the desired emotion based on the
emotion-related attributes, the emotion label is mapped to
the attribute values that represent the general emotion with
supervised clustering as shown in Figure 1b.

Specifically, we first extract the values of the selected at-
tributes for each sample in an emotion-annotated dataset.
Based on the emotion labels given by the dataset, among the
samples of each emotion label, we calculate the mean value
for each attribute to obtain the center. Then, the attribute
values of the sample that is the closest to the center are used
to represent the features of the emotion.

This process is supervised since emotion labels are used as
clustering guidance to group samples into categories. This
is also a clustering process since the samples are grouped
in such a way that samples in the same group share similar

--- Page 4 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

emotional information, while samples in different groups
convey distinct emotions. Through this supervised cluster-
ing method, the obtained attribute values should be able to
represent the general emotion given this emotion label and
avoid the subjective bias coming from emotion labels.

3.3. Attribute-to-Music Generation

Attribute values can be easily extracted from music se-
quences, which is much more precise for controlling music
generation. The training process is shown in Figure lc, we
extract the values of the emotion-related attributes from the
target music sequence and represent them in a d-dimension
vector, we then take this vector as control signals into an
autoregressive Transformer based model for generating the
corresponding music sequence. The model is trained with
the mapped attribute vectors as supervisory signals in a self-
supervised way. Through this self-supervised learning step,
the learned Transformer model is able to generate music
whose attributes are controlled by the input attribute val-
ues. When inference, the attribute values mapped in the
emotion-to-attribute mapping stage are leveraged as the con-
trol signals to guide the music generation process. Since the
generation process is completely disentangled from emotion
labels, it is not affected by the subjective bias from emotion
labels.

3.4. Merits of EMOGEN

This proposed framework is beneficial for generating music
with desired emotion in the following aspects:

¢ Ability to eliminate subjective bias. By leveraging
the supervised clustering and self-supervised learning
paradigm in the two stages, EMOGEN can eliminate
subjective bias from emotion labels to achieve better
emotion control accuracy. Specifically, by mapping
emotions to emotion-related attributes with supervised
clustering, we obtain the values of attributes on behalf
of the general emotion. By training the autoregressive
Transformer based model with attribute values as con-
trol signals in a self-supervised way, we disentangle
emotion labels from the generation process and build
an explicit relationship between control signals and
music sequences. The emotion labels are not directly
used in the whole generation process so that we can
avoid the subjective bias that exists in emotion labels.

* Ability to precisely control generation process. Music
attributes are good tools to concretely direct the gen-
eration. A single emotion label is too ambiguous to
define what the corresponding music should be. For
example, it is hard to define what a piece of happy
music should be like. In contrast, music attributes are
concrete to designate specific aspects of music (Tan &

Herremans, 2020; Wu & Yang, 2021; von Riitte et al.,
2022; Di et al., 2021; Chen et al., 2020). For example,
the tempo value tells exactly the duration of one beat,
and the type of scale determines what sets of notes are
used in generated music. By simply manipulating the
values of the music attributes, we can precisely control
the generated music.

* Ability to be free from labeled data. EMOGEN can gen-
erate emotion-conditioned music without requiring any
emotion annotation. Manual annotation is expensive,
and there are only a few datasets (Hung et al., 2021)
that contain emotion annotations. Unlike the previous
methods that require emotion-music paired data for
training the generative model, in EMOGEN, emotion
labels are only used to determine the emotion-related
attributes and the mapped attribute values that repre-
sent the general emotion. Once they are determined,
they will not be changed. After that, we can simply
extract the attribute values on an arbitrary dataset to
train the generative model on it with self-supervised
learning. Therefore, EMOGEN can be used to generate
emotion-conditioned music even if the dataset has no
emotion annotations.

4. Experiment

In this section, we first introduce the experiment setup
(§4.1), followed by the comparison with previous meth-
ods. After that, we give a comprehensive discussion on how
EMOGEN eliminates the subjective bias of emotion labels.
Then we show the comprehensive analysis of EMOGEN.
Finally, we show the results of applying the framework of
EMOGEN to other arbitrary datasets with no annotations.

4.1. Experiment Setup

Datasets We use altogether three datasets including one
emotion-labeled dataset namely EMOPIA (Hung et al.,
2021), and two unlabeled datasets namely Pop1k7 (Hsiao
et al., 2021) and LMD-Piano, where LMD-Piano is con-
structed by using the samples that only contain piano tracks
from the Lakh MIDI (LMD) dataset (Raffel, 2016). The
information of these datasets is shown in Table 1. EMOPIA
uses Russell’s 4Q model (Russell, 1980) as the emotion
classification criterion, which is also leveraged in our eval-
uation process. EMOPIA with emotion labels is used to
determine the designed attributes and the mapped attribute
values in the emotion-to-attribute stage. Once they are de-
termined, they are kept unchanged and emotion labels will
not be used. It is also used in the fine-tuning stage when
compared with previous methods. Pop1k7 and LMD-Piano
are used for the pre-training stage when compared with pre-
vious methods. We randomly split each dataset by 8/1/1
for training/validation/test, respectively.

--- Page 5 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Table 1: The information of training datasets.

Name Music type Size Label type
EMOPIA Piano 1,078 — Russell’s 4Q
Pop1k7 Piano 1,748 None
LMD-Piano Piano 22,643 None

System Configurations We use a REMI-like (Huang &
Yang, 2020) representation method to convert MIDI into
token sequences. We apply jSymbolic (McKay et al., 2018)
to extract attribute values from music, and train the Ran-
dom Forest classifier on EMOPIA, then select the top-100
attributes that are most related to emotions as described
in §3.1. In the emotion-to-attribute mapping stage (§3.2),
supervised clustering is implemented on EMOPIA. In the
attribute-to-music generation stage (§3.3), we binarize the
mapped attribute vector with the median, which is then fed
into a 2-layer feed-forward network to obtain the attribute
embedding. It is then added onto the token embedding at
the input of an autoregressive Transformer model. We lever-
age Linear Transformer (Katharopoulos et al., 2020) as the
backbone model, which consists of 6 Transformer layers
with causal attention and 8 attention heads. The attention
hidden size is 512 and FFN hidden size is 2,048. The max
sequence length of each sample is 1,280. During training,
the batch size is set to 8. We use Adam optimizer (Kingma
& Ba, 2015) with 6, = 0.9, 62 = 0.98 and « = 10~°. The
learning rate is 1 x 10-4 with warm-up step 16000 and an
inverse-square-root decay. The dropout rate is 0.1. During
inference, we apply top-p sampling with ratio p = 0.9 and
temperature 7 = 1.0.

Compared Methods We compare EMOGEN with two
representative methods of different emotion control manners.
The first one is Conditional Sampling (CS) (Hung et al.,
2021), which uses extra emotion tokens in the model input
as the emotion conditions. The other one is Predictor Upper
Confidence for Trees (PUCT) (Ferreira et al., 2022), which
uses an emotion classifier and a music discriminator trained
on labeled data to direct the inference process.

Evaluations and Metrics We conduct both subjective and
objective evaluations to evaluate EMOGEN. Each model is
applied to generate 1,000 music pieces, with 250 for each of
the four emotions. In subjective evaluation, human scorers
are asked to rate each music piece. We report the following
subjective metrics: 1) Subjective accuracy: Whether the per-
ceived emotion by subjects is consistent with the emotion
label. It represents emotion controllability. 2) Humanness:
How similar it sounds to the music composed by a human.
It represents the music quality. 3) Overall: an overall score.
In objective evaluation, following (Ferreira et al., 2022), we

use a Linear Transformer-based emotion classifier trained
on EMOPIA to predict the emotion label of each gener-
ated music piece. Then we calculate objective accuracy by
comparing the emotion input for generating music with the
predicted emotion class by this classifier. We report the
objective accuracy of the classification, which functions as
a supplementary metric to the subjective accuracy. For more
details about the human rating process and the evaluation
metrics, please refer to Appendix §B.1.

4.2. Comparison with Previous Methods

In order to conduct thorough evaluations, we design two
training settings:

1) Setting 1 (S1): To ensure the music quality of generated
music, following previous work (Hung et al., 2021; Ferreira
et al., 2022; Neves et al., 2022), we pre-train the models
on Pop1k7+LMD-Piano before fine-tuning on EMOPIA.
For EMOGEN, we first pre-train the language model with
designed attributes as control signals, then fine-tune it on
EMOPIA with attributes as control signals. For CS, fol-
lowing the work of (Hung et al., 2021), we pre-train the
language model with the control of the emotion token set-
ting to “<None>” as a placeholder, followed by attributes.
After pre-training, we finetune the model on EMOPIA with
emotion tokens assigned to the placeholder, followed by
attributes as control signals. For PUCT, we first pre-train
the language model, then fine-tune it on EMOPIA with an
extra classification head to get the emotion classifier. We
train the music discriminator by fine-tuning the language
model with an extra classification head to classify real/fake
samples. All of the methods are pre-trained on Pop1k7 and
LMD-Piano.

2) Setting 2 (S2): The training methods in the above setting
have their limitations in that they can only generate music
similar to the dataset used in the fine-tuning stage. This con-
strains the ability of EMOGEN, which can naturally leverage
arbitrary datasets for emotional music generation. To test
this ability, we train the generative model on Pop1k7+LMD-
Piano+EMOPIA in the attribute-to-music generation stage,
and use the designed and mapped attributes for generating
corresponding music with given emotions. Please note that
since CS and PUCT require only labeled data in training,
they cannot work in this setting. Therefore, we compare
EMOGEN with the ground truth, the EMOPIA dataset.

The results are shown in Table 2. We can observe that: 1)
Compared with CS and PUCT, EMOGEN achieves better
performance on all the metrics in $1. Particularly, EMOGEN
has much better emotion controllability on both the sub-
jective and objective accuracy. It demonstrates the supe-
riority of EMOGEN in generating music with designated
emotion. Besides, the higher humanness and overall score
of EMOGEN indicate that EMOGEN is capable of improving

--- Page 6 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Table 2: The results of the subjective and objective evaluation. For humanness and overall scores, we report mean opinion

scores with 95% confidence interval.

Subjective Objective
Setting Method Accuracyt Humannesst Overall + Accuracy ¢
EMOPIA Ground truth 0.433 4.26+0.15 4.19+0.15 0.628
$1: Pre-traini Pop1k7+LMD-Pi CS 0.250 3.48 + 0.22 3.59 + 0.22 0.439
: Pre-training on Pop -Piano, L 29R +1 99 7
fine-tuning on EMOPIA PUCT 0.150 £0.24 3.2640.23 0.260
EMOGEN 0.650 3.59 + 0.18 0.715
S2: Training on Pop1k7+LMD-Piano+EMOPIA EMOGEN | 0.550 3.65 + 0.20 | 0.658

the music quality.

2) In $2, EMOGEN achieves higher performance on all
the metrics than CS and PUCT in $1. It demonstrates
that EMOGEN can not only leverage an arbitrary dataset
for emotional music generation but also have fairly good
controllability and humanness. We will also show its ability
on a dataset of more diverse and multi-instrument music in
§4.5.

3) The accuracy of EMOGEN is even higher than that of
the ground truth (EMOPIA). This shows that, on the one
hand, there are samples with ambiguous emotion labels in
the labeled dataset that affect the judgment of their emotions.
On the other hand, the two-stage framework of EMOGEN,
especially the mapped attribute values from emotions deter-
mined by supervised clustering in the emotion-to-attribute
mapping stage, can help avoid the subjective bias from emo-
tion labels. Thus, we choose S2 as our basic framework and
use it to do further analysis of EMOGEN.

4.3. Verification on Eliminating Subjective Bias

To demonstrate that EMOGEN can eliminate subjective bias
in emotion labels, we conduct experiments to show 1) the
existence of subjective bias in a labeled dataset and 2)
EMOGEN’s ability to eliminate subjective bias. We use
the subjective and objective accuracy described above as the
metrics.

Existence of Subjective Bias in Emotion Labels Subjec-
tive bias exists in emotion labels, which can result in poor
controlling performance for end-to-end methods. To prove
the existence of the subjective bias in emotion labels, we
compare the emotion accuracy of the center samples and
that of the boundary samples. Specifically, all the samples
of EMOPIA are firstly clustered by emotion labels to get
four emotion clusters. Then, we calculate the attribute aver-
age in each emotion cluster to get the clustering center. We
choose 50 samples that are closest to the clustering center
(i.e. center samples) and 50 samples that are distant from
the clustering center (i.e., boundary samples). We ask 10

listeners to classify the samples into four emotion categories
to get subjective accuracy and use the classification model
to get objective accuracy. As shown in Table 3, both the
subjective and objective accuracy in classifying the center
samples is higher than that in classifying the boundary sam-
ples, which indicates that subjective bias exists in the dataset,
especially in the labels of the boundary samples, and this
subjective bias can hinder classification performance. We
urther validate this by performing t-SNE visualization of
the samples in EMOPIA with central mapping and distance
analysis of attribute vectors of samples from EMOPIA. The
t-SNE visualization shown in Figure 2a reveals that the sam-
ples in EMOPIA fail to be grouped separately. As shown in
Figure 3a, the wider area between two curves indicates bet-
ter performance in differentiating different groups, however,
there is not much distance between the curves calculated
rom samples of EMOPIA. The above results further prove
that subjective bias exists in emotion labels.

Subjective Bias Elimination To prove that EMOGEN can
eliminate subjective bias in emotion labels, we compare the
classification accuracy of the generated center samples and
that of the generated boundary samples. Specifically, the
center samples are generated by using the attribute values
of the center samples extracted in the emotion-to-attribute
mapping stage. Similarly, the generated boundary samples
use attribute values of the boundary samples. As shown
in Table 3, both the subjective and objective accuracy of
the center samples are higher than those of the boundary
samples, which indicates the effectiveness of the supervised
clustering in the emotion-to-attribute mapping stage in elim-
inating subjective bias. We further validate this by per-
forming t-SNE visualization of the samples generated by
EMOGEN with central mapping and distance analysis of
attribute vectors of samples generated by EMOGEN. As
shown in Figure 2b, the samples generated by EMOGEN
can be clustered into four distinct groups. As shown in Fig-
ure 3b, the intra-class distance of EMOGEN is smaller than
that of EMOPIA, which indicates that samples generated by
EMOGEN are more similar in emotion expression in each

--- Page 7 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Table 3: Subjective and objective accuracy of using the
center and boundary samples.

Subjective | Objective
Method Accuracy | Accuracy
EMOPIA (center) 0.722 0.850
EMOPIA (boundary) 0.528 0.655
EMOGEN (center) 0.675 0.658
EMOGEN (boundary) 0.300 0.251
emotion category. Besides, the area between the two curves
is much wider than that of EMOPIA, which indicates better

performance in differentiating samples from different emo-
tion categories. The above results both show that EMOGEN
can eliminate subjective bias from emotion labels.

@
1 @
a3
cy

mt ie

(a) EMOPIA (b) EMOGEN

Figure 2: T-SNE visualization of attribute vectors from
samples in EMOPIA and samples generated by EMOGEN.
”Qi” represents the i-th quadrant of Russell’s 4Q model.

4.4. Comprehensive Analysis

In this subsection, we conduct analysis experiments on dif-
ferent modules: 1) Emotion-to-attribute mapping methods;
2) Attribute design methods; 3) Top-k attributes. More
experiment implementation details can refer to Appendix
§B.3.

Emotion-to-Attribute Mapping Methods In the
emotion-to-attribute mapping stage, we need to determine
a set of attribute values as the mapped attribute values for
each emotion category, for which we consider the following
methods: 1) Closest: Directly using the attribute values of
the sample whose attribute values are closest to the average
attribute values of all the samples of the emotion. It is the
default method of EMOGEN. 2) Center: Directly using the
average attribute values of all the samples of the emotion
as the mapped attribute values; 3) K-Means: Clustering
the samples of each emotion with the K-Means clustering
algorithm (Lloyd, 1982) and selecting the attribute values

110 110
100 SF —=—_ 100

= inter-class 60 + inter-class
—— intra-class —— intra-ciass

ai a

Lt distance
\
\
Lt distance

a3 as ar a2 a3 as

2
Emotion Emotion

(a) EMOPIA (b) EMOGEN

Figure 3: Intra and inter L1 distance of attribute vectors from
samples in EMOPIA and samples generated by EMOGEN.
*Intra-class” means the average distance of attribute vectors
with the same emotional labels and ”inter-class” means that
with different emotional labels.

Table 4: Evaluation results of different emotion-to-attribute
mapping methods.

Subjective Objective

Method | Accuracy Humanness | Accuracy
Closest 0.714 3.64 0.658
Center 0.464 3.66 0.657
K-Means 0.607 3.71 0.565

of the center of the largest cluster as the mapped ones.

From the evaluation results shown in Table 4, we can see that
Closest achieves better subjective and objective accuracy
than Center and K-Means. Since Closest obtains attribute
values out of a real sample in the dataset, it can maintain the
original attribute distribution, and thus can achieve higher
accuracy. On the contrary, the attribute values obtained
by Center and K-Means are not from a real sample, so
the value distribution deviates from a real one, which may
result in poor control accuracy. As for the music quality,
although the humanness score of Center and K-Means is
higher than Closest, the difference is not significant. There-
fore, we choose Closest as the default mapping method in
the emotion-to-attribute mapping stage.

Attributes Design We compare altogether four alterna-
tives of the attribute design module: 1) Top-100: Using the
top-100 attributes according to feature importance, which is
the default method of EMOGEN; 2) Random: Selecting 100
attributes randomly according to attribute groups described
in §3.1. The details of how to select these attributes are de-
scribed in Appendix §B.3. 3) Manual: Following previous
work (Zheng et al., 2021; Tan & Herremans, 2020; McKay
et al., 2018; Kim et al., 2010), we use 17 manually designed
music attributes that are related to emotions.

The results are shown in Table 5. We can observe that:
1) EMOGEN (Top-100) improves subjective accuracy and

--- Page 8 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Table 5: Evaluation results of different attribute design meth-
ods.

Subjective Objective
Method | Accuracy Humanness | Accuracy
Top-100 0.785 3.73 0.658
Random 0.428 3.45 0.493
Manual 0.428 3.52 0.359

objective accuracy by more than 35.7% and 16.5% sepa-
rately compared with Random and Manual, which shows
better controllability for our attribute design methods. The
humanness score of EMOGEN is also superior to Random
and Manual, which demonstrates the better performance of
EMOGEN to generate high-quality music. 2) The subjec-
tive and objective emotion accuracy of Random is lower
than Top-100, which indicates that the randomly selected
attributes may not be enough to convey emotion-related
information. This is reasonable since without the design
to build the relationship between attributes and emotions,
it is hard for the model to be trained with the controlling
of emotional information. 3) The subjective and objective
accuracy of Manual is lower than Top-100, which indicates
that designing attributes through prior knowledge cannot
well model the relationship between emotions and music.
Therefore, we choose the top-100 attributes as the designed
attributes of EMOGEN.

Different Top-k Attributes We further analyze the in-
fluence of the number of designed attributes (i.e., k)
on the model performance. Specifically, we vary k in
(10, 50, 100, 300, 500) and evaluate the model with respect
to both controllability and music quality.

The evaluation results are shown in Table 6. We can observe
that: 1) Top-100 achieves the highest subjective accuracy.
With k increasing from 10 to 500, the subjective accuracy
increases first and then decreases, which indicates that more
attributes can help improve control accuracy, yet too many
can harm the controllability and this may be because they
have introduced more noises; 2) The objective accuracy of
top-300 and top-500 is higher than top-100. The reason may
be that a large number of attributes can cause the mapping
relationship to overfit the labeled datasets and let the model
generate music very similar to the ground truth, to which the
emotion classifier tends to give a more correct prediction.
Due to this matter, we believe that subjective accuracy is
more credible than objective one.

3) As k increases, the humanness score generally decreases,
which indicates that more attributes would cause lower mu-
sic quality. This is reasonable because if there are much
more attributes, it would be more difficult and more data-

Table 6: Evaluation results of different top-k attributes.

Subjective Objective

Top-k value | Accuracy Humanness | Accuracy
10 0.321 3.79 0.450
50 0.464 3.77 0.606
100 0.750 3.73 0.658
300 0.714 3.46 0.790
500 0.643 3.49 0.784

scarce for the generative model to learn the mapping from
the attributes to the corresponding music, and accordingly
cause the loss of music quality. However, top-100 can still
have relatively good quality. Combing the performances on
both subjective accuracy and music quality, we set k = 100
as the default value of EMOGEN.

4.5. Application on Multi-Instrument Datasets

To evaluate EMOGEN’s ability to generate emotional mu-
sic on the arbitrary dataset, we conduct experiments of
EMOGEN on TopMAGD (Ferraro & Lemstrém, 2018),
which is a multi-instrument dataset containing 22535 sam-
ples with no emotion annotations. Specifically, since the
mapped attributes in the emotion-to-attribute mapping stage
are determined, we only need to train the attribute-to-music
generation model on TopMAGD. We use subjective accu-
racy, humanness, and overall as subjective metrics. For de-
tails of the subjective experiment, please refer to Appendix
§B.4. We compare EMOGEN with the results in Table 2.

The subjective accuracy of EMOGEN on TopMAGD is
0.433, and the humanness and overall score are 3.72 + 0.17
and 3.67 + 0.15, respectively. We can observe that: Com-
pared with the results in Table 2, EMOGEN training on
TopMAGD performs better than CS and PUCT in $1 in con-
trol accuracy and music quality. In conclusion, EMOGEN
is able to generate music with desired emotion on multi-
instrumental datasets. Generated samples are available via
this link?.

5. Conclusion

In this paper, we propose EMOGEN, an emotional music
generation system that leverages a set of emotion-related
music attributes as the bridge between emotion and mu-
sic. EMOGEN divides emotional music generation into
two stages: in music-to-attribute mapping stage, EMOGEN
map the emotion label to attribute values that can represent
the general emotion by supervised clustering; in attribute-
to-music generation stage, EMOGEN train the generative
model via self-supervised learning without emotion labels.

Shttps://emo-gen.github.io/

--- Page 9 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

Benefiting from two stages, EMOGEN eliminates the sub-
jective bias in emotion labels, so as to achieve better control
accuracy. Experiment results show that EMOGEN is able
to generate music with better emotion control accuracy and
music quality compared to the previous methods.

In the future, we will consider improving or extending
EMOGEN in the following aspects: First, EMOGEN selects
the sample closest to the attribute center in the emotion-
to-attribute mapping stage, which may ignore the diversity
of emotions. It is worth exploring how to cluster attribute
vectors in fine-grained emotion classes to get more diverse
emotional mapping. Second, EMOGEN controls the mu-
sic generation with song-level attributes globally, we will
further explore how to control this process dynamically to
achieve emotion transitions between bar, phrase, and sec-
tion levels. Finally, we expect to extend EMOGEN to more
tasks and domains, such as emotion/style-controlled text
generation.

References

Bao, C. and Sun, Q. Generating music with emotions. IEEE
Transactions on Multimedia, 2022.

Chen, K., Wang, C., Berg-Kirkpatrick, T., and Dubnov,
S. Music sketchnet: Controllable music generation via
factorized representations of pitch and rhythm. In Pro-
ceedings of International Society for Music Information
Retrieval Conference (ISMIR), pp. 77-84, 2020.

Di, S., Jiang, Z., Liu, S., Wang, Z., Zhu, L., He, Z., Liu,
H., and Yan, S. Video background music generation
with controllable music transformer. In Proceedings of
ACM International Conference on Multimedia (MM), pp.
2037-2045, 2021.

Ferraro, A. and Lemstrém, K. On large-scale genre clas-
sification in symbolically encoded music by automatic
identification of repeating patterns. In Proceedings of
International Conference on Digital Libraries for Musi-
cology (DL{M), pp. 34-37, 2018.

Ferreira, L. and Whitehead, J. Learning to generate music
with sentiment. In Proceedings of International Society
for Music Information Retrieval Conference (ISMIR), pp.
384-390, 2019.

Ferreira, L. N., Lelis, L. H. S., and Whitehead, J. Computer-
generated music for tabletop role-playing games. In Pro-
ceedings of the Sixteenth AAAI Conference on Artificial
Intelligence and Interactive Digital Entertainment (AI-
IDE), pp. 59-65, 2020.

Ferreira, L. N., Mou, L., Whitehead, J., and Lelis, L. H. S.
Controlling perceived emotion in symbolic music gen-
eration with monte carlo tree search. In Proceedings of

AAAI Conference on Artificial Intelligence and Interac-
tive Digital Entertainment (AIDE), pp. 163-170, 2022.

Grekow, J. and Dimitrova-Grekow, T. Monophonic music
generation with a given emotion using conditional vari-
ational autoencoder. [EEE Access, 9:129088—129101,
2021.

Hernandez-Olivan, C. and Beltran, J. R. Music composition
with deep learning: A review. Advances in Speech and
Music Technology: Computational Aspects and Applica-
tions, pp. 25-50, 2022.

Ho, T. K. Random decision forests. In Proceedings of Inter-
national Conference on Document Analysis and Recogni-
tion (ICDAR), pp. 278-282, 1995.

Hsiao, W.-Y., Liu, J.-Y., Yeh, Y.-C., and Yang, Y.-H. Com-
pound word transformer: Learning to compose full-song
music over dynamic directed hypergraphs. In Proceed-
ings of AAAI Conference on Artificial Intelligence (AAAI),
volume 35, pp. 178-186, 2021.

Huang, Y.-S. and Yang, Y.-H. Pop music transformer: Beat-
based modeling and generation of expressive pop piano
compositions. In Proceedings of ACM International Con-
ference on Multimedia (MM), pp. 1180-1188, 2020.

Hung, H., Ching, J., Doh, S., Kim, N., Nam, J., and Yang, Y.
EMOPIA: A multi-modal pop piano dataset for emotion
recognition and emotion-based music generation. In Pro-
ceedings of International Society for Music Information
Retrieval Conference (ISMIR), pp. 318-325, 2021.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In International Conference on
Machine Learning, pp. 5156-5165. PMLR, 2020.

Kawai, L., Esling, P., and Harada, T. Attributes-aware deep
music transformation. In Proceedings of International
Society for Music Information Retrieval Conference (IS-
MIR), pp. 670-677, 2020.

Kim, Y. E., Schmidt, E. M., Migneco, R., Morton, B. G.,
Richardson, P., Scott, J., Speck, J. A., and Turnbull, D.
Music emotion recognition: A state of the art review. In
Proc. ismir, volume 86, pp. 937-952, 2010.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In Proceedings of International Conference
on Learning Representations (ICLR), 2015.

Lloyd, S. Least squares quantization in pcm. IEEE transac-
tions on information theory, 28(2):129-137, 1982.

Madhok, R., Goel, S., and Garg, S. Sentimozart: Music
generation based on emotions. In Proceedings of Inter-

national Conference on Agents and Artificial Intelligence
(ICAART), pp. 501-506, 2018.

--- Page 10 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

McKay, C., Cumming, J., and Fujinaga, I. JSYMBOLIC
2.2: Extracting features from symbolic music for use
in musicological and MIR research. In Proceedings of
International Society for Music Information Retrieval
Conference (ISMIR), pp. 348-354, 2018.

Neves, P., Fornari, J., and Florindo, J. Generating music
with sentiment using transformer-gans. arXiv preprint
arXiv:2212.11134, 2022.

Pangestu, M. A. and Suyanto, S. Generating music with
emotion using transformer. In Proceedings of Interna-
tional Conference on Computer Science and Engineering
(IC2SE), volume 1, pp. 1-6, 2021.

Raffel, C. Learning-Based Methods for Comparing Se-
quences, with Applications to Audio-to-MIDI Alignment
and Matching. PhD thesis, Columbia University, 2016.

Russell, J. A. A circumplex model of affect. Journal of
personality and social psychology, 39(6):1161, 1980.

Shih, Y.-J., Wu, S.-L., Zalkow, F., Muller, M., and Yang,
Y.-H. Theme transformer: Symbolic music generation
with theme-conditioned transformer. [EEE Transactions
on Multimedia, 2022.

Sulun, S., Davies, M. E. P., and Viana, P. Symbolic music
generation conditioned on continuous-valued emotions.
IEEE Access, 10:44617-44626, 2022.

Tan, H. H. and Herremans, D. Music fadernets: Controllable
music generation based on high-level features via low-
level feature modelling. In Proceedings of International
Society for Music Information Retrieval Conference (IS-
MIR), pp. 109-116, 2020.

Tseng, B., Shen, Y., and Chi, T. Extending music based
on emotion and tonality via generative adversarial net-
work. In Proceedings of IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
pp. 86-90, 2021.

von Riitte, D., Biggio, L., Kilcher, Y., and Hoffman, T.
Figaro: Generating symbolic music with fine-grained
artistic control. arXiv preprint arXiv:2201.10936, 2022.

Wu, S.-L. and Yang, Y.-H. Musemorphose: Full-song and
fine-grained music style transfer with one transformer
vae. arXiv preprint arXiv:2105.04090, 2021.

Yu, B., Lu, P., Wang, R., Hu, W., Tan, X., Ye, W., Zhang, S.,
Qin, T., and Liu, T.-Y. Museformer: Transformer with
fine-and coarse-grained attention for music generation.
arXiv preprint arXiv:2210.10349, 2022.

Zhao, J., Xia, G., and Wang, Y. Domain adversarial train-
ing on conditional variational auto-encoder for control-
lable music generation. arXiv preprint arXiv:2209.07144,
2022.

Zheng, K., Meng, R., Zheng, C., Li, X., Sang, J., Cai, J., and
Wang, J. Emotionbox: a music-element-driven emotional
music generation system using recurrent neural network.
arXiv preprint arXiv:2112.08561, 2021.

--- Page 11 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

A. Selected Attributes List

We extract 1495 music attributes from jSymbolic. The definition of these music attributes can be found at https: //jmir.
sourceforge.net/manuals/jSymbolic_manual/home.html. We first train a Random Forest classifier on
EMOPIA, then select 100 attributes according to their feature importance. The first 10 attributes are shown in Table 7.

Table 7: The first ten selected attributes.

Note Density per Quarter Note

Note Density per Quarter Note Variability
Total Number of Notes

Relative Note Density of Highest Line
Prevalence of Long Rhythmic Values
Prevalence of Very Long Rhythmic Values
Average Note to Note Change in Dynamics
Pitch Class Histogram_8

Rhythmic Value Histogram_10

Vertical Interval Histogram_43

For more details about selected attributes, please refer to https: //emo-gen.github.io/.

B. Details of Experiments
B.1. Comparison with Previous Methods

We invite 15 participants to evaluate 32 songs which consist of 4 emotion categories for each setting and method. The
participant needs to rate music samples on a five-point scale with respect to 1) Valence: Is the music piece negative or
positive; 2) Arousal: Is the music piece low or high in arousal; 3) Humanness: How similar it sounds to a piece composed
by a human; 4) Overall: An overall score. For objective metrics, we apply the emotion classifier to classify the generated
1000 samples for each method, then we calculate objective accuracy by comparing the emotion input for generating music
with the predicted emotion class by this classifier.

Table 8: Detail results of subjective evaluation. Metrics hy, lv, ha, la stand for high valence, low valence, high arousal, and
low arousal respectively. For all metrics, we report mean opinion scores and 95% confidence interval.

Setting Method hvt Iv} hat lal
EMOPIA Ground truth 3.83 + 0.40 2.83 + 0.53 4.13 + 0.34 2.53 + 0.42
cs 3.00 + 0.44 3.23 + 0.40 3.9 £0.31 2.93 + 0.43

S1: Pre-training on Popl1k7+LMD-Piano,

fine-tuning on EMOPIA PUCT 3.20 + 0.35 3.20 + 0.39 3.40 + 0.40 3.07 + 0.41

EMOGEN 3.4340.44 2.40+0.44 4.274+0.19 1.77+40.31

$2: Training on Pop1k7+LMD-Piano+EMOPIA EMOGEN 3.30 + 0.47 2.57 + 0.48 4.17 + 0.29 1.90 + 0.35

To further compare the results of each method, we calculated the average valence and arousal scores, respectively. Detail
evaluation results for valence and arousal are shown in Table 8. As we can see:

1. In $1, EMOGEN outperforms CS and PUCT in hv,lv,ha and la. Thus, EMOGEN controls emotion better than CS and
PUCT under this setting.

2. EMOGEN in Setting 2 controls emotion better than CS and PUCT in Setting 1. Therefore, benefiting from the two-stage
framework, EMOGEN can achieve good performance in arbitrary datasets with no annotations.

--- Page 12 ---
EmoGen: Eliminating Subjective Bias in Emotional Music Generation

B.2. Verification on Eliminating Subjective Bias

Considering that each emotion in EMOPIA contains approximately 250 samples, we select 50 samples from the center and
boundary respectively for each emotion category. And for EMOGEN, we generate 1000 samples for the center and boundary
separately. Each emotion category contains 250 samples. We invite 10 participants to classify the music samples into 4
emotional quadrants according to Russell’s 4Q model(Russell, 1980). Each participant evaluates 16 samples randomly
sampled from 1) Center and boundary of EMOPIA; 2) Center and boundary generated by EMOGEN. Samples are evenly
distributed in four emotion categories. For objective metrics, we apply the emotion classifier to predict the emotion label of
samples from EMOPIA and EMOGEN, then the objective accuracy can be obtained.

B.3. Comprehensive Analysis

For each compared module and method, we generate 1000 samples with 250 for each emotion category. Then we invite 7
participants to 1) Classify the sample into four emotional categories based on Russell’s 4Q model; 2) Rate the humanness
score of the sample on a five-point scale. The higher the score is, the more realistic the sample is to a human-composed
one. Each participant receives 44 samples evenly distributed in 4 emotion categories, which are divided into 3 groups: 1)
Group for 3 compared methods in emotion-to-attribute mapping, which consists of 12 samples; 2) Group for 3 compared
methods in attribute design, which consists of 12 samples; 3) Group for 5 values of top-k, which consists of 20 samples. For
objective metrics, we apply the emotion classifier to predict the emotion labels of 1000 generated samples and objective
accuracy is calculated similarly to Appendix §B.1.

Details of Attribute Design We present details about another two attribute design methods in Table 5:

1. Random: Since jSymbolic divides attributes into seven groups (please refer to https: //jmir.sourceforge.
net/manuals/jSymbolic_manual/home.htm1 for detail), we select 100 attributes on average according to
the 7 groups.

2. Manual: Following previous work (Zheng et al., 2021; Tan & Herremans, 2020; McKay et al., 2018; Kim et al., 2010),
we choose pitch class histogram, note density, rhythm density, and mean pitch/duration/velocity as manually designed
attributes related to music emotion, which form a 17-dimensional attribute vector.

B.4. Application on Multi-Instrument Datasets

We keep the emotion-to-attribute mapping stage unchanged and train the attribute-to-music generation stage on TopMAGD.
We apply EMOGEN to generate 1000 samples with 250 for each emotion category. And we invite 15 participants to rate each
sample by Valence, Arousal, Humanness, and Overall score similar to §B.1. Each participant evaluates 4 samples which
consist of 4 emotion categories. Then we report subjective accuracy (the same as the calculation rule in §B.1), humanness,
and overall score as subjective evaluation metrics.

B.5. Discussion on Output Diversity

EMOGEN uses only one set of attributes to represent each emotion, which will limit the diversity. The emotion of music
contains two levels, one is the emotion felt by most people (i.e., general emotions), and the other is the emotion felt by
individuals (i.e., personalized emotions). General emotions are not as diverse as personalized emotions. In this paper, we
mainly consider controlling the music generation with general emotions not influenced by subjective bias. However, if
needed, EmoGen can also achieve more emotion diversity by mapping other emotions into more sets of attribute values in
the emotion-to-attribute mapping stage.

