Make-A-Protagonist: Generic Video Editing with Visual and Textual Clues
Yuyang Zhao¹, Enze Xie², Lanqing Hong¹, Zhenguo Li³, Gim Hee Lee¹
3
1 National University of Singapore
2 The University of Hong Kong
The Hong Kong University of Science and Technology
https://make-a-protagonist.github.io
arXiv:2305.08850v2 [cs.CV] 19 Feb 2024
T2V Editing with
Protagonist
Background
Editing
Protagonist
Editing
CRED
Reference
Image
திய
Source
Video
இயம்
A Suzuki Jimny driving down a mountain road
A <car> driving down a mountain road
A < Suzuki Jimny > driving down a mountain road at sunset
A <car> driving down a road in the snow
Figure 1. Three applications of Make-A-Protagonist. <car> is the protagonist in the video. Bold blue denotes the textual description.
Abstract
The text-driven image and video diffusion models have
achieved unprecedented success in generating realistic and
diverse content. Recently, the editing and variation of exist-
ing images and videos in diffusion-based generative mod-
els have garnered significant attention. However, previous
works are limited to editing content with text or provid-
ing coarse personalization using a single visual clue, ren-
dering them unsuitable for indescribable content that re-
quires fine-grained and detailed control. In this regard, we
propose a generic video editing framework called Make-A-
Protagonist, which utilizes textual and visual clues to edit
videos with the goal of empowering individuals to become
the protagonists. Specifically, we design a visual-textual-
based video generation model coupled with a mask-guided
fusion method to integrate source video, target visual and
textual clues. Extensive results demonstrate the versatile
and remarkable editing capabilities of Make-A-Protagonist.
1. Introduction
"The protagonist sets the scene for the entire
story."
Ben Okri [24]
Diffusion-based generative models have demonstrated
remarkable success in generating photorealistic and diverse
images [27, 28, 31] and videos [34, 38, 41] conditioned on
text. However, the generation process, while more diverse,
lacks controllability when relying solely on text descrip-
tions. To generate the desired content, researchers explore
two approaches to modify text-conditioned generation: in-
corporating external control signals [5, 13, 23, 40] and edit-
ing existing content with textual information [8, 19, 21, 25].
However, both approaches rely on text descriptions to con-
vey the desired content, which raises an important question:
what if the content cannot be accurately described through
text? As an illustration, we consider the source video shown
in Figure 1. Previous studies [19, 25] are capable of replac-
ing the "Suzuki Jimny" with a recognizable brand, such as
"Porsche" or "Mercedes-Benz” since these options are iden-
tifiable by the text model. Nevertheless, they are unable to
substitute it with an unnamed car. For example, the refer-
ence image in 2rd row of Fig. 1 is generated by a text-to-
image model with the text prompt “a black car”, so it does
not have a corresponding name that can be precisely recog-
nized by the language model.
In contrast, people have the strong motivation to person-
alize their own content and to change the protagonist, which
may not be accurately described through text. This motiva-
tion has served as inspiration for the development of per-
sonalized models [22, 30] and image/video variation tech-
niques [5, 27]. Nevertheless, personalized models necessi-
tate fine-tuning for each specific target using multiple target
images and exhibit sensitivity to hyperparameters and train-
ing configuration. Furthermore, image and video variation
models tend to exhibit bias toward the background of the
reference images. Additionally, both types of generation
models are typically limited to generating content based
on a single visual reference clue, which proves ineffective
when multiple protagonists need to be modified. These lim-
itations motivate us to design a framework for generic video
editing with text and an arbitrary number of reference im-
ages.
One recent work [38] can be applied to generic video
editing by combining personalized modeling [30] with one-
shot video editing. Specifically, this work first fine-tunes
a text-to-image generation (T2I) model with the reference
image. After that, it inflates the T2I model with tempo-
ral layers and optimizes it on the source video. However,
the image-video tuning process has a severe limitation: the
two models should be retrained when changing the refer-
ence image. In addition, it also inherits the issues of per-
sonalized modeling, sensitive to hyperparameters. To this
end, this paper disentangles the personally edited content
(protagonists) from the source video to realize end-to-end
one-stage generic video editing.
To achieve generic video editing, the framework neces-
sitates the ability of leveraging visual clues, leveraging tex-
tual clues and maintaining source motion. Therefore, we
build our framework based on Stable UnCLIP *, which
takes CLIP image and text embeddings as conditions. The
image embedding is directly added to the model features
while text embedding is utilized via cross-attention. Fol-
lowing [34, 38, 41], we inflate the T2I model (Stable Un-
CLIP) to one-shot text-to-video generation (T2V) model
by including additional temporal modules. After tuning
the T2V model with the source video, we propose a novel
mask-guided fusion to incorporate the visual clue, textual
clue and source motion with the mask of the protagonist of
the source video. The mask is obtained from pre-trained
models [4, 16, 18], eliminating the need for data anno-
tation. With the diffusion-based T2V model and effec-
tive mask-guided fusion method, our framework (Make-
A-Protagonist) achieves commendable performance in both
conventional and novel video editing tasks, including pro-
tagonist editing while keeping the background, background
editing of the source content, and text-to-video editing with
the protagonist. Our contributions can be summarized as:
• We present the first end-to-end framework for generic
video editing with both visual and textual clues.
• We design a visual-textual-based video generation model
and a novel mask-guided fusion to incorporate the vi-
sual clue, textual clue and source motion, realizing strong
video editing performance.
• Extensive results demonstrate the versatile applications
of Make-A-Protagonist and the superiority over previous
video editing works.
2. Related Work
Visual Content Generation. Content generation has made
remarkable advancements with powerful generative mod-
els [14, 37, 39]. Recently, equipped with vast and diverse
image-text pairs from the internet, diffusion-based gener-
ative models [27, 28, 31] have outperformed GAN-based
methods in the text-to-image generation (T2I). In view of
the impressive performance in T2I, text-to-video genera-
tion (T2V) [11, 12, 20, 34, 41] has attracted much atten-
tion. T2V models often utilize pre-trained T2I models to
leverage the abundant image-text resources. Additionally,
temporal modules [34, 41] are introduced to facilitate video
representation learning. Furthermore, temporal consistency
*https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip-small
Source Video
Inverted Latents
"A Suzuki Jimny driving down a mountain road"
Attention
Мар
CLIP
Vision
Inverted Latents
Source
Key Frame
CLIP
Image Embed
Attention
Map
Reference
Image
CLIP
Text Embed
ResBlock
Self-Attn
Output
Cross-Attn
Mask-Guided Fusion
ControlNet
"A car driving down a road in the snow"
Figure 2. The overall inference framework of Make-A-Protagonist. Masks and control signals are extracted from the source video. Source
video, visual and textual clues are fused in the video generation model with the mask-guided fusion to enable generic video editing.
is also established in latent space to enhance video genera-
tion [15, 20].
Visual Content Editing and Variation with Text. An al-
ternative direction for content generation is the editing of
existing images [2, 8, 21, 36] and videos [1, 19, 25, 33, 38]
using text, instead of uncontrolled generation solely based
on text descriptions. SDEdit [21] applies noise to the im-
age and recovers the image for editing purposes. Prompt-
to-prompt [8] and Plug-and-Play [36] modify the cross-
attention map by changing the text description. For video
editing, Text2Live [1] divides the video into layers and ed-
its each layer separately using a text description. Tune-
A-Video [38] inflates and fine-tunes the T2I model on a
single video and generates new videos of similar motion.
Video-P2P [19] and FateZero [25] extend the Prompt-to-
prompt into video level. TokenFlow [7] edits several key
frames with prompt-to-prompt and then propagates them to
the whole video via feature matching. However, previous
methods solely focus on content editing through text, mak-
ing them unsuitable for indescribable content.
Visual Content Variation and Personalization. To ad-
dress the indescribable content, DALL-E 2 [27] and Gen-
1 [5] perform image and video variation using CLIP [26]
image embedding. However, since the CLIP vision model
extracts information from the whole image, background in-
formation is inevitably incorporated into the variation. Per-
sonalized models [6, 22, 30] present an alternative approach
for handling indescribable content. However, Dream-
Booth [30] and DreamMix [22] necessitate multiple images
to fine-tune a T21 and T2V model for concept learning, and
the fine-tuning process is sensitive to training configuration.
Considering the limitations of previous works, this paper
presents a framework for generic video editing with both
image and text descriptions.
3. Make-A-Protagonist
3.1. Overview
To realize generic video editing with both visual and tex-
tual clues, we introduce a video generation model that can
take both CLIP image embedding and CLIP text embedding
as conditions (Sec. 3.2). In addition, since the CLIP image
embedding is directly added to the features, it lacks spatial
control compared with the cross-attention used by text em-
bedding. Therefore, we propose a mask-guided fusion tech-
nique, leveraging the mask of the protagonist in the source
video to accurately control the spatial location during in-
ference (Sec. 3.3). Furthermore, we introduce attention fu-
sion and ControlNet to precisely control the semantic and
spatial information of the background and protagonist, re-
spectively. Equipped with the video generation model and
mask-guided fusion, our framework achieves three applica-
tions (Sec. 3.4). The overall framework is depicted in Fig. 2.
3.2. Visual-Textual-based Video Generation Model
Latent Diffusion Models (LDMs) with Image Embed-
ding. LDMs consist of two main components: an autoen-
coder that encodes an RGB image x into a latent z = E(x)
with the encoder E, and a decoder D that reconstructs the
image x D(z) from z. In the latent space, a U-Net [29] ε0
containing residual blocks and self-/cross-attention is used
Source Video
Encoder
Diffusion
Source Flow
Text Flow
Reference Flow
A Suzuki Jimny driving
down a mountain road
CLIP
Text
CLIP
Vision
Reconstruction Loss
ENLIER
Output
Reference
Image
Figure 3. Training process of visual-textual-based video generation model. The model is trained with the video caption and randomly
sampled reference frame.
to remove the noise with the objective:
min Ezo,e,t√√at ε - √√1 - αt · 20
€0 (zt, t, C, I)||2,
(1)
where C and I denote the embedding of the text and image,
respectively. Noise ε is added to zo according to step t to
obtain zt. Note that instead of directly predicting the noise
Є [10], following [5, 11], we use v-parameterization [32]
to improve color consistency of videos. Regarding text and
image conditions, the text condition serves as the key and
value for cross-attention while the image condition is di-
rectly added to features of residual blocks, which aligns
with [28] and [27], respectively.
DDIM Inversion. Deterministic DDIM sampling is used
to generate each frame from a latent noise in T denoising
steps:
-
Zt-1 = √at-1 (√√ất Zt
+ at-1 (√√at ε0 + √√Ī
at εe)
at Zt),
where t: 1 →T denotes the timestamp, at is a param-
eter for noise scheduling [35]. Same with Eq. 1, we use
v-parameterization for denoising.
Since Make-A-Protagonist focuses on video editing, in-
stead of using random noise during inference, we use DDIM
inversion [35] to transform the source video into the initial
latent code that requires denoising. This approach enables
the preservation of both the motion and structural informa-
tion presented in the source video, while also enhancing the
temporal consistency within the latent space. The DDIM
inversion is illustrated as:
Zt+1 =
√α++1 (√αt zt
- at εe)
(3)
+ √√1 - αt+1 (√√αt ε0 + √1 − αt Zt).
Video Generation Model. In this paper, we propose a
visual-textual video generation model based on LDMs (La-
tent Difference Models) to integrate source video, visual
and textual clues to achieve generic video editing. The
training process of our video generation model is depicted
in Fig. 3. Following [34, 38, 41], we initialize our model
with text-to-image LDMs and we modify the U-Net to cap-
ture and exploit temporal correlations via temporal convolu-
tion, temporal attention, and spatio-temporal attention [38].
Our video generation model takes both textual and visual
clues as conditioning inputs. To obtain the textual clue,
we employ BLIP-2 [17] to extract the video caption. The
caption is then transformed into a text embedding using the
CLIP Text Model [26], which serves as the key and value in
the cross-attention mechanism within the U-Net. To ensure
the ability to incorporate image embedding, we randomly
select one frame from the video as the reference frame at
each iteration. The reference frame is encoded using CLIP
Vision Model [26] and added to residual blocks in U-Net.
Note that text and image embeddings are applied to all the
blocks in the U-Net.
3.3. Mask-Guided Fusion
Source Video Masks. Since the visual clues (CLIP image
embedding) are added directly to the intermediate features
in the residual blocks instead of operations with spatial in-
formation (e.g., cross-attention), the model cannot precisely
control the location of the reference object. Therefore, we
leverage the masks of the protagonist in the source video
to separate foreground and background for spatial control.
Specifically, we first extract the protagonist mask in the first
frame with Segment Anything [16]. Then XMem [4] is
adopted to track the mask across the whole video.
Overall Fusion Process. During the inference stage, the
video generation model employs the DDIM inversed latent
code as a starting point to denoise the code using the target
textual and visual clues, as indicated by Eq. 2. Since the
CLIP Vision Model [26] encodes the whole reference im-
age, the background in the reference image is inevitably in-
troduced into the generated result. To address this, we mask
out the desired reference part via Grounded-SAM [16, 18].
However, when attempting to change the background of the
source video, e.g. "in the desert” in Fig. 2, the editing pro-
cess may not yield satisfactory results due to the masked
reference image. The reason behind this is that the refer-
ence image embedding, which is directly added to the fea-
tures, exerts a more dominant influence compared to the
textual clues employed by the cross-attention mechanism.
Therefore, we incorporate the DALL-E 2 Prior [27], capa-
ble of converting text embeddings into image embeddings,
to enhance the representation of textual clues. The prior em-
bedding and reference image embedding are fused together
with the source masks.
Despite splitting the protagonist and background with
the source masks, the protagonist and background may not
be consistent across the video as the spatial information in-
troduced by the source masks is quite coarse. Instead, self-
attention maps can serve as good guidance for spatial lo-
cation in each frame, and the spatial locations across the
source video are consistent. Therefore, we can seek the
background self-attention maps from the source video and
use these maps to extract information from the target back-
ground. The attention fusion technique can improve the rep-
resentation of the target background while keeping a similar
temporal consistency as the source video. As for the protag-
onist part, we additionally leverage ControlNet [40] to pro-
vide more precise spatial control. Feature fusion, attention
fusion, and ControlNet are introduced as follows.
Feature Fusion. Given the latent feature z Є
RFXCXHXW in residual block, the image embedding I E
R1×C is added to all frames and spatial location. We lever-
age the video protagonist masks MЄ RFXHXW to fuse
the reference image embedding ZË E R¹×C and prior con-
verted text embedding Ip = R¹×C:
2t
=
F
Zt
+ MIR+ (1 - M) · Tp.
.
(4)
In addition, since IR represent the information of the
protagonist, we introduce a timestamp parameter TF to con-
trol the ending step of the feature fusion operation:
=
√ z²² + M · LR + (1 − M) · Lp
F
Z+IR
-
ift<TF
otherwise.
(5)
Since the spatial location is determined in the early steps
of the denoising process while details are refined in the lat-
ter stages, introducing TF can lead the model to contain
more detailed information about the reference image.
Attention Fusion. For the self-attention latent feature
ZA Є RFXCXHXW, frame-wise self-attention maps At €
RFXHW×HW
are calculated from the query and key pro-
jected features Q(z) and K(ZA). We leverage the video
protagonist masks MЄ RFXHXW to fuse these self-
attention maps with the source video self-attention maps A
obtained during DDIM inversion:
At = M · At + (1 − M) · A¾.
(6)
Similarly, a timestamp parameter TA is introduced to
control the ending step of the attention fusion operation:
At
SA₁ = M · At + (1 − M) · A§ ift <TA
(7)
=
At
otherwise.
ControlNet. Source masks are coarse spatial constraint that
applies the same information within them. Incorporating
visual clues is difficult and such coarse constraint cannot
lead to satisfactory results in most cases. Therefore, we ad-
ditionally use a ControlNet [40] to enhance the spatial re-
quirements within the mask. Specifically, the ControlNet
branch takes one type of condition (pose or depth in this
paper) together with the CLIP image embedding of the ref-
erence image. Then the ControlNet features are masked out
by the source video masks and added to intermediate fea-
tures in the diffusion U-Net. Equipped with feature fusion,
attention fusion and ControlNet, Make-A-Protagonist can
generate consistent video with both visual and textual clues.
3.4. Applications of Make-A-Protagonist
We divide a video into background and protagonist, and
Make-A-Protagonist can edit either or both of them. Fig. 1
illustrates three applications supported by our method. In
this section, we denote the reference image embedding IR
and the prior converted text embedding Ip in Eq. 5 as pro-
tagonist embedding and background embedding.
Protagonist Editing: In this application (2nd row in Fig.1),
the background of the edited video should remain the same
as the source video. Thus, instead of employing DALL-
E 2 Prior [27] to convert the text into background embed-
ding, we utilize the CLIP Vision Model [26] to encode each
frame of the source video as the background embedding
(Ip), while the image embedding from the reference image
serves as the protagonist embedding (IR).
Background Editing: To edit the background while pre-
serving the protagonist (3rd row in Fig. 1), we encode the
frames of the source videos as the protagonist embedding
IR, and the text describing the background is converted to
background embedding Ip by DALL-E 2 Prior [27].
Text-to-Video Editing with Protagonist: By leveraging a
reference image and text, both the background and protago-
nist can be edited simultaneously (4th row in Fig. 1). In this
application, the reference image is encoded as the protago-
nist embedding (IR) using the CLIP Vision Model [26], and
the text is transformed into background embedding (Ip) us-
ing DALL-E 2 Prior [27].
DreamBooth-V
Gen-1
Ours
Tune-A-Video
Source Video
FateZero
L
L
Cuma
A man playing basketball
A Suzuki Jimny driving down a mountain road
A man playing basketball on the beach, anime style
A man playing basketball on the beach, anime style
No Prompt
LE
LB
www.
A car moving in the desert
A car moving in the desert
No Prompt
A [V] playing basketball on the beach, anime style
A [V] moving in the desert
A man playing basketball on the beach, anime style
A car moving in the desert
Figure 4. Comparison with other video generation and editing methods. Make-A-Protagonist can change the protagonist with the visual
clue while editing the background with the textual clue.
4. Experiments
4.1. Implementation Details
Our video generation model is based on text-to-image (T2I)
latent diffusion models with image embedding [28] (Stable
UnCLIP). After initializing with the T2I LDM and inserting
the temporal modules into the model, the video generation
model is fine-tuned on 8 frames from a video at a resolution
of 768 x 768. The model is trained for 200 steps with a
learning rate of 3 × 10−5. During inference, we use DDIM
sampler [35] with classifier-free guidance [9] for 20 steps.
It takes about 10 minutes to train a single video and 30 sec-
onds for inference.
4.2. Comparison with Baselines
Baselines. Since there are no previous methods conduct-
ing generic video editing, we compare Make-A-Protagonist
with text-based editing [25, 38], video variation [5], and
Ours
Table 1. Quantitative evaluation. Make-A-Protagonist achieves comparable model evaluation with DreamBooth-V while overwhelming
preference in the user study.
Source Video
Model Evaluation
CLIP-T↑ DINO ↑
User Study
Quality ↑ Subject ↑ Prompt ↑
Make-A-Protagonist
DreamBooth-V [30, 38]
Both Equally
0.337
0.485
71.6%
67.2%
69.8%
0.301
0.509
21.0%
21.2%
12.4%
7.4%
11.6%
17.8%
A man riding a dirt bike through a river
A <man> riding a <motorbike> on the beach, anime style
Figure 5. Example of editing two protagonists in one video.
personalized [30] methods: (1) Tune-A-Video [38] fine-
tunes an inflated LDM on a single video to generate related
content. (2) FateZero [25] edits an existing video by con-
trolling the cross-attention maps. (3) Gen-1 [5] uses refer-
ence images or text to change the source video. Since Gen-1
can only use one condition (image or text), we use the refer-
ence image without text input. (4) DreamBooth-V [30, 38]
is a baseline with a similar application as ours, combining
DreamBooth [30] and Tune-A-Video [38]. Specifically, we
first train a personalized DreamBooth model with one ref-
erence image and the corresponding text token [V] and then
use this personalized model as the initialization for single
video fine-tuning [38]. During inference, text token [V] can
be used to generate content with the protagonist.
Qualitative Evaluation. We compare the baselines on two
videos in Fig 4. First, text-based editing methods [25, 38]
cannot integrate the visual clues, limiting their capability
to generate indescribable protagonists. Second, Gen-1 [5]
cannot use both visual and textual clues simultaneously for
generic editing. Moreover, Gen-1 [5] incorporates the back-
ground information from the reference image in the gener-
ated results. Third, DreamBooth-V [30, 38] can perform
generic video editing as Make-A-Protagonist. However,
one reference image is not enough to learn a good Dream-
Booth model for complex protagonists (e.g., the man in the
left video) and thus DreamBooth-V struggles to generate
realistic videos. Furthermore, even though the personalized
model is well-trained, it requires a separate model for each
protagonist with a careful selection of training configura-
tions. In addition, DreamBooth-V is limited to editing one
protagonist in a video while Make-A-Protagonist demon-
strates the ability to edit multiple protagonists in Fig. 5.
Quantitative Evaluation. We conduct a quantitative com-
parison with DreamBooth-V [30, 38] on generic video edit-
ing. Following [30], prompt fidelity is measured by the av-
erage cosine similarity between the prompt and each video
frame ViT-B/32 CLIP [26] embeddings, denoted as CLIP-
T. Subject fidelity is measured by the average cosine simi-
larity between the ViT-S/16 DINO [3] embeddings of each
generated video frame and the reference image, denoted as
DINO. In addition, we conduct a user study to compare the
two methods in terms of video quality, subject fidelity, and
prompt fidelity. We asked 20 users to answer questionnaires
of 25 comparative questions. Each comparative question
shows the reference image, text prompt, and videos gen-
erated by the two methods in random order. For video
quality, users are asked to answer the question: "Which
of the two videos is more natural and realistic?", and we
include a "Cannot Determine/Both Equally" option. Sim-
ilarly, we ask "Which of the two videos better reproduces
the identity (e.g. item type and details) of the reference im-
age?" and "Which of the two videos is better described
by the text? (No need to take the reference image into ac-
count)" for subject fidelity and prompt fidelity, respectively.
As shown in Tab. 1, Make-A-Protagonist achieves a bet-
ter prompt fidelity evaluated by CLIP while DreamBooth-V
Reference Image
A <car> driving down a track at night
(a) Source Video
(c) w.o. DALL-E 2 Prior
(e) w.o. Attention Fusion
(b) w.o. ControlNet
(d) w.o. Feature Fusion
(f) Ours
Figure 6. Qualitative ablation studies. Removing the proposed components impairs the subject fidelity, prompt fidelity and video quality.
Table 2. Quantitative ablation studies.
CLIP-T↑
DINO ↑
w.o ControlNet [40]
0.324
0.482
w.o. DALL-E 2 Prior [27]
0.297
0.490
w.o. Feature Fusion
0.293
0.472
w.o. Attention Fusion
0.335
0.479
0.337
0.485
Make-A-Protagonist
obtains a higher DINO score. However, we conjecture that
the DINO score does not fully reflect the generated quality,
since DreamBooth-V tends to overfit to the reference im-
age. The overfitting leads to less realistic generated results
but a higher DINO score (refer to qualitative comparison in
the supplementary material). Regarding the user study, we
find an overwhelming preference for Make-A-Protagonist
in terms of video quality, subject fidelity, and prompt fi-
delity, demonstrating the superiority of our method.
4.3. Ablation Studies
To verify the effectiveness of each component in Make-A-
Protagonist, we conduct ablation studies on the Control-
Net [40], DALL-E 2 Prior [27], feature fusion and attention
fusion both quantitatively (Tab. 2) and qualitatively (Fig. 6).
ControlNet. Removing ControlNet can preserve the pro-
tagonist information but the motion is not consistent. In
the first row of Tab. 2, removing ControlNet slightly influ-
ences the subject fidelity (DINO score) but impairs prompt
fidelity (CLIP-T score). In addition, due to lacking precise
spatial location for the protagonist, the orientation of the car
is wrong in the first several frames (Fig. 6(b)).
DALL-E 2 Prior. Deactivating DALL-E 2 Prior results
in the text embedding being solely used in cross-attention.
Consequently, the textual clue becomes weaker compared
to the visual clue, leading to generating results that fail to
align with the text description. Instead, since the CLIP im-
age embedding for the background is zero, it would not in-
fluence the representation of the reference image. There-
fore, the image similarity is slightly increased.
Feature Fusion. Feature fusion is crucial for maintain-
ing the visual information, resulting in a 0.013 improve-
ment in DINO score. Furthermore, removing feature fusion
equals to integrating the CLIP image embedding of the ref-
erence image to the whole frames, and the DALL-E 2 Prior
embedding is also excluded. Thus, the prompt fidelity is
poor when deactivating feature fusion, which is also demon-
strated in Fig. 6(d).
Attention Fusion. Attention fusion aims to improve the
temporal consistency by providing spatial information for
the background. Comparing Fig. 6(e) and Fig. 6(f), atten-
tion fusion can address the inconsistent lane markings. With
the integration of DALL-E 2 Prior and mask-guided fusion,
Make-A-Protagonist showcases remarkable performance in
both quantitative and qualitative assessments.
5. Conclusion
This paper introduces Make-A-Protagonist, the first end-to-
end framework for generic video editing using textual and
visual clues. To edit the protagonist and background with
these clues, we design a visual-textual-based video gener-
ation model coupled with a mask-guided fusion method to
integrate diverse information sources. The fusion approach
effectively employs the masks, control signals and attention
maps from the source video to provide precise spatial lo-
cations for editing both protagonist and background. By
combining the video generation model with this fusion ap-
proach, Make-A-Protagonist empowers versatile and pow-
erful generic video editing applications, including back-
ground editing, protagonist editing, and text-to-video edit-
ing with the protagonist.
Limitations & Social Impacts. One limitation of our ap-
proach is that representing visual clues with CLIP image
embedding may not be optimal. It may struggle to encom-
pass the full range of possible variations in a subject. Ad-
ditionally, the effectiveness of visual representation varies
depending on the subject; for instance, our model demon-
strates superior performance with cars compared to humans.
Regarding social impacts, while personalized image gener-
ation offers numerous benefits, it simultaneously heightens
the risk of misuse. Therefore, it is better to introduce a
safety check for the source video and reference image to
alleviate the possible negative impacts.
References
[1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2live: Text-driven layered image
and video editing. In ECCV, 2022. 3
[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
arXiv preprint arXiv:2211.09800, 2022. 3
[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers.
ICCV, 2021. 7
In
[4] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-
term video object segmentation with an atkinson-shiffrin
memory model. In ECCV, 2022. 2, 4
[5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
arXiv preprint arXiv:2302.03011, 2023. 2, 3, 4, 6, 7
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618, 2022. 3
[7] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for consistent video
editing. arXiv preprint arxiv:2307.10373, 2023. 3
[8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626, 2022. 2, 3
[9] Jonathan Ho and Tim Salimans.
Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 6
[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS, 2020. 4
[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els. arXiv preprint arXiv:2210.02303, 2022. 2, 4
[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang. Cogvideo: Large-scale pretraining for
text-to-video generation via transformers. arXiv preprint
arXiv:2205.15868, 2022. 2
[13] Lianghua Huang, Di Chen, Yu Liu, Shen Yujun, Deli Zhao,
and Zhou Jingren. Composer: Creative and controllable im-
age synthesis with composable conditions. arXiv preprint
arxiv:2302.09778, 2023. 2
[14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scal-
ing up gans for text-to-image synthesis. arXiv preprint
arXiv:2303.05511, 2023. 2
[15] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439, 2023. 3
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and
Ross Girshick. Segment anything. arXiv:2304.02643, 2023.
2,4,5
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597, 2023. 4
[18] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499, 2023. 2,5
[19] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya
Jia. Video-p2p: Video editing with cross-attention control.
arXiv:2303.04761, 2023. 2, 3
[20] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and
Tieniu Tan. Videofusion: Decomposed diffusion mod-
els for high-quality video generation. arXiv preprint
arXiv:2303.08320, 2023. 2, 3
[21] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073, 2021. 2, 3
[22] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav
Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid
Hoshen. Dreamix: Video diffusion models are general video
editors. arXiv preprint arXiv:2302.01329, 2023. 2, 3
[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2
[24] Ben Okri. The Famished Road. Jonathan Cape, 1991. 2
[25] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero:
Fusing attentions for zero-shot text-based video editing.
arXiv:2303.09535, 2023. 2, 3, 6, 7
[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 3, 4, 5,7
[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022. 2, 3, 4, 5, 8
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 2, 4, 6
[29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 3
[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242, 2022. 2, 3,
7
[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS, 2022. 2
[32] Tim Salimans and Jonathan Ho. Progressive distillation
for fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512, 2022. 4
[33] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,
and Sungroh Yoon. Edit-a-video: Single video editing with
object-aware consistency. arXiv preprint arXiv:2303.07945,
2023. 3
[34] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792,
2022. 2, 4
[35] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020. 4,6
[36] Narek Tumanyan, Michal Geyer, Shai Bagon, and
Tali Dekel. Plug-and-play diffusion features for text-
driven image-to-image translation. arXiv preprint
arXiv:2211.12572, 2022. 3
[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS, 2017. 2
[38] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of image
diffusion models for text-to-video generation. arXiv preprint
arXiv:2212.11565, 2022. 2, 3, 4, 6, 7
[39] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789, 2022. 2
[40] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543, 2023. 2, 5,8
[41] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018, 2022. 2,4
