arXiv:2307.12612v1 [cs.CV] 24 Jul 2023
Less is More: Focus Attention for Efficient DETR
Dehua Zheng,
1,2 Wenhui Dong² Hailin Hu² Xinghao Chen² Yunhe Wang²*
Huazhong University of Science and Technology 2Huawei Noah's Ark Lab
dwardzheng@hust.edu.cn
{wenhui.dong, hailin.hu, xinghao.chen, yunhe.wang} @huawei.com
Abstract
DETR-like models have significantly boosted the perfor-
mance of detectors and even outperformed classical con-
volutional models. However, all tokens are treated equally
without discrimination brings a redundant computational
burden in the traditional encoder structure. The recent
sparsification strategies exploit a subset of informative to-
kens to reduce attention complexity maintaining perfor-
mance through the sparse encoder. But these methods tend
to rely on unreliable model statistics. Moreover, simply re-
ducing the token population hinders the detection perfor-
mance to a large extent, limiting the application of these
sparse models. We propose Focus-DETR, which focuses at-
tention on more informative tokens for a better trade-off be-
tween computation efficiency and model accuracy. Specifi-
cally, we reconstruct the encoder with dual attention, which
includes a token scoring mechanism that considers both lo-
calization and category semantic information of the objects
from multi-scale feature maps. We efficiently abandon the
background queries and enhance the semantic interaction
of the fine-grained object queries based on the scores. Com-
pared with the state-of-the-art sparse DETR-like detectors
under the same setting, our Focus-DETR gets comparable
complexity while achieving 50.4AP (+2.2) on COCO. The
code is available at torch-version and mindspore-version.
1. Introduction
Object detection is a fundamental task in computer vi-
sion that aims to predict the bounding boxes and classes of
objects in an image, as shown in Fig. 1 (a), which is of great
importance in real-world applications. DETR proposed by
Carion et al.[1] uses learnable queries to probe image fea-
tures from the output of Transformer encoders and bipar-
tite graph matching to perform set-based box prediction.
DETR-like models [18, 36, 14, 32, 21, 26, 2, 30, 37] have
made remarkable progress and gradually bridged the gap
*Corresponding author
(a) Image
(b) Sparse DETR (c) Focus-DETR (d) Focus-DETR
foreground
foreground object tokens
Figure 1: Visualization and comparison of tokens selected by
Sparse DETR [26] and our Focus-DETR. (a) is the original im-
ages, (b) and (c) represent the foreground selected by models. (d)
indicates the object tokens with more fine-grained category seman-
tic. Patches with smaller sizes come from higher-level features.
with the detectors based on convolutional neural networks.
Global attention in the DETR improves the detection
performance but suffers from computational burden and in-
efficiency due to redundant calculation without explicit dis-
crimination for all tokens. To tackle this issue, Deformable
DETR [37] reduces the quadratic complexity to linear com-
plexity through key sparsification, and it has developed into
a mainstream paradigm due to the advantages of leveraging
multi-scale features. Herein, we further analyze the com-
putational burden and latency of components in these mod-
els (Fig. 2). As shown in Fig. 2, we observe that the cal-
culation cost of the encoder is 8.8× that of the decoder in
Deformable DETR [37] and 7.0× in DINO [36]. In addi-
tion, the latency of the encoder is approximately 4~8 times
that of the decoder in Deformable DETR and DINO, which
emphasizes the necessity to improve the efficiency in the
encoder module. In line with this, previous works have
generally discussed the feasibility of compressing tokens in
the transformer encoder. For instance, PnP-DETR [29] ab-
stracts the whole features into fine foreground object feature
vectors and a small number of coarse background contex-
tual feature vectors. IMFA [34] searches key points based
on the prediction of decoder layer to sample multi-scale fea-
tures and aggregates sampled features with single-scale fea-
tures. Sparse DETR [26] proposes to preserve the 2D spa-
tial structure of the tokens through query sparsity, which
makes it applicable to Deformable DETR [37] to utilize
multi-scale features. By leveraging the cross-attention map
in the decoder as the token importance score, Sparse DETR
achieves performance comparable to Deformable DETR
only using 30% of queries in the encoder.
Despite all the progress, the current models [29, 26]
are still challenged by sub-optimal token selection strat-
egy. As shown in Fig. 1 (b), the selected tokens contain
a lot of noise and some necessary object tokens are obvi-
ously overlooked. In particular, Sparse DETR's supervision
of the foreground predictor relies heavily on the decoder's
cross-attention map (DAM), which is calculated based on
the decoder's queries entirely from encoder priors. Prelimi-
nary experiments show severe performance decay when the
Sparse DETR is embedded into the models using learnable
queries due to weak correlation between DAM and the re-
tained foreground tokens. However, state-of-the-art DETR-
like models, such as DINO [36], have proven that the se-
lected features are preliminary content features without fur-
ther refinement and could be ambiguous and misleading to
the decoder. In this case, DAM's supervision is inefficient.
Moreover, in this monotonous sparse encoder, the number
of retained foreground tokens remains numerous, and per-
forming the query interaction without more fine-grained se-
lection is not feasible due to computational cost limitations.
To address these issues, we propose Focus-DETR to al-
locate attention to more informative tokens by stacking the
localization and category semantic information. Firstly, we
design a scoring mechanism to determine the semantic level
of tokens. Foreground Token Selector (FTS) aims to aban-
don background tokens based on top-down score modula-
tions across multi-scale features. We assign {1,0} labels to
all tokens from the backbone with reference to the ground
truth and predict the foreground probability. The score of
the higher-level tokens from multi-scale feature maps mod-
ulates the lower-level ones to impose the validity of selec-
tion. To introduce semantic information into the token se-
lection process, we design a multi-category score predictor.
The foreground and category scores will jointly determine
the more fine-grained tokens with strong category seman-
tics, as shown in Fig. 1 (d). Based on the reliable scores and
selection from different semantic levels, we feed foreground
tokens and more fine-grained object tokens to the encoder
with dual attention. Thus, the limitation of deformable at-
tention in distant information mixing is remedied, and then
the semantic information of foreground queries is enhanced
by fine-grained token updates.
To sum up, Focus-DETR reconstructs the encoder's cal-
150
100
50
0
84.12
Latency(ms)
GFLOPS
142.8
33
encoder decoder
35
encoder decoder
27.2
30
25
20
49.78
15
20.4
20.9
10
9.6
5
0
Deformable DINO Focus-DETR
DETR
Deformable
DETR
DINO
Focus-DETR
16.2
8.8
8.7
Figure 2: Distribution of calculation cost and latency in the
Transformer part of the DETR-like models, e.g., Deformable
DETR [37], DINO [36] and our Focus-DETR.
culation process with dual attention based on obtaining
more accurate foreground information and focusing on fine-
grained tokens by gradually introducing semantic informa-
tion, and further enhances fine-grained tokens with mini-
mal calculation cost. Extensive experiments validate Focus-
DETR's performance. Furthermore, Focus-DETR is gen-
eral for DETR-like models that use different query con-
struction strategies. For example, our method can achieve
50.4AP (+2.2) on COCO compared to Sparse DETR with a
similar computation cost under the same setting.
2. Related work
Transformer-based detectors.
Recently, Carion et
al.[1] proposed an end-to-end object detector named DETR
(Detection Transformer) based on Vision Transformer [7].
DETR transforms object detection into a set prediction task
through the backbone, encoder, and decoder and supervises
the training process through Hungarian matching algo-
rithms. A lot of recent works [18, 14, 37, 36, 21, 3, 35, 2, 4]
have boosted the performance of Transformer-based de-
tectors from the perspective of accelerating training con-
vergence and improving detection precision. Representa-
tively DINO[36] establishes DETR-like models as a main-
stream detection framework, not only for its novel end-to-
end detection optimization, but also for its superior perfor-
mance. Fang et al. [8] propose YOLOS and reveal that
object detection can be accomplished in a pure sequence-
to-sequence manner with minimal additional inductive bi-
ases. Li et al.[15] propose ViTDet to explore the plain, non-
hierarchical ViT as a backbone for object detection. Dai et
al.[5] propose a pretext task named random query patch de-
tection to Unsupervisedly Pre-train DETR (UP-DETR) for
object detection. IA-RED² [22] introduces an interpretable
module for dynamically discarding redundant patches.
Lightweight Vision Transformers. As we all know,
vision Transformer (ViT) suffers from its high calculation
complexity and memory cost. Lu et al. [23] propose an
efficient ViT with dynamic sparse tokens to accelerate the
inference process. Yin et al.[33] adaptively adjust the infer-
ence cost of ViT according to the complexity of different in-
put images. Xu et al.[31] propose a structure-preserving to-
ken selection strategy and a dual-stream token update strat-
egy to significantly improve model performance without
changing the network structure. Tang et al. [28] presents
a top-down layer by layer patch slimming algorithm to re-
duce the computational cost in pre-trained Vision Trans-
formers. The core strategy of these algorithms and other
similar works[11, 13, 19] is to abandon redundant tokens to
reduce the computational complexity of the model.
In addition to the above models focused on sparsity
backbone structure applied on classification tasks, some
works[26, 29] lie in reducing the redundant calculation in
DETR-like models. Efficient DETR [32] reduces the num-
ber of layers of the encoder and decoder by optimizing the
structure while keeping the performance unchanged. PnP-
DETR and Sparse DETR have achieved performance com-
parable to DETR or Deformable by abandoning background
tokens with weak semantics. However, these methods are
suboptimal in judging background information and lack en-
hanced attention to more fine-grained features.
3. Methodology
We first describe the overall architecture of Focus-
DETR. Then, we elaborate on our core contributions: (a)
Constructing a scoring mechanism that considers both lo-
calization and category semantic information from multi-
scale features. Thus we obtain two-level explicit discrim-
ination for foreground and fine-grained object tokens; (b)
Based on the scoring mechanism, we feed tokens with dif-
ferent semantic levels into the encoder with dual attention,
which enhances the semantic information of queries and
balances model performance and calculation cost. A de-
tailed analysis of the computational complexity is provided.
3.1. Model Architecture
As shown in Fig. 3, Focus-DETR is composed of a back-
bone, a encoder with dual attention and a decoder. The
backbone can be equipped with ResNet [10] or Swin Trans-
former [20]. To leverage multi-scale features {fi}\/\_₁ (L =
4) from the backbone, where fi Є RC×H₁×W₁, we obtain
the feature maps {ƒ1, ƒ2, ƒ3} in three different scales (i.e.,
1/8, 1/16, 1/32) and downsample ƒ³ to get ƒ4 (i.e., 1/64).
Before being fed into the encoder with dual attention, the
multi-scale feature maps {fi}\/\₁ first go through a fore-
ground token selector (Section 3.2) using a series of top-
down score modulations to indicate whether a token belongs
to the foreground. Then, the selected foreground tokens of
each layer will pass through a multi-category score predic-
tor to select tokens with higher objectiveness score by lever-
aging foreground and semantic information (Section 3.2).
These object tokens will interact further with each other
and complement the semantic limitation of the foreground
queries through the proposed dual attention (Section 3.3).
3.2. Scoring mechanism
Foreground Token Selector. Sparse DETR[26] has
demonstrated that only involving a subset of tokens for en-
coders can achieve comparable performance. However, as
illustrated in Fig. 4, the token selection provided by Sparse
DETR [26] has many drawbacks. In particular, many pre-
served tokens do not align with foreground objects.
We think the challenge from Sparse DETR lies in that its
supervision of token selection relies on DAM. The correla-
tion between DAM and retained foreground tokens will be
reduced due to learnable queries, which brings errors dur-
ing training. Instead of predicting pseudo-ground truth [26],
we leverage ground truth boxes and labels to supervise the
foreground selection inspired by [17]. To properly provide
a binary label for each token on whether it appears in fore-
ground, we design a label assignment protocol to leverage
the multi-scale features for objects with different scales.
In particular, we first set a range of sizes for the bound-
ing boxes of different feature maps, and add the overlap
of the adjacent interval by 50% to enhance the prediction
near boundary. Formally, for each token t(i,j) with stride sı,
where is the index of scale level, and (i, j) is the position
in the feature map, we denote the corresponding coordinate
(x, y) in the original image as ([ ¾ ] + i · 81, | ¾⁄1 ] + j · 81).
Considering the adjacent feature map, our protocol deter-
mines the label ((i,j) according to the following rules, i.e.,
Ть
l(i,j)
=
S1,
10. (x, y) & D Bbox V d) []
(x, y) = D Bbox Ad(i,j) = [rb, re]
¢
=
2
²+1
=
= ∞.
(1)
where DBbox (x, y, w, h) denotes the ground truth boxes,
d(i,j) =max(1,2) = [rf, rl], represents the maximum
checkerboard distance between (x, y) and the bounding box
center, [r, r] represents the interval of object predicted
by the l-layer features and r < rob+1 < r < r²+1 and
+1 (r₁+r²), 1 = {0, 1, 2, 3}, ro
O and r³
Another drawback of DETR sparse methods is the insuf-
ficient utilization of multi-scale features. In particular, the
semantic association and the discrepancy in the token se-
lection decisions between different scales are ignored. To
fulfill this gap, we construct the FTS module with top-down
score modulations. We first design a score module based
on Multi-Layer Perceptron (MLP) to predict the foreground
score in each feature map. Considering that high-level fea-
ture maps contain richer semantic than low-level features
with higher resolution, we leverage the foreground score of
high-level semantics as complement information to mod-
ulate the feature maps of adjacent low-level semantics. As
shown in Fig. 5, our top-down score modulations only trans-
mits foreground scores layer by layer through upsampling.
Formally, given the feature map fɩ where l = {2, 3, 4},
Si-1
=
MLPF (fi−1(1+ UP(αɩ * S₁))),
(2)
Top-down Score Modulations
= {~
XN Encoder
Image
Foreground Token Selector
Top K (layer-wise)
Dual Attention
Box
Class
Deformable Attention
XN Decoder
Update
Self Attention)
Multi-Category Top K
Score Predictor
Query Selection
FFN
Cross Attention
Self Attention
COCO-O
Figure 3: The architecture overview of the proposed Focus-DETR. Our Focus-DETR comprises a backbone network, a Transformer
encoder, and a Transformer decoder. We design a foreground token selector (FTS) based on top-down score modulations across multi-
scale features. And the selected tokens by a multi-category score predictor and foreground tokens go through the encoder with dual
attention to remedy the limitation of deformable attention in distant information mixing.
Sparse
DETR
Ours
f1
f2
f3
f4
fi
MLP
S₁
Element-wise Multiplication
Number Multiplication
S1-1
UP Sample
κα
Figure 4: The foreground tokens preserved in different feature
maps of Sparse DETR and our Focus-DETR. The red dots indi-
cate the position of the reserved token corresponding to the origi-
nal image based on the stride.
where S indicates the foreground score of the 1-th feature
map, UP() is the upsampling function using bilinear in-
terpolation, MLPF (.) is a global score predictor for tokens
in all the feature maps, {a} //--¹³ is a set of learnable modu-
lation coefficients, and L indicates the layers of multi-scale
feature maps. The localization information of different fea-
ture maps is correlated with each other in this way.
Multi-category score predictor. After selecting tokens
with a high probability of falling in the foreground, we then
seek an efficient operation to determine more fine-grained
tokens for query enhancement with minimal computational
cost. Intuitively, introducing more fine-grained category in-
formation would be beneficial in this scenario. Following
this motivation, we propose a novel more fine-grained token
selection mechanism coupled with the foreground token se-
lection to make better use of the token features. As shown in
Fig. 3, to avoid meaningless computation of the background
token, we employ a stacking strategy that considers both lo-
calization information and category semantic information.
Specifically, the product of foreground score and category
score calculated by a predictor MLPC (·) will be used as
fl-1
→MLP →
Figure 5: The operation of top-down score modulation. For multi-
scale feature maps, we use a shared MLP to calculate {S1, S2, ...}.
S₁ is incorporated in the calculation of S1-1 by a dynamic coeffi-
cient a and feature map fi-1.
our final criteria på for determining the fine-grained tokens
involved in the attention calculation, i.e.,
Pj = 8; × Cj = 8; × MLPc(T³½³),
(3)
where and
Sj 
c; represent foreground score and category
probabilities of T respectively. Unlike the query selection
strategy of two-stage Deformable DETR [37] from the en-
coder's output, our multi-category probabilities do not in-
clude background categories (Ø). We will determine the to-
kens for enhanced calculation based on the pj.
3.3. Calculation Process of Dual Attention
The proposed reliable token scoring mechanism will en-
able us to perform more fine-grained and discriminatory
calculations. After the foreground and fine-grained object
tokens are gradually selected based on the scoring mecha-
nism, we first exploit the interaction information of the fine-
grained object tokens and corresponding position encoding
Algorithm 1 Encoder with Dual Attention
Input: All tokens Ta, foreground tokens Tƒ, position em-
bedding PEf, object token number k, foregroud score
Sf, foreground token index If
Output: all tokens T and foreground tokens T'½ after one
encoder layer
1: category score Cf ← MLPc (Tƒ)
2: maximum of category score S. + max (Cf)
3: object token score Sp
obj
=
4: Idx TopK(Sp, k)
Se Sf
5: To Tƒ[Idxº¹³], PE。 ← PEƒ[Idxobj]
6: q = k = PE。 + To, v = To
7: TMHSA(q, k, v)
8: ToNorm(v + To)
9: update T in Tƒ according to Idxobj
10: q
q = T'ƒ, k = Ta + PEƒ, v =^
11: TMSDeformAttn (q, k, v)
12: update T in To according to If
Ta
by enhanced self-attention. Then, the enhanced object to-
kens will be scattered back to the original foreground to-
kens. This way, Focus-DETR can leverage the foreground
queries with enhanced semantic information. In addition,
because of reliable fine-grained token scoring, dual atten-
tion in Encoder effectively boosts the performance with
only a negligible increase in calculation cost compared to
the unsophisticated query sparse strategy. We utilize Algo-
rithm 1 to illustrate the fine-grained feature selection and
enhancement process in the encoder with dual attention.
3.4. Complexity Analysis
We further analyze the results in Fig. 2 and our claim
that the fine-grained tokens enhanced calculation adds only
a negligible calculation cost mathematically. We denote the
computational complexity of deformable attention in the en-
coder and decoder as {GDA, GA), respectively. We cal-
culate GDA with reference to Deformable DETR [37] as
follows:
GDA = O(KC +3MK +C+5K)NC,
=
(4)
where N₁ (N₁ ≤ HW
hiwi) is the number of
queries in encoder or decoder, K is the sampling number
and C is the embedding dims. For encoder, we set Ne
as HW, where y is the ratio of preserved foreground to-
kens. For decoder, we set Nad to be a constant. In addition,
the complexity of the self-attention module in decoder is
O(2NqdC² + NC). For an image whose token number
is approximately 1 × 104, GDA is approximately 7 under
the common setting {K
{K 
= 4,C = 256, Ngd = 900, y =
GODA
1}. When y equals 0.3, the calculation cost in the Trans-
former part will reduce over 60%. This intuitive comparison
demonstrates that the encoder is primarily responsible for
redundant computing. Then we define the calculation cost
of the fine-grained tokens enhanced calculation as GOEC:
GOECO(2NC² + NC),
(5)
where No represents the number of fine-grained tokens that
obtained through scoring mechanism. When No = 300,
GOEC is only less than 0.025, which has a negligible
(GDA+GA)
impact on the overall model calculation.
3.5. Optimization
Like DETR-like detectors, our model is trained in an
end-to-end manner, and the loss function is defined as:
L = \mÊmatch + \dÂdn + λƒÂƒ + λeÊenc, (6)
where match is the loss for pair-wise matching based on
Hungarian algorithm, Ian is the loss for denoising models,
Ef is the loss for foreground token selector, Lenc is the loss
for auxiliary optimization through the output of the last en-
coder layer, Am, λd, λf, λa are scaling factors.
Loss for feature scoring mechanism. Focus-DETR ob-
tains foreground tokens by the FTS module. Focal Loss [17]
is applied to train FTS as follow:
f = -aƒ(1 - p)"log(pƒ),
=
(7)
where pf represents foreground probability, af 0.25 and
Y = 2 are empirical hyperparameters.
4. Experiments
4.1. Experimental Setup
Dataset: We conduct experiments on the challenging
COCO 2017 [16] detection dataset, which contains 117K
training images and 5K validation images. Following the
common practice, we report the standard average precision
(AP) result on the COCO validation dataset.
Implementation Details: The implementation details of
Focus-DETR mostly align with the original model in de-
trex [25]. We adopt ResNet-50 [10], which is pretrained us-
ing ImageNet [6] as the backbone and train our model with
8xNvidia V100 GPUs using the AdamW [12] optimizer.
In addition, we perform experiments with ResNet-101 and
Swin Transformer as the backbone. The initial learning
rate is set as 1 × 10−5 for the backbone and 1 × 10-4 for
the Transformer encoder-decoder framework, along with a
weight decay of 1 × 10−4. The learning rate decreases at a
later stage by 0.1. The batch size per GPU is set to 2. For the
scoring mechanism, the loss weight coefficient of the FTS
is set to 1.5. The MLP c() shares parameters with the cor-
responding in the decoder layer and is optimized along with
the training of the entire network. In addition, we decrease
Model
Epochs
Faster-RCNN[24]
108
42.0
62.4
DETR(DC5)[1]
500
43.3 63.1
AP AP50 AP75
44.2
45.9 22.5
APS APM APL Params
GFLOPS FPS
20.5
45.8
61.1
42M
180
25.3
47.3
61.1
41M
187
11.2
Efficient-DETR[32]
36
44.2 62.2
48.0
28.4 47.5
56.6
32M
159
-
Anchor-DETR-DC5[30]
500
44.2
64.7
47.5
24.7
48.2
60.6
19.0
PnP-DETR(a = = 0.33)[29]
500
42.7
62.8
45.1
22.4 46.2
60
42.5
Conditional-DETR-DC5[21]
108
45.1
65.4
48.5
25.3
49.0
62.2
44M
195
11.5
Conditional-DETR-V2[3]
50
44.8 65.3
48.2
25.5
48.6
62.0
46M
161
Dynamic DETR(5 scales) [4]
50
47.2
65.9
51.1
28.6
49.3
59.1
58M
DAB-Deformable-DETR[18]
50
46.9
66.0
50.8
30.1
50.4
62.5
44M
256
14.8
UP-DETR[5]
300
42.8
63.0
45.3
20.8
47.1
61.7
-
SAM-DETR[35]
50
45.0 65.4
47.9
26.2
49.0
63.3
58M
210
24.4
Deformable DETR[37]
50
46.2
65.2
50.0
28.8
49.2
61.7
40M
173
19.0
Sparse DETR(a = 0.3)[26]
50
46.0
65.9
49.7
29.1
49.1
60.6
41M
121
23.2
DN-Deformable-DETR[14]
50
48.6
67.4
52.7
31.0
52.0
63.7
48M
265
18.5
DINO[36]
+ Sparse DETR(a = = 0.3)
or + Focus-DETR (Ours)(a = 0.3)
3330
36
50.9
69.0
55.3
34.6
54.1
64.6
47M
279
14.2
36
48.2
65.9
52.5
30.4
51.4
63.1
47M
152
20.2
36
50.4 68.5
55.0
34.0
53.5
64.4
48M
154
20.0
Table 1: Results for our Focus-DETR and other detection models with the ResNet50 backbone on COCO val2017. Herein, a indicates the
keep ratio for methods that prune background tokens. All reported FPS are measured on a NVIDIA V100.
the cascade ratio by an approximate arithmetic sequence,
and the lower threshold is 0.1. We provide more detailed
hyper-parameter settings in Appendix A.1.1, including the
reserved token ratio in the cascade structure layer by layer
and the object scale interval for each layer.
Epochs AP AP 50 AP75 |Params GFLOPS
Model
Faster RCNN-FPN [24]
DETR-DC5 [1]
Anchor-DETR* [30]
DN DETR [14]
DN DETR-DC5 [14]
50
50
108 44.0 63.9 47.8
500 44.9 64.7 47.7
45.1 65.7
45.2 65.5
60M
60M
246
253
48.8 58M
48.3
63M
50 47.3 67.5
50.8
63M
174
282
Conditional DETR-DC5 [21]
DAB DETR-DC5 [18]
Focus-DETR (Ours)
108
50 46.6 67.0 50.2
45.9 66.8 49.5
63M
262
63M
296
221
36 51.4 70.0 55.7 67M
Table 2: Comparison of Focus-DETR (DINO version) and other
models with ResNet101 backbone. Our Focus-DETR preserve
30% tokens after the backbone. The models with superscript *
use 3 pattern embeddings.
Model
Deformable DETR (priori)
+ Sparse DETR (a = 0.3)
or + Focus-DETR (a = 0.3)
Deformable DETR (learnable)
+ Sparse DETR (a = 0.3)
or Focus-DETR (a = 0.3)
DN-Deformable-DETR (learnable)
+ Sparse DETR (a = 0.3)
or + Focus-DETR (a = 0.3)
DINO (mixed)
+ Sparse DETR (a = 0.3)
or + Focus-DETR (a = 0.3)
АР
46.2
Corr
46.0 0.7211±0.0695
121
GFLOPS FPS
177
19
23.2
46.6
123
23.0
45.4
173
19
43.5 0.5081±0.0472
45.2
118
24.2
120
23.9
48.6
195
18.5
47.4 0.5176 0.0452
137
23.9
48.5
138
23.6
50.9
279
14.2
48.2 0.5784±0.0682
50.4
152
20.2
154
20.0
Table 3: Corr: the correlation of DAM and retained fore-
ground(5k validation set). "priori”: position and content
query (encoder selection); "learnable”: position and content
query (initialization); "mixed”: position query (encoder selec-
tion), content query (initialization).
4.2. Main Results
Benefiting from the well-designed scoring mechanisms
towards the foreground and more fine-grained object to-
kens, Focus-DETR can focus attention on more fine-grained
features, which further improves the performance of the
DETR-like model while reducing redundant computations.
Table 1 presents a thorough comparison of the proposed
Focus-DETR (DINO version) and other DETR-like detec-
tors [1, 32, 37, 30, 29, 21, 3, 9, 27, 4, 18, 14, 5, 35, 26], as
well as Faster R-CNN [24]. We compare our model with
efficient DETR-based detectors [29, 26], our Focus-DETR
with keep-ratio of 0.3 outperforms PnP-DETR [29] (+7.9
AP). We apply the Sparse DETR to DINO to build a solid
baseline. Focus-DETR outperforms Sparse DETR (+2.2
AP) when embedded into DINO. When applied to the
DINO [36] and compared to original DINO, we lose only
0.5 AP, but the computational cost is reduced by 45% and
the inference speed is improved 40.8%.
In Fig. 7, we plot the AP with GFLOPS to provide a
clear picture of the trade-off between accuracy and com-
putation cost. Overall, Our Focus-DETR (DINO version)
achieve state-of-the-art performance when compared with
other DETR-like detectors.
To verify the adaptability of Focus-DETR to the stronger
backbone ResNet-101 [10] and the effect of the ratio of the
preserved foreground on model performance, we perform a
series of extensive experiments. As shown in Table 2, com-
pared to other DETR-like models [18, 14, 30, 1, 9, 27, 24],
Focus-DETR (DINO version) achieves higher AP with
fewer GFLOPs. Moreover, using a Swin Transformer pre-
trained on ImageNet as backbone, we also achieve excellent
performance, as shown in Appendix A.2.1.
Img 1
Img 2
Img 3
fall
f1 f2 ₤3 ₤4
Img 3
Img 4
layer 1
layer 2
foreground score
layer 3
layer 4
layer 5
layer 6
(a)
(b)
Figure 6: Visualization results of preserved foreground tokens distribution at multi-scale feature maps as shown (a) and k object tokens
evolution at different encoder layers as shown (b). {Img1, Img2, Img3, Img4} represent four test images, {ƒ1, ƒ2, ƒ³, ƒ4} represent
foreground tokens at four feature maps, {layer 1, layer 2 ...} represent different encoder layers.
52
R101, 30%
R50, 50%
R50◆
R50, 30%
50
R50, 10%
*
R101
R50, 30%
R50, 50%
+
48
R50, 10%
Swin-T
R50, 50%
DCE-R50
R50, 30%
R50
DCSR50
46
R101
R50, 10%
DCE-R50
44
R101
R50
R101
R50
R50
42
100
150
200
250
GFLOPS
DCE-R101
DCS-R101
DC-R101
Conditional DETR
DAB DETR
DN DETR
Sparse DETR
Deformable DETR
DINO
Sparse DETR+DINO
Focus DETR
300
350
Figure 7: Performance of recent object detectors in terms of aver-
age precision(AP) and GFLOPs. The GFLOPS is measured using
100 validation images.
4.3. Extensive Comparison
Sparse DETR is state-of-the-art for lightweight DETR-
like models. As mentioned earlier, sparse DETR will cause
significant performance degradation when using learnable
queries. To verify the universality of Focus-DETR, we com-
pare our model with excellent and representative DETR-like
models equipped with Sparse DETR, including Deformable
DETR [37], DN-DETR [14] and DINO [36].
In addition to the Sparse DETR, we apply the
Sparse DETR to Deformable DETR(two-stage off), DN-
Deformable DETR and DINO to construct three baselines.
We retain all the Sparse DETR's designs for a fair enough
comparison, including the auxiliary encoder loss and re-
lated loss weight. We also optimize these baselines by ad-
justing hyperparameters to achieve the best performance.
As shown in Table 3, when applying Sparse DETR to De-
formable DETR without two-stage, DN-Deformable-DETR
and DINO, the AP decreases 1.9, 1.2 and 2.7. We calcu-
late Corr proposed by Sparse DETR that denotes the cor-
relation bewteen DAM and selected foreground token, we
calculate the top 10% tokens to compare the gap more in-
tuitively. As shown in Table 3, their Corrs are far lower
than original Sparse DETR, which means foreground selec-
tor does not effectively learn DAM. Compared to Sparse
DETR, Focus-DETR achieves 1.7, 1.1 and 2.2 higher AP
with similar latency in Deformable DETR(two-stage off),
DN-Deformable DETR and DINO.
As shown in Fig. 3, it seems that our encoder using
dual attention can be independently embedded into Sparse
DETR or other DETR-like models. However, a precise
scoring mechanism is critical to dual attention. We added
the experiments of applying the encoder with dual atten-
tion to Sparse DETR in Appendix A.2.3. Results show
us that fine-grained tokens do not bring significant perfor-
mance gains.
4.4. Ablation Studies
We conduct ablation studies to validate the effectiveness
of our proposed components. Experiments are performed
with ResNet-50 as the backbone using 36 epochs.
Effect of foreground token selection strategy. Firstly,
simply obtaining the token score using a foreground score
predictor without supervision achieves only 47.8 AP and
is lower than that (48.2 AP) of DINO pruned by Sparse
DETR. As shown in the second row of Table 4, by adding
supervision with our improved label assignment strategy,
Focus-DETR yields a significant improvement of +1.0 AP.
In addition, top-down score modulations optimize the per-
formance of FTS by enhancing the scoring interaction be-
tween multi-scale feature maps. As shown in the third row
of Table 4, Focus-DETR equipped with the top-down score
modulation achieves +0.4 AP. As the visualization shown
in Fig. 6 (a), we can observe that our method precisely se-
lect the foreground tokens. Moreover, feature maps in dif-
ferent levels tend to focus on objects with different scales.
Furthermore, we find that that there is an overlap between
the object scales predicted by adjacent feature maps due to
our scale overlap setting. We provide more detailed overlap
setting details in the Appendix A.1.2.
FTS
score
predictor supervision modulations
V
cascade
dual
attention
AP | AP50 AP75 FPS
47.8 65.2 52.1 20.4
20.4
20.3
20.3
50.4 68.5 55.0 20.0
Table 4: Ablation studies on the FTS and dual attention. FTS
is the foreground token selector. Dual attention represents the our
encoder structure. Supervision indicates the label assignment from
the ground truth boxes.
Effect of cascade token selection. When keeping a
fixed number of tokens in the encoder, the accumulation of
pre-selection errors layer by layer is detrimental to the de-
tection performance. To increase the fault tolerance of the
scoring mechanism, we design the cascade structure for the
encoder to reduce the number of foreground tokens layer by
layer (Section 3.2). As shown in Fig. 6 (b), we can see the
fine-grained tokens focusing process in the encoder as the
selecting range decreases, which enhances the model's fault
tolerance and further improves the model's performance.
As illustrated in the fourth row of Table 4, Focus-DETR
equipped with cascade structure achieves +0.5 AP.
Effect of the dual attention. Unlike only abandoning
the background tokens, one of our contributions is recon-
structing the encoder using dual attention with negligible
computational cost. Tokens obtained after the enhanced
calculation supplement the semantic weakness of the fore-
ground queries due to the limitation in distant token mixing.
We further analyze the effects of the encoder with dual at-
tention. As shown in the fifth row of Table 4, the encoder
with dual attention brings +0.8 AP improvement. These re-
sults demonstrate that enhancing fine-grained tokens is ben-
eficial to boost detection performance and the effectiveness
of our stacked position and semantic information for fine-
grained feature selection, as shown in Fig. 1.
Top-down
Bottom-up
AP
AP50
AP75
49.7
66.9
54.0
50.4
68.5
55.0
50.2
68.4
54.6
Table 5: Association methods between scores of multi-scale fea-
ture maps. We try top-down and bottom-up modulations.
Effect of top-down score modulation. We further anal-
ysis the effect of the multi-scale scoring guidance mecha-
nisms in our method. As shown in Table 5, we can observe
that utilizing multi-scale information for score prediction
brings consistent improvement (+0.5 or +0.7 AP). We also
conduct ablation experiments for different score modula-
tion methods. The proposed top-down score guidance strat-
egy (Section 3.2) achieves 0.2 higher AP than bottom-down
strategy, which justifies our motivation that using high-level
scores to modulating low-level foreground probabilities is
beneficial for the final performance.
Effect of pruning ratio. As shown in Table 6, we
analyze the detection performance and model complexity
when changing the ratio of foreground tokens retained by
different methods. Focus-DETR achieves optimal perfor-
mance when keeping the same ratio. Specifically, Focus-
DETR achieves +2.7 AP than Sparse DETR and +1.4AP
than DINO equipped with Sparse DETR's strategies with
similar computation cost at 128 GFLOPS.
Model
Sparse DETR [26]
(epoch=50)
DINO [36]
α AP APS APM APL GFLOPS FPS
0.1 45.3 28.4
48.3
60.1
105
25.4
0.2 45.6 28.5
48.6
60.4
113
24.8
0.3 46.0
29.1
49.1
60.6
121
23.2
0.4 46.2 28.7
49.0
61.4
128
21.8
0.5 46.3 29.0
49.5
60.8
136
20.5
0.1 47.5 29.1
50.7
62.7
126
23.9
0.2 47.9 30.0
51.1
62.9
139
21.4
0.3 48.2 30.5
51.4
63.1
152
20.2
0.4 48.4 30.5
51.8
63.2
166
18.6
0.5 48.4 30.6
51.8
63.4
181
18.1
0.1 48.9 32.6
52.6
64.1
128
23.7
0.2 49.8 32.3
52.9
64.0
141
21.3
Focus-DETR
0.3 50.4
33.9
53.5
64.4
154
20.0
(epoch=36)
0.4 50.4 34.0 53.7
0.5 50.5 34.4 53.8
64.1
169
18.5
64.0
183
17.9
+ Sparse DETR [26]
(epoch=36)
Table 6: Experiment results in performance and calculation cost
when changing the ratio of foreground tokens retained by Focus-
DETR, Sparse DETR, and DINO+Sparse DETR.
4.5. Limitation and Future Directions
Although Focus-DETR has designed a delicate token
scoring mechanism and fine-grained feature enhancement
methods, more hierarchical semantic grading strategies,
such as object boundaries or centers, are still worth explor-
ing. In addition, our future work will be constructing a uni-
fied feature semantic scoring mechanism and fine-grained
feature enhancement algorithm throughout the Transformer.
5. Conclusion
This paper proposes Focus-DETR to focus on more in-
formative tokens for a better trade-off between computa-
tion efficiency and model accuracy. The core component
of Focus-DETR is a multi-level discrimination strategy for
feature semantics that utilizes a scoring mechanism consid-
ering both position and semantic information. Focus-DETR
achieves a better trade-off between computation efficiency
and model accuracy by precisely selecting foreground and
fine-grained tokens for enhancement. Experimental results
show that Focus-DETR has become the SOTA method in to-
ken pruning for DETR-like models. Our work is instructive
for the design of transformer-based detectors.
References
[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European Confer-
ence of Computer Vision, 2020. 1, 2, 6
[2] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong
Wang. Group DETR: fast training convergence with decou-
pled one-to-many label assignment. CORR, abs/2207.13085,
2022. 1,2
[3] Xiaokang Chen, Fangyun Wei, Gang Zeng, and Jingdong
Wang. Conditional DETR V2: efficient detection trans-
former with box queries. CoRR, abs/2207.08914, 2022. 2,
6
[4] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan
Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-
end object detection with dynamic attention. In International
Conference on Computer Vision, pages 2968-2977, 2021. 2,
6
[5] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.
Up-detr: Unsupervised pre-training for object detection with
transformers. In Computer Vision and Pattern Recognition,
pages 1601-1610, 2021. 2, 6
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In Computer Vision and Pattern Recognition,
pages 248-255, 2009. 5, 12
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In AAAI Conference on Artificial Intelligence. Open-
Review.net, 2021. 2
[8] Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang,
Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu. You
only look at one sequence: Rethinking transformer in vision
through object detection. arXiv preprint arXiv:2106.00666,
2021. 2
[9] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai,
and Hongsheng Li. Fast convergence of detr with spa-
tially modulated co-attention. In International Conference
on Computer Vision, pages 3601-3610, 2021. 6
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition, pages 770-778, 2016. 3, 5,
6
[11] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami,
Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned
token pruning for transformers. In Aidong Zhang and Huzefa
Rangwala, editors, Knowledge Discovery and Data Mining,
pages 784-794. ACM, 2022. 3
[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Yoshua Bengio and Yann LeCun,
editors, International Conference on Learning Representa-
tions, 2015. 5
[13] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei
Niu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, and
Yanzhi Wang. Spvit: Enabling faster vision transformers via
soft token pruning. ArXiv, abs/2112.13890, 2021. 3
[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,
and Lei Zhang. Dn-detr: Accelerate detr training by intro-
ducing query denoising. In Computer Vision and Pattern
Recognition, 2022. 1, 2, 6, 7, 11
[15] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
Exploring plain vision transformer backbones for object de-
tection. arXiv preprint arXiv:2203.16527, 2022. 2
[16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In David J. Fleet, Tomás Pajdla, Bernt Schiele, and
Tinne Tuytelaars, editors, European Conference of Computer
Vision, volume 8693 of Lecture Notes in Computer Science,
pages 740-755. Springer, 2014. 5
[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
42(2):318–327, 2020. 3, 5
[18] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,
Hang Su, Jun Zhu, and Lei Zhang. DAB-DETR: Dynamic
anchor boxes are better queries for DETR. In International
Conference on Learning Representations, 2022. 1, 2, 6
[19] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive
sparse vit: Towards learnable adaptive token pruning by fully
exploiting self-attention. CoRR, abs/2209.13802, 2022. 3
[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
International Conference on Computer Vision (ICCV), 2021.
3,11
[21] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,
Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.
Conditional detr for fast training convergence. In Interna-
tional Conference on Computer Vision (ICCV), 2021. 1, 2,
6
[22] Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang
Ia-red²:
Wang, Rogério Feris, and Aude Oliva.
Interpretability-aware redundancy reduction for vision trans-
formers. CoRR, abs/2106.12620, 2021. 2
[23] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision
transformers with dynamic token sparsification. In Advances
in Neural Information Processing Systems, 2021. 2
[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In C. Cortes, N. Lawrence, D. Lee, M.
Sugiyama, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems, volume 28. Curran Associates,
Inc., 2015. 6
[25] Tianhe Ren, Shilong Liu, Feng Li, Hao Zhang, Ailing Zeng,
Jie Yang, Xingyu Liao, Ding Jia, Hongyang Li, He Cao,
Jianan Wang, Zhaoyang Zeng, Xianbiao Qi, Yuhui Yuan,
Jianwei Yang, and Lei Zhang. detrex: Benchmarking de-
tection transformers, 2023. 5
[26] Byungseok Roh, Jae Woong Shin, Wuhyun Shin, and Sae-
hoon Kim. Sparse DETR: Efficient end-to-end object detec-
tion with learnable sparsity. In International Conference on
Learning Representations, 2022. 1, 2, 3, 6, 8, 11, 12, 13
[27] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.
Rethinking transformer-based set prediction for object detec-
tion. In International Conference on Computer Vision, pages
3591-3600, 2021. 6
[28] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan
Guo, Chao Xu, and Dacheng Tao. Patch slimming for ef-
ficient vision transformers. In Computer Vision and Pattern
Recognition (CVPR), pages 12155-12164, 2022. 3
[29] Tao Wang, Li Yuan, Yunpeng Chen, Jiashi Feng, and
Shuicheng Yan. Pnp-detr: Towards efficient visual analysis
with transformers. In International Conference on Computer
Vision, 2021. 1, 2, 3, 6
[30] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun.
Anchor detr: Query design for transformer-based detector.
In AAAI Conference on Artificial Intelligence, 2022. 1, 6
[31] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke
Li, Weiming Dong, Liqing Zhang, Changsheng Xu, and
Xing Sun. Evo-vit: Slow-fast token evolution for dynamic
vision transformer. In AAAI Conference on Artificial Intelli-
gence, volume 36, pages 2964–2972, 2022. 2
[32] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Ef-
ficient DETR: improving end-to-end object detector with
dense prior. CORR, abs/2104.01318, 2021. 1, 3, 6
[33] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya,
Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for
efficient vision transformer. In Computer Vision and Pattern
Recognition (CVPR), pages 10799-10808, 2022. 2
[34] Gongjie Zhang, Zhipeng Luo, Zichen Tian, Jingyi Zhang,
Xiaoqin Zhang, and Shijian Lu. Towards efficient use of
multi-scale features in transformer-based object detectors. In
CVPR, 2023. 2
[35] Gongjie Zhang, Zhipeng Luo, Yingchen Yu, Kaiwen Cui,
and Shijian Lu. Accelerating detr convergence via semantic-
aligned matching. In Computer Vision and Pattern Recogni-
tion (CVPR), pages 939-948, 2022. 2, 6
[36] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M. Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection, 2022. 1, 2, 6, 7, 8, 11, 12
[37] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection. In International Conference
on Learning Representations, 2021. 1, 2, 4, 5, 6, 7, 11
fi
H/SxW/s
☑
max
Bax-(w,h)
HxW
|₤1-1
H/SL-1 XW/SL-1
HXW
(0.0)
Model
Figure 8: Visualization of the label assignment process. fi, fi-1
are feature maps with different scales (s1, S1-1).
A. Appendix
A.1. More Implementation Details
A.1.1 Cascade Structure
In order to increase the fault tolerance of our model, we
gradually reduce the scope of foreground regions through
a cascade structure. As we show in Section 3.4, the com-
putational complexity of deformable attention [37] is linear
with the number of preserved tokens. Therefore, there is no
significant difference in complexity between the even struc-
tures (e.g., {0.4,0.4,0.4,0.4,0.4,0.4} and the cascade struc-
tures(e.g., {0.65,0.55,0.45,0.35,0.25,0.15}). Table 7 lists
different average keep – ratio and corresponding ratios of
different layers designed in this paper.
Average keep·
- ratio
0.1
0.2
0.3
0.4
0.5
Ratios
{0.1, 0.1, 0.1, 0.1, 0.1, 0.1}
{0.3, 0.3, 0.2, 0.2, 0.1, 0.1}
{0.5, 0.4, 0.3, 0.3, 0.2, 0.1}
{0.65,0.55,0.45,0.35,0.25,0.15}
{0.75,0.65,0.55,0.45,0.35,0.25}
Table 7: Detailed cascade keep-ratio desiged by Focus-DETR.
A.1.2 Label Assignment
Unlike the traditional label assignment scheme for multi-
scale feature maps, the ranges are allowed to overlap be-
tween the two adjacent feature scales to enhance the pre-
diction near the boundary. This strategy increases the num-
ber of foreground samples while ensuring that the multi-
scale feature map predicts object heterogeneity. Intuitively,
we assign the interval boundaries to be a series of integer
power of two. As shown in Table 8, our overlapping in-
terval setting improves the detection accuracy of the model
when compared to non-overlapping ones using similar in-
terval boundaries. As shown in Fig. 8, we present a visu-
alization of the mapping between ground truth boxes in the
original image and tokens from feature maps with different
scales.
Interval
AP AP50 AP75
{[-1, 64], [64, 128], [128, 256], [256, ∞]} |50.2 68.2 54.9
non-overlapping
{[-1, 128], [128,256], [256,512], [512, ∞]} 50.2 68.1 54.8
55.0
overlapping {[-1, 64], [64, 256], [128, 512], [256, ∞]} |50.4 68.5
Table 8: Effect of preset scale intervals of multi-scale feature maps
on experimental performance. Interval represents different scale
intervals of multi-scale feature maps and ∞ = 999999 in experi-
ments.
A.2. Supplementary Experiments
A.2.1 Using Swin Transformer as the Backbone
When using Swin Transformer [20] as the backbone, Focus-
DETR also achieves excellent performance. As shown in
the following table, when Focus-DETR uses Swin-T as
the backbone, the AP reaches 51.9 and achieve 56.0AP
using Swin-B-224-22K and 55.9AP using Swin-B-384-
22K. Compared with Deformable DETR [37] and Sparse
DETR [26], our model achieves significant performance
improvements, as shown in Table 9.
A.2.2 Convergence Analysis
In order to better observe the changes in model perfor-
mance with the training epoch, we measured the changes
in Focus-DETR test indicators and compared them with
DINO. Experimental results show that Focus-DETR outper-
forms DINO even at 12 epochs when using ResNet50 as the
backbone, as shown in Table 10. In addition, we found that
the Focus-DETR reached the optimal training state at 24
epochs due to special foreground selection and fine-grained
feature enhancement.
A.2.3 Apply Dual Attention to Other Models
As we mentioned in Section 4.3 of the main text, a pre-
cise scoring mechanism is critical to the proposed dual at-
tention. We add the experiments of applying the encoder
with dual attention to those models equipped with Sparse
DETR, such as Deformable DETR [37], DN DETR [14]
and DINO [36]. As shown in Table 11, the proposed
dual attention for fine-grained tokens enhancement brings
only +0.3AP in Deformable DETR(two-stage), 0.0AP in
Deformable DETR(without two-stage), -0.1AP in DN-
Deformable-DETR and +0.3 AP in DINO. Results show
us that untrusted fine-grained tokens do not bring signifi-
cant performance gains, which is still inefficient compared
to Focus-DETR.
A.3. Visualization
As shown in Fig. 10, we visualize eight test images with
diverse categories, complex backgrounds, overlapping tar-
Model
Epochs
Backbone
AP
AP50
AP75
APS
APM
APL
Params
GFLOPS
FPS
Deformable-DETR
Swin-T
50
48.0
68.0
52.0
30.3
51.4
63.7
41M
185
Sparse DETR
Swin-T
50
49.1
69.5
53.5
31.4
52.5
65.1
41M
129
18.9
Swin-T
36
52.5
70.9
57.5
34.8
55.8
67.6
49M
163
15.3
Focus-DETR
Swin-B-224-22K
36
56.0
74.8
61.1
40.1
59.5
72.0
109M
368
15.3
Swin-B-384-22K
36
56.2
75.1
61.7
38.2
60.0
72.5
109M
390
8.5
Table 9: Results for our Focus-DETR using Swin Transformer as the backbone. Herein, Swin-T indicates the tiny version pretrained on
ImageNet-1K [6]. Swin-B-224-22K represents the base version pretrained on ImageNet-22K [6] and the resolution of training set is 224.
All reported FPS are measured on a NVIDIA V100 GPU.
Model
Backbone | Epochs | AP | AP50 | AP75 | APS | APM | APL
DINO [36]
R-50
R-50
Focus-DETR R-101
Swin-T
2222ELIEFIE
24 504 683 548 34.3 53.7 64.8
24 50.3 68.4 55.1 33.9
24 51.2 69.7 55.9 32.9
53.5 64.4
54.8 656
65.5
12 49.9 68.2 54.3 32.9 52.8 65.1
24 51.9 70.4 56.6 35.4 54.9 67.0
36 52.5 70.9 57.5 34.8 55.8 67.6
Table 10: Focus-DETR uses different backbones at different train-
ing epochs and provides comparison results with DINO [36]. R-50
and R-101 is ResNet backbone, Swin-T represents Swin Trans-
former of the tiny version.
Model
Deformable DETR (priori)
epoch AP GFLOPS FPS
50
46.2
177
19
+ Sparse DETR (a = 0.3)
50
46.0
121
23.2
+ Sparse DETR(dual attention) (a = 0.3)
50
46.3
123
23.0
or + Focus-DETR (a = 0.3)
50
46.6
123
23.0
Deformable DETR (learnable)
50
45.4
173
19
+ Sparse DETR (a = 0.3)
50
43.5
118
24.2
+ Sparse DETR(dual attention) (a = 0.3)
50
43.5
120
23.9
or + Focus-DETR (a = 0.3)
50
45.2
120
23.9
DN-Deformable-DETR (learnable)
50
48.6
195
18.5
+ Sparse DETR (a = 0.3)
50
47.4
137
23.9
+ Sparse DETR(dual attention) (a = 0.3)
50
47.3
138
23.7
or + Focus-DETR (a = 0.3)
DINO (mixed)
50
48.5
138
23.6
36
50.9
279
14.2
+ Sparse DETR (a = 0.3)
36
48.2
152
20.2
+ Sparse DETR(dual attention) (a = 0.3)
36
48.5
154
20.0
or + Focus-DETR (a = 0.3)
36
50.4
154
20.0
Table 11: Apply dual attention to the classic models equipped with
Sparse DETR and compare them with Focus-DETR.
gets, and different scales. We analyze the foreground fea-
tures retained by different encoder layers. Visualization re-
sults show that foreground areas focus on a more refined
area layer by layer in the encoder. Specifically, the result
of Layer-6 captures a more accurate foreground with fewer
tokens. The final test results of Focus-DETR are also pre-
sented, as shown in the first column.
In addition, we compare the differences of multi-scale
feature maps retention object tokens due to our label as-
signment strategy. We also visualize Sparse DETR [26]
to demonstrate the performance. As shown in first col-
umn of Fig. 9, Focus-DETR can obtain more precise fore-
ground than Sparse DETR. According to the results of
{f1, f2, f3, f4}, the multi-scale feature map of Focus-
DETR can retain tokens according to different object scales,
which further proves the advantages of our tag allocation
and top-down score modulations strategy.
Focus
DETR
Sparse
DETR
Focus
DETR
Sparse
DETR
Focus
DETR
Sparse
DETR
fall fifz f3f4
xerc
xerc
Figure 9: Visualized comparison result of foreground tokens reserved in different feature maps. We analyze the difference between Focus-
DETR and Sparse DETR [26] by using three images with obvious object scale differences. fall is the tokens retained by all feature maps,
{ƒ1, f2, ƒ3, ƒ4} represents different feature maps.
Image
Layer-1
Layer-2 Layer-3
Layer-4 Layer-5 Layer-6
N
Figure 10: Visualization result of foreground tokens reserved at each encoder layer, and final detection results are provided. Layer-
{1, 2, 3, ...} indicates different encoder layers.
