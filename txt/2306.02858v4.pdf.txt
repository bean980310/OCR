--- Page 1 ---
arXiv:2306.02858v4 [cs.CL] 25 Oct 2023

BE) Video-LLaMA

An Instruction-tuned Audio-Visual Language Model for Video
Understanding

Hang Zhang!”

Xin Li! ?*

Lidong Bing!”

'DAMO Academy, Alibaba Group
2 Hupan Lab, 310023, Hangzhou, China
{zh401075, xinting.1x, 1.bing}@alibaba-inc.com

Abstract

We present Video-LLaMA! a multi-modal
framework that empowers Large Language
Models (LLMs) with the capability of under-
standing both visual and auditory content in the
video. Video-LLaMA bootstraps cross-modal
training from the frozen pre-trained visual &
audio encoders and the frozen LLMs. Unlike
previous works that complement LLMs to pro-
cess the visual or audio signals only (Zhu et al.,
2023; Liu et al., 2023; Huang et al., 2023a),
Video-LLaMA enables video comprehension
by tackling two challenges: (1) capturing the
temporal changes in visual scenes, (2) integrat-
ing audio-visual signals. To counter the first
challenge, we propose a Video Q-former to as-
semble a pre-trained image encoder into our
video encoder and introduce a video-to-text
generation task to learn video-language cor-
respondence. For the second challenge, we
leverage ImageBind (Girdhar et al., 2023), a
universal embedding model aligning multiple
modalities, as the pre-trained audio encoder
and introduce an Audio Q-former on top of
ImageBind to learn reasonable auditory query
embeddings for the LLM module. To align the
output of both visual & audio encoders with
LLM’s embedding space, we first train Video-
LLaMA on massive video/image-caption pairs
and then tune our model with visual-instruction
datasets of moderate amount but higher qual-
ity. We found Video-LLaMA shows the ability
to perceive and comprehend video content and
generate meaningful responses grounded in the
visual and auditory information presented in
the videos.

1 Introduction

Large Language Models (LLMs) (Chowdhery et al.,
2022; Bai et al., 2022; OpenAI, 2023) have demon-
strated remarkable capability of understanding and

*Xin Li is the corresponding author.
'The video demonstration is available at https: //youtu.
be/RDNYs3Rswhc

following user intentions and instructions*>4. Typ-
ically, the user requests and the corresponding re-
sponses from LLMs are all in texts, however, text-
only human-computer interaction is not sufficient
for many application scenarios because real-world
information is usually multi-modal. In order to
further explore the potential of LLMs, many re-
searchers attempt to endow LLMs with the capabil-
ity of understanding multi-modal content (Huang
et al., 2023a; Zhang et al., 2023b; Yin et al., 2023).

Among these efforts, Alayrac et al. (2022b);
Wang et al. (2022); Huang et al. (2023b); Xu et al.
(2023b); Zhang et al. (2023b); Sun et al. (2023) pre-
train multi-modal LLMs with massive interleaved
image-text data or speech-text data to accommo-
date multi-modal input. Meanwhile, another group
of works adopts a more parameter-efficient way by
complementing LLMs with off-the-shelf vision or
speech foundation models to achieve multi-modal
understanding (Li et al., 2023b; Zhu et al., 2023;
Liu et al., 2023; Ye et al., 2023; Zhang et al., 2023a;
Huang et al., 2023a; Wu et al., 2023b; Su et al.,
2023; Li et al., 2023a).

Despite their effectiveness, these approaches are
dedicated to aligning the input from exactly one
additional modality with text (ie., image or au-
dio), which is unsatisfactory for video understand-
ing. Concretely, empowering LLMs to understand
video requires comprehensive processing for dif-
ferent modalities including visual input, auditory
input, and textual output, which is more challeng-
ing than image-only understanding and audio-only
understanding tasks. Although there are several
recent works attempt to unleash the video under-
standing capability of LLMs (Li et al., 2023c; Maaz
et al., 2023; Luo et al., 2023), their primary objec-
tive is to comprehend only the visual content of the
video, with the auditory content remaining unused.

*https://chat.openai.com/chat
3https://www.anthropic.com/product
‘https://bard.google.com/

--- Page 2 ---
Ability

Model Na . aan 7
oder Name Static Image Silent Video Audio

BLIP2 (Li et al., 2023b)
MiniGPT4 (Zhu et al. 2023)
LLaVA (Liu et al., 2023)

v
v
v
mPLUG-OwI (Ye et al., 2023) v v
VideoChat (Li et al., 2023c¢) v v
AudioGPT (Huang et al., 2023a) v
Video-ChatGPT (Maaz et al., 2023) v v
| Video-LLaMA | v v v |

Table 1: Comparison with popular multi-modal large
language models. Video-LLaMA has the unique ability
to comprehend auditory and visual information simulta-
neously.

In this work, to fill in the blank of audio-visual
LLMs, we investigate the possibility of building
multi-modal LLMs that support the input of video
and allow users to chat with computers around
the user-uploaded video, which is usually com-
posed of multiple video frames and audio. Instead
of employing external perception models to con-
vert visual/auditory signals to textual signals (Shen
et al., 2023; Li et al., 2023c), we choose to build
an end-to-end model that can handle the data from
multiple modalities within one single framework.
Specifically, we adopt the idea of BLIP-2 (Li et al.,
2023b) to guarantee the efficiency of cross-modal
pre-training. To explicitly capture the change of
visual scenes in the video, we use a pre-trained
visual encoder to separately compute frame repre-
sentations. Then, we introduce a frame embedding
layer to inject temporal information and a video
Q-Former to generate visual query tokens. As for
the audio signals from the video, we additionally
leverage a pre-trained audio encoder as well as an
audio Q-former to learn reasonable auditory query
embeddings (see the right part of Figure 1).

To align textual output with video, we devise
multi-branch cross-modal pre-training to learn the
vision-language correspondence and the audio-
language correspondence. For vision-language cor-
respondence, we first pre-train the vision-related
components on a large-scale video caption dataset
with a video-clips-to-text generation task. To
enhance the understanding of static visual con-
cepts, we also add image-caption data into this
pre-training stage. Then, we further fine-tune these
components on a video-based conversation dataset
to execute visual instruction tuning. For the align-
ment between the audio encoder and language de-
coder, we further pre-train the audio-related com-
ponents on an audio caption dataset with an audio-

to-text generation task. For the audio-language
correspondence, we leverage Imagebind (Girdhar
et al., 2023) as an encoder, which performs excep-
tionally well in aligning different modalities to a
common embedding space. Given the limited avail-
ability of audio-text data, we also utilize vision-text
data to train the audio-related components. These
components learn to align the common embedding
space provided by Imagebind with the embedding
space of LLMs. Despite not being explicitly trained
with audio-text data, Video-LLaMA exhibits a re-
markable zero-shot audio understanding capability
during inference.

As shown in Table 1, our Video-LLaMA stands
out from other existing multi-modal LLMs in terms
of its distinctively comprehensive comprehension
of audiovisual modal information in videos. In
summary, our contributions are as follows:

e We propose Video-LLaMA, a multi-modal
framework that enables LLM to simultaneously
process both the visual and auditory content of a
given video and engage in conversation with hu-
mans.

e To empower LLMs with video understanding
capability, we propose a multi-branch cross-modal
pre-training framework to achieve both vision-
language alignment and audio-language alignment.

e We open-source the entire codebase for pre-
training and fine-tuning as well as the model
weights of all the variants of Video-LLaMA°. We
also prepared the demos for video-grounded con-

versation®”.

2 Method

Video-LLaMA aims to empower frozen LLMs with
the capability of understanding both visual and au-
ditory content in videos. As shown in Figure 1,
we design two branches, namely Vision-Language
Branch and Audio-Language Branch, to respec-
tively transform the video frames and audio signals
into query representations that are compatible with
the textual inputs of LLMs. In this section, we first
introduce the overall architecture and the building
blocks of each branch. Then, we delineate the pro-
cedures of the proposed multi-branch cross-modal
pre-training and audio-visual instruction tuning.

Shttps: //github. com/DAMO-NLP-SG/Video-LLaMA

°https: //huggingface.co/spaces/DAMO-NLP-SG/
Video-LLaMA

Thttps: //modelscope.cn/studios/damo/
video-llama/summary

--- Page 3 ---
:This video is an animation of a rocket 1
launching from a launch pad at night... |

Vision-Language Branch

Visual Encoder
(ViT & Q-Former)

Figure 1: Overall architecture of Video-LLaMA.

2.1 Architecture

2.1.1 Vision-Language Branch

The Vision-Language Branch is designed for en-
abling the LLMs to understand visual inputs. As
shown in the left part of Figure 1, it is composed
of a frozen pre-trained image encoder to extract
features from video frames, a position embedding
layer to inject temporal information into video
frames, a video Q-former to aggregate frame-level
representations and a linear layer to project the
output video representations into the same dimen-
sion as the text embeddings of LLMs. Given one
video consists of N frames, the image encoder will
first map each frame/image into Ky image embed-
ding vectors, yielding video frame representations
V = [v1, Va,.--, Vn] where v; € R*/*4* is the
set of ds-dimensional image embeddings corre-
sponding to the i-th frame.

Since the frame representations v; from the
frozen image encoder are computed without consid-
ering any temporal information, we further apply
position embeddings as the indicator of temporal
information to the representations from different
frames. Then, we feed the position-encoded frame
representations to Video Q-former, which shares
the same architecture with Query Transformer (Q-
Former) in BLIP-2 (Li et al., 2023b), to obtain ky
video embedding vectors of dimension d, as the
representation ¥ € R*V 4» of the video.

To adapt the video representations to the input of
LLMs, we add a linear layer to transform the video
embedding vectors into the video query vectors.
The video query vectors are of the same dimension
as the text embeddings of LLMs. In the forward
pass, they will be concatenated to text embeddings
as a video soft prompt and guide the frozen LLMs

--- Page 4 ---
to generate text conditioned on video content.

As for the implementation of the Vision-
Language Branch, we utilize the pre-trained vi-
sion component of BLIP-2 (Li et al., 2023b) as
the frozen visual encoder, which includes a ViT-
G/14 from EVA-CLIP (Fang et al., 2022) and a
pre-trained Q-former. The remaining components,
including the position embedding layer, Video Q-
former, and Linear layer are randomly initialized
and optimized to well connect the output of the
frozen visual encoder to frozen LLMs.

2.1.2 Audio-Language Branch

To deal with the auditory content of the given video,
we introduce the Audio-Language Branch. Con-
cretely, it consists of a pre-trained audio encoder
to compute features given a short segment of ori-
gin audio, a position embedding layer to inject
temporal information to audio segments, an audio
Q-former to fuse the features of different audio
segments, and a linear layer to map the audio rep-
resentation into the embedding space of LLMs.

In practice, we utilize the pre-trained Image-
bind (Girdhar et al., 2023) as the audio encoder.
We first uniformly sample / segments of 2-second
short audio clips from the video, then convert each
2-second audio clip into spectrograms using 128
mel-spectrogram bins. After obtaining the spec-
trogram list of input audio, the audio encoder will
map each spectrogram into a dense vector. So the
generated audio representation of the given video
can be denoted as A = [a1, ag, ..., an].

Similar to Video Q-Former, the Audio Q-former
injects temporal information by adding learnable
positional embeddings to audio segments. It then
generates fixed-length audio features by computing
the interaction across the position-encoded audio
segments. Audio Q-Former adopts the same archi-
tecture as Q-Former. It projects the variable-length
audio representation list A into a fixed-length se-
quence Ae IR¥«xda where the K, is the number
of audio embedding vectors and d, is the dimen-
sion of each vector. Finally, we employ a linear
layer to map audio features to the embedding space
of the LLM.

2.2, Multi-branch Cross-Modal Training

We train the vision-language and audio-language
branches separately. In the first stage, large-
scale vision-caption datasets are used for training,
and in the second stage, high-quality instruction-
following datasets were used for fine-tuning. The

image is treated as a one-frame video.

2.2.1 Training of Vision-Language Branch

For pre-training vision-language branch, we uti-
lized Webvid-2M (Bain et al., 2021), a large-scale
dataset of short videos with textual descriptions
sourced from stock footage sites. Moreover, we em-
ployed the image caption dataset CC595k, which
is sourced from CC3M (Sharma et al., 2018) and
filtered by Liu et al. (2023). We adopt a video-to-
text generation task during the pre-training stage,
ie., given the representation of a video, prompting
the frozen LLM to generate the corresponding text
description. We find that a significant portion of
textual descriptions are insufficient to reflect the en-
tire content of the videos. Therefore, the visual se-
mantics in the videos are not fully aligned with the
textual semantics in the video descriptions. Never-
theless, this stage aimed to utilize a vast amount of
data and enable video features to contain as much
visual knowledge as possible. We left the abilities
of vision-text alignment and instruction-following
for the next stage.

After the pre-training stage, the model can gen-
erate content about information in the video, but its
ability to follow instructions has decreased. There-
fore, in the second stage, we fine-tuned the model
using high-quality instruction data. We integrated
the image-detail-description dataset from MiniGPT-
4 (Zhu et al., 2023), the image-instruction dataset
from LLaVA (Liu et al., 2023), and the video-
instruction dataset from Video-Chat (Li et al.,
2023c). After fine-tuning, Video-LLaMA exhibited
remarkable abilities in following instructions and
comprehending images and videos.

2.2.2 Training of Audio-Language Branch

Training the audio-language branch directly using
audio-text data is highly challenging due to the
rarity of such data. The objective of the learn-
able parameters in the audio-language branch is
to align the output embedding of the frozen au-
dio encoder with the embedding space of LLM.
Given the scarcity of audio-text data, we employ a
workaround strategy to achieve this objective. Im-
ageBind, which is used as our audio encoder, has a
remarkable ability to align different modalities’ em-
beddings to one common space, demonstrating im-
pressive performance on cross-modal retrieval and
generation tasks. In light of the scarcity of audio-
text data and the abundance of visual-text data, we
train the audio-language branch using visual-text

--- Page 5 ---
data, following the same data and process as the vi-
sion branch. Thanks to the shared embedding space
provided by ImageBind, Video-LLaMA exhibits
the ability to comprehend audio during inference,
even though the audio interface has never been
trained on audio data.

3 Related Works

Large Language Models: Large language mod-
els (LLMs) (Black et al., 2022; Scao et al., 2022;
OpenAI, 2023; Tsimpoukelli et al., 2021) have
demonstrated remarkable language understanding
and reasoning abilities, enabling the generation of
high-quality natural language text across various
domains, including articles, conversations, stories,
and poetry. LLMs have already sparked a techno-
logical revolution and have been widely applied
in different applications. Moreover, a series of
open source large models, such as LLaMA (Tou-
vron et al., 2023), BLOOM (Scao et al., 2022) and
OPT (Zhang et al., 2022), have greatly promoted
technological advancement and made outstanding
contributions to the NLP community. Building
upon these LLMs, researchers have further ex-
tended their capabilities and developed excellent
models for various NLP tasks. Examples include
Vicuna (Chiang et al., 2023) and Baize (Xu et al.,
2023a). Our work is based on these LLMs and
provides plug-and-play plugins that empower them
with the capability of comprehending both visual
and auditory content in videos.

Multi-modal Large Language Models: Re-
searchers have been actively exploring the use
of LLMs for processing multi-modal inputs (Gao
et al., 2023; Li et al., 2023c). Existing approaches
can be categorized into two main groups. The
first category involves employing LLMs as con-
trollers and utilizing existing multi-modal models
as tools. In this approach, when receiving the user’s
text instruction, the LLM recognizes the user’s in-
tention and makes decisions about which tools to
call. It then generates comprehensive responses by
incorporating the results obtained from these off-
the-shelf multi-modal models. Examples include
ChatGPT (Wu et al., 2023a), HuggingGPT (Shen
et al., 2023), and AudioGPT (Huang et al., 2023a).
The second category focuses on training funda-
mental large-scale multi-modal models. The key
idea of this line of work is to align the pre-trained
foundation models for other modalities to textual
LLMs. For instance, Flamingo (Alayrac et al.,

2022a) utilizes a perceiver resampler and a gated
cross-attention layer to connect a frozen image en-
coder and LLM. BLIP2 (Li et al., 2023b) intro-
duces a Q-Former to map learned image queries
to the textual embedding space of LLMs. (Liu
et al., 2023), mPLUG-owl (Ye et al., 2023) and
MiniGPT4 (Zhu et al., 2023) develop instruction-
following image-LLMs using image-instruction-
following dataset. Video-Chat (Li et al., 2023c)
and Video-ChatGPT (Maaz et al., 2023) extend im-
age encoders to video encoders and connect them
with LLMs to understand visual content in videos.
PandaGPT (Su et al., 2023) utilizes multi-modal
encoders from ImageBind, trained exclusively on
image-instruction pairs, to enable large models to
understand six modalities. Our work falls into the
second category, where we train fundamental mod-
els to comprehend both the visual and auditory
content in videos.

4 Examples

In this section, we show some cases to demonstrate
Video-LLaMA’s multi-modal instruction-following
capability in video/audio/image-grounded conver-
sations. Figure 2 shows some of these examples,
while additional cases can be found in Appendix A.

(1) Audio-visual integration perception ability.
Figure 2(a) and Figure 3 show Video-LLaMA’s
unique ability to comprehend auditory and visual
information simultaneously. The videos in both
cases contain audio. In each conversation, we pose
two questions related to visual and auditory content
respectively. If the model could only receive one
modal, it would be unable to answer both of these
questions. However, we can observe that Video-
LLaMA accurately responds to both visual and
auditory questions in both cases.

(2) The ability to capture temporal dynamics
in videos. Figure 2(b) and Figure 4 illustrate the
capability of Video-LLaMA to identify actions over
time. It successfully describes the actions of the
girl and the moving direction of the boat.

(3) The ability to perceive and understand static
images. Figure 2(c) and Figure 5 show Video-
LLaMA’s ability to perceive and understand pic-
tures. Figure 2(c) demonstrates Video-LLaMA’s
ability to understand the concept of "unusual" and
specifically describe the unusual scene. In Figure 5,
not only does Video-LLaMA accurately describe

--- Page 6 ---
hear sounds of footsteps on the floor in the background,
and there is also a dog barking in the house.

(Does theman wear ase? |

| Yes, he wears a pair of glasses in the video.

(a) A case that Video-LLaMA answers the questions based
on the background sound and visual content of the video

Unusual about this image is that a man in a yellow shirt is
standing on top of a parked car while holding an ironing board
and pressing clothes

(c) A case demonstrating Video-LLaMA’s ability to com-
prehend static images.

{summarise this vdeo in one sentence. | |

The video shows a beautiful scenery of a cherry blossom
lined river flowing by a boat on the water, and a cityscape
with tall buildings in the background.

(What recon ste stp one | dla

fl | The ship is going towards the right side of the video.

(b) A case showing Video-LLaMA’s ability to identify ac-
tions over time.

The building in the image appears to be the United States,
Capitol. Itis a neoclassical style building located in Washington,
D.C,, the capital of the United States. It serves as the home of
the U.S. Congress and is one of the most iconic landmarks in
the country

(d) A case demonstrating Video-LLaMA’s ability to recog-
nize famous landmarks.

Figure 2: Some examples generated by Video-LLaMA.

the main content, but it also associates it with the
friendly interaction between a dog and a human.

(4) The ability of common-knowledge concept
recognition. Figure 2(d) and Figure 6 demon-
strate Video-LLaMA’s remarkable capacity for rec-
ognizing common-knowledge concepts in visual
signals. Video-LLaMA successfully recognizes fa-
mous landmarks and characters and can engage in
common-sense question-answering.

5 Conclusion

In this paper, we present Video-LLaMA, a cutting-
edge multi-modal framework that empowers large
language models with both audio & video under-
standing capabilities. Our experiments demon-
strated the impressive abilities of Video-LLaMA
in audio and video-grounded conversations, high-
lighting its potential as a promising prototype for
audio-visual AI assistants. We have open-sourced
the entire training code and various model weights,
along with detailed instructions to assist developers
in utilizing our code for further development. In ad-
dition, we have provided online demo websites and
offline demo deployment guides for users to experi-

ence Video-LLaMA’s capabilities firsthand. We are
committed to constantly maintaining and improv-
ing Video-LLaMA, and will continue to contribute
to the open-source community.

6 Limitations

Although Video-LLaMA has demonstrated impres-
sive abilities in understanding both visual and au-
ditory content in videos, it is still an early-stage
prototype and has some limitations, including: (1)
Limited perception capacities: Video-LLaMA’s
performance is hindered by the quality and scale
of the current training dataset. We are actively con-
structing a high-quality audio-video-text alignment
dataset to enhance the model’s perception capa-
bilities. (2) Limited ability to handle long videos.
Long videos(such as movies, and TV shows) con-
tain a large volume of information and impose
higher demands on computational resources. This
challenge remains a crucial issue that the research
community is actively working to address. (3) Hal-
lucination. Video-LLaMA inherits the hallucina-
tion problem from the frozen LLMs. We will con-
tinue to address these challenges and develop more
powerful versions for video understanding.

--- Page 7 ---
References

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022a. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems, 35:23716-23736.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katie Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda
Han, Zhitao Gong, Sina Samangooei, Marianne
Monteiro, Jacob Menick, Sebastian Borgeaud, Andy
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko-
laj Binkowski, Ricardo Barreira, Oriol Vinyals,
Andrew Zisserman, and Karen Simonyan. 2022b.
Flamingo: a visual language model for few-shot
learning. arXiv preprint arXiv:2204.14198.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073.

Max Bain, Arsha Nagrani, Giil Varol, and Andrew Zis-
serman. 2021. Frozen in time: A joint video and
image encoder for end-to-end retrieval. In IEEE In-
ternational Conference on Computer Vision.

Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al.
2022. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
2023. Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.

Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell
Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,
and Yue Cao. 2022. Eva: Exploring the limits of
masked visual representation learning at scale. arXiv
preprint arXiv:2211.07636.

Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-
jie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui
He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao.
2023. Llama-adapter v2: Parameter-efficient visual
instruction model. arXiv preprint arXiv:2304.15010.

Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-
nat Singh, Kalyan Vasudev Alwala, Armand Joulin,

and Ishan Misra. 2023. Imagebind: One embed-
ding space to bind them all. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 15180-15190.

Rongjie Huang, Mingze Li, Dongchao Yang, Jia-
tong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,
Zhiging Hong, Jiawei Huang, Jinglin Liu, et al.
2023a. Audiogpt: Understanding and generating
speech, music, sound, and talking head. arXiv
preprint arXiv:2304.12995.

Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Ly, Lei
Cui, Owais Khan Mohammed, Qiang Liu, Kriti Ag-
garwal, Zewen Chi, Johan Bjorck, Vishrav Chaud-
hary, Subhojit Som, Xia Song, and Furu Wei.
2023b. Language is not all you need: Aligning
perception with language models. arXiv preprint
arXiv:2302.14045.

Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. 2023a. Otter: A
multi-modal model with in-context instruction tuning.
arXiv preprint arXiv:2305.03726.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023b. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. arXiv preprint arXiv:2301.12597.

Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen
Wang, Ping Luo, Yali Wang, Limin Wang, and
Yu Qiao. 2023c. Videochat: Chat-centric video un-
derstanding. arXiv preprint arXiv:2305.06355.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning. arXiv preprint
arXiv:2304.08485.

Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,
Ming-Hui Qiu, Pengcheng Lu, Tao Wang, and
Zhongyu Wei. 2023. Valley: Video assistant with
large language model enhanced ability. arXiv
preprint arXiv:2306.07207.

Muhammad Maaz, Hanoona Rasheed, Salman Khan,
and Fahad Shahbaz Khan. 2023. Video-chatgpt:
Towards detailed video understanding via large
vision and language models. —_arXiv_ preprint
arXiv:2306.05424.

OpenAI. 2023. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774.

Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili¢é, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, Frangois Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.

Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic im-
age captioning. In Proceedings of the 56th Annual

--- Page 8 ---
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 2556-2565.
Association for Computational Linguistics.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580.

Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan
Wang, and Deng Cai. 2023. Pandagpt: One
model to instruction-follow them all. arXiv preprint
arXiv:2305.16355.

Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,
Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang.
2023. Generative pretraining in multimodality. arXiv
preprint arXiv:2307.05222.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Roziére, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-
timodal few-shot learning with frozen language mod-
els. Advances in Neural Information Processing Sys-
tems, 34:200-212.

Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,
and Hongxia Yang. 2022. Unifying architectures,
tasks, and modalities through a simple sequence-to-
sequence learning framework. In International Con-
ference on Machine Learning.

Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong
Wang, Zecheng Tang, and Nan Duan. 2023a.
Visual chatgpt: Talking, drawing and editing
with visual foundation models. arXiv preprint
arXiv:2303.04671.

Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yilun
Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren,
Linquan Liu, and Yu Wu. 2023b. On decoder-only
architecture for speech-to-text and large language
model integration. arXiv preprint arXiv:2307.03917.

Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
2023a. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv
preprint arXiv:2304.01196.

Haiyang Xu, Qinghao Ye, Mingshi Yan, Yaya Shi, Ji-
abo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi-
uchen Qian, Wei Wang, Guohai Xu, Ji Zhang, Song-
fang Huang, Feiran Huang, and Jingren Zhou. 2023b.
mplug-2: A modularized multi-modal foundation
model across text, image and video. arXiv preprint
arXiv:2302.00402.

Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming
Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng
Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong
Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and
Feiyan Huang. 2023. mplug-owl: Modularization
empowers large language models with multimodality.
arXiv preprint arXiv:2304.14178.

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing
Sun, Tong Xu, and Enhong Chen. 2023. A survey on
multimodal large language models. arXiv preprint
arXiv:2306.13549.

Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan
Liu, and Tat-Seng Chua. 2023a. Transfer vi-
sual prompt generator across Ilms. arXiv preprint
arXiv:23045.01278.

Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan,
Peng Peng Wang, Yaqian Zhou, and Xipeng Qiu.
2023b. Speechgpt: Empowering large language mod-
els with intrinsic cross-modal conversational abilities.
arXiv preprint arXiv:2305.11000.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068.

Deyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592.

A Appendix

--- Page 9 ---
The audience was really enthusiastic and clapped loudly for

| the performer.

He is playing a saxophone on stage at the concert.

Figure 3: A case showing Video-LLaMA’s ability to identify the sound of applause in a video and infer the positive

response from the audience. Additionally, it infers that a man is playing the saxophone on stage based on the visual
content.

AA
[ecsinaipci |

FF. The video shows a young red-haired woman, who is
wearing a striped shirt, posing for the camera in front of a
gray background. She is holding her fingers up to her mouth,
as if she is trying to be quiet, and the caption mentions her
long hair and red lips. The woman is young, with fair skin,
and looks calm and confident. She is holding her fingers to
her mouth, which suggests she might be trying to signal
something to the camera or trying to convey a message.

Figure 4: A case where Video-LLaMA provides a detailed description of the visual content in a dynamic video.

--- Page 10 ---
/ The image features a beautiful and playful golden retriever

J i running on a bright green lawn. The dog is running with a
tennis ball in its mouth, as it is an active and lively animal
that enjoys playing and having fun. The green grass in the
background creates a vibrant contrast with the golden color
of the dog's coat, adding a touch of color and texture to the
scene. The dog's fur is well-groomed, shiny, and nicely
groomed, highlighting its health and appearance. The
brightness of the green lawn and the playful behavior of the
dog suggest a joyful atmosphere, inviting viewers to engage
in this lovely scenery and the dog's energetic play.

It's worth mentioning that the dog's focus and dedication to
playing with the ball also showcase its natural instincts and
its hunting abilities. Additionally, this image captures a fun
and heartwarming moment of bonding between the dog and
its human companion, as the dog's positive attitude towards
the ball reflects its enjoyment of interacting with its owner
and its environment. Overall, the image conveys an inviting
\ _and joyful atmosphere.

Figure 5: A case where Video-LLaMA provides a detailed description of the static image content.

--- Page 11 ---
The man on the right is Jon Snow, played by Kit Harington in the
television series Game of Thrones.

De

The lady on the left is likely Daenerys Targaryen, played by Emilia
Clarke in the television series Game of Thrones.

Be

Jon Snow and Daenerys Targaryen are both characters in the
television series Game of Thrones. In the show, they have a
romantic relationship. They first meet in the seventh season, and
their attraction to each other grows over the course of the eighth
season. Despite their initial attraction, they have a complicated
relationship due to their respective backgrounds and the
challenges they face in the series

Figure 6: A case showing Video-LLaMA’s ability to recognize renowned characters and participate in video-
grounded question answering.

