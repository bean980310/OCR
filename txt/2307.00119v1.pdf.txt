--- Page 1 ---
arXiv:2307.00119v1 [cs.CL] 30 Jun 2023

Meta-training with Demonstration Retrieval for
Efficient Few-shot Learning

Aaron Mueller!* Kanika Narang?

Qifan Wang?

Lambert Mathias?
Hamed Firooz”

! Johns Hopkins University, Baltimore, MD
? Meta AI, Menlo Park, CA
amueller@jhu.edu, {kanika13,mathiasl,wqfcr,mhfirooz}@meta.com

Abstract

Large language models show impressive results
on few-shot NLP tasks. However, these models
are memory and computation-intensive. Meta-
training allows one to leverage smaller mod-
els for few-shot generalization in a domain-
general and task-agnostic manner (Min et al.,
2022a; Wei et al., 2022; Chen et al., 2022);
however, these methods alone results in mod-
els that may not have sufficient parameteri-
zation or knowledge to adapt quickly to a
large variety of tasks. To overcome this is-
sue, we propose meta-training with demon-
stration retrieval, where we use a dense pas-
sage retriever to retrieve semantically similar
labeled demonstrations to each example for
more varied supervision. By separating exter-
nal knowledge from model parameters, we can
use meta-training to train parameter-efficient
models that generalize well on a larger vari-
ety of tasks. We construct a meta-training set
from UNIFIEDQA and CROSSFIT, and propose
a demonstration bank based on UNIFIEDQA
tasks. To our knowledge, our work is the first
to combine retrieval with meta-training, to use
DPR models to retrieve demonstrations, and
to leverage demonstrations from many tasks
simultaneously, rather than randomly sampling
demonstrations from the training set of the tar-
get task. Our approach outperforms a variety
of targeted parameter-efficient and retrieval-
augmented few-shot methods on QA, NLL
and text classification tasks (including SQUAD,
QNLI, and TREC). Our approach can be meta-
trained and fine-tuned quickly on a single GPU.

1 Introduction

Large language models (LLMs) have become in-
creasingly popular due to their impressive few-
shot performance on many NLP tasks and domains
(Brown et al., 2020; Chowdhery et al., 2022). This
has resulted in many few-shot learning methods
based on LLMs that require ever-larger GPUs and

*Work done as an intern at Meta.

question: How many
casualties were there
in World War 1? Output (y))
\n answer: 40,000,000

Retrieved
Demonstrations,

(adi)

‘Memory Bank (7) t
question: How many
casualties were there

in World War 17 | input (x)
\n answer: <mask>
\n context: —

BoolQ )

Meta-training Tasks

Figure 1: Our approach. Given an input x from one
of many possible QA tasks, we use a dense passage
retriever to retrieve A’ semantically similar demonstra-
tions Z = {z}1,...,« from a memory bank z composed
of labeled examples. We meta-train BART, supervising
it to generate the (question and) answer y given x and
Z across a diverse collection of QA tasks.

increasing computation. Methods requiring no pa-
rameter updates such as in-context learning (Brown
et al., 2020) and parameter-efficient methods like
Adapters (Houlsby et al., 2019) partially mitigate
these downsides, but ultimately, larger computation
budgets are increasingly necessary to achieve state-
of-the-art few-shot performance—even to simply
load models and perform inference.
Meta-learning (Vilalta and Drissi, 2002; Finn
et al., 2017) and meta-training (Min et al., 2022a)
are methods that make smaller language models
capable of quicker and more robust few-shot per-
formance across multiple tasks and domains. How-
ever, smaller models may not be able to store
enough knowledge for effective generalization in
many domains and tasks simultaneously. Retrieval
is one way to overcome this: by separating para-
metric knowledge in the language model from ex-
ternal knowledge (stored as retrievable text), one
can leverage much more information than could be
stored in the parameters of a language model. For
example, retrieval-augmented generation (RAG;

--- Page 2 ---
Lewis et al., 2020) and retrieval-enhanced trans-
formers (RETRO; Borgeaud et al., 2022) retrieve
natural language passages to improve performance
on knowledge-intensive NLP tasks, although they
do not perform meta-learning or meta-training
and only evaluate on high-resource knowledge-
intensive tasks.

We thus propose meta-training with demon-
stration retrieval as a more parameter-efficient
way to leverage demonstrations for few-shot learn-
ing. We retrieve semantically similar labeled
demonstrations for each training and test exam-
ple during meta-training and fine-tuning. On
a relatively small sequence-to-sequence model
(BARTharge, 440M parameters), we show our pro-
posed approach is capable of generalizing quickly
and well on a variety of downstream tasks (Table 1).
Inspired by retrieval-augmented generation (RAG)
models (Lewis et al., 2020), we use a dense passage
retriever (DPR; Karpukhin et al., 2020) to retrieve
demonstrations instead of Wikipedia passages. We
retrieve semantically similar demonstrations from
a large and diverse bank (§3.3) that is compiled
from many existing question answering tasks (App.
A), rather than randomly sampling demonstrations
from the training set of the target task like most
contemporary work (Min et al., 2022a; Brown et al.,
2020; Gao et al., 2021).

Our experiments show that our method (§3) out-
performs tailored efficient few-shot baselines and
other retrieval-augmented models on various tasks,
including natural language inference (NLD, para-
phrase detection, and extractive question answer-
ing (§5). To our knowledge, our work is the first
to combine retrieval with meta-training (or multi-
task training more broadly), to use DPR models
to retrieve demonstrations, and to leverage demon-
strations from many tasks simultaneously, rather
than retrieving random or k-nearest demonstrations
from the training set of the target task.

Our code is available on GitHub.!

2 Related Work

Meta-learning (Vilalta and Drissi, 2002; Finn
et al., 2017) is a class of methods that supervise
a model on how to learn; the goal is to leverage
a collection of meta-training tasks to learn a bet-
ter learning algorithm that generalizes to held-out
tasks. Inspired by meta-learning, some recent stud-

‘https: //github.com/facebookresearch/
metatrained-demRAG

ies have attempted to induce specific abilities in
language models in a task- and domain-agnostic
manner via meta-training; this entails directly
supervising a model on labeled examples from
various tasks (sometimes using some controlled
format or template (Chen et al., 2022; Wei et al.,
2022)) to directly induce specific abilities or better
inductive biases that improve generalization. Meta-
training is typically accomplished via some form
of controlled multi-task learning, as in Min et al.
(2022a). Many studies have explored multi-task
and multi-domain learning (Khashabi et al., 2020;
Zhong et al., 2021; Aghajanyan et al., 2021; Ye
et al., 2021; Wei et al., 2022), but these studies
often leverage tasks that improve a model’s abili-
ties for some specific (set of) downstream tasks. In
meta-training, we aim to directly improve the learn-
ing algorithm via controlled supervision, which
should improve out-of-distribution generalization
by teaching a model some helpful ability—such
as in-context learning—that can result in gains on
various downstream tasks (Min et al., 2022a). We
focus on meta-training with examples from QA
datasets.

Few-shot learning is a common setting in
which a model is supervised on only a few la-
beled examples. Many methods for improving few-
shot performance are based on scaling model and
data size (Brown et al., 2020; Chowdhery et al.,
2022). Our goal is to improve few-shot perfor-
mance across tasks in a computation- and memory-
efficient manner, so we focus on smaller models
that can be trained efficiently on a single GPU.
Some parameter-efficient few-shot methods have
been proposed, including cloze-style prompting
(Schick and Schiitze, 2021b), fine-tuning with man-
ually tuned (Schick and Schiitze, 2021a) and auto-
matically tuned prompts and demonstrations (Gao
et al., 2021), and meta-learning (Yu et al., 2018;
Bansal et al., 2020; Bao et al., 2020). One advan-
tage of our approach is that it does not require sig-
nificant prompt tuning: rather, we standardize all
of our tasks into a single format, similar to Chada
and Natarajan (2021). This saves human time and
computational resources.

Crucially, these approaches compare probabil-
ities of single tokens or small pre-selected label
sets; thus, they cannot be used for open-domain
tasks like question answering. Some work has
proposed generative few-shot methods for open-
domain tasks: this includes reformatting the input

--- Page 3 ---
data to match a model’s pre-training format (Chada
and Natarajan, 2021), pre-training models to select
relevant spans from context passages (Ram et al.,
2021), and running a secondary pre-training step
on labeled classification data (Mueller et al., 2022).
Our model should be effective on many tasks, even
when the label space is large and differs across ex-
amples; thus, our method is based on a generative
sequence-to-sequence model.

In-context learning (ICL; Brown et al., 2020)
is increasingly used in few-shot methods; here, la-
beled demonstrations are concatenated to the same
context as a test example to teach a model how to
perform a task without additional gradient updates.
Studies have analyzed what kinds of demonstra-
tions are most effective (Liu et al., 2022), as well
as what makes demonstrations effective (Min et al.,
2022b; Xie et al., 2022). Our demonstration re-
trieval approach is most similar to Liu et al. (2022),
who encode demonstrations and test examples into
a sentence embedding space and retrieve the k-
nearest demonstrations. Our method differs in mul-
tiple ways: we use dense passage retrievers instead
of sentence embeddings; we use demonstrations
from many training sets instead of the training set
of the target task; and we perform gradient updates
with demonstrations, which is more feasible on our
relatively small BARTiarge-based model.

Wei et al. (2022) find that very large LMs (>68B
parameters) are required for ICL to be effective, but
Min et al. (2022a) find that meta-training can be
used to make a much smaller model (GPT2jarge,
774M parameters) capable of leveraging demon-
strations. Here, we make BART arge (440M param-
eters) better at leveraging demonstrations through
meta-training with demonstrations, like Min et al.
(2022a); however, their method is designed for
zero-shot generalization, and it selects from a con-
strained set of pre-defined labels. Our method is
designed for few-shot settings and can be applied
to open-domain tasks.

Retrieval-augmented generation models con-
sist of two components: generators and retriev-
ers. The generator is typically a decoder-only
LM (Guu et al., 2020) or sequence-to-sequence
(seq2seq) model (Lewis et al., 2020; Izacard and
Grave, 2021); we use seq2seq models. The re-
triever is most often a dense passage retrieval
(DPR; Karpukhin et al., 2020) model based on
BERThase. RAG models are typically evaluated
on knowledge-intensive tasks like abstractive QA

and fact verification. Thus, the memory bank typ-
ically consists of Wikipedia passages, which aug-
ments the model with additional factual knowledge
separate from the generator’s parameters. Izacard
et al. (2022) adapts this architecture for few-shot
knowledge-intensive tasks using a very large gen-
erator (T5x(xyL) and a Contriever-based (Izacard
et al., 2021) retriever. However, we are interested
in more general-purpose methods, as well as more
parameter- and memory-efficient methods that train
or fine-tune quickly on a single GPU. Thus, we pro
pose a task-agnostic and domain-general method
to improve smaller generative models for few-shot
settings: specifically, a retrieval-augmented meta-
training step and a memory bank of labeled QA
demonstrations instead of Wikipedia passages.

3 Method

3.1 Retrieval-augmented Generation

As we wish to retrieve similar labeled examples
for every input, our architecture takes inspiration
from retrieval-augmented generation (RAG) mod-
els (Lewis et al., 2020), which consist of a pre-
trained sequence-to-sequence component (we use
BARThiarge) and a pre-trained dense passage re-
triever (DPR) component. Given an input x, the
DPR component retrieves the A’ most semantically
similar memory entries {z;}1,..,« from the mem-
ory bank z. Retrieval is performed using a BERT-
based input encoder E; on x and BERT-based
demonstration encoder Ep on z to encode both
into a vector space, and then running maximum
inner product search:”

{eho = top-K {Ey(x)"Ep(2))} A)

22

The DPR component also returns the inner products
themselves as document scores pp(z|2).

The input and retrieved entries are then passed
to a pre-trained sequence-to-sequence model,
BARTharge, for autoregressive generation. At each
timestep, we marginalize over the retrieved demon-
strations by creating A separate input contexts,
consisting of the input x and one retrieved entry
zz. We then sum over BART’s token probabilities
po given each context, weighted by z,’s document

?Maximum inner product search can be approximately
solved in sub-linear time (Johnson et al., 2021). We
use the faiss library for this: https://github.com/
facebookresearch/faiss

--- Page 4 ---
Category Dataset Type #Train #Test L \y|
SQuAD (Rajpurkar et al., 2016) Open QA 86,588 10,507 10/120 -
Extractive QA BioASQ (Tsatsaronis et al., 2015) Open QA 24,559 1,504 10/200 -
QASC (Khot et al., 2020) Multi-choice QA 8,134 926 «8/18
Knowledge- TriviaQA (Joshi et al., 2017) Open QA 61,688 7,785 13/677 -
intensive QA TextbookQA (Kembhavi et al., 2017) Open QA 15,154 1,503 10/581 -
TREC (Voorhees and Tice, 2000) S 5,452 500 10 6
MRPC (Dolan and Brockett, 2005) 2 3,668 408 22/21 2
Classification MNLI (Williams et al., 2018) NLI 392,702 9,815 22/11 3
MNLI-mm (ibid.) NLI 392,702 9,832 22/11 3
QNLI (Wang et al., 2018) NLI 104,743 5,463 11/30 3

Table 1: Evaluation sets used in this study. LZ: mean # of words in question/context or input sentence(s). For more
straightforward comparison to prior few-shot question answering and classification methods, we use Ram et al.
(2021)’s few-shot splits of SQUAD and BioASQ derived from MRQA, as well as Gao et al. (2021)’s splits of TREC,
MRPC, MNLI(-mm), and QNLI. We generate our own few-shot splits of QASC using 5 random seeds for each split

size.

score:

N K

v(y|x) © TL. pn (eal)po ler, 20, ys) (2)
ik

3.2 Meta-training

To adapt a sequence-to-sequence model for general-
purpose demonstration retrieval and answer gener-
ation, we perform a meta-training step by supervis-
ing the model with demonstrations on a collection*
of 18 QA tasks (Table 7). We update the parame-
ters of the BART component of our model during
meta-training by supervising BART (using its nor-
mal cross-entropy loss) to generate the question
and its answer given the question and a set of re-
trieved demonstrations. We use QA tasks due to the
semantic diversity of inputs and labels; compare
to text classification tasks, where the label space is
much smaller and labels are often less informative.

We modify and use the QA meta-training task
collections from (Min et al., 2022a).This consists
of various extractive, multiple-choice, and/or ab-
stractive QA tasks from CROSSFIT and a subsam-
ple of UNIFIEDQA (Khashabi et al., 2020, 2022),
including NaturalQuestions, MCTest, BIOMRC,
inter alia. We modify the meta-training collec-
tions by (1) removing our evaluation sets if they are
present,” and (2) standardizing the format of each

SThi milar to the RAG-Token approach in Lewis et al.
(2020). The number of demonstrations we can use is not
limited by the context length since we marginalize over each
demonstration in its own separate context.

‘Throughout this study, we use “task” to refer to a single
dataset like SQUAD or NaturalQuestions, and “collection” to
refer to the dataset obtained by concatenating a set of tasks.

5We also remove any examples where the question has a
Jaccard similarity > 0.9 with any training or test question

task. Our final meta-training collection contains
32 tasks, which we subsample to 18 tasks based
on semantic similarity to our evaluation tasks; see
Appendix A for a full list of tasks and details on
our semantic subsampling procedure, and §5.2 for
a description of the downstream effect of semantic
subsampling.

Following Chada and Natarajan (2021), we
standardize each input in the meta-training data
to a “question:... \n answer: [MASK] \n
context:...” format. Then, the output se-
quence consists of both the question and answer
sequences,° which aligns with BART’s pre-training
objective of reconstructing the entire input se-
quence (not just masked spans). Like Chada and
Natarajan (2021), we find that aligning the in-
put/output format with BART’s pre-training objec-
tive makes a positive difference for downstream per-
formance. For QASC, which is a multiple-choice
QA task, we put all of the answer options in the
context field before the two context sentences and
generate the full answer string. This outperformed
all other formats we tried by a significant margin.”

For classification tasks, we use the same
question/answer/context format. For our single-
sentence classification task (TREC), we place the
input in the question field, and present all of the
possible labels in the context field using a similar
format as for QASC. For sentence-pair classifica-

in our evaluation tasks, and where the answers are the same;
only 4 such examples existed in our data.

We only compute F, on the answer sequences.

7We tried placing the answer options in the question field,
not including the answer options at all, and only generating the
letter label instead of the full answer string. See Appendix B
for examples and scores.

--- Page 5 ---
tion tasks (MRPC, MNLI(-mm), QNLI), we place
the first sentence or hypothesis in the question
field and place the second sentence or premise in
the context field. As with QA tasks, we generate
both the question and answer fields in the target se-
quence, but only evaluate F on answer sequences.

3.3. Demonstration Memory

For the demonstration memory bank, we use train-
ing sets from UNIFIEDQA, excluding our evalu-
ation tasks; the memory contains examples from
16 tasks. UnifiedQA has approximately 40% over-
lap with the QA meta-training collection, and no
overlap with the non-QA collection. See Table 8
in Appendix A for a full list of tasks in our demon-
stration memory bank.

We format each demonstration in the memory
bank in the same question/answer/context format as
described above, except that demonstrations have
the ground-truth label after the answer: header in-
stead of a [MASK] token. Note that memory entries
consist of a text passage (the demonstration) and a
title; for the title, we simply use the answer to the
question.

4 Experimental Setup

We evaluate on a variety of QA and classification
tasks (Table 1). We select open-domain QA tasks
from the MRQA shared task (Fisch et al., 2019) to
reflect a variety of extractive QA formats, including
a standard QA benchmark (SQuAD), a domain-
specific challenging benchmark (BioASQ), and
two knowledge-intensive QA benchmarks (Triv-
iaQA and TextbookQA).® Our few-shot QA splits
of size {16, 32, 64, 128} for these tasks are from
Ram et al. (2021), which are themselves derived
from MRQA (Fisch et al., 2019). We also gener-
ate few-shot splits for QASC, which is a multiple-
choice QA task; we evaluate on QASC to determine
whether our model is also effective in dealing with
much shorter contexts, and to ensure that it is not
overfitting to more typical MRQA-style extractive
tasks.

Our few-shot classification task splits are from
Gao et al. (2021). We evaluate on sentence pair

5While “knowledge-intensive” does not have a standard
definition or straightforward measurement, the length of the
contexts may act as a proxy for how knowledge-intensive
a question answering task is. Contexts for our knowledge-
intensive tasks are much longer, and thus require a model to
synthesize much more information and/or retrieve information
that is more relevant to the inputs to semantically prime the
model for question-specific information.

classification tasks which are not contained in our
meta-training or demonstration tasks; sentence pair
classification tasks like natural language inference
(NLI) and paraphrase classification can be eas-
ily reformatted to our question/answer/context for-
mat. We also evaluate on TREC, which is a single-
sentence text classification task where the model
must guess the category of the answer to a ques-
tion (e.g., human, location, number), rather than
the answer itself.

For each task and few-shot split size, we average
scores across 5 random few-shot samples.

4.1 Baselines

We compare against strong efficient few-shot meth-
ods, as well as similar models that will tell us why
our method performs better. Note that our approach
is generative, unlike iPET and LM-BFF; thus, it is
usable on a wider variety of tasks.

FewshotQA (Chada and Natarajan, 2021). A
few-shot question answering method. We com-
pare to the FewshotBARTL model, which is based
on BARTiage like our model and is the best-
performing variant. We use the same few-shot
splits such that we can directly compare to the
numbers reported in that paper. We also try meta-
training this non-retrieval-augmented model, which
is essentially our method without retrieval; we call
this baseline FewshotQA-m.

Splinter (Ram et al., 2021). A few-shot question
answering model pre-trained to select salient spans
from context passages.

RAG (Lewis et al., 2020). The original RAG-
Token model with a memory of Wikipedia pas-
sages. We use the released model fine-tuned
on NaturalQuestions (NQ), as this was the best-
performing RAG model on our tasks. To see
whether our demonstration memory is more effec-
tive than Wikipedia passages when meta-training,
we also try meta-training the RAG model with its
Wikipedia memory; we call this baseline RAG-m.

iPET (Schick and Schiitze, 2021b). A manual
prompt-tuning approach that induces better few-
shot performance than GPT3 with much smaller
LMs. We tune the best-performing ALBERT,.
(Lan et al., 2020) model on our tasks.

LM-BFF (Gao et al., 2021). An automatic
prompt-tuning approach based on RoBERTAjarge
(Liu et al., 2019). It requires no unlabeled text data
to work well, unlike iPET. This model and iPET
compare token probabilities to perform classifica-

--- Page 6 ---
Splinter FewshotQa

SQuAD

—= FewshotQa-m

— RAG —*- RAGm —*— Ours

BioASQ QASC

32 64
# Examples

128 32

# Examples

64 128 16 32 64

# Examples

128

Figure 2: F scores at each few-shot split size for extractive and multiple-choice question answering evaluation tasks.

Scores are averaged across 5 random few-shot samples.

Our model outperforms or maintains similar performance

to the strongest baselines on each task and split size. Performance gains on SQUAD are especially large—up to 3.9
F (4.9% improvement). FewshotQA and Splinter scores are from Chada and Natarajan (2021).

tion, so we cannot use them for open-domain tasks
like question answering. Thus, we only compare to
these models on classification.

4.2 Hyperparameters

For meta-training, we use hyperparameters from
Min et al. (2022a) where possible: init. LR
1x10, effective batch size 8,° training for a max-
imum of 30,000 steps. We checkpoint every 2,000
steps and select the checkpoint with the lowest
mean loss on our 16-shot QA training sets. Meta-
training finishes in +14 hours on 1 A100 GPU
(40GB).!°

For fine-tuning, we use hyperparameters from
Chada and Natarajan (2021) where possible: init.
LR 2 x 107°, batch size 4, fine-tuning for a max-
imum of 1,000 steps or 35 epochs (whichever is
larger). We checkpoint every 2 epochs and select
the checkpoint with the highest exact match on the
training set. Fine-tuning finishes in 30-60 minutes
on 1 A100 GPU (40GB).

For each meta-training and fine-tuning input, we
retrieve 5 demonstrations from the memory.!!

5 Results

Our model’s F scores for extractive question an-
swering (Figure 2) are higher than models of sim-
ilar parameterizations, including similar models
that have been meta-trained using the same train-
ing data. Our model also outperforms strong clas-

°We use gradient accumulation to get this effective batch
size on a single GPU.

‘Our model could also be trained and tuned on a cheaper
32GB GPU (e.g., a V100) in similar time.

"Higher values result in better performance (§5.2), but
this saturates at 5—10 retrieved demonstrations, and retrieving
more demonstrations slows down training.

TREC} MNLI MNLI-mm  QNLI MRPC_ Avg.
Majority 18.8 32.7 33.3 49.5 81.2 43.1
RoBERTa *88.821 "45.854 — *47.853 "60.265 76.625 63.8
iPET "85.041 71.217 71.826 *70.362 70.447 73.7
LM-BFF "89.417 70.713. *72.012 *69.219 *78.134 75.9
FewshotQA 91.029 *47.9%53 *46.159  *61.054 *67.648 62.7
FewshotQA-m 92.414 *50.l10 —*50.625 #71821 74.037 67.8
RAG *BLilo9 *62.499 — *61.812 *74.9)5 70.233 70.1
RAG-m *87.817  *70.014 69.114 *83.215 74.928 77.0
Ours 91.713 72917 69.614 84413 73.425 78.4

Table 2: Accuracies on classification tasks, averaged
across 5 random few-shot samples (std. dev. in sub-
script). All datasets are well-balanced except MRPC;
thus, we report accuracies for all tasks except MRPC,
where we report macro-F,. LM-BFF and RoBERTa
scores are from Gao et al. (2021). * indicates that
p < .05 in a t-test between our model’s score and the
marked score.

sification approaches on TREC, MNLLI, and QNLI
(Table 2). Thus, meta-training with semantically
similar demonstrations induces a more general-
purpose system that can perform well across a
variety of low-resource downstream tasks.
Contrast this with RAG, which often performs
worst out of each model we test across tasks. Thus,
the architecture itself is not inherently strong in
few-shot settings, suggesting that meta-training
makes a significant contribution to increased perfor-
mance. This is also supported by the increased per-
formance we observe with FewshotQA and RAG
after meta-training, though note that meta-training
does not help FewshotQA to the same extent it
helps retrieval-augmented models. Also note that
FewshotQA does not perform well on classification
tasks, whereas our method achieves performance
exceeding or close to the strongest baselines. This
means that the combination of meta-training and re-
trieval enables a more general-purpose model than

--- Page 7 ---
—=- FewshotQAm 4 RAG —+ RAGm —* Ours

TriviaQA

Splinter FewshotQA

70

60
50

40

30

Fl

20

16 32 64 128
# Examples

TextbookQA
50
45

40

_———

25

Fl

20

16 32 64 128
# Examples

Figure 3: F\ scores for each few-shot split size for
knowledge-intensive question answering tasks. Our
model is outperformed by a strong few-shot QA base-
line, though meta-training still greatly improves perfor-
mance.

either of these components separately.

With meta-training, RAG-m obtains perfor-
mance much closer to our model. This tells us that
meta-training is responsible for much of the per-
formance gains we observe, though the demon-
stration memory bank also improves performance
to a lesser extent. On MRPC, RAG-m outperforms
our model, indicating that there exist some non-
knowledge-intensive tasks where Wikipedia pas-
sages are more helpful than QA demonstrations.

5.1 Knowledge-intensive QA

We also evaluate on few-shot knowledge-intensive
QA tasks (Figure 3): here, TriviaQA and Text-
bookQA, using the few-shot splits from the MRQA
shared task. While these are also technically extrac-
tive QA tasks, their contexts have an average length
of 677 and 581 words, respectively, meaning that
BART will likely struggle more to synthesize all of
the information in these tasks (even with retrieval).
We find that FewshotQA outperforms our method
on both of these tasks, and that even Splinter out-
performs our method at larger split sizes for Text-
bookQA. This means that demonstration retrieval

Model SQUAD BioASQ QASC_ TriviaQA TbQA
FewshotQA 68.9 63.0 82.6 65.2 37.7
FewshotQA-m 76.6 63.4 85.9 65.9 38.2
RAG-m 80.0 62.9 88.9 66.6 27.7
Ours 83.9 64.7 89.2 62.9 37.2
Ours (oracle) 93.5 94.2 99.1 80.7 83.2

Table 3: F scores on QA tasks for our strongest base-
lines, our approach, and our approach where the mem-
ory has been replaced with labeled test examples (ora-
cle). The oracle approach establishes an approximate
upper bound for our model. Large gaps between our
approach and the oracle indicate room for improvement
in what constitutes our memory bank.

may be actively harmful for these tasks. Thus, our
meta-training method is optimizing RAG architec-
tures for non-knowledge-intensive tasks, but not for
knowledge-intensive tasks. Wikipedia passages are
more effective than demonstrations in the memory
bank for TriviaQA as well, as indicated by RAG-m
outperforming our approach.

However, meta-training with or without the
memory bank still induces far better performance
than the base RAG model, which performs worse
than all baselines except Splinter. Thus, our method
is still improving over RAG, making this model
more versatile and better able to handle such tasks
even if it is not the optimal approach.

5.2 Ablations

Here, we perform further analyses to understand
the contribution of individual model components
and (meta-)training decisions.

Memory bank. We find that performance is
generally higher for question answering and clas-
sification when retrieving demonstrations instead
of Wikipedia passages, as in Figure 2 and Table 2.
This raises two questions: how much could the
memory bank impact downstream performance in
the best-case scenario? Relatedly, what is the upper
bound on performance for our model given the best
possible demonstration memory bank?

To obtain an estimate, we create an oracle mem-
ory consisting of labeled test examples from our
evaluation data. We find that scores significantly
improve over our method and others in this setting,
indicating that this architecture has significant
potential to achieve further gains if the memory
bank is improved.

Number of retrieved demonstrations. _ Is
retrieving more demonstrations always bet-
ter? We compare performance when retrieving

--- Page 8 ---
as SQuaD MNLI

be BO
83
82
“er
80
79

78
05 (5 50 100 05 (25 50 100
Number of Demonstrations Number of Demonstrations

Figure 4: F scores for an extractive QA (SQuAD) and
sentence pair classification (MNLI) task by the number
of retrieved demonstrations ({0, 1,5, 10, 25,50, 100})
during fine-tuning. Scores generally increase with
the number of retrieved demonstrations, though per-
formance saturates early at 5-10 demonstrations.

100

16-shot SQUAD F1
a8 8S 88 8B 8
16-shot TriviaQA FL

(0.1) (1,2) {2,4) [4,8) {8,16) (16.inf)
Answer frequency in retrieved docs

(0.1) 11.2) (2.4) (4,8) (8,16) [16.int)
Answer frequency in retrieved docs

Figure 5: F, scores for non-knowledge-intensive
(SQuAD, left) and knowledge-intensive (TriviaQA,
right) QA tasks by the frequency of the true answer
string in the retrieved demonstrations. While not mono-
tonic, there is a clear correlation between these variables,
indicating that lexical features may be responsible for
much of retrieval’s contributions to performance.

K = {0, 1, 5, 10, 25, 50} demonstrations dur-
ing fine-tuning and evaluation on non-knowledge-
intensive QA (SQuAD) and sentence-pair classifi-
cation (MNLI). Our results (Figure 4) show that F
scores begin to saturate at 5-10 demonstrations for
both tasks. However, using more demonstrations
generally does not harm performance; the model is
able to handle less helpful demonstrations without
performance decreasing significantly.

Why is retrieval helpful? Is the model abstract-
ing semantic content from the retrieved demon-
strations for improved performance, or is it simply
learning to copy token sequences from the retrieved
demonstrations? As an initial test, we can correlate
the frequency of the ground-truth answer sequence
in the retrieved documents with F) scores on our
QA tasks. Our results (Figure 5) suggest that the
model is indeed learning to retrieve certain text
strings from the demonstrations. This provides one
possible path forward for improving the memory
bank: higher semantic overlap with one’s evalua-
tion task increases the likelihood of these overlaps,
so future work could focus on collecting (or per-
haps generating) more semantically similar demon-

Retriever SQuAD BioASQ QASC_ TriviaQA TbQA
Random 18 1S 1.2 18 2.3
DPR (Wiki) 11.5 18 15.7 49 243
DPR (PAQ) 16.9 15 26.1 29.3 24.0
Contriever 14.1 73 28.0 27.9 24.3

Table 4: The proportion of test examples for which each
retriever retrieves at least 1 demonstration containing
the ground-truth answer as a substring. DPR (PAQ)
and Contriever appear to be better at retrieving more
relevant demonstrations on average, though this does
not necessarily lead to higher downstream performance
(Table 5).

Retriever SQuAD BioASQ QASC_ TriviaQA TbQA
Random 74.3 61.8 88.7 56.5 29.6
DPR (Wiki) 83.9 64.7 89.2 62.9 37.2
DPR (PAQ) 78.8 63.5 86.8 57.6 33.5
Contriever 81.1 62.5 88.7 58.9 32.4

Table 5: F scores on 16-shot extractive QA tasks
across retrievers. We fine-tune with different retriev-
ers given the same (best) meta-trained model. Despite
DPR (Wiki)’s lower retriever scores (Table 4), its down-
stream performance is the best among the retrievers we
try.

strations that feature more lexical overlaps.

However, this does not explain how retrieval im-
proves performance on classification tasks, where
the label space is small and labels are less infor-
mative. For NLI, the label space includes “entail-
ment’’/“‘neutral’/“contradiction”, which we would
not expect to see often in our demonstrations and
which do not carry significant semantic content.
Yet retrieval-augmented models outperform Few-
shotQA by a large margin on MNLI(-mm), so what
is helping our model? There could exist some
QA demonstrations which semantically prime our
model toward correct completions, though sentence
embedding similarity may not capture this help-
fulness. Future work could ablate over specific
features in the demonstrations.

What type of retriever is best? For our exper-
iments thus far, we have used the DPR compo-
nent of the RAG-Token (NQ) model, which is
pre-trained on Wikipedia and fine-tuned on Nat-
uralQuestions. Is this an optimal starting point, or
would some other retreiver be better? We compare
against a DPR model pre-trained on the Probably-
Asked Questions (PAQ; Lewis et al., 2021) dataset,
as well as the Contriever model (Izacard et al.,
2021). Contrievers are unsupervised, whereas DPR
models receive explicit supervision during pre-

--- Page 9 ---
Memory SQUAD BioASQ QASC TriviaQA TbQA

All tasks 83.5 63.2 89.2 614 36.8
Semantically similar tasks 83.9 64.7 89.2 62.9 37.2

Table 6: 16-shot F, scores on QA tasks after meta-
training on either all QA tasks from MetaICL’s QA
meta-training collection, or QA tasks subsampled by
semantic similarity to our evaluation tasks. A full list of
meta-training tasks can be found in Appendix A.

training. DPR tends to perform better when the
downstream task is similar to the pre-training or
fine-tuning data; however, in our case, demonstra-
tion retrieval is dissimilar from Wikipedia passage
retrieval, and Contriever may handle larger train-
test shifts better (Izacard et al., 2021).

We evaluate both the relevance of the retrieved
demonstrations (Table 4) and downstream F, (Ta-
ble 5) on our QA tasks. We find that DPR (PAQ)
and Contriever are both better at retrieving similar
demonstrations, as measured by the frequency with
which they retrieve examples that contain the an-
swer. For BioASQ, only Contriever retrieves more
relevant demonstrations than a random retriever.

However, retrieving more relevant demonstra-
tions does not translate into increased downstream
performance: DPR (Wiki) consistently outper-
forms the others. Why? Through qualitative anal-
ysis, we find that DPR (Wiki) retrieves more se-
mantically diverse demonstrations, whereas DPR
(PAQ) and Contriever retrieve demonstrations that
are technically more similar to the test example,
but also less diverse across test examples.Thus,
there should be a balance between diversity and
relevance: completely random retrieval is not effec-
tive (as indicated by our random retrieval baseline
scoring worst), but neither is the more constrained
demonstration set we retrieve using an arguably
more optimal retriever.

Meta-training data. Is meta-training helpful
because of the variety of tasks included in our
setup (the more is better hypothesis), or would it
be better to select meta-training data in a more
principled way (the similar datasets are better
hypothesis)? We compare downstream perfor-
mance when meta-training on all QA tasks from
MetaICL versus the top tasks by mean instance-
level semantic similarity to our evaluation tasks
(Table 6). To compute semantic similarity, we
use the stsb-roberta-base-v2 model from Sen-
tenceTransformers (Reimers and Gurevych, 2019)
and compute the mean pairwise cosine similarity

between the 16-shot training examples in our evalu-
ation tasks and all examples in a meta-training task.
We then select the top tasks by similarity until we
have over 240,000 examples (enough for 30,000
training steps using batch size 8). See Appendix A
for a list of meta-training tasks before and after
subsampling.

We find that selecting meta-training data
based on semantic similarity to our evaluation
tasks is helpful for both our QA and non-QA
tasks: F increases across tasks when only meta-
training on the most similar data. This contrasts
with the findings of Min et al. (2022a), who find
that more meta-training tasks is generally better.

6 Conclusions

We have proposed a meta-training method (§3.2)
that retrieves (§3.1) semantically similar demon-
strations from a diverse demonstration bank (§3.3).
Our method achieves higher performance on aver-
age across many tasks than other strong parameter-
efficient few-shot baselines ($5). In future work,
one could explore a mixture of demonstration
retrieval and passage retrieval for improved per-
formance on a wider variety of tasks—including
knowledge-intensive tasks.

Limitations

Our method requires access to a large set of la-
beled examples for the memory bank—ideally with
some relevance to the evaluation tasks. This limits
the languages and tasks that are optimal for this
method: there does not exist a large variety of train-
ing examples for low-resource language varieties,
nor for certain much more specific tasks—as in,
for example, industry applications with domain-
specific customer data. And while multilingual
models could leverage cross-lingual transfer, it is
unclear how well this model would generalize into
low-resource languages when (for example) using
multilingual BART.

When using the full demonstration memory,
meta-training does not run on a 16GB GPU us-
ing our current implementation. While this does
exclude more common GPUs, our approach could
still run quickly on a 32GB GPU in a few hours,
thus costing far less than pre-training a language
model of comparable few-shot performance from
scratch.

--- Page 10 ---
References

Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
2021. Muppet: Massive multi-task representations
with pre-finetuning. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing, pages 5799-5811, Online and Punta
Cana, Dominican Republic. Association for Com-
putational Linguistics.

Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,
and Andrew McCallum. 2020. Self-supervised meta-
learning for few-shot natural language classification
tasks. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 522-534, Online. Association for
Computational Linguistics.

Yujia Bao, Menghua Wu, Shiyu Chang, and Regina
Barzilay. 2020. Few-shot text classification with dis-
tributional signatures. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.
2022. Improving language models by retrieving from
trillions of tokens.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877-1901. Curran Associates,
Inc.

Rakesh Chada and Pradeep Natarajan. 2021. Few-
shotQA: A simple framework for few-shot learning
of question answering tasks using pre-trained text-to-
text models. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6081-6090, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

Yanda Chen, Ruigqi Zhong, Sheng Zha, George Karypis,
and He He. 2022. Meta-learning via language model
in-context tuning. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 719-730,

Dublin, Ireland. Association for Computational Lin-
guistics.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR, abs/2204.02311.

William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017,
volume 70 of Proceedings of Machine Learning Re-
search, pages 1126-1135. PMLR.

Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,
Eunsol Choi, and Danqi Chen. 2019. MRQA 2019
shared task: Evaluating generalization in reading
comprehension. In Proceedings of the 2nd Work-
shop on Machine Reading for Question Answering,
MRQA@EMNLP 2019, Hong Kong, China, Novem-
ber 4, 2019, pages 1-13. Association for Computa-
tional Linguistics.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 3816-3830, Online. Association for Computa-
tional Linguistics.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
language model pre-training. In Proceedings of the
37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research,
pages 3929-3938. PMLR.

--- Page 11 ---
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In Pro-
ceedings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings
of Machine Learning Research, pages 2790-2799.
PMLR.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Towards unsupervised
dense information retrieval with contrastive learning.
CoRR, abs/2112.09118.

Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open do-
main question answering. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume,
EACL 2021, Online, April 19 - 23, 2021, pages 874—
880. Association for Computational Linguistics.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022. Few-shot learning with retrieval aug-
mented language models. CoRR, abs/2208.03299.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.
Billion-scale similarity search with gpus. IEEE
Trans. Big Data, 7(3):535-547.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1601-1611, Vancouver,
Canada. Association for Computational Linguistics.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6769-6781,
Online. Association for Computational Linguistics.

Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,
Jonghyun Choi, Ali Farhadi, and Hannaneh Ha-
jishirzi. 2017. Are you smarter than a sixth grader?
textbook question answering for multimodal machine
comprehension. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages
5376-5384.

Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-
jishirzi. 2022. Unifiedqa-v2: Stronger general-
ization via broader cross-format training. CoRR,
abs/2202.12359.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter Clark, and Hannaneh

Hajishirzi. 2020. Unifiedqa: Crossing format bound-
aries with a single QA system. In Findings of the
Association for Computational Linguistics: EMNLP
2020, Online Event, 16-20 November 2020, volume
EMNLP 2020 of Findings of ACL, pages 1896-1907.
Association for Computational Linguistics.

Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020. Qasc: A
dataset for question answering via sentence com-
position. Proceedings of the AAAI Conference on
Artificial Intelligence, 34(05):8082-8090.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. ALBERT: A lite BERT for self-supervised
learning of language representations. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rock-
tischel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Advances in Neural Infor-
mation Processing Systems, volume 33, pages 9459-
9474. Curran Associates, Inc.

Patrick Lewis, Yuxiang Wu, Linging Liu, Pasquale Min-
ervini, Heinrich Kiittler, Aleksandra Piktus, Pontus
Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 mil-
lion probably-asked questions and what you can do
with them. Transactions of the Association for Com-
putational Linguistics, 9:1098-1115.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022. What
makes good in-context examples for GPT-3? In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures,
pages 100-114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Dangi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR, abs/1907.11692.

Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022a. MetalCL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 2791-2809, Seattle, United States.
Association for Computational Linguistics.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022b. Rethinking the role of demonstra-
tions: What makes in-context learning work? CoRR,
abs/2202. 12837.

--- Page 12 ---
Aaron Mueller, Jason Krone, Salvatore Romeo, Saab
Mansour, Elman Mansimoy, Yi Zhang, and Dan Roth.
2022. Label semantic aware pre-training for few-
shot text classification. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 8318-
8334, Dublin, Ireland. Association for Computational
Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQUAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383-2392, Austin,
Texas. Association for Computational Linguistics.

Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-
son, and Omer Levy. 2021. Few-shot question an-
swering by pretraining span selection. In Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 3066-3079, Online.
Association for Computational Linguistics.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982-3992, Hong Kong, China. Association for Com-
putational Linguistics.

Timo Schick and Hinrich Schiitze. 2021a. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume,
pages 255-269, Online. Association for Computa-
tional Linguistics.

Timo Schick and Hinrich Schiitze. 2021b. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 2339-2352, Online. Association
for Computational Linguistics.

George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R. Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, Yannis Almirantis, John Pavlopoulos, Nico-
las Baskiotis, Patrick Gallinari, Thierry Artiéres,
Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric
Gaussier, Liliana Barrio-Alvers, Michael Schroeder,
Ion Androutsopoulos, and Georgios Paliouras. 2015.
An overview of the BIOASQ large-scale biomedical
semantic indexing and question answering competi-
tion. BMC Bioinform., 16:138:1-138:28.

Ricardo Vilalta and Youssef Drissi. 2002. A perspective
view and survey of meta-learning. Artif: Intell. Rev.,
18(2):77-95.

Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’00, page 200-207, New York, NY,
USA. Association for Computing Machinery.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP, pages
353-355, Brussels, Belgium. Association for Com-
putational Linguistics.

Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew Mingbo Dai, and Quoc V. Le. 2022. Finetuned
language models are zero-shot learners. In Interna-
tional Conference on Learning Representations.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112-1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2022. An explanation of in-context
learning as implicit bayesian inference. In Jnterna-
tional Conference on Learning Representations.

Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 7163-7189, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Sa-
loni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,
and Bowen Zhou. 2018. Diverse few-shot text clas-
sification with multiple metrics. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1206-1215, New Orleans, Louisiana.
Association for Computational Linguistics.

Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021. Adapting language models for zero-shot learn-
ing by meta-tuning on dataset and prompt collections.
In Findings of the Association for Computational
Linguistics: EMNLP 2021, pages 2856-2878, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.

--- Page 13 ---
A. Tasks

Meta-training. Our meta-training data is from
MetaICL’s (Min et al., 2022a) meta-training sets.
Specifically, we use the QA task collection from
the paper, which is a mixture of CROSSFIT and
UNIFIEDQA tasks as shown in Table 7. We exclude
any task on which we evaluate. As in MetaICL,
we subsample 16,384 examples per task such that
no individual task is overrepresented during meta-
training. Some tasks are sampled twice due to
the inclusion of both CROSSFIT and UNIFIEDQA
versions of some tasks, as in Min et al. (2022a).

All meta-training tasks:
biomre, boolq, fr a, hotpot_qa, kilt_hotpotqa, kilt_nq,
kilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-trex,
mc_taco, numer_sense, quoref, ropes, search_qa, superglue-
superglue-record, tweet_qa, web_questions, unifiedqa:boolq,
unifiedqa:drop, _unifiedqa:narrativeqa,
atural_questions_with_dpr_para, unifiedqa:newsqa, uni-
fiedga:physical_i unifiedqa:quoref, unifiedqa:race_string, uni-
fiedqa:ropes, unifiedqa:social_i

a, unifiedqa:winogrande_x]

Subsampled by similarity:

biomrc, boolq, freebase_qa, hotpot_qa, lama-google_re, quoref,
ropes, superglue-multirc, superglue-record, unifiedqa:boolq, uni-
fiedqa:commonsenseqa, —_unifiedqa:drop, _unifiedqa:narrativeqa,
unifiedqa:natural_q ions_with_dpr_para, unifiedga:newsqa, uni-
fiedqa:quoref, unifiedqa:r: string, unifiedqa:ropes

Table 7: Tasks used in our meta-training data. We sub-
sample 16,384 examples per task to ensure balanced
supervision during meta-training. All tasks are from
CROSSFIT unless prefixed with “unifiedqa:”.

We also perform a targeted subsampling proce-
dure, where we select tasks by semantic similar-
ity to our evaluation tasks. For this, we compute
the mean pairwise semantic similarity between a
meta-training task’s examples and one 16-shot split
of each of our evaluation tasks, then select meta-
training tasks in decreasing order of similarity. Se-
mantic similarity is computed by calculating the
cosine similarity of the sentence embeddings from
the stsb-roberta-base-v2 model in Sentence-
Transformers (Reimers and Gurevych, 2019).

Demonstrations. Our demonstrations are from
the UNIFIEDQA collection, which includes extrac-
tive, abstractive, and multiple-choice QA tasks as
shown in Table 8. We exclude any task on which
we evaluate.

Note that there is some overlap between the
demonstration set and the meta-training set, though
the demonstrations contain the correct answer
whereas the meta-training examples do not.

Demonstration task bank:

unifiedqa:ai2_science_middle, unifiedqa:boolq, uni-
mmonsenseqa, unifiedqa:drop, _unifiedqa:metest, —_ uni-
rrativeqa, unifiedga:natural_questions_with_dpr_para, uni-
fiedqa:newsqa, unifiedqa:openbookqa, unifiedqa:openbookqa_with_ir,
unifiedqa:physical_iqa, _ unifiedqa:quoref, —_unifiedqa:race_string,
unifiedqa:ropes, unifiedqa:social_iqa, unifiedqa:winogrande_x]

Table 8: Tasks used in our demonstration memory bank.
Note that there is no subsampling within each task, since
the retriever can simply ignore irrelevant demonstra-
tions. All tasks are from UNIFIEDQA.

B_ Format Tuning for Multiple-choice QA

Chada and Natarajan (2021) observe significant
performance gains by simply changing the for-
mat of the QA inputs and outputs. We use a for-
mat similar to theirs for most QA tasks, but it
is not immediately clear how to extend the ques-
tion/answer/context format to multiple-choice QA,
or if including the answer options in the context
would be helpful at all. Thus, we try three different
formats for QASC and compare performance.

Every example consists of a question g, two con-
text sentences c; and C9, a set of 8 answer options
with letter labels {a.4, ag, ..., @y}, and a correct
answer a € {a4,...,a}. We can generate either
the full answer string, or the letter label of the an-
swer i, where i € {A,B,..., H}. We try putting
the answer options in the question or the context,
excluding the answer options altogether, generat-
ing the answer string a, and generating the answer
letter i.

Our results using BARTharge (Table 9) indicate
that generating the answer is better than just gen-
erating the letter label, that including the options
in the context is helpful, and that excluding the
options from the context or putting the options in
the question is harmful to performance. The perfor-
mance gap between different formats is very large,
which aligns with the findings of Chada and Natara-
jan (2021): using an example format aligned with
the model’s pre-training format is one of the most
important factors contributing to few-shot perfor-
mance.

--- Page 14 ---
Format name

Format

Example

Fy

Options in question,
generate letter

Options in question,
generate answer

Options in context,
generate answer

No options, gener-
ate answer

question: q? {a4,...,ay} \n

answer: [MASK] \n context: c,.

c2. => question: q? \n answer: 2

question: q? {a4,...,ay} \n

answer: [MASK] \n context: c,.

C2. => question: q? \n answer: a

question: g? \n answer: [MASK]

\n context: {a4,...,aH}. ¢1.

c2. => question: q? \n answer: a

question: g? \n answer: [MASK]

\n context: cy). cg. = question:

q? \n answer: a

question: What does sunlight do
for a plant? (A) during the day
(B) Kills it (C) it can be seen
(D) Helps it survive (E) Helps it
drink water (F) It gets heated up
(G) adding heat (H) Makes the
color darker \n answer: [MASK]
\n context: A plant requires food
for survival. All plants require
sunlight to make their food. >
question: ... \n answer: D
question: What does sunlight do
for a plant? (A) during the day
(B) Kills it (C) it can be seen
(D) Helps it survive (E) Helps it
drink water (F) It gets heated up
(G) adding heat (H) Makes the
color darker \n answer: [MASK]
\n context: A plant requires food
for survival. All plants require
sunlight to make their food. >
question: ... \n answer: Helps it
survive

question: What does sunlight do
for a plant? \n answer: [MASK]
\n context: (A) during the day
(B) Kills it (C) it can be seen (D)
Helps it survive (E) Helps it drink
water (F) It gets heated up (G)
adding heat (H) Makes the color
darker. A plant requires food for
survival. All plants require sun-
light to make their food. = ques-
tion: ... \n answer: Helps it sur-
vive

question: What does sunlight do
for a plant? \n answer: [MASK]
\n context: A plant requires food
for survival. All plants require
sunlight to make their food. >
question: ... \n answer: Helps it
survive

15.6

82.6

49.8

Table 9: The formats we try for QASC and 16-shot F; scores from BARTharge (no retrieval) after fine-tuning on each
format. We find that generating the answer is better than just generating the letter label, that including the options in
the context is helpful, and that excluding the options from the context is harmful to performance. “=>” separates the
input from the output sequence, and “\n’ indicates a newline.

