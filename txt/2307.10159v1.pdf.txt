arXiv:2307.10159v1 [cs.CV] 19 Jul 2023
FABRIC: PERSONALIZING DIFFUSION MODELS WITH
ITERATIVE FEEDBACK
Dimitri von Rütte¹*
Elisabetta Fedele¹*
¹ETH Zurich, Zurich, Switzerland
Jonathan Thomm¹*
Lukas Wolf1*
{dvruette, efedele, jthomm, wolflu}@ethz.ch
user 1
user 2
FABRIC
"a painting of the sea"
FABRIC
Output for user 1
Output for user 2
Figure 1: Illustration of the proposed approach. FABRIC generates images based not only on text
prompt, but also on user preferences expressed during multiple rounds of generation.
1
ABSTRACT
In an era where visual content generation is increasingly driven by machine learning,
the integration of human feedback into generative models presents significant
opportunities for enhancing user experience and output quality. This study explores
strategies for incorporating iterative human feedback into the generative process
of diffusion-based text-to-image models. We propose FABRIC, a training-free
approach applicable to a wide range of popular diffusion models, which exploits
the self-attention layer present in the most widely used architectures to condition
the diffusion process on a set of feedback images. To ensure a rigorous assessment
of our approach, we introduce a comprehensive evaluation methodology, offering
a robust mechanism to quantify the performance of generative visual models that
integrate human feedback. We show that generation results improve over multiple
rounds of iterative feedback through exhaustive analysis, implicitly optimizing
arbitrary user preferences. The potential applications of these findings extend to
fields such as personalized content creation and customization.
INTRODUCTION
The field of artificial intelligence (AI) has witnessed a surge in interest in generative visual models,
primarily due to their transformative potential across a myriad of applications, encompassing content
creation, customization, data augmentation, and virtual reality. These models leverage advanced deep
learning methodologies, such as GAN (Goodfellow et al., 2014) and VAE (Kingma & Welling, 2022),
to generate high-fidelity and visually compelling images from given inputs or descriptions Brock
et al. (2019); Razavi et al. (2019). The significant advancements in generative visual models have
catalyzed the exploration of novel possibilities in the realms of computer vision, natural language
processing, and human-computer interaction Radford et al. (2016).
Diffusion models, in particular, have emerged as a powerful tool in the field of image synthesis, often
delivering results that are comparable to, or even exceed, those produced by GANs and VAEs Ho
et al. (2020); Rombach et al. (2022). These models are characterized by their ability to generate a
diverse array of visually coherent images, while demonstrating superior stability and reduced mode
collapse during the training phase Song et al. (2021). This has led to their widespread adoption
*Equal contribution
1
Prompt(s)
Feedback Images
Feedback Source
photo of a dog running
on grassland
U-Net Layer (N)
z~N(0, 1)
Self-Attention
ResNet Block
H
Cross-Attention
FFN
Figure 2: Illustration of the proposed approach. FABRIC improves generated results by incorporating
user feedback through an attention-based conditioning mechanism.
among researchers investigating the frontiers of generative visual modeling. Moreover, the utility of
diffusion models extends beyond image synthesis, finding applications in various other domains such
as inpainting, super-resolution, and style transfer Chan et al. (2020); Park et al. (2019).
Text conditioning serves as a crucial component of generative visual models, enabling them to
synthesize images based on human-readable descriptions Reed et al. (2016). However, despite the
robust capabilities of diffusion models in generating a wide spectrum of images, steering the model
towards a specific desired output can pose challenges Luccioni et al. (2023). Users often embark on
an iterative
process of
prompt refinement to achieve their intended results, and articulating personal
preferences in the form of text can be a complex task. Nevertheless, users possess the ability to
readily evaluate the quality of the generated images. This opens up the possibility of integrating
sparse human feedback into the generative process, which can be harnessed to enhance the results
and better align them with user preferences. Given the stability and controllability of the generative
process offered by diffusion models, they present an ideal platform for the incorporation of human
feedback to refine the text-to-image generation process.
In this work, we focus on this iterative workflow and propose a technique based on sparse feedback
that aims to aid in steering the generative process towards desirable and away from undesirable
outcomes. This is achieved by using positive and negative feedback images (e.g. gathered on previous
generations) to manipulate future results through reference image-conditioning. Simply repeating the
setup allows for iterative refinement of the generated images based on an arbitrary feedback source
(including human feedback). Our contributions are three-fold:
a
• We introduce FABRIC (Feedback via Attention-Based Reference Image Conditioning)
novel approach that enables the integration of iterative feedback into the generative process
without requiring explicit training that can be combined with many other extensions to
Stable Diffusion.
•
• We propose two experimental settings that facilitate the automatic evaluation of generative
visual models over multiple rounds.
Using these proposed settings, we evaluate FABRIC and demonstrate its superiority over
baseline methods.
2 RELATED WORK
2.1 TEXTUAL INVERSION AND STYLE TRANSFER
A popular method for personalizing text-to-image diffusion models is textual inversion (Gal et al.,
2023; Ruiz et al., 2023), a technique for learning semantic text embeddings from images depicting
a common subject or style. This enables the synthesis of photorealistic images in various scenes
and conditions while preserving some set of desirable features, but requires multiple images that
incorporate those features as well as additional training to learn the semantic embedding.
'The code is publicly available: https://github.com/sd-fabric/fabric.git
2
Mokady et al. (2022) introduce an accurate inversion technique for text-guided diffusion models,
enabling text-based real-image editing capabilities. The proposed approach consists of two novel
components: pivotal inversion for diffusion models and null-text optimization, allowing for high-
fidelity editing of real images without tuning the model's weights. StyleDrop (Sohn et al. (2023))is a
novel method developed by Google Research for synthesizing images that adhere to a specific style
using a text-to-image model. The method captures intricate details of a user-provided style, including
color schemes, shading, design patterns, and local and global effects, and enhances the quality
through iterative training with either human or automated feedback, outperforming other methods
for style-tuning text-to-image models. Xu et al. (2023) introduces a novel approach to incorporate
user-provided reference images instead of text prompts. This approach, called Prompt-Free Diffusion,
leverages a Semantic Context Encoder (SeeCoder) to transform these inputs into meaningful visual
embeddings, which are then used as conditional inputs for a Text-to-Image (T2I) model, generating
high-quality, customized outputs.
2.2 HUMAN PREFERENCE MODELLING
Recently, modeling human preferences in generative models experienced increased attention. In the
domain of diffusion models, Wu et al. (2023) provide a novel dataset containing almost 100 thousand
images collected from over 25 thousand prompts. A sample from the dataset contains between two
and four images and a particular user's choice when being offered this batch of images. The authors
fine-tune a CLIP (Radford et al. (2021)) model in order to classify chosen images against non-chosen
ones. The Human Preference Score (HPS) is derived from this classifier. In addition to that, the
authors fine-tune a low-rank adaptation (LoRA, Hu et al. (2021)) of stable diffusion (Rombach et al.
(2022)) that significantly improves the image quality.
Kirstain et al. (2023) introduces Pick-a-Pic, a large-scale dataset of user preferences for text-to-image
generation collected from real users of their web interface. The web app presents users with two
generated images conditioned on their prompt and asks them to select their preferred option or
indicate a tie if they have no strong preference. The rejected (non-preferred) image is then replaced
with a newly generated image, and the process repeats. The dataset, containing 14,000 preferences, is
used to train a scoring function called PickScore, which outperforms other available scoring functions
in predicting user preferences, even surpassing expert human annotators. The authors advocate for
the use of Pick-a-Pic prompts in evaluating text-to-image models, highlighting the dataset's potential
for various applications such as model evaluation, image quality improvement, and text-to-image
model enhancement.
Fan et al. (2023) proposes an approach called DPOK for fine-tuning text-to-image diffusion models
using online reinforcement learning (RL). DPOK integrates policy optimization with KL regular-
ization and updates pre-trained models using policy gradient to maximize feedback-trained reward.
The authors demonstrate that DPOK generally outperforms supervised fine-tuning in terms of both
image-text alignment and image quality.
2.3 ITERATIVE FEEDBACK
In the generative text-to-image workflow, the user typically thinks of a prompt, generates images
with that prompt, inspects the results, makes adjustments to the prompt, and repeats this process until
they're happy with the result. In order to remove this burden from the user, Tang et al. (2023) propose
a novel zeroth-order optimization algorithm with theoretical guarantees for problems with black
box objective functions evaluated through ranking oracles, such as Reinforcement Learning with
Human Feedback (RLHF). The algorithm's effectiveness is demonstrated in improving image quality
generated by a diffusion generative model using human ranking feedback, offering a promising
alternative to existing RLHF methods.
To summarize, the literature of the field has predominantly focused on concept learning, style transfer
from reference images, and modeling human preferences to fine-tune models accordingly. However,
the iterative incorporation of human feedback into the model training process, a potentially significant
aspect for enhancing model performance and alignment with human preference, remains relatively
unexplored.
3
3 FABRIC
We will now introduce our proposed method, FABRIC, which tackles the problem of incorporating
multiple rounds of positive and negative human feedback into the generative process.
Attention-based Reference Image Conditioning FABRIC takes inspiration from a technique
implemented in an update to the widely used ControlNet repository by Zhang (2023) that introduces
the ability to generate synthetic images similar to some reference. This method exploits the self-
attention module in the U-Net. The intuition is that the self-attention module's weights have learned
to "pay attention” to other pixels in the image; therefore, adding additional keys and values from a
reference image offers a way to inject additional information. In order to be compatible with the
generated image at a specific time step of the denoising process, the reference latent image is partially
noised up to the current timestep t (using the standard random forward noising process:
.
Zref √√āt xref + √√1 − āt · N(0, 1))
.
(1)
However, how to compute keys and values for a reference image in the U-Net layers is not immediately
clear since simply concatenating the images does not work. Therefore, the keys and values for
injection (or equivalently the hidden states right before, see Algorithm 0) are computed by passing
the noised reference image through the U-Net of Stable Diffusion. All keys and values are stored in
the self-attention layers of the U-Net.
Then, for a particular U-Net denoising step of the user prompt and the current partially denoised
latent image Zt+1, the stored keys and values are appended in the self-attention in the respective
U-Net layers. This way, the denoising process can attend to the reference image and include the
semantic information. By reweighting the attention scores (see Equation 2), we additionally obtain
control over the strength of the reference influence.
Note that precomputing the hidden states of reference images requires an additional U-Net forward
pass, roughly doubling the inference time. Further, concatenating additional keys and values in the
self-attention layer also increases inference time and results in scaling of the required memory that
is quadratic in the number of feedback images (or linear if a memory-efficient implementation of
attention is used) (Dao et al., 2022; Rabe & Staats, 2022).
Incorporating Feedback in the Generation Process The method outlined in the previous section
incorporated a reference image into the generative process. We will now provide an extension to
incorporate multi-round positive and negative feedback: Given a set of liked and disliked images, we
do a separate U-Net pass for every liked and disliked image. The keys and values of the liked and
disliked images are concatenated to the conditional and unconditional U-Net pass respectively (see
classifier-free guidance, Ho & Salimans (2022)). Further, we reweight the attention scores with a
factor depending on whether it is the conditional or the unconditional pass and the current time step
in the denoising process.
Unlike the ControlNet (Zhang, 2023) approach, we also explore this linear interpolation which
allows us to emphasize coarse features (having a large reference attention weight for t> aT) or fine
details from the reference (i.e., larger attention weight when t ≤ aT). The feedback process can be
scheduled according to the denoising steps. This allows for only including feedback in some of the
denoising steps. We find that the best results can be achieved when incorporating the feedback in the
first half of the denoising process. Additional experiments with adapted feedback schedules can be
found in Section 5.3.
Extending to Iterative Feedback Now that we can incorporate both positive and negative feedback
reference images, we adapt the algorithm for multiple rounds. Initially, we generate images with no
feedback images, resulting in a vanilla Stable Diffusion generation. From those images, we append
any set of liked and disliked images to the positive and negative feedback respectively. How this
feedback is obtained is arbitrary, although we envision either a human or an automated process being
the feedback source. Then, we generate a new batch of images using the given feedback. We repeat
this process every round, extending the sets of liked and disliked images and refining the next batch
according to them.
4
4 EVALUATION
4.1 FABRIC MODELS
We conduct an evaluation of two versions of FABRIC in our study. The first version, referred to as
FABRIC, utilizes the methodology outlined in Section 3, built upon a fine-tuned Stable Diffusion
1.5 checkpoint (dreamlike-photoreal-2.0). The second version, named FABRIC+HPS LORA, further
enhances the FABRIC approach by applying it on top of the Low-Rank Adaptation (LORA) of Stable
Diffusion 1.5 based on Human Preference Score proposed by Wu et al. (2023). We opted to include
the FABRIC+HPS LORA version in our evaluation due to its proven ability to generate images that
closely match human preferences.
4.2
BASELINES
Since, to the best of our knowledge, there doesn't exist a method designed to incorporate iterative
feedback gathered over multiple rounds, we compare the proposed method to standard Stable
Diffusion models in the following manner. First, we run Stable Diffusion 1.5 (using the base model²
or a fine-tuned version called Dreamlike Photoreal³ with or without HPS LORA checkpoints) N
times, each with a different seed, generating N batches of images. Then, we collect the desired
evaluation metrics for the generated batch at each round and we use this values to perform quantitative
comparisons with FABRIC.
It is important to note that while we may add images from each round as feedback to future rounds for
our method, the baselines do not have a mechanism to incorporate feedback into future generations.
Therefore, the baseline models generate images independently in each round without taking previous
rounds into consideration. An overview of the all models taken into account can be found in Table 1.
Checkpoint
LORA
Name
SD version
FABRIC
Baseline
1.5
stable-diffusion-v1.5
х
Dreamlike Photoreal
1.5
dreamlike-photoreal-2.0
HPS LORA
1.5
dreamlike-photoreal-2.0 HPS LORA
✓
FABRIC (SD)
1.5
stable-diffusion-v1.5
FABRIC
1.5
dreamlike-photoreal-2.0
FABRIC + HPS LORA
1.5
dreamlike-photoreal-2.0
Χ
HPS LORA
Table 1: Specifics of FABRIC models and baselines used during evaluation
4.3
METRICS
4.3.1
PREFERENCE MODEL
In order to automatically evaluate the experiments we use the PickScore introduced in Section 2 as a
proxy score for general human preference.
4.3.2 CLIP SIMILARITY TO FEEDBACK IMAGES
To assess the effectiveness of incorporating feedback into the generation process, we compute the
CLIP similarity between the generated and the previous positive and negative feedback images.
(k)
(1)
(k)
For a generated image x we compute the average CLIP-similarity to feedback images ypos, ..., ypos or
(1)
Yneg, Yneg. Specifically, let CLIP (x, y) denote the cosine similarity between CLIP-embeddings
of x and y. Then the positive CLIP similarity is defined as follows:
k
1
Spos(x)
=
Σ CLIP (x, ypos)
(i)
i=1
2https://huggingface.co/runwayml/stable-diffusion-v1-5
https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0
5
The negative similarity sneg (x) is defined analogously.
4.3.3 IN-BATCH IMage DiversitY
In the process of steering the generation of images toward user preferences it is crucial to balance
exploration (offering diverse image options for user selection) against exploitation (generating images
aligned with previous feedback). To quantify the trade-off between these two factors across different
rounds, we introduce a metric called In-batch Image Diversity.
For a batch of images x1,...,xn, we define the in-batch diversity based on the average CLIP-
similarity between images in the batch. Specifically, with CLIP similarity defined as above, the
In-batch Image Diversity d is defined as follows:
n
i-1
2
d(x1,...,xn) = 1
n(n - 1)
Σ CLIP (xi, xj)
i=2 j=1
where "(n-1) is the number of elements in the upper triangular cosine similarity matrix.
5
EXPERIMENTS
In order to evaluate the capabilities of our model, we designed two experimental settings, each
employing a different criterion for selecting feedback images during rounds.
In Sec. 5.1 we present a Preference Model-Based approach, where we select liked and disliked images
using a preference score. In Sec. 5.2 we present a Target Image-Based approach, where feedback
selection relies on the similarity with a target image provided at the start of the experiment (but not
used as feedback directly).
Both the experiments use a batch size of 4 and consist on 3 rounds of generation. After each round
one image is selected as liked and another one as disliked. To demonstrate robustness with respect to
the feedback strength, the preference model-based setup uses a feedback strength of w = 0.1 while
the target image-based setup uses w = 0.8.
5.1
PREFERENCE MODEL-BASED FEEDBACK SELECTION
The first experiment aimed to evaluate FABRIC by assessing how humans generally perceive gener-
ated images through an universal preference score. Therefore, this experiment operates under the
assumption that all humans have the same preference when providing feedback, which in general
may not always hold and is challenged in the second experiment.
In practice, the experiment is conducted as follows. First, a random set of 1000 prompts is sampled
from the HPS dataset (Wu et al., 2023). Next, for each prompt, after having initialized the user's
liked and disliked images as empty sets, we simulate a 3-round interaction between the model and a
user. In each round, the following steps are performed. First, FABRIC is run with the prompt and
feedback images as input, generating a batch of 4 images. Then, the Human Preference Score of
each generated image is computed. Using those scores as a proxy for human feedback, the generated
image with the highest score is added to the set of liked images, and the one with the lowest score is
added to the disliked. For each batch of generated images we measure the average PickScore as well
as the average CLIP similarity to both positive and negatives images. We compare each FABRIC
model (dreamlike-photoreal-2.0 and HPS LORA) against their respective baselines in terms of these
two scores.
The results from our experiments are depicted in Figure 3. In Figure 3a we address the concern that
simply running multiple rounds and achieving higher image variety may lead to an increase in the
maximum PickScore. Indeed we show that, even if in general this value increases over rounds, our
model outperforms both the baselines. In Figure 3b we observe that starting from the second round
our model outperforms the baselines not only in terms of maximum PickScore but also in terms
average and minimum value, indicating an overall enhancement in the quality of generated images.
In Figure 3c we evaluate the similarity of generated images to positive and negative feedback. We
observe that even after just one round, the CLIP similarity score is higher for the positive samples, and
6
PickScore
21.2-
21.0
20.8-
20.6-
20.4-
20.2-
Round
21.2
21.0
20.8
20.6
PickScore
20.4
20.2
SD 1.5
FABRIC (SD)
20.0
Dreamlike Photoreal
FABRIC
19.8-
-x-HPS LORA
HPS LORA + FABRIC
19.6 min
Round
FABRIC
HPS LORA
HPS LORA + FABRIC
Feedback Image Similarity in 2nd round
Positive
Negative
SD 1.5
FABRIC (SD)
Dreamlike Photoreal
FABRIC
HPS LORA
FABRIC+ HPS LORA
(a) Highest PickScore of a generated
image over all previous rounds.
(b) Min. (dotted), mean (dashed) (c) Similarity of generated images
and max. (solid) PickScore in each to positive/negative feedback.
round.
Figure 3: Results of preference model-based feedback selection.
CLIP Score
82
80
78-
76
74
72
2
Round
SD 1.5
FABRIC (SD)
Dreamlike Photoreal
FABRIC
Round
1
max
HPS LORA
mean
▾ FABRIC + HPS LORA
..... min
Round
Figure 4: Results of target image-based feedback selection. Positive feedback improves target
similarity over the baseline and using both positive and negative feedback improves it even further.
At the same time, any kind of feedback reduces the diversity of generated images drastically.
lower for the negatives, compared to the baseline. This confirms that FABRIC effectively conditions
the direction of generation based on the provided feedback.
5.2 TARGET IMAGE-BASED FEEDBACK SELECTION
In this experiment, we challenge the assumption that all humans have the same preference, according
to which they select liked/disliked images. Instead, we assume that the user has some target image in
mind, some imagined picture that they would like to express.
To reflect this scenario, we manually gather a dataset of prompt-image pairs from the AI-art-sharing
platform prompthero.com where users post their favorite generations along with the prompt that was
used to generate the image. Due to this survivorship bias, we claim that it's appropriate to assume that
shared images express the respective user's creative vision for the given prompt or, more generally,
correspond to the user's preference. The dataset is provided in our public GitHub repository.4
During the experiment, we generate a batch of images using a prompt from the dataset and select
feedback images based on CLIP-similarity to the associated target image: the most and least similar
image is selected as positive and negative feedback respectively. Note that the images in the dataset
are generated with various different models and settings, which prevents the evaluated model from
generating exact replications of the target image.
We compare the SD-1.5 and Dreamlike Photoreal baselines (using no feedback) to regular FABRIC in
terms of similarity to the specified target image and in terms of in-batch image diversity. The results
of this are illustrated in Figure 4. We find that both of them outperform the respective baselines,
improving both best-case and worst-case outcomes in rounds 2 and 3. Especially the per-round
minimum similarity drastically improves over the baseline when any kind of feedback is introduces.
Qualitative results from this experiment are shown in Appendix A.2.
https://github.com/sd-fabric/fabric
7
CLIP similarity
Max CLIP similarity over all rounds
84.5
22
-dropout 0.3
dropout=0.0
84.0-
20
83.5
83.0-
82.5
82.0-
81.5-
81.0
2
Round
In-batch diversity
18
16
14
12
10
8
6
In-batch image diversity per round
2
Round
-dropout 0.3
dropout=0.0
(a) The effect of prompt dropout on the best
CLIP similarity to the target image.
(b) The effect of prompt dropout on the in-
batch image diversity.
Figure 5: Prompt dropout appears to be an effective way of trading CLIP similarity for more diversity
in the generative distribution.
5.3 ADAPTING THE FEEDBACK SCHEDULE
In addition to the experiments reported in the main part, we investigated the adaptation of the feedback
schedule. The default configuration of FABRIC adds feedback in every denoising step. Our findings
show that restricting feedback in the first half of the denoising process improves performance by a
large margin. On the other hand, only including feedback in the second half of the denoising process
reduces performance. This leads to the hypothesis that feedback is useful in the early stages of the
denoising process, but fine-grained details are largely determined by the prompt, where feedback
does not help.
5.4 INCREASING DIVERSITY WITH PROMPT DROPOUT
As can be seen in Appendix A.2, the diversity of generated images quickly collapses as soon as any
kind of feedback is introduced. This is likely due to the conditioning mechanism, which pushes the
modal towards generating images very similar to the reference. From a user perspective, however,
this drop in diversity is undesirable, as it prevents future iterations from discovering new and possibly
better results.
We investigate one possible approach to combatting this collapse by using prompt-dropout, i.e.
dropping every word in the prompt with some probability. Using the same setup as in Section 5.2,
we compare the best CLIP similarity to the target as well as the in-batch image diversity of using
prompt dropout with p = 0.3 against not using it (p = 0.0). We find that there is a direct trade-off
between CLIP similarity and diversity: p 0.3 significantly increases diversity but also produces
worse images over all. Note that an increased initial diversity especially will also increase diversity in
feedback images, which may explain why even after 3 rounds, the diversity of p = 0.3 is still above
the initial diversity of p = 0.0. Thanks to more varied feedback, prompt dropout might be able to
catch up to the baseline given enough feedback.
=
6 DISCUSSION
Limitations While FABRIC works well in our experiments, some limitations apply. We noticed
that FABRIC is effective at constraining the generative distribution to a subset of preferable outcomes,
but it struggles to expand the distribution beyond the initial text-conditioned distribution given
by the model. Especially since feedback is already sampled from the model's output, additional
modifications will be needed to overcome this shortcoming. For the same reason, the diversity of
generated images quickly collapses to a single mode close to the feedback images. We propose
prompt dropout as a possible remedy, but this might run the risk of dropping crucial words in the
prompt and changing the generations completely.
8
Another limitation lays in the way we collect feedback. Currently, we only allow users to provide
binary preferences over images, which does not allow for specific conditioning in the image generation
process.
Future Work Future work will investigate approaches toward increasing diversity or generally
controlling the exploration-exploitation trade-off in a more principled fashion. An interesting avenue
for this could be to retrieve candidate images from an external image corpus (e.g., the user's previously
liked images or a corpus of general high-quality images) and use them as a seed for the feedback
loop. A big advantage of FABRIC is its orthogonality to most other stable diffusion variations,
like checkpoints, LORA weights, modifying or replacing the text conditioning (Xu et al. (2023))
while achieving significant improvements on top of them (see FABRIC + LoRA). Ultimately, one
could specify precise weights for each liked or disliked image and rate if rather coarse features or
fine-grained features are liked or disliked, respectively. One potential improvement in this direction
would be to specify whether user preferences are expressed based on the structure or style of the
image. This distinction would allow for more specific conditioning in the image-generation process.
In addition, FABRIC provides a well-defined action space with different parameters that can affect
the generated results. This opens up the avenue for performing Bayesian optimization on an arbitrary
objective (e.g., direct user feedback or a score given by a preference model).
7 CONCLUSION
We present FABRIC, a training-free method that incorporates iterative feedback into the generation
process of text-to-image models, leveraging attention-based reference image conditioning. Our
experimental findings suggest that FABRIC is capable of implicitly optimizing a variety of objective
functions such as human preference and similarity to a designated target image.
These objectives noticeably improve with more feedback rounds, demonstrating that FABRIC's
effectiveness significantly exceeds merely sampling more images without providing additional
feedback. Remarkably, even without training or hyperparameter tuning, FABRIC can outperform the
HPS LORA, a model explicitly trained to optimize human preference, on the relevant metric.
Notably, FABRIC tends to trade exploration for exploitation, often collapsing to a uniform distribution
after a handful of feedback rounds. We have examined a few strategies to alleviate this collapse,
though further investigation of this trade-off remains a prospect for future work.
The iterative setting is paramount to how generative visual models are used in practice. Despite
this, recent research on aligning text-to-image models has left it largely unexplored. We believe that
this study contributes towards the formation of a framework that aids in devising and evaluating
methodologies intended to address this setting.
ETHICAL IMPLICATIONS
Text-to-image models have the potential to make creative visual expression more accessible to
everyone by allowing individuals without artistic skills or technical expertise to generate visually
appealing content. Our proposed method aims to further enhance the accessibility and personalization
of these models by incorporating user preferences into the image generation process. This is achieved
through the utilization of positive and negative feedback, allowing users to provide natural and
intuitive guidance based on prior images or previously generated ones.
By adopting our approach, users gain increased control over the generated content, promoting ethical
usage. However, this heightened control also raises concerns regarding the potential for misuse or
abuse of the system. To address these concerns, it becomes crucial for the community as well as
society at large to establish clear guidelines regarding the legal and ethical utilization of such systems.
By placing responsibility on individual users to ensure responsible and ethical usage, we can mitigate
the risks and foster a positive and constructive environment for creative expression.
9
REFERENCES
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis, 2019.
Kelvin C. K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative
latent bank for large-factor image super-resolution, 2020.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and
memory-efficient exact attention with io-awareness, 2022.
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel,
Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for
fine-tuning text-to-image diffusion models, 2023.
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and
Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using
textual inversion. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=NAQvF08TcyG.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-
pic: An open dataset of user preferences for text-to-image generation, 2023.
Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias:
Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408, 2023.
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for
editing real images using guided diffusion models, 2022.
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization, 2019.
Markus N. Rabe and Charles Staats. Self-attention does not need o(n²) memory, 2022.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks, 2016.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision, 2021.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2, 2019.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis, 2016.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models, 2022.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023.
10
Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred
Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip
Krishnan. Styledrop: Text-to-image generation in any style, 2023.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations, 2021.
Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. Zeroth-order optimization meets human feedback:
Provable learning via ranking oracles, 2023.
Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better Aligning Text-to-Image
Models with Human Preference. ArXiv, abs/2303.14420, 2023.
Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free
diffusion: Taking "text" out of text-to-image diffusion models, 2023.
.
Lvmin Zhang. [Major Update] Reference-only Control · Mikubill/sd-webui-controlnet · Discus-
sion #1236, 2023. URL https://github.com/Mikubill/sd-webui-controlnet/
discussions/1236.
A
APPENDIX
A.1
METHOD DETAILS
Here we provide the Pseudocode of the FABRIC algorithm (see Algorithm 0).
พ
Weighted Attention, (Q, K, V) = (~,
พ
○ softmax
|| w || 1
(②KT)) VT
(2)
A.2 TARGET IMAGE-BASED FEEDBACK SELECTION EXAMPLES
Here, we provide some example feedback-trajectories from the promthero-dataset, see Figure 6
A.3 FABRIC WORKFLOW
In Figure 7 we show the high-level workflow of FABRIC.
11
Algorithm 1 FABRIC: Feedback via Attention-Based Reference Image Conditioning
Require: Let N be the number of feedback rounds, n be the batch size of generated images in each
round and model be a diffusion model capable of reference-conditioning.
1: procedure FABRIC
2:
pos, neg · [], []
3:
for i = {1,..., N} do
4:
prompt get_prompt(i)
5:
images [GENERATE(prompt, pos, neg) for n times]
6:
Xpos, neg get_feedback(images)
7:
pos.put(xpos)
8:
9:
▷ we focus on one like and one dislike
neg.put(xneg)
end for
10: end procedure
11: function GENERATE (prompt, positives, negatives)
for t = {T, ………, 1} do
hiddens {}
for xref = {... positives, ... negatives} do
Zref√√āt xref + √√1 − āt· € (t)
.
Cref
▷ forward diffusion noising, e
ref
~N(0,1)
h← PRECOMPUTEHIDDENSTATES (Zref, t)
hiddens.put(h)
12:
ZT =
initial_noise()
13:
14:
15:
16:
17:
18:
19:
end for
20:
21:
22:
23:
24:
end for
25:
return zo
(t)
compute who and who according to the feedback settings
.(t)
Єcond,t-1 ← MODIFIEDUNET(zt, t, get_positive(hiddens), wptos)
(t)
Euncond,t-1 ← MODIFIEDUNET(zt, t, get_negative(hiddens), weg)
Zt-1 step_with_cfg(zt, Єcond,t, uncond,t) ▷ ancestral Euler sampling in our case
26: end function
27: function PRECOMPUTEHIDDENSTATES(z, t)
hiddens
[]
for i-th layer in the Unet do
apply ResNet block(s) to z
hiddens.put(i, z)
28:
29:
30:
31:
32:
33:
34:
end for
35:
return z
apply self-attention to z
apply cross-attention and FFN to z
▷ just before self-attention
▷ for hiddens
36: end function
37: function MODIFIED UNET(z, t, hiddens, w)
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:
for i-th layer in the Unet do
hhiddens.at(i)
apply ResNet block(s) to z
Q+W().z
KWconcat(z, h)
V←
W().concat(z, h)
z WeightedAttention (Q, K,V)
apply cross-attention and FFN to z
end for
return z
48: end function
12
=
this function is a standard U-Net
Target Image
Round 0
Round 1
Round 2
k
Figure 6: Examples of feedback rounds from our target-image-based experiment.
13
13
Feedback via Attention-Based Reference Image Conditioning
Unet Layer
III
× 1.0
conditional denoising step
× 0.8 × schedule
Figure 7: FABRIC: Reference images are noised up to a certain step, and the extracted keys and
values are injected into the self-attention of the U-Net during the denoising process.
14
