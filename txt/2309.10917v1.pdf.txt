arXiv:2309.10917v1 [eess.AS] 19 Sep 2023
END-TO-END SPEECH RECOGNITION CONTEXTUALIZATION WITH LARGE
LANGUAGE MODELS
Egor Lakomkin, Chunyang Wu, Yassir Fathullah, Ozlem Kalinli, Michael L. Seltzer, Christian Fuegen
Meta AI
ABSTRACT
In recent years, Large Language Models (LLMs) have
garnered significant attention from the research community
due to their exceptional performance and generalization ca-
pabilities. In this paper, we introduce a novel method for con-
textualizing speech recognition models incorporating LLMs.
Our approach casts speech recognition as a mixed-modal lan-
guage modeling task based on a pretrained LLM. We provide
audio features, along with optional text tokens for context,
to train the system to complete transcriptions in a decoder-
only fashion. As a result, the system is implicitly incentivized
to learn how to leverage unstructured contextual information
during training. Our empirical results demonstrate a signifi-
cant improvement in performance, with a 6% WER reduction
when additional textual context is provided. Moreover, we
find that our method performs competitively and improve by
7.5% WER overall and 17% WER on rare words against a
baseline contextualized RNN-T system that has been trained
on more than twenty five times larger speech dataset. Over-
all, we demonstrate that by only adding a handful number of
trainable parameters via adapters, we can unlock contextu-
alized speech recognition capability for the pretrained LLM
while keeping the same text-only input functionality.
Index Terms contextual biasing, large language mod-
els, speech recognition
1. INTRODUCTION
In recent years, there has been growing interest in Large Lan-
guage Models (LLMs) due to their remarkable efficacy in
enhancing performance in tasks like question answering and
summarization, surpassing specialized models [1, 2]. LLMs
are trained on vast quantities of text data, thereby encapsu-
lating a wealth of world knowledge within the network. This
accumulated knowledge and contextual understanding prove
to be particularly beneficial in the field of Automatic Speech
Recognition (ASR), especially when additional context sur-
rounding an utterance is available beyond the audio alone.
For example, video titles and descriptions can provide in-
sights into the topic of the video or offer clues about named
entities that might be mentioned [3, 4]. Such contextual in-
Work done during internship at Meta AI.
LLM decoder
Text-text LoRa
attention
adapters
<bos>
Audio
tokens
Contextual text tokens
(video title, description)
Audio
encoder
Recognized
spoken text
Fig. 1. A speech recognition model with mixed-modal con-
text consisting of audio and optional text tokens based on a
pretrained LLM backbone. Speech encoder and LLM decoder
are both initially pretrained. The LLM weights are frozen
(orange blocks), while audio encoder and LoRa adapters are
fine-tuned during training (blue blocks).
formation can assist in disambiguating challenging pronun-
ciations, as certain words, domain-specific terms, or named
entities can often be inferred from context alone. Traditional
approaches to ASR contextualization [4, 3, 5, 6] operate at the
token or phrase level, employing techniques like biasing with
weighted finite state transducers (WFSTs) or using special-
ized attention networks. These are typically either incorpo-
rated during the decoding stage or trained as separate compo-
nents. Consequently, contextualization significantly improves
the ASR system's ability to recognize named entities or spe-
cialized in-domain terms. However, there are some limita-
tions to these approaches:
The biasing is limited towards individual phrases or
words, as opposed to contextualizing based on external infor-
mation as a whole (for example, topic-based biasing).
The biasing strength is usually controlled via a hyper-
parameter or requires specialized architectural changes and
training procedures to ensure the system is not overbiased.
Some of the contextualization methods influence only
the decoder state without interacting with the encoder di-
rectly.
In this work, we propose a Speech LLAMA - a decoder-
only architecture inspired by recent developments in LLMs
tailored towards speech recognition. It is trained to use the
contextual information end-to-end without any additional hy-
perparameters. Specifically, 1) we prepend the whole avail-
able textual context as a prompt to an ASR system along with
audio tokens.
The Speech LLAMA hence have the full flexibility to look
back and cross-corellate the contextual text tokens and the
acoustic tokens when decoding the next spoken word. And
2) we employ the publicly available 7B LLAMA LLM [1] as
a pretrained decoder for the Speech LLAMA. This simplifies
the overall design of a contextual ASR as speech recogni-
tion can be considered as mixed-modal language model with
next-token prediciton. Our intuition behind this is the pre-
trained LLMs already distill the linguistic information which
should be particularly useful when reasoning which part of
the context is relevant given the utterance. Our results on a
competitive benchmark suggest a feasibility of this modelling
approach.
2. RELATED WORK
There have been several works on speech recognition models
contextualization including deep and shallow biasing [8, 4].
Le et al. [4] introduced a weighted finite state transducer
(WFST) composed from biasing strings which is attached dy-
namically during decoding and the scores of the RNN-T sys-
tem and biasing WFST are interpolated. The advantage of
such approaches is that they could be attached to any system
after the training is completed. Another line of research is
deep biasing methods that incorporate contextualization end-
to-end during the model training [9, 3, 6, 10, 11, 5]. A com-
mon limitation of these approaches is that the bias on the
phrase level, rather than providing on the full context avail-
able. In addition, these approaches require a specialized bias-
ing modules added to the main ASR architecture.
In parallel to this reseach several approaches were pre-
sented incorporating LLMs for speech related tasks. Wu at
al. [12] incorporated LLaMA LLM for speech translation by
concatenating a textual prompt ("Translate audio to language
X") with audio representations. AudioPalm [13] model was
proposed mixing audio and text tokens for speech-to-text and
speech to speech tasks. Fathullah et al. [14] presented re-
sults on enabling speech recognition capabilities for LLAMA
model on the multi-lingual data. Recently a Whisper model
[15] incorporated a biasing approach, where the previous seg-
ment's transcription was added to the prompt for the long-
form speech recognition. In difference to their work, we bias
the system on the unstructed and sometimes unrelated textual
context as not always video title and description match the
context of speech.
3. EXPERIMENTAL SETUP
Model: Figure 1 illustrates the overview of our proposed
model. This speech LLM architecture consists of two main
blocks: audio encoder and text decoder. The audio encoder
firstly applies four downsampling blocks resuling in 16x time
reduction of audio representations. After that a stack of Con-
former [16] blocks with rotary positional embeddings [17] are
applied with hidden dimensionality of 512 and kernel size of
9. At the end we add an additional downsampling block.
As a result the decoder observes audio tokens sampled ev-
ery 320ms with dimensionality of size 4,096. We pretrained
the audio encoder with Connectionist Temporal Classifica-
tion [18] criterion for 300k training steps on the same train-
ing data. We used a pretrained 7B LLaMA (v1) [1] as a de-
coder. To adapt text-only LLAMA to speech recognition task,
we have added Low-Rank Adapters [19] to query, key, value,
and output projection matrices in the self-attention layer of
every decoder layer while keeping the rest of LLM param-
eters frozen throughout the training. We used the following
LoRa parameters: rank of size 32, dropout rate of 5%, and
0.05 scaling parameter. Overall LoRa parameters add 30 mil-
lion trainable parameters to the LLM decoder and the rest 6.7
billion are kept frozen.
We used 80 dimensional log Mel features computed every
10ms with a window of 25ms. SpecAugment [20] with two
frequency masks of width 27 and ten time masks with max-
imum width of 4% of the length of an utterance. We trained
our models for 200,000 updates with mixed precision, lin-
early increasing the learning rate to 5e-4 in the first 20,000
updates and exponentially decaying to 1e-5 over the remain-
ing updates. We use Adam with parameters ẞ1 = 0.9, 82
0.98, weight decay 1e-5 and clip the gradient norm to 1.
Our model is trained with 128 A100 GPUs for 3 days using
Fairseq library [21].
=
=
Data: The models are trained on an in-house dataset that
was de-identified with no personally identifiable information
(PII) derived from public Facebook and Instagram videos.
The data was further augmented with two distortion methods:
speed perturbation [22] and randomly sampled additive back-
ground noise. For evaluation, we have sampled 3,200 videos
comprising around 34 hours of speech that have context of
at least 100 characters length with at least one non-frequent
word from the context occurs in the transcription.
Metrics: To evaluate our models, we report both the over-
all Word Error Rate (WER) and Rare WER, which considers
only rare words. A word is considered rare if it does not occur
in the 90% percentile of the most frequent words estimated on
training data.
Textual context: Similar to Xiao et al. [7] we incorpo-
rate video title and video description as an external context.
We perform basic text post-processing like unicode character
normalization and removal of all non-ascii symbols. Overall
approximately 25% of videos from supervised video dataset
have non-empty text context. When video title or description
are present, we first concatenate and then tokenize them with
the LLAMA tokenizer. After that, we prepend the <bos> to-
ken with the textual tokens. When both video title and de-
scriptions are missing, the input corresponds to a traditional
ASR setup without contextual information. The cross-entropy
Table 1. Evaluation results of Speech LLaMA compared to large-scale RNN-T baseline on English speech data. We report
overall WER and Rare WER. Rare WER specifically focuses on the accuracy of recognizing rare words in the dataset.
Model
Speech
data (h)
Trainable
params (M)
Context presence
Training
WER (%)
SUB INS DEL
Rare WER (%)
Evaluation
1B RNN-T [7]
4M
1000
12.34
6.53 3.21 2.60
30.80
1B RNN-T [7]
4M
1000
12.13
6.23 3.05
2.85
28.96
Speech LLaMa
150k
130
11.70
6.09 3.20 2.38
27.33
Speech LLaMa
150k
130
11.98
Speech LLaMa 150k
130
11.22
6.28 3.07 2.63
5.76 3.14 2.32
28.64
23.88
loss is masked for the contextual tokens and only computed
for spoken tokens. In these experiments we limit the textual
content to a maximum of 50 tokens for computational rea-
sons. If the context is longer than the threshold, we perform
a random crop of size 50 during training and crop the leading
tokens during inference.
Baseline: As a baseline we used a transformer based
RNN-T system with one billion parameters [7], which is
trained on four million hours of supervised and semi-supervised
speech data. The RNN-T system architecture consists of 60
transformer layers in the encoder and 3 LSTM layers in
the decoder. For contextualization it uses an WFST bias-
ing method with neural language modelling shallow fusion
[4], where the biasing FST is composed from video title and
description. We are using exactly the same contextual in-
formation during decoding for the RNN-T baseline and our
Speech LLAMA.
4. RESULTS
Table 1 presents a summary of our decoding results on the
evaluation set. We compare the Speech LLAMA against the
offline RNN-T 1b model, considering two scenarios: with
and without presenting contextualization information during
decoding. The WER scores obtained for these scenarios us-
ing RNN-T are 12.34% and 12.13% respectively. Contextual
biasing resuts in a relative WER reduction of approximately
1.7%.
Even without the use of contextual information during
training and evaluation, Speech LLAMA achieves a WER of
11.70%, a relative reduction of 5.2% over the RNN-T system
trained on much more data.
By incorporating context during training and evaluation,
we achieve a significant improvement reaching an overall
WER of 11.22% and resulting in 17% relative improvement
in Rare WER, surpassing the performance of the RNN-T
model with contextualization.
It is worth noting that when we evaluate the Speech
LLAMA trained with context but do not provide the context
during inference, we obtain a WER of 11.98%. This corre-
sponds to a slight WER gap compared to the model trained
without context. We leave to address this minor performance
difference to the future work, where adding a certain jitter
to the context may improve the generalization of a model
towards presence of the context.
4.1. Ablation studies
4.1.1. Context sensitivity
To better understand how the model learns to use the context,
we studied how receptive the model is to context perturba-
tions. For this we tried a few ways to modify the prompt and
measure its effect on the decoding. Specifically, we experi-
mented with:
1. Replacing the actual context with words randomly sam-
pled from the training data.
2. Replacing the context with the ground truth words. We
filter out frequent words in this experiment as we as-
sume that the model should not have significant issues
in transcribing them. We expect a significant reduc-
tion of WER if the model is capable of copy-pasting
the words from the context.
3. Replacing the contextual words with phonetical re-
spellings of the words that appear in the transcripts.
Our intuition is that such replacements are particularly
challenging for the model and we should expect a big-
ger WER change compared to random substitutions. To
generate re-spellings we employed a G2G [23] model.
For every rare word in the ground truth we sample an
alternative spelling from the G2G model and add it to
the context. For example, if the word ball is present in
the context and ground truth we replace it by bawl and
use that as context instead of the original token.
4. In addition to the previous perturbation we probe ap-
pending a similar sounding word to the context (e.g.
both tokens ball and bawl will be present in the con-
text). This tests the ability of an ASR system to dis-
ambiguate the actual spoken word given a competitive
word in context.
Table 2. WER under different context perturbations during
decoding stage.
Table 4. Performance comparison of decoder-only Speech
LLM and cross-attention Speech LLM.
Context noise
WER (%)
Rare WER (%)
(Original context)
11.22
23.88
(Remove all context)
11.98
28.64
Random
12.07
28.85
Respellings
11.89
28.31
Respellings (append)
Ground Truth
11.46
10.50
25.59
19.54
We present our results in Table 2. We note that replacing
the whole context with random words sampled from the train-
ing data results in only a marginal difference in WER com-
pared to removing all external context (11.98% vs. 12.07%).
This indicates that the model is robust against some contex-
tual noise and can distinguish relevant from irrelevant context.
Substituting rare words that match both the context and the
ground truth with G2G respellings results in a significant drop
in WER (11.22% → 11.89%), almost matching with not us-
ing any context. This hints that the majority of gains observed
are due to the model being able to copy certain words from
the context. In contrast, when we instead of replacing the
matching contextual word rather append a competing similar-
sounding word, we observe a smaller WER drop (11.22% →
11.46%). This indicates that the model does not necessar-
ily get confused by similarly pronounced words with differ-
ent meanings. Furthermore, when we take the rare words
from the ground truth into the context, the WER improves
to 10.50% (6% relative change) and Rare WER improves by
18% relative. This further proves the ability of the model to
utilize contextual information when present in order to better
recognize rare entities.
Table 3. Impact of the context masking structure on the WER.
Masking
Causal
Full-Mask
WER (%)
11.22
11.15
4.1.2. Causal vs Full Masking
Traditionally causal masking is used in all self-attention lay-
ers for decoder-only language models to prevent future infor-
mation leakage. However for offline speech recognition we
have full audio and text context observed at the time of decod-
ing and only transcription tokens are necessary to be masked
causally. In this section we experiment the impact of apply-
ing causal masking on all input tokens and contrast it with
applying full mask on the text and audio context followed by
causal masking on transcription tokens. While the audio rep-
resentations are fully contextualized already, we hypothesize
Decoder
WER (%)
Decoder-only
Encoder-decoder
11.22
11.18
that textual tokens may benefit from full masking.
We present our results in Table 3. The full-mask shows
only marginally better WER then causal masking (improving
from 11.22% to 11.15%). This comes at a cost as effi-
cient self-attention implementations are currently tailored to-
wards causal masking (Flash-Attention v2) and using a cus-
tom masking slows down training by 10%.
4.1.3. Decoder-only vs Cross-attention
Furthermore, we compared the decoder-only approach to a
traditional encoder-decoder model by converting the Speech
LLM architecture to Listen-Attend-Spell architecture [24].
To achieve that, instead of concatenating audio and text
tokens we treated them separaterely. We added trainable
cross-attention matrices to every LLM decoder layer. Table
presents the results of this study. We observed that the two
approaches perform similarly, with only minor improvement
for the Encoder-Decoder architecture (11.22% 11.18%).
This indicates that the decoder-only approach is a viable and
straightforward method for performing ASR with or without
contextualization.
3
However, one limitation of the decoder-only approach is
the quadratic attention complexity, which can impose restric-
tions on the overall sequence length. This limitation becomes
significant as the context grows. To address this issue, we can
employ techniques such as lower precision training (8 or 4
bits) and linear attention approximation methods [25, 26].
5. CONCLUSIONS AND FUTURE WORK
In this work, we have presented to our knowledge the first
results on utilizing pretrained LLMs to leverage contextual
information in order to improve speech recognition. We have
demonstrated that with a simple decoder-only architecture we
can condition the ASR output on the unstructured text. Our
approach shows superior performance against a strong base-
line, proving the feasability of the proposed method at scale.
End-to-end contextualization via text promping with LLMs
shows better context utilization compared to our strong RNN-
T based baselines. In addition, our ablation studies show that
the system is robust to noise perturbations and shows abilities
to perform a phonetic disambiguation. As part of the future
work, we plan to extend the methods towards long context
and other modalities.
6. REFERENCES
[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.,
"Llama: Open and efficient foundation language mod-
els," 2023.
[2] OpenAI, "Gpt-4 technical report,” 2023.
[3] Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey
Zweig, Florian Metze, and Yatharth Saraf, “Contex-
tual RNN-T for open domain ASR," in INTERSPEECH.
2020, pp. 11-15, ISCA.
[4] Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim, et al.,
"Contextualized streaming end-to-end speech recogni-
tion with trie-based deep biasing and shallow fusion,”
in INTERSPEECH, 2021, pp. 1772–1776.
[5] Kanthashree Mysore Sathyendra, Thejaswi Muniyappa,
Feng-Ju Chang, et al., “Contextual adapters for per-
sonalized speech recognition in neural transducers,” in
ICASSP. 2022, pp. 8537–8541, IEEE.
[6] Golan Pundak, Tara N. Sainath, et al., “Deep context:
End-to-end contextual speech recognition,” in 2018
IEEE Spoken Language Technology Workshop. 2018,
pp. 418-425, IEEE.
[7] Alex Xiao, Weiyi Zheng, Gil Keren, et al., “Scaling
asr improves zero and few shot learning," in INTER-
SPEECH, 2021.
[8] Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon,
et al., "Shallow-fusion end-to-end contextual biasing,"
in INTERSPEECH. 2019, pp. 1418–1422, ISCA.
[9] Duc Le, Gil Keren, Julian Chan, et al., “Deep shallow
fusion for rnn-t personalization,” in 2021 IEEE Spoken
Language Technology Workshop (SLT), 2021, pp. 251–
257.
[10] Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur
Gandhe, et al., "Robust acoustic and semantic con-
textual biasing in neural transducers for speech recog-
nition," CORR, vol. abs/2305.05271, 2023.
[11] Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al.,
"Adaptive contextual biasing for transducer based
streaming speech recognition," 2023.
[12] Jian Wu, Yashesh Gaur, et al., “On decoder-only ar-
chitecture for speech-to-text and large language model
integration," 2023.
[13] Paul K. Rubenstein et al., “Audiopalm: A large language
model that can speak and listen,” 2023.
[14] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al.,
"Prompting large language models with speech recogni-
tion abilities," 2023.
[15] Alec Radford, Jong Wook Kim, et al., “Robust speech
recognition via large-scale weak supervision,” in ICML.
23-29 Jul 2023, vol. 202 of Proceedings of Machine
Learning Research, pp. 28492–28518, PMLR.
[16] Anmol Gulati, James Qin, et al., "Conformer:
Convolution-augmented transformer for speech recog-
nition," in INTERSPEECH. 2020, pp. 5036-5040,
ISCA.
[17] Jianlin Su, Yu Lu, et al., “Roformer: Enhanced trans-
former with rotary position embedding," CoRR, vol.
abs/2104.09864, 2021.
[18] Alex Graves et al., “Connectionist temporal classifica-
tion: labelling unsegmented sequence data with recur-
rent neural networks," in ICML. 2006, vol. 148 of ACM
International Conference Proceeding Series, pp. 369–
376, ACM.
[19] Edward J. Hu, Yelong Shen, et al., “Lora: Low-rank
adaptation of large language models," in ICLR. 2022,
OpenReview.net.
[20] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le,
“Specaugment: A simple data augmentation method for
automatic speech recognition," INTERSPEECH, Sep
2019.
[21] Myle Ott, Sergey Edunov, Alexei Baevski, et al.,
“fairseq: A fast, extensible toolkit for sequence mod-
eling," in ACL (Demonstrations), Minneapolis, Min-
nesota, June 2019, pp. 48–53, Association for Compu-
tational Linguistics.
[22] Tom Ko, Vijayaditya Peddinti, et al., “Audio augmenta-
tion for speech recognition," in INTERSPEECH. 2015,
pp. 3586-3589, ISCA.
[23] Duc Le, Thilo Koehler, Christian Fuegen, and
Michael L. Seltzer, "G2g: Tts-driven pronunciation
learning for graphemic hybrid asr," in ICASSP, 2020,
pp. 6869-6873.
[24] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol
Vinyals, “Listen, attend and spell," CORR, vol.
abs/1508.01211, 2015.
[25] Tim Dettmers and Luke Zettlemoyer, “The case for 4-
bit precision: k-bit inference scaling laws," in ICML.
2023, vol. 202, pp. 7750–7774, PMLR.
[26] Jiayu Ding, Shuming Ma, et al., “Longnet: Scaling
transformers to 1,000,000,000 tokens," 2023.
