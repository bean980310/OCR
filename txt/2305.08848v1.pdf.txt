arXiv:2305.08848v1 [cs.CL] 15 May 2023
Small Models are Valuable Plug-ins for Large Language Models
Canwen Xu¹*, Yichong Xu², Shuohang Wang², Yang Liu²,
Chenguang Zhu², Julian McAuley¹
¹University of California, San Diego, 2Microsoft
1{cxu, jmcauley}@ucsd.edu, 2{yicxu, shuowa, yaliu10, chezhu}@microsoft.com
1
Abstract
Large language models (LLMs) such as GPT-
3 and GPT-4 are powerful but their weights
are often publicly unavailable and their im-
mense sizes make the models difficult to be
tuned with common hardware. As a result,
effectively tuning these models with large-
scale supervised data can be challenging. As
an alternative, In-Context Learning (ICL) can
only use a small number of supervised exam-
ples due to context length limits. In this pa-
per, we propose Super In-Context Learning
(SuperICL) which allows black-box LLMs to
work with locally fine-tuned smaller models,
resulting in superior performance on super-
vised tasks. Our experiments demonstrate that
SuperICL can improve performance beyond
state-of-the-art fine-tuned models while ad-
dressing the instability problem of in-context
learning. Furthermore, SuperICL can enhance
the capabilities of smaller models, such as mul-
tilinguality and interpretability.¹
Introduction
Large-scale pre-trained language models, such as
GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI,
2023), have demonstrated remarkable capabilities
in a wide range of NLP tasks. Despite the impres-
sive performance of these recently released models,
their size and limited accessibility of model weights
can lead to difficulties in fine-tuning these models
with supervised data, which is an effective way to
adapt the models to specific tasks (Liu et al., 2019).
An alternative approach, In-Context Learning
(ICL, Brown et al., 2020), involves concatenating a
few labeled examples with the test input, enabling
the model to learn from the context. However,
ICL is limited by the maximum context length of
the LLM, restricting the number of examples it
can utilize. Consequently, while ICL can usually
perform few-shot learning with 16 or 32 examples,
* Work done during internship at Microsoft.
Code available at https://aka.ms/SuperICL.
it cannot fully exploit supervised data when there
are hundreds or thousands of examples.
To address these limitations, we propose Super
In-Context Learning (SuperICL), a novel approach
that enables black-box language models (e.g., GPT-
3.5) to work with locally fine-tuned smaller models
(e.g., ROBERTA, Liu et al., 2019), resulting in im-
proved performance on supervised tasks. SuperICL
is designed to overcome the challenges of poor per-
formance and instability of ICL.
SuperICL builds on the strengths of ICL while
mitigating its limitations. As shown in Figure 1,
SuperICL leverages a combination of an LLM with
smaller models, which act as plug-ins, to perform
supervised tasks efficiently. Specifically, we use the
plug-in model to predict labels with confidence for
in-context examples and concatenate them with the
input text and ground-truth labels as context. For
test examples, we also add the plug-in model's pre-
diction and confidence to the test input and let the
LLM predict the final label and an explanation. As
these plug-in models have been fine-tuned on the
task-specific data, they serve as a bridge between
the large pre-trained model and the task-specific
data, allowing for effective knowledge transfer and
improved performance.
We conduct extensive experiments to evaluate
the effectiveness of SuperICL on GLUE (Wang
et al., 2019), a standard benchmark for natural lan-
guage understanding. Our results show that Super-
ICL: (1) achieves superior performance compared
to state-of-the-art fine-tuned models and LLMs;
(2) addresses the instability problem of ICL by al-
lowing the plug-in models to absorb task-specific
information while leaving the LLMs to focus on
more general language understanding; (3) enhances
the capabilities of plug-in models such as extending
their multilinguality to cover a wider range of lan-
guages; (4) provides interpretability via the LLM
by providing explanations for why it overrides pre-
dictions made by plug-in models.
Supervised Sample
Dataset
Input Ground Truth
In-Context Examples
+
Test Input
LLM
(a) ICL
Supervised Sample
Dataset
Input Ground Truth
In-Context Examples
Plug-in
Model
Input Predicted Label Confidence Ground Truth
Constructed Context
+
Test Input
Plug-in
Model
Test Input Predicted Label Confidence
(b) SuperICL
LLM
Final Prediction
Final Prediction
Explanation
Figure 1: The workflow of ICL and SuperICL. There are three steps in SuperICL: (1) A context is constructed by
randomly sampling from the training data and incorporating the plug-in model's predictions, including predicted
labels and their corresponding confidence scores. (2) The test input is concatenated after the context, with the
plug-in model's prediction attached. (3) Finally, a language model generates the final prediction along with an
optional explanation.
We then conduct a thorough analysis of how each
component contributes to the final performance of
SuperICL, as well as the impact from the number
of in-context examples. We also explore the effects
of adversarial attacks on plug-in models and how
this affects SuperICL's performance. Our findings
demonstrate the potential of combining large and
small, cloud and local models, shedding light on a
promising new paradigm for supervised learning in
the era of large language models.
2 Related Work
In-Context Learning Originally proposed in the
GPT-3 paper (Brown et al., 2020), In-Context
Learning (ICL) is considered as a new paradigm
that exploits LLMs on new tasks without updating
the parameters of the model. It prepends few-shot
training examples before the test input as a prompt,
to enable large language models to find patterns
and "learn" to predict. There have been success-
ful applications of ICL in downstream tasks, such
as Machine Translation (Lin et al., 2021; Agrawal
et al., 2022) and data generation (Ye et al., 2022).
Despite its success in few-shot learning, a major
drawback of ICL is instability. The performance
of ICL is sensitive to the selected in-context exam-
ples (Zhao et al., 2021) and even their order (Lu
et al., 2022). Based on these discoveries, there is
a line of studies focused on constructing the con-
text. LM-BFF (Gao et al., 2021) and KATE (Liu
et al., 2022) select training examples that are se-
mantically similar to the test example. Another
line of works (Su et al., 2022; Levy et al., 2022;
Ye et al., 2023) focus on mining diverse and repre-
sentative examples from a training set. Zhang et al.
(2022) utilizes active learning and reinforcement
learning to select examples for ICL. Self-adaptive
ICL (Wu et al., 2022, 2023b) proposes a two-stage
search framework to obtain the optimal in-context
examples for each test input without using a sep-
arate validation set. Different from these works,
SuperICL demonstrates that smaller models can
be integrated into large language models for su-
pervised tasks. Although it is orthogonal to these
prior works, by fine-tuning the plug-in model with
the entire training set, SuperICL reduces the neces-
sity for selecting the optimal examples from the
training set.
Moreover, prior studies also investigate how
to prepare language models for ICL. Zhao et al.
(2021) propose calibration with an empty test input
to reduce the influence of the label distribution and
ordering. MetaICL (Min et al., 2022a) meta-trains
the language model to generalize to unseen tasks
for better ICL performance. Chen et al. (2022)
propose four self-supervised objectives as interme-
diate tasks, to improve performance of language
models on ICL. Notably, both Min et al. (2022a)
Algorithm 1 Super In-Context Learning (SuperICL)
Require: Training set D = {(x1,y1), ..., (xn, Yn)}, LLM M, a small pre-trained language model P
Ensure: Predicted label yt and optional explanation et
1: Fine-tune P on D to obtain the fine-tuned plug-in model P'
2: Randomly sample a set of examples (xi, yi) from D to be the set of in-context examples D'
3: for each example (xi, yi) in D' do
4:
Predict y' and c¿ with P' where y½ is the predicted label and c¿ is the confidence score
5: end for
6: Construct the context C by concatenating all (xi, Yi, Ci, Yi)
7: for each test example xt do
8:
9:
10:
11:
Predict y½ and ct with P' for the test example
Formulate complete input I = C = (xt, Yt, Ct) where denotes concatenation
Use M to predict yt from I
(Optional) If yt yt, ask M to generate an explanation et for overriding the prediction of P'
12: end for
and Chen et al. (2022) require updating the weights,
thus are not applicable to larger black-box models
like GPT-3/4.
Besides studies aiming to improve ICL's perfor-
mance, some studies have analyzed the underlying
mechanism of ICL. Min et al. (2022b) find that
the label space, the distribution of the input text,
and the overall format of the sequence are the key
factors to ICL's performance. They also claim that
the ground labels are not significant to the perfor-
mance of ICL, but this conclusion is contradicted
by a later study (Yoo et al., 2022). Additionally,
prior studies suggest ICL could be implicitly per-
forming Bayesian inference (Xie et al., 2022) or
gradient descent (Akyürek et al., 2022; von Oswald
et al., 2022; Dai et al., 2022).
Language Model Plug-ins Large language mod-
els can exploit external tools to improve their capa-
bilities. Toolformer (Schick et al., 2023) introduces
special symbols that allow the large language mod-
els to call external APIs to complete tasks. Visual
ChatGPT (Wu et al., 2023a) plugs vision models
into ChatGPT, allowing for multimodal generation.
HuggingGPT (Shen et al., 2023) uses ChatGPT to
conduct task planning and select models according
to their function descriptions available in Hugging
Face, execute each subtask with the selected AI
model, and summarize the response according to
the execution results. Different from these works,
our work is under a classic supervised learning and
demonstrates that even tasks like text classification,
which is sometimes considered "solved" by smaller
language models, can still benefit from combina-
tion with a large language model.
3
3 Super In-Context Learning
Super In-Context Learning (SuperICL) combines
LLMs with locally fine-tuned smaller models, al-
lowing them to work together to improve perfor-
mance on supervised tasks. The smaller models
act as plug-ins, providing task-specific knowledge
and predictions, while the large pre-trained models
focus on general language understanding. The over-
all workflow of SuperICL is shown in Figure 1 and
the complete algorithm is depicted in Algorithm 1.
Plug-in Model Fine-tuning The first step in
the SuperICL process is fine-tuning a small NLP
model, e.g., ROBERTa (Liu et al., 2019), on task-
specific labeled data. This fine-tuning process on
the entire training data is made possible due to
the smaller size of the model and its local acces-
sibility. This is in contrast to ICL, whose usage
of labeled data is severely limited by the LLM's
context length. The fine-tuned small model is then
integrated as a plug-in for the LLM in subsequent
steps as follows.
Context Construction Next, a context is con-
structed for the LLM to utilize the task-specific
knowledge provided by the smaller model. This
context consists of a set of examples randomly
sampled from the training data, along with their
corresponding predictions by the smaller plug-in
model. The predictions include both the predicted
labels and their associated confidence scores. An
example is shown in Table 1.
On one hand, by incorporating the predicted la-
bels by the plug-in model, the LLM can better
understand the relationship among the input exam-
(a) Context
(b) Test Input
(c) Label Prediction
(d) Explanation
Sentence 1: Federal agent Bill Polychronopoulos said it was not known if the man, 30, would be
charged.
Sentence 2: Federal Agent Bill Polychronopoulos said last night the man involved in the Melbourne
incident had been unarmed.
ROBERTa-Large Prediction: equivalent (Confidence: 0.51)
Label: not equivalent
Sentence 1: Five more human cases of West Nile virus, were reported by the Mesa County Health
Department on Wednesday.
Sentence 2: As of this week, 103 human West Nile cases in 45 counties had been reported to the
health department.
ROBERTa-Large Prediction: not_equivalent (Confidence: 0.98)
Label: not equivalent
...
Sentence 1: Cooley said he expects Muhammad will similarly be called as a witness at a pretrial
hearing for Malvo.
Sentence 2: Lee Boyd Malvo will be called as a witness Wednesday in a pretrial hearing for fellow
sniper suspect John Allen Muhammad.
ROBERTa-Large Prediction: equivalent (Confidence: 0.82)
Label: not equivalent
Explanation for overriding the prediction: The two sentences are talking about dif-
ferent people, John Allen Muhammad and Lee Boyd Malvo, and thus the prediction should be
not_equivalent.
Table 1: An example of the constructed context and inference procedure from the MRPC dataset. We first construct
the context by sampling from the supervised dataset and attach the plug-in model's predictions. Then, for each test
example, we ask the large language model to predict the label based on the input and the plug-in model's prediction.
We use a prompt to ask the model to explain the decision if the label predicted by the plug-in model is overridden.
The text field names (e.g., Sentence 1) are the original field names provided in the dataset.
ples, ground-truth labels and the plug-in model's
expertise. This will help the LLM in the subse-
quent decision-making process to produce final
predictions. On the other hand, confidence scores
provide a measure of the plug-in model's uncer-
tainty in its predictions. By incorporating these
scores in the context, the LLM can trust predic-
tions where the plug-in model is highly confident
and be more cautious when the plug-in model is
uncertain. Furthermore, confidence scores can help
guide the LLM's attention towards in-context exam-
ples that are more challenging, enabling it to learn
from these difficult cases and potentially improve
its overall performance on the task.
In summary, by considering both the predicted
label and the associated confidence from the plug-
in model, the LLM decides whether to follow the
given predictions or to rely on its own understand-
ing of the task, leading to more accurate predictions
overall.
Inference
Once the context has been constructed,
the test input (an example shown in Table 1(b)) is
concatenated with the context, forming a complete
input for the large language model. The plug-in
model's prediction for the test input, including the
predicted label and confidence score, is also at-
tached to the input. Thus, the LLM's input includes
the context, test input, and plug-in model's predic-
tion. The LLM then generates a final prediction
for the test input, as shown in Table 1(c). Option-
ally, as shown in Table 1(d), the LLM can also
provide an explanation for its prediction, giving
insight into why it chose to override or follow the
plug-in model's prediction. This additional inter-
pretability can be valuable for understanding the
decision-making process of the combined Super-
ICL model.
4 Experiments
4.1 Experimental Settings
Benchmarks We focus on the full supervised set-
ting, where we have access to the entire training
set. We conduct experiments on two widely-used
benchmarks: the GLUE benchmark (Wang et al.,
2019) for natural language understanding tasks and
the XNLI benchmark (Conneau et al., 2018) for
zero-shot cross-lingual natural language inference
tasks, where the models are trained on English and
tested on other languages. Our goal is to examine
the learning ability of SuperICL on standard bench-
marks and whether it can empower smaller models
with its multilingual capability. For both ICL and
4
Methods
MNLI-m MNLI-mm
GPT-3.5 ICL
ROBERTa-Large
80.80
88.68
82.39
89.47
SuperICL
89.31
89.61
SST-2 QNLI MRPC QQP COLA RTE Avg.
91.39 80.52 60.05 81.64 60.51 86.28 81.32
96.44 94.07 83.09 92.11 64.55 87.00 88.68
96.79 94.16 86.03 92.14 64.57 87.73 89.90
Table 2: Experimental results on GLUE (Wang et al., 2019) development set. The metric for CoLA is Matthews
Correlation and all other tasks use accuracy.
Lang.
GPT-3.5 ICL XLM-V SuperICL
en
74.03
83.55
83.87
ar
60.15
70.78
72.28
bg
67.64
77.09
77.74
de
71.78
75.23
80.28
el
65.85
72.73
74.29
es
76.79
77.07
81.38
fr
74.99
77.01
77.47
hi
56.29
69.62
70.02
ru
65.39
73.53
76.85
SW
56.13
67.43
68.94
th
57.03
68.90
69.36
tr
66.01
72.34
72.63
ur
51.18
63.57
57.90
vi
62.91
72.91
74.45
zh
67.90
73.75
74.21
Avg.
64.94
73.03
74.11
Table 3: Experimental results on the XNLI (Conneau
et al., 2018) test set. The metric is accuracy.
SuperICL, we only consider the prediction to be
correct when the generated label matches the prede-
fined label exactly. For analytical experiments, we
evaluate the model on a subset of GLUE consisting
of three representative tasks: MNLI, SST-2 and
MRPC, due to budget constraints.
Large Language Model and Plug-ins We use
OpenAI's text-davinci-003 language model,
also known as GPT-3.5. For the GLUE benchmark,
we use ROBERTa-large (Liu et al., 2019) as the
plug-in model. For the XNLI benchmark, we use
XLM-V (Liang et al., 2023), as the plug-in model.
Both models are fine-tuned on their respective tasks
to serve as plug-ins for SuperICL. For GLUE tasks,
we randomly select 32 examples from the training
set. For XNLI, as the input is multilingual, the BPE
tokenizer used in GPT-3.5 results in a longer token
sequence. Thus, we use at most 16 examples for
each language. Note that for some languages (e.g.,
Thai), the in-context examples are fewer than 16,
as we fit as many examples as possible to the maxi-
mum allowed sequence length of 4,096 of GPT-3.5.
For our main experiments and all analysis experi-
ments, we compare the performance of SuperICL
with ICL (Brown et al., 2020) on the same selection
of in-context examples and the predictions made
by the plug-in models alone.
4.2 Main Results
GLUE As shown in Table 2, SuperICL outper-
forms both GPT-3.5 ICL and the plug-in model
ROBERTa-Large with an average advantage of 8.58
and 1.22 on GLUE, respectively. It is worth noting
that SuperICL consistently outperforms the base-
lines on all tasks, which makes it a reliable choice
that would not compromise the performance of the
plug-in model.
XNLI For XNLI, as presented in Table 3, while
XLM-V (Liang et al., 2023) is specifically designed
for multilingual tasks, combining it with GPT-3.5
can still lead to significant improvements in most
languages. However, SuperICL fails to enhance the
performance of XLM-V for Urdu. It is worth men-
tioning that GPT-3.5 ICL also exhibits poor perfor-
mance for Urdu, implying that GPT-3.5 may lack
ability for low-resource languages like Urdu. This
is also consistent with recent analysis on the multi-
linguality of GPT-3.5/ChatGPT (Lai et al., 2023).
Additionally, since the BPE tokenizer used in GPT-
3.5 yields more tokens for non-Latin languages, the
number of in-context examples is limited, adversely
affecting the model's performance. We believe that
subsequent GPT models that employ a multilingual
tokenizer, train on more non-English data, and have
a longer maximum context can achieve even better
performance for cross-lingual SuperICL.
4.3 Ablation Study
We conduct an ablation study to understand the
effect of each component in SuperICL. We inves-
tigate the performance of the three components in
SuperICL: (a) Context, which comprises in-context
examples; (b) The confidence scores of the plug-in
model in both in-context examples and test input;
(c) The plug-in model's prediction for the test input.
The experimental results are shown in Table 4:
Components
MNLI SST-2 MRPC
(a) Ctxt. (b) Conf. (c) Ref.
GPT-3.5 ICL
ROBERTa-Large
(1) ✓
(2) ✓
(3) X
✓
(4) X
✓
✓
80.80 91.39 60.05
88.68 96.44 83.09
81.23 92.43 65.69
88.75 96.67
83.09
88.89 96.44 83.59
88.84 96.44 83.58
89.31 96.79 86.03
Table 4: Experimental results of the ablation study. (a)
Ctxt.
means the in-context examples from the train-
ing set; (b) Conf. represents the plug-in model's confi-
dence score; (c) Ref. means whether we use the plug-in
model's prediction for the test input.
Method
MNLI
SST-2 MRPC
0.22%
0.23% 12.50%
%Overriden
Overridden Accuracy 81.81% 100.00% 64.71%
Table 5: Statistics of overridden predictions. "%Over-
ridden" indicates the percentage of final predictions
that differ from the plug-in model's predictions, out of
the total number of examples. "Overridden Accuracy"
represents the percentage of correct predictions among
the overridden ones.
300
All
Overriden
(1) We first attempt to remove the plug-in model's
prediction for the test input. This has a signifi-
cant negative impact on the performance of ICL
as it creates a mismatch between in-context ex-
amples and test input. Interestingly, even though
we remove the plug-in model for test input, Super-
ICL can still outperform ICL. We suspect this is
due to an in-context effect similar to knowledge
distillation (Hinton et al., 2015), which transfers
task knowledge from the fine-tuned ROBERTa to
GPT-3.5. (2) We attempt to remove the confidence
scores from SuperICL which results in a decrease
in its performance. This is because GPT-3.5 be-
comes unaware of the uncertainty of ROBERTA,
and as a result, it is unable to determine when to
override the prediction. Also, similar to remov-
ing the softmax score from knowledge distillation,
removing the confidence score makes knowledge
transfer less effective. (3) When removing all in-
context examples, SuperICL is essentially doing
zero-shot inference for the test input. Although
there is a slight improvement over ROBERTa, we
can see adding in-context examples help Super-
ICL learn to calibrate the confidence and override
ROBERTa's prediction. Also similar to ICL versus
zero-shot inference, adding in-context examples
helps GPT-3.5 to improve its own task-specific per-
formance. (4) Further removing confidence scores
from zero-shot inference also slightly decreases the
performance.
4.4
Analysis on Prediction Overrides
We also analyze the statistics of predictions over-
ridden by GPT-3.5, as displayed in Table 5. A
significant difference can be observed between var-
ious datasets. On both MNLI and SST-2, GPT-3.5
overrides only a minimal portion of examples (ap-
#Examples
250
200
150
100
50
0
0.5
0.6
0.7
0.8
0.9
1.0
Confidence
Figure 2: Effect of plug-in model confidence for over-
rides. The figure is the distribution of ROBERTa confi-
dence for all examples (blue) and examples with a final
prediction overridden by GPT-3.5 (orange) on MRPC.
proximately 0.2%), but with high accuracy. Con-
versely, GPT-3.5 overrides a substantial percentage
of 12.5% of predictions made by ROBERTa, al-
though with lower accuracy. These findings suggest
that the override behavior of SuperICL is heavily
reliant on the specific dataset and the performance
of the plug-in model.
To gain insights into the decision-making pro-
cess of the LLM in overriding the predictions of
the plug-in model, we examine the distribution of
confidence levels exhibited by ROBERTa and the
extent to which GPT-3.5 overrides them, as shown
in Figure 2. The findings reveal a pattern where
GPT-3.5 tends to override predictions when the
plug-in model's confidence is low. This behavior
supports our motivation and indicates that GPT-3.5
recognizes the uncertainty associated with the plug-
in model's predictions via the confidence scores.
4.5 Analysis on Example Selection
We compare ICL and SuperICL by analyzing the
sensitivity of different in-context examples. To en-
sure a fair comparison, we randomly sample five
batches of in-context examples for each dataset us-
ing different random seeds, ensuring that the same
90
80
70
MNLI
SST-2
98
93
88
87
82
77
MRPC
60
72
83
50
67
40
78
62
1
2
4
8
16
32
1
2
4
8
16
32
1
2
4
8
16
32
⚫ICL
ROBERTa-Large
SuperlCL
ICL
ROBERTa-Large
-SuperICL
ICL
ROBERTa-Large
-SuperICL
Figure 3: Effect of number of examples on the performance of ICL and SuperICL. The results are averages of
three runs.
MRPC
SST-2
MNLI
Method
42
ICL
ICL
Random seed
Method
MNLI SST-2
MRPC
Var.
2
3
ICL
80.80 91.39 60.05
ROBERTa-Large
SuperICL + ROBERTa
88.68 96.44 83.09
89.31 96.79 86.03
DeBERTa V3-Large
90.49 96.56 90.44
0
1
80.80 81.26 79.74 81.26 80.79 0.39
ROBERTa 88.68 88.68 88.68 88.68 88.68
SuperICL 89.31 88.94 88.79 89.17 88.78 0.06
91.39 94.04 94.38 93.12 93.46 1.35
ROBERTa 96.44 96.44 96.44 96.44 96.44
SuperICL 96.79 96.56 96.56 96.56 96.56 0.01
60.05 73.53 73.28 73.28 65.44 37.50
ROBERTa 83.09 83.09 83.09 83.09 83.09
SuperICL 86.03 87.99 87.75 84.31 86.52 2.20
ICL
Table 6: Accuracy and variance of ICL and SuperICL
with example selections randomly sampled with differ-
ent seeds.
SuperICL + DeBERTa 90.76 96.79 90.93
Table 7: Experimental results of SuperICL with differ-
ent plug-in models.
Test Set
# Examples
ICL
R1
1000
R2 R3
All
1000 1200 3200
in-context examples are used for both ICL and Su-
perICL.
Our results, as shown in Table I show that ICL
demonstrates a larger variance compared to Super-
ICL, especially on MRPC, and its performance is
drastically affected by the selection of in-context
examples. On the other hand, SuperICL consis-
tently outperforms both ICL and ROBERTa, while
maintaining a more stable performance.
We argue that SuperICL's improved stability is
due to the added reference prediction from the plug-
in model, which has been trained on a large super-
vised training set. This helps the LLM to focus
on learning to correct ROBERTa's predictions and
override them when needed, minimizing the differ-
ences caused by different in-context examples.
4.6 Analysis on Number of Examples
We compare the effect of the number of examples
on the performance of ICL and SuperICL in Fig-
ure 3. As shown, SuperICL can already achieve the
best accuracy with merely 2 or 4 examples, while
ICL requires more examples to achieve reason-
able accuracy. Also, on the more difficult dataset
7
59.50 52.40 52.58 54.69
ROBERTa-Large 41.60 27.40 24.58 30.78
SuperICL
56.10 42.70 44.17 47.44
Table 8: Zero-shot results on ANLI (Nie et al., 2019).
ICL and SuperICL use in-context examples sampled
from MNLI. The ROBERTa-Large model is fine-tuned
on MNLI. R1, R2 and R3 denote the first, second and
third round of adversarial attacks, respectively.
MRPC, SuperICL can improve the performance of
ROBERTA by a large margin with more in-context
examples present.
4.7 Analysis on Plug-in Models
We also attempt to replace the plug-in model with
a state-of-the-art model, DeBERTa V3-Large (He
et al., 2021) fine-tuned on the datasets. Our re-
sults, presented in Table 7, demonstrate that Su-
perICL continues to enhance the performance of
state-of-the-art models, although the improvement
is smaller compared to ROBERTa. This reduction
may be attributed to the smaller capability gap be-
tween the small and large models. However, we
expect that using an even more advanced large lan-
guage model in the future will resolve this issue.
(1) MNLI
Premise: Then he sobered.
Hypothesis: He was always sober.
ROBERTA Prediction: neutral (Confidence: 0.7) X
Final Prediction: contradiction ✓
Explanation: The premise states that he sobered, implying that he was not sober before, while the hypothesis states that he
was always sober, which contradicts the premise.
(2) MNLI
Premise: Without the discount, nobody would buy the stock.
Hypothesis: Nobody would buy the stock if there was a discount.
ROBERTA Prediction: contradiction (Confidence: 0.6) ✓
Final Prediction: entailment X
Explanation: The premise and hypothesis are essentially the same statement, but with the premise being a negative
statement and the hypothesis being a positive statement. The model interprets this as a contradiction, but it should be an
entailment.
(3) SST-2
Sentence: no screen fantasy-adventure in recent memory has the showmanship of clones' last 45 minutes.
ROBERTA Prediction: negative (Confidence: 0.54) X
Final Prediction: positive ✓
Explanation: The sentence is actually a positive statement, praising the last 45 minutes of the movie for its showmanship.
The prediction was flipped because the sentence also contains the word "no" which can be interpreted as a negative sentiment.
Table 9: Examples of SuperICL's predictions and explanations. The contexts are omitted. The prompts here are
modified for clarity. The original prompt template is shown in Table 1. The correct and incorrect predictions are
marked with ✔and X, respectively.
4.8 Analysis on Adversarial Robustness
Additionally, we analyze the adversarial robust-
ness of SuperICL by testing it on ANLI (Nie et al.,
2019). ANLI is a dataset for evaluating the ro-
bustness and generalization of natural language
inference (NLI) models. It consists of 16,000
premise-hypothesis pairs that are categorized into
three classes: entailment, contradiction, and neu-
tral. The dataset is constructed with three rounds
(R1, R2, and R3) and thus has three splits, with
R3 being the most challenging and diverse. ANLI
is collected using a human-and-model-in-the-loop
training method, where human annotators act as
adversaries and attempt to fool the model into mis-
classifying while still being understandable to other
humans. This benchmark is designed to be chal-
lenging for language models including ROBERTa
as ROBERTA is attacked in R2 and R3 of data con-
struction.
As shown in Table 8, GPT-3.5 ICL is rather ro-
bust while ROBERTa-Large is vulnerable to adver-
sarial attack. However, this directly has a negative
impact on SuperICL. Although SuperICL achieves
better performance than ROBERTa-Large, it under-
performs ICL. This finding suggests that Super-
ICL's performance relies on the performance of
the incorporated plug-in model and adversarial at-
tack to the plug-in model could lead to a drastic
performance drop for SuperICL.
5 Case Study
We conduct a case study to better understand the be-
havior of SuperICL, with three examples presented
in Table 9. We find that even without any explicit
task instruction, GPT-3.5 demonstrates the ability
to comprehend the tasks and explain its own rea-
soning. In the first example from Table 9, GPT-3.5
effectively grasps the implication in the premise
that "he" was not sober. However, in the second
example, GPT-3.5 incorrectly flips the prediction,
possibly due to confusion caused by negation. This
phenomenon has been recognized as a common
flaw in LLMs, as noted by Hosseini et al. (2021)
and Jang et al. (2023). In the last example, GPT-3.5
not only corrects RoBERTa's prediction success-
fully but also provides an analysis explaining why
ROBERTA makes the wrong prediction.
6 Conclusion and Future Work
In this paper, we propose SuperICL, a simple yet
effective method for combining a large language
model API with a locally fine-tuned plug-in model.
For future work, we would like to explore using
large language models to plan for the fine-tuning
of the local plug-in model for an unseen task and
automate the entire workflow. Also, a theoretical
analysis may be important to further reveal the
internal mechanism of SuperICL.
8
Limitations
Additional Delay and Cost Since SuperICL in-
volves serialized small and large models, the total
inference delay equals to the sum of the inference
delay of the two models. Also, calling the API of
large language model can be expensive compared
to using a locally deployed small model.
Adversarial Vulnerability As discussed in Sec-
tion 4.8, the vulnerability of the plug-in model to
adversarial attacks can be inherited by SuperICL.
Thus, when the plug-in model is under adversarial
attack, the entire system could underperform ICL.
Limited Evaluation Tasks Due to space and
budget limit, we only investigate text classification
in this paper. However, it would be interesting to
also look into generation tasks such as text summa-
rization, question answering, and semantic parsing.
Broader Impact
As a technique that combines large and small lan-
guage models for improved predictions, SuperICL
shares the potential social biases of language mod-
els. While our approach is not likely to amplify
these biases compared to other methods, it is im-
portant to investigate whether SuperICL has any
effect on increasing or decreasing them. Further-
more, incorporating small models as plug-ins to the
inference of large language models may lead to a
slightly higher carbon footprint, resulting in a nega-
tive environmental impact. Therefore, practitioners
should carefully consider the trade-offs between
performance gains and environmental costs when
using SuperICL.
Acknowledgements
We would like to thank Junheng Hao, Ziyi Yang,
Dan Iter and Daya Guo for discussion.
References
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke
Zettlemoyer, and Marjan Ghazvininejad. 2022. In-
context examples selection for machine translation.
arXiv preprint arXiv:2212.02437.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas,
Tengyu Ma, and Denny Zhou. 2022. What learning
algorithm is in-context learning? investigations with
linear models. arXiv preprint arXiv:2211.15661.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In NeurIPS.
Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor
Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa
Kozareva. 2022. Improving in-context few-shot
learning via self-supervised training. In NAACL-
HLT, pages 3558-3573. Association for Computa-
tional Linguistics.
Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-
ina Williams, Samuel R Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. Xnli: Evaluating cross-
lingual sentence representations. arXiv preprint
arXiv:1809.05053.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang
Sui, and Furu Wei. 2022. Why can gpt learn in-
context? language models secretly perform gra-
dient descent as meta optimizers. arXiv preprint
arXiv:2212.10559.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In ACL-IJCNLP, pages 3816-3830. Asso-
ciation for Computational Linguistics.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style
pre-training with gradient-disentangled embedding
sharing. arXiv preprint arXiv:2111.09543.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.
Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R. De-
von Hjelm, Alessandro Sordoni, and Aaron C.
Courville. 2021. Understanding by understand-
ing not: Modeling negation in language models.
In NAACL-HLT, pages 1301–1312. Association for
Computational Linguistics.
Joel Jang, Seonghyeon Ye, and Minjoon Seo.
2023. Can large language models truly understand
prompts? a case study with negated prompts. In
Transfer Learning for Natural Language Processing
Workshop, pages 52-62. PMLR.
Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben
Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui,
and Thien Huu Nguyen. 2023. Chatgpt beyond en-
glish: Towards a comprehensive evaluation of large
language models in multilingual learning. arXiv
preprint arXiv:2304.05613.
Itay Levy, Ben Bogin, and Jonathan Berant. 2022.
Diverse demonstrations improve in-context
compositional generalization. arXiv preprint
arXiv:2212.06800.
Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Na-
man Goyal, Marjan Ghazvininejad, Luke Zettle-
moyer, and Madian Khabsa. 2023. Xlm-v:
Overcoming the vocabulary bottleneck in multilin-
gual masked language models. arXiv preprint
arXiv:2301.10472.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.
Few-shot learning with multilingual language mod-
els. arXiv preprint arXiv:2112.10668.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022. What
makes good in-context examples for gpt-3? In Dee-
LIO@ACL, pages 100–114. Association for Compu-
tational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2022. Fantastically
ordered prompts and where to find them: Overcom-
ing few-shot prompt order sensitivity. In ACL, pages
8086-8098. Association for Computational Linguis-
tics.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022a. Metaicl: Learning to learn
in context. In NAACL-HLT, pages 2791-2809. As-
sociation for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022b. Rethinking the role of demonstra-
tions: What makes in-context learning work?
EMNLP, pages 11048-11064. Association for Com-
putational Linguistics.
In
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2019. Ad-
versarial nli: A new benchmark for natural language
understanding. arXiv preprint arXiv: 1910.14599.
OpenAI. 2023. Gpt-4 technical report.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì,
Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. 2023. Tool-
former: Language models can teach themselves to
use tools. arXiv preprint arXiv:2302.04761.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,
Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A Smith, et al. 2022. Selec-
tive annotation makes language models better few-
shot learners. arXiv preprint arXiv:2209.01975.
Johannes von Oswald, Eyvind Niklasson, Ettore Ran-
dazzo, João Sacramento, Alexander Mordvintsev,
Andrey Zhmoginov, and Max Vladymyrov. 2022.
Transformers learn in-context by gradient descent.
arXiv preprint arXiv:2212.07677.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In ICLR.
OpenReview.net.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023a. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671.
Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiang-
tao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu.
2023b. Openicl: An open-source framework for in-
context learning. arXiv preprint arXiv:2303.02913.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2022. Self-adaptive in-context learning.
arXiv preprint arXiv:2212.10375.
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2022. An explanation of in-context
learning as implicit bayesian inference. In ICLR.
OpenReview.net.
Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,
Tao Yu, and Lingpeng Kong. 2022. Progen: Pro-
gressive zero-shot dataset generation via in-context
feedback. In EMNLP (Findings), pages 3671–3683.
Association for Computational Linguistics.
Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu,
and Lingpeng Kong. 2023. Compositional ex-
emplars for in-context learning. arXiv preprint
arXiv:2302.05698.
Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim,
Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-
goo Lee, and Taeuk Kim. 2022. Ground-truth labels
matter: A deeper look into input-label demonstra-
tions. In EMNLP, pages 2422-2437. Association for
Computational Linguistics.
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-
tive example selection for in-context learning. In
EMNLP, pages 9134-9148. Association for Compu-
tational Linguistics.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language models.
In ICML, volume 139 of Proceedings of Machine
Learning Research, pages 12697-12706. PMLR.
10
