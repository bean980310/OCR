--- Page 1 ---
arX1v:2307.03692v1 [cs.CL] 5 Jul 2023

Becoming self-instruct: introducing early stopping
criteria for minimal instruct tuning

Waseem AlShikh ManhalDaaboul Kirk Goddard BrockImel Kiran Kamble

Parikshith Kulkarni

Melisa Russak

Writer, Inc.
{waseem,...,melisa}@writer.com

Abstract

In this paper, we introduce the Instruction Following Score (IFS), a metric that
detects language models’ ability to follow instructions. The metric has a dual
purpose. First, IFS can be used to distinguish between base and instruct models.
We benchmark publicly available base and instruct models, and show that the
ratio of well formatted responses to partial and full sentences can be an effective
measure between those two model classes. Secondly, the metric can be used as
an early stopping criteria for instruct tuning. We compute IFS for Supervised
Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to
follow instructions relatively early in the training process, and the further finetuning
can result in changes in the underlying base model semantics. As an example of
semantics change we show the objectivity of model predictions, as defined by an
auxiliary metric ObjecQA. We show that in this particular case, semantic changes
are the steepest when the IFS tends to plateau. We hope that decomposing instruct

tuning into IFS and semantic factors starts a new trend in better controllable instruct
tuning and opens possibilities for designing minimal instruct interfaces querying

foundation models.

1 Introduction

Large Language Models (LLMs) finetuned on in-
struct data can behave like conversational agents
(Alpaca: Taori et al. [2023] Self-Instruct: Wang
et al. |2023). The recipe for a chat model is well-
defined: one needs to perform instruction tuning,
which means supervised finetuning (SFT) of an
LLM on tuples of instruction and response (Long-

pre et al.|2023).

Open-source datasets vary in quality and quantity,
ranging from 1k examples (Zhou et al. {2023) to
over 800k examples (Anand et al.{2023). In addi-
tion, there are more than a dozen open-source
base LLMs, such as LLaMA (Touvron et al.

, OPT (Zhang et al. , GPT-Neo (Gao
et al. , Palmyra (Writer , and others,

Preprint. Under review.

which result in a plethora of possible combina-
tions leading to distinct instruct models.

We can see instruct tuning attempts through the
lens of the "imitation models" - concept intro-
duced by Gudibande et al. ie., efforts to
distil closed (and possibly much bigger) propri-
etary models like ChatGPT (OpenAI , Bard
(Pichai{2023), and Claude (AnthropicAT[2023).

Little is known about the qualitative impact of
the distillation process on the base model (Hin-
ton, Vinyals, and Dean 2013) Imitation success
is measured in terms of knowledge (e.g., HELM
Liang et al.{2022), skills (e.g., Natural Questions
Kwiatkowski et al or manual checks based
on human preferences (Zhou et al. . There is
no consensus whether a manual check that might
skew the metric towards style and formatting of
responses is a good overall metric (Gudibande


--- Page 2 ---
et al. |2 . A fairly recent attempt to more ro-
bustly evaluate instruct models is the Hugging-
face Leaderboard (Huggingface [2023b), which
evaluates models against four key benchmarks
from the Eleuther AI Language Model Evalua-
tion Harness (Gao et al.{2021).

Ablation studies have shown that both the diver-
sity and quality of the training data play a cru-
cial role in model performance (Chen et al.[2023}
Zhou et al.[2023). Low Training Data Instruction
Tuning (LTD Tuning) suggests that task-specific
models can gain 2% performance when trained on
less than 0.5% of the original data. Moreover, pro-
longed instruction tuning can decrease the founda-
tional model knowledge (Gudibande et al.

and can be seen as the out-of-distribution task for
a downstream task of instruct-tuning (Kumar et al.
(2022).

In this study, we want to lay the foundation for
instruct models research by defining the neces-
sary (but not sufficient) condition for an instruct
model. Let’s conduct a thought experiment.

Let’s put all models behind a closed API (a recent
equivalent of a black box). Is the model instruct-
tuned or not? Knowledge benchmarks could be
similar for vanilla and instruct models for LTD
tuning. Skills tests would highly depend on the
model size, which is not known. The simplest
way of solving the riddle would be to ...chat
with the model and judge the tone of the response.
For a vanilla model, we expect a next prediction
word attempt, whereas for instruct models, we ex-
pect them to follow instructions. We introduce a
metric that captures this tone difference - Instruct
Following Score (IFS). We call this problem a
"tone alignment" issue.

The IFS is defined as a ratio of "answer-like"
responses to "continuation-like" responses on a
predefined set of instructions, where class of a
response is determined by a binary classifier.

We benchmark publicly available base and in-
struct models, and show that the ratio of well
‘ormatted responses to partial and full sentences
can be an effective measure between vanilla and
instruct following models. Moreover, we calcu-
late IFS for SFT for 7B and 13B LLaMA models,
in the hope of finding a stopping criterion for a
minimal instruct tuning.

To draw a comparison between the learning curve
‘or response tone and the acquisition of seman-
tic and domain-specific knowledge, we propose
a supplementary metric called ObjecQA. This
auxiliary metric quantifies the objectivity of a
model’s predictions, as this signal can be identi-
fied within the dataset. While this feature choice
is arbitrary, we aim to discover possibly more

general heuristics for better control over the train-
ing phases, including identification of "format-
infusion" and "knowledge-infusion" stages.

The paper is organised as follows. In Section 2,
we discuss the necessary conditions for a model to
be considered an instruct model and data prepara-
tion for IFS. The response tone classifier training
is described in Section 4. In Section 5, we present
results for instruct models and compare them to
baseline vanilla models in terms of instruct tone
and semantic shifts. The study ends with conclu-
sions and future directions proposed in Section
6.

2 Background and Related Work

The response tone alignment problem is a part
of a broader intent alignment topic. In princi-
ple, LLMs are not aligned with users’ intents
because their language modeling objective, e.g.,
predicting the next token of a training document,
is different from the following instruction target.

One successful approach for aligning both objec-
tives is to prompt models using zero- or n-shot
techniques, where the response would look like a
completion of a document containing QA (Brown

et al.[2020] Radford et al.[2018).

Another approach is to instruct and tune a vanilla
model on tuples of instruction and response, so
the model, as part of learning, acquires skills to
imitate the correct format response (Alpaca: Taori
et al.[2023} Self-Instruct: Wang et al.|2

In the InstructGPT paper (Ouyang et al.
the criterion "fails to follow the correct instruc-
tion / task" was included in the list of human
evaluation metadata for a reward model (RM)
used in the PPO algorithm (Schulman et al./2017)
to fine-tune the SFT models to maximize their
reward.

We aim to isolate and understand the tone compo-
nent by evaluating each strategy as a style format-
ting problem rather than using knowledge and lan-
guage understanding-based metrics, e.g., MMLU

(Hendrycks et al.{2021).

3 Instruction Following Index

3.1 Motivation

An instruction following model intuitively be-
haves like a conversational agent, i.e. always
assume the input is an instruction, and depending
on its understanding tries to provide an answer or
ask follow up questions. In contrast, a model that
does not follow instructions will try to predict

--- Page 3 ---
next tokens and optionally provide an answer or
continue with the next instruction. The distinc-
tion between two model classes becomes more
clear for an instruction that is an incomplete sen-
tence fragment. An instruction following model
will never try to complete the instruction.

It is crucial to emphasise that the quality of re-
sponses is purposely beyond the scope of this
classification. The above criteria are thus neces-
sary but not sufficient conditions for a chat model.

In this paper, we introduce the Instruction Fol-
lowing Score (IFS), defined as a ratio of "answer-
like" responses to "continuation-like" responses
to a predefined set of instructions. The class of
a response is determined by a binary classifier
(called subsequently as "response tone classifier").
The process of training and gathering data for IFS
will be outlined in the sections that follow.

In this paper, we use interchangeably "conver-
sational tone" and "instruction following tone,"
meaning a class of "answer-like" responses. The
process of fine-tuning a base model to obtain an
instruct model is called "instruction tuning."

I - Instruction

I, - Partial (fragmented) instruc-
tion

* Inference outputs:
I,, - Continuation of the instruction

R - Response

In fact, combinations of those 4 parts gives all
possible pairs of inputs and outputs for vanilla
and chat models. In the table below we recom-
bine the parts and give and assign them a binary
score depending whether the model responds like
a chat model.

(I, R) The response R for instruction I is
conversational. A model whose all re-
sponses would resemble the above form
would be an instruction following, so the
response has label 1.

3.2 Dataset
. : (Ip,R) The response R for partial instruction [,,
The dataset for IFS is derived from a chat dataset, is also conversational, but in this case
which originally consists of pairs (instruction, re- the model has not enough context to pro-
sponse). We will need to model inputs and out- vide any answer except requesting for
puts for models that aren’t following instructions. more information. This response is also
The main idea for data generation is to append in- labeled as 1.
struction to response and then consider different
subdivisions into two phrases, as shown in Figure
(Ip,I-) The model completes the fragmented in-
struction (executing next word predic-
o tion task). The pair does not look like a
ferent datapoint splits : :
j R conversation, so the label is 0.
What is the capital of France? The capital of France is Paris.
aa le+R (I , I.) The model generates next instructions
What is the capital of France? The capital of France is Paris. (similarly to next word prediction task
Ip Ie again), which gives the response label 0.
What is the capital of France? The capital of France is Paris.
tructon vesponse (Ip,.Jc+R) In this case, the model completes the

Figure 1: IFS dataset generation. Different splits
define fragments: I, R, I, Ic.

If the cut regenerates (instruction, response) we
get the ideal input and output for a chat model. If
we shift the split to the right or to the left, we can

obtain incomplete sentences (fragmented) which (J,J.+R)

represent unfinished instructions or continuation
of instructions followed by responses. To summa-
rize, we can get:

* Inference inputs:

instruction then replies (executing next
word prediction task too). Although au-
thors might imagine people attempting
have such dialogue, we treat instruction
completion as a sign of failed conversa-
tion. Label is 0.

The model generates another instruction
then replies to its generation. The dia-
logue fails giving the response label 0.

Examples for each case are shown in Table[]]

--- Page 4 ---
Case Example chat?

I: What if people had 40
legs?

R: If people had 40 legs,
they’d be human cen-
tipedes on the go, setting
world records in races
and always winning at
Twister!

(, R)

I,: What if

R: It seems like your
question is incomplete.
Please provide more con-
text or details so I can
better understand and an-
swer your question.

(Up.R)

Ip: What if

(Ipc) I: people had 40 legs?

I: What if people had 40

legs? 0
I: What if people had 3

eyes?

(I, Ie)

Ip: What if

I. + R: people had 40
legs? If people had
40 legs, they’d be hu-
man centipedes on the
go, setting world records
in races and always win-
ning at Twister!

(pte + R)

I: What if people had 40
legs? 0
I, + R: What if peo-
ple had 3 eyes? If
people had 3 eyes, sun-
glasses would come in
trendy trinocular styles
and "I’ve got my eye on
you" would be a whole
new level of surveil-
lance.

(Le + R)

Table 1: Examples of possible combinations of
fragments J, R, I,, I.. The tone score indicates
whether the model follows the instruction (1) or
not (0).

In summary, among the six potential combina-
tions, only two instruct model cases exist: (Jp, R)
and (J, R). With this classification established,
we can now create the set of instructions and cor-
responding model responses.

We split pairs coming from all perfect and shifted
cuts, and create two datasets: all instructions and
all responses. The set of instructions is used to
generate data used for prompting models, while

the set of responses is used to generate data for
the binary classifier. Figure [2] shows how chat
data is split and used for in our experiment.

As a source of clean text, we utilized the OpenAs-
sistant chat dataset (KGpf et al.[2023). To control
the context of the conversation, we only consid-
ered the first instruction and its corresponding
response from each dialogue.

3.2.1 Instructions dataset

In the instruction dataset, data points consist of
instructions sourced from OpenAssistant data, ei-
ther unmodified (J) or fragmented (I,). We ob-
tained a total of 7340 examples, with an approxi-
mate 50% split between fragments and complete
sentences. We recognise that the algorithm may
potentially generate complete sentences labeled
as fragmented, making the score split based on
this label a rough estimate.

Table[2|shows examples of full and partial instruc-
tions.

Instruction Label
What is the difference between _ partial
HTML

What is the difference between full
HTML and JavaScript?

Who wears partial
Who wears short shorts? full

Table 2: Examples of instructions and their cate-
gory.

3.2.2 Responses dataset

The set of responses represents the right side of
Fig. fi ie., original responses or responses shifted
to the right. The collected classes are:

Label 0 : I, +R

Labell : R

We drop the fine-grained classification of re-
sponses and assign them only to "answer-like"
(label !) or "continuation-like" (label 0). These
samples are later used to train the binary classifier.
Table [3]shows examples of responses and their
labels.

--- Page 5 ---
Response chat?
it fly so fast? The fastest flying bird 0
is the peregrine falcon.

agent? I’m not a FBI agent. 0
When onions are cut, they release a 1
chemical called sulfuric acid.

James Madison was the primary au- 1

thor of the Constitution and the Bill
of Rights.

Table 3: Examples of responses and their cate-
gories.

4 Binary classifier and Instruction
Following Score

The binary classifier for tone response classifica-
tion has been chosen as the best binary classifier,
trained on the set of responses using Hugging-
face AutoTrain (Huggingface 20232). Since the
dataset consisted of a roughly equal split of neg-
ative and positive samples, we have chosen ac-
curacy as the comparison metric. The winning
architecture was BertForSequenceClassification,
and the final classifier metrics (as reported by
AutoTrain) are presented in Table[4

Metric Value
Accuracy 0.970
Precision 0.983
Recall 0.925

Table 4: Validation metrics

We define Instruction Following Score (IFS) as a
ratio of all responses classified as "answer-like"
(label 1) to all responses obtained by prompting
the instructions dataset. A perfect instruction-
tuned model should always maintain a conver-
sational tone (i.e. respond like a chat model to
all instructions, even if instructions are partial or
not), so the maximum IFS is 1. We can addi-
tionally define two related metrics IFSpartia and
IFS¢un, being ratio of "answer-like" responses to
all partial and full instructions respectively.

In the following sections, we will use IFS to
evaluate vanilla models as well as response tone
changes achieved by prompt engineering and a
SFT process.

5 Results
5.1 Baseline

We used the IFS metric to evaluate several pub-
licly available models. Since the dataset consists
of less than 50% fragmented instructions (includ-
ing false positives generated by the algorithm),
we expected the base model to obtain IFS be-
low this level when prompted without additional
affixes. Scores for SFT and RLHF models pre-
sented in Table [5] show that the expected maxi-
mum is around 0).8-0.9, whereas the most promi-
nent difference between a base and instruction-
following LLMs is the relative difference between
TFSpartiat and IFS fun.

Model IFS [FSpartiaa = TF Spun
GPT-2 0.68 0.67 0.7
RedPajama-3B 0.33 0.17 0.49
LLaMa-7B 0.34 0.19 0.5
LLaMA-13B 0.81 0.79 0.82
LLaMA-33B 0.74 0.68 0.81
davinci 0.29 0.17 0.42
Palmyra-x 0.68 0.45 0.91
Palmyra-base 0.32 0.17 0.48
Palmyra-large 0.32 0.17 0.47
text-davinci-003 0.62 0.37 0.88
GPT-3.5-turbo 0.9 0.83 0.97
GPT-4 0.88 0.8 0.97
Palmyra-instruct 0.61 0.36 0.86

Table 5: Baseline: Instruction Following Score
(IFS) for selected publicly available models.

5.2. Prompt engineering

A very simple method to encourage LMs to fol-
low instructions is to add extra prompt suffixes
or wrappers around instructions, which could dis-
rupt the next token prediction task and produce
responses. Figure [3] presents three versions of
prompts:

Instruction tuning prompts

A Alpaca prompt B. Our prompt .No prompt

Below isan instruction that {Instruction} {instruction}

dserbes a task, White a response
that appropriately completes the

uy ‘Ht Response:

{response}
‘#8 Inetruction: Ce)
{instruction}

‘Ht Response:

{response}

Figure 3: Comparative illustration of instruction
tuning prompts. A. Alpaca prompt, a wrapper
around instruction, B. only Alpaca suffix, C. no
prompt, the baseline

--- Page 6 ---
Stage 1: Classifier
training & LM prediction

training

instruction

IFS
Stage 2: Evaluation
binary inference
classifier
LM
response

Figure 2: IFS training and evaluation pipeline

The results presented in Table [6] show that vari-
ants of both prompts are equally effective. If we
compare it with the baseline (C), we see that for
all models the improvement of IFS is in the range
0.5-0.6. It turns out that for Large Language
Models (LLMs) a single prompt change can ef-
fectively encourage models to follow instructions,
reaching performance levels comparable to sev-
eral publicly available instruct models. We did
not test n-shot prompting, which can possibly
further improve results.

Dataset TFS UFSpartiat  UFStun
LLaMa-7B, 0.74 071 077
LLaMa-7Bg 0.75 0.73 0.78
LLaMa-7Be 0.34 0.19 0.5
LLaMA-13B, 0.81 0.74 0.88
LLaMA-13Bg_ 0.81 0.79 0.82
LLaMA-13Be 0.31 0.18 0.43
LLaMA-33Bq 0.87 0.85 0.89
LLaMA-33Bg__ 0.74 0.68 0.81
LLaMA-33Be 0.33 0.18 0.47

Table 6: Instruction Following Score (IFS) for
models with and without prompt suffixes.

5.3. Supervised finetuning

In this study, we opted for 7B and 13B LLaMA
models as the base LLMs for SFT. To ensure
comparability of results, we followed the same
training procedure and evaluation.

We used the gpt4all v1.3-groovy introduced in
Anand et al.[2023]as the instruct dataset. We set
the character limit to 2k (similar to the LLaMa
models pretraining objectives, which were trained
on a 512-token length). Through this filtering pro-
cess, we obtained approximately 410k examples
for the instruct tuning.

Models were trained with the modified Alpaca
prompt:

PROMPT_DICT = {
"prompt_input": ("{instruction}\n\n{
<> input}### Response:"),
"prompt_no_input": ("{instruction}###
<> Response:"),

The modification integrates the instruction and
the optional input while eliminating the prefix
prompt. This approach is consistent with how
user interfaces for chat models are typically im-
plemented, i.e., as a single dialog input box. We
could use the full Alpaca wrapper, but since both
prompting techniques lead to similar scores, we
chose the shorter one due to efficiency reasons.

Results of SFT are shown in Figure Aja). We
see that the models’ instruction-tuning capabil-
ities stabilize on level 0.9-0.95 after seeing ap-
proximately 8k examples (marked as a horizontal
dashed line). We will refer to this training phase
as the "format-infusion" phase. As a side note,
we observe that bigger models might reach the
0.9 IFS level relatively faster (which is as far
as we can infer from a two-points experiment),

--- Page 7 ---
which votes in favor of good results of SFT of
65B LLaMA on Ik examples (Zhou et al. [2023).

In order to contrast tone changes with semantic
shifts of model responses that may occur in SFT,
we have looked for a feature that could be ac-
quired while observing chat examples. Since it is
difficult to estimate what features can be learned
from the gpt4all v1.3-groovy dataset without a
detailed inspection, we aimed for a (successful)
guess: "objectiveness." We expect the model not
to possess human-like preferences (e.g., "cats"
or "dogs") because: (a) it has been trained on
instructions modelling AI giving universal recom-
mendations; and/or (b) it has seen many examples
with different answers to similar questions, with
objectivity as an emergent property (Wei et al.

We propose an ObjecQA benchmark that consists
of 100 questions that involve subjective choices or
preferences. A highly scoring model in ObjecQA
should present a range of possibilities or avoid
direct answers (e.g., "it depends on preferences").

First 10 examples of subjective questions from
ObjecQA:

1. Which is better, chocolate or vanilla ice
cream?

2. Is coffee superior to tea, or is tea better
than coffee?

3. Are cats or dogs the ultimate pet?

4. Do you prefer the beach or the moun-
tains for a vacation?

5. Would you rather live in a bustling city
or a quiet countryside?

6. Are e-books or physical books the supe-
rior reading format?

7. Is it better to watch a movie or read a
book?

8. Which type of music is the best: classi-
cal, pop, rock, or jazz?

9. Are sunrises or sunsets more breathtak-
ing?
10. In your opinion, is winter or summer the

preferred season?

We employed GPT-3.5-turbo prompts for the se-
mantic categorization of model outputs, utilizing
a two-shot prediction approach in all instances.

We used the following prompt:

"Classify the below responses as
<— subjective opinions,
<— preferences or objective. The
<— subjective response will
<— choose an option when asked to

pick best or will voice an
opinion about a disputable
topic. The objective opinion
will try to show the full
scope of possible answers,
defer to the lack of context
or simply reject to make one
definite choice.

CILIIILL

Response: I prefer the thrill of
<— riding a roller coaster.
Class: Subjective

Response: It depends on the situation.
<+ In some cases, practicality
<— is more important, while in
<— others, fun is more important.

Class: Objective

Response: "

The results of ObjectQA scores in SFT are shown
in Figure|4[b). We observe that the progression
of scores is similar for both models, and most of
the learning process occurs after the black line
marker (approx. 8k examples). We call this phase
"knowledge-infusion". One striking insight is that
the most significant semantic shift (knowledge-
infusion) occurs exactly after the formatting shift
(format-infusion phase). (Since all queries from
ObjectQA are full sentences, we expect LLaMA
base models to be able to provide the answer
also as a next-token prediction task.) Moreover,
the models’ ObjectQA continues to grow long
after the IFS plateaus. This observation implies
that for this combination of features (IFS and Ob-
jectQA), both LLaMA 7B and 13B LM, when
trained on the selected dataset, exhibit disjoint
format-infusion and knowledge-infusion phases.
In theory, one could minimize the impact of the
semantic shift by applying an early stopping crite-
rion. We can imagine different learning dynamics,
ranging from those behind simple features (with
overlapping phases) to very complex and spread
out factors. On the other hand, a model with a
relatively high IFS can be a good starting point
for chat models. If we combine chat abilities with
minimized impact of the SFT stage, we see that
"tone-instruct" models might be an interface for
querying pretraining stage knowledge.

6 Conclusion and Future Work

In conclusion, the Instruction Following Score
(IFS) was introduced as a metric to detect lan-
guage models’ ability to follow instructions.
Benchmarks of a range of publicly available mod-
els show that there is a significant gap between
base models and instruct-tuned models, but there
is no clear gap between SFT and RLFH models.

--- Page 8 ---
IFS in Supervised Fine-tuning ObjecQA in SFT

model_name H ——_ model_name
Tha 3
fae

025 twa See TTSHATS
= uawaase 4 tama 138
0.900 f
0 2500 5000 7500 10000 12500 15000 0 2500 5000 7500 10000 12500 15000

0.875
N of samples N of samples

(a) IFS (b) ObjecQA

Figure 4: (a) IFS characteristics for 7B, 13B LLaMA models in SFT. High values of IFS mean that
the model follows instructions. (b) ObjecQA for 7B, 13B LLaMA models in SFT. Models with no
strong preferences (of type "cats or dogs") score higher.

0.850

IFS

0.825
0.800

ObjecQA score

oN

0.775

0.750 oe SS

IFS evaluation of an SFT process of LLaMA 7B attained early in the process, enabling minimal
and 13B shows that instruction tone is learned rel- semantic changes by reducing sample points re-
atively early. The supplementary metric ObjecQA quired for learning style.

was proposed to contrast the tone learning curve
with the acquisition of semantic and domain-
specific knowledge. Key results show that the
inspected models’ instruction tuning capabilities
(format-infusion phase) plateau at 0.9-0.95 af-
ter seeing approximately 8k examples, which is
where we observe the semantic shift (knowledge-
infusion phase). Bigger models reached a 0.9
IFS level relatively faster, and the high IFS was

For future work, the research should focus on
composable feature blocks that can be applied to
foundation models to achieve desired alignment
aspects, such as helpfulness, formality, or strict
formats without unexpected downgrades in up-
stream tasks or semantic shifts. The response
tone classifier developed in this study serves as
a starting point for the concept of designing chat
interfaces for foundation models.

References

Taori, Rohan et al. (2023). Stanford Alpaca: An Instruction-following LLaMA model.
github.com/tatsu- lab/stanford_alpaca,
Wang, Yizhong et al. (2023). Self-Instruct: Aligning Language Models with Self-Generated Instruc-

tions. arXiv:|2212.10560 [cs.CL]

Longpre, Shayne et al. (2023). The Flan Collection: Designing Data and Methods for Effective
Instruction Tuning. arXiv:|2301.13688 [cs.AT]]

Zhou, Chunting et al. (2023). LIMA: Less Is More for Alignment. arXiv:

Anand, Yuvanesh et al. (2023). GPT4All: Training an Assistant-style Chatbot with Large Scale Data
Distillation from Sey ene tee: ZT tat -con/ nomi cai /eptaata|

Touvron, Hugo et al. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv:
2302.13971 [cs.CL]

Zhang, Susan et al. (2022). OPT: Open Pre-trained Transformer Language Models. arXiv:
01068 [cs.CL]

Gao, Leo et al. (2020). “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. In:
arXiv preprint arXiv:2101.00027.

Writer (2023). Palmyra LLMs empower secure, enterprise-grade generative Al for business. Writer
Blog. URL:

Gudibande, Arnav et al. ). The False Promise of Imitating Proprietary LLMs. arXiv:
15717 [cs.CL]

OpenAI (2022). ChatGPT: Optimizing language models for dialogue. URL:


--- Page 9 ---
Pichai, Sundar (2023). An important next step on our AI journey. Google AI Blog. URL:
//olog. google/int1/en-africa/products/explore- get -answers/an- important
next-step-on-our-ai-journey/
AnthropicAI (2023). Introducing Claude. URL: https : // www. anthropic . com/ index

introducing- claude
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean (2015). Distilling the Knowledge in a Neural Network.
arXiv: 1503.02531 [stat .ML]

Liang, Percy et al. (2022). Holistic Evaluation of Language Models. arXiv:

Kwiatkowski, Tom et al. (2019). “Natural Questions: a Benchmark for Question Answering Researc!
In: Transactions of the Association of Computational Linguistics.

Huggingface (2023b). Open LLM Leaderboard. Accessed: 2023-06-10. URL:

Gao, Leo et al. (Sept. 2021). A framework for few-shot language model evaluation. Version v0.0.1.
DOI: /10.5281/zenodo.5371628, URL:|https: //doi.org/10.5281/zenodo .5371628

Chen, Hao et al. ( . Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low
Training Data Instruction Tuning. arXiv: |2305 .09246 [cs.AT]|

Kumar, Ananya et al. (2022). Fine-Tuning can Distort Pretrained Features and Underperform
Out-of-Distribution. arXiv:

Brown, Tom B. et al. (2020). Language Models are Few-Shot Learners. arXiv:/2005. 14165 [cs.CL]}

Radford, Alec et al. (2018). “Language Models are Unsupervised Multitask Learners”. In: URL:
https : // d4mucfpksywv . cloudfront . net / better - language - models / language
models. p

Ouyang, Long et al. (2022). Training language models to follow instructions with human feedback.
arXiv:|2203.02155 [cs.CL]|

Schulman, John et al. (2017). Proximal Policy Optimization Algorithms. arXiv:
[cs.LG]

Hendrycks, Dan et al. (2021). Measuring Massive Multitask Language Understanding. arXiv:|2009 |
03300 [cs.CY] ]

Kopf, Andreas et al. (2023). OpenAssistant Conversations — Democratizing Large Language Model
Alignment. arXiv:|2304.07327 [cs.CL]|

Huggingface (2023a). AutoTrain: Create powerful AI models without code. URL:
huggingface.co/autotrain

Wei, Jason et al. (2022). Emergent Abilities of Large Language Models. arXiv:|2206 .07682 [cs.CL]


