--- Page 1 ---
arX1iv:2309.11523v5 [cs.CV] 2 Dec 2023

RMT: Retentive Networks Meet Vision Transformers

Qihang Fan !?, Huaibo Huang', Mingrui Chen!?, Hongmin Liu®, Ran He

1,26

IMAIS & CRIPAC, Institute of Automation, Chinese Academy of Sciences, Beijing, China
School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
3University of Science and Technology Beijing, Beijing, China

fangihang.159@gmail.com,
charmier@hust.edu.cn,

Abstract

Vision Transformer (ViT) has gained increasing attention
in the computer vision community in recent years. How-
ever, the core component of ViT, Self-Attention, lacks ex-
plicit spatial priors and bears a quadratic computational
complexity, thereby constraining the applicability of ViT.
To alleviate these issues, we draw inspiration from the re-
cent Retentive Network (RetNet) in the field of NLP, and
propose RMT, a strong vision backbone with explicit spa-
tial prior for general purposes. Specifically, we extend
the RetNet’s temporal decay mechanism to the spatial do-
main, and propose a spatial decay matrix based on the
Manhattan distance to introduce the explicit spatial prior
to Self-Attention. Additionally, an attention decomposi-
tion form that adeptly adapts to explicit spatial prior is
proposed, aiming to reduce the computational burden of
modeling global information without disrupting the spa-
tial decay matrix. Based on the spatial decay matrix and
the attention decomposition form, we can flexibly integrate
explicit spatial prior into the vision backbone with lin-
ear complexity. Extensive experiments demonstrate that
RMT exhibits exceptional performance across various vi-
sion tasks. Specifically, without extra training data, RMT
achieves 84.8% and 86.1% top-1 acc on ImageNet-1k with
27M/4.5GFLOPs and 96M/18.2GFLOPs. For downstream
tasks, RMT achieves 54.5 box AP and 47.2 mask AP on the
COCO detection task, and 52.8 mloU on the ADE20K se-
mantic segmentation task. Code is available at https:
//github.com/qhfan/RMT

1. Introduction

Vision Transformer (ViT) [12] is an excellent visual archi-
tecture highly favored by researchers. However, as the core
module of ViT, Self-Attention’s inherent structure lacking

“Ran He is the corresponding author.

huaibo.huang@cripac.ia.ac.cn,
hmliu_82@163.com,

rhe@nlpr.ia.ac.cn

-- RMT(Ours) uo
<¢- SMT eee
85} -+- BiFormer ss *
-@- MaxViT oo a
-
s 84 ‘Model Params Topl- Ace.
oa MaxViT-T [31] 31M 83.6
g SMTS [34] 20M 83.7
< BiFormer-S [75] 26M 83.8
ms RMT-S (Ours) 27™M 84.1
o 83 RMT-S* (Ours) 27™M. 34.8
ze a
BiForm 157 57™M 843
MaxViT-S [19] 69M 84.5
RMT-B (Ours) 54M 85.0
RMT-B* (Ours) 55M 85.6
SMTL [34] 81M 84.6
82 MaxVit-B ttt t20M 849
RMT-L (Ours) 95M 85.5
RMT-L* (Ours) 96M 86.1
5 10 15 20
FLOPs(G)
Figure 1. FLOPs v.s. Top-1 accuracy on ImageNet-1K with

224 x 224 resolution. “*” indicates the model trained with to-
ken labeling [27].

explicit spatial priors. Besides, the quadratic complexity
of Self-Attention leads to significant computational costs
when modeling global information. These issues limit the
application of ViT.

Many works have previously attempted to alleviate these
issues [13, 16, 30, 35, 50, 57, 61]. For example, in Swin
Transformer [35], the authors partition the tokens used for
self-attention by applying windowing operations. This op-
eration not only reduces the computational cost of self-
attention but also introduces spatial priors to the model
through the use of windows and relative position encoding.
In addition to it, NAT [19] changes the receptive field of
Self-Attention to match the shape of convolution, reducing
computational costs while also enabling the model to per-
ceive spatial priors through the shape of its receptive field.

Different from previous methods, we draw inspira-
tion from the recently successful Retentive Network (Ret-
Net) [46] in the field of NLP. RetNet utilizes a distance-
dependent temporal decay matrix to provide explicit tem-
poral prior for one-dimensional and unidirectional text data.

--- Page 2 ---
(a) Vanilla Self-Attention

(b) Window Self-Attention

(c) Neighborhood Self-Attention

CT] : Receptive Field

(d) Manhattan Self-Attention

Figure 2. Comparison among different Self-Attention mechanisms. In MaSA, darker colors represent smaller spatial decay rates, while
lighter colors represent larger ones. The spatial decay rates that change with distance provide the model with rich spatial priors.

ALiBi [41], prior to RetNet, also applied a similar approach
and succeeded in NLP tasks. We extend this temporal decay
matrix to the spatial domain, developing a two-dimensional
bidirectional spatial decay matrix based on the Manhattan
distance among tokens. In our space decay matrix, for a tar-
get token, the farther the surrounding tokens are, the greater
the degree of decay in their attention scores. This prop-
erty allows the target token to perceive global information
while simultaneously assigning different levels of attention
to tokens at varying distances. We introduce explicit spatial
prior to the vision backbone using this spatial decay matrix.
We name this Self-Attention mechanism, which is inspired
by RetNet and incorporates the Manhattan distance as the
explicit spatial prior, as Manhattan Self-Attention (MaSA).
Besides explicit spatial priors, another issue caused
by global modeling with Self-Attention is the enormous
computational burden. Previous sparse attention mecha-
nisms [11, 35, 53, 63, 75] and the way retention is decom-
posed in RetNet [46] mostly disrupt the spatial decay ma-
trix, making them unsuitable for MaSA. In order to sparsely
model global information without compromising the spa-
tial decay matrix, we propose a method to decompose Self-
Attention along both axes of the image. This decomposition
method decomposes Self-Attention and the spatial decay
matrix without any loss of prior information. The decom-
posed MaSA models global information with linear com-
plexity and has the same receptive field shape as the original
MaSA. We compare MaSA with other Self-Attention mech-
anisms in Fig. 2. It can be seen that our MaSA introduces
richer spatial priors to the model than its counterparts.
Based on MaSA, we construct a powerful vision back-
bone called RMT. We demonstrate the effectiveness of the
proposed method through extensive experiments. As shown
in Fig. 1, our RMT outperforms the state-of-the-art (SOTA)
models on image classification tasks. Additionally, our
model exhibits more prominent advantages compared to

other models in tasks such as object detection, instance seg-
mentation, and semantic segmentation. Our contributions
can be summarized as follows:

* We propose a spatial decay matrix based on Manhattan
distance to augment Self-Attention, creating the Manhat-
tan Self-Attention (MaSA) with an explicit spatial prior.

* We propose a decomposition form for MaSA, enabling
linear complexity for global information modeling with-
out disrupting the spatial decay matrix.

« Leveraging MaSA, we construct RMT, a powerful vision
backbone for general purposes. RMT attains high top-1
accuracy on ImageNet-1k in image classification without
extra training data, and excels in tasks like object detec-
tion, instance segmentation, and semantic segmentation.

2. Related Work

Transformer. Transformer architecture was firstly pro-
posed in [52] to address the training limitation of recur-
rent model and then achieve massive success in many NLP
tasks. By splitting the image into small, non-overlapped
patches sequence, Vision Transformer (ViTs) [12] also have
attracted great attention and become widely used on vision
tasks [5, 14, 18, 39, 58, 66]. Unlike in the past, where RNNs
and CNNs have respectively dominated the NLP and CV
fields, the transformer architecture has shined through in
various modalities and fields [26, 37, 42, 60]. In the com-
puter vision community, many studies are attempting to in-
troduce spatial priors into ViT to reduce the data require-
ments for training [6, 19, 49]. At the same time, various
sparse attention mechanisms have been proposed to reduce
the computational cost of Self-Attention [13, 53, 54, 57].

Prior Knowledge in Transformer. Numerous attempts
have been made to incorporate prior knowledge into the
Transformer model to enhance its performance. The orig-
inal Transformers [12, 52] use trigonometric position en-

--- Page 3 ---
coding to provide positional information for each token. In
vision tasks, [35] proposes the use of relative positional en-
coding as a replacement for the original absolute positional
encoding. [6] points out that zero padding in convolutional
layers could also provide positional awareness for the ViT,
and this position encoding method is highly efficient. In
many studies, Convolution in FFN [13, 16, 54] has been
employed for vision models to further enrich the positional
information in the ViT. For NLP tasks, in the recent Reten-
tive Network [46], the temporal decay matrix has been in-
troduced to provide the model with prior knowledge based
on distance changes. Before RetNet, ALiBi [41] also uses a
similar temporal decay matrix.

3. Methodology
3.1. Preliminary

Temporal decay in RetNet. Retentive Network (RetNet)
is a powerful architecture for language models. This work
proposes the retention mechanism for sequence modeling.
Retention brings the temporal decay to the language model,
which Transformers do not have. Retention firstly considers
a sequence modeling problem in a recurrent manner. It can
be written as Eq. 1:

n
On = > om (Qne?) (Kme tom (1)

m=1

For a parallel training process, Eq. | is expressed as:

Q=(XWe) 008, K=(XWK) O08, V=XWy

yom n>m

ind
On =e", Dam =
0, n<m

Retention(X) = (QKT © D)V
(2)
where © is the complex conjugate of 9, and D € RI#!*!#!
contains both causal masking and exponential decay, which
symbolizes the relative distance in one-dimensional se-
quence and brings the explicit temporal prior to text data.

3.2. Manhattan Self-Attention

Starting from the retention in RetNet, we evolve it into Man-
hattan Self-Attention (MaSA). Within MaSA, we transform
the unidirectional and one-dimensional temporal decay ob-
served in retention into bidirectional and two-dimensional
spatial decay. This spatial decay introduces an explicit spa-
tial prior linked to Manhattan distance into the vision back-
bone. Additionally, we devise a straightforward approach to
concurrently decompose the Self-Attention and spatial de-
cay matrix along the two axes of the image.

From Unidirectional to Bidirectional Decay: In RetNet,
retention is unidirectional due to the causal nature of text

data, allowing each token to attend only to preceding tokens
and not those following it. This characteristic is ill-suited
for tasks lacking causal properties, such as image recogni-
tion. Hence, we initially broaden the retention to a bidirec-
tional form, expressed as Eq. 3:

BiRetention(X) = (QKT © D®)V
pe = inom

nm

(3)
where BiRetention signifies bidirectional modeling.

From One-dimensional to Two-dimensional Decay:
While retention now supports bi-directional modeling, this
capability remains confined to a one-dimensional level and
is inadequate for two-dimensional images. To address this
limitation, we extend the one-dimensional retention to en-
compass two dimensions.

In the context of images, each token is uniquely posi-
tioned with a two-dimensional coordinate within the plane,
denoted as (2, yn) for the n-th token. To adapt to this, we
adjust each element in the matrix D to represent the Man-
hattan distance between the respective token pairs based on
their 2D coordinates. The matrix D is redefined as follows:

Dt = rylten—#m FLY Yo (4)

In the retention, the Softmax is abandoned and replaced
with a gating function. This variation gives RetNet multi-
ple flexible computation forms, enabling it to adapt to par-
allel training and recurrent inference processes. Despite
this flexibility, when exclusively utilizing RetNet’s paral-
lel computation form in our experiments, the necessity of
retaining the gating function becomes debatable. Our find-
ings indicate that this modification does not improve results
for vision models; instead, it introduces extra parameters
and computational complexity. Consequently, we continue
to employ Softmax to introduce nonlinearity to our model.
Combining the aforementioned steps, our Manhattan Self-
Attention is expressed as

MaSA(X) = (Softmax(QKT™) © D*4)V
D24

nm

(5)

— yltn—m|+1Yn—Ym
=7! [+1¥n—Ym|

Decomposed Manhattan Self-Attention. In the early
stages of the vision backbone, an abundance of tokens leads
to high computational costs for Self-Attention when at-
tempting to model global information. Our MaSA encoun-
ters this challenge as well. Utilizing existing sparse atten-
tion mechanisms [11, 19, 35, 53, 63], or the original Ret-
Net’s recurrent/chunk-wise recurrent form directly, disrupts
the spatial decay matrix based on Manhattan distance, re-
sulting in the loss of explicit spatial prior. To address this,
we introduce a simple decomposition method that not only

--- Page 4 ---
WId}§ AUOD

Manhattan Self-Attention

Stage4
RMT Block
x La

Figure 3. Overall architecture of RMT.

® : matrix multiplication

Figure 4. Spatial decay matrix in the decomposed MaSA.

decomposes Self-Attention but also decomposes the spa-
tial decay matrix. The decomposed MaSA is represented in
Eq. 6. Specifically, we calculate attention scores separately
for the horizontal and vertical directions in the image. Sub-
sequently, we apply the one-dimensional bidirectional de-
cay matrix to these attention weights. The one-dimensional
decay matrix signifies the horizontal and vertical distances

between tokens (DM, = 7!¥n—¥ml, DW = ylen—aml):
Attny = Softmax(QyK],) © D",
Attnw = Softmax(Qw Kj) © D”, (6)

MaSA(X) = Atiny(AtinwV)T

Based on the decomposition of MaSA, the shape of the
receptive field of each token is shown in Fig. 4, which is
identical to the shape of the complete MaSA’s receptive
field. Fig. 4 indicates that our decomposition method fully
preserves the explicit spatial prior.

To further enhance the local expression capability of
MaSA, following [75], we introduce a Local Context En-
hancement module using DWConv:

Xout = MaSA(X) + LCE(V); )

3.3. Overall Architecture

We construct the RMT based on MaSA, and its architec-
ture is illustrated in Fig. 3. Similar to previous general vi-
sion backbones [35, 53, 54, 71], RMT is divided into four
stages. The first three stages utilize the decomposed MaSA,
while the last uses the original MaSA. Like many previous
backbones [16, 30, 72, 75], we incorporate CPE [6] into our
model.

4. Experiments

We conducted extensive experiments on multiple vision
tasks, such as image classification on ImageNet-1K [9],
object detection and instance segmentation on COCO
2017 [33], and semantic segmentation on ADE20K [74].
We also make ablation studies to validate the importance
of each component in RMT. More details can be found in
Appendix.

4.1. Image Classification

Settings. We train our models on ImageNet-1K [9] from
scratch. We follow the same training strategy in [49],
with the only supervision being classification loss for a fair
comparison. The maximum rates of increasing stochastic
depth [24] are set to 0.1/0.15/0.4/0.5 for RMT-T/S/B/L [24],
respectively. We use the AdamW optimizer with a cosine
decay learning rate scheduler to train the models. We set the
initial learning rate, weight decay, and batch size to 0.001,
0.05, and 1024, respectively. We adopt the strong data
augmentation and regularization used in [35]. Our settings
are RandAugment [8] (randm9-mstd0.5-incl), Mixup [70]
(prob=0.8), CutMix [69] (prob=1.0), Random Erasing [73]

--- Page 5 ---
Parmas FLOPs | Topl-acc Parmas FLOPs | Topl-acc
Cost Model (M) G) (%) Cost Model (M) G) (%)
PVTv2-b1 [54] 13 2.1 78.7 Swin-S [35] 50 8.7 83.0
QuadTree-B-b1 [48] 14 2.3 80.0 ConvNeXt-S [36] 50 8.7 83.1
RegionViT-T [3] 14 24 80.4 CrossFormer-B [55] 52 9.2 83.4
MPViT-XS [29] 11 2.9 80.9 NAT-S [19] 51 78 83.7
tiny-MOAT-2 [62] 10 2.3 81.0 Quadtree-B-b4 [48] 64 11.5 84.0
3 oO VAN-BI [17 14 2.5 81.1 Ortho-B [25] 50 8.6 84.0
2g 2 BiFormer-T [75] 13 2.2 81.4 ScaleViT-B [65] 81 8.6 84.1
> 2 Conv2Former-N [23] 15 2.2 81.5 _ MOAT-1 [62] 42 9.1 84.2
3 CrossFormer-T [55] 28 2.9 81.5 3 oO InternImage-S [56] 50 8.0 84.2
NAT-M [19 20 2.7 81.8 2g 8 DaViT-S [10] 50 8.8 84.2
QnA-T [1] 16 2.5 82.0 2 2 GC-ViT-S [20] 51 8.5 84.3
GC-ViT-XT [20] 20 2.6 82.0 sé BiFormer-B [75] 57 9.8 84.3
SMT-T [34 12 24 82.2 MViTv2-B [31] 52 10.2 84.4
RMT-T 14 25 82.4 iFormer-B [45] 48 9.4 84.6
Deil-S [49 2 46 79.9 BMEB 2 ea B50
Swin-T [35 29 45 813 WaveViT-B* [66] 34 7.2 84.8
ConvNeXt-T [36] 29 45 82.1 UniFormer-B* 30] 50 8.3 85.1
Focal-T [63 29 49 92.2 Dual-ViT-B* [67] 43 9.3 85.2
FocalNet-T [64] 29 45 823 BiFormer-B* [75] 58 9.8 85.4
Region ViT-S [3] 31 53 82.6 JERI 55 97 85.6
CSWin-T [11 23 43 82.7 Swin-B [35 88 15.4 83.3
MPViT-S [29 23 47 83.0 CaiT-M24 [50] 186 36 83.4
ScalableViT-S [65] 32 4.2 83.1 LITv2 [39] 87 13.2 83.6
SG-Former-S [15] 23 48 83.2 CrossFormer-L [55] 92 16.1 84.0
3 MOAT-0 [62 28 5.7 83.3 Ortho-L [25 88 15.4 84.2
3 g Ortho-S [25 24 45 83.4 CSwin-B [11] 78 15.0 84.2
ia ~ InternImage-T [56] 30 5.0 83.5 SMT-L [34 81 17.7 84.6
2° CMT-S [16 25 4.0 83.5 3 MOAT-2 [62 73 17.2 84.7
° MaxViT-T [51] 31 5.6 83.6 gg SG-Former-B [15] 78 15.6 84.7
SMT-S [34 20 48 83.7 E % iFormer-L [45] 87 14.0 84.8
BiFormer-S [75] 26 45 83.8 22 InterImage-B [56] 97 16.0 84.9
RMT-S 27 45 84.1 ” MaxViT-B [51] 120 23.4 84.9
LV-ViT-S* [27] 26 6.6 83.3 GC-ViT-B [20] 90 14.8 85.0
UniFormer-S* [30] 24 4.2 83.4 RMT-L 95 18.2 85.5
Wave ViT-S* [66] 23 47 83.9 VOLO-D3* [68] 86 20.6 85.4
Dual-ViT-S* [67] 25 5.4 84.1 WaveViT-L* [66] 58 14.8 85.5
VOLO-D1* [68] 27 6.8 84.2 UniFormer-L* [30] 100 12.6 85.6
BiFormer-S* [75] 26 45 84.3 Dual-ViT-L* [67] 73 18.0 85.7
RMT-S* 27 45 84.8 RMT-L* 96 18.2 86.1

Table 1. Comparison with the state-of-the-art on ImageNet-1K classification. “*” indicates the model trained with token labeling [27].

(prob=0.25). In addition to the conventional training meth-
ods, similar to LV-ViT [27] and VOLO [68], we train a
model that utilizes token labeling to provide supplementary
supervision.

Results. We compare RMT against many state-of-the-art
models in Tab. |. Results in the table demonstrate that RMT
consistently outperforms previous models across all set-
tings. Specifically, RMT-S achieves 84.1% Top1-accuracy
with only 4.5 GFLOPs. RMT-B also surpasses iFormer [45]
by 0.4% with similar FLOPs. Furthermore, our RMT-L
model surpasses MaxViT-B [51] in topl-accuracy by 0.6%
while using fewer FLOPs. Our RMT-T has also outper-
formed many lightweight models. As for the model trained

using token labeling, our RMT-S outperforms the current
state-of-the-art BiFormer-S by 0.5%.

4.2. Object Detection and Instance Segmentation

Settings. We adopt MMDetection [4] to implement Reti-
naNet [32], Mask-RCNN [22] and Cascade Mask R-
CNN [2]. We use the commonly used “1x” (12 training
epochs) setting for the RetinaNet and Mask R-CNN. Be-
sides, we use “3 x +MS” for Mask R-CNN and Cascade
Mask R-CNN. Following [35], during training, images are
resized to the shorter side of 800 pixels while the longer
side is within 1333 pixels. We adopt the AdamW optimizer
with a learning rate of 0.0001 and batch size of 16 to op-
timize the model. For the “1x” schedule, the learning rate

--- Page 6 ---
Backbone Params FLOPs Mask R-CNN 1x Params FLOPs RetinaNet 1x
(M) (G) |AP’ AP% AP, AP™ APs API; (M)  (G) | AP’ AP% AP?; AP§ APh, APZ
PVT-T [53 33 240 |39.8 62.2 43.0 374 59.3 39.9 23 221 | 394 59.8 42.0 25.5 42.0 52.1
PVTv2-B1 [54] 33 243 | 41.8 543 45.9 38.8 61.2 41.6 23 225 | 41.2 61.9 43.9 25.4 44.5 54.3
MPViT-XS [29] 30 231 |44.2 66.7 484 404 634 43.4 20 211 | 43.8 65.0 47.1 28.1 47.6 56.5
RMT-T 33 218 | 47.1 68.8 51.7 426 65.8 45.9 23 199 | 45.1 66.2 48.1 28.8 48.9 61.1
Swin-T [35 48 267 |43.7 66.6 47.7 39.8 63.3 42.7 38 248 | 41.7 63.1 44.3 27.0 45.3 54.7
CMT-S [16 45 249 |446 668 48.9 40.7 63.9 43.4 44 231 | 44.3 65.5 47.5 27.1 48.3 59.1
CrossFormer-S [55] 50 301 |454 68.0 49.7 414 648 44.6 41 272 |444 65.8 474 28.2 484 594
ScalableViT-S [65] 46 256 |45.8 67.6 50.0 41.7 64.7 448 36 238 | 45.2 66.5 484 29.2 49.1 60.3
MPViT-S [29] 43 268 |464 68.6 51.2 424 65.6 45.7 32 248 | 45.7 57.3 48.8 28.7 49.7 59.2
CSWin-T [11] 42 279 |46.7 68.6 51.3 42.2 65.6 45.4 - - - - - - - -
InternImage-T [56] 49 270 |47.2 69.0 52.1 425 66.1 45.8 - - - - - - - -
SMT-S [34 40 265 |47.8 69.5 52.1 43.0 66.6 46.1 - - - - - - - -
BiFormer-S [75] - - 47.8 69.8 52.3 43.2 668 46.5 - - 45.9 66.9 494 30.2 49.6 61.7
RMT-S 46 262 | 49.0 70.8 53.9 43.9 67.8 47.4 36 244 | 47.8 69.1 51.8 32.1 518 63.5
ResNet-101 [21] 63 336 |404 61.1 442 364 57.7 388 58 315 | 38.5 57.8 41.2 214 426 51.1
Swin-S [35 69 359 | 45.7 67.9 504 41.1 64.9 44.2 60 339 | 44.5 66.1 474 29.8 48.5 59.1
ScalableViT-B [65] 95 349 |46.8 68.7 S515 425 65.8 45.9 85 330 | 45.8 67.3 49.2 29.9 49.5 61.0
InternImage-S [56] 69 340 | 47.8 69.8 52.8 43.3 67.1 46.7 - - - - - - - -
CSWin-S [11] 54 342 |47.9 70.1 52.6 43.2 67.1 46.2 - - - - - - - -
BiFormer-B [75] - - 48.6 70.5 53.8 43.7 67.6 47.1 - - 47.1 685 504 31.3 508 62.6
RMT-B 73 373, | 51.1 72.5 56.1 45.5 69.7 49.3 63 355 | 49.1 70.3 53.0 32.9 53.2 64.2
Swin-B [35 107 496 | 46.9 69.2 51.6 42.3 66.0 45.5 98 477 |45.0 66.4 483 284 49.1 60.6
PVTv2-B5 [54] 102 557 | 474 68.6 51.9 425 65.7 46.0 - - - - - - - -
Focal-B [63 110 533. | 47.8 70.2 52.5 43.2 67.3 465 101 514 | 46.3 68.0 49.8 31.7 504 60.8
MPViT-B [29] 95 503 | 48.2 70.0 52.9 43.5 67.1 468 85 482 | 47.0 684 50.8 294 51.3 61.5
CSwin-B [11] 97 526 | 48.7 70.4 53.9 43.9 67.8 47.3 - - - - - - - -
InternImage-B [56] |} 115 S01 | 48.8 70.9 54.0 44.0 67.8 47.4 - - - - - - - -
RMT-L 114 557 | 51.6 73.1 565 45.9 70.3 49.8 104 537 | 49.4 70.6 53.1 34.2 53.9 65.2
Table 2. Comparison to other backbones using RetinaNet and Mask R-CNN on COCO val2017 object detection and instance segmentation.
Backbone [Params FLOPs| Mask R-CNN 3x+MS Backbone [Params FLOPs} Cascade Mask R-CNN 3x+MS
(M) (G) |AP? APY, AP? AP™ AP AP (M) (G) |AP? APY, AP®, AP™ AP AP
ConvNeXt-T [36] } 48 262 |46.2 67.9 50.8 41.7 65.0 45.0 Swin-T [35 86 745 |50.5 69.3 54.9 43.7 66.6 47.1
Focal-T [63 49 291 |47.2 69.4 51.9 42.7 66.5 45.9 NAT-T [19] 85 737 |51.4 70.0 55.9 44.5 67.6 47.9
NAT-T [19 48 258 |47.8 69.0 52.6 42.6 66.0 45.9 GC-ViT-T [20 85 770 |51.6 70.4 56.1 44.6 67.8 48.3
GC-ViT-T [20] 48 291 |47.9 70.1 52.8 43.2 67.0 46.7 SMT-S [34] 78 744 |51.9 70.5 56.3 44.7 67.8 48.6
MPViT-S [29] 43 268 |48.4 70.5 52.6 43.9 67.6 47.5 UniFormer-S [30]} 79 TAT |52.1 71.1 56.6 45.2 68.3 48.9
Ortho-S [25 44 277 |48.7 70.5 53.3 43.6 67.3 47.3 Ortho-S [25 81 755 |52.3 71.3 56.8 45.3 68.6 49.2
SMT-S [34 40 265 |49.0 70.1 53.4 43.4 67.3 46.7 HorNet-T [43 80 728 |52.4 71.6 56.8 45.6 69.1 49.6
CSWin-T [11] 42 279 |49.0 70.7 53.7 43.6 67.9 46.6 CSWin-T [11 80 757 |52.5 71.5 57.1 45.3 68.8 48.9
InternImage-T [56]} 49 270 |49.1 70.4 54.1 43.7 67.3 47.3 RMT-S 83 741 |53.2 72.0 57.8 46.1 69.8 49.8
RMS G1) _ 220) [uy LD) GE) GIO ail Gas Swin-S [35] | 107 838 [51.9 70.7 56.3 45.0 68.2 48.8
ConvNeXt-S [36] |] 70 348 |47.9 70.0 52.7 42.9 66.9 46.2 NAT-S [19 108 809 |51.9 70.4 56.2 44.9 68.2 48.6
NAT-S [19 70 330 |48.4 69.8 53.2 43.2 66.9 46.4 GC-ViT-S [20 108 866 |52.4 71.0 57.1 45.4 68.5 49.3
Swin-S [35 69 359 |48.5 70.2 53.5 43.3 67.3 46.6 DAT-S [58 107-857 {52.7 71.7 57.2 45.5 69.1 49.3
InternImage-S [56]} 69 340 |49.7 71.1 54.5 44.5 68.5 47.8 HorNet-S [43 108 827 |53.3 72.3 57.8 46.3 69.9 50.4
SMT-B [34 52 328 |49.8 71.0 54.4 44.0 68.0 47.3 CSWin-S [11 92 820 |53.7 72.2 58.4 46.4 69.6 50.6
CSWin-S [11] 54 342 |50.0 71.3 54.7 44.5 68.4 47.7 UniFormer-B [30]} 107 878 53.8 72.8 58.5 46.4 69.9 50.4
RMT-B 73 373 |52.2 72.9 57.0 46.1 70.4 49.9 RMT-B 111 852 |54.5 72.8 59.0 47.2 70.5 51.4

Table 3. Comparison to other backbones using Mask R-CNN with

”3 x +MS” schedule.

declines with the decay rate of 0.1 at the epoch 8 and 11.
While for the “3 x +MS” schedule, the learning rate de-

clines with the decay rate of 0.1 at the epoch 27 and 33.

Table 4. Comparison to other backbones using Cascade Mask R-
CNN with "3 x +MS” schedule.

Results. Tab. 2, Tab. 3 and Tab. 4 show the results with
different detection frameworks. The results demonstrate
that our RMT performs best in all comparisons. For the

--- Page 7 ---
Backbone Method | Params(M) | FLOPs(G) | mloU(%)
ResNet 18 [21] FPN 15.5 32.2 32.9
PVTv2-B1 [54] FPN 17.8 34.2 42.5
VAN-B1 [17 FPN 18.1 34.9 42.9
EdgeViT-S [38] FPN 16.9 32.1 45.9

RMT-T FPN 17.0 33.7 46.4
DAT-T [58 FPN 32 198 42.6
RegionViT-S+ [3] FPN 35 236 45.3
CrossFormer-S [55] FPN. 34 221 46.0
UniFormer-S [30] FPN 25 247 46.6
Shuted-S [44 FPN 26 183 48.2
RMT-S FPN 30 180 49.4
DAT-S [58 FPN 53 320 46.1
RegionViT-B+ [3] FPN 77 459 47.5
UniFormer-B [30] FPN 54 350 47.7
CrossFormer-B [55] FPN. 56 331 47.7
CSWin-S [11 FPN 39 271 49.2
RMT-B FPN 57 294 50.4
DAT-B [58 FPN 92 481 47.0
CrossFormer-L [55] FPN. 95 497 48.7
CSWin-B [11 FPN 81 464 49.9
RMT-L FPN 98 482 51.4
DAT-T [58 UperNet 60 957 45.5
NAT-T [19 UperNet 58 934 47.1
InternImage-T [56] | UperNet 59 944 47.9
MPViT-S [29 UperNet 52 943 48.3
SMT-S [34 UperNet 50 935 49.2
RMT-S UperNet 56 937 49.8
DAT-S [58 UperNet 81 1079 48.3
SMT-B [34 UperNet 62 1004 49.6
HorNet-S [43 UperNet 85 1027 50.0
InterImage-S [56] UperNet 80 1017 50.2
MPViT-B [29 UperNet 105 1186 50.3
CSWin-S [11 UperNet 65 1027 50.4
RMT-B UperNet 83, 1051 52.0
Swin-B [35] UperNet 121 1188 48.1
GC ViT-B [20 UperNet 125 1348 49.2
DAT-B [58] UperNet 121 1212 49.4
InternImage-B [56] | UperNet 128 1185 50.8
CSWin-B [11 UperNet 109 1222 S11
RMT-L UperNet 125 1241 52.8
Table 5. Comparison with the state-of-the-art on ADE20K.

RetinaNet framework, our RMT-T outperforms MPViT-XS
by +1.3 AP, while S/B/L also perform better than other
methods. As for the Mask R-CNN with “1x” schedule,
RMT-L outperforms the recent InternImage-B by +2.8 box
AP and +1.9 mask AP. For “3 x +MS” schedule, RMT-
S outperforms InternImage-T for +1.6 box AP and +1.2
mask AP. Besides, regarding the Cascade Mask R-CNN, our
RMT still performs much better than other backbones. All
the above results tell that RMT outperforms its counterparts
by evident margins.

4.3. Semantic Segmentation

Settings. We adopt the Semantic FPN [28] and Uper-
Net [59] based on MMSegmentation [7], apply RMTs
which are pretrained on ImageNet-1K as backbone. We use
the same setting of PVT [53] to train the Semantic FPN,
and we train the model for 80k iterations. All models are

trained with the input resolution of 512 x 512. When test-
ing the model, we resize the shorter side of the image to
512 pixels. As for UperNet, we follow the default settings
in Swin [35]. We take AdamW with a weight decay of 0.01
as the optimizer to train the models for 160K iterations. The
learning rate is set to 6 x 10° with 1500 iterations warmup.

Results. The results of semantic segmentation can be
found in Tab. 5. All the FLOPs are measured with the res-
olution of 512 x 2048, except the group of RMT-T, which
are measured with the resolution of 512 x 512. All our
models achieve the best performance in all comparisons.
Specifically, our RMT-S exceeds Shunted-S for +1.2 mloU
with Semantic FPN. Moreover, our RMT-B outperforms the
recent InternImage-S for +1.8 mloU. All the above results
demonstrate our model’s superiority in dense prediction.

4.4, Ablation Study

Strict comparison with previous works. In order to
make a strict comparison with previous methods, we align
RMT’s hyperparameters (such as whether to use hierarchi-
cal structure, the number of channels in the four stages of
the hierarchical model, whether to use positional encod-
ing and convolution stem, etc.) of the overall architec-
ture with DeiT [49] and Swin [35], and only replace the
Self-Attention/Window Self-Attention with our MaSA. The
comparison results are shown in Tab. 6, where RMT signif-
icantly outperforms DeiT-S, Swin-T, and Swin-S.

MaSA._ We verify the impact of Manhattan Self-Attention
on the model, as shown in the Tab. 6. MaSA improves
the model’s performance in image classification and down-
stream tasks by a large margin. Specifically, the classifica-
tion accuracy of MaSA is 0.8% higher than that of vanilla
attention.

Softmax. In RetNet, Softmax is replaced with a non-
linear gating function to accommodate its various compu-
tational forms [46]. We replace the Softmax in MaSA with
this gating function. However, the model utilizing the gat-
ing function cannot undergo stable training. It is worth not-
ing that this does not mean the gating function is inferior to
Softmax. The gating function may just not be compatible
with our decomposed form or spatial decay.

LCE. Local Context Enhancement also plays a role in the
excellent performance of our model. LCE improves the
classification accuracy of RMT by 0.3% and enhances the
model’s performance in downstream tasks.

CPE. Just like previous methods, CPE provides our
model with flexible position encoding and more positional
information, contributing to the improvement in the model’s
performance in image classification and downstream tasks.

--- Page 8 ---
Model Params(M) FLOPs(G) Top1-acc(%) AP? Ap™ mloU(%)
DeiT-S [49] 22 46 79.8 - - -
RMT-DeiT-S 22 4.6 81.7(+1.9) - - =
Swin-T [35] 29 45 81.3 43.7 39.8 44.5
RMT-Swin-T 29 47 83.6(+2.3) 47.8(+4.1) 43.1(+3.3) 49.1(+4.6)
Swin-S [35] 50 8.8 83.0 45.7 41.1 47.6
RMT-Swin-S 50 9.1 84.5(+1.5) 49.5(+3.8) 44.2(+3.1) 51.0 (43.4)

RMT-T 14.3 2.5 82.4 47.1 42.6 46.4

MaSA->Attention 14.3 2.5 81.6(-0.8) 44.6(-2.5) 40.7(-1.9) 43.9(-2.5)
Softmax—Gate 15.6 2.7 Nan - - -
w/o LCE 14.2 2.4 82.1 46.7 42.3 46.0
w/o CPE 14.3 2.5 82.2 47.0 42.4 46.4
w/o Stem 14.3 2.2 82.2 46.8 42.3 46.2

Table 6. Ablation study. We make a strict comparison among RMT, DeiT, and Swin-Transformer.

3rd stage | FLOPs(G) Topl(%) | FLOPs(G) mloU(%)
MaSA-d 4.5 84.1 180 49.4
MaSA 4.8 84.1 246 49.7

Table 7. Comparison between decomposed MaSA (MaSA-d) and
original MaSA.

Params FLOPs|  Throughputt | Topl
Method | ™ ©) timgs/s) | (%)
Parallel 27 10.9 262 -
Chunklen_4 27 45 192 -
Chunklen_49 27 AT 446 82.1
Recurrent 27 45 61 -
MaSA | 27 45 876 84.1
Table 8. Comparison between MaSA and retention in RMT-S’s
architecture.
. Params FLOPs| Throughput} Top!
Model Mp) G) (imgs/s) (%)
BiFormer-T [75] 13 2.2 1602 814
CMT-XS [16] 15 15 1476 81.8
SMT-T [34 12 2.4 636 82.2
RMT-T 14 25 1650 82.4
CMT-S [16, 25 4.0 848 83.5
MaxViT-T [51] 31 5.6 826 83.6
SMT-S [34 20 48 356 83.7
BiFormer-S [75] 26 45 766 83.8
RMT-Swin-T 29 47 1192 83.6
RMT-S 27 45 876 84.1
SMT-B [34 32 V7 237 84.3
BiFormer-B [75] 57 9.8 498 84.3
CMT-B [16 46 9.3 447 84.5
MaxViT-S [51] 69 117 546 84.5
RMT-Swin-S 50 9.1 722 84.5
RMT-B 54 9.7 457 85.0
SMT-L [34 80 17.7 158 84.6
MaxViT-B [51] 120 23.4 306 84.9
RMT-L 95 18.2 326 85.5

Table 9. Comparison of inference speed among SOTA models.

Convolutional Stem. The initial convolutional stem of
the model provides better local information, thereby further
enhancing the model’s performance on various tasks.

Decomposed MaSA. In RMT-S, we substitute the de-
composed MaSA (MaSA-d) in the third stage with the origi-
nal MaSA to validate the effectiveness of our decomposition
method, as illustrated in Tab. 7. In terms of image classifi-
cation, MaSA-d and MaSA achieve comparable accuracy.
However, for semantic segmentation, employing MaSA-d
significantly reduces computational burden while yielding
similar result.

MaSA v.s. Retention. As shown in Tab. 8, we replace
MaSA with the original retention in the architecture of
RMT-S. We partition the tokens into chunks using the
method employed in Swin-Transformer [35] for chunk-wise
retention. Due to the limitation of retention in modeling
one-dimensional causal data, the performance of the vi-
sion backbone based on it falls behind RMT. Moreover, the
chunk-wise and recurrent forms of retention disrupt the par-
allelism of the vision backbone, resulting in lower inference
speed.

Inference Speed. We compare the RMT’s inference
speed with the recent best performing vision backbones in
Tab. 9. Our RMT demonstrates the optimal trade-off be-
tween speed and accuracy.

5. Conclusion

In this work, we propose RMT, a vision backbone with ex-
plicit spatial prior. RMT extends the temporal decay used
for causal modeling in NLP to the spatial level and intro-
duces a spatial decay matrix based on the Manhattan dis-
tance. The matrix incorporates explicit spatial prior into the
Self-Attention. Additionally, RMT utilizes a Self-Attention

--- Page 9 ---
decomposition form that can sparsely model global infor-
mation without disrupting the spatial decay matrix. The
combination of spatial decay matrix and attention decom-
position form enables RMT to possess explicit spatial prior
and linear complexity. Extensive experiments in image clas-
sification, object detection, instance segmentation, and se-
mantic segmentation validate the superiority of RMT.

A. Architecture Details

Our architectures are illustrated in the Tab. 10. For con-
volution stem, we apply five 3 x 3 convolutions to embed
the image into 56 x 56 tokens. GELU and batch normal-
ization are used after each convolution except the last one,
which is only followed by batch normalization. 3 x 3 con-
volutions with stride 2 are used between stages to reduce
the feature map’s resolution. 3 x 3 depth-wise convolutions
are adopted in CPE. Moreover, 5 x 5 depth-wise convolu-
tions are adopted in LCE. RMT-DeiT-S, RMT-Swin-T, and
RMT-Swin-S are models that we used in our ablation exper-
iments. Their structures closely align with the structure of
DeiT [49] and Swin-Transformer [35] without using tech-
niques like convolution stem, CPE, and others.

B. Experimental Settings

ImageNet Image Classification. We adopt the same
training strategy with DeiT [49] with the only supervi-
sion is the classification loss. In particular, our mod-
els are trained from scratch for 300 epochs. We use the
AdamW optimizer with a cosine decay learning rate sched-
uler and 5 epochs of linear warm-up. The initial learn-
ing rate, weight decay, and batch size are set to 0.001,
0.05, and 1024, respectively. Our augmentation settings
are RandAugment [8] (randm9-mstd0.5-inc1), Mixup [70]
(prob=0.8), CutMix [69] (probe=1.0), Random Erasing [73]
(prob=0.25) and Exponential Moving Average (EMA) [40].
The maximum rates of increasing stochastic depth [24] are
set to 0.1/0.15/0.4/0.5 for RMT-T/S/B/L, respectively. For a
more comprehensive comparison, we train two versions of
the model. The first version uses only classification loss as
the supervision, while the second version, in addition to the
classification loss, incorporates token labeling introduced
by [27] for additional supervision. Models using token la-
beling are marked with**”.

COCO Object Detection and Instance Segmentation.
We apply RetinaNet [32], Mask-RCNN [22] and Cascaded
Mask-CNN [2] as the detection frameworks to conduct
experiments. We implement them based on the MMDe-
tection [4]. All models are trained under two common
settings:““1 x” (12 epochs for training) and“3x+MS” (36
epochs with multi-scale augmentation for training). For the
“1x” setting, images are resized to the shorter side of 800

pixels. For the “3x+MS”, we use the multi-scale training
strategy and randomly resize the shorter side between 480
to 800 pixels. We apply AdamW optimizer with the initial
learning rate of le-4. For RetinaNet, we use the weight
decay of le-4 for RetinaNet while we set it to 5e-2 for
Mask-RCNN and Cascaded Mask-RCNN. For all settings,
we use the batch size of 16, which follows the previous
works [35, 63, 64]

ADE20K Semantic Segmentation. Based on MMSeg-
mentation [7], we implement UperNet [59] and Seman-
ticFPN [28] to validate our models. For UperNet, we
follow the previous setting of Swin-Transformer [35] and
train the model for 160k iterations with the input size of
512 x 512. For SemanticFPN, we also use the input resolu-
tion of 512 x 512 but train the models for 80k iterations.

C. Efficiency Comparison

We compare the inference speed of RMT with other back-
bones, as shown in Tab. 11. Our models achieve the best
trade-off between speed and accuracy among many com-
petitors.

D. Details of Explicit Decay

We use different y for each head of the multi-head ReSA to
control the receptive field of each head, enabling the ReSA
to perceive multi-scale information. We keep all the + of
ReSA’s heads within a certain range. Assuming the given
receptive field control interval of a specific ReSA module is
[a, b], where both a and b are positive real numbers. And
the total number of the ReSA module’s heads is N. The y
for its ith head can be written as Eq. 8:

(b=a)i

st Oo (8)

For different stages of different backbones, we use dif-
ferent values of a and b, with the details shown in Tab. 12.

--- Page 10 ---
Model Blocks Channels Heads Ratios Params(M) FLOPs(G)
RMT-T [2, 2, 8, 2] [64, 128, 256, 512] [4,4, 8,16]  [3, 3,3, 3] 14 2.5
RMT-S (3,4, 18,4]  [64, 128, 256, 512] [4,4, 8,16]  [4, 4,3, 3] 27 4.5
RMT-B [4, 8, 25,8]  [80, 160,320,512] [5,5, 10,16] [4, 4,3, 3] 54 9.7
RMT-L [4, 8, 25, 8] [112, 224, 448, 640] [7,7, 14,20]  [4, 4, 3, 3] 95 18.2
RMT-DeiT-S [12] [384] [6] [4] 22 4.6
RMT-Swin-T | [2, 2, 6, 2] [96, 192, 384, 768]  [3,6, 12,24] [4,4,4, 4] 29 47
RMT-Swin-S | [2,2, 18,2] [96, 192,384,768]  [3,6, 12,24] [4,4,4, 4] 50 9.1
Table 10. Detailed Architectures of our models.
Params FLOPs_ Troughput | Top! Params FLOPs_ Troughput | Top!
Model (M)  G)_—imgs/s)_ | (%) Model (M) = (G)_—Cimgsis) | (%)
MPViT-XS [29] 11 2.9 1496 80.9 Focal-S [63] 51 9.1 351 83.5
Swin-T [35 29 4.5 1704 81.3 Eff-B5 [47] 30 9.9 302 83.6
BiFormer-T [75] 13 2.2 1602 81.4 SGFormer-M [15] 39 75 598 84.1
GC-ViT-XT [20] 20 2.6 1308 82.0 SMT-B [34] 32 77 237 84.3
SMT-T [34 12 24 636 82.2 BiFormer-B [75] 57 9.8 498 84.3
RMT-T 14 2.5 1650 82.4 RMT-Swin-S 50 9.1 722 84.5
Focal-T [63 29 49 582 | 82.2 vee a ° y ne ve
cswin-Ttil] | 22 43 isol | 92.7 MTB [I6] °. 7 °
Eff-B4 [47] 19 42 627 82.9 iFormer-B [45] 48 9.4 688 84.6
MPViTS [29] | 23. 47 986 | 83.0 EMES oh _Eb “o7/_|| Hw
Swin-S [35 50 8.8 1006 83.0 Swin-B [35] 88 5.5 756 83.5
SGFormer-S [15] 23 4.8 952 83.2 Eff-B6 [47] 43 9.0 172 84.0
iFormer-S [45] 20 4.8 1051 83.4 Focal-B [63] 90 6.4 256 84.0
CMT-S [16] 25 4.0 848 83.5 CSWin-B [11] 78 5.0 660 84.2
RMT-Swin-T 29 47 1192 83.6 MPViT-B [29] 75 6.4 498 84.3
CSwin-S [11] 35 6.9 972 83.6 SMT-L [34] 80 77 158 84.6
MaxViT-T [51] 31 5.6 826 83.6 SGFormer-B [15] 78 5.6 388 84.7
SMT-S [34] 20 4.8 356 83.7 iFormer-L [45] 87 4.0 410 84.8
BiFormer-S [75] 26 4.5 766 83.8 MaxViT-B [51] 120 23.4 306 84.9
RMT-S 27 4.5 876 84.1 RMT-L 95 8.2 326 85.5
Table 11. Comparison of inference speed.
Model A b References
RMT-T (2, 2, 2,2 [6, 6, 8, 8] 1] Moab Arar, Ariel Shamir, and Amit H. Bermano. Learned
RMT-S (2,2,2,2] [6,6, 8,8] queries for efficient local attention. In CVPR, 2022. 5
RMT-B (2,2,2,2] [7,7, 8, 8] 2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving
RMT-L (2,2,2,2] [8, 8, 8,8] into high quality object detection. In CVPR, 2018. 5,9
7 3] Chun-Fu (Richard) Chen, Rameswar Panda, and Quanfu
RMT-DeiT'S [2] [8] Fan. RegionViT: Regional-to-Local Attention for Vision
RMT-Swin-T | [2,2,2,2] [8, 8, 8, 8] Transformers. In JCLR, 2022. 5,7
RMT-Swin-S | [2, 2, 2,2 [8, 8, 8, 8] 4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, et al. MMDetec-
tion: Open mmlab detection toolbox and benchmark. arXiv
Table 12. Details about the y decay. preprint arXiv:1906.07155, 2019. 5,9
5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-

ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.
Twins: Revisiting the design of spatial attention in vision

--- Page 11 ---
12

13

14

15

16

17

18

19

20

21

22

23

24

25

transformers. In NeurIPS, 2021. 2

Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and
Chunhua Shen. Conditional positional encodings for vision
transformers. In JCLR, 2023. 2, 3, 4

MMSegmentation Contributors. Mmsegmentation, an open
source semantic segmentation toolbox, 2020. 7, 9

Ekin D Cubuk, Barret Zoph, Jonathon Shlens, et al. Ran-
daugment: Practical automated data augmentation with a re-
duced search space. In CVPRW, 2020. 4, 9

Jia Deng, Wei Dong, Richard Socher, et al. Imagenet: A
large-scale hierarchical image database. In CVPR, 2009. 4
Mingyu Ding, Bin Xiao, Noel Codella, et al. Davit: Dual
attention vision transformers. In ECCV, 2022. 5

Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al. Cswin
transformer: A general vision transformer backbone with
cross-shaped windows. In CVPR, 2022. 2, 3, 5, 6,7, 10
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
et al. An image is worth 16x16 words: Transformers for
image recognition at scale. In JCLR, 2021. 1,2

Qihang Fan, Huaibo Huang, Jiyang Guan, and Ran He. Re-
thinking local perception in lightweight vision transformer,
2023. 1, 2,3

Li Gao, Dong Nie, Bo Li, and Xiaofeng Ren. Doubly-fused
vit: Fuse information from vision transformer doubly with
local representation. In ECCV, 2022. 2

SG-Former: Self guided Transformer with Evolving To-
ken Reallocation. Sucheng ren, xingyi yang, songhua liu,
xinchao wang. In JCCV, 2023. 5, 10

Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang,
Chunjing Xu, and Yunhe Wang. Cmt: Convolutional neural
networks meet vision transformers. In CVPR, 2022. 1, 3, 4,
5, 6, 8, 10

Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming
Cheng, and Shi-Min Hu. Visual attention network. arXiv
preprint arXiv:2202.09741, 2022. 5,7

Kai Han, An Xiao, Enhua Wu, et al. Transformer in trans-
former. In NeurIPS, 2021. 2

Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and
Humphrey Shi. Neighborhood attention transformer. In
CVPR, 2023. 1, 2, 3,5, 6,7

Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz,
and Pavlo Molchanov. Global context vision transformers.
In ICML, 2023. 5, 6, 7, 10

Kaiming He, Xiangyu Zhang, Shaoging Ren, and Sun Jian.
In CVPR,

Deep residual learning for image recognition.
2016. 6,7

Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B.
Girshick. Mask r-cnn. In ICCV, 2017. 5,9

Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi
Feng. Conv2former: A simple transformer-style convnet for
visual recognition. arXiv preprint arXiv:2211.11943, 2022.
5

Gao Huang, Yu Sun, and Zhuang Liu. Deep networks with
stochastic depth. In ECCV, 2016. 4, 9

Huaibo Huang, Xiaoqiang Zhou, and Ran He. Orthogonal
transformer: An efficient vision transformer backbone with
token orthogonalization. In NeurIPS, 2022. 5,6

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, et al. Scaling
up visual and vision-language representation learning with
noisy text supervision. In JCML, 2021. 2

Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun
Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens
matter: Token labeling for training better vision transform-
ers. In NeurIPS, 2021. 1,5, 9

Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr
Dollar. Panoptic feature pyramid networks. In CVPR, 2019.
7,9

Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju
Hwang. Mpvit: Multi-path vision transformer for dense pre-
diction. In CVPR, 2022. 1, 5, 6,7, 10

Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu,
Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer
for efficient spatiotemporal representation learning, 2022. 1,
4,5,6,7

Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-
galam, Bo Xiong, Jitendra Malik, and Christoph Feichten-
hofer. Mvitv2: Improved multiscale vision transformers for
classification and detection. In CVPR, 2022. 1,5

Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, and Kaim-
ing He andPiotr Dollar. Focal loss for dense object detection.
In ICCV, 2017. 5,9

Tsung-Yi Lin, Michael Maire, Serge Belongie, et al. Mi-
crosoft coco: Common objects in context. In ECCV, 2014.
4
Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, and Lian-
wen Jin. Scale-aware modulation meet transformer. In JCCV,
2023. 1,5, 6,7, 8, 10
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCYV, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, et al. A convnet
for the 2020s. In CVPR, 2022. 5, 6

Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-io: A unified
model for vision, language, and multi-modal tasks. In JCLR,
2023. 2
Junting Pan, Adrian Bulat, Fuwen Tan, et al. Edgevits: Com-
peting light-weight cnns on mobile devices with vision trans-
formers. In ECCV, 2022. 7

Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision
transformers with hilo attention. In NeurIPS, 2022. 2,5
Boris T Polyak and Anatoli B Juditsky.
of stochastic approximation by averaging.
arXiv:1906.07155, 2019. 9

Ofir Press, Noah Smith, and Mike Lewis. Train short, test
long: Attention with linear biases enables input length ex-
trapolation. In JCLR, 2022. 2, 3

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, et al. Learning transferable visual models from nat-
ural language supervision. In JCML, 2021. 2

Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou,
Ser-Lam Lim, and Jiwen Lu. Hornet: Efficient high-order
spatial interactions with recursive gated convolutions. In
NeurIPS, 2022. 6,7

Acceleration
arXiv preprint


--- Page 12 ---
44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and
Xinchao Wang. Shunted self-attention via multi-scale token
aggregation. In CVPR, 2022. 7

Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xin-
chao Wang, and Shuicheng YAN. Inception transformer. In
NeurIPS, 2022. 5, 10

Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing
Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Reten-
tive network: A successor to Transformer for large language
models. ArXiv, abs/2307.08621, 2023. 1, 2, 3,7

Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In ICML, 2019.
10

Shitao Tang, Jiahui Zhang, Siyu Zhu, et al. Quadtree atten-
tion for vision transformers. In JCLR, 2022. 5

Hugo Touvron, Matthieu Cord, Matthijs Douze, et al. Train-
ing data-efficient image transformers & distillation through
attention. In JCML, 2021. 2, 4, 5,7, 8,9

Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and Hervé Jégou. Going deeper with im-
age transformers. In JCCV, 2021. 1,5

Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit:
Multi-axis vision transformer. In ECCV, 2022. 1,5, 8, 10
Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Atten-
tion is all you need. In NeurIPS, 2017. 2

Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-
mid vision transformer: A versatile backbone for dense pre-
diction without convolutions. In CCV, 2021. 2, 3, 4, 6,7
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pytv2: Improved baselines with pyramid vision transformer.
Computational Visual Media, 8(3):1-10, 2022. 2, 3, 4, 5, 6,
7

Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai,
Xiaofei He, and Wei Liu. Crossformer: A versatile vision
transformer hinging on cross-scale attention. In JCLR, 2022.
5, 6,7

Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,
Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,
Hongsheng Li, et al. Internimage: Exploring large-scale vi-
sion foundation models with deformable convolutions. In
CVPR, 2023. 5, 6,7

Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-
ing convolutions to vision transformers. arXiv preprint
arXiv:2103.15808, 2021. 1, 2

Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao
Huang. Vision transformer with deformable attention. In
CVPR, 2022. 2, 6,7

Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In ECCV, 2018. 7,9

Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,
Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jin-
gren Zhou. mplug-2: A modularized multi-modal foundation
model across text, image and video. In JCML, 2023. 2
Chenglin Yang, Yilin Wang, Jianming Zhang, et al. Lite
vision transformer with enhanced self-attention. In CVPR,
2022. 1

Chenglin Yang, Siyuan Qiao, Qihang Yu, et al. Moat: Alter-
nating mobile convolution and attention brings strong vision
models. In JCLR, 2023. 5

Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang
Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-
attention for local-global interactions in vision transformers.
In NeurIPS, 2021. 2, 3, 5, 6, 9, 10

Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.
Focal modulation networks. In NeurIPS, 2022. 5,9

Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng
Xiao, Min Zheng, and Xiu Li. Scalablevit: Rethinking
the context-oriented generalization of vision transformer. In
ECCYV, 2022. 5, 6

Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao
Mei. Wave-vit: Unifying wavelet and transformers for visual
representation learning. In Proceedings of the European con-
ference on computer vision (ECCV), 2022. 2,5

Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping
Zhang, and Tao Mei. Dual vision transformer. TPAMI, 2023.
5

Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and
Shuicheng Yan. Volo: Vision outlooker for visual recog-
nition. TPAMI, 2022. 5

Sangdoo Yun, Dongyoon Han, Seong Joon Oh, et al. Cut-
mix: Regularization strategy to train strong classifiers with
localizable features. In ICCV, 2019. 4,9

Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, et al.
mixup: Beyond empirical risk minimization. In JCLR, 2018.
4,9

Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu
Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-
former: A new vision transformer for high-resolution image
encoding. In ICCV, 2021. 4

Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao.
Vsa: Learning varied-size window attention in vision trans-
formers. In ECCV, 2022. 4

Zhun Zhong, Liang Zheng, Guoliang Kang, et al. Random
erasing data augmentation. In AAAI, 2020. 4, 9

Bolei Zhou, Hang Zhao, Xavier Puig, et al. Scene parsing
through ade20k dataset. In CVPR, 2017. 4

Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and
Rynson Lau. Biformer: Vision transformer with bi-level
routing attention. In CVPR, 2023. 1, 2,4, 5, 6, 8, 10


