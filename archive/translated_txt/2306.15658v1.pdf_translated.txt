--- ABSTRACT ---
최근 연구인 CLIPA[12]는 CLIP 학습을 위한 역 스케일링 법칙을 제시하는데, 사용된 이미지/텍스트 인코더가 클수록 학습에 적용할 수 있는 이미지/텍스트 토큰의 시퀀스 길이가 짧아집니다. 이 결과를 통해 상당히 줄어든 계산으로 고성능 CLIP 모델을 학습할 수 있습니다. 이 연구를 바탕으로 두 가지 주요 기여를 통해 CLIPA-v2를 제시합니다. 기술적으로 이 역 스케일링 법칙은 미세 조정 단계에도 적용 가능하여 계산 요구 사항을 더욱 줄일 수 있습니다. 경험적으로 대규모 CLIPA를 탐색하여 학습 중에 약 13B개의 이미지-텍스트 쌍이 있는 H/모델까지 실험을 확장했습니다. 우리의 결과는 흥미진진합니다. 10,000달러의 예산만 할당함으로써 우리의 CLIP 모델은 인상적인 제로샷 ImageNet 정확도인 81.1%를 달성하여 이전의 최고 CLIP 모델(OpenCLIP에서 80.1%)을 1.0% 능가하고 동시에 계산 비용을 약 39배 줄였습니다. 또한 4,000달러를 추가로 투자하면 제로샷 ImageNet 정확도를 81.8%까지 더욱 높일 수 있습니다. CLIP[17]은 텍스트와 이미지 간의 격차를 메우는 선구적인 기반 모델로 등장하여 컴퓨터 비전 연구를 &quot;포스트-ImageNet&quot; 시대로 이끌었습니다[10, 13, 27, 1, 18, 20, 22, 25, 4]. 그러나 CLIP의 까다로운 계산 요구 사항은 광범위한 탐색을 방해합니다. 최근 작업 CLIPA[12]는 다음을 통해 계산 효율적인 솔루션을 제공합니다.
