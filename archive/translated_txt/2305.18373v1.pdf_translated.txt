--- ABSTRACT ---
이미지 광고 이해는 광범위한 실제 세계 응용 프로그램을 가진 중요한 과제입니다. 다양한 비정형 장면, 실제 세계 엔터티, 장면 텍스트에 대한 추론이 관련되어 매우 어렵지만, 이미지 광고를 해석하는 방법은 상대적으로 탐구가 부족하며, 특히 인상적인 일반화와 적응성을 특징으로 하는 기초적 시각 언어 모델(VLM) 시대에 그렇습니다. 이 논문에서 우리는 사전 훈련된 VLM의 렌즈를 통해 이미지 광고 이해에 대한 최초의 실증적 연구를 수행합니다. 우리는 이러한 VLM을 이미지 광고 이해에 적응시키는 데 있어서 벤치마킹하고 실질적인 과제를 밝힙니다. 우리는 이미지 광고에 대한 다중 모달 정보를 효과적으로 융합하고 실제 세계 엔터티에 대한 지식으로 더욱 강화하기 위한 간단한 기능 적응 전략을 제안합니다. 우리는 우리의 연구가 광고 산업과 광범위하게 관련이 있는 이미지 광고 이해에 더 많은 관심을 끌기를 바랍니다.
--- INTRODUCTION ---
광고가 인간 사회에서 필수적인 역할을 하기 때문에 이미지 광고 이해는 광고 타겟팅(Hussain et al., 2017), 시각적 은유 이해(Abokhoza et al., 2019) 및 창의적인 광고 생성(Chilton et al., 2019; Akula et al., 2022)과 같은 많은 실제 세계 응용 프로그램을 가지고 있습니다. 또한 그림 2에서 예시된 것처럼 여러 가지 이유로 인해 매우 어렵습니다. 첫째, 이미지 광고는 일반적인 학술 데이터 세트를 넘어서는 창의적으로 합성된 비사실적 객체 및 비정형 장면을 포함한 다양한 시각적 요소로 구성됩니다. 둘째, 기존 작업(Su et al., 2018; Li et al., 2022a)에서는 다루기 어려운 브랜드 및 제품과 같은 많은 실제 세계 엔터티에 대한 지식이 필요합니다. 마지막으로, 많은 사람들이 장면 텍스트를 포함한 다양한 시각적 요소에 대한 추론을 요구하는 시각적 또는 다중 모달 수사학을 채택하고, 때로는 인간을 피하기도 합니다(Petridis 및 * Google에서 인턴십을 하는 동안 부분적으로 수행한 작업. zjia@eng.ucsd.edu로 서신 보내기. 지식 CLIP 사전 학습된 VLM 인코더 X (a) 이국적이고 재미있기 때문에 관광지를 방문해야 합니다. ✓ (b) 가볍기 때문에 &quot;브랜드 이름&quot; 컴퓨터를 사야 합니다. X (c) 스타일리시하기 때문에 &quot;다른 브랜드 이름&quot;을 착용해야 합니다. 브랜드 이해 주의 기반 기능 어댑터 KAFA 그림 1: 브랜드 이해 모듈을 통해 외부 지식을 활용하고 가벼운 주의 기반 기능 어댑터를 통해 다양한 모달리티의 기능을 결합하여 이미지 광고의 올바른 메시지를 디코딩하는 것을 제안합니다. VLM 기준선은 혼동되어 잘못된 것을 제공합니다. 모든 브랜드 정보는 익명화됩니다. Chilton, 2019). 그러나 이미지 광고 이해는 기계 학습 커뮤니티에서 상대적으로 탐구가 부족하며, 특히 최근에 개발되어 엄청난 양의 이미지 및 텍스트 설명 데이터를 사용하여 사전 학습된 기초적 시각 언어 모델(VLM)이 있는 경우에 그렇습니다. 사전 학습된 VLM은 일반화 기능이 뛰어나고 실제 세계 지식을 포함하고(암묵적으로) 데이터 효율적인 방식으로 광범위한 다운스트림 작업에 적용할 수 있는 것으로 나타났습니다(Radford et al., 2021; Alayrac et al., 2022). 따라서 이미지 광고 이해를 위해 VLM을 활용하는 것은 당연합니다. 이 논문에서는 일반적으로 시각적 질의 응답(Hussain et al., 2017)으로 공식화되는 이미지 광고에서 전달되는 전체 메시지를 디코딩하는 작업에 VLM을 적용하는 최초의 실증적 연구를 수행합니다. 구체적으로, 정렬 기반이고 공개적으로 사용 가능한 세 가지 인기 있는 사전 훈련된 VLM인 CLIP(Radford 등, 2021), ALBEF(Li 등, 2021) 및 LiT(Zhai 등, 2022)를 살펴봅니다. 우리는 제로 샷 성능과 적응 전략을 살펴보고 VLMS를 이미지 광고에 적용하는 데 따른 실질적인 과제를 밝힙니다. 우리는 VLM 기능을 효과적으로 활용하는 간단한 기능 적응 전략을 제안합니다. 우리는 또한 중요한 의미를 가져오는 외부 브랜드 지식(실제 엔터티)을 통합하는 것을 제안합니다. 주차장 Aision Parkin 로고 브랜드 그림 2: 다양한 시각적 요소, 비정형적 장면 및 수사학을 사용하여 메시지를 창의적으로 전달하는 예시 이미지 광고. 모든 브랜드 정보는 익명으로 처리됩니다. 상당한 성능 향상. 우리의 기여는 세 가지입니다. 첫째, 우리는 경험적으로 사전 학습에 사용된 모델의 순수한 데이터 규모와 용량이 이미지 광고 이해 성능에 가장 중요한 요소라는 것을 발견했는데, 이는 부분적으로 VLM이 실제 지식을 저장할 수 있는 능력 때문이며, 이는 VLM을 비교하는 데 일반적으로 사용되는 지표로는 잘 포착되지 않습니다. 둘째, 우리는 이미지 광고 이해를 위해 VLM을 조정하는 데 따르는 실질적인 과제(즉, 제한된 학습 데이터 및 감독 신호에 대한 과적합 및 하드 네거티브 마이닝의 높은 계산 부담)를 밝히고 이전 조정 전략보다 VLM 기능을 더 잘 활용하는 간단한 솔루션(주의 기반 기능 조정)을 제안합니다. 마지막으로, 우리는 경험적으로 이미지 광고 이해를 더욱 향상시키는 것으로 입증된 브랜드 이해를 위해 외부 지식을 활용할 것을 제안합니다. 앞서 언급한 조정 전략과 함께, 우리는 우리의 접근 방식을 지식 증강 기능 조정(KAFA)이라고 부릅니다. 2
--- RELATED WORK ---
이미지 광고 이해 이미지 광고를 자동으로 해석하는 학습은 Pitt 이미지 광고 데이터 세트(Hussain et al., 2017)에서 제안되었으며, 각 광고에는 &quot;광고에 따라 무엇을 해야 하며 그 이유는 무엇인가?&quot;라는 질문에 대한 답이 있는 캡션이 주석으로 달려 있습니다. 기존의 이미지 캡션과 달리 이 작업은 섹션 1의 시작 부분에서 논의한 것처럼 매우 간단하지 않습니다.
--- METHOD ---
s는 외부 상징적 지식을 통해 문화적 의미를 활용하고(Ye 및 Kovashka, 2018), GNN을 통해 장면 텍스트와 객체 간의 관계를 포착하고(Dey 등, 2021), 사전 훈련된 언어 모델을 활용하여 다중 모델 정보를 결합합니다(Kalra 등, 2020). 아무도 시각 언어 모델(VLM)과 실제 세계 엔터티(예: 브랜드)에 대한 지식을 활용하지 않았습니다. 광고 산업에서의 광범위한 응용 프로그램 외에도 이후의 연구는 이미지 광고 연구가 훨씬 더 광범위한 연구 주제와 관련이 있음을 암시합니다(Singh 등, 2019; Akula 등, 2022). 기초 정렬 기반 VLM 텍스트 설명과 함께 제공되는 엄청난 이미지 컬렉션의 최근 급증(Schuhmann 등, 2022)은 다운스트림 작업을 위한 효율적인 제로샷 또는 로우샷 학습기인 기초 VLM의 정렬 기반 사전 훈련(즉, 대조 학습)을 가능하게 합니다. 이미지와 텍스트를 공유된 의미 공간에 임베드하는 법을 학습함으로써, 그들은 오픈 어휘 방식(실제 세계 지식 포함)으로 도메인 변화를 처리합니다.여기에는 CLIP(Radford 등, 2021), ALIGN(Jia 등, 2021), LiT(Zhai 등, 2022) 및 BASIC(Pham 등, 2021)이 있습니다.또 다른 작업 라인은 마스크 언어 모델링, 이미지 캡션 손실 및 객체 수준 정렬을 추가로 채택합니다.예: ALBEF(Li 등, 2021), Florence(Yuan 등, 2021), CoCa(Yu 등, 2022) 및 GLIP(Li 등, 2022b) VLM의 전이 학습 VLM의 전이 학습은 이미지 분류 작업에서 CLIP의 제로 샷 성능으로 인해 인기를 얻었습니다. 직접적인 접근 방식은 다운스트림 작업에 맞게 조정된 추가 신경망(선택 사항)을 사용하여 VLM을 (부분적으로) 미세 조정하는 것입니다.예: TAP-C(Song 등, 2022), CPT(Yao 등, 2021), KAT(Gui 등, 2021) 및 VL-Adapter(Sung 등, 2022). VLM을 조정할 필요성을 우회하는 또 다른 접근 방식은 즉시 학습입니다.예를 들어, Coop(Zhou 등, 2022b) 및 CoCoOp(Zhou 등, 2022a)은 VLM에 대한 학습 가능한 입력만 조정합니다.메모리 및 계산 부담을 더욱 줄이는 세 번째 접근 방식은 피처 어댑터로, 입력의 VLM 피처가 전이 학습 전에 미리 계산됩니다. 예로는 CLIP-Adapter(Gao et al., 2021), SVL-Adapter(Pantazis et al., 2022) 및 Attention-Adapter(Zhao et al., 2022)가 있습니다. 지식 증강 이미지 이해 많은 이미지 이해 작업에는 입력 데이터로 포착할 수 있는 것 이상의 실제 지식이 필요합니다. 예를 들어, FVQA(Wang et al., 2017) 및 OK-VQA(Marino et al., 2019)는 외부 사실 기반 지식을 처리하는 모델이 필요합니다. TextVQA(Singh et al., 2019)는 야생에서 명명된 엔터티를 이해하려고 합니다. Pitt Dataset(Hussain et al., 2017)은 대량의 브랜드 인식을 포함합니다. 기존 작업에서는 구조화되거나 구조화되지 않은 지식 기반(Wang 등, 2015; Gardères 등, 2020; Ye 및 Kovashka, 2018)을 통해 명시적으로 외부 지식을 통합하거나 사전 학습된 모델에 저장된 지식에서 암묵적으로(Kalra 등, 2020; Kim 등, 2022) 또는 둘 다(Marino 등, 2021; Gui 등, 2021) 통합합니다. 3 이미지 광고 이해에서 사전 학습된 VLM에 실제로 중요한 것은 무엇일까요? 우리의 경험적 연구의 첫 번째 통찰력은 사전 학습에 사용된 데이터의 엄청난 크기와 모델이 이미지 광고 이해를 위한 VLM의 성능을 결정하는 핵심 요소라는 것입니다. 재현성을 높이기 위해 Pitt 데이터 세트(Hussain et al., 2017)에서 제로샷 방식으로 공개적으로 액세스할 수 있는 세 가지 정렬 기반 VLM(즉, CLIP, ALBEF 및 LiT)을 평가합니다.이 데이터 세트는 광고 이해를 이미지-텍스트 검색으로 공식화합니다.우리는 모델에 12805개 테스트 샘플 각각에 대해 15개 후보(12개 잘못된 메시지 포함) 세트에서 이미지 광고가 전달하는 3개의 올바른 메시지 중 하나를 선택하도록 요청하는 공식 평가 프로토콜을 채택합니다.특히 정렬 기반 VLM이 주어지면 정규화된 출력을 갖는 인코더를 이미지 및 텍스트 분기에 대해 각각 fi(·) 및 fr(·)로 표시하겠습니다.이미지 x와 기준 진실 텍스트 y가 주어지면 VLM은 점곱 점수 fi(x) fr(y)에 따라 후보 C(x)에서 y를 검색합니다. 그런 다음 문헌에서 일반적으로 사용되는 3가지 지표를 사용하여 VLM의 성능을 측정합니다.정확도(순위 1로 검색된 긍정적 텍스트가 있는 이미지의 비율), 순위(모든 이미지에서 가장 많이 검색된 긍정적 텍스트의 평균 순위), 평균 순위(모든 이미지에서 모든 긍정적 텍스트의 평균 순위)입니다.표 1에 보고된 결과를 통해 몇 가지 결과를 얻을 수 있습니다.첫째, VLM의 사전 학습 중에 사용된 데이터가 많을수록 이미지 광고 도메인으로 일반화하는 데 더 효과적입니다.비교를 위해 CLIP에서는 4억 개의 이미지-텍스트 쌍, LiT는 1억 개, ALBEF는 1,400만 개를 확인했습니다.둘째, 모델의 용량이 클수록 이미지 광고를 더 잘 이해합니다.표 1에 표시된 세 가지 크기 외에도 다양한 크기의 CLIP 모델을 평가했으며 추세는 동일하게 유지되었습니다. 셋째, ImageNet(Russakovsky et al., 2015) 검증 세트의 제로샷 정확도(LiT가 CLIP보다 성능이 우수하다고 주장) 및 Flickr30K(Young et al., 2014)의 이미지-텍스트 검색 정확도(ALBEF가 CLIP보다 성능이 우수하다고 주장)를 포함하여 VLM을 비교하는 데 일반적으로 사용되는 지표는 이미지 광고 이해 성능을 잘 반영하지 못합니다. 우리는 이것이 부분적으로 이미지 VILBERT(Lu 등, 2019) VS(v1)(Dey 등, 2021) BERT-FT(Kalra 등, 2020) ALBEF(Li 등, 2021) ALBEF(Flickr30k에서 ft.) ALBEF(MSCOCO에서 ft.) LIT(L16L)(Zhai 등, 2022) CLIP(VIT-B/32)(Radford 등, 2021) CLIP(VIT-B/16) CLIP(VIT-L/14@336px) KAFA(우리) Acc ↑ 순위↓ m이기 때문이라고 가설을 세웠습니다. 순위↓ 61.8 1.4.86.8 1.3.89.7 1.2.57.2.4.64.2 2.5.64.0 2.4.64.0 1.4.88.1 1.2.92.2 1.2.95.2 1.97.4 1.2.2.표 1: 공식 평가 프로토콜(테스트 이미지당 3개의 긍정적 텍스트와 12개의 부정적 텍스트)이 적용된 Pitt 데이터 세트(Hussain et al., 2017)에 대한 제로샷 VLM 성능. 최상의 CLIP 모델은 이미 이전의 최첨단 결과(BERT-FT)를 능가합니다. VLM 사전 학습에 사용된 데이터와 모델의 크기는 결과에 큰 영향을 미칩니다. 메트릭에 대한 자세한 내용은 3절을 참조하세요. 완전성을 위해 제안하는 방법(KAFA)의 결과도 여기에 포함합니다. 광고 이해에는 사전 훈련된 모델에 포함된 실제 세계 엔터티(예: 브랜드)에 대한 지식이 필요합니다. 더 큰 규모의 훈련 데이터와 모델 용량에 의해 주도된 GPT 언어 모델(Brown et al., 2020)의 극적인 성능 향상과 유사하게, 더 많은 지식을 추출하여 더 큰 모델과 더 많은 사전 훈련 데이터가 있는 사전 훈련된 VLM의 가중치에 암묵적으로 저장할 수 있습니다. 우리는 VLM이 이미지에서 브랜드를 인식하는 기능이 광고에서 메시지를 디코딩하는 성능과 일치한다는 것을 경험적으로 검증했습니다. 표 4에서 결과를 참조하세요. 이미지 광고에 대한 VLM 적응의 4가지 과제와 직관적 솔루션 명확한 챔피언인 CLIP을 사용하여 최상의 CLIP 모델(ViT-L/14@336px)을 백본으로 사용하여 이미지 광고 이해를 위한 VLM 적응을 추가로 연구합니다. 우리는 이미지에서 추출한 장면 텍스트와 같은 추가 정보의 도움으로 사전 훈련된 VLM을 이미지 광고 도메인에 더 잘 적응시켜 이미지 광고 이해에 대한 더 나은 성능을 가능하게 하는 것을 목표로 합니다. 4.1 과대적합 및 높은 계산 복잡도 문제 사전 학습된 VLM을 이미지 광고에 적용하는 데 두 가지 실질적인 과제가 있습니다.첫째, 제한된 이미지 광고와 강력한 감독 신호의 부족으로 인해 미세 조정에서 과대적합 문제가 발생하고 둘째, 이전 과제의 솔루션으로 인해 발생하는 높은 계산 부담입니다.일반적으로 이미지 광고의 주석을 얻기 어렵기 때문에(Akula et al., 2022) 제한된 학습 데이터만 사용하는 것이 일반적입니다(예: Pitt 데이터 세트에는 51,223개의 이미지-텍스트 쌍만 포함됨).이로 인해 VLM이 적응하는 동안 과대적합에 취약해집니다.원래 CLIP 논문에서와 같이 대칭적 교차 엔트로피 손실을 사용하여 Pitt 데이터 세트에서 CLIP을 대조적으로 직접 미세 조정하면 조기 중단과 신중하게 조정된 학습 속도 일정을 채택하지 않는 한 제로 샷보다 성능이 떨어집니다.또한, Tab.에 보고된 대로 1, CLIP의 최상의 제로샷 성능은 이미 이전 최신 기술을 능가하고 100%에 매우 가까워 바닐라 미세 조정에 대한 매우 약한 감독 신호로 이어집니다.따라서 강력한 훈련 신호가 필요합니다.훨씬 더 큰 배치 크기에 필요한 GPU 메모리를 절약하기 위해 미니 배치 내에서가 아니라 매우 큰 후보 세트에서 하드 네거티브를 선택하는 하드 네거티브 마이닝(Xuan et al., 2020)을 채택합니다.그러나 하드 네거티브 마이닝(HNM) 전략은 일반적으로 큰 계산 부담을 초래합니다.완전 온라인 하드 네거티브 마이닝(전체 HNM으로 표시)에서 각 훈련 이미지 x와 해당 텍스트 y에 대해 먼저 전체 훈련 데이터에서 샘플링된 Ncand개의 네거티브 텍스트 {yy y}를 온라인 유사도 점수(현재 VLM 모델에서 계산된 점곱 점수 f(x) · fr(y))에 따라 순위를 매긴 다음 가장 유사한 Nhard 1 y를 하드 네거티브로 선택합니다. 이는 본질적으로 훨씬 더 어려운 후보 집합 C(x)를 구성하지만, 모든 그래디언트 단계에서 모든 학습 텍스트의 기능을 계산해야 하며, 이는 엄청나게 비쌉니다. 기존 방법은 모든 학습 텍스트 기능의 희소하게 업데이트된 뱅크(메모리 뱅크)(Wu et al., 2018)를 유지하거나 모멘텀 업데이트된 텍스트 인코더(MoCo)(He et al., 2020)의 도움을 받아 복잡성을 줄이는 것을 제안합니다. 그럼에도 불구하고, 우리는 이러한 방법을 우리의 설정¹에 맞게 조정하고 전체 HNM보다 성능이 떨어지는 것을 발견했습니다. 우리는 공식 프로토콜보다 더 어려운 평가 프로토콜을 사용하여 테스트 텍스트에서 무작위로 추출한 더 많은 수(K)의 부정 샘플(따라서 더 어려운 부정 세트)을 사용하여 표에서 정확도(%)를 보고합니다. 이는 이미지 광고 이해에 세밀한 정보 추출(예: 제품의 특정 브랜드)이 필요하고 이 두 가지가 모두 &#39;우리는 이러한 방법을 사용하여 유사성 점수를 계산하지만 여전히 GPU 메모리를 절약하기 위해 미세 조정을 위해 가장 어려운 부정만 선택합니다(HNM의 목적). 후보 수 K 제로샷 직접 FT 직접 FT + 메모리 뱅크 직접 FT + MoCo 직접 FT + 전체 HNM 20 100 50091.7 80.7 64.4 56.92.4 82.2 66.7 59.92.8 82.9 67.5 60.93.3 83.8 69.8 62.93.7 84.6 70.0 62.표 2: Pitt 데이터 세트의 테스트 세트에서 후보 세트의 크기(K)에 따라 보고된 정확도(%). 더 큰 K는 더 강한 음성 샘플을 의미합니다. 제로샷은 최상의 CLIP 모델의 제로샷 성능입니다. FT는 최상의 CLIP 모델을 미세 조정하는 것을 의미합니다. 두 가지 전략은 완전히 온라인 방식이 아닌 방식으로 손실을 계산하기 때문에 이러한 정보가 파괴될 수 있습니다. 특히, 대조적 미세 조정에 사용되는 텍스트 특징은 항상 다른 VLM 인코더, 즉 과거 체크포인트 또는 모멘텀 업데이트 버전에서 나옵니다.전체 HNM을 사용한 직접 미세 조정이 다른 것보다 성능이 뛰어나지만 매우 비효율적이어서 비실용적입니다.4.2 솔루션으로서의 특징 적응 우리는 적응 중에 앞서 언급한 문제를 처리하고 더 나은 이미지 광고 이해를 위해 추가 정보(예: 장면 텍스트)를 통합할 수 있는 간단하고 직관적인 솔루션인 주의 기반 특징 어댑터를 제안합니다.특징 어댑터는 최근 VLM의 매우 효율적인 적응 전략 계열로 제안되었습니다(Gao et al., 2021; Zhang et al., 2021; Pantazis et al., 2022; Zhao et al., 2022).그들은 사전 학습된 VLM의 가중치를 동결하고, 인코더를 사용하여 특징을 사전 계산하고, 추가적인 경량 어댑터 네트워크를 사용하여 이러한 특징을 처리합니다. 결과적으로, 방대한 후보 집합에 대한 즉석 피처 계산이 계산적으로 가능해지고, 가벼운 네트워크를 통해 온라인에서 적응된 피처만 계산하기 때문에 완전한 온라인 하드 네거티브 마이닝도 가능해집니다. 보다 효율적으로, 텍스트 어댑터를 항등 함수로 설정할 수 있습니다(즉, 이미지 피처에 대한 어댑터만 사용). 더 중요한 것은, 피처 어댑터가 여러 소스의 정보를 융합하는 데 적합하다는 것입니다. 이전의 피처 어댑터는 대부분 이미지 분류를 위해 설계되었지만, 우리는 이를 잠재적으로 다른 모달리티의 여러 입력 브랜치를 집계하는 전략으로 간주합니다. 예를 들어, VS(Dey et al., 2021)와 같은 이미지 광고 이해를 위한 이전의 방법은 성능을 향상시키기 위해 이미지에서 추출한 장면 텍스트(OCR 사용)를 활용합니다. 마찬가지로, VLM의 텍스트 인코더를 사용하여 장면 텍스트에서 텍스트 피처를 추출하고 r &quot;모든 상표&quot; 클래스 독립적 객체 제안 네트워크 MAVL OCR 텍스트 매칭 BrandSet-110K 앙상블 VLM 분류기 CLIP !브레인 이름: 설명! 그림 3: 텍스트 매칭과 비전 기반 인식의 앙상블인 브랜드 이해 모듈의 예시. 입력 이미지 광고가 주어지면 MAVL을 사용하여 &quot;모든 상표&quot;를 프롬프트하여 영역을 제안하고 CLIP이 있는 영역에 대한 BrandSet-110K에서 항목을 검색합니다. 몇 가지 간단한 규칙을 통해 영역과 텍스트 매칭 결과에 대한 예측을 집계하여 최종 출력을 생성합니다(부록 B 참조). 피처 어댑터를 통해 이미지 인코더(동일한 VLM)에서 추출한 이미지 피처. 이를 통해 이미지 광고에 대한 더 나은 표현을 얻습니다. 구체적으로, TinyAttention Adapter(Zhao et al., 2022)와 유사하게 피처 어댑터 설계로 다중 헤드 어텐션(Vaswani et al., 2017)의 한 계층을 채택할 것을 제안합니다. 여기서 어텐션 계층에 대한 입력 시퀀스는 Transformers에서 일반적으로 시간적 또는 공간적으로 달라지는 대신 모달리티(그림 4와 같이 브랜드, 장면 텍스트 및 이미지)에 따라 달라집니다. 정렬 기반 VLM의 특성상 모든 정보(장면 텍스트 또는 시각적 요소와 같은 텍스트 형식)는 벡터로 내장되어 공유 의미 공간에 있습니다. 그런 다음 이 속성을 활용하여 보완 정보(예: 이미지 피처 및 장면 텍스트 피처)를 하나의 피처로 융합합니다. 또한 어텐션 피처 뒤에 선형 레이어를 추가하고 잔여 연결을 제공합니다. 이전 섹션의 표기법을 사용하고 xst를 Google OCR API에서 이미지 x에서 추출한 장면 텍스트로 표시합니다. 그런 다음 어댑터는 fatt(x) = n(f1(x) + A[ƒ1(x), ƒT(xst), ...][0])로 표현됩니다. 여기서 n()은 정규화 함수이고 A는 멀티헤드 어텐션입니다(여기에 &quot;...&quot;를 남겨두어 다른 입력 분기를 위한 공간을 남겨둡니다). 이미지의 텍스트 설명(이미지 광고의 레이블)에 어댑터를 사용하지 않는다는 점에 유의하세요. 이는 전체 HNM 동안 한 번에 모든 텍스트 피처를 계산하고 캐시하기만 하면 되므로 계산 복잡성을 더욱 줄입니다. 비교를 위해 인기 있는 CLIP-Adapter(Gao et al., 2021)도 강력한 기준으로 평가하고, 3개의 2계층 잔여 MLP를 훈련하여 설정에 맞게 조정합니다. 구현 세부 정보는 부록을 참조하세요. 표 3에 보고된 대로, 주의 기반 어댑터(KAFA w/o K로 표시)를 사용하는 제안은 동일한 의미 공간에 이미 있는 멀티모달 피처를 정렬하여 VLM 피처를 잘 활용하고 CLIPAdapter보다 성능이 우수합니다. 다른 기존 작업(Shen et al., 2021; Gui et al., 2021)은 기초 모델을 활용하여 여러 정보 분기를 병합하지만, 계산 집약적인 대규모 인코더-디코더 네트워크에 의존하고 우리의 경우처럼 제한된 훈련 데이터에서는 제대로 작동하지 않을 수 있습니다. 5 외부 지식을 통한 이미지 광고 이해 개선 이미지 광고 이해도를 더욱 개선하기 위해 제품 및 브랜드 정보와 같은 실제 엔터티에 대한 외부 지식을 활용할 것을 제안합니다. 광고의 주요 초점은 브랜드 인지도를 높이는 것입니다(Macdonald et al., 2003). 때로는 브랜드 정보가 광고를 올바르게 해석하는 데 필수적이기도 합니다. 브랜드 정보는 모호성을 제거하고 청중에게 시각적 단서를 제공하기 때문입니다(예: 그림 2의 청소 제품 광고). 그런 다음 이전에 도입한 기능 어댑터에 이미지에서 브랜드 정보를 추출하는 브랜드 이해 모듈을 제공하는 것이 자연스럽습니다. 여기에서는 VLM을 상당히 활용하는 학습이 필요 없는 브랜드 이해 모듈을 소개합니다. 5.1 브랜드 이해 모듈 매우 실제 세계에서 브랜드의 규모가 너무 커서 이미지에서 브랜드 정보를 추출하는 것은 어렵습니다. 기존에 발표된 연구(Su et al., 2018; Li et al., 2022a)와 상용 API조차도 적절한 적용 범위에 미치지 못하는 경향이 있습니다. 이 문제를 해결하려면 브랜드 모듈 VLM 텍스트 인코더 OCR VLM 텍스트 인코더 | 주의 기반 특징 어댑터 대조적 미세 조정 VLM 이미지 인코더 CLIP 조정된 KAFA 구성 요소 잠긴 VLM 인코더 VLM 텍스트 인코더 텍스트 CLIP 그림 4: 제안하는 KAFA의 전체 학습 파이프라인. 여기서 정보의 세 가지 분기가 미세 조정 프로세스에서 자유로운 유일한 신경 모듈인 주의 기반 특징 어댑터에 공급됩니다. 대조적 미세 조정의 양쪽에 VLM 인코더를 활용합니다. 문제에서 기존 데이터 세트보다 훨씬 더 나은 브랜드를 포함하는 지식 기반을 구성합니다. 지식 기반은 KFC 형식을 갖습니다. KFC는 패스트푸드 체인으로, 이미지 광고에 나타나는 브랜드, 회사, 조직 및 기타 이름을 포함하는 약 110,000개의 항목이 있습니다. 이 데이터 세트를 BrandSet-110K라고 합니다(부록 B에서 자세한 내용을 참조하세요). 다음으로, 이미지 광고가 주어진 경우 BrandSet110K에서 관련 브랜드 항목을 감지하고 검색하기 위해 앙상블 방식을 취합니다. 한편, OCR에서 이미지에서 추출한 장면 텍스트를 사용하여 BandSet-110K의 모든 이름에 대한 문자열 매칭을 수행하여 브랜드를 검색합니다. 반면, OCR 실패, 감지 안 됨(일부 로고에 텍스트 없음) 또는 여러 항목이 감지된 경우(대부분 이미지 광고가 한 번에 하나의 브랜드만 홍보하므로 잠재적으로 거짓 양성) 더 강력한 비전 기반 모듈을 사용합니다. 구체적으로, 최첨단 VLM인 MAVL(Maaz et al., 2022)을 채택하여 텍스트 프롬프트 &quot;모든 상표&quot;에 따라 개체 영역을 제안합니다. 그런 다음 최상의 CLIP 모델을 사용하여 신중하게 설계된 프롬프트 집합을 기반으로 영역 분류를 수행합니다. 그런 다음 제안된 영역에 따라 BrandSet-110K에서 최상의 항목을 선택합니다. 마지막으로 그림 3과 같이 텍스트 매칭과 비전 기반 모듈에서 검색된 결과를 결합하기 위해 몇 가지 간단한 규칙을 사용합니다(부록에서 자세한 내용 참조). 전반적으로, 우리의 브랜드 이해 모듈은 훈련이 필요 없고, 이전에 발표된 작업보다 훨씬 더 많은 엔터티를 포괄하며, 표 5.2에서 보고된 것처럼 작은 검증 세트에 대한 평가에서 일부 상업용 로고 감지 API보다 성능이 뛰어납니다. 전체 파이프라인 및 최종 결과 제안된 브랜드 이해 모듈과 결합하여 그림에서 전체 파이프라인을 설명하고 이 접근 방식을 지식 증강 기능 적응(KAFA)이라고 합니다. 표에서 3, 우리는 KAFA가 VLM 기준선 방법 입력 100 500 제로샷 I 직접 FT + 전체 HNM 클립 어댑터 I I+ST 91.7 80.7 64.4 56.93.7 84.6 70.0 62.93.9 85.0 70.2 62.I+ST 95.0 86.8 72.7 65. KAFA w/o K KAFA w/o ST KAFA(저희) I+K 94.7 86.5 72.3 64.I+ST+K 95.6 87.7 73.9 66. 표 3: Pitt 데이터 세트에서 보고된 정확도(%). KAFA(외부 지식을 활용한 주의 기반 어댑터)는 다른 접근 방식이나 입력이 적은 버전(K = 브랜드 지식, ST 장면 텍스트, I = 이미지)과 비교했을 때 가장 좋은 결과를 달성합니다. 참고: &quot;직접 FT + 전체 HN&quot;은 매우 비효율적입니다. VLM 기반(ALBEF) VLM 기반(LIT) VLM 기반(CLIP) = Acc(%) Acc(%) 14.29.64. 텍스트 매칭 Google Cloud API 결합(텍스트 + CLIP) 36.42.66. 표 4: ~600개의 검증 이미지 광고에 대한 브랜드 인지 정확도. 이는 브랜드 이해 모듈을 정당화하고 브랜드를 더 잘 인식하는 모델이 이미지 광고 이해도도 더 뛰어나다는 것을 더욱 검증합니다. 그리고 더 적은 입력으로 다른 절제 버전보다 지속적으로 우수한 성과를 보이며, 제안하는 브랜드 이해 모듈이 이미지 광고 이해도를 더욱 개선하는 데 도움이 된다는 것을 정당화합니다. 그림 1에 기준을 통한 방법 개선을 설명하는 예를 제시합니다. 더 나은 표시를 위해 부정적인 텍스트 설명 2개만 표시합니다. 부록 G에서 더 많은 예를 확인하세요. 6 추가 분석 6.1 평가에서의 하드 네거티브 샘플 공식 프로토콜보다 더 어려운 평가 프로토콜을 사용하여 주요 결과를 보고합니다. 사실, 수행하는 것이 어려운 검색 작업에서 효과적인 평가(Akula et al., 2020). 일반적으로 후보 집합의 크기를 늘려 모델의 기능을 더 잘 반영하기 위해 하드 네거티브가 필요하지만, 이러한 하드 네거티브가 실제 네거티브가 되기를 원합니다.그림 5(오른쪽)에서 볼 수 있듯이 두 회사는 매우 유사한 확률 밀도를 공유하는 두 가지 다른 이미지 광고를 가질 수 있습니다.이미지 간 평균 유사도 가이드 이미지 내 평균 유사도 겨울 운전 로고 X 브랜드 로고 Y 출처 평가 이미지 경고 소비로 인해 NAN 에너지 폭발이 발생할 수 있음 브랜드 로고 YA LAION-0.2에서 발견된 유사한 이미지 0.0.2 0.4 0.6 0.8 1.코사인 유사도 [-1, 1] 자동차 로고 Y 그림 5: (왼쪽) 동일한 텍스트와 이미지 간 텍스트의 유사도 분포. 둘 다 하드 네거티브를 샘플링하기 위한 쉬운 차단 임계값 없이 퍼져 있습니다. (오른쪽) 두 개의 서로 다른 광고가 &quot;어디서나 운전할 수 있기 때문에 이 차를 운전해야겠다&quot;는 동일한 메시지를 공유하며, 이는 하드 네거티브 샘플을 샘플링하는 것이 어려운 것을 보여줍니다. 메시지. 따라서 이미지가 주어졌을 때 다른 이미지의 텍스트를 단순히 네거티브로 사용하는 것은 효과가 없을 수 있습니다. 쉬운 해결책은 없습니다. 우리는 (Hussain et al., 2017)에서 다른 텍스트 간의 유사성을 측정하기 위해 일반 문장 인코더를 사용할 수 있으며, 타겟 텍스트(기본 진실)와 의미적으로 다른 텍스트만 네거티브로 샘플링할 수 있습니다. 우리는 MiniLM(Wang et al., 2020)을 기반으로 하는 강력한 문장 인코더(여기에서 공개적으로 사용 가능)를 채택하여 의미적 유사성을 측정합니다. 우리는 같은 광고에 대한 설명과 다른 광고에 대한 설명 간의 유사성을 계산합니다. 유사성 분포는 그림 5(왼쪽)에서 볼 수 있듯이 확산되어 있으며, 네거티브 샘플을 하드 샘플과 진정한 네거티브 샘플로 만드는 쉬운 차단 임계값이 없습니다. 대신, 우리는 K = 20, 100, 500, 1000인 후보 세트의 여러 다른 크기 K를 사용하는 것을 제안합니다. Pitt 데이터 세트(Hussain et al., 2017)의 각 이미지에 대해, 우리는 무작위로 기준 진실에서 텍스트를 선택하고 다른 이미지에서 K 1 부정(더 큰 K를 가진 더 어려운 부정)을 균일하게 샘플링합니다. = _ 대부분의 기존 방법은 공식 평가 프로토콜로 평가(Hussain et al., 2017)하는 반면(비교의 용이성을 위해 표 1에 이 프로토콜에 의한 결과도 제공함), 하드 부정이 부족하다는 단점이 있습니다. 각 이미지 광고에는 긍정을 포함하여 무작위로 샘플링된 후보 텍스트가 15개만 포함되어 있어 무작위 모델에 20%의 정확도를 제공합니다. 더욱이, 부정은 긍정과 의미적으로 구별되는 경향이 있으므로 쉬워서 모델을 더 세부적으로 조사하기 어렵습니다. 부록 E에 있는 공식 프로토콜과 우리 프로토콜에서 샘플링된 부정을 비교하기 위한 예를 제공합니다. 6.2 VLM과 관련된 데이터 누출 우리가 사용하는 CLIP(Radford et al., 2021) 모델
--- EXPERIMENT ---
s는 엄청난 양의 그림 6: 평가 이미지와 LAION-400M에서 발견된 이미지에 대해 사전 학습되었습니다. 참고로, 이 이미지의 캡션은 다음과 같습니다. 에너지를 충전할 수 있기 때문에 &quot;브랜드 이름&quot;을 마셔야겠어요. 인터넷에는 400M개의 이미지-텍스트 쌍이 있습니다. 우려되는 점은 데이터 유출이 있을 수 있다는 것입니다. 즉, 사전 훈련된 VLM이 이미 평가 세트에서 이미지를 보았을 수 있으며, 이로 인해 결과가 부풀려질 수 있습니다. 분석을 수행하여 이런 일이 일어날 가능성이 낮다는 결론을 내렸습니다. 무작위로 샘플링한 100개의 평가 이미지-텍스트 쌍 세트와 의미적으로 유사한 LAION-400M 데이터 세트(Schuhmann et al., 2021)의 이미지를 수동으로 검사했습니다. CLIP을 훈련하는 데 사용된 데이터 세트는 공개되지 않았지만, LAION-400M은 CLIP 모델에서 필터링한 데이터 규모가 비슷한 매우 유사한 데이터 세트입니다. 구체적으로, 100개의 무작위 샘플 각각에 대해 오픈 소스 CLIP 검색 도구(여기)를 사용하여 LAION-400M에서 가장 가까운 이미지를 찾습니다. 샘플 텍스트와 이미지 둘 다. 우리는 실질적으로 중복된 콘텐츠나 거의 중복된 내용을 찾지 못했습니다(예시로 그림 6 참조). 게다가, 제안된 방법은 VLM 기준선에 비해 상당한 성능 향상을 이루었고, 둘 다 동일한 CLIP 모델을 기반으로 합니다. 따라서 데이터 누출은 그다지 문제가 되지 않습니다. 7
--- CONCLUSION ---
이 논문에서 우리는 까다로운 이미지 광고 이해 과제를 위해 사전 학습된 정렬 기반 VLM의 적응을 연구합니다. 우리는 VLM을 적응시키는 데 있어서 실질적인 과제를 벤치마킹하고 밝히고, 특징 적응을 위한 간단하고 직관적인(하지만 효과적인) 전략을 제안하고, 외부 브랜드 지식으로 이미지 광고 이해도를 더욱 개선합니다. 우리는 주로 단순성 때문에 이미지-텍스트 검색 과제에 집중하지만, 추가 연구를 통해 이미지 광고가 주어진 텍스트 설명을 직접 생성하거나 설명이 주어진 이미지 광고를 생성하는 것으로 확장할 수 있다고 믿습니다. 우리는 우리의 연구가 광고 산업과 관련이 있고 더 광범위한 머신 러닝 커뮤니티에 통찰력을 제공하는 이미지 광고 이해에 더 많은 관심을 끌기를 바랍니다. 한계점 Pitt 데이터 세트(Hussain et al., 2017)의 데이터는 우리 논문에 유용하지만, 성별과 같은 민감한 특성에 따라 유해한 고정관념을 영속시킬 수 있는 많은 이미지와 주석을 포함하고 있으며 머신 러닝 모델에 의한 증폭의 위험을 안고 있습니다. 우리는 AI 견고성 연구자들과 협력하여 이러한 사례를 식별하고 견고성과 신뢰성 측면에서 ML 모델을 개선하기 위한 방법을 개발할 계획입니다. 참고문헌 Reneh Abokhoza, Sherehan Hamdalla Mohamed, Sumit Narula. 2019. 광고가 문화와 가치를 반영하는 방식: 정성적 분석 연구. Journal of Content, Community and Communication, 10(9):3. Arjun Akula, Spandana Gella, Yaser Al-Onaizan, Songchun Zhu, Siva Reddy. 2020. 단어만으로는 충분하지 않다. 단어의 순서가 중요하다. 시각적 참조 표현을 근거로 하는 견고성에 관하여. Association for Computational Linguistics의 제58회 연례 회의록, 6555-6565쪽. Arjun R Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo, Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas, William T Freeman, et al. 2022. Metaclue: 포괄적인 시각적 은유 연구를 향하여. arXiv 사전 인쇄본 arXiv:2212.09898. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. 2022. Flamingo: few-shot 학습을 위한 시각적 언어 모델. arXiv 사전 인쇄본 arXiv:2204.14198. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901. Lydia B Chilton, Savvas Petridis, Maneesh Agrawala. 2019. Visiblends: 시각적 혼합을 위한 유연한 워크플로. 2019년 CHI 컴퓨팅 시스템의 인간적 요인 컨퍼런스 회의록, 1-14페이지. Arka Ujjal Dey, Suman K Ghosh, Ernest Valveny, Gaurav Harit. 2021. 시각적 의미론 너머: 이미지 이해에서 장면 텍스트의 역할 탐구. Pattern Recognition Letters, 149:164–171. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao. 2021. Clip-adapter: 기능 어댑터를 사용한 더 나은 시각 언어 모델. arXiv 사전 인쇄본 arXiv:2110.04544. François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. 2020. Conceptbert: 시각적 질의응답을 위한 개념 인식 표현. EMNLP 2020 협회의 연구 결과: 489-498쪽. Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat: 시각과 언어를 위한 지식 증강 변환기. arXiv 사전 인쇄본 arXiv:2112.08614. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. 비지도 시각적 표현 학습을 위한 모멘텀 대비. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9729-9738쪽. Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong 및 Adriana Kovashka. 2017. 이미지 및 동영상 광고 자동 이해. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 컨퍼런스 진행, 1705-1715페이지. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li 및 Tom Duerig. 2021. 시끄러운 텍스트 감독을 통해 시각적 및 비전 언어 표현 학습을 확장합니다. 기계 학습에 관한 국제 회의, 4904-4916페이지. PMLR. Kanika Kalra, Bhargav Kurma, Silpa Vadakkeeveetil Sreelatha, Manasi Patwardhan 및 Shirish Karande. 2020. bert를 이용한 광고 이해. Association for Computational Linguistics의 제58회 연례 회의록, 75427547쪽. 김수영, 박현진, 신경규용, 김경민. 2022. Ask me what you need: Product retrieval using knowledge from gpt-3. arXiv 사전 인쇄본 arXiv:2207.02516. Diederik P Kingma와 Jimmy Ba. 2014. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980. Chenge Li, István Fehérvári, Xiaonan Zhao, Ives Macedo, Srikar Appalaraju. 2022a. Seetek: 텍스트 인식 메트릭 학습을 통한 매우 대규모 오픈 세트 로고 인식. IEEE/CVF Winter Conference on Applications of Computer Vision의 진행 사항, 2544-2553쪽. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu Hong Hoi. 2021. 융합 전 정렬: 모멘텀 증류를 통한 비전 및 언어 표현 학습. 신경 정보 처리 시스템의 발전, 34:9694-9705. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. 2022b. 접지된 언어-이미지 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1096510975페이지. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee. 2019. Vilbert: 비전 및 언어 작업을 위한 작업 독립적 시각 언어 표현 사전 학습. 신경 정보 처리 시스템의 발전, 32. Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, Ming-Hsuan Yang. 2022. 다중 모드 변환기를 사용한 클래스 독립적 객체 감지. 유럽 컴퓨터 비전 컨퍼런스에서. Springer. Emma Macdonald, Byron Sharp, et al. 2003. 광고 효과의 지표로서 브랜드 인지도의 중요성에 대한 경영진의 인식. Massey University, 마케팅학과 박사 학위 논문. Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. 2021. Krisp: Integrating implicit and symbolic knowledge for opendomain knowledge-based vqa. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 14111–14121쪽. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: 외부 지식이 필요한 시각적 질문 답변 벤치마크. IEEE/cvf Conference on Computer Vision and Pattern Recognition의 회의록, 3195-3204쪽. Omiros Pantazis, Gabriel Brostow, Kate Jones, and Oisin Mac Aodha. 2022. Svl-adapter: 비전 언어 사전 학습 모델을 위한 자체 감독 어댑터. arXiv 사전 인쇄본 arXiv:2210.03794. Savvas Petridis와 Lydia B Chilton. 2019. 시각적 은유를 해석하는 데 있어서의 인간적 오류. 2019년 창의성과 인지에 관한 회의록, 187-197쪽. Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. 2021. Openvocabulary 이미지 분류를 위한 결합된 스케일링. arXiv 사전 인쇄본 arXiv: 2111.10050. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 국제 기계 학습 컨퍼런스, 8748-8763페이지. PMLR. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet 대규모 시각적 인식 챌린지. 국제 컴퓨터 비전 저널, 115(3):211-252. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: 차세대 이미지 텍스트 모델을 학습하기 위한 개방형 대규모 데이터 세트. arXiv 사전 인쇄본 arXiv:2210.08402. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: 클립필터링된 4억 개의 이미지-텍스트 쌍의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2111.02114. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. 클립은 시각 및 언어 작업에 얼마나 도움이 될 수 있을까? arXiv 사전 인쇄본 arXiv:2107.06383. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. 읽을 수 있는 vqa 모델을 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8317-8326쪽. Haoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, Furu Wei. 2022. 클립 모델은 few-shot 학습자입니다: vqa와 시각적 함의에 대한 경험적 연구. arXiv 사전 인쇄본 arXiv:2203.07190. Hang Su, Shaogang Gong, Xiatian Zhu. 2018. 확장 가능한 딥 러닝 로고 감지. arXiv 사전 인쇄본 arXiv:1803.11417. Yi-Lin Sung, Jaemin Cho, Mohit Bansal. 2022. Vl-adapter: 시각 및 언어 작업을 위한 매개변수 효율적 전이 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5227-5237쪽. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30. Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, Anton Van Den Hengel. 2017. Fvqa: 사실 기반 시각적 질의 응답. IEEE 패턴 분석 및 머신 인텔리전스 거래, 40(10):24132427. Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick. 2015. 시각적 질의 응답을 위한 명시적 지식 기반 추론. arXiv 사전 인쇄본 arXiv:1511.02570. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: 사전 훈련된 트랜스포머의 작업 독립적 압축을 위한 심층적 자기 주의 증류. 신경 정보 처리 시스템의 발전, 33:5776-5788. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface의 트랜스포머: 최첨단 자연어 처리. arXiv 사전 인쇄본 arXiv:1910.03771. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. 비모수적 인스턴스 구별을 통한 비지도 특징 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3733-3742쪽. Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless. 2020. 하드 네거티브 예는 어렵지만 유용합니다. European Conference on Computer Vision, 126-142쪽. Springer. Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun. 2021. Cpt: 사전 학습된 시각 언어 모델을 위한 다채로운 프롬프트 튜닝. arXiv 사전 인쇄본 arXiv:2109.11797. Keren Ye와 Adriana Kovashka. 2018. Advise: 광고 디코딩을 위한 상징주의와 외부 지식. European Conference on Computer Vision(ECCV) 회의록, 837-855쪽. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier. 2014. 이미지 설명에서 시각적 의미로: 이벤트 설명에 대한 의미 추론을 위한 새로운 유사성 메트릭. Association for Computational Linguistics의 거래, 2:67–78. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu. 2022. Coca: 대조 캡셔너는 이미지-텍스트 기반 모델입니다. arXiv 사전 인쇄본 arXiv:2205.01917. Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: 컴퓨터 비전을 위한 새로운 기반 모델. arXiv 사전 인쇄본 arXiv:2111.11432. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022. Lit: 잠긴 이미지 텍스트 튜닝을 사용한 제로 샷 전송. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18123-18133쪽. Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. 2021. Tip-adapter: 더 나은 시각-언어 모델링을 위한 훈련 없는 클립 어댑터. arXiv 사전 인쇄본 arXiv:2111.03930. Hongyu Zhao, Hao Tan, and Hongyuan Mei. 2022. Tiny-attention 어댑터: 컨텍스트는 매개변수 수보다 더 중요합니다.arXiv 사전 인쇄본 arXiv:2211.01979. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu. 2022a. 시각 언어 모델을 위한 조건부 프롬프트 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16816-16825페이지. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu. 2022b. 시각 언어 모델을 위한 프롬프트 학습. International Journal of Computer Vision, 130(9):2337-2348. OCR을 통한 장면 텍스트 추출 저희 논문에서는 Pitt 데이터 세트(Hussain et al., 2017)의 실험을 위한 입력 중 하나로 장면 텍스트를 사용합니다. Google Cloud OCR API(링크)를 사용하여 API에 의해 문단별로 그룹화된 모든 텍스트 토큰을 추출합니다. 그런 다음 간단한 휴리스틱 규칙(예: 글꼴 크기가 비슷한 두 개의 연속된 문단은 동일한 블록에서 고려해야 함)에 따라 문단을 블록으로 그룹화한 다음 API에서 제공하는 평균 예측 신뢰도 점수가 0.7 미만인 블록을 필터링합니다. B B. 브랜드 인지 BrandSet-110K 먼저 공개 웹사이트에서 항목을 컴파일하여 BrandSet-110K를 구성합니다. 구체적으로 Pitt Dataset(Hussain et al., 2017)의 주제 목록(자동차 및 의료 등)의 경우 Google에서 &quot;Y의 상위 XX개 브랜드/회사&quot;라는 쿼리로 검색하여 수천 개의 일반적인 브랜드, 조직 등의 목록을 얻습니다(소스 I). Google Knowledge Graph Search API(링크)를 더 스크래핑하여 &quot;브랜드&quot;, &quot;회사&quot; 등의 범주에 속하는 훨씬 더 큰 명명된 엔터티 목록을 찾습니다(소스 II). 각 항목에는 한 단락 설명이 포함됩니다. Knowledge Graph(KG)의 결과는 약간 노이즈가 많고 일부 인기 있는 엔터티를 놓칠 수 있으므로 소스 I에 의존하여 상업 세계에서 가장 널리 나타나는 엔터티가 데이터 세트에 포함되도록 합니다. 그런 다음 소스 I에서 KG의 항목을 쿼리하여 설명도 얻습니다. KG에서 이러한 항목을 찾을 수 없으면 &quot;X는 Y 산업의 브랜드 이름입니다&quot;라는 설명을 사용합니다. 소스 II와 함께 원시 결합 지식 기반을 얻습니다. 그런 다음 영어: 일반적인 영어 단어인 항목을 필터링합니다(항목이 영어 사전에 나타나는 경우(링크) 또는 NLTK의 단어 세트인 경우(링크)). 이렇게 하는 이유는 브랜드 감지 중에 너무 많은 거짓 양성 반응이 발생하는 &quot;Everyday&quot;와 같은 항목을 제거하기 위해서입니다. 또한 단일 문자로 구성된 항목도 제거합니다. 결국 약 110K 항목, 즉 이름과 설명 쌍이 생깁니다. KG에서 반환한 설명은 매우 길 수 있으므로 학습 기반 문장 파서를 사용하여 설명의 첫 번째 문장만 선택합니다(일반적으로 &quot;X는 Z 기능을 가진 Y 산업의 브랜드/회사/조직입니다&quot; 형식). spaCy를 기반으로 하는 Hugging Face(Wolf et al., 2019)의 이 API(링크)를 사용합니다. B.2 텍스트 매칭을 통한 브랜드 인식 텍스트 기반 브랜드 인식 모듈은 기본적으로 텍스트 매칭을 수행하여 OCR에서 추출한 장면 텍스트를 고려하여 BrandSet-110K의 모든 항목을 철저히 검색합니다. BrandSet-110K에서 6자보다 큰 각 이름에 대해 대소문자를 구분하지 않고 텍스트를 일치시킵니다. 그렇지 않으면 거짓 양성을 줄이기 위해 대소문자를 구분하여 일치시킵니다. 텍스트의 구문인 경우 이름이 scenetext에서 일치되도록 설정됩니다(&quot;abc&quot;는 &quot;abc def&quot;에서 일치하지만 &quot;abcdef&quot;에서는 일치하지 않음). 텍스트 일치 성능만 평가하는 절제 연구를 수행할 때 여러 예측이 있는 경우 출력으로 무작위로 하나를 선택합니다. B.3 비전 기반 브랜드 인식 비전 기반 브랜드 인식 모듈은 텍스트 기반 모듈이 실패하는 상황(텍스트가 너무 작거나 흐릿하거나 예술적이어서 OCR이 작동하지 않는 경우, 또는 로고가 순전히 그래픽인 경우)을 처리합니다. 비전 기반 모듈은 여러 단계로 구성된 파이프라인입니다. 클래스에 구애받지 않는 지역 제안(MAVL(Maaz et al., 2022)에서 가장 뛰어난 모델을 사용, 최신 모델)은 브랜드 로고나 브랜드 정보를 보여주는 시각적 요소가 포함된 후보 지역을 생성하는 데 채택되었습니다. 우리는 다음과 같은 다른 후보들과 함께 &quot;모든 상표&quot;를 최상의 프롬프트로 선택합니다. • &quot;모든 소형 물체&quot;, &quot;모든 브랜드 로고&quot;, • &quot;모든 브랜드 아이콘&quot;, &quot;모든 브랜드&quot;, &quot;모든 로고&quot; 영역 제안 후, 우리는 최상의 CLIP(Radford et al., 2021) 모델(시각적 인코더)을 사용하여 영역 피처를 계산합니다. 우리는 전체 이미지를 추가 제안 영역으로 포함합니다. 그런 다음, 다음 6개 프롬프트의 텍스트 피처(CLIP 텍스트 인코더를 통해)를 사용하여 BrandSet-110K에서 최상의 항목을 찾습니다. 즉, • &quot;X의 브랜드 로고&quot;, &quot;X의 로고&quot;, • &quot;X의 상표&quot;, &quot;X. Y의 브랜드 로고&quot;, • &quot;X. Y의 로고&quot;, &quot;X. Y의 상표&quot; 여기서 X는 이름이고 Y는 BrandSet-110K의 해당 설명입니다. 우리는 먼저 모든 6개 프롬프트에서 영역 피처와 브랜드 피처의 점곱을 평균합니다. 그런 다음 두 가지 후보를 찾습니다. (1) 다음 중 예측 점수가 가장 높은 이름 X 영어: 모든 이름과 이미지의 모든 영역 및 (2) 적어도 한 지역에서 챔피언인 모든 이름 중에서 모든 지역에서 평균화된 가장 큰 예측 점수를 가진 이름 X. 최종 출력은 글로벌 이미지 피처의 점곱과 프롬프트 &quot;X의 광고&quot;의 두 텍스트 피처 중 더 높은 값에 의해 선택됩니다(다른 사소한 프롬프트 엔지니어링 프로세스 후에 이 프롬프트를 선택함).B.4 텍스트 매칭과 비전 기반 브랜드 인식의 앙상블 간단한 휴리스틱 규칙을 사용하여 텍스트 매칭 결과와 비전 기반 결과를 앙상블합니다.특히, 텍스트 매칭에서 감지된 이름이 없으면 비전 기반 결과를 반환합니다.텍스트 매칭에서 감지된 이름이 하나만 있으면 텍스트 기반 결과를 반환합니다. 영어: 텍스트 매칭에서 두 개 이상의 이름이 감지되면 글로벌 이미지 피처와 &quot;X의 광고&quot;의 텍스트 피처의 점곱의 가장 높은 값으로 텍스트 및 비전 기반 모듈을 모두 감지하여 이름을 선택합니다. 앙상블 모듈은 마지막으로 단일 이름과 해당 설명을 BrandSet-110K에 반환합니다. C 주의 기반 피처 어댑터의 네트워크 아키텍처 피처 적응을 위해 매우 가벼운 네트워크를 채택합니다. 입력의 각 모달리티(예: Pitt 데이터 세트의 KAFA에 대한 입력은 장면 텍스트 피처, 이미지 피처 및 브랜드 피처의 세 가지 벡터임)에 대해 먼저 학습 가능한 위치 임베딩(다른 모달리티를 구별하는 데 사용됨)을 추가한 다음 다중 헤드 주의 계층(Vaswani et al., 2017)을 적용하여 벡터 목록을 얻습니다. 마지막으로 첫 번째 벡터(이미지 피처 입력 분기에 해당)를 사용하고 입력 이미지 피처(위치 임베딩 이전)에서 잔차 연결을 추가하여 최종 출력(정규화 포함)을 생성합니다. 더 명확하게 하기 위해 의사코드도 제공합니다. import torch.nn. Parameter as param import torch.nn. functional as F # args는 입력 기능의 목록입니다. # 예: [img_fs, scene_text_fs, brand_fs] pos_emb_list for attn = [ ] in range(n_input): pos_emb_list.append( param (torch.zeros ([input_d]))) = torch.nn.Multihead Attention ( embed_dim=input_d, num_heads=8, batch first=True) inputs = [ ] for i in range (n_input): inputs append( args[i] + pos_emb_list [i]) x = torch stack (inputs, 1) x, _ = attn (x, x, x, need_weights=False) # 첫 번째는 이미지 기능입니다. x = x[:, 0] + args[0] x = F. normalize (x, dim=-1) D Pitt 데이터 세트의 데이터 정리 Pitt 데이터 세트의 훈련 및 평가 데이터에서 데이터 정리를 수행합니다(주 논문에서 문제가 논의되는 공식 평가 프로토콜을 사용하여 평가하는 경우 원시 평가 세트를 고수합니다).데이터 세트의 모든 텍스트(&quot;광고에 따르면 어떻게 해야 하며 그 이유는?&quot; 질문에 대한 응답)에 대해 잘못된 텍스트(예: &quot;모르겠습니다&quot;, &quot;광고가 아닙니다&quot;, &quot;잘 모르겠습니다&quot;)를 제거하고 오타(예: &quot;becasue&quot;, &quot;becaus&quot;)를 수정하고 &quot;why&quot; 질문에 답하지 않는 텍스트를 제거합니다. 또한 명사를 언급하지 않거나 그다지 유익하지 않은 명사만 있는 텍스트를 필터링합니다(데이터 세트에 자주 등장하는 &quot;제품&quot;, &quot;사물&quot;, &quot;공급업체&quot;와 같은 유익하지 않은 명사 목록을 컴파일합니다). 이 단계는 &quot;이 제품을 사야 하는 이유는...&quot;과 같은 비특정 텍스트를 제거하는 것입니다. 마지막으로, 우리는 무작위로 하나의 텍스트(고정된 난수 시드 사용)를 이미지의 기준 진실로 선택합니다. 데이터 정리를 통해 이미지의 모든 텍스트가 제거된 경우 데이터 세트에서 이미지를 제거합니다. 이러한 이미지가 모든 이미지의 3% 미만을 구성하는 것을 발견했습니다. E Pitt 데이터 세트에서 평가를 위한 하드 부정 대 이지 부정 여기서는 평가 중에 더 많은 수의 후보 K를 사용하는 이유를 설명합니다. 교차 모달 검색을 위한 모델 평가는 어렵습니다(Akula et al., 2020). Pitt 데이터 세트의 공식 평가 프로토콜은 모델의 지각 및 추론 능력을 완전히 반영할 하드 부정이 부족하다는 문제가 있습니다. 프로토콜의 각 이미지에는 긍정 텍스트가 3개 있고 부정 텍스트는 12개만 있어 무작위 추측 모델의 정확도는 20%입니다. 반대로, 주요 논문에서 소개한 대로 평가 프로토콜에서 후보 수를 늘리면 효과적으로 하드 부정이 생성됩니다. 예를 들어, 그림 7의 이미지 광고에서 기준 진실이 &quot;창작에 도움이 될 것이므로 브랜드 A 카메라를 사야겠다&quot;인 경우 후보 수를 10개(즉, 부정)로 설정하면 최상의 CLIP 모델은 모든 쉬운 부정을 사용하여 올바른 선택을 합니다.가장 혼란스러운 것은 다음과 같습니다.• &quot;나이가 빠지기 때문에 브랜드 B를 마셔야겠다&quot;• &quot;나쁜 선택은 당신을 죽음에 이르게 할 것이기 때문에 결정에 빠져서는 안 된다&quot;• &quot;봉인이 가능하기 때문에 이 가방을 사야겠다&quot; 총 후보 수를 50개(즉, 부정)로 설정하면 다시 CLIP 기준선은 가장 혼란스러운 것이 여전히 비교적 쉬운 부정인 경우에도 올바르게 예측합니다.• &quot;당신을 아름답게 만들어 주기 때문에 브랜드 C 화장품을 사용해야겠다&quot;• &quot;신뢰할 수 있기 때문에 브랜드 D 제품을 사야겠다&quot;• &quot;재밌기 때문에 영화를 봐야겠다&quot; 더 많은 수(예: 총 후보 수 100개)의 경우 CLIP 모델은 다음과 같은 하드 부정에서 실수를 하기 시작합니다. seductive&quot; • &quot;나는 브랜드 F 화장품을 사야겠다. 왜냐하면 그것이 나를 아름답게 만들어 줄 테니까&quot; • &quot;나는 이 화장품을 사야겠다. 왜냐하면 그것이 나를 빛나게 만들어 줄 테니까&quot; 개인정보 보호를 위해 이 예에서 모든 브랜드 이름은 익명으로 처리되었습니다.F 학습 세부 정보 F.1 CLIP의 직접 미세 조정 배치 크기 8, 대칭 교차 엔트로피 손실(CLIP의 원래 논문에서 사용한 것) 및 가중치 감소 1e-4인 Adam 옵티마이저(Kingma 및 Ba, 2014)를 사용하여 Pitt 데이터 세트의 학습 이미지에서 최상의 CLIP 모델을 미세 조정합니다. CLIP의 원래 구현에서와 같이 Adam의 다른 매개변수를 설정합니다. Pitt 데이터 세트에서 CLIP을 미세 조정하려면 매우 작은 학습률(예: le - 7)을 사용해야 한다는 것을 알았습니다. 그렇지 않으면 CLIP 모델이 쉽게 과적합될 수 있습니다. 같은 이유로 조기 중단을 채택하고 최대 4개 에포크 동안만 모델을 미세 조정합니다. 다음 섹션에서 온라인 하드 네거티브 마이닝(본 논문에서 제안한 대로 매우 계산 집약적)을 사용한 미세 조정 버전에 대한 세부 정보를 제공합니다.C 은닉 사용 안 함 6) FAW WILLENZE가 있는 경우 D 브랜드 로고 그림 7: 평가에서 쉬운 네거티브 샘플 문제를 설명하는 예.F.2 완전 온라인 하드 네거티브 마이닝(전체 HNM) 훈련 중에 하드 네거티브 마이닝을 수행할 때 미니 배치의 각 이미지에 대해 먼저 많은 수의 무작위로 샘플링된 네거티브 텍스트의 VLM 특징을 계산합니다(실험에서 1000개가 충분히 큰 것으로 나타났지만 더 큰 수는 최종 성능을 약간 향상시킬 수 있지만 더 큰 계산 부담이 발생합니다).그런 다음 현재 이미지 특징과 모든 샘플링된 텍스트 특징의 점곱을 계산하고 마지막으로 점곱을 순위를 매기고 손실의 그래디언트 계산에 포함할 상위 N 1개의 네거티브를 선택합니다(N = 8이 효과적임). HNM을 채택할 때 이미지당 부정의 수가 배치 크기와 같지 않기 때문에 CLIP 사전 학습에서 비대칭 버전과 비교되는 교차 엔트로피 손실(즉, 정상 손실)의 비대칭 버전을 사용합니다. 온라인 HNM을 사용할 때마다 배치 크기를 4로 줄여 단일 V100 Nvidia GPU로 가장 큰 CLIP 모델을 직접 미세 조정할 수 있습니다. 우리는 여전히 CLIP 사전 학습에서 학습 가능한 &quot;로짓 스케일&quot; 매개변수를 적용하는데, 이는 효과적으로 대조 학습을 보다 안정적으로 만듭니다. 전체 HNM의 경우, CLIP 모델을 직접 미세 조정하는 경우 모든 기울기 단계에서 학습 세트의 모든 텍스트의 텍스트 피처를 계산해야 합니다. 이는 계산적으로 금지되어 있지만, 피처 어댑터 전략을 채택하여 모든 텍스트 피처를 한 번 캐시하고 텍스트 인코더를 업데이트하지 않습니다. 경고 에너지 폭발 지식 브랜드 이해 CLIP 사전 학습된 VLM 인코더 주의 기반 피처 어댑터 KAFA 지식 브랜드 이해 CLIP 사전 학습된 VLM 인코더 주의 기반 피처 어댑터 KAFA X(a) 아이에게 총 쏘는 법을 가르치고 싶기 때문에 &quot;브랜드 이름 X&quot; 총을 사야 합니다. (b) 에너지를 재충전할 수 있기 때문에 &quot;브랜드 로고 Y&quot;를 마셔야 합니다. X(c) 에너지를 얻을 수 있기 때문에 &quot;브랜드 로고 Z&quot;를 마셔야 합니다. X(a) 더 많은 것을 만들 수 있기 때문에 &quot;브랜드 X&quot; 메이크업을 사용해야 합니다. 유혹적 (b) X를 만드는 데 도움이 될 것이기 때문에 &quot;브랜드 Y&quot; 카메라를 사야겠다 (c) 예뻐 보일 것이기 때문에 &quot;브랜드 Z&quot;를 착용해야겠다 그림 8: VLM 베이스라인과 미세 조정 중 텍스트 피처에 대한 KAFA의 개선 사항을 보여주는 추가 사례.F.3 추가 소거 연구 본 논문(특히 표 1)에 제시된 실험에서 온라인 HNM 사용, 피처 적응에 대한 추가 입력(장면 텍스트 및 브랜드 정보) 및 베이스라인 어댑터에 비해 주의 기반 어댑터의 이점을 정당화했습니다.또한 주의 기반 피처 어댑터의 여러 변형에 대한 실험을 수행한 결과, 두 개 이상의 주의 계층을 사용하거나 인코더-디코더 변환기(Vaswani et al., 2017)에서와 같이 계층 규범 및 추가 선형 투영을 추가하면 모델이 과적합에 더 취약해진다는 것을 발견했습니다.F. 피처 어댑터의 추가 세부 정보 피처 어댑터(CLIP-Adapter 및 KAFA)의 경우 이전 섹션에서 설명한 대로 미세 조정을 위해 전체 HNM을 사용합니다. 영어: 우리는 추가 입력 분기를 제외하고는 &quot;Direct ft + HMN&quot;과 동일한 훈련 설정을 사용합니다.CLIP-Adapter의 경우 3개의 2계층 잔여 MLP를 훈련하여 설정에 맞게 조정합니다.특히, VLM에서 추출한 이미지 및 텍스트 피처와 이러한 피처의 혼합을 기반으로 구축된 9т 및 hmlp로 표시하겠습니다.x에 대한 조정된 피처는 mlp mlp fmlp (x) = n(f(x) + grip (f(x))) ƒmp (xst) = n(fT(xst) + m² (fT(xst))) mlp nlp mlp fmlp (x) = n(hip (cat [f(x), f (xst),...]))가 됩니다.여기서 cat은 연결입니다.여기서 텍스트 레이블 y에 대한 조정된 피처를 생략합니다.그리고 텍스트 레이블 y에 대한 조정된 피처는 mlp fmlp (y) = n(fr(y) + grip (fr(y)))가 되며 전체 HNM은 미세 조정을 위해 사용합니다.CLIP-Adapter와 KAFA 모두의 미세 조정을 위해 훨씬 더 큰 학습률(즉, le 4)이 효과적이며 조기 중단과 최대 10개의 에포크로 모델을 유사하게 훈련합니다.특징 어댑터의 출력을 VLM 이미지 특징에 가깝게 유지하기 위해 추가적인 정규화 손실을 추가하여 훈련을 안정화하는 것이 도움이 됩니다.특히, 두 가지 사이의 점곱의 음수(미니 배치의 모든 데이터 포인트에 대한 평균)를 전체 훈련 목표에 추가합니다.이 정규화 항목의 경우 Pitt 데이터 세트의 모든 실험에서 계수 5를 사용합니다.G 추가 예제 그림 8에 기준선에 대한 방법의 개선 사항을 설명하기 위해 2개의 추가 예를 제시합니다.다시 말하지만, 더 나은 표시를 위해 2개의 부정적인 텍스트 설명만 표시하고 모든 브랜드 정보를 익명화합니다.
