--- ABSTRACT ---
최근 자기 지도 사전 학습 기술의 등장으로 양식 문서 이해에서 멀티모달 학습 사용이 급증했습니다. 그러나 마스크 언어 모델링을 다른 모달리티로 확장하는 기존 접근 방식은 신중한 멀티태스크 튜닝, 복잡한 재구성 대상 설계 또는 추가 사전 학습 데이터가 필요합니다. FormNetV2에서 중앙 집중식 멀티모달 그래프 대조 학습 전략을 도입하여 모든 모달리티에 대한 자기 지도 사전 학습을 하나의 손실로 통합합니다. 그래프 대조 목적은 멀티모달 표현의 일치를 극대화하여 특별한 사용자 지정 없이 모든 모달리티에 대한 자연스러운 상호 작용을 제공합니다. 또한 그래프 에지로 연결된 토큰 쌍을 결합하는 바운딩 박스 내에서 이미지 피처를 추출하여 정교하고 별도로 사전 학습된 이미지 임베더를 로드하지 않고도 보다 타겟팅된 시각적 단서를 포착합니다. FormNetV2는 더 컴팩트한 모델 크기로 FUNSD, CORD, SROIE 및 결제 벤치마크에서 새로운 최첨단 성능을 확립합니다. 1
--- INTRODUCTION ---
자동화된 정보 추출은 많은 실용적인 애플리케이션에 필수적이며, 양식과 같은 문서는 기사와 같은 문서에 비해 고유한 과제를 안고 있어 이 분야에서 최근 많은 연구가 이루어졌습니다. 특히 양식과 같은 문서는 종종 표, 열, 채울 수 있는 영역과 같은 구조화된 객체를 포함하는 복잡한 레이아웃을 갖습니다. 레이아웃 인식 언어 모델링은 많은 성공에 중요했습니다(Xu et al., 2020; Majumder et al., 2020; Lee et al., 2022). 성능을 더욱 높이기 위해 많은 최근 접근 방식은 여러 모달리티를 채택합니다(Xu et al., *모든 작업은 Google에서 수행됨. 문의처: ChenYu Lee<chenyulee@google.com> , 리춘량<chunliang@google.com> 2021; Huang 등, 2022; Appalaraju 등, 2021). 구체적으로, 이미지 모달리티는 기존 레이아웃 및 텍스트 모달리티에 더 많은 구조적 정보와 시각적 단서를 추가합니다. 따라서 마스크 언어 모델링(MLM)을 텍스트에서 마스크 이미지 모델링(MIM)으로 확장하여 교차 모달 학습을 위한 이미지 및 텍스트-이미지 정렬(TIA)을 수행합니다. 정렬 목표는 텍스트 레이아웃이나 문서 구조를 직접 포함하지는 않지만 레이아웃 모달리티를 준비하는 데 도움이 될 수도 있습니다. 이 연구에서는 양식 정보 추출을 위한 멀티모달 변환기 모델인 FormNetV2를 제안합니다. 전체 이미지를 하나의 표현(Appalaraju 등, 2021)이나 이미지 패치(Xu 등, 2021) 또는 토큰 경계 상자의 이미지 특징(Xu 등, 2020)을 사용할 수 있는 기존 작업과 달리 구성된 그래프에서 연결된 토큰 쌍으로 경계가 지정된 영역에서 추출된 이미지 특징을 사용하는 것을 제안합니다. 이를 통해 엔티티 내 및 엔티티 간 정보의 보다 풍부하고 타겟팅된 시각적 구성 요소를 캡처할 수 있습니다.또한 각 개별 모달리티에 대해 여러 개의 자기 지도 목표를 사용하는 대신 그래프 대조 학습(Li et al., 2019; You et al., 2020; Zhu et al., 2021)을 도입하여 멀티모달 임베딩을 공동으로 학습합니다.FormNetV1에 대한 이 두 가지 추가 기능(Lee et al., 2022)을 통해 그래프 합성곱이 더 나은 슈퍼 토큰을 생성하여 성능이 향상되고 모델 크기가 작아집니다.실험에서 FormNetV2는 이전 버전인 FormNetV1과 기존 멀티모달 접근 방식보다 4가지 표준 벤치마크에서 성능이 우수합니다.특히 FormNetV1과 비교할 때 FormNetV2는 FUNSD(86.35 대 84.69)와 Payment(94.90 대 92.19)에서 큰 차이로 성능이 우수합니다. DocFormer(Appalaraju et al., 2021)와 비교했을 때 FormNetV2는 매개변수 수가 약 2.5배 더 적어 FUNSD 및 CORD에서 DocFormer보다 성능이 뛰어납니다. 2
--- RELATED WORK ---
초기 양식 문서 정보 추출 작업은 규칙 기반 모델 또는 수작업 기능이 있는 학습 기반 모델(Lebourgeois et al., 1992; O&#39;Gorman, 1993; Ha et al., 1995; Simon et al., 1997; Marinai et al., 2005; Chiticariu et al., 2013)을 기반으로 합니다. 나중에 다양한 딥 신경 모델이 제안되었는데, 여기에는 다음이 포함됩니다.
--- METHOD ---
순환 네트워크(Palm 등, 2017; Aggarwal 등, 2020), 합성곱 네트워크(Katti 등, 2018; Zhao 등, 2019; Denk 및 Reisswig, 2019) 및 변환기(Majumder 등, 2020; Garncarek 등, 2020; Wang 등, 2022c)를 기반으로 합니다. 최근 연구자들은 텍스트 외에도 OCR 단어 읽기 순서(Lee et al., 2021; Gu et al., 2022b), 텍스트 좌표(Majumder et al., 2020; Xu et al., 2020; Garncarek et al., 2020; Li et al., 2021a; Lee et al., 2022), 레이아웃 그리드(Lin et al., 2021) 및 레이아웃 그래프(Lee et al., 2022)와 같은 폼 문서 모델링의 레이아웃 속성을 탐구했습니다. 이미지 속성은 또한 글꼴, 색상 및 크기와 같은 필수적인 시각적 단서를 제공합니다. 로고 및 폼 테이블의 구분선을 포함한 다른 시각적 신호도 유용할 수 있습니다. Xu et al.(2020)은 Faster R-CNN(Ren et al., 2015)을 사용하여 토큰 이미지 기능을 추출합니다. Appalaraju et al. (2021)은 ResNet50(He et al., 2016)을 사용하여 전체 문서 이미지 특징을 추출합니다. Li et al. (2022)은 FPN(Lin et al., 2017)과 함께 ViT(Dosovitskiy et al., 2020)를 사용하여 겹치지 않는 패치 이미지 특징을 추출합니다. 이러한 정교한 이미지 임베더는 외부 이미지 데이터 세트(예: ImageNet(Russakovsky et al., 2015) 또는 PubLayNet(Zhong et al., 2019))를 사용하는 별도의 사전 학습 단계가 필요하며, 때로는 이산 변분 자동 인코더(dVAE)로 사전 학습된 시각적 코드북에 의존합니다. 여러 모달리티가 작용하는 경우 다양한 지도 또는 자기 지도 다중 모달 사전 학습 기술이 제안되었습니다. 여기에는 마스크 예측, 재구성 및 하나 이상의 모달리티에 대한 매칭이 포함됩니다(Xu 등, 2020, 2021; Appalaraju 등, 2021; Li 등, 2021b; Gu 등, 2022a; Huang 등, 2022; Li 등, 2022; Pramanik 등, 2020). 다음 단어 예측(Kim 등, 2022) 또는 길이 예측(Li 등, 2021c)은 텍스트와 이미지 모달리티를 연결하기 위해 연구되었습니다. 직접 및 상대 위치 예측(Cosma 등, 2020; Wei 등, 2020; Li 등, 2021a; Wang 등, 2022a; Li 등, 2021c)은 문서의 기본 레이아웃 의미를 탐색하기 위해 제안되었습니다. 그럼에도 불구하고, 이러한 사전 학습 목표를 달성하려면 강력한 도메인 전문 지식, 전문 설계 및 관련 모달리티 간의 다중 작업 튜닝이 필요합니다. 이 작업에서 제안하는 그래프 대조 학습은 중앙 집중식 설계에서 다중 모달 사전 학습을 수행하여 사전 도메인 지식이 필요 없이 모든 관련 모달리티 간의 상호 작용을 통합합니다. 3 FormNetV 3.1절에서 백본 아키텍처 FormNetV1(Lee et al., 2022)을 간략히 검토하고 3.2절에서 다중 모달 입력 설계를 소개하고 3.3절에서 다중 모달 그래프 대조 학습을 자세히 설명합니다. 3.1 예비 지식 ETC. FormNetV1(Lee et al., 2022)은 장문 문서에 대한 주의의 이차 메모리 비용을 해결하기 위해 확장 변압기 구성(ETC; Ainslie et al., 2020)을 백본으로 사용합니다. ETC는 시퀀스의 모든 토큰에 참석할 수 있는 몇 개의 특수 토큰만 허용합니다(전역 주의). 영어: 다른 모든 토큰은 이러한 특수 토큰(로컬 어텐션) 외에도 작은 창 내의 k 로컬 이웃에만 주의를 기울일 수 있습니다.이렇게 하면 스코어링이 필요한 O(n²) 쿼리-키 쌍에서 O(kn)으로 계산 복잡도가 줄어듭니다.Eq. (2)는 인덱스 0에 하나의 글로벌 토큰이 있는 모델에 대한 어텐션 벡터 ao의 계산을 공식화하고, Eq. (2)는 모델의 나머지 토큰에 대한 어텐션 벡터 a&gt;0의 계산을 공식화합니다.ao attendance(ho, [ho, h1, ..., hn]) (1) a&gt;0= attendance(h, [họ, hk,…,h+]) (2) .Rich Attention. 영어: 불완전한 OCR 직렬화로 인해 생성된 토큰의 왜곡된 의미적 관련성을 해결하기 위해 FormNetV1은 Rich Attention을 제안하여 토큰 간의 공간적 관계를 모델링하는 어텐션 메커니즘을 적용합니다. Rich Attention은 조회 테이블의 고유한 임베딩과 관련된 영역으로 문서를 양자화하지 않고도 저수준 공간적 특징에 대한 어텐션을 조절하는 수학적으로 타당한 방법입니다. Rich Attention에서 모델은 여러 구성 요소에서 (사전 소프트맥스) 어텐션 점수(식 10)를 구성합니다. 일반적인 변환기 어텐션 점수(식 7), x축과 y축을 따라 토큰의 순서(식 8), 토큰 간의 로그 거리(픽셀 수)(다시 두 축을 따라)(식 9). x축에 Rich Attention이 있는 변환기 헤드에 대한 표현식은 식 (3–10)에 제공됩니다. 우리 DAVIS POLK DAVIS POLK D VOIS P₁K Vo VSender Charles Duggan 입력 문서 Sender Charles Duggan OCR + 그래프 구성 SVVCh V3 es DVA 5개 노드 및 5개 에지로 구성된 VVGraph 문서 표현 그림 1: 양식의 샘플 영역 그래프. 토큰 경계 상자가 식별되고 이를 통해 그래프가 구성됩니다. 노드에 레이블이 지정되고 그래프 구조는 콘텐츠에서 추상화되어 표시됩니다. 에지 수준 피처 노드 수준 피처 레이아웃 이미지 텍스트 [DAVIS POLK] Vo eDAVIS DAVIS ePOLK [VSender Sender: 토큰 유니온 상자의 VImage 피처 토큰 공간 관계에 따른 기하학적 피처 BERT 토크나이저에 따른 텍스트 피처 그림 2: 멀티모달 그래프 표현은 세 가지 모달리티로 구성됩니다. 노드 수준의 텍스트, 에지 수준의 레이아웃 및 이미지 연결. 자세한 내용은 관심 있는 독자는 Lee et al.(2022)을 참조하세요. Oij = int(x; &lt; xj) (3) dij = ln(1+|xi − xj|) (4) = Pij Sigmoid(affine (P)([qi; kj])) (5) Hij = affine(&quot;) ([qi; kj]) (6) = q₁kj = Sij (7) Oij ln(Pij) + (1 − Oij) ln(1 – Pij) (8) 0² (dij — Hij)² Sij+S Sij = S +s (9) (10) GCN. 마지막으로 FormNetV1에는 ETC 변환기 구성 요소로 보낼 텍스트를 직렬화하기 전에 그래프 합성 네트워크(GCN) 맥락화 단계가 포함됩니다. GCN의 그래프는 토큰 임베딩을 합성하여 그림 1에 표시된 대로 슈퍼토큰 표현을 구축하기 전에 지리적 &quot;근접성&quot;으로 광범위하게 정의된 각 토큰에 대해 최대 K개의 이웃을 찾습니다. 이를 통해 네트워크 영어: 로컬 어텐션에 의해 제약을 받는 Rich Attention보다 약하지만 더 완전한 레이아웃 모달리티 그림을 구축합니다.최종 시스템은 표준 마스크 언어 모델링(MLM) 목표로 종단 간 사전 학습되었습니다.자세한 내용은 부록의 섹션 A.3을 참조하십시오.DAVIS POLK &amp; 발신자 Charles Duggan (a) 엔터티 내부 DAVIS POLK 발신자 Charles Duggan (b) 교차 엔터티 그림 3: 이미지 피처는 (a) 엔터티 내의 유사한 패턴 또는 (b) 엔터티 간의 서로 다른 패턴 또는 구분선을 포착하기 위해 에지로 연결된 토큰 쌍을 연결하는 경계 상자(빨간색)에서 추출됩니다.3. 다중 모달 입력 FormNetV2에서는 FormNetV1에서 이미 사용된 텍스트 및 레이아웃 모달리티 외에도 모델에 이미지 모달리티를 추가하는 것을 제안합니다(Lee et al. (2022)의 섹션 3.3).문서의 이미지 피처에는 글꼴, 색상 및 OCR 단어의 크기와 같이 텍스트나 레이아웃에 없는 정보가 포함될 것으로 예상합니다. 이를 위해 ConvNet을 실행하여 전체 문서 이미지에서 고밀도 이미지 피처를 추출한 다음 관심 영역(ROI) 풀링(He et al., 2017)을 사용하여 GCN 에지로 연결된 토큰 쌍을 결합하는 경계 상자 내의 피처를 풀링합니다. 마지막으로 RoI 풀링된 피처는 정제를 위해 또 다른 작은 ConvNet을 거칩니다. 이미지 피처가 추출된 후 GCN 에지의 기존 레이아웃 피처와 연결하여 네트워크에 주입합니다. 그림은 이 작업에서 세 가지 모달리티가 모두 어떻게 활용되는지 보여주고 섹션 4.2에서는 아키텍처를 자세히 설명합니다. 이미지 모달리티를 통합하는 최근의 대부분 접근 방식(표 1)은 (a) 전체 이미지에서 하나의 벡터로, (b) 변환기에 대한 추가 입력 토큰으로 겹치지 않는 이미지 패치로, (c) 모든 토큰의 텍스트 피처에 추가되는 토큰 경계 상자에서 피처를 추출합니다. 그러나 문서 이미지에는 종종 개별적으로 비교적 작고 텍스트 블록에 밀집되어 분포된 OCR 단어가 포함됩니다. 또한 텍스트가 없는 많은 부분의 배경 영역이 포함되어 있습니다.따라서 앞서 언급한 방법(a)은 큰 노이즈가 있는 배경 영역이 있는 전역 시각적 표현만 생성하지만 노드 수준 에지 수준 ~ 에지 삭제 + 레이아웃 Vo 노드 피처 삭제 에지 피처 삭제 + V₁ 이미지 텍스트 VStochastic 그래프 손상 V₁ Vo 레이아웃 이미지 VText VVV₁ Vo 레이아웃 VA VEdge 삭제 + 에지 피처 삭제 + 노드 피처 삭제 이미지 VText VVInductive Multimodal Graph Contrastive Learning 서로 다른 손상된 그래프의 같은 노드의 양의 쌍 서로 다른 손상된 그래프의 다른 노드의 음의 쌍 서로 다른 손상된 그래프의 다른 노드의 음의 쌍 그림 4: 멀티모달 그래프 대조 학습.두 개의 손상된 그래프는 그래프 토폴로지(에지)와 속성(멀티모달 피처)의 손상으로 인해 입력 그래프에서 샘플링됩니다.시스템은 모든 손상된 노드 쌍(동일한 그래프 내 포함)에서 어떤 노드 쌍이 동일한 노드에서 왔는지 식별하도록 훈련됩니다.대상 엔터티 표현; 방법(b)는 패치 크기에 민감한 경향이 있으며 종종 OCR 단어나 긴 엔터티를 다른 패치로 잘라내는 반면, 토큰 길이가 늘어나 계산 비용이 증가합니다. 그리고 방법(c)는 각 토큰의 경계 상자 내의 영역만 보고 토큰 사이 또는 토큰 외부의 컨텍스트가 없습니다. 반면에 제안된 에지 수준 이미지 피처 표현은 모든 무관하거나 방해가 되는 영역을 무시하면서 두 개의 인근의 잠재적으로 관련된 &quot;이웃&quot; 토큰과 주변 영역 간의 관계를 정확하게 모델링할 수 있습니다. 그림 3은 연합 경계 상자를 통한 대상 RoI 이미지 피처 풀링이 엔터티(왼쪽) 내의 모든 유사한 패턴(예: 글꼴, 색상, 크기) 또는 엔터티 간의 서로 다른 패턴이나 구분선(오른쪽)을 캡처할 수 있음을 보여줍니다. 자세한 내용은 4.4절을 참조하십시오. 3.3 다중 모달 그래프 대조 학습 다중 모달 문서 이해에 대한 이전 작업에서는 사전 학습 중에 하나 이상의 모달리티에서 임베딩을 학습하기 위해 여러 개의 지도 또는 자체 지도 목표를 조작해야 합니다. 이와 대조적으로, FormNetV2에서 우리는 대조적 손실을 가진 다중 모드 임베딩을 학습하기 위해 문서의 그래프 표현을 활용하는 것을 제안합니다. 구체적으로, 우리는 먼저 확률적 그래프 손상을 수행하여 각 학습 인스턴스의 원래 입력 그래프에서 두 개의 손상된 그래프를 샘플링합니다. 이 단계는 부분적 맥락을 기반으로 노드 임베딩을 생성합니다. 그런 다음, 노드 수준에서 토큰 간의 일치를 최대화하여 대조적 목적을 적용합니다. 즉, 모델은 동일한 그래프 내부와 그래프 간에 모든 노드 쌍에서 어떤 노드 쌍이 동일한 원래 노드에서 왔는지 식별하도록 요청받습니다. 우리는 모든 노드 쌍에서 온도가 0.1인 표준 정규화된 온도 스케일 교차 엔트로피(NT-Xent) 손실 공식(Chen et al., 2020; Wu et al., 2018; Oord et al., 2018; Sohn, 2016)을 채택합니다.
--- EXPERIMENT ---
s, FormNetV2는 이전 모델인 FormNetV1과 기존 멀티모달 접근 방식보다 4가지 표준 벤치마크에서 성능이 뛰어납니다. 특히 FormNetV1과 비교할 때 FormNetV2는 FUNSD(86.35 대 84.69)와 Payment(94.90 대 92.19)에서 큰 차이로 성능이 뛰어납니다. DocFormer(Appalaraju et al., 2021)와 비교할 때 FormNetV2는 매개변수 수가 약 2.5배 적으면서 FUNSD와 CORD에서 성능이 뛰어납니다. 2 관련 작업 양식 문서 정보 추출에 대한 초기 작업은 수작업 기능이 있는 규칙 기반 모델 또는 학습 기반 모델을 기반으로 합니다(Lebourgeois et al., 1992; O&#39;Gorman, 1993; Ha et al., 1995; Simon et al., 1997; Marinai et al., 2005; Chiticariu et al., 2013). 이후, 순환 신경망(Palm 등, 2017; Aggarwal 등, 2020), 합성곱 신경망(Katti 등, 2018; Zhao 등, 2019; Denk and Reisswig, 2019), 변환기(Majumder 등, 2020; Garncarek 등, 2020; Wang 등, 2022c)를 기반으로 하는 방법을 포함한 다양한 딥 신경망 모델이 제안되었습니다. 최근 연구자들은 텍스트 외에도 OCR 단어 읽기 순서(Lee et al., 2021; Gu et al., 2022b), 텍스트 좌표(Majumder et al., 2020; Xu et al., 2020; Garncarek et al., 2020; Li et al., 2021a; Lee et al., 2022), 레이아웃 그리드(Lin et al., 2021) 및 레이아웃 그래프(Lee et al., 2022)와 같은 폼 문서 모델링의 레이아웃 속성을 탐구했습니다. 이미지 속성은 또한 글꼴, 색상 및 크기와 같은 필수적인 시각적 단서를 제공합니다. 로고 및 폼 테이블의 구분선을 포함한 다른 시각적 신호도 유용할 수 있습니다. Xu et al.(2020)은 Faster R-CNN(Ren et al., 2015)을 사용하여 토큰 이미지 기능을 추출합니다. Appalaraju et al. (2021)은 ResNet50(He et al., 2016)을 사용하여 전체 문서 이미지 특징을 추출합니다. Li et al. (2022)은 FPN(Lin et al., 2017)과 함께 ViT(Dosovitskiy et al., 2020)를 사용하여 겹치지 않는 패치 이미지 특징을 추출합니다. 이러한 정교한 이미지 임베더는 외부 이미지 데이터 세트(예: ImageNet(Russakovsky et al., 2015) 또는 PubLayNet(Zhong et al., 2019))를 사용하는 별도의 사전 학습 단계가 필요하며, 때로는 이산 변분 자동 인코더(dVAE)로 사전 학습된 시각적 코드북에 의존합니다. 여러 모달리티가 작용하는 경우 다양한 지도 또는 자기 지도 다중 모달 사전 학습 기술이 제안되었습니다. 여기에는 마스크 예측, 재구성 및 하나 이상의 모달리티에 대한 매칭이 포함됩니다(Xu 등, 2020, 2021; Appalaraju 등, 2021; Li 등, 2021b; Gu 등, 2022a; Huang 등, 2022; Li 등, 2022; Pramanik 등, 2020). 다음 단어 예측(Kim 등, 2022) 또는 길이 예측(Li 등, 2021c)은 텍스트와 이미지 모달리티를 연결하기 위해 연구되었습니다. 직접 및 상대 위치 예측(Cosma 등, 2020; Wei 등, 2020; Li 등, 2021a; Wang 등, 2022a; Li 등, 2021c)은 문서의 기본 레이아웃 의미를 탐색하기 위해 제안되었습니다. 그럼에도 불구하고, 이러한 사전 학습 목표를 달성하려면 강력한 도메인 전문 지식, 전문 설계 및 관련 모달리티 간의 다중 작업 튜닝이 필요합니다. 이 작업에서 제안하는 그래프 대조 학습은 중앙 집중식 설계에서 다중 모달 사전 학습을 수행하여 사전 도메인 지식이 필요 없이 모든 관련 모달리티 간의 상호 작용을 통합합니다. 3 FormNetV 3.1절에서 백본 아키텍처 FormNetV1(Lee et al., 2022)을 간략히 검토하고 3.2절에서 다중 모달 입력 설계를 소개하고 3.3절에서 다중 모달 그래프 대조 학습을 자세히 설명합니다. 3.1 예비 지식 ETC. FormNetV1(Lee et al., 2022)은 장문 문서에 대한 주의의 이차 메모리 비용을 해결하기 위해 확장 변압기 구성(ETC; Ainslie et al., 2020)을 백본으로 사용합니다. ETC는 시퀀스의 모든 토큰에 참석할 수 있는 몇 개의 특수 토큰만 허용합니다(전역 주의). 영어: 다른 모든 토큰은 이러한 특수 토큰(로컬 어텐션) 외에도 작은 창 내의 k 로컬 이웃에만 주의를 기울일 수 있습니다.이렇게 하면 스코어링이 필요한 O(n²) 쿼리-키 쌍에서 O(kn)으로 계산 복잡도가 줄어듭니다.Eq. (2)는 인덱스 0에 하나의 글로벌 토큰이 있는 모델에 대한 어텐션 벡터 ao의 계산을 공식화하고, Eq. (2)는 모델의 나머지 토큰에 대한 어텐션 벡터 a&gt;0의 계산을 공식화합니다.ao attendance(ho, [ho, h1, ..., hn]) (1) a&gt;0= attendance(h, [họ, hk,…,h+]) (2) .Rich Attention. 영어: 불완전한 OCR 직렬화로 인해 생성된 토큰의 왜곡된 의미적 관련성을 해결하기 위해 FormNetV1은 Rich Attention을 제안하여 토큰 간의 공간적 관계를 모델링하는 어텐션 메커니즘을 적용합니다. Rich Attention은 조회 테이블의 고유한 임베딩과 관련된 영역으로 문서를 양자화하지 않고도 저수준 공간적 특징에 대한 어텐션을 조절하는 수학적으로 타당한 방법입니다. Rich Attention에서 모델은 여러 구성 요소에서 (사전 소프트맥스) 어텐션 점수(식 10)를 구성합니다. 일반적인 변환기 어텐션 점수(식 7), x축과 y축을 따라 토큰의 순서(식 8), 토큰 간의 로그 거리(픽셀 수)(다시 두 축을 따라)(식 9). x축에 Rich Attention이 있는 변환기 헤드에 대한 표현식은 식 (3–10)에 제공됩니다. 우리 DAVIS POLK DAVIS POLK D VOIS P₁K Vo VSender Charles Duggan 입력 문서 Sender Charles Duggan OCR + 그래프 구성 SVVCh V3 es DVA 5개 노드 및 5개 에지로 구성된 VVGraph 문서 표현 그림 1: 양식의 샘플 영역 그래프. 토큰 경계 상자가 식별되고 이를 통해 그래프가 구성됩니다. 노드에 레이블이 지정되고 그래프 구조는 콘텐츠에서 추상화되어 표시됩니다. 에지 수준 피처 노드 수준 피처 레이아웃 이미지 텍스트 [DAVIS POLK] Vo eDAVIS DAVIS ePOLK [VSender Sender: 토큰 유니온 상자의 VImage 피처 토큰 공간 관계에 따른 기하학적 피처 BERT 토크나이저에 따른 텍스트 피처 그림 2: 멀티모달 그래프 표현은 세 가지 모달리티로 구성됩니다. 노드 수준의 텍스트, 에지 수준의 레이아웃 및 이미지 연결. 자세한 내용은 관심 있는 독자는 Lee et al.(2022)을 참조하세요. Oij = int(x; &lt; xj) (3) dij = ln(1+|xi − xj|) (4) = Pij Sigmoid(affine (P)([qi; kj])) (5) Hij = affine(&quot;) ([qi; kj]) (6) = q₁kj = Sij (7) Oij ln(Pij) + (1 − Oij) ln(1 – Pij) (8) 0² (dij — Hij)² Sij+S Sij = S +s (9) (10) GCN. 마지막으로 FormNetV1에는 ETC 변환기 구성 요소로 보낼 텍스트를 직렬화하기 전에 그래프 합성 네트워크(GCN) 맥락화 단계가 포함됩니다. GCN의 그래프는 토큰 임베딩을 합성하여 그림 1에 표시된 대로 슈퍼토큰 표현을 구축하기 전에 지리적 &quot;근접성&quot;으로 광범위하게 정의된 각 토큰에 대해 최대 K개의 이웃을 찾습니다. 이를 통해 네트워크 영어: 로컬 어텐션에 의해 제약을 받는 Rich Attention보다 약하지만 더 완전한 레이아웃 모달리티 그림을 구축합니다.최종 시스템은 표준 마스크 언어 모델링(MLM) 목표로 종단 간 사전 학습되었습니다.자세한 내용은 부록의 섹션 A.3을 참조하십시오.DAVIS POLK &amp; 발신자 Charles Duggan (a) 엔터티 내부 DAVIS POLK 발신자 Charles Duggan (b) 교차 엔터티 그림 3: 이미지 피처는 (a) 엔터티 내의 유사한 패턴 또는 (b) 엔터티 간의 서로 다른 패턴 또는 구분선을 포착하기 위해 에지로 연결된 토큰 쌍을 연결하는 경계 상자(빨간색)에서 추출됩니다.3. 다중 모달 입력 FormNetV2에서는 FormNetV1에서 이미 사용된 텍스트 및 레이아웃 모달리티 외에도 모델에 이미지 모달리티를 추가하는 것을 제안합니다(Lee et al. (2022)의 섹션 3.3).문서의 이미지 피처에는 글꼴, 색상 및 OCR 단어의 크기와 같이 텍스트나 레이아웃에 없는 정보가 포함될 것으로 예상합니다. 이를 위해 ConvNet을 실행하여 전체 문서 이미지에서 고밀도 이미지 피처를 추출한 다음 관심 영역(ROI) 풀링(He et al., 2017)을 사용하여 GCN 에지로 연결된 토큰 쌍을 결합하는 경계 상자 내의 피처를 풀링합니다. 마지막으로 RoI 풀링된 피처는 정제를 위해 또 다른 작은 ConvNet을 거칩니다. 이미지 피처가 추출된 후 GCN 에지의 기존 레이아웃 피처와 연결하여 네트워크에 주입합니다. 그림은 이 작업에서 세 가지 모달리티가 모두 어떻게 활용되는지 보여주고 섹션 4.2에서는 아키텍처를 자세히 설명합니다. 이미지 모달리티를 통합하는 최근의 대부분 접근 방식(표 1)은 (a) 전체 이미지에서 하나의 벡터로, (b) 변환기에 대한 추가 입력 토큰으로 겹치지 않는 이미지 패치로, (c) 모든 토큰의 텍스트 피처에 추가되는 토큰 경계 상자에서 피처를 추출합니다. 그러나 문서 이미지에는 종종 개별적으로 비교적 작고 텍스트 블록에 밀집되어 분포된 OCR 단어가 포함됩니다. 또한 텍스트가 없는 많은 부분의 배경 영역이 포함되어 있습니다.따라서 앞서 언급한 방법(a)은 큰 노이즈가 있는 배경 영역이 있는 전역 시각적 표현만 생성하지만 노드 수준 에지 수준 ~ 에지 삭제 + 레이아웃 Vo 노드 피처 삭제 에지 피처 삭제 + V₁ 이미지 텍스트 VStochastic 그래프 손상 V₁ Vo 레이아웃 이미지 VText VVV₁ Vo 레이아웃 VA VEdge 삭제 + 에지 피처 삭제 + 노드 피처 삭제 이미지 VText VVInductive Multimodal Graph Contrastive Learning 서로 다른 손상된 그래프의 같은 노드의 양의 쌍 서로 다른 손상된 그래프의 다른 노드의 음의 쌍 서로 다른 손상된 그래프의 다른 노드의 음의 쌍 그림 4: 멀티모달 그래프 대조 학습.두 개의 손상된 그래프는 그래프 토폴로지(에지)와 속성(멀티모달 피처)의 손상으로 인해 입력 그래프에서 샘플링됩니다.시스템은 모든 손상된 노드 쌍(동일한 그래프 내 포함)에서 어떤 노드 쌍이 동일한 노드에서 왔는지 식별하도록 훈련됩니다.대상 엔터티 표현; 방법(b)는 패치 크기에 민감한 경향이 있으며 종종 OCR 단어나 긴 엔터티를 다른 패치로 잘라내는 반면, 토큰 길이가 늘어나 계산 비용이 증가합니다. 그리고 방법(c)는 각 토큰의 경계 상자 내의 영역만 보고 토큰 사이 또는 토큰 외부의 컨텍스트가 없습니다. 반면에 제안된 에지 수준 이미지 피처 표현은 모든 무관하거나 방해가 되는 영역을 무시하면서 두 개의 인근의 잠재적으로 관련된 &quot;이웃&quot; 토큰과 주변 영역 간의 관계를 정확하게 모델링할 수 있습니다. 그림 3은 연합 경계 상자를 통한 대상 RoI 이미지 피처 풀링이 엔터티(왼쪽) 내의 모든 유사한 패턴(예: 글꼴, 색상, 크기) 또는 엔터티 간의 서로 다른 패턴이나 구분선(오른쪽)을 캡처할 수 있음을 보여줍니다. 자세한 내용은 4.4절을 참조하십시오. 3.3 다중 모달 그래프 대조 학습 다중 모달 문서 이해에 대한 이전 작업에서는 사전 학습 중에 하나 이상의 모달리티에서 임베딩을 학습하기 위해 여러 개의 지도 또는 자체 지도 목표를 조작해야 합니다. 이와 대조적으로, FormNetV2에서 우리는 대조적 손실을 가진 멀티모달 임베딩을 학습하기 위해 문서의 그래프 표현을 활용하는 것을 제안한다. 구체적으로, 우리는 먼저 확률적 그래프 손상을 수행하여 각 훈련 인스턴스의 원래 입력 그래프에서 두 개의 손상된 그래프를 샘플링한다. 이 단계는 부분적 맥락을 기반으로 노드 임베딩을 생성한다. 그런 다음, 우리는 노드 수준에서 토큰 간의 일치를 최대화하여 대조적 목적을 적용한다. 즉, 모델은 동일한 그래프 내부와 그래프 전반에 걸쳐 모든 노드 쌍에서 어떤 노드 쌍이 동일한 원래 노드에서 왔는지 식별하도록 요청받는다. 우리는 모든 실험에서 온도가 0.1인 표준 정규화된 온도 스케일 교차 엔트로피(NT-Xent) 손실 공식(Chen et al., 2020; Wu et al., 2018; Oord et al., 2018; Sohn, 2016)을 채택한다. 여러 입력 모달리티 간의 상호 작용을 통합하는 중앙 집중식 대조 손실을 구축하기 위해 그래프 토폴로지 수준과 그래프 피처 수준에서 모두 원래 그래프를 손상시킵니다. 토폴로지 손상에는 원래 그래프에서 에지를 무작위로 제거하여 에지 삭제가 포함됩니다. 피처 손상에는 세 가지 모달리티 모두에 삭제를 적용하는 것이 포함됩니다. 에지에서 레이아웃 및 이미지 피처를 삭제하고 노드에서 텍스트 피처를 삭제합니다. 그래프 대조 학습 중에 문서의 의미적으로 의미 있는 그래프 표현을 활용하기 위해 GCN 인코더에서만 그래프를 손상시키고 ETC 디코더는 그대로 유지합니다. 두 개의 손상된 그래프에서 컨텍스트를 더욱 다양화하고 특정 모달리티에 지나치게 의존하도록 모델을 학습하는 위험을 줄이기 위해 두 개의 손상된 그래프 간에 모달리티의 불균형 삭제율을 채택하여 귀납적 그래프 피처 삭제 메커니즘을 추가로 설계합니다. 정확히 말해서 주어진 모달리티에 대해 첫 번째 손상된 그래프에서 피처의 p퍼센트를 삭제하고 두 번째 손상된 그래프에서 피처의 1-p퍼센트를 삭제합니다. 4.4절의 실험은 p 0가 경험적으로 가장 잘 작동하고 귀납적 특징 삭제 메커니즘이 바닐라 버전보다 성능을 더욱 향상시킨다는 것을 보여줍니다. 우리는 정규화에 대한 이러한 붐 앤 버스트 접근 방식을 통해 모델이 특정 특징 상호 작용에 지나치게 의존하지 않고도 모델의 용량을 최대한 활용하는 풍부하고 복잡한 표현을 학습할 수 있다고 규정합니다. 그림 4는 전반적인 = 프로세스를 보여줍니다. 제안된 그래프 대조 목적은 또한 다른 손상 메커니즘(Zhu et al., 2020; Hassani and Khasahmadi, 2020; You et al., 2020; Velickovic et al., 2019)을 채택하기에 원칙적으로 충분히 일반적입니다. 다중 모달 특징 삭제는 하나의 손실 설계에서 여러 입력 모달리티 간의 상호 작용을 사용하고 허용할 수 있는 자연스러운 놀이터를 제공합니다. 도메인 전문가가 특수 손실을 수작업으로 만들 필요 없이 프레임워크를 확장하여 더 많은 모달리티를 포함하는 것은 간단합니다. 저희가 아는 한, 저희는 양식 문서 이해를 위해 사전 훈련 중에 그래프 대조 학습을 사용한 최초의 사례입니다.4 평가 4.1 데이터 세트 FUNSD.FUNSD(Jaume et al., 2019)에는 구조와 모양이 매우 다양한 연구, 마케팅 및 광고 양식 컬렉션이 포함되어 있습니다.이 데이터 세트는 9,707개의 엔터티와 헤더, 질문, 답변 및 기타의 4개 엔터티 유형에 대한 31,485개의 단어 수준 주석이 있는 199개의 주석이 있는 양식으로 구성됩니다.저희는 훈련 및 테스트 세트에 공식적인 75-25 분할을 사용합니다.CORD.CORD(Park et al., 2019)에는 상점 및 레스토랑의 인도네시아 영수증 11,000개 이상이 포함되어 있습니다.주석은 매장 이름, 메뉴 수량, 세액, 할인 가격 등과 같은 30개의 세분화된 의미 엔터티로 제공됩니다.저희는 훈련, 검증 및 테스트 세트에 공식적인 800-100-100 분할을 사용합니다. SROIE. 스캔된 영수증 OCR 및 주요 정보 추출(SROIE)에 대한 ICDAR 2019 챌린지(Huang et al., 2019)는 1,000개의 전체 스캔된 영수증 이미지와 주석을 제공합니다. 626개의 샘플은 훈련용이고 347개의 샘플은 테스트용입니다. 작업은 회사, 날짜, 주소 또는 총계의 네 가지 미리 정의된 엔터티를 추출하는 것입니다. 지불. 우리는 약 10,000개의 문서와 인간 주석자로부터 얻은 7개의 의미 엔터티 레이블로 구성된 대규모 지불 데이터(Majumder et al., 2020)를 사용합니다. 우리는 Majumder et al.(2020)에서 사용된 것과 동일한 평가 프로토콜과 데이터 세트 분할을 따릅니다. 4.2 실험 설정 우리는 제안된 방법에서 사용된 여러 모달리티를 통합하기 위해 약간의 수정을 거친 FormNetV1(Lee et al., 2022) 아키텍처를 따릅니다. 우리의 백본 모델은 구조 인식 슈퍼 토큰을 생성하는 6계층 GCN 인코더와 문서 엔티티 추출을 위한 Rich Attention이 장착된 12계층 ETC 트랜스포머 디코더로 구성됩니다.숨겨진 유닛의 수는 GCN과 ETC 모두에 대해 768로 설정됩니다.어텐션 헤드의 수는 GCN에서 1로, ETC에서 12로 설정됩니다.최대 시퀀스 길이는 1024로 설정됩니다.다른 하이퍼 매개변수 설정은 Ainslie et al.(2020), Lee et al.(2022)을 따릅니다.이미지 임베더 아키텍처에 대해서는 부록의 Sec A.1을 참조하십시오.사전 학습.우리는 두 가지 비지도 목적을 사용하여 FormNetV2를 사전 학습합니다.Masked Language Modeling(MLM)(Taylor, 1953; Devlin et al., 2019)과 제안된 다중 모드 그래프 대조 학습(GCL). BERT(Devlin et al., 2019)와 달리, 여기에서 MLM은 Appalaraju et al.(2021); Xu et al.(2021, 2020)과 유사하게 사전 학습 중에 레이아웃 및 이미지 모달리티에 액세스할 수 있습니다. 그럼에도 불구하고, 레이아웃 및 이미지 피처는 노드 수준이 아닌 에지 수준에서 구성되어 사소한 정보를 직접 유출하지 않고 더 나은 기본 표현 학습을 위해 텍스트 피처를 보완합니다. GCL은 대조적인 방식으로 문서에서 세 가지 모달리티 간의 효과적인 상호 작용을 위한 자연스러운 놀이터를 제공합니다. 문서의 각 그래프 표현에 대해 각각 {0.3, 0.8, 0.8}의 삭제 비율로 에지 삭제, 에지 피처 삭제 및 노드 피처 삭제로 두 개의 손상된 뷰를 생성합니다. GCN과 ETC의 가중치 행렬은 두 뷰에서 공유됩니다. 우리는 Appalaraju et al.(2021); Xu et al.을 따릅니다. (2021, 2020) 및 사전 학습을 위해 1,100만 개의 문서 이미지가 포함된 대규모 IIT-CDIP 문서 컬렉션(Lewis et al., 2006)을 사용합니다. 배치 크기가 512인 Adam 옵티마이저를 사용하여 처음부터 모델을 학습합니다. 학습률은 0.0002로 설정되고 워밍업 비율은 0.01입니다. GCL이 일반적으로 MLM보다 빠르게 수렴한다는 것을 발견했으므로 MLM과 GCL의 손실 가중치를 각각 1과 0.5로 설정했습니다. 표 1에 나와 있는 다른 최근 접근 방식에서 수행한 것처럼 이미지 임베더에 대해 별도로 사전 학습하거나 사전 학습된 체크포인트를 로드하지 않는다는 점에 유의하세요. 사실, 우리 구현에서 ImageNet(Russakovsky et al., 2015)과 같이 정교한 이미지 임베더를 사용하거나 자연스러운 이미지로 사전 학습하는 것이 최종 다운스트림 데이터 세트 방법을 개선하지 않는다는 것을 발견했습니다. PR FF1* 모달리티 이미지 임베더 #Params FUNSD SPADE(Hwang et al., 2021) UniLMv2(Bao et al., 2020) LayoutLMv1(Xu et al., 2020) DocFormer(Appalaraju et al., 2021) 70.T+L 110M 67.80 73.91 70.75.36 80.61 77.81.33 85.44 83.T 355M T+L 343M T+L+I ResNet502M FormNet V1 (Lee et al., 2022) LayoutLMv1 (Xu et al., 2020) LayoutLMv2 (Xu et al., 2021) DocFormer (Appalaraju et al., 2021) StructuralLM (Li et al., 2021a) LayoutLMv3(Huang 외, 2022) FormNet V2(당사) 85.21 84.18 84.76.77 81.95 79.83.24 85.19 84.82.29 86.94 84.T+L 217M CORD SPADE(Hwang 외, 2021) UniLMv2 (바오 외, 2020) LayoutLMv1(Xu et al., 2021) 85.81.35 83.75 82.53 92.85.78 86.94 86.35 92.91.T+L+I T+L+I T+L+I T+L T+L+I ResNet160M ResNeXt101-FPN 426M ResNet536M 355M 토큰화 368M T+L+I 3층 ConvNet 204M T+L 110M DocFormer(Appalaraju et al., 2021) 91.23 92.89 92.94.32 95.54 94.96.46 96.14 96.T 355M T+L 343M T+L+I ResNet502M FormNet V1(Lee 등, 2022) LayoutLMv2(Xu 등, 2021) TILT(Powalski 등, 2021) DocFormer(Appalaraju 등, 2021) LayoutLMv3(Huang 등, 2022) FormNet V2(당사) 98.02 96.55 97.95.65 96.37 96.96.T+L 345M T+L+I ResNeXt101-FPN 426M T+L+I U-Net 780M 97.25 96.74 96.T+L+I ResNet536M 95.82 96.03 95.92 97.T+L+I 토큰화 368M 97.74 97.00 97.37 97.T+L+I 3층 ConvNet 204M SROIE UniLMv2(Bao 등, 2020) 94.T 355M LayoutLMv1(Xu 등, 2021) LayoutLMv2(Xu 등, 2021) 95.24 95.24 95.99.04 96.61 97.T+L 343M T+L+I ResNeXt101-FPN 426M FormNetV2(저희) 98.98.98.T+L+I 3층 ConvNet 204M Payment NeuralScoring(Majumder 등, 2020) FormNetV1(Lee 등, 2022) 87.T+L 92.91.69 92.FormNetV2(저희) 94.11 95.71 94.T+L T+L+I 217M 3층 ConvNet 204M 표 1: 4가지 표준 벤치마크에 대한 엔티티 수준의 정밀도, 재현율 및 F1 점수 비교. “T/L/I❞는 &quot;텍스트/레이아웃/이미지&quot; 모달리티를 나타냅니다. 제안된 FormNetV2는 네 가지 데이터 세트 모두에서 최신 최신 결과를 확립합니다. FormNet V2는 각각 38% 및 55% 크기의 모델을 사용하면서도 최신 DocFormer(Appalaraju et al., 2021) 및 LayoutLMv3(Huang et al., 2022)보다 성능이 훨씬 뛰어납니다. LayoutLMv3(Huang et al., 2022) 및 StructuralLM(Li et al., 2021a)은 실제 응용 프로그램에 덜 실용적인 기준 진실 엔터티 경계 상자를 통합하는 세그먼트 수준 레이아웃 위치를 사용합니다. 그럼에도 불구하고 동일한 프로토콜에 따라 결과를 F1* 열에 보고합니다. 자세한 내용은 부록의 Sec 4.3 및 Sec A.2를 참조하십시오. 엔터티 추출 F1 점수가 낮고 때로는 성능을 저하시키기도 합니다. 이는 양식 문서에 표시된 시각적 패턴이 영어: 여러 실제 객체가 있는 자연스러운 이미지와는 크게 다릅니다. 기존 비전 작업(분류, 감지, 분할)에 대한 모범 사례는 양식 문서 이해에 최적이 아닐 수 있습니다. 미세 조정. 배치 크기가 8인 Adam 옵티마이저를 사용하여 실험에서 다운스트림 엔터티 추출 작업에 대한 모든 모델을 미세 조정합니다. 학습률은 워밍업 없이 0.0001로 설정됩니다. 미세 조정은 가장 큰 코퍼스에서 약 10시간 동안 Tesla V100 GPU에서 수행됩니다. 다른 하이퍼 매개변수는 Lee et al. (2022)의 설정을 따릅니다. 4.3 벤치마크 결과 표 1은 동일한 평가 프로토콜¹을 기반으로 한 결과를 나열합니다. &#39;Xu et al. (2021)의 구현을 따른 FUNSD, CORD 및 SROIE에 대한 Micro-F1; 매크로-F1 for PayF1 점수(%) 87.FormNetV2-family 85.82.FormNetV----- DocFormer-large DocFormer-base LayoutLMv2-large LayoutĹMv3-large LayoutLMv2-base 80.77.LayoutLM-base 75.LayoutLM-large 72.70.SPADE UniLMv2-large300 400 매개변수 수(백만 개)그림 5: FUNSD 벤치마크에서 모델 크기 대 엔터티 추출 F1 점수. FormNetV2 제품군은 다른 최근 접근 방식보다 상당히 우수한 성능을 보입니다. FormNetV2는 DocFormer(84.55%; Appalaraju et al., 2021)보다 2.6배 더 작은 모델을 사용하면서도 가장 높은 F1 점수(86.35%)를 달성합니다. FormNetV2는 더 적은 매개변수를 사용하면서도 FormNetV1(Lee et al., 2022)보다 큰 차이(1.F1)로 성능이 뛰어납니다. 이 분야가 활발하게 성장함에 따라 연구자들은 시스템에 추가 정보(Majumder et al., 2020)를 통합하는 방법을 모색하기 시작했습니다. 예를 들어, LayoutLMv3(Huang et al., 2022)와 StructuralLM(Li et al., 2021a)은 엔터티의 범위를 결정하는 {Begin, Inside, Outside, End, Single} 스키마 정보(Ratinov and Roth, 2009)가 모델에 제공되므로 실제 응용 프로그램에는 덜 실용적입니다. 그럼에도 불구하고 표 1의 열 F1*에 동일한 프로토콜에 따라 결과를 보고합니다. 비교를 위해 기준 엔터티 세그먼트 없이 LayoutLMv3 결과도 보고합니다. 또한 UDoc(Gu et al., 2022a)는 타사 OCR 엔진인 EasyOCR²에서 반환된 추가적인 문단 수준 감독을 사용합니다. 추가적인 PubLayNet(Zhong et al., 2019) 데이터 세트를 사용하여 비전 백본을 사전 학습합니다. UDoc는 또한 다른 작업에서 채택한 공식 분할(800/100) 대신 CORD에서 다른 학습/테스트 분할(626/247)을 사용합니다. ERNIE-mmLayout(Wang et al., 2022b)은 타사 라이브러리 spaCy³를 사용하여 시스템의 Common Sense Enhancement 모듈에 대한 외부 지식을 제공합니다. FUNSD와 CORD의 F1 점수는 외부 지식 없이 85.74%와 96.31%입니다. 위의 논의가 표준 평가 프로토콜을 명확히 하고 모델링 설계 대 추가 정보에서 성능 개선을 분리하는 데 도움이 되기를 바랍니다. 그림 5는 직접 비교할 수 있는 최근 접근 방식에 대한 모델 크기 대 F1 점수를 보여줍니다. 제안된 방법은 F1 점수와 매개변수 효율성 모두에서 다른 접근 방식보다 상당히 우수한 성능을 보입니다. FormNetV2는 DocFormer(84.55%; Appalaraju et al., 2021)보다 38% 크기의 모델을 사용하면서도 가장 높은 F1 점수(86.35%)를 달성합니다. FormNetV2는 또한 더 적은 매개변수를 사용하면서도 FormNetV1(Lee et al., 2022)보다 큰 차이(1.66 F1)로 우수한 성능을 보입니다. 표 1에서 FormNetV2는 각각 55%와 57% 크기의 모델을 사용하면서도 LayoutLMv3(Huang et al., 2022)와 StructuralLM(Li et al., 2021a)보다 상당한 성능 향상을 보입니다. 표에서 우리는 또한 세 가지 모달리티(텍스트+레이아웃+이미지)를 모두 사용하는 것이 일반적으로 두 가지 모달리티(텍스트+레이아웃)를 사용하는 것보다 우수하고, 두 가지 모달리티(텍스트+레이아웃)를 사용하는 것이 다른 접근 방식에서 한 가지 모달리티(텍스트)만 사용하는 것보다 우수하다는 것을 관찰했습니다. 2 https://github.com/JaidedAI/EasyOCR spacy.io 4.4 Ablation 연구 우리는 이미지 모달리티, 그래프 대조 학습 및 분리된 그래프 손상의 효과에 대한 연구를 수행합니다. 이러한 연구의 백본은 512개의 숨겨진 유닛이 있는 4계층 1-어텐션-헤드 GCN 인코더 다음에 오는 4계층 8-어텐션-헤드 ETC 변환기 디코더입니다. 이 모델은 1M IIT-CDIP 하위 집합에서 사전 학습되었습니다. 다른 모든 하이퍼파라미터는 Sec 4.2. 이미지 모달리티 및 이미지 임베더의 효과를 따릅니다. 표 2는 (a) 백본만, (b) 이미지 패치에서 구성된 추가 토큰, (c) 그래프의 모서리에서 추출된 제안된 이미지 기능을 사용한 FormNetV1의 결과를 나열합니다. 네트워크는 이미지 모달리티를 사용한 입력의 영향을 보여주기 위해 MLM으로만 사전 학습되었습니다. (b)는 약간의 Fscore 개선을 제공하지만 기준선 (a)에 비해 32%의 추가 매개변수가 필요합니다. 제안된 (c) 접근 방식은 기준선 (a)에 비해 1% 미만의 추가 매개변수로 상당한 F1 부스트를 달성합니다. 둘째, 보다 진보된 이미지 임베더(He et al., 2016)의 성능이 여기에서 사용된 3층 ConvNet보다 떨어지는 것을 발견했는데, 이는 이러한 방법이 이미지 모달리티를 활용하는 데 효과적이지 않을 수 있음을 시사합니다. 그럼에도 불구하고 결과는 멀티모달 입력의 일부로서 이미지 모달리티의 중요성을 보여줍니다. 다음으로 그래프 대조 학습을 통해 효과적인 멀티모달 사전 학습 메커니즘의 중요성을 검증합니다. 방법 FormNet V82.95.82.95.FUNSD CORD #Params 81.7M 107.0M 83.95.82.3M FormNet V1+이미지 패치 FormNetV1+에지 이미지(저희) 표 2: 다양한 이미지 모달리티 설정을 사용한 F1. 그래프 대조 학습의 효과. 제안된 멀티모달 그래프 대조 학습의 그래프 손상 단계(그림 4)는 토폴로지와 피처 수준 모두에서 원래 그래프를 손상시켜야 합니다. 손상이 에지, 에지 피처, 노드 피처 등 여러 곳에서 발생한다는 점을 고려하면, 순진한 그래프 손상 구현은 모든 곳에서 동일한 삭제율 값을 사용하는 것입니다. 그림 6(a)(b)에서는 그래프 대조 사전 학습 중에 삭제율 값을 변경하여 FUNSD 및 CORD 데이터 세트에서 다운스트림 엔터티 추출 F1 점수를 보여줍니다. 선택된 4 32x32 이미지 패치 크기로 실험하여 모델에 256개의 추가 이미지 토큰을 추가합니다. Edge &amp; Node Feature Dropping Rate --MLM ◆ MLM+GCL(당사) 84.83.96.96.--MLM ◆ MLM+GCL(당사) 84.84.Edge &amp; Node Feature Dropping Rate 0.9 0.8 0.96.AMTI 83.82.82.0.1 0.3 0.5 0.7 0.일반적인 Dropping Rate (a) FUNSD 96.95.95.83.83.82.0.7 0.0.0.1 0.3 0.일반적인 Dropping Rate 0.96.96.96.95.0.0.0.■ 0.0.8 0.0.(b) CORD Edge Dropping Rate (c) FUNSD Edge Dropping Rate (d) CORD 그림 6: FUNSD 및 CORD 벤치마크. (a)(b)는 모달리티 전반에 걸쳐 동일한 드롭률을 사용한 결과를 보여줍니다. 제안된 멀티모달 그래프 대조 학습은 거의 모든 드롭률에서 MLM 사전 학습을 개선합니다. (c)(d)는 모달리티 전반에 걸쳐 다른 드롭률을 사용한 결과를 보여줍니다. 분리된 드롭 메커니즘은 분리되지 않은 대응물에 비해 F1 점수를 더욱 높일 수 있습니다. 논의는 4.4절을 참조하세요. 드롭률은 앞서 언급한 모든 곳에서 공유됩니다. _ 결과는 제안된 멀티모달 그래프 대조 학습이 광범위한 드롭률에서 즉시 작동한다는 것을 보여줍니다. 토폴로지 수준과 기능 수준에서 멀티모달 손상의 필요성을 보여줍니다. 모델이 MLM과 제안된 그래프 대조 학습을 통해 MLM에서만 사전 학습될 때 각각 FUNSD와 CORD에서 최대 0.66%와 0.64%의 F1 부스트를 가져옵니다. 우리의 방법은 또한 다른 드롭률의 교란에도 안정적입니다. 극단적인 드롭률을 사용하면 성능 향상이 적거나 전혀 없습니다. 예를 들어, 10%의 에지와 피처를 삭제하거나 90%의 에지와 피처를 삭제합니다. 직관적으로 너무 적거나 너무 많은 정보를 삭제하면 노드 컨텍스트가 변경되지 않거나 효과적인 대조 학습을 위해 서로 다른 손상된 그래프에서 너무 적은 노드 컨텍스트가 남습니다. 분리된 그래프 손상의 효과. 이 연구에서는 그래프 손상의 다른 위치에서 삭제율을 분리하면 사전 학습 중에 더 나은 표현을 학습하고 다운스트림 엔터티 추출 작업을 더욱 개선할 수 있는지 조사합니다. 구체적으로, 에지 수준에서 에지, 레이아웃 및 이미지 피처, 노드 수준에서 텍스트 피처의 네 가지 다른 위치에 대해 다른 삭제율을 선택합니다. 피처 수준(레이아웃, 이미지, 텍스트)에서 손상된 그래프 중 하나가 특정 피처에 대해 삭제율 p를 선택하면 다른 손상된 그래프는 3.3절에서 소개한 것과 같이 동일한 피처에 대해 선택된 삭제율 1 p의 보수를 사용합니다. 이 귀납적 다중 모드 대조 설계는 두 개의 손상된 뷰 사이의 피처에 대한 확률적으로 불균형한 정보 액세스를 생성합니다. 그것은 다른 뷰에서 노드 수준에서 더 다양한 맥락을 제공하고 대조적 목적의 최적화를 더 어렵게 만들어 이상적으로는 세 가지 모달리티 간에 더 의미적으로 의미 있는 표현을 생성합니다.그림 6(c)(d)는 세 가지 다른 에지 삭제 속도와 세 가지 다른 피처 삭제 속도로 사전 학습하여 FUNSD 및 CORD 데이터 세트에서 다운스트림 엔터티 추출 F1 점수를 보여줍니다.다양한 수준에서 삭제 속도를 분리하면 두 데이터 세트 모두에서 성능이 더욱 향상되고 분리된 삭제 속도가 비분리된 삭제 속도보다 사용될 때 FUNSD 및 CORD에서 각각 0.34% 및 0.07%의 Fboost가 추가로 향상되는 것을 관찰합니다.또한 에지 수준과 피처 수준에서 서로 다른 삭제 속도 간에 비선형 상호 작용이 관찰됩니다.가장 성능이 좋은 피처 삭제 속도는 다른 에지 삭제 속도가 적용될 때 최적이 아닐 수 있습니다.이는 주목할 만하지만 놀라운 동작은 아닙니다.다른 에지 삭제 속도는 그래프 토폴로지(따라서 노드 임베딩)를 크게 변경하기 때문입니다. 그래프 토폴로지가 변경될 때 두 손상된 그래프 사이의 노드 컨텍스트의 일치를 최대화하는 데 필요한 정보의 양이 다를 것으로 예상합니다. 그럼에도 불구하고 낮은 에지 삭제율(예: 0.3)이 일반적으로 높은 에지 삭제율보다 성능이 더 좋다는 것을 발견했으며, 따라서 최종 설계에서 낮은 에지 삭제율을 선택합니다. 시각화. MLM만 사전 학습하고 MLM+GCL로만 사전 학습했지만 미세 조정하기 전인 CORD 예제의 로컬-로컬 어텐션 점수를 그림 7(a)에서 시각화합니다(Vig, 2019). GCL을 사용하면 모델이 더 의미 있는 토큰 클러스터링을 식별하고 다중 모달을 활용할 수 있음을 알 수 있습니다.HEADER QUESTION ANSWER BAT BATY } STB STB STB STB PRO PRO PRO PRO MO MO MO MO 이름: Y SAMS 타일 첫 번째 중간 성 47th TCRC 등록 양식 47th TCRC 등록 양식 이름: Y 기관: BBB 주소: 거리 PO Be BAT BATY Y 타일 첫 번째 중간 성 기관: 주소: 거리 PO Be 우편번호 국가 우편번호 국가 전화: 팩스 # 전화: 팩스: J.STB PROMO [17500] 게스트 이름: 게스트 이름: BAS ཁ ༔ གཽ སྒྱུ ཁ ཙྩུ སྒྱུ ཁ 영어: སྒྲ ྗBASY.B.BATBAS BAS Continental Breakfam on Tuesday Breakfast on Wednesday BASO PROM Fall Foliage Tour on Tuesday Craftsman Village Tour on Wednesday Continental Breakfan on Tuesday Fall Foliage Tour on Tuesday Breakfast on Wednesday Craftman Village Tour on Wednesday PRO TOTALPRO PRO M MCASHM 사전 등록 수수료: (9월 1일 이전) 늦은 등록 수수료: (9월 1일 이후) $135.사전 등록 수수료: (9월 1일 이전) $135.$150.늦은 등록 수수료: (9월 1일 이후) $150.추가 총계총계MLM 현금현금 지상 톤 라운드 i $30.추가 연회 티켓 @$40.지상 교통(왕복) $30.총계총계} 녹스빌 공항에서 왕복 교통 수단이 필요한 경우 이 부분을 작성해 주십시오. 참여자 수: 녹스빌 공항에서 오고 가는 교통수단이 필요한 경우 이 부분을 작성해 주십시오.참여자 수: 날짜 시간 항공사 항공편 # 날짜 시간 항공사 실제 결과 항공편 # 입력 이미지 MLM + GCL(저희) (a) GCL 포함 및 미포함 FormNetV2 출력의 주의 점수 (b) 어려운 사례에 대한 모델 출력.그림 7: (a) 미세 조정 전 CORD의 MLM 및 MLM+GCL(그래프 대조 학습) 모델에 대한 주의 점수.제안된 GCL로 사전 학습한 경우 모델은 더 의미 있는 토큰 클러스터링을 식별하여 다중 모드 입력을 효과적으로 활용할 수 있습니다.(b) 모델 예측이 인간이 주석한 실제 결과와 일치하지 않는 어려운 사례.이 시각화에서는 의견 불일치만 강조하여 더 효과적으로 표현할 수 있습니다.또한 그림 7(b)에서 인간이 주석한 실제 결과와 일치하지 않는 샘플 모델 출력도 보여줍니다. 이 모델은 양식 상단의 &#39;헤더&#39;와 &#39;기타&#39;를 혼동하고 양식 하단의 객관식 질문에 대한 &#39;질문&#39;과 &#39;답변&#39;을 혼동합니다. 자세한 시각화는 부록 5의 그림 9에서 찾을 수 있습니다.
--- CONCLUSION ---
FormNetV2는 인접한 토큰 쌍으로 경계가 지정된 이미지 피처와 손상된 두 버전의 입력 그래프의 멀티모달 토큰 표현을 구별하는 방법을 학습하는 그래프 대조 목적으로 기존의 강력한 FormNetV1 백본을 확장합니다. 중앙 집중화된 디자인은 멀티모달 형태 이해에 대한 이해에 새로운 빛을 비춥니다. 6 제한 사항 저희의 작업은 훈련 및 테스트 세트에 동일한 사전 정의된 엔터티 목록이 포함된다는 일반적인 가정을 따릅니다. 추가 또는 필요한 수정 없이 모델의 few-shot 또는 zero-shot 기능은 제한될 것으로 예상됩니다. 향후 작업에는 사전 훈련과 미세 조정을 동일한 쿼리 기반 절차로 통합하기 위한 프롬프트 기반 아키텍처를 탐색하는 것이 포함됩니다. 7 윤리 고려 사항 저희는 ACL 윤리 강령을 읽고 편집했습니다. 제안된 FormNetV2는 널리 퍼진 대규모 사전 훈련 후 미세 조정 프레임워크를 따릅니다. 모든 실험에서 사전 훈련을 위해 표준 IITCDIP 데이터 세트를 사용하지만 제안된 방법은 사전 훈련을 위해 특정 데이터 세트를 사용하는 데 국한되지 않습니다. 따라서 사전 학습 데이터의 편향 및 개인 정보 보호 고려 사항과 같은 기존 대규모 언어 모델과 동일한 잠재적 우려 사항을 공유합니다. 대중에게 공개되는 애플리케이션을 위해 사전 학습 데이터를 준비할 때 엄격하고 신중한 프로토콜을 따르는 것이 좋습니다. 참고문헌 Milan Aggarwal, Hiresh Gupta, Mausoom Sarkar, Balaji Krishnamurthy. 2020. Form2seq: 고차 형식 구조 추출을 위한 프레임워크. EMNLP에서. Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang. 2020. 기타: 변환기에서 길고 구조화된 데이터 인코딩. EMNLP에서. Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, R Manmatha. 2021. Docformer: 문서 이해를 위한 엔드투엔드 변환기. ICCV에서. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: 통합 언어 모델 사전 학습을 위한 의사 마스크 언어 모델. ICML에서. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton. 2020. 시각적 표현의 대조 학습을 위한 간단한 프레임워크. ICML에서. Laura Chiticariu, Yunyao Li, Frederick Reiss. 2013. 규칙 기반 정보 추출은 죽었다! 규칙 기반 정보 추출 시스템 만세! EMNLP에서. Adrian Cosma, Mihai Ghidoveanu, Michael PanaitescuLiess, Marius Popescu. 2020. 문서 이미지에 대한 자기 지도 표현 학습. 국제 문서 분석 시스템 워크숍, 103-117페이지. Springer. Timo I Denk 및 Christian Reisswig. 2019. Bertgrid: 2D 문서 표현 및 이해를 위한 문맥화된 임베딩. arXiv 사전 인쇄본 arXiv:1909.04948. Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. NAACL-HLT에서. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929. Powalski, Tomasz Łukasz Garncarek, Rafał Stanisławek, Bartosz Topolski, Piotr Halama, Michał Turski, and Filip Graliński. 2020. Lambert: 정보 추출을 위한 레이아웃 인식(언어) 모델링. arXiv 사전 인쇄본 arXiv:2002.08087. Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Nikolaos Barmpalios, Rajiv Jain, Ani Nenkova, and Tong Sun. 2022a. 문서 이해를 위한 통합 사전 학습 프레임워크. arXiv 사전 인쇄본 arXiv:2204.10939. Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu, and Liqing Zhang. 2022b. Xylayoutlm: 시각적으로 풍부한 문서 이해를 위한 레이아웃 인식 멀티모달 네트워크를 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 45834592페이지. Jaekyu Ha, Robert M Haralick, Ihsin T Phillips. 1995. 연결 구성 요소의 경계 상자를 사용한 재귀적 xy 절단. ICDAR에서. Kaveh Hassani와 Amir Hosein Khasahmadi. 2020. 그래프에서 대조적 다중 뷰 표현 학습. 기계 학습 국제 컨퍼런스에서. PMLR. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. 2017. Mask r-cnn. ICCV에서. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016. 이미지 인식을 위한 심층적 잔여 학습. CVPR에서. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3: 통합 텍스트 및 이미지 마스킹을 사용한 문서 AI 사전 학습. 제30회 ACM 국제 멀티미디어 컨퍼런스 논문집. Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. 2019. 스캔 영수증 OCR 및 정보 추출에 대한 Icdar2019 경연 대회. ICDAR에서. Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo. 2021. 반구조화된 문서 정보 추출을 위한 공간 종속성 구문 분석. ACL-IJCNLP에서(Findings). Guillaume Jaume, Hazim Kemal Ekenel, and JeanPhilippe Thiran. 2019. Funsd: 노이즈가 있는 스캔 문서에서 양식 이해를 위한 데이터 세트. ICDAROST에서. Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Höhne, Jean Baptiste Faddoul. 2018. Chargrid: 2D 문서를 이해하기 위해. EMNLP에서. Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeong Yeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park. 2022. OCR 없는 문서 이해 변환기. European Conference on Computer Vision에서, 498-517쪽. Springer. Frank Lebourgeois, Zbigniew Bublinski, Hubert Emptoz. 1992. 제약 없는 문서에서 텍스트 문단과 그래픽을 추출하는 빠르고 효율적인 방법. ICPR에서. Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, Tomas Pfister. 2022. Formnet: 양식 문서 정보 추출에서 순차적 모델링을 넘어서는 구조적 인코딩. ACL에 게재. Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, Siyang Qin, Ashok Popat, Tomas Pfister. 2021. Rope: 그래프 기반 문서 정보 추출을 위한 읽기 순서 등가 위치 인코딩. ACL-IJCNLP에 게재. David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David Grossman, Jefferson Heard. 2006. 복잡한 문서 정보 처리를 위한 테스트 컬렉션 구축. 정보 검색 연구 및 개발에 관한 제29회 국제 ACM SIGIR 연례 컨퍼런스 회의록에 게재. Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si. 2021a. Structurallm: 형태 이해를 위한 구조적 사전 학습. ACL에서. Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei. 2022. Dit: 문서 이미지 변환기를 위한 자체 감독 사전 학습. arXiv 사전 인쇄본 arXiv:2203.02378. Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, Hongfu Liu. 2021b. Selfdoc: 자체 감독 문서 표현 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5652-5660페이지. Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. 2019. 그래프 구조 객체의 유사성을 학습하기 위한 그래프 매칭 네트워크. 국제 기계 학습 컨퍼런스, 3835-3845쪽. PMLR. Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui Ding. 2021c. Structext: 다중 모달 변환기를 사용한 구조화된 텍스트 이해. 제29회 ACM 국제 멀티미디어 컨퍼런스 회의록, 1912-1920쪽. Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. 객체 감지를 위한 피처 피라미드 네트워크. CVPR에서. Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren, Qiang Huo. 2021. Vibertgrid: 문서에서 핵심 정보 추출을 위한 공동 훈련된 다중 모달 2D 문서 표현. 국제 문서 분석 및 인식 컨퍼런스, 548-563페이지. Springer. Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork. 2020. 양식과 유사한 문서에서 정보 추출을 위한 표현 학습. ACL에서. Simone Marinai, Marco Gori, Giovanni Soda. 2005. 문서 분석 및 인식을 위한 인공 신경망. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션. Lawrence O&#39;Gorman. 1993. 페이지 레이아웃 분석을 위한 문서 스펙트럼. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션. Aaron van den Oord, Yazhe Li, Oriol Vinyals. 2018. 대조적 예측 코딩을 통한 표현 학습. arXiv 사전 인쇄본 arXiv:1807.03748. Rasmus Berg Palm, Ole Winther, Florian Laws. 2017. Cloudscan-순환 신경망을 사용한 구성 없는 송장 분석 시스템. ICDAR에 게재. Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, Hwalsuk Lee. 2019. Cord: 사후 OCR 분석을 위한 통합 영수증 데이터 세트. NeurIPS 2019에서 열린 문서 인텔리전스 워크숍에 게재. Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michał Pietruszka, Gabriela Pałka. 2021. 텍스트-이미지-레이아웃 변환기를 사용한 문서 이해에 대한 풀 틸트 부기. ICDAR에서. Subhojeet Pramanik, Shashank Mujumdar, Hima Patel. 2020. 문서 표현 학습을 위한 다중 모달, 다중 작업 학습 기반 사전 학습 프레임워크를 향해. arXiv 사전 인쇄본 arXiv:2009.14457. Lev Ratinov와 Dan Roth. 2009. 명명된 엔터티 인식의 설계 과제와 오해. Computational Natural Language Learning(CoNLL) 컨퍼런스에서. Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 2015. 더 빠른 r-cnn: 영역 제안 네트워크를 사용한 실시간 객체 감지를 향해. 신경 정보 처리 시스템의 발전. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet 대규모 시각 인식 챌린지. IJCV. Anikó Simon, JC Pret, and A Peter Johnson. 1997. 하향식 문서 레이아웃 분석을 위한 빠른 알고리즘. IEEE Transactions on Pattern Analysis and Machine Intelligence. Kihyuk Sohn. 2016. 다중 클래스 n-페어 손실 목적을 사용한 개선된 심층적 메트릭 학습. 신경 정보 처리 시스템의 발전. Wilson L Taylor. 1953. &quot;cloze 절차&quot;: 가독성을 측정하는 새로운 도구. 저널리즘 분기별. Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm. 2019. Deep graph infomax. ICLR. Jesse Vig. 2019. 변압기 모델에서 주의의 다중 스케일 시각화. ACL: System Demonstrations에서. Jiapeng Wang, Lianwen Jin, Kai Ding. 2022a. Lilt: 구조화된 문서 이해를 위한 간단하면서도 효과적인 언어 독립적 레이아웃 변환기. arXiv 사전 인쇄본 arXiv:2202.13669. Wenjin Wang, Zhengjie Huang, Bin Luo, Qianglong Chen, Qiming Peng, Yinxu Pan, Weichong Yin, Shikun Feng, Yu Sun, Dianhai Yu, et al. 2022b. Ernie-mmlayout: 다중 그레인 문서 이해를 위한 멀티모달 변환기. 제30회 ACM 국제 멀티미디어 컨퍼런스의 회의록. Zifeng Wang, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong Su, Hao Zhang, Jennifer Dy, Vincent Perot, Tomas Pfister. 2022c. Queryform: 간단한 제로샷 폼 엔터티 쿼리 프레임워크. arXiv 사전 인쇄본 arXiv:2211.07730. Mengxi Wei, Yifan He, Qiong Zhang. 2020. 사전 학습된 언어 모델을 사용하여 시각적으로 풍부한 문서를 위한 견고한 레이아웃 인식 ie. 제43회 국제 ACM SIGIR 정보 검색 연구 및 개발 컨퍼런스 회의록, 2367-2376페이지. Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, Ion Stoica. 2021. 글로벌 어텐션을 갖춘 그래프 신경망에 대한 장거리 컨텍스트 표현. 신경 정보 처리 시스템의 발전, 34:13266-13279. Zhirong Wu, Yuanjun Xiong, Stella X Yu, Dahua Lin. 2018. 비모수적 인스턴스 구별을 통한 비지도 특징 학습. CVPR에서. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2021. Layoutlmv2: 시각적으로 풍부한 문서 이해를 위한 다중 모달 사전 학습. ACL-IJCNLP에서. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou. 2020. Layoutlm: 문서 이미지 이해를 위한 텍스트 및 레이아웃 사전 학습. KDD에서. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. 증강을 통한 그래프 대조 학습. 신경 정보 처리 시스템의 발전. Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang. 2019. Cutie: 합성곱 범용 텍스트 정보 추출기를 사용하여 문서를 이해하는 방법. ICDAR에 실림. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. Publaynet: 문서 레이아웃 분석을 위한 역대 최대 규모의 데이터 세트. ICDAR에 실림. Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. 2021. 그래프 대조 학습에 대한 실증적 연구. arXiv 사전 인쇄본 arXiv:2109.01116. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. 심층 그래프 대조 표현 학습. arXiv 사전 인쇄본 arXiv:2006.04131. A 부록 A.1 이미지 임베더 아키텍처 이미지 임베더는 필터 크기가 {32, 64, 128}이고 커널 크기가 전체적으로 3인 3층 ConvNet입니다. 중간 계층에서는 스트라이드 2를 사용하고 다른 모든 계층에서는 스트라이드 1을 사용합니다. 입력 문서 이미지의 크기를 종횡비는 고정하고 배경 영역에는 패딩을 0으로 하여 512×512로 조정합니다. 전체 입력 이미지의 밀집 특징을 추출한 후 GCN 에지로 연결된 토큰 쌍을 결합하는 경계 상자 내에서 특징 RoI 풀링(He et al., 2017)을 수행합니다. 풀링된 영역의 높이와 너비는 각각 3과 16으로 설정됩니다. 마지막으로 풀링된 특징은 필터 크기가 {64, 32, 16}이고 커널 크기가 전체적으로 3인 또 다른 3층 ConvNet을 거칩니다. 첫 번째 2개 레이어에서는 수평으로 스트라이드를 사용하고 다른 모든 곳에서는 스트라이드를 사용합니다. 백본 모델에서 이미지 모달리티를 사용하기 위해 그림 2에서와 같이 GCN의 에지 수준에서 풀링된 이미지 피처를 기존 레이아웃 피처와 간단히 연결합니다. A.2 추가 구현 세부 정보 LayoutLMv3(Huang et al., 2022)의 기본 및 대형 버전을 사용하여 FUNSD 및 CORD에 대한 추가 실험을 수행합니다. 실제 결과에서 추론된 엔터티 세그먼트 인덱스를 사용하는 대신 OCR에서 제공하는 단어 상자를 사용합니다. 모델이 세그먼트 수준이 아닌 단어 수준 상자 정보에 액세스할 때 상당한 성능 저하가 관찰됩니다. 결과는 표 3에 나와 있습니다. 방법 FUNSD CORD 96.LayoutLMv3-base 설정 보고됨 90.Reproduced 90.95.Word 상자 78.95.92.97.92.96.82.95.LayoutLMv3-large 보고됨 Reproduced Word 상자 표 3: 엔티티 세그먼트 인덱스(Reproduced) 또는 단어 수준 인덱스(Word 상자)가 있는 LayoutLMv3 결과. 모델이 세그먼트 수준 대신 단어 수준 상자 정보에 액세스할 때 상당한 성능 저하를 관찰합니다. A.3 예비 사항 FormNetV1(Lee et al., 2022)은 문서 엔티티 추출 작업을 근본적으로 텍스트 중심으로 프레이밍하여 단순화한 다음, 이로 인해 즉시 발생하는 github.com/Jyouhou/unilm-test 문제를 해결하려고 합니다. 직렬화된 양식은 매우 길 수 있으므로 FormNetV1은 로컬 어텐션 윈도우(ETC)를 백본으로 하는 트랜스포머 아키텍처를 사용하여 어텐션의 2차 메모리 비용을 해결합니다. 이 시스템 구성 요소는 텍스트 모달리티를 효과적으로 포착합니다. OCR 직렬화는 또한 의미적 관련성의 강력한 단서를 왜곡합니다. 다른 단어 바로 위에 있는 단어는 관련이 있을 수 있지만 상위 단어의 오른쪽이나 하위 단어의 왼쪽에 토큰이 많은 경우 직렬화 후 두 단어 사이에 개입하고 모델은 근처 토큰이 관련이 있는 경향이 있다는 휴리스틱을 활용할 수 없습니다. 이를 해결하기 위해 FormNetV1은 어텐션 메커니즘을 Rich Attention을 사용하여 토큰 간의 공간 관계를 모델링합니다. Rich Attention은 문서를 조회 테이블의 개별 임베딩과 관련된 영역으로 양자화하지 않고도 저수준 공간적 특징에 대한 어텐션을 조절하는 수학적으로 타당한 방법입니다. 이를 통해 시스템은 로컬 어텐션 윈도우 내에 있는 토큰에 대한 레이아웃 모달리티에서 강력한 표현을 구축할 수 있습니다. 마지막으로, Rich Attention이 로컬 어텐션의 잠재력을 극대화하는 반면, 두 개의 관련 토큰 사이에 너무 많은 개입자가 있어서 로컬 어텐션 창에 포함되지 않고 서로 전혀 주의를 기울일 수 없는 경우 어떻게 해야 하는지에 대한 문제가 남습니다. 이를 위해 FormNetV에는 변환기 구성 요소로 보낼 텍스트를 직렬화하기 전에 그래프 합성 네트워크(GCN) 문맥화 단계가 포함됩니다. GCN의 그래프는 OCR 직렬화 후 변환기에 공급될 토큰 표현을 구축하기 위해 합성하기 전에 각 토큰에 대해 최대 K개의 잠재적으로 관련된 이웃을 찾습니다. Rich Attention과 달리 &quot;위&quot;, &quot;아래&quot; 및 무한히 많은 정도의 &quot;근접성&quot;과 같은 개념을 직접 학습하는 반면, 이 단계의 그래프는 &quot;이웃임&quot;과 &quot;이웃이 아님&quot;을 넘어서는 공간적 관계를 고려하지 않습니다(그림 1 참조). 이를 통해 네트워크는 로컬 어텐션에 의해 제약을 받는 Rich Attention보다 약하지만 더 완전한 레이아웃 모달리티 그림을 구축할 수 있습니다. Wu et al.(2021)은 유사한 아키텍처가 그래프 학습 과제에서도 유용한 것으로 나타났습니다. 따라서 FormNetV의 세 가지 주요 구성 요소는 서로의 약점을 커버하여 시스템이 에지 수준 레이아웃 Vo V레이아웃 이미지 Vo VVA 그래프 대조 학습 텍스트 Vo V₁ 레이아웃 이미지 텍스트 VIIX를 구성할 수 있도록 전략적으로 표현 능력과 계산 효율성을 상쇄합니다. }}} VImage 확률적 그래프 손상 VText VNode 수준 에지 수준 Vo V₁ 레이아웃 이미지 VText VVBOISE 체계 분류 (a) 사전 학습 (b) 미세 조정 그림 8: (a) 멀티모달 그래프 대조 사전 학습 동안 그래프 토폴로지(에지)와 속성(멀티모달 기능)의 손상으로 인해 입력 그래프에서 두 개의 손상된 그래프가 샘플링됩니다. (b) 작업별 미세 조정 동안에는 원래 입력 그래프만 사용됩니다. 데이터를 전 세계적 정보로 전환 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 ROPER STAR CH 데이터를 전 세계적 정보로 전환 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 팩스 (a) 날짜: 9월 22일, 날짜: 9월 22일, 받는 사람: Ron Milstein 보내는 사람: &quot;JJ&quot; Klein 받는 사람: Ron Milstein 보내는 사람: &quot;JJ&quot; Klein 회사: Lorillard 회사: Lorillard 팩스 번호: (910) 335페이지(표지 포함): 팩스 번호: (910) 335페이지(표지 포함): 년도: (b) 브랜드 이름: (၁) VAR. DESC: 종류 단위 판매: 담배 보고서 양식 팩당 번호: 년도: 브랜드 이름: VAR. DESC: (설명 참조) VARIETY DOLLAR SALES: VARIETY UNIT SALES: 담배 보고서 양식 팩당 번호: _ (설명 참조) VARIETY DOLLAR SALES: 독성 위험에 대한 의사결정 트리 추정 독성 위험에 대한 의사결정 트리 추정 AME JD Ergle 및 RF Dufresne 날짜 8월 14일, AME JD Ergle 및 RF Dufresne GATE 8월 14일, Vanitrope | CH,CH,O OH Vanitrope 구조 CH,CH,O OH 헤더 질문 답변 FormNetV2 출력 기준 진실 그림 9: 모델 예측이 인간이 주석을 단 기준 진실과 일치하지 않는 모호한 사례. 이 시각화에서는 일치하지 않는 엔터티만 보여줍니다. 탐색 각주를 단순화하면서 유용한 표현을 제공합니다. lem은 시각적이기보다는 근본적으로 텍스트적이어야 합니다. 최종 시스템은 표준 마스크 언어 모델링(MLM) 목표를 사용하여 대규모 레이블이 지정되지 않은 양식 문서에서 종단 간 사전 학습되었습니다. A.4 출력 시각화 그림 9는 FUNSD에서 추가 FormNetV2 모델 출력을 보여줍니다. A.5 라이선스 또는 조건 해당 문서에서 IIT-CDIPб, FUNSD7, CORD8 및 SROIE⁹에 대한 라이선스 또는 조건을 참조하세요.6ir.nist.gov/cdip/README.txtguillaumejaume.github.io/FUNSD/work/8 github.com/clovaai/cord/blob/master/LICENSE-CC-BY rrc.cvc.uab.es/?ch=
