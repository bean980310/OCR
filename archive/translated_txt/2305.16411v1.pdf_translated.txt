--- ABSTRACT ---
최근 텍스트-이미지 생성 분야의 발전으로 제로샷 3D 모양 생성에서 상당한 진전이 이루어졌습니다. 이는 스코어 증류를 통해 달성되는데, 스코어 증류는 사전 훈련된 텍스트-이미지 확산 모델을 사용하여 3D 신경 표현의 매개변수(예: Neural Radiance Field(NeRF))를 최적화하는 방법입니다. 기존 방법은 유망한 결과를 보여주지만 종종 인체와 같은 복잡한 모양의 기하학을 보존하지 못합니다. 이러한 과제를 해결하기 위해 최적화 프로세스에 앞서 명시적인 3D 인체를 도입하는 방법인 ZeroAvatar를 제시합니다. 구체적으로, 먼저 단일 이미지에서 매개변수 인체의 매개변수를 추정하고 개선합니다. 그런 다음 최적화하는 동안 포즈를 취한 매개변수 인체를 추가 기하학 제약 조건으로 사용하여 확산 모델과 기본 밀도 필드를 정규화합니다. 마지막으로 UV 가이드 텍스처 정규화 항을 제안하여 보이지 않는 신체 부위의 텍스처 완성을 더욱 안내합니다. 우리는 ZeroAvatar가 최적화 기반 이미지-3D 아바타 생성의 견고성과 3D 일관성을 크게 향상시켜 기존의 제로샷 이미지-3D 방법보다 성능이 뛰어나다는 것을 보여줍니다. 1
--- INTRODUCTION ---
단일 이미지에서 풍부하고 정확한 3D 정보를 추출하는 기능은 사실적이고 몰입감 있는 시각적 경험이 중요한 콘텐츠 제작에서 매우 중요합니다. 아티스트와 디자이너는 객체의 3D 구조와 모양을 자동으로 유추하여 생생한 가상 장면, 캐릭터 및 객체를 효율적으로 생성할 수 있습니다. 콘텐츠 제작 및 AR/VR의 영역을 넘어 단일 이미지에서 3D 기하학과 모양을 인식하는 기능은 더 광범위한 의미를 가지며 로봇 공학과 장면 이해에 중요한 역할을 합니다[40, 46]. 컴퓨터 비전의 최근 발전에도 불구하고 단일 이미지에서 3D 인식은 여전히 어려운 작업입니다. 이는 주로 이미지 투사 프로세스에서 정보가 손실되어 중요한 깊이 신호와 객체 특성이 가려지기 때문입니다. 결과적으로 연구자들은 이 문제를 해결하기 위해 다양한 접근 방식과 알고리즘을 제안하여 단일 이미지에서 정확하고 자세한 3D 정보를 추출하는 것을 목표로 합니다. 단일 이미지에서 3D 재구성은 전통적으로 학습 기반 방법을 통해 접근되었습니다[38, 9, 43]. 이러한 접근 방식에는 입력 이미지를 해당 3D 표현에 매핑하도록 신경망을 훈련하는 것이 포함됩니다. 그러나 이 접근 방식의 과제는 고품질 3D 훈련 데이터가 부족하다는 것입니다. 최근 대규모 데이터에서 훈련된 대규모 언어 모델(LLM)[27]과 텍스트-2D 생성 모델[30, 24, 29]의 발전에 힘입어 새로운 탐색 경로가 열렸으며, 3D 표현에 대한 제로샷 생성의 가능성을 제공합니다. 이러한 유형의 접근 방식[17, 32, 36, 45, 26]은 사전 훈련된 모델에 포함된 사전 정보를 활용하여 객체의 암묵적 3D 표현 매개변수를 최적화하여 재구성된 3D 기하학과 모양의 충실도를 향상시킵니다. Preprint에서 인상적인 결과를 보여줬음에도 불구하고. 검토 중입니다. 입력 이미지 (a) Zero 1-to-(b) Make-it-3D (c) ZeroAvatar(저희의 것) 그림 1: 저희는 단일 뷰 이미지에서 고충실도 3D 아바타를 생성하는 제로 샷 방법인 ZeroAvatar를 제안합니다. ZeroAvatar는 인체 구조를 보존하는 데 있어 기존의 제로 샷 방법보다 상당히 개선되었습니다. Zero 1-to-3은 새로운 각도에서 2D 뷰를 예측하는 반면, Make-it-3D와 ZeroAvatar는 3D 모델을 출력합니다. 이러한 접근 방식은 다양한 객체의 경우 복잡한 포즈를 가진 인간을 정확하게 재구성하는 데 있어 상당한 한계가 있습니다(그림 1). 이러한 방법은 인체 구조를 명시적으로 모델링하지 않기 때문입니다. 이 연구에서 저희는 단일 이미지에서 고충실도 3D 아바타를 생성하는 제로 샷 3D 생성 방법인 ZeroAvatar를 제시합니다. 저희는 여러 전략을 채택하여 기존 작업의 한계를 해결합니다. 먼저, 추정된 인체 모양(매개변수 신체 모델 형태)에서 대략적인 모양을 도출하여 신경 복사장의 평균 밀도 필드(NeRF) [22]를 시작합니다. 그런 다음 포즈를 취한 인체 모델에서 얻은 깊이 정보를 텍스트-이미지 모델(예: 안정 확산 [29])에 대한 추가 조건으로 사용하여 포즈를 취한 인간의 기하학적 특성과 더 잘 일치하는 결과를 생성할 수 있습니다. 나아가, 보이지 않는 점의 모양을 정규화하기 위해 UV 가이드 텍스처를 통합합니다. ZeroAvatar가 생성된 인간의 전반적인 충실도와 사실성을 효과적으로 향상시킨다는 것을 보여줍니다. 주요 기여는 다음과 같이 요약할 수 있습니다. • 사전 학습된 텍스트-이미지 확산 모델을 사전으로 사용하여 단일 이미지에서 충실도 높은 3D 아바타를 만드는 방법인 ZeroAvatar를 제안합니다. • SMPL 바디 모델을 명시적 지오메트리 사전으로 통합하고, depthconditioned score distillation loss와 보이지 않는 바디 파트에 대한 UV-guided prior를 통합함으로써 ZeroAvatar는 생성된 아바타의 지오메트리와 모양을 크게 개선하여 기존의 최첨단 zero-shot 3D 생성 기술을 능가합니다. • ZeroAvatar는 zero-shot text-to-3D 아바타 생성과 같은 애플리케이션을 지원합니다. 사전 학습된 text-to-image 모델에서 생성된 이미지를 발판으로 사용하여 ZeroAvatar가 포즈 또는 텍스트 제어를 통해 3D 아바타를 생성할 수 있음을 보여주며, 이를 통해 다양한 다운스트림 애플리케이션이 가능합니다. 2
--- RELATED WORK ---
2.1 2D 관찰에서 3D 생성 다중 뷰 데이터에서 3D 기하학을 추론하는 것은 고전적인 컴퓨터 비전 문제입니다.이전의 접근 방식은 다중 뷰 또는 다중 모달 퓨전을 수행하는데, 이는 여러 뷰 또는 모달리티의 정보를 결합하여 보다 정확한 3D 표현을 생성하는 것을 포함합니다.그러나 이러한 유형의 접근 방식에서는 관찰의 완전성이 필수적이며, 이를 위해서는 다양한 각도에서 장면을 밀집하여 관찰해야 합니다.최근 몇 년 동안 암묵적 신경 표현(예: 신경 광도장(NeRF))[33, 22]을 통해 이 분야에서 획기적인 진전이 있었으며, 이는 밀집 관찰의 필요성을 줄여줍니다.특히 관절이 있는 물체(예: 사람)의 경우, 이전 작업에서는 단안 비디오[34, 39]와 같은 2D 관찰이나 희소한 2D 관찰[51]에서 포즈 조건화된 NeRF를 학습합니다. 2.2 단일 이미지에서 3D 인간 단일 이미지에서 인간의 자세와 모양을 추정하는 작업을 단일 뷰 인간 메시 복구(HMR)라고 합니다. 일반적으로 모델 기반 HMR에는 학습 기반과 최적화 기반의 두 가지 접근 방식이 있습니다. 학습 기반
--- METHOD ---
사전 학습된 텍스트-이미지 확산 모델을 사용하여 3D 신경 표현의 매개변수를 최적화하는 ology, 예: Neural Radiance Field(NeRF). 유망한 결과를 보여주지만 기존 방법은 종종 인체와 같은 복잡한 모양의 기하학을 보존하지 못합니다. 이러한 과제를 해결하기 위해 최적화 프로세스에 앞서 명시적인 3D 인체를 도입하는 방법인 ZeroAvatar를 제시합니다. 구체적으로, 먼저 단일 이미지에서 매개변수 인체의 매개변수를 추정하고 개선합니다. 그런 다음 최적화하는 동안 포즈된 매개변수 인체를 추가 기하학 제약 조건으로 사용하여 확산 모델과 기본 밀도 필드를 정규화합니다. 마지막으로, 보이지 않는 신체 부위의 텍스처 완성을 더욱 안내하기 위해 UV 가이드 텍스처 정규화 항을 제안합니다. ZeroAvatar가 최적화 기반 이미지-3D 아바타 생성의 견고성과 3D 일관성을 크게 향상시켜 기존의 제로샷 이미지-3D 방법보다 성능이 우수함을 보여줍니다. 1 서론 단일 이미지에서 풍부하고 정확한 3D 정보를 추출하는 기능은 사실적이고 몰입감 있는 시각적 경험이 중요한 콘텐츠 제작에서 매우 중요합니다. 아티스트와 디자이너는 객체의 3D 구조와 모양을 자동으로 추론하여 생생한 가상 장면, 캐릭터 및 객체를 효율적으로 생성할 수 있습니다. 콘텐츠 제작 및 AR/VR의 영역을 넘어 단일 이미지에서 3D 기하학과 모양을 인식하는 기능은 더 광범위한 의미를 가지며 로봇 공학과 장면 이해에 중요한 역할을 합니다[40, 46]. 컴퓨터 비전의 최근 발전에도 불구하고 단일 이미지에서 3D 인식은 여전히 어려운 작업입니다. 이는 주로 이미지 투사 프로세스에서 정보가 손실되어 중요한 깊이 신호와 객체 특성이 가려지기 때문입니다. 결과적으로 연구자들은 이 문제를 해결하기 위해 다양한 접근 방식과 알고리즘을 제안하여 단일 이미지에서 정확하고 자세한 3D 정보를 추출하는 것을 목표로 합니다. 단일 이미지에서 3D 재구성은 전통적으로 학습 기반 방법을 통해 접근되었습니다[38, 9, 43]. 이러한 접근 방식에는 입력 이미지를 해당 3D 표현에 매핑하도록 신경망을 훈련하는 것이 포함됩니다. 그러나 이 접근 방식의 과제는 고품질 3D 훈련 데이터가 부족하다는 것입니다. 최근 대규모 데이터에서 훈련된 대규모 언어 모델(LLM)[27]과 텍스트-2D 생성 모델[30, 24, 29]의 발전에 힘입어 새로운 탐색 경로가 열렸으며, 3D 표현에 대한 제로샷 생성의 가능성을 제공합니다. 이러한 유형의 접근 방식[17, 32, 36, 45, 26]은 사전 훈련된 모델에 포함된 사전 정보를 활용하여 객체의 암묵적 3D 표현 매개변수를 최적화하여 재구성된 3D 기하학과 모양의 충실도를 향상시킵니다. Preprint에서 인상적인 결과를 보여줬음에도 불구하고. 검토 중입니다. 입력 이미지 (a) Zero 1-to-(b) Make-it-3D (c) ZeroAvatar(저희의 것) 그림 1: 저희는 단일 뷰 이미지에서 고충실도 3D 아바타를 생성하는 제로 샷 방법인 ZeroAvatar를 제안합니다. ZeroAvatar는 인체 구조를 보존하는 데 있어 기존의 제로 샷 방법보다 상당히 개선되었습니다. Zero 1-to-3은 새로운 각도에서 2D 뷰를 예측하는 반면, Make-it-3D와 ZeroAvatar는 3D 모델을 출력합니다. 이러한 접근 방식은 다양한 객체의 경우 복잡한 포즈를 가진 인간을 정확하게 재구성하는 데 있어 상당한 한계가 있습니다(그림 1). 이러한 방법은 인체 구조를 명시적으로 모델링하지 않기 때문입니다. 이 연구에서 저희는 단일 이미지에서 고충실도 3D 아바타를 생성하는 제로 샷 3D 생성 방법인 ZeroAvatar를 제시합니다. 저희는 여러 전략을 채택하여 기존 작업의 한계를 해결합니다. 먼저, 추정된 인체 모양(매개변수 신체 모델 형태)에서 대략적인 모양을 도출하여 신경 복사장의 평균 밀도 필드(NeRF) [22]를 시작합니다. 그런 다음 포즈를 취한 인체 모델에서 얻은 깊이 정보를 텍스트-이미지 모델(예: 안정 확산 [29])에 대한 추가 조건으로 사용하여 포즈를 취한 인간의 기하학적 특성과 더 잘 일치하는 결과를 생성할 수 있습니다. 나아가, 보이지 않는 점의 모양을 정규화하기 위해 UV 가이드 텍스처를 통합합니다. ZeroAvatar가 생성된 인간의 전반적인 충실도와 사실성을 효과적으로 향상시킨다는 것을 보여줍니다. 주요 기여는 다음과 같이 요약할 수 있습니다. • 사전 학습된 텍스트-이미지 확산 모델을 사전으로 사용하여 단일 이미지에서 충실도 높은 3D 아바타를 만드는 방법인 ZeroAvatar를 제안합니다. • ZeroAvatar는 SMPL 바디 모델을 명시적 기하 사전으로 통합하고, depthconditioned score distillation loss와 보이지 않는 바디 파트에 대한 UV 가이드 사전을 함께 사용하여 생성된 아바타의 기하와 모양을 크게 개선하여 기존의 최첨단 zero-shot 3D 생성 기술을 능가합니다.• ZeroAvatar는 zero-shot text-to-3D 아바타 생성과 같은 애플리케이션을 지원합니다.우리는 사전 훈련된 text-to-image 모델에서 생성된 이미지를 디딤돌로 사용하여 ZeroAvatar가 포즈 또는 텍스트 제어를 통해 3D 아바타를 생성할 수 있음을 보여주며, 이를 통해 광범위한 다운스트림 애플리케이션이 가능합니다.2 관련 연구 2.1 2D 관찰에서 3D 생성 다중 뷰 데이터에서 3D 기하를 유추하는 것은 고전적인 컴퓨터 비전 문제입니다.이전 접근 방식은 다중 뷰 또는 다중 모달 퓨전을 수행하는데, 이는 여러 뷰 또는 모달리티의 정보를 결합하여 더 정확한 3D 표현을 생성하는 것을 포함합니다. 그러나 이러한 유형의 접근 방식에서는 관찰의 완전성이 필수적이며, 다양한 각도에서 장면을 밀집해서 관찰해야 합니다.최근 몇 년 동안 암묵적 신경 표현(예: 신경 광도장(NeRF))[33, 22]을 통해 이 분야에서 획기적인 진전이 있었으며, 이를 통해 밀집 관찰의 필요성이 줄었습니다.특히 관절이 있는 물체(예: 사람)의 경우 이전 연구에서는 단안 비디오[34, 39]와 같은 2D 관찰이나 희소한 2D 관찰[51]에서 포즈 조건화된 NeRF를 학습했습니다.2.2 단일 이미지에서 3D 사람 단일 이미지에서 사람의 포즈와 모양을 추정하는 작업을 단일 뷰 인간 메시 복구(HMR)라고 합니다.일반적으로 모델 기반 HMR에는 학습 기반과 최적화 기반의 두 가지 접근 방식이 있습니다. 학습 기반 방법[14]은 대용량 인간 포즈 데이터 세트에서 HMR 모델을 종단 간에 학습시키며, 3D 기준 진실과 결합된 학습 데이터의 부족으로 인해 병목 현상이 발생할 수 있습니다. 결과적으로 어려운 도메인 외부 시나리오의 경우 도메인 갭을 메우기 위해 도메인 적응[10, 41]이 종종 필요합니다. 반면, 최적화 기반 방법[2, 25]은 반복적 피팅 루틴을 사용하여 매개 변수 인간 신체 모델의 신체 포즈와 모양을 추정합니다.IUV 예측 인간 메시 복구 IUV 맵 가려진 부분의 UV 초기화된 지오메트리 NeRF(MLP) 새로운 뷰 UV 손실 깊이 안내 SDS 안정된 확산 깊이 손실 볼륨 렌더링 새로운 뷰 최적화 참조 뷰 재구성 손실 참조 이미지 깊이 손실 깊이 맵 참조 뷰 최적화 그림 2: ZeroAvatar 개요. 단일 이미지가 주어지면 먼저 사람의 신체 포즈, 모양 및 UV 맵을 추정합니다. 추정된 신체 메시를 사용하여 3D 표현의 밀도 필드를 초기화합니다. 그런 다음, 포즈를 취한 신체 모델의 깊이 정보를 텍스트(예: 이미지 캡션) 외에도 컨디셔닝으로 사용하여 Score Distillation Sampling을 사용하여 사람의 모양을 최적화하고 기하학을 개선합니다. 샘플링된 새로운 뷰가 주어졌을 때 최적화할 때 보이지 않는 신체 부위에 추론된 UV를 추가로 사용하여 모양 학습을 돕습니다. 이는 2D 관찰, 예를 들어 2D 관절 위치 또는 인간 실루엣을 가장 잘 설명합니다. 이미지 특징과 모델의 일치를 명시적으로 최적화하기 때문에 신체 모델은 참조 이미지와 잘 정렬됩니다. 모델 기반 인간 복구 외에도 단일 뷰 이미지에서 인간의 체적 표현을 직접 학습하는 작업 라인도 있습니다[31, 44, 43]. 이러한 방법은 종단 간 학습이 필요하므로 사용 가능한 3D 인간 스캔이 부족하여 일반화 기능이 제한적입니다. ZeroAvatar는 입력 이미지와 일관성을 유지하기 위해 인체의 3D 표현을 최적화한다는 점에서 최적화 기반 모델 기반 HMR 방법과 유사합니다. 대조적으로, 우리의 방법은 옷과 밀접하게 상호 작용하는 물체(예: 손에 든 농구공)와 같은 표면 세부 정보를 복구함으로써 모델 기반 방법을 넘어선다. 단일 이미지에서 체적 표현을 예측하는 학습 기반 방법과 비교할 때, 우리의 방법은 더 광범위한 일반화를 보여 실제 인간과 만화 캐릭터와 같은 가상 아바타에서 뛰어난 성능을 보여준다. 이처럼 광범위한 인간을 처리할 수 있는 능력은 가상 캐릭터 생성 및 애니메이션 분야에서 우리 방법의 잠재적인 응용 프로그램과 창의적인 가능성을 확장한다. 2.3 제로샷 3D 생성 기존의 제로샷 3D 생성 방법은 일반적으로 사전 훈련된 시각 언어 모델을 사용한다. Text2Mesh [21] 및 DreamFields [13]와 같은 초기 작업은 사전 훈련된 CLIP 모델 [27]을 지침으로 사용하여 3D 표현의 모양과 기하학(예: 메시 또는 신경 광도장(NeRF))을 최적화한다. 영어: 유사하게 인간 피험자의 경우 CLIP-actor[47] 및 AvatarCLIP[12]은 텍스트 설명을 기반으로 양식화된 인간 아바타를 생성하기 위해 CLIP 가이드 손실을 사용합니다.최근의 접근 방식(예: DreamFusion[26], Magic3D[16], Score Jacobian Chaining[37])은 대규모 확산 모델[30, 29]을 활용하여 생성된 3D 표현의 품질을 새로운 수준으로 끌어올리는 데 상당한 진전을 이루었습니다.이러한 작업은 사전 학습된 텍스트-이미지 확산 모델(예: [30])에서 파생된 스코어 증류 손실을 최적화를 위한 사전으로 사용하는 스코어 증류 샘플링(SDS)의 아이디어를 활용하여 무작위 각도에서 렌더링한 2D 렌더링이 텍스트-이미지 모델에서 생성된 좋은 세대처럼 보이도록 합니다.이러한 선구적인 작업에 따라 최근 작업[36, 17]은 제로 샷 이미지 조건의 3D 생성 문제에 접근했습니다. 이러한 방법은 간단한 기하학을 가진 객체의 고충실도 3D 표현을 복구할 수 있지만, 비교적 더 복잡한 구조의 기하학을 환각하는 데 종종 어려움을 겪습니다.이러한 한계를 극복하기 위해 점점 더 많은 연구[18, 45, 32]에서 기하학 학습을 돕는 보다 강력한 가이드로 모양 사전 확률을 활용하기 시작했습니다.ISS[18]와 Dream3D[45]는 모양 사전 확률을 생성할 때 미세 조정된 이미지-메시 모델을 활용하고 3D Fuse[32]는 기성 모델을 활용하여 장면의 거친 포인트 클라우드를 생성합니다.3 ZeroAvatar 인간 아바타의 참조 이미지 I가 주어지면 ZeroAvatar는 3D 아바타를 나타내는 신경 광도장(NeRF)[22]을 최적화합니다.이를 달성하기 위해 먼저 매개 변수 신체 모델(섹션 3.2)에서 계산된 거친 모양으로 NERF의 평균 밀도장을 초기화합니다. 그런 다음 포즈를 취한 신체 모델의 깊이를 안정 확산 모델에 대한 추가 조건으로 사용하여 포즈를 취한 인간의 기하학에 더 충실한 세대를 생성합니다(섹션 3.3). 또한 인간 텍스처의 공통적인 속성을 활용하기 전에 UV 가이드 텍스처를 사용하여 보이지 않는 점의 모양을 정규화합니다(섹션 3.4). 3.1 배경 신경 광도장(NeRF)[22]은 암시적 함수 F0(√(x)) = (σ(x), c(x)) (1)을 통해 3D 장면을 나타냅니다.여기서 (.)는 주파수 인코더이고 σ와 c는 밀도와 색상이며 매개변수 0이 있는 작은 MLP에서 학습할 수 있습니다.Mildenhall 등의 볼륨 렌더링 방정식을 사용하여 신경장을 렌더링합니다.[22] 각 이미지 픽셀에 대해 픽셀 위치에서 3D 장면으로 광선 r을 투사하고 픽셀 위치의 RGB 값은 D 샘플링된 3D 점 x의 밀도 및 색상 값(Fe로 예측)을 사용하여 계산할 수 있습니다. r을 따라. 형식적으로 색상 C(r)은 Ꭰ C(r) = Wi(a;j로 표현될 수 있습니다.
--- EXPERIMENT ---
s 구현 세부 정보. 효율성을 위해 InstantNGP [23]의 다중 해상도 해시 인코딩을 채택하고 뷰 샘플링 및 셰이딩 증가와 같은 [26]의 여러 가지 설계 선택을 따릅니다. UV 좌표 회귀에는 DensePose [11]를 사용하고 인간 메시 복구에는 PIXIE [7]를 사용합니다. PIXIE는 SMPL-X [25] 모델의 매개변수를 출력하고 최상의 메시 적합성을 나타내는 SMPL 매개변수를 찾아 이를 SMPL 모델 매개변수로 변환합니다. 학습 중 NeRF 렌더링 해상도는 100 x 100이고 시야는 20도로 설정됩니다. 이전 작업 [36]에 따른 점진적 학습을 채택하여 참조 뷰를 중심으로 대칭인 90도 시야의 좁은 범위에서 시작한 다음 학습 중에 범위를 점진적으로 확장하여 360도를 커버합니다. 좁은 뷰는 1,000회 반복으로 학습되고 넓은 뷰는 5,000회 반복으로 학습됩니다. 우리는 기하학과 모양의 과도한 정규화를 피하기 위해 2,000번의 반복 후에 Linvisible-RGB와 LSMPL-깊이를 끕니다.우리는 학습 속도 0.001의 Adam[15] 최적화기를 사용하고 단일 이미지에 대해 6,000번의 반복을 최적화하는데, 이는 단일 NVIDIA TITAN RTX GPU에서 약 50분이 걸립니다.평가 및 측정.우리의 방법은 분포 밖 인간과 실제 진실을 정리하기 어렵고 따라서 학습 기반 방법에는 실행 불가능한 가상 아바타를 재구성하는 데 가장 큰 적용 가능성을 보여줍니다.이러한 이유로 우리는 다양한 가상 아바타와 도전적인 실제 인간이 포함된 27개 이미지 세트에서 ZeroAvatar의 성능을 평가합니다.평가 세트의 사람들은 다양한 외모, 신체 포즈 및 방향을 가지고 있습니다. 표 1에서는 학습된 지각적 이미지 패치 유사성(LPIPS) [50], 맥락적 손실 [20] 및 CLIP [27] 유사도 점수를 보고합니다. LPIPS는 참조 뷰에서 계산됩니다. 맥락적 손실 및 CLIP 유사도 점수는 인간 주위의 360도 뷰에서 계산되며, 각 뷰는 서로 45도 간격으로 배치됩니다. CLIP 유사도는 새로운 뷰와 참조 이미지(또는 텍스트) 간의 의미적 유사도를 측정합니다. 구체적으로, CLIP-I는 CLIP 이미지 임베딩을 사용하여 계산된 새로운 뷰와 참조 뷰 간의 (정규화 코사인) 유사도 점수이고, CLIP-T는 새로운 뷰 이미지 임베딩과 텍스트(즉, 이미지 캡션) 임베딩 간의 유사도입니다. 각 테스트 장면에 대한 모든 메트릭을 계산하고 모든 테스트 장면에 대한 평균을 보고합니다. 표 1: 정량적 결과. 가장 좋은 숫자는 굵은 글씨로 표시되었습니다. (*: Stable-Dream Fusion*은 DreamFusion의 변형으로, 몇 번의 반복마다 참조 뷰에 대한 재구성 손실을 추가로 최소화합니다.) 모델 LPIPS(↓) 문맥별(↓). CLIP-I(1) CLIP-T(1) Stable-DreamFusion* [26] 0.3.73.26.Stable-DreamFusion(Zero 1-to-3 포함) [17] 0.3.76.27.3D Fuse [32] 0.3.81.30.Make-it-3D Coarse [36] 0.3.85.32.ZeroAvatar(저희) 0.3.87.32.Results. 우리는 ZeroAvatar를 최첨단 제로샷 방법(그림 3)인 DreamFusion[26], Zero 1-to-3[17], 3D Fuse[32], Make-it-3D[36]와 비교합니다. DreamFusion[26]의 경우 오픈 소스 구현인 Stable-DreamFusion[35]을 사용합니다. DreamFusion*(열 b)은 DreamFusion의 이미지 조절 변형으로, 몇 번의 반복마다 참조 뷰에 대한 재구성 손실을 추가로 최소화합니다. 열 c에서는 Liu et al.[17]의 사전 학습된 Zero 1-to-model을 감독으로 사용하여 신경 광도장을 최적화합니다. Zero 1-to-는 800K+ 3D 모델[5]이 포함된 대규모 오픈 소스 데이터 세트에서 학습되었으며, 새로운 관점에서 객체가 어떻게 보여야 하는지에 대한 좋은 사전 학습을 합니다. 그러나 일반적으로 인간의 경우 Zero 1-to-3에서 예측한 새로운 뷰는 감독 신호로 충분하지 않습니다.Make-it-3D[36](열 d)는 두 단계의 학습이 있는 최신 최첨단 이미지 조건화 제로샷 방법입니다.두 번째 단계의 코드가 아직 릴리스되지 않았으므로 첫 번째 단계 이후의 결과와 비교합니다.그러나 객체의 기하학은 주로 첫 번째 단계에서 학습되고 두 번째 단계는 텍스처 정제를 위한 것입니다.따라서 단계 I 이후의 결과와 비교하면 학습한 기하학이 그들의 기하학과 어떻게 비교되는지 잘 알 수 있습니다.Make-it-3D는 간단한 기하학(예: 아래 행과 같이 거의 원통형 볼륨)을 가진 인간에 대해 합리적인 기하학을 학습하는 경향이 있음을 관찰했습니다.그러나 더 어려운 포즈를 가진 인간의 경우 기하학은 새로운 뷰에 대해 빠르게 발산하고 텍스트 조건화를 사용한 점수 증류 손실은 기하학을 수정하지 못합니다.새로운 뷰의 퇴화된 기하학은 더 높은 맥락적 손실과 더 낮은 CLIP 점수를 초래합니다. 참조 뷰 ICON 정면도 측면도 ECON ECON + TEXTure ZeroAvatar(저희) 그림 4: 학습 기반 방법과의 비교: ICON[44] 및 ECON[43]. 학습 기반 방법은 훈련 데이터의 다양성이 제한적이기 때문에 일반화 능력이 제한적입니다. 이에 반해 ZeroAvatar는 제로샷 방법으로서 케이프(3번째 행)와 농구공(4번째 행)과 같은 드문 세부 사항을 복구할 수 있습니다. 3D Fuse[32](열 e)는 거친 포인트 클라우드 형태로 3D 인식을 통합하여 학습된 신경 광도장의 견고성과 3D 일관성을 향상시킵니다. 비교를 위해 3D Fuse의 이미지-3D 버전을 사용합니다. 기본 모양을 명시적으로 모델링하기 때문에 3D Fuse는 DreamFusion 및 Make-it-3D에 비해 훨씬 더 나은 지오메트리를 생성합니다. 그러나 표시된 대로 복잡한 포즈(예: 3번째 줄)의 인간에 관해서는 결과가 여전히 자주 Janus 문제(학습된 3D 모델에 여러 얼굴이 있는 경우)에 부딪힙니다.또한 3D Fuse는 이미지 수준 재구성 손실에서 작동하지 않기 때문에 출력 모델이 참조 이미지와 잘 맞지 않습니다.이에 비해 ZeroAvatar는 입력 이미지와 더 일관된 더욱 사실적인 아바타를 지속적으로 생성합니다.정량적으로(표 1), ZeroAvatar는 모든 메트릭에서 DreamFusion, DreamFusion with Zero 1-to-3, 3D Fuse[26, 17, 32]보다 지속적으로 우수한 성과를 보였으며, 이는 이러한 기준선과 비교했을 때 효과적이고 우수함을 보여줍니다.ZeroAvatar는 Make-it-3D[36]와 유사한 메트릭을 달성합니다. 그러나 일부 테스트 이미지의 경우 정량적 지표가 ZeroAvatar와 비슷하거나 더 낮은 반면, 새로운 뷰에 대한 정성적 결과는 학습된 지오메트리가 참조 뷰에 과도하게 맞춤되고 새로운 각도에서 볼 때 크게 왜곡되는 경향이 있는 Make-it-3D [36]보다 상당히 개선되었음을 알 수 있습니다(그림 3). 우리 방법이 인체 구조를 보존하는 데 더 뛰어나다는 것을 더욱 보여주기 위해, 우리는 새로운 뷰에서 기성품 인간 감지기 [8]의 감지 점수를 인간 피험자의 구조적 무결성을 평가하기 위한 대리로 사용합니다. 평균적으로 DreamFusion의 새로운 뷰(이미지 조건화 또는 Zero 1-to-3 포함)는 모두 50% 미만의 감지 점수를 갖습니다. Make-it-3D와 3D Fuse는 각각 67%와 74%를 달성하고 ZeroAvatar는 83%를 달성합니다. 이는 ZeroAvatar가 다른 접근 방식에 비해 인간 피험자의 구조적 무결성을 보존하는 데 더 뛰어나다는 것을 나타냅니다. (a) 심도 조절된 SDS 없음 (b) UV 손실 없음 추가적인 정성적 결과와 애니메이션 결과는 보충 자료에서 찾을 수 있습니다.학습 기반 방법과의 비교.단일 뷰 이미지에서 사람의 체적 표현을 직접 추정하는 연구 분야가 있습니다.ICON[44] 및 ECON[43]과 비교합니다(그림 4).또한 ECON에서 예측한 메시를 사용하여 TEXTure[28]에서 최적화된 텍스처를 포함합니다.ICON과 ECON은 Renderpeople[1]과 같은 3D 스캔에서 종단 간 학습되기 때문입니다.비교 가능한 이미지(예: 실제 사람)에서 인상적인 결과를 얻지만 만화 캐릭터와 분포 밖 시나리오(예: 공을 든 농구 선수)와 같은 가상 아바타에 대한 일반화 기능이 제한적입니다.그림 4에서 볼 수 있듯이 ZeroAvatar가 일반화 기능이 더 뛰어납니다.실제 사람(4번째 행)의 경우 ZeroAvatar는 복잡한 세부 사항을 포착하고 전반적으로 더 나은 지오메트리를 표시하는 데 뛰어난 기능을 보여줍니다.(c) 전체 모델 (a) 삭제된 모델. 참조 이미지 참조 보기 (b) 실패 사례. 측면 보기 모델 절제. 그림 5a에서 방법의 각 구성 요소의 효과를 보여줍니다. 먼저 SMPL 바디 메시의 점유율로만 밀도 필드를 초기화하지만 SDS에 SMPL의 깊이 값을 사용하지 않습니다. 그림 5a(a)에서 볼 수 있듯이 학습된 바디는 종종 비현실적인 해부학적 구조를 보입니다. 둘째, UV 사전 손실을 제거하고 사람의 보이지 않는 측면의 모습이 전체 모델보다 덜 현실적임을 관찰합니다. 응용 프로그램: 제로 샷 텍스트-3D 아바타 생성. ZeroAvatar는 텍스트-이미지 모델과 함께 사용하여 선택적 포즈 제어를 통해 제로 샷 텍스트-3D 기능을 구현할 수 있습니다. Diffusion HPC[42] 및 ControlNet[49]을 사용하여 이러한 응용 프로그램을 보여줍니다. 그림 6의 상단 절반에서 텍스트 프롬프트가 주어지면 Diffusion HPC[42]를 사용하여 참조 이미지로 합성 이미지를 생성한 다음 참조 이미지에 ZeroAvatar를 적용합니다. 표시된 대로 텍스트 프롬프트가 주어진 3D 표현을 직접 최적화하는 기준 DreamFusion 및 3D Fuse와 비교할 때, 우리의 결과는 중간의 사실적인 합성 이미지를 사용함으로써 훨씬 더 높은 충실도를 달성합니다.그림 5: 절제 및 제한 사항.포즈 조건의 텍스트-3D 생성(그림 6의 하단 절반)의 경우, ControlNet[49]을 사용하여 2D 키포인트 형태의 입력 포즈가 주어진 합성 이미지를 생성합니다.우리는 사람의 모양을 정규화하기 위해 이중 공간(즉, 표준 및 관찰된 포즈 공간) 최적화 체계를 채택하는 SDS 기반 방법인 DreamAvatar[3]와 비교합니다.이는 추가적인 계산 비용이 발생하고 GPU에서 단일 모델을 최적화하는 데 약 2시간이 걸립니다.반면에 텍스트-이미지 생성을 발판으로 사용하는 ZeroAvatar는 추가 정규화의 필요성을 완화하고 훨씬 적은 시간(약 50분)으로 비슷하거나 더 나은 충실도를 달성합니다.제한 사항 및 향후 방향. ZeroAvatar는 참조 이미지의 인간 아바타의 기하학이 매개변수화된 신체 모델(즉, SMPL)로 근사될 수 있다고 가정합니다. SMPL 모델은 인간 신체의 평균적인 모양과 포즈 변화를 포착하도록 설계되었습니다. 따라서 인간의 비율이 SMPL로 표현된 모양 공간에서 너무 많이 벗어나는 시나리오에서 결과 추정은 불균형한 표현으로 이어질 수 있습니다(그림 5b). 미래 연구를 위한 유망한 길은 사전에 인간 신체의 일반화 가능성을 향상시키는 것이 될 수 있습니다. 또한 ZeroAvatar가 인간 기하학을 보존하는 뛰어난 기능에도 불구하고 학습된 밀도 필드에서 추출된 3D 메시는 여전히 비교적 거칠습니다. 그럼에도 불구하고, 우리의 작업은 기하학 및 텍스처 해상도를 개선하는 다른 접근 방식[4]과 상승적으로 결합되어 시각적 모양과 기하학적 충실도를 더욱 향상시킬 수 있습니다. &quot;남자가 달리고 있다&quot; &quot;운동선수가 축구를 하고 있다&quot; 입력 텍스트 DreamFusion 3D Fuse 참조. 이미지(확산-HPC) ZeroAvatar(저희) &quot;슈퍼맨&quot; &quot;드래곤볼&quot; 입력 텍스트 입력 포즈 DreamAvatar 참조 이미지(ControlNet) ZeroAvatar(저희) 그림 6: 텍스트-이미지 모델을 디딤돌로 사용하여 텍스트-3D 아바타 생성에 ZeroAvatar를 적용한 예. 위: 텍스트-3D 생성. 아래: 포즈 조건의 텍스트-3D 생성. 텍스트에서 NeRF를 직접 최적화하는 DreamFusion[26], 3D Fuse[32] 및 DreamAvatar[3]와 비교하면, 저희 방법은 합성 이미지를 중간 표현으로 사용하여 텍스트에서 고충실도 3D 모델을 효율적으로 재구성할 수 있습니다. 5
--- CONCLUSION ---
이 작업에서 우리는 사전 훈련된 텍스트-이미지 확산 모델을 사전 학습으로 사용하여 단일 이미지에서 고충실도 3D 아바타를 만드는 제로샷 방법인 ZeroAvatar를 제안했습니다. ZeroAvatar는 최적화 기반 이미지-3D 아바타 생성의 견고성을 크게 향상시키고 3D 일관성을 보장합니다. 포즈를 취한 사람의 이미지에서 ZeroAvatar는 최적화된 기하학과 모양 측면에서 기존의 제로샷 방법을 능가합니다. 또한 ZeroAvatar는 사전 훈련된 텍스트-2D 방법과 완벽하게 결합되어 텍스트 또는 포즈 제어를 사용하여 3D 아바타를 생성하여 광범위한 사용 시나리오와 창의적인 가능성을 허용합니다. 일반적인 영향. ZeroAvatar는 콘텐츠 제작자가 이미지, 텍스트 또는 포즈 제어를 사용하여 3D 인간 아바타 모델을 생성하는 효율적인 방법을 제공합니다. 사전 훈련된 텍스트-이미지 모델을 사용하기 때문에 우리의 접근 방식은 이와 관련된 모든 편향과 제한을 상속받습니다. 개인의 동의 없이 개인의 3D 모델을 생성할 잠재적인 위험도 있습니다. 우리는 사용자가 적절한 사용 관행을 준수할 것을 권장합니다.부록 포즈 복잡도의 효과 정성적으로 ZeroAvatar가 기준선에 비해 가장 큰 개선을 보이는 것을 관찰했습니다.특히 포즈가 표준 포즈에서 크게 벗어날 때(따라서 더 &quot;복잡함&quot;) 그렇습니다.포즈가 최적화 결과에 어떤 영향을 미치는지 더 잘 이해하기 위해 다양한 복잡도를 가진 포즈에 대한 평가를 수행합니다.특히, 사전 학습된 포즈 사전인 VPoser[25]를 사용하여 포즈의 복잡도를 정량화합니다.이것은 VPoser 임베딩 공간에서 표준 포즈와의 편차 양으로 정의합니다.형식적으로 복잡도 점수는 score= mean (EVPoser (2)²) (13)으로 계산됩니다.여기서 는 사람의 포즈이고 Exposer는 VPoser의 인코더입니다.점수가 높을수록 포즈가 더 복잡함을 나타냅니다.그런 다음 포즈 복잡도 점수에 따라 테스트 사례를 쉬움, 보통, 어려움 범주로 나눕니다.쉬운 포즈는 50번째 백분위수에 속하는 포즈이고, 보통 포즈는 75번째 백분위수, 그리고 하드 포즈는 나머지 사례를 포함합니다. 표 2에서 우리는 분류된 범주에 대한 정량적 결과를 보고합니다. 우리는 ZeroAvatar가 맥락적 손실과 CLIP 유사도 점수 측면에서 기준선에 비해 다양한 유형의 포즈에 대해 일관된 개선을 보인다는 것을 관찰합니다. 때때로 더 나쁜 LPIPS를 달성하지만, 그것은 기준선이 전체 기하학을 왜곡하는 대가로 참조 뷰에 과도하게 맞춤되는 경향이 있기 때문이라는 것을 관찰합니다(예: 그림 9). 표 2: 쉬움/보통/하드 포즈가 있는 테스트 사례에 대한 정량적 결과. 모델 Stable-DreamFusion* [26] Stable-DreamFusion (Zero 1-to-3 포함) [17] 3D Fuse [32] Make-it-3D Coarse [36] ZeroAvatar (저희) LPIPS (↓) 0.60/0.51 0.0.42 0.33/0.0.68 0.670.0.37/0.34/0.0.38/0.34/0.Contextual (↓) CLIP-I (†) CLIP-T (↑) 3.8/3.4/3.3.5/3.3/3.3.3/3.1/3.77.180.3/76.76.9/77.1/74.80.784.183.3.4/3.3/3.3.3/3.0/3.83.7 89.7/85.85.0/89.7/87.28.0/29.0/29.27.7/27.0/26.30.5 31.6/32.32.133.5/32.32.2/33.8/32. 추가 정성적 결과 최적화 진행 상황을 시각적으로 보여주기 위해 에포크 5와 10의 끝에서 3가지 관점에서 렌더링을 보여줍니다(그림 7). 표시된 대로 ZeroAvatar는 훨씬 빠른 수렴 속도를 보입니다. 초기화로 거친 기하 구조를 사용하므로 최적화된 NeRF는 5번째 에포크에서 대략적으로 정확한 기하 구조와 색상을 보여주는 반면, 기본 Make-it-3D는 제한된 모양을 얻는 데 최소 10번째 에포크가 걸립니다.또한 Make-it-3D는 가끔씩 발산하는 것을 관찰했습니다(그림 8).이 지점 이후에는 참조 뷰 재구성 손실을 최적화해도 사람의 기하 구조를 바로잡는 데 거의 효과가 없습니다.이에 비해 ZeroAvatar는 지속적으로 더 강력한 결과를 생성합니다.마지막으로 SMPL 신체 모델[19]로 사람의 모양을 정확하게 표현할 수 없는 실패 사례(그림 9)에 대한 예를 포함합니다. 표시된 대로 ZeroAvatar는 입력 이미지와 약간 정렬이 틀리지만, 새로운 뷰에서 나온 사람의 모양은 참조 뷰와 더 일관성이 있는 반면, Make-it-3D의 새로운 뷰 렌더링은 종종 왜곡됩니다.EpochEpochEpochEpochMake-it-3D ZeroAvatar 그림 7: 에포크 5와 10의 끝에서 Make-it-3D [36]와 ZeroAvatar를 사용하여 학습된 NeRF의 렌더링.EpochAAA EpochA Make-it-3D ZeroAvatar 그림 8: Make-it-3D [36]의 최적화는 때때로 발산을 초래하는 반면, ZeroAvatar는 더 강력한 최적화를 보여줍니다.Make-it-3D ZeroAvatar 그림 9: SMPL이 아바타의 비율을 충실하게 표현할 수 없는 실패 시나리오.그럼에도 불구하고, ZeroAvatar는 여전히 기준선(Make-it-3D)에 비해 참조 뷰와 더 일관된 새로운 뷰 렌더링을 생성합니다.참고 문헌 [1] Renderpeople. URL https://renderpeople.com/. [2] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and MJ Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, 2016년 10월 11-14일, Proceedings, Part V 14, 561-578쪽. Springer, 2016. [3] Y. Cao, Y.-P. Cao, K. Han, Y. Shan, and K.-YK Wong. Dreamavatar: Text-and-shape Guided 3D Human Avatar generation via diffusion models. arXiv 사전 인쇄본 arXiv:2304.00916, 2023. [4] R. Chen, Y. Chen, N. Jiao, and K. Jia. Fantasia3d: 고품질 텍스트-3D 콘텐츠 생성을 위한 기하학 및 모양 풀기.arXiv 사전 인쇄본 arXiv:2303.13873, 2023. [5] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, A. Farhadi. Objaverse: 주석이 달린 3D 객체의 우주.arXiv 사전 인쇄본 arXiv:2212.08051, 2022. [6] A. Eftekhar, A. Sax, J. Malik, A. Zamir. Omnidata: 3D 스캔에서 다중 작업 중간 수준 비전 데이터 세트를 만드는 확장 가능한 파이프라인. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스의 진행 사항, 10786-10796페이지, 2021. [7] Y. Feng, V. Choutas, T. Bolkart, D. Tzionas, MJ Black. 조절을 사용한 표현적 신체의 협력적 회귀. 2021 국제 3D 비전 컨퍼런스(3DV), 792-804페이지. IEEE, 2021. [8] R. Girshick. 고속 r-cnn. IEEE 국제 컴퓨터 비전 컨퍼런스의 진행 사항, 1440-1448페이지, 2015. [9] G. Gkioxari, J. Malik, J. Johnson. 메시 r-cnn. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 9785-9795쪽, 2019. [10] S. Guan, J. Xu, Y. Wang, B. Ni, X. Yang. 도메인 외부 인간 메시 재구성을 위한 2단계 온라인 적응. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 10472-10481쪽, 2021. [11] RA Güler, N. Neverova, I. Kokkinos. Densepose: 야생에서의 밀집 인간 포즈 추정. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 7297-7306쪽, 2018. [12] F. Hong, M. Zhang, L. Pan, Z. Cai, L. Yang, Z. Liu. Avatarclip: 3D 아바타의 제로샷 텍스트 기반 생성 및 애니메이션.arXiv 사전 인쇄본 arXiv:2205.08535, 2022. [13] A. Jain, B. Mildenhall, JT Barron, P. Abbeel, and B. Poole. 꿈 필드를 사용한 제로샷 텍스트 유도 객체 생성.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 867-876페이지, 2022. [14] A. Kanazawa, MJ Black, DW Jacobs, and J. Malik. 인체 모양과 포즈의 종단 간 복구.IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7122-7131페이지, 2018. [15] DP Kingma and J. Ba. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980, 2014. [16] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, T.-Y. Lin. Magic3d: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.10440, 2022. [17] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, C. Vondrick. Zero-1-to-3: 제로 샷 한 이미지를 3D 개체로. arXiv 사전 인쇄본 arXiv:2303.11328, 2023. [18] Z. Liu, P. Dai, R. Li, X. Qi, C.-W. Fu. Iss: 텍스트 기반 3D 모양 생성을 위한 스테팅 스톤으로서의 이미지.arXiv 사전 인쇄본 arXiv:2209.04145, 2022. [19] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, MJ Black.Smpl: 스킨이 적용된 다중 사람 선형 모델.ACM 그래픽스 거래(TOG), 34(6):1–16, 2015. [20] R. Mechrez, I. Talmi, L. Zelnik-Manor. 정렬되지 않은 데이터를 사용한 이미지 변환의 맥락적 손실.유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 768-783페이지, 2018. [21] O. Michel, R. Bar-On, R. Liu, S. Benaim, R. Hanocka.Text2mesh: 메시를 위한 텍스트 기반 신경 스타일화. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 13492-13502페이지, 2022.[22] B. Mildenhall, PP Srinivasan, M. Tancik, JT Barron, R. Ramamoorthi, R. Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ACM 커뮤니케이션, 65(1):99–106, 2021. [23] T. Müller, A. Evans, C. Schied, A. Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM 그래픽스 트랜잭션(ToG), 41(4):1-15, 2022. [24] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, M. Chen. Glide: 텍스트 가이드 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. arXiv 사전 인쇄본 arXiv:2112.10741, 2021. [25] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, AAA Osman, D. Tzionas, and MJ Black. 표현적인 신체 캡처: 단일 이미지에서 3D 손, 얼굴 및 신체. IEEE Conf. on Computer Vision and Pattern Recognition(CVPR) 회의록, 2019. [26] B. Poole, A. Jain, JT Barron, and B. Mildenhall. Dreamfusion: 2D 확산을 사용한 텍스트-3D. arXiv 사전 인쇄본 arXiv:2209.14988, 2022. [27] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. 자연어 감독에서 전이 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스, 8748-8763페이지. PMLR, 2021. [28] E. Richardson, G. Metzer, Y. Alaluf, R. Giryes, and D. Cohen-Or. 텍스처: 3D 모양의 텍스트 가이드 텍스처링. arXiv 사전 인쇄본 arXiv:2302.01721, 2023. [29] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 10684-10695페이지, 2022년 6월. [30] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, SKS Ghasemipour, BK Ayan, SS Mahdavi, RG Lopes, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487, 2022. [31] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li. Pifu: 고해상도 옷을 입은 인간 디지털화를 위한 픽셀 정렬 암시적 함수. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 2304-2314쪽, 2019. [32] J. Seo, W. Jang, M.-S. Kwak, J. Ko, H. Kim, J. Kim, J.-H. Kim, J.-H. Lee, and S. Kim. 강력한 텍스트-3D 생성을 위해 2D 확산 모델에 3D 일관성을 알려주자. arXiv 사전 인쇄본 arXiv:2303.07937, 2023. [33] V. Sitzmann, M. Zollhöfer, and G. Wetzstein. 장면 표현 네트워크: 연속적인 3D 구조 인식 신경 장면 표현. 신경 정보 처리 시스템의 발전, 32, 2019. [34] S.-Y. Su, F. Yu, M. Zollhöfer, and H. Rhodin. A-nerf: 인체 모양, 외모 및 포즈를 학습하기 위한 관절형 신경 광도장. 신경 정보 처리 시스템의 발전, 34:12278-12291, 2021. [35] J. Tang. Stable-dreamfusion: stable-diffusion을 사용한 텍스트-3d, 2022. https://github.com/ashawkey/stabledreamfusion. [36] J. Tang, T. Wang, B. Zhang, T. Zhang, R. Yi, L. Ma, D. Chen. Make-it-3d: 확산 사전을 사용한 단일 이미지에서 고화질 3d 생성. arXiv 사전 인쇄본 arXiv:2303.14184, 2023. [37] H. Wang, X. Du, J. Li, RA Yeh, G. Shakhnarovich. 점수 야코비안 체이닝: 3D 생성을 위한 사전 학습된 2D 확산 모델 리프팅. arXiv 사전 인쇄본 arXiv:2212.00774, 2022. [38] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: 단일 RGB 이미지에서 3D 메시 모델 생성. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 52-67페이지, 2018. [39] C.-Y. Weng, B. Curless, PP Srinivasan, JT Barron, and I. Kemelmacher-Shlizerman. Humannerf: 단안 비디오에서 움직이는 사람의 자유 시점 렌더링. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16210-16220페이지, 2022. [40] Z. Weng 및 S. Yeung. 단일 뷰 이미지에서 전체적인 3D 인간 및 장면 메시 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 334-343페이지, 2021. [41] Z. Weng, K.-C. Wang, A. Kanazawa 및 S. Yeung. 야생 인간 메시 복구를 위한 도메인 적응형 3D 포즈 증강. 국제 3D 비전(3DV) 컨퍼런스, 2022.[42] Z. Weng, L. Bravo-Sánchez 및 S. Yeung. 확산-hpc: 사실적인 인간을 사용하여 합성 이미지 생성. arXiv 사전 인쇄본 arXiv:2303.09541, 2023. [43] Y. Xiu, J. Yang, X. Cao, D. Tzionas, and MJ Black. 경제학: 정상인에서 얻은 명시적 옷을 입은 인간. arXiv 사전 인쇄본 arXiv:2212.07422, 2022. [44] Y. Xiu, J. Yang, D. Tzionas, and MJ Black. 아이콘: 정상인에서 얻은 암묵적 옷을 입은 인간. 2022년 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 13286-13296페이지. IEEE, 2022. [45] J. Xu, X. Wang, W. Cheng, Y.-P. Cao, Y. Shan, X. Qie, and S. Gao. Dream3d: 3D 모양 사전 및 텍스트-이미지 확산 모델을 사용한 제로샷 텍스트-3D 합성. arXiv 사전 인쇄본 arXiv:2212.14704, 2022. [46] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and C. Shen. 단일 이미지에서 3D 장면 모양을 복구하는 방법 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 204-213페이지, 2021. [47] K. Youwang, K. Ji-Yeon, and T.-H. Oh. Clip-actor: 인간 메시 애니메이션을 위한 텍스트 기반 추천 및 스타일 지정. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part III, pages 173–191. Springer, 2022. [48] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, and Z. Sun. Pymaf: 3D Human Possibility and Shape Regression with Pymafal Mesh Alignment Feedback Loop. IEEE/CVF International Conference on Computer Vision의 회의록, pages 11446-11456, 2021. [49] L. Zhang and M. Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023. [50] R. Zhang, P. Isola, AA Efros, E. Shechtman, and O. Wang. 지각적 척도로서의 딥 피처의 비합리적 효과성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 586-595페이지, 2018. [51] F. Zhao, W. Yang, J. Zhang, P. Lin, Y. Zhang, J. Yu, and L. Xu. Humannerf: 희소 입력에서 일반화 가능한 신경 인간 광휘장. arXiv 사전 인쇄본 arXiv:2112.02789, 2021.
