--- ABSTRACT ---
단안 모션 캡처에 대한 학습 기반 접근법은 최근 데이터 중심 방식으로 회귀를 학습하여 유망한 결과를 보여주었습니다. 그러나 데이터 수집 및 네트워크 설계의 과제로 인해 기존 솔루션이 월드 공간에서 정확하면서도 실시간 풀바디 캡처를 달성하는 것은 여전히 어렵습니다. 이 연구에서 우리는 2D 스켈레톤 시퀀스와 3D 회전 모션의 프록시 데이터 세트에서 월드 공간 모션을 학습하는 인간 중심 프록시-모션 학습 체계인 ProxyCap을 소개합니다. 이러한 프록시 데이터를 통해 일반화 문제를 완화하는 동시에 정확한 월드 공간 감독을 갖춘 학습 기반 네트워크를 구축할 수 있습니다. 월드 공간에서 보다 정확하고 물리적으로 타당한 예측을 위해 우리 네트워크는 인간 중심 관점에서 인간의 모션을 학습하도록 설계되어 다양한 카메라 궤적으로 캡처된 동일한 모션을 이해할 수 있습니다. 또한 우리 네트워크에서 접촉 인식 신경 모션 하강 모듈을 제안하여 프록시 관찰과 발-지면 접촉 및 모션 정렬 오류를 인식할 수 있습니다. 제안된 학습 기반 솔루션을 통해, 우리는 핸드헬드 이동 카메라를 사용하더라도 세계 공간에서 가능한 발-지면 접촉을 가진 최초의 실시간 단안 전신 캡처 시스템을 시연합니다. 프로젝트 페이지는 https://zhangyux15.github.io/ ProxyCapv2입니다. 1.
--- INTRODUCTION ---
단안 비디오의 모션 캡처는 게임, VR/AR, 스포츠 분석 등과 같은 다양한 응용 분야에 필수적인 기술입니다. 궁극적인 목표 중 하나는 정확하고 세계 공간에서 물리적으로 그럴듯하게 실시간 캡처를 달성하는 것입니다. 최근의 발전에도 불구하고 이 작업은 특히 휴대용 이동 카메라로 야외에서 캡처하는 설정에서 아직 해결되지 않았습니다. 최적화 기반 방법[4, 10, 12, 20, 40, 50, 64]과 비교할 때 학습 기반 접근 방식[15, 19, 70, 72]은 이미지에서 사람의 포즈를 직접 회귀하여 계산 효율성을 크게 향상시키는 동시에 최적화 기반 방법의 초기화 감도 및 국소 최적화 함정에 내재된 문제를 해결할 수 있습니다. 데이터 기반 솔루션으로서 학습 기반 방법론의 성능 및 일반화 기능은 훈련 데이터의 정확도와 다양성에 따라 크게 제한됩니다. 불행히도 기존 데이터 세트는 이러한 요구 사항을 동시에 충족할 수 없습니다. 한편, 순차적 지상 진실 3D 포즈 주석이 있는 데이터 세트[11, 13, 30, 51]는 대부분 마커 기반 또는 다중 뷰 시스템에서 캡처되므로 인간의 외모와 배경에서 만족스러운 수준의 다양성으로 확장하기 어렵습니다. 반면에 수많은 야생 데이터 세트[1, 23]는 인간과 시나리오 다양성의 풍부함에서 뛰어나지만 실제 세계의 3D 동작이 부족하고 대부분은 비디오 대신 개별 이미지만 제공합니다. 최근 연구자들은 제어 가능한 카메라로 인간 아바타를 렌더링하여 합성 데이터[2, 3, 38]를 만들려고 시도했지만 실제 세계 이미지와 렌더링된 이미지 간의 도메인 갭을 메우는 것이 여전히 어렵고 확장하기에는 너무 비쌉니다. 이 논문에서는 합성 데이터를 만드는 정신을 따르지만 사람 이미지 대신 2D 프록시 표현을 렌더링하는 방향으로 전환합니다. 프록시 표현(즉, 실루엣[39, 60], 분할[16, 16, 37, 47], IUV[62, 71] 및 2D 스켈레톤[21, 28, 31, 32, 39, 52, 54, 58])을 사용하면 전체 모션 캡처 파이프라인을 이미지-프록시 추출과 프록시-모션 리프팅의 두 단계로 나눌 수 있습니다.분할된 파이프라인에서 이미지-프록시 추출은 실제 데이터 세트에 주석이 달린 2D 기준 진실이 풍부하기 때문에 매우 정확하고 견고하며, 프록시-모션 단계는 더 다양한 교육 데이터를 활용하여 일반화 문제를 완화하고 도메인 갭을 줄일 수 있습니다.여기서는 단순성과 3D 모션과의 높은 상관 관계 때문에 2D 스켈레톤 시퀀스를 프록시 표현으로 채택합니다. 한편, 우리는 기존의 대규모 동작 시퀀스 데이터베이스인 AMASS [29]에 무작위 가상 카메라 궤적을 결합하여 프록시-모션 리프팅을 학습하기 위한 거의 무한한 데이터를 합성합니다. 제안된 프록시 데이터 세트는 대규모로 생성할 수 있지만 회귀 기반 네트워크가 프록시 데이터에서 물리적으로 그럴듯한 동작을 학습하려면 두 가지 과제가 남아 있습니다. i) 움직이는 카메라 아래에서 세계 공간의 인간 동작과 ii) 꾸준한 신체-지면 접촉이 있는 물리적으로 그럴듯한 동작을 복구하는 방법입니다. 움직이는 카메라로 촬영한 비디오의 경우 인간과 카메라의 궤적/회전이 서로 결합되어 세계 공간의 인간 동작을 복구하는 것이 매우 어렵습니다. 이 문제를 해결하기 위해 최신 솔루션 [18, 63]은 SfM [48, 56]을 사용하여 배경에서 카메라 포즈를 추정한 다음 카메라 중심 관점에서 인간의 동작을 추정합니다. 그러나 SfM은 텍스처가 풍부한 배경이 필요하며 전경의 움직이는 캐릭터가 이미지를 지배하는 경우 실패할 수 있습니다. 그들의 사후 처리 최적화 파이프라인은 값비싼 계산 비용으로 인해 실시간 애플리케이션에 적합하지 않습니다. 게다가, 이러한 이전 솔루션은 [17]과 같은 카메라 중심 관점에서 인간의 동작을 학습하는데, 이는 실제로 회귀 네트워크에 모호합니다. 이 논문에서 우리는 주요 과제 중 하나가 이전 솔루션의 카메라 중심 설정에서 발생한다는 점을 지적하고자 합니다. 이러한 설정에서 다른 카메라로 촬영한 동일한 동작 시퀀스는 여러 인간의 동작 궤적으로 표현되어 네트워크가 내재적인 동작을 사전에 이해하기 어렵게 만듭니다. 이와 대조적으로, 우리는 합성 데이터에서 다른 카메라 궤적에서 일관된 인간 동작 출력을 보장하기 위해 인간 중심 동작을 학습하는 것을 제안합니다. 구체적으로, 우리 네트워크는 이 공간에서 상대적인 카메라 외부 매개변수와 함께 인간 좌표계에서 로컬 변환과 포즈를 학습합니다. 그 후, 우리는 각 프레임에서 인간의 로컬 변환을 축적하여 글로벌 카메라 궤적을 얻습니다. 제안된 프록시-투-모션 데이터 세트의 이점을 활용하여 동일한 동작 시퀀스에 대한 다른 카메라 궤적을 합성하여 인간 동작 일관성을 학습할 수 있습니다. 이런 방식으로, 우리 네트워크는 SfM 없이 강력한 모션 사전을 통해 움직이는 카메라에서 사람의 포즈를 더 쉽게 분리할 수 있습니다. 인간 중심의 모션 회귀에 더해, 접촉 인식 신경 모션 하강 모듈을 도입하여 물리적 타당성을 더욱 향상시킵니다. 구체적으로, 우리 네트워크는 먼저 거친 모션을 예측한 다음 발과 땅의 접촉과 프록시 관찰과의 모션 불일치를 기반으로 반복적으로 이를 정제합니다. 이전 작업[18, 63, 67]에서 사용된 전역 후처리 최적화와 비교할 때, 우리 방법은 명시적 그래디언트 역전파 대신 하강 방향과 단계를 학습합니다. 우리는 ProxyCap이라는 이름의 우리 방법이 더 강력하고 실시간 애플리케이션을 지원하는 데 훨씬 빠르다는 것을 보여줍니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 데이터 부족 문제를 해결하기 위해 2D 스켈레톤 시퀀스를 프록시 표현으로 채택하고 무작위 가상 카메라 궤적을 사용하여 세계 공간에서 프록시 데이터를 생성합니다. • 우리는 인간 중심적 지각으로부터 동작을 학습하는 네트워크를 설계하여, 우리의 회귀자가 다른 카메라 궤적에서 인간 동작의 일관성을 이해할 수 있도록 합니다. • 우리는 더 정확하고 물리적으로 타당한 예측을 위해 접촉 인식 신경 하강 모듈을 제안합니다. 우리의 네트워크는 대리 관찰과 함께 발-지면 접촉과 동작 부정렬을 인식할 수 있습니다. • 우리는 움직이는 카메라 아래에서 세계 공간에서 타당한 신체-지면 접촉을 가진 실시간 전신 캡처 시스템을 시연합니다. 2.
--- RELATED WORK ---
단안 모션 캡처는 최근 활발한 연구 분야였습니다.저희는 저희 연구와 관련된 연구에 대한 간략한 검토를 제공하고 보다 포괄적인 조사를 위해 독자들에게 [57]을 참조하시기 바랍니다.모션 캡처 데이터 세트.기존 모션 캡처 데이터 세트는 마커 기반 [13, 51] 또는 마커 없는 [42, 65, 75, 76] 시스템으로 캡처됩니다.마커 또는 다중 뷰 설정이 필요하기 때문에 이러한 데이터 세트의 다양성은 실제 데이터 세트에 비해 제한적입니다.모션 데이터 세트를 풍부하게 하기 위해 이미지 공간에서 더 나은 정렬로 가상 기준 진실 레이블을 생성하는 데 많은 노력 [14, 19, 34, 36, 49]이 전념했지만 세계 공간에서의 모션은 고려하지 않았습니다.반면에 연구자들은 제어 가능한 관점과 배경으로 인간 모델을 렌더링하여 합성 데이터 [38, 53, 59]를 사용하기도 했습니다.그러나 이러한 합성 데이터 세트는 만들기에 너무 비용이 많이 들거나 실제 이미지와 큰 도메인 갭이 있습니다. 인간 메시 복구를 위한 프록시 표현. 주석이 달린 데이터가 부족하고 인간의 모습과 배경이 다양하기 때문에 딥 신경망에서도 원시 RGB 이미지에서 정확한 3D 동작을 학습하는 것은 어렵습니다. 이 문제를 완화하기 위해 이전 접근 방식에서는 실루엣[39, 60], 2D/3D 랜드마크[21, 28, 31, 32, 39, 52, 54,58], 분할[16, 16, 37, 47], IUV[62, 71]를 포함한 다양한 프록시 표현을 활용했습니다. 이러한 프록시 표현은 신경망에 대한 지침을 제공하여 학습 프로세스를 더 쉽게 만들 수 있습니다. 그러나 프록시 표현은 관찰을 단순화하고 깊이와 규모에 대한 추가적인 모호성을 도입합니다. 특히 단일 프레임에서 프록시 표현을 사용할 때 그렇습니다[52, 62, 71]. 이 작업에서 우리는 2D 골격 시퀀스를 프록시 표현으로 채택하여 이 문제를 완화하고 세계 공간에서 정확한 동작을 포함하는 프록시 데이터를 생성하는 것을 제안합니다.전신 모션 캡처.최근의 최첨단 접근 방식[16, 70]은 신체 전용[16, 70], 손 전용[22] 및 얼굴 전용[8] 모델을 추정하는 데 유망한 결과를 얻었습니다.노력을 결합하여 이러한 회귀 기반 접근 방식이 단안 전신 모션 캡처에 활용되었습니다.이러한 접근 방식[5, 7, 35, 46, 72, 78]은 일반적으로 세 개의 전문가 네트워크를 통해 신체, 손 및 얼굴 모델을 회귀시키고 다양한 전략으로 이를 통합합니다.예를 들어, PIXIE[7]는 협력 회귀로 적분을 학습하는 반면 PyMAF-X[72]는 부자연스러운 손목 포즈를 피하기 위해 팔꿈치 비틀기 보상이 있는 적응적 적분 전략을 채택합니다.진전에도 불구하고 기존 솔루션이 세계 공간에서 정확하면서 실시간으로 실행하는 것은 여전히 어렵습니다. 이 작업에서 우리는 새로운 데이터 생성 전략과 새로운 네트워크 아키텍처를 도입하여 그럴듯한 발-지면 접촉을 통해 실시간 전신 캡처를 달성합니다. 모션 캡처를 위한 신경 하강. 최적화 기반 접근 방식[4]은 일반적으로 3D 매개변수 모델을 2D 증거에 맞추지만 초기화 민감도와 까다로운 포즈를 처리하지 못하는 문제가 있습니다. 보다 효율적이고 견고한 모션 예측을 달성하기 위해 반복적 세분화를 위해 신경망의 학습 능력을 활용하려는 시도가 여러 가지 있습니다. HUND[68]는 모델 매개변수의 업데이트를 회귀시키기 위해 순환 네트워크를 기반으로 하는 학습-학습 접근 방식을 제안합니다. Song et al.[52]은 예측된 신체 모델의 포즈를 세분화하기 위해 학습된 경사 하강을 제안합니다. 유사한 세분화 전략은 이미지 기능을 입력으로 활용하여 PyMAF[70] 및 LVD[6]에서도 활용됩니다. 우리 작업에서 우리는 접촉 인식 신경 하강 모듈을 제안하고 보다 효과적인 모션 업데이트를 위해 전통적인 사용을 활용합니다. 세계 공간에서의 그럴듯한 모션 캡처. 기존 단안 모션 캡처
--- METHOD ---
, ProxyCap은 세계 공간에서 그럴듯한 발-지면 접촉을 통해 정확한 인간 동작을 생성하는 실시간 단안 전신 캡처 솔루션입니다. 초록 단안 모션 캡처에 대한 학습 기반 접근 방식은 최근 데이터 중심 방식으로 회귀를 학습하여 유망한 결과를 보여주었습니다. 그러나 데이터 수집 및 네트워크 설계의 과제로 인해 기존 솔루션이 세계 공간에서 정확하면서도 실시간 전신 캡처를 달성하는 데는 여전히 어려움이 있습니다. 이 연구에서는 2D 골격 시퀀스와 3D 회전 모션의 프록시 데이터 세트에서 세계 공간 모션을 학습하는 인간 중심 프록시-모션 학습 방식인 ProxyCap을 소개합니다. 이러한 프록시 데이터를 사용하면 일반화 문제를 완화하는 동시에 정확한 세계 공간 감독을 갖춘 학습 기반 네트워크를 구축할 수 있습니다. 세계 공간에서 보다 정확하고 물리적으로 그럴듯한 예측을 위해 네트워크는 인간 중심 관점에서 인간 동작을 학습하도록 설계되었으며, 이를 통해 다른 카메라 궤적으로 캡처된 동일한 동작을 이해할 수 있습니다. 또한, 네트워크에서 접촉 인식 신경 모션 하강 모듈을 제안하여 프록시 관찰과의 발-지면 접촉 및 모션 오정렬을 인식할 수 있습니다. 제안된 학습 기반 솔루션을 사용하여 핸드헬드 이동 카메라를 사용하더라도 세계 공간에서 그럴듯한 발-지면 접촉을 가진 최초의 실시간 단안 전신 캡처 시스템을 시연합니다. 프로젝트 페이지는 https://zhangyux15.github.io/ ProxyCapv2입니다. 1. 소개 단안 비디오의 모션 캡처는 게임, VR/AR, 스포츠 분석 등과 같은 다양한 응용 프로그램에 필수적인 기술입니다. 궁극적인 목표 중 하나는 세계 공간에서 정확하고 물리적으로 그럴듯한 실시간 캡처를 달성하는 것입니다. 최근의 발전에도 불구하고 이 작업은 여전히 해결되지 않았으며, 특히 핸드헬드 이동 카메라를 사용한 야생 캡처 설정에서는 더욱 그렇습니다. 최적화 기반 방법[4, 10, 12, 20, 40, 50, 64]과 비교했을 때, 학습 기반 접근 방식[15, 19, 70, 72]은 이미지에서 인간의 포즈를 직접 회귀시켜 초기화 민감도와 국소 최적화 함정의 최적화 기반 방법의 고유한 문제를 해결하는 동시에 계산 효율성을 크게 향상시킬 수 있습니다.데이터 기반 솔루션으로서, 학습 기반 방법론의 성능과 일반화 기능은 훈련 데이터의 정확도와 다양성에 의해 크게 제한됩니다.안타깝게도, 기존 데이터 세트는 이러한 요구 사항을 동시에 충족할 수 없습니다.한편, 순차적 기준 진실 3D 포즈 주석이 있는 데이터 세트[11, 13, 30, 51]는 대부분 마커 기반 또는 다중 뷰 시스템에서 캡처되므로 인간의 외모와 배경에서 만족스러운 수준의 다양성으로 확장하기 어렵습니다. 반면, 수많은 야생 데이터 세트[1, 23]는 인간과 시나리오 다양성의 풍부함에서 탁월하지만 실제 세계의 3D 동작이 부족하고 대부분은 비디오 대신 개별 이미지만 제공합니다.최근 연구자들은 제어 가능한 카메라로 인간 아바타를 렌더링하여 합성 데이터[2, 3, 38]를 만들려고 시도했지만 실제 세계 이미지와 렌더링된 이미지 간의 도메인 갭을 메우는 것이 여전히 어렵고 확장하기에는 너무 비쌉니다.이 논문에서는 합성 데이터를 만드는 정신을 따르지만 사람 이미지 대신 2D 프록시 표현을 렌더링하는 방향으로 전환합니다. 프록시 표현(즉, 실루엣[39, 60], 분할[16, 16, 37, 47], IUV[62, 71] 및 2D 스켈레톤[21, 28, 31, 32, 39, 52, 54, 58])을 사용하면 전체 모션 캡처 파이프라인을 이미지-프록시 추출과 프록시-모션 리프팅의 두 단계로 나눌 수 있습니다.분할된 파이프라인에서 이미지-프록시 추출은 실제 데이터 세트에 주석이 달린 2D 기준 진실이 풍부하기 때문에 매우 정확하고 견고하며, 프록시-모션 단계는 더 다양한 교육 데이터를 활용하여 일반화 문제를 완화하고 도메인 갭을 줄일 수 있습니다.여기서는 단순성과 3D 모션과의 높은 상관 관계 때문에 2D 스켈레톤 시퀀스를 프록시 표현으로 채택합니다. 한편, 우리는 기존의 대규모 동작 시퀀스 데이터베이스인 AMASS [29]에 무작위 가상 카메라 궤적을 결합하여 프록시-모션 리프팅을 학습하기 위한 거의 무한한 데이터를 합성합니다. 제안된 프록시 데이터 세트는 대규모로 생성할 수 있지만 회귀 기반 네트워크가 프록시 데이터에서 물리적으로 그럴듯한 동작을 학습하려면 두 가지 과제가 남아 있습니다. i) 움직이는 카메라 아래에서 세계 공간의 인간 동작과 ii) 꾸준한 신체-지면 접촉이 있는 물리적으로 그럴듯한 동작을 복구하는 방법입니다. 움직이는 카메라로 촬영한 비디오의 경우 인간과 카메라의 궤적/회전이 서로 결합되어 세계 공간의 인간 동작을 복구하는 것이 매우 어렵습니다. 이 문제를 해결하기 위해 최신 솔루션 [18, 63]은 SfM [48, 56]을 사용하여 배경에서 카메라 포즈를 추정한 다음 카메라 중심 관점에서 인간의 동작을 추정합니다. 그러나 SfM은 텍스처가 풍부한 배경이 필요하며 전경의 움직이는 캐릭터가 이미지를 지배하는 경우 실패할 수 있습니다. 그들의 사후 처리 최적화 파이프라인은 값비싼 계산 비용으로 인해 실시간 애플리케이션에 적합하지 않습니다. 게다가, 이러한 이전 솔루션은 [17]과 같은 카메라 중심 관점에서 인간의 동작을 학습하는데, 이는 실제로 회귀 네트워크에 모호합니다. 이 논문에서 우리는 주요 과제 중 하나가 이전 솔루션의 카메라 중심 설정에서 발생한다는 점을 지적하고자 합니다. 이러한 설정에서 다른 카메라로 촬영한 동일한 동작 시퀀스는 여러 인간의 동작 궤적으로 표현되어 네트워크가 내재적인 동작을 사전에 이해하기 어렵게 만듭니다. 이와 대조적으로, 우리는 합성 데이터에서 다른 카메라 궤적에서 일관된 인간 동작 출력을 보장하기 위해 인간 중심 동작을 학습하는 것을 제안합니다. 구체적으로, 우리 네트워크는 이 공간에서 상대적인 카메라 외부 매개변수와 함께 인간 좌표계에서 로컬 변환과 포즈를 학습합니다. 그 후, 우리는 각 프레임에서 인간의 로컬 변환을 축적하여 글로벌 카메라 궤적을 얻습니다. 제안된 프록시-투-모션 데이터 세트의 이점을 활용하여 동일한 동작 시퀀스에 대한 다른 카메라 궤적을 합성하여 인간 동작 일관성을 학습할 수 있습니다. 이런 방식으로, 우리 네트워크는 SfM 없이 강력한 모션 사전을 통해 움직이는 카메라에서 사람의 포즈를 더 쉽게 분리할 수 있습니다. 인간 중심의 모션 회귀에 더해, 접촉 인식 신경 모션 하강 모듈을 도입하여 물리적 타당성을 더욱 향상시킵니다. 구체적으로, 우리 네트워크는 먼저 거친 모션을 예측한 다음 발과 땅의 접촉과 프록시 관찰과의 모션 불일치를 기반으로 반복적으로 이를 정제합니다. 이전 작업[18, 63, 67]에서 사용된 전역 후처리 최적화와 비교할 때, 우리 방법은 명시적 그래디언트 역전파 대신 하강 방향과 단계를 학습합니다. 우리는 ProxyCap이라는 이름의 우리 방법이 더 강력하고 실시간 애플리케이션을 지원하는 데 훨씬 빠르다는 것을 보여줍니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 데이터 부족 문제를 해결하기 위해 2D 스켈레톤 시퀀스를 프록시 표현으로 채택하고 무작위 가상 카메라 궤적을 사용하여 세계 공간에서 프록시 데이터를 생성합니다. • 우리는 인간 중심적 지각으로부터 동작을 학습하는 네트워크를 설계하여 회귀자가 다양한 카메라 궤적에서 인간 동작의 일관성을 이해할 수 있도록 합니다.• 우리는 더 정확하고 물리적으로 그럴듯한 예측을 위해 접촉 인식 신경 하강 모듈을 제안합니다.우리의 네트워크는 대리 관찰과 함께 발-지면 접촉과 동작 부정렬을 인식할 수 있습니다.• 우리는 움직이는 카메라 아래에서 세계 공간에서 그럴듯한 신체-지면 접촉을 포함하는 실시간 전신 캡처 시스템을 보여줍니다.2. 관련 연구 단안 모션 캡처는 최근 활발한 연구 분야였습니다.우리는 우리와 관련된 연구에 대한 간략한 검토를 제공하고 보다 포괄적인 조사를 위해 독자들에게 [57]을 참조하시기 바랍니다.모션 캡처 데이터 세트.기존 모션 캡처 데이터 세트는 마커 기반 [13, 51] 또는 마커 없는 [42, 65, 75, 76] 시스템으로 캡처됩니다.마커 또는 다중 뷰 설정이 필요하기 때문에 이러한 데이터 세트의 다양성은 야생 데이터 세트와 비교할 때 제한적입니다. 동작 데이터 세트를 풍부하게 하기 위해 이미지 공간에서 더 나은 정렬을 갖는 가상 기준 진실 레이블을 생성하는 데 많은 노력[14, 19, 34, 36, 49]이 기울여졌지만 세계 공간에서의 동작은 고려하지 않았습니다.반면에 연구자들은 제어 가능한 관점과 배경으로 인간 모델을 렌더링하여 합성 데이터[38, 53, 59]를 사용하기도 했습니다.그러나 이러한 합성 데이터 세트는 생성 비용이 너무 많이 들거나 실제 이미지와 큰 도메인 갭이 있습니다.인간 메시 복구를 위한 프록시 표현.주석이 달린 데이터가 부족하고 인간의 모습과 배경이 다양하기 때문에 딥 신경망에서도 원시 RGB 이미지에서 정확한 3D 동작을 학습하는 것은 어렵습니다. 이 문제를 완화하기 위해 이전 접근 방식에서는 실루엣[39, 60], 2D/3D 랜드마크[21, 28, 31, 32, 39, 52, 54,58], 분할[16, 16, 37, 47], IUV[62, 71]를 포함한 다양한 프록시 표현을 활용했습니다. 이러한 프록시 표현은 신경망에 대한 지침을 제공하여 학습 프로세스를 더 쉽게 만들 수 있습니다. 그러나 프록시 표현은 관찰을 단순화하고 깊이와 규모에 대한 추가적인 모호성을 도입합니다. 특히 단일 프레임에서 프록시 표현을 사용할 때[52, 62, 71]. 이 작업에서는 2D 스켈레톤 시퀀스를 프록시 표현으로 채택하여 이 문제를 완화하고 세계 공간에서 정확한 동작을 포함하는 프록시 데이터를 생성하는 것을 제안합니다. 전신 모션 캡처. 최근의 최첨단 접근 방식[16, 70]은 신체 전용[16, 70], 손 전용[22] 및 얼굴 전용[8] 모델 추정에 대해 유망한 결과를 얻었습니다. 이러한 노력을 결합하여 이러한 회귀 기반 접근 방식은 단안적 전신 모션 캡처에 활용되었습니다. 이러한 접근 방식[5, 7, 35, 46, 72, 78]은 일반적으로 세 개의 전문가 네트워크에 의해 신체, 손 및 얼굴 모델을 회귀시키고 다양한 전략과 함께 통합합니다. 예를 들어, PIXIE[7]는 협력적 회귀로 적분을 학습하는 반면 PyMAF-X[72]는 부자연스러운 손목 포즈를 피하기 위해 팔꿈치 비틀기 보상이 있는 적응적 적분 전략을 채택합니다. 이러한 진전에도 불구하고 기존 솔루션이 세계 공간에서 정확하면서 실시간으로 실행하는 것은 여전히 어렵습니다. 이 작업에서 우리는 새로운 데이터 생성 전략과 새로운 네트워크 아키텍처를 도입하여 그럴듯한 발-지면 접촉으로 실시간 전신 캡처를 달성합니다. 모션 캡처를 위한 신경 하강. 최적화 기반 접근 방식[4]은 일반적으로 3D 매개변수 모델을 2D 증거에 맞추지만 초기화 민감도와 까다로운 포즈를 처리하지 못하는 문제가 있습니다. 보다 효율적이고 견고한 동작 예측을 달성하기 위해 반복적 세분화를 위해 신경망의 학습 능력을 활용하려는 시도가 여러 가지 있습니다. HUND[68]는 모델 매개변수의 업데이트를 회귀시키기 위해 순환 네트워크를 기반으로 하는 학습-학습 접근 방식을 제안합니다. Song et al.[52]은 예측된 신체 모델의 포즈를 세분화하기 위해 학습된 경사 하강법을 제안합니다. 유사한 세분화 전략은 이미지 특징을 입력으로 활용하여 PyMAF[70] 및 LVD[6]에서도 활용됩니다. 저희 작업에서는 접촉 인식 신경 하강 모듈을 제안하고 보다 효과적인 동작 업데이트를 위해 기존 방식을 활용합니다. 세계 공간에서의 그럴듯한 동작 캡처. 기존 단안 동작 캡처 방법은 잘 정렬된 결과를 생성할 수 있지만 여전히 세계 공간에서 지면 관통 및 발 스케이팅과 같은 아티팩트가 발생할 수 있습니다. 더욱 물리적으로 타당한 재구성을 위해 이전 연구[17, 67]에서는 학습 과정에서 보다 정확한 카메라 모델을 활용하려는 시도를 했습니다.인간 메시의 적절한 접촉을 장려하기 위해 Rempe et al.[44]은 신체 접촉 정보를 명시적으로 학습하기 위한 물리학 기반 궤적 최적화를 제안합니다.HuMoR[45]은 동작 시퀀스에서 포즈 변화의 분포를 학습하기 위한 조건부 VAE를 도입하여 더욱 타당한 인간 포즈 예측을 위한 동작 사전을 제공합니다.LEMO[73]는 제안된 동작 부드러움을 사전에 학습하고 물리학에서 영감을 받은 접촉 마찰 항으로 최적화합니다.타당한 결과에도 불구하고 이러한 방법은 일반적으로 높은 계산 비용이 필요하고 실시간 응용 프로그램에 적합하지 않습니다.물리적 제약 조건을 보다 효과적으로 학습하기 위해 강화 학습을 통해 학습 과정에 물리 시뮬레이션을 통합하려는 시도[27, 66]가 여러 가지 있습니다.그러나 이러한 방법[27, 66]은 일반적으로 물리 기반 공식화로 인해 3D 장면 모델링에 의존합니다. 최근에는 SLAM 기법[18, 63]을 통해 카메라 동작을 복구하거나 인간의 동작 궤적을 회귀시키려는 시도[43, 55]도 있습니다. 이러한 발전에도 불구하고 이러한 방법이 실시간으로 실행되거나 세계 공간에서 물리적으로 그럴듯한 결과를 생성하는 것은 여전히 어려운 일입니다. 저희 연구에서는 인간 중심 동작을 학습하는 새로운 네트워크를 설계하여 세계 공간에서 그럴듯한 발판 접촉을 통해 실시간 캡처를 달성합니다. 3. 프록시 데이터 생성 데이터 문제를 해결하기 위해 세계 공간에서 2D 골격과 해당 3D 회전 동작을 기반으로 순차적인 프록시-동작 데이터를 합성합니다. 다음에서는 신체 동작, 손짓, 접촉 레이블을 포함하여 프록시 데이터의 다양한 유형의 레이블을 합성하고 통합하는 방법을 설명합니다. 신체 프록시 데이터. AMASS 데이터 세트[29]의 동작 시퀀스를 채택하여 신체 부위에 대한 프록시-동작 쌍을 생성합니다. AMASS 데이터 세트는 다양하고 복잡한 신체 포즈를 특징으로 하는 3,772분의 모션 시퀀스 데이터로 구성된 대규모 신체 모션 시퀀스 데이터 세트입니다. 모션 데이터를 초당 60프레임으로 다운샘플링하여 9407K 프레임을 생성합니다. 손짓과의 통합. AMASS 데이터 세트의 손짓은 주로 정적이므로 초당 30프레임으로 캡처된 1361K 프레임의 제스처 데이터가 포함된 InterHand[33] 데이터 세트의 손짓으로 프록시 데이터를 보강합니다. 우리는 구면 선형 보간(Slerp)을 사용하여 손 포즈 데이터를 40, 50, 60fps로 업샘플링하고 이를 body2D 포즈 추정기 인간 중심 프록시-투-모션 복구 Fbody Bº,0w, tw RW, TW 초기 모션 초기 카메라 포즈 RH, TH 바디 디코더 인간 공간 좌표 변환 세계 공간 바디 인코더 F&#39; 바디 크로스 어텐션 손 인코더 Fhand Xt-S 접촉 접촉 표시기 ind | RW, TW 투사 ind, XI-Fhand 접촉 예측기 지면 제약 SMPL I Sproj e Scontact Fbody, Fhand 크로스 어텐션 바이+핑거 제스처 g Αβ&#39;, Δθμ. Δεμ AR, AT 인간 공간 | 좌표 변환 손 디코더 Αβ&#39;, Αθήν. Δεν ARI, AT World Space N=3C 업데이트 신경 운동 하강 루프 그림 2. 제안된 방법 ProxyCap의 그림. 이 방법은 슬라이딩 윈도우에서 추정된 2D 골격을 입력으로 사용하여 인간 좌표 공간에서 상대적인 3D 동작을 추정합니다. 이러한 로컬 동작은 프레임별로 누적되어 글로벌 3D 동작을 복구합니다. 보다 정확하고 물리적으로 타당한 결과를 위해 초기 동작 예측을 개선하기 위해 접촉 인식 신경 운동 하강 모듈을 제안합니다. AMASS 동작 시퀀스의 포즈. 접촉 레이블과의 통합. 다음과 같이 3D 골격에 대한 연속 접촉 지표 ind를 계산합니다. (Umax - Vi). Sigmoid( indi Sigmoid(= kv Zmax Zi (1) 여기서 v¿와 z¿는 주어진 관절의 xz 평면에 대한 속도와 높이를 나타냅니다. Vmax와 Zmax는 0.2m/s와 0.08m로 설정되고 k₁와 k₂는 0.04와 0.008로 설정됩니다. 카메라 설정. 각 3D 모션 시퀀스에 대해 서로 다른 가상 카메라 궤적(실제로는 4개의 카메라)에서 2D 프록시 데이터를 생성합니다. 이러한 프록시 데이터는 2D 프록시와 3D 모션 간의 고유한 관계와 다양한 카메라 관점에서의 일관성에 대한 학습을 향상시킵니다. 구체적으로, 카메라의 내재적 매개변수에 대해 30°에서 90°까지 시야(FOV)를 균일하게 샘플링합니다. 카메라 궤적에 대한 외부 매개변수를 설정할 때 카메라를 사람 주변 1m에서 5m의 거리와 0.5m에서 2m의 높이에 배치합니다. 지면에. 마지막으로, 이러한 가상 카메라를 사용하여 3D 관절을 2D 픽셀 평면에 투영하여 가상 2D 골격 주석을 생성합니다. 또한 2D 검출기의 지터를 시뮬레이션하기 위해 투영 전에 3D 관절에 가우시안 노이즈 AX ~ N(0, 0.01)을 추가합니다. 4. 방법 그림 2에서 설명한 것처럼 먼저 이미지에서 2D 골격을 검출하여 프록시-투-모션 네트워크에 입력하여 인간 공간에서 3D 로컬 모션을 복구합니다. 이러한 상대적 로컬 모션은 세계 좌표계로 변환되어 슬라이딩 윈도우를 따라 누적됩니다. 또한 신경 하강 모듈을 활용하여 정확도와 물리적 타당성을 개선합니다. 이 섹션에서는 모션 예측의 인간 중심 설정을 소개하는 것으로 시작합니다. 4.1. 인간 중심 모션 모델링 보다 정확한 카메라 모델링을 위해 단순화된 직교 또는 약한 원근 투영[9, 15, 19, 41, 70, 77]을 사용하는 대신 고전적인 핀홀 카메라 모델을 채택합니다.그림 3에서 볼 수 있듯이 두 개의 인접한 프레임에서 글로벌 인간 모션과 카메라 궤적을 로컬 인간 공간으로 변환합니다.여기서 프레임 t에서의 모양, 포즈 및 이동의 매개변수를 나타내기 위해 {ẞ Є R¹º,0 € R22×3, tЄ R³t를 채택하고 카메라의 외부 매개변수를 나타내기 위해 {R = R³×³‚T Є R³}+를 채택합니다.포즈와 모양 매개변수가 주어지면 SMPL 레이어 내에서 조인트와 정점을 얻을 수 있습니다.{J,V} {J,V} = X(B,t,0,g).X(B,t, 0, g).다음에서 인간 좌표계와 세계 좌표계를 구분하기 위해 아래 첨자 H와 W를 사용합니다. 인간 중심 동작을 학습하는 동안, 우리는 시간적 인간 동작 사전 확률에 관한 이전 연구[24, 45, 67]에서 사용된 것과 유사한 설정을 채택합니다. 구체적으로, 각 동작 시퀀스는 xz 평면에서 초기 변환을 제거하고 루트 방향을 z축 방향으로 회전시키기 위해 강체 변환에 의해 정규화됩니다. 이 설정을 사용하면 네트워크는 관찰 시점과 무관한 인간의 상대적 동작을 학습할 수 있습니다.{RH, TH}t {0u,thời yw 프레임 t xw ZH 세계 공간 Zw ΧΗ Ун {0u,th}t_프레임 t-인간 공간 xz 평면 그림 3. 세계 공간 동작을 인간 중심 좌표와 상대적 카메라 포즈로 분리하는 그림. 인간에서 세계 좌표로의 변환과 세계 공간에서의 전역 동작 축적에 대한 자세한 구현은 보충 자료에서 찾을 수 있습니다. 4.2. 순차적 전체 신체 동작 초기화 그림 2에서 보듯이, 방법의 첫 번째 단계에서 골격 시퀀스는 시간 인코더에 의해 처리된 다음 초기 동작 {3º, 6º, tº}, 초기 카메라 {RT}, 접촉 표시기 ind 및 손 포즈 g의 예측을 위해 디코더에 입력됩니다. 이전 기준선[41]에 따라 이러한 인코더를 시간 확장 합성 신경망에 구축합니다. 더 나은 신체-손 호환성을 위해 교차 주의 메커니즘을 활용하여 신체 및 손 복구 중에 동작 컨텍스트 공유를 용이하게 합니다. 구체적으로, 먼저 시간 인코더에서 초기 신체 특징 Fbody와 손 특징 Fhand를 얻고 각각 Qbody/hand, Kbody/hand 및 Vbody/hand의 형태로 쿼리, 키, 값 행렬로 매핑합니다. 그런 다음 신체 특징 Fbody와 손 특징 Fa를 다음과 같이 업데이트합니다. 손 Fbody = Vbody + Softmax(Qhand Kody body Vbody, √dk Qbody Kand √dk Fhand = Vhand + Softmax(hand)Vhand. 업데이트된 특징 {Fbody, Fhand}는 접촉 표시기 ind에서 추가로 활용할 수 있으며 곧 설명할 것처럼 신경 하강 모듈에서 시간적 맥락으로 사용할 수 있습니다.
--- EXPERIMENT ---
s, 우리는 Eq. 2의 특징 융합이 두 가지 작업을 서로에게 이롭게 만들어 전신 모델에서 더욱 비슷한 손목 포즈를 생성할 수 있음을 보여줍니다.4.3. 접촉 인식 신경 운동 하강 방법의 두 번째 단계에서는 제안된 접촉 인식 신경 운동 하강 모듈을 사용하여 초기 동작 예측을 더욱 정확하고 물리적으로 타당하게 개선합니다.그림 2에서 볼 수 있듯이 이 모듈은 2D 정렬 불량과 신체-지면 접촉 상태를 입력으로 사용하여 반복 중에 동작 매개변수를 업데이트합니다.정렬 불량 및 접촉 계산.i Є {0, 1, ..., N}의 반복에서 이미지 평면에 3D 관절을 투영하여 2D 정렬 불량 상태를 계산하고 재투영된 2D 관절과 대리 관찰 간의 차이를 계산합니다.Sproj II(J¿, {K, R¼µ‚Â¼}) − Ĵ2D. 여기서 II(·)는 원근 투영 함수를 나타내고 K는 내재적 매개변수를 나타냅니다. = 접촉 상태의 경우 3D 관절 vin xz-평면의 속도와 건조 상태의 지면까지의 거리를 계산합니다. 또한 2D 골격의 입력에서 시간적 특징을 활용하여 신체-지면 접촉을 가리는 지표로 사용될 접촉 레이블 ind를 예측합니다. 그런 다음 현재 예측의 접촉 상태를 Scontact ind(vz, d)로 얻을 수 있습니다. 여기서 ●는 Hadamard 곱 연산을 나타냅니다. = 동작 업데이트. 접촉 및 오정렬 상태를 얻은 후 동작 업데이트를 위해 이를 신경 동작 하강 모듈에 공급합니다. 그림 4에서 볼 수 있듯이 하강 모듈은 두 그룹의 텐서를 입력으로 사용합니다. i) 상태 그룹에는 인간 좌표계 ẞ², t¹, 0²¼, 카메라 포즈 R, TH 및 순차적 동작 컨텍스트 Fseq {Fbody, Fhand}의 현재 SMPL 매개변수가 포함됩니다. ii) 편차 그룹에는 현재 오정렬 상태 Sproj와 접촉 상태 Scontact가 포함됩니다. H, 간단한 해결책은 MLP를 사용하여 이 두 그룹의 텐서를 처리하는 것입니다. 그러나 이 두 그룹의 값은 상당한 차이를 보입니다. 예를 들어, 상태 텐서의 값은 매끄럽게 변하는 반면 편차 텐서의 값은 오정렬 및 접촉 상태와 함께 빠르게 변할 수 있습니다. 단순히 입력으로 연결하면 학습 프로세스에 어려움이 발생합니다. 편차 텐서의 크기는 매개변수 업데이트와 높은 상관 관계가 있습니다. 신체 모델이 발 스케이팅이나 지면 침투 없이 잘 정렬되면 편차 텐서의 값은 거의 0이 되므로 리파인더는 포즈 매개변수의 추가 변경을 방지하기 위해 0을 출력해야 함을 나타냅니다. 그렇지 않으면 리파인더는 동작 조정을 위해 더 큰 업데이트 값을 출력해야 합니다. 이러한 상관 관계를 활용하기 위해 교차 어텐션 모듈을 활용하여 보다 효과적인 아키텍처를 구축합니다. 그림 4에서 보듯이, 두 개의 완전 연결 계층이 상태 및 편차 그룹의 텐서를 처리하고 교차 주의 모듈에 대한 쿼리, 키 및 값을 생성하는 데 활용됩니다. 이런 방식으로, 우리의 접촉 인식 신경 운동 하강 모듈은 상태와 편차 그룹 간의 관계를 효과적으로 학습하여 더 정확한 운동 업데이트를 생성할 수 있습니다. 더욱이, 순차적 운동 맥락 Fseq도 우리의 신경 하강에 활용됩니다.신경 운동 하강을 위한 교차 주의 표 1. EgoBody[74] 및 RICH[11]에 대한 양적 비교.기호 †는 SLAM에 의존하는 방법을 나타냅니다.I | 상태 그룹 인간 운동 ẞi, FC Q 카메라 포즈 R, TH Seq 특징 Fbody, Fhand | I 편차 그룹 K 2D 정렬 오류 Sproj FC 접촉 제약 조건 Scontact V 소프트 맥스 방법 EgoBody 데이터 세트 W-MPJPE↓ WA-MPJPE↓↓ PA-MPJPE↓ ACCEL↓ † SLAHMR [63] 141.101.79.25.† PACE [18] 147.101.66.6.Δβί, GLAMR [67] 416.239.114.173.Αθ 우리 385.131.73.49.MLP Δεμ RICH 데이터 세트 ARH ΔΤΗ +SLAHMR [63] 571.323.52.9.+PACE [18] 380.197.49.8.CN GLAMR [67] 우리 653.365.79.107.629.343.56.25.그림 4. 신경 하강 모듈 구현.모듈은 깊이 불확실성을 완화하고 동작 예측을 개선합니다.이전 작업[45, 52, 68, 73]과 비교할 때, 제안된 접촉 인식 신경 동작 하강 모듈은 테스트 중에 명시적인 기울기 계산이나 하이퍼파라미터의 수동 조정의 막대한 비용에서 해방된다는 이점을 제공합니다.또한, 이 모듈은 합성 데이터 세트의 접촉 정보로 인간 동작 사전을 학습할 수 있어 국소 최소값을 벗어나고 더 빠른 수렴을 달성하기 위한 보다 적합한 하강 방향과 단계를 제공합니다.4.4. 손실 함수 우리 솔루션에서 전신 동작 복구 모듈과 접촉 인식 신경 동작 하강 모듈은 순차적으로 학습됩니다.프록시-투-모션 학습의 이점을 활용하여 합성 데이터 세트에서 실제 전신 포즈 0, g 및 인체 모양 ẞ를 감독을 위해 얻을 수 있습니다. 전반적으로 동작 복구의 목적 함수는 다음과 같이 작성할 수 있습니다. Lrec = L3D + L 2D + Lo + LB + Lcam + £consist+Lsmooth (3) 구체적으로 3D는 3D MPJPE 손실과 3D 궤적 L1 손실을 수반하는 반면 L2D는 투영된 2D MPJPE 손실입니다. Le, LB, Leam은 추정된 인간 포즈, 모양 및 카메라 포즈와 합성 기준 진실 사이의 L1 손실을 나타냅니다.consist는 가상 카메라의 다른 관찰을 통해 동일한 3D 동작 시퀀스의 로컬 동작 출력 OH, tH의 일관성을 제한하기 위한 L1 손실입니다. Lsmooth는 추정과 기준 진실 사이의 속도와 가속도에 페널티를 부여하여 [69]에서 채택되었습니다. 신경 하강 모듈의 경우, 목적 손실은 다음과 같이 쓸 수 있습니다. Ldesc = Σk UN-k (Lrec + Lcontact) contact = indgt ○ (||Vxz||2 + ||dy||2) LindΣ Entropy (indgt, indest) (4) 여기서 k = 1, 2, ..., N은 반복 시간이고 u는 마지막 반복을 강조하는 감소 비율입니다. 실제로는 K = 3, u = 0으로 설정합니다. Lcontact에는 궤적 표류, 발 떠다니기 또는 지면 관통 오류가 포함됩니다. Lind는 예측된 접촉 레이블과 실제 결과 사이의 손실을 나타냅니다. 5. 실험 이 섹션에서는 방법의 효능을 검증하고 세계 공간에서 물리적으로 그럴듯한 발-지면 접촉으로 정확한 인간 동작 캡처 결과를 보여줍니다. 데이터 세트. RICH 데이터 세트[11]는 다중 뷰 정적 카메라 시스템과 하나의 이동 카메라로 수집되었으며 실제 3D 인간 동작은 공간 스테레오 기하학을 사용하여 복구할 수 있습니다. EgoBody[74]는 멀티 카메라 장비와 헤드 마운트 디바이스로 캡처되어 3D 장면의 사회적 상호 작용에 초점을 맞춥니다.Dynamic Human3.6M은 사람 주변의 작은 뷰 창으로 무작위로 잘라내어 Human3.6M[13]에서 움직이는 카메라를 시뮬레이션하기 위해 [67]에서 제안된 벤치마크입니다.메트릭스.실험에서는 이전 연구[67]에 따라 다양한 메트릭을 보고하며, 주로 세계 좌표계 평가에 초점을 맞춥니다.WA-MPJPE 메트릭은 예측된 궤적과 GT의 전체 궤적을 Procrustes Alignment를 통해 정렬한 후의 MPJPE를 보고합니다.W-MPJPE 메트릭은 시퀀스의 첫 번째 프레임을 정렬한 후의 MPJPE를 보고합니다.PA-MPJPE 메트릭은 각 프레임에 기준 진실 궤적을 적용한 후의 MPJPE 오류를 보고합니다.ACCEL 메트릭은 관절 가속도를 평가하는 데 사용됩니다.5.1. 최신 기술과의 비교 우리는 우리의 접근 방식을 GLAMR[67], SLAMHR[63] 및 PACE[18]를 포함한 동적 카메라에서 인간의 동작을 복구하는 최신 기술 접근 방식과 비교합니다.SLAMHR과 PACE는 모두 카메라 궤적을 해결하기 위해 장면을 재구성하는 전처리 SLAM이 필요합니다(표 4 참조).이러한 프로세스는 시간이 많이 걸리고 텍스처가 풍부한 배경이 필요하므로 적용 범위가 좁아집니다.제안된 솔루션의 효과를 검증하기 위해 우리는 주로 SLAM 없이 실행되는 GLAMR과 우리 방법을 비교합니다.또한 표에 표시된 것처럼 RICH 및 EgoBody 데이터 세트에 대한 비교 실험을 수행합니다. 1. 표에서 볼 수 있듯이, 우리의 방법은 이전의 (a (b (c) (d) (e) (f) 그림 5. (a,g) 3DPW [61], (b) EHF [40], (c) Human3.6M [13] 데이터 세트와 (d,e,f) 인터넷 비디오에서 다양한 사례에 걸친 결과. 우리는 우리의 방법이 실시간 성능으로 움직이는 카메라에서 정확하고 그럴듯한 인간의 동작을 복구할 수 있음을 보여줍니다. 구체적으로, (g)는 모든 메트릭에서 폐색 입력 솔루션에서도 우리 방법의 견고성과 시간적 일관성을 보여줍니다. 이전의 다양한 솔루션과의 시각적 비교도 그림 6과 보충 자료의 비디오에 나와 있으며, 여기서도 우리 방법은 다시 한번 세계 공간에서 모델-이미지 정렬과 발-지면 접촉 측면에서 뛰어난 결과를 보여줍니다. (a) 발-지면 거리가 주어진 임계값에서 멀리 떨어진 프레임. 우리는 그림 7에서 지면까지의 거리에 대한 GP 및 FF의 곡선을 보고합니다. 대수적 척도로, 신경 하강 알고리즘이 지면 접촉 타당성을 크게 개선할 수 있다는 결론을 내릴 수 있습니다.10% (b) 1% 0.1% --- FF w/o desc ---GP w/o desc ---FF w/ desc ---GP w/ desc 0.0.0.지면까지의 거리/m 0.0.(c) (d) 그림 7. 발이 떠 있는 비율(FF)과 지면 관통력(GP)에 대한 절제 연구. 임계값을 1cm에서 3cm로 변경하여 해당 FF 및 GP 메트릭을 계산합니다.5.3. 런타임 제안된 방법이 이전 방법에 비해 속도가 10배 향상되었다는 점도 주목할 만합니다. 다양한 방법의 속도는 표 4에 보고되어 있습니다.우리의 방법은 RTX4060이 있는 노트북에서 30FPS의 실시간 성능에 도달할 수 있으며, 이는 가상 인간과 관련된 다양한 응용 프로그램을 구현하는 데 매우 유망합니다. 표 3. 최신 방법과의 런타임 비교.방법 FPS SLAMHR PACE GLAMR 0.2.2.우리의 그림 6. 이전 최신 방법과의 정성적 비교: (a) PyMAF-X [72], (c) GLAMR [67], (b)(d) 우리의.5.2. 절제 연구 우리는 [67]의 설정에 따라 Dynamic Human3.6M 데이터 세트에서 제안된 신경 하강 방법의 효과를 검증하기 위해 절제 연구를 수행합니다.표 2에서 볼 수 있듯이 신경 하강 모듈은 세계 공간에서 동작 오류를 크게 줄일 수 있습니다.표 2. Dynamic Huaman3.6M 데이터 세트에서 신경 하강 모듈의 절제 연구. 신경 하강 w/ow/ W-MPJPE↓ 644.605.PA-MPJPE↓ 48.45.또한 Human3.6M [13] 데이터 세트에서 지면 관통 [66] (GP) 및 발 떠 있기(FF)의 메트릭을 보고합니다.GP는 지면으로 관통하는 프레임의 백분율로 정의됩니다.FF는 6의 백분율로 정의됩니다.
--- CONCLUSION ---
이 논문에서 우리는 세계 공간에서 물리적으로 타당한 발-지면 접촉을 이용한 실시간 단안 전신 모션 캡처 접근법인 ProxyCap을 제시합니다. 우리는 세계 공간에서 정확한 3D 회전 모션을 이용한 2D 골격 시퀀스를 기반으로 하는 프록시 데이터 세트를 활용합니다. 프록시 데이터를 기반으로 우리 네트워크는 인간 중심의 지각으로부터 모션을 학습하여 다양한 카메라 궤적에서 인간의 모션 예측의 일관성을 향상시킵니다. 보다 정확하고 물리적으로 타당한 모션 캡처를 위해 우리는 네트워크가 발-지면 접촉과 모션 오정렬을 인식할 수 있도록 접촉 인식 신경 모션 하강 모듈을 추가로 제안합니다. 제안된 솔루션을 기반으로 움직이는 카메라 아래에서 실시간 단안 전신 캡처 시스템을 시연합니다. 제한 사항. 우리 방법은 2D 관절 관찰에서 3D 모션을 복구하기 때문에 사람이 거의 정적인 포즈로 캡처될 때 특히 깊이 모호성 문제가 남아 있습니다.(a) 원래 확장된 변환 (b) 부분 확장된 변환 그림 8. 인간 복구의 예. 왼쪽 부분(a)은 [41]의 원래 확장 합성곱 백본을 나타내고, 오른쪽 부분(b)은 제안한 부분 확장 합성곱 아키텍처를 설명합니다. 저희의 접근 방식은 입력 신호에서 손상된 데이터를 선택적으로 제외하여 동작 특징을 보다 정확하고 효과적으로 추출할 수 있습니다. 구체적으로, 매우 빠른 동작이나 심각한 폐색 상황에서는 감지 실패(녹색 모자이크 사각형으로 표시)가 발생할 수 있지만, 저희 아키텍처는 네트워크 순방향 처리 중에 교란 전송을 방지하기 위해 손상된 데이터와의 연결을 차단합니다. 보충 자료 7. 구현 세부 정보 7.1. 인간-세계 좌표 변환 저희는 프록시-동작 네트워크에서 각 프레임의 로컬 인간 포즈를 추정한 다음, 이를 글로벌 세계 공간으로 변환합니다. 첫 번째 프레임에서 xz 평면 txz에서 누적된 인간 변환은 0으로 설정됩니다. 이후의 인간 공간 추정의 경우, 먼저 인간 공간에서 카메라의 전면 축을 Rfront만큼 회전하여 yz 평면을 정렬합니다. 세계 공간에서 카메라의 대상 매개변수를 Rw, Tw로 표시하고 인간 공간에서 예측 매개변수를 RH, TH로 표시합니다. Rw RH Rfront, Tw -Rw (RFront (-R · TH) + txz)입니다. 그리고 인간의 . . = = 방향도 상대적 정지 상태를 유지하기 위해 같은 시간에 회전해야 합니다. Ow (루트) = R Front OH (루트). 세계 이동은 tw = R Front (t+ Jroot) – Jroot + txz로 계산할 수 있습니다. 여기서 Jroot는 SMPL 모델의 루트 조인트로, SMPL 모델의 원래 점과 루트 정렬이 맞지 않아 발생하는 오류를 제거하기 위한 T-포즈입니다. 마지막으로 xz 평면 txz에서 인간의 누적 이동은 t+1 = t/xz + R front tH로 업데이트됩니다. 7.2. 부분 확장 합성곱. 손 영역은 끝부분이기 때문에 심한 모션 블러와 심각한 폐색의 영향을 받기 쉽고, 이로 인해 감지가 누락됩니다. 단순히 손상된 데이터를 0으로 설정하는 것은 원래의 합성 커널이 정상 데이터와 손상된 데이터를 구별할 수 없기 때문에 실행 가능한 솔루션이 아니며, 노이즈가 네트워크 계층을 통해 전파되면서 성능이 크게 저하됩니다. 이러한 과제를 극복하기 위해 부분 합성[25]을 사용하여 1D 확장 합성 프레임워크를 개선합니다. 그림 8에서 볼 수 있듯이 원래 합성 연산자에서처럼 입력 신호를 무차별적으로 처리하는 대신 마스크 가중 부분 합성을 사용하여 손상된 데이터를 입력에서 선택적으로 제외합니다. 이를 통해 빠른 움직임과 심한 폐색이 포함된 시나리오에서 손을 복구하는 견고성이 향상됩니다. 구체적으로, 잠재 코드 Xo는 각 프레임 ƒ = 1, 2, ..., L에 대한 J 관절의 (x, y) 좌표의 연결로 처음 설정되고, 마스크 Mo는 감지 신뢰도에 해당하는 0 또는 1의 값을 갖는 이진 변수로 초기화됩니다. 그런 다음 [25]의 2D 부분 합성곱 연산과 마스크 업데이트 함수를 1D 확장 합성곱 프레임워크에 통합했습니다. [ Mk+= I (sum(Mk) &gt; 0) Xk+1 = Mk+1 (W} (Xk © Mk) size(Mk) sum(Mk) + bk (5) 여기서 W와 b는 레이어 k의 합성곱 필터 가중치와 바이어스를 나타내고 ○는 요소별 곱셈을 나타냅니다. 또한 학습 절차에서 순차적 입력의 절반은 배포 환경에서 발생할 수 있는 감지 실패를 시뮬레이션하기 위해 무작위로 마스크됩니다. 7.3. 학습 NVIDIA RTX 4090에서 1e-4의 학습 속도와 1024의 배치 크기를 사용하여 Adam 최적화 도구를 사용하여 네트워크를 학습합니다. 2단계 학습 방식을 채택합니다. 먼저 50개 에포크 동안 프록시-투-모션 초기화 네트워크(4.2절)를 학습합니다. 그런 다음 모션의 가중치를 고정합니다. 복구 네트워크를 사용하고 신경 하강 네트워크(4.3절)를 50번 더 학습합니다.7.4. 프록시 데이터 세트 우리는 합성 프록시 데이터 세트에서 학습 프로세스를 수행합니다.학습 세트의 3D 신체 회전 동작은 AMASS[29]에서 샘플링합니다.[ACCAD, BMLmovi, BMLrub, CMU, CNRS, DFaust, EKUT, Eyes Japan Dataset, GRAB, HDM05, HumanEva, KIT, MoSh, PosePrior, SFU, SOMA, TCDHands, TotalCapture, WEIZMANN] 및 [SSM, Transitions]는 테스트용입니다.그 외에 손 동작을 생성하기 위해 InterHand[33]의 동일한 데이터 세트 부문을 채택합니다.그런 다음 SMPL-X 메시를 애니메이션화하고 가상 카메라를 생성하여 가상 2D 레이블을 얻습니다.8. 계산 복잡도 이 섹션에서는 우리 방법의 추론 속도를 비교합니다. 영어: 실시간 단안 전신 캡처 시스템은 단일 노트북(NVIDIA RTX 4060 GPU)에 구현할 수 있습니다. 특히 2D 포즈 추정기의 경우 기성품 Mediapipe[26]와 MMPose를 활용하여 NVIDIA TensorRT 플랫폼에 다시 구현합니다. 표 4에 각 모듈의 추론 시간을 보고합니다. 표 4. 파이프라인의 각 모듈의 시간 비용. 네트워크 입력 속도 바디 크롭 넷 바디 랜드마크 넷 224 x 224 x2ms 384 x 288 x5ms 2 x 256 x 256 x1.5ms 256 x 256 x81 x 67 x2ms 3ms 10ms 핸드 크롭 넷 핸드 랜드마크 넷 포즈 초기화 넷 신경 하강 넷 참고 문헌 [1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler 및 Bernt Schiele. 2d 인체 포즈 추정: 새로운 벤치마크 및 최신 분석. CVPR, 3686-3693페이지, 2014년.[2] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T Freeman, Rahul Sukthankar, Cristian Sminchisescu. Hspace: 복잡한 환경에서 애니메이션화된 합성 매개변수 인체. arXiv 사전 인쇄본 arXiv:2112.12867, 2021년.[3] Michael J. Black, Priyanka Patel, Joachim Tesch, Jinlong Yang. BEDLAM: 세부적인 실물과 같은 애니메이션 동작을 보이는 신체의 합성 데이터 세트. CVPR, 8726-8737페이지, 2023년.[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J Black. SMPL 유지: 단일 이미지에서 3D 인체 포즈 및 모양 자동 추정. ECCV, 561-578페이지. Springer, 2016. 1,[5] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, Michael J Black. 신체 중심 주의를 통한 단안 표현 신체 회귀. ECCV, 20-40페이지. Springer, 2020.[6] Enric Corona, Gerard Pons-Moll, Guillem Alenyà, Francesc Moreno-Noguer. 학습된 정점 하강: 3D 인체 모델 피팅을 위한 새로운 방향. ECCV, 146-165페이지, 2022.[7] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, Michael J Black. 조절을 사용한 표현적 신체의 협력적 회귀. 13DV, 792804페이지, 2021.[8] Yao Feng, Haiwen Feng, Michael J. Black 및 Timo Bolkart. 야생 이미지에서 애니메이션 가능한 세부 3D 얼굴 모델 학습. TOG, 40(4):88:1-88:13, 2021.[9] Kehong Gong, Bingbing Li, Jianfeng Zhang, Tao Wang, Jing Huang, Michael Bi Mi, Jiashi Feng 및 Xinchao Wang. Posetriplet: 자기 감독 하에 공진화하는 3D 인간 포즈 추정, 모방 및 환각. CVPR, 11017-11027페이지, 2022.[10] Peng Guan, Alexander Weiss, Alexandru O Balan 및 Michael J Black. 단일 이미지에서 인체 모양과 포즈 추정. ICCV, 1381-1388페이지. IEEE, 2009.[11] Chun-Hao P Huang, Hongwei Yi, Markus Höschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, Michael J Black. 밀도 있는 전신 인체-장면 접촉 캡처 및 추론. CVPR, 13274-13285페이지, 2022. 1,[12] Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V Gehler, Javier Romero, Ijaz Akhter, Michael J Black. 시간 경과에 따른 정확한 마커 없는 인체 모양 및 포즈 추정. 13DV, 421-430페이지. IEEE, 2017.[13] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: 자연 환경에서 3D 인간 감지를 위한 대규모 데이터 세트 및 예측 방법. TPAMI, 36(7):1325-1339, 2014. 1, 2, 6, 7,[14] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. 야생에서의 3D 인간 포즈 추정을 위한 3D 인간 포즈 피팅을 위한 모범적 미세 조정. 13DV, 42-52페이지, 2021.[15] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. 인간 모양과 포즈의 종단 간 복구. CVPR, 7122-7131페이지, 2018. 1,[16] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges, 및 Michael J Black. PARE: 3D 인체 추정을 위한 부분 주의 회귀자. ICCV, 11127-11137페이지, 2021. 2,[17] Muhammed Kocabas, Chun-Hao P Huang, Joachim Tesch, Lea Muller, Otmar Hilliges, 및 Michael J Black. SPEC: 추정된 카메라로 야외에서 사람 보기. ICCV. 11035-11045페이지, 2021. 2,[18] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, 및 Umar Iqbal. PACE: 야생 비디오에서 인간 및 동작 추정. 3DV, 2024. 2, 3,[19] Nikos Kolotouros, Georgios Pavlakos, Michael J Black 및 Kostas Daniilidis. 루프에서 모델 피팅을 통해 3D 인간 포즈 및 모양을 재구성하는 방법 학습. ICCV, 2252-2261페이지, 2019. 1, 2,[20] Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J Black 및 Peter V Gehler. 사람들을 통합하세요: 3D 및 2D 인간 표현 간 루프 닫기. CVPR, 6050-6059페이지, 2017.[21] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang 및 Cewu Lu. HybrIK: 3D 인간 포즈 및 모양 추정을 위한 하이브리드 분석-신경 역 운동학 솔루션. CVPR, 3383-3393페이지, 2021. 2,[22] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu, Feng Chen, Tao Yu, Yebin Liu. 단일 이미지 양손 재구성을 위한 상호 작용 주의 그래프. CVPR, 2761-2770페이지, 2022.[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 맥락 속의 공통 객체. ECCV, 740-755페이지. Springer, 2014.[24] Hung Yu Ling, Fabio Zinno, George Cheng, Michiel van de Panne. 모션바를 사용하는 캐릭터 컨트롤러. ACM 트랜스. 그래프., 39(4), 2020.[25] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao 및 Bryan Catanzaro. 부분 컨볼루션을 사용하여 불규칙한 구멍에 대한 이미지 인페인팅. 컴퓨터 비전에 관한 유럽 회의(ECCV) 간행물, 2018.[26] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, 이주현 외. Mediapipe: 인식 파이프라인을 구축하기 위한 프레임워크입니다. arXiv 사전 인쇄 arXiv:1906.08172, 2019.[27] Zhengyi Luo, Shun Iwase, Ye Yuan, Kris Kitani. 구현된 장면 인식 인간 포즈 추정. NeurIPS, 2022.[28] Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, Yizhou Wang. 가상 마커에서 3D 인간 메시 추정. CVPR, 534-543페이지, 2023. 2,[29] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, Michael J Black. AMASS: 표면 모양으로서의 모션 캡처 아카이브. ICCV, 54425451페이지, 2019. 2, 3,[30] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, Christian Theobalt. 개선된 CNN 감독을 사용한 야생에서의 단안 3D 인간 포즈 추정. 13DV, 506-516페이지, 2017.[31] 경식 문 및 경무 이. I2L-MeshNet: 단일 RGB 이미지에서 정확한 3D 인간 포즈 및 메시 추정을 위한 이미지-lixel 예측 네트워크. ECCV, 752-768페이지. Springer, 2020. 2,[32] 경식 문 및 경무 이. Pose2Pose: 표현력이 풍부한 3D 인간 포즈 및 메시 추정을 위한 3D 위치 포즈 안내 3D 회전 포즈 예측. arXiv 사전 인쇄본 arXiv:2011.11534, 2020. 2,[33] 경식 문, Shoou-I Yu, He Wen, Takaaki Shiratori, 및 Kyoung Mu Lee. InterHand2.6M: 단일 RGB 이미지에서 3D 상호작용 손 자세 추정을 위한 데이터 세트 및 베이스라인. ECCV에서, 548-564페이지. Springer, 2020. 3,[34] Kyungsik Moon, Hongsuk Choi, Kyoung Mu Lee. NeuralAnnot: 3D 인간 메시 학습 세트를 위한 신경 주석기. CVPRW에서, 2299-2307페이지, 2022.[35] Kyungsik Moon, Hongsuk Choi, Kyoung Mu Lee. 전신 3D 인간 메시 추정을 위한 정확한 3D 손 자세 추정. CVPRW에서, 2022.[36] Lea Müller, Ahmed AA Osman, Siyu Tang, Chun-Hao P Huang, Michael J Black. 자기 접촉과 인간 자세에 관하여. CVPR, 9990-9999페이지, 2021.[37] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, Bernt Schiele. 신경 바디 피팅: 딥 러닝과 모델 기반 인간 포즈 및 형태 추정 통합. 13DV, 484-494페이지. IEEE, 2018. 2,[38] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, David T Hoffmann, Shashank Tripathi, Michael J Black. AGORA: 회귀 분석에 최적화된 지리학의 아바타. CVPR, 13468-13478페이지, 2021. 2,[39] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, Kostas Daniilidis. 단일 색상 이미지에서 3D 인체 포즈와 형태를 추정하는 방법 학습. CVPR, 459-468페이지, 2018. 2,[40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, Michael J. Black. 표현적인 신체 캡처: 단일 이미지에서 3D 손, 얼굴 및 신체. CVPR, 2019. 1,[41] Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli. 시간적 합성곱 및 반지도 학습을 통한 비디오에서의 3D 인체 포즈 추정. 페이지 7753-7762, 2019. 4, 5,[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou. 신경체: 역동적인 인간의 새로운 관점 합성을 위한 구조화된 잠재 코드를 포함하는 암묵적 신경 표현. CVPR, 페이지 9054-9063, 2021.[43] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Jitendra Malik. 3D 모양, 위치 및 포즈를 예측하여 사람 추적. CVPR, 페이지 2740-2749, 2022.[44] Davis Rempe, Leonidas J Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, Jimei Yang. 단안 비디오의 접촉 및 인간 역학. ECCV, 71-87페이지. Springer, 2020.[45] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas J. Guibas. 유머: 견고한 포즈 추정을 위한 3D 인체 동작 모델. ICCV, 2021. 3, 4,[46] Yu Rong, Takaaki Shiratori, Hanbyul Joo. FrankMocap: 회귀 및 적분을 통한 단안 3D 전신 포즈 추정 시스템. ICCV, 2021.[47] Nadine Rueegg, Christoph Lassner, Michael Black, Konrad Schindler. 체인 표현 사이클링: 표현 간 사이클링을 통해 3D 인체 포즈와 모양을 추정하는 방법 학습. AAAI, 5561-5569페이지, 2020. 2,[48] Johannes Lutz Schönberger 및 Jan-Michael Frahm. 구조-동작 재검토. 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 2016.[49] Akash Sengupta, Ignas Budvytis 및 Roberto Cipolla. 야생에서 정확한 3D 인간 포즈 및 모양 추정을 위한 합성 훈련. BMVC, 2020.[50] Leonid Sigal, Alexandru Balan 및 Michael J Black. 결합된 차별적 및 생성적 관절 포즈 및 비강체 모양 추정. NeurIPS, 1337-1344페이지, 2008.[51] Leonid Sigal, Alexandru O Balan 및 Michael J Black. HumanEva: 동기화된 비디오 및 모션 캡처 데이터 세트와 관절형 인간 모션 평가를 위한 기준 알고리즘. IJCV, 87(1-2):4, 2010. 1,[52] Jie Song, Xu Chen 및 Otmar Hilliges. 학습된 경사 하강법을 통한 인체 모델 피팅. ECCV, 744-760페이지. Springer, 2020. 2, 3,[53] Jiajun Su, Chunyu Wang, Xiaoxuan Ma, Wenjun Zeng 및 Yizhou Wang. Virtualpose: 가상 데이터에서 일반화 가능한 3D 인간 포즈 모델 학습. ECCV, 55-71페이지. Springer, 2022.[54] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu 및 Tao Mei. 골격 풀림 표현을 통해 단안 이미지에서 인간 메시 복구. ICCV, 53495358페이지, 2019. 2,[55] Yu Sun, Qian Bao, Wu Liu, Tao Mei, Michael J Black. TRACE: 3D 환경에서 동적 카메라를 사용한 아바타의 5D 시간 회귀. CVPR, 8856-8866페이지, 2023.[56] Zachary Teed 및 Jia Deng. DROID-SLAM: 단안, 스테레오 및 RGB-D 카메라를 위한 심층 시각 SLAM. 신경 정보 처리 시스템의 발전, 2021.[57] Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang. 단안 이미지에서 3D 인간 메시 복구: 조사. TPAMI, 2023.[58] Hsiao-Yu Fish Tung, Hsiao-Wei Tung, Ersin Yumer, Katerina Fragkiadaki. 동작 캡처의 자기 감독 학습. NeurIPS, 5236-5246페이지, 2017. 2,[59] Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, Cordelia Schmid. 합성 인간으로부터 학습. CVPR, 109-117페이지, 2017.[60] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, Cordelia Schmid. BodyNet: 3D 인체 모양의 체적 추론. ECCV, 20-36페이지, 2018. 2,[61] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, Gerard Pons-Moll. imus와 움직이는 카메라를 사용하여 야외에서 정확한 3D 인간 포즈 복구. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2018.[62] Yuanlu Xu, Song-Chun Zhu 및 Tony Tung. DenseRaC: dense render-andcompare를 통한 공동 3D 포즈 및 모양 추정. ICCV, 7760-7770페이지, 2019. 2,[63] Vickie Ye, Georgios Pavlakos, Jitendra Malik 및 Angjoo Kanazawa. 야생에서 비디오에서 인간 및 카메라 동작 분리. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 2023. 2, 3,[64] Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas, Muhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus Thies 및 Michael J. Black. 시각적 환경 재구성을 위한 인간 인식 객체 배치. CVPR, 3959-3970페이지, 2022.[65] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, Hyun Soo Park. HUMBI: 인체 표정의 대규모 다중 시점 데이터 세트. CVPR, 2990-3000페이지, 2020.[66] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, Jason Saragih. SimPoE: 3D 인간 포즈 추정을 위한 시뮬레이션된 캐릭터 제어. CVPR, 7159-7169페이지, 2021. 3,[67] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, Jan Kautz. GLAMR: 동적 카메라를 사용한 글로벌 오클루전 인식 인간 메시 복구. CVPR, 1103811049페이지, 2022. 2, 3, 4, 6,[68] Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T Freeman, Rahul Sukthankar, Cristian Sminchisescu. 시각적 3D 인간 포즈 및 모양을 위한 신경 하강. CVPR, 14484-14493페이지, 2021. 3,[69] Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang, Qiang Xu. SmoothNet: 비디오에서 인간 포즈를 정제하기 위한 플러그 앤 플레이 네트워크. ECCV, 625-642페이지. Springer, 2022.[70] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang 및 Zhenan Sun. PyMAF: 피라미드 메쉬 정렬 피드백 루프를 사용한 3D 인간 자세 및 모양 회귀. ICCV, 페이지 11446–11456, 2021. 1, 3,[71] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang 및 Zhenan Sun. 조밀한 신체 부위로부터 3D 인간의 형태와 포즈를 학습합니다. TPAMI, 2022. 2,[72] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun 및 Yebin Liu. PyMAF-X: 단안 이미지에서 잘 정렬된 전신 모델 회귀를 지향합니다. TPAMI, 2023. 1, 3,[73] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys, 및 Siyu Tang. 3D 장면에서 4D 인체 캡처를 위한 학습 동작 사전. ICCV, 11343-11353페이지, 2021. 3,[74] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, 및 Siyu Tang. Egobody: 헤드 마운트 장치에서 상호 작용하는 사람들의 인체 모양 및 동작. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022.[75] Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, 및 Yebin Liu. 여러 대의 비디오 카메라를 사용한 실시간 다중인 동작 캡처를 위한 4D 연관 그래프. CVPR, 1324-1333페이지, 2020.[76] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu, 및 Yebin Liu. 희소한 멀티뷰 카메라를 사용한 경량 다중인물 전체 동작 캡처. ICCV, 5560-5569페이지, 2021.[77] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, 및 Zhengming Ding. 공간 및 시간 변환기를 사용한 3D 인간 포즈 추정. ICCV, 11656-11665페이지, 2021.[78] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, 및 Feng Xu. 부분 간 상관 관계를 사용한 단안 실시간 전신 캡처. CVPR, 4811-4822쪽, 2021년.
