--- ABSTRACT ---
Botao Yu³ Xu Tan² Wei Ye 4 Shikun Zhanghttps://github.com/microsoft/muzic 음악은 감정을 전달하는 데 사용되므로 자동 음악 생성에서 감정적 음악을 생성하는 것이 중요합니다. 감정적 음악 생성에 대한 이전 연구는 주석이 달린 감정 레이블을 제어 신호로 직접 사용하는데, 이는 주관적 편향으로 어려움을 겪습니다. 다른 사람들은 같은 음악에 다른 감정에 주석을 달 수 있고, 한 사람은 다른 상황에서 다른 감정을 느낄 수 있습니다. 따라서 종단 간 방식으로 감정 레이블을 음악 시퀀스에 직접 매핑하면 학습 프로세스가 혼란스러워지고 모델이 일반적인 감정으로 음악을 생성하는 데 방해가 됩니다. 이 논문에서는 감정과 음악 사이의 다리 역할을 하는 감정 관련 음악 속성 집합을 활용하고 생성을 두 단계로 나누는 감정적 음악 생성 시스템인 EMOGEN을 제안합니다. 감독 클러스터링을 사용한 감정-속성 매핑과 자체 감독 학습을 사용한 속성-음악 생성입니다. 두 단계 모두 유익합니다. 첫 번째 단계에서 클러스터링 센터 주변의 속성 값은 이러한 샘플의 일반적인 감정을 나타내며, 이는 감정 레이블의 주관적 편향의 영향을 제거하는 데 도움이 됩니다. 두 번째 단계에서 생성은 감정 레이블에서 완전히 분리되어 주관적 편향에서 자유로워집니다. 주관적 평가와 객관적 평가 모두 EMOGEN이 감정 제어 정확도와 음악 품질에서 각각 이전 방법보다 우수한 것으로 나타났으며, 이는 감정적 음악을 생성하는 데 있어 당사의 우수성을 보여줍니다. EMOGEN에서 생성한 음악 샘플은 이 링크¹에서 사용할 수 있으며, 코드는 이 링크²에서 사용할 수 있습니다. *동등한 기여 상하이 교통대학교, 중국 Microsoft Research Asia 3 난징대학교, 중국 *페킹대학교, 소프트웨어 공학을 위한 국가 공학 연구 센터, 중국. 서신: Xu Tan<xuta@microsoft.com> . &#39;https://ai-muzic.github.io/emogen/ 2https://github.com/microsoft/muzic/ 1.
--- INTRODUCTION ---
Jiang Bian심층 학습의 발전으로 자동 음악 생성이 빠르게 발전하고 점점 더 많은 관심을 끌고 있습니다(Hernandez-Olivan &amp; Beltran, 2022; Shih et al., 2022; Yu et al., 2022). 음악에 대한 감정의 중요성으로 인해 감정적 음악 생성은 중요하고 실용적인 작업이지만 아직 충분히 탐구되지 않았습니다. 이전 연구는 감정 신호를 적용하는 방식에 따라 두 가지 유형으로 나눌 수 있습니다. 첫 번째 유형은 감정 레이블을 임베딩으로 변환하여 모델 입력으로 사용하는 것입니다(Madhok et al., 2018; Hung et al., 2021; Pangestu &amp; Suyanto, 2021; Sulun et al., 2022; Grekow &amp; Dimitrova-Grekow, 2021). 두 번째 유형은 감정 분류기를 훈련하여 디코딩 프로세스를 안내하기 위해 두 모델 출력에 적용하는 것입니다(Ferreira &amp; Whitehead, 2019; Ferreira et al., 2020; 2022; Bao &amp; Sun, 2022) 또는 변분 자동 인코더의 잠재 공간(Tan &amp; Herremans, 2020)과 생성적 적대 네트워크(Tseng et al., 2021)를 사용하여 잠재 벡터의 분포를 제한합니다. 그러나 위의 두 가지 유형의 작업은 모두 음악 시퀀스를 엔드투엔드 방식으로 생성하기 위한 제어 신호로 감정 레이블을 직접 사용하는데, 이는 최적이 아닙니다. 데이터 주석자가 제공한 감정 레이블은 객관적 요인과 주관적 요인 모두의 영향을 받을 수 있습니다. 템포와 음표 밀도와 같은 객관적 요인은 음악 감정과 높은 관련이 있습니다. 주관적 요인에 있어서, 지각된 감정은 사회적 정체성, 성격, 청취자의 즉각적인 감정 상태 등과 높은 관련이 있다. 예를 들어, 청취자가 화가 난 상태일 때 행복한 노래를 슬픈 노래라고 생각할 가능성이 높다. 이러한 인간 감정의 주관성으로 인해, 서로 다른 데이터 주석자는 동일한 감정을 가진 샘플에 서로 다른 감정 레이블을 부여할 수 있으며, 이는 감정 레이블에 주관적인 편향을 초래한다. 일관되지 않은 감정 레이블로 인해 이러한 엔드투엔드 방법은 감정과 음악 시퀀스 간의 관계를 학습하기 어렵고, 따라서 모델은 원하는 감정과 정확히 일치하는 음악을 생성하는 데 부족할 수 있다. 본 논문에서는 감정 레이블의 주관적 편향의 영향을 제거할 수 있는 감정 음악 생성 시스템인 EMOGEN을 제안한다. 감정 레이블을 음악 시퀀스에 종단 간 방식으로 직접 매핑하는 대신, EmoGen: 감정적 음악 생성에서 주관적 편향 제거에서는 감정과 높은 상관 관계가 있는 음악 속성 집합을 다리로 활용하여 이 작업을 두 단계로 나눕니다. 지도 클러스터링을 사용한 감정-속성 매핑과 자기 지도 학습을 사용한 속성-음악 생성입니다. 구체적으로, 감정과 음악 간의 격차를 메우기 위해 속성은 감정과 높은 상관 관계가 있어야 합니다. 레이블이 지정된 데이터 세트에서 감정 분류기를 학습하고 기능 중요도가 높은 속성을 선택하여 속성 집합을 설계합니다. 감정-속성 매핑 단계에서는 샘플을 감정 레이블로 클러스터링하고 각 클러스터의 평균 속성 값을 계산하여 얻은 클러스터링 센터에 가장 가까운 샘플의 속성 값에 감정을 매핑합니다. 이 클러스터링 프로세스는 감정 레이블을 사용하여 샘플을 감정 범주로 클러스터링하기 때문에 지도됩니다. 이 지도 클러스터링을 통해 매핑된 속성 값은 클러스터링 센터 주변의 샘플에서 일반적인 감정을 나타낼 수 있습니다. 따라서 감정 레이블의 주관적 편향 문제를 제거할 수 있습니다. 속성-음악 생성 단계에서 음악 시퀀스에서 속성 값을 추출하고 이러한 속성을 제어 신호로 사용하여 자기 지도 방식으로 Transformer 기반 모델을 학습합니다. 속성 값은 음악 시퀀스에서 직접 추출할 수 있으므로 이 생성 모델은 레이블이 지정된 데이터가 필요 없이 제어 신호와 음악 간의 관계를 학습할 수 있습니다. 이 프로세스는 감정 레이블과 완전히 분리되어 있으므로 감정 레이블의 주관적 편향의 영향을 받지 않습니다. 주관적 편향을 피하기 위한 지도 클러스터링과 자기 지도 학습을 기반으로 하는 두 단계의 이점을 통해 EMOGEN은 감정 음악 생성에서 보다 정확한 감정 제어를 달성할 수 있습니다. 이 작업의 주요 기여는 다음과 같습니다. • 감정 레이블의 주관적 편향을 제거할 수 있는 감정 음악 생성 시스템인 EMOGEN을 제안합니다. 이는 감정 관련 속성을 브리지로 활용하여 두 단계, 즉 지도 클러스터링을 사용한 감정-속성 매핑과 자기 지도 학습을 사용한 속성-음악 생성을 통해 원하는 감정을 가진 음악을 생성합니다. • • 실험 결과에 따르면 EMOGEN은 감정 제어 정확도와 음악 품질에서 이전 방법보다 성능이 뛰어납니다. 실험은 또한 EMOGEN이 감정 레이블에서 주관적 편견을 제거하는 능력을 보여줍니다. 2.
--- RELATED WORK ---
2.1. 감성적 음악 생성 감성에 따른 음악 생성은 딥러닝 시대에 빠르게 발전하고 있습니다. 감정 신호를 적용하는 방식에 따라 이전 연구는 두 가지 유형으로 나눌 수 있습니다. 첫 번째 유형은 감정 레이블을 임베딩으로 변환하여 모델 입력으로 사용하는 것입니다(Madhok et al., 2018; Hung et al., 2021; Pangestu &amp; Suyanto, 2021; Sulun et al., 2022; Grekow &amp; Dimitrova-Grekow, 2021). Madhok et al.은 one-hot 감정 레이블을 기반으로 감정적 음악을 생성합니다. 일부 작업(Hung et al., 2021; Pangestu &amp; Suyanto, 2021)은 MIDI 이벤트에 추가 감정 토큰을 추가하여 특정 감정이 있는 음악을 생성합니다. Sulun et al.은 연속 값의 가치와 각성 레이블을 조건으로 음악 생성을 제어합니다. 두 번째 유형은 감정 분류기를 훈련하고 휴리스틱 검색을 통해 두 모델 출력에 적용하는 것입니다.
--- METHOD ---
각각 감정 제어 정확도와 음악 품질에 대한 s는 감정 음악을 생성하는 데 있어서 우리의 우월성을 보여줍니다. EMOGEN에서 생성한 음악 샘플은 이 링크¹에서 사용할 수 있으며, 코드는 이 링크²에서 사용할 수 있습니다. *동등한 기여 상하이 교통 대학교, 중국 Microsoft Research Asia 3 난징 대학교, 중국 * 국가 소프트웨어 공학 연구 센터, 베이징 대학교, 중국. 서신: Xu Tan<xuta@microsoft.com> . &#39;https://ai-muzic.github.io/emogen/ 2https://github.com/microsoft/muzic/ 1. 서론 Jiang Bian 딥 러닝의 발전으로 자동 음악 생성이 빠르게 발전하고 점점 더 많은 관심을 끌고 있습니다(Hernandez-Olivan &amp; Beltran, 2022; Shih et al., 2022; Yu et al., 2022). 음악에 대한 감정의 중요성으로 인해 감정 음악 생성은 중요하고 실용적인 작업이지만 아직 탐구되지 않았습니다. 이전 작업은 감정 신호를 적용하는 방식에 따라 두 가지 유형으로 나눌 수 있습니다. 첫 번째 유형은 감정 레이블을 임베딩으로 변환하여 모델 입력으로 사용하는 것입니다(Madhok 등, 2018; Hung 등, 2021; Pangestu &amp; Suyanto, 2021; Sulun 등, 2022; Grekow &amp; Dimitrova-Grekow, 2021). 두 번째 유형은 감정 분류기를 훈련하여 디코딩 프로세스를 안내하기 위해 두 모델 출력에 적용하거나(Ferreira &amp; Whitehead, 2019; Ferreira 등, 2020; 2022; Bao &amp; Sun, 2022) 변분 자동 인코더(Tan &amp; Herremans, 2020)의 잠재 공간과 생성적 적대 네트워크(Tseng 등, 2021)에 적용하여 잠재 벡터의 분포를 제한하는 것입니다. 그러나 위의 두 가지 유형의 작업은 모두 음악 시퀀스를 엔드투엔드 방식으로 생성하기 위한 제어 신호로 감정 레이블을 직접 사용하는데, 이는 최적이 아닙니다. 데이터 주석자가 제공한 감정 레이블은 객관적 요인과 주관적 요인 모두의 영향을 받을 수 있습니다. 템포와 음표 밀도와 같은 객관적 요인은 음악 감정과 높은 관련이 있습니다. 주관적 요인의 경우, 지각된 감정은 사회적 정체성, 성격, 청취자의 즉각적인 감정 상태 등과 높은 관련이 있습니다. 예를 들어, 청취자가 화가 난 상태일 때 행복한 노래가 슬프다고 생각할 가능성이 높습니다. 이러한 인간 감정의 주관성으로 인해 다른 데이터 주석자는 동일한 감정을 가진 샘플에 다른 감정 레이블을 지정할 수 있으며, 이는 감정 레이블에 주관적 편향을 초래합니다. 감정 레이블이 일관되지 않으면 이러한 엔드투엔드 방법이 감정과 음악 시퀀스 간의 관계를 학습하기 어렵고, 따라서 모델은 원하는 감정과 정확히 일치하는 음악을 생성하는 데 부족할 수 있습니다. 이 논문에서 우리는 감정 레이블의 주관적 편향의 영향을 제거할 수 있는 감정 음악 생성 시스템인 EMOGEN을 제안합니다. 감정 레이블을 엔드투엔드 방식으로 음악 시퀀스에 직접 매핑하는 대신, EmoGen: 감정 음악 생성에서 주관적 편향 제거 우리는 감정과 높은 상관 관계가 있는 음악 속성 세트를 브리지로 활용하고 이 작업을 두 단계로 나눕니다. 감독 클러스터링을 사용한 감정-속성 매핑과 자기 감독 학습을 사용한 속성-음악 생성입니다. 구체적으로, 감정과 음악 간의 격차를 메우기 위해 속성은 감정과 높은 상관 관계가 있어야 합니다. 우리는 레이블이 지정된 데이터 세트에서 감정 분류기를 학습하고 기능 중요도가 높은 속성을 선택하여 속성 세트를 설계합니다. 감정-속성 매핑 단계에서 우리는 샘플을 감정 레이블로 클러스터링하고 각 클러스터의 평균 속성 값을 계산하여 얻은 클러스터링 센터에 가장 가까운 샘플의 속성 값에 감정을 매핑합니다. 이 클러스터링 프로세스는 감정 레이블을 사용하여 샘플을 감정 범주로 클러스터링하기 때문에 감독됩니다. 이 지도 클러스터링을 통해 매핑된 속성 값은 클러스터링 센터 주변의 샘플에서 일반적인 감정을 나타낼 수 있습니다. 따라서 감정 레이블의 주관적 편향 문제를 제거할 수 있습니다. 속성-음악 생성 단계에서 음악 시퀀스에서 속성 값을 추출하고 이러한 속성을 제어 신호로 사용하여 자기 지도 방식으로 Transformer 기반 모델을 학습합니다. 속성의 값은 음악 시퀀스에서 직접 추출할 수 있으므로 이 생성 모델은 레이블이 지정된 데이터가 필요 없이 제어 신호와 음악 간의 관계를 학습할 수 있습니다. 이 프로세스는 감정 레이블에서 완전히 분리되어 있으므로 감정 레이블의 주관적 편향에 영향을 받지 않습니다. 주관적 편향을 피하기 위한 지도 클러스터링과 자기 지도 학습을 기반으로 하는 두 단계의 이점을 통해 EMOGEN은 감정 음악 생성에서 보다 정확한 감정 제어를 달성할 수 있습니다. 이 작업의 주요 기여는 다음과 같습니다. • 우리는 감정 레이블에서 주관적 편견을 제거할 수 있는 감정 음악 생성 시스템인 EMOGEN을 제안합니다. 이는 감정 관련 속성을 브리지로 활용하여 두 단계, 즉 감독 클러스터링을 통한 감정-속성 매핑과 자기 감독 학습을 통한 속성-음악 생성을 통해 원하는 감정을 담은 음악을 생성합니다. • •
--- EXPERIMENT ---
모든 결과에 따르면 EMOGEN은 감정 제어 정확도와 음악 품질에서 이전 방법보다 성능이 뛰어납니다. 실험은 또한 EMOGEN이 감정 레이블의 주관적 편견을 제거하는 능력을 보여줍니다. 2. 관련 연구 2.1. 감정 음악 생성 감정 조절 음악 생성은 딥 러닝 시대에 빠르게 발전하고 있습니다. 감정 신호를 적용하는 방식에 따라 이전 연구는 두 가지 유형으로 나눌 수 있습니다. 첫 번째 유형은 감정 레이블을 임베딩으로 변환하여 모델 입력으로 사용하는 것입니다(Madhok et al., 2018; Hung et al., 2021; Pangestu &amp; Suyanto, 2021; Sulun et al., 2022; Grekow &amp; Dimitrova-Grekow, 2021). Madhok et al.은 원핫 감정 레이블을 기반으로 감정 음악을 생성합니다. 일부 작업(Hung et al., 2021; Pangestu &amp; Suyanto, 2021)은 MIDI 이벤트에 추가 감정 토큰을 추가하여 특정 감정이 있는 음악을 생성합니다.Sulun et al.은 연속 값의 가치와 각성 레이블에 따라 음악 생성을 제어합니다.두 번째 유형은 감정 분류기를 훈련하여 디코딩 프로세스를 안내하는 휴리스틱 검색 방법(Ferreira &amp; Whitehead, 2019; Ferreira et al., 2020; 2022; Bao &amp; Sun, 2022)을 통해 모델 출력에 적용하거나 변형 자동 인코더(Tan &amp; Herremans, 2020)의 잠재 공간 또는 생성적 적대 네트워크(Tseng et al., 2021)를 사용하여 잠재 벡터의 분포를 제한하는 것입니다.Ferreira &amp; Whitehead는 유전 알고리즘을 사용하여 장단기 메모리(LSTM)의 가중치를 최적화하여 원하는 감정이 있는 음악을 생성합니다. 일부 연구(Bao &amp; Sun, 2022; Ferreira et al., 2020; 2022)는 검색 알고리즘(예: 빔 검색 및 트리 검색)을 적용하여 원하는 감정을 가진 음악 생성을 지시합니다. 그러나 위의 두 유형 모두 음악 생성을 안내하는 제어 신호로 감정 레이블을 직접 사용하는데, 이는 §1에서 설명한 대로 감정 레이블의 주관적 편향의 영향을 무시합니다. 따라서 기존 방법으로는 원하는 감정과 일치하는 음악을 생성하는 것이 어렵습니다. 2.2. 속성 기반 제어 가능 음악 생성 음악 속성은 음악 시퀀스에서 추출되며 음악 생성을 제어하기 위해 조작할 수 있습니다. 이전 연구에서는 이러한 속성을 활용하여 음악 생성 프로세스를 제어하려고 시도했습니다. 이러한 연구(Tan &amp; Herremans, 2020; Kawai et al., 2020; Zhao et al., 2022)는 리듬 밀도, 피치 및 리듬 가변성, 코드와 같은 속성을 추출하고 VAE 기반 프레임워크를 적용하여 음악 속성에 따라 음악 생성을 제어합니다. 판별기는 속성 조건을 만족시키기 위해 숨겨진 공간 분포를 제어하는 데 사용됩니다.Wu &amp; Yang은 VAE의 잠재 공간에 리듬과 폴리포니 강도를 추가하여 음악 생성을 제어하는 MuseMorphose를 제안합니다.von Rütte 등은 VQ-VAE 시스템으로 음악 생성을 제어하기 위해 전문가 설명(음표 밀도, 평균 피치 등)과 학습된 설명(잠재 표현)을 설계하는 FIGARO를 제안합니다.감정적 음악 생성에 이러한 속성을 직접 사용하는 것만으로는 충분하지 않습니다.감정과 속성 간의 관계 구축을 고려하지 않거나 속성과 감정 간의 구체적인 상관 관계를 구성하지 못하기 때문입니다.이로 인해 제어 정확도가 떨어질 수 있습니다.감정→감정-속성 매핑 속성→속성-음악 생성→음악 속성 설계 3. 방법 그림 1a는 EMOGEN의 파이프라인을 보여줍니다.이 파이프라인은 감정-속성 매핑과 속성-음악 생성의 두 단계로 구성되며, 설계된 속성 집합을 다리로 사용합니다. 속성 설계에서 감정과 높은 상관관계를 갖는 속성 집합을 열거하고 선택하여 감정과 음악 간에 일관된 관계를 구축하는 데 도움이 될 수 있습니다. 감정-속성 매핑 단계에서는 클러스터링 중심에 가장 가까운 값을 선택하여 매핑된 속성 값을 얻습니다. 매핑된 속성 값은 일반적인 감정을 잘 나타낼 수 있으므로 감정 레이블의 주관적 편향을 완화할 수 있습니다. 속성-음악 생성 단계에서는 음악 시퀀스에서 직접 추출한 속성 값을 제어 신호로 사용하여 자기 회귀 Transformer 기반 모델을 학습하여 자체 감독 방식으로 해당 음악 시퀀스를 생성합니다. 생성 프로세스를 감정 레이블에서 분리함으로써 감정 레이블의 주관적 편향을 피하고 감정 음악 생성에서 더 나은 제어를 달성할 수 있습니다. §3.4에서 시스템의 장점에 대해 논의합니다. 3.1. 감정 관련 속성 설계 갭 주관적 편향이 있는 감정 레이블의 조건으로 음악 시퀀스를 생성하는 대신 감정과 해당 음악 간의 가교 역할을 하는 감정 관련 속성을 도입합니다. 감정 레이블과 비교했을 때, 이러한 객관적 속성은 해당 음악이 정확히 무엇이어야 하는지 알려줍니다. 예를 들어, 템포 값은 비트의 지속 시간을 알려주고, 키 스케일은 어떤 음표를 사용할 수 있는지 알려줍니다. 음악 시퀀스에서 이러한 속성의 값을 직접 추출하면 감정과 음악 간의 명확한 관계를 구축하는 데 도움이 될 수 있습니다. 구체적으로, 피치 정전, 코드 및 수직 간격, 리듬 및 역학과 같은 저수준 피처에서 멜로디 간격, 악기 연주 및 음악적 질감과 같은 고수준 피처에 이르기까지 음악 속성을 수집했습니다(McKay et al., 2018). 그러나 이 중 많은 부분이 감정과 관련이 없기 때문에 모두 직접 사용하면 많은 노이즈가 발생합니다. 따라서 감정 주석이 달린 데이터 세트에서 랜덤 포레스트(RF)(Ho, 1995) 분류기를 학습한 다음, 최종 속성 세트로 피처 중요도 순위에 따라 상위 k 속성을 선택하여 감정과 높은 상관 관계가 있는 속성을 선택합니다. 이 프로세스를 통해 설계된 속성은 감정 정보를 나타내고 음악 생성을 제어하는 데 도움이 될 수 있습니다. 이러한 설계된 속성에 대한 자세한 내용은 부록 A를 참조하십시오.감정 레이블 (a) EMOGEN의 파이프라인.지도 클러스터링 각성 ● 가치 (b) 지도 클러스터링을 사용한 감정-속성 매핑.감정 공간은 러셀의 4Q 모델(Russell, 1980)을 기반으로 각성과 가치에 따라 4개 사분면으로 나뉩니다.일반적인 감정을 나타내는 감정 레이블에서 매핑된 속성 값은 빨간색 점으로 표시하고, 주관적 편향이 포함된 감정 레이블에서 매핑된 속성 값은 노란색 점으로 표시합니다.x 속성 값 추출 음악 시퀀스 생성 재구성 (c) 자기 지도 학습에서 속성-음악 생성의 훈련 프로세스.x는 대상 음악 시퀀스를 나타내고, x&#39;는 생성된 음악 시퀀스를 나타내고, v는 대상 음악 시퀀스에서 추출된 속성 값의 d차원 벡터를 나타냅니다.그림 1: EMOGEN 개요.3.2. 감정-속성 매핑 감정 관련 속성을 기반으로 원하는 감정을 가진 음악을 생성하기 위해 감정 레이블은 그림 1b에 표시된 것처럼 지도 클러스터링을 통해 일반적인 감정을 나타내는 속성 값에 매핑됩니다. 구체적으로, 먼저 감정 주석이 달린 데이터 세트에서 각 샘플에 대해 선택된 속성의 값을 추출합니다. 데이터 세트에서 제공된 감정 레이블을 기반으로 각 감정 레이블의 샘플 중에서 각 속성에 대한 평균 값을 계산하여 중심을 구합니다. 그런 다음 중심에 가장 가까운 샘플의 속성 값을 사용하여 감정의 특징을 나타냅니다. 이 프로세스는 감정 레이블이 샘플을 범주로 그룹화하기 위한 클러스터링 지침으로 사용되므로 지도됩니다. 또한 샘플이 같은 그룹의 샘플이 유사한 EmoGen: 감정적 음악 생성에서 주관적 편향 제거 감정 정보를 공유하는 반면 다른 그룹의 샘플은 서로 다른 감정을 전달하는 방식으로 그룹화되므로 클러스터링 프로세스이기도 합니다. 이 지도 클러스터링 방법을 통해 얻은 속성 값은 이 감정 레이블이 주어진 일반적인 감정을 나타내고 감정 레이블에서 발생하는 주관적 편향을 피할 수 있어야 합니다. 3.3. 속성-음악 생성 속성 값은 음악 시퀀스에서 쉽게 추출할 수 있어 음악 생성을 제어하는 데 훨씬 더 정확합니다. 학습 프로세스는 그림 1c에 나와 있습니다. 대상 음악 시퀀스에서 감정 관련 속성 값을 추출하여 d차원 벡터로 표현한 다음 이 벡터를 제어 신호로 사용하여 해당 음악 시퀀스를 생성하는 자기 회귀 Transformer 기반 모델에 넣습니다. 이 모델은 매핑된 속성 벡터를 감독 신호로 사용하여 자기 감독 방식으로 학습합니다. 이 자기 감독 학습 단계를 통해 학습된 Transformer 모델은 입력 속성 값에 의해 속성이 제어되는 음악을 생성할 수 있습니다. 추론 시 감정-속성 매핑 단계에서 매핑된 속성 값을 제어 신호로 활용하여 음악 생성 프로세스를 안내합니다. 생성 프로세스는 감정 레이블에서 완전히 분리되므로 감정 레이블의 주관적 편향에 영향을 받지 않습니다. 3.4. EMOGEN의 장점 이 제안된 프레임워크는 다음과 같은 측면에서 원하는 감정을 가진 음악을 생성하는 데 유용합니다. • 주관적 편향을 제거하는 기능. 두 단계에서 지도 클러스터링과 자기 지도 학습 패러다임을 활용함으로써, EMOGEN은 감정 레이블에서 주관적 편향을 제거하여 더 나은 감정 제어 정확도를 달성할 수 있습니다. 구체적으로, 지도 클러스터링을 사용하여 감정을 감정 관련 속성에 매핑함으로써, 우리는 일반적인 감정을 대신하여 속성의 값을 얻습니다. 자기 지도 방식으로 속성 값을 제어 신호로 사용하여 자기 회귀 Transformer 기반 모델을 학습함으로써, 우리는 생성 프로세스에서 감정 레이블을 분리하고 제어 신호와 음악 시퀀스 간의 명확한 관계를 구축합니다. 감정 레이블은 전체 생성 프로세스에서 직접 사용되지 않으므로 감정 레이블에 존재하는 주관적 편향을 피할 수 있습니다. •생성 프로세스를 정확하게 제어하는 능력. 음악 속성은 생성을 구체적으로 지시하는 좋은 도구입니다. 단일 감정 레이블은 해당 음악이 무엇이어야 하는지 정의하기에는 너무 모호합니다. 예를 들어, 행복한 음악이 어떤 것인지 정의하기 어렵습니다. 대조적으로, 음악 속성은 음악의 구체적인 측면을 지정하는 데 구체적입니다(Tan &amp; Herremans, 2020; Wu &amp; Yang, 2021; von Rütte et al., 2022; Di et al., 2021; Chen et al., 2020). 예를 들어, 템포 값은 한 박자의 정확한 지속 시간을 알려주고, 음계 유형은 생성된 음악에서 어떤 음표 세트가 사용되는지 결정합니다. 음악 속성의 값을 간단히 조작하면 생성된 음악을 정확하게 제어할 수 있습니다. • 레이블이 지정된 데이터에서 자유로울 수 있는 기능. EMOGEN은 감정 주석이 필요 없이 감정 조건 음악을 생성할 수 있습니다. 수동 주석은 비용이 많이 들고, 감정 주석이 포함된 데이터 세트는 몇 개뿐입니다(Hung et al., 2021). 생성 모델을 학습하기 위해 감정-음악 페어링 데이터가 필요한 이전 방법과 달리, EMOGEN에서 감정 레이블은 감정 관련 속성과 일반적인 감정을 나타내는 매핑된 속성 값을 결정하는 데만 사용됩니다. 일단 결정되면 변경되지 않습니다. 그 후, 우리는 임의의 데이터 세트에서 속성 값을 추출하여 자기 지도 학습으로 생성 모델을 훈련할 수 있습니다. 따라서 EMOGEN은 데이터 세트에 감정 주석이 없더라도 감정 조건 음악을 생성하는 데 사용할 수 있습니다. 4. 실험 이 섹션에서는 먼저 실험 설정(§4.1)을 소개한 다음 이전 방법과 비교합니다. 그런 다음 EMOGEN이 감정 레이블의 주관적 편향을 제거하는 방법에 대해 포괄적으로 논의합니다. 그런 다음 EMOGEN의 포괄적인 분석을 보여줍니다. 마지막으로 주석이 없는 다른 임의의 데이터 세트에 EMOGEN 프레임워크를 적용한 결과를 보여줍니다. 4.1. 실험 설정 데이터 세트 우리는 모두 세 가지 데이터 세트를 사용하는데, 여기에는 감정 레이블이 붙은 데이터 세트인 EMOPIA(Hung et al., 2021)와 레이블이 붙지 않은 데이터 세트인 Pop1k7(Hsiao et al., 2021)과 LMD-Piano가 포함됩니다. 여기서 LMD-Piano는 Lakh MIDI(LMD) 데이터 세트(Raffel, 2016)에서 피아노 트랙만 포함하는 샘플을 사용하여 구성됩니다. 이러한 데이터 세트의 정보는 표 1에 나와 있습니다. EMOPIA는 감정 분류 기준으로 Russell의 4Q 모델(Russell, 1980)을 사용하는데, 이는 또한 우리의 평가 프로세스에 활용됩니다. 감정 레이블이 있는 EMOPIA는 감정-속성 단계에서 설계된 속성과 매핑된 속성 값을 결정하는 데 사용됩니다. 일단 결정되면 변경되지 않고 감정 레이블은 사용되지 않습니다. 또한 이전 방법과 비교할 때 미세 조정 단계에서도 사용됩니다. Pop1k7과 LMD-Piano는 이전 방법과 비교했을 때 사전 학습 단계에 사용됩니다. 각 데이터 세트를 각각 학습/검증/테스트를 위해 8/1/로 무작위로 분할했습니다. EmoGen: 감성 음악 생성에서 주관적 편견 제거 표 1: 학습 데이터 세트의 정보. 이름 EMOPIA Pop1kLMD-Piano | 음악 유형 피아노 크기 1, 피아노 1, 피아노 22, 레이블 유형 Russell의 4Q 없음 없음 시스템 구성 REMI와 유사한(Huang &amp; Yang, 2020) 표현 방법을 사용하여 MIDI를 토큰 시퀀스로 변환합니다. jSymbolic(McKay et al., 2018)을 적용하여 음악에서 속성 값을 추출하고 EMOPIA에서 Random Forest 분류기를 학습한 다음 §3.1에 설명된 대로 감정과 가장 관련이 있는 최상위 속성을 선택합니다. 감정-속성 매핑 단계(§3.2)에서 EMOPIA에 지도 클러스터링을 구현합니다. 속성-음악 생성 단계(§3.3)에서 매핑된 속성 벡터를 중앙값으로 이진화한 다음, 이를 2층 피드포워드 네트워크에 공급하여 속성 임베딩을 얻습니다. 그런 다음 자기 회귀 Transformer 모델의 입력에서 토큰 임베딩에 추가합니다. 인과적 주의가 있는 6개의 Transformer 층과 8개의 주의 헤드로 구성된 백본 모델로 선형 Transformer(Katharopoulos et al., 2020)를 활용합니다. 주의 숨김 크기는 512이고 FFN 숨김 크기는 2,048입니다. 각 샘플의 최대 시퀀스 길이는 1,280입니다. 학습하는 동안 배치 크기는 8로 설정됩니다. ẞ₁ = 0.9, ẞ2 = 0.98 및 € = = 10-9인 Adam 최적화 도구(Kingma &amp; Ba, 2015)를 사용합니다. 학습률은 1 × 10−4이고 워밍업 단계는 16000이고 역제곱근 감소입니다. 드롭아웃 비율은 0.1입니다. 추론하는 동안 비율 p = 0.9 및 온도 7 = 1.0인 상위 p 샘플링을 적용합니다. 비교 방법 서로 다른 감정 제어 방식의 두 가지 대표적인 방법을 비교하여 EMOGEN을 수행했습니다. 첫 번째는 모델 입력에서 추가 감정 토큰을 감정 조건으로 사용하는 조건부 샘플링(CS)(Hung et al., 2021)입니다. 다른 하나는 레이블이 지정된 데이터로 학습된 감정 분류기와 음악 판별기를 사용하여 추론 프로세스를 지시하는 Predictor Upper Confidence for Trees(PUCT)(Ferreira et al., 2022)입니다.평가 및 지표 EMOGEN을 평가하기 위해 주관적 및 객관적 평가를 모두 수행합니다.각 모델을 적용하여 4가지 감정 각각에 대해 250개씩 총 1,000개의 음악 작품을 생성합니다.주관적 평가에서 인간 채점자는 각 음악 작품을 평가하도록 요청받습니다.다음과 같은 주관적 지표를 보고합니다.1) 주관적 정확도: 피험자가 인지한 감정이 감정 레이블과 일치하는지 여부입니다.감정 제어 가능성을 나타냅니다.2) 인간성: 인간이 작곡한 음악과 얼마나 유사한지입니다.음악 품질을 나타냅니다.3) 전반적: 전반적인 점수입니다.객관적 평가에서(Ferreira et al., 2022) EMOPIA로 학습된 선형 변압기 기반 감정 분류기를 사용하여 생성된 각 음악 작품의 감정 레이블을 예측합니다. 그런 다음 음악을 생성하기 위한 감정 입력과 이 분류기가 예측한 감정 클래스를 비교하여 객관적인 정확도를 계산합니다. 주관적 정확도에 대한 보완적 지표로 기능하는 분류의 객관적인 정확도를 보고합니다. 인간 평가 프로세스와 평가 지표에 대한 자세한 내용은 부록 §B.1을 참조하십시오. 4.2. 이전 방법과의 비교 철저한 평가를 수행하기 위해 두 가지 학습 설정을 설계합니다. 1) 설정 1(S1): 생성된 음악의 음악적 품질을 보장하기 위해 이전 작업(Hung et al., 2021; Ferreira et al., 2022; Neves et al., 2022)에 따라 EMOPIA에서 미세 조정하기 전에 Pop1k7+LMD-Piano에서 모델을 사전 학습합니다. EMOGEN의 경우 먼저 설계된 속성을 제어 신호로 사용하여 언어 모델을 사전 학습한 다음 속성을 제어 신호로 사용하여 EMOPIA에서 미세 조정합니다. CS의 경우 (Hung et al., 2021)의 작업에 따라 감정 토큰 설정을 제어하여 언어 모델을 사전 학습합니다.<none> &quot;를 플레이스홀더로 사용하고 속성 뒤에 붙입니다. 사전 학습 후, 플레이스홀더에 할당된 감정 토큰과 제어 신호로 속성을 사용하여 EMOPIA에서 모델을 미세 조정합니다. PUCT의 경우, 먼저 언어 모델을 사전 학습한 다음, 추가 분류 헤드를 사용하여 EMOPIA에서 미세 조정하여 감정 분류기를 얻습니다. 추가 분류 헤드를 사용하여 언어 모델을 미세 조정하여 실제/가짜 샘플을 분류하여 음악 판별기를 학습합니다. 모든 방법은 Pop1k7과 LMD-Piano에서 사전 학습합니다. 2) 설정 2(S2): 위 설정의 학습 방법은 미세 조정 단계에서 사용된 데이터 세트와 유사한 음악만 생성할 수 있다는 점에서 한계가 있습니다. 이는 감정 음악 생성을 위해 임의의 데이터 세트를 자연스럽게 활용할 수 있는 EMOGEN의 능력을 제한합니다. 이 능력을 테스트하기 위해 속성-음악 생성 단계에서 Pop1k7+LMDPiano+EMOPIA에서 생성 모델을 학습하고, 설계되고 매핑된 속성을 사용하여 해당 음악 생성 주어진 감정으로. CS와 PUCT는 학습에서 레이블이 지정된 데이터만 필요하기 때문에 이 설정에서는 작동할 수 없다는 점에 유의하세요. 따라서 EMOGEN을 실제 결과인 EMOPIA 데이터 세트와 비교합니다. 결과는 표 2에 나와 있습니다. 다음을 관찰할 수 있습니다. 1) CS 및 PUCT와 비교하여 EMOGEN은 S1의 모든 메트릭에서 더 나은 성능을 달성합니다. 특히 EMOGEN은 주관적 및 객관적 정확도에서 훨씬 더 나은 감정 제어성을 가지고 있습니다. 이는 지정된 감정으로 음악을 생성하는 데 있어 EMOGEN의 우수성을 보여줍니다. 또한 EMOGEN의 더 높은 인간성과 전체 점수는 EMOGEN이 EmoGen을 개선할 수 있음을 나타냅니다. 감성적 음악 생성에서 주관적 편향 제거 표 2: 주관적 및 객관적 평가 결과. 인간성과 전체 점수의 경우 95% 신뢰 구간을 사용하여 평균 의견 점수를 보고합니다. 주관적 방법 정확도↑ 인간성 전반적 객관적 정확도↑ 실제 결과 0.4.26 ± 0.4.19 0.0.CS 0.3.48 ± 0.3.59 ± 0.0.PUCT 0.0.3.3.24 ± 0.0.3.26 ± 0.0.3.59 0.0.0.3.67± 0.3.65 ± 0.0.EMOPIA S1 설정: Pop1k7+LMD-Piano에 대한 사전 학습, EMOPIA EMOGEN S2에 대한 미세 조정: Pop1k7+LMD-Piano+EMOPIA EMOGEN에 대한 음악 품질 학습.2) S2에서 EMOGEN은 S1의 CS 및 PUCT보다 모든 지표에서 더 높은 성능을 달성합니다.이는 EMOGEN이 감정적 음악 생성을 위해 임의의 데이터 세트를 활용할 수 있을 뿐만 아니라 상당히 좋은 제어성과 인간성을 가지고 있음을 보여줍니다. 또한 §4.5에서 더욱 다양하고 여러 악기의 음악 데이터 세트에 대한 EMOGEN의 능력을 보여줄 것입니다.3) EMOGEN의 정확도는 기준 진실(EMOPIA)보다 훨씬 높습니다.이는 한편으로는 레이블이 지정된 데이터 세트에 감정 판단에 영향을 미치는 모호한 감정 레이블이 있는 샘플이 있음을 보여줍니다.반면에 EMOGEN의 2단계 프레임워크, 특히 감정-속성 매핑 단계에서 감독 클러스터링에 의해 결정된 감정의 매핑된 속성 값은 감정 레이블의 주관적 편향을 피하는 데 도움이 될 수 있습니다.따라서 S2를 기본 프레임워크로 선택하여 EMOGEN에 대한 추가 분석을 수행합니다.4.3. 주관적 편향 제거에 대한 검증 EMOGEN이 감정 레이블의 주관적 편향을 제거할 수 있음을 보여주기 위해 1) 레이블이 지정된 데이터 세트에서 주관적 편향의 존재와 2) EMOGEN의 주관적 편향 제거 능력을 보여주는 실험을 수행합니다.위에서 설명한 주관적 및 객관적 정확도를 메트릭으로 사용합니다. 감정 레이블에 대한 주관적 편향의 존재 감정 레이블에는 주관적 편향이 존재하여 엔드투엔드 방법에 대한 제어 성능이 저하될 수 있습니다. 감정 레이블에 주관적 편향이 존재함을 증명하기 위해 중심 샘플과 경계 샘플의 감정 정확도를 비교합니다. 구체적으로, EMOPIA의 모든 샘플은 먼저 감정 레이블로 클러스터링하여 4개의 감정 클러스터를 얻습니다. 그런 다음 각 감정 클러스터의 속성 평균을 계산하여 클러스터링 중심을 얻습니다. 클러스터링 중심(즉, 중심 샘플)에 가장 가까운 50개 샘플과 클러스터링 중심에서 멀리 떨어진 50개 샘플(즉, 경계 샘플)을 선택합니다. 청취자에게 샘플을 4개의 감정 범주로 분류하여 주관적 정확도를 얻고 분류 모델을 사용하여 객관적 정확도를 얻도록 요청합니다. 표 3에서 볼 수 있듯이 중심 샘플을 분류하는 데 있어서 주관적 및 객관적 정확도가 모두 경계 샘플을 분류하는 데 있어서보다 높은데, 이는 데이터 세트, 특히 경계 샘플의 레이블에 주관적 편향이 존재함을 나타내며, 이 주관적 편향이 분류 성능을 방해할 수 있습니다. 우리는 EMOPIA의 샘플에 대한 중심 사상과 EMOPIA의 샘플 속성 벡터의 거리 분석을 통해 t-SNE 시각화를 수행하여 이를 더욱 검증합니다.그림 2a에 표시된 t-SNE 시각화는 EMOPIA의 샘플이 별도로 그룹화되지 않는다는 것을 보여줍니다.그림 3a에서 볼 수 있듯이 두 곡선 사이의 영역이 넓을수록 다른 그룹을 구별하는 데 더 나은 성능을 나타내지만 EMOPIA의 샘플에서 계산된 곡선 사이의 거리는 크지 않습니다.위의 결과는 감정 레이블에 주관적 편향이 존재한다는 것을 더욱 증명합니다.주관적 편향 제거 EMOGEN이 감정 레이블의 주관적 편향을 제거할 수 있다는 것을 증명하기 위해 생성된 중앙 샘플과 생성된 경계 샘플의 분류 정확도를 비교합니다.특히, 중앙 샘플은 감정-속성 사상 단계에서 추출된 중앙 샘플의 속성 값을 사용하여 생성됩니다.마찬가지로, 생성된 경계 샘플은 경계 샘플의 속성 값을 사용합니다. 표 3에서 보듯이, 중심 샘플의 주관적 및 객관적 정확도는 경계 샘플의 주관적 및 객관적 정확도보다 높았는데, 이는 감정-속성 매핑 단계에서 감독 클러스터링이 주관적 편향을 제거하는 데 효과적임을 나타냅니다. EMOGEN에서 생성한 샘플의 중심 매핑과 EMOGEN에서 생성한 샘플의 속성 벡터에 대한 거리 분석을 통해 이를 추가로 검증합니다. 그림 2b에서 보듯이, EMOGEN에서 생성한 샘플은 네 개의 뚜렷한 그룹으로 클러스터링할 수 있습니다. 그림 3b에서 보듯이, EMOGEN의 클래스 내 거리는 EMOPIA보다 작았는데, 이는 EMOGEN에서 생성한 샘플이 각 방법에서 감정 표현이 더 유사함을 나타냅니다 EmoGen: 감성적 음악 생성에서 주관적 편향 제거 표 3: 중심 및 경계 샘플을 사용한 주관적 및 객관적 정확도. 주관적 객관적 정확도 정확도 L1 거리L1 거리계층 간 계층 내 계층 간-✶ 계층 내 EMOPIA(중앙) EMOPIA(경계선) 0.0.QQQQQQQEmotion 감정 감정 0.0.EMOGEN(중앙) EMOGEN(경계선) 0.(a) EMOPIA 0.0.0.감정 범주. 또한 두 곡선 사이의 영역이 EMOPIA보다 훨씬 넓어 다양한 감정 범주의 샘플을 구별하는 데 더 나은 성능을 나타냅니다. 위의 결과는 모두 EMOGEN이 감정 레이블의 주관적 편향을 제거할 수 있음을 보여줍니다.(b) EMOGEN 그림 3: EMOPIA의 샘플과 EMOGEN에서 생성한 샘플의 속성 벡터의 계층 내 및 계층 간 L1 거리. &quot;계층 내&quot;는 동일한 감정 레이블을 가진 속성 벡터의 평균 거리를 의미하고 &quot;계층 간&quot;은 서로 다른 감정 레이블을 가진 속성 벡터의 평균 거리를 의미합니다. 표 4: 다양한 감정-속성 매핑 방법에 대한 평가 결과.---Q6 8 8 ៖ QQ--주관적 6 8 8 ៖ QMethod Closest 0.Center 0.K-Means 0.Accuracy Humanness Objective Accuracy 3.0.3.0.3.0.----Dim(a) EMOPIA --20 -Dim(b) EMOGEN 그림 2: EMOPIA의 샘플과 EMOGEN에서 생성한 샘플의 속성 벡터에 대한 T-SNE 시각화. &quot;Qi&quot;는 러셀의 4Q 모델의 i 사분면을 나타냅니다. 4.4. 종합 분석 이 하위 섹션에서는 다양한 모듈에 대한 분석 실험을 수행합니다. 1) 감정-속성 매핑 방법; 2) 속성 설계 방법; 3) 상위 k 속성. 실험 구현에 대한 자세한 내용은 부록 §B.3을 참조하세요. 감정-속성 매핑 방법 감정-속성 매핑 단계에서는 각 감정 범주에 대한 매핑된 속성 값으로 속성 값 집합을 결정해야 하며, 이를 위해 다음과 같은 방법을 고려합니다.1) Closest: 모든 감정 샘플의 평균 속성 값에 가장 가까운 속성 값을 갖는 샘플의 속성 값을 직접 사용합니다.EMOGEN의 기본 방법입니다.2) Center: 모든 감정 샘플의 평균 속성 값을 매핑된 속성 값으로 직접 사용합니다.3) K-Means: 각 감정 샘플을 K-Means 클러스터링 알고리즘(Lloyd, 1982)으로 클러스터링하고 가장 큰 클러스터의 중심의 속성 값을 매핑된 속성 값으로 선택합니다.표 4에 나와 있는 평가 결과에서 Closest가 Center와 K-Means보다 주관적, 객관적 정확도가 더 우수함을 알 수 있습니다.Closest는 데이터 세트의 실제 샘플에서 속성 값을 얻기 때문에 원래의 속성 분포를 유지할 수 있으며, 따라서 더 높은 정확도를 얻을 수 있습니다. 반면, Center와 K-Means로 구한 속성값은 실제 샘플에서 나온 것이 아니므로 값 분포가 실제 분포와 달라 제어 정확도가 떨어질 수 있다. 음악 품질에 관해서는 Center와 K-Means의 인간성 점수가 Closest보다 높지만 차이가 크지 않다. 따라서 감정-속성 매핑 단계에서 기본 매핑 방법으로 Closest를 선택한다. 속성 설계 속성 설계 모듈의 네 가지 대안을 모두 비교한다. 1) Top-100: EMOGEN의 기본 방법인 특징 중요도에 따라 상위 100개 속성을 사용한다. 2) Random: §3.1에 설명된 속성 그룹에 따라 무작위로 속성을 선택한다. 이러한 속성을 선택하는 방법에 대한 자세한 내용은 부록 §B.3에 설명되어 있다. 3) 수동: 이전 연구(Zheng et al., 2021; Tan &amp; Herremans, 2020; McKay et al., 2018; Kim et al., 2010)에 따라 감정과 관련된 17개의 수동으로 설계된 음악 속성을 사용합니다. 결과는 표 5에 나와 있습니다. 다음을 관찰할 수 있습니다. 1) EMOGEN(Top-100)은 주관적 정확도를 개선하고 EmoGen: 감성 음악 생성에서 주관적 편향 제거 표 5: 다양한 속성 설계 방법의 평가 결과. 표 6: 다양한 상위 k 속성의 평가 결과. 주관적 객관적 주관적 객관적 Top-k 값 정확도 인간성 정확도 방법 정확도 인간성 정확도0.3.0.Top-100 0.Random 0.Manual 0.3.0.0.3.0.3.0.3.0.0.3.0.3.0.0.3.0.objective 정확도는 Random 및 Manual과 비교하여 각각 35.7% 및 16.5% 이상 높았으며, 이는 속성 설계 방법에 대한 더 나은 제어 가능성을 보여줍니다.EMOGEN의 인간성 점수도 Random 및 Manual보다 우수하여 EMOGEN이 고품질 음악을 생성하는 데 더 나은 성능을 발휘함을 보여줍니다.2) Random의 주관적 및 객관적 감정 정확도는 Top-100보다 낮으며, 이는 무작위로 선택된 속성으로는 감정 관련 정보를 전달하기에 충분하지 않을 수 있음을 나타냅니다. 이는 속성과 감정 간의 관계를 구축하는 설계 없이는 감정 정보를 제어하여 모델을 훈련하기 어렵기 때문에 합리적입니다. 3) Manual의 주관적 및 객관적 정확도는 Top-100보다 낮은데, 이는 사전 지식을 통해 속성을 설계하면 감정과 음악 간의 관계를 잘 모델링할 수 없음을 나타냅니다. 따라서 EMOGEN의 설계된 속성으로 Top-100 속성을 선택합니다. 다른 Top-k 속성 우리는 설계된 속성의 수(즉, k)가 모델 성능에 미치는 영향을 추가로 분석합니다. 구체적으로, k를 (10, 50, 100, 300, 500)으로 변화시키고 제어 가능성과 음악 품질에 관해 모델을 평가합니다. 평가 결과는 표 6에 나와 있습니다. 다음을 관찰할 수 있습니다. 1) Top-100은 가장 높은 주관적 정확도를 달성합니다. k가 10에서 500으로 증가함에 따라 주관적 정확도가 먼저 증가한 다음 감소하는데, 이는 속성이 많을수록 제어 정확도를 개선하는 데 도움이 되지만 너무 많으면 제어 가능성에 해를 끼칠 수 있으며 이는 더 많은 노이즈를 도입했기 때문일 수 있습니다. 2) Top-300 및 Top-500의 객관적 정확도는 Top-100보다 높습니다. 그 이유는 많은 수의 속성으로 인해 매핑 관계가 레이블이 지정된 데이터 세트에 과대적합되어 모델이 실제 사실과 매우 유사한 음악을 생성할 수 있고, 감정 분류기가 더 정확한 예측을 제공하는 경향이 있기 때문일 수 있습니다. 이 문제로 인해 주관적 정확도가 객관적 정확도보다 더 신뢰할 수 있다고 생각합니다. 3) 요청이 증가하면 인간성 점수가 일반적으로 감소하여 속성이 많을수록 음악 품질이 낮아짐을 나타냅니다. 이는 속성이 훨씬 많으면 생성 모델이 속성에서 해당 음악으로의 매핑을 학습하기가 더 어렵고 데이터가 부족하여 음악 품질이 저하되기 때문에 합리적입니다. 그러나 상위 100개는 여전히 비교적 좋은 품질을 가질 수 있습니다. 주관적 정확도와 음악 품질에 대한 성과를 결합하여 k =를 EMOGEN의 기본값으로 설정했습니다. 4.5. 다중 악기 데이터 세트에 대한 응용 임의의 데이터 세트에서 EMOGEN이 감정 음악을 생성하는 능력을 평가하기 위해 감정 주석이 없는 22535개 샘플을 포함하는 다중 악기 데이터 세트인 TopMAGD(Ferraro &amp; Lemström, 2018)에서 EMOGEN에 대한 실험을 수행했습니다. 구체적으로, 감정-속성 매핑 단계에서 매핑된 속성이 결정되었으므로 TopMAGD에서 속성-음악 생성 모델만 훈련하면 됩니다. 주관적 정확도, 인간성 및 전반을 주관적 지표로 사용합니다. 주관적 실험에 대한 자세한 내용은 부록 §B.4를 참조하세요. EMOGEN을 표 2의 결과와 비교합니다. TopMAGD에서 EMOGEN의 주관적 정확도는 0.433이고, 인간성과 전반적 점수는 각각 3.72 ± 0.과 3.67 ± 0.15입니다. 우리는 다음을 관찰할 수 있습니다. 표 2의 결과와 비교했을 때, TopMAGD에 대한 EMOGEN 훈련은 S1에서 제어 정확도와 음악 품질에서 CS 및 PUCT보다 더 나은 성과를 보입니다.
--- CONCLUSION ---
, EMOGEN은 다중 악기 데이터 세트에서 원하는 감정을 담은 음악을 생성할 수 있습니다. 생성된 샘플은 이 링크³에서 사용할 수 있습니다. 5. 결론 이 논문에서는 감정과 음악 사이의 다리 역할을 하는 감정 관련 음악 속성 세트를 활용하는 감정 음악 생성 시스템인 EMOGEN을 제안합니다. EMOGEN은 감정 음악 생성을 두 단계로 나눕니다. 음악-속성 매핑 단계에서 EMOGEN은 감정 레이블을 지도 클러스터링을 통해 일반적인 감정을 표현할 수 있는 속성 값에 매핑합니다. 속성-음악 생성 단계에서 EMOGEN은 감정 레이블 없이 자체 지도 학습을 통해 생성 모델을 학습합니다. 3 https://emo-gen.github.io/ EmoGen: 감정 음악 생성에서 주관적 편향 제거 EMOGEN은 두 단계의 이점을 활용하여 감정 레이블의 주관적 편향을 제거하여 더 나은 제어 정확도를 달성합니다. 실험 결과에 따르면 EMOGEN은 이전 방법에 비해 더 나은 감정 제어 정확도와 음악 품질로 음악을 생성할 수 있습니다. 앞으로 EMOGEN을 다음과 같은 측면에서 개선하거나 확장하는 것을 고려할 것입니다. 첫째, EMOGEN은 감정-속성 매핑 단계에서 속성 중심에 가장 가까운 샘플을 선택하는데, 이는 감정의 다양성을 무시할 수 있습니다. 더욱 다양한 감정 매핑을 얻기 위해 세분화된 감정 클래스에서 속성 벡터를 클러스터링하는 방법을 살펴볼 가치가 있습니다. 둘째, EMOGEN은 노래 수준 속성으로 음악 생성을 전역적으로 제어하며, 이 프로세스를 동적으로 제어하여 막대, 구절 및 섹션 수준 간의 감정 전환을 달성하는 방법을 추가로 탐구할 것입니다. 마지막으로, 감정/스타일 제어 텍스트 생성과 같은 더 많은 작업과 도메인으로 EMOGEN을 확장할 것으로 예상합니다. 참고문헌 Bao, C. 및 Sun, Q. Generating music with emotionals. IEEE Transactions on Multimedia, 2022. Chen, K., Wang, C., Berg-Kirkpatrick, T., 및 Dubnov, S. Music sketchnet: Controllable music generation via factorized representations of pitch and rhythm. In Proceedings of International Society for Music Information Retrieval Conference (ISMIR), pp. 77–84, 2020. Di, S., Jiang, Z., Liu, S., Wang, Z., Zhu, L., He, Z., Liu, H., and Yan, S. 제어 가능한 음악 변환기를 사용한 비디오 배경 음악 생성. In Proceedings of ACM International Conference on Multimedia (MM), pp. 2037–2045, 2021. Ferraro, A. and Lemström, K. 반복 패턴의 자동 식별을 통한 기호로 인코딩된 음악의 대규모 장르 분류. In Proceedings of International Conference on Digital Libraries for Musicology (DLfM), pp. 34-37, 2018. Ferreira, L. and Whitehead, J. 감정을 담은 음악을 생성하는 방법 학습. 국제 음악 정보 검색 학회(ISMIR) 회의록, 384-390쪽, 2019년. Ferreira, LN, Lelis, LHS, and Whitehead, J. 테이블탑 롤플레잉 게임을 위한 컴퓨터 생성 음악. 제16회 AAAI 인공 지능 및 대화형 디지털 엔터테인먼트(AIIDE) 회의록, 59-65쪽, 2020년. Ferreira, LN, Mou, L., Whitehead, J., and Lelis, LHS 몬테카를로 트리 검색을 통한 상징적 음악 생성에서 지각된 감정 제어. AAAI 인공 지능 및 대화형 디지털 엔터테인먼트(AIIDE) 회의록, 163-170쪽, 2022년. Grekow, J. and Dimitrova-Grekow, T. 조건부 변분 자동 인코더를 사용하여 주어진 감정을 가진 모노포닉 음악 생성. IEEE Access, 9:129088-129101, 2021. Hernandez-Olivan, C. 및 Beltran, JR 딥 러닝을 통한 음악 작곡: 리뷰. 음성 및 음악 기술의 발전: 계산적 측면 및 응용 프로그램, pp. 25-50, 2022. Ho, TK 무작위 결정 숲. 국제 문서 분석 및 인식 컨퍼런스(ICDAR) 회의록, pp. 278–282, 1995. Hsiao, W.-Y., Liu, J.-Y., Yeh, Y.-C. 및 Yang, Y.-H. 합성어 변환기: 동적 방향성 하이퍼그래프에서 전체 노래 음악을 작곡하는 방법. AAAI 인공지능 컨퍼런스(AAAI) 회의록, 35권, pp. 178–186, 2021. Huang, Y.-S. 및 Yang, Y.-H. 팝 음악 변환기: 비트 기반 모델링 및 표현력 있는 팝 피아노 작곡 생성. ACM 국제 멀티미디어 컨퍼런스(MM) 회의록, pp. 1180-1188, 2020. Hung, H., Ching, J., Doh, S., Kim, N., Nam, J., 및 Yang, Y. EMOPIA: 감정 인식 및 감정 기반 음악 생성을 위한 다중 모달 팝 피아노 데이터 세트. 국제 음악 정보 검색 컨퍼런스(ISMIR) 회의록, pp. 318-325, 2021. Katharopoulos, A., Vyas, A., Pappas, N., 및 Fleuret, F. 변환기는 rnns: 선형 어텐션이 있는 고속 자기 회귀 변환기. 국제 기계 학습 컨퍼런스, pp. 5156-5165. PMLR, 2020. Kawai, L., Esling, P., and Harada, T. Attributes-aware deep music transformation. In Proceedings of International Society for Music Information Retrieval Conference (ISMIR), pp. 670–677, 2020. Kim, YE, Schmidt, EM, Migneco, R., Morton, BG, Richardson, P., Scott, J., Speck, JA, and Turnbull, D. Music emotion awareness: A state of the art review. In Proc. ismir, volume 86, pp. 937-952, 2010. Kingma, DP and Ba, J. Adam: A method for stochastic optimization. In Proceedings of International Conference on Learning Representations (ICLR), 2015. Lloyd, S. Least squares quantization in pcm. IEEE 정보 이론 거래, 28(2):129–137, 1982. Madhok, R., Goel, S., 및 Garg, S. Sentimozart: 감정에 기반한 음악 생성. International Conference on Agents and Artificial Intelligence(ICAART)의 회의록, 501-506쪽, 2018. EmoGen: 감정적 음악 생성에서 주관적 편견 제거 McKay, C., Cumming, J., 및 Fujinaga, I. JSYMBOLIC 2.2: 음악학 및 MIR 연구에 사용하기 위해 상징적 음악에서 특징 추출. International Society for Music Information Retrieval Conference(ISMIR)의 회의록, 348–354쪽, 2018. Neves, P., Fornari, J., 및 Florindo, J. transformer-gans를 사용하여 감정을 담은 음악 생성. arXiv 사전 인쇄본 arXiv:2212.11134, 2022. Pangestu, MA 및 Suyanto, S. 변환기를 사용하여 감정이 담긴 음악 생성. 국제 컴퓨터 과학 및 공학 컨퍼런스(IC2SE) 회의록, 1권, 1-6쪽, 2021. Raffel, C. 오디오-MIDI 정렬 및 매칭에 응용되는 시퀀스 비교를 위한 학습 기반 방법. 컬럼비아 대학교 박사 학위 논문, 2016. Russell, JA 감정의 순환 모델. 성격 및 사회 심리학 저널, 39(6):1161, 1980. Shih, Y.-J., Wu, S.-L., Zalkow, F., Muller, M. 및 Yang, Y.-H. 테마 변환기: 테마 조건 변환기를 사용한 상징적 음악 생성. IEEE Transactions on Multimedia, 2022. Sulun, S., Davies, MEP, Viana, P. 연속 값의 감정에 따라 조건화된 상징적 음악 생성. IEEE Access, 10:44617-44626, 2022. Tan, HH, Herremans, D. 음악 페이더넷: 저수준 피처 모델링을 통한 고수준 피처 기반 제어 가능한 음악 생성. 국제 음악 정보 검색 컨퍼런스(ISMIR) 회의록, pp. 109-116, 2020. Tseng, B., Shen, Y., Chi, T. 생성적 적대 네트워크를 통한 감정 및 음조 기반 음악 확장. IEEE 국제 음향, 음성 및 신호 처리(ICASSP) 컨퍼런스 논문집, 86-90쪽, 2021. von Rütte, D., Biggio, L., Kilcher, Y., Hoffman, T. Figaro: 세밀한 예술적 제어를 통한 상징적 음악 생성. arXiv 사전 인쇄본 arXiv:2201.10936, 2022. Wu, S.-L., Yang, Y.-H. Musemorphose: 하나의 변압기 vae를 사용한 전체 노래 및 세밀한 음악 스타일 전송. arXiv 사전 인쇄본 arXiv:2105.04090, 2021. Yu, B., Lu, P., Wang, R., Hu, W., Tan, X., Ye, W., Zhang, S., Qin, T., Liu, T.-Y. Museformer: 음악 생성을 위한 미세하고 거친 주의력을 갖춘 변압기.arXiv 사전 인쇄본 arXiv:2210.10349, 2022. Zhao, J., Xia, G., and Wang, Y. 제어 가능한 음악 생성을 위한 조건부 변형 자동 인코더에 대한 도메인 적대적 훈련.arXiv 사전 인쇄본 arXiv:2209.07144, 2022. Zheng, K., Meng, R., Zheng, C., Li, X., Sang, J., Cai, J., and Wang, J. Emotionbox: 순환 신경망을 사용한 음악 요소 중심의 감정적 음악 생성 시스템.arXiv 사전 인쇄본 arXiv:2112.08561, 2021. EmoGen: 감정적 음악 생성에서 주관적 편견 제거 A. 선택된 속성 목록 jSymbolic에서 1495개의 음악 속성을 추출합니다. 이러한 음악 속성의 정의는 https://jmir.sourceforge.net/manuals/jSymbolic_manual/home.html에서 찾을 수 있습니다.먼저 EMOPIA에서 랜덤 포레스트 분류기를 훈련한 다음, 기능 중요도에 따라 100개의 속성을 선택합니다.처음 10개의 속성은 표 7에 나와 있습니다.표 7: 선택된 처음 10개의 속성.노트 분기당 노트 밀도 노트 분기당 노트 밀도 노트 변동성 노트의 총 수 상대 노트 밀도 가장 높은 줄의 밀도 긴 리드미컬 값의 유행 매우 긴 리드미컬 값의 유행 노트 간 평균 역학 변화 피치 클래스 히스토그램_리듬적 값 히스토그램_수직 간격 히스토그램_선택한 속성에 대한 자세한 내용은 https://emo-gen.github.io/를 참조하세요.B. 실험 세부 정보 B.1. 이전 방법과의 비교 15명의 참가자를 초대하여 각 설정 및 방법에 대한 4가지 감정 범주로 구성된 32곡을 평가했습니다. 참가자는 1) 가치: 음악 작품이 부정적이거나 긍정적입니까? 2) 각성: 음악 작품의 각성이 낮습니까 높습니까? 3) 인간성: 인간이 작곡한 작품과 얼마나 유사한지? 4) 전반: 전반적인 점수에 대해 5점 척도로 음악 샘플을 평가해야 합니다. 객관적인 지표의 경우 감정 분류기를 적용하여 각 방법에 대해 생성된 1000개 샘플을 분류한 다음, 음악을 생성하기 위한 감정 입력을 이 분류기가 예측한 감정 클래스와 비교하여 객관적인 정확도를 계산합니다. 표 8: 주관적 평가의 세부 결과. 지표 hv, lv, ha, la는 각각 높은 가치, 낮은 가치, 높은 각성 및 낮은 각성을 의미합니다. 모든 지표에 대해 평균 의견 점수와 95% 신뢰 구간을 보고합니다. EMOPIA 방법 설정 hv↑ 실제 결과 3.83 0.S1: Pop1k7+LMD-Piano에 대한 사전 학습, EMOPIA CS PUCT EMOGEN EMOGEN에 대한 미세 조정 3.00±0.3.20±0.3.43±0.3.30±0.lv↓ 2.83±0.3.23±0.S2: Pop1k7+LMD-Piano+EMOPIA에 대한 학습 3.20 0.2.40±0.2.57±0.hat 4.13±0.3.9±0.3.40±0.4.27±0.4.17±0.la↓ 2.53±0.2.93±0.3.07±0.1.77±0.1.90±0. 각 방법의 결과를 더 비교하기 위해 평균을 계산했습니다. 가치와 각성 점수입니다. 가치와 각성에 대한 자세한 평가 결과는 표 8에 나와 있습니다. 보시다시피: 1. S1에서 EMOGEN은 hv, lv, ha 및 la에서 CS와 PUCT보다 성능이 뛰어납니다. 따라서 EMOGEN은 이 설정에서 CS와 PUCT보다 감정을 더 잘 제어합니다. 2. 설정 2의 EMOGEN은 설정 1의 CS와 PUCT보다 감정을 더 잘 제어합니다. 따라서 2단계 프레임워크의 이점을 활용하여 EMOGEN은 주석이 없는 임의의 데이터 세트에서 좋은 성능을 달성할 수 있습니다. EmoGen: 감성 음악 생성에서 주관적 편향 제거 B.2. 주관적 편향 제거에 대한 검증 EMOPIA의 각 감정이 약 250개의 샘플을 포함한다는 점을 고려하여 각 감정 범주에 대해 각각 중심과 경계에서 50개의 샘플을 선택합니다. 그리고 EMOGEN의 경우 중심과 경계에 대해 별도로 1000개의 샘플을 생성합니다. 각 감정 범주에는 250개의 샘플이 포함됩니다. 우리는 10명의 참가자를 초대하여 Russell의 4Q 모델(Russell, 1980)에 따라 음악 샘플을 감정 사분면으로 분류하도록 합니다. 각 참가자는 1) EMOPIA의 중심과 경계, 2) EMOGEN에서 생성된 중심과 경계에서 무작위로 샘플링한 16개의 샘플을 평가합니다. 샘플은 4개의 감정 범주에 균등하게 분포됩니다. 객관적인 지표의 경우, 우리는 감정 분류기를 적용하여 EMOPIA와 EMOGEN의 샘플의 감정 레이블을 예측한 다음 객관적인 정확도를 얻을 수 있습니다. B.3. 종합 분석 비교된 각 모듈과 방법에 대해 각 감정 범주에 대해 250개씩 총 1000개의 샘플을 생성합니다. 그런 다음 우리는 참가자에게 1) Russell의 4Q 모델을 기반으로 샘플을 4개의 감정 범주로 분류하도록 초대합니다. 2) 5점 척도로 샘플의 인간성 점수를 평가합니다. 점수가 높을수록 샘플이 인간이 작곡한 샘플에 더 현실적입니다. 각 참가자는 4가지 감정 범주에 균등하게 분포된 44개의 샘플을 받는데, 이는 3개 그룹으로 나뉩니다. 1) 감정-속성 매핑에서 비교된 3가지 방법에 대한 그룹으로, 12개 샘플로 구성됩니다. 2) 속성 설계에서 비교된 3가지 방법에 대한 그룹으로, 12개 샘플로 구성됩니다. 3) 상위 k의 5가지 값에 대한 그룹으로, 20개 샘플로 구성됩니다. 객관적인 지표의 경우, 감정 분류기를 적용하여 생성된 1000개 샘플의 감정 레이블을 예측하고 객관적인 정확도는 부록 §B.1과 유사하게 계산합니다. 속성 설계 세부 정보 표 5에서 다른 두 가지 속성 설계 방법에 대한 세부 정보를 제시합니다. 1. 무작위: jSymbolic은 속성을 7개 그룹으로 나누므로(자세한 내용은 https://jmir.sourceforge.net/manuals/jSymbolic_manual/home.html 참조), 7개 그룹에 따라 평균 100개의 속성을 선택합니다. 2. 매뉴얼: 이전 연구(Zheng et al., 2021; Tan &amp; Herremans, 2020; McKay et al., 2018; Kim et al., 2010)에 따라 피치 클래스 히스토그램, 음표 밀도, 리듬 밀도, 평균 피치/기간/속도를 음악 감정과 관련된 수동으로 설계된 속성으로 선택하여 17차원 속성 벡터를 형성합니다.B.4. 다중 악기 데이터 세트에 대한 응용 프로그램 감정-속성 매핑 단계는 변경하지 않고 TopMAGD에서 속성-음악 생성 단계를 학습합니다.EMOGEN을 적용하여 각 감정 범주에 대해 250개씩 총 1000개의 샘플을 생성합니다.그리고 15명의 참가자를 초대하여 각 샘플을 가치, 각성, 인간성, 전체 점수로 평가하도록 합니다(§B.1과 유사).각 참가자는 4개의 감정 범주로 구성된 4개의 샘플을 평가합니다. 그런 다음 주관적 정확도(§B.1의 계산 규칙과 동일), 인간성 및 전체 점수를 주관적 평가 지표로 보고합니다.B.5. 출력 다양성에 대한 논의 EMOGEN은 각 감정을 나타내는 데 한 세트의 속성만 사용하므로 다양성이 제한됩니다.음악의 감정에는 두 가지 수준이 있습니다.하나는 대부분의 사람이 느끼는 감정(즉, 일반적인 감정)이고 다른 하나는 개인이 느끼는 감정(즉, 개인화된 감정)입니다.일반적인 감정은 개인화된 감정만큼 다양하지 않습니다.이 논문에서는 주로 주관적 편향의 영향을 받지 않는 일반적인 감정으로 음악 세대를 제어하는 것을 고려합니다.그러나 필요한 경우 EmoGen은 감정-속성 매핑 단계에서 다른 감정을 더 많은 속성 값 세트에 매핑하여 더 많은 감정 다양성을 달성할 수도 있습니다.
