--- ABSTRACT ---
Chaoxu Guo* chaoxu.guo@nlpr.ia.ac.cn Alibaba Group Beijing, China Tiezheng Ge tiezheng.gtz@alibaba-inc.com Alibaba Group Chunjie Luo* luochunjie@ict.ac.cn Beijing, China Institute of Computing Technology, Chinese Academy of Sciences Beijing, China 비디오 아웃페인팅은 비디오 프레임의 가장자리에서 누락된 영역을 적절히 완성하는 것을 목표로 합니다. 이미지 아웃페인팅과 비교할 때, 모델이 채워진 영역의 시간적 일관성을 유지해야 하기 때문에 추가적인 과제가 있습니다. 이 논문에서는 비디오 아웃페인팅을 위한 마스크 3D 확산 모델을 소개합니다. 마스크 모델링 기술을 사용하여 3D 확산 모델을 학습합니다. 이를 통해 여러 가이드 프레임을 사용하여 여러 비디오 클립 추론의 결과를 연결하여 시간적 일관성을 보장하고 인접 프레임 간의 지터를 줄일 수 있습니다. 한편, 비디오의 글로벌 프레임을 프롬프트로 추출하고 교차 주의를 사용하여 모델이 현재 비디오 클립 이외의 정보를 얻도록 안내합니다. 또한 아티팩트 축적 문제를 완화하기 위해 하이브리드 거친-정밀 추론 파이프라인을 도입합니다. 기존의 거친-정밀 파이프라인은 채우기 전략만 사용하는데, 이는 희소 프레임의 시간 간격이 너무 크기 때문에 저하를 초래합니다. 저희 파이프라인은 마스크 모델링의 양방향 학습의 이점을 누리므로 희소 프레임을 생성할 때 채우기와 보간의 하이브리드 전략을 사용할 수 있습니다. 실험 결과 저희 방법은 *두 저자 모두 Alibaba Group에서 인턴으로 일하면서 이 연구에 동등하게 기여했습니다. *연락 저자. CC BY 이 작업은 Creative Commons Attribution International 4.0 License에 따라 라이선스가 부여되었습니다. MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 © 2023 저작권은 소유자/저자에게 있습니다. ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.Litong Gong gonglitong.glt@alibaba-inc.com Alibaba Group Beijing, China Yuning Jiang mengzhu.jyn@alibaba-inc.com Jianfeng Zhan zhanjianfeng@ict.ac.cn Alibaba Group Beijing, China Institute of Computing Technology, Chinese Academy of Sciences Beijing, China University of Chinese Academy of Sciences Beijing, China achieves state-of-the-art results in video outpainting task. 더 많은 결과와 코드는 프로젝트 페이지에서 제공됩니다. CCS 개념 ⚫ 컴퓨팅 방법론 → 컴퓨터 비전 문제. 키워드 비디오 아웃페인팅, 확산 모델, 마스크 모델링, coarse-to-fine ACM 참조 형식: Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. 2023. 비디오 아웃페인팅을 위한 계층적 마스크 3D 확산 모델. 31회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;23)의 회의록, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와. ACM, 뉴욕, 뉴욕, 미국, 13페이지. https://doi.org/10.1145/3581783.
--- INTRODUCTION ---
비디오 아웃페인팅의 과제는 제공된 맥락 정보(비디오의 중간 부분)에 따라 비디오의 가장자리 영역을 확장하는 것입니다. 최근 몇 년 동안 이미지 아웃페인팅[4, 5, 22, 28, 30, 38, 42]이 심도 있게 연구되었고 GAN(Generative Adversarial Network)과 확산 모델의 출현으로 매우 유망한 결과를 얻었습니다. 그러나 비디오 아웃페인팅은 현재 이상적인 결과를 달성하는 데는 거리가 멉니다. 단일 이미지의 공간적 모양만 고려하는 이미지 아웃페인팅과 달리 비디오 아웃페인팅은 비디오 프레임 간의 시간적 일관성을 보장하기 위해 동작 정보를 모델링해야 합니다. 게다가 실제 시나리오의 비디오는 일반적으로 5초 이상입니다. 두 가지 추가 과제가 있습니다. 1) 비디오가 Long Videos Short Videos MM &#39;23, October 29-November 3, 2023, Ottawa, ON, Canada Fanda Fan et al. 그림 1: 우리는 비디오 아웃페인팅을 위한 Masked 3D Diffusion Model(M3DDM)과 coarse-to-fine 추론 파이프라인을 제안합니다. 우리의 방법은 높은 시간적 일관성과 합리적인 아웃페인팅 결과를 생성할 뿐만 아니라 긴 비디오 아웃페인팅에서 아티팩트 축적 문제를 완화할 수 있습니다. 위쪽 행은 5개 비디오 클립의 첫 번째와 마지막 프레임을 보여줍니다. 아래의 각 행은 우리 방법의 비디오 아웃페인팅 결과를 보여줍니다. GPU의 긴 지속 시간과 메모리 제약. 동일한 비디오의 다른 클립 간에 생성된 콘텐츠의 시간적 일관성을 보장하는 것은 어렵습니다. 2) 긴 비디오 아웃페인팅은 아티팩트 축적 문제가 있고 그 사이에 많은 양의 계산 리소스가 필요합니다. 몇몇 연구에서 비디오 아웃페인팅을 조사했습니다. Dehan[6]은 비디오 객체 분할 및 비디오 인페인팅 방법을 사용하여 배경 추정을 형성했으며 광학 흐름[10, 34]을 도입하여 시간적 일관성을 보장했습니다. 그러나 복잡한 카메라 동작이 있는 시나리오와 전경 객체가 프레임을 벗어날 때 종종 결과가 좋지 않습니다. MAGVIT [44]은 비디오 아웃페인팅 작업에도 사용할 수 있는 일반적인 마스크 기반 비디오 생성 모델을 제안했습니다. 그들은 비디오를 양자화하고 다중 작업 조건부 마스크 토큰 모델링을 위한 변환기를 설계하기 위해 3D-VectorQuantized(3DVQ) 토크나이저를 도입했습니다. 이러한 방법은 상당히 짧은 비디오 클립을 생성할 수 있지만 긴 비디오에 대한 여러 클립으로 구성된 전체 결과는 좋지 않습니다. 그 이유는 전체 비디오에서 높은 시간적 일관성을 달성하는 능력이 부족하고 여러 클립 추론에서 아티팩트 축적이 발생하기 때문입니다. 이 연구에서는 비디오 아웃페인팅 작업에 중점을 둡니다. 위의 문제를 해결하기 위해 마스크된 3D 확산 모델(M3DDM)과 하이브리드 거친-미세 추론 파이프라인을 제안합니다. 최근 확산 모델[8, 19, 26]은 이미지 합성[14, 28, 30]과 비디오 생성[2, 18, 31]에서 인상적인 결과를 얻었습니다. 우리의 비디오 아웃페인팅 방법은 잠재 확산 모델(LDM)[29]에 기반을 둡니다. 여기서 LDM을 선택하는 데는 두 가지 이점이 있습니다. 1) 픽셀 공간 대신 잠재 공간에서 비디오 프레임을 인코딩하여 메모리가 덜 필요하고 효율성이 더 높습니다. 2) 사전 훈련된 LDM은 자연스러운 이미지 콘텐츠와 구조에 대한 우수한 사전 정보를 제공하여 모델이 비디오 아웃페인팅 작업에서 빠르게 수렴하는 데 도움이 될 수 있습니다. 단일 클립과 동일한 비디오의 다른 클립에서 높은 시간적 일관성을 보장하기 위해 두 가지 기술을 사용합니다. 1) 마스크된 가이드 프레임은 의미적으로 더 일관되고 이웃 클립과의 지터가 적은 현재 클립을 생성하는 데 도움이 됩니다. 마스크 모델링은 이미지[4]와 비디오 생성[4, 15]에서 효과적인 것으로 입증되었습니다. 훈련 단계에서 우리는 무작위로 맥락 정보를 원시 프레임으로 대체합니다. 원시 프레임은 가장자리 영역이 있고 가이드 프레임 역할을 합니다. 이런 방식으로 모델은 맥락 정보뿐만 아니라 인접한 가이드 프레임을 기반으로도 가장자리 영역을 예측할 수 있습니다. 인접한 가이드 프레임은 보다 일관되고 덜 흔들리는 결과를 생성하는 데 도움이 될 수 있습니다. 추론 단계에서는 프레임을 반복적으로 희소하게 아웃페인팅하여 이전에 생성된 프레임을 가이드 프레임으로 사용할 수 있습니다. 마스크 모델링 접근 방식을 사용하는 데는 두 가지 이점이 있습니다. 한편으로는 마스크 모델링의 양방향 학습 모드를 통해 모델이 맥락적 정보를 더 잘 인식하여 더 나은 단일 클립 추론이 가능합니다. 다른 한편으로는 하이브리드 거친-미세 추론 파이프라인을 사용할 수 있습니다. 하이브리드 파이프라인은 가이드 프레임으로 첫 번째와 마지막 프레임을 사용하는 채우기 전략을 사용할 뿐만 아니라 보간 전략 CTF Dense Hierarchical Masked 3D Diffusion Model for Video OutpaintingMM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와를 사용하여 비디오를 생성합니다. 실험 결과 하이브리드 거친-미세 추론 파이프라인은 긴 비디오 생성에서 아티팩트를 줄이고 더 나은 결과를 가져옵니다. 우리의 주요 기여는 다음과 같습니다.그림 2: 긴 비디오 아웃페인팅의 아티팩트 축적 문제.우리는 M3DDM을 통해 두 가지 추론 방법, 즉 밀집 추론과 거친-미세(CTF) 추론을 비교합니다.비디오 프레임의 인덱스는 이미지 위에 레이블이 지정되어 있습니다.이 사례는 마스크 비율이 0.5인 수평 비디오 아웃페인팅을 보여줍니다.첫 번째 이미지에서 확장할 영역을 빨간색 선으로 표시합니다.여러 중간 프레임을 가이드 프레임으로 사용합니다.2) 프롬프트로서의 글로벌 비디오 클립은 전체 비디오에서 g개의 글로벌 프레임을 균일하게 추출하고, 경량 인코더를 사용하여 이를 피처 맵으로 인코딩한 다음, 교차 주의를 통해 현재 비디오 클립(비디오 클립의 중간 부분)의 컨텍스트와 상호 작용합니다.이 기술을 사용하면 모델은 현재 클립을 생성할 때 일부 글로벌 비디오 정보를 얻을 수 있습니다.누출을 방지하기 위해 입력하는 비디오의 글로벌 프레임에는 채워야 할 가장자리 영역이 포함되지 않는다는 점에 유의해야 합니다. 실험 결과, 복잡한 카메라 움직임과 전경 객체가 앞뒤로 움직이는 장면에서 우리 방법은 시간적으로 일관된 완전한 비디오를 생성할 수 있습니다. 우리 방법으로 생성된 일부 결과는 그림 1에서 볼 수 있습니다. 우리의 하이브리드 거친-정밀 추론 파이프라인은 긴 비디오 아웃페인팅에서 아티팩트 축적 문제를 완화할 수 있습니다. 추론 단계에서 가이드 프레임을 사용하여 반복 생성하기 때문에 이전 단계에서 생성된 나쁜 사례가 후속 생성 결과를 오염시킵니다(그림 2에 표시됨. 나중에 자세히 설명). 긴 비디오 생성 작업의 경우 거친-정밀 추론 파이프라인[17, 43]이 최근 제안되었습니다. 거친 단계에서 파이프라인은 먼저 비디오의 키 프레임을 희소하게 생성합니다. 그런 다음 키 프레임에 따라 각 프레임을 조밀하게 생성합니다. 비디오를 직접 조밀하게 생성하는 것과 비교할 때 거친 단계는 (희소하기 때문에) 반복이 덜 필요하므로 긴 비디오에서 아티팩트 축적 문제가 완화됩니다. 기존의 조대-정밀 추론 파이프라인[17, 43]은 3단계 계층 구조를 사용했습니다. 그러나 조대에서 정밀로 비디오 생성을 안내하기 위해 첫 번째와 마지막 프레임을 사용한 채우기 전략만 사용했습니다. 이 전략은 가장 조대 단계(첫 번째 수준)에서 생성된 키 프레임 사이에 큰 시간 간격을 초래하여 생성된 결과의 저하를 초래합니다(그림 6a 참조). 또한 비디오 아웃페인팅에도 조대-정밀 추론 파이프라인을 사용합니다. 학습 단계에서 마스크 전략 덕분에 채우기 전략과 보간 전략을 함께 혼합할 수 있습니다. 즉, 첫 번째와 마지막 프레임을 3단계 조대-정밀 구조의 가이드로 사용할 수 있을 뿐만 아니라 여러 프레임 보간⚫을 사용할 수도 있습니다. 저희가 아는 한, 비디오 아웃페인팅에 마스크된 3D 확산 모델을 사용하여 최첨단 결과를 달성한 것은 저희입니다. • 3D 확산 모델을 학습하기 위해 마스크 모델링을 사용한 양방향 학습 방법을 제안합니다. 또한, 가이드 프레임을 사용하여 동일한 비디오의 다른 클립을 연결하면 시간적 일관성이 높고 지터가 낮은 비디오 아웃페인팅 결과를 효과적으로 생성할 수 있음을 보여줍니다. • 비디오의 글로벌 프레임에서 프롬프트로 글로벌 시간 및 공간 정보를 추출하여 교차 주의의 형태로 네트워크에 공급하여 모델이 더 합리적인 결과를 생성하도록 안내합니다. • 희소 프레임을 생성할 때 채우기와 보간을 결합하는 하이브리드 거친-미세 생성 파이프라인을 제안합니다. 실험 결과 파이프라인은 시간적 일관성을 좋은 수준으로 유지하면서 긴 비디오 아웃페인팅에서 아티팩트 축적을 줄일 수 있음을 보여줍니다.
--- RELATED WORK ---
이 섹션에서는 관련 확산 모델, 마스크 모델링 및 Coarse-to-Fine 파이프라인을 소개합니다. 확산 모델. 확산 모델[19, 26, 32]은 최근 이미지 생성[28, 30], 특히 비디오 생성[18, 25, 31]에서 최고의 기술이 되었습니다. GAN[12]과 비교할 때 더 풍부한 다양성과 더 높은 품질의 샘플을 생성할 수 있습니다[8]. 비디오 생성에서 확산 모델의 중요한 성과를 고려하여 이를 비디오 아웃페인팅의 주요 내용으로 채택합니다.
--- METHOD ---
*두 저자는 Alibaba Group에서 인턴으로 일하는 동안 이 연구에 동등하게 기여했습니다. *연락 저자. CC BY 이 저작물은 Creative Commons Attribution International 4.0 License에 따라 라이선스가 부여되었습니다. MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 © 2023 저작권은 소유자/저자에게 있습니다. ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.Litong Gong gonglitong.glt@alibaba-inc.com Alibaba Group Beijing, China Yuning Jiang mengzhu.jyn@alibaba-inc.com Jianfeng Zhan zhanjianfeng@ict.ac.cn Alibaba Group Beijing, China Institute of Computing Technology, Chinese Academy of Sciences Beijing, China University of Chinese Academy of Sciences Beijing, China achieves state-of-the-art results in video outpainting task. 더 많은 결과와 코드는 프로젝트 페이지에서 제공됩니다. CCS 개념 ⚫ 컴퓨팅 방법론 → 컴퓨터 비전 문제. 키워드 비디오 아웃페인팅, 확산 모델, 마스크 모델링, coarse-to-fine ACM 참조 형식: Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. 2023. 비디오 아웃페인팅을 위한 계층적 마스크 3D 확산 모델. 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와에서 열린 제31회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;23)의 논문집. 미국 뉴욕주 뉴욕주 ACM, 13페이지. https://doi.org/10.1145/3581783. 서론 비디오 아웃페인팅의 과제는 제공된 맥락 정보(비디오의 중간 부분)에 따라 비디오의 가장자리 영역을 확장하는 것입니다. 최근 몇 년 동안 이미지 아웃페인팅[4, 5, 22, 28, 30, 38, 42]이 심도 있게 연구되었고 GAN(Generative Adversarial Network)과 확산 모델의 등장으로 매우 유망한 결과를 얻었습니다. 그러나 비디오 아웃페인팅은 현재 이상적인 결과를 달성하는 데는 거리가 멉니다. 이미지 아웃페인팅과 달리 단일 이미지의 공간적 모양만 고려하는 비디오 아웃페인팅은 비디오 프레임 간의 시간적 일관성을 보장하기 위해 동작 정보의 모델링이 필요합니다.게다가 실제 시나리오에서 비디오는 일반적으로 5초 이상입니다.두 가지 추가 과제가 있습니다.1) 긴 비디오 짧은 비디오 MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 Fanda Fan et al.그림 1: 비디오 아웃페인팅을 위한 Masked 3D Diffusion Model(M3DDM)과 coarse-to-fine 추론 파이프라인을 제안합니다.우리의 방법은 높은 시간적 일관성과 합리적인 아웃페인팅 결과를 생성할 수 있을 뿐만 아니라 긴 비디오 아웃페인팅에서 아티팩트 축적 문제를 완화할 수도 있습니다.위쪽 행은 5개 비디오 클립의 첫 번째와 마지막 프레임을 보여줍니다.아래쪽 각 행은 우리 방법의 비디오 아웃페인팅 결과를 보여줍니다.GPU의 긴 지속 시간과 메모리 제약. 동일한 비디오의 여러 클립 간에 생성된 콘텐츠의 시간적 일관성을 보장하는 것은 어렵습니다. 2) 긴 비디오 아웃페인팅은 아티팩트 축적 문제가 있고 그 사이에 많은 양의 계산 리소스가 필요합니다. 몇몇 연구에서 비디오 아웃페인팅을 조사했습니다. Dehan[6]은 비디오 객체 분할 및 비디오 인페인팅 방법을 사용하여 배경 추정을 형성했으며 광학 흐름[10, 34]을 도입하여 시간적 일관성을 보장했습니다. 그러나 복잡한 카메라 동작이 있는 시나리오와 전경 객체가 프레임을 벗어날 때 종종 결과가 좋지 않습니다. MAGVIT[44]은 비디오 아웃페인팅 작업에도 사용할 수 있는 일반적인 마스크 기반 비디오 생성 모델을 제안했습니다. 그들은 비디오를 양자화하고 다중 작업 조건부 마스크 토큰 모델링을 위한 변환기를 설계하기 위해 3D-VectorQuantized(3DVQ) 토크나이저를 도입했습니다. 이러한 방법은 상당히 짧은 비디오 클립을 생성할 수 있지만 긴 비디오에 대한 여러 클립으로 구성된 전체 결과는 좋지 않습니다. 그 이유는 전체 비디오에서 높은 시간적 일관성을 달성하는 능력이 부족하고 여러 클립 추론에서 아티팩트 축적이 발생하기 때문입니다.이 연구에서는 비디오 아웃페인팅 작업에 중점을 둡니다.위의 문제를 해결하기 위해 마스크된 3D 확산 모델(M3DDM)과 하이브리드 거친-미세 추론 파이프라인을 제안합니다.최근 확산 모델[8, 19, 26]은 이미지 합성[14, 28, 30]과 비디오 생성[2, 18, 31]에서 인상적인 결과를 얻었습니다.저희의 비디오 아웃페인팅 방법은 잠재 확산 모델(LDM)[29]을 기반으로 합니다.여기서 LDM을 선택하는 데는 두 가지 이점이 있습니다.1) 픽셀 공간 대신 잠재 공간에서 비디오 프레임을 인코딩하여 메모리를 덜 사용하고 효율성을 높입니다.2) 사전 학습된 LDM은 자연스러운 이미지 콘텐츠와 구조에 대한 우수한 사전 정보를 제공하여 모델이 비디오 아웃페인팅 작업에서 빠르게 수렴하는 데 도움이 될 수 있습니다. 단일 클립과 동일한 비디오의 다른 클립에서 높은 시간적 일관성을 보장하기 위해 두 가지 기술을 사용합니다. 1) 마스크된 가이드 프레임은 의미적으로 더 일관되고 이웃 클립과의 지터가 적은 현재 클립을 생성하는 데 도움이 됩니다. 마스크 모델링은 이미지 [4] 및 비디오 생성 [4, 15]에서 효과적인 것으로 입증되었습니다. 학습 단계에서 문맥 정보를 가장자리 영역이 있고 가이드 프레임 역할을 하는 원시 프레임으로 무작위로 대체합니다. 이런 방식으로 모델은 문맥 정보뿐만 아니라 인접한 가이드 프레임을 기반으로 가장자리 영역을 예측할 수 있습니다. 인접한 가이드 프레임은 더 일관되고 지터가 적은 결과를 생성하는 데 도움이 될 수 있습니다. 추론 단계에서는 프레임을 반복적으로 희소하게 아웃페인팅하여 이전에 생성된 프레임을 가이드 프레임으로 사용할 수 있습니다. 마스크 모델링 접근 방식을 사용하는 데는 두 가지 이점이 있습니다. 한편, 마스크 모델링의 양방향 학습 모드를 통해 모델이 문맥 정보를 더 잘 인식하여 더 나은 단일 클립 추론이 가능합니다. 반면에, 그것은 우리가 하이브리드 거친-미세 추론 파이프라인을 사용할 수 있게 해줍니다. 하이브리드 파이프라인은 가이드 프레임으로 첫 번째와 마지막 프레임을 사용하는 채우기 전략을 사용할 뿐만 아니라 보간 전략 CTF Dense Hierarchical Masked 3D Diffusion Model for Video OutpaintingMM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와를 사용하여 비디오를 생성합니다.
--- EXPERIMENT ---
s는 저희 방법이 *두 저자 모두 Alibaba Group에서 인턴으로 일하는 동안 이 연구에 동등하게 기여했습니다. *연락 저자. CC BY 이 저작물은 Creative Commons Attribution International 4.0 License에 따라 라이선스가 부여되었습니다. MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 © 2023 저작권은 소유자/저자에게 있습니다. ACM ISBN 979-8-4007-0108-5/23/10. https://doi.org/10.1145/3581783.Litong Gong gonglitong.glt@alibaba-inc.com Alibaba Group Beijing, China Yuning Jiang mengzhu.jyn@alibaba-inc.com Jianfeng Zhan zhanjianfeng@ict.ac.cn Alibaba Group Beijing, China Institute of Computing Technology, Chinese Academy of Sciences Beijing, China University of Chinese Academy of Sciences Beijing, China achieves state-of-the-art results in video outpainting task. 더 많은 결과와 코드는 프로젝트 페이지에서 제공됩니다. CCS 개념 ⚫ 컴퓨팅 방법론 → 컴퓨터 비전 문제. 키워드 비디오 아웃페인팅, 확산 모델, 마스크 모델링, coarse-to-fine ACM 참조 형식: Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. 2023. 비디오 아웃페인팅을 위한 계층적 마스크 3D 확산 모델. 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와에서 열린 제31회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;23)의 논문집. 미국 뉴욕주 뉴욕주 ACM, 13페이지. https://doi.org/10.1145/3581783. 서론 비디오 아웃페인팅의 과제는 제공된 맥락 정보(비디오의 중간 부분)에 따라 비디오의 가장자리 영역을 확장하는 것입니다. 최근 몇 년 동안 이미지 아웃페인팅[4, 5, 22, 28, 30, 38, 42]이 심도 있게 연구되었고 GAN(Generative Adversarial Network)과 확산 모델의 등장으로 매우 유망한 결과를 얻었습니다. 그러나 비디오 아웃페인팅은 현재 이상적인 결과를 달성하는 데는 거리가 멉니다. 이미지 아웃페인팅과 달리 단일 이미지의 공간적 모양만 고려하는 비디오 아웃페인팅은 비디오 프레임 간의 시간적 일관성을 보장하기 위해 동작 정보의 모델링이 필요합니다.게다가 실제 시나리오에서 비디오는 일반적으로 5초 이상입니다.두 가지 추가 과제가 있습니다.1) 긴 비디오 짧은 비디오 MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 Fanda Fan et al.그림 1: 비디오 아웃페인팅을 위한 Masked 3D Diffusion Model(M3DDM)과 coarse-to-fine 추론 파이프라인을 제안합니다.우리의 방법은 높은 시간적 일관성과 합리적인 아웃페인팅 결과를 생성할 수 있을 뿐만 아니라 긴 비디오 아웃페인팅에서 아티팩트 축적 문제를 완화할 수도 있습니다.위쪽 행은 5개 비디오 클립의 첫 번째와 마지막 프레임을 보여줍니다.아래쪽 각 행은 우리 방법의 비디오 아웃페인팅 결과를 보여줍니다.GPU의 긴 지속 시간과 메모리 제약. 동일한 비디오의 여러 클립 간에 생성된 콘텐츠의 시간적 일관성을 보장하는 것은 어렵습니다. 2) 긴 비디오 아웃페인팅은 아티팩트 축적 문제가 있고 그 사이에 많은 양의 계산 리소스가 필요합니다. 몇몇 연구에서 비디오 아웃페인팅을 조사했습니다. Dehan[6]은 비디오 객체 분할 및 비디오 인페인팅 방법을 사용하여 배경 추정을 형성했으며 광학 흐름[10, 34]을 도입하여 시간적 일관성을 보장했습니다. 그러나 복잡한 카메라 동작이 있는 시나리오와 전경 객체가 프레임을 벗어날 때 종종 결과가 좋지 않습니다. MAGVIT[44]은 비디오 아웃페인팅 작업에도 사용할 수 있는 일반적인 마스크 기반 비디오 생성 모델을 제안했습니다. 그들은 비디오를 양자화하고 다중 작업 조건부 마스크 토큰 모델링을 위한 변환기를 설계하기 위해 3D-VectorQuantized(3DVQ) 토크나이저를 도입했습니다. 이러한 방법은 상당히 짧은 비디오 클립을 생성할 수 있지만 긴 비디오에 대한 여러 클립으로 구성된 전체 결과는 좋지 않습니다. 그 이유는 전체 비디오에서 높은 시간적 일관성을 달성하는 능력이 부족하고 여러 클립 추론에서 아티팩트 축적이 발생하기 때문입니다.이 연구에서는 비디오 아웃페인팅 작업에 중점을 둡니다.위의 문제를 해결하기 위해 마스크된 3D 확산 모델(M3DDM)과 하이브리드 거친-미세 추론 파이프라인을 제안합니다.최근 확산 모델[8, 19, 26]은 이미지 합성[14, 28, 30]과 비디오 생성[2, 18, 31]에서 인상적인 결과를 얻었습니다.저희의 비디오 아웃페인팅 방법은 잠재 확산 모델(LDM)[29]을 기반으로 합니다.여기서 LDM을 선택하는 데는 두 가지 이점이 있습니다.1) 픽셀 공간 대신 잠재 공간에서 비디오 프레임을 인코딩하여 메모리를 덜 사용하고 효율성을 높입니다.2) 사전 학습된 LDM은 자연스러운 이미지 콘텐츠와 구조에 대한 우수한 사전 정보를 제공하여 모델이 비디오 아웃페인팅 작업에서 빠르게 수렴하는 데 도움이 될 수 있습니다. 단일 클립과 동일한 비디오의 다른 클립에서 높은 시간적 일관성을 보장하기 위해 두 가지 기술을 사용합니다. 1) 마스크된 가이드 프레임은 의미적으로 더 일관되고 이웃 클립과의 지터가 적은 현재 클립을 생성하는 데 도움이 됩니다. 마스크 모델링은 이미지 [4] 및 비디오 생성 [4, 15]에서 효과적인 것으로 입증되었습니다. 학습 단계에서 문맥 정보를 가장자리 영역이 있고 가이드 프레임 역할을 하는 원시 프레임으로 무작위로 대체합니다. 이런 방식으로 모델은 문맥 정보뿐만 아니라 인접한 가이드 프레임을 기반으로 가장자리 영역을 예측할 수 있습니다. 인접한 가이드 프레임은 더 일관되고 지터가 적은 결과를 생성하는 데 도움이 될 수 있습니다. 추론 단계에서는 프레임을 반복적으로 희소하게 아웃페인팅하여 이전에 생성된 프레임을 가이드 프레임으로 사용할 수 있습니다. 마스크 모델링 접근 방식을 사용하는 데는 두 가지 이점이 있습니다. 한편, 마스크 모델링의 양방향 학습 모드를 통해 모델이 문맥 정보를 더 잘 인식하여 더 나은 단일 클립 추론이 가능합니다. 반면에, 그것은 우리가 하이브리드 coarse-to-fine 추론 파이프라인을 사용할 수 있게 해줍니다. 하이브리드 파이프라인은 가이드 프레임으로 첫 번째와 마지막 프레임을 사용하는 인필링 전략을 사용할 뿐만 아니라 비디오를 생성하기 위해 보간 전략 CTF Dense Hierarchical Masked 3D Diffusion Model for Video OutpaintingMM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와. 실험 결과, 우리의 하이브리드 coarse-to-fine 추론 파이프라인은 긴 비디오 생성에서 더 낮은 아티팩트와 더 나은 결과를 가져옵니다. 우리의 주요 기여는 다음과 같습니다. 그림 2: 긴 비디오 아웃페인팅에서의 아티팩트 축적 문제. 우리는 M3DDM을 통해 두 가지 추론 방법, 즉 dense 추론과 coarse-to-fine(CTF) 추론을 비교합니다. 비디오 프레임의 인덱스는 이미지 위에 레이블이 지정되어 있습니다. 이 사례는 마스크 비율이 0.5인 수평 비디오 아웃페인팅을 보여줍니다. 우리는 첫 번째 이미지에서 확장할 영역을 빨간색 선으로 표시합니다. 여러 중간 프레임을 가이드 프레임으로 사용합니다. 2) 프롬프트로서의 글로벌 비디오 클립은 전체 비디오에서 g개의 글로벌 프레임을 균일하게 추출하고, 가벼운 인코더를 사용하여 이를 피처 맵으로 인코딩한 다음, 교차 주의를 통해 현재 비디오 클립(비디오 클립의 중간 부분)의 컨텍스트와 상호 작용합니다. 이 기술을 사용하면 모델은 현재 클립을 생성할 때 일부 글로벌 비디오 정보를 얻을 수 있습니다. 입력하는 비디오의 글로벌 프레임에는 누출을 방지하기 위해 채워야 할 가장자리 영역이 포함되지 않는다는 점에 유의해야 합니다. 실험 결과 복잡한 카메라 모션과 앞뒤로 움직이는 전경 객체가 있는 장면에서 우리 방법이 시간적으로 더 일관된 전체 비디오를 생성할 수 있습니다. 우리 방법으로 생성된 일부 결과는 그림 1에서 볼 수 있습니다. 우리의 하이브리드 거친-정밀 추론 파이프라인은 긴 비디오 아웃페인팅에서 아티팩트 축적 문제를 완화할 수 있습니다. 추론 단계에서 가이드 프레임을 사용하여 반복 생성하기 때문에 이전 단계에서 생성된 나쁜 사례가 후속 생성 결과를 오염시킵니다(그림 2에 표시됨. 나중에 자세히 설명). 긴 비디오 생성 작업의 경우, 최근 coarse-to-fine 추론 파이프라인[17, 43]이 제안되었습니다. coarse 단계에서 파이프라인은 먼저 비디오의 키 프레임을 sparsely 생성합니다. 그런 다음 키 프레임에 따라 각 프레임을 densely 생성합니다. dense한 방식으로 비디오를 직접 생성하는 것과 비교할 때, coarse 단계는 sparse하기 때문에 반복이 덜 필요하므로 긴 비디오에서 아티팩트 축적 문제가 완화됩니다. 기존의 coarse-to-fine 추론 파이프라인[17, 43]은 3단계 계층 구조를 사용했습니다. 그러나 첫 번째와 마지막 프레임으로 채우기 전략만을 사용하여 coarse에서 fine으로 비디오 생성을 안내했습니다. 이 전략은 가장 coarse한 단계(첫 번째 레벨)에서 생성된 키 프레임 사이에 큰 시간 간격을 초래하여 생성된 결과의 저하를 초래합니다(그림 6a 참조). 또한 비디오 아웃페인팅에도 coarse-to-fine 추론 파이프라인을 사용합니다. 학습 단계에서 마스크 전략을 사용함으로써, 우리는 인필링 전략과 보간 전략을 함께 하이브리드화할 수 있습니다.즉, 3단계의 조악-미세 구조에 대한 가이드로 첫 번째와 마지막 프레임을 사용할 수 있을 뿐만 아니라 다중 프레임 보간⚫을 사용할 수도 있습니다.우리가 아는 한, 우리는 비디오 아웃페인팅을 위해 마스크된 3D 확산 모델을 사용하여 최첨단 결과를 달성한 최초의 사례입니다.• 우리는 3D 확산 모델을 학습하기 위해 마스크 모델링을 사용한 양방향 학습 방법을 제안합니다.또한, 가이드 프레임을 사용하여 동일한 비디오의 다른 클립을 연결하면 높은 시간적 일관성과 낮은 지터로 비디오 아웃페인팅 결과를 효과적으로 생성할 수 있음을 보여줍니다.• 우리는 비디오의 글로벌 프레임에서 프롬프트로 글로벌 시간 및 공간 정보를 추출하여 교차 어텐션의 형태로 네트워크에 공급합니다.이는 모델이 더 합리적인 결과를 생성하도록 안내합니다.• 우리는 희소 프레임을 생성할 때 인필링과 보간을 결합하는 하이브리드 조악-미세 생성 파이프라인을 제안합니다. 실험 결과 파이프라인이 시간적 일관성을 좋은 수준으로 유지하면서 긴 비디오 아웃페인팅에서 아티팩트 축적을 줄일 수 있음을 보여줍니다.관련 연구 이 섹션에서는 관련 확산 모델, 마스크 모델링 및 Coarse-to-Fine 파이프라인을 소개합니다.확산 모델.확산 모델[19, 26, 32]은 최근 이미지 생성[28, 30], 특히 비디오 생성[18, 25, 31]에서 최고의 기술이 되었습니다.GAN[12]과 비교할 때 더 풍부한 다양성과 더 높은 품질의 샘플을 생성할 수 있습니다[8].비디오 생성에서 확산 모델의 중요한 성과를 고려하여 비디오 아웃페인팅 방법의 본문으로 채택했습니다.LDM[29]은 잠재 공간의 확산 모델로 GPU 메모리 사용량을 줄이고 오픈 소스 매개변수는 비디오 아웃페인팅 작업에 대한 뛰어난 이미지 사전 확률입니다.마스크 모델링.마스크 모델링은 언어 표현 학습을 위한 NLP 분야의 BERT[7]에서 처음 제안되었습니다. BERT는 문장의 토큰을 무작위로 마스크하고 맥락에 따라 마스크된 토큰을 예측하여 양방향 학습을 수행합니다.MAE[16]는 마스크 모델링이 컴퓨터 비전 분야의 비지도 이미지 표현 학습에 효과적으로 사용될 수 있음을 보여주었습니다.이는 이미지의 패치 토큰을 마스크하고 맥락에 따라 원래 패치 토큰을 예측하여 달성됩니다.최근에 마스크 모델링은 비디오 생성 분야에서도 사용되었습니다[15].더 최근에는 마스크 모델링과 확산 모델의 조합이 이미지[14, 40] 및 비디오 생성[37] 작업에 적용되었습니다.이 논문에서는 이미지나 비디오 전체 프레임에 마스크를 적용하지 않고 비디오 아웃페인팅의 특징을 고려하여 확률로 채워야 하는 비디오 주변 영역에 마스크를 적용합니다.실험 결과 비디오 아웃페인팅 작업의 경우 마스크 모델링과 함께 확산 모델 기술을 사용하면 더 높은 품질의 결과를 생성할 수 있습니다. MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 원시 비디오 클립 조건부 입력 MSE 손실 노이즈 제거 3D U-Net Fanda Fan 등 노이즈 추가 Q KV QQ KV KV KV KV KVKV Zt 이진 마스크 타임스텝 경량 인코더 FPS 속도 E 랜덤 마스크 비디오 클립 마스크 비디오 잠재 영상 p 확률을 갖는 프레임이 주어짐 글로벌 비디오 클립 학습 가능한 네트워크 Q KV 자기 주의 Q KV 시간적 주의 교차 주의 KV 그림 3: 마스크 3D 확산 모델 프레임워크.학습하는 동안 손상된 원시 비디오 잠재 영상, 랜덤 마스크 비디오 잠재 영상 및 마스크를 연결한 다음 3D UNet 네트워크에 공급합니다.네트워크는 손상된 원시 잠재 영상의 노이즈를 예측하여 추가된 노이즈로 MSE 손실을 계산할 수 있습니다.또한 비디오에서 g개의 글로벌 프레임을 프롬프트로 균일하게 선택하여 학습 가능한 비디오 인코더에 공급합니다. 그런 다음 글로벌 프레임 피처 맵은 3D UNet의 교차 주의 모듈에 배치됩니다.Coarse-to-Fine 파이프라인.긴 비디오를 생성할 때 모델은 종종 자기 회귀 전략으로 인해 아티팩트 축적으로 어려움을 겪습니다.가이드 프레임이 있는 비디오를 생성하는 방법의 경우 이전 비디오 클립의 아티팩트가 종종 이후 반복에 영향을 미칩니다.최근 연구[2, 17, 43]는 비디오 생성을 위해 coarse-to-fine 생성 파이프라인을 채택합니다.먼저 비디오의 희소 키 프레임을 생성하고 반복 횟수를 줄여 아티팩트 문제를 완화합니다.비디오 아웃페인팅 작업에서 coarse-to-fine 추론 파이프라인을 채택하고 두 개의 가이드 프레임을 사용한 채우기 전략과 여러 개의 가이드 프레임을 사용한 보간 전략을 모두 사용하여 긴 비디오의 아티팩트 축적 문제를 완화합니다. 3 방법론 3. 예비 확산 모델[8, 19, 26, 32]은 확률적 모델로, 원래 분포에 노이즈를 먼저 전방으로 추가하여 데이터 분포 Pdata를 학습한 다음, 정규 분포 변수의 노이즈를 점진적으로 제거하여 원래 분포를 복구합니다. 전방 노이즈 처리 과정에서 샘플 xo는 다음의 전이 커널을 사용하여 t = 0에서 t = T로 손상될 수 있습니다. qt(xt|xt−1) = N(xt; √1 − ẞtxt−1, ẞtI). (1) 그리고 xt는 다음의 누적 커널을 사용하여 xo에서 직접 샘플링될 수 있습니다. xt = √ã+x0 + √1 − ãƒ€, (2) = П(1 - Bs), € S=~ 여기서 N(0, 1)입니다. 노이즈 처리 과정에서 일반적으로 딥 모델은 손상된 신호 xt의 노이즈를 예측하도록 학습됩니다. 모델의 손실 함수는 LDM = Ex,e~N(0,1),t [||€ – €0 (xt, c, t) ||{}]로 간단히 작성할 수 있습니다. 여기서 c는 조건부 입력이고 t는 {1,...,T}에서 균일하게 샘플링한 것입니다. LDM[29]은 또한 인코더 E를 학습하여 픽셀 공간에서 잠재 공간으로 원래 xo를 매핑하여 메모리 사용량을 크게 줄이고 허용 가능한 손실로 모델을 더 효율적으로 만들었습니다. 그런 다음 디코더 D를 사용하여 zo를 다시 픽셀 공간으로 매핑합니다. 비디오 아웃페인팅 작업에 큰 메모리가 필요하다는 점을 고려하여 파이프라인으로 LDM 프레임워크를 선택했습니다. 또한 LDM의 사전 학습 매개변수는 좋은 이미지 사전 역할을 할 수 있어 모델이 더 빨리 수렴하는 데 도움이 됩니다. 방정식 3에서 x를 z로 다시 씁니다. 3. 마스크 3D 확산 모델 LDM의 도움으로, 순진한 접근법은 원시 비디오 클립의 노이즈가 있는 잠재 데이터를 조건부 입력으로 비디오 클립의 맥락과 연결하고 추가된 노이즈를 예측하는 모델을 학습시키는 것입니다. 따라서 모델은 무작위로 샘플링된 가우시안 노이즈 분포에서 원시 비디오 클립(원본 비디오)을 복구할 수 있습니다. 비디오에는 일반적으로 수백 개의 프레임이 포함되어 있으므로 모델은 동일한 비디오의 다른 클립에 대해 별도로 추론을 수행해야 하며, 생성된 클립을 함께 연결하여 최종 아웃페인팅 Hierarchical Masked 3D Diffusion Model for Video Outpainting MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와, 완전한 비디오의 최종 아웃페인팅 결과를 형성합니다. 이러한 상황에서 위의 순진한 접근법은 예측된 비디오 클립의 시간적 일관성을 보장할 수 없습니다. 이를 해결하기 위해, 그림 3에 개요가 나와 있는 마스크 처리된 3D 확산 모델을 제안합니다. 저희 모델은 한 번에 F개의 프레임을 생성할 수 있습니다. 부록 C.1에서 네트워크 아키텍처를 설명합니다. 초당 프레임 수(fps)가 다른 비디오 프레임을 샘플링하고 추가로 fps를 3D UNet에 공급합니다. 이를 통해 하나의 통합 모델을 사용하여 프레임 속도가 다른 비디오에 적응할 수 있습니다. 저희 프레임워크는 LDM을 따르고 먼저 사전 훈련된 인코더 E를 통해 픽셀 공간의 비디오 프레임을 잠재 공간에 매핑합니다. 훈련 단계에서 각 컨텍스트 프레임은 인코더 E에 공급되기 전에 확률 Pframe이 있는 원시 비디오 프레임으로 대체됩니다. 따라서 저희 모델은 추론 단계에서 가이드 프레임을 사용할 수 있으며, 두 개 이상의 프레임을 조절하여 다른 프레임을 생성하기 쉽게 할 수 있습니다. 이 수정에는 두 가지 이점이 있습니다. 첫째, 거친-미세 추론 파이프라인을 활성화하여 여러 패스에서 일관된 추론 시간을 보장합니다. 둘째, 첫 번째 또는 마지막 원시 프레임만을 입력 조건으로 사용하는 것에 비해 양방향 학습은 모델이 맥락적 정보를 더 잘 인식하도록 돕고, 따라서 생성 품질을 개선할 수 있습니다. 우리는 절제 연구에서 이 지점을 검증할 것입니다. 3.2.1 마스크 전략. 비디오 아웃페인팅을 위한 훈련 샘플을 구성하기 위해 각 프레임의 가장자리를 무작위로 마스크합니다. 우리는 다양한 방향 전략으로 프레임을 마스크합니다: 4방향, 단일 방향, 양방향(좌우 또는 위에서 아래로), 4개 방향 중 임의의 방향, 모두 마스크. 실제 적용 시나리오를 고려하여 이 다섯 가지 전략의 비율을 각각 0.2, 0.1, 0.35, 0.1 및 0.25로 채택합니다. &quot;모두 마스크&quot; 전략을 사용하면 모델이 무조건 생성을 수행할 수 있으므로 추론 단계에서 분류기 없는 안내[20] 기술을 채택할 수 있습니다. 실제 적용 시나리오에서 아웃페인팅해야 하는 에지 영역의 크기를 고려하여, [0.15, 0.75]에서 프레임의 마스크 비율을 균일하게 무작위로 샘플링합니다. 마스크된 가이드 프레임을 생성하기 위해 세 가지 경우에 컨텍스트 프레임을 원시 프레임으로 바꿉니다. 1) 모든 F 프레임에 컨텍스트 정보만 주어지고, 각 프레임은 위의 마스킹 전략으로 마스크됩니다. 2) F 프레임의 첫 번째 프레임 또는 첫 번째와 마지막 프레임은 마스크되지 않은 원시 프레임으로 대체되고 나머지 프레임에는 컨텍스트 정보만 주어집니다. 3) 모든 프레임은 확률 Pframe = 0.5로 마스크되지 않은 원시 프레임으로 대체됩니다. 가이드 프레임을 사용하면 모델이 컨텍스트 정보뿐만 아니라 인접한 가이드 프레임을 기반으로도 에지 영역을 예측할 수 있습니다. 인접한 가이드 프레임은 보다 일관되고 덜 흔들리는 결과를 생성하는 데 도움이 될 수 있습니다. 세 가지 경우의 학습 비율을 균등하게 분배합니다. 이 세 가지 경우의 비율은 각각 0.3, 0.35 및 0.35입니다. 우리는 예측 단계에서 처음 두 사례가 더 자주 사용될 것이라고 생각했기 때문에 사례 3만을 사용하여 훈련하지 않습니다.3.2.2 프롬프트로서의 글로벌 비디오 클립.모델이 현재 클립 너머의 글로벌 비디오 정보를 인식할 수 있도록 하기 위해 비디오에서 g 프레임을 균일하게 샘플링합니다.이러한 글로벌 프레임은 학습 가능한 경량 인코더를 통해 전달되어 피처 맵을 얻은 다음 교차 주의를 통해 3D-UNet에 공급됩니다.3D-UNet의 입력 계층에서 글로벌 프레임을 공급하지 않습니다.비조건 보간 채우기...거친 0*30*0*15* 미세 그림 4: 거친-미세 파이프라인.모델은 한 번에 16개의 프레임을 생성할 수 있습니다.각 프레임 위의 인덱스에 레이블을 지정하고, *가 있는 것은 이전 단계에서 이미 결과가 생성되어 현재 단계에서 모델의 조건부 입력으로 사용되었음을 나타냅니다.우리의 파이프라인에는 채우기와 보간의 하이브리드 전략이 포함됩니다. 왜냐하면 교차 주의가 마스크된 프레임이 글로벌 프레임과 더 철저히 상호 작용하는 데 도움이 될 수 있다고 제안하기 때문입니다. 여기에 전달된 글로벌 프레임은 현재 비디오 클립의 컨텍스트에 맞춰 정렬되고 정보 누출을 방지하기 위해 다른 프레임과 동일한 방식으로 마스크된다는 점에 주목할 가치가 있습니다. 3.2.3 분류자 없는 안내. 분류자 없는 안내[20]는 확산 모델에서 효과적인 것으로 입증되었습니다. 분류자 없는 안내는 암묵적 분류자 po(c|zt)가 조건 c에 높은 확률을 할당하는 조건 생성의 결과를 개선합니다. 우리의 경우 두 가지 조건 입력이 있습니다. 하나는 비디오 c₁의 컨텍스트 정보이고 다른 하나는 글로벌 비디오 클립 c2입니다. 우리는 c₁와 c2를 확률 p₁와 P2로 고정된 null 값 Ø에 무작위로 설정하여 무조건 및 조건 모델을 공동으로 훈련합니다. 추론 시간에 우리는 두 가지 조건부 입력에 대해 Brooks의 [3] 접근 방식을 따르고 조건부 및 무조건부 점수 추정치의 다음과 같은 선형 조합을 사용합니다. ê(Zt, C1, C2) = €(Zt, Ø, Ø) + S1 (€(Zt, C1, Ø) — €(zt, Ø, Ø)) +S2 (€ (zt, C1, C2) - €(Zt, C1, Ø)), 여기서 $1과 s2는 안내 척도입니다. 안내 척도는 생성된 비디오가 비디오의 컨텍스트에 더 많이 의존하는지 아니면 비디오의 글로벌 프레임에 더 많이 의존하는지 여부를 제어합니다. 3.3 비디오 아웃페인팅을 위한 하이브리드 거친-정밀 파이프라인 (4) 비디오 생성 작업에서 긴 비디오를 생성하면 종종 아티팩트가 축적되어 성능이 저하됩니다. 최근 연구[2, 17, 43]에서는 먼저 계층적 구조를 사용하여 비디오의 희소한 주요 프레임을 생성한 다음 채우기 전략을 사용하여 밀집된 비디오 프레임을 채웁니다. 채우기 전략에는 첫 번째 및 마지막 프레임이 다음 레벨 생성을 안내하는 가이드 프레임으로 필요합니다. 그러나 채우기만 사용하면 큰 Ours Dehan Ours SDM Dehan Ours SDM Dehan MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 Fanda Fan et al. 그림 5: 짧은 비디오 아웃페인팅의 정성적 비교. 비율 비율이 0.4, 0.5, 0.6인 수평 방향 비디오 아웃페인팅의 세 그룹의 결과를 제시합니다. 첫 번째 이미지에서 확장할 영역을 빨간색 선으로 표시합니다. 0.0.70― M3DDM 0.69-*- SDM 0.15 20 25 30 35 40 45시간 간격 (a) SSIM 0.0.0.0.-*- SS2 3 4 5유도 스케일 가중치 (b) 그림 6: 다양한 시간 간격과 유도 스케일 가중치 평가. 거친 단계에서 프레임 간 시간 간격. 예를 들어, 그림 4에서 보듯이 채우기 전략만 사용하는 경우 모델은 가장 거친 수준에서 30 대신 225의 프레임 간격이 필요합니다. 문제의 어려움과 학습 세트에 긴 비디오 데이터가 없기 때문에 이로 인해 결과가 나타날 수 있습니다. 나쁨 양방향 학습 덕분에 3D UNet은 채우기와 보간을 결합하여 비디오 아웃페인팅을 수행할 수 있습니다. 이를 통해 거친 생성 단계에서 큰 프레임 간격 문제를 피할 수 있습니다. 그림 4는 우리의 coarse-to-fine 프로세스 다이어그램을 보여줍니다. 우리의 coarse-to-fine 파이프라인은 세 레벨로 나뉩니다. 첫 번째 레벨(coarse)에서 우리는 무조건적으로 첫 번째 비디오 클립을 생성한 다음 이전 반복의 마지막 프레임의 결과에 따라 모든 키 프레임을 반복적으로 생성합니다. 두 번째 레벨(coarse)에서 우리는 첫 번째 레벨에서 생성된 키 프레임을 조건부 입력으로 사용하여 보간을 통해 더 많은 키 프레임을 생성합니다. 세 번째 레벨(fine)에서 우리는 프레임을 사용하여 최종 비디오 아웃페인팅 결과를 생성합니다. Ours SDM Ours SDM Ours SDM 비디오 아웃페인팅을 위한 계층적 마스크 3D 확산 모델 MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 C MIN 그림 7: 긴 비디오 아웃페인팅의 정성적 비교. 우리는 0.6의 비율로 수평 방향 비디오 아웃페인팅의 세 그룹의 결과를 제시합니다. 첫 번째 이미지에서 확장할 영역을 빨간색 선으로 표시합니다. 간격은 1이고, 첫 번째와 마지막 프레임을 밀집 생성을 위한 가이드 프레임으로 사용합니다. 실험 비디오 아웃페인팅을 위한 마스크된 3D 확산 모델의 효과를 검증하기 위해 DAVIS[27], YouTube-VOS[41], 5M E-commerce 데이터 세트의 세 가지 데이터 세트에 대한 평가를 수행합니다. DAVIS와 YouTube-VOS는 비디오 인페인팅 및 아웃페인팅에 일반적으로 사용되는 데이터 세트입니다. 그러나 평균 비디오 길이가 짧습니다. 따라서 긴 비디오에 대한 아웃페인팅 성능을 검증하기 위해 5M E-commerce 데이터 세트라는 전자 상거래 현장에서 긴 비디오를 수집합니다. 5M E-commerce 데이터 세트에는 500만 개가 넘는 비디오가 포함되어 있으며 평균 비디오 길이는 약 20초입니다. 여기에는 광고주가 가구, 가정용품, 전자 제품, 의류, 식품 및 기타 상품을 포함하여 제품을 보여주기 위해 제공한 비디오가 포함됩니다. 부록 C.2에서 구현 세부 정보를 설명합니다. 4. 기준선 및 평가 지표 우리는 다음 방법과 비교합니다.1) Dehan [6]은 비디오 아웃페인팅을 위한 프레임워크를 제안했습니다.그들은 전경과 배경을 분리하고 완전한 결과로 통합하기 전에 흐름 추정과 배경 추정을 별도로 수행했습니다.2) 우리는 또한 안정된 확산 [29]을 기반으로 하는 단순 확산 모델(SDM)을 기준선으로 훈련합니다.이것은 마스크 모델링을 사용하지 않고 입력 계층의 컨텍스트 비디오 클립과 연결된 조건 프레임으로 첫 번째 프레임과 마지막 프레임을 채택하고 노이즈 제거 3D UNet에 공급합니다.한편, 우리는 전역적 특징을 프롬프트로 사용하지 않으며 교차 주의는 제거됩니다.3) MAGVIT [15]은 마스크 모델링 기술을 사용하여 3D 벡터 양자화 [11, 36] 공간에서 비디오를 생성하기 위한 변환기 [9]를 훈련했습니다. 우리는 부록 B에 이 비교 세트를 포함했습니다.우리는 [6]을 따르고 일반적으로 사용되는 다섯 가지 평가 메트릭을 사용합니다: 평균 제곱 오차(MSE), 피크 신호 대 잡음비(PSNR), 구조적 유사성 지수 측정(SSIM) [39], 학습된 지각적 이미지 패치 유사성(LPIPS) [45], 및 Frechet 비디오 거리(FVD) [35].MSE, PSNR, SSIM 및 FVD를 평가하기 위해 생성된 결과를 값 범위 [0, 1]의 비디오 프레임으로 변환하는 반면 LPIPS는 [-1, 1]의 값 범위를 사용하여 평가됩니다.FVD 평가 메트릭의 경우 평가를 위해 비디오당 16개 프레임의 균일한 샘플링을 사용합니다.4.2 짧은 비디오 아웃페인팅 4.2.1 정성적 비교.그림 5에서 수평 비디오 아웃페인팅을 위한 세 가지 방법의 결과를 제시합니다. Dehan [6]은 더 나은 배경을 생성할 수 있지만 흐름 예측 결과에 의존하기 때문에 전경 결과가 좋지 않은 것을 알 수 있습니다. 채우기 영역에 있는 피사체의 구조적 정보는 본질적으로 손실되어 비합리적인 결과가 발생합니다. 강력한 확산 도구와 가이드 프레임 추가의 도움으로 SDM은 짧은 간격 내에 채우기 영역의 공간 구조를 보존할 수 있습니다. 그러나 전역 정보가 부족하기 때문에 전체 비디오를 생성하는 데 많은 합리적인 예측도 손실됩니다. 2023년 10월 29일~11월 3일, 캐나다 온타리오주 오타와에서 실시한 MM &#39;23의 세 번째 결과 그룹 표 1: DAVIS 및 YouTube-VOS 데이터 세트에서 비디오 아웃페인팅의 정량적 평가. 방법 Dehan [6] SDM [29] Ours MSE 0.0.0.Davis 데이터 세트 [27] PSNR ↑ SSIM↑ LPIPS ↓ FVD↓ MSE↓ 17.96 0.6272 0.2331 363.1 0.02312 18.25 0.7195 0.2278 149.20.02 0.7078 0.2165 334.6 0.01687 19.91 0.7277 0.2001 94.20.26 0.7082 0.2026 300.0 0.01636 20.20 0.7312 0.1854 66.YouTube-VOS 데이터세트 [41] PSNR ↑ SSIM↑ LPIPS↓ FVD↓ Fanda Fan et al. 그림 5에서 마스크 비율이 0.6인 경우 SDM은 약간의 노이즈가 있는 결과와 함께 나쁜 케이스를 생성합니다. 마스크 모델링을 도입하면 확산 모델에서 생성된 나쁜 케이스의 비율을 완화할 수 있음을 발견했습니다. 이에 대해서는 절제 연구에서 더 자세히 논의할 것입니다. 방법에서 볼 수 있듯이 채우기 영역에서 전경 피사체의 공간 정보를 보존할 뿐만 아니라 적절한 배경도 생성합니다. 전역 비디오 정보를 도입한 덕분에 이 방법은 오토바이가 초기 단계에서 세 번째 그룹 3의 채우기 영역에 나타나야 한다는 것을 인식할 수 있습니다. 게다가 SDM과 비교할 때 추가 마스크 모델링은 더 적은 나쁜 케이스를 생성할 수 있습니다. 4.2.2 정량적 결과. 마스크 비율 0.25와 0.666을 사용하여 Dehan [6] 및 SDM과 함께 데이터 세트 DAVIS 및 YouTube-VOS에서 수평 방향으로 아웃페인팅 결과를 비교합니다. 각 평가 지표에 대해 모든 테스트 샘플에서 평균값을 보고합니다. DAVIS 및 YouTube-VOS 데이터 세트에 대한 평가 결과는 표 1에 나와 있습니다. 4.3 긴 비디오 아웃페인팅 그림 2에서 긴 비디오에 대한 고밀도 예측과 CTF(Coarse-to-fine) 예측을 비교합니다. 고밀도 예측은 비디오의 초기 예측에서 비합리적인 결과를 생성할 뿐만 아니라 이전 반복 작업의 아티팩트가 축적되는 문제가 있음을 알 수 있습니다. CTF 예측 방법은 더 긴 비디오 클립 정보를 고려하여 초기 예측에서 더 합리적인 결과를 생성할 수 있으며, 자기 회귀 추론 시간이 감소하여 아티팩트 축적 문제를 완화할 수 있다고 주장합니다. 4.3.1 프레임 간 시간 간격 연구. 고밀도 단계에서 생성된 프레임 간격과 그림 6a의 결과 간의 관계를 탐구합니다. 5M 전자 상거래 데이터 세트에서 테스트 세트로 100개의 긴 비디오를 무작위로 선택합니다. 간격 15는 2단계 예측 구조를 의미하고 15보다 크면 3단계 구조를 의미합니다. 우리는 3단계 구조에 의해 생성된 결과가 2단계 구조에 의해 생성된 결과보다 더 나은 것을 발견했습니다.그러나 3단계에서 프레임 간 간격을 더 늘리면 M3DDM 및 SDM 모델에서 성능이 저하되었습니다.특히 채우기 전략만 사용하는 경우 225의 프레임 간격은 SDM과 M3DDM 모두에서 더 큰 저하를 초래했습니다.SDM은 첫 번째와 마지막 프레임을 가이드 프레임으로 사용하기 때문에 3단계에서 225의 시간 간격만 사용할 수 있다는 점에 주목할 가치가 있습니다.정성적 비교를 위해 5M 전자 상거래 데이터 세트의 3개 긴 비디오에서 SDM과 접근 방식을 대조합니다.여기서 SDM은 각각 [15, 1]의 시간 간격을 갖는 2단계 CTF를 채택합니다.그림 7에서 볼 수 있듯이 M3DDM은 전경 피사체를 생성할 뿐만 아니라 표 2: 전자 상거래 데이터 세트에 대한 소거 연구.&#39;w/o&#39;는 없음을 의미합니다. 방법 SDM MSE PSNR ↑ SSIM ↑ LPIPS ↓ FVD ↓ 0.01134 17.92 0.6783 0.2139 110.MSDM w/o prompt 0.00914 19.22 0.6912 0.2012 70.우리의 0.00791 20.01 0.7112 0.1931 68.채워야 할 영역에서 잘 작동하지만 더 일관된 배경 결과도 생성합니다.4.절제 연구 5M 전자상거래 데이터 세트에 대한 절제 연구를 수행합니다.5M 전자상거래 데이터 세트에서 평균 길이가 20초인 400개 비디오를 무작위로 선택합니다.단순 확산 모델(SDM)에서 마스크 모델링과 글로벌 프레임을 통합하지 않고 비디오 클립의 컨텍스트와 연결된 첫 번째 및 마지막 가이드 프레임만 사용하여 학습합니다. 확산 모델에서 마스크 모델링의 개선 효과를 독립적으로 검증하기 위해 SDM을 채택하고 마스크 모델링과 결합하여(3.2.1절에서 언급했듯이) 마스크된 SDM(MSDM)을 학습시켰습니다. 우리의 접근 방식은 마스크된 SDM을 기반으로 글로벌 비디오 클립을 프롬프트로 도입하는 것입니다. 긴 비디오 추론에서 SDM에 2단계의 대략-정밀 추론 구조를 사용하고(3단계는 성능이 저하됨) 마스크된 SDM과 우리의 접근 방식에서 3단계의 대략-정밀 추론 파이프라인을 사용합니다. 표 2에서 볼 수 있듯이 짧은 비디오와 비교했을 때 우리의 접근 방식과 SDM은 긴 비디오에서 성능 격차가 더 큽니다. SDM과 비교했을 때 MSDM은 더 나은 비디오 아웃페인팅 결과를 생성했습니다. 4.4.1 안내 척도의 효과. 그림 6b에서 안내 척도의 효과를 보여줍니다. s₁을 변경하면 s2를 4로 고정합니다. s2를 변경하면 s₁을 2로 고정합니다. s₁은 비디오 컨텍스트와 더 관련성이 높은 결과를 생성하도록 모델을 제어하고, s2는 카메라가 움직이거나 전경 피사체가 움직이는 장면에서 모델이 더 합리적인 결과를 생성하도록 돕습니다. 비디오 컨텍스트에 대한 분류자 없는 안내가 더 중요하다는 것을 발견했습니다. 비디오 컨텍스트에 대한 분류자 없는 안내가 없으면 성능이 크게 저하됩니다. 동시에 비디오 컨텍스트와 글로벌 프레임에 대한 분류자 없는 안내가 있으면 더 나은 결과를 얻을 수 있습니다.
--- CONCLUSION ---
이 논문에서 우리는 비디오 아웃페인팅을 위한 마스크 모델링을 기반으로 한 3D 확산 모델을 제안한다. 우리는 양방향 학습과 비디오 프레임을 비디오 아웃페인팅 컨텍스트를 위한 계층적 마스크 3D 확산 모델과 함께 교차 주의에 대한 프롬프트로 전역적으로 인코딩한다. 마스크 모델링의 양방향 학습 접근 방식을 사용하면 인접 프레임 정보를 더 잘 인식하는 동시에 추론 단계에서 더 유연한 전략을 사용할 수 있다. 프롬프트로 전역 비디오 클립을 추가하면 방법의 성능이 더욱 향상된다. 대부분의 카메라 움직임과 전경 객체 슬라이딩의 경우 전역 프레임은 모델이 영역을 채우는 데 더 합리적인 결과를 생성하는 데 도움이 된다. 우리는 또한 채우기와 보간 전략을 결합하는 비디오 아웃페인팅을 위한 하이브리드 거친-미세 추론 파이프라인을 제안한다. 실험 결과 우리의 방법은 최첨단 결과를 달성한다. 참고문헌 [1] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. IEEE 국제 컴퓨터 비전 컨퍼런스에서. [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis. 2023. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)에서. [3] Tim Brooks, Aleksander Holynski, Alexei A. Efros. 2023. InstructPix2Pix: 이미지 편집 지침을 따르는 법. CVPR에서. [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T Freeman. 2022. Maskgit: Masked generative image transformer. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록에서. 11315-11325. [5] Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and Ming-Hsuan Yang. 2022. InOut: 다양한 이미지 아웃페인팅(GAN inversion). IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 11431-11440. [6] Loïc Dehan, Wiebe Van Ranst, Patrick Vandewalle, and Toon Goedemé. 2022. 완전하고 시간적으로 일관된 비디오 아웃페인팅(Complete and temporally consistency). IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 687-695. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805(2018). [8] Prafulla Dhariwal 및 Alexander Nichol. 2021. 확산 모델은 이미지 합성에서 gans를 이겼습니다. 신경 정보 처리 시스템의 발전 34(2021), 8780-8794. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929(2020). [10] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, Thomas Brox. 2015. Flownet: 합성 신경망을 사용한 광학 흐름 학습. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 2758-2766. [11] Patrick Esser, Robin Rombach, Bjorn Ommer. 2021. 고해상도 이미지 합성을 위한 변압기 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 12873-12883. [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 2020. 생성적 적대 네트워크. Commun. ACM 63, 11 (2020), 139-144. [13] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. 2017. 시각적 상식을 학습하고 평가하기 위한 &quot;something something&quot; 비디오 데이터베이스. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집. 5842-5850. [14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo. 2022. 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중.10696-10706. [15] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei. 2022. Maskvit: 비디오 예측을 위한 마스크 처리된 시각적 사전 학습.arXiv 사전 인쇄본 arXiv:2206.11894(2022). [16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. 2022. 마스크 처리된 자동 인코더는 확장 가능한 비전 학습기입니다.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중.16000-16009. [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan 및 Qifeng Chen. 2022. 임의 길이를 사용한 고충실도 비디오 생성을 위한 잠재 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13221(2022). [18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022. Imagen 비디오: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303(2022). MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 [19] Jonathan Ho, Ajay Jain 및 Pieter Abbeel. 2020. 확산 확률적 모델의 노이즈 제거. 신경 정보 처리 시스템의 발전 33(2020), 6840–6851. [20] Jonathan Ho 및 Tim Salimans. 2022. 분류자 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598(2022). [21] Diederik P Kingma 및 Jimmy Ba. 2014. Adam: 확률적 최적화를 위한 방법. arXiv 사전 인쇄본 arXiv:1412.6980(2014). [22] Han Lin, Maurice Pagnucco 및 Yang Song. 2021. Edge가 진행적으로 생성하는 이미지 아웃페인팅을 안내. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 806-815. [23] Luping Liu, Yi Ren, Zhijie Lin 및 Zhou Zhao. 2022. 매니폴드의 확산 모델을 위한 가상 수치 방법. 학습 표현에 관한 국제 컨퍼런스에서. https://openreview.net/forum?id=PIKWVd2yBkY [24] Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet 및 Roland Memisevic. 2018. 전이 학습을 위한 작업 세분성의 효과에 관하여. arXiv 사전 인쇄본 arXiv:1804.09235(2018). [25] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan 및 Yedid Hoshen. 2023. Dreamix: 비디오 확산 모델은 일반적인 비디오 편집기입니다. arXiv 사전 인쇄본 arXiv:2302.01329 (2023). [26] Alexander Quinn Nichol 및 Prafulla Dhariwal. 2021. 개선된 노이즈 제거 확산 확률적 모델. 기계 학습 국제 컨퍼런스에서. PMLR, 8162-8171. [27] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross 및 Alexander Sorkine-Hornung. 2016. 비디오 객체 분할을 위한 벤치마크 데이터 세트 및 평가 방법론. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 724-732. [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu 및 Mark Chen. 2022. 클립 잠재 데이터를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125 (2022). [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10684-10695. [30] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, Mohammad Norouzi. 2022. 팔레트: 이미지 간 확산 모델. ACM SIGGRAPH 2022 컨퍼런스 회의록. 1-10. [31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.(2022). [32] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 2015. 비평형 열역학을 사용한 심층 비지도 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 2256-2265. [33] Shiqi Sun, Shancheng Fang, Qian He, Wei Liu. 2023. Design Booster: 공간 레이아웃 보존을 통한 이미지 변환을 위한 텍스트 가이드 확산 모델. arXiv 사전 인쇄본 arXiv:2302.02284 (2023). [34] Zachary Teed 및 Jia Deng. 2020. Raft: 광학 흐름을 위한 반복적인 모든 쌍 필드 변환. Computer Vision-ECCV 2020: 제16회 유럽 컨퍼런스, 영국 글래스고, 2020년 8월 23-28일, 회의록, 2부 16. Springer, 402-419. [35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski 및 Sylvain Gelly. 2018. 비디오의 정확한 생성 모델을 향하여: 새로운 지표 및 과제. arXiv 사전 인쇄본 arXiv:1812.01717 (2018). [36] Aaron Van Den Oord, Oriol Vinyals 등. 2017. 신경 이산 표현 학습. 신경 정보 처리 시스템의 발전 30(2017). [37] Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal. 2022. 예측, 생성 및 보간을 위한 마스크 조건부 비디오 확산. arXiv 사전 인쇄본 arXiv:2205.09853(2022). [38] Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang. 2021. 스케치 가이드 풍경 이미지 아웃페인팅. IEEE 이미지 처리 저널(2021), 2643-2655. [39] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli. 2004. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. 이미지 처리에 대한 IEEE 트랜잭션 13, 4(2004), 600-612. [40] Chen Wei, Karttkeyya Anglam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille 및 Christoph Feichtenhofer. 2023. 마스크된 자동 인코더로서의 확산 모델. arXiv 사전 인쇄 arXiv:2304.(2023). [41] N Xu, L Yang, Y Fan, D Yue, Y Liang, J Yang 및 T YouTube-VOS Huang. 2018. 대규모 비디오 객체 분할 벤치마크. arXiv 사전 인쇄(2018). [42] Chiao-An Yang, Cheng-Yo Tan, Wan-Cyuan Fan, Cheng-Fu Yang, Meng-Lin Wu 및 Yu-Chiang Frank Wang. 2022. 의미 기반 이미지 아웃페인팅을 위한 장면 그래프 확장. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의 진행 중. 15617-15626. [43] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang 등. 2023. NUWAXL: 매우 긴 비디오 생성을 위한 확산보다 확산. arXiv 사전 인쇄 arXiv:2303.12346 (2023). [44] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2022. MAGVIT: Masked Generative Video Transformer. arXiv 사전 인쇄본 arXiv:2212.MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 Fanda Fan et al. (2022). [45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 2018. 지각적 지표로서 딥 피처의 비합리적인 효과성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 586-595. 비디오 아웃페인팅을 위한 계층적 마스크 3D 확산 모델 표 3: something-something-v2에서 FVD를 사용하여 비디오 아웃페인팅의 성능을 평가합니다. MAGVIT에서 직접 결과를 얻습니다. 방법 AVG OPC OPV OPH MAGVIT-L-MT [44] 18.3 21.1 16.8 17. 우리의 16.0 19.2 14.5 14.A 부록 개요 보충 자료에서는 추가적인 실험 결과와 비교 방법을 제공하여 접근 방식을 더 잘 평가합니다. 동시에 공간 제한으로 인해 본문에서 확장되지 않은 구현 세부 정보도 보완합니다. 보충 자료는 다음 섹션에 설명되어 있습니다. • Something-Something V.2(SSv2) 데이터 집합에서 MAGVIT과 비교. 또한 MAGVIT [44]와 비교 실험을 수행합니다. 논문에서 정량적 결과를 직접 얻고 SSv2 데이터 집합에서 동일한 설정을 사용하여 비교합니다. B • 네트워크 아키텍처 및 구현 세부 정보. • 제한 사항. 우리 방법으로 생성된 몇 가지 나쁜 사례를 간략히 제시했습니다. MAGVIT과 비교 본문의 서론에서 MAGVIT [44]을 간략하게 소개했습니다. 그들은 마스크 모델링 기술을 사용하여 3D 벡터 양자화 [11, 36] 공간에서 비디오 생성을 위한 변환기 [9]를 훈련했습니다. 그들은 또한 논문에서 비디오 아웃페인팅 작업에서 MAGVIT의 성능을 평가했습니다. 그러나 MAGVIT은 동일한 비디오의 다른 클립에 대한 제약 조건이 없어 다른 클립 간에 생성된 결과에서 시간적 일관성이 좋지 않습니다. 확산 모델을 활용하고 마스크 모델링 및 가이드 프레임 기술과 함께 글로벌 비디오 프레임을 프롬프트로 도입하는 M3DDM 모델은 긴 비디오를 생성하는 데 좋은 성능을 보일 뿐만 아니라 짧은 비디오 아웃페인팅에서도 MAGVIT [44]을 능가합니다. MAGVIT [44]와 비교하기 위해 논문에서 직접 평가 결과를 얻었습니다. 그들은 Something-Something V.2(SSv2) [13, 24] 데이터 세트에서 세 가지 유형의 비디오 아웃페인팅 FVD [35] 점수를 평가했습니다. 세 가지 유형의 아웃페인팅은 Central Outpainting(OPC), Vertical Outpainting(OPV), Horizontal Outpainting(OPH)입니다. 각 유형의 마스크 비율은 OPC의 경우 0.75, OPV의 경우 0.5, OPH의 경우 0.5입니다. 우리는 SSvdataset에서 169K 비디오를 훈련에 사용하고 24K 비디오를 평가에 사용하여 이들의 설정을 엄격히 따릅니다. 우리는 24개의 A100 GPU를 사용하여 데이터 세트를 훈련하고, 배치 크기는 240이며 126k 단계로 미세 조정합니다. SSv2의 평균 비디오 길이는 약 30프레임이고, 우리는 주요 논문에서 보고한 짧은 비디오 아웃페인팅의 설정을 따르는 dense 예측을 사용합니다. 우리는 각 비디오에 대해 16프레임을 사용하여 그들과 동일한 FVD[35] 평가 메트릭을 사용합니다. 각 평가된 비디오는 2개의 시간 창과 128의 프레임 크기를 가진 중앙 크롭으로 샘플링됩니다. 비교 결과는 표 3에 나와 있습니다. 또한 그림 8에서 세 가지 유형의 비디오 아웃페인팅에 대한 정성적 결과를 제시합니다. MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 C 네트워크 아키텍처 및 C. 구현 세부 정보 네트워크 아키텍처 우리의 접근 방식은 3D 노이즈 제거 Unet과 경량 비디오 인코더의 두 가지 학습 가능한 네트워크로 구성됩니다. 우리의 3D 노이즈 제거 UNet은 LDM의 텍스트-이미지 모델에서 사전 학습된 매개변수를 사용합니다. 3D 구조의 작업에 맞게 조정하기 위해 시간적 합성, 셀프 어텐션 및 교차 어텐션 작업을 사용하여 서로 다른 프레임 간의 상호 작용을 보장합니다. 우리의 3D 노이즈 제거 Unet은 (batch_size, num_frames_of_video, in_channels, height, weight) 차원을 가진 VAE 인코더[29]에서 잠재 데이터를 입력으로 받습니다. 우리의 3D 노이즈 제거 Unet은 (batch_size, num_frames_of_video, out_channels, height, weight) 모양의 노이즈를 예측합니다. 우리의 구현에서 in_channels는 9이고, 여기서 8차원은 원본 비디오 프레임과 마스크된 프레임(각각 4차원)의 잠재 데이터를 나타내고, 1차원은 마스크를 나타냅니다. out_channels는 4로 원본 비디오 프레임의 잠재 데이터와 동일합니다. VAE로 압축한 후, 높이와 가중치의 차원은 32가 됩니다. 우리의 3D 노이즈 제거 UNet은 Make-A-Video[31]의 네트워크 구조를 많이 참조합니다. 우리는 잠재 확산 모델(LDM) [29] 내에서 사전 훈련된 텍스트-이미지 모델을 활용하기 위해 Pseudo-3D 합성곱 및 어텐션 계층을 활용하여 Make-A-Video [31]를 따릅니다. 각 공간 2D 합성곱 계층 뒤에 시간 1D 합성곱 계층이 옵니다. 우리는 각 계층에 노이즈의 타임스텝 임베딩을 추가할 뿐만 아니라 fps 속도 임베딩도 추가합니다. 이를 통해 하나의 모델을 사용하여 다른 프레임 간격으로 비디오 클립을 생성할 수 있습니다. 우리의 3D 노이즈 제거 Unet에는 4개의 다운샘플링 계층과 4개의 업샘플링 계층이 있으며, 각 계층은 다음과 같은 수의 채널을 출력합니다: [320, 640, 1280, 1280]. 우리의 3D Unet에는 총 1299.28M개의 매개변수가 있습니다. 자세한 내용은 Make-A-Video [31]의 네트워크 아키텍처를 참조하는 것이 좋습니다. 그림 9에 경량 비디오 인코더를 나타냈습니다. 경량 비디오 인코더는 VAE에서 얻은 전역 비디오 잠복을 허용하고 교차 주의를 위해 차원을 4에서 320으로 늘립니다. C.2 구현 세부 사항 샘플링 세부 사항. 확산 모델(PNDM)에 대한 의사 수치 방법의 PNDMScheduler를 사용합니다[23]. 추론 단계와 0에서 시작하여 0.012에서 끝나는 확장된 선형 ß 일정을 사용합니다. 3D 잡음 제거 UNet은 단일 추론에서 F = 16 프레임을 생성할 수 있으며 g = 16 전역 프레임을 사용합니다. 각 프레임 간에 동일한 간격으로 비디오 클립에서 F 프레임을 무작위로 추출합니다. 프레임 간격은 fps에서 균일하게 샘플링됩니다[1,30]. 학습률이 1e-4인 Adam[21] 최적화기를 사용하고 워밍업 학습률 단계는 1k입니다. 우리는 WebVid 데이터 세트[1]에서 4개 에포크 동안 모델을 훈련한 다음 5M 전자상거래 데이터 세트에서 4개 에포크 동안 미세 조정했습니다.모든 훈련은 A100 GPU에서 수행되었으며 전체 훈련 프로세스는 약 2주가 걸렸습니다.우리는 짧은 비디오 아웃페인팅에 밀집 예측 형태를 사용하고 긴 비디오 아웃페인팅에 [30, 15, 1]의 시간 간격을 갖는 3단계 조대-미세 구조를 사용합니다.우리는 [15, 5, 1]의 프레임 간격을 갖는 추론 방법이 거의 똑같이 효과적이라는 것을 발견했습니다.그러나 긴 비디오의 길이를 고려하여 [30, 15, 1]의 프레임 간격을 갖는 추론 방법을 선택했습니다.Ours GT Ours GT Ours GT Ours GT Ours GT MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 Fanda Fan et al.그림 8: SSv2 데이터 세트에서 세 가지 유형의 비디오 아웃페인팅. GT라는 용어는 기준 진실을 나타내며, 각 GT 세트의 경우 채워야 할 영역은 첫 번째 이미지에 빨간색 곡선으로 표시됩니다(빨간색 선 밖의 영역이 채우려는 영역입니다).실험 결과 이렇게 하면 좋은 아웃페인팅 결과가 나오기 때문에 s₁ = 2, s2 = 4로 설정했습니다.입력 비디오의 해상도는 256 x 256 x 3입니다.테스트 단계에서 16GB 그래픽 카드에서 배치 크기가 2인 테스트 샘플을 추론할 수 있습니다(사용하는 테스트 환경은 Tesla v100 16Gb).학습 단계에서는 80GB A100 GPU 24개를 사용하여 총 배치 크기가 240이었습니다.D 제한 사항 및 나쁜 사례 모델에서 생성된 나쁜 사례를 그림 10에 표시합니다.이 방법은 고정 이미지 VAE[29] 인코더를 사용하여 픽셀 공간 비디오를 잠재 공간으로 변환합니다.VAE는 종종 인간 얼굴과 일부 미세 구조에서 거친 성능을 보입니다. 또한, 우리의 방법은 훈련 데이터와 문제의 어려움에 의해 제한되어 비디오 내에서 텍스트를 생성하는 데 있어 결과가 좋지 않습니다. 우리의 확산 모델은 샘플링하는 동안 초기 가우시안 노이즈에 민감하며, 일부 비디오는 에지 흐림 현상이 발생할 수 있습니다. 우리는 OpenCV inpaint 함수를 사용하여 예측할 비디오의 확장된 영역에 간단한 전처리 단계를 수행했고, 비디오 아웃페인팅 입력을 위한 계층적 마스크 3D 확산 모델: [g, 4, 32, 32] ↓ Res_Stage out_channel =stride =MM &#39;23, 2023년 10월 29일-11월 3일, 캐나다 온타리오주 오타와 와우.와우. &quot;와.와. 시간 +00:00:46:ime +00:00:47:ime +00:00:48:**시간+00:00:49:**시간+00:00:49:Res_Stage out_channel=stride =Res_Stage out_channel =stride =Res_Stage stride =out_channel =Res_Stage stride =out_channel=Conv2d kernel =stride =출력: [g, 320, 14, 14] 그림 9: 가벼운 비디오 인코더. g는 입력된 글로벌 비디오 프레임의 총 수를 나타냅니다. 우리 구현에서 g = 16입니다. 우리는 designbooster [33]의 이미지 인코더를 참조했습니다. 그림 10: 우리 방법의 나쁜 경우. 가우시안 노이즈에서 예측 견고성 문제를 부분적으로 해결합니다.
