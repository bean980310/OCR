--- INTRODUCTION ---
개인화된 아바타 생성-얼굴 특징을 애니메이션, 사용자 지정 및 렌더링할 수 있는 3D 가상 복제본에 매핑하는 기능은 영화, 메타버스 및 원격 존재에 큰 희망을 안겨주는 새로운 기술입니다. 이 분야의 발전은 세부 사항과 애니메이션에서 더욱 진실성이 있는 디지털 트윈으로 이어질 수 있습니다.SIGGRAPH 컨퍼런스 회의록, 2023년 8월 6-10일 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein 및 Sameh Khamis는 다운스트림 애플리케이션 및 파이프라인에 더 쉽게 통합됩니다. 단일 샷 개인화된 아바타 생성을 통해 보다 특수화된 캡처 설정 또는 절차가 필요한 방법보다 더 편리하고 유연하게 개별 RGB 이미지에서 얼굴 아바타를 재구성할 수 있습니다. 애니메이션이 가능한 3D 아바타 생성에 대한 기존 접근 방식은 종종 모양과 외관 변화를 저차원 얼굴 표현으로 풀어내는 3D 변형 가능 모델(3DMM)[Blanz 및 Vetter 1999]을 기반으로 합니다.이를 기반으로 최근의 접근 방식은 명시적(텍스처가 있는) 템플릿 메시[Daněček 등 2022; Feng 등 2021; Grassal 등 2022; Khakhulin 등 2022; Li 등 2017; Tran 및 Liu 2019] 또는 신경 암묵적 표현[Mildenhall 등 2021; Park 등 2019; Sitzmann 등 2019]을 활용하는 경우가 많습니다.템플릿 기반 접근 방식은 자산 추출과 직관적 편집을 쉽게 할 수 있지만 고품질 지오메트리와 텍스처를 포착하지 못하는 경우가 많습니다.새로운 암묵적 얼굴 모델은 머리카락과 같은 더 복잡한 기하학적 특징을 모델링하여 더욱 사실적인 표현을 얻을 수 있습니다[Cao 등 2022b; Giebenhain et al. 2022; Zheng et al. 2022a]. 그러나 암묵적 얼굴 표현은 종종 해석 가능성을 손상시키고 제어하기 쉽지 않습니다. 이러한 고도로 매개변수화된 모델에서 학습한 얽힌 잠재 공간은 편집하기 어렵습니다. 저희의 접근 방식은 템플릿 기반 3DMM의 해석 가능성 및 편집 가능성 이점을 암묵적 3D 표현의 품질 및 토폴로지 유연성과 결합하는 것을 목표로 합니다. 중요한 점은 모양과 지오메트리를 네트워크 아키텍처의 두 가지 분기로 분리한다는 것입니다. UV 매개변수화 네트워크를 통합하여 연속적이고 일관된 텍스처 맵을 학습함으로써 아바타를 텍스처 메시로 내보내 기존 그래픽 파이프라인에서 텍스처 맵 편집 및 재조명과 같은 다운스트림 애플리케이션을 지원할 수 있습니다(그림 1 참조). 반면, 지오메트리를 암묵적 부호 거리 필드(SDF)로 표현함으로써 얼굴 모양은 메시 기반 접근 방식에 비해 해상도와 토폴로지에 의해 덜 제한됩니다. 저희는 제안한 하이브리드 표현이 얼굴의 지오메트리, 모양 및 표정 공간을 효과적으로 포착한다는 것을 보여줍니다. 우리는 제안된 표현을 기반으로 단일 샷 야생 초상화 이미지를 아바타에 효과적으로 매핑할 수 있으며 이러한 아바타가 사진적 사실성, 기하학 및 단안 표현 전송에서 이전의 최첨단 기술을 개선한다는 것을 보여줍니다.또한 얼굴 기하학 및 모양 속성과 같은 직접적인 텍스처 편집 및 얽힘 해제된 속성 편집을 가능하게 하는 강력한 기능을 보여줍니다.요약하면, 우리 작업의 기여는 다음과 같습니다.· 암묵적 표현의 고품질 기하학 및 유연한 토폴로지와 명시적 UV 텍스처 맵의 편집 가능성을 결합한 하이브리드 변형 가능 얼굴 모델을 제안합니다.· 단일 야생 RGB 이미지를 암묵적 3D 변형 가능 모델 표현에 매핑하기 위한 단일 샷 반전 프레임워크를 제시합니다.반전된 아바타는 새로운 뷰 렌더링, 비선형 얼굴 재생, 얽힘 해제된 모양 및 모양 제어, 직접적인 텍스처 맵 편집 및 다운스트림 애플리케이션을 위한 텍스처 메시 추출을 지원합니다. • 단일 뷰 재구성 설정에서 사실적인 렌더링, 기하학 및 표현 정확도에 대한 최첨단 재구성 정확도를 보여줍니다.표 1. 최근 이전 작업과의 비교.저희가 아는 한, 저희 방법은 유연한 토폴로지와 명시적 텍스처 맵 제어를 지원하면서 단일 이미지 입력을 일반화하는 최초의 암묵적 3D 얼굴 모델입니다.일반화 가능한 단일 이미지 암묵적 EMOCA [2022] ROME [2022] 신경 매개 변수 머리 모델 [2022] IM-Avatar [2022a] 신경 머리 아바타 [2022] 전화 스캔에서 얻은 체적 아바타 [2022b] HeadNeRF [2022] Ours 2
--- RELATED WORK ---
2. 명시적 표현 텍스처 제어 X ✓ ✓ XX xx X xx 메시 기반 3D 변형 가능 모델 x ✓ Blanz와 Vetter의 선구적 연구는 200개의 얼굴 스캔에서 주성분 분석(PCA)을 통해 계산된 선형 부분 공간을 사용하여 템플릿 메시에서 얼굴 모양과 텍스처를 모델링하는 선형 3D 변형 가능 모델(3DMM)[Blanz와 Vetter 1999]을 제안했습니다. 이 저차원 얼굴 모양과 텍스처 공간은 3DMM을 얼굴 애니메이션을 견고하게 캡처하고 단안 설정에서 3D 얼굴을 재구성하는 데 적합하게 만듭니다. 사진에서 모양, 텍스처 및 조명을 재구성하기 위해 이전 작업에서는 얼굴 랜드마크 및 픽셀 색상과 같은 제약 조건을 사용하여 연속 최적화를 채택했습니다[Cao et al. 2014, 2016; Garrido et al. 2013, 2016; Ichim et al. 2015; Li et al. 2017; Romdhani 및 Vetter 2005; Shi 등 2014; Thies 등 2016] 및 더 최근에는 딥 러닝 기반 추론 [BR 등 2021; Daněček 등 2022; Deng 등 2019b; Dib 등 2021a,b; Dou 등 2017; Feng 등 2021; Genova 등 2018; Luo 등 2021; Tewari 등 2019; Tewari 등 2017; Tuan Tran 등 2017; Wu 등 2019]. 3DMM에 의존하는 접근 방식은 견고한 경향이 있지만 모델의 선형성과 낮은 차원으로 인해 고충실도 지오메트리 및 텍스처 세부 정보를 재구성하는 데 효과적이지 않습니다. 기타 다양한
--- METHOD ---
암묵적 기하 표현과 명시적 텍스처 맵을 결합하여 고품질 편집 가능 3D 디지털 아바타(열 2 및 3)를 재구성합니다. 제안된 접근 방식은 자연스럽게 큰 포즈 변화, 표현적이고 비선형적인 얼굴 애니메이션 공간(열 4~6), 텍스처 맵 편집에 대한 직접 사용자 액세스(열 7), 재조명과 같은 추가 다운스트림 애플리케이션을 위한 3D 자산 추출(열 8)에서 새로운 뷰 합성을 지원합니다. 원본 이미지는 COD Newsroom/flickr(위) 및 Malcolm Slaney/flickr(아래)에서 제공되었습니다. 애니메이션화 및 사용자 정의가 가능한 고품질 3D 아바타를 접근하기 쉽게 만드는 데 대한 수요가 증가하고 있습니다. 3D 모핑 가능 모델은 편집 및 애니메이션에 대한 직관적인 제어와 단일 뷰 얼굴 재구성에 대한 견고성을 제공하지만 기하학적 및 외관 세부 사항을 쉽게 포착할 수 없습니다. 영어: SDF(signed distance function) 또는 신경 복사 필드와 같은 신경 암묵적 표현을 기반으로 하는 방법은 사실적인 사진에 접근하지만 애니메이션을 적용하기 어렵고 보이지 않는 데이터로 일반화되지 않습니다. 이 문제를 해결하기 위해 일반화 가능하고 편집하기 직관적인 암묵적 3D 변형 가능 얼굴 모델을 구성하는 새로운 방법을 제안합니다. 고품질 3D 스캔 모음에서 학습한 얼굴 모델은 학습된 SDF와 명시적 UV 텍스처 매개변수화를 사용하여 기하학, 표현 및 텍스처 잠재 코드로 매개변수화됩니다. 학습이 완료되면 학습된 사전을 활용하여 이미지를 모델의 잠재 공간에 투사하여 단일 야생 이미지에서 아바타를 재구성할 수 있습니다. 암묵적 변형 가능 얼굴 모델을 사용하여 *NVIDIA에서 인턴십 기간 동안 수행한 작업. 이 작업의 일부 또는 전체를 개인 또는 교실에서 사용하기 위해 디지털 또는 하드 카피로 만드는 것은 무료이며, 사본이 수익 또는 상업적 이점을 위해 만들어지거나 배포되지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문이 있어야 합니다. 이 작업의 타사 구성 요소에 대한 저작권은 존중되어야 합니다. 다른 모든 용도에 대해서는 소유자/저자에게 문의하십시오. SIGGRAPH Conference Proceedings, Aug 6-10,© 2023 저작권은 소유자/저자에게 있습니다. ACM ISBN 979-8-4007-0159-7/23/08. https://doi.org/10.1145/3588432.avatar 새로운 관점에서, 표정 코드를 수정하여 얼굴 표정을 애니메이션화하고, 학습된 UV 텍스처 맵에 직접 페인팅하여 텍스처를 편집합니다. 우리는 우리의 방법이 최첨단 방법에 비해 사진적 사실성, 기하학 및 표현 정확도를 향상시킨다는 것을 정량적, 질적으로 입증합니다. 모델링/기하학. CCS 개념: 컴퓨팅 방법론 → 추가 키워드 및 구문: 신경 아바타, 암묵적 표현, 텍스처 맵, 애니메이션, 역전 ACM 참조 형식: Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis. 2023. 일관된 텍스처 매개변수화를 사용한 단일 샷 암묵적 모핑 가능 얼굴. 컴퓨터 그래픽 및 대화형 기술에 대한 특수 관심 그룹 컨퍼런스 컨퍼런스 회의록(SIGGRAPH &#39;23 컨퍼런스 회의록), 2023년 8월 6~10일, 미국 캘리포니아주 로스앤젤레스. ACM, 뉴욕, 뉴욕, 미국, 13페이지. https://doi.org/10.1145/3588432.서론 개인화된 아바타 생성-얼굴 특징을 애니메이션, 사용자 지정 및 렌더링할 수 있는 3D 가상 복제본에 매핑하는 기능은 영화, 메타버스 및 원격 존재에 큰 희망을 안겨주는 새로운 기술입니다.이 분야의 발전은 세부 사항과 애니메이션에서 더욱 진실성이 있는 디지털 트윈으로 이어질 수 있습니다.SIGGRAPH 컨퍼런스 회의록, 2023년 8월 6-10일 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein 및 Sameh Khamis는 다운스트림 애플리케이션 및 파이프라인에 더 쉽게 통합됩니다.단일 샷 개인화된 아바타 생성을 통해 보다 특수화된 캡처 설정 또는 절차가 필요한 방법보다 더 편리하고 유연하게 개별 RGB 이미지에서 얼굴 아바타를 재구성할 수 있습니다. 애니메이션이 가능한 3D 아바타 생성에 대한 기존 접근 방식은 종종 모양과 외관 변화를 저차원 얼굴 표현으로 풀어내는 3D 변형 가능 모델(3DMM)[Blanz 및 Vetter 1999]을 기반으로 합니다.이를 기반으로 최근의 접근 방식은 명시적(텍스처가 있는) 템플릿 메시[Daněček 등 2022; Feng 등 2021; Grassal 등 2022; Khakhulin 등 2022; Li 등 2017; Tran 및 Liu 2019] 또는 신경 암묵적 표현[Mildenhall 등 2021; Park 등 2019; Sitzmann 등 2019]을 활용하는 경우가 많습니다.템플릿 기반 접근 방식은 자산 추출과 직관적 편집을 쉽게 할 수 있지만 고품질 지오메트리와 텍스처를 포착하지 못하는 경우가 많습니다.새로운 암묵적 얼굴 모델은 머리카락과 같은 더 복잡한 기하학적 특징을 모델링하여 더욱 사실적인 표현을 얻을 수 있습니다[Cao 등 2022b; Giebenhain et al. 2022; Zheng et al. 2022a]. 그러나 암묵적 얼굴 표현은 종종 해석 가능성을 손상시키고 제어하기 쉽지 않습니다. 이러한 고도로 매개변수화된 모델에서 학습한 얽힌 잠재 공간은 편집하기 어렵습니다. 저희의 접근 방식은 템플릿 기반 3DMM의 해석 가능성 및 편집 가능성 이점을 암묵적 3D 표현의 품질 및 토폴로지 유연성과 결합하는 것을 목표로 합니다. 중요한 점은 모양과 지오메트리를 네트워크 아키텍처의 두 가지 분기로 분리한다는 것입니다. UV 매개변수화 네트워크를 통합하여 연속적이고 일관된 텍스처 맵을 학습함으로써 아바타를 텍스처 메시로 내보내 기존 그래픽 파이프라인에서 텍스처 맵 편집 및 재조명과 같은 다운스트림 애플리케이션을 지원할 수 있습니다(그림 1 참조). 반면, 지오메트리를 암묵적 부호 거리 필드(SDF)로 표현함으로써 얼굴 모양은 메시 기반 접근 방식에 비해 해상도와 토폴로지에 의해 덜 제한됩니다. 저희는 제안한 하이브리드 표현이 얼굴의 지오메트리, 모양 및 표정 공간을 효과적으로 포착한다는 것을 보여줍니다. 우리는 제안된 표현을 기반으로 단일 샷 야생 초상화 이미지를 아바타에 효과적으로 매핑할 수 있으며 이러한 아바타가 사진적 사실성, 기하학 및 단안 표현 전송에서 이전의 최첨단 기술을 개선한다는 것을 보여줍니다.또한 얼굴 기하학 및 모양 속성과 같은 직접적인 텍스처 편집 및 얽힘 해제된 속성 편집을 가능하게 하는 강력한 기능을 보여줍니다.요약하면, 우리 작업의 기여는 다음과 같습니다.· 암묵적 표현의 고품질 기하학 및 유연한 토폴로지와 명시적 UV 텍스처 맵의 편집 가능성을 결합한 하이브리드 변형 가능 얼굴 모델을 제안합니다.· 단일 야생 RGB 이미지를 암묵적 3D 변형 가능 모델 표현에 매핑하기 위한 단일 샷 반전 프레임워크를 제시합니다.반전된 아바타는 새로운 뷰 렌더링, 비선형 얼굴 재생, 얽힘 해제된 모양 및 모양 제어, 직접적인 텍스처 맵 편집 및 다운스트림 애플리케이션을 위한 텍스처 메시 추출을 지원합니다. • 단일 뷰 재구성 설정에서 사진처럼 사실적인 렌더링, 기하학 및 표현 정확도에 대한 최첨단 재구성 정확도를 보여줍니다. 표 1. 최근 이전 작업과의 비교. 저희가 아는 한, 저희 방법은 유연한 토폴로지와 명시적 텍스처 맵 제어를 지원하면서 단일 이미지 입력을 일반화하는 최초의 암묵적 3D 얼굴 모델입니다. 일반화 가능한 단일 이미지 암묵적 EMOCA [2022] ROME [2022] 신경 매개 변수 머리 모델 [2022] IM-Avatar [2022a] 신경 머리 아바타 [2022] 전화 스캔에서 얻은 체적 아바타 [2022b] HeadNeRF [2022] Ours 2 관련 연구 2. 명시적 표현 텍스처 제어 X ✓ ✓ XX xx X xx 메시 기반 3D 변형 가능 모델 x ✓ Blanz와 Vetter의 선구적 연구는 200개의 얼굴 스캔에서 주성분 분석(PCA)을 통해 계산된 선형 부분 공간을 사용하여 템플릿 메시에서 얼굴 모양과 텍스처를 모델링하는 선형 3D 변형 가능 모델(3DMM) [Blanz 및 Vetter 1999]을 제안했습니다. 이 저차원 얼굴 모양과 텍스처 공간은 3DMM을 얼굴 애니메이션을 견고하게 캡처하고 단안 설정에서 3D 얼굴을 재구성하는 데 적합하게 만듭니다. 사진에서 모양, 질감 및 조명을 재구성하기 위해 이전 작업에서는 얼굴 랜드마크 및 픽셀 색상과 같은 제약 조건을 사용하는 연속 최적화[Cao et al. 2014, 2016; Garrido et al. 2013, 2016; Ichim et al. 2015; Li et al. 2017; Romdhani and Vetter 2005; Shi et al. 2014; Thies et al. 2016]를 사용했고 최근에는 딥 러닝 기반 추론[BR et al. 2021; Daněček et al. 2022; Deng et al. 2019b; Dib et al. 2021a,b; Dou et al. 2017; Feng et al. 2021; Genova et al. 2018; Luo et al. 2021; Tewari et al. 2019; Tewari et al. 2017; Tuan Tran 등 2017; Wu 등 2019]. 3DMM에 의존하는 접근 방식은 견고한 경향이 있지만 모델의 선형성과 낮은 차원성으로 인해 충실도 높은 지오메트리 및 텍스처 세부 정보를 재구성하는 데 효과적이지 않습니다. 다양한 다른 방법은 3DMMS를 확장하여 비선형 모양을 캡처하고 [Chandran 등 2020; Li 등 2020; Tewari 등 2018; Tran 등 2019; Tran 및 Liu 2018, 2019; Wang 등 2022b], 신경 렌더링 또는 최적화를 사용한 사실적인 모습 [Gecer 등 2019; Nagano 등 2018; Saito 등 2017; Thies 등 2019], 또는 재조명 가능한 아바타 생성을 위한 반사율 및 지오메트리 세부 정보 [Chen 등 2019; Huynh 등 2018; Lattas 등 2020; Yamaguchi 등 2018]. 최근 접근법은 머리카락과 같은 얼굴 외 영역을 재구성하기 위해 템플릿 메시에 대한 지오메트리 오프셋을 예측합니다 [Grassal 등 2022; Khakhulin 등 2022]. 독자는 3DMM 기술에 대한 심층 조사는 Egger 등 [2020]을 참조하고 신경 렌더링의 최근 진전 보고서는 Tewari 등 [2022]을 참조하세요. 메시 기반 3DMM은 공유 템플릿 메시로 지오메트리를 나타내므로 고정된 토폴로지는 머리카락이나 미세한 세부 사항과 같은 복잡한 지오메트리를 캡처하기 위해 모델을 확장하는 기능을 제한합니다. 또한, 사실적인 얼굴 텍스처를 합성하는 기능은 템플릿 메시와 개별 텍스처 맵의 해상도에 의해 제한될 수 있습니다. 부호 거리 함수로 지오메트리를 매개변수화하고 연속 텍스처 맵으로 색상을 매개변수화함으로써, 저희 방법은 이러한 해상도 문제를 피하고 개별적으로 지오메트리와 텍스처를 제어하기 위한 3DMM과 유사한 직관적 매개변수를 유지하면서 모델 용량에 따라 더 효율적으로 확장할 수 있습니다.저희의 일관된 텍스처 매개변수화는 UV 공간에서의 직접적인 텍스처 편집, 일관된 텍스처 매개변수화를 통한 단일 샷 암묵적 모핑 가능 얼굴뿐만 아니라 얼굴 랜드마크를 통한 얼굴 모델과 입력 이미지 간의 의미적 대응을 가능하게 하며, 이를 활용하여 단일 샷 재구성 품질을 개선할 수 있습니다.2.2 모델링 및 렌더링을 위한 암묵적 표현 단일 샷 3D 재구성 방법에서는 폭셀[Girdhar et al. 2016; Tulsiani et al. 2017; Wu et al. 2018; Yan et al. 2016; Yang et al. 2018; Zhu et al. 2017], 포인트 클라우드[Fan et al. 2017], 메시[Xu 등 2019], 기하학적 원시[Niu 등 2018; Zou 등 2017], 깊이 맵[Wu 등 2020], 암시적 표현은 최근 점유 또는 부호 거리 필드(SDF)[Chen 및 Zhang 2019; Mescheder 등 2019; Xu 등 2019]를 사용하여 더 높은 해상도 재구성을 달성하는 데 활용되었습니다.신경 복사 필드(NeRF)[Mildenhall 등 2021] 및 부호 거리 필드(SDF)[Park 등 2019]와 같은 암시적 표현은 3D 모양과 체적 장면에 대해 높은 재구성 품질을 보여주었습니다.PIFu[Saito 등 2019] 및 후속 작업[Cao 등 2022a; Saito 등 2020]은 암묵적 필드를 사용하여 인체와 의복을 모델링합니다.AtlasNet[Groueix et al. 2018]은 입력 이미지 또는 포인트 클라우드가 주어진 매개변수 표면 요소 집합을 예측하여 3D 모양 생성을 시연했습니다.NeuTex[Xiang et al. 2021]는 NeRF의 광도 예측을 조명 방향에 따라 조건화된 학습된 UV 텍스처 매개변수화로 대체합니다.우리의 방법은 또한 UV 사이클 일관성 손실을 사용하지만, 우리는 1) SDF 설정에서 작동하고 매개변수화를 기하학 및 표현 잠재 코드에 조건화하여 단일 장면에 과적합하지 않고 샘플 전체에 일반화하고, 2) 희소한 얼굴 랜드마크 제약 조건을 사용하여 의미적으로 직관적이고 일관된 매개변수화를 학습하고, 3) 단일 이미지 재구성 중에 학습된 일관된 매개변수화로 가능해진 2D에서 3D로의 얼굴 랜드마크 대응 관계를 명시적으로 활용합니다.암묵적 표현은 또한 더 높은 품질의 3D 생성 모델을 탄생시켰습니다[Chan et al. 2022;Or-El et al. 2022; Xue et al. 2022] 및 후속 작업에서는 단일 뷰 3D 재구성을 위해 사전 훈련된 3D GAN의 잠재 공간으로 이미지를 반전하는 것을 연구했습니다[Ko et al. 2023; Lin et al. 2022; Roich et al. 2022]. 그러나 신중한 최적화와 추가 사전 확률[Xie et al. 2022; Yin et al. 2022]이 없으면 이 3D GAN 반전은 알 수 없는 카메라 포즈[Ko et al. 2023]와 단안 설정에서 NeRF 학습의 다중 뷰 특성으로 인해 덜 견고해지는 경향이 있습니다. 반면, 우리 모델의 컴팩트한 얼굴 표현은 단일 샷 재구성 설정에서 견고한 초기화를 제공합니다. 2.3 암묵적 얼굴 모델 얼굴 모델링을 위한 기존 메시 기반 3DMM과 비교할 때 암묵적 표현은 잠재 코드 컨디셔닝을 통해 자연스럽게 유연한 토폴로지와 비선형 표현 애니메이션을 제공합니다. 일부 접근 방식은 입력 3D 얼굴 스캔에서 암묵적 3DMM을 재구성하는 방법을 학습하지만 [Alldieck et al. 2021; Cao et al. 2022b; Giebenhain et al. 2022; Yenamandra et al. 2021; Zanfir et al. 2022; Zheng et al. 2022b], 다른 연구에서는 RGB 비디오에서 암묵적 얼굴 모델을 모델링하는 방법을 탐구했습니다 [Grassal et al. 2022; Ma et al. 2022; Zheng et al. 2022a,c]. 그러나 위의 접근 방식은 단일 샷 야생 이미지로 일반화할 수 없거나 지원하지 않습니다. 다중 뷰 방법도 암묵적 머리 모델을 재구성하는 데 사용되었습니다 [Athar et al. 2021, 2022; Hong et al. 2022; Kellnhofer et al. 2021; Li et al. 2022; SIGGRAPH Conference Proceedings, Aug 6-10, Ramon et al. 2021; Wang et al. 2022a]. HeadNeRF[Hong et al. 2022]는 우리 연구에 가장 가깝고 훈련 중에 다중 뷰 이미지에서 매개변수 머리 모델을 학습합니다. 테스트 시간에는 3D 재구성을 위해 입력 이미지를 반전할 수 있습니다. 그러나 HeadNeRF는 제한된 이미지 해상도에서 체적 렌더링을 수행하고 업샘플링 CNN 모듈에 의존하여 새로운 뷰 합성 중에 깊이 오류로 인해 깜빡이는 아티팩트가 발생합니다. 더욱이 기존의 암묵적 변형 가능 모델은 보간을 넘어서는 텍스처 조작을 지원하지 않습니다. 대조적으로, 우리가 학습한 명시적 텍스처 매개변수화는 문신이나 콧수염을 추가하는 것과 같은 직관적이고 도메인 외부 편집을 가능하게 합니다(그림 1 참조).3.방법 암묵적 모핑 가능 얼굴 매개변수화 우리는 각 얼굴 아바타를 신원과 표정으로 분리합니다.여기서 신원은 기하 및 색상 잠재 코드로 인코딩되고 표정은 표정 잠재 코드로 캡처됩니다.고품질 기하와 해석 가능한 텍스처를 모두 얻기 위해, 우리 모델은 암묵적 기하 분기와 UV 텍스처 매개변수화 분기로 구성됩니다.기하 분기에는 구면 추적 중에 3D 점 p를 SDF 값 SDF(p)로 매핑하는 다층 퍼셉트론(MLP)이 포함됩니다.UV 텍스처 분기는 p를 구면 좌표 UV(p)로 매핑하는 매개변수화 MLP, UV(p)에서 p로의 역 매핑을 학습하는 매개변수화 정규화기 MLP, UV(p)에서 출력 RGB를 예측하는 색상 네트워크로 구성됩니다.모델 파이프라인 다이어그램은 그림 2를 참조하세요.모델 아키텍처 세부 정보는 보충 자료를 참조하세요. 우리는 주제와 표정의 양과 다양성을 위해 Triplegangers [2022] 3D 스캔 데이터 세트에서 모델을 훈련합니다. RenderPeople [2022] 데이터 세트는 머리카락과 옷을 추가로 모델링하지만 중립적인 표정의 주제가 120개에 불과하여 제약 없는 야외 사진에서 아바타를 재구성하기에 적합하지 않습니다. 우리의 훈련 샘플은 3D 헤드 메시, UV 확산 텍스처 맵, 확산 조명이 있는 6개의 정면 RGB 이미지로 구성됩니다. 데이터 세트에는 각각 20개의 표정이 있는 515개의 다른 주제가 포함되어 총 10,300개의 데이터 샘플이 있습니다. 우리의 전체 모델은 주제가 같은 감정을 다르게 표현하기 때문에 515개의 지오메트리 코드, 515개의 색상 코드, 10,300개의 표정 코드로 구성된 AutoDecoder 사전을 학습합니다. 같은 훈련 주제에 대한 다른 표정은 같은 지오메트리와 색상 코드를 공유하여 모델이 표정을 기본 지오메트리와 텍스처에서 분리할 수 있도록 합니다. 우리의 훈련 데이터 예는 보충 자료를 참조하세요. 3.2 훈련 손실 모델은 기하, 색상 및 정규화 손실에 대해 훈련됩니다.L = Lgeom+ Lcolor + Lreg (1) 그림 2에 따라 ƒ는 SDF MLP, 9는 UV 매개변수화 MLP, g¯¹는 역 UV 매개변수화 MLP, X는 훈련 중 무작위로 샘플링된 표면 점 집합입니다.기하 손실은 표면, Eikonal [Gropp et al. 2020], 정규 및 UV 손실로 구성됩니다.표면 손실 서프는 SDF 제로 레벨 세트를 최적화하고, Eikonal 손실 leikonal은 SDF 그래디언트를 정규화하고, 정규 손실 {normal}은 SDF 그래디언트를 기준 진실 메시 노멀 ñ와 맞춥니다. UV 손실 luv는 학습된 매핑을 SIGGRAPH 컨퍼런스 회의록, 2023년 8월 6-10일 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis SDF 기하 브랜치 UV 텍스처 브랜치 잠재 코드 및 좌표 W geom We SDF(p) expr 텍스처 맵 편집 PE(p) gw geom Wexpr 새로운 뷰 합성 구 추적 W geom Wexpr P 표현 애니메이션 Wcolor 6.g UV(p) W RGB(p) expr 그림 2. 파이프라인. 아바타는 기하, 표현 및 색상 잠재 코드 {wgeom, Wexpr, Wcolor}로 표현되며 각각은 512차원입니다. 구면 추적 중 각 3D 좌표 p에서 SDF 네트워크 f와 UV 매개변수화 네트워크 g는 Wgeom, Wexpr 및 위치 인코딩 PE(p)에 따라 조건지어져 각각 부호 거리 SDF(p) 및 UV 좌표 UV(p)를 예측합니다. 역 UV 매개변수화 네트워크는 학습된 매핑을 표면 매개변수화 g¯¹ (UV(p); Wgeom, Wexpr) = p로 정규화하는 반면, 색상 네트워크 h는 연관된 RGB 텍스처 RGB(p) = h(UV(p); Wcolor, Wexpr)을 예측합니다. 학습 후 아바타는 텍스처와 얼굴 표정을 직접 제어하여 자유롭게 렌더링하거나 독립형 텍스처 메시 자산으로 추출할 수 있습니다. 섹션 3.5에 설명된 단일 샷 역전 파이프라인에서 사용된 텍스처와 지오메트리 간의 대응 관계를 가능하게 하는 가역 표면 매개변수화를 따릅니다. i. 디라이팅 및 인코더 초기화 9-W geomlsurf = ☑ Σ \f(x)| LUMOS (2) xЄX leikonal = Ex (||▼xf(x)|| − 1)² (3){normal = Σ ||√xf(x) - ñ(x)||2² (4) W geom XЄX Wexpr 우리의 모델 ---► luv ΙΣ (5) W color Lgeom (6) = Σ||x-9¹ (g(x))||² XЄX lsurf + leikonal + {normal + luv 색상 손실은 기준 진실 텍스처 Î에 대한 재구성 손실(tex)과 기준 진실 이미지 I와 구면 추적을 통해 얻은 렌더링 이미지 I 사이의 얼굴 영역 Iface에 대한 지각적 [Zhang et al. 2018] 및 재구성 손실 limg로 구성됩니다.ii. 코드 최적화 E Wexpr W color iii. 모델 미세 조정 그림 3. 단일 샷 역전 파이프라인. 입력 이미지에 빛을 비추고 잠재 코드를 초기화합니다. 사전 학습된 인코더(윗줄). 그런 다음 PTI[Roich et al. 2022]를 수행하여 최종 재구성(아랫줄)을 얻습니다. 원본 이미지는 Brett Jordan/flickr에서 제공합니다. 얼굴 랜드마크 제약 조건의 희소 집합을 통해 매개변수화가 일관성을 유지하도록 강제합니다. ltex = |☑| x Χ Σ||f(x)-h(g(x))||² limg = LPIPS (Î face, Iface) + ||Î face - I face ||Lcolor=ltex + limg (8) (9) llandmark|L| Σ ||9(x) − g(x)||² + ||x − g¯¹ (g(x))||2|_ (11) XEL - 마지막으로 기하학, 색상 및 표현 코드의 크기에 페널티를 주어 학습된 잠재 공간의 밀집도를 강제합니다. 3.Lreg = ||Wgeom||² + ||Wcolor ||² + || Wexpr||² (10) UV 매개변수화 학습 피험자 간에 해석 가능한 텍스처 공간과 일관된 의미적 대응 관계를 학습하기 위해 Lreg에 보조 손실 항을 추가합니다.첫 번째 항은 학습된 UV 매핑이 3D 얼굴 랜드마크 포인트 L 집합에 대한 기준 진실 UV 매핑 ĝ과 일치하도록 하고, 두 번째 항은 이 매핑이 가역적이 되도록 합니다.그림은 학습된 UV 매개변수화의 일관성을 보여줍니다.대부분 일관성이 있지만 빌보드 기하학과 기준 진실 데이터에서 비롯된 오류로 인해 안쪽 입과 눈 주위에서 완벽한 등록을 얻는 것은 어렵습니다.일관된 텍스처 매개변수화 입력 이미지 재구성 및 애니메이션을 사용한 단일 샷 암묵적 모핑 가능 얼굴 SIGGRAPH 컨퍼런스 회의록, 8월 6-10일, 최적화. 인코더를 사용하여 입력 이미지 Î에 대한 잠재 코드를 초기화한 후, 모델 가중치를 동결하고 이미지, 실루엣, 다중 뷰 일관성, 얼굴 랜드마크 및 정규화 손실을 최소화하면서 잠재 코드를 최적화합니다.limg = LPIPS(Îface› Iface) + ||Î face − I face ||² 입 랜드마크: X 좌표 0.0.0.-0.-0.-0.0.0.0.0.0.애니메이션 단계 -0.-0.-0.-0.-0.-0.0.입 랜드마크: Y 좌표 -0.0.0.0.1.རྒྱ་ སྠཽ་ གཽ་ཤྲཱི་ཧྥེ་ སྭ་ ཙྭ་ རྒྱ་ 0.입 랜드마크: Z 좌표 0.1.애니메이션 단계 그림 4. 비선형 애니메이션 공간. 소스와 타겟 표현 코드 사이를 선형 보간함으로써, 우리 모델은 시각화된 3D 입 정점에서 비선형 변형 궤적을 보여줍니다. 원본 이미지는 David Shankbone/flickr에서 제공되었습니다. 3.4 애니메이션 훈련 후, 아바타는 표현 잠재 코드를 조작하여 애니메이션을 적용할 수 있습니다. 표현 코드 Wexpr, 대상 표현 코드 w&#39;expr 및 애니메이션 타임스텝 t ¤ [0, 1]을 갖는 소스 피사체의 경우 다음과 같이 표현 애니메이션 궤적을 정의합니다. Wexpr(t) = Wexpr + t * (Wexpr - Wexpr) (12) 기존의 선형 3DMM 접근 방식과 달리, 당사의 표현 공간은 그림 4에서 볼 수 있듯이 고품질 3D 스캔에서 학습한 비선형 궤적을 따릅니다. 3. 싱글샷 반전 보이지 않는 피사체를 재구성하고 애니메이션화하기 위해 사전 훈련된 모델의 잠재 공간에 입력 RGB 이미지를 투영하고 Pivotal Tuning Inversion(PTI) [Roich et al. 2022]과 유사하게 모델 가중치를 가볍게 미세 조정합니다. 보이지 않는 조명 조건을 처리하기 위해 LUMOS [Yeh et al. 2022]를 사용하여 입력 이미지의 조명을 낮추고 별도로 훈련된 인코더를 통해 기하학, 색상 및 표현 코드를 초기화합니다. 경험적으로 이 인코더 초기화가 야생 입력 이미지에 대한 견고한 결과를 얻는 데 중요하다는 것을 발견했습니다(그림 9 참조).이미지 인코더.DeepLabV3+ [Chen et al. 2018] 인코더를 학습하여 각 학습 이미지 Î와 이전 AutoDecoder 학습 단계에서 이미 계산된 해당 잠재 코드 Ŵ를 재구성하여 잠재 코드 초기화를 얻습니다.Lenc = ||Î – I||² + ||Ŵ – w||² W = [Wgeom; Wcolor; Wexpr] (13) (14) 야생 이미지를 반전할 때 가장 큰 과제 중 하나는 실제 이미지에 존재하는 보이지 않는 정체성, 액세서리, 헤어스타일 및 오클루전을 처리하는 것입니다.Tripleganger는 헤어스타일이나 배경에 변형이 없는 제한된 정체성을 포함하기 때문입니다.따라서 [Yeh et al. 2018]의 합성적으로 증강된 Tripleganger 이미지로 인코더의 학습 데이터 세트를 증강합니다. 2022], 그림 9에 표시된 초기화 및 최종 반전 재구성의 견고성을 개선합니다. = 실루엣 Σ f(x) xЄÎface^x*Iface {ID = ArcFace (Î, I, Irand) Σld-proj2D (9¹Â))||² llandmark = dЄD(Î) lreg = || Wgeom||2 + ||Wcolor ||2 + ||Wexpr||² (15) (16) (17) (18) (19) 여기서 실루엣 손실 실루엣은 기준 진실 얼굴 영역 Î face에 포함된 점을 반복하지만 예측 얼굴 영역 Iface에는 포함하지 않아 점을 SDF 제로 레벨 세트에 더 가깝게 가져옵니다. ArcFace [Deng et al. 2019a]는 다양한 뷰 간의 얼굴 유사성을 측정하고 Irand는 무작위로 교란된 카메라 포즈에서 예측된 렌더링입니다. D는 기성품 얼굴 랜드마크 검출기[King 2009]이고 â는 Eq. 11에서 적용된 기준 진실 얼굴 랜드마크 UV 매핑입니다. 일관된 UV 매개변수화가 얼굴 랜드마크 정렬 손실 랜드마크에 대한 대응 관계를 직접 활성화한다는 점에 유의하세요. 그림 10은 이 손실을 통합하는 이점을 보여줍니다. 정규화 손실 freg는 최적화된 코드가 표정 애니메이션을 위한 사전 훈련된 잠재 공간의 매니폴드 근처에 있도록 하는 데 중요합니다. 사전 훈련된 BiSeNet[Yu et al. 2018]을 사용하여 얼굴 마스크를 얻고 800단계로 최적화합니다. 미세 조정. 입력 이미지에서 더 미세한 세부 사항을 재구성하기 위해 최적화 후 잠재 코드를 동결하고 위의 손실에 대한 모델 가중치를 미세 조정합니다. 모델 가중치가 동결 해제되면 지오메트리가 부풀어 오르는 경향이 있으므로 실루엣 손실은 생략합니다. 모델을 미세 조정하면 재구성 품질이 향상되지만 애니메이션이나 새로운 뷰 합성 기능이 방해를 받습니다. 따라서 60단계에 대해서만 모델 미세 조정을 수행합니다. mayRESULTS 우리는 제안한 방법의 결과를 EMOCA [Daněček et al. 2022], ROME [Khakhulin et al. 2022] 및 FaceVerse [Wang et al. 2022b], 싱글샷 3D 아바타 생성을 위한 세 가지 최신 메시 기반 접근 방식과 신경 광도장을 사용하는 암묵적 접근 방식인 HeadNeRF [Hong et al. 2022]와 비교하여 제시합니다. 우리의 방법은 기준선에 비해 얼굴 영역에서 더 높은 충실도의 텍스처 및 지오메트리 재구성을 달성합니다. 질적, 양적으로, 우리의 방법은 또한 야생 소스와 대상 이미지 사이에서 보다 충실한 표현과 포즈 전송을 보여줍니다. 마지막으로, 학습된 텍스처 맵은 편집하기 쉽고 애니메이션 중에 자연스럽게 전파됩니다. 4. 구현 세부 정보 우리의 모델은 두 단계로 학습됩니다. 첫 번째 단계에서는 텍스처 맵과 멀티뷰 이미지 모두를 감독하는 것이 모델의 일관된 UV 매핑 학습 능력에 부정적인 영향을 미친다는 것을 알았으므로 실제 멀티뷰 이미지를 보류합니다.두 번째 단계에서는 SIGGRAPH Conference Proceedings, 2023년 8월 6-10일 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis 입력 이미지 조명 해제 이미지 Ours HeadNeRF ROME EMOCA Ours HeadNeRF ROME EMOCA 대상 표정 및 포즈 재구성 표정 및 포즈 전송 그림 5. 표정 및 포즈 전송을 사용한 FFHQ의 단일 샷 재구성.왼쪽에는 입력 FFHQ 소스 이미지, LUMOS를 사용한 조명 해제 입력 이미지[Yeh et al. 2022], 각 방법에 대한 재구성 결과가 나와 있습니다. 오른쪽에서는 단안 퍼포먼스 캡처와 리타게팅을 보여줍니다.여기서 우리는 대상 이미지(가장 오른쪽 열)에서 표정과 포즈를 재구성하여 소스 이미지 아이덴티티(가장 왼쪽 열)로 전송합니다.왼쪽에서 위에서 아래로, 원본 이미지는 José Carlos Cortizo Pérez/filckr, Montclair Film/flickr, Pham Toan/flickr, Javier Morales/flickr, Khiet Nguyen/flickr, Malcolm Slaney/flickr에서 제공했습니다.오른쪽에서 위에서 아래로, 원본 이미지는 Adam Charnock/flickr, Daughterville Festival/flickr, Delaney Turner/flickr, South African Tourism/flickr, Pat (Cletch) Williams/flickr, Collision Conf/flickr에서 제공했습니다.표 2. 야생에서의 단일 샷 재구성(왼쪽)과 자기 표현 리타게팅(오른쪽)에 대한 정량적 결과. 왼쪽: FFHQ에서 샘플링한 500개 이미지에서 이미지, 포즈, 아이덴티티 메트릭을 계산합니다. 깊이 메트릭은 H3DS 데이터 세트에서 계산합니다. 이미지, 아이덴티티, 깊이 메트릭은 얼굴 영역에서만 계산합니다. EMOCA는 더 작은 얼굴 크롭을 사용하여 평가합니다. 오른쪽: Triplegangers 테스트 분할에서 샘플링한 32개 표정 쌍에서 표정과 포즈를 전송한 후 FACS 계수와 얼굴 랜드마크를 계산합니다. 재구성 LPIPS↓ DISTS↓ SSIM↑ 포즈↓ ID↑ LDepth↓ RMSE 깊이↓ 리타게팅 FACS↓ 얼굴 랜드마크↓ EMOCA ROME HeadNeRF 0.1122 0.1268 0.9182 0.0681 0.0.1054 0.1130 0.9317 0.0600 0.0.1090 0.1199 0.9268 0.0606 0.2334 0.Ours(최적화 없음) 0.1427 0.1465 0.9053 0.0549 0.1082 0.Ours(인코더 없음) 0.0890 0.0921 0.9441 0.0533 0.4600 0.Ours 0.0879 0.0905 0.9451 0.0563 0.4670 0.0.0.0.0.0.0.0.0.0.EMOCA ROME HeadNeRF 3.Ours 1.4.3.0.0.0.0.표 3. 단일 샷 야생 재구성을 위해 500개 샘플링된 FFHQ 이미지에서 FaceVerse [Wang et al. 2022b]와의 정량적 비교. 재구성 FaceVerse Ours LPIPS DISTS↓ SSIM↑ 0.1280 0.1119 0.0.0879 0.0905 0.UV 네트워크 {9,9¯¹}를 동결하고 다중 뷰 이미지를 사용하여 768×512 해상도에서 이미지 재구성을 렌더링하는 동안 학습된 텍스처 맵을 미세 조정합니다. 카메라 포즈는 실제 훈련 데이터와 함께 제공되며, Deep3DFaceRecon[Deng et al. 2019b]을 사용하여 야생 FFHQ 이미지에 대한 카메라 포즈를 추정합니다. 우리는 광선당 50단계에 대한 구면 추적을 수행하고 기하학, 색상 및 표현 잠재 코드에 대해 512차원을 사용합니다. 우리는 8개의 NVIDIA A40 GPU에서 AutoDecoder를 1000에포크(약 1주일) 동안 훈련하고 역 인코더를 200에포크(약 1일) 동안 훈련합니다. 우리는 양적 표현에 대해 386/129의 Triplegangers 훈련/테스트 분할을 사용합니다.
