--- ABSTRACT ---
생체 모방적이고 능숙한 로봇 손은 인간이 할 수 있는 많은 작업을 복제하고 일반적인 조작 플랫폼으로서의 지위를 달성할 수 있는 잠재력을 가지고 있습니다. 강화 학습(RL) 프레임워크의 최근 발전은 사족 보행 및 능숙한 조작 작업에서 놀라운 성과를 달성했습니다. 수천 대의 로봇을 병렬로 시뮬레이션할 수 있는 GPU 기반의 고도로 병렬화된 시뮬레이션과 결합하여 RL 기반 컨트롤러는 더욱 확장 가능하고 접근하기 쉬워졌습니다. 그러나 RL 학습 정책을 현실 세계로 가져오기 위해서는 물리적 액추에이터 및 센서와 함께 작동할 수 있는 정책을 출력하는 학습 프레임워크와 접근 가능한 소재로 제조할 수 있지만 대화형 정책을 실행할 수 있을 만큼 견고한 하드웨어 플랫폼이 필요합니다. 이 연구에서는 생체 모방 힘줄 구동 Faive Hand와 힘줄 구동 롤링 접촉 조인트를 사용하여 3D로 인쇄 가능한 견고한 고 자유도 손 디자인을 달성하는 시스템 아키텍처를 소개합니다. 우리는 손의 각 요소를 모델링하고 이를 GPU 시뮬레이션 환경에 통합하여 RL로 정책을 훈련하고, 능숙한 손 안 구체 회전 기술을 실제 로봇 손으로 바로 전송하는 것을 달성합니다.¹ A. 동기 I.
--- METHOD ---
로봇의 동적 모델로 컨트롤러가 명확하게 추론하는 s. 이는 특히 여러 손가락의 조정이 필요한 움직임에 인간형 로봇 손을 적용하는 능숙한 조작 작업의 경우입니다. 이러한 조정된 동작을 달성하면 창고에서의 픽앤플레이스, 공장 라인에서의 조립 또는 일상 생활에서의 지원과 같은 많은 반복적인 작업을 대체할 수 있는 잠재력이 있습니다. 이 작업에서 능숙한 조작 작업을 위한 플랫폼인 Faive Hand를 소개합니다. 우리는 그 모델을 RL 환경에 통합하는 것에 대한 현재 진행 상황을 보고하고, 인간과 같은 조작을 향한 첫 번째 단계로 능숙한 손 안 구체 회전을 달성하기 위해 실제 로봇에 폐쇄 루프 컨트롤러를 적용합니다. B. 능숙한 조작의 최신 기술 a) 학습 기반 제어: 능숙한 조작 작업은 학습 기반 제어 접근 방식의 이점을 얻습니다.취리히 1ETH {ytoshimitsu, bforrai, bcangan, ulsteger, knechtma, sweirich, rkk}@ethz.ch 2Max Planck ETH 학습 시스템 센터 https://srl-ethz.github.io/get-ball-rolling/ 비디오: https://youtu.be/YahsMhqNUa b Os 2s 4s 2s 4s Os 그림 1. (a) RL 정책을 훈련하기 위해 4096개의 로봇 손을 병렬로 시뮬레이션하는 GPU 기반 병렬 시뮬레이션 환경. (b) 롤링 접촉 관절이 있는 힘줄 구동 로봇 손에 배포되는 훈련된 정책. 모델 기반 접근 방식은 컨트롤러가 처리해야 하는 다양한 접촉 상태의 수를 처리하는 데 어려움을 겪습니다. 접촉은 링크 체인을 따라 어디에서나 발생할 수 있으므로 접촉 상태의 수는 이동과 같은 다른 작업의 상태 수보다 몇 배나 많을 수 있습니다.실제 로봇의 능숙한 조작에 학습 기반 방법을 적용하는 데 있어 첫 번째 주요 돌파구는 OpenAI가 Shadow Hand에서 인핸드 큐브 회전을 위해 달성했습니다[1].그러나 CPU 기반 시뮬레이터와 샘플 비효율적 RL 알고리즘을 결합하려면 각각 16개의 CPU 코어가 있는 384개의 머신에서 최대 50시간의 계산이 필요하여 정책 경험을 수집하기 위해 병렬로 실행해야 했기 때문에 다양한 작업에 대한 확장성과 다른 기관에서 동일한 설정을 복제하는 접근성이 제한되었습니다.그 후로 수천 개의 로봇을 병렬로 시뮬레이션할 수 있는 IsaacGym[2]과 같은 GPU 기반 시뮬레이터가 도입되면서 RL 에이전트를 훈련하는 데 필요한 리소스가 크게 줄었습니다.이동과 같은 일부 작업은 단일 GPU에서 몇 분 안에 훈련할 수도 있습니다[3]. 작업을 직접 비교할 수는 없지만 이 논문의 모든 능숙한 조작 정책은 단일 NVIDIA A10G GPU에서 약 1시간 만에 학습했습니다.IsaacGym과 같은 병렬 시뮬레이션 환경에서 학습한 조작 정책을 실제 로봇 손에 적용하는 데 최근 많은 진전이 있었습니다.Handa 등은 자동 도메인 무작위화(ADR)의 벡터화된 구현과 함께 비전 기반 정책을 제안하여 외부 감각 입력에 RGB 카메라만 사용하여 Allegro 손에서 큐브 회전 작업을 실행할 수 있었습니다[4].Chen 등은 시뮬레이션에서 여러 개체를 재조정할 수 있는 정책을 만들었습니다[5].그런 다음 Yin 등은 고유 감각 터치 센서 입력만을 사용하여 원하는 축을 중심으로 개체를 손에 회전시키는 파이프라인을 제안했습니다[6].Allshire 등은 IsaacGym을 사용하여 TriFinger 로봇 시스템에서 능숙한 조작을 달성했습니다[7]. b) 로봇 손 하드웨어: 유능한 로봇에는 하드웨어와 컨트롤러의 조합이 필요하다는 점에 유의하는 것이 중요하며, 능숙한 조작 연구에 사용되는 주요 로봇 손을 살펴보겠습니다. 이전 작업의 대부분은 각 관절에 서보 모터가 포함된 실물보다 큰 4개 손가락 손인 Allegro 손[8]이나 인간형 5개 손가락 힘줄 구동 로봇 손인 Shadow 손[9]을 사용했습니다. 또한 연구 기관에서 다른 로봇 손도 만들어졌습니다. LEAP 손은 Allegro 손과 비슷한 디자인을 가지고 있지만 관절 레이아웃과 견고성이 개선되었습니다[10]. IsaacGym 기반 RL 작업[7]과 오프라인 RL 경쟁[11]에서 사용되는 TriFinger 로봇은 BLDC 모터[12]를 사용하는 인간형이 아닌 3개 손가락 조작기입니다. 우리는 인간 환경에서 조작을 달성하기 위해서는 인간의 형태에 더 가깝게 하는 것이 더 유리하다고 주장합니다. 우리 환경의 도구와 물체는 원래 인간이 사용하도록 설계되었으므로 인간과 유사한 손 디자인이 이러한 도구와 물체와 상호 작용하기에 더 적합합니다. 또한 인간의 시범을 통해 학습할 때 조작 작업을 유사한 구조를 가진 로봇으로 더 쉽게 전환할 수 있습니다. Allegro 손과 같이 각 축을 구동하는 서보 모터가 있는 로봇 손은 구성하기 간단하지만(결과적으로 비용이 낮음) 손가락이 인간보다 상당히 두꺼워집니다. Shadow Hand는 지금까지 가장 인간과 유사한 손이지만 웹사이트에 따르면 110k GBP라는 엄청난 가격표가 있어 물리적인 작업을 수행하기에 접근성이 제한됩니다.
--- EXPERIMENT ---
s. 높은 구매 및 유지 관리 비용으로 인해 RL 훈련 정책의 빈번한 sim2real 실험이 억제될 수 있습니다.이 정책은 실제 로봇에서 처음에는 불규칙하게 작동할 수 있으며 현실에서 완전히 작동할 때까지 집중적인 시행착오가 필요하기 때문입니다.C. 접근 방식 Soft Robotics Lab에서 우리는 Faive Hand를 개발했습니다.이것은 민첩한 조작을 탐구하기 위한 생체 모방 민첩한 힘줄 구동 로봇 플랫폼입니다.현재 버전의 손은 접근 가능하고 간단한 제조를 위해 3D 인쇄 구성 요소와 서보 모터를 사용합니다.그러나 조작을 위한 고 자유도 로봇 손을 제어하는 데 내재된 과제 외에도 이 손은 회전축이 고정되지 않은 회전 접촉 관절과 같이 RL로 훈련된 다른 민첩한 손에는 없는 기능을 가지고 있습니다.기존 회전 인코더는 이 설계에서 사용하기 어렵기 때문에 현재 이 손에는 이후 버전의 손을 위해 개발 중인 내부 관절 각도 인코더가 없습니다.이러한 제한으로 인해 관절 각도는 서보 모터 각도에서 계산할 수 있는 힘줄 길이에서 추정해야 합니다. 이러한 기능은 시뮬레이션 프레임워크와 저수준 컨트롤러에서 구현되었으며, 이를 통해 실제 로봇에서 폐쇄 루프 RL 학습 정책을 실행할 수 있었습니다.우리는 로봇이 대상 방향으로 구를 능숙하게 회전시키는 Shi et al. [13]과 유사한 작업을 선택했습니다.D. 기여 • IsaacGym 시뮬레이터에 롤링 접촉 관절 모델을 통합합니다.시뮬레이션에서 학습된 폐쇄 루프 정책을 적용하여 힘줄 구동 생체 모방 Faive Hand를 제어합니다.그리고 • 능숙한 조작을 위한 접근 가능한 플랫폼으로 설계된 Faive Hand의 프로토타입 버전을 소개합니다.II. 견고한 롤링 접촉 관절을 갖춘 힘줄 구동 능숙한 손 이 섹션에서는 Faive Hand 2의 생체 모방 관절 구조를 소개하고 실제 로봇에서 RL 정책을 실행할 수 있도록 이를 모델링하고 제어하는 방법을 설명합니다. Faive Hand는 생체 모방 조작을 연구하기 위해 저희 연구실에서 개발되었으며, 궁극적으로 많은 연구 기관에서 실제 하드웨어에 대한 능숙한 조작 연구를 가능하게 하는 저비용 플랫폼을 제공하여 인간형 로봇 손을 실제 응용 분야에 적용하는 것을 가속화하는 것을 목표로 합니다.A. 손의 롤링 접촉 관절 설계 여기서는 이 논문의 실험에 사용된 Faive Hand의 프로토타입 버전을 Proto 0이라고 명명하여 설명합니다. 이 버전은 엄지 손가락에 3개, 나머지 손가락에 2개씩 총 11개의 구동 자유도를 포함합니다. 각 손가락에는 원위부에 연결된 관절이 있으므로 총 16개의 관절이 있습니다. 인간의 손가락과 유사하게, 저희의 로봇 손가락 설계는 그림 2와 같이 인체 해부학에서 파생된 관절 명명 규칙을 적용한 3개의 관절로 구성됩니다. 각 손가락의 원위 지절간(DIP) 관절은 연결 힘줄을 사용하여 근위 지절간(PIP) 관절에 연결되어 함께 구부러집니다. 따라서 DIP 및 PIP 관절은 하나의 굴곡건과 별도의 신전건에 의해 구동되어 두 관절을 동시에 구동합니다.중수지절(MCP) 관절은 굴곡건과 신전건이 모두 부착된 단일 모터에 의해 길항적으로 작동됩니다.핀 조인트[9], [14], 가공된 스프링[15], [16], 소프트 메커니즘[17], [18] 및 롤링 접촉 조인트[19], [20]와 같이 생체 모방 관절형 손에 사용된 조인트 설계가 많이 있습니다.2개의 힌지 조인트를 사용하여 재현된 엄지손가락의 손목중수관절을 제외하고 모든 조인트는 롤링 접촉 조인트로 구현됩니다.이러한 롤링 조인트는 그림 2에 표시된 대로 한 쌍의 십자형 인대 끈으로 연결된 인접한 곡선 접촉 표면이 있는 두 개의 관절체로 구성됩니다(어린이 장난감 Jacob&#39;s ladder와 유사). 그들은 충격 준수, 낮은 마찰 또는 더 넓은 범위의 운동과 같은 장점이 있으며 로봇 공학, 임플란트 및 보철물에 제안되었습니다[21], [22]. 롤링 접촉 조인트에 대한 확장도 있었습니다.2https://www.faive-robotics.com/ 프로젝트에서 손을 사용하는 것에 대한 문의 사항은 웹사이트의 연락처 양식을 사용하십시오.a DIP/PIP 커플링 원위 인대-지골(DIP) 조인트 근위 지골(PIP) 조인트 PIP 굴곡 신근 롤링 접촉 표면 중수지MCP 지골 굴곡(MCP) 조인트 MCP 신근 교차 인대 실리콘 패딩 롤링 접촉 조인트 힘줄 마주보는 엄지 관절 힘줄 그림 2. (a) Faive Hand는 인간 손가락의 힘줄과 인대를 모방한 롤링 접촉 조인트 디자인을 가지고 있습니다.(b) Faive Hand의 각 구성 요소에 대한 개요. a HULE b 东 그림 3. (a) Faive Hand는 최대 10kg의 하중을 잡을 수 있으며, 여기서는 덤벨로 시연했습니다. (b) 고정 축을 중심으로 회전하지 않는 MuJoCo 시뮬레이터와 실제 로봇 손의 롤링 접촉 관절의 동작. 인공 피부로 둘러싸인 유체 윤활 관절[20] 또는 가변 강성[19]으로 제안되었습니다. 저희의 손가락 디자인은 이전 작업을 기반으로 하면서 디자인을 더욱 단순화하여 더 견고하고 컴팩트하며 제조하기 쉽게 만들었습니다. 그림 3에서 볼 수 있듯이, 저희는 아래를 향한 5개 손가락 파워 그립을 사용하여 최대 10kg의 하중을 시연했으며, 손 시스템의 총 중량은 1.1kg에 불과했습니다. C. 관절 제어 및 감지를 가능하게 하는 저수준 컨트롤러 OpenAI의 작업[1]에서 Shadow Hand가 모델링된 방식과 유사하게, 손은 힘줄 수준 정보를 무시하고 관절 구동 로봇으로 시뮬레이션되었습니다. 대신 로봇의 저수준 컨트롤러는 관절 명령과 측정값을 힘줄 대응물에서 변환하는 프로그램을 실행했습니다.a) 관절 각도 명령을 모터 수준 명령으로 변환: 힘줄은 16개의 Dynamixel XC330-T288-T 서보 모터로 제어됩니다.그 중 6개에는 두 개의 힘줄이 적대적으로 부착되어 있습니다.한 모터에 두 개 이상의 힘줄이 있으면 필요한 서보 수가 줄어들지만 두 힘줄의 동작이 항상 서로에 대해 일정하게 조정된 관계를 가질 때만 사용할 수 있으며, 이는 스풀 반경의 비율로 표현됩니다.이는 손가락³의 근위 관절에 해당합니다.그러나 힘줄의 라우팅으로 인해 원위 관절의 힘줄 길이는 근위 관절의 각도에 따라 달라지며 이러한 일정한 관계가 없습니다.따라서 원위 관절에는 단일 힘줄에 대한 전용 모터가 필요합니다. CAD 데이터를 기반으로 관절의 롤링 동작과 힘줄 경로를 기하학적으로 모델링하면 관절 각도 = q를 힘줄 길이 1로 매핑하는 함수 lf(q)를 분석적으로 설명할 수 있습니다. 이 함수를 사용하면 원하는 관절 각도 q를 원하는 힘줄 길이 T = f(g)로 변환한 다음 원하는 서보 모터 각도(힘줄 스풀 반경으로 나눔)로 변환하여 Dynamixel 모터로 보낼 수 있습니다. b) 확장 칼만 필터를 사용한 관절 각도 감지: 함수 f(·)는 모든 관절 각도 집합을 힘줄 길이를 표현하는 공간의 매니폴드로 매핑합니다. 이것은 단사 변환이 아니며 이 매핑은 쉽게 역전될 수 없습니다. 관절 공간의 구성으로 다시 매핑되지 않는 힘줄 길이의 조합이 있습니다. 따라서 힘줄 길이 측정값에서 관절 각도를 추정하기 위해 확장 칼만 필터(EKF)를 사용하는 Ookubo 등이 사용한 방법을 채택합니다[23]. 여기서 상태와 관찰치를 각각 x와 z로 표시하고, 다음과 같이 위치와 속도를 연결하여 공식화합니다. x = Є Rx =Є R(1) B. 시뮬레이션에서 롤링 접촉 조인트 모델링 그림 3에서 볼 수 있듯이 이러한 롤링 접촉 조인트에는 회전 축이 하나도 없습니다. 따라서 두 개의 &quot;가상&quot; 힌지 조인트로 시뮬레이션에서 모델링했습니다. 이러한 힌지 조인트의 축은 각 롤링 접촉 표면을 구성하는 원통의 축을 통과하도록 배치했습니다. 이들은 관절 각도의 선형 조합에 제약 조건을 적용하는 힘줄/고정 요소로 연결하여 MJCF(MuJoCo의 모델링 형식) 파일에서 함께 회전하도록 제한되었습니다. 힘줄 모델 정보는 IsaacGym에 MJCF 파일이 로드될 때 전달되며, 각 힘줄 속성에 limit_stiffness 및 damping 값을 설정하여 활성화할 수 있습니다. 전이 모델은 Xi+Idt I xi+w (2)로 설명할 수 있습니다.여기서 아래 첨자 i는 시간 단계를 나타내고, I는 단위 행렬, O는 영 행렬, dt는 시간 단계를 나타내고, w는 상태 전이의 노이즈입니다.sympy [24]와 같은 기호 계산 라이브러리를 사용하면 관절 각도를 힘줄 길이로 매핑하는 함수 f(·)의 편미분을 기호적으로 유도하여 근육 야코비안 Jm af(q)/aq를 얻을 수 있습니다.그러면 비선형 3 이론상 모멘트 암이 변하기 때문에 길항 관계가 약간 달라지지만 실제로 이 변화는 무시할 만큼 작습니다.steptrain RL 정책 행위자 관찰 비평가 관찰 보상 비평가 MLP PPO 병렬화된 시뮬레이션 값 행위자 MLP 동작 단계 2 실제 로봇 M에서 실행 실제 로봇 관절 각도 추정치 q EKF 힘줄 길이 힘줄 동작. 관절-힘줄 명령 매핑 f(•) 그림 4. 롤링 접촉 관절이 있는 힘줄 구동 로봇 손에서 능숙한 조작을 달성하기 위한 RL 훈련 프레임워크 개요. 시뮬레이션 환경 내에서 정책을 훈련한 후 액터 네트워크가 실제 로봇으로 전송됩니다. EKF에 사용된 관찰 모델은 다음과 같이 설명할 수 있습니다. Zi = h(xi) + vh(i) == f(qi) Jm(qi)ġi (3) 모터 위치(스풀 각도)와 힘줄 길이 간의 관계를 설정하는 교정 절차는 로봇이 부팅될 때마다 실행됩니다. 로봇 손 관절을 알려진 포즈로 제한하는 지그를 장착하고 힘줄이 팽팽해질 때까지 가볍게 당깁니다. 이 프로그램은 그 순간의 모터 위치를 기준으로 힘줄 길이를 계산합니다. EKF에서 추정된 관절 각도 â는 후속 섹션에서 설명하는 대로 RL 정책에 대한 고유 감각 측정으로 사용할 수 있습니다. III. 능숙한 조작을 위한 강화 학습 훈련 정책을 훈련하고 실제 로봇에서 실행하기 위한 파이프라인 개요는 그림 4에 나와 있습니다. 정책은 비대칭 관찰(행위자와 비평가에게 서로 다른 관찰 세트가 제공됨)을 사용하여 이점 행위자-비평가(A2C)가 있는 RL로 훈련됩니다. 오픈 소스 저장소 rl_games [26]에서 구현한 PPO 알고리즘 [25]을 사용합니다. 행위자와 비평가로 MLP 네트워크를 사용합니다. 훈련이 완료된 후, 행위자의 MLP는 교차 호환 ML 모델 형식인 ONNX 형식으로 내보내져 로봇에서 실행되었습니다. 섹션 II-C에서 소개한 관절-힘줄 매핑과 EKF는 Faive Hand의 관절 수준 감지 및 제어를 가능하게 하는 데 사용되었습니다. A. 보상 표 I에는 작업에 사용된 보상과 해당 공식이 나와 있습니다. 구체 회전 작업에 특정한 보상은 객체 회전 보상으로, y축의 회전 속도가 3~+1 rad s¹ 사이일 때 최대 보상을 반환하고 이 영역 외부에서는 선형적으로 감소합니다. 보상 공식에서 wy의 부호를 뒤집어 원하는 회전 방향을 반대로 할 수 있습니다. IsaacGym의 객체 각속도 측정값에 상당한 노이즈가 포함되어 있는 것을 발견했는데, 이는 손의 일부가 객체에 닿을 때마다 충격력을 적용하는 충돌 계산 때문이라고 가정했습니다. 이 각속도 측정값에서 객체 회전 보상을 계산하면 시뮬레이터의 물리를 활용하는 정책이 생성되는 경향이 있어 시뮬레이터 내에서도 실제로 구를 회전시키지 않는 동작이 발생합니다. 따라서 시간 단계 사이에서 객체 방향의 변화를 수치적으로 미분하여 계산한 객체 회전 보상에 수치 각속도를 사용했습니다. B. 관찰 공간 액터와 비평가는 두 개의 별도 MLP로 구현되므로 서로 다른 관찰 집합을 받을 수 있습니다. 비평가의 MLP는 훈련 중에만 필요하므로 시뮬레이션 내에서 얻을 수 있지만 실제 로봇에서는 얻을 수 없는 데이터인 특권 정보를 입력으로 사용할 수 있습니다.표 II에는 액터와 비평가에 사용된 관찰 결과가 나와 있습니다.관절 위치는 로봇의 관절 범위를 사용하여 -1과 1 사이의 범위로 정규화되었습니다.실제 로봇의 측정값에는 특히 속도 측정의 경우 약간의 노이즈가 포함되어 있으므로 관절 속도가 암묵적으로 표현되는 관절 위치 측정의 지난 5단계를 사용하기로 했습니다.대상 작업은 모든 축에서 대칭인 구를 회전하는 것입니다.이러한 대칭성으로 인해 물체의 방향은 손이 물체와 상호 작용하는 방식에 영향을 미치지 않습니다.또한 성능에 부정적인 영향을 미치지 않고 액터 관찰에서 물체의 위치 측정을 제거할 수도 있습니다.따라서 액터는 고유 감각 관절 데이터만 사용하여 sim2real 프로세스와 물체 포즈의 정확하고 저지연 측정을 얻는 기술적 과제를 단순화합니다.C. 액션 공간 액션 a는 관절 각도 명령의 상대적 변화를 표현합니다. 먼저 -1과 1 사이로 클리핑한 다음 원하는 관절 각도 ā를 clip(ā+ Vmax Ata, Imin, Imax) (4)로 증가시킵니다. 여기서 ▲t는 타임스텝이고 Umax는 정책에서 관절의 최대 속도를 제한하는 상수 스칼라입니다. Umax는 5 rad s-1로 설정되었습니다. 작업이 원하는 관절 각도를 업데이트한 후 로봇 손의 최소 qmin과 최대 qmax 관절 각도 사이에 클리핑됩니다. D. 도메인 무작위화 물리 엔진의 부정확성을 보상하고 sim2real 갭을 극복하기 위해 정책을 보다 강력하게 만들기 위해 도메인 무작위화가 관찰, 감쇠 및 강성과 같은 물리 속성에 적용되었습니다. 표 I 훈련 중 사용되는 보상 및 패널티. 보상 공식 가중치 객체 회전 토크 페널티 min(wy + 1, 2, ±wy + 4) 0. 액션 페널티 ||7|||a||-0.-0. 드롭 페널티 ||xobj xhand||2 &gt; 24 cm -1. 표 II ACTOR 및 CRITIC 네트워크에서 사용하는 관찰. 입력 차원 actor critic joint posjoint pos commandjoint pos historyjoint velocityjoint 토크object posobject quatobject linear velobject angular velfingertip positionfingertip quaternionfingertip lin velfingertip ang velfingertip force이전 동작justification reward y 축의 회전(역전 가능) joints가 큰 토크를 적용하지 못하도록 방지 큰 동작 방지 object drops penalize(환경이 재설정된 후 일회성 페널티) 총 보상 [-] 보상 ᎠᎡ No DR DR, reverse-wy [rad/s] 평균 각속도 ᎠᎡ No DR DR, reverseTime [min]Time [min] 그림 5. 도메인 무작위화(DR)를 사용하거나 사용하지 않고 학습한 정책과 역방향 대상 회전 방향에 대한 학습 곡선 진화. 각 접근 방식에 대해 9회 학습 라운드의 평균을 구했습니다. 두 플롯 주변에 ±σ 영역이 표시됩니다. 힘줄과 관절, 관절의 움직임 범위, 로봇과 물체의 질량과 마찰, 물체의 크기.IV. 실험 A. 시뮬레이션에서 정책 학습 GPU 기반 고성능 물리 시뮬레이터 IsaacGym [2]을 사용하여 정책 학습을 위해 로봇을 시뮬레이션했습니다.4096개의 환경이 단일 NVIDIA A10G GPU에서 병렬로 시뮬레이션되었습니다.시뮬레이션은 atHz로 실행되었고 정책은 3단계마다 실행되어 aHz 정책이 생성되었습니다.actor 및 critic 네트워크는 차원 [512, 512, 256, 128]과 ELU 활성화의 4개 은닉 계층이 있는 별도의 MLP였습니다.실제 로봇에서 정책 성능에 미치는 효과를 비교하기 위해 도메인 무작위화(DR)를 사용하거나 사용하지 않고 학습했습니다.그림 5는 결과 학습 곡선을 보여줍니다.RL 성능은 난수 시드 선택에 크게 의존하므로 여러 난수 시드로 학습을 실행하고 평균과 표준 편차를 표시했습니다. 보상 외에 정책 성과의 추가 지표로 y축을 따라 원하는 방향으로 각속도를 기록했습니다. 보상의 wy를 wx와 w₂로 바꿔서 x축과 z축에서도 공을 회전시키려고 했습니다. 그러나 RL 알고리즘은 해당 축에서 공을 일관되게 회전시키는 정책을 내놓지 못했습니다. 이 현상은 손가락을 벌리고 모으는 관절인 이 손 프로토타입에 외전과 내전을 위한 관절이 없기 때문이라고 생각합니다. 우리 스스로도 손에서 x축과 y축에서 물체를 회전시키려고 할 때 이러한 관절이 필요하다는 것을 알 수 있습니다. 그림 5에서 처음 10분 동안 성능이 급격히 증가한 다음 점차 안정을 찾는 것을 볼 수 있습니다. DR이 활성화된 학습 실행 중 하나에서 정책 성과가 약 30분에 갑자기 붕괴되어 보상의 표준 편차가 증가하고 평균이 약간 낮아졌습니다. 이는 RL 설정의 실제 성능을 평가하는 데 서로 다른 난수 시드로 여러 번 실행하는 것이 중요함을 시사합니다. 기록된 두 메트릭 모두 DR이 비활성화되었을 때 성능이 더 높습니다. 이는 DR이 아닌 정책이 물리 매개변수의 단일 집합이 있는 환경에 미세 조정할 수 있는 반면 DR 정책은 여러 다른 물리 매개변수와 함께 작동해야 하기 때문에 예상할 수 있습니다. B. 실제 로봇에서 정책 실행 서로 다른 난수 시드에 따른 성능 차이는 시뮬레이션 내에서보다 실제 로봇에 적용했을 때 더 컸습니다. 일부 정책은 정책을 실행한 지 몇 초 후에 손가락이 움직이지 않아 정책이 동작에 대해 0 값을 출력하는 손 포즈에 갇혔습니다. 따라서 별도의 난수 시드로 학습된 각 정책을 실행하고 각 조건에서 가장 성능이 좋은 정책을 선택하여 각각에 대한 최상의 성능을 평가했습니다. 흥미롭게도, 정책에 보내기 전에 관절 위치 측정값을 0.5로 곱하면 실제 로봇에서 실행할 때 정책 성능이 향상되고 시뮬레이션에서와 훨씬 더 가까운 동작을 달성한다는 것을 발견했습니다. 우리는 손가락이 공을 누르면 힘줄이 늘어나고 구조가 변형되어 접촉 순간보다 힘줄을 더 멀리 당긴다고 가정합니다.이는 관절 각도 추정치에 영향을 미칩니다.-wy [rad/s]DR(sim) DR(real) No DR(sim) No DR(real) DR, reverse(sim) DR, reverse(real) 그림 6. DR이 있는 정책과 없는 정책에 대해 실제 로봇과 시뮬레이션된 로봇의 객체 회전 속도 분포.회색 줄은 객체 회전 보상이 최대값인 영역을 나타냅니다.마름모는 평균을 나타냅니다.EKF에서 손가락이 실제보다 더 많이 구부러졌다고 잘못 추정하는 것입니다.우리는 관찰치를 축소하는 것이 이 효과에 대한 간단한 교정 조치로 작동한다고 믿습니다. 공의 회전을 측정하기 위해 구형 가챠폰 캡슐 장난감 공에 IMU와 블루투스 장치(Arduino Nano 33 BLE)를 내장하고 블루투스를 통해 물체의 방향(가속도계와 자이로스코프 측정값을 Madgwick 필터와 융합하여 얻음)과 회전 속도를 수신했습니다. 이러한 측정값은 정책에 사용되지 않았으며 성능 평가에만 사용되었습니다. 회전 속도는 로봇의 프레임으로 변환하고 지수 평활화 필터로 평활화하여 측정값에서 노이즈를 제거했습니다. 결과는 그림 6에 나와 있습니다. 회색 줄은 물체 회전 보상이 최대값인 영역, 즉 정책의 &quot;대상 영역&quot;을 나타냅니다. 시뮬레이션 내에서 물체의 각속도 평균은 DR 정책과 비 DR 정책에서 비슷합니다. 그러나 DR 정책은 속도에서 더 큰 분포를 보이는데, 아마도 무작위 물리 매개변수로 인한 성능의 분산 때문일 것입니다. 실제 로봇에 적용하면 DR의 중요성이 분명해집니다. DR이 아닌 정책은 구를 회전시키지 못하고 손에서 앞뒤로 흔들기만 하기 때문입니다. DR 정책은 공을 일관되게 회전시키는 데 성공하여 대부분의 측정에서 목표 회전 속도를 달성합니다. 역회전 방향으로 학습한 정책도 실제 로봇에 적용했는데, 샘플링된 측정의 대부분에서 다시 목표 속도 내로 공을 회전시키는 데 성공했습니다. 그림 1은 로봇에서 실행되는 정책의 2초 간격으로 촬영한 세 개의 스냅샷을 보여 주고, 첨부된 비디오는 실제 로봇에서 실행되는 각 정책의 비디오도 보여줍니다. V.
--- CONCLUSION ---
자율 조작에 사용할 수 있는 인간형 손 플랫폼을 소개했습니다. 이 플랫폼의 경우 롤링 접촉 조인트를 모델링, 제어 및 감지하는 방법을 개발하여 병렬화된 시뮬레이션 환경에 통합하여 폐쇄 루프 정책을 훈련할 수 있습니다. 훈련된 정책을 실제 로봇 손에서 실행하여 능숙한 구 회전을 달성할 수 있음을 보여줍니다. 능숙한 조작을 위한 RL 훈련을 위한 시뮬레이터와 환경이 최근에 더욱 유능하고 접근 가능해졌지만, 5개 손가락 생체 모방 로봇 손에서는 아직 동일한 것을 보지 못했습니다. Soft Robotics Lab에서 개발한 Faive Hand는 능숙한 조작기를 더욱 유능하고 접근 가능하게 만드는 것을 목표로 합니다. 이 작업에서 손이 IsaacGym 시뮬레이터에서 RL로 훈련한 기술의 제로샷 전송을 달성할 수 있음을 보여주며, 이 손이 RL로 훈련된 다른 작업에 사용될 수 있는 잠재력을 보여줍니다. 그러나 소프트웨어와 하드웨어에는 여전히 한계가 있습니다. [1]과 같은 큐브 재배향 작업을 적용하려고 했을 때 정책은 시뮬레이션에서는 작동했지만 실제 로봇에서는 실패했습니다. 이 동작은 물체가 구와 대칭이 아니고 세 축을 중심으로 회전해야 하기 때문에 단일 축 구 회전 작업보다 더 복잡합니다. 실제 로봇의 실패는 EKF의 관절 각도 측정 불량(특히 접촉 시) 및 IsaacGym 시뮬레이터에서 정확한 작동 역학을 보장하기 위한 적절한 시스템 식별 부족과 같은 여러 요인에 기인합니다. 고유 감각 측정을 위한 더 많은 센서 통합 또는 sim2real 갭을 줄이기 위한 더 나은 시스템 식별 프로세스와 같은 물리적 및 프로그램적 접근 방식을 결합하여 이러한 과제를 해결하기 위해 로봇 손을 계속 개발할 것입니다. 감사의 말 저자는 롤링 접촉 관절 손가락의 두 손가락 프로토타입을 만든 Jonas Lauener에게 감사드립니다. Yasunori Toshimitsu는 Takenaka Scholarship Foundation, Max Planck ETH Center for Learning Systems 및 Swiss Government Excellence Scholarship에서 부분적으로 자금을 지원받았습니다. 이 연구는 Amazon Research Awards에서 부분적으로 자금을 지원받았습니다. 본 연구는 또한 ETH 취리히 재단의 지원을 받은 ETH RobotX 연구 보조금으로 수행되었습니다. 참고문헌 [1] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, W. Zaremba, &quot;손재주 있는 손 조작 학습&quot;, 2018년 8월. [2] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, Gavriel State, &quot;Isaac gym: 로봇 학습을 위한 고성능 GPU 기반 물리 시뮬레이션&quot;, 2021년 11월. [3] N. Rudin, D. Hoeller, P. Reist, M. Hutter, 영어: “대량 병렬 딥 강화 학습을 사용하여 몇 분 안에 걷는 법 배우기”, 제5회 로봇 학습 컨퍼런스 논문집, ser. 기계 학습 연구 논문집, A. Faust, D. Hsu, G. Neumann 편집, vol. 164. PMLR, 2022, pp. 91–100. [4] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk, K. Van Wyk, A. Zhurkevich, B. Sundaralingam, Y. Narang, J.-F. Lafleche, D. Fox 및 Gavriel State, &quot;DeXtreme: 시뮬레이션에서 현실로 민첩한 핸드 조작 전환&quot;, 2022년 10월. [5] T. Chen, J. Xu 및 P. Agrawal, &quot;일반적인 핸드 객체 재조정을 위한 시스템&quot;, 제5회 로봇 학습 컨퍼런스 회의록, ser. A. Faust, D. Hsu 및 G. Neumann 편집, 기계 학습 연구 회의록, vol. 164. PMLR, 2022, pp. 297–307. [6] Z.-H. Yin, B. Huang, Y. Qin, Q. Chen 및 X. Wang, &quot;보지 않고 회전: 촉각을 통한 핸드 민첩성으로&quot;, 2023년 3월. [7] A. Allshire, M. Mittal, V. Lodaya, V. Makoviychuk, D. Makoviichuk, F. Widmaier, M. Wüthrich, S. Bauer, A. Handa, and A. Garg, “Transferring dexterous Manipulation from GPU Simulation to a remote real-world TriFinger,” in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct. 2022, pp. 11 802-11 809. [8] W. Robotics, &quot;Allegro hand: Highly adaptive robotic hand for r&amp;d,&quot; 2023. [온라인]. 제공: https://www.wonikrobotics.com/ research-robot-hand [9] SR Company, &quot;Shadow dexterous hand series research and development tool,&quot; 2023. [온라인]. 제공: https://www. shadowrobot.com/dexterous-hand-series/ [10] K. Shaw, A. Agarwal 및 D. Pathak, “LEAP 손: 로봇 학습을 위한 저렴하고 효율적이며 인간형 손,&quot; https://www. roboticsproceedings.org/rss19/p089.pdf, 접속: 2023-7-11. [11] N. Gürtler, F. Widmaier, C. Sancaktar, S. Blaes, P. Kolev, S. Bauer, M. Wüthrich, M. Wulfmeier, M. Riedmiller, A. Allshire, Q. Wang, R. McCarthy, H. Kim, JB Pohang, W. Kwon, S. Qian, Y. Toshimitsu, MY Michelis, A. Kazemipour, A. Raayatsanati, H. Zheng, BG Cangan, B. Schölkopf 및 G. Martius, &quot;Real robot challenge 2022: Learning dexterous Manipulation from Offline data in the real world,&quot; 2023년 8월. [12] M. Wuthrich, F. Widmaier, F. Grimminger, S. Joshi, V. Agrawal, B. Hammoud, M. Khadiv, M. Bogdanovic, V. Berenz, J. Viereck, M. Naveau, L. Righetti, B. Schölkopf, and S. Bauer, &quot;TriFinger: An open-source robot for learning dexterity,&quot; in Proceedings of theConference on Robot Learning, ser. Proceedings of Machine Learning Research, J. Kober, F. Ramos, and C. Tomlin, Eds., vol. 155. PMLR, 2021, pp. 1871-1882. [13] F. Shi, T. Homberger, J. Lee, T. Miki, M. Zhao, F. Farshidian, K. Okada, M. Inaba, M. Hutter, &quot;Circus ANYmal: 사지로 능숙한 조작을 배우는 사족보행자&quot;, 2021 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA). IEEE, 2021년 5월. [14] JK Salisbury 및 JJ Craig, &quot;관절 손: 힘 제어 및 운동학적 문제&quot;, Int. J. Rob. Res., vol. 1, no. 1, pp. 4-17, 1982년 3월. [15] K. Kawaharazuka, S. Makino, M. Kawamura, S. Nakashima, Y. Asano, K. Okada, and M. Inaba, &quot;인간의 숙련된 동작을 위한 방사척골관절과 유연한 기계 가공 스프링 손가락을 갖춘 인간 모방 전완 및 손 설계,&quot; Journal of Robotics and Mechatronics, vol. 32, no. 2, pp. 445-458, 2020. [16] S. Makino, K. Kawaharazuka, A. Fujii, M. Kawamura, T. Makabe, M. Onitsuka, Y. Asano, K. Okada, K. Kawasaki, and M. Inaba, &quot;기계 가공 스프링과 가변 강성 관절의 조합을 사용한 광범위한 엄지 손가락이 있는 5개 손가락 손,&quot; 2018 IEEE/RSJ 국제 컨퍼런스 영어: 지능형 로봇 및 시스템(IROS), 2018년 10월, 4562-4567쪽. [17] S. Puhlmann, J. Harris, 및 O. Brock, &quot;RBO hand 3: 소프트한 손재주 조작을 위한 플랫폼,&quot; IEEE Trans. Rob., vol. 38, no. 6, pp. 3434-3449, 2022년 12월. [18] C. Schlagenhauf, D. Bauer, K.-H. Chang, JP King, D. Moro, S. Coros, 및 N. Pollard, &quot;힘줄 구동 소프트 폼 로봇 손의 제어,&quot; 2018년 IEEE-RAS 제18회 휴머노이드 로봇(Humanoids) 국제 컨퍼런스, 2018년 11월, 1-7쪽. [19] S. Kim, E. Sung, and J. Park, “ARC joint: Anthropomorphic rolling contact joint with kinematically variable torsional stiffness,&quot; IEEE Robotics and Automation Letters, vol. 8, no. 3, pp. 1810–1817, 2023년 3월. [20] Y.-J. Kim, J. Yoon, and Y.-W. Sim, &quot;인간과 같은 충격 흡수 기능을 위한 유체 윤활식 능숙한 손가락 메커니즘,&quot; IEEE Robot. Autom. Lett., vol. 4, no. 4, pp. 3971-3978, 2019년 10월. [21] SW Hong, J. Yoon, Y.-J. Kim, and HS Gong, &quot;최적화된 롤링 접촉 관절 메커니즘을 사용한 근위지절관절의 새로운 임플란트 설계,&quot; J. Orthop. Surg. Res., vol. 14, no. 1, p. 212, 2019년 7월. [22] 김승현, 인현철, 송재림, 김경진. Cho, “콤팩트 구조를 위한 롤링 접촉 조인트의 힘 특성,&quot; 2016년 6월 제6회 IEEE 생체의학 로봇 및 생체메카트로닉스 국제회의(BioRob), pp. 1207-1212. [23] S. Ookubo, Y. Asano, T. Kozuki, T. Shirai, K. Okada, M. Inaba, &quot;기하학적 모델이 없는 힘줄 구동 근골격계 로봇을 향한 비선형 근육-관절 상태 매핑 학습,&quot; 2015년 IEEE-RAS 제15회 휴머노이드 로봇 국제회의(휴머노이드), 11월 제2015년, pp. 765-770. [24] A. Meurer, CP Smith, M. Paprocki, O. Čertík, SB Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, JK Moore, S. Singh, T. Rathnayake, S. Vig, BE Granger, RP Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, MJ Curry, AR Terrel, v. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, A. Scopatz, &quot;Sympy: python에서의 심볼릭 컴퓨팅,&quot; PeerJ Computer Science, vol. 3, p. e103, 2017년 1월. [온라인]. 사용 가능: https://doi.org/10.7717/peerj-cs.[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, &quot;근위 정책 최적화 알고리즘,&quot; 2017년 7월. [26] D. Makoviichuk 및 V. Makoviychuk, &quot;rl-games: A 강화 학습을 위한 고성능 프레임워크,&quot; https://github.com/Denys88/rl_ games, 2021년 5월.
