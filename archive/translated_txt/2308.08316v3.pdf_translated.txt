--- ABSTRACT ---
최근 확산 모델이 등장하면서 텍스트-비디오 생성에 대한 관심이 커지고 있습니다. 하지만 여기서 중요한 병목 현상은 생성 비디오가 종종 깜빡임과 아티팩트를 포함하는 경향이 있다는 것입니다. 이 연구에서는 비디오 생성 시 콘텐츠 변형의 일관성을 개선하기 위해 듀얼 스트림 확산 네트(DSDN)를 제안합니다. 특히, 설계된 두 확산 스트림인 비디오 콘텐츠와 모션 브랜치는 개인화된 비디오 변형과 콘텐츠를 생성하기 위해 개인 공간에서 별도로 실행될 수 있을 뿐만 아니라 설계된 교차 변환기 상호 작용 모듈을 활용하여 콘텐츠와 모션 도메인 간에 잘 정렬될 수 있으며, 이는 생성된 비디오의 부드러움에 도움이 될 것입니다. 또한 비디오 모션 작업을 용이하게 하기 위해 모션 분해기와 결합기도 도입했습니다. 정성적 및 정량적 실험은 우리의 방법이 깜빡임이 적은 놀라운 연속 비디오를 생성할 수 있음을 보여줍니다(그림 1 참조). 보충 자료의 비디오를 참조하세요.
--- INTRODUCTION ---
인공 지능이 생성한 콘텐츠의 영역에서 가장 흥미롭고 도전적인 작업 중 하나는 텍스트에서 시각적 콘텐츠로 변환하는 것입니다. 이 작업은 자연어 처리에 대한 이해에 도움이 될 뿐만 아니라 컴퓨터 비전 기술을 촉진합니다. 한편, 엔터테인먼트, 광고, 교육 및 감시 분야에서 엄청난 잠재적 응용 프로그램을 만들어 낼 것입니다. 지난 몇 년 동안 텍스트 설명을 이미지로 변환하는 모델 개발에 상당한 진전이 있었습니다(Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022). 이미지와 달리 비디오는 더 풍부한 콘텐츠를 담고 표현할 수 있습니다. 텍스트 설명의 경우 비디오는 복잡한 내러티브를 잘 포착하고 전달할 수 있습니다(Bain et al. 2021; Smaira et al. 2020). 최근 텍스트-비디오 생성은 확산 모델의 증가와 함께 점점 더 많은 관심을 받았습니다(코드 포함 정보는 익명 웹사이트에서 찾을 수 있음: https://anonymous.4open.science/r/Private-C3E). 한 가지 중요한 과제는 이미지 기반 확산 모델에서 학습한 공간적 콘텐츠뿐만 아니라 비디오의 공간-시간 영역에서 합리적이고 연속적인 콘텐츠 생성에 있습니다. 현재 텍스트-비디오 생성의 기존 방법(Blattmann et al. 2023; Hong et al. 2022; Singer et al. 2022)은 주로 텍스트에서 시각적 콘텐츠를 재생하는 데 초점을 맞추었습니다. 비디오 역학을 모델링하는 데 부족하기 때문에 생성된 비디오에는 종종 많은 깜빡임이 포함되고 시각적 효과가 간헐적입니다. 이 문제를 해결하기 위해 이 작업에서 우리의 목표는 생성된 비디오의 모션 다양성을 증가시키는 동시에 이러한 비디오 프레임 간의 모션과 콘텐츠의 일관성을 높여 시각적으로 더 나은 연속 비디오를 생성하는 것입니다. 이를 위해 여기에서는 비디오 생성 시 콘텐츠 변형의 일관성을 높이기 위한 듀얼 스트림 확산 네트(DSDN)를 제안합니다. 특히 비디오의 모션 정보를 특성화하기 위해 비디오 콘텐츠의 분기 외에도 비디오 콘텐츠 변형을 인코딩하는 모션 분기를 도입합니다. 여기서, 우리는 비디오 모션 스트림과 비디오 콘텐츠 스트림인 2개 분기 확산 네트워크를 구성합니다. 대규모 이미지 생성 모델을 완성하기 위해 콘텐츠 스트림은 사전 훈련된 텍스트-이미지 조건부 확산 모델에서 실행되지만, 그 사이에 개인화된 비디오 콘텐츠 생성을 위한 병렬 네트워크로 점진적으로 업데이트됩니다. 병렬 단계에서 비디오 프레임의 변형, 즉 모션은 3D-UNet을 사용하여 별도의 확률 확산 프로세스를 거쳐 개인화된 모션 정보를 생성할 수 있습니다. 생성된 콘텐츠와 모션을 정렬하기 위해 두 스트림 간의 교차 주의를 사용하여 이중 스트림 변환 상호 작용 모듈을 설계합니다. 그에 따라 모션 스트림은 노이즈 제거 프로세스 동안 콘텐츠 스트림과 통합되어 각 스트림이 다른 스트림의 맥락적 정보 역할을 할 수 있습니다. 또한 모션 분해기와 결합기를 도입하여 모션에 대한 작업을 용이하게 합니다. 우리는 정성적, 정량적 실험 검증을 수행하였고, 실험은 우리의 방법이 그림 1에서 보인 것처럼 시각적으로 연속적인 비디오를 더 잘 생성할 수 있음을 보여줍니다. 마지막으로, 텍스트-비디오 생성 영역에 대한 기여를 간략히 요약합니다. i) 생성된 비디오의 일관성과 다양성을 향상시키기 위한 듀얼 스트림 확산 네트(DSDN)를 제안합니다. 여기서 동작은 대부분의 기존 비디오 확산 방법과 구별되는 단일 브랜치로 특별히 모델링됩니다. ii) 개인화된 콘텐츠/동작 생성, 듀얼 스트림 변환 상호 작용을 포함한 몇 가지 유용한 모듈을 설계하여 생성된 샘플의 다양성을 보존하면서 콘텐츠와 동작을 정렬합니다. iii) 정성적, 정량적 평가는 DSDN이 놀라운 일관성과 다양성을 갖춘 비디오를 효과적으로 생성할 수 있음을 보여줍니다.
--- RELATED WORK ---
텍스트 설명을 시각적 콘텐츠로 변환하기 위한 모델의 개발 및 진화는 인공 지능 분야에서 지속적으로 중점적으로 다루어졌습니다. 연구는 점차 텍스트-이미지 모델에서 보다 역동적이고 복잡한 텍스트-비디오 생성 모델로 전환되었습니다. 텍스트-이미지 생성 초기에는 텍스트-이미지 합성 기술을 개발하는 데 주력했습니다. 노이즈 제거 확산 확률적 모델(DDPM)(Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2022)은 고품질 이미지를 생성하는 놀라운 능력으로 인해 상당한 주목을 받았습니다. 이 혁신적인 모델은 이전의 생성적 적대 신경망(GAN)(Goodfellow et al. 2020)의 성능을 능가하여 이 분야에서 새로운 벤치마크를 설정했습니다. 또한 DDPM에는 고유한 기능이 있습니다. 텍스트 안내를 통해 학습할 수 있어 사용자가 텍스트 입력에서 이미지를 생성할 수 있습니다. 이 분야에서 주목할 만한 발전이 몇 가지 이루어졌습니다. 예를 들어, GLIDE(Nichol et al. 2021)는 분류자 없는 안내를 채택하고 대규모 텍스트-이미지 쌍을 사용하여 확산 모델을 훈련합니다. DALLE-2(Ramesh et al. 2022)는 CLIP(Radford et al. 2021) 잠재 공간을 조건으로 사용하여 성능을 크게 향상시킵니다. Imagen(Saharia et al. 2022)은 T5(Raffel et al. 2020)를 계단식 확산 모델과 결합하여 고해상도 이미지를 생성합니다. 잠재 확산 모델(LDM)(Ramesh et al. 2022)은 잠재 공간에서 확산 프로세스를 전달하여 다른 확산 모델보다 더 높은 효율성을 보여줍니다. 텍스트-비디오 생성 텍스트-이미지 모델의 이러한 발전에도 불구하고 텍스트-비디오 합성으로의 전환은 주로 비디오 프레임 간의 시간적 종속성과 비디오 시퀀스 전체에서 동작 의미론을 유지해야 하는 필요성으로 인해 새로운 과제를 안겨주었습니다. 이와 관련된 초기 작업에는 GAN 기반
--- METHOD ---
. 초록 최근 확산 모델이 등장하면서 텍스트-비디오 생성이 점점 더 주목을 받고 있습니다. 하지만 여기서 중요한 병목 현상은 생성 비디오가 종종 깜빡임과 아티팩트를 포함하는 경향이 있다는 것입니다. 이 연구에서 우리는 비디오 생성 시 콘텐츠 변형의 일관성을 개선하기 위해 듀얼 스트림 확산 네트(DSDN)를 제안합니다. 특히, 설계된 두 확산 스트림인 비디오 콘텐츠와 모션 브랜치는 개인화된 비디오 변형과 콘텐츠를 생성하기 위해 개인 공간에서 별도로 실행될 수 있을 뿐만 아니라 설계된 교차 변환기 상호 작용 모듈을 활용하여 콘텐츠와 모션 도메인 간에 잘 정렬될 수 있으며, 이는 생성된 비디오의 부드러움에 도움이 될 것입니다. 또한 비디오 모션 작업을 용이하게 하기 위해 모션 분해기와 결합기도 도입합니다. 정성적 및 정량적
--- EXPERIMENT ---
s는 우리 방법이 깜빡임이 적은 놀라운 연속 비디오를 생성할 수 있음을 보여줍니다(그림 1 참조). 보충 자료의 비디오를 보고 더 많은 정보를 얻으십시오. 소개 인공 지능이 생성한 콘텐츠의 영역에서 가장 흥미롭고 도전적인 작업 중 하나는 텍스트에서 시각적 콘텐츠로 변환하는 것입니다. 이 작업은 자연어 처리에 대한 이해에 도움이 될 뿐만 아니라 컴퓨터 비전 기술을 촉진합니다. 한편, 엔터테인먼트, 광고, 교육 및 감시 분야에서 엄청난 잠재적 응용 프로그램을 가져올 것입니다. 지난 몇 년 동안 텍스트 설명을 이미지로 변환하는 모델 개발에 상당한 진전이 있었습니다(Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022). 이미지와 달리 비디오는 더 풍부한 콘텐츠를 담고 표현할 수 있습니다. 텍스트 설명의 경우 비디오는 그 안에서 복잡한 내러티브를 잘 포착하고 전달할 수 있습니다(Bain et al. 2021; Smaira et al. 2020). 최근 텍스트-비디오 생성은 점점 더 주목을 받고 있으며, 특히 확산 모델의 부상과 함께 나타났습니다. 중요한 과제 중 하나는 이미지 기반 확산 모델에서 학습한 공간적 콘텐츠뿐만 아니라 비디오의 시공간적 영역에서 합리적이고 지속적인 콘텐츠를 생성하는 것입니다. 현재 텍스트-비디오 생성의 기존 방법(Blattmann et al. 2023; Hong et al. 2022; Singer et al. 2022)은 주로 텍스트에서 시각적 콘텐츠를 재생하는 데 초점을 맞추었습니다. 비디오 역학을 모델링하는 데 부족하기 때문에 생성된 비디오에는 종종 많은 깜빡임이 포함되고 시각적 효과가 간헐적입니다. 이 문제를 해결하기 위해 이 작업에서 우리의 목표는 생성된 비디오의 모션 다양성을 증가시키는 동시에 이러한 비디오 프레임 간의 모션과 콘텐츠의 일관성을 높여 시각적으로 더 나은 연속 비디오를 생성하는 것입니다. 이를 위해 여기서는 비디오 생성 시 콘텐츠 변형의 일관성을 높이기 위한 듀얼 스트림 확산 네트(DSDN)를 제안합니다. 특히 비디오의 모션 정보를 특성화하기 위해 비디오 콘텐츠의 분기 외에도 비디오 콘텐츠 변형을 인코딩하기 위한 모션 분기를 도입합니다. 이를 통해 비디오 모션 스트림과 비디오 콘텐츠 스트림이라는 두 분기 확산 네트워크를 구성합니다. 대규모 이미지 생성 모델을 완성하기 위해 콘텐츠 스트림은 사전 학습된 텍스트-이미지 조건부 확산 모델에서 실행되지만, 그동안 개인화된 비디오 콘텐츠 생성을 위한 병렬 네트워크로 점진적으로 업데이트됩니다. 병렬 단계에서 비디오 프레임의 변형, 즉 모션은 3D-UNet을 사용하여 별도의 확률 확산 프로세스를 거쳐 개인화된 모션 정보를 생성할 수 있습니다. 생성된 콘텐츠와 모션을 정렬하기 위해 두 스트림 간의 교차 주의를 사용하여 듀얼 스트림 변환 상호 작용 모듈을 설계합니다. 따라서 모션 스트림은 노이즈 제거 프로세스 중에 콘텐츠 스트림과 통합되어 각 스트림이 다른 스트림에 대한 맥락적 정보 역할을 할 수 있습니다. 또한, 동작에 대한 연산을 용이하게 하기 위해 동작 분해기와 결합기를 도입합니다. 정성적, 정량적 실험 검증을 수행하였고, 실험 결과, 그림 1에서 보듯이, 저희 방법이 시각적으로 연속적인 영상을 더 잘 생성할 수 있음을 보여줍니다. 마지막으로, 텍스트-비디오 생성 분야에 대한 기여를 간략히 요약합니다. i) 동작이 대부분의 기존 영상 확산 방법과 구별되는 단일 분기로 특별히 모델링되는 생성된 영상의 일관성과 다양성을 향상시키기 위한 듀얼 스트림 확산 네트(DSDN)를 제안합니다. ii) 개인화된 콘텐츠/동작 생성, 듀얼 스트림 변환 상호 작용을 포함한 몇 가지 유용한 모듈을 설계하여 생성된 샘플의 다양성을 유지하면서 콘텐츠와 동작을 정렬합니다. iii) 정성적, 정량적 평가 결과, DSDN이 놀라울 정도로 일관성 있고 다양한 영상을 효과적으로 생성할 수 있음을 보여줍니다. 관련 연구 텍스트 설명을 시각적 콘텐츠로 변환하기 위한 모델의 개발과 발전은 인공지능 분야에서 꾸준히 집중되어 왔습니다. 연구는 점차 텍스트-이미지 모델에서 보다 역동적이고 복잡한 텍스트-비디오 생성 모델로 전환되었습니다.텍스트-이미지 생성 초기에는 텍스트-이미지 합성 기술을 개발하는 데 주력했습니다.잡음 제거 확산 확률적 모델(DDPM)(Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2022)은 고품질 이미지를 생성하는 놀라운 능력으로 인해 상당한 주목을 받았습니다.이 혁신적인 모델은 이전의 생성적 적대 신경망(GAN)(Goodfellow et al. 2020)의 성능을 능가하여 이 분야에서 새로운 벤치마크를 설정했습니다.또한 DDPM은 텍스트 안내로 학습할 수 있다는 고유한 기능이 있어 사용자가 텍스트 입력에서 이미지를 생성할 수 있습니다.이 분야에서 주목할 만한 발전이 몇 가지 이루어졌습니다.예를 들어, GLIDE(Nichol et al. 2021)는 분류기 없는 안내를 채택하고 대규모 텍스트-이미지 쌍을 사용하여 확산 모델을 학습합니다. DALLE-2(Ramesh et al. 2022)는 CLIP(Radford et al. 2021) 잠재 공간을 조건으로 사용하여 성능을 크게 향상시킵니다.Imagen(Saharia et al. 2022)은 T5(Raffel et al. 2020)를 계단식 확산 모델과 결합하여 고해상도 이미지를 생성합니다.잠재 확산 모델(LDM)(Ramesh et al. 2022)은 잠재 공간에서 확산 프로세스를 전달하여 다른 확산 모델보다 더 높은 효율성을 보여줍니다.텍스트-비디오 생성 텍스트-이미지 모델의 이러한 발전에도 불구하고 텍스트-비디오 합성으로의 전환은 주로 비디오 프레임 간의 시간적 종속성과 비디오 시퀀스 전체에서 동작 의미론을 유지해야 하는 필요성으로 인해 새로운 과제를 제시했습니다. 이와 관련된 초기 작업에는 GAN 기반 방법(Vondrick et al. 2016; Clark et al. 2019)과 자기 회귀 방법(Kalchbrenner et al. 2017; Hong et al. 2022)이 포함됩니다. 무조건 비디오 생성의 맥락에서 Ho et al.(Ho et al. 2022)은 원래 이미지를 위해 설계된 DDPM 모델을 비디오 도메인으로 성공적으로 확장하여 3D U-Net 아키텍처를 개발했습니다. Harvey et al.(Harvey et al. 2022)은 자기 회귀 방식으로 후속 비디오 프레임의 분포를 모델링하는 혁신적인 접근 방식을 제시했습니다. 그러나 우리의 주요 초점은 제어 가능한 방식으로 비디오를 합성하는 데 있습니다. 더 구체적으로는 텍스트 조건부 비디오 생성에 있습니다. 이 경로를 탐색하면서 Hong et al. (Hong et al. 2022)은 주어진 텍스트와 이전 프레임에 조건을 부여하여 비디오 시퀀스를 모델링하는 자기 회귀 프레임워크인 CogVideo를 제안했습니다.마찬가지로 Levon et al. (Khachatryan et al. 2023)은 텍스트-이미지 모델 안정 확산(Rombach et al. 2022)을 기반으로 하는 텍스트-비디오 생성 방법인 Text2Video-Zero를 제안했는데, 이는 텍스트-비디오를 직접 생성할 뿐만 아니라 이미지 편집 작업도 직접 완료할 수 있습니다.텍스트-비디오 도메인의 현재 문제는 생성 비디오가 종종 깜빡임과 아티팩트를 포함하는 경향이 있다는 것입니다.비디오의 시각적 측면과 동적 측면을 모두 포착하려는 시도에는 Ni et al. (Ni et al. 2023)이 제안한 잠재 스트림 확산 모델과 공간적 및 시간적 정보 흐름을 통합하여 긴 비디오를 생성하기 위한 et al. (Yu et al. 2023)의 투영 잠재 비디오 확산 모델이 있습니다. 이것들은 텍스트 설명으로부터 고품질 이미지를 생성하는 것과 같은 작업에서 성공적으로 사용되었지만(Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022), 텍스트로부터 동적 비디오를 생성하는 잠재력은 여전히 크게 활용되지 않았습니다. 저희의 작업은 이러한 이전 연구 노력에서 영감을 얻었으며 기존 모델에서 흔히 볼 수 있는 함정을 해결하고자 합니다. 저희는 비디오 생성 시 콘텐츠 변형의 일관성을 개선하기 위해 새로운 듀얼 스트림 확산 네트워크를 소개합니다. 방법 이 섹션에서는 먼저 네트워크에 대한 개요를 제공한 다음 그 안의 세부 정보를 설명합니다. 1:L =1:L 개요 제안된 DSDN 네트워크 아키텍처는 그림 2에 나와 있습니다. 처음에 입력 비디오 x1:L은 프레임별 인코더 ε를 통해 잠재 공간으로 투영됩니다. 이는 E(x1)로 표시되며 여기서 L은 비디오의 길이입니다. 시간적 역학 없이 프레임별 인코딩만 하기 때문에 를 콘텐츠 특징으로 부릅니다. 더 나은 비디오 생성을 위해 이러한 시간적 단서를 채굴하기 위해 L로 표시되는 해당 모션 정보를 추출하는 모션 분해기를 도입합니다. 두 잠재 특징 zl과 L을 입력으로 사용하여 듀얼 스트림 확산 방식을 사용하여 개인화된 비디오 콘텐츠와 모션 변형을 생성한 다음, 두 구성 요소를 통합하여 비디오를 생성하는 변형된 상호 작용 방식을 제안합니다. 1:L 1:L 1:L 듀얼 스트림 확산 단계에서 두 가지 유형의 잠재 특징은 별도의 전방 확산 프로세스(FDP)를 통해 표준 가우시안 사전으로 변환됩니다. 그런 다음 콘텐츠 특징 사전은 개인화된 콘텐츠 생성 스트림(PCGS)을 따라 노이즈 제거를 거치며, 그 결과 1:Ĺ 순수 노이즈 제거된 콘텐츠 특징 2017이 생성됩니다. 마찬가지로 모션 특징 사전은 개인화된 모션 생성 스트림에 의해 노이즈 제거되며, 그 결과 순수 노이즈 제거된 모션 특징 Z가 생성됩니다. 생성된 콘텐츠와 모션을 추가로 정렬하여 깜빡임을 억제하기 위해 두 가지 유형의 생성 스트림을 연결하는 듀얼 스트림 변환 상호 작용 모듈을 설계합니다. 정렬 학습 후, 모션 합성기를 사용하여 비디오 콘텐츠에 동적 정보를 보상하고, 마지막으로 대상 비디오의 잠재 특징을 형성한 다음, 디코더 D를 사용하여 픽셀 공간에서 비디오를 생성합니다.전방 확산 프로세스 리소스 소비를 줄이기 위해, Denoising Diffusion Probabilistic Models(DDPM)(Ho, Jain, and Abbeel 2020)에 기반한 잠재 확산 모델(Rombach et al. 2022)과 유사한 방식을 취합니다.확산 전에, 사전 학습된 벡터 양자화 변분 자동 인코더(VQ-VAE)(van den Oord, Vinyals, and Kavukcuoglu 2018)를 사용하여 비디오 프레임을 잠재 특징 공간, 즉 21:1 L = E(x)에 투사합니다.단순화를 위해, 학습하는 동안 인코더 &amp;를 동결했습니다. 콘텐츠 특징 21:L은 동작 분해기를 통해 처리되어(다음 부분 참조: 동작 분해 및 결합) 동작 특징 21을 얻습니다.두 부분의 특징은 사전 정의된 마르코프 프로세스를 통해 노이즈 교란을 받습니다.형식적으로, q(zt|Zt−1) = N(zt; √√1 − ßtzt−1, ßtI), q′(žt|žt−1) = N(žt; √√1 – ßtžt−1, ßtI), (1) 여기서 t = 1, T이고 T는 확산 단계의 수입니다.... ẞt는 각 반복 단계에서의 노이즈 강도를 정의합니다.두 스트림에 대한 공유 노이즈 일정이 우리의 경험상 잘 작동한다는 점에 주목할 가치가 있습니다. DDPM에 따르면, 위의 재귀 공식은 다음과 같은 축약된 버전으로 도출될 수 있다.1:L 1: L = 1:L √√ātz + √√1-āƒ€1, €1 ~ N(0, I), = √√ā₁ž₁:L + √√1 − āt€2, €2 ~ N(0,1), (2) 여기서 at = 1 at, at = 1 − ßt. 지금까지 콘텐츠와 동작 특징에 대한 순방향 확산 프로세스를 성공적으로 완료했다.이를 통해 사전 확률 1:L을 얻을 수 있으며, 이는 이어지는 노이즈 제거 프로세스를 구동하는 데 도움이 된다.1:LT 개인화된 콘텐츠 생성 스트림 잘 훈련된 이미지 기반 확산 모델을 활용하기 위해 대규모 텍스트-이미지 모델인 안정적 확산(Rombach et al. 2022)을 비디오 콘텐츠 생성의 기본 모델로 활용한다. 그러나 개인화된 비디오 콘텐츠 생성을 더 잘 지원하기 위해 LORA(Hu et al. 2021)와 유사한 방식을 사용하여 콘텐츠 생성을 개선하는 증분 학습 모듈을 설계합니다. 그림 2에서 볼 수 있듯이 각각 콘텐츠 기본 단위와 콘텐츠 증가 단위로 지칭합니다. 두 단위의 모델 매개변수는 콘텐츠 기능을 강화하기 위해 적응적으로 통합됩니다. 이러한 방식은 대규모 이미지 기반 생성 모델의 장점을 계승할 뿐만 아니라 고유하고 개인화된 콘텐츠 생성을 부여하여 방법의 전반적인 개선에 기여합니다. 구체적으로 콘텐츠 기본 단위는 수정된 UNet 아키텍처를 사용하며, 각 해상도 레벨은 셀프 어텐션 및 크로스 어텐션 메커니즘이 있는 2D 합성곱 계층을 통합합니다. 동시에 콘텐츠 증가 단위는 미세 조정을 위한 몇 가지 조정 가능한 매개변수가 있는 추가 네트워크 분기를 사용합니다. 기본 단위가 매개변수 W를 갖는 것으로 가정하면 사후 조정된 가중치는 다음과 같습니다. W&#39; = W +λAW, 여기서 AW는 업데이트 수량이고 \는 스텝 길이입니다. 하이퍼 파라미터 \는 튜닝 프로세스가 미치는 영향을 지시하여 사용자에게 생성 결과에 대한 광범위한 제어를 제공합니다. 잠재적인 과적합을 방지하고 계산 오버헤드를 줄이기 위해 AW = Rmxn은 LORA(Hu et al. 2021)에서 사용되는 것처럼 두 개의 저랭크 행렬로 분해됩니다. AW = ABT로 표시하겠습니다. 여기서 A Є Rmxr, BERnxr, r
--- CONCLUSION ---
이 연구는 비디오 생성 시 콘텐츠 변형의 일관성을 개선하기 위해 새로운 듀얼 스트림 확산 네트(DSDN)를 제시했습니다. 구체적으로, 설계된 두 확산 스트림인 비디오 콘텐츠와 동작 분기는 개인화된 비디오 변형과 콘텐츠를 생성하기 위해 각자의 개인 공간에서 별도로 실행될 수 있을 뿐만 아니라 설계된 교차 변환기 상호 작용 모듈을 활용하여 콘텐츠와 동작 도메인 간에 잘 정렬될 수 있습니다. 이는 생성된 비디오의 부드러움에 도움이 되고 생성된 프레임의 일관성과 다양성을 향상시킵니다. 동작은 대부분의 기존 비디오 확산 방법과 구별되는 단일 분기로 특별히 모델링됩니다. 또한 비디오 동작에 대한 작업을 용이하게 하기 위해 동작 분해기와 결합기도 도입했습니다. 정성적 및 정량적 실험을 통해 저희 방법이 깜빡임이 적은 더 나은 연속 비디오를 생성한다는 것이 입증되었습니다. 동작 단위 참조 없음 Bain, M.; Nagrani, A.; Varol, G.; 및 Zisserman, A. 2021. Frozen in Time: 종단 간 검색을 위한 공동 비디오 및 이미지 인코더. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록. Blattmann, A.; Rombach, R.; Ling, H.; Dockhorn, T.; Kim, SW; Fidler, S.; 및 Kreis, K. 2023. 잠재체 정렬: 잠재 확산 모델을 사용한 고해상도 비디오 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 2256322575. Civitai. 2022. Civitai. https://civitai.com/. Clark, A.; Donahue, J.; 및 Simonyan, K. 2019. 복잡한 데이터 세트에 대한 적대적 비디오 생성. arXiv 사전 인쇄본 arXiv:1907.06571. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; 및 Bengio, Y. 2020. 생성적 적대적 네트워크. ACM 커뮤니케이션. Guo, Y.; Yang, C.; Rao, A.; Wang, Y.; Qiao, Y.; Lin, D.; 및 Dai, B. 2023. AnimateDiff: 특정 튜닝 없이 개인화된 텍스트-이미지 확산 모델에 애니메이션을 적용합니다. arXiv 사전 인쇄본 arXiv:2307.04725. Harvey, W.; Naderiparizi, S.; Masrani, V.; Weilbach, C.; 및 Wood, F. 2022. 긴 비디오의 유연한 확산 모델링. arXiv 사전 인쇄본 arXiv:2205.11495. Ho, J.; Jain, A.; 및 Abbeel, P. 2020. 잡음 제거 확산 확률적 모델. 신경 정보 처리 시스템(NeurIPS). Ho, J.; Salimans, T.; Gritsenko, A.; Chan, W.; Norouzi, M.; 및 Fleet, DJ 2022. 비디오 확산 모델. arXiv 사전 인쇄본 arXiv:2204.03458. Hong, W.; Ding, M.; Zheng, W.; Liu, X.; 및 Tang, J. 2022. CogVideo: Transformers를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868. Hu, EJ; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; 및 Chen, W. 2021. LORA: 대규모 언어 모델의 저순위 적응.arXiv 사전 인쇄본 arXiv:2106.09685. Jiang, B.; Wang, M.; Gan, W.; Wu, W.; 및 Yan, J. 2019. STM: 동작 인식을 위한 시공간 및 동작 인코딩.IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 회의록, 2000-2009. Kalchbrenner, N.; Oord, A.; Simonyan, K.; Danihelka, I.; Vinyals, O.; Graves, A.; ; 및 Kavukcuoglu, K. 2017. 비디오 픽셀 네트워크. 1771-1779년 제34회 기계 학습 국제 컨퍼런스 회의록. Khachatryan, L.; Movsisyan, A.; Tadevosyan, V.; Henschel, R.; Wang, Z.; Navasardyan, S.; 및 Shi, H. 2023. Text2 Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators. IEEE/CVF International Conference on Computer Vision(ICCV)의 회의록. Ni, H.; Shi, C.; Li, K.; Huang, SX; 및 Min, MR 2023. Latent Flow Diffusion Models를 사용한 조건부 이미지-비디오 생성. IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)의 회의록, 18444-18455. Nichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin, P.; McGrew, B.; Sutskever, I.; 및 Chen, M. 2021. Glide: 텍스트 가이드 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해.arXiv 사전 인쇄본 arXiv:2112.10741. Radford, A.; Kim, JW; Hallacy, C.; Ramesh, A.; Goh, G.; 및 Agarwal, S. 2021. 자연어 감독에서 전이 가능한 시각적 모델 학습. 제38회 기계 학습 및 PMLR 국제 컨퍼런스 논문집, 8748-8763. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; 및 Narang, S. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색.The Journal of Machine Learning Research, 5485-5551. Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; 및 Chen, M. 2022. CLIP Latents를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; 및 Ommer., B. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 10684-10695. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, EL; 및 Ghasemipour, K. 2022. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 36479-36494. Schuhmann, C.; Vencu, R.; 보몬트, R.; 쿰베스, T.; 고든, C.; 카타, A.; Kaczmarczyk, R.; ; 및 Jitsev, J. 2022. LAION-5B: 개방형 대규모 다중 모드 데이터 세트의 새로운 시대. arXiv 사전 인쇄 arXiv:2307.04725. 가수 유.; 폴리악, A.; 헤이즈, T.; 음, X.; 안, J.; 장, S.; 후, Q.; 양, H.; 아슈알, 오.; 및 Gafni, O. 2022. Makea-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄 arXiv:2209.14792. 스마이라, L.; 카레이라, J.; 놀랜드, E.; 클랜시, E.; 우, A.; 및 Zisserman, A. 2020. Kinetics-700-Human Action Dataset에 대한 간략한 참고 사항.arXiv:2010.10864. Song, J.; Meng, C.; 및 Ermon, S. 2022. Denoising Diffusion Implicit Models.arXiv 사전 인쇄본 arXiv:2010.02502. van den Oord, A.; Vinyals, O.; 및 Kavukcuoglu, K. 2018. Neural Discrete Representation Learning. Advances in Neural Information Processing Systems. Vondrick, C.; Pirsiavash, H.; 및 Torralba, A. 2016. Generating Videos with Scene Dynamics.arXiv 사전 인쇄본 arXiv:1609.02612. Xue, H.; Hang, T.; Zeng, Y.; Sun, Y.; Liu, B.; Yang, H.; Fu, J.; ; 및 Guo, B. 2022. 대규모 비디오 대본을 통한 고해상도 비디오 언어 표현 향상. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 5036-5045. Yu, S.; Sohn, K.; Kim, S.; 및 Shin, J. 2023. 투사된 잠재 공간에서의 비디오 확률적 확산 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 18456–18466.
