--- INTRODUCTION ---
생성적 적대 신경망(GANS)[Goodfellow et al. 2014]과 같은 심층 생성 모델은 무작위적인 사실적인 이미지를 합성하는 데 전례 없는 성공을 거두었습니다. 실제 응용 프로그램에서 이러한 학습 기반 이미지 합성 방법의 중요한 기능 요구 사항은 합성된 시각적 콘텐츠에 대한 제어 가능성입니다. 예를 들어, 소셜 미디어 사용자는 우연히 찍은 사진에서 사람이나 동물의 위치, 모양, 표정 및 신체 포즈를 조정하고 싶을 수 있습니다. 전문적인 영화 사전 시각화 및 미디어 편집은 특정 레이아웃의 장면 스케치를 효율적으로 만들어야 할 수 있습니다. 자동차 디자이너는 자신의 작품의 모양을 대화형으로 수정하고 싶을 수 있습니다. 이러한 다양한 사용자 요구 사항을 충족하기 위해 이상적인 제어 가능한 이미지 합성 접근 방식은 다음과 같은 속성을 가져야 합니다. 1) 유연성: 생성된 객체나 동물의 위치, 포즈, 모양, 표정 및 레이아웃을 포함한 다양한 공간 속성을 제어할 수 있어야 합니다. 2) 정밀도: 공간 속성을 높은 정밀도로 제어할 수 있어야 합니다. 3) 일반성: 다양한 객체 범주에 적용 가능해야 하지만 특정 범주에 국한되지 않아야 합니다. 이전 연구에서는 이러한 속성 중 하나 또는 두 가지만 충족했지만 이 연구에서는 모든 속성을 달성하는 것을 목표로 합니다. 대부분의 이전 접근 방식은 이전 3D 모델[Deng et al. 2020; Ghosh et al. 2020; Tewari et al. 2020]이나 수동으로 주석이 달린 데이터에 의존하는 지도 학습[Abdal et al. 2021; Isola et al. 2017; Ling et al. 2021; Park et al. 2019; Shen et al. 2020]을 통해 GAN의 제어 가능성을 얻었습니다. 따라서 이러한 접근 방식은 새로운 객체 범주로 일반화하는 데 실패하고 종종 제한된 범위의 공간 속성을 제어하거나 편집 프로세스를 거의 제어하지 못합니다. 최근에는 텍스트 안내 이미지 합성이 주목을 받았습니다[Ramesh et al. 2022; Rombach et al. 2021; Saharia et al. 2022]. 그러나 텍스트 안내는 공간 속성을 편집하는 측면에서 정밀성과 유연성이 부족합니다. 예를 들어, 특정 픽셀 수만큼 객체를 이동하는 데 사용할 수 없습니다. GANS의 유연하고 정밀하며 일반적인 제어 가능성을 달성하기 위해 이 작업에서 강력하지만 훨씬 덜 탐색된 대화형 포인트 기반 조작을 탐구합니다. 구체적으로, 사용자가 이미지에서 원하는 수의 핸들 포인트와 대상 포인트를 클릭할 수 있도록 하며 목표는 핸들 포인트를 해당 대상 포인트에 도달하도록 하는 것입니다. 그림 1에서 볼 수 있듯이 이 포인트 기반 조작을 통해 사용자는 다양한 공간 속성을 제어할 수 있으며 객체 범주에 구애받지 않습니다. 우리와 가장 가까운 설정을 가진 접근 방식은 UserControllableLT[Endo 2022]로, 드래그 기반 조작도 연구합니다. 이와 비교했을 때, 이 연구에서 연구한 문제에는 두 가지 과제가 더 있습니다. 1) 우리는 그들의 접근 방식이 잘 처리하지 못하는 두 개 이상의 포인트 제어를 고려합니다. 2) 우리는 핸들 포인트가 대상 포인트에 정확하게 도달해야 하지만 그들의 접근 방식은 그렇지 않습니다. 실험에서 보여드리겠지만, 정밀한 위치 제어로 여러 지점을 처리하면 훨씬 더 다양하고 정확한 이미지 조작이 가능합니다. 논문 이러한 상호 작용적인 지점 기반 조작을 달성하기 위해, 1) 핸들 지점이 타겟을 향해 이동하도록 감독하고 2) 각 편집 단계에서 핸들 지점의 위치를 알 수 있도록 핸들 지점을 추적하는 두 가지 하위 문제를 해결하는 DragGAN을 제안합니다. 저희의 기술은 GAN의 특징이 공간 X를 가능하게 할 만큼 충분히 차별적이라는 핵심 통찰력을 기반으로 구축되었습니다. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt 모션 감독과 정밀한 지점 추적을 모두 가능하게 합니다. 구체적으로, 모션 감독은 잠재 코드를 최적화하는 이동된 특징 패치 손실을 통해 달성됩니다. 각 최적화 단계에서는 핸들 지점이 타겟에 더 가깝게 이동합니다. 따라서 특징 공간에서 최근접 이웃 검색을 통해 지점 추적이 수행됩니다. 이 최적화 프로세스는 핸들 지점이 타겟에 도달할 때까지 반복됩니다. DragGAN을 사용하면 사용자가 관심 영역을 그려 영역별 편집을 수행할 수도 있습니다. DragGAN은 RAFT[Teed and Deng 2020]와 같은 추가 네트워크에 의존하지 않으므로 효율적인 조작이 가능하며 대부분의 경우 단일 RTX 3090 GPU에서 몇 초만 걸립니다.이를 통해 사용자가 원하는 출력이 달성될 때까지 다양한 레이아웃에서 빠르게 반복할 수 있는 실시간 대화형 편집 세션이 가능합니다.우리는 동물(사자, 개, 고양이, 말), 인간(얼굴과 전신), 자동차, 풍경을 포함한 다양한 데이터 세트에서 DragGAN에 대한 광범위한 평가를 수행합니다.그림 1에서 볼 수 있듯이, 우리의 접근 방식은 사용자가 정의한 핸들 포인트를 대상 포인트로 효과적으로 이동하여 많은 객체 범주에서 다양한 조작 효과를 달성합니다.단순히 워핑[Igarashi et al. 2005]을 적용하는 기존의 모양 변형 접근 방식과 달리, 우리의 변형은 기본 객체 구조를 따르는 경향이 있는 GAN의 학습된 이미지 매니폴드에서 수행됩니다. 예를 들어, 우리의 접근 방식은 사자 입 안의 이빨과 같은 가려진 내용을 환각할 수 있으며, 말 다리의 굽힘과 같이 물체의 경직성에 따라 변형될 수 있습니다. 또한 사용자가 이미지를 클릭하기만 하면 조작을 대화형으로 수행할 수 있는 GUI를 개발합니다. 정성적, 정량적 비교 모두 UserControllableLT에 비해 우리의 접근 방식이 유리하다는 것을 확인합니다. 나아가, 우리의 GAN 기반 포인트 추적 알고리즘은 GAN 생성 프레임의 경우 RAFT[Teed and Deng 2020] 및 PIP[Harley et al. 2022]와 같은 기존 포인트 추적 접근 방식보다 성능이 뛰어납니다. 나아가, GAN 역전 기술과 결합함으로써 우리의 접근 방식은 실제 이미지 편집을 위한 강력한 도구 역할도 합니다. 2
--- RELATED WORK ---
2. 대화형 콘텐츠 생성을 위한 생성 모델 최신
--- METHOD ---
ologies → 컴퓨터 비전. 추가 키워드 및 구문: GANS, 대화형 이미지 조작, 포인트 추적 ACM 참조 형식: Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, Christian Theobalt. 2023. Drag Your GAN: 생성 이미지 매니폴드에서의 대화형 포인트 기반 조작. 컴퓨터 그래픽 및 대화형 기술에 대한 특수 관심 그룹 컨퍼런스 컨퍼런스 회의록(SIGGRAPH &#39;23 컨퍼런스 회의록), 2023년 8월 6-10일, 미국 캘리포니아주 로스앤젤레스. ACM, 뉴욕, 뉴욕, 미국, 11페이지. https://doi.org/10. 1145/3588432.SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 서론 생성적 적대 네트워크(GANS)[Goodfellow et al. 2014]와 같은 딥 생성 모델은 무작위적인 사실적인 이미지를 합성하는 데 전례 없는 성공을 거두었습니다. 실제 응용 프로그램에서 이러한 학습 기반 이미지 합성 방법의 중요한 기능 요구 사항은 합성된 시각적 콘텐츠에 대한 제어 가능성입니다. 예를 들어, 소셜 미디어 사용자는 우연히 찍은 사진에서 사람이나 동물의 위치, 모양, 표정 및 신체 포즈를 조정하고 싶을 수 있습니다. 전문적인 영화 사전 시각화 및 미디어 편집에는 특정 레이아웃이 있는 장면의 스케치를 효율적으로 만들어야 할 수 있습니다. 자동차 디자이너는 자신의 창작물의 모양을 대화형으로 수정하고 싶을 수 있습니다. 이러한 다양한 사용자 요구 사항을 충족하기 위해 이상적인 제어 가능한 이미지 합성 접근 방식은 다음과 같은 속성을 가져야 합니다.1) 유연성: 생성된 객체 또는 동물의 위치, 포즈, 모양, 표정 및 레이아웃을 포함한 다양한 공간 속성을 제어할 수 있어야 합니다.2) 정밀도: 높은 정밀도로 공간 속성을 제어할 수 있어야 합니다.3) 일반성: 다양한 객체 범주에 적용 가능하지만 특정 범주에 국한되지 않아야 합니다.이전 연구는 이러한 속성 중 하나 또는 두 가지만 충족했지만 이 작업에서는 모든 속성을 달성하는 것을 목표로 합니다.대부분의 이전 접근 방식은 이전 3D 모델[Deng et al. 2020; Ghosh et al. 2020; Tewari et al. 2020]이나 수동으로 주석이 달린 데이터에 의존하는 지도 학습[Abdal et al. 2021; Isola et al. 2017; Ling et al. 2021; Park et al. 2019; Shen et al. 2020]을 통해 GAN의 제어 가능성을 얻습니다. 따라서 이러한 접근 방식은 새로운 객체 범주로 일반화하는 데 실패하고, 종종 제한된 범위의 공간 속성을 제어하거나 편집 프로세스에 대한 제어를 거의 제공하지 못합니다.최근 텍스트 가이드 이미지 합성이 주목을 받았습니다[Ramesh et al. 2022; Rombach et al. 2021; Saharia et al. 2022].그러나 텍스트 가이드는 공간 속성을 편집하는 측면에서 정밀성과 유연성이 부족합니다.예를 들어, 특정 픽셀 수만큼 객체를 이동하는 데 사용할 수 없습니다.GANS의 유연하고 정밀하며 일반적인 제어 가능성을 달성하기 위해 이 작업에서 강력하지만 훨씬 덜 탐구된 대화형 포인트 기반 조작을 탐구합니다.특히, 사용자가 이미지에서 원하는 수의 핸들 포인트와 대상 포인트를 클릭할 수 있도록 허용하고 목표는 핸들 포인트를 해당 대상 포인트에 도달하도록 하는 것입니다.그림 1에서 볼 수 있듯이 이 포인트 기반 조작을 통해 사용자는 다양한 공간 속성을 제어할 수 있으며 객체 범주와 무관합니다.우리의 설정과 가장 가까운 접근 방식은 UserControllableLT[Endo 2022]로, 드래그 기반 조작도 연구합니다. 이와 비교했을 때, 이 문제에서 연구된 문제는 두 가지 더 많은 과제를 가지고 있습니다. 1) 우리는 그들의 접근 방식이 잘 처리하지 못하는 두 개 이상의 지점에 대한 제어를 고려합니다. 2) 우리는 핸들 지점이 목표 지점에 정확하게 도달해야 하지만 그들의 접근 방식은 그렇지 않습니다. 우리가 다음에서 보여줄 것처럼
--- EXPERIMENT ---
s, 정확한 위치 제어로 두 개 이상의 지점을 처리하면 훨씬 더 다양하고 정확한 이미지 조작이 가능합니다. 논문 이러한 상호 작용적인 지점 기반 조작을 달성하기 위해 1) 핸들 지점이 타겟을 향해 이동하도록 감독하고 2) 핸들 지점을 추적하여 각 편집 단계에서 위치를 알 수 있도록 하는 두 가지 하위 문제를 해결하는 DragGAN을 제안합니다. 저희의 기술은 GAN의 특징이 공간 X를 가능하게 할 만큼 충분히 차별적이라는 핵심 통찰력을 기반으로 구축되었습니다. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt 모션 감독과 정확한 지점 추적을 모두 가능하게 합니다. 구체적으로, 모션 감독은 잠재 코드를 최적화하는 이동된 특징 패치 손실을 통해 달성됩니다. 각 최적화 단계에서는 핸들 지점이 타겟에 더 가깝게 이동합니다. 따라서 특징 공간에서 최근접 이웃 검색을 통해 지점 추적이 수행됩니다. 이 최적화 프로세스는 핸들 지점이 타겟에 도달할 때까지 반복됩니다. DragGAN을 사용하면 사용자가 관심 영역을 그려 영역별 편집을 수행할 수도 있습니다. DragGAN은 RAFT[Teed and Deng 2020]와 같은 추가 네트워크에 의존하지 않으므로 효율적인 조작이 가능하며 대부분의 경우 단일 RTX 3090 GPU에서 몇 초만 걸립니다.이를 통해 사용자가 원하는 출력이 달성될 때까지 다양한 레이아웃에서 빠르게 반복할 수 있는 실시간 대화형 편집 세션이 가능합니다.우리는 동물(사자, 개, 고양이, 말), 인간(얼굴과 전신), 자동차, 풍경을 포함한 다양한 데이터 세트에서 DragGAN에 대한 광범위한 평가를 수행합니다.그림 1에서 볼 수 있듯이, 우리의 접근 방식은 사용자가 정의한 핸들 포인트를 대상 포인트로 효과적으로 이동하여 많은 객체 범주에서 다양한 조작 효과를 달성합니다.단순히 워핑[Igarashi et al. 2005]을 적용하는 기존의 모양 변형 접근 방식과 달리, 우리의 변형은 기본 객체 구조를 따르는 경향이 있는 GAN의 학습된 이미지 매니폴드에서 수행됩니다. 예를 들어, 우리의 접근 방식은 사자 입 안의 이빨과 같이 가려진 내용을 환각으로 볼 수 있고, 말 다리가 구부러지는 것처럼 물체의 경직성에 따라 변형될 수 있습니다. 또한 사용자가 이미지를 클릭하기만 하면 대화형으로 조작할 수 있는 GUI를 개발합니다. 정성적, 정량적 비교를 통해 UserControllableLT에 비해 우리의 접근 방식이 유리하다는 것이 확인되었습니다. 나아가, 우리의 GAN 기반 포인트 추적 알고리즘은 GAN 생성 프레임의 경우 RAFT[Teed and Deng 2020] 및 PIP[Harley et al. 2022]와 같은 기존의 포인트 추적 접근 방식보다 성능이 뛰어납니다. 나아가, GAN 역전 기술과 결합하여 우리의 접근 방식은 실제 이미지 편집을 위한 강력한 도구 역할도 합니다. 2 관련 연구 2. 대화형 콘텐츠 생성을 위한 생성 모델 대부분의 현재 방법은 제어 가능한 이미지 합성을 위해 생성적 적대 네트워크(GAN) 또는 확산 모델을 사용합니다. 무조건 GANS. GAN은 저차원 무작위 샘플링된 잠재 벡터를 사실적인 이미지로 변환하는 생성 모델입니다.이들은 적대적 학습을 사용하여 훈련되며 고해상도의 사실적인 이미지를 생성하는 데 사용될 수 있습니다[Creswell et al. 2018; Goodfellow et al. 2014; Karras et al. 2021, 2019].StyleGAN[Karras et al. 2019]과 같은 대부분의 GAN 모델은 생성된 이미지의 제어 가능한 편집을 직접 활성화하지 않습니다.조건부 GAN.여러 방법에서 이러한 제한을 해결하기 위해 조건부 GAN을 제안했습니다.여기서 네트워크는 무작위 샘플링된 잠재 벡터 외에도 분할 맵[Isola et al. 2017; Park et al. 2019]이나 3D 변수[Deng et al. 2020; Ghosh et al. 2020]와 같은 조건부 입력을 수신하여 사실적인 이미지를 생성합니다. 조건부 분포를 모델링하는 대신 EditGAN[Ling et al. 2021]은 먼저 이미지와 분할 맵의 공동 분포를 모델링한 다음 편집된 분할 맵에 해당하는 새 이미지를 계산하여 편집을 가능하게 합니다.GAN 드래그: 무조건부 GAN을 사용한 생성 이미지 매니폴드 제어 가능성에 대한 대화형 포인트 기반 조작. 입력 잠재 벡터를 조작하여 무조건부 GAN을 편집하기 위한 여러 방법이 제안되었습니다. 일부 접근 방식은 수동 주석 또는 이전 3D 모델에서 지도 학습을 통해 의미 있는 잠재 방향을 찾습니다[Abdal et al. 2021; Leimkühler and Drettakis 2021; Patashnik et al. 2021; Shen et al. 2020; Tewari et al. 2020]. 다른 접근 방식은 지도되지 않은 방식으로 잠재 공간에서 중요한 의미적 방향을 계산합니다[Härkönen et al. 2020; Shen and Zhou 2020; Zhu et al. 2023]. 최근에는 중간 &quot;블롭&quot;[Epstein et al. 2022]이나 히트맵[Wang et al. 2022b]을 도입하여 거친 객체 위치의 제어 가능성을 달성했습니다. 이러한 모든 접근 방식은 모양과 같은 이미지 정렬 의미 속성이나 객체 위치 및 포즈와 같은 거친 기하학적 속성을 편집할 수 있도록 합니다. Editing-in-Style[Collins et al. 2020]은 일부 공간 속성 편집 기능을 보여주지만 서로 다른 샘플 간에 로컬 의미론을 전송하여 이를 달성할 수 있습니다. 이러한 방법과 달리, 우리의 접근 방식은 사용자가 포인트 기반 편집을 사용하여 공간 속성을 세밀하게 제어할 수 있도록 합니다. GANWarping[Wang et al. 2022a]도 포인트 기반 편집을 사용하지만 분포 외부 이미지 편집만 가능하게 합니다. 몇 개의 워핑된 이미지를 사용하여 생성 모델을 업데이트하여 모든 생성된 이미지가 유사한 워프를 보이도록 할 수 있습니다. 그러나 이 방법은 워프가 사실적인 이미지로 이어진다는 것을 보장하지 않습니다. 또한 변경과 같은 제어를 가능하게 하지 않습니다. 객체의 3D 포즈. 저희와 비슷하게 UserControllableLT [Endo 2022]는 GAN의 잠재 벡터를 변환하여 포인트 기반 편집을 가능하게 합니다. 그러나 이 접근 방식은 이미지에서 단일 포인트를 드래그하여 편집하는 것만 지원하고 다중 포인트 제약 조건을 잘 처리하지 못합니다. 또한 제어가 정확하지 않습니다. 즉, 편집 후 대상 포인트에 도달하지 못하는 경우가 많습니다. 3D 인식 GAN. 여러 방법이 GAN의 아키텍처를 수정하여 3D 제어를 가능하게 합니다 [Chan et al. 2022, 2021; Chen et al. 2022; Gu et al. 2022; Pan et al. 2021; Schwarz et al. 2020; Tewari et al. 2022; Xu et al. 2022]. 여기서 모델은 물리 기반 분석 렌더러를 사용하여 렌더링할 수 있는 3D 표현을 생성합니다. 그러나 저희의 접근 방식과 달리 제어는 글로벌 포즈 또는 조명으로 제한됩니다. 확산 모델.최근에는 확산 모델[Sohl-Dickstein et al. 2015]을 통해 고품질의 이미지 합성이 가능해졌습니다[Ho et al. 2020; Song et al. 2020, 2021].이러한 모델은 무작위로 샘플링된 노이즈를 반복적으로 노이즈 제거하여 사실적인 이미지를 만듭니다.최근 모델은 텍스트 입력을 조건으로 한 표현력 있는 이미지 합성을 보여주었습니다[Ramesh et al. 2022; Rombach et al. 2021; Saharia et al. 2022].그러나 자연어는 이미지의 공간적 속성을 세밀하게 제어할 수 없으므로 모든 텍스트 조건부 방법은 고수준 의미 편집으로 제한됩니다.또한 현재 확산 모델은 여러 노이즈 제거 단계가 필요하기 때문에 느립니다.효율적인 샘플링을 향한 진전이 있었지만 GAN은 여전히 훨씬 더 효율적입니다.2.2 이미지 변형 사용자의 포인트 드래그 명령에 따라 이미지를 변형하는 방법은 컴퓨터 그래픽스의 고전적인 문제입니다. 기존 방식[Botsch 및 Sorkine 2007]은 일반적으로 이미지를 메시로 변환한 다음, 2023년 8월 6-10일, 미국 캘리포니아주 로스앤젤레스에서 열린 SIGGRAPH &#39;23 컨퍼런스 회의록에서 강성[Igarashi et al. 2005; Sorkine 및 Alexa 2007] 및 라플라시안 매끄러움[Lipman et al. 2004, 2005; Sorkine et al. 2004]과 같은 기하학적 제약 조건에 따라 메시를 변형합니다. 이전 연구에서는 수작업으로 만든 특징을 기반으로 한 모양 변형 개념을 제안했습니다[Beier 및 Neely 2023]. 그러나 이러한 기하학적 제약 조건과 수작업으로 만든 특징은 편집된 객체의 기본 구조와 강성에 대한 지식이 부족하여 종종 최적이 아닌 변형을 생성합니다. 또한 필요할 때 가려진 영역을 합성하는 것과 같이 새로운 콘텐츠를 환각할 수 없습니다. 또한 포인트 드래그 편집은 비디오에서 탐색하여 근사할 수 있지만 비디오 데이터가 필요하기 때문에 적용성이 제한됩니다[Goldman et al. 2008]. 이와 대조적으로 이 연구에서는 객체 구조와 모양에 대한 풍부한 정보를 포착하는 강력한 생성 이미지 사전을 기반으로 이미지 변형을 연구합니다. 2. 포인트 추적 비디오에서 포인트를 추적하기 위한 명확한 접근 방식은 연속 프레임 간의 광학 흐름 추정을 통한 것입니다. 광학 흐름 추정은 두 이미지 간의 동작 필드를 추정하는 고전적인 문제입니다. 기존 접근 방식은 수작업으로 만든 기준으로 최적화 문제를 해결합니다[Brox and Malik 2010; Sundaram et al. 2010]. 반면 딥 러닝 기반 접근 방식은 더 나은 성능으로 인해 최근 몇 년 동안 이 분야를 지배하기 시작했습니다[Dosovitskiy et al. 2015; Ilg et al. 2017; Teed and Deng 2020]. 이러한 딥 러닝 기반 접근 방식은 일반적으로 지상 진실 광학 흐름이 있는 합성 데이터를 사용하여 딥 신경망을 훈련합니다. 그 중 현재 가장 널리 사용되는 방법은 반복적 알고리즘을 통해 광 흐름을 추정하는 RAFT[Teed and Deng 2020]입니다. 최근 Harley et al.[2022]은 이 반복적 알고리즘을 기존의 &quot;입자 비디오&quot; 접근 방식과 결합하여 PIPS라는 새로운 포인트 추적 방법을 탄생시켰습니다. PIP는 여러 프레임에 걸친 정보를 고려하므로 이전 접근 방식보다 장거리 추적을 더 잘 처리합니다. 이 연구에서 우리는 앞서 언급한 접근 방식이나 추가 신경망을 사용하지 않고도 GAN에서 생성된 이미지의 포인트 추적을 수행할 수 있음을 보여줍니다. 우리는 GAN의 특징 공간이 충분히 구별적이어서 특징 매칭을 통해 간단히 추적할 수 있음을 밝힙니다. 일부 이전 연구에서도 의미 분할에서 구별적 특징을 활용했지만[Tritrong et al. 2021; Zhang et al. 2021], 우리는 처음으로 포인트 기반 편집 문제를 구별적 GAN 특징의 직관에 연결하고 구체적인 방법을 설계했습니다. 추가 추적 모델을 제거하면 대화형 편집을 지원하기 위해 접근 방식을 훨씬 더 효율적으로 실행할 수 있습니다. 접근 방식이 간단함에도 불구하고 실험에서 RAFT 및 PIP를 포함한 최첨단 포인트 추적 접근 방식보다 성능이 우수함을 보여줍니다. 3 방법 이 작업은 사용자가 이미지를 클릭하여 (핸들 포인트, 대상 포인트)의 일부 쌍을 정의하고 핸들 포인트를 해당 대상 포인트에 도달하도록 구동하기만 하면 되는 GAN에 대한 대화형 이미지 조작 방법을 개발하는 것을 목표로 합니다. 저희 연구는 StyleGAN2 아키텍처[Karras et al. 2020]를 기반으로 합니다. 여기서 이 아키텍처의 기본 사항을 간략하게 소개합니다. StyleGAN 용어. StyleGAN2 아키텍처에서 512차원 잠재 코드 z = N(0, I)는 중간에 매핑됩니다.SIGGRAPH &#39;23 Conference Proceedings, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 잠재 코드 w W&#39; X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt w* 핸들 포인트 ● 대상 포인트 사용자 입력 생성기 OOO 동작 감독 OOOO OOOO 초기 이미지 1차 최적화 단계 포인트 추적 동작 감독 업데이트 포인트OOOO 최종 이미지 그림 2. 파이프라인 개요 GAN에서 생성된 이미지가 주어지면 사용자는 여러 핸들 포인트(빨간색 점), 대상 포인트(파란색 점)만 설정하고, 선택적으로 편집 중에 움직일 수 있는 영역(더 밝은 영역)을 나타내는 마스크도 설정하면 됩니다. 저희의 접근 방식은 동작 감독(3.2절)과 포인트 추적(3.3절)을 반복적으로 수행합니다. 동작 감독 단계는 핸들 포인트(빨간색 점)를 목표 포인트(파란색 점) 쪽으로 이동시키고 포인트 추적 단계는 핸들 포인트를 업데이트하여 이미지의 객체를 추적합니다. 이 프로세스는 핸들 포인트가 해당 목표 포인트에 도달할 때까지 계속됩니다. 매핑 네트워크를 통한 잠재 코드 w = R 512. w의 공간은 일반적으로 W라고 합니다. 그런 다음 w가 생성기 G로 전송되어 출력 이미지 I = G(w)를 생성합니다. 이 프로세스에서 w는 여러 번 복사되어 생성기 G의 다른 계층으로 전송되어 다른 수준의 속성을 제어합니다. 또는 다른 계층에 대해 다른 w를 사용할 수도 있는데, 이 경우 입력은 w = R¹×512 =W+가 되며, 여기서 l은 계층 수입니다. 이 덜 제약된 W+는 더 표현력이 뛰어난 것으로 나타났습니다[Abdal et al. 2019]. 생성기 G가 저차원 잠재 공간에서 훨씬 더 높은 차원의 이미지 공간으로의 매핑을 학습함에 따라 이미지 매니폴드를 모델링하는 것으로 볼 수 있습니다[Zhu et al. 2016]. 3. 대화형 포인트 기반 조작 공간 = 그림 2에 이미지 조작 파이프라인 개요가 나와 있습니다. 잠재 코드 w가 있는 GAN에서 생성된 모든 이미지 I € R³×HW에 대해 사용자가 여러 핸들 포인트 {pi (xp,i, yp,i)|i = 1, 2, ..., n}와 해당 대상 포인트 {t; = (xt,i, Yt,i)|i = 1, 2, ..., n}을 입력하도록 허용합니다(즉, pi의 해당 대상 포인트는 t;). 목표는 핸들 포인트의 의미적 위치(예: 그림 2의 코와 턱)가 해당 대상 포인트에 도달하도록 이미지에서 객체를 이동하는 것입니다. 또한 사용자가 선택적으로 이미지의 어느 영역이 움직일 수 있는지 나타내는 이진 마스크 M을 그릴 수 있도록 허용합니다. 이러한 사용자 입력이 주어지면 최적화된 방식으로 이미지 조작을 수행합니다. 그림 2에 표시된 대로 각 최적화 단계는 1) 동작 감독 및 2) 포인트 추적을 포함한 두 개의 하위 단계로 구성됩니다. 동작 감독에서 핸들 포인트가 대상 포인트로 이동하도록 강제하는 손실을 사용하여 잠재 코드 w를 최적화합니다. 한 최적화 단계 후에 새로운 잠재 코드 w&#39;와 새로운 이미지 I&#39;를 얻습니다. 업데이트로 인해 이미지의 객체가 약간 이동합니다. 동작 감독 단계는 각 핸들 포인트를 작은 단계만큼 대상 방향으로 이동시키지만 단계의 정확한 길이는 복잡한 최적화 역학의 영향을 받아 객체와 부품마다 다르기 때문에 불분명합니다. 따라서 핸들 포인트 {pi}의 위치를 업데이트하여 객체의 해당 포인트를 추적합니다. 이 추적 프로세스는 핸들 포인트(예: 사자의 코)가 정확하게 추적되지 않으면 다음 동작 감독 단계에서 잘못된 포인트(예: 사자의 얼굴)가 감독되어 잠재 코드 w 생성기 L1_loss( .detach())w&#39; pt Feature Nearest --fo →▸ Neighbor oooo 그림 3. 방법. 동작 감독은 생성기의 피처 맵에서 이동된 패치 손실을 통해 달성됩니다. 가장 가까운 이웃 검색을 통해 동일한 피처 공간에서 포인트 추적을 수행합니다. 원치 않는 결과가 발생합니다. 추적 후 새 핸들 포인트와 잠재 코드를 기반으로 위의 최적화 단계를 반복합니다. 이 최적화 프로세스는 핸들 포인트 {p;}가 대상 포인트 {t;}의 위치에 도달할 때까지 계속되며, 일반적으로 실험에서는 30번의 반복이 필요합니다. 사용자는 중간 단계에서 최적화를 중지할 수도 있습니다. 편집 후 사용자는 새 핸들과 대상 포인트를 입력하고 결과에 만족할 때까지 편집을 계속할 수 있습니다. 3. 모션 감독 GAN에서 생성된 이미지의 포인트 모션을 감독하는 방법은 이전에 많이 탐구되지 않았습니다. 이 작업에서 우리는 추가 신경망에 의존하지 않는 모션 감독 손실을 제안합니다. 핵심 아이디어는 생성기의 중간 특징이 매우 차별적이어서 간단한 손실로 모션을 감독하기에 충분하다는 것입니다. 구체적으로, 우리는 StyleGAN2의 6번째 블록 이후의 특징 맵 F를 고려하는데, 이는 해상도와 차별성 간의 좋은 균형으로 인해 모든 특징 중에서 가장 좋은 성능을 보입니다. 우리는 선형 보간을 통해 F의 크기를 조정하여 최종 이미지와 동일한 해상도를 갖도록 합니다. 그림 3과 같이 핸들 포인트 p¡를 대상 포인트 t;로 이동하려면 piDrag Your GAN: 생성 이미지 매니폴드에서의 대화형 포인트 기반 조작 SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 UserControllableLT 입력 그림 4. 핸들 포인트(빨간색 점)를 대상 포인트(파란색 점)로 이동하는 작업에서 UserControllableLT [Endo 2022]에 대한 접근 방식의 정성적 비교. 우리의 접근 방식은 다양한 데이터 세트에서 더 자연스럽고 우수한 결과를 달성합니다. 그림 10에서 더 많은 예를 제공합니다. (빨간색 원)을 작은 단계(파란색 원)만큼 ti 방향으로 이동합니다. 우리는 p¡와의 거리가 ₹1보다 작은 픽셀을 나타내기 위해 Q1(Pi, 1)을 사용합니다.그러면 모션 감독 손실은 다음과 같습니다.n L = Σ Σ i=0 qiЄ(pir) ||F(qi) − F(qi + di)||1 + λ||(F – Fo) · (1 - M)||1, 여기서 F(q)는 픽셀 q에서 F의 피처 값을 나타내고, di ti-Pi (1) = ||ti-Pi||는 pi에서 ti를 가리키는 정규화된 벡터(t₁ = pi인 경우 di = 0)이고 Fo는 초기 이미지에 해당하는 피처 맵입니다.첫 번째 항은 모든 핸들 포인트 {pi}에 대해 합산됩니다.qi+d;의 구성 요소가 정수가 아니므로 선형 보간을 통해 F(qi+d;)를 얻습니다.중요한 점은 이 손실을 사용하여 역전파를 수행할 때 그래디언트가 F(qi)를 통해 역전파되지 않는다는 것입니다. 이것은 pi가 p; +d;로 이동하도록 동기를 부여하지만 그 반대는 아닙니다. 이진 마스크 M이 주어진 경우, 우리는 두 번째 항으로 표시된 재구성 손실로 마스크되지 않은 영역을 고정합니다. 각 동작 감독 단계에서 이 손실은 한 단계 동안 잠재 코드 w를 최적화하는 데 사용됩니다. w는 사용자가 더 제한된 이미지 매니폴드를 원하는지 여부에 따라 W 공간 또는 W+ 공간에서 최적화할 수 있습니다. W+ 공간은 분포 외 조작을 달성하기 쉽기 때문에(예: 그림 16의 cat) 이 작업에서는 더 나은 편집성을 위해 W*를 사용합니다. 실제로, 우리는 이미지의 공간적 속성이 처음 6개 레이어의 w에 의해 주로 영향을 받는 반면 나머지는 모양에만 영향을 미친다는 것을 관찰합니다. 따라서 스타일 믹싱 기술[Karras et al. 2019]에서 영감을 받아 모양을 보존하기 위해 다른 레이어를 수정하는 동안 처음 6개 레이어의 w만 업데이트합니다. 이 선택적 최적화는 이미지 콘텐츠의 원하는 약간의 움직임을 가져옵니다. 3. 포인트 추적 이전 모션 감독은 새로운 잠재 코드 w&#39;, 새로운 피처 맵 F&#39; 및 새로운 이미지 I&#39;를 생성합니다. 모션 감독 단계는 핸들 포인트의 정확한 새 위치를 쉽게 제공하지 않으므로 여기서의 목표는 각 핸들 포인트 p를 업데이트하여 객체의 해당 포인트를 추적하는 것입니다. 포인트 추적은 일반적으로 광 흐름 추정 모델 또는 입자 비디오 접근 방식을 통해 수행됩니다[Harley et al. 2022]. 다시 말하지만, 이러한 추가 모델은 효율성을 크게 손상시킬 수 있으며 특히 GAN에 앨리어스 아티팩트가 있는 경우 누적 오류가 발생할 수 있습니다. 따라서 GAN에 대한 새로운 포인트 추적 접근 방식을 제시합니다. 통찰력은 GAN의 차별적 특징이 밀집 대응을 잘 포착하므로 특징 패치에서 최근접 이웃 검색을 통해 추적을 효과적으로 수행할 수 있다는 것입니다. 구체적으로, 초기 핸들 포인트의 특징을 fi = Fo(pi)로 표시합니다. pi 주변의 패치를 22(pi, r2) = {(x, y) | |x − xp,i|로 표시합니다. &lt; r2, |Y – Yp,i| &lt; r2}. 그런 다음 추적된 점은 Q2(pi, r2)에서 fi의 가장 가까운 이웃을 검색하여 얻습니다. Pi == arg min ||F&#39; (qi) – fi||1. qi Q2 (Pi,r2) (2) 이런 식으로 pį가 객체를 추적하도록 업데이트됩니다. 핸들 점이 두 개 이상인 경우 각 점에 대해 동일한 프로세스를 적용합니다. 여기서 StyleGAN2의 6번째 블록 뒤에 있는 피처 맵 F&#39;도 고려하고 있다는 점에 유의하세요. 피처 맵은 256×256의 해상도를 가지며 필요한 경우 이미지와 동일한 크기로 선형 보간되므로 실험에서 정확한 추적을 수행하기에 충분합니다. 이 선택은 4.2절에서 분석합니다. 3.4 구현 세부 정보 PyTorch[Paszke et al. 2017]를 기반으로 접근 방식을 구현합니다. 영어: FFHQ를 위해 2e-3의 단계 크기로 잠재 코드 w를 최적화하기 위해 Adam 최적화기[Kingma 및 Ba 2014]를 사용합니다[Karras et al.SIGGRAPH &#39;23 Conference Proceedings, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 실제 이미지 1차 편집(포즈) 2차 편집(머리카락) X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt 3차 편집(모양) 4차 편집(표정) CCCCCCCC GAN 반전 그림 5. 실제 이미지 조작. 실제 이미지가 주어지면 GAN 반전을 적용하여 StyleGAN의 잠재 공간에 매핑한 다음 포즈, 머리카락, 모양 및 표정을 각각 편집합니다. 2019], AFHQCat[Choi et al. 2020], LSUN Car[Yu et al. 2015] 데이터 세트 및 다른 데이터 세트의 경우 1e-3을 사용합니다. 하이퍼 매개변수는 λ = 20, r1 = round(3/512 × 크기), r2 = round(12/512 × 크기)로 설정되며, 여기서 크기는 생성된 이미지의 해상도입니다. 구현 방식에서는 모든 핸들 포인트가 해당 대상 포인트에서 d 픽셀 이내로 떨어져 있을 때 최적화 프로세스를 중지합니다. 여기서 d는 핸들 포인트가 5개 이내이면 1로 설정하고 그렇지 않으면 2로 설정합니다. 또한 대화형 이미지 조작을 지원하는 GUI를 개발합니다. 접근 방식의 계산 효율성 덕분에 사용자는 각 편집을 위해 몇 초만 기다리면 되며 만족할 때까지 편집을 계속할 수 있습니다. 독자 여러분께서 대화형 세션의 라이브 녹화를 위한 보충 비디오를 참조하시기를 적극 권장합니다.실험 데이터 세트. StyleGAN2[Karras et al. 2020] 다음 데이터세트에 대해 사전 학습됨(사전 학습된 StyleGAN2의 해상도는 괄호 안에 표시됨): FFHQ(512) [Karras et al. 2019], AFHQCat(512) [Choi et al. 2020], SHHQ(512) [Fu et al. 2022], LSUN Car(512) [Yu et al. 2015], LSUN Cat(256) [Yu et al. 2015], Landscapes HQ(256) [Skorokhodov et al. 2021], Microscope(512) [Pinkney 2020] 및 Lion(512), Dog(1024), Elephant(512)를 포함한 [Mokady et al. 2022]의 자체 정제 데이터세트. 기준선. 우리의 주요 기준선은 우리의 방법과 가장 가까운 설정을 가진 UserControllableLT[Endo 2022]입니다.UserControllableLT는 마스크 입력을 지원하지 않지만 사용자가 여러 개의 고정점을 정의할 수 있도록 합니다.따라서 마스크 입력이 있는 테스트 케이스의 경우 이미지에서 일반 16×16 그리드를 샘플링하고 마스크 외부의 점을 UserControllableLT에 대한 고정점으로 사용합니다.또한 점 추적을 위해 RAFT[Teed and Deng 2020] 및 PIP[Harley et al. 2022]와도 비교합니다.이를 위해 점 추적 부분(Sec.3.3)을 이 두 가지 추적 방법으로 대체하는 두 가지 접근 방식 변형을 만듭니다.4. 정성적 평가 그림 4는 우리의 방법과 UserControllableLT 간의 정성적 비교를 보여줍니다.여러 다른 객체 범주와 사용자 입력에 대한 이미지 조작 결과를 보여줍니다. 우리의 접근 방식은 핸들 포인트를 정확하게 움직여 목표 지점에 도달하게 하여 동물의 포즈, 자동차 모양, 풍경의 레이아웃을 바꾸는 것과 같은 다양하고 자연스러운 조작 효과를 달성합니다.반대로, UserControllableLT는 핸들 포인트를 목표 지점으로 충실하게 움직일 수 없으며 종종 이미지에서 원치 않는 변화(예: 사람의 옷과 자동차 배경)를 일으킵니다.또한 고양이 이미지에서 볼 수 있듯이 마스크되지 않은 영역을 우리처럼 고정하지도 못합니다.그림 10에서 더 많은 비교를 보여줍니다.입력 조작 프로세스 추적이 없는 우리의 PIPS RAFT 그림 6. RAFT[Teed and Deng 2020], PIP[Harley et al. 2022] 및 추적이 없는 우리의 접근 방식에 대한 정성적 추적 비교.우리의 접근 방식은 베이스라인보다 핸들 포인트를 더 정확하게 추적하여 더 정확한 편집을 생성합니다. 그림 6에서는 PIPS와 RAFT를 사용한 접근 방식을 비교했습니다. 우리의 접근 방식은 사자 코 위의 핸들 포인트를 정확하게 추적하여 성공적으로 목표 위치로 이동합니다. PIP와 RAFT에서 추적된 포인트는 조작 프로세스 중에 코에서 벗어나기 시작합니다. 결과적으로 잘못된 부분을 목표 위치로 이동합니다. 추적을 수행하지 않으면 고정된 핸들 포인트는 몇 걸음 후에 곧 이미지의 다른 부분(예: 배경)을 구동하기 시작하고 언제 멈출지 알지 못해 편집 목표를 달성하지 못합니다. 실제 이미지 편집. StyleGAN의 잠재 공간에 실제 이미지를 포함하는 GAN 반전 기술을 사용하여 우리의 접근 방식을 실제 이미지를 조작하는 데 적용할 수도 있습니다. 그림 5는 실제 이미지에 PTI 반전[Roich et al. 2022]을 적용한 다음 일련의 조작을 수행하여 이미지에서 얼굴의 포즈, 머리카락, 모양 및 표정을 편집하는 예를 보여줍니다. 그림 13에서 더 많은 실제 이미지 편집 사례를 보여줍니다.4. 양적 평가 얼굴 랜드마크 조작 및 페어링된 이미지 재구성을 포함한 두 가지 설정에서 방법을 양적으로 평가합니다.얼굴 랜드마크 조작.얼굴 랜드마크 감지는 기성품 도구[King 2009]를 사용하여 매우 안정적이므로 예측을 기준 진실 랜드마크로 사용합니다.특히 FFHQ에서 학습된 StyleGAN을 사용하여 두 개의 얼굴 이미지를 무작위로 생성하고 랜드마크를 감지합니다.목표는 첫 번째 이미지의 랜드마크를 조작하여 두 번째 이미지의 랜드마크와 일치시키는 것입니다.조작 후 최종 이미지의 랜드마크를 감지하고 대상 랜드마크까지의 평균 거리(MD)를 계산합니다.Drag Your GAN: 생성 이미지 매니폴드 입력 대상 UserControllableLT에 대한 대화형 포인트 기반 조작 저희 SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 미국 캘리포니아주 로스앤젤레스, 마스크 사용 마스크 없음 H 그림 8. 마스크의 효과. 영어: 우리의 접근 방식은 움직일 수 있는 영역을 마스크하는 것을 허용한다. 개의 머리 영역을 마스크한 후, 나머지 부분은 거의 변하지 않을 것이다.그림 7. 얼굴 랜드마크 조작. UserControllableLT [Endo 2022]와 비교할 때, 우리의 방법은 적은 매칭 오류로 입력 이미지에서 감지된 랜드마크를 조작하여 대상 이미지에서 감지된 랜드마크와 일치시킬 수 있다.표 1. 얼굴 키포인트 조작에 대한 정량적 평가. 편집된 포인트와 대상 포인트 사이의 평균 거리를 계산한다. FID와 시간은 &#39;1포인트&#39; 설정을 기반으로 보고된다.방법 1포인트 5포인트 68포인트 FID 시간(초) 편집 없음 12.11.16.UserControllableLT 11.10.10.25.RAFT 추적이 있는 우리의 것 13.13.15.51.PIPS 추적이 있는 우리의 것 2.4.5.우리의 것 2.3.4.31.9.0.15.6.2.결과는 1000개의 테스트에 걸쳐 평균화된다. 모든 방법을 평가하는 데 동일한 테스트 샘플 집합을 사용합니다. 이런 식으로 최종 MD 점수는 이 방법이 랜드마크를 목표 위치로 얼마나 잘 옮길 수 있는지를 반영합니다. 핸들 포인트의 수가 다를 때 접근 방식의 견고성을 보여주기 위해 1, 5, 68을 포함한 다른 수의 랜드마크를 사용하여 3가지 설정에서 평가를 수행합니다. 또한 편집된 이미지와 초기 이미지 간의 FID 점수를 이미지 품질의 지표로 보고합니다. 저희 접근 방식과 그 변형에서 최대 최적화 단계는 300으로 설정됩니다. 결과는 표 1에 나와 있습니다. 저희 접근 방식은 다른 수의 포인트에서 UserControllableLT보다 상당히 우수한 성능을 보입니다. 정성적 비교는 그림 7에 나와 있으며, 저희 방법은 입을 벌리고 턱 모양을 조정하여 목표 얼굴과 일치시키는 반면 UserControllableLT는 그렇지 못합니다. 더욱이 저희 접근 방식은 FID 점수에서 알 수 있듯이 더 나은 이미지 품질을 유지합니다. 더 나은 추적 기능 덕분에 RAFT 및 PIPS보다 더 정확한 조작을 달성합니다. 부정확한 추적은 또한 과도한 조작으로 이어지고, 이는 FID 점수에서 보여지듯이 이미지 품질을 저하시킵니다.UserControllableLT가 더 빠르지만, 우리의 접근 방식은 이 작업의 상한을 크게 밀어내어 사용자에게 편안한 실행 시간을 유지하면서 훨씬 더 충실한 조작을 달성합니다.페어 이미지 재구성.이 평가에서 우리는 UserControllableLT [Endo 2022]와 동일한 설정을 따릅니다.특히, 우리는 잠재 코드 w₁를 샘플링하고 [Endo 2022]와 같은 방식으로 무작위로 교란하여 w₂를 얻습니다.11과 I2를 두 잠재 코드에서 생성된 StyleGAN 이미지라고 합니다.그런 다음 I1과 I2 사이의 광학 흐름을 계산하고 사용자 입력 U로 흐름 필드에서 무작위로 32개 픽셀을 샘플링합니다.목표는 I₁과 U에서 12를 재구성하는 것입니다.우리는 MSE와 LPIPS [Zhang et al. 2018]를 보고하고 1000개 샘플에 대한 결과의 평균을 냅니다. 최대 최적화 단계는 그림 9에서와 같이 설정됩니다.분포 외 조작.우리의 접근 방식은 예를 들어 매우 벌어진 입과 크게 확대된 바퀴와 같이 훈련 이미지 분포에서 벗어난 이미지를 생성하기 위한 외삽 기능이 있습니다.우리의 접근 방식과 그 변형에서 100까지.표 2에서 볼 수 있듯이, 우리의 접근 방식은 다른 객체 범주에서 모든 기준선보다 성능이 뛰어나며, 이는 이전 결과와 일치합니다.삭제 연구.여기서는 동작 감독 및 포인트 추적에 사용할 기능의 효과를 연구합니다.우리는 다양한 기능을 사용하여 얼굴 랜드마크 조작의 성능(MD)을 보고합니다.표 3에서 볼 수 있듯이, 동작 감독과 포인트 추적 모두에서 StyleGAN의 6번째 블록 이후의 기능 맵이 가장 좋은 성능을 보이며, 해상도와 판별력 사이에서 최상의 균형을 보여줍니다.또한 표 4에서 r₁의 효과를 제공합니다.성능은 r1의 선택에 크게 민감하지 않고, r₁ = 3이 약간 더 나은 성능을 보이는 것을 볼 수 있습니다.4.토론 마스크의 효과. 우리의 접근 방식은 사용자가 움직일 수 있는 영역을 나타내는 이진 마스크를 입력할 수 있도록 합니다. 그림 8에서 그 효과를 보여줍니다. 개 머리 위의 마스크가 주어지면 다른 영역은 거의 고정되고 머리만 움직입니다. 마스크가 없으면 조작으로 개 몸 전체가 움직입니다. 이는 또한 점 기반 조작에는 종종 여러 가지 가능한 솔루션이 있으며 GAN은 훈련 데이터에서 학습한 이미지 매니폴드에서 가장 가까운 솔루션을 찾는 경향이 있음을 보여줍니다. 마스크 함수는 모호성을 줄이고 특정 영역을 고정하는 데 도움이 될 수 있습니다. 분포 외 조작. 지금까지 보여준 점 기반 조작은 &quot;분포 내&quot; 조작입니다. 즉, 훈련 데이터 세트의 이미지 분포 내에서 자연스러운 이미지로 조작 요구 사항을 충족할 수 있습니다. 여기서는 그림 9에서 일부 분포 외 조작을 보여줍니다. 우리의 접근 방식에는 약간의 외삽 기능이 있어 훈련 이미지 분포 외부의 이미지(예: 매우 벌린 입과 큰 바퀴)를 생성할 수 있습니다. 어떤 경우에는 사용자가 항상 이미지를 훈련 분포에 유지하고 이러한 분포 밖 조작에 도달하지 못하도록 하기를 원할 수 있습니다. 이를 달성하는 잠재적인 방법은 이 논문의 주요 초점이 아닌 잠재 코드 w에 추가적인 정규화를 추가하는 것입니다. 제한 사항. 일부 외삽 기능에도 불구하고 편집 품질은 여전히 훈련 데이터의 다양성에 영향을 받습니다. 그림 14(a)에 예시된 것처럼, SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 표 2. 쌍 이미지 재구성에 대한 정량적 평가. [Endo 2022]의 평가를 따르고 MSE (×10²)↓ 및 LPIPS (×10)↓ 점수를 보고합니다. X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt 표 3. 사용할 기능의 효과. x+y는 두 가지 특징의 연결을 의미합니다. 얼굴 랜드마크 조작(1점)의 성능(MD)을 보고합니다. 블록 번호. 동작 지원. 추적 데이터 세트 메트릭 사자 LSUN 고양이 개 MSE LPIPS MSE LPIPS MSE LPIPS UserControllableLT 1.1.1.0.1.0.Ours w. RAFT 추적 Ours w. PIPS 추적 1.0.1.1.0.0.LSUN 자동차 MSE LPIPS 1.98 0.2.37 0.0.0.82 1.0.0.0.Ours 0.66 0.1.0.0.0.1.1.81 0.0.4 5 6 7 5+6 6+2.73 2.50 2.44 2.51 2.47 2.3.61 2.55 2.44 2.58 2.47 2.표 4. r₁의 효과. rMD 2.2.51 2.44 2.2. 훈련 분포는 아티팩트를 초래할 수 있습니다. 게다가, 텍스처가 없는 영역의 핸들 포인트는 때때로 그림 14(b)(c)에서 볼 수 있듯이 추적에서 더 많은 드리프트를 겪습니다. 따라서 가능하다면 텍스처가 풍부한 핸들 포인트를 선택하는 것이 좋습니다. 사회적 영향. 우리의 방법은 이미지의 공간적 속성을 변경할 수 있으므로 가짜 포즈, 표정 또는 모양을 가진 실제 사람의 이미지를 만드는 데 오용될 수 있습니다. 따라서 우리의 접근 방식을 사용하는 모든 응용 프로그램이나 연구는 인격권과 개인정보 보호 규정을 엄격히 준수해야 합니다.
--- CONCLUSION ---
직관적인 포인트 기반 이미지 편집을 위한 대화형 접근 방식인 DragGAN을 제시했습니다. 저희의 방법은 사전 훈련된 GAN을 활용하여 사용자 입력을 정확하게 따를 뿐만 아니라 사실적인 이미지의 매니폴드에 머무르는 이미지를 합성합니다. 이전의 많은 접근 방식과 달리 저희는 도메인별 모델링이나 보조 네트워크에 의존하지 않음으로써 일반적인 프레임워크를 제시합니다. 이는 두 가지 새로운 요소를 사용하여 달성됩니다. 여러 핸들 포인트를 대상 위치로 점진적으로 이동하는 잠재 코드의 최적화와 핸들 포인트의 궤적을 충실하게 추적하는 포인트 추적 절차입니다. 두 구성 요소 모두 GAN의 중간 피처 맵의 차별적 품질을 활용하여 픽셀 단위의 정확한 이미지 변형과 대화형 성능을 제공합니다. 저희는 저희의 접근 방식이 GAN 기반 조작에서 최첨단 기술을 능가하고 생성적 사전 확률을 사용하여 강력한 이미지 편집을 위한 새로운 방향을 열었다는 것을 입증했습니다. 향후 작업에서는 포인트 기반 편집을 3D 생성 모델로 확장할 계획입니다. 감사의 말 Christian Theobalt는 ERC Consolidator Grant 4DReply(770784)의 지원을 받았습니다. Lingjie Liu는 Lise Meitner 박사후 펠로우십의 지원을 받았습니다. 이 프로젝트는 Saarbrücken Research Center for Visual Computing, Interaction and AI의 지원도 받았습니다. 참고문헌 Rameen Abdal, Yipeng Qin, Peter Wonka. 2019. Image2stylegan: 이미지를 stylegan 잠재 공간에 임베드하는 방법?. ICCV에서. Rameen Abdal, Peihao Zhu, Niloy J Mitra, Peter Wonka. 2021. Styleflow: 조건부 연속 정규화 흐름을 사용하여 stylegan에서 생성된 이미지에 대한 속성 조건 탐색. ACM Transactions on Graphics(ToG) 40, 3(2021), 1-21. Thaddeus Beier와 Shawn Neely. 2023. 특징 기반 이미지 변형. Seminal Graphics Papers: Pushing the Boundaries, Volume 2. 529-536. Mario Botsch와 Olga Sorkine. 2007. 선형 변분 표면 변형 방법에 관하여. IEEE 시각화 및 컴퓨터 그래픽 거래 14, 1(2007), 213-230. Thomas Brox와 Jitendra Malik. 2010. 대변위 광학 흐름: 변분 동작 추정에서의 기술자 매칭. IEEE 패턴 분석 및 머신 인텔리전스 거래 33, 3(2010), 500-513. Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein. 2022. 효율적인 기하학 인식 3D 생성적 적대 네트워크. CVPR에서. Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein. 2021. pi-gan: 3D 인식 이미지 합성을 위한 주기적 암묵적 생성적 적대 네트워크. CVPR에서. Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, Jingyi Yu. 2022. Sofgan: 동적 스타일링을 갖춘 초상화 이미지 생성기. ACM Transactions on Graphics(TOG) 41, 1(2022), 1-26. Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha. 2020. StarGAN v2: 다중 도메인을 위한 다양한 이미지 합성. CVPR에서. Edo Collins, Raja Bala, Bob Price, Sabine Susstrunk. 2020. 스타일 편집: gan의 로컬 의미론 밝히기. CVPR에서. 5771-5780. Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath. 2018. 생성적 적대적 네트워크: 개요. IEEE 신호 처리 매거진 35, 1(2018), 53-65. Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong. 2020. 3D 모방-대조 학습을 통한 얽힘 해제 및 제어 가능한 얼굴 이미지 생성. CVPR에서. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, Thomas Brox. 2015. Flownet: 합성곱 네트워크를 사용한 광학 흐름 학습. ICCV에서. Yuki Endo. 2022. StyleGAN 이미지 레이아웃 편집을 위한 사용자 제어 가능 잠재 변환기. Computer Graphics Forum 41, 7 (2022), 395–406. https://doi.org/10.1111/ cgf.Dave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, Alexei A Efros. 2022. Blobgan: 공간적으로 분리된 장면 표현. ECCV에서. 616–635. Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy, Wayne Wu, Ziwei Liu. 2022. StyleGAN-Human: 인간 세대의 데이터 중심 오디세이. ECCV에서. Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael J Black, Timo Bolkart. 2020. GIF: 생성적 해석 가능한 얼굴. 3D 비전(3DV) 국제 컨퍼런스에서. Dan B Goldman, Chris Gonterman, Brian Curless, David Salesin, Steven M Seitz. 2008. 비디오 객체 주석, 탐색 및 구성. 사용자 인터페이스 소프트웨어 및 기술에 대한 제21회 연례 ACM 심포지엄의 진행 중. 3-12. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 2014. 생성적 적대적 네트워크. NeurIPS에서. Jiatao Gu, Lingjie Liu, Peng Wang, Christian Theobalt. 2022. StyleNeRF: 고해상도 이미지 합성을 위한 스타일 기반 3D 인식 생성기. ICLR에서. Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris. 2020. GANSpace: 해석 가능한 GAN 제어 발견. arXiv 사전 인쇄본 arXiv:2004.02546(2020). Adam W. Harley, Zhaoyuan Fang, Katerina Fragkiadaki. 2022. Particle Video Revisited: 점 궤적을 사용한 폐색 추적. ECCV에서. Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020. 확산 확률적 모델의 노이즈 제거. NeurIPS에서. Takeo Igarashi, Tomer Moscovich, John F Hughes. 2005. 가능한 한 단단한 모양 조작. ACM 그래픽스 거래(TOG) 24, 3(2005), 1134–1141. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox. 2017. Flownet 2.0: 심층 네트워크를 사용한 광학 흐름 추정의 진화. CVPR에서. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros. 2017. 조건부 적대 네트워크를 사용한 이미지 간 변환. CVPR에서. Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila. 2021. 별칭 없는 생성적 적대 네트워크. NeurIPS에서. Tero Karras, Samuli Laine, Timo Aila. 2019. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처. CVPR에서. 4401-4410. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila. 2020. 스타일간의 이미지 품질 분석 및 개선. CVPR에서. 8110-8119. GAN 드래그: 생성 이미지 매니폴드에서의 대화형 포인트 기반 조작 SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 Davis E. King. 2009. Dlib-ml: 머신 러닝 툴킷. Journal of Machine Learning Research 10(2009), 1755-1758. Diederik P Kingma와 Jimmy Ba. 2014. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980(2014). Thomas Leimkühler와 George Drettakis. 2021. FreeStyleGAN: 카메라 매니폴드를 사용한 자유 시점 편집 가능 초상화 렌더링. 40, 6(2021). https://doi.org/10.1145/ 3478513. Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. 2021. Editgan: 고정밀 의미적 이미지 편집. NeurIPS에서. Yaron Lipman, Olga Sorkine, Daniel Cohen-Or, David Levin, Christian Rossi, and Hans-Peter Seidel. 2004. 대화형 메시 편집을 위한 미분 좌표. Proceedings Shape Modeling Applications, 2004에서. IEEE, 181-190. Yaron Lipman, Olga Sorkine, David Levin, and Daniel Cohen-Or. 2005. 메시를 위한 선형 회전 불변 좌표. ACM Transactions on Graphics(ToG) 24, 3(2005), 479-487. Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel Cohen-Or, and Michal Irani. 2022. Self-distilled stylegan: Towards generation from internet photos. ACM SIGGRAPH 2022 Conference Proceedings. 1–9. Xingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, and Bo Dai. 2021. 셰이딩 기반 생성 암묵적 모델로 형상 정확도 3D 인식 이미지 합성. NeurIPS에 실림. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. 공간적 적응 정규화를 통한 의미적 이미지 합성. CVPR에 실림. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer. 2017. PyTorch에서의 자동적 차별화. (2017). Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski. 2021. Styleclip: Stylegan 이미지의 텍스트 기반 조작. ICCV에서. Justin NM Pinkney. 2020. Awesome 사전 학습된 StyleGAN2. https://github.com/ justinpinkney/awesome-pretrained-stylegan2. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125 (2022). Daniel Roich, Ron Mokady, Amit H Bermano, Daniel Cohen-Or. 2022. 실제 이미지의 잠재 기반 편집을 위한 피벗 튜닝. ACM Transactions on Graphics (TOG) 42, 1 (2022), 1-13. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2021. 잠재 확산 모델을 사용한 고해상도 이미지 합성. arXiv:2112.10752 [cs.CV] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes 등. 2022. 심층 언어 이해를 통한 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487(2022). Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: 3D 인식 이미지 합성을 위한 생성적 광도 필드. NeurIPS에서. Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. 의미적 얼굴 편집을 위한 gans의 잠재 공간 해석. CVPR에서. Yujun Shen과 Bolei Zhou. 2020. GANS에서 잠재 의미론의 폐쇄형 형태 인수분해. arXiv 사전 인쇄본 arXiv:2007.06600(2020). Ivan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. 2021. 연결 불가능한 것을 연결하기 위한 잠재 공간과 이미지 공간 정렬. arXiv 사전 인쇄본 arXiv:2104.(2021). Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 2015. 비평형 열역학을 사용한 심층적 비지도 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 2256-2265. Jiaming Song, Chenlin Meng, Stefano Ermon. 2020. 확산 암시적 모델의 노이즈 제거. ICLR에서. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 2021. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. 학습 표현 국제 컨퍼런스에서. Olga Sorkine, Marc Alexa. 2007. 가능한 한 단단한 표면 모델링. 기하학 처리 심포지엄, Vol. 4. Citeseer, 109-116. Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian Rössl, and HP Seidel. 2004. 라플라시안 표면 편집. 기하학 처리에 대한 2004 Eurographics/ACM SIGGRAPH 심포지엄의 진행 중. 175-184. Narayanan Sundaram, Thomas Brox, and Kurt Keutzer. 2010. GPU 가속된 대변위 광학 흐름에 의한 고밀도 점 궤적. ECCV에서. Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, and Huachun Zhu. 2018. 내부 표현 콜라주를 사용한 공간적으로 제어 가능한 이미지 합성. arXiv 사전 인쇄본 arXiv:1811.10153(2018). Zachary Teed와 Jia Deng. 2020. Raft: 광학 흐름을 위한 반복적인 모든 쌍 필드 변환. ECCV에서. Ayush Tewari, Mallikarjun BR, Xingang Pan, Ohad Fried, Maneesh Agrawala, Christian Theobalt. 2022. Disentangled3D: 단안 이미지에서 Disentangled Geometry와 Appearance를 사용하여 3D 생성 모델 학습. CVPR에서. Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Pérez, Michael Zollhofer, Christian Theobalt. 2020. StyleRig: 인물 이미지에 대한 3D 제어를 위한 StyleGAN 리깅. CVPR에서. Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn. 2021. 원샷 의미적 부분 분할을 위한 gans 재활용. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 4475-4485. Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou. 2022b. 공간 인식을 높여 gan 평형 개선. CVPR에서. 1128511293. Sheng-Yu Wang, David Bau, Jun-Yan Zhu. 2022a. GAN의 기하학적 규칙 다시 쓰기. ACM Transactions on Graphics(TOG)(2022). Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, Bolei Zhou. 2022. 구조 및 질감 표현 학습을 통한 3D 인식 이미지 합성. CVPR에서. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser 및 Jianxiong Xiao. 2015. Lsun: 인간이 루프에 참여하는 딥 러닝을 사용하여 대규모 이미지 데이터 세트 구축. arXiv 사전 인쇄 arXiv:1506.03365 (2015). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman 및 올리버 왕. 2018. 지각 지표로서 심층 기능의 불합리한 효과. CVPR에서. Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba 및 Sanja Fidler. 2021. DatasetGAN: 최소한의 인간 노력으로 효율적인 레이블이 지정된 데이터 팩토리. CVPR에서. Jiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli Zhao, Qifeng Chen. 2023. LinkGAN: 제어 가능한 이미지 합성을 위해 GAN 잠재 데이터를 픽셀에 연결. arXiv 사전 인쇄본 arXiv:2301.04604(2023). Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A Efros. 2016. 자연스러운 이미지 매니폴드에 대한 생성적 시각적 조작. ECCV.SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 X. Pan, A. Tewari, T. Leimkühler, L. Liu, A. Meka, C. Theobalt Ours UserControllableLT Inputs Ours UserControllableLT Inputs 그림 10. 정성적 비교. 이는 그림 4의 확장입니다. 입력 대상 Ours 입력 대상 Ours 그림 11. 얼굴 랜드마크 조작. 우리의 방법은 이처럼 밀집된 키포인트 경우에도 잘 작동합니다. GAN 드래그: 생성 이미지 매니폴드에서의 대화형 포인트 기반 조작 SIGGRAPH &#39;23 컨퍼런스 회의록, 2023년 8월 6-10일, 캘리포니아주 로스앤젤레스, 미국 1차 편집(발) 2차 편집(입) 3차 편집(귀) 藏藏 그림 12. 연속적 이미지 조작. 사용자는 이전 조작 결과에 따라 조작을 계속할 수 있습니다. 실제 이미지 GAN 반전 GAN 반전. 1차 편집(머리카락) 2차 편집(표정) GAN 반전 3차 편집(포즈) GAN 반전. GAN 반전. 그림 13. 실제 이미지 조작. (a) 분포 이탈 포즈 (b) 텍스처 없는 핸들 포인트 (c) 텍스처가 풍부한 핸들 포인트 그림 14. 한계. (a) StyleGAN-human [Fu et al. 2022]은 대부분 팔과 다리가 아래로 향한 패션 데이터 세트에서 학습되었습니다. 분포 이탈 포즈 쪽으로 편집하면 다리와 손에 표시된 것처럼 왜곡 아티팩트가 발생할 수 있습니다. (b)&amp;(c) 텍스처가 없는 영역의 핸들 포인트(빨간색)는 백미러와의 상대적 위치에서 볼 수 있듯이 추적 중에 더 많은 드리프트가 발생할 수 있습니다. 입력 W+ W 그림 15. 마스크의 효과. 전경 개체를 마스크하면 배경을 고정할 수 있습니다. 나무와 풀의 세부 사항은 거의 변경되지 않습니다. 더 나은 배경 보존은 잠재적으로 특징 혼합을 통해 달성될 수 있습니다[Suzuki et al. 2018]. 그림 16. W/W+ 공간의 효과. W+ 공간에서 잠재 코드를 최적화하면 고양이의 한쪽 눈만 감는 것과 같은 분포 밖 조작을 달성하기가 더 쉽습니다. 반면, W 공간은 이미지를 훈련 데이터 분포 내에 유지하는 경향이 있으므로 이를 달성하는 데 어려움을 겪습니다.
