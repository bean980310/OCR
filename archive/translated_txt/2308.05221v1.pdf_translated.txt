--- ABSTRACT ---
Alexa Prize 프로그램은 수많은 대학생이 SocialBot Grand Challenge 및 TaskBot Challenge와 같은 도전을 통해 대화형 에이전트를 구축하는 데 재능을 탐구하고, 실험하고, 선보일 수 있도록 지원했습니다. 대화형 에이전트가 멀티모달 및 구체화된 맥락에서 점점 더 많이 등장함에 따라 컴퓨터 비전과 물리적 구체화로 증강된 대화형 상호 작용의 가능성을 탐구하는 것이 중요합니다. 이 논문에서는 대학 팀이 시뮬레이션된 물리적 환경에서 작업을 완료하는 로봇 조수를 구축하기 위해 경쟁하는 새로운 도전인 SimBot Challenge에 대해 설명합니다. 이 논문은 온라인 및 오프라인 도전 단계를 모두 포함하는 SimBot Challenge에 대한 개요를 제공합니다. Alexa Arena, 시뮬레이션된 환경, 팀에 제공된 ML 툴킷을 포함하여 팀에 제공된 인프라와 지원을 설명하여 비전 및 언어 모델 구축을 가속화합니다. 참여 팀이 연구 과제를 극복하기 위해 취한 접근 방식을 요약하고 얻은 주요 교훈을 추출합니다. 마지막으로 경쟁 중 경쟁하는 SimBot의 성과에 대한 분석을 제공합니다. 1
--- EXPERIMENT ---
, SocialBot Grand Challenge 및 TaskBot Challenge와 같은 챌린지를 통해 대화형 에이전트를 구축하는 재능을 선보입니다. 대화형 에이전트가 멀티모달 및 구체화된 맥락에서 점점 더 많이 등장함에 따라 컴퓨터 비전과 물리적 구체화로 증강된 대화형 상호 작용의 가능성을 탐구하는 것이 중요합니다. 이 논문에서는 대학 팀이 시뮬레이션된 물리적 환경에서 작업을 완료하는 로봇 조수를 구축하기 위해 경쟁하는 새로운 챌린지인 SimBot Challenge에 대해 설명합니다. 이 논문은 온라인 및 오프라인 챌린지 단계를 모두 포함하는 SimBot Challenge에 대한 개요를 제공합니다. Alexa Arena, 시뮬레이션 환경, 팀에 제공된 ML 툴킷을 포함하여 팀에 제공된 인프라와 지원을 설명하여 비전 및 언어 모델 구축을 가속화합니다. 참여 팀이 연구 과제를 극복하기 위해 취한 접근 방식을 요약하고 얻은 주요 교훈을 추출합니다. 마지막으로 경쟁 중 경쟁하는 SimBot의 성과에 대한 분석을 제공합니다. 1 서론 Amazon Alexa, Apple의 Siri, Google Assistant와 같은 대화형 비서는 사람들이 정보와 콘텐츠에 액세스하고 스마트 콘센트, 조명, 홈 보안 시스템과 같은 연결된 장치를 제어하는 점점 더 흔한 방법이 되었습니다. 대화형 1st Proceedings of Alexa Prize SimBot(Alexa Prize 2023)의 핵심 전선입니다. AI는 말로 하는 대화에서 발전하여 대화 에이전트가 물리적 세계를 인식하고, 그 안에서 탐색하고, 객체를 이동하고 조작할 수 있는 구체화된 대화 시스템을 가능하게 합니다. 미래에는 일상적인 대화형 비서가 물리적 세계에서 탐색하고 작동할 수 있는 세상을 상상합니다. 예를 들어, 오믈렛을 만들어 주거나, 커피를 따라 주거나, 슬리퍼를 찾기 위해 집을 탐험하거나, 열려 있는 문이나 수도꼭지 누수와 같은 문제를 식별하여 해결할 수 있습니다. Alexa Prize¹는 Amazon Alexa가 후원하는 프로그램으로, 최근 몇 년 동안 수백 명의 대학생과 교수진이 대화형 AI의 최첨단 기술을 발전시키기 위해 경쟁할 수 있게 했습니다. 2016년부터 SocialBot Grand Challenge는 전 세계 대학이 최고의 SocialBot, 즉 인기 있는 주제와 시사에 관해 사용자와 확장된 오픈 도메인 대화를 나눌 수 있는 Alexa 스킬을 만드는 대회를 개최했습니다[1]. 2021년부터 TaskBot Challenge는 팀이 요리법이나 DIY(Do It Yourself) 프로젝트와 같은 복잡한 작업을 완료하는 데 도움을 줄 수 있는 대화형 도우미를 만드는 데 참여하게 했습니다[2]. 이 프로그램의 주요 장점 중 하나는 대학 팀이 Alexa를 통해 대규모로 실제 사용자와 함께 테스트를 통해 접근 방식을 빠르게 테스트하고 반복할 수 있다는 것입니다. * 그림 1: 시뮬레이션된 방에서 로봇의 자기중심적 관점. 세 번째 Alexa Prize Challenge인 SimBot Challenge의 동기는 구체화된 AI의 경계를 넓히고 위의 비전을 향해 나아가는 것입니다. 게임이 핵심 AI 역량의 진화를 보여주고 촉진하는 데 중요한 역할을 했다는 데 영감을 받아 SimBot Challenge는 게임 디자인 요소를 개발 프로세스에 통합합니다. 게임 Go에 AI를 적용하는 것에 대한 Zobrist의 논문[3]에서 강조했듯이 &quot;이 복잡한 게임에 대한 추가 연구를 통해 인간의 지각 및 문제 해결 능력에 대한 새로운 통찰력을 얻을 수 있으며 인공 지능을 위한 새로운 기술 개발을 촉진할 수 있습니다.&quot; Alexa 사용자가 시스템과 상호 작용하도록 용이하게 하기 위해, 그리고 물리적 구현의 전조로서 SimBot Challenge는 Unity 게임 엔진을 사용하여 만든 Alexa Arena[4]라는 시뮬레이션 사무실/연구실 환경을 제공합니다. 이 환경은 다양한 장치와 물체가 장착된 여러 개의 방으로 구성되어 있습니다. 사용자는 로봇 동반자 SimBot에게 지침을 제공하여 환경과 상호 작용합니다. 게임에서 영감을 받은 프레임워크를 채택함으로써 이 챌린지는 사용자에게 Echo Show 및 FireTV와 같은 화면 지원 장치를 통해 시뮬레이션과 상호 작용할 수 있는 매력적이고 역동적인 환경을 제공합니다. SimBot은 사용자의 음성 명령에 응답하여 환경으로 이동하여 조작하고, ¹https://www.amazon.science/alexa-prize구두 질문을 하고 피드백을 제공하는 등의 행동을 취합니다. 게임 메커니즘과 AI 개발의 융합으로 몰입형 사용자 경험이 가능해져 가상 시뮬레이션과 물리적 구현 간의 격차를 메웁니다. 그림 1에서 로봇이 시뮬레이션된 환경을 Echo Show 기기에 표시하는 모습을 볼 수 있습니다. SimBot은 2022년 11월에 Amazon 직원 집단을 대상으로 테스트를 위해 처음 출시되었고, 2022년 12월에 일반 대중에 출시되었습니다. 이 때 지원되는 기기를 사용하는 미국의 Alexa 사용자는 화면에 표시된 Alexa 기기에 &quot;alexa play with robot&quot;이라고 말하여 참여하는 SimBot과 상호 작용할 수 있었습니다. 프로그램의 초기 사전 단계로 TEACh 구현 AI 데이터 세트[5]를 사용하여 오프라인 평가를 수행했습니다. 이 단계는 섹션 2에서 요약합니다. 섹션 3에서는 온라인 챌린지의 운영에 대해 자세히 설명합니다. 섹션 3.1에서는 도전을 지원하는 인프라와 팀에 제공된 도구 및 기능을 설명합니다.섹션 4에서는 경쟁의 과학적 혁신 및 진전에 대해 논의합니다.SimBots의 성능은 섹션 5에서 검토하고 SimBot Challenge에서 수집한 통찰력은 섹션 6에서 논의합니다.2 오프라인 도전 팀이 구체화된 작업 완료를 위한 모델링 기능을 개발하기 위한 초기 단계로 TEACh 데이터 세트[5]를 사용하여 오프라인 도전을 수행했습니다.TEACh 데이터 세트는 주석자가 시뮬레이션된 홈 환경에서 작업을 완료하기 위해 협업하는 사령관(사용자)과 추종자(로봇) 간의 상호 작용을 롤플레잉하는 것으로 구성됩니다.데이터는 AI2-THOR 시뮬레이터[6]를 사용하여 수집되었으며, 두 주석자 모두 1인칭 관점에서 시뮬레이션된 홈을 탐색하고 관찰할 수 있는 웹 인터페이스를 사용했습니다.이 설정에서 사령관만 작업 세부 정보에 액세스할 수 있는 반면 추종자는 실내의 개체와 상호 작용하여 작업을 완료할 수 있으므로 라이브 텍스트 채팅을 통한 두 사람 간의 통신 및 조정이 필요했습니다. 여러 턴에 걸친 상호작용을 장려하기 위해 사령관은 직접 보이지 않는 물체를 추가로 검색하여 추종자에게 적절한 방향을 제공할 수 있습니다. 추종자는 8가지 가능한 물체 상호작용 동작(픽업, 배치, 열기, 닫기, 토글 켜기, 토글 끄기, 슬라이스, 붓기)을 수행할 수 있습니다. 작업을 성공적으로 완료하려면 방을 돌아다니고, 캐비닛이나 냉장고와 같은 용기 내부의 물체를 검색하고, 물리적 상태 변화에 대해 추론해야 합니다(예: 스토브 위에 있는 팬에 감자 조각을 놓고 스토브를 켜서 요리하기). 각 데이터 수집 세션에서 각 주석자가 수행한 초기 상태, 대화 발화 및 동작을 저장하여 게임플레이 세션을 AI2-THOR 환경에서 재생할 수 있었습니다. Alexa Prize SimBot Offline Challenge에서는 팀이 이 데이터 세트를 기반으로 EDH(대화 기록에서 실행) 작업에 대한 모델을 구축해야 했습니다. 추종자의 일부 대화 기록과 일부 동작 시퀀스 및 해당 자기중심적 이미지 관찰이 주어지면 EDH 모델은 추종자의 후속 동작을 예측해야 합니다. EDH 인스턴스는 모든 대화 발화 쌍 사이의 동작 시퀀스를 조사하여 데이터 수집 세션에서 구성됩니다. 대상 동작 시퀀스는 비어 있지 않은 이전 대화 기록, 동작 시퀀스 내에 하나 이상의 개체 상호 작용이 있는지, 하나 이상의 작업 관련 개체에 상태 변경이 포함되어 있는지와 같은 기준에 따라 선택됩니다. EDH 모델은 대화 기록, 팔로워의 동작 기록, 해당 자기중심적 이미지 관찰을 포함하는 입력을 수신합니다. 각 시간 단계에서 모델은 개체 상호 작용, 탐색 동작 또는 특수 중지 동작이 될 수 있는 동작을 예측할 책임이 있습니다. 모델이 개체 상호 작용 동작을 예측하는 경우, 동작에 대한 대상 개체를 식별하기 위해 팔로워의 자기중심적 관찰에서 (x, y) 좌표를 추가로 예측해야 합니다. 모델이 예측한 동작이 시뮬레이터에서 실행된 후, 시뮬레이터 상태가 업데이트되고 모델은 업데이트된 자기중심적 이미지 관찰을 수신합니다. 모델이 중지 동작을 예측하거나, 1,000개의 동작이 실행되거나, 30개의 동작이 API 실패로 이어질 때까지 실행 프로세스가 계속됩니다. 모델은 주석자가 수행한 동작과 모델의 예측된 동작으로 인한 상태 변화를 비교하여 평가합니다.팀에는 TEACh 세션을 재생하는 코드와 TEACh EDH 모델에 대한 추론을 훈련하고 수행하는 래퍼 코드가 제공되었습니다.또한 Episodic Transformer[7] 모델을 기반으로 하는 기준 모델이 팀에 제공되었습니다.챌린지의 다음 단계로 진행하려면 팀은 기준 ET 모델보다 성능이 뛰어난 모델을 개발하여 제출해야 했습니다.3 온라인 챌린지 챌린지의 다음 단계는 온라인 챌린지로, 모델이 런타임 서비스에 통합되어 Alexa 멀티모달 장치에서 실시간 게임 경험을 지원합니다.온라인 경험에서 로봇은 Unity 게임 엔진으로 구동되는 시뮬레이션된 사무실/연구실 환경에서 작동합니다.대학 팀은 비전 모델에서 지원하는 로봇 동작 추론 서비스를 설정하는 과제를 맡았으며, 이는 Alexa Prize 팀에서 개발한 런타임 시스템에 원활하게 통합되었습니다.전체 상호 작용 경험은 Alexa 스킬을 기반으로 구축되어 Alexa Web API for Games 인터페이스를 활용합니다. 사용자는 음성 명령을 통해 로봇과 상호 작용하고 비디오 스트리밍을 통해 장치로 시뮬레이션된 환경에서 업데이트를 관찰할 수 있습니다. 장치를 통해 전송된 음성 요청은 Alexa 음성 인식 서비스에 의해 텍스트로 변환되고, 처음에는 SimBot 기술에 의해 처리됩니다. 그런 다음 사용자 명령은 SimBot 런타임 시스템으로 전달되고, 여기서 시뮬레이션된 환경 내에서 실행 가능한 동작을 생성하기 위해 추가 해석을 거칩니다. 모델은 입력 발화 텍스트와 로봇의 자기중심적 관점 이미지를 기반으로 로봇의 동작을 예측하거나 대화에 참여하는 역할을 합니다. SimBot과의 상호 작용을 마치면 사용자는 구두 평가와 선택적 자유형 코멘트 형태로 피드백을 제공할 수 있는 기회를 얻습니다. 이러한 평가와 피드백은 대학 팀과 공유하는 귀중한 리소스로, 이를 통해 통찰력을 얻고 모델 성능을 개선할 수 있습니다. SimBot 온라인 단계는 2022년 8월에 포괄적인 3일간의 부트캠프로 시작되었습니다. 이 이벤트 동안 10개 대학 팀이 Amazon Web Service(AWS) 교육, SimBot 툴링 및 실습 개발 경험을 받도록 독점적으로 초대되었습니다. 부트캠프 내내 10개 팀 모두 Alexa Prize에서 제공한 기준 모델을 사용하여 AWS와 Alexa에서 제공하는 리소스를 활용하여 SimBot을 성공적으로 개발했습니다. 이 교육을 마친 후, 팀은 10월 말까지 SimBot을 개선하고 향상시키는 데 노력을 기울였으며, 궁극적으로 Alexa 사용자와의 통합에 필요한 기술 인증 프로세스를 완료했습니다. 그런 다음 베타 사용자로부터 조기 피드백을 수집하기 위한 초기 피드백 단계가 진행되었고, 그 후 2022년 12월에 일반 공급이 시작되었습니다. 10개 팀 모두 출시 단계에서 진행되어 2023년 2월 2일부터 3월 22일까지 준결승에 진출했습니다. 준결승에서 5개 팀이 결승 진출자로 성공적으로 자격을 얻었고 2023년 4월 28일에 종료된 결승 단계에 참여했습니다. 비공개 결승 이벤트는 2023년 5월 3일에 진행되었으며, 팀은 최고의 영예를 위해 경쟁했습니다. 3.1 팀에 제공되는 기능 SimBot 개발을 용이하게 하기 위해 대학 팀은 다양한 Amazon Alexa 리소스, 기술 및 전문가에 대한 독점 액세스 권한을 부여받았습니다. 다음은 그들에게 제공된 리소스에 대한 개요입니다.3.1.1 Alexa Arena 시뮬레이션 환경 Alexa Arena[4]는 Amazon Alexa AI가 구축한 Unity 기반 3D 구현 AI 시뮬레이터입니다.Alexa Arena에서 에이전트는 다양한 실내 객체 상호 작용 작업을 지원하는 3D 환경에서 작동합니다.Alexa Arena는 고품질 그래픽, 애니메이션, 탐색 및 객체 조작을 제공하여 고도로 상호 작용적이고 사용자 중심의 멀티모달 구현 AI 연구를 가능하게 합니다.Alexa Arena에는 336개의 고유한 객체가 있습니다.각 객체에는 특정 유형의 로봇-객체 상호 작용이 가능한지 여부를 지정하는 속성 집합(예: 제공 기능)이 있습니다.예를 들어, 에이전트는 객체 속성이 토글 가능하므로 3D 프린터를 토글할 수 있습니다.픽업 가능, 열 수 있음, 깨질 수 있음, 용기, 토글 가능, 전원 공급 가능, 더러워질 수 있음, 가열 가능, 먹을 수 있음, 냉장 가능, 채울 수 있음, 조리 가능, 감염 가능 및 장식을 포함하여 총 14개의 객체 속성이 있습니다. 각 객체 속성에는 동작을 받을 때 전환되는 해당 동작과 객체 상태가 있습니다.예를 들어, break는 breakable에 대한 해당 동작이고 broken은 동작이 수행된 후의 해당 상태입니다.Alexa Arena에서 로봇 동작은 두 가지 유형으로 분류할 수 있습니다.1) 대화를 시작하거나 장면에서 객체를 강조 표시하여 사용자와 통신하기 위한 사용자 상호 작용 동작², 2) 시뮬레이션 환경과 상호 작용하기 위한 로봇 물리적 동작.로봇 물리적 동작에는 탐색과 객체 상호 작용이 모두 포함됩니다.사용자 경험을 개선하기 위해 모든 탐색 및 상호 작용 동작은 연속적인 방식으로 애니메이션화되고 환경 사운드가 함께 제공됩니다.2강조 표시는 로봇의 지시적 제스처에 대한 프록시로 사용됩니다.현재 세대의 SimBot은 팔을 사용하여 가리킬 수 없습니다.3.1.2 ML 툴킷 Alexa Arena 시뮬레이터와 함께 모델 개발을 지원하기 위해 팀에 ML 툴킷도 제공했습니다.이 툴킷은 SimBot Challenge에서 게임 미션을 완료하기 위한 기본적인 시각적 인식, 동작 예측 및 대화 관리를 처리할 수 있는 기준 로봇 모델(그림 2)을 제공합니다. 또한 툴킷에는 로봇 모델 학습을 돕는 두 개의 데이터 세트가 포함되어 있습니다. 첫 번째 데이터 세트는 실제 로봇 동작 궤적과 사람이 주석을 단 대화가 페어링된 하이브리드 데이터 세트입니다. 두 번째 데이터 세트는 객체 감지 모델을 학습하는 데 사용할 수 있는 객체 분할로 레이블이 지정된 600,000개 이상의 이미지 컬렉션으로 구성되어 있습니다. 색상 이미지 객체 마스크 비전 모델 객체 클래스 자연어 명령 공동 참조 해상도 공동 참조 고정 명령 의미 파서 모델 동작 동작 요청 생성기 명령 내역 DynamoDB 테이블 마지막 동작 상태 게임 환경 동작 요청 JSON 그림 2: ML 툴킷에서 제공하는 기준 모델. 기준 모델 내에서 시각 인식 모듈은 제공된 이미지 데이터 세트에서 학습된 Mask-RCNN 모델입니다. 이 모델은 RGB 이미지를 입력으로 사용하여 이미지에 있는 모든 객체 인스턴스(86개 객체 클래스에 걸쳐)에 대한 마스크를 예측합니다. 이 모델은 검증 세트에서 합리적인 객체 감지 성능을 보입니다. 표 1은 소형, 중형 및 대형 객체에 대한 평균 평균 정밀도(mAP)를 보여줍니다. Obj 범주 소형 중형 대형 전체 면적(px²) MAP 0 -37.1296-60.9216-64.0-46.표 1: 소형, 중형 및 대형 객체에 대한 제공된 Mask-RCNN 모델에 대한 평가 결과.3.1.3 자동 음성 인식 및 텍스트 음성 변환 SimBots와 상호 작용하는 사용자의 경험을 개선하기 위해 사용자 발화를 텍스트로 변환하는 자동 음성 인식(ASR) 기술과 SimBots에서 음성 응답을 생성하는 텍스트 음성 변환(TTS) 기술을 제공했습니다.또한 참여 대학 팀은 각 토큰에 대한 신뢰도 점수가 포함된 토큰화된 N-best ASR 가설에 액세스할 수 있었습니다.이 리소스를 통해 팀은 사용자와 보다 정확하고 효과적으로 통신하기 위해 SimBots를 미세 조정하고 최적화할 수 있었습니다.ASR의 정확도를 더욱 개선하기 위해 SimBot 기술 상호 작용 모델을 확장하고 SimBot 기술 의도에 대한 힌트를 도입했습니다. 이 모델에는 다양한 범위의 지원되는 로봇 동작과 객체를 포함하는 10,000개 이상의 샘플 발화가 포함되어 있으며, 팀에 모델에 통합할 템플릿으로 제공되었습니다. 광범위한 가능한 상호 작용을 포괄하는 포괄적인 힌트 세트를 통해 팀은 사용자 요청을 더 잘 이해하고 이에 대응할 수 있는 보다 정확한 모델을 만들 수 있었습니다.Alexa 기기 음성 Alexa 서비스 시각적 요소 및 TTS 로봇 연구실에서 스패너를 받으세요.e Alexa 사용자 원시 텍스트 ASK 기술 시각적 요소 및 응답 원시 텍스트 및 시각적 요소 SimBot 런타임 시스템 동작 시퀀스 시뮬레이션 컨테이너 동작 경기장 시뮬레이션 엔진(Unity) 시각적 요소 동작 추론 서비스 추론 모델 그림 3: SimBot 런타임 서비스와의 사용자 상호 작용 시퀀스 흐름을 보여주는 흐름도.3.1. 인프라 및 SDK Alexa Prize SimBot Challenge의 일환으로 참여 대학 팀에 Echo Show 및 Fire TV를 포함한 다양한 기기에서 최종 사용자를 위한 게임 경험에 모델을 원활하게 통합하는 강력한 런타임 시스템을 제공했습니다. 이 시스템은 팀이 모델을 선보이고 사용자에게 매력적인 대화형 경험을 제공할 수 있는 기회를 제공합니다.다음 시퀀스 흐름은 최종 사용자와의 한 가지 상호 작용에 대해 그림 3에 표시된 각 단계를 설명합니다.⚫ 1: Alexa 사용자가 &quot;로봇 연구실에서 스패너 하나 가져와&quot;와 같은 자연어 명령을 사용하여 SimBot Skill과 상호 작용합니다.• 2: SimBot Skill이 사용자 발화를 수신하고 컨텍스트 정보로 런타임 시스템을 호출합니다.•⚫ 3: SimBot 런타임 시스템이 로봇의 현재 관점(자기 중심적 관점)에서 이미지를 사용자의 입력 발화와 함께 Action Inference Service(해당 대학 팀에서 개발)로 전송합니다.• 4-5: 대학 모델은 다음 동작 시퀀스(예: 2단계 전진)를 예측하거나 적절한 텍스트 응답을 생성합니다.• 6-7: 예측된 시퀀스(4단계)의 각 동작은 Arena Simulation Engine(Unity를 사용하여 빌드)에서 실행되고 비주얼은 Alexa 기기에 라이브 스트리밍됩니다. 참고: 4-7단계를 반복하여 후속 시퀀스(아래를 보기→램프 토글 켜기/끄기 찾기)를 실행한 후, 대학 모델이 액션 시퀀스가 완료되었는지 확인하고/또는 텍스트 응답을 생성할 때까지 진행합니다.• 8-9: 대학 SimBot(있는 경우)의 언어 응답이 Alexa 기기에서 재생되고, 사용자와의 후속 상호 작용을 위해 마이크가 열립니다.각 턴이 끝나면 SimBot 런타임 시스템이 시뮬레이션 환경의 상태를 확인하여 해당 목표가 성공적으로 완료되었는지 확인합니다.목표가 성공적으로 완료될 때까지 [1-9]단계를 반복합니다.게임 세션이 끝나면 사용자 만족도 점수(1-5)를 요청합니다.위의 시퀀스 흐름에는 아래 나열된 주요 구성 요소가 포함되었습니다.1. SimBot 애플리케이션: AWS Elastic Container Service(ECS) Fargate에서 호스팅되는 RESTful 웹 애플리케이션입니다.최종 사용자와의 게임 세션 수명 주기를 관리합니다. SimBotAlexa 장치 비디오 스트리밍 시뮬레이션 컨테이너 서비스 GameSessionStore 게임 및 장면 정의 SimBot 애플리케이션 WSS WebSocket 프록시 게임 세션 관리자 시뮬레이션 컨테이너 클라이언트 SimulationContainer 상태 캐시 로드 웹 앱 정적 웹 리소스 게임 컨트롤러 시뮬레이션 추론 오케스트레이터 로봇 동작 예측 요청/응답 Alexa Skill Kit SimBot Skill Lambda 게임 요청 aws RobotActionInferenceService 그림 4: SimBot 런타임 시스템 다이어그램 및 워크플로 애플리케이션은 로봇 동작 추론 서비스와 시뮬레이션 컨테이너 간의 입력을 오케스트레이션합니다. 사용자 명령은 시뮬레이션 컨테이너에서 실행할 수 있는 로봇 동작으로 해석됩니다. 또한 멀티모달 장치에서 시뮬레이션 컨테이너로의 WebSocket 연결을 검증하고 프록시하는 프록시 계층 역할도 합니다. 2. 시뮬레이션 컨테이너 서비스: 이 서비스는 Alexa Arena 시뮬레이션 엔진에 대한 래퍼 역할을 하여 로봇 동작을 실행하고 엔진에서 업스트림 애플리케이션으로 비주얼을 스트리밍합니다. 3. 로봇 동작 추론 서비스: 이 구성 요소는 구체화된 에이전트의 두뇌를 나타냅니다. 최종 사용자의 자연어 지침과 Alexa Arena 시뮬레이션 엔진의 라이브 비주얼은 Action Inference 서비스에서 처리되어 해당 로봇 동작 시퀀스와 선택적 설명 대화 상자를 생성합니다. 이를 달성하기 위해 이 서비스는 ML 모델을 호스팅하고 런타임에 추론을 수행합니다. 4. SimBot Skill: 이것은 Alexa Skills Kit(ASK) 위에 구축된 Alexa Skill입니다. SimBot Skill은 Alexa 서비스에서 ASR 출력을 수신합니다. AWS Lambda 함수는 스킬 요청을 처리하고 요청을 SimBot 애플리케이션에 위임합니다. 이 스킬은 또한 Alexa 지원 장치에서 실행되는 멀티모달 게임 경험을 지원하는 Alexa Web API for Games 인터페이스를 사용합니다. Alexa Prize SimBot Challenge는 대학 팀이 머신 러닝과 대화형 AI에 대한 전문성을 입증할 수 있는 기회를 제공합니다. 대학 팀이 과학적 혁신에 집중할 수 있도록 엔지니어링 작업과 운영 작업을 단순화하는 CLI, 스크립트 및 유틸리티를 제공하는 SDK를 개발했습니다. 대학 팀은 SDK를 실행하는 데 최소한의 수동 작업을 사용하고, 자체 시스템에 사소한 코드 변경을 하고, 생성된 후 운영상 유지 관리할 수 있습니다. SimBot SDK는 AWS Cloud Development Kit(CDK)를 활용하여 AWS 계정 내에서 리소스를 프로비저닝하고 관리합니다. SimBot용 CDK 애플리케이션은 ASK 기술 Lambda, Virtual Private Cloud(VPC), Identity Access Management(IAM) 역할, Cloud Watch 로그 및 챌린지에 필요한 기타 리소스의 배포를 자동화합니다. Action Inference 서비스와 기술 Lambda에 대한 지속적인 통합을 제공하여 개발자가 서비스를 반복적으로 업데이트하기 쉽게 합니다. 또한 향상된 안정성을 위해 테스트 및 프로덕션 단계를 분리합니다. 또한 SimBot SDK에는 Action Inference 서비스의 API 계약을 기반으로 하는 템플릿 구현과 같은 여러 유틸리티가 포함되어 있으며, 기준 부트스트랩 모델 및 DynamoDB 테이블과 통합됩니다. 유틸리티는 또한 profanity-filter, spaCy 및 AllenNLP 라이브러리와 같은 ML 유틸리티에 대한 타사 라이브러리에 대한 포인터를 제공합니다.취한 작업 - ASK CLI를 통해 스킬 만들기 ASK CLI개발자 기준 모델 생성 및 테스트 데이터로 평가 생성된 아티팩트 SimBot SDK CDK ML 도구 상자 유틸리티 Alexa 스킬 A - SDK 패키지 가져오기 AWS 코드 커밋/SLambda ECSimBot SDK - AWS 구성 요소 인스턴스화 CDK ML 도구 상자 유틸리티 I Do I Do I Do I Do I Do I Do Do Developer Developer Developer Developer Developer Developer SimBot SDK REST 서비스에서 모델 참조 업데이트 CDK ML 도구 상자 유틸리티 - 스킬 Lambda 참조 업데이트 ASK CLI - 엔드 투 엔드 흐름 테스트 Alexa 장치 모델 개선, 재배포 및 재테스트 CDK SimBot SDK ML 도구 상자 유틸리티 모델</> 코드 파이프라인 IAM VPC VPC 피어링 CloudWatch S입 추론 서비스 완전한 Alexa 스킬(개선됨) 모델 그림 5: 대학 팀의 개발자 경험 3.2 고객 피드백 데이터 및 평가 지표 대학 팀에 제공된 주요 이점은 Alexa 사용자와 함께 SimBot을 필드에 배치할 수 있는 기능이었습니다. 일반 공급 및 준결승 단계 내내 사용자는 SimBot과 상호 작용한 후 만족도 평가와 경험에 대한 자유 형식 피드백을 요청받았습니다. 또한 시스템은 대화 기간과 게임 미션 완료 상태를 포착하도록 계측되었습니다. 미션 완료는 미션 성공률(MSR)이라는 지표로 측정되며, 이는 게임에서 팀의 미션 목표를 성공적으로 완료하는 평균 비율을 계산합니다. N(성공한 미션) N(총 미션) MSR = 평균 사용자 만족도 평가와 미션 성공률은 챌린지의 주요 평가 지표로 사용되었습니다. 각 대학 팀은 이러한 지표에 액세스할 수 있었고 도전에 참여한 모든 SimBot의 평균 지표와 순위를 제시하는 익명화된 리더보드를 매일 받았습니다.이것은 팀에 성과를 평가하고 다른 팀과 비교하여 상대적인 성과를 측정할 수 있는 귀중한 정보를 제공했습니다.또한 팀은 팀의 SimBot과의 상호 작용이 끝날 때 사용자가 공유한 자유형 정성적 피드백의 필사본에 액세스할 수 있어 팀이 SimBot에 대한 사용자의 인상에 대한 정성적 통찰력을 얻을 수 있었습니다.3.3 Alexa Prize 팀의 지원 데이터와 인프라를 제공하는 것 외에도 여러 가지 방법으로 대학 팀과 협력하여 지원과 피드백을 제공했습니다.• SDK에 대학 팀을 온보딩하고 팀을 부트캠프에 준비시키는 가상 사전 부트캠프.• 교육 자료, 모범 사례 및 디자인 지침이 포함된 실습 부트캠프.• CX 디자인, 모델 교육 및 평가, 경쟁 지침에 대한 대학 팀과의 두 가지 가상 세션을 통해 팀이 경쟁의 각 단계에 대비할 수 있도록 준비합니다. • • 모든 Alexa 사용자에게 일반에 공개되기 전에 SimBot 성능을 알리고 개선하는 데 도움이 되는 Amazon 직원의 트래픽을 제공하는 내부 베타 단계.• SimBot의 기능과 익명성을 유지하고 부적절한 상호 작용을 처리하는 SimBot의 능력을 평가하는 공개 출시 전 SimBot 경험에 대한 자세한 보고서.전담 Alexa Prize Solutions Architect, 프로그램 관리자, UX 디자이너 및 Alexa 과학 및 엔지니어링 팀 구성원과의 1:1 컨설팅을 위한 주간 업무 시간.Slack 및 이메일을 통한 Alexa Prize 담당자에게 주문형 액세스.4 과학적 발전 과제 동안 참가자들은 게임 상호 작용 중 사용자 만족도를 높이고 작업 완료율을 개선하기 위해 로봇을 개선하기 위해 적극적으로 노력했습니다.여기에는 데이터 생성 및 주석, 효율적인 데이터 저장 및 검색, 사용자 상호 작용 추적, 시각화 시스템, 대화 관리, 시각적 접지, 동작 예측, 멀티모달 및 언어 이해, 지속적인 개선 워크플로를 포함한 여러 영역에 걸친 과학적 혁신 및 엔지니어링 최적화가 포함됩니다.이 섹션에서는 로봇 구현 중에 참가자들이 탐구한 주요 과학적 발전에 대한 요약을 제시합니다. 각 참여 팀은 이 회의록에서 논문의 일부로 구체적인 혁신을 더 자세히 설명했습니다. 과학적 기여는 구현된 AI 에이전트의 원활한 기능에 도움이 되는 여러 측면에 걸쳐 있습니다. 최종 사용자는 Echo Show 또는 Fire TV 기기를 통해 음성 명령을 사용하여 구현된 AI 에이전트와 상호 작용합니다. 그런 다음 음성 명령은 Alexa ASR 시스템을 사용하여 텍스트로 변환됩니다. 이 변환에 따라 팀은 텍스트 입력을 사용하여 자연어 이해를 수행하고 로봇의 퍼스트 파티 뷰를 사용하여 비전 이해 및 접지를 수행하고 두 가지 모달리티를 결합하여 결국 Alexa Arena에서 사용자가 의도한 지침을 실행합니다. 모델의 일반화는 핵심 과학적 주제였으며 경쟁 단계의 구조에 영향을 미쳤습니다. 도전 내내 참여 로봇은 보이는 게임 미션과 보이지 않는 게임 미션 모두에서 평가되었습니다. 보이는 게임 미션이 있는 단계에서 팀은 게임을 플레이하고 사용자 피드백을 검토하고 그에 따라 로봇을 업데이트할 기회를 가졌습니다. 보이지 않는 게임이 있는 단계에서는 로봇이 이전에 본 적이 없는 미션을 해결하는 능력에 대해 평가되었으며, 로봇 모델에 대한 업데이트나 수정은 허용되지 않았습니다.이러한 보이지 않는 게임에서 로봇은 이전에 보지 못한 미션 목표를 완료하는 동안 새로운 객체와 새로운 동작 유형을 만날 수 있습니다.이러한 과제를 해결하기 위해 팀은 (a) 모든 작업 관련 객체를 포괄하는 강력한 비전 모듈, (b) 사용자 의도를 안정적으로 예측하고 로봇 동작에 매핑할 수 있는 자연어 이해 메커니즘, (c) 보이지 않는 시나리오에서도 사용자에게 유익한 응답과 귀중한 지침을 제공하는 적응형 대화 관리 전략을 구축하여 로봇 내의 다양한 측면의 일반화 가능성을 개선하는 데 중점을 두었습니다.4.1 자연어 이해 및 동작 예측 SimBot Challenge의 각 게임 미션 동안 사용자에게 작업 설명과 하위 목표 목록이 제공되는 반면, SimBot은 사용자의 언어 입력을 통해서만 이 정보에 액세스할 수 있으며, 이 실제 시나리오에서 사용자는 원하는 방식으로 SimBot에게 지시할 수 있습니다. 사용자 발화는 종종 불완전하거나 모호하거나 완전히 도메인 밖입니다. 게다가 사용자 발화는 다양한 수준의 추상화를 가질 수 있습니다. 일부 사용자는 절차적 단계별 지침(예: &quot;머그잔을 집어&quot;)을 제공하는 것을 선호하는 반면, 다른 사용자는 고수준 명령(예: &quot;깨진 그릇을 수리&quot;)이나 동작 조합(예: &quot;휴게실에 있는 냉장고로 가서 머그잔을 집어&quot;)을 제공하는 것을 선호할 수 있습니다. 사용자 지침의 이러한 다양성은 언어 이해의 견고성에 큰 과제를 제기합니다. 사용자 발화를 견고하게 처리하기 위해 대부분의 팀은 모듈형 아키텍처를 채택했습니다. 여기서 입력 언어는 먼저 자연어 처리 모듈(예: 품사 태그, 의미 역할 레이블 지정, 명명된 엔터티 인식, 의도 분류) 또는 신경 모델(예: Transformer[8] 기반 딥 모델)에 의해 처리되어 사용자 의도 및 관련 객체를 식별합니다. 그런 다음 입력의 중간 표현은 종종 로봇 상태와 환경 상태를 고려하여 컨텍스트 인식 예측을 수행하는 상징적 플래너, 사전 정의된 템플릿/규칙 또는 훈련된 신경 모델을 통해 로봇 동작 시퀀스에 매핑됩니다.게다가 일부 팀은 상식적 지식을 동작 예측 프로세스에 주입하기도 했습니다.예를 들어, 객체의 어포던스에 대한 지식은 종종 로봇이 가능성 없는 동작-객체 예측을 제거하는 데 도움이 될 수 있습니다.팀이 직면한 주요 과제 중 하나는 이해를 Alexa Arena에 기반을 두는 것이었습니다.팀은 이 변환을 수행하기 위해 규칙 기반 및 신경 아키텍처의 조합을 제안하여 솔루션을 다른 애플리케이션에도 더욱 다재다능하게 만들었습니다.EMMA 팀은 사전 학습 전략, 데이터 세트 및 사전 학습 작업 커리큘럼을 사용하여 기본 모델을 학습한 다음 결국 Alexa Arena에서 구현된 AI 작업에 대한 모델을 미세 조정하는 기본 변환기 기반 엔드투엔드 모델을 제안했습니다.이 접근 방식은 오프라인과 온라인 모두에서 좋은 성능을 보였습니다.같은 팀은 또한 제안된 사전 학습 전략과 제공된 Alexa Arena 데이터 세트를 사용하여 sim-2-real 전송에 대한 예비 결과를 보여줍니다. 4.2 시각적 접지 게임에서 로봇은 퍼스트파티 RGBD 카메라를 통해 3D 장면을 관찰할 수 있습니다. 객체 관련 동작(객체로 이동하거나 객체를 조작하는 것과 같은)은 로봇이 현재 퍼스트파티 뷰의 이미지를 기반으로 올바른 객체 마스크를 제공해야 합니다. 따라서 로봇이 장면에서 객체를 효율적으로 인식하고 사용자 발화를 해당 객체에 올바르게 접지하는 것이 필수적입니다. 사용자 지시를 장면에서 객체에 접지하기 위해 팀은 신경망 방법을 사용하여 객체 감지(또는 의미 분할)를 수행합니다. 그들의 기여에는 마스크 예측을 위한 기준 마스크 RCNN 모델을 미세 조정하고 객체 범주, 상태 및 관계를 감지하기 위한 추가 모델을 구축하는 것이 포함됩니다. 예를 들어, GauchoAI 팀은 제공된 이미지 데이터 세트를 사용하여 MaskFormer 모델[9]을 미세 조정하여 더 나은 시각적 이해 기능을 보여주었습니다(제공된 기준 시스템에 비해 중형 및 대형 객체의 경우 12%-22% mAP의 절대적 개선). Team Seagull은 거친 객체 유형을 감지하는 Mask2Former 모델, 세밀한 객체 유형 및 상태를 감지하는 ResNet 모델, 높은 정확도로 객체 공간 관계를 검증하는 휴리스틱 방법을 포함한 계층적 시각 인식 시스템을 구축했습니다. Team EMMA는 사전 학습된 VinVL 모델[10]을 Alexa Arena 데이터 세트로 미세 조정하여 감지 정확도를 개선했습니다. 팀에서 객체 감지 클래스의 수도 수정했기 때문에 숫자는 기준 모델 메트릭과 직접 비교할 수 없습니다. 또한 Team EMMA는 공개 GQA 데이터 세트[11]에서 큐레이팅한 합성 데이터 세트를 벤치마킹하여 객체 감지를 위한 sim2real 전송에 대한 예비 결과도 보여주었으며, Alexa Arena 객체와 Alexa Arena 데이터 세트에 없는 다른 객체 간에 유사한 성능을 보였습니다. 같은 팀은 또한 참조된 객체가 여러 번 나타나는 경우 효율적으로 접지 명령을 내리기 위해 시각적 모호성 감지기 모듈을 훈련했습니다. 출력은 접지에서 모호성의 존재를 먼저 예측하는 시퀀스로 모델링된 다음 다운스트림 접지 모듈에서 사용됩니다. KnowledgeBot 팀은 기준 모델을 사용하여 객체 마스크를 생성했지만 플래너에서 생성된 객체를 기반으로 검색할 마스크를 결정합니다.SlugJARVIS 팀은 MaskFormer와 ResNet 기반 분류기 모델을 훈련하여 거친 객체와 세밀한 객체 감지를 모두 수행하여 세밀한 객체 분류에서 93%의 높은 정확도를 보였습니다.또한 객체 상태 감지기와 객체 관계 감지기를 결합하여 객체 상태와 객체 간의 공간 관계를 식별했습니다.팀 전체에서 시각적 접지는 휴리스틱을 사용하거나 시각 언어 모델을 효율적으로 통합하여 수행됩니다.시각적 접지는 언어 입력에 의존하기 때문에 팀은 매우 상호 연결된 언어 및 시각적 접지 모듈을 제안했습니다.객체 접지의 일반적인 과제는 동일한 객체 유형의 인스턴스가 여러 개 있는 경우 발생합니다.때로는 사용자 발화에 위치 정보나 객체 속성(예: 색상)과 같이 모호성을 해소하는 데 도움이 되는 세부 정보가 있습니다.대부분의 팀은 간단한 규칙이나 통계적 모델(예: K-평균 클러스터링)을 기반으로 객체 속성 감지기를 구축했습니다. 효율적인 탐색과 객체 현지화를 용이하게 하기 위해 여러 팀은 각 시간 단계에서 시각적 관찰을 통해 의미 맵과 장면 그래프를 포함한 상징적 장면 표현을 유지합니다. 이 표현을 통해 로봇은 가상 환경을 효율적으로 탐색하고 요청된 객체로 이동할 수 있습니다. 일부 팀은 또한 다양한 방의 다양한 객체에 대한 신념으로 채워진 시각적 메모리를 통합하기 위해 메모리 뱅크를 도입하여 미션 중에 주기적으로 업데이트합니다. 이 접근 방식은 사용자 지침이 이전에 본 객체를 암시할 때 로봇이 쉽게 탐색하는 데 사용할 수 있는 주어진 위치에 대한 보이는 객체의 확률 분포를 제공합니다. 4.3 지식 사용자가 게임 미션을 효율적으로 완료하도록 지원하려면 로봇이 객체와 그 속성, 동작 및 효과와 같은 환경 상태 전환 메커니즘에 대한 충분한 배경 지식을 보유하는 것이 중요합니다. 대부분의 팀은 일반적인 동작 시퀀스와 같은 게임 미션 기반 지식과 객체 어포던스 및 객체 별칭과 같은 보다 일반적인 공통 지식을 포함하여 오프라인 지식 소스 모음을 유지 관리합니다. 오프라인 지식은 동작 예측, 시각적 접지, 객체 모호성 해소, 대화 관리 및 응답 생성에 대한 지침을 제공합니다. 또한 SlugJARVIS 팀은 비전, 텍스트 및 실행된 작업의 다중 모달 정보가 포함된 온라인 지식 세트를 유지 관리하고 적극적으로 업데이트합니다. 그들은 보이지 않는 작업을 식별하고 과거의 성공적인 상호 작용을 활용하여 다양한 환경 및 작업에 적응할 수 있는 점진적이고 진화하는 작업 경험 검색 알고리즘을 제안합니다. 4.4 대화 관리 일반 사용자의 경우 언어를 통해 로봇에게 게임 미션을 완료하도록 지시하는 경험은 특히 로봇의 기능이나 게임 환경의 한계에 익숙하지 않은 경우 스스로 게임 미션을 플레이하는 것과 상당히 다릅니다. 따라서 적절한 피드백을 적극적으로 제공하는 것은 사용자 신뢰를 구축하고 매력적인 사용자 경험을 제공하는 데 중요해집니다. 대부분의 팀은 쉽게 확장 가능하고 지속적인 개발을 용이하게 하는 다양한 템플릿 기반 대화 생성 모듈을 제안합니다. 이러한 모듈에는 대화 행위를 저장하는 데이터 구조, 템플릿 기반 대화 생성 및 추적, 사용자 응답을 이해하기 위한 질문 답변 기반 아키텍처가 포함됩니다. 생성된 응답을 보다 자연스럽고 인간답게 만들기 위해 팀은 또한 다양한 응답 템플릿을 생성하기 위해 대규모 언어 모델(LLM)을 사용하고 음성에 감정적 음조를 추가하는 것을 포함한 다양한 기술을 사용합니다. 게임 미션을 완료하기 위한 사용자의 노력을 더욱 단순화하기 위해 여러 팀은 현재 게임 상태를 기반으로 다음 동작을 사전에 제안하기 위한 전략을 제안합니다. 로봇은 게임 미션 설명에 직접 액세스할 수 없으며 대화 기록과 이전에 실행된 동작을 기반으로 다음 적절한 동작을 추론해야 합니다. 예를 들어, GauchoAI 팀은 최근에 상호 작용한 객체와 그 제공 가능성을 기반으로 제안을 합니다. 예를 들어, 로봇이 가열 가능한 물체를 손에 들고 전자레인지에 접근하면 사용자가 물체를 가열하고자 할 가능성이 높습니다. 4.5 훈련 및 데이터 생성 제공된 Alexa Arena 시뮬레이터, 기준 모델, 궤적 및 비전 데이터 세트를 활용하여 여러 팀은 모델 훈련을 더욱 향상시키기 위해 보다 합성적인 데이터를 생성했습니다. 여기에는 다중 모달 비전 및 언어 데이터 세트와 언어 기반 작업 분해 및 공지시 해결 데이터 세트가 포함됩니다. 예를 들어, 팀 ScottyBot은 템플릿 기반 합성 언어-행동 데이터를 사용하여 사용자 발화에서 행동 시퀀스 예측을 위한 BART 모델[12]을 훈련합니다. 팀 SlugJARVIS는 ASR 오류를 처리하기 위해 LLM을 사용하여 행동 예측을 위한 시뮬레이션된 ASR 오류가 있는 사용자 발화를 생성합니다. 다른 예로는 다중 모달 비전 및 언어 데이터와 언어 기반 공지시 해결 데이터를 생성하는 것이 있습니다. 이러한 데이터 세트를 생성하는 것 외에도 팀은 오프라인 접근 방식을 사용하거나 온라인 사용자 상호 작용 데이터를 활용하여 이러한 데이터 세트를 만들고 개선하기 위한 전담 주석 시스템을 구축합니다. 5 SimBot 성능: 결과 및 분석 SocialBot 및 TaskBot 챌린지의 성공을 바탕으로 사용자의 명시적 평가 및 피드백을 사용하여 SimBot을 평가했습니다. 또한, 미션 성공률이라는 작업 지향적 측정이 도입되어 게임 미션 내에서 SimBot이 작업을 수행하는 데 얼마나 효과적인지 직접 평가할 수 있는 방법이 제공되었습니다. 나아가, 경쟁 중에 SimBot의 일반화 가능성을 평가하기 위해 새로운 보이지 않는 게임 미션이 도입되었습니다. 이 섹션에서는 결선 진출자, 모든 SimBot 및 기준 시스템 간의 비교를 포함하여 경쟁의 첫 해에 SimBot의 성과를 평가하기 위한 다양한 지표를 제공합니다.5. 만족도 평가 SimBot을 평가하는 주요 메커니즘은 명확한 사용자 만족도 평가를 수집하는 것입니다.각 상호 작용 후 Alexa 사용자는 &quot;로봇과의 상호 작용을 어떻게 평가하시겠습니까?&quot;라는 프롬프트에 따라 SimBot과의 상호 작용을 1~5점 척도로 평가하도록 요청받았습니다.SimBot 평가 프롬프트는 SocialBot 경쟁(&quot;이 SocialBot과 다시 대화하는 것에 대해 어떻게 생각하십니까?&quot;) 및 Taskbot 경쟁(&quot;이 TaskBot이 귀하를 돕는 데 얼마나 도움이 되었습니까?&quot;)에서 사용된 프롬프트와 다르다는 점에 유의하는 것이 중요하며, 따라서 평가를 다른 경쟁 간에 직접 비교해서는 안 됩니다. 그림 6에서 보듯이, 파이널리스트들은 경쟁에서 22주 동안 7일 평균 평점을 30%(3.0에서 3.9) 향상시켰습니다. 모든 팀의 누적 평균 평점도 3% 증가하여 경쟁 내내 3.6에서 3.7로 상승했습니다. 평균 평점 4.4.4.4.3.3.3.3.3.2.2.2.준결승을 위한 4개의 새로운 미션 도입 보이지 않는 5개의 미션 도입 2.7경쟁 주-7일 연속 평균 평점(전체 10개 팀) 7일 연속 평균 평점(결승 진출자)-7일 연속 평균 평점(기준선)-누적 평점(기준선 제외) 그림 6: 경쟁 기간 동안 모든 SimBot(파란색), 결승 진출자(녹색), 기준선을 제외한 모든 SimBot의 누적 평점 진행(주황색), 기준선(회색)에 대한 사용자 만족도의 7일 연속 평균 평점. 파선으로 된 녹색과 파란색 선은 데이터가 누락된 주를 나타냅니다. 5.2 작업 완료 지표 만족도 평가 외에도 13주차에 작업 지향 지표로 미션 성공률(MSR)이 도입되었습니다. 각 팀의 미션 성공률은 성공적인 미션 수를 해당 팀이 수행한 총 미션 수로 나누어 계산했습니다. 그림 7에서 볼 수 있듯이, 결선 진출자들은 8주간의 경쟁에서 7일 평균 MSR을 4%(49%에서 52%) 향상시켰습니다. 모든 팀의 누적 평균 MSR도 경쟁 기간 동안 8% 증가하여 41%에서 49%가 되었습니다. 경쟁 17주차에 SimBot이 이전에 본 적이 없는 5개의 새로운 게임 미션을 도입했습니다. 보이지 않는 미션을 성공적으로 완료하기 위해 SimBot은 새로운 동작을 완료하고 새로운 물체와 상호 작용해야 했습니다. 표 2는 보이는 미션과 보이지 않는 미션을 비교한 결과를 보여줍니다. 보이지 않는 미션에서 파이널리스트 팀의 MSR은 53%에서 55%로 2% 향상되었고, 10개 대학 팀은 모두 45%에서 47%로 2% 향상되었습니다. 기준 시스템은 보이지 않는 미션에서 45%에서 55%로 10% 향상되었습니다. 특히 모든 팀에서 고객 만족도(CSAT)와 MSR 간에 높은 상관관계가 관찰되었습니다. 준결승 동안 다음 사이에 0.92(피어슨 상관관계)의 상관관계가 있었습니다.임무 성공률 60% 55% 50% 45% 40% 35% 30%보이지 않은 임무 5개 도입 경쟁 주간-7일 연속 평균 MSR(전체 10개 팀) 7일 연속 평균 MSR(결승 진출자) 7일 연속 평균 MSR(기준선) 누적 MSR(기준선 제외) 그림 7: 모든 SimBot(파란색), 결승 진출자(녹색), 기준을 제외한 모든 SimBot의 MSR 진행 상황(주황색) 및 기준선(회색)에 대한 경쟁 기간 동안의 7일 연속 평균 MSR. 모든 10개 대학 팀의 CSAT 및 MSR은 사용자 만족도와 작업 완료 간에 강력한 관계가 있음을 강조합니다. MSR 보이는 미션 보이지 않는 미션 분산 모든 10개 팀 결승 진출 팀 기준 시스템 45% 47% 2% 53% 55% 2% 45% 55% 10% 표 2: 모든 10개 팀, 결승 진출 팀 및 기준 시스템의 보이는 미션과 보이지 않는 미션에 대한 MSR 비교.6 토론 및
--- CONCLUSION ---
s AI를 적용하여 매력적이고 유용한 대화형 도우미를 만드는 데 상당한 진전이 있었지만, 실제 세계를 탐색하고, 사물을 조작하고, 작업을 완료할 수 있는 구체화된 대화형 에이전트를 만드는 데는 여전히 상당한 과제가 남아 있습니다. SimBot Challenge를 통해 전 세계 대학 팀은 Alexa 사용자에게 제공된 시뮬레이션 환경에서 작동할 수 있는 효과적이고 사용 가능한 구체화된 대화형 AI 에이전트를 만들기 위해 경쟁할 수 있었습니다. 이것은 SimBot Challenge의 첫 번째 에디션이었고 멀티모달 장치에서 Alexa 사용자가 사용할 수 있는 구체화된 AI에서 경쟁을 개발하는 것은 매우 어려운 임무였습니다. 견고한 대화형 음성 대화 시스템을 만드는 인프라를 제공하는 것 외에도 Alexa Arena 시뮬레이션 환경을 설계 및 구축하고, 일상 사용자를 위한 매력적인 게임 미션을 개발하고, 로봇의 1인칭 시점을 캡처하고 컴퓨터 비전 모델을 적용하기 위한 지원을 보장해야 했습니다. 경쟁 과정에서 팀의 평가와 미션 완료율은 꾸준히 향상되었고 팀은 보이지 않는 사물과 작업으로 일반화되는 접근 방식을 만들 수 있었습니다. Alexa Prize 팀과 참여 대학 팀의 협력적 노력은 Alexa Prize SimBot Challenge를 확대하고, 체화된 대화형 AI의 발전을 이끌고, 미래에 Alexa 사용자의 가능성을 확장하기 위한 견고한 기반을 마련했습니다.감사의 말 대회에 참여한 모든 대학생과 그들의 자문위원(Alexa Prize SimBot 팀)에게 감사드리고 싶습니다.이 전체 프로그램을 통해 비전과 지원을 보여준 Alexa Natural Understanding(NU) 조직의 Amazon 리더십과 Alexa 원칙에 감사드립니다.Alexa Prize SimBot Challenge에 올바른 메시지와 트래픽을 유도하여 참여 팀이 연구에 대한 실제 피드백을 받도록 한 Marketing에 감사드립니다.Alexa Prize SimBot 기술을 활성화하기 위한 작업을 한 Alexa Engineering에 감사드립니다.대학 봇을 인증하기 위해 신속하게 작업한 많은 인증 요청에 대해 Alexa Developer Experience and Customer Trust(ADECT) Gadgets 팀에 감사드립니다. 또한, 최고의 사용자 경험을 구축하는 데 중요한 입력을 팀에 제공하여 고객 집착의 본보기를 보여준 NU-Customer Experience 팀에 감사드립니다. 가상으로 대학 팀을 방문하여 팀으로부터 배우고 디자인을 개선하도록 돕기 위해 시간을 내어준 리더들에게 감사드립니다. Speech, NLU, Data Services, Conversation Modeling 리더십을 포함한 모든 Alexa 조직의 지원 없이는 이 대회가 불가능했을 것입니다. 마지막으로, 새로운 Alexa Prize SimBot Challenge와 많은 상호 작용에 참여하고 올해 내내 팀의 개선에 도움이 되는 피드백을 제공한 Alexa 사용자에게 감사드립니다. 참고문헌 [1] S. Hu, Y. Liu, A. Gottardi, B. Hedayatnia, A. Khatri, A. Chadha, Q. Chen, P. Rajan, A. Binici, V. Somani 외, &quot;제4회 Alexa Prize Socialbot 그랜드 챌린지에서 오픈 도메인 대화 시스템의 추가 발전&quot;, 2021. [2] A. Gottardi, O. Ipek, G. Castellucci, S. Hu, L. Vaz, Y. Lu, A. Khatri, A. Chadha, D. Zhang, S. Sahai 외, &quot;Alexa, 함께 일해요: 대화형 작업 지원을 위한 첫 번째 Alexa Prize Taskbot 챌린지 소개&quot;, arXiv 사전 인쇄본 arXiv:2209.06321, 2022. [3] AL Zobrist, &quot;바둑 게임을 위한 시각적 구성 모델&quot;, 5월 14-16일 회의록, 1969, 봄 공동 컴퓨터 컨퍼런스, pp. 103–112, 1969. [4] Q. Gao, G. Thattai, X. Gao, S. Shakiah, S. Pansare, V. Sharma, G. Sukhatme, H. Shi, B. Yang, D. Zheng, et al., “Alexa arena: A user-centric interactive platform for embodied ai,&quot; arXiv preprint arXiv:2303.01586, 2023. [5] A. Padmakumar, J. Thomason, A. Shrivastava, P. Lange, A. Narayan-Chen, S. Gella, R. Piramuthu, G. Tur, and D. Hakkani-Tur, “Teach: Task-driven embodied agents that chat,&quot; in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 2017–2025, 2022. [6] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta 및 A. Farhadi, &quot;Ai2-thor: 시각적 AI를 위한 대화형 3D 환경&quot;, arXiv 사전 인쇄 arXiv:1712.05474, 2017. [7] A. Pashevich, C. Schmid 및 C. Sun, 컴퓨터 비전 국제 회의(ICCV)에서 &quot;비전 및 언어 탐색을 위한 에피소드 변환기&quot;, 2021. [8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin, “주의. 영어: 이것은 필요한 전부입니다.&quot; 신경 정보 처리 시스템의 발전, 제30권, 2017년. [9] B. Cheng, A. Schwing 및 A. Kirillov, &quot;픽셀당 분류는 의미론적 분할에 필요한 전부가 아닙니다.&quot; 신경 정보 처리 시스템의 발전, 제30권, 2017년. 34, pp. 17864–17875, 2021. [10] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao, “Vinvl: Revisiting visual representations in vision-language models,”&quot; 2021. [11] DA Hudson and CD Manning, “Gqa: A new dataset for real-world visual reasoning and compositional question answering,” in the Proceedings of the IEEE/CVF conference on computer vision and pattern awareness, pp. 6700-6709, 2019. [12] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence 자연어 생성, 번역 및 이해를 위한 사전 학습,” arXiv 사전 인쇄본 arXiv:1910.13461, 2019.
