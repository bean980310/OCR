--- ABSTRACT ---
CLIP과 같은 이미지-텍스트 대조 모델은 제로 샷 분류, 이미지-텍스트 검색 및 전이 학습에 광범위하게 적용됩니다. 그러나 이러한 모델은 종종 구성적 시각 언어 작업(예: 속성 바인딩 또는 객체 관계)에서 어려움을 겪으며, 이러한 작업의 성능은 무작위적 우연보다 나을 것이 없습니다. 이를 해결하기 위해 가볍고 샘플 효율적인 증류 방법인 SDS-CLIP을 도입하여 CLIP의 구성적 시각 언어 추론을 향상시킵니다. 저희의 접근 방식은 강력한 시각 언어 추론 능력으로 알려진 StableDiffusion과 같은 대규모 텍스트-이미지 생성 모델에서 빌린 증류 목표를 사용하여 CLIP을 미세 조정합니다. 까다로운 Winoground 벤치마크에서 SDS-CLIP은 다양한 CLIP 모델의 시각 언어 성능을 최대 7%까지 개선하는 반면, ARO 데이터 세트에서는 성능을 최대 3%까지 높입니다. 이 연구는 생성 모델에서 잘 설계된 증류 목표의 잠재력을 강조하여 향상된 시각 언어 추론 기능을 통해 대조 이미지 텍스트 모델을 향상시킵니다. 1
--- INTRODUCTION ---
최근 몇 년 동안 CLIP(Radford et al., 2021a)과 같은 멀티모달 모델은 제로샷 분류, 이미지-텍스트 검색 및 이미지 캡션(Mu et al., 2021; Yu et al., 2022; Li et al., 2022; Mokady et al., 2021)과 같은 작업에서 탁월한 성과를 거두었습니다. 이러한 모델은 또한 분할 및 객체 감지와 같은 작업을 위한 다양한 최첨단 파이프라인에서 중요한 구성 요소입니다(Wang et al., 2021; Lüddecke and Ecker, 2021; Minderer et al., 2022; Zhong et al., 2021). 그러나 이미지에서 객체 간의 공간적 관계를 결정하는 것과 같은 시각 언어 추론 작업에는 어려움을 겪습니다(Yuksekgonul et al., 2023; Huang et al., 2023). 특히, 시각 언어 추론을 평가하도록 설계된 벤치마크인 까다로운 Winoground(Thrush et al., 2022; Diwan et al., 2022)에서 CLIP의 성과는 무작위적 우연에 가깝습니다. 이러한 단점은 검색을 위한 단축키를 우선시하는 CLIP의 대조적 목적에 기인하며, 따라서 세분화된 객체 세부 정보와 해당 위치를 이해하는 능력에 영향을 미칩니다(Diwan et al., 2022; Thrush et al., 2022). 반면, Stable Diffusion(Rombach et al., 2021)과 같은 텍스트-이미지 모델은 시각 언어 작업에서 탁월한 성과를 보이는데, 이는 텍스트 컨디셔닝이 교차 주의 맵에서 의미적 일관성을 향상시키기 때문일 가능성이 높습니다(Li et al., 2023; Clark and Jaini, 2023). Li et al. (2023)은 최근 Winoground 벤치마크에서 이를 입증했으며, 잡음 제거 확산 점수를 사용하여 미세한 공간적 차이가 있는 이미지에 캡션을 안정적으로 일치시켰습니다(그림 1 참조). Imagen(Clark 및 Jaini, 2023)을 포함한 다른 텍스트-이미지 모델에서도 유사한 결과가 나타났으며, 이러한 방법의 거의 대부분이 동일한 작업에서 CLIP 변형보다 성능이 우수했습니다. 이러한 연구에서는 시각 언어 작업에 생성 텍스트-이미지 모델을 사용할 수 있는 잠재력을 보여주었지만 여전히 계산 집약적입니다. 예를 들어, 이미지-텍스트 매칭을 위한 잡음 제거 확산 점수를 계산하려면 다양한 잡음 수준과 시간 단계로 UNet 모델(약 892M 매개변수)을 여러 번 통과해야 합니다. 엔트리 레벨 GPU에서는 단일 이미지-텍스트 매칭 작업에 최대 1분이 걸릴 수 있으므로 실제 및 실시간 애플리케이션에는 비실용적입니다. 반면, CLIP 모델은 이미지를 최대 18배 더 빠르게 분류할 수 있으며(그림 1 참조), 이미지와 텍스트 인코더를 한 번만 통과하면 됩니다. 따라서 유망한 연구 방향은 텍스트-이미지 모델의 강력한 시각 언어적 역량과 CLIP의 빠른 추론을 결합하는 방법을 찾는 것입니다. 이를 위해, 우리는 CLIP을 위한 가볍고 샘플 효율적인 미세 조정 방식인 SDS-CLIP을 소개합니다. 이 방식은 안정적 확산에서 지식을 추출하고 CLIP의 시각 추론 역량을 향상시킵니다. 구체적으로, 우리는 CLIP의 표준 대조 손실 기반 시간(로그 분) 6.0 확산 점수 5.55.4.4.3.5 VIT-L-336 VIT-L/RN-VIT-B/VIT-B/3.0 X RN-50(Gelu) 0.0.0에 정규화 항목을 추가합니다.그림 1: 0. Winoground 점수 캡션 c*는 캡션 C = 집합에서 추출합니다. 여기서 모든 캡션은 동일한 단어를 포함하지만 각각은 객체의 다른 공간적 배열을 설명하며 올바른 것은 하나만 있습니다. 이 논문과 동시에 진행된 작업(Clark 및 Jaini, 2023; Li et al., 2023; Krojer et al., 2023)은 텍스트-이미지 생성 모델의 노이즈 제거 확산 점수를 사용하여 이러한 이미지 매칭 작업을 수행할 수 있음을 보여주었습니다. 이는 다음과 같이 공식화할 수 있습니다. 이미지 x와 캡션 c에 대해 d(x, c)로 표시되는 노이즈 제거 확산 점수는 0.0으로 정의됩니다. CLIP 변형은 Stable Diffusion의 Diffusion Score와 비교했을 때 시각 언어 추론 벤치마크인 Winoground에서 성능이 떨어집니다. 확산 점수는 Stable Diffusion의 손실 함수에서 계산됩니다. 확산 점수는 추론(확산 점수 계산 중 샘플링 사용)에 CLIP 변형보다 18배 더 많은 시간이 걸립니다. 점수 증류 샘플링(SDS)에서(Poole et al., 2022). 이 정규화는 CLIP의 임베딩이 텍스트-이미지 모델의 노이즈 제거 확산 손실에 맞춰지도록 합니다. 이 정규화된 목적으로 CLIP을 MS-COCO의 118k 이미지 텍스트 쌍인 작은 쌍의 이미지-텍스트 데이터 세트에서 미세 조정하여 Winoground와 ARO라는 두 가지 매우 어려운 시각-언어적 추론 벤치마크에서 바닐라 CLIP에 비해 1.57%의 성능 향상을 보였습니다. 주목할 점은 CLIP의 LayerNorm 매개변수만 업데이트하여 달성했다는 것입니다. 나아가 SDSCLIP의 제로샷 성능이 광범위한 다운스트림 데이터 세트에 영향을 미치지 않는다는 것을 보여줍니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 텍스트-이미지 모델의 증류 기반 정규화 항을 통합하는 새로운 샘플 효율적이고 매개변수 효율적인 미세 조정 방법인 SDS-CLIP을 소개합니다. • 어려운 벤치마크에서 접근 방식을 경험적으로 검증하고 제로샷 기능에 해를 끼치지 않고 CLIP의 시각-언어적 추론이 개선되었음을 보여줍니다. 2 시각-언어적 추론을 위한 잡음 제거 확산 점수 Winoground 벤치마크는 모델의 시각-언어적 추론 능력을 측정하기 위한 까다로운 이미지-텍스트 매칭 과제를 수립합니다.이미지 x가 주어졌을 때, 모델은 올바른 d(x, c) = Et~T,e~N(0,1)|[||€0(va(x), t, c) — e||²] (1)와 매칭해야 합니다.이 잡음 제거 확산 점수는 다음과 같이 C에서 올바른 캡션 c*를 선택하는 데 사용할 수 있습니다.C* = arg min Et~T,ε~N(0,1)|||€0 (va(x), t, c)—e||²] CEC (2) 여기서 t는 샘플링된 시간 단계, ε는 잡음 예측 UNet, va는 이미지 x를 잠재 코드에 매핑하는 인코더(예: VQ-VAE)이며 €는 샘플링된 가우시안 잡음입니다. 이전 연구(Krojer et al., 2023)에서는 이 접근 방식을 채택함으로써 텍스트-이미지 모델이 Winoground와 같은 시각 언어 추론 벤치마크에서 강력한 성능을 발휘하여 CLIP과 같은 대조 모델보다 상당한 차이로 성능이 우수함을 보였습니다(그림 1 참조). ARO의 경우 확산 점수로 0.63의 정확도를 얻었으며 이는 CLIP 모델보다 우수합니다. 3 SDS-CLIP: 방법 접근 방식의 핵심 아이디어는 Stable Diffusion의 잡음 제거 확산 점수로 CLIP의 대조 목적을 정규화하는 것입니다(식(1) 참조). 이 방법은 3D NeRF 모델의 출력을 Stable Diffusion의 UNet 입력 공간에 매핑하고 잡음 제거 확산 손실(스코어 증류 샘플링(SDS)이라고도 함)로 매개변수를 최적화하는 (Poole et al., 2022)의 최근 연구를 기반으로 합니다. 마찬가지로 SDS를 사용하여 CLIP의 매개변수를 미세 조정합니다. 직관적으로, 우리의 설정은 교사가 텍스트-이미지 모델이고 학생이 CLIP인 지식 증류의 한 형태로 볼 수 있습니다. 결과적으로 추론에서 CLIP은 텍스트-이미지 확산 모델에서 이미 학습한 시각 언어 추론 기능의 이점을 얻을 수 있습니다. 형식적으로, 우리는 CLIP의 이미지 인코더 출력을 Stable Diffusion의 UNet의 입력 공간에 매핑합니다. 구체적으로, 우리는 주어진 이미지 x를 모델 VIT-B/16(CLIP) Wino-Overall 객체 관계 둘 다 1개의 주요 예측 2개의 주요 예측 ARO-Overall ARO-Relation ARO-Attribution 0.0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.ViT-B/32(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.ViT-L/14(CLIP) 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.VIT-L/14-336(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP 0.0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.ResNet-50(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.여기서 O, Y, w는 CLIP의 이미지 인코더, 텍스트 인코더 및 CLIP과 사이의 선형 맵의 학습 가능한 매개변수입니다. Stable Diffusion의 UNet. 표 1: 우리의 미세 조정 방법 SDS-CLIP은 Winoground 벤치마크에서 CLIP 성능을 1.5%~7%까지 개선하고 다양한 CLIP 변형에서 ARO-Relation 및 Attribution 작업의 경우 최대 3%까지 개선합니다. 구체적으로, 우리의 방법은 Winoground의 대부분 작업을 구성하는 객체 스왑 및 관계 이해와 관련된 하위 범주에서 개선된다는 것을 발견했습니다. 증류 손실 없이 MSCOCO의 이미지-텍스트 쌍으로만 미세 조정하는 것은 개선으로 이어지지 않습니다. OpenCLIP 결과는 부록 I. CLIP의 이미지 인코더 fo 및 매핑<CLS> 선형 맵 hw Є Rdx4×64x를 통해 Stable Diffusion의 UNet Єg의 입력 공간에 임베딩합니다.이것은 ε0 (hw(fø(x)), t, c)로 공식화할 수 있습니다.여기서 t는 시간 단계이고 c는 주어진 이미지에 대한 해당 텍스트 캡션입니다.그런 다음 Eq. (2)에서 ε0 (va(x), t, c) 대신 이 항을 사용하여 확산 손실의 피드백으로 이미지-텍스트 바인딩을 촉진하는 잡음 제거 확산 손실 LSDS로 도출합니다.LSDS = Et~T,E~N(0,1) [|| 0 (hw(f(x)), t, c) — e||² (3) 우리는 이 LSDS 손실을 CLIP의 원래 대조 목적에 추가하여 정규화 도구로 작용하도록 실제적으로 구현합니다. Ltotal = LCLIP+\Lsds (4) 여기서 LCLIP는 부록 C.1에 정의되어 있으며 X는 그리드 검색으로 설정할 수 있는 하이퍼 매개변수입니다. 확산 손실을 CLIP 목적에 통합하는 데는 여러 가지 방법이 있다는 점에 유의합니다. 추가 손실 항이 최상의 결과를 가져온다는 것을 알았지만, 부록에서 고려한 전체 설계 선택 사항을 포함했습니다. 주어진 함수가 이미지 생성 프로세스를 통한 역전파로 최적화된 미분 가능 이미지 매개변수화(Mordvintsev et al., 2018)와 유사하게 UNet 매개변수는 최적화 프로세스 동안 고정된 상태로 유지됩니다. 구체적으로, Ltotal (O, Y, w, 0)이 주어지면: *, √*, w* = min Ltotal (O, Y, w, 0) φ,γ,ω (5) 4가지 실험 이 섹션에서는 제안하는 방법인 SDS-CLIP을 두 가지 유형의 작업에서 경험적으로 검증합니다.i) 두 가지 까다로운 벤치마크(Winoground, ARO)를 사용한 시각 언어 추론 및 ii) 다운스트림 데이터 세트(ImageNet, CIFAR-100 및 기타)를 사용한 제로 샷 이미지 분류.전반적으로, 우리 방법이 Winoground와 ARO의 몇 가지 핵심 작업에서 CLIP의 성능을 상당히 향상시키는 동시에 다운스트림 제로 샷 분류 성능도 약간 향상시킨다는 것을 보여줍니다.4.실험 설정 CLIP 모델.우리는 실험에서 다음과 같은 CLIP 변형을 고려합니다.(i) CLIP ViT-B/16;(ii) CLIP VIT-B/32;(iii) CLIP-ViT-L-14; (iv) CLIPViT-L-14 336px; (v) CLIP-ResNet-50. 구현 세부 정보. 계산 한계로 인해 처음부터 학습하는 대신 공개적으로 사용 가능한 체크포인트에서 CLIP을 미세 조정합니다. 특히, 선형 변환 hw와 함께 CLIP의 LayerNorm 매개변수만 미세 조정합니다(Basu et al., 2023). 약 8M개의 학습 가능 매개변수만 고려합니다. 이러한 매개변수는 MSCOCO의 이미지-텍스트 쌍을 사용하여 미세 조정합니다(Lin et al., 2014). 특히 MSCOCO를 선택하는 이유는 CC12M(Sharma et al., 2018)과 같은 다른 이미지-텍스트 데이터 세트보다 비교적 작고 노이즈가 적기 때문입니다. 이 두 가지 요소로 인해 우리의 미세 조정 방법은 매우 sampleImageNet STL-CIFAR-MNIST Zero-Shot 결과 CIFAR-Food(a) ViT-B/VIT-B/16(CLIP) Ours PETS Flowers DTD ImageNet Aircraft STL-Zero-Shot 결과 CIFAR-CIFAR-MNIST Food(b) ViT-B/Zero-Shot 결과 VIT-B/32(CLIP) CIFAR-Ours PETS CIFAR-Flowers DTD ImageNet STL-Aircraft VIT-L/14(CLIP) Ours PETS MNIST Flowers Food(၁) ViT-L/DTD Aircraft 그림 2: 우리의 미세 조정 방법은 CLIP의 제로 샷 기능에 해를 끼치지 않습니다. 실제로 특정 다운스트림 데이터 세트(예: ImageNet, CIFAR-10, MNIST, Aircraft)의 경우 ViT-B/16의 제로 샷 성능이 1%-8% 향상되는 것을 관찰했습니다. 다른 CLIP 모델의 경우, 제로샷 성능 저하가 발견되지 않았습니다. 매개변수 효율적입니다. 기준선. 두 가지 다른 기준선과 방법을 비교합니다. (i) 사전 학습된(바닐라) CLIP 체크포인트; (ii) 정규화 항 없이 표준 대조 손실이 있는 MSCOCO에서 미세 조정된 CLIP. 4.2 결과 Winoground. 까다로운 시각 언어 추론 벤치마크인 Winoground(Thrush et al., 2022)에서 SDS-CLIP을 평가합니다. 표(1)에서 접근 방식이 모든 Winoground 하위 범주와 CLIP 변형에서 성능을 지속적으로 개선하여 1.5%~7% 범위의 절대적 개선을 보이는 것을 알 수 있습니다. 7%의 가장 큰 이득은 ViT-B/16(CLIP)에서 관찰되었으며 다른 CLIP 변형은 1.5%~2%의 일관된 개선을 보였습니다. 부록(표 2)에서 유사한 개선이 관찰된 공개 데이터에서 사전 학습된 CLIP 변형에 대한 결과를 제공합니다. Winoground 하위 범주를 자세히 살펴보면 SDS-CLIP이 &quot;객체 교환&quot;과 &quot;관계&quot;에서 일관된 개선을 보인다는 것을 알 수 있습니다. &quot;객체 교환&quot;과 &quot;관계&quot; 태그를 모두 결합한 &quot;둘 다&quot; 하위 범주는 모든 작업의 5%에 불과하므로 객체 교환과 관계적 이해가 모두 포함된 모든 시나리오를 완전히 대표하지 못할 수 있습니다. 또한 캡션의 술어 수에 대한 SDS-CLIP의 견고성을 분석하여 전반적으로 술어가 하나와 두 개 모두 있는 작업에서 성능이 향상됨을 발견했습니다. ARO. ARO 데이터 세트(Yuksekgonul et al., 2023)는 (i) 속성 이해 및 (ii) 관계적 이해에 대한 작업으로 구성됩니다. 표 1에서 SDS-CLIP은 &quot;속성 바인딩&quot; 및 &quot;관계적 이해&quot; 작업에서 성능을 1%-3% 향상시킵니다. CLIP의 제로샷 성능에 미치는 영향. 그림 2에서 SDS-CLIP의 제로샷 분류 기능은 vanilla CLIP에 비해 영향을 받지 않는다는 것을 알 수 있습니다. 사실, ViT-B/16의 제로샷 성능이 다양한 다운스트림 데이터 세트에서 개선된다는 것을 알 수 있습니다(MNIST의 경우 최대 8% 개선). Stable-Diffusion은 CLIP보다 훨씬 더 큰 이미지-텍스트 쌍 세트에서 사전 학습되었지만, 부록 K에서 LAION-2B에서 사전 학습된 CLIP 변형은 여전히 Winoground에서 어려움을 겪는다는 것을 보여줍니다. 사실, SDS-CLIP을 사용하면 이러한 CLIP 변형의 구성적 추론을 개선할 수 있음을 보여줍니다. 부록 H에서 더 큰 CC-3M에서 미세 조정을 사용한 결과를 보여줍니다(Sharma et al., 2018). 5
--- RELATED WORK ---
s CLIP 모델(Radford 등, 2021a)은 강력한 제로샷 분류로 유명하지만, 최근 연구(Thrush 등, 2022; Diwan 등, 2022)에서는 시각 언어 추론의 한계가 드러났습니다. 반면, 최근 연구에서는 텍스트-이미지 모델(Clark 및 Jaini, 2023; Li 등, 2023; Krojer 등, 2023; Chen 등, 2023)이 추론 작업에서 CLIP보다 성능이 우수하다는 것이 입증되었습니다. 실제로 이러한 모델은 확산 목표에서 계산된 점수를 활용합니다. (Poole 등, 2022)이 텍스트에서 3D 생성을 위해 점수 증류 샘플링을 사용하는 반면, 우리의 연구는 공식을 정규화기로 적용하고 CLIP에서 구성 능력을 개선한 최초의 연구라는 점에 유의합니다. 6 결론 본 논문에서는 새로운 데이터 및 매개변수 효율적인 SDS-CLIP을 소개합니다.
--- METHOD ---
CLIP의 구성적 시각 언어 추론을 강화합니다. 저희의 접근 방식은 강력한 시각 언어 추론 능력으로 알려진 StableDiffusion과 같은 대규모 텍스트-이미지 생성 모델에서 빌린 증류 목표를 사용하여 CLIP을 미세 조정합니다. 까다로운 Winoground 벤치마크에서 SDS-CLIP은 다양한 CLIP 모델의 시각 언어 성능을 최대 7%까지 개선하는 반면, ARO 데이터 세트에서는 성능을 최대 3%까지 높입니다. 이 작업은 생성 모델에서 잘 설계된 증류 목표가 향상된 시각 언어 추론 기능으로 대조적 이미지 텍스트 모델을 강화할 수 있는 잠재력을 강조합니다. 1 서론 최근 몇 년 동안 CLIP(Radford et al., 2021a)과 같은 멀티모달 모델은 제로샷 분류, 이미지-텍스트 검색 및 이미지 캡션(Mu et al., 2021; Yu et al., 2022; Li et al., 2022; Mokady et al., 2021)과 같은 작업에서 탁월한 성과를 거두었습니다. 이러한 모델은 또한 분할 및 객체 감지와 같은 작업을 위한 다양한 최첨단 파이프라인에서 중요한 구성 요소입니다(Wang et al., 2021; Lüddecke and Ecker, 2021; Minderer et al., 2022; Zhong et al., 2021). 그러나 이미지에서 객체 간의 공간적 관계를 결정하는 것과 같은 시각 언어 추론 작업에는 어려움을 겪습니다(Yuksekgonul et al., 2023; Huang et al., 2023). 특히, 시각 언어 추론을 평가하도록 설계된 벤치마크인 까다로운 Winoground(Thrush et al., 2022; Diwan et al., 2022)에서 CLIP의 성과는 무작위적 우연에 가깝습니다. 이러한 단점은 검색을 위한 단축키를 우선시하는 CLIP의 대조적 목적에 기인하며, 따라서 세분화된 객체 세부 정보와 해당 위치를 이해하는 능력에 영향을 미칩니다(Diwan et al., 2022; Thrush et al., 2022). 반면, Stable Diffusion(Rombach et al., 2021)과 같은 텍스트-이미지 모델은 시각 언어 작업에서 탁월한 성과를 보이는데, 이는 텍스트 컨디셔닝이 교차 주의 맵에서 의미적 일관성을 향상시키기 때문일 가능성이 높습니다(Li et al., 2023; Clark and Jaini, 2023). Li et al. (2023)은 최근 Winoground 벤치마크에서 이를 입증했으며, 잡음 제거 확산 점수를 사용하여 미세한 공간적 차이가 있는 이미지에 캡션을 안정적으로 일치시켰습니다(그림 1 참조). Imagen(Clark 및 Jaini, 2023)을 포함한 다른 텍스트-이미지 모델에서도 유사한 결과가 나타났으며, 이러한 방법의 거의 대부분이 동일한 작업에서 CLIP 변형보다 성능이 우수했습니다. 이러한 연구에서는 시각 언어 작업에 생성 텍스트-이미지 모델을 사용할 수 있는 잠재력을 보여주었지만 여전히 계산 집약적입니다. 예를 들어, 이미지-텍스트 매칭을 위한 잡음 제거 확산 점수를 계산하려면 다양한 잡음 수준과 시간 단계로 UNet 모델(약 892M 매개변수)을 여러 번 통과해야 합니다. 엔트리 레벨 GPU에서는 단일 이미지-텍스트 매칭 작업에 최대 1분이 걸릴 수 있으므로 실제 및 실시간 애플리케이션에는 비실용적입니다. 반면, CLIP 모델은 이미지를 최대 18배 더 빠르게 분류할 수 있으며(그림 1 참조), 이미지와 텍스트 인코더를 한 번만 통과하면 됩니다. 따라서 유망한 연구 방향은 텍스트-이미지 모델의 강력한 시각 언어적 역량과 CLIP의 빠른 추론을 결합하는 방법을 찾는 것입니다. 이를 위해, 우리는 CLIP을 위한 가볍고 샘플 효율적인 미세 조정 방식인 SDS-CLIP을 소개합니다. 이 방식은 안정적 확산에서 지식을 추출하고 CLIP의 시각 추론 역량을 향상시킵니다. 구체적으로, 우리는 CLIP의 표준 대조 손실 기반 시간(로그 분) 6.0 확산 점수 5.55.4.4.3.5 VIT-L-336 VIT-L/RN-VIT-B/VIT-B/3.0 X RN-50(Gelu) 0.0.0에 정규화 항목을 추가합니다.그림 1: 0. Winoground 점수 캡션 c*는 캡션 C = 집합에서 추출합니다. 여기서 모든 캡션은 동일한 단어를 포함하지만 각각은 객체의 다른 공간적 배열을 설명하며 올바른 것은 하나만 있습니다. 이 논문과 동시에 진행된 작업(Clark 및 Jaini, 2023; Li et al., 2023; Krojer et al., 2023)은 텍스트-이미지 생성 모델의 노이즈 제거 확산 점수를 사용하여 이러한 이미지 매칭 작업을 수행할 수 있음을 보여주었습니다. 이는 다음과 같이 공식화할 수 있습니다. 이미지 x와 캡션 c에 대해 d(x, c)로 표시되는 노이즈 제거 확산 점수는 0.0으로 정의됩니다. CLIP 변형은 Stable Diffusion의 Diffusion Score와 비교했을 때 시각 언어 추론 벤치마크인 Winoground에서 성능이 떨어집니다. 확산 점수는 Stable Diffusion의 손실 함수에서 계산됩니다. 확산 점수는 추론(확산 점수 계산 중 샘플링 사용)에 CLIP 변형보다 18배 더 많은 시간이 걸립니다. 점수 증류 샘플링(SDS)에서(Poole et al., 2022). 이 정규화는 CLIP의 임베딩이 텍스트-이미지 모델의 노이즈 제거 확산 손실에 맞춰지도록 합니다. 이 정규화된 목적으로 CLIP을 MS-COCO의 118k 이미지 텍스트 쌍인 작은 쌍의 이미지-텍스트 데이터 세트에서 미세 조정하여 Winoground와 ARO라는 두 가지 매우 어려운 시각-언어적 추론 벤치마크에서 바닐라 CLIP에 비해 1.57%의 성능 향상을 보였습니다. 주목할 점은 CLIP의 LayerNorm 매개변수만 업데이트하여 달성했다는 것입니다. 나아가 SDSCLIP의 제로샷 성능이 광범위한 다운스트림 데이터 세트에 영향을 미치지 않는다는 것을 보여줍니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 텍스트-이미지 모델의 증류 기반 정규화 항을 통합하는 새로운 샘플 효율적이고 매개변수 효율적인 미세 조정 방법인 SDS-CLIP을 소개합니다. • 어려운 벤치마크에서 접근 방식을 경험적으로 검증하고 제로샷 기능에 해를 끼치지 않고 CLIP의 시각-언어적 추론이 개선되었음을 보여줍니다. 2 시각-언어적 추론을 위한 잡음 제거 확산 점수 Winoground 벤치마크는 모델의 시각-언어적 추론 능력을 측정하기 위한 까다로운 이미지-텍스트 매칭 과제를 수립합니다.이미지 x가 주어졌을 때, 모델은 올바른 d(x, c) = Et~T,e~N(0,1)|[||€0(va(x), t, c) — e||²] (1)와 매칭해야 합니다.이 잡음 제거 확산 점수는 다음과 같이 C에서 올바른 캡션 c*를 선택하는 데 사용할 수 있습니다.C* = arg min Et~T,ε~N(0,1)|||€0 (va(x), t, c)—e||²] CEC (2) 여기서 t는 샘플링된 시간 단계, ε는 잡음 예측 UNet, va는 이미지 x를 잠재 코드에 매핑하는 인코더(예: VQ-VAE)이며 €는 샘플링된 가우시안 잡음입니다. 이전 연구(Krojer et al., 2023)에서는 이 접근 방식을 채택함으로써 텍스트-이미지 모델이 Winoground와 같은 시각 언어 추론 벤치마크에서 강력한 성능을 발휘하여 CLIP과 같은 대조 모델보다 상당한 차이로 성능이 우수함을 보였습니다(그림 1 참조). ARO의 경우 확산 점수로 0.63의 정확도를 얻었으며 이는 CLIP 모델보다 우수합니다. 3 SDS-CLIP: 방법 접근 방식의 핵심 아이디어는 Stable Diffusion의 잡음 제거 확산 점수로 CLIP의 대조 목적을 정규화하는 것입니다(식(1) 참조). 이 방법은 3D NeRF 모델의 출력을 Stable Diffusion의 UNet 입력 공간에 매핑하고 잡음 제거 확산 손실(스코어 증류 샘플링(SDS)이라고도 함)로 매개변수를 최적화하는 (Poole et al., 2022)의 최근 연구를 기반으로 합니다. 마찬가지로 SDS를 사용하여 CLIP의 매개변수를 미세 조정합니다. 직관적으로, 우리의 설정은 교사가 텍스트-이미지 모델이고 학생이 CLIP인 지식 증류의 한 형태로 볼 수 있습니다. 결과적으로 추론에서 CLIP은 텍스트-이미지 확산 모델에서 이미 학습한 시각 언어 추론 기능의 이점을 얻을 수 있습니다. 형식적으로, 우리는 CLIP의 이미지 인코더 출력을 Stable Diffusion의 UNet의 입력 공간에 매핑합니다. 구체적으로, 우리는 주어진 이미지 x를 모델 VIT-B/16(CLIP) Wino-Overall 객체 관계 둘 다 1개의 주요 예측 2개의 주요 예측 ARO-Overall ARO-Relation ARO-Attribution 0.0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.ViT-B/32(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.ViT-L/14(CLIP) 0.0.0.0.0.0.0.0.0.0.FT와 LCLIP 0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.VIT-L/14-336(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP 0.0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.ResNet-50(CLIP) 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP 0.0.0.0.0.0.0.0.0.0.0.FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.여기서 O, Y, w는 CLIP의 이미지 인코더, 텍스트 인코더 및 CLIP과 사이의 선형 맵의 학습 가능한 매개변수입니다. Stable Diffusion의 UNet. 표 1: 우리의 미세 조정 방법 SDS-CLIP은 Winoground 벤치마크에서 CLIP 성능을 1.5%~7%까지 개선하고 다양한 CLIP 변형에서 ARO-Relation 및 Attribution 작업의 경우 최대 3%까지 개선합니다. 구체적으로, 우리의 방법은 Winoground의 대부분 작업을 구성하는 객체 스왑 및 관계 이해와 관련된 하위 범주에서 개선된다는 것을 발견했습니다. 증류 손실 없이 MSCOCO의 이미지-텍스트 쌍으로만 미세 조정하는 것은 개선으로 이어지지 않습니다. OpenCLIP 결과는 부록 I. CLIP의 이미지 인코더 fo 및 매핑<CLS> 선형 맵 hw Є Rdx4×64x를 통해 Stable Diffusion의 UNet Єg의 입력 공간에 임베딩합니다.이것은 ε0 (hw(fø(x)), t, c)로 공식화할 수 있습니다.여기서 t는 시간 단계이고 c는 주어진 이미지에 대한 해당 텍스트 캡션입니다.그런 다음 Eq. (2)에서 ε0 (va(x), t, c) 대신 이 항을 사용하여 확산 손실의 피드백으로 이미지-텍스트 바인딩을 촉진하는 잡음 제거 확산 손실 LSDS로 도출합니다.LSDS = Et~T,E~N(0,1) [|| 0 (hw(f(x)), t, c) — e||² (3) 우리는 이 LSDS 손실을 CLIP의 원래 대조 목적에 추가하여 정규화 도구로 작용하도록 실제적으로 구현합니다. Ltotal = LCLIP+\Lsds (4) 여기서 LCLIP는 부록 C.1에 정의되어 있으며 X는 그리드 검색으로 설정할 수 있는 하이퍼 매개변수입니다. 확산 손실을 CLIP 목적에 통합하는 데는 여러 가지 방법이 있다는 점에 유의합니다. 추가 손실 항이 최상의 결과를 가져온다는 것을 알았지만, 부록에서 고려한 전체 설계 선택 사항을 포함했습니다. 주어진 함수가 이미지 생성 프로세스를 통한 역전파로 최적화된 미분 가능 이미지 매개변수화(Mordvintsev et al., 2018)와 유사하게 UNet 매개변수는 최적화 프로세스 동안 고정된 상태로 유지됩니다. 구체적으로, Ltotal (O, Y, w, 0)이 주어지면: *, √*, w* = min Ltotal (O, Y, w, 0) ψ,γ,Ω (5) 4
--- EXPERIMENT ---
s 이 섹션에서는 제안하는 방법인 SDS-CLIP을 두 가지 유형의 작업에 대해 경험적으로 검증합니다. i) 두 가지 까다로운 벤치마크(Winoground, ARO)를 사용한 시각 언어 추론 및 ii) 다운스트림 데이터 세트(ImageNet, CIFAR-100 및 기타)를 사용한 제로샷 이미지 분류. 전반적으로, 우리 방법이 Winoground와 ARO의 몇 가지 핵심 작업에서 CLIP의 성능을 상당히 개선하는 동시에 다운스트림 제로샷 분류 성능도 약간 개선한다는 것을 보여줍니다. 4. 실험 설정 CLIP 모델. 실험에서 다음 CLIP 변형을 고려합니다. (i) CLIP ViT-B/16; (ii) CLIP VIT-B/32; (iii) CLIP-ViT-L-14; (iv) CLIPViT-L-14 336px; (v) CLIP-ResNet-50. 구현 세부 정보. 계산 한계로 인해, 처음부터 학습하는 대신 공개적으로 사용 가능한 체크포인트에서 CLIP을 미세 조정합니다. 특히, 선형 변환 hw와 함께 CLIP의 LayerNorm 매개변수만 미세 조정합니다(Basu et al., 2023). 즉, 약 8M개의 학습 가능 매개변수만 고려합니다. 이러한 매개변수는 MSCOCO의 이미지-텍스트 쌍을 사용하여 미세 조정합니다(Lin et al., 2014). 특히, CC12M(Sharma et al., 2018)과 같은 다른 이미지-텍스트 데이터 세트보다 비교적 작고 노이즈가 적기 때문에 MSCOCO를 선택합니다. 이 두 가지 요소로 인해 우리의 미세 조정 방법은 매우 sampleImageNet STL-CIFAR-MNIST Zero-Shot 결과 CIFAR-Food(a) ViT-B/VIT-B/16(CLIP) Ours PETS Flowers DTD ImageNet Aircraft STL-Zero-Shot 결과 CIFAR-CIFAR-MNIST Food(b) ViT-B/Zero-Shot 결과 VIT-B/32(CLIP) CIFAR-Ours PETS CIFAR-Flowers DTD ImageNet STL-Aircraft VIT-L/14(CLIP) Ours PETS MNIST Flowers Food(၁) ViT-L/DTD Aircraft 그림 2: 우리의 미세 조정 방법은 CLIP의 제로 샷 기능에 해를 끼치지 않습니다. 실제로 특정 다운스트림 데이터 세트(예: ImageNet, CIFAR-10, MNIST, Aircraft)의 경우 ViT-B/16의 제로 샷 성능이 1%-8% 향상되는 것을 관찰했습니다. 다른 CLIP 모델의 경우, 제로샷 성능 저하가 발견되지 않았습니다. 매개변수 효율적입니다. 기준선. 두 가지 다른 기준선과 방법을 비교합니다. (i) 사전 학습된(바닐라) CLIP 체크포인트; (ii) 정규화 항 없이 표준 대조 손실이 있는 MSCOCO에서 미세 조정된 CLIP. 4.2 결과 Winoground. 까다로운 시각 언어 추론 벤치마크인 Winoground(Thrush et al., 2022)에서 SDS-CLIP을 평가합니다. 표(1)에서 접근 방식이 모든 Winoground 하위 범주와 CLIP 변형에서 성능을 지속적으로 개선하여 1.5%~7% 범위의 절대적 개선을 보이는 것을 알 수 있습니다. 7%의 가장 큰 이득은 ViT-B/16(CLIP)에서 관찰되었으며 다른 CLIP 변형은 1.5%~2%의 일관된 개선을 보였습니다. 부록(표 2)에서 유사한 개선이 관찰된 공개 데이터에서 사전 학습된 CLIP 변형에 대한 결과를 제공합니다. Winoground 하위 범주를 자세히 살펴보면 SDS-CLIP이 &quot;객체 교환&quot;과 &quot;관계&quot;에서 일관된 개선을 보인다는 것을 알 수 있습니다. &quot;객체 교환&quot;과 &quot;관계&quot; 태그를 모두 결합한 &quot;둘 다&quot; 하위 범주는 모든 작업의 5%에 불과하므로 객체 교환과 관계적 이해가 모두 포함된 모든 시나리오를 완전히 대표하지 못할 수 있습니다. 또한 캡션의 술어 수에 대한 SDS-CLIP의 견고성을 분석하여 전반적으로 술어가 하나와 두 개 모두 있는 작업에서 성능이 향상됨을 발견했습니다. ARO. ARO 데이터 세트(Yuksekgonul et al., 2023)는 (i) 속성 이해 및 (ii) 관계적 이해에 대한 작업으로 구성됩니다. 표 1에서 SDS-CLIP은 &quot;속성 바인딩&quot; 및 &quot;관계적 이해&quot; 작업에서 성능을 1%-3% 향상시킵니다. CLIP의 제로샷 성능에 미치는 영향. 그림 2에서 SDS-CLIP의 제로샷 분류 기능은 vanilla CLIP에 비해 영향을 받지 않는다는 것을 알 수 있습니다. 사실, ViT-B/16의 제로샷 성능이 다양한 다운스트림 데이터 세트에서 개선된다는 것을 알 수 있습니다(MNIST의 경우 최대 8% 개선). Stable-Diffusion은 CLIP보다 훨씬 더 큰 이미지-텍스트 쌍 세트에서 사전 학습되었지만, 부록 K에서 LAION-2B에서 사전 학습된 CLIP 변형은 여전히 Winoground에서 어려움을 겪는다는 것을 보여줍니다. 사실, SDS-CLIP을 사용하면 이러한 CLIP 변형의 구성적 추론을 개선할 수 있음을 보여줍니다. 부록 H에서 더 큰 CC-3M(Sharma et al., 2018)에서 미세 조정을 사용한 결과를 보여줍니다. 5 관련 연구 CLIP 모델(Radford 등, 2021a)은 강력한 제로샷 분류로 유명하지만, 최근 연구(Thrush 등, 2022; Diwan 등, 2022)는 시각 언어 추론에서 한계가 있음을 드러냈습니다. 반면, 최근 연구에서는 텍스트-이미지 모델(Clark 및 Jaini, 2023; Li 등, 2023; Krojer 등, 2023; Chen 등, 2023)이 추론 작업에서 CLIP보다 성능이 우수하다는 것이 입증되었습니다. 사실, 이러한 모델은 확산 목표에서 계산된 점수를 활용합니다. (Poole 등, 2022)이 텍스트에서 3D 생성을 위해 점수 증류 샘플링을 사용하는 반면, 우리의 연구는 공식을 정규화 도구로 적용하고 CLIP에서 구성 능력을 개선한 최초의 연구라는 점에 유의합니다. 6
--- CONCLUSION ---
본 논문에서는 텍스트-이미지 모델에서 지식을 추출하여 CLIP의 시각 언어 추론 능력을 효과적으로 향상시키는 동시에 제로샷 능력을 손상시키지 않는 새로운 데이터 및 매개변수 효율적 방법인 SDS-CLIP을 소개합니다.7 한계점 본 방법의 주요 한계점은 중간 크기의 GPU에서 큰 배치 크기를 사용할 수 없다는 것입니다.이는 정규화기 LSDS가 매개변수가 고정되어 있더라도 UNet을 통한 전체 역방향 패스가 필요하기 때문입니다.또한 원래 확산 점수는 객체 이해, 속성 이해 및 관계 이해 작업에는 좋지만 ARO 데이터 세트의 정렬 작업에서는 성능이 좋지 않다는 것을 발견했습니다.이러한 이유로 Stable-Diffusion에서 추출하는 것은 정렬 작업에서 CLIP의 성능을 개선하는 데 효과적이지 않을 수 있습니다.(Krojer et al., 2023)과 같은 동시 작업에서도 유사한 결과가 관찰되었습니다. 8 윤리적 고려 사항 CLIP과 같은 시각 언어 모델은 훈련 데이터로 인해 편향을 상속하는 것으로 알려져 있습니다(Agarwal et al., 2021). 저희의 작업은 미세 조정 절차에 널리 사용되는 잘 알려진 데이터 세트(MS-COCO)를 사용하므로 추가적인 편향을 도입하지 않습니다. 사실, 저희의 증류 방법은 이전에 좋은 추론 능력으로 이어지지 않았던 CLIP의 일부 상속된 편향을 완화합니다. 참고 문헌 Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, Miles Brundage. 2021. CLIP 평가: 보다 광범위한 역량과 다운스트림 영향의 특성화를 향해. CoRR, abs/2108.02818. Samyadeep Basu, Daniela Massiceti, Shell Xu Hu, Soheil Feizi. 2023. 매개변수 효율적인 few-shot 미세 조정을 위한 강력한 기준선. Huanran Chen, Yinpeng Dong, Zhengyi Wang, X. Yang, Chen-Dong Duan, Hang Su 및 Jun Zhu. 2023. 단일 확산 모델을 통한 강력한 분류. ArXiv, ABS/2305.15241. 케빈 클라크와 프리양크 제이니. 2023. 텍스트-이미지 확산 모델은 제로샷 분류기입니다. Anuj Diwan, Layne Berry, 최은솔, David Harwath, Kyle Mahawald. 2022. 위노그라운드는 왜 어려운가? 시각언어학적 구성의 실패를 조사합니다. Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang, Xinfeng Zhang, Weijie Chen, Zeng Zhao, Tangjie Lv, Zhipeng Hu 및 Wen Zhang. 2023. Structure-clip: 구조 지식으로 다중 모달 언어 표현을 향상시킵니다. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross B. Girshick. 2016. CLEVR: 구성 언어 및 기본 시각적 추론을 위한 진단 데이터 세트. CoRR, abs/1612.06890. Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, Siva Reddy. 2023. 확산 모델은 시각 및 언어 추론기인가? Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, Deepak Pathak. 2023. 귀하의 확산 모델은 비밀리에 제로샷 분류기입니다. Junnan Li, Dongxu Li, Caiming Xiong, Steven CH Hoi. 2022. BLIP: 통합 시각 언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. CORR, abs/2201.12086. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: 컨텍스트 내 공통 객체. CoRR, abs/1405.0312. Timo Lüddecke and Alexander S. Ecker. 2021. Prompt 기반 다중 모달 이미지 분할. CORR, abs/2112.10003. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby. 2022. 비전 변환기를 사용한 간단한 오픈 어휘 객체 감지. Ron Mokady, Amir Hertz, Amit H. Bermano. 2021. Clipcap: 이미지 캡션을 위한 클립 접두사. Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, Chris Olah. 2018. 미분 가능한 이미지 매개변수화. Https://distill.pub/2018/differentiabletill. 매개변수화. ner, DisNorman Mu, Alexander Kirillov, David A. Wagand Saining Xie. 2021. SLIP: 자기 감독이 언어-이미지 사전 학습을 만난다. CORR, abs/2112.12750. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. Dreamfusion: 2D 확산을 사용한 텍스트-3D. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021a. 자연어 감독에서 이전 가능한 시각적 모델 학습. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 2021b. 자연어 감독을 통한 전이 가능한 시각적 모델 학습. CORR, abs/2103.00020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2021. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CORR, abs/2112.10752. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 2018. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. 제56회 Association for Computational Linguistics 연례 회의록(제1권: 장문 논문), 1~2556~2565쪽, 호주 멜버른. Association for Computational Linguistics. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross. 2022. Winoground: 시각 언어적 구성을 위한 비전 및 언어 모델 탐색. Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, Tongliang Liu. 2021. CRIS: 클립 기반 참조 이미지 분할. CoRR, abs/2111.15174. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, Shalini De Mello. 2023. 텍스트-이미지 확산 모델을 사용한 Openvocabulary 파노라마 분할. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. Coca: 대조 캡셔너는 이미지-텍스트 기반 모델입니다. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2023. 시각-언어 모델이 단어 모음처럼 작동하는 경우와 그 이유는 무엇이며, 이에 대해 어떻게 해야 할까요? 제11회 국제 학습 표현 컨퍼런스에서. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. 2021. Regionclip: 지역 기반 언어 이미지 사전 학습. CoRR, abs/2112.09106. 벤치마크 데이터 세트 A.1 벤치마크 데이터 세트 Winoground(Thrush 등, 2022; Diwan 등, 2022)는 대조적으로 훈련된 이미지-텍스트 모델의 시각 언어적 특성을 평가하기 위한 까다로운 시각-언어 데이터 세트입니다. 이 데이터 세트는 400개의 작업으로 구성되며, 각 작업은 두 쌍의 이미지-텍스트로 구성됩니다. 목표는 각 이미지에 올바른 텍스트 캡션을 독립적으로 할당하는 것입니다. 각 작업에는 작업에 개체 이해, 관계 이해 또는 둘 다가 필요한지에 해당하는 메타데이터도 주석으로 달려 있습니다. Winoground의 작업은 이미지가 세밀하게 다르고 올바른 텍스트 캡션을 할당하려면 고유한 구성적 시각적 추론이 필요하기 때문에 어렵습니다. ARO(Yuksekgonul 등, 2023)도 마찬가지로 시각 언어적 추론을 테스트하며 세 가지 유형의 작업으로 구성됩니다. (i) 개체 속성에 대한 이해를 테스트하기 위한 시각적 게놈 귀속; (ii) 객체 간의 관계적 이해를 테스트하기 위한 Visual Genome Attribution; 및 (iii) 이미지-텍스트 매칭을 수행할 때 텍스트의 단어 순서 민감도를 테스트하기 위한 COCO-Order 및 Flickr30k-Order. Winoground는 ARO보다 크기가 약간 작지만 시각-언어적 구성 지식을 넘어서는 추론이 필요하기 때문에 더 어렵다는 점을 강조합니다(Diwan et al., 2022). A.2 UNet에서 직접 기능을 증류하면 도움이 되나요? = UNet을 통한 추론은 확산 모델에서 다른 차별 모델로 지식을 증류하는 데 중요합니다. B SDS-CLIP: 알고리즘 알고리즘 1 Stable-Diffusion에서 증류하여 시각 언어적 추론을 개선하기 위한 CLIP을 미세 조정하는 알고리즘 필요: D: 이미지-텍스트 쌍, fo: CLIP의 이미지 인코더, gy: CLIP의 텍스트 인코더, ε: UNet; N: 에포크 수; X: 정규화 프로그램의 하이퍼 매개변수, |B|: 배치 크기. while i + N do {xj, Yj } } == D에서 배치 샘플링 tDDPM을 사용하여 샘플 시간 단계 € 가우시안 잡음 샘플링 ε ~ N(0, 1) Lclip 식 (7)과 같이 대조 손실 계산 LSDS 식 (3)과 같이 SDS 손실 계산 Ltotal ← Lclip +\LSDS Ltotal.backward() i + i +end while ▷ Backprop 관련 매개변수 업데이트 C 예비 작업 C.1 CLIP = CLIP (Radford et al., 2021b)은 대조 목적을 사용하여 사전 학습된 이미지-텍스트 모델입니다. (Xu et al., 2023)과 같은 이전 작업에서는 일반적으로 인터넷 규모 데이터에서 UNet의 고정된 기능에는 구조적 기능이 포함되어 있습니다. 학습 목표의 핵심 직관은 이미지에 대한 텍스트 정보를 정렬하는 것입니다. 이것에 동기를 부여하고, 이미지-텍스트 쌍의 이미지 임베딩을 통해 우리는 또한 지식을 직접 공유하는 임베딩 공간을 증류하는지 조사합니다. 이를 위해 동결된 UNet 기능에서 CLIP을 구성하는 것이 유익합니다. 두 가지 구성 요소로 구성된 경우: (i) 이미지 인코더 이미지 x와 해당 캡션 c, 원시 이미지 x¿를 UNet의 imf로 변환하는 동결된 기능 fo를 추출할 수 있습니다(여기서 I(x, c) 임베딩 Cimg(xi) €(vα(x), t, c), fo(xi) Rd, (Xu et al., 2023)과 유사). 우리는<CLS> 토큰; 및 (ii) 텍스트 ent그런 다음 코더 g에서 이러한 동결된 내부 표현을 사용하여 원시 텍스트 캡션 ci를 UNet으로 변환하여 이미지 en의 기능을 텍스트 임베딩 텍스트(Ci)gy(ci)€ Rd로 정규화합니다. CLIP의 코더. 특히: 또한 다음으로 표시됨<EOS> 토큰은 둘 다 임베딩 차원 d에 매핑됩니다. 이미지-텍스트 쌍의 데이터 세트 D = {(xi, c)}가 주어지면 (xi, yi)는 i번째 이미지-텍스트 쌍이고, CLIP은 대조 목적을 사용하여 일치하는 쌍의 이미지 및 텍스트 임베딩을 함께 끌어오고 일치하지 않는 쌍의 임베딩은 떼어냅니다. 공식적으로 대조 목적은 다음과 같이 정의할 수 있습니다. Ltotal = LCLIP +\||hw(fø(x) — I(x, c))||½ (6) 그러나 이런 방식의 증류가 시각 언어 추론의 성능을 향상시키지는 않는다는 것을 발견했습니다. 사실, ViT-B/16(CLIP)의 경우 Winoground 점수가 0.24에서 0.23으로 감소하는 것을 발견했습니다. 이 결과는 역전파를 포함하는 점수증류 샘플링을 사용하여 LCLIP = Limage-text + Ltext-image (7) 여기서: Limage text = − 2NLtext-image = 2N IM² M² log{ log{ exp(eimg(xj)etext (Cj)/T) Σ= exp((eimg(xj) Tetext (Ck)/T)) exp(Cimg(xj) etext (C;)/T) Σ1 exp((eimg(xk) Tetext (C;)/T)) (8) } (9) 여기서 7은 학습 가능한 온도 매개변수입니다. 일반적으로 D는 수백만 개의 이미지-텍스트 쌍으로 구성된 인터넷 규모 데이터 세트입니다. 또한 사전 학습 중에 임베딩 Cimg (xi) 및 Etext (Ci)는 단위 노름을 갖도록 정규화됩니다. D 언제 증류가 CLIP에 도움이 되지 않습니까? Stable-Diffusion에서 CLIP으로 지식을 추출하는 것이 객체 교환, 관계 이해 및 속성 바인딩 시각 언어 작업에 도움이 되지만 텍스트 순서가 교란되는 작업(예: ARO 데이터 세트의 COCO-Order 및 Flickr-Order 작업)에는 도움이 되지 않습니다. 사실, 방정식(1)의 노이즈 제거 확산 점수는 COCO-Order의 경우 정확도가 0.24, Flickr-Order의 경우 정확도가 0.34로 나타나며 이는 실제로 CLIP 모델보다 낮습니다. 동시 작업(Krojer et al., 2023)도 텍스트 순서 지정 작업에 대해 비슷하게 낮은 성능을 보였습니다. 잠재적인 이유는 순서 지정 작업이 현재 텍스트 인코더가 효과적으로 모델링할 수 없는 문법적 이해만 테스트하기 때문일 수 있습니다. 또 다른 이유는 노이즈 제거 확산 점수가 단어 순서의 영향을 받지 않기 때문일 수 있는데, 그 결과 이미지 의미가 변경되지 않기 때문입니다. E 미세 조정 데이터 세트에 대한 참고 사항 다중 모드 학습에 널리 사용되는 MS-COCO(Lin et al., 2014)를 사용합니다. 이 데이터 세트에는 이름이 없으며 개별 사람이나 공격적인 콘텐츠를 고유하게 식별하지 않습니다. F 추가 실험 세부 정보 하이퍼 매개변수. ViT-B/16에 대한 학습 속도와 정규화 하이퍼 매개변수 \에 대한 하이퍼 매개변수 스윕을 수행합니다. ViT-B/32, ViT-B/14, ViT-L/14-336px 및 ResNet-50을 포함한 다양한 CLIP 변형에 동일한 하이퍼 매개변수를 사용합니다. 특히 &gt; = 0.001을 설정하고 학습 속도를 5 × 10-5로 설정합니다. 모든 다양한 CLIP 모델에 대해 배치 크기를 32로 사용합니다. 실험에서 교사 모델로 Stable-Diffusion v1-4를 사용합니다. 전체 미세 조정에 대한 참고 사항 모든 실험은 주로 LayerModel 전체 객체 관계 둘 다 1 주요 예측 2 주요 예측 VIT-B/16(LAION 400M) 0.0.0.0.0.0.COCO FT와 LCLIP COCO FT와 LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0.0.표 2: 공개 데이터(LAION400M)로 사전 학습된 VIT-B/16 CLIP을 사용한 Winoground의 추가 결과. Norm 매개변수. 프로젝트의 초기 단계에서는 CLIP의 텍스트 및 이미지 인코더의 모든 매개변수를 미세 조정했지만 표(1)에 보고된 것보다 성능이 떨어졌습니다. 잠재적으로 이는 새로운 정규화자와 함께 사용할 때 과적합 문제로 인해 발생할 수 있습니다. 따라서 최상의 결과로 이어지므로 LayerNorm 조정으로 모든 실험을 실행합니다. 총 GPU 시간. 모든 실험에 NVIDIA-A6000을 사용했으며 각 미세 조정 실험은 6시간이 걸립니다.G Stable-Diffusion-v2를 사용한 추가 결과-특히 Stable-Diffusion v-2.1을 교사로 사용한 증류 전략을 사용하여 Winoground에서 다음과 같은 결과를 얻었습니다.(i) ViT-B/16: 0.35; (ii) ViT-B/32: 0.33; (iii) ViT-L/14: 0.31; (iv) ViT-L/14-336px: 0.31; (iv) ResNet-50: 0.28; 모든 점수는 Stable-Diffusion-v1-4를 교사로 사용한 미세 조정 모델보다 높으므로 구성 생성 기능이 더 나은 교사가 더 나은 선택임을 강조합니다. H_개념적 캡션을 사용한 미세 조정 우리는 주로 MS-COCO를 다음과 같은 이유로 사용합니다.(i) 비교적 작은 데이터 세트로 미세 조정 단계를 비교적 작게 유지할 수 있으며 미세 조정 데이터 세트를 확장하면 미세 조정 시간이 늘어납니다.(ii) 커뮤니티에서 사용하는 잘 정립되고 비교적 다양하며 주석이 잘 달린 이미지-텍스트 데이터 세트입니다.우리는 또한 CC-3M(Sharma et al., 2018)으로 미세 조정했지만 MS-COCO를 사용한 경우와 유사한 라인에서 개선이 이루어졌다는 것을 발견했습니다.예를 들어, CC-3M을 사용한 Winoground에서 StableDiffusion-v1-4로 증류한 후 다음과 같은 성능을 발견했습니다.(i) ViT-B/16: 0.32;(ii) ViT-B/32: 0.32;(iii) ViT-L/14: 0.30;(iv) ViT-L/14-336px: 0.28; (iv) ResNet-50: 0.27. 이 점수는 MS-COCO를 사용하는 것보다 약간 더 나았지만 데이터 세트 크기가 30배 이상이므로 Model Overall Object Relation Both 1 Main Pred 2 Main Preds VIT-B/16(LAION 2B) COCO FT with LCLIP + LSDS 0.0.0.0.0.0.0.0.0.0.0과 같은 고품질 데이터 세트가 더 나은 것으로 나타났습니다.표 3: CLIP(2B 이미지로 사전 학습)은 여전히 Winoground에서 성능이 떨어집니다. LAION-2B(Stable-Diffusion과 유사한 규모의 학습 데이터)로 학습한 경우에도 CLIP이 Stable-Diffusion의 확산 점수보다 성능이 떨어집니다. 이는 데이터의 규모만으로는 CLIP의 추론 능력을 완화하는 데 유용할 수 없음을 보여줍니다. MS-COCO는 CLIP의 구성 능력을 향상시키기에 충분합니다. L CLIP 너머 2B 이미지-텍스트 쌍으로 사전 학습된 Open-CoCa(Yu et al., 2022)가 Winoground에서 0.30의 점수를 얻는 것을 발견했습니다. 증류 전략을 사용하면 점수가 0.33으로 향상되어 증류 전략을 CLIP 너머의 모델에 사용할 수 있음을 알 수 있습니다. 다양한 시각-언어 모델에 대한 증류 방법의 영향에 대한 전체 조사는 향후 작업으로 미루겠습니다. I OpenCLIP을 사용한 결과 표 2에서 저희 방법이 OpenCLIP과 호환됨을 보여줍니다. 특히 OpenCLIP으로 증류하면 시각 언어 점수가 0.24에서 0.30으로 향상되는 것을 알 수 있습니다. 이러한 결과는 증류 방법의 일반화 가능성을 강조합니다. J CLEVR에 대한 추가 결과 저희는 미세 조정된 모델을 CLEVR 작업(Johnson et al., 2016)에 적용합니다. 이 작업은 공간 추론이나 속성 바인딩과 같은 현상을 분리하는 3D 모양의 이미지로 구성됩니다. 우리는 확산 점수가 0.67의 점수로 이어지는 반면, 테스트 베드에서 가장 좋은 CLIP 변형(CLIP VITL/14)은 0.63의 점수를 받았습니다. 미세 조정 중 증류 손실로 이 점수는 2% 이득으로 0.65로 개선되었습니다. K 사전 학습 데이터의 규모가 도움이 되는가?표 3에서 우리는 안정적 확산(LAION-2B)과 동일한 규모의 사전 학습 데이터에서 학습한 경우에도 CLIP 모델이 Winoground 데이터 세트에서 어려움을 겪는다는 것을 보여줍니다. 우리는 특히 CLIP(2B 이미지-텍스트 쌍에서 사전 학습된 경우)이 0.27의 점수를 얻는 반면, 유사한 사전 학습 코퍼스에서 학습된 확산 모델은 0.35의 점수를 얻는다는 점을 강조합니다. 이는 유사한 사전 학습 규모에서 확산 모델(확산 목적 포함)이 CLIP 유사 모델보다 구성적 학습기가 더 뛰어나다는 것을 분명히 보여줍니다. Stable-Diffusion을 활용한 증류 방법은 CLIP(2B 이미지-텍스트 쌍으로 사전 학습됨)에서 Winoground 점수를 0.27에서 0.31로 향상시켰습니다.
