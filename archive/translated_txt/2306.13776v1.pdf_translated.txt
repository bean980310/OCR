--- ABSTRACT ---
Transformer 모델은 언어 작업에서 성공한 후 컴퓨터 비전에서 큰 잠재력을 보여주었습니다. Swin Transformer는 정확도 측면에서 합성곱 기반 아키텍처보다 성능이 뛰어나고, 입력 크기에 비해 2차 복잡도를 갖는 Vision Transformer(VIT) 및 그 변형과 비교했을 때 효율성이 향상된 모델 중 하나입니다. Swin Transformer는 자체 주의 계산을 겹치지 않는 로컬 윈도우로 제한하면서 크로스 윈도우 연결을 허용하는 윈도우 이동 기능이 있습니다. 그러나 윈도우 이동은 런타임의 상당 부분을 차지하는 메모리 복사 작업을 도입합니다. 이 문제를 완화하기 위해 로컬 윈도우 간의 교차 연결을 달성하기 위해 윈도우를 이동하는 대신 단계에 따라 크기가 변하는 윈도우를 적용하는 Swin-Free를 제안합니다. 이 간단한 설계 변경으로 Swin-Free는 추론에서 Swin Transformer보다 더 빠르게 실행되고 정확도가 더 좋습니다. 또한 Swin Transformer 대응 제품보다 빠른 몇 가지 SwinFree 변형도 제안합니다. 1.
--- INTRODUCTION ---
최근까지 합성곱 신경망(CNN)은 자율 주행[2-7]과 같이 과거에는 너무 어렵다고 여겨졌던 컴퓨터 비전 작업에서 주목할 만한 혁신을 주도해 왔습니다. 그러나 최근 CNN의 주도적 역할이 Transformer 기반 네트워크로 이전되고 있습니다[1,8,9]. Transformer 모델은 텍스트 분류 및 기계 번역과 같은 자연어 처리(NLP) 작업을 위해 처음 제안되었으며 큰 성공을 거두었습니다[10-12]. 언어 영역에서의 이러한 획기적인 발전은 컴퓨터 비전 커뮤니티에서 큰 관심을 불러일으켰고 최근 이미지 분류[1,8] 및 의미 분할[13]과 같은 다양한 작업에서 유망한 결과를 이끌어냈습니다. Transformer 아키텍처의 핵심 구성 요소는 시퀀스의 다른 요소에 대한 한 요소의 관련성을 학습하는 셀프 어텐션 모듈입니다. 제한된 범위 내에서만 맥락에 주의를 기울일 수 있는 LSTM[14]과 같은 순환 네트워크와 달리 셀프 어텐션 메커니즘은 시퀀스의 모든 엔터티 간의 상호 작용을 명시적으로 모델링합니다. 이를 통해 Transformers는 글로벌 컨텍스트를 한 번에 학습할 수 있어 많은 애플리케이션에서 성공할 수 있습니다[8, 12, 15]. 그러나 단점은 셀프 어텐션의 계산 복잡도가 입력 시퀀스의 길이에 비해 2차적으로 증가한다는 것입니다. 이는 특히 컴퓨터 비전 작업에서 심각한 문제가 될 수 있는데, 이는 종종 이미지 해상도에 의해 결정되는 시퀀스 길이가 엄청나게 클 수 있기 때문입니다. Swin Transformer[1]는 이미지를 겹치지 않는 창으로 분할하고 로컬 창 내에서 셀프 어텐션을 계산하여 2차 복잡도 문제를 완화합니다. 겹치지 않는 창을 연결하기 위해 Swin Transformer는 연속적인 셀프 어텐션 계층 간에 창 파티션을 이동하여 로컬 창 간에 교차 연결을 제공합니다. 이러한 설계 선택은 효율성과 정확성을 향상시키지만 창을 이동하기 위한 작업은 메모리에서 데이터 이동을 초래합니다. 실제로 표 1에서 볼 수 있듯이 NVIDIA TensorRT[16]로 추론을 수행할 때 창을 이동하는 것은 Swin Transformer 모델의 총 런타임의 약 8.7%를 차지합니다. Swin Transformer의 이러한 단점을 완화하기 위해 데이터 이동을 줄이기 위해 로컬 창을 이동하지 않는 Swin-Free를 제안합니다. 대신 겹치지 않는 창 간의 교차 연결을 달성하기 위해 Swin-Free는 여러 단계에서 창 크기를 변경합니다(표 2 참조). 예를 들어, Swin-Free는 이전 단계의 더 작은 로컬 창 간의 교차 주의를 모델링하기 위해 단계에서 창 크기를 두 배로 늘릴 수 있습니다. 실험 결과에 따르면 크기가 변하는 창을 특징으로 하는 Swin-Free는 주로 창 이동을 피하고 더 큰 입력으로 더 빠른 행렬 곱셈을 활용할 수 있기 때문에 Swin Transformer에 비해 모델 런타임을 크게 줄입니다. 최신 GPU에서는 큰 커널을 사용한 합성곱과 같은 수학 연산의 효율적인 구현이 널리 사용 가능하다는 점에 유의하세요. Swin-Free에서 런타임의 더 큰 부분은 StageLayerNorm Window Partition Window Attention Window Merging 1st Block Stage1st Block LayerNorm Window Partition Window MLP གཞི 2nd Block Window MergingReverse Cyclic Window Shift LayerNorm MLP (a) Swin 변환기의 기존 윈도우 이동 블록 구조 [1] LayerNorm MLP HH LayerNorm Window Partition Window Attention Window Merging 2nd Block LayerNorm MLP Stage(b) 다양한 윈도우 크기를 갖는 제안된 블록 구조 그림 1. Swin과 Swin-Free의 기능 블록 비교. Swin-Free에서 윈도우 이동이 제거되고 로컬 윈도우의 크기가 단계에 따라 다르다는 점에 유의하세요. 윈도우 크기는 단계에 따라 다릅니다. 표 1. NVIDIA RTX 3080 GPU에서 Swin Transformer 모델(Swin-B)의 작동 프로필. 런타임에서의 작업 백분율(%) TensorRT PyTorch(FP16) (FP32) Shifting windows 8.4.LayerNorm GELU 10.9.13.3.메모리 복사보다는 계산에 사용되어 GPU 활용도가 더 좋음을 나타냄.동시에 Swin-Free는 분류 정확도도 향상시켜 크기가 변하는 windows가 일정한 windows 크기를 갖는 shifting windows보다 더 나은 모델링 성능을 제공할 수 있음을 의미함.또한 정확도보다 지연 시간을 우선시하는 여러 Swin-Free 변형을 제안함.다시 말해, 동등한 정확도로 Swin-Free 변형은 Swin Transformer 대응 제품보다 더 빠르게 설계됨.또한 런타임의 상당 부분을 차지하는 일반적으로 사용되지만 비용이 많이 드는 LayerNorm 및 GELU 계층 대신 BatchNorm 및 ReLU와 같은 더 효율적인 계층을 사용하여 Swin-Free를 더욱 단순화함(표 1 참조).이러한 설계 요소를 통해 Swin-B에 비해 지연 시간을 19% 개선할 수 있었음. 또한, 우리는 Swin-Free의 향상된 모델링 능력을 활용함으로써 모델의 깊이를 더욱 줄일 수 있음을 보여줍니다. 예를 들어, Swin-Free의 변형은 정확도 손실 없이 Swin보다 약 33% 더 빠릅니다(표 6 참조). 2.
--- RELATED WORK ---
합성곱 신경망(CNN): 지난 10년 동안 CNN은 컴퓨터 비전의 사실상 표준이었으며 아키텍처 설계의 혁신을 통해 정확도를 계속 개선하고 있습니다[2–5]. 동시에 효율성을 위해 CNN 모델의 복잡성을 줄이기 위한 많은 노력도 이루어졌습니다. 이러한 방향에는 모델 압축, 양자화 및 깊이별 합성곱과 같은 저비용 작업이 포함됩니다[6,7]. CNN이 여전히 컴퓨터 비전 작업에서 지배적이기는 하지만 최근의 많은 연구에서 Transformer 기반 모델이 최첨단 CNN 기반 모델보다 성능이 우수하다는 것이 입증되었습니다[1,8,9]. 논란의 여지가 있지만 컴퓨터 비전에서 CNN에서 Trans-StageWindow 크기로 패러다임이 전환되는 것을 곧 보게 될 것입니다. 이전 단계에서 크기가 일정합니다. Transformer 아키텍처: 기계 번역 작업을 위한 선구적 작업[17]에서 도입된 Transformer는 NLP 작업을 위한 최첨단 모델이 되었으며 대부분의 LSTM 기반 시퀀스 대 시퀀스 접근 방식을 대체했습니다[10-12, 18, 19]. 영어: 단기 컨텍스트를 재귀적으로 처리하는 순환 네트워크와 달리 Transformer 아키텍처는 시퀀스의 모든 요소 간의 상대적 중요성을 명시적으로 모델링하여 시퀀스 전체 관계를 학습하는 어텐션 메커니즘을 기반으로 합니다.즉, Transformer는 시퀀스를 전체적으로 처리하고 재귀는 완전히 방지됩니다.시각에서의 Transformer: 최소한의 시각 관련 수정으로 ViT[8]는 이미지 분류 작업에 어텐션 메커니즘을 적용합니다.입력 토큰 임베딩의 대응물인 ViT는 이미지를 패치 임베딩 시퀀스로 나누어 표준 Transformer에 공급합니다.ViT는 이미지 분류에서 CNN보다 성능이 뛰어나지만 CNN에 비해 학습하기 어렵다고 종종 보고되었습니다.어텐션 연산의 계산 복잡도는 입력 크기에 2차적으로 비례하기 때문에 ViT는 고해상도 이미지를 입력으로 사용하는 데 어려움이 있습니다.DETR[20] 및 SETR[21]과 같은 다른 Transformer 기반 시각 모델도 이러한 2차 복잡도 문제를 가지고 있습니다. 3. 예비: Swin Transformer Swin Transformer[1]는 다단계 계층적 아키텍처를 활용합니다. 여기서 입력 이미지는 먼저 작은 크기의 패치로 나뉘고 피처 맵은 단계를 따라 이웃 패치와 점진적으로 병합됩니다. 이러한 계층적 표현을 통해 Swin Transformer는 객체 감지 및 분할과 같은 밀집 예측 작업에 쉽게 적용할 수 있습니다. Swin Transformer는 겹치지 않는 로컬 윈도우 내에서 셀프 어텐션을 계산하여 선형적 계산 복잡도를 달성합니다. 로컬 윈도우 간의 상호 작용을 캡처하기 위해 연속적인 Transformer 블록에서 두 윈도우 구성 간에 번갈아가는 이동된 윈도우 체계가 사용됩니다. 윈도우 이동은 Swin Transformer가 주장하는 정확도를 달성하는 데 중요한 역할을 하지만 많은 메모리 이동도 도입합니다. 표 1에서 볼 수 있듯이 Swin-B(Swin Transformer 변형 중 하나)의 이동 윈도우 작업은 NVIDIA TensorRT(FP16 정밀도)에서 총 런타임의 8.7%, PyTorch(FPprecision)에서 4.4%를 차지합니다. 이는 메모리 이동을 최소화할 수 있다면 지연 시간을 개선할 여지가 있음을 시사합니다. 또한 Swin Transformer에서 사용되는 LayerNorm과 GELU는 표 1에 표시된 대로 런타임의 상당 부분을 담당합니다. 그림 2의 ONNX 표현[22]에서 이 두 가지 연산을 살펴보면 이 두 계층을 충족하는 일련의 수학 연산을 식별할 수 있습니다. 이전 연구에서는 BatchNorm 및 ReLU 레이어를 전략적으로 사용하면 Trans1x3136xSub 1x3136xMul B(128) Add B&lt;128&gt; Div T Pow Y=1x3136xReduceMean 1x3136x1x3136xReduceMean 1x3136x1x3136xDiv B=1.41421353... 1x3136x1x3136xAdd B = 0.00000999... 1x3136x1x3136xErf Sqrt Add 1x3136xB=1x3136xMul 1x3136x1x3136x1x3136xMul 1x3136xB=0.1x3136x(a) LayerNorm (b)의 정확도가 향상됨을 제안했습니다. GELU 그림 2. LayerNorm과 GELU의 ONNX 표현 예. 이전 모델은 크게 저하되지 않습니다[23]. 이 논문에서 우리는 정확도와 런타임을 위해 Swin Transformer를 기반으로 개선을 시도하고 다음 섹션에서 설명할 Swin-Free를 제안합니다. 4.
--- METHOD ---
4.1. Swin-Free 개요 그림 3에 표시된 기본 아키텍처는 Swin Transformer [1]와 유사하지만 이동된 창을 사용하지 않습니다. 입력 이미지는 먼저 패치됩니다. 각 단계는 패치에 대해 여러 개의 Swin 스타일 Transformer 블록을 적용합니다. 여기서 셀프 어텐션 계산은 겹치지 않는 각 로컬 창 내에서 수행됩니다. 여기서 로컬 창은 M × M 패치에서 작동합니다. Swin Transformer와 마찬가지로 패치 병합 계층에 의해 각 단계에서 패치 수가 절반으로 줄어듭니다. Swin Transformer와의 유일한 차이점은 로컬 창을 이동하지 않는다는 것입니다. 대신 각 단계에서 로컬 창(즉, M)의 크기를 변경하기로 선택했으며, 이는 섹션 4.2에서 더 자세히 설명합니다. 입력 크기가 224×224일 때 Swin과 Swin-Free의 차이는 표 2에 요약되어 있습니다. 2단계와 3단계에서 Swin-Free는 Swin보다 더 큰 창 크기를 사용하므로 Image Patchify Swin-Free Transformer Blocks StageStageStageStagePatch Merging Swin-Free Transformer Blocks Patch Merging Swin-Free Transformer Blocks Patch Merging Swin-Free Transformer Blocks 그림 3. Swin-Free의 전체 아키텍처. 표 2. 입력 크기가 224×224일 때 Swin과 Swin-Free의 비교. 여기서 P는 단계 시작 시 패치 수를 의미합니다. M과 N의 값은 각각 로컬 창의 크기와 단계에서 겹치지 않는 창의 수를 나타냅니다. Swin Transformer는 모든 Transformer 블록에서 창을 이동시키는 반면 Swin-Free는 창을 이동시키지 않습니다. Swin-Free P = 56 ×M =N =P = 28 xM =N =Stage Swin P = 56 ×M =N =P = 28 xM =N =P = 14 ×M =N =P = 7 xP = 7 xM =N =M =N =P = 14 ×M =N =Swin-Free는 해당 단계에서 Swin보다 작습니다.그림은 또한 Swin-Free가 블록 수준에서 Swin과 어떻게 다른지 자세히 보여줍니다.Swin-Free에서 Swin Transformer에서 사용된 창 이동과 그 역연산이 제거되고 창의 크기가 각 단계마다 변경됩니다.4.2. 크기가 변하는 창 Swin Transformer에서 로컬 창을 이동하는 것은 창 간의 교차 연결을 달성하는 효과적인 방법이지만 메모리에서 데이터를 이동해야 합니다.이는 일반적으로 GPU에서의 수학 계산보다 비용이 많이 들기 때문에 모델 효율성에 부정적인 영향을 미칠 수 있습니다.사실, 표 1에서 볼 수 있듯이 창 이동은 전체 런타임의 상당 부분을 차지합니다. 이동된 창을 사용하지 않기 위해 각 단계에서 로컬 창의 크기를 변경하여 겹치지 않는 창 간의 교차 연결을 활성화합니다. M은 로컬 창의 크기입니다. 표 2에서 볼 수 있듯이 입력 크기 224×224에 대한 구현에서 4단계에 대해 M의 값을 M 7, 14, 14, 7로 변경합니다. 이 설정에서 2단계와 3단계에서 네 개의 인접한 7×7 로컬 창 간의 교차 연결을 고려합니다. 즉, 현재 단계의 14×로컬 창에는 이전 단계의 7×7 로컬 창 4개가 효과적으로 포함됩니다. 위의 변경 사항은 어텐션 블록에서 창 크기가 확대되어 단일 로컬 창의 GPU 계산 부하가 증가할 수 있습니다. 그러나 표 2에서 Swin-Free의 stagesand3에서 겹치지 않는 로컬 창의 수(즉, N)가 Swin의 1/4이 됩니다. 다시 말해, Swin-Free의 행렬 곱셈에서 행렬의 크기는 더 크지만 처리할 행렬의 수는 더 적습니다. GPU에서 7×7 로컬 창 4개를 처리하는 것에 비해 14×14 로컬 창을 처리하는 것이 지연 시간을 증가시키지 않고, 오히려 방대한 병렬 컴퓨팅 기능 덕분에 지연 시간을 감소시키는 것을 관찰했습니다. 이 점에 대해서는 섹션 5에서 더 자세히 논의합니다. 4.3. LayerNorm 및 GELU의 추가 최적화 대체: 그림 2에서 볼 수 있듯이 LayerNorm과 GELU는 여러 수학 계층으로 구성되어 일반적으로 사용되는 BatchNorm 및 ReLU 계층에 비해 더 많은 계산이 필요합니다. 표 1에서 LayerNorm과 GELU는 TensorRT로 실행할 때 Swin Transformer 모델의 총 런타임의 약 24%를 차지하는 것으로 관찰되었습니다. 따라서 애플리케이션에서 지연 시간도 중요한 경우 정확도가 크게 저하되지 않고 BatchNorm 및 ReLU로 대체합니다[23]. 섹션 5에서 이러한 수정을 통해 SwinFree가 정확도 측면에서 Swin Transformer를 능가하면서도 더 빠르게 실행할 수 있음을 알 수 있습니다. 깊이 감소: 지연을 우선시하는 또 다른 방법은 모델의 깊이를 줄이는 것입니다. 구체적으로, 3단계에서 Transformer 블록의 수를 줄이는 것을 고려합니다. 예를 들어, 3단계가 Transformer 블록으로 구성된 Swin-B와 비교할 때, 14개 블록만 사용하는 것을 고려할 수 있습니다. 섹션 5에서 이 Swin-Free 변형이 지연에서 상당한 개선을 통해 Swin Transformer보다 더 나은 정확도를 달성할 수 있음을 알 수 있습니다.5.
--- EXPERIMENT ---
모든 결과에 따르면 크기가 변하는 창을 특징으로 하는 Swin-Free는 주로 창 이동을 피하고 더 큰 입력으로 더 빠른 행렬 곱셈을 활용할 수 있기 때문에 Swin Transformer에 비해 모델 런타임을 상당히 줄입니다. 최신 GPU에서는 큰 커널을 사용한 합성과 같은 수학 연산의 효율적인 구현이 널리 사용 가능합니다. Swin-Free에서 런타임의 더 큰 부분은 StageLayerNorm Window Partition Window Attention Window Merging 1st Block Stage1st Block LayerNorm Window Partition Window MLP གཞི 2nd Block Window MergingReverse Cyclic Window Shift LayerNorm MLP (a) Swin Transformer의 기존 창 이동 블록 구조 [1] LayerNorm MLP HH LayerNorm Window Partition Window Attention Window Merging 2nd Block LayerNorm MLP Stage(b) 다양한 창 크기를 갖는 제안된 블록 구조 그림 1. Swin과 Swin-Free의 기능 블록 비교. Swin-Free에서는 윈도우 이동이 제거되고 로컬 윈도우의 크기가 단계에 따라 다릅니다. 윈도우 크기는 단계에 따라 다릅니다. 표 1. NVIDIA RTX 3080 GPU에서 Swin Transformer 모델(Swin-B)의 작업 프로필. 런타임에서의 작업 비율(%) TensorRT PyTorch(FP16)(FP32) 윈도우 이동은 메모리 복사가 아닌 계산에 사용되어 GPU 활용도가 더 높음을 나타냅니다. 동시에 Swin-Free는 분류 정확도도 향상시켜 크기가 변하는 윈도우가 윈도우 크기가 일정한 윈도우 이동보다 더 나은 모델링 성능을 제공할 수 있음을 의미합니다. 또한 정확도보다 지연 시간을 우선시하는 여러 가지 Swin-Free 변형을 제안합니다. 즉, 동등한 정확도로 Swin-Free 변형은 Swin Transformer 대응 제품보다 더 빠르게 설계됩니다. 또한, 일반적으로 사용되지만 비용이 많이 드는 LayerNorm 및 GELU 계층 대신 BatchNorm 및 ReLU와 같은 보다 효율적인 계층을 사용하여 Swin-Free를 더욱 단순화합니다.이 계층은 런타임의 상당 부분을 차지합니다(표 1 참조).이러한 설계 요소를 사용하여 Swin-B에 비해 대기 시간을 19% 개선할 수 있었습니다.또한 Swin-Free의 향상된 모델링 성능을 활용하면 모델의 깊이를 더욱 줄일 수 있음을 보여줍니다.예를 들어, Swin-Free의 변형은 정확도 손실 없이 Swin보다 약 33% 더 빠릅니다(표 6 참조).2. 관련 작업 합성곱 신경망(CNN): 지난 10년 동안 CNN은 컴퓨터 비전의 사실상 표준이 되었으며 아키텍처 설계의 혁신을 통해 정확도를 계속 개선하고 있습니다[2–5]. 동시에 효율성을 위해 CNN 모델의 복잡성을 줄이기 위한 많은 노력도 이루어졌습니다.이러한 방향에는 모델 압축, 양자화 및 깊이별 합성곱과 같은 저비용 작업이 포함됩니다[6,7]. CNN이 여전히 컴퓨터 비전 작업에서 우세하지만, 최근 많은 연구에서 Transformer 기반 모델이 최첨단 CNN 기반 모델보다 성능이 우수하다는 것이 입증되었습니다[1,8,9]. 논란의 여지가 있지만, 컴퓨터 비전에서 CNN에서 Trans-StageWindow 크기로 패러다임이 전환되는 것을 곧 보게 될 것입니다. Transformer 아키텍처: 기계 번역 작업을 위한 선구적 작업[17]에서 도입된 Transformer는 NLP 작업을 위한 최첨단 모델이 되었으며, 대부분의 LSTM 기반 시퀀스 대 시퀀스 접근 방식을 대체했습니다[10-12, 18, 19]. 단기 컨텍스트를 재귀적으로 처리하는 순환 네트워크와 달리 Transformer 아키텍처는 시퀀스의 모든 요소 간의 상대적 중요성을 명시적으로 모델링하여 시퀀스 전체 관계를 학습하는 어텐션 메커니즘을 기반으로 합니다. 즉, Transformer는 시퀀스를 전체적으로 처리하고 재귀는 완전히 방지됩니다. 비전에서의 Transformer: 최소한의 비전 관련 수정으로 ViT[8]는 어텐션 메커니즘을 이미지 분류 작업에 적용합니다. 입력 토큰 임베딩의 대응물인 ViT는 이미지를 패치 임베딩 시퀀스로 나누어 표준 Transformer에 공급합니다.ViT는 이미지 분류에서 CNN보다 성능이 뛰어나지만 CNN에 비해 훈련하기 어렵다는 보고가 많습니다.어텐션 연산의 계산 복잡도는 입력 크기에 2차적으로 비례하기 때문에 ViT는 고해상도 이미지를 입력으로 받는 데 어려움이 있습니다.DETR[20] 및 SETR[21]과 같은 다른 Transformer 기반 비전 모델도 이러한 2차 복잡도 문제를 가지고 있습니다.3. 예비: Swin Transformer Swin Transformer[1]는 다단계 계층적 아키텍처를 활용합니다.여기서 입력 이미지는 먼저 작은 크기의 패치로 나누어지고 피처 맵은 단계를 따라 이웃 패치와 점진적으로 병합됩니다.이러한 계층적 표현을 통해 Swin Transformer는 객체 감지 및 분할과 같은 밀집 예측 작업에 쉽게 적용할 수 있습니다.Swin Transformer는 겹치지 않는 로컬 윈도우 내에서 셀프 어텐션을 계산하여 선형 계산 복잡도를 달성합니다. 로컬 윈도우 간의 상호 작용을 캡처하기 위해 연속적인 Transformer 블록에서 두 윈도우 구성 간에 번갈아가는 이동된 윈도우 방식이 사용됩니다. 윈도우 이동은 Swin Transformer가 주장하는 정확도를 달성하는 데 중요한 역할을 하지만 많은 메모리 이동도 도입합니다.표 1에서 볼 수 있듯이 Swin-B(Swin Transformer 변형 중 하나)의 이동 윈도우 연산은 NVIDIA TensorRT(FP16 정밀도)를 사용하여 전체 런타임의 8.7%를 차지하고 PyTorch(FPprecision)를 사용하여 4.4%를 차지합니다.이는 메모리 이동을 최소화할 수 있다면 지연 시간을 개선할 여지가 있음을 시사합니다.또한 Swin Transformer에서 사용되는 LayerNorm과 GELU도 표 1에서 볼 수 있듯이 런타임의 상당 부분을 담당합니다.그림 2의 ONNX 표현[22]에서 이 두 연산을 살펴보면 이 두 계층을 충족하는 일련의 수학 연산을 식별할 수 있습니다. 이전 연구에서는 BatchNorm 및 ReLU 레이어를 전략적으로 사용하면 Trans1x3136xSub 1x3136xMul B(128) Add B&lt;128&gt; Div T Pow Y=1x3136xReduceMean 1x3136x1x3136xReduceMean 1x3136x1x3136xDiv B=1.41421353... 1x3136x1x3136xAdd B = 0.00000999... 1x3136x1x3136xErf Sqrt Add 1x3136xB=1x3136xMul 1x3136x1x3136x1x3136xMul 1x3136xB=0.1x3136x(a) LayerNorm (b)의 정확도가 향상됨을 제안했습니다. GELU 그림 2. LayerNorm 및 GELU의 ONNX 표현 예. 이전 모델은 크게 저하되지 않습니다[23]. 이 논문에서는 Swin Transformer를 기반으로 정확도와 런타임을 개선하고 다음 섹션에서 설명할 Swin-Free를 제안합니다. 4. 방법 4.1. Swin-Free 개요 그림 3에 표시된 기본 아키텍처는 Swin Transformer[1]와 유사하지만 이동된 창을 사용하지 않습니다. 입력 이미지는 먼저 패치됩니다. 각 단계는 패치에 대해 여러 개의 Swin 스타일 Transformer 블록을 적용합니다. 여기서 셀프 어텐션 계산은 겹치지 않는 각 로컬 창 내에서 수행됩니다. 여기서 로컬 창은 M × M 패치에서 작동합니다. Swin Transformer와 마찬가지로 패치 병합 계층에 의해 각 단계에서 패치 수가 절반으로 줄어듭니다. Swin Transformer와의 유일한 차이점은 로컬 창을 이동하지 않는다는 것입니다. 대신, 각 단계에서 로컬 윈도우(즉, M)의 크기를 다르게 하기로 했습니다.이에 대해서는 섹션 4.2에서 더 자세히 설명합니다.입력 크기가 224x224일 때 Swin과 Swin-Free의 차이점은 표 2에 요약되어 있습니다.단계 2와 3에서 Swin-Free는 Swin보다 더 큰 윈도우 크기를 사용하므로 Image Patchify Swin-Free Transformer Blocks StageStageStageStagePatch Merging Swin-Free Transformer Blocks Patch Merging Swin-Free Transformer Blocks Patch Merging Swin-Free Transformer Blocks 그림 3. Swin-Free의 전체 아키텍처.표 2. 입력 크기가 224x224일 때 Swin과 Swin-Free의 비교.여기서 P는 단계 시작 시 패치 수를 의미합니다.M과 N의 값은 각각 로컬 윈도우의 크기와 단계에서 겹치지 않는 윈도우의 수를 나타냅니다. Swin Transformer는 다른 모든 Transformer 블록에서 창을 이동시키는 반면, Swin-Free는 창을 이동시키지 않는다는 점에 유의하십시오.Swin-Free P = 56 ×M =N =P = 28 xM =N =Stage Swin P = 56 ×M =N =P = 28 xM =N =P = 14 ×M =N =P = 7 xP = 7 xM =N =M =N =P = 14 ×M =N =Swin-Free는 해당 단계에서 Swin보다 작습니다.그림은 또한 Swin-Free가 블록 수준에서 Swin과 어떻게 다른지 자세히 보여줍니다.Swin-Free에서 Swin Transformer에서 사용된 창 이동과 그 역연산이 제거되고 창의 크기가 각 단계마다 변경됩니다.4.2. 크기가 변하는 창 Swin Transformer에서 로컬 창을 이동하는 것은 창 간의 교차 연결을 달성하는 효과적인 방법이지만 메모리에서 데이터를 이동해야 합니다.이는 일반적으로 GPU에서의 수학 계산보다 비용이 많이 들기 때문에 모델 효율성에 부정적인 영향을 미칠 수 있습니다. 사실, 표 1에 표시된 대로 창을 이동하는 데 전체 런타임의 상당 부분이 사용됩니다. 이동된 창을 사용하지 않기 위해 각 단계에서 로컬 창의 크기를 변경하여 겹치지 않는 창 간의 교차 연결을 활성화합니다. M은 로컬 창의 크기입니다. 표 2에 표시된 대로 입력 크기 224×224에 대한 구현에서 M의 값을 4단계에 대해 M 7, 14, 14, 7로 변경합니다. 이 설정에서 2단계와 3단계에서 네 개의 인접한 7×7 로컬 창 간의 교차 연결을 고려합니다. 즉, 현재 단계의 14×로컬 창에는 이전 단계의 7×7 로컬 창 4개가 효과적으로 포함됩니다. 위의 변경 사항은 어텐션 블록에서 창 크기가 확대되어 단일 로컬 창의 GPU 계산 부하가 증가할 수 있습니다. 그러나 표 2에서 Swin-Free의 stagesand3에서 겹치지 않는 로컬 창의 수(즉, N)가 Swin의 1/4이 됩니다. 다시 말해, Swin-Free의 행렬 곱셈에서 행렬의 크기는 더 크지만 처리할 행렬의 수는 더 적습니다. GPU에서 7×7 로컬 창 4개를 처리하는 것에 비해 14×14 로컬 창을 처리하는 것이 지연 시간을 증가시키지 않고, 오히려 방대한 병렬 컴퓨팅 기능 덕분에 지연 시간을 감소시키는 것을 관찰했습니다. 이 점에 대해서는 섹션 5에서 더 자세히 논의합니다. 4.3. LayerNorm 및 GELU의 추가 최적화 대체: 그림 2에서 볼 수 있듯이 LayerNorm과 GELU는 여러 수학 계층으로 구성되어 일반적으로 사용되는 BatchNorm 및 ReLU 계층에 비해 더 많은 계산이 필요합니다. 표 1에서 LayerNorm과 GELU는 TensorRT로 실행할 때 Swin Transformer 모델의 총 런타임의 약 24%를 차지하는 것으로 관찰되었습니다. 따라서 애플리케이션에서 지연 시간도 중요한 경우 정확도가 크게 저하되지 않고 BatchNorm 및 ReLU로 대체합니다[23]. 섹션 5에서 이러한 수정을 통해 SwinFree가 정확도 측면에서 Swin Transformer를 능가하면서도 더 빠르게 실행할 수 있음을 알 수 있습니다.깊이 감소: 지연 시간을 우선시하는 또 다른 방법은 모델의 깊이를 줄이는 것입니다.특히, 3단계에서 Transformer 블록의 수를 줄이는 것을 고려합니다.예를 들어, 3단계가 Transformer 블록으로 구성된 Swin-B와 비교할 때 14개 블록만 사용하는 것을 고려할 수 있습니다.섹션 5에서 이 Swin-Free 변형이 지연 시간을 크게 개선하여 Swin Transformer보다 더 나은 정확도를 달성할 수 있음을 알 수 있습니다.5. 실험 실험의 초점은 분류 작업에서 지연 시간과 정확도 측면에서 Swin-Free를 Swin Transformer와 비교하는 것입니다.모든 지연 시간 결과는 NVIDIA RTX 3080, PyTorch 1.13 및 TensorRT 8.5와 CUDA 11.8을 사용하여 측정됩니다.평가는 1K 클래스와 입력 모양 224x224의 ImageNet 데이터 세트[24]로 수행됩니다. 우리는 표 3a에 표시된 Swin Transformer와 동일한 변형 모델을 고려합니다. Swin에서 사용되는 임베딩 차원 192의 Large(L) 변형은 더 이상 사용할 수 없는 22Kclass 데이터 세트의 fall11 버전이 필요하기 때문에 고려하지 않는다는 점에 유의하세요. Swin-B와 마찬가지로 모델 이름에 접미사를 추가하여 변형을 나타냅니다(예: Swin-Free-B). 또한 섹션 4.3에서 언급한 표 3b의 수정으로 인해 발생하는 다른 변형도 고려합니다. 이러한 추가 최적화는 모델의 대기 시간을 늘리지만 정확도가 떨어질 수 있습니다. 이러한 변형의 약어(예: BR 또는 DRx)도 모델 이름에 접미사로 추가됩니다. 5.1. Swin의 이동된 창 Swin-Free를 평가하기 전에 먼저 Swin-B의 각 단계에서 이동된 창의 중요성을 이해하고자 합니다. 표 4는 이동 창을 활성화하거나 비활성화한 단계에 따라 Swin-B의 상위 1 정확도를 보여줍니다. Case 1은 모든 단계에 이동된 창을 사용하므로 SwinB와 정확히 동일합니다. Case 8에서 이동된 창이 없으면 훈련을 성공적으로 완료하기 어렵고 따라서 이동된 창이 Swin에서 실제로 중요하다는 것을 먼저 알 수 있습니다. 또한 Case 4~7에서 단계 3이 이동된 창을 사용하는 데 중요한 단계임을 알 수 있습니다. 이는 단계 3이 Swin-B의 지배적인 부분이기 때문에 어느 정도 놀라운 일이 아닙니다. 그러나 Case 1~3에서 각 단계에 이동된 창을 선택적으로 사용하면 정확도를 높이는 데 약간 도움이 된다는 것을 알 수 있습니다. 따라서 Swin-B의 모든 단계에 적용하는 것이 중요합니다. 5.2. Swin-Free의 창 크기 이 섹션에서는 Swin-Free의 각 단계에 더 적합한 창 크기 구성을 보여줍니다. Swin과의 공정한 비교를 위해 입력 크기는 224×224이고 가장 작은 윈도우 크기는 7이라고 가정합니다. 이러한 이유로 1~3단계의 윈도우 크기는 7과 14의 두 가지 옵션만 있는 반면, 4단계는 항상 윈도우 크기가 7이어야 합니다. 이를 염두에 두고 표 5는 윈도우를 이동하지 않은 Swin-B에서 가질 수 있는 모든 가능한 구성에 대한 지연 시간과 정확도를 보여줍니다. 구성이 동일한 Swin-B의 경우 1이라는 점은 언급할 가치가 있습니다. 처음부터 학습한 Swin-B의 상위 1 정확도는 83.4%로 [1]에서 보고된 83.5%보다 약간 낮습니다. &#39;7, 7, 7, 7&#39;은 이동된 창이 없는 Swin-B와 동일하며, 이는 표 4의 사례 8과 동일합니다. 표 5의 사례 2~4에서 먼저 14를 창 크기로 사용하는 가장 효과적인 단계는 단계 3임을 알 수 있습니다. 단계 3에서 창 크기를 14로 늘리면 단계 1 또는 2에서 창 크기 14를 사용하는 것에 비해 대기 시간과 정확도가 가장 좋습니다. 이는 단계 3이 깊이 측면에서 Swin-B의 지배적인 부분이라는 사실에서 다시 비롯됩니다. 단계 3에서 14×14 로컬 창을 사용하면 단계 2에서 네 개의 인접한 7×로컬 창 간의 교차 연결을 고려합니다. 더 큰 창 크기를 사용하면 더 큰 커널 행렬 곱셈을 처리해야 하지만 이러한 행렬 곱셈의 수(즉, 겹치지 않는 창의 수)가 더 작아집니다(표 2 참조). Case 1과 2의 지연 결과를 비교하면 지연을 줄이는 데 도움이 됩니다. 14의 윈도우 크기를 사용하여 1단계 또는 2단계에서 동일한 지연 개선을 주장할 수 있지만 해당 단계가 깊이 2에 불과하다는 점을 고려하면 의미 있는 속도 향상을 관찰할 수 없습니다. Case 3과 4는 소수점 첫째 자리까지 Case 1과 동일한 지연을 얻습니다. Case 5~7에서는 동시에 두 단계에서 14×14 로컬 윈도우를 사용합니다. 3단계에서 14×로컬 윈도우를 사용하지 않으면 정확도와 지연이 모두 저하되어 3단계의 중요성을 다시 한 번 강조합니다. Case 5와 6에서도 3단계 외에도 1단계 또는 2단계에서 14×14 로컬 윈도우를 사용하면 Case 2보다 지연이 의미 있게 개선되어 가장 빠른 변형이 됨을 알 수 있습니다. Case 8을 살펴보면, stagesto 3에서 14의 윈도우 크기를 사용해도 Case 5나 6보다 지연 시간이 더 개선되지 않습니다. 정확도는 오히려 약간 떨어집니다. 그 이유는 크로스 윈도우 연결 모델링이 초기 단계에서는 효과적이지 않기 때문일 수 있습니다. 이 연구에서 우리는 정확도와 지연 시간 모두에서 가장 좋은 것 중 하나인 Case 5로 Swin-Free의 구성을 선택했습니다(표 2 참조). 5.3. Swin과 Swin-Free의 비교 표 6에는 처음부터 학습한 모든 Swin-Free 변형과 일부 Swin 패밀리가 나열되어 있습니다. 먼저 Cases와 6에서 Base(B) 변형에 대해 Swin-Free와 Swin을 비교할 수 있습니다. Swin-Free-B가 Swin-B보다 FLOPS와 매개변수가 더 많지만 PyTorch(12.ms 대 14.3ms) 또는 TensorRT(2.0ms 대 2.1ms)를 사용한 추론에서 Swin-Free-B가 Swin-B보다 빠르다는 것을 알 수 있습니다. 표 5의 연구에서 우리는 SwinFree-B가 각 창이 SwinFree-B에서 더 크지만 단계 2와 3에서 겹치지 않는 창의 수가 적기 때문에 이런 일이 발생한다는 것을 알 수 있습니다. 또한 Swin-Free-B가 Swin-B보다 더 나은 정확도를 달성한다는 것을 알 수 있습니다. 이는 이동된 창을 사용하지 않고도 특정 단계에서 로컬 창의 크기를 변경하면 교차 연결을 잘 모델링할 수 있음을 의미합니다.표 3. 모델 변형: (a) 주어진 아키텍처의 하이퍼 매개변수를 변경하여 변형을 고려합니다. (b) 주어진 모델에 아키텍처 수정을 적용합니다. 각 변형의 약어 기호는 접미사로 모델 이름에 추가됩니다. 변형 (a) 하이퍼 매개변수별 변형. 패치당 임베딩 차원 # 단계의 블록(깊이) 작음(T) 작음(S){2,2,6,2} {2,2,18,2}{2,2,18,2} 기본(B) 변형 BatchNorm/ReLU(BR) x로의 깊이 감소(DRx) (b) 수정에 의한 변형.수정 LayerNorm을 BatchNorm으로, GELU를 ReLU로 교체합니다.단계 3에서 Transformer 블록의 수를 x로 줄입니다.표 4. 각 단계에서 Swin-B의 이동 창 켜기/끄기: 1은 &#39;켜짐&#39;을 의미합니다.예를 들어, &#39;1, 1, 1, 1&#39;은 모든 단계에서 이동된 창을 사용함을 의미하며, 이는 정확히 Swin-B를 의미합니다.기호 &#39;-&#39;는 학습을 성공적으로 완료할 수 없음(즉, 발산)을 의미합니다. 영어: Case On/off on cyclic shift Top-1 accuracy (%)1, 1, 1,83.0, 1, 1,82.0, 0, 1,82.0, 0, 0,0, 0, 1,82.0, 1, 0,1, 0, 0,0, 0, 0, 이웃 창 사이에서. 일관되게, Case 5와 6의 Swin-Free-T와 Swin-Free-S도 Swin의 해당 변형(여기서는 표시하지 않음, [1] 참조)보다 더 나은 정확도를 달성합니다. 또한 입력 크기가 224×224인 경우 SwinV2-B [9]가 Swin-Free-B와 동일한 정확도를 얻지만 대기 시간이 훨씬 더 느리다는 것을 관찰했습니다. 따라서 대기 시간이 중요한 애플리케이션의 경우 Swin-Free가 SwinV2보다 더 나은 선택이 될 것입니다. 5.4. BatchNorm/ReLU(BR) 변형 Swin-Free의 LayerNorm과 GELU를 각각 BatchNorm과 ReLU로 바꾸면 표 6의 사례 7~9의 변형을 얻습니다. 먼저 이러한 대체에서 발생하는 정확도 저하가 사소하다는 것을 알 수 있습니다. 즉, Swin-Free-B-BR만 Swin-Free-B보다 정확도가 약간 낮은 반면 다른 모델은 해당 모델과 동일한 정확도를 유지합니다. 지연 시간과 관련하여 BR 변형은 Pytorch에서는 그렇지 않지만 TensorRT에서 의미 있는 속도 향상을 달성합니다. 그럼에도 불구하고 TensorRT가 딥 러닝 모델을 배포하는 데 있어 사실상의 표준이라는 점을 고려할 때 BR 변형은 지연 시간이 중요한 애플리케이션의 경우 좋은 대안이 될 것입니다. Case에서 원래 SwinB에 BR 수정을 적용하는 것만으로는 Swin-Free-B-BR과 유사한 정확도나 지연 시간을 얻을 수 없다는 점도 주목할 가치가 있습니다. 5.5. 깊이 감소(DRx) 변형 표 6의 사례 10~13은 SwinFree-B의 DRx 변형을 보여줍니다. 말할 것도 없이 Swin-Free-B의 D10, D12, D14 및 D16 변형은 FLOP과 매개변수 수를 줄여 Swin-Free-B의 지연 시간을 개선합니다. 사례 11에서 Swin-Free-B-DR12는 Swin-B보다 FLOPS가 더 낮고 TensorRT 런타임이 Swin-Free-B와 비교했을 때 2ms에서 1.5ms로 줄었습니다. 정확도와 관련하여 Swin-Free-B와 동일하게 유지되는 것을 볼 수 있습니다. 이는 크기가 변하는 창을 사용하면 3단계에서 Swin의 그렇게 깊은 깊이가 필요하지 않을 수 있음을 의미합니다. 사례 14~16에서 BR과 DRx를 조합하면 지연 시간을 더욱 개선하는 동시에 Swin-B에 비해 정확도가 여전히 우수할 수 있음을 알 수 있습니다. 예를 들어, Swin-Free-B-BR-DR14는 정확도가 83.7%이고 대기 시간이 1.4ms인 반면 Swin-B는 각각 83.4%와 2.1ms입니다. 사례 1과 14에서 정확도를 약간 희생(83.4%에서 83.3%)함으로써 Swin-Free-B-BRDR12는 대기 시간을 상당히 줄일 수 있습니다(2ms에서 1.3ms로, Swin-B에서 약 38% 감소). 이러한 종류의 Swin-Free 변형은 대기 시간이 정확도보다 더 중요한 상황에서 Swin에 대한 매력적인 대안이 될 수 있습니다. 6.
--- CONCLUSION ---
이 논문에서는 쉬프트 윈도우 방식으로 인해 발생하는 메모리 트래픽을 줄임으로써 Swin Transformer보다 지연 시간을 개선하려는 Swin-Free를 제시합니다. 대신 SwinFree는 쉬프트 윈도우의 메커니즘을 모방하여 단계별로 윈도우 크기를 변경합니다. 이 간단한 기술은 Swin 대응 제품에 비해 지연 시간을 줄이고 정확도를 높이는 것으로 나타났습니다. 또한 순환 쉬프트를 사용하지 않고 fur-표 5. Swin-B의 각 단계에서 윈도우 크기 변화에 따른 지연 시간과 정확도. 예를 들어, &#39;7, 7, 14, 7&#39;은 3단계가 윈도우 크기로 14를 사용하는 반면 1, 2, 4단계는 7을 사용한다는 것을 의미합니다. 기호 &#39;-&#39;는 학습이 성공적으로 완료되지 못했음을 의미합니다(즉, 발산). 단계 Top-1 정확도(%)에서의 Case Window 크기 PyTorch(FP32)에서의 지연 시간(ms)7,7,7,13.7, 7, 14,83.12.7, 14, 7,81.13.14,7,7,81.13.7, 14, 14,83.12.14, 7, 14,83.12.14, 14, 7,81.13.14, 14, 14,83.12.표 6. 처음부터 ImageNet-1K로 학습한 모델. FLOP 및 매개변수 개수는 [25]에서 측정합니다. SwinV2는 이 도구와 함께 작동하지 않았으므로 여기서는 &#39;-&#39;로 표시했습니다. 케이스 모델 FLOPS 매개변수 수 상위 1 정확도(%) 지연 시간(ms) TensorRT(FP16) PyTorch(FP32)Swin-B 15.9G 88.7M 83.2.14.Swin-B-BR 15.6G 88.7M 83.1.15.SwinV2-B 83.3.21.Swin-Free-B 16.8G 99.4M 83.2.12.Swin-Free-T 5.0G 31.6M 82.0.6.Swin-Free-S 9.7G 58.3M 83.1.12.Swin-Free-T-BR 4.8G 31.6M 82.0.7.Swin-Free-S-BR 9.5G 58.3M 83.1.13.스윈-프리-B-BR 16.4G 99.4M 83.1.13.스윈-프리-B-DR11.3G 69.3M 83.1.9.스윈-프리-B-DR12.7G 76.8M 83.1.9.스윈-프리-B-DR14.0G 84.4M 83.1.10.스윈-프리-B-DR15.4G 91.9M 83.1.11.스윈-프리-B-BR-DR12 12.4G 76.9M 83.1.10.15 스윈-프리-B-BR-DR14 13.7G 16 스윈-프리-B-BR-DR84.4M 83.1.11.15.1G 91.9M 83.1.12. 정확도 손실 없이 더 간단한 연산과 더 얕은 블록을 사용하면 더 빠른 속도를 얻을 수 있습니다. 따라서 제안된 모델은 효율성이 향상된 프로덕션 배포에 특히 적합합니다. 향후 작업에서는 더 큰 입력 해상도를 사용하여 객체 감지 및 의미 분할과 같은 다른 비전 작업에 Swin-Free를 적용할 계획입니다. 추론을 위한 GPU 활용도를 더욱 개선하기 위해 다양한 단계에 걸친 동적 창 크기와 같은 추가 최적화도 조사할 것입니다. 참고문헌 [1] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin transformer: shifted windows를 사용하는 계층적 비전 변환기. IEEE/CVF International Conference on Computer Vision(ICCV) 회의록, 10012-10022페이지, 2021년 10월. 1, 2, 3, 5,[2] Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton. 심층 합성곱 신경망을 사용한 Imagenet 분류. F. Pereira, CJ Burges, L. Bottou, KQ Weinberger 편집, 신경 정보 처리 시스템의 발전, 25권. Curran Associates, Inc., 2012. 1,[3] Karen Simonyan, Andrew Zisserman. 대규모 이미지 인식을 위한 매우 심층적인 합성곱 신경망. CORR, abs/1409.1556, 2014. 1,[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. 합성곱을 더 심화시키기. CoRR, abs/1409.4842, 2014. 1,[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 이미지 인식을 위한 심층적 잔여 학습. CoRR, abs/1512.03385, 2015. 1,[6] Song Han, Huizi Mao, William J. Dally. 심층 압축: 가지치기, 훈련된 양자화 및 허프만 코딩을 사용한 심층 신경망 압축. Yoshua Bengio와 Yann LeCun 편집자, 제4회 학습 표현 국제 컨퍼런스, ICLR 2016, 푸에르토리코 산후안, 2016년 5월 2-4일, 컨퍼런스 트랙 회의록, 2016. 1,[7] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. Mobilenets: 모바일 비전 애플리케이션을 위한 효율적인 합성곱 신경망. CoRR, abs/1704.04861, 2017. 1,[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기. 국제 학습 표현 컨퍼런스, 2021. 1, 2,[9] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. Swin transformer V2: 용량 및 해상도 확장. CoRR, abs/2111.09883, 2021. 1, 2,[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: 언어 이해를 위한 딥 양방향 변압기의 사전 학습. CoRR, abs/1810.04805, 2018. 1,[11] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 생성적 사전 학습을 통한 언어 이해 향상. 2018. 1,[12] Tom B. Brown et al. 언어 모델은 few-shot 학습기입니다. CORR, abs/2005.14165, 2020. 1,[13] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. SegFormer: 변환기를 사용한 의미론적 분할을 위한 간단하고 효율적인 설계. Neural Information Processing Systems 사전 회의록(NeurIPS), 2021.[14] Sepp Hochreiter 및 Jürgen Schmidhuber. 장단기 기억. Neural Computation, 9(8):1735-1780, 1997.[15] Aakanksha Chowdhery et al. Palm: 경로로 언어 모델링 확장, 2022.[16] NVIDIA TensorRT. https://developer.nvidia.com/tensorrt.[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 5998-6008페이지, 2017.[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. CoRR, abs/1910.10683, 2019.[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, Ruslan Salakhutdinov. Transformer-xl: 고정 길이 컨텍스트를 넘어서는 주의 깊은 언어 모델. CORR, abs/1901.02860, 2019.[20] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. 변환기를 사용한 엔드투엔드 객체 감지. CORR, ABS/2005.12872, 2020.[21] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr 및 Li Zhang. 변환기를 사용하여 시퀀스 간 관점에서 의미론적 분할을 다시 생각합니다. CORR, ABS/2012.15840, 2020.[22] Junjie Bai, Fang Lu, Ke Zhang 등. ONNX: 개방형 신경망 교환. https://github.com/onnx/onnx, 2019.[23] 존 양(John Yang), 레안(Le An), 아누락 딕시트(Anurag Dixit), 구진규(Jinku Koo), 수 인 파크(Su Inn Park). 단순화된 변환기를 사용한 깊이 추정, 2022. 3,[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스. 2009년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 248-255페이지, 2009.[25] ThanatosShinji. onnx-tool. https://github.com/ Thanatos Shinji/onnx-tool, 2023.
