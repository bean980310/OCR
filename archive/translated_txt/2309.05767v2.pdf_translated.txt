--- ABSTRACT ---
오디오-언어 모델은 Zero-Shot 추론을 가능하게 하는 멀티모달 텍스트와 오디오 표현을 공동으로 학습합니다. 모델은 인코더에 의존하여 입력의 강력한 표현을 만들고 소리, 음악, 음성에 이르기까지 다양한 작업으로 일반화합니다. 모델은 놀라운 성능을 달성했지만 작업별 모델과는 여전히 차이가 있습니다. 이 논문에서는 Zero-Shot 추론을 위한 두 개의 혁신적인 인코더를 사용하여 다양한 4.6M 오디오-텍스트 쌍으로 사전 학습된 Contrastive Language-Audio Pretraining 모델을 제안합니다. 오디오 표현을 학습하기 위해 표준적인 사운드 이벤트 분류 학습 대신 22개의 오디오 작업에서 오디오 인코더를 학습했습니다. 언어 표현을 학습하기 위해 표준 인코더 전용 모델 대신 자기 회귀 디코더 전용 모델을 학습했습니다. 그런 다음 오디오 및 언어 표현은 Contrastive Learning을 사용하여 공동 멀티모달 공간으로 가져옵니다. 인코더를 사용하여 다운스트림 성능을 크게 개선했습니다. 우리는 문헌에서 가장 큰 규모인 26개의 다운스트림 작업에 대한 표현의 일반화를 광범위하게 평가했습니다. 우리 모델은 여러 작업에서 최첨단 결과를 달성하여 4개의 다른 모델을 능가하고 범용 오디오 표현을 향한 길을 선도합니다. 코드는 GitHub¹에 있습니다. 색인 용어 대조 학습, 범용 오디오 표현, 제로샷, 언어, 소리 1.
--- METHOD ---
s는 주어진 다운스트림 작업에서 표현을 사용하기 위해 추가적인 미세 조정을 거쳐야 합니다. Zero-Shot 모델은 모든 작업에 적용하여 유연성과 일반화를 직접 달성할 수 있습니다. 가장 성공적인 유형 중 하나는 멀티모달 텍스트와 오디오 표현을 공동으로 학습하는 Contrastive Language-Audio Pretraining(CLAP) 모델입니다. *Equal Contribution https://github.com/microsoft/CLAP. [3]의 저자는 16개의 다운스트림 작업에서 최첨단(SOTA)을 달성한 CLAP 모델을 소개했습니다. 이후의 문헌에서는 오디오 및 텍스트 인코더를 선택하는 것이 강력한 표현을 생성하고 작업 전반에서 성능을 높이는 데 중요하다는 것을 보여주었습니다. [4, 5, 6, 7]. 예를 들어, CNN에서 오디오 변환기(HTSAT)로 업그레이드하여 오디오를 인코딩하고 BERT에서 ROBERTa로 업그레이드하여 텍스트를 인코딩합니다. 또 다른 결론은 학습 쌍의 수를 늘리면 전반적인 성능이 향상된다는 것입니다. 그러나 단순히 쌍을 추가하면 특정 도메인 및 작업에서 성능이 떨어질 수 있습니다. [4, 5, 3, 6]. CLAP의 성능은 텍스트와 오디오 훈련 쌍의 다양성과 그것들이 얼마나 노이즈가 많은지에 따라 달라집니다.Wav2clip[8]과 Audioclip[9]은 사운드 이벤트에 대한 주석이 달린 데이터 세트인 AudioSet에서 각각 200k와 1.7M 오디오-텍스트 쌍을 사용했습니다.저자들은 문장 수준의 설명이 아닌 클래스 레이블과 오디오를 페어링하여 설명의 맥락과 언어 의미를 놓칠 가능성이 있지만 각각 3개의 작업에서 좋은 Zero-Shot 성능을 보였습니다.CLAP[3]은 128k 쌍을 사용했지만 텍스트는 오디오 캡션과 웹 소스 데이터 세트에서 나온 설명이었습니다.16개 작업에서 평가되었고 이전 버전보다 상당히 개선되었습니다.LAION CLAP[4]은 2.5M 쌍의 컬렉션을 사용하여 작업에서 성능을 더욱 개선했습니다.저자들은 나중에 음악과 음성 관련 훈련 쌍을 추가했지만 사운드 이벤트 분류(ESC50)의 성능은 절대적으로 1%만큼 저하되었습니다. Wavcaps[6]는 500k 쌍을 사용했지만 ChatGPT 언어 모델을 사용하여 노이즈가 많은 웹 소스 설명을 정리했습니다. 결과는 8개 작업에서 문헌보다 우수했습니다. 따라서 쌍을 확장할 때 다양한 도메인과 작업에서 일반화를 평가하여 성능 상충 관계를 확인하는 것이 필수적입니다. 이 논문에서 우리는 다음과 같은 기여를 합니다. 오디오 표현을 학습하기 위해 22개 오디오 작업에서 오디오 인코더를 훈련했습니다. 언어 표현을 학습하기 위해 자기 회귀 디코더 전용 모델을 훈련했습니다. 우리는 전례 없는 460만 개의 오디오-텍스트 쌍으로 CLAP 모델을 사전 훈련하고 문헌에서 가장 큰 26개 다운스트림 작업에서 표현의 일반화를 광범위하게 평가하여 여러 작업에서 SoTA 결과를 달성했습니다. 2. 방법 대조적 언어-오디오 사전 훈련(그림 1)은 오디오와 텍스트 인코더를 공동으로 훈련하여 다양한 유형의 추론에 사용할 수 있는 다중 모달 표현을 학습합니다. 영어: Contrast Pretraining Water Draining Text Encoder Text audio pairs T₁ Classes Dog barking Zero-Shot Classification Rain drops Tw Text Encoder A₁ A₁T, A₁₂ A₁T- AT Siren wooling … Audio Encoder Az AAA Ax Art Art Axt Text to Audio Retrieval Text Query A crow crying Text int the forest Encoder Audio Database AN ANTI ANT ANTS ANTN Testing audio Audio Encoder AAT, AT A₁T Audio Captioning Dog barking A campfire A tarsaches A₁ AT &gt;&gt;&gt;&gt; Audio Encoder Testing audio Az A₂T → Audio Encoder Fig. 1: CLAP A As AN ANT Mapper GPT-Mapping 오디오 임베딩을 GPT2 입력 시퀀스에 매핑하여 멀티모달 공간에서 비교할 수 있는 오디오 및 텍스트 임베딩을 학습합니다. 사전 학습된 인코더는 Zero-Shot 분류, 텍스트-오디오 및 오디오-텍스트 검색, 오디오 캡션에 사용할 수 있습니다.2.1. 대조적 언어-오디오 사전 학습 처리된 오디오를 Xa st Xa Є RF×T라고 하자.여기서 F는 스펙트럼 성분(예: Mel 빈)의 수이고 T는 시간 빈의 수이다.텍스트를 Xt로 나타내자.N개의 배치에 있는 각 오디오-텍스트 쌍은 {Xa, Xti로 표현되며, 여기서 i Є [0, N]이다.편의상 i 표기법을 삭제하고, 앞으로 {Xa, Xt}는 N개의 배치를 나타낸다.쌍에서 오디오와 텍스트는 각각 오디오 인코더 fa(.)와 텍스트 인코더 ft(.)로 전달된다.N개의 배치에 대해: Ŷa = fa(Xa); Ŷt = ft(Xt) (1) 여기서 Ŷ a Є R³×V는 차원 V의 오디오 표현이고 Âµ Є R³×ʊ는 차원 U의 텍스트 표현입니다. 학습 가능한 투영 계층을 사용하여 오디오 및 텍스트 표현 Ŷa와 Ŷt를 차원 d의 조인트 멀티모달 공간으로 가져왔습니다. Ea = La(Xa); Et = Lt(Xt) (2) 여기서 Ea Є Rxd, Et Є R³×d, La 및 Lɩ는 각각 오디오 및 텍스트에 대한 투영입니다. 이제 오디오 및 텍스트 임베딩(Ea, Et)이 비교 가능하므로 유사도를 측정할 수 있습니다. C = T (Et · ET) .NXN (3) 여기서 7은 로짓 범위를 조정하는 온도 매개변수입니다. 유사도 행렬 CERN에는 대각선에 N개의 일치하는 쌍이 있고 대각선 외의 N² – N개의 일치하지 않는 쌍이 있습니다. L = 0.5(ltext(C) + laudio(C)) = A(4) 여기서 lk log diag(softmax(C))는 각각 텍스트와 오디오 축을 따라 계산됩니다. 유사도 행렬에 대한 이 대칭적 교차 엔트로피 손실(L)을 사용하여 오디오 및 텍스트 인코더와 해당 투영 계층을 공동으로 학습했습니다. 2.2. 오디오 및 텍스트 인코더 오디오 인코더: 오디오를 처리하기 위해 이 논문[11]과 유사한 방법을 사용하여 22개 오디오 작업에서 트랜스포머 기반 오디오 인코더(HTSAT[10])를 학습했습니다. 이를 HTSAT-22라고 명명했습니다. 여러 오디오 작업에서 학습된 인코더가 일반화를 개선하고 따라서 작업 전반에서 성능을 향상시킬 것이라는 가설을 세웠습니다. 이 방법은 오디오 인코더와 매퍼 네트워크를 학습하여 대규모 언어 모델이 분류, 캡션, 검색 및 오디오 Q&amp;A와 같은 여러 오디오 작업을 수행하도록 합니다. 아키텍처는 본질적으로 캡션 시스템으로 훈련되며, 오디오 프롬프트 p²에 따라 자기 회귀 방식으로 자유형 텍스트 출력 c²를 생성하는 방법을 학습합니다. y는 모델의 훈련 가능한 매개변수를 나타냅니다. 손실 함수는 교차 엔트로피입니다. NL= log p(cpi, P2, C, 1) (5) i=1 j=2k&#39; 텍스트 인코더: 텍스트를 처리하기 위해 텍스트 작업에 인상적인 능력을 보여준 자기 회귀 모델인 GPT2(124M 기반)를 채택했습니다. 우리는 과제를 해결했습니다. 자기 회귀 모델이 문장 수준 표현을 생성하도록 만드는 방법은 무엇일까요? 변환기-디코더 블록으로 구축된 자기 회귀 모델은 입력 텍스트를 받고 가장 가능성 있는 단어(토큰) 시퀀스를 하나씩 출력합니다. 반면에 변환기-인코더 블록(BERT 또는 ROBERTA)으로 구축된 모델은 연속 공간에서 문장 수준 표현을 출력합니다. GPT2가 문장 수준 표현을 출력하도록 하기 위해 특수 토큰|endoftext| &gt; 각 입력 텍스트의 끝에. 대조적 사전 학습 동안 이 토큰의 표현을 문장 수준 표현으로 사용합니다. 이렇게 하면 토큰에 텍스트 입력의 집계 정보가 포함됩니다. 2.3. 평가 Zero-Shot 추론: CLAP의 오디오와 텍스트 간의 유사성을 결정하는 기능을 사용했습니다. C 클래스 레이블과 N 테스트 오디오가 있는 대상 데이터 세트를 고려해 보겠습니다. 먼저 사전 학습된 인코더를 사용하여 N 오디오와 C 클래스에 대한 CLAP의 오디오 및 텍스트 임베딩을 계산합니다. 둘째, 각 테스트 오디오와 모든 클래스 레이블 간의 코사인 유사성을 계산합니다. 검색의 경우 텍스트 쿼리를 클래스로 취급합니다. 각 테스트 오디오는 클래스만큼 많은 로짓을 갖습니다. 셋째, 로짓은 이진 또는 다중 클래스 분류의 경우 소프트맥스를 적용하여 확률 분포로 변환되고, 다중 레이블 분류의 경우 시그모이드를 적용하고, 검색을 위해 변경하지 않습니다. 오디오 캡션: 그림 1의 아키텍처에서 테스트 오디오는 사전 훈련된 오디오 인코더로 전달된 다음 매퍼 네트워크로 전달되고, 그런 다음 GPT2로 전달되어 설명을 생성합니다. 훈련 시간에는 캡션 손실(Eq.5)과 훈련 분할을 사용하여 매퍼 네트워크의 가중치만 학습됩니다. Zero-Shot Score ↑ 사운드 이벤트 분류 ↑ 보컬 사운드 분류 ↑ 감시 사운드 분류 ↑ 액션 분류 ↑ 음향 장면 분류 ↑ DCASEVocal ESCModel 평균 ESCFSD50K US8K SESA TUTTaskSound 액션 CNN14+BERT 0.0.0.0.0.0.0.0.0.0.HTSAT+CLIP 0.0.0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.0.0.0.HTSAT-22+ROBERTa 0.0.0.0.0.0.0.0.0.0.0.HTSAT-22+CLIP 0.0.0.0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.0.0.0.0.0.0.음악 분류 ↑ 악기 분류 ↑ 음성 감정 KWS↑ 화자 분류↑ 계산↑ GTZAN GTZAN 모델 베이징 NS Instr. 음악 음성 장르 오페라 계열 CRE RAV MA-D DESS 음성 명령 LibriCountCNN14+BERT HTSAT+CLIP0.0.0.0.178 0.0.0.0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.0.HTSAT-22+ROBERTa0.0.0.0.0.0.0.0.HTSAT-22+CLIP0.0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.0.297 0.0.0.표 1: 16개 다운스트림 작업과 119k 트레이닝 쌍에 대한 Zero-Shot 성능. 제안하는 인코더(HTSAT-22+GPT2)는 문헌에서 가장 우수한 조합보다 성능이 우수했습니다. 모든 숫자에 대해 높을수록 좋습니다. 메트릭은 FSD50k 및 ESC50-actions의 mAP, DCASE17의 F1-score, 나머지는 모두 Accuracy를 사용합니다. Zero-Shot 점수는 메트릭의 평균입니다. 이것은 16가지 작업을 가진 문헌에서 인코더를 비교한 첫 번째 사례이며, 일반적으로 몇 개의 인코더와 소수의 작업만 고려합니다. 3.
--- EXPERIMENT ---
S 훈련 데이터 세트. 쌍을 수집하는 것은 아마도 CLAP 모델을 확장하는 데 있어서 가장 큰 병목 현상일 것입니다. 우리는 다양한 데이터 세트와 웹 아카이브에서 460만 개의 오디오 및 텍스트 쌍으로 가장 큰 컬렉션을 수집했습니다. 오디오는 인간의 소리와 활동, 환경 소리, 음향 장면, 음악, 음향 효과 및 음성 감정을 설명합니다. 표 1의 인코더 효과를 연구하기 위해 CLAP [3]과 동일한 훈련 세트를 사용했습니다. 저자와 달리 AudioCaps 및 Clotho의 테스트 세트를 포함하지 않았으므로 쌍의 수는 128k가 아닌 119k였습니다. 4.6M 컬렉션의 교육 데이터 세트는 다음과 같습니다.WavCaps[6], AudioSet[2], FSD50K[12], Clotho[13], AudioCaps[14], MACS[15], WavText5k[5], SoundDesc[16], NSynth[17], FMA[18], Mosi[19], Meld[20], Iemocap[21], Mosei[22], MSP-Podcast[23], CochlScene[24], LJspeech[25], EpicKitchen[26], Kinectics700[27], findsounds.com. 자세한 내용은 GitHub에서 확인하세요. 다운스트림 작업. 다양한 도메인에서 26개의 다운스트림 작업을 사용했으며, 그 중 몇몇은 HEAR[1]에서 가져왔습니다. 사운드 이벤트, 음성 사운드, 감시 사운드 및 음향 장면 분류, 오디오 캡션, 검색; 음악, 악기 및 음표 속성 분류; 음성 감정 및 언어 분류; 키워드 스포팅; 화자 계산. 표 1의 인코더 효과를 연구하기 위해 16개 작업의 하위 집합을 사용했습니다. 전처리. 샘플링 속도가 44.1KHz, 홉 크기가 320프레임, 윈도우 크기가 1024프레임, 50-8000Hz 범위의 64개 Mel 빈을 갖는 오디오의 로그 Mel 스펙트로그램 표현을 사용했습니다. 학습하는 동안 각 오디오 클립은 7초의 연속 세그먼트로 무작위로 잘리거나 더 짧으면 패딩됩니다. 쌍이 있는 배치는 무작위로 샘플링됩니다. 인코더. 제안된 CLAP 모델의 경우 Sec.2.2에 설명된 오디오 및 텍스트 인코더 HTSAT-22+GPT2를 사용했습니다. 비교를 위해 표 1에서 문헌에서 가장 좋은 두 가지 인코더 조합인 CNN14+BERT와 HTSAT+ROBERTa[3, 4, 6]를 사용했습니다. 또한 여러 저자가 사용했기 때문에 CLIP의 텍스트 인코더도 포함했습니다[9, 8, 4]. 오디오와 텍스트 임베딩은 모두 1024의 출력 차원을 가진 독립적인 학습 가능한 투영 레이어가 있는 멀티모달 공간에 투영됩니다. 학습. 두 인코더의 포에포크를 모두 동결 해제하여 학습했지만 전체 성능은 처음 10개 에포크에서 최고조에 달했습니다. 가장 좋은 ZeroShot 점수(모든 작업의 평균)를 산출한 에포크에 해당하는 다운스트림 작업의 성능을 보고합니다. 이러한 에포크에 해당하는 모델이 보이지 않는 데이터 세트에 더 잘 일반화되고 커뮤니티에 더 잘 봉사할 것이라고 가정합니다. 각 작업의 성능이 다른 에포크에서 더 높거나 낮았을 수 있습니다. 배치 크기는 1,536이었습니다. 초기 학습 속도가 10-3인 Adam Optimiser를 사용했고, 인내심이 15인 플래토에서 학습 속도를 10-1만큼 줄였습니다. 온도 매개변수 7은 학습 가능하며 0.007로 초기화되었습니다. 4. 결과 및 논의 다양한 오디오 및 텍스트 인코더를 비교한 결과는 표 1에 있고 제안하는 CLAP의 결과는 표 2에 있습니다.4.1. 제안된 오디오 및 텍스트 인코더 제안하는 인코더 HTSAT-22+GPT2는 표 1에 표시된 대로 문헌에 나와 있는 두 가지 최고의 인코더 조합보다 성능이 우수했습니다.전체 성능을 비교하기 위해 모든 16개 작업의 메트릭 평균인 Zero-Shot 점수를 사용했습니다.HTSAT-22+GPT2는 0.480을 달성했으며, 가장 일반적인 조합인 HTSAT+ROBERTa 및 CNN14+BERT(각각 0.431 및 0.)보다 절대적으로 9% 더 높았습니다.모든 인코더 조합이 더 나은 성능을 보였습니다.모델 벤치마크 HTSAT-22+GPTESC[28] 0.948 [6] 0.FSD50K [12] 0.302 [3] 사운드 이벤트 분류 ↑ US8K 0.[29] 0.806 [6] 0.DCASETask 4 [30] 0.3 [3] 0.AudioSet [2] 0.058 [3] 0.Vocal Sound Classification ↑ Vocal Surveillance Sound Classif.↑ Action Classification↑ Acoustic Scene Classification↑ SESA Sound [31] [32] ESCActions [33] 0.495 [3] 0.0.0.TUT[30] 0.296 [3] 0.0.0.Speech Emotion Music Classification ↑ Instrument Classification ↑ KWS↑ Classification↑ GTZAN GTZAN 모델 Music Speech [1] 장르 NS 피치 NS 속도 NS 품질 베이징 NS 악기 CRE RAV Opera family MA-D DESS [1] [17] [17] [17] [1] [17] [1] [34] 음성 명령 [1] 스피커 계산↑ Libri Count[1] 벤치마크 1 [3] 0.25 [3] 0.0.HTSAT-22+GPT0.0.0.0.0.0.0.0.4746 [3] 0.0.0.0.178 [3] 0.0.159 [3] 0.106 [3] 0.178 [3] 0.0.0.Audio Captioning ↑ 모델 벤치마크 HTSAT-22+GPTAudioCaps Clotho [14] [13] 0.438[35] 0.215[35] 0.0.AudioCaps R@0.517[6] 0.Audio-Text 검색 ↑ AudioCaps Clotho MAP@10 R@0.457 [4] 0.234[6] 0.0.텍스트-오디오 검색 ↑ Clotho MAP@0.138[4] 0.AudioCaps R@0.397[6] 0.AudioCaps Clotho Clotho MAP@10 R@1 MAP@0.51[4] 0.195[6] 0.204[4] 0.0.0.표 2: 제안한 인코더와 4.6M개의 학습 쌍을 사용하여 26개의 다운스트림 작업에서 수행한 성능. 벤치마크로서 문헌에서 가장 좋은 숫자를 사용했으며, 숫자를 사용할 수 없는 경우에는 무작위 성능을 사용했습니다. 모든 작업에서 높을수록 좋습니다. 평가 지표는 FSD50k, ESC50-Actions, AudioSet 및 NS Qualities의 경우 MAP이고, DCASE17의 경우 F1-score입니다. 및 캡션을 위한 SPIDER; 다른 모든 것은 정확도를 사용합니다. 무작위보다. 다른 조합이 다른 작업에서 더 나은 성과를 거두었지만, 어느 것도 특정 도메인에서 뛰어나지 않았습니다. 저희의 HTSAT-22 오디오 인코더는 성능 향상에 가장 크게 기여합니다. HTSAT-22는 오디오 작업에서 사전 학습된 반면, HTSAT는 사운드 이벤트 분류에서만 사전 학습되었습니다. 따라서 여러 오디오 작업에서 사전 학습을 생성하면 오디오 인코더의 표현을 개선할 수 있음을 시사합니다. HTSAT22+GPT2를 HTSAT+GPT2와 비교한 결과 LibriCount10(절대 10%), NS Instrument(절대 7%), ESC50(절대 6%)과 같은 주요 개선 사항이 입증되었습니다. 제안된 GPT2 자기 회귀 모델은 인기 있는 ROBERTa를 개선합니다. GPT2를 HTSAT 또는 HTSAT-22와 함께 사용하면 다른 텍스트 인코더보다 최상의 성능을 얻을 수 있었습니다. 저희는 이러한 개선 사항이 두 가지 이유에서 비롯된다고 가정합니다. 첫째, GPT2는 30k인 BERT 및 ROBERTa에 비해 50k 토큰의 더 큰 어휘를 가지고 있습니다.둘째, 수정된 GPT2 자기 회귀는 문장 수준 표현에 사용되는 &lt;|endoftext&gt;까지의 토큰을 예측합니다.이는 자기 감독 역할을 하며 모델이 단어 순서를 학습하고 강조하도록 합니다.4.2. 제안된 CLAP 아키텍처 확장 CLAP 모델은 표 2에 표시된 대로 26개의 다운스트림 작업 대부분에서 새로운 Zero-Shot SoTA를 확립하여 4가지 다른 SoTA 모델보다 우수한 성과를 보였습니다.모델을 벤치마킹하기 위해 다양한 모델에서 나온 문헌에서 가장 좋은 숫자를 사용했습니다.숫자가 없는 경우에는 무작위 성능을 사용했습니다.어떤 경우에는 성능 개선이 벤치마크 문헌의 두 배 이상입니다.몇 가지 주요 내용은 음악 장르가 58.4% 정확도 대 25%, 보컬 사운드가 80% 정확도 대 49.5%, 어쿠스틱 장면이 53.8% 정확도 대 29.6%입니다. 일부 다운스트림 작업은 훈련 세트의 오디오 파일이 4.6M 쌍의 일부이기 때문에 진정한 ZeroShot 설정을 구성하지 않습니다(3절 참조). 예를 들어, FSD50k 오디오 및 웹 설명은 훈련에 사용되었지만 클래스 레이블은 사용되지 않았습니다. 우리는 어떤 다운스트림 작업에 대해서도 CLAP 인코더를 미세 조정하지 않았습니다. 우리는 ESC50에 대한 오디오 인코더만 미세 조정했고 이전 CLAP 모델의 성능을 96.70%에서 98.25% 정확도로 개선하여 새로운 SoTA를 확립했습니다. 4.3. 일반화 및 개별 도메인 성능 훈련에서 다양성을 추가하고 오디오-텍스트 쌍을 확장하면 일부 작업에서는 성능이 향상되지만 다른 작업에서는 성능이 저하되는 상충 관계가 발생합니다. 예상대로 주어진 작업의 도메인과 유사한 훈련 쌍을 추가하면 도움이 되므로 다양성은 일반화에 필수적입니다. 예를 들어, CLAP[3]에는 감정 인식 훈련 쌍이 포함되지 않았으며 RAVDESS에서 17.1% 정확도, CREMAD에서 23.4% 정확도를 달성했습니다. 감정 관련 쌍을 추가하고 정확도를 각각 31.5%와 30%로 개선했습니다. 그럼에도 불구하고 더 많은 쌍은 분포 이동을 일으켜 훈련 데이터와 일부 테스트 데이터 간에 불일치를 생성할 수 있습니다. 예를 들어, 저희 모델은 ESC에서 500k 쌍으로 훈련된 모델[6]보다 약간 낮은 점수를 얻었습니다(94.8% 대 93.9% 정확도). 또 다른 예는 GTZAN Music vs Speech에서 128k 쌍을 사용한 모델[3]이 99.2%를 기록한 저희 모델보다 100% 정확도를 달성했습니다. Table의 저희 모델조차 119k 쌍으로 100% 정확도를 달성했습니다. 훈련 쌍을 추가함에 따라 작업 간 성능이 달라질 것으로 예상해야 합니다. 따라서 제로샷 모델은 특정 작업에 대한 과적합보다는 일반화에 초점을 맞춰 다양한 도메인과 작업에서 평가해야 합니다. 오디오-텍스트(AT) 및 텍스트-오디오(TA) 검색 성능은 벤치마크에 미치지 못했습니다. 우리는 IEEE DCASE의 순위 지표인 mAP@10과 R@1을 사용하여 작업을 측정했습니다. 우리 모델은 Clotho의 mAP@10 측면에서 문헌보다 성능이 우수했고(AT: 0.155 대 0.138, TA: 0.257 대 0.204), AT AudioCaps에서만 어려움을 겪었습니다(AT: 0. 대 0.457, TA: 0.51 대 0.51). 두 데이터 세트 모두 도메인 외부 학습 데이터에 민감하며 학습 쌍을 추가해도 개선으로 이어지지 않았습니다. 이는 [5]의 저자가 SounDesc에서 39k 파일을 추가하려 했지만 실패했고 [4]의 저자가 Wavcaps에서 500k 파일을 추가하려 했고 [6]의 저자가 AudioSet에서 1.7M 파일을 추가하려 했지만 실패했습니다. 5.
--- CONCLUSION ---
훈련 쌍의 수를 늘리면 전반적인 성능이 향상된다는 것입니다. 그러나 단순히 쌍을 추가하면 특정 도메인 및 작업에서 성능이 떨어질 수 있습니다[4, 5, 3, 6]. CLAP의 성능은 텍스트 및 오디오 훈련 쌍의 다양성과 노이즈 정도에 따라 달라집니다. Wav2clip[8]과 Audioclip[9]은 사운드 이벤트에 대한 주석이 달린 데이터 세트인 AudioSet에서 각각 200k 및 1.7M 오디오-텍스트 쌍을 사용했습니다. 작성자는 문장 수준의 설명이 아닌 클래스 레이블과 오디오를 쌍으로 사용하여 설명의 맥락과 언어 의미를 놓칠 가능성이 있지만 각각 3개의 작업에서 우수한 Zero-Shot 성능을 보였습니다. CLAP[3]은 128k 쌍을 사용했지만 텍스트는 오디오 캡션과 웹 소싱 데이터 세트에서 가져온 설명이었습니다. 16개 작업에서 평가되었으며 이전 버전보다 상당히 개선되었습니다. LAION CLAP[4]은 2.5M 쌍 컬렉션을 사용하여 작업에서 성능을 더욱 개선했습니다. 저자는 나중에 음악 및 음성 관련 훈련 쌍을 추가했지만 사운드 이벤트 분류(ESC50)의 성능은 절대적으로 1%만큼 저하되었습니다.Wavcaps[6]는 500k 쌍을 사용했지만 ChatGPT 언어 모델로 노이즈가 많은 웹 소스 설명을 정리했습니다.결과는 8개 작업에서 문헌보다 성능이 우수했습니다.따라서 쌍을 확장할 때 다양한 도메인과 작업에서 일반화를 평가하여 성능 상충 관계를 확인하는 것이 필수적입니다.이 논문에서 우리는 다음과 같은 기여를 합니다.오디오 표현을 학습하기 위해 22개 오디오 작업에서 오디오 인코더를 학습했습니다.언어 표현을 학습하기 위해 자기 회귀 디코더 전용 모델을 학습했습니다.우리는 전례 없는 460만 개의 오디오-텍스트 쌍으로 CLAP 모델을 사전 학습하고 문헌에서 가장 큰 26개 하위 작업에서 표현의 일반화를 광범위하게 평가하여 여러 가지에서 SoTA 결과를 달성했습니다. 2. 방법 대조적 언어-오디오 사전 학습(그림 1)은 오디오와 텍스트 인코더를 공동으로 학습하여 다양한 유형의 추론에 사용할 수 있는 다중 모달 표현을 학습합니다. 영어: Contrast Pretraining Water Draining Text Encoder Text audio pairs T₁ Classes Dog barking Zero-Shot Classification Rain drops Tw Text Encoder A₁ A₁T, A₁₂ A₁T- AT Siren wooling … Audio Encoder Az AAA Ax Art Art Axt Text to Audio Retrieval Text Query A crow crying Text int the forest Encoder Audio Database AN ANTI ANT ANTS ANTN Testing audio Audio Encoder AAT, AT A₁T Audio Captioning Dog barking A campfire A tarsaches A₁ AT &gt;&gt;&gt;&gt; Audio Encoder Testing audio Az A₂T → Audio Encoder Fig. 1: CLAP A As AN ANT Mapper GPT-Mapping 오디오 임베딩을 GPT2 입력 시퀀스에 매핑하여 멀티모달 공간에서 비교할 수 있는 오디오 및 텍스트 임베딩을 학습합니다. 사전 학습된 인코더는 Zero-Shot 분류, 텍스트-오디오 및 오디오-텍스트 검색, 오디오 캡션에 사용할 수 있습니다.2.1. 대조적 언어-오디오 사전 학습 처리된 오디오를 Xa st Xa Є RF×T라고 하자.여기서 F는 스펙트럼 성분(예: Mel 빈)의 수이고 T는 시간 빈의 수이다.텍스트를 Xt로 나타내자.N개의 배치에 있는 각 오디오-텍스트 쌍은 {Xa, Xti로 표현되며, 여기서 i Є [0, N]이다.편의상 i 표기법을 삭제하고, 앞으로 {Xa, Xt}는 N개의 배치를 나타낸다.쌍에서 오디오와 텍스트는 각각 오디오 인코더 fa(.)와 텍스트 인코더 ft(.)로 전달된다.N개의 배치에 대해: Ŷa = fa(Xa); Ŷt = ft(Xt) (1) 여기서 Ŷ a Є R³×V는 차원 V의 오디오 표현이고 Âµ Є R³×ʊ는 차원 U의 텍스트 표현입니다. 학습 가능한 투영 계층을 사용하여 오디오 및 텍스트 표현 Ŷa와 Ŷt를 차원 d의 조인트 멀티모달 공간으로 가져왔습니다. Ea = La(Xa); Et = Lt(Xt) (2) 여기서 Ea Є Rxd, Et Є R³×d, La 및 Lɩ는 각각 오디오 및 텍스트에 대한 투영입니다. 이제 오디오 및 텍스트 임베딩(Ea, Et)이 비교 가능하므로 유사도를 측정할 수 있습니다. C = T (Et · ET) .NXN (3) 여기서 7은 로짓 범위를 조정하는 온도 매개변수입니다. 유사도 행렬 CERN에는 대각선에 N개의 일치하는 쌍이 있고 대각선 외의 N² – N개의 일치하지 않는 쌍이 있습니다. L = 0.5(ltext(C) + laudio(C)) = A(4) 여기서 lk log diag(softmax(C))는 각각 텍스트와 오디오 축을 따라 계산됩니다. 유사도 행렬에 대한 이 대칭적 교차 엔트로피 손실(L)을 사용하여 오디오 및 텍스트 인코더와 해당 투영 계층을 공동으로 학습했습니다. 2.2. 오디오 및 텍스트 인코더 오디오 인코더: 오디오를 처리하기 위해 이 논문[11]과 유사한 방법을 사용하여 22개 오디오 작업에서 트랜스포머 기반 오디오 인코더(HTSAT[10])를 학습했습니다. 이를 HTSAT-22라고 명명했습니다. 여러 오디오 작업에서 학습된 인코더가 일반화를 개선하고 따라서 작업 전반에서 성능을 향상시킬 것이라는 가설을 세웠습니다. 이 방법은 오디오 인코더와 매퍼 네트워크를 학습하여 대규모 언어 모델이 분류, 캡션, 검색 및 오디오 Q&amp;A와 같은 여러 오디오 작업을 수행하도록 합니다. 아키텍처는 본질적으로 캡션 시스템으로 훈련되며, 오디오 프롬프트 p²에 따라 자기 회귀 방식으로 자유형 텍스트 출력 c²를 생성하는 방법을 학습합니다. y는 모델의 훈련 가능한 매개변수를 나타냅니다. 손실 함수는 교차 엔트로피입니다. NL= log p(cpi, P2, C, 1) (5) i=1 j=2k&#39; 텍스트 인코더: 텍스트를 처리하기 위해 텍스트 작업에 인상적인 능력을 보여준 자기 회귀 모델인 GPT2(124M 기반)를 채택했습니다. 우리는 과제를 해결했습니다. 자기 회귀 모델이 문장 수준 표현을 생성하도록 만드는 방법은 무엇일까요? 변환기-디코더 블록으로 구축된 자기 회귀 모델은 입력 텍스트를 받고 가장 가능성 있는 단어(토큰) 시퀀스를 하나씩 출력합니다. 반면에 변환기-인코더 블록(BERT 또는 ROBERTA)으로 구축된 모델은 연속 공간에서 문장 수준 표현을 출력합니다. GPT2가 문장 수준 표현을 출력하도록 하기 위해 특수 토큰|endoftext| &gt; 각 입력 텍스트의 끝에. 대조적 사전 학습 동안 이 토큰의 표현을 문장 수준 표현으로 사용합니다. 이렇게 하면 토큰에 텍스트 입력의 집계 정보가 포함됩니다. 2.3. 평가 Zero-Shot 추론: CLAP의 오디오와 텍스트 간의 유사성을 결정하는 기능을 사용했습니다. C 클래스 레이블과 N 테스트 오디오가 있는 대상 데이터 세트를 고려해 보겠습니다. 먼저 사전 학습된 인코더를 사용하여 N 오디오와 C 클래스에 대한 CLAP의 오디오 및 텍스트 임베딩을 계산합니다. 둘째, 각 테스트 오디오와 모든 클래스 레이블 간의 코사인 유사성을 계산합니다. 검색의 경우 텍스트 쿼리를 클래스로 취급합니다. 각 테스트 오디오는 클래스만큼 많은 로짓을 갖습니다. 셋째, 로짓은 이진 또는 다중 클래스 분류의 경우 소프트맥스를 적용하여 확률 분포로 변환되고, 다중 레이블 분류의 경우 시그모이드를 적용하고, 검색을 위해 변경하지 않습니다. 오디오 캡션: 그림 1의 아키텍처에서 테스트 오디오는 사전 훈련된 오디오 인코더로 전달된 다음 매퍼 네트워크로 전달되고, 그런 다음 GPT2로 전달되어 설명을 생성합니다. 훈련 시간에는 캡션 손실(Eq.5)과 훈련 분할을 사용하여 매퍼 네트워크의 가중치만 학습됩니다. Zero-Shot Score ↑ 사운드 이벤트 분류 ↑ 보컬 사운드 분류 ↑ 감시 사운드 분류 ↑ 액션 분류 ↑ 음향 장면 분류 ↑ DCASEVocal ESCModel 평균 ESCFSD50K US8K SESA TUTTaskSound 액션 CNN14+BERT 0.0.0.0.0.0.0.0.0.0.HTSAT+CLIP 0.0.0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.0.0.0.HTSAT-22+ROBERTa 0.0.0.0.0.0.0.0.0.0.0.HTSAT-22+CLIP 0.0.0.0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.0.0.0.0.0.0.음악 분류 ↑ 악기 분류 ↑ 음성 감정 KWS↑ 화자 분류↑ 계산↑ GTZAN GTZAN 모델 베이징 NS Instr. 음악 음성 장르 오페라 계열 CRE RAV MA-D DESS 음성 명령 LibriCountCNN14+BERT HTSAT+CLIP0.0.0.0.178 0.0.0.0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.0.HTSAT-22+ROBERTa0.0.0.0.0.0.0.0.HTSAT-22+CLIP0.0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.0.297 0.0.0.표 1: 16개 다운스트림 작업과 119k 트레이닝 쌍에 대한 Zero-Shot 성능. 제안하는 인코더(HTSAT-22+GPT2)는 문헌에서 가장 우수한 조합보다 성능이 우수했습니다. 모든 숫자에 대해 높을수록 좋습니다.측정 항목은 FSD50k 및 ESC50-actions의 경우 mAP, DCASE17의 경우 F1-점수이며 나머지는 모두 정확도를 사용합니다.Zero-Shot 점수는 측정 항목의 평균입니다.이것은 문헌에서 16개 작업을 가진 인코더에 대한 첫 번째 비교이며, 일반적으로 몇 개의 인코더와 소수의 작업만 고려합니다.3. 실험 학습 데이터 세트.쌍을 수집하는 것은 아마도 CLAP 모델을 확장하는 데 있어 주요 병목 현상일 것입니다.우리는 다양한 데이터 세트와 웹 아카이브에서 460만 개의 오디오 및 텍스트 쌍으로 가장 큰 컬렉션을 수집했습니다.오디오는 인간의 소리와 활동, 환경 소리, 음향 장면, 음악, 음향 효과 및 음성 감정을 설명합니다.표 1의 인코더 효과를 연구하기 위해 CLAP[3]과 동일한 학습 세트를 사용했습니다.저자들과 달리 AudioCaps 및 Clotho의 테스트 세트를 포함하지 않았으므로 쌍의 수는 128k 대신 119k였습니다. 4.6M 컬렉션의 교육 데이터 세트는 다음과 같습니다.WavCaps[6], AudioSet[2], FSD50K[12], Clotho[13], AudioCaps[14], MACS[15], WavText5k[5], SoundDesc[16], NSynth[17], FMA[18], Mosi[19], Meld[20], Iemocap[21], Mosei[22], MSP-Podcast[23], CochlScene[24], LJspeech[25], EpicKitchen[26], Kinectics700[27], findsounds.com. 자세한 내용은 GitHub에서 확인하세요. 다운스트림 작업. 다양한 도메인에서 26개의 다운스트림 작업을 사용했으며, 그 중 몇몇은 HEAR[1]에서 가져왔습니다. 사운드 이벤트, 음성 사운드, 감시 사운드 및 음향 장면 분류, 오디오 캡션, 검색; 음악, 악기 및 음표 속성 분류; 음성 감정 및 언어 분류; 키워드 스포팅; 화자 계산. 표 1의 인코더 효과를 연구하기 위해 16개 작업의 하위 집합을 사용했습니다. 전처리. 샘플링 속도가 44.1KHz, 홉 크기가 320프레임, 윈도우 크기가 1024프레임, 50-8000Hz 범위의 64개 Mel 빈을 갖는 오디오의 로그 Mel 스펙트로그램 표현을 사용했습니다. 학습하는 동안 각 오디오 클립은 7초의 연속 세그먼트로 무작위로 잘리거나 더 짧으면 패딩됩니다. 쌍이 있는 배치는 무작위로 샘플링됩니다. 인코더. 제안된 CLAP 모델의 경우 Sec.2.2에 설명된 오디오 및 텍스트 인코더 HTSAT-22+GPT2를 사용했습니다. 비교를 위해 표 1에서 문헌에서 가장 좋은 두 가지 인코더 조합인 CNN14+BERT와 HTSAT+ROBERTa[3, 4, 6]를 사용했습니다. 또한 여러 저자가 사용했기 때문에 CLIP의 텍스트 인코더도 포함했습니다[9, 8, 4]. 오디오와 텍스트 임베딩은 모두 1024의 출력 차원을 가진 독립적인 학습 가능한 투영 레이어가 있는 멀티모달 공간에 투영됩니다. 학습. 두 인코더의 포에포크를 모두 동결 해제하여 학습했지만 전체 성능은 처음 10개 에포크에서 최고조에 달했습니다. 가장 좋은 ZeroShot 점수(모든 작업의 평균)를 산출한 에포크에 해당하는 다운스트림 작업의 성능을 보고합니다. 이러한 에포크에 해당하는 모델이 보이지 않는 데이터 세트에 더 잘 일반화되고 커뮤니티에 더 잘 봉사할 것이라고 가정합니다. 각 작업의 성능이 다른 에포크에서 더 높거나 낮았을 수 있습니다. 배치 크기는 1,536이었습니다. 초기 학습 속도가 10-3인 Adam Optimiser를 사용했고, 인내심이 15인 플래토에서 학습 속도를 10-1만큼 줄였습니다. 온도 매개변수 7은 학습 가능하며 0.007로 초기화되었습니다. 4. 결과 및 논의 다양한 오디오 및 텍스트 인코더를 비교한 결과는 표 1에 있고 제안하는 CLAP의 결과는 표 2에 있습니다.4.1. 제안된 오디오 및 텍스트 인코더 제안하는 인코더 HTSAT-22+GPT2는 표 1에 표시된 대로 문헌에 나와 있는 두 가지 최고의 인코더 조합보다 성능이 우수했습니다.전체 성능을 비교하기 위해 모든 16개 작업의 메트릭 평균인 Zero-Shot 점수를 사용했습니다.HTSAT-22+GPT2는 0.480을 달성했으며, 가장 일반적인 조합인 HTSAT+ROBERTa 및 CNN14+BERT(각각 0.431 및 0.)보다 절대적으로 9% 더 높았습니다.모든 인코더 조합이 더 나은 성능을 보였습니다.모델 벤치마크 HTSAT-22+GPTESC[28] 0.948 [6] 0.FSD50K [12] 0.302 [3] 사운드 이벤트 분류 ↑ US8K 0.[29] 0.806 [6] 0.DCASETask 4 [30] 0.3 [3] 0.AudioSet [2] 0.058 [3] 0.Vocal Sound Classification ↑ Vocal Surveillance Sound Classif.↑ Action Classification↑ Acoustic Scene Classification↑ SESA Sound [31] [32] ESCActions [33] 0.495 [3] 0.0.0.TUT[30] 0.296 [3] 0.0.0.Speech Emotion Music Classification ↑ Instrument Classification ↑ KWS↑ Classification↑ GTZAN GTZAN 모델 Music Speech [1] 장르 NS 피치 NS 속도 NS 품질 베이징 NS 악기 CRE RAV Opera family MA-D DESS [1] [17] [17] [17] [1] [17] [1] [34] 음성 명령 [1] 스피커 계산↑ Libri Count[1] 벤치마크 1 [3] 0.25 [3] 0.0.HTSAT-22+GPT0.0.0.0.0.0.0.0.4746 [3] 0.0.0.0.178 [3] 0.0.159 [3] 0.106 [3] 0.178 [3] 0.0.0.Audio Captioning ↑ 모델 벤치마크 HTSAT-22+GPTAudioCaps Clotho [14] [13] 0.438[35] 0.215[35] 0.0.AudioCaps R@0.517[6] 0.Audio-Text 검색 ↑ AudioCaps Clotho MAP@10 R@0.457 [4] 0.234[6] 0.0.텍스트-오디오 검색 ↑ Clotho MAP@0.138[4] 0.AudioCaps R@0.397[6] 0.AudioCaps Clotho Clotho MAP@10 R@1 MAP@0.51[4] 0.195[6] 0.204[4] 0.0.0.표 2: 제안한 인코더와 4.6M개의 학습 쌍을 사용하여 26개의 다운스트림 작업에서 수행한 성능. 벤치마크로서 문헌에서 가장 좋은 숫자를 사용했으며, 숫자를 사용할 수 없는 경우에는 무작위 성능을 사용했습니다. 모든 작업에서 높을수록 좋습니다. 평가 지표는 FSD50k, ESC50-Actions, AudioSet 및 NS Qualities의 경우 MAP이고, DCASE17의 경우 F1-score입니다. 및 캡션을 위한 SPIDER; 다른 모든 것은 정확도를 사용합니다. 무작위보다. 다른 조합이 다른 작업에서 더 나은 성과를 거두었지만, 어느 것도 특정 도메인에서 뛰어나지 않았습니다. 저희의 HTSAT-22 오디오 인코더는 성능 향상에 가장 크게 기여합니다. HTSAT-22는 오디오 작업에서 사전 학습된 반면, HTSAT는 사운드 이벤트 분류에서만 사전 학습되었습니다. 따라서 여러 오디오 작업에서 사전 학습을 생성하면 오디오 인코더의 표현을 개선할 수 있음을 시사합니다. HTSAT22+GPT2를 HTSAT+GPT2와 비교한 결과 LibriCount10(절대 10%), NS Instrument(절대 7%), ESC50(절대 6%)과 같은 주요 개선 사항이 입증되었습니다. 제안된 GPT2 자기 회귀 모델은 인기 있는 ROBERTa를 개선합니다. GPT2를 HTSAT 또는 HTSAT-22와 함께 사용하면 다른 텍스트 인코더보다 최상의 성능을 얻을 수 있었습니다. 저희는 이러한 개선 사항이 두 가지 이유에서 비롯된다고 가정합니다. 첫째, GPT2는 30k인 BERT 및 ROBERTa에 비해 50k 토큰의 더 큰 어휘를 가지고 있습니다.둘째, 수정된 GPT2 자기 회귀는 문장 수준 표현에 사용되는 &lt;|endoftext&gt;까지의 토큰을 예측합니다.이는 자기 감독 역할을 하며 모델이 단어 순서를 학습하고 강조하도록 합니다.4.2. 제안된 CLAP 아키텍처 확장 CLAP 모델은 표 2에 표시된 대로 26개의 다운스트림 작업 대부분에서 새로운 Zero-Shot SoTA를 확립하여 4가지 다른 SoTA 모델보다 우수한 성과를 보였습니다.모델을 벤치마킹하기 위해 다양한 모델에서 나온 문헌에서 가장 좋은 숫자를 사용했습니다.숫자가 없는 경우에는 무작위 성능을 사용했습니다.어떤 경우에는 성능 개선이 벤치마크 문헌의 두 배 이상입니다.몇 가지 주요 내용은 음악 장르가 58.4% 정확도 대 25%, 보컬 사운드가 80% 정확도 대 49.5%, 어쿠스틱 장면이 53.8% 정확도 대 29.6%입니다. 일부 다운스트림 작업은 훈련 세트의 오디오 파일이 4.6M 쌍의 일부이기 때문에 진정한 ZeroShot 설정을 구성하지 않습니다(3절 참조). 예를 들어, FSD50k 오디오 및 웹 설명은 훈련에 사용되었지만 클래스 레이블은 사용되지 않았습니다. 우리는 어떤 다운스트림 작업에 대해서도 CLAP 인코더를 미세 조정하지 않았습니다. 우리는 ESC50에 대한 오디오 인코더만 미세 조정했고 이전 CLAP 모델의 성능을 96.70%에서 98.25% 정확도로 개선하여 새로운 SoTA를 확립했습니다. 4.3. 일반화 및 개별 도메인 성능 훈련에서 다양성을 추가하고 오디오-텍스트 쌍을 확장하면 일부 작업에서는 성능이 향상되지만 다른 작업에서는 성능이 저하되는 상충 관계가 발생합니다. 예상대로 주어진 작업의 도메인과 유사한 훈련 쌍을 추가하면 도움이 되므로 다양성은 일반화에 필수적입니다. 예를 들어, CLAP[3]에는 감정 인식 훈련 쌍이 포함되지 않았으며 RAVDESS에서 17.1% 정확도, CREMAD에서 23.4% 정확도를 달성했습니다. 감정 관련 쌍을 추가하고 정확도를 각각 31.5%와 30%로 개선했습니다. 그럼에도 불구하고 더 많은 쌍은 분포 이동을 일으켜 훈련 데이터와 일부 테스트 데이터 간에 불일치를 생성할 수 있습니다. 예를 들어, 저희 모델은 ESC에서 500k 쌍으로 훈련된 모델[6]보다 약간 낮은 점수를 얻었습니다(94.8% 대 93.9% 정확도). 또 다른 예는 GTZAN Music vs Speech에서 128k 쌍을 사용한 모델[3]이 99.2%를 기록한 저희 모델보다 100% 정확도를 달성했습니다. Table의 저희 모델조차 119k 쌍으로 100% 정확도를 달성했습니다. 훈련 쌍을 추가함에 따라 작업 간 성능이 달라질 것으로 예상해야 합니다. 따라서 제로샷 모델은 특정 작업에 대한 과적합보다는 일반화에 초점을 맞춰 다양한 도메인과 작업에서 평가해야 합니다. 오디오-텍스트(AT) 및 텍스트-오디오(TA) 검색 성능은 벤치마크에 미치지 못했습니다. 우리는 IEEE DCASE의 순위 지표인 mAP@10과 R@1을 사용하여 작업을 측정했습니다.우리의 모델은 Clotho의 mAP@10 측면에서 문헌보다 성능이 우수했고(AT: 0.155 대 0.138 및 TA: 0.257 대 0.204), AT AudioCaps에서만 어려움을 겪었습니다(AT: 0. 대 0.457 및 TA: 0.51 대 0.51).두 데이터 세트 모두 도메인 외부 학습 데이터에 민감하며 학습 쌍을 추가해도 개선으로 이어지지 않았습니다.이는 [5]의 저자들이 SounDesc에서 39k 파일을 추가하려고 시도했지만 실패했고 [4]의 저자들이 Wavcaps에서 500k 파일을 추가하려고 시도했지만 [6]의 저자들이 AudioSet에서 1.7M 파일을 추가하려고 시도했지만 실패했습니다.5. 결론 우리는 제안한 인코더와 4.6M 학습 쌍을 사용하여 CLAP 모델을 도입했습니다. 제로샷 모델은 특정 작업에 대한 과적합보다는 일반화에 초점을 맞춰 다양한 작업에 걸쳐 평가되어야 합니다. 우리는 26개 작업에서 CLAP을 평가하고 대부분에서 SOTA를 확립하여 범용 오디오 표현 분야에서 선두를 달리고 있습니다. 6. 참고문헌 [1] Joseph Turian, Jordie Shier 외, &quot;HEAR: Holistic Evaluation of Audio Representations,&quot; NeurIPSCompetitions and Demonstrations Track, 2022. [2] Jort F. Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence 외, &quot;Audio set: An ontonology and human-labeled dataset for audio events,&quot; IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), 2017. [3] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, Huaming Wang, &quot;Clap learning audio concepts from natural language supervisor,&quot; IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), 2023. [4] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov, &quot;특징 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습,&quot; arXiv 사전 인쇄본 arXiv:2211.06687, 2022. [5] Soham Deshmukh, Benjamin Elizalde 및 Huaming Wang, &quot;WavText5K 및 CLAP 학습을 통한 오디오 검색,&quot; Proc. INTERSPEECH 2023, 2023. [6] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley 외, &quot;Wavcaps: 오디오-언어 멀티모달 연구를 위한 chatgpt 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트,&quot; arXiv 사전 인쇄본 arXiv:2303.17395, 2023. [7] Benjamin Elizalde, Shuayb Zarar 및 Bhiksha Raj, &quot;텍스트 및 오디오 기반 조인트 임베딩을 사용한 크로스 모달 오디오 검색 및 검색,&quot; ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리(ICASSP) 국제 컨퍼런스. IEEE, 2019, pp. 4095-4099. [8] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar 외, &quot;Wav2clip: 클립에서 강력한 오디오 표현 학습&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2022. [9] Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel, &quot;Audioclip: 클립을 이미지, 텍스트 및 오디오로 확장&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2022. [10] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick 외, &quot;Hts-at: 사운드 분류 및 감지를 위한 계층적 토큰 의미 오디오 변환기&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2022. [11] Soham Deshmukh, Benjamin Elizalde, Rita Singh 및 Huaming Wang, &quot;Pengi: 오디오 작업을 위한 오디오 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2305.11834, 2023. [12] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font 및 Xavier Serra, &quot;Fsd50k: 인간이 레이블을 지정한 사운드 이벤트의 오픈 데이터 세트,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2022. [13] Konstantinos Drossos, Samuel Lipping 및 Tuomas Virtanen, &quot;Clotho: 오디오 캡션 데이터 세트,&quot; IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 2020. [14] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee 및 Gunhee Kim, &quot;AudioCaps: 야생 오디오를 위한 캡션 생성,&quot; NAACL-HLT, 2019. [15] Irene Martín-Morató 및 Annamaria Mesaros, &quot;What is the ground truth? confidence of multi-annotator data for audio tagging,&quot; 제29회 유럽 신호 처리 컨퍼런스(EUSIPCO), 2021. [16] A. Sophia Koepke, Andreea-Maria Oncescu, Joao Henriques, Zeynep Akata 및 Samuel Albanie, &quot;Audio retrieval with natural language queries: A benchmark study,&quot; IEEE Transactions on Multimedia, 2022. [17] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, et al., &quot;Neural audio synthesis of musical notes with wavenet autoencoders,&quot; International Conference on Machine Learning. PMLR, 2017. [18] Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, “Fma: 음악 분석을 위한 데이터 세트,&quot; 제18회 국제 음악 정보 검색 학회 컨퍼런스, 2017. [19] Amir Zadeh, Rowan Zellers, Eli Pincus, LouisPhilippe Morency, “Mosi: 온라인 의견 비디오에서 감정 강도와 주관성 분석의 다중 모드 코퍼스,&quot; arXiv 사전 인쇄본 arXiv:1606.06259, 2016. [20] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, et al., “Meld: 대화에서 감정 인식을 위한 다중 모드 다자간 데이터 세트,&quot; 제57회 계산 언어학 협회 연례 회의록, 2019. [21] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, et al., &quot;Iemocap: Interactive emotional dyadic motion capture database,&quot; 언어 리소스 및 평가, 2008. [22] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency, &quot;Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph,&quot; Association for Computational Linguistics의 제56회 연례 회의록(제1권: 장편 논문), 2018. [23] Reza Lotfian과 Carlos Busso, &quot;기존 팟캐스트 녹음에서 감정적 음성을 검색하여 자연스러운 감정적으로 균형 잡힌 음성 코퍼스 구축,&quot; IEEE Transactions on Affective Computing, 2017. [24] 정일영, 박정수, &quot;Cochlscene: 크라우드소싱을 이용한 음향 장면 데이터 수집&quot;, 아시아태평양 신호 및 정보 처리 협회 연례 정상회의 및 컨퍼런스, 2022. [25] Keith Ito, Linda Johnson, &quot;lj 음성 데이터 세트&quot;, https://keithito.com/LJ-Speech-Dataset/, 2017. [26] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Kazakos, et al., &quot;자기중심적 비전 재조정: epic-kitchens-100에 대한 수집, 파이프라인 및 과제&quot;, International Journal of Computer Vision(IJCV), 2022. [27] Lucas Smaira, João Carreira, Eric Noland, Ellen Clancy, Amy Wu, Andrew Zisserman, &quot;kinetics-700-2020 인간 행동 데이터 세트에 대한 간략한 참고 사항&quot;, 2020. [28] Karol J. Piczak, &quot;ESC: 환경 소음 분류를 위한 데이터 세트&quot;, ACM 멀티미디어 제23회 연례 컨퍼런스 회의록. pp. 1015-1018, ACM Press. [29] Justin Salamon, Christopher Jacoby 외, &quot;도시 사운드 연구를 위한 데이터 세트 및 분류법&quot;, 제22회 ACM 멀티미디어 국제 컨퍼런스, 2014년. [30] Annamaria Mesaros, Aleksandr Diment, Benjamin Elizalde, Toni Heittola 외, &quot;dcase 2017 챌린지에서의 사운드 이벤트 감지&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2019년. [31] Yuan Gong, Jin Yu 및 James Glass, &quot;Vocalsound: 인간의 보컬 사운드 인식 개선을 위한 데이터 세트&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 2022년. [32] Tito Spadini, &quot;감시 애플리케이션을 위한 사운드 이벤트&quot;, 2019년 10월. [33] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhiksha Raj, Ian Lane 및 Laurie M Heller, &quot;사운드 이벤트 분류를 위한 동작 식별&quot;, 2021 IEEE 오디오 및 음향에 대한 신호 처리 응용 워크숍(WASPAA). IEEE, 2021, 26-30쪽. [34] Steven R. Livingstone 및 Frank A. Russo, &quot;감정적 말과 노래의 Ryerson 오디오-비주얼 데이터베이스(RAVDESS)&quot;, 2018년 4월. [35] Minkyu Kim, Kim Sung-Bin 및 Tae-Hyun Oh, &quot;자동화된 오디오 캡션을 위한 접두사 튜닝&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 2023년.
