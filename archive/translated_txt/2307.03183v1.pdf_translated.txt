--- ABSTRACT ---
이 논문에서는 다양한 조건에서 녹음된 방대한 680k 시간의 레이블이 지정된 음성 코퍼스로 학습된 최근 자동 음성 인식 모델인 Whisper[1]에 초점을 맞춥니다. 먼저 Whisper가 실제 배경 소리(예: 음악)에 대해 매우 강력하지만 오디오 표현은 실제로 잡음 불변이 아니라 음성이 아닌 소리와 높은 상관관계를 갖는다는 흥미로운 결과를 보여줍니다. 이는 Whisper가 잡음 유형에 따라 음성을 인식한다는 것을 나타냅니다. 이 결과를 바탕으로 Whisper의 백본을 동결하고 그 위에 가벼운 오디오 태그 모델을 학습하여 통합 오디오 태그 및 음성 인식 모델인 Whisper-AT를 구축합니다. 1% 미만의 추가 계산 비용으로 Whisper-AT는 단일 전방 패스에서 말한 텍스트 외에도 오디오 이벤트를 인식할 수 있습니다. 1.
--- INTRODUCTION ---
최근 몇 년 동안 자동 음성 인식(ASR) 성능을 향상시키는 데 상당한 진전이 이루어졌습니다. 구체적으로 wav2vec2.0[2] 및 Hubert[3]와 같은 자기 지도 학습 계획은 최소한의 레이블이 지정된 교육 데이터를 요구함으로써 큰 성공을 거두었습니다. 그러나 공개 모델 체크포인트는 깨끗한 음성 데이터(예: Librispeech[4] 또는 Libri-light[5])로 교육되기 때문에 실제 환경에서의 견고성이 제한됩니다. 노이즈 견고성을 개선하기 위해 Whisper[1] 모델은 다양한 환경과 녹음 설정을 사용하여 인터넷에서 수집한 680K시간 분량의 레이블이 지정된 음성을 교육 데이터로 사용하고 기존 ASR 모델보다 더 나은 견고성을 보고합니다. 이 논문에서 우리는 먼저 Whisper가 배경 소리(ASR의 경우 잡음)에 강인하지만 오디오 표현은 실제로 잡음 불변이 아니라 음성이 아닌 배경 소리의 풍부한 정보를 인코딩한다는 반직관적인 결과를 보여줍니다(그림 1에 표시되고 섹션 3에서 자세히 설명). 이는 Whisper 모델이 잡음 불변 표현을 학습하지 않고 잡음 유형을 인코딩한 다음 잡음 유형에 따라 음성을 인식한다는 것을 나타냅니다. 위의 결과를 흥미롭게 응용한 사례 중 하나는 Whisper를 기반으로 ASR 및 오디오 태깅(즉, 일반 오디오 이벤트 인식)에 대한 통합 모델을 구축할 수 있다는 것입니다. 1) 잡음에 강하고 2) 풍부한 일반 오디오 이벤트 정보를 인코딩하기 때문입니다. 현재 ASR 및 오디오 태깅(AT) 모델은 일반적으로 독립적으로 수행됩니다. 비디오 필사, 음성 지원, 보청기 시스템과 같은 많은 응용 프로그램에서 우리는 오디오에서 말한 텍스트와 음향 장면 분석을 모두 얻고자 하지만 두 시스템을 실행하는 것은 컴퓨팅 측면에서 비용이 많이 듭니다. 이 작업에서 우리는 1% 미만의 추가 계산 비용으로 Whisper가 단일 포워드 패스에서 음성 이벤트와 음성 텍스트를 함께 인식하도록 할 수 있음을 보여줍니다. 우리 모델은 AudioSet에서 41.5의 mAP를 달성하는데, 이는 단독 AT 모델보다 약간 나쁘지만 그럼에도 불구하고 40배 이상 빠릅니다. 레이어 #의 표현을 입력으로 사용하여 분류 그림 1: 놀랍게도 ASR 모델의 잡음 견고성은 중간 표현에 인코딩된 일반 배경 소리(ASR의 경우 잡음) 정보의 양과 양의 상관 관계가 있습니다. 위 그림에서 우리는 음성(Librispeech)이 ESC-50[6]의 점점 더 많은 양의 배경음으로 오염되었을 때 Whisper가 눈에 띄게 더 강력하다는 것을 보여줍니다.아래 그림에서 우리는 Whisper의 중간 표현이 동일한 ESC-50 데이터에서 가장 좋은 선형 탐색 사운드 분류 정확도로 이어진다는 것을 보여줍니다.이는 Whisper가 대부분의 배경음 정보를 인코딩한다는 것을 나타냅니다.다른 모델과 달리 Whisper는 가장 깊은 계층에서도 배경음 정보를 인코딩합니다.PR=자체 감독 사전 학습;FT=PR 및 미세 조정 모델.
--- RELATED WORK ---
: 저희가 아는 한, 견고한 ASR이 실제로 노이즈 변형 표현을 학습한다는 것을 보고한 것은 처음입니다. 대부분의 이전 연구는 노이즈 불변 표현에 초점을 맞춥니다[7, 8, 9, 10, 11]. ASR 및 AT 모델 통합의 경우 가장 가까운 연구는 [12, 13, 14, 15]입니다. [12]에서는 통합 키워드 스포팅 및 오디오 태그 지정 모델을 제안했지만 키워드 스포팅은 최대 35개 단어만 고려하고 저희가 목표로 하는 대규모 어휘 연속 음성 인식 작업보다 훨씬 간단한 작업입니다. [13, 14]에서는 공동 ASR 및 오디오 태그 지정/캡션 훈련 프레임워크를 제안했지만 이 작업에서는 Whisper가 명시적인 오디오 태그 지정 훈련 없이도 이미 풍부한 일반 오디오 정보를 인코딩한다는 것을 보여줍니다. [15]에서는 오디오 태그 지정 작업에 대해 ASR 표현을 테스트했지만 전반적인 성능은 만족스럽지 않습니다. 2. Whisper 견고한 ASR 모델 Whisper [1]는 최근에 제안된 견고한 ASR 모델로, 표준 Transformer [16] 기반 인코더-디코더 아키텍처를 특징으로 합니다. Whisper의 주요 참신함은 아키텍처가 아니라 학습 데이터와 학습 방식입니다. 구체적으로, 680Khour의 비공개 학습 세트에는 다양한 환경, 녹음 설정, 화자 및 언어의 매우 광범위한 오디오 분포를 가진 인터넷에서 수집된 오디오-대본 쌍이 포함되어 있습니다. 저품질 데이터를 걸러내기 위해 상당한 노력이 기울여졌습니다. 오디오북에서 수집된 가장 일반적으로 사용되는 Librispeech(960시간) 및 Libri-light(60K시간) 데이터와 비교할 때, Whisper 학습 데이터는 훨씬 더 크고 다양하지만 노이즈가 있는 레이블도 있습니다. 이것이 Whisper를 기존 ASR 모델과 차별화하는 주요 요인이라고 생각합니다. Whisper 학습 중에는 텍스트 대본만 감독 신호로 사용되고 오디오 이벤트 레이블은 제공되지 않습니다. 이 논문에서 달리 언급하지 않는 한 Whisper-Large 모델을 사용합니다. Whisper는 인코더-디코더 모델이므로 1280차원의 32개 Transformer 레이어로 구성된 Whisper의 오디오 인코더 부분만 오디오 태깅에 사용합니다. 3. 잡음에 강한 ASR은 잡음 변형 표현을 학습합니다 다양한 680K 시간의 학습 데이터 덕분에 Whisper는 다른 모델보다 백색 및 펍 잡음에서 더 강한 것으로 나타났습니다[1]. 다양한 신호 대 잡음비(SNR)의 ESC-50[6] 환경 소리로 오염된 Librispeech 클린 음성 데이터에서 Whisper 및 기타 최신 ASR 모델을 평가하여 이 점을 확인했습니다. 그림 1(위)에서 볼 수 있듯이 Whisper는 성능이 더 뛰어납니다. Whisper의 잡음에 강한 메커니즘은 무엇입니까? 일반적으로 견고한 ASR 모델의 표현은 잡음 불변이어야 한다고 믿어지며, 연구자들은 종종 잡음 불변성을 견고한 ASR에 대한 명시적 귀납적 편향으로 설정합니다(예: [7, 8, 9, 10, 11]). 그러나 우리는 놀랍게도 Whisper의 표현이 실제로 잡음 가변적이고 풍부한 비음성 배경음 정보를 인코딩한다는 것을 발견했습니다. 구체적으로, 우리는 전체 Whisper 모델과 ESC-50 환경음 데이터 세트[6]의 입력 오디오 샘플을 동결했습니다. 그런 다음 Whisper의 모든 계층에서 중간 표현을 추출하고 그 위에 선형 계층을 학습하여 50가지 가능한 클래스에서 사운드 클래스를 분류했습니다. Whisper가 배경음 정보를 인코딩하지 않거나 표현이 배경음에 불변이라면 사운드 분류 결과는 낮을 것이고 그 반대의 경우도 마찬가지입니다. 그림 1(아래)에 표시된 대로, Whisper 표현은 다른 SOTA ASR 모델과 비교했을 때 최고의 ESC-50 사운드 분류 정확도를 보였으며, 이는 Whisper 표현이 대부분의 배경 사운드 정보를 인코딩한다는 것을 나타냅니다. 또한 다른 모든 ASR 모델의 경우 더 깊은 계층의 표현으로 인해 사운드 분류 정확도가 낮아져 모델이 음성 정보를 인코딩하고 배경 사운드 정보를 무시하도록 학습하고 있음을 보여줍니다. Whisper는 더 깊은 계층의 표현도 배경 사운드 정보를 인코딩하기 때문에 이런 동작이 없습니다. Whisper가 노이즈에 강하고 표현이 풍부한 배경 사운드 정보를 인코딩한다는 사실은 Whisper의 강인성 메커니즘이 다른 ASR 모델(wav2vec2-robust [17] 포함)과 다르다는 것을 보여줍니다. 노이즈 불변 표현을 학습하는 대신 먼저 배경 사운드를 인코딩한 다음 노이즈 유형에 따라 텍스트를 필사합니다. 우리는 Whisper의 특정 배경음에 대한 견고성 간의 클래스별 관계를 추가로 확인하여 이 점을 확인했습니다. 20dB에서 -10dB SNR(%)Wind50+헬리콥터 개구리 고양이 문 나무 두드리기 물방울 캔 열기 개 시계 똑딱거림 수탉 시계 알람 뇌우 울음 아기 키보드 타이핑, 사이렌 마우스 클릭 코골이 휴식 마시기 홀짝거림 재채기 기침 유리 불꽃놀이 웃음 딱딱거리는 불 암탉 발자국 귀뚜라미 물 붓기 소 까마귀 숨쉬기 문 나무 삐걱거림 지저귐 새 교회 종 비행기 돼지 엔진 양치질 자동차 경적 곤충 양 박수 손 톱단어 오류율 증가 &gt; 50%(그림에 표시되지 않음): 세탁기, 기차, 전기톱, 진공 청소기, 파도 보기, 비, 변기 물내리기 ESC-50 클래스별 F1 점수 그림 2: 특정 배경음에 대한 Whisper의 견고성 간의 관계에 대한 클래스별 분석 클래스와 소리를 인식할 수 있는 잠재적 능력. 우리는 깨끗한 음성(20dB SNR)에서 ESC-50의 특정 배경 소리로 오염된 음성(-10dB SNR)으로의 WER 증가로 속삭임 견고성을 측정합니다. WER 증가가 낮을수록 모델이 더 견고합니다(Y축). 우리는 동일한 ESC-50 데이터 세트에서 사운드 분류 작업을 위해 Whisper 인코더의 마지막 레이어 표현 위에 선형 레이어를 학습하여 Whisper가 소리를 인식할 수 있는 잠재적 능력을 추정하고(음성이 혼합되지 않은 경우 Whisper 모델은 고정됨) 클래스별 Fl 점수를 보여줍니다. F1 점수가 높을수록 Whisper가 사운드 클래스를 잠재적으로 더 잘 인식할 수 있습니다(X축). 파란색 점선: 배경 소리 유형에 대한 Whisper의 견고성과 이를 인식할 수 있는 잠재적 능력 사이에 양의 상관 관계가 있음을 관찰합니다. 파란색 음영: 대부분의 사운드 클래스가 오른쪽 하단 삼각형 영역에 있음을 관찰하여 Whisper가 사운드 유형을 인식할 수 없다면 해당 사운드 유형에 견고하지 않음을 나타냅니다. 오른쪽 하단 이상치: Whisper가 잠재적으로 인식할 수 있지만 견고하지 않은 일부 배경음이 있는데, 이는 일부 소음이 음성과 크게 겹치고 견고할 수 없기 때문에 예상되는 것입니다. 간단히 말해, 소리 유형을 인식할 수 있는 잠재적인 능력은 Whisper가 이에 견고하기 위한 필요 조건이지만 충분한 조건은 아니라는 것을 알게 되었습니다. 기본 소리 클래스와 그림 2에서 소리 클래스를 인식할 수 있는 잠재적인 능력입니다. 우리는 실제로 이들 사이에 긍정적인 상관관계가 있음을 발견했습니다. 모델에 소음 유형을 수동으로 입력해야 하는 소음 인식 학습[18]과 비교할 때, Whisper는 방대한 680K 시간 학습 세트에서 직접 학습합니다. 이 섹션에서의 논의는 대부분 Whisper를 기반으로 하며, 우리의 실험은 소음 불변성이 소음 강건 ASR에 도움이 되지 않거나 소음 강건 ASR의 표현이 소음 가변적이어야 한다는 것을 나타내지 않습니다. 사실, 우리는 잡음 불변 표현[7, 8, 9, 10, 11]을 장려하는 것이 자기 감독 학습 또는 소규모 데이터 사례에서 실용적인 솔루션이라고 믿습니다.속삭임 훈련에는 산업 수준의 계산 리소스가 필요하고 비쌉니다.우리가 전달하고자 하는 것은 잡음에 강한 ASR 모델은 잡음 불변 표현을 학습할 필요가 없으며 잡음에 강할 수 있는 다른 방법이 있다는 것입니다.속삭임과 같은 잡음 조절 모델은 매우 잘 작동할 수 있고 실제로 작동합니다.4. ASR 및 오디오 태깅 모델 통합 섹션 3의 발견에 대한 흥미로운 응용 프로그램 중 하나는 음성 텍스트와 배경 소리(예: 음악, 경적 등)를 동시에 인식하기 위해 속삭임에 기반한 ASR 및 오디오 태깅에 대한 통합 모델을 구축할 수 있다는 것입니다.이는 비디오 필사, 음성 비서 및 보청기 시스템과 같은 응용 프로그램에서 매우 바람직합니다. Whisper는 이러한 통합 모델의 백본으로 이상적입니다. 1) 배경음에 강하고 2) 중간 표현이 풍부한 일반 오디오 이벤트 정보를 인코딩하여 오디오 태그 지정을 위한 견고한 기반 역할을 하기 때문입니다. 그럼에도 불구하고 원래 Whisper는 사운드 레이블을 출력하지 않으므로 Whisper 중간 표현 위에 모델을 학습하여 사운드 클래스를 예측할 수 있도록 해야 합니다. 의도적으로 Whisper 모델의 원래 가중치를 수정하지 않고 대신 새로운 오디오 태그 지정 레이어를 추가하여 Whisper ASR 기능이 변경되지 않고 단일 전방 패스에서 텍스트와 오디오 레이블을 생성할 수 있습니다. 이 통합 ASR 및 오디오 태그 지정 모델을 Whisper-AT라고 합니다. 이전 섹션에서는 탐색 목적으로 단일 레이어의 표현에 기본 선형 레이어를 적용했습니다. 이 섹션에서는 보다 고급
--- METHOD ---
더 나은 오디오 태그 성능으로 이어지는 s. 1. Last-MLP: 가장 기본적인 방법으로, 먼저 Whisper의 마지막 레이어 표현에 시간 평균 풀링을 적용한 다음 선형 레이어를 적용하여 예측에 매핑합니다. 2. WA-MLP: 그림 3에서 볼 수 있듯이, 마지막 레이어가 모든 사운드 클래스에 최적이 아니라는 것을 발견했습니다. 따라서 모든 레이어의 표현을 가중 평균(WA)하고 시간 평균 풀링과 선형 레이어 전에 가중치를 학습 가능한 것으로 설정하여 이 접근 방식은 모든 레이어의 표현을 활용합니다. 3. WA-Tr: 시간 평균 풀링은 모든 시간적 세부 정보를 제거하고 단일 선형 레이어는 오디오 태그에 너무 단순할 수 있습니다. 따라서 이 모델에 대해 WA-MLP의 선형 레이어를 단일 헤드 시간 변환기 레이어로 대체합니다. 4. TL-Tr: 시간 및 레이어별 변환기(그림 4에 표시된 주요 방법). 가중 평균은 모든 레이어의 표현을 활용하지만, 모든 사운드 클래스는 고정된 가중치 세트를 사용합니다. 그림 3에서 서로 다른 사운드 클래스가 서로 다른 표현 계층을 사용하여 최상의 성능을 달성하는 것을 보여줍니다. 따라서 이상적으로는 각 클래스가 고유한 가중치 집합을 가져야 합니다. 이는 계층에 대한 어텐션 메커니즘을 구축하도록 동기를 부여합니다. 구체적으로, 시간적 변환기의 출력에 다른 계층별 변환기를 적용합니다. 효율적인 설계: Whisper-AT의 원래 목표는 두 개의 독립적인 ASR 및 AT 모델보다 계산 효율성이 더 높은 것이므로 오디오 태그 지정에 대한 추가 비용을 최소화하는 것을 목표로 합니다. WA-Tr 및 TL-Tr에 새로운 변환기 계층을 도입하는 것은 비교적 비쌉니다. 변환기의 복잡도를 O(d²n + dn²)로 간주합니다. 여기서 d는 변환기의 차원이고 n은 입력 길이이며, 각 10초 입력 오디오에 대해 각 Whisper 계층의 표현은 (n=500, d=1280) 형태입니다. 시간적 및 계층 변환기가 Whisper와 동일한 n 및 d를 갖는 경우 계산 비용은 무시할 수 없습니다. 따라서 그림 4에 도시된 바와 같이, 우리는 다음과 같은 효율적인 설계를 제안한다.1) 우리는 각 Whisper 표현에 평균 풀링 계층을 추가하여 시간 시퀀스 길이 n을 25로 낮춘다.2) 우리는 오디오 태그 변환기(TL-Tr512로 표시) 전에 d를 1280에서 512로 낮추기 위해 선택적 선형 투영 계층을 추가한다.3) WA-Tr의 경우, 우리는 먼저 가중 평균을 수행한 다음 시간 변환기를 적용한다.TL-Tr의 경우, 우리는 모든 계층에 대해 단일 시간 변환기를 사용한다.따라서 WA-Tr과 TL-Tr 모두 하나의 시간 변환기만 필요하다.# 클래스الاساس اساس 3 5 79 11 13 15 17 19 21 23 25 27 29계층의 표현 그림 3: 50개의 ESC-50 사운드 클래스에 대한 최상의 Whisper 표현 계층(1-32)의 히스트로그램. 우리는 ESC-50 사운드 분류를 위해 32개의 Whisper 레이어 각각의 표현 위에 선형 레이어를 훈련시키고, 클래스별 F1-Score를 계산하고, 각 사운드 클래스에 대한 최상의 표현 레이어를 찾습니다. 다른 사운드 클래스는 다른 레이어의 표현에서 최상의 F1-Score를 얻습니다. (n=500, d=1280) (n=500, d=1280) (n=500, d=1280) (n=500, d=1280) (25, 1280) (선택) (25,512) 시간 풀링 선형 투영 시간 변환기 (25, 512) 시간 풀링 (25, 1280) 시간 풀링 선형 투영 (25,512) 시간 변환기 -공유 --(25, 512) 시간 풀링 (25, 1280) --텍스트 시간 풀링 (25,512) 공유 선형 투영 시간 변환기 (25,512) 시간 풀링 (512) (512) (512) (32, 512) (512) 계층 평균 변환기 풀링 선형 분류기 오디오 레이블 그림 4: 제안된 시간 및 계층별 변환기 모델.5.
--- EXPERIMENT ---
s는 잡음 불변성이 잡음 강건한 ASR에 도움이 되지 않는다는 것을 나타내지 않으며, 잡음 강건한 ASR의 표현이 잡음 가변적이어야 한다는 것을 나타내지 않습니다. 사실, 우리는 잡음 불변 표현[7, 8, 9, 10, 11]을 장려하는 것이 자기 지도 학습이나 소규모 데이터 사례에서 실용적인 솔루션이라고 믿습니다. Whisper 훈련에는 산업 수준의 계산 리소스가 필요하고 비용이 많이 듭니다. 우리가 전달하고자 하는 것은 잡음 강건한 ASR 모델이 잡음 불변 표현을 학습할 필요가 없으며 잡음 강건할 수 있는 다른 방법이 있다는 것입니다. Whisper와 같은 잡음 조절 모델은 매우 잘 작동할 수 있고 실제로 그렇게 합니다. 4. ASR 및 오디오 태깅 모델 통합 섹션 3의 발견에 대한 흥미로운 응용 프로그램 중 하나는 Whisper를 기반으로 한 ASR 및 오디오 태깅에 대한 통합 모델을 구축하여 음성 텍스트와 배경 사운드(예: 음악, 경적 등)를 동시에 인식할 수 있다는 것입니다. 이는 비디오 필사, 음성 지원, 보청기 시스템과 같은 응용 프로그램에서 매우 바람직합니다. Whisper는 이러한 통합 모델의 백본으로 이상적입니다. 1) 배경 사운드에 강하고 2) 중간 표현이 풍부한 일반 오디오 이벤트 정보를 인코딩하여 오디오 태깅을 위한 견고한 기반 역할을 하기 때문입니다. 그럼에도 불구하고 원래 Whisper는 사운드 레이블을 출력하지 않으므로 Whisper 중간 표현 위에 모델을 학습하여 사운드 클래스를 예측할 수 있도록 해야 합니다. Whisper 모델의 원래 가중치를 의도적으로 수정하지 않고 대신 그 위에 새로운 오디오 태깅 레이어를 추가하여 Whisper ASR 기능이 변경되지 않고 단일 포워드 패스에서 텍스트 및 오디오 레이블을 생성할 수 있습니다. 이 통합 ASR 및 오디오 태깅 모델을 Whisper-AT라고 합니다. 이전 섹션에서는 탐색 목적으로 단일 레이어의 표현에 기본 선형 레이어를 적용했습니다. 이 섹션에서는 더 나은 오디오 태깅 성능으로 이어지는 보다 고급 방법에 대해 설명합니다. 1. Last-MLP: 가장 기본적인 방법으로, 먼저 Whisper의 마지막 레이어 표현에 시간 평균 풀링을 적용한 다음 선형 레이어를 적용하여 예측에 매핑합니다. 2. WA-MLP: 그림 3에서 볼 수 있듯이 마지막 레이어가 모든 사운드 클래스에 최적이 아니라는 것을 알았습니다. 따라서 모든 레이어의 표현을 가중 평균(WA)하고 시간 평균 풀링 및 선형 레이어 전에 가중치를 학습 가능하도록 설정했으므로 이 접근 방식은 모든 레이어의 표현을 활용합니다. 3. WA-Tr: 시간 평균 풀링은 모든 시간적 세부 정보를 제거하고 단일 선형 레이어는 오디오 태깅에 너무 단순할 수 있습니다. 따라서 이 모델에 대해 WA-MLP의 선형 레이어를 단일 헤드 시간 변환기 레이어로 대체합니다. 4. TL-Tr: 시간 및 계층별 변환기(그림 4에 표시된 주요 방법). 가중 평균은 모든 계층의 표현을 활용하지만, 모든 사운드 클래스는 고정된 가중치 집합을 사용합니다. 그림 3에서 서로 다른 사운드 클래스가 서로 다른 표현 계층을 사용하여 최상의 성능을 달성한다는 것을 보여줍니다. 따라서 이상적으로는 각 클래스가 고유한 가중치 집합을 가져야 합니다. 이는 계층에 대한 어텐션 메커니즘을 구축하도록 동기를 부여합니다. 구체적으로, 시간 변환기의 출력에 또 다른 계층별 변환기를 적용합니다. 효율적인 설계: Whisper-AT의 원래 목표는 두 개의 독립적인 ASR 및 AT 모델보다 계산 효율성이 더 높기 때문에 오디오 태그 지정에 대한 추가 비용을 최소화하는 것을 목표로 합니다. WA-Tr 및 TL-Tr에 새로운 변환기 계층을 도입하는 것은 비교적 비쌉니다. Transformer의 복잡도를 O(d²n + dn²)로 간주하고, 여기서 d는 Transformer의 차원이고 n은 입력 길이입니다. 각 10초 입력 오디오에 대해 각 Whisper 계층의 표현은 (n=500, d=1280) 형태입니다. 시간 및 계층 Transformer가 Whisper와 동일한 n과 d를 갖는 경우 계산 비용은 무시할 수 없습니다. 따라서 그림 4에서 설명한 대로 다음과 같은 효율적인 설계를 제안합니다. 1) 각 Whisper 표현에 평균 풀링 계층을 추가하여 시간 시퀀스 길이 n을 25로 줄입니다. 2) 선택적 선형 투영 계층을 추가하여 오디오 태그 Transformer(TL-Tr512로 표시) 전에 d를 1280에서 512로 줄입니다. 3) WA-Tr의 경우 먼저 가중 평균을 수행한 다음 시간 Transformer를 적용합니다. TL-Tr의 경우 모든 계층에 단일 시간 Transformer를 사용합니다. 따라서 WA-Tr과 TL-Tr 모두 하나의 시간 변환기만 필요합니다. # 클래스الاساس اساس 3 5 79 11 13 15 17 19 21 23 25 27 29 레이어 표현 그림 3: 50개 ESC-50 사운드 클래스에 대한 최상의 Whisper 표현 레이어(1-32)의 히스트로그램. ESC-50 사운드 분류를 위해 32개 Whisper 레이어 각각의 표현 위에 선형 레이어를 학습하고, 클래스별 F1 점수를 계산하고, 각 사운드 클래스에 대한 최상의 표현 레이어를 찾습니다. 다른 사운드 클래스는 다른 레이어의 표현에서 최상의 F1 점수를 얻습니다. (n=500, d=1280) (n=500, d=1280) (n=500, d=1280) (n=500, d=1280) (25, 1280) (선택) (25,512) 시간 풀링 선형 투영 시간 변환기 (25, 512) 시간 풀링 (25, 1280) 시간 풀링 선형 투영 (25,512) 시간 변환기 -공유 --(25, 512) 시간 풀링 (25, 1280) --텍스트 시간 풀링 (25,512) 공유 선형 투영 시간 변환기 (25,512) 시간 풀링 (512) (512) (512) (32, 512) (512) 계층 평균 변환기 풀링 선형 분류기 오디오 레이블 그림 4: 제안된 시간 및 계층별 변환기 모델. 5. 실험 섹션 4에서 언급했듯이, 우리는 의도적으로 원래 Whisper 모델의 가중치를 동결했으므로 Whisper-AT의 ASR 성능은 원래 Whisper와 정확히 동일합니다[1]. 따라서 우리는 오디오 태그 작업에 대한 실험만 수행합니다.5.1. 실험 설정 데이터 세트: 우리는 표준 평가 프로토콜을 따르는 AudioSet 및 ESC-50 데이터 세트를 사용합니다.AudioSet[20]은 YouTube 비디오에서 추출한 200만 개가 넘는 10초 오디오 클립 컬렉션으로, 527개 레이블 세트에서 클립에 포함된 사운드로 레이블이 지정되었습니다.우리는 균형 잡힌 훈련 세트(AS-20K)와 전체 훈련 세트(AS-2M)로 모델을 훈련하고 평가 세트에 대한 mAP를 보고합니다.ESC-50[6]은 50개 클래스로 구성된 2,000개의 5초 환경 오디오 녹음으로 구성되어 있습니다.우리는 공식 5겹 교차 검증 프로토콜을 사용하여 모델을 평가합니다. 하이퍼 매개변수: 이전 AT 작업에서 사용된 표준 학습 파이프라인을 사용합니다[21, 22, 26, 27]. 모든 실험에서 배치 크기 48과 Adam 최적화 도구[28]를 사용합니다. 제안된 TL-Tr512 모델의 경우 초기 학습률 2e-4, 1e-4, 5e-4를 사용하고 AS20K, AS-2M, ESC-50에 대해 각각 30, 5, 30 에포크 동안 모델을 학습합니다. 기준 방법의 경우 공정한 비교를 보장하기 위해 학습률을 검색합니다. 5.2. 실험 결과 표 1에 주요 결과를 표시합니다. 주요
--- CONCLUSION ---
s는 다음과 같습니다. 표 1: AS-20K, AS-2M(mAP), ESC-50(정확도)에서의 오디오 태깅 성능 비교. *ASR 백본 매개변수와 FLOP는 포함되지 않았습니다. *속도 향상 = 1/FLOP, AST와 비교; FLOPS는 fvcore[19]에 의해 계산되었습니다. *: 레이블이 지정된 AS-2M 데이터도 사용됩니다. AS-2M 실험은 비용이 많이 들기 때문에 AS-20K 및 ESC50 실험에서 이미 명확한 차이가 나타났으므로 건너뜁니다. 엔드투엔드 미세 조정 결과는 비교가 정확히 공정하지 않으므로 회색 텍스트로 표시됩니다. ** 모델 학습 설정 방법 AS-20K AS-2M ESC-AT #Params+ AT 속도 향상** 기존 독립형 오디오 태그 모델 AudioSet Baseline [20] 엔드투엔드 미세 조정 31.AST [21] 엔드투엔드 미세 조정 34.45.88.87M SSAST [22] 엔드투엔드 미세 조정 31.88.87M 1 x (133G FLOPs) 1 x PANNS [23] 엔드투엔드 미세 조정 27.43.94.7* 81M 2.5 x MAE-AST [24] 엔드투엔드 미세 조정 30.90.87M 2.7 x Audio-MAE [25] 엔드투엔드 미세 조정 37.47.94.87M 2.7 x 기존 자동 음성 인식 모델 Hubert X-Large [3] Frozen WA-MLP 18.82.0.7M 195K × Hubert X-Large [3] Frozen TL-Tr20.83.40M wav2vec2-Large-Robust [17] Frozen WA-MLP 18.78.0.5M 5 x 244K X wav2vec2-Large-Robust [17] Frozen TL-Tr20.82.26M 17 x Whisper-AT Whisper-Large Frozen Last-MLP 20.20.87.0.7M 195K X Whisper-Large Frozen WA-MLP 25.32.90.0.7M 195K X Whisper-Large Frozen WA-Tr 32.41.91.20M 270 x Whisper-Large Frozen TL-Tr33.42.91.40M 8 × Whisper-Large Frozen TL-Tr32.41.91.7M 42 × Whisper-Large Whisper-Small Fine-Tuning Fine-Tuning End-to-End End-to-End 34.45.90.655M 0.4 x 31.44.88.94M 2.5 x 먼저, Whisper-AT는 오디오 태그에 대해 Hubert XLarge [3] 및 wav2vec2-Large-Robust [17]보다 훨씬 강력하여 Whisper가 가장 강력한 ASR 모델일 뿐만 아니라 가장 강력한 오디오 태그 백본임을 보여줍니다. 둘째, 네 가지 Whisper-AT 모델을 비교했을 때, 제안된 TL-Tr 모델은 더 높은 계산 오버헤드로 가장 좋은 성능을 이끌어냅니다. 그러나 Transformer 차원을 1280에서 512로 투영함으로써 TL-Tr512는 FLOP이 Whisper ASR FLOP의 1% 미만이지만 TL-Tr1280과 거의 동일한 성능을 보이기 때문에 성능과 효율성 간의 균형을 이룹니다. 표 2에서 TL-Tr에 대한 오디오 태그 성능과 Transformer 차원 d 간의 관계를 추가로 연구합니다. TL-Tr128도 상당히 좋은 오디오 태그 성능을 제공하는 반면 계산 비용은 거의 무료입니다(Whisper ASR FLOP의 &lt;0.1% FLOPS). 셋째, Whisper-AT는 SOTA 독립형 오디오 태그 모델보다 약간 나쁘지만 훨씬 더 효율적입니다. 제안된 TL-Tr512는 AS-20K, AS-2M 및 ESC-50에서 각각 32.8 mAP, 41.5 mAP 및 91.7 정확도를 달성하며 AST보다 42배 빠르고 11배 작습니다[21]. 사용자가 이미 ASR을 실행 중이고 추가 오디오 레이블을 얻고자 하는 경우를 대상으로 하므로 AT의 추가 비용만 비교하고 이 비교에 ASR의 비용은 포함하지 않습니다.넷째, Whisper는 종단 간 미세 조정 설정에서 어떻게 수행되며 SOTA 오디오 태그 모델과 비교하면 어떻습니까?우리는 Whisper 인코더 위에 새로운 Transformer 계층을 추가하고 전체 모델을 종단 간으로 학습합니다(새로운 계층은 10-100배 더 큰 학습률을 사용함).공정한 비교를 위해 SOTA 오디오 태그 모델과 비슷한 크기의 Whisper-Small도 테스트합니다. 우리는 미세 조정 후 Whisper-Small이 SSAST [22] 및 MAE-AST [24]와 같은 이전의 자체 감독 사전 학습된 모델과 유사한 성능을 보이는 것을 발견했습니다.마지막으로, 우리는 더 작은 Whisper 모델의 오디오 태그 성능을 테스트합니다.그림 5에서 볼 수 있듯이, 더 작은 모델은 오디오 태그 성능이 약하지만 Whisper-Small, Medium 및 Large 간의 차이는 미미합니다.또한 ESC50 배경 소리로 오염된 음성에 대한 이러한 모델의 ASR 노이즈 강건성을 테스트합니다.더 큰 모델이 더 강건합니다.우리는 다시 ASR 노이즈 roTable 2: TL-Tr Transformer 차원 d의 성능 및 효율성 영향 사이에 양의 상관 관계를 관찰합니다. AudioSet-2M MAP Tr Dim d FLOPS (G) #Params (M) AS-20K ESC-0.0.30.91.0.2.32.92.3.7.32.91.6.15.33.91.16.40.33.91.Tiny TL-Tr E2E FT 단어 오류율(%)속삭임-대속삭임-중속삭임-소속속삭임-기본속삭임-소형 ---- Hubert-XLarge-FT--기본 소중속 대속삭임 모델 크기 신호 대 잡음비(dB) 그림 5: AS-2M 오디오 태깅 성능(왼쪽) 및 Whisper 모델 제품군의 ASR 견고성(오른쪽). 버스트니스 및 AT 성능. 또한 Whisper-Base(74M 매개변수)는 이미 ASR에서 더 견고하고 Hubert-X-Large(964M 매개변수)보다 오디오 태그 지정에서 더 강력합니다. 6. 결론 Whisper ASR 모델은 방대하고 다양한 훈련 코퍼스를 사용하여 지도 학습 체계를 되살립니다. 이 논문에서 우리는 Whisper의 흥미로운 특성을 보고합니다. 매우 견고하지만 Whisper의 오디오 표현은 실제로 잡음 변형적이고 풍부한 배경 사운드 정보를 인코딩합니다. 이 발견을 바탕으로 Whisper-AT라는 통합 오디오 태그 지정 및 ASR 모델을 제안합니다. 1% 미만의 추가 비용으로 Whisper-AT는 단일 전방 패스에서 말한 텍스트 외에도 배경 사운드를 인식할 수 있습니다. 감사의 말: 이 연구는 MITIBM Watson AI Lab의 지원을 받았습니다. 7. 참고문헌 [1] A. Radford, JW Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, &quot;Robust speechrecognition via large-scale weak supervisor,&quot; arXiv preprint arXiv:2212.04356, 2022. [2] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, &quot;wav2vec 2.0: A framework for self-supervised learning of speech representations,&quot; Advances in Neural Information Processing Systems, vol. 33, pp. 12449–12460, 2020. [3] W.-N. Hsu, B. Bolte, Y.-HH Tsai, K. Lakhotia, R. Salakhutdinov, A. Mohamed, &quot;Hubert: 숨겨진 단위의 마스크 예측을 통한 자체 감독 음성 표현 학습,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, vol. 29, pp. 3451-3460, 2021. [4] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, &quot;Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 asr 코퍼스,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2015, pp. 5206-5210. [5] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen 등, &quot;Libri-light: 제한적 또는 전혀 감독이 없는 asr에 대한 벤치마크&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2020, 7669-7673쪽. [6] KJ Piczak, &quot;Esc: 환경 소리 분류를 위한 데이터 세트&quot;, 제23회 ACM 국제 멀티미디어 컨퍼런스 회의록, 2015, 1015-1018쪽. [7] M. Van Segbroeck 외, &quot;시간 주파수 패치의 비지도 학습을 통한 음성의 잡음 강건 표현&quot;, Speech Communication, 제51권, 제11호, 1124-1138쪽, 2009. [8] D. Serdyuk, K. Audhkhasi, P. Brakel, B. Ramabhadran, S. Thomas, Y. Bengio, &quot;잡음이 있는 음성 인식을 위한 불변 표현&quot;, NIPS 2016 음성 및 오디오 처리를 위한 종단 간 학습 워크숍, 2016. [9] A. Sriram, H. Jun, Y. Gaur, S. Satheesh, &quot;생성적 적대 네트워크를 사용한 강건한 음성 인식&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2018, 5639-5643쪽. [10] D. Liang, Z. Huang 및 ZC Lipton, &quot;강력한 음성 인식을 위한 잡음 불변 표현 학습&quot;, 2018 IEEE Spoken Language Technology Workshop(SLT). IEEE, 2018, 56-63쪽. [11] Q.-S. Zhu, J. Zhang, Z.-Q. Zhang, M.-H. Wu, X. Fang 및 LR Dai, &quot;자동 음성 인식을 위한 잡음 강건 자체 감독 사전 학습 모델 기반 음성 표현 학습&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2022, 3174-3178쪽. [12] H. Dinkel, Y. Wang, Z. Yan, J. Zhang 및 Y. Wang, &quot;Unikwat: 통합 키워드 발견 및 오디오 태그 지정&quot;, arXiv 사전 인쇄본 arXiv:2209.11377, 2022. [13] N. Moritz, G. Wichern, T. Hori, 및 J. Le Roux, &quot;올인원 트랜스포머: 음성 인식, 오디오 태그 지정 및 이벤트 감지 통합.&quot; INTERSPEECH, 2020, pp. 3112–3116. [14] C. Narisetty, E. Tsunoo, X. Chang, Y. Kashiwagi, M. Hentschel, 및 S. Watanabe, &quot;공동 음성 인식 및 오디오 캡션&quot;, ICASSP 2022-2022 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2022, pp. 7892-7896. [15] J. Turian, J. Shier, HR Khan, B. Raj, BW Schuller, CJ Steinmetz, C. Malloy, G. Tzanetakis, G. Velarde, K. McNally et al., &quot;듣기: 오디오 표현의 전체적 평가&quot;, NeurIPS 2021 경쟁 및 데모 트랙 PMLR, 2022, pp. 125-145. [16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, L. Kaiser 및 I. Polosukhin, 신경 정보 처리 시스템의 발전, 2017. [17] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. 칸, A. Lee, R. Collobert, G. Synnaeve, and M. Auli, &quot;강력한 wav2vec 2.0: 자기감독 사전 훈련에서 도메인 이동 분석&quot;, Proc. Interspeech 2021, 2021, pp. 721-725. [18] ML Seltzer, D. Yu, 및 Y. Wang, &quot;노이즈 강건 음성 인식을 위한 딥 신경망에 대한 조사,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2013, pp. 7398-7402. [19] &quot;vcore,&quot; https://github.com/facebookresearch/fvcore. [20] JF Gemmeke, DP Ellis, D. Freedman, A. Jansen, W. Lawrence, RC Moore, M. Plakal, 및 M. Ritter, &quot;오디오 세트: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2017. [21] Y. Gong, Y.-A. Chung, 및 J. Glass, &quot;AST: 오디오 스펙트로그램 변환기,&quot; Proc. Interspeech 2021, 2021, pp. 571–575. [22] Y. Gong, C.-I. Lai, Y.-A. Chung, 및 J. Glass, &quot;Ssast: 자기 감독 오디오 스펙트로그램 변환기&quot;, AAAI 인공지능 컨퍼런스 논문집, vol. 36, no. 10, 2022, pp. 10 699-10 709. [23] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, 및 MD Plumbley, &quot;Panns: 오디오 패턴 인식을 위한 대규모 사전 학습 오디오 신경망&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, vol. 28, pp. 2880–2894, 2020. [24] A. Baade, P. Peng, 및 D. Harwath, &quot;MAE-AST: 마스크 자동 인코딩 오디오 스펙트로그램 변환기&quot;, Proc. Interspeech 2022, 2022, 2438-2442쪽. [25] P.-Y. Huang, H. Xu, JB Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer, &quot;수신하는 마스크 자동 인코더&quot;, 신경 정보 처리 시스템의 발전, 2022. [26] Y. Gong, Y.-A. Chung 및 J. Glass, &quot;Psla: 사전 학습, 샘플링, 레이블링 및 집계를 통한 오디오 태그 개선,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2021. [27] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid 및 C. Sun, &quot;다중 모드 융합을 위한 주의 병목 현상,&quot; 신경 정보 처리 시스템의 발전, 제 34권, 14200-14213쪽, 2021. [28] DP Kingma 및 J. Ba, &quot;Adam: 확률적 최적화 방법,&quot; 국제 학습 표현 컨퍼런스, 2015.
