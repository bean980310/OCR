--- INTRODUCTION ---
영어: Neural Radiance Fields(NeRFs) [Mildenhall et al. 2020]는 놀라운 새로운 뷰 합성(NVS) 결과를 얻을 수 있으며, 가상/혼합 현실, 로봇 공학, 계산 사진 분야의 애플리케이션을 구동합니다. 저자 주소: Barbara Roessle, 독일 뮌헨 공과대학교; Norman Müller, 독일 뮌헨 공과대학교 및 Meta Reality Labs 취리히, 스위스; Lorenzo Porzi, 스위스 Meta Reality Labs 취리히; Samuel Rota Bulò, 스위스 Meta Reality Labs 취리히; Peter Kontschieder, 스위스 Meta Reality Labs 취리히; Matthias Nießner, 독일 뮌헨 공과대학교. 포즈를 취한 입력 이미지 세트가 주어지면 NeRF는 복잡하고 시점에 따라 달라지는 장면 정보를 5D 입력 벡터(3D 좌표 + 2D 시야 방향)로 매개변수화하여 신경망으로 모델링된 체적 밀도 및 색상 필드로 추출합니다. 그런 다음 체적 렌더링 기술을 적용하여 이러한 분야의 새로운 카메라 뷰에 대한 사실적인 2D 출력 이미지를 생성합니다. NeRF는 사실적인 NVS를 가능하게 하는 컴팩트한 장면 표현을 제공하는 데 매우 효과적이지만, 그럼에도 불구하고 실제 사용 사례에는 적용 가능성이 제한적입니다. 즉, NeRF는 훈련 데이터 세트의 모양 정보에 과적합하도록 최적화되어 있어 신중하게 수집된 입력 데이터와 적절하게 선택된 정규화 전략에 크게 의존하게 됩니다. 사실, 모양-광도 모호성[Zhang et al. 2020]은 기본 지오메트리를 존중하지 않고도 NeRF에서 훈련 이미지 세트를 완벽하게 재생성할 수 있지만, 단순히 뷰 종속 광도를 활용하여 실제 장면 지오메트리를 시뮬레이션할 수 있다고 설명합니다. 결과적으로 훈련되지 않은 카메라 뷰에 대한 새로운 뷰 합성 품질이 크게 저하되고 생성된 이미지는 잘 알려진 흐린 플로터 아티팩트를 나타냅니다. 불행히도 캡처 노력을 상당히 증가시키고 입력 데이터가 장면에 대한 밀도 있는 적용 범위를 포함하는 경우에도 조명 조건이 변하는 영역이나 반사 또는 질감이 낮은 영역에서 재구성 문제는 모호한 상태로 남을 수 있습니다.마지막으로 NeRF를 보다 널리 채택하고 적용하기 위해서는 이미지 캡처 프로세스가 간단하고 노력이 적게 들면서도 고품질 재구성 결과를 얻어야 합니다.저희의 작업에서는 생성적 적대 신경망(GAN)을 활용하여 까다로운 실제 세계 2에서 NeRF 품질을 개선합니다.Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner 시나리오.이를 위해 NeRF 재구성에서 직접 불완전성을 해결하기 위한 새로운 접근 방식인 GANERF를 소개합니다.저희의 핵심 아이디어는 엔드투엔드 방식으로 적대적 손실 공식을 활용하여 장면당 2D 판별기에서 추가 렌더링 제약 조건을 도입하는 것입니다. 특히, 관찰이 제한된 지역에서는 실제 이미지 패치의 분포를 보다 밀접하게 따르는 패치를 생성하기 위해 복사장 표현을 강제합니다. 결과적으로 GANERF는 제한된 적용 범위, 이미지 왜곡 또는 조명 변화와 같은 근본 원인과 관계없이 불완전한 입력 데이터로 인한 NVS의 품질 저하 효과를 현저하게 완화합니다. NeRF 최적화 중에 공동 적대적 훈련을 제안하여 2D 패치 판별기가 NeRF에 렌더링된 패치의 사실성 정도를 알려줍니다. 3D 장면 표현으로의 그래디언트 피드백을 통해 복사장 재구성의 일반적인 불완전성을 줄이는 동시에 본질적으로 3D 일관성이 있고 사실적인 NeRF 렌더링을 장려합니다. 여러 스케일에서 2D NeRF 렌더링에서 작동하는 후속 생성기로 출력 품질을 더욱 개선하는 방법을 보여 주며, 이를 세부적으로 조정하여 장면 이미지의 실제 분포와 더 가깝게 일치시킵니다. 새로운 ScanNet++ [Yeshwanth et al.의 까다로운 실내 장면에서 방법을 평가합니다. 2023] 및 잘 알려진 Tanks 및 Temples [Knapitsch et al. 2017] 데이터 세트를 사용하고 NeRFS 내에서 적대적 공식을 활용하면 이전 작업보다 이미지 품질이 크게 향상됨을 보여줍니다. 모든 테스트 장면에서 LPIPS(28-48% 감소)와 같은 지각적 지표에 대해 성능이 가장 좋은 기준선 [Barron et al. 2022; Tancik et al. 2023]보다 현저히 향상되었으며, 일관되게 더 나은 PSNR 및 SSIM 점수를 유지했습니다. 요약하면 다음과 같은 기여를 제공합니다. • 2D 판별기에서 얻은 패치 기반 렌더링 제약 조건을 부과하여 3D 일관된 광도장 표현을 최적화하는 새로운 적대적 공식을 소개합니다. • 렌더링 출력을 더욱 개선하는 2D 생성기를 제안하여 어렵고 대규모 장면에서 새로운 뷰 합성에서 최첨단 방법보다 상당한 개선을 보여줍니다.
--- RELATED WORK ---
영어: Neural Radiance Fields(NeRFs)는 3D 장면을 체적 함수로 모델링하여 임의의 관점에서 렌더링하여 매우 사실적인 이미지를 생성할 수 있습니다. [Mildenhall et al. 2020]의 획기적인 논문은 이 함수를 다층 퍼셉트론(MLP)으로 인코딩했지만, 최근 연구에서는 폭셀 그리드[Sara Fridovich-Keil and Alex Yu et al. 2022; Sun et al. 2022], 평면 기반 인수분해[Chen et al. 2022] 또는 다중 스케일 3D 해시 그리드[Müller et al. 2022]와 같은 공간 데이터 구조를 기반으로 하는 대체 표현을 제안했습니다. Mip-NeRF[Barron et al. 2021]는 광선 대신 원뿔대 기반의 대체 렌더링 공식을 도입하여 앨리어싱 문제를 해결했으며, [Barron et al. 2022] 무한 장면을 설명하고 [Barron et al. 2023]에서 해시 그리드 기반 표현에 적용되었습니다. 저희 작업에서는 [Tancik et al. 2023]의 Nerfacto 모델을 따르는데, 이는 Instant-NGP [Müller et al. 2022]의 필드 아키텍처와 MipNeRF-360 [Barron et al. 2022]의 다단계 제안을 결합하여 속도와 품질 간에 좋은 균형을 이룹니다. 2.1 사전 확률이 있는 NeRF 이전 섹션에서 논의한 접근 방식은 인상적인 시각적 충실도를 달성하지만 일반적으로 제약이 부족한 장면 영역을 표현하는 데 어려움을 겪습니다. 대부분의 NeRF는 &quot;플로터&quot;와 같은 일반적인 아티팩트를 해결하기 위해 하나 이상의 휴리스틱을 통합합니다. 예를 들어, 정점 밀도를 촉진하는 손실을 추가합니다 [Barron et al. 2021, 2022; Hedman et al. 2021], 학습 중에 모델에 노이즈를 주입[Mildenhall et al. 2020]하거나, 표면 매끄러움을 촉진[Oechsle et al. 2021; Wang et al. 2021; Zhang et al. 2021]합니다. 그러나 데이터가 부족하거나 모호한 경우 모델을 정규화하기 위해 더 효과적인 사전 확률이 필요합니다. 소수 샷 설정에서 기하학적 사전 확률은 특히 효과적인 것으로 나타났습니다. 사전 학습된 모델은 예측된 깊이[Deng et al. 2022; Roessle et al. 2022] 또는 표면 법선[Yu et al. 2022]으로 NeRF를 감독하는 데 사용할 수 있습니다. RegNeRF[Niemeyer et al. 2022]는 기하학적(표면 매끄러움, 샘플링 공간 어닐링) 및 모양(흐름 정규화) 정규화기를 결합하여 최소 3개의 이미지에서 NeRF를 학습할 수 있습니다. 마찬가지로 SinNeRF[Xu et al. 2022]는 기하학적 및 의미적 의사 레이블을 사용하여 단일 뷰에서 NeRF 재구성을 안내하는 반지도 학습 접근 방식을 제안합니다. 저희 작업에서는 기하학적 사전 확률이 미미한 개선만을 제공하는 것으로 관찰된 더 밀도가 높은 커버리지가 있는 장면에 초점을 맞춥니다[Niemeyer et al. 2022]. 다른 작업에서는 외관 기반 사전 확률에 초점을 맞춥니다. 예를 들어, DiffusioNeRF[Wynn and Turmukhambetov 2023]는 RGBD 이미지에서 학습된 2D 잡음 제거 확산 모델을 사용하여 비지도 손실 항을 구성하여 NeRF가 관찰되지 않은 관점에서 그럴듯한 이미지를 렌더링하도록 합니다. 마지막으로, 일부 접근 방식은 NeRF 재구성을 일반화 문제로 캐스팅하여 장면 기반 사전 확률을 학습합니다. MVSNERF[Chen et al. 2021]는 신경 볼륨에 인코딩된 비용 볼륨을 구성하여 몇 개의 이미지에서만 일관된 렌더링을 허용합니다. PixelNeRF[Yu et al. 2021] 및 GenVS [Chan et al. 2023]는 새로운 뷰에서 렌더링할 수 있는 3D 인식 신경 표현으로 몇 가지 입력 뷰를 들어올리기 위해 이미지 인코더를 훈련합니다. 그러나 이러한
--- METHOD ---
ologies → 재구성. 추가 키워드 및 구문: 신경 복사장, 새로운 뷰 합성 소개 신경 복사장(NeRF) [Mildenhall et al. 2020]은 놀라운 새로운 뷰 합성(NVS) 결과를 얻을 수 있으며, 가상/혼합 현실, 로봇 공학, 계산 분야의 애플리케이션을 구동합니다. 저자 주소: Barbara Roessle, 뮌헨 공과대학교, 독일; Norman Müller, 뮌헨 공과대학교 및 Meta Reality Labs 취리히, 스위스; Lorenzo Porzi, Meta Reality Labs 취리히, 스위스; Samuel Rota Bulò, Meta Reality Labs 취리히, 스위스; Peter Kontschieder, Meta Reality Labs 취리히, 스위스; Matthias Nießner, 뮌헨 공과대학교, 독일. 사진 및 기타 다수. 포즈를 취한 입력 이미지 집합이 주어지면 NeRF는 5D 입력 벡터(3D 좌표 + 2D 시야 방향)로 매개변수화된 복잡하고 시점에 따라 달라지는 장면 정보를 신경망으로 모델링된 체적 밀도 및 색상 필드로 추출합니다. 그런 다음 체적 렌더링 기술을 적용하여 이러한 필드에서 새로운 카메라 뷰에 대한 사실적인 2D 출력 이미지를 생성합니다. NeRF는 사실적인 NVS를 가능하게 하는 컴팩트한 장면 표현을 제공하는 데 매우 효과적이지만, 실제 사용 사례에는 여전히 적용 가능성이 제한적입니다. 즉, NeRF는 학습 데이터 세트의 모양 정보에 과적합하도록 최적화되어 있어 신중하게 수집된 입력 데이터와 적절하게 선택된 정규화 전략에 크게 의존합니다. 사실, 모양-광도 모호성[Zhang et al. 2020]은 기본 지오메트리를 존중하지 않고도 NeRF에서 학습 이미지 집합을 완벽하게 재생성할 수 있지만 단순히 뷰 종속 광도를 활용하여 실제 장면 지오메트리를 시뮬레이션할 수 있다고 설명합니다. 결과적으로, 비훈련 카메라 뷰에 대한 새로운 뷰 합성 품질이 크게 저하되고 생성된 이미지는 잘 알려진 흐린 플로터 아티팩트를 보입니다. 불행히도, 캡처 노력을 상당히 증가시키고 입력 데이터가 장면에 대한 밀도 있는 커버리지를 포함하는 경우에도 조명 조건이 변하는 영역이나 반사 또는 질감이 낮은 영역에서 재구성 문제는 모호한 상태로 남을 수 있습니다. 마지막으로, NeRF를 보다 널리 채택하고 적용 가능하게 하기 위해서는 이미지 캡처 프로세스가 간단하고 노력이 적게 들면서도 고품질 재구성 결과를 얻어야 합니다. 저희 작업에서는 생성적 적대 신경망(GAN)을 활용하여 까다로운 실제 시나리오에서 NeRF 품질을 개선합니다. 2. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner. 이를 위해 NeRF 재구성에서 직접 불완전성을 해결하기 위한 새로운 접근 방식인 GANERF를 소개합니다. 우리의 핵심 아이디어는 엔드투엔드 방식으로 적대적 손실 공식을 활용하여 장면당 2D 판별기에서 추가 렌더링 제약을 도입하는 것입니다. 특히, 관찰이 제한된 영역에서 이는 실제 세계 이미지 패치의 분포를 보다 밀접하게 따르는 패치를 생성하기 위해 광도장 표현을 강제합니다. 결과적으로 GANERF는 제한된 적용 범위, 이미지 왜곡 또는 조명 변경과 같은 근본 원인과 관계없이 불완전한 입력 데이터로 인한 NVS의 품질 저하 효과를 현저히 완화합니다. 우리는 NeRF 최적화 중에 공동 적대적 훈련을 제안하여 2D 패치 판별기가 렌더링된 패치의 포토리얼리즘 정도에 대해 NeRF에 알려줍니다. 3D 장면 표현으로의 그래디언트 피드백을 통해 우리는 본질적으로 3D 일관성이 있고 포토리얼리즘한 NeRF 렌더링을 장려하는 동시에 광도장 재구성의 일반적인 불완전성을 줄입니다. 우리는 여러 스케일에서 2D NeRF 렌더링을 처리하는 후속 생성기를 사용하여 출력 품질을 더욱 개선하는 방법을 보여줍니다.이는 장면 이미지의 실제 분포와 더 가깝게 일치하도록 렌더링을 정제함으로써 가능합니다.우리는 새로운 ScanNet++ [Yeshwanth et al. 2023]와 잘 알려진 Tanks and Temples [Knapitsch et al. 2017] 데이터 세트의 까다로운 실내 장면에서 방법을 평가하고 NeRFS 내에서 적대적 공식화를 활용하면 이전 작업보다 이미지 품질이 크게 향상됨을 보여줍니다.모든 테스트 장면에서 성능이 가장 좋은 기준선 [Barron et al. 2022; Tancik et al. 2023] LPIPS(28-48% 감소)와 같은 지각적 지표에 대해 지속적으로 더 나은 PSNR 및 SSIM 점수를 유지합니다.요약하면 다음과 같은 기여를 제공합니다.• 2D 판별기에서 얻은 패치 기반 렌더링 제약 조건을 부과하여 3D 일관성 있는 광도장 표현을 최적화하는 새로운 적대적 공식을 소개합니다.• 렌더링 출력을 더욱 개선하여 어렵고 대규모 장면에서 새로운 뷰 합성에서 최첨단 방법보다 상당한 개선을 보여주는 2D 생성기를 제안합니다.관련 작업 Neural Radiance Fields(NeRFs)는 3D 장면을 체적 함수로 모델링하여 임의의 관점에서 렌더링하여 매우 사실적인 이미지를 생성할 수 있습니다.[Mildenhall et al. 2020]은 이 함수를 다층 퍼셉트론(MLP)으로 인코딩했으며, 최근 작업에서는 폭셀 그리드[Sara Fridovich-Keil 및 Alex Yu 등 2022; Sun 등 2022], 평면 기반 인수분해[Chen 등 2022] 또는 다중 스케일 3D 해시 그리드[Müller 등 2022]와 같은 공간 데이터 구조를 기반으로 하는 대체 표현을 제안했습니다.Mip-NeRF[Barron 등 2021]는 광선 대신 원뿔대 기반의 대체 렌더링 공식을 도입하여 앨리어싱 문제를 해결했으며, 이는 [Barron 등 2022]에서 무한 장면을 설명하기 위해 더욱 확장되었고, [Barron 등 2023]에서 해시 그리드 기반 표현에 적용되었습니다.저희 작업에서는 [Tancik 등 2022]의 Nerfacto 모델을 따릅니다. 2023]은 Instant-NGP의 필드 아키텍처 [Müller et al. 2022]와 MipNeRF-360의 다단계 제안 [Barron et al. 2022]을 결합하여 속도와 품질 간에 적절한 균형을 이룹니다. 2.1 사전 기반 NeRF 이전 섹션에서 논의한 접근 방식은 인상적인 시각적 충실도를 달성하지만 일반적으로 제약이 부족한 장면 영역을 표현하는 데 어려움을 겪습니다. 대부분의 NeRF는 &quot;플로터&quot;와 같은 일반적인 아티팩트를 해결하기 위해 하나 이상의 휴리스틱을 통합합니다. 예를 들어, 정점 밀도를 촉진하는 손실을 추가 [Barron et al. 2021, 2022; Hedman et al. 2021]하거나, 학습 중에 모델에 노이즈를 주입 [Mildenhall et al. 2020]하거나, 표면 매끄러움을 촉진 [Oechsle et al. 2021; Wang et al. 2021; Zhang et al. 2021]합니다. 그러나 데이터가 부족하거나 모호한 경우 모델을 정규화하기 위해 보다 효과적인 사전 지식이 필요합니다.소수 샷 설정에서 기하학적 사전 지식이 특히 효과적인 것으로 나타났습니다.사전 훈련된 모델을 사용하여 예측된 깊이[Deng et al. 2022; Roessle et al. 2022] 또는 표면 법선[Yu et al. 2022]으로 NeRF를 감독할 수 있습니다.RegNeRF[Niemeyer et al. 2022]는 기하학적(표면 매끄러움, 샘플링 공간 어닐링) 및 모양(정규화 흐름) 정규화기를 결합하여 최소 3개의 이미지에서 NeRF를 훈련할 수 있습니다.마찬가지로 SinNeRF[Xu et al. 2022]는 기하학적 및 의미적 의사 레이블을 사용하여 단일 뷰에서 NeRF 재구성을 안내하는 반지도 학습 방식을 제안합니다.저희 작업에서는 기하학적 사전 지식이 미미한 개선만을 제공하는 것으로 관찰된 더 밀도가 높은 커버리지의 장면에 초점을 맞춥니다[Niemeyer et al. 2022]. 다른 연구는 외관 기반 사전에 초점을 맞춥니다. 예를 들어, DiffusioNeRF [Wynn 및 Turmukhambetov 2023]는 RGBD 이미지에서 학습된 2D 노이즈 제거 확산 모델을 사용하여 비지도 손실 항을 구성하여 NeRF가 관찰되지 않은 관점에서 그럴듯한 이미지를 렌더링하도록 합니다. 마지막으로, 일부 접근 방식은 NeRF 재구성을 일반화 문제로 캐스팅하여 장면 기반 사전을 학습합니다. MVSNERF [Chen et al. 2021]는 신경 볼륨에 인코딩된 비용 볼륨을 구성하여 몇 개의 이미지에서만 일관된 렌더링을 허용합니다. PixelNeRF [Yu et al. 2021] 및 GenVS [Chan et al. 2023]는 이미지 인코더를 학습하여 몇 개의 입력 뷰를 새로운 뷰에서 렌더링할 수 있는 3D 인식 신경 표현으로 끌어올립니다. 그러나 이러한 방법은 여전히 훈련 데이터의 가용성에 의해 제한을 받습니다.특히 장면 기반 사전 확률에 기반한 접근 방식은 세부 정보가 부족한 결과를 생성하거나 훈련된 장면 유형에 따라 장면 콘텐츠 및 렌더링 궤적에 강력한 가정을 부과하는 경향이 있습니다.반면에, 우리의 장면별 적대적 최적화 접근 방식은 외부 데이터가 필요 없습니다.2. 이미지 정제를 위한 GANS 생성적 적대 신경망(GANS)[Goodfellow et al. 2014; Karras et al. 2020; Mescheder et al. 2018]은 적대적 손실을 최적화하여 주어진 데이터 분포에서 이미지를 생성하도록 훈련됩니다.원래 무조건 이미지 생성을 위해 제안된 GANS는 이미지 간 변환 및 정제 작업[Isola et al. 2017; Park et al. 2019; Wang et al. 2018a]에도 적용되었으며, 예를 들어 색상화[Anwar et al. 2020], 초고해상도 [Ledig et al. 2017], 인페인팅 [Elharrouss et al. 2020]. 비슷한 설정에서 StyleGAN2 [Karras et al. 2020]에 기반한 조건부 적대 공식을 최적화하여 생성된 이미지를 정제합니다.NeRF Feature Grid f E Volume d σ 렌더링 MLP Multi-View RGB 입력 광선 샘플링 패치 샘플링 GANERF: 판별기를 활용하여 신경 광도장 최적화 다중 해상도 축소.입력 렌더링 패치 생성기 Gw 실제 패치 판별기 Do 실수 또는 → 또는 가짜 정제된 패치 판별기 D 실수 또는 → 또는 가짜그림 2. GANERF 방법 개요: 이 방법은 포즈를 취한 이미지 세트를 입력으로 사용하여 3D 광도장 표현을 최적화합니다. 우리의 핵심 아이디어는 NeRF 재구성 프로세스를 안내하는 적대적 공식에 멀티뷰 패치 기반 재렌더링 제약 조건을 통합하고 조건 생성기 네트워크를 사용하여 렌더링된 이미지를 정제하는 것입니다. 특히 제약이 부족한 영역에서 이는 결과 렌더링 품질을 크게 개선합니다. NERF를 통해 주어진 장면의 데이터 분포와 더욱 밀접하게 일치시킵니다. 이전 작업에서는 알려지지 않은 카메라 포즈[Meng et al. 2021], 객체 및 구성의 생성적 NeRF[Niemeyer and Geiger 2021; Schwarz et al. 2020], 3D 인식 생성기[Kwak et al. 2022; Skorokhodov et al. 2022] 및 장면 생성[Son et al. 2023]과 같은 다양한 설정에 대해 GANS와 NeRF를 결합했습니다. 4K-NeRF[Wang et al. 2022]는 초고해상도를 위해 저해상도 NeRF와 3D 인식 디코더를 결합합니다. 저희 작업에서 판별기에서 전체 해상도 NeRF로의 직접 역전파가 뷰 일관성 있는 새로운 뷰 합성에 필수적이라는 것을 발견했습니다. NeRFLIX [Zhou et al. 2023]에서의 작업과 동시에 관련된 아이디어를 탐구하는데, 이는 비적대적 설정에서 네트워크를 훈련하여 NeRF 아티팩트를 반전합니다. 반면, [Zhou et al. 2023]의 접근 방식은 여러 장면에서 훈련이 필요하고 NeRF 노이즈를 시뮬레이션하기 위해 수작업으로 만든 모델에 의존합니다. 방법 정적 3D 장면을 캡처하는 포즈 입력 이미지 세트가 주어지면 해당 장면의 새로운 뷰를 합성하는 문제에 집중합니다. 저희는 최근의 Neural Radiance Fields(NeRF) [Mildenhall et al. 2020], 특히 [Tancik et al. 2023]의 Nerfacto 모델을 기반으로 구축합니다. 그러나 우리의 아이디어는 다음에 표시된 것처럼 다른 NeRF 아키텍처에도 적용될 수 있습니다.
--- EXPERIMENT ---
s(4.5.2절). 새로운 뷰에서 사실감을 개선하기 위해, 우리 방법(그림 2)은 3D 장면 표현을 직접 업데이트하는 2D 적대적 손실을 활용합니다. 이를 위해 판별기는 훈련 데이터에서 이미지 패치의 분포를 학습합니다. 적대적 훈련을 통해 3D NERF 표현(3.1절)은 이 분포와 일치하는 패치를 렌더링하도록 업데이트됩니다(3.2절). 그에 더해, 2D 생성기는 여러 해상도에서 NeRF 렌더링을 고려하여 두 번째 판별기의 피드백을 기반으로 이를 개선합니다(3.3절). 3.1 NeRF 예비 지식 NeRF 모델은 3D 공간의 각 지점 x = R³에 대해 밀도 σ(x)와 RGB 색상(x, d)을 제공하여 장면을 표현합니다. 색상은 뷰 종속 효과를 설명하기 위해 시야 방향 d = R³에 따라 추가로 달라집니다. 이 광도장 표현(σ0, 50)은 학습 가능한 매개변수 0에 따라 달라지며, 카메라 원점 o € R³에서 d 방향으로 픽셀을 통과하는 광선을 투사하고 밀도장에 따라 분포되는 광선을 따라 예상되는 관찰 색상을 계산하여 이미지의 픽셀을 렌더링할 수 있습니다[Mildenhall et al. 2020]. 형식적으로 광도장(σė, Šė)과 광선 r에 대한 렌더링 함수는 ∞0으로 주어집니다. R₁(r) := √ σ0(rt) exp Exp |- (r)ds](r,d)dt.(1) 여기서 rt = 0 + td는 시간 t에서 광선을 따라 있는 3D 지점입니다. Re(r)은 광선을 따라 밀도와 색상 샘플이 주어지면 적분 법칙으로 근사할 수 있습니다[Mildenhall et al. 2020]. NeRF 매개변수 0은 일반적으로 렌더링된 색상과 기준 진실 색상 간의 차이에 페널티를 주는 예상되는 광선당 평균 제곱 오차를 최소화하도록 최적화됩니다. 구체적으로, p(r, c)로 분포된 광선/색상 쌍 (r, c)가 주어지면 rgb(0) = Ep ||Ro(r) – c||²를 최소화합니다. (2) 상위 첨자 N은 NeRF를 학습하는 데 사용되는 손실과 생성기를 학습하는 데 사용되는 손실을 구분하며, 이는 나중에 정의됩니다. 3. 판별기를 사용한 NeRF 최적화 장면에 반복되는 요소가 많고 표면이 다른 관점에서 보더라도 종종 비슷하게 보이는 경우가 많습니다. 따라서 한 뷰의 패치는 다른 뷰의 패치에 대한 좋은 사전을 형성합니다.Upsample ConvNorm Std Conv 3 ×Norm Std 4. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner Upsample Conv 3 ×Norm Std 8 ×128x256 xRendered Patch 4 xBilinear Downsample Bilinear Downsample Bilinear Downsample Conv 3 xConv 3 ×Conv 3 ×Norm Std Norm Std Norm Std Conv 3 ×Norm Std Upsample Conv 3 ×Norm Std Conv 3 ×Norm Std ↑ 정제된 패치 범례 잡음 추가 잡음 잡음 잡음 잡음(II) 연결 그림 3. 조건 생성기 아키텍처는 여러 해상도에서 작동하는 6개 블록의 기능 추출 피라미드로 구성됩니다.4 ×다운 샘플링 패치에서 시작하여 합성곱 블록은 업샘플링되고 생성 노이즈와 함께 추가되어 다음 특징 추출 수준에 대한 레벨별 특징을 추출합니다. 최종 출력 해상도는 256×256의 입력 해상도와 일치합니다. rgb 뷰. 이 사전을 우리의 훈련 계획에 인코딩하기 위해, 우리는 b를 적대적 목적 L로 보완합니다. Adv(0, 4)는 §로 매개변수화된 보조 신경망 Dø(일명 판별자)를 활용합니다. 이 목적은 판별자가 주어진 이미지 패치가 실제인지 아니면 0으로 매개변수화된 NeRF(일명 가짜)에 의해 생성된 것인지 분류하는 쪽으로 밀려나고, NeRF는 판별자를 속여 현실적인 패치를 렌더링하도록 권장되도록 최적화됩니다. 패치 P는 q(P)에 따라 분포된다고 가정하고, 우리는 광선 집합을 rp로, 패치 P에 해당하는 픽셀 색상을 cp로 표시합니다. 적대적 목적은 적절한 손실로서 NeRF 매개변수 0에 대해 최소화되어야 하지만 판별자 매개변수에 대해 최대화되어야 합니다. [Mescheder et al. 2018], 우리는 비음수 스칼라 op로 균형을 이룬 판별자에 R₁ 그래디언트 페널티 항을 채택하여 다음과 같은 형태의 적대적 목적을 도출합니다. gp Lady(0, 0) = Eq [f(D¢(Rø(rp))) + ƒ(−D$(cp)) N _xl|VD(cp)ll. (3) 여기서 f(x) := log(1 + exp(-x))로 설정하여 선구적 GAN 논문 [Goodfellow et al. 2014]에서 원래 제안한 손실의 정규화된 버전을 얻습니다. 우리는 또한 2D 조건부 GANS [Wang et al. 2016]와 함께 성공을 보여준 VGG 기반 지각적 손실을 채택합니다. 2018b]: (4) Ly perc (0) := Eq [||ÞVGG (Rµ(rp)) – ÞvGG(Cp)||], 여기서 PVGG는 VGG19 네트워크의 최대 풀링 작업 이전의 처음 5개 피처 레이어의 벡터화된 연결이며 [Simonyan 및 Zisserman 2015], 각 레이어는 항목 수의 제곱근으로 정규화됩니다. L perc (0)은 다른 세분성에서 실제 및 가짜 패치 피처의 유사성을 장려합니다. NeRF를 훈련하기 위해 최소화하는 최종 손실은 다음과 같습니다. LN(0) = L (0) + 1 perc L^ perc (0) + 2 Nd max N adv adv (0,0), (5) ΦΕΦ rgb N 여기서 ☀는 가능한 판별자 집합을 나타내고 N은 다양한 손실에 대한 음이 아닌 밸런싱 요소입니다. 이 목적은 모든 학습 이미지(각각 p 및 q 분포)에서 균일하게 샘플링된 광선 및 패치를 결합하는 배치에서 계산된 확률적 기울기로 최적화됩니다.또한, GAN 학습 방식에서 일반적으로 수행되는 것처럼 안장점 문제를 해결하기 위해 NeRF와 판별기 사이에서 매개변수 업데이트를 번갈아 가며 수행합니다.3. 조건 생성기 지금까지 아티팩트는 3.2절에서 소개한 판별기 안내를 통해 3D에서 직접 복구되었습니다.그러나 합성된 이미지를 후처리하여 2D에서 렌더링 품질을 더욱 개선할 수 있습니다.이를 위해 NeRF 렌더링과 보조 랜덤 벡터 z를 입력으로 사용하여 정리된 이미지를 생성하는 w로 매개변수화된 조건 생성기 Gw를 설계하여 확률적 잡음 제거기 역할을 합니다.이 아키텍처는 StyleGAN2 생성기[Karras et al. 2020]에서 영감을 받았지만 조건 생성기에 적용하고 매핑 네트워크를 생략했습니다(그림 3). 우리는 z에 따라 가산 노이즈를 유지하는데, 이는 pix2pix [Isola et al. 2017]가 조건부 GAN 설정에서 무작위성의 원천으로 드롭아웃을 사용하는 방식과 정신적으로 비슷합니다.그림 3에서 볼 수 있듯이, 조건부 입력 패치는 2배로 6배 선형적으로 다운샘플링되어 다중 스케일 분석이 가능합니다.다운스케일링된 패치는 합성곱 계층을 통해 32개의 피처 채널로 인코딩되고, 이는 해당 스케일의 생성기 계층 출력에 연결됩니다.StyleGAN2에서와 같이, 우리는 누수 ReLU 활성화, 입력/출력 건너뛰기 연결 및 정규화 계층을 사용합니다.가산 노이즈는 정규화 계층 뒤에 주입되고, 노이즈는 학습 가능한 계층별 인자로 추가로 스케일링됩니다.NeRF 최적화와 유사하게, 생성기는 정규화된 적대적 설정에서 훈련됩니다.여기에는 ¼로 매개변수화된 두 번째 판별자 Dy가 포함되며, 이는 이전에 도입된 Do와 다릅니다. 실제로 NeRF와 조건 생성기는 독립적인 판별기를 통해 보다 효과적으로 해결할 수 있는 출력 이미지에 여러 유형의 오류를 도입합니다. 생성기를 학습하는 데 사용된 적대적 목적은 다음과 같은 형식을 취합니다. adv Lady(w, 4|0) := Eq,n [f(Dy(Gw(R₁0(rp), z))) +f(−D4 (cp)) − 2gp||VD4 (CP)||], (6) 여기서 기대값은 표준 정규 분포 n(z)로 분포된 z에 대한 추가적으로 취해집니다. 적대적 GANERF: 판별기를 활용하여 신경 광도장 최적화 목적은 생성기의 매개변수 w에 대해 최소화되어야 하지만 판별기 매개변수에 대해 최대화되어야 합니다. 또한, 우리는 NeRF 매개변수 e를 통해 그래디언트를 멈추는데, 이는 10으로 표시됩니다. 적대적 목적은 NeRF를 훈련하는 데 사용된 동일한 지각적 손실, 즉 Lperc(w|0) = Eq,n[||ÞVGG(Gw(R10(rp), z)) – ÞVGG(CP)||] (7) 및 조건부 GAN pix2pix[Isola et al.]에서와 같이 L₁ 색상 손실로 보완됩니다. 2017], 패치에서 작동하며 다음과 같이 정의됩니다.Logb (w10) === = Eq‚n [||Gw(R₁0(rp), z) – cp||1] · 전체 생성기 손실은 LG (@10*) := 10 perc Lpperc (@10*) + 10gb Logb (@10*) rgb+ max (w, 0*), (9) ψεΨ adv로 정의됩니다.여기서 Y&#39;는 가능한 판별자 집합을 나타내고, 16*은 손실 구성 요소를 균형 잡는 음이 아닌 요인이며, 0*은 LN을 최적화하여 얻은 NeRF의 매개변수화입니다.LN에 대해 설명한 것과 유사하게 생성기와 판별자의 업데이트를 번갈아가며 수행하여 안장점 문제를 해결합니다.실험 4.1 데이터 집합 및 메트릭 4.1.1 ScanNet++.ScanNet++ [Yeshwanth et al. 2023] 데이터 세트에서 사무실, 실험실 및 아파트 환경으로 구성된 5개의 실내 장면을 평가합니다. 각 장면에는 평균 ~ 700개의 이미지가 포함되어 있으며, 그 중 ~ 20개가 테스트 세트로 정의됩니다. NeRF 작업에서 일반적으로 사용되는 다른 데이터 세트(예: LLFF [Mildenhall et al. 2020] 또는 [Barron et al. 2022]의 360° 장면)와 달리 ScanNet++의 테스트 뷰는 모델의 일반화 기능을 명시적으로 평가하기 위해 교육 뷰와 비교하여 공간적으로 분포 밖에 있도록 선택됩니다. 구조에서 동작으로 제공된 카메라 포즈가 있는 DSLR 이미지[Schönberger and Frahm 2016]를 사용하고 왜곡을 해제하고 768 × 1152로 크기를 조정합니다. 4.1.2 탱크와 사원. 더 큰 규모의 재구성을 위한 벤치마크로서 탱크와 사원의 고급 장면 세트에서 4개의 장면을 고려합니다[Knapitsch et al. 2017] 데이터 세트: 강당, 무도실, 법정 및 박물관. 이러한 장면은 세부적인 기하학과 복잡한 조명이 있는 대규모 실내 환경을 묘사합니다. 각 장면에서 사용 가능한 이미지의 10%를 테스트 세트로 무작위로 선택하여 평균 270개의 훈련 및 ~테스트 뷰로 나눕니다. 1080 x 1920의 원래 해상도를 사용합니다. 4.1.~ 평가 지표. PSNR, SSIM [Wang et al. 2004] 및 LPIPS [Zhang et al. 2018]의 세 가지 주요 시각적 품질 지표에 따라 결과를 평가합니다. 이미지 생성 관련 문헌의 이전 연구에 따라 모델의 출력이 장면의 시각적 분포와 얼마나 일치하는지 측정하는 KID [Bińkowski et al. 2018] 점수도 보고합니다. 4.2 훈련 및 구현 세부 정보 4.2.NeRF. 각 훈련 반복에서 우리는 단일 랜덤 입력 이미지에서 4096개의 랜덤 광선과 256 × 256 패치를 렌더링합니다. 지각적 손실은 패치를 전체적으로 처리하는 반면, 적대적 손실은 겹치지 않는 64 × 64 크기의 16개의 더 작은 패치로 세분화합니다. Do는 StyleGAN2 판별기[Karras et al. 2020]로, 합성 채널 수를 절반으로 줄여 64 × 64 패치를 처리하도록 조정되었습니다. 생성된 &quot;가짜&quot; 패치마다 판별기에는 실제 패치도 제공됩니다. 광도장과 판별기 Dô는 Adam 옵티마이저[Kingma and Ba 2015]와 RMSprop[Tieleman and Hinton. 2012]를 사용하여 400k 반복 동안 훈련되었으며 학습률은 각각 1 × 10−2와 1 × 10¯³입니다. 손실 가중치는 ✗N = 0.0003, 1 perc = 0.0003 및 p = 0.1로 설정됩니다.adv = 4.2.2 조건 생성기.다운 샘플링 전 초기 학습 패치 크기는 256×256입니다.그러나 추론 시점에서 생성기는 NeRF 렌더링을 정제하기 위해 고해상도 이미지에서 완전히 합성곱적으로 실행할 수 있습니다.생성기는 8의 배치 크기를 사용하여 학습됩니다.인지적 손실은 전체 학습 패치에서 작동하는 반면, 적대적 손실은 각 패치를 겹치지 않고 크기가 128×128인 4개의 작은 패치로 세분화하므로 D₁는 배치 크기 32로 학습됩니다.판별기는 StyleGAN2[Karras et al. 2020]에서 영감을 받았으며 128x128 패치를 처리하도록 조정되었습니다. 생성기와 판별기 D₁는 모두 학습률 2 × 10-³의 Adam 최적화기[Kingma 및 Ba 2015]를 사용하여 3000에포크 동안 학습됩니다.손실 가중치는 10 perc = 1.0, = 3.0 및 Ap 4.Baseline Comparisons rgb = 5.0으로 설정됩니다.우리는 우리의 접근 방식을 최근 문헌의 여러 기준선과 비교합니다.우리는 시각적 품질과 속도에 맞게 각각 최적화된 NeRF 아키텍처의 현재 상태를 대표하는 Mip-NeRF 360[Barron et al. 2022]과 Instant NGP[Müller et al. 2022]를 선택합니다.또한 우리는 제안하는 접근 방식의 기반이 되는 Nerfacto[Tancik et al. 2023] 모델의 네 가지 변형을 평가합니다.i Nerfacto: 수정 사항이 없는 기준선 모델. ii Nerfacto + 추가 용량: 이 고용량 변형은 MLP의 은닉 차원 수를 두 배로 늘리고, 그리드 해상도를 두 배로 늘리고, 기본 Nerfacto에 비해 해시 테이블 크기를 4배로 늘립니다.이로 인해 ~ 44M 개의 학습 가능한 매개변수가 생성되는데, 이는 우리 모델(NeRF + 생성기)보다 ~ 25% 더 많고 기본 Nerfacto 매개변수 수의 ~ 3.4배(~ 12.9M)입니다.iii Nerfacto + pix2pix: Nerfacto 결과는 pix2pix [Isola et al. 2017] 생성기로 개선됩니다.공식 pix2pix 저장소에서 Wasserstein 목적 및 그래디언트 페널티 [Gulrajani et al. 2017]로 학습한 9개의 ResNet [He et al. 2016] 블록을 사용하는 생성기가 가장 좋은 성능을 보인다는 것을 발견했습니다.우리는 이 모델을 VGG 지각 손실로 동일하게 학습하여 성능을 개선했습니다. iv Nerfacto + ControlNet: Nerfacto 결과는 [Hecong 2023]의 구현을 사용하여 Nerfacto 렌더링 및 LORA [Hu et al. 2022] 미세 조정에 대한 ControlNet [Zhang and Agrawala 2023] 조건과 Stable Diffusion [Rombach et al. 2022]에 의해 개선됩니다. 또한 최근 작업인 4K-NeRF [Wang et al. 2022]와 비교합니다. 공정한 비교를 위해 모든 베이스라인은 수렴될 때까지 학습됩니다. 표 1과 표 2에서 볼 수 있듯이, 우리의 접근 방식은 눈에 띄는 6. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner 표 1. 5개 ScanNet++ 장면에 대한 정량적 결과. 영어: 우리의 방법은 지속적으로 더 나은 PSNR 및 SSIM 점수를 유지하면서도 LPIPS 및 KID와 같은 지각적 지표에서 기준선을 큰 폭으로 능가합니다.표 3. Tanks와 Temples의 강당 장면에 대한 소거 연구.우리 방법의 개별 부분을 제거했을 때 성능이 저하되는 것은 우리의 설계 선택을 확인시켜줍니다.방법 Mip-NeRF 360 [Barron et al. 2022] Instant NGP [Müller et al. 2022] 4K-NeRF [Wang et al. 2022] 24.25.22.Nerfacto [Tancik et al. 2023] 25.Nerfacto 추가 용량 25.Nerfacto+pix2pix [Isola et al. 2017] 24.0.0.854 0.0.Nerfacto ControlNet [Zhang 및 23.0.PSNR↑ SSIM↑ LPIPS↓ KID↓ 0.862 0.225 0.0.0.269 0.0.807 0.254 0.0.848 0.245 0.0.0.0.Method PSNR↑ SSIM↑ LPIPS↓ KID↓ Nerfacto [Tancik et al. 2023] 저희 21.22. 판별기 없음 21.0.843 0.304 0.0.862 0.158 0.0.857 0.178 0. 생성기 없음 21.0.854 0.247 0. NeRF 학습 w/o adv. 손실 21.0.0.175 0.Agrawala 2023] 판별기 없는 우리 25.생성기 없는 우리 25.0.0.0.0.177 0.비율 없는 손실 21.0.0.187 0.생성기 학습 우리 0.198 0.26.1 0.864 0.161 0.고해상도 패치 없음 22.저해상도 패치 없음 22.0.851 0.0.862 0.RGB 인코딩 없음 22.표 2. 4개의 탱크와 사원 장면에 대한 정량적 결과.우리의 방법은 지각적 지표에서 특히 강력한 개선을 달성하여 새로운 뷰 렌더링의 시각적 세부 사항과 선명도를 개선합니다.비율 없는 손실, 광고 없음. 손실 22.w/o RGB 손실 21.21.0.0.0.0.860 0.167 0.0.861 0.174 0.0.834 0.184 0.0.163 0.방법 Mip-NeRF 360 [Barron et al. 2022] PSNR↑ SSIM↑ LPIPS↓ 18. 인스턴트 NGP [Müller et al. 2022] 19.4K-NeRF [Wang et al. 2022] 19.Nerfacto [Tancik et al. 2023] 19.0.Nerfacto 추가 용량 19.Nerfacto+pix2pix [Isola et al. 2017] 20.Nerfacto ControlNet [Zhang 및 19.Agrawala 2023] 판별기가 없는 우리의 것 생성기가 없는 우리의 것 20.19.20.KID↓ 0.709 0.327 0.0.700 0.369 0.0.656 0.356 0.0.329 0.0.733 0.291 0.0.0.0.0.706 0.213 0.0.0.192 0.0.739 0.251 0.0.776 0.169 0.기준선과 비교했을 때 개선됨. 특히 두 가지 지각적 지표(LPIPS, KID)는 상대적으로 가장 큰 개선을 보였으며, 이는 우리의 접근 방식이 색상 유사도 지표(PSNR, SSIM)로 종종 제대로 측정되지 않는 많은 작은 시각적 아티팩트를 수정할 수 있음을 시사합니다. 이는 그림 4~6에 표시된 정성적 평가에서도 확인됩니다. Nerfacto + ControlNet은 높은 지각적 품질(LPIPS, KID)을 달성하지만, 결과는 뷰 일관성이 낮습니다(4.5.1절 참조). 이는 낮은 PSNR에도 반영됩니다. 4. 절제 실험 추가된 구성 요소의 효과를 확인하기 위해 ScanNet++ 및 Tanks and Temples 데이터 세트에 대한 절제 연구를 수행합니다. 정량적 결과(표 1 및 표 2)와 그림 7의 정성적 결과는 우리 방법의 전체 버전이 가장 높은 성능을 달성함을 보여줍니다. 4.4. 판별기 없음. 이 실험에서 우리는 RGB 손실만으로 광도장을 최적화하고 생성기를 순수한 후처리 단계로 렌더링에 적용하는 것의 영향을 조사합니다. 우리는 장면의 지오메트리와 텍스처 세부 정보가 전체 방법으로 달성한 것과 같은 정도로 복구될 수 없다는 것을 관찰합니다(그림 7). 모든 메트릭의 감소(표 1 및 2의 &quot;판별기 없음&quot;)는 3D에서 NeRF 표현을 알리는 패치 기반 감독의 중요성을 명확히 강조합니다. 이는 3D 표현에 그래디언트 역전파를 통합하는 당사 방법이 순수한 2D 후처리 접근 방식을 능가한다는 것을 나타냅니다. 4.4.2 생성기 없음. 생성기를 생략하면 결과가 덜 선명해집니다(예: 그림 7의 샘플 4, 5 및 7). 이로 인해 방법의 전체 버전에 비해 성능이 낮아집니다(표 1 및 2의 &quot;생성기 없음&quot;). 이는 생성기가 고해상도 렌더링을 달성하는 데 도움이 됨을 보여줍니다. 4.4.3 NeRF 학습. 표 3 및 5는 NeRF 최적화에 대한 보다 자세한 절제를 제공하며, 이는 적대적 손실을 삭제(&quot;adv. loss 없음&quot;)하거나 지각적 손실을 삭제(&quot;perc. loss 없음&quot;)하면 성능에 부정적인 영향을 미친다는 것을 보여줍니다. 4.4.4 생성기 학습. 생성기 학습에 대한 자세한 절제는 표 3과 5에 나와 있습니다. 우리는 모델에서 특정 입력 패치 해상도를 생략한 영향을 조사합니다. 6가지 해상도 중 가장 높은 두 해상도를 제거하면 눈에 띄는 성능 저하(&quot;고해상도 패치 없음&quot;)가 발생하여 생성기에 고해상도 입력이 필수적인 역할을 한다는 것을 강조합니다. 반대로 가장 낮은 두 해상도를 제거하면(&quot;저해상도 패치 없음&quot;) 품질이 저하되어 생성기도 저해상도 입력에 의존한다는 것을 알 수 있습니다. 또한 작은 RGB 인코더, 즉 다중 해상도 패치의 하나의 합성곱 계층(그림 3)이 생성기에 이롭다는 것을 보여줍니다(&quot;RGB 인코딩 없음&quot;). 나아가 절제 실험은 개별 손실 함수, 즉 적대적 손실(&quot;adv. loss 없음&quot;), 지각적 손실(&quot;perc. loss 없음&quot;), L1 RGB 손실(&quot;RGB loss 없음&quot;)을 조사합니다. 결과는 이러한 손실 함수 각각이 생성기의 전반적인 성능에 상당히 기여한다는 것을 보여줍니다.4.4. 감소된 이미지 수.표 4는 가장 큰 장면(800개 이미지)의 이미지 수를 400, 200, 100, 50, 25개로 줄였을 때의 결과를 나열하는데, 이는 실내 규모의 장면에서는 극히 희소합니다.이는 우리 방법이 더 밀도가 높은 설정에서 관찰된 것과 유사한 마진으로 Nerfacto보다 지속적으로 더 우수한 성능을 보인다는 것을 보여줍니다.Mip-NeRFInstant NGP Nerfacto GANERF: 판별기를 활용하여 신경 복사장 최적화우리의 기준 진실 그림 4. 탱크와 사원에서의 비교.우리의 방법은 바닥의 얇은 구조물이나 패턴과 같이 기준선보다 더 많은 세부 정보를 복구합니다. 8. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner ALP Jeal demity Mip-NeRFInstant NGP Nerfacto Ours Ground truth 그림 5. Scannet++에서의 비교. 저희 방법은 안개가 덜한 아티팩트를 생성하여 베이스라인에 비해 더 선명한 렌더링을 제공합니다. 4K-NeRF Nerfacto+pix2pix Nerfacto + ControlNet GANERF: 판별기를 활용하여 신경 복사장 최적화 저희의 Ground truth 그림 6. Tanks와 Temples에서의 비교. NERF 표현에 대한 역전파와 효과적인 생성기 컨디셔닝을 사용하여 저희의 새로운 뷰는 생성 모델을 기반으로 하는 다른 접근 방식과 비교했을 때 Ground truth 패턴과 매우 일치합니다. 표 4. 큰 ScanNet++ 장면에서 이미지 수가 줄어든 Ablation 연구. 영어: 입력 희소성 수준에 관계없이 우리 방법은 지속적으로 Nerfacto보다 비슷한 마진으로 우수한 성능을 보입니다.표 5. ScanNet++ 장면에 대한 절제 연구. 우리 방법의 개별 부분을 제거하면 성능이 저하되어 다양한 아키텍처 선택과 학습 전략의 중요성을 나타냅니다.방법 #images PSNR↑ SSIM↑ LPIPS↓ KID↓ 방법 PSNR↑ Nerfacto [Tancik et al. 2023]우리의 Nerfacto [Tancik et al. 2023] 우리의24.2 0.844 0.247 0.24.7 0.870 0.169 0.24.2 0.843 0.252 0.24.3 0.864 0.169 0.Nerfacto [Tancik et al. 2023] 우리 24.24.판별기 없음 24.생성기 없음 24.SSIM↑ LPIPS↓ 0.844 0.247 0.0.870 0.169 0.0.859 0.188 0.0.865 0.201 0.KID↓ Nerfacto [Tancik et al. 2023] 23.우리 23.0.825 0.274 0.0.862 0.182 0.Adv.손실 없는 NeRF 학습 24.Nerfacto [Tancik et al. 2023] 우리 22.22.0.0.0.307 0.0.216 0.perc 없음. 손실 24.0.867 0.170 0.0.866 0.184 0. 생성기 훈련 Nerfacto [Tancik et al. 2023] 우리의 20.20.0.Nerfacto [Tancik et al. 2023] 우리의 15.16.0.771 0.347 0.0.282 0.0.686 0.524 0.0.727 0.408 0. 고해상도 패치 없음 24.0.0.202 0. 저해상도 패치 없음 24.0.870 0.171 0. RGB 인코딩 없음 24.6 0.870 0.171 0. 사전 손실 없음 perc 없음 손실 w/o RGB 손실 24.24.24.0.869 0.175 0.0.858 0.170 0.0.869 0.170 0.4.4.5.평가 뷰 일관성.뷰 일관성 결과를 얻으려면 먼저 NeRF로 역전파하고 기본 3D 표현을 최적화하는 것이 필수적입니다.[Lai et al. 2018]에 따라 표 6은 광학 흐름으로 계산된 테스트 뷰의 일관성을 보여줍니다[Teed and Deng 2020].역전파가 없는 NeRF(즉, &quot;판별기 없음&quot;)에 대한 Nerfacto와 우리 방법을 비교하면 생성기가 시각적 품질(즉, KID)을 크게 개선하는 반면, 미비한 불일치를 추가합니다.그러나 전체 방법은 3D 표현으로 역전파하여 추가된 불일치를 절반 이상 줄입니다. 비디오 결과는 불일치가 거의 눈에 띄지 않고 질적 개선이 지배적임을 보여줍니다. ControlNet(Nerfacto + ControlNet)을 사용한 대체적 정제는 높은 시각적 품질을 달성하지만 정제된 뷰는 매우 불일치합니다. [Wang et al. 2022]에 따라 그림 8은 Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner의 뷰 일관성을 시각화합니다. Nerfacto 판별자가 없는 우리 생성기가 없는 우리 우리의 실제 결과 To Fory Gallery 그림 7. Tanks와 Temples의 소거 실험. &quot;구별자가 없는 우리 방식&quot;은 패턴(샘플 4 및 5)에서 상당히 어려움을 겪고 얇은 구조(샘플 3 및 6)를 놓칩니다. &quot;생성기가 없는 우리 방식&quot;은 패턴을 더 잘 복구하지만 전체 방식(샘플 4, 5 및 7)에 비해 흐릿한 결과를 생성합니다. 표 6. ScanNet++에서 뷰 일관성 평가. 우리 방식의 새로운 뷰는 뷰 일관성이 매우 높고 시각적 품질이 높습니다(즉, KID). 가장 좋은 결과와 두 번째로 좋은 결과가 강조 표시됩니다. GANERF: 구분자를 활용하여 신경 광도장 최적화 •첫 번째 프레임 마지막 프레임 첫 번째 프레임에서 마지막 프레임까지 시간에 따른 픽셀 열 방식 뷰 일관성 MSE ↓ KID↓ Nerfacto [Tancik et al. 2023] 0.0.Nerfacto ControlNet [Zhang and Agrawala 2023] 0.0.구별자가 없는 우리 방식 0.0.생성기가 없는 우리 방식 우리 방식 0.0.0.0.Table 7. ScanNet++에서 다양한 NeRF 백본 사용.우리의 방법은 Instant NGP와 같은 대체 NeRF 표현을 사용하는 데 유연합니다.기본 접근 방식은 Nerfacto를 기반으로 합니다.방법 PSNR SSIM↑ LPIPS↓ KID↓ Instant NGP [Müller et al. 2022] Instant NGP를 사용한 우리 방법 25.25.0.844 0.269 0.0.857 0.173 0.Nerfacto [Tancik et al. 2023] 우리 방법 25.26.0.848 0.245 0.0.864 0.161 0.표 8. Scan Net++에서의 런타임 비교.렌더링 시간은 Nerfacto와 경쟁력이 있습니다.학습 시간은 4개의 GPU에서 학습된 Mip-NeRF와 비슷할 가능성이 높은 반면, 우리 방법은 단일 GPU에서 학습되었습니다.방법 Mip-NeRF 360 [Barron et al. 2022] 인스턴트 NGP [Müller et al. 2022] Nerfacto [Tancik et al. 2023] Ours 렌더링 시간↓ 프레임당(768x1152) 24초 1x NVIDIA VTraining 시간↓ 18시간 4x NVIDIA V3.8시간 3.5시간 ✗ 2일 10시간 VICIIAN 0.47초 RTX A0.87초 0.89초 VICIIAN RTX A 생성기 없는 Ours 판별기 없는 Ours Nerfacto ControlNet Nerfacto가 비디오 프레임 시퀀스에서 픽셀 열을 추적: Nerfacto + ControlNet의 불일치는 수직 줄무늬 패턴으로 명확히 드러납니다. 우리 방법의 전체 버전은 그림 8에서 뷰 일관성과 시각적 품질의 최상의 조합을 보여줍니다. 4.5.2 대체 NeRF 백본. 우리는 Nerfacto 복사장 표현에 기반하여 방법을 구축하지만, 다른 NeRF 백본으로 그래디언트를 역전파하는 것이 유연합니다. Instant NGP를 대체 NeRF로 사용하면 Nerfacto를 사용하는 것과 비교하여 모든 메트릭에서 비슷한 규모의 개선이 이루어집니다(표 7). Instant NGP 백본을 사용하면 Sec. 4.2에서 설명한 대로 매개변수를 사용하지만 Instant NGP 렌더링의 메모리 소비가 더 높기 때문에 패치 크기를 192 x 192로 줄이고 여기서 판별자를 위해 4개의 64 x 64 패치를 잘라냅니다. 그런 다음 손실 가중치를 λN, = 0.001, 1: = 0.001 및 N = 0.1로 설정합니다. &#39;perc 4.5.adv gp 런타임. 표 8은 학습 및 렌더링 시간을 비교합니다. 우리 방법의 렌더링 시간은 기본 NeRF의 렌더링 시간에 의해 지배되는데, 생성기 순방향 패스가 비교적 빠르기 때문입니다. 즉, 13ms만 걸립니다. 따라서 우리의 방법은 Nerfacto와 실질적으로 동일한 속도로 새로운 뷰를 렌더링하고 Mip-NeRF 360보다 훨씬 빠릅니다. Instant NGP를 대체 NeRF 백본으로 사용하면(4.5.2절) Instant NGP 렌더링 시간으로 렌더링 속도가 빨라집니다. 우리의 훈련 시간은 Nerfacto나 우리의 것보다 느립니다. 그림 8. 뷰 일관성 시각화. 비디오 프레임 시퀀스에서 각 프레임의 동일한 위치에 있는 픽셀 열을 추출하고(왼쪽) 수평으로 연결하여 뷰 일관성을 시각화합니다(오른쪽). Instant NGP; 그러나 우리의 방법은 시각적 품질을 크게 개선합니다. 모든 방법은 수렴할 때까지 훈련됩니다. 4.5.4 시각적 품질에 대한 뷰 커버리지의 영향. 그림 9-왼쪽은 새로운 뷰의 PSNR이 뷰 커버리지와 함께 증가하고 75개 이상의 훈련 뷰에서 관찰된 영역에서 정체됨을 보여줍니다. Nerfacto에 대한 우리 방법의 개선(그림 9-오른쪽)은 &lt; 25 훈련 PSNR에서 관찰된 낮은 커버리지 영역에서 두 배나 높습니다. 우리의 방법 25.12. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner 0.6Nerfacto보다 향상됨 RA0.5 ALU B22.20.A PSNR 0.4 0.3V2A ALU Späne Späne 17.5+0.2 - #개의 학습 뷰에서 관찰됨 #개의 학습 뷰에서 관찰됨 그림 9. ScanNet++ 장면에서 PSNR에 대한 뷰 커버리지의 영향(왼쪽)/Nerfacto보다 향상됨(오른쪽). 뷰 커버리지는 기준 진실 깊이를 사용하여 계산되고 테스트 뷰에서 픽셀로 투사되는 #개의 학습 뷰를 계산합니다. Nerfacto ALPIPS:10.LPIPS 향상으로 오버레이된 기준 진실 그림 10. Nerfacto보다 인지적 향상. 우리의 방법은 질감이 없는 표면에 비해 물체나 바닥 패턴이 있는 구조화된 영역을 특히 향상시킵니다.LPIPS는 패치에서 계산됩니다. 뷰, 더 높은 커버리지 영역과 비교.그림 10은 합성 방식으로 테스트 뷰의 작은 96 x 96 패치에서 LPIPS를 계산하여 LPIPS 측면에서 개선 사항을 추가로 시각화합니다.이것은 특히 질감이 없는 벽보다는 테이블 위 물체나 반복적인 바닥 패턴과 같이 구조가 풍부한 영역에서 개선이 발생한다는 것을 보여줍니다.4.5.5 GAN 환각 효과.재구성을 위해 생성적 접근 방식을 사용하면 뷰와 일관되지 않을 수 있는 환각 콘텐츠의 위험이 따릅니다.NeRF 표현과 생성기 컨디셔닝으로의 역전파를 통해 새로운 Ours Nearby 학습 뷰 그림 11. GAN 환각 효과.비문의 재구성은 어려워서 환각 문자(왼쪽)가 발생할 수 있습니다. 뷰는 뷰 일관성이 매우 높고(4.5.1절) 기준 진실과 밀접하게 일치합니다(예: 그림 6의 어려운 패턴). 그러나 우리는 쓰여진 텍스트를 재구성하는 것이 매우 어렵다는 것을 관찰했으며, 이는 그림 11에서와 같이 환각적이고 잘못된 문자로 이어질 수 있습니다. 4.5.6 보이지 않는 뷰의 판별기. 훈련 뷰가 아닌 샘플링된 포즈에서 가짜 패치를 렌더링하면 NeRF 표현에 추가 판별기 피드백을 제공할 수 있는 잠재력이 있습니다. 그러나 훈련 포즈 주변의 가우시안에서 포즈를 샘플링하여 가짜 패치를 렌더링하려는 시도는 평균 성능에 변화가 없었습니다. 추가 감독의 이점을 얻는 패치를 샘플링하려면 보다 정교한 전략이 필요할 것입니다. 4. 제한 사항 우리의 결과는 최첨단 방법에 비해 상당한 개선을 이룰 수 있음을 보여줍니다. 동시에 여전히 중요한 제한 사항이 있다고 생각합니다. 예를 들어, 현재 패치 판별기는 장면별로 훈련됩니다. 그러나 기능을 개선하기 위해 더 큰 장면 코퍼스에 액세스할 수 있는 보다 일반적인 사전 학습을 훈련하는 것이 유익할 것입니다. 가능하지만, 순진한 일반화 가능 판별기는 장면 분류기로 축소되는 경향이 있습니다. 즉, 주로 패치가 같은 장면에 속하는지 여부를 식별합니다. 이 문제를 극복하기 위한 한 가지 가능한 해결책은 각 학습 장면에 대한 NERF 표현을 동시에 학습하는 것입니다. 이는 상당한 계산 비용을 수반합니다. 또 다른 한계는 현재 정적 장면에만 초점을 맞추고 있다는 것입니다. 그러나 [Işık et al. 2023; Kirschstein et al. 2023; Park et al. 2021; Tretschk et al. 2021]과 같은 최근의 변형 가능하고 동적 NeRF 접근 방식으로 접근 방식을 확장하는 것이 흥미로울 것이라고 생각합니다. 5
--- CONCLUSION ---
우리는 신경 복사장의 적대적 최적화를 위한 새로운 접근법인 GANERF를 소개합니다. 우리 접근법의 주요 아이디어는 복사장 재구성에 패치 기반 렌더링 제약 조건을 부과하는 것입니다. 장면 패치 판별기에서 그래디언트를 역전파하여 기존 NeRF 방법에서 비롯된 일반적인 불완전성과 렌더링 아티팩트를 효과적으로 해결합니다. 특히, 적용 범위가 제한된 영역에서 이는 렌더링 품질을 크게 개선하여 질적, 양적으로 최첨단 방법보다 성능이 뛰어납니다. 동시에, 우리의 방법은 렌더링 사전 확률과 하이엔드 신규 뷰 합성을 결합하기 위한 발판일 뿐입니다. 예를 들어, 장면 전체에 걸쳐 일반화하면 이 작업에서 제안된 것과 유사한 아이디어를 활용할 수 있는 수많은 기회가 제공될 것이라고 믿습니다. GANERF: 판별기를 활용하여 신경 복사장 최적화 감사의 말 이 작업은 Meta SRA에서 자금을 지원했습니다. Matthias Nießner는 또한 ERC Starting Grant Scan2CAD(804724)에서 지원을 받았습니다. 비디오 내레이션을 해주신 Angela Dai에게 감사드립니다. 참고문헌 Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz Khan, Abdul Wahab Muzaffar. 2020. Image colorization: A survey and dataset. arXiv 사전 인쇄본 arXiv:2008.10774(2020). Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, Pratul P. Srinivasan. 2021. Mip-NeRF: 앤티 앨리어싱 신경 광도 필드에 대한 다중 스케일 표현. ICCV(2021). Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. 2022. Mip-NeRF 360: 무한한 앤티 앨리어싱 신경 복사장. CVPR(2022). Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, Peter Hedman. 2023. Zip-NeRF: 앤티 앨리어싱 그리드 기반 신경 복사장. arXiv 사전 인쇄본 arXiv:2304.06706(2023). Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, Arthur Gretton. 2018. mmd gans의 신비 해제. arXiv 사전 인쇄본 arXiv:1801.01401(2018). Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, Gordon Wetzstein. 2023. GeNVS: 3D 인식 확산 모델을 사용한 생성적 새로운 뷰 합성. arXiv에서. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su. 2022. TensoRF: 텐서리얼 광도 필드. 유럽 컴퓨터 비전 컨퍼런스(ECCV)에서. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su. 2021. Mvsnerf: 멀티뷰 스테레오에서 빠르게 일반화 가능한 광도 필드 재구성. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록에서. 14124-14133. Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan. 2022. 심층 감독 NeRF: 더 적은 뷰와 더 빠른 무료 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스의 진행 중. Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, Younes Akbari. 2020. 이미지 인페인팅: 리뷰. Neural Processing Letters 51(2020), 2007-2028. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 2014. 생성적 적대적 네트워크. 신경 정보 처리 시스템의 발전, Vol. 27. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville. 2017. Wasserstein GAN의 개선된 훈련. 제31회 신경 정보 처리 시스템 국제 컨퍼런스(미국 캘리포니아주 롱비치)(NIPS&#39;17)의 회의록. Curran Associates Inc., 미국 뉴욕주 레드훅, 5769-5779. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016. 이미지 인식을 위한 심층적 잔차 학습. 2016년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR). 770-778. https://doi.org/10.1109/CVPR.2016.Wu Hecong. 2023. ControlLoRA: 안정된 확산 공간 정보를 제어하는 경량 신경망. https://github.com/HighCWu/ControlLoRA Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, Paul Debevec. 2021. 실시간 뷰 합성을 위한 신경 복사 필드 베이킹. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 5875-5884. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022. LORA: 대규모 언어 모델의 저순위 적응. 국제 학습 표현 컨퍼런스에서. Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner. 2023. HumanRF: 움직이는 인간을 위한 고충실도 신경 복사 필드. ACM Transactions on Graphics(TOG) 42,(2023), 1-12. https://doi.org/10.1145/Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros. 2017. 조건부 적대 네트워크를 사용한 이미지 간 변환. CVPR(2017). Justin Johnson, Alexandre Alahi, Li Fei-Fei. 2016. 실시간 스타일 전송 및 초고해상도를 위한 지각적 손실. 컴퓨터 비전 - ECCV 2016, Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling(편집자). 694-711. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila. 2020. StyleGAN의 이미지 품질 분석 및 개선. Proc. CVPR. Diederik P. Kingma 및 Jimmy Ba. 2015. Adam: 확률적 최적화 방법. CORR abs/1412.6980(2015). Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter 및 Matthias Nieẞner. 2023. NeRSemble: 인간 머리의 다중 시점 광도 필드 재구성. https://doi.org/10.48550/arXiv.2305.03027 arXiv:2305.03027 [cs.CV] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou 및 Vladlen Koltun. 2017. 탱크와 사원: 대규모 장면 재구성 벤치마킹. ACM Transactions on Graphics (ToG) 36, 4 (2017), 1-13. 곽정기, 리위안밍, 윤동식, 김동현, 데이비드 한, 고한석. 2022. 편집 가능한 인물 이미지 합성을 위해 StyleGAN에 제어 가능한 NeRF-GAN의 3D 인식 주입. 유럽 컴퓨터 비전 컨퍼런스에서. Springer, 236-253. 라이 웨이성, 황지아빈, 왕 올리버, 셰흐트만 엘리, 유머 에르신, 양 밍슈안. 2018. 맹목적인 비디오 시간적 일관성 학습. 유럽 컴퓨터 비전 컨퍼런스에서. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Wang Zehan 등. 2017. 생성적 적대 신경망을 사용한 사실적인 단일 이미지 초고해상도. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 4681-4690. Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu. 2021. GNeRF: 포즈 카메라가 없는 GAN 기반 신경 복사장. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 논문집. Lars Mescheder, Sebastian Nowozin, Andreas Geiger. 2018. GAN에 대한 어떤 훈련 방법이 실제로 수렴하는가?. 국제 머신 러닝 컨퍼런스(ICML) 논문. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. 2020. NeRF: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ECCV에서. Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 2022. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 프리미티브. ACM Trans. Graph. 41, 4, Article 102(2022년 7월), 15페이지. https://doi.org/10.1145/3528223. Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, Noha Radwan. 2022. RegNeRF: 희소 입력에서 뷰 합성을 위한 신경 광도 필드 정규화. Proc. IEEE Conf. on Computer Vision and Pattern Recognition(CVPR)에서. Michael Niemeyer와 Andreas Geiger. 2021. GIRAFFE: 장면을 구성적 생성적 신경 특징 필드로 표현. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 회의록에서. Michael Oechsle, Songyou Peng, Andreas Geiger. 2021. Unisurf: 다중 뷰 재구성을 위한 신경 암묵적 표면과 광도 필드 통합. IEEE/CVF 컴퓨터 비전 국제 회의록에서. 5589-5599. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz. 2021. HyperNeRF: 위상적으로 변하는 신경 광도 필드에 대한 고차원 표현. ACM Trans. Graph. 40, 6, Article 238(2021년 12월). Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu. 2019. 공간적 적응적 정규화를 통한 의미적 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 2337-2346. Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, Matthias Nieẞner. 2022. 희소 입력 뷰에서 신경 복사장에 대한 고밀도 심도 사전. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)의 진행 중. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer. 2022. 잠재 확산 모델을 통한 고해상도 이미지 합성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)의 진행 중. Sara Fridovich-Keil과 Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa. 2022. Plenoxels: 신경망이 없는 광도 필드. CVPR에서. Johannes Lutz Schönberger와 Jan-Michael Frahm. 2016. Structure-from-Motion Revisited. 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR). Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger. 2020. GRAF: 3D 인식 이미지 합성을 위한 생성적 광도 필드. Advances in Neural Information Processing Systems 33, Vol. 25에서. Curran Associates, Inc., Red Hook, NY, 20154-20166. https://papers.nips.cc/paper_files/paper/2020/hash/ e92e1b476bb5262d793fd40931e0ed53-Abstract.html Karen Simonyan과 Andrew Zisserman. 2015. 대규모 이미지 인식을 위한 매우 깊은 합성곱 네트워크. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, 2015년 5월 7-9일, Conference Track Proceedings, Yoshua Bengio와 Yann LeCun(편집자). http://arxiv.org/abs/1409.Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. 2022. EpiGRAF: 3D GAN의 학습 재고. 신경 정보 처리 시스템의 발전, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, 및 Kyunghyun Cho(편). https://openreview.net/forum?id=TTM7iEFOTZJ Minjung Son, Jeong Joon Park, Leonidas Guibas, 및 Gordon Wetzstein. 2023. SinGRAF: 단일 장면을 위한 3D 생성적 광채 필드 학습. CVPR에서. Cheng Sun, Min Sun, 및 Hwann-Tzong Chen. 2022. 직접 복셀 그리드 최적화: 광채 필드 재구성을 위한 초고속 수렴. CVPR에서. Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, 및 Angjoo Kanazawa. 2023. Nerfstudio: 신경 복사장 개발을 위한 모듈식 프레임워크. arXiv 사전 인쇄본 arXiv:2302.04264(2023). Zachary Teed와 Jia Deng. 2020. RAFT: 광학 흐름을 위한 순환적 모든 쌍 필드 변환. 유럽 컴퓨터 비전 컨퍼런스에서. Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nieẞner T. Tieleman과 G. Hinton. 2012. 강의 6.5 - rmsprop: 최근 크기의 이동 평균으로 기울기를 나눕니다. (2012). Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt. 2021. 비강체 신경 복사장: 단안 비디오에서 동적 장면의 재구성 및 새로운 뷰 합성. IEEE 국제 컴퓨터 비전 컨퍼런스(ICCV)에서. IEEE. Chaoyue Wang, Chang Xu, Chaohui Wang, Dacheng Tao. 2018b. 이미지-이미지 변환을 위한 지각적 적대적 네트워크. IEEE 이미지 처리 트랜잭션 27, 8(2018), 4066-4079. https://doi.org/10.1109/TIP.2018.Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. 2021. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암묵적 표면 학습. arXiv 사전 인쇄본 arXiv:2106.10689(2021). Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018a. 조건부 GAN을 통한 고해상도 이미지 합성 및 의미 조작. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE 이미지 처리 거래 13, 4(2004), 600-612. Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 2022. 4K-NeRF: 초고해상도에서의 고충실도 신경 복사장. arXiv 사전 인쇄본 arXiv:2212.04701(2022). Jamie Wynn과 Daniyar Turmukhambetov. 2023. DiffusioNeRF: Denoising Diffusion Models를 사용한 신경 복사장 정규화. arxiv에 게재. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, Zhangyang Wang. 2022. Sinnerf: 단일 이미지에서 복잡한 장면에 대한 신경 복사장 훈련. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXII. Springer, 736–753. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, Angela Dai. 2023. ScanNet++: 3D 실내 장면의 고충실도 데이터 세트. International Conference on Computer Vision(ICCV)의 Proceedings에 게재. Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa. 2021. pixelNeRF: 하나 또는 몇 개의 이미지에서 추출한 신경 복사장. CVPR에서. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, Andreas Geiger. 2022. Monosdf: 신경 암묵적 표면 재구성을 위한 단안적 기하학적 단서 탐색. arXiv 사전 인쇄본 arXiv:2206.00665(2022). Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. 2020. NeRF++: 신경 복사장 분석 및 개선. arXiv:2010.07492(2020). Lvmin Zhang, Maneesh Agrawala. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv:2302.05543 [cs.CV] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 2018. 지각적 지표로서의 딥 피처의 비합리적인 효과성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 586-595. Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, Jonathan T Barron. 2021. Nerfactor: 알려지지 않은 조명 하에서 모양과 반사도의 신경적 요인 분석. ACM Transactions on Graphics(TOG) 40, 6(2021), 1-18. Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu. 2023. NeRFLIX: DegradationDriven Inter-viewpoint Mixer 학습을 통한 고품질 신경 뷰 합성. arxiv에 게재.
