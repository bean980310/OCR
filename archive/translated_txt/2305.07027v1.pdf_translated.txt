--- ABSTRACT ---
비전 트랜스포머는 높은 모델 성능 덕분에 큰 성공을 거두었습니다. 그러나 놀라운 성능에는 많은 계산 비용이 수반되어 실시간 애플리케이션에는 적합하지 않습니다. 이 논문에서는 EfficientViT라는 고속 비전 트랜스포머 제품군을 제안합니다. 기존 트랜스포머 모델의 속도는 일반적으로 메모리 비효율적 연산, 특히 MHSA의 텐서 리셰이핑 및 요소별 함수에 의해 제한된다는 것을 발견했습니다. 따라서 효율적인 FFN 레이어 사이에 단일 메모리 바운드 MHSA를 사용하여 샌드위치 레이아웃으로 새로운 빌딩 블록을 설계하여 메모리 효율성을 개선하고 채널 통신을 향상시킵니다. 또한 어텐션 맵이 헤드 간에 높은 유사성을 공유하여 계산 중복성이 발생한다는 것을 발견했습니다. 이를 해결하기 위해 전체 기능의 다양한 분할을 어텐션 헤드에 공급하는 계단식 그룹 어텐션 모듈을 제시하여 계산 비용을 절감할 뿐만 아니라 어텐션 다양성도 개선합니다. 포괄적인 실험을 통해 Efficient ViT가 기존의 효율적 모델보다 성능이 뛰어나 속도와 정확도 사이에서 좋은 균형을 이룬다는 것이 입증되었습니다. 예를 들어, Efficient ViT-M5는 MobileNetV3-Large보다 정확도가 1.9% 더 높고, Nvidia VGPU와 Intel Xeon CPU에서 각각 40.4%와 45.2% 더 높은 처리량을 얻습니다. 최근의 효율적인 모델인 Mobile ViT-XXS와 비교했을 때, Efficient ViT-M은 GPU/CPU에서 5.8×/3.7× 더 빠르고, ONNX 포맷으로 변환하면 7.4× 더 빠른 정확도를 달성합니다. 코드와 모델은 여기에서 확인할 수 있습니다. 1.
--- INTRODUCTION ---
영어: Vision Transformers(ViTs)는 높은 모델 기능과 탁월한 성능으로 인해 컴퓨터 비전 도메인을 강타했습니다[18,44,69]. 그러나 지속적으로 향상된 정확도는 모델 크기와 계산 오버헤드의 증가라는 대가를 치릅니다. 예를 들어, SwinV2[43]는 3.0B 매개변수를 사용하는 반면 V-MoE[62]는 14.7B 매개변수를 사용하여 Ima*에서 최첨단 성능을 달성합니다. Xinyu가 Microsoft Research의 인턴이었을 때 수행한 작업. 상위 1 정확도(%)MMM- 당사 -MobileNetVShuffleNetVGhostNet EfficientNet -MobileViTVM→ EdgeViT -*- ConvNext PVT Mobile-Former M1 LVT мо처리량(초당 이미지) 그림 1. ImageNet-1K 데이터 세트가 있는 Nvidia V100 GPU에서 테스트한 Efficient ViT(당사)와 다른 효율적인 CNN 및 ViT 모델 간의 속도 및 정확도 비교[17].geNet[17]. 이렇게 큰 모델 크기와 이에 따른 높은 계산 비용으로 인해 이러한 모델은 실시간 요구 사항이 있는 애플리케이션에 적합하지 않습니다[40, 78, 86]. 최근 가볍고 효율적인 비전 변환기 모델을 설계하는 여러 연구가 있습니다[9,19,29,49,50,56,79,81]. 안타깝게도 이러한 방법의 대부분은 속도에 대한 간접적인 지표이며 모델의 실제 추론 처리량을 반영하지 않는 모델 매개변수 또는 플롭을 줄이는 것을 목표로 합니다. 예를 들어, 700M 플롭을 사용하는 Mobile ViT-XS[50]는 Nvidia V100 GPU에서 1,220M 플롭을 사용하는 DeiT-T[69]보다 훨씬 느리게 실행됩니다. 이러한 방법은 플롭이나 매개변수가 더 적어 좋은 성능을 달성했지만, 그 중 대부분은 표준 동형 또는 계층적 변환기(예: DeiT[69] 및 Swin[44])에 비해 상당한 벽시계 속도 향상을 보이지 않았으며 널리 채택되지 않았습니다. 이 문제를 해결하기 위해 이 논문에서는 비전 변환기를 사용하여 더 빠르게 작업하는 방법을 살펴보고 효율적인 변환기 아키텍처를 설계하기 위한 원칙을 찾습니다. 널리 사용되는 비전 변환기 DeiT[69] 및 Swin[44]을 기반으로 메모리 액세스, 계산 중복성 및 매개변수 사용을 포함하여 모델 추론 속도에 영향을 미치는 세 가지 주요 요소를 체계적으로 분석합니다. 특히 변환기 모델의 속도는 일반적으로 메모리에 따라 결정됩니다. 즉, 메모리 액세스 지연으로 인해 GPU/CPU의 컴퓨팅 전력을 완전히 활용할 수 없습니다[21, 32, 72]. 이는 변환기의 런타임 속도에 치명적인 부정적인 영향을 미칩니다[15,31]. 가장 메모리 효율이 낮은 연산은 멀티헤드 셀프 어텐션(MHSA)의 빈번한 텐서 리셰이핑 및 요소별 함수입니다. MHSA와 FFN(피드포워드 네트워크) 계층 간의 비율을 적절히 조정하면 성능을 저하시키지 않고도 메모리 액세스 시간을 크게 줄일 수 있음을 관찰했습니다. 또한 일부 어텐션 헤드가 유사한 선형 투영을 학습하는 경향이 있어 어텐션 맵에서 중복이 발생하는 것을 발견했습니다. 분석 결과 다양한 기능을 제공하여 각 헤드의 계산을 명시적으로 분해하면 계산 효율성을 개선하는 동시에 이 문제를 완화할 수 있음을 보여줍니다. 또한 기존의 경량 모델은 주로 표준 변압기 모델[44, 69]의 구성을 따르기 때문에 다양한 모듈의 매개변수 할당이 간과되는 경우가 많습니다. 매개변수 효율성을 개선하기 위해 구조적 가지치기[45]를 사용하여 가장 중요한 네트워크 구성 요소를 식별하고 모델 가속을 위한 매개변수 재할당에 대한 경험적 지침을 요약합니다. 분석과 결과를 바탕으로 Efficient ViT라는 새로운 메모리 효율적 변압기 모델 패밀리를 제안합니다. 구체적으로 샌드위치 레이아웃이 있는 새로운 블록을 설계하여 모델을 구축합니다. 샌드위치 레이아웃 블록은 FFN 레이어 사이에 단일 메모리 바운드 MHSA 레이어를 적용합니다. MHSA에서 메모리 바운드 작업으로 인한 시간 비용을 줄이고 더 많은 FFN 레이어를 적용하여 서로 다른 채널 간의 통신을 허용하므로 메모리 효율성이 더 높습니다. 그런 다음, 계산 효율성을 개선하기 위해 새로운 계단식 그룹 어텐션(CGA) 모듈을 제안합니다. 핵심 아이디어는 어텐션 헤드에 공급되는 피처의 다양성을 향상시키는 것입니다. 모든 헤드에 동일한 피처를 사용하는 이전 셀프 어텐션과 달리 CGA는 각 헤드에 다른 입력 분할을 공급하고 헤드 전체에 출력 피처를 계단식으로 전달합니다. 이 모듈은 다중 헤드 어텐션에서 계산 중복성을 줄일 뿐만 아니라 네트워크 깊이를 늘려 모델 용량을 높입니다. 마지막으로, FFN의 숨겨진 차원과 같이 중요도가 낮은 항목을 축소하는 동안 값 투영과 같은 중요한 네트워크 구성 요소의 채널 폭을 확장하여 매개 변수를 재분배합니다. 이 재할당은 결국 모델 매개 변수 효율성을 촉진합니다. 실험 결과, 그림 1에서 보듯이 우리 모델은 속도와 정확도 측면에서 기존의 효율적 CNN 및 ViT 모델보다 뚜렷한 개선을 이룬다는 것이 입증되었습니다.예를 들어, Efficient ViT-M5는 Nvidia V100 GPU에서 10,621개 이미지/초, Intel Xeon E52690 v4 CPU @ 2.60GHz에서 56.8개 이미지/초의 처리량으로 ImageNet에서 77.1%의 상위 1개 정확도를 얻어 MobileNetV3Large [26]보다 정확도에서 1.9%, GPU 추론 속도에서 40.4%, CPU 속도에서 45.2% 더 우수한 성능을 보였습니다.또한 Efficient ViTM2는 70.8%의 정확도를 얻어 Mobile ViT-XXS [50]보다 1.8% 더 뛰어나며 GPU/CPU에서 5.8×/3.7× 더 빠르게 실행되고 ONNX [3] 형식으로 변환하면 7.4배 더 빠릅니다. 모바일 칩셋, 즉 Apple A13 Bionic 칩에 배포하는 경우 44.9% 5.4% 7.0% 6.4% 14.6% 텐서 곱셈 32.2% 14.0% 22.6% Swin-T Reshape Normalization Copy 10.6% 6.0% 10.4% Addition 25.8% 기타 DeiT-T 그림 2. 두 가지 표준 비전 변환기 Swin-T 및 DeiT-T에서의 런타임 프로파일링. 빨간색 텍스트는 메모리 바운드 작업을 나타냅니다. 즉, 작업에 걸리는 시간은 주로 메모리 액세스에 의해 결정되는 반면 계산에 소요되는 시간은 훨씬 적습니다. iPhone 11에서 효율적인 ViT-M2 모델은 CoreML[1]을 사용하는 Mobile ViT-XXS[50]보다 2.3배 더 빠르게 실행됩니다. 이 작업의 기여는 두 가지입니다.요약, • 비전 변환기의 추론 속도에 영향을 미치는 요소에 대한 체계적인 분석을 제시하여 효율적인 모델 설계를 위한 일련의 지침을 도출합니다.• 효율성과 정확성 사이에서 좋은 균형을 이루는 새로운 비전 변환기 모델 패밀리를 설계합니다.이 모델은 또한 다양한 다운스트림 작업에서 좋은 전송 능력을 보여줍니다.2. 비전 변환기로 더 빠르게 이 섹션에서는 메모리 액세스, 계산 중복성 및 매개변수 사용의 세 가지 관점에서 비전 변환기의 효율성을 개선하는 방법을 살펴봅니다.경험적 연구를 통해 근본적인 속도 병목 현상을 식별하고 유용한 설계 지침을 요약합니다.2.1. 메모리 효율성 메모리 액세스 오버헤드는 모델 속도에 영향을 미치는 중요한 요소입니다[15,28,31,65]. 변환기의 많은 연산자[71], 예를 들어 빈번한 재형성, 요소별 추가 및 정규화는 메모리 비효율적이어서 그림 2에서와 같이 여러 메모리 단위에서 시간이 많이 걸리는 액세스가 필요합니다. 표준 소프트맥스 자기 주의의 계산을 단순화하여 이 문제를 해결하기 위해 제안된 몇 가지 방법이 있지만(예: 희소 주의[34, 57, 61, 75] 및 저랭크 근사[11, 51, 74]), 종종 정확도 저하와 제한된 가속이라는 대가를 치릅니다. 이 작업에서 우리는 메모리 비효율적인 계층을 줄여 메모리 액세스 비용을 절감하는 방법을 알아봅니다. 최근 연구에 따르면 메모리 비효율적인 연산은 FFN 계층이 아닌 MHSA에 주로 위치합니다[31, 33]. 그러나 대부분의 기존 ViT[18, 44, 69]는 이 두 계층을 동일한 수로 사용하므로 최적의 효율성을 달성하지 못할 수 있습니다. 이를 통해 빠른 추론을 통해 작은 모델에서 MHSA 및 FFN 계층의 최적 할당을 탐구합니다. 구체적으로, 우리는 Swin-T[44]와 DeiT-T[69]를 1.25배와 1.5배 더 높은 추론 처리량을 가진 여러 개의 작은 하위 네트워크로 축소하고 MHSA 계층의 비율이 다른 하위 네트워크의 성능을 비교합니다. 그림 3에서 볼 수 있듯이 20%-40%의 MHSA 계층을 가진 하위 네트워크는 더 나은 정확도를 얻는 경향이 있습니다. 이러한 비율은 Top-1(%)보다 훨씬 작습니다.70.Swin-T-1.25x Swin-T-1.50x DeiT-1.25x DeiT-1.50x 76.65.63.5.60.61.55.50.45.0-40.0.0 0.2 0.0.0.1.0.0.2 0.0.0.1.평균 코사인 유사도(%) Swin-T-1.25x Swin-T-1.25xw/ 헤드 분할 DeiT-1.25xDeiT-1.25xw/ 헤드 분할 MW블록 인덱스 블록 인덱스 MHSA 비율 MHSA 비율 그림 3. 각 선의 점은 비슷한 처리량을 가진 하위 네트워크를 나타내는 다양한 MHSA 계층 비율을 가진 다운스케일된 기준선 모델의 정확도. 왼쪽: 기준선으로서의 Swin-T. 오른쪽: 기준선으로서 DeiT-T. 1.25×/1.5×는 각각 기준선 모델을 1.25/1.5배 가속화함을 나타냅니다. 50% MHSA 계층을 채택한 일반적인 ViT입니다. 또한, 재구성, 요소별 추가, 복사 및 정규화를 포함한 메모리 액세스 효율성을 비교하기 위해 메모리 바운드 작업의 시간 소모를 측정합니다. 메모리 바운드 작업은 20% MHSA 계층을 갖는 Swin-T-1.25×에서 총 런타임의 44.26%로 줄었습니다. 이 관찰은 DeiT 및 1.5배 속도 향상을 보이는 더 작은 모델에도 일반화됩니다. MHSA 계층 사용률을 적절히 줄이면 모델 성능을 개선하는 동시에 메모리 효율성을 높일 수 있음이 입증되었습니다. 2.2. 계산 효율성 MHSA는 입력 시퀀스를 여러 부분 공간(헤드)에 임베드하고 어텐션 맵을 별도로 계산하는데, 이는 성능 개선에 효과적인 것으로 입증되었습니다[18, 69, 71]. 그러나 어텐션 맵은 계산 비용이 많이 들고, 연구에 따르면 그 중 다수는 매우 중요하지 않은 것으로 나타났습니다[52, 73]. 계산 비용을 절약하기 위해 작은 ViT 모델에서 중복된 어텐션을 줄이는 방법을 탐구합니다. 1.25배 추론 속도 향상으로 폭이 축소된 Swin-T[44] 및 DeiT-T[69] 모델을 훈련하고 각 헤드와 각 블록 내의 나머지 헤드의 최대 코사인 유사도를 측정합니다. 그림 4에서 어텐션 헤드 사이에, 특히 마지막 블록에서 높은 유사도가 존재함을 관찰합니다. 이 현상은 많은 헤드가 동일한 전체 특징에 대한 유사한 투영을 학습하고 계산 중복을 초래한다는 것을 시사합니다. 헤드가 다른 패턴을 학습하도록 명시적으로 장려하기 위해 전체 특징의 일부만 각 헤드에 공급하여 직관적인 솔루션을 적용합니다. 이는 [10, 87]의 그룹 합성곱 아이디어와 유사합니다. 우리는 수정된 MHSA를 사용하여 축소된 모델의 변형을 훈련하고, 그림 4에서 주의 유사도를 계산합니다. MHSA처럼 모든 헤드에 동일한 전체 기능을 사용하는 대신, 다른 헤드에서 기능의 다른 채널별 분할을 사용하면 주의 계산 중복성을 효과적으로 완화할 수 있음을 보여줍니다. 2.3 매개변수 효율성 일반적인 ViT는 주로 NLP 변환기[71]의 설계 전략을 상속받습니다. 예를 들어, Q, K, V 투영에 대해 동등한 너비를 사용하고, 단계에 따라 헤드를 늘리고, FFN에서 확장 비율을 4로 설정합니다. 가벼운 modFigure 4의 경우. 다른 블록에서 각 헤드의 평균 최대 코사인 유사도. 왼쪽: 축소된 Swin-T 모델. 오른쪽: 축소된 DeiT-T 모델. 파란색 선은 Swin-T-1.25×/DeiT-T1.25× 모델을 나타내고, 진한 파란색 선은 전체 기능의 분할만 각 헤드에 공급하는 변형을 나타냅니다. 입력에 대한 비율 4.3.3.2.2.1.1.0.0.블록 인덱스 QKV 비율(원점) QK 비율(정리됨) V 비율(정리됨) FFN 비율(원점) FFN 비율(정리됨)그림 5. Swin-T를 정리하기 전과 후의 입력 임베딩에 대한 채널 비율. 기준 정확도: 79.1%; 정리된 정확도: 76.5%. DeiT-T에 대한 결과는 보충 자료에 나와 있습니다. 따라서 이러한 구성 요소의 구성을 신중하게 재설계해야 합니다[7,8, 39]. [45, 82]에서 영감을 얻어 Taylor 구조적 정리[53]를 채택하여 Swin-T 및 DeiT-T에서 중요한 구성 요소를 자동으로 찾고 매개변수 할당의 기본 원칙을 탐구합니다. 정리 방법은 특정 리소스 제약 하에서 중요하지 않은 채널을 제거하고 가장 중요한 채널을 유지하여 정확도를 가장 잘 유지합니다. 그것은 채널 중요도로 그래디언트와 가중치의 곱을 사용하는데, 이는 채널을 제거할 때 손실 변동을 근사합니다[38]. 나머지 출력 채널과 입력 채널 간의 비율은 그림에 표시되어 있으며, 가지치기되지 않은 모델의 원래 비율도 참고로 제공됩니다. 다음이 관찰됩니다. 1) 처음 두 단계는 더 많은 차원을 보존하는 반면 마지막 단계는 훨씬 적게 보존합니다. 2) Q, K 및 FFN 차원은 크게 다듬어지는 반면 V의 차원은 거의 보존되고 마지막 몇 블록에서만 감소합니다. 이러한 현상은 1) 각 단계 후에 채널을 두 배로 늘리는[44] 일반적인 채널 구성이나 모든 블록에 대해 동등한 채널을 사용하는[69] 일반적인 채널 구성은 마지막 몇 블록에서 상당한 중복성을 생성할 수 있음을 보여줍니다. 2) 동일한 차원을 가질 때 Q, K의 중복성은 V보다 훨씬 큽니다. V는 입력 임베딩 차원에 가까운 비교적 큰 채널을 선호합니다. 3. 효율적인 비전 변환기 위의 분석을 바탕으로 이 섹션에서는 Efficient ViT라는 빠른 추론을 갖춘 새로운 계층적 모델을 제안합니다.아키텍처 개요는 그림 6에 나와 있습니다.(a) HXW XInput Overlap PatchEmbed HW 16x16x Stage× CEfficientViT Block X LEfficientViT Subsample (b) !(c) ¡HeadFeed Forward Network Token Interaction Cascaded Group Attention Input Feed Forward Network Head h H Token Interaction HeadH W 32x32x C₂ HW Stage64x64 × CStageEfficientViT Block × LInteraction Self-attention QKVQK ▼ Token Interaction Self-attention Self-attention Efficient ViT Subsample Concat &amp; Projection Output EfficientViT Block × L그림 6. Efficient ViT 개요.(a) Efficient ViT의 아키텍처; (b) 샌드위치 레이아웃 블록; (c) 계단식 그룹 어텐션.3.1. 효율적인 ViT 빌딩 블록 그림 6(b)에 표시된 것처럼 비전 변환기에 대한 새로운 효율적인 빌딩 블록을 제안합니다.메모리 효율적인 샌드위치 레이아웃, 계단식 그룹 어텐션 모듈 및 매개변수 재할당 전략으로 구성되어 있으며, 각각 메모리, 계산 및 매개변수 측면에서 모델 효율성을 개선하는 데 중점을 둡니다.샌드위치 레이아웃.메모리 효율적인 블록을 구축하기 위해 채널 통신을 위해 메모리 바운드 셀프 어텐션 계층이 적고 메모리 효율적인 FFN 계층을 사용하는 샌드위치 레이아웃을 제안합니다.특히, 공간 혼합을 위해 단일 셀프 어텐션 계층 Þ을 적용하며, 이는 FFN 계층 사이에 샌드위치됩니다.계산은 다음과 같이 공식화할 수 있습니다.Xi+= NN · [] } (Þ î ([] Þ³ (X;))),(1) 여기서 X는 i번째 블록에 대한 전체 입력 기능입니다. 블록은 단일 자기 주의 계층 전후에 N개의 FFN을 갖는 X¿를 X¿+1로 변환합니다. 이 설계는 모델의 자기 주의 계층으로 인해 발생하는 메모리 시간 소모를 줄이고 더 많은 FFN 계층을 적용하여 서로 다른 피처 채널 간의 효율적인 통신을 허용합니다. 또한 깊이별 합성곱(DWConv)을 사용하여 각 FFN 앞에 추가 토큰 상호 작용 계층을 적용합니다[27]. 이는 모델 기능을 향상시키기 위해 로컬 구조 정보의 귀납적 편향을 도입합니다[14]. 계단식 그룹 주의. 주의 헤드 중복성은 MHSA에서 심각한 문제로, 계산 비효율성을 유발합니다. 효율적인 CNN의 그룹 합성곱[10, 37, 64, 87]에서 영감을 얻어 비전 변환기를 위한 계단식 그룹 주의(CGA)라는 새로운 주의 모듈을 제안합니다. 각 헤드에 전체 피처의 다른 분할을 공급하여 헤드 전체에서 주의 계산을 명시적으로 분해합니다. 형식적으로 이 주의는 다음과 같이 공식화할 수 있습니다. X₁₁ = Attn(X₁₁W, X., WK, X., WY), Ñij ij&quot; ij Concat[ij]j=1:hW}, (2) = Xi+1 = 여기서 j번째 헤드는 입력 피처 Xi의 j번째 분할인 Xij에 대한 셀프 어텐션을 계산합니다. 즉, Xi [Xi1, Xi2, . . ., Xih]이고 1 ≤ j ≤ h입니다. h는 헤드의 총 수이고 W, WK, W¼는 입력 피처 분할을 다른 부분 공간으로 매핑하는 투영 계층이고 WP는 연결된 출력 피처를 입력과 일치하는 차원으로 다시 투영하는 선형 계층입니다. 각 헤드에 대해 전체 피처 대신 피처 분할을 사용하는 것이 더 효율적이고 계산 오버헤드를 절약하지만, Q, K, V 계층이 더 풍부한 정보가 있는 피처에 대한 투영을 학습하도록 장려하여 용량을 계속 개선하고 있습니다. 다음을 계산합니다. 그림 6(c)에 도시된 바와 같이 계단식 방식으로 각 헤드의 주의 맵을 생성하여 각 헤드의 출력을 후속 헤드에 추가하여 특징 표현을 점진적으로 정제합니다. Xíj = Xij+Ẵi(j−1), 1
--- RELATED WORK ---
효율적인 CNN. 리소스가 제한된 시나리오에 CNN을 배포해야 한다는 요구로 인해 효율적인 CNN이 문헌에서 집중적으로 연구되었습니다[23, 24, 26, 48, 63, 67, 87]. Xception[10]은 깊이별 분리형 합성곱으로 구축된 아키텍처를 제안합니다. MobileNetV2[63]는 입력을 더 높은 차원으로 확장하는 역 잔차 구조를 구축합니다. MobileNetV3[26] 및 EfficientNet[67]은 신경 아키텍처 검색 기술을 사용하여 컴팩트 모델을 설계합니다. 하드웨어에서 실제 속도를 높이기 위해 ShuffleNetV2[48]는 채널 그룹 간 정보 전달을 개선하기 위해 채널 분할 및 셔플 작업을 도입합니다. 그러나 합성곱 커널의 공간적 지역성은 CNN 모델이 long(%) I-do 16.0k 71.71.71.1✓ ✓70.QK DimensionThrouguput (imgs/s) 70.40.0.0.16.5k 16.0k 15.5k 1.1.V Ratio wrt Input Througuput (imgs/s) 그림 7. 각 헤드의 QK 차원에 대한 절제와 V 차원과 입력 임베딩의 비율. 범위 종속성을 포착하여 모델 용량을 제한합니다. 효율적인 ViT. ViT와 그 변형[18, 44, 69, 76]은 다양한 비전 작업에서 성공을 거두었습니다. 뛰어난 성능에도 불구하고 대부분은 추론 속도 면에서 일반적인 CNN보다 열등합니다. 최근 몇 가지 효율적인 변환기가 제안되었으며 1) 효율적인 셀프 어텐션, 2) 효율적인 아키텍처 설계의 두 가지 범주로 나뉩니다. 효율적인 자기 관리
--- METHOD ---
s는 속도에 대한 간접적인 지표이며 모델의 실제 추론 처리량을 반영하지 않는 모델 매개변수 또는 플롭을 줄이는 것을 목표로 합니다. 예를 들어, 700M 플롭을 사용하는 Mobile ViT-XS[50]는 Nvidia V100 GPU에서 1,220M 플롭을 사용하는 DeiT-T[69]보다 훨씬 느리게 실행됩니다. 이러한 방법은 플롭이나 매개변수가 더 적어서 좋은 성능을 달성했지만, 그 중 대부분은 표준 동형 또는 계층적 변환기(예: DeiT[69] 및 Swin[44])에 비해 상당한 벽시계 속도 향상을 보이지 않았으며 널리 채택되지 않았습니다. 이 문제를 해결하기 위해 이 논문에서는 비전 변환기로 더 빠르게 작업하는 방법을 살펴보고 효율적인 변환기 아키텍처를 설계하기 위한 원칙을 찾습니다. 널리 사용되는 비전 변환기 DeiT[69] 및 Swin[44]를 기반으로 메모리 액세스, 계산 중복성, 매개변수 사용을 포함하여 모델 추론 속도에 영향을 미치는 세 가지 주요 요소를 체계적으로 분석합니다. 특히, 우리는 변압기 모델의 속도가 일반적으로 메모리에 의해 결정된다는 것을 발견했습니다. 즉, 메모리 접근 지연은 GPU/CPU의 컴퓨팅 파워를 완전히 활용하는 것을 금지합니다[21, 32, 72], 이는 변압기의 런타임 속도에 치명적인 부정적인 영향을 미칩니다[15,31]. 가장 메모리 비효율적인 연산은 멀티 헤드 셀프 어텐션(MHSA)의 빈번한 텐서 리셰이핑과 요소별 함수입니다. 우리는 MHSA와 FFN(피드포워드 네트워크) 계층 간의 비율을 적절히 조정함으로써 성능을 저하시키지 않고도 메모리 접근 시간을 상당히 줄일 수 있음을 관찰했습니다. 게다가, 일부 어텐션 헤드는 유사한 선형 투영을 학습하는 경향이 있어 어텐션 맵에 중복성이 발생합니다. 분석 결과, 다양한 기능을 제공하여 각 헤드의 계산을 명시적으로 분해하면 계산 효율성을 개선하는 동시에 이 문제를 완화할 수 있음을 보여줍니다. 또한, 기존의 경량 모델은 주로 표준 변압기 모델의 구성을 따르기 때문에 다양한 모듈의 매개변수 할당을 간과하는 경우가 많습니다[44, 69]. 매개변수 효율성을 개선하기 위해 구조적 가지치기[45]를 사용하여 가장 중요한 네트워크 구성 요소를 식별하고 모델 가속을 위한 매개변수 재할당의 경험적 지침을 요약합니다. 분석과 결과를 바탕으로 Efficient ViT라는 새로운 메모리 효율적 변압기 모델 패밀리를 제안합니다. 구체적으로 샌드위치 레이아웃이 있는 새로운 블록을 설계하여 모델을 구축합니다. 샌드위치 레이아웃 블록은 FFN 레이어 사이에 단일 메모리 바운드 MHSA 레이어를 적용합니다. MHSA에서 메모리 바운드 작업으로 인한 시간 비용을 줄이고 더 많은 FFN 레이어를 적용하여 다른 채널 간의 통신을 허용하므로 메모리 효율성이 더 높습니다. 그런 다음 계산 효율성을 개선하기 위해 새로운 계단식 그룹 어텐션(CGA) 모듈을 제안합니다. 핵심 아이디어는 어텐션 헤드에 공급되는 기능의 다양성을 향상시키는 것입니다. 모든 헤드에 동일한 기능을 사용하는 이전 셀프 어텐션과 달리 CGA는 각 헤드에 다른 입력 분할을 공급하고 헤드 간에 출력 기능을 계단식으로 전달합니다. 이 모듈은 멀티 헤드 어텐션에서 계산 중복성을 줄일 뿐만 아니라 네트워크 깊이를 늘려 모델 용량을 높입니다. 마지막으로, 우리는 FFN의 숨겨진 차원과 같이 중요도가 낮은 것을 축소하는 동시에 가치 예측과 같은 중요한 네트워크 구성 요소의 채널 폭을 확장하여 매개변수를 재분배합니다. 이 재분배는 결국 모델 매개변수 효율성을 증진합니다.
--- EXPERIMENT ---
s는 Efficient ViT가 기존의 효율적 모델보다 성능이 뛰어나 속도와 정확도 간에 좋은 균형을 이룬다는 것을 보여줍니다. 예를 들어, Efficient ViT-M5는 MobileNetV3-Large보다 정확도가 1.9% 높고, Nvidia VGPU와 Intel Xeon CPU에서 각각 40.4%와 45.2% 더 높은 처리량을 얻습니다. 최근의 효율적 모델인 Mobile ViT-XXS와 비교할 때 Efficient ViT-M은 GPU/CPU에서 5.8×/3.7× 더 빠르고 ONNX 형식으로 변환하면 7.4배 더 빠른 정확도를 달성합니다. 코드와 모델은 여기에서 확인할 수 있습니다. 1. 소개 Vision Transformers(ViT)는 높은 모델 기능과 뛰어난 성능으로 인해 컴퓨터 비전 도메인을 강타했습니다[18,44,69]. 그러나 지속적으로 향상된 정확도는 모델 크기와 계산 오버헤드가 증가하는 대가를 치릅니다. 예를 들어, SwinV2[43]는 3.0B 매개변수를 사용하는 반면 V-MoE[62]는 14.7B 매개변수를 사용하여 Ima*에서 최첨단 성능을 달성합니다.Xinyu가 Microsoft Research의 인턴이었을 때 수행한 작업.Top-1 정확도(%)MMM- Ours -MobileNetVShuffleNetVGhostNet EfficientNet -MobileViTVM→ EdgeViT -*- ConvNext PVT Mobile-Former M1 LVT мо처리량(초당 이미지) 그림 1. ImageNet-1K 데이터 세트가 있는 Nvidia V100 GPU에서 테스트한 Efficient ViT(Ours)와 다른 효율적인 CNN 및 ViT 모델 간의 속도 및 정확도 비교[17].geNet[17]. 이처럼 모델 크기가 크고 계산 비용이 많이 들기 때문에 이러한 모델은 실시간 요구 사항이 있는 애플리케이션에는 적합하지 않습니다[40, 78, 86]. 최근에는 가볍고 효율적인 비전 변환기 모델을 설계하는 여러 연구가 있습니다[9,19,29,49,50,56,79,81]. 불행히도 이러한 방법의 대부분은 속도에 대한 간접적인 지표이며 모델의 실제 추론 처리량을 반영하지 않는 모델 매개변수 또는 플롭을 줄이는 것을 목표로 합니다. 예를 들어, 700M 플롭을 사용하는 Mobile ViT-XS[50]는 Nvidia V100 GPU에서 1,220M 플롭을 사용하는 DeiT-T[69]보다 훨씬 느리게 실행됩니다. 이러한 방법은 더 적은 플롭 또는 매개변수로 좋은 성능을 달성했지만, 그 중 대부분은 표준 동형 또는 계층적 변환기(예: DeiT[69] 및 Swin[44])에 비해 상당한 벽시계 속도 향상을 보이지 않았으며 널리 채택되지 않았습니다. 이 문제를 해결하기 위해 이 논문에서는 비전 변환기로 더 빠르게 작업하는 방법을 살펴보고 효율적인 변환기 아키텍처를 설계하기 위한 원칙을 찾습니다. 우세한 비전 변환기 DeiT [69] 및 Swin [44]을 기반으로 메모리 액세스, 계산 중복성 및 매개변수 사용을 포함하여 모델 추론 속도에 영향을 미치는 세 가지 주요 요소를 체계적으로 분석합니다. 특히 변환기 모델의 속도는 일반적으로 메모리에 따라 결정됩니다. 즉, 메모리 액세스 지연으로 인해 GPU/CPU의 컴퓨팅 전력을 완전히 활용할 수 없습니다[21, 32, 72]. 이는 변환기의 런타임 속도에 치명적인 부정적인 영향을 미칩니다[15,31]. 가장 메모리 비효율적인 연산은 다중 헤드 셀프 어텐션(MHSA)의 빈번한 텐서 리셰이핑 및 요소별 함수입니다. MHSA와 FFN(피드포워드 네트워크) 계층 간의 비율을 적절히 조정하면 성능을 저하시키지 않고도 메모리 액세스 시간을 크게 줄일 수 있음을 관찰했습니다. 게다가 일부 어텐션 헤드는 유사한 선형 투영을 학습하는 경향이 있어 어텐션 맵에서 중복성이 발생하는 것을 발견했습니다. 분석 결과, 다양한 기능을 공급하여 각 헤드의 계산을 명시적으로 분해하면 계산 효율성을 개선하는 동시에 이 문제를 완화할 수 있음을 보여줍니다. 또한 기존의 경량 모델은 주로 표준 변압기 모델[44, 69]의 구성을 따르기 때문에 다양한 모듈의 매개변수 할당을 간과하는 경우가 많습니다. 매개변수 효율성을 개선하기 위해 구조적 가지치기[45]를 사용하여 가장 중요한 네트워크 구성 요소를 식별하고 모델 가속을 위한 매개변수 재할당에 대한 경험적 지침을 요약합니다. 분석과 결과를 바탕으로 Efficient ViT라는 새로운 메모리 효율적 변압기 모델 패밀리를 제안합니다. 구체적으로 샌드위치 레이아웃이 있는 새로운 블록을 설계하여 모델을 구축합니다. 샌드위치 레이아웃 블록은 FFN 레이어 사이에 단일 메모리 바운드 MHSA 레이어를 적용합니다. MHSA에서 메모리 바운드 작업으로 인한 시간 비용을 줄이고 더 많은 FFN 레이어를 적용하여 서로 다른 채널 간의 통신을 허용하므로 메모리 효율성이 더 높습니다. 그런 다음 계산 효율성을 개선하기 위해 새로운 계단식 그룹 어텐션(CGA) 모듈을 제안합니다. 핵심 아이디어는 어텐션 헤드에 공급되는 기능의 다양성을 향상시키는 것입니다. 모든 헤드에 동일한 기능을 사용하는 이전 셀프 어텐션과 달리 CGA는 각 헤드에 다른 입력 분할을 공급하고 헤드 간에 출력 기능을 캐스케이드합니다. 이 모듈은 다중 헤드 어텐션에서 계산 중복성을 줄일 뿐만 아니라 네트워크 깊이를 늘려 모델 용량을 높입니다. 마지막으로, FFN의 숨겨진 차원과 같이 중요도가 낮은 항목을 축소하는 동시에 값 투영과 같은 중요한 네트워크 구성 요소의 채널 폭을 확장하여 매개변수를 재분배합니다. 이 재할당은 결국 모델 매개변수 효율성을 증진합니다. 실험 결과, 그림 1에서 보듯이 우리 모델은 속도와 정확도 측면에서 기존의 효율적 CNN 및 ViT 모델보다 뚜렷한 개선을 이룬다는 것이 입증되었습니다.예를 들어, Efficient ViT-M5는 Nvidia V100 GPU에서 10,621개 이미지/초, Intel Xeon E52690 v4 CPU @ 2.60GHz에서 56.8개 이미지/초의 처리량으로 ImageNet에서 77.1%의 상위 1개 정확도를 얻어 MobileNetV3Large [26]보다 정확도에서 1.9%, GPU 추론 속도에서 40.4%, CPU 속도에서 45.2% 더 우수한 성능을 보였습니다.또한 Efficient ViTM2는 70.8%의 정확도를 얻어 Mobile ViT-XXS [50]보다 1.8% 더 뛰어나며 GPU/CPU에서 5.8×/3.7× 더 빠르게 실행되고 ONNX [3] 형식으로 변환하면 7.4배 더 빠릅니다. 모바일 칩셋, 즉 Apple A13 Bionic 칩에 배포하는 경우 44.9% 5.4% 7.0% 6.4% 14.6% 텐서 곱셈 32.2% 14.0% 22.6% Swin-T Reshape Normalization Copy 10.6% 6.0% 10.4% Addition 25.8% 기타 DeiT-T 그림 2. 두 가지 표준 비전 변환기 Swin-T 및 DeiT-T에서의 런타임 프로파일링. 빨간색 텍스트는 메모리 바운드 작업을 나타냅니다. 즉, 작업에 걸리는 시간은 주로 메모리 액세스에 의해 결정되는 반면 계산에 소요되는 시간은 훨씬 적습니다. iPhone 11에서 효율적인 ViT-M2 모델은 CoreML[1]을 사용하는 Mobile ViT-XXS[50]보다 2.3배 더 빠르게 실행됩니다. 이 작업의 기여는 두 가지입니다.요약, • 비전 변환기의 추론 속도에 영향을 미치는 요소에 대한 체계적인 분석을 제시하여 효율적인 모델 설계를 위한 일련의 지침을 도출합니다.• 효율성과 정확성 사이에서 좋은 균형을 이루는 새로운 비전 변환기 모델 패밀리를 설계합니다.이 모델은 또한 다양한 다운스트림 작업에서 좋은 전송 능력을 보여줍니다.2. 비전 변환기로 더 빠르게 이 섹션에서는 메모리 액세스, 계산 중복성 및 매개변수 사용의 세 가지 관점에서 비전 변환기의 효율성을 개선하는 방법을 살펴봅니다.경험적 연구를 통해 근본적인 속도 병목 현상을 식별하고 유용한 설계 지침을 요약합니다.2.1. 메모리 효율성 메모리 액세스 오버헤드는 모델 속도에 영향을 미치는 중요한 요소입니다[15,28,31,65]. 변환기의 많은 연산자[71], 예를 들어 빈번한 재형성, 요소별 추가 및 정규화는 메모리 비효율적이어서 그림 2에서와 같이 여러 메모리 단위에서 시간이 많이 걸리는 액세스가 필요합니다. 표준 소프트맥스 자기 주의의 계산을 단순화하여 이 문제를 해결하기 위해 제안된 몇 가지 방법이 있지만(예: 희소 주의[34, 57, 61, 75] 및 저랭크 근사[11, 51, 74]), 종종 정확도 저하와 제한된 가속이라는 대가를 치릅니다. 이 작업에서 우리는 메모리 비효율적인 계층을 줄여 메모리 액세스 비용을 절감하는 방법을 알아봅니다. 최근 연구에 따르면 메모리 비효율적인 연산은 FFN 계층이 아닌 MHSA에 주로 위치합니다[31, 33]. 그러나 대부분의 기존 ViT[18, 44, 69]는 이 두 계층을 동일한 수로 사용하므로 최적의 효율성을 달성하지 못할 수 있습니다. 이를 통해 빠른 추론을 통해 작은 모델에서 MHSA 및 FFN 계층의 최적 할당을 탐구합니다. 구체적으로, 우리는 Swin-T[44]와 DeiT-T[69]를 1.25배와 1.5배 더 높은 추론 처리량을 가진 여러 개의 작은 하위 네트워크로 축소하고 MHSA 계층의 비율이 다른 하위 네트워크의 성능을 비교합니다. 그림 3에서 볼 수 있듯이 20%-40%의 MHSA 계층을 가진 하위 네트워크는 더 나은 정확도를 얻는 경향이 있습니다. 이러한 비율은 Top-1(%)보다 훨씬 작습니다.70.Swin-T-1.25x Swin-T-1.50x DeiT-1.25x DeiT-1.50x 76.65.63.5.60.61.55.50.45.0-40.0.0 0.2 0.0.0.1.0.0.2 0.0.0.1.평균 코사인 유사도(%) Swin-T-1.25x Swin-T-1.25xw/ 헤드 분할 DeiT-1.25xDeiT-1.25xw/ 헤드 분할 MW블록 인덱스 블록 인덱스 MHSA 비율 MHSA 비율 그림 3. 각 선의 점은 비슷한 처리량을 가진 하위 네트워크를 나타내는 다양한 MHSA 계층 비율을 가진 다운스케일된 기준선 모델의 정확도. 왼쪽: 기준선으로서의 Swin-T. 오른쪽: 기준선으로서 DeiT-T. 1.25×/1.5×는 각각 기준선 모델을 1.25/1.5배 가속화함을 나타냅니다. 50% MHSA 계층을 채택한 일반적인 ViT입니다. 또한, 재구성, 요소별 추가, 복사 및 정규화를 포함한 메모리 액세스 효율성을 비교하기 위해 메모리 바운드 작업의 시간 소모를 측정합니다. 메모리 바운드 작업은 20% MHSA 계층을 갖는 Swin-T-1.25×에서 총 런타임의 44.26%로 줄었습니다. 이 관찰은 DeiT 및 1.5배 속도 향상을 보이는 더 작은 모델에도 일반화됩니다. MHSA 계층 사용률을 적절히 줄이면 모델 성능을 개선하는 동시에 메모리 효율성을 높일 수 있음이 입증되었습니다. 2.2. 계산 효율성 MHSA는 입력 시퀀스를 여러 부분 공간(헤드)에 임베드하고 어텐션 맵을 별도로 계산하는데, 이는 성능 개선에 효과적인 것으로 입증되었습니다[18, 69, 71]. 그러나 어텐션 맵은 계산 비용이 많이 들고, 연구에 따르면 그 중 다수는 매우 중요하지 않은 것으로 나타났습니다[52, 73]. 계산 비용을 절약하기 위해 작은 ViT 모델에서 중복된 어텐션을 줄이는 방법을 탐구합니다. 1.25배 추론 속도 향상으로 폭이 축소된 Swin-T[44] 및 DeiT-T[69] 모델을 훈련하고 각 헤드와 각 블록 내의 나머지 헤드의 최대 코사인 유사도를 측정합니다. 그림 4에서 어텐션 헤드 사이에, 특히 마지막 블록에서 높은 유사도가 존재함을 관찰합니다. 이 현상은 많은 헤드가 동일한 전체 특징에 대한 유사한 투영을 학습하고 계산 중복을 초래한다는 것을 시사합니다. 헤드가 다른 패턴을 학습하도록 명시적으로 장려하기 위해 전체 특징의 일부만 각 헤드에 공급하여 직관적인 솔루션을 적용합니다. 이는 [10, 87]의 그룹 합성곱 아이디어와 유사합니다. 우리는 수정된 MHSA를 사용하여 축소된 모델의 변형을 훈련하고, 그림 4에서 주의 유사도를 계산합니다. MHSA처럼 모든 헤드에 동일한 전체 기능을 사용하는 대신, 다른 헤드에서 기능의 다른 채널별 분할을 사용하면 주의 계산 중복성을 효과적으로 완화할 수 있음을 보여줍니다. 2.3 매개변수 효율성 일반적인 ViT는 주로 NLP 변환기[71]의 설계 전략을 상속받습니다. 예를 들어, Q, K, V 투영에 대해 동등한 너비를 사용하고, 단계에 따라 헤드를 늘리고, FFN에서 확장 비율을 4로 설정합니다. 가벼운 modFigure 4의 경우. 다른 블록에서 각 헤드의 평균 최대 코사인 유사도. 왼쪽: 축소된 Swin-T 모델. 오른쪽: 축소된 DeiT-T 모델. 파란색 선은 Swin-T-1.25×/DeiT-T1.25× 모델을 나타내고, 진한 파란색 선은 전체 기능의 분할만 각 헤드에 공급하는 변형을 나타냅니다. 입력에 대한 비율 4.3.3.2.2.1.1.0.0.블록 인덱스 QKV 비율(원점) QK 비율(정리됨) V 비율(정리됨) FFN 비율(원점) FFN 비율(정리됨)그림 5. Swin-T를 정리하기 전과 후의 입력 임베딩에 대한 채널 비율. 기준 정확도: 79.1%; 정리된 정확도: 76.5%. DeiT-T에 대한 결과는 보충 자료에 나와 있습니다. 따라서 이러한 구성 요소의 구성을 신중하게 재설계해야 합니다[7,8, 39]. [45, 82]에서 영감을 얻어 Taylor 구조적 정리[53]를 채택하여 Swin-T 및 DeiT-T에서 중요한 구성 요소를 자동으로 찾고 매개변수 할당의 기본 원칙을 탐구합니다. 정리 방법은 특정 리소스 제약 하에서 중요하지 않은 채널을 제거하고 가장 중요한 채널을 유지하여 정확도를 가장 잘 유지합니다. 그것은 채널 중요도로 그래디언트와 가중치의 곱을 사용하는데, 이는 채널을 제거할 때 손실 변동을 근사합니다[38]. 나머지 출력 채널과 입력 채널 간의 비율은 그림에 표시되어 있으며, 가지치기되지 않은 모델의 원래 비율도 참고로 제공됩니다. 다음이 관찰됩니다. 1) 처음 두 단계는 더 많은 차원을 보존하는 반면 마지막 단계는 훨씬 적게 보존합니다. 2) Q, K 및 FFN 차원은 크게 다듬어지는 반면 V의 차원은 거의 보존되고 마지막 몇 블록에서만 감소합니다. 이러한 현상은 1) 각 단계 후에 채널을 두 배로 늘리는[44] 일반적인 채널 구성이나 모든 블록에 대해 동등한 채널을 사용하는[69] 일반적인 채널 구성은 마지막 몇 블록에서 상당한 중복성을 생성할 수 있음을 보여줍니다. 2) 동일한 차원을 가질 때 Q, K의 중복성은 V보다 훨씬 큽니다. V는 입력 임베딩 차원에 가까운 비교적 큰 채널을 선호합니다. 3. 효율적인 비전 변환기 위의 분석을 바탕으로 이 섹션에서는 Efficient ViT라는 빠른 추론을 갖춘 새로운 계층적 모델을 제안합니다.아키텍처 개요는 그림 6에 나와 있습니다.(a) HXW XInput Overlap PatchEmbed HW 16x16x Stage× CEfficientViT Block X LEfficientViT Subsample (b) !(c) ¡HeadFeed Forward Network Token Interaction Cascaded Group Attention Input Feed Forward Network Head h H Token Interaction HeadH W 32x32x C₂ HW Stage64x64 × CStageEfficientViT Block × LInteraction Self-attention QKVQK ▼ Token Interaction Self-attention Self-attention Efficient ViT Subsample Concat &amp; Projection Output EfficientViT Block × L그림 6. Efficient ViT 개요.(a) Efficient ViT의 아키텍처; (b) 샌드위치 레이아웃 블록; (c) 계단식 그룹 어텐션.3.1. 효율적인 ViT 빌딩 블록 그림 6(b)에 표시된 것처럼 비전 변환기에 대한 새로운 효율적인 빌딩 블록을 제안합니다.메모리 효율적인 샌드위치 레이아웃, 계단식 그룹 어텐션 모듈 및 매개변수 재할당 전략으로 구성되어 있으며, 각각 메모리, 계산 및 매개변수 측면에서 모델 효율성을 개선하는 데 중점을 둡니다.샌드위치 레이아웃.메모리 효율적인 블록을 구축하기 위해 채널 통신을 위해 메모리 바운드 셀프 어텐션 계층이 적고 메모리 효율적인 FFN 계층을 사용하는 샌드위치 레이아웃을 제안합니다.특히, 공간 혼합을 위해 단일 셀프 어텐션 계층 Þ을 적용하며, 이는 FFN 계층 사이에 샌드위치됩니다.계산은 다음과 같이 공식화할 수 있습니다.Xi+= NN · [] } (Þ î ([] Þ³ (X;))),(1) 여기서 X는 i번째 블록에 대한 전체 입력 기능입니다. 블록은 단일 자기 주의 계층 전후에 N개의 FFN을 갖는 X¿를 X¿+1로 변환합니다. 이 설계는 모델의 자기 주의 계층으로 인해 발생하는 메모리 시간 소모를 줄이고 더 많은 FFN 계층을 적용하여 서로 다른 피처 채널 간의 효율적인 통신을 허용합니다. 또한 깊이별 합성곱(DWConv)을 사용하여 각 FFN 앞에 추가 토큰 상호 작용 계층을 적용합니다[27]. 이는 모델 기능을 향상시키기 위해 로컬 구조 정보의 귀납적 편향을 도입합니다[14]. 계단식 그룹 주의. 주의 헤드 중복성은 MHSA에서 심각한 문제로, 계산 비효율성을 유발합니다. 효율적인 CNN의 그룹 합성곱[10, 37, 64, 87]에서 영감을 얻어 비전 변환기를 위한 계단식 그룹 주의(CGA)라는 새로운 주의 모듈을 제안합니다. 각 헤드에 전체 피처의 다른 분할을 공급하여 헤드 전체에서 주의 계산을 명시적으로 분해합니다. 형식적으로 이 주의는 다음과 같이 공식화할 수 있습니다. X₁₁ = Attn(X₁₁W, X., WK, X., WY), Ñij ij&quot; ij Concat[ij]j=1:hW}, (2) = Xi+1 = 여기서 j번째 헤드는 입력 피처 Xi의 j번째 분할인 Xij에 대한 셀프 어텐션을 계산합니다. 즉, Xi [Xi1, Xi2, . . ., Xih]이고 1 ≤ j ≤ h입니다. h는 헤드의 총 수이고 W, WK, W¼는 입력 피처 분할을 다른 부분 공간으로 매핑하는 투영 계층이고 WP는 연결된 출력 피처를 입력과 일치하는 차원으로 다시 투영하는 선형 계층입니다. 각 헤드에 대해 전체 피처 대신 피처 분할을 사용하는 것이 더 효율적이고 계산 오버헤드를 절약하지만, Q, K, V 계층이 더 풍부한 정보가 있는 피처에 대한 투영을 학습하도록 장려하여 용량을 계속 개선하고 있습니다. 그림 6(c)에 도시된 바와 같이 계단식 방식으로 각 헤드의 주의 맵을 생성하여 각 헤드의 출력을 후속 헤드에 추가하여 특징 표현을 점진적으로 정제합니다. Xíj = Xij+Ẵi(j−1), 1
--- CONCLUSION ---
이 논문에서 우리는 비전 변환기의 추론 속도에 영향을 미치는 요인들에 대한 체계적인 분석을 제시하고, 메모리 효율적인 연산과 계단식 그룹 어텐션을 갖춘 새로운 패밀리의 빠른 비전 변환기인 Efficient ViT를 제안했습니다. 광범위한 실험을 통해 Efficient ViT의 효능과 빠른 속도가 입증되었으며, 다양한 다운스트림 벤치마크에서도 우수함을 보여주었습니다. 한계점. Efficient ViT의 한 가지 한계점은 높은 추론 속도에도 불구하고 도입된 샌드위치 레이아웃의 추가 FFN으로 인해 최첨단 효율적 CNN[26]에 비해 모델 크기가 약간 더 크다는 것입니다. 게다가, 우리의 모델은 효율적 비전 변환기를 구축하기 위한 파생된 가이드라인에 따라 수동으로 설계되었습니다. 향후 작업에서는 모델 크기를 줄이고 자동 검색 기술을 통합하여 모델 용량과 효율성을 더욱 향상시키는 데 관심이 있습니다. 감사의 말. Yuan 교수는 홍콩 연구 지원 위원회(RGC) 일반 연구 기금 11211221 및 혁신 기술 위원회-혁신 기술 기금 ITS/100/20에서 일부 지원을 받았습니다.참고문헌 [1] Apple. Coremltools: coremltools를 사용하여 타사 라이브러리의 머신 러닝 모델을 core ml 형식으로 변환, 2021. 2,[2] Jimmy Lei Ba, Jamie Ryan Kiros 및 Geoffrey E Hinton. 계층 정규화. arXiv, 2016. 5,[3] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx: 개방형 신경망 교환. https://github.com/onnx/onnx, 2019. 2,[4] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai 및 Aäron van den Oord. imagenet은 끝났을까요? arXiv 사전 인쇄 arXiv:2006.07159, 2020.[5] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan 및 Wanli Ouyang. Glit: 글로벌 및 로컬 이미지 변환기에 대한 신경 아키텍처 검색. ICCV에서, 2021.[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu 등. MmDetection: mmlab 감지 도구 상자 및 벤치마크를 엽니다. arXiv 사전 인쇄 arXiv: 1906.07155, 2019.[7] 첸 밍하오(Minghao Chen), 펑허웬(Houwen Peng), 푸젠롱(Jianlong Fu), 링 하이빈(Haibin Ling). Autoformer: 시각적 인식을 위해 변환기를 검색합니다. ICCV에서, 2021.[8] Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang 및 Denny Zhou. 교육 없이 자동 크기 조정 비전 변환기. ICLR, 2021.[9] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan 및 Zicheng Liu. Mobileformer: 모바일넷과 변압기를 연결합니다. CVPR, 2022. 1, 6,[10] François Chollet. Xception: 깊이 분리 가능한 컨볼루션을 사용한 딥 러닝. CVPR, 2017. 3, 4, 5,[11] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 공연자와 함께 주의 재고. ICLR, 2021. 2,[12] Xiangxiang Chu, Bo Zhang, Ruijun Xu. Fairnas: 가중치 공유 신경 구조 검색의 평가 공정성 재고. ICCV, 12239-12248쪽, 2021.[13] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V Le. 자동 증강: 데이터에서 증강 전략 학습. CVPR, 113-123페이지, 2019.[14] Zihang Dai, Hanxiao Liu, Quoc V Le, Mingxing Tan. Coatnet: 모든 데이터 크기에 대한 합성곱과 주의 결합. NeurIPS, 34:3965-3977, 2021.[15] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré. Flashattention: io 인식을 갖춘 빠르고 메모리 효율적인 정확한 주의. arXiv 사전 인쇄본 arXiv:2205.14135, 2022. 1, 2,[16] Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, Yi Tay. 효율성 오용. ICLR, 2022.[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 및 Li Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스. CVPR, 2009. 1, 5, 6, 7,[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. ICLR, 2021. 1, 2, 3, 7,[19] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, qiang liu, 및 Vikas Chandra. NASVit: 그래디언트 충돌 인식 슈퍼넷 훈련을 통한 효율적인 비전 변환기를 위한 신경 구조 검색. ICLR, 2022.[20] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze. Levit: 더 빠른 추론을 위한 convnet의 옷을 입은 비전 변환기. ICCV, 2021. 5, 7,[21] Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Mingjie Liu, Zixuan Jiang, Ray T Chen, David Z Pan. 다중 레벨 현장 생성을 통한 메모리 효율적인 신경망을 향해. ICCV, 5229-5238페이지, 2021.[22] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun. 균일한 샘플링을 사용한 단일 경로 원샷 신경 구조 검색. ECCV, 544-560페이지. Springer, 2020.[23] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. CVPR, 1580-1589페이지, 2020. 6,[24] Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, and Qi Tian. Ghostnets on heterogeneous devices via cheap operations. Int. J. Comput. Vis., 130(4):1050-1069, 2022.[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv, 2016.[26] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Mobilenetv3 검색. ICCV에서, 2019. 2, 5, 6, 7,[27] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. Mobilenets: 모바일 비전 애플리케이션을 위한 효율적인 합성곱 신경망. arXiv 사전 인쇄본 arXiv:1704.04861, 2017. 4,[28] Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc Le. 선형 시간의 변압기 품질. ICML에서, 9099-9117페이지. PMLR, 2022.[29] Tao Huang, Lang Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Lightvit: Towards light-weight convolutionfree vision transformers. arXiv preprint arXiv:2207.05557, 2022.[30] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reduction internal covariate shift. In ICML, 2015.[31] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. MLSys, 3:711–732, 2021. 1,[32] Congfeng Jiang, Yitao Qiu, Weisong Shi, Zhefeng Ge, Jiwei Wang, Shenglei Chen, Christophe Cerin, Zujie Ren, GuoyaoXu, Jiangbin Lin. Alibaba 클라우드 데이터센터의 공동 배치 워크로드 특성화. IEEE Trans. on Cloud Comput., 2020.[33] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, Tushar Krishna. 주의 성능 병목 현상 완화를 위한 최적화된 데이터 흐름. arXiv 사전 인쇄본 arXiv:2107.06419, 2021.[34] Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya. Reformer: 효율적인 변압기. ICLR, 2020. 2,[35] Jonathan Krause, Michael Stark, Jia Deng 및 Li Fei-Fei. 세분화된 분류를 위한 3D 객체 표현. ICCVW, 554-561페이지, 2013.[36] Alex Krizhevsky, Geoffrey Hinton 외. 작은 이미지에서 여러 계층의 기능 학습. 2009.[37] Alex Krizhevsky, Ilya Sutskever 및 Geoffrey E Hinton. 딥 합성곱 신경망을 사용한 Imagenet 분류. Commun. ACM, 60(6):84–90, 2017.[38] Yann LeCun, John Denker 및 Sara Solla. 최적의 뇌 손상. NeurIPS, 2, 1989.[39] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li 및 Xiaojun Chang. Ds-net++: CNN 및 비전 변환기에서 효율적인 추론을 위한 동적 가중치 슬라이싱.IEEE Trans. Pattern Anal. Mach. Intell., 2022.[40] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren. Efficientformer: 모바일넷 속도의 비전 변환기. NeurIPS, 2022. 1,[41] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár. 밀집 객체 감지를 위한 초점 손실. ICCV, 2980-2988페이지, 2017.[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft coco: 컨텍스트의 일반적인 객체. ECCV, 2014. 5,[43] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin Transformer v2: 용량 및 해상도 확장. CVPR에서는 2022.[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin 및 Baining Guo. Swin 변환기: 이동된 창을 사용하는 계층적 비전 변환기입니다. ICCV, 2021. 1, 2, 3, 5, 6, 7,[45] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang 및 Trevor Darrell. 네트워크 가지치기의 가치를 다시 생각해보세요. ICLR, 2018. 2,[46] Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. ICLR, 2018.[47] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang 및 Rongrong Ji. 시각 및 언어 작업을 위한 그룹별 변환을 통한 경량 변환기를 향해. IEEE Trans. Image Process., 31:3386-3398, 2022.[48] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng 및 Jian Sun. Shufflenet v2: 효율적인 CNN 아키텍처 설계를 위한 실용적인 가이드라인. ECCV, 페이지 116–131, 2018. 6,[49] Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed Waqas Zamir, Rao Muhammad Anwer 및 Fahad Shahbaz Khan. Edgenext: 모바일 비전 애플리케이션을 위해 효율적으로 통합된 cnn-transformer 아키텍처입니다. ECCVW, 2022. 1,[50] Sachin Mehta 및 Mohammad Rastegari. Mobilevit: 가볍고 범용이며 모바일 친화적인 비전 변환기입니다. ICLR, 2021. 1, 2, 6,[51] Sachin Mehta 및 Mohammad Rastegari. 모바일 비전 변환기를 위한 분리 가능한 selfattention입니다. arXiv 사전 인쇄본 arXiv:2206.02680, 2022. 2, 6,[52] Paul Michel, Omer Levy, Graham Neubig. 열여섯 개의 머리가 정말로 하나보다 낫습니까? NeurIPS, 32, 2019.[53] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, Jan Kautz. 신경망 가지치기를 위한 중요성 추정. CVPR, 11264–11272페이지, 2019.[54] Vinod Nair와 Geoffrey E Hinton. 정류 선형 단위가 제한 볼츠만 머신을 개선합니다. ICML, 2010.[55] ME Nilsback과 Andrew Zisserman. 꽃 분류를 위한 시각적 어휘. CVPR, 2006.[56] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, Brais Martinez. Edgevits: 비전 변환기를 갖춘 모바일 기기의 경쟁 경량 CNN. ECCV에서, 2022. 1,[57] Zizheng Pan, Jianfei Cai, Bohan Zhuang. hilo 어텐션을 갖춘 빠른 비전 변환기. NeurIPS, 2022. 2,[58] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, CV Jawahar. 고양이와 개. CVPR에서, 3498-3505쪽, 2012.[59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: 필수 스타일, 고성능 딥 러닝 라이브러리. NeurIPS, 2019.[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár. 네트워크 설계 공간 설계. CVPR, 10428-10436페이지, 2020.[61] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, Bo Dai. Combiner: 희소 계산 비용을 갖춘 전체 어텐션 변환기. NeurIPS, 34:22470-22482, 2021. 2,[62] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, Neil Houlsby. 전문가의 희소한 혼합으로 비전 확장. NeurIPS, 34:8583-8595, 2021.[63] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. Mobilenetv2: 역 잔차 및 선형 병목 현상. CVPR, 4510-4520페이지, 2018. 5, 6, 7,[64] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna. 컴퓨터 비전을 위한 인셉션 아키텍처 재고. CVPR, 2818-2826페이지, 2016.[65] Hamid Tabani, Ajay Balasubramaniam, Shabbir Marzban, Elahe Arani, Bahram Zonooz. 리소스 제약이 있는 장치의 변압기 효율성 개선. DSD, 449-456페이지, 2021.[66] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le. Mnasnet: 모바일을 위한 플랫폼 인식 신경 구조 검색. CVPR, 2820-2828페이지, 2019.[67] Mingxing Tan과 Quoc Le. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. ICML, 2019. 6,[68] Mingxing Tan과 Quoc V Le. Mixconv: 혼합 깊이별 합성곱 커널. BMVC, 2019.[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. 데이터 효율적인 이미지 변환기 교육 및 주의를 통한 증류. ICML에서. PMLR, 2021. 1, 2, 3, 5, 7,[70] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel 및 Anurag Ranjan. 향상된 1밀리초 모바일 백본. arXiv 사전 인쇄 arXiv:2206.04040, 2022. 6,[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. NeurIPS, 2017. 2, 3,[72] Anand Venkat, Tharindu Rusira, Raj Barik, Mary Hall 및 Leonard Truong. Swirl: 딥 신경망을 위한 고성능 다중 코어 CPU 코드 생성. Int. J. High Perform. Comput. Appl., 33(6):1275-1289, 2019. 1,[73] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov. 다중 헤드 자기 주의 분석: 전문화된 헤드가 힘든 작업을 수행하고 나머지는 정리할 수 있습니다. ACL, 5797-5808페이지, 2019.[74] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, Hao Ma. Linformer: 선형 복잡도를 갖춘 자기 주의. arXiv 사전 인쇄본 arXiv:2006.04768, 2020. 2,[75] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. 피라미드 비전 변환기: 합성곱 없는 고밀도 예측을 위한 다재다능한 백본. ICCV에서, 2021. 2,[76] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. Pvtv2: 피라미드 비전 변환기를 사용한 개선된 기준선. Comput. Vis. Media, 2022. 7,[77] Ross Wightman. Pytorch 이미지 모델, 2019.[78] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, Lu Yuan. Tinyvit: 작은 비전 변환기를 위한 빠른 사전 훈련 증류. ECCV에서, 2022.[79] Sitong Wu, Tianyi Wu, Haoru Tan, Guodong Guo. Pale transformer: Pale-shaped attention을 갖춘 일반 비전 변환기 백본. AAAI에서, 2022.[80] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor Darrell, Ross Girshick. 초기 합성은 변환기가 더 잘 볼 수 있도록 도와줍니다. NeurIPS, 2021.[81] Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe Lin, Alan Yuille. 향상된 자기 주의가 있는 가벼운 비전 변환기. CVPR, 페이지 11998-12008, 2022. 1,[82] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li 및 Jan Kautz. Nvit: 비전 변환기 압축 및 매개변수 재분배. arXiv 사전 인쇄 arXiv:2110.04869, 2021.[83] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng 및 Shuicheng Yan. Metaformer는 실제로 비전에 필요한 것입니다. CVPR, 페이지 10819–10829, 2022. 6,[84] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng 및 Shuicheng Yan. Tokensto-token vit: imagenet에서 처음부터 비전 변환기를 교육합니다. ICCV, 2021.[85] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin 및 David Lopez-Paz. 혼동: 경험적 위험 최소화를 넘어서는 것입니다. ICLR, 2018.[86] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu 및 Lu Yuan. Minivit: 가중치 다중화를 통해 비전 변환기를 압축합니다. CVPR, 2022. 1,[87] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin 및 Jian Sun. Shufflenet: 모바일 장치를 위한 매우 효율적인 컨벌루션 신경망입니다. CVPR, 2018. 3, 4, 5,[88] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li 및 Yi Yang. 무작위 삭제 데이터 증대. AAAI, 2020.[89] Barret Zoph, Vijay Vasudevan, Jonathon Shlens 및 Quoc V Le. 확장 가능한 이미지 인식을 위한 학습 가능한 아키텍처. CVPR, 8697-8710페이지, 2018. 6,
