--- ABSTRACT ---
대규모 언어 모델은 매우 유연하여 추상적 요약 및 개방형 질의응답과 같은 광범위한 생성 작업을 해결할 수 있는 것으로 입증되었습니다. 이 논문에서는 작은 오디오 인코더를 직접 연결하여 음성 인식을 수행하도록 함으로써 LLM의 기능을 확장합니다. 텍스트 토큰 임베딩에 청각 임베딩 시퀀스를 직접 추가하면 LLM을 자동 음성 인식(ASR) 시스템으로 변환하여 텍스트 대응물과 정확히 동일한 방식으로 사용할 수 있습니다. Multilingual LibriSpeech(MLS)에 대한 실험에서는 오픈 소스 LLaMA-7B에 컨포머 인코더를 통합하면 단일 언어 기준선보다 18% 더 우수한 성능을 발휘하고 LLAMA가 영어 텍스트로 압도적으로 훈련되었음에도 불구하고 다국어 음성 인식을 수행할 수 있음을 보여줍니다. 또한 LLM을 훈련 중에 완전히 동결하여 원래 기능을 유지하고 오디오 인코더를 확장하고 오디오 인코더 스트라이드를 늘려 임베딩을 줄일 수 있는지 조사하기 위해 절제 연구를 수행합니다. 이러한 연구 결과는 LLM이 정지된 경우나 오디오 인코더에서 거의 1초에 달하는 스트라이드가 사용되어 LLM이 장문 오디오에서 작동할 가능성을 열어주는 경우에도 다국어 ASR이 가능하다는 것을 보여줍니다.
--- METHOD ---
ology 우리의 접근 방식은 임베딩의 모달리티와 상관없이 임베딩 시퀀스를 모델링하기 위해 대규모 언어 모델(LLM)을 사용하는 것을 중심으로 합니다. 텍스트 임베딩과 동일한 공간에서 고정 길이의 시각적 임베딩 시퀀스를 생성하기 위해 시각적 인코더를 활용하는 [11, 24]의 작업에서 영감을 얻어 사전 학습된 오디오 인코더를 사용하여 가변 길이의 청각적 임베딩 시퀀스를 생성합니다. 청각적 임베딩을 조건화함으로써 대규모 언어 모델은 음성 인식 및 기타 음성 기반 작업을 수행할 수 있습니다. 따라서 기존 LLM과 제안 사이의 유일한 한계적인 차이점은 서로 다른 모달리티의 임베딩을 혼합한 것입니다. 2. 청각적 임베딩 우리는 컨포머 기반 오디오 인코더를 사용하여 임베딩 시퀀스를 생성합니다. 이는 프롬프트와 유사하지만 임베딩 공간에서 LLM을 조건화하는 데 사용됩니다. 오디오 인코더가 유용한 임베딩을 추출할 수 있도록 처음에는 간단한 연결주의 시간 분류(CTC) 손실에 대해 학습합니다. 이 인코더의 시퀀스 출력은 매우 길 수 있으므로 연속적인 임베딩을 쌓아 길이를 더 줄일 수 있으며, 더 크지만 더 적은 임베딩이 생성됩니다. 인코더 구조는 그림 1을 참조하세요. 이 작업에서는 평균적으로 단일 벡터에 여러 토큰 가치의 정보를 포함하는 960ms의 오디오를 인코딩하는 임베딩까지 다양한 수준의 스태킹을 조사합니다. 그런 다음 쌓인 임베딩을 큰 언어 모델의 숨겨진 차원으로 투영하여 텍스트 임베딩에 추가할 수 있도록 합니다. 2.2 큰 언어 모델 대부분
--- EXPERIMENT ---
영어: Multilingual LibriSpeech(MLS)에 대한 s는 오픈 소스 LLaMA-7B에 컨포머 인코더를 통합하면 단일 언어 기준선보다 18% 더 우수한 성능을 보이고 LLAMA가 영어 텍스트에서 압도적으로 훈련되었음에도 불구하고 다국어 음성 인식을 수행할 수 있음을 보여줍니다. 나아가, LLM을 훈련 중에 완전히 동결하여 원래 기능을 유지하고 오디오 인코더를 확장하고 오디오 인코더 스트라이드를 늘려 더 적은 임베딩을 생성할 수 있는지 조사하기 위해 절제 연구를 수행합니다. 이러한 연구의 결과는 LLM이 동결되거나 오디오 인코더에서 거의 1초의 스트라이드가 사용되어 LLM이 장문 오디오에서 작동할 가능성이 열려도 다국어 ASR이 가능하다는 것을 보여줍니다. 서론 대규모 언어 모델(LLM)[4, 7, 23, 21]은 광범위한 작업을 해결할 수 있는 매우 유연한 모델임이 입증되었습니다. 이러한 시스템은 방대한 양의 비지도 텍스트 데이터에서 다음 토큰을 예측하도록 훈련되어 네트워크 매개변수에서 세계 지식을 인코딩하는 방법을 학습하여 추상적 요약, 질의 응답, 지식 검색, 텍스트 생성 및 기계 번역과 같은 많은 다운스트림 오픈 도메인 생성 작업에 유용합니다. 그러나 LLM과 순전히 텍스트를 통해 상호 작용하는 것은 많은 경우 제한적일 수 있습니다. 텍스트로 포착하기 어려운 정보를 인코딩하는 다른 많은 구조화된 모달리티가 있습니다. 예를 들어, 오디오는 사람의 말에서 광범위한 감정을 인코딩할 수 있으며 이미지는 텍스트로 설명하기 훨씬 어려울 수 있는 객체의 기하학과 위치를 나타낼 수 있습니다. 최근에 발표된 연구에서는 다른 모달리티를 수집하는 기능으로 LLM을 확장했습니다. 멀티 모달 PALM-E[11]는 대규모 사전 훈련된 시각 변환기[10]와 PaLM LLM[7]을 결합하여 로봇 작업에서 최첨단 성능을 달성할 수 있었습니다. 마찬가지로 [24]의 작업은 사전 훈련된 시각적 모델과 LLaMA [5]의 파생 모델인 대규모 언어 모델 Vicuna를 활용하여 시각적 및 텍스트 입력 모두로 추론할 수 있는 정렬된 모델을 생성합니다. 나아가 [12]는 오디오 질문 답변 코퍼스에서 훈련된 정렬된 오디오 인코더가 있는 LLaMA의 확장인 LTU를 제안하여 소리로 추론하고 이해할 수 있도록 합니다. 그러나 LTU는 음성 이해 및 인식 능력이 제한적입니다. *Meta AI에서 인턴십 중에 수행한 작업. 이러한 대규모 언어 모델 지향 시스템에는 엄청난 수의 매개변수가 있기 때문에 전체 시스템을 새로운 작업에 적용하는 것이 종종 계산적으로 비실용적이고 비용이 많이 들 수 있습니다. [24]의 작업은 언어 모델에 맞춰 시각적 인코더의 출력을 조정하는 단일 투영 계층을 훈련하여 매개변수 효율성이 매우 높은 접근 방식을 나타냅니다. 그러나 이는 새로운 작업에 대한 시스템의 적응성과 성능을 심각하게 제한합니다. 반대로 다중 모달 PaLM-E [11]는 전체 시각적 인코더와 언어 모델을 함께 훈련하는 것을 조사했습니다. 그러나 전체 언어 모델을 적용하는 것은 매우 비용이 많이 들고 비실용적입니다.대안적인 접근 방식으로는 새로운 작업에 대해 학습된 어댑터 계층[20, 13] 또는 접두사 임베딩[18]을 삽입하는 것이 있습니다.이러한 접근 방식은 효과적인 매개변수 효율적 접근 방식이지만 추론 비용을 증가시킵니다.저랭크 적응[14]은 저랭크 행렬을 사용하여 시스템의 일부 매개변수를 수정하여 이러한 문제를 해결하며 매우 유망한 것으로 나타났습니다.이 접근 방식은 학습 중에 메모리 효율적이며 추론 런타임에 영향을 미치지 않습니다.기여: 이 논문에서는 가변 길이 오디오 임베딩 시퀀스에 LLM을 조건화하여 대규모 언어 모델에 음성 인식 기능을 제공하는 방법을 조사합니다.오디오 시퀀스에 조건화된 디코더 전용 대규모 언어 모델이 단일 언어 감독 학습된 기준선보다 우수한 다국어 음성 인식을 수행할 수 있음을 보여줍니다.또한 이 논문에서는 오디오 인코더 모델 크기와 프레임 속도, LLM 매개변수의 저랭크 적응, 텍스트 토큰 마스킹 및 대규모 언어 모델의 유형과 같이 더 나은 인식 성능을 가능하게 할 수 있는 다양한 요소를 살펴봅니다. 마지막으로 오디오 인코더의 출력을 분석하여 오디오 임베딩이 유사하고 텍스트 토큰과 정렬되어 있음을 보여줍니다.2 방법론 우리의 접근 방식은 임베딩의 모달리티에 관계없이 임베딩 시퀀스를 모델링하기 위해 대규모 언어 모델(LLM)을 사용하는 것을 중심으로 합니다. 텍스트 임베딩과 동일한 공간에서 고정 길이의 시각적 임베딩 시퀀스를 생성하기 위해 시각적 인코더를 활용하는 [11, 24]의 작업에서 영감을 얻어 사전 학습된 오디오 인코더를 사용하여 가변 길이의 청각적 임베딩 시퀀스를 생성합니다. 청각적 임베딩을 조건으로 하여 대규모 언어 모델이 음성 인식 및 기타 음성 기반 작업을 수행할 수 있습니다. 따라서 기존 LLM과 제안 간의 유일한 한계적 차이점은 서로 다른 모달리티의 임베딩을 혼합한 것입니다. 2. 청각 임베딩 우리는 컨포머 기반 오디오 인코더를 사용하여 임베딩 시퀀스를 생성합니다.이 임베딩 시퀀스는 프롬프트와 유사하지만 임베딩 공간에서 LLM을 조건화하는 데 사용됩니다.오디오 인코더가 유용한 임베딩을 추출할 수 있도록 처음에는 간단한 연결주의 시간 분류(CTC) 손실에 대해 학습합니다.이 인코더의 시퀀스 출력은 매우 길 수 있으므로 연속적인 임베딩을 쌓아 길이를 더 줄일 수 있으며, 더 크지만 더 적은 임베딩이 생성됩니다.인코더 구조는 그림 1을 참조하세요.이 작업에서 우리는 평균적으로 단일 벡터에 여러 토큰 가치의 정보를 포함하는 960ms의 오디오를 인코딩하는 임베딩까지 다양한 수준의 스태킹을 조사합니다.그런 다음 쌓인 임베딩을 큰 언어 모델의 숨겨진 차원으로 투영하여 텍스트 임베딩에 추가할 수 있도록 합니다.2.2 큰 언어 모델 대부분의 실험은 가장 작은 LLaMA-7B 모델[23]을 사용합니다. 이 시스템의 인과적 자기 주의 매개변수는 매개변수 효율적인 저랭크 적응(LORA)[14]을 사용하여 조정되고 다른 모든 매개변수는 고정됩니다.절제에서 ASR을 수행하기 위해 LLM 매개변수를 전혀 조정해야 하는지 조사합니다.또한 LLaMA를 다양한 BLOOM 모델[21]로 대체하여 LLM 선택이 중요한지 조사합니다.ASR-LLM 문제는 LLM이 오디오 시퀀스의 정보를 다시 토해내야 하는 복사/번역 작업으로 재해석될 수 있습니다.오디오 인코더가 텍스트 임베딩과 정렬된 임베딩 시퀀스를 제공하는 경우 문제는 LLM의 전체 용량이 필요하지 않은 반복 작업으로 축소됩니다.이 해석은 섹션 4에서 조사합니다.시스템 개요는 그림 2를 참조하세요.10ms 80ms 百目 필터뱅크 기능 CNN 컨포머 인코더 HO-COD 그림 1: 오디오 인코더 아키텍처.초기 컨포머는 CTC 손실에 대해 훈련됩니다. 그 후 출력은 스택되어 LLM의 차원으로 투영되어 호환성을 보장합니다. 이 그림은 240ms 임베딩을 초래하는 3의 스태킹 팩터를 보여줍니다. 240ms 오디오 인코더<bos> 기타와 피아노 치는 걸 좋아해요! 텍스트 임베딩 매트릭스 대형 언어 모델 TTTTTTTT 기타와 피아노 치는 걸 좋아해요!<eos> 그림 2: 모델 아키텍처. 오디오 인코더에서 생성된 임베딩 시퀀스는 텍스트 임베딩 시퀀스에 직접 추가됩니다. 이것은 다음 토큰을 예측하는 작업을 맡은 디코더 전용 LLM에 직접 공급됩니다. LLM은 동결되거나 LORA와 같은 매개변수 효율적 접근 방식으로 조정되거나 완전히 미세 조정될 수 있습니다. 이 작업에서는 앞의 두 가지를 조사합니다. 3 실험 평가 3.1 데이터 세트 Multilingual LibriSpeech(MLS)는 LibriVox [19]의 오디오북 읽기에서 파생된 50k 시간 ASR 코퍼스입니다. 영어(en), 독일어(de), 네덜란드어(nl), 프랑스어(fr), 스페인어(es), 이탈리아어(it), 포르투갈어(pt) 및 폴란드어(pl)의 8개 언어로 구성된 데이터 세트는 주로 영어로 44.5k 시간입니다. 포르투갈어와 폴란드어와 같은 일부 저자원 언어는 각각 161시간과 103시간만 있습니다. 데이터 세트의 불균형을 설명하기 위해 하위 리소스 언어에서 오버샘플링을 통해 [9, 1]에 설명된 전략을 따릅니다. 각 발화는 최대 20초 길이입니다. 보고된 단어 오류율에는 MLS에서 제공하는 n-gram 모델 사용이 포함되지 않습니다. 3.2 모델 설정 및 학습 세부 정보 오디오 인코더 오디오 인코더는 10ms 프레임 속도로 80d 필터뱅크 피처에서 작동합니다. 8의 거친 유효 스트라이드를 가진 합성곱 피처 추출기와 출력을 512차원으로 투영하는 선형 계층과 18개의 비마카롱 컨포머 블록 계층으로 구성됩니다. 블록은 512의 숨겨진 차원, 2048의 피드포워드 네트 차원, 11의 합성곱 커널 크기 및 8개의 어텐션 헤드를 갖습니다. 마지막 선형 계층은 1547 크기의 SentencePiece [16] 어휘를 사용하여 CTC 손실을 사용하여 오디오 인코더를 사전 학습하는 데 사용됩니다. 최종 선형 계층은 사전 학습 후 삭제됩니다. 7,200만 개의 매개변수를 갖는 이 비교적 작은 오디오 인코더의 효율성은 크기를 확장하고, 스트라이드 수준을 줄이고, 다양한 비지도 및 반지도 학습 접근 방식을 활용하면 상당히 개선될 수 있습니다[9, 1, 22, 2, 3, 6, 8]. 그러나 우리는 더 간단한 설정으로 제한하고 지도 학습만을 사용하여 모델을 훈련합니다. 우리는 LLM이 음성 인식을 수행하도록 조건화될 수 있음을 보여주는 데 주력하고 이 작업 수행 능력을 향상시키는 요인이 무엇인지 조사합니다. 청각 임베딩 인코더의 출력은 프레임 속도가 80ms인 512차원 벡터의 시퀀스입니다. 시퀀스 길이와 메모리 소비를 줄이기 위해 연속된 n개의 프레임을 쌓아 512n차원 프레임을 형성하고, 이 프레임은 LLAMA-7B 차원과 일치하도록 4096차원 임베딩에 투영되며, 그 결과 프레임 속도는 80nms입니다. 우리는 12개의 연속된 프레임을 쌓는 것에 해당하는 최대 960ms의 프레임 속도까지 임베딩을 생성하는 것을 조사합니다. 이러한 임베딩은 텍스트 임베딩(그림 2에서 지정한 대로)에 추가되어 다음 텍스트 기반 토큰을 예측하는 작업을 맡은 LLM에 공급됩니다. 대규모 언어 모델 적응 우리는 피드 포워드 네트, 임베딩 및 최종 선형 출력 계층을 변경하지 않고 셀프 어텐션 메커니즘의 키, 쿼리, 값 및 출력 계층을 적응시키기 위해 저랭크 적응(LoRA) 접근 방식을 사용합니다. 달리 지정하지 않는 한 기본 LoRA 하이퍼파라미터는 R = 8 및 a = 16의 순위로 설정됩니다. 우리는 절제 연구에서 R의 영향을 조사합니다. = 훈련 오디오 인코더는 처음에 ẞ₁ = 0.9, ẞ0.98 [15]의 Adam 옵티마이저를 사용하여 훈련되었습니다. 학습률은 1e-3의 피크 값까지 20k 학습 단계에 걸쳐 선형적으로 워밍업한 다음 지수 감소 일정에 따라 수행되었습니다. 이는 최대 500초의 오디오를 GPU당 배치 크기로 사용하여 4개의 그래디언트 축적을 사용하여 16개의 NVIDIA A100 40GB에서 수행되었습니다. 가장 좋은 검증 손실이 있는 체크포인트가 선택되었습니다. 오디오 인코더와 LLM이 있는 조인트 시스템은 그 후 250k 단계에 걸쳐 5e-4의 피크 학습률까지 5k 워밍업 단계의 유사한 일정으로 학습되어 5e-6으로 감소했습니다. 학습은 종종 100k 단계 이내에 일찍 중단되었습니다. 이는 최대 80초의 배치 크기를 사용하여 4개의 그래디언트 축적 단계를 사용하여 64개의 NVIDIA A100 40GB에서 수행되었습니다. 가장 낮은 검증 손실이 있는 체크포인트가 평가를 위해 선택되었습니다. 평가 보고된 모든 단어 오류율(WER)은 [19]에서 제공한 외부 언어 모델 사용을 제외합니다. 디코딩은 최대 출력 토큰 길이가 200인 탐욕적 검색을 사용하여 수행됩니다.3.3 기준선 우리의 접근 방식은 전적으로 지도 학습에 의존하므로 가장 관련성 있는 기준선은 MLS[19]에서 제공하는 단일 언어 모델입니다.우리는 동일한 데이터 샘플링 전략을 따르고 표 1: MLS 데이터 세트에 대한 언어별 및 평균 WER 성능.첫 번째 블록 단일 언어 모델은 각 언어에 대해 별도의 모델을 학습하는 것을 말합니다.두 번째 블록 다국어 모델은 모든 언어에 대해 단일 모델을 동시에 학습하는 것을 말합니다.마지막 블록은 모든 언어에 대해 모델을 사전 학습한 다음 각 언어에 대해 별도로 사전 학습된 검사점을 미세 조정하는 것을 말합니다. 학습 가능한 매개변수 en de nl fr es it pt pl 평균 지도 학습: 단일 언어 모델 36L Transformer CTC [19] 36L Transformer CTC [19] LM을 사용한 지도 학습: 다국어 모델 디코더 전용 LLaMA-7B(960ms) 0.3B 0.3B6.8 7.1 13.1 6.6 6.7 11.8 20.5 21.7 11.5.9 6.5 12.0 5.6 6.1 10.5 19.5 20.4 10.디코더 전용 LLaMA-7B(480ms) 디코더 전용 LLaMA-7B(240ms) 디코더 전용 LLaMA-7B(160ms) 디코더 전용 LLaMA-7B(80ms) 0.10B 7.6 7.4 11.9 7.0 6.1 11.4 18.6 19.1 11.0.09B 7.3 7.4 11.9 6.7 6.1 11.5 18.3 17.0 10.0.09B 7.0 7.2 11.4 6.4 6.11.5 17.5 16.7 10.0.08B 6.9 7.0 11.3 6.2 5.4 11.6 17.4 14.8 10.0.08B 6.2 6.7 11.3 5.5 5.2 10.8 16.2 15.9 9. 자기 지도 학습 + 단일 언어 미세 조정 w2v2 XLSR-53 w/ LM 0.3B 7.0 10.8 7.6 6.3 10.4 14.7 17.10.setup [9]에서와 같이 단일 언어 미세 조정을 기준으로 자체 감독 XLSR-53도 포함합니다.문헌에는 MLS 벤치마크에서 매우 경쟁력 있는 결과를 달성한 많은 대안적이고 강력한 오디오 인코더가 있지만, 이러한 시스템은 종종 훨씬 더 많은 계산 및 학습 가능한 매개변수를 사용하여 자체/반 감독 접근 방식을 사용하여 학습되어 목표에 대한 직교적 기여를 나타냅니다.3.4 주요 결과 LLM의 대부분 매개변수를 고정하고 매우 작은 오디오 인코더를 사용하므로 우리의 접근 방식은 기준선에 비해 학습 가능한 매개변수가 훨씬 적습니다(표 1 참조).예상대로 프레임 속도가 가장 높은(80ms) 디코더 전용 LLaMA는 프레임 속도가 낮은 시스템보다 성능이 뛰어나며 단일 언어 모델보다 평균 단어 오류율이 18%, 10% 더 높습니다. 프레임 속도를 줄이면 성능이 저하되지만 큰 스트라이드(480/960ms)를 가진 시스템조차도 원래 필터뱅크 시퀀스를 최대 96배까지 줄여 단일 언어 기준선과 경쟁할 수 있습니다. 이러한 높은 스트라이드 시스템은 오디오 시퀀스 길이를 수십 배 압축하여 장문 오디오에서 작동할 수 있는 실행 가능한 방법 중 하나가 될 수도 있습니다. 3.5 Ablation Studies for Larger Audio Encoder 오디오 인코더 스트라이드 수준은 LLAMA의 음성 인식 능력에 상당한 영향을 미칩니다. 따라서 오디오 인코더의 레이어 수를 조사하여 7200만에서 1억 4200만 매개변수까지 확장합니다(표 2 참조). 가장 큰 오디오 인코더는 표 2: MLS 데이터 세트에서 오디오 인코더의 레이어 수의 영향 조사. pt pl Avg trainable params en de nl fr es it 18L Conformer(240ms) 0.09B 7.0 7.2 11.24L Conformer(240ms) 0.11B 6.6 6.6 10.36L Conformer(240ms) 0.16B 6.1 6.3 11.6.4 6.5.9 5.5.5 4.11.5 17.5 16.7 10.11.5 14.5 16.8 9.11.1 15.9 16.7 9.36 conformer 레이어와 240ms 스트라이딩은 80ms 스트라이딩을 사용하는 18 레이어 오디오 인코더의 성능과 일치하는 평균 WER 9.7%를 생성합니다. 이는 LLM 컨디셔닝에 사용되는 더 높은 품질의 임베딩을 생성하는 데 있어 오디오 인코더의 중요성을 보여줍니다.낮은 순위 적응 모든 실험은 LLAMA 셀프 어텐션 매개변수를 조정하기 위해 낮은 순위 적응 매개변수를 R =로 고정했습니다.RE [0, 8, 16, 32]를 조정하여 LoRA의 영향을 추가로 조사합니다.R = 0으로 설정하는 것은 LLaMA를 완전히 동결하는 것과 같습니다.표 3의 모든 실험은 240ms 스트라이딩을 사용합니다.각 순위는 약 100만 개의 학습 가능한 값을 추가합니다.표 3: 순위 R의 영향 조사.R = 0으로 설정하는 것은 LLM을 동결하는 것과 같습니다. en de nl fr es it 7.5 7.4 12.0 6.8 5.9 11.pt pl Avg 18.2 17.4 10.trainable 매개변수 0.08B 0.09B 7.0 7.2 11.4 6.4 6.0 11.5 17.5 16.7 10.0.10B 6.3 6.8 11.4 5.7 5.5 10.8 16.3 15.0 9.0.11B 6.0 6.5 11.1 5.4 5.2 10.9 15.7 15.3 9.디코더 전용 LLaMA-7B(240ms) R =디코더 전용 LLaMA-7B(240ms) R =디코더 전용 LLaMA-7B(240ms) R = 디코더 전용 LLaMA-7B(240ms) R = 매개변수. 흥미롭게도 LLaMA를 동결하고 오디오 인코더만 학습하면 평균 WER이 10.9%인 합리적인 결과가 나옵니다. 이렇게 하면 LLM의 원래 기능도 유지됩니다. 다른 모든 미세 조정 설정은 텍스트 기반 작업을 수행하는 LLAMA의 기능에 부정적인 영향을 미칩니다[11]. 나아가 학습 가능한 매개변수의 순위를 높이면 성능이 크게 향상되며, R 32는 평균 WER 9.5%를 달성하여 80ms 스트라이딩과 R 8을 사용하는 표 1의 최고 시스템보다 성능이 뛰어납니다. 이러한 결과를 바탕으로 LLM 전체를 매개변수 조정하면 성능이 추가로 향상될 수 있지만 학습 비용이 훨씬 더 많이 듭니다. = = 마스킹 학습 작업은 인과적 다음 토큰 예측을 기반으로 하지만 필요한 정보가 포함된 오디오 시퀀스에 따라 달라지므로 텍스트 토큰을 마스킹하면 성능을 높이는 데 유용할 수 있습니다[17]. 아래 표는 텍스트 토큰의 일부 F = [0.000, 0.125, 0.250, 0.375, 0.500]을 무작위로 대체했을 때의 성능을 보여줍니다.<unk> 훈련 중 토큰을 마스킹합니다. 훈련 중 마스크된 텍스트 토큰을 도입하면 주목할 만한 결과가 발생할 수 있습니다. 표 4: 훈련 중 텍스트 토큰의 일부 F를 마스킹합니다. 디코더 전용 LLaMA-7B(240ms) F = 0.디코더 전용 LLaMA-7B(240ms) F = 0.디코더 전용 LLaMA-7B(240ms) F = 0.디코더 전용 LLaMA-7B(240ms) F = 0.디코더 전용 LLaMA-7B(240ms) F = 0.디코더 전용 LLaMA-7B(240ms) F = 0.trainable 매개변수 0.09B en de nl fr es it pt pl Avg 7.0 7.0.09B 6.7 7.0.09B 0.09B 0.09B 6.5 6.6.5 7.6.4 7.11.4 6.4 6.0 11.5 17.5 16.7 10.11.3 6.1 5.6 11.3 16.8 16.3 10.11.3 6.1 5.6 11.2 16.5 15.1 9.11.4 6.1 5.4 11.3 17.4 16.2 10.11.5 6.2 5.1 11.1 17.1 16.8 10.= 성능이 향상되었으며 F 0.250은 기준선 F = 0.000에 비해 평균 5.7%의 WER 개선으로 이어졌습니다. 그러나 이 지점을 넘어서면 마스킹 수준을 높이면 리소스가 낮은 포르투갈어와 폴란드어에 부정적인 영향을 미칩니다. 언어별 데이터의 양에 따라 다른 마스킹 수준을 설정할 수 있지만 이 연구는 향후 작업으로 미룹니다. 대규모 언어 모델 LLAMA는 주로 영어 텍스트에서 훈련되었으며 다른 언어를 포함하는 부분도 일부 포함되었습니다[23]. 반면 BLOOM[21]은 다국어로 특별히 설계되었으며 훨씬 더 많은 언어를 지원합니다. 따라서 LLM의 영향과 LLM 규모가 커짐에 따라 성능이 어떻게 변하는지 이해하기 위해 LLAMA-7B를 {BLOOM-560M, BLOOM-1B7, BLOOM-7B1} 중 하나로 대체합니다. 표 5를 참조하세요. 표 5 비교: LLaMA-7B를 다양한 BLOOM 언어 모델로 대체. 학습 가능 en de nl fr es it pt pl Avg 디코더 전용 LLaMA-7B(240ms) 디코더 전용 BLOOM-560M(240ms) 디코더 전용 BLOOM-1B7(240ms) 디코더 전용 BLOOM-7B1(240ms) 매개변수 0.09B 7.0 7.2 11.4 6.4 6.0.07B 8.2 8.4 12.6 7.3 6.5 12.5 18.0.08B 7.5 8.3 12.2 6.7 5.8 12.2 16.6 19.0 11.0.08B 7.0 7.8 12.1 5.9 5.3 11.8 15.6 17.7 10.11.17.16.7 | 10.19.8 11.LLAMA-7B와 비슷한 크기의 BLOOM-7B1에서 평균 WER에 유의미한 차이가 없음을 관찰했습니다. BLOOM은 다국어이지만 시스템이 다국어 음성 데이터 세트에서 학습되면 이 기능이 그렇게 큰 영향을 미치지 않는 듯합니다. 그러나 컨포머 오디오 인코더를 고정한 채 LLM을 확장하면 성능이 상당히 향상되는 명확한 추세가 있습니다. 4 오디오 인코더 텍스트 정렬 분석 섹션 2.2에서 가정한 대로 음성 인식 작업은 역류 작업으로 해석할 수 있습니다. 언어 모델은 오디오 인코더 출력 시퀀스에 있는 정보를 정리하고(동일한 순서로) 반복하는 작업을 담당합니다. 오디오 인코더는 텍스트 임베딩과 동일한 의미 공간에서 임베딩을 생성하도록 학습되었으므로 적절하게 학습된 시스템의 경우 오디오 및 텍스트 임베딩이 단조롭게 정렬되어야 함을 의미합니다. 따라서 영어 테스트 세트 예제에 대한 각 가능한 오디오 및 텍스트 임베딩 쌍 간의 코사인 유사도를 계산합니다. 이는 정렬의 영향에 대한 스트라이드 증가의 영향을 이해하기 위해 1의 LLAMA 모델에 대해 수행됩니다(그림 3 참조). 이러한 정렬 플롯은 인코더가 오디오 임베딩을 단조로운 방식으로 텍스트에 정렬하려고 시도하고 있다는 가설을 뒷받침합니다. 스트라이드가 증가함에 따라 오디오를 텍스트에 정렬하는 작업이 점점 더 어려워집니다. 또한 이는 언어 모델을 통한 다음 토큰 예측을 통해 간접적으로 학습하는 대신 오디오 인코더가 출력을 텍스트에 단조롭게 정렬하도록 학습하여 추가 감독을 통해 이점을 얻을 수 있는지 여부에 대한 의문을 제기합니다.
--- CONCLUSION ---
텍스트 토큰 텍스트 토큰 텍스트 토큰 오디오 토큰 (a) 오디오 토큰 (b) 오디오 토큰 오디오 토큰 (d) 오디오 토큰 그림 3: 영어 세트의 주어진 테스트 예제에 대한 모든 오디오 및 텍스트 임베딩 쌍 간의 쌍별 코사인 유사도. 하위 그림 (a)-(e)는 80ms에서 960ms까지의 스트라이딩을 갖는 표 1의 모델을 나타냅니다. 전반적으로 이 작업은 대규모 언어 모델을 사용하여 다국어 음성 인식을 구현하는 간단한 절차를 보여주었습니다. 오디오 임베딩 시퀀스를 앞에 추가하면 대규모 언어 모델을 트리거하여 디코더 전용 방식으로 음성 인식을 수행할 수 있습니다. 또한 이 작업은 오디오 인코더 스트라이드 및 크기 분석을 포함하여 더 나은 인식 성능을 구현하는 데 중요한 다양한 요소를 조사합니다. 이 논문에서는 또한 LLaMA와 BLOOM을 비교하여 LLM의 중요성, 낮은 순위 어댑터를 사용하여 LLM을 조정하는 것의 중요성, 마지막으로 LLM이 마스킹으로 입력을 증강하여 어떻게 더 나은 인식을 수행할 수 있는지 조사합니다. 인코더와 LLM의 공동 학습 후 오디오 임베딩이 텍스트 임베딩과 정렬되는 경향이 있는 것으로 나타났습니다. 향후 작업에서는 오디오 인코더를 언어 모델과 정렬하도록 직접 학습하여 이 관찰을 활용할 수 있습니다. 텍스트 토큰 텍스트 토큰 참고문헌 [1] Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. &quot;XLS-R: 규모에 따른 자체 감독 교차 언어 음성 표현 학습&quot;. arXiv 사전 인쇄본 arXiv:2111.09296(2021). [2] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed 및 Michael Auli. &quot;Wav2vec 2.0: 음성 표현의 자기 감독 학습을 위한 프레임워크&quot;. 2020년 제34회 신경 정보 처리 시스템 국제 컨퍼런스 회의록. [3] Junwen Bai, Bo Li, Yu Zhang, Ankur Bapna, Nikhil Siddhartha, Khe Chai Sim 및 Tara N. Sainath. &quot;다국어 ASR을 위한 공동 비지도 및 지도 학습&quot;. 2022 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). 2022. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. &quot;언어 모델은 몇 번의 샷 학습자입니다&quot;. Advances in Neural Information Processing Systems. 2020. [5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, Eric P. Xing. &quot;비쿠나: 인상적인 오픈소스 챗봇 영어: 90%* ChatGPT 품질의 GPT-4”. (2023)에서. [6] Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. &quot;음성 인식을 위한 랜덤 투영 양자화기를 사용한 자기 감독 학습&quot;. 39회 국제 기계 학습 컨퍼런스 논문집에서. 2022. [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “Palm: 경로를 통한 언어 모델링 확장”. arXiv 사전 인쇄본 arXiv:2204.02311(2022). [8] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu. &quot;w2v-BERT: 자기 감독 음성 사전 학습을 위한 대조 학습 및 마스크 언어 모델링 결합&quot;. 2021 IEEE 자동 음성 인식 및 이해 워크숍(ASRU). 2021. [9] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli. &quot;음성 인식을 위한 감독되지 않은 교차 언어 표현 학습&quot;. In: Interspeech. 2021. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin 등 &quot;확장 비전 220억 개의 매개변수로 변환&quot;. In: arXiv 사전 인쇄 arXiv:2302.05442(2023). [11] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu 등. “Palm-e: An 구현된 다중 모드 언어 모델&quot;. In: arXiv 사전 인쇄본 arXiv:2303.03378 (2023). [12] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. &quot;듣고, 생각하고, 이해하세요&quot;. In: arXiv 사전 인쇄본 arXiv:2305.10790 (2023). [13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. &quot;NLP를 위한 매개변수 효율적 전이 학습&quot;. In: 제36회 기계 학습 국제 컨퍼런스 회의록. Vol. 97. 2019. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. &quot;LORA: 대규모 언어 모델의 저순위 적응&quot;. International Conference on Learning Representations에서. 2022. [15] Diederik P. Kingma 및 Jimmy Ba. &quot;Adam: 확률적 최적화를 위한 방법&quot;. International Conference on Learning Representations(ICLR)에서. 2015. [16] Taku Kudo 및 John Richardson. &quot;SentencePiece: 신경망 텍스트 처리를 위한 간단하고 언어 독립적인 하위 단어 토크나이저 및 디토크나이저&quot;. 2018 자연어 처리 경험적 방법에 대한 컨퍼런스의 진행 사항: 시스템 데모. 2018. [17] Ke Li, Jay Mahadeokar, Jinxi Guo, Yangyang Shi, Gil Keren, Ozlem Kalinli, Michael L. Seltzer, Duc Le. &quot;스트리밍 심의를 통한 고속-저속 인코더 기반 변환기 개선&quot;. 2023년 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP)에서. [18] Xiang Lisa Li와 Percy Liang. &quot;접두사 튜닝: 생성을 위한 연속 프롬프트 최적화&quot;. 59회 계산 언어학 협회 연례 회의 및 11회 자연어 처리 국제 공동 컨퍼런스(제1권: 긴 서류). 2021. [19] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve 및 Ronan Collobert. &quot;MLS: 음성 연구를 위한 대규모 다국어 데이터 세트&quot;. 에서: Interspeech. 2020. [20] Sylvestre-Alvise Rebuffi, 하칸 빌렌, 안드레아 베달디. &quot;잔여 어댑터를 사용하여 여러 시각적 영역 학습&quot;. In: 신경 정보 처리 시스템의 발전. Vol. 30. 2017. [21] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé 등. “Bloom: 176b-매개변수 오픈 액세스 다국어 언어 모델”. arXiv 사전 인쇄본 arXiv:2211.05100(2022).[22] Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli. “wav2vec: 음성 인식을 위한 비지도 사전 학습”. Interspeech. 2019. [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: 개방적이고 효율적인 기반 언어 모델”. arXiv 사전 인쇄본 arXiv:2302.13971(2023). [24] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li 및 Mohamed Elhoseiny. &quot;Minigpt-4: 고급 대규모 언어 모델을 통한 시각 언어 이해 향상&quot;. arXiv 사전 인쇄본 arXiv:2304.(2023).
