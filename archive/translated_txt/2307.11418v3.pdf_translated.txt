--- ABSTRACT ---
최근 Neural Radiance Fields(NeRF)의 발전으로 고충실도 3D 얼굴 재구성과 새로운 뷰 합성이 가능해지면서, 이를 조작하는 것도 3D 비전에서 필수적인 작업이 되었습니다. 그러나 기존 조작 방법은 사용자가 제공하는 의미 마스크와 비전문가 사용자에게는 적합하지 않은 수동 속성 검색과 같은 광범위한 인적 노동이 필요합니다. 대신, 저희의 접근 방식은 NeRF로 재구성된 얼굴을 조작하는 데 단일 텍스트가 필요하도록 설계되었습니다. 이를 위해 먼저 동적 장면에서 장면 조작기인 잠재 코드 조건부 변형 가능 NeRF를 학습시켜 잠재 코드를 사용하여 얼굴 변형을 제어합니다. 그러나 단일 잠재 코드로 장면 변형을 표현하는 것은 다른 인스턴스에서 관찰된 로컬 변형을 합성하는 데 불리합니다. 따라서 제안된 Positionconditional Anchor Compositor(PAC)는 공간적으로 다양한 잠재 코드로 조작된 장면을 표현하는 방법을 학습합니다. 그런 다음 장면 조작기를 사용한 렌더링을 최적화하여 텍스트 기반 조작을 위한 CLIP 임베딩 공간에서 대상 텍스트와 높은 코사인 유사도를 얻습니다. 저희가 아는 한, 저희 접근 방식은 NeRF로 재구성된 얼굴의 텍스트 기반 조작을 다루는 최초의 방식입니다. 광범위한 결과, 비교 및 절제 연구는 저희 접근 방식의 효과를 보여줍니다. 1.
--- INTRODUCTION ---
3D 얼굴 표현의 쉬운 조작은 3D 디지털 인간 콘텐츠의 발전에 필수적인 측면입니다[32]. Neural Radiance Field[20](NeRF)가 3D 장면 재구성에서 큰 진전을 이루었지만, 많은 조작 방법은 세부적인 얼굴 표정 편집 작업에 적합하지 않은 색상[4, 34] 또는 강체 기하학[45, 15, 41, 14] 조작을 목표로 합니다. 최근 연구에서는 지역적으로 제어 가능한 얼굴 편집 방법[13]을 제안했지만, 큐레이팅된 학습 프레임에서 얼굴 부분의 사용자 주석 마스크를 수집한 다음 수동 속성 제어를 통해 원하는 조작을 달성하는 철저한 프로세스가 필요합니다. 얼굴별 암묵적 표현 방법[6, 47]은 변형 가능한 얼굴 모델[36]의 매개변수를 사전 확률로 활용하여 관찰된 얼굴 표정을 높은 충실도로 인코딩합니다. 그러나 이러한 조작은 수동으로 수행될 뿐만 아니라 다양한 얼굴 표정을 포함하는 약 6000개 프레임의 광범위한 학습 세트가 필요하여 데이터 수집 및 조작 단계 모두에서 노력이 많이 듭니다. 반대로, 우리의 접근 방식은 NeRF에서 얼굴 조작을 수행하기 위해 단일 텍스트만 사용하고, 그림 1a와 같이 몇 가지 유형의 얼굴 변형 예를 포함하는 약 300개의 훈련 프레임이 있는 동적 인물 비디오를 훈련합니다.얼굴 변형을 제어하기 위해, 우리의 방법은 먼저 HyperNeRF[23]를 활용하여 표준 공간에서 관찰된 변형을 학습하고 분리합니다.특히, 프레임당 변형 잠재 코드와 공유 잠재 코드 조건부 암묵적 장면 네트워크가 훈련 프레임을 통해 훈련됩니다.우리의 핵심 통찰력은 조작 작업을 위해 여러 개의 공간적으로 변하는 잠재 코드로 장면의 변형을 표현하는 것입니다.이 통찰력은 원하는 얼굴 변형을 나타내는 단일 잠재 코드를 검색하는 HyperNeRF의 공식을 조작 작업에 순진하게 채택하는 단점에서 비롯됩니다.예를 들어, 여러 인스턴스에서 관찰된 로컬 변형의 조합이 필요한 얼굴 표정은 단일 잠재 코드로 표현할 수 없습니다. 이 작업에서 우리는 이러한 문제를 &quot;연결된 로컬 속성 문제&quot;로 정의하고 공간적으로 변하는 잠재 코드가 있는 조작된 장면을 표현하여 이 문제를 해결합니다. 결과적으로, 우리의 조작은 그림 2a에서 빨간색 경계로 강조된 이미지 렌더링에서 볼 수 있는 로컬하게 관찰된 변형의 조합을 표현할 수 있습니다. 이를 위해 우리는 먼저 모든 관찰된 변형을 앵커 코드 세트로 요약하고 MLP가 앵커 코드를 구성하여 여러 위치 조건 잠재 코드를 생성하도록 학습하게 합니다. 대상 텍스트의 시각적 속성에 대한 잠재 코드의 반사도는 CLIP[27] 임베딩 공간에서 대상 텍스트에 가깝도록 잠재 코드의 렌더링된 이미지를 최적화하여 달성됩니다. 요약하면, 우리의 작업은 다음과 같은 기여를 합니다. • • NeRF로 재구성된 얼굴의 텍스트 기반 조작 파이프라인 제안. 공간적으로 변하는 잠재 코드가 있는 장면을 표현하도록 학습하는 조작 네트워크 설계. • 우리가 아는 한 NeRF로 재구성된 얼굴의 텍스트 기반 조작을 수행한 최초의 사례. w(초공간) F(X1, W12) (b) 보간 슬라이싱 볼륨 학습된 코드의 슬라이싱 표면 보간 슬라이싱 표면 비보간 슬라이싱 표면 F(X2, W21) H(x&#39;, w₁) H(x&#39;, W₂) F(X1, W11) X1(입) F(X2, W22) X(정준 공간) X2(눈) Wij = H(xi, w;) (a) 그림 2: (a) 초공간의 연결된 로컬 속성 문제의 그림. 장면 변형을 장면 잠복 코드로 표현하면 다른 인스턴스에서 관찰된 로컬 얼굴 변형을 구성할 수 없습니다. (b) 장면 조작기 훈련 중 관찰된 얼굴 변형의 유형. (c) 장면 조작기를 사용하여 보간된 잠복 코드의 렌더링. 2.
--- RELATED WORK ---
s NeRF 및 변형형 NeRF 대상 장면의 다양한 관점에서 촬영한 여러 이미지가 주어지면 NeRF[20]는 암묵적 체적 장면 함수와 체적 렌더링 방식[12]을 사용하여 높은 충실도를 가진 사실적이고 새로운 관점 이미지를 합성합니다. 이는 많은 후속 작업[1, 35, 19, 37, 44]의 영감을 제공했습니다. NeRF는 정적 장면을 가정하므로 최근 작업[22, 23, 26, 16]에서는 다음을 제안합니다.
--- METHOD ---
s는 사용자 제공 의미 마스크 및 비전문가 사용자에게 적합하지 않은 수동 속성 검색과 같은 광범위한 인적 노동이 필요합니다. 대신, 우리의 접근 방식은 NeRF로 재구성된 얼굴을 조작하는 데 단일 텍스트가 필요하도록 설계되었습니다. 이를 위해 먼저 동적 장면에서 장면 조작기, 잠재 코드 조건부 변형 가능 NeRF를 학습하여 잠재 코드를 사용하여 얼굴 변형을 제어합니다. 그러나 단일 잠재 코드로 장면 변형을 표현하는 것은 다른 인스턴스에서 관찰된 로컬 변형을 합성하는 데 불리합니다. 따라서 제안된 Positionconditional Anchor Compositor(PAC)는 공간적으로 다양한 잠재 코드로 조작된 장면을 표현하는 방법을 학습합니다. 그런 다음 장면 조작기를 사용한 렌더링을 최적화하여 텍스트 기반 조작을 위한 CLIP 임베딩 공간에서 대상 텍스트와 높은 코사인 유사성을 얻습니다. 우리가 아는 한, 우리의 접근 방식은 NeRF로 재구성된 얼굴의 텍스트 기반 조작을 다루는 최초의 방식입니다. 광범위한 결과, 비교 및 절제 연구는 우리 접근 방식의 효과를 입증합니다.1. 서론 3D 얼굴 표현의 쉬운 조작은 3D 디지털 인간 콘텐츠의 발전에 필수적인 측면입니다[32]. Neural Radiance Field[20](NeRF)가 3D 장면 재구성에서 큰 진전을 이루었지만, 많은 조작 방법은 세부적인 얼굴 표정 편집 작업에 적합하지 않은 색상[4, 34] 또는 강체 기하학[45, 15, 41, 14] 조작을 목표로 합니다.최근 연구에서는 지역적으로 제어 가능한 얼굴 편집 방법[13]을 제안했지만, 큐레이팅된 훈련 프레임에서 얼굴 부분의 사용자 주석 마스크를 수집한 다음 수동 속성 제어를 통해 원하는 조작을 달성하는 철저한 프로세스가 필요합니다.얼굴별 암묵적 표현 방법[6, 47]은 변형 가능한 얼굴 모델[36]의 매개변수를 사전 확률로 활용하여 관찰된 얼굴 표정을 높은 충실도로 인코딩합니다. 그러나 이러한 조작은 수동으로만 수행되는 것이 아니라 다양한 얼굴 표정을 포함하는 약 6000개 프레임의 광범위한 학습 세트가 필요하며, 이는 데이터 수집 및 조작 단계 모두에서 노력이 많이 듭니다.반대로, 우리의 접근 방식은 NeRF에서 얼굴 조작을 수행하기 위해 단일 텍스트만 사용하고, 그림 1a와 같이 몇 가지 유형의 얼굴 변형 예를 포함하는 약 300개의 학습 프레임이 있는 동적 초상화 비디오에서 학습합니다.얼굴 변형을 제어하기 위해, 우리의 방법은 먼저 HyperNeRF[23]를 활용하여 관찰된 변형을 표준 공간에서 학습하고 분리합니다.특히, 프레임당 변형 잠재 코드와 공유 잠재 코드 조건부 암묵적 장면 네트워크가 학습 프레임에서 학습됩니다.우리의 핵심 통찰력은 조작 작업을 위해 여러 개의 공간적으로 변하는 잠재 코드가 있는 장면의 변형을 표현하는 것입니다.이 통찰력은 HyperNeRF의 공식을 조작 작업에 순진하게 채택하는 단점에서 비롯되며, 원하는 얼굴 변형을 나타내는 단일 잠재 코드를 검색하는 것입니다. 예를 들어, 여러 인스턴스에서 관찰된 로컬 변형의 조합이 필요한 얼굴 표정은 단일 잠재 코드로 표현할 수 없습니다. 이 작업에서 우리는 이러한 문제를 &quot;연결된 로컬 속성 문제&quot;로 정의하고 공간적으로 다양한 잠재 코드가 있는 조작된 장면을 표현하여 이 문제를 해결합니다. 결과적으로, 우리의 조작은 그림 2a에서 빨간색 경계로 강조된 이미지 렌더링에서 볼 수 있듯이 로컬로 관찰된 변형의 조합을 표현할 수 있습니다. 이를 위해 먼저 모든 관찰된 변형을 앵커 코드 세트로 요약하고 MLP가 앵커 코드를 구성하여 여러 위치 조건 잠재 코드를 생성하도록 학습시킵니다. 그런 다음 대상 텍스트의 시각적 속성에 대한 잠재 코드의 반사도는 CLIP[27] 임베딩 공간에서 대상 텍스트에 가깝도록 잠재 코드의 렌더링된 이미지를 최적화하여 달성됩니다. 요약하면, 우리의 작업은 다음과 같은 기여를 합니다. • • NeRF로 재구성된 얼굴의 텍스트 기반 조작 파이프라인 제안. 공간적으로 변하는 잠재 코드가 있는 장면을 표현하는 법을 배우는 조작 네트워크의 설계. • NeRF로 재구성된 얼굴에 대한 텍스트 기반 조작을 우리가 아는 한 최초로 수행. w (초공간) F(X1, W12) (b) 보간 슬라이싱 볼륨 학습된 코드의 슬라이싱 표면 보간 슬라이싱 표면 비보간 슬라이싱 표면 F(X2, W21) H(x&#39;, w₁) H(x&#39;, W₂) F(X1, W11) X1 (입) F(X2, W22) X (정준 공간) X2 (눈) Wij = H(xi, w;) (a) 그림 2: (a) 초공간에서 연결된 로컬 속성 문제의 예. 장면 변형을 장면별 잠재 코드로 표현하면 다른 인스턴스에서 관찰된 로컬 얼굴 변형을 구성할 수 없습니다. (b) 장면 조작기 훈련 중에 관찰된 얼굴 변형의 유형. (c) 장면 조작기를 사용한 보간된 잠재 코드의 렌더링.2. 관련 연구 NeRF 및 변형 가능 NeRF 대상 장면의 다른 관점에서 촬영한 여러 이미지가 주어지면 NeRF[20]는 암묵적 체적 장면 함수와 체적 렌더링 방식[12]을 사용하여 높은 충실도의 사실적인 새로운 관점 이미지를 합성합니다.이는 많은 후속 연구[1, 35, 19, 37, 44]에 영감을 주었습니다.NeRF는 정적 장면을 가정하므로 최근 연구[22, 23, 26, 16]는 관심 있는 동적 장면을 인코딩하는 방법을 제안합니다.이 연구의 공통적인 방식은 훈련 프레임당 잠재 코드를 훈련하고 모든 훈련된 잠재 코드가 공유하는 단일 잠재 조건부 NeRF 모델을 사용하여 장면 변형을 처리하는 것입니다.본 연구는 이러한 설계 선택을 기반으로 관찰된 변형을 표준 공간에서 학습하고 분리하지만 조작 단계에서 공간적으로 다양한 잠재 코드로 조작된 장면을 표현하여 한계를 극복합니다. 텍스트 기반 3D 생성 및 조작 많은 연구에서 이미지나 3D 조작에 텍스트를 사용했습니다[38, 9, 25, 11, 29, 10]. CLIP-NeRF[38]는 CLIP[27] 공간에서 텍스트 임베딩으로 감독되는 생성 공식에서 얽힘이 풀린 조건부 NeRF 아키텍처를 제안하고, 객체의 모양과 외관에 대한 텍스트 및 예시 기반 편집을 수행했습니다. Dreamfields[9]는 CLIP 임베딩 공간에서 생성 텍스트를 생성 텍스트로 감독하여 생성 텍스트-3D 합성을 수행했습니다. 우리는 이러한 연구 라인을 확장하여 NeRF로 재구성된 얼굴의 CLIP 기반 조작을 시작합니다. NeRF 조작 NeRF 조작을 연구한 많은 연구[18, 45, 36, 13, 34, 33, 7, 48, 15] 중에서 EditNeRF[18]는 명시적인 감독 없이도 모양 부분의 암묵적 의미를 학습하기 위해 모양 범주에 대한 조건부 NeRF를 학습합니다. 그런 다음, 조작 프로세스는 사용자가 제공한 낙서를 편집을 위해 적절한 개체 영역으로 전파합니다. NeRF-Editing[45]은 학습된 NERF에서 메시를 추출하고 사용자가 메시 변형을 수행하도록 합니다. 편집된 장면의 새로운 뷰는 해당 광선을 구부려 네트워크를 다시 학습하지 않고도 합성할 수 있습니다. CoNeRF[13]는 사용자가 제공한 얼굴 영역의 마스크 주석을 사용하여 제어 가능한 신경 광도장을 학습하므로 사용자는 영역 내에서 원하는 속성을 제어할 수 있습니다. 그러나 이러한 방법은 힘든 주석과 수동 편집 프로세스가 필요한 반면, 우리의 방법은 얼굴을 자세히 조작하기 위해 단일 텍스트만 필요합니다. 신경 얼굴 모델 여러 연구[42, 28, 47]에서 신경 암묵적 모양 표현을 사용하여 3D 얼굴 모델을 구축했습니다. 연구 중 i3DMM[42]은 얼굴 정체성, 헤어스타일, 표정을 풀어서 분리된 구성 요소를 수동으로 편집할 수 있도록 합니다. NeRF 기반 얼굴 표현 작업도 활용되었습니다[39, 36, 47]. Wang et al.[39]은 인간 얼굴을 사진처럼 사실적으로 렌더링하기 위한 구성적 3D 표현을 제안했지만, 얼굴 표정 조작을 위한 암묵적으로 제어 가능한 코드를 추출하기 위해 안내 이미지가 필요합니다. NerFACE[36]와 IMavatar[47]는 학습된 3D Morphable Model[2] 매개변수를 사전 확률로 사용하여 포즈와 표정에 대한 제어 가능성을 달성하기 위해 인간 얼굴의 모양과 역학을 모델링합니다. 그러나 이 방법은 많은 얼굴 표정 예를 포함하는 많은 수의 훈련 프레임과 조작 작업을 위한 사전 확률의 수동 조정이 필요합니다. 3. 예비 단계 3.1. NeRF NERF[20]는 MLP를 사용한 공간의 기하학과 색상의 암묵적 표현입니다. 구체적으로, 점 좌표 x = (x, y, z)와 시야 방향 d가 주어지면 MLP 함수 F는 (c, σ) = F(x, d)로 점의 밀도와 색상을 산출하도록 학습됩니다. M개의 점이 계층적 샘플링 방법에서 수집된 거리 {t;} 0을 사용하여 M a ray r = 0 + td를 따라 샘플링됩니다. F는 각 점의 색상과 밀도를 예측하고, 이 모든 것이 렌더링되어 원래 광선의 픽셀 색상을 예측합니다. = M Ĉ(r) = (1-exp(−σidi))ci, i== (1) 여기서 Si ti+1 - ti이고 Ti exp(-;;)는 누적 투과율입니다. 그런 다음 F는 해당 알려진 픽셀 색상으로 감독하여 렌더링 손실을 최소화하도록 학습됩니다. 3.2. HyperNeRF 정적 장면을 위해 설계된 NeRF와 달리 HyperNeRF[23]는 큰 위상적 변화가 있는 매우 동적인 장면을 인코딩할 수 있습니다.그 핵심 아이디어는 해석을 위해 점을 표준 초공간에 투사하는 것입니다.특히 잠재 코드 w가 주어지면 공간 변형 필드 T는 점을 표준 공간에 매핑하고 슬라이싱 표면 필드 H는 템플릿 NeRF F에 대한 점의 해석을 결정합니다.특히, x&#39; = T(x, w), w = H(x, w), (c, σ) = F(x&#39;, w, d), = (2) (3) (4) 여기서 w wn = {w₁WN} W는 각 N개의 학습 프레임에 해당하는 학습 가능한 프레임당 잠재 코드입니다.그런 다음 렌더링 손실은 마지막으로 L₁ = Σ ||로 정의됩니다. Cn(r&quot;) – În(r&quot;)||¾2, nЄ{1...N}, rn ERn(5) 여기서 Cn(r&quot;)은 광선 rn의 n번째 학습 프레임에서의 기준 진실 색상이고 Rn은 n번째 카메라의 광선 집합입니다. (x&#39;, w)와 H(x, w)는 종종 표준 초공간과 슬라이싱 표면이라고 합니다. 그림 2a에서 볼 수 있듯이 x&#39;는 w에 따라 다르게 해석될 수 있기 때문입니다. 4. 제안 방법 우리는 NeRF로 재구성된 얼굴을 조작하고자 하는데, 조작을 위해 원하는 얼굴 표정(예: &quot;울음 얼굴&quot;, &quot;윙크하는 눈과 웃는 입&quot;)을 나타내는 타겟 텍스트가 주어졌을 때 그렇게 한다. 이를 위해 제안하는 방법은 먼저 얼굴 변형을 제어하는 잠재 코드 조건부 신경장인 장면 조작기를 훈련한다(§4.1). 그런 다음, 타겟 텍스트를 조작하기 위한 파이프라인을 자세히 설명한다(§4.2). 그런 다음, 학습된 변형과 장면 조작기를 적절히 사용하여 타겟 텍스트의 속성을 반영하는 얼굴이 있는 장면을 렌더링하는 MLP 네트워크를 제안한다(§4.3). 조작 중 훈련 및 고정 WR W {7} 변형장 → d → F [7]→w 주변 슬라이싱 표면 → (a) 장면 조작기 템플릿 NeRF 조작 훈련 Llip XX LACRG¬ W₁A ·α [x,1]&#39;* WA * GP σ WA Lc LCLIP [x,K]• W&amp; W 앵커 코드 앵커 합성 네트워크 G (b) 바닐라 반전 (c) 위치 조건 앵커 합성기(PAC)LCLIP 그림 3: (a) 장면 조작기 G의 네트워크 구조. (b) 조작을 위한 바닐라 반전 방법. (c) 조작을 위한 위치 조건 앵커 합성기(PAC). 4.1. 장면 조작기 먼저, HyperNeRF[23]를 사용하여 장면 조작기를 구성하여 장면 조작기의 매개변수를 고정하고 잠재 코드를 조작하여 장면의 변형을 제어할 수 있습니다. 구체적으로, [23]에 따라 Eq.(4)로 공식화된 네트워크로 관심 있는 동적 장면을 학습한 후, 학습된 T, H, F 및 W 매개변수를 동결하고 조작 핸들을 사용합니다. 또한, 우리는 경험적으로 변형 네트워크 T가 머리 자세와 같은 강체 변형을 학습하는 경향이 있는 반면, 슬라이싱 표면 필드 H는 입과 눈의 모양과 같은 비강체적이고 세부적인 변형을 학습하는 경향이 있음을 발견했습니다. 따라서 우리는 T에 대한 훈련된 잠재 코드를 선택하여 고정하고 H에 공급된 잠재 코드만 조작합니다. 요약하면, 그림 3(a)에 도시된 바와 같이, 우리의 잠재 코드 조건부 장면 조작기 G는 G(x, d, w) := F(T(x, wr), Ħ(x, w), d), (6)으로 정의됩니다. 여기서 는 매개변수가 조작을 위해 훈련되고 고정되었음을 나타내고, WR은 학습된 잠재 코드 W 세트에서 선택된 원하는 머리 자세의 고정된 잠재 코드입니다. 보충 자료에서 우리는 더 자세히 보고합니다.
--- EXPERIMENT ---
WR의 머리 자세 제어 가능성에 대한 모든 결과 및 논의. Lipschitz MLP G는 제한된 집합의 학습 가능한 잠재 코드 W에 대해서만 조건화되도록 학습되므로, 조작을 위해 G의 표현 가능성을 최대화하기 위해 학습된 잠재 코드 외부의 그럴듯한 변형을 생성하는 w의 부분 공간을 공식화해야 합니다. 한편, HyperNeRF는 두 개의 학습된 잠재 코드에서 선형 보간된 잠재 코드의 이미지를 적당히 렌더링하는 것으로 나타났습니다. 따라서 유효한 잠재 부분 공간 W는 학습된 잠재 코드뿐만 아니라 두 개의 학습된 잠재 코드 사이에 선형 보간된 코드도 포함하도록 공식화할 수 있습니다. 구체적으로, (7) W &gt; {y * ŵi + (1 − y) * ŵj | Ŵi, Ŵj Є W, 0≤≤ 1}. 그러나 보간된 잠재 코드의 이미지의 충실도는 조작에 활용되기 위해 더 높아야 한다는 것을 알게 되었습니다. 따라서 우리는 훈련 단계 동안 장면 조작기의 MLP를 Lipschitz 연속이 되도록 정규화합니다. L개의 층과 ReLU와 같은 조각 선형 함수를 갖는 신경망의 Lipschitz 경계는 c = II П ±1 ||W|| [17, 43]로 근사될 수 있습니다. 여기서 W²는 i번째 층의 MLP 가중치입니다. c-Lipschitz인 함수 f는 속성 (8) ||ƒ (w1) – ƒ (w2)||p ≤ C||W1 – W2||p를 가지므로 c의 성공적인 정규화는 인접한 잠재 코드의 출력 간 차이를 줄여 보간된 변형이 시각적으로 더 자연스럽게 보이도록 합니다. 따라서 [17]을 따르고 y&#39; = o(Ŵ&#39;x+b²), Ŵ&#39;; = W¹;로 추가 학습 가능 매개변수 c²를 도입하여 F의 l번째 층에서 학습 가능한 행렬을 정규화합니다. · min(1, softplus(c)), (9) ||W||∞ 여기서 W는 l번째 층 W에서 학습 가능한 행렬의 j번째 행이고, || ||∞는 행렬 ∞-norm입니다. 그런 다음 층의 Trinable Lipschitz 상수는 손실 함수가 L Llip softplus (c&#39;)로 정의된 기울기 기반 최적화를 통해 최소화됩니다. 1=(10) 요약하면, Eq. (4)의 네트워크는 장면 조작기 목적 함수 LSM = c Lc + Alip Llip을 사용하여 F, T, H 및 W를 검색하도록 학습됩니다. 여기서 및 lip은 하이퍼 매개변수입니다. 4.2. 텍스트 기반 조작 (11) 학습된 장면 조작기 G가 주어지면, 한 가지 조작 방법은 G를 사용하여 렌더링된 이미지가 CLIP[27] 임베딩 공간에서 대상 텍스트와 가장 높은 코사인 유사도를 생성하는 단일 최적 잠복 코드 w를 찾는 것입니다. 이렇게 하면 조작된 이미지가 대상 텍스트의 시각적 속성을 반영할 수 있습니다. 구체적으로, 유효한 카메라 포즈 집합 [R_t]에서 G와 w로 렌더링된 이미지가 IG이고 조작을 위한 대상 텍스트 p인 경우, 이 방법의 목표는 다음 문제를 해결하는 것입니다. w* = arg max DCLIP (IP), พ (12) 여기서 DCLIP는 렌더링된 이미지와 사전 학습된 CLIP 모델에서 추출한 대상 텍스트 간의 피처의 코사인 유사도를 측정합니다. 그림 3b에서 볼 수 있듯이 최적의 잠재 임베딩 w*를 찾기 위한 간단한 바닐라 접근 방식은 w의 그래디언트 기반 최적화인 역전으로, 손실 함수를 LCLIP 1-DCLIP (RP)로 정의하여 Eq. (12) = -G,w를 최대화합니다. 그러나 이 방법은 필연적으로 연결된 로컬 속성 문제로 인해 어려움을 겪고 제안하는 방법으로 이를 해결한다는 것을 보여줌으로써 최적이 아님을 보여줍니다. 연결된 로컬 속성 문제 바닐라 역전 방법의 솔루션은 W의 솔루션과 동일한 변형을 나타내는 데 국한됩니다. 그러나 W는 본질적으로 W를 구성하는 두 개의 학습된 잠재 코드 간의 보간으로 인해 서로 다른 위치의 얼굴 속성이 동시에 변경되므로 로컬로 관찰된 변형의 모든 가능한 조합을 나타낼 수 없습니다. 예를 들어, 그림 2b의 변형이 있는 장면과 그림 2c의 두 개의 학습된 잠재 코드 간의 보간 렌더링을 고려합니다. 놀랍지 않게도 학습된 잠재 코드나 보간된 코드는 입을 벌린 채로 눈을 뜨거나 입을 감은 채로 눈을 감는 것을 표현할 수 없습니다. 학습된 잠재 코드와 보간의 모든 쌍에 대해 유사한 실험을 수행하여 동일한 결과를 얻을 수 있습니다.
--- CONCLUSION ---
. 우리는 3.2절에서 소개된 표준 초공간의 슬라이싱 표면 관점에서 이 문제에 접근할 수 있다. 그림 2a에서와 같이, 초공간은 모든 공간 위치의 전역 변형을 나타내는 슬라이싱 표면의 인스턴스를 나타내는 잠재 코드를 하나만 허용한다. 이러한 표현은 보간 중에 한 위치에서 한 유형의 변형이 다른 위치에서 다른 유형의 변형에 동일한 정도로 변경되도록 한다. 우리의 방법은 관찰에 의해 동기를 부여받았으며 따라서 연결된 로컬 속성 문제를 해결하기 위해 다른 위치 x가 다른 잠재 코드로 표현될 수 있도록 설계되었다. 4.3. 위치 조건부 앵커 합성기 이와 관련하여, 위치 조건부 앵커 합성기(PAC)는 조작 파이프라인이 다른 공간 위치에 적합한 잠재 코드를 학습할 수 있는 자유를 부여하기 위해 제안된다. 구체적으로, 우리는 앵커 코드 {wƒ‚……. w^{} WA CW, 학습된 잠재 코드의 하위 집합으로, 각 repաշ WA a[x,2] W* + a[x,1] A a[x,3] W보간, 유효 영역 외삽, 보이지 않는 영역 그림 4: K = 3일 때 유효하게 표현력이 있는 영역에 대한 잠재 코드의 중심 보간의 예. 유효하게 탐색 가능한 잠재 공간을 사전으로 설정하기 위해 관찰된 얼굴 변형의 다른 유형을 전송합니다. 고정된 카메라 포즈에서 W의 모든 코드에서 렌더링된 이미지에서 DECA[5]를 사용하여 얼굴 표정 매개변수를 추출하여 앵커 코드를 검색합니다. 그런 다음 DBSCAN[3]을 사용하여 추출된 표정 매개변수를 클러스터링하고 각 클러스터의 평균에 가장 가까운 표정 매개변수에 해당하는 잠재 코드를 선택합니다. 예를 들어, 그림 1a 및 그림 2b의 예제 장면의 경우 K = 앵커 코드를 얻을 수 있습니다. 그런 다음 모든 공간적 위치에 대해 위치 조건부 MLP는 이러한 앵커 코드를 구성하는 방법을 학습하여 적절한 잠재 코드를 생성합니다. 그렇게 함으로써 조작된 장면을 여러 개의 점별 잠재 코드로 암묵적으로 표현할 수 있습니다. 구체적으로 앵커 구성 네트워크 PR(3+dw) → R¹은 앵커의 중심 보간[8]을 통해 모든 공간 위치 x에 대해 w*를 생성하는 방법을 학습합니다. â[x‚k] = P(x@w^), w² = Σ σk (â[x, k)]) W^^, (13) k 여기서 dw는 잠재 코드의 차원이고, 는 연결이며, σk는 k 네트워크 출력을 따른 소프트맥스 활성화입니다. 또한 표기의 편의를 위해 a[x,k] = σ σk (â[x,k])를 앵커 구성 비율(ACR)로 표시합니다. 그림 4의 설명적 예에서와 같이 설계의 핵심은 합성된 코드가 잠재의 외삽 영역으로 발산하는 것을 방지하는 것입니다. 따라서 중심 보간은 시각적으로 자연스러운 렌더링을 위한 합성된 잠재 코드의 안전한 경계를 정의합니다. 마지막으로 유효한 카메라 포즈에서 투사된 광선에서 샘플링된 점 집합과 해당 잠재 코드 집합 [w]은 G에 의해 쿼리되고, 그 출력은 LCLIP = DCLIP (G ,p)로 조작하기 위해 CLIP 임베딩 공간에서 감독할 이미지로 렌더링됩니다. (14) 앵커 구성 비율에 대한 총 변동 손실 PAC의 점별 표현력으로 인해 상호 제약 없이 인접한 잠재 코드가 달라질 수 있고 P는 총 변동(TV) 손실로 정규화됩니다. 더 부드러운 ACR 필드는 유사한 잠재 임베딩을 통해 특정 얼굴 위치를 커버할 수 있습니다.참조 &quot;감긴 눈과 웃는 입&quot; &quot;감고, 찡그린 눈과 웃는 입&quot; &quot;윙크 눈과 웃는 입&quot; &quot;뜬 눈과 웃는 입&quot; &quot;찡그린 눈과 삐죽삐죽한 입술&quot; &quot;감긴 눈과 삐죽삐죽한 입술&quot; &quot;윙크 눈과 크게 벌린 입&quot; &quot;닫힌 눈과 벌린 입&quot; &quot;깜짝 놀란 얼굴&quot; &quot;뜬 눈과 크게 벌린 입&quot; &quot;찡그린 눈과 닫힌 입&quot; &quot;닫힌, 찡그린 눈과 &quot;윙크, 찡그린 눈과 닫힌 입&#39; 닫힌 입&quot; &quot;행복한 얼굴&quot; 3D 그림 5: 우리 방법을 사용하여 설명적 텍스트로 조작한 정성적 결과. 로컬 얼굴 변형은 텍스트만 사용하여 쉽게 제어할 수 있습니다. 더 자연스럽게 렌더링된 이미지를 생성하기 위해 상황을 변경합니다. 구체적으로, a[x,k]는 정규화를 위해 Eq. (1)의 렌더링 방정식을 사용하여 유효한 카메라 평면에 렌더링됩니다. 레이 ruv(t) = 0 tduv가 주어지면 ACR은 카메라 평면의 (u, v)에 위치한 이미지 픽셀에서 각 앵커 k에 대해 렌더링될 수 있으며, TV 손실로 akuv M Σ Ti(1 – exp(-σidi))¤[ruv(ti),k], i=(15) LACR = Σ ||ãk(u+1)v − ˜kuv||2 + ||ãku(v+1) — Õkuv||2. k,u,v (16) 요약하면, 텍스트 기반 조작은 P를 최적화하고 다음 손실을 최소화함으로써 수행됩니다. Ledit=CLIPLCLIP + ACRLACR 여기서 ACLIP 및 AACR은 하이퍼 매개변수입니다. 5. 실험 (17) 데이터 세트 Apple iPhone 13을 사용하여 6명의 자원봉사자로부터 인물 사진을 수집했으며, 각 자원봉사자에게 그림 1a 및 그림 2b에 표시된 4가지 유형의 얼굴 변형을 하도록 요청했습니다. 사전 훈련된 인간 분할 네트워크는 COLMAP[31]을 사용하여 카메라 포즈 계산 중에 장면의 동적 부분에서 설명자를 제외하는 데 사용되었습니다.관찰된 얼굴 변형의 예 참조 NeRF+FT Nerfies +1 HyperNeRF +1 우리의 그림 6: 우리 방법과 기준선의 텍스트 기반 조작 결과.우리의 결과는 시각적 품질과 얼굴 정체성을 보존하면서 대상 감정 텍스트의 암묵적 속성을 잘 반영합니다.각 장면에 대한 훈련 중에 발생한 변형은 보충 자료에 보고되었습니다.조작 텍스트 우리는 조작 실험을 위해 두 가지 유형의 텍스트를 선택했습니다.첫 번째는 각 얼굴 부분의 변형을 특징짓는 설명적 텍스트입니다.두 번째는 감정 표현 텍스트로, 설명적 텍스트로 설명하기 어려운 모든 얼굴 부분의 여러 로컬 변형 집합을 암묵적으로 표현한 것입니다.우리는 실험을 위해 자주 사용되고 구별되는 7가지 감정 표현 텍스트를 선택했습니다: &quot;울음&quot;, &quot;실망&quot;, &quot;놀람&quot;, &quot;행복&quot;, &quot;화남&quot;, &quot;무서움&quot; 및 &quot;잠들기&quot;. 영어: 텍스트 임베딩 노이즈를 줄이기 위해 동일한 의미를 가진 문장의 증강 임베딩을 평균화하여 [24]를 따랐습니다.기준선 문제 정의와 평행한 사전 작업이 없기 때문에 비교를 위해 기존 최첨단 방법을 사용하여 3개의 기준선을 공식화했습니다.(1) NeRF+FT는 CLIP 손실을 사용하여 전체 네트워크를 미세 조정하는 NeRF[20]의 간단한 확장입니다.(2) Nerfies+1은 Nerfies[22]를 변형 네트워크로 사용한 다음 조작을 위해 §4.2절에 도입된 바닐라 역전 방법을 수행합니다.(3) HyperNeRF+1은 (2)의 Nerfies를 HyperNeRF[23]로 대체합니다.텍스트 기반 조작 그림 5의 설명적 문장 집합으로 구동되는 방법의 정성적 조작 결과를 보고합니다.우리의 방법은 설명을 충실하게 반영할 뿐만 아니라 문장에서 단어를 간단히 변경하여 국소적 얼굴 변형을 쉽게 제어할 수 있습니다. 우리는 또한 감정적 exReference &quot;화난&quot; &quot;울음&quot; &quot;놀람&quot; &quot;실망&quot; &quot;행복&quot; &quot;무서움&quot; &quot;잠자기&quot;에 의해 구동되는 조작된 결과를 보고합니다. 그림 7: 우리의 방법을 사용하여 자주 사용되는 감정 표현 텍스트 세트에 의해 구동된 광범위한 얼굴 조작 결과. 감정 표현 텍스트에 대한 조작은 설명하기 어려운 미묘한 얼굴 변형의 구성을 암묵적으로 요구하기 때문에 어렵습니다. 우리의 방법은 그림 7에서 조작 텍스트의 속성을 합리적으로 반영합니다. 볼 수 있듯이, 우리의 방법은 감정 텍스트가 많은 국소적 얼굴 변형의 암묵적 표현이더라도 성공적인 조작을 수행합니다. 예를 들어, 그림 7의 첫 번째 행에서 &quot;울음&quot;으로 조작한 결과는 단순히 울음처럼 보이는 눈과 입으로 표현되지 않고, 국소 변형에 대한 명시적 감독 없이 얼굴 전체에 울음처럼 보이는 눈썹과 피부가 포함됩니다. 또한 정성적 결과를 그림 6의 기준선의 결과와 비교합니다. 우리의 결과는 대상 텍스트 속성의 가장 높은 반사를 나타냅니다. NeRF+FT는 시각적 품질이 상당히 저하되는 반면, Nerfies+는 재구성 품질이 낮고 대상 텍스트 속성이 반사되는 문제가 약간 있습니다. HyperNeRF+ I는 모든 기준선 중에서 가장 높은 시각적 품질을 보이지만 대상 텍스트의 시각적 속성을 반영하지 못합니다. 다양한 조작 텍스트에서 높은 반사율은 연결된 로컬 속성 문제를 해결하는 PAC에 기인할 수 있습니다. 그림 8에서 각 앵커 코드 k에 대해 ãkuv를 시각화합니다. 이는 이미지 평면에서 Eq. (15)의 ACR a[x,k]를 렌더링한 것입니다. 렌더링의 흰색 영역은 1에 더 가까워서 해당 앵커 코드가 대부분 합성되었음을 나타냅니다. 영어: 영역의 잠재 코드를 생성합니다. 또한 각 앵커 코드의 로컬 속성을 이해하는 데 도움이 되도록 왼쪽에 각 앵커 코드의 이미지 렌더링을 표시합니다. 볼 수 있듯이 PAC는 다양한 위치에 적합한 앵커 코드를 구성합니다. 예를 들어, 잠자는 얼굴을 조작할 때 PAC는 감은 눈을 반사합니다 R-Prec.[40] ↑ LPIPS [46]↓ CFS NERF + FT Nerfies I HyperNeRF + I 0.0.0.0.0.0.0.0.0.Ours 0.780 (+0.017) 0.082 (-0.116) 0.749 (+0.028) 표 1: 정량적 결과. R-Prec.는 R-정밀도를 나타내고 CFS는 코사인 얼굴 유사도를 나타냅니다. 성능 순위를 최고 및 2위로 표시합니다. TR↑ VR ↑ FP↑ NERF + FT Nerfies + I HyperNeRF + I 2.0.0.0.3.4.2.4.4.Ours 4.67 (+0.28) 4.15 (+1.30) 4.58 (+0.16) 표 2: 사용자 연구 결과. TR, VR 및 FP는 각각 텍스트 반사도, 시각적 사실성 및 얼굴 정체성 보존성을 나타냅니다. 가장 좋은 것과 두 번째로 좋은 것이 강조 표시됩니다. 한 앵커 코드에서 중립적인 입과 다른 앵커 코드에서. 울음, 화남, 두려움 및 실망한 얼굴의 경우 PAC는 단일 잠재 코드로 표현할 수 없는 복잡한 학습된 변형 구성을 생성하는 방법을 학습합니다. 정량적 결과 우선, 우리는 텍스트 속성 반사도를 측정하기 위해 Rprecision[40]을 측정했습니다.앵커 코드에 의한 이미지 렌더링 조작 결과 &quot;행복&quot; &quot;화남&quot; &quot;놀람&quot; &quot;울음&quot; &quot;실망&quot; &quot;잠들다&quot; &quot;무서워&quot; Llip w/o Llip Start End에 의해 렌더링된 ACR 맵 그림 9: 선형 보간된 잠재 코드의 렌더링.립시츠 정규화된 장면 조작기는 보이지 않는 모양을 보다 자연스럽게 보간합니다.그림 8: 다른 조작 텍스트에 대한 각 앵커 코드에 대한 학습된 ACR 맵의 렌더링.조작.우리는 각 텍스트의 top-R 검색을 위해 AffectNet[21]으로 사전 학습된 얼굴 표정 인식 모델[30]을 사용했습니다.특히, 얼굴당 1000개의 새로운 뷰 이미지가 렌더링되며, 여기서 200개의 이미지는 구별 가능하고 존재하는 5개의 텍스트 각각으로 조작된 얼굴에서 렌더링됩니다. AffectNet 레이블: &quot;행복함&quot;, &quot;놀람&quot;, &quot;두려움&quot;, &quot;화남&quot;, &quot;슬픔&quot;. 또한 조작 후 시각적 품질을 추정하기 위해 조작이 없는 얼굴(중립 얼굴)과 각각 새로운 뷰에서 렌더링된 7개의 텍스트로 조작된 얼굴 간의 LPIPS[46]를 측정했습니다. 텍스트 기반 조작의 픽셀 단위 기준 진실이 없기 때문에 LPIPS가 시각적 품질에 대한 최상의 추정치라는 점에 유의하세요. 마지막으로 조작 후 얼굴 정체성이 얼마나 보존되는지 측정하기 위해 중립 얼굴과 텍스트 조작된 얼굴에서 추출한 얼굴 정체성 특징¹ 간의 코사인 유사도를 측정했습니다. 모두 200개의 새로운 뷰에서 렌더링되었습니다. 표 1은 모든 텍스트에 대한 평균 결과를 보고하며, 이는 우리 방법이 모든 기준에서 우수한 성과를 거두었음을 보여줍니다. 사용자 연구 사용자는 기준에 따라 0~5점을 받도록 요청받았습니다. (i) 텍스트 반사도: 조작된 렌더링이 대상 텍스트를 얼마나 잘 반영하는지, (ii) 시각적 사실성: 조작된 이미지가 얼마나 사실적으로 보이는지, (iii) 얼굴 정체성 보존성: 조작된 이미지가 우리 방법과 기준선에 비해 원래 얼굴의 정체성을 얼마나 잘 보존하는지. 다음 결과는 표 2에 보고되어 있습니다. 우리 방법은 모든 기준선, 특히 https://github.com/ronghuaiyang/arcface-pytorch W/O LACR w/o Llip W/O LACR보다 성능이 뛰어납니다. (a) 우리 방법 W/ LACR (b) 그림 10: (a) 절제 연구의 정성적 결과. 조작은 대상 텍스트로 &quot;울부짖는 얼굴&quot;을 사용하여 수행됩니다. (b) LACR이 있고 없는 렌더링된 ACR 맵. 텍스트 반사도에서 큰 차이로 특히 우수합니다. 사용자 응답에서의 우수한 성과가 정량적 결과의 우수한 성과와 일치하여 평가의 일관성을 뒷받침합니다. 보간 우리는 선형 보간된 잠재 코드에서 렌더링된 이미지의 시각적 품질을 비교하여 장면 조작기에 대한 Lipschitz 정규화의 효과를 실험하고 그림 9에 결과를 보고합니다.Lipschitz 정규화된 장면 조작기는 시각적으로 더 자연스러운 이미지를 생성하는데, 이는 학습된 앵커 합성 잠재 코드 [w*] 세트가 Lipschitz 정규화된 장면 조작기에서 사실적으로 보간된 국소 변형을 렌더링할 가능성이 더 높다는 것을 의미합니다.절제 연구 우리는 정규화 방법인 Llip과 LACR에 대한 절제 연구를 수행했습니다.그림 10a에서 볼 수 있듯이 Llip 없이 조작하면 시각적 품질이 낮습니다.LACR 없이 조작하면 입과 눈썹과 같이 변형 범위가 큰 얼굴 부분이 부자연스럽게 렌더링됩니다.이는 그림 10b에서 PAC의 학습된 ACR 맵으로 해석할 수 있습니다.LACR로 학습한 ACR 맵은 동적 얼굴 부분의 경계에 잠재 코드의 합리적인 연속성을 도입하여 자연스럽게 보간된 얼굴 부분을 생성합니다. 6. 결론 우리는 변형 가능한 NeRF를 사용하여 3D 얼굴의 텍스트 기반 조작 파이프라인인 FaceCLIPNeRF를 제시했습니다. 우리는 먼저 얼굴 변형의 제어 핸들로 잠재 코드를 사용하는 조건부 MLP인 Lipshitz-regularized scene manipulator를 제안했습니다. 우리는 다른 인스턴스에서 관찰된 변형을 구성할 수 없는 기존의 변형 가능한 NeRF의 연결된 로컬 속성 문제를 해결했습니다. 따라서 우리는 CLIP 임베딩 공간에서 대상 텍스트와 높은 코사인 유사도를 생성하도록 장면 조작기를 사용한 렌더링이 학습된 공간적으로 변하는 잠재 코드를 생성하는 방법을 학습하는 PAC를 제안했습니다. 우리의 실험은 우리의 방법이 3D 얼굴의 시각적 품질과 정체성을 보존하면서 설명적 텍스트와 감정적 텍스트의 시각적 속성을 충실하게 반영할 수 있음을 보여주었습니다. 감사의 말 이 자료는 공군 과학연구실의 지원 번호 FA2386-22-1-4024, KAIST-NAVER 하이퍼크리에이티브 AI 센터, 한국 정부(MSIT)가 지원하는 정보통신기술 기획평가원(IITP)의 연구비(No.20190-00075, 인공지능대학원(KAIST))의 지원을 바탕으로 작성되었습니다. 참고문헌 [1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan. Mip-nerf: 앤티앨리어싱 신경 광도 필드를 위한 다중 스케일 표현. IEEE/CVF 컴퓨터 비전 국제 학술 대회 논문집, 5855-5864쪽, 2021.[2] Volker Blanz and Thomas Vetter. 3D 얼굴 합성을 위한 변형 가능 모델. 26회 컴퓨터 그래픽 및 상호작용 기술 연례 컨퍼런스 회의록, 187-194페이지, 1999.[3] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 노이즈가 있는 대규모 공간 데이터베이스에서 클러스터를 발견하기 위한 밀도 기반 알고리즘. kdd, 96권, 226-231페이지, 1996.[4] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, Zhangyang Wang. 통합 암묵적 신경 양식화. arXiv 사전 인쇄본 arXiv:2204.01943, 2022.[5] Yao Feng, Haiwen Feng, Michael J. Black, Timo Bolkart. 야생 이미지에서 애니메이션 가능한 세부 3D 얼굴 모델 학습. 40권, 2021.[6] Guy Gafni, Justus Thies, Michael Zollhofer, Matthias Nießner. 단안 4D 얼굴 아바타 재구성을 위한 동적 신경 광도 필드. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8649-8658페이지, 2021년.[7] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang. Headnerf: 실시간 nerf 기반 매개변수 헤드 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2037-420-384페이지, 2022년.[8] Kai Hormann. 중심 보간. Approximation Theory XIV: San Antonio 2013, 197-218페이지. Springer, 2014년.[9] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. 꿈의 필드를 사용한 제로샷 텍스트 가이드 객체 생성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 867-876페이지, 2022.[10] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. 꿈의 필드를 사용한 제로샷 텍스트 가이드 객체 생성. 2022.[11] Nikolay Jetchev. Clipmatrix: 텍스트 제어 3D 텍스처 메시 생성. arXiv 사전 인쇄본 arXiv:2109.12922, 2021.[12] James T Kajiya and Brian P Von Herzen. 레이 트레이싱 볼륨 밀도. ACM SIGGRAPH 컴퓨터 그래픽, 18(3):165– 174, 1984.[13] Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzciński, Andrea Tagliasacchi. CoNeRF: 제어 가능한 신경 광도장. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022. 2,[14] Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann. 특징 필드 증류를 통한 편집을 위한 nerf 분해. 신경 정보 처리 시스템의 발전, 35권, 2022.[15] Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, Gerard Pons-Moll. Control-nerf: 장면 렌더링 및 조작을 위한 편집 가능한 특징 볼륨. arXiv 사전 인쇄본 arXiv:2204.10850, 2022. 2,[16] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. 다중 뷰 비디오에서 신경 3D 비디오 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5521-5531페이지, 2022.[17] Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, Or Litany. Lipschitz 정규화를 통해 원활한 신경 기능 학습. arXiv 사전 인쇄본 arXiv:2202.08345, 2022.[18] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell. 조건부 광도 필드 편집. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 5773-5783페이지, 2021.[19] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, Jonathan T Barron. 어둠 속의 Nerf: 노이즈가 많은 원시 이미지에서 높은 동적 범위 뷰 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16190-16199페이지, 2022.[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ECCV, 2020. 1, 2, 3,[21] Ali Mollahosseini, Behzad Hasani 및 Mohammad H Mahoor. Affectnet: 야생에서의 얼굴 표정, 가치 및 각성 컴퓨팅을 위한 데이터베이스. IEEE Transactions on Affective Computing, 10(1):18-31, 2017.[22] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz 및 Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 광도장. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 5865-5874쪽, 2021. 2,[23] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, Steven M. Seitz. Hypernerf: 위상적으로 변하는 신경 광도장에 대한 고차원 표현. ACM Trans. Graph., 40(6), 2021년 12월. 2, 3, 4,[24] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski. Styleclip: 스타일간 이미지의 텍스트 기반 조작. 국제 컴퓨터 비전 컨퍼런스, 2085-2094쪽, 2021.[25] Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall. Dreamfusion: 2D 확산을 사용한 텍스트-3D. arXiv 사전 인쇄본 arXiv:2209.14988, 2022.[26] Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer. D-nerf: 동적 장면을 위한 신경 광도장. arXiv 사전 인쇄본 arXiv:2011.13961, 2020.[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763페이지. PMLR, 2021. 2, 3,[28] Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i Nieto, Francesc MorenoNoguer. H3d-net: Few-shot 고충실도 3D 헤드 재구성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 5620-5629페이지, 2021.[29] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan. Clip-forge: 제로샷 텍스트-모양 생성을 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18603-18613페이지, 2022.[30] Andrey V Savchenko. 모바일 기기를 위한 얼굴 표정, 가치, 각성 및 액션 단위의 프레임 수준 예측. arXiv 사전 인쇄본 arXiv:2203.13436, 2022.[31] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, Jan-Michael Frahm. 비정형 다중 뷰 스테레오를 위한 픽셀별 뷰 선택. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2016.[32] Sahil Sharma 및 Vijay Kumar. 딥 러닝 시대의 3D 얼굴 재구성: 조사. Archives of Computational Methods in Engineering, 1-33페이지, 2022.[33] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu. Ide-3d: 고해상도 3D 인식 초상화 합성을 위한 대화형 얽힘 없는 편집. arXiv 사전 인쇄본 arXiv:2205.15517, 2022.[34] Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang. Fenerf: 신경 광도장에서의 얼굴 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7672-7682페이지, 2022. 1,[35] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, Ren Ng. 좌표 기반 신경 표현을 최적화하기 위한 학습된 초기화. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2846-2855페이지, 2021.[36] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, Matthias Nießner. Face2face: RGB 비디오의 실시간 얼굴 캡처 및 재연. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2387-2395페이지, 2016. 2,[37] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, Pratul P. Srinivasan. Ref-NeRF: 신경 광도장에 대한 구조화된 뷰 종속 모양. CVPR, 2022.[38] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao. Clip-nerf: 신경 광도장의 텍스트 및 이미지 기반 조작. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3835-3844페이지, 2022.[39] Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollhofer. 동적 인간 머리의 구성적 광도장 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 57045713페이지, 2021.[40] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He. Attngan: 주의 생성적 적대적 네트워크를 사용한 세분화된 텍스트에서 이미지 생성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 13161324페이지, 2018.[41] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui. 편집 가능한 장면 렌더링을 위한 학습 객체-구성 신경 광도 필드. 국제 컴퓨터 비전 컨퍼런스(ICCV), 2021년 10월.[42] Tarun Yenamandra, Ayush Tewari, Florian Bernard, HansPeter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt. i3dmm: 인간 머리의 딥 임플리멘탈 3D 모핑 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12803-12813페이지, 2021.[43] Yuichi Yoshida 및 Takeru Miyato. 딥 러닝의 일반화 가능성을 개선하기 위한 스펙트럼 규범 정규화. arXiv 사전 인쇄본 arXiv:1705.10941, 2017.[44] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa. 신경 광도장의 실시간 렌더링을 위한 Plenoctrees. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 57525761페이지, 2021.[45] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, Lin Gao. Nerf 편집: 신경 광도장의 기하 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18353-18364페이지, 2022. 2,[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 지각적 척도로서의 딥 피처의 비합리적인 효과. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 586-595페이지, 2018. 7,[47] Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C Bühler, Xu Chen, Michael J Black, Otmar Hilliges. Im avatar: 비디오에서 가져온 암묵적으로 변형 가능한 머리 아바타. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 13545-13555페이지, 2022. 2,[48] Yiyu Zhuang, Hao Zhu, Xusen Sun 및 Xun Cao. Mofanerf: 변형 가능한 얼굴 신경 광채장. arXiv 사전 인쇄본 arXiv:2112.02308, 2021.
