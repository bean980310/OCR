--- ABSTRACT ---
Transformers는 고정된 컨텍스트 길이의 작업에서 인상적인 일반화 기능을 가지고 있습니다. 그러나 문자열 복제와 같은 겉보기에 간단한 작업에서도 임의 길이의 시퀀스로 일반화하는 데 실패합니다. 게다가 글로벌 어텐션 메커니즘의 2차 계산 복잡도로 인해 더 긴 시퀀스에서 단순히 학습하는 것은 비효율적입니다. 이 작업에서 우리는 이 실패 모드가 더 긴 시퀀스(상대적 인코딩의 경우에도)에 대한 위치 인코딩이 분포에서 벗어나는 것과 관련이 있음을 보여주고 이 문제를 극복할 수 있는 새로운 위치 인코딩 패밀리를 소개합니다. 구체적으로, 우리의 무작위 위치 인코딩 체계는 더 긴 시퀀스의 위치를 시뮬레이션하고 시퀀스의 길이에 맞는 순서가 있는 하위 집합을 무작위로 선택합니다. 15개의 알고리즘 추론 작업에 걸쳐 6000개의 모델에 대한 대규모 실증적 평가는 우리의 방법이 Transformers가 보이지 않는 길이의 시퀀스로 일반화할 수 있음을 보여줍니다(평균적으로 테스트 정확도를 12.0% 증가시킴). 1
--- INTRODUCTION ---
트랜스포머는 시퀀스 대 시퀀스 모델링(Vaswani et al., 2017), 이미지 인식(Dosovitskiy et al., 2021), 멀티태스크 학습(Reed et al., 2022)을 포함한 많은 최근의 획기적인 발전을 뒷받침하면서 머신 러닝의 새로운 워크호스로 부상하고 있습니다. 그러나 최근 연구(Delétang et al., 2023)에 따르면 트랜스포머는 이진 덧셈과 같은 겉보기에 간단한 작업에서 더 긴 시퀀스로 일반화하지 못하는 것으로 나타났습니다. 따라서 특정 문제는 길이 일반화 없이도 해결할 수 있지만 알고리즘 추론에는 일반적으로 온라인 또는 지속적인 학습과 같은 많은 실제 환경과 유사하게 이 기능이 필요합니다. 트랜스포머의 주의 메커니즘은 복잡한 관계를 인식할 수 있지만 *동등한 기여. ¹DeepMind. 2 스위스 AI 연구소, IDSIA, USI 및 SUPSI. *저자가 DeepMind에 있는 동안 수행한 작업. {anianr, gdelt} @deepmind.com으로 문의하세요. 표준 위치 인코딩 훈련 12...N 평가 12 ... NN+1 ...M 분포 밖 무작위 위치 인코딩(저희의 것) 12-NML 훈련 |1| 4 |-- L-각 배치에 대한 무작위 위치 인코딩 벡터 평가 12. NN N+1 ... M 분포 내 그림 1: 더 긴 입력을 사용한 테스트 시간 평가. 표준 위치 인코딩 벡터는 훈련 중에 관찰된 것보다 큰 값을 갖습니다. 저희의 접근 방식은 각 훈련 예제에 가능한 테스트 위치의 전체 범위를 사용하여 무작위(순서화된) 위치 인코딩 벡터를 할당함으로써 이 문제를 피합니다. 입력 시퀀스에서 ken이 많으면 위치 인식이 부족하여 제한을 받습니다. 따라서 일반적으로 입력 시퀀스는 위치 정보를 계산에 주입하기 위해 위치 인코딩으로 보강됩니다. 그러나 현재 접근 방식은 최대 훈련 시퀀스 길이 N까지의 위치만 고려하므로 길이가 M까지인 테스트 시퀀스에 대한 모든 위치 N+1, . . ., M은 평가 중에 분포 밖인 것처럼 보입니다(그림 1 상단). 이 작업 우리는 알고리즘 추론 작업에서 Transformer의 길이 일반화 기능을 크게 개선하는 새로운 무작위 위치 인코딩 패밀리를 소개합니다.우리의 접근 방식은 기존의 모든 위치 인코딩 체계와 호환되며 훈련 또는 평가 중에 관찰된 것보다 훨씬 더 넓은 범위의 위치에서 정렬된 위치 집합을 하위 샘플링하여 기존 방법을 보강합니다(즉, LM까지; 그림 1 하단).따라서 훈련 과정에서 Transformer는 매우 큰 위치 인코딩을 처리하는 방법을 학습하고 따라서 평가 중에 분포 밖 입력을 더 이상 만나지 않습니다.중요한 점은 우리의 방법이 도메인 내 일반화 성능에 영향을 미치지 않으며 더 긴 시퀀스에서 Transformer를 단순히 훈련하는 순진한 접근 방식보다 훨씬 더 효율적이라는 것입니다.우리의 주요 기여는 다음과 같습니다.• Transformer의 길이 일반화 기능을 크게 개선하는 동시에 도메인 내 일반화 성능에 영향을 미치지 않는 새로운 위치 인코딩 체계 패밀리. • 광범위한 알고리즘 추론 과제에 대한 대규모 실증적 평가는 기존 작업보다 우리 방법의 우수성을 보여줍니다(테스트 정확도가 평균 12.0% 증가하고 특정 과제에서는 최대 43.5% 증가). • https://github.com/deepmind/randomized_positional_encodings에서 사용 가능한 우리 방법의 오픈 소스 구현. 2
--- RELATED WORK ---
저희의 작업은 Transformers의 위치 인코딩에 대한 증가하는 연구 라인과 가장 밀접하게 관련되어 있습니다.첫 번째 접근 방식은 단순히 토큰의 위치 변환, 예를 들어 확장된 사인파(Vaswani et al., 2017) 또는 학습된 임베딩(Gehring et al., 2017)을 입력 시퀀스의 임베딩에 추가했습니다.Dai et al.(2019)은 이후 키와 쿼리 벡터 간의 상대 거리를 사용하여 어텐션(모든 계층에서)을 계산하면 장기(컨텍스트 간) 종속성 모델링이 개선됨을 보여주었습니다.마찬가지로 Su et al.(2021)은 상대 거리에 따라 키-쿼리 제품을 회전하여 위치 정보를 주입하는 것을 제안했습니다.마지막으로 Press et al.(2022)은 각 키-쿼리 어텐션 점수(거리에 비례)에 상수 편향을 추가하여 자연어 처리 작업에서 길이 일반화를 개선했습니다. 그러나 4절의 실험에서 알 수 있듯이 이러한 접근 방식은 알고리즘 추론 과제에서 길이 일반화에 실패하는데, 바로 이것이 우리 연구의 목표입니다. 동시 연구에서는 무작위로 학습된 위치 인코딩(Li and McClelland, 2022)을 개발했는데, 이는 무작위로 위치 인코딩 패밀리의 특별한 경우입니다. 또한 길이 일반화를 위한 특징 및 위치 무작위화의 필요성이 Transformers를 포함하는 그래프 신경망의 맥락에서 논의되었다는 점에 주목합니다(Ibarz et al., 2022; Sato et al., 2021). 마지막으로 Liu et al.(2020b)은 훈련 시간 동안 본 것보다 더 긴 시퀀스를 처리하기 위해 위치 정보를 연속적인 동적 시스템으로 모델링할 것을 제안했습니다. 저희의 작업은 또한 Transformers의 체계적(길이) 일반화 기능을 개선하는 연구 분야와 관련이 있습니다(Ontañón et al., 2022). 여기에는 임베딩 스케일링 또는 조기 중단(Csordás et al., 2021), 적응적 계산 시간(Dehghani et al., 2019), 방향 위치 인코딩 및 게이팅을 사용한 기하학적 주의(Csordás et al., 2022), 계층적 강화 학습(Liu et al., 2020a)을 조사하는 접근 방식이 포함됩니다. 이러한 길이 일반화 연구는 종종 형식 언어 이론의 맥락에서 수행되며, 저희는
--- METHOD ---
Transformers가 보이지 않는 길이의 시퀀스로 일반화할 수 있게 합니다(평균적으로 테스트 정확도를 12.0% 증가시킴). 1 서론 Transformers는 시퀀스 대 시퀀스 모델링(Vaswani et al., 2017), 이미지 인식(Dosovitskiy et al., 2021), 멀티태스크 학습(Reed et al., 2022)을 포함한 많은 최근의 획기적인 발전을 뒷받침하면서 머신 러닝의 새로운 워크호스로 부상하고 있습니다. 그러나 최근 연구(Delétang et al., 2023)는 Transformers가 이진 덧셈과 같은 겉보기에 간단한 작업에서 더 긴 시퀀스로 일반화하지 못한다는 것을 보여주었습니다. 따라서 특정 문제는 길이 일반화 없이도 해결할 수 있지만, 알고리즘 추론에는 일반적으로 온라인이나 지속적 학습과 같은 많은 실제 환경과 유사하게 이 기능이 필요합니다. Transformer의 주의 메커니즘은 복잡한 관계를 인식할 수 있지만, *동등한 기여. ¹DeepMind. 2 스위스 AI 연구소, IDSIA, USI &amp; SUPSI. *저자가 DeepMind에 있을 때 수행한 작업. {anianr, gdelt} @deepmind.com으로 문의하세요. 표준 위치 인코딩 훈련 12...N 평가 12 ... NN+1 ...M 분포 외 무작위 위치 인코딩(저희의 것) 12-NML 훈련 |1| 4 |-- L-각 배치에 대한 무작위 위치 인코딩 벡터 평가 12. NN N+1 ... M 분포 내 그림 1: 더 긴 입력을 사용한 테스트 시간 평가. 표준 위치 인코딩 벡터는 훈련 중에 관찰된 값보다 큰 값을 갖습니다. 저희의 접근 방식은 각 훈련 예제에 가능한 테스트 위치의 전체 범위를 사용하여 무작위(순서화된) 위치 인코딩 벡터를 할당하여 이 문제를 방지합니다. 입력 시퀀스에서 ken이 많을수록 위치 인식이 부족하여 제한을 받습니다. 따라서 일반적으로 입력 시퀀스는 위치 정보를 계산에 주입하기 위해 위치 인코딩으로 증강됩니다. 그러나 현재의 접근 방식은 최대 학습 시퀀스 길이 N까지의 위치만 고려하므로 최대 M 길이의 테스트 시퀀스에 대한 모든 위치 N+1, . . ., M은 평가 중에 분포에서 벗어난 것으로 나타납니다(그림 1 상단). 이 작업에서는 알고리즘 추론 작업에서 Transformer의 길이 일반화 기능을 크게 개선하는 새로운 무작위 위치 인코딩 패밀리를 소개합니다. 저희의 접근 방식은 기존의 모든 위치 인코딩 체계와 호환되며 학습 또는 평가 중에 관찰된 것보다 훨씬 더 넓은 범위의 위치에서 정렬된 위치 집합을 하위 샘플링하여 기존 방법을 보강합니다(예: LM까지, 그림 1 하단). 따라서 학습 과정에서 Transformer는 매우 큰 위치 인코딩을 처리하는 방법을 학습하므로 평가 중에 분포에서 벗어난 입력을 더 이상 접하지 않습니다. 중요한 점은 저희의 방법이 도메인 내 일반화 성능에 영향을 미치지 않으며 더 긴 시퀀스에서 Transformer를 학습하는 단순한 접근 방식보다 훨씬 더 효율적이라는 것입니다. 우리의 주요 기여는 다음과 같습니다. • Transformers의 길이 일반화 기능을 크게 개선하는 동시에 도메인 내 일반화 성능에는 영향을 미치지 않는 새로운 위치 인코딩 체계 계열 • 이전 작업보다 우리 방법이 우수함을 보여주는 광범위한 알고리즘 추론 작업에 대한 대규모 실증적 평가(테스트 정확도가 평균 12.0% 증가하고 특정 작업에서는 최대 43.5% 증가). • https://github.com/deepmind/randomized_positional_encodings에서 사용 가능한 우리 방법의 오픈 소스 구현. 2 관련 작업 우리의 작업은 Transformers의 위치 인코딩에 대한 연구 분야와 가장 밀접하게 관련되어 있습니다. 첫 번째 접근 방식은 토큰 위치의 변환(예: 확장된 사인파(Vaswani et al., 2017) 또는 학습된 임베딩(Gehring et al., 2017))을 입력 시퀀스의 임베딩에 추가했습니다. Dai et al. (2019)는 이후 키와 쿼리 벡터 간의 상대 거리를 사용하여 주의(모든 계층에서)를 계산하면 장기(문맥 간) 종속성 모델링이 개선됨을 보여주었습니다. 마찬가지로 Su et al. (2021)은 키-쿼리 제품을 상대 거리에 따라 회전하여 위치 정보를 주입하는 것을 제안했습니다. 마지막으로 Press et al. (2022)는 각 키-쿼리 주의 점수(거리에 비례)에 상수 편향을 추가하여 자연어 처리 작업에서 길이 일반화를 개선했습니다. 그러나 우리의
--- EXPERIMENT ---
4절의 s에서 보여드릴 것처럼, 이러한 접근 방식은 알고리즘 추론 작업에서 길이 일반화에 실패하는데, 이는 바로 우리 작업의 목표입니다. 동시 연구에서는 무작위로 학습된 위치 인코딩(Li and McClelland, 2022)을 개발했는데, 이는 무작위로 위치 인코딩 패밀리의 특별한 경우입니다. 또한 길이 일반화를 위한 특징 및 위치 무작위화의 필요성이 Transformers를 포함하는 그래프 신경망의 맥락에서 논의되었다는 점에 유의합니다(Ibarz et al., 2022; Sato et al., 2021). 마지막으로, Liu et al.(2020b)은 훈련 시간 동안 본 것보다 더 긴 시퀀스를 처리하기 위해 위치 정보를 연속적인 동적 시스템으로 모델링할 것을 제안했습니다. 우리의 작업은 또한 Transformers의 체계적(길이) 일반화 기능을 개선하는 연구 분야와 관련이 있습니다(Ontañón et al., 2022). 여기에는 임베딩 스케일링 또는 조기 중단(Csordás et al., 2021), 적응적 계산 시간(Dehghani et al., 2019), 방향적 위치 인코딩 및 게이팅을 사용한 기하학적 주의(Csordás et al., 2022), 계층적 강화 학습(Liu et al., 2020a)을 조사하는 접근 방식이 포함됩니다. 이러한 길이 일반화 연구는 종종 형식 언어 이론의 맥락에서 수행되며, 우리는 Delétang et al.의 최근 벤치마크에서 우리의 방법을 평가합니다. (2023), 이는 Transformers의 형식 언어 인식 능력에 대한 많은 연구를 통합한 것입니다(Ackerman 및 Cybenko, 2020; Bhattamishra et al., 2020; Ebrahimi et al., 2020; Hahn, 2020; Hao et al., 2022; Merrill, 2019; Merrill 및 Sabharwal, 2022). 3 무작위 위치 인코딩 토큰을 한 번에 한 단계씩 펼치는 RNN(Elman, 1990)과 달리 Transformers는 전역 어텐션을 통해 입력 시퀀스의 대량 청크를 병렬로 처리합니다(Vaswani et al., 2017). 결과적으로 Transformers는 이전 토큰을 &quot;기억&quot;할 필요가 없지만 어텐션 메커니즘의 순열 불변성을 깨야 합니다. 이를 위해 입력 시퀀스의 임베딩은 일반적으로 위치 인코딩으로 보강됩니다. 예를 들어, vanilla Transformer는 어텐션 계층에 전달하기 전에 다음 위치 인코딩을 임베디드 입력 시퀀스에 추가합니다.PE(pos, 2i) = sin PE(pos, 2i + 1) = COS pos 2i (1) 10000 dmodel pos(2) 10000 dmodel 여기서 pos는 시퀀스에서 토큰의 위치이고, dmodel Є N은 입력 임베딩의 차원이며, i = {1, 2, . . ., dmodel/2}입니다.위치 인코딩은 일반적으로 고정 길이의 시퀀스에 필요한 위치 정보를 유도하는 데 성공하지만 길이 일반화를 방해하는 주요 실패 모드 중 하나입니다.구체적으로, 최대 길이 N의 시퀀스 커리큘럼에서 학습된 표준 위치 인코딩이 있는 Transformer의 경우, 길이가 M &gt; N인 테스트 시퀀스는 결과 위치 인코딩의 분포를 학습에서 볼 수 있는 분포에서 벗어나게 하며, M이 커질수록 이 변화가 점점 더 커집니다. 이를 해결하기 위해, 순서 정보에만 의존하고 구성 가능한 하이퍼파라미터 L을 사용하여 최대 길이 M의 시퀀스까지 일반화할 수 있는 무작위 인코딩 방식을 제안합니다.여기서 N &lt; M ≤ L입니다.~ 무작위 위치 인코딩 각 학습 단계에서 고정된 크기의 데이터 배치에 대한 손실 최소화 단계를 수행한다고 가정합니다.U(S)가 집합 S에 대한 이산 균일 분포를 나타내고 Pk := {S ≤ {1,..., L} | |S| = k}라고 합니다.각 학습 단계에서 먼저 무작위 길이 n ~ · U({1,...,N})(Delétang et al., 2023에 따름)을 샘플링한 다음 무작위 인덱스 집합 IU(Pn)을 샘플링합니다.그런 다음 I를 오름차순으로 정렬하여 I = {i1, ………, in}(i₁ &lt; i2 &lt; I... &lt; in)을 설정합니다.여기서 I는 중복 없이 샘플링됩니다. 마지막으로 토큰 1 ≤ j ≤ N에 대한 무작위 위치 인코딩을 RPE(j, ·) · PE(ij, ·)로 계산합니다. 테스트 시간에 길이 M&gt; N의 시퀀스를 처리할 때 모든 토큰 위치 1 ≤ j ≤ M에 대해 동일한 절차를 사용합니다. 우리 방법의 직관은 상대 인코딩의 알려진 좋은 속성을 보존하지만 최대 학습 길이 N과 독립적인 방식으로 유지하여 테스트 시간에 더 긴 시퀀스로 일반화할 수 있도록 하는 것입니다. = 무작위 위치 인코딩 체계를 적용할 때 확장된 위치를 배치당 한 번만 하위 샘플링하고 모든 시퀀스에 대해 개별적으로 샘플링하지 않습니다. sin / cos(Vaswani et al., 2017), 학습된(Gehring et al., 2017), ROPE 인코딩(Su et al., 2021)의 경우 위에 설명한 대로 방법을 적용합니다. 즉, 원래 토큰 위치를 샘플링된 대응 위치로 직접 대체합니다. 상대적 인코딩(Dai et al., 2019)의 경우 원래 위치 대신 샘플링된 위치 간의 상대적 거리를 계산합니다. 마지막으로 ALiBi(Press et al., 2022)의 경우 확장된 위치 집합에서 바이어스 값을 샘플링합니다. 결과적으로 토큰의 위치 인코딩은 더 이상 정확한 위치와 직접 관련이 없습니다(인코딩은 모든 단계에서 재샘플링되므로 학습 중에도 변경됩니다). 그러나 인코딩의 순서를 유지하므로 Transformer는 여전히 하위 샘플링된 인코딩에서 관련 위치 정보를 추출하는 방법을 학습할 수 있습니다. 실제로 부록 B.1의 절제 연구에서 샘플링된 위치를 정렬할 필요성을 검증합니다. 따라서 인코딩 방식의 성공은 Transformer 아키텍처의 귀납적 바이어스에 대한 흥미로운 통찰력을 제공합니다. 4절에서 보여드리겠지만, 최대 N 길이로만 학습한 무작위 인코딩은 최대 M 길이로 학습한 기존 방식과 마찬가지로 길이 M의 시퀀스에서 동일한 성능을 보입니다.따라서 저희 방법은 (i) 더 긴 시퀀스가 동일한 구조를 공유하고 (ii) 더 긴 위치가 학습 중에 관찰되는 한 Transformers가 짧은 시퀀스에서 효율적으로 학습될 수 있음을 보여줍니다.또한, 시퀀스 길이 l에 대해 글로벌 어텐션의 실행 시간이 O(12)이므로 저희 인코딩 방식은 긴 시퀀스에서 모델을 직접 학습하는 것보다 훨씬 빠릅니다.또한, 저희의 무작위 위치 인코딩 방식은 도메인 내 일반화 성능에는 크게 영향을 미치지 않으면서 길이 일반화를 크게 향상시킨다는 점도 알 수 있습니다(그림 4 참조).저희 방식의 주요 한계는 L &gt; M을 선택하기 위해 최대 테스트 시퀀스 길이 M을 미리 알아야 한다는 것입니다.그러나 저희 방식은 L에 대한 광범위한 값과 호환되며(부록 B.1 참조), 이는 단순히 긴 시퀀스에서 학습하는 순진한 방식에 필요한 가정보다 훨씬 약하다는 점에 유의합니다. 그러나 L이 N 또는 M보다 훨씬 크게 선택된 경우 학습 중에 모델이 충분한 고유 인덱스를 만날 가능성이 이론적으로 낮아 성능이 저하될 가능성이 있습니다(분포 내 및 분포 외 모두).4 실험 평가 문제 설정 Delétang et al.(2023)의 실험 설정을 면밀히 따르고 모듈러 산술, 문자열 반전/복제, 이진 덧셈/곱셈, 버킷 정렬과 같은 광범위한 알고리즘 추론 작업에서 방법을 평가합니다.작업은 형식 언어 인식에서 파생되었으므로 언어를 일반(R), 문맥 자유, 문맥 민감(CS) 및 재귀적으로 열거 가능한 언어로 구분하는 Chomsky 계층(Chomsky, 1956)에 따라 그룹화됩니다.일반 작업은 유한 상태 오토마타(FSA)로 해결할 수 있습니다.표 1: 정확도(백분율)는 모든 테스트 길이에 걸쳐 평균화되고 10개의 난수 시드와 3개의 학습 속도에서 최대화되었습니다. 무작위 정확도는 50%이며, MODULAR ARITHMETIC (SIMPLE), CYCLE NAVIGATION, BUCKET SORT, MODULAR ARITHMETIC은 20%입니다. 무작위 방법은 평균적으로 테스트 정확도를 12.0% 높입니다. 무작위 학습 인코딩(*로 표시)은 레이블 기반 인코딩과 동일합니다(Li 및 McClelland, 2022). †는 위치 정보 없이도 해결할 수 있는 순열 불변 작업을 나타냅니다. 무작위(우리의) 수준 과제 없음 sin/cos 상대 ALiBi ROPE 학습 sin/cos 상대 ALiBi ROPE 학습⭑ 짝수 쌍 50.50.96.67.3 51.50.100.100.81.5 100.97.모듈러 산술(단순) 20.20.21.24.2 21.20.25.28.21.2 25.21.R 패리티 검사+ 51.50.51.51.7 51.50.52.52.50.52.52.사이클 탐색+ 61.26.23.37.23.24.59.58.29.73.49.스택 조작 50.50.53.57.51.49.72.77.9 70.6 68.69.역행렬 52.50.58.62.3 51.50.75.95.1 77.69.52.DCF 모듈러 산술 31.28.30.32.5 25.25.33.34.31.32.31.방정식 풀기 20.21.23.25.23.20.24.28.22.24.22.중복 문자열 52.50.51.51.3 50.50.72.75.68.68.53.누락된 중복 52.51.54.54.56.51.52.100.0 79.88.52.ODDS FIRST 52.51.52.51.51.50.65.69.64.7 65.52.CS 이진 덧셈 50.49.54.51.4 50.49.64.64.56.60.61.이진 곱셈 49.50.52.51.50.49.52.50.50.51.51.제곱근 계산 50.50.52.50.9 50.50.52.53.51.2 52.52.버킷 정렬+ 23.30.91.38.30.25.100.100.99.99.99.터미널 컨텍스트 프리(DCF) 작업은 FSA가 액세스할 수 있는 결정론적 스택과 CS 작업은 제한된 테이프에 액세스할 수 있는 FSA로 해결할 수 있습니다. Chomsky 계층과의 관계는 우리 작업과 크게 관련이 없으며 완전성을 위해서만 포함되었습니다. 우리는 현재 Transformers에 도달할 수 없고 알고리즘 추론 작업에서 일반화에 실패했음을 분명히 보여주기 때문에 Delétang et al.(2023)의 벤치마크에서 우리 방법을 평가합니다. 자세한 내용은 관심 있는 독자에게 원본 논문을 참조하세요. 우리는 BERT(Devlin et al., 2019) 또는 Gopher(Rae et al., 2021)와 같은 인기 있는 사전 학습된 언어 모델에서 사용되는 원래 seq-to-seq Transformer(Vaswani et al., 2017)의 인코더 전용 모델을 고려합니다. 따라서 다중 토큰 출력 시퀀스 y(예: 문자열 복제)가 필요한 작업의 경우 입력 시퀀스를 y개의 빈 토큰으로 패딩하고 패딩된 시퀀스에서 전체 Transformer 출력을 계산합니다(즉, 자기 회귀 샘플링을 사용하지 않습니다). 우리는 N = 40인 U(1, N)에서 균일하게 샘플링된 길이의 시퀀스에서 모델을 훈련하고, M = 500인 길이 {N+1,..., M}의 시퀀스에서 평가합니다. 우리는 최대 위치 L을 2048로 설정하고(부록 B.1에서 성능에 대한 다른 값의 영향을 시각화합니다). 우리는 10개의 다른 매개변수 초기화 시드와 3개의 학습 속도 1 × 10-4, 3 × 10-4, 5 × 10-4 중에서 가장 성능이 좋은 모델에 대해 보이지 않는 모든 시퀀스 길이, 즉 N + 1,..., M에 걸쳐 평균화된 정확도를 보고합니다. 우리는 Delétang et al. (2023)과 동일한 하이퍼파라미터를 사용하고 부록 A에 전체 실험 설정을 제공합니다. 우리는 https://github.com/deepmind/randomized_positional_encodings에서 코드를 공개적으로 제공합니다. 이전 작업과의 비교 우리는 우리 방법을 광범위한 위치 인코딩과 비교합니다: 없음, sin/cos(Vaswani 등, 2017), 상대적(Dai 등, 2019), ALiBi(Press 등, 2022), ROPE(Su 등, 2021), 학습된(Gehring 등, 2017), 레이블 기반(Li 및 McClelland, 2022). Li 및 McClelland(2022)가 제안한 레이블 인코딩은 무작위로 학습된 위치 인코딩과 동일하므로 우리 방법에 포함됩니다. 우리는 위의 모든 인코딩으로 무작위 위치 인코딩 체계를 인스턴스화하고 표 1에 평균 테스트 정확도를 표시합니다(부록 B.2에 테스트 길이에 따른 성능 곡선 포함). 우리는 무작위 버전이 대부분 작업에서 테스트 정확도를 상당히 높이는 것을 관찰합니다(평균 12.0%, 최대 43.5%). 특히, 무작위 상대 인코딩은 이전에는 이전 작업으로는 도달할 수 없었던 작업(예: REVERSE STRING 또는 MISSING DUPLICATE)을 해결합니다. 효율성 비교 이제 우리는 우리의 방법을 통해 짧은 시퀀스에서 모델을 훈련하고 90% 이상의 테스트 정확도를 얻을 수 있음을 보여줍니다. 이는 긴 시퀀스에서 모델을 훈련하는 순진한 접근 방식보다 약 35.4배 빠릅니다. 영어: 이러한 목적을 위해, 우리는 평균 테스트 정확도 1.0.90.0.0.6에 대한 무작위 상대적(우리의) U(1, 40)에 대한 상대적 U(1, 500)에 대한 상대적 0에 대한 무작위 상대적 인코딩을 훈련합니다.훈련 시간(초)그림 2: 두 모델에 대한 훈련 시간(초)에 대한 누락된 중복 작업에 대한 보이지 않는 테스트 길이에 대한 평균 정확도: (i) 최대 훈련 시퀀스 길이가 40인 무작위 상대적 위치 인코딩, (ii) 최대 훈련 길이가 500인 기존 상대적 위치 인코딩. 최대 길이가 40인 시퀀스와 최대 길이가 500인 시퀀스에 대한 기존 상대적 위치 인코딩(Dai et al., 2019)을 비교하고, 그림 2에서 훈련 시간(초)에 대한 테스트 정확도(길이 41~500에 대한 평균)를 보여줍니다. 우리 모델은 글로벌 어텐션의 2차 비용(시퀀스 길이 측면에서)으로 인해 훨씬 더 빠르게 강력한 테스트 정확도를 얻습니다. 즉, 우리 모델은 순진한 접근 방식(NVIDIA V100 GPU에서)의 경우 초당 22.1단계에 비해 초당 168단계로 학습합니다. 5
--- CONCLUSION ---
우리는 Transformers의 길이 일반화 기능을 크게 개선하는 새로운 위치 인코딩 패밀리를 도입했습니다. 우리의 위치 인코딩은 시퀀스 길이를 늘리면 기존 위치 인코딩이 분포에서 벗어날 것이라는 통찰력을 기반으로 합니다. 따라서 이 문제를 해결하기 위해 테스트 시간에 본 길이보다 더 넓은 범위에서 인코딩을 무작위로 샘플링하면서 순서를 유지합니다. 우리의 대규모 실증적 평가는 우리의 방법이 더 긴 시퀀스에서 모델을 훈련하는 순진한 접근 방식보다 뛰어난 계산 성능을 제공하는 동시에 길이 일반화 측면에서 이전 작업보다 상당히 우수하다는 것을 보여줍니다. 한계 우리의 연구는 임의 길이의 시퀀스에 대한 Transformers의 일반화 기능을 개선하는 데 유망한 결과를 보여주지만 몇 가지 한계를 고려해야 합니다. 첫째, 우리의 평가는 자연어의 복잡성과 다양성을 완전히 포착하지 못할 수 있는 합성 알고리즘 추론 작업에 국한됩니다. 우리는 Transformer 아키텍처의 명확하고 다소 놀라운 한계를 보여주었기 때문에 합성 데이터 세트에 초점을 맞추었습니다(Delétang et al., 2023). 그러나 다른 작업 및 도메인에 대한 우리 접근 방식의 일반화 가능성은 여전히 미지수이며, SCAN(Lake and Baroni, 2018), CFQ(Keysers et al., 2020), COGS(Kim and Linzen, 2020) 또는 Long Range Arena(Tay et al., 2021)에 대한 평가와 같은 추가 연구가 실제 응용 프로그램에서의 잠재력을 이해하는 데 필요합니다.둘째, 우리 접근 방식은 최대 시퀀스 위치 L이라는 새로운 하이퍼파라미터를 도입합니다.부록 B.1의 실험에서 우리 방법의 성능은 L의 정확한 값에 크게 영향을 받지 않지만 실무자는 여전히 특정 문제 도메인에 따라 매개변수를 조정해야 할 수 있습니다.셋째, 우리는 합성 데이터 세트에서 Transformer 길이 일반화의 하나의 실패 모드만 분리하고 개선합니다.그러나 주의가 긴 시퀀스에서 덜 정점화되는 것과 같이 길이 일반화가 좋지 않은 데 기여하는 다른 요인도 있습니다(Chiang and Cholak, 2022). 전반적으로, 우리 연구의 한계는 미래 연구에 대한 몇 가지 흥미로운 방향을 제공한다고 믿습니다.감사의 말 도움이 되는 피드백을 주신 Chris Cundy, Elliot Catt, Kevin Li, Laurent Orseau, Marcus Hutter, Petar Veličković, Vincent Dutordoir 및 익명의 검토자에게 감사드립니다.참고문헌 A Joshua Ackerman 및 George Cybenko.2020. 신경망 및 형식 언어 조사.arXiv:2006.01338. Satwik Bhattamishra, Kabir Ahuja 및 Navin Goyal.2020. 형식 언어를 인식하는 변환기의 능력 및 한계에 관하여.자연어 처리의 경험적 방법에 대한 2020년 컨퍼런스 회의록에서.David Chiang 및 Peter Cholak.2022. 자기 주의의 이론적 한계 극복.계산 언어학 협회의 제60차 연례 회의록에서.노암 촘스키. 1956. 언어 설명을 위한 세 가지 모델. IRE Trans. Inf. 이론. Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber. 2021. 악마는 세부 사항에 있다: 간단한 트릭은 변압기의 체계적 일반화를 개선한다. 2021년 자연어 처리 경험적 방법 컨퍼런스의 회의록에서. Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber. 2022. 신경 데이터 라우터: 변압기의 적응 제어 흐름은 체계적 일반화를 개선한다. 제10회 학습 표현 국제 컨퍼런스에서. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, Ruslan Salakhutdinov. 2019. Transformer-xl: 고정 길이 맥락을 넘어서는 주의 깊은 언어 모델. 제57회 계산 언어학 협회 컨퍼런스의 회의록에서. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal transformers. 제7회 학습 표현 국제 컨퍼런스에서. Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega. 2023. Neural networks and the Chomsky hierarchy. 제11회 학습 표현 국제 컨퍼런스에서. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 컨퍼런스 회의록: 인간 언어 기술,. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 2021. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변압기. 제9회 국제 학습 표현 컨퍼런스에서. Javid Ebrahimi, Dhruv Gelda, Wei Zhang. 2020. 셀프 어텐션 네트워크는 어떻게 dyck-n 언어를 인식할 수 있을까요? 계산 언어학 협회의 연구 결과에서. Jeffrey L. Elman. 1990. 시간에서 구조 찾기. 인지 과학. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin. 2017. 합성곱 시퀀스 대 시퀀스 학습. 제34회 기계 학습 국제 컨퍼런스의 회의록에서. Michael Hahn. 2020. 신경 시퀀스 모델에서 셀프 어텐션의 이론적 한계. Trans. Assoc. Comput. Linguistics. Yiding Hao, Dana Angluin, and Robert Frank. 2022. 하드 어텐션 변환기를 통한 형식 언어 인식: 회로 복잡도의 관점. Trans. Assoc. Comput. Linguistics. Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bosnjak, Alex Vitvitskyi, Yulia Rubanova, Andreea Deac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, and Petar Velickovic. 2022. 일반 신경 알고리즘 학습자. Learning on Graphs Conference, LoG 2022, 2022년 12월 9일, 가상 이벤트. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, Olivier Bousquet. 2020. Measuring compositional generalization: A comprehensive method on Realistic data. In 8th International Conference on Learning Representations. Najoung Kim and Tal Linzen. 2020. COGS: A compositional generalization challenge based on semantic interpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations. Brenden M. Lake와 Marco Baroni. 2018. 체계성 없는 일반화: 시퀀스-투-시퀀스 순환 네트워크의 구성 기술에 관하여. 제35회 국제 기계 학습 컨퍼런스 논문집. Yuxuan Li와 James L. McClelland. 2022. 구조화된 작업에서 학습된 변압기의 체계적 일반화와 새로운 구조. arXiv:2210.00400. Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, Dongmei Zhang. 2020a. 분석적 표현식을 학습하여 구성적 일반화. Advances in Neural Information Processing Systems 33. Xuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, Cho-Jui Hsieh. 2020b. 연속 동적 모델을 사용하여 변압기의 위치를 인코딩하는 방법 학습. 기계 학습에 대한 제37회 국제 컨퍼런스의 진행 사항.William Merrill.2019. Sequential neural networks as automata.arXiv:1906.01615.William Merrill과 Ashish Sabharwal.2022. Logprecision transformers are constant-depth uniform threshold circuits.arXiv:2207.00729.Santiago Ontañón, Joshua Ainslie, Zachary Fisher, and Vaclav Cvicek.2022. Transformers를 사용하여 구성적 과제를 해결합니다.Computational Linguistics 협회의 제60회 연례 회의 진행 사항(제1권: 장문 논문).Ofir Press, Noah Smith, and Mike Lewis.2022. Train short, test long: Attention with linear biases enable input length extrapolation.The Tenth International Conference on Learning Representations에서. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna 쿤코로, 아이다 네마차데, 엘레나 그리보브스카야, 도메닉 도나토, 안젤리키 라자리두, 아서 멘쉬, 장바티스트 레피아우, 마리아 침포우켈리, 니콜라이 그리고레프, 더그 프리츠, 티볼트 소티오, 만타스 파자르스카스, 토비 폴렌, 지타오 공, 다니엘 토야마, 시프리앙 드 마송 도툼, 유지아 리, 타이펀 테르지, 블라디미르 Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving. 2021. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. arXiv:2112.11446. Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas. 2022. 일반 에이전트. Trans. Mach. Learn. Res. Ryoma Sato, Makoto Yamada, Hisashi Kashima. 2021. 무작위 피처가 그래프 신경망을 강화합니다. 2021 SIAM 국제 데이터 마이닝 컨퍼런스 논문집에서. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: 회전 위치 임베딩을 사용한 향상된 변압기. arXiv:2104.09864. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. 장거리 경기장: 효율적인 변압기의 벤치마크. 제9회 국제 학습 표현 컨퍼런스에서. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전 30. 실험 세부 사항 우리는 Delétang et al.(2023)이 제안한 실험 모음을 사용하는데, 이는 15개의 알고리즘 추론 작업으로 구성되어 있으며 Apache 2.0 라이선스에 따라 https://github.com/deepmind/ neural networks_chomsky_hierarchy에서 공개적으로 사용할 수 있습니다.작업은 고정 크기 데이터 세트로 구성되지 않지만 지속적으로 샘플링할 수 있는 학습 및 테스트 분포를 정의합니다.우리는 128의 배치 크기로 2000,000단계에 대해 모델을 학습하는데, 이는 256,000,000개(잠재적으로 고유하지 않음)의 학습 예제에 해당합니다.테스트 시간에는 {41,..., 500}의 모든 시퀀스 길이에 대해 크기가 500인 단일 배치를 평가하는데, 이는 230,000개의 테스트 예제에 해당합니다. 우리는 세 가지 학습 속도, 즉 1 × 10-4, 3 × 10-4, 5 × 10-4에 걸쳐 그래디언트 클리핑과 스윕을 적용한 Adam 옵티마이저(Kingma 및 Ba, 2015)를 사용합니다. 나아가 각 작업 및 위치 인코딩에 대해 다른 매개변수 초기화 난수 시드를 사용합니다. . = = 우리는 각각 8개 헤드의 블록 5개와 dmodel 64를 갖는 인코더 전용 Transformer 아키텍처(Vaswani et al., 2017)를 고려합니다. 이는 249,026개 매개변수(상대 및 무작위 상대 위치 인코딩의 경우 270,146개)에 해당합니다. 우리는 내부 클러스터의 단일 NVIDIA VGPU에서 모든 작업-인코딩 하이퍼 매개변수 트리플릿을 실행합니다. 결과적으로 15(작업) 13(위치 인코딩)을 사용했습니다. 3(학습률) · 10(시드) 표 1, 4, 5와 그림 4의 결과에 대해 5850 GPU 단위. 그림 2의 결과에 대해 추가로 2(위치 인코딩) · 3(학습률) · 10(시드) = 60 GPU 단위를 사용했습니다. 마지막으로 그림 3의 경우 4(최대 위치) · 3(학습률)을 사용했습니다. 10(시드) = 120 GPU 단위, 총 6030 GPU 단위가 됩니다. 표 2에 모든 실행 시간을 보고하고 있으며, 이 방법이 무시할 수 있는 계산 오버헤드를 유발한다는 것을 관찰했습니다. B 추가 결과 B.1 절제 연구. 이 섹션에서는 이 방법의 두 가지 주요 구성 요소에 대한 절제 연구를 수행합니다. (i) 최대 샘플링 위치 L, (ii) 하위 샘플링된 위치의 정렬. 우리는 다양한 최대 위치 L(1024, 2048, 4096, 8192)에 대해 무작위 상대 위치 인코딩을 훈련합니다.그림 정확도 정확도 1.0.9-0.0.0.60.시퀀스 길이(a) 역방향 문자열(DCF) 1.0.90.8.0.7-0.0.시퀀스 길이(b) 누락된 중복(CS) 그림 3: 무작위 상대 위치 인코딩에 대한 최대 위치 L에 대한 스윕.테스트 정확도(보이지 않는 시퀀스 길이에 대한 평균)는 L의 구체적인 값(상당히 작은 L 값)에 크게 영향을 받지 않아 방법의 안정성을 보여줍니다.그러나 L이 최대 훈련(N) 또는 테스트(M) 시퀀스 길이보다 훨씬 큰 경우 모델이 훈련 시간 동안 충분한 고유 인덱스를 만날 가능성이 낮기 때문에 성능이 저하될 것으로 예상됩니다. 테스트 정확도(모든 보이지 않는 시퀀스 길이에 대한 평균)가 REVERSE STRING 및 MISSING DUPLICATE 작업에서 L 값에 크게 영향을 받지 않는다는 것을 보여줍니다. 결과적으로, 우리의 방법을 적용하고자 하는 실무자는 이 매개변수를 광범위하게 조정할 필요가 없습니다(최대 평가 시퀀스 길이 M보다 크지만, 지나치게 크지 않은 한). 다음으로, 우리는 하위 샘플링된 위치를 정렬하고 정렬하지 않은 무작위 sin/cos 위치 인코딩의 성능을 조사합니다. 이 실험은 Transformer가 순서 정보 없이는 잘 수행되지 않을 것으로 예상하기 때문에 &quot;정신 건강 검사&quot;로 의도되었습니다. 표 3은 두 가지 버전의 방법에 대한 테스트 정확도(모든 보이지 않는 시퀀스 길이에 대한 평균)를 보여줍니다. 우리는 위치를 정렬하는 것이 중요하다는 것을 관찰했습니다. 왜냐하면 그것은 테스트 정확도를 평균 15.7% 증가시키기 때문입니다. 표 2: 모든 위치 인코딩 및 작업에 대한 실행 시간(시간)의 평균 및 표준 편차. 무작위(저희의) 수준 과제 없음 sin/cos PARITY CHECK+ 0.86 ± 0.REVERSE STRING 1.17 ± 0.R CYCLE NAVIGATION* 0.86 ± 0.짝수 쌍 0.86 ± 0.0.87±0.1.18 ± 0.0.87±0.0.87± 0.상대적 1.63 ± 0.2.61 ± 0.1.62 0.1.63 ± 0.ALiBi ROPE 0.87±0.1.17 ± 0.0.86 ± 0.0.86 ± 0.1.41 ± 0.2.01 ± 0.1.41 ± 0.1.41 ± 0.1.41 ± 0.학습됨 0.90±0.1.23±0.0.91 ± 0.0.91 ± 0.sin/cos 0.92 0.1.24 ± 0.0.92±0.0.92 0.스택 조작 8.090.모듈러 산술 DCF 이진 곱셈 이진 덧셈 5.48 ± 0.1.83 ±0.1.83 ± 0.8.00 ± 0.5.55 ± 0.1.83±0.1.82 ± 0.9.50±0.6.32 ± 0.2.86 ± 0.2.89 ± 0.이진 덧셈 제곱근 계산 방정식 풀기 CS 중복 문자열 모듈러 산술(단순) 누락 중복 확률 첫 번째 버킷 정렬 1.83 ± 0.1.39 ± 0.5.60±0.1.58±0.0.99 0.0.88±0.1.17±0.1.17±0.1.82±0.1.40±0.5.60±0.1.59±0.1.00±0.0.90±0.1.19±0.1.18±0.2.89± 0.2.200.6.41 ± 0.4.10 ± 0.1.740.1.81 ± 0.1.40±0.5.63 ± 0.1.58 ± 0.0.99 ± 0.1.64±0.27 0.88 ± 0.2.61±0.2.61 ± 0.1.17 ± 0.1.16 ±0.2.34 0.1.860.6.14±0.2.71±0.1.51±0.1.430.2.00±0.2.01±0.2.22±0.1.73±0.5.74±0.1.64±0.1.03±0.0.93±0.1.23± 0.1.22±0.2.22±0.1.72±0.5.78±0.1.65±0.1.05 0.0.94±0.1.24±0.1.24±0.8.07±0.5.50±0.1.84 0.1.81 0.8.87±0.6.07± 0.2.32±0.2.34±0.8.46±0.5.69±0.2.24±0.2.22±0.8.47±0.5.66±0.2.23±0.2.220.6.56 0.3.13±0.3.170.3.17 0.2.43± 0.6.69±0.4.24±0.1.87±0.1.78 0.2.74±0.2.74±0.8.55 0.5.69±0.2.24±0.2.24±0.2.24±0.1.740.5.83 0.1.67± 0.1.06±0.10.61±1.58. 6.41±0.3.21±0.3.29±0.학습됨⭑1.12±0.1.62±0.1.12±0.1.12±0.9.581.12. 5.92 ± 0.2.88 ± 0.2.90 ± 0.2.90 ± 0.2.23 0.6.01 ± 0.2.05 0.1.23 ± 0.1.15 ± 0.1.59 ± 0.1.60 ± 0.상대적 1.75 ± 0.2.75 ± 0.1.75 0.1.75 ± 0.ALiBi 0.94 ± 0.1.27 ± 0.0.94 ± 0.0.95 ± 0.ROPE 1.66 ± 0.2.42 ± 0.1.66 ± 0.1.65 ± 0.10.04 0.3.29 ± 0.2.53 ± 0.6.50 0.3.18 ± 0.1.74 ± 0.0.97 0.19 1.66 ±0.1.26 ± 0.23 2.40±0.1.25 0.23 2.40±0.표 3: 모든 테스트 길이에 걸쳐 평균화된 정확도(백분율)이며, 하위 샘플링된 위치를 정렬하고 정렬하지 않은 무작위 sin/cos 위치 인코딩에 대해 10개 시드와 3개 학습률에 대해 최대화되었습니다. 레벨 과제 짝수 쌍 R 모듈러 산술(단순) 패리티 검사+ 사이클 탐색+ 무작위 sin/cos w/o 정렬 w/ 정렬 100.25.50.20.52.52.59.59.스택 조작 50.72.문자열 반전 52.75.DCF 모듈러 산술 31.33.방정식 풀기 20.24.문자열 중복 52.중복 누락 53.52.홀수 우선 52.65.CS 이진 덧셈 50.64.이진 곱셈 49.52.제곱근 계산 50.버킷 정렬+ 23.52.100.72.최대 76.3% 특정 작업에서는. 사실, 정렬 없이는 우리의 접근 방식은 CYCLE NAVIGATION 작업을 제외한 모든 작업에서 (기준) 무작위 정확도를 이기지 못하는데, CYCLE NAVIGATION 작업은 순열 불변(즉, 위치 정보 없이도 풀 수 있음)입니다. 이는 Transformer가 위치 인코딩의 상대적 순서(정확한 값은 아님)만 알면 되고 토큰의 위치와 순서가 일치하지 않는 위치 인코딩이 제시되면 작업을 풀지 못한다는 우리의 직관을 확인해줍니다.B.2 이전 작업과의 비교 4장에서 우리는 우리 방법을 다양한 위치 인코딩과 비교했습니다: 없음, sin/cos(Vaswani 등, 2017), 상대(Dai 등, 2019), ALiBi(Press 등, 2022), ROPE(Su 등, 2021), 학습(Gehring 등, 2017), 레이블 기반(Li 및 McClelland, 2022). 여기서 우리는 이러한 실험에 대한 추가 결과와 Csordás et al. (2022)의 기하학적 주의 및 방향 인코딩과의 비교를 제공합니다. 표 1에서 10개 매개변수 초기화 시드와 세 가지 다른 학습 속도에서 최대화된 테스트 정확도를 보여주었다는 점을 기억하세요. 우리는 아키텍처가 전혀 작업을 해결할 수 있는지(평균적으로는 아님)를 조사하는 Delétang et al. (2023)의 실험 설정에 따른 최대값을 보고했습니다. 그러나 우리는 또한 가장 성능이 좋은 학습 속도에 대한 평균과 표준 편차(랜덤 시드에 대한)를 표 4에 보고합니다. 우리는 우리의 무작위 위치 인코딩이 평균적으로 원래의 대응물보다 상당히 우수한 성능을 보인다는 것을 관찰했습니다. 그림 4에서 시퀀스 길이당 테스트 정확도를 시각화합니다. 무작위 정확도 기준선을 이기지 못하는 학습된 위치 인코딩의 경우를 강조합니다(표 1 및 4 참조). 이는 최대 학습 길이 N보다 큰 위치에 해당하는 임베딩 행렬의 열이 학습 중에 학습되지 않아 완전히 무작위이기 때문입니다. 대조적으로, 학습된 인코딩의 무작위 버전은 훈련 중에 가능한 모든 임베딩 열을 고려하므로 대부분 작업에서 사소하지 않은 일반화에서 강력한 길이 일반화까지 달성합니다.마지막으로, 우리는 또한 우리의 방법을 Transformers의 체계적인 일반화 기능을 개선하기 위해 개발된 Neural Data Router(NDR)의 변형(Csordás et al., 2022)과 비교합니다.우리는 NDR 아키텍처의 가장 관련성 있는 측면, 즉 기하학적 어텐션과 방향 인코딩만 고려합니다(우리는 게이팅이나 공유 레이어를 사용하지 않습니다).표 5는 기하학적 어텐션과 방향 인코딩의 테스트 정확도를 비교합니다.표 4: 주요 실험 결과에 대한 점수(모든 테스트 길이에 대한 평균 정확도)의 평균 및 표준 편차(무작위 시드로 계산)(표 1 참조).무작위 정확도는 50%이며, CYCLE NAVIGATION, BUCKET SORT 및 모듈러 산술 작업의 경우 20%입니다. 우리는 위치 정보 없이도 풀 수 있는 순열 불변 과제를 †로 표시합니다. 굵은 글씨로 표시된 숫자는 과제당 가장 좋은 성과를 낸 과제입니다. 이러한 결과는 우리 방법의 우수성을 강조하며, 특히 상대적 위치 인코딩에 적용할 때 더욱 그렇습니다. 레벨 과제 짝수 쌍 R 모듈러 산술(단순) 패리티 검사 사이클 탐색* 없음 sin/cos 50.1±0.1 50.4±0.상대 67.6±15.ALiBi 로프 20.0±0.0 20.2±0.50.4±0.33.9±10.50.3±0.23.8±1.20.70.50.4±0.21.7±0.59.8±3.23.2±0.50.5±0.31.1±3.50.4±0.20.8±0.50.4±0.22.3±0.학습됨 50.4±0.20.1±0.50.0±0.21.01.sin/cos 상대 ALiBi 99.7±0.24.2±1.51.1±1.30.3±10.71.4±5.20.8±0.50.0±0.26.32.스택 조작 50.2±0.역스트링 52.7±0.DCF 모듈러 산술 방정식 풀기 31.00.20.10.47.3±1.50.4±0.24.3±2.20.9±0.50.1±3.54.2±1.26.1 2.21.9±0.51.08.56.3±2.28.1±3.23.6±1.49.6± 3.51.2±0.24.0±2.21.9±0.44.93.50.4±0.22.3±1.20.2±0.69.23.72.9±1.29.64.23.6±0.무작위(저희) 99.6±0.24.9±1.51.4±0.45.9±9.69.5±1.71.7±4.77.1±6.6 75.1±1.28.85.5 29.31.25.41.821.1±0.ROPE 학습⭑ 100.0 0.0 96.2±0.23.5 1.50.41.52.9±15.20.2± 0.50.6±0.31.9±8.중복 문자열 누락 중복 52.70.51.4±1.50.4±0.51.0±0.51.0±0.2 50.4±0.2 50.4±0.50.1 0.51.11.1 53.5±0.4 53.91.6 50.1±0.ODDS FIRST 52.7±0.51.3±0.51.50.5 51.1±0.50.8±0.CS 이진 덧셈 49.4±0.47.3±3.51.7±1.3 48.5±3.47.8±5.이진 곱셈 49.8± 0.48.8±1.50.2±3.5 49.9±2.제곱근 계산 50.2±0.버킷 정렬 23.7±0.50.1±0.25.6±2.51.5±0.83.46.50.5±0.29.36.49.6±0.50.3±0.23.6±3.50.5±0.48.9±0.48.7±1.50.1±0.20.7±2.69.0±2.50.4±1.62.5±2.61.2±1.51.8±0.51.9±0.99.3±0.66.2.66.1±2.67.7±1.52.7±0.28.63.30.3 ± 2.22.31.21.1±0.62.2±1.54.3±1.49.2±1.51.10.73.11.5 67.9±1.91.49.8 75.2±3.65.91.62.0 ± 1.39.1 ± 7.52.4 0.51.8±0.3 51.0±0.99.4±0.3 98.8±0.7 99.3±0.3 98.9±0.67.12.52.8±0.73.21.51.2 ± 1.62.9 1.52.7±0.57.41.59.9±1.45.76.6 51.6±0.표 5: 방향성을 가진 기하학적 주의에 대한 모든 테스트 길이에 걸쳐 평균화된 정확도(%) 인코딩. 평균 ± SD 표 최대 수준 작업 표 짝수 쌍 100. 모듈러 산술(단순) 28. 기하 100.43.R 패리티 검사 52.52. 사이클 탐색+ 73.41.100.0 ± 0.2 4.9 ± 1.5 1.4 ± 0.5 2.9 ± 15. 스택 조작 77.58.7 1.7 ± 4. 역 문자열 95.65.7 7.1 ± 6. DCF 모듈러 산술 34.36.30.3 ± 2. 방정식 풀기 28.31.25.41. 중복 문자열 75.58.7 3.1 ± 1. 누락된 중복 100.64.9 1.49. 홀수 첫 번째 69.64.65.91.CS 이진 덧셈 64.54.이진 곱셈 50.정렬 계산 53.62.01.53.6 51.8±0.54.1 52.4±0.버킷 정렬 100.78.3 99.5±0.기하 94.5±8.27.28.51.6±0.32.9±4.55.6±2.59.33.32.8±2.28.5±2.54.91.60.32.58.1±2.53.5±1.52.1±2.52.3±0.57.7±11.표 1(최대값) 및 표 4(평균)에서 가장 좋은 결과를 얻었습니다. 우리는 무작위 위치 인코딩이 기하학적 주의보다 전반적으로 더 나은 성과를 거두는 것을 관찰했습니다(평균적으로 최대 테스트 정확도가 9.7% 더 높음).하지만 모든 작업에서 그런 것은 아닙니다.특히, 기하학적 주의는 내재적인 지역성 편향이 있는 MODULAR ARITHMETIC(SIMPLE)에서 상당히 더 나은 성과를 거두었습니다.즉, 연산 기호에 더 가까운 숫자가 일반적으로 더 관련성이 높으며, 이는 기하학적 주의에서처럼 &quot;바깥쪽으로 방사&quot;하여 포착할 수 있습니다.B.3 분석 활성화 분석그림 1에서 볼 수 있듯이, 무작위 인코딩의 주요 직관은 최대 학습 길이보다 긴 시퀀스에서 평가할 때 분포 밖 활성화로 이어지지 않는다는 것입니다.그림 5의 분석에서 이 직관을 확인합니다.이 분석은 길이가 40(즉, 파란색으로 표시된 최대 학습 길이 N)과 길이가 150(즉, 주황색으로 표시된 일반화 체제)인 시퀀스에서 평가할 때 처음 두 개의 주성분에 대한 활성화의 2D 투영을 보여줍니다.동일한 변환을 사용합니다. 무작위 상대 인코딩의 활성화가 모든 계층에서 훈련 및 일반화 체제에 대해 강력하게 겹치는 반면, 표준 상대 인코딩은 계층 3 및 4에서 시퀀스 길이에 대한 분포 밖 활성화로 이어집니다.우리는 sin/cos 및 학습된 인코딩에 대해 질적으로 유사한 결과를 얻었습니다.그림 5의 결과를 계산하기 위해 REVERSE STRING 작업에서 각각 길이가 40 및 150인 30개의 시퀀스를 생성하고 상대 또는 무작위 상대 인코딩이 있는 잘 훈련된 모델에 통과시켰습니다.표시된 각 계층에 대해 시퀀스 길이 40에서 얻은 활성화에 (미백되지 않은) 2D PCA를 맞추고 동일한 변환을 사용하여 시퀀스 길이의 모든 활성화를 2차원으로 투영했습니다(계층당 30 × 40 및 30 × 150 활성화 데이터 포인트 생성).무작위 상대 인코딩(우리의 방법)은 길이가 40 및 150인 30개의 시퀀스에서 각각 1. 및 0.994의 평균 정확도를 얻습니다. 표준 상대 인코딩(베이스라인)은 시퀀스 길이 40에서 평균 정확도 1.0, 길이 150에서 0.596을 달성하는데, 이는 모델이 표준 상대 인코딩에서 일반화에 실패했음을 나타냅니다.어텐션 행렬 분석 상대 위치 인코딩으로 학습한 어텐션 행렬과 해당 randomAccuracy Accuracy Accuracy Accuracy 1.1.00.1.0.9. 0.80.80.7Accuracy 0.Accuracy 0.8. 0.70.0.6&#39; 0.60.0.0.시퀀스 길이시퀀스 길이200시퀀스 길이(a) 짝수 쌍(R) (b) 모듈러 산술(단순)(R) (c) 패리티 검사(R) 1.01.1.0.0.9. 0.€ 0.6정확도 0.80.정확도 0.80.0.0.60.0.20.0.시퀀스 길이시퀀스 길이200시퀀스 길이(d) 사이클 탐색(R) (e) 스택 조작(DCF) (f) 역스트링(DCF) 1.1.01.없음 SIN COS +상대적 0.90.80.80.6정확도 0.60.40.알리바이 회전 학습된 ONOISY SIN_COS 노이즈 상대적 ▼ 노이즈 알리바이 노이즈 회전 정확도 0.80.노이즈 학습된 0.0.0.0.시퀀스 길이시퀀스 길이시퀀스 길이(g) 모듈러 산술(DCF) (h) 방정식 풀기 (DCF) (i) 중복 문자열(CS) 1.1.0.90.90.80.7정확도 0.8. 0.7정확도 1.0.0.8. 0.70.0.0.0.0.0.시퀀스 길이시퀀스 길이(j) 누락된 중복(CS) (k) ODDS FIRST(CS) (1) 이진 덧셈(CS) 1.1.1.0. 0.0.0.0.80.7정확도 0.80.7정확도 0.0.0.0.0.0.0.시퀀스 길이 200시퀀스 길이시퀀스 길이(m) 이진 곱셈(CS) (n) 제곱근 계산(CS) (0) 버킷 정렬(CS) 그림 4: 모든 위치 인코딩에 대한 모든 작업의 성능 곡선. 점선 수직 빨간색 선은 훈련 범위로, 오른쪽의 시퀀스는 훈련 중에 보이지 않았으므로 길이 일반화를 측정한다는 것을 의미합니다.점선 왼쪽의 시퀀스는 도메인 내 일반화 성능을 시각화합니다.0-초기 임베딩 LayerLayer° 5.2.5° 0.• 길이• 길이-2.초기 임베딩 °Layer-2.5 0.0 2.LayerLayerLayer5.2.2.50.0.0-2.-2.5-(a) 상대 위치 인코딩(Dai et al., 2019). LayerLayerLayerLayerU-50(b) 무작위 상대 위치 인코딩(저희).C-5 0그림 5: REVERSE STRING 작업에서 30개 시퀀스에 대한 초기 임베딩 및 인코더 레이어 활성화의 2D PCA 투영. 영어: 훈련 길이를 넘는 시퀀스 길이(주황색으로 표시)의 경우, 표준 상대 인코딩은 최대 훈련 길이(파란색으로 표시)로 얻은 것과 비교할 때 3, 4층에 대한 분포 밖 활성화로 명확히 이어진다.반대로, 우리의 무작위 버전은 최대 훈련 길이보다 긴 시퀀스에 대해 분포 밖 활성화로 이어지지 않아 그림 1의 직관을 확인한다.REVERSE STRING 작업에 대한 무작위화된 버전.그러기 위해, 우리는 Csordás et al. (2022)을 따르고 그림 6에서 5개 층 각각에 대한 8개의 어텐션 행렬(헤드당 하나씩)에 대한 최대값을 시각화한다.우리는 길이가 40(즉, 최대 훈련 길이)인 시퀀스와 150(즉, 최대 훈련 길이보다 상당히 깁니다)인 시퀀스에 대한 어텐션 행렬을 비교한다.길이 40의 경우, 두 인코딩 모두 눈에 띄는 X 패턴을 생성하는데, 이는 문자열의 반전에 해당한다. 그러나 길이가 150인 경우 패턴은 무작위 인코딩에 대해서만 보이는 반면 원래 버전에서는 깨져서 일반화에 실패했음을 나타냅니다.LayerLayerLayerLayerLayerLayerLayerLayer(a) 길이 40(인 분포)의 시퀀스를 갖는 상대적(기준선).LayerLayerLayerLayer(b) 길이 150(아웃 분포)의 시퀀스를 갖는 상대적(기준선).LayerLayerLayerLayer(c) 길이 40(인 분포)의 시퀀스를 갖는 무작위적 상대적(저희 방법).LayerLayerLayerLayer(d) 길이 150(아웃 분포)의 시퀀스를 갖는 무작위적 상대적(저희 방법).그림 6: 길이 40(즉, 최대 학습 길이) 및 150(즉, 학습 길이 초과)의 시퀀스를 사용하여 REVERSE STRING 작업에서 상대적 및 무작위적 상대적 위치 인코딩에 대한 어텐션 행렬 분석. 우리는 레이어당 8개 헤드에 대한 최대값을 시각화하고(Csordás et al., 2022에 따름) 시퀀스의 반전에 해당하는 명확한 X 패턴을 관찰합니다. 우리의 무작위 상대 인코딩은 더 긴 시퀀스에서 그 패턴을 유지하는 반면, 표준 상대 인코딩에서는 그 패턴이 깨집니다.
