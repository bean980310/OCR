--- ABSTRACT ---
단안 깊이 추정을 위한 MiDaS v3.1¹을 출시하여 다양한 인코더 백본을 기반으로 하는 다양한 새로운 모델을 제공합니다. 이 릴리스는 컴퓨터 비전에서 변환기의 성공에 의해 동기를 부여받았으며, 현재 다양한 사전 학습된 비전 변환기가 제공됩니다. 가장 유망한 비전 변환기를 이미지 인코더로 사용하는 것이 MiDaS 아키텍처의 깊이 추정 품질과 런타임에 어떤 영향을 미치는지 살펴봅니다. 또한 이미지 분류 작업에서 비전 변환기와 비슷한 품질을 달성하는 최근의 합성곱 접근 방식도 조사에 포함됩니다. 이전 릴리스 MiDaS v3.0은 바닐라 비전 변환기 ViT만을 활용하는 반면, MiDaS v3.1은 BEIT, Swin, SwinV2, Next-ViT 및 LeViT를 기반으로 하는 추가 모델을 제공합니다. 이러한 모델은 다양한 성능-런타임 트레이드오프를 제공합니다. 최상의 모델은 깊이 추정 품질을 28% 향상시키는 반면 효율적인 모델은 높은 프레임 속도가 필요한 다운스트림 작업을 가능하게 합니다. 또한 새로운 백본을 통합하기 위한 일반적인 프로세스도 설명합니다. 1.
--- INTRODUCTION ---
단안 깊이 추정은 단일 입력 이미지 또는 카메라 뷰에서만 고밀도 깊이를 회귀시키는 작업을 말합니다. 이 문제를 해결하는 것은 생성 AI [1-3], 3D 재구성 [4-6] 및 자율 주행 [7, 8]과 같은 다운스트림 작업에 수많은 응용 프로그램이 있습니다. 그러나 단안 깊이 추정은 제약이 부족한 문제이기 때문에 단일 이미지만으로 개별 픽셀에서 깊이 정보를 추론하는 것은 특히 어렵습니다. 깊이 추정의 최근 상당한 진전은 학습 기반 방법에 기인할 수 있습니다. 특히 데이터 세트 혼합 및 스케일 및 이동 불변 손실 구성을 통해 MiDaS [9]를 사용하여 견고하고 일반화 가능한 단안 깊이 추정이 가능해졌습니다. 해당 작업의 초기 개발 이후로 더 강력한 백본 [10]과 모바일 애플리케이션을 위한 가벼운 변형을 제공하는 새로운 모델을 제공하는 MiDaS가 여러 번 릴리스되었습니다. github.com/isl-org/MiDaS 깊이 추정을 위한 많은 딥 러닝 모델이 인코더-디코더 아키텍처를 채택합니다. 영어: 과거에 사용된 합성곱 인코더 외에도 컴퓨터 비전을 위한 변환기와 함께 새로운 범주의 인코더 옵션이 등장했습니다.원래 자연어 처리[11]를 위해 개발되었고 현재 ChatGPT[12]와 같은 대규모 언어 모델의 기반이 된 변환기는 최초의 비전 변환기 VIT[13] 이후로 다양한 새로운 비전 인코더로 이어졌습니다.이러한 새로운 인코더 중 다수는 이전 합성곱 인코더의 성능을 능가했습니다.이에 영감을 받아 깊이 추정을 위한 가장 유망한 변환기 기반 인코더를 식별하여 MiDaS에 통합했습니다.합성곱 인코더를 경쟁력 있게 만들려는 시도도 있었기 때문에[14–16] 포괄적인 조사를 위해 이것들도 포함했습니다.이 논문의 초점인 최신 릴리스 MiDaS v3.1은 다양한 최첨단 백본을 갖춘 새로운 깊이 추정 모델의 대규모 컬렉션을 제공합니다. 본 논문의 목적은 이러한 백본을 MiDaS 아키텍처로 통합하는 방법을 설명하고, 사용 가능한 다양한 v3 모델을 철저히 비교 및 분석하며, MiDaS를 향후 백본과 함께 사용할 수 있는 방법에 대한 지침을 제공하는 것입니다.
--- RELATED WORK ---
단안 깊이 추정은 본질적으로 메트릭 스케일 모호성과 같은 문제에 직면한 잘못된 문제입니다. 메트릭 깊이를 직접 회귀시키는 것을 목표로 하는 학습 기반 접근 방식[17-21]은 대표적 환경(예: 실내 또는 실외 장면에 초점을 맞춤)이 있는 동질 데이터 세트에 대한 지도 학습을 사용하여 지도 네트워크가 적절한 메트릭 스케일을 학습하도록 장려하려고 했습니다. 그러나 이는 좁은 깊이 범위에 대한 과적합으로 이어지고 환경 전체에 대한 일반화 가능성을 저하시킵니다. 또는 상대적 깊이 추정(RDE) 접근 방식[9, 10, 22]은 서로에 대해 정확하지만 메트릭 의미가 없는 픽셀 단위 깊이 예측을 회귀시키는 것을 목표로 합니다. 스케일 계수와 잠재적으로 이동 계수는 알려지지 않았습니다. 메트릭 스케일을 인수 분해함으로써 이러한 RDE 접근 방식은 불일치 레이블을 통해 지도 학습할 수 있으며, 이를 통해 다양한 메트릭 깊이 스케일과 카메라 매개변수가 있는 이기종 데이터 세트의 조합에 대한 학습이 가능합니다. 이를 통해 환경 전체에 대한 모델 일반화 가능성이 향상됩니다. MiDas 모델 패밀리는 우수한 제로샷 교차 데이터세트 성능을 달성하기 위해 데이터세트를 혼합하는 유용성을 보여준 상대적 깊이 추정 공간의 핵심 작업에서 유래했습니다[9]. 깊이 예측은 시차 공간(즉, 스케일 및 시프트까지의 역 깊이)에서 수행되고, 학습은 기준 진실 레이블의 모호성을 처리하기 위해 스케일 및 시프트 불변 손실을 활용합니다. 기존 깊이 추정 데이터세트는 함께 혼합되고 3D 영화의 프레임 및 시차 레이블로 보완되어 대규모 메타 데이터세트를 형성합니다. MiDaS 릴리스가 여러 버전을 거치면서 시간이 지남에 따라 더 많은 데이터세트가 통합되었습니다. 데이터세트는 섹션 3.3에서 학습 개요의 일부로 논의됩니다. MiDaS의 네트워크 구조는 인코더가 이미지 분류 네트워크를 기반으로 하는 기존 인코더-디코더 구조를 따릅니다. 원래 MiDaS v1 및 v2.0 모델은 Xian 등의 ResNet 기반[23] 다중 스케일 아키텍처를 사용합니다.[24] EfficientNet-Lite [25] 백본을 사용하는 모바일 친화적 변형이 MiDaS v2.1의 일부로 출시되었습니다. 변압기 기반 백본은 MiDaS v3.0 [10]에서 탐구되며, 여기서 ViT [13]의 변형이 MiDaS 아키텍처에 통합되어 Dense Prediction Transformers [10]를 개발합니다. 이 보고서는 이러한 노력을 계속하여 합성곱 및 변압기 기반 백본을 MiDaS에 통합하는 방법과 이러한 새로운 인코더 백본에서 깊이 추정 성능이 어떻게 이점을 얻는지 보여줍니다. 새로운 모델은 MiDaS v3.1로 출시되었습니다. 3.
--- METHOD ---
s. 특히, 데이터 세트 혼합 및 스케일-시프트-불변 손실 구성을 통해 MiDaS를 사용한 견고하고 일반화 가능한 단안 깊이 추정이 가능해졌습니다[9]. 해당 작업의 초기 개발 이후로 더 강력한 백본[10]과 모바일 애플리케이션을 위한 가벼운 변형을 제공하는 새로운 모델을 제공하는 MiDaS가 여러 번 릴리스되었습니다. github.com/isl-org/MiDaS 깊이 추정을 위한 많은 딥 러닝 모델이 인코더-디코더 아키텍처를 채택합니다. 과거에 사용된 합성곱 인코더 외에도 컴퓨터 비전을 위한 변환기와 함께 새로운 범주의 인코더 옵션이 등장했습니다. 원래 자연어 처리[11]를 위해 개발되었으며 현재 ChatGPT[12]와 같은 대규모 언어 모델의 기반이 된 변환기는 최초의 비전 변환기 VIT[13] 이후 다양한 새로운 비전 인코더를 탄생시켰습니다. 이러한 새로운 인코더 중 다수는 이전 합성곱 인코더의 성능을 능가했습니다. 여기에서 영감을 얻어, 우리는 깊이 추정을 위한 가장 유망한 변압기 기반 인코더를 식별하여 MiDaS에 통합했습니다. 합성곱 인코더를 경쟁력 있게 만들려는 시도도 있었기 때문에 [14–16], 우리는 포괄적인 조사를 위해 이것들도 포함시켰습니다. 이 논문의 초점인 최신 릴리스 MiDaS v3.1은 다양한 최첨단 백본을 갖춘 새로운 깊이 추정 모델의 대규모 컬렉션을 제공합니다. 이 논문의 목표는 이러한 백본을 MiDaS 아키텍처에 통합하는 방법을 설명하고, 사용 가능한 다양한 v3.models에 대한 철저한 비교 및 분석을 제공하며, MiDaS를 향후 백본과 함께 사용할 수 있는 방법에 대한 지침을 제공하는 것입니다. 2. 관련 연구 단안 깊이 추정은 본질적으로 메트릭 스케일 모호성과 같은 문제에 직면한 잘못된 문제입니다. 메트릭 깊이를 직접 회귀시키는 것을 목표로 하는 학습 기반 접근 방식[17-21]은 대표적 환경(예: 실내 또는 실외 장면에 초점을 맞춤)이 있는 동질 데이터 세트에 대한 지도 학습을 사용하여 지도 네트워크가 적절한 메트릭 척도를 학습하도록 장려하려고 했습니다. 그러나 이는 좁은 깊이 범위에 대한 과적합을 초래하고 환경 전체에 걸친 일반화 가능성을 저하시킵니다. 대안으로, 상대적 깊이 추정(RDE) 접근 방식[9, 10, 22]은 서로에 대해 정확하지만 메트릭 의미가 없는 픽셀 단위 깊이 예측을 회귀시키는 것을 목표로 합니다. 척도 계수와 잠재적으로 이동 계수는 알려지지 않았습니다. 메트릭 척도를 인수 분해함으로써 이러한 RDE 접근 방식은 불일치 레이블을 통해 지도 학습할 수 있으며, 이를 통해 다양한 메트릭 깊이 척도와 카메라 매개변수가 있는 이기종 데이터 세트의 조합에 대한 학습이 가능합니다. 이를 통해 환경 전체에 걸친 향상된 모델 일반화 가능성이 가능해집니다. MiDas 모델 패밀리는 우수한 제로샷 교차 데이터세트 성능을 달성하기 위해 데이터세트를 혼합하는 유용성을 보여준 상대적 깊이 추정 공간의 핵심 작업에서 유래했습니다[9]. 깊이 예측은 시차 공간(즉, 스케일 및 시프트까지의 역 깊이)에서 수행되고, 학습은 기준 진실 레이블의 모호성을 처리하기 위해 스케일 및 시프트 불변 손실을 활용합니다. 기존 깊이 추정 데이터세트는 함께 혼합되고 3D 영화의 프레임 및 시차 레이블로 보완되어 대규모 메타 데이터세트를 형성합니다. MiDaS 릴리스가 여러 버전을 거치면서 시간이 지남에 따라 더 많은 데이터세트가 통합되었습니다. 데이터세트는 섹션 3.3에서 학습 개요의 일부로 논의됩니다. MiDaS의 네트워크 구조는 인코더가 이미지 분류 네트워크를 기반으로 하는 기존 인코더-디코더 구조를 따릅니다. 원래 MiDaS v1 및 v2.0 모델은 Xian 등의 ResNet 기반[23] 다중 스케일 아키텍처를 사용합니다.[24] EfficientNet-Lite [25] 백본을 사용하는 모바일 친화적 변형이 MiDaS v2.1의 일부로 출시되었습니다.MiDaS v3.0 [10]에서 Transformer 기반 백본을 탐색하며, 여기서 ViT [13]의 변형이 MiDaS 아키텍처에 통합되어 Dense Prediction Transformers [10]를 개발합니다.이 보고서는 이러한 노력을 이어가며, 합성곱 및 변압기 기반 모두의 새로운 백본을 MiDaS에 통합하는 방법과 이러한 새로운 인코더 백본에서 깊이 추정 성능이 어떻게 이점을 얻는지 보여줍니다.새로운 모델은 MiDaS v3.1로 출시되었습니다.3. 방법론 이 섹션에서는 먼저 MiDaS v3.1용 모델을 개발할 때 탐색하는 합성곱 및 변압기 기반 백본에 대한 자세한 개요를 제공합니다.그런 다음 이러한 인코더 백본이 MiDaS 아키텍처에 어떻게 통합되는지 설명합니다.마지막으로 학습 설정을 설명하고 향후 확장을 위해 새로운 백본을 추가하기 위한 일반적인 전략을 논의합니다. 3.1. 인코더 백본 개요 새로운 백본을 탐색하기 위한 핵심 지침은 MiDaS [9] 아키텍처의 대체 인코더의 깊이 추정 품질과 컴퓨팅 요구 사항이 일반적으로 이미지 분류인 원래 작업에서의 동작과 대략적으로 상관 관계가 있어야 한다는 것입니다.높은 품질과 낮은 컴퓨팅 요구 사항은 일반적으로 상호 배타적입니다.다운스트림 작업에 대한 두 가지 상충 관계를 모두 충족하기 위해 가장 높은 깊이 추정 품질을 제공하거나 가장 적은 리소스가 필요한 다양한 유형의 인코더를 구현하고 검증했습니다.3.1.게시된 모델 MiDaS v3.1을 출시하기 위해 높은 깊이 추정 품질이나 실시간 애플리케이션에 대한 낮은 컴퓨팅 요구 사항으로 인해 다운스트림 작업에 가장 유망해 보이는 5가지 인코더 유형을 선택했습니다.이 선택 기준은 소형 및 대형과 같이 일반적으로 인코더 유형에 사용할 수 있는 다양한 크기에도 적용됩니다. 따라서 개요는 세 부분으로 나뉩니다. MiDaS v3.1 릴리스의 일부인 새로운 백본이 있는 모델, 탐색되었지만 릴리스되지 않은 백본이 있는 모델, 완전성을 위해 일부가 MiDaS v3.1에 레거시 모델로 포함되었기 때문에 이전 MiDaS 버전의 모델도 포함합니다. MiDaS v3.1에서 릴리스된 새로운 백본부터 시작하겠습니다. 이는 모두 변환기 백본입니다. 가장 높은 깊이 추정 품질은 BEiT [26] 변환기로 달성되며, 여기서 BEIT 512-L, BEIT384-L 및 BEIT 384-B 변형을 제공합니다. 숫자는 2차 학습 해상도 512x512 및 384x384를 나타내고 문자 L과 B는 크고 기본을 나타냅니다. BEIT 변환기 아키텍처는 또한 두 가지 새로운 버전을 제공하지만 BEIT v2 [27]와 BEiT-3 [28]은 탐색하지 않았습니다. BEiT v2[27]의 경우 384x384 이상의 해상도를 가진 사전 학습된 체크포인트는 사용할 수 없었고, 224x224의 체크포인트만 사용할 수 있었습니다. BEIT-3[28]은 연구를 완료한 후 출시되었습니다. 두 번째로 높은 깊이 추정 품질을 제공하는 인코더 유형은 Swin 변환기로, Swin[29]과 SwinV2[30] 백본을 모두 갖춘 모델을 제공합니다. 높은 깊이 추정 품질을 가진 사용 가능한 변형은 Swin-L, SwinV2-L 및 SwinV2-B로, 모두 해상도가 384x384입니다. 컴퓨팅 리소스가 낮은 다운스트림 작업의 경우 해상도가 256x256이고 T는 아주 작음을 나타내는 SwinV2-T 기반 모델도 제공합니다. PyTorch Image Models 저장소[31]에서 제공하는 Swin 및 SwinV2 변환기 백본을 기반으로 하는 MiDaS v3.1 모델의 특징은 2차 추론 해상도만 사용할 수 있다는 것입니다.이것은 추론 해상도가 학습 해상도와 다를 수 있는 다른 새로 출시된 모델과 다릅니다.MiDaS v3.1에서 출시된 마지막 두 가지 인코더 유형은 Next-ViT[32]와 낮은 계산 다운스트림 작업을 위한 LeViT[33]입니다.Next-ViT의 경우 384x384 해상도의 Next-ViT-L ImageNet-1K-6M 인코더 기반 모델을 제공합니다.LeViT의 경우 224x224 해상도의 변형 LeViT-384가 있으며 Swin 변환기와 같은 2차 추론 해상도에서만 사용할 수 있습니다. LeViT 논문 [33]의 명명 규칙에 따라 변압기 모델 이름 LeViT-의 숫자 384는 학습 해상도가 아니라 LeViT 아키텍처의 첫 번째 단계에 있는 채널 수를 나타냅니다. MiDaS 모델은 모델 이름에 학습 해상도를 사용한다는 규칙을 따르므로 변압기 백본 LeViT-384를 기반으로 하는 MiDaS 모델은 LeViT224.3.1.2라고 합니다. 미공개 모델 다음으로, 최종적으로 결과 깊이 추정 모델의 경쟁력이 떨어져 거부된 MiDaS v3.1을 개발할 때 탐색한 백본에 대한 개요를 제공합니다. 이 개요에는 변압기와 합성곱 백본이 모두 포함됩니다. 변압기 백본의 경우 먼저 Next-ViT [32]로 돌아가서 Next-ViT-L ImageNet-1K도 테스트했습니다. 탐색에는 바닐라 비전 변압기의 변형인 ViT-L Hybrid도 포함됩니다. 다음 유형의 변환기는 DeiT3[34]로, 여기서 우리는 바닐라 DeiT3-L과 ImageNet-22k에서 사전 학습되고 ImageNet1K에서 미세 조정된 DeiT3L을 탐색했습니다. 이 네 가지 변환기 백본은 모두 384x384 해상도입니다. 마지막으로 덜 강력한 하드웨어를 위한 Mobile ViTv2[35]가 있는데, 여기서 우리는 가장 작은 변형인 Mobile ViTv2-0.5를 256x 해상도로 구현했고 가장 큰 변형인 Mobile ViTv2-2.0을 384x384로 구현했습니다. 후자는 ImageNet-22K에서 사전 학습되고 ImageNet-1K에서 미세 조정되었습니다. 변환기 이름의 숫자 0.5와 2.0은 Mobile ViTv 아키텍처에서 사용된 폭 배수를 나타냅니다. 우리는 ConvNext[14]와 EfficientNet[15]을 고려하는 합성곱 백본 탐색을 진행합니다. ConvNext의 경우 ImageNet-22K에서 사전 학습되고 ImageNet1K에서 미세 조정된 ConvNeXt-L과 ConvNeXt-XL이라는 두 가지 변형을 구현했습니다.EfficientNet[36]의 경우 기본 변형 EfficientNet-B0~EfficientNet-B7을 고려하지 않았지만 가장 큰 모델 EfficientNet-B7의 더 넓고 깊은 버전인 EfficientNet-L2[15]를 고려했습니다.탐색된 모든 합성곱 백본의 해상도는 384x384입니다.그러나 충분히 높은 깊이 추정 품질을 제공하는 MiDaS 모델을 생성하지 못하기 때문에 v3.1 릴리스에는 어느 것도 없습니다.3.1.3 레거시 모델 완전성을 위해 이전 MiDaS 릴리스에서 사용된 백본도 고려했습니다. MiDaS v3.0은 384x384 해상도의 바닐라 비전 변환기 [13, 37] 백본 ViTL 및 ViT-B Hybrid를 기반으로 합니다. 또한 MiDaS v2.1의 합성곱 인코더를 레거시 백본으로 포함합니다. 이는 384x384(=midas_v21_384)의 ResNeXt-101 32x8d [38] 및 256x256(=midas_v21_256_small)의 모바일 친화적 efficientnet-lite3 [36]입니다. 이 네 개의 백본은 MiDaS v3.1에 레거시 모델로 포함됩니다. 이전 백본은 포함되지 않았으며, 이는 MiDaS v2.0의 경우 384x384의 합성 모델 ResNext-101 32x8d [38] 및 MiDaS v1.0의 경우 224x224의 ResNet-50 [39]입니다.EfficientNet-Lite3의 경우 MiDaS v3.1은 또한 OpenVINO 최적화 버전(=openvino_midas_v21_small_256)을 제공합니다.3.2. MiDaS에 백본 통합 다음에서는 MiDaS v3.1에서 릴리스된 새로운 백본이 구현되는 방법에 대한 기술적 세부 정보를 제공합니다. 이것들은 BEIT512-L, BEIT 384-L, BEIT 384-B, Swin-L, SwinV2-L, SwinV2-B, SwinV2-T, Next-ViT-L ImageNet1K-6M 및 Le ViT-224 [26,29, 30, 32, 33]입니다. 구현 노력을 최소화하기 위해 가능한 한 PyTorch Image Models(=timm) 저장소 [31]를 사용합니다. 이 저장소는 백본을 쉽게 교환할 수 있는 공통 인터페이스를 제공하기 때문입니다. 다른 백본은 원하는 모델의 이름을 제공하여 모델을 만드는 timm 함수를 사용하여 호출됩니다. 유일한 예외는 timm에서 지원하지 않지만 후드 아래에서 사용하는 Next-ViT입니다. Next-ViT [32]를 외부 종속성으로 가져옵니다. 이미지의 경우 백본은 분류를 위해 훈련되었으므로 깊이 추정 기능을 본질적으로 포함하지 않습니다. MiDaS에서 사용되는 새로운 인코더 백본은 단지 특징 추출기일 뿐이며 깊이 디코더에 적절히 연결되어야 합니다. 그러나 모든 새로운 백본은 깊이 디코더에 있는 디코딩 단계와 유사한 연속적인 인코딩 단계를 통해 입력 이미지를 처리한다는 공통적인 속성을 공유합니다. 따라서 새로운 백본을 통합하는 작업은 적절한 후크를 배치하여 인코딩 및 디코딩 단계를 적절히 연결하는 것입니다. 즉, 인코더에서 계산된 텐서를 가져와 디코더의 한 단계에서 입력으로 사용할 수 있도록 합니다. 이를 위해 디코더에 맞게 이러한 텐서의 모양을 변경하는 추가 연산자가 필요할 수 있습니다. 3.2.1 BEIT BEIT 인코더 백본의 기술적 세부 사항부터 시작합니다[26]. 기존의 바닐라 비전 변환기 대신 BEiT 변환기를 MiDaS에 도입하는 것은 간단합니다. 위에서 언급한 timm 모델 생성 기능을 사용하고 VIT에 대해 MiDaS v3.0에서 이미 사용 가능한 동일한 후킹 메커니즘을 사용할 수 있기 때문입니다[13]. 우리는 BEIT 인코더에 있는 트랜스포머 블록에 대한 절대 후크 위치를 제공하여 후크를 지정합니다. ViT에 대해 선택된 후크에 따라 BEIT 512-L 및 BEIT 384-L의 절대 후크 위치 5, 11, 17, 23과 BEIT 384B의 2, 5, 8, 11을 선택합니다. 이 선택의 직관은 위치가 등거리이며 한 위치가 마지막 트랜스포머 블록에 있고 시작 부분에 갭이 있다는 것입니다. 그 외에도 인코더 백본을 연결하려면 연결된 단계에 대한 채널 선택도 필요합니다. 새로운 인코더의 모든 트랜스포머 블록에는 동일한 수의 채널이 포함되어 있는 반면 깊이 디코더는 계층 수준마다 다른 채널 번호를 갖기 때문입니다. 여기에서도 ViT에 사용 가능한 값을 따르면 BEIT512-L 및 BEIT 384-L의 스테이지당 채널 수는 256, 512, 1024, 1024이고 BEiT384-B의 경우 96, 192, 384, 768입니다. 후크 위치와 스테이지당 채널 수는 MiDaS v3.0 선택 사항에 따라 달라지며 최적이 아닐 수 있습니다. MiDaS v3.1에서 BEIT 변환기를 구현하는 것을 간단하지 않게 만드는 중요한 점이 하나 있습니다. timm에서 BEiT를 구현하면 임의의 창 크기를 허용하지만 timm 모델 생성 기능으로 만든 BEIT 인코더당 하나의 크기만 선택할 수 있습니다. 모델을 다시 만들지 않고도 다른 입력 해상도를 사용할 수 있도록 MiDaS 내부의 여러 timm 함수를 덮어써서 timm의 원래 BEiT 코드를 수정했습니다. 여기서 핵심 문제는 상대 위치 인덱스를 포함하는 변수 relative_position_indices가 해상도에 따라 달라진다는 것입니다.modification은 단일 MiDaS 실행에서 보이지 않는 해상도가 발생할 때마다 새 인덱스를 생성하여 성능에 약간의 영향을 미칠 수 있습니다.이전에 발생한 해상도의 경우 이미 계산된 인덱스가 재사용됩니다.3.2.2 Swin 마찬가지로 Swin 및 SwinV2 변환기[29,30]도 MiDaS v3.1에서 동일한 기본 구현을 공유합니다.그러나 BEIT 및 ViT와의 주요 차이점은 Swin 및 SwinV2가 계층적 인코더라는 것입니다.이는 변환기 블록의 구조를 변경합니다.BeiT 및 ViT 인코더는 출력이 항상 동일한 모양의 랭크 2의 텐서인 일련의 비전 변환기 블록을 기반으로 합니다.여기서 한 차원은 패치 수(클래스 토큰의 경우 1개 추가)를 반영하고 다른 차원은 임베딩 차원입니다. 이와 대조적으로 계층적 인코더의 경우 연속적인 계층 수준이 있으며, 각 수준에는 여러 개의 변환기 블록이 포함됩니다. 계층 수준이 하나 낮아지면 두 이미지 방향 각각의 해상도가 반으로 줄어들어 패치 수가 4만큼 늘어나고 임베딩 공간의 크기는 두 배가 됩니다. 따라서 변환기 블록의 출력 모양은 계층 수준 내에서만 일정하고 계층 간에는 일정하지 않습니다. 이 구조의 장점은 ViT 및 BEIT에서 인코더 백본의 후크된 텐서에 대한 해상도와 채널 수를 변경하여 깊이 디코더에 맞추는 데 사용되는 합성곱 및 완전 연결 계층과 같은 일부 연산자를 생략할 수 있다는 것입니다. 대신 전치 및 비평면화 연산자만 필요합니다. 계층적 구조의 결과는 계층 수준당 정확히 하나의 후크가 있어야 한다는 것입니다. 즉, 후크를 자유롭게 선택할 수 없습니다. 따라서 Swin 및 SwinV2 변환기의 후크는 계층 수준의 첫 번째 변환기 블록에 대한 상대 위치로 제공됩니다. ViT와 BEIT의 동작을 반영하기 위해 가능한 한 큰 후크 위치를 선택합니다. 여기서 마지막 변환기 블록은 항상 후크됩니다. 따라서 세 백본 Swin-L, SwinV2-L 및 SwinV2-B 모두에 대해 상대적인 후크 위치 1, 1, 17, 1을 얻습니다. 이 선택이 얼마나 합리적인지 평가하기 위해 절제를 수행하지 않았다는 점에 유의하세요. 계층 수준당 채널 수의 경우 선택할 수 없지만 백본 자체에서 제공하는 숫자인 Swin-L 및 SwinV2-L의 경우 192, 384, 768, 1536, SwinV2-B의 경우 128, 256, 512, 1024를 사용해야 합니다. 3.2.3 Next-ViT 다음 인코더 유형은 Next-ViT-L ImageNet-1K6M[32]으로, 역시 단계가 있는 계층적 변환기입니다. 각 단계는 다음 변환기 블록과 다음 합성 블록으로 구성됩니다. Swin 및 SwinVtransformers와 유사하게, 우리는 후크에 대해 계층 레벨당 마지막 블록을 선택합니다. 그러나 Next-ViT에서 블록의 구현은 순차적이므로 상대적인 후크 위치가 아닌 절대적인 후크 위치를 제공합니다. 이는 구현을 간소화하기 때문입니다. 허용되는 범위는 0-2, 3-6, 7-36, 37-39이며, 후크 위치를 2, 6, 36, 39로 선택합니다. 후크당 채널 수는 다시 인코더 백본에 의해 제공되며 이번에는 96, 256, 512, 1024입니다([32]의 표 3 참조). Swin 및 SwinV2와의 차이점은 후크된 블록의 출력 텐서가 랭크 2가 아닌 랭크 3의 텐서라는 점입니다.블록의 해상도는 정사각형 입력 해상도의 경우 96x96에서 12x12로 떨어지고 채널 수는 96에서 1024로 증가합니다.따라서 이러한 텐서의 모양을 변경하는 데 추가 연산자가 필요하지 않으며 깊이 디코더 단계에 직접 연결할 수 있습니다.정사각형이 아닌 해상도도 지원됩니다.또 다른 중요한 점은 Next-ViT의 시작 부분에 이미 384x384 해상도에서 96x96까지 인코딩의 일부를 수행하는 합성곱 스템이 있다는 것입니다.이는 예를 들어 ViT 앞의 합성곱 패칭과 비교할 수 있으며, 이 역시 해상도 감소를 일으킵니다.3.2.4 LeViT 이전 백본과의 주요 차이점은 LeViT[33]가 계층적 인코더이기는 하지만 세 가지 계층 수준만을 기반으로 한다는 것입니다. 따라서 이 백본에 대해 깊이 디코더를 세 계층 수준으로 줄입니다. 여전히 224x224 해상도의 이미지를 처리할 수 있도록 LeViT-224는 어텐션 부분 전에 추가 합성곱 스템을 사용하여 해상도를 14x14의 작은 값으로 줄입니다. 이 효과를 상쇄하기 위해 깊이 디코더에 유사한 디합성곱 디코더를 삽입합니다. 깊이 디코더는 계층적 부분과 헤드로 구성됩니다. 디합성곱 디코더는 이 두 부분 사이에 삽입됩니다. 합성곱 인코더는 각 두 블록 사이에 Hardswish 활성화 함수[40]가 있는 블록(Conv2D, BatchNorm2d)의 4배로 구성됩니다. 디합성곱 디코더의 경우 두 블록 사이에 Hardswish가 있는 두 개의 (ConvTranspose2D, BatchNorm2d) 블록을 사용하고 끝에도 사용합니다(합성곱 인코더의 경우와 같이 커널 크기 3, 스트라이드 2). 4개 대신 2개 블록만 사용하는 이유는 MiDaS에서 깊이 맵의 해상도를 깊이 디코더를 최소한으로 변경하여 입력 해상도와 동일하게 만드는 데 충분하기 때문입니다.또한 처리 단계당 채널 수도 살펴봐야 합니다.인코더 스템의 4개 블록은 3개의 RGB 채널을 16 → 32 → 64 → 128로 늘립니다.반면 깊이 디코더는 여러 개의 마찬가지로 처리 단계에서 채널 수를 줄여야 합니다.깊이 디코더의 계층적 부분에는 256개의 출력 채널이 있는데, 이는 MiDaS v3.1의 모든 백본에서 고정된 숫자이며 MiDaS v3.0에서 가져온 선택입니다.다른 백본의 경우 이 숫자는 128 → 32 → 1로 순차적으로 감소하는데, 여기서 1은 역상대 깊이를 표현하는 데 필요한 단일 채널입니다.그러나 LeViT의 경우 추가 디컨볼루션 디코더는 이미 깊이 디코더 헤드의 시작 부분에서 128 → 64로 감소합니다. 따라서 남은 채널 감소를 조정해야 하며 점진적인 감소를 위해 32 →8 → 1을 사용합니다.후크의 경우 상황은 Swin 및 SwinV2 변환기와 유사하며, 인코더 백본에 후크된 텐서는 랭크 2이므로 깊이 디코더에 맞는 모양을 얻으려면 전치 및 평탄화 해제 연산자만 필요합니다.후크 위치는 절대적이며 3, 11, 21로 선택되었습니다.3.2.5 기타 탐색했지만 공개되지 않은 다른 백본은 NextViT-L ImageNet-1K, ViT-L Hybrid, vanilla DeiT3-L, ImageNet-22k에서 사전 학습되고 ImageNet-1K에서 미세 조정된 DeiT3-L, Mobile ViTv2-0.5, Mobile ViTv22.0, ConvNeXt-L, ConvNeXt-XL 및 EfficientNetL2입니다[13-15, 32, 34, 35]. 처음 네 개의 백본에는 새로운 기능이 필요하지 않습니다. Next-ViT-L은 Next-ViT-L ImageNet1K-6M에 대해 이전에 도입한 수정 사항을 재사용합니다. ViT-L Hybrid는 MiDaS v3.0의 일부인 ViT-B Hybrid의 또 다른 변형일 뿐입니다. 두 개의 DeiT 백본은 ViT에 사용된 기능을 기반으로 합니다. 따라서 Mobile ViTv2, ConvNeXt 및 EfficientNet-L만 MiDaS 코드를 수정해야 합니다. 그러나 이러한 모든 경우에 이 수정은 사소한데, 추가 변환 연산자 없이도 깊이 디코더에 직접 연결할 수 있는 계층 수준이 항상 네 개 있기 때문입니다. Mobile ViTv2의 경우 후크를 선택할 수 있는 방법에 대한 자유로운 선택권조차 없습니다. ConvNext 및 EfficientNet-L의 경우 앞서 설명한 후킹 메커니즘과 유사하게 진행했습니다. ConvNext에 대해 선택된 상대적 후크 위치는 2, 2, 26, 2이고 허용 범위는 0-2, 0-2, 0-26, 0-2입니다. EfficientNet-L의 경우 이 선택은 10, 10, 15, 5이고 범위는 0-10, 0-10, 0-15, 0-5입니다. 3.3. 훈련 설정 우리는 동일한 것을 따릅니다.
--- EXPERIMENT ---
영어: MiDaS v3.0 [10]을 훈련하는 데 사용된 al 프로토콜은 Adam [42]과 함께 다목적 최적화 [41]를 사용하고, 인코더 백본을 업데이트하는 데는 학습률을 1e-5로 설정하고 디코더에는 1e-4로 설정합니다. 인코더는 ImageNet [43] 가중치로 초기화되는 반면, 디코더 가중치는 무작위로 초기화됩니다. 훈련 데이터 세트 믹스는 최대 12개의 데이터 세트로 구성됩니다. [9]와 유사하게, 먼저 데이터 세트 믹스의 하위 세트에서 60개의 에포크(첫 번째 훈련 단계) 동안 모델을 사전 훈련한 다음, 전체 데이터 세트에서 60개의 에포크 동안 훈련합니다(두 번째 훈련 단계). 데이터 세트 믹스 3+10. 이 믹스는 MiDaS v3.0을 훈련하는 데 사용된 믹스와 동일합니다. 사용된 10개의 데이터 세트에는 ReDWeb[24], DIML[44], Movies[9], MegaDepth[45], WSVD[46], TartanAir[47], HRWSI[48], ApolloScape[49], Blended MVS[50], IRS[51]가 포함됩니다. 3개의 데이터 세트(ReDWeb, HRWSI, Blended MVS)로 구성된 하위 세트는 전체 10개 데이터 세트에 대한 학습에 앞서 모델을 사전 학습하는 데 사용됩니다. 데이터 세트 믹스 5+12. 이 믹스는 NYUDepth v2[52] 및 KITTI[53]를 포함하여 위에서 설명한 믹스를 확장합니다. 이 두 데이터 세트는 제로 샷 테스트를 가능하게 하기 위해 이전 버전의 MiDaS에서 학습 믹스에서 제외되었습니다. 이 두 데이터 세트를 학습에 포함하기로 한 결정은 MiDaS가 메트릭 깊이 추정 파이프라인에 통합된 응용 프로그램에서 동기를 부여받았습니다. 우리는 추가적인 훈련 데이터가 그러한 애플리케이션에서 실내 및 실외 도메인에 대한 모델 일반화를 강화한다는 것을 관찰했습니다. 이 확장된 데이터 세트를 사용하는 실험에서 현재 5개의 데이터 세트(ReDWeb, HRWSI, BlendedMVS, NYU Depth v2, KITTI)로 구성된 하위 세트가 전체 12개 데이터 세트에서 훈련하기 전에 모델을 사전 훈련하는 데 사용됩니다. 3.4. 새로운 백본 사용에 대한 논의 마지막으로, 향후 확장을 위해 MiDaS 아키텍처에 새로운 백본을 추가하는 일반적인 전략을 설명합니다. 예는 섹션 3.2를 참조하십시오. 주요 단계는 다음과 같습니다. 가능하다면 PyTorch 이미지 모델 리포지토리[31] 또는 비슷한 프레임워크를 사용하여 구현 노력을 줄이기 위해 새로운 인코더 백본을 만들어야 합니다. 이 백본은 인코더 백본에서 후크 위치를 선택해야 하는 깊이 디코더에 연결되어야 합니다. 후킹에 사용된 텐서의 모양에 따라 깊이 디코더의 해당 입력에 맞게 모양을 변경하려면 일련의 연산자가 필요할 수 있습니다.백본에 처음에 합성 스템과 그 뒤에 어텐션 부분과 같이 근본적으로 다른 여러 부분이 포함된 경우 가장 쉬운 방법은 가능하면 어텐션 부분에서만 후킹을 수행하는 것입니다.깊이 디코딩 중에 적절한 해상도를 얻으려면 계층적 부분이나 헤드를 수정해야 할 수 있습니다.즉, 네트워크 내의 계층 단계 수를 변경하거나 인코더 백본의 연산자를 반전하여 디코더 헤드에 삽입하는 것을 의미할 수 있습니다(LeViT 백본을 통합할 때 한 것처럼).마지막으로 특정 네트워크 계층의 채널 수를 조정해야 할 수 있습니다.이를 위해 이전에 통합된 유사한 백본의 구조가 유용한 지침이 될 수 있습니다.4. 실험 이 섹션에서는 평가 프로토콜을 설명하고 이전 릴리스의 몇 가지 레거시 모델과 함께 MiDaS v3.1의 다양한 모델을 비교합니다. 그런 다음 MiDaS에 통합된 백본을 수정하는 실험을 수행하면서 수행된 절제 연구를 다룹니다.4.1. 평가 모델은 DIW[54], ETH3D[55], Sintel[56], KITTI[53], NYU Depth v2[52] 및 TUM[57]의 6개 데이터 세트에서 평가됩니다.각 데이터 세트에 대해 계산된 오류 유형은 원래 MiDaS 논문[9]에서 선택한 내용에 따라 제공됩니다.DIW의 경우 계산된 메트릭은 WHDR(Weighted Human Disagreement Rate)입니다.ETH3D 및 Sintel의 경우 상대 오류(REL) did/d의 평균 절대값이 사용됩니다.여기서 M은 픽셀 수, d¿는 상대 깊이, 별표(예: d)는 기준 진실을 나타냅니다. 나머지 3개 데이터 세트의 경우, max(di/d, d/di) &gt; 1.25인 불량 깊이 픽셀 ₁의 백분율이 계산됩니다.M 빠른 모델 비교를 위해 MiDaS v3.0의 가장 큰 모델 ViT-L에 대한 상대적 개선을 도입합니다. 상대적 개선은 6개 데이터 세트에 대해 평균화된 상대적 제로 샷 오류로 정의됩니다. 모든 오류를 εs로 표시하고 s = {1, ..., 6}을 데이터 세트 인덱스로 하면 개선은 다음과 같이 정의됩니다. I =Σ Ed €d, ViT-L% (1) d 여기서 €d, ViT-L384는 모델 ViT-L 384에 대한 각각의 오류입니다. 해상도의 차이가 제로 샷 오류의 비교 가능성과 따라서 개선을 제한한다는 점에 유의하십시오. 이는 이러한 양이 이미지의 픽셀에 대한 평균이며 더 높은 해상도에서 제공되는 더 많은 세부 정보의 잠재적 이점을 고려하지 않기 때문입니다. 그림 1에 프레임 속도에 대한 상대적 개선의 시각화가 나와 있습니다. 또한 학습 중 모델을 추가로 비교하기 위해 차이의 평균 제곱근 오차(RMSE) |D₁ - D2 |2]½를 사용합니다(표 3 참조). 4.2. 결과 및 분석 검증 결과 개요는 표 1, 2 및 3에 나와 있습니다. 표 1과 2는 완전히 학습된 모델, 즉 학습이 두 단계로 수행된 모델을 보여 주지만, 표 3의 모델은 첫 번째 단계 이후로는 학습되지 않았습니다(3.3절 참조). 그곳에서 관찰된 깊이 추정 품질이 추가 학습을 정당화하기에 너무 낮기 때문입니다. 이러한 모델은 학습이 완료되지 않았음에도 불구하고 허용된 백본과 삭제된 백본을 모두 보여주기 위해 제시됩니다. 이 섹션에서는 표 1의 모델, 표 2의 수평 구분 기호 위에 있는 모델, 표 3의 첫 번째와 마지막 수평 구분 기호 사이의 모델에 대해 설명합니다. 나머지 모델은 비교를 위해 포함되었거나 실험적입니다. 이에 대한 자세한 설명은 4.3절에서 찾을 수 있습니다.4.2.1 게시된 모델 탭 1에는 MiDaS v3.1의 일부로 릴리스된 모델이 포함되어 있습니다.BEIT 512-L은 정사각형 및 제한 없는 해상도 모두에 가장 적합한 모델입니다.제한 없는 해상도는 데이터 세트에 의해 정의된 종횡비를 의미합니다.BEIT512-L 모델의 품질은 표 1의 상대적 개선 I에서 확인할 수 있는데, 정사각형 해상도의 경우 36%, 높이 512의 해상도의 경우 19%, 높이가 384인 경우 28%입니다.4.1절에서 언급한 상대적 개선의 한계로 인해 다른 추론 해상도는 별도로 고려해야 합니다.MiDaS v3.1에는 이전 버전보다 더 많은 모델이 포함되어 경량 모델을 포함하여 가능한 다운스트림 작업을 더 잘 포괄합니다.이는 표의 LeViT-224와 같은 새로운 모델에서 반영됩니다. 1은 초당 73프레임(fps)의 프레임 속도로 가장 빠른 새로운 모델입니다. 속도 면에서는 90fps로 실행되는 레거시 모델 EfficientNetLite3에 이어 두 번째입니다. 4.2.2 미공개 모델 표 2의 모델은 출시된 모델에 비해 깊이 추정 품질이 낮아 출시되지 않았습니다. 이 모델 중 첫 번째 모델은 데이터 세트 구성 3+10에서 학습된 Swin-L입니다. 여기서는 표 1에 표시된 대로 구성 5+12에서 학습된 변형만 출시했습니다. 표 1과 2의 가장 오른쪽 열에서 볼 수 있듯이 데이터 세트 수가 늘어나면서 품질 측정 I가 2%에서 21%로 향상되어 상당한 도약입니다. 이 증가의 주요 원인은 구성 5+12에서 학습할 때 KITTI와 NYUDepth v2가 더 이상 제로샷 데이터 세트가 아니기 때문입니다. 이는 KITTI와 NYUDepth v의 ₁ 점수가 각각 12.15와 6.571에서 6.601과 3.343으로 감소한 반면 나머지 오류는 약간만 감소한 것에서 확인할 수 있습니다(표 1 및 2 참조).표 2의 다음 미공개 모델은 Swin-T인데, 이는 MiDaS v3.1의 일부가 아닙니다.SwinV2가 일반적으로 Swin보다 더 나은 결과를 제공하기 때문입니다.마지막으로, 1,300만 개의 매개변수를 가진 가장 작은 모델인 Mobile ViTv2-0.5를 포함하는 Mobile ViTv2 변환기 제품군도 연구했습니다.그러나 Mobile ViTv2-0.5와 Mobile ViTv2-2.0 두 변형 모두 I 값이 -300%로 관련성이 있을 만큼 품질이 낮습니다.표의 수평 구분 기호 아래에 있는 모델은 4.3절에서 설명했으므로, 표의 첫 번째와 마지막 수평 구분 기호 사이의 모델로 진행합니다. 3. 거기에 표시된 모델은 점선 구분 기호로 구분된 변압기 및 합성곱 인코더 백본이 있는 모델로 분할되었습니다. 변압기 모델부터 시작하는데, 여기서는 먼저 DeiT3-L-22K-1K와 DeiT3L이 있습니다. 이 두 모델은 높은 깊이 추정 품질을 가지고 있습니다. 예를 들어 BlendedMVS 데이터 세트의 상대 오차(REL)에 대해 0.070이며, 이는 비교를 위해 표 2에서 볼 수 있는 BEIT 384-L의 값과 같습니다. 그러나 DeiT 변압기는 BEIT 384-L의 품질을 능가하지 않으므로 첫 번째 단계 이후로는 훈련하지 않았습니다. 동일한 기준이 ViT-L Hybrid에도 적용됩니다. ViTB Hybrid는 MiDaS v3.0의 일부이기 때문입니다(표 1 참조). Next-ViTL-1K 및 Next-ViT-L-1K-6M의 경우, 탭 3에 따르면 NextVIT-L-1K-6M인 두 변형 중 더 나은 것을 MiDaS v3.1에 포함하기로 결정했습니다.마지막으로, 세 개의 합성곱 모델 ConvNeXt-XL, ConvNeXt-L 및 EfficientNet-L2도 살펴보았습니다.최고 품질의 모델을 얻기 위해 이 모델들을 살펴보았지만 BEIT 384-L을 이길 수 없었기 때문에 이 모델들은 버렸습니다.특히, EfficientNet-L은 탭 3에 따르면 각각 0.165, 0.227 및 0.219의 오차로 낮은 깊이 추정 품질을 보였습니다.4.3. 절제 연구 다음에서, 관련 구성을 더 잘 이해하는 데 도움이 되는 조사된 백본 중 일부의 실험적 수정 사항을 논의합니다.수정 사항은 탭 하단에서 찾을 수 있습니다. 2 및 3. 그 외에도 표 2의 맨 위에 있는 모델을 살펴보겠습니다. 이 모델은 해당 표의 다른 모델과 비교하기 위해 포함되었습니다. 표 3의 맨 위에 있는 4개의 참조 모델부터 시작합니다. 이러한 모델의 변형 모델도 표 1에서 사용할 수 있습니다. BEIT 384-L 및 Next-ViT-L-1K-6M의 경우, 이들은 서로 다른 학습 데이터 세트를 사용하는 모델입니다. 즉, 표에서는 3+10이고 표 1에서는 5+12입니다. Swin-L의 경우 두 표 사이에 그러한 차이가 없습니다. 그러나 표 3에서는 학습 프로세스의 분산을 근사화하기 위해 두 개의 별도 학습 실행을 포함했습니다. ViTL은 두 표에서 기본적으로 동일한 모델이지만, 표 3에 필요한 데이터를 얻기 위해 재학습이 필요했기 때문에 학습 실행은 독립적입니다. 표 3의 맨 아래에 있는 두 가지 실험적 수정 사항을 계속 진행하며, 이는 단 한 번의 학습 단계를 거쳤습니다. 첫 번째 수정은 ViT-L Reversed로 표시되며, MiDaS v3.0에서 이미 출시된 바닐라 비전 트랜스포머 백본 ViTL이지만 후크 순서가 반전되었습니다. 깊이 디코더 후크에 절대 위치 5, 11, 17, 23을 제공하는 대신 23, 17, 11, 5로 설정했습니다. 이는 ViT 인코더 제품군이 Swin 트랜스포머의 계층 구조와 같은 트랜스포머 블록과 다르지 않은 일련의 유사한 트랜스포머 블록을 기반으로 하기 때문에 가능합니다. 놀랍게도 표 3에서 볼 수 있듯이 후크를 반전해도 깊이 추정 품질에 거의 영향을 미치지 않습니다. 따라서 디코더의 네 가지 계층 수준이 인코더의 트랜스포머 블록에 순방향 또는 역방향으로 연결되어도 큰 차이가 없습니다. 두 번째 실험은 Swin-L Equidistant로, ViT-L과 유사하게 후크가 가능한 한 등거리로 선택됩니다. 여기서 Swin 변환기를 고려하므로 후크 위치는 상대적이며 0-1, 0-1, 0-17, 01로 제한됩니다(3.2절 참조). 후크 간 거리를 균일화하기 위해 Swin-L의 위치 1, 1, 17, 1을 1, 1, 9, 1로 대체합니다. 첫 번째 후크를 0으로 설정하면 거리를 훨씬 더 유사하게 만들 수 있습니다. 그러나 여기서는 첫 번째 후크 앞에 갭을 선택하는 ViT-L을 따릅니다. 표 3에서 볼 수 있듯이 수정으로 인해 수정되지 않은 모델 Swin-L과 비교했을 때 깊이 추정 품질이 약간 감소하여 해당 모델을 출시하지 않았습니다. 또한 이 변경의 중요성에 대한 대략적인 추정치를 얻기 위해 표 3에서 훈련 1과 2로 표시된 SwinL에 대한 두 개의 독립적인 훈련 실행을 실제로 포함했습니다. 보시다시피 훈련 분산은 Swin-L의 경우 다소 작아 보입니다. 표. 2는 4개의 추가 수정 사항을 보여주는데, 여기서 우리는 또한 두 번째 단계를 훈련시켰습니다. 먼저 우리는 인코더 시작 부분에서 후크 갭을 제거하여 후크가 넓어진 BEIT 384-L Wide 모델을 고려합니다. 표 1(3.2절 참조)의 BEiT384-L의 절대 후크 위치 5, 11, 17, 23 대신, 수정은 0, 7, 15, 23을 사용합니다. 표 2에서 볼 수 있듯이, 깊이 추정 품질에는 거의 영향이 없습니다. 제약 없는 해상도의 경우, 넓어진 변형의 상대적 개선 I는 17.4%이고 따라서 표 1의 원래 변형의 값 16.8%보다 약간 더 좋습니다. 정사각형 해상도의 경우 상황은 반대이며, 값은 32.7%와 33.0%입니다. 효과가 너무 작기 때문에 후크 갭을 유지하기로 결정했습니다. 표의 나머지 세 가지 수정 사항은 다음과 같습니다. 2는 BEIT 384-L 5+12+12K, BEIT 384-L 5K+12K 및 BEIT 384-L 5A+12A로 표시되며, 표 1의 NYU Depth v2의 $1 = 2.212와 비교할 때 BEIT 384-L의 제한 없는 해상도에 대한 KITTI의 큰 값 ₁ = 9.847을 다룹니다. 큰 d₁ 값의 이유는 KITTI의 학습 이미지가 해상도 1280x384로 인해 높은 종횡비를 가지기 때문입니다. 여기서 모델 리소스 제한 없는 해상도 인코더/백본 데이터 Par. FPS DIW 믹스 ↓ 个 WHDR BEIT 512-L[26] BEIT 384-L [26] 5+12 344BEIT 512-L@384 [26] 5+12 345 5.5+12 345 5.7 0.0.0.0.066 0.237 11.57* 1.862* 6.1320.067 0.255 9.847* 2.212* 7.176 16.0.068 0.218 6.283* 2.161* 6.132SwinV2-L [30] 5+12 213SwinV2-B [30] 5+12 102Swin-L [29] 5+12 213BEIT 384-B [26] 5+12 1120.0.0.290 26.60* 3.919* 9.-Next-ViT-L-1K-6M [32] 5+12 720.0.ViT-L [13] ViT-B 하이브리드 [13] SwinV2-T [30] 3+10 3443+10 1235+12 420.0.0.230 6.895* 3.479* 9.2150.089 0.270 8.461 8.318 9.9660.093 0.274 11.56 8.69 10.89 -제곱 해상도 ETH3D Sintel KITTI NYU TUM I DIW ETH3D Sintel KITTI NYU TUM I REL↓ REL↓ 1↓ 1 ↓ 1 ↓ % ↑ WHDR↓ REL↓ REL↓ 1 ↓ §1 ↓ §1 ↓ % ↑ 0.112 0.061 0.209 5.005* 1.902* 6.4650.111 0.064 0.222 5.110* 2.229* 7.453 33.0.117 0.070 0.223 6.545* 2.582* 6.8040.111 0.073 0.244 5.840* 2.929* 8.8760.110 0.079 0.240 5.976* 3.284* 8.9330.113 0.085 0.243 6.601* 3.343* 8.7500.114 0.085 0.250 8.180* 3.588* 9.2760.106 0.093 0.254 8.842* 3.442* 9.8310.112 0.091 0.286 9.173 8.557 10.160.0.111 0.287 10.13* 5.553* 13.43 -3+10 1050.0.116 0.329 16.8.71 12.51 -EfficientNet-Lite3 [36] 5+12 513+10 210.131 0.0.315 15.27* 8.642* 18.21 -0.134 0.134 0.337 29.27 13.43 14.53 -ResNeXt-101 [38] Le ViT-224 [33] 표 1. 릴리스된 모델 평가(2차 학습 단계 이후). 이 표는 MiDaS v3.1에서 릴리스된 모델에 대한 2차 학습 단계(3.3절 참조)의 검증을 보여줍니다. 학습에 사용된 데이터 세트 정의 3+10 및 5+12는 3.3절에서 찾을 수 있습니다. 모델당 필요한 리소스는 백만 단위의 매개변수 수(Par.)와 초당 프레임 수(제한 없는 해상도의 경우 가능한 경우 FPS)로 제공됩니다. 검증은 DIW [54], ETH3D [55], Sintel [56], KITTI [53], NYU Depth v2 [52] 및 TUM [57] 데이터 세트에서 수행되었으며 검증 오류는 섹션 4.1에 설명되어 있습니다. 해상도는 제한되지 않습니다. 즉, 종횡비는 데이터 세트의 이미지에 의해 제공되거나 이미지가 정사 해상도로 변환됩니다. 전반적인 모델 품질은 ViT-L에 대한 상대적 개선 I에 의해 제공됩니다(식 (1) 참조). Next-ViT-L-1K-6M 및 ResNeXt-101은 Next-ViT-L ImageNet-1K6M 및 ResNeXt-101 32x8d의 약어입니다. 접미사 @384는 모델이 추론 해상도 384x384(훈련 해상도와 다름)에서 검증되었음을 의미합니다. MiDaS v3.0 및 2.1의 레거시 모델은 기울임체로 표시되며, ResNeXt-101=midas_v21_384 및 Efficientnetlite3-midas_v21_256_small입니다. 해당 해상도를 지원하지 않는 모델로 인해 평가할 수 없는 검증 오류는 -로 표시됩니다. 다른 이유로 평가되지 않은 양은 -로 표시됩니다. 별표 *는 KITTI 및 NYU Depth v2에서 학습한 결과로 인한 0이 아닌 샷 오류를 나타냅니다. 행은 제곱 해상도에 대한 상대적 개선 값이 더 나은 모델이 맨 위에 오도록 정렬됩니다. 열당 가장 좋은 숫자는 굵게 표시되고 두 번째로 좋은 숫자는 밑줄로 표시됩니다. 모델 인코더/백본 Swin-L [30] 리소스 데이터 Par. FPS 믹스 ↓ ↑ 3+10 213Swin-T [30] 3+10 42모바일 ViTv2-0.5 [35] 5+12 13모바일 ViTv2-2.0 [35] 5+12 34BEIT 384-L 5K+12K .K 344BEIT 384-L 와이드 5+12 344BEIT 384-L 5+12+12K +12K 344BEIT 384-L A5+12A .A 344무제한 해상도 DIW ETH3D Sintel KITTI NYU TUM I WHDR↓ REL↓ REL↓ 1↓↓ 1↓ 1↓ % ↑ 정사각형 해상도 DIW ETH3D Sintel KITTI NYU TUM I WHDR↓ REL↓ REL↓ 1↓1 ↓ 1 ↓ %↑ 0.430 0.268 0.418 51.77* 45.32* 39.33 -0.509 0.263 0.422 37.67* 48.65* 40.63 -0.0.0.0.0.086 0.246 12.15 6.571 9.7450.120 0.334 15.66 12.69 14.56 -0.263 0.422 37.67* 48.65* 40.63 -0.0.433 59.94* 48.32* 41.79 -0.0.0.0.0.0.213 2.967* 2.235* 6.5700.0.0.0.068 0.247 10.73* 2.146* 7.217 17.0.065 0.216 2.967* 2.066* 7.4170.061 0.207 2.802* 1.891* 7.5330.0.0.0.0.0.064 0.217 5.631* 2.259* 7.6590.070 0.213 6.504* 2.179* 7.9460.212 5.929* 2.296* 6.7720.221 5.078* 2.216* 7.401 32.표 2. 미공개 모델 평가(2차 이후) 학습 단계). 이 표는 낮은 깊이 추정 품질로 인해 MiDaS v3.1에서 출시되지 않은 모델의 두 번째 학습 단계(3.3절 참조)에 대한 검증을 보여줍니다. 수평 구분 기호 아래의 모델은 4.3절에서 설명한 실험적 수정을 기반으로 합니다. 일반적인 표 레이아웃은 표 1과 비슷합니다. .K와 같은 추가 데이터 세트 혼합은 4.3절에서 설명합니다. 너비가 높이보다 훨씬 큽니다. 이는 해상도가 512x인 NYU Depth v2의 경우와 다르므로 종횡비가 상당히 낮습니다. 그러나 BEIT384-L에서는 해상도 1280x384가 무작위 자르기로 384x로 줄어들어 학습과 추론 간에 해상도 차이가 크게 발생합니다. 제약 없는 해상도의 경우 추론이 원래 해상도 1280x384로 수행되기 때문입니다. 수정에서 우리는 원래 해상도 1280x384에서 KITTI를 학습시켜 이러한 불일치를 제거합니다. 이런 방식으로 KITTI를 학습시킬 때마다 데이터 세트 카운터 뒤에 접미사로 문자 K를 추가합니다. 이를 통해 첫 번째 수정 BEIT 384-L 5+12+12K로 이어지며, 여기서 우리는 데이터 5+12에서 두 단계로 학습된 원래 모델 BEIT 384-L을 가져와서 두 번째 단계의 12개 데이터 세트에서 학습되었지만 이제는 원래 KITTI 해상도로 학습된 세 번째 단계를 추가합니다. 표 2에서 볼 수 있듯이 이로 인해 S₁ 값이 9.847에서 2.967로 낮아집니다. 단순화를 위해 표의 데이터 열에 있는 전체 설명 5+12+12K가 아닌 데이터 세트 변경 +12K만 제공한다는 점에 유의하세요. 1. BEIT 384-L 5K+12K의 경우, 두 개의 학습 단계만 사용 개선점---v3.1 BEIT-L 512(384x) 개선점 대 FPS v3.1 Swin2-Lv3.1 Swin-Lv3.1 Next-ViT L-v3.1 BEIT-Lv3.1 BEIT-L 512(512x)-v3.1 Swin2-Bv3.1 BEIT-B+ v3.0 DPT-Hv2.1 Largev3.0 DPT-L|||| + v2.0 LargeFPS(GPU RTX 3090) v3.1 Swin2-Tv3.1 LeViTv2.1 Small그림 1. FPS 대비 개선점. 이 플롯은 MiDaS v3.0의 가장 큰 모델 DPTL 384(=VIT-L 384)에 비해 MiDaS v3.1의 모든 모델이 초당 프레임 수와 비교하여 개선되었음을 보여줍니다. 프레임 속도는 RTX 3090 GPU에서 측정되었습니다. 거품으로 덮인 면적은 해당 모델의 매개변수 수에 비례합니다. 모델 설명에서 MiDaS 버전을 제공하는데, 이는 일부 MiDaS v3.1 모델이 이전 MiDaS 릴리스에서 이미 도입된 레거시 모델이기 때문입니다. 모델 이름의 처음 3자리 숫자는 항상 제곱 해상도인 학습 해상도를 나타냅니다. 두 BEIT 모델의 경우 추론 해상도가 학습 해상도와 다르기 때문에 모델 설명의 끝에 추론 해상도도 제공합니다. 개선은 Sec. 4.1에서 설명한 대로 6개 데이터 세트에 대해 평균화된 상대적 제로 샷 오류로 정의되며 원래 KITTI 해상도로 학습합니다. 따라서 데이터 세트를 5+12 대신 5K+12K, 즉 간단히 K로 표시합니다. 이렇게 하면 제약 없는 해상도의 KITTI ₁ 값이 변경되지 않지만 전체 모델 품질이 약간 향상됩니다. 제약 없는 해상도의 상대적 개선 I는 33%에서 35%로 증가하고 제곱 해상도의 경우 32%에서 33%로 증가합니다. 또한 다른 데이터 세트에 대한 학습 중에 학습 이미지의 원래 종횡비를 사용하도록 접근 방식을 확장하여 테스트합니다. 학습 해상도가 학습 이미지에서 일정하지 않으면 32%의 배수로 조정된 평균 해상도를 사용합니다. 그러면 ReDWeb [24]의 경우 480x448, MegaDepth [45]의 경우 480x448, WSVD [46]의 경우 384x384, HRWSI [48]의 경우 544x384가 됩니다. 결과적으로 수정된 모델은 BEIT 384-L 5A+12A이고, 여기서 문자 A는 &#39;모두&#39;를 의미하며, 각 단계의 모든 학습 데이터 세트가 이제 원래 해상도(표 2의 데이터 열에 있는 A)에 가까운 해상도를 갖는다는 것을 나타냅니다. 이 변경의 결과는 제약 없는 해상도에 대한 KITTI의 ₁ 점수가 가장 낮고 따라서 가장 좋은 값인 2.802로 떨어진다는 것입니다. 또한 상대적 개선은 I = 37%인 수정된 모델에서 가장 좋습니다. 그러나 학습 이미지의 해상도에 과적합이 있을 수 있는데, 정사각형 해상도의 경우 상대적 개선이 33%에서 29%로 떨어지고 따라서 표 1의 BEIT 512-L 모델의 36%보다 낮기 때문입니다. 따라서 BEIT 384-L 5A+12A를 출시하지 않았지만 향후 개선을 위한 한 가지 옵션을 보여줍니다. 5. 응용 프로그램 MiDaS v3.1 제품군의 일부로 출시된 모델은 환경 전반에 걸쳐 성공적인 견고성과 일반화 가능성과 함께 높은 상대적 깊이 추정 정확도를 보여줍니다.이 모델은 상대적 및 메트릭 깊이 추정을 결합하는 아키텍처[58,59], 이미지 합성을 위한 아키텍처[1, 4, 60], 텍스트-RGBD 생성을 위한 아키텍처[3,61]를 포함한 많은 응용 프로그램에 유망한 후보입니다.v3.1 BEIT L-v3.1 BEIT L-v3.1 Swin2 L-v3.1 Swin2 B-v3.1 Swin L-v3.1 BEIT L-v3.1 Next_VIT L-v3.1 BEIT B-v3.0 DPT L-v3.0 DPT H-v2.1 Largev3.1 Swin2 T-256 ☐ v2.1 SmallOpenVINO v2.1 Smallv3.1 LeViT그림 2. 백본 비교. 이 표는 왼쪽 위에 있는 예시 RGB 입력 이미지에 대한 레거시 모델을 포함한 MiDaS v3.1의 다양한 모델의 역 상대 깊이 맵을 보여줍니다. 색상이 밝을수록 역 상대 깊이가 커집니다. 즉, 표현된 객체가 카메라에 더 가깝습니다. 각 깊이 맵의 왼쪽 하단에 모델 이름이 표시됩니다. 여기에는 MiDaS 버전, 백본 이름 및 크기, 학습 해상도가 포함됩니다. 정사각형 해상도에서만 평가되는 모델은 흰색 텍스트 끝에 정사각형 기호로 표시됩니다. 맨 아래 행의 두 번째 마지막 모델은 OpenVINO 모델입니다. 메트릭 깊이 추정. 메트릭 깊이가 필요한 실제 응용 프로그램의 경우 MiDaS 모델 자체로는 깊이 출력이 스케일 및 이동까지만 정확하기 때문에 충분하지 않습니다. 최근 연구에서는 MiDaS의 깊이 출력에서 메트릭 스케일을 해결하는 두 가지 접근 방식이 나타났습니다. 단안 시각 관성 깊이 추정[59]은 시각 관성 오도미터와 함께 MiDaS와 같은 일반화 가능한 깊이 모델을 통합하여 메트릭 스케일이 있는 고밀도 깊이 추정치를 생성합니다. 제안된 파이프라인은 희소한 메트릭 깊이에 대해 비메트릭 깊이 맵의 전역 스케일 및 이동 정렬을 수행한 다음 학습 기반 고밀도 정렬을 수행합니다. 파이프라인의 모듈식 구조는 다양한 MiDaS 모델을 통합할 수 있게 하며, 이 접근 방식은 새로운 MiDaS v3.1 모델을 활용할 때 향상된 메트릭 깊이 정확도를 달성합니다. 위의 작업이 시각 및 관성 데이터의 조합에 의존하는 반면, ZoeDepth[58]는 순수하게 시각적 데이터 기반 접근 방식에서 상대적 및 메트릭 깊이 추정을 결합하려고 합니다. 플래그십 모델인 ZoeD-M12NK는 디코더에 추가된 새로 제안된 메트릭 깊이 비닝 모듈이 있는 BEITL 인코더와 MiDaS v3.1 아키텍처를 통합합니다. 훈련은 다음을 결합합니다.제곱 해상도 HRWSI 모델 RMSE↓ BEIT 384-L [26] 0.Swin-L [29] 훈련 0.0.BlendedMVS ReDWeb REL↓ RMSE↓ 0.0.0.Training0.0.0.ViT-L [13] 0.0.0.Next-ViT-L-1K-6M [32] 0.0.0.DeiT3-L-22K-1K [34] 0.0.0.ViT-L 하이브리드 [13] 0.0.0.Next-ViT-L-1K [32] 0.0.0.DeiT3-L [34] 0.0.0.ConvNeXt-XL [14] 0.0.0.ConvNeXt-L [14] 0.0.0.EfficientNet-L2 [15] 0.0.0.ViT-L 역전 0.0.Swin-L 등거리 0.0.0.0.표 3. 모델 평가(첫 번째 학습 단계 이후). 이 표는 대부분 첫 번째 학습 단계에서만 학습되었고 깊이 추정 품질이 낮아 두 번째 단계에서는 학습되지 않은 미공개 모델의 검증을 보여줍니다(3.3절 참조). 수평 구분선 위의 모델(Next-ViTL-1K-6M과 DeiT3-L-22K-1K 사이)은 다른 모델과 비교하기 위해 포함되었으며 표 1에 최소한 릴리스된 변형이 있지만 직접 릴리스되지는 않았습니다(자세한 내용은 4절 참조). Swin-L의 경우 두 가지 다른 학습 실행이 표시됩니다. 점선 구분선 위의 모델은 변압기 백본을 기반으로 하는 모델이고 점선과 점선 사이의 모델은 합성곱 모델입니다. 점선 구분선 아래의 행은 4.3절에서 설명한 대로 실험적 수정이 적용된 모델입니다. 이 표의 모든 모델은 3+데이터세트 구성에서 학습되었습니다(표 1과 2의 혼합과 대조적으로). 검증은 HRWSI[48], BlendedMVS[50] 및 ReDWeb[24] 데이터세트에서 수행됩니다. 검증에 사용된 오류는 차이의 평균 제곱근 오류(RMSE)와 상대 오류의 평균 절대값(REL)입니다(섹션 4.1 참조). DeiT3L-22K-1K는 ImageNet-22k에서 사전 학습되고 ImageNet-1K에서 미세 조정된 DeiT3-L이고 Next-ViT-L-1K는 NextViT-L ImageNet-1K의 축약형이고 Next-ViT-L-1K-6M은 Next-ViTL ImageNet-1K-6M을 의미합니다. 이탤릭체로 표시된 모델은 MiDaS v3.0에서 재학습된 레거시 모델입니다. 행은 더 나은 모델이 맨 위에 오도록 정렬되었습니다. 열당 최상의 숫자는 굵게 표시하고 두 번째로 밑줄이 그어집니다.Sec. 3.3에서 설명한 대로 5+12 데이터 세트 혼합에 대한 MiDaS 아키텍처에 대한 상대적 깊이 학습, 그 다음에는 bins 모듈의 예측 헤드에 대한 메트릭 깊이 미세 조정.광범위한 결과를 통해 ZoeDepth 모델이 MiDaS v3.1을 통한 상대적 깊이 학습의 이점을 얻어 한 번에 두 개의 메트릭 깊이 데이터 세트(NYU Depth v2 및 KITTI)에 대한 미세 조정이 가능하고 다양한 보이지 않는 메트릭 깊이 데이터 세트에 대한 전례 없는 제로 샷 일반화 성능을 달성한다는 것이 확인되었습니다.깊이 조절된 이미지 확산.MiDaS는 이미지 대 이미지 생성을 위한 모양 보존 안정적 확산 모델을 제공하기 위해 안정적 확산[1]에 통합되었습니다.MiDaS의 단안 상대적 깊이 출력은 확산 모델을 조절하여 입력 이미지에서 볼 수 있는 의미적 모양을 유지하면서 예술적 스타일이 다를 수 있는 출력 샘플을 생성하는 데 사용됩니다. 영어: Stable Diffusion v2의 일부로 출시된 depthguided 모델은 단안 깊이 추정을 위해 MiDaS v3.0의 DPT-Hybrid를 사용합니다. 따라서 MiDaS v3 모델도 유사하게 통합될 수 있으며, 깊이 추정 정확도가 개선되어 이미지 대 이미지 확산에서 구조 보존이 더욱 향상될 수 있다는 점은 매우 유망합니다.공동 이미지 및 깊이 확산.텍스트 대 이미지 확산 공간에서 진행 중인 작업은 주어진 텍스트 프롬프트에서 공동 이미지 및 깊이 데이터를 생성하는 3D용 Latent Diffusion Model(LDM3D) [61]의 개발을 촉진했습니다.RGBD 확산을 가능하게 하기 위해 LDM3D는 캡션, RGB 이미지 및 깊이 맵이 포함된 튜플 데이터 세트에서 미세 조정된 사전 학습된 Stable Diffusion 모델을 활용합니다.학습 데이터는 이미지-캡션 쌍을 제공하는 LAION-400M 데이터 세트에서 샘플링됩니다. 이미지에 해당하는 깊이 맵은 MiDaS v3.0의 DPT-Large를 사용하여 얻습니다. 감독된 미세 조정을 통해 LDM3D는 RGB 및 상대적 깊이 맵 쌍을 생성하여 텍스트 프롬프트에서 현실적이고 몰입감 있는 360도 뷰 생성이 가능합니다. MiDaS v3.models를 사용하여 LDM3D 미세 조정을 위한 깊이 데이터를 생성하면 LDM3D 깊이 출력 및 후속 장면 뷰 생성의 품질을 더욱 향상시킬 수 있습니다. 6.
--- CONCLUSION ---
새로운 릴리스 MiDaS v3.1에서 강력한 깊이 추정 모델 컬렉션을 제시합니다. 릴리스를 위해 합성곱 백본도 탐색하지만, MiDaS 아키텍처로 충분히 높은 깊이 추정 품질을 제공하는 것은 변압기 기반 백본뿐입니다. 릴리스 v3.1은 새로운 변압기 백본 BEiT, Swin, SwinV2, Next-ViT 및 LeViT를 사용한 깊이 모델로 구성되어 있으며, BEIT 및 SwinV2에 대해 여러 가지 다른 변형을 제공합니다. 512xis 해상도의 BEIT 512-L은 비정사각형 해상도의 경우 MiDaS v3.0보다 평균 28% 더 정확합니다. MiDaS의 훈련은 원래 10개 데이터 세트에서 12개로 확장되었으며, 이제 BTS 분할을 사용하는 KITTI 및 NYU Depth V2가 포함됩니다[62]. 릴리스된 모든 백본 유형에 대해 MiDaS 아키텍처에 통합되는 방법에 대한 세부 정보를 제공합니다. 또한 이 경험을 MiDaS가 향후 백본과 함께 사용될 수 있는 방법에 대한 일반 가이드로 통합합니다.참고문헌 [1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성, 2021. 1, 9,[2] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가.arXiv 사전 인쇄본 arXiv:2302.05543, 2023.[3] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson 및 Matthias Nieẞner. Text2room: 2D 텍스트-이미지 모델에서 텍스처가 있는 3D 메시 추출. arXiv 사전 인쇄본 arXiv:2303.11989, 2023. 1,[4] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도장으로 장면 표현. 2021. 1, ACM 커뮤니케이션, 65(1):99–106, [5] Shoukang Hu, Kaichen Zhou, Kaiyu Li, Longhui Yu, Lanqing Hong, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Ziwei Liu. Consistentnerf: 희소 뷰 합성을 위한 3D 일관성으로 신경 광도장 향상. arXiv 사전 인쇄본 arXiv:2305.11031, 2023.[6] Shu Chen, Junyao Li, Yang Zhang, Beiji Zou. 새로운 뷰 합성을 위한 깊이 인식 최적화를 통한 신경 광도 필드 개선. arXiv 사전 인쇄본 arXiv:2304.05218, 2023.[7] Fei Liu, Zihao Lu, Xianke Lin. 자율 주행을 위한 비전 기반 환경 인식. arXiv 사전 인쇄본 arXiv:2212.11453, 2022.[8] Michaël Fonder, Damien Ernst, Marc Van Droogenbroeck. M4depth: 보이지 않는 환경에서 자율 주행차를 위한 단안 깊이 추정. arXiv 사전 인쇄본 arXiv:2105.09847, 2021.[9] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향하여: 제로샷 교차 데이터 세트 전송을 위한 데이터 세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 거래, 44(3):1623–1637, 2020. 1, 2, 5,[10] René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. 밀도 예측을 위한 비전 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 회의록, 12179-12188페이지, 2021년 10월. 1,2,[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017.[12] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, Bao Ge. chatgpt/gpt-4 연구 요약 및 대규모 언어 모델의 미래에 대한 관점, 2023.[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변압기. 2021년 5월 37일 오스트리아에서 열린 제9회 학습 표현 국제 컨퍼런스, ICLR 2021, 가상 이벤트. OpenReview.net, 2021. 1, 2, 3, 5, 8,[14] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. 2020년대를 위한 convnet. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2022. 1, 3,5,[15] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V Le. 노이즈가 있는 학생을 통한 자체 훈련으로 이미지넷 분류가 개선됨. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1068710698페이지, 2020. 1, 3, 5,[16] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, Shuicheng Yan. Metaformer는 실제로 비전에 필요한 것입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10819-10829페이지, 2022.[17] Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka. Adabins: 적응형 빈을 사용한 깊이 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4009-4018페이지, 2021.[18] Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka. Localbins: 지역 분포를 학습하여 깊이 추정 개선. European Conference on Computer Vision, 480-496페이지. Springer, 2022.[19] Jinyoung Jun, Jae-Han Lee, Chul Lee, Chang-Su Kim. 단안 깊이 추정을 위한 깊이 맵 분해. arXiv 사전 인쇄본 arXiv:2208.10762, 2022.[20] Zhenyu Li, Xuyang Wang, Xianming Liu, Junjun Jiang. Binsformer: 단안 깊이 추정을 위한 적응형 빈 재검토. arXiv 사전 인쇄본 arXiv:2204.00987, 2022.[21] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, Ping Tan. 새로운 crfs: 단안 깊이 추정을 위한 신경 창 완전 연결 crfs. arXiv 사전 인쇄본 arXiv:2203.01502, 2022.[22] Jae-Han Lee 및 Chang-Su Kim. 상대적 깊이 맵을 사용한 추정. 단안 깊이 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9729-9738페이지, 2019.[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Jian Sun. 이미지 인식을 위한 심층 잔여 학습. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2016년 6월.[24] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li 및 Zhenbo Luo. 웹 스테레오 데이터 감독을 통한 단안 상대적 깊이 인식. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 311-320페이지, 2018. 2, 5, 9,[25] Mingxing Tan 및 Quoc V. Le. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. 2019.[26] Hangbo Bao, Li Dong 및 Furu Wei. Beit: 이미지 변환기의 BERT 사전 학습. CORR, abs/2106.08254, 2021. 2, 3, 8,[27] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye 및 Furu Wei. BEIT v2: 벡터 양자화된 시각적 토큰화기를 사용한 마스크 이미지 모델링. 2022.[28] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei. 외국어로서의 이미지: 비전 및 비전 언어 작업을 위한 BEIT 사전 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023.[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin 변환기: 이동된 창을 사용하는 계층적 비전 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 10012-10022쪽, 2021. 2, 3, 4, 8,[30] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. Swin transformer v2: 용량 및 해상도 확장. 2022년 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 11999-12009쪽, 2022. 2, 3, 4,[31] Ross Wightman. Pytorch 이미지 모델. https://github.com/rwightman/pytorch - 이미지 모델, 2019. 2, 3,[32] Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan. Next-vit: 현실적인 산업 시나리오에서 효율적인 배포를 위한 차세대 비전 변환기. arXiv 사전 인쇄본 arXiv:2207.05501, 2022. 2, 3, 4, 5, 8,[33] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve Jegou, Matthijs Douze. Levit: 더 빠른 추론을 위한 convnet의 옷을 입은 비전 변환기. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 12259-12269페이지, 2021년 10월. 2, 3, 4,[34] Hugo Touvron, Matthieu Cord 및 Hervé Jégou. Deit iii: vit의 복수. Computer Vision-ECCV 2022: 제17회 유럽 컨퍼런스, 이스라엘 텔아비브, 2022년 10월 23-27일, 회의록, XXIV부, 516-533페이지. Springer, 2022. 3, 5,[35] Sachin Mehta 및 Mohammad Rastegari. 모바일 비전 변환기를 위한 분리형 셀프 어텐션. arXiv 사전 인쇄본 arXiv:2206.02680, 2022. 3, 5,[36] Mingxing Tan 및 Quoc Le. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. ICML, 6105-6114쪽. PMLR, 2019. 3,[37] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, Lucas Beyer. vit을 어떻게 훈련시킬까요? 비전 변환기의 데이터, 증강 및 정규화. arXiv 사전 인쇄본 arXiv:2106.10270, 2021.[38] Dhruv Kumar Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten. 약한 감독 사전 훈련의 한계 탐구. ECCV, 2018. 3,[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 이미지 인식을 위한 심층 잔여 학습. CVPR, 770-778쪽, 2016.[40] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Mobilenetv3 검색. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 1314-1324쪽, 2019.[41] Ozan Sener 및 Vladlen Koltun. 다목적 최적화로서의 다중 작업 학습. 신경 정보 처리 시스템의 발전, 2018.[42] Diederik P. Kingma 및 Jimmy Ba. Adam: 확률적 최적화 방법. 3rd International Conference on Learning Representations, ICLR, 2015.[43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 248-255쪽. IEEE, 2009.[44] Youngjung Kim, Hyungjoo Jung, Dongbo Min, and Kwanghoon Sohn. Deep monocular depth estimation via integration of global and local predictions. IEEE Transactions on Image Processing, 27(8):4131-4144, 2018.[45] Zhengqi Li and Noah Snavely. Megadepth: 인터넷 사진에서 단일 시점 깊이 예측 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2041-2050쪽, 2018. 5,[46] Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang. 동적 장면에서 깊이 예측을 위한 웹 스테레오 비디오 감독. 2019년 3D 비전(3DV) 국제 컨퍼런스, 348-357쪽. IEEE, 2019. 5,[47] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, Sebastian Scherer. Tartanair: 시각적 충격의 한계를 넓히는 데이터 세트. 2020.[48] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, Zhiguo Cao. 단일 이미지 깊이 예측을 위한 구조 기반 순위 손실. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 611-620페이지, 2020. 5, 9,[49] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng 및 Ruigang Yang. 자율 주행 및 그 응용 프로그램을 위한 apolloscape 오픈 데이터 세트. IEEE 패턴 분석 및 머신 인텔리전스 저널, 42(10):2702-2719, 2020.[50] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang 및 Long Quan. Blendedmvs: 일반화된 다중 뷰 스테레오 네트워크를 위한 대규모 데이터 세트. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 1790-1799쪽, 2020. 5,[51] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, Xiaowen Chu. Irs: 시차 및 표면 법선 추정을 위한 심층 모델을 훈련하기 위한 대규모 자연스러운 실내 로봇 스테레오 데이터 세트. 2021 IEEE 멀티미디어 및 엑스포 국제 컨퍼런스(ICME), 1-6쪽, 2021.[52] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus. RGBD 이미지에서 실내 분할 및 지원 추론. Computer Vision - ECCV 2012, 746760쪽, 베를린, 하이델베르크, 2012. Springer Berlin Heidelberg. 5,6,[53] Moritz Menze 및 Andreas Geiger. 자율 주행차를 위한 객체 장면 흐름. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2015년 6월. 5, 6,[54] Weifeng Chen, Zhao Fu, Dawei Yang 및 Jia Deng. 야생에서의 단일 이미지 깊이 인식. 신경 정보 처리 시스템의 발전, 29, 2016. 6,[55] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys 및 Andreas Geiger. 고해상도 이미지와 다중 카메라 비디오를 갖춘 다중 뷰 스테레오 벤치마크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3260-3269페이지, 2017. 6,[56] Daniel J Butler, Jonas Wulff, Garrett B Stanley, Michael J Black. 광학 흐름 평가를 위한 자연스러운 오픈 소스 영화. Computer Vision-ECCV 2012: 12th European Conference on Computer Vision, 피렌체, 이탈리아, 2012년 10월 7-13일, 회의록, 6부 12, 611-625페이지. Springer, 2012. 6,[57] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers. rgb-d 슬램 시스템 평가를 위한 벤치마크. 2012 IEEE/RSJ 지능형 로봇 및 시스템 국제 컨퍼런스, 573-580페이지. IEEE, 2012. 6,[58] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, Matthias Müller. Zoedepth: 상대적 깊이와 측정적 깊이를 결합하여 제로 샷 전송, 2023. 9,[59] Wofk, Diana 및 Ranftl, René 및 Müller, Matthias 및 Koltun, Vladlen. 단안 시각-관성 깊이 추정. IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA), 2023. 9,[60] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020.[61] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal. Ldm3d: 3d를 위한 잠복 확산 모델. arXiv 사전 인쇄본 arXiv:2305.10853, 2023. 9,[62] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, Il Hong Suh. 큰 것에서 작은 것으로: 단안 깊이 추정을 위한 다중 스케일 로컬 평면 안내. arXiv 사전 인쇄본 arXiv:1907.10326, 2019.
