--- ABSTRACT ---
코드 실행은 코드의 정확한 동작을 반영하는 j 프로그래밍 언어 의미론의 기본적인 측면입니다. 그러나 코드 인텔리전스를 위한 대부분의 사전 학습된 모델은 실행 추적을 무시하고 소스 코드와 구문 구조에만 의존합니다. 이 논문에서는 사전 학습된 모델이 코드 실행을 얼마나 잘 이해하고 수행할 수 있는지 조사합니다. 우리는 Codex와 같은 기존 모델에 도전하는 대규모의 현실적인 Python 데이터 세트와 코드 실행 작업을 만드는 돌연변이 기반 데이터 증강 기술을 개발합니다. 그런 다음 의미 이해를 향상시키기 위해 코드 실행 사전 학습 및 커리큘럼 학습을 활용하는 Transformer 모델인 CodeExecutor를 제시합니다. 우리는 코드 실행에서 Code Executor를 평가하고 유망한 성능과 한계를 보여줍니다. 또한 제로샷 코드-코드 검색 및 텍스트-코드 생성과 같은 코드 인텔리전스 작업에 대한 잠재적 이점을 보여줍니다. 우리의 분석은 코드 실행을 위한 사전 학습된 모델의 학습 및 일반화 능력에 대한 통찰력을 제공합니다. 1
--- INTRODUCTION ---
사전 학습된 모델은 자연어(NL) 작업에서 놀라운 결과를 달성했으며(Radford 등, 2018; Devlin 등, 2019; Raffel 등, 2020), 프로그래밍 언어(PL) 작업을 위한 사전 학습된 모델 개발에 영감을 주었습니다(Kanade 등, 2020; Feng 등, 2020; Svyatkovskiy 등, 2020; Wang 등, 2021b; Guo 등, 2021, 2022). 이러한 모델은 소스 코드와 추상 구문 트리(AST)(Wang 등, 2021a; Guo 등, 2022) 및 데이터 흐름(Guo 등, 2021)과 같은 코드 구조를 활용하여 코드 관련 작업을 학습합니다. 이러한 구조는 유용하지만 실행 추적에 반영되는 실행 중 코드의 동적 동작을 표현하기에 충분하지 않습니다. 그림 1을 *Microsoft에서 인턴십을 하는 동안 수행한 작업으로 사용합니다. Shuai Lu와 Nan Duan은 책임 저자입니다. 예를 들어, 실행 추적은 제어 흐름과 변수의 상태 변경을 반영하여 실행 중에 코드가 어떻게 동작하는지 보여줍니다. 반면 Casalnuovo et al.(2020)에서 언급했듯이 소스 코드에는 자연적 및 형식적이라는 두 가지 정보 채널이 있습니다. 식별자 및 주석과 같은 자연적 채널(Hindle et al., 2012)은 언어 모델을 활용하여 코드 관련 작업을 이해할 수 있도록 합니다. 형식적 채널은 인터프리터와 컴파일러에서 실행을 지정하는 데 사용되며 정확한 의미를 갖습니다. 형식적 채널은 코드에 고유하며 코드를 실행 가능하게 만듭니다. 실행 추적은 코드와 자연어를 구별하는 형식적 정보 채널을 보여주고 코드 실행을 정확하게 가능하게 하기 때문에 두 번째 범주에 속합니다(Casalnuovo et al., 2020; Chakraborty et al., 2022). 이 작업에서 우리는 사전 학습된 모델에 실제 코드 실행 프로세스를 가르치는 것을 목표로 합니다. 우리는 임의의 프로그램을 실행하고 실행 추적을 예측하는 방법을 학습하는 Transformer 기반 모델인 CodeExecutor를 제안합니다. 대규모 데이터에 대한 사전 학습을 지원하기 위해 CodeNet(Puri et al., 2021)의 경쟁 프로그래밍 문제에 대한 제출물을 기반으로 돌연변이를 생성하고 Python 공식 튜토리얼에서 적용한 단일 줄 Python 변환 및 프로그램을 생성하여 Python CodeNetMut 데이터 세트를 구성합니다. 우리는 줄 순서와 실행 추적의 중간 상태를 모두 예측하는 사전 학습 작업을 설계하고 커리큘럼 학습을 적용하여 프로그램의 난이도를 점진적으로 높입니다. 우리는 CodeExecutor를 코드 실행 작업에서 평가하고 기존 모델보다 성능이 뛰어나고 유망한 기능을 보여준다는 것을 보여줍니다. 또한 모델의 성능에 대한 심층 분석을 수행하고 강점과 약점을 밝힙니다. 또한 CodeExecutor가 제로샷 코드-코드 검색 및 텍스트-코드 생성과 같은 다운스트림 작업을 개선할 수 있음을 보여주며, 이는 실행 추적을 활용할 수 있는 잠재력을 나타냅니다.h =2 W =n =for i in range(min(h, w)): n = n - max(h, w)if n &lt;= 0:print(i + 1) break (a) 소스 코드<line> 1<state> 시간:<line> 2<state> 높이: 3 ; 너비:<line> 3<state> h: 3%; w: 7 ; n:<line> 4<state> h: 3 ; w: 7 ; n 10%; i:<line> 5<state> h : 3 ; w : 7 ; n : 3; 나:<line> 6<state> h : 3 ; w : 7; n : 3; 1 :<line> 4<state> h: 3%; w: 7; n: 3; i:<line> 5<state> h : 3 %; w: 7; n: -4; i :<line> 6<state> h 3 ; w : 7 %; n -4 %; i:<output><line> 7<state> 시간<line> 8<state> h 3 %; w : 7 ; n 3 %; w : 7 ; n : (b) 실행 추적 -43 i그림 1: 코드 실행 작업에서 샘플 소스 코드와 실행 추적. 코드 인텔리전스를 향상시킵니다. 저희의 모델과 데이터 세트는 공개적으로 사용 가능합니다¹. 요약하면, 이 논문의 기여는 다음과 같습니다. • 저희는 돌연변이 기반 데이터 증강 접근 방식을 사용하여 실제 코드 실행을 위한 대규모 사전 학습 데이터 세트를 구축하는 첫 번째 시도를 제시합니다. • 저희는 코드 실행 사전 학습 작업과 커리큘럼 학습을 사용하여 실행 추적을 예측하는 방법을 학습하는 CodeExecutor라는 새로운 사전 학습된 모델을 제안합니다. • 저희는 코드 실행 작업을 위한 CodeExecutor에 대한 포괄적인 평가를 수행하여 모델의 성능에 대한 자세한 이해를 제공합니다. • CodeExecutor는 제로샷 코드-코드 검색 및 텍스트-코드 생성과 같은 코드 인텔리전스 작업을 크게 개선합니다. 2
--- RELATED WORK ---
2.1 실행 학습 이전 연구에서는 실행 학습 작업을 프로그램을 읽고 프로그램의 출력을 계산하는 문제로 구성했습니다. 이러한 작업은 순환 신경망(Zaremba 및 Sutskever, 2014), 그래프 신경망(Bieber 등, 2020; Wang 등, 2020) 및 변압기(Dehghani 등, 2019; Yan 등, 2020; Austin 등, 2021; Nye 등, 2021)와 같은 아키텍처를 활용합니다. 또 다른 관련 작업 알고리즘 귀납은 정수 덧셈 또는 다항식 평가와 같은 짧은 프로그램을 읽고 출력을 계산하는 것입니다. 알고리즘 귀납 작업(Graves 등, 2014; Kurach 등, 2016; Kaiser 및 Sutskever, 2016; Graves &#39;https://github.com/microsoft/CodeBERT/tree/master/CodeExecutor 등, 2016; Reed 및 de Freitas, 2016; Dehghani 등, 2019; Velickovic 등, 2020a,b; Nye 등, 2021)은 코드 실행 작업에서 임의의 프로그램과 비교하여 알고리즘별 직접 감독을 통해 특정 알고리즘을 대상으로 합니다. 일부 새로운 연구에서는 두 작업을 해결하기 위해 사전 학습된 모델을 사용하기도 합니다. Lu 등(2022)은 Bit XOR과 같은 간단한 알고리즘 귀납 작업을 포함하여 언어가 아닌 작업에서 GPT2(Radford 등, 2019)의 가중치의 일부를 미세 조정합니다. Austin 등. (2021)은 200만에서 1,370억 개의 매개변수에 이르는 웹 문서와 대화 데이터에서 사전 학습된 모델을 평가하고, 가장 큰 모델은 일반적으로 few-shot이든 fine-tuning이든 프로그램의 출력을 예측할 수 없음을 보여줍니다.Nye et al. (2021)은 &quot;스크래치패드&quot;를 사용하여 다단계 계산을 수행하는 중간 계산 단계를 저장하여 Austin et al. (2021)의 모델의 능력을 향상시킵니다.프로그램의 출력을 예측하고 주로 특정 알고리즘을 다루는 이전 작업과 달리, 우리는 프로그램의 전체 실행 추적을 예측하고 실제 세계의 임의의 프로그램 실행 동작을 모방하는 데 중점을 둡니다.또한 실행을 사용하여 코드 의미를 포착함으로써 우리의 작업은 코드 인텔리전스와 관련된 작업에 유익합니다.2.2 수학적 문제 해결 수학적 문제 해결은 코드 실행과 관련된 도메인입니다.최근 연구에서는 언어 모델이 수학 문제를 해결할 수 있는 능력을 보여주는데, 이를 위해서는 결정론적 답에 도달하기 위해 소프트 알고리즘을 실행하는 법을 배워야 합니다.Amini et al. (2019);Ling et al. (2017) 수학 문제를 연산 프로그램에 매핑하고 시퀀스-투-프로그램 생성에 집중합니다.Saxton 등(2019)은 덧셈, 목록 정렬, 함수 평가와 같은 플러그앤척 문제가 포함된 DeepMind Mathematics 데이터 세트를 소개합니다.Henighan 등(2020) 연산자 CRP 상수 대체 AOD 산술 연산자 삭제 AOR 산술 연산자 대체 ASR 할당 연산자 대체 BCR 중단 계속 대체 COD 조건 연산자 삭제 LCR 논리적 커넥터 대체 ROR 관계 연산자 대체 SIR 슬라이스 인덱스 제거 1회 반복 루프 역방향 반복 루프 OIL RIL ZIL 0회 반복 루프 설명 숫자 및 문자열 리터럴 변경.단항 산술 연산자 &#39;+&#39; 또는 &#39;-&#39; 삭제.산술 연산자를 다른 연산자로 대체.예: x를 x/y로 변형.확장 할당 연산자를 다른 연산자로 대체.루프 본문에서 키워드 break와 continue를 바꿉니다. * y는 삭제 단항 부정 연산자 not 또는 멤버십 연산자 not in의 부정일 수 있습니다. 논리 연산자 and를 or로 바꾸고 그 반대로도 가능합니다. 관계 연산자를 대체합니다. 예를 들어 x &lt;= y는 x &gt; y로 변형할 수 있습니다. 컬렉션[start:end:step]의 인수 하나를 삭제합니다. break 문을 추가하여 루프를 한 번만 실행합니다. reversed() 함수로 루프 반복의 방향을 변경합니다. 첫 번째 반복 중에 루프의 실현을 중단합니다. 표 1: 코드 예제를 변형하기 위해 구현한 12개의 연산자가 포함된 변형 연산자 세트입니다. DeepMind Mathematics 데이터 세트의 대부분 문제는 대형 Transformers로 간단히 해결할 수 있음을 보여줍니다. Hendrycks 등(2021)은 LATEX 및 자연어로 작성된 단계별 솔루션이 있는 경쟁 수학 문제로 구성된 MATH 데이터 세트를 소개합니다. Cobbe 등(2021)은 초등학교 수학 문제와 자연어 솔루션을 포함하여 GSM8K를 출시합니다. 최근 Zhou 등은 (2022)는 수학 문제 해결에 대한 대규모 언어 모델의 성능을 개선하기 위한 알고리즘적 프롬핑을 제안하는데, 이는 덧셈, 뺄셈, 곱셈, 패리티를 포함하는 기술을 배우는 것에서 시작한다. 코드 실행에는 덧셈, 뺄셈, 곱셈, 나눗셈, 지수, 모듈러스와 같은 계산이 포함되며, 이는 수학 문제를 푸는 것과 유사하다. 변수, 데이터 구조, 제어 흐름 및 기타 프로그래밍 개념을 관리하는 복잡성이 추가되어 코드 실행을 배우려면 수학을 배우는 것과 다른 기술과 지식이 필요하지만 일부 중복이 있다. 3 돌연변이 기반 데이터 증강 코드 실행 작업의 목표는 인터프리터로 프로그램을 실행하지 않고도 실행을 에뮬레이트하는 법을 배우는 것이다. 우리는 작업을 생성 작업으로 취급한다. 소스 코드 c가 주어지면 실행 추적 t를 생성해야 한다. 실행 추적은 두 가지 구성 요소로 구성된다. 하나는 컴퓨터가 명령문을 실행하는 순서이고, 다른 하나는 한 명령문에서 다른 명령문으로 점프할 때 변수의 상태가 어떻게 변하는지이다. 일반적으로 프로그램 내의 명령문은 순차적으로 실행되지 않습니다.특히 프로그램이 복잡한 논리와 풍부한 의미를 구현하는 실제 시나리오에서는 더욱 그렇습니다.게다가 변수는 다양한 특성과 연산을 가진 다양한 유형의 데이터 구조와 관련이 있습니다.이 작업의 복잡성과 어려움을 감안할 때 대규모 데이터 세트를 구축하고 코드 실행을 위한 대규모 언어 모델의 기능과 경계를 탐색하는 것이 매우 중요합니다.3.1 소스 코드 변형 실제 코드 실행을 위한 대규모 Python 데이터 세트를 구성하는 것은 매우 어렵습니다.GitHub 2와 같은 소프트웨어 개발 플랫폼에서 검색한 프로그램은 쉽게 사용할 수 없는 특정 외부 리소스에 의존하기 때문에 대부분 대규모로 실행할 수 없습니다.외부 리소스의 예로는 프로그램 입력, 파일 내용, 외부 모듈 및 타사 패키지가 있습니다.같은 이유로 StackOverflow 3과 같은 코딩 질문 답변 웹사이트의 게시물에서 프로그램을 수집하는 것은 실용적이지 않습니다.우리는 CodeNet 벤치마크(Puri et al., 2021)의 경쟁 프로그래밍 문제에 대한 제출물을 기반으로 Python 코드 실행 데이터 세트를 구축합니다. 각 제출물을 샌드박스 환경에서 실행하여 실행 추적을 얻고 시간 및 추적 제한을 초과하거나 런타임 오류를 발생시키는 프로그램을 필터링합니다. 실행 가능한 프로그램의 대규모 데이터 세트를 구성하기 위해 돌연변이 기반 데이터 증강 접근 방식을 제안합니다. 각 제출물에 대해 이 접근 방식은 프로그램의 일부를 수정하여 다양한 돌연변이를 생성하여 다른 실행 추적을 생성합니다. 이러한 수정 사항의 사양을 돌연변이 연산자라고 합니다. 이는 소프트웨어 엔지니어링에서 돌연변이 테스트(Hamlet, 1977; Jia 및 Harman, 2https://github.com/ https://stackoverflow.com/ 2011)에서 영감을 받았으며, 이는 프로그램에 대한 고품질 테스트 모음의 설계를 지원하는 인기 있는 기술입니다. Python 프로그램에 돌연변이 테스트 기술을 적용한 Derezińska 및 Hałas(2014)에 따라 먼저 표 1에 표시된 대로 돌연변이 연산자 세트를 제시합니다. 대부분은 강력한 유형의 범용 언어에서 사용되는 선택된 연산자에 해당하며 Python 언어에 채택되었습니다. Python 기능을 위해 설계된 연산자도 포함되어 있습니다. 여기에는 Slice Index Removal(SIR) 및 Reverse Iteration Loop(RIL)가 포함됩니다. 그런 다음 프로그램을 AST로 변환하고 노드 유형 정보를 추출하여 모든 변경 가능한 리터럴, 연산자 및 명령문의 후보 목록을 가져옵니다. 마지막으로 뮤턴트를 생성하고 실행할 수 없는 뮤턴트를 제거합니다. CodeNet Mutants(CodeNetMut)를 사용하여 사전 학습 데이터 세트를 빌드합니다. 데이터 세트 생성 프로세스에 대한 자세한 내용은 부록 A에서 확인할 수 있습니다. 3.2 데이터 세트 구성 실제 완전한 프로그램에서 모델을 학습하는 데 어려움이 있으므로 사전 학습을 위해 CodeNetMut과 함께 두 개의 더 간단한 데이터 세트를 빌드합니다. 첫 번째는 Fraser Greenlee 4에서 수집한 Python SingleLine 데이터 세트로, 약 900만 개의 단일 줄 변환 예로 구성되어 있습니다. 각 예제에는 초기 값에 지정된 여러 변수, 단일 Python 코드 줄, 해당 줄을 실행하여 생성된 새 변수 및 값 세트가 포함됩니다. 처음 두 개를 입력 코드로 결합하고 마지막 하나를 대상 추적으로 사용합니다. 데이터 세트를 다시 실행하지 않습니다. SingleLine 데이터에서 사전 학습할 때 줄별 설명 없이 마지막 코드 줄의 최종 상태만 예측하도록 모델에 요청합니다.그림 2(a)(b)는 이러한 데이터의 예를 보여줍니다.개별 코드 줄이 실제 복잡한 프로그램을 구성하므로 데이터 세트는 코드 실행에 대한 학습의 기초가 됩니다.두 번째는 Python Tutorial 데이터 세트입니다.이 데이터 세트는 공식 Python 튜토리얼 5에 나타나는 모든 실행 가능한 코드 예를 크롤링하고 필터링하여 생성됩니다.공식 튜토리얼은 Python 언어의 기본 개념과 가장 주목할만한 기능을 소개합니다.이 데이터 세트를 생성하기 위해 상수 대체 연산자(표 1의 첫 번째 행)를 적용하여 숫자 리터럴을 다양한 값으로 변경합니다.이 접근 방식으로 340만 개의 prohttps://www.kaggle.com/frasergreenlee/ python-state-changes Shttps://docs.python.org/3/tutorial 코드: 1 c =2 z =3 c += Z 추적: C101; z:(a) 코드: 1 f = &#39;ifailuhkqq&#39; 2 1 [&#39;a&#39;, &#39;i&#39;] 3 x =4 y =5 1 = list(f[x:y]) 추적: fifailuhkqq&#39;; 1 [&#39;a&#39;, &#39;1&#39;, &#39;1&#39;]; x: 2; y(b) 코드: 1 스택 [3, 866, -325] 2 stack.append(6) 3 stack.append(7) 4 stack.pop() 5 stack.pop() 6 stack.pop() 7 스택 추적:<line><state> 스택: [3, 866, -325]<line> 2<state> 스택: [3, 866, -325, 6]<line> 3<state> 스택: [3, 866, -325, 6, 7]<line> 4<state> 스택: [3, 866, -325, 6]<line> 5<state> 스택: [3, 866, -325]<line> 6<state> 스택: [3, 866]<line> 7<state> stack: [3, 866] (c) 그림 2: (a)와 (b)는 SingleLine 데이터 세트의 예입니다. (c)는 Tutorial 데이터 세트의 예입니다. 그램. 그림 2(c)는 뮤턴트의 예를 보여줍니다. Tutorial 데이터 세트는 포괄적이지 않고 모든 기능을 포괄하지는 않지만 Python의 풍미와 스타일을 잘 나타내므로 일반적으로 사용되는 코드 블록의 실행을 모델링하는 데 귀중한 감독을 제공합니다. 따라서 Python 코드 실행 데이터 세트는 SingleLine 데이터 세트, Tutorial 데이터 세트 및 CodeNetMut 데이터 세트를 포함하여 쉬운 것에서 어려운 것으로의 패러다임을 따르는 일련의 데이터 세트입니다. 4 CodeExecutor CodeExecutor는 Transformer 기반 프레임워크를 활용하여 사전 학습을 통해 코드 실행을 학습합니다. 먼저 모델 아키텍처(§4.1)를 설명한 다음 사전 학습 작업(§4.2)을 설명하고 마지막으로 커리큘럼 학습 전략(§4.3)을 설명합니다. 4.1 모델 아키텍처 이 모델은 Transformer를 기반으로 하며 UniXcoder와 동일한 아키텍처를 채택합니다(Guo et al., 2022). UniXcoder는 인코더 전용, 디코더 전용 및 인코더-디코더 모드가 있는 프로그래밍 언어에 대한 통합된 교차 모달 사전 학습된 모델입니다. 이는 동작을 제어하기 위해 접두사 어댑터가 있는 마스크 어텐션 행렬(Dong et al., 2019)을 활용합니다. 우리는 입력 앞의 접두사로 특수 토큰 [E2D]을 사용하여 인코더-디코더 방식을 취합니다. CodeExecutor는 Transformer 레이어로 구성됩니다. 각 Transformer 레이어는 구조적으로 동일하며 멀티 헤드 셀프 어텐션 풀링(Vaswani et al., 2017)과 피드 포워드 네트워크가 차례로 포함됩니다. 4.2 사전 학습 작업 우리는 코드 실행이라는 새로운 사전 학습 작업을 제안합니다. 이 작업에 대한 우리의 동기는 우리 모델이 코드를 이해하고 실행하는 능력을 향상시키는 것입니다. 언어 모델링이나 노이즈 제거 목표와 같은 기존의 사전 학습 작업에는 코드 실행이 포함되지 않으므로 이러한 작업에 대해 학습된 모델은 코드를 실행하는 능력이 제한적입니다. 코드 실행 작업에 대해 모델을 사전 학습함으로써 코드와 추적의 이중 모드 데이터에서 유용한 패턴을 학습하여 모델의 능력을 개선하는 것을 목표로 합니다. 이를 통해 모델은 보다 정확한 추적을 생성하고 코드의 동작을 이해할 수 있으며, 이는 코드 이해가 필요한 광범위한 코드 인텔리전스 애플리케이션에 매우 중요합니다. 모델은 코드의 작동 방식에 대한 지식을 바탕으로 코드의 기본 논리를 더 잘 이해하고 이러한 이해를 사용하여 이러한 작업을 더 잘 수행할 수 있습니다. 이 작업에 대해 UniXcoder 사전 학습을 계속합니다. 사전 학습 단계에서 모델은 코드를 입력으로 받고 추적을 생성하는 방법을 학습합니다. 코드를 더 잘 이해할 수 있도록 줄 번호를 나타내는 특수 토큰 [i]와 들여쓰기를 나타내는 [INDENT] [DETENT]가 코드에 삽입됩니다. 추적의 각 줄은 [LINE], [i], [STATE], V1, : 81, [DICTSEP], ..., [DICTSEP], Vk, •, Sk, [STATEEND]로 표현할 수 있습니다. 여기서 k는 변수의 개수를 나타내고 k번째 변수 vk의 상태는 sk입니다. [DICTSEP] 기호는 사전 내의 쌍을 구분하고 [STATEEND]는 상태의 끝을 나타냅니다. 이 표현을 통해 모델은 실행의 각 단계에서 변수의 상태를 학습할 수 있으며, 이는 코드의 동작을 이해하는 데 중요합니다. &quot; 4.3 커리큘럼 학습 일반화 능력을 향상시키기 위해 사전 학습 중에 커리큘럼 학습 전략을 따릅니다. 커리큘럼 학습(Bengio et al., 2009)(CL)은 쉬운 인스턴스에서 시작하여 점차적으로 더 어려운 인스턴스를 처리하는 학습 전략으로, 인간 커리큘럼의 의미 있는 학습 순서를 모방합니다. 사전 학습 프로세스에서 Python 코드 실행 데이터 세트의 학습을 간단한 인스턴스, 즉 SingleLine 데이터로 시작하는 커리큘럼에 따라 구성합니다. 먼저, 수렴할 때까지 CodeExecutor를 사전 학습하기 위해 900만 개의 SingleLine 변환을 모두 사용합니다. 균형 잡힌 데이터 세트를 얻기 위해 SinDifficulty 레벨 언어 사전 학습 # 테스트 # 쉬움 Python 8,950,7, SingleLine 튜토리얼 CodeNetMut 중간 어려움 Python 3,422,13, Python 2,838,19, Avg Code Len 3.4.8.1.11.2.1.22.3.Avg Trace Len Avg State Num 표 2: 사전 학습 데이터 세트의 통계. &quot;Avg Code Len&quot;과 &quot;Avg Trace Len&quot;은 각각 프로그램과 추적의 평균 줄 수를 나타냅니다. &quot;Avg State Num&quot;은 추적에서 줄당 도달한 최대 상태 수의 평균을 나타냅니다. 모델에서 생성하기 가장 어려운 gleLine을 찾아 Tutorial 데이터를 사전 학습 코퍼스에 추가합니다. 또한 CodeNetMut 데이터를 사전 학습 코퍼스에 추가하고 모든 예제에 수렴하도록 모델을 사전 학습합니다. 난이도를 구분하기 위해 입력 앞에 접두사 p = {[SINGLELINE], [TUTORIAL], [CODENETMUT]}을 추가하여 데이터 종류를 나타냅니다. 예를 들어 [SINGLELINE]은 SingleLine 데이터를 수신함을 의미합니다. 사전 학습 설정 및 모델 구성에 대한 자세한 내용은 부록 B에서 확인할 수 있습니다.5 실험 설정 5. 데이터 세트 섹션 3에서 설명한 대로 사전 학습 데이터 세트를 빌드합니다.표 2는 몇 가지 기본 통계를 보여줍니다.CodeNetMut 테스트 분할의 19,541개 예제는 CodeNet의 39개 보이지 않는 프로그래밍 문제에서 가져온 것이며 변형 프로세스를 거치지 않았습니다.또한 사전 학습 중에 각 데이터 세트에서 10,000개 프로그램을 검증 분할로 보류했습니다.Tutorial 및 CodeNetMut의 경우 기준 진실 추적은 전체 프로그램의 실행 결과입니다.SingleLine의 경우 인스턴스가 변수 선언과 한 줄 변환으로 구성된 간단한 프로그램이므로 모델은 한 줄 추적 형태로 제공되는 변수의 최종 상태만 예측하도록 요청받습니다.CodeNetMut의 코드와 추적의 평균 길이는 Tutorial의 약 두 배인 것을 관찰했습니다.또한 CodeNetMut에서 프로그램을 실행하려면 다양한 상태의 더 많은 변수를 관리해야 합니다.5.2 모델 코드 실행 작업에 대한 여러 모델을 평가합니다. Codex 모델 code-cushman-001은 GitHub 코드(Chen et al., 2021)에서 미세 조정된 특수 GPT 모델입니다. 우리는 few-shot learning General Line Identifier Dataset Model Output Acc. Trace Acc를 사용합니다. 정밀 리콜 Codex 36.36.SingeLine CEL-S93.93.F36.87 36.93.32 93.정밀 리콜 71.96.F69.34 70.96.86 96.CodeExecutor 94.94.94.03 94.97.97.18 97.Codex 13.CEL-S79.85.95.84.24 89.97.87.30 92.튜토리얼 CEL-S7.8.26.21.23.26.19.47 22.CodeExecutor 76.80.94.76.84.95.69.15 80.Codex 17.CodeNetMut CEL-S43.29.59.41.76 49.68.41.69 51.CodeExecutor 48.33.58.-w/o CL 45.30.60.43.42.49.67.45.29 54.49.68.41.58 51.표 3: 코드 실행 작업의 결과. Tutorial 및 CodeNetMut 데이터 세트에서 Codex는 균일한 형식으로 실행 추적을 생성할 수 없습니다. 따라서 이러한 데이터 세트에서 Codex의 출력 정확도만 보고합니다. Codex에 코드 실행 작업에 대한 세 개의 코드 및 실행 추적 쌍을 제공합니다. CodeExecutorLimited(CEL)는 코드 실행 목표로 사전 학습된 3단계 모델입니다. CEL은 모든 데이터 세트를 동시에 활용할 수 있는 Code Executor와 달리 각 단계에서 제한된 데이터에만 액세스할 수 있습니다(자세한 비교는 부록 C 참조). UniXcoder의 공개적으로 사용 가능한 체크포인트를 사용하여 초기화하고 SingleLine 데이터로 계속 학습하여 CELS1이라고 하는 모델 CodeExecutorLimited-Stage1을 생성합니다.두 번째 단계에서는 CELS1로 초기화하고 Tutorial 데이터를 사용하여 사전 학습하므로 모델 CEL-S2를 얻습니다.CEL-S2 사전 학습을 계속하여 CodeNetMut을 사용하여 세 번째 단계에서 실제 프로그램을 실행하는 용량을 개선합니다.CEL-S3은 위에서 언급한 이러한 단계 이후에 생성됩니다.커리큘럼 학습이 없는 CodeExecutor(CL이 없는 CodeExecutor)는 세 가지 데이터 세트에서 함께 학습된 단일 단계 모델입니다.5.3 평가 지표 세 가지 데이터 세트의 테스트 세트에서 코드를 실행하는 모델 기능을 테스트합니다.세 가지 관점에서 샘플링된 추적의 기능적 정확성을 측정합니다.일반적인 측면을 평가하기 위해 출력 정확도와 추적 정확도를 보고합니다.출력 정확도는 모델이 표준 출력이 있는 프로그램에 대해서만 계산된 코드 실행과 동일한 메시지를 인쇄하는지 확인합니다. 추적 정확도는 추적 라인의 상태 순서에 관계없이 모델이 코드 실행과 동일한 추적을 생성하는지 확인합니다. 각 라인의 정확성과 추적의 식별자 상태를 평가하기 위해 라인별 점수와 식별자 점수도 평가합니다. 라인 정밀도는 모델에서 생성한 추적의 모든 라인 중에서 올바르게 식별된 라인의 비율에 따라 결정됩니다. 라인 재현율은 실제 추적의 모든 라인 중에서 모델에서 예측한 올바르게 식별된 라인의 비율입니다. 마찬가지로 추적의 식별자에 대한 점수도 계산합니다. 모델 동작과 오류 모드에 대한 이해를 심화하기 위해 샘플을 검사하여 정성적 분석도 수행합니다. 테스트 세트에서 무작위로 50개의 코드-추적 쌍을 샘플링하고 최소 5년 이상의 경력을 가진 두 명의 프로그래머에게 CodeExecutor가 7가지 측면에서 프로그램을 올바르게 실행하는지 평가하도록 요청합니다. 기본 범주에는 수학 연산자, 증강 할당 연산자, 비교 연산자, 변수와 같은 Python 초보자를 위한 기본 지식이 포함됩니다. 목록, 튜플 등의 범주는 목록, 튜플, 사전, 집합 및 관련 조작 함수와 같은 일반적인 Python 데이터 구조로 구성됩니다.표 4에서 볼 수 있듯이 분류를 안내하는 핸드북과 함께 분류법을 작성합니다.각 검토자는 생성된 추적을 줄별로 검토하고 각 범주의 발생 빈도를 계산합니다.추적 줄에 여러 범주가 포함되는 경우 이러한 모든 범주를 계산합니다.오류가 발생하면 모델이 어떤 종류의 지식 범주를 잘못 파악했는지 식별합니다.마지막으로 오류 귀속의 차이를 논의하고 합의에 도달하기 위해 함께 작업합니다.6 결과 및 분석 이 섹션에서는 코드 실행 작업(§6.1)에서 CodeExecutor를 평가하고 모델 동작과 오류 모드를 이해하기 위한 심층 분석을 수행한 다음(§6.2) 두 개의 다운스트림 작업(§6.3)을 수행합니다.코드: 1 rec = [&#39;10&#39;, &#39;3&#39;, &#39;5&#39;] 2n, a, b = map(int, rec) 3 nin [a, b]nmax = min(nin) 예측:<line><state> 추천: [10, 3, 5]<line> 2<state> rec: [10, 3, 5]; n:10; a:3; b:<line> 3<state> 기록: [10, 3, 5]; n:10; 가:3; b:5; 닌: [3, 5]<line> 4<state> 기록: [10, 3, 5]; n:10; 가:3; b:5; 닌: [3, 5]; nmax:5 nmin = n min(n, (n-nin[0])+(n-nin[1])) 6 print(str(nmax) + &quot; &quot; + str(nmin))<line> 5<state> 기록: [10, 3, 5]; n:10; a:3; b:5; 닌: [3, 5]; n최대: 3; nmin:<output> 3<line> 6<state> rec: [10, 3, 5]; n:10; a:3; b:5; nin: [3, 5]; nmax:3; nmin:그림 3: CodeNetMut 테스트 분할의 예. 여기서 CodeExecutor는 불완전한 예측을 생성하며 실수는 밑줄로 강조 표시되어 있습니다. 6.1 전체 결과 SingleLine, Tutorial 및 CodeNetMut 데이터 세트에서 모델의 성능을 평가합니다. SingleLine의 결과를 표(위)에 표시합니다. CodeExecutor는 단일 줄 변환의 약 94%를 올바르게 실행할 수 있는 반면 Codex는 대부분의 경우 이를 수행하지 못합니다. CodeExecutor는 또한 CEL-S1보다 0.7% 향상되어 사전 학습 중에 어려운 프로그램을 학습하면 쉬운 예제를 더 잘 해결하는 데 도움이 됩니다. 각 SingleLine 프로그램은 항상 표준 출력 없이 단일 줄 추적을 생성하므로 출력 정확도를 보고하지 않으며 줄 정밀도/재현 점수는 추적 정확도와 같습니다. Table(medium)의 Tutorial 실험의 경우, CodeExecutor는 출력 정확도에서 Codex보다 상당히 우수한 성능을 보였습니다(76.42% 대 13.07%). CodeExecutor의 점수가 CEL-S2보다 낮은 것은 Tutorial 데이터 세트가 Tutorial 웹사이트의 몇몇 프로그램에서만 나온 돌연변이로 구성되어 다양성이 제한되기 때문에 Tutorial과 CodeNet의 코드 예제 간에 차이가 있음을 시사합니다. CEL-S3는 추적을 생성하는 데 어려움을 겪으며, 이는 마지막 학습 단계에서 Tutorial 데이터에서 습득한 대부분의 지식을 잊어버린다는 것을 나타냅니다. CodeNetMut 결과는 SingleLine 및 Tutorial 데이터 세트의 결과보다 훨씬 낮아 실제 시나리오에서 추적을 생성하는 것이 더 어렵다는 것을 보여줍니다. CodeExecutor는 거의 절반의 예제(48.06%)에 대해 올바른 출력을 생성하고, 추적의 약 1/3이 기준 진실과 정확히 일치합니다(33.38%). CodeExecutor는 코드 실행 작업에 대한 사전 학습을 통해 Codex보다 출력 성능을 절대 포인트로 30.6% 높였습니다. 또한 CodeExecutor는 CEL-S3보다 4.3%의 출력 정확도 점수와 3.9%의 추적 정확도 점수 향상을 보이는데, 이는 4.3에서 설명한 학습 전략의 효과를 나타낸다. 커리큘럼 학습을 제거한 후 출력 정확도 점수는 48.06%에서 45.93%로 떨어지고 추적 정확도 점수는 33.38%에서 30.98%로 떨어지며, 이는 기여도를 보여준다. 범주 기본 총 정답 정확도89. 내장 함수83. 리스트, 튜플 등 문자열77.52. 조건문95. 루프84. 함수 호출100. 표 4: 인간 평가 결과. CodeExecutor의 기능을 평가하기 위해 Python 프로그래밍 지식을 7가지 범주로 분류하고 이러한 범주를 처리할 때 생성된 추적이 올바른지 틀린지 수동으로 분석한다. 세 번째 범주에는 목록, 튜플, 사전 및 집합과 같은 Python 데이터 구조가 포함된다. 커리큘럼 학습. 이러한 결과는 Codex와 같은 소스 코드에서 사전 학습된 모델에 대한 코드 실행 작업이 어렵다는 것을 보여줍니다. 그러나 CodeExecutor 모델은 간단한 프로그램을 실행하는 데 높은 성능을 달성할 수 있으며 실제 프로그램에 대한 복잡한 실행 추적을 예측할 수 있습니다. 6.2 모델 성능에 대한 심층 연구 샘플(표 4)을 검토하여 모델 성능에 대한 정성적 분석을 수행한 결과 다음과 같은 결과가 나왔습니다. 부록 D에서 더 많은 예를 찾을 수 있습니다. 모델은 일반적으로 제어 흐름에 대한 기본적인 감각을 갖습니다. 조건문, 루프 및 함수 호출은 프로그램의 제어 흐름을 보여줍니다. 제어 흐름은 프로그램 코드가 실행되는 순서를 반영합니다. 프로그램을 이해하는 데 중요하며 특정 결정을 통해 코드를 제어하고 어떤 명령문을 실행해야 하고 어떤 명령문을 건너뛰어야 하는지 모니터링하기 때문에 종종 복잡합니다. 표 4에서 CodeExecutor는 고급 다중 줄 제어 흐름에 대한 기초적인 이해력을 가지고 있으며 특히 조건문과 함수 호출에 능숙합니다. 60개 조건문 중 57개와 사용자 정의 함수에 대한 모든 5개 호출이 예측됨 모델 MAP GraphCodeBERT 23.+ CodeExecutor 55.UniXcoder + CodeExecutor 71.79.표 5: 제로샷 설정에서 코드 간 검색 작업의 MAP 점수(%). 올바르게. 루프의 정확도는 84%인 반면, 잘못된 루프는 잘못된 반복 시간을 거칩니다. 그림 1(a)를 예로 들어보겠습니다. CodeExecutor는 (b)의 기준 진실과 정확히 동일한 추적을 예측합니다. 모델은 4번째 줄에서 발생한 for 루프가 여러 번 실행될 것임을 인식합니다. 두 번째 반복에서 &quot;n&quot;은 &quot;n &lt;= O&quot; 조건을 충족하여 &quot;break&quot; 문이 생성되고 루프가 종료됩니다. 이 모델은 for 루프의 코드 블록에서 잘 작동하여 제어 흐름을 이해하는 능력을 보여줍니다. 이 모델은 특히 데이터 구조와 관련된 복잡한 작업을 처리하는 데 어려움을 겪습니다. 복잡한 프로그램에는 종종 여러 범주의 프로그래밍 지식이 필요합니다. 그림 3은 목록과 문자열을 사용하는 예를 보여줍니다. &quot;a&quot;명이 I을 구독하고 &quot;b&quot;명이 II를 구독한다는 점을 감안하여 &quot;n&quot; 중 신문 I과 II를 모두 구독하는 사람의 최대 및 최소 수를 결정합니다. CodeExecutor는 5번째 줄에서 &quot;nmin&quot;을 잘못 계산하여 0을 예상했지만 2를 얻었습니다. 이 계산에는 목록에서 값을 검색하고, 더하기, 빼기를 수행하고, &quot;min&quot; 함수를 사용하는 것이 포함됩니다. 이러한 작업의 구성성으로 인해 모델이 코드를 완전히 이해하고 정확한 상태를 생성하는 것이 어렵습니다. 또한 &quot;목록&quot;에서 비교적 낮은 정확도에서 알 수 있듯이 튜플 등 (77.27%) 및 표 4의 &quot;문자열&quot;(52.63%)에서 우리는 모델이 목록 및 문자열과 같은 데이터 구조를 이해하는 데 부족하다는 것을 관찰합니다. 데이터 구조를 이해하려면 모델이 객체가 생성, 수정, 추가 또는 삭제된 후의 동작을 학습해야 합니다. 이러한 작업은 변경 가능하고 모델이 파악하기 어려울 수 있습니다. 이는 모델이 여러 작업과 데이터 구조를 포함하는 복잡한 프로그램을 처리하는 데 어려움을 겪을 수 있음을 시사합니다. 6.3 다운스트림 작업 코드 의미론을 표현하는 CodeExecutor의 효과를 확인하기 위해 두 코드 모델 Pass@1 Pass@Codex 12.45.+ CodeExecutor 17.49에 적용합니다. 표 6: 텍스트-코드 생성 작업에 대한 HumanEval 벤치마크 결과. 두 설정 모두에서 각 문제에 대해 50개의 솔루션이 평가됩니다. 인텔리전스 작업 제로샷 코드-코드 검색 작업 및 텍스트-코드 생성 작업. 제로샷 코드-코드 검색 이 작업은 Guo et al.(2022)에 의해 도입되었습니다. 중복을 피하기 위해 연관 데이터 세트와 사전 학습 코퍼스를 사용하여 CodeNet(Puri et al., 2021)에서 9,987개의 Python 함수를 수집하여 새로운 데이터 세트를 구성합니다. 각 함수는 48개의 문제 중 하나를 해결합니다. 주어진 함수 하나에서 동일한 문제를 해결하는 모든 함수를 검색합니다. 먼저 기준 모델의 마지막 숨겨진 상태의 평균 벡터를 사용하여 두 함수 간의 유사도를 계산합니다. 코드 실행이 코드 간 검색을 어떻게 용이하게 하는지 알아보기 위해 테스트 케이스를 제공하여 각 함수를 실행합니다. 그런 다음 CodeExecutor에서 생성한 실행 추적에서 추출한 프로그램 출력을 활용하여 쿼리 프로그램의 출력과 비교하여 편집 유사도에 따라 후보를 정렬합니다. 표 5에서 CodeExecutor는 GraphCodeBERT(Guo et al., 2021)에 비해 32.8포인트 이상 향상되고 UniXcoder에 비해 약 7.2포인트 향상되어 코드 실행이 코드 의미론에 대한 이해를 크게 향상시킬 수 있음을 보여줍니다. 텍스트-코드 생성 우리는 인간이 작성한 프로그래밍 문제를 포함하는 HumanEval 벤치마크(Chen et al., 2021)를 사용합니다. 우리는 먼저 Codex(code-cushman-001)를 활용하여 각 문제에 대한 200개의 솔루션을 생성합니다. 그런 다음 CodeExecutor를 사용하여 문제 설명에 예제 테스트 사례를 입력하여 각 솔루션의 출력을 예측합니다. 우리는 출력과 예상 출력 간의 편집 유사성에 따라 200개의 솔루션을 순위를 매깁니다. 마지막으로, 우리는 각 문제에 대한 처음 50개 솔루션의 정확성을 평가합니다. 다른 필터링 전략과 달리, 우리의
--- EXPERIMENT ---
al 설정 5. 데이터 세트 섹션 3에서 설명한 대로 사전 학습 데이터 세트를 빌드합니다. 표 2는 몇 가지 기본 통계를 보여줍니다. CodeNetMut 테스트 분할의 19,541개 예제는 CodeNet에서 보지 못한 39개 프로그래밍 문제에서 가져온 것이며 변형 프로세스를 거치지 않았습니다. 또한 사전 학습 중에 각 데이터 세트에서 10k 프로그램을 검증 분할로 보류했습니다. Tutorial 및 CodeNetMut의 경우 기준 진실 추적은 전체 프로그램의 실행 결과입니다. SingleLine의 경우 인스턴스가 변수 선언과 한 줄 변환으로 구성된 간단한 프로그램이기 때문에 모델은 한 줄 추적 형태로 제공되는 변수의 최종 상태만 예측하도록 요청받습니다. CodeNetMut의 코드와 추적의 평균 길이는 Tutorial의 코드와 추적의 약 두 배인 것을 관찰했습니다. 또한 CodeNetMut에서 프로그램을 실행하려면 다양한 상태의 더 많은 변수를 관리해야 합니다. 5.2 모델 코드 실행 작업에 대한 여러 모델을 평가합니다. Codex 모델 code-cushman-001은 GitHub 코드(Chen et al., 2021)에서 미세 조정된 특수 GPT 모델입니다. 우리는 few-shot learning General Line Identifier Dataset Model Output Acc. Trace Acc를 사용합니다. 정밀 리콜 Codex 36.36.SingeLine CEL-S93.93.F36.87 36.93.32 93.정밀 리콜 71.96.F69.34 70.96.86 96.CodeExecutor 94.94.94.03 94.97.97.18 97.Codex 13.CEL-S79.85.95.84.24 89.97.87.30 92.튜토리얼 CEL-S7.8.26.21.23.26.19.47 22.CodeExecutor 76.80.94.76.84.95.69.15 80.Codex 17.CodeNetMut CEL-S43.29.59.41.76 49.68.41.69 51.CodeExecutor 48.33.58.-w/o CL 45.30.60.43.42.49.67.45.29 54.49.68.41.58 51.표 3: 코드 실행 작업의 결과. Tutorial 및 CodeNetMut 데이터 세트에서 Codex는 균일한 형식으로 실행 추적을 생성할 수 없습니다. 따라서 이러한 데이터 세트에서 Codex의 출력 정확도만 보고합니다. Codex에 코드 실행 작업에 대한 세 개의 코드 및 실행 추적 쌍을 제공합니다. CodeExecutorLimited(CEL)는 코드 실행 목표로 사전 학습된 3단계 모델입니다. CEL은 모든 데이터 세트를 동시에 활용할 수 있는 Code Executor와 달리 각 단계에서 제한된 데이터에만 액세스할 수 있습니다(자세한 비교는 부록 C 참조). UniXcoder의 공개적으로 사용 가능한 체크포인트를 사용하여 초기화하고 SingleLine 데이터로 계속 학습하여 CELS1이라고 하는 모델 CodeExecutorLimited-Stage1을 생성합니다.두 번째 단계에서는 CELS1로 초기화하고 Tutorial 데이터를 사용하여 사전 학습하므로 모델 CEL-S2를 얻습니다.CEL-S2 사전 학습을 계속하여 CodeNetMut을 사용하여 세 번째 단계에서 실제 프로그램을 실행하는 용량을 개선합니다.CEL-S3은 위에서 언급한 이러한 단계 이후에 생성됩니다.커리큘럼 학습이 없는 CodeExecutor(CL이 없는 CodeExecutor)는 세 가지 데이터 세트에서 함께 학습된 단일 단계 모델입니다.5.3 평가 지표 세 가지 데이터 세트의 테스트 세트에서 코드를 실행하는 모델 기능을 테스트합니다.세 가지 관점에서 샘플링된 추적의 기능적 정확성을 측정합니다.일반적인 측면을 평가하기 위해 출력 정확도와 추적 정확도를 보고합니다.출력 정확도는 모델이 표준 출력이 있는 프로그램에 대해서만 계산된 코드 실행과 동일한 메시지를 인쇄하는지 확인합니다. 추적 정확도는 추적 라인의 상태 순서에 관계없이 모델이 코드 실행과 동일한 추적을 생성하는지 확인합니다. 각 라인의 정확성과 추적의 식별자 상태를 평가하기 위해 라인별 점수와 식별자 점수도 평가합니다. 라인 정밀도는 모델에서 생성한 추적의 모든 라인 중에서 올바르게 식별된 라인의 비율에 따라 결정됩니다. 라인 재현율은 실제 추적의 모든 라인 중에서 모델에서 예측한 올바르게 식별된 라인의 비율입니다. 마찬가지로 추적의 식별자에 대한 점수도 계산합니다. 모델 동작과 오류 모드에 대한 이해를 심화하기 위해 샘플을 검사하여 정성적 분석도 수행합니다. 테스트 세트에서 무작위로 50개의 코드-추적 쌍을 샘플링하고 최소 5년 이상의 경력을 가진 두 명의 프로그래머에게 CodeExecutor가 7가지 측면에서 프로그램을 올바르게 실행하는지 평가하도록 요청합니다. 기본 범주에는 수학 연산자, 증강 할당 연산자, 비교 연산자, 변수와 같은 Python 초보자를 위한 기본 지식이 포함됩니다. 목록, 튜플 등의 범주는 목록, 튜플, 사전, 집합 및 관련 조작 함수와 같은 일반적인 Python 데이터 구조로 구성됩니다.표 4에서 볼 수 있듯이 분류를 안내하는 핸드북과 함께 분류법을 작성합니다.각 검토자는 생성된 추적을 줄별로 검토하고 각 범주의 발생 빈도를 계산합니다.추적 줄에 여러 범주가 포함되는 경우 이러한 모든 범주를 계산합니다.오류가 발생하면 모델이 어떤 종류의 지식 범주를 잘못 이해했는지 식별합니다.마지막으로 오류 귀속의 차이를 논의하고 합의에 도달하기 위해 함께 작업합니다.6 결과 및 분석 이 섹션에서는 코드 실행 작업(§6.1)에서 CodeExecutor를 평가하고 모델 동작과 오류 모드를 이해하기 위한 심층 분석을 수행한 다음(§6.2) 두 개의 다운스트림 작업(§6.3)을 수행합니다.코드: 1 rec = [&#39;10&#39;, &#39;3&#39;, &#39;5&#39;] 2n, a, b = map(int, rec) 3 nin [a, b]nmax = min(nin) 예측:<line><state> 추천: [10, 3, 5]<line> 2<state> rec: [10, 3, 5]; n:10; a:3; b:<line> 3<state> 기록: [10, 3, 5]; n:10; 가:3; b:5; 닌: [3, 5]<line> 4<state> 기록: [10, 3, 5]; n:10; a:3; b:5; 닌: [3, 5]; nmax:5 nmin = n min(n, (n-nin[0])+(n-nin[1])) 6 print(str(nmax) + &quot; &quot; + str(nmin))<line> 5<state> 기록: [10, 3, 5]; n:10; a:3; b:5; 닌: [3, 5]; n최대: 3; nmin:<output> 3<line> 6<state> rec: [10, 3, 5]; n:10; a:3; b:5; nin: [3, 5]; nmax:3; nmin:그림 3: CodeNetMut 테스트 분할의 예로, CodeExecutor가 불완전한 예측을 생성하고 실수는 밑줄로 강조 표시되어 있습니다.6.1 전체 결과 SingleLine, Tutorial 및 CodeNetMut 데이터 세트에서 모델의 성능을 평가합니다.SingleLine의 결과를 표(위)에 표시합니다.CodeExecutor는 단일 줄 변환의 약 94%를 올바르게 실행할 수 있는 반면 Codex는 대부분의 경우 이를 수행하지 못합니다.CodeExecutor는 또한 CEL-S1보다 0.7% 향상되어 사전 학습 중에 어려운 프로그램을 학습하면 쉬운 예제를 더 잘 해결하는 데 도움이 됩니다.각 SingleLine 프로그램은 항상 표준 출력 없이 단일 줄 추적을 생성하므로 출력 정확도를 보고하지 않으며 줄 정밀도/재현 점수는 추적 정확도와 같습니다. Table(medium)의 Tutorial 실험의 경우, CodeExecutor는 출력 정확도에서 Codex보다 상당히 우수한 성능을 보였습니다(76.42% 대 13.07%). CodeExecutor의 점수가 CEL-S2보다 낮은 것은 Tutorial 데이터 세트가 Tutorial 웹사이트의 몇몇 프로그램에서만 나온 돌연변이로 구성되어 다양성이 제한되기 때문에 Tutorial과 CodeNet의 코드 예제 간에 차이가 있음을 시사합니다. CEL-S3는 추적을 생성하는 데 어려움을 겪으며, 이는 마지막 학습 단계에서 Tutorial 데이터에서 습득한 대부분의 지식을 잊어버린다는 것을 나타냅니다. CodeNetMut 결과는 SingleLine 및 Tutorial 데이터 세트의 결과보다 훨씬 낮아 실제 시나리오에서 추적을 생성하는 것이 더 어렵다는 것을 보여줍니다. CodeExecutor는 거의 절반의 예제(48.06%)에 대해 올바른 출력을 생성하고, 추적의 약 1/3이 기준 진실과 정확히 일치합니다(33.38%). CodeExecutor는 코드 실행 작업에 대한 사전 학습을 통해 Codex보다 출력 성능을 절대 포인트로 30.6% 높였습니다. 또한 CodeExecutor는 CEL-S3보다 4.3%의 출력 정확도 점수와 3.9%의 추적 정확도 점수 향상을 보이는데, 이는 4.3에서 설명한 학습 전략의 효과를 나타낸다. 커리큘럼 학습을 제거한 후 출력 정확도 점수는 48.06%에서 45.93%로 떨어지고 추적 정확도 점수는 33.38%에서 30.98%로 떨어지며, 이는 기여도를 보여준다. 범주 기본 총 정답 정확도89. 내장 함수83. 리스트, 튜플 등 문자열77.52. 조건문95. 루프84. 함수 호출100. 표 4: 인간 평가 결과. CodeExecutor의 기능을 평가하기 위해 Python 프로그래밍 지식을 7가지 범주로 분류하고 이러한 범주를 처리할 때 생성된 추적이 올바른지 틀린지 수동으로 분석한다. 세 번째 범주에는 목록, 튜플, 사전 및 집합과 같은 Python 데이터 구조가 포함된다. 커리큘럼 학습. 이러한 결과는 Codex와 같은 소스 코드에서 사전 학습된 모델에 대한 코드 실행 작업이 어렵다는 것을 보여줍니다. 그러나 CodeExecutor 모델은 간단한 프로그램을 실행하는 데 높은 성능을 달성할 수 있으며 실제 프로그램에 대한 복잡한 실행 추적을 예측할 수 있습니다. 6.2 모델 성능에 대한 심층 연구 샘플(표 4)을 검토하여 모델 성능에 대한 정성적 분석을 수행한 결과 다음과 같은 결과가 나왔습니다. 부록 D에서 더 많은 예를 찾을 수 있습니다. 모델은 일반적으로 제어 흐름에 대한 기본적인 감각을 갖습니다. 조건문, 루프 및 함수 호출은 프로그램의 제어 흐름을 보여줍니다. 제어 흐름은 프로그램 코드가 실행되는 순서를 반영합니다. 프로그램을 이해하는 데 중요하며 특정 결정을 통해 코드를 제어하고 어떤 명령문을 실행해야 하고 어떤 명령문을 건너뛰어야 하는지 모니터링하기 때문에 종종 복잡합니다. 표 4에서 CodeExecutor는 고급 다중 줄 제어 흐름에 대한 기초적인 이해력을 가지고 있으며 특히 조건문과 함수 호출에 능숙합니다. 60개 조건문 중 57개와 사용자 정의 함수에 대한 모든 5개 호출이 예측됨 모델 MAP GraphCodeBERT 23.+ CodeExecutor 55.UniXcoder + CodeExecutor 71.79.표 5: 제로샷 설정에서 코드 간 검색 작업의 MAP 점수(%). 올바르게. 루프의 정확도는 84%인 반면, 잘못된 루프는 잘못된 반복 시간을 거칩니다. 그림 1(a)를 예로 들어보겠습니다. CodeExecutor는 (b)의 기준 진실과 정확히 동일한 추적을 예측합니다. 모델은 4번째 줄에서 발생한 for 루프가 여러 번 실행될 것임을 인식합니다. 두 번째 반복에서 &quot;n&quot;은 &quot;n &lt;= O&quot; 조건을 충족하여 &quot;break&quot; 문이 생성되고 루프가 종료됩니다. 이 모델은 for 루프의 코드 블록에서 잘 작동하여 제어 흐름을 이해하는 능력을 보여줍니다. 이 모델은 특히 데이터 구조와 관련된 복잡한 작업을 처리하는 데 어려움을 겪습니다. 복잡한 프로그램에는 종종 여러 범주의 프로그래밍 지식이 필요합니다. 그림 3은 목록과 문자열을 사용하는 예를 보여줍니다. &quot;a&quot;명이 I을 구독하고 &quot;b&quot;명이 II를 구독한다는 점을 감안하여 &quot;n&quot; 중 신문 I과 II를 모두 구독하는 사람의 최대 및 최소 수를 결정합니다. CodeExecutor는 5번째 줄에서 &quot;nmin&quot;을 잘못 계산하여 0을 예상했지만 2를 얻었습니다. 이 계산에는 목록에서 값을 검색하고, 더하기, 빼기를 수행하고, &quot;min&quot; 함수를 사용하는 것이 포함됩니다. 이러한 작업의 구성성으로 인해 모델이 코드를 완전히 이해하고 정확한 상태를 생성하는 것이 어렵습니다. 또한 &quot;목록&quot;에서 비교적 낮은 정확도에서 알 수 있듯이 튜플 등 (77.27%) 및 표 4의 &quot;문자열&quot;(52.63%)에서 우리는 모델이 목록 및 문자열과 같은 데이터 구조를 이해하는 데 부족하다는 것을 관찰합니다. 데이터 구조를 이해하려면 모델이 객체가 생성, 수정, 추가 또는 삭제된 후의 동작을 학습해야 합니다. 이러한 작업은 변경 가능하고 모델이 파악하기 어려울 수 있습니다. 이는 모델이 여러 작업과 데이터 구조를 포함하는 복잡한 프로그램을 처리하는 데 어려움을 겪을 수 있음을 시사합니다. 6.3 다운스트림 작업 코드 의미론을 표현하는 CodeExecutor의 효과를 확인하기 위해 두 코드 모델 Pass@1 Pass@Codex 12.45.+ CodeExecutor 17.49에 적용합니다. 표 6: 텍스트-코드 생성 작업에 대한 HumanEval 벤치마크 결과. 두 설정 모두에서 각 문제에 대해 50개의 솔루션이 평가됩니다. 인텔리전스 작업 제로샷 코드-코드 검색 작업 및 텍스트-코드 생성 작업. 제로샷 코드-코드 검색 이 작업은 Guo et al.(2022)에 의해 도입되었습니다. 중복을 피하기 위해 연관 데이터 세트와 사전 학습 코퍼스를 사용하여 CodeNet(Puri et al., 2021)에서 9,987개의 Python 함수를 수집하여 새로운 데이터 세트를 구성합니다. 각 함수는 48개의 문제 중 하나를 해결합니다. 주어진 함수 하나에서 동일한 문제를 해결하는 모든 함수를 검색합니다. 먼저 기준 모델의 마지막 숨겨진 상태의 평균 벡터를 사용하여 두 함수 간의 유사도를 계산합니다. 코드 실행이 코드 간 검색을 어떻게 용이하게 하는지 알아보기 위해 테스트 케이스를 제공하여 각 함수를 실행합니다. 그런 다음 CodeExecutor에서 생성한 실행 추적에서 추출한 프로그램 출력을 활용하여 쿼리 프로그램의 출력과 비교하여 편집 유사도에 따라 후보를 정렬합니다. 표 5에서 CodeExecutor는 GraphCodeBERT(Guo et al., 2021)에 비해 32.8포인트 이상 향상되고 UniXcoder에 비해 약 7.2포인트 향상되어 코드 실행이 코드 의미론에 대한 이해를 크게 향상시킬 수 있음을 보여줍니다. 텍스트-코드 생성 우리는 인간이 작성한 프로그래밍 문제를 포함하는 HumanEval 벤치마크(Chen et al., 2021)를 사용합니다. 우리는 먼저 Codex(code-cushman-001)를 활용하여 각 문제에 대해 200개의 솔루션을 생성합니다. 그런 다음 CodeExecutor를 사용하여 문제 설명에 예제 테스트 사례를 입력하여 각 솔루션의 출력을 예측합니다. 우리는 출력과 예상 출력 간의 편집 유사성에 따라 200개 솔루션을 순위를 매깁니다. 마지막으로 각 문제에 대한 처음 50개 솔루션의 정확성을 평가합니다. 다른 필터링 전략과 달리, 우리의 방법은 실제 코드 실행자가 필요하지 않고 실행 결과를 예측하기 위해 모델만 사용한다는 점에 유의하세요. 표 6은 CodeExecutor를 솔루션 필터로 사용하면 텍스트-코드 생성 성능이 향상되어 CodeExecutor가 다른 코드 인텔리전스 작업에 유익하다는 것을 보여줍니다. 7
--- CONCLUSION ---
우리는 Codex와 같은 현재 모델에 상당한 과제를 제기하는 대규모의 현실적인 Python 코드 실행 데이터 세트와 작업을 생성하기 위한 돌연변이 기반 데이터 증강 방법을 제안합니다. 우리는 사전 학습 목표로 코드 실행을 활용하고 커리큘럼 학습 전략을 채택하는 Transformer 모델인 CodeExecutor를 개발합니다. CodeExecutor는 코드 실행에서 기존 모델을 능가할 뿐만 아니라 코드-코드 검색 및 텍스트-코드 생성과 같은 다운스트림 작업에 대한 일반화 가능성을 보여줍니다. 우리의 연구는 코드 실행 및 기타 코드 인텔리전스 작업을 위한 새롭고 효과적인 솔루션을 제공합니다. 제한 사항 CodeExecutor의 몇 가지 제한 사항(예: Python에만 적용, 생성된 결과의 충실성 부족, 추적 생성의 최대 길이 제한)은 향후 작업에 대한 흥미로운 방향을 제시합니다. 프로그래밍 언어 현재 모델의 한 가지 제한 사항은 현재 Python에만 적용되어 다른 프로그래밍 언어로 작성된 프로그램을 실행하는 데 사용 및 효과가 제한된다는 것입니다. 이는 모델의 적용 가능성을 다른 언어로 확장하기 위한 향후 작업의 필요성을 강조합니다. 충실도 복잡한 논리, 긴 루프 또는 많은 분기가 있는 것과 같이 어려운 예를 처리할 때 결과가 충분히 충실하지 않을 수 있습니다. 예를 들어, &quot;alpha = list(&#39;abcdefg&#39;)&quot;라는 할당이 모두 포함된 두 개의 복잡한 프로그램에서 우리 모델은 한 경우에는 &quot;alpha&quot;의 값을 올바르게 예측하지만 다른 경우에는 잘못 예측하는 것을 관찰했습니다. 충실도의 부족은 코드 실행에 대한 추가 연구를 위해 연구해야 합니다. 생성 창 크기 생성된 추적의 길이를 1024개 토큰으로 제한합니다. 특히 루프가 있는 긴 실행 추적이 있는 프로그램의 경우 제한이 될 수 있습니다. Transformers가 더 긴 시퀀스를 처리하는 기능을 개선하면(Tay et al., 2021, 2022) 코드 실행 작업에 유익할 가능성이 높습니다. 윤리적 성명 이 작업은 윤리적 원칙을 준수하여 수행됩니다. 이 논문에서 소개한 데이터 세트는 공개적으로 사용 가능한 데이터만 사용했습니다. 인간 평가의 주석은 논문의 두 저자가 수행했으므로 보상과 관련된 우려 사항이 없습니다. 따라서 연구와 관련된 잠재적 위험은 없습니다. 참고문헌 Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi. 2019. Mathqa: 연산 기반 형식주의를 통한 해석 가능한 수학 단어 문제 해결을 향하여. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, NAACL-HLT 2019, 미네소타주 미니애폴리스, 2019년 6월 2-7일, 1권(긴 논문 및 짧은 논문), 2357-2367쪽. 컴퓨터 언어학 협회. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, Charles Sutton. 2021. 대규모 언어 모델을 사용한 프로그램 합성. CORR, abs/2108.07732. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston. 2009. 커리큘럼 학습. 26th Annual International Conference on Machine Learning, ICML 2009, 몬트리올, 퀘벡, 캐나다, 2009년 6월 14-18일, ACM International Conference Proceeding Series의 382권, 41-48페이지. ACM. David Bieber, Charles Sutton, Hugo Larochelle, Daniel Tarlow. 2020. 명령어 포인터 어텐션 그래프 신경망을 사용한 프로그램 실행 학습. Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상. Casey Casalnuovo, Earl T. Barr, Santanu Kumar Dash, Prem Devanbu, Emily Morgan. 2020. 이중 채널 제약 이론. ICSE-NIER 2020: 42nd International Conference on Software Engineering, New Ideas and Emerging Results, 서울, 대한민국, 2020년 6월 27일~7월 19일, 25~28쪽. ACM. Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T. Devanbu, Baishakhi Ray. 2022. Natgen: 소스 코드를 &quot;자연스럽게&quot; 만드는 생성적 사전 학습. 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, 싱가포르, 싱가포르, 2022년 11월 14~18일, 18~30쪽. ACM. 마크 첸, 제리 트워렉, 희우 전, 치밍 위안, 헨리크 폰데 드 올리베이라 핀토, 자렛 카플란, 해리슨 에드워즈, 유리 버다, 니콜라스 조셉, 그렉 브록만, 알렉스 레이, 라울 푸리, 그레첸 크루거, 마이클 페트로프, 하이디 클라프, 기리시 사스트리, 파멜라 미슈킨, 브룩 찬, 스콧 그레이, 닉 라이더, 미하일 파블로프, 알레시아 파워, 루카스 카이저, 모하마드 바바리안, 클레멘스 윈터, 필립 틸렛, 펠리페 페트로스키 수치, 데이브 커밍스, 마티아스 플래퍼트, 포티오스 찬치스, 엘리자베스 반스, 아리엘 허버트-보스, 윌리엄 헵겐 거스, 알렉스 니콜, 알렉스 파이노, 니콜라스 테작, 지에 탕, 이고르 바부슈킨, 수치르 발라지, 샨타누 자인, 윌리엄 손더스, 크리스토퍼 헤세, 앤드류 N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba. 2021. 코드에서 학습된 대규모 언어 모델 평가. CORR, abs/2107.03374. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman. 2021. 수학 단어 문제를 풀기 위한 검증자 학습. CoRR, abs/2110.14168. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser. 2019. Universal transformers. 제7회 International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, 2019년 5월 6-9일. OpenReview.net. Anna Derezińska와 Konrad Hałas. 2014. Operators for mutation testing of python programs. Res. Rep. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 Association for Computational Linguistics: Human Language Technologies 북미 지부 회의록, NAACL-HLT 2019, Minneapolis, MN, USA, 2019년 6월 2-7일, 1권(긴 논문과 짧은 논문), 4171-4186쪽. Association for Computational Linguistics. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou 및 Hsiao-Wuen Hon. 2019. 자연어 이해 및 생성을 위한 통합 언어 모델 사전 학습. 신경 정보 처리 시스템의 발전 32: 신경 정보 처리 시스템에 관한 연례 컨퍼런스 2019, NeurIPS 2019, 2019년 12월 8~14일, 캐나다 BC 주 밴쿠버, 페이지 13042-13054. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang 및 Ming Zhou. 2020. Codebert: 프로그래밍 및 자연어를 위한 사전 훈련된 모델입니다. Findings of the Association for Computational Linguistics: EMNLP 2020, 온라인 이벤트, 2020년 11월 16-20일, ACL의 Findings EMNLP 권, 1536-1547페이지. Association for Computational Linguistics. Alex Graves, Greg Wayne, Ivo Danihelka. 2014. 신경 튜링 머신. CoRR, abs/1410.5401. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, Demis Hassabis. 2016. 동적 외부 메모리가 있는 신경망을 사용한 하이브리드 컴퓨팅. Nat., 538(7626):471–476. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin. 2022. Unixcoder: 코드 표현을 위한 통합 크로스모달 사전 학습. 60th Annual Meeting of the Association for Computational Linguistics(제1권: 장문 논문)의 회의록, ACL 2022, 아일랜드 더블린, 2022년 5월 22-27일, 7212-7225쪽. Association for Computational Linguistics. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou. 2021. Graphcodebert: 데이터 흐름을 통한 사전 학습 코드 표현. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, 2021년 5월 3-7일. OpenReview.net. Richard G. Hamlet. 1977. Testing programs with the aid of acompiler. IEEE Trans. Software Eng., 3(4):279–290. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish. 2020. 자기 회귀 생성 모델링을 위한 스케일링 법칙. CORR, abs/2010.14701. Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, Premkumar T. Devanbu. 2012. 소프트웨어의 자연스러움에 관하여. 제34회 국제 소프트웨어 공학 컨퍼런스, ICSE 2012, 2012년 6월 29일, 스위스 취리히, 837-847쪽. IEEE 컴퓨터 학회. Yue Jia와 Mark Harman. 2011. 돌연변이 검사 개발에 대한 분석 및 조사. IEEE Trans. Software Eng., 37(5):649-678. Lukasz Kaiser와 Ilya Sutskever. 2016. 신경 GPU 학습 알고리즘. 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, 2016년 5월 2-4일, Conference Track Proceedings. Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi. 2020. 소스 코드의 컨텍스트 임베딩 학습 및 평가. 37th International Conference on Machine Learning, ICML 2020, 2020년 7월 13-18일, Virtual Event, Proceedings of Machine Learning Research의 119권, 5110-5121페이지. PMLR. Karol Kurach, Marcin Andrychowicz, Ilya Sutskever. 2016. Neural Random-Access Machines. 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, 2016년 5월 2-4일, Conference Track Proceedings. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom. 2017. Program induction by reasone generation: Learning tosolve and explain algebraic word problems. 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 158–167. Association for Computational Linguistics. Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch. 2022. Frozen pretrained transformers as universal computing engines. 제36회 AAAI 인공지능 컨퍼런스, AAAI 2022, 제34회 인공지능 혁신적 응용 컨퍼런스, IAAI 2022, 제12회 인공지능 교육 진전 심포지엄, EAAI 2022 가상 이벤트, 2022년 2월 22일-3월 1일, 7628-7636페이지. AAAI 출판부. Maxwell I. Nye, Anders Johan Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena. 2021. 작품 소개: 언어 모델을 사용한 중급 계산을 위한 스크래치패드. CORR, abs/2112.00114. Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, Frederick Reiss. 2021. Codenet: 다양한 코딩 작업을 학습하기 위한 대규모 AI for code 데이터 세트. Neural Information Processing Systems Track on Datasets and Benchmarks 1의 회의록, NeurIPS Datasets and Benchmarks 2021, 2021년 12월, 가상. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. 생성적 사전 학습을 통한 언어 이해 향상. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21:140:1-140:67. Scott E. Reed and Nando de Freitas. 2016. 신경 프로그래머-통역사. 제4회 학습 표현 국제 컨퍼런스, ICLR 2016, 푸에르토리코 산후안, 2016년 5월 2-4일, 컨퍼런스 트랙 회의록. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. 신경 모델의 수학적 추론 능력 분석. 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, 2019년 5월 6-9일. OpenReview.net. Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: code generation using transformer. ESEC/FSE &#39;20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, 2020년 11월 8-13일, 1433-1443쪽. ACM. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. 장거리 경기장: 효율적인 변압기의 벤치마크. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. 효율적인 변압기: 설문 조사. ACM Computing Surveys, 55(6):1–28. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30. Petar Velickovic, Lars Buesing, Matthew C. Overlan, Razvan Pascanu, Oriol Vinyals, and Charles Blundell. 2020a. 포인터 그래프 네트워크. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6일-12일, 가상. Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. 2020b. 그래프 알고리즘의 신경 실행. 제8회 학습 표현 국제 컨퍼런스, ICLR 2020, 에티오피아 아디스아바바, 2020년 4월 26일-30일. OpenReview.net. Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. 2021a. Syncobert: 코드 표현을 위한 구문 안내 다중 모드 대조 사전 학습. arXiv 사전 인쇄본 arXiv:2108.04556. Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. 2020. 그래프 간격 신경망을 사용한 의미 프로그램 임베딩 학습. Proc. ACM Program. Lang., 4(OOPSLA):137:1-137:27. Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven CH Hoi. 2021b. Codet5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 학습된 인코더-디코더 모델. 2021 자연어 처리 경험적 방법에 대한 컨퍼런스의 진행 사항, EMNLP 2021, 가상 이벤트/푼타카나, 도미니카 공화국, 2021년 11월 7-11일, 8696-8708쪽. Association for Computational Linguistics. Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, Milad Hashemi. 2020. 신경 실행 엔진: 서브루틴 실행 학습. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상. Wojciech Zaremba와 Ilya Sutskever. 2014. 실행 학습. CORR, abs/1410.4615. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron C. Courville, Behnam Neyshabur, and Hanie Sedghi. 2022. Teaching algorithmic reasoning via in-context learning. CoRR, abs/2211.09066. 데이터 세트 세부 정보 실행 가능한 프로그램을 얻기 위해 CodeNet(Puri et al., 2021)의 경쟁 프로그래밍 문제에 제출된 내용을 기반으로 Python 코드 실행 데이터 세트를 빌드합니다. 실제 복잡성을 가진 이러한 인간이 작성한 프로그램은 온라인 심사 웹사이트 AIZU 6 및 AtCoder 7에서 파생되었습니다. CodeNet에는 8,00개의 고유한 프로그래밍 문제를 해결하는 것을 목표로 하는 240,000개의 Python 제출물이 포함되어 있습니다. 각 제출물은 stdin에서 읽고 stdout에 쓰는 단일 파일 Python 프로그램입니다. 각 프로그래밍 문제는 최소 1개의 샘플 입력과 최대 4개의 샘플 입력을 제공합니다. 프로그램을 실행하는 것은 입력에 의존하므로 입력 스트림에서 읽는 명령문을 변수에 입력 값을 할당하는 할당 명령문으로 바꿉니다. 각 제출물을 샌드박스 환경에서 실행하여 해당 프로그램의 실행 추적을 얻습니다. 프로그램은 실행 시간 1초와 실행 추적 1024줄로 제한되며, 제한을 초과하면 필터링됩니다. 또한 프로그램에서 발생한 Python 예외를 잡아서 구문 분석이나 실행 중에 런타임 오류가 발생하는 프로그램을 제거합니다. 이를 통해 각각 추적과 쌍을 이루는 387k개의 실행 프로그램 데이터 집합이 생성됩니다. 실행 가능한 프로그램의 대규모 데이터 집합을 구성하기 위해 돌연변이 기반 데이터 증강 접근 방식을 제안합니다. 먼저 표 1에 표시된 대로 돌연변이 연산자 집합을 제시합니다. 대부분은 강력한 유형의 범용 언어에서 사용되는 선택된 연산자에 해당하며 Python 언어에 채택되었습니다. 슬라이스 인덱스 제거(SIR) 및 역방향 반복 루프(RIL)와 같이 Python 기능을 위해 설계된 연산자도 포함됩니다. 그런 다음 tree-sitter³를 활용하여 프로그램을 추상 구문 트리로 변환한 다음 노드 유형 정보를 추출하여 모든 가변 리터럴, 연산자 및 명령문의 후보 목록을 얻습니다. 각 가변 후보에 대해 50% 확률로 관련 돌연변이 연산자를 적용합니다. 구체적으로 숫자 리터럴 x를 평균 x와 표준 편차 100인 가우시안 분포의 난수로 변경합니다. 하나 또는 두 개의 난수 문자로 문자열을 확장하거나 문자열을 줄입니다. 각 루프를 처리할 때 세 개의 루프 관련 연산자 중 하나를 무작위로 선택하거나 그대로 유지합니다. 모든 연산자는 돌연변이된 프로그램 실행 전에 적용할 수 있으며 오류가 있는 가능한 돌연변이는 실행 중에 감지하여 제거합니다. 각 프로그램을 20번 돌연변이시켜 각각 추적과 쌍을 이루는 3.2M개의 중복 제거된 프로그램을 얻습니다. CodeNet Mutants(CodeNetMut)를 사용하여 사전 학습 데이터 세트를 빌드합니다. 데이터 유출을 방지하기 위해 동일한 문제에 대한 모든 제출은 동일한 분할의 일부가 됩니다. 우리는 돌연변이가 있는 710개 문제의 제출을 사용하여 사전 학습 데이터 세트를 구축합니다. 돌연변이는 다양성을 크게 향상시키므로 이러한 프로그램은 풍부한 의미론과 복잡한 연산을 구현합니다. 돌연변이가 없는 다른 제출은 검증 및 테스트 데이터 세트를 구축하는 데 사용됩니다. 이러한 인간이 작성한 프로그램은 평가 데이터의 품질을 보장합니다. B 모델 구성 우리는 768차원 숨은 상태와 어텐션 헤드가 있는 Transformer의 12개 레이어를 기반으로 모델을 구축합니다. 200개 줄 번호, 3개 사전 학습 데이터 세트 이름을 나타내고 https://tree-sitter.github.io/tree-sitter/를 추적하기 위해 210개의 추가 특수 토큰을 어휘에 추가합니다. 모델 CEL CodeExecutor Stage1(S1) SingleLine SingleLine Stage2(S2) Tutorial Stage3(S3) CodeNetMut SingleLine(3M), Tutorial SingleLine(3M), Tutorial, CodeNetMut 표 7: CEL 및 CodeExecutor가 3단계 사전 학습에 사용하는 데이터 세트. “SingleLine(3M)”은 CodeExecutor가 생성하기 가장 어려운 SingleLine 내의 백만 개의 인스턴스를 나타냅니다. 코드: 1 from string import ascii_lowercase 예측:def main():s = [&#39;a&#39;]<line> 1<state> ascii_lowercase: abcdefghijklmnopqrstuvwxyz<line> 2<state> ascii_lowercase: abcdefghijklmnopqrstuvwxyz ; 메인:<function><line> 14<state> ascii_lowercase: abcdefghijklmnopqrstuvwxyz ; 메인:<function><line> 15<state> 케이 =<line> 2<state> i, c에 대해 열거형(들)을 사용합니다.<line> 3<state> s [a]num ord(c) - ord(&quot;a&quot;)<line> 4<state> s [a] ; Kif num &gt; 및 K (26 num) &gt;= 0:<line> 5<state> s: [a] ; K 25; i: 0; cas[i] = &quot;a&quot;<line> 6<state> s [a] ; KK (26 숫자)<line> 7<state> s [a] ; K 25; 10; 약; 숫자:25; 10; 약; 숫자:숫자 ord(s[-1]) - ord(&quot;a&quot;)마지막 = (숫자 K) %s[-1] ascii_lowercase [마지막]print(&quot;&quot;.join(s))<line> 5<state> 에스<line> 10<state> 에스:<line> 11<state> 에스<line> 12<state> 에스<output> z [a] ;K [a] ;K [a] ;K [z];K: 25;1:0;약;숫자:25;1:0;약;숫자:25;i: 0;약;숫자:0;마지막25;i: 0;c:a;숫자:0;마지막14이름이_main__인경우&quot; :main()<line> 13<state> s [z]; K 25; 10; 약; 숫자: 0%; 마지막<line> 13<state> ascii_lowercase: abcdefghijklmnopqrstuvwxyz ; 메인:<function> 그림 4: Python 프로그래밍 지식의 모든 범주를 포괄하는 CodeNetMut 테스트 분할의 예.CodeExecutor는 올바른 예측을 제공합니다.§4.2에 설명된 구조.사전 학습 중에 입력 시퀀스의 최대 길이와 배치 크기를 각각 1024와 256으로 설정합니다.Adam 옵티마이저를 사용하여 4e-4 학습률로 모델 매개변수를 업데이트합니다.먼저 SingleLine 데이터 세트를 사용하여 500k 단계에 대한 코드 실행 목표로 모델을 사전 학습합니다.그런 다음 모델에서 생성하기 가장 어려운 SingleLine의 백만 개 인스턴스를 예약하고 Tutorial 데이터를 코퍼스에 추가하여 300k 단계에 대한 사전 학습을 수행합니다.CodeNetMut을 코퍼스에 추가하고 300k 단계에 대한 사전 학습을 추가로 수행합니다.32GB 메모리가 있는 16개의 NVIDIA Tesla V100 클러스터에서 모델을 사전 학습하며 총 학습 시간은 약 한 달입니다. 추론을 위해 빔 검색을 10으로 설정했습니다.C 3단계 사전 학습 표 7에는 CodeExecutorLimited(CEL)와 CodeExecutor가 각각 3단계 사전 학습에 사용하는 데이터 세트를 나열했습니다.CEL에 대한 사전 학습의 첫 번째 단계는 SingleLine 데이터 세트를 사용하여 모델 CEL-S1을 생성합니다.두 번째 단계에서 CEL은 CEL-S로 초기화되고 Tutorial 데이터 세트로 사전 학습되어 모델 CEL-S2를 생성합니다.세 번째 단계에서 CEL은 CEL-S2로 초기화되고 CodeNetMut 데이터 세트로 사전 학습되어 모델 CELS3을 생성합니다.반면에 CodeExecutor는 먼저 SingleLine 데이터 세트로 사전 학습한 다음, 모델의 손실을 기반으로 이후 학습 단계에 가장 어려운 300만 개의 SingleLine 데이터를 선택합니다.두 번째 단계에서 CodeExecutor는 Tutorial 데이터 세트와 함께 어려운 300만 개의 SingleLine 데이터로 사전 학습합니다. 세 번째 단계에서 CodeExecutor는 300만 개의 어려운 SingleLine 데이터, 전체 Tutorial 데이터 세트 및 CodeNetMut 데이터 세트로 사전 학습됩니다.D 정성적 예제 여기에 추가 예제가 나와 있습니다.그림 4는 표 4에 있는 모든 Python 프로그래밍 지식 범주를 포괄하는 예를 보여줍니다.CodeExecutor는 실제 결과와 동일한 추적을 생성합니다.그림 5는 소수점으로 나누기 계산을 수행하는 예입니다.CodeExecutor는 처음 15자리는 올바르게 생성하고 나머지 두 자리는 오류가 있습니다.코드: x = 1.2379400392853809ex = 실제 결과: x 2.475880078570762e-예측: x 2.4758800785707618e-그림 5: 소수점으로 나누기 계산의 예.CodeExecutor는 처음 15자리를 올바르게 생성하고 실수는 밑줄로 강조 표시했습니다.
