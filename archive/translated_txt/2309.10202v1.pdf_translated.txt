--- ABSTRACT ---
대규모 언어 모델(LLM)은 자연어 처리에 혁명을 일으켰지만, RLHF를 사용하여 이러한 모델을 인간의 가치와 선호도에 맞추는 것은 여전히 중요한 과제입니다. 이 과제는 보상 해킹 및 재앙적 망각과 같은 다양한 불안정성을 특징으로 합니다. 이 기술 보고서에서 우리는 RLHF 훈련을 안정화하기 위한 두 가지 혁신을 제안합니다. (i) 이점 모델, 즉 예상 보상에 비해 추가 보상인 이점 점수를 직접 모델링하고 보상 해킹을 방지하기 위해 작업 전체에 걸쳐 점수 분포를 조절합니다. (ii) 선택적 리허설, PPO 훈련 및 지식 리허설을 위한 데이터를 전략적으로 선택하여 재앙적 망각을 완화합니다. 공개 및 독점 데이터 세트에 대한 실험 분석 결과, 제안된 방법은 RLHF 훈련의 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성합니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 자연어 처리(NLP)와 인공 지능(AI)을 발전시키는 데 기본적인 요소가 되었으며, 의미적으로나 맥락적으로 모두 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 학습되기 때문에 정보를 조작하거나 편향적이거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 관여할 위험이 있습니다. 이는 인간의 가치, 의도 및 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬 과제를 해결하기 위해 많은 접근법이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근법 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도에 맞추는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 예시를 포함하여 SoTA LLM을 훈련하는 데 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 단순히 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 응답(하지만 높은 보상)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되어 반복적인 실패로 이어집니다. 이러한 불안정성에는 여러 가지 원인이 있습니다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)로 이어질 수 있습니다. 이는 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상입니다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다. 공개 및 독점 데이터에 대한 실험은 Advantage Model이 다양한 예제에서 보상 점수 분포를 성공적으로 균형 잡는 동시에 순위 정밀도를 유지하고, SFT 모델에 비해 더 높은 보상 점수와 승률을 달성하도록 PPO 훈련을 안내한다는 것을 보여주었습니다. 나아가, Selective Rehearsal은 PPO 훈련에 가장 적합한 예제를 선택하여 과도한 최적화를 피할 수 있으므로 전문가 중심 예제에서 성과를 유지할 수 있습니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • RLHF 훈련의 불안정성의 여러 원인을 분석하고 식별합니다. 즉, 보상 해킹 및 치명적인 망각 문제로 이어지는 불균형 학습된 보상 점수 분포와 특정 PPO 훈련 데이터의 과도한 최적화입니다. • 다양한 범주에서 보상 점수 분포를 균형 잡기 위해 Advantage Model을 도입하고, 어떤 예제를 사용해야 하는지 분별하기 위해 Selective Rehearsal 전략을 도입합니다. 2전문가 중심 예제는 전문가가 정한 표준 및 기준을 충족하고 인간의 선호도와 긴밀하게 일치하는 데이터 샘플입니다. 이러한 예는 SFT 모델 학습 및 평가, PPO 학습에 사용되며 SFT 단계에서 축적된 지식을 연습하는 데 사용해야 합니다.• 공개 및 독점 데이터 세트에 대한 광범위한 실험을 통해 Advantage Model과 Selective Rehearsal이 RLHF 학습을 안정화하여 더 높은 보상 점수와 승률을 달성할 수 있음을 보여줍니다.예비 최근 머신 러닝 연구에서 RLHF(Ouyang et al., 2022; Bai et al., 2022a)는 LLM을 인간의 목표(예: 도움이 되고 무해함)에 맞추기 위한 핵심 전략으로 등장했습니다.RLHF는 일반적으로 SFT 단계를 따르며, SFT는 (프롬프트, 응답) 쌍에 대한 교사 강제를 사용하여 LLM을 인간의 목표에 맞춥니다.그러나 이러한 정렬에도 불구하고 LLM은 보이지 않는 작업에 직면했을 때 일반화에 어려움을 겪을 수 있습니다.LLM과 인간 간의 상호 작용에서 보상 함수를 학습하고 강화 학습을 사용하여 학습된 보상 함수로 LLM을 최적화하는 것은 LLM 정렬 문제를 해결하는 효과적인 접근 방식으로 나타났습니다. Leike et al. 2018; Stiennon et al. 2020; Ouyang et al.은 인간 피드백에서 강화 학습을 포함하는 방법을 제안했는데, 여기서 RM은 동일한 입력에서 생성된 두 모델 출력 간의 비교 데이터 세트에서 훈련됩니다. 목표는 다른 사람보다 인간 레이블러가 선호하는 출력에 더 높은 보상을 할당하는 것입니다. 일반적으로 이는 마지막 umembedding 레이어가 제거된 사전 훈련된 변압기 기반 LM에 스칼라 값을 출력하는 값 헤드를 추가하여 달성됩니다. 구체적으로 보상 모델링 손실은 다음과 같습니다.LRM = E(x,ye, yr)~DRM(log(σ(ro(x, Yc) - ro(x, yr)))] (1) 여기서 re(x, y)는 매개변수 0을 갖는 프롬프트 x와 응답 y에 대한 보상 점수를 나타내고, ye는 쌍 yɩ와 yr의 선호되는 응답이며, DRM은 전체 비교 데이터 세트입니다.다음과 같이, 안정성과 단순성 면에서 강점을 보이는 Proximal Policy Optimization(PPO)(Schulman et al., 2017)은 정책을 최적화하는 강화 학습 알고리즘으로 일반적으로 채택됩니다.특히, 프롬프트 데이터 세트 D에 대한 정책 π에 대한 PPO 목적은 다음과 같이 정의됩니다.LPPO = Ex~DPPO, y~π(x) [10(x, y) - ßlog (76(yx)/πinit (yx))] 여기서 rø(x, y)는 (프롬프트, 응답) (x, y) 쌍; init은 RLHF 이전의 정책을 나타내며 RLHF 학습 동안 일정하게 유지됩니다. ẞ는 KL-발산 항에 대한 계수입니다. PPO 외에도 기각 샘플링(Touvron et al., 2023)은 최근 LLM을 정렬하는 간단한 방법으로 관심을 얻고 있습니다. 오프라인 정책 학습 알고리즘으로 반복적 프로세스를 채택합니다. 각 반복 n에 대해 먼저 기준 F에 따라 주 정책 π에서 (x, y) 쌍을 선택하여 새 데이터 집합 Dn을 구성합니다. DPPO = . n = {(x, y) F(x, y) 여기서 x ~ DPPO , Y ~ π(x)} = 1ro (x, y)&gt;T (3) 여기서 일반적으로 사용되는 기준 F에는 RM 점수가 특정 임계값을 초과하는 샘플만 포함됩니다. 7. 그런 다음 정책은 DPPO에 대한 교사 강제로 업데이트됩니다. |y| LRS = E(x,y)~DPPO T¢(Yt\Y
--- RELATED WORK ---
LLM과 인간의 선호도의 일치. LLM은 일반적으로 광범위한 데이터 세트에서 사전 학습되며 다양한 다운스트림 작업에 적용할 수 있습니다. LLM을 효과적으로 활용하는 데 있어 중요한 측면 중 하나는 인간의 선호도와 일치하도록 보장하는 것입니다. 이는 안전하지 않거나, 독성이 있거나, 성적으로 노골적이거나, 편향적이거나, 범죄적인 반응을 피하는 데 도움이 됩니다(Leike et al., 2018). 이를 달성하기 위한 주요 전략은 RLHF입니다. 여기에는 인간의 피드백을 기반으로 보상 모델을 학습하고 PPO를 활용하여 LLM을 미세 조정하는 것이 포함됩니다(Christiano et al., 2017; Bai et al., 2022a; Glaese et al., 2022; Bai et al., 2022b; Stiennon et al., 2020; Qiu et al., 2022). RLHF의 불안정성. 성공에도 불구하고 RLHF 접근 방식은 본질적으로 복잡하고 상당한 과제를 안겨주므로 더 간단한
--- METHOD ---
s는 RLHF 훈련에서 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성합니다.서론 대규모 언어 모델(LLM)은 자연어 처리(NLP)와 인공 지능(AI)을 발전시키는 데 기본적인 요소가 되었으며, 의미적으로나 맥락적으로 모두 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 훈련되기 때문에 정보를 조작하거나 편향되거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 관여할 위험이 있습니다. 이는 인간의 가치, 의도, 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬의 과제를 해결하기 위해 많은 접근 방식이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근 방식 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도와 정렬하는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 모범 사례를 포함한 SoTA LLM을 훈련하는 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 반응(하지만 보상이 높음)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되며, 이는 반복적인 실패로 이어진다. 이러한 불안정성에는 여러 가지 원인이 있다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)를 야기할 수 있다. 보상 해킹은 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상이다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다.
--- EXPERIMENT ---
공개 및 독점 데이터 세트에 대한 al 분석은 제안된 방법이 RLHF 훈련의 안정성을 높일 뿐만 아니라 더 높은 보상 점수와 승률¹을 달성한다는 것을 보여줍니다.서론 대규모 언어 모델(LLM)은 자연어 처리(NLP) 및 인공 지능(AI)을 발전시키는 기본 요소가 되었으며, 의미적으로나 맥락적으로 관련성이 있는 텍스트를 생성하는 인상적인 능력을 보여줍니다(OpenAI, 2023; Köpf et al., 2023; Touvron et al., 2023). 이러한 발전에도 불구하고 LLM은 저품질 소스를 포함할 수 있는 광범위한 데이터에서 훈련되기 때문에 정보를 조작하거나 편향되거나 유해하거나 심지어 위험한 콘텐츠를 생성하는 것과 같은 바람직하지 않은 행동에 참여할 위험이 있습니다. 이는 인간의 가치, 의도, 선호도와 LLM 정렬의 필요성을 강조했습니다(Brown et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). LLM 정렬의 과제를 해결하기 위해 많은 접근 방식이 제시되었습니다(Bai et al., 2022a; OpenAI, 2023; Askell et al., 2021). 이러한 접근 방식 중에서 인간 피드백을 통한 강화 학습(RLHF)은 언어 모델을 인간의 선호도와 정렬하는 데 효과적임을 입증했습니다. RLHF는 OpenAI의 GPT-4(OpenAI, 2023), Anthropic의 Claude(Bai et al., 2022a), Google의 Sparrow(Glaese et al., 2022), Bard, Meta의 Llama 2-Chat(Touvron et al., 2023)과 같은 모범 사례를 포함한 SoTA LLM을 훈련하는 핵심 구성 요소 역할을 합니다. RLHF는 LLM의 역량을 훈련 데이터 분포를 모델링하는 것 이상으로 끌어올립니다. LLM에 인간이 선호하는 방식으로 텍스트 생성 분포를 조정할 수 있는 능력을 부여합니다. 그러나 RLHF를 사용하여 LLM을 훈련하는 것은 의심할 여지 없이 어려운 일이며, 인간 판사를 근사하는 정확하고 신뢰할 수 있는 보상 모델과 지속적인 정책 개선을 위한 강력한 PPO 알고리즘이 필요합니다. 세심한 구성에도 불구하고 불안정성, 예를 들어 횡설수설하는 반응(하지만 보상이 높음)(Stiennon et al., 2020; Skalse et al., 2022), 학습한 지식을 잊는 것은 일반적으로 훈련 중에 관찰되며, 이는 반복적인 실패로 이어진다. 이러한 불안정성에는 여러 가지 원인이 있다. (i) 보상 모델은 다양한 범주에 대해 서로 다른 보상 점수 분포를 학습하여 잠재적으로 보상 해킹 문제(Skalse et al., 2022)를 야기할 수 있다. 보상 해킹은 모델이 보상을 극대화하는 의도치 않은 방법을 찾는 현상이다. 그림 1a에 나와 있듯이 보상 모델은 코드 생성 및 QA 작업에 대한 보상 점수 분포에서 눈에 띄는 차이를 학습합니다. *동등한 기여&#39; 진행 중 작업 빈도 코드 생성 QA 학습 점수 예상 점수 0.0.승패율 0.0.0.0.0.RM-PPO 0.-4 --2 -1 0 1 2 3보상 점수 에포크 (a) 보상 점수 분포. (b) GPT-4에서 평가한 망각 세트에서 SFT 모델에 대한 승률. 그림 1: 왼쪽: QA 및 코드 생성 작업에 대한 보상 점수 분포. 분포가 비슷해야 한다는 예상에도 불구하고 두 작업 간에 학습된 보상 점수 분포에 눈에 띄는 차이가 있습니다. 오른쪽: 망각 세트에서 SFT 모델에 대한 승패율은 상당한 감소를 보입니다. 이 승률의 감소는 보상 해킹과 치명적 망각 현상에 기인할 수 있습니다. 선호도 데이터에 존재하는 61개 작업. 보상 점수 정규화를 사용하더라도 변동하는 평균과 분산은 더 높은 보상 점수로 인해 코드 생성의 응답 패턴을 QA 예제로 전송하는 것과 같이 예상치 못한 모델 동작을 유발할 수 있습니다. (ii) 감독 미세 조정(SFT) 단계에서 인간과 잘 정렬된 예제에서 PPO로 과도하게 최적화하면 치명적인 망각 문제가 발생합니다(McCloskey &amp; Cohen, 1989; Gupta et al., 2023; Khetarpal et al., 2022). 모델은 SFT 단계에서 학습한 내용을 간과하는 경향이 있습니다. 즉, PPO 모델은 그림 1b에 표시된 것처럼 전문가 정렬 예제 2에서 SFT 모델보다 성능이 떨어집니다. 따라서 이 기술 보고서에서는 RLHF 훈련의 안정성과 효과를 향상시키는 두 가지 기술을 소개합니다. 첫째, 다양한 범주에 걸쳐 보상 점수 분포를 균형 있게 조정하기 위해 Advantage Model을 제안하여 눈에 띄는 차이 점수 분포로 인해 종종 유발되는 보상 해킹 딜레마를 피합니다. 이는 Advantage 점수, 즉 예상 보상과 비교하여 한 응답이 얻을 수 있는 추가 보상을 직접 모델링하고, 훈련 중에 Advantage 점수 분포를 동적으로 조절하여 분산과 평균이 합리적인 범위 내에서 유지되도록 함으로써 달성됩니다. 둘째, 우리는 치명적인 망각 문제를 완화하기 위해 Selective Rehearsal을 도입합니다. 우리는 모든 데이터가 PPO 훈련에서 동일하게 최적화되어야 하는 것은 아니라고 가정합니다. 따라서 우리는 PPO 훈련에 사용할 수 있는 예제를 자동으로 식별하고 SFT 단계에서 축적된 지식을 리허설하는 데 사용해야 하는 강력하고 효과적인 데이터 선택기를 제안하여 시간이 지남에 따라 전문가 정렬 예제에 대한 모델의 성능이 저하되는 것을 방지합니다. 공개 및 독점 데이터에 대한 실험은 Advantage Model이 다양한 예제에서 보상 점수 분포를 성공적으로 균형 잡는 동시에 순위 정밀도를 유지하고, SFT 모델에 비해 더 높은 보상 점수와 승률을 달성하도록 PPO 훈련을 안내한다는 것을 보여주었습니다. 나아가, Selective Rehearsal은 PPO 훈련에 가장 적합한 예제를 선택하여 과도한 최적화를 피할 수 있으므로 전문가 중심 예제에서 성과를 유지할 수 있습니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • RLHF 훈련의 불안정성의 여러 원인을 분석하고 식별합니다. 즉, 보상 해킹 및 치명적인 망각 문제로 이어지는 불균형 학습된 보상 점수 분포와 특정 PPO 훈련 데이터의 과도한 최적화입니다. • 다양한 범주에서 보상 점수 분포를 균형 잡기 위해 Advantage Model을 도입하고, 어떤 예제를 사용해야 하는지 분별하기 위해 Selective Rehearsal 전략을 도입합니다. 2전문가 중심 예제는 전문가가 정한 표준 및 기준을 충족하고 인간의 선호도와 긴밀하게 일치하는 데이터 샘플입니다. 이러한 예는 SFT 모델 학습 및 평가, PPO 학습에 사용되며 SFT 단계에서 축적된 지식을 연습하는 데 사용해야 합니다.• 공개 및 독점 데이터 세트에 대한 광범위한 실험을 통해 Advantage Model과 Selective Rehearsal이 RLHF 학습을 안정화하여 더 높은 보상 점수와 승률을 달성할 수 있음을 보여줍니다.예비 최근 머신 러닝 연구에서 RLHF(Ouyang et al., 2022; Bai et al., 2022a)는 LLM을 인간의 목표(예: 도움이 되고 무해함)에 맞추기 위한 핵심 전략으로 등장했습니다.RLHF는 일반적으로 SFT 단계를 따르며, SFT는 (프롬프트, 응답) 쌍에 대한 교사 강제를 사용하여 LLM을 인간의 목표에 맞춥니다.그러나 이러한 정렬에도 불구하고 LLM은 보이지 않는 작업에 직면했을 때 일반화에 어려움을 겪을 수 있습니다.LLM과 인간 간의 상호 작용에서 보상 함수를 학습하고 강화 학습을 사용하여 학습된 보상 함수로 LLM을 최적화하는 것은 LLM 정렬 문제를 해결하는 효과적인 접근 방식으로 나타났습니다. Leike et al. 2018; Stiennon et al. 2020; Ouyang et al.은 인간 피드백에서 강화 학습을 포함하는 방법을 제안했는데, 여기서 RM은 동일한 입력에서 생성된 두 모델 출력 간의 비교 데이터 세트에서 훈련됩니다. 목표는 다른 사람보다 인간 레이블러가 선호하는 출력에 더 높은 보상을 할당하는 것입니다. 일반적으로 이는 마지막 umembedding 레이어가 제거된 사전 훈련된 변압기 기반 LM에 스칼라 값을 출력하는 값 헤드를 추가하여 달성됩니다. 구체적으로 보상 모델링 손실은 다음과 같습니다.LRM = E(x,ye, yr)~DRM(log(σ(ro(x, Yc) - ro(x, yr)))] (1) 여기서 re(x, y)는 매개변수 0을 갖는 프롬프트 x와 응답 y에 대한 보상 점수를 나타내고, ye는 쌍 yɩ와 yr의 선호되는 응답이며, DRM은 전체 비교 데이터 세트입니다.다음과 같이, 안정성과 단순성 면에서 강점을 보이는 Proximal Policy Optimization(PPO)(Schulman et al., 2017)은 정책을 최적화하는 강화 학습 알고리즘으로 일반적으로 채택됩니다.특히, 프롬프트 데이터 세트 D에 대한 정책 π에 대한 PPO 목적은 다음과 같이 정의됩니다.LPPO = Ex~DPPO, y~π(x) [10(x, y) - ßlog (76(yx)/πinit (yx))] 여기서 rø(x, y)는 (프롬프트, 응답) (x, y) 쌍; init은 RLHF 이전의 정책을 나타내며 RLHF 학습 동안 일정하게 유지됩니다. ẞ는 KL-발산 항에 대한 계수입니다. PPO 외에도 기각 샘플링(Touvron et al., 2023)은 최근 LLM을 정렬하는 간단한 방법으로 관심을 얻고 있습니다. 오프라인 정책 학습 알고리즘으로 반복적 프로세스를 채택합니다. 각 반복 n에 대해 먼저 기준 F에 따라 주 정책 π에서 (x, y) 쌍을 선택하여 새 데이터 집합 Dn을 구성합니다. DPPO = . n = {(x, y) F(x, y) 여기서 x ~ DPPO , Y ~ π(x)} = 1ro (x, y)&gt;T (3) 여기서 일반적으로 사용되는 기준 F에는 RM 점수가 특정 임계값을 초과하는 샘플만 포함됩니다. 7. 그런 다음 정책은 DPPO에 대한 교사 강제로 업데이트됩니다. |y| LRS = E(x,y)~DPPO T¢(Yt\Y
--- CONCLUSION ---
이 보고서에서 우리는 LLM의 RLHF 훈련에서 중요한 장애물인 보상 해킹과 치명적 망각을 식별하고 분석했습니다. 이러한 문제는 학습된 보상 점수 분포의 분산과 특정 훈련 사례의 과도한 최적화로 인해 발생하여 RLHF 훈련이 불안정해집니다. 이러한 문제를 완화하기 위해 우리는 RLHF 훈련 프로세스를 안정화하기 위해 고안된 혁신적인 전략인 Advantage Model과 Selective Rehearsal을 도입했습니다. Advantage Model은 다양한 범주와 사례에 걸쳐 균형 잡힌 보상 점수 분포를 유지하여 보상 해킹으로 인해 발생하는 합병증을 방지하는 것을 목표로 합니다. 반면, Selective Rehearsal은 PPO 훈련에 대한 최적의 사례, PPO 훈련에 대한 최적의 사례를 선택적으로 식별하여 SFT 단계에서 중요한 지식을 유지하도록 장려하고 시간이 지남에 따라 성과가 저하되는 것을 방지합니다. 다양한 데이터 세트에 대해 수행된 실증 분석은 제안된 기술의 효능을 입증했으며, 이는 RLHF 훈련의 안정성을 향상시켰을 뿐만 아니라 SFT 모델의 보상 점수와 승률도 개선했습니다. 참고 문헌 Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma 등. 정렬을 위한 실험실로서의 일반 언어 보조원. arXiv 사전 인쇄 arXiv:2112.00861, 2021. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 인간 피드백을 통한 강화 학습을 통해 유용하고 무해한 어시스턴트를 훈련합니다. arXiv 사전 인쇄 arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon 등 헌법적 AI: AI 피드백의 무해성. arXiv 사전 인쇄본 arXiv:2212.08073, 2022b. 톰 브라운, 벤자민 맨, 닉 라이더, 멜라니 수비아, 재러드 D 카플란, 프라풀라 다리왈, 아빈드 닐라칸탄, 프라나브 샤얌, 기리쉬 사스트리, 아만다 애스켈, 외. 언어 모델은 소수 샷 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. 웨이린 치앙, 주오한 리, 지 린, 잉 셩, 장하오 우, 하오 장, 리안민 정, 시위안 셩, 용하오 셩, 조셉 E. 곤잘레스, 이온 스토이카, 에릭 P. 싱. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시킨 오픈소스 챗봇, 2023년 3월. URL https://lmsys.org/blog/2023-03-30-vicuna/. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei. 인간의 선호도에서 심층 강화 학습. 신경 정보 처리 시스템의 발전, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 수학 단어 문제를 풀기 위한 검증자 훈련. arXiv 사전 인쇄본 arXiv:2110.14168, 2021. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang. Raft: 생성 기반 모델 정렬을 위한 보상 순위 조정. arXiv 사전 인쇄본 arXiv:2304.06767, 2023. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 피해를 줄이기 위한 레드 팀 언어 모델: 방법, 스케일링 동작 및 얻은 교훈. arXiv 사전 인쇄본 arXiv:2209.07858, 2022. Tianyu Gao, Xingcheng Yao, Danqi Chen. Simcse: 문장 임베딩의 단순 대조 학습. 2021년 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 6894-6910쪽, 2021. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 타깃 인간 판단을 통해 대화 에이전트의 정렬 개선. arXiv 사전 인쇄본 arXiv:2209.14375, 2022. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 언어 모델링을 위한 강화된 자기 학습(휴식). arXiv 사전 인쇄본 arXiv:2308.08998, 2023. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort. 대규모 언어 모델의 지속적인 사전 학습: 모델을 (재)워밍하는 방법? arXiv 사전 인쇄본 arXiv:2308.04014, 2023. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. Deberta: disentangled attention을 사용한 디코딩 강화 bert. arXiv 사전 인쇄본 arXiv:2006.03654, 2020. Khimya Khetarpal, Matthew Riemer, Irina Rish, Doina Precup. 지속적인 강화 학습을 향하여: 리뷰와 관점. 인공지능 연구 저널, 75:14011476, 2022. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-대규모 언어 모델 정렬 민주화. arXiv 사전 인쇄본 arXiv:2304.07327, 2023. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg. 보상 모델링을 통한 확장 가능한 에이전트 정렬: 연구 방향. arXiv 사전 인쇄본 arXiv:1811.07871, 2018. Michael McCloskey와 Neal J Cohen. 연결주의 네트워크에서의 재앙적 간섭: 순차적 학습 문제. Psychology of learning and motivation, 24권, 109-165쪽. Elsevier, 1989. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 멀티태스크 미세 조정을 통한 교차 언어 일반화. arXiv 사전 인쇄본 arXiv:2211.01786, 2022. R OpenAI. Gpt-4 기술 보고서. arXiv, pp. 2303-08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35: 27730-27744, 2022. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. gpt-4를 사용한 지시 튜닝. arXiv 사전 인쇄본 arXiv:2304.03277, 2023. Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu. Valuenet: 인간의 가치 중심 대화 시스템을 위한 새로운 데이터 세트. AAAI 인공지능 컨퍼런스 회의록, 36권, 11183-11191쪽, 2022년. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. arXiv 사전 인쇄본 arXiv:2305.18290, 2023년. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 근접 정책 최적화 알고리즘. arXiv 사전 인쇄본 arXiv:1707.06347, 2017년. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, David Krueger. 보상 게임 정의 및 특성화. 신경 정보 처리 시스템의 발전, 35:9460–9471, 2022. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang. 인간 정렬을 위한 선호도 순위 최적화. arXiv 사전 인쇄본 arXiv:2306.17492, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano. 인간 피드백을 통한 요약 학습. 신경 정보 처리 시스템의 발전, 33:3008-3021, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 라마 2: 오픈 기반 및 미세 조정된 채팅 모델. arXiv 사전 인쇄본 arXiv:2307.09288, 2023. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi. 자체 지시: 언어 모델을 자체 생성 지시와 정렬. arXiv 사전 인쇄본 arXiv:2212.10560, 2022. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 낙타는 얼마나 멀리 갈 수 있을까? 오픈 리소스에 대한 명령어 튜닝 상태 탐구. arXiv 사전 인쇄본 arXiv:2306.04751, 2023. Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun. Instructiongpt-4: minigpt-4 미세 조정을 위한 200개 명령어 패러다임. arXiv 사전 인쇄본 arXiv:2308.12067, 2023. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang. Rrhf: 눈물 없이 언어 모델을 인간 피드백과 정렬하기 위한 순위 응답. arXiv 사전 인쇄본 arXiv:2304.05302, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: 정렬은 적을수록 더 좋다. arXiv 사전 인쇄본 arXiv:2305.11206, 2023.
