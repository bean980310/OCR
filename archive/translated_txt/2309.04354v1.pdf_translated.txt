--- ABSTRACT ---
희소 전문가 혼합 모델(MoE)은 주어진 입력 토큰에 대해 모델 매개변수의 작은 하위 집합만 활성화하여 모델 크기를 추론 효율성에서 분리할 수 있는 능력으로 인해 최근 인기를 얻었습니다. 따라서 희소 MoE는 전례 없는 확장성을 가능하게 하여 자연어 처리 및 컴퓨터 비전과 같은 도메인에서 엄청난 성공을 거두었습니다. 이 작업에서는 대신 희소 MoE를 사용하여 Vision Transformers(ViT)를 축소하여 리소스가 제한된 비전 애플리케이션에 더 매력적으로 만드는 방법을 살펴봅니다. 이를 위해 개별 패치가 아닌 전체 이미지가 전문가에게 라우팅되는 단순화되고 모바일 친화적인 MoE 설계를 제안합니다. 또한 라우터를 안내하기 위해 슈퍼클래스 정보를 사용하는 안정적인 MoE 학습 절차를 제안합니다. 경험적으로 희소 모바일 비전 MOES(V-MOE)가 해당 밀집 ViT보다 성능과 효율성 사이에서 더 나은 균형을 이룰 수 있음을 보여줍니다. 예를 들어, ViT-Tiny 모델의 경우, 당사의 Mobile V-MoE는 ImageNet1k에서 dense 대응 모델보다 3.39% 더 나은 성능을 보였습니다. 추론 비용이 54M FLOPS에 불과한 훨씬 더 작은 ViT 변형 모델의 경우, 당사의 MoE는 4.66%의 개선을 달성했습니다. 1.
--- EXPERIMENT ---
s, 우리가 잘 작동하는 것으로 발견). 이러한 슈퍼 클래스 구분은 종종 데이터 세트와 함께 쉽게 제공됩니다(예: CIFAR-10/100 또는 MS-COCO의 경우). 데이터 세트에 슈퍼 클래스 구분이 제공되지 않으면 다음과 같이 쉽게 얻을 수 있습니다. 1) 먼저 데이터 세트에서 고밀도 기준 모델을 학습합니다. 2) 그런 다음 보류된 검증 세트에 대한 모델의 혼동 행렬을 계산합니다. 3) 마지막으로 혼동 행렬에서 혼동 그래프를 구성하고 그래프 클러스터링 알고리즘을 적용하여 슈퍼 클래스 구분을 얻습니다[8]. 이 접근 방식은 상위 클래스가 의미적으로 유사한 이미지를 포함하도록 장려합니다.클래스 복서, 퍼그, 로트바일러 오랑우탄, 족제비, 팬더 투칸, 플라밍고, 타조 장어, 전갈, 망치머리 미니밴, 구급차, 택시 잠수함, 카누, 해적 과카몰리, 핫도그, 바나나 백팩, 파자마, 기모노 모니터, 아이팟, 복사기, 실로폰, 하프, 트럼펫 상위 클래스 개 다른 포유류 새 다른 동물 육상 차량 해상 차량 식품 의류 기술 장치 악기 표 1. E = 10에 대한 상위 클래스 구분. 각 상위 클래스에 대해 의미적으로 관련이 있는 것으로 밝혀진 세 개의 무작위로 선택된 클래스 이름과 가능한 상위 클래스 이름을 함께 나열합니다. 모델이 종종 혼동하는 이미지입니다. 직관적으로, 다른 MoE 전문가가 다른 의미 데이터 클러스터를 전문으로 할 수 있도록 하면 혼동이 심한 클래스의 성능이 향상될 것입니다. 우리는 ImageNet-1k에 대한 실험에서 이 접근 방식을 사용하여 밀집 ViT-S/16 모델을 통해 혼동 행렬을 계산합니다. E 10 전문가에 대한 결과적인 슈퍼클래스 구분은 표 1에 나와 있습니다. 슈퍼클래스에는 의미적으로 관련된 클래스가 포함됩니다. 3. 실험 = 이제 표준 ImageNet-1k 분류 벤치마크[14]에 대한 경험적 결과를 제시합니다. 128만 개의 이미지로 구성된 ImageNet-1k 학습 세트에서 모든 모델을 처음부터 학습한 다음 50,000개의 이미지로 구성된 보류된 검증 세트에서 상위 1개 정확도를 평가합니다. 3.1절에서는 먼저 다양한 모델 규모에서 제안한 희소 모바일 V-MoE를 평가하고 해당 밀집 ViT 기준선보다 성능 대 효율성 트레이드오프가 더 우수함을 보여줍니다. 3.2절에서는 제안한 희소 MoE 모델 설계와 학습 절차의 속성을 더 잘 이해하기 위해 여러 가지 절제 연구를 수행합니다. 3.1. ViT 스케일에 따른 정확도 대 효율성 우리는 총 층 수(12, 9 또는 6개 사용)와 은닉 임베딩 크기(384, 192, 96 또는 64 사용)를 조정하여 다양한 크기의 ViT 모델(MoE와 해당 고밀도 베이스라인 모두)을 고려합니다. 다양한 은닉 임베딩 크기에 대한 다중 헤드 셀프 어텐션 헤드의 수는 (6, 3, 3, 2)입니다. MLP의 임베딩 크기는 일반적인 관행대로 은닉 임베딩 크기의 4배입니다. 우리는 MoE에 총 10명의 전문가를 사용하며, 이 중 k =는 입력 이미지당 활성화됩니다. 우리의 MoE는 (10, 7 또는 4)개의 고밀도 ViT 층 뒤에 오는 L =MOE-VIT 층으로 구성됩니다(그림 2c 참조). 우리는 모든 모델에 32 × 32의 패치 크기를 사용합니다. 이는 패치 크기가 E =Model FLOPS Top-1 정확도 E 라우터 MOE A k FLOPS Dense MOE Δ Dense MoE A12x9x6x2297M 71.88 74.23 +2.1752M 69.94 72.1207M 63.21 66.+2.+3.86.43 72.33 +1.87.43 73.13 +2.4487.12 73.52 +2.8384.16 73.10 +2.4184.08 73.36 +2.2534M 2769M 71.70 74.42 +2.3005M 73.58 74.44 +0.3476M 74.87 74.32를 효과적으로 제어하기 때문입니다. -0.4653M 75.10 74.37 -0.70.79 73.44 +2.12x192¹ 618M 9x478M 56.59.51 62.59.6x338M 51.12x176M 53.9×140M 6x103M 46.51.27 52.50.+3.+3.55.69 +4.55.39 +1.+1.(b) 전문가 총 수 E. (d) 이미지당 전문가 수 k. L 라우터 MOE Δ 라우팅 입력 Acc. Δ+3.12×88M 42.90 46.+3.82.9x6×71M 40.46 43.54M 36.64 41.+3.+4.72.90.17 72.14 +1.87.12 73.52 +2.71.67 +0.77.70 70.07 -0.64.47 -6.Dense Super-class N/A 71.image 74.42 +2.Rand.class End-to-end End-to-end image 69.22 -2.image 73.+1.(c) MoE 레이어 수 L. (a) ViT 스케일에 따른 정확도 대 효율성. 토큰 74.(e) 라우팅 전략. +3.그림 3. 경험적 결과. (a) 당사의 모바일 V-MoE는 모델 규모에서 각각의 고밀도 ViT보다 성능이 뛰어납니다. 모델 이름(예: 12×192)은 레이어 수(12)와 임베딩 크기(192)를 나타냅니다. (be) 기본적으로 k = 1, E = 10, L = 2인 DeiT-Ti/16[17]을 사용한 소거 연구. (b) E 10 전문가 총계, (c) L = 2 MoE 레이어(총 12개 레이어 중), (d) 이미지당 활성화된 전문가 k = 1 또는 k = 2, (e) 당사의 의미적 슈퍼클래스 라우팅을 통해 최상의 성능 대 효율성 트레이드오프가 달성됩니다. (a)에서 사용된 설정은 굵은 글씨로 표시했습니다. FLOP와 모델 매개변수 수 간의 트레이드오프: FLOP를 최적화하는 것을 목표로 하므로 더 큰 패치 크기(패치 수가 줄어듬)가 유익합니다. 또한 16×16의 더 작은 패치 크기를 사용해 보았는데, 여기서 결과 추세는 기본적으로 동일했습니다(그러나 FLOPS 수가 모델 용량과 정확도에 비해 더 높았습니다).숨겨진 크기가 384 및 192인 ViT의 경우 DeiT 학습 레시피[17]를 사용하는 반면, 숨겨진 크기가 96 및 64인 경우 과소적합을 피하기 위해 표준 ViT 학습 레시피[2]를 사용합니다.그림 1과 3a는 상위 1 검증 정확도와 FLOP를 비교합니다.저희의 모바일 V-MoE는 모든 모델 크기에서 해당 고밀도 ViT 기준선보다 성능이 뛰어납니다.3.2. 절제 연구 = 저희는 입력당 E = 10명의 전문가 중 k 1과 L = 2개의 MoE 층(달리 언급되지 않는 한)을 사용하여 DeiT-Tiny[17](총 12개 층, 192개 임베딩 크기, 16×16 패치 크기)를 학습합니다.고밀도 ViT 기준선은 70.79%의 정확도를 달성합니다. 전문가의 총 수 E. MoE의 다른 폭, 즉 전문가의 다른 수 E(따라서 슈퍼 클래스)를 고려하며 범위는 E 5와 E = 20 사이입니다. 전체 MoE 모델(즉, 1,000방향 분류 작업)의 정확도와 라우터의 정확도(즉, E방향 슈퍼 분류 작업)를 모두 보고합니다. 그림 3b는 전체 성능이 E = 10까지 향상되다가 그 지점부터 정체되는 것을 보여줍니다. 라우터 정확도도 E방향 슈퍼 분류 문제의 난이도가 높아져 E 10을 넘어서면 떨어집니다. MoE 층 수 L. MoE의 다른 깊이, 즉 MoE 층 수 L을 고려합니다. 범위1 이는 패치 크기가 32 × 32인 ViT-Tiny 모델[17]에 해당합니다. = L = ― L = 1과 L = 8(총 12개의 ViT 층 중) 사이입니다. 다시 전체 MoE와 라우터 정확도를 보고합니다.그림 3c는 전체 성능이 2에서 최고조에 달하고 L이 클수록 빠르게 감소함을 보여줍니다.이는 라우터 정확도로, 라우터가 얻는 정보가 줄어들면서(12L VIT 계층에서) L이 증가함에 따라 감소합니다.이미지당 전문가 수 k.이미지당 활성화되는 전문가 수 k를 변경합니다.MoE의 추론 FLOP와 일치하도록 숨겨진 차원이 k로 조정된 MLP를 사용하는 밀집 기준선과 비교합니다.그림 3d는 k = 1 및 k = 2가 더 큰 k에 대해 성능 델타가 감소하면서(밀집 기준선에 비해) 가장 좋은 성능을 보임을 보여줍니다.라우팅 전략.제안한 의미적 슈퍼클래스당 이미지 라우팅과 엔드투엔드 학습 라우팅(이미지당 및 토큰당) 및 무작위 슈퍼클래스가 있는 기준선(k=2)을 비교합니다. 그림 3e는 학습된 토큰별 라우팅(일반 V-MoE [13], 그림 2b에서와 같이)을 제외하고 우리 방법(그림 2c)이 더 나음을 보여줍니다. 그러나 이 경우 각 입력 이미지에 대해 훨씬 더 많은 전문가를 활성화하고 매개변수를 모델링해야 합니다(최대 11.05M, 우리 방법의 경우 6.31M).
--- CONCLUSION ---
s 및 향후 작업 우리는 ViT를 리소스가 제한된 애플리케이션에 더 적합하게 만들기 위해 밀도가 높은 ViT에 비해 희소한 MoE가 성능 대 효율성 트레이드오프를 개선할 수 있음을 보여주었습니다. 앞으로는 ViT보다 모바일 친화적인 모델, 예를 들어 MobileNets [5,6,15] 또는 ViT-CNN 하이브리드 [1, 11, 18]와 같은 경량 CNN에 MoE 설계를 적용하는 것을 목표로 합니다. 또한 객체 감지와 같은 다른 비전 작업도 고려하는 것을 목표로 합니다. 마지막으로 모든 모델에 대한 실제 온디바이스 지연 시간 측정값을 얻는 것을 목표로 합니다. 참고문헌 [1] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, Zicheng Liu. Mobileformer: 모바일넷과 변압기 연결. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 컨퍼런스 진행, 페이지 5270-5279, 2022. 2,[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly 등 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기입니다. arXiv 사전 인쇄 arXiv:2010.11929, 2020. 1,[3] William Fedus, Jeff Dean 및 Barret Zoph. 딥러닝의 희소 전문가 모델에 대한 검토입니다. arXiv 사전 인쇄 arXiv:2209.01667, 2022. 1, 2,[4] William Fedus, Barret Zoph 및 Noam Shazeer. 스위치 변압기: 간단하고 효율적인 희소성을 갖춘 1조 개 매개변수 모델로 확장.J. Mach. Learn. Res, 23:1-40, 2021. 1,[5] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Mobilenetv3 검색. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 1314-1324쪽, 2019. 2,[6] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. Mobilenets: 모바일 비전 애플리케이션을 위한 효율적인 합성곱 신경망. arXiv 사전 인쇄본 arXiv:1704.04861, 2017. 2,[7] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, Geoffrey E Hinton. 지역 전문가의 적응적 혼합. 신경 계산, 3(1):79–87, 1991.[8] Ruochun Jin, Yong Dou, Yueqing Wang, Xin Niu. 혼동 그래프: 대규모 이미지 분류에서 혼동 커뮤니티 감지. IJCAI, 1980–1986페이지, 2017.[9] Michael I Jordan 및 Robert A Jacobs. 전문가의 계층적 혼합과 em 알고리즘. 신경 계산, 6(2):181-214, 1994.[10] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen. Gshard: 조건부 계산 및 자동 샤딩을 사용한 거대 모델 확장. arXiv 사전 인쇄본 arXiv:2006.16668, 2020. 1,[11] Sachin Mehta 및 Mohammad Rastegari. Mobilevit: 가볍고 범용이며 모바일 친화적인 비전 변환기. arXiv 사전 인쇄본 arXiv:2110.02178, 2021. 2,[12] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton 및 Neil Houlsby. limoe를 사용한 다중 모달 대조 학습: 전문가의 언어-이미지 혼합. arXiv 사전 인쇄 arXiv:2206.02770, 2022. 1,[13] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers 및 Neil Houlsby. 소수의 전문가 조합으로 비전을 확장합니다. 신경 정보 처리 시스템의 발전, 34:8583-8595, 2021. 1, 2, 3,[14] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, 외. 이미지넷 대규모 시각인식 챌린지. 컴퓨터 비전 국제 저널, 115:211-252, 2015.[15] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. Mobilenetv2: 역 잔차 및 선형 병목 현상. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4510-4520페이지, 2018. 2,[16] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean. 엄청나게 큰 신경망: 희소 게이트 혼합 전문가 계층. arXiv 사전 인쇄본 arXiv:1701.06538, 2017. 1,[17] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. 데이터 효율적인 이미지 변환기 교육 및 주의를 통한 증류. 기계 학습에 관한 국제 컨퍼런스, 10347-10357페이지. PMLR, 2021.[18] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, Anurag Ranjan. 개선된 1밀리초 모바일 백본. arXiv 사전 인쇄본 arXiv:2206.04040, 2022. 2,[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. 1,[20] Zhao You, Shulin Feng, Dan Su, Dong Yu. Speechmoe: 전문가의 동적 라우팅 혼합을 통한 대규모 음향 모델로의 확장. arXiv 사전 인쇄 arXiv:2105.03036, 2021.[21] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer 및 William Fedus. 효과적인 희소 전문가 모델 설계. arXiv 사전 인쇄 arXiv:2202.08906, 2022. 1,
