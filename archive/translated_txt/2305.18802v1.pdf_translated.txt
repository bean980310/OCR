--- ABSTRACT ---
논문에서는 텍스트 음성 변환(TTS)에 사용하도록 설계된 &quot;LibriTTSR&quot;이라는 새로운 음성 데이터 세트를 소개합니다. 이 데이터 세트는 2,456명의 화자와 해당 텍스트에서 24kHz 샘플링 속도로 585시간 분량의 음성 데이터로 구성된 LibriTTS 코퍼스에 음성 복원을 적용하여 파생되었습니다. LibriTTS-R의 구성 샘플은 LibriTTS의 샘플과 동일하며, 음질만 개선되었습니다. 실험 결과에 따르면 LibriTTS-R의 기준 진실 샘플은 LibriTTS의 샘플에 비해 상당히 향상된 음질을 보였습니다. 또한 LibriTTS-R로 훈련된 신경 종단 간 TTS는 기준 진실 샘플과 동일한 수준의 음성 자연스러움을 달성했습니다. 이 코퍼스는 http://www.openslr.org/141/에서 무료로 다운로드할 수 있습니다. 색인 용어: 텍스트 음성 변환, 데이터 세트, 음성 복원 1.
--- METHOD ---
[32]. 코덱 아티팩트를 시뮬레이션하기 위해 MP3, Vorbis, A-law, Adaptive Multi-Rate Wideband(AMR-WB), OPUS 중 하나를 랜덤 비트 전송률로 랜덤하게 적용했습니다. 자세한 시뮬레이션 매개변수는 [20]에 나와 있습니다. 먼저 피처 클리너와 WaveFit 신경 보코더를 각각 150k 및 1M 단계로 사전 학습했으며, WaveFit은 깨끗한 w2v-BERT 피처에서 파형을 재구성하도록 학습했습니다. 그런 다음 사전 학습된 피처 클리너로 정리된 w2v-BERT 피처를 사용하여 WaveFit 신경 보코더 350k 단계를 미세 조정했습니다. 3.3. 음성 복원 파이프라인 먼저 [20]에 설명된 스피커 인코더를 사용하여 원본 24kHz 샘플링 파형에서 필사본과 스피커 임베딩에서 PnG-BERT [3] 피처를 계산했습니다. 여기서, 파형 길이가 2초보다 짧은 음성 샘플의 경우, 이를 반복하여 가짜로 더 긴 파형을 얻은 후 화자 임베딩을 계산했습니다. w2v-BERT [21] 모델은 16kHz 파형에서 학습되었으므로, w2v-BERT 특징을 계산하기 위해 LibriTTS 샘플에 다운 샘플링을 적용했습니다. 마지막으로, WaveFit [8]을 사용하여 복원된 24kHz 샘플링 파형을 합성했습니다. 4.
--- EXPERIMENT ---
모든 결과에 따르면 LibriTTS-R 기준 진실 샘플은 LibriTTS의 샘플에 비해 상당히 향상된 음질을 보였습니다. 또한 LibriTTS-R로 훈련한 신경 종단 간 TTS는 기준 진실 샘플과 동등한 수준의 자연스러운 음성을 달성했습니다. 이 코퍼스는 http://www.openslr.org/141/에서 무료로 다운로드할 수 있습니다. 색인 용어: 텍스트 음성 변환, 데이터 세트, 음성 복원 1. 서론 텍스트 음성 변환(TTS) 기술은 급속히 발전했습니다. 딥 러닝[1-6]의 개발과 함께 스튜디오 품질의 녹음된 음성 데이터를 통해 음향 모델[2,3]과 고충실도 신경 보코더[7,8]를 훈련할 수 있습니다. 이를 통해 인간의 음성과 거의 같은 자연스러운 독해 스타일로 음성을 합성할 수 있었습니다. 또한 최신 TTS 모델의 많은 구현이 게시되었으며[9,10] TTS 연구의 관문이 확실히 넓어지고 있습니다. 고품질 TTS 시스템을 개발하는 데 남은 장벽 중 하나는 방대하고 고품질의 공개 데이터 세트가 부족하다는 것입니다. 고품질 TTS 모델을 훈련하려면 스튜디오 품질의 데이터가 대량 필요합니다. 여러 TTS 논문에서 100시간 이상의 스튜디오 녹음 데이터가 사용되었습니다[3, 8, 11]. 안타깝게도 이러한 스튜디오 녹음 데이터 세트는 공개적으로 사용할 수 없으므로 다른 사람이 결과를 재생산하는 것이 어렵습니다. 동시에 음성 생성 모델을 사용하여 음성 복원(SR)이 발전했습니다[12-18]. 이러한 최첨단 모델은 반향 강의와 역사적 연설을 스튜디오 녹음 품질로 변환할 수 있습니다[16-18]. 이러한 결과에서 영감을 받아 SR을 공개 데이터 세트에 적용하면 위에 언급된 장벽을 제거할 수 있다는 아이디어를 내놓았습니다. 이 논문을 통해 LibriTTS의 품질이 개선된 버전인 LibriTTS-R을 게시합니다[19]. LibriTTS는 2,456명의 화자로부터 얻은 585시간 분량의 음성 데이터와 해당 텍스트로 구성된 제한 없는 라이선스 다중 화자 TTS 코퍼스입니다. w2v-BERT [21] 기능 클리너와 WaveFit 신경 보코더 [8]를 사용하는 텍스트 정보 SR 모델인 Miipher [20]를 적용하여 LibriTTS를 정리했습니다. 주관적인 실험을 통해 LibriTTSR로 학습한 TTS 모델의 음성 자연스러움이 LibriTTS로 학습한 모델보다 크게 향상되었으며 실제 결과와 비슷함을 보였습니다. LibriTTS-R은 동일한 제한 없는 라이선스로 http://www. openslr.org/141/에서 공개적으로 사용할 수 있습니다. 실제 결과 및 TTS에서 생성된 샘플의 오디오 샘플은 데모 페이지¹에서 사용할 수 있습니다. 2. LibriTTS 코퍼스 LibriTTS 코퍼스는 TTS 용도로 설계된 가장 큰 다중 화자 음성 데이터 세트 중 하나입니다. 이 데이터 세트는 2,456명의 화자로부터 24kHz 샘플링 속도의 시간 단위 음성 데이터와 해당 텍스트로 구성되어 있습니다. 오디오 및 텍스트 자료는 자동 음성 인식 시스템을 훈련하고 평가하는 데 사용된 LibriSpeech 코퍼스[22]에서 파생되었습니다. 원래 LibriSpeech 코퍼스에는 샘플링 속도 및 텍스트 정규화 문제를 포함하여 TTS에 대한 여러 가지 바람직하지 않은 속성이 있으므로 LibriTTS의 샘플은 LibriSpeech의 원래 자료(LibriVox의 MP3 및 Project Gutenberg의 텍스트)에서 다시 파생되었습니다. 한 가지 문제는 LibriTTS의 음질이 LJspeech[23]와 같은 더 작은 규모이지만 더 높은 품질의 TTS 데이터 세트와 동일하지 않다는 것입니다. TTS 출력의 품질은 모델 훈련에 사용된 음성 샘플의 품질에 크게 영향을 받습니다. 따라서 LibriTTS에서 훈련된 TTS 모델의 생성된 샘플의 품질은 기준 진실 샘플의 품질과 일치하지 않습니다[24, 25]. 예를 들어, Glow-TTS는 LibriTTS에서 3.mean-opinion-score(MOS)를 달성했는데, 보코더가 기준 진실 멜 스펙트로그램에서 얻은 음성은 4.22였습니다[24]. 생성된 음성과 기준 진실에 대한 LJspeech의 MOSS는 각각 4.01과 4.19였습니다[24]. 이 결과는 LibriTTS의 음성 샘플 품질이 고품질 TTS 모델을 학습하기에 부적절하다는 것을 시사합니다. 3. 데이터 처리 파이프라인 노이즈가 있는 TTS 데이터 세트는 고급 TTS 모델 학습에 유용하지만[26-28], 대규모 고품질 데이터 세트에 대한 액세스도 TTS 기술을 발전시키는 데 똑같이 중요합니다. 공개적인 대규모 고품질 TTS 데이터 세트를 제공하기 위해 LibriTTS에 SR 모델을 적용합니다. 3.1. 음성 복원 모델 개요 데이터 세트를 정리하기 위한 SR 모델의 중요한 요구 사항 중 하나는 견고성입니다. SR 모델이 아티팩트가 있는 많은 수의 샘플을 생성하는 경우 후속 TTS 모델 학습에 부정적인 영향을 미칩니다. 따라서 우리의 목적을 위해서는 복구에 실패한 샘플 수를 최대한 줄여야 합니다. https://google.github.io/df-conformer/ librittsr/ Libri TTS 샘플 24kHz 원본 파형 Frozen Speaker 인코더 대본 리샘플링 PnG-BERT w2v-BERT W2v-BERT 기능 클리너 WaveFit-Table 1: 95% 신뢰 구간이 있는 기준 진실 샘플에 대한 MOS 및 SxS 테스트 결과. 양의 SxS 점수는 LibriTTS-R이 선호되었음을 나타냅니다. MOS (†) 분할 SxS LibriTTS LibriTTS-R 테스트-정리 테스트-기타 4.36 ± 0.3.94 ± 0.4.41 0.4.09 ± 0.0.80 ± 0.1.42 ± 0.24 kHz 복원된 파형 Libri TTS-R 샘플 대본 그림 1: 데이터 처리 파이프라인 개요. LibriTTS 코퍼스의 음성 샘플은 Miipher [20]를 사용하여 복원됩니다. 이 요구 사항을 충족시키기 위해 그림 1에 표시된 것처럼 텍스트 정보 매개 변수 재합성 기반 SR 모델인 Miipher [20]를 사용합니다. 이 모델에서 먼저 w2v-BERT 특징은 w2v-BERT [21]에 의해 노이즈가 있는 파형에서 추출됩니다. 그런 다음 DFConformer [29] 기반 특징 클리너가 정리된 파형의 w2v-BERT 특징을 예측합니다. 마지막으로 복원된 파형은 WaveFit-5 신경 보코더[8]를 사용하여 합성됩니다. Miipher를 선택한 이유는 LibriTTS 샘플에서 관찰되는 두 가지 특히 복원하기 어려운 저하 패턴을 해결하기 때문입니다. 첫 번째 저하란 음소 마스킹입니다. 음성 신호는 때때로 잡음 및/또는 반향에 의해 마스킹되어 추가 정보 없이는 잡음과 구별하기 어려운 음성이 생성됩니다. 두 번째 저하란 음소 삭제입니다. 일부 음소의 중요한 주파수 부분은 비선형 오디오 처리 및/또는 다운 샘플링으로 인해 신호에서 누락될 수 있습니다. 이러한 문제를 해결하기 위해 Miipher는 두 가지 기술을 도입했습니다. (i) 입력 특징의 경우 기존 SR 모델[17]에서 사용되는 log-mel 스펙트로그램 대신 w2v-BERT[21] 특징을 사용하고, (ii) 잡음이 있는 음성에 해당하는 필사본에서 PnG-BERT[3]가 추출한 언어적 특징 컨디셔닝을 사용합니다. w2vBERT는 대량의 저하된 음성 샘플로 학습되었고 ASR 성능을 개선하기 때문에 음성 저하에 대해 SR 모델을 견고하게 만드는 데 효과적일 것으로 기대합니다. 또한 텍스트 정보를 사용하여 음성 인페인팅 성능[30]을 개선하므로 음성 복원 성능도 개선된다고 생각합니다. 자세한 내용은 원본 논문[20]을 참조하세요. 3.2. 음성 복원 모델 학습 2,680시간 분량의 노이즈가 있는 음성과 스튜디오 품질의 음성 쌍이 포함된 독점 데이터 세트로 Miipher 모델을 학습했습니다. 대상 음성 데이터 세트에는 24kHz 샘플링으로 스튜디오에서 녹음한 호주, 영국, 인도, 나이지리아, 북미 영어 670시간이 포함되어 있습니다. 노이즈 데이터 세트의 경우 TAU Urban Audio-Visual Scenes 2021 데이터 세트[31]를 사용했으며 카페, 주방, 자동차와 노이즈 소스와 같은 조건을 시뮬레이션하는 내부적으로 수집된 노이즈 스니펫을 사용했습니다. 잡음이 있는 발화는 신호 대 잡음비(SNR)가 5dB에서 30dB인 이러한 데이터 세트에서 무작위로 선택된 음성 및 잡음 샘플을 혼합하여 생성되었습니다. 또한 잔향 및 코덱 아티팩트의 존재 여부에 따라 4가지 패턴으로 잡음이 있는 데이터 세트를 증가시켰습니다. 각 샘플에 대한 실내 임펄스 응답(RIR)은 이미지 방법[32]을 사용하여 확률적 RIR 생성기로 생성했습니다. 코덱 아티팩트를 시뮬레이션하기 위해 MP3, Vorbis, A-law, Adaptive Multi-Rate Wideband(AMR-WB), OPUS 중 하나를 무작위 비트 전송률로 무작위로 적용했습니다. 자세한 시뮬레이션 매개변수는 [20]에 나와 있습니다. 먼저 특징 클리너와 WaveFit 신경 보코더를 각각 150k 및 1M 단계로 사전 학습했으며, 여기서 WaveFit은 깨끗한 w2v-BERT 특징에서 파형을 재구성하도록 학습되었습니다. 그런 다음 사전 훈련된 특징 클리너로 정리된 w2v-BERT 특징을 사용하여 WaveFit 신경 보코더를 350k 단계로 미세 조정했습니다.3.3. 음성 복원 파이프라인 먼저 [20]에 설명된 화자 인코더를 사용하여 원래 24kHz 샘플링 파형에서 전사본과 화자 임베딩에서 PnG-BERT [3] 특징을 계산했습니다.여기서 파형 길이가 2초보다 짧은 음성 샘플의 경우, 의사적으로 더 긴 파형을 얻기 위해 반복한 후 화자 임베딩을 계산했습니다.w2v-BERT [21] 모델은 16kHz 파형에서 훈련되었으므로 w2v-BERT 특징을 계산하기 위해 LibriTTS 샘플에 다운 샘플링을 적용했습니다.마지막으로 WaveFit [8]을 사용하여 복원된 24kHz 샘플링 파형을 합성했습니다.4. 실험 4.1. 기준 진실 샘플에 대한 주관적 실험 4.1.1. 실험 설정 먼저 LibriTTS-R의 지상 진실 음성 샘플의 품질을 LibriTTS의 지상 진실 음성 샘플과 비교했습니다. &quot;test-clean&quot; 및 &quot;test-other&quot; 하위 집합을 사용하여 음질을 평가했습니다. 각 하위 집합에서 무작위로 620개의 샘플을 선택했습니다. &quot;train*&quot; 및 &quot;dev-*&quot; 하위 집합도 동일한 단어 오류율(WER) 기반 기준에 따라 &quot;clean&quot; 및 &quot;other&quot;로 구분되므로 이 두 하위 집합의 음질을 평가하여 전체 데이터 세트의 음질을 예측할 수 있습니다. 주관적 품질을 평가하기 위해 평균 의견 점수(MOS) 및 나란히(SxS) 선호도 테스트를 통해 음성 품질을 평가했습니다. MOS 테스트에서 자연스러움을 평가하고 SxS 테스트에서 &quot;어느 음질이 더 낫습니까?&quot;라고 물었습니다. MOS의 척도는 5점 척도(1: 나쁨, 2: 나쁨, 3: 보통, 4: 좋음, 5: 매우 좋음)로 평가 간격은 0.5이고, SxS는 7점 척도(-3~3)였습니다. 테스트 자극은 무작위로 선택되었고 각 자극은 한 명의 피험자가 평가했습니다. 각 피험자는 최대 6개의 자극을 평가할 수 있었습니다. 즉, 이 실험에는 100명 이상의 피험자가 참여하여 각 조건에서 640개의 샘플을 평가했습니다. 피험자는 미국에서 돈을 받고 영어를 모국어로 사용하는 사람들이었습니다. 그들은 조용한 방에서 헤드폰을 사용하도록 요청받았습니다. 오디오 샘플은 데모 페이지¹에서 제공됩니다. 4.1.2. 결과 표 1은 MOS 및 SxS 테스트 결과를 보여줍니다. 음성 자연성 측면에서 LibriTTS는 높은 MOSS를 달성했습니다. 테스트-클린 및 테스트-기타에서 각각 4.36 및 3.94입니다. LibriTTS-R이 두 분할에서 LibriTTS보다 더 나은 MOSS를 달성했지만, 차이Mel-filter 지수Mel-filter 지수0+1.3.25.10.1.3.02.4.시간 [초] 시간 [초] 시간 [초] 시간 [초] 그림 2: (위) LibriTTS 및 (아래) LibriTTS-R의 기준 진실 파형의 로그-멜 스펙트로그램. 왼쪽 두 개와 오른쪽 두 개의 예는 각각 &quot;test-clean&quot; 및 &quot;test-other&quot; 분할에서 가져온 것입니다. 자연스러움에서 차이가 작은 이유는 기준 진실 샘플이 사람이 말한 실제 음성이기 때문일 수 있습니다. 반면, SxS 테스트로 평가한 음질 측면에서 두 분할에서 상당한 차이가 관찰되었습니다. 복원된 음성 샘플의 텍스트 내용과 화자가 유지되는지 확인하기 위해 WER 및 화자 유사도를 평가했습니다. WER을 계산하기 위해 [33]에서 제안한 &quot;Pre-trained Conformer XXL&quot; 모델을 사용했습니다. LibriTTS의 &quot;test-clean&quot; 및 &quot;test-other&quot; 분할의 WER은 각각 3.4 및 5.1인 반면 LibriTTS-R의 WER은 각각 3.2 및 5.1이었습니다². 따라서 텍스트 내용은 변경되지 않은 것으로 간주됩니다. 화자 유사도를 평가하기 위해 화자 임베딩의 코사인 유사도를 사용했습니다[34, 35]. 동일한 데이터 세트에서 동일한 화자가 말한 다른 발화 간의 유사도를 계산했습니다. 이는 LibriTTS의 샘플이 왜곡되어 있기 때문이며 LibriTTS와 LibriTTS-R의 해당 샘플 간의 유사도가 작더라도 반드시 화자 유사도를 나타내는 것은 아닙니다. LibriTTS의 &quot;test-clean&quot;과 &quot;test-other&quot; 분할의 코사인 유사도는 0.784와 0.755였고, LibriTTS-R의 코사인 유사도는 0.762와 0.745였습니다. 다른 화자가 말한 LibriTTS의 샘플에서 계산된 유사도는 0.302였으므로 각 화자의 음성 특성은 일관된 것으로 간주됩니다. 그림 2는 LibriTTS와 LibriTTS-R의 음성 샘플의 128-dim log-mel 스펙트로그램을 보여줍니다. LibriTTS 샘플은 test-clean 분할에서 나온 것이라 하더라도 다양한 요인에 의해 저하된 것을 볼 수 있습니다. 왼쪽에서 오른쪽으로 음성 샘플은 각각 다운샘플링, 환경 잡음, 잔향 및 비선형 음성 향상에 의해 저하된 것으로 간주할 수 있습니다. LibriTTS-R 샘플의 스펙트로그램을 볼 수 있듯이 SR 모델은 이러한 음성 샘플을 고품질 샘플로 복원했습니다. 이것이 상당한 차이의 이유일 수 있습니다. SxS 테스트에서. LibriTTS 음성 샘플이 SxS 비교에서 더 나은 점수를 얻은 몇 가지 예를 찾았습니다. 이러한 예를 들어보면 640개의 LibriTTS-R 음성 샘플 중 2개가 SR 실패로 인해 왜곡되었습니다. ASR 모델이 잡음이 있는 음성과 다른 텍스트 정규화기로 정규화된 필사본에 대해 학습되었기 때문에 2WER이 원래 논문[33]에서 보고된 것보다 약간 더 높았습니다. 모든 샘플을 수동으로 검사하기 위해 LibriTTS-R의 모든 음성 샘플을 검사하지 않았습니다. 따라서 훈련 분할의 샘플에도 소수의 왜곡된 샘플이 포함될 수 있습니다. 4.2. TTS에서 생성된 샘플에 대한 주관적 실험 4.2.1. 실험 설정 LibriTTS 또는 LibriTTS-R 코퍼스를 사용하여 동일한 아키텍처와 동일한 하이퍼 매개변수를 가진 다중 화자 TTS 모델을 훈련했습니다. TTS 모델은 조인트 미세 조정 없이 다음 음향 모델과 신경 보코더를 연결하여 구축되었습니다. 음향 모델: 우리는 fine-grained variational autoencoder(FVAE) [11]를 갖춘 지속 시간 비지도 NonAttentive Tacotron(NAT)을 사용했습니다. 우리는 원래 논문 [11]에 나열된 동일한 하이퍼 매개변수와 교육 매개변수를 사용했습니다. 우리는 1,024의 배치 크기로 150k 단계에 대해 이 모델을 교육했습니다. 신경 보코더: 우리는 512개의 은닉 유닛이 있는 단일 장기 단기 메모리 계층, 멜 스펙트로그램 피처를 처리하기 위한 컨디셔닝 스택으로 512개의 채널을 가진 5개의 합성곱 계층, 출력 계층으로 로지스틱 분포의 10개 구성 요소 혼합물로 구성된 WaveRNN [36]을 사용했습니다. 학습률은 첫 번째 단계에서 선형적으로 10-4로 증가한 다음 200k에서 300k 단계로 10-6으로 지수적으로 감소했습니다. 우리는 512의 배치 크기로 500k 단계에 대해 Adam 최적화 도구[37]를 사용하여 이 모델을 훈련했습니다.TTS 모델은 Train-460과 Train-960의 두 가지 유형의 훈련 데이터 세트에서 훈련되었습니다.Train-460은 &quot;train-clean-100&quot;과 &quot;train-clean-360&quot; 하위 집합으로 구성되며, Train-indicates는 Train-460에 더하여 &quot;train-other-500&quot;을 사용합니다.테스트 문장의 경우, 우리는 test-clean 분할에서 620개의 평가 문장을 무작위로 선택했습니다.우리는 LibriTTS 기준 실험[19]에 사용된 6명의 화자(여성 3명과 남성 3명)로 파형을 합성했습니다.여성과 남성 독자 ID는 각각 (19, 103, 1841)과 (204, 1121, 5717)이었습니다.주관적인 품질을 평가하기 위해 MOS 및 나란히(SxS) 선호도 테스트를 통해 음성의 자연스러움을 평가했습니다. 청취 테스트 설정은 Sec. 4.1과 동일했습니다. 생성된 음성의 오디오 샘플은 데모 페이지¹에서 사용할 수 있습니다. Mel-filter index Mel-filter index 스피커 ID: 스피커 ID: 스피커 ID: 스피커 ID: 1.2.6 시간 [초] 1. 시간 [초] 2.5 1.2.5 1.2. 시간 [초] 시간 [초] 그림 3: 다중 스피커 TTS 모델이 각각 (위) LibriTTS 및 (아래) LibriTTS-R에서 학습된 TTS 생성 파형의 Log-mel 스펙트로그램. 입력 텍스트는 &quot;The Free State Hotel served as barracks&quot;였습니다. 표 2: 95% 신뢰 구간이 있는 기준 다중 스피커 TTS 모델 출력에 대한 MOSS. 학습 데이터 세트 Speaker ID LibriTTS Train-LibriTTS Train-LibriTTS-R Train-LibriTTS-R Train-2.49 ± 0.2.59 0.4.11 ± 0.4.06 ± 0.2.94 ± 0.2.75 ± 0.4.09 ± 0.4.31 ± 0.3.40 ± 0.3.35 0.3.88 ± 0.4.20 ± 0.2.88 ± 0.2.74 ± 0.3.67 ± 0.4.11 ± 0.2.72 ± 0.2.830.3.92 0.4.23 ± 0.2.86 ± 0.2.97 ± 0.3.67 0.4.08 0.표 3: 기준 다중 스피커 TTS 모델 출력에 대한 SxS 테스트 결과 95% 신뢰 구간. 양의 점수는 LibriTTS-R에서 학습하는 것이 선호되었음을 나타냅니다. 학습 데이터 세트 Train-Speaker ID2.38 0.1.84 ± 0.Train-2.510.2.20 ± 0.4.2.2. 결과 표 2는 MOS 결과를 보여줍니다. ID 19를 제외한 모든 화자 ID에서 LibriTTS-R Train-960을 학습 데이터 세트로 사용한 TTS 모델이 가장 높은 MOS를 달성했습니다. 화자 ID 19의 경우 LibriTTS-R Train-460을 사용한 모델이 가장 높은 MOS를 달성했는데, 이는 LibriTTSR Train-960을 사용한 모델과 크게 다르지 않았습니다. 다른 화자 ID에서 LibriTTS-R Train960의 MOSS는 LibriTTS-R Train-460의 MOSS보다 상당히 우수했습니다. 이러한 추세는 LibriTTS에서는 관찰되지 않았고, 어떤 경우에는 LibriTTS Train-960을 사용함으로써 MOS가 감소했습니다. 이러한 저하의 이유는 &quot;train-other500&quot; 분할에 저하된 음성 샘플이 많이 포함되어 있기 때문일 수 있습니다. 이 결과는 LibriTTS &quot;train-other-500&quot; 분할을 사용하면 TTS의 출력 품질이 오히려 저하됨을 시사합니다. 반면 LibriTTS-R &quot;train-other-500&quot; 분할의 음성 샘플은 고품질 음성 샘플로 복원되어 많은 양의 고품질 학습 데이터를 사용하고 TTS 출력의 자연스러움을 개선할 수 있습니다. 또한 LibriTTS-R Train-960에서 학습한 TTS 모델은 LibriTTS에서 사람이 말한 음성 샘플과 동등한 MOSS를 달성했으며, 학습에서 왜곡된 음성 샘플 몇 개의 효과는 크지 않은 것으로 간주할 수 있습니다. 표 3은 SxS 결과를 보여줍니다. LibriTTS-R을 사용하면 TTS 출력의 자연스러움뿐만 아니라 음질도 향상되는 것을 관찰했습니다. 그림 3은 TTS 출력의 128-dim log-mel 스펙트로그램을 보여줍니다. 다음을 볼 수 있습니다. 고조파 구조는 LibriTTS에서 학습된 TTS 모델의 ID 5717 출력에서 끊어집니다(오른쪽 위). 이러한 샘플이 있는 것이 MOS 테스트에서 자연스러움 점수가 낮은 이유일 수 있습니다. 또한 ID 103과 1121 예에서 LibriTTS에서 학습된 TTS 모델의 출력에서 배경 잡음을 관찰할 수 있습니다. 이러한 배경 잡음은 LibriTTS-R에서 학습된 TTS 모델의 출력에는 존재하지 않습니다. 이러한 결과에서 LibriTTS-R 코퍼스가 LibriTTS 코퍼스보다 더 나은 TTS 코퍼스이며, 고품질 TTS 모델을 학습할 수 있다는 결론을 내립니다. 5.
--- CONCLUSION ---
s 이 논문에서는 LibriTTS의 음질이 개선된 버전인 LibriTTS-R을 소개했습니다[19]. 우리는 SR 모델[20]을 적용하여 LibriTTS 코퍼스에서 음성 샘플을 정리했습니다. 주관적인 실험을 통해 LibriTTS-R로 학습한 TTS 모델의 음성 자연스러움이 LibriTTS로 학습한 모델보다 향상되었으며 실제 결과와 비슷함을 보였습니다. 이 코퍼스는 온라인에 공개되었으며 http://www.openslr.org/141/에서 무료로 다운로드할 수 있습니다. 이 코퍼스의 공개로 TTS 연구가 가속화되기를 바랍니다. 6. 참고문헌 [1] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, K. Kavukcuoglu, &quot;WaveNet: 원시 오디오를 위한 생성 모델,&quot; arXiv:1609.03499, 2016. [2] J. Shen, R. Pang, RJ Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, RA Saurous, Y. Agiomvrgiannakis, Y. Wu, &quot;mel 스펙트로그램 예측에 대한 WaveNet 조건 지정을 통한 자연스러운 TTS 합성,&quot; Proc. ICASSP, 2018. [3] Y. Jia, H. Zen, J. Shen, Y. Zhang, 및 Y. Wu, &quot;PnG BERT: 신경 TTS를 위한 음소 및 문자에 대한 증강 BERT,&quot; Proc. Interspeech, 2021. [4] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, RJ Weiss, 및 Y. Wu, &quot;병렬 타코트론: 비자기회귀 및 제어 가능한 TTS,&quot; Proc. ICASSP, 2021. [5] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, 및 T.-Y. Liu, &quot;FastSpeech: 빠르고 견고하며 제어 가능한 텍스트 음성 변환,&quot; Proc. NeurIPS, 2019. [6] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao 및 T.-Y. Liu, &quot;FastSpeech 2: 빠르고 고품질의 엔드투엔드 텍스트 음성 변환,&quot; Proc. Int. Conf. Learn. Represent. (ICLR), 2021. [7] J. Kong, J. Kim 및 J. Bae, &quot;HiFi-GAN: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크&quot;, Proc. NeurIPS, 2020. [8] Y. Koizumi, K. Yatabe, H. Zen 및 M. Bacchiani, &quot;WaveFit: 고정점 반복을 기반으로 하는 반복적이고 비자기회귀 신경 보코더&quot;, Proc. IEEE SLT, 2023. [9] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang 및 X. Tan, &quot;ESPnet-TTS: 통합되고 재현 가능하며 통합 가능한 오픈 소스 종단 간 텍스트 음성 변환 툴킷&quot;, Proc. ICASSP, 2020. [10] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. 추, S.-L. 응, S.-W. 푸, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. De Mori 및 Y. Bengio, &quot;SpeechBrain: 범용 음성 도구 키트&quot; arXiv:2106.04624, 2021. [11] J. Shen, Y. Jia, M. Chrzanowski, Y. Zhang, I. Elias, H. Zen 및 Y. Wu, &quot;비주의 타코트론: 비지도 기간 모델링을 포함한 강력하고 제어 가능한 신경 TTS 합성,&quot; arXiv:2010.04301, 2020. [12] S. Maiti 및 MI Mandel, &quot;신경 보코더를 사용한 파라메트릭 재합성&quot;, Proc. IEEE WASPAA, 2019. [13] &quot;신경 보코더의 화자 독립성과 매개 변수 재합성 음성 향상에 미치는 영향&quot;, Proc. ICASSP, 2020. [14] T. Saeki, S. Takamichi, T. Nakamura, N. Tanji, and H. Saruwatari, &quot;SelfRemaster: 채널 모델링을 사용한 분석-합성 접근법을 통한 자체 감독 음성 복원&quot;, Proc Interspeech, 2022. [15] J. Su, Z. Jin, and A. Finkelstein, &quot;HiFi-GAN: 적대적 네트워크의 음성 딥 피처를 기반으로 한 고충실도 잡음 제거 및 반향 제거&quot;, Proc. Interspeech, 2020. [16] —, &quot;HiFi-GAN-2: 음향 피처에 따라 조건화된 생성적 적대적 네트워크를 통한 스튜디오 품질의 음성 향상&quot;, Proc. IEEE WASPAA, 2021. [17] H. Liu, X. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, &quot;VoiceFixer: 고충실도 음성 복원을 위한 통합 프레임워크,&quot; Proc Interspeech, 2022. [18] J. Serrà, S. Pascual, J. Pons, RO Araz, and D. Scaini, &quot;점수 기반 확산을 통한 범용 음성 향상,&quot; arXiv:2206.03065, 2022. [19] H. Zen, R. Clark, RJ Weiss, V. Dang, Y. Jia, Y. Wu, Y. Zhang, and Z. Chen, &quot;LibriTTS: 텍스트 음성 변환을 위한 LibriSpeech에서 파생된 코퍼스,&quot; Proc. Interspeech, 2019. [20] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, Y. Zhang, W. Han, A. Bapna 및 M. Bacchiani, &quot;Miipher: 자기 지도 음성 및 텍스트 표현을 통합한 강력한 음성 복원 모델&quot;, arXiv:2303.01664, 2023. [21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang 및 Y. Wu, &quot;w2v-BERT: 자기 지도 음성 사전 훈련을 위한 대조 학습 및 마스크 언어 모델링 결합&quot;, Proc. IEEE ASRU, 2021. [22] V. Panayotov, G. Chen, D. Povey 및 S. Khudanpur, “도서관 연설: AN 공개 도메인 오디오 북을 기반으로 한 ASR 코퍼스,&quot; Proc. ICASSP, 2015. [23] K. Ito 및 L. Johnson, &quot;lj 음성 데이터 세트,&quot; https://keithito. com/LJ-Speech-Dataset/, 2017. [24] J. Kim, S. Kim, J. Kong 및 S. Yoon, &quot;Glow-tts: 단조 정렬 검색을 통한 텍스트 음성 합성을 위한 생성 흐름,&quot; Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2020. [25] R. Valle, KJ Shih, R. Prenger 및 B. Catanzaro, &quot;Flowtron: 텍스트 음성 합성을 위한 자기 회귀 흐름 기반 생성 네트워크,&quot; Proc. Int. Conf. Learn. Represent. (ICLR), 2021. [26] E. Casanova, J. Weber, C. Shulby, AC Junior, E. Gölge, 및 M. Antonelli Ponti, &quot;YourTTS: 모든 사람을 위한 제로샷 멀티스피커 TTS 및 제로샷 음성 변환을 향해,&quot; arXiv:2112.02418, 2021. [27] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, 및 F. Wei, &quot;신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다,&quot; arXiv:2301.02111, 2023. [28] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, 및 N. Zeghidour, &quot;말하고, 읽고, 촉구: 고충실도 텍스트 음성 변환 영어: 최소한의 감독으로,&quot; arXiv:2302.03540, 2023. [29] Y. Koizumi, S. Karita, S. Wisdom, H. Erdogan, JR Hershey, L. Jones, and M. Bacchiani, &quot;DF-Conformer: 음성 향상을 위한 선형 복잡도 자기 주의를 사용하는 Conv-TasNet 및 Conformer의 통합 아키텍처&quot;, Proc. IEEE WASPAA, 2021. [30] Z. Borsos, M. Sharifi, and M. Tagliasacchi, &quot;SpeechPainter: 텍스트 조건화된 음성 인페인팅&quot;, Proc. 영어: Interspeech, 2022. [31] S. Wang, A. Mesaros, T. Heittola 및 T. Virtanen, &quot;시청각 장면 분석을 위한 큐레이트된 도시 장면 데이터 세트&quot;, Proc. ICASSP, 2021. [32] JB Allen 및 DA Berkley, &quot;소공간 음향을 효율적으로 시뮬레이션하기 위한 이미지 방법&quot;, J. Acoust. Soc. Am., 1979. [33] Y. Zhang, J. Qin, DS Park, W. Han, C.-C. Chiu, R. Pang, QV Le, 및 Y. Wu, &quot;자동 음성 인식을 위한 반지도 학습의 한계 확장&quot;, Proc. NeurIPS SASWorkshop, 2020. [34] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, z. Chen, P. Nguyen, R. Pang, I. Lopez Moreno, 및 Y. Wu, &quot;화자 검증에서 다중 화자 텍스트-음성 합성으로의 학습 전환&quot;, Proc. NeurIPS, 2018. [35] Y. Chen, Y. Assael, B. Shillingford, D. Budden, S. Reed, H. Zen, Q. Wang, LC Cobo, A. Trask, B. Laurie, C. Gulcehre, A. van den Oord, O. Vinyals, 및 N. de Freitas, &quot;샘플 효율적 적응 텍스트 음성 변환,&quot; Proc. ICLR, 2019. [36] N. Kalchbrenner, W. Elsen, K. Simonyan, S. Noury, N. Casagrande, W. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, K. Kavukcuoglu, &quot;효율적인 신경 오디오 합성&quot;, Proc. ICML, 2018. [37] DP Kingma 및 JL Ba, &quot;Adam: 확률론적 최적화 방법&quot; in Proc. ICLR, 2015.
