--- INTRODUCTION ---
스토리 시각화는 시각적 스토리텔링이라고도 하며, 다양한 청중에게 내러티브 콘텐츠를 효과적으로 전달하는 데 중요한 방법입니다. 교육 및 엔터테인먼트 분야에서 광범위하게 활용됩니다[Yin et al. 2022]. 예를 들어, 아동용 만화책이 있습니다. 이 연구에서 스토리 시각화는 다음과 같은 문제로 공식화됩니다. 즉, 일반 텍스트로 된 스토리와 몇몇 캐릭터의 초상화 이미지가 주어졌을 때 스토리를 시각적으로 표현하는 일련의 이미지를 생성합니다. 2. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 적격 스토리 시각화는 내러티브의 정확한 시각적 표현을 제공하기 위해 몇 가지 필수 요구 사항을 충족해야 합니다. 첫째, 정체성 일관성. 모든 프레임이나 장면에서 캐릭터와 환경에 대한 일관된 묘사를 유지하는 것이 중요합니다. 둘째, 텍스트-시각적 정렬. 시각적 콘텐츠는 텍스트 내러티브와 긴밀하게 일치하여 스토리에 설명된 이벤트와 상호 작용을 정확하게 표현해야 합니다. 셋째, 명확하고 논리적인 레이아웃. 생성된 이미지 내의 객체와 캐릭터는 합리적이고 논리적인 레이아웃으로 배열되어야 합니다. 이러한 구성은 시청자의 주의를 내러티브를 통해 원활하게 안내하여 이해하기 쉽게 만듭니다. 스토리 시각화의 선구적인 작업은 일반적으로 일관된 캐릭터와 스타일을 포함하는 특정 데이터 세트에서 모델을 학습합니다. 인기 있는 두 가지 데이터 세트에는 만화 스타일과 제한된 캐릭터 변형이 특징인 PorotoSV [Li et al. 2019a]와 FlintstonesSV [Maharana and Bansal 2021]가 있습니다. 대부분의 초기 접근 방식 [Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021, 2022]은 GAN 또는 VAE 기반 방법에 의존하여 텍스트 인코더를 통합하여 텍스트를 잠재 공간에 투사하고, 디코더를 사용하여 텍스트에 따라 이미지를 생성하고, 이미지 및 스토리 수준 판별기를 사용하여 시각적 품질과 일관성을 유지합니다. 일부 연구에서는 이제 조건부 분포를 포착하기 위해 확산 모델을 활용하며, 종종 초기화를 위해 사전 훈련된 T2I 모델을 사용합니다. 예를 들어, AR-LDM[Pan et al. 2022]은 현재 프레임을 예측하기 위해 과거 캡션과 합성 이미지에 대한 자기 회귀적 조건을 갖춘 잠재 확산 모델을 도입합니다. Make-AStory[Rahman et al. 2022]는 시각적 메모리 모듈이 있는 자기 회귀 모델을 제안하여 콘텐츠 일관성을 위해 생성된 프레임에서 배우와 배경 컨텍스트를 포착합니다. 그러나 이러한 방법은 피할 수 없는 두 가지 한계에 부딪힙니다. 첫째, 두 가지 주요 요구 사항을 충족하기 위해 특정 데이터 세트에 대해 훈련되었기 때문에 새로운 배우와 장면으로 일반화하는 데 어려움이 있습니다. 최근 연구[Jeong et al. 2023]는 사전 훈련된 T2I 모델을 사용하여 모든 새로운 캐릭터와 장면에 적응할 수 있는 제로 샷 스토리 시각화의 잠재력을 조사합니다. 이 프로세스에는 이미지를 생성한 다음 제공된 얼굴로 인간의 얼굴을 대체하는 것이 포함됩니다. 안타깝게도 이 접근 방식은 여러 캐릭터를 수용하지 못하고 인간의 얼굴 외의 객체를 지원하지 않습니다. 둘째, 이러한 방법 중 어느 것도 세 번째 요구 사항, 즉 모든 정보가 텍스트에 의해 암묵적으로 제어되는 생성된 이미지의 레이아웃 또는 로컬 객체 구조를 고려하지 않습니다. 여러 텍스트-이미지 및 레이아웃-이미지 방법[Hong et al. 2018; Li et al. 2023; Liang et al. 2023; Rombach et al. 2022; Zhang et al. 2017]이 레이아웃을 입력 또는 중간 결과로 통합하지만, 이러한 방법은 크로스 프레임 일관성을 고려하지 않고 스토리 시각화보다는 단일 이미지 생성에만 초점을 맞춥니다. 이 연구에서는 광범위한 코퍼스에서 학습된 대규모 언어 및 텍스트-이미지(T2I) 모델에 대한 지식을 기반으로 세 가지 요구 사항을 모두 충족하는 다재다능한 대화형 스토리 시각화 시스템을 소개합니다. 이 시스템은 다양한 새로운 문자에 적응할 수 있으며 이전 방법의 기능을 넘어서는 레이아웃 및 로컬 구조 편집을 지원할 수 있습니다. 저희 시스템은 스토리-프롬프트 생성(S2P), 텍스트-레이아웃 생성(T2L), 제어 가능한 텍스트-이미지 생성(C-T2I), 이미지-비디오 애니메이션(I2V)의 네 가지 구성 요소로 구성되어 있습니다. 스토리가 주어지면 S2P는 대규모 언어 모델을 활용하여 이벤트, 장면, 캐릭터를 포함한 지침에 따라 이미지의 시각적 콘텐츠를 묘사하는 프롬프트를 생성합니다. 그런 다음 T2L은 프롬프트를 사용하여 주요 피사체에 대한 위치 안내를 제공하는 동시에 레이아웃의 대화형 개선을 허용하는 이미지 레이아웃을 만듭니다. 핵심 구성 요소인 C-T2I는 여러 캐릭터의 정체성을 유지하면서 레이아웃, 로컬 스케치 및 프롬프트에 따라 이미지를 렌더링합니다. 프롬프트는 이미지 콘텐츠를 전달하는 반면 레이아웃과 로컬 스케치는 각각 피사체의 위치와 자세한 로컬 구조를 나타냅니다. 모델은 정체성을 유지하기 위해 각 캐릭터에 대한 개인화된 가중치 세트를 학습합니다. C-T2I는 로컬 구조의 대화형 편집과 캐릭터를 새 구조로 원활하게 교체하는 것을 용이하게 합니다. 마지막으로, I2V는 더욱 생생한 프레젠테이션을 위해 생성된 이미지를 애니메이션화하여 시각화 프로세스를 풍부하게 합니다. 시각적 결과는 그림 1에 나와 있습니다. 우리의 주요 기여는 두 가지 핵심 측면에 있습니다. • 우리는 일반 텍스트의 스토리에서 비디오를 생성하기 위해 대규모 언어와 사전 훈련된 T2I 모델을 활용하는 다재다능하고 일반적인 스토리 시각화 시스템을 제안합니다. 이 시스템은 여러 개의 새로운 캐릭터와 장면을 처리할 수 있습니다. • 우리는 시각화 시스템의 핵심 구성 요소 역할을 하는 제어 가능한 다중 모달 텍스트-이미지 생성 모듈인 C-T2I를 개발합니다. 이 모듈은 여러 캐릭터의 정체성 보존에 초점을 맞추고 레이아웃과 로컬 구조 측면에서 구조 제어를 강조합니다. 2
--- RELATED WORK ---
2.1 스토리 시각화 스토리 시각화 분야의 초기 연구는 주로 GAN 또는 VAE 기반 접근 방식에 의존했습니다[Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021; Song et al. 2020]. 예를 들어, StoryGAN[Li et al. 2019a]은 전체 스토리와 개별 문장을 모두 입력으로 사용하여 이미지 및 스토리 판별자를 사용하여 문맥적으로 관련성 있는 이미지를 생성합니다. 반면, DUCOStoryGAN[Maharana et al. 2021]은 비디오 캡션을 사용하여 스토리와 생성된 이미지 간의 의미적 정렬을 향상시키는 이중 학습 프레임워크를 도입합니다. 여러 연구에서 VP-CSV[Chen et al. 2022] 및 StoryDALLE[Maharana et al. 2023]와 같은 변환기의 장거리 종속성 속성을 활용합니다. 2022]. 후자는 사전 학습된 모델을 특정 데이터 세트에 적용하여 사전 지식을 활용합니다. 최근 확산 모델은 이미지, 비디오 및 오디오 생성을 포함한 다양한 응용 프로그램에서 성공을 거두었습니다 [Ho et al. 2022; Liu et al. 2023; Rombach et al. 2022]. 여러 연구에서 확산 모델을 스토리 시각화에 통합하여 GAN을 대체했습니다 [Pan et al. 2022; Rahman et al. 2022]. AR-LDM [Pan et al. 2022]은 스토리 연속을 위한 StoryDALL-E의 설정을 채택하고 잠재 확산 모델을 이미지 생성기로 활용하면서 자기 회귀 모델을 사용하여 현재 및 이전 프롬프트에서 정보를 집계합니다. Make-A-Story [Rahman et al. 2022]는 시각적 메모리 모듈을 특징으로 하는 자기 회귀 확산 기반 프레임워크를 제안합니다. AR-LDM과 유사하게, 텍스트 임베딩보다는 크로스 어텐션 메커니즘에 의존하고 피처 공간에서 상호 작용하지만, 역사적 결과를 활용합니다. 그러나 이러한
--- METHOD ---
다양한 청중에게 내러티브 콘텐츠를 효과적으로 전달하기 위한 것입니다. 교육 및 엔터테인먼트에 광범위하게 적용됩니다[Yin et al. 2022], 예를 들어 아동 만화책. 이 작업에서 스토리 시각화는 다음과 같은 문제로 공식화됩니다. 즉, 일반 텍스트로 된 스토리와 몇몇 캐릭터의 초상화 이미지가 주어지면 스토리를 시각적으로 표현하기 위한 일련의 이미지를 생성합니다. 2. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 적격 스토리 시각화는 내러티브의 정확한 시각적 표현을 제공하기 위한 몇 가지 필수 요구 사항을 충족해야 합니다. 첫째, 정체성 일관성. 모든 프레임이나 장면에서 캐릭터와 환경에 대한 일관된 묘사를 유지하는 것이 중요합니다. 둘째, 텍스트-시각적 정렬. 시각적 콘텐츠는 텍스트 내러티브와 긴밀하게 일치하여 스토리에 설명된 이벤트와 상호 작용을 정확하게 표현해야 합니다. 셋째, 명확하고 논리적인 레이아웃. 생성된 이미지 내의 객체와 캐릭터는 합리적이고 논리적인 레이아웃으로 배열되어야 합니다. 이러한 구성은 시청자의 주의를 내러티브를 통해 원활하게 안내하여 이해하기 쉽게 만듭니다. 스토리 시각화의 선구적인 작업은 일반적으로 일관된 캐릭터와 스타일을 포함하는 특정 데이터 세트에서 모델을 학습합니다. 인기 있는 두 가지 데이터 세트에는 만화 스타일과 제한된 캐릭터 변형이 특징인 PorotoSV [Li et al. 2019a]와 FlintstonesSV [Maharana and Bansal 2021]가 있습니다. 대부분의 초기 접근 방식 [Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021, 2022]은 GAN 또는 VAE 기반 방법에 의존하여 텍스트 인코더를 통합하여 텍스트를 잠재 공간에 투사하고, 디코더를 사용하여 텍스트에 따라 이미지를 생성하고, 이미지 및 스토리 수준 판별기를 사용하여 시각적 품질과 일관성을 유지합니다. 일부 연구에서는 이제 조건부 분포를 포착하기 위해 확산 모델을 활용하며, 종종 초기화를 위해 사전 훈련된 T2I 모델을 사용합니다. 예를 들어, AR-LDM[Pan et al. 2022]은 현재 프레임을 예측하기 위해 과거 캡션과 합성 이미지에 대한 자기 회귀적 조건을 갖춘 잠재 확산 모델을 도입합니다. Make-AStory[Rahman et al. 2022]는 시각적 메모리 모듈이 있는 자기 회귀 모델을 제안하여 콘텐츠 일관성을 위해 생성된 프레임에서 배우와 배경 컨텍스트를 포착합니다. 그러나 이러한 방법은 피할 수 없는 두 가지 한계에 부딪힙니다. 첫째, 두 가지 주요 요구 사항을 충족하기 위해 특정 데이터 세트에 대해 훈련되었기 때문에 새로운 배우와 장면으로 일반화하는 데 어려움이 있습니다. 최근 연구[Jeong et al. 2023]는 사전 훈련된 T2I 모델을 사용하여 모든 새로운 캐릭터와 장면에 적응할 수 있는 제로 샷 스토리 시각화의 잠재력을 조사합니다. 이 프로세스에는 이미지를 생성한 다음 제공된 얼굴로 인간의 얼굴을 대체하는 것이 포함됩니다. 안타깝게도 이 접근 방식은 여러 캐릭터를 수용하지 못하고 인간의 얼굴 외의 객체를 지원하지 않습니다. 둘째, 이러한 방법 중 어느 것도 세 번째 요구 사항, 즉 모든 정보가 텍스트에 의해 암묵적으로 제어되는 생성된 이미지의 레이아웃 또는 로컬 객체 구조를 고려하지 않습니다. 여러 텍스트-이미지 및 레이아웃-이미지 방법[Hong et al. 2018; Li et al. 2023; Liang et al. 2023; Rombach et al. 2022; Zhang et al. 2017]이 레이아웃을 입력 또는 중간 결과로 통합하지만, 이러한 방법은 크로스 프레임 일관성을 고려하지 않고 스토리 시각화보다는 단일 이미지 생성에만 초점을 맞춥니다. 이 연구에서는 광범위한 코퍼스에서 학습된 대규모 언어 및 텍스트-이미지(T2I) 모델에 대한 지식을 기반으로 세 가지 요구 사항을 모두 충족하는 다재다능한 대화형 스토리 시각화 시스템을 소개합니다. 이 시스템은 다양한 새로운 문자에 적응할 수 있으며 이전 방법의 기능을 넘어서는 레이아웃 및 로컬 구조 편집을 지원할 수 있습니다. 저희 시스템은 스토리-프롬프트 생성(S2P), 텍스트-레이아웃 생성(T2L), 제어 가능한 텍스트-이미지 생성(C-T2I), 이미지-비디오 애니메이션(I2V)의 네 가지 구성 요소로 구성되어 있습니다. 스토리가 주어지면 S2P는 대규모 언어 모델을 활용하여 이벤트, 장면, 캐릭터를 포함한 지침에 따라 이미지의 시각적 콘텐츠를 묘사하는 프롬프트를 생성합니다. 그런 다음 T2L은 프롬프트를 사용하여 주요 피사체에 대한 위치 안내를 제공하는 동시에 레이아웃의 대화형 개선을 허용하는 이미지 레이아웃을 만듭니다. 핵심 구성 요소인 C-T2I는 여러 캐릭터의 정체성을 유지하면서 레이아웃, 로컬 스케치 및 프롬프트에 따라 이미지를 렌더링합니다. 프롬프트는 이미지 콘텐츠를 전달하는 반면 레이아웃과 로컬 스케치는 각각 피사체의 위치와 자세한 로컬 구조를 나타냅니다. 모델은 정체성을 유지하기 위해 각 캐릭터에 대한 개인화된 가중치 세트를 학습합니다. C-T2I는 로컬 구조의 대화형 편집과 캐릭터를 새 구조로 원활하게 교체하는 것을 용이하게 합니다. 마지막으로, I2V는 생성된 이미지에 애니메이션을 적용하여 더욱 생생한 표현을 제공함으로써 시각화 프로세스를 풍부하게 합니다.시각적 결과는 그림 1에 나와 있습니다.저희의 주요 기여는 두 가지 핵심 측면에 있습니다.· 일반 텍스트의 스토리에서 비디오를 생성하기 위해 대규모 언어와 사전 훈련된 T2I 모델을 활용하는 다재다능하고 일반적인 스토리 시각화 시스템을 제안합니다.이 시스템은 여러 새로운 캐릭터와 장면을 처리할 수 있습니다.· 시각화 시스템의 핵심 구성 요소 역할을 하는 제어 가능한 다중 모달 텍스트-이미지 생성 모듈인 C-T2I를 개발합니다.이 모듈은 여러 캐릭터의 정체성 보존에 초점을 맞추고 레이아웃과 로컬 구조 측면에서 구조 제어를 강조합니다.2 관련 연구 2.1 스토리 시각화 스토리 시각화 분야의 초기 연구는 주로 GAN 또는 VAE 기반 접근 방식에 의존했습니다[Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021; Song et al. 2020]. 예를 들어, StoryGAN [Li et al. 2019a]은 전체 스토리와 개별 문장을 모두 입력으로 사용하여 이미지 및 스토리 구분자를 사용하여 문맥적으로 관련된 이미지를 생성합니다. 반면, DUCOStoryGAN [Maharana et al. 2021]은 비디오 캡션을 사용하여 스토리와 생성된 이미지 간의 의미적 정렬을 향상시키는 이중 학습 프레임워크를 도입합니다. 여러 연구에서 VP-CSV [Chen et al. 2022] 및 StoryDALLE [Maharana et al. 2022]와 같은 변환기의 장거리 종속성 속성을 활용합니다. 후자는 사전 학습된 모델을 특정 데이터 세트에 적용하여 사전 지식을 활용합니다. 최근 확산 모델은 이미지, 비디오 및 오디오 생성을 포함한 다양한 응용 프로그램에서 성공을 거두었습니다 [Ho et al. 2022; Liu et al. 2023; Rombach et al. 2022]. 여러 연구에서는 GAN을 대체하여 스토리 시각화에 확산 모델을 통합했습니다[Pan et al. 2022; Rahman et al. 2022]. AR-LDM[Pan et al. 2022]은 스토리 연속을 위한 StoryDALL-E의 설정을 채택하고 잠재 확산 모델을 이미지 생성기로 활용하면서 자기 회귀 모델을 사용하여 현재 및 이전 프롬프트에서 정보를 집계합니다. Make-A-Story[Rahman et al. 2022]는 시각 메모리 모듈을 특징으로 하는 자기 회귀 확산 기반 프레임워크를 제안합니다. AR-LDM과 유사하게 텍스트 임베딩이 아닌 교차 어텐션 메커니즘에 의존하고 피처 공간에서 상호 작용하지만 과거 결과를 활용합니다. 그러나 이러한 방법은 종종 새로운 캐릭터와 장면으로 일반화하는 데 어려움을 겪습니다. 이는 본질적으로 모델을 특정 지침 &quot;안정적인 확산을 위해 스토리에서 K 프롬프트를 생성하여 이벤트, 캐릭터 및 장면을 묘사하는 이미지를 생성&quot; 스케치 P1: &quot;고양이와 개...&quot; TaleCrafter: 여러 캐릭터를 사용한 대화형 스토리 시각화 여러 캐릭터 P1: &quot;고양이와 개...&quot; 레이아웃 S2P TXT P2: &quot;... 숲을 걷다...&quot; Pk: &quot;...공놀이...&quot; T2L C-Tdog 고양이 이미지 12V 비디오 스토리 그림 2. 대화형 스토리 시각화 시스템의 파이프라인. 이 시스템은 네 가지 구성 요소로 구성되어 있습니다. (a) 스토리-프롬프트(S2P): 대규모 언어 모델을 사용하여 문학적 및 예술적 설명과 T2l 모델에 입력된 설명 간의 격차를 메웁니다. 주어진 스토리의 내용을 이해하고 주어진 지침에 따라 T2l 모델에 적합한 프롬프트로 변환합니다. (b) 텍스트-레이아웃(T2L): 주요 프롬프트의 주제. (c) 제어 가능한 텍스트-이미지(C-T2I): 프롬프트, 레이아웃, 스케치 및 각 캐릭터의 몇 가지 이미지와 같은 다양한 조건이 주어지면 일관된 캐릭터 이미지를 생성합니다. 스케치를 통해 캐릭터, 레이아웃 및 로컬 구조를 대화형으로 편집할 수 있습니다. (d) 이미지-비디오(12V): 이미지에서 깊이를 추출하고 새로운 뷰 합성을 위한 카메라 경로를 설정하여 비디오로 변환합니다. FlintstonesSV와 같은 데이터 세트. 결과적으로 모델은 훈련 데이터 세트의 캐릭터와 장면만 기억할 수 있습니다. 이러한 제한을 없애기 위해 최근 연구[Jeong et al. 2023]는 새로운 캐릭터와 장면을 지원하는 제로 샷 스토리 시각화에 초점을 맞추고 확산 모델을 사용하여 이미지에서 캐릭터 정체성을 대체하는 방법을 제안합니다. 이는 얼굴 교환을 연상시키는 접근 방식입니다. 그러나 이 방법의 범위는 단일 인간 얼굴에 국한되며 이미지 전체의 정체성 보존 및 일관성은 여전히 만족스럽지 않습니다. 저희의 방법은 또한 여러 새로운 캐릭터와 장면을 지원하는 제로 샷 스토리 시각화를 목표로 합니다. 정체성 일관성을 보장하기 위해 각 캐릭터에 대한 작은 모델 가중치 세트를 최적화하고 여러 캐릭터를 구성하기 위한 개인화된 인페인팅 방법을 제안합니다.또한, 우리의 접근 방식은 레이아웃과 로컬 객체 구조를 제어할 수 있어 이전 연구의 역량을 능가합니다.2. 텍스트-이미지 생성 상당수의 텍스트-이미지(T2I) 방법은 GANS에 기반을 두고 있습니다[Reed et al. 2016; Xu et al. 2018; Zhang et al. 2017].일반적으로 이러한 방법에는 텍스트 인코더와 이미지 생성기가 포함됩니다.일부 접근 방식[Hong et al. 2018; Li et al. 2019b; Qiao et al. 2021]은 중간 레이아웃 생성을 사용하여 텍스트에서 직접 이미지 생성을 단순화합니다.최근 확산 모델은 여러 연구[Ramesh et al. 2022, 2021; Rombach et al. 2022; Saharia et al. 2022] 확산 모델을 사용하여 이미지 품질과 다양성을 개선합니다. 그러나 이러한 방법은 주로 여러 이미지에서 정체성 일관성을 고려하지 않고 텍스트와 단일 생성된 이미지 간의 정렬에 집중합니다. 확산 기반 T2I 생성에서 정체성을 유지하는 데 역전 방법[Gal et al. 2022; Ruiz et al. 2022; Shi et al. 2023; Yang et al. 2023]의 성공에도 불구하고 일반적으로 여러 개념을 처리하는 데 어려움을 겪는 반면 단일 개념에는 뛰어납니다. 사용자 지정 확산[Kumari et al. 2022]은 여러 개념을 구성하는 것을 목표로 하지만 고양이와 개와 같이 비슷하게 생긴 개념을 처리할 때는 실패합니다. 역전을 사용하여 이미지에서 여러 개념이나 문자를 구성하는 것은 여전히 어려운 일이므로 다른 관점에서 구성을 처리하는 개인화된 인페인팅을 위한 제어 가능한 T모델을 제안합니다. 3 방법 문자, 레이아웃 및 로컬 구조의 대화형 편집을 지원하는 대화형 스토리 시각화 시스템을 제안합니다. 대부분의 이전 작업과 달리 여러 캐릭터의 일관된 생성을 처리하고 새로운 캐릭터와 장면으로 일반화할 수 있습니다.그림 2에서 볼 수 있듯이 이 시스템은 스토리-프롬프트(S2P), 텍스트-레이아웃(T2L), 제어 가능한 텍스트-이미지(C-T2I), 이미지-비디오(I2V)의 네 가지 구성 요소로 구성됩니다.3. 스토리-프롬프트 생성 주어진 스토리는 &quot;고양이와 개가 멋진 하루를 보냈다&quot;와 같은 짧은 문장일 수도 있고, 문학적 기법을 사용한 길고 자세한 문장일 수도 있습니다. 둘 다 이미지의 이벤트, 장면, 객체를 묘사하는 캡션으로 훈련된 현재 T2I 모델의 취향에 맞지 않을 수 있습니다.최근 GPT-4[OpenAI 2023] 및 PaLM 2[et al 2023]와 같은 대규모 언어 모델 개발에서 놀라운 혁신이 이루어졌습니다. GPT-4는 시각과 언어를 포함한 방대한 멀티모달 코퍼스에서 학습되었으며, 이는 문학적 설명과 T2I 모델 설명 간의 격차를 메우는 데 적합한 도구입니다. 이 작업에서는 GPT-4를 사용합니다. GPT-4에서 지침은 중요합니다. T2I 모델 설명의 기본 요소는 이벤트, 장면 및 객체입니다. 따라서 주어진 스토리에 대해 &quot;이벤트, 캐릭터 및 장면을 묘사하는 이미지를 생성하기 위해 Stable Diffusion을 위한 스토리에서 K개의 프롬프트를 생성하세요&quot;와 같은 지침을 사용합니다. 사전 학습된 T2I 모델의 기능을 활용하여 텍스트의 키워드를 활용하여 스타일을 제어합니다. 예를 들어 &quot;유화 스타일&quot;을 프롬프트의 접미사로 사용합니다. S는 일반 텍스트의 스토리를 나타냅니다. R과 F는 각각 지침과 스타일을 나타냅니다. S2P 구성 요소의 함수는 = [P1, P2, PK] S2P(S, R, F, K), (1)로 정의할 수 있습니다. 여기서 K는 생성할 프롬프트의 수입니다. pi는 i번째 프롬프트입니다.4. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 3.2 텍스트-레이아웃 생성 변환기는 레이아웃 분포를 캡처하는 데 가장 널리 사용되는 기술입니다[Gupta et al. 2021; Jiang et al. 2022]. 그러나 자기 회귀 디코더는 고정된 생성 순서로 인해 부분 입력을 처리하는 데 유연하지 않은 것으로 나타났습니다[Kong et al. 2022]. 최근 LayoutDM[Inoue et al. 2023]에서 레이아웃 생성을 위한 이산 확산 모델[Austin et al. 2021; Gu et al. 2022]이 도입되었습니다. 이는 만족스러운 성능을 달성하고 다양한 제약 조건을 허용합니다. LayoutDM에 따라 텍스트-레이아웃 생성을 위해 이산 확산 모델을 활용합니다. L = {B;}\\₁가 N개의 객체가 있는 레이아웃을 나타내며, 여기서 B₁ = (bi, li)이다. b¡ = (xi, Yi, Wi, hi) = {1, ..., M}4는 중심이 (xi, yi)이고 너비와 높이가 (wi, h;)인 경계 상자를 나타낸다. 좌표는 정규화되고 양자화되었으며, M은 빈의 수이다. li; = {1, 2, ..., C}는 객체 범주이다. C는 객체 범주의 수이다. 평평해진 이산 벡터 L= {x1,y1, W1, h1, 11, x2, y2, ...}는 가변 길이의 잠재 변수로 처리된다. D3PM의 이산 확산 모델 정의에 따라 [Austin et al. 2021], 시간 단계 t, zt = {1, 2, ..., D}에서 D 범주를 갖는 이산 스칼라에 대한 순방향 확산 프로세스는 다음과 같이 정의할 수 있습니다. q(zt|zt−1) = v(zt)TQtv(zt−1)T, (2) 여기서 v(z+)는 zt의 원핫 벡터이고 Qt Є [0,1] DXD는 전이 행렬입니다. Z₁ = {1, 2, ..., D}N이 벡터인 경우 순방향 프로세스는 각 요소에 독립적으로 적용됩니다. DDPM [Ho et al. 2020]과 유사하게 역방향 잡음 제거 프로세스는 네트워크에 의해 추정됩니다. 즉, på(Zt−1|Zt) € [0,1] N×D¸ 0은 양방향 Transformer의 매개변수를 나타냅니다. DP3M은 po(Zt-1|Zt)를 다음과 같이 분해하고 대신 pe(zo|zt)를 추정하는 법을 학습합니다. α Po(Zt-1|Zt) ∞ Σ9(Zt−1|Zt, Zo)Ã¤(Zo|Zt), Zo (3) 여기서 q(Zt-1 Zt, Zo)는 확산 과정의 정의에 따라 폐쇄형 솔루션이 있습니다. 평탄화된 L은 가변 길이를 가지므로 LayoutDM의 패딩 트릭을 이용하여 이를 처리합니다. 널리 사용되는 변분 하한 Lvb와 추가 손실을 포함하는 D3PM의 학습 목표를 사용합니다. 즉, Ls2p = Lvb + Eq(zo) Eq(zt |zo) [— log pe(zo|zt)], 여기서 는 트레이드오프 하이퍼파라미터입니다. (4) T2L의 목적은 주어진 텍스트에 따라 레이아웃을 생성하는 것이므로 위의 공식을 직접 적용할 수 없습니다. 언어 처리 도구[Liu et al. 2021] 텍스트에서 명사를 추출하여 대상 객체로 처리합니다. 명사를 범주형 레이블로 변환하기 위해 Objectdataset의 클래스 이름을 사용합니다[Shao et al. 2019]. Object365는 이미지의 객체에 대한 경계 상자와 범주형 레이블을 제공하므로 T2L 모델을 학습하는 데 사용합니다. T2L 구성 요소의 기능은 L = = {Bi}\₁ = T2L(p), (5)로 표현할 수 있습니다. 여기서 p는 S2P 구성 요소에서 생성된 프롬프트입니다. 3. 제어 가능한 텍스트-이미지 생성 C-T2I는 스토리 시각화 시스템의 핵심 구성 요소입니다(그림 3 참조). 다중 모달 입력이 있으며 ID, 위치 및 로컬 프롬프트를 포함한 다중 레벨 제어가 있는 이미지를 생성합니다. &quot;고양이와 개...&quot;를 임베드합니다. Ет 스케치 Zt 개 ResBlock ET 고양이<box, text> Es Cross-Att Aug Self-Att Add Aq BQ Ax Bx Ay By QKV Unet Block Zt-C-TFig. 3. C-T2I 구성 요소의 구조. 노이즈가 있는 이미지를 입력으로 받고 프롬프트, 스케치, 설명이 있는 경계 상자 등 여러 유형의 안내를 조건으로 단일 노이즈 제거 단계를 통해 이미지를 생성합니다. 정체성 일관성을 위해 LORA를 사용하여 셀프 및 교차 어텐션 레이어의 개인화된 가중치와 각 문자에 대한 특정 토큰을 학습합니다. 구조. 많은 연구가 텍스트-이미지 [Rombach et al. 2022], 레이아웃-이미지 [Li et al. 2023], 스케치-이미지 [Voynov et al. 2022] 및 정체성 보존 [Kumari et al. 2022]의 개별 작업에 초점을 맞추고 있지만 대화형 스토리 시각화의 필수 기능인 다단계 제어를 동시에 처리할 수는 없습니다. 예를 들어, [Li et al. 2023]은 텍스트의 안내를 받는 레이아웃-이미지 방법입니다. 입력 상자로 객체를 찾을 수 있지만 로컬 구조와 ID를 제어할 수 없습니다. [Chen et al. 2023]은 지정된 하나의 객체의 위치를 지정할 수 있지만 여러 객체를 처리할 수 없습니다. [Kumari et al. 2022]는 두 개의 객체를 이미지에 넣으려고 하지만 위치와 포즈를 지정할 수 없습니다. 잠재 확산 모델(LDM)에서 영감을 받아 [Rombach et al. 2022], 다중 모달 조건 생성 모델은 잠재 공간에서 학습되고 분포는 확산 모델을 사용하여 포착됩니다. C-T21의 구조는 그림 3에 나와 있습니다. 여러 유형의 입력을 주입할 수 있도록 LDM에서 원래 UNet의 구조를 수정합니다. 각 UNet 블록에서 셀프 및 크로스 어텐션 블록을 업그레이드하고 추가 블록을 추가로 도입합니다. ID 보존. 사전 학습된 CLIP [Radford et al. 2021] 텍스트 인코더 E로 입력 프롬프트를 임베딩에 매핑합니다. 임베딩은 공간적 특징과 상호 작용하기 위해 교차 어텐션 모듈에 공급됩니다. 신원 보존을 위해 [Ruiz et al. 2022] 및 [Kumari et al. 2022]와 달리 학습된 T2I 모델의 모든 매개변수를 미세 조정하는 대신 LORA를 사용하여 셀프 및 교차 어텐션 계층에서 추가적인 저순위 가중치를 업데이트합니다. LORA는 쿼리, 키 및 값 매핑의 MLP에만 적용됩니다. 원래 가중치가 유지되고 매우 작은 매개변수 집합만 학습되므로 과적합 문제와 개념 망각을 완화할 수 있습니다. 문자의 몇 가지 이미지가 주어지면 토큰과 LORA 가중치 집합이 특별히 훈련됩니다. 가중치는 두 문자를 구성하는 성공률이 만족스럽지 않기 때문에 여러 문자에 대해 공동으로 최적화되지 않습니다. 특히 비슷한 모양의 문자의 경우 더욱 그렇습니다. W € Rdxk가 선형 사상의 매개변수를 나타내도록 하고, 여기서 d와 k는 각각 입력 및 출력 벡터의 차원입니다. h = Wx가 출력 벡터를 나타내도록 합니다. x = Rk는 TaleCrafter: 여러 캐릭터를 사용한 대화형 스토리 시각화 입력 벡터입니다. LORA를 사용하여 새로운 개념을 기억하면서 W를 업데이트하는 대신 두 개의 저랭크 행렬 A = Rdxr 및 B = Rrk를 학습합니다. 여기서 r &lt; min(d, k). r이 작을 때 A와 B는 W보다 훨씬 적은 매개변수를 갖습니다. 순방향 패스는 다음과 같이 다시 쓸 수 있습니다. h = Wx+BAX. 객체 현지화. 텍스트-레이아웃 구성 요소에서 레이아웃에는 좌표와 범주가 포함됩니다. C-I2I 구성 요소에서 범주를 학습된 문자 토큰을 주입하는 동안 객체를 묘사하는 문구로 바꿉니다. 예를 들어, 개인화를 위해 &quot;개&quot;를 &quot;sks 개&quot;로 바꿉니다. 여기서 sks는 학습된 토큰입니다. [Li et al. 2023], 텍스트 임베딩은 CLIP을 사용하여 추출하는 반면, 좌표는 Fourier 임베딩을 사용하여 인코딩됩니다 [Mildenhall et al. 2021]. 두 임베딩은 연결되어 정보 정렬을 위해 MLP 계층을 거친 다음 두 개의 자기 주의 계층으로 구성된 증강 자기 주의 모듈에 공급됩니다. 하나는 시각적 특징 간의 상호 작용만 포함하는 일반적인 자기 주의 계층입니다. 다른 하나는 시각적 특징과 객체 위치를 주입하기 위한 위치 임베딩 간의 상호 작용을 포함합니다. = f가 시각적 특징을 나타내도록 합니다. 예를 들어 [ET(p), Fourier(b)]가 접지 텍스트 p와 상자 b = (x, y, w, h)의 연결된 임베딩을 나타내도록 합니다. 첫 번째 자기 주의는 다음과 같이 쓸 수 있습니다. f ← f+SA(f), 여기서 SA(·)는 자기 주의 연산을 나타냅니다. 두 번째 게이트형 자기 주의는 다음과 같이 쓸 수 있습니다. f ← f + tanh(α) · TS(SA([f, eg])), 여기서 a는 초기화가 0인 변수입니다. TS()는 상호 작용 후 시각적 특징을 선택합니다. 로컬 구조 제어. 거의 모든 현재 스토리 시각화 작업은 객체의 로컬 구조 제어를 고려하지 않습니다. 레이아웃과 객체 구조는 텍스트에 의해서만 암묵적으로 결정됩니다. 구조 제어와 대화형 편집의 유연성을 도입하기 위해 스케치를 하나의 입력으로 사용합니다. T21 어댑터[Mou et al. 2023]에서 영감을 받아 시각적 인코더 Es를 사용하여 입력 스케치를 UNet의 특징 공간에 매핑합니다. 인코더는 4개의 잔여 블록의 스택입니다[He et al. 2016]. 예측된 스케치 특징은 추가 모듈에 의해 시각적 특징과 결합됩니다. 먼저 빈 캔버스에서 입력 스케치 Is를 해당 경계 상자에 따라 변환하고 크기를 조정합니다. 그런 다음 생성된 스케치 이미지 Ĩs가 Es에 입력됩니다. fs = Es(Is)는 Es가 추출한 특징을 나타냅니다. 로컬 구조 제어는 시각적 특징 f와 생성된 특징 f를 더하여 실현됩니다. 즉, f ← f + ßfs입니다. 여기서 ẞ는 구조 제약 조건을 적용하는 강도를 제어하는 매개변수입니다. 학습을 위해 ß = 1로 설정되지만 추론 중에 조정할 수 있습니다. ẞ 추론 시 스케치 입력이 필요하지 않음을 의미합니다. 반복 생성의 경우 = 0입니다. 각 문자에는 고유한 개인화된 가중치가 있으므로 여러 문자에 대한 추론 중에 토큰과 LORA 가중치가 다른 모달리티의 모듈과 함께 반복적으로 적용됩니다. 예를 들어, &quot;숲 속의 개와 고양이&quot;라는 텍스트와 개와 고양이의 상자와 스케치가 주어지면 먼저 개의 개인화된 가중치와 수정된 텍스트 &quot;a<sks> 숲 속의 개와 고양이&quot;.<sks> &quot;는 개에 대한 학습된 토큰입니다. 개 상자의 텍스트는 &quot;로 설정됩니다.<sks> 개&quot;. 고양이 상자가 있으므로 상자의 내용을 텍스트 &quot;로 칠합니다.<yty> 고양이&quot;. &quot;<yty> &quot;는 고양이의 학습된 토큰입니다. 인페인팅 모델은 증강된 입력을 가진 C-T2I의 변형입니다. 차이점은 노이즈가 있는 이미지, 원본 이미지, 상자의 영역 마스크를 연결하여 9개 채널의 입력을 형성한다는 것입니다. 학습 절차는 C-T2I와 동일합니다. 학습 목표. LDM[Rombach et al. 2022]에 따라 학습을 위한 변분 하한을 사용합니다. (6) LC-T21 = Ez~E1(x),€~N(0,1),t [||€ – €g (Zt, t, C)||], 여기서 x는 이미지를 나타내고 E1 (·)은 이미지를 잠재 공간으로 투사하는 이미지 인코더입니다. Є는 샘플링된 노이즈이고 E()는 예측된 노이즈입니다. t는 타임스텝이고 zt는 노이즈가 있는 이미지입니다. C = {ET (p), Es (Ĩs), ET (L)}는 조건의 임베딩을 나타냅니다. 즉, 프롬프트, 스케치 및 레이아웃입니다.3. 이미지-비디오 생성 스토리 시각화를 생생하게 만들기 위해 시스템에 이미지-비디오 구성 요소를 도입했습니다.이 구성 요소에서 주로 카메라 움직임에 초점을 맞춰 이미지 깊이를 고려하여 비디오를 생성합니다.3D 사진 방법[Shih et al. 2020]을 사용하여 깊이를 추출하고 참신한 관점에서 이미지를 합성하여 입체 인식 세부 정보를 향상시키고 정적 이미지보다 장면을 더 사실적으로 만들 수 있습니다.이 방법을 사용하면 확대, 원 및 스윙과 같은 다양한 효과에 대한 카메라 경로를 설정할 수 있습니다.4.
--- EXPERIMENT ---
s 및 사용자 연구는 제안된 시스템의 대화형 편집의 효과성과 유연성을 검증하기 위해 수행되었습니다.서론 스토리 시각화, 즉 시각적 스토리텔링은 다양한 청중에게 내러티브 콘텐츠를 효과적으로 전달하는 데 중요한 방법입니다.교육 및 엔터테인먼트 분야에서 광범위한 응용 분야가 있습니다[Yin et al. 2022], 예를 들어 아동 만화책.이 연구에서 스토리 시각화는 다음과 같은 문제로 공식화됩니다.즉, 일반 텍스트로 된 스토리와 몇몇 캐릭터의 초상화 이미지가 주어지면 스토리를 시각적으로 표현하기 위한 일련의 이미지를 생성합니다.2. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 적격 스토리 시각화는 내러티브의 정확한 시각적 표현을 제공하기 위해 몇 가지 필수 요구 사항을 충족해야 합니다.첫째, 정체성 일관성.모든 프레임이나 장면에서 캐릭터와 환경에 대한 일관된 묘사를 유지하는 것이 중요합니다.둘째, 텍스트-시각적 정렬. 시각적 콘텐츠는 텍스트 내러티브와 긴밀하게 일치하여 스토리에 설명된 이벤트와 상호 작용을 정확하게 표현해야 합니다. 셋째, 명확하고 논리적인 레이아웃입니다. 생성된 이미지 내의 객체와 캐릭터는 합리적이고 논리적인 레이아웃으로 배열되어야 합니다. 이러한 구성은 시청자의 주의를 내러티브를 통해 원활하게 안내하여 이해하기 쉽게 만듭니다. 스토리 시각화의 선구적인 작업은 일반적으로 일관된 캐릭터와 스타일을 포함하는 특정 데이터 세트에서 모델을 학습합니다. 두 가지 인기 있는 데이터 세트에는 만화 스타일과 제한된 캐릭터 변형이 특징인 PorotoSV[Li et al. 2019a]와 FlintstonesSV[Maharana and Bansal 2021]가 있습니다. 대부분의 초기 접근 방식[Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021, 2022]는 GAN 또는 VAE 기반 방법에 의존하여 텍스트를 잠재 공간에 투사하는 텍스트 인코더, 텍스트에 따라 이미지를 생성하는 디코더, 시각적 품질과 일관성을 유지하기 위한 이미지 및 스토리 수준 판별기를 통합합니다. 일부 연구에서는 현재 확산 모델을 활용하여 조건부 분포를 포착하고 종종 초기화를 위해 사전 훈련된 T2I 모델을 사용합니다. 예를 들어, AR-LDM[Pan et al. 2022]은 현재 프레임을 예측하기 위해 과거 캡션과 합성 이미지에 따라 자기 회귀적으로 조건화된 잠재 확산 모델을 도입합니다. Make-AStory[Rahman et al. 2022]는 시각적 메모리 모듈이 있는 자기 회귀 모델을 제안하여 콘텐츠 일관성을 위해 생성된 프레임에서 배우와 배경 컨텍스트를 포착합니다. 그러나 이러한 방법은 두 가지 피할 수 없는 한계에 부딪힙니다. 첫째, 주요 두 가지 요구 사항을 충족하기 위해 특정 데이터 세트에 대해 훈련되었기 때문에 새로운 배우와 장면으로 일반화하는 데 어려움이 있습니다. 최근 작업[Jeong et al. 2023]은 사전 훈련된 T2I 모델을 사용하여 모든 새로운 캐릭터와 장면에 적응할 수 있도록 하는 제로 샷 스토리 시각화의 잠재력을 조사합니다. 이 프로세스에는 이미지를 생성한 다음 제공된 얼굴로 사람의 얼굴을 대체하는 것이 포함됩니다. 안타깝게도 이 접근 방식은 여러 캐릭터를 수용하지도 않고 사람의 얼굴 외의 객체를 지원하지도 않습니다. 둘째, 이러한 방법 중 어느 것도 세 번째 요구 사항, 즉 생성된 이미지의 레이아웃이나 로컬 객체 구조를 고려하지 않으며 모든 정보는 텍스트에 의해 암묵적으로 제어됩니다. 여러 텍스트-이미지 및 레이아웃-이미지 방법[Hong et al. 2018; Li et al. 2023; Liang et al. 2023; Rombach et al. 2022; Zhang et al. 2017]이 레이아웃을 입력 또는 중간 결과로 통합하지만, 프레임 간 일관성을 고려하지 않고 스토리 시각화보다는 단일 이미지 생성에만 초점을 맞춥니다. 이 작업에서 우리는 세 가지 요구 사항을 모두 충족하는 다재다능한 대화형 스토리 시각화 시스템을 소개하는데, 이는 광범위한 코퍼스에서 훈련된 대규모 언어 및 텍스트-이미지(T2I) 모델에 대한 지식을 기반으로 합니다. 이 시스템은 다양한 새로운 캐릭터에 적응할 수 있으며 이전 방법의 기능을 넘어서는 레이아웃 및 로컬 구조 편집을 지원할 수 있습니다. 우리 시스템은 스토리-프롬프트 생성(S2P), 텍스트-레이아웃 생성(T2L), 제어 가능한 텍스트-이미지 생성(C-T2I), 이미지-비디오 애니메이션(I2V)의 네 가지 구성 요소로 구성됩니다. 스토리가 주어지면 S2P는 대규모 언어 모델을 활용하여 이벤트, 장면 및 캐릭터를 포함한 지침에 따라 이미지의 시각적 콘텐츠를 묘사하는 프롬프트를 생성합니다. 그런 다음 T2L은 프롬프트를 사용하여 주요 주제에 대한 위치 안내를 제공하는 이미지 레이아웃을 만들고 동시에 레이아웃의 대화형 정제를 허용합니다. 핵심 구성 요소인 C-T2I는 여러 캐릭터의 정체성을 유지하면서 레이아웃, 로컬 스케치 및 프롬프트에 따라 이미지를 렌더링합니다. 프롬프트는 이미지 콘텐츠를 전달하는 반면, 레이아웃과 로컬 스케치는 각각 피사체의 위치와 자세한 로컬 구조를 나타냅니다. 모델은 신원을 보존하기 위해 각 캐릭터에 대한 개인화된 가중치의 작은 집합을 학습합니다. C-T2I는 로컬 구조의 대화형 편집과 새로운 캐릭터로의 원활한 교체를 용이하게 합니다. 마지막으로, I2V는 더욱 생생한 표현을 위해 생성된 이미지에 애니메이션을 적용하여 시각화 프로세스를 풍부하게 합니다. 시각적 결과는 그림 1에 나와 있습니다. 우리의 주요 기여는 두 가지 핵심 측면에 있습니다. • 우리는 일반 텍스트의 스토리에서 비디오를 생성하기 위해 대규모 언어와 사전 훈련된 T2I 모델을 활용하는 다재다능하고 일반적인 스토리 시각화 시스템을 제안합니다. 이 시스템은 여러 새로운 캐릭터와 장면을 처리할 수 있습니다. • 우리는 시각화 시스템의 핵심 구성 요소 역할을 하는 제어 가능한 다중 모달 텍스트-이미지 생성 모듈인 C-T2I를 개발합니다. 이 모듈은 여러 캐릭터의 신원 보존에 중점을 두고 레이아웃과 로컬 구조 측면에서 구조 제어를 강조합니다. 2 관련 연구 2.1 스토리 시각화 스토리 시각화 분야의 초기 연구는 주로 GAN 또는 VAE 기반 접근 방식에 의존했습니다[Chen et al. 2022; Li 2022; Li et al. 2019a; Maharana and Bansal 2021; Maharana et al. 2021; Song et al. 2020]. 예를 들어, StoryGAN[Li et al. 2019a]은 전체 스토리와 개별 문장을 모두 입력으로 사용하여 이미지 및 스토리 판별자를 사용하여 문맥적으로 관련성 있는 이미지를 생성합니다. 반면, DUCOStoryGAN[Maharana et al. 2021]은 비디오 캡션을 활용하여 스토리와 생성된 이미지 간의 의미적 정렬을 향상시키는 이중 학습 프레임워크를 도입합니다. 여러 연구에서 VP-CSV[Chen et al. 2022] 및 StoryDALLE[Maharana et al. 2023]와 같은 변환기의 장거리 종속성 속성을 활용합니다. 2022]. 후자는 사전 학습된 모델을 특정 데이터 세트에 적용하여 사전 지식을 활용합니다. 최근 확산 모델은 이미지, 비디오 및 오디오 생성을 포함한 다양한 응용 프로그램에서 성공을 거두었습니다 [Ho et al. 2022; Liu et al. 2023; Rombach et al. 2022]. 여러 연구에서 확산 모델을 스토리 시각화에 통합하여 GAN을 대체했습니다 [Pan et al. 2022; Rahman et al. 2022]. AR-LDM [Pan et al. 2022]은 스토리 연속을 위한 StoryDALL-E의 설정을 채택하고 잠재 확산 모델을 이미지 생성기로 활용하면서 자기 회귀 모델을 사용하여 현재 및 이전 프롬프트에서 정보를 집계합니다. Make-A-Story [Rahman et al. 2022]는 시각적 메모리 모듈을 특징으로 하는 자기 회귀 확산 기반 프레임워크를 제안합니다. AR-LDM과 비슷하게 텍스트 임베딩보다는 과거 결과를 활용하지만, 교차 어텐션 메커니즘에 의존하고 특징 공간에서 상호 작용합니다. 그러나 이러한 방법은 종종 새로운 캐릭터와 장면으로 일반화하는 데 어려움을 겪습니다. 이는 본질적으로 모델을 특정 지침 &quot;안정적인 확산을 위해 스토리에서 K 프롬프트를 생성하여 이벤트, 캐릭터 및 장면을 묘사하는 이미지를 생성&quot; 스케치 P1: &quot;고양이와 개...&quot; TaleCrafter: 여러 캐릭터를 사용한 대화형 스토리 시각화 여러 캐릭터 P1: &quot;고양이와 개...&quot; 레이아웃 S2P TXT P2: &quot;... 숲을 걷다...&quot; Pk: &quot;...공놀이...&quot; T2L C-Tdog 고양이 이미지 12V 비디오 스토리 그림 2. 대화형 스토리 시각화 시스템의 파이프라인. 이 시스템은 네 가지 구성 요소로 구성되어 있습니다. (a) 스토리-프롬프트(S2P): 대규모 언어 모델을 사용하여 문학적 및 예술적 설명과 T2l 모델에 입력된 설명 간의 격차를 메웁니다. 주어진 스토리의 내용을 이해하고 주어진 지침에 따라 T2l 모델에 적합한 프롬프트로 변환합니다. (b) 텍스트-레이아웃(T2L): 주요 프롬프트의 주제. (c) 제어 가능한 텍스트-이미지(C-T2I): 프롬프트, 레이아웃, 스케치 및 각 캐릭터의 몇 가지 이미지와 같은 다양한 조건이 주어지면 일관된 캐릭터 이미지를 생성합니다. 스케치를 통해 캐릭터, 레이아웃 및 로컬 구조를 대화형으로 편집할 수 있습니다. (d) 이미지-비디오(12V): 이미지에서 깊이를 추출하고 새로운 뷰 합성을 위한 카메라 경로를 설정하여 비디오로 변환합니다. FlintstonesSV와 같은 데이터 세트. 결과적으로 모델은 훈련 데이터 세트의 캐릭터와 장면만 기억할 수 있습니다. 이러한 제한을 없애기 위해 최근 연구[Jeong et al. 2023]는 새로운 캐릭터와 장면을 지원하는 제로 샷 스토리 시각화에 초점을 맞추고 확산 모델을 사용하여 이미지에서 캐릭터 정체성을 대체하는 방법을 제안합니다. 이는 얼굴 교환을 연상시키는 접근 방식입니다. 그러나 이 방법의 범위는 단일 인간 얼굴에 국한되며 이미지 전체의 정체성 보존 및 일관성은 여전히 만족스럽지 않습니다. 저희의 방법은 또한 여러 새로운 캐릭터와 장면을 지원하는 제로 샷 스토리 시각화를 목표로 합니다. 정체성 일관성을 보장하기 위해 각 캐릭터에 대한 작은 모델 가중치 세트를 최적화하고 여러 캐릭터를 구성하기 위한 개인화된 인페인팅 방법을 제안합니다.또한, 우리의 접근 방식은 레이아웃과 로컬 객체 구조를 제어할 수 있어 이전 연구의 역량을 능가합니다.2. 텍스트-이미지 생성 상당수의 텍스트-이미지(T2I) 방법은 GANS에 기반을 두고 있습니다[Reed et al. 2016; Xu et al. 2018; Zhang et al. 2017].일반적으로 이러한 방법에는 텍스트 인코더와 이미지 생성기가 포함됩니다.일부 접근 방식[Hong et al. 2018; Li et al. 2019b; Qiao et al. 2021]은 중간 레이아웃 생성을 사용하여 텍스트에서 직접 이미지 생성을 단순화합니다.최근 확산 모델은 여러 연구[Ramesh et al. 2022, 2021; Rombach et al. 2022; Saharia et al. 2022] 확산 모델을 사용하여 이미지 품질과 다양성을 개선합니다. 그러나 이러한 방법은 주로 여러 이미지에서 정체성 일관성을 고려하지 않고 텍스트와 단일 생성된 이미지 간의 정렬에 집중합니다. 확산 기반 T2I 생성에서 정체성을 유지하는 데 역전 방법[Gal et al. 2022; Ruiz et al. 2022; Shi et al. 2023; Yang et al. 2023]의 성공에도 불구하고 일반적으로 여러 개념을 처리하는 데 어려움을 겪는 반면 단일 개념에는 뛰어납니다. 사용자 지정 확산[Kumari et al. 2022]은 여러 개념을 구성하는 것을 목표로 하지만 고양이와 개와 같이 비슷하게 생긴 개념을 처리할 때는 실패합니다. 역전을 사용하여 이미지에서 여러 개념이나 문자를 구성하는 것은 여전히 어려운 일이므로 다른 관점에서 구성을 처리하는 개인화된 인페인팅을 위한 제어 가능한 T모델을 제안합니다. 3 방법 문자, 레이아웃 및 로컬 구조의 대화형 편집을 지원하는 대화형 스토리 시각화 시스템을 제안합니다. 대부분의 이전 작업과 달리 여러 캐릭터의 일관된 생성을 처리하고 새로운 캐릭터와 장면으로 일반화할 수 있습니다.그림 2에서 볼 수 있듯이 이 시스템은 스토리-프롬프트(S2P), 텍스트-레이아웃(T2L), 제어 가능한 텍스트-이미지(C-T2I), 이미지-비디오(I2V)의 네 가지 구성 요소로 구성됩니다.3. 스토리-프롬프트 생성 주어진 스토리는 &quot;고양이와 개가 멋진 하루를 보냈다&quot;와 같은 짧은 문장일 수도 있고, 문학적 기법을 사용한 길고 자세한 문장일 수도 있습니다. 둘 다 이미지의 이벤트, 장면, 객체를 묘사하는 캡션으로 훈련된 현재 T2I 모델의 취향에 맞지 않을 수 있습니다.최근 GPT-4[OpenAI 2023] 및 PaLM 2[et al 2023]와 같은 대규모 언어 모델 개발에서 놀라운 혁신이 이루어졌습니다. GPT-4는 시각과 언어를 포함한 방대한 멀티모달 코퍼스에서 학습되었으며, 이는 문학적 설명과 T2I 모델 설명 간의 격차를 메우는 데 적합한 도구입니다. 이 작업에서는 GPT-4를 사용합니다. GPT-4에서 지침은 중요합니다. T2I 모델 설명의 기본 요소는 이벤트, 장면 및 객체입니다. 따라서 주어진 스토리에 대해 &quot;이벤트, 캐릭터 및 장면을 묘사하는 이미지를 생성하기 위해 Stable Diffusion을 위한 스토리에서 K개의 프롬프트를 생성하세요&quot;와 같은 지침을 사용합니다. 사전 학습된 T2I 모델의 기능을 활용하여 텍스트의 키워드를 활용하여 스타일을 제어합니다. 예를 들어 &quot;유화 스타일&quot;을 프롬프트의 접미사로 사용합니다. S는 일반 텍스트의 스토리를 나타냅니다. R과 F는 각각 지침과 스타일을 나타냅니다. S2P 구성 요소의 함수는 = [P1, P2, PK] S2P(S, R, F, K), (1)로 정의할 수 있습니다. 여기서 K는 생성할 프롬프트의 수입니다. pi는 i번째 프롬프트입니다.4. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 3.2 텍스트-레이아웃 생성 변환기는 레이아웃 분포를 캡처하는 데 가장 널리 사용되는 기술입니다[Gupta et al. 2021; Jiang et al. 2022]. 그러나 자기 회귀 디코더는 고정된 생성 순서로 인해 부분 입력을 처리하는 데 유연하지 않은 것으로 나타났습니다[Kong et al. 2022]. 최근 LayoutDM[Inoue et al. 2023]에서 레이아웃 생성을 위한 이산 확산 모델[Austin et al. 2021; Gu et al. 2022]이 도입되었습니다. 이는 만족스러운 성능을 달성하고 다양한 제약 조건을 허용합니다. LayoutDM에 따라 텍스트-레이아웃 생성을 위해 이산 확산 모델을 활용합니다. L = {B;}\\₁가 N개의 객체가 있는 레이아웃을 나타내며, 여기서 B₁ = (bi, li)이다. b¡ = (xi, Yi, Wi, hi) = {1, ..., M}4는 중심이 (xi, yi)이고 너비와 높이가 (wi, h;)인 경계 상자를 나타낸다. 좌표는 정규화되고 양자화되었으며, M은 빈의 수이다. li; = {1, 2, ..., C}는 객체 범주이다. C는 객체 범주의 수이다. 평평해진 이산 벡터 L= {x1,y1, W1, h1, 11, x2, y2, ...}는 가변 길이의 잠재 변수로 처리된다. D3PM의 이산 확산 모델 정의에 따라 [Austin et al. 2021], 시간 단계 t, zt = {1, 2, ..., D}에서 D 범주를 갖는 이산 스칼라에 대한 순방향 확산 프로세스는 다음과 같이 정의할 수 있습니다. q(zt|zt−1) = v(zt)TQtv(zt−1)T, (2) 여기서 v(z+)는 zt의 원핫 벡터이고 Qt Є [0,1] DXD는 전이 행렬입니다. Z₁ = {1, 2, ..., D}N이 벡터인 경우 순방향 프로세스는 각 요소에 독립적으로 적용됩니다. DDPM [Ho et al. 2020]과 유사하게 역방향 잡음 제거 프로세스는 네트워크에 의해 추정됩니다. 즉, på(Zt−1|Zt) € [0,1] N×D¸ 0은 양방향 Transformer의 매개변수를 나타냅니다. DP3M은 po(Zt-1|Zt)를 다음과 같이 분해하고 대신 pe(zo|zt)를 추정하는 법을 학습합니다. α Po(Zt-1|Zt) ∞ Σ9(Zt−1|Zt, Zo)Ã¤(Zo|Zt), Zo (3) 여기서 q(Zt-1 Zt, Zo)는 확산 과정의 정의에 따라 폐쇄형 솔루션이 있습니다. 평탄화된 L은 가변 길이를 가지므로 LayoutDM의 패딩 트릭을 이용하여 이를 처리합니다. 널리 사용되는 변분 하한 Lvb와 추가 손실을 포함하는 D3PM의 학습 목표를 사용합니다. 즉, Ls2p = Lvb + Eq(zo) Eq(zt |zo) [— log pe(zo|zt)], 여기서 는 트레이드오프 하이퍼파라미터입니다. (4) T2L의 목적은 주어진 텍스트에 따라 레이아웃을 생성하는 것이므로 위의 공식을 직접 적용할 수 없습니다. 언어 처리 도구[Liu et al. 2021] 텍스트에서 명사를 추출하여 대상 객체로 처리합니다. 명사를 범주형 레이블로 변환하기 위해 Objectdataset의 클래스 이름을 사용합니다[Shao et al. 2019]. Object365는 이미지의 객체에 대한 경계 상자와 범주형 레이블을 제공하므로 T2L 모델을 학습하는 데 사용합니다. T2L 구성 요소의 기능은 L = = {Bi}\₁ = T2L(p), (5)로 표현할 수 있습니다. 여기서 p는 S2P 구성 요소에서 생성된 프롬프트입니다. 3. 제어 가능한 텍스트-이미지 생성 C-T2I는 스토리 시각화 시스템의 핵심 구성 요소입니다(그림 3 참조). 다중 모달 입력이 있으며 ID, 위치 및 로컬 프롬프트를 포함한 다중 레벨 제어가 있는 이미지를 생성합니다. &quot;고양이와 개...&quot;를 임베드합니다. Ет 스케치 Zt 개 ResBlock ET 고양이<box, text> Es Cross-Att Aug Self-Att Add Aq BQ Ax Bx Ay By QKV Unet Block Zt-C-TFig. 3. C-T2I 구성 요소의 구조. 노이즈가 있는 이미지를 입력으로 받고 프롬프트, 스케치, 설명이 있는 경계 상자 등 여러 유형의 안내를 조건으로 단일 노이즈 제거 단계를 통해 이미지를 생성합니다. 정체성 일관성을 위해 LORA를 사용하여 셀프 및 교차 어텐션 레이어의 개인화된 가중치와 각 문자에 대한 특정 토큰을 학습합니다. 구조. 많은 연구가 텍스트-이미지 [Rombach et al. 2022], 레이아웃-이미지 [Li et al. 2023], 스케치-이미지 [Voynov et al. 2022] 및 정체성 보존 [Kumari et al. 2022]의 개별 작업에 초점을 맞추고 있지만 대화형 스토리 시각화의 필수 기능인 다단계 제어를 동시에 처리할 수는 없습니다. 예를 들어, [Li et al. 2023]은 텍스트의 안내를 받는 레이아웃-이미지 방법입니다. 입력 상자로 객체를 찾을 수 있지만 로컬 구조와 ID를 제어할 수 없습니다. [Chen et al. 2023]은 지정된 하나의 객체의 위치를 지정할 수 있지만 여러 객체를 처리할 수 없습니다. [Kumari et al. 2022]는 두 개의 객체를 이미지에 넣으려고 하지만 위치와 포즈를 지정할 수 없습니다. 잠재 확산 모델(LDM)에서 영감을 받아 [Rombach et al. 2022], 다중 모달 조건 생성 모델은 잠재 공간에서 학습되고 분포는 확산 모델을 사용하여 포착됩니다. C-T21의 구조는 그림 3에 나와 있습니다. 여러 유형의 입력을 주입할 수 있도록 LDM에서 원래 UNet의 구조를 수정합니다. 각 UNet 블록에서 셀프 및 크로스 어텐션 블록을 업그레이드하고 추가 블록을 추가로 도입합니다. ID 보존. 사전 학습된 CLIP [Radford et al. 2021] 텍스트 인코더 E로 입력 프롬프트를 임베딩에 매핑합니다. 임베딩은 공간적 특징과 상호 작용하기 위해 교차 어텐션 모듈에 공급됩니다. 신원 보존을 위해 [Ruiz et al. 2022] 및 [Kumari et al. 2022]와 달리 학습된 T2I 모델의 모든 매개변수를 미세 조정하는 대신 LORA를 사용하여 셀프 및 교차 어텐션 계층에서 추가적인 저순위 가중치를 업데이트합니다. LORA는 쿼리, 키 및 값 매핑의 MLP에만 적용됩니다. 원래 가중치가 유지되고 매우 작은 매개변수 집합만 학습되므로 과적합 문제와 개념 망각을 완화할 수 있습니다. 문자의 몇 가지 이미지가 주어지면 토큰과 LORA 가중치 집합이 특별히 훈련됩니다. 가중치는 두 문자를 구성하는 성공률이 만족스럽지 않기 때문에 여러 문자에 대해 공동으로 최적화되지 않습니다. 특히 비슷한 모양의 문자의 경우 더욱 그렇습니다. W € Rdxk가 선형 사상의 매개변수를 나타내도록 하고, 여기서 d와 k는 각각 입력 및 출력 벡터의 차원입니다. h = Wx가 출력 벡터를 나타내도록 합니다. x = Rk는 TaleCrafter: 여러 캐릭터를 사용한 대화형 스토리 시각화 입력 벡터입니다. LORA를 사용하여 새로운 개념을 기억하면서 W를 업데이트하는 대신 두 개의 저랭크 행렬 A = Rdxr 및 B = Rrk를 학습합니다. 여기서 r &lt; min(d, k). r이 작을 때 A와 B는 W보다 훨씬 적은 매개변수를 갖습니다. 순방향 패스는 다음과 같이 다시 쓸 수 있습니다. h = Wx+BAX. 객체 현지화. 텍스트-레이아웃 구성 요소에서 레이아웃에는 좌표와 범주가 포함됩니다. C-I2I 구성 요소에서 범주를 학습된 문자 토큰을 주입하는 동안 객체를 묘사하는 문구로 바꿉니다. 예를 들어, 개인화를 위해 &quot;개&quot;를 &quot;sks 개&quot;로 바꿉니다. 여기서 sks는 학습된 토큰입니다. [Li et al. 2023], 텍스트 임베딩은 CLIP을 사용하여 추출하는 반면, 좌표는 Fourier 임베딩을 사용하여 인코딩됩니다 [Mildenhall et al. 2021]. 두 임베딩은 연결되어 정보 정렬을 위해 MLP 계층을 거친 다음 두 개의 자기 주의 계층으로 구성된 증강 자기 주의 모듈에 공급됩니다. 하나는 시각적 특징 간의 상호 작용만 포함하는 일반적인 자기 주의 계층입니다. 다른 하나는 시각적 특징과 객체 위치를 주입하기 위한 위치 임베딩 간의 상호 작용을 포함합니다. = f가 시각적 특징을 나타내도록 합니다. 예를 들어 [ET(p), Fourier(b)]가 접지 텍스트 p와 상자 b = (x, y, w, h)의 연결된 임베딩을 나타내도록 합니다. 첫 번째 자기 주의는 다음과 같이 쓸 수 있습니다. f ← f+SA(f), 여기서 SA(·)는 자기 주의 연산을 나타냅니다. 두 번째 게이트형 자기 주의는 다음과 같이 쓸 수 있습니다. f ← f + tanh(α) · TS(SA([f, eg])), 여기서 a는 초기화가 0인 변수입니다. TS()는 상호 작용 후 시각적 특징을 선택합니다. 로컬 구조 제어. 거의 모든 현재 스토리 시각화 작업은 객체의 로컬 구조 제어를 고려하지 않습니다. 레이아웃과 객체 구조는 텍스트에 의해서만 암묵적으로 결정됩니다. 구조 제어와 대화형 편집의 유연성을 도입하기 위해 스케치를 하나의 입력으로 사용합니다. T21 어댑터[Mou et al. 2023]에서 영감을 받아 시각적 인코더 Es를 사용하여 입력 스케치를 UNet의 특징 공간에 매핑합니다. 인코더는 4개의 잔여 블록의 스택입니다[He et al. 2016]. 예측된 스케치 특징은 추가 모듈에 의해 시각적 특징과 결합됩니다. 먼저 빈 캔버스에서 입력 스케치 Is를 해당 경계 상자에 따라 변환하고 크기를 조정합니다. 그런 다음 생성된 스케치 이미지 Ĩs가 Es에 입력됩니다. fs = Es(Is)는 Es가 추출한 특징을 나타냅니다. 로컬 구조 제어는 시각적 특징 f와 생성된 특징 f를 더하여 실현됩니다. 즉, f ← f + ßfs입니다. 여기서 ẞ는 구조 제약 조건을 적용하는 강도를 제어하는 매개변수입니다. 학습을 위해 ß = 1로 설정되지만 추론 중에 조정할 수 있습니다. ẞ 추론 시 스케치 입력이 필요하지 않음을 의미합니다. 반복 생성의 경우 = 0입니다. 각 문자에는 고유한 개인화된 가중치가 있으므로 여러 문자에 대한 추론 중에 토큰과 LORA 가중치가 다른 모달리티의 모듈과 함께 반복적으로 적용됩니다. 예를 들어, &quot;숲 속의 개와 고양이&quot;라는 텍스트와 개와 고양이의 상자와 스케치가 주어지면 먼저 개의 개인화된 가중치와 수정된 텍스트 &quot;a<sks> 숲 속의 개와 고양이&quot;.<sks> &quot;는 개에 대한 학습된 토큰입니다. 개 상자의 텍스트는 &quot;로 설정됩니다.<sks> 개&quot;. 고양이 상자가 있으므로 상자의 내용을 텍스트 &quot;로 칠합니다.<yty> 고양이&quot;. &quot;<yty> &quot;는 고양이의 학습된 토큰입니다. 인페인팅 모델은 증강된 입력을 가진 C-T2I의 변형입니다. 차이점은 노이즈가 있는 이미지, 원본 이미지, 상자의 영역 마스크를 연결하여 9개 채널의 입력을 형성한다는 것입니다. 학습 절차는 C-T2I와 동일합니다. 학습 목표. LDM[Rombach et al. 2022]에 따라 학습을 위한 변분 하한을 사용합니다. (6) LC-T21 = Ez~E1(x),€~N(0,1),t [||€ – €g (Zt, t, C)||], 여기서 x는 이미지를 나타내고 E1 (·)은 이미지를 잠재 공간으로 투사하는 이미지 인코더입니다. Є는 샘플링된 노이즈이고 E()는 예측된 노이즈입니다. t는 타임스텝이고 zt는 노이즈가 있는 이미지입니다. C = {ET (p), Es (Ĩs), ET (L)}는 조건의 임베딩을 나타냅니다. 즉, 프롬프트, 스케치 및 레이아웃입니다.3. 이미지-비디오 생성 스토리 시각화를 생생하게 만들기 위해 시스템에 이미지-비디오 구성 요소를 도입했습니다.이 구성 요소에서 주로 카메라 움직임에 초점을 맞춰 이미지 깊이를 고려하여 비디오를 생성합니다.3D 사진 방법[Shih et al. 2020]을 사용하여 깊이를 추출하고 참신한 관점에서 이미지를 합성하여 입체 인식 세부 정보를 향상시키고 정적 이미지보다 장면을 더 사실적으로 만들 수 있습니다.이 방법을 사용하면 확대, 원, 스윙과 같은 다양한 효과에 대한 카메라 경로를 설정할 수 있습니다.4.실험 설정 데이터 세트.T2L 구성 요소의 학습을 위해 365개 클래스, 200만 개의 이미지 및 3,000만 개의 경계 상자가 포함된 Object365 데이터 세트[Shao et al. 2019]를 사용합니다.C-T2I 구성 요소의 경우 사전 학습된 Stable Diffusion(v1.4)[Rombach et al. 2022] LAION-5B [Schuhmann et al. 2022]에서 CLIP 인코더, 이미지 자동 인코더 및 확산 모델을 포함한 사전 모델로 사용합니다. 스케치 인코더와 증강된 자기 주의 모듈은 Flickr 데이터 세트 [Plummer et al. 2015]에서 학습합니다. 스케치는 PiDiNet [Su et al. 2021]에서 추출합니다. 이 두 부분의 학습 난이도를 줄이기 위해 T2I Adapter [Mou et al. 2023]의 인코더 가중치와 GLIGEN [Li et al. 2023]의 게이트된 자기 주의 가중치를 학습을 위한 초기화로 사용합니다. 그런 다음 결과 모델을 기반으로 주어진 5-9개 이미지로 각 캐릭터에 대한 개인화된 LORA 가중치를 학습합니다. 스토리-프롬프트 생성을 위해 대규모 언어 모델 GPT-4를 사용합니다. 평가 지표. 두 차원에 따라 방법을 평가합니다. 첫째, 생성된 이미지의 텍스트 정렬을 평가하기 위해 CLIP 피처 공간에서 텍스트-이미지 유사성을 적용합니다[Hessel et al. 2021]. 그런 다음 문자의 일관성을 측정하기 위해 CLIP 이미지 피처 공간에서 이미지-이미지 유사성을 활용합니다[Gal et al. 2022]. 또한 평가를 위해 인간의 선호도 연구도 수행합니다. 기준선. 우리는 우리의 방법을 CustomDiffusion[Kumari et al. 2022], Paint-by-Example[Yang et al. 2022], Make-a-Story[Rahman et al. 2022]의 세 가지 접근 방식과 비교합니다. Custom-Diffusion은 T2I 모델에 대한 미세 조정 기술을 사용하고 단일 이미지 내에서 여러 개념과 그 구성에 대한 공동 학습을 가능하게 합니다. Paint-by-Example은 예시 기반 이미지 편집을 구성합니다. 프롬프트 #1 소녀는 고요한 작은 마을에 살고 있습니다. 6. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang 고양이 소녀 우리의 페인트-바이-예제 커스텀-디퓨전 ALE MOHOL TIE THE CIERZALDWE #2 소녀와 고양이가 멀리서 무지개 호를 바라보고 있습니다. #3 소녀와 고양이가 환상적인 세계로 들어가고, 솜과 같은 구름이 떠다니고, 다채로운 나비가 공기를 가득 채웠습니다. #4 소녀가 바람에 춤을 추고 있습니다. #5 소녀가 학교에 가고 있습니다. #6 소녀가 많은 관중을 끌어모으는 마을 광장에서 춤을 추고 있습니다. 그림 4. 커스텀-디퓨전과 페인트-바이-예제와의 비교. 한 캐릭터는 애니메이션 캐릭터이고 다른 하나는 실제 고양이입니다. 스타일은 세 가지 방법 모두에 대해 &quot;Ghibli&quot;에서 지정합니다. 스토리텔링 목적으로 이미지에 캐릭터를 삽입하는 것을 용이하게 하는 방법입니다. Make-a-Story는 향상된 캐릭터와 배경 일관성을 보이는 스토리를 생성하도록 설계된 자기 회귀적 심층 생성 프레임워크를 도입합니다. 그러나 학습을 위해 광범위한 스토리 데이터에 의존하고 제한된 데이터 세트와 호환되지 않기 때문에 정성적 실험만 수행합니다. 구현 세부 정보. 모델과 Custom-Diffusion은 모두 동일한 대상 데이터 세트와 정규화 데이터 세트를 사용하여 학습하며, 각각 학습률은 1e-4와 1e-5입니다. 각 스토리에서 두 캐릭터에 대해 Custom-Diffusion을 공동으로 학습합니다. Paint-by-Example이 제로 샷 이미지 편집 방법을 구성한다는 점을 고려하여 학습 세트의 대상 이미지를 사용하여 동일한 배경 이미지와 경계 상자를 편집합니다. 공정성을 보장하기 위해 이를 비교할 때 스케치 안내를 생략합니다. 4.2 정성적 비교. 그림 4와 그림 5에서 제안하는 방법을 최첨단 기준선과 비교하여 정성적으로 평가합니다. Make-a-Story와 비교하기 위해 Make-a-Story 논문의 예를 활용하여 동일한 프롬프트와 캐릭터를 사용하여 콘텐츠를 생성했습니다. 결과는 그림 5에 나와 있습니다. 저희 방법은 아티팩트가 적고 이미지 품질이 더 높으며 캐릭터의 정체성을 더 잘 보존합니다. Make-a-Story의 결과에서 여성의 드레스가 이미지 전체에서 바뀌는 것을 볼 수 있습니다. 저희의 결과는 더 일관적입니다. 게다가 저희 방법은 FlintstonesSV와 같은 특정 데이터에 대한 학습이 필요하지 않은 일반적인 방법이므로 FlintstonesSV의 범위를 벗어나는 새로운 장면을 생성할 수 있습니다. 프롬프트 남자 여자 Make-a-Story 사용자 정의 확산 페인트-예제 저희 # 1 남자와 여자는 # 2 남자가 방에 서서 서로 이야기하고 있고, 남자는 짜증 난 표정으로 가방을 들고 있고, 옷 더미를 들고 있는 유화, 그림 # 3 여자가 방에 서 있고, 유화. # 4 여인이 방 안에 서서 킥킥 웃고 있습니다.유화.그림 5. FlintsonesSV 데이터 세트의 캐릭터를 사용하여 Make-a-Story, Custom-Diffusion 및 Paint-byExample과 비교.그림 4는 애니메이션 소녀와 실제 고양이의 스토리 시각화 결과를 보여줍니다.Paint-by-Example과 Custom-Diffusion은 Method Custom-Diffusion text-image sim.0.image-image sim.0.Paint-by-Example Ours 0.0.0.0.표 1. 양적 비교.텍스트-이미지 및 이미지-이미지 유사도는 CLIP 피처 공간에서 계산됩니다. 방법 Custom-Diffusion Paint-by-Example Ours Correspondence 1.1.2.Coherence 1.1.2.Quality 1.1.2.TaleCrafter: 여러 캐릭터 참조 마스크를 사용한 대화형 스토리 시각화 GLIGEN OursTable 2. 텍스트-이미지 정렬, 신원 보존 및 이미지 품질에 대한 사용자 연구. 점수가 높을수록 성능이 우수함을 나타냅니다. 신원 보존. 그들은 항상 손수건을 놓치거나 부정확한 손수건을 생성합니다. 게다가 Paint-by-Example은 이미지 스타일에 따라 고양이를 애니메이션 고양이로 변환하지 못합니다. Custom-Diffusion은 두 번째 열에서와 같이 두 캐릭터를 구성할 확률이 낮습니다. 다르게, 우리의 방법은 신원 교차 이미지를 보존하는 데 더 나은 성과를 보이며 이미지 스타일을 기반으로 캐릭터를 조화롭게 구성할 수도 있습니다. 4.3 양적 비교. 우리는 프롬프트당 20개의 샘플이 있는 35개의 프롬프트를 포함하는 5개의 스토리를 평가하여 총 700개의 생성된 이미지를 생성합니다. 우리는 50단계로 구성된 DDIM 샘플링과 모든 접근 방식에 걸쳐 6의 분류자 없는 안내 값을 사용합니다. CLIP 피처 공간의 텍스트-이미지 유사도는 프롬프트와 생성된 이미지 간의 정렬을 측정하는 데 사용됩니다. 이미지-이미지 유사도는 신원 보존의 성능을 측정합니다. 우리는 주어진 문자 이미지의 평균 임베딩을 계산한 다음 생성된 이미지와 그 이미지 간의 유사도를 계산합니다. 표 1에서 보여 주듯이, 우리의 방법은 경쟁 방법을 능가합니다. 가장 높은 텍스트-이미지 및 이미지-이미지 유사도를 확보함으로써, 우리의 방법은 스토리 생성에서 향상된 정확도를 보여줍니다. 4.4 사용자 연구 우리는 Custom-Diffusion, Paint-by-Example 및 우리의 방법에 의한 9개 스토리의 시각화 결과에 대한 사용자 연구를 수행합니다. 각 스토리에는 프롬프트가 있으며, 그 결과 3개의 생성된 이미지가 생성됩니다. 주어진 각 스토리에 대해 50명의 참가자에게 다음 세 가지 측면에서 세 가지 경쟁 방법의 성능을 순위를 매기도록 요청합니다. 첫째, 생성된 이미지가 입력 텍스트 설명을 정확하게 반영하는지 여부. 둘째, 이미지가 캐릭터 신원을 일관되게 보존하는지 여부. 셋째, 생성된 이미지의 시각적 품질. 점수 범위는 1~3입니다. 점수가 높을수록 성능이 우수함을 나타냅니다. 세 가지 측면의 평균 점수는 표 2에 나와 있습니다. 일반적으로 우리의 접근 방식은 모든 측면에서 가장 높은 점수를 달성하여 제안된 프레임워크의 효능을 보여줍니다. 4.5 대화형 편집 및 이미지 애니메이션 대화형 편집. 우리 시스템은 레이아웃, 캐릭터 및 로컬 구조의 대화형 편집을 허용합니다. 여기서는 캐릭터와 로컬 구조의 제어 기능을 제시합니다. 그림 6과 같이 GLIGEN의 신원 보존을 비교합니다. GLIGEN은 참조 이미지를 입력으로 사용하여 마스크된 영역을 채우는 반면 우리 방법은 개인화된 토큰과 가중치를 사용합니다. 그림 6. 신원 제어에 대한 GLIGEN과의 비교. 그림 7. 스케치 제어 시각화. 우리 모델은 입력 스케치를 기반으로 로컬 객체 구조에 대한 제어를 사용하여 이미지를 생성할 수 있습니다. 생성된 캐릭터가 GLIGEN보다 참조와 더 비슷하다는 것을 관찰했습니다. 구조 제어의 검증은 그림 7에 나와 있습니다. 다른 스케치가 주어지면 합성된 캐릭터는 해당 포즈와 제스처 아래에 있습니다. 이미지 애니메이션. 12V 구성 요소는 이미지에서 깊이를 추출하고 카메라 경로를 설정하여 이미지를 비디오로 변환합니다. 또한 텍스트 음성 변환을 사용하여 스토리를 오디오로 변환하고 생성된 비디오와 결합합니다. 비디오 결과는 보충 자료에 나와 있습니다. 5 제한 사항 시스템은 사전 학습된 Stable Diffusion을 기반으로 합니다. 합성 이미지의 품질은 사전 학습된 모델의 기능에 크게 의존합니다. Stable Diffusion(v1.4)은 얼굴 생성에서 성능이 좋지 않기 때문에, 특히 얼굴이 이미지의 작은 영역만 덮는 경우 시스템에서 이러한 단점을 물려받았습니다. 또 다른 제한 사항은 스케치를 현재 제공해야 한다는 것입니다. 소스 스케치는 이미지 검색, 그리기, 3D 렌더링 또는 T2I 생성 이미지에서 가져올 수 있습니다. 참조 이미지와 텍스트를 기반으로 하는 자동 스케치 생성은 향후 작업으로 처리할 수 있습니다.
--- CONCLUSION ---
우리는 새로운 캐릭터와 장면을 처리할 수 있는 혁신적인 일반 대화형 스토리 시각화 시스템을 제시하는 동시에 8. Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang은 정체성 일관성, 텍스트와 시각적 콘텐츠 간의 정렬, 합리적인 개체 레이아웃을 유지합니다. 시스템의 네 가지 상호 연결된 구성 요소(스토리-프롬프트 생성(S2P), 텍스트-레이아웃 생성(T2L), 제어 가능한 텍스트-이미지 생성(C-T2I), 이미지-비디오 애니메이션(12V))는 조화롭게 작동하여 대화형이고 유연한 시각화 솔루션을 만듭니다. 광범위한 실험과 사용자 연구를 통해 스토리 시각화에서 제안된 시스템의 효과가 입증되었습니다. 참고문헌 Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg. 2021. 이산 상태 공간에서의 구조화된 노이즈 제거 확산 모델. 신경 정보 처리 시스템의 발전 34(2021), 17981-17993. Hong Chen, Rujun Han, Te-Lin Wu, Hideki Nakayama, and Nanyun Peng. 2022. 시각적 계획 및 토큰 정렬을 통한 캐릭터 중심 스토리 시각화. arXiv 사전 인쇄본 arXiv:2210.08465(2022). Minghao Chen, Iro Laina, and Andrea Vedaldi. 2023. 교차 주의 안내를 통한 훈련 없는 레이아웃 제어. arXiv 사전 인쇄본 arXiv:2304.03373(2023). Rohan Anil et al. 2023. PaLM 2 기술 보고서. arXiv:2305.10403 [cs.CL] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618(2022). Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo. 2022. 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10696-10706. Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S Davis, Vijay Mahadevan, Abhinav Shrivastava. 2021. Layouttransformer: 셀프 어텐션을 사용한 레이아웃 생성 및 완성. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록. 1004-1014. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016. 이미지 인식을 위한 심층적 잔여 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 770-778. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi. 2021. Clipscore: 이미지 캡션을 위한 참조 없는 평가 지표. arXiv 사전 인쇄본 arXiv:2104.08718(2021). Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303(2022). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840-6851. Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. 2018. Inferring semantic layout for hierarchical text-to-image synthesis. IEEE conference on computer vision and pattern awareness에서. 7986-7994. Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. 2023. LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. arXiv 사전 인쇄본 arXiv:2303.08137 (2023). Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. 2023. 확산 모델을 사용하여 일반 텍스트 스토리에서 일관된 스토리북의 제로샷 생성. arXiv 사전 인쇄본 arXiv:2302.03900(2023). Zhaoyun Jiang, Shizhao Sun, Jihua Zhu, Jian-Guang Lou, Dongmei Zhang. 2022. 그래픽 레이아웃을 위한 Coarse-to-Fine 생성 모델링. AAAI 인공지능 컨퍼런스 회의록, Vol. 36. 1096-1103. Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, Irfan Essa. 2022. BLT: 제어 가능한 레이아웃 생성을 위한 양방향 레이아웃 변환기. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII. Springer, 474-490. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv 사전 인쇄본 arXiv:2212.04488 (2022). Bowen Li. 2022. Word-Level Fine-Grained Story Visualization. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI. Springer, 347-362. Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, Jianfeng Gao. 2019b. 적대적 학습을 통한 객체 기반 텍스트-이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 12174-12182. Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao. 2019a. Storygan: 스토리 시각화를 위한 순차적 조건부 gan. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 6329-6338. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li 및 Yong Jae. 2023. GLIGEN: 개방형 기반 텍스트-이미지 생성. arXiv 사전 인쇄 arXiv:2301.07093(2023). Jiadong Liang, Wenjie Pei 및 Feng Lu. 2023. 레이아웃 브리징 텍스트-이미지 합성. 비디오 기술을 위한 회로 및 시스템에 관한 IEEE 거래(2023). Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang 및 Mark D Plumbley. 2023. Audioldm: 잠재 확산 모델을 사용한 텍스트-오디오 생성. arXiv 사전 인쇄 arXiv:2301.12503 (2023). Lemao Liu, Haisong Zhang, Haiyun Jiang, Yangming Li, Enbo Zhao, Kun Xu, Linfeng Song, Suncong Zheng, Botong Zhou, Jianchen Zhu, Xiao Feng, Tao Chen, Tao Yang, Dong Yu, Feng Zhang, Zhanhui Kang 및 Shuming Shi. 2021. TexSmart: 향상된 자연어 이해를 위한 시스템. ACL-IJCNLP에서. 아디아샤 마하라나(Adyasha Maharana)와 모히트 반살(Mohit Bansal). 2021. 시공간적, 언어적, 상식적 구조를 스토리 시각화에 통합합니다. arXiv 사전 인쇄 arXiv:2110.(2021). 아디아샤 마하라나(Adyasha Maharana), 대릴 한난(Darryl Hannan), 모히트 반살(Mohit Bansal). 2021. 의미론적 일관성을 통해 시각적 스토리의 생성 및 평가를 개선합니다. arXiv 사전 인쇄 arXiv:2105.(2021). Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022. Storydall-e: 스토리 연속을 위한 사전 학습된 텍스트-이미지 변환기 조정. Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII. Springer, 70-87. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. Commun. ACM 65, 1 (2021), 99–106. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023. T2i-adapter: 텍스트-이미지 확산 모델에 대한 보다 제어 가능한 기능을 발굴하기 위한 학습 어댑터. arXiv 사전 인쇄본 arXiv:2302.08453(2023). OpenAI. 2023. GPT-4 기술 보고서. arXiv:2303.08774 [cs.CL] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen. 2022. 자기 회귀 잠복 확산 모델을 사용한 일관된 스토리 합성. arXiv 사전 인쇄본 arXiv:2211.10950(2022). Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, Svetlana Lazebnik. 2015. Flickr30k 엔티티: 더 풍부한 이미지-문장 모델을 위한 영역-구문 대응 수집. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집. 2641-2649. Yanyuan Qiao, Qi Chen, Chaorui Deng, Ning Ding, Yuankai Qi, Mingkui Tan, Xincheng Ren, Qi Wu. 2021. R-GAN: 생성적 적대적 네트워크를 통한 합리적인 텍스트-이미지 합성을 위한 인간과 유사한 방법 탐색. 제29회 ACM 국제 멀티미디어 컨퍼런스 논문집. 2085-2093. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 등. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스에서. PMLR, 8748-8763. Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, Leonid Sigal. 2022. Make-A-Story: 시각적 기억 조건 일관된 스토리 생성. arXiv 사전 인쇄본 arXiv:2211.13319(2022). Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125(2022). Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 2021. Zero-shot text-to-image generation. 국제 기계 학습 컨퍼런스에서. PMLR, 8821-8831. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee. 2016. 생성적 적대적 텍스트-이미지 합성. 국제 기계 학습 컨퍼런스에서. PMLR, 1060-1069. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 10684-10695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2022. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄본 arXiv:2208.12242(2022). Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전 35(2022), 36479-36494. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: 차세대 이미지-텍스트 모델을 학습하기 위한 오픈 대규모 데이터 세트. arXiv 사전 인쇄본 arXiv:2210.08402(2022). Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun. 2019. Objects365: 객체 감지를 위한 대규모 고품질 데이터 세트. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 8430-8439. Jing Shi, Wei Xiong, Zhe Lin, Hyun Joon Jung. 2023. Instant Booth: 테스트 시간 미세 조정 없이 개인화된 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2304. TaleCrafter: 여러 캐릭터를 사용한 대화형 스토리 시각화(2023). Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 컨텍스트 인식 계층 깊이 인페인팅을 사용한 3D 사진. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 8028-8038. Yun-Zhu Song, Zhi Rui Tam, Hung-Jen Chen, Huiao-Han Lu, and Hong-Han Shuai. 2020. 문자 보존 일관된 스토리 시각화. Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, 2020년 8월 23-28일, 회의록, Part XVII 16. Springer, 18-33. Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietikäinen, and Li Liu. 2021. 효율적인 에지 감지를 위한 픽셀 차이 네트워크. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스의 진행 중. 5117-5127. Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2022. 스케치 가이드 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2211.13752 (2022). Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: 주의 생성적 적대적 네트워크를 사용한 세분화된 텍스트-이미지 생성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 1316-1324. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen 및 Fang Wen. 2022. 예시별 그림 그리기: 확산 모델을 사용한 예시 기반 이미지 편집. arXiv 사전 인쇄 arXiv:2211.13227 (2022). Jianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang Chen 및 Junbo Zhao. 2023. 개인화된 텍스트-이미지 생성을 위한 제어 가능한 텍스트 반전. arXiv 사전 인쇄 arXiv:2304.05265 (2023). Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang 및 Yujiu Yang. 2022. StyleHEAT: 사전 훈련된 StyleGAN을 통한 원샷 고해상도 편집 가능 화자 생성. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, 2022년 10월 23-27일, Proceedings, Part XVII. Springer, 85-101. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas. 2017. Stackgan: 스택 생성적 적대적 네트워크를 사용한 텍스트에서 사진과 같은 사실적인 이미지 합성. IEEE 국제 컴퓨터 비전 컨퍼런스의 Proceedings. 5907-5915.
