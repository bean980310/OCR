--- ABSTRACT ---
제어 가능한 비디오 생성은 최근 몇 년 동안 상당한 주목을 받았습니다. 그러나 두 가지 주요 한계가 있습니다. 첫째, 대부분의 기존 연구는 텍스트, 이미지 또는 궤적 기반 제어에 초점을 맞추어 비디오에서 세밀한 제어를 달성할 수 없습니다. 둘째, 궤적 제어 연구는 아직 초기 단계에 있으며 대부분의 실험은 Human3.6M과 같은 간단한 데이터 세트에서 수행되고 있습니다. 이러한 제약으로 인해 모델이 오픈 도메인 이미지를 처리하고 복잡한 곡선 궤적을 효과적으로 처리할 수 있는 기능이 제한됩니다. 이 논문에서는 오픈 도메인 확산 기반 비디오 생성 모델인 DragNUWA를 제안합니다. 기존 작업에서 제어 세분성이 부족하다는 문제를 해결하기 위해 텍스트, 이미지 및 궤적 정보를 동시에 도입하여 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠에 대한 세밀한 제어를 제공합니다. 현재 연구에서 제한된 오픈 도메인 궤적 제어 문제를 해결하기 위해, 우리는 세 가지 측면을 갖춘 궤적 모델링을 제안합니다. 임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성에서 궤적을 제어하는 멀티스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 훈련(AT) 전략입니다. 우리의 실험은 DragNUWA의 효과를 검증하여 비디오 생성에서 세밀한 제어에서 뛰어난 성능을 보여줍니다. 홈페이지 링크는 https://www.microsoft.com/en-us/research/project/dragnuwa/입니다. 두 저자 모두 이 연구에 동등하게 기여했습니다. Shengming, Jian, Jie는 Chenfei의 멘토십을 받아 인턴십을 진행했습니다. 연락 저자.*진행 중인 작업 호수 위를 항해하는 보트 숲에서 스키를 타는 소녀 푸른 숲을 여행하는 증기 기관차 거리를 걷는 사람들 몇 명 숲을 걷는 코끼리 무리 얼룩말이 달리고 있다 길을 달리는 개 청명절 강을 따라 테이블에 앉아 토론하는 사람들 그림 2: DragNUWA에서 생성한 샘플이 표시되어 있으며, 첫 번째 열에는 텍스트, 이미지, 궤적의 세 가지 입력 컨트롤이 표시됩니다. 두 번째, 세 번째, 네 번째 열에는 각각 출력 비디오의 5번째, 10번째, 15번째 프레임이 표시됩니다. 총 576 x 320 해상도의 프레임이 16개 있습니다. DragNUWA는 카메라, 여러 객체, 복잡한 궤적의 움직임을 동시에 제어할 수 있어 실제 장면과 예술적 그림이 모두 포함된 비디오를 생성할 수 있습니다.*진행 중인 작업
--- INTRODUCTION ---
제어 가능한 비디오 생성은 연구에서 뜨거운 주제입니다. 이러한 연구의 대부분은 제어 가능한 시각적 생성에 초점을 맞춥니다. 초기 연구는 주로 이미지 대 비디오 생성을 강조하여 생성된 비디오를 공간적으로 조작하기 위한 제어로 초기 프레임 이미지를 사용했습니다(Lotter et al. (2016); Srivastava et al. (2015); Chiappa et al. (2016). 그러나 제어로 이미지에만 의존하면 미래 비디오의 후속 프레임을 결정할 수 없습니다. 결과적으로 텍스트를 사용하여 비디오 생성을 의미적으로 제한하는 텍스트 대 비디오 연구에 대한 관심이 커지고 있습니다(Wu et al. (2021; 2022); Hong et al. (2022); Singer et al. (2022); Ho et al. (2022). 일부 연구에서는 비디오 생성을 보다 정밀하게 제어하기 위해 텍스트와 이미지 조건을 모두 활용하기도 합니다(Hu et al. (2022); Yin et al. (2023); Esser et al. (2023). 그럼에도 불구하고, 언어와 이미지는 카메라 움직임과 복잡한 객체 궤적과 같은 비디오의 시간 정보를 표현하는 데 여전히 제한적입니다. 비디오의 시간 정보를 제어하기 위해 궤적 기반 제어는 점점 더 많은 관심을 받고 있는 사용자 친화적인 접근 방식으로 등장했습니다. CVG Hao 등(2018)과 C2M Ardino 등(2021)은 이미지와 궤적을 인코딩하여 제어 가능한 비디오 생성을 위한 중간 결과로 광학 흐름 맵과 워프 기능을 예측합니다. 그러나 워프 연산은 종종 부자연스러운 왜곡을 초래합니다. 이 문제를 해결하기 위해 II2V Blattmann 등(2021b)과 iPOKE Blattmann 등(2021a)은 비디오를 밀도 있는 잠재 공간으로 압축하고 RNN을 사용하여 이러한 잠재 변수를 조작하는 방법을 학습합니다. 마찬가지로 MCDiff Chen 등(2023)은 자기 회귀 방식으로 확산 잠재로 미래 프레임을 예측합니다. MCDiff는 유망한 결과를 얻었지만, HRNet Wang et al. (2021)에 의존하여 각 사람에 대해 17개의 핵심 포인트를 추출하여 데이터를 구성하지만, 인간의 동작만 제어할 수 있습니다. 또한 MCDiff와 앞서 언급한 모델은 언어 제어를 고려하지 않아 비디오를 효과적으로 제어하는 능력이 제한됩니다. 앞서 언급한 연구는 제어 가능한 비디오 생성에 대한 두 가지 비전을 제시했습니다. 1) 첫째, 기존 작업에서 텍스트, 이미지 및 궤적 기반 제어에 대한 현재 고려 사항은 충분히 포괄적이지 않습니다. 이 세 가지 유형의 제어는 각각 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠의 조절에 기여하기 때문에 필수적이라고 주장합니다. 그림 1에서 볼 수 있듯이 텍스트와 이미지의 조합만으로는 비디오에 존재하는 복잡한 동작 세부 정보를 전달하기에 충분하지 않으며, 궤적 정보를 통합하여 보완할 수 있습니다. 또한 이미지와 궤적만으로는 비디오의 미래 객체를 적절하게 표현하지 못할 수 있지만 언어는 이러한 단점을 보완할 수 있습니다. 마지막으로, 궤적과 언어에만 의존하면 실제 물고기와 물고기 그림을 구별하는 것과 같은 추상적인 개념을 표현할 때 모호함이 발생할 수 있지만 이미지는 필요한 구별을 제공할 수 있습니다.2) 둘째, 궤적 제어에 대한 현재 연구는 아직 초기 단계에 있으며 대부분의 실험은 Human3.6M과 같은 간단한 데이터 세트에서 수행되고 있습니다.이러한 제약으로 인해 모델이 오픈 도메인 이미지를 처리하고 복잡한 곡선 궤적, 여러 객체 이동 및 카메라 모션을 동시에 효과적으로 처리하는 기능이 제한됩니다.이러한 관찰을 바탕으로 오픈 도메인 비디오 생성 모델인 DragNUWA를 제안합니다.기존 작업에서 제어 세분성이 부족하다는 문제를 해결하기 위해 텍스트, 이미지 및 궤적 정보를 동시에 도입하여 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠에 대한 세밀한 제어를 제공합니다. 현재 연구에서 제한된 오픈 도메인 궤적 제어 문제를 해결하기 위해 세 가지 측면으로 궤적을 모델링합니다.임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성으로 궤적을 제어하는 멀티스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 학습(AT) 전략입니다.본 연구의 주요 기여는 다음과 같습니다.• 텍스트, 이미지, 궤적이라는 세 가지 필수 제어를 완벽하게 통합하여 강력하고 사용자 친화적인 제어 기능을 제공하는 엔드투엔드 비디오 생성 모델인 DragNUWA를 소개합니다.• 세 가지 측면으로 궤적 모델링에 중점을 둡니다.임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성으로 궤적을 제어하는 멀티스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 학습(AT) 전략입니다. • 우리는 DragNUWA의 효율성을 검증하기 위해 광범위한 실험을 수행하여 비디오 합성에서 세분화된 제어에서 뛰어난 성능을 보여줍니다.*진행 중인 작업 2
--- RELATED WORK ---
S 2.1 비디오 합성에서의 텍스트/이미지 제어 초기 연구는 주로 이미지-비디오 생성에 중점을 두었으며, 환경은 결정적이며 가능한 미래가 하나뿐이라는 공통된 가정이 있었습니다.Lotter et al. (2016); Srivastava et al. (2015); Chiappa et al. (2016). 그러나 이 가정은 무한한 가능성을 가진 실제 세계 비디오의 요구 사항을 충족시킬 수 없습니다. 이 문제를 해결하기 위해 텍스트-비디오 생성은 최근 몇 년 동안 널리 연구되었으며(GODIVA Wu et al. (2021), NUWA Wu et al. (2022), Cog Video Hong et al. (2022), Make A Video Singer et al. (2022), Imagen Video Ho et al. (2022)), 비디오 생성의 내용을 의미적으로 제어하기 위해 텍스트 설명을 도입했습니다. 그러나 텍스트만으로는 시각적 요소의 공간 정보를 정확하게 설명할 수 없습니다. 따라서 MAGE Hu et al. (2022)는 텍스트-이미지-비디오를 강조하여 텍스트의 의미 정보와 이미지의 공간 정보를 모두 사용하여 정확한 비디오 제어를 수행합니다. 마찬가지로 GEN-1 Esser et al. (2023)은 제어를 위해 교차 주의 메커니즘을 사용하여 텍스트와 깊이 맵을 통합합니다. 긴 비디오 생성 영역에서는 텍스트-이미지-비디오도 널리 사용되었습니다. 예를 들어, Phenaki Villegas et al. (2022)는 이전 프레임과 텍스트를 자동 회귀적으로 도입하여 후속 프레임을 생성하여 긴 비디오 생성을 달성합니다. NUWA-XL Yin et al. (2023)은 계층적 확산 아키텍처를 사용하여 이전 프레임과 텍스트를 기반으로 중간 프레임을 지속적으로 완성합니다. 텍스트와 이미지는 의미와 모양을 효과적으로 전달할 수 있지만 복잡한 동작 정보와 카메라 움직임을 적절하게 표현하는 데 어려움을 겪습니다. 이러한 접근 방식과 달리 DragNUWA는 텍스트와 이미지 제어에 궤적 제어를 추가하여 의미, 모양 및 동작 측면에서 비디오를 세밀하게 제어할 수 있습니다. 2. 비디오 합성에서의 궤적 제어 비디오의 동작을 보다 잘 제어하기 위해 미래 비디오 예측
--- METHOD ---
s는 주어진 비디오 프레임을 기반으로 후속 프레임 생성을 제어합니다 Wu et al. (2020); Wichers et al. (2018); Walker et al. (2017); Liang et al. (2017). 반면, 비디오-비디오 생성은 완전한 비디오 또는 비디오 스케치의 스타일을 새로운 도메인으로 전송하여 풍부한 제어 정보를 제공합니다 Chan et al. (2019); Wang et al. (2019). 그러나 이를 위해서는 사용자가 비디오 입력을 제공해야 하며 스타일 전송이 원본 비디오의 스켈레톤을 기반으로 하기 때문에 세부적인 제어가 제한됩니다. 결과적으로 이미지-궤적-비디오 방법이 등장하여 이미지에서 주어진 궤적을 통해 비디오 개발을 제어합니다. CVG Hao et al. (2018) 및 C2M Ardino et al. (2021)은 이미지와 궤적을 인코딩하여 제어 가능한 비디오 생성을 위한 중간 결과로 광학 흐름 맵과 워프 기능을 예측합니다. 그러나 워프 작업은 종종 부자연스러운 왜곡을 초래합니다. II2V Blattmann 등(2021b)과 iPOKE Blattmann 등(2021a)은 비디오를 고밀도 잠재 공간으로 압축하고 순환 신경망을 사용하여 이러한 잠재 변수를 조작하는 방법을 학습합니다. 그러나 궤적 제어는 픽셀 수준에서 작동하기 때문에 희소하고 모호하기 쉽습니다. 이 문제를 해결하기 위해 희소한 스트로크를 먼저 고밀도 흐름으로 변환한 다음 자기 회귀를 사용하여 고밀도 흐름을 기반으로 미래 프레임을 예측합니다. 그럼에도 불구하고 MCDiff Chen 등(2023)은 HRNet Wang 등(2021)에 의존하여 각 사람에 대해 17개의 키포인트를 추출하여 데이터를 구성하기 때문에 사람의 동작만 제어할 수 있습니다. 오픈 도메인 객체의 제어를 달성하기 위해 Video Composer Wang 등(2023)은 아주 최근에 MPEG-4를 사용하여 비디오에서 동작 벡터 정보를 추출하여 훈련 조건으로 사용했지만 동작 벡터에 고수준 의미 정보가 부족하여 간단한 객체 움직임만 제어할 수 있었습니다. 이전 연구가 인간의 동작이나 기본적인 객체 움직임에만 초점을 맞춘 것과 달리, DragNUWA는 이미지에서 모든 객체를 드래그하여 여러 객체를 제어하고 복잡한 궤적과 카메라 움직임을 수용할 수 있도록 함으로써 세분화된 오픈 도메인 비디오 생성을 달성하는 선구적인 접근 방식으로 돋보입니다.3 방법 텍스트 기반 Wu et al.(2021), 이미지 기반 Zhang et al.(2020) 또는 궤적 기반 제어 Hu et al.(2022)만 지원하는 이전 연구와 달리 DragNUWA는 세 가지 유형의 제어를 모두 통합하는 동시에 세 가지 측면에서 궤적 모델링을 강조하도록 설계되었습니다.€ EN (0, 1) xt Χρ *진행 중인 작업 v Unimatch Trajectory Sampler(TS) fg 반복된 첫 번째 프레임 이미지 Enc Adaptive Training(AT) Trajectory Enc Multiscale Fusion(MF) 프롬프트: 타호 호수 근처의 구불구불한 길을 빠르게 운전합니다. Text Enc Down Down Down Down Mid dn Up Up hout hin hs € (x+, S, g,p) MSE Loss 그림 3: DragNUWA의 학습 프로세스 개요.DragNUWA는 텍스트 p, 이미지, 궤적 g의 세 가지 선택적 입력을 지원하고 세 가지 측면에서 궤적을 설계하는 데 중점을 둡니다.첫째, 궤적 샘플러(TS)는 오픈 도메인 비디오 흐름에서 궤적을 동적으로 샘플링합니다.둘째, 멀티스케일 퓨전(MF)은 UNet 아키텍처의 각 블록 내에서 궤적을 텍스트와 이미지와 긴밀하게 통합합니다.마지막으로, 적응형 학습(AT)은 모델을 광학 흐름 조건에서 사용자 친화적인 궤적으로 조정합니다.궁극적으로 DragNUWA는 여러 객체와 복잡한 궤적이 있는 오픈 도메인 비디오를 처리할 수 있습니다. • • • 1) 임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하기 위해, 3.1절에 소개된 궤적 샘플러(TS)를 사용하여 훈련 중에 오픈 도메인 비디오 흐름에서 궤적을 직접 샘플링합니다. 이는 MCDiff Chen et al.(2023)에서 사용된 인간 포즈 궤적과 같은 특정 도메인과는 대조적입니다. 2) 다양한 궤적 세분성에 대한 제어를 달성하기 위해, 3.2절에 소개된 멀티스케일 퓨전(MF)을 사용하여 궤적을 다양한 스케일로 다운샘플링하고 UNet 아키텍처의 각 블록 내에서 텍스트와 이미지와 심층적으로 통합합니다. Chen et al.(2023); Wang et al.(2023)에서처럼 확산 노이즈로 제어를 직접 연결하는 대신입니다. 3) 안정적이고 일관된 비디오를 생성하기 위해 적응형 훈련(AT)(3.3절에서 소개) 방식을 채택하여 처음에는 밀도 흐름에 조건을 두어 비디오 생성을 안정화한 다음 희소 궤적에 대해 훈련하여 모델을 적응시킵니다.다음 섹션에서는 3.1절에서 3.3절까지의 훈련 프로세스를 소개하는 데 중점을 두고, 특히 모델이 &lt;v, p&gt;로 표시된 입력 비디오와 텍스트 쌍을 사용하여 손실을 계산하는 방법을 설명합니다.3.4절에서는 모델이 입력 텍스트 p, 이미지 s, 궤적 g를 처리하여 생성된 비디오 v를 출력하는 방법을 보여주는 추론 프로세스를 소개합니다.3.1 궤적 샘플러(TS) 훈련 데이터에는 비디오와 텍스트 쌍 &lt;v,p&gt;만 포함되어 있으므로 비디오에서 궤적을 추출하는 것이 필수적입니다.이전 연구에서는 주로 주요 지점 추적 모델을 사용하여 훈련을 위해 비디오 궤적을 미리 추출했습니다.그러나 이 방식에는 두 가지 주요 단점이 있습니다. 첫째, 이러한 모델은 인간 포즈와 같은 특정 도메인에서 학습되므로 오픈 도메인 비디오를 처리하는 능력이 제한적입니다. 둘째, 실제 응용 프로그램에서는 사용자가 주요 지점에서 정확하게 궤적을 입력하도록 보장하기 어려워서 학습과 추론 사이에 차이가 발생합니다. 오픈 도메인 비디오 궤적을 용이하게 하고 사용자가 임의의 궤적을 입력할 수 있도록 하기 위해 비디오 광학 흐름에서 궤적을 직접 샘플링하는 궤적 샘플러(TS)를 설계했습니다. 이를 통해 모델은 오픈 도메인 설정에서 다양한 가능한 궤적을 학습할 수 있습니다. 프레임 L개, 채널 C개, 높이 H개, 너비 W개가 있는 비디오 v ER¹×C×H×W가 주어지면 먼저 광학 흐름 추정기인 Unimatch Xu et al. (2023)을 사용하여 밀도가 높은 광학 흐름 fЄ R(L-1)×C×H×W를 추출합니다. 명확성을 위해 첫 번째와 두 번째 프레임의 광학 흐름을 fo ERC×H×W로 나타냅니다. 간단한 방법은 광학 흐름의 강도에 따라 fo에서 궤적을 직접 샘플링하는 것입니다. 그러나 이렇게 하면 움직임이 큰 객체에 과도한 샘플링이 발생하는 반면 움직임이 작은 객체는 학습 기회가 제한됩니다. 이 문제를 해결하기 위해 X 간격으로 앵커 포인트를 균일하게 분산합니다. 또한 전체 이미지 영역을 가능한 한 많이 커버하기 위해 앵커 포인트에 -1/2에서 1/2까지의 무작위 섭동을 추가합니다. 마지막으로, 우리는 다음의 약간 더 희소한 고정된 광학 흐름을 얻습니다: (0, else fo,i,j = \fo,i,j, (i+8)%λ = 0 &amp; (j + 8)%λ =(1) 여러 궤적에 대한 제어를 지원하기 위해, 우리는 최대 궤적 수 N을 정의하고 궤적 수 n ~ U[1, N]을 무작위로 샘플링합니다. 흐름 강도에 따라 궤적을 선택하는 동안 크고 작은 모션 객체를 모두 수용하기 위해, 우리는 다항 분포 M(n, ||fo,i,j||2)에 따라 fő,¿¿에서 n개의 앵커 추적 포인트를 샘플링합니다. 이것은 n개의 추적 포인트를 포함하는 fo의 더 희소한 흐름을 초래합니다. 에는 첫 번째 프레임의 추적 포인트만 포함되므로, 전체 궤적 ƒ³를 얻기 위해, 우리는 해당 광학 흐름 f에 따라 추적 포인트의 위치를 업데이트하여 궤적을 반복적으로 추적합니다. fs가 매우 희소하다는 점을 감안할 때, 모델이 이것들로부터 학습하는 데 도움이 되지 않습니다. 궤적. 따라서 우리는 fs에 가우시안 필터를 적용하여 향상된 궤적 맵 fº Є R(L-1)×CHW를 얻습니다. f³와 비교하여 f9는 견고성을 개선하고 모델이 궤적 정보를 더 잘 포착하는 데 도움이 됩니다. 3.2 비디오의 멀티스케일 퓨전(MF) 인코딩 학습하는 동안 비디오 v = RL×C×H×W를 독립된 프레임으로 취급하고 사전 학습된 이미지 자동 인코더 Rombach et al. (2022)를 사용하여 x0 = RLXcxhxw로 인코딩합니다. xo의 아래 첨자 0은 첫 번째 프레임을 나타내지 않고 확산 프로세스의 초기 단계를 나타낸다는 점에 유의하는 것이 중요합니다. 사전 정의된 확산 과정 q(xt|xt−1)N(xt; √αt xt−1, (1 — α) I)을 따르고 xo에 노이즈를 추가합니다.xt = √āt x0 + √√ (1–āt)€ € ~N(0, 1) = (2) 여기서 Є는 노이즈, xt는 확산 과정의 t번째 중간 상태, at, at는 확산 모델의 하이퍼파라미터입니다.텍스트 제어 인코딩 텍스트 프롬프트가 주어지면 CLIP Radford et al. (2021) 텍스트 인코더로 인코딩하여 프롬프트 임베딩 p = R¹×cp를 얻습니다.여기서 Ɩp는 토큰 길이이고 Ср는 프롬프트 임베딩 차원입니다.이미지 제어 인코딩 이미지 제어의 경우 비디오의 첫 번째 프레임을 조건으로 활용하여 모양, 스타일, 레이아웃과 같은 일반 정보를 제공합니다.퓨전을 위한 크기를 맞추기 위해 첫 번째 프레임을 L번 반복합니다. 그 후, 사전 학습된 이미지 자동 인코더 Rombach et al. (2022)과 일련의 합성곱 계층을 사용하여 각 프레임을 표현 s ERL×c,×h×w로 독립적으로 인코딩합니다. 3.1절에서 소개한 궤적 샘플러에 의한 궤적 제어 인코딩을 통해 오픈 도메인 비디오 v에서 직접 ƒ9 € R(L-1)xCxHxW를 얻습니다. 퓨전 크기를 맞추기 위해 f9 앞에 완전한 제로 프레임을 패딩하고 일련의 합성곱 계층을 사용하여 인코딩하여 g € RLxcgxhxw를 얻습니다. 다양한 세분성의 여러 제어를 융합하기 위해 텍스트 p, 이미지 및 궤적 g를 동시에 조건으로 받아들이고 서로 다른 해상도로 병합할 수 있는 다중 스케일 퓨전(MF)을 제안합니다. 다중 스케일 퓨전은 먼저 궤적 9와 이미지를 다양한*진행 중인 작업 스케일 g() 및 s)로 다운샘플링합니다. 여기서 상위 첨자 /는 다운샘플링 깊이를 나타냅니다. 그런 다음 궤적과 이미지는 건너뛰기 연결이 있는 다중 스케일 다운블록 및 업블록으로 구성된 UNet 아키텍처의 텍스트 p와 통합됩니다. (l) 이미지 조건 s와 궤적 조건 g의 경우 선형 투영을 통해 숨겨진 상태 h로 융합됩니다. UNet 아키텍처의 1번째 블록에서 s(2), m(1) 및 9(1)은 먼저 0으로 초기화된 합성곱 계층을 통해 스케일 w&quot;, wm, w 및 시프트 b), bm, b)로 전송됩니다. 여기서 m(1)은 프레임이 조건으로 제공되는지 여부를 나타내는 이진 마스크입니다. 그런 다음 스케일 w와 시프트 b는 간단한 선형 투영을 통해 h로 융합됩니다. h = w().h + b) + hh = w().h + b + hh = w().h+b+ h(3) (4) (5) 텍스트 조건 p의 경우 숨겨진 상태 h를 쿼리로 처리하고 텍스트 p를 키와 값으로 처리하는 Prompt Cross-Attention을 통해 숨겨진 상태 h에 주입됩니다. 다양한 조건 조합을 지원하기 위해 Multiscale Fusion에 공급하기 전에 텍스트, 이미지 및 궤적을 무작위로 생략하여 학습 프로세스에 무작위성을 도입합니다. 삭제된 텍스트의 경우 빈 문자열을 대체로 사용하는 반면 삭제된 이미지와 궤적의 경우 모든 0이 사용됩니다. 혼합된 조건을 포함하는 이 훈련 패러다임을 통해, 우리 모델은 다양한 조건 조합에 걸쳐 추론하는 동안 일관된 비디오를 생성할 수 있습니다. 3.3 적응적 훈련(AT) 시각적 일관성을 유지하면서 이미지와 희소 궤적 모두에서 비디오 생성 프로세스를 동시에 조절하는 것은 상당한 과제입니다. 이 문제를 해결하기 위해 적응적 훈련(AT) 전략을 사용하여 DragNUWA를 최적화합니다. 첫 번째 단계에서 시각적으로 동적으로 일관된 비디오를 생성하기 위해 모델에 프롬프트 p, 고밀도 광학 흐름 f, 반복되는 첫 번째 프레임 s를 조건으로 제공하고, 모델은 UNet 0(xt, p, s, f)의 출력과 추가된 노이즈 e 사이의 거리를 최소화하도록 최적화됩니다. 광학 흐름의 밀도를 고려하여 향상을 위해 가우시안 필터링을 적용하지 않습니다. Lo = ||€ – €0(xt, p, s, , ƒ) ||(6) 완전한 광학 흐름 f를 조건으로 제공하면 첫 번째 프레임을 보존하면서 동적으로 일관된 비디오를 생성하는 것이 훨씬 쉽습니다. 두 번째 단계에서는 완전한 광학 흐름에서 사용자 친화적인 궤적으로 모델을 조정하기 위해 Trajectory Sampler(TS)를 사용하여 원래 광학 흐름 ƒ에서 궤적 fg를 샘플링하여 모델 학습을 계속합니다.Lo = ||€ – €o (xt, p, 8, 9)||(7) 궤적이 광학 흐름보다 상당히 희소하더라도 모델은 이전 학습에서 학습한 안정성과 일관성을 유지하면서 궤적과 일관된 역학을 생성할 수 있습니다.3.4 추론 추론 중에 텍스트, 이미지 및 궤적이 주어지면 DragNUWA는 현실적이고 맥락적으로 일관된 비디오 v를 생성할 수 있습니다.텍스트는 CLIP Radford et al. (2021) 텍스트 인코더로 인코딩하여 텍스트 임베딩 p를 얻습니다.이미지는 L번 반복되어 s에 인코딩됩니다.입력 궤적은 먼저 가우시안 필터와 제로 프레임 패딩으로 처리한 다음 g에 인코딩됩니다. 그 후, xo는 Unet ε(xt, p, s, g)를 사용하여 순수 가우시안 노이즈 x에서 반복적으로 샘플링됩니다. 마지막으로, 샘플링된 잠재 코드 xo는 이미지 자동 인코더에 의해 비디오 픽셀 v로 디코딩됩니다.* 진행 중인 작업 데이터 증강 궤적 샘플러(TS) 버전 데이터 집합 샘플 해상도(가로 x 높이) 최대 지속 시간 프레임(L) 프레임 속도 RandomResizeCrop ColorJitter RandomStartFrame 최대 궤적(N) 가우시안 커널 앵커 간격(\) 텍스트 제어(p) DragNUWA-LD 웹 비디오 DragNUWA-HD 웹 비디오+VideoHD 10M+75K 10M 320 x2s 8f 4fps 576 x4s 16f 스케일=(0.9, 1.), 비율=(5/3, 5/3) 밝기 0.05, 대비=0.15, 채도=0.[0, video_duration-2] [0, video_duration-4]kernel_size=99, sigma=77xMultiscale Fusion(MF) 8 x 320 x 40 x이미지 제어(들) 궤적 제어(g) 8 x 320 x 20 ×8 x 640 × 10 ×8 x 1280 × 5 ×8 x 320 x 40 x8 x 320 x 20 x8 x 640 × 10 ×8 x 1280 × 5 ×16 x 320 × 72 ×16 x 320 × 36 ×16 x 640 × 18 ×16 x 1280 × 9 ×16 x 320 × 72 ×16 x 320 × 36 ×16 x 640 × 18 ×16 x 1280 × 9 ×적응형 훈련(AT) 4
--- EXPERIMENT ---
s는 Human3.6M과 같은 간단한 데이터 세트에서 수행됩니다. 이러한 제약은 모델의 개방 도메인 이미지를 처리하고 복잡한 곡선 궤적을 효과적으로 처리하는 기능을 제한합니다. 이 논문에서는 개방 도메인 확산 기반 비디오 생성 모델인 DragNUWA를 제안합니다. 기존 작업에서 제어 세분성이 부족하다는 문제를 해결하기 위해 텍스트, 이미지 및 궤적 정보를 동시에 도입하여 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠에 대한 세밀한 제어를 제공합니다. 현재 연구에서 제한된 개방 도메인 궤적 제어 문제를 해결하기 위해 세 가지 측면을 갖춘 궤적 모델링을 제안합니다. 임의의 궤적에 대한 개방 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성에서 궤적을 제어하는 다중 스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 학습(AT) 전략입니다. 실험을 통해 DragNUWA의 효과를 검증하여 비디오 생성에서 세밀한 제어에서 뛰어난 성능을 보여줍니다. 홈페이지 링크는 https://www.microsoft.com/en-us/research/project/dragnuwa/입니다.두 저자 모두 이 연구에 동등하게 기여했습니다.Shengming, Jian, Jie는 Chenfei의 멘토링 하에 인턴십을 진행했습니다.교신 저자.*진행 중인 작업 호수를 항해하는 보트 숲에서 스키를 타는 소녀 푸른 숲을 여행하는 증기 기관차 거리를 걷는 사람들 몇 명 숲을 걷는 코끼리 무리 얼룩말이 달리고 있다 길을 달리는 개 청명절 강을 따라 테이블에 앉아 토론하는 사람들 그림 2: DragNUWA에서 생성한 샘플이 표시되어 있으며, 첫 번째 열에는 텍스트, 이미지, 궤적의 세 가지 입력 컨트롤이 표시됩니다.두 번째, 세 번째, 네 번째 열에는 각각 출력 비디오의 5번째, 10번째, 15번째 프레임이 표시됩니다.총 576×320 해상도의 프레임이 16개 있습니다. DragNUWA는 카메라의 움직임, 여러 객체 및 복잡한 궤적을 동시에 제어할 수 있어 실제 장면과 예술적 그림을 모두 특징으로 하는 비디오를 생성할 수 있습니다.*진행 중인 작업소개 제어 가능한 비디오 생성은 연구에서 뜨거운 주제입니다.이러한 연구의 대부분은 제어 가능한 시각적 생성에 초점을 맞춥니다.초기 연구는 주로 이미지-비디오 생성에 중점을 두었으며, 초기 프레임 이미지를 제어로 사용하여 생성된 비디오를 공간적으로 조작했습니다.Lotter et al.(2016); Srivastava et al.(2015); Chiappa et al.(2016).그러나 제어로 이미지에만 의존하면 미래 비디오의 후속 프레임을 결정할 수 없습니다.결과적으로 비디오 생성을 의미적으로 제한하기 위해 텍스트를 사용하는 텍스트-비디오 연구에 대한 관심이 커지고 있습니다.Wu et al.(2021; 2022); Hong et al.(2022); Singer et al.(2022); Ho et al.(2022). 일부 연구에서는 비디오 생성을 보다 정밀하게 제어하기 위해 텍스트와 이미지 조건을 모두 활용합니다(Hu et al. (2022); Yin et al. (2023); Esser et al. (2023). 그럼에도 불구하고 언어와 이미지는 카메라 움직임과 복잡한 객체 궤적과 같은 비디오의 시간 정보를 표현하는 데 여전히 제한적입니다. 비디오의 시간 정보를 제어하기 위해 궤적 기반 제어는 연구에서 점점 더 주목을 받고 있는 사용자 친화적인 접근 방식으로 등장했습니다. CVG Hao et al. (2018)과 C2M Ardino et al. (2021)은 이미지와 궤적을 인코딩하여 제어 가능한 비디오 생성을 위한 중간 결과로 광학 흐름 맵과 워프 기능을 예측합니다. 그러나 워프 연산은 종종 부자연스러운 왜곡을 초래합니다. 이 문제를 해결하기 위해 II2V Blattmann et al. (2021b)과 iPOKE Blattmann et al. (2021a)은 비디오를 밀집된 잠재 공간으로 압축하고 RNN을 사용하여 이러한 잠재 변수를 조작하는 방법을 학습합니다. 마찬가지로 MCDiff Chen et al. (2023)은 자기 회귀적 방식으로 확산 잠재성을 통해 미래 프레임을 예측합니다. MCDiff는 유망한 결과를 얻었지만 HRNet Wang et al. (2021)에 의존하여 각 사람에 대해 17개의 키포인트를 추출하여 데이터를 구성하지만 인간의 동작만 제어할 수 있습니다. 또한 MCDiff와 앞서 언급한 모델은 언어 제어를 고려하지 않아 비디오를 효과적으로 제어하는 능력이 제한됩니다. 앞서 언급한 연구는 제어 가능한 비디오 생성에 대한 두 가지 비전을 제시했습니다. 1) 첫째, 기존 작업에서 텍스트, 이미지 및 궤적 기반 제어에 대한 현재 고려 사항은 충분히 포괄적이지 않습니다. 이 세 가지 유형의 제어는 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠의 조절에 기여하기 때문에 없어서는 안 될 것이라고 주장합니다. 그림 1에서 볼 수 있듯이 텍스트와 이미지의 조합만으로는 비디오에 존재하는 복잡한 동작 세부 정보를 전달하기에 충분하지 않으며, 궤적 정보를 통합하여 보완할 수 있습니다. 또한 이미지와 궤적만으로는 비디오에서 미래의 객체를 적절하게 표현할 수 없지만, 언어는 이러한 단점을 보완할 수 있습니다. 마지막으로, 궤적과 언어에만 의존하면 실제 물고기와 물고기 그림을 구별하는 것과 같은 추상적인 개념을 표현할 때 모호함이 발생할 수 있지만, 이미지는 필요한 구별을 제공할 수 있습니다. 2) 둘째, 궤적 제어에 대한 현재 연구는 아직 초기 단계이며, 대부분의 실험은 Human3.6M과 같은 간단한 데이터 세트에서 수행되고 있습니다. 이러한 제약으로 인해 모델이 오픈 도메인 이미지를 처리하고 복잡한 곡선 궤적, 여러 객체 움직임 및 카메라 모션을 동시에 효과적으로 처리할 수 있는 기능이 제한됩니다. 이러한 관찰을 바탕으로 오픈 도메인 비디오 생성 모델인 DragNUWA를 제안합니다. 기존 작업에서 제어 세분성이 부족하다는 문제를 해결하기 위해 텍스트, 이미지 및 궤적 정보를 동시에 도입하여 의미적, 공간적 및 시간적 관점에서 비디오 콘텐츠에 대한 세밀한 제어를 제공합니다. 현재 연구에서 제한된 오픈 도메인 궤적 제어 문제를 해결하기 위해 세 가지 측면으로 궤적을 모델링합니다. 임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성으로 궤적을 제어하는 멀티스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 학습(AT) 전략입니다. 우리 작업의 주요 기여는 다음과 같습니다. • 텍스트, 이미지 및 궤적의 세 가지 필수 제어를 원활하게 통합하여 강력하고 사용자 친화적인 제어 기능을 제공하는 종단 간 비디오 생성 모델인 DragNUWA를 소개합니다. • 우리는 세 가지 측면을 통해 궤적 모델링에 집중합니다.임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하는 궤적 샘플러(TS), 다양한 세분성에서 궤적을 제어하는 멀티스케일 퓨전(MF), 궤적을 따라 일관된 비디오를 생성하는 적응형 훈련(AT) 전략입니다.• 우리는 DragNUWA의 효과를 검증하기 위해 광범위한 실험을 수행하여 비디오 합성에서 세밀한 제어에서 뛰어난 성능을 보여줍니다.*진행 중인 연구 2 관련 연구 2.1 비디오 합성에서의 텍스트/이미지 제어 초기 연구는 주로 이미지-비디오 생성에 중점을 두었으며, 환경이 결정적이며 가능한 미래가 하나뿐이라는 공통된 가정이 있었습니다.Lotter et al.(2016); Srivastava et al.(2015); Chiappa et al.(2016). 그러나 이 가정은 무한한 가능성을 가진 실제 비디오의 요구 사항을 충족시킬 수 없습니다. 이 문제를 해결하기 위해 최근 몇 년 동안 텍스트-비디오 생성이 널리 연구되었습니다(GODIVA Wu et al. (2021), NUWA Wu et al. (2022), Cog Video Hong et al. (2022), Make A Video Singer et al. (2022), Imagen Video Ho et al. (2022)). 텍스트 설명을 도입하여 비디오 생성의 내용을 의미적으로 제어합니다. 그러나 텍스트만으로는 시각적 요소의 공간 정보를 정확하게 설명할 수 없습니다. 따라서 MAGE Hu et al. (2022)은 텍스트-이미지-비디오를 강조하여 텍스트의 의미 정보와 이미지의 공간 정보를 모두 사용하여 정확한 비디오 제어를 수행합니다. 마찬가지로 GEN-1 Esser et al. (2023)은 제어를 위해 교차 주의 메커니즘을 사용하여 깊이 맵을 텍스트와 통합합니다. 긴 비디오 생성 영역에서 텍스트-이미지-비디오도 널리 사용되었습니다. 예를 들어, Phenaki Villegas et al. (2022)는 이전 프레임과 텍스트를 자동 회귀적으로 도입하여 후속 프레임을 생성하여 긴 비디오 생성을 달성합니다.NUWA-XL Yin et al. (2023)은 계층적 확산 아키텍처를 사용하여 이전 프레임과 텍스트를 기반으로 중간 프레임을 지속적으로 완성합니다.텍스트와 이미지는 의미와 모양을 효과적으로 전달할 수 있지만 복잡한 동작 정보와 카메라 움직임을 적절하게 표현하는 데 어려움을 겪습니다.이러한 접근 방식과 달리 DragNUWA는 텍스트 및 이미지 제어에 궤적 제어를 추가하여 의미, 모양 및 동작 측면에서 비디오를 세밀하게 제어할 수 있습니다.2.비디오 합성의 궤적 제어 비디오의 동작을 더 잘 제어하기 위해 미래의 비디오 예측 방법은 주어진 비디오 프레임을 기반으로 후속 프레임 생성을 제어합니다 Wu et al. (2020); Wichers et al. (2018); Walker et al. (2017); Liang et al. (2017). 반면, 비디오-비디오 생성은 완전한 비디오 또는 비디오 스케치의 스타일을 새로운 도메인으로 전송하여 풍부한 제어 정보를 제공합니다(Chan et al. (2019); Wang et al. (2019). 그러나 이를 위해서는 사용자가 비디오 입력을 제공해야 하며 스타일 전송이 원본 비디오의 골격을 기반으로 하기 때문에 세부적인 제어가 제한됩니다. 결과적으로 이미지-궤적-비디오 방법이 등장하여 이미지에 제공된 궤적을 통해 비디오 개발을 제어합니다. CVG Hao et al. (2018) 및 C2M Ardino et al. (2021)은 이미지와 궤적을 인코딩하여 제어 가능한 비디오 생성을 위한 중간 결과로 광학 흐름 맵과 워프 기능을 예측합니다. 그러나 워프 작업은 종종 부자연스러운 왜곡을 초래합니다. II2V Blattmann et al. (2021b) 및 iPOKE Blattmann et al. (2021a)은 비디오를 밀집된 잠재 공간으로 압축하고 순환 신경망을 사용하여 이러한 잠재 변수를 조작하는 방법을 학습합니다. 그러나 궤적 제어는 픽셀 수준에서 작동하기 때문에 희소하고 모호하기 쉽습니다. 이 문제를 해결하기 위해 희소한 스트로크를 먼저 밀집된 흐름으로 변환한 다음 자기 회귀를 사용하여 밀집된 흐름을 기반으로 미래 프레임을 예측합니다. 그럼에도 불구하고 MCDiff Chen 등(2023)은 HRNet Wang 등(2021)에 의존하여 각 사람에 대해 17개의 키포인트를 추출하여 데이터를 구성하기 때문에 사람의 동작만 제어할 수 있습니다. 오픈 도메인 객체의 제어를 달성하기 위해 Video Composer Wang 등(2023)은 아주 최근에 MPEG-4를 사용하여 비디오에서 동작 벡터 정보를 추출하여 훈련 조건으로 사용했지만 동작 벡터에 고수준 의미 정보가 부족하여 간단한 객체 움직임만 제어할 수 있었습니다. 이전 연구가 인간의 동작이나 기본적인 객체 움직임에만 초점을 맞춘 것과 달리, DragNUWA는 이미지에서 모든 객체를 드래그하여 여러 객체를 제어하고 복잡한 궤적과 카메라 움직임을 수용할 수 있도록 함으로써 세분화된 오픈 도메인 비디오 생성을 달성하는 선구적인 접근 방식으로 돋보입니다.3 방법 텍스트 기반 Wu et al.(2021), 이미지 기반 Zhang et al.(2020) 또는 궤적 기반 제어 Hu et al.(2022)만 지원하는 이전 연구와 달리 DragNUWA는 세 가지 유형의 제어를 모두 통합하는 동시에 세 가지 측면에서 궤적 모델링을 강조하도록 설계되었습니다.€ EN (0, 1) xt Χρ *진행 중인 작업 v Unimatch Trajectory Sampler(TS) fg 반복된 첫 번째 프레임 이미지 Enc Adaptive Training(AT) Trajectory Enc Multiscale Fusion(MF) 프롬프트: 타호 호수 근처의 구불구불한 길을 빠르게 운전합니다. Text Enc Down Down Down Down Mid dn Up Up hout hin hs € (x+, S, g,p) MSE Loss 그림 3: DragNUWA의 학습 프로세스 개요.DragNUWA는 텍스트 p, 이미지, 궤적 g의 세 가지 선택적 입력을 지원하고 세 가지 측면에서 궤적을 설계하는 데 중점을 둡니다.첫째, 궤적 샘플러(TS)는 오픈 도메인 비디오 흐름에서 궤적을 동적으로 샘플링합니다.둘째, 멀티스케일 퓨전(MF)은 UNet 아키텍처의 각 블록 내에서 궤적을 텍스트와 이미지와 긴밀하게 통합합니다.마지막으로, 적응형 학습(AT)은 모델을 광학 흐름 조건에서 사용자 친화적인 궤적으로 조정합니다.궁극적으로 DragNUWA는 여러 객체와 복잡한 궤적이 있는 오픈 도메인 비디오를 처리할 수 있습니다. • • • 1) 임의의 궤적에 대한 오픈 도메인 제어를 가능하게 하기 위해, 3.1절에 소개된 궤적 샘플러(TS)를 사용하여 훈련 중에 오픈 도메인 비디오 흐름에서 궤적을 직접 샘플링합니다. 이는 MCDiff Chen et al.(2023)에서 사용된 인간 포즈 궤적과 같은 특정 도메인과는 대조적입니다. 2) 다양한 궤적 세분성에 대한 제어를 달성하기 위해, 3.2절에 소개된 멀티스케일 퓨전(MF)을 사용하여 궤적을 다양한 스케일로 다운샘플링하고 UNet 아키텍처의 각 블록 내에서 텍스트와 이미지와 심층적으로 통합합니다. Chen et al.(2023); Wang et al.(2023)에서처럼 확산 노이즈로 제어를 직접 연결하는 대신입니다. 3) 안정적이고 일관된 비디오를 생성하기 위해 적응형 훈련(AT)(3.3절에서 소개) 방식을 채택하여 처음에는 밀도 흐름에 조건을 두어 비디오 생성을 안정화한 다음 희소 궤적에 대해 훈련하여 모델을 적응시킵니다.다음 섹션에서는 3.1절에서 3.3절까지의 훈련 프로세스를 소개하는 데 중점을 두고, 특히 모델이 &lt;v, p&gt;로 표시된 입력 비디오와 텍스트 쌍을 사용하여 손실을 계산하는 방법을 설명합니다.3.4절에서는 모델이 입력 텍스트 p, 이미지 s, 궤적 g를 처리하여 생성된 비디오 v를 출력하는 방법을 보여주는 추론 프로세스를 소개합니다.3.1 궤적 샘플러(TS) 훈련 데이터에는 비디오와 텍스트 쌍 &lt;v,p&gt;만 포함되어 있으므로 비디오에서 궤적을 추출하는 것이 필수적입니다.이전 연구에서는 주로 주요 지점 추적 모델을 사용하여 훈련을 위해 비디오 궤적을 미리 추출했습니다.그러나 이 방식에는 두 가지 주요 단점이 있습니다. 첫째, 이러한 모델은 인간 포즈와 같은 특정 도메인에서 학습되므로 오픈 도메인 비디오를 처리하는 능력이 제한적입니다. 둘째, 실제 응용 프로그램에서는 사용자가 주요 지점에서 정확하게 궤적을 입력하도록 보장하기 어려워서 학습과 추론 사이에 차이가 발생합니다. 오픈 도메인 비디오 궤적을 용이하게 하고 사용자가 임의의 궤적을 입력할 수 있도록 하기 위해 비디오 광학 흐름에서 궤적을 직접 샘플링하는 궤적 샘플러(TS)를 설계했습니다. 이를 통해 모델은 오픈 도메인 설정에서 다양한 가능한 궤적을 학습할 수 있습니다. 프레임 L개, 채널 C개, 높이 H개, 너비 W개가 있는 비디오 v ER¹×C×H×W가 주어지면 먼저 광학 흐름 추정기인 Unimatch Xu et al. (2023)을 사용하여 밀도가 높은 광학 흐름 fЄ R(L-1)×C×H×W를 추출합니다. 명확성을 위해 첫 번째와 두 번째 프레임의 광학 흐름을 fo ERC×H×W로 나타냅니다. 간단한 방법은 광학 흐름의 강도에 따라 fo에서 궤적을 직접 샘플링하는 것입니다. 그러나 이렇게 하면 움직임이 큰 객체에 과도한 샘플링이 발생하는 반면 움직임이 작은 객체는 학습 기회가 제한됩니다. 이 문제를 해결하기 위해 X 간격으로 앵커 포인트를 균일하게 분산합니다. 또한 전체 이미지 영역을 가능한 한 많이 커버하기 위해 앵커 포인트에 -1/2에서 1/2까지의 무작위 섭동을 추가합니다. 마지막으로, 우리는 다음의 약간 더 희소한 고정된 광학 흐름을 얻습니다: (0, else fo,i,j = \fo,i,j, (i+8)%λ = 0 &amp; (j + 8)%λ =(1) 여러 궤적에 대한 제어를 지원하기 위해, 우리는 최대 궤적 수 N을 정의하고 궤적 수 n ~ U[1, N]을 무작위로 샘플링합니다. 흐름 강도에 따라 궤적을 선택하는 동안 크고 작은 모션 객체를 모두 수용하기 위해, 우리는 다항 분포 M(n, ||fo,i,j||2)에 따라 fő,¿¿에서 n개의 앵커 추적 포인트를 샘플링합니다. 이것은 n개의 추적 포인트를 포함하는 fo의 더 희소한 흐름을 초래합니다. 에는 첫 번째 프레임의 추적 포인트만 포함되므로, 전체 궤적 ƒ³를 얻기 위해, 우리는 해당 광학 흐름 f에 따라 추적 포인트의 위치를 업데이트하여 궤적을 반복적으로 추적합니다. fs가 매우 희소하다는 점을 감안할 때, 모델이 이것들로부터 학습하는 데 도움이 되지 않습니다. 궤적. 따라서 우리는 fs에 가우시안 필터를 적용하여 향상된 궤적 맵 fº Є R(L-1)×CHW를 얻습니다. f³와 비교하여 f9는 견고성을 개선하고 모델이 궤적 정보를 더 잘 포착하는 데 도움이 됩니다. 3.2 비디오의 멀티스케일 퓨전(MF) 인코딩 학습하는 동안 비디오 v = RL×C×H×W를 독립된 프레임으로 취급하고 사전 학습된 이미지 자동 인코더 Rombach et al. (2022)를 사용하여 x0 = RLXcxhxw로 인코딩합니다. xo의 아래 첨자 0은 첫 번째 프레임을 나타내지 않고 확산 프로세스의 초기 단계를 나타낸다는 점에 유의하는 것이 중요합니다. 사전 정의된 확산 과정 q(xt|xt−1)N(xt; √αt xt−1, (1 — α) I)을 따르고 xo에 노이즈를 추가합니다.xt = √āt x0 + √√ (1–āt)€ € ~N(0, 1) = (2) 여기서 Є는 노이즈, xt는 확산 과정의 t번째 중간 상태, at, at는 확산 모델의 하이퍼파라미터입니다.텍스트 제어 인코딩 텍스트 프롬프트가 주어지면 CLIP Radford et al. (2021) 텍스트 인코더로 인코딩하여 프롬프트 임베딩 p = R¹×cp를 얻습니다.여기서 Ɩp는 토큰 길이이고 Ср는 프롬프트 임베딩 차원입니다.이미지 제어 인코딩 이미지 제어의 경우 비디오의 첫 번째 프레임을 조건으로 활용하여 모양, 스타일, 레이아웃과 같은 일반 정보를 제공합니다.퓨전을 위한 크기를 맞추기 위해 첫 번째 프레임을 L번 반복합니다. 그 후, 사전 학습된 이미지 자동 인코더 Rombach et al. (2022)과 일련의 합성곱 계층을 사용하여 각 프레임을 표현 s ERL×c,×h×w로 독립적으로 인코딩합니다. 3.1절에서 소개한 궤적 샘플러에 의한 궤적 제어 인코딩을 통해 오픈 도메인 비디오 v에서 직접 ƒ9 € R(L-1)xCxHxW를 얻습니다. 퓨전 크기를 맞추기 위해 f9 앞에 완전한 제로 프레임을 패딩하고 일련의 합성곱 계층을 사용하여 인코딩하여 g € RLxcgxhxw를 얻습니다. 다양한 세분성의 여러 제어를 융합하기 위해 텍스트 p, 이미지 및 궤적 g를 동시에 조건으로 받아들이고 서로 다른 해상도로 병합할 수 있는 다중 스케일 퓨전(MF)을 제안합니다. 다중 스케일 퓨전은 먼저 궤적 9와 이미지를 다양한*진행 중인 작업 스케일 g() 및 s)로 다운샘플링합니다. 여기서 상위 첨자 /는 다운샘플링 깊이를 나타냅니다. 그런 다음 궤적과 이미지는 건너뛰기 연결이 있는 다중 스케일 다운블록 및 업블록으로 구성된 UNet 아키텍처의 텍스트 p와 통합됩니다. (l) 이미지 조건 s와 궤적 조건 g의 경우 선형 투영을 통해 숨겨진 상태 h로 융합됩니다. UNet 아키텍처의 1번째 블록에서 s(2), m(1) 및 9(1)은 먼저 0으로 초기화된 합성곱 계층을 통해 스케일 w&quot;, wm, w 및 시프트 b), bm, b)로 전송됩니다. 여기서 m(1)은 프레임이 조건으로 제공되는지 여부를 나타내는 이진 마스크입니다. 그런 다음 스케일 w와 시프트 b는 간단한 선형 투영을 통해 h로 융합됩니다. h = w().h + b) + hh = w().h + b + hh = w().h+b+ h(3) (4) (5) 텍스트 조건 p의 경우 숨겨진 상태 h를 쿼리로 처리하고 텍스트 p를 키와 값으로 처리하는 Prompt Cross-Attention을 통해 숨겨진 상태 h에 주입됩니다. 다양한 조건 조합을 지원하기 위해 Multiscale Fusion에 공급하기 전에 텍스트, 이미지 및 궤적을 무작위로 생략하여 학습 프로세스에 무작위성을 도입합니다. 삭제된 텍스트의 경우 빈 문자열을 대체로 사용하는 반면 삭제된 이미지와 궤적의 경우 모든 0이 사용됩니다. 혼합된 조건을 포함하는 이 훈련 패러다임을 통해, 우리 모델은 다양한 조건 조합에 걸쳐 추론하는 동안 일관된 비디오를 생성할 수 있습니다. 3.3 적응적 훈련(AT) 시각적 일관성을 유지하면서 이미지와 희소 궤적 모두에서 비디오 생성 프로세스를 동시에 조절하는 것은 상당한 과제입니다. 이 문제를 해결하기 위해 적응적 훈련(AT) 전략을 사용하여 DragNUWA를 최적화합니다. 첫 번째 단계에서 시각적으로 동적으로 일관된 비디오를 생성하기 위해 모델에 프롬프트 p, 고밀도 광학 흐름 f, 반복되는 첫 번째 프레임 s를 조건으로 제공하고, 모델은 UNet 0(xt, p, s, f)의 출력과 추가된 노이즈 e 사이의 거리를 최소화하도록 최적화됩니다. 광학 흐름의 밀도를 고려하여 향상을 위해 가우시안 필터링을 적용하지 않습니다. Lo = ||€ – €0(xt, p, s, , ƒ) ||(6) 완전한 광학 흐름 f를 조건으로 제공하면 첫 번째 프레임을 보존하면서 동적으로 일관된 비디오를 생성하는 것이 훨씬 쉽습니다. 두 번째 단계에서는 완전한 광학 흐름에서 사용자 친화적인 궤적으로 모델을 조정하기 위해 Trajectory Sampler(TS)를 사용하여 원래 광학 흐름 ƒ에서 궤적 fg를 샘플링하여 모델 학습을 계속합니다.Lo = ||€ – €o (xt, p, 8, 9)||(7) 궤적이 광학 흐름보다 상당히 희소하더라도 모델은 이전 학습에서 학습한 안정성과 일관성을 유지하면서 궤적과 일관된 역학을 생성할 수 있습니다.3.4 추론 추론 중에 텍스트, 이미지 및 궤적이 주어지면 DragNUWA는 현실적이고 맥락적으로 일관된 비디오 v를 생성할 수 있습니다.텍스트는 CLIP Radford et al. (2021) 텍스트 인코더로 인코딩하여 텍스트 임베딩 p를 얻습니다.이미지는 L번 반복되어 s에 인코딩됩니다.입력 궤적은 먼저 가우시안 필터와 제로 프레임 패딩으로 처리한 다음 g에 인코딩됩니다. 그 후, xo는 Unet ε(xt, p, s, g)를 사용하여 순수 가우시안 노이즈 x에서 반복적으로 샘플링됩니다. 마지막으로, 샘플링된 잠재 코드 xo는 이미지 자동 인코더에 의해 비디오 픽셀 v로 디코딩됩니다.* 진행 중인 작업 데이터 증강 궤적 샘플러(TS) 버전 데이터 집합 샘플 해상도(가로 x 높이) 최대 지속 시간 프레임(L) 프레임 속도 RandomResizeCrop ColorJitter RandomStartFrame 최대 궤적(N) 가우시안 커널 앵커 간격(\) 텍스트 제어(p) DragNUWA-LD 웹 비디오 DragNUWA-HD 웹 비디오+VideoHD 10M+75K 10M 320 x2s 8f 4fps 576 x4s 16f 스케일=(0.9, 1.), 비율=(5/3, 5/3) 밝기 0.05, 대비=0.15, 채도=0.[0, video_duration-2] [0, video_duration-4]kernel_size=99, sigma=77xMultiscale Fusion(MF) 8 x 320 x 40 x이미지 제어(들) 궤적 제어(g) 8 x 320 x 20 ×8 x 640 × 10 ×8 x 1280 × 5 ×8 x 320 x 40 x8 x 320 x 20 x8 x 640 × 10 ×8 x 1280 × 5 ×16 x 320 × 72 ×16 x 320 × 36 ×16 x 640 × 18 ×16 x 1280 × 9 ×16 x 320 × 72 ×16 x 320 × 36 ×16 x 640 × 18 ×16 x 1280 × 9 ×적응형 학습(AT) 4가지 실험 제어 드롭 비율 배치 크기 학습률 스케줄러 최적화 매개변수 텍스트: 0.1, 이미지: 0.1, 궤적: 0.5 ×-WarmupLinear, warmup_ratio=0.Adam 1.60B 표 1: DragNUWA의 구현 세부 정보. 4.1 데이터 세트 훈련 과정에서 WebVid와 VideoHD를 활용하여 DragNUWA를 최적화합니다. 4.of • WebVid는 Bain et al. (2021)이 다양한 실제 시나리오와 해당 캡션을 포함하는 1,000만 개의 웹 비디오로 구성된 방대한 데이터 세트입니다. 광범위한 동작 패턴을 포괄하므로 오픈 도메인 궤적 기반 비디오 생성에 적합합니다. VideoHD 웹 크롤링 비디오를 기반으로 VideoHD 데이터 세트를 구축합니다. 먼저 인터넷에서 75,000개의 고해상도, 최고 품질의 비디오 클립을 수집했습니다. 그런 다음 BLIP2 Li et al. (2023)을 사용하여 이러한 클립에 주석을 달았습니다. 마지막으로 생성된 결과에서 일부 오류를 수동으로 필터링합니다. . 구현 세부 정보 DragNUWA-LD와 DragNUWA-HD라는 두 가지 버전의 DragNUWA를 구현합니다. DragNUWA-LD는 320×192 해상도의 8개 프레임 비디오에서 학습한 반면, DragNUWAHD는 576×320 해상도의 16개 프레임에서 학습했습니다. TS(Trajectory Sampler)의 경우, 최대 궤적 수 N은 8이고 앵커 간격 \은 16입니다. 가우시안 커널 크기는 99이고 시그마 값은 10으로 설정되었습니다. 다양한 조건 조합을 지원하기 위해 0.1의 확률로 텍스트, 이미지 및 궤적을 무작위로 생략합니다. 배치 크기가 128이고 학습 속도가 5×10−6인 Adam 최적화 도구 Kingma &amp; Ba(2014)를 사용하여 모델을 학습합니다. 자세한 구현 내용은 표 1에서 확인할 수 있습니다.* 진행 중인 작업 4.3 궤적 제어성 텍스트나 이미지 제어에 초점을 맞춘 기존 연구와 달리 DragNUWA는 주로 궤적 제어 모델링을 강조합니다. 궤적 제어의 효과를 검증하기 위해 DragNUWA를 카메라 움직임과 복잡한 궤적의 두 가지 측면에서 테스트합니다. 카메라 움직임. 비디오 제작에서 카메라 움직임은 관객을 위한 역동적이고 매력적인 비주얼을 만드는 데 중요한 역할을 합니다. 다양한 유형의 카메라 움직임은 내러티브 스토리텔링 또는 장면 내의 요소를 강조하는 데 도움이 될 수 있습니다. 일반적인 카메라 움직임에는 수평 및 수직 움직임뿐만 아니라 확대 및 축소도 포함됩니다. 그림 4에서 볼 수 있듯이 DragNUWA는 카메라 움직임을 명시적으로 모델링하지 않지만 오픈 도메인 궤적 모델링에서 다양한 카메라 움직임을 학습한다는 것을 발견했습니다. 입력 텍스트 p: 만리장성 입력 이미지: 입력 궤적 g: 출력 비디오 v: *** &quot;&quot;&quot; 그림 4: 드래그 궤적을 변경하면서 동일한 텍스트와 이미지를 활용하여 다양한 카메라 이동 효과를 얻을 수 있습니다. 예를 들어, 원하는 줌 위치에 방향 궤적을 그려 줌인 및 줌아웃 효과를 표현할 수 있습니다.*진행 중인 작업 복잡한 궤적. 비디오 생성에서의 동작 모델링은 여러 개의 움직이는 객체, 복잡한 동작 궤적, 서로 다른 객체 간의 다양한 동작 진폭으로 인해 어려움이 있습니다. 복잡한 동작을 정확하게 모델링하는 DragNUWA의 역량을 평가하기 위해 그림 5에 나와 있는 것처럼 동일한 이미지와 텍스트를 사용하여 다양한 복잡한 드래그 궤적에 대한 테스트를 수행했습니다. 연구 결과에 따르면 DragNUWA는 복잡한 동작을 안정적으로 제어할 수 있습니다. 여기에는 여러 측면이 포함됩니다. 첫째, DragNUWA는 복잡한 곡선 궤적을 지원하여 특정 복잡한 궤적을 따라 움직이는 객체를 생성할 수 있습니다(6행 참조). 둘째, DragNUWA는 가변적인 궤적 길이를 허용하며, 더 긴 궤적은 더 큰 동작 진폭을 초래합니다(행 7-8 참조). 마지막으로, DragNUWA는 여러 객체의 궤적을 동시에 제어할 수 있는 기능을 가지고 있습니다. 저희가 아는 한, 기존의 비디오 생성 모델은 그러한 궤적 제어 가능성을 효과적으로 달성하지 못했으며, 이는 DragNUWA가 향후 애플리케이션에서 제어 가능한 비디오 생성을 발전시킬 수 있는 상당한 잠재력을 강조합니다. 스케이트보드를 타는 두 소년 입력 텍스트 p: 입력 이미지: 경사로에서 입력 궤적 g: 출력 비디오 v: 셔터 urstsek Shotterstock sbutterst #lensteek estarle shutter sck 그림 5: 드래그 궤적을 변경하면서 동일한 텍스트와 이미지를 사용하여 다양한 복잡한 궤적 효과를 얻을 수 있습니다. DragNUWA는 복잡한 곡선 궤적을 지원하고, 다양한 궤적 길이를 허용하며, 여러 객체에 대한 궤적의 동시 제어를 지원합니다.*진행 중인 작업 4.4 세 가지 제어의 필수 요소 s2v 입력 텍스트 p: 입력 궤적 g: 입력 이미지: 출력 비디오 v: 눈 위에서 서핑하는 남자 p2v gs2v 눈 위에서 서핑하는 남자 ps2v × × 눈 위에서 서핑하는 남자 pgs2v 그림 6: DragNUWA는 의미적, 공간적, 시간적 측면에 각각 해당하는 세 가지 필수 제어인 텍스트, 이미지, 궤적을 통합하여 세분화된 비디오 생성을 달성합니다.DragNUWA는 주로 궤적 제어 모델링을 강조하지만, 텍스트와 이미지의 제어도 통합합니다.우리는 텍스트, 이미지, 궤적이 각각 비디오의 세 가지 기본 제어 측면인 의미적, 공간적, 시간적 관점 중 하나를 나타낸다고 믿습니다. 그림 6은 s2v, p2v, gs2v, ps2v, pgs2v를 포함한 텍스트(p), 궤적(g), 이미지(s)의 다양한 조합을 보여줌으로써 이러한 조건의 필요성을 보여줍니다. 이미지가 없는 궤적은 의미가 없다고 믿기 때문에 g2v와 pg2v를 모델링하지 않았다는 점에 유의하는 것이 중요합니다. s2v와 p2v는 개별 조건으로 활용될 때 이미지와 텍스트 제어의 제약을 보여줍니다. s2v에서 보듯이 이미지만으로도 잠재적인 의미 및 운동 정보를 제공하지만 배경과 캐릭터의 움직임을 정확하게 제어할 수는 없습니다. p2v에서 보듯이 텍스트만 제공되면 모델은 텍스트와 관련된 비디오를 성공적으로 생성하지만 모양과 역학은 전혀 제어할 수 없습니다. gs2v와 ps2v는 텍스트(p)와 궤적(g)의 중요성을 강조합니다. 텍스트가 없으면 모호한 이미지(들)가 바다에서 서핑을 나타내는지 눈 위에서 서핑을 나타내는지 판단할 수 없습니다. 궤적이 없으면 모델은 자동으로 캐릭터가 왼쪽으로 이동한다고 가정합니다. pgs2v는 세 가지 필수 조건의 결합된 영향을 보여주며, 눈 위에서 서핑을 제어하고 오른쪽으로 이동하는 것을 가능하게 합니다. 일부 연구에서 비디오를 조건으로 통합한다는 점은 언급할 가치가 있는데, 이는 이 연구의 범위를 벗어납니다. 우리는 기본 조건에 초점을 맞추는 반면, 비디오 조건은 과도한 정보를 제공하여 비디오 생성을 상당히 제한하고 주로 스타일 전송 목적으로 사용됩니다. 게다가 비디오 조건은 사용자가 특정 비디오 자료를 제공해야 하므로 실제 적용에 상당한 어려움이 있습니다. 5
--- CONCLUSION ---
DragNUWA는 텍스트, 이미지, 궤적 입력을 완벽하게 통합하여 의미적, 공간적, 시간적 관점에서 세분화되고 사용자 친화적인 제어를 가능하게 하는 엔드투엔드 비디오 생성 모델입니다. 또한 궤적 샘플러(TS), 멀티스케일 퓨전(MF), 적응형 훈련(AT)으로 구성된 궤적 모델링 프레임워크는 오픈 도메인 궤적 제어의 과제를 해결하여 복잡한 궤적에 따라 일관된 비디오를 생성할 수 있습니다. 실험을 통해 DragNUWA가 기존 접근 방식보다 우수하다는 것이 검증되었으며, 세분화된 비디오를 효과적으로 생성할 수 있는 능력이 입증되었습니다.*진행 중인 작업 참고문헌 Pierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa Ricci, Stéphane Lathuilière. 클릭하여 이동: 희소 모션으로 비디오 생성 제어. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 14749–14758, 2021. Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoding for end-to-end retrieval. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 1728-1738, 2021. Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Björn Ommer. Ipoke: 제어된 확률적 비디오 합성을 위한 정지 이미지 찌르기. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, pp. 14707-14717, 2021a. Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. 대화형 이미지-비디오 합성을 위한 객체 역학 이해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중, 5171-5181쪽, 2021b. Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros. Everybody Dance Now. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스의 진행 중, 5933-5942쪽, 2019. Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, Ming-Hsuan Yang. 제어 가능한 비디오 합성을 위한 모션 조건 확산 모델. arXiv 사전 인쇄본 arXiv:2304.14404, 2023. Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, Shakir Mohamed. 반복적 환경 시뮬레이터. 국제 학습 표현 컨퍼런스, 2016년 11월. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis. 확산 모델을 사용한 구조 및 콘텐츠 가이드 비디오 합성. arXiv 사전 인쇄본 arXiv:2302.03011, 2023. Zekun Hao, Xun Huang, Serge Belongie. 희소 궤적을 사용한 제어 가능한 비디오 생성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 7854-7863쪽, 2018. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet. 이미지 비디오: 확산 모델을 사용한 고 ~비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang. CogVideo: Transformers를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868, 2022. Yaosi Hu, Chong Luo, Zhenzhong Chen. Make It Move: 텍스트 설명을 사용한 제어 가능한 이미지-비디오 생성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 18219–18228, 2022. Diederik P. Kingma와 Jimmy Ba. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980, 2014. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2: Frozen Image Encoders와 Large Language Models를 사용한 LanguageImage 사전 학습 부트스트래핑. arXiv 사전 인쇄본 arXiv:2301.12597, 2023. Xiaodan Liang, Lisa Lee, Wei Dai, Eric P. Xing. Future-Flow Embedded Video Prediction을 위한 Dual Motion GAN. IEEE International Conference on Computer Vision의 회의록, pp. 1744-1752, 2017. William Lotter, Gabriel Kreiman, David Cox. 비디오 예측 및 비지도 학습을 위한 딥 예측 코딩 네트워크. International Conference on Learning Representations, 2016년 11월.*진행 중인 작업 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, and Jack Clark. 자연어 감독에서 학습 가능한 시각적 모델. International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, pp. 10684-10695, 2022. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman. Make-A-Video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성, 2022년 9월. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov. LSTM을 사용한 비디오 표현의 비지도 학습. 제32회 국제 머신 러닝 컨퍼런스 회의록, pp. 843-852. PMLR, 2015년 6월. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze 및 Dumitru Erhan. Phenaki: 오픈 도메인 텍스트 설명에서 가변 길이 비디오 생성. ICLR, 2022년 9월. Jacob Walker, Kenneth Marino, Abhinav Gupta 및 Martial Hebert. 포즈는 알고 있다: 포즈 미래를 생성하여 비디오 예측. 컴퓨터 비전에 관한 IEEE 국제 컨퍼런스 진행, pp. 3332-3341, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu 및 Bin Xiao. 시각적 인식을 위한 심층 고해상도 표현 학습. IEEE 패턴 분석 및 머신 인텔리전스 저널, 43(10):3349–3364, 2021년 10월. ISSN 1939-3539. doi: 10.1109/TPAMI. 2020.2983686. Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Fewshot 비디오-비디오 합성. 33rd International Conference on Neural Information Processing Systems의 회의록, 5013-5024쪽, 미국 뉴욕주 레드훅, 2019년 12월. Curran Associates Inc. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Motion Controllability를 갖춘 구성적 비디오 합성, 2023년 6월. Nevan Wichers, Ruben Villegas, Dumitru Erhan, Honglak Lee. 감독 없는 계층적 장기 비디오 예측. 제35회 국제 머신 러닝 컨퍼런스 논문집, 6038-6046쪽. PMLR, 2018년 7월. Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan. GODIVA: 자연어 설명에서 오픈 도메인 비디오 생성. arXiv:2104.[cs], 2021년 4월. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan. N\&quot;UWA: 신경 시각 세계 창조를 위한 시각 합성 사전 훈련. 유럽 컴퓨터 비전 회의(ECCV) 회의록, 2022. 웨 우, 롱롱 가오, 재식 박, 치펭 첸. 객체 모션 예측을 통한 미래 비디오 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 회의록, 5539-5548쪽, 2020. 하오페이 쉬, 징 장, 지안페이 카이, 하미드 레자토피기, 피셔 유, 다청 타오, 안드레아스 가이거. 흐름, 스테레오 및 깊이 추정 통합. IEEE 패턴 분석 및 머신 인텔리전스 저널, 2023. 쉔밍 잉, 첸페이 우, 후안 양, 지안펭 왕, 샤오동 왕, 민헝 니, 정위안 양, 린지에 리, 슈광 류, 및 Fan Yang. NUWA-XL: eXtremely Long Video Generation을 위한 Diffusion over Diffusion. arXiv 사전 인쇄본 arXiv:2303.12346, 2023.* 진행 중인 작업 Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, Yunliang Jiang. DTVNet: 단일 정지 이미지를 통한 동적 타임랩스 비디오 생성. Andrea Vedaldi, Horst Bischof, Thomas Brox, Jan-Michael Frahm(편), Computer Vision - ECCV 2020, Lecture Notes in Computer Science, pp. 300–315, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58558-7. doi: 10.1007/978-3-030-58558-7_18.
