--- ABSTRACT ---
고도로 맞춤화된 텍스트 설명과 포즈 안내를 통해 표현력 있고 다양하며 고품질의 3D 아바타를 만드는 것은 세부 사항과 다양한 스타일(사실적, 허구적 등)을 보장하는 3D 모델링과 텍스처링의 복잡성으로 인해 어려운 작업입니다. 텍스트 설명과 포즈 안내만으로 표현력 있고 고품질의 3D 아바타를 생성하기 위한 안정적인 파이프라인인 AvatarVerse를 소개합니다. 구체적으로, DensePose 신호에 따라 조건화된 2D 확산 모델을 소개하여 2D 이미지를 통해 아바타의 3D 포즈 제어를 확립하고, 부분적으로 관찰된 시나리오에서 뷰 일관성을 향상시킵니다. 이는 악명 높은 Janus 문제를 해결하고 생성 프로세스를 상당히 안정화합니다. 또한, 생성된 3D 아바타의 품질을 상당히 개선하는 점진적인 고해상도 3D 합성 전략을 제안합니다. 이를 위해 제안된 AvatarVerse 파이프라인은 이전 작업보다 표현력이 풍부할 뿐만 아니라 품질과 충실도가 더 높은 3D 아바타의 제로샷 3D 모델링을 달성합니다. 엄격한 정성적 평가와 사용자 연구는 Avatar Verse가 고충실도 3D 아바타를 합성하는 데 있어 우수함을 보여주며, 고품질의 안정적인 3D 아바타 제작의 새로운 표준으로 이어집니다. 프로젝트 페이지는 https://avatarverse3d.github.io/입니다. 1.
--- INTRODUCTION ---
고품질 3D 아바타의 제작은 게임 제작, 소셜 미디어 및 커뮤니케이션, 증강 및 가상 현실(AR/VR), 인간-컴퓨터 상호 작용과 같은 분야에서 널리 사용되기 때문에 상당한 관심을 얻었습니다. 이러한 복잡한 3D 모델의 전통적인 수동 구성은 노동 집약적이고 시간이 많이 걸리는 프로세스로, 광범위한 미적 및 3D 모델링 전문 지식을 보유한 숙련된 아티스트가 수천 시간을 필요로 합니다. 결과적으로 자연어 설명만을 사용하여 고품질 3D 아바타를 생성하는 것을 자동화하는 것은 리소스를 절약할 수 있는 잠재력을 가진 훌륭한 연구 전망을 제공하며, 이는 또한 우리 작업의 목표입니다. 최근 몇 년 동안 다중 시점 비디오(Isik 등 2023; Jiang 등 2022; Li 등 2023b; Wang 등 2023a; Zheng 등 2023) 또는 참조 이미지(Wang 등 2021; Xiu 등 2022)에서 충실도 높은 3D 아바타를 재구성하는 데 상당한 노력이 이루어졌습니다. 이러한 방법은 주로 비디오나 참조 이미지에서 얻은 제한된 시각적 사전 지식에 의존하여 복잡한 텍스트 프롬프트가 있는 창의적인 아바타를 생성하는 능력이 제한되었습니다. 2D 이미지 생성에서 확산 모델(Rombach 등 2021; Saharia 등 2022; Zhang 및 Agrawala 2023)은 주로 대규모 텍스트-이미지 쌍의 가용성으로 인해 상당한 창의성을 보여줍니다. 그럼에도 불구하고, 3D 모델의 희소성과 제한된 다양성은 3D 확산 모델을 효과적으로 훈련하는 데 과제를 제시합니다. 최근 연구(Cao et al. 2023; Huang et al. 2023; Kolotouros et al. 2023; Poole et al. 2022)에서는 사전 훈련된 텍스트-이미지 생성 모델을 사용하여 Neural Radiance Fields(NeRF)(Mildenhall et al. 2020)를 최적화하여 충실도 높은 3D 모델을 생성하는 방법을 조사했습니다. 그러나 다양한 포즈, 모습, 모양을 보이는 고품질 3D 아바타를 안정적으로 만드는 것은 여전히 어려운 작업입니다. 예를 들어, 추가 제어 없이 NeRF 최적화를 안내하기 위해 공통 점수 증류 샘플링(SDS)(Poole et al. 2022)을 사용하면 Janus(다중 얼굴) 문제가 발생하는 경향이 있습니다. 또한, 현재 접근 방식으로 생성된 아바타는 눈에 띄는 흐릿함과 거칠기를 보이는 경향이 있어 고해상도 로컬 텍스처 세부 정보, 액세서리 및 기타 관련 기능이 없습니다. 이러한 약점을 해결하기 위해 텍스트 설명과 포즈 안내에서 고품질의 안정적인 3D 아바타를 생성하도록 설계된 새로운 프레임워크인 Avatar Verse를 제안합니다. 먼저 800K 이미지에 걸쳐 인간의 DensePose 조건(Güler, Neverova 및 Kokkinos 2018)으로 새로운 ControlNet을 학습합니다. 그런 다음 2D DensePose 신호에 대한 SDS 손실 조건이 ControlNet 위에 구현됩니다. 이런 방식으로 다양한 2D 뷰 간 및 모든 2D 뷰와 3D 공간 간의 정확한 뷰 대응 관계를 얻습니다. 저희의 접근 방식은 생성된 아바타의 포즈 제어를 가능하게 할 뿐만 아니라 대부분의 기존 방법에서 겪는 Janus 문제도 제거합니다. 따라서 보다 안정적이고 뷰에 일관된 아바타 생성 프로세스가 보장됩니다. 또한 DensePose가 제공하는 정확하고 유연한 감독 신호의 이점을 활용하여 생성된 아바타는 SMPL 모델의 관절과 매우 일치하여 간단하고 효과적인 골격 바인딩 및 제어가 가능합니다. DensePose 조건화 ControlNet에만 의존하면 로컬 아티팩트가 발생할 수 있지만 로컬 지오메트리의 충실도와 세부 정보를 향상시키기 위해 점진적인 고해상도 생성 전략을 도입합니다. 생성된 아바타의 거칠기를 완화하기 위해 부드러움 손실을 통합하여 합성 절차를 정규화합니다.겨울왕국의 엘사, 디즈니의 우디, 토이 스토리의 캡틴 아메리카, 슈퍼사이언의 고쿠, 버즈 라이트이어, 영화 주토피아의 닉 와일드, 라이온 킹의 심바, 탱크탑을 입은 바디빌더, 베니스 카니발에서 차려입은 사람, 흰색 탱크탑과 반바지를 입은 남자, 젤다의 링크, 스파이더맨의 링크, 헐크, 로널드 위즐리, 잭 스패로우 선장, 모빌 슈트 건담, 아바타 시리즈의 제이크, 플래시의 링크, 데드풀, 경비원, 검은 벨트를 맨 가라테 마스터, 헤일로 시리즈의 마스터 치프, 스타워즈의 요다, 배트맨, 스톰트루퍼 시리즈, 몽키 D. 루피, 곱슬 머리에 안경을 낀 청년 그림 1: 간단한 텍스트 설명을 기반으로 Avatar Verse에서 생성한 고품질 3D 아바타. 전반적인 기여는 다음과 같습니다. • 텍스트 설명과 참조 인간 포즈만으로 고품질 3D 아바타를 자동으로 생성할 수 있는 방법인 Avatar Verse를 제시합니다. • DensePose-Conditioned Score Distillation Sampling Loss를 제시합니다. 이는 포즈 인식 3D 아바타 합성을 용이하게 하고 Janus 문제를 효과적으로 완화하여 시스템 안정성을 향상시키는 접근 방식입니다. • 점진적인 고해상도 생성 전략을 통해 생성된 3D 아바타의 품질을 강화합니다. 이 방법은 꼼꼼한 조악-정밀 정제 프로세스를 통해 손, 액세서리 등의 요소를 포함하는 뛰어난 디테일의 3D 아바타를 합성합니다. • Avatar Verse는 품질과 안정성 모두에서 탁월한 성능을 제공합니다. 포괄적인 사용자 연구로 보완된 엄격한 정성적 평가는 고품질 3D 아바타를 제작하는 데 있어 Avatar Verse의 탁월함을 강조하며, 최고 품질의 안정적이고 제로샷 3D 아바타 제작에서 새로운 기준을 제시합니다.
--- RELATED WORK ---
2.1. 텍스트 기반 3D 콘텐츠 생성 텍스트 기반 2D 이미지 생성의 성공은 텍스트 기반 3D 콘텐츠 생성 개발의 길을 열었습니다.
--- METHOD ---
s는 주로 비디오나 *이 저자들도 동등하게 기여했습니다.참조 이미지에서 얻은 제한된 시각적 사전 정보에 의존하여 복잡한 텍스트 프롬프트가 있는 창의적인 아바타를 생성하는 능력이 제한되었습니다.2D 이미지 생성에서 확산 모델(Rombach et al. 2021; Saharia et al. 2022; Zhang and Agrawala 2023)은 주로 대규모 텍스트-이미지 쌍의 가용성으로 인해 상당한 창의성을 보여줍니다.그럼에도 불구하고 3D 모델의 희소성과 제한된 다양성은 3D 확산 모델을 효과적으로 학습하는 데 과제를 제시합니다.최근 연구(Cao et al. 2023; Huang et al. 2023; Kolotouros et al. 2023; Poole et al. 2022)에서는 사전 학습된 텍스트-이미지 생성 모델을 사용하여 NeRF(Neural Radiance Fields)(Mildenhall et al. 2020)를 최적화하여 충실도 높은 3D 모델을 생성하는 방법을 조사했습니다. 그러나 다양한 포즈, 모습, 모양을 보이는 고품질 3D 아바타를 안정적으로 만드는 것은 여전히 어려운 작업입니다. 예를 들어, 추가 제어 없이 NeRF 최적화를 안내하기 위해 공통 점수 증류 샘플링(SDS)(Poole et al. 2022)을 사용하면 Janus(다중 얼굴) 문제가 발생하는 경향이 있습니다. 또한 현재 접근 방식으로 생성된 아바타는 눈에 띄는 흐릿함과 거칠기를 보이는 경향이 있어 고해상도 로컬 텍스처 세부 정보, 액세서리 및 기타 관련 기능이 없습니다. 이러한 약점을 해결하기 위해 텍스트 설명과 포즈 안내에서 고품질의 안정적인 3D 아바타를 생성하도록 설계된 새로운 프레임워크인 Avatar Verse를 제안합니다. 먼저 800K 이미지에 걸쳐 인간의 DensePose 조건(Güler, Neverova, and Kokkinos 2018)으로 새로운 ControlNet을 훈련합니다. 그런 다음 2D DensePose 신호에 대한 SDS 손실 조건이 ControlNet 위에 구현됩니다. 이런 방식으로, 우리는 서로 다른 2D 뷰 사이와 모든 2D 뷰와 3D 공간 사이의 정확한 뷰 대응을 얻습니다. 우리의 접근 방식은 생성된 아바타의 포즈 제어를 가능하게 할 뿐만 아니라 대부분의 기존 방법에서 겪는 Janus Problem도 제거합니다. 따라서 보다 안정적이고 뷰에 일관된 아바타 생성 프로세스가 보장됩니다. 또한 DensePose가 제공하는 정확하고 유연한 감독 신호의 이점을 활용하여 생성된 아바타를 SMPL 모델의 관절과 매우 일치시켜 간단하고 효과적인 골격 바인딩과 제어를 가능하게 합니다. DensePose 조건화 ControlNet에만 의존하면 로컬 아티팩트가 발생할 수 있지만, 로컬 지오메트리의 충실도와 세부 사항을 향상시키기 위해 점진적인 고해상도 생성 전략을 도입합니다. 생성된 아바타의 거칠기를 완화하기 위해 부드러움 손실을 통합하여 합성 절차를 정규화합니다.겨울왕국의 엘사, 디즈니의 우디, 토이 스토리의 캡틴 아메리카, 슈퍼사이언의 고쿠, 버즈 라이트이어, 영화 주토피아의 닉 와일드, 라이온 킹의 심바, 탱크탑을 입은 바디빌더, 베니스 카니발에서 차려입은 사람, 흰색 탱크탑과 반바지를 입은 남자, 젤다의 링크, 스파이더맨의 링크, 헐크, 로널드 위즐리, 잭 스패로우 선장, 모빌 슈트 건담, 아바타 시리즈의 제이크, 플래시의 링크, 데드풀, 경비원, 검은 벨트를 맨 가라테 마스터, 헤일로 시리즈의 마스터 치프, 스타워즈의 요다, 배트맨, 스톰트루퍼 시리즈, 몽키 D. 루피, 곱슬 머리에 안경을 낀 청년 그림 1: 간단한 텍스트 설명을 기반으로 Avatar Verse에서 생성한 고품질 3D 아바타. 전반적인 기여는 다음과 같습니다. • 텍스트 설명과 참조 인간 포즈만으로 고품질 3D 아바타를 자동으로 생성할 수 있는 방법인 Avatar Verse를 제시합니다. • DensePose-Conditioned Score Distillation Sampling Loss를 제시합니다. 이는 포즈 인식 3D 아바타 합성을 용이하게 하고 Janus 문제를 효과적으로 완화하여 시스템 안정성을 향상시키는 접근 방식입니다. • 점진적인 고해상도 생성 전략을 통해 생성된 3D 아바타의 품질을 강화합니다. 이 방법은 꼼꼼한 조악-정밀 정제 프로세스를 통해 손, 액세서리 등의 요소를 포함하는 뛰어난 디테일의 3D 아바타를 합성합니다. • Avatar Verse는 품질과 안정성 모두에서 탁월한 성능을 제공합니다. 포괄적인 사용자 연구로 보완된 엄격한 정성적 평가는 Avatar Verse가 고품질 3D 아바타를 제작하는 데 있어 우월함을 강조하며, 이를 통해 최고 품질의 안정적이고 제로샷 3D 아바타 제작에서 새로운 벤치마크를 설정합니다. 2. 관련 연구 2.1. 텍스트 가이드 3D 콘텐츠 생성 텍스트 가이드 2D 이미지 생성의 성공은 텍스트 가이드 3D 콘텐츠 생성 방법 개발의 길을 열었습니다. CLIP-forge(Sanghi et al. 2021), DreamFields(Jain et al. 2021), CLIP-Mesh(Khalid et al. 2022)는 CLIP 모델(Radford et al. 2021)을 활용하여 메시 및 NeRF와 같은 기본 3D 표현을 최적화합니다. DreamFusion(Poole et al. 2022)은 먼저 3D 생성 중에 사전 훈련된 확산 모델(Saharia et al. 2022)로부터 감독을 얻기 위해 스코어 증류 샘플링(SDS) 손실을 제안합니다. Latent-NeRF(Metzer et al. 2022)는 잠재 공간에서 확산 프로세스를 작동하는 NeRF를 최적화하여 DreamFusion을 개선합니다. TEXTure(Richardson et al. 2023)는 주어진 3D 메시에 대한 깊이 확산 모델을 사용하여 텍스처 맵을 생성합니다. ProlificDreamer(Wang et al. 2023b)는 변분 스코어 증류를 제안하고 고해상도 및 고충실도 결과를 생성합니다. 이러한 방법은 3D 일반 콘텐츠 생성에서 유망한 성능을 보였지만 아바타를 생성할 때 종종 최적이 아닌 결과를 생성하여 낮은 품질, Janus(다면적) 문제 및 잘못된 신체 부위와 같은 문제가 나타납니다. 이와 대조적으로, 우리의 Avatar Verse는 텍스트 프롬프트에서 정확하고 고품질의 3D 아바타를 생성할 수 있게 해줍니다. 2.2. 텍스트 가이드 3D 아바타 생성 Avatar-CLIP(Hong et al. 2022)은 먼저 모양 VAE 네트워크로 3D 인간 기하학을 초기화하고 CLIP(Radford et al. 2021)을 활용하여 기하학 조각 및 텍스처 생성을 용이하게 합니다. DreamAvatar(Cao et al. 2023)와 AvatarCraft(Jiang et al. 2023)는 SMPL 모델을 모양 사전으로 사용하고 사전 학습된 텍스트-이미지 확산 모델을 활용하여 3D 아바타를 생성합니다. DreamFace(Zhang et al. 2023)는 개인화된 3D 얼굴 구조를 생성하기 위해 조악-미세 체계를 도입합니다. HeadSculpt(Han et al. 2023)는 랜드마크 기반 제어와 머리의 뒷면 모양을 나타내는 학습된 텍스트 임베딩을 활용하여 3D 머리 아바타를 생성합니다. 저희 작업과 동시에 DreamWaltz(Huang et al. 2023)는 3D 일치 오클루전 인식 스코어 증류 샘플링을 제시하는데, 이는 뷰 정렬 감독을 위한 3D 인식 스켈레톤 컨디셔닝을 통합합니다. 원래의 훈련 데이터에 의해 제약을 받는 스켈레톤 컨디셔닝 확산 모델은 원하는 아바타의 뒷면을 생성하지 못하거나 부분적인 스켈레톤 정보가 제공될 때 특정 신체 부위를 생성하는 데 어려움을 겪는 등 여전히 뷰 불일치를 보일 수 있습니다. 또한 스켈레톤의 희소성으로 인해 모델이 아바타 윤곽과 모서리를 결정하기 어려워 품질이 낮은 결과가 발생합니다. 반대로, 제안된 DensePose 조건화 ControlNet은 전신, 다리, 머리 등을 포함한 다양한 관점과 신체 부위에 대한 고품질의 뷰 일관성 있는 이미지 생성을 보장하여 뛰어난 아바타 품질을 보장합니다.2.3. 고품질 3D 아바타 생성 최근, 고품질 또는 고충실도 3D 생성 및 재구성에 대한 초점이 커지고 있습니다.일부 방법은 다중 시점 RGB 비디오에서 고충실도 3D 인간 아바타를 생성하려고 시도합니다(Isik et al. 2023; Jiang et al. 2022; Li et al. 2023b; Wang et al. 2023a; Zheng et al. 2023). 또한 고해상도 잠재 확산 모델을 최적화하여 텍스처가 있는 3D 메시 모델을 정제함으로써 거칠게-미세하게 방법론을 탐구한 연구(Lin et al. 2022)도 있었습니다. DreamHuman(Kolotouros et al. 2023)은 저희 작업과 병행하여 최적화 중에 6개의 중요한 신체 영역에 대해 64×64 이미지를 확대하여 렌더링합니다. 그러나 Mip-NeRF-360의 계산 요구 사항에 의해 제한되어 고해상도 세부 정보가 없는 저해상도 아바타만 생성할 수 있습니다. 또한 DreamHuman은 직접적인 기하학적 감독을 위해 SMPL 모양을 사용하는데, 이는 피부에 꼭 맞는 아바타를 제공하는 경향이 있습니다. 반면에 저희 방법은 더 제어 가능하고 유연하여 더 광범위한 액세서리, 의류 및 기타 기능을 생성할 수 있습니다. 저희의 Avatar Verse는 점진적 고해상도 생성 전략을 도입합니다. 여기에는 카메라의 반경을 점차적으로 줄이고 뚜렷한 신체 부위에 초점을 맞추는 것이 포함되며, 이를 통해 다양한 액세서리, 의류 및 기타 요소를 생성할 수 있습니다. 저희의 점진적 그리드 사용은 또한 세분화된 생성을 보장합니다. 3. 방법론 이 섹션에서는 텍스트 설명과 신체 포즈만으로 사실적인 3D 아바타를 만들 수 있는 완전 자동 파이프라인인 Avatar Verse를 소개합니다.몇 가지 예비 사항을 소개한 후, 먼저 포즈 인식 3D 아바타 합성을 용이하게 하고 Janus 문제를 효과적으로 완화하는 DensePose 조건 SDS 손실을 설명합니다.그런 다음 합성 품질을 향상시키는 새로운 전략인 점진적 고해상도 생성 전략과 아바타 표면 평활화 전략을 소개합니다.densepose 렌더링 densepose 조건 볼륨 렌더링 LSDS 명시적 NeRF V(밀도) V(색상) 얕은 MLP Caption America ControlNet 공유 관점의 DLSR 사진 i (1) 점진적 그리드 (2) bbox 조임 P (3) 점진적 반경 (4) 초점 모드 (a) 아바타 생성 (b) 점진적 고해상도 생성 그림 2: Avatar Verse 개요. 우리 네트워크는 텍스트 프롬프트와 DensePose 신호를 입력으로 사용하여 DensePose-COCO 사전 훈련된 ControlNet을 통해 명시적 NERF를 최적화합니다. 우리는 점진적 그리드, 점진적 반경 및 초점 모드를 포함한 전략을 사용하여 고해상도 및 고품질 3D 아바타를 생성합니다. 3.1. 예비 단계 (1) DreamFusion(Poole et al. 2022)에서 처음 제안한 점수 증류 샘플링은 사전 훈련된 확산 모델 ε에서 사전 지식을 미분 가능한 3D 표현 0으로 증류합니다. 미분 가능한 NeRF 모델 g에서 렌더링된 이미지 x = g(0)이 주어지면 랜덤 노이즈 €를 추가하여 노이즈가 있는 이미지를 얻습니다. SDS는 예측된 (x+; y, t)와 추가된 노이즈 e의 차이를 최소화하여 매개변수의 그래디언트를 계산합니다. 노이즈 Еф = w(t) (Es VeLSDS(0, xe) = Et,€ | w(t) (€ (zt; y, t) — €) მე &quot; ae (1) 여기서 zt는 노이즈 레벨 t에서 노이즈가 있는 이미지를 나타내고, w(t)는 노이즈 레벨 t와 텍스트 프롬프트 y에 따라 달라지는 가중치 함수입니다. (2) SMPL(Bogo et al. 2016; Loper et al. 2015)은 3D 매개변수 인체 모델입니다. 여기에는 6,890개의 신체 정점과 24개의 키포인트가 포함되어 있습니다. 포즈 매개변수 ε € RK×³와 신체 모양 매개변수 BE R10을 조립하면 3D SMPL 모델을 다음과 같이 표현할 수 있습니다. T(ẞ,) = T+Bs(ß) + Bp(§) (3) M(ẞ, §) = LBS (T(ß, §), J(ß), §, W), 여기서 T(B, §)는 표준 공간의 평균 템플릿 모양 T, 모양에 따라 달라지는 변형 Bs(B) Є RN×3, 포즈에 따라 달라지는 변형 Bp (§) € RN×³를 결합한 비강체 변형을 나타냅니다. LBS(•)는 관절 변형에 해당하는 선형 블렌드 스키닝 함수를 나타냅니다. 이는 해당 키포인트 위치 J (B) Є RN×³, 포즈 및 블렌드 가중치 W = RNK를 기반으로 T(B, ε)를 매핑합니다. 관찰 포즈 아래의 본체 정점 v는 K Vo Σ Wk Gk (§, jk), k=(4)입니다. 여기서 wk는 스키닝 가중치이고 Gk (§, jk)는 k번째 관절 jk를 표준 공간에서 관찰 공간으로 변환하는 아핀 변형입니다. (3) DensePose(Güler, Neverova, and Kokkinos 2018)는 2D 이미지와 인체의 3D 표면 기반 모델 간의 밀집 대응 관계를 확립하는 데 도움이 되는 선구적인 기술입니다. SMPL 모델(Loper et al. 2015)을 활용하여 DensePose는 SMPL 메시 내의 각 삼각형 면을 미리 정의된 24개의 신체 부위 중 하나에 할당할 수 있습니다. 이 대응 관계를 통해 SMPL 메시에서 연관된 영역을 렌더링하여 주어진 관점에서 부위 레이블이 지정된 2D 신체 이미지를 생성할 수 있습니다. 3.2. DensePose SDS 손실 이전 연구(Lin et al. 2022; Poole et al. 2022)는 주로 &quot;정면도&quot; 또는 &quot;위쪽도&quot;와 같은 보충 텍스트 프롬프트를 사용하여 뷰 일관성을 향상시킵니다. 그러나 텍스트 프롬프트에만 의존하면 임의의 뷰에서 2D 확산 모델을 정확하게 조건화하는 데 부적절하다는 것이 입증되었습니다. 이러한 부적절성은 3D 모델 합성에서 불안정성을 초래하여 Janus 문제와 같은 문제가 발생합니다. 해결책으로 그림 2에서 볼 수 있듯이 DensePose(Güler, Neverova 및 Kokkinos 2018)를 보다 견고한 제어 신호로 활용하는 것을 제안합니다. (a) (b) (c) 그림 3: DensePose 조건화 ControlNet의 정성적 결과. (a) 다양한 관점과 신체 부위를 가진 DensePose로 제어되는 생성된 10개 이미지. (b) 인간 포즈(Openpose) 신호로 제어되는 동일한 관점을 가진 10개 해당 이미지. 종종 아바타의 뒷면을 생성하지 못하고(4번째(b)) 부분 생성(마지막 두 열)에 어려움을 겪습니다.(c) 피부에 꼭 맞지 않는 생성은 사실적인 아바타와 허구의 아바타를 모두 생성합니다. 우리는 DensePose를 조건으로 선택했는데, 이는 2D 이미지에서 3D 신체 부위의 정확한 현지화를 제공하여 골격이나 다른 유형의 조건에서 간과될 수 있는 복잡한 세부 사항과 경계 조건을 제공하기 때문입니다. 특히 어려운 시나리오에서 회복성을 보여 신체 부위가 부분적으로 가려져 있어도 정확한 제어가 가능합니다. 먼저 DeepFashion(Liu et al. 2016) 데이터 세트를 사용하여 DensePose 부분 레이블 주석으로 조건화된 ControlNet(Zhang and Agrawala 2023)을 학습합니다. 그림 3은 다양한 관점과 전신, 다리, 머리 등과 같은 신체 부위를 포함하여 고품질의 뷰 일관성 있는 이미지를 생성하는 ControlNet의 기능을 보여줍니다. 특정 카메라 시점과 포즈 P가 주어지면, 해당 포즈 P로 partlabeled SMPL 모델을 렌더링하여 DensePose 조건 이미지 c를 생성합니다. 조건부 SDS 손실은 다음 방정식에 나와 있습니다. VeLP-SDS (0, x = g(0, P)) = Et,€ [w(t) (ĉ – €) მუ ê = € (zt; y, t, c = h(SMPL, P)) (5) (6) 여기서 g와 h는 각각 NeRF 렌더링 함수와 SMPL 렌더링 함수를 나타냅니다. NeRF 모델과 SMPL 포즈 모델은 동일한 카메라 시점을 공유합니다. 시점을 이렇게 정렬하면 NeRF로 캡처한 장면과 SMPL로 모델링한 해당 인간 포즈 간에 일관되고 일관된 표현이 가능해져 아바타를 더 잘 생성할 수 있습니다. DensePose 조건부 ControlNet은 그림 3(c)에 표시된 것처럼 다양한 피부에 꼭 맞지 않는 사실적이고 허구적인 아바타를 생성할 수 있습니다. 3.3. 점진적 고해상도 생성 이전 연구에서는 일반적으로 전신에 SDS 손실을 적용했는데, 이러한 글로벌 안내는 특히 손, 얼굴 등의 영역에서 고품질 세부 정보를 생성하지 못하는 경우가 많습니다. 이러한 접근 방식은 고품질의 세부적인 지오메트리와 텍스처를 생성하는 데 효과적인 안내 메커니즘이 부족합니다. 이러한 한계를 해결하기 위해 점진적 그리드, 초점 모드, 점진적 반경을 포함하여 정확하고 자세한 표현 생성을 촉진하는 다양한 안내 전략을 제안합니다. 점진적 그리드 점진적 학습 전략은 일반적으로 2D 생성 및 3D 재구성 방법(Karras et al. 2019; Liu et al. 2020; Sun, Sun, and Chen 2021)에서 사용되지만, 깔끔하고 효율적인 3D 아바타 생성을 위한 방법에서 중요하다고 생각합니다. 최종 모델 해상도로 미리 정해진 수의 폭셀 N₁을 설정하고 특정 최적화 단계 후에 폭셀 수를 두 배로 늘립니다. 폭셀 크기 sy는 그에 따라 업데이트됩니다. 학습 초기 단계에서는 대략적인 아바타 모양만 생성하면 됩니다. 그리드를 적게 할당하면 학습 공간을 줄이고 떠다니는 아티팩트를 최소화할 수 있습니다. 이 전략을 사용하면 최적화 프로세스 전반에 걸쳐 아바타를 점진적으로 개선하여 모델이 계산 리소스를 적응적으로 할당할 수 있습니다. 또한 NeRF 최적화의 초기 단계는 자유 공간(즉, 밀도가 낮은 공간)이 지배합니다. 이 사실에 동기를 부여하여 거친 아바타 영역을 찾고 이 중요한 영역에 계산 및 메모리 리소스를 할당하는 것을 목표로 합니다. 대상 영역을 구분하기 위해 밀도 임계값을 사용하여 장면을 필터링하고 경계 상자(bbox)를 사용하여 이 영역을 단단히 둘러싸고 있습니다. dx, dy, dz가 조여진 bbox의 길이를 나타내도록 하고, 3 dxxdyxdz 폭셀 크기는 다음과 같이 계산할 수 있습니다. sv = Nv로 bbox의 길이를 줄이면 폭셀 크기가 줄어들어 고해상도와 아바타 주변의 더 많은 폭셀이 가능해집니다. 이렇게 하면 세밀한 신체 윤곽, 얼굴 특징, 옷 주름과 같은 복잡한 세부 사항을 포착하고 모델링하는 모델의 능력이 향상됩니다.진행형 반경 pg_ckpt를 체크포인트 단계 집합이라고 합니다.pg_ckpt에서 학습 단계에 도달하면 카메라 반경을 20% 줄입니다.이렇게 하면 세밀한 세부 사항을 단계별로 점진적으로 렌더링할 수 있습니다.ap(a) ** (b) DreamFusion DreamAvatar DreamWaltz Ours DreamHuman Ours 그림 4: 4가지 SOTA 방법과의 정성적 비교.AvatarVerse에서 생성한 엄선되지 않은 여러 결과를 보여줍니다.우리의 방법은 다른 방법에 비해 고해상도 세부 사항을 생성하고 세밀한 지오메트리를 유지합니다.(a): &quot;스파이더맨&quot;; 흰색 탱크탑과 반바지를 입은 남자&quot;,(b): &quot;조커&quot;; &quot;검은띠를 두른 가라테 마스터&quot;,(c): &quot;스톰트루퍼&quot;; &quot;갑옷을 입은 로마 군인&quot;. 조건부 SDS 손실을 아바타의 더 작은 영역에 적용하면 모델은 복잡한 특징을 포착하고 강조하여 궁극적으로 더욱 사실적이고 시각적으로 매력적인 출력을 생성할 수 있습니다.초점 모드 마찬가지로 특정 신체 부위에서 더 나은 복잡성을 생성하기 위해 거친 단계와 미세 단계 모두에서 초점 모드(그림 2(b) 참조)를 도입합니다.SMPL 사전 덕분에 주어진 포즈에 대한 원시 신체 부위 위치를 쉽게 계산할 수 있습니다.카메라를 중요한 신체 부위에 가깝게 배치하면 512 x 512 해상도의 매우 작은 아바타 영역에서 손실 계산을 수행할 수 있습니다.그림 2에서 볼 수 있듯이 DensePose ControlNet의 안정적인 성능 덕분에 추가적인 계산 리소스 없이 부분 신체를 생성할 수 있습니다.따라서 초점 모드는 고품질 아바타 세부 정보를 만드는 데 도움이 될 수 있습니다.메시 세분화 합리적인 메모리 제약과 계산 예산 내에서 미세한 고해상도 아바타를 렌더링하기 위해 변형 가능한 사면체 그리드(Lin et al. 2022; Shen et al. 2021) 생성된 아바타의 텍스처 3D 메시를 학습합니다. (Lin et al. 2022)와 유사하게, 우리는 훈련된 명시적 NeRF를 메시 지오메트리의 초기화로 사용하고 DensePose 조건 SDS 그래디언트(Eq. 5)를 사용하여 역전파를 통해 메시를 최적화합니다. 3.4. 아바타 표면 평활화 최적화 중에 명시적 그리드에 대해 전역적으로 일관된 아바타 모양을 유지하는 것은 높은 자유도와 공간적 일관성 부족으로 인해 어려울 수 있습니다. 각 폭셀 포인트의 개별 최적화는 그리드 전체에서 정보 공유를 제한하여 생성된 아바타의 표면이 덜 매끄럽고 일부 국소적 최소값이 발생합니다. 이 문제를 해결하기 위해 (Wu et al. 2022)의 가우시안 합성곱 G의 정의를 따르고 다음과 같이 공식화된 수정된 평활도 정규화를 포함합니다. Lsmooth(V) = ||G(V, kg, σg) – V ||2|(7) 여기서 kg는 커널 크기를 나타내고 σg는 표준 편차를 나타냅니다. 이 평활도 항을 밀도 폭셀 그리드의 그래디언트에 적용하여 그래디언트 평활도 손실 smooth(VV(밀도))를 생성합니다. 이를 통해 더 매끄러운 표면이 촉진되고 자유 공간에서 노이즈가 있는 점의 존재가 완화됩니다. 이 접근 방식의 전체 손실은 다음과 같이 정의되며, \는 평활도 계수를 나타냅니다. L = LP-SDS+ * smooth(V) 4.
--- EXPERIMENT ---
s (8) 이 섹션에서는 제안하는 방법의 효과성을 설명합니다. 제안하는 각 전략의 효능을 보여주고 최근의 최신 방법과 자세히 비교합니다. 4.1. 구현 세부 정보 우리는 (Sun, Sun, and Chen 2021)에 따라 우리 방법에서 명시적 NeRF를 구현합니다. 각 텍스트 프롬프트에 대해 Avatar Verse를 각각 거친 단계와 메시 세분화 단계에서 5000회와 4000회 반복으로 훈련합니다. 전체 생성 프로세스는 단일 NVIDIA A100 GPU에서 약 2시간이 걸립니다. 이 섹션에서는 초기화, densepose 훈련 및 점진적 고해상도 생성 세부 정보를 포함합니다. 보다 포괄적인 실험 세부 정보는 보충 자료를 참조하세요. 초기화 최적화의 초기 단계를 돕기 위해 (Poole et al. 2022)에서 영감을 받은 기술을 채택하고 원점 주위에 작은 타원형 밀도 &quot;블롭&quot;을 도입합니다. XYZ 축의 &quot;blob&quot;의 크기는 SMPL 포즈 모델의 좌표 범위에 따라 결정됩니다. 또한, 아바타 생성을 용이하게 하기 위해 추가적인 SMPL 유도 밀도 편향(Cao et al. 2023)을 통합합니다. Dense Pose Training 사전 학습된 DensePose(Güler, Neverova, and Kokkinos 2018) 모델을 사용하여 DeepFashion 데이터 세트(Liu et al. 2016)에 주석을 달아 800,000개가 넘는 이미지 쌍을 생성합니다. ControlNet은 BLIP2에서 생성된 텍스트 프롬프트(Li et al. 2023a)가 있는 이러한 이미지 쌍을 사용하여 학습합니다. 접근 방식에 사용된 확산 모델은 SD1.5입니다. 점진적 고해상도 생성 점진적 그리드의 경우, 거친 단계에서 500, 1500, 2000번 반복할 때 폭셀 수를 두 배로 늘립니다. 거친 단계에서 3000단계를 거친 후, 경계 상자를 밀도가 0.1을 초과하는 영역. 진행형 반경은 세 단계로 구성되며, 카메라 반경은 각각 1.4~2.1, 1~1.5, 0.8~1.2입니다. 두 단계에서 1000회와 2000회 반복에서 반경을 줄입니다. 초점 모드는 거친 단계에서 1000번째 단계부터 시작하여 메시 세분화 단계 전체에서 일관되게 사용됩니다. 4.2. SOTA 방법과의 정성적 결과 비교 그림 4에서 DreamFusion(Poole et al. 2022), DreamAvatar(Cao et al. 2023), DreamWaltz(Huang et al. 2023), DreamHuman(Kolotouros et al. 2023)과의 정성적 비교를 제시합니다. 저희 방법은 기하학과 텍스처 품질 측면에서 이러한 접근 방식보다 지속적으로 우수한 성과를 보입니다. 우리의 방법으로 생성된 아바타의 표면은 점진적인 고해상도 생성 전략 덕분에 매우 선명합니다. DreamHuman과 비교할 때, 우리의 방법으로 생성된 아바타는 모든 경우에 피부, 얼굴 특징, 옷 등을 포함하여 더 풍부한 세부 정보를 보여줍니다. 유연한 아바타 생성 그림 5에서 우리는 DensePose 제어가 없기 때문에 다른 기존 방법으로는 달성할 수 없는 3D 부분 아바타를 생성하는 우리 방법의 역량을 보여줍니다. 영어: 저희의 방법은 입력 DensePose 신호를 직접 수정하여 부분 생성을 가능하게 하며, &quot;The head of...&quot; 또는 &quot;The upper body of...&quot;와 같은 추가 설명 정보가 필요 없습니다. 이를 통해 전신, 반신, 머리만, 손만 등 다양한 유형의 부분 아바타를 생성할 수 있습니다. 또한 Avatar Verse는 다양한 포즈로 아바타를 생성할 수 있어 뷰 일관성에 대한 안정적인 제어를 보여줍니다. 4.3. 다양한 방법 간의 사용자 연구 선호도 DreamFusion -| 0.5% DreamAvatar - 1.5% DreamWaltz Ours DreamHuman 13.0% 85.0% 19.0% 81.0% Ours 0% 20% 40% 60% 80% 100% 그림 6: 사용자 연구의 정량적 결과. 생성된 3D 아바타의 품질을 더욱 평가하기 위해 동일한 텍스트 프롬프트에서 4가지 SOTA 방법과 결과의 성능을 비교하는 사용자 연구를 수행합니다. 생성된 결과 30개(렌더링된 회전 비디오로 표시)를 무작위로 선택하고 16명의 자원봉사자에게 기하학 및 텍스처 품질을 기준으로 가장 좋아하는 결과에 투표하도록 요청합니다. 그림 6에서 Avatar Verse를 DreamFusion(Poole et al. 2022), DreamAvatar(Cao et al. 2023), DreamWaltz(Huang et al. 2023)와 비교하여 다른 세 가지 방법보다 우리 방법을 상당히 선호한다는 것을 보여줍니다. 또한 현실적인 인간 측면에서 우리 방법을 DreamHuman(Kolotouros et al. 2023)과 비교합니다. 놀랍게도 자원봉사자의 81%가 Avatar Verse에 찬성표를 던졌습니다. 4.4. Ablation Study 진행형 전략의 효과 Avatar Verse의 디자인 선택을 평가하기 위해 b) 진행형 그리드, c) 진행형 반경, d) 초점 모드, e) 메시 정제의 효과에 대한 Ablation Study를 수행합니다. 이러한 구성 요소를 순차적으로 추가하고 그림 7에 결과를 보고합니다. 초기 결과에는 세부 정보가 부족하고(예: 등에 검 없음, 팔 보호대 없음) 수많은 떠다니는 아티팩트가 나타납니다. 전반적인 품질이 흐릿하고 불분명합니다. 진행형 그리드를 통합하면 아바타 영역 주위에 더 많은 폭셀이 모여 아바타에 더 많은 세부 정보가 도입됩니다. 카메라 거리를 점진적으로 좁히면 모델은 잠재 확산에 내재된 세부 정보를 활용하여 많은 수의 떠다니는 아티팩트를 제거하고 등의 검과 같은 로컬 세부 정보를 향상시킬 수 있습니다. 초점 모드는 추가로 확대하고 512 x 512의 해상도를 사용하여 특정 신체 부위를 타겟팅하고 최적화하여 고화질 및 복잡한 로컬 세부 정보를 생성합니다. 메시 정제는 거친 아바타의 3D 메시를 더욱 최적화하여 더 미세한 아바타 텍스처를 생성합니다.(c) DensePose (b) 스켈레톤 (a) 제어 없음 그림 8: 제어 신호의 영향.(a) 추가 제어 없음;(b) 스켈레톤 제어 있음;(c) DensePose 제어 있음.각 유형에 대해 RGB, 일반, 깊이 및 해당 제어 신호를 보여줍니다.크게 개선되었습니다.(a) + 프로그램 그리드 (b) + 프로그램 방사.(c) D 8 B + 포커스 모드 + 메시 정제 (d) (e) (a) 표면 평활화 없음 (b) 표면 평활화 있음 그림 7: 점진적 전략의 영향.(a) 점진적 전략 없음;(b) 점진적 그리드 추가;(c) (b)에 점진적 반경 추가;(d) (c)에 포커스 모드 추가;(e) 메시 정제 추가, 전체 방법.DensePose 제어의 효과 그림 8은 다양한 제어 신호의 영향을 보여줍니다. 골격에 의해 조건지어지면 모델은 인간 형상에 더 가까운 아바타를 생성할 수 있습니다.그러나 아바타의 가장자리는 흐릿하게 보이고 여전히 심각한 Janus 문제에 직면합니다.DensePose 제어를 프레임워크에 통합함으로써 보다 정확한 아바타 경계, 복잡한 세부 사항 및 안정적인 아바타 제어를 달성하여 생성된 아바타의 전반적인 품질과 모양이 상당히 개선되었습니다.표면 평활화의 효과 아바타 표면 평활화는 생성된 아바타가 컴팩트한 지오메트리와 매끄러운 표면을 나타내도록 보장하기 때문에 Avatar Verse 프레임워크에서 중요한 역할을 합니다.그림 9에서 볼 수 있듯이 매끄러운 손실과 조건부 SDS 손실 간의 균형을 찾으면 아바타의 시각적 품질과 사실성이 향상됩니다.그림 9: 표면 평활화 전략의 영향.(a) 표면 평활화 없음;(b) 표면 평활화 있음.결과는 동일한 텍스트 프롬프트로 생성됩니다.
--- CONCLUSION ---
이 논문에서는 텍스트 프롬프트와 포즈에서 고품질의 안정적인 3D 아바타를 생성하도록 설계된 새로운 프레임워크인 Avatar Verse를 소개합니다. 훈련된 DensePose 조건화 ControlNet을 사용하여 명시적 NeRF 최적화 중에 안정적인 부분적 또는 전신 제어를 용이하게 합니다. 점진적인 고해상도 생성 전략 덕분에 3D 아바타 결과는 뛰어난 텍스처와 기하학적 품질을 보여줍니다. 더욱이 생성된 아바타는 SMPL 모델의 관절과 높은 정렬을 보이기 때문에 골격 바인딩을 통해 쉽게 애니메이션을 적용할 수 있습니다. 포괄적인 실험과 사용자 연구를 통해 Avatar Verse가 이전 및 현대적 접근 방식을 크게 능가한다는 것을 보여줍니다. 우리는 우리의 접근 방식이 신경 및 프롬프트 상호 작용 시대에 고품질 3D 아바타 생성을 갱신한다고 믿습니다. 참고 문헌 Bogo, F.; Kanazawa, A.; Lassner, C.; Gehler, P.; Romero, J.; 및 Black, MJ 2016. Keep It SMPL: 단일 이미지에서 3D 인간 포즈 및 모양의 자동 추정.ArXiv, abs/1607.08128. Cao, Y.;Cao, Y.-P.;Han, K.;Shan, Y.;Wong, KYK 2023. DreamAvatar: 확산 모델을 통한 텍스트 및 모양 안내 3D 인간 아바타 생성.ArXiv, abs/2304.00916. Güler, RA;Neverova, N.;Kokkinos, I. 2018. DensePose: 야생에서의 밀집 인간 포즈 추정.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스, 7297-7306. Han, X.;Cao, Y.;Han, K.;Zhu, X.;Deng, J.;Song, Y.-Z.; Xiang, T.; 및 Wong, K.-YK 2023. HeadSculpt: 텍스트로 3D 헤드 아바타 제작.ArXiv, abs/2306.03038. Hong, F.; Zhang, M.; Pan, L.; Cai, Z.; Yang, L.; 및 Liu, Z. 2022. AvatarCLIP: 3D 아바타의 제로샷 텍스트 기반 생성 및 애니메이션.ACM Trans. Graph., 41: 161:1– 161:19. Huang, Y.; Wang, J.; Zeng, A.; Cao, H.; Qi, X.; Shi, Y.; Zha, Z.; 및 Zhang, L. 2023. DreamWaltz: 복잡한 3D 애니메이션 아바타로 장면 만들기.ArXiv, abs/2305.12529. Isik, M.; Rünz, M.;Georgopoulos, M.;Khakhulin, T.;Starck, J.;de Agapito, L.;Nießner, M. 2023. HumanRF: 움직이는 인간을 위한 고충실도 신경 복사장.ArXiv, abs/2305.06356.Jain, A.;Mildenhall, B.;Barron, JT;Abbeel, P.;Poole, B. 2021. Dream Fields를 사용한 Zero-Shot Text-Guided Object Generation.2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 857–866.Jiang, R.;Wang, C.;Zhang, J.;Chai, M.;He, M.;Chen, D.; 및 Liao, J. 2023. AvatarCraft: 매개변수화된 모양 및 포즈 제어를 사용하여 텍스트를 신경 인간 아바타로 변환.ArXiv, abs/2303.17606. Jiang, T.; Chen, X.; Song, J.; 및 Hilliges, O. 2022. InstantAvatar: 단초점 비디오에서 아바타 학습.ArXiv, abs/2212.10550. Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; 및 Aila, T. 2019. StyleGAN의 이미지 품질 분석 및 개선.2020 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 8107-8116. Khalid, NM; Xie, T.; Belilovsky, E.; 및 Popa, T. 2022. CLIP-Mesh: 사전 학습된 이미지-텍스트 모델을 사용하여 텍스트에서 텍스처 메시 생성. SIGGRAPH Asia 2022 컨퍼런스 논문. Kolotouros, N.; Alldieck, T.; Zanfir, A.; Bazavan, EG; Fieraru, M.; 및 Sminchisescu, C. 2023. DreamHuman: 텍스트에서 애니메이션화 가능한 3D 아바타. ArXiv, abs/2306.09329. Li, J.; Li, D.; Savarese, S.; 및 Hoi, S. 2023a. BLIP-2: 동결된 이미지 인코더 및 대규모 언어 모델을 사용한 언어-이미지 사전 학습 부트스트래핑. ICML에서. Li, Z.; Zheng, Z.; Liu, Y.; Zhou, B.; 및 Liu, Y. 2023b. PoseVocab: 인간 아바타 모델링을 위한 조인트 구조 포즈 임베딩 학습.ArXiv, abs/2304.13006. Lin, C.-H.; Gao, J.; Tang, L.; Takikawa, T.; Zeng, X.; Huang, X.; Kreis, K.; Fidler, S.; Liu, M.-Y.; Lin, T.-Y. 2022. Magic3D: 고해상도 텍스트-3D 콘텐츠 생성.ArXiv, abs/2211.10440. Liu, L.; Gu, J.; Lin, KZ; Chua, T.-S.; Theobalt, C. 2020. Neural Sparse Voxel Fields.Arxiv, abs/2007.11571. Liu, Z.; Luo, P.; Qiu, S.; Wang, X.; 및 Tang, X. 2016. DeepFashion: 풍부한 주석을 통한 강력한 옷 인식 및 검색 기능 강화. 2016 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 10961104. Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; 및 Black, MJ 2015. SMPL: 스킨 처리된 다중 사람 선형 모델. ACM Trans. Graph., 34: 248:1-248:16. Metzer, G.; Richardson, E.; Patashnik, O.; Giryes, R.; 및 Cohen-Or, D. 2022. 3D 모양 및 텍스처의 모양 기반 생성을 위한 Latent-NeRF. arXiv 사전 인쇄본 arXiv:2211.07600. Mildenhall, B.; Srinivasan, PP; Tancik, M.; Barron, JT; Ramamoorthi, R.; 및 Ng, R. 2020. NeRF: 뷰 합성을 위한 신경 복사장으로 장면 표현. ArXiv, abs/2003.08934. Poole, B.; Jain, A.; Barron, JT; 및 Mildenhall, B. 2022. DreamFusion: 2D 확산을 사용한 텍스트-3D. ArXiv, abs/2209.14988. Radford, A.; Kim, JW; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; 및 Sutskever, I. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스에서. Richardson, E.; Metzer, G.; Alaluf, Y.; Giryes, R.; and Cohen-Or, D. 2023. TEXTure: 3D 모양의 텍스트 가이드 텍스처링. ArXiv, abs/2302.01721. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2021. 잠재 확산 모델을 사용한 고해상도 이미지 합성. 2022 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 1067410685. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, EL; Ghasemipour, SKS; Ayan, BK; Mahdavi, SS; Lopes, RG; Salimans, T.; Ho, J.; Fleet, DJ; 및 Norouzi, M. 2022. 심층 언어 이해를 통한 사실적인 텍스트-이미지 확산 모델.ArXiv, abs/2205.11487. Sanghi, A.; Chu, H.; Lambourne, J.; Wang, Y.; Cheng, CY; 및 Fumero, M. 2021. CLIP-Forge: 제로 샷 텍스트-모양 생성을 향하여.2022 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 18582– 18592. Shen, T.; Gao, J.; Yin, K.; Liu, M.-Y.; 및 Fidler, S. 2021. 딥 마칭 사면체: 고해상도 3D 모양 합성을 위한 하이브리드 표현.ArXiv, abs/2111.04276. Sun, C.; Sun, M.; 및 Chen, H.-T. 2021. Direct Voxel Grid Optimization: Radiance Fields Reconstruction을 위한 초고속 수렴. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5449–5459. Wang, C.; Chai, M.; He, M.; Chen, D.; 및 Liao, J. 2021. 3D 가이드를 사용한 크로스 도메인 및 얽힘이 풀린 얼굴 조작. IEEE Transactions on Visualization and Computer Graphics, 29: 2053-2066. Wang, L.; Zhao, X.; Sun, J.; Zhang, Y.; Zhang, H.; Yu, T.; 및 Liu, Y. 2023a. StyleAvatar: 단일 비디오에서 실시간 사실적인 인물 아바타. ArXiv, abs/2305.00942. Wang, Z.; Lu, C.; Wang, Y.; Bao, F.; Li, C.; Su, H.; and Zhu, J. 2023b. ProlificDreamer: Variational Score Distillation을 통한 고충실도 및 다양한 Text-3D 생성. ArXiv, abs/2305.16213. Wu, T.; Wang, J.; Pan, X.; Xu, X.; Theobalt, C.; Liu, Z.; and Lin, D. 2022. Voxurf: Voxel 기반 효율적이고 정확한 신경 표면 재구성. ArXiv, abs/2208.12697. Xiu, Y.; Yang, J.; Cao, X.; Tzionas, D.; and Black, MJ 2022. ECON: Normals에서 얻은 명백한 옷을 입은 인간. ArXiv, abs/2212.07422. Zhang, L.; 및 Agrawala, M. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가. Arxiv, abs/2302.05543. Zhang, L.; Qiu, Q.; Lin, H.; Zhang, Q.; Shi, C.; Yang, W.; Shi, Y.; Yang, S.; Xu, L.; 및 Yu, J. 2023. DreamFace: 텍스트 안내에 따른 애니메이션 가능한 3D 얼굴의 점진적 생성. ArXiv, abs/2304.03117. Zheng, Z.; Zhao, X.; Zhang, H.; Liu, B.; 및 Liu, Y. 2023. AvatarReX: 실시간 표현력 있는 전신 아바타. Arxiv, abs/2305.04789.
