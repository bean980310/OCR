--- ABSTRACT ---
이미지와 시각적 콘텐츠가 디지털 환경을 지배하는 시대에 이러한 이미지를 조작하고 개인화하는 기능은 필수가 되었습니다. 사진에서 햇살이 비치는 창틀에 누워 있는 얼룩 고양이를 장난기 어린 강아지로 매끄럽게 대체하는 것을 상상해 보세요. 그러면서도 이미지의 원래 매력과 구성을 보존할 수 있습니다. 기존 이미지에서 개인화된 피사체 교체를 통해 몰입형 이미지 편집 경험을 가능하게 하는 새로운 접근 방식인 Photoswap을 소개합니다. Photoswap은 먼저 참조 이미지에서 피사체의 시각적 개념을 학습한 다음 사전 훈련된 확산 모델을 사용하여 훈련 없이 대상 이미지로 교체합니다. 적절한 자기 주의 및 교차 주의 조작을 통해 잘 개념화된 시각적 피사체를 모든 이미지로 매끄럽게 전송할 수 있으며, 교체된 피사체의 포즈와 이미지의 전반적인 일관성을 유지할 수 있음을 확인했습니다. 포괄적인 실험을 통해 개인화된 피사체 교체에서 Photoswap의 효능과 제어 가능성을 강조합니다. 또한 Photoswap은 주제 교환, 배경 보존 및 전반적인 품질에 걸쳐 인간 평가에서 기준 방법을 크게 능가하여 엔터테인먼트에서 전문 편집에 이르기까지 광범위한 응용 잠재력을 보여줍니다. Jing Gu 및 Xin Eric Wang, {jgu110,xwang366}@ucsc.edu로 연락하십시오. 사전 인쇄본. 1
--- INTRODUCTION ---
현실과 창의성의 경계가 모호한 디지털 세계를 상상해보세요. 햇살이 비치는 창틀에 누워 있는 얼룩 고양이 사진을 장난기 어린 강아지가 같은 포즈를 취한 모습으로 손쉽게 바꿀 수 있는 곳입니다. 아니면 유명한 영화 장면의 일부가 되어 장면의 본질과 구성을 그대로 유지하면서 원래 캐릭터로 완벽하게 대체되는 모습을 상상해보세요. 전문가 수준의 사진 조작 기술뿐만 아니라 자동화되고 사용자 친화적인 방식으로 이 수준의 개인화된 이미지 편집을 달성할 수 있을까요? 이 질문은 개인화된 주제 교체의 핵심입니다. 원래 포즈와 구성의 무결성을 유지하면서 이미지의 주제를 사용자가 지정한 주제로 대체하는 어려운 작업입니다. 엔터테인먼트, 광고, 전문 편집과 같은 분야에서 수많은 응용 프로그램을 열어줍니다. 개인화된 주제 교체는 고유한 과제를 안고 있는 복잡한 작업입니다. 이 작업을 위해서는 원래 주제와 대체 주제에 내재된 시각적 개념을 심도 있게 이해해야 합니다. 동시에 새 주제를 기존 이미지에 완벽하게 통합해야 합니다. 주제 교체의 중요한 목표 중 하나는 대체 주제의 유사한 포즈를 유지하는 것입니다. 교체된 주제가 원래 포즈와 장면에 완벽하게 들어맞아 자연스럽고 조화로운 시각적 구성을 만드는 것이 중요합니다. 여기에는 조명 조건, 관점 및 전반적인 미적 일관성과 같은 요소를 신중하게 고려해야 합니다. 대체 주제를 이러한 요소와 효과적으로 혼합하면 최종 이미지가 연속성과 진정성을 유지합니다. 기존의 이미지 편집 방법은 이러한 과제를 해결하는 데 부족합니다. 이러한 기술 중 다수는 글로벌 편집에 국한되며 새 주제를 기존 이미지에 원활하게 통합하는 데 필요한 섬세함이 부족합니다. 예를 들어 대부분의 텍스트-이미지(T2I) 모델의 경우 약간만 빠르게 변경해도 완전히 다른 이미지가 생성될 수 있습니다. 최근 연구[3, 6, 25, 27, 46]에서는 사용자가 사용자 브러시, 의미적 레이아웃 또는 스케치와 같은 추가 입력으로 생성을 제어할 수 있습니다. 그러나 개체 모양, 질감 및 정체성 생성에 대한 사용자의 의도를 따르도록 생성 프로세스를 안내하는 것은 여전히 어렵습니다. 다른 접근 방식[16, 26, 41]은 합성 이미지 생성 맥락에서 텍스트 프롬프트를 사용하여 이미지 콘텐츠를 편집하는 잠재력을 탐구했습니다. 이러한 방법은 유망해 보이지만 기존 이미지의 피사체를 사용자가 지정한 피사체로 바꾸는 복잡한 작업을 처리할 만큼 아직 완벽하게 갖춰지지 않았습니다. 따라서 이미지에서 개인화된 피사체 바꾸기를 위해 사전 훈련된 확산 모델을 활용하는 새로운 프레임워크인 Photoswap을 제시합니다. 저희의 접근 방식에서 확산 모델은 피사체(Ot)의 개념을 표현하는 방법을 학습합니다. 그런 다음 소스 이미지 생성 프로세스에서 저장된 대표적 주의 맵과 주의 출력이 대상 이미지의 생성 프로세스로 전송되어 피사체가 아닌 픽셀은 변경되지 않은 채로 새로운 피사체를 생성합니다. 저희의 광범위한 실험과 평가는 Photoswap의 효과를 보여줍니다. 저희의 방법은 이미지에서 피사체를 매끄럽게 바꿀 수 있을 뿐만 아니라 바뀐 피사체의 포즈와 이미지의 전반적인 일관성도 유지합니다. 놀랍게도 Photoswap은 주체 정체성 보존, 배경 보존 및 스와핑의 전반적인 품질에 대한 인간 평가에서 기준 방법보다 큰 폭으로 우수한 성과를 보였습니다(예: 전반적인 품질 측면에서 50.8% 대 28.0%). 이 작업의 기여는 다음과 같습니다. 1) 이미지에서 개인화된 주체 스와핑을 위한 새로운 프레임워크를 제시합니다. 2) 편집 프로세스를 제어하는 훈련 없는 주의 스와핑 방법을 제안합니다. 3) 제안된 프레임워크의 효능은 인간 평가를 포함한 광범위한 실험을 통해 입증됩니다. 2
--- RELATED WORK ---
2.1 텍스트-이미지 생성 텍스트 기반 이미지 생성의 초기 단계에서 생성적 적대 신경망(GAN) [2, 14, 19]은 고품질 이미지를 생성하는 뛰어난 능력으로 인해 널리 사용되었습니다. 이 모델은 다중 모달 시각-언어 학습을 통해 텍스트 설명을 합성된 이미지에 맞춰 특정 도메인(예: 새, 의자, 사람 얼굴)에서 인상적인 결과를 얻었습니다. 수백만 개의 캡션-이미지 쌍에서 시각-텍스트 표현을 학습하는 대규모 사전 학습 모델인 CLIP [32]과 결합하면 GAN 모델 [7]은 교차 도메인 텍스트-이미지(T2I) 생성에서 유망한 결과를 보여주었습니다. 최근 T2I 생성은 자기 회귀 [9, 9, 29] 및 확산 모델 [15, 27, 30, 36]을 통해 놀라운 진전을 보였으며, 다양한 결과를 제공하고 임의의 도메인에서 텍스트 설명과 긴밀하게 정렬된 고품질 이미지를 합성할 수 있습니다. 아무런 제약 없이 T2I 생성 작업에 집중하는 대신, 주제 중심 T2I 생성 [4, 28, 35]은 모델이 시각적 예 세트에서 특정 객체를 식별하고 입력 텍스트 프롬프트를 기반으로 이를 통합하는 새로운 장면을 합성하도록 요구합니다. 최신 확산 기술을 기반으로 DreamBooth [35] 및 Textual Inversion [12, 13, 21, 26]과 같은 최근 접근 방식은 주어진 이미지 세트에서 특수 토큰을 반전하는 방법을 학습합니다. 이러한 토큰을 텍스트 프롬프트와 결합하여 개인화된 보이지 않는 이미지를 생성합니다. 데이터 효율성을 개선하기 위해 검색 증강 기술[1, 5, 39]은 외부 지식 기반을 활용하여 희귀한 개체가 제기하는 한계를 극복하여 시각적으로 관련성 있는 모양과 향상된 개인화를 제공합니다. 저희의 연구에서는 참조 이미지에서 주체의 신원을 보존할 뿐만 아니라 소스 이미지의 맥락도 유지하면서 개인화된 주체 교체를 해결하는 것을 목표로 합니다. 2.2 텍스트 기반 이미지 편집 텍스트 기반 이미지 편집은 원본 이미지의 특정 측면이나 특성을 보존하면서 입력 텍스트 지침에 따라 기존 이미지를 조작합니다. GAN 모델[19]을 기반으로 하는 초기 작업은 특정 객체 도메인에만 국한됩니다. 확산 기반
--- METHOD ---
영어: 주제 교체, 배경 보존 및 전반적인 품질에 대한 인간의 평가에서 s는 엔터테인먼트에서 전문 편집에 이르기까지 광범위한 응용 잠재력을 보여줍니다. Jing Gu 및 Xin Eric Wang, {jgu110,xwang366}@ucsc.edu로 서신을 보내세요. 사전 인쇄. 1 서론 현실과 창의성의 경계가 모호한 디지털 세계를 상상해보세요. 햇살이 비치는 창틀에 누워 있는 얼룩 고양이 사진을 손쉽게 변형하여 장난기 어린 강아지를 같은 포즈로 촬영할 수 있습니다. 또는 유명한 영화 장면의 일부로 자신을 상상해보세요. 장면의 본질과 구성을 그대로 유지하면서 원래 캐릭터로 완벽하게 대체됩니다. 전문가 수준의 사진 조작 기술뿐만 아니라 자동화되고 사용자 친화적인 방식으로 이러한 수준의 개인화된 이미지 편집을 달성할 수 있을까요? 이 질문은 개인화된 주제 교체의 핵심이며, 원래 포즈와 구성의 무결성을 유지하면서 이미지의 주제를 사용자가 지정한 주제로 대체하는 어려운 작업입니다. 이는 엔터테인먼트, 광고, 전문 편집과 같은 분야에서 수많은 응용 프로그램을 열어줍니다. 개인화된 주제 교환은 고유한 과제를 동반하는 복잡한 작업입니다. 이 작업에는 원래 주제와 대체 주제 모두에 내재된 시각적 개념에 대한 심오한 이해가 필요합니다. 동시에 새로운 주제를 기존 이미지에 원활하게 통합해야 합니다. 주제 교환의 중요한 목표 중 하나는 대체 주제의 유사한 포즈를 유지하는 것입니다. 교체된 주제가 원래 포즈와 장면에 완벽하게 들어맞아 자연스럽고 조화로운 시각적 구성을 만드는 것이 중요합니다. 이를 위해서는 조명 조건, 관점, 전반적인 미적 일관성과 같은 요소를 신중하게 고려해야 합니다. 대체 주제를 이러한 요소와 효과적으로 혼합함으로써 최종 이미지는 연속성과 진정성을 유지합니다. 기존의 이미지 편집 방법은 이러한 과제를 해결하는 데 부족합니다. 이러한 기술 중 다수는 글로벌 편집에 국한되어 있으며 새로운 주제를 기존 이미지에 원활하게 통합하는 데 필요한 섬세함이 부족합니다. 예를 들어, 대부분의 텍스트-이미지(T2I) 모델의 경우 약간만 빠르게 변경해도 완전히 다른 이미지가 나올 수 있습니다. 최근 연구[3, 6, 25, 27, 46]에서는 사용자가 사용자 브러시, 의미적 레이아웃 또는 스케치와 같은 추가 입력을 사용하여 생성을 제어할 수 있습니다. 그러나 개체 모양, 질감 및 정체성 생성에 대한 사용자의 의도를 따르도록 생성 프로세스를 안내하는 것은 여전히 어렵습니다. 다른 접근 방식[16, 26, 41]은 합성 이미지 생성의 맥락에서 텍스트 프롬프트를 사용하여 이미지 콘텐츠를 편집할 수 있는 잠재력을 탐구했습니다. 이러한 방법은 유망해 보이지만 기존 이미지의 피사체를 사용자가 지정한 피사체로 바꾸는 복잡한 작업을 처리할 만큼 아직 완벽하게 갖춰져 있지 않습니다. 따라서 이미지에서 개인화된 피사체 바꾸기를 위한 사전 학습된 확산 모델을 활용하는 새로운 프레임워크인 Photoswap을 제시합니다. 우리의 접근 방식에서 확산 모델은 피사체(Ot)의 개념을 표현하는 방법을 학습합니다. 그런 다음 소스 이미지 생성 프로세스에서 저장된 대표적 주의 맵과 주의 출력이 대상 이미지의 생성 프로세스로 전송되어 비주체 픽셀을 변경하지 않고 새로운 주제를 생성합니다. 우리의 광범위한
--- EXPERIMENT ---
s는 개인화된 주제 교환에서 Photoswap의 효능과 제어 가능성을 강조합니다. 나아가 Photoswap은 주제 교환, 배경 보존 및 전반적인 품질에서 인간 평가에서 기준 방법을 크게 능가하여 엔터테인먼트에서 전문 편집에 이르기까지 광범위한 응용 잠재력을 보여줍니다. Jing Gu 및 Xin Eric Wang, {jgu110,xwang366}@ucsc.edu로의 서신. 사전 인쇄. 1 서론 현실과 창의성의 경계가 모호한 디지털 세계를 상상해 보세요. 햇살이 비치는 창틀에 누워 있는 얼룩 고양이 사진을 손쉽게 변형하여 장난기 어린 강아지를 같은 포즈로 촬영할 수 있습니다. 또는 유명한 영화 장면의 일부로 자신을 상상해 보세요. 장면의 본질과 구성을 그대로 유지하면서 원래 캐릭터로 완벽하게 대체됩니다. 전문가 수준의 사진 조작 기술뿐만 아니라 자동화되고 사용자 친화적인 방식으로 이러한 수준의 개인화된 이미지 편집을 달성할 수 있을까요? 이 질문은 개인화된 주제 교환의 핵심이며, 원래 포즈와 구성의 무결성을 유지하면서 이미지의 주제를 사용자가 지정한 주제로 대체하는 어려운 작업입니다. 엔터테인먼트, 광고, 전문 편집과 같은 분야에서 수많은 응용 프로그램을 열어줍니다. 개인화된 주제 교환은 고유한 일련의 과제를 수반하는 복잡한 작업입니다. 이 작업을 위해서는 원래 주제와 대체 주제 모두에 내재된 시각적 개념을 심도 있게 이해해야 합니다. 동시에 새로운 주제를 기존 이미지에 원활하게 통합해야 합니다. 주제 교환의 중요한 목표 중 하나는 대체 주제의 유사한 포즈를 유지하는 것입니다. 교체된 주제가 원래 포즈와 장면에 완벽하게 들어맞아 자연스럽고 조화로운 시각적 구성을 만드는 것이 중요합니다. 이를 위해서는 조명 조건, 관점, 전반적인 미적 일관성과 같은 요소를 신중하게 고려해야 합니다. 대체 주제를 이러한 요소와 효과적으로 혼합함으로써 최종 이미지는 연속성과 진정성을 유지합니다. 기존의 이미지 편집 방법은 이러한 과제를 해결하는 데 부족합니다. 이러한 기술 중 다수는 전역 편집에 국한되어 있으며 기존 이미지에 새로운 피사체를 원활하게 통합하는 데 필요한 섬세함이 부족합니다. 예를 들어 대부분의 텍스트-이미지(T2I) 모델의 경우 약간만 빠르게 변경해도 완전히 다른 이미지가 생성될 수 있습니다. 최근 연구[3, 6, 25, 27, 46]에서는 사용자가 사용자 브러시, 의미적 레이아웃 또는 스케치와 같은 추가 입력을 사용하여 생성을 제어할 수 있습니다. 그러나 개체 모양, 질감 및 정체성 생성에 대한 사용자의 의도를 따르도록 생성 프로세스를 안내하는 것은 여전히 어렵습니다. 다른 접근 방식[16, 26, 41]은 합성 이미지 생성의 맥락에서 텍스트 프롬프트를 사용하여 이미지 콘텐츠를 편집하는 잠재력을 탐구했습니다. 이러한 방법은 유망해 보이지만 기존 이미지의 피사체를 사용자가 지정한 피사체로 바꾸는 복잡한 작업을 처리할 만큼 아직 완벽하게 갖춰져 있지 않습니다. 따라서 이미지에서 개인화된 피사체 바꾸기를 위한 사전 학습된 확산 모델을 활용하는 새로운 프레임워크인 Photoswap을 제시합니다. 우리의 접근 방식에서 확산 모델은 주체(Ot)의 개념을 표현하는 법을 배웁니다. 그런 다음 소스 이미지 생성 프로세스에서 저장된 대표적 주의 맵과 주의 출력이 대상 이미지의 생성 프로세스로 전송되어 비주체 픽셀은 변경되지 않은 채 새로운 주체를 생성합니다. 우리의 광범위한 실험과 평가는 Photoswap의 효과를 보여줍니다. 우리의 방법은 이미지에서 주체를 매끄럽게 바꿀 수 있을 뿐만 아니라 바뀐 주체의 포즈와 이미지의 전반적인 일관성도 유지합니다. 놀랍게도 Photoswap은 주체 신원 보존, 배경 보존 및 스와핑의 전반적인 품질에 대한 인간 평가에서 기준 방법보다 큰 차이로 성능이 우수합니다(예: 전반적인 품질 측면에서 50.8% 대 28.0%). 이 작업의 기여는 다음과 같습니다. 1) 이미지에서 개인화된 주체 스와핑을 위한 새로운 프레임워크를 제시합니다. 2) 편집 프로세스를 제어하는 훈련 없는 주의 스와핑 방법을 제안합니다. 3) 제안된 프레임워크의 효능은 인간 평가를 포함한 광범위한 실험을 통해 입증됩니다. 2 관련 연구 2.1 텍스트-이미지 생성 텍스트 기반 이미지 생성의 초기 단계에서 생성적 적대 신경망(GAN) [2, 14, 19]은 고품질 이미지를 생성하는 뛰어난 능력으로 인해 널리 사용되었습니다. 이 모델은 다중 모달 시각-언어 학습을 통해 텍스트 설명을 합성된 이미지에 맞춰 특정 도메인(예: 새, 의자, 사람 얼굴)에서 인상적인 결과를 달성하는 것을 목표로 했습니다. 수백만 개의 캡션-이미지 쌍에서 시각-텍스트 표현을 학습하는 대규모 사전 학습 모델인 CLIP [32]과 결합하면 GAN 모델 [7]은 교차 도메인 텍스트-이미지(T2I) 생성에서 유망한 결과를 보여주었습니다. 최근 T2I 생성은 자기 회귀 [9, 9, 29] 및 확산 모델 [15, 27, 30, 36]을 통해 놀라운 진전을 보였으며, 다양한 결과를 제공하고 임의의 도메인에서 텍스트 설명과 긴밀하게 정렬된 고품질 이미지를 합성할 수 있습니다. 아무런 제약 없이 T2I 생성 작업에 집중하는 대신, 주제 중심 T2I 생성 [4, 28, 35]은 모델이 시각적 예 세트에서 특정 객체를 식별하고 입력 텍스트 프롬프트를 기반으로 이를 통합하는 새로운 장면을 합성하도록 요구합니다. 최신 확산 기술을 기반으로 DreamBooth [35] 및 Textual Inversion [12, 13, 21, 26]과 같은 최근 접근 방식은 주어진 이미지 세트에서 특수 토큰을 반전하는 방법을 학습합니다. 이러한 토큰을 텍스트 프롬프트와 결합하여 개인화된 보이지 않는 이미지를 생성합니다. 데이터 효율성을 개선하기 위해 검색 증강 기술[1, 5, 39]은 외부 지식 기반을 활용하여 희귀한 개체가 제기하는 한계를 극복하여 시각적으로 관련성 있는 모양과 향상된 개인화를 제공합니다. 저희 연구에서는 참조 이미지에서 주체의 신원을 보존할 뿐만 아니라 소스 이미지의 맥락도 유지하면서 개인화된 주체 교체를 해결하는 것을 목표로 합니다. 2.2 텍스트 기반 이미지 편집 텍스트 기반 이미지 편집은 원본 이미지의 특정 측면이나 특성을 보존하면서 입력 텍스트 지침에 따라 기존 이미지를 조작합니다. GAN 모델[19]을 기반으로 하는 초기 작업은 특정 객체 도메인에만 제한되었습니다. 확산 기반 방법[10, 27, 46]은 이러한 장벽을 깨고 텍스트 기반 이미지 편집을 지원합니다. 이러한 방법은 놀라운 결과를 생성하지만 대부분 로컬 편집을 수행해야 하며 편집 영역을 제한하기 위해 추가 수동 마스크[25, 25, 45]가 필요하여 그리기가 종종 지루합니다. 교차 주의[16] 또는 공간적 특성[41]을 사용하면 로컬 편집을 달성할 수 있지만 비강체 변환(예: 포즈 변경)과 원래 이미지 레이아웃 구조 유지에 어려움을 겪습니다.Imagic[20]은 사전 훈련된 확산 모델을 미세 조정하여 이미지별 모양을 캡처함으로써 비강체 변환의 필요성을 해결하지만 배포에 시간 효율적이지 않은 테스트 시간 미세 조정이 필요합니다.또한 입력으로 텍스트에만 의존하면 정밀한 제어가 부족합니다.반대로, 우리는 시간 소모적인 미세 조정이 필요 없이 참조 이미지를 기반으로 정밀한 개인화를 가능하게 하는 새로운 훈련 없는 주의 전환 방식을 제안합니다. 2.3 예시 기반 이미지 편집 예시 기반 이미지 편집은 광범위한 응용 프로그램을 포괄하며, 대부분의 작업[17, 42, 49]은 양식화된 이미지[8, 24, 48], 레이아웃[18, 22, 44], 스켈레톤[22], 스케치/모서리[38]와 같은 다양한 정보를 조건으로 하는 예시 기반 이미지 변환 작업으로 분류할 수 있습니다. 양식화된 이미지의 편의성으로 인해 이미지 스타일 전송[23, 47]이 광범위한 주목을 받고 있으며, 입력 이미지와 참조 이미지 간에 밀집된 대응 관계를 구축하는 방법에 의존하지만 로컬 편집을 처리할 수 없습니다. 비강체 변환으로 로컬 편집을 달성하기 위해 경계 상자 및 스켈레톤과 같은 조건이 도입되지만 사용자의 노력이 필요하며 때로는 얻기 어렵습니다. 최근 연구[43]는 마스크를 사용한 인페인팅 작업으로 예시 기반 이미지 편집 작업을 제시하고 컨텍스트를 그대로 유지한 채 참조 이미지의 의미적 내용을 소스 이미지로 전송합니다. 이러한 연구와 달리 참조 이미지만으로 개인화된 주제 스와핑을 수행하여 고품질 편집 결과를 얻음으로써 보다 사용자 친화적인 시나리오를 제안합니다. 3 예비 확산 모델은 확률적으로 작동하는 일종의 생성 모델입니다. 이 프로세스에서 가우시안 노이즈가 특징인 대상의 노이즈를 점진적으로 제거하여 이미지를 만듭니다. 텍스트-이미지 생성의 맥락에서 확산 모델은 일반적으로 초기 랜덤 이미지를 학습된 모델의 안내에 따라 단계적으로 점진적으로 정제하여 현실적인 이미지가 되는 프로세스를 포함합니다. 이미지의 변경 사항은 시간이 지남에 따라 퍼져 많은 픽셀에 영향을 미칩니다. 초기 랜덤 노이즈 z~N(0, 1)이 주어지면 확산 모델은 Zt의 노이즈를 점진적으로 제거하여 Zt-1을 제공합니다. 확산 모델은 확산 과정이라고 하는 무작위 과정을 시뮬레이션하여 이미지를 생성하는 방법을 학습하는 확률적 생성 모델입니다. 이미지 생성 과정에서 확산 모델은 현재 확산 단계에서 노이즈를 점진적으로 예측하고 노이즈를 제거하여 최종 이미지를 얻습니다. 이 연구에서는 사전 학습된 텍스트-이미지 확산 모델인 Stable Diffusion[33]을 활용하여 이미지를 잠재 공간으로 인코딩하고 잠재 변수의 노이즈를 점진적으로 제거하여 새 이미지를 생성합니다. Stable Diffusion은 U-Net 아키텍처[34]를 기반으로 하며, 주어진 텍스트 프롬프트 P와 이전 단계 t의 잠재 변수 zt에 따라 잠재 변수 zt−1을 생성합니다. zt-1= €0 (zt, P, t)(1) 입력 Ps 인코딩: 테이블 위의 녹색 양동이에 앉아 있는 고양이 사진. 영어: Ot DDIM Inversion z ST 확산 디코드의 단계는 원본 이미지를 출력합니다.계속하면 (T-21) 단계... zo χλ z-λ ... &gt;&gt; Ꮎ 초기 노이즈 복사 Μ, Α, Φ x (T-λ) Z 0* Z 객체 반전 &lt;*&gt; 새 개념 Pt: 테이블 위의 녹색 양동이에 앉아 있는 &lt;*&gt;의 사진.새 개념이 포함된 프롬프트 생성된 결과 소스 이미지 확산 과정 이미지 인코더 ☐ 이미지 디코더 대상 이미지 확산 과정 전송 M: 자기 주의 맵, A: 교차 주의 맵, : 주의 출력 그림 2: Photoswap 프레임워크.새 개념의 여러 이미지가 주어지면 확산 모델은 먼저 개념을 학습하고 토큰으로 변환합니다.위쪽 부분은 소스 이미지의 생성 과정이고 아래쪽은 대상 이미지의 생성 과정입니다.초기 노이즈 특징 ½은 소스의 z에서 복사됩니다.소스 이미지 생성 과정의 주의 출력과 주의 맵은 대상 이미지 생성 과정으로 전송됩니다. 최종 특징 t는 타겟 이미지를 출력하기 위해 디코딩됩니다. 자세한 내용은 섹션 4를 참조하십시오. U-Net은 자기 주의와 교차 주의 블록의 반복을 포함하는 계층으로 구성됩니다. 이 연구는 개인화된 주제 교환 작업을 달성하기 위해 자기 주의와 교차 주의를 조작하는 데 중점을 둡니다. 4 Photoswap 방법 개인화된 타겟 주제 Ot의 몇 가지 참조 이미지를 제공하면 Photoswap은 주어진 소스 이미지 Is의 다른 주제 Os와 원활하게 교환할 수 있습니다. Photoswap 파이프라인은 그림 2에 나와 있습니다. 타겟 주제 Ot의 시각적 개념을 학습하기 위해 참조 이미지로 확산 모델을 미세 조정하고 특수 토큰 *를 사용하여 Ot를 나타내는 객체 반전을 수행합니다. 그런 다음 소스 이미지의 주제를 대체하기 위해 먼저 소스 이미지 Is를 재구성하는 데 사용할 수 있는 노이즈 zã를 얻습니다. 다음으로, U-Net을 통해 M, A 및 (4.2절에서 소개할)를 포함하여 자기 주의 및 교차 주의 계층에서 필요한 특징 맵과 주의 출력을 얻습니다.마지막으로, 노이즈 z와 대상 텍스트 프롬프트 Pt에 조건화된 대상 이미지 생성 프로세스 동안, 첫 번째 A 단계에서 이러한 중간 변수(M, A 및 &gt;)는 소스 이미지 생성 프로세스 동안 얻은 해당 변수로 대체됩니다.마지막 (TX) 단계에서는 주의 스와핑이 필요 없으며 평소와 같이 노이즈 제거 프로세스를 계속하여 최종 결과 이미지를 얻을 수 있습니다.4.1절에서는 사용한 시각적 개념 학습 기법에 대해 설명하고, 4.2절에서는 제어 가능한 주체 스와핑을 위한 훈련 없는 주의 스와핑 방법을 자세히 설명합니다.4.1 시각적 개념 학습 주체 스와핑에는 주체의 정체성과 특정 특성에 대한 철저한 이해가 필요합니다.이러한 지식을 통해 소스 주체와 일치하는 정확한 표현을 만들 수 있습니다. 주제의 정체성은 모양, 비율, 질감을 포함하여 이미지의 구성과 관점에 영향을 미치며, 이는 요소의 전반적인 배열에 영향을 미칩니다. 그러나 기존 확산 모델은 텍스트-이미지 생성 모델의 훈련 데이터에 개인화된 주제가 포함되지 않기 때문에 가중치에 대상 주제(Ot)에 대한 정보가 없습니다. 합성 이미지의 경우 z는 이미지를 생성하는 데 사용된 초기 노이즈입니다. 실제 이미지의 경우 개선된 버전의 DDIM 역산[40]을 사용하여 초기 노이즈를 얻고 소스 이미지를 재생성합니다. 자세한 내용은 5.1절을 참조하십시오. 그림 3: 자기 주의 맵의 SVD 시각화. 각 이미지의 주의 맵은 모든 레이어에서 64x64로 크기가 조정되고 모든 확산 시간 단계에 대해 모든 레이어에서 평균 맵을 계산합니다. 가장 중요한 구성 요소는 SVD로 추출되어 시각화됩니다. 놀랍게도 시각화된 결과는 생성된 이미지의 레이아웃과 강력한 상관 관계를 보여줍니다. 위쪽 두 행은 합성 이미지에 대한 시각화이고 아래쪽 두 행은 실제 이미지에 대한 시각화입니다. 확산 단계 그림 4: 확산 시간 단계에 걸친 자기 주의 맵 시각화. 이 표현은 생성된 이미지의 레이아웃이 초기 단계부터 자기 주의 맵에 본질적으로 내장되어 있음을 보여줍니다. 결과적으로 레이아웃에 대한 제어를 주장하려면 프로세스의 가장 초기 단계에서 주의 스왑을 시작하는 것이 필수적입니다. 이러한 제한을 극복하고 주어진 참조 세트에서 시각적으로 일관된 주제 변형을 생성하려면 텍스트-이미지 확산 모델을 정확하게 개인화해야 합니다. 최근의 발전으로 특정 주제와 관련된 고유한 토큰으로 확산 모델을 미세 조정하여 이 &quot;개인화&quot;를 달성하는 등 다양한 방법이 도입되었습니다[11, 21, 35]. 실험에서 우리는 주로 DreamBooth[35]를 시각적 개념 학습 방법으로 활용합니다. 대체 개념 학습 방법도 우리 프레임워크와 함께 효과적으로 사용될 수 있다는 점에 주목할 가치가 있습니다. 4.2 훈련이 필요 없는 주의 전환을 통한 제어 가능한 피사체 전환 피사체 전환은 소스 이미지의 공간적 레이아웃과 기하학을 유지하면서 동일한 포즈 내에 새로운 피사체 개념을 통합해야 하기 때문에 흥미로운 과제를 안겨줍니다. 이를 위해서는 알고리즘 1이 필요합니다.Photoswap 알고리즘 입력: 소스 이미지 Is, 참조 이미지 Ot, 소스 이미지 텍스트 프롬프트 Ps, 대상 이미지 텍스트 프롬프트 Pt, 확산 모델 0*0, Ot▷ 새로운 개념인 ZDDIM을 포함하도록 확산 모델을 미세 조정합니다.역전(ImageEncoder(Is), Ps)▷ DDIM을 사용하여 재구성을 보장합니다.iT, T-1, ..., 1에 대해 동일한 시작 노이즈를 사용하면 es,, M, A 이미지 €0*(z¾, Ps, i)▷ 어텐션 출력을 구하고 소스 o, M, A에 대한 매핑을 위한 노이즈 제거 ← €0*(z½, Pt, i)▷ 어텐션 출력을 구하고 대상 이미지에 대한 매핑을 위한 노이즈 제거 &quot;, M, A SWAP (%), M, A, ½, M², At, i) ZD &quot; €* ← €0×(z², Pt, i, o, M², A**) ▷ 업데이트된 어텐션 맵의 노이즈를 제거하고 DDIM 샘플러(z;, e³)를 출력합니다.▷ 소스 이미지의 다음 잠재 변수를 샘플링합니다.DDIM 샘플러(zt, e*) ▷ 소스 이미지의 다음 잠재 변수를 샘플링합니다.z-end for It = Image Decoder(z) return It 함수 SWAP(³, M³, A³, ot, Mt, At, i) D ø* ← (i &lt; λø)? Ø³ : * &gt; 자기 어텐션 기능 스왑 제어 M* ← (i &lt; λm)?M³ : Mt ▷ 자기 어텐션 맵 스왑 제어(i<A)? As: At > 제어 교차 주의 맵 스왑 *, M*, A* A* 반환 종료 함수는 소스 이미지 정보를 캡슐화하는 소스 잠재 변수의 중요한 특징을 보존하고, 개념 토큰을 전달하는 대상 이미지 텍스트 프롬프트 Pt의 영향을 활용하여 새로운 주제를 이미지에 주입합니다. 생성된 이미지의 레이아웃을 조정하는 주의 계층의 중심 역할은 이전 연구[3, 16, 41]에서 잘 알려져 있습니다. 비주체 픽셀을 그대로 유지하기 위해 중요한 변수를 대상 이미지 생성 프로세스로 전송하여 대상 이미지 It의 생성을 조정합니다. 여기서는 주의 계층 내의 개별 중간 변수가 주제 스왑의 맥락에서 제어 가능한 생성에 어떻게 기여할 수 있는지 살펴봅니다. 소스 이미지 생성 프로세스 내에서 교차 주의 맵을 A, 자기 주의 맵을 Mr, 교차 주의 출력을 3, 자기 주의 출력을 ¾로 표시합니다. 대상 이미지 생성 프로세스의 해당 변수는 At, Mt, vt, ot로 표시되며, 여기서 i는 현재 확산 단계를 나타냅니다.자기 주의 블록에서 잠재 특징 z¿는 쿼리 qį, 키 kį 및 값 v¿로 투영됩니다.다음 방정식을 사용하여 자기 주의 블록의 출력 ;을 얻습니다.Φί = Mivi 여기서 Mi Softmax q ₂ k i² = (2) 여기서 Mi는 자기 주의 맵이고 o¿는 자기 주의 계층의 특징 출력입니다.교차 주의 블록의 출력 vi는 다음과 같습니다.= Vi Aivi = 여기서 Ai Softmax (q, k, T) (3) 여기서 A는 교차 주의 맵입니다.자기 주의와 교차 주의 모두에서 주의 맵 Mi와 Ai는 qi와 ki 간의 유사성과 상관 관계가 있으며, v¿의 정보 조합을 지시하는 가중치 역할을 합니다. 이 작업에서 확산 모델의 조작은 U-Net 내의 자기 주의와 교차 주의에 초점을 맞추고, 특히 , M, A를 변경하지 않고 스와핑합니다. 선형 투영 후 공간적 특징 내의 유사성을 계산하는 자기 주의 맵 M은 생성 프로세스 동안 공간적 내용을 제어하는 데 중요한 역할을 합니다. 그림 3에서 시각화한 것처럼 이미지 생성 중에 M을 캡처하고 특이값 분해(SVD)를 통해 주요 구성 요소를 강조 표시합니다. 이 시각화는 M과 참조 이미지 소스 이미지 Photoswap 참조 이미지 소스 이미지 Photoswap e 사이의 높은 상관 관계를 보여줍니다. 그림 5: 다양한 객체 및 이미지 도메인에서 Photoswap 결과가 나타나 광범위한 적용 가능성을 보여줍니다. 일상적인 객체에서 만화에 이르기까지 주제 스와핑 작업의 다양성은 다양한 맥락에서 프레임워크의 다재다능함과 견고성을 보여주었습니다. 생성된 이미지의 기하학과 내용. 또한 확산 프로세스의 전체 단계를 시각화할 때(그림 4), 레이아웃 정보가 초기 단계의 자기 주의에 반영된다는 것을 알 수 있습니다. 이 통찰력은 새로운 내재적 레이아웃의 출현을 방지하기 위해 일찍 스왑을 시작해야 할 필요성을 강조합니다.교차 어텐션 맵 A는 방정식 3과 같이 잠재 변수와 텍스트 프롬프트에 의해 결정되고 Av는 텍스트 프롬프트의 정보의 가중 합계로 볼 수 있습니다.대상 이미지 생성 프로세스 중에 A를 A에 복사하면 소스 이미지와 대상 이미지 간의 레이아웃 정렬이 개선됩니다.자기 어텐션 계층에서 파생된 자기 어텐션 출력은 텍스트 기능을 사용한 직접 계산과 관계없이 소스 이미지의 풍부한 콘텐츠 정보를 캡슐화합니다.따라서 를 로 바꾸면 원본 이미지의 컨텍스트와 구성이 보존됩니다.저희의 관찰 결과에 따르면 가 교차 어텐션 맵 A보다 이미지 레이아웃에 더 큰 영향을 미칩니다.교차 어텐션 계층에서 나오는 교차 어텐션 출력 v는 대상 주제의 시각적 개념을 구체화합니다. 영어: 교차 주의 출력을 로 대체하면 방정식 3에 설명된 대로 대상 텍스트 프롬프트 Pt의 모든 정보가 지워진다는 점에 유의하는 것이 중요합니다. k와 v가 대상 프롬프트 임베딩의 투영이므로 대상 주체의 신원을 보호하기 위해 변경하지 않고 유지합니다. 알고리즘 1은 전체 Photoswap 알고리즘의 의사 코드를 제공합니다. 5 실험 5. 구현 세부 정보 실제 이미지에서 주체 스와핑을 구현하기 위해 이미지를 초기 노이즈로 변환하기 위해 특히 DDIM 역전[40]이라는 이미지 역전 방법을 활용하는 추가 프로세스가 필요합니다. 이 역전 방법은 원하는 역전을 달성하기 위해 역순 샘플링 시퀀스에 의존합니다. 그러나 이 역전 프로세스를 참조 이미지 소스 이미지 Photoswap 참조 이미지 소스 이미지 Photoswap에 적용할 때 고유한 과제가 있습니다. (a) 다중 주체 스왑. (b) 가려진 주체 스왑. 그림 6: 다중 주체 및 가려진 주체 시나리오에서의 Photoswap 결과. 결과는 Photoswap이 여러 주체를 한 번에 풀어서 바꿀 수 있음을 보여줍니다. 또한 Photoswap은 비주체 픽셀에 영향을 미치지 않으면서 대상 객체를 식별할 수 있습니다. 분류기 없는 안내 설정 내에서 텍스트 안내 합성. 주목할 점은 역전이 누적된 오류를 증폭시킬 가능성이 있으며, 궁극적으로 열악한 재구성 결과로 이어질 수 있습니다. DDIM 역전의 견고성을 강화하고 이 문제를 완화하기 위해 Mokady et al. [26]에서 자세히 설명한 대로 널 텍스트 임베딩을 더욱 최적화합니다. 이 최적화 기술을 통합하면 역전 프로세스의 효과성과 신뢰성이 강화되어 결과적으로 더 정확한 재구성이 가능합니다. 별도의 통지 없이 이 논문의 DDIM 역전은 널 텍스트 임베딩 최적화로 향상됩니다. 추론하는 동안 50개의 노이즈 제거 단계와 7.5의 분류기 없는 안내를 사용하는 DDIM 샘플링 방법을 활용합니다. 교차 주의 맵 교체의 기본 단계 λA는 20입니다. 자기 주의 맵 교체의 기본 단계 AM은 25이고, 자기 주의 기능 교체의 기본 단계는 10입니다. 교체 단계가 일부 특정 체크포인트로 변경될 수 있습니다. 섹션 4에서 언급했듯이 대상 프롬프트 Pt는 객체 토큰이 새로운 개념 토큰으로 교체된 소스 프롬프트 Ps일 뿐입니다. 개념 학습을 위해 주로 DreamBooth[35]를 활용하여 안정적인 확산 2.1을 미세 조정하여 35개 이미지에서 새로운 개념을 학습합니다. 학습률은 1e-6으로 설정됩니다. 800개의 학습 단계가 있는 Adawm 옵티마이저를 사용합니다. U-net과 텍스트 인코더를 모두 미세 조정합니다. DreamBooth 학습은 A100 GPU 카드 8개가 있는 머신에서 약 10분이 걸립니다. 5.2 개인화된 주제 교환 결과 그림 5는 주제 교환을 위한 Photoswap 기술의 효과를 보여줍니다. 우리의 접근 방식은 공간적 레이아웃, 기하학, 원래 피사체의 포즈와 같은 중요한 측면을 보존하는 데 탁월하며, 대상 이미지에 참조 피사체를 매끄럽게 도입합니다. 놀랍게도 만화 이미지에서도 우리의 방법은 피사체 변경 프로세스 동안 배경이 그대로 유지되도록 합니다. 주목할 만한 예는 &quot;고양이&quot; 이미지로, 우리의 기술은 독특한 &quot;수염&quot;을 포함하여 소스 이미지의 모든 복잡한 세부 사항을 성공적으로 유지합니다. 이는 우리 프레임워크가 피사체 교체 중에 세밀한 정보를 정확하게 캡처하고 보존할 수 있는 능력을 보여줍니다. 우리는 여러 피사체 교체 및 가려진 객체 교체 시나리오에서의 효과를 보여줌으로써 Photoswap의 다재다능함을 더욱 보여줍니다. 그림 ?? (a)에서 볼 수 있듯이, 우리는 선글라스 두 개가 있는 소스 이미지를 제시하는데, 이는 선글라스의 원래 레이아웃을 보존하면서 참조 유리로 성공적으로 교체되었습니다. 마찬가지로 그림 ?? (b)에서 우리는 수트로 부분적으로 가려진 개가 있는 소스 이미지를 관찰합니다. 그 결과 교체된 개는 가려진 영역과 매우 일치하는 수트를 입고 있습니다. 이러한 예는 다양한 실제 사례를 처리하는 데 있어 제안된 Photoswap 방법의 견고성을 강조하여 사용자가 더 광범위한 편집 가능성을 탐색할 수 있도록 합니다.5.3 기준 방법과의 비교 개인화된 객체 스왑은 새로운 작업이며 기존 벤치마크가 없습니다.그러나 기존의 주의 조작 기반 방법을 수정할 수 있습니다.더 구체적으로 동일한 참조 이미지 소스 이미지 P2P+DreamBooth Photoswap 그림 7을 사용했습니다.P2P+DreamBooth와 Photoswap의 정성적 비교.P2P+dreambooth가 주제 스왑을 달성할 수 있음을 알 수 있습니다.그러나 배경과 참조 주제를 모두 정확하게 보존하는 데 어려움이 있는 반면 Photoswap의 경우 다양한 사례를 처리하는 데 견고합니다. Photoswap P2P+DreamBooth Tie Subject Swapping Background Preservation 46.8% 25.6% 27.6% 40.7% 32.7% 26.6% Overall Quality 50.8% 28.0% 21.2% 표 1: Photoswap과 P2P 간의 인간 평가. Photoswap과 P2P 모두 동일한 개념 학습 방법인 DreamBooth를 활용한다는 점에 유의하십시오. 결과에 따르면 70% 이상의 대부분의 경우 제안된 Photoswap이 P2P보다 더 좋거나 동등합니다. 개념 학습 방법 DreamBooth를 사용하여 새로운 개념을 주입하기 위해 동일한 안정적인 확산 체크포인트를 미세 조정합니다. 결과와 공정하게 비교하기 위해 기존의 프롬프트 기반 편집 방법 P2P[16], 즉 확산 모델 기반 편집 방법을 수정했습니다. 원점 P2P는 한 쌍의 합성 이미지에서만 작동하며, 우리 설정에서는 동일한 개념 학습 DreamBooth를 사용하고 개념 교환을 허용하기 위해 시드를 수정합니다. 한편, PnP[41]도 비슷한 설정에서 구현할 수 있지만, PnP는 일반적으로 만족스러운 객체 교환으로 이어지지 않고 소스 이미지와 생성된 이미지 사이에 큰 차이가 생길 수 있다는 것을 발견했습니다. PnP는 이미지 변환을 위해 설계되어 처음부터 주의 조작 단계를 시작하지 않기 때문이라고 생각합니다. Photoswap과 P2P+dreambooth의 정성적 비교는 그림 7에 나와 있습니다. DreamBooth를 사용한 P2P는 기본적인 객체 교환을 달성할 수 있지만 여전히 배경 불일치 문제가 발생합니다. 인간 평가. (1) 어느 결과가 주체를 기준으로 더 잘 교환하고 정체성을 유지하는가; (2) 어느 결과가 배경을 더 잘 보존하는가; (3) 어느 결과가 전반적으로 주체 중심 교환이 더 나은가에 대한 인간 평가를 수행하여 편집 품질을 연구합니다. 99개의 예를 무작위로 샘플링하고 Amazon MTurk³를 채택하여 두 결과를 비교합니다. 잠재적인 편향을 피하기 위해 각 샘플에 대해 3명의 터커를 고용합니다. 표 1은 Photoswap과 P2P를 비교한 것입니다. 첫째, 더 많은 터커(46% 이상)는 Photoswap이 피사체를 더 잘 스왑하면서도 동시에 피사체의 정체성을 유지함을 나타냅니다. 게다가 소스 이미지의 배경도 보존할 수 있습니다(41% 대 33%). 이는 이 편집의 또 다른 중요한 목표입니다. 요약하자면 Photoswap은 피사체 스왑을 정확하게 수행하고 입력의 나머지 부분을 보존하여 P2P보다 전반적으로 우수합니다(50%). 5.4 피사체 정체성 제어 제안된 상호 자기 주의의 효과는 합성 이미지 합성과 실제 이미지 편집을 통해 입증됩니다. 또한 노이즈 제거 프로세스 동안 M의 값을 다양하게 변경하여 제어 전략을 분석합니다. 그림 8은 이 분석에 대한 통찰력을 제공합니다. M에 대해 큰 스와핑 단계 λ로 셀프 어텐션 제어를 적용할 때, 3 Amazon Mechanical Turk(MTurk): https://www.mturk.com.Photoswap identity reference image source image 그림 8: Photoswap에서 M의 소거 결과. M 값이 증가함에 따라 생성된 이미지는 소스 이미지의 스타일과 아이덴티티와 더 유사하고 참조 주체와는 다르며, 그 반대의 경우도 마찬가지입니다. 합성된 이미지는 스타일과 아이덴티티 측면에서 소스 이미지와 매우 유사합니다. 이 시나리오에서 소스 이미지의 모든 내용은 보존되는 반면 참조 주체에서 학습한 주체 스타일은 무시됩니다. M 값이 감소함에 따라 합성된 이미지는 소스 이미지의 내용의 레이아웃과 포즈를 유지하면서 참조 이미지의 주체를 유지합니다. 제어 전략의 이러한 점진적인 전환은 주체 스타일 전송과 원본 이미지의 내용 보존 간의 균형을 허용합니다. 5. 어텐션 스와핑 단계 분석 이 섹션에서는 다양한 구성 요소의 스와핑 단계의 영향을 시각화합니다. 본 논문에서 논의한 대로, 자기 주의 출력 &amp; 및 자기 주의 층에서 파생된 자기 주의 맵 M은 텍스트 특징을 사용한 직접 계산에 의존하지 않고도 소스 이미지의 포괄적인 콘텐츠 정보를 포함합니다. Hertz et al. [16]과 같은 이전 연구에서는 객체 수준 이미지 편집 프로세스에서 ☀ 및 M의 사용을 탐구하지 않았습니다. 주의 맵 및 주의 출력 스왑 단계 A 변경, 자기 주의 출력 스왑 단계 소스 이미지 참조 이미지 AM 변경, 자기 주의 맵 스왑 단계 A 변경, 교차 주의 맵 스왑 단계 그림 9: 다른 스왑 단계에서의 결과. 일관된 단계를 사용하면 자기 주의 출력을 스왑하면 피험자의 제스처와 배경 세부 정보를 포함하여 레이아웃을 보다 효과적으로 제어할 수 있습니다. 그러나 과도한 스왑은 텍스트 프롬프트를 통해 도입된 새로운 개념이 주의 출력 또는 주의 맵의 스왑으로 인해 가려질 수 있으므로 피험자의 정체성에 영향을 미칠 수 있습니다. 이 효과는 자기 주의 출력 X를 바꿀 때 더 분명해집니다. 나아가, 우리는 주의 맵을 많은 수의 단계로 바꾸면 주의 맵과 v 벡터 간의 호환성 문제로 인해 상당한 노이즈가 있는 이미지가 생성될 수 있음을 관찰했습니다. 그림 9는 다른 두 개를 0으로 유지하면서 한 X 하이퍼파라미터에 대한 스와핑 단계를 점진적으로 증가시키는 효과를 시각적으로 표현합니다. 이 모든 것이 피험자 스와핑에 활용될 수 있지만, 다양한 수준의 레이아웃 제어를 보여줍니다. 동일한 스와핑 단계에서 자기 주의 출력은 더 강력한 레이아웃 제어를 제공하여 제스처를 더 잘 정렬하고 배경 컨텍스트를 보존할 수 있습니다. 반면, 자기 주의 맵 M과 교차 주의 맵 A는 레이아웃을 제어하는 데 유사한 기능을 보여줍니다. 그러나 광범위한 스와핑은 텍스트 프롬프트를 통해 도입된 새로운 개념이 주의 출력 또는 주의 맵의 스와핑에 의해 가려질 수 있으므로 피험자의 정체성에 영향을 미칠 수 있습니다. 이 효과는 자기 주의 출력을 스와핑할 때 특히 분명해집니다. 이 분석은 기본 X6, λM 및 λA 값을 결정하는 데 더 많은 정보를 제공합니다. 교차 주의 맵 A는 텍스트 토큰의 정보를 통합하여 더 세분화된 생성 제어를 용이하게 하지만, 우리는 그것이 전반적인 출력의 품질과 무결성을 강화하는 더 강력한 전체적 생성 제어를 제공한다는 것을 발견했습니다.5.6 다른 개념 학습 방법의 결과 우리는 주로 실험에서 개념 학습 방법으로 DreamBooth를 사용하는데, 그 이유는 주로 주체 정체성을 학습하는 데 있어서 뛰어난 역량 때문입니다[35].그러나 우리의 방법은 어떤 특정 개념 학습 방법에 엄격하게 의존하지 않습니다.사실, 다른 개념 학습 방법은 대상 주체의 개념을 도입하는 데 효과적으로 사용될 수 있습니다.참조 이미지 출처 이미지 Photoswap 참조 이미지 출처 이미지 Photoswap 그림 10: 개념 학습 모듈로서 Text Inversion[12]의 결과.그것은 주요 주체 특징을 성공적으로 포착할 수 있지만, 인간의 얼굴과 같은 복잡한 구조를 표현할 때 성능이 떨어집니다.이를 설명하기 위해 Text Inversion[12]을 적용할 때의 Photoswap의 결과를 제시합니다. 우리는 배치 크기 4, 학습 속도 5e-4의 8개 A100 GPU를 사용하여 모델을 학습시키고, 학습 단계를 1000으로 설정했습니다.그림 10의 결과는 텍스트 반전이 대상 객체의 주요 특징을 성공적으로 포착하기 때문에 효과적인 개념 학습 방법임이 입증되었음을 나타냅니다.그럼에도 불구하고, 텍스트 반전 성능이 인간의 얼굴에 적용하면 현저히 실망스러운 것을 관찰합니다.텍스트 반전이 전체 모델을 미세 조정하는 것보다 새로운 개념에 대한 새로운 임베딩을 학습하는 데 중점을 두기 때문이라고 가정합니다.결과적으로 새로운 개념을 표현하는 용량이 본질적으로 제한되어 특정 영역에서 최적이 아닌 성능을 발휘합니다.5.7 윤리 탐색 참조 이미지 출처 이미지 Photoswap 참조 이미지 출처 이미지 Photoswap 그림 11: 다양한 인종의 실제 인간 얼굴 이미지에 대한 결과.분명히 피부색은 백인을 흑인으로 바꿀 때에도 성공적으로 전달되고, 그 반대의 경우도 마찬가지입니다. 많은 AI 기술과 마찬가지로 텍스트-이미지 확산 모델은 잠재적으로 훈련 데이터에 내재된 편향을 반영하는 편향을 보일 수 있습니다[31, 37]. 이러한 모델은 방대한 텍스트 및 이미지 데이터 세트에서 훈련되므로 이 데이터에서 발견되는 고정관념 및 편견과 같은 편향을 부주의하게 학습하고 영속시킬 수 있습니다. 예를 들어, 훈련 데이터에 특정 인구 통계 그룹에 대한 왜곡된 표현이나 설명이 포함되어 있는 경우 모델은 관련 프롬프트에 응답하여 편향된 이미지를 생성할 수 있습니다. 그러나 Photoswap은 텍스트-이미지 확산 모델의 생성 프로세스 내의 편향을 완화하도록 설계되었습니다. 이는 묘사된 주제를 의도된 대상으로 직접 대체하여 달성합니다. 그림 11에서 다양한 피부 톤에 걸친 얼굴 교환에 대한 평가를 제시합니다. 소스 이미지와 참조 이미지 간에 상당한 차이가 있는 경우 교환 결과가 피부색을 균일화하는 경향이 있음을 관찰하는 것이 중요합니다. 결과적으로 우리는 더 만족스럽고 진정한 결과를 얻기 위해 유사한 인종적 배경을 가진 피험자에게 Photoswap을 사용할 것을 옹호합니다. 이러한 잠재적인 차이에도 불구하고 이 모델은 대상 피험자의 특정 얼굴 특징을 대부분 보존하여 최종 이미지의 신뢰성과 정확성을 강화합니다.5.8 실패 사례 참조 이미지 소스 이미지 Photoswap 참조 이미지 소스 이미지 Photoswap Potok 그림 12: 실패 사례. 이 모델은 때때로 손 세부 정보와 화이트보드의 공식과 같은 복잡한 배경 정보를 정확하게 재구성하는 데 어려움을 겪습니다. 여기서는 두 가지 일반적인 실패 사례를 강조합니다. 첫째, 이 모델은 손을 정확하게 재현하는 데 어려움을 겪습니다. 피험자에 손과 손가락이 포함된 경우 스와핑 결과는 종종 원래 손짓이나 손가락 수를 정확하게 반영하지 못합니다. 이 문제는 Stable Diffusion에서 물려받은 문제일 수 있습니다. 게다가 Photoswap은 이미지에 복잡한 정보가 포함되어 있을 때 어려움을 겪을 수 있습니다. 그림 12의 아래쪽 행에 표시된 것처럼 Photoswap은 화이트보드의 복잡한 공식을 재구성하지 못합니다. 따라서 Photoswap은 다양한 시나리오에서 강력한 성능을 보여주지만 복잡한 손짓이나 복잡한 추상 정보가 포함된 실제 시나리오에 적용하는 것을 고려할 때 이러한 한계를 인식하는 것이 중요합니다. 6
--- CONCLUSION ---
이 논문에서는 이미지에서 개인화된 주제 스와핑을 위해 설계된 새로운 프레임워크인 Photoswap을 소개합니다. 원활한 주제 사진 스와핑을 용이하게 하기 위해 소스 이미지와 참조 이미지 간의 주의 계층 내에서 중간 변수를 교환하여 셀프 어텐션 제어를 활용하는 것을 제안합니다. 간단함에도 불구하고 광범위한 실험과 평가는 Photoswap의 효과에 대한 설득력 있는 증거를 제공합니다. 저희 프레임워크는 주제 스와핑을 위한 강력하고 직관적인 솔루션을 제공하여 사용자가 선호도에 따라 이미지를 손쉽게 조작할 수 있도록 합니다. 앞으로는 이러한 일반적인 실패 문제를 해결하는 방법을 더욱 발전시켜 개인화된 주제 스와핑의 전반적인 성능과 다양성을 향상시킬 계획입니다. 참고문헌 [1] Blattmann, A., Rombach, R., Oktay, K., Müller, J., and Ommer, B. (2022). Retrieval-Augmented Diffusion Models. NeurIPS에서. [2] Brock, A., Donahue, J., and Simonyan, K. (2018). 영어: 고충실도 자연 이미지 합성을 위한 대규모 gan 훈련.arXiv. [3] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y. (2023). Masactrl: 일관된 이미지 합성 및 편집을 위한 튜닝 없는 상호 자기 주의 제어.arXiv. [4] Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., and Romero-Soriano, A. (2021). Instance-Conditioned GAN. NeurIPS에서. [5] Chen, W., Hu, H., Saharia, C., and Cohen, WW (2023). Re-Imagen: 검색 증강 텍스트-이미지 생성기. ICLR에서. [6] Couairon, G., Verbeek, J., Schwenk, H., and Cord, M. (2022). Diffedit: 마스크 가이드를 사용한 확산 기반 의미적 이미지 편집. arXiv. [7] Crowson, K., Biderman, S., Kornis, D., Stander, D., Hallahan, E., Castricato, L., and Raff, E. (2022). Vqgan-clip: 자연어 가이드를 사용한 오픈 도메인 이미지 생성 및 편집. ECCV에서. [8] Deng, Y., Tang, F., Dong, W., Ma, C., Pan, X., Wang, L., and Xu, C. (2022). Stytr2: 변환기를 사용한 이미지 스타일 전송. CVPR에서. [9] Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. (2021). Cogview: 변환기를 통한 텍스트-이미지 생성 마스터링. NeurIPS. [10] Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana, P., Basu, S., Wang, XE, and Wang, WY (2023). 구성적 텍스트-이미지 합성을 위한 훈련 없는 구조적 확산 가이드. ICLR에서. [11] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, AH, Chechik, G., and Cohen-Or, D. (2022). 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv. [12] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, AH, Chechik, G., 및 Cohen-Or, D. (2023a). 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR에서.[13] Gal, R., Arar, M., Atzmon, Y., Bermano, AH, Chechik, G., 및 Cohen-Or, D. (2023b). 텍스트-이미지 모델의 빠른 개인화를 위한 인코더 기반 도메인 튜닝. arXiv에서. [14] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., 및 Bengio, Y. (2020). 생성적 적대 네트워크. ACM 커뮤니케이션. [15] Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., 및 Guo, B. (2022). 텍스트-이미지 합성을 위한 벡터 양자화 확산 모델. CVPR에서. [16] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., 및 Cohen-Or, D. (2022). 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv에서. [17] Huang, X., Liu, M.-Y., Belongie, S., 및 Kautz, J. (2018). 다중 모드 비지도 이미지-이미지 변환. ECCV에서. [18] Jahn, M., Rombach, R., 및 Ommer, B. (2021). 영어: 변압기를 사용한 고해상도 복합 장면 합성.arXiv. [19] Karras, T., Laine, S., 및 Aila, T. (2019). 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처.CVPR에서. [20] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., 및 Irani, M. (2022). Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집.arXiv. [21] Kumari, N., Zhang, B., Zhang, R., Shechtman, E., 및 Zhu, J.-Y. (2023). 텍스트-이미지 확산의 다중 개념 사용자 지정.CVPR에서. [22] Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and Lee, YJ (2023). Gligen: 오픈 세트 접지 텍스트-이미지 생성. arXiv. [23] Liao, J., Yao, Y., Yuan, L., Hua, G., and Kang, SB (2017). 심층 이미지 유추를 통한 시각적 속성 전송. ACM Transactions on Graphics. [24] Liu, S., Lin, T., He, D., Li, F., Wang, M., Li, X., Sun, Z., Li, Q., and Ding, E. (2021). Adaattn: 임의의 신경 스타일 전송에서 주의 메커니즘 재검토. ICCV에서. [25] Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., 및 Ermon, S. (2021). Sdedit: 확률적 미분 방정식을 사용한 이미지 합성 및 편집. arXiv. [26] Mokady, R., Hertz, A., Aberman, K., Pritch, Y., 및 Cohen-Or, D. (2022). 가이드 확산 모델을 사용하여 실제 이미지 편집을 위한 널 텍스트 반전. arXiv에서. [27] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., 및 Chen, M. (2021). Glide: 텍스트 가이드 확산 모델을 사용한 사실적인 이미지 생성 및 편집을 향해. arXiv. [28] Nitzan, Y., Aberman, K., He, Q., Liba, O., Yarom, M., Gandelsman, Y., Mosseri, I., Pritch, Y., and Cohen-or, D. (2022). MyStyle: A Personalized Generative Prior. 아시아 컴퓨터 그래픽 및 대화형 기술에 대한 특수 관심 그룹(SIGGRAPH Asia). [29] OpenAI(2021). DALL E: 텍스트에서 이미지 생성. https://openai.com/research/dall-e. [30] OpenAI(2022). DALL E2. https://openai.com/product/dall-e-2. [31] Perera, MV and Patel, VM(2023). 확산 기반 얼굴 생성 모델의 편향 분석. arXiv 사전 인쇄본 arXiv:2305.06402. [32] Radford, A., Kim, JW, Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). 자연어 감독에서 이전 가능한 시각적 모델 학습. ICML에서. [33] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR에서. [34] Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: 생물의학 이미지 분할을 위한 합성 네트워크. MICCAI에서. Springer. [35] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., 및 Aberman, K. (2023). DreamBooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR에서. [36] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, EL, Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. (2022). 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. NeurIPS에서. [37] Sasha Luccioni, A., Akiki, C., Mitchell, M., 및 Jernite, Y. (2023). 안정적 편향: 확산 모델에서 사회적 표현 분석. arXiv 전자 인쇄물, arXiv-2303 페이지. [38] Seo, J., Lee, G., Cho, S., Lee, J., 및 Kim, S. (2022). Midms: 샘플 기반 이미지 변환을 위한 매칭 인터리브 확산 모델. arXiv. [39] Sheynin, S., Ashual, O., Polyak, A., Singer, U., Gafni, O., Nachmani, E., 및 Taigman, Y. (2023). KNN-Diffusion: 대규모 검색을 통한 이미지 생성. ICLR에서. [40] Song, J., Meng, C., 및 Ermon, S. (2020). 확산 암시적 모델 잡음 제거. 국제 학습 표현 컨퍼런스에서.[41] Tumanyan, N., Geyer, M., Bagon, S., 및 Dekel, T. (2022). 영어: 텍스트 기반 이미지 대 이미지 변환을 위한 플러그 앤 플레이 확산 기능.arXiv. [42] Wang, M., Yang, G.-Y., Li, R., Liang, R.-Z., Zhang, S.-H., Hall, PM, Hu, S.-M. (2019). 의미적 레이블링에서 예제가이드 스타일 일관성 있는 이미지 합성.CVPR에서.[43] Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., Wen, F. (2022a). 예제로 페인트: 확산 모델을 사용한 예시 기반 이미지 편집.arXiv. [44] Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C., Zeng, M., et al. (2022b). Reco: 영역 제어 텍스트-이미지 생성. arXiv. [45] Zeng, Y., Lin, Z., Zhang, J., Liu, Q., Collomosse, J., Kuen, J., Patel, VM(2022). Scenecomposer: 모든 수준의 의미적 이미지 합성. arXiv. [46] Zhang, L., Agrawala, M.(2023). 텍스트-이미지 확산 모델에 조건 제어 추가. arXiv. [47] Zhang, P., Zhang, B., Chen, D., Yuan, L., Wen, F.(2020). 샘플 기반 이미지 변환을 위한 교차 도메인 대응 학습. CVPR, 5143-5153페이지. [48] Zhang, Y., Huang, N., Tang, F., Huang, H., Ma, C., Dong, W., and Xu, C. (2022). 확산 모델을 통한 역전 기반 창의성 전이. arXiv. [49] Zhou, X., Zhang, B., Zhang, T., Zhang, P., Bao, J., Chen, D., Zhang, Z., and Wen, F. (2021). Cocosnet v2: 이미지 변환을 위한 전체 해상도 대응 학습. CVPR에서.
