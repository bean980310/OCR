--- ABSTRACT ---
출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 초록 최근 AI 커뮤니티는 대규모 멀티모달 데이터 세트에 의해 주도되는 강력한 기초 모델을 개발하는 데 상당한 진전을 이루었습니다. 그러나 오디오 표현 학습의 경우 기존 데이터 세트는 다음과 같은 측면에서 한계가 있습니다. 불충분한 볼륨, 단순한 콘텐츠 및 힘든 수집 절차. © 2024 저작권은 소유자/저자가 보유합니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0686-8/24/10...$15.https://doi.org/10.1145/3664647.MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 고품질 자막이 있는 오디오 데이터 세트를 구축하기 위해 비디오 프레임, 오디오 스트림과 같은 다중 모달 입력을 활용하는 혁신적이고 자동화된 접근 방식을 제안합니다. 구체적으로, 150만 개 이상의 오디오-텍스트 쌍으로 구성된 AutoACD라는 대규모 고품질 오디오-언어 데이터 세트를 구성합니다. 사전 학습된 일련의 모델 또는 API를 활용하여 오디오-비주얼 동기화를 결정하고, 이미지 자막, 객체 감지 또는 특정 비디오에 대한 오디오 태그를 생성합니다. 그런 다음 LLM을 사용하여 추출된 다중 모달 단서에 따라 각 오디오에 대한 일치하는 자막을 의역합니다. 제안된 데이터 세트의 효과를 입증하기 위해, 우리는 우리 데이터 세트에서 널리 사용되는 모델을 훈련시키고 오디오-언어 검색, 오디오 캡션, 제로샷 분류와 같은 다양한 다운스트림 작업에서 성능이 향상되었음을 보여줍니다. 또한, 우리는 환경 정보를 사용하여 새로운 벤치마크를 설정하고 오디오-텍스트 작업에 대한 벤치마크를 제공합니다. CCS 개념 • 컴퓨팅 방법론 → 컴퓨터 비전; • 정보 시스템 → 데이터 구조; 정보 검색. 키워드 오디오-언어 데이터 세트, 오디오-언어 표현 학습, 오디오 캡션 ACM 참조 형식: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie. 2024. Auto-ACD: 오디오-언어 표현 학습을 위한 대규모 데이터 세트. 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최된 제32회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;24)의 회의록. ACM, 뉴욕, 뉴욕, 미국, 13페이지. https://doi.org/10.1145/3664647.
--- INTRODUCTION ---
최근 문헌에서 CLIP[49], GPT[50]의 변형, DALL-E 2[51] 및 Stable Diffusion[53]과 같은 기초 모델은 다양한 이해 및 생성 작업에서 엄청난 성공을 거두었습니다. 아키텍처 또는 알고리즘 설계가 다르지만 MMC4[66], LAION[55], HowTo100M[39]과 같은 대규모 멀티모달 데이터 세트와 같은 공통 기반에 있으며, 이는 모델 중심에서 데이터 중심 표현 학습으로의 새로운 전환을 나타냅니다. 전자는 사전 결정된 데이터 예산의 제약 내에서 모델 설계의 경계를 넓히는 것을 고려하는 반면, 후자는 확장 가능한 방식으로 대규모 고품질 데이터 세트를 큐레이팅하는 데 중점을 둡니다. 오디오 커뮤니티에서는 그림 2에서 볼 수 있듯이 오디오-언어 데이터 세트를 구성하기 위한 최근 노력이 있었습니다. 그러나 기존 데이터 세트는 힘들고 복잡한 수집 프로세스와 텍스트의 단순한 설명이라는 두 가지 한계에 직면할 수 있습니다. 한편, 일반적으로 1~3개의 사운드 이벤트로 구성된 오디오를 담고 있으며 인간 주석자가 제공한 고품질 텍스트 설명이 함께 제공되는 Clotho[11]와 AudioCaps[24]. 이는 확장하기가 분명히 어렵습니다. 반면, LAION-Audio-630K[59]와 WavCaps[38]는 온라인 폴리 웹사이트에서 대량의 원시 데이터를 수집한 다음 문장 템플릿이나 키워드-캡션 모델을 사용하여 원래 오디오 레이블을 자유형 문장으로 변환합니다. 그 결과 언어 설명이 간단한 프롬프트나 사운드 태그보다 추가 정보를 제공하지 않는다는 것은 분명합니다. 따라서 모델은 #Vocab을 훈련했습니다. 311K 자동 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 길이 18.11.8.7.7.29K 22K 5K 4K 수량 30K 57K 400K 630K 1.5M Env. WavCaps LAION-Audio-630K - Clotho AudioCaps Auto-ACD(저희 제품) 그림 2: 다른 오디오 캡션 데이터 세트와의 비교. &quot;Length&quot;와 &quot;# Vocab.&quot;은 평균 길이와 어휘를 나타냅니다. &quot;Env.&quot;와 &quot;Auto.&quot;는 각각 환경 정보와 자동 파이프라인을 나타냅니다. 이러한 데이터 세트에서는 강력한 오디오-언어 표현을 학습할 수 없습니다. 이 논문에서는 최소한의 수동 작업으로 대규모, 고품질, 오디오-언어 데이터 세트를 구성하기 위한 최근의 노력을 소개합니다. 이 데이터 세트는 Auto-ACD(자동 수집을 통한 오디오 캡션 데이터 세트)라고 하며, 방대한 오디오-텍스트 쌍(1.5M), 긴 텍스트(단어) 및 다양한 어휘(23K)를 포함합니다. 구체적으로, 모범적인 오디오 캡션은 네 가지 유형의 정보를 캡슐화해야 합니다. &#39;무엇&#39; - 인지된 소리의 특성, &#39;누구&#39; - 소리를 생성하는 개체, &#39;어떻게&#39; - 소리의 특성, &#39;어디&#39; - 소리가 발생하는 위치입니다. 우리의 핵심 통찰력은 시각적 장면에 대한 포괄적인 이해가 귀중한 정보 소스 역할을 할 것으로 예상되며 때로는 오디오 콘텐츠를 이해하는 데 필요하다는 것입니다.따라서 기존 오디오-비주얼 데이터 세트(예: VGGSound [7], AudioSet [13])의 강력한 오디오-비주얼 대응에 대한 사전을 기반으로 Auto-ACD를 구축합니다.특히, 비전, 언어 및 오디오 모델과 같이 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 사용하여 주어진 비디오 데이터 세트의 오디오 스트림에 대한 포괄적인 언어 설명을 생성하는 자동 파이프라인을 시작합니다.마지막으로, 대규모 언어 모델(LLM)을 사용하여 모든 출력을 집합적으로 동화하고 비논리적인 정보를 식별하여 제거하고 오디오에 대한 포괄적인 설명을 생성합니다.결과적으로 이러한 설명은 소리의 유형과 소스를 묘사할 뿐만 아니라 청각 속성과 발생의 특정 위치도 설명합니다.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 예를 들어, Auto-ACD의 텍스트 설명에서 학습한 오디오 이벤트와 주변 정보와 같은 청각 표현을 종합적으로 검증하기 위해 네 가지 관점에서 실험을 수행했습니다.첫째, InfoNCE 손실[18, 46, 65]을 사용하여 공동 오디오-언어 표현 학습을 시작하고 오디오와 언어 간의 검색 작업을 통해 모델을 평가하여 기존 데이터 세트에 비해 눈에 띄는 개선을 보였습니다.둘째, 제로샷 분류 실험을 수행하여 데이터 세트로 환경 정보를 학습하는 데 효과적인지 보여주었습니다.셋째, 사전 훈련된 오디오 백본과 GPT2[50] 간의 가벼운 매핑 네트워크를 훈련하여 오디오-언어 생성 작업, 특히 자동 오디오 캡션에 대한 벤치마킹을 수행하여 널리 사용되는 벤치마크인 Clotho[11]에서 뛰어난 성능을 보였습니다.넷째, 테스트 세트를 수동으로 필터링하고 오디오-언어 작업을 위한 새로운 벤치마크를 도입했습니다.이 벤치마크는 모델이 단순한 오디오 태그를 넘어 환경 및 세분화된 소리 범주와 같은 정보를 파악할 수 있는 능력을 평가하여 이 분야의 미래 연구를 위한 기준을 설정했습니다. 2
--- RELATED WORK ---
2. 시청각 학습 시청각 이벤트는 종종 야외 비디오에서 동시에 발생하여 소리와 이미지 사이에 깊은 연결을 설정합니다.[1, 2, 23, 47]은 시청각 자기 지도 학습을 사용하여 시청각 대응 관계를 활용하여 표현 학습을 향상시킵니다.특히,[15, 58, 62]는 이러한 대응 관계에 기반하여 오디오-텍스트 표현을 학습합니다.시청각 현지화[6, 21, 40, 41, 56]는 비디오 내에서 시각적 음원의 위치를 식별하는 데 집중합니다.시청각 분할[12, 29, 32, 42, 64]은 시각적 장면에서 소리가 나는 객체의 픽셀 단위 분할 마스크를 정확하게 예측하는 것을 목표로 합니다.이러한 연구는 야외 비디오에서 오디오 및 시각적 이벤트 간의 본질적인 상관 관계를 추가로 입증했으며, 이는 시각 정보에 기반한 오디오-언어 데이터 세트를 만드는 데 영감을 주었습니다. 2. 시청각 데이터 세트 대규모 시청각 데이터 세트는 효과적인 오디오 및 비디오 이해에 필수적입니다. 시청각 학습에는 종종 AudioSet과 VGGSound라는 두 가지 데이터 세트가 관련됩니다. AudioSet[13]은 각 오디오 클립에 대해 레이블이 지정된 여러 오디오 이벤트가 있는 대규모 시청각 데이터 세트입니다. 2M개가 넘는 10초 오디오 클립이 포함되어 있습니다. AudioSet은 문헌과 수동 큐레이션에 따라 안내되는 632개의 오디오 클래스로 구성된 잘 구조화된 계층적 온톨로지의 도움을 받아 수동으로 주석이 달린 데이터 세트입니다. VGGSound[7]는 309개의 오디오 클래스에 대한 200K개의 10초 비디오로 구성되어 있습니다. 이 데이터 세트는 자동화된 파이프라인을 통해 수집되고 주석이 달렸으며 각 비디오에는 하나의 레이블만 지정되었습니다. 이 논문에서는 오디오 및 시각적 신호를 모두 활용하여 오디오에 대한 자세한 설명을 제공하는 것을 목표로 합니다. 2.3 오디오-언어 학습 오디오-언어 분야에서 시각 언어 모델을 적용하는 것은 큰 도약을 의미합니다. 특히, [59]는 오디오-언어 대조 학습을 위해 CLIP 모델을 채택하여 혁신적인 교차 모달 연구의 선례를 만들었습니다. 연구자들은 오디오-텍스트 검색[25, 45], 오디오 분류[20, 48], 자동 오디오 캡션[37, 60], 오디오 질의응답[14, 31]과 같은 작업을 통해 오디오에서 의미 정보를 추출하는 데만 집중하지 않습니다. 그들은 또한 오디오 이벤트 감지[3, 28]를 통해 소리의 시간적 역학을 탐구하는 것을 포함하여 청각 지각의 보다 미묘한 측면에 도전하고 있습니다. 이 확장된 범위에는 장면 내의 소리 계산[44] 및 음향 특성에 따라 환경 분류[10]와 같은 추가 청각 속성이 포함됩니다. 의심할 여지 없이 포괄적이고 대규모, 고품질의 정보가 풍부한 오디오-언어 데이터 세트를 구성하는 것이 가장 중요합니다. 2.4 오디오 언어 데이터 세트 오디오 텍스트 검색, 오디오 캡션, 오디오 질의응답 및 텍스트 가이드 오디오 생성을 포함한 오디오 언어 작업은 널리 사용되는 두 가지 오디오 캡션 데이터 세트인 AudioCaps와 Clotho의 가용성으로부터 큰 이점을 얻었습니다.AudioSet의 하위 세트인 AudioCaps[24]는 50K 10초 길이의 오디오 클립으로 구성되며 각각 단일 캡션에 주석이 달려 있습니다.주석 작성자에게는 필요한 경우 힌트와 비디오로 AudioSet 태그가 제공되었습니다.반면에 Clotho[11]는 15~20초 동안 지속되는 6K 오디오 클립으로 구성되며 각각 캡션, 문법 교정 및 인간 주석자의 평가를 포함하는 3단계 프로세스를 통해 주석이 달린 5개의 캡션이 달려 있습니다.그러나 인간 주석 프로세스로 인해 이러한 데이터 세트는 크기가 제한적이고 비용이 많이 들고 시간이 많이 걸립니다. LAION-Audio-630K [59]는 Freesound¹ 및 BBC Sound Effects²와 같은 인기 있는 플랫폼을 포함한 온라인 폴리 웹사이트에서 오디오와 설명을 수집합니다.WavCaps [38]는 ChatGPT를 사용하여 이러한 원시 설명을 필터링하고 의역하여 인간의 주석과 유사한 정리된 텍스트 데이터가 있는 400K 오디오-텍스트 쌍의 데이터 세트를 생성합니다.오디오 클립에 종종 하나의 사운드 이벤트만 있기 때문에 문장은 대부분 간단합니다.결과적으로 이러한 데이터 세트에서 학습된 모델은 사운드 범주만 학습할 수 있었습니다.오디오-텍스트 모델의 이해 능력을 향상시키려면 보다 다양한 텍스트 및 오디오 데이터 세트가 필요합니다.데이터 세트 구성 풍부한 언어 설명이 포함된 대규모 오디오 데이터 세트를 개발하기 위해 시각적 장면 이해가 강력한 사전 역할을 한다는 가정을 기반으로 합니다.예를 들어, 동기화된 비디오는 종종 청각적 단서를 보여주고 시각 정보는 사운드가 발생하는 음향 환경을 정확하게 표현합니다. 오디오 캡션에서는 사운드 속성, 위치 및 세분화된 레이블을 통합하는 것이 좋습니다. 이를 위해 공개적으로 사용 가능한 도구나 API를 활용하여 오디오 설명에 필요한 정보를 수집하고 결과를 상호 검증할 수 있습니다. 예를 들어, 객체 감지 모델을 사용하여 잠재적인 사운드 소스를 식별하고 환경 분류 모델을 사용하여 장면 범주를 추출할 수 있습니다. 풍부한 정보를 추출하여 정확한 세부 정보를 최대한 적용하고 언어 모델에 충분한 참조를 제공합니다. ¹https://freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie BLIP-GroundingDINO CLIP 프레임 Placeaudio 비디오 visual-audio 레이블 AudioCaption PANNS 역으로 들어오는 기차. train [x:0.5921, y:0.5947, w:0.7879, h:0.3298] Passenger_car train station [prob:0.657] a train horn blows. train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] train horning prompt 기차가 지나갈 때 기차 경적이 울리며 기차역에서 크고 뚜렷한 소리가 납니다. 그림 3: Auto-ACD 수집을 위한 자동 파이프라인. 우리는 4개의 오픈소스 컴퓨터 비전 모델을 활용하여 비디오의 중간 프레임에서 시각적 단서를 추출하고, 2개의 오픈소스 오디오 이해 모델을 활용하여 전체 오디오 콘텐츠를 분석합니다. 결과적으로 우리는 원본 데이터 세트의 레이블을 결합하고, LLM(Large Language Models)을 활용하여 이러한 구성 요소를 해석하고 최종 설명으로 의역합니다. 3.1 도구 또는 API 예를 들어 AudioSet 또는 VGGSound [7, 13]와 같은 기존 대규모 비디오 데이터 세트에서 하나의 샘플이 주어지면 V = {f; a; y}로 표시하고 여기서 f, a 및 y는 각각 프레임 시퀀스, 오디오 스트림 및 시각적 또는 오디오 레이블에 해당합니다. 우리의 목표는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 채택하는 것입니다. 즉, 그림 3과 같이 기성품 비전, 언어 및 오디오 모델을 사용하여 오디오에 대한 언어 설명을 구성하는 것입니다. 이 섹션에서는 이러한 도구에 대해 자세히 설명합니다. 3.1.1 이미지 캡션. 기성품 BLIP-2 [27] 모델을 사용하여 이미지 캡션에 대해 경쟁력 있는 결과를 얻습니다. 이 도구는 전체 이미지를 포괄하고 주요 피사체 또는 환경을 정확하게 묘사하는 캡션을 생성할 수 있습니다. 우리의 경우 비디오의 중간 프레임을 이 모델에 입력합니다. 3.1.2 객체 감지. 우리는 미리 훈련된 Grounding DINO 모델[33]을 사용하여 중간 프레임 내의 객체를 식별하고, 포괄적인 분석을 보장하기 위해 감지된 모든 엔터티와 해당 예측 신뢰도 점수를 보존합니다.3.1.3 이미지 레이블링.우리는 이미지 분류를 위해 미리 훈련된 OpenAI CLIP[49] 모델을 채택합니다.이 애플리케이션에서 우리는 프롬프트: &quot;[레이블}의 사진&quot;을 사용하여 ImageNet[9]의 카테고리 온톨로지를 활용하여 텍스트 임베딩을 생성합니다.3.1.4 장소 인식.우리는 미리 훈련된 PlaceCNN[63]을 사용하여 비디오에서 캡처된 환경 맥락을 추론합니다.오디오와 시각 신호 간의 강력한 대응 관계를 감안할 때, 비디오에 묘사된 환경은 사운드가 발생하는 음향 분위기를 나타낼 가능성이 매우 높습니다.3.1.5 오디오 태그.우리는 미리 훈련된 PANN[26]을 사용하여 오디오 내의 사운드 태그를 예측하고, 신뢰도 점수와 함께 상위 3개의 예측을 보존합니다. 이는 특히 프레임 내에서 볼 수 없는 엔터티에서 나오는 소리의 경우 청각적 시간 정보의 중요한 원천입니다.3.1.6 오디오 캡션.기존 AudioCaption[61] 모델을 사용하여 간결하고 간략한 캡션을 생성합니다.이러한 캡션은 AudioCaps의 스타일과 유사하여 사운드에 대한 추가 설명적 속성이 없는 오디오 이벤트의 범주적 정보에만 초점을 맞춥니다.3.1.7 시청각 동기화.사전 훈련된 Synchformer[22]를 사용하여 비디오와 오디오 간의 동기화 감지를 수행합니다.이 프로세스는 무관하거나 동기화되지 않은 비디오 및 오디오 콘텐츠로 구성된 샘플을 필터링할 수 있습니다.이 경우 분석을 위해 비디오와 오디오를 각각 이 모델에 입력합니다.3.1.8 기존 시청각 레이블.모델의 예측 외에도 기존 데이터 세트의 제공된 레이블도 파이프라인에 통합합니다.예를 들어 VGGSound[7]는 각 비디오에 대해 단일 레이블을 제공하는 반면 AudioSet[13]은 여러 레이블을 제공합니다. 이러한 레이블은 원래 데이터 세트에서 정확하지만 불완전한 오디오-비주얼 정보를 제공합니다.3.1.9 요약.언어 모델의 경우 추론 및 귀납적 요약에서 강력한 성능을 보이는 OpenAI ChatGPT3를 사용하여 위에 언급된 설명 또는 레이블을 오디오에 대한 포괄적인 설명으로 조립합니다.BLIP-2[27]와 같은 많은 연구에서는 기존 도구를 적절히 활용하면 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다.오디오-비주얼 대응 관계와 LLM의 심오한 이해 능력을 활용하여 획득한 풍부한 다중 모달 단서에서 정확한 오디오 캡션을 생성합니다.이 경우 섹션 3.2에 표시된 대로 특별한 프롬프트를 입력합니다.3.2 캡션 생성 비디오에 있는 시각적 및 음향적 단서를 기반으로 구조화된 언어 단락을 만들고 이를 사용하여 ChatGPT에서 오디오에 대한 설명을 생성합니다. 그림 4에서 볼 수 있듯이, 프로세스는 원하는 결과에 대한 구체적인 작업과 기준을 공식화하는 것으로 시작한 다음, 7가지 독특한 시청각적 단서를 입력합니다.3 https://openai.com/chatgpt Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 오디오에 대한 캡션을 생성하도록 ChatGPT 프롬프트하기 비디오와 오디오에서 몇 가지 정보를 제공합니다. 이 오디오는 비디오와 분리되어 있습니다. 오디오에 대한 캡션이 있습니다. 간단한 오디오 캡션으로, 이 문장은 오디오에서 발생하는 일을 간단히 설명합니다. 몇 가지 오디오 태그가 있습니다. 여러 오디오 태그는 이 오디오의 오디오 이벤트를 나타냅니다. 숫자는 확률을 나타냅니다. 시청각 레이블은 데이터 세트 시각적 오디오 레이블입니다. 한 비디오에서 키 프레임을 추출하고, 이것은 이 프레임의 이미지 캡션입니다. 이미지 캡션; 이것은 이미지 레이블입니다. 이미지 레이블; 이것은 객체 감지입니다. 객체 감지; 이것은 장소 감지입니다. 장소 레이블입니다. 이제 일반적인 어휘와 24단어를 넘지 않는 오디오 캡션 하나를 작성하여 오디오에서 발생한 일에 대한 설명을 제공하고 오디오가 발생한 위치를 추론하도록 도와주세요. 위의 정보를 참조할 수 있으며, 일부 시각적 정보는 부정확하여 무시할 수 있습니다. 오디오-비주얼 레이블을 사용하여 캡션의 오디오 이벤트를 확인하세요. 작성한 문장은 다음 예와 같아야 합니다. 숲에서 새들이 지저귀는 동안 종이 세 번 울립니다. 잔디 깎는 기계 엔진이 윙윙거리며 잔디밭에서 몇 번 휴식을 취하기 위해 멈춥니다. 공장에서 간헐적으로 작동하는 기계와 배경에서 사람들이 이야기합니다. 표 1: 정확한 콘텐츠와 충분한 주변 정보가 포함된 Auto-ACD에서 생성된 캡션의 결과. 녹색과 노란색은 오디오가 &quot;어디&quot;와 &quot;어떻게&quot; 들리는지 나타냅니다. 아니요. 생성된 캡션 1. 2. 3. 4. 팀발레가 연주되면서 큰 팝과 뱅 소리가 울려 방 안에 리드미컬한 음악을 만들어냅니다. 물이 졸졸 흐르고 거품이 일며 보트가 미끄러지듯 지나가면서 차분하고 평화로운 수중 분위기를 조성합니다. 새들이 지저귀는 부드러운 소리 속에서 한 여성이 부드럽게 말을 걸며 정원에 고요한 분위기를 조성합니다. 오토바이 엔진이 엔진을 가동하기 전에 공회전하면서 도시 환경에서 큰 소리가 납니다. 오른쪽 Synchformer 허용 오른쪽 오류 비디오-오디오 그림 4: ChatGPT에 제공된 자세한 프롬프트. 시각화 목적으로 다양한 시각-오디오 신호를 강조하기 위해 다양한 색상을 사용합니다. 레이블 레이블 분석 음악 및 음성 기타를 프롬프트에 입력하고 해당 신뢰도 점수를 함께 표시합니다. 또한 AudioCaps 또는 Clotho에서 세 문장 예를 지침으로 제공합니다. 시각화 목적으로 다양한 신호를 구별하기 위해 색상으로 구분된 시스템을 사용합니다. 캡션을 생성하는 동안 ChatGPT에 들리지 않는 정보, 즉 색상과 같이 비논리적이고 시각적으로 지향적인 요소를 제거하도록 명시적으로 요청합니다. 그 결과, 대규모 언어 모델은 제공된 모든 단서에서 시나리오를 분석하고 사운드 범주 및 환경을 사용하여 오디오에 대한 언어 설명을 생성할 수 있습니다. 생성된 캡션 결과는 표 1에 나와 있습니다.3.데이터 세트 필터링 AudioSet은 방대하고 다양하지만 많은 경우 게임 플레이 라이브 스트림 및 설명 비디오와 같이 노이즈로 인해 심하게 손상됩니다.반대로 VGGSound는 자동화된 수집 파이프라인 내에서 비디오와 오디오 간의 강력한 상관 관계를 크게 강조하므로 추가 처리가 필요하지 않습니다.그림 5에서 볼 수 있듯이 비디오-오디오 대응과 원래 레이블 모두에 기반한 필터링 기준을 공식화합니다.각 필터 기준에 대해 수많은 시도를 수행한 후 수동 검증을 수행하며 각 필터링 기준은 90%를 초과하는 정확도를 달성하여 총 0.4백만 개의 비디오를 제거했습니다.3.3.원시 레이블.AudioSet에는 배경 음악이 있는 수많은 설명 비디오가 포함되어 있으며 시각적 정보와 청각적 정보가 종종 일치하지 않습니다.따라서 음성과 음악을 모두 포함하는 다중 레이블에서 비디오를 제거합니다.그림 5: AudioSet의 필터링 프로세스. 데이터 집합을 필터링하기 위해 비디오와 오디오가 동기화되었는지 평가하고 원본 데이터 집합의 레이블을 분석합니다.3.3.2 시청각 동기화.우연한 추론 오류의 가능성을 없애기 위해 각 비디오에 5번의 동기화 평가를 실시합니다.시작 시간과 오프셋에 무작위 변화가 있으며 허용 임계값은 0.6초로 설정됩니다.Synchformer[22]는 정확한 시청각 동기화를 확인하기 위해 0.2초 오프셋을 사용하는 반면, 우리는 더 광범위한 오프셋을 사용하여 시청각적 대응 관계를 확인합니다.결과는 다음과 같이 분류됩니다.(1) 기준 진실과 일치하는 예측은 &quot;올바른 것&quot;으로 간주됩니다.(2) 기준 진실과 다르지만 0.6초 이내에 차이가 있는 예측은 &quot;허용 가능한 것&quot;으로 지정됩니다.(3) 다른 모든 결과는 &quot;오류&quot;로 명명됩니다.가능한 한 많은 데이터를 보존하기 위해 5가지 테스트 모두에서 &quot;오류&quot;로 분류된 비디오는 데이터 집합에서 제거합니다. 3. 데이터 세트 통계 그림 2에서 볼 수 있듯이 AudioSet과 VGGSound에서 총 150만 개의 오디오-언어 쌍을 수집합니다. 저희가 아는 한, Auto-ACD는 훈련, 검증 및 수동으로 필터링된 테스트 세트를 갖춘 최초의 백만 레벨 오디오-언어 데이터 세트입니다. Auto-ACD는 데이터 볼륨, 평균 문장 길이 측면에서 다른 데이터 세트를 능가하며 비교적 광범위한 언어 어휘를 포함합니다. LAIONAudio-630K[59]는 사용자 업로드에서 소스이며 장치 및 타임스탬프와 같은 많은 노이즈 세부 정보를 포함하고 매우 광범위한 어휘를 제공합니다. 또한 Auto-ACD는 MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최됩니다. 개구리가 울고 곤충이 울부짖습니다. 텍스트 인코더 오디오 인코더 ... .. సి mo eee... e 매핑 네트워크 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 접두사 임베딩 Pi 오디오 기능 e ea et ea et ea e ... ea.e GPT ea eeee e³ ea.en ea en een een e en.en 오디오 인코더 캡션 토큰 기차가 달리고 기차 경적 소리가 울립니다.그림 6: 오디오 언어 검색 모델 및 자동 오디오 캡션 모델 프레임워크.CLIP과 유사하게 오디오 언어 검색 모델은 오디오 인코더, 텍스트 인코더 및 대조 손실로 구성됩니다.자동 오디오 캡션 모델은 동결된 오디오 인코더와 언어 모델, 그리고 학습 가능한 매핑 네트워크로 구성됩니다.환경 정보를 포함하는 유일한 오디오 언어 데이터 세트로서 소리의 유형과 출처를 구분할 뿐만 아니라 발생 위치도 지정하여 맥락적 세부 정보의 풍부함을 높입니다. 보충적으로, 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 캡션에 대한 비교 분석을 제시합니다. LAION-Audio-630K 및 WavCaps의 캡션은 간결하며 오디오 태그 너머의 최소한의 정보를 포함합니다. 특히, LAION-Audio-630K에는 상식에서 벗어나는 문장이 포함될 수 있습니다. 예를 들어, &quot;나무를 두드리는&quot;을 &quot;래핑&quot; 오디오 태그에 대해 설명합니다. 반면, WavCaps는 &quot;... 소리가 들린다&quot;와 같이 단조로운 문장 구조를 보입니다. 반면, Auto-ACD는 오디오 장면을 더 풍부하게 묘사하는 더 긴 문장을 특징으로 합니다. 우리는 Auto-ACD에서 무작위로 샘플링한 200개의 오디오 캡션 쌍에 대한 수동 검사를 수행하여 다양한 오픈 소스 도구의 단서와 생성된 캡션을 분석합니다. 우리는 오디오와 모순되는 단서를 오류로 정의하고, 이러한 도구는 높은 정확도를 가지고 있으며, 평균 정확도는 81.3%입니다. 나아가, 우리는 무작위로 샘플링된 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행하고, 92.4%의 캡션이 오디오와 일치하고, 단지 5.3%의 잘못된 단어만 수정해야 하며, 단지 4.4%의 캡션만이 들리지 않는 정보를 포함한다는 것을 발견했습니다. 이러한 결과는 우리가 제안하는 접근 방식이 잘못되거나 들리지 않는 정보가 거의 없는 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다. 4 아키텍처 우리는 오디오-언어 대조 학습과 자동 오디오 캡션이라는 두 가지 일반적인 오디오-언어 작업을 타겟으로 하는 아키텍처를 구축하여 Auto-ACD의 효과를 더욱 검증합니다. 섹션 4.1에서 오디오-언어 대조 학습을 위한 아키텍처에 대한 자세한 설명을 제공합니다. 섹션 4.2에서 손실 함수와 함께 가벼운 자동 오디오 캡션을 위한 프레임워크를 소개합니다. 4.1 오디오-언어 대조 학습 제안된 데이터 세트의 효능을 검증하기 위해 그림 6에서 보인 것처럼 표준 대조 학습(예: infoNCE [49] 손실)을 사용하여 오디오 언어 모델을 훈련합니다. 구체적으로, 사전 훈련된 HTSAT [8]을 오디오 인코더로, 사전 훈련된 ROBERTa [35]를 언어 인코더로 사용합니다. 두 인코더 모두 사전 훈련된 CLAP 모델 [59]에서 초기화되었으며, 데이터 세트에서 추가로 미세 조정되었습니다. 최종 모델을 오디오-텍스트 검색(ATR)이라고 합니다. 오디오-텍스트 쌍(a², t¹)이 주어지면 오디오 인코더 Aenc와 텍스트 인코더 Tenc를 사용하여 각각 오디오 임베딩 è̟ò̟와 텍스트 임베딩 e를 추출합니다.e²₁ = Aenc(a²), e² = Tenc (t¹) 그런 다음 모델은 대조 손실로 훈련되며, 여기서 쌍을 이룬 오디오 및 언어 임베딩은 양수로, 쌍을 이루지 않은 임베딩은 음수로 처리되며 손실 함수는 다음과 같습니다.TNL =2N xp ea e/ (log i=j=Σι exp τ + log Σ.1 exp expe T 여기서 는 학습 가능한 온도 매개변수를 나타냅니다.훈련 단계에서 텍스트 인코더에 입력하기 전에 문장 내의 단어를 무작위로 마스크하는 단어 수준 텍스트 마스킹을 도입했습니다.4.2 자동 오디오 캡션 사전 훈련된 오디오 백본의 효과를 보여주기 위해 평가를 위해 오디오 캡션도 사용합니다.ClipCap [43]과 AutoADs [16, 17]에서 영감을 받아 다음을 채택합니다. 그림 6과 같이 오디오 백본과 언어 모델(GPT-2)이 모두 고정되어 있고 매핑 네트워크만 학습되는 가벼운 오디오 캡션 모델입니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 2: AudioCaps, Clotho 및 ACD 테스트 세트의 오디오-텍스트 검색 결과. &quot;기본&quot;, &quot;LA.&quot; “Wav.&quot;와 &quot;ACD&quot;는 각각 AudioCaps와 Clotho(기본), LAION-Audio-630K(LA), WavCaps(Wav) 및 Auto-ACD(ACD)의 조합을 나타냅니다. &quot;ACDvs&quot;는 VGGSound에서 큐레이션한 Auto-ACD의 하위 집합입니다. “* FT”는 대상 데이터 세트에서 모델을 미세 조정하는 것을 나타냅니다. AudioCaps 테스트 Clotho 테스트 Auto-ACD 테스트 텍스트→오디오 트레인 세트 모델 오디오→텍스트 텍스트 오디오 R@1 R@10 R@1 R@basic+LA.[59] basic+Wav.[38] HTSAT-ROBERTa 45.HTSAT-BERT 51.88.36.82.24.66.17.90.6 39.86.23.63.19.Audio-Text 텍스트→오디오 오디오-텍스트 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@55.4 20.0 65.0 17.9 59.58.basic+ACDys HTSAT-ROBERTa 50.5 90.39.86.24.62.9 20.basic+ACD basic+ACD*FT HTSAT-ROBERTa 53.HTSAT-ROBERTa 56.91.39.85.17.93.42.88.26.52.6 15.67.5 21.58.9 39.2 86.2 39.6 85.52.1 47.1 91.2 49.0 92.61.&#39;k&#39; 오디오-텍스트 쌍(a², c¹)이 주어지면 사전 학습된 오디오 인코더를 사용하여 오디오 특징 e² 2 = Aenc(a¹)을 추출하고 캡션을 토큰으로 변환합니다. 시퀀스, c½,..., c, 여기서 k는 텍스트의 최대 길이를 나타냅니다. 그런 다음 추출된 임베딩을 접두사 임베딩 세트로 변환하기 위해 매핑 네트워크 fmap을 설계합니다. pi = fmap (e¹²). 접두사 임베딩 세트를 자기 회귀 언어 모델로 다음 토큰을 예측하기 위한 조건으로 사용합니다. 따라서 학습하는 동안 올바른 단어를 예측하는 음의 로그 우도를 최소화합니다. L = = Ne Σ logo (c); | Pic,...,c&#39;;_1) i=1 j=여기서 는 학습 가능한 매개변수를 나타냅니다. 실험 이 섹션에서는 오디오 언어 검색, 오디오 캡션 및 제로 샷 분류의 세 가지 작업을 평가합니다. 5.1 오디오 언어 검색 5.1. 데이터 세트. AudioCaps, Clotho, Auto-ACDys 및 Auto-ACD와 같은 여러 데이터 세트에서 오디오 텍스트 검색 실험을 수행합니다. AudioCaps, Clotho 및 Auto-ACD의 훈련, 검증 및 테스트 세트에 대한 분포는 각각 50K/495/975, 3.8K/1045/1045 및 1.5M/2K/1K 데이터 쌍입니다.Auto-ACDys는 Auto-ACD의 하위 집합으로, VGGSound에서만 독점적으로 공급된 190K 데이터 쌍을 포함합니다.특히 Clotho와 AudioCaps(검증 및 테스트 세트)의 경우 각 데이터 쌍은 5개의 해당 캡션이 동반된 하나의 오디오 샘플로 구성되는 반면 나머지 데이터 쌍은 하나의 오디오 캡션 쌍으로만 구성됩니다.5.1.2 Auto-ACD 벤치마크.Auto-ACD 훈련 세트 외에도 검증 세트를 형성하기 위해 2K 데이터 샘플을 무작위로 선택하고 테스트 세트에 1K 샘플을 선택했습니다.언어 설명에서 잘못된 정보를 제거하고 부적절한 어휘 표현을 다시 작성하여 테스트 세트에 대한 수동 검증을 수행합니다. 이 테스트 세트는 오디오 언어 검색 및 자동 오디오 캡션 작업을 모두 평가하는 데 사용됩니다.5.1.3 메트릭. 데이터 세트의 풍부하고 정확한 정보를 검증하기 위해 일반적으로 사용되는 데이터 세트(예: AudioCaps 및 Clotho)에서 기존 메트릭인 Recall@k 성능을 비교합니다.또한 이러한 메트릭을 Auto-ACD 테스트 세트에 채택하여 포괄적인 개요를 제공합니다.5.1.4 학습 세부 정보.제안된 오디오-텍스트 검색(ATR) 모델을 20개 에포크 동안 학습시키고, 배치 크기는 768이며, 워밍업 단계와 코사인 학습 속도 감소 일정이 있는 1e-4의 초기 학습 속도를 사용하는 Adam 옵티마이저를 활용합니다.기존 CLAP 모델 구성과 동일한 하이퍼파라미터를 사용합니다. 오디오 인코더와 텍스트 인코더 출력의 차원은 모두 512입니다. 또한 문장의 단어에 25% 랜덤 마스킹을 도입하고 오디오 샘플의 50%에 노이즈 및 게인과 같은 증강을 랜덤하게 적용하여 모델 학습을 향상시킵니다. 15개 에포크에 대해 초기 학습률 2e-5로 Clotho 및 AudioCaps와 같은 특정 데이터 세트에서 모델을 추가로 미세 조정합니다. 5.1.5 결과. 표 2에서 볼 수 있듯이 다음과 같은 주요 관찰 결과를 도출할 수 있습니다. (i) Laion-Audio-630K에서 학습한 것과 비교할 때 제안된 Auto-ACDys 데이터 세트에서 학습하면 AudioCaps 및 Auto-ACD 벤치마크에서 Recall@k 메트릭이 상당히 향상됩니다. (ii) Auto-ACD에서 학습하고 Auto-ACD 벤치마크에 적용할 수 없는 특정 데이터 세트에서 미세 조정하면 성능이 현저히 향상됩니다. 이러한 개선은 AudioCaps의 테스트 세트에서 모델을 평가할 때 특히 두드러지는데, AudioCaps는 AudioSet의 하위 세트이고 Auto-ACD와 유사한 데이터 분포를 공유하기 때문입니다. 이러한 미세 조정 프로세스를 통해 모델은 오디오 및 텍스트 정보에 대한 보다 포괄적인 이해를 얻을 수 있으므로 검색 성능이 향상됩니다. (iii) 보다 다양한 어휘와 풍부한 언어 설명이 특징인 Auto-ACD 벤치마크에서 Auto-ACD 데이터 세트에 대한 학습은 Laion-Audio-630K에서 학습된 모델보다 상당히 우수한 성능을 보입니다. 5.2 자동 오디오 캡션 5.2. 데이터 세트. 섹션 5.1에서 언급한 데이터 세트 외에도 3.9K 오디오-텍스트 데이터 쌍으로 구성된 MACS 데이터 세트[36]도 사용합니다. 각 오디오에는 2~5개의 캡션과 여러 오디오 태그가 함께 제공됩니다. 전체적으로 Clotho, AudioCaps 및 MACS의 총 58k 데이터 쌍을 활용하여 자동 오디오 캡션 모델을 학습하고 Clotho 및 Auto-ACD 테스트 세트에서 평가합니다. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 5.2.2 지표. Meteor[5], RougeL[30], Spider[34]와 같은 기존 캡션 지표 외에도 SentenceBERT[52]를 추가 평가 지표로 통합하여 어휘 정렬에만 의존하지 않고 캡션 콘텐츠의 의미적 유사성과 정확성을 우선시합니다.5.2.3 훈련 세부 정보. MLP와 transformer라는 두 가지 매핑 네트워크를 고안하고 훈련 과정에서 GPT의 매개변수를 미세 조정합니다. 접두사의 개수를 8로 설정하고, 각 접두사의 차원은 512입니다. 이 오디오 캡션 모델을 MACS[36], Clotho, AudioCaps에서 15개 에포크 동안 배치 크기 128, 초기 학습 속도 5e-4로 학습합니다. 이 작업에서 우리는 벤치마크 데이터 세트인 Clotho와 Auto-ACD에서 두 모델의 매핑 네트워크만 학습하여 사전 학습된 오디오-텍스트 검색 모델의 오디오 인코더와 사전 학습된 CLAP[59]의 오디오 인코더를 비교합니다. 5.2. 결과. 표 3에서 볼 수 있듯이 두 가지 관찰 결과를 도출할 수 있습니다. (i) 사전 학습된 오디오-텍스트 검색 모델에서 오디오 인코더를 초기화한 자동 오디오 캡션 모델은 모든 평가 지표에서 기준선보다 성능이 향상되었습니다. (ii) Auto-ACD에서 평가할 때 결과가 더 두드러집니다. 기준선 접근 방식의 성능은 Auto-ACD의 테스트 세트에서 급격한 감소를 감독합니다. 우리는 CLAP 모델에서 추출한 기준 특징에 환경 정보에 대한 자세한 설명이 없기 때문에 그렇다고 추측합니다. 사전 훈련된 오디오-텍스트 검색 모델을 기반으로 한 자막 모델은 상당한 성능 향상을 보이며 소리가 발생하는 위치를 정확하게 추론할 수 있습니다. 이 관찰 결과는 Auto-ACD가 광범위한 어휘를 보여주어 다양한 문장 구조를 사용하여 주어진 오디오를 묘사할 수 있음을 의미합니다. 반면에 데이터 세트에서 훈련된 모델은 소리가 나는 맥락을 추론할 수 있음을 보여줍니다. 표 3: Clotho 및 Auto-ACD 테스트 세트의 자동 오디오 자막 결과. &quot;S-BERT&quot;는 SentenceBERT를 나타내고 &quot;Env.&quot;는 예측된 자막에 환경 정보가 포함되어 있는지 여부를 나타냅니다. Clotho ☑ ☑ Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. 34.9 20.6 46.36.2 21.2 47.8.10.CLAP Ours 15.16.CLAP 9.Auto-ACD Ours 21.23.37.19.56.5.5.3.Zero-shot Classification ✓ 데이터 집합.Auto-ACD는 텍스트 설명에 환경 정보를 통합하는 것이 특징입니다.Auto-ACD에서 학습한 후, 4가지 시나리오에서 환경 분류를 수행합니다.(i) AudioSet 평가 집합의 샘플 모음으로, AudioSet 온톨로지 내의 &quot;음향 환경&quot;의 자식 클래스로 주석이 달려 있으며, AudioSet Env라고 합니다.데이터 유출을 방지하기 위해, 이 실험에서는 Auto-ACDys에서 사전 학습된 모델만 사용합니다.(ii) 이전에 DCASE 2020 챌린지에서 사용했던 DCASE 2020 Mobile이라고 하는 도시 음향 장면 데이터 집합[19]입니다. (iii) 인기 있는 도시 사운드 이벤트 분류 데이터 세트인 UrbanSound 8k [54]; (iv) 음악 장르 분류 데이터 세트인 GTZANGenres [57]. 5.3.2 지표. 우리는 기존의 의역 템플릿인 &quot;[환경 레이블]의 소리 / [레이블]의 소리&quot;를 사용하여 오디오텍스트 검색 실험으로 제로샷 분류에 접근합니다. 우리는 이 실험에서 환경 분류 결과를 평가하기 위한 지표로 Recall@1을 사용합니다. 5.3.3 결과. 표 4에 나와 있는 실험 결과는 CLAP에 비해 Auto-ACD에서 사전 학습된 ATR의 우수한 환경 인식 기능을 강조합니다. 특히 AudioSet Env에서 우리 모델은 AudioSet에서 우리의 훈련 데이터 세트로 데이터가 누출되지 않고 사전 학습을 위해 Auto-ACDys만 사용했음에도 불구하고 CLAP보다 상당히 우수한 성능을 보였으며, 이는 Auto-ACD의 풍부하고 정확한 환경 정보를 더욱 입증합니다. UrbanSound 8K와 GTZANGenres에 대한 결과는 오디오 이벤트 외에도 캡션에 더 많은 정보(예: 다양한 환경 설명, 세분화된 음악 장르)가 포함될 수 있음을 보여줍니다.66*표 4: Zero-Shot Acoustic Environment Classification. Auto-ACDvs의 사전 학습 모델을 나타냅니다. &quot;US-8K&quot;는 UrbanSound 8K를 나타냅니다.Model AudioSet Env DCASE US-8K GTZANGenres CLAP 75.76.19.Ours 39.5* 32.36.CONCLUSION 31.45. 이 논문에서는 1.5M 데이터 쌍으로 구성된 대규모의 포괄적인 오디오 캡션 데이터 세트와 함께 오디오 캡션 생성을 위한 자동 파이프라인을 제시합니다. 나아가 데이터 세트에서 다양한 오디오-언어 모델의 성능을 평가하여 효과를 인증하고 오디오-언어 작업에 대한 벤치마크와 함께 수동으로 검증된 테스트 세트를 제공합니다. 이러한 실험 결과는 우리 데이터에 내재된 풍부한 정보와 정확한 설명을 밝혀내어 모델이 더욱 강력한 오디오-언어 표현을 학습하도록 돕습니다. 일부 데이터 세트가 자동 파이프라인을 통해 조달된 VGGSound에서 유래되었다는 사실로 인해 온라인 비디오에서 정확한 오디오-언어 쌍으로의 변환은 완전히 자동화되고 복제 가능한 절차로 발전했습니다. 결과적으로 오디오-언어 데이터 세트의 확장된 코퍼스를 획득하는 것은 이제 간단한 노력이 되었습니다. 더욱이 오픈소스 컴퓨터 비전 모델과 대규모 언어 모델(LLM)이 지속적으로 개선되고 발전함에 따라 보다 정확한 오디오-비주얼 지표를 추출하는 능력이 향상되어 추론의 정확도와 최종 오디오 캡션의 의역 품질이 향상됩니다. 감사의 말 이 연구는 중국 국가 핵심 R&amp;D 프로그램(No.2022ZD0161400)의 지원을 받았습니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 참고문헌 [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman. 2020. 자기 감독 다중 모드 다용도 네트워크. 신경 정보 처리 시스템의 발전 33(2020), 25-37. [2] Relja Arandjelovic, Andrew Zisserman. 2017. 보고, 듣고, 배우세요. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 609-617. [3] Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband, Anthony T Chronopoulos. 2017. 오디오 이벤트 감지 개요
--- METHOD ---
ologies → 컴퓨터 비전; • 정보 시스템 → 데이터 구조; 정보 검색. 키워드 오디오-언어 데이터 세트, 오디오-언어 표현 학습, 오디오 캡션 ACM 참조 형식: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie. 2024. Auto-ACD: 오디오-언어 표현 학습을 위한 대규모 데이터 세트. 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최된 제32회 ACM 국제 멀티미디어 컨퍼런스(MM &#39;24) 회의록. ACM, 뉴욕, 뉴욕, 미국, 13페이지. 한국어: https://doi.org/10.1145/3664647.서론 최근 문헌에서 CLIP[49], GPT[50]의 변형, DALL-E 2[51] 및 Stable Diffusion[53]과 같은 기초 모델은 다양한 이해 및 생성 작업에서 엄청난 성공을 거두었습니다.구조적 또는 알고리즘적 설계가 다르지만 대규모 멀티모달 데이터 세트(예: MMC4[66], LAION[55], HowTo100M[39])와 같은 공통 기반에 있으며, 이는 모델 중심 표현 학습에서 데이터 중심 표현 학습으로의 새로운 전환을 나타냅니다.전자는 사전 결정된 데이터 예산의 제약 내에서 모델 설계의 경계를 넓히는 것을 고려하는 반면,후자는 확장 가능한 방식으로 대규모 및 고품질 데이터 세트를 큐레이팅하는 데 중점을 둡니다. 오디오 커뮤니티에서 그림 2에서 보듯이 오디오-언어 데이터 세트를 구성하려는 노력이 최근 있었습니다. 그러나 기존 데이터 세트는 힘들고 복잡한 수집 프로세스와 텍스트의 단순한 설명이라는 두 가지 한계에 직면할 가능성이 있습니다. 한편, 일반적으로 1~3개의 사운드 이벤트로 구성된 오디오를 포함하고 인간 주석자가 제공한 고품질 텍스트 설명이 함께 제공되는 Clotho[11]와 AudioCaps[24]. 이는 확장하기가 분명히 어렵습니다. 반면, LAION-Audio-630K[59]와 WavCaps[38]는 온라인 폴리 웹사이트에서 대량의 원시 데이터를 수집한 다음 문장 템플릿이나 키워드-캡션 모델을 사용하여 원래 오디오 레이블을 자유형 문장으로 변환합니다. 결과 언어 설명이 간단한 프롬프트나 사운드 태그보다 추가 정보를 거의 제공하지 않는다는 것은 분명합니다. 따라서 모델은 #Vocab을 훈련했습니다. 311K 자동 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 길이 18.11.8.7.7.29K 22K 5K 4K 수량 30K 57K 400K 630K 1.5M Env. WavCaps LAION-Audio-630K - Clotho AudioCaps Auto-ACD(저희) 그림 2: 다른 오디오 캡션 데이터 세트와의 비교. &quot;길이&quot; 및 &quot;# Vocab.&quot;은 평균 길이 및 어휘를 나타냅니다. &quot;Env.&quot; 및 &quot;Auto.&quot;는 각각 환경 정보 및 자동 파이프라인을 나타냅니다. 이러한 데이터 세트의 모든 오디오 언어 표현은 견고한 오디오 언어 표현을 학습할 수 없습니다. 이 논문에서는 최소한의 수동 작업으로 대규모, 고품질, 오디오-언어 데이터 세트를 구성하기 위한 최근의 노력을 제시합니다.이 데이터 세트는 Auto-ACD(Audio Captioning Dataset by Automatic Collection)라고 하며 방대한 오디오-텍스트 쌍(1.5M), 긴 텍스트(단어) 및 다양한 어휘(23K)를 포함합니다.특히, 모범적인 오디오 캡션은 네 가지 유형의 정보를 캡슐화해야 합니다.&#39;무엇&#39; - 인지되는 소리의 특성, &#39;누구&#39; - 소리를 생성하는 개체, &#39;어떻게&#39; - 소리의 특성, &#39;어디&#39; - 소리가 발생하는 위치입니다.우리의 핵심 통찰력은 시각적 장면에 대한 포괄적인 이해가 귀중한 정보 소스 역할을 할 것으로 예상되며 때로는 오디오 콘텐츠를 이해하는 데 필요하다는 것입니다.따라서 기존 오디오-비주얼 데이터 세트(예: VGGSound [7], AudioSet [13])의 견고한 오디오-비주얼 대응 관계에 대한 사전 지식에 Auto-ACD를 구축합니다. 특히, 우리는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 도구나 API(예: 비전, 언어 및 오디오 모델)를 사용하여 주어진 비디오 데이터 세트의 오디오 스트림에 대한 포괄적인 언어 설명을 생성하는 자동 파이프라인을 시작합니다. 마지막으로, 우리는 대규모 언어 모델(LLM)을 사용하여 모든 출력을 집합적으로 동화하고, 비논리적인 정보를 식별하여 제거하고, 오디오에 대한 포괄적인 설명을 생성합니다. 결과적으로 이러한 설명은 소리의 유형과 출처를 묘사할 뿐만 아니라 청각적 속성과 발생의 특정 위치도 설명합니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Auto-ACD의 텍스트 설명에서 학습한 오디오 이벤트 및 주변 정보와 같은 청각 표현을 포괄적으로 검증하기 위해 다음을 수행합니다.
--- EXPERIMENT ---
4가지 관점에서: 첫째, InfoNCE 손실[18, 46, 65]을 사용하여 공동 오디오-언어 표현 학습을 시작하고 오디오와 언어 간의 검색 작업을 통해 모델을 평가하여 기존 데이터 세트에 비해 눈에 띄는 개선을 보여줍니다.둘째, 제로샷 분류 실험을 수행하여 데이터 세트로 환경 정보를 학습하는 데 효과적임을 보여줍니다.셋째, 사전 훈련된 오디오 백본과 GPT2[50] 간의 가벼운 매핑 네트워크를 훈련하여 오디오-언어 생성 작업, 특히 자동 오디오 캡션에 대한 벤치마킹을 수행하여 널리 사용되는 벤치마크(예: Clotho[11])에서 뛰어난 성능을 보여줍니다.넷째, 테스트 세트를 수동으로 필터링하고 오디오-언어 작업을 위한 새로운 벤치마크를 도입합니다.이 벤치마크는 모델이 단순한 오디오 태그를 넘어 환경 및 세분화된 소리 범주와 같은 정보를 파악할 수 있는 능력을 평가하며, 이 분야의 미래 연구를 위한 기준을 설정합니다. 2 관련 연구 2. 시청각 학습 시청각 이벤트는 종종 야외 비디오에서 동시에 발생하여 소리와 이미지 사이에 깊은 연결을 설정합니다.[1, 2, 23, 47]은 시청각 자기 지도 학습을 사용하여 시청각 대응 관계를 활용하여 표현 학습을 향상시킵니다.특히,[15, 58, 62]는 이러한 대응 관계에 기반하여 오디오-텍스트 표현을 학습합니다.시청각 현지화[6, 21, 40, 41, 56]는 비디오 내에서 시각적 음원의 위치를 식별하는 데 집중합니다.시청각 분할[12, 29, 32, 42, 64]은 시각적 장면에서 소리가 나는 객체의 픽셀 단위 분할 마스크를 정확하게 예측하는 것을 목표로 합니다.이러한 연구는 야외 비디오에서 오디오 및 시각적 이벤트 간의 본질적인 상관 관계를 추가로 입증했으며, 이는 시각 정보에 기반한 오디오-언어 데이터 세트를 만드는 데 영감을 주었습니다. 2. 시청각 데이터 세트 대규모 시청각 데이터 세트는 효과적인 오디오 및 비디오 이해에 필수적입니다. 시청각 학습에는 종종 AudioSet과 VGGSound라는 두 가지 데이터 세트가 관련됩니다. AudioSet[13]은 각 오디오 클립에 대해 레이블이 지정된 여러 오디오 이벤트가 있는 대규모 시청각 데이터 세트입니다. 2M개가 넘는 10초 오디오 클립이 포함되어 있습니다. AudioSet은 문헌과 수동 큐레이션에 따라 안내되는 632개의 오디오 클래스로 구성된 잘 구조화된 계층적 온톨로지의 도움을 받아 수동으로 주석이 달린 데이터 세트입니다. VGGSound[7]는 309개의 오디오 클래스에 대한 200K개의 10초 비디오로 구성되어 있습니다. 이 데이터 세트는 자동화된 파이프라인을 통해 수집되고 주석이 달렸으며 각 비디오에는 하나의 레이블만 지정되었습니다. 이 논문에서는 오디오 및 시각적 신호를 모두 활용하여 오디오에 대한 자세한 설명을 제공하는 것을 목표로 합니다. 2.3 오디오-언어 학습 오디오-언어 분야에서 시각 언어 모델을 적용하는 것은 큰 도약을 의미합니다. 특히, [59]는 오디오-언어 대조 학습을 위해 CLIP 모델을 채택하여 혁신적인 교차 모달 연구의 선례를 만들었습니다. 연구자들은 오디오-텍스트 검색[25, 45], 오디오 분류[20, 48], 자동 오디오 캡션[37, 60], 오디오 질의응답[14, 31]과 같은 작업을 통해 오디오에서 의미 정보를 추출하는 데만 집중하지 않습니다. 그들은 또한 오디오 이벤트 감지[3, 28]를 통해 소리의 시간적 역학을 탐구하는 것을 포함하여 청각 지각의 보다 미묘한 측면에 도전하고 있습니다. 이 확장된 범위에는 장면 내의 소리 계산[44] 및 음향 특성에 따라 환경 분류[10]와 같은 추가 청각 속성이 포함됩니다. 의심할 여지 없이 포괄적이고 대규모, 고품질의 정보가 풍부한 오디오-언어 데이터 세트를 구성하는 것이 가장 중요합니다. 2.4 오디오 언어 데이터 세트 오디오 텍스트 검색, 오디오 캡션, 오디오 질의응답 및 텍스트 가이드 오디오 생성을 포함한 오디오 언어 작업은 널리 사용되는 두 가지 오디오 캡션 데이터 세트인 AudioCaps와 Clotho의 가용성으로부터 큰 이점을 얻었습니다.AudioSet의 하위 세트인 AudioCaps[24]는 50K 10초 길이의 오디오 클립으로 구성되며 각각 단일 캡션에 주석이 달려 있습니다.주석 작성자에게는 필요한 경우 힌트와 비디오로 AudioSet 태그가 제공되었습니다.반면에 Clotho[11]는 15~20초 동안 지속되는 6K 오디오 클립으로 구성되며 각각 캡션, 문법 교정 및 인간 주석자의 평가를 포함하는 3단계 프로세스를 통해 주석이 달린 5개의 캡션이 달려 있습니다.그러나 인간 주석 프로세스로 인해 이러한 데이터 세트는 크기가 제한적이고 비용이 많이 들고 시간이 많이 걸립니다. LAION-Audio-630K [59]는 Freesound¹ 및 BBC Sound Effects²와 같은 인기 있는 플랫폼을 포함한 온라인 폴리 웹사이트에서 오디오와 설명을 수집합니다.WavCaps [38]는 ChatGPT를 사용하여 이러한 원시 설명을 필터링하고 의역하여 인간의 주석과 유사한 정리된 텍스트 데이터가 있는 400K 오디오-텍스트 쌍의 데이터 세트를 생성합니다.오디오 클립에 종종 하나의 사운드 이벤트만 있기 때문에 문장은 대부분 간단합니다.결과적으로 이러한 데이터 세트에서 학습된 모델은 사운드 범주만 학습할 수 있었습니다.오디오-텍스트 모델의 이해 능력을 향상시키려면 보다 다양한 텍스트 및 오디오 데이터 세트가 필요합니다.데이터 세트 구성 풍부한 언어 설명이 포함된 대규모 오디오 데이터 세트를 개발하기 위해 시각적 장면 이해가 강력한 사전 역할을 한다는 가정을 기반으로 합니다.예를 들어, 동기화된 비디오는 종종 청각적 단서를 보여주고 시각 정보는 사운드가 발생하는 음향 환경을 정확하게 표현합니다. 오디오 캡션에서는 사운드 속성, 위치 및 세분화된 레이블을 통합하는 것이 좋습니다. 이를 위해 공개적으로 사용 가능한 도구나 API를 활용하여 오디오 설명에 필요한 정보를 수집하고 결과를 상호 검증할 수 있습니다. 예를 들어, 객체 감지 모델을 사용하여 잠재적인 사운드 소스를 식별하고 환경 분류 모델을 사용하여 장면 범주를 추출할 수 있습니다. 풍부한 정보를 추출하여 정확한 세부 정보를 최대한 적용하고 언어 모델에 충분한 참조를 제공합니다. ¹https://freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie BLIP-GroundingDINO CLIP 프레임 Placeaudio 비디오 visual-audio 레이블 AudioCaption PANNS 역으로 들어오는 기차. train [x:0.5921, y:0.5947, w:0.7879, h:0.3298] Passenger_car train station [prob:0.657] a train horn blows. train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] train horning prompt 기차가 지나갈 때 기차 경적이 울리며 기차역에서 크고 뚜렷한 소리가 납니다. 그림 3: Auto-ACD 수집을 위한 자동 파이프라인. 우리는 4개의 오픈소스 컴퓨터 비전 모델을 활용하여 비디오의 중간 프레임에서 시각적 단서를 추출하고, 2개의 오픈소스 오디오 이해 모델을 활용하여 전체 오디오 콘텐츠를 분석합니다. 결과적으로 우리는 원본 데이터 세트의 레이블을 결합하고, LLM(Large Language Models)을 활용하여 이러한 구성 요소를 해석하고 최종 설명으로 의역합니다. 3.1 도구 또는 API 예를 들어 AudioSet 또는 VGGSound [7, 13]와 같은 기존 대규모 비디오 데이터 세트에서 하나의 샘플이 주어지면 V = {f; a; y}로 표시하고 여기서 f, a 및 y는 각각 프레임 시퀀스, 오디오 스트림 및 시각적 또는 오디오 레이블에 해당합니다. 우리의 목표는 일반 AI 커뮤니티에서 공개적으로 사용 가능한 다양한 도구 또는 API를 채택하는 것입니다. 즉, 그림 3과 같이 기성품 비전, 언어 및 오디오 모델을 사용하여 오디오에 대한 언어 설명을 구성하는 것입니다. 이 섹션에서는 이러한 도구에 대해 자세히 설명합니다. 3.1.1 이미지 캡션. 기성품 BLIP-2 [27] 모델을 사용하여 이미지 캡션에 대해 경쟁력 있는 결과를 얻습니다. 이 도구는 전체 이미지를 포괄하고 주요 피사체 또는 환경을 정확하게 묘사하는 캡션을 생성할 수 있습니다. 우리의 경우 비디오의 중간 프레임을 이 모델에 입력합니다. 3.1.2 객체 감지. 우리는 미리 훈련된 Grounding DINO 모델[33]을 사용하여 중간 프레임 내의 객체를 식별하고, 포괄적인 분석을 보장하기 위해 감지된 모든 엔터티와 해당 예측 신뢰도 점수를 보존합니다.3.1.3 이미지 레이블링.우리는 이미지 분류를 위해 미리 훈련된 OpenAI CLIP[49] 모델을 채택합니다.이 애플리케이션에서 우리는 프롬프트: &quot;[레이블}의 사진&quot;을 사용하여 ImageNet[9]의 카테고리 온톨로지를 활용하여 텍스트 임베딩을 생성합니다.3.1.4 장소 인식.우리는 미리 훈련된 PlaceCNN[63]을 사용하여 비디오에서 캡처된 환경 맥락을 추론합니다.오디오와 시각 신호 간의 강력한 대응 관계를 감안할 때, 비디오에 묘사된 환경은 사운드가 발생하는 음향 분위기를 나타낼 가능성이 매우 높습니다.3.1.5 오디오 태그.우리는 미리 훈련된 PANN[26]을 사용하여 오디오 내의 사운드 태그를 예측하고, 신뢰도 점수와 함께 상위 3개의 예측을 보존합니다. 이는 특히 프레임 내에서 볼 수 없는 엔터티에서 나오는 소리의 경우 청각적 시간 정보의 중요한 원천입니다.3.1.6 오디오 캡션.기존 AudioCaption[61] 모델을 사용하여 간결하고 간략한 캡션을 생성합니다.이러한 캡션은 AudioCaps의 스타일과 유사하여 사운드에 대한 추가 설명적 속성이 없는 오디오 이벤트의 범주적 정보에만 초점을 맞춥니다.3.1.7 시청각 동기화.사전 훈련된 Synchformer[22]를 사용하여 비디오와 오디오 간의 동기화 감지를 수행합니다.이 프로세스는 무관하거나 동기화되지 않은 비디오 및 오디오 콘텐츠로 구성된 샘플을 필터링할 수 있습니다.이 경우 분석을 위해 비디오와 오디오를 각각 이 모델에 입력합니다.3.1.8 기존 시청각 레이블.모델의 예측 외에도 기존 데이터 세트의 제공된 레이블도 파이프라인에 통합합니다.예를 들어 VGGSound[7]는 각 비디오에 대해 단일 레이블을 제공하는 반면 AudioSet[13]은 여러 레이블을 제공합니다. 이러한 레이블은 원래 데이터 세트에서 정확하지만 불완전한 오디오-비주얼 정보를 제공합니다.3.1.9 요약.언어 모델의 경우 추론 및 귀납적 요약에서 강력한 성능을 보이는 OpenAI ChatGPT3를 사용하여 위에 언급된 설명 또는 레이블을 오디오에 대한 포괄적인 설명으로 조립합니다.BLIP-2[27]와 같은 많은 연구는 기존 도구를 적절히 활용하면 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다.오디오-비주얼 대응과 LLM의 심오한 이해 능력을 활용하여 획득한 풍부한 다중 모달리티 단서에서 정확한 오디오 캡션을 생성합니다.이 경우 섹션 3.2에 표시된 대로 특별한 프롬프트를 입력합니다.3.2 캡션 생성 비디오에 있는 시각적 및 음향적 단서를 기반으로 구조화된 언어 단락을 만들고 이를 사용하여 ChatGPT에서 오디오에 대한 설명을 생성하도록 합니다. 그림 4에서 볼 수 있듯이, 프로세스는 원하는 결과에 대한 구체적인 작업과 기준을 공식화하는 것으로 시작한 다음, 7가지 독특한 시청각적 단서를 입력합니다.3 https://openai.com/chatgpt Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 오디오에 대한 캡션을 생성하도록 ChatGPT 프롬프트하기 비디오와 오디오에서 몇 가지 정보를 제공합니다. 이 오디오는 비디오와 분리되어 있습니다. 오디오에 대한 캡션이 있습니다. 간단한 오디오 캡션으로, 이 문장은 오디오에서 발생하는 일을 간단히 설명합니다. 몇 가지 오디오 태그가 있습니다. 여러 오디오 태그는 이 오디오의 오디오 이벤트를 나타냅니다. 숫자는 확률을 나타냅니다. 시청각 레이블은 데이터 세트 시각적 오디오 레이블입니다. 한 비디오에서 키 프레임을 추출하고, 이것은 이 프레임의 이미지 캡션입니다. 이미지 캡션; 이것은 이미지 레이블입니다. 이미지 레이블; 이것은 객체 감지입니다. 객체 감지; 이것은 장소 감지입니다. 장소 레이블입니다. 이제 일반적인 어휘와 24단어를 넘지 않는 오디오 캡션 하나를 작성하여 오디오에서 발생한 일에 대한 설명을 제공하고 오디오가 발생한 위치를 추론하도록 도와주세요. 위의 정보를 참조할 수 있으며, 일부 시각적 정보는 부정확하여 무시할 수 있습니다. 오디오-비주얼 레이블을 사용하여 캡션의 오디오 이벤트를 확인하세요. 작성한 문장은 다음 예와 같아야 합니다. 숲에서 새들이 지저귀는 동안 종이 세 번 울립니다. 잔디 깎는 기계 엔진이 윙윙거리며 잔디밭에서 몇 번 휴식을 취하기 위해 멈춥니다. 공장에서 간헐적으로 작동하는 기계와 배경에서 사람들이 이야기합니다. 표 1: 정확한 콘텐츠와 충분한 주변 정보가 포함된 Auto-ACD에서 생성된 캡션의 결과. 녹색과 노란색은 오디오가 &quot;어디&quot;와 &quot;어떻게&quot; 들리는지 나타냅니다. 아니요. 생성된 캡션 1. 2. 3. 4. 팀발레가 연주되면서 큰 팝과 뱅 소리가 울려 방 안에 리드미컬한 음악이 만들어집니다. 물이 졸졸 흐르고 거품이 거품처럼 거품을 내며 보트가 미끄러지듯 지나가면서 차분하고 평화로운 수중 분위기를 조성합니다. 새들이 지저귀는 부드러운 소리 속에서 한 여성이 부드럽게 말을 걸며 정원에 고요한 분위기를 조성합니다. 오토바이 엔진이 엔진을 가동하기 전에 공회전하면서 도시 환경에서 큰 소리가 납니다. 오른쪽 Synchformer 허용 오른쪽 오류 비디오-오디오 그림 4: ChatGPT에 제공된 자세한 프롬프트. 시각화를 위해 다양한 시각-오디오 신호를 강조하기 위해 다양한 색상을 사용합니다. 레이블 레이블 분석 음악 및 음성 기타를 프롬프트에 입력하고 해당 신뢰도 점수를 함께 표시합니다. 또한 AudioCaps 또는 Clotho에서 세 문장 예를 지침으로 제공합니다. 시각화를 위해 여기서는 다양한 신호를 구별하기 위해 색상으로 구분된 시스템을 사용합니다. 캡션을 생성하는 동안 ChatGPT에 들리지 않는 정보, 즉 색상과 같이 비논리적이고 시각적으로 지향적인 요소를 제거하도록 명시적으로 요청합니다. 그 결과, 대규모 언어 모델은 제공된 모든 단서에서 시나리오를 분석하고 사운드 범주 및 환경을 사용하여 오디오에 대한 언어 설명을 생성할 수 있습니다. 생성된 캡션 결과는 표 1에 나와 있습니다.3.데이터 세트 필터링 AudioSet은 방대하고 다양하지만 많은 경우 게임 플레이 라이브 스트림 및 설명 비디오와 같이 노이즈로 인해 심하게 손상됩니다.반대로 VGGSound는 자동화된 수집 파이프라인 내에서 비디오와 오디오 간의 강력한 상관 관계를 크게 강조하므로 추가 처리가 필요하지 않습니다.그림 5에서 볼 수 있듯이 비디오-오디오 대응과 원래 레이블 모두에 기반한 필터링 기준을 공식화합니다.각 필터 기준에 대해 수많은 시도를 수행한 후 수동 검증을 수행하며 각 필터링 기준은 90%를 초과하는 정확도를 달성하여 총 0.4백만 개의 비디오를 제거했습니다.3.3.원시 레이블.AudioSet에는 배경 음악이 있는 수많은 설명 비디오가 포함되어 있으며 시각적 정보와 청각적 정보가 종종 일치하지 않습니다.따라서 음성과 음악을 모두 포함하는 다중 레이블에서 비디오를 제거합니다.그림 5: AudioSet의 필터링 프로세스. 데이터 집합을 필터링하기 위해 비디오와 오디오가 동기화되었는지 평가하고 원본 데이터 집합의 레이블을 분석합니다.3.3.2 시청각 동기화.우연한 추론 오류의 가능성을 없애기 위해 각 비디오에 5번의 동기화 평가를 실시합니다.시작 시간과 오프셋에 무작위 변화가 있으며 허용 임계값은 0.6초로 설정됩니다.Synchformer[22]는 정확한 시청각 동기화를 확인하기 위해 0.2초 오프셋을 사용하는 반면, 우리는 더 광범위한 오프셋을 사용하여 시청각적 대응 관계를 확인합니다.결과는 다음과 같이 분류됩니다.(1) 기준 진실과 일치하는 예측은 &quot;올바른 것&quot;으로 간주됩니다.(2) 기준 진실과 다르지만 0.6초 이내에 차이가 있는 예측은 &quot;허용 가능한 것&quot;으로 지정됩니다.(3) 다른 모든 결과는 &quot;오류&quot;로 명명됩니다.가능한 한 많은 데이터를 보존하기 위해 5가지 테스트 모두에서 &quot;오류&quot;로 분류된 비디오는 데이터 집합에서 제거합니다. 3. 데이터 세트 통계 그림 2에서 볼 수 있듯이 AudioSet과 VGGSound에서 총 150만 개의 오디오-언어 쌍을 수집합니다. 저희가 아는 한, Auto-ACD는 훈련, 검증 및 수동으로 필터링된 테스트 세트를 갖춘 최초의 백만 레벨 오디오-언어 데이터 세트입니다. Auto-ACD는 데이터 볼륨, 평균 문장 길이 측면에서 다른 데이터 세트를 능가하며 비교적 광범위한 언어 어휘를 포함합니다. LAIONAudio-630K[59]는 사용자 업로드에서 소스이며 장치 및 타임스탬프와 같은 많은 노이즈 세부 정보를 포함하고 매우 광범위한 어휘를 제공합니다. 또한 Auto-ACD는 MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른에서 개최됩니다. 개구리가 울고 곤충이 울부짖습니다. 텍스트 인코더 오디오 인코더 ... .. సి mo eee... e 매핑 네트워크 Luoyi Sun, Xuenan Xu, Mengyue Wu 및 Weidi Xie 접두사 임베딩 Pi 오디오 기능 e ea et ea et ea e ... ea.e GPT ea eeee e³ ea.en ea en een een e en.en 오디오 인코더 캡션 토큰 기차가 달리고 기차 경적 소리가 울립니다.그림 6: 오디오 언어 검색 모델 및 자동 오디오 캡션 모델 프레임워크.CLIP과 유사하게 오디오 언어 검색 모델은 오디오 인코더, 텍스트 인코더 및 대조 손실로 구성됩니다.자동 오디오 캡션 모델은 동결된 오디오 인코더와 언어 모델, 그리고 학습 가능한 매핑 네트워크로 구성됩니다.환경 정보를 포함하는 유일한 오디오 언어 데이터 세트로서 소리의 유형과 출처를 구분할 뿐만 아니라 발생 위치도 지정하여 맥락적 세부 정보의 풍부함을 높입니다. 보충적으로, 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 캡션에 대한 비교 분석을 제시합니다. LAION-Audio-630K 및 WavCaps의 캡션은 간결하며 오디오 태그 너머의 최소한의 정보를 포함합니다. 특히, LAION-Audio-630K에는 상식에서 벗어나는 문장이 포함될 수 있습니다. 예를 들어, &quot;나무를 두드리는&quot;을 &quot;래핑&quot; 오디오 태그에 대해 설명합니다. 반면, WavCaps는 &quot;... 소리가 들린다&quot;와 같이 단조로운 문장 구조를 보입니다. 반면, Auto-ACD는 오디오 장면을 더 풍부하게 묘사하는 더 긴 문장을 특징으로 합니다. 우리는 Auto-ACD에서 무작위로 샘플링한 200개의 오디오 캡션 쌍에 대한 수동 검사를 수행하여 다양한 오픈 소스 도구의 단서와 생성된 캡션을 분석합니다. 우리는 오디오와 모순되는 단서를 오류로 정의하고, 이러한 도구는 높은 정확도를 가지고 있으며, 평균 정확도는 81.3%입니다. 나아가, 우리는 무작위로 샘플링된 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행하고, 92.4%의 캡션이 오디오와 일치하고, 단지 5.3%의 잘못된 단어만 수정해야 하며, 단지 4.4%의 캡션만이 들리지 않는 정보를 포함한다는 것을 발견했습니다. 이러한 결과는 우리가 제안하는 접근 방식이 잘못되거나 들리지 않는 정보가 거의 없는 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다. 4 아키텍처 우리는 오디오-언어 대조 학습과 자동 오디오 캡션이라는 두 가지 일반적인 오디오-언어 작업을 타겟으로 하는 아키텍처를 구축하여 Auto-ACD의 효과를 더욱 검증합니다. 섹션 4.1에서 오디오-언어 대조 학습을 위한 아키텍처에 대한 자세한 설명을 제공합니다. 섹션 4.2에서 손실 함수와 함께 가벼운 자동 오디오 캡션을 위한 프레임워크를 소개합니다. 4.1 오디오-언어 대조 학습 제안된 데이터 세트의 효능을 검증하기 위해 그림 6에서 보인 것처럼 표준 대조 학습(예: infoNCE [49] 손실)을 사용하여 오디오 언어 모델을 훈련합니다. 구체적으로, 사전 훈련된 HTSAT [8]을 오디오 인코더로, 사전 훈련된 ROBERTa [35]를 언어 인코더로 사용합니다. 두 인코더 모두 사전 훈련된 CLAP 모델 [59]에서 초기화되었으며, 데이터 세트에서 추가로 미세 조정되었습니다. 최종 모델을 오디오-텍스트 검색(ATR)이라고 합니다. 오디오-텍스트 쌍(a², t¹)이 주어지면 오디오 인코더 Aenc와 텍스트 인코더 Tenc를 사용하여 각각 오디오 임베딩 è̟ò̟와 텍스트 임베딩 e를 추출합니다.e²₁ = Aenc(a²), e² = Tenc (t¹) 그런 다음 모델은 대조 손실로 훈련되며, 여기서 쌍을 이룬 오디오 및 언어 임베딩은 양수로, 쌍을 이루지 않은 임베딩은 음수로 처리되며 손실 함수는 다음과 같습니다.TNL =2N xp ea e/ (log i=j=Σι exp τ + log Σ.1 exp expe T 여기서 는 학습 가능한 온도 매개변수를 나타냅니다.훈련 단계에서 텍스트 인코더에 입력하기 전에 문장 내의 단어를 무작위로 마스크하는 단어 수준 텍스트 마스킹을 도입했습니다.4.2 자동 오디오 캡션 사전 훈련된 오디오 백본의 효과를 보여주기 위해 평가를 위해 오디오 캡션도 사용합니다.ClipCap [43]과 AutoADs [16, 17]에서 영감을 받아 다음을 채택합니다. 그림 6과 같이 오디오 백본과 언어 모델(GPT-2)이 모두 고정되어 있고 매핑 네트워크만 학습되는 가벼운 오디오 캡션 모델입니다. Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 2: AudioCaps, Clotho 및 ACD 테스트 세트의 오디오-텍스트 검색 결과. &quot;기본&quot;, &quot;LA.&quot; “Wav.&quot;와 &quot;ACD&quot;는 각각 AudioCaps와 Clotho(기본), LAION-Audio-630K(LA), WavCaps(Wav) 및 Auto-ACD(ACD)의 조합을 나타냅니다. &quot;ACDvs&quot;는 VGGSound에서 큐레이션한 Auto-ACD의 하위 집합입니다. “* FT”는 대상 데이터 세트에서 모델을 미세 조정하는 것을 나타냅니다. AudioCaps 테스트 Clotho 테스트 Auto-ACD 테스트 텍스트→오디오 트레인 세트 모델 오디오→텍스트 텍스트 오디오 R@1 R@10 R@1 R@basic+LA.[59] basic+Wav.[38] HTSAT-ROBERTa 45.HTSAT-BERT 51.88.36.82.24.66.17.90.6 39.86.23.63.19.Audio-Text 텍스트→오디오 오디오-텍스트 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@55.4 20.0 65.0 17.9 59.58.basic+ACDys HTSAT-ROBERTa 50.5 90.39.86.24.62.9 20.basic+ACD basic+ACD*FT HTSAT-ROBERTa 53.HTSAT-ROBERTa 56.91.39.85.17.93.42.88.26.52.6 15.67.5 21.58.9 39.2 86.2 39.6 85.52.1 47.1 91.2 49.0 92.61.&#39;k&#39; 오디오-텍스트 쌍(a², c¹)이 주어지면 사전 학습된 오디오 인코더를 사용하여 오디오 특징 e² 2 = Aenc(a¹)을 추출하고 캡션을 토큰으로 변환합니다. 시퀀스, c½,..., c, 여기서 k는 텍스트의 최대 길이를 나타냅니다. 그런 다음 추출된 임베딩을 접두사 임베딩 세트로 변환하기 위해 매핑 네트워크 fmap을 설계합니다. pi = fmap (e¹²). 접두사 임베딩 세트를 자기 회귀 언어 모델로 다음 토큰을 예측하기 위한 조건으로 사용합니다. 따라서 학습하는 동안 올바른 단어를 예측하는 음의 로그 우도를 최소화합니다. L = = Ne Σ logo (c); | Pic,...,c&#39;;_1) i=1 j=여기서 는 학습 가능한 매개변수를 나타냅니다. 실험 이 섹션에서는 오디오 언어 검색, 오디오 캡션 및 제로 샷 분류의 세 가지 작업을 평가합니다. 5.1 오디오 언어 검색 5.1. 데이터 세트. AudioCaps, Clotho, Auto-ACDys 및 Auto-ACD와 같은 여러 데이터 세트에서 오디오 텍스트 검색 실험을 수행합니다. AudioCaps, Clotho 및 Auto-ACD의 훈련, 검증 및 테스트 세트에 대한 분포는 각각 50K/495/975, 3.8K/1045/1045 및 1.5M/2K/1K 데이터 쌍입니다.Auto-ACDys는 Auto-ACD의 하위 집합으로, VGGSound에서만 독점적으로 공급된 190K 데이터 쌍을 포함합니다.특히 Clotho와 AudioCaps(검증 및 테스트 세트)의 경우 각 데이터 쌍은 5개의 해당 캡션이 동반된 하나의 오디오 샘플로 구성되는 반면 나머지 데이터 쌍은 하나의 오디오 캡션 쌍으로만 구성됩니다.5.1.2 Auto-ACD 벤치마크.Auto-ACD 훈련 세트 외에도 검증 세트를 형성하기 위해 2K 데이터 샘플을 무작위로 선택하고 테스트 세트에 1K 샘플을 선택했습니다.언어 설명에서 잘못된 정보를 제거하고 부적절한 어휘 표현을 다시 작성하여 테스트 세트에 대한 수동 검증을 수행합니다. 이 테스트 세트는 오디오 언어 검색 및 자동 오디오 캡션 작업을 모두 평가하는 데 사용됩니다.5.1.3 메트릭. 데이터 세트의 풍부하고 정확한 정보를 검증하기 위해 일반적으로 사용되는 데이터 세트(예: AudioCaps 및 Clotho)에서 기존 메트릭인 Recall@k 성능을 비교합니다.또한 이러한 메트릭을 Auto-ACD 테스트 세트에 채택하여 포괄적인 개요를 제공합니다.5.1.4 학습 세부 정보.제안된 오디오-텍스트 검색(ATR) 모델을 20개 에포크 동안 학습시키고, 배치 크기는 768이며, 워밍업 단계와 코사인 학습 속도 감소 일정이 있는 1e-4의 초기 학습 속도를 사용하는 Adam 옵티마이저를 활용합니다.기존 CLAP 모델 구성과 동일한 하이퍼파라미터를 사용합니다. 오디오 인코더와 텍스트 인코더 출력의 차원은 모두 512입니다. 또한 문장의 단어에 25% 랜덤 마스킹을 도입하고 오디오 샘플의 50%에 노이즈 및 게인과 같은 증강을 랜덤하게 적용하여 모델 학습을 향상시킵니다. 15개 에포크에 대해 초기 학습률 2e-5로 Clotho 및 AudioCaps와 같은 특정 데이터 세트에서 모델을 추가로 미세 조정합니다. 5.1.5 결과. 표 2에서 볼 수 있듯이 다음과 같은 주요 관찰 결과를 도출할 수 있습니다. (i) Laion-Audio-630K에서 학습한 것과 비교할 때 제안된 Auto-ACDys 데이터 세트에서 학습하면 AudioCaps 및 Auto-ACD 벤치마크에서 Recall@k 메트릭이 상당히 향상됩니다. (ii) Auto-ACD에서 학습하고 Auto-ACD 벤치마크에 적용할 수 없는 특정 데이터 세트에서 미세 조정하면 성능이 현저히 향상됩니다. 이러한 개선은 AudioCaps의 테스트 세트에서 모델을 평가할 때 특히 두드러지는데, AudioCaps는 AudioSet의 하위 세트이고 Auto-ACD와 유사한 데이터 분포를 공유하기 때문입니다. 이러한 미세 조정 프로세스를 통해 모델은 오디오 및 텍스트 정보에 대한 보다 포괄적인 이해를 얻을 수 있으므로 검색 성능이 향상됩니다. (iii) 보다 다양한 어휘와 풍부한 언어 설명이 특징인 Auto-ACD 벤치마크에서 Auto-ACD 데이터 세트에 대한 학습은 Laion-Audio-630K에서 학습된 모델보다 상당히 우수한 성능을 보입니다. 5.2 자동 오디오 캡션 5.2. 데이터 세트. 섹션 5.1에서 언급한 데이터 세트 외에도 3.9K 오디오-텍스트 데이터 쌍으로 구성된 MACS 데이터 세트[36]도 사용합니다. 각 오디오에는 2~5개의 캡션과 여러 오디오 태그가 함께 제공됩니다. 전체적으로 Clotho, AudioCaps 및 MACS의 총 58k 데이터 쌍을 활용하여 자동 오디오 캡션 모델을 학습하고 Clotho 및 Auto-ACD 테스트 세트에서 평가합니다. MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 5.2.2 지표. Meteor[5], RougeL[30], Spider[34]와 같은 기존 캡션 지표 외에도 SentenceBERT[52]를 추가 평가 지표로 통합하여 어휘 정렬에만 의존하지 않고 캡션 콘텐츠의 의미적 유사성과 정확성을 우선시합니다.5.2.3 훈련 세부 정보. MLP와 transformer라는 두 가지 매핑 네트워크를 고안하고 훈련 과정에서 GPT의 매개변수를 미세 조정합니다. 접두사의 개수를 8로 설정하고, 각 접두사의 차원은 512입니다. 이 오디오 캡션 모델을 MACS[36], Clotho, AudioCaps에서 15개 에포크 동안 배치 크기 128, 초기 학습 속도 5e-4로 학습합니다. 이 작업에서 우리는 벤치마크 데이터 세트인 Clotho와 Auto-ACD에서 두 모델의 매핑 네트워크만 학습하여 사전 학습된 오디오-텍스트 검색 모델의 오디오 인코더와 사전 학습된 CLAP[59]의 오디오 인코더를 비교합니다. 5.2. 결과. 표 3에서 볼 수 있듯이 두 가지 관찰 결과를 도출할 수 있습니다. (i) 사전 학습된 오디오-텍스트 검색 모델에서 오디오 인코더를 초기화한 자동 오디오 캡션 모델은 모든 평가 지표에서 기준선보다 성능이 향상되었습니다. (ii) Auto-ACD에서 평가할 때 결과가 더 두드러집니다. 기준선 접근 방식의 성능은 Auto-ACD의 테스트 세트에서 급격한 감소를 감독합니다. 우리는 CLAP 모델에서 추출한 기준 특징에 환경 정보에 대한 자세한 설명이 없기 때문에 그렇다고 추측합니다. 사전 훈련된 오디오-텍스트 검색 모델을 기반으로 한 자막 모델은 상당한 성능 향상을 보이며 소리가 발생하는 위치를 정확하게 추론할 수 있습니다. 이 관찰 결과는 Auto-ACD가 광범위한 어휘를 보여주어 다양한 문장 구조를 사용하여 주어진 오디오를 묘사할 수 있음을 의미합니다. 반면에 데이터 세트에서 훈련된 모델은 소리가 나는 맥락을 추론할 수 있음을 보여줍니다. 표 3: Clotho 및 Auto-ACD 테스트 세트의 자동 오디오 자막 결과. &quot;S-BERT&quot;는 SentenceBERT를 나타내고 &quot;Env.&quot;는 예측된 자막에 환경 정보가 포함되어 있는지 여부를 나타냅니다. Clotho ☑ ☑ Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. 34.9 20.6 46.36.2 21.2 47.8.10.CLAP Ours 15.16.CLAP 9.Auto-ACD Ours 21.23.37.19.56.5.5.3.Zero-shot Classification ✓ 데이터 집합.Auto-ACD는 텍스트 설명에 환경 정보를 통합하는 것이 특징입니다.Auto-ACD에서 학습한 후, 4가지 시나리오에서 환경 분류를 수행합니다.(i) AudioSet 평가 집합의 샘플 모음으로, AudioSet 온톨로지 내의 &quot;음향 환경&quot;의 자식 클래스로 주석이 달려 있으며, AudioSet Env라고 합니다.데이터 유출을 방지하기 위해, 이 실험에서는 Auto-ACDys에서 사전 학습된 모델만 사용합니다.(ii) 이전에 DCASE 2020 챌린지에서 사용했던 DCASE 2020 Mobile이라고 하는 도시 음향 장면 데이터 집합[19]입니다. (iii) 인기 있는 도시 사운드 이벤트 분류 데이터 세트인 UrbanSound 8k [54]; (iv) 음악 장르 분류 데이터 세트인 GTZANGenres [57]. 5.3.2 지표. 우리는 기존의 의역 템플릿인 &quot;[환경 레이블]의 소리 / [레이블]의 소리&quot;를 사용하여 오디오텍스트 검색 실험으로 제로샷 분류에 접근합니다. 우리는 이 실험에서 환경 분류 결과를 평가하기 위한 지표로 Recall@1을 사용합니다. 5.3.3 결과. 표 4에 나와 있는 실험 결과는 CLAP에 비해 Auto-ACD에서 사전 학습된 ATR의 우수한 환경 인식 기능을 강조합니다. 특히 AudioSet Env에서 우리 모델은 AudioSet에서 우리의 훈련 데이터 세트로 데이터가 누출되지 않고 사전 학습을 위해 Auto-ACDys만 사용했음에도 불구하고 CLAP보다 상당히 우수한 성능을 보였으며, 이는 Auto-ACD의 풍부하고 정확한 환경 정보를 더욱 입증합니다. UrbanSound 8K와 GTZANGenres에 대한 결과는 오디오 이벤트 외에도 캡션에 더 많은 정보(예: 다양한 환경 설명, 세분화된 음악 장르)가 포함될 수 있음을 보여줍니다. 66*표 4: Zero-Shot Acoustic Environment Classification. Auto-ACDvs.의 사전 학습 모델을 나타냅니다. &quot;US-8K&quot;는 UrbanSound 8K를 나타냅니다. 모델 AudioSet Env DCASE US-8K GTZANGenres CLAP 75.76.19. Ours 39.5* 32.36.
--- CONCLUSION ---
31.45. 이 논문에서는 1.5M 데이터 쌍으로 구성된 대규모의 포괄적인 오디오 캡션 데이터 세트와 함께 오디오 캡션 생성을 위한 자동 파이프라인을 제시합니다. 나아가, 우리는 데이터 세트에서 다양한 오디오-언어 모델의 성능을 평가하여 효과를 인증하고, 오디오-언어 작업에 대한 벤치마크와 함께 수동으로 검증된 테스트 세트를 제공합니다. 이러한 실험 결과는 우리 데이터에 내재된 풍부한 정보와 정확한 설명을 밝혀내어 모델이 더욱 강력한 오디오-언어 표현을 학습할 수 있도록 돕습니다. 우리 데이터 세트의 일부가 자동 파이프라인을 통해 조달된 VGGSound에서 유래되었다는 사실 때문입니다. 온라인 비디오에서 정확한 오디오-언어 쌍으로의 변환은 완전히 자동화되고 복제 가능한 절차로 발전했습니다. 결과적으로 오디오-언어 데이터 세트의 확장된 코퍼스를 획득하는 것은 이제 간단한 작업입니다. 또한 오픈소스 컴퓨터 비전 모델과 대규모 언어 모델(LLM)이 지속적으로 개선되고 발전함에 따라 보다 정확한 시청각 지표를 추출하는 능력이 향상되어 추론의 정확도와 최종 오디오 캡션의 의역 품질이 향상됩니다.감사의 말 이 연구는 중국 국가중점연구개발프로그램(No.2022ZD0161400)의 지원을 받았습니다.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른.참고문헌 [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman. 2020. 자기 감독 다중 모달 다용도 네트워크.신경 정보 처리 시스템의 발전 33(2020), 25-37. [2] Relja Arandjelovic 및 Andrew Zisserman. 2017. 보고, 듣고, 배우세요. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 609-617. [3] Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband 및 Anthony T Chronopoulos. 2017. 특징 추출에서 분류까지 오디오 이벤트 감지 방법 개요. 응용 인공지능 31, 9-10(2017), 661-714. [4] Max Bain, Jaesung Huh, Tengda Han 및 Andrew Zisserman. 2023. WhisperX: 장문 오디오의 시간 정확한 음성 필사. INTERSPEECH 컨퍼런스 회의록(2023). [5] Satanjeev Banerjee 및 Alon Lavie. 2005. METEOR: 인간 판단과의 상관관계가 개선된 MT 평가를 위한 자동 메트릭. 기계 번역 및/또는 요약을 위한 내재적 및 외재적 평가 측정에 관한 ACL 워크숍의 진행 중. 65-72. [6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman. 2021. 시각적 소리를 어렵게 현지화하기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 16867-16876. [7] Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman. 2020. Vggsound: 대규모 오디오비주얼 데이터 세트. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 중. 721-725. [8] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 2022. HTS-AT: 사운드 분류 및 감지를 위한 계층적 토큰-의미 오디오 변환기. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 646-650. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. 2009. Imagenet: 대규모 계층적 이미지 데이터베이스. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항. 248-255. [10] Biyun Ding, Tao Zhang, Chao Wang, Ganjun Liu, Jinhua Liang, Ruimin Hu, Yulin Wu, Difei Guo. 2023. 음향 장면 분류: 포괄적 조사. 응용 분야별 전문가 시스템(2023), 121902. [11] Konstantinos Drossos, Samuel Lipping 및 Tuomas Virtanen. 2020. Clotho: 오디오 캡션 데이터 세트. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 736-740. [12] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang 및 Tong Lu. 2024. Avsegformer: 변압기를 사용한 오디오-비주얼 분할. AAAI 인공지능 컨퍼런스의 회의록, 제38권. 12155-12163. [13] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal 및 Marvin Ritter. 2017. 오디오 세트: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 776-780. [14] Di Hu Guangyao li, Yixin Xu. 2023. 오디오 질의응답을 위한 다중 스케일 주의. INTERSPEECH 컨퍼런스의 진행 사항(2023). [15] Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel. 2022. Audioclip: 클립을 이미지, 텍스트 및 오디오로 확장. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 진행 사항. 976-980. [16] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman. 2023. AutoAD II: 속편 - 영화 오디오 설명에서 누구, 언제, 무엇. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스의 진행 사항.[17] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman.2023. AutoAD: 맥락에서의 영화 설명.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항.18930-18940.[18] Tengda Han, Weidi Xie, Andrew Zisserman.2019. 밀도 예측 코딩을 통한 비디오 표현 학습.IEEE/CVF 국제 컴퓨터 비전 워크숍 컨퍼런스의 진행 사항.1-10.[19] Toni Heittola, Annamaria Mesaros, Tuomas Virtanen.2020. TAU Urban Acoustic Scenes 2020 Mobile, 개발 데이터 세트.https://doi.org/10.5281/zenodo.[20] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. 2017. 대규모 오디오 분류를 위한 CNN 아키텍처. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 131-135. [21] Xixi Hu, Ziyang Chen, and Andrew Owens. 2022. 믹스 및 로컬라이즈: 혼합물에서 음원 로컬라이징. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 10483-10492. བུ་སྐྱ་ཀླུ [22] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: 희소한 단서로부터의 효율적인 동기화. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스의 회의록. 5325-5329. [23] Simon Jenni, Alexander Black, and John Collomosse. 2023. 시간적 자기 감독을 통한 시청각 대조 학습. AAAI 인공지능 컨퍼런스의 회의록, Vol. 37. 7996-8004. [24] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. Audiocaps: Generating captions for audios in the wild. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 119-132. [25] A Sophia Koepke, Andreea-Maria Oncescu, João F Henriques, Zeynep Akata, Samuel Albanie. 2022. 자연어 쿼리를 통한 오디오 검색: 벤치마크 연구. IEEE 멀티미디어 저널 25(2022), 2675-2685. [26] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, Mark D Plumbley. 2020. Panns: 오디오 패턴 인식을 위한 대규모 사전 학습된 오디오 신경망. IEEE/ACM 오디오, 음성 및 언어 처리 저널 28(2020), 2880-2894. [27] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. 2023. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용하여 언어-이미지 사전 학습 부트스트래핑. 기계 학습 국제 회의록에서.1973019742. [28] Kang Li, Yan Song, Li-Rong Dai, Ian McLoughlin, Xin Fang, Lin Liu.2023. Ast-sed: 오디오 스펙트로그램 변환기를 기반으로 한 효과적인 사운드 이벤트 감지 방법.IEEE 음향, 음성 및 신호 처리 국제 회의록에서.1-5. Catr: [29] Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xiao.2023. 오디오-비주얼 비디오 분할을 위한 조합 종속 오디오 쿼리 변환기.제31회 ACM 멀티미디어 국제 회의록에서.1485-1494. [30] Chin-Yew Lin 및 Eduard Hovy.2003. n-gram 동시 발생 통계를 사용한 요약의 자동 평가. 북미 계산언어학 협회 인간언어기술 컨퍼런스 회의록. 150-157. [31] Samuel Lipping, Parthasarathy Sudarsanam, Konstantinos Drossos, Tuomas Virtanen. 2022. Clotho-aqa: 오디오 질의응답을 위한 크라우드소싱 데이터 세트. 제30회 유럽 신호 처리 컨퍼런스 회의록. 1140-1144. [32] Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, Weidi Xie. 2024. 주석 없는 오디오비주얼 분할. IEEE/CVF 컴퓨터 비전 응용 겨울 컨퍼런스 회의록. 5604-5614. [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu 등. 2023. 공룡 접지: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합. arXiv 사전 인쇄 arXiv:2303.05499 (2023). [34] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama 및 Kevin Murphy. 2017. 스파이더의 정책 그라데이션 최적화를 통해 이미지 캡션이 개선되었습니다. 컴퓨터 비전에 관한 IEEE 국제 회의 진행 중. 873-881. [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 및 Veselin Stoyanov. 2019. Roberta: 강력하게 최적화된 bert 사전 학습 접근법.arXiv 사전 인쇄본 arXiv:1907.(2019). [36] Irene Martin Morato 및 Annamaria Mesaros. 2021. 오디오 캡션 데이터 세트의 다양성 및 편향.음향 장면 및 이벤트의 감지 및 분류.90-94. [37] Xinhao Mei, Xubo Liu, Mark D Plumbley 및 Wenwu Wang. 2022. 자동 오디오 캡션: 최근 진행 상황 및 새로운 과제에 대한 개요.EURASIP 오디오, 음성 및 음악 처리 저널 2022, 1(2022), 26. [38] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou 및 Wenwu Wang. 2023. WavCaps: 오디오-언어 멀티모달 연구를 위한 chatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트. arXiv 사전 인쇄본 arXiv:2303.17395(2023). [39] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic. 2019. Howto 100m: 1억 개의 내레이션 비디오 클립을 시청하여 텍스트-비디오 임베딩 학습. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 2630-2640. [40] Shentong Mo 및 Pedro Morgado. 2022. 약하게 감독되는 시청각 소스 현지화에 대한 자세한 살펴보기. 신경 정보 처리 시스템의 발전(2022), 37524-37536. [41] Shentong Mo 및 Yapeng Tian. 2023. 혼합물에서 사운드 현지화를 위한 오디오-비주얼 그룹화 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10565-10574. [42] Shentong Mo 및 Yapeng Tian. 2023. AV-SAM: 모든 것을 분할하는 모델이 오디오-비주얼 현지화 및 분할을 충족합니다. arXiv 사전 인쇄본 arXiv:2305.(2023). [43] Ron Mokady, Amir Hertz 및 Amit H Bermano. 2021. Clipcap: 이미지 캡션을 위한 클립 접두사. arXiv 사전 인쇄본 arXiv:2111.09734(2021). [44] Michael Nigro 및 Sridhar Krishnan. 2023. SARdBScene: 오디오 장면 소스 계산 및 분석을 위한 데이터 세트 및 Resnet 기준선. 음향, 음성 및 신호 처리에 관한 IEEE 국제 회의 진행 중. 1~5. [45] Andreea-Maria Oncescu, A Koepke, Joao F Henriques, Zeynep Akata 및 Samuel Albanie. 2021. 자연어 쿼리를 통한 오디오 검색. arXiv 사전 인쇄 arXiv:2105.02192 (2021). [46] Aaron van den Oord, Yazhe Li 및 Oriol Vinyals. 2018. 대조 예측 코딩을 사용한 표현 학습. arXiv 사전 인쇄 arXiv:1807.03748 (2018). MM &#39;24, 2024년 10월 28일~11월 1일, 호주 VIC 멜버른. Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie [47] Andrew Owens 및 Alexei A Efros. 2018. 자기 감독 다중 감각적 특징을 사용한 시청각 장면 분석. 유럽 컴퓨터 비전 컨퍼런스의 회의록에서. 631-648. [48] Kamalesh Palanisamy, Dipika Singhania, Angela Yao. 2020. 오디오 분류를 위한 CNN 모델 재고. arXiv 사전 인쇄본 arXiv:2007.11154(2020). [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 국제 기계 학습 컨퍼런스의 회의록에서. 8748-8763. [50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그 1, 8(2019), 9. [51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125 1, 2(2022), 3. [52] Nils Reimers 및 Iryna Gurevych. 2019. Sentence-bert: 샴 bert 네트워크를 사용한 문장 임베딩. arXiv 사전 인쇄본 arXiv:1908.10084(2019). [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10684-10695. [54] Justin Salamon, Christopher Jacoby, Juan Pablo Bello. 2014. 도시 사운드 연구를 위한 데이터 세트 및 분류법. 제22회 ACM 국제 멀티미디어 컨퍼런스 회의록. 1041–1044. [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman 등. 2022. Laion-5b: 차세대 이미지-텍스트 모델을 학습하기 위한 대규모 오픈 데이터 세트. 신경 정보 처리 시스템의 발전 35(2022), 25278-25294. [56] Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, Nick Barnes. 2023. 거짓 부정 인식 대조 학습을 통한 시청각 소스 현지화 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 6420-6429. [57] George Tzanetakis 및 Perry Cook. 2002. 오디오 신호의 음악 장르 분류. IEEE 음성 및 오디오 처리 트랜잭션 10, 5(2002), 293–302. [58] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello. 2022. Wav2clip: 클립에서 강력한 오디오 표현 학습. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 45634567. [59] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 2023. 기능 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스의 회의록. 1-5. [60] Xuenan Xu, Mengyue Wu, Kai Yu. 2022. 자동 오디오 캡션에 대한 포괄적 조사. arXiv 사전 인쇄본 arXiv:2205.05357(2022). [61] Xuenan Xu, Zhiling Zhang, Zelin Zhou, Pingyue Zhang, Zeyu Xie, Mengyue Wu, Kenny Q Zhu. 2023. Blat: 오디오셋 태그 가이드 합성 데이터를 기반으로 한 언어-오디오 사전 학습 부트스트래핑. 제31회 ACM 국제 멀티미디어 컨퍼런스 논문집. 2756-2764. [62] Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, Yejin Choi. 2022. 시각적 지식 전송을 통해 병렬 데이터 없이 오디오와 텍스트 간의 점 연결. 계산 언어학 협회 북미 지부 컨퍼런스 논문집. 4492-4507. [63] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba. 2017. 장소: 장면 인식을 위한 1,000만 개 이미지 데이터베이스. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션 40, 6(2017), 1452-1464. [64] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, Yiran Zhong. 2022. 시청각 분할. 유럽 컴퓨터 비전 컨퍼런스 회의록. 386-403. [65] Xiao Zhou, Yujie Zhong, Zhen Cheng, Fan Liang, Lin Ma. 2023. 객체 재식별을 위한 적응적 희소 쌍 손실. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 19691–19701. [66] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi. 2024. 멀티모달 c4: 텍스트가 섞인 개방형 10억 규모 이미지 코퍼스. 신경 정보 처리 시스템의 발전 36(2024). Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 데이터셋 분석 이 섹션에서는 제안된 데이터셋인 Auto-ACD에 대한 보다 철저한 분석을 수행합니다. 섹션 7.1과 섹션 7.2에서는 Auto-ACD를 기존 오디오-언어 데이터셋과 비교하고 데이터 필터링의 필요성에 대해 논의합니다. 섹션 7.3에서는 어휘 분포를 제시합니다. 7.4절과 7.5절에서 Laion-Audio-630K, WavCaps, Auto-ACD의 캡션을 비교하고, Auto-ACD 하위 집합의 품질을 수동으로 검사합니다.7.6절에서 Auto-ACD의 추가 예를 제시합니다.7. 데이터 집합 통계 총 150만 개의 오디오 샘플을 수집했으며, 각각 10초 길이이고 자세한 캡션이 하나 첨부되었습니다.표 5에서 알 수 있듯이 다른 데이터 집합과 비교할 때 AutoACD는 볼륨 면에서 상당히 우수할 뿐만 아니라 평균 문장 길이가 더 깁니다.설명에 환경 정보를 포함하는 유일한 대규모 데이터 집합으로 자리 잡고 있습니다.Laion-Audio-630k는 어휘 수가 더 많을 수 있지만 대부분의 사전은 오디오 콘텐츠와 관련이 없는 사용자가 업로드한 장치 정보와 타임스탬프로 구성됩니다.표 5: 다른 오디오 캡션 데이터 집합과의 비교.&quot;길이&quot;와 &quot;# 어휘.&quot;는 평균 길이와 어휘를 나타냅니다. 영어: &quot;Env.&quot; 및 &quot;Auto.&quot;는 각각 환경 정보와 자동 파이프라인을 나타냅니다. 수량 길이 #Vocab. Env. Auto. 데이터 세트 AudioCaps [24] 57K 8.5K ☑ ✗ Clotho [11] 30K 11.4K ☑ LAION-Audio-630K [59] 630K 7.311K WavCaps [38] 400K 7.29K ☑ Auto-ACD(저희) 1.5M 18.22K 7. 데이터 세트 필터링 당사의 데이터 수집 절차는 강력한 오디오-비주얼 대응 관계에 의존합니다. 그러나 AudioSet 내의 많은 항목에는 상당한 노이즈가 포함되어 있어 이러한 일관성을 달성하는 데 어려움이 있습니다. 예를 들어 배경 음악이 있는 비디오, 고요한 연설, 게임 플레이를 묘사하는 비디오 또는 소프트웨어 튜토리얼이 있습니다. 이러한 비디오는 일반적으로 음성과 음악의 두 가지 유형의 오디오 이벤트만 포함합니다. 결과적으로 생성된 캡션은 종종 정보가 희소하고 오류율이 높습니다. 우리는 오디오-비주얼 레이블과 동기화 모델에 대한 분석을 사용하여 이러한 샘플을 필터링합니다. 이 필터링 프로세스의 구체적인 세부 사항은 본문의 섹션 3.3에 설명되어 있습니다. 그림 7에서 우리는 비디오 프레임 시퀀스의 몇 가지 예와 제외된 데이터에 대한 WhisperX [4]의 오디오 ASR(자동 음성 인식) 결과를 제시합니다. 대부분의 삭제된 항목은 오디오와 비디오가 관련이 없거나 동기화되지 않았기 때문입니다. 7. 데이터 세트 코퍼스 우리는 워드 클라우드로 데이터 세트의 캡션을 시각화합니다. 그림 8과 같이 일반적인 오디오 태그인 man speak와 music play가 여전히 데이터 내에서 빈도가 우세합니다. 작은 방과 음악 스튜디오와 같은 설정을 설명하는 용어도 상당한 빈도로 나타난다는 점이 주목할 만합니다. 이는 새 지저귐, 엔진 공회전, 물 튀김과 같은 수많은 오디오 이벤트로, Auto-ACD의 다양한 오디오 이벤트를 더욱 잘 보여줍니다. &quot;어쨌든, 그래서 나는 전쟁의 북소리와 한 지점을 제외하고는 거의 모든 팔을 달렸어... &quot;는 3을 5로 나눈 것과 같을 거야. 이제 세타의 탄젠트를 살펴보자. &quot;이제 두 가지 다른 방법으로 할 수 있습니다.&quot; [배경 음악만] Chitty Chitty Bang B Chiny Chitty Bang B Chitty Chitty Bang Ba Chi Chitty Bang [배경 음악만] 그림 7: 필터 처리에서 삭제된 샘플. 오른쪽의 텍스트는 WhisperX를 사용하여 처리한 비디오의 오디오에서 발화의 전사본을 나타냅니다. 음악 재생을 제안하는 시골 배경 Mengine 한가한 여자 노래 Upeople 이야기, 남자 speakmusic 스튜디오 레이블 Speechheavy retal 재생 작은 방 만들기 들리는 오락 아케이드 시각적 레이블 기타 strum 응원 벨 링 부드러운 음악 bo가 따라 말하다 열정적으로 전달하다 야외 배경 활기찬 분위기 방 만들기 그룹 차량 엔진 텔레비전 스튜디오 전동 공구 소리 남자 speak ergetic 표시된 활기찬 분위기 바람 불다 아기 울음 소리 오디오 시각적. draughing 엔진 run theat ous atmosphe 군중 응원 여자 speak man 열정적으로 사이렌 blare 오디오 태그 lause 자동차 hop 악기 전자 음악 ultiple times sing man speaking indho Speech le sing hop 기타 새 지저귐 오디오 이벤트 crophone 합창단 노래 대화 crowdtalking 음악 재생 Rain falls 자동차 엔진 큰 엔진 굉음이 작은 배경 속에서 큰 배경 음악이 멜로디한 분위기를 연출하고, 리스코테크 음악이 록 음악 배경을 암시하며 배경을 만들어냅니다.그림 8: Auto-ACD의 코퍼스. 발생 빈도가 높을수록 해당 단어의 글꼴 크기가 커집니다.7. 데이터 세트 비교 표 6에서는 동일한 오디오 샘플에 대한 LAION-Audio-630K, WavCaps 및 Auto-ACD의 예시 캡션을 보여줍니다.세 데이터 세트의 원래 사운드가 겹치므로 비교를 위해 다른 데이터 세트에서 동일한 오디오의 설명을 선택합니다.특히 LAION-Audio-630K는 키워드-캡션 모델을 사용하여 태그 레이블을 캡션으로 변환합니다.WavCaps는 ChatGPT를 사용하여 태그 레이블을 간단한 캡션으로 다시 표현합니다.LAIONAudio-630K 및 WavCaps의 캡션은 간결한 경향이 있으며 오디오 태그 외에 최소한의 정보만 포함합니다. 특히 LAION-Audio-630K의 캡션은 짧으며, 예를 들어 &quot;나무를 두드리는 것&quot;과 같이 상식에서 벗어나는 정보를 포함할 수 있습니다. 반면 WavCaps는 &quot;...소리가 들립니다&quot;와 같이 간단한 문장 구조를 보입니다. 이 두 데이터 세트의 캡션은 거의 나타나지 않습니다. MM &#39;24, 2024년 10월 28일-11월 1일, 멜버른, 빅토리아주, 호주. Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie 표 6: LAION-Audio-630K 및 WavCaps와의 캡션 비교, &quot;LA.&quot;, &quot;WavC.&quot; 및 &quot;ACD&quot;는 각각 LAION-Audio-630K, WavCaps 및 Auto-ACD를 나타냅니다. 아니요. 데이터 세트에서 생성된 캡션 LA. 사람이 나무를 두드리는 중입니다. 1. WavC. ACD. LA. 남자가 랩을 하면서 음악이 재생됩니다. 한 여성이 힙합 음악이 배경에서 재생되는 동안 노래하며 컴퓨터실에서 랩 오디오 이벤트를 만들어냅니다. 슬러시가 묻은 수련. 워터파크에서 물이 배경에서 흔들리면서 많은 사람들이 소리치고 환호합니다. 2. WavC. 스트림 소음, 군중 및 물보라 소리. ACD. LA. 3. WavC. 소방차 사이렌이 들립니다. ACD. 4. LA. WavC. ACD. 사이렌이 달린 트럭과 비상 시 소방차. 소방차가 주거 지역을 질주하면서 비상 차량 사이렌이 크게 울립니다. 엔진이 중간 주파수로 공회전하는 차량. 중간 엔진 소리가 들립니다. 중간 크기의 엔진이 공회전하고 진동하는 동안 성인 남성이 달리는 차량 근처에서 배경에서 말합니다. 오디오 태그보다 더 많은 정보. 이와 대조적으로 Auto-ACD는 더 긴 문장을 제공하고 오디오 장면에 대한 보다 포괄적인 설명을 제공합니다. 7. 데이터 세트 품질 검사 자동 ACD의 품질을 평가하기 위해 무작위로 샘플링한 1000개의 오디오-캡션 쌍에 대한 수동 검사를 수행합니다.표 7에서 볼 수 있듯이 (i) 생성된 캡션과 원본 오디오 간의 대응 관계를 평가합니다.(ii) 최종 캡션의 잘못된 단어를 수정하고 수정된 어휘의 백분율을 계산합니다.(iii) 색상과 같이 들리지 않는 정보가 포함된 캡션의 비율을 계산합니다.높은 대응 관계와 낮은 오류 단어 백분율이라는 결과는 제안하는 접근 방식이 최소한의 잘못된 정보 또는 들리지 않는 정보로 고품질의 확장 가능한 캡션 생성을 가능하게 한다는 것을 나타냅니다.표 7: 자동 ACD에 대한 수동 검사 통계.통계 대응 0.수정 0.들리지 않음 0.또한 캡션 생성 중 각 단계, 즉 시각적 단서를 생성하는 데 사용된 다양한 도구에 대한 수동 검사를 추가로 수행합니다. 무작위로 샘플링한 200개의 오디오-캡션 쌍에 대한 수동 검사를 수행하여 6개의 서로 다른 오픈소스 도구에서 나온 단서와 생성된 캡션의 품질을 분석합니다.표 8에서 볼 수 있듯이 오디오와 모순되는 단서를 잘못된 것으로 정의하고 각 도구와 캡션의 정확도를 계산하고 각 샘플에서 올바른 단서의 수를 센다.이러한 도구는 높은 정확도를 가지고 있으며 평균 정확도는 81.3%이고 가장 높은 정확도는 91.5%입니다.샘플의 94.0%에 적어도 4개의 올바른 단서가 포함되어 있음을 알 수 있습니다.생성된 캡션의 88.0%가 오디오와 일치한다는 사실은 LLM이 잘못된 정보를 제거하고 일관된 오디오 캡션을 생성할 수 있음을 더욱 잘 보여줍니다.7.데이터 세트 시각화표 9에서 볼 수 있듯이 VGGSound와 AudioSet의 오디오에 대해 생성된 캡션을 더 많이 보여줍니다.비디오 시퀀스를 제시하여 시각적 정보가 오디오에 대한 언어 설명을 어떻게 도울 수 있는지 보여줍니다. Auto-ACD의 캡션은 사운드 이벤트를 정확하게 묘사할 뿐만 아니라 오디오에서도 추론할 수 있는 시각적 사전 정보를 기반으로 추가 정보를 추론한다는 것을 알 수 있습니다.예를 들어, (i) &quot;활기찬 공연장&quot;, &quot;음악 스튜디오&quot; 및 &quot;평화로운 선정원&quot;과 같은 환경 세부 정보, (ii) &quot;민방위 사이렌이 크게 울린다&quot; 및 &quot;배경에서 음악이 흐른다&quot;와 같은 사운드 속성, (iii) &quot;오토바이 엔진이 위아래로 회전한다&quot; 및 &quot;자동차가 흙길을 질주한다&quot;와 같은 사운드 변화.표 8: 오픈소스 도구에서 수동 검사의 정확도. &quot;캡션.&quot;은 AudioCaption 모델을 나타냅니다.BLIP-2 DINO CLIP Place365 캡션.PANNS 0.755 0.805 0.정확도 0.0.0.Auto-ACD MM &#39;24, 2024년 10월 28일-11월 1일, 호주 빅토리아주 멜버른. 표 9: Auto-ACD에서의 데이터 시각화. 각 샘플에서 맨 위 줄은 비디오 프레임 시퀀스를 보여주고, 맨 아래 줄은 해당 오디오 캡션을 보여줍니다. 캡션의 사운드 이벤트는 굵은 글씨로 강조 표시되고, 환경 정보는 기울임꼴로 표시됩니다. 아니요. 생성된 캡션 1. 한 남자가 음악 스튜디오에서 컨트리 음악과 드럼 소리에 맞춰 기타를 치며 노래합니다. 2. 2010 Cruz N 3. 4. ©2016 Cruz ©2016 Cruz No ©2010 Cruz N ©2018 Cruz N 2016 Cruz 민방위 사이렌이 크게 울려 도시나 도시 환경에서 비상 상황을 알립니다. 오토바이 엔진이 주거 지역을 달리면서 위아래로 회전하며 말소리와 가벼운 엔진 소리가 들립니다. 많은 사람들이 환호하는 가운데 배경에서 음악이 재생되어 콘서트에서 활기찬 분위기를 조성합니다. 5. 밤에 비포장 도로를 질주하는 차에서 큰 엔진 소리가 들립니다. 6. 노래하는 그릇의 소리가 공명하고, 사인파의 희미한 음조와 음叉 소리가 평화로운 선정원에서 울려 퍼진다. 7. 8. 사람들의 무리가 환호하고 노래하는 가운데, 도시적인 전투 함성이 배경에서 울려 퍼진다. 음악이 흐르고 군중이 환호하고 밴드가 무대에서 연주하며, 활기찬 공연장에서 밝은 조명이 비추고 있다.
