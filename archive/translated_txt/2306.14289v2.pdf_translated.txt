--- ABSTRACT ---
Segment Anything Model(SAM)은 인상적인 제로 샷 전송 성능과 다양한 비전 애플리케이션(예: 세밀한 제어를 통한 이미지 편집)에 대한 높은 다재다능성으로 인해 상당한 주목을 받았습니다. 이러한 애플리케이션 중 다수는 모바일 폰과 같은 리소스 제약이 있는 에지 디바이스에서 실행되어야 합니다. 이 연구에서는 무거운 이미지 인코더를 가벼운 인코더로 대체하여 SAM을 모바일 친화적으로 만드는 것을 목표로 합니다. 원래 SAM 논문에서와 같이 새로운 SAM을 훈련하는 순진한 방법은 특히 제한된 훈련 소스를 사용할 때 만족스럽지 못한 성능을 초래합니다. 이는 주로 이미지 인코더와 마스크 디코더의 결합된 최적화로 인해 발생하며, 이를 동기로 분리된 증류를 제안합니다. 구체적으로 무거운 이미지 인코더(원래 SAM의 ViT-H)의 지식을 가벼운 이미지 인코더로 증류하여 원래 SAM의 마스크 디코더와 자동으로 호환될 수 있습니다. 훈련은 하루도 채 걸리지 않고 단일 GPU에서 완료할 수 있으며, 결과적으로 가벼운 SAM은 MobileSAM이라고 하며, 60배 이상 작지만 원래 SAM과 동일한 성능을 보입니다. 추론 속도의 경우, 단일 GPU에서 MobileSAM은 이미지당 약 10ms로 실행됩니다. 이미지 인코더에서 8ms, 마스크 디코더에서 4ms입니다. 뛰어난 성능으로 MobileSAM은 동시 FastSAM보다 약 5배 빠르고 7배 작아 모바일 애플리케이션에 더 적합합니다. 게다가 MobileSAM은 CPU에서 비교적 원활하게 실행될 수 있음을 보여줍니다. 저희 프로젝트의 코드는 MobileSAM에서 제공되며, 데모에서는 MobileSAM이 CPU에서 비교적 원활하게 실행될 수 있음을 보여줍니다. 1
--- INTRODUCTION ---
ChatGPT Zhang et al. [2023a]는 NLP 분야에 혁명을 일으켜 생성 AI(AIGC, 일명 인공 지능 생성 콘텐츠) Zhang et al. [2023b]에서 획기적인 진전을 이루었습니다.이를 가능하게 한 것은 GPT 시리즈 모델 Brown et al. [2020], Radford et al. [2018, 2019]에 있습니다.이는 Bommasani et al. [2021]이 웹 규모 텍스트 데이터 세트에서 학습한 기초 모델입니다.NLP에서 기초 모델의 성공에 이어 여러 연구 He et al. [2020], Qiao et al. [2023a], Zhang et al. [2022a]는 대조 학습을 통해 텍스트 인코더와 함께 이미지 인코더를 학습했습니다.He et al. [2020], Zhang et al. [2022b].아주 최근에 Meta Research 팀은 &quot;Segment Anything&quot; 프로젝트를 발표했습니다.Kirillov et al. [2023], SAM이라는 프롬프트 가이드 비전 기초가 제안되었고 비전에 대한 GPT 모멘트로 여겨진다. SAM은 순서대로 작동하는 ViT 기반 이미지 인코더와 프롬프트 가이드 마스크 디코더의 두 가지 구성 요소로 구성된다(그림 1 참조). SAM은 등장한 이래로 여러 가지 이유로 상당한 주목을 받았다. 첫째, 비전이 NLP를 따라 기초 모델과 프롬프트 엔지니어링을 결합하는 경로를 추구할 수 있음을 보여준 최초의 모델이다. 둘째, 레이블 예측과 병행되는 기본적인 비전 작업인 레이블 없는 분할을 수행한 최초의 모델이다. Zhang et al. [2023c]. 또한 이 기본적인 작업으로 인해 SAM은 다른 모델과 호환되어 고급 비전 애플리케이션을 실현할 수 있습니다.* chaoningzhang1990@gmail.com을 통해 저자에게 문의하실 수 있습니다.이미지 이미지 ViT 기반 이미지 인코더 이미지 인코더(632M) 중량 임베딩 프롬프트 가이드 마스크 디코더 프롬프트 마스크 디코더(3.87M) ↑ 프롬프트 인코더(0.006M) 경량 그림 1: Segment Anything 모델 개요. 텍스트 가이드 세분화 및 세밀한 제어가 가능한 이미지 편집과 같은 경우입니다. 그러나 이러한 사용 사례 중 다수는 모바일 앱과 같이 리소스가 제한된 에지 장치에서 실행해야 합니다. 공식 데모에서 보듯이 처리된 이미지 임베딩을 사용하면 마스크 디코더가 가볍기 때문에 SAM이 리소스가 제한된 장치에서 작동할 수 있습니다. SAM 파이프라인 계산을 무겁게 만드는 것은 거대한 이미지 인코더에 있습니다. 이 작업에서 우리는 리소스가 제한된 모바일 장치에 적합한 가벼운 SAM을 얻는 방법을 조사합니다. 따라서 이를 MobileSAM이라고 합니다. 표 1: 다양한 이미지 인코더를 사용한 SAM의 매개변수. 매개변수 ViT 기반 인코더 프롬프트 가이드 인코더 SAM(VIT-H) SAM(VIT-L) SAM(VIT-B) 632M 0.006M 307M 0.006M 86M 0.006M SAM의 기본 이미지 인코더가 ViT-H 기반이라는 점을 감안할 때 MobileSAM을 얻는 간단한 방법은 Kirillov et al. [2023]의 공식 파이프라인을 따라 더 작은 이미지 인코더로 새 SAM을 다시 학습시키는 것입니다. 예를 들어 ViT-H를 더 작은 ViT-L 또는 더 작은 ViT-B로 교체하는 것입니다. 다양한 이미지 인코더 변형을 사용한 SAM의 매개변수는 표 1에 요약되어 있습니다. Kirillov et al. [2023]에서 언급했듯이 ViT-L 또는 ViT-B를 이미지 인코더로 사용하여 새 SAM을 학습하려면 며칠 동안 128개의 GPU가 필요합니다. 이러한 리소스 집약적 재교육은 결과를 재현하거나 개선하는 데 있어 사소한 부담이 될 수 있습니다. 이러한 최적화 어려움은 주로 이미지 인코더와 마스크 디코더의 결합된 최적화에서 비롯됩니다. 이러한 이해에 따라 이미지 인코더와 마스크 디코더의 최적화를 분리하는 것을 제안합니다. 구체적으로, 먼저 기본 이미지 인코더 ViT-H에서 작은 ViT로 지식을 추출합니다. 그런 다음 원래 SAM의 마스크 디코더를 미세 조정하여 추출된 이미지 인코더와 더 잘 정렬할 수 있습니다. 정렬 최적화는 선택 사항이라는 점을 강조할 가치가 있는데, 가벼운 이미지 인코더가 기본 이미지 인코더에서 추출된다는 사실은 기본 마스크 디코더와의 고유한 정렬을 보장하기 때문입니다. 새로운 SAM 파이프라인을 찾는 문제를 분리된 증류로 전환함으로써, 우리의 접근 방식은 간단하고 효과적이며 저렴한 비용(하루도 채 걸리지 않는 단일 GPU에서)으로 재현 가능하다는 장점이 있습니다. 그 결과 MobileSAM은 인코더 매개변수를 100배, 전체 매개변수를 60배까지 줄입니다. 놀랍게도, 이렇게 가벼운 MobileSAM은 원래의 무거운 SAM과 동등한 성능을 보이는데, 이는 모바일 애플리케이션에 SAM을 적용하기 위한 중요한 단계입니다. MobileSAM을 사용한 추론의 경우, 단일 이미지가 약 10ms에 실행됩니다. 이미지 인코더에서 8ms, 마스크 디코더에서 4ms입니다. MobileSAM이 동시 FastSAM인 Zhao et al. [2023]보다 약 5배 빠르고 7배 작으면서도 뛰어난 성능을 달성한다는 점을 강조할 가치가 있습니다. 2
--- RELATED WORK ---
SAM: 일반화와 다재다능함. 올해 4월 초에 처음 등장한 이래로 다양한 관점에서 SAM을 조사하는 수많은 프로젝트와 논문이 등장했습니다. SAM은 모든 것을 분할한다고 주장하기 때문에 의료 이미지 Ma와 Wang [2023], Zhang et al. [2023d], 위장된 물체 Tang et al. [2023], 투명한 물체 Han et al. [2023]을 포함하여 실제 상황에서의 성능을 보고한 연구가 많이 있습니다. 연구 결과에 따르면 SAM은 일반적인 설정에서는 잘 작동하지만 위에서 언급한 어려운 설정에서는 작동하지 않는 것으로 나타났습니다. 또 다른 중요한 연구 방향은 SAM을 향상시켜 실용성을 개선하는 데 중점을 두었습니다. Attack-SAM Zhang et al. [2023e]은 SAM의 출력 마스크가 악의적으로 생성된 적대적 섭동을 통한 적대적 공격으로 쉽게 조작될 수 있음을 보여주었습니다. 또 다른 연구인 Qiao et al. [2023b]는 스타일 전이와 일반적인 손상에서 국소 폐색 및 적대적 섭동에 이르기까지 SAM의 포괄적인 견고성 평가를 추가로 수행합니다. Qiao et al.에서 발견됩니다. [2023b] SAM은 견고성이 높지만 적대적 섭동에는 적합하지 않으며 이는 Zhang et al. [2023e]의 결과와 잘 일치합니다. 또 다른 연구 분야는 SAM의 다재다능함을 보여주는 데 중점을 둡니다. Grounded SAM IDEA-Research[2023]는 Grounding DINO Liu et al. [2023a]와 SAM을 결합하여 텍스트 입력이 있는 모든 것을 분할하는 선구적인 연구입니다. 구체적으로 Grounding DINO에 의존하여 텍스트에서 경계 상자를 생성한 다음 생성된 상자를 프롬프트로 사용하여 마스크를 분할할 수 있습니다. SAM은 레이블이 없는 마스크를 예측하고 여러 작업 Chen et al. [2023], Park [2023]는 SAM을 CLIP Radford et al.과 같은 다른 모델과 결합합니다. [2021] 의미적으로 무엇이든 분할합니다. 객체 분할을 넘어 여러 연구에서 이미지 편집 Rombach et al. [2022], 인페인팅 작업 Yu et al. [2023], 비디오 내 객체 추적 Yang et al. [2023], Zxyang [2023]을 포함한 다른 분야에서도 다재다능함을 보여주었습니다. 2D 비전을 넘어 SAM에 대한 연구는 3D 객체 재구성으로 확장되었습니다 Shen et al. [2023], Kang et al. [2022], 단일 이미지에서 3D 모델 생성을 지원하는 기능을 보여주었습니다. SAM에 대한 전체 설문 조사는 Zhang et al. [2023c]를 참조하는 것이 좋습니다. ViT: 가볍고 효율적입니다. 초기 모바일 비전 애플리케이션은 주로 MobileNet Howard et al. [2017] 및 개선된 varinats Sandler et al. [2018], Howard et al. [2019]. MobileNet의 핵심 아이디어는 일반적인 합성곱 블록을 깊이별 합성곱과 점별 합성곱으로 분리하는 데 있으며, 이를 통해 모드 매개변수와 계산 시간이 크게 단축됩니다. ViT Dosovitskiy et al. [2021]의 출현 이후 수많은 연구에서 이를 가볍고 효율적으로 만들려고 시도했습니다. 원래 ViT 논문 Dosovitskiy et al. [2021]의 ViT-Huge(ViT-H), ViT-Large(ViT-L), ViT-Base(ViT-B)를 보완하기 위해 Touvron et al. [2020]에서 더 작은 ViT가 도입되었으며 Deit-Small(Deit-S) 및 Deit-Tiny(Deit-T) ViT-Small 및 ViT-Tiny로 표시됩니다. Mobile ViT Mehta 및 Rastegari [2021]는 ViT를 표준 합성곱과 결합하여 성능을 개선하는 선구적인 작업으로, MobileNet v2 Sandler et al. [2018]보다 성능이 우수합니다. 주된 동기는 CNN의 지역적 표현 능력을 활용하는 것이며, 이 관행에 이어 EfficientFormer Li et al. [2022a], Efficient ViT Liu et al. [2023b], Next-ViT Li et al. [2022b] 및 Tiny-ViT Wu et al. [2022]를 포함하여 모델 속도를 향상시키는 것을 목표로 하는 여러 후속 작업이 이어졌습니다. 가볍고 빠른 ViT의 최근 진전은 리소스가 제한된 모바일 기기에 적합한 차세대 SAM을 만들기 위한 제안된 분리 증류를 보완합니다. 3 모바일 친화적 SAM 3.1 배경 및 프로젝트 목표는 SAM에 대한 배경을 생성합니다. 여기서 먼저 SAM의 구조와 작동 방식을 요약합니다. SAM은 ViT 기반 이미지 인코더와 프롬프트 가이드 마스크 디코더로 구성됩니다. 이미지 인코더는 이미지를 입력 및 임베딩으로 사용하여 마스크 디코더에 공급합니다. 마스크 디코더는 점(또는 상자)과 같은 프롬프트를 기반으로 배경에서 모든 객체를 잘라내는 마스크를 생성합니다. 또한 SAM은 모호성 문제를 해결하기 위해 동일한 프롬프트에 대해 여러 마스크를 생성할 수 있으므로 귀중한 유연성을 제공합니다. 이를 고려하여 이 작업에서는 먼저 ViT 기반 인코더를 채택하여 이미지 임베딩을 생성한 다음 프롬프트 가이드 디코더를 채택하여 원하는 마스크를 생성하는 SAM의 파이프라인을 유지합니다. 이 파이프라인은 &quot;모든 것을 분할&quot;하기 위해 최적으로 설계되었으며, 이는 다운스트림 작업인 &quot;모든 것을 분할&quot;하는 데 사용할 수 있습니다(자세한 내용은 4.3절 참조). 프로젝트 목표. 이 프로젝트의 목표는 가벼운 방식으로 만족스러운 성능을 달성하고 원래 SAM보다 훨씬 빠른 모바일 친화적 SAM(MobileSAM)을 생성하는 것입니다. 원래 SAM의 프롬프트 가이드 마스크 디코더는 4M 미만의 매개변수를 가지므로 가벼운 것으로 간주됩니다. 공개 데모에서 보여지는 것처럼 인코더에서 처리된 이미지 임베딩이 주어지면 마스크 디코더가 가볍기 때문에 SAM은 리소스가 제한된 장치에서 작동할 수 있습니다. 그러나 원래 SAM의 기본 이미지 인코더는 600M 이상의 매개변수를 가진 ViT-H를 기반으로 하며, 이는 매우 무겁고 전체 SAM 파이프라인을 모바일 장치와 호환되지 않게 만듭니다. 따라서 모바일 친화적인 SAM을 얻는 핵심은 무거운 이미지 인코더를 가벼운 인코더로 대체하는 데 있으며, 이는 원래 SAM의 모든 기능과 특성을 자동으로 유지합니다. 다음에서 제안된 내용을 자세히 설명합니다.
--- METHOD ---
이 프로젝트 목표를 달성하기 위해.3.2 제안된 방법 결합 증류.프로젝트 목표를 실현하는 간단한 방법은 Kirillov et al. [2023]의 공식 파이프라인을 따라 더 작은 이미지 인코더로 새 SAM을 재교육하는 것입니다.Kirillov et al. [2023]에서 언급했듯이 ViT-H 이미지 인코더로 SAM을 학습하려면 256개의 A100 GPU에서 68시간이 걸립니다.ViT-H를 ViT-L 또는 ViT-B로 교체하면 필요한 GPU가 128개로 줄어들지만 이는 커뮤니티의 많은 연구자들이 결과를 재현하거나 개선하기에 여전히 사소한 부담입니다.이러한 접근 방식에 따라 훨씬 더 작은 이미지 인코더를 채택하고 제공된 11-T 분할 데이터 세트로 새 SAM을 재교육할 수 있습니다.제공된 데이터 세트의 마스크는 사전 학습된 SAM(ViT 이미지 인코더 포함)에서 제공됩니다.본질적으로 이 재교육 프로세스는 Hinton et al.의 지식 증류입니다. [2015], ViT-H 기반 SAM에서 더 작은 이미지 인코더가 있는 SAM으로 지식을 전송합니다(그림 2 왼쪽 참조).이미지 VIT 기반(큰) 이미지 인코더 Teacher SAM 프롬프트 가이드 마스크 마스크 디코더 증류 이미지 VIT 기반(큰) 이미지 인코더 Teacher SAM 프롬프트 가이드 마스크 디코더 마스크 복사 증류 VIT 기반(작은) 이미지 인코더 프롬프트 가이드 마스크 디코더 마스크 VIT 기반(작은) 이미지 인코더 프롬프트 가이드 마스크 디코더 마스크 학습 가능 학습 가능 학습 가능 냉동 그림 2: SAM의 결합된 지식 증류.왼쪽 하위 그림은 완전히 결합된 증류를 나타내고 오른쪽 하위 그림은 반결합된 증류를 나타냅니다.반결합에서 분리된 증류로.원래 SAM에서 더 작은 이미지 인코더가 있는 SAM으로 KD를 수행할 때 어려움은 주로 이미지 인코더와 결합된 디코더의 결합된 최적화에 있습니다.직관적으로 이미지 인코더의 최적화는 이미지 디코더의 품질에 따라 달라지고 그 반대의 경우도 마찬가지입니다. SAM의 두 모듈이 모두 나쁜 상태일 때, 둘 다 좋은 상태로 훈련시키는 것이 더 어렵습니다. 분할 정복 알고리즘 Zhang et al. [2022c]에서 영감을 얻어 KD 작업을 이미지 인코더 증류와 마스크 디코더 미세 조정의 두 가지 하위 작업으로 나누는 것을 제안합니다. 구체적으로, 먼저 ViT-H의 지식을 더 작은 인코더로 전송하여 이미지 인코더에서 KD를 수행합니다. 원래 SAM의 마스크 디코더는 이미 가볍기 때문에 아키텍처를 유지할 계획입니다. 이렇게 하면 처음부터 훈련하는 대신 미세 조정을 위해 쉽게 사용할 수 있는 결합 디코더의 이점이 있습니다. 결합 증류의 최적화 문제를 완화하기 위한 간단한 방법은 복사되고 동결된 마스크 디코더로 이미지 인코더를 최적화하는 것입니다(오른쪽 그림 2 참조). 동결 작업은 마스크 디코더의 품질이 불량한 이미지 인코더로 인해 저하되는 것을 방지하는 데 도움이 될 수 있습니다. 이 증류를 반결합이라고 부르는 이유는 이미지 인코더의 최적화가 아직 마스크 디코더에서 완전히 분리되지 않았기 때문입니다. 경험적으로, 우리는 프롬프트의 선택이 무작위적이기 때문에 이 최적화가 여전히 어렵다는 것을 발견했고, 이는 마스크 디코더를 가변적으로 만들어 최적화의 어려움을 증가시킵니다.따라서 우리는 결합된 디코더에 의존하지 않고 원래 SAM의 ViT-H에서 직접 작은 이미지 인코더를 증류하는 것을 제안합니다.이를 분리된 증류라고 합니다(그림 3 참조).이미지 임베딩에 증류를 수행하는 또 다른 장점은 Kirillov 등[2023]에서와 같이 마스크 예측을 위해 Lin 등[2017]과 주사위 손실 Milletari 등[2016]의 조합을 사용하는 대신 간단한 MSE 손실을 채택할 수 있다는 것입니다.미세 조정(선택 사항) VIT 기반(대형) 이미지 인코더 이미지 임베딩 프롬프트 가이드 마스크 디코더 마스크 증류 VIT 기반(소형) 이미지 인코더 이미지 임베딩 이미지 그림 3: SAM에 대한 분리된 증류.복사 프롬프트 가이드 마스크 디코더 마스크 마스크 디코더 미세 조정의 필요성에 대해. 반결합 증류와 달리, 위의 분리 증류는 원래의 동결 마스크 디코더와 잘 맞지 않을 수 있는 가벼운 이미지 인코더를 생성합니다. 경험적으로, 학생 이미지 인코더에서 생성된 이미지 인코딩이 원래 교사 인코더의 이미지 인코딩과 충분히 가까울 수 있기 때문에 이는 사실이 아니라는 것을 발견했으며, 이는 두 번째 단계에서 결합된 디코더의 미세 조정을 선택 사항으로 만듭니다. 동결된 가벼운 이미지 인코더에서 마스크 디코더를 미세 조정하거나 두 가지를 함께 미세 조정하면 성능이 더욱 향상될 것으로 예상됩니다. 예비 평가. 여기서는 결합 증류와 분리 증류를 비교하기 위한 예비 조사를 수행합니다. 여기서 성능 평가를 위해 교사 SAM과 학생 SAM이 동일한 프롬프트 지점에서 생성한 두 마스크 간의 mIoU를 계산합니다. 직관적으로, 더 높은 mIoU는 ViT-H가 생성한 마스크가 기준 진실이라고 가정하여 더 높은 마스크 예측 성능을 나타냅니다. 결합 증류의 경우, 원래 SAM Kirillov et al에서 제공된 ViT-B가 있는 SAM을 채택합니다. [2023]. 180k 반복을 위해 GPU(GPU당 1개 샘플)에서 SA-1B(11M개 이미지)에 대해 학습했습니다. 반면, 분리된 증류 설정에서 우리는 55k 반복을 위해 SA-1B 데이터 세트(11k) 이미지의 0.1% 샘플에서 GPU(GPU당 2개 샘플)에서 모델을 학습했습니다. 전반적으로 분리된 증류는 결합된 증류보다 1% 미만의 계산 리소스를 사용하면서 결합된 sit(200개 샘플 평균)의 0.72 대비 0.75의 mIoU라는 우수한 성능을 달성했습니다. ViT-B는 여전히 모바일 기기에 대한 사소한 부담이 아니기 때문에 다음에서 우리는
--- EXPERIMENT ---
영어: Tiny ViT(5M 매개변수 포함)를 사용한 Wu et al. [2022]은 제안된 분리 증류를 기반으로 합니다.표 2: 이미지 인코더로 ViT-B를 사용한 SAM의 결합 증류와 분리 증류 비교.분리 증류는 결합 증류보다 성능이 우수하고 1% 미만의 계산 리소스가 필요합니다.SAM(결합 증류) SAM(분리 증류) MIOU 학습 GPU 배치 크기 반복 학습 데이터 0.0.180k 55k 11M 11K 4 실험 4.1 실험 설정 표 3: 원래 SAM과 MobileSAM의 이미지 인코더에 대한 매개변수와 속도 비교.추론 속도는 단일 GPU에서 측정됩니다.원래 SAM MobileSAM 매개변수 속도 5.78M 8ms 632M 452ms 경량 이미지 인코더. 본 프로젝트의 목표는 기본 ViT-H를 모바일 기기용 경량 이미지 인코더로 대체하여 효율적인 SAM을 얻는 것입니다. ViT 기반 백본인 ViT-Tiny는 Deit-Tiny와 유사한 매개변수를 갖지만 성능이 더 좋습니다. 예를 들어, ImageNet-1K에서 Deit-Yiny는 72.2%의 정확도를 달성하는 반면 ViT-Tiny는 79.1%를 달성합니다. 따라서 원래 SAM보다 훨씬 빠를 수 있는 경량 MobileSAM을 학습하기 위해 제안된 분리 증류의 효과를 보여주기 위한 개념 증명을 위해 ViT-Tiny를 채택합니다. 채택된 경량 이미지 인코더는 점차적으로 해상도를 줄이는 4단계로 구성됩니다. 첫 번째 단계는 역 잔차가 있는 합성 블록으로 구성되고 Sandler et al. [2018], 나머지 3단계는 변환기 블록으로 구성됩니다. 모델 시작 부분에는 해상도를 다운샘플링하기 위한 스트라이드가 2인 합성 블록이 2개 있습니다. 다른 단계 간의 다운샘플링 연산은 스트라이드 2의 합성 블록으로 처리됩니다.Wu et al. [2022]와 달리 최종 다운샘플링 합성에서 스트라이드 2를 1로 설정하여 최종 해상도를 원래 SAM의 ViT-H 이미지 인코더와 일치시켰습니다.MobileSAM의 매개변수 추론 속도는 표 3에 요약되어 있습니다.섹션 2에서 설명한 다른 효율적인 이미지 인코더도 이미지 인코더로 채택할 수 있습니다.학습 및 평가 세부 정보.이미지 인코더의 분리된 KD의 경우, 단일 GPU에서 8개 에포크 동안 Kirillov et al. [2023]의 SA-1B 데이터 세트의 1%로 경량 인코더를 학습합니다.채택한 학생 이미지 인코더(위 참조)보다 훨씬 무겁다는 점을 고려할 때 교사 이미지 인코더의 순방향 프로세스에 더 많은 계산이 사용된다는 것을 관찰했습니다.증류를 더 빠르게 하기 위해 Wu et al.의 관행을 따릅니다. [2022] 미리 이미지 임베딩을 저장하여 포워드 프로세스를 한 번만 실행하면 됩니다. 단일 GPU로 하루도 채 걸리지 않고 MobileSAM을 얻을 수 있습니다. 더 많은 GPU로 MobileSAM을 더 오랜 시간 동안 학습하면 더 나은 성능을 얻을 수 있을 것으로 예상됩니다. 마스크 디코더 미세 조정을 수행하는 초기 조사는 MobileSAM의 성능을 더욱 개선하지만, 단순화를 위해 이 버전의 논문에서는 이 단계를 생략합니다. 증류된 SAM의 정량적 평가를 위해 원래 SAM과 MobileSAM에서 예측한 마스크 사이의 mIoU를 계산합니다. (a) 원래 SAM (b) MobileSAM THE STAG STAGE (c) 원래 SAM 그림 4: 프롬프트로 단일 포인트를 사용한 마스크 예측. 4.2 MobileSAM은 원래 SAM과 동일한 성능을 보입니다. JATNOUE (d) MobileSAM 주요 결과에 대해 포인트와 상자의 두 가지 유형의 프롬프트로 예측된 마스크를 보고합니다. 영어: SAM의 공식 github 프로젝트가 텍스트 가이드 마스크 디코더에 대한 사전 학습된 모델을 제공하지 않기 때문에 텍스트 프롬프트가 있는 결과는 보고하지 않습니다. 프롬프트로 점을 사용한 결과는 그림 4에 표시되고 프롬프트로 상자를 사용한 결과는 그림 5에 표시됩니다. MobileSAM이 원래 SAM과 유사한 만족스러운 마스크 예측을 하는 것을 관찰했습니다. (a) 원래 SAM (b) MobileSAM (c) 원래 SAM (d) MobileSAM 그림 5: 프롬프트로 상자를 사용한 마스크 예측.절제 연구. 여기서는 SAM 성능에 대한 학습 계산의 영향에 대한 절제 연구를 수행합니다. 표 4의 결과는 동일한 반복 횟수에서 배치 크기를 늘리면 모델 성능이 향상됨을 보여줍니다. 게다가 배치 크기에서 학습 에포크를 늘려 업데이트 반복 횟수를 늘리면 성능이 향상됩니다. 모든 실험은 단일 GPU에서 수행된다는 점에 유의하세요. 더 큰 배치 크기를 허용하기 위해 GPU 수를 늘리거나 반복 횟수를 더 늘리면 성능이 더욱 향상될 것으로 예상합니다. 4.3 MobileSAM은 FastSAM보다 성능이 우수함 표 4: MobileSAM 성능에 대한 학습 계산의 영향에 대한 절제 연구.배치 크기 에포크 반복 횟수 mIoU50k 0.50k 0.100k 0.표 5: segment anything과 segment everything의 비교.개체 수 prompt-aware anything everythingN yes no Segment anything vs segment everything.원래 SAM 논문 Kirillov et al. [2023]의 제목이 &quot;segment everything&quot;이 아니라 &quot;segment anything&quot;인 점에 유의하세요.Kirillov et al. [2023]에서 강조했듯이, SAM은 &quot;모든 분할 프롬프트가 주어지면 유효한 분할 마스크를 반환하는&quot; 프롬프트 가능 분할 작업을 수행합니다(Kirillov et al. [2023]에서 인용).프롬프트의 역할은 이미지에서 무엇을 분할할지 지정하는 것입니다. 이론상 프롬프트가 적절하게 설정되는 한 모든 객체를 분할할 수 있으므로 &quot;segment anything&quot;이라고 합니다. 반면 &quot;segment everything&quot;은 본질적으로 프롬프트가 필요하지 않은 객체 제안 생성 Kirillov et al. [2023]입니다. Kirillov et al. [2023]에서 &quot;segment everything&quot;(객체 제안 생성)은 제로샷 전송 성능을 보여주기 위한 다운스트림 작업 중 하나로 선택되었습니다. 요약하자면 &quot;segment anything&quot;은 모든 객체에 대한 프롬프트 가능 분할의 기초 작업을 해결하는 반면 &quot;segment everything&quot;은 모든 객체에 대한 마스크 제안 생성의 다운스트림 작업을 해결합니다. &quot;segment everything&quot;은 반드시 프롬프트가 필요하지 않으므로 FastSAM은 프롬프트 없는 방식으로 YOLO v8로 마스크 제안을 직접 생성합니다. 프롬프트 가능 분할을 가능하게 하기 위해 제안 마스크 세트에서 마스크를 선택하도록 매핑 알고리즘이 설계되었습니다. 일반화/견고성을 평가하거나 다재다능함을 조사하는 후속 작업은 주로 모든 것 대신 모든 것 모드에 초점을 맞추는데, 이는 전자가 기초 작업을 다루기 때문입니다. 따라서 FastSAM과의 비교는 주로 &quot;모든 것 분할&quot;에 초점을 맞추지만, 완전성을 위해 &quot;모든 것 분할&quot;에 대한 비교도 제공합니다. MobileSAM은 더 빠르고 더 작습니다. FastSAM은 YOLOV8 기반 감지 분기와 YOLACT 기반 분할 분기로 구성되어 프롬프트 없는 마스크 제안 생성을 수행합니다. 매개변수가 68M개이고 이미지를 처리하는 데 40ms가 걸립니다. 반면 MobileSAM은 매개변수가 10M개로 훨씬 더 작습니다. 추론 속도의 경우 단일 GPU에서 이미지를 처리하는 데 40ms가 걸리는 반면, 저희는 10ms만 걸리며, 이는 FastSAM보다 4배 빠릅니다(표 6 참조). 표 6: FastSAM과 MobileSAM의 비교. 크기 속도 FastSAM MobileSAM 68M 9.66M 64ms 12ms 비율 ≈≈mIoU 세그먼트 무엇이든 모드에서 비교. 또한 예측된 마스크 간의 moU를 원래 SAM의 moU와 비교합니다. FastSAM은 여러 지점으로 마스크를 예측하는 것으로 제안되며, 그중 하나는 전경에, 다른 하나는 배경에 선택합니다. 표 7의 결과는 FastSAM의 mIoU가 MobileSAM의 mIoU보다 훨씬 작다는 것을 보여주며, FastSAM의 마스크 예측이 원래 SAM의 마스크 예측과 매우 다르다는 것을 시사합니다. 게다가 두 프롬프트 지점 사이의 거리가 멀어질 때 FastSAM의 mIoU가 매우 빠르게 감소합니다. 이는 주로 전경 프롬프트 지점이 배경 프롬프트 지점에 너무 가깝게 설정될 때 FastSAM이 종종 객체를 예측하지 못하기 때문에 발생합니다. 표 7: mIoU 비교. 원래 SAM의 예측 마스크가 기준 진실이라는 가정 하에 mIoU가 높을수록 성능이 더 좋음을 나타냅니다. 100 200 300 400 FastSAM 0.27 0.33 0.37 0.41 0. MobileSAM 0.73 0.71 0.74 0.73 0. 모든 것을 세분화한 결과입니다. &quot;모든 것을 분할&quot;에 대한 결과는 그림 6에 나와 있습니다. 완전성을 위해, 만족스러운 객체 제안을 생성하는 원래 SAM의 결과도 보고합니다. 두 가지 주요 관찰 사항이 있습니다. 첫째, MobileSAM의 결과는 원래 SAM의 결과와 놀라울 정도로 잘 일치합니다. 반면에 FastSAM의 결과는 종종 만족스럽지 않습니다. 예를 들어, FastSAM은 종종 첫 번째 이미지의 지붕과 같은 일부 객체를 예측하지 못합니다. 게다가 마스크 제안은 때때로 해석하기 어렵습니다(첫 번째 이미지의 무대 마스크와 두 번째 이미지의 하늘 마스크 참조). 둘째, FastSAM은 종종 매끄럽지 않은 경계가 있는 마스크를 생성합니다. (a) 이미지 (b) MobileSAM (c) 원래 SAM 그림 6: 모든 것을 분할한 결과 비교. (d) FastSAM 독자에게 그림 6의 세부 사항을 확인하기 위해 확대해 볼 것을 제안합니다. 예를 들어, 세 번째 이미지의 기둥은 매끄럽지 않은 경계가 있는 반면, 원래 SAM과 MobileSAM에는 이 문제가 없습니다. 5
--- CONCLUSION ---
이 작업에서 우리는 무거운 이미지 인코더를 가벼운 인코더로 대체하여 SAM을 모바일 친화적으로 만드는 것을 목표로 합니다. 우리는 원래 SAM 논문에서와 같이 새로운 SAM을 훈련하는 순진한 방법이 특히 제한된 훈련 소스 설정에서 만족스럽지 못한 성능을 초래한다는 것을 발견했습니다. 이미지 인코더와 마스크 디코더의 결합된 최적화가 그 이유이며, 따라서 우리는 분리된 증류를 제안합니다. 여기서 지식은 원래 SAM의 이미지 인코더 ViT-H에서 가벼운 이미지 인코더로 증류됩니다. 우리는 결과적으로 가벼운 이미지 인코더가 원래 SAM의 마스크 디코더와 자동으로 호환될 수 있음을 보여줍니다. 우리의 MobileSAM은 60배 이상 작지만 원래 SAM과 동일한 성능을 보입니다. 게다가 우리는 동시 FastSAM과 비교를 수행하고 MobileSAM이 더 우수한 성능을 달성한다는 것을 보여줍니다. 우리의 MobileSAM은 또한 동시 FastSAM보다 4배 빠르고 7배 작아서 모바일 애플리케이션에 더 적합합니다. MobileSAM은 원래 SAM의 모든 파이프라인을 유지하고 이미지 인코더만 대체하므로 기존 SAM 기반 프로젝트가 거의 아무런 노력 없이 중량급 SAM에서 경량급 SAM으로 전환하는 데 플러그 앤 플레이 방식이 가능합니다.참고문헌 Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. 생성적 AI를 위한 작은 한 걸음, AGI를 위한 거대한 도약: AIGE 시대의 chatgpt에 대한 완전한 조사.arXiv 사전 인쇄본 arXiv:2304.06488, 2023a. Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. 생성적 AI(aigc)에 대한 완전한 조사: gpt-4에서 gpt-5로의 chatgpt만 있으면 되나요? arXiv 사전 인쇄본 arXiv:2303.11717, 2023b.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 2019. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill 등. 기초 모델의 기회와 위험에 관하여. arXiv 사전 인쇄본 arXiv:2108.07258, 2021. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick. 비지도 시각적 표현 학습을 위한 모멘텀 대비. CVPR에서, 2020. Yu Qiao, Md Munir, Apurba Adhikary, Huy Q Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong 등. Mp-fedcl: 엣지 인텔리전스를 위한 다중 프로토타입 연합 대조 학습. arXiv 사전 인쇄본 arXiv:2304.01950, 2023a. Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, In So Kweon. Simsiam은 부정적인 샘플 없이 붕괴를 어떻게 방지할까요? 자기 지도 대조 학습을 통한 통합 이해. ICLR, 2022a. Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, In-So Kweon. 이중 온도는 많은 부정적인 샘플 없이 대조 학습을 돕습니다. moco를 이해하고 단순화하기 위해. CVPR, 2022b. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 무엇이든 분할하세요. arXiv 사전 인쇄 arXiv:2304.02643, 2023. Chaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois Rameau, Sung-Ho Bae, et al. 세그먼트 무엇이든 모델에 대한 설문 조사(sam): 비전 기반 모델이 신속한 엔지니어링을 충족합니다. 2023c. Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang 및 Jinqiao Wang. 무엇이든 빠르게 분할하세요. arXiv 사전 인쇄 arXiv:2306.12156, 2023. Jun Ma 및 Bo Wang. 의료 이미지의 모든 것을 분할합니다. arXiv 사전 인쇄 arXiv:2304.12306, 2023. Yizhe Zhang, Tao Zhou, Peixian Liang 및 Danny Z Chen. sam을 사용한 입력 증강: 세분화 기반 모델을 사용한 의료 이미지 세분화 향상. arXiv 사전 인쇄본 arXiv:2304.11332, 2023d. Lv Tang, Haoke Xiao, Bo Li. sam이 무엇이든 세분화할 수 있을까? sam이 위장된 객체 감지를 만났을 때. arXiv 사전 인쇄본 arXiv:2304.04709, 2023. Dongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, Choong Seon Hong. 무엇이든 세분화 모델(sam)이 유리를 만났을 때: 거울과 투명한 객체는 쉽게 감지할 수 없음. arXiv 사전 인쇄본, 2023. Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, In So Kweon. 공격-sam: 무엇이든 세분화 모델의 적대적 견고성을 평가하기 위해. arXiv 사전 인쇄본, 2023e. Yu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, Choong Seon Hong. sam의 견고성: 부패와 그 너머에서 무엇이든 분할. arXiv 사전 인쇄본 arXiv:2306.07713, 2023b. IDEA-Research. 접지된 세그먼트 무엇이든, 2023. URL https://github.com/IDEA-Research/ 접지된-세그먼트-아무거나. GitHub 저장소. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 접지된 공룡: 오픈 세트 객체 감지를 위한 접지된 사전 학습과 공룡 결합. arXiv 사전 인쇄본 arXiv:2303.05499, 2023a. Jiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. URL https://github.com/ fudan-zvg/Semantic-Segment-Anything. GitHub 저장소. Curt Park. segment anything with clip, 2023. URL https://github.com/Curt-Park/ segment-anything-with-clip. GitHub 저장소. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 전이 가능한 시각적 모델 학습. ICML, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, Zhibo Chen. 무엇이든 인페인트: 무엇이든 분할하여 이미지 인페인팅을 만납니다. arXiv 사전 인쇄본 arXiv:2304.06790, 2023. Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng. 무엇이든 추적: 무엇이든 분할하여 비디오를 만납니다. arXiv 사전 인쇄본 arXiv:2304.11968, 2023. Zxyang. 무엇이든 분할하고 추적합니다, 2023. Segment-and-Track-Anything. GitHub 리포지토리. URL https://github.com/zx-yang/ Qiuhong Shen, Xingyi Yang, Xinchao Wang. Anything-3d: 야생에서 단일 뷰의 모든 것을 재구성하는 방향으로. arXiv 사전 인쇄본 arXiv:2304.10261, 2023. Minki Kang, Dongchan Min, Sung Ju Hwang. 확산 모델을 사용한 모든 화자 적응형 텍스트-음성 합성. arXiv 사전 인쇄본 arXiv:2211.09383, 2022. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. Mobilenets: 모바일 비전 애플리케이션을 위한 효율적인 합성 신경망. arXiv 사전 인쇄본 arXiv:1704.04861, 2017. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: 역 잔차 및 선형 병목 현상. CVPR에서, 2018. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Mobilenetv3 검색. 컴퓨터 비전에 관한 IEEE/CVF 국제 컨퍼런스 진행, 페이지 1314-1324, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit 및 Neil 홀스비. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기입니다. ICLR, 2021. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles 및 Hervé Jégou. 데이터 효율적인 이미지 변환기 교육 및 주의를 통한 증류. arXiv 사전 인쇄 arXiv:2012.12877, 2020. Sachin Mehta 및 Mohammad Rastegari. Mobilevit: 가볍고, 범용이며, 모바일 친화적인 비전 트랜스포머. arXiv 사전 인쇄본 arXiv:2110.02178, 2021. Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren. Efficientformer: 모바일넷 속도의 비전 트랜스포머. 신경 정보 처리 시스템의 발전, 35: 12934-12949, 2022a. Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, Yixuan Yuan. Efficientvit: 계단식 그룹 주의를 갖춘 메모리 효율적인 비전 트랜스포머. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 14420-14430쪽, 2023b. Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan. Nextvit: 현실적인 산업 시나리오에서 효율적인 배포를 위한 차세대 비전 변환기. arXiv 사전 인쇄본 arXiv:2207.05501, 2022b. Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, Lu Yuan. Tinyvit: 소형 비전 변환기를 위한 빠른 사전 학습 증류. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXI, pages 68–85. Springer, 2022. Geoffrey Hinton, Oriol Vinyals, Jeff Dean. 신경망에서 지식 증류. arXiv 사전 인쇄본 arXiv:1503.02531, 2015. Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, In So Kweon. 자기 지도 적대적 견고성을 위한 분리된 적대적 대조 학습. ECCV, 725-742쪽. Springer, 2022c. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar. 밀집 객체 감지를 위한 초점 손실. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 2980-2988쪽, 2017. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi. V-net: 체적 의료 이미지 분할을 위한 완전 합성 신경망. 2016년 4차 3D 비전(3DV) 국제 컨퍼런스, 565-571쪽. IEEE, 2016.
