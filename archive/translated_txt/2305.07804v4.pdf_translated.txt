--- ABSTRACT ---
대규모 언어 모델(LLM)은 자연어 처리 분야에서 놀라운 진전을 이루었습니다. 그러나 증가하는 크기는 계산 비용 측면에서 과제를 안겨줍니다. 반면 소규모 언어 모델(SLM)은 효율성으로 유명하지만, 특히 특정 도메인에서 제한된 용량과 교육 데이터로 어려움을 겪는 경우가 많습니다. 이 논문에서는 LLM 기반 생성 데이터 증강을 사용하여 의료 도메인에서 SLM을 개선하는 것을 목표로 하는 새로운 방법을 소개합니다. 이 접근 방식의 목적은 특수 애플리케이션에 맞게 특별히 맞춤화된 보다 효율적이고 유능한 모델을 개발하는 것입니다. PubMedQA 데이터 세트에서 수행한 실험을 통해 LLM이 기존 질문-답변 쌍을 정제하고 다양화하는 데 효과적임을 보여줍니다. 이 정제 프로세스는 미세 조정 후 상당히 작은 모델에서 성능이 향상됩니다. 특히 16억 개 미만의 매개변수를 가진 최상의 SLM은 PubMedQA 데이터 세트에서 few-shot GPT-4보다 성능이 뛰어납니다. 코드와 생성된 데이터는 추가 탐색을 용이하게 하기 위해 공개적으로 제공됩니다[1]. 키워드 대규모 언어 모델, 소규모 언어 모델, 의학적 질의응답, 데이터 증강 1.
--- METHOD ---
LLM 기반 생성 데이터 증강을 사용하여 의료 분야에서 SLM을 개선하는 것을 목표로 합니다. 우리 접근 방식의 목적은 특수 응용 프로그램에 맞게 특별히 맞춤화된 보다 효율적이고 유능한 모델을 개발하는 것입니다.
--- EXPERIMENT ---
영어: PubMedQA 데이터 세트에서 수행된 s에서 우리는 기존 질문-답변 쌍을 정제하고 다양화하는 데 있어 LLM의 효과를 보여줍니다. 이 정제 과정은 미세 조정 후 상당히 작은 모델에서 향상된 성능으로 이어집니다. 특히, 16억 개 미만의 매개변수를 가진 최상의 SLM은 PubMedQA 데이터 세트에서 few-shot GPT-4보다 성능이 뛰어납니다. 우리의 코드와 생성된 데이터는 추가 탐색을 용이하게 하기 위해 공개적으로 제공됩니다[1]. 키워드 대규모 언어 모델, 소규모 언어 모델, 의학적 질문-답변, 데이터 증강 1. 서론에서는 LLM 기반 생성 데이터 증강을 통해 의료 분야에서 SLM을 증명합니다. 목표는 수십억 개의 매개변수를 사용하지 않고도 특수 의료 응용 프로그램에 맞게 조정된 보다 효율적이고 유능한 모델을 개발하는 것입니다. PubMedQA 데이터 세트[12]에서의 우리의 결과는 질문-답변 쌍의 정제 및 다양화에서 LLM의 효과를 보여주며, 미세 조정 후 상당히 작은 모델의 성능이 향상됩니다. 표 1에서 볼 수 있듯이 16억 개 미만의 매개변수를 가진 최상의 SLM은 PubMedQA의 few-shot GPT-4보다 성능이 뛰어납니다. 일반적으로, 우리의 방법은 의료 업무에 대한 SLM을 향상시키고, 전문 분야에서 계산 효율성과 모델 성능 간의 격차를 메우는 데 유망합니다. 최근 몇 년 동안 대규모 언어 모델(LLM)은 자연어 처리 분야를 혁신하여 광범위한 작업에서 뛰어난 성능을 보여주었습니다. 막대한 양의 데이터와 광범위한 사전 학습[2]으로 구동되는 이러한 모델은 기계 번역, 프로그램 합성, 질의 응답[3, 4, 5]과 같은 다양한 응용 분야에서 최첨단 기술을 발전시켰습니다. LLM은 인상적인 기능을 가지고 있지만, 크기가 커지면서 실제 응용 프로그램 및 도메인별 작업의 경우 특히 계산 효율성 측면에서 과제가 발생합니다[6, 7]. 의료 질의 응답 또는 법률 문서 분석과 같은 문제에는 범용 LLM으로는 완전히 포착할 수 없는 전문 지식이 필요합니다[8, 9]. 반면, PubMedQA에서 ChatGPT 대 BioGPT를 미세 조정하여 LLM에 대한 계산 효율성이 더 높은 대안으로 사용하는 소규모 언어 모델(SLM)이 있습니다. 그러나 SLM은 제한된 용량과 교육 데이터로 인해 도메인별 작업에서 어려움을 겪는 경우가 많습니다. 이러한 한계로 인해 계산 효율성을 유지하면서 전문 작업에서 SLM의 성능을 향상시키기 위한 새로운 전략을 개발해야 합니다[10, 11]. 이 논문에서는 2023년 8월 6일~10일, 미국 캘리포니아주 롱비치에서 개최되는 제29회 ACM SIGKDD 지식 발견 및 데이터 마이닝(KDD) 컨퍼런스와 동시에 개최되는 imLLM4A1&#39;23: 대규모 AI 모델의 기초 및 응용 프로그램 워크숍 - 사전 교육, 미세 조정 및 프롬프트 기반 학습에 대한 새로운 방법을 소개합니다. *연락처: zguo0525@mit.edu(Z. Guo); yanwei@mit.edu(Y. Wang); wpq@mit.edu(P. Wang); shangdiy@mit.edu(S. Yu) CC + TableModel GPT-3.5-turbo(175B) GPT-4(0-shot) [13] GPT-4(5-shot) [13] Best BioGPT(1.6B) Human Performance [12] Accuracy Macro-F0.0.0.NA 0.NA 0.0.0.0.2. 기술적 배경 2.1. 효율적인 미세 조정 특정 작업에 대한 LLM 미세 조정은 계산 및 시간 관련 과제를 제기합니다[14, 15]. 이러한 2022년 이 논문의 저작권은 저자에게 있습니다. 크리에이티브 커먼즈 라이선스에 따라 사용이 허용되며, 연구자들은 효율적인 미세 조정 Attribution 4.0 International(CC BY 4.0)을 개발했습니다. CEUR 워크숍 회의록(CEUR-WS.org)에서는 접두사 튜닝 및 저순위 적응과 같은 기술을 모델의 가중치를 완전히 업데이트하는 기존의 미세 조정 방법에 대한 대안으로 제시합니다.접두사 튜닝[16]은 사전 훈련된 가중치를 수정하지 않고 언어 모델의 동작을 특정 작업에 맞게 조정합니다.저순위 적응[17]을 사용하면 모델이 데이터의 필수 특성을 포착하고 가중치 행렬을 더 작은 행렬로 분해하여 도메인별 작업에 효과적으로 적응할 수 있습니다.2.2. LLM을 사용한 데이터 증강 LLM은 기존 데이터를 기반으로 사실적인 텍스트 샘플을 생성하는 강력한 도구 역할을 합니다.NLP 작업의 경우 LLM을 사용하여 데이터를 생성하는 데는 텍스트를 의역하거나, 대체 질문-답변 쌍을 만들거나, 새로운 문장을 생성하는 작업이 포함될 수 있습니다[18].입력 데이터의 다양한 표현을 생성하면 모델이 동일한 기본 개념을 표현하는 다양한 방법을 학습하여 실제 데이터 변화에 대한 적응성을 높일 수 있습니다. PubMedQA 데이터 세트에 대한 예비 연구를 위해 GPT-3.5 Turbo와 GPT-4를 사용하여 기존 의학 질의 응답 쌍을 다시 작성하거나 제로 샷 프롬핑을 사용하여 훈련 데이터 세트(크기 450)에서 새로운 쌍을 생성했습니다. 이 접근 방식은 훈련 데이터의 다양성과 적용 범위를 개선하는 데 도움이 되었으며 궁극적으로 증강된 데이터 세트에서 훈련된 의학 질의 응답 모델의 성능을 개선했습니다. 3. 실험 설정 PyTorch 1.12와 Python 3.8을 8개의 NVIDIA V100 GPU와 Intel Xeon Gold 6248 프로세서와 함께 사용하여 MIT Supercloud[19]에서 실험을 수행했습니다. 의학 질의 응답 작업을 위해 BioGPT-Large[20], LLaMA-7b[21], Alpaca-7b[22]에서 접두사 튜닝과 로우랭크 적응의 효과를 조사했습니다. 평가는 PubMedQA 데이터 세트[12]에서 수행되었으며, 450개의 훈련 샘플, 50개의 검증 샘플, 500개의 테스트 샘플로 분할했습니다. 정확도와 F1 점수는 예측된 답변과 기준 진실 답변 간의 하드 매치를 기반으로 계산되었습니다. 접두사 튜닝의 경우 원래 구현[16]을 따르고 토큰 범위 16~512를 탐색한 반면, 저랭크 적응은 고정된 랭크 4로 알파를 16~512로 변경했습니다. 미세 조정에는 학습 속도 5e-5, AdamW 옵티마이저[23], 선형 워밍업 스케줄러[24], 32단계의 그래디언트 축적[25], 배치 크기 1024개 토큰을 사용했습니다. 추론하는 동안 반복 페널티 로짓 프로세서(페널티 계수 2.0), 온도 로짓 워퍼(온도 0.8), 빔 크기 5의 빔 검색 디코딩을 포함한 기술을 적용하여 출력 품질을 보장했습니다. 4.1. Low-rank Adaptation은 Prefix Tuning 정확도 f1 점수 0.60.0.0.0.0.0.0.low-rank adaptation -prefix tuningfine-tuning 하이퍼파라미터보다 성능이 우수합니다.그림 1: BioGPT-Large에 대한 두 가지 미세 조정 기술 비교.BioGPTLarge에 대해 Low-rank Adaptation과 Prefix Tuning이라는 두 기술의 성능을 비교했습니다(그림 1).Low-rank Adaptation은 다른 하이퍼파라미터(16~512)에서 안정성을 보인 반면 Prefix Tuning은 가상 토큰 범위에 민감함을 보였습니다.이러한 결과는 Low-rank Adaptation이 더 강력하고 하이퍼파라미터 선택에 덜 민감하여 효율적인 미세 조정을 위한 일관되고 안정적인 성능을 제공함을 시사합니다.아래의 모든 결과에서 Low-rank Adaptation이 기본 미세 조정 기술입니다.4.2. 명령어 튜닝은 언어 모델의 도메인 적응성을 제약함 표 2에서는 데이터 증가를 기준으로 하지 않고 원래 PubMedQA 데이터 세트에서 미세 조정된 BioGPT-Large, LLAMA-7b 및 Alpaca-7b를 비교합니다. LLaMA-7b의 파생 제품인 Alpaca-7b는 명령어를 따라 작업별 성능을 개선하도록 설계된 명령어 튜닝 LLM입니다. 그러나 이 접근 방식은 순진한 사전 학습 모델과 비교할 때 다른 도메인별 작업에 대한 적응성이 제한됩니다. 실험에서 LLaMA-7b는 원래 PubMedQA 데이터 세트에서만 미세 조정했을 때 더 높은 Fscore를 나타내어 BioGPT-Large에 비해 우수한 일반화성을 보여줍니다. BioGPT-Large에 대해 보고된 정확도는 미세 조정 설정이 다르기 때문에 Luo et al. [20]에서 보고한 수치보다 낮습니다. Luo et al. [20]은 답변 토큰 바로 앞에 가상 토큰을 삽입했지만, 우리는 과적합 위험을 피하기 위해 질문 토큰 앞에 가상 토큰을 삽입했습니다.유용한 새로운 학습 데이터를 표로 정리합니다.GPT-4의 새로운 BioGPT-Large, LLaMA-7b 및 Alpaca-7b 세부 질문-답변 쌍을 원래 PubMed QA 데이터 세트에서 프로튜닝된 학습에 통합하여 사용합니다.모델 정확도 Macro-FBioGPT-Large 0.0.LLaMA-7b Alpaca-7b 0.0.0.0.Table 증강된 PubMedQA에서 세부 조정된 LLaMA-7b와 BioGPT-Large의 비교(Acc.는 정확도를 의미하고 GPT-3.은 GPT-3.5-turbo를 의미하고 BioGPT는 BioGPT-Large를 나타내며 F1은 매크로 F1 점수를 나타냄).F1(최상) 0.SLM LLM 증강.없음 Acc. (최상) 0.rewriteQA 0.0.GPT-3.newQA 0.0.LLAMA combinedQA 0.0.rewriteQA 0.0.GPT-newQA 0.0.combinedQA 0.0.none 0.0.rewriteQA 0.0.GPT-3.newQA 0.0.BioGPT combinedQA 0.0.GPT-rewriteQA newQA 0.0.0.0.combinedQA 0.0.4.3. 생성적 데이터 증강 접근 방식 간 비교 표 3에서는 증강된 PubMedQA 데이터 세트에서 미세 조정된 LLaMA-7b와 BioGPT-Large를 비교합니다. 우리의 실험은 LLM(예: ChatGPT)을 사용하여 질문-답변 쌍을 정제하고 확장하여 도메인별 QA 데이터 세트를 향상시키는 효능을 입증합니다. LLM이 답변 생성에서 거의 무작위적인 성능을 보일 때에도 말입니다(gpt-3.5-turbo의 경우처럼). 질문과 답변에 대한 그 결과 나온 대체 표현은 SLM에 적합한 보다 다양하고 강력한 학습 데이터 세트를 구성하는 데 도움이 되었습니다. 그러나 도메인 지식이 부족한 LLM(gpt-3.5turbo)에 완전히 새로운 질문-답변 쌍을 생성하도록 지시해도 개선으로 이어지지 않고 미세 조정된 SLM의 다운스트림 작업 성능이 저하되는 것을 발견했습니다. 이 관찰 결과는 LLM이 기존 질문-답변 쌍을 정제하고 다양화하는 데 효과적이지만 도메인별 작업을 위한 새롭고 고품질의 쌍을 만드는 능력은 여전히 제한적임을 시사합니다. 반면, PubMedQA에 대한 도메인별 지식과 질의응답 용량을 갖춘 GPT-4와 같은 LLM의 최근 발전은 미세 조정된 소규모 모델의 성능을 크게 개선할 수 있음을 의미합니다. 이 발견은 도메인별 QA 데이터 세트를 향상시키고 다운스트림 작업의 성능을 개선하는 데 도메인별 지식이 있는 LLM의 중요성을 강조합니다. 마지막으로 놀랍지 않게도 BioGPT가 증강된 데이터 세트에서 미세 조정될 때 LLaMA-7B보다 성능이 뛰어납니다. 이는 이전 발견[20]과 일치하며 도메인별 데이터로 사전 학습하는 효과성을 강조하여 BioGPT가 도메인별 작업을 더 잘 이해하고 탁월하게 수행할 수 있도록 합니다. 미세 조정 중에 도메인별 지식을 활용하면 모델의 정확도와 상황적 관련성이 향상되어 도메인별 질문이나 작업에 대한 성능이 더욱 향상됩니다. 5. 향후 연구 향후 연구에 대한 유망한 방향은 지식 증류의 적용을 조사하는 것입니다. 지식 증류는 의학적 질의 응답 작업에서 더 큰 언어 모델의 동작을 모방하도록 더 작은 언어 모델을 훈련하는 인기 있는 기술입니다. 또 다른 잠재적인 접근 방식은 대조 학습을 통한 것입니다. 대조 학습을 사용하여 의학적 질의 응답 데이터에 SLM을 훈련함으로써 대조 손실은 모델이 서로 다른 데이터 인스턴스 간의 유사점과 차이점을 식별하고 새롭고 보이지 않는 데이터로 일반화하는 능력을 향상시키는 데 도움이 될 수 있습니다. 6.
--- CONCLUSION ---
저희의 연구는 도메인별 질문 답변 데이터 세트를 향상시키는 데 있어 LLM 기반 생성 데이터 증강의 효과를 강조합니다. 그러나 GPT-3.5turbo와 같이 도메인 지식이 없는 LLM에 새로운 질문-답변 쌍을 생성하도록 지시하면 미세 조정된 작은 모델의 성능이 저하됩니다. 반대로 GPT-4와 같이 도메인별 지식이 있는 LLM을 활용하면 귀중한 새로운 학습 데이터를 생성하여 미세 조정된 모델의 성능이 크게 향상됩니다. 이러한 결과는 생성 데이터 증강 기술을 적용할 때 도메인별 지식을 통합하는 것의 중요성을 강조합니다. 감사의 말 MIT CSAIL의 Yoon Kim 교수님께 지도와 피드백을 주셔서 감사드립니다. 저자는 컴퓨팅 리소스를 제공해준 MIT SuperCloud와 Lincoln Laboratory Supercomputing Center에 감사드립니다. 또한 API에 대한 액세스를 제공해준 OpenAI에도 감사드립니다. 참고문헌 [1] Z. Guo, P. Wang, Y. Wang, S. Yu, Dr. LLAMA: 생성적 데이터 증강을 통해 도메인별 QA에서 소규모 언어 모델 개선, 2023. URL: https://github.com/zguo0525/Dr.LLAMA. [2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, LA Hendricks, J. Welbl, A. Clark, et al., 컴퓨팅 최적의 대규모 언어 모델 교육, arXiv 사전 인쇄본 arXiv:2203.15556(2022). [3] WX Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong 외, 대규모 언어 모델 조사, arXiv 사전 인쇄 arXiv:2303.18223(2023). [4] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le 외, 대규모 언어 모델을 사용한 프로그램 합성, arXiv 사전 인쇄 arXiv:2108.(2021). [5] M. Chen, J. Tworek, H. Jun, Q. Yuan, HP d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., 코드로 학습된 대규모 언어 모델 평가, arXiv 사전 인쇄본 arXiv:2107.(2021). [6] E. Strubell, A. Ganesh, A. McCallum, NLP에서 딥 러닝을 위한 에너지 및 정책 고려 사항, arXiv 사전 인쇄본 arXiv:1906.02243 (2019). [7] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, H. Poon, 생물의학 자연어 처리를 위한 도메인별 언어 모델 사전 학습, ACM Transactions on Computing for Healthcare(HEALTH) 3(2021) 1-23. [8] O. Ram, Y. Kirstain, J. Berant, A. Globerson, O. Levy, 사전 학습 범위 선택을 통한 몇 가지 질문 답변, arXiv 사전 인쇄본 arXiv:2101.00438(2021). [9] Z. Sun, 법률적 측면에서 대규모 언어 모델 보기에 대한 간략한 조사, arXiv 사전 인쇄본 arXiv:2303.09136(2023). [10] N. Poerner, U. Waltinger, H. Schütze, 사전 학습된 언어 모델의 저렴한 도메인 적응: 생물의학적 신경 및 covid-19 qa에 대한 사례 연구, arXiv 사전 인쇄본 arXiv:2004.03354(2020). [11] FN Iandola, AE Shaw, R. Krishna, KW Keutzer, Squeezebert: 컴퓨터 비전은 NLP에 효율적인 신경망에 대해 무엇을 가르칠 수 있습니까?, arXiv 사전 인쇄본 arXiv:2006.11316(2020). [12] Q. Jin, B. Dhingra, Z. Liu, WW Cohen, X. Lu, Pubmedqa: 생물의학 연구 질문 답변을 위한 데이터 세트, arXiv 사전 인쇄본 arXiv:1909.06146(2019). [13] H. Nori, N. King, SM McKinney, D. Carignan, E. Horvitz, 의학적 도전 문제에 대한 gpt-4의 기능, arXiv 사전 인쇄본 arXiv:2303.(2023). [14] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, CA Raffel, Few-shot 매개변수 효율적 미세 조정은 컨텍스트 내 학습보다 더 뛰어나고 저렴합니다. 신경 정보 처리 시스템의 발전 35(2022) 1950-1965. [15] D. Vos, T. Döhmen, S. Schelter, 접두사 조정을 통한 데이터 정리 작업의 매개변수 효율적 자동화를 향해, NeurIPS 2022 첫 번째 테이블 표현 워크숍, 2022. [16] XL Li, P. Liang, 접두사 조정: 생성을 위한 연속 프롬프트 최적화, arXiv 사전 인쇄본 arXiv:2101.00190(2021). [17] EJ Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: 대규모 언어 모델의 저순위 적응, arXiv 사전 인쇄 arXiv:2106.09685(2021). [18] A. Edwards, A. Ushio, J. Camacho-Collados, H. de Ribaupierre, A. Preece, 소수 텍스트 분류에서 데이터 증가를 위한 생성 언어 모델 안내, arXiv 사전 인쇄 arXiv:2111.(2021). [19] [20] [21] A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Arcand, D. Bestor, B. Bergeron, V. Gadepally, M. Houle, M. Hubbell, M. Jones, A. Klein, L. Milechin, J. Mullen, A. Prout, A. Rosa, C. Yee, P. Michaleas, 기계 학습 및 데이터 분석을 위한 40,000개 코어의 대화형 슈퍼컴퓨팅, 2018 IEEE 고성능 익스트림 컴퓨팅 컨퍼런스(HPEC), IEEE, 2018, pp. 1-6. R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, T.-Y. Liu, Biogpt: 생물의학 텍스트 생성 및 마이닝을 위한 생성적 사전 학습된 변환기, Briefings in Bioinformatics 23(2022). H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., 라마: 개방적이고 효율적인 기초 언어 모델, arXiv 사전 인쇄 arXiv:2302.13971 (2023). [22] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, TB Hashimoto, 스탠포드 알파카: 지시를 따르는 라마 모델, https://github.com/tatsu-lab/stanford_alpaca, 2023. [23] I. Loshchilov, F. Hutter, 분리된 가중치 감소 정규화, arXiv 사전 인쇄 arXiv:1711.(2017). [24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin, 주의만 있으면 됩니다, 신경 정보 처리 시스템의 발전 30(2017). [25] Y. Lin, S. Han, H. Mao, Y. Wang, WJ Dally, 심층 그래디언트 압축: 분산 학습을 위한 통신 대역폭 감소, arXiv 사전 인쇄본 arXiv:1712.01887(2017).
