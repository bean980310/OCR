--- ABSTRACT ---
Rezsa Farahani Google Inc. farahani@google.com Matthew Jagielski Google DeepMind jagielski@google.com 우리는 딥 검색 시스템을 훈련할 때 차등 개인 정보 보호(DP) 보장을 보장하는 과제를 해결합니다. 이러한 시스템을 훈련하는 데는 종종 대조적 스타일의 손실을 사용하는 것이 포함되며, 이는 일반적으로 예제별로 분해할 수 없으므로 일반적인 기술에는 예제별 그래디언트가 필요하기 때문에 DP로 직접 훈련하기 어렵습니다. 이 문제를 해결하기 위해 딥 검색 시스템을 훈련하기 전에 쿼리 개인 정보 보호를 보장하는 것을 우선시하는 접근 방식을 제안합니다. 이 방법은 DP 언어 모델(LM)을 사용하여 원본 데이터를 나타내는 개인 합성 쿼리를 생성합니다. 이러한 합성 쿼리는 개인 정보를 손상시키지 않고 다운스트림 검색 시스템 훈련에 사용할 수 있습니다. 이 접근 방식은 쿼리 수준의 개인 정보 보호 보장을 유지하면서도 직접 DP 훈련에 비해 검색 품질이 상당히 향상되었음을 보여줍니다. 이 연구는 표준 DP 훈련 방법의 한계를 극복하기 위해 LM을 활용할 수 있는 잠재력을 강조합니다. 1
--- INTRODUCTION ---
심층 검색 시스템은 검색에서 광고에 이르기까지 많은 온라인 서비스에서 사용자 질의를 관련 추천과 매치하기 위해 널리 채택되었습니다(Covington et al., 2016; Huang et al., 2020). 많은 응용 프로그램에서 검색 후보 항목은 종종 단일 사용자와 관련된 특정 정보(예: 기사, 제품, 영화, 광고)를 포함하지 않는다는 의미에서 공개적으로 사용 가능한 비개인 정보입니다. 그러나 검색 시스템에 대한 입력 질의에는 종종 사용자 개인 정보가 포함될 수 있습니다. 따라서 사용자 데이터에 대한 심층 검색 시스템을 학습하면 시기적절한 관련성을 통해 사용자 경험을 향상시킬 수 있지만 신경망 모델이 학습 데이터에서 민감한 사용자 정보를 암묵적으로 기억하고 유출하는 것으로 입증되었기 때문에 의도치 않게 사용자 개인 정보를 손상시킬 수도 있습니다(Carlini et al., 2019). 이는 데이터 수집, 학습, 추론 및 이러한 시스템 호스팅의 각 단계에서 개인 정보 보호 민감성을 높입니다. 이 작업에서 우리는 유용성을 크게 방해하지 않고 심층 검색 시스템에서 사용자 쿼리 개인 정보 보호 보장을 보장하는 문제를 해결하고자 합니다. 많은 대규모 머신 러닝 모델에서 학습 데이터의 개인 정보 보호를 보장하는 표준적인 접근 방식은 학습 중에 차등 개인 정보 보호(DP)(Dwork et al., 2014) 보장을 직접 도입하는 것입니다(Abadi et al., 2016; Ponomareva et al., 2023). 이러한 DP 학습 전략은 각 개별 데이터 인스턴스가 전체 모델에 미치는 영향을 제한하여 보장을 제공합니다. 그러나 일부 모델에는 예제별 기여를 제한하는 기능을 본질적으로 방해하는 설계 요소가 포함되어 있어 직접 DP 학습하기가 더 어렵습니다. 여기에는 배치 통계를 계산하는 구성 요소가 있는 모델(예: 배치 정규화 계층(Ponomareva et al., 2023))과 쌍별 및 대조 스타일 손실(Huai et al., 2020; Xue et al., 2021)과 같이 예제별 손실로 분해할 수 없는 손실이 있는 모델이 포함됩니다. 이는 이러한 시스템이 일반적으로 효율적인 벡터 기반 검색 전략을 용이하게 하기 위해 사용자 쿼리와 후보 항목의 의미적 신경 표현을 학습하기 위해 예제별이 아닌 분해 가능한 대조 스타일 손실을 사용하기 때문에 심층 검색 시스템에서 DP 학습을 적용하는 데 제한이 있습니다. 이러한 손실에 대한 DP 보장을 달성하는 데 필요한 주입 노이즈는 예제 수준 손실 계산에 나타나는 후보 항목 수에 따라 확장될 수 있으며, 이는 과도한 검색 품질 저하를 초래할 수 있습니다. 따라서 적절한 개인 정보 보호 성능 트레이드오프를 달성하기 위해 DP 학습을 심층 검색 모델에 적용하기 위해 추가 고려 사항이 종종 필요합니다. 이 작업에서 우리는 비예제별 분해 가능한 대조적 데이터 소유자 DP가 조건부 쿼리 생성 작업에서 LLM을 미세 조정하는 DP LLM 시스템 서버를 사용하여 합성 쿼리를 생성하고 합성 데이터에 대한 다운스트림 검색 시스템을 미세 조정하는 다양한 문제를 피하기 위해 딥 검색 시스템을 학습하기 전에 사용자 쿼리 개인 정보 보호를 보장하는 접근 방식을 취합니다.그림 1: 접근 방식 설명.스타일 손실.우리는 DP 언어 모델(LMS)(Yue et al., 2022; Mattern et al., 2022)을 사용하여 합성 데이터 생성 프레임워크를 기반으로 원래 학습 데이터와 관련하여 쿼리 수준 개인 정보 보호 보장을 통해 모든 다운스트림 딥 검색 시스템을 학습하기 위한 개인 쿼리 공유 접근 방식을 개발합니다.우리는 직접적인 DP 학습 방법과 비교하여 개인 정보 보호 보장을 손상시키지 않고 검색 품질에 대한 접근 방식에서 상당한 개선을 경험적으로 입증합니다. 보다 일반적으로, 우리의 연구는 LM에서 흥미로운 획기적인 발견을 활용하여 직접 DP 훈련 머신 러닝 시스템의 중대한 한계를 극복할 수 있는 새로운 기회에 대한 초기 연구를 제시합니다. 2
--- RELATED WORK ---
DP LMS를 사용한 합성 생성 최근의 여러 연구(Yue 등, 2022; Mattern 등, 2022; Mireshghallah 등, 2022; Putta 등, 2022)에서 텍스트 분류 및 의미 구문 분석을 포함한 점별 손실이 있는 다운스트림 작업에서 DP 미세 조정 LM의 비공개 합성 데이터의 유용성을 조사했습니다. 이러한 연구에서는 비공개 합성 데이터로 학습한 다운스트림 모델이 동일한 개인 정보 보호 예산에서 직접 DP로 학습한 모델보다 성능이 우수하고 비공개가 아닌 합성 데이터 생성은 개인 정보 보호 제약이 없는 경우에도 성능을 향상시킬 수 있음을 발견했습니다. 그 이유는 DP 합성 데이터가 사전 학습된 LM에서 추가 공개 정보를 주입하면 이점을 얻을 수 있기 때문입니다. 저희의 연구는 분해 가능한 손실이 없는 다른 학습 패러다임에서 다운스트림 학습을 위한 비공개 합성 데이터 생성의 이점에 대한 또 다른 탐구에 기여합니다. 특히, 저희의 동기는 높은 유용성을 가진 심층 검색 시스템에서 쿼리 개인 정보 보호 DP 보장을 달성하는 것입니다. 비당사례 분해 가능 손실 하의 DP 훈련 비당사례 분해 가능 손실이 있는 모델을 DP 훈련하는 더 나은 방법을 알아내는 것은 여전히 활발한 연구 주제입니다. 이 분야의 연구는 전적으로 쌍별 손실(Huai et al., 2020; Xue et al., 2021; Kang et al., 2021)에 집중되어 있으며, 볼록성, 매끄러움, Lipshitz 연속성과 같은 특정 조건에서 특수 알고리즘을 도입하여 민감도에 대한 합리적인 경계를 유지합니다. 저희의 연구는 비당사례 분해 가능 손실로 훈련된 시스템에 대한 어느 정도의 프라이버시를 달성하기 위해 이러한 추가 가정 없이 범용 접근 방식을 제시합니다. 3 배경 3.1 심층 검색 심층 검색 시스템은 특정 쿼리와의 의미적 관련성을 기반으로 후보 항목을 찾는 매우 효과적이고 확장 가능한 정보 검색 시스템으로 등장했습니다(Huang et al., 2020; Ni et al., 2021). 이러한 시스템은 일반적으로 쿼리 및 항목의 풍부하고 밀도 있는 표현을 생성할 수 있는 두 개의 신경 인코더로 구성되며 이를 통해 효율적인 근사 최근접 이웃 검색이 가능합니다(그림 2 참조).
--- METHOD ---
영어: DP 언어 모델(LM)을 사용하여 원본 데이터를 나타내는 개인 합성 쿼리를 생성합니다. 이러한 합성 쿼리는 프라이버시를 손상시키지 않고 다운스트림 검색 시스템 교육에 사용할 수 있습니다. 당사의 접근 방식은 쿼리 수준의 프라이버시 보장을 유지하면서도 직접 DP 교육에 비해 검색 품질이 상당히 향상되었음을 보여줍니다. 이 연구는 표준 DP 교육 방법의 한계를 극복하기 위해 LM을 활용할 수 있는 잠재력을 강조합니다. 1 서론 심층 검색 시스템은 검색에서 광고에 이르기까지 많은 온라인 서비스에서 널리 채택되어 사용자 쿼리를 관련 추천과 일치시킵니다(Covington et al., 2016; Huang et al., 2020). 많은 응용 프로그램에서 검색 후보 항목은 종종 단일 사용자와 관련된 특정 정보(예: 기사, 제품, 영화, 광고)를 포함하지 않는다는 의미에서 공개적으로 사용 가능한 비개인 정보입니다. 그러나 검색 시스템에 대한 입력 쿼리에는 종종 사용자 개인 정보가 포함될 수 있습니다. 따라서 사용자 데이터에 대한 심층 검색 시스템을 학습하면 시기적절한 관련성을 통해 사용자 경험을 향상시킬 수 있지만, 신경망 모델이 학습 데이터에서 민감한 사용자 정보를 암묵적으로 기억하고 유출하는 것으로 입증되었기 때문에 의도치 않게 사용자 개인 정보를 손상시킬 수도 있습니다(Carlini et al., 2019). 이는 데이터 수집, 학습, 추론 및 이러한 시스템을 호스팅하는 각 단계에서 개인 정보 보호 민감성을 높입니다. 이 연구에서 우리는 유용성을 크게 방해하지 않고 심층 검색 시스템에서 사용자 쿼리 개인 정보 보호 보장을 보장하는 문제를 해결하고자 합니다. 많은 대규모 머신 러닝 모델에서 학습 데이터의 개인 정보 보호를 보장하는 표준적인 접근 방식은 학습 중에 차등 개인 정보 보호(DP)(Dwork et al., 2014) 보장을 직접 도입하는 것입니다(Abadi et al., 2016; Ponomareva et al., 2023). 이러한 DP-학습 전략은 각 개별 데이터 인스턴스가 전체 모델에 미치는 영향을 제한하여 보장을 제공합니다. 그러나 일부 모델은 본질적으로 예제별 기여를 제한하는 기능을 방해하는 설계 요소를 포함하고 있으므로 직접 DP-학습하기가 더 어렵습니다. 여기에는 배치 정규화 계층(Ponomareva et al., 2023)과 같은 배치 통계를 계산하는 구성 요소가 있는 모델과 쌍별 및 대조 스타일 손실(Huai et al., 2020; Xue et al., 2021)과 같이 예제별 손실로 분해할 수 없는 손실이 있는 모델이 포함됩니다. 이는 이러한 시스템이 일반적으로 효율적인 벡터 기반 검색 전략을 용이하게 하기 위해 사용자 쿼리 및 후보 항목의 의미적 신경 표현을 학습하기 위해 예제별이 아닌 분해 가능한 대조 스타일 손실을 사용하기 때문에 심층 검색 시스템에서 DP-학습을 적용하는 데 제한이 있습니다. 이러한 손실에 대한 DP 보장을 달성하는 데 필요한 주입 노이즈는 예제 수준 손실 계산에 나타나는 후보 항목 수에 따라 확장될 수 있으며, 이는 과도한 검색 품질 저하를 초래할 수 있습니다. 따라서 적절한 개인 정보 보호 성능 트레이드오프를 달성하기 위해 DP 학습을 심층 검색 모델에 적용하기 위해 추가 고려 사항이 종종 필요합니다. 이 작업에서 우리는 비예제별 분해 가능한 대조적 데이터 소유자 DP를 사용하여 심층 검색 시스템을 직접 DP 학습하는 데 따른 다양한 문제를 피하기 위해 심층 검색 시스템을 학습하기 전에 사용자 쿼리 개인 정보를 보장하는 접근 방식을 취합니다. 조건부 쿼리 생성 작업에서 LLM을 미세 조정합니다. DP LLM 시스템 서버를 사용하여 합성 쿼리를 생성합니다. 합성 데이터에 대한 다운스트림 검색 시스템을 미세 조정합니다. 그림 1: 접근 방식의 예. 스타일 손실. 우리는 DP 언어 모델(LMS)을 사용한 합성 데이터 생성 프레임워크를 기반으로(Yue et al., 2022; Mattern et al., 2022) 원래 학습 데이터에 대한 쿼리 수준 개인 정보 보호 보장을 통해 모든 다운스트림 심층 검색 시스템을 학습하기 위한 개인 쿼리 공유 접근 방식을 개발합니다. 우리는 직접 DP 학습 방법과 비교하여 개인 정보 보호 보장을 손상시키지 않으면서 검색 품질에 대한 접근 방식에서 상당한 개선을 경험적으로 입증합니다. 보다 일반적으로, 우리의 연구는 직접 DP 학습 머신 러닝 시스템의 중대한 한계를 극복하기 위해 LM의 흥미로운 혁신을 활용할 수 있는 새로운 기회에 대한 초기 연구를 제시합니다. 2 관련 연구 DP LMS를 사용한 합성 생성 여러 최근 연구(Yue 등, 2022; Mattern 등, 2022; Mireshghallah 등, 2022; Putta 등, 2022)에서 텍스트 분류 및 의미 구문 분석을 포함한 점별 손실이 있는 다운스트림 작업에서 DP 미세 조정 LM의 비공개 합성 데이터의 유용성을 조사했습니다. 이러한 연구에서는 비공개 합성 데이터로 학습한 다운스트림 모델이 동일한 개인 정보 보호 예산에서 직접 DP로 학습한 모델보다 성능이 우수하고 비공개가 아닌 합성 데이터 생성은 개인 정보 보호 제약 없이도 성능을 향상시킬 수 있음을 발견했습니다. 그 이유는 DP 합성 데이터가 사전 학습된 LM에서 추가 공개 정보를 주입하면 이점을 얻을 수 있기 때문입니다. 저희의 연구는 분해 가능한 손실이 없는 다른 학습 패러다임에서 다운스트림 학습을 위한 비공개 합성 데이터 생성의 이점에 대한 또 다른 탐구에 기여합니다. 특히, 저희의 동기는 높은 유용성을 가진 심층 검색 시스템에서 쿼리 개인 정보 보호 DP 보장을 달성하는 것입니다. 비당사례 분해 가능 손실 하의 DP 훈련 비당사례 분해 가능 손실이 있는 모델을 DP 훈련하는 더 나은 방법을 알아내는 것은 여전히 활발한 연구 주제입니다. 이 분야의 연구는 전적으로 쌍별 손실(Huai et al., 2020; Xue et al., 2021; Kang et al., 2021)에 집중되어 있으며, 볼록성, 매끄러움, Lipshitz 연속성과 같은 특정 조건에서 특수 알고리즘을 도입하여 민감도에 대한 합리적인 경계를 유지합니다. 저희의 연구는 비당사례 분해 가능 손실로 훈련된 시스템에 대한 어느 정도의 프라이버시를 달성하기 위해 이러한 추가 가정 없이 범용 접근 방식을 제시합니다. 3 배경 3.1 심층 검색 심층 검색 시스템은 특정 쿼리와의 의미적 관련성을 기반으로 후보 항목을 찾는 매우 효과적이고 확장 가능한 정보 검색 시스템으로 등장했습니다(Huang et al., 2020; Ni et al., 2021). 이러한 시스템은 일반적으로 쿼리와 항목의 풍부하고 조밀한 표현을 생성할 수 있는 두 개의 신경 인코더로 구성되며(그림 2 참조), 효율적인 근사 최근접 이웃 검색 방법(Guo et al., 2020)을 통해 주어진 쿼리와 의미적으로 관련된 항목을 검색할 수 있습니다.심층 검색 시스템은 일반적으로 긍정적 예와 부정적 예의 두 가지 유형의 데이터 예를 사용하는 대조 스타일 손실에 대해 학습합니다.긍정적 예는 인코더가 임베딩 공간에서 관련 쿼리-항목 쌍 임베딩을 서로 가깝게 끌어오도록 학습하는 데 도움이 되는 반면, 부정적 예는 임베딩 공간 붕괴를 방지하는 데 도움이 됩니다.심층 검색에서 손실 함수에 대한 인기 있는 선택은 미니 배치에 이미 로드된 항목을 무작위로 샘플링된 소프트 네거티브로 메모리 효율적으로 사용하는 인배치 소프트맥스 손실입니다(Gillick et al., 2019; Karpukhin et al., 2020; Qu et al., 2020). 특히, 쿼리-항목 쌍 {(qi, di)}iЄB의 학습 배치가 주어졌을 때, 각 d;는 쿼리 qi에 대한 긍정적 항목 문서이고, 배치 내의 다른 모든 항목 문서 {d;}ji는 부정적으로 처리됩니다.배치의 각 샘플에 대한 배치 내 소프트맥스 손실은 다음과 같습니다.= Li - log esim (qi, di) ΣjEB esim (qi, dj)&#39; (1) 여기서 sim(qi, dj)는 qi와 d의 임베딩 간의 코사인 유사도입니다.모든 i에 대해 j = B입니다.배치가 크고 다양할수록 표현 학습에 더 좋습니다.쿼리 임베딩 인코더 쿼리 손실 유사도 문서 임베딩 인코더 문서 그림 2: 딥 검색 듀얼 인코더 모델의 그림.쿼리 및 문서 인코더를 연결하는 점선은 동일한 인코더를 공유할 가능성을 나타냅니다. 3.1. 심층 검색의 개인 정보 보호 위험 심층 검색 시스템의 신경 인코더는 학습 데이터에 있는 민감한 정보를 암묵적으로 기억하는 것으로 알려진 대용량 모델입니다(Carlini et al., 2019). 이러한 민감한 데이터는 이후 이러한 학습된 모델에서 추출할 수 있습니다(Carlini et al., 2021; Lehman et al., 2021). 더욱이 텍스트 생성을 돕기 위해 심층 검색 시스템을 활용하는 검색 증강 텍스트 생성 시스템은 개인 데이터에서 학습된 언어 모델에 비해 개인 데이터 저장소에서 개인 정보가 유출될 가능성이 더 높은 것으로 입증되었습니다(Huang et al., 2023; Zeng et al., 2024). 이는 심층 검색 시스템과 관련된 개인 정보 보호 위험의 증가를 강조합니다. 3.2 조건부 텍스트 생성 조건부 텍스트 생성은 특정 프롬프트가 주어졌을 때 텍스트 시퀀스를 생성하는 작업입니다(Keskar et al., 2019; Schick and Schütze, 2021). GPT-3 및 Tha와 같은 사전 훈련된 생성 LM은 다양한 프롬프트 입력을 조건으로 하는 고품질 텍스트를 생성하는 데 매우 효과적인 것으로 나타났습니다(Raffel et al., 2020; Brown et al., 2020). 맥락 c가 주어졌을 때, 텍스트 시퀀스 x = (x1,. , ...,xn)의 확률 분포는 p(x|c) = [[²²±=1 P(xi|x1, · , xi−1, c)로 분해됩니다. 신경망 Pe는 조건부 분포를 모델링하도록 훈련됩니다. 그런 다음 모델을 사용하여 Po(·|c), Po(·|≈1, c), · · · ‚ Pø (· |˜1, · · · ‚ ≈m−1, c)를 순차적으로 샘플링하여 주어진 컨텍스트 c에 따라 새로운 샘플 x = (x1,..., πm)을 생성할 수 있습니다. 이 작업에서 공개적으로 사전 훈련된 LM이 있는 컨텍스트로 항목 문서가 주어진 경우 쿼리 텍스트의 분포를 모델링합니다. 3.3 차등 개인 정보 보호 차등 개인 정보 보호(DP)는 데이터 익명화를 보장하기 위한 황금 표준이 되었습니다(Dwork et al., 2014). 이 작업에서 우리는 (ε, 8)-차등 개인 정보 보호라고 알려진 차등 개인 정보 보호의 다음과 같은 느슨한 개념을 사용합니다. 정의 3.1(차등 개인 정보 보호). 무작위 알고리즘 M : D → S는 모든 SCS와 정확히 하나의 데이터 포인트만큼 다른 두 개의 이웃 데이터 세트 D, D&#39; ED에 대해 P[M(D) € S] ≤ e€ P[M(D′) € S] + 8인 경우 (e, §)-차등 개인 정보 보호입니다. 이 정의에 따라 검색 데이터 세트에서 쿼리 수준의 차등 개인 정보 보호를 캡처하는 경우, 이웃 데이터 세트는 정확히 하나의 쿼리만큼 다른 데이터 세트입니다. 이 정의는 데이터 세트에서 단일 데이터 포인트의 존재를 구별할 수 없다는 점에 기반한 개인 정보 보호 보장을 캡처합니다. € 및 8 매개변수는 이 개인 정보 보호 보장의 강도를 제어하며, 값이 작을수록 보장이 더 강해집니다. DP의 유용한 속성 중 우리 접근 방식에 중요한 것은 사후 처리 속성(Dwork et al., 2014)으로, 메커니즘 M의 범위에 걸쳐 정의된 모든 결정적 또는 무작위 함수 f에 대해 M이 (e, 8)-DP를 만족하면 M의 구성도 만족한다는 것을 나타냅니다. 사후 처리 속성은 DP 메커니즘의 출력에 대한 임의의 계산으로 인해 추가적인 개인 정보 손실이 발생하지 않도록 보장합니다. 3.3. 차등적 개인 정보 보호 훈련 о 머신 러닝의 맥락에서 DP는 모델을 훈련하는 데 사용된 데이터의 개인 정보를 보호하여 적대자가 특정 훈련 예제의 존재를 유추하지 못하도록 방지하는 데 사용할 수 있습니다. 지금까지 비볼록 ML 모델에 DP를 도입하는 가장 실용적인 방법은 각 개별 데이터 인스턴스가 전체 모델에 미치는 영향을 제한하기 위해 훈련 프로세스를 수정하는 것으로, DP 훈련이라고도 합니다(Ponomareva et al., 2023). DP-훈련을 위한 가장 인기 있는 방법은 차등적 개인 정보 보호 확률적 경사 하강법(DPSGD)(Abadi et al., 2016)과 같은 경사 노이즈 주입 방법입니다.DP-SGD는 예제별 경사를 클리핑하여 노름이 C보다 크지 않도록 하고 클리핑된 경사에 등방성 가우시안 노이즈 N(0,02C2I)을 추가한 다음 집계하여 모델 가중치에 경사 업데이트를 적용합니다.노이즈 승수 σ는 개인 정보 보호 매개변수 ε, 8에 따라 설정되며 개인 정보 보호 회계사를 사용하여 결정할 수 있습니다(Abadi et al., 2016).클리핑은 경사 민감도를 제한하기 위해 수행되며, 이는 단일 예제가 훈련된 모델에 얼마나 영향을 미칠 수 있는지를 포착합니다.C의 특정 값은 실제로 (ε, 8)-DP 보장에 영향을 미치지 않습니다.C의 값이 클수록 보상을 위해 더 많은 노이즈가 추가되기 때문입니다.그러나 클리핑 노름을 설정하는 데 있어서 가장 중요한 과제는 유용성을 극대화하기 위한 적절한 균형을 찾는 것입니다. 클리핑 규범이 너무 낮게 설정된 경우 학습 중에 그래디언트를 지나치게 제한할 수 있습니다.클리핑 규범이 너무 높게 설정된 경우 감도가 덜 제어되고 그래디언트에 너무 많은 노이즈가 추가됩니다.두 경우 모두 모델의 학습 능력을 방해하고 유용성을 악화시킵니다.3.3.2 직접 차등적 개인 정보 보호 학습 검색 시스템의 한계 저희의 작업은 주로 DP-SGD가 듀얼 인코더를 학습하는 데 사용되는 배치 내 소프트맥스 손실과 즉시 호환되지 않는다는 사실에서 동기를 얻었습니다.주된 이유는 이 손실의 예제별 그래디언트가 고려 중인 예제뿐만 아니라 배치의 다른 모든 예제에도 따라 달라지기 때문입니다.따라서 단일 예제가 여러 예제별 그래디언트 계산에 영향을 미칠 수 있으며, 이는 배치 크기에 따라 확장되는 그래디언트의 민감도가 증가함을 즉시 의미합니다.DP 학습에서 민감도가 높을수록 동일한 수준의 개인 정보 보호 보장을 달성하기 위해 학습 중에 그래디언트 업데이트에 더 많은 노이즈를 추가해야 하며, 이는 유용성을 저하시킵니다. 게다가 DP-SGD는 예제 수준의 프라이버시를 보장하며, 이 경우 모든 예제에는 쿼리와 항목이 포함됩니다. 그러나 이 작업에서는 쿼리와 항목을 모두 보호하는 것보다 쉬운 쿼리 수준의 프라이버시를 달성하는 데 관심이 있습니다. 표준 DP-SGD는 이러한 덜 엄격한 수준의 프라이버시를 보장할 수 없습니다. 마지막으로, 배치 내 소프트맥스 손실에 대한 DP 학습의 시스템 수준 문제는 예제별 그래디언트를 보다 빠르게 계산하기 위한 벡터화 및 병렬화 전략을 활용하기 위해(Subramani et al., 2021) 배치의 각 쿼리 항목 예제를 복제하여 배치의 모든 사실상의 예제에 포함해야 하므로 메모리 요구 사항이 2차적으로 증가한다는 것입니다. 고정된 메모리 리소스가 주어지면 이는 상당히 작은 배치 크기를 필요로 하며, 이는 그래디언트 클리핑 및 노이즈 처리 외에도 추가적인 해로운 영향을 미칩니다. 배치 내 소프트맥스 손실에 따른 효과적인 표현 학습은 배치 내 예제의 양과 다양성에 크게 의존하기 때문입니다. 개인 합성 쿼리로 학습하는 우리의 접근 방식은 다운스트림 듀얼 인코더 딥 검색 모델을 학습할 때 위의 제한을 배제합니다.4 접근 방식 우리는 원래 학습 데이터에 대한 쿼리 수준의 프라이버시를 보장하면서 다운스트림 딥 검색 시스템을 학습하기 위한 DP 합성 데이터를 얻기 위한 우리의 범용 접근 방식을 설명합니다.1) 조건부 쿼리 생성 작업에 대한 LM의 DP 학습 먼저, 개인 학습 데이터의 쿼리에 대해 사전 학습되지 않은 적절한 공개적으로 사전 학습된 LM을 얻습니다.DPAdafactor를 사용하여 조건부 쿼리 생성 작업으로 선택된 LM을 DP 미세 조정합니다.DP-Adafactor는 DP-SGD 알고리즘(Abadi et al., 2016)에 따라 클리핑되고 노이즈가 적용된 그래디언트를 수신하는 단순한 Adafactor 옵티마이저(Shazeer and Stern, 2018)입니다. 조건부 쿼리 생성 작업은 다음과 같습니다. 학습 데이터에서 쿼리-항목 문서 쌍(q, d)이 주어지면 LM은 입력 텍스트 &quot;d&quot;가 주어졌을 때 대상 텍스트 &quot;q&quot;를 생성하도록 미세 조정됩니다. 수십억 개의 매개변수가 있는 더 큰 LM의 경우 이러한 대규모 모델을 학습하는 데 드는 높은 비용을 극복하기 위해 매개변수 효율적인 미세 조정 기술(Lester et al., 2021; Hu et al., 2021)을 활용할 수 있습니다. 매개변수 효율적인 DP 미세 조정이 합성적으로 생성된 검색 데이터의 품질에 미치는 영향은 추가 연구 대상입니다. 2) DP LM을 사용한 합성 쿼리 생성 그런 다음 DP 미세 조정된 LM은 실제 쿼리를 대표하고 항목과 관련된 합성 쿼리를 생성할 수 있습니다. 각 항목 문서 d에 대해 모델에 입력 &quot;generate_query: d&quot;를 제공하여 일치하는 합성 쿼리 ğ를 생성합니다. 이 방법을 사용하면 각 문서에서 여러 합성 쿼리를 생성할 수 있습니다. 그런 다음 합성 학습 데이터 집합이 해당 합성 쿼리와 일치하는 원본 문서 집합으로 구성됩니다. 3) DP 합성 데이터로 듀얼 인코더 학습 마지막으로 합성 데이터는 DP의 사후 처리 속성(섹션 3.3 참조)에 의해 보장되는 대로 원본 쿼리에 대한 추가 DP 손실을 감수하지 않고 후속 학습 작업을 위해 안전하게 공유될 수 있습니다. 특히 표준 비공개 학습 방법을 사용하여 합성 학습 데이터에서 배치 내 소프트맥스 손실(식 1 참조)을 사용하여 듀얼 인코더 모델을 학습할 수 있으며, 원본 쿼리의 DP 보호도 보장할 수 있습니다. 5
--- EXPERIMENT ---
al 설정 5.1 데이터 세트 정보 검색 작업에 공개적으로 사용 가능한 데이터 세트를 사용합니다. 미세 조정 및 평가를 위해 광범위한 도메인과 개념을 포함하는 Bing 검색 로그에서 샘플링한 약 533,000개의 쿼리 문서 쌍으로 구성된 MSMARCO 데이터 세트(Bajaj et al., 2016)를 고려합니다. 또한 다양한 도메인에 걸친 정보 검색 데이터 세트가 포함된 BEIR 벤치마크 제품군(Thakur et al., 2021)의 데이터 세트를 고려하여 제로 샷 평가를 수행합니다. 5.2 합성 데이터 생성 5.2. 구현 세부 정보 모델 학습 합성 데이터 생성을 위해 다양한 크기 {Small, Base, Large, XL}와 개인 정보 보호 보장 € = {3,8,16, ∞}를 갖는 다양한 T5 LM(Raffel et al., 2020)을 학습하여 MSMARCO 데이터 세트의 해당 입력 문서가 주어진 합성 쿼리를 생성했습니다. T5 Small, Base, Large, XL 모델은 각각 약 6,000만, 2억 2,000만, 7억 7,000만, 30억 개의 매개변수를 갖습니다. 모든 실험은 TPU v4 칩에서 수행되었습니다. 하이퍼 매개변수 배치 크기 1024로 각 LM 오버에포크를 학습하고 입력 문서와 대상 쿼리에 대해 최대 토큰 길이를 384로 설정했습니다. 학습률이 0.001이고 클립 표준이 0.1인 DP-Adam 옵티마이저를 사용했습니다. (Li et al., 2021)에 따라 개인 정보 보호 매개변수 8 = 1/2n을 설정했습니다. 여기서 n은 학습 데이터 세트 크기입니다. 샘플링에는 p = 0.8인 핵심 샘플링 전략(Holtzman et al., 2019)을 사용했습니다. 위의 하이퍼 매개변수는 MSMARCO 학습 데이터 세트에서 DPfinetuned된 T5-Small 모델에 대한 최적의 하이퍼 매개변수를 식별하기 위해 하이퍼 매개변수 검색에서 선택되었습니다. 최적의 기준은 검증 데이터 세트에서 달성한 가장 높은 BLEU 점수였습니다. 학습률 0.001, 클리핑 규범 0.1, 배치 크기 1024, 에포크 30이 대부분 최상의 모델을 생성하는 것으로 나타났습니다. 다른 모든 T5 모델에서 이러한 하이퍼파라미터를 사용했습니다. 하이퍼파라미터 그리드는 표 1을 참조하세요. 표 1: 하이퍼파라미터 그리드 하이퍼파라미터 토큰 길이 학습률 클리핑 규범 배치 크기 에포크 값 입력: 384, 대상: 0.001.2k(k = {0, 1, 2, 3}) 5.2.2 데이터 합성 {0.1, 0.25, 0.5, 1} {128, 256, 512, 1024} {10, 20, 30} 원래 학습 데이터의 문서가 주어졌을 때 각 DP-finetuned T5 LM을 사용하여 합성 쿼리를 생성했습니다. 이러한 합성 쿼리와 원본 문서 쌍은 새로운 합성 데이터 세트를 구성합니다. 정성적 비교를 위해 표 2에서 원본 쿼리-문서 쌍과 다양한 모델 구성 및 개인 정보 보호 수준에서 생성된 합성 쿼리의 예를 제공합니다. 5.2.3 사전 학습 및 학습 데이터 중복 합성 데이터를 생성하는 데 사용된 사전 학습된 LM이 비공개로 만들려는 원본 쿼리 데이터에 대해 현저하게 학습되지 않았다는 점에 유의합니다. 그렇지 않으면 모델이 이미 데이터를 확인했을 것이므로 개인 정보 보호 보장이 훼손됩니다. 실험에서 이 문제를 해결하기 위해 T5 모델의 사전 학습 데이터인 C4 공통 크롤링 데이터 세트(Raffel et al., 2020)에서 MSMARCO 데이터 세트의 중복 범위를 확인하기 위한 분석을 수행했습니다. 10,000개의 쿼리 및 텍스트 쌍의 무작위 하위 세트를 선택하여 여러 번 실행하여 C4 데이터 세트에 정확한 일치 항목이 있는지 확인했습니다. 분석 결과, MSMARCO 문서의 상당 부분(~22%)이 평균적으로 C4에서 정확히 발견된 반면, MSMARCO 질의의 무시할 수 있는 비율(&lt;1.9%)만이 평균적으로 C4에서 정확히 발견되었습니다. 게다가 발견된 질의는 공개 지식으로 간주될 수 있는 일반적인 검색어인 경향이 있었습니다. 질의 수준의 개인 정보 보호에 관심이 있으므로 이 수준의 데이터 세트 중복은 합리적인 개인 정보 보호 보장을 제공하기에 허용 가능하다고 생각합니다. 섹션 6.3에서 훈련 절차의 경험적 개인 정보 보호 보장에 대한 보다 광범위한 연구를 제공합니다. 표 2: 합성 질의 예. 소스 문서 원래 질의 텍스트 € = ∞ € =T5-Base € =쇼의 주요 출연진인 미키 마우스, 미니 마우스, 도날드 덕, 데이지 덕, 구피, 플루토가 시리즈에 출연하며, 시청자와 상호 작용하여 문제 해결을 자극하는 데 중점을 둡니다. 미키 마우스 클럽하우스 쇼의 캐릭터 미키 마우스 쇼 출연진 시리즈에서 가장 중요한 캐릭터는 무엇입니까? 이 시리즈에는 무엇이 있습니까? € =dfrl T5-XL T5-Large T5-Small에 대한 이 코드의 내용은 무엇인가 € =미키 마우스 쇼의 문제 € =€ =디즈니 만화 쇼에서 가장 애니메이션이 많은 캐릭터는 무엇인가 캐릭터가 아닌 것은 무엇인가 스토리를 만드는 것은 무엇인가 5.3 다운스트림 검색 시스템 5.3.구현 세부 정보 모델 학습 각 데이터 소스(예: 원본 MSMARCO 데이터와 다양한 &amp; 및 모델 크기에 대한 합성 데이터 세트)에 대해 배치 내 소프트맥스 손실에 대해 별도의 이중 인코더 모델을 학습합니다. 쿼리 및 문서 인코더 모두에 대해 별도의 사전 학습된 T5-Base 인코더를 활용하여 두 인코더 간에 매개변수를 공유합니다. 데이터 합성과 유사하게 이러한 종류의 인코더를 사용하여 원래 쿼리에서 상당히 사전 학습되지 않았는지 확인합니다. 검색 모델의 인코더는 합성 데이터를 생성하는 데 사용된 T5 모델과 다르다는 점을 강조합니다. 하이퍼파라미터 듀얼 인코더 모델 학습의 하이퍼파라미터에 대해 학습률 0.001, 배치 크기 32, 에포크 5, 문서의 최대 토큰 길이 384, 쿼리의 최대 토큰 길이 128을 사용했습니다. 직접 DP 미세 조정 실험의 경우 클리핑 규범 0.1을 사용했습니다. 5.3.2 기준 접근 방식 접근 방식에 대한 기준 비교는 원본 데이터에서 직접 DP 학습된 딥 검색 시스템과 비교하는 것입니다. 직접 DP 학습의 경우 위와 동일한 하이퍼파라미터를 사용했지만 섹션 3.3.2에서 설명한 메모리 제약 조건을 감안하여 듀얼 인코더 모델을 DP 학습하기 위한 배치 크기를 32로 크게 줄여야 했습니다. 검색 시스템에서 DP 보장을 달성하기 위한 범용 방법을 비교하는 것이 목적이므로 다른 다운스트림 딥 검색 모델로 실험하지 않습니다. 6 평가 6.1 검색 작업에 대한 평가 우리는 제로샷 평가를 위해 MSMARCO 테스트 데이터 세트와 다양한 다른 BEIR 검색 데이터 세트에서 검색 모델을 평가합니다. 우리는 품질과 위치를 모두 고려하여 추천 목록에서 항목의 관련성과 순위 품질을 측정하는 상위 10개 예측(NDCG@10)에 대한 정규화된 할인 누적 이득 점수에 대해 평가합니다. 우리는 또한 상위 10개 예측에 대한 리콜 점수(리콜@10)를 보고합니다. 이것은 기준 진실 추천이 상위 10개 예측에 나타나는 횟수의 백분율을 측정합니다. 우리는 단일 학습 실행의 평가 결과를 보고합니다. 6.1.1 검색 및 검색 절차 듀얼 인코더 검색 모델을 평가하려면 추론 단계에 대한 쿼리-문서 최근접 이웃 검색 구현이 필요합니다. 실험을 위해 오픈소스 라이브러리인 Scalable Nearest Neighbors(ScaNN) 라이브러리를 사용했는데, 이 라이브러리는 빠르고 확장 가능한 근사 최근접 이웃 검색 절차를 제공합니다(Guo et al., 2020). 이 절차는 ScaNN의 무차별 대입 스코어링과 내적 거리 설정을 사용하여 실행되었습니다. 6.1.MSMARCO 평가 표 3은 다양한 생성 모델 구성과 다양한 개인 정보 보호 수준에서 합성적으로 생성된 데이터에 대해 정기적으로 학습한 딥 검색 모델에 대한 MSMARCO 테스트 세트의 평가 결과를 보여줍니다. 벤치마크 비교를 위해 표 4에 다양한 개인 정보 보호 수준에서 원본 데이터에 대해 직접 DP로 학습한 딥 검색 모델에 대한 평가 결과를 표시합니다. 두 표의 상단 행에는 DP 보장 없이(즉, € = ∞) 원본 데이터에 대해 정기적으로 미세 조정된 듀얼 인코더 모델에 대한 또 다른 기준 참조 평가도 제공합니다. DP가 있는 합성 데이터에서 학습한 검색 모델이 원본 데이터에 대해 DP로 학습한 검색보다 상당히 우수한 성능을 보이는 것을 관찰했습니다. 섹션 3.3.2에서 논의했듯이 대조 스타일 손실을 사용하여 DP 모델을 학습하는 데는 여러 가지 과제가 있습니다. 대조 손실에 대한 DP 학습을 구현하는 우리의 순진한 접근 방식은 원본 데이터에 대한 DP 학습의 유용성이 낮은 이유를 설명할 가능성이 높습니다. 또한, 우리의 DP 합성 데이터는 기본적으로 프로세스에 추가적인 공개 지식을 도입하는데, 이는 우리가 공개적으로 사전 학습된 LM을 활용하기 때문입니다. 게다가, 우리는 비 DP 합성 데이터로 학습된 검색 모델이 원본 데이터로 학습된 검색 모델보다 성능이 우수하다는 것을 발견했습니다. 이는 합성 데이터 생성이 실제로 원본 데이터를 증강시키고 어느 정도 일반화를 개선한다는 것을 시사하는데, 이는 추가적인 공개 정보를 도입하든 데이터 정리를 통해서든 마찬가지입니다. 사실, 심층 검색을 위한 언어 모델을 사용하여 합성 데이터 생성을 통한 데이터 증강은 최근 몇 년 동안 상당한 관심을 받고 있는 연구 분야입니다(Dai et al., 2022; Bonifacio et al., 2022). 또한 우리는 모델 크기가 커질수록 성능이 향상되는 것을 관찰합니다. 이는 과매개변수화된 모델에서 DP-SGD가 이전에 생각했던 것보다 훨씬 더 나은 성능을 보일 수 있음을 보여준 유사한 이전 결과와 일치합니다(De et al., 2022; Li et al., 2021). 전반적으로, 우리는 DP LM의 합성 데이터로 훈련하는 것이 검색 모델에서 DP 보장과 효율성을 달성하는 데 실행 가능하다는 것을 보여줍니다.표 3: 검색 모델 평가.위: 다양한 e와 고정 모델 크기 T5-Base로 DP 합성 데이터로 훈련.아래: 다양한 모델 크기와 고정 € = 3으로 DP 합성 데이터로 훈련.€ |NDCG@10 | Recall@Source Original T5-Base 0.0.∞ 0.0.T5-Base0.0.T5-Base0.0.T5-Base0.0.T5-XL0.0.T5-Large0.0.T5-BaseT5-Small0.0.0.0.표 4: 다양한 €로 원본 데이터에서 직접 DP 학습된 검색 모델 평가 | ∞ 출처 € | NDCG@Recall @Original 0.0.Original0.0.Original0.0.Original0.0.6.1.제로샷 평가 또한 합성 데이터에서 학습된 검색 모델의 제로샷 일반화 기능을 평가합니다. DP가 없는(즉, € = ∞) 원본 데이터에서 학습된 검색 모델과 € = 16을 비교합니다. 결과는 표 5를 참조하세요. 다시 한번, 우리의 결과는 원본 데이터에 대한 DP 훈련과 비교했을 때 DP 합성 데이터의 상당한 이점을 보여주며, 거의 일치하고 어떤 경우에는 비-DP 결과보다 성능이 우수합니다. 이는 합성 데이터 생성의 이점이 적어도 제로샷 일반화 작업에서 합리적인 수준의 개인 정보 보호를 통해 DP 훈련의 유용성 저하보다 더 클 수 있음을 시사합니다. 6.2 합성 데이터 세트와 원본 데이터 세트 간의 유사성 DP 훈련된 T5 모델에서 생성된 합성 데이터와 원본 데이터 간의 유사성 척도를 계산합니다. 합성 데이터는 원본 데이터에서 일대일로 생성되므로 BLEU 점수를 계산하여 유사성을 평가할 수 있습니다(Post, 2018). 또한 텍스트 분포의 유사성을 비교하는 데 더 유능한 것으로 나타난 MAUVE 점수도 계산합니다(Pillutla et al., 2021). 점수는 표 6을 참조하세요. 우리는 비DP finetune 모델이 이러한 지표에서 예상할 수 있는 만큼 유사한 합성 데이터를 생성하고, 유한 e에서 상당한 감소가 있으며, 더 높은 &amp;와 증가하는 모델 크기에 따라 유사성이 증가합니다. 유사성 점수를 검색 평가 결과와 비교함으로써, 더 큰 모델이 합성 데이터 유사성을 극적으로 개선하는 반면, 다운스트림 검색 성능은 모델 크기가 증가함에 따라 비교적 더 적은 이득을 보입니다. 6.3 경험적 프라이버시 DP가 제공하는 증명 가능한 프라이버시는 e가 커짐에 따라 상당히 감소하지만, 이전 연구에 따르면 이러한 큰 값조차도 최첨단 프라이버시 공격에 대한 강력한 보호를 제공할 수 있음이 밝혀졌습니다(Carlini et al., 2019, 2022; Ponomareva et al., 2023). 우리의 훈련 기술이 여전히 이러한 경향을 따르는지 확인하기 위해, 우리는 (Carlini et al., 2019)에서 도입된 카나리아 노출 지표를 사용하여 DP로 훈련된 언어 모델의 경험적 프라이버시 누출을 평가합니다. 이 기술은 경험적 프라이버시를 평가하는 데 자주 사용됩니다(Zanella-Béguelin 등, 2020; Ramaswamy 등, 2020; Jagielski 등, 2022). 이 테스트를 수행하기 위해 카나리아라고 하는 개인 정보가 있는 예제를 구성하고, 표 5: DP 합성 데이터로 학습된 검색 모델과 € = 16인 직접 DP로 학습된 검색 모델의 제로샷 평가. 위 표: NDCG@10. 아래 표: 리콜 @10. 소스 NDCG@arguana cqadup dbpedia fiqa hotpot nfcorpus quora scidocs scifact covid touche Original 0.2653 0.2659 0.3905 0.2257 0.5232 0.2974 0.8126 0.1857 0.4527 0.4971 0.Original 16 0.2132 0.0990 0.1272 0.0870 0.1422 0.1331 0.6856 0.0792 0.2051 0.3133 0.T5-베이스 16 0.2757 0.2474 0.3728 0.2140 0.5122 0.2971 0.7850 0.1750 0.4645 0.4351 0.소스 € arguana cqadup dbpedia Recall@fiqa hotpot nfcorpus quora scidocs scifact covid touche Original 0.5569 0.3368 0.1479 0.2440 0.Original 16 0.4388 0.1325 0.0313 0.0906 0.T5-베이스 16 0.5768 0.3165 0.1217 0.2261 0.0.0.0.0.8989 0.1071 0.5801 0.0116 0.0.7762 0.0503 0.2969 0.0063 0.0.8763 0.1066 0.5848 0.0101 0.표 6: 생성된 합성 데이터의 유사성 점수.위: 다양한 e 및 고정된 모델 크기.아래: 고정된 € = 3인 다양한 모델 크기.모델 € BLEU MAUVE T5-Base 0.2939 0.T5-Base 16 0.0984 0.T5-Base 8 0.3 0.1021 0.€ 매개변수의 하한(Stock et al., 2022)을 사용하면 이러한 순위를 e의 하한 약 0.015로 해석할 수 있습니다.이 큰 격차는 언어 모델에서 DP-SGD의 경험적 개인 정보 보호에 대한 이전 연구 결과와 일치합니다(Carlini et al., 2019; Ponomareva et al., 2023).표 7: 개인 정보 유출. 0.T5-Base0.0.T5-XL T5-LargeT5-Base 0.1096 0.0.0940 0.0.0436 0.Model € 반복 = 순위 누출 T5-Base 1/T5-Base 16 43/67% 0% 1/100 100% 32/0% T5-Small 3 반복 = 순위 누출 원래의 학습 데이터에 하위 집합을 도입하고 모델이 주입된 카나리아를 출력할 가능성을 측정합니다. 일반적으로 카나리아 생성은 도메인에 따라 결정되므로 다음 세 가지 유형의 쿼리-문서 쌍을 사용하여 검색 애플리케이션에 대한 카나리아를 설계합니다. (무작위 쿼리, 무작위 10자리 문자열), (무작위 쿼리, 해당 문서 + 무작위 10자리 문자열), (무작위 쿼리, 무작위 문서 + 무작위 10자리 문자열). 각 카나리아의 비밀 부분은 무작위 10자리 문자열입니다. 우리는 다른 DP 보장을 사용하여 이 수정된 데이터 세트에서 언어 모델을 훈련하고, 합성 데이터 세트를 생성하고, 카나리아 노출을 평가합니다. 우리는 다른 카나리아와 DP 보장을 사용하여 이 실험을 여러 번 수행하고, 메트릭을 평균화하고 결과를 표 7에 보고합니다. 예상대로 DP 없이 훈련하면 상당한 누출이 발생합니다. 10번 반복된 카나리아는 자주 추출 가능하고, 100번 반복된 카나리아는 항상 추출 가능합니다. 그러나 16이라는 큰 €를 사용한 우리의 접근 방식은 모델이 비밀을 누출하는 것을 방지하고 순위를 상당히 높입니다. 공격 성공률을 7로 변환하는 최신 기술
--- CONCLUSION ---
저희의 연구는 심층 검색 시스템을 훈련할 때 DP 보장을 보장하는 데 중점을 두었습니다. 저희는 종종 사용되는 일괄 처리 소프트맥스 손실 함수로 이러한 시스템을 DP 훈련하는 것의 한계에 대해 논의했는데, 이는 예제별로 분해할 수 없습니다. 저희는 DP LM을 사용하여 다운스트림 심층 검색 훈련을 위한 비공개 합성 쿼리를 생성하는 접근 방식을 소개합니다. 이 접근 방식은 다운스트림 훈련 전에 쿼리 수준 개인 정보 보호에 대한 이론적 보장을 보장하여 DP 훈련의 일부 한계를 우회합니다. 나아가 저희는 저희의 접근 방식이 쿼리 개인 정보를 손상시키지 않으면서 직접 DP 훈련에 비해 검색 품질을 향상시킨다는 것을 경험적으로 증명합니다. 저희의 연구는 LM이 DP 훈련 ML 시스템의 중대한 한계를 극복할 수 있는 잠재력을 강조합니다. 한계 저희의 접근 방식에는 몇 가지 한계가 있습니다. 첫째, 저희는 더 큰 LM이 더 높은 품질의 합성 쿼리를 생성한다는 것을 관찰했지만, 그러한 대규모 모델을 훈련하는 데는 컴퓨팅 비용이 너무 많이 들 수 있다는 점에 유의할 가치가 있습니다. DP-학습에 맞춰진 보다 매개변수 효율적인 미세 조정 방법을 탐색하면 이러한 대규모 모델을 학습하는 데 따른 계산 부담을 완화할 수 있습니다. 둘째, 합성 쿼리를 생성하는 데 사용되는 공개적으로 사전 학습된 LM은 우리가 비공개화하려는 원래 쿼리에 대해 사전 학습되지 않아야 합니다. 이는 비공개 합성 쿼리를 생성하는 데 적합한 사전 학습된 LM을 선택하는 데 제약을 가합니다. 다음으로, 표 2의 합성 예에서 볼 수 있듯이 DP-미세 조정된 LM은 때때로 불일치한 쿼리를 생성할 수 있으며, 이는 데이터의 관련성과 해석 가능성을 제한할 수 있습니다. 마지막으로, 우리의 접근 방식은 쿼리 수준의 개인 정보 보호를 독점적으로 보장한다는 것을 인식하는 것이 중요합니다. 보다 일반적인 예 수준의 개인 정보 보호를 달성하려면 다른 접근 방식이나 보다 기존의 DP-학습 방법을 사용해야 할 수 있습니다. 위험 및 윤리적 고려 사항 데이터 개인 정보 보호는 개인화된 머신 러닝 시스템의 책임 있는 개발에 있어 중요한 고려 사항입니다. 우리의 연구는 심층 검색 시스템에서 데이터 개인 정보 보호 문제를 해결하기 위한 잠재적 솔루션을 직접 제공합니다. 그러나 원치 않는 개인정보 보호 위험을 방지하기 위해 우리 접근 방식의 개인정보 보호 보장에 대한 위의 제한 사항을 인정하는 것이 중요합니다.감사의 말 도움이 되는 토론을 해주신 Rishabh Bansal, Manoj Reddy, Andreas Terzis, Sergei Vassilvitskii, Abhradeep Guha Thakurta, Shuang Song, Arthur Asuncion, Heather Yoon에게 감사드립니다.또한 검색 훈련 파이프라인을 설정하는 데 도움을 주신 Jianmo Ni에게 감사드립니다.마지막으로 이 작업 전반에 걸쳐 YouTube 광고 리더십 Shobha Diwakar, Marija Mikic, Ashish Gupta의 지원과 격려에 감사드립니다.참고 문헌 Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang.2016. 차등 개인정보 보호를 통한 딥 러닝.2016 ACM SIGSAC 컴퓨터 및 통신 보안 컨퍼런스 회의록, 308-318페이지. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: 인간이 생성한 기계 독해 이해 데이터 세트. arXiv 사전 인쇄본 arXiv:1611.09268. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: 대규모 언어 모델을 사용한 정보 검색을 위한 데이터 증강. arXiv 사전 인쇄본 arXiv:2202.05144. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer. 2022. 첫 번째 원리에서 멤버십 추론 공격. 2022 IEEE 보안 및 개인 정보 보호 심포지엄(SP), 1897-1914페이지. IEEE. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song. 2019. 비밀 공유자: 신경망에서 의도치 않은 기억 평가 및 테스트. USENIX 보안 심포지엄, 267권. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson 등. 2021. 대규모 언어 모델에서 학습 데이터 추출. 제30회 USENIX 보안 심포지엄(USENIX 보안 21), 2633-2650페이지. Paul Covington, Jay Adams, Emre Sargin. 2016. YouTube 추천을 위한 딥 신경망. 미국 뉴욕에서 열린 제10회 ACM 추천 시스템 컨퍼런스 회의록. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, Ming-Wei Chang. 2022. Promptagator: 8개 예제에서 Few-shot dense 검색. arXiv 사전 인쇄본 arXiv:2209.11755. Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, Borja Balle. 2022. 규모를 통해 높은 정확도의 차등적 개인 정보 보호 이미지 분류 잠금 해제. arXiv 사전 인쇄본 arXiv:2204.13650. Cynthia Dwork, Aaron Roth 외. 2014. 차등 개인 정보 보호의 알고리즘 기반. 이론 컴퓨터 과학의 기초 및 추세Ⓡ, 9(34):211-407. Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, Diego Garcia-Olano. 2019. 엔터티 검색을 위한 밀집 표현 학습. arXiv 사전 인쇄본 arXiv:1909.10506. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar. 2020. 이방성 벡터 양자화를 통한 대규모 추론 가속화. International Conference on Machine Learning, 3887-3896페이지. PMLR. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi. 2019. 신경 텍스트 퇴화의 호기심 많은 사례. arXiv 사전 인쇄본 arXiv:1904.09751. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2021. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685. Mengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu, Aidong Zhang. 2020. 차등 프라이버시 보장을 통한 쌍별 학습. AAAI 인공지능 컨퍼런스 회의록, 34권, 694-701페이지. Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang. 2020. Facebook 검색에서의 임베딩 기반 검색. 제26회 ACM SIGKDD 국제 지식 발견 및 데이터 마이닝 컨퍼런스 회의록, 2553-2561쪽. Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen. 2023. 검색 기반 언어 모델의 개인 정보 보호 의미. arXiv 사전 인쇄본 arXiv:2305.14888. Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot 등. 2022. 기억된 학습 사례의 망각 측정. arXiv 사전 인쇄본 arXiv:2207.00099. Yilin Kang, Yong Liu, Jian Li, Weiping Wang. 2021. 차등적 개인 정보 보호 쌍방향 학습을 위한 보다 날카로운 유틸리티 경계를 향해. arXiv 사전 인쇄본 arXiv:2105.03033. Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. 2020. 오픈 도메인 질문 답변을 위한 밀집 구절 검색. arXiv 사전 인쇄본 arXiv:2004.04906. Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, Richard Socher. 2019. Ctrl: 제어 가능한 생성을 위한 조건부 변환기 언어 모델. arXiv 사전 인쇄본 arXiv:1909.05858. Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C Wallace. 2021. Does bert pretrained on clinical notes reveals sensitive data? arXiv 사전 인쇄본 arXiv:2104.07762. Brian Lester, Rami Al-Rfou, Noah Constant. 2021. The power of scale for parameters-efficient prompt tuning. arXiv 사전 인쇄본 arXiv:2104.08691. Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto. 2021. 대규모 언어 모델은 강력한 차등적 개인 정보 보호 학습자가 될 수 있습니다. arXiv 사전 인쇄본 arXiv:2110.05679. Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schoelkopf, Mrinmaya Sachan. 2022. 안전한 데이터 공유를 위한 차등적 개인 정보 보호 언어 모델. arXiv 사전 인쇄본 arXiv:2210.13918. Fatemehsadat Mireshghallah, Richard Shin, Yu Su, Tatsunori Hashimoto, Jason Eisner. 2022. 의미 파서의 개인 정보 보호 도메인 적응. arXiv 사전 인쇄본 arXiv:2212.10520. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. 대형 이중 인코더는 일반화 가능한 검색기입니다. arXiv 사전 인쇄본 arXiv:2112.07899. 크리슈나 필루틀라, 스와바 스와얌딥타, 로완 젤러스, 존 딕스턴, 숀 웰렉, 예진 최, 자이드 하르차우이. 2021. Mauve: 발산 경계를 사용하여 신경 텍스트와 인간 텍스트 간의 격차 측정. 신경 정보 처리 시스템의 발전, 34:4816-4828. 나탈리아 포노마레바, 후세인 하지메, 알렉스 쿠라킨, 정 쉬, 카슨 데니슨, H 브렌던 맥마한, 세르게이 바실비츠키, 스티브 치엔, 압라딥 타쿠르타. 2023. ML을 dp-fy하는 방법: 차등 개인 정보 보호를 통한 머신 러닝에 대한 실용 가이드. arXiv 사전 인쇄본 arXiv:2303.00654. 맷 포스트. 2018. 블루 점수 보고의 명확성 요구. arXiv 사전 인쇄본 arXiv:1804.08771. Pranav Putta, Ander Steele, Joseph W Ferrara. 2022. 합성 데이터 생성을 위한 차등적 개인 정보 보호 조건부 텍스트 생성. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang. 2020. Rocketqa: 오픈 도메인 질문 답변을 위한 고밀도 구절 검색에 대한 최적화된 학습 접근 방식. arXiv 사전 인쇄본 arXiv:2010.08191. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. The Journal of Machine Learning Research, 21(1):5485-5551. Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, Françoise Beaufays. 2020. 사용자 데이터를 기억하지 않고 프로덕션 언어 모델 학습. arXiv 사전 인쇄본 arXiv:2009.10031. Timo Schick과 Hinrich Schütze. 2021. 자연어 명령어를 사용한 Few-shot 텍스트 생성. 2021 자연어 처리 경험적 방법 컨퍼런스 회의록, 390-402쪽. Noam Shazeer와 Mitchell Stern. 2018. Adafactor: 선형 메모리 비용이 있는 적응 학습 속도. 기계 학습 국제 컨퍼런스, 4596-4604쪽. PMLR. Pierre Stock, Igor Shilov, Ilya Mironov, Alexandre Sablayrolles. 2022. rényi 차등 개인 정보 보호를 사용한 재구성 공격 방어. arXiv 사전 인쇄본 arXiv:2202.07623. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath. 2021. Just-in-Time 컴파일 및 벡터화를 통해 빠른 차등적 개인 정보 보호 sgd 구현. 신경 정보 처리 시스템의 발전, 34:26409-26421. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, Iryna Gurevych. 2021. Beir: 정보 검색 모델의 제로샷 평가를 위한 이질적 벤치마크. arXiv 사전 인쇄본 arXiv:2104.08663. Zhiyu Xue, Shaoyang Yang, Mengdi Huai, Di Wang. 2021. 차등적 개인 정보 보호 쌍별 학습 재검토. IJCAI, 3242-3248페이지. Xiang Yue, Huseyin A Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Huan Sun, David Levitan, and Robert Sim. 2022. 차등 프라이버시를 갖춘 합성 텍스트 생성: 간단하고 실용적인 레시피. arXiv 사전 인쇄본 arXiv:2210.14348. Santiago Zanella-Béguelin, Lukas Wutschitz, Shruti Tople, Victor Rühle, Andrew Paverd, Olga Ohrimenko, Boris Köpf, and Marc Brockschmidt. 2020. 자연어 모델 업데이트의 정보 누출 분석. 컴퓨터 및 통신 보안에 관한 2020 ACM SIGSAC 컨퍼런스 회의록, 363-375쪽. Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang 등. 2024. 좋은 점과 나쁜 점: 검색 증강 생성(rag)의 개인 정보 보호 문제 탐색. arXiv 사전 인쇄 arXiv:2402.16893.
