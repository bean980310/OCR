--- ABSTRACT ---
텍스트 프롬프트나 특정 이미지를 사용하여 고품질 3D 아바타를 생성하는 새로운 방법인 AvatarBooth를 소개합니다. 간단한 텍스트 설명에 기반하여 아바타를 합성할 수 있는 이전 접근 방식과 달리, 저희 방법은 텍스트 기반 모델 생성 및 편집을 지원하면서도 캐주얼하게 캡처한 얼굴이나 신체 이미지에서 개인화된 아바타를 생성할 수 있습니다. 저희의 주요 기여는 인간의 얼굴과 신체에 대해 별도로 이중 미세 조정 확산 모델을 사용하여 정확한 아바타 생성을 제어하는 것입니다. 이를 통해 얼굴 모양, 옷, 액세서리의 복잡한 세부 사항을 캡처하여 매우 사실적인 아바타를 생성할 수 있습니다. 또한, 확산 모델에서 합성된 머리 이미지의 다중 뷰 일관성을 향상시키고 제어되지 않는 인간 포즈의 간섭을 제거하기 위해 최적화 프로세스에 포즈 일관성 제약을 도입합니다. 또한, 3D 아바타 생성의 조대-미세 감독을 용이하게 하는 다중 해상도 렌더링 전략을 제시하여 제안된 시스템의 성능을 향상시킵니다. 결과 아바타 모델은 추가 텍스트 설명을 사용하여 추가로 편집하고 모션 시퀀스로 구동할 수 있습니다. 실험 결과 AvatarBooth는 텍스트 프롬프트나 특정 이미지에서 렌더링과 기하학적 품질 측면에서 이전의 텍스트-3D 방식보다 성능이 뛰어납니다. 키워드 아바타 생성, 확산 모델, 신경 암묵적 필드, 모델 미세 조정
--- INTRODUCTION ---
텍스트나 이미지에서 3D 인간 아바타를 만드는 것은 컴퓨터 비전과 컴퓨터 그래픽 모두에서 오랫동안 어려운 작업으로, 디지털 인간, 영화 산업, 가상 현실을 포함한 광범위한 다운스트림 애플리케이션에 중요합니다. 이전 접근 방식은 고가이고 복잡한 수집 장비에 의존하여 충실도가 높은 아바타 모델을 재구성했습니다[Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022]. 그러나 이러한 방법에는 소비자 수준 애플리케이션에 사용하기에는 너무 비싼 다중 뷰 이미지나 깊이 맵이 필요합니다. 또는 다른 방법은 신경망을 활용하여 단일 이미지 입력에서 그럴듯한 아바타 모델을 예측합니다[Saito et al. 2019; Xiu et al. 2022; Zheng et al. 2021]. 그럼에도 불구하고 이러한 접근 방식은 적합한 이미지의 가용성에 제한이 있으며 참조 이미지가 제공되면 편집할 수 없습니다. 최근, 대규모 사전 훈련된 시각-언어 모델을 기반으로 한 3D 콘텐츠 생성은 유망한 성능을 보였습니다[Lin et al. 2022; Poole et al. 2023; Raj et al. 2023]. 구체적으로, 이러한 방법은 대규모 사전 훈련된 모델에서 학습한 일반적인 2D 이미지 사전을 활용하여 암묵적 3D 표현의 최적화를 안내합니다. 초기 시도에서는 대조적 언어-이미지 사전 훈련(CLIP)[Radford et al. 2021b]을 활용하여 텍스트 프롬프트가 주어진 아바타의 모습을 합성했습니다[Hong et al. 2022; Youwang et al. 2022]. 그런 다음, 점수 증류 샘플링(SDS)[Poole et al. 2023]을 추가로 제안하여 사전 훈련된 확산 모델에서 2D 지식을 증류하여 성능을 높였습니다[Ho et al. 2020; Rombach et al. 2022; Saharia et al. 2022]에서 미분 가능한 렌더링을 통한 3D 콘텐츠 생성으로 전환되었습니다. 상당한 진전이 있었지만, 현재의 방법은 여전히 복잡한 포즈와 천 주름, 얼굴 모양과 같은 세부적인 3D 구조를 포함하는 인간 대상의 고품질 모양과 외관을 합성할 수 없습니다. 반면에 입력 이미지에 해당하는 임의의 신원의 사용자 지정 아바타를 생성하는 것은 여전히 어려운 문제로 남아 있습니다. DreamBooth3D [Raj et al. 2023]는 개인화된 3D 자산을 생성하기 위한 솔루션을 제공했지만 이미지에 표시된 정확한 신원으로 고품질의 인간 얼굴을 재현하는 데 어려움을 겪습니다. 아바타의 세부적인 외관 합성과 멀티모달 기반 사용자 지정을 모두 지원하려면 새로운 미세 조정 전략이 필요합니다. 이 논문에서는 텍스트 프롬프트 또는 이미지 세트에서 고품질의 사용자 지정 가능한 아바타를 생성하기 위한 AvatarBooth라는 새로운 방법을 제안합니다. 이 방법은 특정 개인의 시각적 및 텍스트적 특징을 정확하게 반영하는 신원 사용자 지정 3D 아바타를 생성하는 것을 목표로 합니다. 이를 위해 신경 암묵적 표면[Wang et al. 2021]을 학습하여 얼굴과 몸에 대해 각각 사전 훈련되거나 미세 조정된 잠재 확산 모델로 감독되는 인간 아바타의 모양과 모습을 표현합니다. 한편, 포즈 일관성 제약 조건을 도입하여 외관 사용자 지정 생성 작업에서 확산 모델의 미세 조정을 향상시켜 표준 포즈 공간에서 일관된 모습으로 보다 정확한 다중 뷰 감독을 제공합니다. 또한 다중 해상도 SDS 체계를 도입하여 아바타의 미세 구조와 모습을 조대에서 미세한 방식으로 예측합니다. 이 모델은 사람의 사진 몇 장을 활용하여 개인의 고유한 모습을 가질 뿐만 아니라 입력 텍스트 프롬프트에 지정된 추상적 특징과 일치하는 3D 아바타를 합성할 수 있습니다. 이러한 추상적 특징에는 &#39;특정 스타일의 안경이나 모자 착용&#39;과 같은 속성이 포함되며, 이는 아바타의 전반적인 시각적 정체성을 편집하고 수정하는 데 사용자 친화적입니다. 우리의 접근 방식은 대규모 언어 시각 모델과 구체적인 입력 이미지 모두에서 사전 확률을 활용하여 입력 모양에 충실하면서도 텍스트 프롬프트를 통해 편집 가능한 아바타를 생성하도록 설계되었습니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. • 텍스트 프롬프트와 임의의 이미지를 모두 입력으로 지원하는 3D 인간 아바타 생성 프레임워크를 제안합니다. 이중 잠재 확산 모델을 도입하여 얼굴과 몸 생성을 별도로 감독하여 자세한 얼굴 모양, 옷 및 착용감을 제공합니다. • 포즈 일관성 제약 조건을 도입하여 특정 사람의 사진을 고려하여 대규모 사전 학습된 확산 모델을 사용자 정의합니다. ControlNet[Zhang 및 Agrawala 2023]을 사용하여 합성 이미지의 다중 뷰 일관성을 향상시켜 제어되지 않는 인간 포즈의 간섭을 제거하고 고품질 모양과 오메트리를 제공합니다. ge • 우리는 3D 아바타 생성을 거칠게-미세하게 감독하는 다중 해상도 점수 증류 샘플링 전략을 제시합니다. 실험 결과, 이 전략은 렌더링 품질을 향상시킬 뿐만 아니라 생성의 견고성도 향상시킨다는 것이 밝혀졌습니다.
--- RELATED WORK ---
S 텍스트 기반 2D&amp;3D 생성. 최근 몇 년 동안, 확산 모델[Dhariwal 및 Nichol 2021; Ho et al. 2020; Song et al. 2020]은 고품질 이미지 합성에서 놀라운 성능으로 인해 빠르게 개발되었습니다. 확산 모델의 핵심 구조는 스케줄러에 따라 노이즈를 추가하는 전방 확산 단계와 노이즈를 제거하는 후방 생성 단계로 구성됩니다. 가우시안 노이즈에서만 무조건 생성하는 것 외에도 확산 모델은 텍스트 프롬프트 또는 이미지를 입력으로 사용하여 고품질 이미지를 생성할 수 있습니다. 다양한 확산 모델 중에서 잠재 확산 모델[Rombach et al. 2022]은 이미지 품질과 메모리 사용 간에 좋은 균형을 이루는 유망한 텍스트-이미지 모델로 부상했습니다. 3D 콘텐츠 생성과 관련하여 기존
--- METHOD ---
영어: 프롬프트 생성 모드(빨간색), 모양 사용자 지정 모드(파란색) 또는 하이브리드 모드(녹색)에서 3D 인간 아바타를 생성할 수 있습니다. 초록 텍스트 프롬프트나 특정 이미지를 사용하여 고품질 3D 아바타를 생성하는 새로운 방법인 AvatarBooth를 소개합니다. 간단한 텍스트 설명에 따라 아바타를 합성할 수 있는 이전 접근 방식과 달리, 이 방법은 텍스트 기반 모델 생성 및 편집을 지원하는 동시에 캐주얼하게 캡처한 얼굴이나 신체 이미지에서 개인화된 아바타를 생성할 수 있습니다. 우리의 주요 기여는 인간의 얼굴과 신체에 대해 별도로 이중 미세 조정 확산 모델을 사용하여 정확한 아바타 생성을 제어하는 것입니다. 이를 통해 얼굴 모양, 의복 및 액세서리의 복잡한 세부 사항을 캡처하여 매우 사실적인 아바타를 생성할 수 있습니다. 또한 최적화 프로세스에 포즈 일관성 제약을 도입하여 확산 모델에서 합성된 머리 이미지의 다중 뷰 일관성을 향상시키고 제어되지 않는 인간 포즈의 간섭을 제거합니다. 또한, 우리는 3D 아바타 생성의 조대-미세 감독을 용이하게 하는 다중 해상도 렌더링 전략을 제시하여 제안된 시스템의 성능을 향상시킵니다. 결과 아바타 모델은 추가 텍스트 설명을 사용하여 추가로 편집할 수 있으며 모션 시퀀스로 구동할 수 있습니다.
--- EXPERIMENT ---
s는 AvatarBooth가 텍스트 프롬프트 또는 특정 이미지에서 렌더링 및 기하학적 품질 측면에서 이전의 텍스트-3D 방법보다 성능이 우수함을 보여줍니다.키워드 아바타 생성, 확산 모델, 신경 암묵적 필드, 모델 미세 조정 서론 텍스트나 이미지에서 3D 인간 아바타를 만드는 것은 컴퓨터 비전과 컴퓨터 그래픽 모두에서 오랫동안 어려운 작업으로, 디지털 인간, 영화 산업, 가상 현실을 포함한 광범위한 다운스트림 애플리케이션의 핵심입니다.이전 접근 방식은 고가의 복잡한 수집 장비에 의존하여 고충실도 아바타 모델을 재구성했습니다[Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022].그러나 이러한 방법에는 소비자 수준 애플리케이션에는 감당할 수 없는 다중 뷰 이미지 또는 깊이 맵이 필요합니다.또는 다른 방법은 신경망을 활용하여 단일 이미지 입력에서 그럴듯한 아바타 모델을 예측합니다[Saito et al. 2019; Xiu et al. 2022; Zheng et al. 2021]. 그럼에도 불구하고 이러한 접근 방식은 적합한 이미지의 가용성에 의해 제한되며 참조 이미지가 제공되면 편집할 수 없습니다. 최근 대규모 사전 훈련된 시각 언어 모델을 기반으로 한 3D 콘텐츠 생성은 유망한 성능을 보였습니다 [Lin et al. 2022; Poole et al. 2023; Raj et al. 2023]. 구체적으로 이러한 방법은 대규모 사전 훈련된 모델에서 학습한 일반적인 2D 이미지 사전을 활용하여 암묵적 3D 표현의 최적화를 안내합니다. 초기 시도에서는 대조 언어-이미지 사전 훈련(CLIP) [Radford et al. 2021b]을 활용하여 텍스트 프롬프트가 주어진 아바타의 모습을 합성했습니다 [Hong et al. 2022; Youwang et al. 2022]. 그런 다음 Score Distillation Sampling(SDS) [Poole et al. 2023]은 사전 학습된 확산 모델에서 2D 지식을 추출하여 [Ho et al. 2020; Rombach et al. 2022; Saharia et al. 2022] 미분 가능한 렌더링을 통한 3D 콘텐츠 생성으로 성능을 높이기 위해 추가로 제안되었습니다. 상당한 진전이 이루어졌지만 현재 방법은 여전히 복잡한 포즈와 천 주름 및 얼굴 모양과 같은 자세한 3D 구조를 포함하는 인간 객체의 고품질 모양과 외관을 합성할 수 없습니다. 반면, 입력 이미지에 해당하는 임의의 정체성의 사용자 지정 아바타를 생성하는 것은 여전히 어려운 문제로 남아 있습니다. DreamBooth3D [Raj et al. 2023]는 개인화된 3D 자산을 생성하기 위한 솔루션을 제공했지만 이미지에 표시된 정확한 정체성으로 고품질 인간 얼굴을 재현하는 데 어려움을 겪습니다. 아바타의 자세한 모양 합성과 다중 모달 기반 사용자 지정을 모두 지원하려면 새로운 미세 조정 전략이 필요합니다. 이 논문에서는 텍스트 프롬프트 또는 이미지 세트에서 고품질의 사용자 정의 가능한 아바타를 생성하기 위한 AvatarBooth라는 새로운 방법을 제안합니다. 이 방법은 특정 개인의 시각적 및 텍스트적 특징을 정확하게 반영하는 신원 맞춤형 3D 아바타를 생성하는 것을 목표로 합니다. 이를 위해 신경 암묵적 표면[Wang et al. 2021]을 학습하여 얼굴과 몸에 대해 각각 이중 사전 학습 또는 미세 조정된 잠재 확산 모델로 감독되는 인간 아바타의 모양과 모습을 나타냅니다. 한편, 포즈 일관성 제약 조건을 도입하여 외관 사용자 정의 생성 작업에서 확산 모델의 미세 조정을 향상시켜 표준 포즈 공간에서 일관된 모습으로 보다 정확한 다중 뷰 감독을 제공합니다. 또한 다중 해상도 SDS 방식을 도입하여 아바타의 미세 구조와 모습을 거칠게-미세하게 예측합니다. 사람의 사진 몇 장을 활용함으로써, 이 모델은 개인의 고유한 외모를 가질 뿐만 아니라 입력 텍스트 프롬프트에 지정된 추상적 특징과 일치하는 3D 아바타를 합성할 수 있습니다. 이러한 추상적 특징에는 &#39;특정 스타일의 안경이나 모자 착용&#39;과 같은 속성이 포함되며, 이는 아바타의 전반적인 시각적 정체성을 편집하고 수정하는 데 사용자 친화적입니다. 저희의 접근 방식은 대규모 언어 시각 모델과 구체적인 입력 이미지 모두에서 사전 확률을 활용하도록 설계되어, 입력 외모에 충실하면서도 텍스트 프롬프트를 통해 편집 가능한 아바타를 만들어냅니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. • 텍스트 프롬프트와 임의의 이미지를 모두 입력으로 지원하는 3D 인간 아바타 생성 프레임워크를 제안합니다. 이중 잠재 확산 모델을 도입하여 얼굴과 신체 생성을 별도로 감독하여 자세한 얼굴 외모, 옷, 착용감을 제공합니다. • 포즈 일관성 제약 조건을 도입하여 특정 사람의 사진이 주어지면 대규모 사전 학습된 확산 모델을 사용자 지정합니다. 우리는 ControlNet[Zhang 및 Agrawala 2023]을 사용하여 합성 이미지의 다중 뷰 일관성을 향상시켜 제어되지 않는 인간 포즈의 간섭을 제거하고 고품질 모양과 오메트리를 도출합니다.ge• 우리는 3D 아바타 생성을 조대에서 미세한 방식으로 감독하는 다중 해상도 점수 증류 샘플링 전략을 제시합니다.실험 결과 이 전략은 렌더링 품질을 향상시킬 뿐만 아니라 생성의 견고성도 개선합니다.관련 연구 텍스트 가이드 2D&amp;3D 생성.최근 몇 년 동안 확산 모델[Dhariwal 및 Nichol 2021; Ho et al. 2020; Song et al. 2020]은 고품질 이미지 합성에서 놀라운 성능을 보여 빠르게 개발되었습니다.확산 모델의 핵심 구조는 스케줄러에 따라 노이즈를 추가하는 전방 확산 단계와 노이즈를 제거하는 후방 생성 단계로 구성됩니다. 영어: 가우시안 노이즈에서만 무조건 생성하는 것 외에도 확산 모델은 텍스트 프롬프트 또는 이미지를 입력으로 사용하여 고품질 이미지를 생성할 수 있습니다.다양한 확산 모델 중에서 잠재 확산 모델[Rombach et al. 2022]은 이미지 품질과 메모리 사용 간에 좋은 균형을 이루는 유망한 텍스트-이미지 모델로 떠올랐습니다.3D 콘텐츠 생성과 관련하여 기존 방법[Chen et al. 2023; Lin et al. 2022; Poole et al. 2023]은 사전 학습된 텍스트-이미지 확산 모델을 활용하여 점수 증류 손실(SDS)[Poole et al. 2023]이 있는 좌표 기반 네트워크를 감독합니다.다른 방법[Wang et al. 2023; Wu et al. 2023; Zhang et al. 2023]은 3D 매개 변수 얼굴 모델[Yang et al. 2020; Zhu et al. 2021a], 대규모 사전 훈련된 언어-비전 모델[Radford et al. 2021a; Rombach et al. 2022] 또는 대규모 합성 데이터[Wood et al. 2021]를 사용하여 충실도가 높은 3D 인간 얼굴의 텍스트 가이드 생성을 달성합니다. 또한 일부 접근 방식[Metzer et al. 2022; Seo et al. 2023]은 안정 확산의 잠재 공간을 표현하기 위해 신경 광도장을 채택하여 텍스트 설명에서 새로운 뷰를 합성할 수 있습니다. 가이드에 SDS를 사용하는 것을 제외하고 다른 방법[Hong et al. 2023a; Wang et al. 2022]도 확산 모델의 기울기를 고려하는 텍스트가 있는 3D 자산을 생성하기 위해 점수 야코비안 체이닝을 사용합니다. 이러한 방법을 통해 뷰 일관성이 있는 3D 모델을 성공적으로 생성했지만, 관절형 3D 모양과 외관 다양성과 관련된 복잡성으로 인해 3D 아바타를 생성하는 것은 여전히 어려운 작업입니다.확산 모델의 미세 조정.최근 몇 년 동안 텍스트-이미지 도메인에 대한 관심이 커지면서 개척자 연구자들은 특정 피사체의 사진을 사용하여 텍스트-이미지 모델을 개인화하는 방법을 모색했습니다.대표적인 작업 중 하나는 DreamBooth[Ruiz et al. 2022]로, 사전 보존 손실로 인한 과적합을 방지하면서 특정 피사체나 스타일을 나타내는 희귀 토큰을 활용합니다.다른 전략에서 시작하여 텍스트 역전[Gal et al. 2022]은 입력 개념에 대한 새로운 임베딩을 만들고 몇 장의 사진으로 이 임베딩 벡터를 최적화하여 피사체 중심 이미지 생성을 달성합니다.LoRA[Hu et al. 2022]에서는 사전 훈련된 모델 가중치를 동결하고 동시에 학습 가능한 순위 분해 행렬을 Transformer 네트워크 계층에 주입하는 대규모 언어 모델을 미세 조정하는 것을 제안합니다. [Vaswani et al. 2017]. LORA는 다음을 통해 확산 모델 미세 조정의 용이성을 크게 개선합니다. [V]의 [W] 옷을 입은 고품질 3D 아바타를 생성하려고 합니다. 아바타 모델링(신경 암묵적 표면) 다중 유형 렌더링(텍스처 없는 일반/색상) Phát) Pb(t) 포즈 일관성 제약 ColorNet SDFNet 볼륨 렌더링 사전 학습된 잠재 확산 Enc 확산 프로세스 Dec DreamBooth DreamBooth 머리 몸 머리 미세 조정된 잠재 확산 확산 프로세스 (b) 학습 감독 확산 모델(다른 모드로 선택) MS-SDS 머리 미세 조정 또는 사전 학습된 잠재 확산 잠재 확산: ...... 모드 II 모드 III 모드 I MS-SDS 몸 미세 조정된 잠재 확산 또는 사전 학습된 잠재 확산 사전 학습된 잠재 확산 확산 프로세스 몸 미세 조정된 잠재 확산 DreamBooth 그림 2: 전체 파이프라인. 우리 방법은 SMPL 모양으로 초기화된 NeuS를 가진 인간 아바타를 나타냅니다. 볼륨 렌더러를 통해 아바타 모델은 텍스처리스, 컬러 및 일반 렌더링으로 변환되어 확산 모델의 감독 하에 SDS 훈련에 사용됩니다. 사전 훈련되고 미세 조정된 확산 모델을 선택하거나 조합하여 채택함으로써, 우리의 접근 방식은 세 가지 모드로 수행될 수 있습니다: (I) 프롬프트 생성 모드; (II) 외관 사용자 지정 모드; 및 (III) 하이브리드 모드. 얼굴과 전신에 대한 미세 조정 전략은 각각 (a)와 (b)에 나와 있습니다. 다운스트림 작업에 대한 훈련 가능한 매개변수 수를 줄입니다. DreamBooth3D [Raj et al. 2023]는 DreamBooth를 사용하여 미세 조정된 개인화된 확산 모델을 사용하여 3D 모델을 생성하는 3단계 반복적 접근 방식을 제안합니다. 그러나 DreamBooth3D는 정체성이 일관되고 자세한 인간 아바타를 복구하지 못하는 반면, 생성된 개인화된 콘텐츠는 텍스트 프롬프트를 기반으로 편집할 수 없습니다. 따라서 이러한 한계를 해결하고 이미지 세트와 텍스트 프롬프트를 기반으로 한 아바타 생성의 유연성을 향상시키는 방법을 모색합니다. generatAvatar 생성 모델. 3D 아바타를 생성하는 기존 방법은 종종 3D 데이터 세트에 대한 훈련에 의존하는데, 이는 수집 및 확장이 어려울 수 있습니다. 이러한 과제를 극복하기 위해 최근 방법에서는 EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARFGAN과 같은 저렴한 2D 데이터를 활용하여 신경망을 훈련합니다. 또한 일부 명시적 방법 [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b]와 암묵적 방법[He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021]이 단일 입력 이미지에 따라 인간 아바타를 생성하도록 개발되었습니다. 이러한 방법은 훈련 중에 나타나지 않는 보이지 않는 스타일, 착용 및 외모를 가진 아바타를 생성하는 데 한계가 있으며, 단어 설명을 통해 특정 정체성을 가진 아바타를 개인화하는 것은 말할 것도 없습니다. AvatarCLIP[Hong et al. 2022]은 제로 샷 텍스트 기반 방식으로 3D 아바타를 생성하고 애니메이션화한 최초의 방법인 반면, CLIP-Actor는 메시 변형 및 정점 채색을 위해 CLIP에서 변위 맵을 학습합니다. 최근 AvatarCraft [Jiang et al. 2023b] 및 DreamAvatar [Cao et al. 2023]와 같은 후속 연구에서는 확산 모델을 활용하여 고품질 3D 아바타를 제작했습니다. 그러나 AvatarCraft는 원래 템플릿 모델에 큰 변형을 생성하는 데 제한이 있으며, 그 기하학은 가슴과 등과 같은 곳에 일부 아티팩트가 있습니다. DreamAvatar는 고품질의 의복 기하학과 텍스처를 생성하지만 LatentNeRF의 고유한 특징으로 인해 세밀한 메시를 추출할 수 없어 학습 프로세스가 완료된 후 관절이 없는 결과가 발생합니다. 저희의 방법은 이러한 문제를 제거하고 자유 시야 이미지에서 사용자 지정 아바타를 합성하여 텍스트 프롬프트를 통해 편집할 수 있습니다. 방법 3.1 예비 신경 암묵적 표면(NeuS) [Wang et al. 2021]. NeuS는 3D 표면을 부호 거리 함수(SDF)의 영수준 집합으로 표현하는 신경 암묵적 표현입니다[Park et al. 2019]. 좌표(x, y, z)와 시야 위치/방향(o, d)이 주어지면 두 개의 MLP를 사용하여 각각 SDF 값과 RGB 값을 예측합니다. 그런 다음 볼륨 렌더링 방정식을 사용하여 픽셀 색상을 계산할 수 있습니다. n C(o, d) = (w(t)c(p(t), d)) PER (1) 여기서 p(t)는 샘플링된 지점이고 R은 광선 o+td를 따라 샘플링된 n개의 지점을 포함하고 w(t)는 다음과 같이 공식화됩니다. w(t) = s(f(p(t))) Jobs (f(p(u)))du (2) 여기서 os는 로지스틱 밀도 분포이고 ƒ는 SDF 네트워크입니다. NeRF와 비교했을 때[Mildenhall et al. 2020] 및 그 분산, NeuS의 공식화는 근사의 1차 편향을 제거하여 보다 자세한 표면 재구성으로 이어집니다.점수 증류 샘플링(SDS) [Poole et al. 2023].SDS는 손실 함수를 최적화하여 확산 모델에서 피험자를 생성하는 전략으로, 3D 필드로 표현된 아바타를 최적화하는 데 사용할 수 있습니다.특히, 이 점수 함수의 그래디언트는 렌더링된 이미지의 더 높은 밀도 영역을 나타냅니다.자세한 공식은 섹션 3.2에 소개되어 있습니다.SDS 전략에 따라 개인화된 생성을 달성하기 위해 DreamBooth3D는 DreamBooth가 미세 조정한 확산 모델로 피험자의 외모 학습을 감독하도록 제안합니다 [Ruiz et al. 2022].우리의 방법에서는 미세 조정된 확산 모델과 협력한 SDS 전략을 따른 다음 사용자 정의 기능과 합성된 외모의 정확성을 더욱 개선하기 위해 개선했습니다. 3.2 파이프라인 개요 본 방법은 일련의 이미지 또는 텍스트 프롬프트를 입력으로 사용하여 NeuS로 표현되는 3D 세부 아바타를 합성합니다.그림 2에서 볼 수 있듯이 전체 생성 파이프라인은 세 개의 모듈로 구성됩니다.아바타 모델링 모듈에서 SMPL 모델의 베어 렌더링[Loper et al. 2015; Pavlakos et al. 2019]은 이전 연구[Hong et al. 2022; Jiang et al. 2023b]에 따라 SDF 네트워크 f(x; 0)와 색상 네트워크 c(x; 0)로 구성된 신경 암시적 필드[Wang et al. 2021]로 학습됩니다.렌더링 모듈에서 아바타 공간 주변에 위치한 미리 정의된 가상 카메라에서 세 가지 유형의 렌더링을 얻습니다.경험적으로 색상 및 텍스처 없는 렌더링 {Ic, Ig} 외에도 법선 맵을 렌더링했으며, 실험을 통해 법선 맵을 도입하면 얼굴 윤곽 및 옷 주름과 같은 기하학적 세부 정보가 향상됨이 입증되었습니다. 그런 다음 SDS Loss를 활용하여 NeuS가 수렴하도록 안내합니다.이는 다음과 같이 공식화할 수 있습니다.Vy LSDS (0, 1) = B | w(t) (ê (z/; y, t) − e) І дл (3) 여기서 는 확산 모델의 매개변수를 나타내고, I는 {Ig, In, Ic}를 포함하는 감독에 사용된 이미지이며, z¹은 이미지 I의 해당 잠재 코드입니다.함수 €()는 확산 모델에서 예측한 노이즈를 나타내고, y와 t는 각각 입력 프롬프트와 타임스텝을 나타냅니다.얼굴과 인체를 동시에 최적화하기 위해 얼굴과 전체 인체를 중심으로 하는 두 세트의 렌더링 매개변수를 채택했습니다.이에 대해서는 섹션 3.3에서 자세히 설명합니다.SDS 학습 모듈에서 사전 학습되고 미세 조정된 잠재 확산 모델이 선택되거나 결합되어 렌더링을 통해 Neus의 학습을 감독합니다. SDS의 다중 해상도 훈련은 아바타를 조대-미세 방식으로 모델링하기 위해 구현되며, 이는 3.5절에서 자세히 설명합니다. 잠재 확산 모델의 미세 조정에서 포즈 일관성 제약을 도입할 것을 제안하며, 이는 3.4절에서 자세히 설명합니다. 사전 훈련된 확산 모델이 SDS 훈련에 사용되는 방식에 따라, 프레임워크는 다음 세 가지 모드로 작동할 수 있습니다. (I) 프롬프트 생성 모드. AvatarCLIP [Hong et al. 2022] 및 AvatarCraft [Jiang et al. 2023b]와 유사하게, 사전 훈련된 확산 모델을 미세 조정하지 않고 설명에 맞는 아바타를 생성하기 위해 텍스트 프롬프트만 입력으로 사용합니다. 텍스트 프롬프트는 일반적이거나 잘 알려진 모습만 설명할 수 있으므로, 이 모드는 대략적으로 일치하는 모습 또는 유명인을 가진 아바타를 합성하는 데만 작동합니다. (II) 모습 사용자 지정 모드. 우리는 이미지 세트가 주어진 외모와 일치하도록 확산 모델과 학습된 아바타를 사용자 정의하는 것을 제안합니다. 이러한 이미지는 모든 관점에서 자유롭게 촬영한 전신 또는 얼굴 이미지일 수 있습니다. 입력 사진에 불완전하거나 약간 모순되는 외모만 포함되어 있더라도 외모와 옷의 세부 정보는 아바타 모델을 생성하는 데 전달됩니다. (III) 하이브리드 모드. 위의 두 모드는 단일 모델 생성, 즉 하이브리드 모드에서 동시에 수행할 수 있습니다. 이 모드는 입력 이미지에 따라 외모를 합성한다는 전제 하에 텍스트 프롬프트를 통해 피사체의 옷, 헤어스타일, 나이, 수염 등을 수정하는 것과 같은 보다 복잡한 아바타의 조건부 생성을 달성할 수 있습니다. 3.3 이중 모델 미세 조정 우리는 전신과 머리의 훈련을 감독하기 위해 두 개의 확산 모델을 활용하는 것을 제안하며, 두 모델도 각각 미세 조정됩니다. 이전 연구 [Hong et al. 2022; Jiang et al. 2023b] 얼굴 주변의 렌더링 샘플을 증강하여 얼굴 세부 사항을 개선하지만 미세 조정된 시각-언어 모델의 잠재력을 활용하지 못하므로 개인화된 아바타 생성의 성능을 향상시킬 수 없습니다. 처음에는 전체 신체의 훈련을 감독하기 위해 하나의 확산 모델만 사용합니다. DreamBooth3D의 미세 조정 전략을 사용한 단일 SDS 손실은 얼굴 모양과 신체 옷의 모델링 간의 균형을 맞추지 못하는 것을 관찰했습니다. 구체적으로 초기 훈련 단계에서 신체의 옷 모양은 학습되지만 얼굴 모양은 여전히 불분명합니다. 더 많은 훈련 단계를 거치면 얼굴 모양은 명확해지지만 옷 스타일과 같은 글로벌 피처가 입력 이미지에 과적합되어 하이브리드 모드에서 텍스트 프롬프트를 통해 신체를 편집하기 어려울 수 있습니다. 게다가 DreamBooth3D의 Img2Img 단계는 입력 이미지에 충실한 정확한 캐릭터 정체성을 생성할 수 없다는 것도 관찰했습니다. 우리는 이것이 얼굴 모양과 신체 모양 사이의 척도 차이가 크기 때문이라고 생각하는데, 이로 인해 SDS 훈련에서 수렴 속도가 일관되지 않습니다.500단계 DreamBooth 사전 훈련된 잠재 확산 900단계 DreamBooth 포즈 일관성 제약 다중 뷰 얼굴 이미지 생성 및 £헤드 미세 조정 잠재 확산 그림 3: 포즈 일관성 제약.먼저 많은 단계에 걸쳐 입력 이미지 Ireal을 사용하여 초기 DreamBooth 모델 Dinit을 훈련합니다.Openpose에서 렌더링된 포즈 제약에 따라 입력 이미지의 사람과 동일한 정체성을 공유하는 다중 뷰 얼굴 이미지 Imo를 생성할 수 있습니다.그런 다음 다중 뷰 이미지 Imu와 입력 개인 이미지 Ireal을 결합하여 더 많은 단계에 걸쳐 최종 DreamBooth D 최종 이미지를 미세 조정합니다.이 문제를 해결하기 위해 이중 모델 미세 조정 전략을 제안합니다. 외관 사용자 지정 모드 또는 하이브리드 모드에서 실행할 때 입력 이미지는 전신 샷과 헤드샷으로 나뉘며, 이는 각각 두 개의 사전 훈련된 모델을 미세 조정하는 데 사용됩니다.SDS 훈련 단계에서 얼굴과 전신 주변의 카메라를 무작위로 샘플링한 다음, 각각 머리 지향 렌더링과 전신 렌더링을 사용하여 얼굴과 몸을 생성하는 데 다양한 확산 모델을 사용합니다.헤드 샷 모델의 미세 조정에서 3.4절에서 자세히 설명할 포즈 일관성 제약 조건도 도입합니다.3.포즈 일관성 제약 조건 미세 조정된 확산 모델에서 생성된 아바타의 얼굴 세부 정보를 향상시키기 위해 ControlNet[Zhang 및 Agrawala 2023]을 도입하여 포즈 일관성 미세 조정 방법을 제안합니다.이전 접근 방식[Raj et al. 2023]은 SDS 기반 방법으로 DreamBooth를 직접 사용하면 DreamBooth 모델이 미세 조정 중에 사용된 카메라 뷰에 과적합되는 경향이 있기 때문에 만족스럽지 못한 결과가 발생한다는 것을 증명했습니다. 이 작업에서 우리는 훈련 과정 전에 더 많은 얼굴을 통합하기 위해 ControlNet을 활용하는 2단계 전략을 제안합니다. 구체적으로, 우리는 먼저 입력 이미지 Ireal을 사용하여 초기 DreamBooth 모델 Dinit을 훈련합니다. 그런 다음 키포인트 ControlNet을 사용하여 OpenPose [Cao et al. 2021, 2017]에서 생성한 스켈레톤 제약 조건에 따라 안내되는 멀티뷰 얼굴 이미지 Imo를 생성하여 서라운드 뷰에서 렌더링합니다. 이러한 합성 이미지 Imo는 실제 이미지 Ireal과 결합되어 DreamBooth 방법으로 새로운 확산 모델 Dfinal을 더욱 미세 조정하여 3D 모델의 얼굴 세부 정보를 보강합니다. NeRF의 훈련 과정에서 이 문제를 해결하려는 이전 방법[Raj et al. 2023]과 달리, 우리의 접근 방식은 신경 표면 필드를 훈련하기 전에 ControlNet을 활용하여 이 문제를 해결합니다. 결과적으로 우리는 처음부터 다시 훈련하지 않고도 동일한 DreamBooth 모델 ◇ final을 사용하여 동일한 신원을 가진 다른 아바타를 생성할 수 있습니다. 실험 결과, DreamBooth 모델과 함께 ControlNet을 사용하여 다중 뷰 얼굴 이미지 생성을 안내하면 더 정확하고 사실적인 아바타를 얻을 수 있음이 나타났습니다.3. 다중 해상도 SDS 신경 암시적 필드에서 고해상도 이미지를 직접 렌더링하는 것은 계산적으로 매우 비용이 많이 들기 때문에 일반적인 솔루션은 저해상도 이미지를 렌더링한 다음 SDS 학습을 위해 더 높은 해상도로 업샘플링하는 것입니다[Chen et al. 2023; Lin et al. 2022]. 업샘플링된 이미지는 잠재 공간에 인코딩되어 신경 암시적 필드의 학습을 감독하는 데 사용됩니다.그러나 업샘플링된 해상도를 직접 높이면 학습이 붕괴되거나 모양이 일관되지 않을 수 있음을 관찰했습니다.이러한 문제를 해결하기 위해 점진적으로 업샘플링 해상도를 개선하여 보다 안정적인 SDS 학습을 위한 다중 해상도 최적화 전략을 제안합니다. NeuS에서 렌더링된 H x W 이미지 {Ig, In, Ic}에서 시작하여 몇 단계 동안 512 x 512의 업샘플링된 해상도로 학습하여 네트워크를 초기화한 다음 점차적으로 감독 해상도를 640 x 640 및 768 x 768로 개선합니다. 초기 학습 단계에서 낮은 해상도는 학습 프로세스에 대한 거칠지만 강력한 시작점을 제공하는 반면, 후반 단계에서 높은 해상도는 자세한 기하학과 고품질 모양을 학습하는 데 도움이 됩니다. 실험을 통해 이 간단한 전략이 초기 학습 단계의 안정성을 효율적으로 개선하고 모양 품질을 증가시켜 궁극적으로 더 정확하고 시각적으로 그럴듯한 아바타를 생성한다는 것을 보여줍니다.실험 이 섹션에서는 실험을 통해 제안된 방법의 효과를 검증하고 이전 방법과 비교하고 논의합니다.4.1 구현 세부 정보 신경 암시적 표면을 모델링하기 위해 SDF 네트워크에 6층 MLP를 사용하고 색상 네트워크에 4층 MLP를 사용합니다. 아바타를 생성하기 위해 512×512의 보간 해상도에서 2000단계, 640×640의 보간 해상도에서 2000단계, 768×768의 보간 해상도에서 4000단계를 포함하여 총 8000단계 동안 네트워크를 훈련합니다. 확산 모델 미세 조정 단계에서는 첫 번째 DreamBooth 모델을 900번 반복하여 훈련하여 다중 뷰 이미지를 생성한 다음 생성된 이미지와 개인 이미지를 결합하여 두 번째 DreamBooth 모델을 500단계 동안 훈련합니다. 렌더링을 위해 가상 카메라 위치를 무작위로 샘플링하는데, 여기에는 얼굴을 중심으로 25%, 전신을 중심으로 75%가 포함됩니다. 법선 맵, 그림자 이미지, 색상 이미지는 1:1:8의 비율로 무작위로 렌더링됩니다. 이러한 하이퍼 매개변수는 세 가지 모드에서 모두 동일합니다. Adam 최적화 도구[Kingma 및 Ba 2015]를 사용하여 모델을 학습시키고 학습률은 0.005로 설정했습니다. NVIDIA RTX3090 GPU에서 아바타 모델을 합성하는 데 약 90분이 걸리고 모양에 맞는 생성이 필요한 경우 확산 모델의 미세 조정을 완료하는 데 30분이 더 걸립니다. 4.2 정성적 평가 프롬프트 생성 모드의 결과는 그림 4에 나와 있습니다. 이 방법은 Hilary Clinton Bill Gates Abraham Lincoln TAKERSKobe Cristiano Ronaldo Doctor Superman Kratos Aragon Deadpool Stormtrooper Luffy와 같은 그럴듯한 인간 아바타를 합성하는 것을 볼 수 있습니다. 그림 4: 프롬프트 생성 모드의 결과. 이 방법은 미세한 기하학적 모양과 질감을 복구하고 그 결과 인간 아바타는 텍스트 프롬프트와 매우 일치합니다. Frozen의 Elsa Luke Skywalker CLIP-흰색 정장을 입은 배우 Obama AvatarCLIP TEXTure Ours 그림 5: 정성적 비교-I. 모델을 컬러 이미지와 노멀 맵으로 렌더링하여 우리와 이전 작업에서 생성된 아바타를 시각화합니다. 우리 방법으로 생성된 모델은 옷 라인과 얼굴 특징과 같은 더 나은 기하학적 세부 정보를 포함하고 더 나은 렌더링 품질을 갖습니다. 자세한 기하학 및 미세한 외관은 입력 텍스트 프롬프트와 밀접하게 일치합니다. 외관 사용자 지정 모드와 하이브리드 모드의 결과는 그림 10에 나와 있습니다. 이러한 실험에서 이미지가 자유로운 조건에서 캡처된 경우에도 입력 이미지 세트의 인간 외관이 생성된 아바타로 전송되는 것을 볼 수 있습니다. 사용자 지정된 아바타는 추가 텍스트 프롬프트에 따라 추가로 수정할 수 있습니다. 예를 들어, &quot;[V] 노란색 머리 남자&quot;와 같은 간단한 프롬프트를 사용하면 결과 아바타의 머리에 노란색 머리가 그에 따라 나타납니다. 게다가, &quot;마법사처럼 옷을 입은 [V]&quot;와 &quot;50대의 [V]&quot;와 같은 훨씬 더 추상적인 프롬프트도 효과적임을 보였으며, 하이브리드 모드에서 작업하는 경우 입력 이미지 세트의 사용자 지정 모양이 유지됩니다. 텍스트 프롬프트를 사용하여 다양한 모양과 스타일의 다양한 아바타를 제작하여 사용자에게 원하는 아바타를 만드는 효율적이고 개인화된 방법을 제공할 수 있었습니다. 결과 모델은 애니메이션을 위해 쉽게 조작할 수 있습니다. 더 많은 결과와 주변 뷰 렌더링을 보려면 비디오를 시청하는 것이 좋습니다. 또한 결과를 이전 작업과 정성적으로 비교합니다. 이전 접근 방식이 이미지 세트를 기반으로 한 사용자 지정 아바타 생성을 지원하지 않는다는 점을 고려하여 그림 5와 같이 프롬프트 생성 모드에서의 방법과 이전 텍스트-아바타 방법만 비교합니다. AvatarCraft에 대한 공식 구현이 없으므로 [Jiang et al. 2023b] 및 DreamAvatar [Cao et al. 2023]의 경우 그림 6에서와 같이 실험에서 동일한 설정으로 성능을 비교합니다. 저희 모델은 기하학과 D AvatarCraft DreamAvatar Ours 그림 6: 정성적 비교-III. 저자가 제공한 렌더링 결과를 사용하여 저희 방법을 AvatarCraft 및 DreamAvatar와 비교합니다. 특히 저희 방법은 캐릭터의 정체성을 보존하면서 고품질의 기하학과 텍스처를 생성할 수 있습니다. 저희 방법을 사용하면 CLIP-Actor [Youwang et al. 2022], AvatarCLIP [Hong et al. 2022], TEXTure [Richardson et al. 2023], AvatarCraft [Jiang et al. 2023b]에서는 달성할 수 없는 느슨한 의복과 액세서리가 있는 아바타를 생성할 수 있다는 점에 주목할 가치가 있습니다. 우리는 이것이 이전 방법들이 SMPL의 제약에 의존하기 때문이라고 믿습니다 [Loper et al. 2015; Pavlakos et al. 2019] 텍스트와의 대응 모양 품질 기하 품질 ■얼굴 충실도 5.4.3.Fintuned Images Ours에서 생성한 모델 DreamBooth3D에서 생성한 모델 2.그림 7: 정성적 비교-II. 우리는 모양-사용자 지정 모드와 하이브리드 모드에서 DreamBooth3D와 우리 방법을 비교합니다.1.0.CLIP-Actor AvatarCLIP TEXTure Ours는 아바타 생성을 유지하는 반면, 우리 방법은 시각-언어 사전을 효율적으로 활용하고 SMPL 모양에서 큰 제약 없이 보다 일반화된 아바타 모델을 생성할 수 있습니다.DreamAvatar [Cao et al. 2023]도 SMPL 제약이 없지만 결과 모델의 모양과 기하 구조는 우리 결과보다 덜 자세합니다. 우리는 위의 이미지로 DreamBooth3D의 전략에 따라 NeuS를 훈련시켰고, DreamBooth3D가 입력 이미지와 일치하는 3D 자산을 생성하기 위해 Neural Surface Field를 감독할 수 없다는 것을 발견했습니다. 이는 DreamBooth3D의 Img21mg 단계에서 생성된 얼굴 세부 정보의 부정확성 때문일 수 있습니다. 프롬프트를 변경하기만 하면 DreamBooth3D 결과에서 캐릭터가 서로 다른 정체성을 공유하게 됩니다. DreamBooth3D와 비교해 보면, 우리의 방법은 미세 조정된 이미지에 더 충실한 결과를 생성합니다. 게다가, 우리의 결과는 모든 종류의 텍스트 설명에서 동일한 정체성을 유지합니다. 4.3 양적 평가 사용자 연구. 합성된 아바타의 품질을 정량화하기 위해, 우리는 사용자 연구를 수행하여 우리의 결과를 다른 최첨단 방법, 즉 CLIP-Actor, AvatarCLIP 및 TEXTure에서 생성된 결과와 비교합니다. 우리는 각 방법에 대해 무작위로 선택된 텍스트 프롬프트에서 10개의 아바타를 생성하고 30명의 자원봉사자를 모집하여 텍스트 프롬프트와의 대응, 외관 품질, 기하학적 품질, 얼굴 충실도의 4가지 다른 측면에 대한 결과를 평가합니다.자원봉사자는 각 용어에 대해 1(최악)에서 5(최고)까지 점수를 매겨야 합니다.결과는 그림 8에 나와 있습니다.우리의 방법은 4가지 측면 모두에서 가장 높은 점수를 얻었으며, 이는 우리가 더 자세한 외관과 기하학적 구조를 가진 아바타를 생성할 수 있음을 증명합니다.텍스트 대 이미지 메트릭.우리의 지식에 따르면 텍스트 대 3D 생성 모델을 직접적이고 정량적으로 평가할 수 있는 메트릭은 없으므로 생성된 아바타 모델을 이미지로 렌더링한 다음 텍스트 대 이미지 메트릭을 사용하여 평가합니다.특히, 우리 방법과 이전 작업으로 생성된 모델은 먼저 25가지 다른 관점에서 2000개의 이미지로 렌더링한 다음 아바타 품질을 PickScore [Kirstain et al. 2023]은 학습된 인간의 선호도에 따라 생성된 콘텐츠의 충실도를 측정하는 텍스트-이미지 메트릭입니다.그림 9에 보고된 바와 같이, PickScores는 우리의 방법이 CLIP-Actor, AvatarCLIP 및 TEXTure보다 큰 차이로 성능이 뛰어나다는 것을 보여주며, 이는 우리의 결과가 더 나은 주관적 품질을 가지고 있음을 나타냅니다.그림 8: 사용자 연구.우리는 텍스트 프롬프트, 모양 품질, 기하학적 품질 및 얼굴 충실도와의 대응과 관련하여 우리의 방법과 이전 작업에 대한 사용자의 평가를 조사했습니다.우리의 방법은 네 가지 메트릭 모두에서 최적의 평가를 달성합니다.우리의 74% 18% 8% CLIP-Actor 우리의 76% 우리의 90% 승리 동점 18% 7% AvatarCLIP 8%2% TEXTure 패배 그림 9: PickScore에 의한 평가.PickScore의 결과는 우리의 방법(프롬프트 생성 모드)이 시각적 품질에서 CLIP-Actor, AvatarCLIP 및 TEXTure보다 성능이 뛰어나다는 것을 보여줍니다. 4. 절제 연구 제안된 방법의 각 부분의 효과를 검증하기 위해 전체 파이프라인에서 특정 모듈을 제거하는 절제 실험을 수행한 다음 각 설정에 대한 성능을 비교합니다.각 설정에 대한 설명은 아래와 같습니다.A. 전체 파이프라인.B. 법선 맵 없음(3.2절): 훈련에서 법선 맵이 렌더링되지 않습니다.C. 다중 해상도 SDS 없음(3.4절): 전체 파이프라인의 다중 해상도 SDS가 고정된 고해상도 SDS 전략으로 대체됩니다.D. 얼굴 감독 없음(3.3절): 전신을 타겟팅하는 SDS 훈련에서 확산 모델을 하나만 사용합니다.E. 포즈 일관성 제약 없음(3.4절): 포즈 일관성 제약이 전체 파이프라인에서 제거됩니다. 그림 11에서 A와 B를 비교하면, 설정 B에서 법선 맵 감독 가이드를 제거하면 생성된 아바타에서 기하학적 세부 정보가 상당히 손실되는 반면, 설정 A에서 법선 맵 감독을 사용한 완전한 방법은 얼굴 특징과 옷 주름을 포함한 세부 정보가 향상된 고품질 아바타를 생성합니다.이러한 결과는 다음의 효과성을 뒷받침합니다.사람 이미지 [X] 옷 이미지 [Y] 사람 이미지 [U] 엘사가 입은 경찰 정장을 입은 이쿠터 보카테스 [X], 픽사 스타일 [U] 캡틴 마블 A처럼 차려입은 이미지.전체 파이프라인 B. 법선 맵 없음 [X] 노란색 머리와 노란색 정장을 입은 테일러 스위프트가 노란색 [Y] 옷 이미지 [V] C. 다중 해상도 없음 C. 다중 해상도 없음 D. 이중 모델 없음 SDS(512x512) SDS(768x768) 미세 조정 그림 11: 절제 연구-I. AD 설정에서 생성된 아바타는 각 모듈의 효과를 검증하기 위해 표시됩니다.50대의 [X]가 착용한 [U] [Y] 착용한 오바마 [V] 그림 10: 외모 사용자 지정/하이브리드 모드의 결과.이미지가 자유로운 조건에서 촬영된 경우에도 입력 이미지 세트의 인간 외모가 생성된 아바타로 전송되는 것을 볼 수 있습니다.사용자 지정 아바타는 추가 텍스트 프롬프트에 따라 추가로 수정할 수 있습니다.텍스트 설명에서 사실적인 지오메트리를 생성하는 제안된 일반 맵 안내 전략.그림 11의 A와 C를 비교하면 고정된 고해상도 업샘플링 설정으로 생성된 아바타에는 불분명한 텍스처, 잘못된 텍스처 또는 불합리한 기하학적 세부 정보가 포함되어 있지만 이러한 문제는 다중 해상도 SDS를 구현한 후에는 제거됩니다.이 이유는 다중 해상도 SDS 방식이 거친 것에서 미세한 것으로 다중 스케일 정보를 학습하여 보다 자세하고 명확한 3D 아바타를 생성하는 데 도움이 되기 때문이라고 생각합니다. 그림 11에서 A와 D를 비교하면, 단일 모델로 감독한 생성된 얼굴에는 머리에 명확한 인공물이 있고 얼굴의 기하학과 모양이 덜 자세하다는 것을 알 수 있습니다. 게다가, 머리에 대한 SDS 손실을 제거하면 머리와 몸의 비율이 불균형해집니다. 반면에, 듀얼 모델로 감독한 아바타 얼굴에는 선명한 질감과 자세한 기하학이 포함됩니다. A. 완전한 파이프라인 E. 포즈 일관성 제약 없음 그림 12: 절제 연구-II. A와 E 설정에서 생성된 아바타는 포즈 일관성 제약의 효과를 확인하기 위해 표시됩니다. 그림 12에서 A와 E를 비교하면 포즈 일관성 제약을 제거하면 아바타 얼굴의 표면이 평평하거나 오목한 표면으로 수렴하는 경향이 있음을 알 수 있습니다. 포즈 일관성 제약이 없는 확산 모델로 합성한 머리 포즈가 충돌하기 때문에 다중 뷰 광도 감독에서 잘못된 기하학 회귀가 발생하기 때문이라고 생각합니다. 이와 대조적으로, 포즈 일관성 제약을 도입하면 더욱 그럴듯한 기하학적 모양을 갖춘 3D 아바타가 탄생합니다.
--- CONCLUSION ---
이 논문에서는 텍스트 프롬프트 또는 자유 캡처 이미지 세트 또는 둘 다를 기반으로 아바타 모델을 생성하는 방법을 제안합니다. 합성할 인간 아바타는 신경 암묵적 표면으로 표현되고, 사전 훈련된 대규모 시각 언어 모델이 점수 증류 샘플링 손실을 통해 모델 훈련에 활용됩니다. 포즈 일관성 제약은 아바타의 기하학 및 모양의 정확도를 개선하기 위해 도입되었습니다. 이중 모델 미세 조정 및 다중 해상도 SDS는 텍스트 프롬프트 모드 또는 모양 사용자 지정 모드에서 아바타 품질과 충실도를 더욱 향상시킵니다. 접근 방식에는 여전히 몇 가지 제한이 있습니다. 생성된 모델의 정확도는 여전히 개선될 가능성이 있으며 미세 조정 및 훈련 단계의 속도는 여전히 향상되어야 합니다. 또한 훈련을 위해 3D 인간 데이터 세트를 활용하지 않습니다. 기존 3D 인간 데이터 세트의 데이터 양은 비교적 제한적이지만 이러한 사전 확률은 여전히 아바타 생성의 품질을 크게 개선할 것으로 예상됩니다. 감사의 말. 이 연구는 NSFC 보조금 62001213, 62025108, Huawei의 기부금, Tencent RhinoBird Research Program의 지원을 받았습니다.참고문헌 Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, Paul Debevec.2010. The digital emily project: Achieving a photorealistic digital actor.IEEE Computer Graphics and Applications 30, 4 (2010), 20-31. Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, Marcus Magnor.2019. Tex2shape: 단일 이미지에서 얻은 자세한 인체 전체 형상.IEEE/CVF International Conference on Computer Vision의 회의록.2293-2303. Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric Chan, David Lindell, Gordon Wetzstein. 2022. 생성적 신경 관절 광도장. 신경 정보 처리 시스템의 발전 35(2022), 19900-19916. Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K Wong. 2023. DreamAvatar: 확산 모델을 통한 텍스트 및 모양 가이드 3D 인간 아바타 생성. arXiv 사전 인쇄본 arXiv:2304.00916(2023). Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh. 2021. OpenPose: 부분 친화도 필드를 사용한 실시간 다중 사람 2D 포즈 추정. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션 43, 01(2021), 172-186. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh. 2017. 부분 친화도 필드를 사용한 실시간 다중인 2차원 포즈 추정. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 7291-7299. Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. 2022. 효율적인 기하학 인식 3D 생성적 적대 네트워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 1612316133. Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia. 2023. Fantasia3D: 고품질 텍스트-3D 콘텐츠 생성을 위한 기하학 및 외관의 얽힘 해소. arXiv 사전 인쇄본 arXiv:2303.13873(2023). Prafulla Dhariwal 및 Alexander Nichol. 2021. 확산 모델은 이미지 합성에서 gans를 이긴다. 신경 정보 처리 시스템의 발전 34(2021), 8780-8794. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik 및 Daniel Cohen-Or. 2022. 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618(2022). Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai 및 Yebin Liu. 2017. 단일 rgb-d 카메라를 사용한 실시간 기하학, 반사도 및 동작 재구성. ACM 그래픽스 트랜잭션(ToG) 36, 4(2017), 1. 한상훈, 박민규, 윤주홍, 강주미, 박영재, 전혜곤. 2023. 단일 2K 해상도 이미지에서 고충실도 3D 인간 디지털화. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 통 허, 위안루 쉬, 슌스케 사이토, 스테파노 소아토, 토니 퉁. 2021. Arch++: 애니메이션에 적합한 옷을 입은 인간 재구성 재검토. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 진행 중. 11046-11056. 조나단 호, 아자이 자인, 피터 아빌. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840-6851. Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. 2023b. Eva3d: 2D 이미지 컬렉션에서 구성적 3D 인간 생성. International Conference on Learning Representations에서. Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: 3D 아바타의 제로샷 텍스트 기반 생성 및 애니메이션. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1-19. Susung Hong, Donghoon Ahn, and Seungryong Kim. 2023a. Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. arXiv 사전 인쇄본 arXiv:2303.15413 (2023). Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. LORA: 대규모 언어 모델의 저순위 적응. 국제 학습 표현 컨퍼런스에서. Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung. 2020. Arch: 옷을 입은 사람의 애니메이션 재구성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록에서. 3093-3102. Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao. 2023b. AvatarCraft: 매개변수화된 모양 및 포즈 제어를 사용하여 텍스트를 신경 인간 아바타로 변환. arXiv 사전 인쇄본 arXiv:2303.(2023). Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, Lan Xu. 2023a. HumanGen: 명시적 사전을 사용하여 인간 광채 필드 생성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록에서. Diederik P Kingma와 Jimmy Ba. 2015. Adam: 확률적 최적화를 위한 방법. 국제 학습 표현 컨퍼런스(ICLR)의 회의록에서. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy. 2023. Pick-a-Pic: 텍스트-이미지 생성을 위한 사용자 선호도의 오픈 데이터 세트. arXiv 사전 인쇄본 arXiv:2305.01569 (2023). Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. 2022. Magic3D: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.10440 (2022). Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J. Black. 2015. SMPL: 스킨 처리된 다중 사람 선형 모델. ACM Trans. Graphics(Proc. SIGGRAPH Asia) 34, 6(2015년 10월), 248:1-248:16. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022. 3D 모양과 텍스처의 모양 기반 생성을 위한 Latent-NeRF. arXiv 사전 인쇄본 arXiv:2211.07600 (2022). B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ramamoorthi, and R Ng. 2020. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. 유럽 컴퓨터 비전 컨퍼런스에서. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: 모양 표현을 위한 연속 부호 거리 함수 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 165-174. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J. Black. 2019. 표현적인 신체 캡처: 단일 이미지에서 3D 손, 얼굴 및 신체. IEEE Conf. on Computer Vision and Pattern Recognition(CVPR) 회의록. 10975-10985. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. 신경체: 동적 인간의 새로운 뷰 합성을 위한 구조화된 잠재 코드를 사용한 암묵적 신경 표현. IEEE/CVF Conference on Computer Vision and Pattern Recognition 회의록. 9054-9063. Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2023. Dreamfusion: 2d 확산을 사용한 텍스트-3d. 국제 학습 표현 컨퍼런스(ICLR)의 회의록에서. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021a. 자연어 감독에서 전이 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 8748-8763. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 2021b. 자연어 감독에서 전이 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스(ICML) 논문집. 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. 2023. DreamBooth3D: 주제 중심 텍스트-3D 생성. arXiv 사전 인쇄본 arXiv:2303.(2023). Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023. TEXTure: 3D 모양의 텍스트 기반 텍스처링. ACM Trans. Graphics(Proc. SIGGRAPH)(2023). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 1068410695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2022. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi. 2022. 심층 언어 이해를 통한 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, Vol. 35. 36479-36494. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li. 2019. Pifu: 고해상도 옷을 입은 인간 디지털화를 위한 픽셀 정렬 암시적 함수. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스의 회의록. 2304-2314. Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo. 2020. Pifuhd: 고해상도 3D 인간 디지털화를 위한 다중 레벨 픽셀 정렬 암시적 함수. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 84-93. 서호이지, 김하연, 김광현, 전세영. 2023. DITTO-NeRF: 확산 기반 반복 텍스트에서 전방향 3D 모델로. arXiv 사전 인쇄본 arXiv:2304.02827(2023). 지아밍 송, 첸린 멩, 스테파노 에르몬. 2020. 확산 암시적 모델 잡음 제거. 국제 학습 표현 컨퍼런스에서. 페이통 탄, 하오 주, 자오펭 추이, 시위 주, 마크 폴레페이스, 핑 탄. 2020. 단안 비디오에서 자기 감독 인간 깊이 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 650-659. Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, Cordelia Schmid. 2018. Bodynet: 3D 인체 모양의 체적 추론. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록. 20-36. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전 30(2017). Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, Greg Shakhnarovich. 2022. 점수 Jacobian Chaining: 3D 생성을 위한 사전 학습된 2D 확산 모델 리프팅. arXiv 사전 인쇄본 arXiv:2212.00774(2022). Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. 2021. NeuS: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암묵적 표면 학습. 신경 정보 처리 시스템의 발전(2021), 27171-27183. Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. 2023. Rodin: 확산을 사용하여 3D 디지털 아바타를 조각하기 위한 생성 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 4563-4573. Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, Jamie Shotton. 2021. Fake it till you make it: face analysis in the wild using synthesis data alone. IEEE/CVF international conference on computer vision의 진행 과정에서. 3681-3691. Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang, Yuanxun Lu, Xun Cao. 2023. High-Fidelity 3D Face Generation From Natural Language Descriptions. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 진행 과정에서. 4521-4530. Yunze Xiao, Hao Zhu, Haotian Yang, Zhengyu Diao, Xiangju Lu, Xun Cao. 2022. 암묵적 함수를 학습하여 다중 뷰 이미지에서 자세한 얼굴 형상 복구. AAAI 인공지능 컨퍼런스 논문집, Vol. 36. 2839-2847. Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J Black. 2023. ECON: Normal 적분을 통해 최적화된 명시적 옷을 입은 인간. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J Black. 2022. ICON: Normal에서 얻은 암묵적 옷을 입은 인간. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 13286-13296. Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao. 2020. Facescape: 대규모 고품질 3D 얼굴 데이터 세트 및 세부적인 조작 가능한 3D 얼굴 예측. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 601-610. 김유왕, 김지연, 오태현. 2022. CLIP-Actor: 인간 메시 애니메이션을 위한 텍스트 기반 추천 및 스타일화. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 논문집. 173-191. Lvmin Zhang 및 Maneesh Agrawala. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543(2023). Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu. 2023. DreamFace: 텍스트 안내에 따른 애니메이션 가능한 3D 얼굴의 점진적 생성. arXiv 사전 인쇄본 arXiv:2304.03117 (2023). Zerong Zheng, Tao Yu, Yebin Liu, Qionghai Dai. 2021. Pamir: 이미지 기반 인간 재구성을 위한 매개변수 모델 조건화 암묵적 표현. IEEE 패턴 분석 및 머신 인텔리전스 거래 44, 6 (2021), 3170-3184. Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu. 2019. Deephuman: 단일 이미지에서 3D 인간 재구성. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록. 7739-7749. Hao Zhu, Yebin Liu, Jingtao Fan, Qionghai Dai, Xun Cao. 2016. 비디오 기반 야외 인간 재구성. 비디오 기술을 위한 회로 및 시스템에 관한 IEEE 거래 27, 4(2016), 760-770. Hao Zhu, Hao Su, Peng Wang, Xun Cao 및 Ruigang Yang. 2018. 단일 이미지에서 인체의 추정을 봅니다. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 회의 진행 중. 4450-4459. Hao Zhu, Haotian Yang, Longwei Guo, Yidi Zhang, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang 및 Xun Cao. 2021a. FacesCape: 단일 보기 3D 얼굴 재구성을 위한 3D 얼굴 데이터 세트 및 벤치마크입니다. arXiv 사전 인쇄 arXiv:2111.(2021). Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao 및 Ruigang Yang. 2019. 계층적 메시 변형을 통한 단일 이미지에서 자세한 인간 모양 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 4491-4500. Hao Zhu, Xinxin Zuo, Haotian Yang, Sen Wang, Xun Cao, Ruigang Yang. 2021b. 단일 이미지에서 자세한 아바타 복구. IEEE 패턴 분석 및 머신 인텔리전스 저널 44, 11(2021), 7363-7379.
