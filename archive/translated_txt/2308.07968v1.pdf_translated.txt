--- ABSTRACT ---
교육 Mingyang Zhang Google USA mingyang@google.com Spurthi Amba Hombaiah Google USA spurthiah@google.com Michael Bendersky Google USA bemike@google.com 개인화된 텍스트 생성은 최근 몇 년 동안 많은 주목을 받고 있는 새로운 연구 분야입니다. 이 방향의 대부분 연구는 맞춤형 기능이나 모델을 설계하여 특정 도메인에 초점을 맞춥니다. 이 연구에서는 대규모 언어 모델(LLM)을 사용하여 개인화된 텍스트 생성을 위한 일반적인 접근 방식을 제안합니다. 쓰기 교육의 관행에서 영감을 얻어 개인화된 생성을 위한 LLM을 가르치기 위한 다단계 및 멀티태스크 프레임워크를 개발합니다. 쓰기 교육에서 출처에서 쓰는 작업은 종종 정보 찾기, 평가, 요약, 종합 및 통합을 포함하는 여러 단계로 분해됩니다. 마찬가지로 개인화된 텍스트 생성에 대한 우리의 접근 방식은 검색, 순위 지정, 요약, 종합 및 생성이라는 여러 단계로 구성됩니다. 또한, 교육에서 학생의 독해 능력과 쓰기 능력이 종종 상관관계가 있다는 관찰에서 영감을 받아, 모델의 생성 능력을 더욱 개선하는 데 도움이 되는 멀티태스크 설정을 소개합니다. 각각 다르고 대표적인 도메인을 다루는 세 가지 공개 데이터 세트에 대한 접근 방식을 평가합니다. 결과에 따르면 다양한 기준선에서 상당한 개선이 나타났습니다. 키워드 개인화된 생성, 대규모 언어 모델 *Google에서 방문 연구원으로 수행한 작업. 이 작업의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 하드 카피로 만드는 것은 비용 없이 허가되지만, 이 사본이 이익 또는 상업적 이점을 위해 만들어지거나 배포되지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문이 포함되어야 합니다. ACM 이외의 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 다른 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org에서 허가를 요청하세요. XXX, XXX, XXX © 2023 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06... $15.https://doi.org/XXXXXXX.XXXXXXX ACM 참조 형식: Qiaozhu Mei* University of Michigan USA qmei@umich.edu Yi Liang Google USA yiliang@google.com Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, Michael Bendersky. 2023. LLM에게 개인화 교육에 영감을 받은 접근법을 가르치기. XXX 회의록에서. ACM, New York, NY, USA, 10페이지. https://doi.org/XXXXXXX.XXXXXXX
--- INTRODUCTION ---
인공 지능(AI) 기반 시스템이 콘텐츠 생성을 지원하는 데 점점 더 많이 사용됨에 따라 개인화된 텍스트 생성에 대한 관심이 엄청나게 증가했습니다. 사용자가 이전에 작성한 문서와 같은 보조 컨텍스트를 고려한 맞춤형 응답을 생성하는 것은 특정 대상, 생성 컨텍스트 및 정보 요구 사항을 지원하는 생성 시스템을 개발하는 데 중요합니다. 예시 응용 프로그램에는 트윗 및 뉴스 기사에서 과학 기사 및 허구, 기업 및 개인 커뮤니케이션(이메일, 채팅, 포럼)에 이르기까지 다양한 유형의 콘텐츠를 AI로 작성하고, 주어진 서면 콘텐츠를 요약 또는 반대로 자세히 설명하는 등 다른 스타일로 변환하는 것이 포함됩니다. 연구자들은 리뷰[13, 14], 대화 에이전트[17, 38, 41] 및 소셜 네트워크[8]를 포함하되 이에 국한되지 않는 다양한 도메인에서 개인화된 텍스트 생성을 조사했습니다. 이전 작업은 대부분 도메인별 기능이나 지식에 의존하고 특정 작업을 처리하는 모델을 제안합니다. 모든 시나리오에 적합한 일반적인 접근 방식을 설계하는 방법은 덜 연구된 영역입니다. 반면, 특히 ChatGPT¹ 및 Bard²와 같은 챗봇을 통한 생성 AI의 부상과 함께 대규모 언어 모델(LLM)이 많은 텍스트 생성 작업에서 점점 더 중요한 역할을 하고 있습니다. 그러나 LLM에 개인화 기능을 제공하는 방법을 탐구한 연구는 거의 없습니다. 이 연구에서는 대규모 언어 모델을 사용하여 개인화된 텍스트 생성을 위한 일반적인 접근 방식을 제안합니다. 저희의 작업은 쓰기 교육에서 널리 사용되는 관행에서 영감을 얻었는데, 이는 정보를 찾고, 평가하고, 요약하고, 종합하고, 통합하는 절차에 따라 출처에서 쓰기 작업을 분해합니다[3, 31]. https://chat.openai.com 2https://bard.google.com XXX, XXX, XXX Li et al. 마찬가지로 저희는 개인화된 텍스트 생성을 위한 LLM을 가르치기 위해 다단계 멀티태스크 프레임워크를 채택하며, 유사한 단계는 검색, 순위 지정, 요약, 종합 및 생성입니다. 구체적으로, 사용자가 쓰고 있는 문서의 제목과 시작 문장과 같은 즉각적인 맥락을 고려하여 쿼리를 공식화하고 과거에 사용자가 작성한 문서와 같은 개인적 맥락의 보조 저장소에서 관련 정보를 검색합니다. 그런 다음 검색된 결과를 관련성과 중요도에 따라 순위를 매긴 다음 순위가 매겨진 결과를 요약합니다. 또한 검색된 정보를 주요 요소로 합성하고 마지막으로 검색된 결과, 요약 및 합성된 정보를 새 문서를 생성하기 위한 대규모 언어 모델에 입력합니다. 언어 교육에서 종종 개인의 쓰기 능력의 능숙도는 독해 능력의 능숙도와 높은 상관 관계가 있는 것으로 관찰됩니다[4]. 또한 연구에 따르면 저자 인식 과제는 개인의 독서량과 수준을 측정하는 데 사용할 수 있으며[18], 이는 독해 능력과 상관 관계가 있습니다. 이 두 가지 관찰에서 영감을 받아 대규모 언어 모델의 독해 능력을 향상시키는 것을 목표로 하는 멀티태스크 설정을 만듭니다. 여기서 모델에 주어진 텍스트의 저자를 귀속시키는 보조 과제를 도입합니다. 우리는 이 과제가 모델이 주어진 텍스트를 더 잘 이해하고(즉, 읽고) 결과적으로 더 좋고 개인화된 콘텐츠를 생성(즉, 작성)하는 데 도움이 될 것으로 기대합니다. 우리는 개인 이메일 커뮤니케이션, 소셜 미디어 토론, 제품 리뷰를 포함하는 세 가지 공개 데이터 세트에서 제안된 모델을 평가합니다. 우리의 다단계 및 멀티태스크 프레임워크를 사용함으로써, 우리는 세 가지 데이터 세트 모두에서 다양한 기준에 비해 상당한 개선을 보여줍니다.
--- RELATED WORK ---
우리는 개인화된 텍스트 생성과 관련된 두 가지 작업인 제어된 텍스트 생성 및 텍스트 스타일 전송에 대한 문헌 검토를 제시합니다. 개인화된 텍스트 생성. 일부 연구는 도메인별 기능이나 지식을 활용하여 특정 도메인에 대한 개인화된 생성을 개선하는 데 중점을 둡니다. Li 및 Tuzhilin[14]은 제품 설명, 감정 레이블 및 사용자의 과거 리뷰가 주어진 개인화된 사용자 리뷰를 생성하기 위해 자기 주의 재귀 자동 인코더 기반 모델을 설계합니다. 제품 속성을 활용하기 위해 캡슐 그래프 신경망(CapsGNN)을 기반으로 하는 지식 강화 개인화된 리뷰 생성 모델이 [13]에 제안되었습니다. Gao et al.[8]은 개인화된 소셜 텍스트 생성에 중점을 두며, 여기서 개인화된 기능은 디코더 생성을 안내하기 위해 인코더에 제공됩니다. 대화 에이전트의 개인화에 대한 광범위한 연구가 있습니다[17, 38, 41]. 실제 대화 데이터가 제한되어 있기 때문에 연구자들은 크라우드 워커에게 특정 페르소나에 대한 대화를 작성하도록 요청하여 [41] 데이터를 구성하고 Reddit [17, 38] 및 Weibo [25, 43]에서 사용자 속성 및 발화를 추출하는 방법을 탐구했습니다. 개인화를 위해 미리 정의된 속성 및 주제를 사용하는 방법에 대한 연구가 있습니다. 개인화된 문장 생성
--- METHOD ---
생성적 적대 신경망(GAN)을 기반으로 제안[40]되었습니다. 자주 사용되는 기능어와 내용어는 모델 학습을 위한 입력 및 문장 구조 제약으로 사용됩니다. 덜 탐구된 영역은 도메인별 또는 사용자 정의 기능에 의존하지 않고 다양한 도메인에서 개인화된 생성을 위해 대규모 언어 모델을 활용하는 방법입니다. LaMP[29]는 저희의 작업에 가장 가까운 작업입니다. 이는 세 가지 분류 및 네 가지 텍스트 생성 작업에 대한 개인화된 언어 모델을 학습하고 평가하기 위한 벤치마크를 제공합니다. 이들은 사용자 프로필에서 텍스트를 검색하는 접근 방식을 배포합니다. LaMP에서 제공하는 생성 작업은 문장 수준입니다. 대신 저희는 더 어려운 구절 길이의 긴 텍스트를 생성하는 것을 고려합니다. 방법 측면에서 LaMP의 검색 기반 접근 방식은 저희가 제안한 다단계 프레임워크의 단일 구성 요소를 인스턴스화한 것으로 볼 수 있습니다. Skopyk et al.[34]은 개인화 효과를 달성하기 위해 변압기 계층 어댑터를 학습하는 것을 제안합니다. 이 논문은 어떤 것도 포함하지 않고 방법만 제안합니다.
--- EXPERIMENT ---
al 분석. 제어된 텍스트 생성. 제어된 텍스트 생성은 스타일 또는 의미일 수 있는 사전 정의된 속성 목록으로 텍스트를 생성하는 것을 목표로 합니다. 미세 조정 비용을 줄이기 위해 최근의 제어된 텍스트 생성 작업은 디코딩 시간 방법을 사용하여 추론 중에 사전 훈련된 모델이 원하는 속성에 맞춰 텍스트를 생성하도록 직접 합니다. 이러한 방법에는 PPLM[5], GeDi[11], FUDGE[39] 및 DEXPERTS[16]가 있습니다. 제어된 텍스트 생성은 사전 정의된 속성 집합(제약 조건)이 필요하다는 점에서 개인화된 생성과 다릅니다. 텍스트 스타일 전송. 제어된 텍스트 생성과 관련된 작업은 텍스트 스타일 전송입니다. 그 목표는 생성된 텍스트의 특정 속성을 제어하여 내용을 보존하면서 텍스트를 변환하는 것입니다. 병렬 코퍼스를 사용한 지도 학습과 비병렬 코퍼스를 사용한 비지도 방법의 두 가지 패러다임이 있습니다. 병렬 데이터를 사용하면 표준 시퀀스 대 시퀀스 모델을 직접 사용할 수 있습니다[27]. 비병렬 코퍼스만 사용 가능한 경우 세 가지 접근 방식이 있습니다.첫 번째 접근 방식은 생성 모델링을 위해 텍스트를 콘텐츠와 속성으로 분리하는 것입니다[33].프로토타입 편집이라고 하는 두 번째 접근 방식[12]은 생성을 위해 문장 템플릿과 속성 마커를 추출합니다.세 번째 접근 방식은 모델을 훈련하기 위해 의사 병렬 코퍼스를 구성합니다[42].텍스트 스타일 전송과 달리 개인화된 생성은 원래 텍스트가 이미 주어진다고 가정하지 않습니다.제어된 텍스트 생성과 마찬가지로 대부분의 텍스트 스타일 전송 방법은 미리 정의된 속성 집합을 기대하지만 이는 우리 환경에서 사용할 수 없습니다.또한 우리의 작업은 LLM에게 사고의 사슬을 통해 추론하도록 가르치는 패러다임[36, 37]과 일치하며, 우리의 작업 분해는 쓰기 교육에서 많은 영감을 받았으며 각 작업에 대한 모델 훈련은 프롬프트 엔지니어링을 넘어섭니다.문제 공식화 우리는 사용자가 현재 문서라고 하는 문서를 작성하는 설정을 고려합니다. 즉각적인 맥락과 사용자의 개인적 맥락을 감안할 때, 개인화된 모델의 목표는 사용자가 쓰기를 마친 것처럼 생성된 문서가 실제 현재 문서에 가깝도록 문서를 완성하는 것입니다. 즉각적인 맥락과 개인적 맥락을 정의하는 방법은 여러 가지가 있을 수 있습니다. 단순성과 일반성을 위해 현재 문서의 제목과 짧은 시작 부분을 즉각적인 맥락으로 사용합니다. 사용자의 개인적 맥락은 현재 문서를 쓸 당시 과거에 작성한 문서로 정의됩니다. 이러한 맥락은 학생이 쓰도록 지시받은 출처와 유사합니다[3]. 우리의 훈련 과제는 다음과 같이 공식화됩니다. {(xut, Dut, dut)}의 예제 목록이 주어지면, xut은 LLMs to Personalize를 가르치기 - Writing Education에서 영감을 받은 접근 방식 XXX, XXX, XXX 개인적 맥락의 즉각적인 맥락입니다. 2. 순위 3. 요약 상위 항목 요약 4. 주요 요소 종합 1. 현재 문서의 즉각적인 맥락 검색 LLM 圖貳 생성 예측 개인화된 문서 동일 저자? 문서 쌍 멀티태스킹 그림 1: 개인화된 텍스트 생성을 위한 다단계 멀티태스킹 프레임워크 개요. = 시간 단계 t에서 현재 문서 dut에 대한 사용자 u이고, Dut {du1, du2,. ‚du,t−1}은 과거 문서의 개인적 맥락이므로, d&#39;ut와 dut의 유사성을 최대화할 수 있도록 (xut, Dut)를 기반으로 dut를 생성하는 개인화된 생성 모델 G를 학습하고자 합니다. 맥락에서 명확할 때마다 아래 첨자 u를 생략하고 대신 xt, Dt, dt를 직접 사용합니다. 또한 &quot;사용자&quot;와 &quot;작성자&quot;를 서로 바꿔 사용합니다. 방법 개요 개인화된 텍스트 생성을 위한 다단계 멀티태스킹 프레임워크 개요는 그림 1에 나와 있습니다. = 사용자 u가 작성한 현재 문서의 즉각적인 맥락 xt가 주어지면 검색기 Re(xt, Dt)는 쿼리로 xt를 사용하여 과거 사용자 문서 집합 Dt에서 항목을 검색합니다. 반환된 항목은 순위 매기기 Ra에 공급되어 순위가 매겨진 항목 &amp;t Ra(Re(xt, Dt))를 생성하고, 이 항목은 다음에 의해 소비됩니다. (1) 검색된 여러 문서의 요약을 생성하는 요약 모델 Su(x+, εt); (2) 이러한 문서의 주요 요소를 생성하는 합성 모델 Sy(x+, &amp;t). 개인화된 생성 모델 G는 현재 문서 d&#39;₁ = G(xt, Su(xt, &amp;t), Sy(xt, &amp;εt), &amp;t)를 생성하고 기준 진실 현재 문서 dt에 대해 학습합니다. 또한 모델이 사용자 컨텍스트를 더 잘 이해하고 더 나은 개인화된 콘텐츠를 생성할 수 있도록 돕는 저자 구별이라는 보조 작업을 고려합니다. 사용자 u가 작성한 문서 dui가 주어지면 다른 문서 d₂j를 무작위로 샘플링하여 문서 쌍을 형성합니다. 그런 다음 모델 G는 튜플 집합 {(dui, dvj), y}에 대해 학습합니다. 여기서 레이블 y = v = u이면 true, 그렇지 않으면 y = false입니다. G가 시퀀스-시퀀스 모델이므로 y에 대해 숫자 레이블 대신 텍스트 {true, false}를 사용한다는 점에 유의하세요.개인화된 텍스트 생성 섹션 4에서 설명한 대로 각 단계의 세부 사항을 논의합니다.5. 검색 검색 단계에서 즉각적인 컨텍스트 xt가 주어지면 검색기 Re(xt, Dt)는 x+를 쿼리로 사용하여 과거 문서 집합 Dt에서 관련 텍스트 항목을 검색합니다.모든 시나리오에 적용할 수 있는 즉각적인 컨텍스트를 정의하려면 FirstKCharacters(dt)를 사용합니다.여기서 First KCharacters(·)는 텍스트의 처음 K 문자를 반환합니다.모든 실험에서 K = 150으로 설정했습니다.문서에 제목이 있으면 제목과 본문을 텍스트로 연결합니다.희소 및 밀집 검색기를 모두 사용하여 사용자의 과거 문서에서 관련 항목을 검색합니다.BM25[28]를 희소 검색기로 사용합니다. 우리는 T5X 검색 모델[19, 21], GTR-Large를 우리의 밀집 검색기로 사용합니다. 우리는 유사한 성능을 보이지만 벤치마크 데이터 세트[21]에서 훨씬 더 나쁜 효과-지연 트레이드오프를 보여주기 때문에 더 큰 크기의 모델을 선택하지 않습니다. 밀집 검색의 경우, 우리는 개인 문서 항목을 인덱싱할 때 문서 수준과 스니펫 수준의 두 가지 세분성 수준으로 실험합니다. 우리는 많은 문장이 충분한 맥락 정보를 제공하기에는 너무 짧기 때문에 문장 수준을 선택하지 않습니다. 우리는 이런 방식으로 스니펫을 만듭니다. 250자에 도달하거나 문서의 끝에 도달할 때까지 같은 문서에서 문장을 계속 추가합니다. 검색할 스니펫이 매우 짧기 때문에 문서 수준에서만 희소 검색의 성능을 검토합니다. 5.2 순위 매기기 우리는 문서와 스니펫 수준 모두에서 항목을 인덱싱하는 것을 실험하기 때문에 그에 따라 항목을 순위 매길 수 있습니다. • RANKDOCBM25. 희소 검색의 경우, 우리는 BM25 점수에 따라 문서를 검색하고 순위를 매깁니다. • RANKDOCDENSE. 밀집 검색의 경우 문서 수준에서 항목을 검색할 때 검색된 문서의 임베딩 유사성을 기준으로 검색된 문서를 순위를 매깁니다. • RANKSNIPPET. 마찬가지로 밀집 검색의 경우 스니펫 수준에서 항목을 검색할 때 임베딩 유사성을 기준으로 검색된 스니펫을 순위를 매깁니다. 분석하는 동안 RANKDOCDENSE와 RANKSNIPPET 모두에 문제가 있음을 발견했습니다. RANKDOCDENSE를 통해 검색된 결과는 인코딩할 문서가 길 때 임베딩이 덜 효과적이므로 관련성이 낮을 수 있습니다. 반면 RANKSNIPPET의 경우 유사한 스니펫이 많이 검색되어 생성에 필요한 정보가 충분하지 않습니다. 예를 들어, 즉각적인 맥락이 I really enjoy reading the book인 경우 I enjoy the book, The book is fun, I love this book과 같은 유사한 스니펫을 검색할 수 있습니다. 그것들은 모두 관련이 있지만 이 사용자가 특정 책을 좋아하는 이유에 대한 충분한 세부 정보를 제공하지 못하는데, 이는 구절 수준 생성에 중요합니다.두 가지 문제를 완화하기 위해 검색에서 구절 증거를 사용하는 과거 작업[2]에서 영감을 얻은 또 다른 고밀도 검색 전략인 RANKDOCBYSNPT를 제안합니다.RANKDOCBYSNPT는 스니펫 수준에서 관련 텍스트를 검색하여 문서 임베딩이 덜 효과적이라는 문제를 해결합니다.순위 단계에서는 스니펫을 직접 순위 지정하는 대신 검색된 스니펫이 포함된 문서를 순위 지정하여 RANKSNIPPET을 통해 검색된 스니펫의 다양성 부족을 완화합니다.특히 각 문서 d;에 대해 검색된 각 스니펫 sij Є di와 직접적인 맥락 x+ 사이의 임베딩 유사도 점수를 계산하고, 순위를 매기기 위한 문서 점수로 최대 점수를 사용합니다.즉, score(di, x+) = maxs₁₁ =d; (score(sij, xt)). 모든 순위 매기기 전략을 비교할 수 있도록 순위가 매겨진 항목을 문자열로 연결하고 2,500자로 자릅니다. 따라서 후속 모듈에는 동일한 길이의 입력 텍스트가 제공됩니다. 5. 요약 요약 단계는 검색된 항목에서 중요한 정보를 추출하여 생성 모델이 사용자의 개인적 맥락에서 가장 중요한 정보(주제, 핵심 요점 또는 유용한 XXX, XXX, XXX Li et al.)와 유사도를 계산합니다. 이 책은 놀랍습니다. 모든 스니펫 • 큰 소리로 읽게 했습니다. ⚫ 이 책을 읽는 것이 즐거웠습니다. ⚫ 등장인물이 좋습니다. ⚫ 이 책은 페이지 넘기기가 쉽습니다. 맥락에 따른 요약에 대한 약한 레이블 이 책을 읽는 것이 즐거웠습니다. 등장인물이 좋습니다. 이 책은 페이지 넘기기가 쉽습니다. groundtruth 현재 문서의 스니펫 • 이 책을 정말 즐겼습니다. 등장인물이 사랑스럽습니다. 페이지 넘기기가 쉽습니다. 각 기준 진실 스니펫을 과거 1과 페어링합니다. 기준 진실 현재 문서의 스니펫 유사도가 가장 높은 과거 스니펫 정말 즐거웠습니다. 읽은 것이 즐거웠습니다. 선택한 책을 조인합니다. 이 책. 과거 스니펫을 문자열로 만듭니다. 문자는 I love the loving. 문자입니다. 교차 엔트로피 손실을 최소화합니다. 여전히 모델로 T5-11B[26]를 선택합니다. 한 가지 중요한 사항은 요약 모델을 훈련하는 데 생성 작업에 대한 훈련 데이터의 절반만 사용한다는 것입니다. 이런 식으로 생성된 요약은 훈련 데이터의 보이지 않는 절반에서 더 노이즈가 많습니다(테스트 세트와 유사). 결과적으로 생성 모델은 훈련 중에 노이즈가 있는 요약을 인식하고 다른 정보에 주의를 기울여 이를 처리하는 방법을 학습합니다. 또 다른 사항은 요약 모델을 훈련하는 데 생성 작업에 대한 검증 또는 테스트 데이터가 사용되지 않는다는 것입니다. 페이지 터너입니다. 책은 페이지 터너입니다. 그림 2: 컨텍스트에 따른 요약을 위한 약한 레이블 생성. 구문(출력에 반영될 수 있음) 요약을 사용하면 생성 모델은 고수준 측면을 추출하고 동시에 정확한 단어를 생성하는 작업을 할 필요가 없으므로 생성 작업이 더 쉬워집니다.우리는 문맥 독립 및 문맥 종속 요약의 두 가지 전략을 실험합니다.문맥 종속이란 요약이 즉각적인 문맥에 따라 달라진다는 것을 의미합니다.문맥 독립 요약.우리는 문맥 독립 요약의 간단한 구현을 선택합니다.공개적으로 사용 가능한 요약 데이터 세트에서 독립 LLM T5-11B [26]를 미세 조정하고, 추론을 위해 순위가 매겨진 항목에 미세 조정된 모델을 직접 사용합니다.사용하는 데이터 세트는 CNN/Daily Mail [30], ForumSum [9] 및 Reddit TIFU-long [10]입니다.문맥 종속 요약.문맥 종속 요약 모델을 학습하는 과제는 기준 진실 레이블이 부족하다는 것입니다.우리는 기준 진실 현재 문서를 기반으로 약한 레이블을 생성하여 이 과제를 해결합니다.우리는 현재 문서에서 사용될 가능성이 더 높은 검색된 결과에서 텍스트를 추출하고 싶다는 것이 직감입니다. 이를 위해 현재 문서의 텍스트와 유사한 순위가 매겨진 항목에서 텍스트를 찾는데, 이는 추출 요약 작업으로 공식화할 수 있습니다. 이 아이디어를 설명하는 예는 그림 2에서 찾을 수 있습니다. 구체적으로, 기준 진실 현재 문서의 각 스니펫 sti = dt에 대해, 과거 문서에서 검색된 순위가 매겨진 항목 &amp; = {e1, e2, ...,\ , eR}의 모든 스니펫과의 유사도를 계산합니다. 여기서 e;는 항목 Et의 j번째 스니펫이고, R은 순위에 포함하는 스니펫의 수입니다. 단순화를 위해 검색 단계에서 T5X 검색 모델[19, 21]에서 얻은 임베딩을 재사용하여 유사도를 계산합니다. 유사도 점수가 가장 높은 스니펫 emax₁ = arg maxe; &amp;&amp; (score (ej, Sti))이 후보 스니펫 목록 Lt에 추가됩니다. 선택한 스니펫 emax; 후보 목록 Lt에 이미 있는 경우, 새 스니펫을 찾을 때까지 다음으로 높은 점수를 가진 스니펫을 찾습니다. 모든 스니펫 쌍을 반복할 때까지 순위가 매겨진 항목에서 새로 선택한 스니펫을 후보 목록에 계속 추가합니다. 약한 레이블은 후보 목록의 스니펫을 하나의 문자열로 결합하는 Join(Lt)에 의해 생성됩니다. 즉각적인 컨텍스트 x+와 순위가 매겨진 항목 &amp;t가 주어지면, 컨텍스트 종속 요약 모델 Su(x+, &amp;t)는 약한 레이블 Join(Lt)를 사용하여 학습됩니다. 5. 합성 합성 단계는 현재 쓰기 과제에 대한 전반적인 이해를 위해 상위 검색된 항목에서 공통적인 핵심 요소를 찾는 것을 목표로 합니다. 이 논문에서는 합성 단계로 키워드 추출을 실험합니다. 합성에 대한 보다 정교한 접근 방식은 탐색할 가치가 있으며 향후 작업으로 남겨둡니다. 요약과 유사하게 컨텍스트 독립 합성과 컨텍스트 종속 합성의 두 가지 전략을 조사합니다. 컨텍스트는 또한 즉각적인 컨텍스트를 참조합니다. 컨텍스트 독립 합성. 과거 문서 Dt에서 빈번한 용어를 찾아 키워드를 추출합니다. 대부분의 사용자가 더 큰 n을 갖는 n-그램을 추출하기에 충분한 문서를 가지고 있지 않기 때문에 용어를 유니그램으로 제한합니다. 또한 불용어, 빈도가 1인 단어, 역문서 빈도가 작은(IDF &lt; 1.5) 단어를 제거합니다. 그런 다음 나머지 단어를 빈도에 따라 내림차순으로 정렬한 다음 IDF에 따라 정렬하고 최대 20개의 단어를 유지합니다. 문맥 종속 합성. 문맥 종속 합성을 위한 약한 레이블을 만드는 아이디어는 문맥 종속 요약을 위한 약한 레이블을 만드는 방법과 매우 유사합니다. 즉, 검색된 결과에서 현재 문서 생성에 사용될 가능성이 있는 중요한 단어를 찾는 것을 목표로 합니다. 보다 구체적으로, 실제 현재 문서의 각 소스 단어 wti = dt에 대해 순위가 매겨진 항목에서 각 타겟 단어 vtj ¤ Ɛt와의 유사도를 계산합니다. 소스 및 타겟 단어 모두에 대해 불용어 또는 IDF &lt; 1.5인 단어를 건너뜁니다. 두 단어(Wti, vtj)는 다음 조건 중 하나 이상이 충족되는 경우 유사합니다. • 두 단어가 동일합니다. • 두 단어는 WordNet [6]에서 정의된 동의어입니다.• 두 단어는 임베딩 공간에서 가깝습니다.Common Crawl 데이터 세트에서 사전 학습된 케이스가 지정되지 않은 Glove [24] 임베딩을 사용합니다.유클리드 거리가 4보다 작으면 두 단어를 유사한 것으로 정의합니다.합격된 대상 단어 vt;를 후보 대상 단어 목록 T에 추가합니다.모든 가능한 (소스, 대상) 단어 쌍을 살펴본 후 후보 목록 Tt에 있는 단어를 선택된 횟수에 따라 역으로 정렬한 다음 IDF에 따라 정렬합니다.그런 다음 후보 단어 목록을 문자열로 결합하여 약한 레이블 Join(T+)을 형성합니다.여전히 합성을 위해 T5-11B [26] 모델을 미세 조정합니다.직접적인 컨텍스트 xt와 순위가 매겨진 항목 εt가 주어지면 컨텍스트 종속 합성 모델 Sy(x+, εt)는 약한 레이블 Join(T)를 사용하여 교차 엔트로피 손실을 최소화하기 위해 학습합니다.요약에 사용된 것과 동일한 학습 예제 세트를 사용합니다. 유일한 차이점은 레이블이 조인된 스니펫 목록 Join(Lt)에서 조인된 대상 단어 목록 Join(T)으로 변경된다는 것입니다.LLM에게 개인화를 가르치기 - 쓰기 교육에서 영감을 얻은 접근 방식 5.개인화된 생성 즉각적인 맥락 xt, 과거 문서에서 검색된 순위가 매겨진 항목 &amp;, 요약 모델 Su(xt, &amp;t)의 맥락 독립/종속 요약, 합성 모델 Sy(xt, εt)의 맥락 독립/종속 합성, 개인화된 생성 모델 G(xt, Su(xt, &amp;t), Sy(xt, &amp;t), &amp;t)는 교차 엔트로피 손실을 최소화하기 위해 기준 진실 현재 문서 d₁를 레이블로 사용하여 학습됩니다.모델이 다양한 정보 소스를 구별할 수 있도록 소스를 단일 문자열 입력으로 변환할 때 다른 접두사를 추가합니다.특히 즉각적인 맥락은 구절 시작으로 접두사를 붙이고, 요약은 요약으로 접두사를 붙이고, 합성은 중요 단어로 접두사를 붙이고, 순위가 매겨진 항목 목록은 과거 구절로 접두사를 붙입니다. 5. 멀티태스크 학습 언어 교육 연구에 따르면 쓰기와 읽기 능력은 높은 상관 관계가 있습니다[4]. 또한 연구자들은 저자 인식 과제를 사용하여 개인의 독서량을 평가할 수 있으며[18], 이는 독서 능력과 상관 관계가 있음을 발견했습니다. 위의 연구에서 영감을 받아 쓰기에 해당하는 생성 과제 외에도 저자의 스타일을 더 잘 이해하는 모델의 능력을 향상시키는 것을 목표로 하는 독해 과제를 추가합니다. 구체적으로 저자 구별 과제를 도입하는데, 이는 모델이 주어진 문서 쌍이 같은 저자가 썼는지 여부를 결정해야 합니다. 사용자 u가 쓴 문서 dui가 주어지면 절반의 시간 동안 같은 사용자 u의 다른 문서 du;를 무작위로 샘플링하여 긍정적인 학습 예제(x, y) = ((dui, duj), true)를 형성합니다. 그렇지 않으면 다른 사용자 v(vu)의 다른 문서 dk를 무작위로 샘플링하여 부정적인 예제(x, y) ((dui, duk), false)를 형성합니다. 생성 모델 G가 시퀀스-투-시퀀스 모델이므로 텍스트 레이블 y = {true, false}를 사용합니다. = 생성 모델은 개인화된 생성과 저자 구별의 두 가지 작업에 대해 동시에 학습되므로 모델이 어떤 작업을 수행할지 구별할 수 있도록 작업 수준 지침을 모델 입력에 추가합니다. 개인화된 생성 작업의 경우 지침은 &quot;사용자 음성으로 구절을 완성하세요&quot;입니다. 저자 구별 작업의 경우 지침은 &quot;두 구절이 같은 저자의 것인지 예측하세요&quot;입니다. 저자 구별 작업의 모든 문서는 개인화된 생성 작업의 검증 또는 테스트 데이터에 나타나지 않는 사용자로부터 샘플링됩니다. 실험 설정 이 섹션에서는 데이터 세트, 훈련 세부 정보, 경쟁 방법 및 메트릭을 포함한 실험 설정을 설명합니다. 6. 데이터 세트 각각 대표적인 도메인의 세 가지 공개 데이터 세트에서 모델을 평가합니다. 데이터 통계 요약은 표 1에서 찾을 수 있습니다. 세 가지 데이터 세트 모두에 대해 해당 문서의 정의를 제공합니다. Avocado Research Email Collection[22]은 &quot;Avocado&quot;라는 정보 기술 회사의 279개 계정에서 가져온 이메일과 첨부 파일로 구성되어 있습니다. LaMP[29]의 처리 단계 XXX, XXX, XXX를 따르고 발신자 주소별로 이메일을 그룹화합니다. 회사 외부의 발신자가 있으므로 발신자/사용자의 수는 279개 계정 이상이 될 수 있습니다. 이메일을 문서로 취급하고 제목을 제목으로 취급합니다. Amazon 리뷰 데이터[20]는 다양한 제품 카테고리에 대한 Amazon의 사용자 리뷰를 제공합니다. 전체 데이터 세트가 매우 크기 때문에 리뷰 수가 가장 많은 가장 큰 카테고리인 책을 선택합니다. 리뷰를 사용자별로 그룹화합니다. 리뷰를 문서로 취급하고 리뷰 제목을 문서 제목으로 취급합니다. Reddit 댓글 데이터 세트[35]는 2007년부터 2015년까지 Reddit에서 볼 수 있는 모든 게시물과 댓글로 구성되어 있습니다. 게시물과 댓글을 모두 문서로 취급하고 사용자별로 그룹화합니다. 세 가지 데이터 세트 모두에서 각 사용자의 개인적 맥락에서 동일한 문서를 중복 제거합니다. 문서가 300자를 넘고 문서 작성자가 이 문서 이전에 최소 2개의 문서를 작성한 경우 해당 문서는 현재 문서, 즉 생성할 문서로 적격합니다. 적격한 현재 문서마다 예를 하나 생성합니다. 최소 5개의 예가 있는 사용자만 유지합니다. 데이터 세트가 특정 활성 사용자에 의해 지배되는 경우 사용자당 최대 50개의 예를 포함합니다. 모델의 일반화 능력을 평가하기 위해 데이터 세트를 사용자별로 분할하여 검증 및 테스트 세트에 학습 세트에서 볼 수 없는 사용자의 문서만 포함되도록 합니다. 학습/검증/테스트 세트의 사용자 분할 비율은 85/5/10입니다. 6.2 학습 세부 정보 모든 요약, 합성 및 개인화된 생성 작업에 대해 T5-11B [26] 모델을 미세 조정합니다. T5-11B 모델은 기본 학습 속도가 0.001인 Adafactor 알고리즘 [32]에 의해 최적화됩니다. 우리는 선형 워밍업 스케줄러를 사용하여 워밍업을 위한 첫 번째 1,000개 학습 단계를 사용합니다. 또한 학습률의 제곱근 정규화된 감소를 적용합니다. 우리는 성능이 검증 세트에서 수렴할 때까지 모델을 학습합니다. 빔 탐색[7]은 빔 크기 4로 디코딩하는 데 사용됩니다. 멀티태스크 학습의 경우 개인화된 생성과 저자 구별을 위한 예를 1:1의 비율로 간단히 혼합합니다. 6.3 경쟁 방법 2절에서 언급했듯이 대부분의 개인화된 생성 모델은 도메인별 지식이나 기능에 의존하는 특정 도메인에 대해 제안됩니다. 우리와 가장 가까운 작업은 LLM을 사용하여 개인화를 위한 검색 기반 방법을 사용하는 LaMP[29]입니다. LaMP는 문장 길이 생성에 초점을 맞추기 때문에 이 논문에서 한 것처럼 스니펫 또는 문서 기반 전략을 활용하는 이점을 탐구하지 않습니다. 우리는 크레딧 할당을 더 잘 이해하기 위해 다음과 같은 경쟁 방법을 고려합니다. 기준선. 우리는 이러한 기준선을 고려합니다. • IMMEDCTX. 이 방법은 제목과 현재 문서의 짧은 시작 부분인 즉각적인 컨텍스트 x+만 모델 입력으로 사용합니다.• USERID. 모델 입력에 사용자 ID를 추가하고 다음 스니펫 생성 작업을 사용하여 모델을 학습합니다.사용자 ID는 모델이 각 사용자의 개인 스타일을 기억하는 데 도움이 됩니다.XXX, XXX, XXX 표 1: 데이터 세트 통계.#현재 문서의 평균 문자 수 #현재 문서당 평균 과거 문서 수 #사용자 #현재 문서 (#예제) 훈련 Val.테스트 훈련 Val. 테스트 아보카도 이메일 Amazon 리뷰 Reddit 3,648.1,056.658.42.45.88.354,275 20,789 41,240,626 14,223 28,13,1,5,349,661 311,469 621,3,858,731 231,307 455,Li et al. 이 모델은 학습 중에 본 사용자에게 더 나은 성능을 발휘하므로 검증 및 테스트 세트의 문서를 포함하여 예측에 사용되지 않는 모든 문서를 포함하여 모델을 학습합니다. 다음 스니펫 생성 작업에서는 모델이 문서의 스니펫이 주어지면 다음 스니펫을 생성해야 합니다. 즉각적인 컨텍스트에는 현재 문서의 짧은 시작이 포함되며, 이는 또한 기준 진실 출력의 시작입니다. 다른 생성 모델은 손실을 최소화하기 위해 시작을 출력에 복사하는 방법을 배우지만 이 기준 모델은 그렇게 하도록 학습되지 않았습니다. 비교를 공정하게 하기 위해 사후 처리로 즉각적인 맥락에 포함된 현재 문서의 시작 부분을 모델 출력에 추가합니다.⚫ LLMZEROSHOT. 우리는 제로샷 생성을 위해 새로운 최첨단 LLM인 PaLM 2 모델[1]을 사용합니다.PaLM에는 개인화된 생성 작업을 수행하도록 모델을 촉구하는 작업 지침, 즉각적인 맥락, 맥락에 따른 요약, 맥락에 따른 합성 및 최상의 순위 전략에서 검색된 항목을 포함하여 실험에 기반한 최상의 구성의 입력이 공급됩니다.USERID 기준선과 유사하게 현재 문서의 시작 부분도 모델 출력에 추가합니다.검색 증강 방법.우리는 섹션 5.2에서 소개한 순위 전략을 실험합니다.하나의 희소 검색 기반 방법 RANKDocBM25와 세 개의 밀집 검색 기반 방법 RANKDOCDENse, RankSnippet 및 RankDocBYSNPT. 또한, 과거에 작성된 가장 최근의 문서를 순위가 매겨진 항목으로 검색하는 최근성 기반 기준선인 RECENTDOC를 추가합니다. 이러한 모든 방법에 대해 개인화된 생성 모델 G는 즉각적인 맥락 x+와 순위가 매겨진 항목 &amp;t에 대해 학습합니다. 요약. 최상의 순위 전략 구성 위에 요약을 수행하여 섹션 5.3에서 소개한 요약 방법을 살펴봅니다. 개인화된 생성 모델 G는 즉각적인 맥락 xt, 요약 모델 Su(x+, &amp;t)의 출력, 순위가 매겨진 항목 &amp;t에 대해 학습합니다. 맥락 독립 요약을 SUMCTXIND라고 하고 맥락 종속 요약을 SUMCTX라고 합니다. 합성. 최상의 요약 방법 구성 위에 합성을 적용하여 섹션 5.4에서 소개한 합성 방법을 평가합니다. 개인화된 생성 모델 G는 즉각적인 맥락 xt, 요약 모델 Su(xt, εt)의 요약, 합성 모델 Sy(xt, &amp;t)의 합성, 순위가 매겨진 항목 εt에 대해 학습합니다. 우리는 문맥 독립 합성을 SYNCTXIND로, 문맥 종속 합성을 SYNCTX로 지칭합니다.멀티태스크 훈련.우리는 멀티태스크 설정을 지칭하기 위해 AUTHORPRED를 사용하는데, 여기서 우리는 단일(생성) 작업의 최적 구성 위에 저자 구별 작업을 추가하여 생성 모델을 공동으로 훈련합니다.6. 지표 생성된 각 현재 문서에 대해 우리는 기준 진실 현재 문서와의 중첩을 계산합니다.우리는 개인화된 생성 작업에서 널리 사용된 BLEU[23], Rouge-1, ROUGE-2 및 ROUGE-L[15]을 평가 지표로 채택했습니다[14, 29].우리는 쌍대 t-검정을 사용하여 통계적 유의성 검정을 수행합니다.7.실험 결과 전체 성능 표 2: Avocado 이메일 데이터 세트의 전체 성능(%).*, †는 각각 0.01 수준에서 IMMEDCTX, RANKDOCBM25에 비해 통계적으로 유의미한 개선을 나타냅니다. 아보카도 이메일 BLEU ROUGE-1 ROUGE-2 ROUGE-L 기준선 IMMEDCTX USERID LLMZEROSHOT 17.32.21.28.13.32.20.27.14.35.06* 22.11* 28. 검색 증강 방법 RECENTDOC RANKDOCBM25 21.19* RANKDOCDENSE 19.43* RANKSNIPPET RANKDOCBYSNPT 21.06* 19.57* 35.64* 23.96* 31.25* 37.69* 25.99* 33.07* 35.62* 23.71* 30.90* 18.69* 35.82* 23.26* 30.78* 37.42* 25.65* 32.90* +요약 SUMCTXIND SUMCTX 21.23* 37.58* 25.79* 23.17** 39.31** 26.64** 33.15* 34.37** +합성 SYNCTXIND SYNCTX 23.06** 39.24* 26.72*34.52** 23.44** 40.38** 26.93** 34.34** +멀티태스크 AUTHORPRED 23.27** 41.02** 28.60** 35.70** 세 가지 데이터 세트의 전반적인 성능은 각각 표 2, 3 및 4에 나열되어 있습니다. 일반적으로 검색 증강 방법은 검색된 정보를 활용하지 않는 기준선보다 더 나은 성과를 보입니다. 요약과 합성은 즉각적인 맥락에 따라 달라질 때 추가적인 이득을 가져다줍니다. 멀티태스크 학습 LLM에게 개인화 가르치기 - 쓰기 교육에서 영감을 얻은 접근법 표 3: Amazon 리뷰 데이터 세트의 전체 성과(%). *는 각각 0.01 수준에서 IMMEDCTX, RANKDOCBYSNPT에 비해 통계적으로 유의미한 개선을 나타냅니다. Amazon 리뷰 BLEU ROUGE-1 ROUGE-2 ROUGE-L Baselines IMMEDCTX USERID LLMZEROSHOT 17.36.17.36.22.22.16.38.74* 22.31.32.11* 30. 검색 증강 방법 RECENTDOC 19.09* 37.51* 23.37* 32.67* RANKDOCBM25 19.49* 37.79* 23.56* 32.85* RANKDOCDENSE 19.38* 37.49* 22.92* 32.48* RANKSNIPPET 19.44* 37.45* 23.10* RANKDOCBYSNPT 19.35* 38.28* 23.87* 32.50* 33.23* +요약 SUMCTXIND 19.17* 38.33* 23.66* SUMCTX 19.81** 38.66* 24.25** 33.35* 33.57* +합성 SYNCTXIND SYNCTX 19.84** 19.87** 38.68* 24.31** 39.46** 24.66** 33.61* 33.97** +멀티태스크 AUTHORPRED 19.78** | 39.36** 24.56** 33.87** 대부분 데이터 세트에서 모델의 생성 능력을 더욱 향상시킵니다. 아래에서 중요한 방법을 자세히 비교합니다. 기준선 간 비교. IMMEDCTX와 비교했을 때 USERID는 특히 Amazon 리뷰와 Reddit 데이터에서 놀라울 정도로 좋은 성능을 보였습니다. 이는 USERID 모델이 다음 스니펫 예측 작업에서 학습되어 상대적으로 생성된 출력이 짧아지기 때문입니다. 사용자 ID를 기억함으로써 이 모델은 Amazon 리뷰와 Reddit 데이터와 같이 문서 길이가 짧은 데이터 세트에서 유리합니다. 그러나 이 모델은 사용자 ID를 입력으로 요구하기 때문에 새로운 사용자로 일반화하거나 많은 수의 사용자로 확장하는 데 문제가 있습니다. LLMZEROSHOT은 SYNCTX와 동일한 입력으로 제공됩니다. 성능이 저하된 것은 미세 조정이 개인화된 생성 작업에 중요함을 시사합니다. 검색 증강 방법. 모든 검색 증강 방법은 사용자의 개인적 맥락에서 모든 문서를 제외하는 IMMEDCTX보다 성능이 우수합니다. 이는 모델이 사용자를 위해 현재 문서를 생성할 때 과거 문서가 유용한 정보를 제공함을 나타냅니다. RECENTDOC는 많은 경우 특히 Avocado 이메일 데이터 세트에서 유사성 기반 검색 방법과 비슷한 수준의 성능을 예상치 못하게 발휘합니다. 하지만 여전히 RANKDOCBYSNPT보다 성능이 떨어집니다. 한편으로 RECENTDOC는 사용자의 최근 문서가 유사한 글쓰기 스타일과 유사한 콘텐츠를 공유할 수 있으므로 괜찮은 전략입니다. 예를 들어, 사용자가 특정 주제와 관련된 여러 이메일을 작성하거나 사용자가 특정 책 장르 XXX, XXX, XXX에 관심을 갖기 시작하여 유사한 서평을 작성하는 경우가 있습니다. 반면에 RANKDocBYSNPT를 사용하여 가장 최근의 콘텐츠를 검색하는 대신 즉각적인 맥락에 따라 현재 주제와 더 관련성 있는 정보를 제공하면 생성 성능을 더욱 개선할 수 있습니다. RANKDOCBM25는 Amazon 리뷰 데이터 세트를 제외하고는 RANKDOCBYSNPT와 유사한 성능을 발휘하며, RANKDOCBM25는 Amazon 리뷰 데이터 세트에서 효과가 떨어집니다. 이는 BM25가 일반적인 검색 쿼리보다 쿼리(즉각적인 맥락)가 길고 자연어로 작성된 경우에도 여전히 강력한 검색 전략임을 시사합니다. RANKDOCBYSNPT는 RANK DOCDENSE와 RANKSNIPPET의 강점을 결합하여 다른 검색 기반 방법보다 일관되게 더 좋거나 동일하게 수행됩니다. 스니펫 수준에서 검색하면 단일 임베딩에 더 적은 정보를 인코딩하여 밀집 검색이 더 효과적입니다. 검색된 스니펫이 포함된 문서를 순위를 매기면 생성 모델이 콘텐츠 생성을 위해 더 다양하거나 자세한 정보를 추출할 수 있습니다. RANKDOCDENSE, RANKSNIPPET 및 RANKDOCBYSNPT는 모두 이 데이터 세트의 비교적 짧은 문서 길이로 인해 Reddit 데이터에서 좋은 성능을 보입니다. 요약. SUMCTXIND는 요약 단계가 없는 검색 증강 방법과 비슷한 성능을 보이는 반면, SUMCTX는 검색 증강 방법보다 성능이 뛰어납니다. 이는 일반적인 요약이 생성 모델에 추가 정보를 제공하지 않는다는 것을 나타냅니다. 요약은 즉각적인 맥락을 고려할 때 더욱 유용합니다. 이는 생성 모델이 표 4: Reddit 데이터 세트에서 전반적인 성과(%)를 생성할 때 가능한 주제나 구문을 통합하도록 안내합니다. *, †는 각각 USERID, RANKSNIPPET에 비해 0.01 수준에서 통계적으로 유의미한 개선을 나타냅니다. Reddit BLEU ROUGE-1 ROUGE-2 ROUGE-L 기준선 IMMEDCTX USERID LLMZEROSHOT 24.26.26.43.33.40.44.34.41.43.31.37. 검색 증강 방법 RANK DOCDENSE RECENTDOC 28.23* RANKDOCBM25 28.97* 44.34.41.45.71 1* 35.42.52* 28.82* 45.61* 35.22* 42.56* RANKSNIPPET RANKDOCBYSNPT 29.08* 45.94* 35.39* 42.68* 28.87* 45.67* 35.24* 42.54* +요약 SUMCTXIND SUMCTX 28.91* 45.71* 35.26* 42.57* 28.92* 46.09* 35.82** 43.14** +합성 SYNCTXIND SYNCTX 28.82* 45.91* 35.87** 29.19* 46.45** 43.09** 36.13** 43.27** AUTHOR PRED +멀티태스크 29.13* 47.11** 36.94** 43.95** XXX, XXX, XXX Li et al. 현재 문서. 맥락에 따른 요약이 없으면 생성 모델은 상위 수준 주제와 생성할 정확한 단어를 동시에 고려해야 하므로 작업이 더 어려워집니다. 합성. 우리는 요약과 유사한 패턴을 관찰합니다.SYNCTXIND는 합성이 없는 방법인 SUMCTX와 동등한 성능을 보이는 반면 SYNCTX는 SUMCTX보다 성능이 뛰어납니다.즉, 합성은 즉각적인 맥락에 따라 달라질 때만 유용합니다.또한 SYNCTX가 다른 지표보다 ROUGE-1 지표를 더 많이 개선하는 것을 볼 수 있습니다.이는 중요한 유니그램을 식별하여 합성 단계를 설계에서 선택했기 때문입니다.더 정교한 합성 방법은 ROUGE-1 이외의 지표를 개선하기 위한 향후 작업으로 탐색할 가치가 있습니다.멀티태스크.두 문서가 같은 작성자의 것인지 예측하는 방법을 공동으로 학습함으로써 AuthorPred는 단일 작업 설정을 가진 SYNCTX보다 Avocado 이메일 및 Reddit 데이터 세트에서 더 나은 성능을 보이며 Amazon 리뷰 데이터 세트에서는 유사한 성능을 보입니다.이는 다른 작성자의 글쓰기 스타일을 차별화하여 모델의 독해 능력을 개선하는 것이 종종 이로운 생성 성능에 해를 끼치지 않는다는 것을 나타냅니다. 7. 입력 및 출력 공식화 표 2, 3 및 4는 한 번에 하나의 새 구성 요소를 추가하는 성능을 보고하여 이미 절제 연구를 포함했으므로 입력 및 출력 공식을 변경하면 성능이 크게 변경되는지 추가로 조사합니다. 다음 설정을 연구합니다. • NORANKEDENTRIES. 요약과 합성된 결과가 사용 가능할 때 순위가 매겨진 항목이 여전히 필요한지 조사합니다. 이를 위해 직접 컨텍스트, 컨텍스트 종속 요약 및 컨텍스트 종속 합성을 사용하여 생성 모델을 학습합니다. • REMOVEDOCSTART. 직접 컨텍스트에는 현재 문서의 짧은 시작이 포함됩니다. 이전 모든 실험의 경우 이 짧은 시작은 생성 모델을 학습할 때 현재 문서인 기준 진실 레이블에 여전히 존재합니다. 기준 진실 레이블에서 짧은 시작을 제거한 효과를 연구합니다. 비교를 공정하게 하기 위해 메트릭을 계산할 때 모델에서 생성된 텍스트 앞에 짧은 시작을 추가합니다. • IMMEDCTXATEND. 위의 모든 실험에서 정보 출처는 직접적인 맥락, 요약, 종합, 순위가 매겨진 항목의 순서로 배치되었습니다. 직접적인 맥락을 마지막에 배치하여 순서가 중요한지 여부를 연구합니다. 표 5: AUTHORPRED에 대한 성과의 절대적 변화(%)를 기반으로 한 Avocado 이메일 데이터 세트의 입력 및 출력 공식화 분석. 다른 데이터 세트도 유사한 패턴을 보이므로 공간을 절약하기 위해 생략했습니다. 표 6: 모델 출력의 예. 기울임체 단어는 직접적인 맥락에서 사용됩니다. 실제 진실 문서: 저는 이 책이 재미없었습니다. 몇 주 전에 이 작가를 발견했고 그녀의 칵테일 시리즈를 읽고 있었지만 이 책은 저에게 좋지 않았습니다. 캐릭터가 특별히 마음에 들지 않았고(알겠습니다. 루카스는 해리 왕자처럼 생겼습니다) 플롯이 느리고 재미없었습니다. 저는 완독했지만, 제가 고집이 세서 그럴 뿐입니다. SUMCTX 출력: 저는 이 책이 재미없었습니다. 몇 주 전에 이 작가를 발견했고 그녀의 책을 읽고 있었습니다. 시리즈의 첫 번째 책은 정말 좋았지만 이 책은 제가 가장 좋아하는 책이 아니었습니다. 저는 캐릭터와 공감하지 못했습니다. 여주인공이 너무 순진했고, 저는 그저 주인공에게 빠지지 못했습니다. RANKDOCBYSNPT 출력: 저는 이 책을 즐기지 못했습니다. 저는 몇 주 전에 이 작가를 알게 되었고 그녀의 책을 읽어왔습니다. 저는 이 시리즈의 처음 두 권을 즐겼고 왜 이 책을 좋아하지 않았는지 잘 모르겠습니다. 제가 &quot;친구에서 연인으로&quot;라는 전반적인 것을 좋아하지 않기 때문인지는 모르겠지만, 저는 이 책이 쓰여진 방식이 마음에 들지 않았습니다. SUMCTX가 더 나은 이유: 요약 모델에서 제가 주인공에게 빠지지 않았다고 언급했기 때문에 생성 모델에서 캐릭터에 대해 더 많이 언급합니다. Ground-truth doc: 귀하의 통찰력에 감사드립니다. 저는 웹사이트, 모바일 애플리케이션, 데스크톱 애플리케이션을 만듭니다. 저는 odesk로 많은 성공을 거두었고 그 이후로 계속 사용하고 있습니다. 저는 제 클라이언트를 위해 하는 방식으로 제 자신의 것을 개발하는 것을 생각해 왔습니다. 저는 사이드 프로젝트와 정규직으로 너무 바빠요.SYNCTX 출력: 통찰력에 감사드립니다.저는 웹사이트, 모바일 애플리케이션, 데스크톱 애플리케이션을 다룹니다.저는 odesk로 많은 성공을 거두었고 그 이후로 계속 사용해 왔습니다.저는 많은 고객이 있고 오랫동안 사용해 왔습니다.저는 전문 개발자이고 오랫동안 이 일을 해 왔습니다.SUMCTX 출력: 통찰력에 감사드립니다.저는 웹사이트, 모바일 애플리케이션, 데스크톱 애플리케이션을 다룹니다.저는 odesk로 많은 성공을 거두었고 그 이후로 계속 사용해 왔습니다.저는 많은 프로젝트를 다룹니다.저는 php, javascript, html, css, javascript, html5, c#, javascript, c#에 대한 많은 경험이 있습니다.SYNCTX가 더 나은 이유: 합성 모델에서 개발자와 프리랜서를 언급하여 생성 모델이 개발과 고객이라는 주제에 집중할 수 있습니다.실제 진실 문서: 다른 리뷰어가 썼듯이 이 책은 부드럽게 흘러갑니다. 영어: 액션이 많지 않고 타임라인이 왔다 갔다 하지만 무슨 일이 일어나고 있는지 알아내는 것은 어렵지 않습니다. 하지만 공정하게 경고합니다. 이것은 슬픈 이야기입니다. 한두 번 목이 메었습니다. 매우 잘 쓰여졌지만 스토리는 정말 획기적이지 않습니다. 저자 출력: 다른 리뷰어가 썼듯이 이 책은 부드럽게 흐릅니다. 액션이 많지 않고 타임라인이 왔다 갔다 하지만 혼란스럽지 않습니다. 캐릭터는 잘 발달되었고 스토리는 흥미롭습니다. 결말이 약간 슬프다고 생각했지만, 제가 기대했던 것 같습니다. SYNCTX 출력: 다른 리뷰어가 썼듯이 이 책은 부드럽게 흐릅니다. 액션이 많지 않고 타임라인이 왔다 갔다 하지만 혼란스럽지 않습니다. 이 이야기는 어린 시절부터 친구였던 친구 그룹에 대한 것입니다. 그들은 모두 30대 후반이고 어린 시절부터 친구였습니다. AUTHORPRed가 더 나은 이유: 저자 구분 작업은 이 사용자가 종종 세부 정보로 이동하지 않고 스토리에 대한 높은 수준의 리뷰를 제공한다는 것을 모델이 이해하는 데 도움이 됩니다.Avocado 이메일 BLEU NORANKEDENTRIES -2.REMOVEDOCSTART -0.IMMEDCTXATEND -0.-2.-0.0.--2.0.ROUGE-1 ROUGE-2 ROUGE-L -2.-1.-0.공간 제한으로 인해 표 5에 Avocado 이메일 데이터에 대한 성능만 표시합니다.다른 데이터 세트도 비슷한 패턴을 보입니다.NORANKEDENTRIES는 AUTHORPRED보다 성능이 크게 낮습니다.즉, 생성 모델은 요약과 종합을 사용할 수 있는 경우에도 검색된 Teach LLMs to Personalize - An Approach inspired by Writing Education 항목에 정보(예: 단어 사용 및 글쓰기 스타일)를 계속 의존합니다.REMOVEDOCSTART의 성능이 저하됩니다. 우리는 입력과 실제 레이블에 모두 현재 문서의 짧은 시작이 있으면 모델이 작업을 더 잘 이해하는 데 도움이 된다고 생각합니다. 이는 실제 레이블에 짧은 시작이 포함되면 모델이 학습 중에 더 빨리 수렴한다는 관찰을 통해 뒷받침됩니다. IMMEDCTXATEND와 AUTHORPRED 간의 가까운 성능은 정보 소스를 재정렬해도 적어도 미세 조정의 경우 성능에 영향을 미치지 않는다는 것을 나타냅니다. 7.3 사례 연구 제안된 방법이 개인화된 콘텐츠를 생성하는 방법에 대한 더 나은 지침을 제공할 수 있는 이유를 더 잘 이해하기 위해 표 6에 몇 가지 설명적 예를 제공합니다.
--- CONCLUSION ---
우리는 개인화된 텍스트 생성을 위한 대규모 언어 모델을 가르치기 위한 일반적인 접근 방식을 제안합니다. 학생들이 일련의 단계로 소스에서 글을 쓰도록 지시받는 방식과 유사하게, 제안된 접근 방식은 검색, 순위 매기기, 요약, 종합, 생성이라는 여러 단계로 구성됩니다. 또한, 읽기와 쓰기 기술이 상관관계가 있다는 관찰에서 영감을 받아 주어진 문서 쌍의 저자를 구별하여 모델의 읽기 능력을 향상시키는 멀티태스크 설정을 만듭니다. 이 멀티태스크 설정은 모델의 개인화된 텍스트를 경험적으로 생성하는 능력을 더욱 향상시킵니다. 우리는 대표적인 도메인에서 공개적으로 발표된 세 가지 데이터 세트에 대해 모델을 평가합니다. 우리의 결과는 멀티스테이지 멀티태스크 프레임워크의 효과를 보여줍니다. 예를 들어, 제품 정보와 같은 세계 지식의 통합에 대한 조사는 미래 작업에 유망한 방향입니다. 참고문헌 XXX, XXX, XXX [1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 기술 보고서. arXiv 사전 인쇄본 arXiv:2305.10403 (2023). [2] James P. Callan. 1994. 문서 검색의 구절 수준 증거. SIGIR &#39;94에서, Bruce W. Croft와 CJ van Rijsbergen(편집자). Springer London, London, 302-310. [3] Adeline Cooney, Eamon Darcy, Denis Casey. 2018. 읽기와 쓰기 통합: 출처에서 학생들의 쓰기 지원. Journal of University Teaching &amp; Learning Practice 15, 5 (2018), 3. [4] Marion Crowhurst. 1990. Reading/writing relationships: An intervention study. Canadian Journal of Education/Revue canadienne de l&#39;éducation 15, 2 (1990), 155172. [5] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. [nd]. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. In International Conference on Learning Representations. [6] Christiane Fellbaum. 1998. WordNet: An electronic lexical database. MIT press. [7] Markus Freitag and Yaser Al-Onaizan. 2017. Beam Search Strategies for Neural Machine Translation. ACL 2017 (2017), 56. [8] Yong-Bing GAO, Jun-Tian GAO, Rong MA, and Li-Dong YANG. [nd]. 사용자 세분성 수준의 개인화된 소셜 텍스트 생성 기술에 대한 연구. Journal of Computer Applications ([nd]), 0. [9] Misha Khalman, Yao Zhao, and Mohammad Saleh. 2021. ForumSum: 다중 화자 대화 요약 데이터 세트. In Findings of the Association for Computational Linguistics: EMNLP 2021. 4592-4599. [10] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. 다중 레벨 메모리 네트워크를 사용한 Reddit 게시물의 추상적 요약. NAACL-HLT에서. [11] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani. 2021. GeDi: 생성적 판별기 유도 시퀀스 생성. 계산 언어학 협회의 연구 결과: EMNLP 2021. 4929-4952. [12] Juncen Li, Robin Jia, He He, Percy Liang. 2018. 삭제, 검색, 생성: 감정 및 스타일 전환에 대한 간단한 접근 방식. 계산 언어학 협회 북미 지부 회의록: 인간 언어 기술, 제1권(긴 논문). 1865-1874. [13] Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen. 2020. 캡슐 그래프 신경망을 사용한 지식 강화 개인화된 리뷰 생성. 제29회 ACM 국제정보지식경영학술대회 논문집. 735-744쪽. [14] Pan Li와 Alexander Tuzhilin. 2019. 제어 가능하고 개인화된 리뷰 생성을 향하여. 2019년 자연어 처리 경험적 방법에 관한 학술대회 논문집 및 제9회 국제자연어처리공동학술대회(EMNLP-IJCNLP) 논문집. 3237-3245쪽. [15] Chin-Yew Lin. 2004. Rouge: 요약 자동 평가 패키지. 텍스트 요약이 분기되다. 74-81쪽. 6706. [16] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi. 2021. DExperts: Experts 및 Anti-Experts를 사용한 디코딩-시간 제어 텍스트 생성. 제59회 계산언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문)에서 발췌. 6691[17] Pierre-Emmanuel Mazare, Samuel Humeau, Martin Raison, Antoine Bordes. 2018. 수백만 개의 개인화된 대화 에이전트 훈련. 2018년 자연어 처리 경험적 방법에 대한 컨퍼런스록에서 발췌. 2775-2779. [18] Sean Patrick McCarron 및 Victor Kuperman. 2021. 저자 인식 테스트는 영어 원어민과 비원어민에게 유용한 지표인가? 항목 응답 이론 분석. 행동 연구 방법 53, 5(2021), 2226-2237. [19] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, Yinfei Yang. 2022. Sentence-T5: 사전 학습된 텍스트-텍스트 모델의 확장 가능한 문장 인코더. 계산 언어학 협회의 연구 결과: ACL 2022. 1864-1874. [20] Jianmo Ni, Jiacheng Li, Julian McAuley. 2019. 멀리 레이블이 지정된 리뷰와 세분화된 측면을 사용하여 권장 사항 정당화. 2019년 자연어 처리의 경험적 방법에 관한 컨퍼런스와 자연어 처리에 관한 제9회 국제 공동 컨퍼런스(EMNLP-IJCNLP)의 회의록. 188-197. [21] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. 대형 듀얼 인코더는 일반화 가능한 검색기입니다. arXiv 사전 인쇄본 arXiv:2112.07899(2021). [22] Douglas Oard, William Webber, David Kirsch, Sergey Golitsynskiy. 2015. 아보카도 연구 이메일 수집. Philadelphia: Linguistic Data Consortium(2015). [23] Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. Bleu: 기계 번역의 자동 평가를 위한 방법. Association for Computational Linguistics의 40번째 연례 회의록. 311-318. XXX, XXX, XXX Li et al. [24] Jeffrey Pennington, Richard Socher, Christopher D Manning. 2014. Glove: 단어 표현을 위한 글로벌 벡터. 2014년 자연어 처리(EMNLP)의 경험적 방법에 관한 컨퍼런스의 회의록. 1532-1543. [25] Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Yutao Zhu, Zhanliang Liu, Zhicheng Dou, Ji-Rong Wen. 2021. Pchatbot: 개인화된 챗봇을 위한 대규모 데이터 세트. 정보 검색 연구 및 개발에 관한 제44회 국제 ACM SIGIR 컨퍼런스의 회의록. 24702477. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. 기계 학습 연구 저널 21, 1(2020), 5485-5551. [27] Sudha Rao 및 Joel Tetreault. 2018. 안녕하세요, GYAFC 데이터 세트를 소개해 드립니다: 형식 스타일 전이를 위한 코퍼스, 벤치마크 및 메트릭. 2018년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(장편 논문). 129-140쪽. [28] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford 외. 1995. TREC-3의 Okapi. Nist 특별 출판물 Sp 109(1995), 109. [29] Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani. 2023. LAMP: 대규모 언어 모델이 개인화를 충족하는 경우. arXiv 사전 인쇄본 arXiv:2304.11406(2023). [30] Abigail See, Peter J Liu, Christopher D Manning. 2017. 요점 파악: 포인터 생성기 네트워크를 사용한 요약. 제55회 연례 총회 의사록(제1권: 장편 논문). 1073-1083. [31] Timothy Shanahan. 2015. Common Core State Standards: A new role for writing. The Elementary School Journal 115, 4(2015), 464–479. [32] Noam Shazeer and Mitchell Stern. 2018. Adafactor: 비선형 메모리 비용이 있는 적응 학습 속도. 국제 기계 학습 컨퍼런스에서. PMLR, 4596-4604. [33] Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola. 2017. 교차 정렬을 통한 비병렬 텍스트에서의 스타일 전송. 신경 정보 처리 시스템의 발전 30(2017). [34] Khrystyna Skopyk, Artem Chernodub 및 Vipul Raheja. [nd]. 대규모 언어 모델 개인화. ([nd]). [35] Stuck_In_the_Matrix. 2015. Reddit 공개 댓글(2007-10~201505). (2015). https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_ publicly available_reddit_comment/ [36] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery 및 Denny Zhou. 2022. 자기 일관성은 언어 모델에서 사고의 사슬 추론을 개선합니다. arXiv 사전 인쇄본 arXiv:2203.11171(2022). [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou 외. 2022. Chain-of-thought prompting은 대규모 언어 모델에서 추론을 이끌어냅니다. 신경 정보 처리 시스템의 발전(2022), 24824-24837. [38] Yuwei Wu, Xuezhe Ma, Diyi Yang. 2021. 생성적 분할 메모리 네트워크를 통한 개인화된 응답 생성. 2021년 북미 계산 언어학 협회 회의록: 인간 언어 기술. 1956-1970. [39] Kevin Yang 및 Dan Klein. 2021. FUDGE: 미래의 판별기를 사용한 제어된 텍스트 생성. 2021년 북미 계산언어학회 학술대회 논문집: 인간언어기술. 3511-3535쪽. [40] Chenhan Yuan과 Yi-chin Huang. 2020. 저자별 단어 사용을 통한 생성적 적대 네트워크를 사용한 개인화된 문장 생성. Computación y Sistemas 24, 1(2020), 17-28쪽. [41] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston. 2018. 대화 에이전트 개인화: 저는 개가 있는데, 당신도 반려동물이 있나요?. 계산언어학회 제56회 연례 회의 논문집(제1권: 장문 논문). 2204-2213쪽. [42] Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming Zhou, Enhong Chen. 2018. 무감독 기계 번역으로서의 스타일 전송. arXiv 사전 인쇄본 arXiv:1808.07894(2018). [43] Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, Ji-Rong Wen. 2022. Less is More: 개인화된 대화 생성을 위한 대화 기록 정제 방법 학습. 2022년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술. 5808-5820.
