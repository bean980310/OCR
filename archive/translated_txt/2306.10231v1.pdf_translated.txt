--- ABSTRACT ---
메모리 증강은 외부 정보를 언어 모델에 효율적으로 통합하기 위한 강력한 접근 방식이지만, 텍스트를 검색하는 것에 비해 성능이 저하됩니다. 최근 연구에서는 메모리를 부분적으로 미리 계산하고 더 작은 라이브 인코더로 메모리 표현을 즉석에서 업데이트하는 메모리-검색 하이브리드인 LUMEN을 소개했습니다. 이를 개선한 우리는 1) 메모리 위에 얕은 리랭커를 적용하여 강력한 메모리 표현에 대한 무료 액세스를 활용하여 저렴한 비용으로 검색 품질을 크게 개선하고, 2) 멀티태스크 훈련을 통합하여 일반적이고 더 높은 품질의 메모리와 라이브 인코더를 학습하는 GLIMMER 접근 방식을 제안합니다. GLIMMER는 지식 집약적 작업의 KILT 벤치마크에서 LUMEN 및 FiD에 비해 더 빠른 속도로 성능이 크게 향상됩니다. 1
--- INTRODUCTION ---
검색 증강 언어 모델은 강력한 성능을 달성하지만 검색된 구절을 처리해야 하기 때문에 계산 비용이 많이 듭니다. 많은 연구에서 조건부 계산(Ainslie et al., 2023b; Varshney et al., 2022; Schuster et al., 2022), 재순위 지정(Wang et al., 2018; Yu et al., 2022; Wang et al., 2018) 또는 메모리(de Jong et al., 2022b; Wu et al., 2022a; Li et al., 2022)를 통해 검색된 구절을 읽는 비용을 줄이려고 시도합니다. 재순위 지정은 검색 품질을 개선하여 독자가 처리해야 하는 구절 수를 줄입니다. 그러나 신경망 재순위 지정은 검색된 각 후보가 신경망에 의해 처리되므로 비용이 많이 듭니다. 영어: Late interaction rerankers(Khattab 및 Zaharia, 2020; Cohen 등, 2022; MacAvaney 등, 2020)는 중간 토큰 표현을 미리 계산하고 더 작은 *동등한 기여.* University of Southern California. Google Research에서 수행한 작업. 즉석에서 신경 모델을 사용하여 쿼리 및 문서 표현을 결합하고 순위 점수를 생성합니다. Late interaction은 스토리지 및 사전 계산 오버헤드와 기계를 희생하여 속도를 크게 향상시킵니다. 최근 Late-interaction이라는 아이디어는 검색 증강 생성에도 적용되었습니다. LUMEN(de Jong 등, 2023)은 메모리와 검색 증강 사이를 보간하여 더 나은 품질-계산 트레이드오프를 달성합니다. 우리는 이러한 작업 라인을 결합하여 순위 지정과 메모리를 단일 엔드투엔드 모델로 통합하는 Late interaction 접근 방식인 GLIMMER(Generalized LateInteraction Memory Reranker)를 제안합니다. LUMEN과 마찬가지로 GLIMMER는 검색 문서에 대한 사전 계산된 토큰 표현을 생성하는 메모리 인코더와 검색된 문서의 표현을 쿼리와 결합하는 라이브 인코더로 구성됩니다. 라이브 인코더의 첫 번째 레이어 다음에 순위 레이어가 추가 처리를 위해 보관되는 가장 관련성 있는 구절을 선택합니다. 이 모델은 Perplexity Distillation Auxiliary Loss(Izacard et al., 2022)를 통해 독자에게 유용한 정도에 따라 구절을 순위를 매기도록 훈련됩니다. GLIMMER는 또한 모든 작업에 대해 단일 일반 메모리 및 라이브 인코더를 사용하고 지식 집약적 데이터 세트에 대한 다중 작업 미세 조정으로 훈련하여 LUMEN을 개선합니다. 지식 집약적 작업의 KILT 벤치마크에서 평가합니다(Petroni et al., 2020). 먼저 메모리 및 라이브 인코더의 다중 작업 훈련이 단일 작업에 대한 훈련에 비해 모델 품질을 크게 향상시킨다는 것을 발견했으며, 특히 라이브 인코더에 더 적은 용량을 할당할 때 그렇습니다. 또한 GLIMMER는 멀티태스크 학습된 LUMEN과 FiD보다 품질과 속도 면에서 크게 향상되었습니다. 일반적으로 GLIMMER는 순위 재지정과 메모리를 단일의 효율적이고 고품질의 모델로 성공적으로 통합했습니다. 추론 전에 사전 계산됨 구절 1 메모리 인코더 구절 메모리 인코더 메모리 인코더 구절 N 메모리 인코더 concat L concat concat 상위 K 득점 구절 유지 Concat concat Live-A S Live-A Live-A S Live-A Live-B 디코더 Live-B 질문 메모리 인코더 H 그림 1: GLIMMER 아키텍처 개요. 메모리: 메모리 인코더는 LUMEN과 달리 멀티태스크 학습 중에 업데이트된 후 코퍼스에 적용되어 부분적으로 사전 계산된 메모리 표현을 생성합니다. 메모리 인코더는 추론 중에도 적용되어 메모리와 호환되는 부분적인 질문 표현을 생성합니다. 라이브: 각 구절 메모리는 질문 표현과 연결되고, 라이브 인코더(전체 모델의 비율 a)가 두 단계로 입력에 대한 구절을 조건화하는 데 적용됩니다. 라이브 레이어의 일부인 ẞ로 구성된 첫 번째 단계 후, 채점 레이어는 유지할 높은 점수의 관련 구절의 작은 하위 집합을 선택하고 덜 관련성 있는 구절은 삭제합니다. 선택된 구절 표현은 라이브 인코더의 두 번째 단계에서 업데이트됩니다. 마지막으로, 조건화된 표현은 FiD에서와 같이 디코더에 의해 연결되고 처리됩니다. 2 배경 우리는 품질과 추론 계산 간에 최상의 균형을 이루는 데 관심이 있습니다. 다음 섹션에서는 GLIMMER의 기반이 되는 기준 방법인 FiD와 LUMEN 및 이들의 계산적 속성을 설명합니다. 이러한 방법에 대한 보다 심층적인 분석은 de Jong et al. (2023)에서 찾을 수 있습니다. 2.1 퓨전-인-디코더 퓨전-인-디코더(Izacard and Grave, 2021)는 T5 인코더-디코더 모델(Raffel et al., 2020)을 기반으로 합니다. 각 입력에 대해 여러 관련 텍스트 구절이 검색되고 입력이 각 구절 앞에 추가됩니다. 결과 입력-구절 쌍은 인코더에서 별도로 인코딩되고 인코딩된 쌍은 토큰 표현의 플랫 시퀀스로 연결되어 디코더에서 처리되어 대상 출력을 생성합니다. 각 모델에서 라이브 구성 요소는 파란색으로 표시되고 추론 전에 미리 계산된 구성 요소는 주황색으로 표시됩니다. 계산)은 FFiD == .knp · L · 14d² .+nt · L · 14d² 인코더 및 교차 어텐션 디코더는 피드포워드 계층에서 토큰당 8d², 셀프 어텐션 투영 계층에서 4d², 교차 어텐션 투영 계층에서 2d²의 요소를 갖습니다.de Jong et al. (2023)에는 FiD 모델 복잡도의 파생 내용이 더 자세히 나와 있습니다. 2.2 LUMEN 일반적으로 검색된 구절의 결합 길이는 대상 길이보다 훨씬 길어서 대부분의 FLOP이 검색된 구절을 처리하는 인코더에 의해 소모됩니다. LUMEN은 검색된 구절에 대한 인코더 표현을 부분적으로 미리 계산하여 인코더 추론 비용을 줄입니다. 추론 시점에 LUMEN은 텍스트가 아닌 중간 계층 표현을 검색합니다. 보다 정확하게 말해서 LUMEN은 사전 학습된 T5 인코더-디코더 모델에서 초기화됩니다. 디코더 G = Dec Enc(Q; Passage₁); ... Enc(Q; Passage)]는 표준 FiD 디코더와 동일한 기능을 하지만, k를 구절 수, np를 구절당 토큰 수, nt를 대상 토큰 수, L을 계층 수, d를 모델의 차원이라고 합니다. de Jong 등의 분석에 따르면, (2022a, 2023), FiD의 단일 추론 샘플에 대한 FLOP(주의 점수 무시) T5 인코더는 첫 번째 1- 레이어 비율을 포함하는 대용량 메모리 인코더와 나머지 레이어 비율을 포함하는 더 작은 라이브 인코더로 나뉩니다. 메모리 인코더는 오프라인으로 코퍼스의 구절에 적용되어 메모리 표현을 미리 계산한 다음, 나중에 미세 조정된 라이브 인코더에서 입력 및 작업에 따라 업데이트됩니다. 메모리 표현과 입력이 호환되는지 확인하기 위해 LUMEN은 메모리 표현에 질문 표현을 추가하기 전에 메모리 인코더¹를 입력에 적용합니다. H; = [MemEnc(Q); MemEnc(Passage, G = Dec [Q; LiveEnc (H1); … … . LiveEnc(Hk) = a = 1을 선택하면 FiD에 매우 가까운 모델이 생성되고 0은 전체 메모리 모델입니다. 추론 중에 LUMEN은 레이어의 일부 a만 적용하여 일부 a 주어진 모델 크기에 대한 FiD 리더 FLOPS.FLUMEN = knp aL.12d².인코더 + knp · L · 2d² +nt · 3 GLIMMER.교차 어텐션 L.14d² 디코더 GLIMMER는 두 가지 주요 차이점을 두고 LUMEN을 기반으로 합니다.GLIMMER는 내장된 재순위 매기기 기능을 통합하고 많은 작업에서 메모리와 라이브 인코더를 공유합니다.표준 재순위 매기기 방식은 상충 관계에 어려움을 겪습니다.작은 모델은 구절이 입력과 관련이 있는지 여부를 판단하기에 충분히 강력하지 않을 수 있으며, 큰 모델의 비용은 처음부터 재순위 매기기 기능을 사용하는 목적의 큰 부분을 무너뜨립니다.LUMEN 아키텍처는 대부분의 구절 표현이 미리 계산되므로 이러한 상충 관계를 피할 수 있는 기회를 제공합니다.GLIMMER는 라이브 인코더의 초기 계층을 재순위 매기기에 재사용하여 비교적 적당한 계산 비용으로 강력한 재순위 매기기 모델을 생성합니다. 한편, 작업 간에 가중치를 공유하면 중복된 사전 계산된 표현을 저장하지 않고도 메모리 인코더를 학습할 수 있으며 라이브 인코더의 효과가 크게 높아집니다.그림 1은 GLIMMER 아키텍처의 개요를 보여줍니다.3.1 아키텍처 LUMEN과 비교하여 GLIMMER는 라이브 인코더를 두 개의 구성 요소로 나눕니다.첫 번째 구성 요소는 초기 상호 작용 및 재순위 지정을 담당하고 두 번째 구성 요소는 선택된 구절의 표현에 대한 추가 처리를 수행합니다.첫 번째 구성 요소에는 라이브 인코더 계층의 ẞ 비율이 포함되고 나머지 계층은 두 번째 구성 요소에 있습니다.첫 번째 라이브 인코더 다음에 선형 투영 계층이 각 입력-구절 쌍의 첫 번째 토큰에 적용되어 구절에 대한 관련성 점수가 생성됩니다.원래 k 중에서 가장 높은 점수를 받은 상위 m개의 구절은 두 번째 라이브 인코더에서 처리되고 다른 구절은 삭제됩니다.두 번째 라이브 인코더의 출력은 FiD 및 LUMEN에서와 같이 디코더에 공급됩니다. H₁ = [MemEnc(Q); MemEnc (Passage;)] H = LiveEncA(Hi) R; = H½{ st Rank [Score(H{)] = j G = Dec [Q; LiveEncB (R1); ... LiveEncB (Rm)] 3.2 학습 메모리 인코더, 두 라이브 인코더 구성 요소, 스코어링 프로젝션 및 디코더는 모두 종단 간 학습됩니다. LUMEN과 달리 메모리 인코더는 모든 작업에서 단일 메모리 인코더를 공유하므로 동결할 필요가 없습니다. 스코어링 프로젝션을 학습하고 메모리와 첫 번째 라이브 인코더가 재순위 지정에 적합한 표현을 생성하도록 장려하기 위해 보조 퍼플렉시티 증류 손실(Izacard et al., 2022)을 사용합니다. 이 손실은 해당 입력 구절이 디코더에 직접 공급된 경우 최종 세대의 퍼플렉시티를 얼마나 낮추는지에 따라 구절을 순위 지정하도록 모델에 장려합니다. 특히, 퍼플렉시티 증류는 입력과 구절 표현의 연결에 적용된 첫 번째 라이브 인코더 구성 요소의 출력에서 계산된 재순위 점수에 의해 암시되는 분포와 결과 퍼플렉시티에 의해 암시되는 분포 간의 KL-발산을 최소화합니다.LM Pk rank Pk exp(Score(Passagek, Q)/T) Σ exp(Score (Passage,;, Q)/T) exp(log PLM (Answer|Passagek, Q)/T) Σexp(log PLM (Answer|Passage;, Q)/T) Lpdist = KL(prank, pLM) 3.3 계산 분석 GLIMMER와 LUMEN의 계산 복잡도 차이는 재순위에 있습니다. m개의 선택된 구절은 전체 라이브 인코더에 의해 처리된 다음 디코더를 통해 공급되어 FiD LUMEN GLIMMER FiD 6870 71PerformanceLUMEN PerformanceGLIMMERLUMENFiD0.0.0.0.0.0.0.0.TFLOP당 샘플 TFLOP당 샘플 GLIMMER 그림 2: GLIMMER는 LUMEN보다 빠르고 품질이 더 높으며 LUMEN은 FiD보다 빠르고 품질이 더 높습니다. KILT 개발 세트 및 추론 속도에서 GLIMMER, LUMEN 및 FiD XXL 모델의 평균 성능 비교. FiD는 검색된 구절 5개를 사용하고 LUMEN은 검색된 구절 10개를 사용하고 GLIMMER는 검색된 구절 25개를 사용하여 최종 구절 5개로 다시 순위를 매겼습니다. LUMEN 및 GLIMMER는 라이브 비율 a = m개의 구절(검색된 구절의 전체 수 k보다 적음)로 LUMEN을 적용하는 것과 동일한 계산 비용을 갖습니다. 그러나 선택되지 않은 구절의 경우 GLIMMER는 여전히 첫 번째 라이브 인코더 구성 요소를 적용하여 재순위 지정 비용을 발생시킵니다. FGLIMMER = FM LUMEN +(km)np BaL · 12d² 재순위 지정 선택된 구절 m&lt;를 사용하는 경우
--- RELATED WORK ---
검색 증강(Izacard and Grave, 2021; Borgeaud et al., 2022; Lewis et al., 2020; Khandelwal et al., 2020; Guu et al., 2020)은 추가 컨텍스트로 입력을 증강하여 언어 모델 성능을 개선하는 강력한 기술입니다. 저희의 연구는 검색 증강 언어 모델의 품질-컴퓨팅 트레이드오프를 개선하는 데 중점을 두고 있습니다. 이는 세 가지 연구 분야, 즉 늦은 상호작용 메모리, 늦은 상호작용 재순위 지정, 검색 학습을 통합하여 수행합니다. 저희의 접근 방식은 가장 일반적인 검색 증강 모델 중 하나인 Fusion-in-Decoder(Izacard and Grave, 2021)의 아키텍처 골격을 사용합니다. 저희는 Hofstätter et al.(2022)에서와 같이 KILT(Petroni et al., 2020)에 대한 멀티태스크 학습을 사용합니다. 메모리 검색 증강은 언어 모델에서 처리해야 하는 추가적인 맥락 때문에 비용이 많이 듭니다. TOME(de Jong et al., 2022b), Memorizing Transformer(Wu et al., 2022a) 및 기타 여러 모델(Li et al., 2022; Zhong et al., 2022; Chen et al., 2022; Wu et al., 2022b; Yogatama et al., 2021; Bertsch et al., 2023)과 같은 메모리 모델은 표현을 미리 계산하여 메모리에 저장함으로써 이러한 비용을 피하려고 시도하여 표현을 즉석에서 처리하는 대신 직접 검색할 수 있습니다. 그러나 이러한 접근 방식은 메모리 표현이 각 개별 입력에 따라 조건이 지정되지 않기 때문에 품질이 저하됩니다(Li et al., 2022; de Jong et al., 2023). 후기 상호작용 메모리(de Jong et al., 2023; Milbauer et al., 2023)는 검색 표현을 부분적으로만 미리 계산하고 메모리와 입력 간의 일부 상호작용을 즉석에서 수행함으로써 메모리 접근 방식의 품질을 개선합니다.특히, 저희의 작업은 LUMEN(de Jong et al., 2023)에 매우 밀접하게 기반합니다.재순위 언어 모델 자체와 마찬가지로 검색 절차는 완전한 상호작용이 있는 값비싼 온라인 순위(Chen et al., 2020)와 DPR(Karpukhin et al., 2020) 및 GTR(Ni et al., 2021)과 같은 보다 일반적인 듀얼 인코더 접근 방식 사이에서 균형을 이룹니다.이는 미리 계산된 구절 표현의 코퍼스와의 내적 유사성에 따라 점수를 매깁니다. 종종 검색을 위한 다양한 모델이 파이프라인 방식으로 적용되며, 초기에 저렴한 스코어링 모델이 적용된 다음 더 강력하고 비싼 리랭커가 적용됩니다(Mao et al., 2021; Wang et al., 2018; Yu et al., 2022). 많은 리랭커는 또한 COLBERT(Khattab and Zaharia, 2020; Santhanam et al., 2022), PreTTR(MacAvaney et al., 2020), SDR(Cohen et al., 2022) 및 Poly-encoders(Humeau et al., 2020)와 같이 순위 품질과 속도 사이에서 좋은 균형을 얻기 위해 늦은 상호 작용을 활용합니다. GLIMMER는 늦은 상호 작용 메모리와 리랭킹을 단일 모델로 결합하여 두 사용 사례에 대해 미리 계산된 표현을 공유합니다. 검색 학습 검색 모델은 종종 지도 학습 데이터(Karpukhin 등, 2020; Ni 등, 2021)로 훈련되며, MS-MARCO(Nguyen 등, 2016) 또는 TREC CAR(Dietz 등, 2018)과 같은 데이터 세트의 골드 검색을 사용합니다. 검색 증강 생성에 사용할 구절을 선택할 때, 우리는 독자 모델에 가장 도움이 되는 구절이라는 추가 신호를 갖게 됩니다. 여러 기존 연구에서 이 신호를 사용하여 검색을 개선합니다(Guu 등, 2020; Sachan 등, 2021; Jiang 등, 2022; Sachan 등, 2021; Izacard 등, 2022). 우리는 ATLAS(Izacard et al., 2022)를 따르고, 퍼플렉시티 증류를 사용하여 독자 모델 퍼플렉시티를 낮추는 데 도움이 되는 구절을 선택하도록 재순위 매기는 사람을 훈련합니다.6 결론 검색 증강 언어 모델은 강력하지만 추론이 느린 반면, 사전 계산된 메모리 증강 모델은 품질이 떨어지고 빠릅니다.LUMEN과 같은 하이브리드 후기 상호 작용 모델은 좋은 품질-계산 트레이드오프를 나타냅니다.우리는 더 나은 트레이드오프를 달성하기 위해 학습된 엔드투엔드 재순위 매기기와 멀티태스크 훈련을 통합하는 개선된 후기 상호 작용 모델인 GLIMMER를 소개합니다.GLIMMER는 지식 집약적 작업의 KILT 벤치마크에서 LUMEN 및 FiD에 비해 더 빠른 속도로 품질이 크게 향상됩니다.감사의 말 Google Research의 Luke Vilnis, Tania Bedrax-Weiss 및 기타 분들께 통찰력 있는 의견과 토론에 감사드립니다. 참고문헌 Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai. 2023a. GQA: 다중 헤드 체크포인트에서 일반화된 다중 쿼리 변환기 모델 학습. CoRR, abs/2305.13245. Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, Sumit Sanghai. 2023b. Colt5: 조건부 계산을 사용한 더 빠른 장거리 변환기. CoRR, abs/2303.09752. Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley. 2023. Unlimiformer: 무제한 길이 입력을 가진 장거리 변환기. CORR, abs/2305.01625. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre. 2022. 수조 개의 토큰에서 검색하여 언어 모델 개선. International Conference on Machine Learning, ICML 2022, 2022년 7월 17-23일, 미국 메릴랜드주 볼티모어, Proceedings of Machine Learning Research의 162권, 2206-2240쪽. PMLR. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, John Jumper. 2023. 추측적 샘플링을 통한 대규모 언어 모델 디코딩 가속화. CoRR, abs/2302.01318. Dongmei Chen, Sheng Zhang, Xin Zhang, Kaijing Yang. 2020. 정렬 증강 다국어 BERT를 통한 교차 언어 구절 재순위 지정. IEEE Access, 8:213232-213243. Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William W. Cohen. 2022. 도메인 질문 답변을 위한 qa-메모리를 사용한 사전 학습된 언어 모델 증강. CoRR, abs/2204.04581. openNachshon Cohen, Amit Portnoy, Besnik Fetahu, and Amir Ingber. 2022. SDR: 간결한 문서 표현을 사용한 효율적인 신경망 재순위 지정. 60th Annual Meeting of the Association for Computational Linguistics(제1권: 장문 논문)의 회의록, ACL 2022, 더블린, 아일랜드, 2022년 5월 22-27일, 6624-6637페이지. Association for Computational Linguistics. Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. 2022a. FiDO: 더 강력한 성능과 더 빠른 추론을 위해 최적화된 퓨전-인-디코더. arXiv 사전 인쇄본 arXiv:2212.08153. Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, and William W. Cohen. 2023. 사전 계산된 메모리 또는 즉석 인코딩? 검색 증강에 대한 하이브리드 접근 방식은 컴퓨팅을 최대한 활용합니다. CORR, abs/2301.10448. Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W. Cohen. 2022b. 언급 메모리: 엔티티 언급 주의를 통해 텍스트 지식을 변환기에 통합. 제10회 국제 학습 표현 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net. Laura Dietz, Ben Gamari, Jeff Dalton, Nick Craswell. 2018. TREC 복잡한 답변 검색 개요. 2018년 11월 14-16일 미국 메릴랜드주 게이더스버그에서 열린 제27회 텍스트 검색 컨퍼런스의 회의록, TREC 2018, NIST 특별 간행물 500-331권. 미국 표준 기술 연구소(NIST). Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, Elena Simperl. 2018. T-rex: 지식 기반 트리플을 사용한 자연어의 대규모 정렬. 2018년 5월 7-12일 일본 미야자키에서 열린 제11회 국제 언어 자원 및 평가 컨퍼런스 논문집(LREC 2018)에서 발췌. 유럽 언어 자원 협회(ELRA). Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. 2020. REALM: 검색 증강 언어 모델 사전 학습. CoRR, abs/2002.08909. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, Marc van Zee. 2020. Flax: JAX를 위한 신경망 라이브러리 및 생태계. Sebastian Hofstätter, Jiecao Chen, Karthik Raman, Hamed Zamani. 2022. 관련성 샘플링을 통한 다중 작업 검색 증강 텍스트 생성. CORR, abs/2207.03030. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston. 2020. Poly-encoders: 빠르고 정확한 다중 문장 채점을 위한 아키텍처 및 사전 학습 전략. 8th International Conference on Learning Representations, ICLR 2020, 아디스아바바, 에티오피아, 2020년 4월 26-30일. OpenReview.net. doGautier Izacard and Edouard Grave. 2021. 생성 모델을 사용한 구절 검색을 활용하여 개방형 주요 질문 답변. Association for Computational Linguistics의 유럽 지부 16차 회의록: Main Volume, EACL 2021, 온라인, 2021년 4월 19-23일, 874880페이지. Association for Computational Linguistics. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, Edouard Grave. 2022. 검색 증강 언어 모델을 사용한 Few-shot 학습. CoRR, abs/2208.03299. Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, Graham Neubig. 2022. 주의로서의 검색: 단일 변환기 내에서 검색 및 읽기의 종단 간 학습. 2022년 경험적 학습 컨퍼런스 회의록에서
--- METHOD ---
GLIMMER가 구축된 s와 해당 계산 속성. 이러한 방법에 대한 보다 심층적인 분석은 de Jong et al.(2023)에서 찾을 수 있습니다. 2.1 Fusion-in-Decoder Fusion-in-Decoder(Izacard and Grave, 2021)는 T5 인코더-디코더 모델(Raffel et al., 2020)을 기반으로 합니다. 각 입력에 대해 여러 관련 텍스트 구절이 검색되고 입력이 각 구절 앞에 추가됩니다. 결과 입력-구절 쌍은 인코더에서 별도로 인코딩되고 인코딩된 쌍은 토큰 표현의 플랫 시퀀스로 연결된 다음 디코더에서 처리되어 대상 출력을 생성합니다. 각 모델에서 라이브 구성 요소는 파란색으로 표시되고 추론 전에 미리 계산된 구성 요소는 주황색으로 표시됩니다. 계산)은 FFiD == .knp · L · 14d² 로 제공됩니다. +nt · L · 14d² 인코더 및 교차 어텐션 디코더는 피드포워드 계층에서 토큰당 8d², 셀프 어텐션 투영 계층에서 4d², 교차 어텐션 투영 계층에서 2d²의 요소를 갖습니다. de Jong et al. (2023)은 FiD 모델 복잡도의 도출을 더 자세히 포함하고 있습니다. 2.2 LUMEN 일반적으로 검색된 구절의 결합된 길이는 대상 길이보다 훨씬 길어서 대부분의 FLOP은 검색된 구절을 처리하는 인코더에 의해 소모됩니다. LUMEN은 검색된 구절에 대한 인코더 표현을 부분적으로 미리 계산하여 인코더 추론 비용을 줄입니다. 추론 시간에 LUMEN은 텍스트가 아닌 중간 계층 표현을 검색합니다. 보다 정확하게는 LUMEN은 사전 학습된 T5 인코더-디코더 모델에서 초기화됩니다. 디코더 G = Dec Enc(Q; Passage₁); ... Enc(Q; Passage)]는 표준 FiD 디코더와 동일한 기능을 하지만 k를 구절 수, np를 구절당 토큰 수, nt를 대상 토큰 수, L을 계층 수, d를 모델의 차원이라고 합니다. de Jong 등의 분석에 따르면, (2022a, 2023), FiD의 단일 추론 샘플에 대한 FLOP(주의 점수 무시) T5 인코더는 첫 번째 1- 레이어 비율을 포함하는 대용량 메모리 인코더와 나머지 레이어 비율을 포함하는 더 작은 라이브 인코더로 나뉩니다. 메모리 인코더는 오프라인으로 코퍼스의 구절에 적용되어 메모리 표현을 미리 계산한 다음, 나중에 미세 조정된 라이브 인코더에서 입력 및 작업에 따라 업데이트됩니다. 메모리 표현과 입력이 호환되는지 확인하기 위해 LUMEN은 메모리 표현에 질문 표현을 추가하기 전에 메모리 인코더¹를 입력에 적용합니다. H; = [MemEnc(Q); MemEnc(Passage, G = Dec [Q; LiveEnc (H1); … … . LiveEnc(Hk) = a = 1을 선택하면 FiD에 매우 가까운 모델이 생성되고 0은 전체 메모리 모델입니다. 추론 중에 LUMEN은 레이어의 일부 a만 적용하여 일부 a 주어진 모델 크기에 대한 FiD 리더 FLOPS.FLUMEN = knp aL.12d².인코더 + knp · L · 2d² +nt · 3 GLIMMER.교차 어텐션 L.14d² 디코더 GLIMMER는 두 가지 주요 차이점을 두고 LUMEN을 기반으로 합니다.GLIMMER는 내장된 재순위 매기기 기능을 통합하고 많은 작업에서 메모리와 라이브 인코더를 공유합니다.표준 재순위 매기기 방식은 상충 관계에 어려움을 겪습니다.작은 모델은 구절이 입력과 관련이 있는지 여부를 판단하기에 충분히 강력하지 않을 수 있으며, 큰 모델의 비용은 처음부터 재순위 매기기 기능을 사용하는 목적의 큰 부분을 무너뜨립니다.LUMEN 아키텍처는 대부분의 구절 표현이 미리 계산되므로 이러한 상충 관계를 피할 수 있는 기회를 제공합니다.GLIMMER는 라이브 인코더의 초기 계층을 재순위 매기기에 재사용하여 비교적 적당한 계산 비용으로 강력한 재순위 매기기 모델을 생성합니다. 한편, 작업 간에 가중치를 공유하면 중복된 사전 계산된 표현을 저장하지 않고도 메모리 인코더를 학습할 수 있으며 라이브 인코더의 효과가 크게 높아집니다.그림 1은 GLIMMER 아키텍처의 개요를 보여줍니다.3.1 아키텍처 LUMEN과 비교하여 GLIMMER는 라이브 인코더를 두 개의 구성 요소로 나눕니다.첫 번째 구성 요소는 초기 상호 작용 및 재순위 지정을 담당하고 두 번째 구성 요소는 선택된 구절의 표현에 대한 추가 처리를 수행합니다.첫 번째 구성 요소에는 라이브 인코더 계층의 ẞ 비율이 포함되고 나머지 계층은 두 번째 구성 요소에 있습니다.첫 번째 라이브 인코더 다음에 선형 투영 계층이 각 입력-구절 쌍의 첫 번째 토큰에 적용되어 구절에 대한 관련성 점수가 생성됩니다.원래 k 중에서 가장 높은 점수를 받은 상위 m개의 구절은 두 번째 라이브 인코더에서 처리되고 다른 구절은 삭제됩니다.두 번째 라이브 인코더의 출력은 FiD 및 LUMEN에서와 같이 디코더에 공급됩니다. H₁ = [MemEnc(Q); MemEnc (Passage;)] H = LiveEncA(Hi) R; = H½{ st Rank [Score(H{)] = j G = Dec [Q; LiveEncB (R1); ... LiveEncB (Rm)] 3.2 학습 메모리 인코더, 두 라이브 인코더 구성 요소, 스코어링 프로젝션 및 디코더는 모두 종단 간 학습됩니다. LUMEN과 달리 메모리 인코더는 모든 작업에서 단일 메모리 인코더를 공유하므로 동결할 필요가 없습니다. 스코어링 프로젝션을 학습하고 메모리와 첫 번째 라이브 인코더가 재순위 지정에 적합한 표현을 생성하도록 장려하기 위해 보조 퍼플렉시티 증류 손실(Izacard et al., 2022)을 사용합니다. 이 손실은 해당 입력 구절이 디코더에 직접 공급된 경우 최종 세대의 퍼플렉시티를 얼마나 낮추는지에 따라 구절을 순위 지정하도록 모델에 장려합니다. 특히, 퍼플렉시티 증류는 입력과 구절 표현의 연결에 적용된 첫 번째 라이브 인코더 구성 요소의 출력에서 계산된 재순위 점수에 의해 암시되는 분포와 결과 퍼플렉시티에 의해 암시되는 분포 간의 KL-발산을 최소화합니다.LM Pk rank Pk exp(Score(Passagek, Q)/T) Σ exp(Score (Passage,;, Q)/T) exp(log PLM (Answer|Passagek, Q)/T) Σexp(log PLM (Answer|Passage;, Q)/T) Lpdist = KL(prank, pLM) 3.3 계산 분석 GLIMMER와 LUMEN의 계산 복잡도 차이는 재순위에 있습니다. m개의 선택된 구절은 전체 라이브 인코더에 의해 처리된 다음 디코더를 통해 공급되어 FiD LUMEN GLIMMER FiD 6870 71PerformanceLUMEN PerformanceGLIMMERLUMENFiD0.0.0.0.0.0.0.0.TFLOP당 샘플 TFLOP당 샘플 GLIMMER 그림 2: GLIMMER는 LUMEN보다 빠르고 품질이 더 높으며 LUMEN은 FiD보다 빠르고 품질이 더 높습니다. KILT 개발 세트 및 추론 속도에서 GLIMMER, LUMEN 및 FiD XXL 모델의 평균 성능 비교. FiD는 검색된 구절 5개를 사용하고 LUMEN은 검색된 구절 10개를 사용하고 GLIMMER는 검색된 구절 25개를 사용하여 최종 구절 5개로 다시 순위를 매겼습니다. LUMEN 및 GLIMMER는 라이브 비율 a = m개의 구절(검색된 구절의 전체 수 k보다 적음)로 LUMEN을 적용하는 것과 동일한 계산 비용을 갖습니다. 그러나 선택되지 않은 구절의 경우 GLIMMER는 여전히 첫 번째 라이브 인코더 구성 요소를 적용하여 재순위 지정 비용을 발생시킵니다. FGLIMMER = FM LUMEN +(km)np BaL · 12d² 재순위 지정 선택된 구절 m&lt;를 사용하는 경우
--- EXPERIMENT ---
s 4.1 실험 설정 모델 구성 GLIMMER는 LUMEN과 마찬가지로 T5.1.1 아키텍처(Raffel 등, 2020)를 기반으로 하며 JAX(Heek 등, 2020), Flax(Heek 등, 2020) 및 Flaxformer에 구현되었습니다. 모든 모델은 공개 T5.1.1 체크포인트에서 초기화됩니다. FiD는 원래 논문(Izacard 및 Grave, 2021)의 레시피에 따라 미세 조정됩니다. LUMEN 및 GLIMMER의 경우 라이브 레이어의 비율 a가 주어지면 메모리 인코더는 T5 인코더의 첫 번째 레이어 비율로 초기화되고 라이브 인코더는 T5 인코더의 마지막 레이어 비율로 초기화됩니다. 주요 실험에서는 α = 3을 사용합니다. 미세 조정 미세 조정을 위해 Adafactor 최적화 도구(Shazeer 및 Stern, 2018)를 사용하며, 모든 작업에 대해 학습률 0.0001, 배치 크기 128, 드롭아웃 비율 0.1을 사용합니다. 다중 작업 학습의 경우 작업에서 균일하게 샘플링합니다. 질문에 48개 토큰, 각 구절에 304개 토큰을 할당합니다. 표준 언어 모델링 손실 외에도 재순위 실험에서는 보조 Performance--LUMEN-GLIMMER■LUMEN-GLIMMER ■■■■LUMEN-Selected passages Retrieved passages 그림 3: 라이브 비율이 ½½이고 재순위 비율이 선택된 구절이 5개(왼쪽)인 검색 횟수의 함수인 GLIMMER-Large에 대한 KILT의 평균 개발 성능과 선택된 구절 수가 25개(오른쪽)인 검색 횟수의 함수입니다. 가중치와 온도가 1.0인 퍼플렉시티 증류 손실. 수렴할 때까지 훈련하고 개발 세트에서 가장 높은 성능을 보이는 체크포인트를 선택합니다. 추론을 위해 탐욕적 디코딩을 사용합니다. 데이터 지식 집약적 작업의 KILT 벤치마크(Petroni et al., 2020)의 데이터 세트 하위 세트에서 훈련하고 평가합니다. 특히, 여기에는 질문 답변 데이터 세트 Natural Questions(Kwiatkowski et al., 2019), TriviaQA(Joshi et al., 2017), HotPotQA(Yang et al., 2018), 사실 검증 데이터 세트 FEVER(Thorne et al., 2018) 및 슬롯 채우기 데이터 세트 Zero Shot RE(Levy et al., 2017) 및 TREX(ElSahar et al., 2018)가 포함됩니다. Hofstätter et al.(2022)의 관련성 필터링 절차를 적용하여 불균형 데이터 세트의 문제를 개선합니다. 검색 Hofstätter et al. (2022)의 검색 절차를 사용합니다. Wikipedia는 최대 200개 단어의 청크로 나뉘고, 사전 훈련된 GTR-Base 모델(Ni et al., 2021)에서 계산한 쿼리와 가장 높은 유사도 점수를 가진 구절을 검색합니다. 4.2 주요 결과 주요 결과의 경우 FiD, LUMEN(업데이트된 아키텍처 및 멀티태스크 훈련 포함), GLIMMER를 비교합니다. 내장된 재순위 지정으로 인해 GLIMMER는 구절을 더 효율적으로 처리하므로 LUMEN보다 더 많은 문서를 검색할 수 있으며, LUMEN은 FiD보다 더 많은 문서를 검색할 수 있습니다. 그림 2에서 볼 수 있듯이 이러한 효율성은 더 높은 품질과 더 빠른 모델로 이어지며 GLIMMER는 더 빠른 속도로 LUMEN과 FiD보다 성능이 뛰어납니다. 4. 검색 및 재순위 매기기 주요 결과는 GLIMMER가 처음에 더 많은 구절을 검색하고 훨씬 적은 수의 구절로 재순위 매김함으로써 FiD 및 LUMEN보다 더 낮은 비용으로 더 높은 품질을 달성할 수 있음을 나타냅니다. 여기서 우리는 검색 및 재순위 매기기에 대한 다양한 선택이 결과에 어떤 영향을 미치는지 조사합니다. 검색 및 선택된 구절 수 그림 3은 성능이 검색된 구절의 총 수와 재순위 매기기 후 선택된 구절의 수에 따라 어떻게 달라지는지 보여줍니다. 성능은 검색된 구절의 총 수에서 크게 증가하는 반면 선택된 구절의 수에서는 수익이 급격히 감소합니다. 이러한 결과는 재순위 매기기가 유용한 구절을 효과적으로 선택하며, 병목 현상은 관련 정보가 원래 검색된 구절에 있는지 여부임을 나타냅니다.성능0.GLIMMER ■LUMEN-0.4 0.6 0.8재순위 비율 B 그림 4: 라이브 비율 3, 검색된 구절 25개, 선택된 구절 5개를 갖는 GLIMMER-Large에 대한 KILT의 평균 개발 성능은 재순위 비율 B의 함수입니다.기준선 ẞ는 0.25로, 총 8개의 라이브 레이어 중 2개의 재순위 매기기 레이어에 해당합니다.그림 4는 충분한 재순위 매기기 레이어를 적용하면 모든 25개 검색을 사용하는 성능을 거의 회복하기 때문에 전자의 직관을 더욱 뒷받침합니다.반면에 전체 상호 작용을 사용한 일부 신경 재순위 매기기는 분명히 도움이 되며, 재순위 비율을 0.25 미만(재순위 매기기 레이어 2개 미만)으로 사용하면 성능에 큰 해를 끼칩니다. 흥미롭게도, 그림 5에서 보듯이 검색 횟수가 많을 경우 선택이 충분히 정확하기 때문에 더 많은 구절을 선택하면 관련 없는 맥락에서 주의가 산만해져 성능이 저하됩니다.최적의 선택된 구절 수는 순위 재지정 레이어가 많을수록 낮아지는데, 순위가 가장 높은 구절이 모든 유용한 정보를 더 잘 포착하기 때문입니다.성능70.70.2 순위 재지정 레이어 4 순위 재지정 레이어선택된 구절 그림 5: 선택된 구절 수에 따른 검색 횟수 40회를 기준으로 라이브 비율이 ½½인 GLIMMER-Large에 대한 KILT의 평균 개발 성능.순위 재지정자 GLIMMER(공유) 별도(T5에서) 별도(처음부터) 성능 69.70.68.4.멀티태스크 학습 GLIMMER의 두 번째 주요 개선 사항은 작업 간에 메모리와 라이브 인코더를 공유하고 결과적으로 메모리 인코더를 학습하는 것입니다. 이러한 개선 사항의 효과를 풀어보려는 실험을 제시합니다. 그림 6은 NQ에서만 학습한 모델과 KILT에서 학습한 모델 간의 NQ 성능을 비교하여 멀티태스크 학습의 효과를 보여줍니다. 멀티태스크 학습의 효과를 분리하기 위해 FiD와 LUMEN을 비교하고 이 비교에서 모든 모델의 메모리를 학습합니다. 멀티태스크 학습은 모든 모델에 상당한 이점을 주지만 LUMEN에 불균형적으로 큰 영향을 미치며 특히 라이브 비율이 낮을 때 그렇습니다. 그림 7은 라이브 비율의 함수로서 단일 및 멀티태스크 학습의 차이를 보여주며, 멀티태스크 성능은 더 일찍 평준화되고 더 작은 라이브 비율에서 더 큰 영향을 보여줍니다. 라이브 인코더가 담당하는 늦은 상호 작용은 사전 학습 작업과 다소 다르므로 라이브 인코더가 데이터의 크기와 다양성이 증가하면 불균형적으로 큰 이점을 얻는다는 것은 직관적입니다. 멀티태스크 학습은 또한 메모리 인코더를 학습할 수 있게 합니다. 표 2는 메모리 인코더를 학습하는 것이 성능에 중요하다는 것을 보여줍니다. 이는 사전 학습된 인코더가 기본적으로 메모리 인코더로 작동하도록 설계되지 않았기 때문에 예상할 수 있습니다. NQ 전용 다중 작업 FiD 표 1: 25개의 검색된 구절과 5개의 선택된 구절을 사용하여 GLIMMER-Large에 대한 KILT 개발 세트의 평균 성능(공유, T5에서 별도로 초기화, 처음부터 별도로 초기화).L1/L1/별도의 재순위 매기기 별도의 재순위 매기기와 달리 라이브 인코더를 사용하여 재순위 매기기를 수행하는 효과를 고려하는 것도 유익합니다.표 1은 별도의 재순위 매기기, T5에서 초기화 또는 처음부터 학습한 경우와 GLIMMER의 성능을 비교합니다.별도의 재순위 매기기는 더 복잡한 모델과 추가 메모리 및 계산 오버헤드를 희생하고 비슷한 성능을 달성합니다.사전 학습된 가중치에서 재순위 매기기 초기화가 중요합니다.처음부터 재순위 매기기 계층을 학습하려고 하면 성능이 상당히 낮아집니다.정확한 일치그림 6: 다중 작업 학습은 FiD에 비해 LUMEN에 비례적으로 이점이 있습니다. FiD, GLIMMER-3 및 GLIMMER-Large 모델에 대해 Natural Questions에 대해서만 학습했을 때와 KILT 작업 세트에 대해서만 학습했을 때의 Natural Questions 개발 세트에 대한 정확한 일치.4.5 기타 절제 GLIMMER 아키텍처와 학습 프로세스에는 여러 가지 다른 흥미로운 결정이 있습니다.G 정확한 일치 KILT NQ 전용 0.4 0.0.모델 성능 GLIMMER 69.별도 Qenc 70.PDist = 0.69.PDistX =69.PDist 7 = 0.70.PDist 7 =69.평균 풀 69.0.실시간 비율 a 그림 7: 실시간 비율의 함수로서 KILT와 NQ 전용에서 학습한 LUMEN-Large의 Natural Questions 개발 세트에 대한 성능.표 3: GLIMMER 절제: 별도 질문 인코더, 다른 퍼플렉시티 증류 손실 가중치, 퍼플렉시티 증류 온도 및 평균 풀 채점 방법. 각 모델은 25회 검색과 5회 선택된 구절이 있는 대형 크기이며, KILT 개발 세트에서 평가되었습니다.모델 GLIMMER 동결된 메모리 성능 69.69.표 2: 훈련 메모리는 강력한 GLIMMER 성능에서 중요한 요소입니다.훈련 메모리가 있고 없는 상태에서 25회 검색된 구절과 5회 선택된 구절이 있는 GLIMMER-Large에 대한 KILT 개발 세트의 평균 성능.dure.표 3은 이러한 결정 중 일부의 절제를 나타냅니다.원래 LUMEN 구현에는 메모리 인코더가 미세 조정되지 않았기 때문에 필요한 별도의 질문 인코더가 있었습니다.여기서는 멀티태스크 훈련으로 메모리 인코더를 업데이트하므로 메모리 인코더를 재사용하여 질문을 인코딩하고 아키텍처를 단순화하고 매개변수 수를 줄이기로 했습니다.이러한 단순화가 성능에 약간의 비용을 초래한다는 것을 알 수 있습니다. 또한 재순위와 관련하여 여러 매개변수 선택 사항이 있습니다.퍼플렉시티 증류 손실의 가중치, 점수 및 퍼플렉시티 분포의 온도, 재순위 점수를 생성하는 방법입니다.재순위 손실에 과도하거나 과소한 가중치를 주면 성능이 저하됩니다.그러나 점수 및 퍼플렉시티 분포에 더 낮은 온도를 사용하면 도움이 됩니다.Izacard 등(2022)은 대부분의 개별 구절이 퍼플렉시티에 미치는 영향이 작고, 더 낮은 온도가 이러한 차이점을 구별하는 데 도움이 된다고 주장합니다.마지막으로, 각 구절의 첫 번째 토큰을 사용하면 평균 풀링 표현에서 점수를 생성하는 것과 비슷한 성능을 보이는 것으로 보입니다.5 관련 연구 검색 증강(Izacard 및 Grave, 2021; Borgeaud 등, 2022; Lewis 등, 2020; Khandelwal 등, 2020; Guu 등, 2020)은 추가 컨텍스트로 입력을 증강하여 언어 모델 성능을 개선하는 강력한 기술입니다. 저희의 연구는 검색 증강 언어 모델에 대한 품질-컴퓨팅 트레이드오프를 개선하는 데 중점을 두고 있습니다. 이는 세 가지 연구 분야, 즉 늦은 상호작용 메모리, 늦은 상호작용 재순위 지정, 검색 학습을 통합하여 수행합니다. 저희의 접근 방식은 가장 일반적인 검색 증강 모델 중 하나인 Fusion-in-Decoder(Izacard and Grave, 2021)의 아키텍처 골격을 사용합니다. 저희는 Hofstätter et al.(2022)에서와 같이 KILT(Petroni et al., 2020)에 대한 멀티태스크 학습을 사용합니다. 메모리 검색 증강은 언어 모델에서 처리해야 하는 추가 컨텍스트로 인해 비용이 많이 듭니다. TOME(de Jong et al., 2022b), Memorizing Transformer(Wu et al., 2022a) 및 기타 여러 모델(Li et al., 2022; Zhong et al., 2022; Chen et al., 2022; Wu et al., 2022b; Yogatama et al., 2021; Bertsch et al., 2023)과 같은 메모리 모델은 표현을 미리 계산하여 메모리에 저장함으로써 이러한 비용을 피하려고 시도하여 표현을 즉석에서 처리하는 대신 직접 검색할 수 있습니다. 그러나 이러한 접근 방식은 메모리 표현이 각 개별 입력에 따라 조건이 지정되지 않기 때문에 품질이 저하됩니다(Li et al., 2022; de Jong et al., 2023). 후기 상호작용 메모리(de Jong et al., 2023; Milbauer et al., 2023)는 검색 표현을 부분적으로만 미리 계산하고 메모리와 입력 간의 일부 상호작용을 즉석에서 수행함으로써 메모리 접근 방식의 품질을 개선합니다.특히, 저희의 작업은 LUMEN(de Jong et al., 2023)에 매우 밀접하게 기반합니다.재순위 언어 모델 자체와 마찬가지로 검색 절차는 완전한 상호작용이 있는 값비싼 온라인 순위(Chen et al., 2020)와 DPR(Karpukhin et al., 2020) 및 GTR(Ni et al., 2021)과 같은 보다 일반적인 듀얼 인코더 접근 방식 사이에서 균형을 이룹니다.이러한 접근 방식은 미리 계산된 구절 표현의 코퍼스와의 내적 유사성을 기반으로 점수를 매깁니다. 종종 검색을 위한 다양한 모델이 파이프라인 방식으로 적용되며, 초기에 저렴한 스코어링 모델이 적용된 다음 더 강력하고 비싼 리랭커가 적용됩니다(Mao et al., 2021; Wang et al., 2018; Yu et al., 2022). 많은 리랭커도 늦은 상호작용을 활용하여 랭킹 품질과 속도 사이에서 좋은 균형을 얻습니다. 여기에는 COLBERT(Khattab and Zaharia, 2020; Santhanam et al., 2022), PreTTR(MacAvaney et al., 2020), SDR(Cohen et al., 2022) 및 Poly-encoders(Humeau et al., 2020)가 포함됩니다. GLIMMER는 늦은 상호작용 메모리와 리랭킹을 단일 모델로 결합하여 두 사용 사례에 대해 미리 계산된 표현을 공유합니다. 검색 학습 검색 모델은 종종 지도 학습 데이터(Karpukhin 등, 2020; Ni 등, 2021)로 훈련되며, MS-MARCO(Nguyen 등, 2016) 또는 TREC CAR(Dietz 등, 2018)과 같은 데이터 세트의 골드 검색을 사용합니다. 검색 증강 생성에 사용할 구절을 선택할 때, 우리는 독자 모델에 가장 도움이 되는 구절이라는 추가 신호를 갖게 됩니다. 여러 기존 연구에서 이 신호를 사용하여 검색을 개선합니다(Guu 등, 2020; Sachan 등, 2021; Jiang 등, 2022; Sachan 등, 2021; Izacard 등, 2022). 우리는 ATLAS(Izacard et al., 2022)를 따르고 복잡성 증류를 사용하여 독자 모델 복잡성을 낮추는 데 도움이 되는 구절을 선택하도록 재순위 지정기를 훈련합니다. 6
--- CONCLUSION ---
검색 증강 언어 모델은 강력하지만 추론이 느린 반면, 사전 계산된 메모리 증강 모델은 빠르지만 품질이 떨어집니다. LUMEN과 같은 하이브리드 후기 상호 작용 모델은 우수한 품질-계산 트레이드오프를 나타냅니다. 학습된 엔드투엔드 재랭킹 및 멀티태스크 학습을 통합하여 더욱 나은 트레이드오프를 달성하는 개선된 후기 상호 작용 모델인 GLIMMER를 소개합니다. GLIMMER는 지식 집약적 작업의 KILT 벤치마크에서 LUMEN 및 FiD에 비해 더 빠른 속도로 품질이 크게 향상됩니다. 감사의 말 Google Research의 Luke Vilnis, Tania Bedrax-Weiss 및 기타 분들께 통찰력 있는 의견과 토론에 감사드립니다. 참고 문헌 Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón 및 Sumit Sanghai. 2023a. GQA: 멀티헤드 체크포인트에서 일반화된 멀티쿼리 변환기 모델 학습. CoRR, abs/2305.13245. Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, Sumit Sanghai. 2023b. Colt5: 조건부 계산을 사용한 더 빠른 장거리 변압기. CoRR, abs/2303.09752. Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley. 2023. Unlimiformer: 무제한 길이 입력을 사용한 장거리 변압기. CORR, abs/2305.01625. 세바스찬 보르조, 아서 멘쉬, 조던 호프만, 트레버 카이, 엘리자 러더퍼드, 케이티 밀리칸, 조지 반 덴 드리셰, 장바티스트 레스피오, 보그단 다모크, 에이단 클라크, 디에고 데 라스 카사스, 아우렐리아 가이, 제이콥 메닉, 로만 링, 톰 헤니건, 사프론 황, 로렌 마조레, 크리스 존스, 알빈 카시러, 앤디 브록, 미켈라 파가니니, 제프리 어빙, 오리올 비냔알스, 사이먼 오신데로, 캐런 시모얀, 잭 W. 레이, 에리히 엘슨, 로랑 시프레. 2022. 수조 개의 토큰에서 검색하여 언어 모델 개선. International Conference on Machine Learning, ICML 2022, 2022년 7월 17-23일, 미국 메릴랜드주 볼티모어, Proceedings of Machine Learning Research의 162권, 2206-2240쪽. PMLR. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, John Jumper. 2023. 추측적 샘플링을 통한 대규모 언어 모델 디코딩 가속화. CoRR, abs/2302.01318. Dongmei Chen, Sheng Zhang, Xin Zhang, Kaijing Yang. 2020. 정렬 증강 다국어 BERT를 통한 교차 언어 구절 재순위 지정. IEEE Access, 8:213232-213243. Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William W. Cohen. 2022. 도메인 질문 답변을 위한 qa-메모리를 사용한 사전 학습된 언어 모델 증강. CoRR, abs/2204.04581. openNachshon Cohen, Amit Portnoy, Besnik Fetahu, and Amir Ingber. 2022. SDR: 간결한 문서 표현을 사용한 효율적인 신경망 재순위 지정. 60th Annual Meeting of the Association for Computational Linguistics(제1권: 장문 논문)의 회의록, ACL 2022, 더블린, 아일랜드, 2022년 5월 22-27일, 6624-6637페이지. Association for Computational Linguistics. Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. 2022a. FiDO: 더 강력한 성능과 더 빠른 추론을 위해 최적화된 퓨전-인-디코더. arXiv 사전 인쇄본 arXiv:2212.08153. Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, and William W. Cohen. 2023. 사전 계산된 메모리 또는 즉석 인코딩? 검색 증강에 대한 하이브리드 접근 방식은 컴퓨팅을 최대한 활용합니다. CORR, abs/2301.10448. Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W. Cohen. 2022b. 언급 메모리: 엔티티 언급 주의를 통해 텍스트 지식을 변환기에 통합. 제10회 국제 학습 표현 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net. Laura Dietz, Ben Gamari, Jeff Dalton, Nick Craswell. 2018. TREC 복잡한 답변 검색 개요. 2018년 11월 14-16일 미국 메릴랜드주 게이더스버그에서 열린 제27회 텍스트 검색 컨퍼런스의 회의록, TREC 2018, NIST 특별 간행물 500-331권. 미국 표준 기술 연구소(NIST). Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, Elena Simperl. 2018. T-rex: 지식 기반 트리플을 사용한 자연어의 대규모 정렬. 2018년 5월 7-12일 일본 미야자키에서 열린 제11회 국제 언어 자원 및 평가 컨퍼런스 논문집(LREC 2018)에서 발췌. 유럽 언어 자원 협회(ELRA). Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. 2020. REALM: 검색 증강 언어 모델 사전 학습. CoRR, abs/2002.08909. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, Marc van Zee. 2020. Flax: JAX를 위한 신경망 라이브러리 및 생태계. Sebastian Hofstätter, Jiecao Chen, Karthik Raman, Hamed Zamani. 2022. 관련성 샘플링을 통한 다중 작업 검색 증강 텍스트 생성. CORR, abs/2207.03030. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston. 2020. Poly-encoders: 빠르고 정확한 다중 문장 채점을 위한 아키텍처 및 사전 학습 전략. 8th International Conference on Learning Representations, ICLR 2020, 아디스아바바, 에티오피아, 2020년 4월 26-30일. OpenReview.net. doGautier Izacard and Edouard Grave. 2021. 생성 모델을 사용한 구절 검색을 활용하여 개방형 주요 질문 답변. Association for Computational Linguistics의 유럽 지부 16차 회의록: Main Volume, EACL 2021, 온라인, 2021년 4월 19-23일, 874880페이지. Association for Computational Linguistics. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, Edouard Grave. 2022. 검색 증강 언어 모델을 사용한 Few-shot 학습. CoRR, abs/2208.03299. Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, Graham Neubig. 2022. 주의로서의 검색: 단일 변환기 내에서 검색 및 읽기의 종단 간 학습. 2022년 자연어 처리 경험적 방법 컨퍼런스 회의록, EMNLP 2022, 아랍에미리트 아부다비, 2022년 12월 7-11일, 2336-2349쪽. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer. 2017. Triviaqa: 독해를 위한 대규모 원격 지도 학습 챌린지 데이터 세트. Association for Computational Linguistics의 제55회 연례 회의록, ACL 2017, 캐나다 밴쿠버, 7월 30일 - 8월 4일, 제1권: 장문 논문, 1601-1611페이지. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. 2020. 오픈 도메인 질의응답을 위한 밀집 구절 검색. 2020 자연어 처리 경험적 방법에 대한 컨퍼런스의 진행 사항, EMNLP 2020, 온라인, 2020년 11월 16-20일, 6769-6781쪽. Association for Computational Linguistics. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis. 2020. 기억을 통한 일반화: 최근접 이웃 언어 모델. 제8회 학습 표현 국제 컨퍼런스, ICLR 2020, 에티오피아 아디스아바바, 2020년 4월 26-30일. OpenReview.net. Omar Khattab and Matei Zaharia. 2020. Colbert: BERT를 통한 맥락화된 늦은 상호 작용을 통한 효율적이고 효과적인 구절 검색. 2020년 7월 25-30일, 중국에서 열린 정보 검색 연구 및 개발에 관한 제43회 국제 ACM SIGIR 컨퍼런스의 회의록, SIGIR 2020, 가상 이벤트, 39-48페이지. ACM. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466. Yaniv Leviathan, Matan Kalman, Yossi Matias. 2022. 추측적 디코딩을 통한 변환기로부터의 빠른 추론. CORR, abs/2211.17192. Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer. 2017. 독해 이해를 통한 제로샷 관계 추출. 21st Conference on Computational Natural Language Learning(CoNLL 2017) 회의록, 캐나다 밴쿠버, 2017년 8월 3-4일, 333-342쪽. Association for Computational Linguistics. Patrick SH Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2020. 지식 집약적 NLP 작업을 위한 검색 증강 생성. Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020년 12월 6일-12일, 가상.Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022. 컨텍스트 증강 언어 모델링을 위한 분리된 컨텍스트 처리.CoRR, abs/2210.05758.Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. 사전 계산된 용어 표현을 통한 변환기의 효율적인 문서 재순위 지정.정보 검색 연구 및 개발에 관한 제43회 국제 ACM SIGIR 컨퍼런스 회의록, SIGIR 2020, 가상 이벤트, 중국, 2020년 7월 25일-30일, 49-58쪽.ACM. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen. 2021. 오픈 도메인 질의응답을 위한 독자 안내 구절 재순위. Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, 온라인 이벤트, 2021년 8월 1-6일, Findings of ACL의 ACL/IJCNLP 2021 권, 344350페이지. Association for Computational Linguistics. Jeremiah Lev Milbauer, Annie Louis, Javad Hosseini, Alex Fabrikant, Don Metzler, Tal Schuster. 2023. Lait: 레이어 조정 상호 작용이 있는 변압기에서의 효율적인 다중 세그먼트 인코딩. Association for Computational Linguistics: ACL 2023의 회의록에서. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: 인간이 생성한 기계 독해 이해 데이터 세트. 2016년 12월 9일 스페인 바르셀로나에서 열린 제30회 신경 정보 처리 시스템 연례 컨퍼런스(NIPS 2016)와 동시에 개최된 Workshop on Cognitive Computation: Integrating neural and symbolic ways 2016의 회의록에서. CEUR-WS.org. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang 및 Yinfei Yang. 2021. 대형 이중 인코더는 일반화 가능한 검색기입니다. CoRR, ABS/2112.07899. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick SH Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktäschel 및 Sebastian Riedel. 2020. KILT: 지식 집약적 언어 작업을 위한 벤치마크. CORR, ABS/2009.02252. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색. J. Mach. Learn. Res., 21:140:1-140:67. Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. 2021. 오픈 도메인 질의응답을 위한 다중 문서 리더 및 리트리버의 종단 간 학습. Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, 2021년 12월 6-14일, 가상, 25968-25981쪽. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia. 2022. Colbertv2: 가볍고 늦은 상호작용을 통한 효과적이고 효율적인 검색. 2022년 북미 컴퓨터 언어학 협회 지부 회의록: 인간 언어 기술, NAACL 2022, 워싱턴주 시애틀, 미국, 2022년 7월 10-15일, 37153734쪽. 컴퓨터 언어학 협회. Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, Donald Metzler. 2022. 자신감 있는 적응형 언어 모델링. arXiv 사전 인쇄본 arXiv:2207.07061. Noam Shazeer. 2019. 빠른 트랜스포머 디코딩: 쓰기 헤드 하나만 있으면 됩니다. arXiv 사전 인쇄본 arXiv:1911.02150. Noam Shazeer와 Mitchell Stern. 2018. Adafactor: 선형 메모리 비용이 있는 적응 학습 속도. 35회 기계 학습 국제 컨퍼런스 회의록, ICML 2018, Stockholmsmässan, 스톡홀름, 스웨덴, 2018년 7월 10일-15일, 기계 학습 연구 회의록, 4603-4611페이지. PMLR. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal. 2018. FEVER: 사실 추출 및 검증을 위한 대규모 데이터 세트. 북미 학회 계산언어학: 인간 언어 기술 회의록, NAACL-HLT 2018, 루이지애나주 뉴올리언스, 미국, 2018년 6월 1-6일, 1권(장문 논문), 809-819쪽. 계산언어학 협회. Neeraj Varshney, Man Luo, Chitta Baral. 2022. 오픈 도메인 QA 리더가 인간처럼 외부 지식을 효율적으로 활용할 수 있을까? CORR, abs/2211.12707. Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, Jing Jiang. 2018. R³: 오픈 도메인 질의응답을 위한 강화된 랭커 리더. 32회 AAAI 인공지능 컨퍼런스(AAAI-18), 30회 혁신적 인공지능 응용(IAAI-18), 8회 AAAI 인공지능 교육 발전 심포지엄(EAAI-18), 미국 루이지애나주 뉴올리언스, 2018년 2월 2-7일, 5981-5988쪽. AAAI 출판부. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, Christian Szegedy. 2022a. 변압기 기억하기. 10회 국제 학습 표현 컨퍼런스, ICLR 2022, 가상 이벤트, 2022년 4월 25-29일. OpenReview.net. Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. 2022b. 지식 집약적 NLP 작업을 위한 효율적인 메모리 증강 변환기. CORR, abs/2210.16773. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning. 2018. Hotpotqa: 다양하고 설명 가능한 멀티홉 질의응답을 위한 데이터 세트. 2018년 자연어 처리 경험적 방법에 대한 2018년 컨퍼런스 회의록, 벨기에 브뤼셀, 2018년 10월 31일 - 11월 4일, 2369-2380페이지. Association for Computational Linguistics. Dani Yogatama, Cyprien de Masson d&#39;Autume, Lingpeng Kong. 2021. 적응형 반모수 언어 모델. Trans. Assoc. Comput. Linguistics, 9:362-373. Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2022. Kg-fid: 오픈 도메인 질의응답을 위한 퓨전-인-디코더에서 지식 그래프 주입. Association for Computational Linguistics(제1권: 장문 논문)의 제60회 연례 회의록, ACL 2022, 아일랜드 더블린, 2022년 5월 22-27일, 4961-4974쪽. Association for Computational Linguistics. Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. 메모리 증강을 통한 언어 모델 학습. CORR, abs/2205.12674.
