--- ABSTRACT ---
인기 있는 텍스트-이미지 생성 모델로 부상한 확산 모델은 텍스트 프롬프트에 따라 고품질이고 콘텐츠가 풍부한 이미지를 생성할 수 있습니다. 그러나 입력 프롬프트가 간결한 내러티브일 때 기존 모델에서는 의미 이해와 상식적 추론에 한계가 있어 낮은 품질의 이미지 생성이 발생합니다. 내러티브 프롬프트의 용량을 개선하기 위해 사전 학습된 확산 모델을 위한 의미 이해 및 추론 어댑터(SUR-어댑터)라는 간단하면서도 효과적인 매개변수 효율적인 미세 조정 방식을 제안합니다. 이 목표를 달성하기 위해 먼저 57,000개 이상의 의미적으로 수정된 다중 모달 샘플로 구성된 새로운 데이터 세트 SURD를 수집하고 주석을 달았습니다. 각 샘플에는 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 고품질 이미지가 포함되어 있습니다. 그런 다음, 서사적 프롬프트의 의미적 표현을 복잡한 프롬프트에 맞추고 지식 증류를 통해 대규모 언어 모델(LLM)의 지식을 SUR 어댑터로 전달하여 강력한 의미적 이해 및 추론 기능을 획득하여 텍스트-이미지 생성을 위한 고품질 텍스트 의미적 표현을 구축할 수 있습니다. 여러 LLM과 인기 있는 사전 훈련된 확산 모델을 통합하여 실험을 수행합니다. *두 저자 모두 이 연구에 동등하게 기여했습니다. 책임 저자. 확산 모델이 이미지 품질 저하 없이 간결한 자연어를 이해하고 추론할 수 있도록 하는 접근 방식의 효과를 보여줍니다. 저희의 접근 방식은 더 나은 사용자 경험으로 텍스트-이미지 확산 모델을 사용하기 쉽게 만들 수 있으며, 이는 저희의 접근 방식이 간단한 서사적 프롬프트와 복잡한 키워드 기반 프롬프트 간의 의미적 격차를 메움으로써 사용자 친화적인 텍스트-이미지 생성 모델 개발을 더욱 발전시킬 수 있는 잠재력이 있음을 보여줍니다. 코드는 https://github.com/Qrange-group/SUR-adapter에서 공개됩니다. CCS 개념 • ⚫ 컴퓨팅 방법론 → 자연어 처리; 컴퓨터 비전; 머신 러닝 알고리즘. 키워드 확산 모델, 대규모 언어 모델, 멀티모달 이미지 생성, 어댑터, 지식 증류
--- INTRODUCTION ---
최근 몇 년 동안, 확산 모델 기반 멀티모달 텍스트-이미지 생성 기술은 인상적인 진전을 이루었습니다[50]. 이러한 모델[38, 46]이 방대한 양의 데이터와 모델 매개변수로 학습됨에 따라, 사람들은 복잡한 페인팅 기술이 필요 없이 텍스트 프롬프트와 기타 정보를 통해 텍스트와 관련되고 시각적으로 매력적인 이미지를 처음부터 끝까지 생성할 수 있습니다. 그러나 사전 인쇄본, 기술 보고서, 신선하게 구운 파이 4개 빈티지 유리병 7개 키 큰 나무가 있는 푸른 숲 구불구불한 길을 따라 질주하는 생생한 빨간색 스포츠카와 작은 폭포 복합 프롬프트 Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 단순 프롬프트 단순 프롬프트 + LLM (a) 의미 이해 (b) 상식적 추론 그림 2: 확산 모델에서 텍스트 인코더의 의미 이해 및 상식적 추론 기능. 이러한 기존 확산 모델에서 이미지 생성의 품질은 키워드 기반 텍스트 프롬프트 또는 기타 형태의 텍스트 프롬프트의 복잡하고 정교한 디자인에 크게 의존합니다. 또한 텍스트 프롬프트가 간결한 내러티브나 일상적인 표현인 짧은 문구인 경우 생성된 이미지의 충실도와 텍스트 관련성이 종종 크게 손상됩니다. 이러한 제한으로 인해 우수한 사용자 경험을 제공하는 간결한 내러티브로 확산 모델을 직관적으로 제어하기 어렵습니다. 이 문제의 가장 중요한 이유는 이러한 확산 모델의 텍스트 인코더(종종 이미지 텍스트 대조 학습으로 학습된 사전 학습된 CLIP[34]의 텍스트 인코더)가 텍스트-이미지 생성 작업에 정렬되지 않아 이미지 생성에 대한 의미 이해 및 추론(SUR)이 좋지 않기 때문입니다. 구체적으로 CLIP은 대조 학습을 사용하여 약 400M개의 이미지-텍스트 쌍으로 학습된 다중 모달 신경 모델이며, 이미지 인코더와 텍스트 인코더는 이미지와 텍스트 간의 연관성을 성공적으로 연결할 수 있기 때문에 확산 모델과 같은 다양한 다중 모달 작업이나 모델에 널리 적용되었습니다. CLIP의 학습 목표는 매칭된 이미지와 텍스트 쌍을 피처 공간에서 더 가깝게 끌어서 이미지-텍스트 대응 관계를 확립하는 것이지만, 해당 이미지를 설명하는 텍스트는 간략하여 이미지의 일부 의미와만 일치할 수 있으며, 그 결과 텍스트 인코더에서 생성된 불완전한 피처가 생성됩니다. 그러나 텍스트-이미지 생성 과제는 텍스트 인코더가 간결한 내러티브의 의미를 이해할 수 있을 뿐만 아니라 내러티브에 근거한 암묵적인 상식이나 지식을 추론하고 완성하여 모델이 내러티브와 매우 일관된 정확한 이미지를 생성할 수 있도록 요구합니다. 따라서 조건부 텍스트-이미지 생성을 위해 CLIP의 텍스트 인코더를 확산 모델에 임베드하면 텍스트 인코더에서 의미 이해 및 상식적 추론 능력이 부족하여 입력 텍스트가 자연어인 경우 낮은 품질의 이미지가 생성됩니다. 이러한 결함을 보여주기 위해 먼저 다중 모달 시각적 질의응답에서 세 가지 일반적인 유형의 텍스트 프롬프트[1, 7, 8, 31]를 사용하여 확산 모델에서 텍스트 인코더의 의미 이해 능력을 평가합니다. &quot;세기&quot;, &quot;색상&quot; 및 &quot;동작&quot;. 표 1에서 볼 수 있듯이 각 표에 대해 세 가지 다른 프롬프트를 설계했습니다. 표 1: 확산 모델을 사용하여 간단한 프롬프트로 생성된 이미지의 의미 정확도(Acc.) 평가. 간단한 프롬프트는 &quot;세기&quot;, &quot;색상&quot; 및 &quot;동작&quot;을 포함한 세 가지 유형의 문장으로 구성되었습니다. 각 프롬프트는 이미지를 생성했으며 이미지는 의미 정확도를 위해 수동으로 검사되었습니다. 결과에 따르면 대부분 프롬프트의 의미 정확도는 50% 미만이며 두 가지 유형의 프롬프트조차도 의미 정확도율이 0%입니다. 유형 프롬프트 Acc. &lt; 50%? 갓 구운 파이 4개. 63.08% 세기 그림 같은 풍경 위에 떠 있는 다채로운 풍선 6개. 빈티지 유리 병 7개. 8.46% ✓ 0.00% ✓ 색상 구불구불한 길을 질주하는 강렬한 빨간색 스포츠카.빨간 주스가 담긴 파란색 유리잔.86.15% 17.69% ✓ 파란색과 노란색 단색 옷을 입은 커플.0.00% ✓ 동작 운동장에서 누군가 농구를 슛하고 있습니다.나무를 먹는 기린.41.54% ✓ 25.38% 주방에서 요리사가 피자 반죽을 공중에 던지고 있습니다.15.38% ✓ 유형, 각 텍스트 프롬프트에 대해 텍스트-이미지 확산 모델[38]을 사용하여 130개의 이미지를 생성하고 생성된 이미지가 주어진 텍스트 프롬프트의 의미적 의미를 충족하는지 수동으로 평가했습니다.결과에 대한 통계 분석을 통해 대부분의 텍스트 프롬프트에 대한 의미적 이해의 정확도가 50%를 초과하지 않는다는 것을 발견했습니다. 놀랍게도 &quot;7개의 빈티지 유리병&quot;과 &quot;각각 파란색과 노란색 단색 옷을 입은 커플&quot;과 같은 겉보기에 간단한 내러티브 프롬프트조차도 정확도가 0%로, 확산 모델의 텍스트 인코더가 이미지 생성을 위해 이러한 간단한 텍스트의 의미를 전혀 이해하지 못하고 심각한 정보 편향을 초래했음을 나타냅니다. 그림 2(a)는 의미 이해 능력이 부족하여 발생하는 의미 오류의 예를 추가로 보여줍니다. 다음으로 텍스트 인코더의 상식적 추론 능력을 고려합니다. 안정적인 확산 모델이 널리 검증된 생성 기술에 따라 아름다운 고양이를 생성하기를 바란다면, 다음 프롬프트와 같이 고품질의 생성된 이미지를 얻기 위해 복잡하고 정교한 키워드 기반 프롬프트가 필요합니다. SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models 사전 인쇄본, 기술 보고서(복잡한 프롬프트 예) 8k uhd, RAW 사진, 아름다운 고양이(사실적:1.1), 걸작, RAW 사진, 실제 고양이, RAW 사진, 초고해상도, 사실적, 최상의 품질(고도의 피부, 피부 세부 정보), 보이는 모공, 빛나는 피부, 매우 섬세하고 아름다운, 매우 자세한 8K 배경 화면, 8k 고품질, 필름 그레인, DSLR, 아름다운 세부 정보가 있는 아름다운 고양이(시청자를 바라보는), 전문 사진 조명, 매우 자세한 눈과 얼굴, 아름다운 세부 정보가 있는 눈, 아날로그 스타일, 귀엽고 장난기 있는, 사랑스럽다(화려하고 다채로움:1.1), 고양이의 초상화<lora:mikeneko:0.7> , from side, full body, (brown black white fur) 그림 2(b)와 같이 복잡한 프롬프트를 사용하여 생성된 이미지가 &quot;아름다운 고양이&quot;와 같은 간단한 프롬프트를 사용하여 생성된 이미지에 비해 세부 정보가 더 좋고 윤곽선이 더 정확하며 상식이 정확하다는 것을 알 수 있습니다. (간단한 프롬프트 예) 아름다운 고양이 복잡한 프롬프트를 입력하는 것은 &quot;아름다운&quot;과 &quot;고양이&quot; 사이의 세부 정보와 이해를 텍스트 인코더에 직접 주입하는 것과 같으며, 확산 모델이 기분 좋은 &quot;아름다운 고양이&quot;를 생성할 수 있도록 합니다. 이는 확산 모델이 의미적으로 의미 있는 이미지를 생성할 수 있는 잠재력이 있지만 텍스트 인코더의 상식적 추론 능력에 의해 제한됨을 나타냅니다. 간단한 프롬프트는 텍스트 인코더가 &quot;아름다운 고양이&quot;의 의미를 직접 이해할 수 없게 하며, 인코더 자신의 지식에서 &quot;아름다운&quot;의 의미를 추론할 수도 없습니다. 이러한 문제에 직면하여 ChatGPT 및 LLAMA[45]와 같은 대규모 언어 모델(LLM)의 최근 발전은 향상된 SUR 능력과 함께 놀라운 대화 기능을 보여주어 자연어 처리(NLP) 분야에서 새로운 경지를 창출했습니다. 따라서 ChatGPT를 사용하여 &quot;아름다운 고양이&quot;를 설명하려고 시도했고 다음과 같은 텍스트를 얻었습니다. (LLM의 상식적 추론) 고양이는 부드러운 털, 표현력이 풍부한 눈, 우아한 움직임으로 매혹적인 아름다움으로 유명합니다. 아름다운 고양이는 독특한 패턴의 매끈한 털, 꿰뚫는 눈, 우아한 자세와 같은 독특한 특징을 가질 수 있습니다. 각 고양이는 고유한 방식으로 독특하며, 그들의 아름다움은 보는 사람의 관점에 따라 주관적입니다. 이 텍스트는 LLM의 &quot;아름다움&quot;과 &quot;고양이&quot;에 대한 이해와 어떤 종류의 &quot;고양이&quot;가 &quot;아름답다&quot;고 여겨지는지에 대한 상식적 추론을 보여줍니다. 이 텍스트에서 생성된 이미지는 그림 2(b) 오른쪽 아래에 표시된 것처럼 복잡한 프롬프트를 사용하여 생성된 이미지와 품질이 비슷합니다. 위의 모든 사례 연구는 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하여 확산 모델이 간단한 내러티브 프롬프트를 사용하더라도 의미적으로 정확하고 고품질의 이미지를 생성할 수 있는지 고려하도록 영감을 줍니다. 이 목표를 달성하기 위해 이 논문에서는 먼저 57,000개 이상의 의미적으로 수정된 이미지-텍스트 쌍으로 구성된 SURD라는 새로운 데이터 세트를 수집하여 주석을 달았습니다. 각 이미지-텍스트 쌍에는 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 고품질 이미지가 포함되어 있습니다. SURD를 활용하여 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하고 간단하고 복잡한 프롬프트의 표현을 정렬하는 SUR 어댑터를 제안합니다. 광범위한 실험과 통계적 테스트를 통해 제안된 SUR 어댑터가 사전 훈련된 확산 모델의 텍스트 인코더를 크게 향상시키고 간결한 내러티브 프롬프트와 생성된 이미지 간의 불일치를 완화하는 고품질 이미지를 생성한다는 것이 확인되었습니다. 요약하자면, 우리의 기여는 세 가지입니다.• 57,000개 이상의 의미적으로 수정된 이미지-텍스트 쌍을 포함하는 데이터 세트 SURD를 수집하고 주석을 달았습니다. 각 이미지 텍스트 쌍에는 간단한 프롬프트, 복잡한 프롬프트 및 고품질의 해당 이미지가 포함되어 있습니다.• SURD를 기반으로 LLM의 의미적 이해 및 추론 능력을 사전 훈련된 확산 모델로 효과적으로 전환하여 간단한 프롬프트로 생성된 의미적 불일치 및 저품질 이미지 문제를 완화하는 SUR 어댑터를 제안합니다.• 제안된 SUR 어댑터를 사용하여 생성된 이미지에 대한 광범위한 통계적 테스트와 논의를 수행하여 효과를 분석하고 한계를 추가로 논의합니다.
--- RELATED WORK ---
S 2.1 텍스트-이미지 확산 확산 모델은 텍스트-이미지 생성에 광범위하게 활용되었습니다[2, 11, 23, 25, 38, 41, 46]. 텍스트-이미지 확산은 텍스트 입력을 확산 모델의 조건화 신호로 활용하여 노이즈 추가 및 제거 프로세스를 통해 텍스트 관련 이미지를 생성합니다[38]. 텍스트-이미지 확산의 텍스트 인코더는 종종 CLIP[34]과 같은 사전 학습된 언어 모델을 활용하여 텍스트 입력을 잠재 벡터로 인코딩하여 수행됩니다. 텍스트-이미지 확산은 이미지 초해상도[27, 42], 인페인팅[32], 조작[5, 54], 의미 분할[4, 12], 비디오 생성[51, 56] 등 다양한 분야에서 널리 사용됩니다. 2.2 대규모 언어 모델 최근 NLP 분야에서 LLM이 급증했습니다[17]. Jozefowicz 등[21]은 LSTM을 10억 개의 매개변수로 확장하여 Billion Word 벤치마크에서 최첨단 결과를 얻었습니다. 이후, 확장 변환기는 많은 NLP 작업에서 개선으로 이어졌으며, 주목할 만한 모델로는 BERT[10], GPT-2[35], MegatronLM[43], T5[37]가 있습니다. 1,750억 개의 매개변수를 가진 모델인 GPT-3[6]의 도입은 이 분야에서 큰 획기적인 진전을 이루었으며, Jurassic-1[29], Megatron-Turing NLG[44], Gopher[36], Chinchilla[17], PALM[9], OPT[57], GLM[52], LLAMA[45]와 같은 수많은 LLM의 개발로 이어졌습니다. 또한, 여러 연구[15, 17, 22, 40, 49]에서 LLM 성능에 대한 확장의 영향을 조사하여 사용 편의성을 향상시켰습니다. 3 의미 이해 및 추론 데이터 세트 SURD는 그림 3에서 볼 수 있듯이 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 의미적으로 올바른 이미지의 57,603개 3중항으로 구성된 멀티 모달 데이터 세트입니다.저희가 아는 한, SURD는 간단하고 복잡한 프롬프트를 모두 기록하고 텍스트-이미지 확산 모델의 SUR 문제를 해결하는 데 도움이 되는 의미적으로 올바른 이미지-텍스트 쌍을 제공하는 데 중점을 둔 최초의 데이터 세트입니다.사전 인쇄본, 기술 보고서, BLIP 간단한 프롬프트: 파란색 배경에 큰 눈을 가진 다채로운 동물 복잡한 프롬프트: 귀여운 생물 스타일의 전신 클로즈업 초상화 작고 귀여운 아이소메트릭 알레브리헤 이모티콘, 부드럽고 매끄러운 조명, 굵은 평면 색상, 3D 아이콘 클레이 렌더, 100mm 렌즈, 3D 블렌더 렌더, 자연 환경 풍경, 멕시코에서 영감을 받은 배경, 폴리카운트에서 트렌드, 모듈식 구성주의, 물리 기반 렌더링, 기하학적, 중심, 동적 포즈. alebrije는 멕시코 민속 예술에서 영감을 얻어야 하며 사자, 독수리 또는 뱀과 같은 여러 다른 동물의 조합이어야 합니다., solo Diffusion models 그림 3: SURD의 예. 우리는 공개적으로 사용 가능한 웹사이트에서 확산 모델에 의해 생성된 다양한 복잡한 프롬프트와 해당 이미지를 수집하고 사전 훈련된 BLIP를 활용하여 간단한 프롬프트를 생성합니다. 이를 통해 확산 모델은 간단한 프롬프트만으로 의미적으로 일관된 고품질 이미지를 생성할 수 있습니다. 3.1 데이터 수집 원시 데이터. 내용이 풍부하고 의미적으로 신뢰할 수 있는 데이터 세트를 구성하기 위해 신뢰할 수 있는 프롬프트와 고품질 이미지가 있는 다양한 오픈 소스 이미지 생성 웹사이트를 광범위하게 조사합니다. 그 중에서 Lexica 1, civitai 2, Stable Diffusion Online 3의 세 웹사이트를 선택했습니다. 이러한 웹사이트에서 공개적으로 사용 가능한 이미지는 종종 의미적으로 정확하고 복잡한 프롬프트가 있는 고품질입니다. 따라서 우리는 웹사이트의 프롬프트를 복잡한 프롬프트로 수집합니다. 총 114,148개의 이미지-텍스트 쌍을 수집합니다.데이터 정리. SURD에서 각 샘플의 올바른 의미적 일치를 보장하기 위해 두 단계로 데이터 정리를 수행합니다.첫 번째 단계에서는 BLIP[13]에서 생성된 단순 프롬프트의 의미적 정확성을 보장하기 위해 공개적으로 사용 가능한 사전 학습된 모델 CLIP[34]을 의미적 정리에 사용합니다.대부분 확산 모델의 텍스트 인코더는 섹션 4.1에서 설명할 CLIP 모델의 텍스트 인코더이기 때문입니다.CLIP 모델이 해당 이미지의 의미와 일치하는 단순 프롬프트의 의미를 판단하는 경우 확산 모델은 단순 프롬프트에 따라 유사한 이미지를 생성할 가능성이 높습니다.각 이미지에 대해 CLIP에 단순 프롬프트와 복합 프롬프트를 분류하여 이미지와 의미적으로 가장 잘 일치하는 프롬프트를 선택하도록 요청합니다. 일반적으로 복잡한 프롬프트에는 이미지 품질 설명과 같이 의미적으로 무관한 다른 정보가 포함되는 경우가 많으므로 의미적으로 올바른 간단한 프롬프트는 일반적으로 복잡한 프롬프트보다 CLIP 점수가 높습니다. 따라서 간단한 프롬프트의 CLIP 점수가 해당 복잡한 프롬프트보다 낮지 않으면 샘플을 유지합니다. CLIP 점수에 따른 자동 의미 정리 후 66,408개 샘플을 유지합니다. 두 번째 단계에서는 첫 번째 단계에서 유지한 샘플을 수동으로 추가로 필터링하여 모든 이미지-텍스트 쌍이 의미적으로 일치하도록 합니다. 마지막으로 SURD에는 57,603개 이미지-텍스트 쌍이 포함되어 있으며 각 이미지-텍스트 쌍에는 이미지, 간단한 프롬프트 및 복잡한 프롬프트가 포함됩니다. ¹https://lexica.art, 2https://civitai.com, ³https://stable diffusionweb.com Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin LLM 지식. 우리는 텍스트 인코더의 의미적 이해와 추론 능력을 향상시키기 위해 LLM에서 지식을 추출하고자 하기 때문에 LLM의 단순 프롬프트에 대한 지식도 벡터로 저장합니다. 구체적으로, 우리는 세 가지 다른 매개변수 크기를 갖는 최근 오픈 소스화된 대규모 언어 모델 LLAMA[45]를 사용합니다: 7B(32개 레이어, 차원은 4096), 13B(40개 레이어, 차원은 5120), 33B(60개 레이어, 차원은 6656). 각 단순 프롬프트에 대해, 우리는 서로 다른 길이의 서로 다른 샘플을 균일하게 처리할 수 있도록 지식 표현으로 LLM에서 생성된 각 토큰 임베딩의 평균값을 계산합니다. 또한, 우리는 모든 이미지의 크기를 균일하게 512 × 512로 조정합니다. BLIP, CLIP 및 LLM 사용에 대한 자세한 내용은 부록에서 확인할 수 있습니다. 3. 데이터 분석 프롬프트 길이. 그림 4는 프롬프트에 대한 문장 길이의 분포를 보여줍니다. (a)는 복잡한 프롬프트에 대한 분포를 나타내고 (b)는 간단한 프롬프트에 대한 분포를 나타냅니다. 시각적 명확성을 높이기 위해 300단어보다 긴 프롬프트를 300단어로 통합했습니다. 간단한 프롬프트의 길이 분포는 비교적 집중되어 있으며 문장 길이는 인간 언어 패턴과 일치하는 10을 중심으로 합니다. 반면에 긴 꼬리 분포를 가진 복잡한 프롬프트는 의미를 포함할 뿐만 아니라 정의와 이미지 품질 정보도 포함하여 간단한 프롬프트보다 문장 길이가 상당히 길어집니다. 프롬프트 콘텐츠. 텍스트-이미지 생성을 위한 프롬프트에는 일반적으로 상당수의 명사가 포함되어 있으며 이미지는 서로 다른 객체로 구성되어 있기 때문에 생성된 이미지의 품질과 의미적 일관성에 큰 영향을 미칠 수 있습니다. 따라서 텍스트와 시각적 콘텐츠의 다양성을 보여주기 위해 SURD에서 발생하는 명사의 빈도 분포에 대한 통계적 분석을 수행합니다. 그림 4(c)는 SURD에서 선택한 엔터티의 빈도 비례 분포를 표시합니다. 이러한 개체는 사람, 동물, 식물 및 장면과 같은 다양한 범위의 일반 객체를 포괄하여 SURD의 콘텐츠 다양성을 나타냅니다. 게다가 이러한 개체의 다양성은 사전 훈련된 확산 모델이 더 복잡한 장면에서 텍스트와 시각적 콘텐츠에 대한 강력한 고수준 이해 능력을 가질 수 있게 합니다. 나아가, 우리는 SURD에서 텍스트 어휘의 전반적인 분포를 설명하기 위해 중지 단어를 필터링하여 그림 4(d)에 표시된 대로 텍스트의 워드 클라우드를 제시합니다. &quot;최고의 품질&quot;, &quot;최고의 걸작&quot; 및 &quot;매우 자세한&quot;와 같이 가장 자주 발생하는 문구는 주로 이미지 품질을 제한하고 복잡한 프롬프트에서 유래하므로 이러한 일관된 텍스트 제약 조건이 고품질 이미지 생성에 중요하다는 것을 나타냅니다. 따라서 복잡한 프롬프트의 의미적 표현은 미세 조정을 통해 SUR-어댑터로 확산 모델을 향상시키는 데 중요한 역할을 할 것입니다.
--- METHOD ---
ologies → 자연어 처리; 컴퓨터 비전; 머신 러닝 알고리즘. 키워드 확산 모델, 대규모 언어 모델, 멀티모달 이미지 생성, 어댑터, 지식 증류 서론 최근 몇 년 동안 확산 모델 기반 멀티모달 텍스트-이미지 생성 기술은 인상적인 진전을 이루었습니다[50]. 이러한 모델[38, 46]은 방대한 양의 데이터와 모델 매개변수로 학습되었으며, 사람들은 복잡한 페인팅 기술이 필요 없이 텍스트 프롬프트와 기타 정보를 통해 텍스트와 관련되고 시각적으로 매력적인 이미지를 처음부터 끝까지 생성할 수 있습니다. 그러나 사전 인쇄본, 기술 보고서, 신선하게 구운 파이 4개 빈티지 유리병 7개 키 큰 나무가 있는 푸른 숲 구불구불한 길을 따라 질주하는 강렬한 빨간색 스포츠카와 작은 폭포 복합 프롬프트 Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 단순 프롬프트 단순 프롬프트 + LLM (a) 의미 이해 (b) 상식적 추론 그림 2: 확산 모델에서 텍스트 인코더의 의미 이해 및 상식적 추론 기능. 이러한 기존 확산 모델에서 이미지 생성의 품질은 키워드 기반 텍스트 프롬프트 또는 기타 형태의 텍스트 프롬프트의 복잡하고 정교한 디자인에 크게 의존합니다. 또한 텍스트 프롬프트가 간결한 내러티브이거나 일상적인 표현인 짧은 문구인 경우 생성된 이미지의 충실도와 텍스트 관련성이 종종 크게 손상됩니다. 이러한 제한으로 인해 우수한 사용자 경험을 제공하는 간결한 내러티브로 확산 모델을 직관적으로 제어하기 어렵습니다. 이 문제의 가장 중요한 이유는 이러한 확산 모델의 텍스트 인코더(종종 이미지 텍스트 대조 학습으로 훈련된 사전 훈련된 CLIP[34]의 텍스트 인코더)가 텍스트-이미지 생성 작업에 정렬되지 않아 이미지 생성에 대한 의미 이해 및 추론(SUR)이 좋지 않다는 것입니다. 구체적으로, CLIP은 대조 학습을 사용하여 약 400M개의 이미지-텍스트 쌍에서 훈련된 다중 모달 신경 모델이며, 이미지 인코더와 텍스트 인코더는 이미지와 텍스트 간의 연관성을 성공적으로 연결할 수 있기 때문에 확산 모델과 같은 다양한 다중 모달 작업이나 모델에 널리 적용되었습니다. CLIP의 학습 목표는 매칭된 이미지와 텍스트 쌍을 특징 공간에서 더 가깝게 끌어서 이미지-텍스트 대응 관계를 설정하는 것이지만, 해당 이미지를 설명하는 텍스트가 짧고 이미지에서 일부 의미만 일치할 수 있어 텍스트 인코더에서 생성된 불완전한 특징이 발생할 수 있습니다. 그러나 텍스트-이미지 생성 과제는 텍스트 인코더가 간결한 내러티브의 의미를 이해할 뿐만 아니라 내러티브에 근거한 암묵적 상식이나 지식을 추론하고 완성하여 모델이 내러티브와 매우 일관된 정확한 이미지를 생성할 수 있도록 요구합니다. 따라서 조건부 텍스트-이미지 생성을 위해 CLIP의 텍스트 인코더를 확산 모델에 임베드하면 텍스트 인코더의 의미 이해 및 상식적 추론 능력이 부족하여 입력 텍스트가 자연어인 경우 낮은 품질의 이미지가 생성됩니다. 이러한 결함을 보여주기 위해 먼저 다중 모달 시각적 질문 답변[1, 7, 8, 31]에서 세 가지 일반적인 유형의 텍스트 프롬프트인 &quot;계산&quot;, &quot;색상&quot; 및 &quot;동작&quot;을 사용하여 확산 모델에서 텍스트 인코더의 의미 이해 능력을 평가합니다. 표 1에서 볼 수 있듯이 각 표에 대해 세 가지 다른 프롬프트를 설계했습니다. 1: 확산 모델을 사용하여 간단한 프롬프트로 생성된 이미지의 의미 정확도(Acc.) 평가. 간단한 프롬프트는 &quot;세기&quot;, &quot;색상&quot;, &quot;행동&quot;을 포함한 세 가지 유형의 문장으로 구성되었습니다. 각 프롬프트는 이미지를 생성했으며 이미지는 의미적 정확성을 위해 수동으로 검사되었습니다. 결과에 따르면 대부분 프롬프트의 의미적 정확성은 50% 미만이었고 두 가지 유형의 프롬프트조차도 의미적 정확성 비율이 0%였습니다. 유형 프롬프트 정확도 &lt; 50%? 갓 구운 파이 4개. 63.08% 세기 그림 같은 풍경 위에 떠 있는 다채로운 풍선 6개. 빈티지 유리병 7개. 8.46% ✓ 0.00% ✓ 색상 구불구불한 길을 질주하는 선명한 빨간색 스포츠카. 붉은 주스가 담긴 파란색 유리잔. 86.15% 17.69% ✓ 파란색과 노란색 단색 옷을 입은 커플. 0.00% ✓ 행동 운동장에서 누군가 농구를 치고 있습니다. 나무를 먹는 기린. 41.54% ✓ 25.38% 주방에서 피자 반죽을 공중에 던지는 요리사. 15.38% ✓ 입력하고, 각 텍스트 프롬프트에 대해 텍스트-이미지 확산 모델[38]을 사용하여 130개의 이미지를 생성하고 생성된 이미지가 주어진 텍스트 프롬프트의 의미적 의미를 충족하는지 수동으로 평가했습니다. 결과에 대한 통계적 분석을 통해 대부분의 텍스트 프롬프트에 대한 의미적 이해의 정확도가 50%를 넘지 않는다는 것을 발견했습니다. 놀랍게도 &quot;7개의 빈티지 유리병&quot;과 &quot;각각 파란색과 노란색 단색 옷을 입은 커플&quot;과 같이 겉보기에 간단한 내러티브 프롬프트조차 정확도가 0%였습니다. 이는 확산 모델의 텍스트 인코더가 이미지 생성을 위해 이러한 간단한 텍스트의 의미를 전혀 이해하지 못하고 심각한 정보 편향을 초래했음을 나타냅니다. 그림 2(a)는 의미적 이해 능력이 부족하여 발생하는 의미적 오류의 예를 추가로 보여줍니다. 다음으로 텍스트 인코더의 상식적 추론 능력을 고려합니다. 안정적인 확산 모델이 널리 검증된 생성 기술에 따라 아름다운 고양이를 생성하기를 바란다면, 다음 프롬프트와 같이 고품질의 생성된 이미지를 얻기 위해 복잡하고 정교한 키워드 기반 프롬프트가 필요합니다. SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models 사전 인쇄본, 기술 보고서(복잡한 프롬프트 예) 8k uhd, RAW 사진, 아름다운 고양이(사실적:1.1), 걸작, RAW 사진, 실제 고양이, RAW 사진, 초고해상도, 사실적, 최상의 품질(고도의 피부, 피부 세부 정보), 보이는 모공, 빛나는 피부, 매우 섬세하고 아름다운, 매우 자세한 8K 배경 화면, 8k 고품질, 필름 그레인, DSLR, 아름다운 세부 정보가 있는 아름다운 고양이(시청자를 바라보는), 전문 사진 조명, 매우 자세한 눈과 얼굴, 아름다운 세부 정보가 있는 눈, 아날로그 스타일, 귀엽고 장난기 있는, 사랑스럽다(화려하고 다채로움:1.1), 고양이의 초상화<lora:mikeneko:0.7> , from side, full body, (brown black white fur) 그림 2(b)와 같이 복잡한 프롬프트를 사용하여 생성된 이미지가 &quot;아름다운 고양이&quot;와 같은 간단한 프롬프트를 사용하여 생성된 이미지에 비해 세부 정보가 더 좋고 윤곽선이 더 정확하며 상식이 정확하다는 것을 알 수 있습니다. (간단한 프롬프트 예) 아름다운 고양이 복잡한 프롬프트를 입력하는 것은 &quot;아름다운&quot;과 &quot;고양이&quot; 사이의 세부 정보와 이해를 텍스트 인코더에 직접 주입하는 것과 같으며, 확산 모델이 기분 좋은 &quot;아름다운 고양이&quot;를 생성할 수 있도록 합니다. 이는 확산 모델이 의미적으로 의미 있는 이미지를 생성할 수 있는 잠재력이 있지만 텍스트 인코더의 상식적 추론 능력에 의해 제한됨을 나타냅니다. 간단한 프롬프트는 텍스트 인코더가 &quot;아름다운 고양이&quot;의 의미를 직접 이해할 수 없게 하며, 인코더 자신의 지식에서 &quot;아름다운&quot;의 의미를 추론할 수도 없습니다. 이러한 문제에 직면하여 ChatGPT 및 LLAMA[45]와 같은 대규모 언어 모델(LLM)의 최근 발전은 향상된 SUR 능력과 함께 놀라운 대화 기능을 보여주어 자연어 처리(NLP) 분야에서 새로운 경지를 창출했습니다. 따라서 ChatGPT를 사용하여 &quot;아름다운 고양이&quot;를 설명하려고 시도했고 다음과 같은 텍스트를 얻었습니다. (LLM의 상식적 추론) 고양이는 부드러운 털, 표현력이 풍부한 눈, 우아한 움직임으로 매혹적인 아름다움으로 유명합니다. 아름다운 고양이는 독특한 패턴의 매끈한 털, 꿰뚫는 눈, 우아한 자세와 같은 독특한 특징을 가질 수 있습니다. 각 고양이는 고유한 방식으로 독특하며, 그들의 아름다움은 보는 사람의 관점에 따라 주관적입니다. 이 텍스트는 LLM의 &quot;아름다움&quot;과 &quot;고양이&quot;에 대한 이해와 어떤 종류의 &quot;고양이&quot;가 &quot;아름답다&quot;고 여겨지는지에 대한 상식적 추론을 보여줍니다. 이 텍스트에서 생성된 이미지는 그림 2(b) 오른쪽 아래에 표시된 것처럼 복잡한 프롬프트를 사용하여 생성된 이미지와 품질이 비슷합니다. 위의 모든 사례 연구는 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하여 확산 모델이 간단한 내러티브 프롬프트를 사용하더라도 의미적으로 정확하고 고품질의 이미지를 생성할 수 있는지 고려하도록 영감을 줍니다. 이 목표를 달성하기 위해 이 논문에서는 먼저 57,000개 이상의 의미적으로 수정된 이미지-텍스트 쌍으로 구성된 SURD라는 새로운 데이터 세트를 수집하여 주석을 달았습니다. 각 이미지-텍스트 쌍에는 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 고품질 이미지가 포함되어 있습니다. SURD를 활용하여 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하고 간단하고 복잡한 프롬프트의 표현을 정렬하는 SUR 어댑터를 제안합니다. 광범위한
--- EXPERIMENT ---
s는 여러 LLM과 인기 있는 사전 학습된 확산을 통합하여 *두 저자 모두 이 연구에 동등하게 기여했습니다. 책임 저자. 모델을 사용하여 이미지 품질 저하 없이 확산 모델이 간결한 자연어를 이해하고 추론할 수 있도록 하는 접근 방식의 효과를 보여줍니다. 저희의 접근 방식은 더 나은 사용자 경험으로 텍스트-이미지 확산 모델을 사용하기 쉽게 만들 수 있으며, 이는 저희의 접근 방식이 간단한 내러티브 프롬프트와 복잡한 키워드 기반 프롬프트 간의 의미적 격차를 메움으로써 사용자 친화적인 텍스트-이미지 생성 모델 개발을 더욱 발전시킬 수 있는 잠재력이 있음을 보여줍니다. 코드는 https://github.com/Qrange-group/SUR-adapter에서 공개됩니다. CCS 개념 • ⚫ 컴퓨팅 방법론 → 자연어 처리; 컴퓨터 비전; 머신 러닝 알고리즘. 키워드 확산 모델, 대규모 언어 모델, 멀티모달 이미지 생성, 어댑터, 지식 증류 서론 최근 몇 년 동안 확산 모델 기반 멀티모달 텍스트-이미지 생성 기술은 인상적인 진전을 이루었습니다[50]. 이러한 모델[38, 46]은 방대한 양의 데이터와 모델 매개변수로 학습되었기 때문에 사람들은 복잡한 페인팅 기술 없이도 텍스트 프롬프트와 기타 정보를 통해 텍스트와 관련되고 시각적으로 매력적인 이미지를 처음부터 끝까지 생성할 수 있습니다.그러나 사전 인쇄본, 기술 보고서, 신선하게 구운 파이 4개 빈티지 유리병 7개 키 큰 나무가 있는 푸른 숲 구불구불한 길을 따라 달리는 생생한 빨간색 스포츠카와 작은 폭포 복합 프롬프트 Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 단순 프롬프트 단순 프롬프트 + LLM (a) 의미 이해 (b) 상식적 추론 그림 2: 확산 모델에서 텍스트 인코더의 의미 이해 및 상식적 추론 기능.이러한 기존 확산 모델에서 이미지 생성의 품질은 키워드 기반 텍스트 프롬프트 또는 기타 형태의 텍스트 프롬프트의 복잡하고 정교한 디자인에 크게 의존합니다. 또한 텍스트 프롬프트가 간결한 내러티브나 일상적인 표현인 짧은 문구인 경우 생성된 이미지의 충실도와 텍스트 관련성이 종종 크게 손상됩니다. 이러한 제한으로 인해 우수한 사용자 경험을 제공하는 간결한 내러티브로 확산 모델을 직관적으로 제어하기 어렵습니다. 이 문제의 가장 중요한 이유는 이러한 확산 모델의 텍스트 인코더(종종 이미지 텍스트 대조 학습으로 학습된 사전 학습된 CLIP[34]의 텍스트 인코더)가 텍스트-이미지 생성 작업에 정렬되지 않아 이미지 생성에 대한 의미 이해 및 추론(SUR)이 좋지 않기 때문입니다. 구체적으로 CLIP은 대조 학습을 사용하여 약 400M개의 이미지-텍스트 쌍으로 학습된 다중 모달 신경 모델이며, 이미지 인코더와 텍스트 인코더는 이미지와 텍스트 간의 연관성을 성공적으로 연결할 수 있기 때문에 확산 모델과 같은 다양한 다중 모달 작업이나 모델에 널리 적용되었습니다. CLIP의 학습 목표는 매칭된 이미지와 텍스트 쌍을 피처 공간에서 더 가깝게 끌어서 이미지-텍스트 대응 관계를 확립하는 것이지만, 해당 이미지를 설명하는 텍스트는 간략하여 이미지의 일부 의미와만 일치할 수 있으며, 그 결과 텍스트 인코더에서 생성된 불완전한 피처가 생성됩니다. 그러나 텍스트-이미지 생성 과제는 텍스트 인코더가 간결한 내러티브의 의미를 이해할 수 있을 뿐만 아니라 내러티브에 근거한 암묵적인 상식이나 지식을 추론하고 완성하여 모델이 내러티브와 매우 일관된 정확한 이미지를 생성할 수 있도록 요구합니다. 따라서 조건부 텍스트-이미지 생성을 위해 CLIP의 텍스트 인코더를 확산 모델에 임베드하면 텍스트 인코더에서 의미 이해 및 상식적 추론 능력이 부족하여 입력 텍스트가 자연어인 경우 낮은 품질의 이미지가 생성됩니다. 이러한 결함을 보여주기 위해 먼저 다중 모달 시각적 질의응답에서 세 가지 일반적인 유형의 텍스트 프롬프트[1, 7, 8, 31]를 사용하여 확산 모델에서 텍스트 인코더의 의미 이해 능력을 평가합니다. &quot;세기&quot;, &quot;색상&quot; 및 &quot;동작&quot;. 표 1에서 볼 수 있듯이 각 표에 대해 세 가지 다른 프롬프트를 설계했습니다. 표 1: 확산 모델을 사용하여 간단한 프롬프트로 생성된 이미지의 의미 정확도(Acc.) 평가. 간단한 프롬프트는 &quot;세기&quot;, &quot;색상&quot; 및 &quot;동작&quot;을 포함한 세 가지 유형의 문장으로 구성되었습니다. 각 프롬프트는 이미지를 생성했으며 이미지는 의미 정확도를 위해 수동으로 검사되었습니다. 결과에 따르면 대부분 프롬프트의 의미 정확도는 50% 미만이며 두 가지 유형의 프롬프트조차도 의미 정확도율이 0%입니다. 유형 프롬프트 Acc. &lt; 50%? 갓 구운 파이 4개. 63.08% 세기 그림 같은 풍경 위에 떠 있는 다채로운 풍선 6개. 빈티지 유리 병 7개. 8.46% ✓ 0.00% ✓ 색상 구불구불한 길을 질주하는 강렬한 빨간색 스포츠카.빨간 주스가 담긴 파란색 유리잔.86.15% 17.69% ✓ 파란색과 노란색 단색 옷을 입은 커플.0.00% ✓ 동작 운동장에서 누군가 농구를 슛하고 있습니다.나무를 먹는 기린.41.54% ✓ 25.38% 주방에서 요리사가 피자 반죽을 공중에 던지고 있습니다.15.38% ✓ 유형, 각 텍스트 프롬프트에 대해 텍스트-이미지 확산 모델[38]을 사용하여 130개의 이미지를 생성하고 생성된 이미지가 주어진 텍스트 프롬프트의 의미적 의미를 충족하는지 수동으로 평가했습니다.결과에 대한 통계 분석을 통해 대부분의 텍스트 프롬프트에 대한 의미적 이해의 정확도가 50%를 초과하지 않는다는 것을 발견했습니다. 놀랍게도 &quot;7개의 빈티지 유리병&quot;과 &quot;각각 파란색과 노란색 단색 옷을 입은 커플&quot;과 같은 겉보기에 간단한 내러티브 프롬프트조차도 정확도가 0%로, 확산 모델의 텍스트 인코더가 이미지 생성을 위해 이러한 간단한 텍스트의 의미를 전혀 이해하지 못하고 심각한 정보 편향을 초래했음을 나타냅니다. 그림 2(a)는 의미 이해 능력이 부족하여 발생하는 의미 오류의 예를 추가로 보여줍니다. 다음으로 텍스트 인코더의 상식적 추론 능력을 고려합니다. 안정적인 확산 모델이 널리 검증된 생성 기술에 따라 아름다운 고양이를 생성하기를 바란다면, 다음 프롬프트와 같이 고품질의 생성된 이미지를 얻기 위해 복잡하고 정교한 키워드 기반 프롬프트가 필요합니다. SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models 사전 인쇄본, 기술 보고서(복잡한 프롬프트 예) 8k uhd, RAW 사진, 아름다운 고양이(사실적:1.1), 걸작, RAW 사진, 실제 고양이, RAW 사진, 초고해상도, 사실적, 최상의 품질(고도의 피부, 피부 세부 정보), 보이는 모공, 빛나는 피부, 매우 섬세하고 아름다운, 매우 자세한 8K 배경 화면, 8k 고품질, 필름 그레인, DSLR, 아름다운 세부 정보가 있는 아름다운 고양이(시청자를 바라보는), 전문 사진 조명, 매우 자세한 눈과 얼굴, 아름다운 세부 정보가 있는 눈, 아날로그 스타일, 귀엽고 장난기 있는, 사랑스럽다(화려하고 다채로움:1.1), 고양이의 초상화<lora:mikeneko:0.7> , from side, full body, (brown black white fur) 그림 2(b)와 같이 복잡한 프롬프트를 사용하여 생성된 이미지가 &quot;아름다운 고양이&quot;와 같은 간단한 프롬프트를 사용하여 생성된 이미지에 비해 세부 정보가 더 좋고 윤곽선이 더 정확하며 상식이 정확하다는 것을 알 수 있습니다. (간단한 프롬프트 예) 아름다운 고양이 복잡한 프롬프트를 입력하는 것은 &quot;아름다운&quot;과 &quot;고양이&quot; 사이의 세부 정보와 이해를 텍스트 인코더에 직접 주입하는 것과 같으며, 확산 모델이 기분 좋은 &quot;아름다운 고양이&quot;를 생성할 수 있도록 합니다. 이는 확산 모델이 의미적으로 의미 있는 이미지를 생성할 수 있는 잠재력이 있지만 텍스트 인코더의 상식적 추론 능력에 의해 제한됨을 나타냅니다. 간단한 프롬프트는 텍스트 인코더가 &quot;아름다운 고양이&quot;의 의미를 직접 이해할 수 없게 하며, 인코더 자신의 지식에서 &quot;아름다운&quot;의 의미를 추론할 수도 없습니다. 이러한 문제에 직면하여 ChatGPT 및 LLAMA[45]와 같은 대규모 언어 모델(LLM)의 최근 발전은 향상된 SUR 능력과 함께 놀라운 대화 기능을 보여주어 자연어 처리(NLP) 분야에서 새로운 경지를 창출했습니다. 따라서 ChatGPT를 사용하여 &quot;아름다운 고양이&quot;를 설명하려고 시도했고 다음과 같은 텍스트를 얻었습니다. (LLM의 상식적 추론) 고양이는 부드러운 털, 표현력이 풍부한 눈, 우아한 움직임으로 매혹적인 아름다움으로 유명합니다. 아름다운 고양이는 독특한 패턴의 매끈한 털, 꿰뚫는 눈, 우아한 자세와 같은 독특한 특징을 가질 수 있습니다. 각 고양이는 고유한 방식으로 독특하며, 그들의 아름다움은 보는 사람의 관점에 따라 주관적입니다. 이 텍스트는 LLM의 &quot;아름다움&quot;과 &quot;고양이&quot;에 대한 이해와 어떤 종류의 &quot;고양이&quot;가 &quot;아름답다&quot;고 여겨지는지에 대한 상식적 추론을 보여줍니다. 이 텍스트에서 생성된 이미지는 그림 2(b) 오른쪽 아래에 표시된 것처럼 복잡한 프롬프트를 사용하여 생성된 이미지와 품질이 비슷합니다. 위의 모든 사례 연구는 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하여 확산 모델이 간단한 내러티브 프롬프트를 사용하더라도 의미적으로 정확하고 고품질의 이미지를 생성할 수 있는지 고려하도록 영감을 줍니다. 이 목표를 달성하기 위해 이 논문에서는 먼저 57,000개 이상의 의미적으로 수정된 이미지-텍스트 쌍으로 구성된 SURD라는 새로운 데이터 세트를 수집하여 주석을 달았습니다. 각 이미지-텍스트 쌍에는 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 고품질 이미지가 포함되어 있습니다. SURD를 활용하여 LLM의 SUR 능력을 사전 훈련된 확산 모델로 전환하고 간단하고 복잡한 프롬프트의 표현을 정렬하는 SUR 어댑터를 제안합니다. 광범위한 실험과 통계적 테스트를 통해 제안된 SUR 어댑터가 사전 훈련된 확산 모델의 텍스트 인코더를 크게 향상시키고 간결한 내러티브 프롬프트와 생성된 이미지 간의 불일치를 완화하는 고품질 이미지를 생성한다는 것이 확인되었습니다. 요약하자면, 우리의 기여는 세 가지입니다.• 57,000개 이상의 의미적으로 수정된 이미지-텍스트 쌍을 포함하는 데이터 세트 SURD를 수집하고 주석을 달았습니다. 각 이미지 텍스트 쌍에는 간단한 프롬프트, 복잡한 프롬프트 및 고품질의 해당 이미지가 포함되어 있습니다.• SURD를 기반으로 LLM의 의미적 이해 및 추론 능력을 사전 훈련된 확산 모델로 효과적으로 전환하여 간단한 프롬프트로 생성된 의미적 불일치 및 저품질 이미지 문제를 완화하는 SUR 어댑터를 제안합니다.• 제안된 SUR 어댑터를 사용하여 생성된 이미지에 대한 광범위한 통계적 테스트와 논의를 수행하여 효과를 분석하고 한계를 추가로 논의합니다. 관련 연구 2.1 텍스트-이미지 확산 확산 모델은 텍스트-이미지 생성에 광범위하게 활용되었습니다[2, 11, 23, 25, 38, 41, 46]. 텍스트-이미지 확산은 텍스트 입력을 확산 모델의 조건화 신호로 활용하여 노이즈 추가 및 제거 프로세스를 통해 텍스트 관련 이미지를 생성합니다[38]. 텍스트-이미지 확산의 텍스트 인코더는 종종 CLIP[34]과 같은 사전 학습된 언어 모델을 활용하여 텍스트 입력을 잠재 벡터로 인코딩하여 수행됩니다. 텍스트-이미지 확산은 이미지 초해상도[27, 42], 인페인팅[32], 조작[5, 54], 의미 분할[4, 12], 비디오 생성[51, 56] 등 다양한 분야에서 널리 사용됩니다. 2.2 대규모 언어 모델 최근 NLP 분야에서는 LLM이 급증했습니다[17]. Jozefowicz 등[21]은 LSTM을 10억 개의 매개변수로 확장하여 Billion Word 벤치마크에서 최첨단 결과를 달성했습니다. 이후 확장 변환기로 인해 많은 NLP 작업이 개선되었으며, 주목할 만한 모델로는 BERT[10], GPT-2[35], MegatronLM[43], T5[37]가 있습니다. 1,750억 개의 매개변수를 가진 모델인 GPT-3[6]의 도입은 이 분야에서 중요한 획기적인 진전을 의미하며, Jurassic-1[29], Megatron-Turing NLG[44], Gopher[36], Chinchilla[17], PALM[9], OPT[57], GLM[52] 및 LLAMA[45]와 같은 수많은 LLM의 개발로 이어졌습니다. 또한 여러 연구[15, 17, 22, 40, 49]에서 LLM 성능에 대한 스케일링의 영향을 조사하여 사용 편의성을 향상시켰습니다. 3 의미 이해 및 추론 데이터 세트 SURD는 그림 3에서 볼 수 있듯이 간단한 내러티브 프롬프트, 복잡한 키워드 기반 프롬프트 및 의미적으로 올바른 이미지의 57,603개 3중항으로 구성된 멀티 모달 데이터 세트입니다.저희가 아는 한, SURD는 간단하고 복잡한 프롬프트를 모두 기록하고 텍스트-이미지 확산 모델의 SUR 문제를 해결하는 데 도움이 되는 의미적으로 올바른 이미지-텍스트 쌍을 제공하는 데 중점을 둔 최초의 데이터 세트입니다.사전 인쇄본, 기술 보고서, BLIP 간단한 프롬프트: 파란색 배경에 큰 눈을 가진 다채로운 동물 복잡한 프롬프트: 귀여운 생물 스타일의 전신 클로즈업 초상화 작고 귀여운 아이소메트릭 알레브리헤 이모티콘, 부드럽고 매끄러운 조명, 굵은 평면 색상, 3D 아이콘 클레이 렌더, 100mm 렌즈, 3D 블렌더 렌더, 자연 환경 풍경, 멕시코에서 영감을 받은 배경, 폴리카운트에서 트렌드, 모듈식 구성주의, 물리 기반 렌더링, 기하학적, 중심, 동적 포즈. alebrije는 멕시코 민속 예술에서 영감을 얻어야 하며 사자, 독수리 또는 뱀과 같은 여러 다른 동물의 조합이어야 합니다., solo Diffusion models 그림 3: SURD의 예. 우리는 공개적으로 사용 가능한 웹사이트에서 확산 모델에 의해 생성된 다양한 복잡한 프롬프트와 해당 이미지를 수집하고 사전 훈련된 BLIP를 활용하여 간단한 프롬프트를 생성합니다. 이를 통해 확산 모델은 간단한 프롬프트만으로 의미적으로 일관된 고품질 이미지를 생성할 수 있습니다. 3.1 데이터 수집 원시 데이터. 내용이 풍부하고 의미적으로 신뢰할 수 있는 데이터 세트를 구성하기 위해 신뢰할 수 있는 프롬프트와 고품질 이미지가 있는 다양한 오픈 소스 이미지 생성 웹사이트를 광범위하게 조사합니다. 그 중에서 Lexica 1, civitai 2, Stable Diffusion Online 3의 세 웹사이트를 선택했습니다. 이러한 웹사이트에서 공개적으로 사용 가능한 이미지는 종종 의미적으로 정확하고 복잡한 프롬프트가 있는 고품질입니다. 따라서 우리는 웹사이트의 프롬프트를 복잡한 프롬프트로 수집합니다. 총 114,148개의 이미지-텍스트 쌍을 수집합니다.데이터 정리. SURD에서 각 샘플의 올바른 의미적 일치를 보장하기 위해 두 단계로 데이터 정리를 수행합니다.첫 번째 단계에서는 BLIP[13]에서 생성된 단순 프롬프트의 의미적 정확성을 보장하기 위해 공개적으로 사용 가능한 사전 학습된 모델 CLIP[34]을 의미적 정리에 사용합니다.대부분 확산 모델의 텍스트 인코더는 섹션 4.1에서 설명할 CLIP 모델의 텍스트 인코더이기 때문입니다.CLIP 모델이 해당 이미지의 의미와 일치하는 단순 프롬프트의 의미를 판단하는 경우 확산 모델은 단순 프롬프트에 따라 유사한 이미지를 생성할 가능성이 높습니다.각 이미지에 대해 CLIP에 단순 프롬프트와 복합 프롬프트를 분류하여 이미지와 의미적으로 가장 잘 일치하는 프롬프트를 선택하도록 요청합니다. 일반적으로 복잡한 프롬프트에는 이미지 품질 설명과 같이 의미적으로 무관한 다른 정보가 포함되는 경우가 많으므로 의미적으로 올바른 간단한 프롬프트는 일반적으로 복잡한 프롬프트보다 CLIP 점수가 높습니다. 따라서 간단한 프롬프트의 CLIP 점수가 해당 복잡한 프롬프트보다 낮지 않으면 샘플을 유지합니다. CLIP 점수에 따른 자동 의미 정리 후 66,408개 샘플을 유지합니다. 두 번째 단계에서는 첫 번째 단계에서 유지한 샘플을 수동으로 추가로 필터링하여 모든 이미지-텍스트 쌍이 의미적으로 일치하도록 합니다. 마지막으로 SURD에는 57,603개 이미지-텍스트 쌍이 포함되어 있으며 각 이미지-텍스트 쌍에는 이미지, 간단한 프롬프트 및 복잡한 프롬프트가 포함됩니다. ¹https://lexica.art, 2https://civitai.com, ³https://stable diffusionweb.com Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin LLM 지식. 우리는 텍스트 인코더의 의미적 이해와 추론 능력을 향상시키기 위해 LLM에서 지식을 추출하고자 하기 때문에 LLM의 단순 프롬프트에 대한 지식도 벡터로 저장합니다. 구체적으로, 우리는 세 가지 다른 매개변수 크기를 갖는 최근 오픈 소스화된 대규모 언어 모델 LLAMA[45]를 사용합니다: 7B(32개 레이어, 차원은 4096), 13B(40개 레이어, 차원은 5120), 33B(60개 레이어, 차원은 6656). 각 단순 프롬프트에 대해, 우리는 서로 다른 길이의 서로 다른 샘플을 균일하게 처리할 수 있도록 지식 표현으로 LLM에서 생성된 각 토큰 임베딩의 평균값을 계산합니다. 또한, 우리는 모든 이미지의 크기를 균일하게 512 × 512로 조정합니다. BLIP, CLIP 및 LLM 사용에 대한 자세한 내용은 부록에서 확인할 수 있습니다. 3. 데이터 분석 프롬프트 길이. 그림 4는 프롬프트에 대한 문장 길이의 분포를 보여줍니다. (a)는 복잡한 프롬프트에 대한 분포를 나타내고 (b)는 간단한 프롬프트에 대한 분포를 나타냅니다. 시각적 명확성을 높이기 위해 300단어보다 긴 프롬프트를 300단어로 통합했습니다. 간단한 프롬프트의 길이 분포는 비교적 집중되어 있으며 문장 길이는 인간 언어 패턴과 일치하는 10을 중심으로 합니다. 반면에 긴 꼬리 분포를 가진 복잡한 프롬프트는 의미를 포함할 뿐만 아니라 정의와 이미지 품질 정보도 포함하여 간단한 프롬프트보다 문장 길이가 상당히 길어집니다. 프롬프트 콘텐츠. 텍스트-이미지 생성을 위한 프롬프트에는 일반적으로 상당수의 명사가 포함되어 있으며 이미지는 서로 다른 객체로 구성되어 있기 때문에 생성된 이미지의 품질과 의미적 일관성에 큰 영향을 미칠 수 있습니다. 따라서 텍스트와 시각적 콘텐츠의 다양성을 보여주기 위해 SURD에서 발생하는 명사의 빈도 분포에 대한 통계적 분석을 수행합니다. 그림 4(c)는 SURD에서 선택한 엔터티의 빈도 비례 분포를 표시합니다. 이러한 엔티티는 사람, 동물, 식물 및 장면과 같은 다양한 범위의 일반 객체를 포괄하여 SURD의 콘텐츠 다양성을 나타냅니다. 게다가 이러한 엔티티의 다양성으로 인해 사전 훈련된 확산 모델이 더 복잡한 장면에서 텍스트와 시각적 콘텐츠에 대한 강력한 고수준 이해 능력을 가질 수 있습니다. 나아가, 우리는 SURD에서 텍스트 어휘의 전반적인 분포를 설명하기 위해 중지 단어를 필터링하여 그림 4(d)와 같이 텍스트의 워드 클라우드를 제공합니다. &quot;최고의 품질&quot;, &quot;최고의 걸작&quot; 및 &quot;매우 세부적&quot;과 같이 가장 자주 발생하는 구문은 주로 이미지 품질을 제한하고 복잡한 프롬프트에서 유래하므로 이러한 일관된 텍스트 제약 조건이 고품질 이미지 생성에 중요하다는 것을 나타냅니다. 따라서 복잡한 프롬프트의 의미적 표현은 미세 조정을 통해 SUR-어댑터로 확산 모델을 향상시키는 데 중요한 역할을 할 것입니다. 방법 이 섹션에서는 SUR-어댑터가 대규모 언어 모델의 의미적 이해 및 추론 기능을 어떻게 전달하고 복잡한 프롬프트와 간단한 프롬프트 간의 표현 정렬을 달성하는지 소개합니다. (a)(b) SUR-어댑터: 대규모 언어 모델을 사용하여 텍스트-이미지 사전 학습 확산 모델 향상 빈도(%)단순 프롬프트의 단어 수복합 프롬프트의 단어 수빈도(%) (c) 3.3.2.2.1.1.0.0.hair eyes woman lighting dress portrait shot shirt skirt artstation _ field mouth sleeves (d) quality masterpiece full body sharp focus high quality best quality detail face unity tricate quality ultra 8k lighting aliping blue eye masterpiece best short detail eye ultra detail!extra detail viewer long-ha detail cg black hair 8k wallpaper very detail ainting jacket painting_ sky night outdoors film citycowboyjewelry ribbon ornament school armor studio street flowercathat bed beach horns sweater 대규모 언어 모델(LLM) 단순 프롬프트 복합 프롬프트 -그림 4: (왼쪽) 프롬프트 길이 분포 및 (오른쪽) 프롬프트 내용 분포. → Forward Frozen Transformer LLM -Distillation Learnable FCN Adapter Adapter lLLM Text EncoderlCP Attention Predictor Image Noise Pre-trained Diffusion Model Noise Text Encoder Q 그림 5: SUR-adapter의 그림. FCN은 완전히 연결된 네트워크입니다. (왼쪽) 사전 학습된 확산 모델을 위한 미세 조정 파이프라인. 사전 학습된 확산 모델이 주어지면 어댑터를 사용하여 대규모 언어 모델의 의미적 이해 및 추론 기능을 전달하고 복잡하고 간단한 프롬프트 간의 표현을 맞춥니다. 가중치 계수 ŋ은 어댑터의 효과를 조정하는 데 사용됩니다. (오른쪽) 어댑터의 네트워크 구조. 4.1 예비 확산 모델은 일반적으로 두 단계로 구성된 다중 모달 이미지 생성을 위한 탁월한 방법입니다. (1) 전방 노이즈 프로세스. 학습 데이터 xo가 주어진 분포 p(x0)에서 나왔다고 가정하면, 확산 모델은 먼저 Xo에 T 라운드의 노이즈를 추가하여 시퀀스 X1, X2, XT를 얻습니다.다음과 같습니다.q(x+|xo) = N(x+; α+x0, σI), 이 Eq. (2)는 다음 손실 함수를 통해 효율적으로 최적화할 수 있습니다.simple (0) = E||€ – €0 (αt×0 + σƒ€, t) ||₁₂, (3) 여기서 ê()는 입력 xt에 추가된 노이즈 e를 예측하는 학습 가능한 신경망입니다.이 신경망이 잘 학습되면 xt = α+x0+ σ+€와 몇 가지 특정 샘플링 방법을 사용하여 xo를 추론할 수 있습니다.T → ∞ or가 충분히 커짐에 따라 XT는 정규 분포된 노이즈의 근사치로 볼 수 있습니다. 따라서 정규 분포에서 노이즈 €0를 무작위로 샘플링하고 신경망 εe(·), 예측자라고도 함(그림 5 참조)을 사용하여 이미지 ✰0를 생성할 수 있습니다. 제어 가능한 생성을 달성하기 위해 예측자에 조건 c를 추가할 수 있습니다. 즉, 예측자를 ε0(., c)로 다시 쓸 수 있습니다. 텍스트-이미지 생성 작업의 경우 조건 c는 일반적으로 CLIP의 텍스트 인코더와 같은 텍스트 인코더에서 텍스트 프롬프트에서 생성됩니다. 알고리즘 1 SUR-어댑터를 사용한 사전 학습된 확산 모델의 미세 조정 알고리즘. N i=1&#39; 1: 입력: 데이터 세트 SURD(p&#39;, P&#39;s, I₁)₁, 학습 가능한 변환 9(01) 및 어댑터 9 Ada(302); 고정 매개변수를 가진 대규모 언어 모델 fLLM 및 텍스트 인코더 fen. 학습 단계 To. 2: while 훈련 단계 To ≥ 0 do Eq.(5)에 의해 지식 증류 손실 LLM을 계산합니다.Eq.(7)에 의해 의미 정보 CLM을 측정합니다.I에 노이즈를 추가합니다.Eq.(1)에 의해 αtli + σte를 구합니다.Eq.(8)에 의해 간단한 ($)를 측정합니다.3: // LLM에서 지식 증류 4: 5: 6: // 성능 유지 관리 7: 8: CLLM 사용 9: 10: (1) 11: CLLM과 fEn(p½)을 사용하여 Eq.(9)에 의해 lcp(0) 측정 12: // 매개변수 업데이트 13: 14: 15: 여기서 e는 표준 정규 분포 N(0, 1)에서 샘플링되고 σ는 t에 따라 달라지는 주어진 노이즈 강도이며 at는 일반적으로 αt = 1 - σ로 설정됩니다. 이 지점에서 x₁ = α+0+0+€가 있습니다. (2) 역방향 잡음 제거 프로세스. 전방 잡음 프로세스에서 시퀀스 X1, X2, XT를 얻은 후, xt에서 xt로의 잡음 제거 프로세스는 P0(xt−1|xt) = N(xt−1; 10(xt ), 20(xt)), .... (2)로 모델링할 수 있습니다. 여기서 (xt) 및 Σ0(xt))는 예측 통계이고, 학습 가능한 매개변수입니다. 최근의 많은 연구[16, 38, 53]에서 // 표현 정렬 복합 프롬프트 pr로 fen(p) 측정 총 손실 계산(Eq.(10)으로 total(0)) 학습 가능한 매개변수 = [1, 2] 업데이트 {total(4) To To16: end while 17: return 4.2 SUR 어댑터의 미세 조정 파이프라인 이 섹션에서는 의미 이해 및 추론 어댑터라는 간단하면서도 효과적인 미세 조정 접근 방식을 소개합니다. 사전 인쇄, 기술 보고서, 사전 인쇄, 기술 보고서, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 표 2: 다양한 의미 메트릭 측면에서 섹션 5.1에 설명된 다양한 사전 학습된 모델과 제어된 방법의 평가 결과. CLIP 점수 사전 학습된 모델 제어된 방법 기준선 우리의 기준선 동작(%) 우리의 색상(%) 기준선 우리의 0.0.517 ↑ 75.80.67 ↑ 81.87.33 ↑ 계산(%) 기준선 14.Ours 36.67 ↑ ControlNet(canny) 0.0.76.84.67 ↑ 68.69.33 ↑ 96.94.DM(1.5), ControlNet(seg) 0.0.7.9.10.10.67 ↑ 40.62.00 ↑ LLM(13B) 프롬프트 가중치 0.0.514 ↑ 78.85.33 ↑ 91.88.43.58.00 ↑ MultiDiffusion 0.0.516 ↑ 74.88.6787.81.23.62.67 ↑ 자기 주의 안내 0.0.73.86.00 ↑ 86.86.67 ↑ 12.14.00 ↑ 0.0.490 ↑ 58.68.67 ↑ 82.88.00 ↑ 21.38.00 ↑ ControlNet(canny) 0.0.83.81.47.67.33 ↑ 74.86.DM(만화), ControlNet(세그먼트) 0.0.38.51.28.30.67 ↑ 45.62.00 ↑ LLM(13B) 프롬프트 가중치 0.0.84.79.88.91.33 ↑ 41.50.00 ↑ MultiDiffusion 0.0.587 ↑ 63.80.67 ↑ 88.87.Self-attention Guidance 0.0.560 ↑ 65.73.33 ↑ 72.86.00 ↑ 18.16.36.67 ↑ 39.N (SUR 어댑터) 제어 가능한 텍스트-이미지 확산 모델. SURD 데이터 세트에서 이미지-텍스트 쌍(p², p¹³‚¹)을 고려해 보겠습니다. 여기서 p²와 pis는 각각 i번째 고품질 및 의미적으로 올바른 이미지 Ii에 대한 복합 프롬프트와 단순 프롬프트입니다. 그림 5(왼쪽)에 표시된 것처럼, 먼저 대규모 언어 모델 fLLM, 텍스트 인코더 fen, 사전 학습된 확산 모델에서 예측자 fpre의 모든 학습 가능한 매개변수를 동결한 다음, 완전히 연결된 네트워크(FCN) 9(01)와 학습 가능한 매개변수와 $2를 갖는 어댑터 9Ada(; (2)의 두 개의 학습 가능한 신경망을 추가합니다. 4.2.1 LLM에 의한 지식 증류. 어댑터 9Ada(2)의 구조는 그림 5(오른쪽)에 표시되어 있으며, 완전히 연결된 신경망 또는 Transformer[47]를 사용하여 구현된 세 개의 학습 가능한 변환 h; (·)(j = 1, 2, 3)으로 구성됩니다. 텍스트 인코더의 출력 ƒn(p¹²)에 대해 Qi = h3[ƒn(ps)] 및 K; = h₂[ƒEn(p³)]를 구성하고 [10, 47]로 어텐션 값을 계산합니다. QiK √a attį = softmax( (4) 여기서 d는 Qi와 Ki의 특징 차원입니다. 간단한 프롬프트의 의미 정보가 직접 간섭받지 않도록 하기 위해 어떠한 변환 없이 V₁ = fɛn (p¹³)을 직접 설정합니다. 특히, LLM의 강력한 의미 이해 및 추론 기능을 att;에 내장하기 위해 다음 손실 함수를 통해 LLM에서 지식을 추출합니다. ILLM (0) = KL[WofLLM (P)/T, Qi/T], (5) 여기서 7은 일반적으로 1로 설정되는 온도이고 KL은 KL 발산입니다. Wo는 Kaiming 초기화를 사용하여 무작위로 초기화된 행렬이며 학습할 수 없으므로 fLLM(p()와 Q) 사이의 차원을 맞추는 동안 LLM의 의미 정보가 가능한 한 많이 예약됩니다. 게다가 V = Vi atti로 보정된 의미 정보를 얻습니다. 마지막으로 어댑터의 출력은 학습 가능한 변환 g(; 1)에 의해 변환되어 이전 연구 [14, 20, 58]와 같이 LLM 의미 기능이 있는 출력 CLLM을 얻습니다. 9 {9Ada (fen (P); 2); ¢1} = 9 {V; + V; + h₁[V{ + Vi]; 1}, (6) 예측기에 대한 의미 정보 입력은 다음과 같습니다. CLLM = ŋ · CLLM + (1 − ŋ) · ƒЕn(p&#39;s). (7) 여기서 n은 상수입니다. η 4.2.2 미세 조정 중 DM의 성능 유지. 성능을 유지하려면 영어: 미세 조정 중 확산 모델의 경우, Eq.(1)에 의해 이미지 I¿에 다양한 수준의 노이즈를 추가하고, Eq.(7)에서 얻은 의미 정보 특징 c&#39; LLM을 단순 프롬프트 pgs의 안내에 따라 예측자에 공급합니다. 사전 훈련된 확산 모델이 미세 조정 중에 새 이미지 I에 대해 충분한 노이즈 제거 기능을 유지하도록 하기 위해 다음 손실 함수를 최소화합니다. et simple ($) = E||e — ê(αtli + σte, t, c&#39;Ĺlm) ||² (8) 또한 추가된 어댑터의 안정적인 학습을 보장하고 학습 초기 단계에서 사전 훈련된 확산 모델에 미치는 부정적인 영향을 줄이기 위해 이전 연구 [19, 54]의 설정을 따라 매개변수 1의 행렬의 모든 요소를 0으로 초기화합니다. 4.2.3 복합 프롬프트와 단순 프롬프트 간의 표현 정렬 섹션 3의 설명에서 우리는 이미지 I¿가 pi에 의해 생성된 의미적으로 정확하고 고품질의 이미지라는 것을 알고 있습니다. 간단한 프롬프트로 I와 충분히 유사하고 고품질의 이미지를 생성하려면 C LLM과 fen(p) 사이의 특징의 의미적 표현을 정렬해야 합니다. 구체적으로 다음 손실 함수를 최소화하는 것을 고려합니다. {CP(4) = KL(CLLM/T, fЕn(P²)/T), (9) 여기서 7은 Eq.(5)에서와 같이 설정되고 KL은 KL 발산을 나타냅니다[26]. 요약하면, SUR-어댑터 학습을 위한 최종 손실 함수는 다음과 같습니다. {total($) = 1 · LLM($) +12 · lCP(4) + l&#39; simple(4), (10) 여기서 λ; ≤ 1, i = 1,2는 손실 계수입니다. 알고리즘 1에서 SUR-어댑터의 학습 과정을 제시합니다. 학습 후, 미세 조정된 확산 모델은 이전과 동일한 샘플링 방법을 사용하여 이미지를 생성할 수 있습니다. SUR-어댑터: 대규모 언어 모델을 사용하여 텍스트-이미지 사전 학습된 확산 모델 향상 사전 인쇄본, 기술 보고서, 표 3: 다양한 품질 지표에 대한 섹션 5.1에 설명된 다양한 사전 학습된 모델과 통제된 방법의 평가 결과. 두 개의 독립적인 점수 샘플의 평균에 대한 t-검정을 계산하고, 결과 P 값이 0.05보다 크면 기준선과 SUR-어댑터의 NR 점수 사이에 유의미한 차이가 없음을 의미하므로 생성 품질이 비슷함을 나타냅니다. BRISQUE CLIP-IQA 사전 학습된 모델 통제 방법 기준선 13.Ours (P&gt;0.05?) 기준선 Ours (P&gt;0.05?) 기준선 MUSIQ Ours (P&gt;0.05?) 사용자 선호도(%) 기준선 Ours 14.78 (✓) 0.0.688 (√) 67.67.04 (✓) 48.51.ControlNet (canny) 22.25.15 (X) 0.0.668 (✓) 67.67.14 (✓) 49.50.DM (1.5), ControlNet (seg) 39.42.12 (✓) 0.0.668 (✓) 64.65.71 (X) 53.46.LLM (13B) 프롬프트 가중치 13.13.74() 0.0.691 (✓) 66.67.02 (✓) 47.52.MultiDiffusion 10.11.83 (✓) 0.0.691 () 66.67.95 (✓) 52.47.Self-attention Guidance 15.17.06 (✓) 0.0.706 (✓) 67.68.97 (✓) 48.51.15.19.53 (X) 0.0.707 (✓) 66.67.03 () 50.49.ControlNet (canny) 18.18.49 (✓) 0.0.696 (✓) 67.67.95 (✓) 50.49.DM (만화), ControlNet (세그먼트) 35.31.96 (X) 0.0.701 () 67.67.68() 51.48.LLM(13B) 프롬프트 가중치 17.19.12() 0.0.698(✓) 67.66.46(✓) 51.48.MultiDiffusion 14.Self-attention Guidance 20.15.96(✓) 20.98(✓) 0.0.0.711(✓) 0.706(✓) 68.67.26(✓) 47.52.67.66.90(√) 52.47.표 4: 다양한 LLM 설정에서 확산 모델의 성능. 굵은 글씨와 밑줄은 각각 최적 및 최적이 아닌 성능을 나타냅니다. 사전 학습된 모델 LLM 계층 또는 제어된 방법 CLIP 점수 동작(%) 색상(%) 계산(%) BRISQUE CLIP-IQA MUSIQ0.68.82.32.13.0.67.0.74.84.34.15.0.68.DM(1.5),0.78.81.30.15.0.67.LLM(13B)0.72.90.31.17.0.67.0.80.87.36.14.0.67.DM(만화),LLM(13B) A WNTH0.70.79.26.15.0.67.0.72.82.34.17.0.66.0.76.87.31.16.0.67.0.78.91.38.17.0.66.0.68.88.38.19.0.66.0.80.85.35.12.0.67.ControlNet(canny) 0.82.68.88.22.0.67.DM(1.5), ControlNet(seg) 0.8.8.60.39.0.65.LLM(7B) 프롬프트 가중치 0.84.83.53.14.0.67.MultiDiffusion 0.92.88.63.14.0.67.Self-attention Guidance 0.80.85.18.17.0.67.0.82.88.38.14.0.67.ControlNet(canny) 0.84.70.94.26.0.67.DM(1.5), ControlNet(seg) 0.7.8.64.39.0.65.LLM(33B) 프롬프트 가중치 0.84.92.58.13.0.67.MultiDiffusion 자기 주의 안내 0.87.88.61.13.0.67.0.86.89.20.15.0.67.5 실험 5.1 구현 세부 정보 우리는 서로 다른 매개변수를 갖는 두 개의 사전 학습된 확산 모델(DM)과 세 개의 LLM[45]을 활용합니다. DM(1.5) [38]은 고해상도 이미지 합성을 전문으로 하고 DM(만화)²은 현대 애니메이션 장편 영화 이미지에서 학습했습니다.LLM(s)은 매개변수 크기가 s인 LLaMa 모델을 의미합니다.또한 다양한 제어된 방법으로 SUR-어댑터의 보편성을 검증합니다.ControlNet[54]은 추가 조건을 도입하는 보조 네트워크입니다.실험에는 2개의 정식 사전 학습된 ControlNet이 포함됩니다.즉, ControlNet(canny)을 사용한 에지 감지와 ControlNet(seg)을 사용한 의미 분할입니다.Prompt weighting³은 텍스트 입력의 특정 부분에 더 높은 주의 가중치를 할당하는 간단한 기술입니다.MultiDiffusion[3]은 사전 학습된 확산 모델 위에 여러 확산 생성 방법을 병합하는 새로운 생성 프로세스2https://huggingface.co/nitrosocke/Ghibli-Diffusion3https://github.com/damian0815/compelon을 정의합니다. 자기 주의 안내[18]는 고주파 세부 정보에 의존하지 않는 예측에서 완전히 조건화된 이미지로의 방향을 제공합니다. 고주파 세부 정보는 UNet 자기 주의 맵에서 추출됩니다. 우리는 SURD 데이터 세트를 사용하여 의미와 품질의 두 가지 유형의 지표로 모델을 평가합니다. 모든 지표가 긍정적으로 지향된다는 점에 유의하십시오. 의미 평가를 위해 우리는 각각 15개의 프롬프트가 있는 세 가지 유형의 프롬프트[1, 7, 8, 31], 즉 동작, 색상 및 계산을 설계합니다. 이러한 프롬프트는 기준선과 SUR 어댑터의 의미적 기능을 평가하는 데 사용됩니다. 동작, 색상 및 계산은 모두 다양한 유형의 의미를 충족하는 이미지의 비율을 나타내는 백분율 메트릭입니다. 테스트하는 동안 우리는 각 프롬프트에 대해 10개의 이미지를 생성합니다. 의미적 품질을 추가로 평가하기 위해 우리는 또한 CLIP 점수[34]를 사용합니다. 우리는 CLIP을 사용하여 Preprint, Technical Report, 베이스라인 및 SUR-adapter에 대한 이진 분류 문제를 구성하고 프롬프트를 기반으로 가장 적합한 이미지를 선택합니다. 극단값의 효과를 피하기 위해 Softmax를 적용한 후 베이스라인과 SUR-adapter의 점수를 기록하고 테스트 세트의 평균값을 확산 모델의 최종 CLIP 점수로 사용합니다. 품질 평가를 위해 BRISQUE[33], CLIP-IQA[48], MUSIQ[24] 및 사용자 선호도 연구를 사용합니다. 사용자 선호도 연구는 사용자가 베이스라인과 SUR-adapter에서 생성된 한 쌍의 이미지에서 가장 좋은 품질의 이미지를 선택하는 단일 선택형 질문으로 구성됩니다. 우리는 사용자 선호도 연구에서 유효한 설문지를 수집했습니다. 부록에서 자세한 훈련 레시피를 제공합니다. 5.2 실험 분석 표 2는 베이스라인과 SUR-adapter의 의미적 역량을 보여줍니다. 주목할 점은 결과에서 SUR-adapter가 대부분의 경우 베이스라인의 SUR 성능을 효과적으로 향상시킬 수 있음을 보여줍니다. 또한, 우리는 다음을 그릴 수 있습니다.
--- CONCLUSION ---
s: (a) CLIP에서 상대 점수를 얻기 위해 Softmax를 사용하면 CLIP 점수가 신뢰할 수 없게 될 수 있으며, 특히 기준선과 SUR 어댑터가 모두 똑같이 나쁜 결과를 낼 때 그렇습니다. 예를 들어, ControlNet(seg)은 Action과 Color에 대한 생성 효과가 미비함에도 불구하고 비교적 높은 점수를 얻습니다. (b) ControlNet은 참조로 올바른 양의 정보가 있는 이미지 윤곽선을 활용하기 때문에 Counting 점수에서 좋은 성과를 보입니다. (c) 부정확한 이미지 분할로 인해 ControlNet(seg)을 사용하는 확산 모델이 의미 정보를 무시하고 완전히 흐릿한 이미지를 생성하여 Action과 Color에 대한 생성 효과가 만족스럽지 않을 수 있습니다. 그럼에도 불구하고 ControlNet(seg)의 부정적인 영향은 SUR 어댑터로 완화할 수 있습니다. (d) 사전 학습된 확산 모델의 SUR 기능은 Prompt Weighting과 MultiDiffusion을 사용하여 개선할 수 있으며, SUR 어댑터를 사용하면 더욱 향상시킬 수 있습니다. 그림 5에서 보듯이, 추가된 어댑터는 확산 모델의 의미적 이해와 추론 능력을 향상시키는 데 도움이 되지만, 추가 매개변수를 추가해도 어댑터와 사전 훈련된 모델이 동시에 훈련되지 않기 때문에 사전 훈련된 확산 모델의 원래 이미지 생성 품질이 유지된다는 보장은 없습니다. 그러나 제안하는 SURD는 고품질 이미지를 제공하여 이 문제를 완화하는 데 사용할 수 있습니다. 표 3에서 t-테스트와 사용자 선호도 연구를 통한 여러 이미지 품질 메트릭을 통해 SUR 어댑터가 이미지 생성 품질을 유지할 수 있음을 보여줍니다. 즉, SUR 어댑터의 이미지 품질과 원래 사전 훈련된 확산 모델 사이에 유의미한 차이가 없음을 의미합니다(P 값 ≥ 0.05). 더욱이 이러한 고품질 SURD 이미지도 확산 모델에서 나왔기 때문에 우리 방법에서 사전 훈련된 확산 모델이 생성한 이미지보다 더 높은 품질의 이미지를 생성하지 못합니다. ABLATION 연구 LLM 분석. 3.2절에서 소개한 대로 LLM(13B)에는 40개의 계층이 있습니다. 계층이 다른 LLM 벡터의 성능은 표 4의 처음 두 행에 나와 있습니다. 대부분의 경우 나중 계층에 해당하는 LLM 벡터가 더 나은 것으로 나타났습니다. 이는 더 깊은 계층의 고수준 의미적 특징이 의미적 증류에 더 도움이 됨을 시사합니다. 또한 표 4의 마지막 두 행에 매개변수 크기가 다른 LLM의 성능을 보여줍니다. 표 4, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 2, 3의 분석을 결합하면 매개변수 크기가 다른 LLM 간에 확산 모델 성능에 유의미한 차이가 없음을 알 수 있습니다. 기존 연구에 따르면 매개변수 크기가 큰 모델이 SUR 능력이 더 강하지만 기존 SUR 어댑터는 LLM에서 제한된 의미적 지식만 전송할 수 있습니다. SUR 어댑터의 지식 증류. 표 5에서 보듯이, 그림 5에서 녹색 선으로 표현된 LLM과 보라색 선으로 표현된 복합 프롬프트의 지식 증류에 대한 절제 연구를 수행합니다. LLM 또는 복합 프롬프트에 대한 지식만 증류하면 SUR-어댑터의 SUR 기능이 향상되고 LLM에 기반한 지식 증류의 효과가 복합 프롬프트에 기반한 지식 증류보다 강합니다. 나아가, 두 가지 모두에 대한 지식을 증류하면 SUR-어댑터의 성능을 더욱 향상시킬 수 있습니다. 표 5: SURadapter의 지식 증류에 대한 절제 연구.LLM 복합 프롬프트 BRISQUE 동작(%) 색상(%) 숫자(%) 13.75.81.14.13.78.84.34.12.74.86.32.14.80.87.36.제한 사항 표 2에서 보듯이 SUR-어댑터는 확산 모델을 개선하는 데 제한적인 용량을 가지고 있으며 SUR 문제를 완전히 해결할 수 없습니다. 예를 들어, 개선 후 DM(1.5), LLM(13B)의 계산은 36.67%만 증가했습니다. 그러나 SUR의 결함을 해결하려면 확산 모델의 텍스트 인코더를 최적화하기 위한 대규모 멀티모달 데이터 세트가 필요할 수 있으며, 이는 비용이 많이 들고 어려운 작업입니다. 게다가 섹션 6에서 강조했듯이 증류 후 매개변수 크기가 다른 LLM 간에 성능에 큰 차이가 없으므로 SUR-어댑터는 매개변수 제한과 같은 요인으로 인해 LLM에서 제한된 의미 지식만 전송할 수 있음을 나타냅니다. 따라서 LLM에서 의미 정보를 보다 효과적으로 증류하기 위해 SURadapter에 대한 추가 개선이 필요합니다. 결론 이 논문에서는 간단한 내러티브 프롬프트를 입력으로 제공받았을 때 의미를 이해하고 상식적 추론에 참여하는 능력 측면에서 기존의 사전 훈련된 확산 모델의 한계를 밝혀내어 최적이 아닌 이미지 생성으로 이어집니다. 이 문제를 완화하기 위해, 우리는 57,000개 이상의 의미적으로 수정된 이미지 텍스트 쌍으로 구성된 SURD라는 새로운 데이터 세트와 복잡한 키워드 기반 프롬프트와 대규모 언어 모델에서 의미적 이해와 추론 지식을 추출할 수 있는 SUR-어댑터 모듈을 소개합니다. SURD에 대해 수행된 광범위한 실험과 엄격한 평가는 SUR-어댑터가 이미지 생성 품질을 손상시키지 않고 확산 모델의 의미적 이해를 향상시킬 수 있음을 보여줍니다. 감사의 말 이 연구는 중국 자연과학기금(NSFC)의 보조금 번호 62206314 및 보조금 번호 U1711264, 광둥 기초 및 응용 기초 연구 기금의 보조금 번호 2022A1515011835, 중국 박사후 과학 기금의 보조금 번호 2021M703687에 따라 부분적으로 지원되었습니다. SUR-adapter: 대규모 언어 모델을 사용하여 텍스트-이미지 사전 학습 확산 모델 향상 참고문헌 [1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra. 2015. VQA: 시각적 질의응답. International Journal of Computer Vision 123(2015), 4-31. [2] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shiliang Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu. 2023. One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. ArXiv abs/2303.06555(2023). [3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, Tali Dekel. 2023. MultiDiffusion: 제어된 이미지 생성을 위한 확산 경로 융합.arXiv 사전 인쇄본 arXiv:2302.08113 2 (2023). [4] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko. 2021. 확산 모델을 사용한 레이블 효율적 의미 분할.ArXiv abs/2112.03126 (2021). [5] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, Christian Etmann. 2021. 점수 기반 확산 모델을 사용한 조건부 이미지 생성. [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TJ Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 Few-Shot 학습자입니다. ArXiv abs/2005.14165(2020). [7] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu 및 Yueting Zhuang. 2020. 견고한 시각적 질문 답변을 위한 반사실적 샘플 합성. 2020 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스(2020), 10797-10806. [8] Long Chen, Yuhang Zheng 및 Jun Xiao. 2022. 견고한 시각적 질문 답변을 위한 데이터 증강 재고. ArXiv abs/2207.08739(2022). [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne 이폴리토, 데이비드 루안, 임현택, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. 2022. PaLM: 경로를 통한 언어 모델링 확장. ArXiv ABS/2204.02311 (2022). [10] J. Devlin, MW Chang, K. Lee 및 K. Toutanova. 2018. BERT: 언어 이해를 위한 딥 양방향 변압기의 사전 학습. (2018). [11] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, YuChiang Frank Wang. 2022. Frido: 복잡한 장면 이미지 합성을 위한 피처 피라미드 확산. ArXiv abs/2208.13753 (2022). [12] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras. 2022. 플러그 앤 플레이 사전 모델로서의 확산 모델. ArXiv abs/2206.09012 (2022). [13] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven CH Hoi. 2022. 이미지에서 텍스트 프롬프트까지: 동결된 대규모 언어 모델을 사용한 Zeroshot VQA. arXiv 사전 인쇄본 arXiv:2212.(2022). [14] Kaiming He, X. Zhang, Shaoqing Ren, Jian Sun. 2015. 이미지 인식을 위한 심층 잔여 학습. 2016 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)(2015), 770-778. [15] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou. 2017. 심층 학습 스케일링은 경험적으로 예측 가능합니다. ArXiv abs/1712.00409(2017). [16] Jonathan Ho, Ajay Jain 및 P. Abbeel. 2020. 확산 확률적 모델의 잡음 제거. ArXiv abs/2006.11239(2020). [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals 및 L. Sifre. 2022. 컴퓨팅 최적 대규모 언어 모델 교육. ArXiv abs/2203.15556(2022). [18] Susung Hong, Gyuseong Lee, Wooseok Jang, Seungryong Kim. 2022. Self-Attention Guidance를 사용한 확산 모델의 샘플 품질 개선. arXiv 사전 인쇄본 arXiv:2210.00939 (2022). [19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2021. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685 (2021). [20] Zhongzhan Huang, Senwei Liang, Mingfu Liang, Haizhao Yang. 2019. DIANet: Dense-Implicit Attention Network. AAAI 인공지능 컨퍼런스에서. 사전 인쇄본, 기술 보고서, [21] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, 및 Y. Wu. 2016. 언어 모델링의 한계 탐구. [22] Jared Kaplan, Sam McCandlish, TJ Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, 및 Dario Amodei. 2020. 신경 언어 모델을 위한 스케일링 법칙. ArXiv abs/2001.08361(2020). [23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-Tang Chang, Tali Dekel, Inbar Mosseri, 및 Michal Irani. 2022. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. ArXiv abs/2210.09276 (2022). [24] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, Feng Yang. 2021. Musiq: 다중 스케일 이미지 품질 변환기. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집. 5148-5157. [25] Gwanghyun Kim, Taesung Kwon, Jong-Chul Ye. 2021. DiffusionCLIP: 견고한 이미지 조작을 위한 TextGuided 확산 모델. 2022 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)(2021), 2416-2425. [26] Solomon Kullback 및 Richard A Leibler. 1951. 정보와 충분성에 관하여. 수리통계연보 22, 1 (1951), 79-86. [27] Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhi hai Xu, Qi Li, Yue ting Chen. 2021. SRDiff: 확산 확률 모델을 사용한 단일 이미지 초고해상도. Neurocomputing 479 (2021), 47-59. [28] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. 2022. Blip: 통합 시각 언어 이해 및 생성을 위한 언어 이미지 사전 학습 부트스트래핑. 기계 학습 국제 컨퍼런스에서. PMLR, 12888–12900. [29] Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham. 2021. Jurassic-1: 기술적 세부 사항 및 평가. 백서. A121 연구실 1(2021). [30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár 및 C. Lawrence Zitnick. 2014. 마이크로소프트 COCO: 맥락 속의 공통 객체. 컴퓨터 비전에 관한 유럽 회의에서. [31] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Fang Yang 및 Xiao-Ming Wu. 2021. Slake: 의료 시각적 질문 응답을 위한 의미론적으로 레이블이 지정된 지식 강화 데이터 세트. 2021 IEEE 18차 생체의학 영상(ISBI) 국제 심포지엄(2021), 1650-1654. [32] Andreas Lugmayr, Martin Danelljan, Andrés Romero, Fisher Yu, Radu Timofte 및 Luc Van Gool. 2022. RePaint: Denoising Diffusion Probabilistic Models를 사용한 인페인팅. 2022 IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스(2022), 11451-11461. [33] Anish Mittal, Anush Krishna Moorthy, Alan Conrad Bovik. 2012. 공간 도메인에서 참조 없는 이미지 품질 평가. IEEE 이미지 처리 트랜잭션 21, 12(2012), 4695-4708. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스에서. PMLR, 8748-8763. [35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei 및 Ilya Sutskever. 2019. 언어 모델은 비지도 멀티태스크 학습자입니다. [36] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John FJ Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, 아디구나 Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, NK Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d&#39;Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, LL Bennett, 데미스 Hassabis, Koray Kavukcuoglu 및 Geoffrey Irving. 2021. 언어 모델 확장: Gopher 학습에서 얻은 방법, 분석 및 통찰력. ArXiv abs/2112.11446(2021). [37] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li 및 Peter J. Liu. 2019. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. ArXiv abs/1910.(2019). [38] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser 및 Björn Ommer. 2021. 잠재 확산 모델을 사용한 고해상도 이미지 합성.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)(2021), 10674-10685. [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성.IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)의 진행 중. 10684-10695. [40] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit. 2019. 스케일 전체에 걸친 일반화 오류의 구성적 예측. ArXiv 사전 인쇄본, 기술 보고서, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin abs/1909.12673 (2019). [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. DreamBooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. ArXiv abs/2208.12242 (2022). [42] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. 2021. 반복적 정제를 통한 이미지 초고해상도. IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션 45(2021), 47134726. [43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper 및 Bryan Catanzaro. 2019. Megatron-LM: 모델 병렬 처리를 사용하여 수십억 매개변수 언어 모델 교육. ArXiv abs/1909.08053(2019). [44] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh 티와리, 브라이언 카탄자로. 2022. DeepSpeed 및 Megatron을 사용하여 대규모 생성 언어 모델인 Megatron-Turing NLG 530B 교육. ArXiv ABS/2201.11990(2022). [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar 등. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄 arXiv:2302.13971 (2023). [46] Dani Valevski, Matan Kalman, Y. Matias 및 Yaniv Leviathan. 2022. UniTune: 단일 이미지에서 이미지 생성 모델을 미세 조정하여 텍스트 기반 이미지 편집. ArXiv ABS/2210.09477 (2022). [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전 30(2017). [48] Jianyi Wang, Kelvin CK Chan, Chen Change Loy. 2022. 이미지의 모양과 느낌을 평가하기 위한 CLIP 탐색. arXiv 사전 인쇄본 arXiv:2207.12396(2022). [49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus. 2022. 대규모 언어 모델의 새로운 능력. ArXiv abs/2206.(2022). [50] Ling Yang, Zhilong Zhang, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, Bin Cui. 2022. 확산 모델: 방법 및 응용 프로그램에 대한 포괄적인 조사. ArXiv abs/2209.(2022). [51] Ruihan Yang, Prakhar Srivastava 및 Stephan Mandt. 2022. 비디오 생성을 위한 확산 확률 모델링. ArXiv ABS/2203.09481 (2022). [52] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong 및 Jie Tang. 2022. GLM-130B: 개방형 이중 언어 사전 학습 모델. ArXiv ABS/2210.(2022). [53] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang 및 In-So Kweon. 2023. 생성 AI의 텍스트-이미지 확산 모델: 조사.ArXiv abs/2303.(2023). [54] Lvmin Zhang 및 Maneesh Agrawala. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가.ArXiv abs/2302.05543 (2023). [55] Lvmin Zhang 및 Maneesh Agrawala. 2023. 텍스트-이미지 확산 모델에 조건부 제어 추가.arXiv:2302.05543 [cs.CV] [56] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang 및 Ziwei Liu. 2022. MotionDiffuse: 확산 모델을 사용한 텍스트 기반 인간 동작 생성. ArXiv abs/2208.15001(2022). [57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022. OPT: 개방형 사전 학습된 Transformer 언어 모델. ArXiv abs/2205.01068(2022). [58] Shan Zhong, Wushao Wen, Jinghui Qin. 2023. SPEM: 이미지 인식을 위한 자체 적응 풀링 강화된 주의 모듈. 멀티미디어 모델링 컨퍼런스에서. SUR-adapter: 대규모 언어 모델을 사용하여 텍스트-이미지 사전 훈련된 확산 모델 강화 보충 데이터 세트 정보 A.1 사전 훈련된 모델 BLIP. 우리는 BLIP(Bootstrapping Language-Image Pretraining) [13, 28] 모델을 사용하여 SURD를 위한 이미지의 간단한 내러티브 프롬프트를 생성합니다. 구체적으로, 우리는 공식 문서 CLIP에 제공된 로드 함수를 사용하여 MSCOCO [30] 캡션 데이터 세트에서 미세 조정된 BLIP 캡션 기반 모델을 사용합니다. 우리는 CLIP을 사용하여 간단한 내러티브 프롬프트와 복잡한 키워드 기반 프롬프트의 정확성을 보장합니다. 구체적으로, 우리는 본문의 섹션 3.1에 간략하게 설명된 데이터 정리 프로세스를 설계했습니다. 우리는 CLIP에 간단한 프롬프트와 복잡한 프롬프트를 분류하도록 요청하여 이미지와 프롬프트 간의 의미적 유사성을 활용합니다. 여기서 목표는 이미지의 의미와 가장 잘 일치하는 프롬프트를 선택하는 것입니다. 일반적으로 복잡한 프롬프트에는 이미지 품질 설명과 같은 의미적으로 무관한 정보가 포함되어 있으므로 의미적으로 올바른 간단한 프롬프트는 일반적으로 복잡한 프롬프트보다 더 높은 CLIP 점수를 얻습니다. 해당 간단한 프롬프트의 CLIP 점수가 복잡한 프롬프트의 CLIP 점수보다 낮지 않으면 샘플을 보관합니다. ViT-B/32 아키텍처를 갖춘 공개적으로 사용 가능한 사전 학습된 CLIP 모델을 사용하고 공식 문서 LLMs에 제공된 함수를 사용하여 로드합니다. 이 논문에서는 대규모 언어 모델(LLM)에 대한 지식 증류로 7B에서 65B 매개변수 범위의 기초 언어 모델 컬렉션인 LLAMA[45]를 활용합니다. 구체적으로 LLM에서 간단한 프롬프트의 벡터 표현을 저장하여 확산 모델을 미세 조정하기 위한 텍스트 이해 역할을 합니다. 매개변수 수, 벡터 차원 및 모델 구조를 포함한 실험에 사용된 LLM의 세부 정보는 표 6에 나와 있습니다. 표 6: 본문에서 사용된 LLM의 모델 크기와 아키텍처. LLM 매개변수 차원 n 헤드 n 레이어 7B 6.7B13B 13.0B33B 32.5BA.2 영향 및 윤리 영향 및 사용. 확산 모델의 SUR 기능을 개선하는 것은 연구 커뮤니티에서 제한적으로 주목을 받은 중요한 문제입니다. 이 논문에서 우리는 의미적으로 올바른 데이터 세트인 SURD를 구성하고 지식 증류를 사용하여 복잡한 프롬프트와 LLM에서 의미적 지식을 전달함으로써 새로운 관점에서 이 문제에 접근합니다. SURD는 SUR 문제를 해결하기 위해 확산 모델을 미세 조정하는 데 사용할 수 있을 뿐만 아니라 의미적 정확성이 보장되어 확산 모델의 훈련 데이터 세트로 직접 사용할 수도 있습니다. 사회 윤리. 자연 영역의 많은 멀티모달 데이터 세트와 달리 SURD는 DNN에서 생성된 데이터에 전적으로 구축됩니다. 결과적으로 사람들의 개인 정보를 잠재적으로 침해할 수 있는 감시 시스템에서 사용될 가능성이 적습니다. 또한 https://github.com/salesforce/LAVIS 5https://github.com/openai/CLIP 데이터 정리 중에 수동 검사 단계를 통해 SURD가 성별 및 인종과 같은 민감한 개인 정보를 포함하지 않고, 소외된 커뮤니티에 대한 편견을 악화시킬 수 있는 데이터를 포함하지 않도록 합니다. 따라서 데이터 세트를 신중하게 검토한 결과, 개인에게 직접적인 해를 끼치는 데 사용될 가능성이 낮다고 생각합니다. ILLM (0) 0.0.0.zeros kaiming_normal normal uniform 0.7500 10000Step 15000 175006 x 10¹ 5 x 10¹ 4 x3 x 10¹ 2500 50002500 5000 7500 10000 12500 15000 17500Step 10000 12500 15000 17500Step 그림 6: 다른 초기화를 사용한 SUR-어댑터 학습 중 손실 값. 수학 기호는 Eq.(10)에 해당합니다. B 보충 실험 B.1 보충 구현 세부 정보 우리 연구에서는 두 개의 사전 학습된 확산 모델, 다른 매개변수를 가진 세 개의 LLM 및 다양한 제어 방법을 사용하여 SUR-어댑터의 보편성을 검증했습니다. 달리 명시하지 않는 한, [3, 18, 39, 45, 55]의 설정을 따릅니다. 구체적으로, 모든 모델은 하나의 Nvidia RTX 3090 GPU에서 학습되며, 단계는 5000으로, 배치 크기는 16으로, 해상도는 512로 설정됩니다. 학습하는 동안, 정규화, 중앙 자르기, 수평 뒤집기와 같은 혼합 정밀도 및 표준 데이터 증가 기술을 적용합니다. Eq. (7)과 Eq. (10)의 학습률과 하이퍼 매개변수는 1e-5로 설정됩니다. 모든 제어 방법은 디퓨저 6의 기본 설정을 활용합니다. 게다가, 의미적 요구 사항을 충족하는 이미지 세트를 수동으로 큐레이션했습니다. 이러한 이미지는 ControlNet(canny) 및 ControlNet(seg)의 조건부 입력으로 사용됩니다. MultiDiffusion의 설정은 DM(1.5)에 대한 사전 학습된 모델이 DM(만화)의 스케줄러를 사용하고 그 반대로, DM(만화)에 대한 사전 학습된 모델이 DM(1.5)의 스케줄러를 사용한다는 것입니다. B.2 SUR 어댑터의 시작 그림 5에서 보듯이, 우리는 어댑터와 백본을 연결하기 위해 완전히 연결된 네트워크를 사용합니다. 어댑터의 안정적인 학습을 보장하기 위해, 우리는 잘 알려진 어댑터 관련 연구[19, 54]에 따라 FCN을 0으로 초기화합니다. 또한, 그림 6에서 보듯이, 우리는 또한 SUR 어댑터 손실에 대한 다양한 초기화 방법의 영향을 보여줍니다. 우리는 서로 다른 초기화가 &quot;https://github.com/huggingface/diffusers 사전 인쇄본, 기술 보고서, Shanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, &amp; Liang Lin 표 7: 확산 모델을 사용하여 간단한 프롬프트로 생성된 이미지의 의미 정확도(Acc.) 평가. 간단한 프롬프트는 &quot;counting&quot;, &quot;color&quot;, &quot;action&quot;을 포함한 세 가지 유형의 문장으로 구성되었습니다. 각 프롬프트는 130개의 이미지를 생성했으며 이미지는 의미 정확도를 위해 수동으로 검사되었습니다. 프롬프트 정확도 정확도(저희) Туре 신선하게 구운 파이 4개. 63.08% 73.85% Counting 그림 같은 풍경 위로 떠다니는 다채로운 열기구 6개. 빈티지 유리 병 7개. 8.46% 41.54% 0.00% 36.92% 색상 구불구불한 길을 질주하는 강렬한 빨간색 스포츠카. 빨간색이 담긴 파란색 유리 영어: juice. 86.15% 93.85% 17.69% 20.00% 각각 파란색과 노란색 단색 옷을 입은 커플. 0.00% 6.92% Action 운동장에서 농구를 치는 사람. 나무를 먹는 기린. 41.54% 56.92% 25.38% 50.77% 주방에서 피자 반죽을 공중에 던지는 요리사. 15.38% 32.31% 표 8: 테스트 프롬프트의 예. Туре 프롬프트 액션 색상 계산 우아한 플립과 트위스트로 평형대 루틴을 수행하는 체조 선수. 계단 위로 킥플립을 하는 스케이트보더. 다채로운 물고기와 산호가 둘러싼 물속에서 수영하는 다이버. 고요한 바다 위로 지는 황금빛 태양, 하늘에 주황색과 분홍색 색조가 나타남. 보라색, 분홍색, 노란색 음영의 야생화로 가득 찬 초원의 고요한 풍경. 영어: 밝은 분홍색, 청록색, 은색의 색상 구성이 적용된 펑키하고 복고풍의 다이너. 복잡한 꽃 무늬가 있는 앤틱 티컵과 접시 4개 세트. 다섯 가지 종류의 신선한 과일을 조각내어 접시에 담았습니다. 모래사장에 있는 일곱 개의 화려한 해변 우산. 이미지 품질 평가 각 질문에 대한 두 사진을 비교하여 가장 품질이 좋다고 생각되는 사진을 선택합니다. *1. 다음 사진 중 어느 것이 더 나은 품질이라고 생각하십니까? OA Ов *2. 다음 사진 중 어느 것이 더 나은 품질이라고 생각하십니까? OA ON 그림 7: 사용자 선호도 연구의 제목, 설명 및 몇 가지 질문. Eq.(10)에서 LLM(Ø)에 미치는 영향은 거의 없지만 lcp(Ø)와 확산 모델의 학습에는 상당한 영향을 미치며 이는 기존 연구[19, 54]와 일치합니다. B.3 표에서 SUR-어댑터의 정확도표 7에서 SUR-어댑터 프롬프트의 의미적 정확도에 대한 추가 정보를 제공했으며, 이는 서론의 표 1에 나와 있는 프롬프트 예를 보완합니다.B.4 사용자 선호도 연구 이 논문에서는 수동 판단이 필요한 두 가지 지표가 있습니다.하나는 생성된 이미지(동작, 색상, 계산)의 의미적 정확도로, 객관적인 지표입니다.따라서 저자가 쉽게 평가하고 계산할 수 있습니다.수동 판단이 필요한 다른 지표는 표 3에 나와 있는 사용자 선호도입니다.이 지표는 주관적입니다.이 지표에 대한 데이터를 수집하기 위해 총 89개의 유효한 설문지를 수집했습니다(설문지의 예는 그림 7에 나와 있습니다).우리는 우리의 방법과 기준선에 의해 생성된 이미지를 무작위로 참가자에게 제시하고 &quot;다음 사진 중 어느 것이 더 나은 품질이라고 생각하십니까?&quot;라는 질문에 따라 더 나은 품질이라고 생각되는 사진을 선택하도록 요청했습니다.마지막으로 89개의 설문지를 기반으로 데이터를 수집하고 분석했습니다. B. 테스트 프롬프트 의미 이해 및 추론(SUR)을 평가하기 위해 의미를 동작, 색상 및 계산의 세 가지 주요 유형으로 나누었으며 각 유형에는 표 8에 예시가 표시된 15개의 프롬프트가 있습니다. 각 프롬프트에 대해 테스트하는 동안 10개의 이미지를 생성합니다.
