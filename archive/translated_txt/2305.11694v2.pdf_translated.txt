--- ABSTRACT ---
선택적 정보 요구 사항을 공식화하면 교집합, 합집합, 차집합과 같은 집합 연산을 암묵적으로 지정하는 쿼리가 생성됩니다. 예를 들어, &quot;물떼새가 아닌 해안가 새&quot; 또는 &quot;영국에서 촬영한 공상 과학 영화&quot;를 검색할 수 있습니다. 이러한 정보 요구 사항을 충족하는 검색 시스템의 능력을 연구하기 위해, 우리는 암묵적 집합 연산이 있는 3357개의 자연어 쿼리 데이터 세트인 QUEST를 구성하여 Wikipedia 문서에 해당하는 엔터티 집합에 매핑합니다. 이 데이터 세트는 모델이 쿼리에 언급된 여러 제약 조건을 문서의 해당 증거와 일치시키고 다양한 집합 연산을 올바르게 수행하도록 합니다. 이 데이터 세트는 Wikipedia 범주 이름을 사용하여 반자동으로 구성됩니다. 쿼리는 개별 범주에서 자동으로 구성된 다음, 군중 작업자가 자연스러움과 유창성을 위해 의역하고 추가로 검증합니다. 군중 작업자는 또한 문서를 기반으로 엔터티의 관련성을 평가하고 쿼리 제약 조건의 문서 텍스트 범위에 대한 귀속을 강조합니다. 우리는 여러 현대 검색 시스템을 분석하여 이러한 쿼리에서 종종 어려움을 겪는다는 것을 발견했습니다. 부정과 접속사를 포함하는 질의는 특히 어렵고 시스템은 이러한 연산의 조합으로 더욱 어려워집니다.¹ 1
--- INTRODUCTION ---
사람들은 종종 여러 가지 선호도나 제약 조건으로 정보 요구 사항을 표현합니다. 이러한 요구 사항에 해당하는 질의는 일반적으로 교집합, 차집합, 합집합과 같은 집합 연산을 암묵적으로 표현합니다. 예를 들어, 영화 관람객은 외계인이 등장하지 않는 90년대 공상과학 영화를 찾고 독자는 프랑스를 배경으로 한 역사 소설에 관심이 있을 수 있습니다. 마찬가지로, * Google에서 인턴십을 하는 동안 수행한 작업. 데이터 세트는 https://github.com/google-research/language/tree/master/language/quest에서 제공됩니다. (1) 베네수엘라 안데스 산맥의 새들 (2) 콜롬비아의 새들 베네수엘라 안데스 산맥에서는 발견되지만 콜롬비아에서는 발견되지 않는 새들을 의역 -&gt; (3) ✔유창함 ✓ 자연스러움 (4) 관련성 및 귀속 라벨링 장미관 앵무새 Mérida sunangel ✔부분적 증거 ✔가능성 있는 관련성 ✓완전한 증거 ✔확실히 관련성 그림 1: QUEST의 데이터 세트 구축 프로세스.먼저 (1) 위키피디아 카테고리 이름을 샘플링하고 해당 관련 엔터티 집합을 찾습니다.(2) 그런 다음 집합 연산이 포함된 쿼리를 작성하고 크라우드워커가 이 쿼리를 의역합니다.(3) 이러한 쿼리는 유창성과 자연스러움을 위해 검증됩니다.(4) 마지막으로 크라우드워커는 문서에서 귀속 가능한 기간을 강조 표시하여 엔터티의 관련성을 표시합니다.기억에 근거하여 종을 식별하려는 식물학자는 파나마에서 발견되는 상록수 관목을 검색할 수 있습니다. 또한 제약 조건을 충족하는 엔터티 집합이 비교적 작으면 독자는 이러한 엔터티의 전체 목록을 보고 탐색하고 싶어할 수 있습니다. 또한 시스템의 권장 사항을 확인하고 신뢰하기 위해 사용자는 신뢰할 수 있는 출처에서 증거를 볼 수 있는 이점을 얻습니다(Lamm et al., 2021). 이러한 질의를 처리하는 것은 주로 구조화된 지식 기반(KB)을 사용한 질의 응답 맥락에서 연구되었으며, 여기서 질의 제약 조건은 사전 정의된 술어에 기반을 두고 기호로 실행됩니다. 그러나 KB는 불완전하고 큐레이션하고 유지하는 데 비용이 많이 들 수 있습니다. 한편, 정보 검색의 발전으로 구조화된 KB에 의존하지 않고도 이러한 질의를 처리할 수 있는 시스템을 개발할 수 있으며, 이는 질의 제약 조건을 텍스트 문서의 지원 증거에 직접 일치시킵니다. 그러나 여러 제약 조건을 암묵적 집합 연산과 결합하는 질의는 MSMarco(Nguyen et al., 2016) 및 Natural Questions(Kwiatkowski et al., 2019)와 같은 기존 검색 벤치마크에서 잘 표현되지 않습니다. 또한 이러한 데이터 세트는 포괄적인 문서 세트를 검색하는 데 초점을 맞추지 않고 대신 기준 정보 검색 시스템의 상위 몇 가지 결과에 주석을 제한합니다. 이러한 쿼리에 대한 검색 시스템 성능을 분석하기 위해 Wikipedia 페이지에 해당하는 비교적 포괄적인 엔터티 세트에 매핑된 4개 도메인의 자연어 쿼리가 있는 데이터 세트인 QUEST를 제시합니다. 데이터 세트 구축 접근 방식의 빌딩 블록으로 범주와 Wikipedia의 엔터티에 대한 매핑을 사용하지만 추론 시점에 이 반구조화된 데이터 소스에 대한 액세스를 허용하지 않아 텍스트 기반 검색을 시뮬레이션합니다. Wikipedia 범주는 엔터티 속성에 대한 광범위한 자연어 설명을 나타내며 종종 검색 엔진 사용자가 그럴듯하게 발행할 수 있는 선택적 정보 요구 쿼리에 해당합니다. 속성 이름과 문서 텍스트 간의 관계는 종종 미묘하고 정교한 추론이 필요하여 작업에 내재된 자연어 추론 과제를 나타냅니다. 데이터 세트 구축 프로세스는 그림 1에 설명되어 있습니다. 기본 쿼리는 Wikipedia 범주 이름을 사용하여 반자동으로 생성됩니다. 복잡한 쿼리를 구성하기 위해 범주 이름을 샘플링하고 사전 정의된 템플릿(예: AB \ C)을 사용하여 구성합니다. 그런 다음 크라우드워커에게 이러한 자동 생성된 쿼리를 의역하도록 요청하면서 의역된 쿼리가 유창하고 사용자가 무엇을 찾고 있는지 명확하게 설명하는지 확인합니다. 그런 다음 다른 크라우드워커 집합에서 자연스러움과 유창함을 검증하고 해당 기준에 따라 필터링합니다. 마지막으로 대규모 데이터 하위 집합에 대해 엔터티 문서와 쿼리 제약 조건을 문서 텍스트 범위에 매핑하는 세분화된 텍스트 속성을 기반으로 스칼라 관련성 레이블을 수집합니다. 이러한 주석은 신뢰할 수 있는 출처에서 정확한 추론을 할 수 있는 시스템 개발에 도움이 될 수 있습니다. 이 데이터 세트에서 좋은 성과를 거두려면 쿼리 제약 조건을 문서의 해당 증거와 일치시키고 쿼리에서 암묵적으로 지정한 집합 작업을 처리할 수 있는 시스템이 필요합니다(쿼리 참조: 네바다를 배경으로 한 초자연적 공포 영화 대용량 문서 코퍼스 mmm... mmm... 엔터티: 윌리스 원더랜드 검색 시스템 검색된 집합 문서 텍스트: 윌리스 원더랜드는 GO 파슨스가 각본을 쓰고 케빈 루이스가 감독한 미국의 액션 코미디 공포 영화입니다. 이 영화의 주연은 니콜라스 케이지이며, 그는 또한 [...] 조용한 방랑자가 8명의 살인적인 애니마트로닉 캐릭터가 출몰하는 버려진 가족 엔터테인먼트 센터를 청소하도록 속는 이야기를 다룹니다. 이 프로젝트는 2019년 10월에 발표되었으며, [...] 외딴 시골 길에서 차 타이어가 펑크 나자 조용한 방랑자는 네바다주 헤이스빌 외곽에 버려지게 됩니다. 정비사 제드 러브가 [...] 그림 2: QUEST의 쿼리와 관련 엔터티의 예. 다른 쿼리 제약 조건에 대한 귀속은 문서의 다른 부분에서 나올 수 있습니다. 그림 2) 또한 대규모 엔터티 컬렉션으로 효율적으로 확장합니다. 데이터 세트에서 사전 학습된 모델을 미세 조정하여 여러 검색 시스템을 평가합니다. 시스템은 쿼리가 주어진 경우 다중 문서 세트를 검색하도록 학습됩니다. T5-Large 크기(Raffel et al., 2020)까지의 현재 듀얼 인코더 및 교차 어텐션 모델은 집합 연산이 있는 쿼리에 대한 검색을 수행하는 데 크게 효과적이지 않습니다. 접속사와 부정이 있는 쿼리는 모델에 특히 어려운 것으로 입증되었으며 시스템은 집합 연산의 조합으로 더욱 어려움을 겪습니다. 오류 분석 결과 관련 없는 거짓 양성 엔터티는 종종 모델이 부정된 제약 조건을 무시하거나 쿼리에서 접속사 제약 조건을 무시하여 발생한다는 것을 보여줍니다. 2
--- RELATED WORK ---
질문 답변 및 정보 검색에 대한 이전 작업은 지식 기반에 대한 QA와 오픈 도메인 QA 및 엔터티 또는 문서 집합에 대한 검색에 초점을 맞추었습니다. 아래에서 이것이 우리 작업과 어떻게 관련이 있는지 강조합니다. 지식 기반 QA 지식 기반에 대한 질문 답변에 대해 여러 데이터 세트가 제안되었습니다(Berant et al., 2013; Yih et al., 2016; Talmor and Berant, 2018; Keysers et al., 2020; Gu et al., 2021 등). 이러한 벤치마크는 수반되는 지식 기반에서 노드 또는 관계로 존재하는 엔터티 집합의 검색을 요구합니다. 질문은 선택적으로 논리적 형식으로 보완됩니다. Lan et al. (2021)은 복잡한 KBQA 데이터 세트에 대한 포괄적인 조사를 제공합니다. 이전 작업에서는 대규모 큐레이트된 KB가 불완전하다는 점을 동시에 지적했습니다(Watanabe et al., 2017). 특히, KBQA 시스템은 제한된 답변 스키마를 통해 작동하여 처리할 수 있는 쿼리 유형이 제한됩니다.또한 이러한 스키마는 구성 및 유지 관리 비용이 많이 듭니다.이러한 이유로 저희의 작업은 KB에 대한 액세스를 가정하지 않는 설정에 중점을 둡니다.KBQA 데이터 세트는 KB가 불완전하거나 사용할 수 없는 설정에도 적용되었습니다(Watanabe et al., 2017; Sun et al., 2019).이는 KB에서 일부 데이터 하위 집합을 제거하거나 KB를 완전히 무시하여 수행되었습니다.이러한 데이터 세트와의 주요 차이점은 여러 문서에 대한 멀티홉 추론에 중점을 두지 않는다는 것입니다.대신 엔터티의 관련성은 해당 문서에만 근거하여 판단할 수 있습니다.오픈 도메인 QA 및 검색 구조화되지 않은 텍스트 코퍼스에 대한 QA를 고려하는 많은 오픈 도메인 QA 벤치마크가 이전 작업에서 제안되었습니다. TREC(Craswell 등, 2020), MSMarco(Nguyen 등, 2016), Natural Questions(Kwiatkowski 등, 2019)와 같은 일부 데이터 세트는 검색 엔진에서 실제 사용자 쿼리를 사용하여 &quot;발견된 데이터&quot;를 사용하여 구성됩니다.Thakur 등(2021)은 이러한 기존 데이터 세트를 많이 고려하는 벤치마크를 제시합니다.HotpotQA(Yang 등, 2018) 및 MultiRC(Khashabi 등, 2018)와 같은 데이터 세트는 멀티홉 질의 응답에 초점을 맞추었습니다.다른 작업에서는 전자 상거래 데이터 세트(예: (Kong 등, 2022))를 탐색했지만 공개적으로 공개되지 않았습니다.특히 이러한 데이터 세트의 초점은 포괄적인 답변 세트보다 암묵적 집합 연산을 포함하는 쿼리에 초점을 맞추기 때문에 우리와 다릅니다.이러한 쿼리는 고려된 쿼리 분포의 꼬리에 발생하기 때문에 기존 데이터 세트에 잘 표현되지 않습니다. 다중 답변 검색 관련 연구(Min et al., 2021; Amouyal et al., 2022)도 시스템이 질의에 대한 여러 개의 고유한 답변을 예측해야 하는 다중 답변 검색 문제를 연구합니다.Min et al.(2021)은 기존 데이터 세트(예: WebQuestionsSP(Yih et al., 2016))를 조정하여 이 설정을 연구하고 다중 답변의 철저한 회수율을 평가하는 새로운 메트릭인 MRecall @K를 제안합니다.또한 다중 답변 세트 검색 문제도 고려하지만, 암묵적으로 세트 제약 조건을 포함하는 질의를 고려합니다.동시 작업에서 RomQA(Zhong et al., 2022)는 Wikidata에서 추출한 제약 조건의 조합에 초점을 맞춘 오픈 도메인 QA 데이터 세트를 제안합니다.RomQA는 대규모 답변 세트를 가질 수 있는 다중 제약 조건이 있는 질의에 답할 수 있도록 하는 동기를 공유합니다. RomQA는 인간 주석 없이 증거에 대한 귀속을 실현 가능하게 하기 위해, 구성 요소 제약 조건이 원격 감독을 통해 자동으로 관계로 주석이 달린 위키피디아 초록의 단일 엔터티 연결 문장에서 검증될 수 있는 질문에 초점을 맞추고, 높은 정확도이지만 회수율은 낮을 수 있습니다(티렉스 코퍼스). QUEST에서 우리는 보다 글로벌하고 문서 수준의 추론을 통해 귀속을 허용함으로써 쿼리 증거 매칭 작업의 범위를 넓힙니다. 귀속에 대한 인간 주석을 실현 가능하게 하기 위해, 우리는 답변 세트 크기와 단일 문서에 대한 답변의 증거를 제한합니다. 3 데이터 세트 생성 QUEST는 최대 20개의 해당 엔터티와 페어링된 3357개의 쿼리로 구성됩니다. 각 엔터티에는 위키피디아 페이지에서 파생된 관련 문서가 있습니다. 데이터 세트는 학습을 위한 1307개의 쿼리, 검증을 위한 323개의 쿼리, 테스트를 위한 1727개의 쿼리로 나뉩니다. 시스템의 작업은 주어진 쿼리에 대한 올바른 엔터티 세트를 반환하는 것입니다. 또한 이 컬렉션에는 325,505개의 엔터티가 포함되어 있으므로 이 작업에는 효율적으로 확장할 수 있는 검색 시스템이 필요합니다. 추론 시점에 시스템이 엔터티의 텍스트 설명 외부의 추가 정보에 액세스하는 것을 허용하지 않습니다. 모든 엔터티 문서에서 범주 레이블은 생략됩니다. 3.1 원자 쿼리 데이터 세트의 기본 원자 쿼리(즉, 도입된 집합 연산이 없는 쿼리)는 위키피디아 범주 이름²에서 파생됩니다. 이는 위키피디아의 관련 문서 그룹에 지정된 손으로 정리된 자연어 레이블입니다³. 문서에 범주를 지정하면 정확도가 높고 회수율이 비교적 높은 쿼리에 대한 답변 엔터티 집합을 자동으로 결정할 수 있습니다. 모든 관련 범주의 전이 폐쇄를 계산하여 답변 집합을 결정합니다. 그러나 이러한 범주를 쿼리 구성에 재활용하는 데는 다음과 같은 과제가 있습니다. 1) evi2의 부족 2022년 6월 1일의 위키피디아 버전을 사용합니다. 3 이러한 범주 레이블은 때때로 그 자체가 결합적일 수 있어 복잡성이 증가할 수 있습니다. 영화 A AUB AnB 책 A\B AUBUC An BNC AnB\CA AUB An B 식물 A\B 도메인 템플릿 A AUB An BA\B AUBUC An BПC AnB C AUBUC 예시 전기적 이탈리아 산적 영화 네덜란드 범죄 코미디 또는 로맨틱 코미디 영화 1970년대를 배경으로 한 이탈리아 범죄 영화 크리켓과 관련이 없는 인도 스포츠 영화 2020년대의 네덜란드 또는 스위스 전쟁 영화 또는 전쟁 영화 클리블랜드에서 촬영한 드라마 영화 이스라엘을 배경으로 하지 않은 기독교에 대한 서사시 영화 2004년 독일 소설 1925년 러시아 소설 또는 이반 부닌의 소설 1991년 아이슬란드를 배경으로 한 소설 실제 사건을 기반으로 하지 않은 1900년대를 배경으로 한 소설 난징, 허베이 또는 장쑤를 배경으로 한 소설 영어 Harper &amp; Brothers 아동 소설 전쟁과 관련이 없는 베트남을 배경으로 한 소설 가봉에서만 생산된 식물 매니토바 또는 아북극 아메리카의 나무 전통 아메리카 원주민 의학에 사용되는 관목 영어: 캐나다에서 찾을 수 없는 미국 북서부 과들루프의 나방, 곤충 또는 절지동물 Num. QueriesAn BNC AnB C 식물 북극, 영국, 코카서스는 공통적으로 가지고 있음 인도네시아와 말레이시아의 난초(태국에는 없음) 동물 A AUB An BA\B AUBUC An BNCAn B C캄보디아의 설치류는 무엇인가 쿠바 또는 자메이카의 멸종된 동물 아프리카의 신생 포유류 홀수 발가락 발굽 동물 몽골의 비구북구 조류 아시아 또는 아프리카의 신생대 조류 또는 아시아의 고제3 조류 페루의 조류이기도 한 칠레의 조류 기아나의 동물 대서양과 콜롬비아에서 발견되지만 브라질에서는 발견되지 않는 포유류 표 1: 고려된 4개 도메인의 집합 연산 및 예제가 있는 쿼리를 구성하는 데 사용된 템플릿, 각 도메인 및 템플릿당 예제 수 영어: 문서에서의 연관성: 문서는 범주와의 관련성을 판단하기에 충분한 증거를 포함하지 않을 수 있으며, 문서 텍스트에 기인하는 관련성에 대한 노이즈 신호를 제공할 가능성이 있음, 2) 낮은 회수율: 엔터티가 속한 범주에서 누락될 수 있음.데이터 세트의 약 절반에 대해 문서 텍스트를 기반으로 관련성 레이블과 속성을 크라우드소싱하고 수동 오류 분석(§5)을 통해 회수율을 조사함.쿼리의 다양성을 나타내기 위해 영화, 책, 동물 및 식물의 네 가지 도메인을 선택했습니다.가능한 모든 도메인이 아닌 네 가지 도메인에 초점을 맞추면 더 높은 품질 관리가 가능함.첫 번째 두 개는 일반적인 검색 시나리오를 모델링하는 반면, 두 번째 두 개는 과학적 검색 시나리오를 모델링함.3. 집합 연산 소개 집합 연산을 사용하여 쿼리를 구성하기 위해 원자 쿼리의 그럴듯한 조합을 나타내는 템플릿을 정의합니다.원자 쿼리를 A, B 및 C로 표시하면 템플릿과 다른 도메인의 해당 예가 표 1에 나열되어 있습니다.템플릿은 세 가지 기본 집합 연산(교집합, 합집합 및 차집합)을 구성하여 구성되었습니다. 이들은 비연관적인 집합 연산의 조합을 생략하여 결과 쿼리의 모호하지 않은 해석을 보장하기 위해 선택되었습니다.아래에서는 복잡한 쿼리를 구성하기 위한 원자 쿼리(예: A, B, C)를 샘플링하는 논리를 설명합니다.모든 경우에 크라우드소싱 관련성 판단이 실행 가능하도록 답변 세트에 2~20개의 엔터티가 포함되도록 합니다.템플릿 및 도메인당 200개의 쿼리를 샘플링하여 총 4200개의 초기 쿼리를 만듭니다.데이터 세트는 학습 + 검증(80분할)과 테스트로 동등하게 나뉩니다.이러한 각 세트에서 템플릿당 동일한 수의 쿼리를 샘플링했습니다.교집합.템플릿 AnB에 대한 교집합 연산은 A와 B가 모두 큰 답변 세트를 가지고 있지만 교집합이 작은 경우 특히 흥미롭고 잠재적으로 어렵습니다.각 A와 B의 최소 답변 세트 크기는 상당히 커야 하고(&gt;50개 엔터티), 교집합은 작아야 합니다(2~20개 엔터티).차이. 교차와 유사하게 A와 B 모두에 대한 답변 세트는 상당해야 합니다(&gt;50개 엔터티). 그러나 A(&lt;200개 엔터티)와 B 모두에 최대 크기 제약 조건을 두어야 합니다.
--- METHOD ---
출처에 대한 귀속을 제공하는 동시에 집합 탐색 선택적 질의에 답하는 s도 조사할 수 있습니다.7 한계 자연스러움.저희 데이터 세트는 위키피디아 카테고리 이름과 반자동으로 생성된 구성에 의존하기 때문에 암묵적 집합 연산을 포함하는 실제 검색 질의의 자연스러운 분포에서 편향되지 않은 샘플을 나타내지 않습니다.또한 모호하지 않은 질의에 대한 주의를 제한하고 실제 검색 시나리오에서 모호성으로 인해 발생하는 추가적인 과제를 다루지 않습니다.그러나 저희 데이터 세트의 질의는 실제 사용자 검색 요구 사항과 일치할 가능성이 있다고 판단되었으며 QUEST에서 측정된 시스템 개선은 집합 연산이 있는 자연스러운 검색 엔진 질의의 일부에 대한 개선과 상관관계가 있어야 합니다.회상하세요.또한 위키피디아 카테고리는 모든 관련 엔터티(문서에 충분한 증거가 포함된)에 대한 회수가 불완전하기 때문에 시스템이 거짓 양성으로 평가된 예측된 관련 엔터티에 대해 잘못 처벌받을 수 있습니다. 5절에서 이를 정량화합니다. 또한 엔터티의 신뢰할 수 있는 출처를 위키피디아 문서로 제한했지만 문서에 텍스트 증거가 충분하지 않은 엔터티도 여전히 관련이 있을 수 있습니다. 이상적으로는 여러 신뢰할 수 있는 출처를 고려하고 증거를 집계하여 관련성 결정을 내릴 수 있습니다. RomQA(Zhong et al., 2022)는 증거 귀속이 수동으로 검증되지는 않지만 후자 방향으로 한 걸음 나아갑니다. 답변 세트 크기. 관련성 레이블이 올바르고 검증 가능한지 확인하기 위해 크라우드워커의 도움을 구합니다. 그러나 이는 주석을 실행 가능하게 하기 위해 데이터 세트의 쿼리에 대한 답변 세트 크기를 20으로 제한해야 한다는 것을 의미했습니다. 한편으로는 사용자가 제한된 결과 세트에만 관심이 있을 수 있으므로 검색 시나리오에 현실적입니다. 반면에 데이터 세트는 답변 세트 크기가 훨씬 더 큰 시나리오를 모델링하지 않습니다. 감사의 말 Isabel Kraus-Liang, Mahesh Maddinala, Andrew Smith, Daphne Domansi, 그리고 모든 주석 작성자의 노고에 감사드립니다. 또한 Mark Yatskar, Dan Roth, Zhuyun Dai, Jianmo Ni, William Cohen, Andrew McCallum, Shib Sankar Dasgupta, Nicholas Fitzgerald에게도 유익한 토론에 감사드립니다. 참고문헌 Samuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, Jonathan Berant. 2022. Qampari:: 여러 문단에서 많은 답변이 있는 질문에 대한 오픈 도메인 질문 답변 벤치마크. ArXiv, abs/2205.12665. Iz Beltagy, Matthew E. Peters, Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. 질문-답변 쌍에서 Freebase의 의미적 구문 분석. 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록, 1533-1544쪽, 미국 워싱턴 시애틀. 계산 언어학 협회. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020. TREC 2019 심층 학습 트랙 개요. arXiv 사전 인쇄본 arXiv:2003.07820. Zhuyun Dai and Jamie Callan. 2019. 맥락적 신경 언어 모델링을 통한 ir에 대한 심층적 텍스트 이해. 정보 검색 연구 및 개발에 대한 제42회 국제 ACM SIGIR 컨퍼런스의 회의록, 985-988쪽. Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, Yu Su. 2021. Beyond iid: 지식 기반에서 질문 답변을 위한 세 가지 일반화 수준. 웹 컨퍼런스 2021 회의록, 3477-3488페이지. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang. 2022. LongT5: 긴 시퀀스를 위한 효율적인 텍스트-텍스트 변환기. 미국 시애틀에서 열린 Computational Linguistics 협회의 연구 결과: NAACL 2022, 724736페이지. Association for Computational Linguistics. Matthew Henderson, Rami Al-Rfou, Brian Strope, YunHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos 및 Ray Kurzweil. 2017. 스마트 답장을 위한 효율적인 자연어 응답 제안. arXiv 사전 인쇄 arXiv:1705.00652. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee 및 Olivier Bousquet. 2020. 구성 일반화 측정: 현실적인 데이터에 대한 종합적인 방법. 8th International Conference on Learning Representations, ICLR 2020, 에티오피아 아디스아바바, 2020년 4월 26-30일. OpenReview.net. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. 2018년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문), 252-262쪽, 루이지애나주 뉴올리언스. 컴퓨터 언어학 협회. Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Gupta, Mingyang Zhang, Wensong Xu, Mike Bendersky. 2022. Multi-aspect dense retrieval. 제28회 ACM SIGKDD 지식 발견 및 데이터 마이닝 컨퍼런스의 회의록에서.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov.2019. 자연스러운 질문: 질의 응답 연구를 위한 벤치마크.Association for Computational Linguistics의 거래, 7:452–466.Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, Michael Collins.2021. QED: 질의 응답에서 설명을 위한 프레임워크 및 데이터 세트. Transactions of the Association for Computational Linguistics, 9:790-806. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. 복잡한 지식 기반 질문 답변에 대한 조사: 방법, 과제 및 솔루션. 제30회 국제 인공지능 공동 학술 대회(IJCAI-21)의 회의록. Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. 2021. 다양한 다중 답변 검색을 위한 공동 구절 순위. 2021 자연어 처리 경험적 방법 학술 대회 회의록, 6997-7008쪽, 온라인 및 도미니카 공화국 푼타카나. Association for Computational Linguistics. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: 인간이 생성한 기계 독해 이해 데이터 세트. CoCo@ NIPs에서. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. 대형 듀얼 인코더는 일반화 가능한 검색기입니다. 2022 자연어 처리 경험적 방법 컨퍼런스 회의록, 9844-9855페이지, 아랍에미리트 아부다비. 계산 언어학 협회. Rodrigo Nogueira and Kyunghyun Cho. 2019. bert를 사용한 구절 재순위 지정. arXiv 사전 인쇄본 arXiv:1901.04085. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. Journal of Machine Learning Research, 21:167. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, David Reitter. 2021. 자연어 생성 모델에서 속성 측정. Stephen Robertson, Hugo Zaragoza, et al. 2009. 확률적 관련성 프레임워크: Bm25 이상. Foundations and Trends® in Information Retrieval, 3(4):333-389. K. Sparck Jones와 CJ Van Rijsbergen. 1975. 이상적인 정보 검색 테스트 컬렉션의 필요성과 제공에 대한 보고서. Haitian Sun, Tania Bedrax-Weiss, William Cohen. 2019. PullNet: 지식 기반과 텍스트에 대한 반복적 검색을 통한 오픈 도메인 질문 답변. 2019년 자연어 처리 경험적 방법 컨퍼런스와 제9회 자연어 처리 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 23802390쪽, 중국 홍콩. 계산 언어학 협회. Alon Talmor와 Jonathan Berant. 2018. 복잡한 질문에 답하기 위한 지식 기반으로서의 웹. 2018년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문), 641-651쪽, 루이지애나주 뉴올리언스. 컴퓨터 언어학 협회. 난단 타쿠르, 닐스 라이머스, 안드레아스 뤼클레, 아비쉔 스리바스타바, 이리나 구레비흐. 2021. BEIR: 정보 검색 모델의 제로샷 평가를 위한 이기종 벤치마크. 제35회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 회의(2라운드). 루크 빌니스, 시앙 리, 쉬카르 무르티, 앤드류 맥캘럼. 2018. 상자 격자 측정을 사용한 지식 그래프의 확률적 임베딩. 56회 연례 총회 의사록(제1권: 장문 논문), 263-272쪽, 호주 멜버른. 와타나베 유스케, 부완 딩그라, 루슬란 살라쿠트디노프. 2017. 검색 및 이해를 통한 비정형 텍스트에서의 질의 응답. arXiv 사전 인쇄본 arXiv:1703.08885. 양 지린, 펭 치, 사이젱 장, 요슈아 벤지오, 윌리엄 코헨, 루슬란 살라쿠트디노프, 크리스토퍼 D. 매닝. 2018. HotpotQA: 다양하고 설명 가능한 멀티홉 질의 응답을 위한 데이터 세트. 2018년 자연어 처리 경험적 방법 컨퍼런스 의사록, 2369-2380쪽, 벨기에 브뤼셀. Association for Computational Linguistics. Wen-tau Yih, Matthew Richardson, Chris Meek, MingWei Chang, Jina Suh. 2016. 지식 기반 질의 응답을 위한 의미적 구문 분석 라벨링의 가치. Association for Computational Linguistics의 제54회 연례 회의록(제2권: 단편 논문), 201-206쪽, 베를린, 독일. Association for Computational Linguistics. Victor Zhong, Weijia Shi, Wen-tau Yih, Luke Zettlemoyer. 2022. ROMQA: 견고하고 다중 증거, 다중 답변 질의 응답을 위한 벤치마크. arXiv 사전 인쇄본 arXiv:2210.14353. A
--- EXPERIMENT ---
§4의 s. 이러한 예를 생성하기 위해 모든 도메인의 원자 쿼리를 샘플링하여 QUEST의 어떤 쿼리에도 하위 쿼리로 나타나지 않도록 하고 Wikipedia의 해당 엔터티를 관련 엔터티 집합으로 사용합니다.4 실험 설정 우리는 기준 성능을 확립하기 위해 최신 검색 시스템을 평가합니다.또한 광범위한 오류 분석을 수행하여 모델 오류 패턴과 QUEST의 레이블 품질을 이해합니다.4.1 작업 정의 데이터 세트의 모든 도메인에 걸쳐 엔터티가 포함된 코퍼스 Ɛ를 고려합니다.각 엔터티에는 Wikipedia 페이지를 기반으로 하는 문서가 함께 제공됩니다.데이터 세트의 예는 쿼리 x와 주석이 달린 관련 엔터티 집합 y ≤ E로 구성됩니다.§3에서 설명한 대로 모든 예에 대해 |y| &lt; 20입니다.우리의 작업은 &amp;와 쿼리 x가 주어졌을 때 관련 엔터티 집합 ŷ Cε를 예측하는 시스템을 개발하는 것입니다. 4.2 평가 주요 평가 지표는 예제별 F₁ 점수의 평균인 평균 F1입니다. 예측된 엔터티 집합 ŷ를 주석이 달린 집합 y와 비교하여 각 예제에 대한 F₁을 계산합니다. 4.3 기준 시스템 그림 3에서 볼 수 있듯이 검색기와 분류기의 여러 조합을 평가했습니다. 검색기 구성 요소의 경우 희소 BMretriever(Robertson et al., 2009)와 고밀도 듀얼 인코더 검색기(DE로 표시)를 고려합니다. Ni et al.(2022)에 따라 T5(Raffel et al., 2020) 인코더에서 듀얼 인코더를 초기화하고 일괄 샘플링된 소프트맥스 손실(Henderson et al., 2017)로 학습합니다. 후보 집합이 있으면 관련 엔터티 집합을 결정해야 합니다. 주어진 쿼리에 대한 각 후보 문서의 관련성을 분류하기 위해 T5 인코더와 디코더로 구성된 교차 어텐션 모델을 고려합니다. 4 우리는 BM25에서 검색한 상위 1,000개 문서의 비관련 문서와 무작위 비관련 문서(Nogueira 및 Cho(2019)와 유사)에 기반한 부정적 예제를 사용하여 이진 교차 엔트로피 손실을 사용하여 교차 주의 분류기를 훈련합니다.대량의 후보에 대한 교차 주의 분류는 계산적으로 비용이 많이 들기 때문에 BM25와 듀얼 인코더를 제한하여 후보를 검색한 다음 교차 주의 분류기에서 고려합니다.T5 기반 듀얼 인코더는 최대 512개 토큰만 효율적으로 수용할 수 있으므로 소프트맥스 손실로 훈련된 BM25와 듀얼 인코더의 4점수는 문서의 관련성 확률을 제공하도록 정규화되지 않습니다.이러한 점수에 글로벌 임계값을 순진하게 적용하여 답변 세트를 생성하는 것은 이진 교차 엔트로피 손실로 훈련된 분류기를 사용하여 문서 관련성을 예측하는 것만큼 성능이 좋지 않다는 것을 발견했습니다.검색기(K=100) 분류기 평균 정밀도 평균 재현율 평균 FBMT5-Base 0.0.0.BMT5-Large 0.0.0.T5-Large DE T5-Base 0.0.0.T5-Large DE T5-Large 0.0.0.표 3: 테스트 세트에서 평가된 기준 시스템의 평균 정밀도, 재현율 및 F1. 평균. Recall@KRetriever100BM0.104 0.153 0.T5-Base DE 0.255 0.372 0.T5-Large DE 0.265 0.386 0.MRecall@K1000.395 0.020 0.030 0.037 0.0.726 0.045 0.088 0.127 0.0.757 0.047 0.100 0.142 0.표 4: 다양한 검색기의 평균 Recall 및 MRecall. 문서 텍스트를 잘라냅니다. 이것과 대안의 영향은 §5에서 논의합니다. 또한 T5가 Wikipedia에서 사전 학습되었으므로 부록 D에서 암기의 영향을 조사합니다.추가 세부 정보와 하이퍼파라미터 설정은 부록 A에 있습니다.4.4 수동 오류 주석 최상의 전체 시스템을 위해 오류를 샘플링하고 검증 세트에서 1145개의 쿼리-문서 쌍에 수동으로 주석을 달았습니다.검색기의 경우 최상위 후보 세트에 포함되지 않은 관련 문서와 관련 문서보다 순위가 높은 관련 없는 문서를 샘플링했습니다.분류기의 경우 상위 100개 후보 세트에서 발생한 거짓 양성 및 거짓 음성 오류를 샘플링했습니다.이 주석 프로세스에는 문서 관련성 판단(데이터 세트의 주석과의 일치성을 평가하기 위해)과 문서(및 듀얼 인코더 또는 분류기가 고려한 잘린 버전)에 관련성을 합리적으로 결정하기에 충분한 증거가 있는지 여부가 포함되었습니다.또한 쿼리 내의 각 제약 조건에 대한 관련성에 주석을 달았습니다.이러한 결과는 §5에서 논의합니다. 5 결과 및 분석 표 3에 테스트 세트에서 기준 시스템의 성능을 보고합니다. 이 섹션에서는 이러한 결과와 §4.4에 설명된 오류 주석에 대한 분석에서 얻은 주요 결과를 요약합니다. 듀얼 인코더는 BM25보다 성능이 뛰어납니다. 표 3에서 볼 수 있듯이 최상의 전체 시스템은 검색을 위해 BM25 대신 T5Large 듀얼 인코더를 사용합니다. 듀얼 인코더와 BM25의 재현율을 직접 비교할 때 성능 차이가 훨씬 더 큽니다. 표 4에서 다양한 후보 세트 크기에 대한 평균 재현율(관련 문서 전체 세트의 예제당 평균 재현율)과 MRecall(Min et al., 2021)(후보 세트에 모든 관련 문서가 포함된 예제의 백분율)을 보고합니다. 검색과 분류는 모두 어렵습니다. 검색기에서 상위 100개 후보만 고려하므로 검색기의 재현율 @100은 전체 시스템의 재현율에 대한 상한을 설정합니다. T5-Large Dual Encoder의 경우 100에서의 재현율은 0.476에 불과하고, T5-Large 분류기는 0.165의 정밀도만 달성했음에도 불구하고 전체 재현율은 0.368로 더욱 감소합니다. 이는 두 단계 모두에서 전반적인 점수를 개선할 여지가 있음을 시사합니다. 검색과 분류 모두에서 더 큰 T5 크기에 대한 성능이 향상됨에 따라 추가적인 모델 확장이 유익할 수 있습니다. 모델은 교집합과 차이에 어려움을 겪습니다. 또한 표 5에 나와 있듯이 다양한 템플릿과 도메인에서 결과를 분석했습니다. 제약 조건이 다르면 답변 세트 크기와 사용된 원자 범주에 따라 분포가 달라집니다. 따라서 템플릿 간 F점수의 차이를 해석하기 어려울 수 있습니다. 그럼에도 불구하고 집합 합집합이 있는 쿼리의 평균 F1 점수가 가장 높은 것을 발견했습니다. 집합 교집합이 있는 쿼리의 평균 F1 점수가 가장 낮고, 집합 차이가 있는 쿼리도 어려운 것으로 보입니다. 접속사와 부정이 있는 질의가 어려운 이유를 분석하기 위해 개별 질의 제약 조건의 관련성에 레이블을 지정했습니다(§4.4). 여기서 시스템은 관련성이 없는 문서의 관련성을 잘못 판단합니다. 결과는 표 6에 요약되어 있습니다. 교집합과 관련된 대부분의 거짓 긍정 오류의 경우 적어도 하나의 제약 조건이 충족됩니다. 이는 모델이 관련성을 결정할 때 교집합을 합집합으로 잘못 처리한 것으로 해석될 수 있습니다. 마찬가지로, 차분 집합이 있는 대부분의 예의 경우 부정 제약 조건이 충족되지 않습니다. 이는 시스템이 부정에 충분히 민감하지 않음을 시사합니다. # ConstraintsNeg. 리트리버 An B An BNC 63.36.56.5 37.0 6.A\B 80.3 19.59.A AUB An BA\B AUBUC 0.171 0.248 0.템플릿 필름 책 식물 동물 모두 0.231 0.436 0.209 0.214 0.0.264 0.366 0.229 0.0.0.115 0.138 0.049 0.0.177 0.188 0.216 0.204 0.0.200 0.348 0.An BNC 0.086 0.121 0.An B\C 0.119 0.112 0.모두 An B\C 47.6 40.5 11.9 26. 분류기 An B 83.16.☐ 0. An BNC 73.22.4.A\B 38.0.294 0.0.065 0.0.136 0.0.182 0. An B\C 표 5: 템플릿과 도메인에서 가장 강력한 기준(T5-대형 DE + T5-대형 분류기)의 F1. 정밀도와 재현율을 모두 개선할 수 있는 상당한 여유가 있습니다. 수동 오류 분석(§4.4)의 일환으로, 우리는 관련성에 대한 우리만의 판단을 내리고 QUEST의 관련성 주석과의 일치도를 측정했습니다. 이 분석은 우리의 최고 시스템이 데이터 세트의 관련성 레이블과 일치하지 않는 경우에 초점을 맞추었기 때문에, 이러한 경우에 대한 일치도는 데이터 세트에서 무작위로 선택된 쿼리-문서 쌍에 대한 일치도보다 상당히 낮을 것으로 예상합니다. 따라서 데이터 세트의 여유도와 주석 품질을 판단하는 집중적인 방법을 제공합니다. 거짓 부정 오류의 경우, 우리는 91.1%의 엔터티가 영화 및 도서 도메인과 관련이 있다고 판단했고, 81.4%가 식물 및 동물과 관련이 있다고 판단했습니다. 특히, 우리는 §3에서 설명한 대로 영화 및 도서 도메인에 대한 관련성 레이블을 수집하고 이러한 레이블을 기반으로 일부 엔터티를 제거했는데, 이는 이러한 도메인에서 거짓 부정에 대한 더 높은 일치도를 설명할 수 있습니다. 이는 특히 관련성 레이블을 수집한 도메인에 대해 QUEST에서 정의한 대로 회수를 개선할 수 있는 상당한 여유가 있음을 나타냅니다. 거짓 긍정 오류의 경우, 우리는 28.8%의 엔터티가 관련이 있다고 판단했는데, 이는 데이터 세트의 관련성 레이블과 더 큰 불일치를 보여줍니다. 이는 주로 관련성 레이블로 인해 제거된 엔터티가 아니라 위키피디아 범주 분류법에서 파생된 엔터티 세트에 포함되지 않은 엔터티(97.7%) 때문입니다. 이는 위키피디아 범주 분류법의 회수 문제를 수정하기 위해 모든 엔터티에 대한 관련성을 철저히 레이블하는 것이 실행 가능하지 않기 때문에 완전히 해결하기 어려운 문제입니다. 향후 작업에서는 풀링을 사용하여 관련 문서 집합 81.0 19.95.5 4.6 0.0 68을 지속적으로 늘릴 수 있습니다.표 6: T5Large 분류기의 거짓 양성 오류 분석 및 T5Large 듀얼 인코더에서 관련 없는 문서가 관련 있는 문서보다 순위가 높은 경우.접속사가 있는 질의의 경우, 예측된 문서에서 템플릿의 제약 조건 1, 2, 또는 1이 충족되지 않는 경우의 백분율을 결정했습니다(제약 조건).부정이 있는 질의의 경우, 부정된 제약 조건(부정)이 충족되지 않는 경우의 백분율을 측정했습니다.관련 문서의 수(Sparck Jones 및 Van Rijsbergen, 1975).이에도 불구하고, 대부분의 거짓 양성 예측이 관련 없다고 판단했기 때문에 분석 결과 정밀도를 개선할 여지가 상당히 있음을 시사합니다.문서 텍스트를 자르면 일반적으로 충분한 맥락이 제공됩니다. 실험에서 우리는 듀얼 인코더의 경우 문서 텍스트를 512개 토큰으로, 분류기의 경우 384개 토큰으로 잘라 문서와 쿼리를 연결할 수 있도록 했습니다. 오류 분석(§4.4)에 따르면, 관련성을 판단하기에 충분한 증거가 있는 문서 중 증거가 이 잘린 컨텍스트에서 듀얼 인코더의 경우 93.2%, 분류기의 경우 96.1% 발생했습니다. 이는 긴 문서를 처리하는 데 있어 이 간단한 기준선이 상대적으로 성공한 이유를 설명할 수 있습니다. 또한 대안 전략을 평가했지만 예비 실험에서 성능이 떨어졌습니다5. 향후 작업에서는 효율적인 변환기 변형을 평가할 수 있습니다(Guo et al., 2022; Beltagy et al., 2020). 6
--- CONCLUSION ---
우리는 관련 엔터티 문서의 해당 집합과 함께 암묵적 집합 연산을 포함하는 질의의 새로운 벤치마크인 QUEST를 제시합니다. 우리의 실험은 이러한 질의가 현대 검색 시스템에 대한 5과제를 제시한다는 것을 나타냅니다. 듀얼 인코더의 경우, 우리는 문서를 512개 토큰의 겹치는 청크로 분할하고 추론 시 점수를 집계했습니다(Dai 및 Callan, 2019). 교차 주의 모델의 경우, 우리는 BM25를 사용하여 길이가 128인 상위 3개 구절을 선택했습니다. 향후 작업에서는 자연어 표현에서 집합 연산을 처리하기 위한 더 나은 귀납적 편향을 갖는 접근 방식을 고려할 수 있습니다(예: Vilnis et al. (2018)). QUEST의 귀속은 추론 시 세분화된 귀속을 제공할 수 있는 시스템을 구축하는 데 활용할 수 있습니다. 사전 학습된 생성 LM과 다중 증거 집계 방법을 사용하여 소스에 대한 귀속을 제공하면서 집합 탐색 선택적 질의에 답할 수 있는 잠재력도 조사할 수 있습니다. 7 한계 자연스러움. 데이터 세트는 위키피디아 카테고리 이름과 반자동으로 생성된 구성에 의존하기 때문에 암묵적 집합 연산을 포함하는 실제 검색 쿼리의 자연스러운 분포에서 편향되지 않은 샘플을 나타내지 않습니다. 또한 모호하지 않은 쿼리에 대한 주의를 제한하고 실제 검색 시나리오에서 모호성으로 인해 발생하는 추가적인 과제를 다루지 않습니다. 그러나 데이터 세트의 쿼리는 실제 사용자 검색 요구 사항과 일치할 가능성이 있는 것으로 판단되었으며 QUEST에서 측정된 시스템 개선은 집합 연산이 있는 자연스러운 검색 엔진 쿼리의 일부에 대한 개선과 관련이 있어야 합니다. 상기하십시오. 또한 위키피디아 카테고리는 모든 관련 엔터티(문서에 충분한 증거가 포함된)에 대한 회수가 불완전하기 때문에 시스템이 거짓 양성으로 평가된 예측된 관련 엔터티에 대해 잘못 처벌될 수 있습니다. 섹션 5에서 이를 정량화합니다. 또한 엔터티에 대한 신뢰할 수 있는 출처를 위키피디아 문서로 제한했지만 문서에 텍스트 증거가 충분하지 않은 엔터티도 여전히 관련이 있을 수 있습니다. 이상적으로는 여러 신뢰할 수 있는 출처를 고려하고 증거를 집계하여 관련성 결정을 내릴 수 있습니다. RomQA(Zhong et al., 2022)는 증거 귀속이 수동으로 검증되지는 않지만 후자 방향으로 한 걸음 더 나아갑니다.답변 세트 크기.관련성 레이블이 정확하고 검증 가능한지 확인하기 위해 크라우드워커의 도움을 구합니다.그러나 이는 주석을 실행 가능하게 하기 위해 데이터 세트의 쿼리에 대한 답변 세트 크기를 20으로 제한해야 한다는 것을 의미했습니다.한편으로 이는 사용자가 제한된 결과 세트에만 관심이 있을 수 있으므로 검색 시나리오에 현실적입니다.반면에, 우리 데이터 세트는 답변 세트 크기가 훨씬 더 큰 시나리오를 모델링하지 않습니다.감사의 말 Isabel Kraus-Liang, Mahesh Maddinala, Andrew Smith, Daphne Domansi 및 모든 주석 작성자의 작업에 감사드립니다.또한 Mark Yatskar, Dan Roth, Zhuyun Dai, Jianmo Ni, William Cohen, Andrew McCallum, Shib Sankar Dasgupta 및 Nicholas Fitzgerald에게 유용한 토론에 감사드립니다. 참고문헌 Samuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, Jonathan Berant. 2022. Qampari:: 여러 문단에서 많은 답변이 있는 질문에 대한 오픈 도메인 질문 답변 벤치마크. ArXiv, abs/2205.12665. Iz Beltagy, Matthew E. Peters, Arman Cohan. 2020. Longformer: 긴 문서 변환기. arXiv:2004.05150. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang. 2013. 질문-답변 쌍에서 Freebase의 의미 구문 분석. 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 1533-1544쪽, 미국 워싱턴 시애틀. 계산 언어학 협회. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees. 2020. TREC 2019 심층 학습 트랙 개요. arXiv 사전 인쇄본 arXiv:2003.07820. Zhuyun Dai와 Jamie Callan. 2019. 문맥적 신경 언어 모델링을 통한 ir에 대한 심층적 텍스트 이해. 정보 검색 연구 및 개발에 관한 제42회 국제 ACM SIGIR 컨퍼런스 회의록, 985-988페이지. Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, Yu Su. 2021. iid를 넘어서: 지식 기반에 대한 질의 응답을 위한 세 가지 일반화 수준. 웹 컨퍼런스 회의록 2021, 3477-3488페이지. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang. 2022. LongT5: 긴 시퀀스를 위한 효율적인 텍스트-텍스트 변환기. 미국 시애틀에서 열린 Computational Linguistics 협회의 연구 결과: NAACL 2022, 724736페이지. Association for Computational Linguistics. Matthew Henderson, Rami Al-Rfou, Brian Strope, YunHsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, Ray Kurzweil. 2017. 스마트 회신을 위한 효율적인 자연어 응답 제안. arXiv 사전 인쇄본 arXiv:1705.00652. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, Olivier Bousquet. 2020. Measuring compositional generalization: A comprehensive method on Realistic data. 제8회 International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. 2018년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 제1권(긴 논문), 252-262쪽, 루이지애나주 뉴올리언스. 컴퓨터 언어학회. Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Gupta, Mingyang Zhang, Wensong Xu, Mike Bendersky. 2022. 다중 측면 고밀도 검색. 제28회 ACM SIGKDD 지식 발견 및 데이터 마이닝 학술대회 논문집. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466. Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, Michael Collins. 2021. QED: A Framework and Dataset for Explanations in Question Answering. Transactions of the Association for Computational Linguistics, 9:790-806. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, Ji-Rong Wen. 2021. 복잡한 지식 기반 질문 답변에 대한 조사: 방법, 과제 및 솔루션. 제30회 국제 인공지능 공동 학술 대회(IJCAI-21)의 회의록. Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Hannaneh Hajishirzi. 2021. 다양한 다중 답변 검색을 위한 공동 구절 순위. 2021 자연어 처리 경험적 방법 학술 대회 회의록, 6997-7008쪽, 온라인 및 도미니카 공화국 푼타카나. 계산언어학 협회. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng. 2016. Ms marco: 인간이 생성한 기계 독해 이해 데이터 세트. CoCo@ NIPs에서. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, Yinfei Yang. 2022. 대형 듀얼 인코더는 일반화 가능한 검색기입니다. 2022 자연어 처리 경험적 방법 컨퍼런스 회의록, 9844-9855페이지, 아랍에미리트 아부다비. 계산 언어학 협회. Rodrigo Nogueira와 Kyunghyun Cho. 2019. bert를 사용한 구절 재순위 지정. arXiv 사전 인쇄본 arXiv:1901.04085. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐색. Journal of Machine Learning Research, 21:167. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. 자연어 생성 모델에서 속성 측정. Stephen Robertson, Hugo Zaragoza, et al. 2009. 확률적 관련성 프레임워크: Bm25 이상. Foundations and Trends® in Information Retrieval, 3(4):333-389. K. Sparck Jones and CJ Van Rijsbergen. 1975. 이상적인 정보 검색 테스트 컬렉션의 필요성과 제공에 대한 보고서. Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: 지식 기반과 텍스트에 대한 반복적 검색을 통한 오픈 도메인 질문 답변. 2019년 자연어 처리 경험적 방법 컨퍼런스 및 제9회 자연어 처리 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 2380~2390쪽, 중국 홍콩. Association for Computational Linguistics. Alon Talmor and Jonathan Berant. 2018. 복잡한 질문에 답하기 위한 지식 기반으로서의 웹. 2018년 북미 지부 컨퍼런스 회의록, Association for Computational Linguistics: Human Language Technologies, Volume 1(Long Papers), 641~651쪽, 루이지애나주 뉴올리언스. Association for Computational Linguistics. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, Iryna Gurevych. 2021. BEIR: 정보 검색 모델의 제로샷 평가를 위한 이기종 벤치마크. 제35회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙(2라운드)에서. Luke Vilnis, Xiang Li, Shikhar Murty, Andrew McCallum. 2018. 상자 격자 측정을 사용한 지식 그래프의 확률적 임베딩. Association for Computational Linguistics의 제56회 연례 회의록(제1권: 장문 논문), 263-272쪽, 호주 멜버른. Association for Computational Linguistics. Yusuke Watanabe, Bhuwan Dhingra, Ruslan Salakhutdinov. 2017. 검색 및 이해를 통한 비정형 텍스트에서의 질의 응답. arXiv 사전 인쇄본 arXiv:1703.08885. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: 다양하고 설명 가능한 멀티홉 질의 응답을 위한 데이터 세트. 2018년 자연어 처리 경험적 방법 컨퍼런스 회의록, 2369-2380쪽, 벨기에 브뤼셀. 계산 언어학 협회. Wen-tau Yih, Matthew Richardson, Chris Meek, MingWei Chang, and Jina Suh. 2016. 지식 기반 질의 응답을 위한 의미적 구문 분석 레이블링의 가치. 제54회 Association for Computational Linguistics 연례 회의록(제2권: 단편 논문), 201-206쪽, 독일 베를린. Association for Computational Linguistics. Victor Zhong, Weijia Shi, Wen-tau Yih, Luke Zettlemoyer. 2022. ROMQA: 강력하고 다중 증거, 다중 답변 질의응답을 위한 벤치마크. arXiv 사전 인쇄본 arXiv:2210.14353. 실험 세부 사항 및 하이퍼 매개변수 모든 모델은 T5 1부터 미세 조정되었습니다. 체크포인트 6. 32개 Cloud TPU v3 코어에서 T5 모델을 미세 조정합니다. 모든 모델에서 미세 조정에는 8시간도 걸리지 않습니다. 듀얼 인코더. 듀얼 인코더 모델을 구현하기 위해 t5x_retrieval 라이브러리를 사용했습니다. 검증 세트의 결과에 따라 일부 매개변수를 조정했습니다. 듀얼 인코더를 학습하기 위한 관련 하이퍼파라미터는 다음과 같습니다.• 학습률: 1e-워밍업 단계: 미세 조정 단계:• 배치 크기:• 최대 쿼리 길이:• 최대 후보 길이: 분류기.음수 예제의 경우, 무작위로 관련 없는 문서 250개를 샘플링하고 BM25에서 검색한 상위 1000개 문서에서 관련 없는 문서 250개를 샘플링했습니다.또한 각 긍정적 예제를 50번 복제했습니다.대규모 클래스 불균형으로 학습하는 것보다 대략적으로 동일한 수의 긍정적 및 부정적 예제가 더 나은 성능을 낸다는 것을 발견했습니다.BM25의 무작위 부정적 및 부정적을 조합한 것이 개별 부정적 예제만 사용하는 것보다 더 나은 성능을 낸다는 것을 발견했습니다.또한 BM25에서 부정적 예제를 선택하는 것이 T5-Large 듀얼 인코더에서 부정적 예제를 선택하는 것보다 더 나은 성능을 냈습니다.T5 입력의 경우 쿼리를 연결하고 문서 텍스트를 잘랐습니다. T5 출력은 문자열 &quot;relevant&quot; 또는 &quot;not relevant&quot;입니다. 추론 시점에 문서 관련성을 분류하기 위해 우리는 검증 세트에서 조정한 &quot;relevant&quot; 레이블에 할당된 확률에 임계값을 적용했습니다. BM25 후보를 분류할 때는 임계값 0.9를 사용했고 듀얼 인코더 후보를 분류할 때는 임계값 0.95를 사용했습니다. 분류기를 학습하기 위한 다른 관련 하이퍼파라미터는 다음과 같습니다. &quot;https://github.com/googleresearch/t5x/blob/main/docs/models.md https://cloud.google.com/tpu/ https://github.com/google-research/t5x_retrieval B • • • 학습 속도: 1e-• 워밍업 단계:• 미세 조정 단계: 배치 크기: 최대 소스 길이:• 최대 대상 길이: 세트 차이 및 리콜 표기법 및 가정 위키피디아 범주 그래프에서 파생된 두 세트인 Â와 B가 있다고 가정해 보겠습니다. 위키피디아 범주 그래프에는 일부 관련 엔터티가 누락될 수 있으며, 이때 A와 B는 모든 관련 엔터티를 포함하는 가상 세트로 해석됩니다. 리콜을 rA와 rB로 표시하여 누락된 엔터티의 정도를 정량화합니다. 이때 |Â| = râ × |A|이고 |Â| = rB * |B|입니다. A에서 또한 B에서 rò로, |AB| = rò * |A|가 됩니다.simr₁* 편리성을 위해 A와 B의 겹침이 |ÂN B| = r₁ * |AB|이고 |Ân B = = ˜Â * ˜B * |AB|가 되도록 가정합니다.도출 rA, TB, r의 함수로서 A \ B에 대한 Â \ Â의 재현율(r)과 정밀도(p)는 무엇입니까?먼저 재현율에 대한 이 함수를 도출합니다.r = r = |(A \ B) n (Â \ B)| |(A \ B)| |(Â \ B)| r = |(A \ B)| |Â| - |Ân B r = |A| – |AB| ™Ã * |A| − ˜Â * гn *|A| r = |A| − (r * |A|) гA * (1 − r) * |A| (1 − r) * |A| 정확도는 다음과 같습니다. p = r = r A |(A \ B) n (Â \ B)| |(Â \ B)| &quot;우리는 X와 Y 세트의 쌍의 몇 가지 유용한 속성을 알아봅니다: X\Y=XY, |X \Y| = |X|-|XY], XCY이면 XY = X이고, X CY이면 Y° C Xc입니다. p = p = |(Â\B)| |(Â \ B)| |Â| – |ÂNB| |Â| – LÂn Bà ra * |A| − ra * r *|A| гA * B * r * |A| гA * (1 − r) * |A| p = TA * |A| p = TA * (p = r ·гB *гn) * |A| (1 − r) (1 − гB *rn) 토론 재현율은 단순히 rA와 같지만, 정밀도는 rB와 r의 더 복잡한 함수이며, 큰 ™ 값의 경우 매우 낮을 수 있습니다. 직관적으로, Â에서 Â를 빼면 Â의 대부분이 제거되므로 결과 세트의 정밀도는 다음에 의해 지배됩니다. B에서 누락된 관련 엔터티. 이는 집합 교집합을 포함하는 쿼리를 구성하는 데 사용된 두 집합의 교집합을 제한하는 동기를 부여합니다. 예를 들어 r = 0.95인 경우 rn 0.8을 사용하면 p &gt; 0.83을 보장할 수 있습니다. C 주석 세부 정보 QUEST의 주석 작업은 유료 계약자였던 참가자가 수행했습니다. 그들은 텍사스 오스틴에 거주하며 학사 학위(55%) 또는 이와 동등한 업무 경험(45%)이 있습니다. 그들은 업무에 대해 시간당 급여를 받았고 미국 영어에 대한 지식을 검토한 공급업체에서 모집되었습니다. 그들은 자신의 업무가 어떻게 사용될지 알려졌으며 거부할 수 있었습니다. 그들은 고용 국가의 생계 임금법을 준수하는 표준 계약 임금을 받았습니다. 주석 작성자에게 제시된 주석 인터페이스는 그림 4, 5 및 6에 나와 있습니다.D 사전 학습 데이터 기억의 영향 모델을 초기화하는 데 사용하는 T5 체크포인트는 C4 코퍼스(위키피디아 포함)에서 사전 학습되었으므로 이러한 모델이 위키피디아 카테고리 그래프의 측면을 기억했는지 조사합니다.T5 체크포인트의 사전 학습 날짜 이전에 생성된 위키피디아 문서에 대한 T5 기반 듀얼 인코더 모델의 재현율을 사전 학습 후에 추가된 문서와 비교합니다.이를 BM25 검색기를 사용하여 동일한 문서 세트에 대한 재현율과 함께 표 7에 보고합니다.기준선 Avg. Recall@Retriever Before After BM0.0.T5-Large DE 0.0.표 7: T5 사전 학습 전과 후에 생성된 문서 하위 세트에 대한 100에서의 평균 재현율. 사전 학습 전에 추가된 문서와 사전 학습 후에 추가된 문서 간의 점수 비율이 두 시스템에서 비슷한 것을 알 수 있는데, 이는 암기 이외의 요인이 차이를 설명할 수 있음을 시사합니다. 예를 들어, 사전 학습 날짜 이전에 생성된 문서와 사전 학습 날짜 이후에 생성된 문서의 평균 길이는 각각 759.7단어와 441.2단어입니다. 지침 이 작업에 참여해 주셔서 감사합니다! 이 작업에는 여러 구문으로 구성된 쿼리를 의역하여 원래 쿼리와 새 쿼리가 동일한 의미를 갖도록 하는 작업이 포함됩니다. 예를 들어, &quot;1990년대의 미국 영화가 아닌 미국 공포 영화&quot;와 같이 두 개의 다른 구문이 강조 표시된 쿼리가 주어질 수 있습니다. 쿼리가 유창하고 원래 쿼리에 제시된 모든 구문을 표현하며 사용자가 무엇을 찾고 있는지 명확하게 설명하는지 확인하기 위해 쿼리를 의역해야 합니다. 쿼리의 구조를 변경하고 적합하다고 생각되면 다시 표현하세요. 위의 예에서 이 질의를 &quot;1990년대가 아닌 미국의 공포 영화&quot;로 의역할 수 있습니다. 몇 가지 좋은 예가 있습니다. • &quot;일본 작가의 작품을 원작으로 한 영화는 아니지만 만화를 원작으로 한 실사 영화&quot; -&gt; &quot;일본이 아닌 만화를 원작으로 한 영화&quot;. &quot;베네수엘라의 파충류이지만 브라질의 파충류는 아닌 남미의 뱀&quot; -&gt; &quot;베네수엘라의 뱀이지만 브라질은 아님&quot;. &quot;터키를 배경으로 한 소설이 아닌 터키에 대한 책&quot; -&gt; &quot;터키에 대한 책이지만 터키를 배경으로 한 소설은 아닌 책&quot; &quot;중국에서 온 식물이기도 한 중국의 고사리&quot; -&gt; &quot;중국에서 발견된 고사리&quot;(모든 고사리는 식물이기 때문) • &quot;아프리카 동물이기도 한 남아프리카의 파충류&quot; -&gt; &quot;남아프리카 파충류&quot;(모든 파충류는 동물이고 모든 남아프리카 동물은 아프리카이기도 하기 때문) 다음은 좋지 않은 몇 가지 예입니다. &quot;유령에 대한 것이 아닌 공포 영화이기도 한 나이지리아 영화&quot; -&gt; &quot;나이지리아 공포 영화&quot;. - 이는 &quot;유령에 대한&quot; 제약 조건을 놓쳤습니다. • &quot;슈퍼히어로 영화이기도 한 이탈리아 모험 영화&quot; -&gt; &quot;모험이 있는 프랑스 슈퍼히어로 영화&quot; - 이 제약 조건은 프랑스 영화가 아닌 이탈리아 영화를 요구합니다. 쿼리의 의미가 명확하지 않거나 쿼리 결과가 빈 세트인 경우 건너뛰십시오. 예를 들어, &quot;호주 동물이 아닌 호주의 새&quot; 또는 &quot;프랑스어가 아닌 프랑스 역사 소설&quot;과 같은 질의는 의미가 없으므로 건너뛰어야 합니다. 질의: 하냐 야나기하라의 소설 또는 2022년 미국 소설 또는 인도 판타지 소설 새 질의: 지침 그림 4: 의역 단계에 대한 주석 인터페이스. 건너뛰기 제출 이 작업에 참여해 주셔서 감사합니다! 원래 질의와 의역된 버전의 질의가 제공됩니다. 의역된 질의가 유창하고, 정보 요구 사항을 정확하게 표현하며, 자연스러운지 판단해야 합니다. 새 질의에 대한 주어진 질문에 답하세요. 원래 질의: 1960년대 판타지 드라마 영화 또는 이탈리아 판타지 드라마 영화 새 질의: 1990년대가 아닌 미국의 공포 영화 Q1: 질의의 유창성은 어느 정도입니까? 유창함: 명확하고 문법적으로 올바릅니다. 대체로 유창함: 오류가 몇 개 있거나 자연스럽게 들리지 않지만 이해할 수 있습니다. 유창하지 않음: 오류가 많거나 거의 이해할 수 없음. Q2: 의역된 질의의 의미가 원래 질의의 의미와 얼마나 일치합니까? 동일한 의미: 의역된 질의는 원래 질의와 동일한 항목 집합을 요구합니다. 강조 표시된 모든 절이 포함되어 있습니다. 다른 의미: 질의가 다른 항목 집합을 요구합니다. 원래 질의의 강조 표시된 절 중 하나 이상이 의역된 질의에 표현되지 않았거나 의미가 변경되었습니다. 너무 모호함: 합리적인 판단을 내리기에는 너무 모호합니다. 두 질의의 일부 가능한 해석에 따르면 동일한 의미를 갖지만 다른 가능한 해석에 따르면 다른 의미를 갖습니다. Q3: 이 질의를 실제 사용자가 검색 엔진에 실행할 수 있습니까? 예 - 사용자가 이 질의를 실행할 수 있습니다. 아마도 - 질의가 틈새 시장의 관심을 표현하지만 사용자가 잠재적으로 그러한 질의를 실행할 수 있습니다. 아니요 - 이 질의는 매우 부자연스러운 정보 요구를 표현합니다. 사용자가 이런 질의를 발행할 가능성은 극히 낮아 보인다. 지침 그림 5: 검증 단계에 대한 주석 인터페이스. 1995년 텔레비전 영화, John Erman 감독 The Sunshine Boys는 1996년 미국의 코미디 텔레비전 영화로, John Erman이 감독했으며 Neil Simon의 1972년 동명 희곡을 원작으로 한다. 이 영화는 두 명의 전설적인(그리고 심술궂은) 코미디언이 재결합하여 유명한 연기를 부활시키는 내용이다. 이 영화는 Sarah Jessica Parker와 함께 코미디 듀오로 Woody Allen과 Peter Falk가 출연한다. 이 영화는 1997년 12월 28일 CBS의 &quot;Hallmark Hall of Fame&quot;에서 초연되었다. 줄거리. Al Lewis와 Willy Clark는 한때 &quot;Lewis and Clark&quot;라는 이름으로 알려진 인기 코미디언이었고 Sunshine Boys라고도 불렸던 두 명의 오래된 코미디언이다. 43년을 함께한 후, 11년 전에 적대적인 관계로 헤어졌고 그 이후로 서로 말을 나누지 않았다. 코미디의 역사를 다룬 주요 네트워크 특집에서 재결합을 계획하고 있다. 제작. 1995년, Simon은 자신의 희곡을 Hallmark Entertainment 프로덕션을 위해 각색했습니다. 그의 텔레플레이는 배경을 업데이트하고 두 코미디언을 극작가가 시작한 매체인 텔레비전의 초기 시절의 산물로 만들었습니다. 영화 각색과 달리, 그들은 심술궂게 묘사되기는 하지만, 그들의 적대감은 Matthau와 Burns의 캐릭터들의 나쁜 관계만큼 심하지 않았습니다. Woody Allen은 원래 1975년 영화 각색 &quot;The Sunshine Boys&quot;를 감독해 달라는 요청을 받았지만, 그는 Lewis 역을 맡는 데 더 관심이 있어서 제안을 거절했습니다. 20년 후, 그는 이 텔레비전 각색에서 Lewis 역으로 캐스팅되었습니다. 새 쿼리: 뉴욕에서 촬영된 미국의 텔레비전 및 코미디 영화 문서 추가 The Sunshine Boys(1996년 영화) 등급 확실히 관련성 있음 OL아마도 관련성 있음 OL아마도 관련성 없음 확실히 관련성 없음 증거 추가 ○ 완료 O 부분 증거 없음 그림 6: 관련성 레이블 지정 단계에 대한 주석 인터페이스. 귀속 건너뛰기 제출
