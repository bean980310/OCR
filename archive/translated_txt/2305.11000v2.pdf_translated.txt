--- ABSTRACT ---
다중 모달 대규모 언어 모델은 인공 일반 지능(AGI)을 향한 중요한 단계로 간주되며 ChatGPT의 등장으로 상당한 관심을 얻었습니다. 그러나 현재의 음성 언어 모델은 일반적으로 캐스케이드 패러다임을 채택하여 모달 간 지식 전달을 방해합니다. 이 논문에서는 다중 모델 콘텐츠를 인식하고 생성할 수 있는 고유한 교차 모달 대화 능력을 갖춘 대규모 언어 모델인 SpeechGPT를 제안합니다. 이산 음성 표현을 사용하여 먼저 대규모 교차 모달 음성 지시 데이터 세트인 SpeechInstruct를 구성합니다. 또한 모달 적응 사전 훈련, 교차 모달 지시 미세 조정, 모달 체인 지시 미세 조정을 포함하는 3단계 훈련 전략을 사용합니다. 실험 결과는 SpeechGPT가 다중 모달 인간 지시를 따르는 인상적인 능력을 가지고 있으며 하나의 모델로 여러 모달을 처리할 수 있는 잠재력을 강조한다는 것을 보여줍니다. 데모는 https://0nutation.github.io/SpeechGPT.github.io/에서 확인할 수 있습니다.
--- INTRODUCTION ---
대규모 언어 모델(OpenAI, 2023; Touvron et al., 2023)은 다양한 자연어 처리 작업에서 놀라운 성과를 보였습니다. 한편, GPT-4, PALM-E(Driess et al., 2023), LLaVA(Liu et al., 2023)와 같은 다중 모달 대규모 언어 모델은 LLM이 다중 모달 정보를 이해하는 능력을 탐구했습니다. 그러나 현재 LLM과 일반 인공 지능(AGI) 사이에는 상당한 격차가 있습니다. 첫째, 대부분의 현재 LLM은 다중 모달 콘텐츠만 인식하고 이해할 수 있지만 자발적으로 다중 모달 콘텐츠를 생성할 수 없습니다. 둘째, 이미지 및 음성과 같은 연속 신호는 이산 토큰을 수신하는 LLM에 직접 적용할 수 없습니다. 현재의 음성 언어 모델은 주로 계단식 패러다임을 채택합니다(Huang et al., 2023a). 즉, LLM을 자동 음성 인식(ASR) 모델이나 텍스트 음성(TTS) 모델과 함께 연결하거나 LLM을 제어 허브로 사용하여 여러 음성 처리 모델을 통합하여 여러 오디오 또는 음성 작업을 처리합니다(Huang et al., 2023a; Shen et al., 2023). 생성적 구어 모델에 대한 일부 이전 작업에는 음성 신호를 이산 표현으로 인코딩하고(Baevski et al., 2020; Hsu et al., 2021) 언어 모델로 모델링하는 것(Lakhotia et al., 2021; Borsos et al., 2022; Zhang et al., 2023b; Wang et al., 2023)이 포함됩니다. *책임 저자 프랑스어의 수도는 무엇입니까? 프랑스어의 수도는 파리입니다.(이름이 뭐예요?)(제 이름은 SpeechGPT입니다.) Speech GPT &quot;오늘은 아름다운 날입니다.&quot;라는 문장을 읽어주세요. 물론입니다. 지금 읽겠습니다. II 내용을 기록하세요. 음성의 내용은 &quot;좋은 하루 보내세요!&quot;입니다. 그림 1: 여러 교차 모달 작업을 처리하는 SpeechGPT의 기능. 음성을 인지하고 생성할 수 있지만 기존의 계단식 방법 또는 구어체 언어 모델은 여전히 몇 가지 한계가 있습니다. 첫째, 계단식 모델의 LLM은 콘텐츠 생성기로만 기능합니다. 음성과 텍스트의 표현이 정렬되지 않았기 때문에 LLM의 지식을 음성 모달리티로 전송할 수 없습니다. 둘째, 계단식 접근 방식(Shen et al., 2023; Huang et al., 2023a)은 감정과 음조와 같은 준언어적 신호가 손실되는 문제가 있습니다. 셋째, 기존의 구어 모델(Wang et al., 2023; Zhang et al., 2023b)은 음성을 합성할 뿐 의미 정보를 이해하지 못해 진정한 교차 모달 인식 및 생성을 달성하지 못합니다. 이 논문에서는 다중 모델 콘텐츠를 인식하고 생성할 수 있는 고유한 교차 모달 대화 능력을 갖춘 대규모 언어 모델인 SpeechGPT를 제안합니다. 우리는 음성과 텍스트 간의 모달을 통합하기 위해 자체 감독 학습된 음성 모델로 음성 이산화를 수행합니다. 그런 다음 이산 음성 토큰을 LLM의 어휘로 확장하여 모델에 음성을 인식하고 생성할 수 있는 고유한 역량을 부여합니다. 모델에 다중 모달 지침을 처리할 수 있는 기능을 제공하기 위해 첫 번째 음성 텍스트 교차 모달 지침 따르기 데이터 세트 SpeechInstruct를 구축합니다. 구체적으로, 우리는 음성을 이산 단위로 이산화하고(Hsu et al., 2021) 기존 ASR 데이터 세트를 기반으로 교차 모달 단위-텍스트 쌍을 구성합니다. 한편, 부록 B에 설명된 대로 실제 사용자 지침을 시뮬레이션하기 위해 GPT-4를 사용하여 다양한 작업에 대한 수백 개의 지침을 구성합니다. 또한 모델의 교차 모달 기능을 더욱 향상시키기 위해 Chain-of-Modality 지침 데이터를 설계했습니다. 즉, 모델은 음성 명령을 받고 텍스트에서 프로세스를 생각한 다음 음성으로 응답을 출력합니다. 더 나은 교차 모달 전송과 효율적인 교육을 위해 SpeechGPT는 모달리티 적응 사전 교육, 교차 모달 지침 미세 조정, 사슬 모달리티 지침 미세 조정의 3단계 교육 과정을 거칩니다. 첫 번째 단계에서는 이산 음성 단위 연속 작업을 통해 SpeechGPT에 대한 음성 이해가 가능합니다. 두 번째 단계에서는 SpeechInstruct를 사용하여 모델의 교차 모달 기능을 개선합니다. 세 번째 단계에서는 매개 변수 효율적인 LORA(Hu et al., 2021) 미세 조정을 사용하여 추가 모달리티 정렬을 수행합니다. SpeechGPT의 효과를 평가하기 위해 다양한 인간 평가와 사례 분석을 수행하여 텍스트 작업, 음성-텍스트 교차 모달 작업 및 구어 대화 작업에서 SpeechGPT의 성능을 추정합니다. 결과에 따르면 SpeechGPT는 단일 모달 및 교차 모달 지시 수행 작업과 구어 대화 작업에 강력한 능력을 보입니다. 기여 내용은 다음과 같습니다. • 다중 모달 콘텐츠를 인식하고 생성할 수 있는 최초의 다중 모달 대규모 언어 모델을 구축합니다. • 최초의 대규모 음성-텍스트 교차 모달 지시 수행 데이터 세트인 SpeechInstruct를 구축하여 출시합니다. 지시 조정 메타 프롬프트 음성 데이터 세트 텍스트 데이터 세트 GPT-Speech2Unit Text2Unit 지시 템플릿, 템플릿, Speechinstruct 교차 모달 지시 데이터 모달 체인 지시 데이터 교차 모달 지시 [인간]: 음성을 텍스트로 필사합니다. 입력은 다음과 같습니다. {음성 단위 U}<eoh> . [SpeechGPT]: {전사 T}<eos> . 모달리티 체인 지침 [인간]: 이것은 음성 지침입니다: (음성). 단계별로 수행할 수 있습니다. 지침을 필사하고, 텍스트 응답을 받고, 응답을 말할 수 있습니다.<eoh> [SpeechGPT]: [tq] {텍스트/ }; [ta] {텍스트 R}; [ua] {SpeechR}<eoa> . 전사: 안녕하세요, 저는 SpeechGPT입니다. 만나서 반갑습니다! Unit Vocoder [SpeechGPT]: &lt;99&gt; &lt;5&gt; &lt;69&gt; &lt;597&gt;...... &lt;31&gt; SpeechGPT [인간]: &lt;43&gt; &lt;2&gt; &lt;64&gt; &lt;33&gt; &lt;534&gt; 이산 음성 단위 추출기 전사: 안녕하세요, 이름이 뭐예요? 그림 2: 왼쪽: SpeechInstruct 구축 프로세스 개요. SpeechInstruct 데이터 세트는 두 부분으로 구성됩니다. 교차 모달 지시 데이터와 모달 체인 지시 데이터. 템플릿은 3.1에 나와 있습니다. 템플릿 2는 부록 C에 나와 있습니다. 오른쪽: SpeechGPT 모델 구조의 그림. • 우리는 강력한 인간 지시 추종 능력과 음성 대화 능력을 갖춘 최초의 음성 대화 LLM을 구축합니다. • 우리는 이산 표현을 통해 다른 모달리티를 LLM에 통합할 수 있는 큰 잠재력을 보여줍니다. 2
--- RELATED WORK ---
다중 모달 대규모 언어 모델 현재의 다중 모달 LLM은 주로 시각적 영역에 초점을 맞추고, 사전 훈련된 시각적 인코더에서 얻은 연속적 표현을 LLM에 공급하여 시각 언어 데이터에 대한 전체 매개변수 또는 매개변수 효율적 훈련을 용이하게 합니다(OpenAI, 2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E(Driess et al., 2023)는 540B PaLM(Chowdhery et al., 2022)과 22B Vision Transformer(Dosovitskiy et al., 2021)를 가장 큰 시각 언어 모델에 통합합니다. LLaVA(Liu et al., 2023)는 사전 훈련된 CLIP(Radford et al., 2021) 시각 인코더와 LLAMA(Touvron et al., 2023)를 활용하고 GPT4 지원 시각 지시 데이터에 대한 지시 튜닝을 수행합니다. X-LLM(Chen et al., 2023)은 다중 모달리티를 대규모 언어 모델의 입력으로 X2L 인터페이스를 사용하여 표현으로 변환합니다. 그러나 이러한 구조는 LLMS가 다중 모달 출력을 생성할 수 없는 다중 모달 입력만 처리할 수 있도록 합니다. 이전 연구와 달리, 우리의 접근 방식은 음성 중심 다중 모달 LLM의 개발을 강조하여 다중 모달 입력과 출력을 모두 수용할 수 있는 능력을 부여합니다. 생성적 구어체 언어 모델 이산적 자기 감독 표현 기반 구어체 생성 언어 모델링은 대규모 음성 데이터 세트 훈련에서 놀라운 진전을 이루고 있습니다(Nguyen et al., 2022). AudioLM(Borsos et al., 2022)은 텍스트 없는 환경에서 음성을 합성할 수 있는 의미 코드와 함께 오디오 코덱을 기반으로 음성을 모델링하는 것을 제안합니다.VALL-E(Wang et al., 2023)는 오디오 코덱에서 생성적 구어체 언어 모델을 구축하고 텍스트 음성 변환을 조건부 생성 작업으로 처리합니다.그러나 이러한 모델은 특정 작업을 위해 설계되었으며 LLM의 이점을 얻지 못했습니다.SpeechGPT는 LLM의 기초 위에 구축되었으며 LLM의 지식을 음성 모달리티로 전환하여 결과적으로 더 나은 작업 일반화와 인간 지시 따르기 능력을 얻습니다.음성 지원 LLM 상호 작용 ChatGPT가 등장한 후, 여러 연구는 LLM과 직접적인 음성 상호 작용을 가능하게 하기 위해 전문가 음성 모델을 LLM과 통합하는 데 집중했습니다. HuggingGPT(Shen et al., 2023)는 LLM의 인간 지시에 대한 작업 분해를 용이하게 하고, Huggingface의 모델을 호출하여 다양한 자동 음성 인식(ASR) 및 텍스트-음성 모델을 포함하는 특정 작업을 수행할 수 있도록 합니다.AudioGPT(Huang et al., 2023a)는 다양한 오디오 기반 모델을 활용하여 복잡한 오디오 정보를 처리하고 LLM을 음성 대화를 위한 입출력 인터페이스(ASR, TTS)에 연결합니다.그러나 이러한 모델은 복잡성이 증가하고, 광범위한 리소스를 요구하며, 불가피한 오류 누적 문제가 발생하기 쉽습니다.저희의 접근 방식은 ASR 또는 TTS 시스템에 의존하지 않고도 LLM과 음성 상호 작용을 가능하게 하여 앞서 언급한 단점을 우회합니다.SpeechInstruct 구성 공개적으로 사용 가능한 음성 데이터의 제한과 다양한 음성-텍스트 작업의 부족으로 인해 저희는 음성-텍스트 교차 모달 지시 따르기 데이터 세트인 SpeechInstruct를 구성합니다. 이 데이터 세트는 두 부분으로 구성되어 있습니다. 첫 번째 부분은 Cross-Modal Instruction이라고 하며 두 번째 부분은 Chain-of-Modality Instruction이라고 합니다. SpeechInstruct의 구축 프로세스는 그림 2에 나와 있습니다. 3.1 Cross-modal Instruction 데이터 수집 우리는 Gigaspeech(Chen et al., 2021), Common Voice(Ardila et al., 2020), LibriSpeech(Panayotov et al., 2015)를 포함하여 Cross-Modal Instruction을 구축하기 위해 여러 대규모 영어 ASR 데이터 세트를 수집합니다. 우리는 mHuBERT²를 음성 토크나이저로 사용하여 음성 데이터를 개별 단위로 분리하고 인접 프레임의 반복 단위를 제거하여 축소된 단위를 얻습니다. 궁극적으로 900만 개의 단위-텍스트 데이터 쌍을 얻습니다. 작업 설명 생성 우리는 음성-텍스트 데이터 쌍과 호환되는 ASR 및 TTS 작업 설명을 생성합니다. Self-Instruct와 달리
--- METHOD ---
s 또는 구어 모델에는 여전히 몇 가지 한계가 있습니다. 첫째, 계단식 모델의 LLM은 콘텐츠 생성기로만 기능합니다. 음성과 텍스트의 표현이 정렬되지 않았기 때문에 LLM의 지식을 음성 모달리티로 전송할 수 없습니다. 둘째, 계단식 접근 방식(Shen et al., 2023; Huang et al., 2023a)은 감정 및 음조와 같은 준언어적 신호가 손실되는 문제가 있습니다. 셋째, 기존 구어 모델(Wang et al., 2023; Zhang et al., 2023b)은 음성을 합성할 뿐 의미 정보를 이해하지 못해 진정한 교차 모달 인식 및 생성을 달성하지 못합니다. 이 논문에서는 다중 모델 콘텐츠를 인식하고 생성할 수 있는 고유한 교차 모달 대화 능력을 갖춘 대규모 언어 모델인 SpeechGPT를 제안합니다. 우리는 음성과 텍스트 간의 모달리티를 통합하기 위해 자체 감독 학습된 음성 모델로 음성 이산화를 수행합니다. 그런 다음 이산 음성 토큰을 LLM의 어휘로 확장하여 모델에 음성을 인식하고 생성할 수 있는 고유한 역량을 부여합니다. 모델에 다중 모달 지침을 처리할 수 있는 기능을 제공하기 위해 첫 번째 음성 텍스트 교차 모달 지침 따르기 데이터 세트 SpeechInstruct를 빌드합니다. 구체적으로 음성을 개별 단위로 분리하고(Hsu et al., 2021) 기존 ASR 데이터 세트를 기반으로 교차 모달 단위-텍스트 쌍을 구성합니다. 한편, 부록 B에 설명된 대로 GPT-4를 사용하여 다양한 작업에 대한 수백 개의 지침을 구성하여 실제 사용자 지침을 시뮬레이션합니다. 또한 모델의 교차 모달 기능을 더욱 향상시키기 위해 Chain-of-Modality 지침 데이터를 설계했습니다. 즉, 모델은 음성 명령을 받고 텍스트에서 프로세스를 생각한 다음 음성으로 응답을 출력합니다. 더 나은 교차 모달 전이와 효율적인 훈련을 위해 SpeechGPT는 모달 적응 사전 훈련, 교차 모달 지시 미세 조정, 모달 체인 지시 미세 조정의 3단계 훈련 프로세스를 거칩니다. 첫 번째 단계에서는 이산 음성 단위 연속 과제를 통해 SpeechGPT에 대한 음성 이해를 가능하게 합니다. 두 번째 단계에서는 SpeechInstruct를 사용하여 모델의 교차 모달 기능을 개선합니다. 세 번째 단계에서는 매개변수 효율적인 LORA(Hu et al., 2021) 미세 조정을 사용하여 모달을 더욱 정렬합니다. SpeechGPT의 효과를 평가하기 위해 다양한 인간 평가와 사례 분석을 수행하여 텍스트 과제, 음성-텍스트 교차 모달 과제, 음성 대화 과제에서 SpeechGPT의 성능을 추정합니다. 결과에 따르면 SpeechGPT는 단일 모달 및 교차 모달 지시 후속 과제와 음성 대화 과제에 대한 강력한 능력을 보여줍니다. 우리의 기여는 다음과 같습니다. • 우리는 멀티모달 콘텐츠를 인식하고 생성할 수 있는 최초의 멀티모달 대규모 언어 모델을 구축합니다. • 우리는 최초의 대규모 음성-텍스트 크로스모달 지시를 따르는 데이터 세트인 SpeechInstruct를 구축하고 출시합니다. 지시 조정 메타 프롬프트 음성 데이터 세트 텍스트 데이터 세트 GPT-Speech2Unit Text2Unit 지시 템플릿, 템플릿, Speechinstruct 크로스모달 지시 데이터 체인 오브 모달 지시 데이터 크로스모달 지시 [인간]: 음성을 텍스트로 필사합니다. 이것이 입력입니다: {음성 단위 U}<eoh> . [SpeechGPT]: {전사 T}<eos> . 모달리티 체인 지침 [인간]: 이것은 음성 지침입니다: (음성). 단계별로 수행할 수 있습니다. 지침을 필사하고, 텍스트 응답을 받고, 응답을 말할 수 있습니다.<eoh> [SpeechGPT]: [tq] {텍스트/ }; [ta] {텍스트 R}; [ua] {SpeechR}<eoa> . 전사: 안녕하세요, 저는 SpeechGPT입니다. 만나서 반갑습니다! Unit Vocoder [SpeechGPT]: &lt;99&gt; &lt;5&gt; &lt;69&gt; &lt;597&gt;...... &lt;31&gt; SpeechGPT [인간]: &lt;43&gt; &lt;2&gt; &lt;64&gt; &lt;33&gt; &lt;534&gt; 이산 음성 단위 추출기 전사: 안녕하세요, 이름이 뭐예요? 그림 2: 왼쪽: SpeechInstruct 구축 프로세스 개요. SpeechInstruct 데이터 세트는 두 부분으로 구성됩니다. 교차 모달 지시 데이터와 모달 체인 지시 데이터. 템플릿은 3.1에 나와 있습니다. 템플릿 2는 부록 C에 나와 있습니다. 오른쪽: SpeechGPT 모델 구조의 그림. • 우리는 강력한 인간 지시 추종 능력과 구두 대화 능력을 갖춘 최초의 구두 대화 LLM을 구축합니다. • 우리는 이산 표현을 통해 다른 모달리티를 LLM에 통합할 수 있는 큰 잠재력을 보여줍니다. 2 관련 연구 다중 모달 대규모 언어 모델 현재의 다중 모달 LLM은 주로 시각적 영역에 초점을 맞추고 있으며, 사전 훈련된 시각적 인코더에서 얻은 연속적 표현을 LLM에 공급하여 시각 언어 데이터에 대한 전체 매개변수 또는 매개변수 효율적 훈련을 용이하게 합니다(OpenAI, 2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E(Driess et al., 2023)는 540B PaLM(Chowdhery et al., 2022)과 22B Vision Transformer(Dosovitskiy et al., 2021)를 가장 큰 시각 언어 모델에 통합합니다. LLaVA(Liu et al., 2023)는 사전 훈련된 CLIP(Radford et al., 2021) 시각 인코더와 LLAMA(Touvron et al., 2023)를 활용하고 GPT4 지원 시각 지시 데이터에 대한 지시 튜닝을 수행합니다. X-LLM(Chen et al., 2023)은 다중 모달리티를 대규모 언어 모델의 입력으로 X2L 인터페이스를 사용하여 표현으로 변환합니다. 그러나 이러한 구조는 LLMS가 다중 모달 출력을 생성할 수 없는 다중 모달 입력만 처리할 수 있도록 합니다. 이전 연구와 달리, 우리의 접근 방식은 음성 중심 다중 모달 LLM의 개발을 강조하여 다중 모달 입력과 출력을 모두 수용할 수 있는 능력을 부여합니다. 생성적 구어체 언어 모델 이산적 자기 감독 표현 기반 구어체 생성 언어 모델링은 대규모 음성 데이터 세트 훈련에서 놀라운 진전을 이루고 있습니다(Nguyen et al., 2022). AudioLM(Borsos et al., 2022)은 텍스트 없는 환경에서 음성을 합성할 수 있는 의미 코드와 함께 오디오 코덱을 기반으로 음성을 모델링하는 것을 제안합니다.VALL-E(Wang et al., 2023)는 오디오 코덱에서 생성적 구어체 언어 모델을 구축하고 텍스트 음성 변환을 조건부 생성 작업으로 처리합니다.그러나 이러한 모델은 특정 작업을 위해 설계되었으며 LLM의 이점을 얻지 못했습니다.SpeechGPT는 LLM의 기초 위에 구축되었으며 LLM의 지식을 음성 모달리티로 전환하여 결과적으로 더 나은 작업 일반화와 인간 지시 따르기 능력을 얻습니다.음성 지원 LLM 상호 작용 ChatGPT가 등장한 후, 여러 연구는 LLM과 직접적인 음성 상호 작용을 가능하게 하기 위해 전문가 음성 모델을 LLM과 통합하는 데 집중했습니다. HuggingGPT(Shen et al., 2023)는 LLM의 인간 지시에 대한 작업 분해를 용이하게 하고, Huggingface의 모델을 호출하여 다양한 자동 음성 인식(ASR) 및 텍스트-음성 모델을 포함하는 특정 작업을 수행할 수 있도록 합니다.AudioGPT(Huang et al., 2023a)는 다양한 오디오 기반 모델을 활용하여 복잡한 오디오 정보를 처리하고 LLM을 음성 대화를 위한 입출력 인터페이스(ASR, TTS)에 연결합니다.그러나 이러한 모델은 복잡성이 증가하고, 광범위한 리소스를 요구하며, 불가피한 오류 누적 문제가 발생하기 쉽습니다.저희의 접근 방식은 ASR 또는 TTS 시스템에 의존하지 않고도 LLM과 음성 상호 작용을 가능하게 하여 앞서 언급한 단점을 우회합니다.SpeechInstruct 구성 공개적으로 사용 가능한 음성 데이터의 제한과 다양한 음성-텍스트 작업의 부족으로 인해 저희는 음성-텍스트 교차 모달 지시 따르기 데이터 세트인 SpeechInstruct를 구성합니다. 이 데이터 세트는 두 부분으로 구성되어 있습니다.첫 번째 부분은 Cross-Modal Instruction이라고 하고 두 번째 부분은 Chain-of-Modality Instruction이라고 합니다.SpeechInstruct의 구축 프로세스는 그림 2에 나와 있습니다.3.1 Cross-modal Instruction 데이터 수집 우리는 Gigaspeech(Chen et al., 2021), Common Voice(Ardila et al., 2020), LibriSpeech(Panayotov et al., 2015)를 포함하여 Cross-Modal Instruction을 구축하기 위해 여러 대규모 영어 ASR 데이터 세트를 수집합니다.우리는 mHuBERT²를 음성 토크나이저로 사용하여 음성 데이터를 개별 단위로 분리하고 인접 프레임의 반복적인 단위를 제거하여 축소된 단위를 얻습니다.궁극적으로 900만 개의 단위-텍스트 데이터 쌍을 얻습니다.작업 설명 생성 우리는 음성-텍스트 데이터 쌍과 호환되는 ASR 및 TTS 작업 설명을 생성합니다.Self-Instruct 방법(Wang et al., 2022)과 달리, 우리는 제로샷 접근 방식을 통해 설명을 생성합니다. 구체적으로, 부록 A에 표시된 프롬프트를 OpenAI GPT-4에 직접 입력하여 작업 설명을 생성합니다. 저희의 생성 방법은 각 작업에 대해 100개의 명령어를 생성하며, 부록 B에 몇 가지 예가 나와 있습니다. 명령어 포맷팅 불연속 단위 시퀀스 U와 연관된 전사본 T에 대해, 확률 p에 따라 ASR 작업을 구성하는 데 사용할지 TTS 작업을 구성하는 데 사용할지 결정합니다. 그런 다음, 해당 작업 설명에서 설명 D를 무작위로 선택합니다. 그러면 작업 설명, 불연속 단위 시퀀스, 전사본으로 구성된 삼중항이 생성되고 (D, U, T)로 표시됩니다. 그런 다음, 삼중항은 템플릿 [Human]:{D}를 사용하여 명령어로 조립됩니다. 이것이 입력입니다: {U}<eoh> .[음성GPT]: {T}<eos> .. 멀티턴 대화를 지원하기 위해 조립된 명령어는 모델의 최대 입력 길이를 준수하여 멀티턴 대화 형태로 연결됩니다. 3.2 모달리티 체인 명령어 음성 명령어 생성 음성 입력 및 음성 출력이 있는 명령어 데이터가 부족하기 때문에 텍스트-유닛 생성기를 학습하여 텍스트 명령어 데이터를 음성 명령어 데이터로 변환했습니다. 구체적으로 텍스트-유닛 생성기는 Transformer 인코더-디코더 아키텍처를 채택합니다. 이를 Cross-modal Instruction의 LibriSpeech 단위-텍스트 쌍에서 학습했습니다. 응답 길이가 35단어보다 짧은 moss-002-sft-data 데이터 세트 ³에서 37,969개의 샘플을 선택합니다. 그리고 텍스트-유닛 생성기를 통해 명령어와 응답을 모두 단위 시퀀스로 변환합니다. 결과적으로 음성 명령, 텍스트 명령, 텍스트 응답 및 음성 응답으로 구성된 37,969개의 4중체를 얻었으며 이를 (SpeechI, TextI, TextR, SpeechR)로 표시합니다.명령어 형식 위의 4중체를 사용하여 음성 명령-음성 응답, 음성 명령-텍스트 응답, 텍스트 명령-음성 응답 및 텍스트 명령-텍스트 응답의 4가지 입출력 형식에 대한 사고 사슬 스타일 명령을 구성할 수 있습니다.해당 템플릿은 부록 C에서 찾을 수 있습니다.SpeechGPT 4.1 모델 구조 통합 프레임워크는 다양한 모달리티에서 아키텍처 호환성을 제공하도록 설계되었습니다.그림 2에서 볼 수 있듯이 모델은 개별 단위 추출기, 대규모 언어 모달 및 단위 보코더의 세 가지 주요 구성 요소로 구성됩니다.이 아키텍처에서 LLM은 다중 모달 입력을 인식하고 다중 모달 출력을 생성할 수 있습니다. 이산 단위 추출기 이산 단위 추출기는 Hidden-unit BERT(HUBERT) 모델(Hsu et al., 2021)을 활용하여 연속적인 음성 신호를 이산 단위 시퀀스로 변환합니다. 2 https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt 3 https://huggingface.co/datasets/fnlp/moss-002-sft-dataHUBERT는 모델의 중간 표현에 적용된 k-평균 클러스터링을 기반으로 마스크된 오디오 세그먼트에 대한 이산 레이블을 예측하여 학습하는 자체 감독 모델입니다. 1-D 합성곱 계층과 Transformer 인코더를 결합하여 음성을 연속적인 중간 표현으로 인코딩하고, k-평균 모델은 이러한 표현을 클러스터 인덱스 시퀀스로 추가로 변환합니다. 이어서, 인접한 중복 인덱스가 제거되어 U(u1, U2,..., UT), uiЄ 0, 1, ..., K − 1, V1 ≤ i ≤T, =로 표현되는 이산 단위 시퀀스가 생성됩니다. 여기서 K는 총 클러스터 수를 나타냅니다. 대규모 언어 모델 우리는 Meta AI LLAMA(Touvron et al., 2023) 모델을 대규모 언어 모델로 사용합니다. LLAMA는 임베딩 계층, 여러 변환기 블록 및 LM 헤드 계층으로 구성됩니다. LLaMA의 총 매개변수 수는 7B에서 65B까지입니다. 1조 개의 토큰으로 구성된 광범위한 학습 데이터 세트를 기반으로 한 LLaMA는 다양한 NLP 벤치마크에서 훨씬 더 큰 175B GPT-3와 비교하여 경쟁력 있는 성능을 보여줍니다. 단위 보코더 (Polyak et al., 2021)에서 단일 스피커 유닛 보코더의 한계로 인해, 우리는 이산 표현으로부터 음성 신호를 디코딩하기 위해 다중 스피커 유닛 HiFi-GAN을 훈련합니다.HiFi-GAN 아키텍처는 생성기 G와 여러 개의 판별기 D로 구성됩니다.생성기는 룩업 테이블(LUT)을 사용하여 이산 표현을 임베딩하고 임베딩 시퀀스는 전치 합성곱과 확장된 레이어가 있는 잔여 블록으로 구성된 일련의 블록으로 업샘플링됩니다.화자 임베딩은 업샘플링된 시퀀스의 각 프레임에 연결됩니다.판별기는 (Polyak et al., 2021)과 동일한 아키텍처를 가진 다중 주기 판별기(MPD)와 다중 스케일 판별기(MSD)를 특징으로 합니다.4.2 훈련 음성 이산 표현을 LLM에 통합하기 위해 먼저 어휘와 해당 임베딩 행렬을 확장합니다.우리는 훈련 과정을 세 단계로 나눕니다. 첫 번째 단계는 짝이 맞지 않는 음성 데이터에 대한 ModalityAdaptation 사전 학습입니다.두 번째 단계는 Cross-modal Instruction Fine-Tuning입니다.세 번째 단계는 Chain-of-Modality Instruction Fine-Tuning입니다.어휘 확장 원래 LLM 어휘 V의 크기가 |V|일 때, 음성 이산 표현을 LLM에 통합하기 위해 |V&#39;|= K 크기의 추가 단위 토큰 V&#39;로 어휘를 확장합니다.확장된 어휘 V&quot;는 원래 어휘 V와 새 단어 V&#39;의 합집합입니다.V&quot; = VUV&#39; (1) E 원래 단어 임베딩 행렬을 E € R|V| ×d로 표시합니다.여기서 d는 단어 임베딩의 차원입니다. 확장된 어휘를 수용하기 위해 무작위로 초기화된 단어 임베딩 행렬 E&#39; = RV&quot;|×d를 생성해야 합니다. E의 값을 E&#39;의 첫 번째 |V| 행에 복사하여 원래 단어 임베딩을 보존합니다. E&#39; [0 : [V], :] = E (2) 마지막으로 원래 어휘와 단어 임베딩 행렬을 새 어휘 V&quot;와 단어 임베딩 행렬 E&#39;로 바꿉니다. 1단계: 모달리티 적응 사전 학습 LLM이 이산 단위 모달리티를 처리할 수 있도록 레이블이 없는 음성 코퍼스를 활용하여 다음 토큰 예측 작업에서 LLM을 학습합니다. 이 접근 방식은 LLM의 텍스트 사전 학습 목표와 일치합니다. 음성 U1, U2,..., Um 및 LLM으로 구성된 레이블이 없는 음성 코퍼스 C가 주어지고 L1로 표시되면 음의 로그 가능도 손실은 다음과 같이 공식화할 수 있습니다. m nj L(LC) = log P(ui,j|u
--- EXPERIMENT ---
모든 결과는 SpeechGPT가 다중 모달 인간 지시를 따르는 인상적인 능력을 가지고 있으며 하나의 모델로 다중 모달을 처리할 수 있는 잠재력을 강조한다는 것을 보여줍니다. 데모는 https://0nutation.github.io/SpeechGPT.github.io/에서 볼 수 있습니다. 서론 대규모 언어 모델(OpenAI, 2023; Touvron et al., 2023)은 다양한 자연어 처리 작업에서 놀라운 성과를 보였습니다. 한편, GPT-4, PALM-E(Driess et al., 2023), LLaVA(Liu et al., 2023)와 같은 다중 모달 대규모 언어 모델은 LLM이 다중 모달 정보를 이해하는 능력을 탐구했습니다. 그러나 현재 LLM과 일반 인공 지능(AGI) 사이에는 상당한 격차가 있습니다. 첫째, 대부분의 현재 LLM은 다중 모달 콘텐츠만 인식하고 이해할 수 있지만 자발적으로 다중 모달 콘텐츠를 생성할 수 없습니다. 둘째, 이미지와 음성과 같은 연속적인 신호는 이산 토큰을 수신하는 LLM에 직접 적용할 수 없습니다. 현재의 음성-언어 모델은 주로 캐스케이딩 패러다임을 채택합니다(Huang et al., 2023a). 즉, LLM은 자동 음성 인식(ASR) 모델이나 텍스트 음성(TTS) 모델과 함께 연결되거나 LLM은 제어 허브로 사용되며, 여러 음성 처리 모델이 통합되어 여러 오디오 또는 음성 작업을 처리합니다(Huang et al., 2023a; Shen et al., 2023). 생성적 구어 모델에 대한 일부 이전 작업에는 음성 신호를 이산 표현으로 인코딩(Baevski 등, 2020; Hsu 등, 2021)하고 언어 모델로 모델링(Lakhotia 등, 2021; Borsos 등, 2022; Zhang 등, 2023b; Wang 등, 2023)하는 작업이 포함되었습니다. *책임 저자 프랑스의 수도는 어디입니까? 프랑스의 수도는 파리입니다. (이름이 뭐예요?) (제 이름은 SpeechGPT입니다.) Speech GPT 다음 문장을 읽어주세요: &quot;오늘은 아름다운 날입니다.&quot; 물론입니다. 지금 읽겠습니다. II 내용을 기록하세요: 음성의 내용은 &quot;좋은 하루 보내세요!&quot;입니다. 그림 1: 여러 교차 모달 작업을 처리하는 SpeechGPT의 기능 기존의 캐스케이딩 방법 또는 구어 모델에는 음성을 인식하고 생성할 수 있지만 여전히 여러 가지 한계가 있습니다. 첫째, 계단식 모델의 LLM은 콘텐츠 생성기로만 기능합니다. 음성과 텍스트의 표현이 정렬되지 않았기 때문에 LLM의 지식을 음성 모달리티로 전송할 수 없습니다. 둘째, 계단식 접근 방식(Shen et al., 2023; Huang et al., 2023a)은 감정 및 음조와 같은 준언어적 신호가 손실되는 문제가 있습니다. 셋째, 기존의 구어 모델(Wang et al., 2023; Zhang et al., 2023b)은 음성을 합성할 뿐 의미 정보를 이해하지 못해 진정한 교차 모달 인식 및 생성을 달성하지 못합니다. 이 논문에서는 다중 모델 콘텐츠를 인식하고 생성할 수 있는 고유한 교차 모달 대화 능력을 갖춘 대규모 언어 모델인 SpeechGPT를 제안합니다. 음성과 텍스트 간의 모달리티를 통합하기 위해 자체 감독 학습된 음성 모델로 음성 이산화를 수행합니다. 그런 다음 이산 음성 토큰을 LLM의 어휘로 확장하여 모델에 음성을 인식하고 생성할 수 있는 고유한 역량을 부여합니다. 모델에 다중 모달 지침을 처리할 수 있는 기능을 제공하기 위해 첫 번째 음성 텍스트 교차 모달 지침 따르기 데이터 세트 SpeechInstruct를 빌드합니다. 구체적으로 음성을 개별 단위로 분리하고(Hsu et al., 2021) 기존 ASR 데이터 세트를 기반으로 교차 모달 단위-텍스트 쌍을 구성합니다. 한편, 부록 B에 설명된 대로 GPT-4를 사용하여 다양한 작업에 대한 수백 개의 지침을 구성하여 실제 사용자 지침을 시뮬레이션합니다. 또한 모델의 교차 모달 기능을 더욱 향상시키기 위해 Chain-of-Modality 지침 데이터를 설계했습니다. 즉, 모델은 음성 명령을 받고 텍스트에서 프로세스를 생각한 다음 음성으로 응답을 출력합니다. 더 나은 교차 모달 전이와 효율적인 훈련을 위해 SpeechGPT는 모달 적응 사전 훈련, 교차 모달 지시 미세 조정, 모달 체인 지시 미세 조정의 3단계 훈련 프로세스를 거칩니다. 첫 번째 단계에서는 이산 음성 단위 연속 과제를 통해 SpeechGPT에 대한 음성 이해를 가능하게 합니다. 두 번째 단계에서는 SpeechInstruct를 사용하여 모델의 교차 모달 기능을 개선합니다. 세 번째 단계에서는 매개변수 효율적인 LORA(Hu et al., 2021) 미세 조정을 사용하여 모달을 더욱 정렬합니다. SpeechGPT의 효과를 평가하기 위해 다양한 인간 평가와 사례 분석을 수행하여 텍스트 과제, 음성-텍스트 교차 모달 과제, 음성 대화 과제에서 SpeechGPT의 성능을 추정합니다. 결과에 따르면 SpeechGPT는 단일 모달 및 교차 모달 지시 후속 과제와 음성 대화 과제에 대한 강력한 능력을 보여줍니다. 우리의 기여는 다음과 같습니다. • 우리는 멀티모달 콘텐츠를 인식하고 생성할 수 있는 최초의 멀티모달 대규모 언어 모델을 구축합니다. • 우리는 최초의 대규모 음성-텍스트 크로스모달 지시를 따르는 데이터 세트인 SpeechInstruct를 구축하고 출시합니다. 지시 조정 메타 프롬프트 음성 데이터 세트 텍스트 데이터 세트 GPT-Speech2Unit Text2Unit 지시 템플릿, 템플릿, Speechinstruct 크로스모달 지시 데이터 체인 오브 모달 지시 데이터 크로스모달 지시 [인간]: 음성을 텍스트로 필사합니다. 이것이 입력입니다: {음성 단위 U}<eoh> . [SpeechGPT]: {전사 T}<eos> . 모달리티 체인 지침 [인간]: 이것은 음성 지침입니다: (음성). 단계별로 수행할 수 있습니다. 지침을 필사하고, 텍스트 응답을 받고, 응답을 말할 수 있습니다.<eoh> [SpeechGPT]: [tq] {텍스트/ }; [ta] {텍스트 R}; [ua] {SpeechR}<eoa> . 전사: 안녕하세요, 저는 SpeechGPT입니다. 만나서 반갑습니다! Unit Vocoder [SpeechGPT]: &lt;99&gt; &lt;5&gt; &lt;69&gt; &lt;597&gt;...... &lt;31&gt; SpeechGPT [인간]: &lt;43&gt; &lt;2&gt; &lt;64&gt; &lt;33&gt; &lt;534&gt; 이산 음성 단위 추출기 전사: 안녕하세요, 이름이 뭐예요? 그림 2: 왼쪽: SpeechInstruct 구축 프로세스 개요. SpeechInstruct 데이터 세트는 두 부분으로 구성됩니다. 교차 모달 지시 데이터와 모달 체인 지시 데이터. 템플릿은 3.1에 나와 있습니다. 템플릿 2는 부록 C에 나와 있습니다. 오른쪽: SpeechGPT 모델 구조의 그림. • 우리는 강력한 인간 지시 추종 능력과 구두 대화 능력을 갖춘 최초의 구두 대화 LLM을 구축합니다. • 우리는 이산 표현을 통해 다른 모달리티를 LLM에 통합할 수 있는 큰 잠재력을 보여줍니다. 2 관련 연구 다중 모달 대규모 언어 모델 현재의 다중 모달 LLM은 주로 시각적 영역에 초점을 맞추고 있으며, 사전 훈련된 시각적 인코더에서 얻은 연속적 표현을 LLM에 공급하여 시각 언어 데이터에 대한 전체 매개변수 또는 매개변수 효율적 훈련을 용이하게 합니다(OpenAI, 2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E(Driess et al., 2023)는 540B PaLM(Chowdhery et al., 2022)과 22B Vision Transformer(Dosovitskiy et al., 2021)를 가장 큰 시각 언어 모델에 통합합니다. LLaVA(Liu et al., 2023)는 사전 훈련된 CLIP(Radford et al., 2021) 시각 인코더와 LLAMA(Touvron et al., 2023)를 활용하고 GPT4 지원 시각 지시 데이터에 대한 지시 튜닝을 수행합니다. X-LLM(Chen et al., 2023)은 다중 모달리티를 대규모 언어 모델의 입력으로 X2L 인터페이스를 사용하여 표현으로 변환합니다. 그러나 이러한 구조는 LLMS가 다중 모달 출력을 생성할 수 없는 다중 모달 입력만 처리할 수 있도록 합니다. 이전 연구와 달리, 우리의 접근 방식은 음성 중심 다중 모달 LLM의 개발을 강조하여 다중 모달 입력과 출력을 모두 수용할 수 있는 능력을 부여합니다. 생성적 구어체 언어 모델 이산적 자기 감독 표현 기반 구어체 생성 언어 모델링은 대규모 음성 데이터 세트 훈련에서 놀라운 진전을 이루고 있습니다(Nguyen et al., 2022). AudioLM(Borsos et al., 2022)은 텍스트 없는 환경에서 음성을 합성할 수 있는 의미 코드와 함께 오디오 코덱을 기반으로 음성을 모델링하는 것을 제안합니다.VALL-E(Wang et al., 2023)는 오디오 코덱에서 생성적 구어체 언어 모델을 구축하고 텍스트 음성 변환을 조건부 생성 작업으로 처리합니다.그러나 이러한 모델은 특정 작업을 위해 설계되었으며 LLM의 이점을 얻지 못했습니다.SpeechGPT는 LLM의 기초 위에 구축되었으며 LLM의 지식을 음성 모달리티로 전환하여 결과적으로 더 나은 작업 일반화와 인간 지시 따르기 능력을 얻습니다.음성 지원 LLM 상호 작용 ChatGPT가 등장한 후, 여러 연구는 LLM과 직접적인 음성 상호 작용을 가능하게 하기 위해 전문가 음성 모델을 LLM과 통합하는 데 집중했습니다. HuggingGPT(Shen et al., 2023)는 LLM의 인간 지시에 대한 작업 분해를 용이하게 하고, Huggingface의 모델을 호출하여 다양한 자동 음성 인식(ASR) 및 텍스트-음성 모델을 포함하는 특정 작업을 수행할 수 있도록 합니다.AudioGPT(Huang et al., 2023a)는 다양한 오디오 기반 모델을 활용하여 복잡한 오디오 정보를 처리하고 LLM을 음성 대화를 위한 입출력 인터페이스(ASR, TTS)에 연결합니다.그러나 이러한 모델은 복잡성이 증가하고, 광범위한 리소스를 요구하며, 불가피한 오류 누적 문제가 발생하기 쉽습니다.저희의 접근 방식은 ASR 또는 TTS 시스템에 의존하지 않고도 LLM과 음성 상호 작용을 가능하게 하여 앞서 언급한 단점을 우회합니다.SpeechInstruct 구성 공개적으로 사용 가능한 음성 데이터의 제한과 다양한 음성-텍스트 작업의 부족으로 인해 저희는 음성-텍스트 교차 모달 지시 따르기 데이터 세트인 SpeechInstruct를 구성합니다. 이 데이터 세트는 두 부분으로 구성되어 있습니다.첫 번째 부분은 Cross-Modal Instruction이라고 하고 두 번째 부분은 Chain-of-Modality Instruction이라고 합니다.SpeechInstruct의 구축 프로세스는 그림 2에 나와 있습니다.3.1 Cross-modal Instruction 데이터 수집 우리는 Gigaspeech(Chen et al., 2021), Common Voice(Ardila et al., 2020), LibriSpeech(Panayotov et al., 2015)를 포함하여 Cross-Modal Instruction을 구축하기 위해 여러 대규모 영어 ASR 데이터 세트를 수집합니다.우리는 mHuBERT²를 음성 토크나이저로 사용하여 음성 데이터를 개별 단위로 분리하고 인접 프레임의 반복적인 단위를 제거하여 축소된 단위를 얻습니다.궁극적으로 900만 개의 단위-텍스트 데이터 쌍을 얻습니다.작업 설명 생성 우리는 음성-텍스트 데이터 쌍과 호환되는 ASR 및 TTS 작업 설명을 생성합니다.Self-Instruct 방법(Wang et al., 2022)과 달리, 우리는 제로샷 접근 방식을 통해 설명을 생성합니다. 구체적으로, 부록 A에 표시된 프롬프트를 OpenAI GPT-4에 직접 입력하여 작업 설명을 생성합니다. 저희의 생성 방법은 각 작업에 대해 100개의 명령어를 생성하며, 부록 B에 몇 가지 예가 나와 있습니다. 명령어 포맷팅 불연속 단위 시퀀스 U와 연관된 전사본 T에 대해, 확률 p에 따라 ASR 작업을 구성하는 데 사용할지 TTS 작업을 구성하는 데 사용할지 결정합니다. 그런 다음, 해당 작업 설명에서 설명 D를 무작위로 선택합니다. 그러면 작업 설명, 불연속 단위 시퀀스, 전사본으로 구성된 삼중항이 생성되고 (D, U, T)로 표시됩니다. 그런 다음, 삼중항은 템플릿 [Human]:{D}를 사용하여 명령어로 조립됩니다. 이것이 입력입니다: {U}<eoh> .[음성GPT]: {T}<eos> .. 멀티턴 대화를 지원하기 위해 조립된 명령어는 모델의 최대 입력 길이를 준수하여 멀티턴 대화 형태로 연결됩니다. 3.2 모달리티 체인 명령어 음성 명령어 생성 음성 입력 및 음성 출력이 있는 명령어 데이터가 부족하기 때문에 텍스트-유닛 생성기를 학습하여 텍스트 명령어 데이터를 음성 명령어 데이터로 변환했습니다. 구체적으로 텍스트-유닛 생성기는 Transformer 인코더-디코더 아키텍처를 채택합니다. 이를 Cross-modal Instruction의 LibriSpeech 단위-텍스트 쌍에서 학습했습니다. 응답 길이가 35단어보다 짧은 moss-002-sft-data 데이터 세트 ³에서 37,969개의 샘플을 선택합니다. 그리고 텍스트-유닛 생성기를 통해 명령어와 응답을 모두 단위 시퀀스로 변환합니다. 결과적으로 음성 명령, 텍스트 명령, 텍스트 응답 및 음성 응답으로 구성된 37,969개의 4중체를 얻었으며 이를 (SpeechI, TextI, TextR, SpeechR)로 표시합니다.명령어 형식 위의 4중체를 사용하여 음성 명령-음성 응답, 음성 명령-텍스트 응답, 텍스트 명령-음성 응답 및 텍스트 명령-텍스트 응답의 4가지 입출력 형식에 대한 사고 사슬 스타일 명령을 구성할 수 있습니다.해당 템플릿은 부록 C에서 찾을 수 있습니다.SpeechGPT 4.1 모델 구조 통합 프레임워크는 다양한 모달리티에서 아키텍처 호환성을 제공하도록 설계되었습니다.그림 2에서 볼 수 있듯이 모델은 개별 단위 추출기, 대규모 언어 모달 및 단위 보코더의 세 가지 주요 구성 요소로 구성됩니다.이 아키텍처에서 LLM은 다중 모달 입력을 인식하고 다중 모달 출력을 생성할 수 있습니다. 이산 단위 추출기 이산 단위 추출기는 Hidden-unit BERT(HUBERT) 모델(Hsu et al., 2021)을 활용하여 연속적인 음성 신호를 이산 단위 시퀀스로 변환합니다. 2 https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt 3 https://huggingface.co/datasets/fnlp/moss-002-sft-dataHUBERT는 모델의 중간 표현에 적용된 k-평균 클러스터링을 기반으로 마스크된 오디오 세그먼트에 대한 이산 레이블을 예측하여 학습하는 자체 감독 모델입니다. 1-D 합성곱 계층과 Transformer 인코더를 결합하여 음성을 연속적인 중간 표현으로 인코딩하고, k-평균 모델은 이러한 표현을 클러스터 인덱스 시퀀스로 추가로 변환합니다. 이어서, 인접한 중복 인덱스가 제거되어 U(u1, U2,..., UT), uiЄ 0, 1, ..., K − 1, V1 ≤ i ≤T, =로 표현되는 이산 단위 시퀀스가 생성됩니다. 여기서 K는 총 클러스터 수를 나타냅니다. 대규모 언어 모델 우리는 Meta AI LLAMA(Touvron et al., 2023) 모델을 대규모 언어 모델로 사용합니다. LLAMA는 임베딩 계층, 여러 변환기 블록 및 LM 헤드 계층으로 구성됩니다. LLaMA의 총 매개변수 수는 7B에서 65B까지입니다. 1조 개의 토큰으로 구성된 광범위한 학습 데이터 세트를 기반으로 한 LLaMA는 다양한 NLP 벤치마크에서 훨씬 더 큰 175B GPT-3와 비교하여 경쟁력 있는 성능을 보여줍니다. 단위 보코더 (Polyak et al., 2021)에서 단일 스피커 유닛 보코더의 한계로 인해, 우리는 이산 표현으로부터 음성 신호를 디코딩하기 위해 다중 스피커 유닛 HiFi-GAN을 훈련합니다.HiFi-GAN 아키텍처는 생성기 G와 여러 개의 판별기 D로 구성됩니다.생성기는 룩업 테이블(LUT)을 사용하여 이산 표현을 임베딩하고 임베딩 시퀀스는 전치 합성곱과 확장된 레이어가 있는 잔여 블록으로 구성된 일련의 블록으로 업샘플링됩니다.화자 임베딩은 업샘플링된 시퀀스의 각 프레임에 연결됩니다.판별기는 (Polyak et al., 2021)과 동일한 아키텍처를 가진 다중 주기 판별기(MPD)와 다중 스케일 판별기(MSD)를 특징으로 합니다.4.2 훈련 음성 이산 표현을 LLM에 통합하기 위해 먼저 어휘와 해당 임베딩 행렬을 확장합니다.우리는 훈련 과정을 세 단계로 나눕니다. 첫 번째 단계는 짝이 맞지 않는 음성 데이터에 대한 ModalityAdaptation 사전 학습입니다.두 번째 단계는 Cross-modal Instruction Fine-Tuning입니다.세 번째 단계는 Chain-of-Modality Instruction Fine-Tuning입니다.어휘 확장 원래 LLM 어휘 V의 크기가 |V|일 때, 음성 이산 표현을 LLM에 통합하기 위해 |V&#39;|= K 크기의 추가 단위 토큰 V&#39;로 어휘를 확장합니다.확장된 어휘 V&quot;는 원래 어휘 V와 새 단어 V&#39;의 합집합입니다.V&quot; = VUV&#39; (1) E 원래 단어 임베딩 행렬을 E € R|V| ×d로 표시합니다.여기서 d는 단어 임베딩의 차원입니다. 확장된 어휘를 수용하기 위해 무작위로 초기화된 단어 임베딩 행렬 E&#39; = RV&quot;|×d를 생성해야 합니다. E의 값을 E&#39;의 첫 번째 |V| 행에 복사하여 원래 단어 임베딩을 보존합니다. E&#39; [0 : [V], :] = E (2) 마지막으로 원래 어휘와 단어 임베딩 행렬을 새 어휘 V&quot;와 단어 임베딩 행렬 E&#39;로 바꿉니다. 1단계: 모달리티 적응 사전 학습 LLM이 이산 단위 모달리티를 처리할 수 있도록 레이블이 없는 음성 코퍼스를 활용하여 다음 토큰 예측 작업에서 LLM을 학습합니다. 이 접근 방식은 LLM의 텍스트 사전 학습 목표와 일치합니다. 음성 U1, U2,..., Um 및 LLM으로 구성된 레이블이 없는 음성 코퍼스 C가 주어지고 L1로 표시되면 음의 로그 가능도 손실은 다음과 같이 공식화할 수 있습니다. m nj L(LC) = log P(ui,j|u
--- CONCLUSION ---
이 연구에서는 멀티모달 콘텐츠를 인식하고 생성할 수 있는 고유한 크로스 모달 멀티모달 대규모 언어 모델인 SpeechGPT를 제시합니다. 또한 현재 음성 도메인에서 명령어 데이터 세트의 부족을 완화하기 위해 SpeechInstruct를 제안합니다. 이 첫 번째 음성-텍스트 크로스 모달 명령어-추종 데이터 세트에는 체인 오브 모달 메커니즘을 기반으로 하는 크로스 모달 명령어 데이터와 구어 대화 데이터가 포함되어 있습니다. 향상된 크로스 모달 성능을 얻기 위해 3단계 학습 패러다임을 채택하여 최종 SpeechGPT를 얻습니다. 실험 결과에 따르면 SpeechGPT는 다양한 단일 모달 또는 크로스 모달 작업에서 유망한 결과를 달성하며 개별 음성 토큰을 언어 모델에 결합하는 것이 유망한 방향임을 보여줍니다. 참고문헌 Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, FM, and Weber, G. Common voice: A massively-multilingual speech corpus, 2020. Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. 정렬을 위한 실험실로서의 일반 언어 보조원, 2021. Baevski, A., Zhou, Y., Mohamed, A. 및 Auli, M. wav2vec 2.0: 음성 표현의 자기 지도 학습을 위한 프레임워크. 신경 정보 처리 시스템의 발전, 33: 12449–12460, 2020. Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M. 및 Zeghidour, N. Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식, 2022. Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, SX 및 Xu, B. X-llm: 다중 양식을 외국어로 처리하여 고급 대규모 언어 모델 부트스트래핑. 2023. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y., You, Z., and Yan, Z. Gigaspeech: 10,000시간 분량의 필사 오디오가 포함된 진화하는 다중 도메인 ASR 코퍼스, 2021. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, HW, Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., 다이, AM, 필라이, TS, 펠랏, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with paths, 2022. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 이미지의 가치는 16x단어입니다: 규모에 따른 이미지 인식을 위한 변환기, 2021.Driess, D., Xia, F., Sajjadi, MS, Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: 구현된 다중 모드 언어 모델입니다. arXiv 사전 인쇄 arXiv:2303.03378, 2023. Hsu, W.-N., Bolte, B., Tsai, Y.-HH, Lakhotia, K., Salakhutdinov, R. 및 Mohamed, A. Hubert: 숨겨진 단위의 마스크 예측을 통한 자기 지도 음성 표현 학습. IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 29:3451-3460, 2021. Hu, EJ, Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: 대규모 언어 모델의 저순위 적응, 2021. Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y., Zhao, Z., and Watanabe, S. Audiogpt: 음성, 음악, 소리 및 토킹 헤드 이해 및 생성, 2023a. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, OK, Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, F. 언어만이 필요한 전부는 아니다: 언어 모델에 따른 인식의 정렬, 2023b. Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A. 및 Dupoux, E. Libri-light: ASR에 대한 벤치마크 제한적이거나 감독이 없습니다. ICASSP 2020 2020 IEEE 음향, 음성 및 신호 처리에 관한 국제 컨퍼런스(ICASSP). IEEE, 2020년 5월. doi: 10.1109/icassp40776.2020.9052942. URL https://doi.org/10.1109% 2Ficassp40776.2020.9052942. Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., et al. 원시 오디오에서 생성적 구어 모델링에 관하여. Association for Computational Linguistics의 논문집, 9:1336-1354, 2021. Liu, H., Li, C., Wu, Q., and Lee, YJ Visual instruction tuning. arXiv 사전 인쇄 arXiv:2304.08485, 2023. Nguyen, TA, Kharitonov, E., Copet, J., Adi, Y., Hsu, W.-N., Elkahky, A., Tomasello, P., Algayres, R., Sagot, B., Mohamed, A. 및 Dupoux, E. 생성 음성 대화 언어 모델링, 2022. 오픈AI. Gpt-4 기술 보고서, 2023년. Panayotov, V., Chen, G., Povey, D. 및 Khudanpur, S. Librispeech: 공개 도메인 오디오 북을 기반으로 한 ASR 코퍼스. 2015년 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 5206-5210쪽, 2015. doi: 10.1109/ICASSP.2015.7178964. Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux, E. 이산적으로 분리된 자기 감독 표현으로부터의 음성 재합성, 2021. Radford, A., Kim, JW, Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. 자연어 감독으로부터의 학습 가능한 시각적 모델, 2021. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: huggingface에서 chatgpt와 그 친구들로 ai 작업 해결, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. 신경 코덱 언어 모델은 제로샷 텍스트 음성 합성기입니다., 2023. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, NA, Khashabi, D., and Hajishirzi, H. 자체 지시: 언어 모델을 자체 생성 지침과 정렬, 2022. Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Zero-init attention을 사용한 언어 모델의 효율적인 미세 조정, 2023a. Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling, 2023b.A 과제 설명 생성을 위한 프롬프트 ASR: 음성 인식에 대한 100개의 다양한 과제 지침 세트를 생각해내라는 요청을 받았습니다. 요구 사항은 다음과 같습니다. 1. 이러한 지침은 누군가에게 다음 음성의 내용을 인식하도록 지시해야 합니다. 2. 다양성을 극대화하기 위해 각 지침에 대해 동사를 반복하지 마십시오. 3. 지시에 사용되는 언어도 다양해야 합니다. 예를 들어, 질문을 필수 지침과 결합해야 합니다. 4. 지시의 유형은 다양해야 합니다. 5. 지시는 영어로 작성해야 합니다. 6. 지시는 1~2문장 길이여야 합니다. 명령형 문장이든 질문이든 허용됩니다. 100개 과제 목록: TTS: 음성 인식에 관한 텍스트 음성 변환에 대한 100개의 다양한 과제 지시 세트를 생각해 내라는 요청을 받았습니다. 요구 사항은 다음과 같습니다. 1. 이 지시는 누군가에게 다음 음성의 내용을 인식하도록 지시하는 것이어야 합니다. 2. 다양성을 극대화하기 위해 각 지시에 대해 동사를 반복하지 마십시오. 3. 지시에 사용되는 언어도 다양해야 합니다. 예를 들어, 질문과 명령형 지시를 결합해야 합니다. 4. 지시의 유형은 다양해야 합니다. 5. 지시는 영어로 작성해야 합니다. 6. 지시는 1~2문장 길이여야 합니다. 명령형 문장이든 질문이든 허용됩니다. 100개 과제 목록: B 과제 설명 예 ASR: 말한 단어를 서면 텍스트로 변환하는 것으로 시작합니다. 연설을 서면 형식으로 필사할 수 있습니까? 들리는 내용을 텍스트로 번역하는 데 집중하세요. 연설을 주의 깊게 듣고 필사하세요. 연설 내용을 적어주시겠어요? 연설을 분석하고 서면 필사본을 만드세요. 연설에 참여하여 텍스트 기반 버전을 만드세요. 연설을 서면 형태로 문서화할 수 있나요? 말한 단어를 정확하게 텍스트로 변환하세요. 연설 내용을 글로 옮기는 건 어떨까요? TTS: 이 문장을 소리 내어 읽어주시겠어요? 다음 단어를 평소처럼 낭송하세요. 이 진술을 명확하게 표현하도록 목소리를 높이세요. 이 단어들을 가능한 한 자연스럽게 말해 주시겠어요? 주어진 문장을 부드럽게 속삭이세요. 이 문장의 각 단어를 정확하게 발음하세요. 이 문장을 대화의 어조로 어떻게 표현하시겠어요? 아래의 메시지를 구두로 전달해 주시겠어요? 문장을 읽는 동안 핵심 요점을 강조하세요. 제공된 텍스트를 멜로디한 목소리로 부르세요.C 모달리티 사슬 지침 템플릿 음성 지침-음성 응답: [인간]: 이것은 음성 지침입니다: {음성I}. 그리고 당신의 응답은 말로 해야 합니다. 단계별로 할 수 있습니다. 먼저 지시를 필사하고 텍스트 지시를 받을 수 있습니다. 그런 다음 지시에 대해 생각하고 텍스트 응답을 받을 수 있습니다. 마지막으로 응답을 소리 내어 말해야 합니다.<eoh> . [SpeechGPT]: [tq] {텍스트I}; [타] {TextR}; [ua] {SpeechR}<eoa> . 음성 지시-텍스트 응답: [인간]: 이것은 음성 지시입니다: {SpeechI}. 그리고 응답은 텍스트여야 합니다. 단계별로 할 수 있습니다. 먼저 지시를 필사하여 텍스트 지시를 받을 수 있습니다. 그런 다음 지시에 대해 생각하고 텍스트 응답을 받을 수 있습니다.<eoh> . [SpeechGPT]: [tq] {텍스트]}; [ta] {텍스트R}<eoa> . 텍스트 지시-음성 응답: [인간]: 이것은 텍스트 지시입니다: {TextI}. 그리고 응답은 음성이어야 합니다. 단계별로 수행할 수 있습니다. 지시에 대해 생각하고 텍스트 응답을 받을 수 있습니다. 그런 다음 응답을 소리 내어 말해야 합니다.<eoh> . [SpeechGPT]: [ta] {텍스트R}; [ua] {음성R}<eoa> . 텍스트 지시-텍스트 응답: [인간]: 이것은 텍스트 지시입니다: {TextI}. 그리고 응답은 텍스트여야 합니다. 지시에 대해 생각하고 텍스트 응답을 받을 수 있습니다. [SpeechGPT]: [ta] {TextR}<eoa> . D 하이퍼파라미터 StageStageStageBatch 크기Peak 학습률 2e-2e-2e-Max 길이학습 단계LORA 순위LORA 알파학습 가능 매개변수 13B 13B 6M 학습 장치 96 × A96 × A100 8 × A표 3: SpeechGPT 학습 하이퍼파라미터.
