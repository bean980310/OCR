--- ABSTRACT ---
인간은 상호 작용을 허용하는 여러 잠재적 객체를 묘사하는 단일 이미지를 쉽게 이해할 수 있습니다. 우리는 이 기술을 사용하여 세상과의 상호 작용을 계획하고 상호 작용에 참여하지 않고도 새로운 객체에 대한 이해를 가속화합니다. 이 논문에서는 지능형 에이전트가 3D 장면을 더 잘 탐색하거나 객체를 조작할 수 있도록 기계에 유사한 능력을 부여하고자 합니다. 우리의 접근 방식은 객체의 3D 위치, 물리적 속성 및 가능성을 예측하는 변환기 기반 모델입니다. 이 모델을 구동하기 위해 인터넷 비디오, 자기중심적 비디오 및 실내 이미지가 있는 데이터 세트를 수집하여 접근 방식을 훈련하고 검증합니다. 우리의 모델은 데이터에서 강력한 성능을 제공하고 로봇 데이터에 잘 일반화됩니다. 1.
--- INTRODUCTION ---
그림 1에서 무엇을 할 수 있을까요? 이 단일 RGB 이미지는 여러 객체와 상호 작용할 수 있는 풍부하고 상호 작용적인 3D 세계를 전달합니다. 예를 들어, 두 손으로 의자를 잡으면 딱딱한 객체로 움직일 수 있습니다. 베개는 자유롭게 집어 올려 압축할 수 있습니다. 문은 움직일 수 있지만 회전만 가능합니다. 장면에서 잠재적 가능성을 인식하고 해석하는 이러한 능력은 인간이 상호 작용을 계획하고 객체와 상호 작용하는 방법을 더 빨리 배우는 데 도움이 됩니다. 이 연구의 목표는 컴퓨터에도 동일한 능력을 부여하는 것입니다. 단일 3D 이미지에서 잠재적 상호 작용에 대한 이러한 이해를 얻는 것은 컴퓨터 비전의 여러 다른 하위 분야에 걸쳐 있기 때문에 장면 이해의 현재 기술을 넘어섭니다. 예를 들어, 단일 이미지 3D는 상당한 진전을 이루었지만[54, 51, 73, 20], 주로 장면이 어떻게 될 수 있는지가 아니라 있는 그대로의 장면에 초점을 맞춥니다. 관절을 이해하는 데 대한 관심이 증가하고 있지만[37, 70, 53], 이러한 연구는 발생할 수 있는 관절이 아닌 3D 모델이나 신중하게 수집된 데모에서 발생하는 관절에 주로 초점을 맞춥니다. 마지막으로 로봇이 상호 작용과 잠재적인 상호 작용 지점을 학습할 수 있도록 하는 데 대한 오랜 연구가 있지만[52, 61], 이러한 연구는 주로 동일한 환경(예: 실험실)에서의 평가에 초점을 맞추고 완전히 새로운 환경에서 이해를 적용하는 데 초점을 맞추지 않습니다. 우리는 (1) 문제 공식화, (2) 까다로운 이미지에 대한 풍부한 주석 데이터 세트, (3) 변환기 기반 접근 방식을 개발하여 이러한 상호 작용 이해를 부트스트랩하는 것을 제안합니다. 우리는 관절을 인식하는 문제를 쿼리 위치에서의 예측 문제로 구성합니다. 이미지와 2D 위치가 주어지면 우리 방법은 &quot;여기서 무엇을 할 수 있는가?&quot;라는 질문에 답하는 것을 목표로 합니다. Myst와 같은 고전적인 포인트앤클릭 게임의 스타일로.우리는 일반적인 질문 집합을 통해 &quot;여기서 무엇을 할 수 있는가&quot;를 구성합니다.객체를 움직일 수 있는지, 움직일 때의 범위와 3D에서의 위치, 강성, 움직임에 제약이 있는지, 그리고 어떻게 객체와 상호 작용할 것인지에 대한 추정치입니다.하류 전송의 잠재력을 극대화하기 위해, 우리의 질문은 특정 손이나 엔드 이펙터에 국한되지 않고 일반적인 것으로 선택됩니다.어디에서 행동해야 하는지 또는 객체의 자유도를 아는 것은 엔드 이펙터별 기술을 여전히 배워야 하더라도 강화 학습을 가속화할 수 있습니다.작업을 해결하기 위해, 우리는 변환기 기반 모델을 도입합니다.섹션 5에 설명된 우리의 접근 방식은 객체 감지의 발전과 전문성을 기반으로 하기 위해 Segment-Anything[33]과 같은 감지 백본을 기반으로 합니다.우리는 각각의 &quot;여기서 무엇을 할 수 있는가&quot; 작업을 예측하고 종단 간 학습이 가능한 추가 헤드로 백본을 확장합니다.우리의 공식화의 이점으로, 우리는 희소 주석에 대해 시스템을 학습할 수 있습니다. 우리는 이것이 결국 우리의 직접적 감독을 비디오를 통한 감독으로 전환하는 데 도움이 될 것이라고 믿습니다.우리의 접근 방식을 뒷받침하는 것은 섹션 4에 설명된 새로운 데이터 세트로, 이를 3D 객체 상호 작용 데이터 세트(3DOI)라고 명명했습니다.새로운 환경으로 일반화할 가능성을 최대화하기 위해 기본 데이터는 인터넷 및 자기중심적 비디오와 장면 레이아웃의 3D 렌더링과 같은 다양한 소스에서 제공됩니다.우리는 이 데이터에 대한 작업에 대한 주석을 제공하고, 데이터 소스 덕분에 자연스럽게 깊이와 법선의 형태로 3D 감독을 얻습니다.총체적으로 데이터 세트에는 10,000개 이미지에 걸쳐 50,000개 이상의 객체와 상호 작용이 불가능한 객체(예: 바닥, 벽)에 대한 31,000개 이상의 주석이 있습니다.섹션 6의 실험은 우리의 접근 방식이 잠재적 상호 작용을 얼마나 잘 인식하는지 테스트하며, 3DOI의 보이지 않는 데이터와 로봇 데이터를 모두 테스트합니다.우리는 데모 데이터[53, 50] 및 합성 데이터[70]에서 일반화하는 것을 포함한 여러 대안과 비교하고 대체 네트워크 설계도 비교합니다. 우리의 접근 방식은 이러한 모델보다 성능이 뛰어나며 로봇 데이터 세트 WHIRL [1]에 대한 강력한 일반화를 보여줍니다. 요약하자면, 우리는 우리의 주요 기여를 다음과 같이 봅니다. (1) 단일 RGB 이미지에서 3D 객체 상호 작용을 감지하는 새로운 작업; (2) 상호 작용이 가능한 객체와 해당 위치, 어포던스 및 물리적 속성을 포함하는 최초의 대규모 데이터 세트인 3D 객체 상호 작용 데이터 세트; (3) 이 문제를 해결하기 위한 변압기 기반 모델로, 3DOI 데이터 세트와 로봇 데이터에서 강력한 성능을 보입니다. 2.
--- RELATED WORK ---
s 저희 논문은 단일 이미지에서 3D 객체 상호작용을 추출하는 것을 제안합니다. 이 문제는 3D 비전, 객체 감지, 인간-객체 상호작용 및 장면 이해의 교차점에 있습니다. 또한 다운스트림 로봇 응용 프로그램과 밀접한 관련이 있습니다. 대화형 장면 이해. 최근 컴퓨터 비전 커뮤니티는 객체의 3D 역학을 이해하는 데 점점 더 관심을 갖고 있습니다. 이는 인간-객체 상호작용[5, 19, 58]에서 동기를 얻었지만 인간이 우리 환경에 존재할 필요는 없습니다. 연구자들은 합성 데이터[48, 70, 49, 29, 68, 37, 66], 비디오[53, 24, 21, 50, 45, 36] 또는 포인트 클라우드[30, 28]에서 3D 모양, 축, 가동 부품 및 가능성을 이해하려고 시도합니다. 우리의 작업은 주로 [53, 24, 21]과 관련이 있는데, 이들은 실제 이미지에서 작동하지만 두 가지 관점에서 다릅니다.(1) 이들은 비디오 또는 멀티뷰 입력이 필요하지만, 우리의 입력은 단일 이미지에 불과합니다.(2) 이들의 접근 방식은 상호 작용 중인 객체를 복구하는 반면, 우리의 접근 방식은 상호 작용이 발생하기 전에 잠재적인 상호 작용을 이해합니다.마지막으로, OPD [29, 62]는 관절이 있는 객체에 대한 유사한 문제를 다루지만, 우리의 접근 방식은 관절이 없는 객체에도 작동합니다.객체 감지.앵커 기반 객체 감지 파이프라인을 훈련하는 것은 기본적으로 Mask RCNN [26, 34, 56, 32]의 파이프라인을 따릅니다.변압기 기반 모델의 개발이 진행됨에 따라, DETR [4], AnchorDETR [67] 및 MaskFormer [7]는 객체 감지를 직접 세트 예측 문제로 접근합니다.최근 Kirillov 등은 점이나 상자와 같은 입력 프롬프트에서 객체 마스크를 예측하는 Segment Anything Model [33]을 제안했습니다. 영어: 저희 네트워크는 디코더 기반 백본[4, 7, 33]에 구축되어야 하며 최첨단 성능으로 인해 SAM[33]을 선택했습니다.단일 이미지 3D. 저희 문제는 단일 이미지에서 2D 대신 3D 객체 상호 작용을 복구해야 하므로 단일 이미지 3D와도 관련이 있습니다.최근 몇 년 동안 연구자들은 깊이[73, 54, 39, 6, 15], 표면 법선[64, 16], 3D 평면[43, 42, 31] 및 모양[9, 46, 20, 51]을 포함하여 단일 이미지에서 3D를 복구하는 다양한 접근 방식을 개발했습니다.저희의 작업은 이들의 작업을 기반으로 합니다.특히 저희 아키텍처는 분할과 깊이 추정을 위해 ViT를 훈련하는 DPT[54]에서 동기를 얻었습니다.로봇 조작.객체 조작은 로봇 공학의 장기적 목표입니다. 연구자들은 관절이 있는 물체[61, 52, 10, 12, 69, 23]부터 변형 가능한 물체[71, 72, 65, 8]까지 다양한 장면의 다양한 종류의 물체에 대한 다양한 솔루션을 개발했습니다. 조작이 본 논문의 목표는 아니지만 3D에서 물체와 환경을 이해하는 것은 일반적으로 조작 파이프라인의 중요한 부분입니다. 본 논문은 주로 지각 부분을 개선하여 잠재적으로 조작을 개선할 수 있습니다. 따라서 본 논문에서는 로봇 데이터[1]에 대한 접근 방식도 테스트하여 일반화할 수 있음을 보여줍니다. 3. 개요 단일 이미지가 주어지면 쿼리 지점에 있는 물체로 &quot;여기서 무엇을 할 수 있을까?&quot;라는 질문에 답할 수 있는 것이 목표입니다. 4절에서 주석과
--- METHOD ---
&quot;여기서 내가 무엇을 할 수 있을까?&quot;라는 질문에 답하는 것을 목표로 합니다. Myst와 같은 고전적인 포인트앤클릭 게임의 스타일로.우리는 일반적인 질문 집합을 통해 &quot;여기서 무엇을 할 수 있는가&quot;를 구성합니다.객체를 움직일 수 있는지, 움직일 때의 범위와 3D에서의 위치, 강성, 움직임에 제약이 있는지, 그리고 어떻게 객체와 상호 작용할 것인지에 대한 추정치입니다.하류 전송의 잠재력을 극대화하기 위해, 우리의 질문은 특정 손이나 엔드 이펙터에 국한되지 않고 일반적인 것으로 선택됩니다.어디에서 행동해야 하는지 또는 객체의 자유도를 아는 것은 엔드 이펙터별 기술을 여전히 배워야 하더라도 강화 학습을 가속화할 수 있습니다.작업을 해결하기 위해, 우리는 변환기 기반 모델을 도입합니다.섹션 5에 설명된 우리의 접근 방식은 객체 감지의 발전과 전문성을 기반으로 하기 위해 Segment-Anything[33]과 같은 감지 백본을 기반으로 합니다.우리는 각각의 &quot;여기서 무엇을 할 수 있는가&quot; 작업을 예측하고 종단 간 학습이 가능한 추가 헤드로 백본을 확장합니다.우리의 공식화의 이점으로, 우리는 희소 주석에 대해 시스템을 학습할 수 있습니다. 우리는 이것이 결국 우리의 직접 감독을 비디오를 통한 감독으로 전환하는 데 도움이 될 것이라고 믿습니다. 우리의 접근 방식을 뒷받침하는 것은 섹션 4에 설명된 새로운 데이터 세트로, 우리는 이를 3D 객체 상호 작용 데이터 세트(3DOI)라고 명명했습니다. 새로운 환경으로 일반화할 가능성을 최대화하기 위해 기본 데이터는 인터넷 및 자기중심적 비디오와 장면 레이아웃의 3D 렌더링과 같은 다양한 소스에서 제공됩니다. 우리는 이 데이터에 대한 우리의 작업에 대한 주석을 제공하고, 데이터 소스로 인해 자연스럽게 깊이와 법선의 형태로 3D 감독을 얻습니다. 전체적으로 데이터 세트에는 10,000개 이미지에 걸쳐 50,000개 이상의 객체와 상호 작용할 수 없는 객체(예: 바닥, 벽)에 대한 31,000개 이상의 주석이 있습니다. 우리의
--- EXPERIMENT ---
6절에서는 우리의 접근 방식이 잠재적 상호작용을 얼마나 잘 인식하는지 테스트하고, 3DOI의 보이지 않는 데이터와 로봇 데이터를 모두 테스트합니다. 우리는 데모 데이터[53, 50]와 합성 데이터[70]에서 일반화하는 것과 대체 네트워크 설계를 포함한 여러 대안과 비교합니다. 우리의 접근 방식은 이러한 모델보다 성능이 뛰어나며 로봇 데이터 세트 WHIRL[1]에 대한 강력한 일반화를 보여줍니다. 요약하자면, 우리는 우리의 주요 공헌을 다음과 같이 봅니다. (1) 단일 RGB 이미지에서 3D 객체 상호작용을 감지하는 새로운 작업; (2) 상호작용이 가능한 객체와 해당 위치, 가능성 및 물리적 속성을 포함하는 최초의 대규모 데이터 세트인 3D 객체 상호작용 데이터 세트; (3) 이 문제를 해결하기 위한 변압기 기반 모델로, 3DOI 데이터 세트와 로봇 데이터에서 강력한 성능을 보입니다. 2. 관련 연구 우리 논문은 단일 이미지에서 3D 객체 상호작용을 추출하는 것을 제안합니다. 이 문제는 3D 비전, 객체 감지, 인간-객체 상호작용 및 장면 이해의 교차점에 있습니다. 또한 다운스트림 로봇 응용 프로그램과도 밀접한 관련이 있습니다.대화형 장면 이해.최근 컴퓨터 비전 커뮤니티는 객체의 3D 역학을 이해하는 데 점점 더 관심을 갖고 있습니다.인간-객체 상호 작용에서 동기를 얻었지만 [5, 19, 58], 인간이 우리 환경에 존재할 필요는 없습니다.연구자들은 합성 데이터 [48, 70, 49, 29, 68, 37, 66], 비디오 [53, 24, 21, 50, 45, 36] 또는 포인트 클라우드 [30, 28]에서 3D 모양, 축, 가동 부품 및 가능성을 이해하려고 시도합니다.저희 작업은 주로 [53, 24, 21]과 관련이 있는데, 실제 이미지에서 작업하지만 두 가지 측면에서 다릅니다.(1) 비디오 또는 멀티뷰 입력이 필요하지만 저희의 입력은 단일 이미지에 불과합니다.(2) 그들의 접근 방식은 상호 작용 중인 객체를 복구하는 반면 저희의 접근 방식은 상호 작용이 발생하기 전에 잠재적인 상호 작용을 이해합니다. 마지막으로, OPD[29, 62]는 관절이 있는 물체에 대한 유사한 문제를 다루지만, 우리의 것은 관절이 없는 물체에도 작동합니다.물체 감지.앵커 기반 객체 감지 파이프라인을 훈련하는 것은 기본적으로 Mask RCNN[26, 34, 56, 32]의 파이프라인을 따릅니다.변압기 기반 모델의 개발이 진행됨에 따라 DETR[4], AnchorDETR[67] 및 MaskFormer[7]는 객체 감지를 직접 세트 예측 문제로 접근합니다.최근 Kirillov 등은 점이나 상자와 같은 입력 프롬프트에서 객체 마스크를 예측하는 Segment Anything Model[33]을 제안합니다.우리의 네트워크는 디코더 기반 백본[4, 7, 33]에 구축되어야 하며, 최첨단 성능으로 인해 SAM[33]을 선택했습니다.단일 이미지 3D.우리의 문제는 단일 이미지에서 2D 대신 3D 객체 상호 작용을 복구해야 하므로 단일 이미지 3D와도 관련이 있습니다. 최근 몇 년 동안 연구자들은 깊이[73, 54, 39, 6, 15], 표면 법선[64, 16], 3D 평면[43, 42, 31] 및 모양[9, 46, 20, 51]을 포함하여 단일 이미지에서 3D를 복구하는 여러 가지 접근 방식을 개발했습니다. 저희의 연구는 그들의 업적을 기반으로 합니다. 특히 저희 아키텍처는 분할과 깊이 추정을 위해 ViT를 훈련하는 DPT[54]에서 동기를 얻었습니다. 로봇 조작. 물체 조작은 로봇의 장기적 목표입니다. 연구자들은 관절이 있는 물체[61, 52, 10, 12, 69, 23]에서 변형 가능한 물체[71, 72, 65, 8]에 이르기까지 다양한 장면의 다양한 종류의 물체에 대한 다양한 솔루션을 개발했습니다. 조작이 저희 논문의 목표는 아니지만 3D에서 물체와 환경을 이해하는 것은 일반적으로 조작 파이프라인의 중요한 부분입니다. 저희 논문은 주로 지각 부분을 개선하는데, 이는 잠재적으로 조작을 개선할 수 있습니다.따라서 저희는 또한 로봇 데이터[1]에서 저희의 접근 방식을 테스트하여 일반화할 수 있음을 보여줍니다.3. 개요 단일 이미지가 주어졌을 때, 저희의 목표는 쿼리 지점에 있는 객체로 &quot;여기서 무엇을 할 수 있을까?&quot;라는 질문에 답할 수 있는 것입니다.4절에서 주석을 소개하고, 5절에서 이 작업에 대한 방법을 소개합니다.그렇게 하기 전에, 저희는 답하는 질문에 대한 통합된 설명과 이러한 질문을 선택한 근거를 제시합니다.저희는 질문을 여섯 가지 속성 유형으로 그룹화하는데, 그 중 일부는 더 세분화됩니다.모든 객체가 모든 질문을 지원하는 것은 아닙니다.예를 들어, 움직일 수 없는 객체는 다른 속성이 없고, 자유롭게 움직일 수 있는 객체는 회전 축이 없습니다. 또한 일부 객체가 이러한 2.2.onehand rigid_tral twohand nonrigid twohand nonrigid twohand rigid trans_pull onehand ntwohand vohand nonrigidionrigiɗonehand_no twohand rigid free onehand rigid_rot_pull wohand rigid freel r twohand nonrigid onehand nonri twohand nonrigidnehand nonrigid twohand rigid fre onehand rigid rot push twohand rigid free onehand_nonrigid onehand_nonr twohand nonrigid onehand rigid free 그림 2. 3DOI 데이터 세트의 예시 주석. 이미지는 인터넷 비디오[53], 자기중심적 비디오[11] 및 3D 데이터 세트의 렌더링[14]에서 가져왔습니다. 는 쿼리 지점이고 는 어포던스입니다. 속성 볼 조인트는 예를 들어 2D 부분 공간을 허용합니다. 우리의 목표는 큰 부분 공간을 식별하는 것입니다. 푸프 모션 잠재적 상호 작용. 이동 가능 가장 중요한 하위 구분은 쿼리 지점의 객체를 이동할 수 있는지 여부입니다. 이것은 3D 장면 이해[60]와 인간-객체 상호 작용[58]에서 객체를 얼마나 움직일 수 있는지에 따라 세분화하는 작업에 따른 것입니다.우리는 객체를 얼마나 쉽게 움직일 수 있는지에 따라 객체를 세 가지 범주로 그룹화합니다.(1) 벽과 바닥과 같이 효과적으로 움직일 수 없는 고정물;(2) 물병이나 캐비닛 문과 같이 한 손으로 움직일 수 있는 한 손 객체;(3) 대형 TV와 같이 움직이려면 두 손이 필요한 객체.우리는 작업을 3단계 분류로 구성합니다.로컬화 객체의 범위를 이해하는 것이 중요하므로 세계에서 객체를 로컬화합니다.우리의 객체는 매우 다양한 범주로 구성되어 있으므로 로컬화를 [26, 4]에서와 같이 2D 인스턴스 분할과 3D에서 객체를 로컬화하기 위한 깊이 맵으로 구성합니다.[54, 73].이러한 속성은 대부분의 객체에 대해 추정할 수 있습니다. 강성 동작을 이해하기 위한 한 가지 주요 구분은 강체와 비강체입니다.강체 물체는 상당히 간단한 운동 규칙에 따르기 때문입니다[38].따라서 물체가 강체인지 아닌지 분류합니다.조작 대부분의 강체 물체는 자유형, 회전/회전 또는 평행 이동/프리즘 운동을 허용하는 것으로 더 분해할 수 있습니다[61].이러한 각각은 효과적으로 상호 작용하려면 다른 엔드이펙터 상호 작용이 필요합니다.조작 범주를 3방향 분류 문제로 구성하고 회전 축을 [53]에 따른 선 예측 문제로 인식합니다.동작 또한 물체와 상호 작용하기 위한 잠재적인 동작이 무엇인지 이해하고자 합니다.여기서는 당기기, 밀기 또는 기타의 세 가지 유형의 동작에 초점을 맞춥니다.가능성 마지막으로 물체와 상호 작용해야 하는 위치를 알고 싶습니다.예를 들어 문을 열려면 손잡이를 조작해야 합니다.가능성의 위치 위에 있는 확률 맵을 예측합니다. 4. 3D 객체 상호작용 데이터 세트 우리의 기여의 중요한 구성 요소 중 하나는 공개적으로 사용 가능한 데이터가 없기 때문에 객체 상호작용에 대한 정확한 주석입니다. 이 논문에서는 첫 번째 데이터 세트인 3D 객체 상호작용 데이터 세트(3DOI)를 소개합니다. 우리는 3D 데이터 세트를 포함하여 3D와 쉽게 통합할 수 있는 데이터를 선택하여 접근 방식을 훈련하기 위한 정확한 3D 기준 진실을 얻었습니다. 우리 데이터의 예는 그림 2에 나와 있습니다. 이미지. 우리의 목표는 실제 세계 시나리오를 나타내는 다양한 이미지를 선택하는 것입니다. 특히, 우리는 우리의 이미지에 우리가 상호작용할 수 있는 많은 일상의 물체를 포함하고 싶습니다. 따라서 공개적으로 사용 가능한 데이터 세트 컬렉션에서 10,000개의 이미지를 샘플링합니다. (1) Articulation[53]은 3인칭 Creative Commons 인터넷 비디오에서 가져옵니다. 일반적으로 비디오 클립에는 가정에서 관절이 있는 물체를 조작하는 사람이 포함됩니다. 우리는 관절 데이터 세트에서 무작위로 3,000개의 이미지를 샘플링합니다. (2) EpicKitchen[11]에는 주방 환경에서 음식을 만드는 자기중심적인 비디오가 포함됩니다. 우리는 EpicKitchen에서 2K 이미지를 샘플링합니다.(3) Taskonomy[74]는 실제 2D 이미지와 해당 3D 기준 진실을 포함하는 실내 3D 데이터 세트입니다.우리는 Omnidata[14]의 렌더링을 사용합니다.우리는 Omnidata 스타터 데이터 세트의 taskonomy 분할에서 5k 이미지를 샘플링합니다.전반적으로 10K 이미지가 있습니다.주석.상호 작용할 수 있는 잠재적 객체가 있는 이미지 컬렉션을 사용하여 수동 주석으로 전환합니다.단일 이미지의 경우 크고 작은 객체를 모두 포함하여 약 5개의 상호 작용 가능한 쿼리 포인트를 선택합니다.각 쿼리 포인트에 대해 다음에 주석을 달았습니다.(이동 가능;) 한 손, 두 손 또는 고정물.(지역화 1) 이 포인트가 속한 부분의 경계 상자 및 마스크.(강성) 강성 또는 비강성.(관절) 회전, 변환 또는 자유형.또한 회전 축에 주석을 달았습니다. (동작 당기기, 밀기 또는 기타. (가능성) 개체와 상호 작용해야 하는 위치를 나타내는 키포인트. 동시에, 우리의 taskonomy [74] 이미지에는 깊이 및 표면 법선을 포함한 3D 기준 진실이 제공됩니다. 또한 고정물의 31K 쿼리 포인트에 주석을 달았습니다. 마지막으로 10K 이미지를 각각 8k/1k/1k 분할의 학습/평가/테스트 세트로 분할했습니다. 이미지 인코더 변환기 디코더 깊이 쿼리 이동 가능: 1개 손 강성: 없음 관절: 자유 동작: 자유 이동 가능: 1개 손 강성: 있음 관절: 회전 동작: 당기기 현지화, 가능성 현지화, 가능성 깊이 각 쿼리 포인트 및 가능성에 대해. 그림 3에서. 우리 접근 방식 개요. 우리 네트워크의 입력은 단일 이미지와 일련의 쿼리 포인트입니다. ●●. 이동 가능, 위치 1, 강성, 관절 측면에서 잠재적인 3D 상호 작용을 예측합니다. L, 액션 추가, 변압기 디코더의 입력에는 학습 가능한 깊이 쿼리가 포함되어 있으며, 이는 관절형 객체에 대한 3D 객체 상호작용을 복구하기 위해 밀집 깊이를 추정합니다.가용성 및 윤리.우리의 이미지는 공개적으로 사용 가능한 세 가지 데이터 세트에서 가져왔습니다.Taskonomy에는 인간이 포함되어 있지 않습니다.비디오 관절 데이터 세트는 Creative Commons 인터넷 비디오에서 가져왔습니다.우리는 데이터 세트에서 어떠한 윤리적 문제도 예상하지 않습니다.5. 접근 방식 이제 이미지와 일련의 쿼리 포인트를 가져와서 이동 가능, 국소화, 강성, 관절, 액션 및 가능성을 포함하여 섹션 3에서 물었던 모든 질문에 답할 수 있는 모델을 소개합니다.우리의 접근 방식에 대한 간략한 개요는 그림 3에 나와 있습니다.우리의 입력에는 일련의 쿼리 포인트가 포함되고 출력에는 경계 상자와 분할 마스크가 모두 포함되므로, 우리는 주로 SAM[33]을 확장하여 모델을 구축합니다.Mask R-CNN[26]과 같은 기존의 탐지 파이프라인과 비교할 때, 우리는 쿼리 포인트를 사용하여 SAM이 해당 객체를 탐지하도록 자연스럽게 안내할 수 있습니다. 마스크 R-CNN은 각 이미지에 대해 수천 개의 앵커를 생성하는데, 이는 올바른 매칭을 찾기 어렵습니다. 그러나 우리는 또한 완전성을 위해 실험에서 대체 네트워크 아키텍처와 비교합니다. 우리는 그것들이 SAM보다 나쁘더라도 작동할 수 있다는 것을 발견했습니다. 단순화를 위해 단일 쿼리 포인트만 있다고 가정합니다. 그러나 우리 모델은 한 번에 수백 개의 쿼리 포인트를 허용할 수 있습니다. 5.1. 백본 백본의 목표는 이미지 I와 쿼리 포인트 [x, y]를 풀링된 피처 h = f(I; [x, y])에 매핑하는 것입니다. 전체 세부 정보는 보충 자료에 있습니다. 이미지 인코더. 우리의 이미지 인코더는 SAM [33]을 따르는 MAE [25] 사전 학습된 Vision Transformer(ViT) [13]입니다. 그들은 단일 이미지 I를 변환기 디코더의 메모리에 매핑합니다. 쿼리 포인트 인코더. 우리는 쿼리 포인트 [x, y]를 위치 인코딩 [63]으로 전송한 다음 변환기 디코더에 공급합니다. 임베딩 k를 사용하여 변환기가 다른 쿼리 포인트에 대한 피처 h를 생성하도록 안내합니다.변환기 디코더.디코더는 인코더에서 메모리의 입력과 쿼리 포인트의 임베딩 k를 받습니다.디코더는 각 쿼리 포인트에 대한 임베딩 h를 생성하고 ROI 피처와 같이 모든 속성을 예측하는 데 사용합니다.5.2. 예측 헤드 이제 풀링된 피처 h에서 피처로 매핑하는 방법을 설명합니다.각 예측은 각 출력 유형을 처리하는 별도의 헤드에서 수행합니다.이동 가능 선형 레이어를 추가하고 숨겨진 임베딩 h를 이동 가능의 예측에 매핑합니다.표준 교차 엔트로피 손실을 사용하여 학습합니다.지역화 분할 마스크를 예측하기 위해 SAM 표준 관행을 따릅니다.마스크 디코더를 사용하여 분할 마스크를 예측하고 초점 손실[40] 및 DICE[47]를 손실 함수로 사용하여 학습합니다.깊이의 경우 해당 학습 가능한 깊이 쿼리가 있는 별도의 깊이 변환기 디코더가 있습니다. 우리는 [73, 54, 39]에 따라 스케일 및 시프트 불변 L1 손실과 그래디언트 매칭 손실을 사용하여 깊이를 훈련합니다. 시프트와 스케일은 이미지별로 정규화됩니다. 강성 이동과 유사하게 객체가 강체인지 아닌지 예측하기 위해 선형 레이어를 추가합니다. 우리는 표준 이진 교차 엔트로피 손실을 사용하여 선형 레이어를 훈련합니다. 관절 먼저 대화형 객체가 회전, 이동 또는 자유형인지 예측하기 위해 선형 레이어를 추가하고 표준 교차 엔트로피 손실을 사용하여 이를 훈련합니다. 회전 축의 경우 [53, 75]에 따라 축을 2차원 선 (0, r)로 나타냅니다. 이 선 위의 모든 점은 x cos(0) + y sin(0) = r을 만족하는데, 여기서 0은 각도를 나타내고 r은 객체 중심에서 선까지의 거리를 나타냅니다. 훈련에서 우리는 2차원 선을 (sin 20, cos 20, r)로 나타내므로 축 각도는 연속 공간에 있습니다 [76]. 우리는 경계 상자와 유사하게 축을 예측하기 위해 3계층 MLP를 사용하는데, 두 작업 모두 지역화가 필요하기 때문입니다.우리는 L1 손실을 사용하여 이를 훈련합니다.행동 이동과 유사하게, 우리는 객체와 상호 작용하기 위한 잠재적 행동이 무엇인지 예측하기 위해 선형 계층을 추가합니다.우리는 표준 이진 교차 엔트로피 손실을 사용하여 선형 계층을 훈련합니다.가능성 우리의 가능성 예측은 확률 맵인 반면, 우리의 주석은 단일 키포인트입니다.그러나 가능성에는 여러 솔루션이 있을 수 있습니다.따라서 우리는 가능성의 주석을 2D 가우시안 범프[35]로 변환하고 이진 초점 손실[40]을 사용하여 네트워크를 훈련합니다.우리는 긍정적 예의 가중치를 0으로, 부정적 예의 가중치를 0.05로 설정하여 긍정적 예와 부정적 예의 균형을 맞춥니다.긍정적 예보다 부정적인 예가 더 많기 때문입니다.우리의 총 손실은 위에서 언급한 모든 손실의 가중 선형 조합입니다.자세한 내용은 보충 자료에 있습니다.5.3. 구현 세부 정보 우리의 접근 방식에 대한 전체 아키텍처 세부 정보는 보충 자료에 있습니다. 실제로 마스크, 깊이 및 어포던스에 대해 세 가지 다른 변압기 디코더를 사용합니다.이미지 인코더, 쿼리 포인트 인코더 및 마스크 디코더는 SAM[33]에서 사전 학습됩니다.어포던스 헤드 및 깊이 헤드를 포함한 다른 부분은 처음부터 학습됩니다.학습 속도 10-4의 AdamW 옵티마이저를 사용하고 200에포크 동안 모델을 학습합니다.6. 실험 이미지에서 이동 부품의 속성을 국소화하고 예측할 수 있는 접근 방식을 도입했습니다.실험에서 다음 질문에 답하는 것을 목표로 합니다.(1) 이미지에서 이동 부품의 속성을 얼마나 잘 국소화하고 예측할 수 있습니까?(2) 문제에 대한 대체 접근 방식이 얼마나 잘 수행됩니까?3DOI 데이터 세트에서 접근 방식을 평가하고 로봇 데이터 WHIRL[1]에 대한 일반화를 테스트합니다.6.1. 실험 설정 먼저 실험 설정을 설명합니다.우리의 방법은 단일 RGB 이미지를 살펴보고 키포인트가 주어지면 이동 부품에 대한 정보를 추론하는 것을 목표로 합니다.따라서 다양한 측면을 포착하는 메트릭을 사용하여 두 가지 어려운 데이터 세트에서 접근 방식을 평가합니다. 데이터 세트. 3DOI 데이터 세트(섹션 4에서 설명)와 WHIRL 데이터 세트[1]의 두 데이터 세트에서 접근 방식을 훈련하고 검증합니다. WHIRL[1]은 서랍, 식기 세척기, 다양한 주방의 냉장고, 다양한 캐비닛의 문과 같은 일상적인 물건과 설정을 포함하는 로봇 데이터 세트입니다. WHIRL을 사용하여 로봇 설정에서 접근 방식과 다운스트림 애플리케이션의 일반화를 검증합니다. 모든 WHIRL 비디오의 첫 번째 프레임을 분할하고 데이터 세트와 동일한 파이프라인을 사용하여 주석을 달았습니다. 일반적으로 인간은 첫 번째 프레임에 존재하지 않으며 조작 전입니다. 메트릭. 모든 예측에 대한 표준 평가 관행을 보고합니다. 모든 메트릭에서 높을수록 좋습니다. 이러한 메트릭은 다음과 같이 자세히 설명합니다. • 이동 가능, 강성 및 동작: 이것들이 객관식 문제이므로 정확도를 보고합니다. • 지역화: 경계 상자 및 마스크에 대한 예측에 대해 교차-연합(IoU)을 보고합니다[41]. 우리는 깊이에 대한 임계값 정확도를 보고합니다 [15]. . 관절: 우리는 관절 유형에 대한 정확도를 보고합니다. 회전 축은 2D 선입니다. 따라서 우리는 [53, 75]에 따라 예측과 기준 진실 사이의 EAScore를 보고합니다. EA-Score [75]는 두 선 사이의 각도와 유클리드 거리를 측정하기 위한 [0, 1]의 점수입니다. . 가능성: 확률 맵이며 우리는 [50, 3, 36, 45]에 따라 히스토그램 교차점(또는 SIM)을 보고합니다. 기준선. 우리는 우리의 접근 방식을 일련의 기준선과 비교하여 대안적인 접근 방식이 우리 문제에 얼마나 잘 작동하는지 평가합니다. 우리는 먼저 사전 학습된 체크포인트를 사용하여 3DADN [53], SAPIEN [70] 및 InteractionHotspots [50]을 평가하여 비디오 또는 합성 데이터의 학습이 우리 문제에 얼마나 잘 작동하는지 테스트합니다. 그런 다음 두 개의 쿼리 포인트 기반 모델인 ResNet MLP [27]와 COHESIV [59]를 학습하여 대체 네트워크 아키텍처가 우리 문제에 얼마나 잘 작동하는지 테스트합니다. 세부 사항은 다음과 같습니다. • (3DADN [53]): 3DADN은 Mask R-CNN [26]을 확장하여 인간이 상호 작용하는 관절형 물체를 감지합니다. 인터넷 비디오에서 학습합니다. 단일 이미지에서 작업하므로 시간 최적화 부분을 삭제합니다. 각 이미지에서 관절형 물체와 유형(이동 또는 회전), 경계 상자, 마스크 및 축을 감지할 수 있습니다. 3DADN의 입력에는 쿼리 포인트가 포함되지 않으므로 예측된 경계 상자와 기준 진실을 비교하여 일치하는 감지를 찾고 다른 메트릭을 평가합니다. 감지 임계값을 0.05로 낮춰 기준 진실과 일치하는 감지가 충분한지 확인합니다. • (SAPIEN [70]): 3DADN [53]의 학습 프레임에는 일반적으로 인간 활동이 있습니다. 그러나 우리의 데이터 세트는 인간이 존재할 것을 요구하지 않으므로 일반화 문제가 발생할 수 있습니다.또는 우리는 합성 데이터에서 기술을 바로 배울 수 있는지에 관심이 있습니다.우리는 SAPIEN에서 생성한 합성 객체의 렌더링에 3DADN[53]을 훈련합니다.SAPIEN은 대규모 관절 객체 세트를 포함하는 시뮬레이터입니다.우리는 3DADN에서 제공하는 렌더링과 동일한 평가 전략을 사용합니다.• (InteractionHotspots[50]): 3DADN과 SAPIEN은 관절 객체와 축을 모두 감지할 수 있지만, 어포던스는 알 수 없습니다.InteractionHotspots는 OPRA[17] 또는 Epic-Kitchen[11] 비디오를 시청하여 어포던스를 학습합니다.InteractionHotspots는 객체를 감지할 수 없으므로 쿼리 지점을 기반으로 입력 이미지의 중앙 크롭을 적용하고 표준 입력 모양(224, 224)으로 크기를 조정합니다.OPRA보다 더 잘 전송되므로 Epic-Kitchen에서 훈련된 모델을 사용합니다. 또한, 우리는 3DOI 데이터 세트에서 학습된 대체 네트워크 아키텍처를 테스트하고 싶습니다. 우리는 공정한 비교를 보장하기 위해 3DOI에서 학습하기 위해 우리와 동일한 손실을 사용합니다. • (ResNet MLP [27]): ResNet MLP는 ResNet-50 인코더를 사용하여 입력 이미지에서 기능을 추출합니다. 그런 다음 샘플링 속성 이미지 + 쿼리 예측 강성: 예 현지화 가능성 GT 예측 GT 예측 GT 강성: 예 Mov: 1 손 Mov: 1 손 Arti: 회전 동작: 당기기 Arti: 회전 동작: 당기기 강성: 예 Mov: 1 손 Arti: 거래 동작: 당기기 강성: 예 Mov: 1 손 Arti: 거래 동작: 당기기 강성: 예 Mov: 1 손 Arti: 자유 동작: 무료 강성: 예 Mov: 1 손 Arti: 자유 동작: 무료 강성: 아니요 Mov: 1 손 Arti: 자유 동작: 무료 강성: 아니요 Mov: 1 손 Arti: 자유 동작: 무료 강성: 예 Mov: 2 손 Arti: 자유 동작: 무료 강성: 예 Mov: 2 손 Arti: 자유 동작: 무료 그림 4. 3DOI 데이터 세트의 결과. 쿼리 지점을 나타냅니다. (행 1, 2) 우리의 접근 방식은 관절형 물체와 그 유형(회전 또는 변환), 축, 그리고 가능성을 올바르게 인식할 수 있습니다. (행 3, 4) 우리의 접근 방식은 자기중심적 비디오에서 딱딱한 물체와 딱딱하지 않은 물체를 인식할 수 있습니다. (행 5) 우리의 접근 방식은 TV와 같이 두 손으로 움직여야 하는 물체를 인식할 수 있습니다. 이러한 물체의 가능성에는 여러 가지 해결책이 있다는 점에 유의하십시오. 더 나은 시각화를 위해 가능성은 수동으로 확대됩니다. 가능성 컬러맵: 키포인트의 2D 좌표를 사용하여 피처 맵에서 해당 공간적 특징을 최소화합니다. 마스크, 가능성 및 깊이를 제외한 모든 작업에서 ResNet MLP를 학습합니다. 이러한 작업에는 각 픽셀에 대한 고밀도 예측이 필요하기 때문입니다. ResNet에 별도의 디코더를 추가하면 UNet과 유사한 아키텍처[57]가 되며 이는 ResNet의 범위를 벗어납니다. • ⚫ (COHESIV[59]): 또한 쿼리 위치 예측 문제를 위해 설계된 또 다른 모델 COHESIV를 선택합니다. 입력 이미지와 해당 손 위치가 쿼리로 주어지면 COHESIV는 손과 손에 든 물체의 분할을 예측합니다. 쿼리의 피처 맵을 생성하므로 네트워크를 채택합니다. 쿼리 포인트에 따라 피처 맵에서 임베딩을 샘플링하고 이미지 피처와 연결하여 여러 출력을 생성합니다. 6.2. 결과 먼저 그림 4에 정성적 결과를 보여줍니다. 관절형 물체(서랍, 캐비닛 등)의 경우 접근 방식으로 위치, 운동 모델(회전 또는 이동), 축 및 손잡이를 인식할 수 있습니다. 또한 딱딱하거나 딱딱하지 않은 물체, 가볍거나 무거운 물체도 인식할 수 있습니다. 3인칭 이미지나 자기중심적 비디오 모두에서 작동합니다. 그리고 이 모든 것이 단일 모델에서 달성됩니다. 관절형 obmax의 경우. 이미지 + 쿼리 예측예측예측그림 5. 관절형 물체의 3D 잠재적 상호 작용 예측. 쿼리 포인트를 나타냅니다. 예측 1, 2, 3에서 객체를 회전 축을 따라 회전하거나 법선 방향을 따라 객체를 이동합니다.객체의 경우 출력을 활용하여 그림 5에서 잠재적인 3D 상호 작용을 추가로 보여줍니다.자세한 내용은 보충 자료에 나와 있습니다.그런 다음 접근 방식을 일련의 기준선과 비교합니다.정량적 결과는 표 1에 보고되어 있습니다.3DADN[53]은 관절로 움직이는 객체만 감지할 수 있기 때문에 접근 방식보다 훨씬 나쁩니다.인간과 상호 작용하지 않는 객체는 감지하지 못합니다.대신 apTable 1. 3DOI 데이터 세트의 정량적 결과.Cat.은 범주를 의미합니다.이동 가능, 강성, 관절 및 동작을 포함한 모든 범주 분류에 대한 정확도를 보고합니다.상자 및 마스크에 대한 평균 IoU, 관절 축에 대한 EA-Score 및 가용성에 대한 SIM을 보고합니다.모든 메트릭에서 높을수록 좋습니다.이동 가능 위치 강성 → 관절 방법 Cat.상자 마스크 Cat.Cat.축 동작 Cat. Affordance Probability 3DADN [53] 8.6.44.5.SAPIEN [70] 5.4.41.1.InteractionHotspots [50] 0.ResNet MLP [27] 72.21.81.51.68.58.COHESIV [59] 71.28.35.81.68.67.71.0.Ours 85.69.77.90.89.80.89.0.Image + Query 3DADN SAPIEN Ours Ground Truth Image Query InteractionHotspots Ours Ground Truth 그림 6. 3DADN [53], SAPIEN [70] 및 우리 접근 방식 비교. 쿼리 지점을 나타냅니다. 3DADN은 인간이 있을 때 강력한 성능을 보입니다. 그러나 인간 활동이 없으면 객체를 감지하는 데 어려움이 있습니다. SAPIEN은 실제 이미지에 잘 일반화되지 않습니다. 그러나 인간이 없는 경우 3DADN보다 나은 경우가 있습니다. 접근 방식은 인간 활동과 관계없이 상호 작용할 수 있는 모든 객체를 감지할 수 있습니다. SAPIEN은 3DADN보다 나쁘며, 이는 합성 객체에서 학습하는 데 큰 도메인 갭이 있음을 시사합니다. 이는 3DADN의 관찰과 일치합니다. 시각적 비교는 그림 6에 나와 있습니다. 우리는 우리의 어포던스 맵 예측을 InteractionHotspots[50]와 비교합니다. 우리의 접근 방식은 3.5배 개선되어 InteractionHotspots보다 상당히 우수한 성과를 보입니다. 시각적 비교는 그림 7에 나와 있습니다. InteractionHotspots는 클라우드와 같은 확률 맵을 예측하는 반면, 우리의 접근 방식은 일반적으로 예측에 대해 매우 확신합니다. 그러나 전반적인 성능은 비교적 낮습니다. 이는 주로 변형 가능한 객체에 대한 어포던스의 모호성 때문입니다. 대체 네트워크 아키텍처를 탐색하기 위해 동일한 손실 함수로 데이터에서 학습한 ResNet MLP[27] 및 COHESIV[59]와 우리의 접근 방식을 비교합니다. ResNet MLP는 이동 가능, 강성 및 동작에 대해 합리적입니다. 특히 경계 상자 국소화에 나쁩니다. 따라서 일반적으로 Mask R-CNN[26]과 같은 탐지 파이프라인에 의존합니다. COHESIV는 ResNet MLP보다 엄청난 개선인 합리적인 경계 상자와 마스크를 학습합니다. ResNet MLP와 비교한 이동 가능한 방울의 성능은 운동학 및 동작의 성능이 향상됩니다. 전반적으로 우리의 접근 방식은 주로 변압기를 도입했기 때문에 ResNet MLP와 COHESIV보다 성능이 뛰어납니다. 마지막으로 데이터의 깊이를 평가합니다. 최첨단 깊이 추정을 하는 것은 그림 7. InteractionHotspots[50]와 우리의 접근 방식의 비교. 쿼리 지점을 나타내기 때문에 우리의 목표와 직교합니다. InteractionHotspots는 일반적으로 데이터에 대한 확률 맵과 같은 구름을 만듭니다. 우리 모델은 여러 솔루션이 있을 수 있지만 예측에 대해 매우 확신합니다. 더 나은 시각화를 위해 예측 및 GT가 수동으로 확대됩니다. Affordance colormap: min| max. 3D에서 객체를 로컬라이제이션하고 잠재적인 3D 상호 작용을 렌더링하는 데 적당한 깊이만 필요합니다. 사실, 최신 깊이 추정 모델은 10개 이상의 데이터 세트와 100만 개의 이미지에서 학습된 반면[54, 73, 14], 저희 데이터 세트에는 깊이 기준 진실이 있는 5,000개 이미지만 있습니다. 저희는 모델이 적당한 깊이를 학습했다는 것을 보여주기 위해 깊이 추정의 평가만 보고합니다. 저희 데이터에서 96.7% 픽셀이 1.25 임계값 내에 있고 99.3% 픽셀이 1.25 임계값 내에 있습니다. 6.3. 일반화 결과 저희의 접근 방식과 3DOI 데이터 세트에서 학습된 모델이 일반화될 수 있는지 테스트하기 위해, 저희는 일상적인 객체를 조작하는 로봇 데이터 세트인 WHIRL[1]에서 저희의 접근 방식을 추가로 평가합니다. WHIRL은 소규모 데이터 세트이므로 미세 조정 없이 WHIRL에서 저희 모델을 테스트합니다. 결과는 그림 8에 나와 있습니다. 관절이 있는 물체와 변형 가능한 물체 모두에 대해, 우리의 접근 방식은 운동학 모델, 위치 및 가능성을 성공적으로 복구할 수 있습니다. 또한 WHIRL에 대한 접근 방식을 정량적으로 평가합니다. 결과를 표 2에 보고합니다. 3DOI 데이터 세트와 유사하게, 우리의 접근 방식은 3DADN[53], SAPIEN[70] 및 InteractionHotspots[50]보다 상당히 우수한 성과를 보입니다. 표 2. 로봇 데이터에 대한 정량적 결과[1]. 범주는 범주를 의미합니다. 이동 가능, 강성, 관절 및 동작을 포함한 모든 범주 분류에 대한 정확도를 보고합니다. 상자와 마스크에 대한 평균 IoU, 관절 축에 대한 EA-Score, 가능성 확률 맵에 대한 SIM을 보고합니다. 모든 지표에서 높을수록 좋습니다. 방법 이동 가능 범주. 위치 상자 마스크 강성 범주. 관절 범주. 축 동작 범주. 가능성 확률 3DADN [53] 13.10.53.4.SAPIEN [70] 9.6.51.0.InteractionHotspots [50] 0.ResNet MLP [27] 88.14.80.51.57.51.COHESIV [59] 86.37.38.82.73.66.73.0.Ours 91.68.70.95.80.68.84.0.Image + 쿼리 속성 현지화 강성: 예 Mov: 1 핸드 Arti: 트랜잭션 작업: 당기기 강성: 예 Mov: 1 핸드 Arti: 회전 작업: 당기기 강성: 아니요 Mov: 1 핸드 Arti: 자유 작업: 자유 가능성 속성 현지화 이미지 + 쿼리 예측 GT 예측 GT 강성: 예 Mov: 1 핸드 Arti: 회전 Action: Pull Rigid: Yes Mov: 1 hand Arti: Rot Action: Pull Rigid: Yes Mov: 1 hand Arti: Free Action: Free Rigid: Yes Mov: 1 hand Arti: Free Action: Free Rigid: Yes Mov: 1 hand Arti: Free Action: Free Rigid: Yes Mov: 1 hand Arti: Free Action: Free Rigid: Yes Mov: 1 hand Arti: Free Action: Free Rigid: Yes Mov: 2 hands Arti: Free Action: Free Rigid: No Mov: 1 hand Arti: Free Action: Free 그림 8. 로봇 데이터에 대한 결과 [1]. 쿼리 지점을 나타냅니다. 미세 조정 없이도 접근 방식은 로봇 데이터에 잘 일반화되어 지능형 에이전트가 객체를 더 잘 조작하는 데 도움이 될 수 있는 잠재력을 나타냅니다. 행 1과 2는 관절이 있는 객체입니다. 행 3과 행 4는 변형 가능한 객체입니다. 더 나은 시각화를 위해 어포던스를 수동으로 확대합니다. 어포던스 컬러맵: 최소 최대. 성능 격차는 훨씬 더 큽니다. 이는 데이터 세트의 대부분 이미지에 인간이 없기 때문이라고 생각합니다. 우리는 우리의 접근 방식을 3DOI 데이터 세트에서 훈련된 ResNet MLP[27] 및 COHESIV[58]와 비교합니다. 우리 모델은 ResNet MLP와 COHESIV를 모두 일관되게 능가합니다. 마스크 디코더의 설계로 인해 밀도 예측(지역화 및 가용성)에 대한 개선이 상당합니다. 다른 속성에 대한 개선은 비교적 적습니다. 이는 네트워크 아키텍처에 관계없이 3DOI 데이터 세트에서 훈련된 모델이 로봇 데이터에 잘 일반화됨을 보여줍니다. 6.4. 한계 및 실패 모드 마지막으로 우리의 한계와 실패 모드에 대해 설명합니다. 그림 9에서 시각적 단서에서 일부 예측을 하기 어렵다는 것을 보여줍니다. 일부 관절 객체는 대칭적이고 인간은 상식에 의존하여 회전 축을 추측합니다. 강성과 이동성을 예측할 때도 어려운 예가 있습니다. 마지막으로 단일 키포인트에만 주석을 달았습니다. 그림 9. 우리 접근 방식의 일반적인 실패 모드. 쿼리 포인트를 나타냅니다. 행 1: 객체가 대칭으로 보일 때 예측된 회전 축이 잘못된 쪽에 있습니다. 행 2: 가위가 가려졌을 때 예측된 마스크는 부분적입니다. 행 3: 모델은 쓰레기통을 한 손으로 집어 올릴 수 있다고 생각합니다. 재료가 플라스틱처럼 보이기 때문입니다. 각 객체 인스턴스에 대해 어포던스로. 하지만 일부 객체는 어포던스로 여러 개의 키포인트를 가질 수 있습니다. 7.
--- CONCLUSION ---
우리는 단일 RGB 이미지에서 3D 객체 상호작용을 예측하는 새로운 과제를 제시했습니다. 과제를 해결하기 위해 3D 객체 상호작용 데이터 세트를 수집하고 쿼리 포인트에 따라 모든 객체의 잠재적 상호작용을 예측하는 변환기 기반 모델을 제안했습니다. 실험 결과, 우리의 접근 방식은 기존 접근 방식보다 데이터에 대한 성능이 뛰어나고 로봇 데이터에 잘 일반화됩니다. 우리의 접근 방식은 3D 장면을 이해하고 일상적인 객체를 조작할 수 있는 스마트 로봇을 만드는 데 도움이 되어 긍정적인 영향을 미칠 수 있습니다. 반면에 우리의 접근 방식은 감시 활동에 유용할 수 있습니다. 감사의 말 이 연구는 DARPA Machine Common Sense Program의 지원을 받았습니다. 이 자료는 Grant No. 2142529에 따라 National Science Foundation에서 지원한 연구를 기반으로 합니다. WHIRL 데이터를 도와준 Shikhar Bahl과 Deepak Pathak, 그림을 도와준 Georgia Gkioxari, 유익한 토론을 해준 Tiange Luo, Ang Cao, Cheng Chi, Yixuan Wang, Mohamed El Banani, Linyi Jin, Nilesh Kulkarni, Chris Rockwell, Dandan Shan, Siyi Chen에게 감사드립니다. 참고문헌 [1] Shikhar Bahl, Abhinav Gupta, Deepak Pathak. 야생에서의 인간-로봇 모방. 2022. 2, 5, 7,[2] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, Georgia Gkioxari. Omni3d: 야생에서의 3D 객체 감지를 위한 대규모 벤치마크 및 모델. CVPR에서, 2023.[3] Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand. What do different evaluation metrics tell us about saliency models? IEEE transaction on pattern analysis and machine intelligence, 41(3):740-757, 2018.[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. Transformers를 사용한 종단간 객체 감지. ECCV에서, 2020. 2, 3,[5] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, Jia Deng. Hico: 이미지에서 인간-객체 상호 작용을 인식하기 위한 벤치마크. ICCV에서, 2015.[6] Weifeng Chen, Shengyi Qian, Jia Deng. 품질 평가 네트워크를 사용하여 비디오에서 단일 이미지 깊이 학습. CVPR에서, 2019.[7] Bowen Cheng, Alex Schwing, Alexander Kirillov. 픽셀당 분류만으로는 의미 분할에 필요한 전부가 아닙니다. NeurIPS, 2021.[8] Cheng Chi, Dmitry Berenson. 물리 시뮬레이션 없이 폐색에 강건한 변형 가능 객체 추적. IROS, 2019.[9] Christopher B Choy, Danfei Xu, Jun Young Gwak, Kevin Chen, Silvio Savarese. 3D-R2N2: 단일 및 다중 뷰 3D 객체 재구성을 위한 통합 접근 방식. ECCV, 2016.[10] Cristina Garcia Cifuentes, Jan Issac, Manuel Wüthrich, Stefan Schaal, Jeannette Bohg. 로봇 조작을 위한 확률적 관절 실시간 추적. IEEE Robotics and Automation Letters, 2(2):577-584, 2016.[11] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray. 자기중심적 시각 재조정: epic-kitchens-100에 대한 수집, 파이프라인 및 과제. International Journal of Computer Vision(IJCV), 130:33–55, 2022. 3, 5,[12] Karthik Desingh, Shiyang Lu, Anthony Opipari, Odest Chadwicke Jenkins. 효율적인 비모수적 신념 전파를 사용하여 관절 객체의 요인화된 포즈 추정. ICRA, 2019.[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly 등. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변압기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020.[14] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, Amir Zamir. Omnidata: 3D 스캔에서 다중 작업 중간 수준 비전 데이터 세트를 만드는 확장 가능한 파이프라인. ICCV에서, 2021. 3, 7,[15] David Eigen 및 Rob Fergus. 공통적인 다중 규모 합성곱 아키텍처를 사용하여 깊이, 표면 법선 및 의미 레이블 예측. ICCV, 2015. 2,[16] Rui Fan, Hengli Wang, Bohuan Xue, Huaiyang Huang, Yuan Wang, Ming Liu, Ioannis Pitas. Three-filters-to-normal: 정확하고 초고속 표면 정상 추정기.IEEE Robotics and Automation Letters, 6(3):5405-5412, 2021.[17] Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, Joseph J Lim. Demo2vec: 온라인 비디오에서 추론 객체 가능성.CVPR, 2018.[18] Martin A Fischler 및 Robert C Bolles. 무작위 표본 합의: 이미지 분석 및 자동 지도 제작에 응용 프로그램을 갖춘 모델 피팅을 위한 패러다임.Communications of the ACM, 1981.[19] Georgia Gkioxari, Ross Girshick, Piotr Dollar, Kaiming He. 인간-사물 상호작용 감지 및 인식. CVPR, 2018.[20] Georgia Gkioxari, Jitendra Malik, Justin Johnson. Mesh r-cnn. ICCV, 2019. 1,[21] Mohit Goyal, Sahil Modi, Rishabh Goyal, Saurabh Gupta. 상호작용 객체 이해를 위한 프로브로서의 인간 손. CVPR, 2022.[22] Agrim Gupta, Piotr Dollar, Ross Girshick. Lvis: 대규모 어휘 인스턴스 분할을 위한 데이터 세트. CVPR, 2019.[23] Arjun Gupta, Max E Shepherd, Saurabh Gupta. 일상 사물을 관절로 표현하기 위한 동작 계획 예측. ICRA, 2023.[24] Sanjay Haresh, Xiaohao Sun, Hanxiao Jiang, Angel X Chang, Manolis Savva. RGB 비디오에서 나온 관절형 3D 인간-객체 상호작용: 접근 방식과 과제에 대한 실증적 분석.arXiv 사전 인쇄본 arXiv:2209.05612, 2022.[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다.CVPR에서, 2022.[26] Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick. 마스크 r-cnn.ICCV에서, 2017.2, 3, 4, 5,[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.이미지 인식을 위한 심층 잔여 학습.CVPR에서, 2016.5, 7,[28] Cheng-Chun Hsu, Zhenyu Jiang, Yuke Zhu. Ditto in the house: Building articulation models of indoor scene through interactive perception. ICRA, 2023.[29] Hanxiao Jiang, Yongsen Mao, Manolis Savva, and Angel X Chang. Opd: Single-view 3d openable part detection. ECCV, 2022.[30] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: Building digital twins of articulated object from interaction. CVPR, 2022.[31] Linyi Jin, Shengyi Qian, Andrew Owens, and David F. Fouhey. Planar surface reconstruction from sparse views. ICCV, 2021.[32] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. CVPR, 2019.[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 무엇이든 분할합니다. ICCV, 2023. 2, 4, 5,[34] Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick. Pointrend: 렌더링으로서의 이미지 분할. CVPR, 2020.[35] Hei Law, Jia Deng. Cornernet: 쌍을 이룬 키포인트로 객체 감지. ECCV, 2018. 5,[36] Gen Li, Varun Jampani, Deqing Sun, Laura Sevilla-Lara. Locate: 약한 감독 기반 어포던스 접지를 위해 객체 부분을 로컬라이즈하고 전송합니다. CVPR, 2023. 2,[37] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, 및 Shuran Song. 범주 수준 관절형 객체 포즈 추정. CVPR, 2020. 1,[38] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, 및 Antonio Torralba. 강체, 변형 가능한 객체 및 유체를 조작하기 위한 입자 역학 학습. arXiv 사전 인쇄본 arXiv:1810.01566, 2018.[39] Zhengqi Li 및 Noah Snavely. Megadepth: 인터넷 사진에서 단일 시점 깊이 예측 학습. CVPR, 2018. 2,[40] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, 및 Piotr Dollar. 밀집 객체 감지를 위한 초점 손실. ICCV, 2017. 4, 5,[41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick. Microsoft COCO: 컨텍스트 내 일반 객체. ECCV, 2014.[42] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, Jan Kautz. PlaneRCNN: 단일 이미지에서 3D 평면 감지 및 재구성. CVPR, 2019. 2,[43] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa. Planenet: 단일 rgb 이미지에서 조각별 평면 재구성. CVPR, 2018. 2,[44] Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. arXiv 사전 인쇄본 arXiv:1711.05101, 2017.[45] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao 및 Dacheng Tao. 외심 이미지에서 학습 가능한 접지. CVPR에서, 2022. 2,[46] Tiange Luo, Honglak Lee 및 Justin Johnson. 신경 모양 컴파일러: 텍스트, 포인트 클라우드 및 프로그램 간 변환을 위한 통합 프레임워크. 2022.[47] Fausto Milletari, Nassir Navab 및 Seyed-Ahmad Ahmadi. V-net: 체적 의료 이미지 분할을 위한 완전 합성 신경망. 3DV에서, 2016. 4,[48] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta 및 Shubham Tulsiani. Where2act: 픽셀에서 관절형 3D 객체의 동작까지. ICCV, 2021.[49] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang. A-sdf: 관절형 모양 표현을 위한 얽힘이 없는 부호 거리 함수 학습. ICCV, 2021.[50] Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman. 비디오에서 접지된 인간-객체 상호 작용 핫스팟. ICCV, 2019. 2, 5, 7,[51] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, Jian Jun Zhang. Total3dunderstanding: 단일 이미지에서 실내 장면에 대한 조인트 레이아웃, 객체 포즈 및 메시 재구성. CVPR, 2020. 1,[52] Sudeep Pillai, Matthew R Walter 및 Seth Teller. 시각적 시연에서 관절 동작 학습. RSS, 2014. 1,[53] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen 및 David F. Fouhey. 인터넷 비디오에서 3D 객체 관절 이해. CVPR, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 12,[54] René Ranftl, Alexey Bochkovskiy 및 Vladlen Koltun. 밀도 예측을 위한 비전 변환기. ICCV, 2021. 1, 2, 3, 4,[55] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson 및 Georgia Gkioxari. pytorch3d를 사용한 3D 딥 러닝 가속화. arXiv:2007.08501, 2020.[56] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 더 빠른 r-cnn: 영역 제안 네트워크를 사용한 실시간 객체 감지를 향해. 신경 정보 처리 시스템의 발전, 91-99페이지, 2015.[57] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-net: 생물의학 이미지 분할을 위한 합성곱 네트워크. MICCAI, 2015.[58] Dandan Shan, Jiaqi Geng, Michelle Shu, David Fouhey. 인터넷 규모에서 접촉하는 인간 손 이해. CVPR, 2020. 2, 3,[59] Dandan Shan, Richard EL Higgins, David F. Fouhey. COHESIV: 비디오에서 대조적 객체 및 손 임베딩 분할. NeurIPS, 2021. 5, 6, 7,[60] Nathan Silberman, Derek Hoiem, Pushmeet Kohli 및 Rob Fergus. 실내 분할 및 RGBD 이미지에서 추론 지원. European Conference on Computer Vision, 746-760페이지. Springer, 2012.[61] Jürgen Sturm, Cyrill Stachniss 및 Wolfram Burgard. 관절 객체의 운동학 모델 학습을 위한 확률적 프레임워크. Journal of Artificial Intelligence Research, 41:477-526, 2011. 1, 2,[62] Xiaohao Sun, Hanxiao Jiang, Manolis Savva 및 Angel Xuan Chang. Opdmulti: 여러 객체에 대한 열 수 있는 부분 감지. arXiv 사전 인쇄본 arXiv:2303.14087, 2023.[63] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, Ren Ng. Fourier 피처를 통해 네트워크가 저차원 도메인에서 고주파 함수를 학습할 수 있습니다. NeurIPS, 2020.[64] Xiaolong Wang, David F. Fouhey, Abhinav Gupta. 표면 정규 추정을 위한 딥 네트워크 설계. CVPR, 2015.[65] Yixuan Wang, Dale McConachie, Dmitry Berenson. 기하학적 제약을 적용하면서 부분적으로 가려진 변형 가능 객체 추적. ICRA, 2021.[66] Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, Hao Dong. Adaafford: few-shot interactions를 통해 3D 관절 객체에 대한 조작 가능도를 조정하는 방법 학습. ECCV에서, 2022.[67] Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun. Anchor detr: 트랜스포머 기반 객체 감지를 위한 쿼리 디자인. arXiv 사전 인쇄본 arXiv:2109.07107, 3(6), 2021.[68] Fangyin Wei, Rohan Chabra, Lingni Ma, Christoph Lassner, Michael Zollhöfer, Szymon Rusinkiewicz, Chris Sweeney, Richard Newcombe, Mira Slavcheva. 자기 감독 신경 관절 모양 및 외관 모델. CVPR에서, 2022.[69] Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas Guibas 및 Hao Dong. Vat-mart: 3D 관절 개체를 조작하기 위한 시각적 동작 궤적 제안을 학습합니다. arXiv 사전 인쇄 arXiv:2106.14440, 2021.[70] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang 등 Sapien: 시뮬레이션된 부품 기반 대화형 환경입니다. CVPR, 2020. 1, 2, 5, 7,[71] Zhenjia Xu, Cheng Chi, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng 및 Shuran Song. Dextairity: 변형 가능한 조작은 아주 간단할 수 있습니다. arXiv 사전 인쇄본 arXiv:2203.01197, 2022.[72] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, Andrew Owens. 터치 앤 고: 인간이 수집한 시각과 촉각에서 배우기. NeurIPS에서, 2022. 2,[73] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, Chunhua Shen. 단일 이미지에서 3D 장면 모양을 복구하는 방법 학습. CVPR에서, 2021. 1, 2, 3, 4,[74] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, Silvio Savarese. Taskonomy: 작업 전이 학습의 얽힘 해소. CVPR에서, 2018.[75] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, MingMing Cheng. 의미적 줄 감지를 위한 딥 허프 변환. IEEE 패턴 분석 및 머신 인텔리전스 저널, 2021. 4,[76] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li. 신경망의 회전 표현의 연속성에 관하여. CVPR, 2019.A. 구현 변압기 디코더. 변압기 디코더 D는 인코더에서 메모리 m과 N개의 포인트 쿼리 kp와 하나의 깊이 쿼리 kɖ를 포함한 쿼리 세트를 가져옵니다. 포인트 풀링된 피처 h₁, ..., hN과 깊이 풀링된 피처 hd 세트를 예측합니다. 즉, .k(N), ka) (1) h1, h2,..., hy, ha = D(m; k, 1), k2),...k 모든 이미지가 15개 미만의 쿼리 포인트를 갖기 때문에 N = 15로 설정합니다. 15개 쿼리 포인트가 없는 이미지의 경우 입력을 15로 패딩하고 이러한 패딩 예제에 대해 학습하지 않습니다.깊이 쿼리 kd는 DETR [4]의 객체 쿼리와 유사한 학습 가능한 임베딩입니다.모든 쿼리는 서로 독립적이므로 병렬로 디코더에 공급됩니다.예측 헤드.DETR [4]은 선형 계층을 사용하여 객체 클래스를 예측하고 3계층 MLP를 사용하여 h를 기반으로 경계 상자를 회귀시킵니다.DETR에서 영감을 받아 이동, 강성, 관절 클래스 및 동작을 예측하는 데 선형 계층을 사용합니다.국소화가 필요하므로 3계층 MLP를 사용하여 경계 상자와 회전 축을 예측합니다.반경이 5인 경우 가능한 기준 진실에 대해 가우시안 범프 [35]를 추가합니다.손실 함수의 균형.각 예측에 여러 손실 함수를 사용하고 각 손실의 범위가 다르므로 균형을 맞춰야 합니다.손실의 가중치를 하이퍼파라미터로 처리하고 이에 따라 조정합니다. 이동 가능, 강성, 관절 클래스 및 동작 손실의 가중치는 0.5입니다. 마스크 손실(초점 손실[40] 및 DICE[47] 모두)의 가중치는 2.0입니다. 상자 L1 손실의 가중치는 5.0이고 일반화된 IoU 손실은 2.0입니다. 축 각도 손실의 가중치는 1.0이고 축 오프셋 손실은 10.0입니다. 어포던스 손실의 가중치는 100.0입니다. 깊이 손실의 가중치는 1.0입니다. 분할 마스크와 어포던스 맵의 초점 손실 모두에 대해 Y = 2를 사용합니다. 분할 마스크의 초점 손실에 대해 긍정적 및 부정적 예를 균형 잡기 위해 a = 0.25를 사용합니다. 어포던스에서는 부정이 긍정적보다 훨씬 많기 때문에 표준 a = 0.95를 사용합니다. 학습 세부 정보. 이미지 인코더, 프롬프트 인코더 및 마스크 디코더는 Segment-Anything[33]에서 사전 학습되었습니다. GPU 메모리를 절약하기 위해 가장 가벼운 사전 학습된 모델인 SAM-VIT-b를 이미지 인코더로 사용했습니다. 다른 헤드(예: 어포던스)는 처음부터 학습했습니다. 학습 속도 10-4의 Adam W 옵티마이저[44]를 사용하고 200에포크 동안 모델을 학습했습니다. 입력 및 출력 해상도는 768×1024입니다. 배치 크기는 2입니다. 분산 데이터가 병렬인 4개의 NVIDIA A40 GPU에서 모델을 학습했습니다. 3D 상호 작용 렌더링. 이러한 모든 예측을 감안하면 단일 이미지에서 관절 객체의 잠재적인 3D 객체 상호 작용을 예측할 수 있습니다. 회전 축이 있는 관절 객체의 경우 예측된 깊이[53]를 기반으로 예측된 2D 축을 먼저 3D로 역투영합니다. 그런 다음 3D 축을 따라 객체 포인트 클라우드를 회전하고 정규화된 높이 쿼리 포인트 BBox 중심 어포던스 관절형 강성, 비관절형 변형 가능 객체 유형 정규화된 너비 75.1% 회전 이동 관절 유형 14.5%38.1% 두 손 이동 가능 유형 고정물 그림 10. 3DOI 데이터 세트의 통계.(행 1) LVIS[22] 및 Omni3D[2]와 유사하게 정규화된 이미지 좌표에서 쿼리 포인트, 상자 중심 및 어포던스의 분포를 보여줍니다.(행 2) 객체 유형, 관절 유형 및 이동 가능 유형의 분포를 보여줍니다. 다시 2D로 변환합니다. RANSAC[18]을 사용하여 회전된 객체 포인트와 원래 객체 포인트 간의 호모그래피를 맞춥니다. 마지막으로 원래 객체 마스크에서 호모그래피를 워프합니다. 이동 축이 있는 관절형 객체의 경우에도 유사한 절차가 있습니다. 대신, 우리는 객체의 평균 표면 법선을 추정하고 그것을 변환 축의 방향으로 사용합니다[43, 42, 53]. 게다가, 변형 가능한 객체의 상호작용은 그 재료에 크게 의존하는데, 이는 순수한 시각적 단서로는 예측하기 어렵습니다[72]. 반면에, 자유형 객체는 어떠한 제약 없이 움직일 수 있습니다. 따라서 이 논문에서는 관절이 있는 객체에 대한 3D 상호작용만 렌더링합니다. 우리는 pytorch3D[55]와 opencv를 사용하여 투영과 호모그래피 피팅을 구현합니다. 최종 결과는 애니메이션 비디오에 나와 있습니다. B. 데이터 수집 이 섹션에서는 데이터 주석의 단계를 소개합니다. 그림 10에 데이터 세트의 통계를 보여줍니다. 또한 그림 11에 추가 주석을 보여줍니다. 쿼리 포인트 선택. 먼저 작업자에게 각 이미지에 대해 약 5개의 쿼리 포인트를 선택하도록 요청합니다. 쿼리 포인트는 대화형 객체에 있어야 합니다. 일부 쿼리 포인트는 큰 객체에 있어야 하고 다른 쿼리 포인트는 작은 객체에 있어야 합니다. 나중에 픽스처의 더 많은 쿼리 포인트에 주석을 달는데, 픽스처에는 추가 주석이 필요하지 않기 때문입니다. 바운딩 박스. 쿼리 포인트에 따라, 우리는 작업자들에게 바운딩 박스를 그리라고 요청합니다. 바운딩 박스는 객체의 움직일 수 있는 부분만 덮어야 합니다. 예를 들어, 쿼리 포인트가 냉장고 문에 있는 경우, 바운딩 박스는 냉장고 전체를 덮어야 하는 것이 아니라 문만 덮어야 합니다. &quot;여기서 무엇을 할 수 있을까?&quot;라고 묻고 있기 때문입니다. 객체의 속성. 그런 다음 객체의 속성에 주석을 달았습니다. 이것은 일련의 객관식 문제입니다. (1) 객체를 한 손으로 움직일 수 있습니까, 아니면 두 손으로 움직일 수 있습니까? (2) onehand nonrigid onehand nonrigid onehand rigid trans puonehand rigid_rot_pull onehand rigid_rot pull onehand rigid rot pull onehand rigid rot push onehand rigid_rot_pull onehand rigid free onehand rigid rot push onehand rigid trans p Rilakk onehand rigid free oreriano rigid free onehand rigid rot onehand rigid rot pull lonehand rigid rot pull onehand rigid rot twohand rigid free (各种状况从这里开始出现……….) onehand rigid free onehand rigid_rot_pus twohand nonrigid onehand nonrigid onehand rigid trans_pull id rconehand rigid rot pull Janehand rigid_rot_pullit 풀 한손 rigid free 한손_rigid_free 한손 rigid free 네혼핸드 rigid free 한손 rigid fronehand rigid free 한손 rigid free 한손 rigid free 한손 rigid free 한손_nonrigid 한손 rigid_trans_pull 한손 rigid free 한손 한손 rigid free 한손 rigid free 두손 rigid free 한손 rigid rot 풀 한손 rigid freel 한손 rigid free 한손 rigid free 한손 nonrigid 한손 ri 한손 rigid free 두손 rigid free 한손 rigid free 두손_rigid_free 한손 rigid free 한손 rigid_rot_pull 한손_rigid_rot_pull 한손 rigid_rot_pull 한손 rigid tone 한손 rigid oneh 한손 한손 rigid_free 한손 rigidtwohand_rigid_free 두손 rigid free 두하 두손 rigid 두손 rigid free 한손 nonrigid 두손 rigid_free| twohand rigid free twohand nonrigid gid onehand_rigid_rot_pull twohand twohand_nonrigid&quot; onehand rigid free Itwohand_nonrigid onehand rigid free onehand rigid_rot_pull onehand rigid rot p onehand onehand_nonrigid onehand_nonri onehand rigid free oneha twohand nonrigid twohand_nonrigid twohand_nonrigid twohand rigid free twohand_rigid_f onehand_nonrigid onehand rigid_rot_pull twohand onehand_rigid_rot_pull onehand rigid fr onehand rigiconehand rigid rot pull twohand_nonrigid onehand rigid trans_pull onehand rigid rot_puil onehand_rigid-rot_pull twohand_nonrigid twohand nitwohand nonrinid twohand twohar 그림 11. 3DOI 데이터 세트의 예시 주석. 행 1-2는 인터넷 비디오[53]에서 가져왔습니다. 행 3-4는 자기중심적 비디오[11]에서 가져왔습니다. 행 5-6은 3D 데이터 세트의 렌더링에서 가져온 것입니다[14]. 쿼리 포인트이고 어포던스는 물체입니다. 물체가 딱딱한지 아닌지?(3) 딱딱한 경우 관절이 있거나 자유형입니까?(4) 관절이 있는 경우 동작은 회전입니까 아니면 이동입니까?(5) 관절이 있는 물체와 상호 작용하고 싶은 경우 밀거나 당겨야 합니까? 회전 축. 회전할 수 있는 물체의 경우 작업자에게 회전 축을 나타내는 2D 선을 그리도록 요청합니다. 분할 마스크. 모든 물체에 대해 작업자에게 이동 가능한 부분의 분할 마스크를 그리도록 요청합니다. 고정물. 마지막으로 10,000개의 이미지를 더 수집하고 각 이미지에 대해 무작위로 5개의 쿼리 포인트를 샘플링합니다. 작업자에게 고정물인지 아닌지 주석을 달도록 요청합니다. 이러한 주석과 데이터 세트를 혼합합니다.
