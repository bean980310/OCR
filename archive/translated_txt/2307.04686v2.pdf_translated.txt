--- ABSTRACT ---
우리는 음악 합성, 압축, 인페인팅, 변형에 대한 마스크된 어쿠스틱 토큰 모델링 접근법인 VampNet을 소개합니다. 우리는 훈련 중에 가변적인 마스킹 일정을 사용하여 추론 중에 다양한 마스킹 접근법(프롬프트라고 함)을 적용하여 모델에서 일관된 음악을 샘플링할 수 있습니다. VampNet은 비자기 회귀적이며, 전방 패스에서 모든 토큰을 처리하는 양방향 변압기 아키텍처를 활용합니다. VampNet은 단 36번의 샘플링 패스로 일관된 고충실도 음악 파형을 생성할 수 있습니다. 우리는 다양한 방식으로 VampNet을 프롬프팅하여 음악 압축, 인페인팅, 아웃페인팅, 연속, 변형을 통한 루핑(뱀핑)과 같은 작업에 적용할 수 있음을 보여줍니다. 적절하게 프롬프팅된 VampNet은 음악의 스타일, 장르, 악기 연주 및 기타 고급 측면을 유지할 수 있습니다. 이러한 유연한 프롬프팅 기능은 VampNet을 강력한 음악 공동 창작 도구로 만듭니다. 코드 ³와 오디오 샘플은 온라인에서 제공됩니다.1.
--- METHOD ---
s이며 과거와 미래의 시퀀스 요소에 대한 조건이 필요한 infill과 같은 작업에 더 적합합니다.이 작업에서 병렬 반복 디코딩을 음향 토큰 모델링과 결합하여 음악 오디오 합성에 적용합니다.저희가 아는 한, 저희 모델은 병렬 반복 디코딩을 신경 오디오 음악 생성에 확장한 첫 번째 사례입니다.VampNet이라고 하는 저희 모델은1 저희 작업이 동료 검토를 받는 동안 Google은 저희와 유사한 병렬 반복 디코딩 접근 방식을 활용하는 SoundStorm[6]을 출시했습니다.CC + OH.Flores García, P. Seetharaman, R. Kumar 및 B. Pardo.크리에이티브 커먼즈 저작자표시 4.0 국제 라이선스(CC BY 4.0)에 따라 라이선스가 부여되었습니다.저작자 표시: H. Flores García, P. Seetharaman, R. Kumar 및 B. Pardo, &quot;VampNet: 마스크된 음향 토큰 모델링을 통한 음악 생성&quot;, Proc. of the 24th Int. 음악 정보 검색 협회, 이탈리아 밀라노, 2023. 토큰 반복 병렬 디코딩 Fine Coarse Time Prompt K VampNet(Coarse) Coarse 토큰(샘플링됨) 반복 병렬 디코딩 VampNet(Coarse-to-fine) Detokenizer(RVQ-VAE) Tokenizer(RVQ-VAE) 출력 Vamp 토큰(샘플링됨) 그림 1. VampNet 개요. 먼저 오디오 토크나이저를 사용하여 오디오를 이산 토큰 시퀀스로 변환합니다. 토큰을 마스크한 다음 마스크 생성 모델에 전달하여 두 수준에서 효율적인 반복 병렬 디코딩 샘플링 절차를 통해 마스크된 토큰의 값을 예측합니다. 그런 다음 결과를 오디오로 다시 디코딩합니다. 토큰 기반 프롬프팅을 통해 다양한 애플리케이션에 유연하게 적용할 수 있습니다. 선택적으로 마스크된 음악 토큰 시퀀스로 VampNet 생성을 안내하여 빈칸을 채우도록 요청할 수 있음을 보여줍니다. 이 절차의 출력은 고품질 오디오 압축 기술부터 스타일, 장르, 비트 및 악기 편성 면에서 원래 입력 음악과 일치하는 원래 입력 음악의 변형에 이르기까지 다양하지만 음색과 리듬의 세부 사항은 다양합니다.일부 접두사 오디오를 프롬프트로 사용하고 모델이 그럴듯하게 뒤에 올 수 있는 음악을 생성하도록 하는 음악 연속만 수행할 수 있는 자기 회귀 음악 모델[2, 3]과 달리 우리의 접근 방식은 프롬프트를 어디에나 배치할 수 있습니다.우리는 주기적, 압축 및 음악적으로 알려진 것(예: 비트 마스킹)을 포함한 다양한 프롬프트 디자인을 탐구합니다.우리는 우리 모델이 루프와 변형을 만드는 프롬프트에 잘 반응한다는 것을 발견했으며, 따라서 VampNet 2라는 이름이 붙었습니다.우리는 코드를 오픈 소스³로 만들고 독자들에게 오디오 샘플을 들어보라고 적극 권장합니다.4.2 뱀프는 변형을 통해 짧은 음악 구절을 반복하는 것입니다. 한국어: https://github.com/hugofloresgarcia/vampnet4 오디오 샘플: https://tinyurl.com/bdfj7rdx 2. 배경 생성 모델링에 대한 2단계 접근 방식은 주로 계산 효율성 덕분에 이미지[4,5,7,8] 및 오디오[2, 3, 6, 9] 합성에서 인기를 얻었습니다. 첫 번째 단계에서는 관심 도메인에 대한 &quot;토큰&quot;의 이산 어휘를 학습합니다. 입력은 인코더를 통해 이러한 토큰을 얻고, 이는 해당 디코더를 통해 입력 도메인으로 다시 변환할 수 있습니다. 두 번째 단계에서는 모델을 학습하여 토큰을 생성하고, 선택적으로 생성을 안내하기 위해 일부 조건(예: 이전 토큰, 텍스트 설명, 클래스 레이블)을 적용합니다. 2.1 1단계: 토큰화 이미지에서 시각적 토큰화는 최첨단 분류[10] 및 합성[4,7,8,11]에 활용되었습니다. 가장 인기 있는 접근 방식은 잠재 공간에서 벡터 양자화를 사용하는 것입니다. 오디오에 대해서도 유사한 접근 방식이 탐구되었지만[12], 최근까지 이러한 접근 방식은 낮은 샘플링 속도(예: 16kHz)로 제한되었거나 음성 오디오로 제한되었습니다. 잠재 공간의 &quot;샘플링 속도&quot;(오디오를 나타내는 데 필요한 매초 잠재 벡터 수)는 토큰화 체계의 중요한 측면입니다. 잠재 공간의 샘플링 속도가 낮을수록 더 쉽습니다. 다음 단계(세대)는 달성하는 것입니다.최근에, 높은 압축률에서 오디오 토큰화에 대한 잔여 벡터 양자화[13, 14] 기반 방법이 높은 샘플 레이트 오디오의 우수한 재구성 품질을 위해 제안되었습니다.오디오 토큰화를 위해 우리가 활용하는 주요 작업은 Descript Audio Codec(DAC)[15]입니다.DAC를 사용하면 오디오가 완전 합성 인코더를 통해 토큰 시퀀스로 인코딩됩니다.이 인코더의 출력은 벡터 양자화기의 계층적 시퀀스를 사용하여 양자화됩니다[11].각 양자화기는 이전 양자화기의 잔여 오류에서 작동합니다.이 잔여 벡터 양자화 덕분에 DAC는 높은 압축률에서 매우 높은 품질로 오디오를 재구성할 수 있습니다.이것은 이전 버전[13, 14]과 함께 AudioLM[2], MusicLM[3], VALL-E[1]와 같은 오디오 언어 모델을 활성화하는 데 도움이 됩니다. 나중에 토크나이저에 대해 간략하게 설명하지만, 저희 연구의 주요 기여는 모든 오디오 토크나이저의 출력에 적용할 수 있으며 저희의 특정 오디오 토크나이저는 이 연구의 초점이 아닙니다.2.2 2단계: 생성 토큰으로 인코딩된 오디오의 경우 일반적인 접근 방식은 생성을 위해 자기 회귀 모델[16]을 사용하는 것입니다.AudioLM[2], MusicLM[3], JukeBox[17]와 같은 최신(SOTA) 오디오 생성 접근 방식은 이 접근 방식을 사용하여 변압기 기반[18] 디코더 전용 모델을 사용하여 단계별 방식으로 시퀀스의 각 음향 토큰을 생성합니다.자기 회귀 샘플링은 추론 시간에 필요한 단계 수가 많기 때문에 본질적으로 느립니다[4].또한 자기 회귀 모델은 생성된 각 토큰이 이전 토큰에만 조건이 지정되므로 다운스트림 애플리케이션을 본질적으로 제한합니다. 자기 회귀 모델이 인페인팅(&quot;중간 채우기&quot;)과 같은 작업을 수행하려면 학습 중에 데이터를 다시 정렬해야 합니다[19]. 언어에서 마스크 모델링은 고품질 의미 표현을 위한 사전 학습 절차로 광범위하게 사용되었습니다[20]. 이 절차는 이미지[21] 및 오디오[22]에서 표현 학습을 위해 확장되었습니다. 표현 학습을 위한 마스크 모델링은 일반적으로 일정한 마스크 확률을 갖습니다. 예를 들어, BERT[20]에서 토큰은 학습 중 15%의 시간 동안 마스크됩니다. 이 접근 방식은 노이즈 처리 절차에 마스크를 사용하는 단일 단계 이산 확산 모델[23]과 동일하다는 것이 밝혀졌습니다. 따라서 학습 중에 토큰을 마스크할 확률을 변경하여 마스크 모델링을 마스크 생성 모델링으로 확장할 수 있습니다. 이는 MaskGIT[4] 및 언어[23]에서 이미지 생성을 위해 수행되었습니다. 데이터를 합성하려는 확산 모델링[24, 25]과 유사합니다. 무작위 노이즈에서 시작하여 일련의 노이즈 제거 단계를 거치는 마스크 생성 모델링은 완전히 마스크된 데이터에서 일련의 &quot;언마스킹&quot; 단계를 거쳐 데이터를 합성하려고 합니다. MaskGIT 및 관련 접근 방식의 효율성의 핵심은 병렬 반복 디코딩 절차입니다. 병렬 반복 디코딩에서 모델은 단일 포워드 패스에서 출력 시퀀스의 모든 토큰을 예측합니다. 그러나 모델의 포워드 패스를 한 번만 거친 후에는 출력의 품질이 높지 않은 경우가 많습니다. 첫 번째 샘플링 단계의 출력은 더 낮은 마스킹 확률로 다시 마스크된 다음 다시 모델을 거칩니다. 이런 식으로 마스크 생성 모델은 출력을 효율적으로 정제하여 고품질 생성을 달성할 수 있습니다. 무조건 생성 작업에서 모델은 아무런 지침 없이 처음부터 대상 데이터 분포에서 현실적인 샘플을 생성하도록 요청받습니다. 이는 많은 대상 데이터 분포가 매우 멀티모달이기 때문에 어려운 문제입니다. 무조건 생성 모델은 모드 붕괴[26], 흐릿한 샘플, 모드 평균화 및 기타 문제[27]에 취약합니다. 따라서 일부 컨디셔닝은 모델이 멀티모달리티를 해결할 수 있는 신호를 제공하므로 도움이 됩니다.컨디셔닝은 또한 시스템 출력을 원하는 콘텐츠로 안내하는 데 일반적으로 사용되는 방법입니다.컨디셔닝은 클래스 레이블, 장르 태그 또는 가사[17] 또는 연관된 텍스트 설명[3,8,28]의 형태를 취할 수 있습니다.컨디셔닝은 AudioLM[2]의 의미 토큰이나 텍스트-음성 생성을 위한 정렬된 텍스트 또는 음소[1]와 같이 모든 타임스텝에 적용할 수도 있습니다.이 작업에서 우리는 MaskGIT[4] 및 Paella[5]와 같은 비전 작업에서 영감을 받아 병렬 반복 디코딩 절차를 갖춘 마스크 생성 모델링 접근 방식을 채택합니다(그림 1 참조).인코딩된 오디오에서 마스크되지 않은 토큰이 제공하는 것 이상의 컨디셔닝은 적용하지 않습니다.나중에 보여드리겠지만 추론 시간에 적용되는 다양한 마스킹 접근 방식을 사용하여 유용하고 예술적인 방식으로 생성을 조정할 수 있습니다.훈련에서 토큰은 시퀀스 전체에서 무작위로 마스크됩니다. 그런 다음 모델은 단일 전방 패스에서 각 마스크된 토큰의 가치를 예측하도록 요청받지만, 미래와 과거의 모든 마스크되지 않은 토큰에 대한 조건이 적용됩니다. 토큰의 수를 다양하게 변경합니다. Fine Training Coarse Time [MASK] Y(r) = (0,1] L. mask VampNet(coarse-to-fine) VampNet(coarse) 압축 주기적 프롬프팅 mask 병렬 반복 디코딩을 통한 샘플링 MM MMMM MMMMMM MM MM M MMMMM MMMMMM MMMMMM MMMMMMMM ■■33■■M|M|M|M|M|M|MMMMMMMM MM MMM t =t =t =t =M t = T 비트 구동 w = 예측 비트 마크 인페인팅 H 그림 2. VampNet의 훈련, 샘플링 및 프롬프팅. 훈련: Masked Acoustic Token Modeling을 사용하여 VampNet을 훈련합니다. 여기서 입력 음향 토큰 세트의 일부를 무작위로 마스크하고 가변 마스킹 일정을 사용하여 마스크된 토큰 세트를 예측하는 방법을 학습합니다. Coarse 모델 훈련은 거친 토큰을 마스크합니다. Coarse-to-fine 훈련은 fine 토큰만 마스크합니다. 샘플링: 영어: 병렬 반복 디코딩을 사용하여 VampNet에서 음향 토큰의 새로운 시퀀스를 샘플링합니다. 여기서 각 반복에서 가장 신뢰할 수 있는 예측 토큰의 하위 집합을 샘플링합니다. 프롬프트: VampNet은 음악을 생성하기 위해 여러 가지 방법으로 프롬프트될 수 있습니다. 예를 들어, 입력 시퀀스의 모든 P번째 타임스텝이 마스크 해제되는 주기적으로 프롬프트되거나, 노래의 비트 표시 주변의 타임스텝이 마스크 해제되는 비트 중심 방식으로 프롬프트될 수 있습니다. 훈련 중에 마스크되어 샘플링 절차를 통해 추론 시간에 오디오를 생성할 수 있습니다. 이제 우리의 방법을 더 자세히 설명합니다. 3. 방법 우리는 MaskGIT [4]에서 제안된 Masked Visual Token Modeling 절차를 오디오에 적용하여 시각과 오디오 도메인 간의 몇 가지 주요 차이점을 설명합니다. 우리의 접근 방식을 Masked Acoustic Token Modeling이라고 합니다. 3.1 Masked Acoustic Token Modeling 먼저 DAC [15]에 설명된 기술을 기반으로 오디오 토크나이저를 훈련합니다. MaskGIT의 시각적 토큰과 달리, 음향 토큰은 잔여 벡터 양자화로 인해 본질적으로 계층적입니다.첫 번째 단계로, 오디오 신호 x는 각 시간 단계 t에서 aa D 차원 잠재 벡터 Z로 인코딩됩니다.그런 다음 N 벡터 양자화기를 사용하여 Z를 양자화합니다.양자화기 1은 잔여 오류 R₁ = Z – 21을 갖는 Z의 양자화된 근사치인 Ź₁을 생성합니다.그런 다음 각 양자화기 i의 잔여는 다음 양자화기 i + 1로 전달되고, 이는 나머지 잔여 오류의 양자화된 근사치를 생성합니다: R¿ ≈ Z₁+1. 벡터 Z는 N 양자화기의 출력을 합산하여 재구성됩니다: Z = Zi.인코딩된 신호는 각 시간 단계에서 N개의 이산 토큰의 양자화된 벡터로 표현되므로 각 시간 단계에서 마스크되거나 마스크되지 않을 수 있는 N개의 토큰이 있습니다. 한 번에 모든 토큰을 생성하려고 하기보다는 AudioLM에서처럼 N개의 토큰을 Nc개의 &quot;거친&quot; 토큰과 Nf개의 &quot;미세&quot; 토큰으로 분할합니다.그런 다음 두 개의 생성 모델을 학습합니다.하나는 거친 토큰을 조건으로 주어 미세 토큰을 생성하고, 다른 하나는 거친 토큰 시퀀스가 주어지면 거친 토큰을 생성합니다.샘플(그림 1)을 생성하려면 두 모델을 연결합니다.먼저 거친 모델을 적용하여 거친 토큰 시퀀스를 생성합니다.그런 다음 거친-미세 모델을 적용하여 미세 토큰을 생성합니다.오디오 토크나이저의 디코더를 사용하여 토큰을 44.1kHz 파형으로 디코딩합니다.3.2 학습 절차 YЄ RTN을 어떤 오디오 세그먼트에 대한 인코더의 출력을 나타내는 행렬이라고 합니다.Y의 각 요소 Yt,n은 타임스텝 t에서 n번째 레벨 코드북의 토큰입니다. YM을 Y의 모든 마스크된 토큰의 집합이라 하고 Yu를 Y의 모든 마스크되지 않은 토큰의 집합이라 하자. 모델은 마스크되지 않은 토큰과 모델 매개변수 0이 주어졌을 때 각 토큰 y Є YM에 대한 가능한 코드북 값 집합에 대한 확률 분포를 생성한다. 학습 목표는 진짜 토큰의 확률을 최대화하는 것이다. 이는 음의 로그 우도를 최소화하는 것과 같다. L = log p(y Yu,0) VyЄYM (1) 마스크된 토큰을 예측하기 위해 다층 양방향 변환기를 사용하는데, 이는 모든 양자화기에 대해 모든 타임스텝에서 각 가능한 토큰의 확률을 예측한다. 각 양자화기의 코드북 크기가 C개의 가능한 값이며 N개의 양자화가 있는 경우, 네트워크의 마지막 계층은 (E,CN) 모양의 완전 연결 계층이 된다. 여기서 E는 마지막 계층의 출력의 차원이다. 그런 다음 이 출력을 (EN, C)로 재구성하고 실제 onehot 토큰과 예측 토큰 간의 교차 엔트로피 손실을 계산합니다.변환기는 양방향이므로 입력 시퀀스의 모든 토큰을 처리하여 각 토큰의 손실을 최적화할 수 있습니다.거친-미세 생성 모델의 경우 입력 시퀀스에는 항상 거친 토큰이 없고 마스킹 작업은 Nf개의 미세 토큰으로 제한됩니다.이 네트워크의 마지막 계층은 마스크된 미세 토큰만 예측합니다.그 외에는 두 모델의 학습 절차가 동일합니다.3.3 샘플링 MaskGIT에서 사용된 것과 동일한 반복적 신뢰 기반 샘플링 방식을 따릅니다.더 구체적으로, 마스크된 토큰 집합을 YM, 마스크되지 않은 토큰 집합을 Yu라고 할 때 다음을 수행합니다.1. 추정.Yм의 각 마스크된 토큰 y에 대해 코드북 값 V의 어휘에 대한 조건부 확률 분포를 추정합니다.2. 샘플링. 각 마스크된 토큰에 대해 분포에서 샘플링하여 연관된 토큰 추정치 ŷ E V를 생성합니다. 이 단계에서는 어떠한 샘플링 트릭도 사용하지 않고, 각 토큰에 대한 범주형 확률 분포에서 그대로 샘플링합니다. 3. 신뢰도에 따른 순위 매기기. 예측 로그 확률을 취하고 이에 temperatureannealed Gumbel 노이즈를 추가하여 샘플링된 각 토큰에 대한 신뢰도 측정값을 계산합니다. confidence (ŷt) = log(p(ŷt)) + temp · 9t (2). 여기서 ŷt는 시간 단계 t에서의 토큰 추정치입니다. Gumbel(0,1) [29]에서 추출한 iid 샘플이고 temp는 샘플링 반복 횟수에 따라 O로 선형적으로 어닐링된 하이퍼파라미터입니다. 그런 다음 위에서 계산된 신뢰도에 따라 샘플링된 토큰 추정치 세트를 정렬합니다. 높은 temperature 값(예: &gt; 6.0)이 더 높은 품질의 샘플을 생성합니다. 4. 선택합니다. 다음 샘플링 반복 k에서 마스킹 일정 5에 따라 마스킹할 토큰 수를 선택합니다. 가장 낮은 신뢰도 추정치 k개를 가져와 버리고 토큰을 다시 마스킹합니다. 나머지 높은 신뢰도 토큰 추정치를 Yu에 두고 토큰을 Yê·에서 제거합니다. 5. 반복 반복 횟수에 도달할 때까지 1단계로 돌아갑니다. 3.4 프롬프트 마스크되지 않은 토큰의 조건 프롬프트를 통해 샘플링 절차에 인간의 안내를 통합하여 대화형 음악 편집을 활성화할 수 있습니다. 우리의 접근 방식은 입력 오디오 자체 이외의 다른 신호에 따라 조건화되지 않으므로 다양한 유형의 프롬프트가 모델에서 샘플링할 때 다중 모달의 양을 낮추므로 일관된 샘플을 얻는 데 유용하다는 것을 알았습니다. AudioLM과 마찬가지로 일정 기간(일반적으로 1~4초)의 접두사 오디오로 모델을 프롬프트할 수 있으며 해당 오디오의 연속을 제공합니다. AudioLM 및 기타 자동 회귀적 접근 방식과 달리 접미사 오디오로 모델을 프롬프트할 수도 있으며, t는 현재 반복이고, t는 총 반복 횟수이며, D는 시퀀스의 총 토큰 수인 5k = (+)D를 생성합니다. 스케줄링 함수는 코사인 스케줄입니다. 해당 접미사로 이어지는 오디오입니다. 접두사 및 접미사 오디오를 제공할 수 있으며, 모델은 지정된 접두사 및 접미사가 주어지면 적절한 나머지 오디오를 생성합니다. 또한 모든 P번째 타임스텝을 제외한 모든 타임스텝이 마스크되는 &quot;주기적&quot; 프롬프트를 적용할 수도 있습니다. P가 낮을수록 생성된 오디오는 모델이 고도로 조건화되어 있기 때문에 원본과 더 비슷하게 들립니다. 예를 들어 P = 2인 경우 모델은 본질적으로 업샘플러처럼 작동하여 다른 모든 타임스텝에 대한 토큰을 입력합니다. P가 증가함에 따라 모델은 압축 모드에서 생성 모드로 작동하여 원본의 스타일과 일치하는 변형을 만듭니다. 또 다른 유용한 프롬프트 스타일은 &quot;압축&quot; 프롬프트로, 가장 거친 코드북을 제외한 모든 코드북이 마스크 처리됩니다. 이렇게 하면 모델이 모든 타임스텝에서 강력한 컨디셔닝을 하게 되므로 모델은 원본과 거의 일치하는 오디오를 생성할 가능성이 높습니다. 이 프롬프트를 낮은 P를 가진 주기적 프롬프트와 결합하여 훨씬 더 극단적인 압축 비율을 얻을 수 있습니다. 코드북 수 N, 주기적 프롬프트에 대한 다운샘플링 속도 P, 유지된 코드북 수 Nk를 갖는 코덱 B의 비트 전송률을 감안하면 B/P(N - Nk)의 비트 전송률을 얻을 수 있습니다. 마지막으로 음악 구조에 대한 지식을 활용하는 음악별 프롬프트를 설계할 수 있습니다. 보다 구체적으로, 비트 중심 프롬프트를 살펴보겠습니다. 비트 위나 주변에 있는 타임스텝은 마스크 처리되지 않습니다. 모델은 이러한 비트 사이에 음악을 생성하여 원본 음악에 흥미로운 변형을 만들어냅니다. 이러한 프롬프트를 모두 결합하여 매우 유용한 음악 제작 도구를 만들 수 있습니다. 잘 설계된 사용자 인터페이스와 더불어 VampNet은 차세대 음악 편집 및 제작 제품군의 기반으로서 유망한 모습을 보이고 있습니다. 4.
--- EXPERIMENT ---
S 저희 실험은 3.4절에 설명된 다양한 프롬프팅 전략을 고려하여 VampNet이 음악을 압축하고 생성하는 기능을 평가하는 것을 목표로 합니다. 객관적인 오디오 품질 측정을 위해 다중 스케일 멜 재구성 오류와 Fréchet 오디오 거리(FAD)를 사용합니다. 멜 재구성 오류는 다양한 시간 스케일에서 로그 멜 스펙트로그램 간의 L1 거리로 정의됩니다. Df,m = ||Ŝf,m – SF,M||(3) 여기서 F는 각 스펙트로그램의 FFT 크기이고 M은 멜 주파수 빈의 수입니다. 저희는 FЄ [2048, 512]와 MЄ [150, 80]을 사용하며, 홉 크기는 FFT 크기입니다. 멜 재구성은 압축 품질을 위한 지표로는 가치가 있지만 생성 품질에는 가치가 없습니다. 모델이 원래 대상 오디오와 일대일로 일치하지 않는 오디오를 생성할 가능성이 있기 때문입니다. 생성 품질을 위해 실제 오디오와 생성된 오디오의 분포 간 중첩을 측정하는 FAD를 사용합니다. melreconstruction과 달리 FAD는 샘플 품질이 실제 오디오의 데이터 분포 내에 있는지 평가하는 데 더 중점을 두고 있으며 생성 품질을 평가하는 데 사용할 수 있습니다. FAD+ Mel Spectrogram Loss +2.2. 샘플링 단계 수 1.1.0.토크나이저(기준선) 1단계 4단계 12단계 36단계 64단계 72단계토크나이저(기준선) 1단계 4단계 12단계 36단계 64단계 72단계 FAD+ Mel Spectrogram Loss 0.3.2.2.0 생성 품질에 대한 프롬프트의 효과 토크나이저 기준선 주기적 P=주기적 P=inpaint 1초 컨텍스트 토크나이저 기준선 주기적 P=주기적 P=inpaint 비트 구동 2초 컨텍스트 75ms/비트 = 잡음 비율: 0.잡음 inpaint inpaint 비트 구동 1초 컨텍스트 2초 컨텍스트 75ms/비트 비율: 0.그림 3. P = 주기적 프롬프트를 사용하여 샘플링 단계 수를 다양하게 변경하여 수집한 VampNet 샘플에 대한 Mel 재구성 오류(위) 및 Fréchet 오디오 거리(FAD, 아래) 16. 샘플은 매우 낮은 비트레이트(bps)로 토큰을 압축 해제하여 생성되었으며, 입력 신호의 변형을 효과적으로 생성했습니다.4.1 데이터 세트 JukeBox [17]와 유사하게 인기 있는 음악 녹음의 대규모 데이터 세트를 수집합니다.데이터 세트는 32kHz의 샘플링 속도를 가진 797k 트랙으로 구성되어 있습니다.이 트랙은 토크나이저와 호환되도록 44.1kHz로 리샘플링됩니다.데이터 세트에는 Echo Nest의 Every Noise at Once에 설명된 장르에 걸쳐 수천 명의 아티스트의 음악이 포함되어 있습니다.2k 트랙의 하위 집합을 검증에 사용하고 2k 트랙의 다른 하위 집합을 테스트에 사용합니다.트레이닝, 검증 및 테스트 트랙 간에 아티스트가 겹치지 않도록 합니다.또한 DAC [15]에 설명된 데이터 세트를 사용하여 토크나이저를 학습하는 데 사용된 음악 및 비음악 데이터(음성, 환경 소리) 세트를 수집합니다.모든 오디오는 -24dbFS로 정규화됩니다.모델이 무조건적으로 학습되므로 학습 중에 이러한 파일에 대한 메타데이터를 사용하지 않습니다. 4.2 네트워크 아키텍처 및 하이퍼파라미터 우리가 사용하는 오디오 토크나이저 모델은 입력으로 44.1kHz 오디오를 사용하고 코드북을 사용하여 768배의 다운샘플링 속도로 8kbps의 비트 전송률로 압축합니다.따라서 잠재 공간은 57Hz이고 모든 타임스텝에서 예측할 토큰은 14개입니다.이러한 토큰 중 4개를 거친 토큰으로 지정하고 나머지 10개를 미세 토큰으로 지정합니다.토크나이저 아키텍처에 대한 자세한 내용은 Descript Audio Codec [15]을 참조하십시오.토크나이저를 250k 단계 동안 훈련합니다.VampNet 아키텍처(거친 모델과 거친-미세 모델 모두)는 상대적 어텐션[30]이 있는 양방향 변압기[18]와 1280의 임베딩 차원, 20개의 어텐션 헤드로 구성됩니다. 거친 모델에는 주의 계층이 있고, 거친-미세 모델에는 16개가 있습니다.각각 1M과 500k 단계에 대해 거친 모델과 거친-미세 모델을 훈련합니다.B1과 B2를 각각 0.9와 0.999로 설정한 AdamW 옵티마이저[31]로 훈련합니다.6 https://everynoise.com/engenremap.html 그림 4. 다양한 유형의 프롬프트로 수집한 VampNet 10초 샘플에 대한 다중 스케일 멜 스펙트로그램 오류(위)와 프레셰 오디오 거리(FAD, 아래).Vaswani et al[18]이 도입한 학습 속도 스케줄러를 사용하고 목표 학습 속도는 0.001, 워밍업 단계는 10k입니다.GPU 메모리 예산은 72GB이고 드롭아웃은 0.1, 배치 크기는 25입니다.4.3 VampNet의 효율성 먼저 VampNet이 적은 단계로 사실적인 음악 오디오를 생성할 수 있는지 검증합니다. 이를 위해 우리는 테스트 세트에서 10초 발췌 부분에 대해 우리의 프롬프트 중 하나(P = 16인 주기적 프롬프트)를 사용하여 VampNet을 실행합니다. 우리는 [1, 4, 8, 12, 36, 64, 72]에서 샘플링 단계 수를 다르게 하고 각 샘플링 단계에 대한 메트릭을 보고합니다. 4.4 프롬프트의 효과 우리는 섹션 3.4에서 논의된 것처럼 VampNet이 다른 프롬프트에 어떻게 반응하는지 이해하고자 합니다. 프롬프트는 음악을 낮은 비트레이트로 압축하는 &quot;압축&quot; 프롬프트에서 보다 창의적인 &quot;생성&quot; 프롬프트까지 다양합니다. 압축 및 생성 프롬프트가 연속체에 존재하는지, 낮은 비트레이트에서 압축을 해제하면 생성 동작이 발생하는지 살펴봅니다. 평가 데이터 세트에서 2000개의 10초 예제를 추출하고 오디오 토크나이저로 토큰 스트림으로 인코딩한 다음 토큰 스트림을 네 가지 방법으로 조작합니다. 1. 압축 프롬프트: C 코드북은 가장 거친 코드북부터 시작하여 마스크되지 않은 상태로 둡니다. 다른 모든 토큰은 마스크됩니다. Nk = 1로 설정합니다. 2. 주기적 프롬프트: 모든 P번째 타임스텝은 마스크되지 않은 상태로 둡니다. 마스크되지 않은 타임스텝에서 모든 코드북의 토큰은 마스크되지 않습니다. 다른 모든 토큰(예: 주기 P에 해당하지 않는 타임스텝의 토큰)은 마스크됩니다. P = [8, 16, 32]로 설정합니다. 3. 접두사 및 접미사(인페인트) 프롬프트: 시퀀스의 시작과 끝에 있는 세그먼트는 마스크되지 않은 채로 둡니다. 다른 모든 토큰은 마스크됩니다. 이 프롬프트는 초 단위의 컨텍스트 길이로 매개변수화됩니다. 컨텍스트를 1초 또는 2초로 설정하는데, 이는 57 또는 114개의 타임스텝에 해당합니다. 4. 비트 기반 프롬프트: 먼저 비트 추적기[32]로 오디오 파형을 처리합니다. 그런 다음 감지된 각 비트 주변에서 비트 오른쪽의 타임스텝을 마스크 해제합니다. 각 비트 주변의 75ms 마스크 해제 섹션을 조사하는데, 이는 비트당 약 4개의 타임스텝입니다. 프롬프트로 입력 토큰 스트림을 조작한 후 VampNet을 사용하여 이러한 마스크된 토큰 스트림에서 새로운 음악 신호를 생성하고 생성된 신호와 음악 데이터 세트의 입력 신호 간의 FAD 및 멜 재구성 오류를 계산합니다. 노이즈가 있는 토큰 스트림 베이스라인을 포함하는데, 여기서 입력 토큰 스트림의 토큰 일부(마스크 비율 r에 따라 결정됨)가 무작위 토큰으로 대체됩니다. 또한 코덱 자체와 coarse-to-fine 모델을 기준으로 포함합니다. 마지막으로 이러한 프롬프트가 특히 압축 및 주기적 프롬프트와 어떻게 결합될 수 있는지 살펴봅니다. 이러한 프롬프트(C 및 P)의 하이퍼파라미터를 조작하여 모델 동작을 압축에서 생성으로 전환할 수 있습니다. 더 많은 타임스텝이 마스크됨에 따라 모델은 마스크되지 않은 타임스텝을 연결하는 그럴듯한 음악 발췌문을 생성해야 하며, 이는 입력 음악과 일치하지 않을 수 있습니다. 5. 결과 및 논의 VampNet으로 샘플을 생성하는 데 사용된 샘플링 단계 수를 변경한 실험 결과는 그림 3에 나와 있습니다. VampNet은 36개의 샘플링 단계로 가장 낮은 FAD를 달성하지만 12개의 샘플링 단계로 비슷한 성능을 달성한다는 것을 알 수 있습니다. 실제로 24개의 단계로 수집한 샘플은 생성 품질과 컴퓨팅 속도 간에 공정한 균형을 이루며, NVIDIA RTX3090에서 10초 샘플을 샘플링하는 데 약 6초가 걸립니다. 대조적으로, 자기 회귀 모델을 사용하여 10초 분량의 오디오를 생성하려면 steps가 필요하며, 이는 우리와 동일한 수의 매개변수와 동일한 토크나이저를 사용하는 자기 회귀 모델에서 10초 분량의 오디오를 생성하는 데 약 1분이 걸립니다. 각 프롬프트의 효과에 대한 연구 결과는 그림 4에 나와 있습니다. 첫째, 노이즈가 있는 토큰 베이스라인은 모든 프롬프트와 비슷한 멜 재구성을 갖지만 FAD 측면에서는 성능이 매우 낮습니다. 이는 우리의 프롬프트 전략이 원래 입력 오디오와 완벽하게 일치하지 않는 오디오를 생성할 수 있지만 여전히 그럴듯한 음악 분포 내에 속한다는 것을 나타냅니다. 제안된 프롬프트 중에서 비트 중심 프롬프트가 가장 좋은 성능을 보이며 모든 프롬프트 중 가장 낮은 FAD를 달성합니다. 여기서 주목할 만한 결과는 P = 16(35개 컨디셔닝 타임스텝)인 주기적 프롬프트가 1초의 컨텍스트(57개 컨디셔닝 타임스텝)를 사용한 인페인팅과 동일한 성능을 보인다는 것입니다. 따라서 조건 토큰을 시퀀스 전체에 분산시키는 프롬프트 기술(주기적 프롬프트)은 모든 조건 토큰을 시퀀스의 시작과 끝에 배치하는 샘플링 기술(인페인팅)을 사용하여 생성한 샘플보다 더 적은 조건 타임스텝을 사용하여 비슷한 품질의 샘플을 생성할 수 있습니다. 정성적으로, 우리는 또한 비트 중심 프롬프트가 다른 프롬프트보다 더 꾸준한 템포를 유지할 수 있지만, 그 출력은 periFAD + Mel Spectrogram Loss + 2.2. 오디오 품질 대 압축 양 1.1.= 0.tokenizer coarse2fine Nk =~8kbps ~800bps P=~600bps Nk =P=200bps Nk =P=~50bps Nk =P=~25bps noise noise (r=0.5) (r=0.75) 10.7.5.2.0.tokenizer coarse2fine Nk =~8kbps ~800bps P=Nk =P=noise noise Nk =Nk =P=P=32 (r=0.5) (r=0.75) ~600bps ~200bps -50bps -25bps 그림 5. 다양한 비트레이트에서 VampNet 샘플에 대한 Mel-스펙트로그램 오류(위) 및 Fréchet 오디오 거리(FAD)(아래). 입력 시퀀스의 토큰을 노이즈 비율 r.odic 프롬프트에 따라 랜덤 토큰으로 대체하여 기준선을 제공합니다. 실제로 비트 기반, 주기적 및 인페인팅 프롬프트를 혼합하여 창의적인 방식으로 VampNet을 조종할 수 있습니다. 예를 들어, 독자는 첨부된 사운드 샘플 7을 들어보시기를 적극 권장합니다. 그런 다음 주기적 및 압축 프롬프트를 결합하여 더 많은 토큰이 마스크됨에 따라 모델의 동작이 재구성 및 생성 작업 사이에서 어떻게 바뀌는지 보여줍니다. 이 실험의 결과는 그림 5에 나와 있습니다. 더 높은 비트레이트(600bps 이상)에서 VampNet은 원래 음악 신호를 정확하게 재구성하여 평가 음악 오디오에 대한 낮은 Mel-스펙트로그램 오류 및 FAD 값을 달성할 수 있습니다. 200bps 이하의 비트레이트에서 VampNet은 노이즈가 있는 토큰 베이스라인과 비슷한 재구성 품질을 보이는데, 이는 샘플링된 VampNet 신호가 더 이상 미세한 스펙트럼 구조 측면에서 입력 오디오와 유사하지 않음을 나타냅니다. 그러나 낮은 비트레이트에서 VampNet 샘플의 FAD는 노이즈가 있는 베이스라인의 FAD보다 훨씬 낮습니다. 이는 VampNet이 낮은 비트레이트에서 입력 음악 신호를 재구성할 수 없더라도 여전히 노이즈가 있는 베이스라인보다 &quot;실제 음악&quot;의 분포에 더 가까운 음악적 구조를 가진 일관된 오디오 신호를 생성할 수 있음을 나타냅니다. 6.
--- CONCLUSION ---
우리는 음악 생성을 위한 마스크된 어쿠스틱 토큰 모델링 접근법인 VampNet을 소개했습니다. VampNet은 양방향이며 입력 오디오 파일을 사용하여 다양한 방식으로 프롬프트할 수 있습니다. 다양한 프롬프트 기술을 통해 VampNet은 음악 압축과 생성 사이의 연속체에서 작동할 수 있으며 음악 작품의 변형을 생성하는 데 탁월한 도구입니다. VampNet을 사용하면 음악가가 짧은 루프를 녹음하고 VampNet에 공급한 다음 루프 영역이 반복될 때마다 VampNet이 녹음된 아이디어에 대한 음악적 변형을 만들 수 있습니다. 향후 작업에서는 VampNet과 프롬프트 기술의 대화형 음악 공동 창작 잠재력을 조사하고 마스크된 어쿠스틱 토큰 모델링의 표현 학습 기능을 탐색하고자 합니다. 7개의 오디오 샘플: https://tinyurl.com/bdfj7rdx 7. 참고 자료 [1] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., “신경 코덱 언어 모델은 음성 합성기에 대한 제로샷 텍스트입니다.&quot; arXiv 사전 인쇄 arXiv:2301.02111, 2023. [2] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, O. Teboul, D. Grangier, M. Tagliasacchi 및 N. Zeghidour, &quot;Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식&quot;, arXiv 사전 인쇄 arXiv:2209.03143, 2022. [3] A. 아고스티넬리, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi 외, &quot;Musiclm: 텍스트에서 음악 생성,&quot; arXiv 사전 인쇄본 arXiv:2301.11325, 2023. [4] H. Chang, H. Zhang, L. Jiang, C. Liu, WT Freeman, &quot;Maskgit: 마스크 생성 이미지 변환기,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022, pp. 11 315-11 325. [5] D. Rampas, P. Pernias, E. Zhong, M. Aubreville, &quot;벡터 양자화 잠재 공간에서의 빠른 텍스트 조건부 이산 잡음 제거,&quot; arXiv 사전 인쇄본 arXiv:2211.07292, 2022. [6] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, M. Tagliasacchi, &quot;Soundstorm: 효율적인 병렬 오디오 생성&quot;, 2023. [7] P. Esser, R. Rombach, B. Ommer, &quot;고해상도 이미지 합성을 위한 변압기 길들이기&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021, pp. 12 873-12 883. [8] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022. [9] 영어: J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; 2023. [10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., &quot;이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기,&quot; arXiv 사전 인쇄본 arXiv: 2010.11929, 2020. [11] A. Van Den Oord, O. Vinyals et al., &quot;신경 이산 표현 학습,&quot; 신경 정보 처리 시스템의 발전, vol. 30, 2017. [12] C. Gârbacea, A. van den Oord, Y. Li, FS Lim, A. Luebs, O. Vinyals 및 TC Walters, &quot;vq-vae 및 wavenet 디코더를 사용한 저비트율 음성 코딩&quot;, ICASSP 2019-2019 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2019, 735-739쪽. [13] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, 및 M. Tagliasacchi, &quot;Soundstream: 엔드투엔드 신경 오디오 코덱,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 제30권, 495-507쪽, 2021년. [14] A. Défossez, J. Copet, G. Synnaeve, 및 Y. Adi, &quot;고충실도 신경 오디오 압축,&quot; arXiv 사전 인쇄본 arXiv:2210.13438, 2022년. [15] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, 및 K. Kumar, &quot;개선된 rvqgan을 갖춘 고충실도 오디오 압축,&quot; 2023년. [16] A. Radford, K. Narasimhan, T. Salimans, I. 영어: Sutskever et al., &quot;생성적 사전 훈련을 통한 언어 이해 향상&quot;, 2018. [17] P. Dhariwal, H. Jun, C. Payne, JW Kim, A. Radford, and I. Sutskever, &quot;주크박스: 음악을 위한 생성 모델&quot;, arXiv 사전 인쇄본 arXiv:2005.00341, 2020. [18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, and I. Polosukhin, &quot;주의만 있으면 됩니다&quot;, 신경 정보 처리 시스템의 발전, vol. 30, 2017. [19] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen, &quot;중간을 채우기 위한 언어 모델의 효율적인 학습,&quot; arXiv 사전 인쇄본 arXiv:2207.14255, 2022. [20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, &quot;Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습,&quot; arXiv 사전 인쇄본 arXiv:1810.04805, 2018. [21] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, &quot;Masked autoencoders are scalable vision learners,&quot; IEEE/CVF Conference on Computer Vision and Pattern의 회의록 인식, 2022, pp. 16000-16009. [22] Y.-A. 정, Y. 장, W. 한, C.-C. Chiu, J. Qin, R. Pang 및 Y. Wu, &quot;W2v-bert: 자기 지도 음성 사전 훈련을 위한 대조 학습 및 마스크 언어 모델링 결합&quot;, 2021 IEEE 자동 음성 인식 및 이해 워크숍(ASRU). IEEE, 2021, pp. 244–250. 영어: [23] J. Austin, DD Johnson, J. Ho, D. Tarlow 및 R. van den Berg, &quot;이산 상태 공간의 구조화된 잡음 제거 확산 모델&quot;, 신경 정보 처리 시스템의 발전, 34권, 1798-117993쪽, 2021. [24] Y. Song 및 S. Ermon, &quot;데이터 분포의 기울기 추정을 통한 생성 모델링&quot;, 신경 정보 처리 시스템의 발전, 34권, 1798-117993쪽, 2021. 32, 2019. [25] J. Ho, A. Jain, 및 P. Abbeel, &quot;Denomiising diffusion probabilistic models,&quot; Advances in Neural Information Processing Systems, vol. 33, pp. 6840-6851, 2020. [26] A. Srivastava, L. Valkov, C. Russell, MU Gutmann, 및 C. Sutton, &quot;Veegan: 암묵적 변분 학습을 사용하여 gans에서 모드 붕괴 감소&quot;, Advances in neural information processing systems, vol. 30, 2017. [27] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, 및 X. Chen, &quot;gans 훈련을 위한 개선된 기술&quot;, Advances in neural information processing systems, vol. 29, 2016. [28] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, WT Freeman, M. Rubinstein et al., &quot;Muse: 마스크 생성 변환기를 통한 텍스트-이미지 생성&quot;, arXiv 사전 인쇄본 arXiv:2301.00704, 2023. [29] EJ Gumbel, &quot;극단값의 통계 이론 및 몇 가지 실제 응용 프로그램; 일련의 강의.&quot; 워싱턴, 1954. [30] P. Shaw, J. Uszkoreit, 및 A. Vaswani, &quot;상대적 위치 표현을 사용한 자기 주의&quot;, arXiv 사전 인쇄본 arXiv:1803.02155, 2018. [31] I. Loshchilov 및 F. Hutter, &quot;adam에서 가중치 감소 정규화 수정&quot;, CoRR, vol. abs/1711.05101, 2017. [온라인]. 사용 가능: http://arxiv.org/abs/1711.[32] CJ Steinmetz 및 JD Reiss, &quot;WaveBeat: 시간 영역에서의 엔드투엔드 비트 및 다운비트 추적&quot;, 제151회 AES 컨벤션, 2021.
