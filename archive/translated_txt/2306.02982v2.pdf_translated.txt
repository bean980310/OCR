--- ABSTRACT ---
우리는 음성-음성 번역(S2ST) 시스템을 위한 언어 모델 기반 프레임워크인 PolyVoice를 제안합니다. 우리의 프레임워크는 번역 언어 모델과 음성 합성 언어 모델의 두 가지 언어 모델로 구성되어 있습니다. 우리는 완전히 비지도 방식으로 생성되는 이산화된 음성 단위를 사용하므로 우리의 프레임워크는 비문자 언어에 사용될 수 있습니다. 음성 합성 부분에서는 기존의 VALL-E X 방식을 채택하고 단위 기반 오디오 언어 모델을 구축합니다. 이를 통해 우리의 프레임워크는 원래 음성의 음성 특성과 말하기 스타일을 보존할 수 있습니다. 우리는 중국어-영어와 영어-스페인어 쌍에서 우리의 시스템을 검토합니다. 실험 결과에 따르면 우리 시스템은 높은 번역 품질과 오디오 품질로 음성을 생성할 수 있습니다. 음성 샘플은 https://speechtranslation.github.io/polyvoice에서 제공됩니다.
--- INTRODUCTION ---
음성 대 음성 번역(S2ST)은 자동 음성 인식(ASR), 기계 번역(MT) 및 텍스트 대 음성(TTS) 합성의 모든 어려움에 부딪히기 때문에 까다로운 작업입니다. 기존의 캐스케이드 접근 방식(Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006)과 달리 직접 접근 방식(Jia et al., 2019, 2022a)은 대기 시간이 짧고 파이프라인이 간소화된다는 장점이 있습니다. 기존의 직접 S2ST 접근 방식은 모델이 연속적인 멜 스펙트로그램 특징(Dong et al., 2022)을 예측하는지 또는 불연속적인 단위(Lee et al., 2022)를 예측하는지에 따라 추가로 분류할 수 있습니다. 단위 기반 접근 방식은 다음과 같은 여러 가지 이유로 더욱 인기를 얻었습니다. (1) 연구자들이 음향 단위를 새로운 언어로 처리하여 기존 NLP 모델링 기술을 활용할 수 있습니다. (2) 스펙트로그램을 방출하는 모델링의 어려움을 완화합니다. (3) 단위는 *균등한 기여. 진행 중. 완전히 비지도 방식으로 생성될 수 있으며 모든 비문자 언어를 다룰 수 있습니다. 일반적으로 사용되는 이산화된 음성 단위에는 의미 단위와 음향 단위의 두 가지 종류가 있습니다. 의미 단위는 일반적으로 HUBERT(Hsu et al., 2021), mHuBERT(Lee et al., 2021) 또는 w2v-BERT(Chung et al., 2021)와 같은 음성 인코더 모델에서 생성된 표현에서 파생됩니다. 이들은 음성의 음성학 및 의미적 내용을 포착합니다. 이러한 단위의 제작은 원래 음성 인코더를 훈련하기 위한 대상으로 사용되도록 개발되었지만, 최근에는 이러한 단위를 의미적 작업의 입력/출력으로 직접 사용하려는 시도가 있습니다(Meng et al.; Zhang et al.). 음향 단위는 코덱 단위라고도 합니다. 이들은 원래 제한된 대역폭에서 고품질 음성 신호를 전송하기 위해 개발되었습니다. AudioLM(Borsos et al., 2022)은 오디오 생성을 위해 언어 모델(LM)을 사용하는 선구적 작업입니다. 그들은 두 종류의 단위를 모두 사용하고 서로 다른 해상도로 여러 LM을 구축합니다. VALL-E(Wang et al., 2023)는 AudioLM 프레임워크를 더욱 확장하여 TTS에 적용합니다. 그들은 LM의 맥락 내 학습 기능이 음소 및 코덱 단위의 맥락에서 유사하게 복제될 수 있음을 성공적으로 보여줍니다. 지도 학습 프로세스를 포함해야 하는 음소 단위와 달리 의미 단위와 음향 단위는 모두 완전히 지도되지 않은 방식으로 생성될 수 있습니다. 최근 언어 모델링은 NLP에서 많은 획기적인 진전을 이루었습니다. GPT 모델(Brown et al., 2020; Ouyang et al., 2022)의 성공은 커뮤니티를 새로운 시대로 이끌고 있습니다. 현재, 인코더디코더 모델은 여전히 음성 모델링에서 지배적이며, LM 기반 방법이 막 등장하기 시작했습니다. 따라서 우리는 S2ST에서 LM 기반 방법의 성능을 조사하고자 합니다. 이 논문에서 우리는 S2ST 시스템을 위한 의미 단위 기반 프레임워크를 제안합니다. 우리의 프레임워크는 번역 LM과 음성 합성 LM의 두 개의 LM으로 구성됩니다. 번역 LM은 소스 언어의 의미 단위를 처리하고 시퀀스를 대상 언어의 의미 단위로 번역합니다. 음성 합성 부분에서는 음성 복제 기능을 위해 VALLEX 접근 방식(Zhang et al., 2023)을 채택합니다. 우리는 소스 및 타겟 의미 단위와 소스 음향 단위를 연결하고 전체 시퀀스를 프롬프트로 오디오 LM에 공급합니다. 그런 다음 오디오 LM은 타겟 음향 단위를 예측하고 이는 단위 보코더에 의해 파형으로 변환됩니다. 실험 결과에 따르면 우리 시스템은 높은 번역 품질과 오디오 품질로 음성을 생성할 수 있습니다. 우리는 다음과 같이 우리의 기여를 요약합니다. • 우리는 직접 번역을 수행하기 위해 디코더 전용 모델을 사용할 것을 제안하는 반면, 인코더디코더 모델은 이전 작업에서 지배적인 구조입니다. • 우리는 음성 합성을 위한 단위 기반 오디오 LM을 구축합니다. VALL-E X와 비교하여, 우리는 비지도 이산화 단위를 사용하고 비문어를 다룰 수 있습니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 섹션 2에서는 소개
--- RELATED WORK ---
TTS와 S2ST의 s. 자세한 내용은 다음과 같습니다.
--- METHOD ---
s는 방금 등장하기 시작했습니다. 따라서 우리는 S2ST에서 LM 기반 방법의 성능을 조사하고자 합니다. 이 논문에서 우리는 S2ST 시스템을 위한 의미 단위 기반 프레임워크를 제안합니다. 우리의 프레임워크는 두 개의 LM으로 구성됩니다. 번역 LM과 음성 합성 LM입니다. 번역 LM은 소스 언어의 의미 단위를 처리하고 시퀀스를 대상 언어의 의미 단위로 번역합니다. 음성 합성 부분에서는 음성 복제 기능을 위해 VALLEX 접근 방식(Zhang et al., 2023)을 채택합니다. 우리는 소스 및 타겟 의미 단위와 소스 음향 단위를 연결하고 전체 시퀀스를 오디오 LM에 프롬프트로 제공합니다. 그런 다음 오디오 LM은 타겟 음향 단위를 예측하고 이는 단위 보코더에 의해 파형으로 변환됩니다.
--- EXPERIMENT ---
모든 결과는 우리 시스템이 높은 번역 품질과 오디오 품질로 음성을 생성할 수 있음을 보여줍니다.음성 샘플은 https://speechtranslation.github.io/polyvoice에서 제공됩니다.소개 음성 대 음성 번역(S2ST)은 자동 음성 인식(ASR), 기계 번역(MT) 및 텍스트 대 음성(TTS) 합성의 모든 어려움에 부딪히기 때문에 어려운 작업입니다.기존의 캐스케이드 방식(Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006)과 달리 직접 방식(Jia et al., 2019, 2022a)은 대기 시간이 짧고 파이프라인이 간소화된다는 장점이 있습니다.기존의 직접 S2ST 방식은 모델이 연속적인 멜-스펙트로그램 특징(Dong et al., 2022)을 예측하는지 아니면 개별 단위(Lee et al., 2022)를 예측하는지에 따라 추가로 분류할 수 있습니다. 단위 기반 접근법은 여러 가지 이유로 더욱 인기를 얻었습니다.(1) 연구자들이 음향 단위를 새로운 언어로 처리하여 기존 NLP 모델링 기술을 활용할 수 있습니다.(2) 스펙트로그램을 방출하는 모델링의 어려움을 완화합니다.(3) 단위는 완전히 비지도 방식으로 생성될 수 있으며 모든 비문자 언어를 다룰 수 있습니다. 일반적으로 사용되는 이산화된 음성 단위에는 의미 단위와 음향 단위의 두 가지 종류가 있습니다. 의미 단위는 일반적으로 HUBERT(Hsu et al., 2021), mHuBERT(Lee et al., 2021) 또는 w2v-BERT(Chung et al., 2021)와 같은 음성 인코더 모델에서 생성된 표현에서 파생됩니다. 이들은 음성의 음성학과 의미적 내용을 포착합니다. 이러한 단위의 제작은 원래 음성 인코더를 훈련하기 위한 대상으로 사용되도록 개발되었지만 최근에는 이러한 단위를 의미적 작업의 입력/출력으로 직접 사용하려는 시도가 있습니다(Meng et al.; Zhang et al.). 음향 단위는 코덱 단위라고도 합니다. 원래는 제한된 대역폭에서 고품질 음성 신호를 전송하기 위해 개발되었습니다. AudioLM(Borsos et al., 2022)은 오디오 생성을 위해 언어 모델(LM)을 사용하는 선구적인 작업입니다. 그들은 두 종류의 단위를 모두 사용하고 서로 다른 해상도의 여러 LM을 구축합니다. VALL-E(Wang et al., 2023)는 AudioLM 프레임워크를 더욱 확장하여 TTS에 적용합니다. 그들은 LM의 맥락 내 학습 기능이 음소 및 코덱 단위의 맥락에서 유사하게 복제될 수 있음을 성공적으로 입증했습니다. 지도 학습 프로세스를 포함해야 하는 음소 단위와 달리 의미 단위와 음향 단위는 모두 완전히 비지도 방식으로 생성될 수 있습니다. 최근 언어 모델링은 NLP에서 많은 획기적인 진전을 이루었습니다. GPT 모델(Brown et al., 2020; Ouyang et al., 2022)의 성공은 커뮤니티를 새로운 시대로 이끌고 있습니다. 현재, 인코더디코더 모델은 여전히 음성 모델링에서 지배적이며, LM 기반 방법이 막 등장하기 시작했습니다. 따라서 우리는 S2ST에서 LM 기반 방법의 성능을 조사하고자 합니다. 이 논문에서 우리는 S2ST 시스템을 위한 의미 단위 기반 프레임워크를 제안합니다. 우리의 프레임워크는 두 개의 LM, 즉 번역 LM과 음성 합성 LM으로 구성됩니다. 번역 LM은 소스 언어의 의미 단위를 처리하고 시퀀스를 대상 언어의 의미 단위로 번역합니다. 음성 합성 부분에서는 음성 복제 기능을 위해 VALLEX 방식(Zhang et al., 2023)을 채택합니다. 우리는 소스 및 타겟 의미 단위와 소스 음향 단위를 연결하고 전체 시퀀스를 프롬프트로 오디오 LM에 공급합니다. 그런 다음 오디오 LM은 타겟 음향 단위를 예측하고, 이는 단위 보코더에 의해 파형으로 변환됩니다. 실험 결과에 따르면 우리 시스템은 높은 번역 품질과 오디오 품질로 음성을 생성할 수 있습니다. 우리는 다음과 같이 우리의 기여를 요약합니다. • 우리는 직접 번역을 수행하기 위해 디코더 전용 모델을 사용할 것을 제안합니다. 반면, 이전 연구에서는 인코더디코더 모델이 지배적인 구조였습니다. • 우리는 음성 합성을 위한 단위 기반 오디오 LM을 구축합니다. VALL-E X와 비교하여, 우리는 비지도 이산화 단위를 사용하고 기록되지 않은 언어를 다룰 수 있습니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 2절에서는 TTS 및 S2ST의 관련 연구를 소개합니다. 우리 방법에 대한 세부 사항은 3절에 설명되어 있습니다. 4절에서는 실험 설정을 소개합니다. 5절에서는 절제 연구를 제시합니다. 마지막으로, 마지막 섹션에서 연구를 마무리합니다. 2 관련 연구 2.1 TTS 최근 몇 년 동안 신경 텍스트-음성(TTS) 합성은 상당한 발전을 이루었고, 신경망 구조의 진보는 합성 음성의 지능을 지속적으로 향상시킵니다(Wang et al., 2017; Ren et al., 2019; Kim et al., 2021; Popov et al., 2021). 실제 세계 응용 프로그램의 요구 사항 때문에 연구자들은 제로샷 다중 화자 TTS 및 교차 언어 TTS(Jia et al., 2018; Cooper et al., 2020)에 많은 관심을 기울였습니다. 화자 검증 과제에서 화자 임베딩 훈련을 사용하는 다중 화자 TTS는 보이는 화자에 대해 유사한 음색을 생성할 수 있습니다. 그러나 보이지 않는 화자에 대한 제로샷 화자 복제는 여전히 해결되지 않은 문제입니다. 방대한 음성 데이터 코퍼스에서 훈련된 VALL-E(Wang et al., 2023)는 접두사 언어 모델링의 맥락 내 기능을 활용하여 제로샷 화자 복제에 대한 최첨단(sota) 성능을 달성합니다. 교차 언어 TTS는 대상 화자가 말하지 않는 특정 언어로 음성을 합성할 수 있는 시스템을 구축하는 것을 목표로 합니다. 화자 임베딩, 언어 임베딩, 강세 및 톤 임베딩과 같은 다양한 임베딩이 교차 언어 TTS 모델에서 활용되어 모국어/외국어의 보이는/보이지 않는 화자에 대한 고품질의 자연스럽고 알아들을 수 있는 모국어 음성을 생성합니다(Liu 및 Mak, 2019). 사전 훈련된 화자 인코더에서 추출한 고정 화자 임베딩과 비교하여, 화자 분류를 동시에 훈련하여 교차 언어 화자 유사성을 향상시키는 다중 작업 학습 프레임워크가 제안되었습니다(Yang 및 He, 2022). VALL-E의 접두사 언어 모델링을 기반으로, VALL-E X(Zhang et al., 2023)는 이 방법을 이중 언어 중국어-영어 데이터로 교차 언어 TTS 훈련에 적용합니다. 소스 음성, 소스 및 대상 언어 텍스트 프롬프트가 제공되면 VALL-E X는 대상 언어의 음성의 코덱 토큰을 예측합니다. 모델은 맥락 내 기능을 통해 음향 환경, 소스 언어 화자 및 감정과 같은 소스 음성 프롬프트의 음향 정보를 유지하는 것을 목표로 합니다.2.2 S2ST 음성 대 음성 번역(Lavie et al., 1997; Baldridge, 2004; Nakamura et al., 2006)은 소스 언어 음성에서 타겟 언어 음성을 생성할 수 있는 모델을 개발하는 것을 목표로 합니다.순진한 시스템은 전통적으로 자동 음성 인식(ASR) 모델, 기계 번역(MT) 모델 및 텍스트 대 음성 합성(TTS) 모델을 통해 입력을 순차적으로 처리하는 파이프라인(Nakamura et al., 2006)을 사용합니다.최근에는 엔드투엔드 패러다임(Jia et al., 2019)이 S2ST 분야에서 인기를 얻었는데, 이는 단일 모델이 앞서 언급한 작업 중 하나 이상을 수행할 수 있으므로 결과적으로 오류 전파 및 지연이 줄어들기 때문입니다. 다양한 기술 중에서 텍스트 데이터를 기반으로 한 보조 감독은 훈련 중에 특히 효과적이었습니다(Jia et al., 2019; Kano et al., 2021). 그러나 이 접근 방식은 문자가 아닌 언어를 다룰 때는 실행 불가능합니다. 이 과제를 해결하기 위해 음성에서 추출한 개별 단위(Hsu et al., 2021)를 사용하여 대상 텍스트를 대체한 다음 음성으로 합성할 수 있습니다(Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022). 대규모 연구에서는 다양한 음성 처리 작업에서 강력한 성능이 입증되었습니다(Nguyen et al., Source Speech S2UT Extractor + U-XLM Merged Target Units U2S ☐ Source Semantic Unit Target Semantic Unit Source Acoustic Unit Target Acoustic Unit U-XLM Language Model ↑ 1 1 1 ↑ ↑ 병합되지 않은 대상 단위 Dur U-SLM Target Speech ག་ soundstream decoder Target Speech U-SLM Language Model ↑ ↑ ↑ ^ U LTranslate [src lang] units ☐☐☐ to [tgt lang] units Merged Source Units MERGE OOOO Source Units Dur Semantic unit extractor soundstream encoding Source Speech repeat Language Model12(D) 2Source Speech Figure 1: Overview of Poly Voice. 이 프레임워크는 두 개의 LM 기반 구성 요소로 구성됩니다. 번역을 위한 S2UT 프런트 엔드와 합성을 위한 U2S 백엔드. 2022). 음성 대 음성 번역에 대한 현재 연구는 주로 번역 품질에 중점을 두고 있으며, 자동 평가 지표(BLEU 등) 또는 자연스러움에 대한 인간의 평가에서 눈에 띄는 개선이 관찰되었습니다. 그러나 실용적인 시스템을 개발하는 데는 두 가지 지속적인 과제가 남아 있습니다. 첫째, 이러한 시스템은 주로 소규모 벤치마크에서 개발 및 평가되는 반면, 실제 시나리오에는 종종 ASR, MT 및 S2T 데이터를 포함한 대량의 레이블이 지정된 데이터가 포함됩니다. 자원이 부족하거나 기록되지 않은 언어의 경우에도 레이블이 지정되지 않은 음성 또는 텍스트를 활용하면 귀중한 정보를 제공할 수 있습니다(Lee et al., 2022). 따라서 다양한 데이터 유형을 공동으로 활용하는 통합 모델을 개발하는 것은 아직 달성되지 않은 중요한 연구 목표입니다. 둘째, 엄격한 요구 사항은 아니지만 번역 중에 소스 화자의 스타일을 유지하는 것은 사용자 경험을 개선하는 데 중요한 측면입니다(Zhang et al., 2023). 그러나 개별 화자의 고유한 특성을 포착하는 것은 어려운 작업입니다. 화자 임베딩(Jia et al., 2019) 및 다중 화자 TTS 시스템(Jia et al., 2018)과 같은 현재의 접근 방식은 이 방향으로 어느 정도 진전을 이루었지만, 여전히 실제적인 요구 사항에는 미치지 못합니다. 위의 고려 사항에 대해 서면 및 비서면 언어 설정 모두에 적용할 수 있는 다재다능한 프레임워크인 Poly Voice를 제시합니다. Poly Voice는 언어 모델 기반 프레임워크 내의 다양한 데이터 소스를 효과적으로 활용하고 합성하는 동안 소스 화자의 스타일을 보존하여 실제 시스템에서 엄청난 잠재력을 가지고 있습니다. 3 방법 서면 및 비서면 언어를 모두 처리할 수 있는 음성 대 음성 번역을 위한 새로운 언어 모델 기반 프레임워크인 PolyVoice를 소개합니다. 제안된 프레임워크는 HuBERT(Hsu et al., 2021)와 같은 자체 감독 학습 방법을 통해 얻은 개별 단위를 소스 음성과 대상 음성 사이의 중간 표현으로 활용합니다. 두 부분으로 구성됩니다. 음성 단위 변환(S2UT) 프런트엔드는 소스 언어의 음성을 대상 언어의 단위로 변환하고, 단위 음성(U2S) 백엔드는 소스 화자의 스타일을 보존하면서 번역된 음성을 합성합니다. 그림 1은 ASR을 제공합니다. [lang] 데이터:<unit, text> &quot; &quot;Prompt1: [lang] 단위 {unit}을 [lang] 텍스트: {text}로 번역합니다. Prompt2: [lang] 텍스트 &quot; {text} &quot;을 [lang] 단위 &quot; {unit}로 번역합니다. MT: [src lang] → [tgt lang] 데이터:<src_text, tgt_text> 프롬프트: [src lang] 텍스트 &quot;{src_text}&quot;를 [tgt lang] 텍스트로 번역: S2ST: [src lang] → [tgt lang] 데이터:<src_unit, tgt_unit, src_text, tgt_text> &quot; &quot; &quot; &quot; {tgt_text}&quot; &quot; {tgt_unit} &quot; &quot; Prompt1: [src lang] 단위 &quot; {src_unit} &quot;를 [tgt lang] 단위로 번역: Prompt2: [src lang] 단위 &quot; {src_unit} &quot;를 [src lang] 텍스트로 번역: &quot;{src_text} Prompt3: [src lang] 단위 &quot; {src_unit} &quot;를 [tgt lang] 텍스트로 번역: {tgt_text} Prompt4: [src lang] 텍스트 &quot; {src_text}&quot;를 [tgt lang] 단위로 번역: &quot;{tgt_unit} Prompt5: [tgt lang] 텍스트 &quot; {tgt_text}&quot;를 [tgt lang] 단위로 번역: &quot;{tgt_unit} &quot; &quot; &quot; 표 1: 다양한 프롬프트에 따른 U-XLM 모델의 데이터 구성. 접근 방식 개요. 3.1 음성-단위 번역 (S2UT) 자기 지도 학습을 통해 얻은 이산 단위를 사용함으로써 연속적인 음성 표현에서 의미적으로 무관한 정보를 제거하고 NLP 패러다임에서 효과적인 학습을 용이하게 합니다. 그리고 S2UT는 언어 모델을 활용하여 단위 기반 교차 언어 생성을 학습합니다. 의미 단위 추출기 S2UT는 먼저 의미 단위 추출기를 통해 원시 음성을 처리합니다. 여기서 우리는 HuBERT를 채택하는데, 이는 먼저 합성곱과 Transformer 계층을 스택하여 20ms 프레임마다 연속적인 표현으로 음성을 인코딩한 다음 k-평균 클러스터링을 사용하여 표현을 클러스터 인덱스 Z=2_1,,z_T 세트로 이산화합니다. T는 프레임 수이고 z_t = [K]이며, 여기서 K는 클러스터 중심 수입니다. 그런 다음 중복된 단위의 연속적인 시퀀스를 병합하여 시퀀스 길이를 압축하여 계산 비용을 줄이고 수렴을 돕습니다. 단위 기반 교차 언어 모델(UXLM) 지난 몇 년 동안 인코더-디코더 아키텍처는 시퀀스-투-시퀀스 모델링을 위한 가장 두드러진 패러다임으로 부상했습니다(Sutskever et al., 2014). 그러나 GPT 패밀리의 최근 발전(Brown et al., 2020; Ouyang et al., 2022)은 디코더 전용 아키텍처에 의한 언어 모델링의 강력한 기능을 보여주었습니다. 이는 생성 언어 모델링을 통해 소스 음성의 단위에서 타겟 언어의 의미 단위를 예측하는 단위 기반 교차 언어 모델을 개발하도록 영감을 주었습니다. 우리는 소스 언어와 타겟 언어의 음성 단위로 구성된 훈련 샘플을 다음과 같이 표시합니다.<src_unit, tgt_unit> 인코더-디코더 아키텍처에서 인코더는 소스 단위를 입력으로 받고 디코더는 대상 단위를 예측합니다. 교차 언어적 단위 생성을 가능하게 하기 위해 간단한 프롬프트를 사용하여 단위 쌍에서 자연어의 훈련 샘플을 구성할 수 있습니다(예: [src lang] 단위 &quot;{src_unit} &quot;을 [tgt lang] 단위: &quot;{tgt_unit}&quot;로 번역). 훈련 위의 U-XLM 모델을 훈련하기 위해 경쟁적 성과를 위해 대규모 데이터가 필요합니다. 지도 학습 데이터인 교차 언어적 단위 쌍은 실제 시나리오에서는 부족합니다. 보조 모델을 사용하여 TTS 모델을 사용하여 대상 음성을 합성하는 것과 같이 가상 레이블을 생성할 수 있지만 지도 학습 데이터의 직접 훈련이 예상됩니다. 데이터 부족의 과제를 더욱 해결하기 위해 이전 연구에서는 멀티태스크 학습을 통해 인코더-디코더 아키텍처에 추가 손실 함수를 도입했습니다(Jia et al., 2022a; Lee et al., 2022). 언어 모델링 덕분에 ASR 및 MT 데이터와 같은 다양한 데이터 소스를 사용할 수 있는 더 간단한 방식을 채택했습니다. 표 1에서 볼 수 있듯이, 프롬프트를 약간 수정하여 다양한 유형의 데이터 소스에 대한 학습 샘플을 구성한 다음 매개변수 공유를 통해 모델을 학습하여 보조 목표의 설계를 간소화합니다. 레이블이 지정되지 않은 텍스트와 음성도 이 접근 방식에서 직접 사용할 수 있습니다. 이런 방식으로 모델은 음성 단위와 텍스트에서 표현 공간의 정렬을 암묵적으로 개선합니다. U-XLM은 서면 및 비서면 언어 설정을 모두 처리하는 기능, 다국어 모델링 기능, 대량의 레이블이 지정되지 않은 데이터를 활용하여 제로샷 예측 가능성을 포함한 여러 가지 장점을 제공합니다. 이러한 기능은 U-XLM을 음성 대 음성 번역 연구를 발전시키는 데 유망한 프레임워크로 만듭니다. 3.2 단위 대 음성 합성(U2S) 단위 대 음성 언어 모델(U-SLM) 그림 1에서 볼 수 있듯이 U-SLM은 U-XLM에서 예측한 의미 단위를 처리하고 소스 화자의 말하기 스타일을 내장한 코덱 단위를 생성합니다. VALL-E X와 마찬가지로 U-SLM에는 자기 회귀 모델과 비자기 회귀 모델이 포함됩니다. 음소 대신, 이산화된 의미 단위가 우리의 경우에 사용됩니다. 단위 추출기는 완전히 비지도 방식으로 학습될 수 있으며, 이는 비문자 언어에 적합합니다. SoundStream SoundStream 코덱 우리는 음향 토큰의 임베딩을 계산하기 위해 신경 오디오 코덱인 (Zeghidour et al., 2021)을 사용합니다. 우리는 6개의 벡터 양자화기와 1024개의 기호로 구성된 어휘를 가진 잔여 벡터 양자화기(RVQ)를 가진 SoundStream을 다시 학습합니다. 우리의 구성에서 음향 토큰은 24kHz의 입력 파형에 대해 80Hz에서 생성됩니다. 이는 샘플링 속도가 24000/ 80 = 300배 감소한 것입니다. U2S 모델이 SoundStream 코덱으로 표현된 음향 토큰을 예측한 후, SoundStream의 디코더는 이를 파형으로 재구성합니다. 지속 시간 모델 우리는 경험적으로 이산화된 단위의 지속 시간 정보가 합성된 음성의 안정성에 매우 중요하다는 것을 발견했습니다. 우리의 작업에서는 LM을 사용하여 지속 시간을 예측합니다.그림 1에서 볼 수 있듯이 병합된 소스 의미 단위 시퀀스, 병합된 타겟 의미 단위 시퀀스 및 소스 지속 시간 값(D) 시퀀스가 연결되어 지속 시간 LM에 프롬프트로 제공됩니다.그러면 지속 시간 LM이 지속 시간 값 시퀀스를 예측하고 각 타겟 의미 단위는 그에 따라 반복됩니다.4 실험 우리는 두 가지 음성 대 음성 벤치마크 데이터 세트인 EMIME(Wester 및 Liang, 2011)와 CVSS(Jia et al., 2022b)에서 방법을 평가합니다.그런 다음 두 구성 요소의 개별 결과를 보여줍니다.Туре 데이터 세트 크기 LibriLight(En) 60K 시간 ASR 사내(Zh) 60K 시간 MT 사내 44M sent GigaSpeech 10K 시간 S2S WenetSpeech 10K 시간 표 2: U-XLM 모델의 학습 데이터. 4.1 데이터 세트 및 전처리 4.1.S2UT 의미 토큰 U-XLM은 HUBERT(Hsu et al., 2021) 모델에 의해 오디오에서 추출된 교차 언어 단위 데이터에 의해 훈련됩니다. 중국어 오디오의 경우 WenetSpeech 중국어 음성 1을 기반으로 하는 오픈 소스 모델을 활용합니다. 영어와 스페인어 오디오의 경우 오픈 소스 다국어 모델(영어, 스페인어, 프랑스어) 2을 사용합니다. 두 모델의 k-평균 알고리즘의 클러스터 중심은 각각 500과 1,000입니다. 어휘 어휘 부족 문제를 해결하고 언어 간 매개변수 공유를 가능하게 하기 위해 각 문자를 바이트 크기의 조각으로 분해하는 바이트 수준 하위 단어 단위 3을 활용하여 56,407(1,500개의 클러스터 중심 포함)의 어휘 크기를 달성합니다. 데이터 세트 쌍대 음성-대-음성(S2S) 데이터가 희소하다는 점을 고려하여 사내 MT 및 TTS 시스템을 활용하여 ASR 데이터에서 의사 데이터를 합성합니다. 또한 대규모 ASR 및 MT 데이터와 같이 다양한 유형의 데이터 리소스가 UXLM 모델을 더 잘 학습합니다. 자세한 통계는 표 2에 나와 있습니다. S2S 데이터는 WenetSpeech(Zhang et al., 2022) 및 GigaSpeech(Chen et al., 2021)에서 가져왔습니다. WenetSpeech는 YouTube에서 수집한 10,000시간 이상의 음성 데이터가 있는 중국어 ASR 데이터 세트입니다. 그리고 오디오북, 팟캐스트, YouTube에서 수집한 영어 ASR 데이터 세트인 GigaSpeech(Chen et al., 2021)의 10,000시간 하위 세트를 활용합니다. 그런 다음 다양한 유형의 데이터 세트에 대한 특정 프롬프트를 사용하여 교육 데이터를 확장합니다. 저희는 LibriLight(Kahn et al., 2020)와 자체 &#39;https://github.com/TencentGameMate/chinese_speech_pretrain 2https://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_speech/docs/textless_s2st_real_data.md https://github.com/huggingface/tokenizers ASV ↑ ASR-BLEU ↑ Naturalness ↑ tgt 대 src hyp 대 src hyp 대 tgt Cascade(VALL-E X 논문) 0.0.27.3.+ oracle 대상 텍스트 사용 0.0.80.3.0.VALL-E X(VALL-E X 논문) 0.0.30.3.+ oracle 대상 텍스트 사용 0.0.86.3.S2UT 0.0.29.3.Poly Voice(S2UT + U2S) 0.0.0.29.4.+ w/ oracle target semantic unit 0.0.76.3.표 3: 중국어-영어 EMIME 데이터 세트에 대한 S2ST 결과. ASR 데이터 세트. LibriLight는 약 60,000시간 분량의 음성을 포함하는 레이블이 없는 영어 음성 데이터 세트입니다. LibriLight는 긴 오디오가 많기 때문에 음성 활성 감지(VAD) 방법과 사내 ASR 시스템을 기반으로 오디오를 분할하고 인식하여 0.5~25초 범위의 오디오 길이를 생성하고 평균 길이는 7초입니다. 사내 ASR 데이터 세트는 60,000시간 분량의 음성을 포함하는 중국어 ASR 데이터 세트입니다. 또한 44M 문장 쌍으로 구성된 사내 중국어-영어 MT 데이터 세트를 사용합니다. 4.1.2 U2S U-SLM은 대규모 오픈 소스 이중 언어 음성 데이터인 WenetSpeech(Zhang et al., 2022) 및 LibriLight(Kahn et al., 2020)에서 학습되었습니다. Librilight는 UXLM과 동일한 방식으로 처리됩니다. WenetSpeech는 원래 데이터 길이를 변경하지 않고 오디오 길이 범위는 0.5~20초이며 평균 길이는 2.5초입니다. 또한 추가로 250시간 내부 중국어 TTS 데이터와 400시간 내부 영어 TTS 데이터를 사용했습니다. 4.2 평가 시스템의 성능을 측정하기 위해 번역 품질과 음성 품질을 모두 평가합니다. 번역 품질 이전 설정에 따라 사내 ASR 시스템으로 음성 출력을 인식하여 S2ST 결과에 대한 BLEU 점수(ASRBLEU)를 계산합니다. 음성 품질 음성 품질은 여러 가지 메트릭을 통해 평가됩니다. 음성 복제의 기능은 화자 유사도(ASVScore)로 측정되며, 이는 ASV 모델 https://github.com/Sanyuan-Chen/UniSpeech/tree/t-sch en/asv_eval/downstreams/speaker_verification#example-을 통해 계산되어 합성된 음성이 실제 음성과 같은 화자인지 여부를 판별합니다. 음성 출력의 자연스러움은 NISQA 5를 사용하는 자동 메트릭으로 평가합니다. 그리고 발음 정확도는 hubert-large 6에 기반한 ASR 모델과 함께 WER 점수(ASR-WER)를 사용하여 평가합니다. 4.3 모델 설정 4.3.1 S2UT S2UT 프런트 엔드에서 U-XLM의 모델 아키텍처는 은닉 크기가 1600인 48개 레이어, 피드포워드 네트워크(FFN) 크기가 6400인 25개 어텐션 헤드로 구성된 단방향 Transformer 디코더입니다. 총 매개변수는 1.6B입니다.U-XLM은 500k 단계에 대해 GPU당 3072개 토큰의 배치 크기로 8/32 NVIDIA TESLA A100 80GB GPU에서 학습됩니다.4.3.2 U2S U2S 백엔드에서 U-SLM은 변압기 층으로 구성됩니다.이러한 각 층은 16개의 어텐션 헤드, 1024의 어텐션 차원, 자기 회귀(AR) 모델과 비자기 회귀(NAR) 모델 모두에서 4096의 FFN 차원으로 구성됩니다.우리는 800k 단계에 대해 GPU당 8개의 발언의 배치 크기로 NVIDIA TESLA A100 80GB GPU를 사용하여 모델을 학습합니다.모든 단계에 대한 학습에는 약 5일이 걸립니다.4.4 결과 및 분석 4.4.1 S2ST 결과 표 3은 S2ST에 대한 방법의 전반적인 성능을 요약한 것입니다. 우리는 가장 유사한 작업인 VALL-E X와 직접 비교할 수 있도록 EMIME 데이터 세트에 대한 실험을 수행합니다. 캐스케이드 Shttps://github.com/gabrielmittag/NISQA &quot;https://huggingface.co/facebook/hubert-large-ls960-ft CVSS Ground-truth ASV ↑ BLEU ↑ Naturalness↑ 0.89.3.Poly Voice 0.18.3.+ w/ oracle target unit 0.70.3.표 4: 영어-스페인어 CVSS 데이터 세트에 대한 결과. 우리는 텍스트 정보 없이 GigaSpeech에서 확장된 쌍을 이룬 음성 대 음성 데이터 세트로 모델을 훈련합니다. BLEU는 ASR-BLEU를 의미하고, target unit은 oracle 스페인어 단위를 의미합니다. 시스템은 S2ST를 ASR 모델, MT 모델, 다중 화자 YourTTS 모델을 순차적으로 실행하는 파이프라인으로 처리합니다. 합성 프로세스 동안 화자 정보는 화자 임베딩을 사용하여 통합됩니다. 우리는 먼저 다음을 평가합니다. ASV 점수를 사용하여 출력 음성에서 소스 화자의 음성을 보존하는 기능. 소스 음성, 타겟 음성 및 합성 음성 간의 화자 유사도를 계산합니다. 음성이 단위 기반 보코더에 의해 합성되는 U-XLM만 실행합니다. 화자 특성에 대한 명시적 모델링이 부족하기 때문에 특히 낮은 ASV 점수가 생성됩니다. 컨텍스트 내 학습을 채택한 VALL-E X 및 Poly Voice 시스템은 모두 화자 임베딩보다 우수한 성능을 보여줍니다. 특히, 우리의 방법은 기준 진실 대상 정보를 사용할 수 있을 때 더 나은 음성 복제 기능을 보여줍니다. Poly Voice는 약간 저하된 번역 품질(ASR-BLEU)을 달성하지만 VALL-E X와 비교하여 음성 품질(자연스러움)이 현저히 향상되었습니다. 기준 진실 대상 정보를 입력으로 사용할 때 PolyVoice는 약 10 BLEU 포인트의 큰 격차로 VALLEX보다 열등하지만 자연스러움은 상당히 향상됩니다. 의미 단위는 비지도 학습을 통해 음성에서 추출되며 이는 필연적으로 오류를 발생시킵니다. 단위는 &quot;의미적&quot; 토큰으로 간주되지만 여전히 일부 음향 정보를 보존합니다. 따라서 단위 기반 모델링은 음성 품질은 향상되지만 번역 품질은 저하됩니다. 반면 텍스트에서 얻은 음소는 의미적 정확성을 보장하지만 음향 정보를 잃습니다. 따라서 현재 성능이 약간 저하되더라도 단위가 더 많은 잠재력을 가지고 있다고 믿습니다. 그리고 향후 작업은 의미 정보 추출을 강화하여 번역 품질을 개선하는 데 집중할 수 있습니다. 흥미롭게도 PolyVoice는 예측된 단위를 사용하여 더 나은 자연스러운 https://github.com/facebookresearch/fairseq/blob/main/ examples/speech_to_speech/docs/textless_s2st_real_data.md Arch ASR-BLEU 인코더-디코더 16.+ w/ U2S 18. 디코더 전용 20.+ w/ U2S 22. 표 5: 다양한 아키텍처의 성능. U-XLM은 대규모 단위 데이터에 대한 음성 분포를 학습하고 보다 자연스러운 단위 시퀀스를 생성하는 경향이 있습니다. 그러나 이는 번역의 정확성을 방해할 수 있습니다. 이 문제는 나중에 살펴보겠습니다. 4.4.2 비문어 시나리오 소스가 서면 언어이고 대상이 비문어인 경우 제안된 프레임워크를 살펴봅니다. 저희 설정에서는 스페인어 텍스트 필사본을 사용하지 않고 영어→스페인어 S2ST 시스템을 학습하고 평가합니다. 표 4에 결과가 요약되어 있습니다. ASR-BLEU(18.3)는 저희 시스템에서 생성한 스페인어 음성이 의미적으로 이해 가능하다는 것을 나타냅니다. 이는 비문어에 대한 저희 S2ST 시스템의 능력을 보여줍니다. 5 Ablation 연구 5.1 디코더 전용 대 인코더-디코더 자연어 처리 분야의 실증적 연구에 따르면 디코더 전용 접근 방식의 모든 잠재력은 대규모 모델 크기와 광범위한 데이터 세트를 사용하여 실현할 수 있음이 밝혀졌습니다. S2ST에 언어 모델을 적용하는 것을 탐구하는 선구자로서, 우리는 표 5에서 두 아키텍처의 공정한 비교를 제시합니다. 두 모델은 동일한 학습 데이터로 학습되었습니다. 흥미롭게도, 디코더 전용 모델은 작업 S2ST(BLEU ↑) ASR(CER↓) ST(BLEU ↑) MT(BLEU ↑) TTS(WER ↓) S2S + MTL 4.30.33.6.22.29에 비해 3.9 BLEU 포인트의 현저한 개선을 보였습니다.표 6: EMIME 데이터 세트에 대한 여러 작업의 성능. 각 작업에 대한 설명은 다음과 같습니다. S2ST: 중국어 음성에서 영어 음성으로; ASR: 중국어 음성에서 중국어 텍스트로; ST: 중국어 음성에서 영어 텍스트로; MT: 중국어 텍스트에서 영어 텍스트로; TTS: 영어 텍스트에서 영어 음성으로. 방법 WER ASV ↑ Naturalness ↑ VALL-E X(논문) 4.0.3.U2S 6.0.3.31.0.3.4.0.3.+ w/o semantic2dur + w/mHUBERT_zh_en 표 7: 음성 합성기 평가. 인코더-디코더 대응물. 보코더 대신 U2S로 음성을 합성하면 성능 격차가 줄어들어 U2S 백엔드의 견고성이 강조됩니다. 5. 멀티태스크 훈련 3장에서 논의했듯이 언어 모델링은 특정 프롬프트를 활용하여 다양한 데이터 소스에 대한 직접 훈련이 가능합니다. 이런 식으로 우리는 추가적인 대규모 ASR 및 MT 데이터를 결합하여 우리 방법의 잠재력을 완전히 탐색합니다. 표 6에서 보듯이, U-XLM은 확장된 데이터 설정에서 관련된 여러 작업(S2ST, ASR, ST, MT 및 TTS 포함)에 대해 유망한 성능을 달성하여 디코더 전용 아키텍처에서 일반 모델링의 기능을 검증합니다. 기존 패러다임에서는 다중 작업 학습을 결합하기 위해 복잡한 방식을 설계해야 했지만 언어 모델링은 프롬프트만 수정하여 학습 데이터를 구성합니다. 5.3 의미 단위 및 지속 시간 모델 표 7은 다양한 음성 합성기의 재합성 성능을 보여줍니다. TTS는 ASV와 자연스러움에서 모두 더 나은 성능을 얻습니다. WER의 증가는 음소와 비지도 단위가 전달하는 의미 정보의 양의 차이에 기인합니다. 이는 mHUBERT 및 AudioLM에서 보고된 관찰과 일치합니다. U2S에서 지속 시간 모델을 제거하면 WER이 극적으로 증가합니다. 우리의 추측은 다음과 같습니다. 우리는 다음 코드를 사용하여 인코더-디코더 아키텍처를 훈련합니다: https://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_speech/docs/direct_s2st_discrete_units.md. 단위 자체는 음소만큼 많은 지속 시간 정보를 포함하지 않습니다. 따라서 비지도 단위를 사용할 때 지속 시간 모델이 필수적입니다. 우리는 중국어와 영어 데이터를 조합하여 우리만의 다국어 HuBERT 모델(mHUBERT_zh_en)을 추가로 훈련합니다. 모델 크기는 (Hsu et al., 2021)의 HUBERT-large 모델과 동일합니다. 우리는 mHuBERT_zh_en에서 생성된 의미 단위를 사용하면 WER이 향상되는 것을 발견했습니다. 따라서 더 큰 모델이 더 나은 의미 단위를 생성할 수 있다고 믿습니다. 우리는 영어-&gt;스페인어 실험을 실행하기 위해 (Lee et al., 2021)의 mHuBERT가 필요하기 때문에 S2ST 실험에서 mHuBERT_zh_en을 사용하지 않습니다. 전체 S2ST에 mHuBERT_zh_en을 사용하는 이점은 향후 작업으로 남겨둡니다. 6
--- CONCLUSION ---
및 향후 작업 이 논문에서 우리는 S2ST를 위한 의미 단위 기반 프레임워크를 제안합니다. 우리의 프레임워크는 두 개의 LM으로 구성됩니다. 번역 LM(U-XLM)과 음성 합성 LM(U-SLM)입니다. 우리는 우리의 단위 기반 S2ST 시스템이 ASR-BLEU, ASV 및 자연스러움 측면에서 기존 시스템보다 성능이 우수함을 보여줍니다. 나아가, 우리는 스페인어 텍스트 필사본을 사용하지 않고도 비문자 언어 시나리오에서 시스템의 능력을 보여줍니다. 우리 시스템 성능은 의미 단위의 품질과 높은 관련이 있으므로 향후 작업에서는 더 나은 개별 단위 세트를 생성하는 방법을 조사할 것입니다. 또한 훨씬 더 큰 모델을 사용하여 성능을 더욱 개선할 수 있는 방법을 조사할 계획입니다. 참고문헌 Jason Baldridge. 2004. Verbmobil: Foundations of Speech-to-Speech Translation, 저자: wolfgang wahlster(편집자). springer, 2000. ISBN 3-54067783-6. 가격 £44.50(양장본). xii+679페이지. 자연어학, 10(2):200–204. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour. 2022. Audiolm: 오디오 생성을 위한 언어 모델링 접근 방식. arXiv 사전 인쇄본 arXiv:2209.03143. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You 및 Zhiyong Yan. 2021. Gigaspeech: 10,000시간 분량의 오디오가 녹음된 진화하는 다중 도메인 ASR 코퍼스입니다. Interspeech 2021, 국제 음성 커뮤니케이션 협회 제22차 연례 컨퍼런스, 체코 브르노, 2021년 8월 30일~9월, 페이지 3670-3674. ISCA. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021. W2v-bert: 자기 지도 음성 사전 학습을 위한 대조 학습과 마스크 언어 모델링 결합. 2021 IEEE 자동 음성 인식 및 이해 워크숍(ASRU), 244-250쪽. IEEE. Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, and Junichi Yamagishi. 2020. 최첨단 신경 스피커 임베딩을 사용한 제로샷 다중 스피커 텍스트 음성 변환. ICASSP 2020-2020 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP), 6184-6188쪽. IEEE. Qianqian Dong, Fengpeng Yue, Tom Ko, Mingxuan Wang, Qibing Bai, Yu Zhang. 2022. 의사 레이블이 지정된 데이터를 활용하여 직접 음성 대 음성 번역 개선. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. 2021. Hubert: 숨겨진 단위의 마스크 예측을 통한 자체 감독 음성 표현 학습. IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 29:3451-3460. Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz. 2022a. Translatotron 2: 음성 보존을 통한 고품질 직접 음성 대 음성 번역. 기계 학습 국제 컨퍼런스, 10120-10134페이지. PMLR. Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, Heiga Zen. 2022b. Cvss 코퍼스와 대규모 다국어 음성 대 음성 번역. arXiv 사전 인쇄본 arXiv:2201.03713. Ye Jia, Ron J. Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, Yonghui Wu. 2019. 시퀀스 대 시퀀스 모델을 사용한 직접 음성 대 음성 번역. Interspeech 2019, 국제 음성 커뮤니케이션 협회 제20차 연례 컨퍼런스, 오스트리아 그라츠, 2019년 9월 15-19일, 1123-1127쪽. ISCA. Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al. 2018. 스피커 검증에서 다중 스피커 텍스트-음성 합성으로의 전이 학습. 신경 정보 처리 시스템의 발전, 31. Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, Emmanuel Dupoux. 2020. Libri-light: 제한적 또는 전혀 감독되지 않는 ASR의 벤치마크. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, ICASSP 2020, 스페인 바르셀로나, 2020년 5월 4-8일, 76697673페이지. IEEE. Takatomo Kano, Sakriani Sakti, Satoshi Nakamura. 2021. 트랜스코더를 사용한 트랜스포머 기반 직접 음성-음성 변환. IEEE Spoken Language Technology Workshop, SLT 2021, 중국 선전, 2021년 1월 19-22일, 958-965쪽. IEEE. 김재현, 공정일, 손주희. 2021. 엔드투엔드 텍스트-음성을 위한 적대적 학습을 사용한 조건부 변형 자동 인코더. 기계 학습 국제 컨퍼런스, 5530-5540쪽. PMLR. 알론 라비, 알렉스 와이벨, 로리 S. 레빈, 마이클 핀케, 도나 게이츠, 마르살 가발다, 토르스텐 제펜펠트, 푸밍 잔. 1997. 야누시이: 여러 언어로 된 음성-음성 변환. 1997년 IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP &#39;97, 독일 뮌헨, 1997년 4월 21-24일, 99-102쪽. IEEE 컴퓨터 학회. Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. 2022. 이산 단위를 사용한 직접 음성 대 음성 변환. 60회 연례 총회 회의록(제1권: 장문 논문), ACL 2022, 아일랜드 더블린, 2022년 5월 22-27일, 3327-3339쪽. 전산 언어학 협회. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. 2021. 실제 데이터에 대한 텍스트 없는 음성-음성 변환. arXiv 사전 인쇄본 arXiv:2112.08352. Zhaoyu Liu 및 Brian Mak. 2019. 보이지 않는 화자에 대한 병렬 코퍼스를 사용하지 않고 음성 복제를 위한 교차 언어 다중 화자 텍스트-음성 합성. arXiv 사전 인쇄본 arXiv:1911.11601. Chutong Meng, Junyi Ao, Tom Ko, Mingxuan Wang 및 Haizhou Li. Cobert: 코드 표현 학습을 통한 자체 감독 음성 표현 학습. Interspeech 2023에서. 나카무라 사토시, 콘스탄틴 마르코프, 나카이와 히로미, 키쿠이 겐이치로, 카와이 히사시, 지츠히로 타카토시, 장진송, 야마모토 히로후미, 스미타 에이이치로, 야마모토 세이이치. 2006. ATR 다국어 음성대화 번역 시스템. IEEE 트랜스. 음성 오디오 프로세스., 14(2):365–376. Tu Anh Nguyen, Benoît Sagot, Emmanuel Dupoux. 2022. 음성 언어 모델링에 개별 단위가 필요한가요? IEEE J. 셀. 맨 위. 신호 프로세스., 16(6):1415–1423. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 학습. 신경 정보 처리 시스템의 발전, 35:27730-27744. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov. 2021. Grad-tts: 텍스트 음성 변환을 위한 확산 확률적 모델. 기계 학습 국제 컨퍼런스, 8599-8608페이지. PMLR. Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu. 2019. Fastspeech: 빠르고 강력하며 제어 가능한 텍스트 음성 변환. 신경 정보 처리 시스템의 발전, 32. Ilya Sutskever, Oriol Vinyals, Quoc V. Le. 2014. 신경망을 이용한 시퀀스 대 시퀀스 학습. 신경 정보 처리 시스템의 발전 27: 신경 정보 처리 시스템 연례 컨퍼런스 2014, 2014년 12월 8일-13일, 캐나다 퀘벡주 몬트리올, 3104-3112쪽. Andros Tjandra, Sakriani Sakti, Satoshi Nakamura. 2019. 전사되지 않은 알려지지 않은 언어 간의 음성 대 음성 변환. IEEE 자동 음성 인식 및 이해 워크숍, ASRU 2019, 싱가포르, 2019년 12월 14일-18일, 593-600쪽. IEEE. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li 등 2023. 신경 코덱 언어 모델은 음성 합성기에 대한 제로샷 텍스트입니다. arXiv 사전 인쇄 arXiv:2301.02111. Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark 및 Rif A. Saurous. 2017. Tacotron: 엔드투엔드 음성 합성을 향하여. Proc에서 Interspeech 2017, 페이지 40064010. Mirjam Wester 및 Hui Liang. 2011. emime 중국어 이중 언어 데이터베이스. 기술 보고서, 에든버러 대학교. Jingzhou Yang과 Lei He. 2022. 멀티태스크 학습과 화자 분류기 공동 훈련을 사용한 언어 간 텍스트 음성 변환. arXiv 사전 인쇄본 arXiv:2201.08124. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, Marco Tagliasacchi. 2021. Soundstream: 엔드투엔드 신경 오디오 코덱. IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 30:495–507. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, Zhendong Peng. 2022. WENETSPEECH: 음성 인식을 위한 10,000시간 이상의 다중 도메인 중국어 코퍼스. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, ICASSP 2022, 가상 및 싱가포르, 2022년 5월 23-27일, 6182-6186페이지. IEEE. Chen Zhang, Xu Tan, Yi Ren, Tao Qin, Kejun Zhang, Tie-Yan Liu. 2021. Uwspeech: 비문자 언어에 대한 음성 대 음성 번역. 제35회 AAAI 인공 지능 컨퍼런스, AAAI 2021, 제33회 인공 지능 혁신적 응용 컨퍼런스, IAAI 2021, 제11회 인공 지능 교육 진전 심포지엄, EAAI 2021, 가상 이벤트, 2021년 2월 2-9일, 14319-14327페이지. AAAI 출판사. Dong Zhang, Rong Ye, Tom Ko, Wang Mingxuan 및 Zhou Yaqian. 더빙(Dub): 음성 번역을 위한 개별 단위 역번역. ACL 2023 조사 결과. Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li 등. 2023. 자신의 목소리로 외국어 말하기: 교차 언어 신경 코덱 언어 모델링. arXiv 사전 인쇄 arXiv:2303.03926.
